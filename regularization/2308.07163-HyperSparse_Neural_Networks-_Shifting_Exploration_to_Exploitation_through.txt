# 2308.07163.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/regularization/2308.07163.pdf
# File size: 966831 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HyperSparse Neural Networks: Shifting Exploration to Exploitation through
Adaptive Regularization
Patrick Glandorf∗, Timo Kaiser∗, Bodo Rosenhahn
Institute for Information Processing (tnt)
L3S - Leibniz Universit ¨at Hannover, Germany
{glandorf, kaiser, rosenhahn }@tnt.uni-hannover.de
Abstract
Sparse neural networks are a key factor in developing
resource-efficient machine learning applications. We pro-
pose the novel and powerful sparse learning method Adap-
tive Regularized Training (ART) to compress dense into
sparse networks. Instead of the commonly used binary mask
during training to reduce the number of model weights,
we inherently shrink weights close to zero in an iterative
manner with increasing weight regularization. Our method
compresses the pre-trained model “knowledge” into the
weights of highest magnitude. Therefore, we introduce a
novel regularization loss named HyperSparse that exploits
the highest weights while conserving the ability of weight
exploration. Extensive experiments on CIFAR and TinyIm-
ageNet show that our method leads to notable performance
gains compared to other sparsification methods, especially
in extremely high sparsity regimes up to 99.8%model spar-
sity. Additional investigations provide new insights into the
patterns that are encoded in weights with high magnitudes.1
1. Introduction
Recent years have shown tremendous progress in the
field of machine learning based on the use of neural net-
works (NN). Alongside the increasing accuracy in nearly all
tasks, also the computational complexity of NNs increased,
e.g., for Transformers [5,7] or Large Language Models [2].
The complexity causes high energy costs, limits the appli-
cability for cost efficient systems [10], and is counterpro-
ductive for the sake of fairness and trustworthiness due to
dwindling interpretability [38].
Facing these issues, recent years have also led to a grow-
ing community in the field of sparse NNs [12]. The goal is
to find small subgraphs ( a.k.a sparse NNs) in well perform-
ing NNs that have similar or comparable capabilities regard-
ing the main tasks while being significantly less complex
*These authors contributed equally to this work
1Code available at https://github.com/GreenAutoML4FAS/HyperSparse00.20.4|wi|Weight Magnitudes and Gradients
Epoch
0 20
30 40
0.7 0.8 0.9 1012
L1
Relative Weight Index idLHS
dwi
Figure 1: Magnitude of weights with their corresponding
gradients at different epochs derived from our HyperSparse
loss sorted by the weight magnitude. The weights and gra-
dients belong to a ResNet-32 trained on Cifar-100, where
the desired pruning rate is κ= 90% . The smallest weight
wκthat remains after pruning is marked by a dashed line.
Note that we added the gradient for the L1loss in green.
and therefore cheaper and potentially better interpretable.
Standard methods usually create sparse NNs by obtaining
a binary mask that limits the number of used weights in a
NN [20, 34, 42]. The most prominent method is Iterative
Magnitude Pruning (IMP) [16] that is based on the Lot-
tery Ticket Hypothesis (LTH) [9]. Assuming that impor-
tant weights have high magnitudes after training, it trains
a dense NN and removes an amount of elements from the
mask that correspond to the lowest weights. Afterward, the
sparse NN is reinitialized and retrained from scratch. The
process is iterated until a sparsity level is reached.
The assumption of magnitude pruning that highest
weights in dense NNs encode most important decision rules
for a diverse set of classes is problematic, because it is not
guaranteed. Removed weights that are potentially usefularXiv:2308.07163v2  [cs.CV]  16 Aug 2023

--- PAGE 2 ---
to the prediction can no longer be reactivated during fine-
tuning. In the worst case, a “layer collapse” can prohibit
a useful forward propagation [37]. The lack of exploration
ability still persists in the more accurate but resource con-
suming iterative IMP approach.
Reviving the key ideas of Han et al . [10] and Narang
et al. [27] (comparable to [26]), we introduce a lightweight
and powerful method called Adaptive Regularized Training
(ART) to obtain highly sparse NNs, which implicitly “re-
moves” weights with increasing regularization until a de-
sired sparsity level is reached. ART strongly regularizes
the weights before magnitude pruning. First, a dense NN
is pre-trained until convergence. In the second stage, the
NN is trained with an increasing and weight decaying regu-
larization until the hypothetical magnitude pruned NN per-
forms on par with the dense counterpart. Lastly, we ap-
ply magnitude pruning and fine-tune the NN without reg-
ularization. Avoiding binary masks in the second stage al-
lows exploration and regularization forces the exploitation
of weights that remain in the sparse NN. We introduce the
new regularization approach HyperSparse for the second
stage that overcomes static regularization like Lasso [39]
orWeight Decay [44] and adapts to the weight magnitude
by penalizing small weights. HyperSparse balances the ex-
ploration/exploitation tradeoff and thus increases the accu-
racy while leading to faster convergence in the second stage.
The combination of our regularization schedule and Hyper-
Sparse improves the classification accuracy and optimiza-
tion time significantly, especially in high sparsity regimes
with up to 99.8%zero weights. We evaluate our method
on CIFAR-10/100 [19] and TinyImageNet [6] with ResNet-
32 [11] and VGG-19 [33].
Moreover, we analyze the gradient and weight distribu-
tion during regularized training, showing that HyperSparse
leads to faster convergence to sparse NNs. The experiments
also shows that the claim of [34], that optimal sparse NNs
can be obtained via simple weight distribution heuristics,
does not hold in general. Finally, we analyze the process of
compressing dense NNs into sparse NNs and show that the
highest weights in NNs do not encode decision rules for a
diverse set of classes with equal priority.
In summary , this paper
• introduces HyperSparse , a superior adaptive regular-
ization loss that implicitly promotes configurable net-
work sparsity by balancing the exploration and ex-
ploitation tradeoff.
• introduces the novel framework ART to obtain sparse
networks using regularization with increasing lever-
age, which improves the optimization time and classi-
fication accuracy of sparse neural networks, especially
in high sparsity regimes.
• analyzes the continuous process of compressing pat-
terns from dense to sparse neural networks.2. Related Work
Sparse Learning methods that find binary masks to re-
move a predefined amount of weights can be categorized
as static or dynamic ( e.g., in [4, 12, 14]). According
to [4], in dynamic sparse training “[...] removed elements
[from masks] have chances to be grown back if they poten-
tially benefit to predictions” whereas static training incor-
porates fixed masks.
Static methods are usually based on Frankle et al . [9],
who introduce LTH and show that well performing sparse
NNs in random initialised NNs can be found after dense
training via magnitude pruning. The magnitude pruning
method is improved by IMP [16] that iterates the process.
Replacing the time consuming training procedure, methods
likeSNIP [20] or GraSP [42] find sparse NNs in random ini-
tialized dense NNs using a single network prediction and its
gradients. To also address the risk of layer collapse during
pruning, SynFlow [37] additionally conserves the total flow
in the network. Contrary to the latter works, Su et al . [34]
claim that appropriate sparse NNs do not depend on data or
weight initialization and provide a general heuristic for the
distribution of weights.
Different from static methods, dynamic methods prune
and re-activate zero elements in the binary mask. The
weights that are reactivated can be selected randomly [25]
or determined by the gradients [3, 4, 8]. For example,
RigL [8] iteratively prunes weights with low magnitude and
therefore reactivates weights with highest gradient. Also,
modern dynamic methods utilize continuous masks. For ex-
ample, Tai et al . [36] relax the IMP framework by introduc-
ing a parameterized softmask to obtain a weighted average
between IMP andTop-KAST [15]. Similar, [24, 31] relaxes
the binary mask and optimizes its L0-norm. Another way
is to inherently prune the model, e.g., by reducing the gra-
dients of weights with small magnitude [32]. Compared to
static methods, Liu et al . [22, 23] show that dynamic sparse
training methods overcomes most static methods by allow-
ing weight exploration.
Another property to distinguish modern sparse learning
methods is the complexity during mask generation, e.g., as
done by Schwarz et al . [32]. The more resource efficient
sparse→sparse methods sustain sparse NNs during train-
ing [4,8,20,32,34,37,42], whereas dense→sparse methods
utilize all parameters before finding the final mask [9, 15,
16, 24, 31, 36].
However, as explained later, our approach belongs to
dense→sparse methods that inherently reduce the model
complexity without masking before magnitude pruning to
obtain a static sparse mask for fine-tuning. We want to
mention the primary works of Han et al . [10], Narang et
al. [27] and Molchanov et al . [26] whose combination is
a role model for us. Han et al . useL1andL2regular-
ization to reduce the number of non-zero elements during

--- PAGE 3 ---
training. Their early framework uses regularization without
bells and whistles and has no ability to control the sparsity
level. Narang et al . and Molchanov et al . remove weights in
fine-grained portions with an increasing removal-threshold,
but do not incorporate weight exploration.
Interpretability and Understanding of machine learn-
ing is closely related to sparse learning and is also addressed
in this paper. There is an increasing number of works in re-
cent years that utilize sparse learning for other benefits, for
example, to find interpretable correlations between feature-
and image-space [38] or to visualize inter-class ambigui-
ties [18]. The work of Paul et al . [28] gives details about
the early learning stage which is crucial, e.g., to determine
memorization of label noise [17]. They show that most data
is not necessary to obtain suitable subnetworks. The general
relationship between LTH and generalization is investigated
in [30]. Varma et al . [35] show that sparse NNs are better
suited in data limited and noisy regimes. On the other hand
Hooker et al . [13] show that sparse NNs have non-trivial
impact on ethical bias by investigating which samples are
“forgotten” first during network compression. The underly-
ing research question of the latter work is altered to “Which
samples are compressed first?” and discussed in this paper.
3. Method
Sparsification aims to reduce the number of non-zero
weights in a NN. To address this problem, we use a cer-
tain schedule for regularization such that small weights con-
verge to zero and our model implicitly becomes sparse.
In Sec. 3.1, we formally define the sparsification prob-
lem. Then, we present Adaptive Regularized Training
(ART) in Sec. 3.2, which iteratively increases the leverage
of regularization to maximize the number of close-to-zero
weights. Moreover, we introduce our regularization loss
HyperSparse in Sec. 3.3 that is integrated in ART. It simul-
taneously allows the exploration of new topologies while
exploiting weights of the final sparse subnetwork.
3.1. Preliminaries
We consider a NN f(W, x)with topology fand weights
Wthat is trained to classify images from a dataset
S={(xn, yn)}N
n=1, where ynis the ground truth class
to an image sample xn. The training is structured in
epochs, which are iterative optimizations of the weights
W={w1, . . . , w D}over all samples in Sto minimize the
loss objective L. The obtained weights after epoch eare
denoted as We, with W0denoting the weights before opti-
mization. Furthermore, the classification accuracy of a NN
is measured by a rating function ψ(W).
The goal in sparsification is to reduce the cardinality of
Wby removing a pre-defined ratio of weights κ, while max-
imizing ψ(W). The network is pruned by the Hadamardproduct m⊙Wof a binary mask m∈[0,1]Dand the
model-weights W. The mask is usually created by apply-
ing magnitude pruning m=ν(W)[3, 4, 9, 16], which is a
technique that sets the κ-lowest weights to zero.
3.2. Adaptive Regularized Training (ART)
Regularization losses like the L1-norm ( Lasso -
regression) [39] or L2-norm [44] are used to prevent
overfitting by shrinking the magnitude of weights. We use
this effect in ART for sparsification, as weights with low
magnitude have low effect on changing the output and thus
can be removed with only little impact on ψ(W).
Regularization during training can be expressed as a
mixed loss
Ltotal=Lclass+λinit·ηe· Lreg, (1)
where Lclassis the classification loss and Lregthe regular-
ization loss. The gradient of Lregshrinks a set of weights to
approximately zero and creates a inherent sparse network
of an undefined pruning rate [39]. Increasing ηleverages
the regularization Lregin an ascending manner, but current
approaches use a fixed regularization rate η= 1[4, 10, 26].
After unregularized training of a dense NN to conver-
gence, ART employs the standard regularization framework
and modifies it by setting η > 1and a low initialisation
ofλinit. Subsequently, the regularization loss Lreghas al-
most no effect on Ltotalin the beginning, but starts to shrink
weights without much impact on Lclassto zero. However,
it allows every weight wito potentially get a high magni-
tude such that wiis shifted into the sparse NN of highest
weights (exploration). With increasing regularization, the
influence of the gradientdLreg
dwionwiincreases and is more
likely to overcome the gradientdLclass
dwi. Regularization im-
pedes proper exploration of small weights by pulling the
magnitude to zero. On the other hand, the larger weights
Algorithm 1 Adaptive Regularized Training (ART)
Parameter: Pre-trained weights Wpre, initial rate λinit, rat-
ing function ψ(W), magnitude pruning ν(W), increas-
ing factor η > 1, classification loss Lclass, regularization
lossLreg, training data S, optimizer SGD (W,L, S)
Result: Best weights for fine-tuning Wbest
1:W0, W best←Wpre
2:e←0
3:while ψ(ν(Wbest)⊙Wbest)< ψ(We)do
4: We+1←SGD(We,Lclass+λinit·ηe· Lreg, S)
5: ifψ(ν(We+1)⊙We+1)> ψ(ν(Wbest)⊙Wbest)then
6: Wbest←We+1
7: end if
8: e←e+ 1
9:end while

--- PAGE 4 ---
need to be exploited to conserve the classification results.
Therefore, our increasing regularization continually shifts
the exploration/exploitation tradeoff from exploration to ex-
ploitation. The method allows reordering weights to find
better topologies, but forces to exploit the highest weights
regarding the classification task. Due to the increasing
number of weights that are approximately zero, the dense
model converges to a inherently sparse model. We stop
the regularized training if the NN with best pruned weights
ψ(ν(Wbest)⊙Wbest)has higher accuracy than with the latest
unpruned weights ψ(We)and choose Wbestas our candidate
for fine-tuning.
The overall training pipeline is defined as follows:
Step 1: Pre-train dense model until convergence without
regularization.
Step 2: Remove weights implicitly using ART as de-
scribed in algorithm 1.
Step 3: Apply magnitude pruning and fine-tune pruned
network until convergence.
ART relaxes the iterative IMP approach that prunes the
least important weights over certain iterations. Analogous
to the increasing pruning ratio in standard iterative methods,
we iteratively increase the amount of weights that are close
to zero and thus approximate a binary mask implicitly.
3.3. HyperSparse Regularization
The latter Section 3.2 describes the process of shrink-
ing weights in Wby penalizing with ascending regulariza-
tion. A drawback of this procedure is that also weights
that remain after pruning are penalized by the regulariza-
tion. This negatively affects the exploitation regarding the
main task. Thus, remaining weights should not be penal-
ized. On the other hand, if small weights are strongly pe-
nalized, the desired exploration property of dynamic prun-
ing methods to “grow” back these elements is restricted. To
address this tradeoff between exploitation and exploration,
we introduce the sparsity inducing adaptive regularization
lossHyperSparse .
Incorporating the Hyper bolic Tangent function applied
on the magnitude denoted as t(·) = tanh( |·|)for simplicity,
theHyperSparse loss is defined as
LHS(W) =1
A|W|X
i=1
|wi||W|X
j=1t(s·wj)
−|W|X
i=1|wi|
with A:=X
w∈Wt(s·w)
and∀w∈W:dA
dw= 0,(2)
where Ais treated as a pseudo-constant in the gradient com-
putation and sis an alignment factor that is described later.The regularization penalizes weights depending on the gra-
dient and can vary for different weights. The gradient of
HyperSparse with respect to a weight wiis approximately
dLHS(W)
dwi=sign(wi)·t′(s·wi)·P|W|
j=1|wj|
P|W|
j=1t(s·wj),
with wi, wj∈W, t′(·)∈(0,1].(3)
The derivative t′=dt
dwiconverges towards 1for small
magnitudes |wi| ≈0and towards 0for large magnitudes
|wi| ≫0. Thus, the second term in Eq. (3) is adaptive to
the weights and highly penalizes small magnitudes, but is
breaking down to zero for large ones. Details for the gradi-
ent calculation and analysis can be found in the supplemen-
tary material, Sec. D.
The alignment factor sis mandatory to exploit the afore-
mentioned properties for the sparsification task with a spe-
cific pruning rate κ. Since LHSis dependent on the weights
magnitude, but there is no determinable value range for
weights, our loss LHSis not guaranteed to adapt reasonably
to a given W. For example, considering a fixed s= 1 and
all weights in Ware close to zero, the gradient from Eq. (3)
results into nearly the same value for every weight. There-
fore, we adapt sto the smallest weight |wκ|that would re-
main after magnitude pruning, such that t′′′(s·wκ) = 0 ,
which is the point of inflection of t′. According to this
alignment, the gradients in Eq. (3) of remaining weights
|w| ≥ | wκ|are shifted closer to 1 and are increased for
weights |w| ≤ |wκ|, while adhering a smooth gradient from
remaining to removed weights. Moreover, the denominator
in Eq. (3) decreases over time, if more weights in Ware
close to zero subsequent to ascending regularization. The
gradient for different weight distributions of a NN based on
HyperSparse is shown in Fig. 1 and visualizes the described
gradient behavior of adaptive weight regularization.
4. Experiments
This section presents experiments showing that our pro-
posed method ART outperforms comparable methods, espe-
cially in extreme high sparsity regimes. Our experimental
setup is described in Sec. 4.1. In the subsequent section,
we show that HyperSparse has a large positive impact on
the optimization time and classification accuracy. This im-
provement is explained by analyzes of the tradeoff between
exploration and exploitation, the gradient and weight distri-
bution in Sec. 4.3 and 4.4. Finally, we analyze and discuss
the compression behaviour during regularized training and
derive further insights about highest magnitude weights in
Sec. 4.5.
4.1. Experimental Setup
We evaluate ART on the datasets CIFAR-10/100 [19] and
TinyImageNet [6] to cover different complexities, given by

--- PAGE 5 ---
ψ↑ResNet-32 −→ VGG-19 −→
κ= 90% κ= 95% κ= 98% κ= 99% κ= 99.5% κ= 99.8% κ= 90% κ= 95% κ= 98% κ= 99% κ= 99.5% κ= 99.8%CIFAR-10No Mask 94.70 ±0.19 93.84±0.12
SNIP [20] 92.72 ±0.18 91.35±0.15 88.02±0.27 83.94±0.39 71.64±7.46 23.72±21.11 93.63±0.25 93.36±0.20 76.12±21.96 10.00±0.00 10.00±0.00 10.00±0.00
GraSP [42] 92.86 ±0.19 91.80±0.23 89.00±0.24 85.63±0.28 80.25±0.67 62.56±11.25 92.97±0.04 92.79±0.24 92.16±0.14 91.27±0.15 51.45±38.46 10.00±0.00
SRatio [34] 93.02 ±0.17 91.85±0.16 88.91±0.16 85.97±0.22 80.73±0.37 64.38±0.50 93.86±0.19 93.58±0.20 92.33±0.24 91.14±0.21 89.14±0.15 43.64±20.04
LTH [9] 92.68 ±0.32 91.45±0.19 88.48±0.15 85.99±0.30 81.19±0.40 69.34±0.43 93.71±0.17 93.31±0.15 41.17±42.76 10.00±0.00 10.00±0.00 10.00±0.00
IMP [16] 94.69±0.17 94.00±0.18 91.35±0.18 87.35±0.55 82.00±0.34 69.12±0.50 93.96±0.17 94.02±0.07 93.48±0.24 91.29±0.29 25.43±34.50 10.00±0.00
RigL [8] 94.21 ±0.10 93.07±0.22 90.65±0.17 86.50±0.83 62.89±5.18 32.78±4.11 93.48±0.13 92.92±0.14 91.41±0.15 89.08±0.37 84.79±0.90 70.81±1.10
ART +L1 94.20±0.16 93.14±0.16 91.34±0.46 88.18±1.07 84.52±1.24 79.35±1.85 93.97±0.13 93.82±0.10 93.85±0.12 93.10±0.23 92.17±0.25 90.42±0.50
ART +L2 93.49±0.21 92.91±0.24 89.60±0.73 85.80±2.38 82.24±0.60 71.73±0.88 93.18±0.18 92.65±0.40 79.38±4.92 78.85±8.74 72.68±2.67 56.28±26.33
ART +LHS(no preTrain) 93.13±0.13 92.85±0.18 91.79±0.14 90.79±0.30 89.01±0.21 84.64±0.51 93.58±0.12 93.53±0.09 93.15±0.12 92.56±0.08 92.12±0.13 91.24±0.09
ART +LHS 94.22±0.20 93.76±0.18 92.69±0.22 91.16±0.28 89.35±0.23 84.45±0.55 93.93±0.20 93.83±0.10 93.75±0.23 93.51±0.15 92.91±0.10 91.62±0.19CIFAR-100No Mask 74.60 ±0.14 72.88±0.34
SNIP [20] 69.78 ±0.22 65.54±0.26 53.20±0.30 37.45±1.42 14.76±3.35 04.52±2.16 72.76±0.20 71.50±0.27 25.34±9.16 1.00±0.00 1.00±0.00 1.00±0.00
GraSP [42] 69.64 ±0.38 66.84±0.14 59.59±0.30 49.42±1.04 36.46±2.73 15.62±3.20 71.10±0.13 70.39±0.17 68.25±0.45 65.84±0.36 59.56±0.47 1.10±0.10
SRatio [34] 69.80 ±0.18 67.08±0.41 60.44±0.32 51.60±0.63 38.57±0.75 18.35±0.97 72.84±0.32 71.67±0.19 68.84±0.38 65.00±0.22 51.16±2.67 1.02±0.04
LTH [9] 69.23 ±0.31 66.80±0.49 60.28±0.10 51.92±0.11 40.18±0.28 20.31±1.63 72.55±0.27 70.46±0.26 9.80±15.98 1.00±0.00 1.00±0.00 1.00±0.00
IMP [16] 73.91 ±0.37 71.21±0.36 64.67±0.29 55.89±0.34 41.53±0.74 14.97±0.69 73.92±0.33 73.77±0.32 70.99±0.34 4.03±4.69 1.00±0.00 1.00±0.00
RigL [8] 73.09 ±0.29 71.46±0.37 64.46±0.36 45.58±1.78 21.80±1.54 8.47±4.24 72.00±0.24 70.42±0.30 67.48±0.36 63.31±0.51 55.56±1.33 24.57±13.21
ART +L1 73.16±0.45 70.98±0.48 66.10±0.76 59.36±2.08 50.50±3.43 37.43±1.64 73.16±0.20 72.80±0.20 71.23±0.25 69.18±0.22 65.71±0.63 59.08±1.07
ART +L2 71.39±0.60 68.21±1.25 58.49±3.93 56.61±0.88 47.11±1.00 28.73±1.18 61.54±3.91 55.22±7.34 44.42±6.14 39.40±4.78 26.94±23.75 29.78±16.25
ART +LHS(no preTrain) 72.49±0.35 71.57±0.36 69.08±0.12 65.48±0.28 59.49±0.53 48.63±0.66 71.49±0.42 70.24±0.67 68.57±0.38 67.59±0.47 65.59±0.17 61.66±0.50
ART +LHS 74.08±0.13 72.85±0.31 70.08±0.37 65.86±0.26 59.58±0.26 48.31±0.53 73.23±0.24 72.70±0.41 71.97±0.13 70.83±0.23 69.02±0.36 64.53±0.24TinyImageNetNo Mask 62.87 ±0.27 61.41±0.12
SNIP [20] 55.23 ±0.47 48.78±0.40 34.93±0.83 23.20±1.41 12.25±1.50 3.19±1.52 61.47±0.16 59.00±0.20 4.77±4.23 0.50±0.00 0.50±0.00 0.50±0.00
GraSP [42] 56.16 ±0.25 51.52±0.47 40.32±2.24 28.41±1.26 15.81±2.30 4.29±3.73 60.50±0.08 58.97±0.14 56.70±0.12 53.12±0.49 43.76±0.40 0.51±0.03
SRatio [34] 55.19 ±0.35 51.70±0.48 44.04±0.36 34.14±0.12 8.31±1.26 1.98±0.29 61.21±0.19 59.10±0.32 55.94±0.24 51.13±0.34 39.76±0.32 0.50±0.00
LTH [9] 55.72 ±0.22 52.22±0.48 43.73±0.85 33.22±0.39 20.78±0.40 7.65±0.58 59.91±0.59 58.74±0.43 56.38±0.16 54.02±0.60 46.78±0.81 2.89±2.32
IMP [16] 60.71 ±0.24 56.97±0.26 47.29±0.57 33.21±0.11 8.59±0.67 2.40±0.34 62.42±0.32 61.28±0.23 57.39±0.10 54.26±0.26 47.19±0.28 3.10±0.96
RigL [8] 59.29 ±0.21 55.53±0.16 44.72±1.34 26.07±1.59 8.76±0.30 4.51±0.37 61.47±0.29 61.69±0.41 59.41±0.53 54.59±0.68 47.11±0.62 20.81±0.99
ART +L1 58.00±0.49 55.30±0.94 46.59±0.45 39.34±1.32 29.80±3.53 18.06±2.89 61.29±0.24 60.21±0.31 57.04±0.53 54.61±0.77 51.27±1.80 43.59±1.33
ART +L2 56.94±0.76 51.40±0.99 43.03±2.23 35.19±1.42 24.62±1.96 7.79±0.59 60.62±0.69 51.10±5.94 47.96±7.73 45.90±8.76 30.32±17.94 5.55±11.30
ART +LHS(no preTrain) 57.96±0.39 57.01±0.34 53.27±0.32 47.32±0.46 40.53±0.26 28.96±0.69 60.95±0.27 59.67±0.17 56.72±0.48 53.79±0.33 51.66±0.19 47.49±0.17
ART +LHS 60.97±0.18 58.78±0.28 53.92±0.14 47.97±0.42 40.68±0.82 28.95±0.52 61.55±0.24 61.36±0.31 59.79±0.25 58.01±0.21 55.34±0.22 49.44±0.18
Table 1: Classification accuracy of sparse NNs for varying pruning rates κbased on our proposed method ART withL1,L2,
andHyperSparse regularization LHScompared to dense models, and masks obtained by SNIP [20], GraSP [42], SRatio [34],
LTH [9],IMP [16] and RigL [8]. The best accuracy per configuration is highlighted, the second is underlined. It shows that
our method outperforms IMP significantly in the domain of high sparsity. We recommend the pdf version and zooming in.
#Epochs ↓ResNet-32 −→ VGG-19 −→
κ: 90% 98% 99.5% 90% 98% 99.5%CIFAR-
10L1 34.2±3.1 68.2±4.8 94.2±8.0 5.8±0.4 24.2±2.4 56.0±3.0
L2 75.6±63.6 49.6±92.45 116.6±19.5 116.2±5.3175.8±15.8 178.4±9.7
LHS 26.6±1.8 55.4±2.3 77.8±1.9 4.0±0.0 18.2±1.6 42.8±1.6CIFAR-
100L1 53.2±2.8 77.5±4.1 101.3±9.6 11.2±0.4 47.7±2.1 66.5±4.6
L2 112.0±7.6141.8±17.0 120.0±11.4 153.2±2.9 168.8±6.9 75.4±122.7
LHS 39.2±0.84 63.4±0.5 88.2±0.8 8.2±0.4 35.8±0.4 55.8±0.4Tiny-
Image-
NetL1 52.0±3.4 81.6±3.5 101.2±10.8 20.8±0.4 43.2±3.5 59.0±8.0
L2 110.2±7.3129.8±11.6 93.2±45.5 27.6±81.1 148.4±22.3 107.4±94.6
LHS 36.6±1.3 67.8±3.8 100.0±9.3 14.4±0.5 34.0±0.0 52.3±1.5
Table 2: Number of epochs with regularization to obtain the
final mask, evaluated for multiple datasets, network topolo-
gies, and pruning rates κ. It shows that our HyperSparse
LHSloss reduces the training time significantly.
a varying number of class labels. Furthermore, we use dif-
ferent model complexities, where ResNet-32 [11] is a sim-
ple model with 1.8 M parameters and VGG-19 [33] is a
complex model with 20 M parameters. Note that we use
the implementation given in [34]. As explained in Sec. 3.2,
we group our training in 3 steps. First we train our model
for 60 epochs until convergence (step 1), using a constant
learning rate of 0.1. In the following regularization step, we
initialize the regularization with λinit= 5·10−6,η= 1.05,
and use the same learning rate as used in pre-training. Thefine-tuning-step (step 3) is similar to [34], as we train for
160 epochs in CIFAR-10/100 and for 300 epochs on Tiny-
ImageNet, using a learning rate of 0.1and apply a mul-
tiplied decay of 0.1 at 2/4 and 3/4 of the total number of
epochs. We also adapt the batch size of 64 and weight-
decay of 10−4. All experiments are averaged over 5 runs.
We compare our method ART toSNIP [20], Grasp [42],
SRatio [34], and LTH [9] similar as done in [34, 41]. In ad-
dition we evaluate IMP [16] and RigL [8] as dynamic prun-
ing methods. For comparability, all competitors in our ex-
periments are trained with the same setup as given in the
fine-tuning-step. To improve the performance of RigL , we
extend the training duration by 360 epochs. Further details
are given in the supplementary material, Sec. A.
4.2. Sparsity Level
In this section, we compare the performances of
ART to other methods on different sparsity levels
κ∈ {90%,95%,98%,99%,99.5%,99.8%}, using differ-
ent datasets and models. To demonstrate the advantages of
our novel regularization loss, we additionally substitute Hy-
perSparse withL1[39] and L2[44]. Table 1 shows the
resulting accuracies with standard deviations.
Our method ART combined with HyperSparse outper-

--- PAGE 6 ---
02040608010012014016018020000.20.40.60.81
EpochRelative IntersectionIntersection between largest Weights and final Mask
LHS
L1
L2
Figure 2: Intersection of the set of weights with highest
magnitude during training and the final mask measured dur-
ingART with ResNet-32, CIFAR-100, pruning rate κ=
98% and different regularization losses. Horizontal bars
mark the intersection one epoch before pruning and the
dashed line at epoch 60indicates the start of regularization.
OurHyperSparse loss reduces the optimization time and the
high intersection before pruning suggests a higher stability
during regularization, which leads to better exploitation.
formes the methods SNIP [20], Grasp [42], SRatio [34],
LTH [9] and RigL [8] on all sparsity levels. Considering
the high sparsity of 99%,99.5%and99.8%, all competitors
drop drastically in accuracy, even to the minimal classifica-
tion bound of random prediction for SNIP and LTH using
VGG-19. However, ART is able keep high accuracy even
on extreme high sparsity levels. In comparison to the regu-
larization losses L1andL2, our HyperSparse loss achieves
higher accuracy in nearly all settings and even minimizes
the variance. If we skip the Pre-train-step (step 1) of ART,
the performance slightly drops. However, ART without pre-
training still has good results.
Moreover, we present the number of trained epochs for
the regularization phase (step 2) in Tab. 2. In almost all
cases, HyperSparse requires less epochs to terminate com-
pared to L1andL2and converges faster to a well perform-
ing sparse model. As a second aspect, ART dynamically
varies the training-length to the sparsity level, model and
data complexity. Thus, ART trains longer if higher sparsity
is required or the model has more parameters and is more
complex like VGG-19. In comparison of the two datasets
CIFAR-10 and CIFAR-100, which have the same number
of training samples and thus the same number of optimiza-
tion steps per epoch, ART extends the training-length for
the more complex classification problem in CIFAR-100.
ART trains the model for 60 epochs in pre-training
(step 1) and 160 epochs in fine-tuning (step 3). Considering
the dynamic training-length in step 2, the epochs of ART
usingLHSsum up from 226.2to301.2epochs in mean. In
comparison, iterative pruning methods are computationally
much more expensive, since each model is trained multi-
ple times. For example, IMP [16] requires 860 epochs on
CIFAR-10/100 in our experiments.RES
DSRES
DSRES
LLWeight Distribution after Regularization
1 1114 2225 3302,0004,0006,000
Layer Index iWeights per Layer
IMP SRatio ART +LHS
Figure 3: Distribution of weights per layer after pruning
in a ResNet-32 model that is trained on CIFAR-100 with
pruning rate κ= 98% . Layer index idescribes the execu-
tion order. We group the model in residual blocks (RES),
downsampling blocks (DS) and the linear layer (LL). Our
method distributes the weights comparable to IMP [16], but
it has more weights in the downsampling layers.
4.3. Exploration and Exploitation aware Gradient
The training-schedule of ART allows to explore new
topologies of sparse networks, while compressing the dense
network into the remaining weights that are exploited to
minimize the loss Lclass. To reduce the tradeoff between
exploration and exploitation, our regularization loss Hyper-
Sparse penalizes small weights with a higher regularization
and forces the most weights to be close to zero, while pre-
serving the magnitude of weights that remain after pruning.
To highlight the beneficial behaviour of HyperSparse , this
section visualizes and analyzes the gradient. Fig. 1 shows
the values and the corresponding gradients of all weights,
sorted by the weights magnitude. Note that we only fo-
cus on the second step of ART, where the regularization is
incorporated. Epoch 0represents the first epoch using reg-
ularization.In the lower subfigure, we observe that the gra-
dient of HyperSparse with respect to weights larger than
|wκ|is closer to 0 than for smaller weights. In compari-
son,L1remains constantly 1 for all weights. The effect
of increasing regularization of small weights is stronger for
networks with more weights close to zero and therefore am-
plifies over time, since increasing regularization shrinks the
weights magnitude. For example, epoch 40 shows higher
gradients for small weights compared to epoch 0, while hav-
ing more weights with lower magnitude. The pruning-rate
κdependent LHSincreases the gradient for small weights
|w|<|wκ|over time but conserves the low gradient of
larger weights |w|>|wκ|approximately at 0 to favor

--- PAGE 7 ---
0-2500(a) Dense Model ( κ= 0% )
0-2500 (b) Low Sparsity ( κ= 90% )
0-2500 (c) High Sparsity ( κ= 99.8%)
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
Figure 4: First 5% CIFAR-10 samples that are compressed into the remaining highest weights after pruning with κ∈
{0%,90%,99.8%}deduced by the CP-metric. While dense networks learn samples approximately uniform-distributed over
classes, the highest weights compress decision rules only for a subset of classes in the early learning stage. Note that we
sampled by factor 10 for visualization purposes and ellipses represent the double standard deviation of cluster centers.
CPHuman
Label
ErrorsCIFAR-10 Class
deer bird cat truck airplane horseDense
Model
(0%)0 0.482 0.554 0.639 0.400 0.452 0.429
1 0.548 0.632 0.722 0.479 0.547 0.493
2 0.653 0.741 0.808 0.588 0.676 0.710
3 0.760 0.823 0.862 0.695 0.783 0.769Low
Sparsity
(90%)0 0.166 0.499 0.494 0.567 0.580 0.524
1 0.217 0.573 0.601 0.613 0.671 0.570
2 0.292 0.678 0.721 0.676 0.790 0.737
3 0.392 0.772 0.796 0.710 0.841 0.808High
Sparsity
(99.8%)0 0.056 0.296 0.387 0.622 0.798 0.824
1 0.061 0.317 0.411 0.647 0.835 0.835
2 0.069 0.342 0.439 0.677 0.882 0.893
3 0.081 0.363 0.469 0.699 0.902 0.929
Table 3: Compression Position (see Sec. 4.5) for dense NNs
(during pre-training) and κpruned NNs (during regulariza-
tion) for six CIFAR-10 classes. Samples of a class are split
into 4 subsets according to the number of human label errors
in CIFAR-N to indicate the difficulty. In sparse networks,
different classes are compressed at different times and diffi-
cult samples are compressed later. All classes and pruning
rates can be found in the supplementary material, Tab. 2.
exploitation. During optimization, the gradient remains
smooth and increases slowly for weights that are smaller,
but close to |wκ|. This favors exploration in the domain
of weights close to wκ. Therefore, the model becomes in-
herently sparse and the behaviour shifts continuously from
exploration to exploitation.
4.4. Reordering Weights
We use the regularization loss with ascending leverage
to find a reasonable set of weights, that remain after prun-
ing. We implicitly do this by shrinking small weights close
to zero. During training, weights are reordered and thus
can change the membership from the set of pruned to re-
maining weights, and vice versa. We analyze the reordering
procedure in Fig. 2, which shows the intersection of the in-
termediate and final mask over all epochs, using differentregularization losses in ART. The model is pre-trained to
convergence without regularization for the first 60 epochs
(step 1) and with regularization in further epochs (step 2).
Fine-tuning is not visualized (step 3). After pre-training,
the highest weights only intersects up to 20% with the final
mask obtained by L1andL2, while HyperSparse leads to an
intersection of approximately 35%. This results show Hy-
perSparse changes less parameter while reordering weights,
which implies that more structures from the dense model
are exploited. It also shows that HyperSparse has a sig-
nificantly smaller learning duration than L1andL2. The
horizontal bars point to the intersection before last training
epoch and show that L1andL2only intersect by 60% and
50%, while HyperSparse is getting very close to the final
mask with more than 90% intersection. This indicates that
HyperSparse finds a more stable set of high valued weights
and reduces exploration, as the mask has less variation in
the final epochs. More results for other training settings are
shown in the supplementary material, Sec. B.
Moreover, we analyze the resulting weight distribution
of our method and compare it to IMP [16] and SRatio [34].
Fig. 3 shows the number of remaining weights per layer
for ResNet-32 that consists of three scaling levels, which
end up with the linear layer (LL). Each scaling level con-
sists of four residual blocks (RES), which are connected by
a downsampling-block (DS). The basic topology of ART
andIMP looks similar, since both methods show a con-
stant keep-ratio over the residual blocks. Furthermore, ART
andIMP use more parameters in downsampling and lin-
ear layers. We conclude that these two layer types require
more weights and consequently are more important to the
model. The higher accuracy discussed earlier suggest that
our method exploit these weights better. To show that this
results are also obtained on other datasets, models, and spar-
sity levels, we describe further weight distributions in the
supplementary material, Sec. C and show that the number
of parameters in the linear layer decreases drastically for a
small set of classes in CIFAR-10. Moreover, the compared

--- PAGE 8 ---
method SRatio assumes that suitable sparse networks can
be obtained using handcrafted keep-ratios per layer. It has
a quadratic decreasing keep-ratio that can be observed in
Fig. 3. As shown in Tab. 1, our method ART performs sig-
nificantly better than SRatio and therefore we deduce that
fixed keep-ratios have an adverse effect on performance.
Reordering weights during training favors well performing
sparse NNs, especially in high sparsity regimes.
4.5. What do networks compress first?
Along with the introduction of ART, we are faced with
the question of which patterns are compressed first into
the large weights that remain after magnitude pruning dur-
ing regularization. This question is in contrast to Hooker’s
question “What Do Compressed Deep Neural Networks
Forget?” [13] and challenges the fundamental assumption
of magnitude pruning, which assumes large weights to be
most important. In this section, we analyze the chronologi-
cal order of how samples are compressed and introduce the
metric Compression Position (CP) to determine it.
According to our method, regularization starts at epoch
eSand ends at eEand therefore the weights Whave dif-
ferent states W={We}eEe=eSduring training. We measure
the individual accuracy over time ψIreached by the sparse
network for a training sample (x, y)∈S, defined by
ψI 
x, y, f, W
=
We∈ W | f(ν(We)⊙We, x) =y	
eE−eS.
(4)
After computing the individual accuracy for all samples
Ψ =
ψI 
xn, yn, f,W	N
n=1and sorting Ψin descending
order, the metric CP 
x, y, f, W
describes the relative po-
sition of ψI 
x, y, f, W
in sort (Ψ). In other words, early
compressed and correctly classified samples obtain a low
CP close to 0, and those compressed later closer to 1.
We calculate the CP metric for all samples in CIFAR-10
during training of dense, low, and high sparsity NNs. The
compression behaviour for dense NNs is measured during
the pre-training phase ( eS= 0andeE= 60 ) and for sparse
NNs during regularization phase ( eS= 60 andeE=emax).
To show, which samples are compressed first into the re-
maining highest weights, the 5%samples with lowest CP
are visualized in Fig. 4 in the latent space of the well known
CLIP framework [29] mapped by t-SNE [40]. As com-
monly known, the dense model compresses easy samples
of all classes in the early stages [17, 21], while the low
sparsity model already loses some. In the high sparsity
regime no discriminative decision rules are left at beginning
of training, and the remaining classes are compressed step
by step as the training continues (see supplementary mate-
rial, Sec. E). In our experiments, we have seen continuously
that there is a bias towards the class deer. We call this effect
“the deer bias” , which must be reduced with regularisation.Thedeer bias suggests that large weights in dense NNs do
not encode decision rules for all classes.
To quantify the above results, Tab. 3 shows the aver-
age CP for all samples belonging to a specific class. Ad-
ditionally, we split the class sets into four subsets accord-
ing to their difficulty. We estimate the difficulty of a sam-
ple by counting the human label errors that are made from
three human annotators derived from CIFAR-N [43], e.g.,
2 means that two of three persons mislabeled the sample.
The first observation is that the above mentioned separa-
tion of classes is confirmed, since CP values are similar in
dense NNs, but diverge in sparse NNs. In high sparsity
regimes, the deer bias is persistent before first samples of
other classes are compressed. The classes horse andair-
plane are only included at the end of the training. The sec-
ond observation is, that within a closed set of samples be-
longing to a class, difficult samples are compressed later.
This nature is similar to the training process of dense NNs.
Implementation details and more fine-grained results are
available in the supplementary material, Sec. E.
5. Conclusion
Our work presents Adaptive Regularized Training
(ART), a method that utilizes regularization to obtain sparse
neural networks. The regularization is amplified contin-
uously and used to shrink most weight magnitudes close
to zero. We introduce the novel regularization loss Hyper-
Sparse that induces sparsity inherently while maintaining a
well balanced tradeoff between exploration of new sparse
topologies and exploitation of weights that remain after
pruning. Extensive experiments on CIFAR and TinyIma-
geNet show that our novel framework outperforms sparse
learning competitors. HyperSparse is superior to standard
regularization losses and leads to impressive performance
gains in extremely high sparsity regimes and is much faster.
Additional investigations provide new insights about the
weight distribution during network compression and about
patterns that are encoded in high valued weights.
Overall, this work provides new insights into sparse
neural networks and helps to develop sustainable machine
learning by reducing neural network complexity.
6. Acknowledgments
This work was supported by the Federal Ministry of
Education and Research (BMBF), Germany under the
project AI service center KISSKI (grant no. 01IS22093C),
the Deutsche Forschungsgemeinschaft (DFG) under Ger-
many’s Excellence Strategy within the Cluster of Excel-
lence PhoenixD (EXC 2122), and by the Federal Ministry of
the Environment, Nature Conservation, Nuclear Safety and
Consumer Protection, Germany under the project GreenAu-
toML4FAS (grant no. 67KI32007A).

--- PAGE 9 ---
References
[1] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David
Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan
Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and
Simon Lacoste-Julien. A closer look at memorization in deep
networks. In International Conference on Machine Learning
(ICML) , 2017. 5
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In Conference on Neural Information Pro-
cessing Systems (NeurIPS) , 2020. 1
[3] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang,
and Zhangyang Wang. Chasing sparsity in vision transform-
ers: An end-to-end exploration. In Conference on Neural
Information Processing Systems (NeurIPS) , 2021. 2, 3
[4] Tianlong Chen, Zhenyu Zhang, pengjun wang, Santosh Bal-
achandra, Haoyu Ma, Zehao Wang, and Zhangyang Wang.
Sparsity winning twice: Better robust generalization from
more efficient training. In International Conference on
Learning Representations (ICLR) , 2022. 2, 3
[5] Yuren Cong, Michael Ying Yang, and Rosenhahn Bodo.
Reltr: Relation transformer for scene graph generation. IEEE
Transactions on Pattern Analysis and Machine Intelligence
(PAMI) , 2023. 1
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2009. 2, 4
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions (ICLR) , 2021. 1, 4
[8] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Cas-
tro, and Erich Elsen. Rigging the lottery: Making all tickets
winners. In International Conference on Machine Learning
(ICML) , 2020. 2, 5, 6
[9] Jonathan Frankle and Michael Carbin. The lottery ticket hy-
pothesis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations (ICLR) ,
2018. 1, 2, 3, 5, 6
[10] Song Han, Jeff Pool, John Tran, and William Dally. Learning
both weights and connections for efficient neural network.
InConference on Neural Information Processing Systems
(NeurIPS) , 2015. 1, 2, 3
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Conferenceon Computer Vision and Pattern Recognition (CVPR) , 2016.
2, 5
[12] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dry-
den, and Alexandra Peste. Sparsity in deep learning: Prun-
ing and growth for efficient inference and training in neural
networks. Journal of Machine Learning Research (JMLR) ,
2021. 1, 2
[13] Sara Hooker, Aaron Courville, Gregory Clark, Yann
Dauphin, and Andrea Frome. What do compressed deep neu-
ral networks forget? In arXiv:1911.05248 , 2019. 3, 8
[14] Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding,
and Zhangyang Wang. Training your sparse neural network
better with any mask. In International Conference on Ma-
chine Learning (ICML) , 2022. 2
[15] Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon
Osindero, and Erich Elsen. Top-kast: Top-k always sparse
training. In Conference on Neural Information Processing
Systems (NeurIPS) , 2020. 2
[16] Karolina Gintare Jonathan Frankle, Dziugaite, Daniel Roy,
and Michael Carbin. Linear mode connectivity and the lot-
tery ticket hypothesis. In International Conference on Ma-
chine Learning (ICML) , 2020. 1, 2, 3, 5, 6, 7, 8
[17] Timo Kaiser, Lukas Ehmann, Christoph Reinders, and Bodo
Rosenhahn. Blind knowledge distillation for robust image
classification. In arXiv:2211.11355 , 2022. 3, 8, 5
[18] Timo Kaiser, Christoph Reinders, and Bodo Rosenhahn.
Compensation learning in semantic segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops , 2023. 3
[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. 2009. 2, 4
[20] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
Snip: Single-shot network pruning based on connection sen-
sitivity. In International Conference on Learning Represen-
tations (ICLR) , 2018. 1, 2, 5, 6
[21] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Car-
los Fernandez-Granda. Early-learning regularization pre-
vents memorization of noisy labels. In Conference on Neural
Information Processing Systems (NeurIPS) , 2020. 8, 5
[22] Shiwei Liu, Tim Van der Lee, Anil Yaman, Zahra Atash-
gahi, Davide Ferraro, Ghada Sokar, Mykola Pechenizkiy,
and Decebal Constantin Mocanu. Topological insights into
sparse neural networks. In European Conference on Machine
Learning and Knowledge Discovery in Databases (ECML
PKDD) , 2021. 2
[23] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and
Mykola Pechenizkiy. Do we actually need dense over-
parameterization? in-time over-parameterization in sparse
training. In International Conference on Machine Learning
(ICML) , 2021. 2
[24] Christos Louizos, Max Welling, and Diederik P. Kingma.
Learning sparse neural networks through l0 regularization.
InInternational Conference on Learning Representations
(ICLR) , 2018. 2
[25] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone,
Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta.
Scalable training of artificial neural networks with adaptive

--- PAGE 10 ---
sparse connectivity inspired by network science. Nature
communications , 2018. 2
[26] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for
resource efficient inference. In International Conference on
Learning Representations (ICLR) , 2017. 2, 3
[27] Sharan Narang, Greg Diamos, Shubho Sengupta, and Erich
Elsen. Exploring sparsity in recurrent neural networks. In In-
ternational Conference on Learning Representations (ICLR) ,
2017. 2
[28] Mansheej Paul, Brett W Larsen, Surya Ganguli, Jonathan
Frankle, and Gintare Karolina Dziugaite. Lottery tickets on
a data diet: Finding initializations with sparse trainable net-
works. In Conference on Neural Information Processing Sys-
tems (NeurIPS) , 2022. 3
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In International
Conference on Machine Learning (ICML) , 2021. 8, 4
[30] Keitaro Sakamoto and Issei Sato. Analyzing lottery
ticket hypothesis from PAC-bayesian theory perspective.
InConference on Neural Information Processing Systems
(NeurIPS) , 2022. 3
[31] Pedro Savarese, Hugo Silva, and Michael Maire. Winning
the lottery with continuous sparsification. In Conference on
Neural Information Processing Systems (NeurIPS) , 2020. 2
[32] Jonathan Schwarz, Siddhant Jayakumar, Razvan Pascanu,
Peter E Latham, and Yee Teh. Powerpropagation: A spar-
sity inducing weight reparameterisation. In Conference on
Neural Information Processing Systems (NeurIPS) , 2021. 2
[33] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations (ICLR) ,
2015. 2, 5
[34] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi
Gao, Liwei Wang, and Jason D Lee. Sanity-checking prun-
ing methods: Random tickets can win the jackpot. In Confer-
ence on Neural Information Processing Systems (NeurIPS) ,
2020. 1, 2, 5, 6, 7, 8
[35] Mukund Varma T, Xuxi Chen, Zhenyu Zhang, Tianlong
Chen, Subhashini Venugopalan, and Zhangyang Wang.
Sparse winning tickets are data-efficient image recognizers.
InConference on Neural Information Processing Systems
(NeurIPS) , 2022. 3
[36] Kai Sheng Tai, Taipeng Tian, and Ser-Nam Lim. Spar-
tan: Differentiable sparsity via regularized transportation.
InConference on Neural Information Processing Systems
(NeurIPS) , 2022. 2
[37] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya
Ganguli. Pruning neural networks without any data by iter-
atively conserving synaptic flow. In Conference on Neural
Information Processing Systems (NeurIPS) , 2020. 2
[38] Bodo Rosenhahn Thomas Norrenbrock, Marco Rudolph.
Take 5: Interpretable image classification with a handful
of features. In Conference on Neural Information Process-ing Systems, Workshop Progress and Challenges in Building
Trustworthy Embodied AI (NeurIPSW) , 2022. 1, 3
[39] Robert Tibshirani. Regression shrinkage and selection via
the lasso. Journal of the royal statistical society series b-
methodological , 1996. 2, 3, 5
[40] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research
(JMLR) , 2008. 8, 4
[41] Vinay Kumar Verma, Nikhil Mehta, Shijing Si, Ricardo
Henao, and Lawrence Carin. Pushing the efficiency limit
using structured sparse convolutions. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , 2023. 5
[42] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Pick-
ing winning tickets before training by preserving gradient
flow. In International Conference on Learning Representa-
tions (ICLR) , 2019. 1, 2, 5, 6
[43] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu,
Gang Niu, and Yang Liu. Learning with noisy labels re-
visited: A study using real-world human annotations. In In-
ternational Conference on Learning Representations (ICLR) ,
2022. 8, 4
[44] Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger
Grosse. Three mechanisms of weight decay regularization,
2018. 2, 3, 5

--- PAGE 11 ---
Supplementary Material
This document provides supplementary material for the
paper HyperSparse Neural Networks: Shifting Exploration
to Exploitation through Adaptive Regularization . At first,
Sec. A gives detailed information about the implementation
of our method. Subsequently, Sec. B presents more detailed
results of the intersection of largest weights during training
and the final pruning mask. The weight distribution after
training with our introduced method shown in the main pa-
per is analyzed for a wider set of configurations in Sec. C.
Moreover, Sec. D and Sec. E elaborate the gradient and the
compression behaviour during regularization presented in
the main paper more into detail.
A. Detailed Experimental Setup
As described in [34], we evaluated our method on the
datasets CIFAR-10/100 [19] and TinyImageNet [6] with the
models ResNet-32 [11] and VGG-19 [33]. CIFAR-10 is
a dataset for a classification task with 50 000 training and
10 000 validation samples on 32x32 color-images labeled
with 10 classes. Respectively CIFAR-100 has 100 classes
and the same amount of samples. The dataset TinyIma-
geNet consists of 100 000 training and 10 000 validation
samples with an image-size of 64x64, where samples are
labeled with a set of 200 classes.
As done in [34], we train our models for 160 epochs
on CIFAR-10/100 and 300 epochs on TinyImageNet using
SGD-optimizer, with an initial learning rate of 0.1 and a
batch size of 64. We decay the learning rate by factor 0.1 at
epoch 2/4 and 3/4 of the total number of epochs. The weight
decay is set to 1·10−4. In our experiments all results are
averaged over 5 runs.
In the original implementation of SmartRatio [34],
weights in the final linear layer are pruned with a fixed prun-
ing rate of 70%. Thus, too much weights remain when train-
ing on ResNet-32 with a pruning ratio of 99.8% on dataset
CIFAR-100 and TinyImageNet. To this reason, we change
the pruning ratio in the linear layer to 90% for this two
training settings only. The methods SNIP [20], GraSP [42],
SmartRatio [34], and LTH [9] suggest rules to obtain fixed
masks. This mask is applied to the model weights before
training. In contrast, IMP [16] iteratively trains a model
to epoch Tand prunes 20% of the remaining weights until
the desired pruning rate is reached. After each iteration the
weights and learning rate are reset to epoch kand retrained
again to epoch T. To be comparable, we define k= 20
andT= 160 for CIFAR-10/100 as well as k= 40 and
T= 300 for TinyImageNet. As described in [8], RigL per-
formes better with a longer training duration. To this reason
we extend the optimization time of the uniform distributed
RigL-method by training for 360 epochs with a learning rate
of 0.1, followed by the fine-tuning-step of 160 epochs onCIFAR-10/100 and 300 epochs on TinyImageNet. The fine-
tuning step is equal to ART. All further hyperparameters of
RigL are adopted from [8].
Our proposed method ART, described in Sec. 3.2 in the
main paper, consists of three steps. In the first step we train
our model to convergence for 60 epochs using a fix learning
rate of 0.1. Subsequently we enable the used regularization
term, with a small initialisation rate of λinit= 5·10−6and
increasing factor of η= 1.05. To reduce noise in choosing
the best pruned model, we average the accuracy ψ(ν(We)⊙
We)over epoch ( e−1,e,e+ 1), where edescribes the
current epoch and νdenotes magnitude pruning that obtains
a binary mask. The first two steps are used to obtain the
weights and masks for fine-tuning. During fine-tuning, we
use the training schedule described above as done in [34].
B. Mask intersection in Regularized Training
In this section we show further results of our experiments
measuring the mask intersection over epoch efrom Sec. 4.4
in the main paper. We measure the relative overlap between
the weights with highest magnitude at epoch eand the final
mask in different settings with different models, datasets,
regularization losses, and pruning rates. Therefore, Tab. 2
shows the important keypoints of intersection at the end of
pre-training (epoch 60) and one epoch before the final mask
was found ( e=K−1). We observe that our regularization
lossLHShas a higher intersection in nearly all settings at
epoch 60and epoch K−1compared to L1andL2loss. This
indicates that our HyperSparse loss changes less parameter
while reordering weights from remaining to pruned and vice
versa.
In addition, Tab. 2 presents the total number of training
epochs to obtain the final mask (including step 1 and step 2).
It shows that our HyperSparse loss needs less epochs to
terminate in nearly all settings. Since ART terminates, if
the best pruned model outperforms the unpruned model at
epoch e, we deduce that LHScreates a well performing
sparse network faster compared to L1andL2loss.
C. Weight Distribution
In this section, we show further experiments of the
weight distribution per layer in the final mask, as evalu-
ated in Sec. 4.4 in the main paper. We analyse the resulting
masks for dataset CIFAR-10 and CIFAR-100, pruning-rate
κ∈ {90%,98%,99.5%}as well as for model ResNet-32
and VGG-19. Weight distributions obtained by the methods
IMP [16], SRatio [34] and ART using HyperSpase loss are
analyzed. All values are averaged over 5 runs.
In Fig. 2, we show the resulting weight distributions for
ResNet-32. Note that the model is grouped in three residual
blocks (RES), two downsampling blocks (DS) and a lin-
ear layer (LL). We observe that ART +LHSandIMP have

--- PAGE 12 ---
comparable distributions of weights. Both methods show a
relative constant distribution in the residual layers, except
the last one. This last layer has an decreasing number of
weights, especially in the simpler task given in CIFAR-10.
In comparison, SRatio uses a fixed keep-ratio in a quadratic
decreasing manner and thus the weight distribution is not
dependent on data. Since ART andIMP outperform SRatio
by far in accuracy (Tab. 1 in the main paper), this hand-
crafted rule has adverse effect on performance. Moreover,
we observe a relatively high number of weights in the down-
sampling layer for ART andIMP, which indicates that these
layers are more important.
Further, we present the weight distribution for VGG-19
in Fig. 3. We observe that the layer around index 5 has
more weights for ART andIMP. Nearly no weights remain
in layer with index higher than 10, except the final linear
layer. Considering the increasing sparsity, the weight dis-
tribution is shifted towards the earlier layers with low in-
dex. We deduce that in higher sparsity regimes the weight
in earlier layer are more important in VGG-19. The hand-
crafted rule of SRatio shows a relatively flat weight distri-
bution. Overall, the number of weights in the linear layer
increases for CIFAR-100, due to the increasing number of
classes compared to CIFAR-10 in ResNet-32 and for VGG-
19.
D. HyperSparse Gradient Analysis
In this section, we analyze the gradient of our Hyper-
Sparse regularization loss with respect to the model weights
w∈W. Assuming that important weights have large mag-
nitudes, we show that HyperSparse subsides to no regular-
isation for important values and evolves to a strong penal-
ization for unimportant values. This behaviour allows ex-
ploitation in the set of the important weights that remain af-
ter magnitude pruning. Furthermore, we show that our loss
ensures a smooth transition in the gradient between unim-
portant and important weights, such that exploration in the
set of unimportant weights is possible during training.
Gradient. Our loss evolves sparseness and adapts on the
weight magnitude by utilizing the non-linearity of the Hy-
perbolic Tangent function
tanh( x) =ex−e−x
ez+e−x∈(−1,1), (2)
which is the reason for the name HyperSparse . The max-
imum of the derivative of tanh is1atx= 0 and strongly
vanishes close to zero for large values:
arg max
xdtanh( x)
dx= 0 and lim
x→±∞dtanh( x)
dx= 0.
(3)In this paper, the Hyperbolic Tangent function of a magni-
tudetanh(| · |)is denoted by t(·)for simplicity.
For the sake of completeness, we recapitulate the defini-
tion of HyperSparse from Eq. (2) in the main paper:
LHS(W) =1
A|W|X
i=1
|wi||W|X
j=1t(s·wj)
−|W|X
i=1|wi|
with A:=X
w∈Wt(s·w)
and∀w∈W:dA
dw= 0,(4)
and want to note again, that Adenotes a pseudo-constant
term that is considered to be a constant in the gradient com-
putation, and sis a scaling factor discussed in the end of this
section. In this section, sum notations asP|W|
i=1will be sim-
plified byP
wior byP
wifwis unique. Furthermore, we
will leave out declarations of set memberships like wi∈W
and state that every wis in the set of model weights W.
Also the scope of formulations is consistently defined as
∀w∈W.
With this notations and simplifications, the derivative of
Eq.(4) w.r.t. to a weight wican be defined as follows:
dLHS
dwi=d
dwi|wi|P
wjt(s·wj)
A| {z }
I+
d
dwiP
wn̸=wi|wn|P
wjt(s·wj)
A| {z }
II−sign(wi)
=sign(wi)·|wi| ·t′(s·wi) +P
wjt(s·wj)
A| {z }
I+
sign(wi)·P
wn̸=wi|wn| ·t′(s·wi)
A| {z }
II−sign(wi)
=sign(wi)·P
wjt(s·wj)
A=P
wjt(s·wj)+
P
wn|wn| ·t′(s·wi)
A=P
wjt(s·wj)−1
=sign(wi)·t′(s·wi)·P
wn|wn|P
wjt(s·wj).
(5)
The gradient consists of a term that is depending on the
weight distribution in Wand the derivative t′=dt
dwiat
the considered weights magnitude |wi|scaled with s. The
behaviour of HyperSparse can be explained with the gra-
dients for very small and very large magnitudes: For large

--- PAGE 13 ---
magnitudes |wi| ≫0, the derivative in Eq. (5) collapses to
dLHS
dwi
|s·wi|≫0≈sign(wi)·0 = 0 (6)
which is effectively no regularisation. For very small values
wi≈0, the derivative
dLHS
dwi
|s·wi|≈0≈sign(wi)·P
wn|wn|P
wjt(s·wj)(7)
is larger and increases, if the weights in Ware clearly sep-
arated in two sets of important (large magnitude) and unim-
portant weights (low magnitude). The gradient of weights
that are not assigned to one of those sets is between Eq. (6)
and (7) and therefore allows an easier exploration of those
weights during training.
Aligning with s.In the definition of HyperSparse , the
scaling factor saligns the loss with the actual weight dis-
tribution. The aim is that weights |w|>|wκ|are not or
softly and |w|<|wκ|strongly penalized. A weight dis-
tribution does not need to be aligned with the derivative of
the Hyperbolic Tangent function such that large weights are
mapped close to 0and small weights close to 1. To fix this,
we align Wby scaling wκwithsso that it lies on the in-
flection point of the gradient. The desired scaling factor can
be derived by
t′′′(s· |wκ| ≈0.6585) = 0 (8)
and setting s=0.6586
|wκ|. Large scaling factors slead to ram-
pant gradient distribution at weight wκtowards weights of
low magnitude. Examples can be found in Fig. 1 in the main
paper.
E. Interpretable Compression
This chapter discusses the process of knowledge com-
pression that compresses patterns from a pre-trained dense
network into a sparse network that consists of the set of 1−κ
highest weights, where κdenotes the desired pruning rate.
The first subsection presents the CIFAR-N [43] dataset
that is used to analyze the compression behavior in Sec. 4.5.
We show how it relates to CIFAR [19] and how we visu-
alize the label distribution with the modern CLIP frame-
work [29]. Then we elaborate the introduced metric Com-
pression Position more in detail. For the sake of complete-
ness, we lastly present and discuss figures and tables that
show results for additional settings that could not be pre-
sented in the main paper due to lack of space.
CIFAR-N
To analyze human-like label errors and to provide real-
world label noise for researchers, Wei et al . introduced the#Epochs
to Final MaskIntersection
ate= 60Intersection
ate=K−1
κLHSL1L2LHSL1L2LHSL1L2CIFAR-10
ResNet-
3290% 87 95 23 0.46 0.35 0.41 0.84 0.72 0.83
98% 112 130 194 0.29 0.15 0.1 0.86 0.54 0.4
99.5% 136 152 177 0.23 0.12 0.17 0.91 0.54 0.49VGG-
1990% 64 66 185 0.77 0.71 0.46 0.87 0.83 0.67
98% 76 81 230 0.51 0.45 0.02 0.83 0.76 0.7
99.5% 104 118 232 0.32 0.19 0.02 0.82 0.59 0.52CIFAR-100
VGG-
1990% 68 71 211 0.66 0.6 0.13 0.85 0.81 0.67
98% 96 106 238 0.39 0.28 0.02 0.83 0.64 0.66
99.5% 116 121 226 0.27 0.16 0.01 0.85 0.62 0.48ResNet-
3290% 99 113 177 0.45 0.28 0.2 0.88 0.63 0.49
98% 123 134 183 0.34 0.2 0.18 0.91 0.61 0.49
99.5% 147 150 163 0.26 0.19 0.28 0.95 0.7 0.62
Table 2: Complementary key-points to the experiments
about mask intersection in Sec. 4.4 in the main paper. The
intersection indicates the relative overlap of weights with
highest magnitude during training to remaining weights in
the final mask obtained by ART. For simplicity, we only
show the intersection at the end of pre-training ( e= 60 ) and
one epoch before the final mask was found ( e=K−1).
In addition, this table shows the number of training epochs
to the final mask. The information are demonstrated for L1,
L2andHyperSparse lossLHS, over different pruning rates
κ, models and datasets.
CIFAR-N [43] dataset that uses the CIFAR [19] training
dataS={(xn, yn)}N
n=1, but has different ground truth la-
bels. Every sample was labeled by 3different persons, in-
ducing their subjective human bias, such that the dataset for-
mulation can be defined as S={(xn,{y1
n, y2
n, y3
n})}N
n=1.
They show, that single persons consistently induce an er-
ror rate between 10-20% (compared to original CIFAR).
Moreover, they show that human-like label noise is harder
to tackle in robust learning scenarios compared to synthetic
label noise.
We use the multi-label ym
ifrom CIFAR-N and the
most likely correct label yifrom CIFAR-10 and derive a
“hardness-score” hi. For a sample (xi, yi), the score
hi=
ym
i∈ {y1
i, y2
i, y3
i}	
|ym
i̸=yi	∈[0,3] (9)
describes, how often a sample was mislabeled in CIFAR-
N and therefore relates to the difficulty. To illustrate the
distribution of classes and labels, we map all images of
CIFAR-10 to the latent space of the high performing diffu-
sion model CLIP [29] that is using vision transformers [7]
and is trained on large training data. After mapping the
images to the CLIP latent space, we reduce the dimen-
sions with the t-SNE [40] algorithm to two dimensions as
shown in Fig. 4. The four sub-figures split the samples
from CIFAR-10 according to their score hi. It shows that all

--- PAGE 14 ---
classes have samples with every score. Moreover, the vari-
ance of the samples per class grows with increasing hard-
ness. As harder samples are more likely to have a larger
distance to cluster centers, because they differ to the “easy”
and unambiguous class templates, the increasing variance
indicates that the CLIP latent space combined with t-SNE is
a good tool to visualize a human-like sample distribution.
According to the well known and often discussed effect
that samples with easy patterns and unambiguous labels are
memorized first [1,17,21], samples with a higher hardness-
score should be compressed later in the training process.
We use the hardness-score to evaluate if this effect is also
present in the process of compressing patterns from a pre-
trained dense neural network into a dense sub-network us-
ing our method.
Compression Position (CP)
The next section formally defines the evaluation metric
Compression Position as described in Sec. 4.5 in the main
paper. Measurement of classification capabilities of neu-
ral networks f(W, x)is usually performed by the accuracy
metric
ψ 
S, f, W
=
(x, y)∈S|f(W, x) =y	
|S|. (10)
The accuracy of a specific class can be obtained by calcu-
lating ψfor a subset Sc⊂Swith only samples of a spe-
cific class y=c. To answer the question “Which classes
are represented first in a neural network?” , one can mea-
sure the class accuracy after every training epoch eand plot
them. To reduce the complex plot into a single metric, the
area-under-curve (AUC) could be obtained for every spe-
cific class. Drawbacks from the AUC mesurements are, that
the absolute values of AUC are not comparable between dif-
ferent settings ( i.e., datasets, models, . . . ). For example,
large and complex data will lead to lower AUCs. More-
over, the class specific accuracy metric is not satisfying for
the question “Which classes are compressed first into the
higher magnitude weights?” that is addressed in this pa-
per. We noticed, that the class accuracy of sparse networks
underlie high noise rates and are therefore hard to interpret.
To tackle the drawbacks and generate a suitable metric
for our work, we introduce the Compression Position (CP)
metric that is basically a sample based accuracy over time.
It aims to quantify the relative position in time between
epoch eSandeE, where a sample xis compressed from
the dense weights Winto the weights with high magnitude
ν(W), so that the sparse neural network is able to predict
the correct ground truth label f(ν(W)⊙W, x) =y.
First, we redefine Eq. (10) into a individual sample basedaccuracy for the pruned model that is defined as
ψI 
x, y, f, W
=
We∈ W | f(ν(We)⊙We, x) =y	
eE−eS(11)
andW={We}eEe=eSdenotes a set of weight sets during the
training between epoch eSandeE. The CP metric of xis
the normalized position of ψI 
x, y, f, W
in a sorted list of
the sample accuracy Ψ = sort
ψI 
xn, yn, f,W	N
n=1
in descending order, such that
CP(x, y, f, W, S) :ψI(x, y, f, W)!= Ψ CP(x,y,f,W)·|Φ|
(12)
holds. The CP metric indicates the temporal position when
a sample is compressed into the sparse weights ν(W)⊙W,
because CP increases if the corresponding sample is classi-
fied correct early and continuously in the training process.
Compression Behaviour
We present the main impressions of our investigations about
the compression behaviour on class level in Tab. 3 and on
sample level in Fig. 4 in the main paper. For the sake of
completeness and to strengthen the claims, we report more
detailed results in Tab. 3 and Fig. 5 and 6.
The order of compression Ψsortfor a dense, two low spar-
sity, and three high sparsity networks is visualized in Fig. 5
and 6. Every sub-figure shows a consecutive set of 2500
samples from Ψsortand gives an intuition, which patterns
are compressed into the sparse network in the beginning,
middle phase and end of training. First, we observe that
the diversity of classes in the first 2500 samples decreases
with increasing sparsity. Second, it shows that the intra-
class variance increases over time. The first observation
suggests that the highest weights do not make any decisions
at the beginning, or only between a few classes. In the same
way that only a few classes are compressed at the begin-
ning, the remaining classes are compressed in isolation at
the end (see Fig. 6c). This is important for magnitude prun-
ing based methods and high sparsity rates: If the highest
weights have no capabilities in classification for all classes
after dense training, perhaps the basic assumption that high-
est weights encode most important decision rules is wrong.
Interestingly, our experiments consistently show, that the
class deer tends to be compressed first and moreover, deer
is the center in the t-SNE mapped latent space of CLIP . It
seems like deer is the general prototype of the dataset and
therefore we call the effect of preferring one class in the first
compression stage The deer bias . The second observation
reveals the main commonality between dense training and
compression through regularization. Derived from the hu-
man ability to reproduce simple patterns faster, dense and

--- PAGE 15 ---
sparse networks learn the general patterns first during com-
pression and encode the high frequency samples later.
The Tab. 3 quantifies the results discussed before. It
shows the compression rate for every class in CIFAR-10,
subdivided by the hardness score introduced earlier. The
dense networks compression rate for every class is more or
less uniform-distributed. This promotes the first observa-
tion that all classes are encoded into the weights at the same
time in dense networks. With increasing pruning rate κ, the
classes are successively compressed into the high weights
during regularization. The second observation is confirmed
by dividing the classes according to their human label er-
rors. The samples with higher label error are consistently
compressed later into the high weights.

--- PAGE 16 ---
RES
DSRES
DSRES
LLCIFAR-10
RES
DSRES
DSRES
LLCIFAR-100
1 1114 2225 33010,00020,00030,000κ= 90%
1 1114 2225 33010,00020,00030,000
1 1114 2225 3302,0004,0006,000κ= 98%
1 1114 2225 3302,0004,0006,000
1 1114 2225 3302,0004,000
Layer Index iκ= 99.5%
1 1114 2225 3302,0004,000
Layer Index i
IMP SRatio ART +LHSNumber of Weights per Layer after Pruning with Pruning Rate κ
Figure 2: Distribution of weights per layer in ResNet-32 after pruning. We visualize the results in the left column for
CIFAR-10 and right column for CIFAR-100 as well as for pruning rates κ∈ {90%,98%,99.5%}in each row. The layer
index describes the execution order which means that higher indices are calculated later in inference. All results are averaged
over 5 runs. We group the model in residual blocks (RES), downsampling blocks (DS) and the linear layer (LL). Our
Method ART +LHSdistributes weights comparable to IMP [16], but has more weights in the downsampling layers. The
method SRatio [34] has a quadratic decreasing keep-ratio. We observe the linear layer in CIFAR-100 deserves more weights
compared to CIFAR-10, due to the bigger number of classes.

--- PAGE 17 ---
0 5 10 150200,000400,000600,000κ= 90%CIFAR-10
0 5 10 150200,000400,000600,000CIFAR-100
0 5 10 15050,000100,000κ= 98%
0 5 10 15050,000100,000
0 5 10 15010,00020,00030,000
Layer Index iκ= 99.5%
0 5 10 15010,00020,000
Layer Index i
IMP SRatio ART +LHSNumber of Weights per Layer after Pruning with Pruning Rate κ
Figure 3: Distribution of weights per layer in VGG-19 after pruning. We visualize the results in the left column for CIFAR-10
and right column for CIFAR-100 as well as for pruning rates κ∈ {90%,98%,99.5%}in each row. The layer index describes
the execution order which means that higher indices are calculated later in inference. All results are averaged over 5 runs.
In [34], VGG-19 is constructed using multiple convolutional layers, which irregularly increase in the number of parameter
over index i. The model ends with a linear layer. Our Method ART +LHSdistributes weights comparable to IMP [16], while
SRatio [34] uses more weights in layers with higher index. We observe the linear layer in CIFAR-100 deserves more weights
compared to CIFAR-10, due to the bigger number of classes.

--- PAGE 18 ---
CPHuman
Label
ErrorsCIFAR-10 Class
airplane automobile bird cat deer dog frog horse ship truckSparsity Level
(κ)
Dense
Model
(0%)0 0.452 0.295 0.554 0.639 0.482 0.551 0.425 0.429 0.357 0.400
1 0.547 0.329 0.632 0.722 0.548 0.628 0.482 0.493 0.439 0.479
2 0.676 0.389 0.741 0.808 0.653 0.699 0.572 0.710 0.592 0.588
3 0.783 0.542 0.823 0.862 0.760 0.826 0.665 0.769 0.720 0.695Low
Sparsity
(90%)0 0.580 0.612 0.499 0.494 0.166 0.460 0.257 0.524 0.470 0.567
1 0.671 0.640 0.573 0.601 0.217 0.540 0.303 0.570 0.522 0.613
2 0.790 0.666 0.678 0.721 0.292 0.625 0.376 0.737 0.631 0.676
3 0.841 0.764 0.772 0.796 0.392 0.754 0.461 0.808 0.762 0.710Low
Sparsity
(95%)0 0.739 0.685 0.604 0.696 0.187 0.486 0.400 0.173 0.223 0.543
1 0.796 0.709 0.665 0.763 0.217 0.538 0.439 0.212 0.253 0.578
2 0.868 0.733 0.745 0.836 0.280 0.581 0.517 0.383 0.363 0.625
3 0.912 0.808 0.822 0.880 0.366 0.701 0.574 0.468 0.494 0.655Low
Sparsity
(98%)0 0.742 0.635 0.579 0.640 0.045 0.504 0.349 0.471 0.313 0.452
1 0.798 0.666 0.643 0.721 0.053 0.563 0.395 0.523 0.350 0.492
2 0.870 0.696 0.727 0.800 0.066 0.623 0.479 0.700 0.452 0.546
3 0.906 0.797 0.797 0.851 0.079 0.753 0.547 0.767 0.577 0.583High
Sparsity
(99%)0 0.752 0.651 0.578 0.532 0.108 0.500 0.487 0.261 0.289 0.551
1 0.812 0.678 0.643 0.614 0.135 0.554 0.527 0.315 0.327 0.597
2 0.889 0.711 0.728 0.703 0.175 0.616 0.598 0.500 0.428 0.657
3 0.919 0.820 0.800 0.775 0.234 0.736 0.647 0.593 0.539 0.709High
Sparsity
(99.5%)0 0.761 0.691 0.472 0.441 0.049 0.571 0.364 0.758 0.246 0.429
1 0.817 0.720 0.525 0.495 0.055 0.615 0.401 0.787 0.279 0.466
2 0.885 0.748 0.586 0.568 0.067 0.665 0.469 0.881 0.373 0.513
3 0.911 0.854 0.652 0.623 0.079 0.758 0.522 0.911 0.465 0.534High
Sparsity
(99.8%)0 0.798 0.770 0.296 0.387 0.056 0.618 0.206 0.824 0.308 0.622
1 0.835 0.787 0.317 0.411 0.061 0.647 0.214 0.835 0.331 0.647
2 0.882 0.807 0.342 0.439 0.069 0.678 0.241 0.893 0.399 0.677
3 0.902 0.863 0.363 0.469 0.081 0.741 0.257 0.929 0.469 0.699
Table 3: Compression Position (see Sec. E) for dense NNs (during pre-training) and κpruned NNs (during regularization) for
all CIFAR-10 classes. Samples of a class are split into 4 subsets according to the number of human label errors in CIFAR-N
to indicate the difficulty. In sparse networks, different classes are compressed at different times and difficult samples are
compressed later. These results are supplemental to Tab. 3 in the main paper.

--- PAGE 19 ---
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
CIFAR-N 1-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
CIFAR-N 2-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
CIFAR-N 3-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truckFigure 4: CIFAR-10N samples in the CLIP latent space, mapped by t-SNE into two dimensions. The datset is split into four
subsets deduced by the hardness score explained in Sec. E. The term “ h-Mislabeled” explains that hof 3 persons mislabeled
the corresponding sample. Ellipses indicate the double standard deviation of a class in the t-SNE space. It shows that samples
with higher hardness score hlead to a larger standard deviation and suggest that CLIP in combination with t-SNE is a suitable
visualization tool to show visualize human-like recognition behaviour.

--- PAGE 20 ---
0-2500
20000-22500
47500-50000
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck(a) Dense Model ( κ= 0% )
0-2500
20000-22500
47500-50000
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
(b) Low Sparsity ( κ= 90% )
0-2500
20000-22500
47500-50000
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
(c) Low Sparsity ( κ= 95% )
Figure 5: First 5%, 40%-45%, and 95%-100% CIFAR-10 samples that are compressed into the remaining highest weights
after pruning with κ∈ {0%,90%,95%}deduced by the CP-metric. While dense networks learn samples approximately
uniform-distributed over classes, the highest weights compress decision rules only for a subset of classes in the early learning
stage. Note that we sampled by factor 10 for visualization purposes and ellipses represent the double standard deviation of
cluster centers.

--- PAGE 21 ---
0-2500
20000-22500
47500-50000
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck(a) Low Sparsity ( κ= 98% )
0-2500
20000-22500
47500-50000
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
(b) High Sparsity ( κ= 99.5%)
0-2500
20000-22500
47500-50000
CIFAR-N 0-Mislabeled
label
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
(c) High Sparsity ( κ= 99.8%)
Figure 6: First 5%, 40%-45%, and 95%-100% CIFAR-10 samples that are compressed into the remaining highest weights
after pruning with κ∈ {98%,99.5%,99.8%}deduced by the CP-metric. While dense networks learn samples approximately
uniform-distributed over classes, the highest weights compress decision rules only for a subset of classes in the early learning
stage. Note that we sampled by factor 10 for visualization purposes and ellipses represent the double standard deviation of
cluster centers.
