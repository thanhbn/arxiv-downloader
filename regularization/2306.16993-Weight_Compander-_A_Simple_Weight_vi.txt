# 2306.16993.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/regularization/2306.16993.pdf
# Kích thước tệp: 469625 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Weight Compander: Một Phương Pháp Tái Tham Số Hóa Trọng Số Đơn Giản cho Điều Chuẩn
Rinor Cakaj
Xử Lý Hình Ảnh
Robert Bosch GmbH & Đại học Stuttgart
71229 Leonberg, Đức
Rinor.Cakaj@de.bosch.com

Jens Mehnert
Xử Lý Hình Ảnh
Robert Bosch GmbH
71229 Leonberg, Đức
JensEricMarkus.Mehnert@de.bosch.com

Bin Yang
ISS
Đại học Stuttgart
70550 Stuttgart, Đức
bin.yang@iss.uni-stuttgart.de

Tóm tắt —Điều chuẩn là một tập hợp các kỹ thuật được sử dụng để cải thiện khả năng tổng quát hóa của mạng neural sâu. Trong bài báo này, chúng tôi giới thiệu weight compander (WC), một phương pháp hiệu quả mới để cải thiện tổng quát hóa bằng cách tái tham số hóa mỗi trọng số trong mạng neural sâu sử dụng một hàm phi tuyến. Đây là một phương pháp tổng quát, trực quan, rẻ và dễ triển khai, có thể được kết hợp với nhiều kỹ thuật điều chuẩn khác. Các trọng số lớn trong mạng neural sâu là dấu hiệu của một mạng phức tạp hơn đã bị overfitting với dữ liệu huấn luyện. Hơn nữa, các mạng được điều chuẩn có xu hướng có phạm vi trọng số lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn. Chúng tôi giới thiệu một hàm tái tham số hóa trọng số được áp dụng cho mỗi trọng số và ngầm giảm overfitting bằng cách hạn chế độ lớn của các trọng số trong khi buộc chúng tránh xa số không cùng lúc. Điều này dẫn đến việc ra quyết định dân chủ hơn trong mạng. Thứ nhất, các trọng số riêng lẻ không thể có quá nhiều ảnh hưởng trong quá trình dự đoán do việc hạn chế độ lớn của chúng. Thứ hai, nhiều trọng số được sử dụng trong quá trình dự đoán, vì chúng bị buộc tránh xa số không trong quá trình huấn luyện. Điều này thúc đẩy việc trích xuất nhiều đặc trưng hơn từ dữ liệu đầu vào và tăng mức độ dư thừa trọng số, làm cho mạng ít nhạy cảm hơn với sự khác biệt thống kê giữa dữ liệu huấn luyện và kiểm tra. Từ quan điểm tối ưu hóa, hiệu ứng thứ hai của WC có thể được xem như việc tái kích hoạt các trọng số "chết" (gần số không) để tham gia vào huấn luyện. Điều này tăng xác suất tìm ra một tập hợp trọng số hoạt động tốt hơn trong nhiệm vụ đã cho. Chúng tôi mở rộng phương pháp để học các siêu tham số của hàm tái tham số hóa trọng số được giới thiệu. Điều này tránh việc tìm kiếm siêu tham số và mang lại cho mạng cơ hội để điều chỉnh việc tái tham số hóa trọng số với tiến trình huấn luyện. Chúng tôi cho thấy thực nghiệm rằng việc sử dụng weight compander cùng với các phương pháp điều chuẩn tiêu chuẩn cải thiện hiệu suất của mạng neural. Hơn nữa, chúng tôi phân tích thực nghiệm phân phối trọng số có và không có weight compander sau huấn luyện để xác nhận tác động companding của phương pháp lên các trọng số.

© 2023 IEEE. Việc sử dụng cá nhân tài liệu này được cho phép. Cần có sự cho phép từ IEEE cho tất cả các mục đích sử dụng khác, trong bất kỳ phương tiện nào hiện tại hoặc tương lai, bao gồm in lại/xuất bản lại tài liệu này cho mục đích quảng cáo hoặc khuyến mãi, tạo ra các tác phẩm tập thể mới, để bán lại hoặc phân phối đến máy chủ hoặc danh sách, hoặc tái sử dụng bất kỳ thành phần có bản quyền nào của tác phẩm này trong các tác phẩm khác.

I. GIỚI THIỆU
Mạng neural sâu chứa nhiều lớp ẩn phi tuyến làm cho chúng trở thành hệ thống học máy mạnh mẽ [1]. Tuy nhiên, các mạng như vậy dễ bị overfitting [2] do kích thước hạn chế của dữ liệu huấn luyện, khả năng cao của mô hình và sự hiện diện của quá nhiều nhiễu trong dữ liệu huấn luyện [3]. Overfitting mô tả hiện tượng mà một mạng neural (NN) phù hợp hoàn hảo với dữ liệu huấn luyện trong khi đạt hiệu suất kém trên dữ liệu kiểm tra. Điều chuẩn là một tập hợp các kỹ thuật được sử dụng để giảm overfitting và do đó là một yếu tố quan trọng trong học sâu [4]. Nó cho phép mô hình tổng quát hóa tốt với dữ liệu chưa thấy.

Nhiều phương pháp đã được phát triển để điều chuẩn NN, ví dụ như dừng sớm [5], phạt trọng số như điều chuẩn L1 [6] và weight decay [7], chia sẻ trọng số mềm [8], dropout [1], tăng cường dữ liệu [9] và các phương pháp học tập hợp [10]. Kiến trúc của NN như ResNet [11], EfficientNet [12] hoặc Transformer [13] khác nhau rộng rãi qua các ứng dụng, nhưng chúng thường chia sẻ cùng các khối xây dựng, ví dụ như trọng số, neuron và kernel tích chập.

Chúng ta xem xét kỹ hơn các trọng số của một NN. Trong quá trình huấn luyện, các trọng số được điều chỉnh để giảm hàm mất mát trên tập huấn luyện. Sự thành công của quy trình tối ưu hóa phụ thuộc vào việc tham số hóa các trọng số [14]. Việc tái tham số hóa của một trọng số w∈R được đưa ra bởi một hàm tái tham số hóa Ψ:R→R, v↦Ψ(v) được sử dụng để biểu thị trọng số w theo một trọng số có thể học mới v bằng w=Ψ(v). Ý tưởng sử dụng các hàm tái tham số hóa đã được đề xuất trước đây để cắt tỉa hoặc tăng tốc huấn luyện của NN sâu [14], [15].

Trong công trình này, chúng tôi giới thiệu weight compander, một phương pháp mới sử dụng hàm tái tham số hóa để điều chuẩn NN tổng quát, tức là để cải thiện khả năng tổng quát hóa của chúng.

Các trọng số lớn trong mạng neural sâu là dấu hiệu của một mạng phức tạp hơn đã bị overfitting với dữ liệu huấn luyện. Hơn nữa, các mạng được điều chuẩn có xu hướng có phạm vi trọng số lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn [16].

Được thúc đẩy bởi những quan sát này, chúng tôi đề xuất một hàm tái tham số hóa, hạn chế độ lớn của các trọng số trong khi buộc chúng tránh xa số không cùng lúc. Do đó, hàm tái tham số hóa của chúng tôi khuyến khích mô hình không dựa vào một vài trọng số lớn mà thay vào đó sử dụng nhiều trọng số nhỏ để hưởng lợi từ tất cả thông tin có sẵn. Vì vậy, phương pháp của chúng tôi thúc đẩy dân chủ trọng số, tức là ảnh hưởng của các trọng số riêng lẻ bị hạn chế và nhiều trọng số được tham gia vào quá trình dự đoán.

Việc tái tham số hóa của chúng tôi là một bộ nén cho các trọng số có độ lớn lớn và một bộ mở rộng cho các trọng số có độ lớn nhỏ. Do đó, phương pháp của chúng tôi được gọi là weight compander. Hàm tái tham số hóa phi tuyến Ψ:R→R, v↦a·arctan(v/b) với các siêu tham số a, b>0, được chia sẻ

arXiv:2306.16993v1 [cs.LG] 29 Jun 2023

--- TRANG 2 ---
qua tất cả các trọng số trong NN, và trọng số có thể học mới v∈R đáp ứng yêu cầu của chúng tôi. Tham số a được sử dụng để chia tỷ lệ và hạn chế giá trị tuyệt đối của các trọng số w và tham số b để nén và kéo dãn hàm trong trục v. Hàm bị giới hạn, tức là -aπ/2 < a·arctan(v/b) < aπ/2 với mọi v∈R. Điều này hạn chế độ lớn của các trọng số. Đạo hàm của Ψ(v) tương đối lớn hơn với |v| nhỏ so với |v| lớn. Vì đạo hàm của Ψ(v) xuất hiện trong gradient của v, tính chất này được sử dụng trong quá trình huấn luyện để buộc các trọng số tránh xa số không.

Tóm lại, weight compander tái tham số hóa mỗi trọng số trong một NN sử dụng hàm phi tuyến Ψ(v). Trong suốt quá trình huấn luyện, v được điều chỉnh để giảm hàm mất mát trên tập huấn luyện. Một minh họa đồ họa của một NN kết nối đầy đủ được tái tham số hóa được hiển thị trong Hình 1.

A. Đóng Góp của Bài Báo này và Ứng Dụng
Trong công trình này, chúng tôi trình bày weight compander, một phương pháp mới tái tham số hóa các trọng số w để giảm overfitting và có thể được áp dụng cho bất kỳ NN cơ sở nào. Các đóng góp cốt lõi của chúng tôi là:
• Định nghĩa một hàm tái tham số hóa Ψ(v) điều chuẩn NN.
• Phân tích tác động của phương pháp trong quá trình huấn luyện.
• Chứng minh thực nghiệm rằng việc sử dụng weight compander cùng với các phương pháp điều chuẩn tiêu chuẩn tăng hiệu suất của các mô hình khác nhau trên CIFAR-10, CIFAR-100, TinyImageNet và ImageNet. Chúng tôi cải thiện độ chính xác kiểm tra của ResNet50 trên CIFAR-10 sebesar 0.75%, trên CIFAR-100 sebesar 1.56%, trên TinyImageNet sebesar 0.86% và trên ImageNet sebesar 0.22%.

II. CÔNG TRÌNH LIÊN QUAN

A. Điều Chuẩn
Điều chuẩn là một trong những yếu tố quan trọng của học sâu [4], cho phép mô hình tổng quát hóa tốt với dữ liệu chưa thấy ngay cả khi được huấn luyện trên tập huấn luyện hữu hạn hoặc với quy trình tối ưu hóa không hoàn hảo [17]. Có nhiều kỹ thuật để điều chuẩn NN có thể được phân loại thành các nhóm. Các phương pháp tăng cường dữ liệu như cắt, lật và điều chỉnh độ sáng hoặc độ sắc nét [9] và cutout [18] biến đổi tập dữ liệu huấn luyện để tránh overfitting. Các kỹ thuật điều chuẩn như dropout [1], dropblock [19] và dropconnect [2] loại bỏ các neuron hoặc trọng số khỏi NN trong quá trình huấn luyện để ngăn các đơn vị đồng thích ứng quá mức [1]. Hơn nữa, NN có thể được điều chuẩn bằng cách sử dụng các điều khoản phạt trong hàm mất mát. Weight decay [7] khuyến khích các trọng số của NN có độ lớn nhỏ. Điều chuẩn L1 [6] buộc các trọng số của các đặc trưng không liên quan về số không. Các kỹ thuật chuẩn hóa [20]–[22] như batch normalization [20], chuẩn hóa các đặc trưng theo mean và variance được tính trong một mini-batch, hoạt động trong một số trường hợp như bộ điều chuẩn. Sự không chắc chắn ngẫu nhiên của thống kê batch có thể có lợi cho tổng quát hóa [22] và trong một số trường hợp loại bỏ nhu cầu về dropout [20]. Hơn nữa, NN có thể được điều chuẩn bằng cách sử dụng dừng sớm [5] và các phương pháp học tập hợp [10]. Tái tham số hóa trọng số chưa được sử dụng cho điều chuẩn. Do đó phương pháp được giới thiệu của chúng tôi là mới.

B. Tái Tham Số Hóa Trọng Số
Ý tưởng sử dụng các hàm tái tham số hóa đã được đề xuất trước đây để cắt tỉa NN [15] hoặc để tăng tốc huấn luyện của chúng [14]. Vì các hàm tái tham số hóa này có mục tiêu khác, ví dụ như nhiều trọng số có độ lớn thấp để dễ dàng cắt tỉa mạng, chúng không phù hợp để điều chuẩn NN.

1) Cắt Tỉa: Các phương pháp cắt tỉa được sử dụng để giảm số lượng kết nối trong một mạng [23] với việc giảm hiệu suất không đáng kể của mạng [24].

Powerpropagation [15] là một phương pháp tái tham số hóa trọng số, tập trung vào hiệu ứng vốn có của huấn luyện dựa trên gradient đối với độ thưa của mô hình. Ý tưởng là tái tham số hóa các trọng số w của một NN như Ψ(v) = v·|v|^(α-1) với bất kỳ α≥1 nào. Do quy tắc dây chuyền của phép tính, độ lớn của các tham số được nâng lên lũy thừa α-1 xuất hiện trong tính toán gradient. Do đó, các tham số có độ lớn nhỏ hơn nhận được cập nhật gradient nhỏ hơn, trong khi các tham số có độ lớn lớn hơn nhận được cập nhật lớn hơn. Mạng sau đó được cắt tỉa bằng cách loại bỏ các trọng số có độ lớn thấp.

Interspace pruning [25] là một phương pháp cắt tỉa khác tái tham số hóa các bộ lọc tích chập như các tổ hợp tuyến tính của các vector cơ sở bộ lọc thích ứng. Cho h∈R^(K×K) là một bộ lọc với kích thước kernel K×K, thì h cũng có thể được mô hình hóa trong không gian tuyến tính {∑(n=1 to K²)λₙ·g^(n): λₙ∈R} trong đó F = {g^(1), ..., g^(K²)} ⊂ R^(K×K) là cơ sở bộ lọc. Phương pháp này

--- TRANG 3 ---
đặt các hệ số cơ sở đã cắt tỉa về số không trong khi huấn luyện các hệ số cơ sở chưa cắt tỉa và các vector cơ sở một cách đồng thời, dẫn đến hiệu suất tốt nhất cho các phương pháp cắt tỉa không có cấu trúc.

2) Tăng Tốc Huấn Luyện Mạng Neural Sâu: Giảm thời gian huấn luyện cho một NN với việc giảm hiệu suất không đáng kể là một nhiệm vụ quan trọng trong học sâu. Có nhiều phương pháp để tăng tốc huấn luyện, ví dụ như các phương pháp chuẩn hóa như batch normalization [20], đóng băng lớp một cách tiến bộ [26], ưu tiên các ví dụ có mất mát cao ở mỗi lần lặp [27] hoặc cắt tỉa động các mẫu dữ liệu [28].

Weight normalization [14] là một phương pháp tái tham số hóa trọng số tăng tốc hội tụ của tối ưu hóa SGD. Nó tái tham số hóa các vector trọng số của mỗi lớp sao cho độ dài của các vector trọng số đó được tách rời khỏi hướng của chúng. Cụ thể, họ biểu thị vector trọng số w∈R^d bằng w = Ψ(v,g) = (g/||v||)v, trong đó v∈R^d là vector trọng số mới và g∈R là tham số vô hướng. Trái ngược với các công trình trước đây [29], họ trực tiếp thực hiện SGD trên vector trọng số mới v và tham số vô hướng g.

III. WEIGHT COMPANDER

Được truyền cảm hứng từ các tái tham số hóa trọng số cho cắt tỉa [15] và huấn luyện nhanh hơn [14], chúng tôi giới thiệu weight compander để cải thiện khả năng tổng quát hóa của NN. Chúng tôi trình bày hàm tái tham số hóa và phân tích cách các thực hành học sâu đã thiết lập có thể được kết hợp với weight compander. Hơn nữa, chúng tôi mở rộng phương pháp bằng cách học các siêu tham số mới của weight compander với SGD thay vì tối ưu hóa siêu tham số.

A. Phương Pháp
Mục tiêu của chúng tôi là tìm một hàm Ψ(v) để tái tham số hóa các trọng số w∈R của một NN để giảm overfitting. Nói cách khác, chúng tôi muốn viết lại các trọng số w dưới dạng tương đương w = Ψ(v) và tối ưu hóa các trọng số mới v∈R để cải thiện hiệu suất của NN.

Để chọn hàm tái tham số hóa tối ưu, trước tiên chúng tôi phân tích các hiệu ứng của weight decay [7] và dropout [1] đối với các trọng số của một NN. Weight decay hạn chế sự tăng trưởng của các trọng số và do đó ngăn các trọng số tăng quá lớn [7]. Dropout ngẫu nhiên bỏ qua các đơn vị ẩn khỏi mạng với một xác suất để các đơn vị ẩn không thể dựa vào các đơn vị ẩn khác hiện diện. Điều này ngăn các đơn vị đồng thích ứng quá mức [1]. Vì các đơn vị ẩn được bỏ qua ngẫu nhiên trong quá trình huấn luyện, NN không thể dựa vào một vài trọng số. Do đó, các trọng số được huấn luyện có phạm vi lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn [16].

Do đó, chúng tôi muốn tìm một hàm tái tham số hóa tăng cường các hiệu ứng điều chuẩn của cả dropout và weight decay, tức là:
1) Chúng tôi muốn hạn chế độ lớn của các trọng số.
2) Chúng tôi muốn có phạm vi trọng số lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn.

Hạn chế độ lớn của các trọng số hạn chế quyền lực ra quyết định của các trọng số riêng lẻ. Trái ngược với weight decay [7], phương pháp của chúng tôi ngăn các trọng số lớn từ epoch đầu tiên (xem Phần IV).

Có phạm vi trọng số lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn tạo ra một môi trường mà nhiều trọng số được tham gia với nhiều đóng góp ít bằng nhau hơn và nhiều thông tin đầu vào được sử dụng trong quá trình dự đoán. Do đó phương pháp của chúng tôi thúc đẩy dân chủ trọng số, tức là NN không nên dựa vào một vài trọng số lớn mà thay vào đó sử dụng nhiều trọng số nhỏ để hưởng lợi từ tất cả thông tin có sẵn. Trái ngược với dropout [1], phương pháp của chúng tôi không có xung đột với batch normalization [30]. Phương pháp của chúng tôi không gây ra bất kỳ "variance shifts" nào có thể làm hại hiệu suất trên DNN sử dụng batch normalization (xem Phần IV). Hơn nữa, phương pháp của chúng tôi có thể được sử dụng cùng với dropout (ví dụ trong VGG ở Phần IV), tức là tiếp tục tăng hiệu suất của DNN.

Trong khi phạm vi của bài báo này là điều chuẩn, từ quan điểm tối ưu hóa, hiệu ứng thứ hai của WC có thể được xem như việc tái kích hoạt các trọng số "chết" (gần số không) để tham gia vào huấn luyện, mà nếu không sẽ ở gần số không trong quá trình huấn luyện. Điều này tăng xác suất tìm ra một tập hợp trọng số hoạt động tốt hơn trong nhiệm vụ đã cho.

Tại thời điểm này, người ta có thể nghĩ rằng phương pháp của chúng tôi mâu thuẫn với việc cắt tỉa để giảm mô hình, ví dụ [31], cho rằng các mô hình thưa thót vượt trội về mặt độ phức tạp mô hình vì chúng có số lượng kết nối nhỏ hơn trong khi giảm hiệu suất không đáng kể của mạng. Trong Phần IV, chúng tôi phân tích phân phối trọng số trong các mạng có và không có WC. Hình 4(b) cho thấy gần 40% trọng số của lớp tích chập đầu tiên trong một ResNet34 đã huấn luyện xấp xỉ 0 mặc dù sử dụng WC. Hơn nữa, phân tích của chúng tôi cho thấy sự khác biệt trong phân phối trọng số giữa DNN tiêu chuẩn và mạng sử dụng WC ít rõ ràng hơn ở giữa và cuối mạng. Do đó, WC không buộc mạng sử dụng tất cả trọng số. Nó kích thích quá trình tối ưu hóa để sử dụng nhiều trọng số hơn trong huấn luyện, để tìm tập hợp trọng số tối ưu.

Lưu ý rằng yêu cầu thứ hai không mâu thuẫn với mục tiêu của weight decay, vì weight decay chỉ ngăn các trọng số tăng quá lớn nhưng không buộc chúng về số không [7]. Mặc dù điều chuẩn L1 có hiệu ứng điều chuẩn, nói chung nó không điều chuẩn tốt như weight decay [32]. Điều chuẩn L1 dẫn đến ma trận trọng số thưa thót, tức là nhiều trọng số bằng số không. Trái ngược với điều đó, weight decay dẫn đến NN có mật độ trọng số cao hơn, tức là weight decay chỉ thu nhỏ các trọng số gần số không, thay vì bằng số không. Những mạng này sử dụng nhiều thông tin hơn từ dữ liệu đầu vào cho dự đoán của chúng, điều này giải thích hiệu suất tăng so với điều chuẩn L1.

Để tăng cường các hiệu ứng điều chuẩn, chúng tôi giới thiệu hàm tái tham số hóa sau:
w = Ψ(v) = a·arctan(v/b)     (1)

với các giá trị vô hướng a, b>0, được chia sẻ qua tất cả các trọng số trong NN. Hình 2 cho thấy đồ thị của Ψ(v) và đạo hàm của nó. Vì phương pháp của chúng tôi nhắm vào các trọng số của một NN, chúng tôi không tái tham số hóa các lớp batch normalization hoặc bias.

--- TRANG 4 ---
Việc tái tham số hóa hạn chế độ lớn của các trọng số nằm trong khoảng (-aπ/2, aπ/2). Do đó, tham số a có thể được sử dụng để chia tỷ lệ và hạn chế giá trị tuyệt đối của các trọng số. Gradient của mất mát tái tham số hóa L(·,Ψ(v)) đối với v là

∂L(·,Ψ(v))/∂v = ∂L(·,Ψ(v))/∂Ψ(v) · ∂Ψ(v)/∂v     (2)
                = ∂L(·,Ψ(v))/∂Ψ(v) · a/(b·(1+v²/b²))     (3)

Tham số b có thể được sử dụng để nén và kéo dãn hàm trong trục v, do đó thay đổi độ dốc của hàm, tức là gradient. Lưu ý rằng cả hai tham số a và b đều có tác động đến gradient. Trong Phương trình 2, chúng ta thấy rằng việc tái tham số hóa của chúng tôi thêm yếu tố nhân ∂Ψ(v)/∂v vào đạo hàm ∂L(·,Ψ(v))/∂Ψ(v). Đạo hàm ∂Ψ(v)/∂v tương đối lớn hơn với |v| nhỏ so với |v| lớn (xem Hình 2). Do đó, các trọng số gần số không được cập nhật mạnh hơn tương đối so với các trọng số xa số không hơn. Điều này thúc đẩy các trọng số gần số không có phạm vi lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn.

Bây giờ chúng tôi khảo sát cách các thực hành học sâu đã thiết lập có thể được sử dụng với weight compander. Vì chúng tôi muốn so sánh NN tiêu chuẩn với NN tái tham số hóa, sơ đồ khởi tạo và điều chuẩn weight decay phải tương tự như NN tiêu chuẩn.

1) Khởi Tạo: Hai khía cạnh quan trọng của huấn luyện đáng tin cậy là khởi tạo trọng số [33] và các lớp chuẩn hóa như batch-norm [20] hoặc layer-norm [21]. Được thúc đẩy bởi [15], chúng tôi khởi tạo v sao cho Ψ(v) bằng khởi tạo ban đầu của w (ví dụ khởi tạo Kaiming [33]). Do đó, v được khởi tạo bằng cách tính hàm nghịch đảo của (1):

v = Ψ⁻¹(w) = b·tan(w/a)     (4)

Điều này đảm bảo rằng việc tái tham số hóa trọng số của chúng tôi duy trì tất cả tính chất của các sơ đồ khởi tạo điển hình. Phương pháp khởi tạo này được sử dụng trong tất cả thí nghiệm ở Phần IV.

Nói chung, các trọng số của DNN tiêu chuẩn được khởi tạo ngẫu nhiên với các giá trị nhỏ, tức là -π/2 < w < π/2. Nói chung, Phương trình (4) được định nghĩa rõ ràng với a≥0.5 vì khi đó w/a cách xa các cực của tan. Trong các thí nghiệm của chúng tôi, chúng tôi phát hiện ra rằng nếu b>0.5, thì a<0.5 thường làm hại hiệu suất. Trong phần sau, chúng tôi sẽ giả định rằng a, b≥0.5.

2) Weight Decay: Weight Decay là một phương pháp điều chuẩn được áp dụng cho các trọng số của một NN [7]. Cho w là vector của tất cả trọng số trong một NN. Weight decay (hoặc điều chuẩn L2) thêm một phạt trên chuẩn L2 của các trọng số vào hàm mất mát, tức là

L̄(w) = L(w) + λw^T w     (5)

trong đó λ là giá trị xác định cường độ của phạt và w là vector trọng số chứa tất cả trọng số của một NN. Để có được kết quả có thể so sánh trong các thí nghiệm của chúng tôi, chúng tôi áp dụng weight decay trên w = Ψ(v) và không phải trên v.

3) Tác Động của Các Thuật Toán Tối Ưu Hóa Hiện Đại: Weight compander hoạt động tốt với bộ tối ưu SGD, vì SGD tính toán cùng bước gradient như được nêu trong Phương trình 2. Tuy nhiên, điều quan trọng là phân tích tác động của các thuật toán tối ưu hóa thích ứng hiện đại [34]–[36] đối với phương pháp của chúng tôi. Bộ tối ưu Adam [34] tính các trung bình động của gradient và gradient bình phương để có được bước gradient thích ứng bất biến với việc tái chia tỷ lệ gradient. Do đó, Adam và các thuật toán tối ưu hóa thích ứng khác [34], [36] có thể giảm hiệu ứng của phương pháp chúng tôi, vì weight compander tái chia tỷ lệ gradient một cách rõ ràng. Để giảm thiểu vấn đề này, chúng tôi giới thiệu bộ tối ưu thích ứng đã sửa đổi từ [15]. Như được nêu trong Phần III-A, chúng ta có thể chia gradient ∂L/∂v thành hai thành phần ∂L/∂Ψ(v) và ∂Ψ(v)/∂v. Bộ tối ưu thích ứng đã sửa đổi đầu tiên tính bước gradient thích ứng dựa trên gradient thô ∂L/∂Ψ(v). Thứ hai, nó nhân ∂Ψ(v)/∂v với bước gradient thích ứng đã tính để tái chia tỷ lệ nó. Chi tiết hơn có thể tìm thấy trong [15].

4) Suy Luận: Thời gian suy luận là lượng thời gian cần thiết để một NN đưa ra dự đoán trên dữ liệu kiểm tra. Các NN tái tham số hóa có nỗ lực tính toán cao hơn một chút, vì chúng cần đánh giá hàm tái tham số hóa phi tuyến cho mỗi trọng số trong NN.

Tuy nhiên, điều này chỉ ảnh hưởng đến huấn luyện (≈7%–14% overhead thời gian chạy tùy thuộc vào kích thước của DNN) vì chúng ta có thể loại bỏ hàm tái tham số hóa sau khi huấn luyện hoàn thành. Chúng ta có thể sao chép các giá trị w = Ψ(v) cho v đã tối ưu hóa sang một mạng khác có cùng kiến trúc. Thay vì đánh giá hàm Ψ(·) cho mỗi trọng số v trong mạng, chúng ta trực tiếp lưu các giá trị Ψ(v) như trọng số. Điều này tạo ra một NN có cùng thời gian suy luận như NN tiêu chuẩn. Do đó, weight compander chỉ cải thiện tổng quát hóa của NN trong quá trình huấn luyện và không có tác động tiêu cực đến bộ nhớ và độ phức tạp tính toán trong quá trình suy luận.

B. Tham Số Có Thể Học
Cho đến nay chúng tôi xử lý các tham số a và b như siêu tham số với giá trị cố định được đặt trước khi huấn luyện. Bây giờ chúng tôi trình bày cách các tham số này có thể được học bằng SGD. Theo nghĩa này, a và b trở thành tham số có thể học thay vì siêu tham số. Vì chúng tôi không muốn tăng số lượng tham số trong NN một cách đáng kể, chúng tôi sẽ xem xét hai phiên bản khác nhau của weight compander có thể học, khác nhau về độ chi tiết. Phiên bản thô chia sẻ các tham số a và b qua tất cả trọng số trong NN. Trong phiên bản tinh, mỗi lớp chia sẻ một cặp tham số a và b.

Trái ngược với weight compander tiêu chuẩn với siêu tham số cố định a và b, chúng tôi không áp dụng weight decay trên Ψ(v). Giả sử weight decay được áp dụng trên Ψ(v). Các tham số a và b được học thông qua SGD, do đó chúng bị ảnh hưởng bởi weight decay. Vì a và b được chia sẻ qua một lớp hoặc toàn bộ mạng, weight decay ảnh hưởng đến nhiều trọng số riêng lẻ cùng lúc. Chúng tôi có kết quả tốt hơn khi áp dụng weight decay chỉ trên các trọng số v và không trên các tham số a và b. Việc chia sẻ các tham số a và b qua một lớp hoặc toàn bộ mạng ngầm điều chuẩn a và b. Chúng tôi sẽ cung cấp kết quả thực nghiệm trong Phần IV.

IV. THÍ NGHIỆM
Chúng tôi xác thực thực nghiệm tính hữu ích của phương pháp trong nhận dạng hình ảnh có giám sát, tức là trên CIFAR-10/100 [37], trên TinyImageNet [38] và trên ImageNet [39]. Hơn nữa, chúng tôi phân tích sự khác biệt trong phân phối trọng số giữa NN có và không có tái tham số hóa và xác nhận rằng weight compander thực sự (i) hạn chế độ lớn của các trọng số và (ii) thúc đẩy các trọng số có phạm vi lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn. Chúng tôi so sánh NN sử dụng weight compander cùng với các phương pháp điều chuẩn tiêu chuẩn với NN chỉ sử dụng các phương pháp điều chuẩn tiêu chuẩn.

A. Phân Loại Hình Ảnh trên CIFAR-10/100
Chúng tôi đánh giá hiệu suất của WC trên tập dữ liệu phân loại CIFAR-10/100 [37]. Tập dữ liệu CIFAR-10/100 bao gồm 60,000 hình ảnh màu 32×32, chứa 50,000 hình ảnh huấn luyện và 10,000 hình ảnh kiểm tra. Chúng tôi sử dụng nhiều kiến trúc mạng khác nhau, các CNN ResNet18, ResNet34, ResNet50 [11], WideResNet-28-10 [40] và VGG16/19 với BN [41]. Đối với các thí nghiệm của chúng tôi, chúng tôi sử dụng PyTorch 1.10.1 [42] và một GPU Nvidia GeForce 1080Ti. Các thí nghiệm được chạy năm lần với các seed ngẫu nhiên khác nhau trong 200 epoch, dẫn đến các khởi tạo mạng khác nhau, thứ tự dữ liệu và bổ sung các tăng cường dữ liệu khác nhau. Đối với mỗi trường hợp, chúng tôi báo cáo độ chính xác kiểm tra trung bình và độ lệch chuẩn. Chúng tôi sử dụng phân chia 9/1 giữa các ví dụ huấn luyện và ví dụ xác thực và lưu mô hình tốt nhất trên tập xác thực. Mô hình này sau đó được sử dụng để đánh giá trên tập dữ liệu kiểm tra.

Tất cả mạng được huấn luyện với batch size 128. Chúng tôi sử dụng bộ tối ưu SGD với momentum 0.9 và learning rate ban đầu 0.1. Đối với ResNet18, ResNet34 và ResNet50 được huấn luyện trên CIFAR-10, chúng tôi giảm learning rate 0.1 tại epoch 90 và 136 và sử dụng weight decay với hệ số 1e-4 (như trong [11]). Đối với tất cả mạng được huấn luyện trên CIFAR-100 và đối với VGG16 với BN, VGG19 với BN, chúng tôi giảm learning rate

--- TRANG 5 ---
BẢNG I: Kết quả tìm kiếm siêu tham số cho các tham số a và b.

Model CIFAR10 CIFAR100
ResNet18 a=1.0, b=0.6 a=0.6, b=0.6
ResNet34 a=0.8, b=0.5 a=0.7, b=0.8
ResNet50 a=1.0, b=0.6 a=1.0, b=0.8
VGG16 a=0.7, b=0.7 a=0.7, b=1.0
VGG19 a=0.5, b=0.7 a=0.7, b=0.8

0.2 tại epoch 60, 120 và 160 và sử dụng weight decay với hệ số 5e-4 (như trong [40]). Đối với WideResNet-28-10, chúng tôi sử dụng tỷ lệ dropout 0.3. Dropout cũng được sử dụng trong các lớp kết nối đầy đủ trong VGG16 với batch normalization và VGG19 với batch normalization. Chúng tôi muốn chỉ ra rằng chúng tôi không thực hiện tìm kiếm siêu tham số cho learning rate, weight decay, v.v. Chúng tôi sử dụng cùng cài đặt như trong [40].

Các mạng ResNet được khởi tạo Kaiming-uniform [33]. Trong các mạng VGG, các lớp tuyến tính được khởi tạo Kaiming-uniform. Các lớp tích chập được khởi tạo với phân phối chuẩn Gaussian với mean 0 và độ lệch chuẩn 2/n, trong đó n là kích thước của kernel nhân với số kênh đầu ra trong cùng lớp. Để đạt được so sánh công bằng, chúng tôi khởi tạo các trọng số v sao cho việc khởi tạo Ψ(v) bằng việc khởi tạo của mạng tiêu chuẩn, như được giải thích trong Phần III-A1.

Để hoàn thiện, chúng tôi so sánh weight compander với powerpropagation [15] và weight normalization [14] sử dụng ResNet trên CIFAR-10/100.

1) Siêu Tham Số: Đối với mỗi kiến trúc mạng và cả hai tập dữ liệu, trước tiên chúng tôi thực hiện tìm kiếm lưới cho các tham số a và b (tức là a, b∈{0.5, 0.6, 0.7, 0.8, 0.9, 1.0}) cho một seed và chọn các giá trị tạo ra kết quả tốt nhất. Sau đó chúng tôi tiến hành các thí nghiệm cho các seed khác với các siêu tham số a và b từ seed đầu tiên. Các tham số a và b tạo ra kết quả tốt nhất được hiển thị trong Bảng I. Đối với WideResNet-28-10 được huấn luyện trên CIFAR-100, các tham số tạo ra kết quả tốt nhất là a=0.7 và b=0.7.

2) Kết Quả: Bảng II và III trình bày kết quả các thí nghiệm của chúng tôi trên CIFAR-10 và CIFAR-100. Các phần bổ sung vào tên của các mô hình mô tả phương pháp tái tham số hóa, tức là PP = Powerpropagation, WN = Weight Normalization, WC = Weight Compander. Phương pháp của chúng tôi cải thiện độ chính xác cho tất cả ResNet và VGG. Lợi ích hiệu suất bổ sung bằng cách tái tham số hóa các trọng số trong mạng ResNet50 đáng chú ý, tức là +0.75% trên CIFAR-10 và +1.56% trên CIFAR-100. Chúng tôi quan sát thấy các mạng ResNet tiêu chuẩn có xu hướng tổng quát hóa tệ hơn nếu có nhiều tham số có thể huấn luyện hơn. Điều này có thể được chống lại bằng cách sử dụng weight compander.

Powerpropagation [15] cho phép các tham số có độ lớn lớn hơn thích ứng, trong khi các tham số có độ lớn nhỏ hơn bị hạn chế. Do đó, các tham số có độ lớn thấp hầu như không bị ảnh hưởng bởi học tập. Sự hạn chế mạnh này dẫn đến giảm hiệu suất mô hình.

Weight normalization [14] cải thiện điều kiện của bài toán tối ưu hóa và tăng tốc hội tụ của SGD

--- TRANG 6 ---
BẢNG II: Độ chính xác của các thí nghiệm trên CIFAR-10.

Model Accuracy
ResNet18 94.04%±0.08%
ResNet18 + PP 92.94%±0.17%
ResNet18 + WN 94.26%±0.14%
ResNet18 + WC 94.44%±0.12%
ResNet34 93.69%±0.30%
ResNet34 + PP 93.29%±0.16%
ResNet34 + WN 94.17%±0.22%
ResNet34 + WC 94.43%±0.25%
ResNet50 93.31%±0.36%
ResNet50 + PP 93.47%±0.14%
ResNet50 + WN 93.89%±0.23%
ResNet50 + WC 94.06%±0.27%
VGG16-BN 93.27%±0.12%
VGG16-BN + WC 93.52%±0.19%
VGG19-BN 93.21%±0.08%
VGG19-BN + WC 93.33%±0.25%

BẢNG III: Độ chính xác của các thí nghiệm trên CIFAR-100.

Model Accuracy
ResNet18 76.47%±0.20%
ResNet18 + PP 74.70%±1.32%
ResNet18 + WN 76.83%±0.22%
ResNet18 + WC 76.73%±0.09%
ResNet34 77.07%±0.45%
ResNet34 + PP 74.70%±1.32%
ResNet34 + WN 76.98%±0.39%
ResNet34 + WC 77.54%±0.42%
ResNet50 76.17%±0.69%
ResNet50 + PP 75.23%±0.25%
ResNet50 + WN 76.10%±0.25%
ResNet50 + WC 77.73%±0.40%
WRN-28-10 80.09%±0.14%
WRN-28-10 + WC 80.24%±0.36%
VGG16-BN 72.48%±0.35%
VGG16-BN + WC 72.74%±0.25%
VGG19-BN 71.34%±0.14%
VGG19-BN + WC 71.86%±0.26%

[14]. Điều này cải thiện hiệu suất một chút so với các mạng cơ sở. Tuy nhiên, weight compander vượt trội hơn weight normalization (trừ ResNet18 trên CIFAR-10). Đặc biệt đối với các mạng sâu (ResNet34, ResNet50) được huấn luyện trên các nhiệm vụ phức tạp hơn (CIFAR-100), weight compander hoạt động tốt hơn nhiều so với weight normalization.

B. Phân Loại Hình Ảnh trên TinyImageNet
TinyImageNet [38] chứa 100,000 hình ảnh thuộc 200 lớp (500 cho mỗi lớp) được thu nhỏ thành hình ảnh màu 64×64. Mỗi lớp có 500 hình ảnh huấn luyện và 50 hình ảnh xác thực. Đối với các thí nghiệm của chúng tôi trên TinyImageNet, chúng tôi sử dụng PyTorch 1.10.1 [42] và một GPU Nvidia GeForce 1080Ti. Các thí nghiệm được chạy ba lần với các seed ngẫu nhiên khác nhau trong 300 epoch sử dụng ResNet50. Theo thực hành chung, chúng tôi báo cáo độ chính xác phân loại top-1 trên tập xác thực. Tương tự với các thí nghiệm trên CIFAR, chúng tôi xác định các siêu tham số tối ưu a=0.7 và b=0.8 cho ResNet50. Chúng tôi huấn luyện tất cả các mô hình sử dụng bộ tối ưu SGD

BẢNG IV: Độ chính xác Top-1 của các thí nghiệm trên TinyImageNet.

Model Accuracy
ResNet50 65.08%±0.38%
ResNet50 + WC 65.93%±0.12%

BẢNG V: Độ chính xác Top-1 của các thí nghiệm trên ImageNet.

Model Accuracy
ResNet50 76.80%±0.11%
ResNet50 + WC 77.02%±0.08%

với momentum 0.9 và learning rate ban đầu 0.1 giảm theo hệ số 0.1 tại epoch 75, 150, và 225. Hơn nữa, chúng tôi sử dụng weight decay với hệ số 1e-4. Batch size được đặt là 128. Dữ liệu huấn luyện được tăng cường bằng cách sử dụng random crop với kích thước 224, random horizontal flip và chuẩn hóa. Dữ liệu xác thực được tăng cường bằng cách resize thành 256×256, sử dụng center crop với kích thước 224 và chuẩn hóa.

1) Kết Quả: Bảng IV trình bày độ chính xác top-1 của các thí nghiệm của chúng tôi trên TinyImageNet. Sử dụng WC tăng độ chính xác của ResNet50 trên TinyImageNet 0.86%.

C. ImageNet
Tập dữ liệu phân loại ILSVRC 2012 [39] chứa 1.2 triệu hình ảnh huấn luyện, 50,000 hình ảnh xác thực, và 150,000 hình ảnh kiểm tra. Hình ảnh được gắn nhãn với 1,000 danh mục. Đối với các thí nghiệm của chúng tôi trên ImageNet, chúng tôi sử dụng PyTorch 1.10.1 [42] và 4 GPU Nvidia GeForce 1080Ti. Các thí nghiệm được chạy ba lần với các seed ngẫu nhiên khác nhau trong 300 epoch sử dụng ResNet50. Theo thực hành chung, chúng tôi báo cáo độ chính xác phân loại top-1 trên tập xác thực. Tương tự với các thí nghiệm trên CIFAR và TinyImageNet, chúng tôi xác định các siêu tham số tối ưu a=0.6 và b=0.7. Chúng tôi huấn luyện tất cả các mô hình sử dụng bộ tối ưu SGD với momentum 0.9 và learning rate ban đầu 0.1 giảm theo hệ số 0.1 tại epoch 75, 150, 225. Hơn nữa, chúng tôi sử dụng weight decay với hệ số 1e-4. Batch size được đặt là 256. Dữ liệu huấn luyện và dữ liệu xác thực được tăng cường như được mô tả cho TinyImageNet.

1) Kết Quả: Bảng V trình bày độ chính xác top-1 của các thí nghiệm của chúng tôi trên ImageNet. Sử dụng WC tăng độ chính xác của ResNet50 trên ImageNet 0.22%.

D. Phân Phối Trọng Số
Trong phần sau, chúng tôi phân tích hiệu ứng của weight compander đối với phân phối trọng số của các mạng. Chúng tôi xác nhận thực nghiệm rằng weight compander tăng cường hai hiện tượng được đề cập trong Phần III-A, tức là (i) nó hạn chế độ lớn của các trọng số và (ii) nó buộc mạng có phạm vi trọng số lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn.

Đặt a=0.8, b=0.5 tạo ra kết quả tốt nhất cho ResNet34 trên CIFAR-10 sử dụng weight compander cùng với các phương pháp điều chuẩn tiêu chuẩn. Hình 3 cho thấy phân phối trọng số của lớp tích chập đầu tiên của ResNet34 được huấn luyện trên CIFAR-10 theo epoch có và không có weight compander. Lưu ý rằng cả hai NN đều có cùng phân phối trọng số tại epoch 0. Hình 3(a) cho thấy các trọng số trong một mạng neural tiêu chuẩn có thể tăng rất lớn ở đầu huấn luyện. Weight decay không đóng vai trò quan trọng ở đầu quy trình tối ưu hóa do mất mát huấn luyện cao và do đó không ngăn các trọng số tăng rất lớn trong những epoch đầu. Hình 3(b) minh họa rằng weight compander giới hạn giá trị tối đa và tối thiểu của các trọng số và do đó buộc các trọng số ở trong khoảng (-aπ/2, aπ/2) trong quá trình huấn luyện. Vì mỗi trọng số có độ lớn nhỏ, ảnh hưởng của các trọng số riêng lẻ trong quá trình dự đoán bị hạn chế.

Các NN tiêu chuẩn nói chung không có các trọng số rất lớn ở cuối huấn luyện. Vì chúng tôi hạn chế độ lớn của các trọng số từ đầu huấn luyện, phương pháp của chúng tôi buộc SGD tìm kiếm từ epoch đầu tiên cho một giải pháp với các trọng số nhỏ hơn. Điều này cũng giải thích hiệu suất tăng của phương pháp chúng tôi.

Hình 4 cho thấy weight compander khuyến khích các trọng số phân tán xung quanh số không, tức là có phạm vi trọng số lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn. Do đó, nhiều trọng số được tham gia trong quá trình dự đoán. Do mức độ dư thừa trọng số tăng, mô hình ít nhạy cảm hơn với sự khác biệt thống kê giữa dữ liệu huấn luyện và kiểm tra.

Phân phối trọng số cho thấy weight compander thúc đẩy dân chủ trọng số trong NN, tức là NN không dựa vào một vài trọng số lớn mà thay vào đó sử dụng nhiều trọng số nhỏ để hưởng lợi từ tất cả thông tin có sẵn.

Sự khác biệt trong phân phối trọng số ít rõ ràng hơn ở giữa và cuối mạng. Các lớp tích chập đầu phát hiện các đặc trưng cấp thấp như cạnh và đường cong, trong khi các bộ lọc ở các lớp cao hơn được học để mã hóa các đặc trưng trừu tượng hơn. Do đó, các DNN tái tham số hóa trích xuất nhiều thông tin hơn từ dữ liệu đầu vào so với DNN tiêu chuẩn. Tuy nhiên, mỗi lớp phải được tái tham số hóa để đạt hiệu suất tốt nhất.

E. Tham Số Có Thể Học
Vì tìm kiếm siêu tham số tốn rất nhiều thời gian và có thể tốn kém về mặt tính toán, chúng tôi phân tích hiệu suất của weight regulation có thể học trong đó các tham số a và b được học bằng SGD. Các thí nghiệm được thực hiện trong cùng cài đặt như được mô tả ở đầu Phần IV. Chúng tôi thực hiện tìm kiếm siêu tham số nhỏ để tìm các giá trị khởi tạo tối ưu cho a và b, tức là a, b ∈ {0.5, 1.0}. Như được mô tả trong Phần III-B, chúng tôi so sánh hai phiên bản của weight compander có thể học, tức là phiên bản tinh trong đó các tham số a và b được chia sẻ qua mỗi lớp và phiên bản thô trong đó các tham số được chia sẻ qua tất cả trọng số trong NN. Tuy nhiên, phiên bản tinh hoạt động tốt hơn đáng kể so với phiên bản thô. Điều này ngụ ý rằng các lớp khác nhau trong mạng cần các tái tham số hóa khác nhau. Do đó, chúng tôi chỉ trình bày kết quả của phiên bản tinh.

BẢNG VI: Độ chính xác của weight compander có thể học trên CIFAR-10 cho phiên bản tinh.

Model Accuracy
ResNet18 94.04%±0.08%
ResNet18 + WC 94.16%±0.08%
ResNet34 93.69%±0.30%
ResNet34 + WC 94.46%±0.07%
ResNet50 93.31%±0.36%
ResNet50 + WC 94.80%±0.14%

Khởi tạo phiên bản tinh với a=0.5 và b=1.0 cho hiệu suất tốt nhất. Bảng VI cho thấy kết quả cho ResNet được huấn luyện trên CIFAR-10. Trái ngược với weight compander với siêu tham số cố định a và b, weight compander có thể học có thể đạt hiệu suất tốt hơn đáng kể trên ResNet50. Huấn luyện NN đòi hỏi việc tối thiểu hóa một hàm mất mát phi tuyến và phi lồi nhiều chiều. Các kiến trúc mạng khác nhau ảnh hưởng đến cảnh quan mất mát và do đó lỗi tổng quát hóa và khả năng huấn luyện [43]. Trong trường hợp weight compander có thể học, SGD có cơ hội thay đổi việc tái tham số hóa của mỗi lớp trong quá trình huấn luyện. Do đó cảnh quan mất mát thay đổi trong quá trình huấn luyện. Chúng tôi nghĩ rằng weight compander có thể học hoạt động tốt hơn vì quy trình tối ưu hóa bây giờ có cơ hội ảnh hưởng đến cảnh quan mất mát. Hiệu suất trên ResNet18 kém hơn một chút so với trường hợp với siêu tham số cố định. Những kết quả này có thể được cải thiện bằng cách tìm các siêu tham số tối ưu cho phiên bản có thể học.

Chúng tôi ghi lại các giá trị của các tham số a và b cho mỗi lớp để hiểu tác động của chúng đối với hiệu suất. Các tham số a trong các lớp tích chập không thay đổi nhiều so với lớp tuyến tính. Chúng ở gần các giá trị ban đầu hoặc di chuyển trong một vùng lân cận xung quanh giá trị ban đầu (±0.35). Thú vị là, tham số a trong lớp tuyến tính cuối cùng có sự phát triển rất bất thường. Các giá trị tuyệt đối của chúng rất cao, ví dụ 8.5 cho ResNet18 trên một seed. Các giá trị tương tự hoặc thậm chí lớn hơn cho a được quan sát thấy trên ResNet34 và ResNet50.

Các tham số b đã thay đổi nhiều hơn trong quá trình tối ưu hóa. Đối với các lớp tích chập, chúng chủ yếu nằm trong khoảng [0.5, 2.5] ở cuối huấn luyện. Thú vị là, các giá trị cho các tham số b cao hơn ở các lớp đầu so với các lớp sau trong mạng. Một lần nữa, lớp tuyến tính là một ngoại lệ trong đó các tham số b nhận các giá trị từ 0.5 đến 11.15 qua năm seed. Tuy nhiên, mạng tự động hội tụ đến phân phối trọng số mong muốn được mô tả trong các phần trước. Đối với một số lớp, sự khác biệt của phân phối trọng số ít rõ ràng hơn.

F. So Sánh với Hàm Hình S Khác
Bổ sung cho hàm tái tham số hóa của chúng tôi được trình bày trong bài báo, chúng tôi nghiên cứu các hàm hình S lẻ khác, tức là các hàm có tính đối xứng quay đối với gốc tọa độ. Điều này có nghĩa là đồ thị của chúng không thay đổi sau khi quay 180 độ quanh gốc tọa độ. Lưu ý rằng không thể sử dụng một hàm tái tham số hóa tùy ý, ví dụ

--- TRANG 7 ---
0 25 50 75 100 125 150 175 200
Training Epoch-10.0
-7.5
-5.0
-2.5
0.0
2.5
5.0
7.5
10.0Trọng số Phân Phối Trọng Số của ResNet34(a) Không có weight compander

0 25 50 75 100 125 150 175 200
Training Epoch-1.0
-0.5
0.0
0.5
1.0Trọng số Phân Phối Trọng Số của ResNet34 Tái Tham Số Hóa (b) Có weight compander

Hình 3: Phân phối trọng số của w theo epoch trong lớp tích chập đầu tiên của ResNet34 được huấn luyện trên CIFAR10 không có weight compander 3(a) và có weight compander 3(b). Từ trên xuống dưới, các đường biểu thị phân vị với các giá trị: tối đa, 93%, 84%, 69%, 50%, 31%, 16%, 7%, tối thiểu.

0 25 50 75 100 125 150 175 200
Training Epoch-1.0
-0.5
0.0
0.5
1.0Trọng số Phân Phối Trọng Số của ResNet34
(a) Không có weight compander

0 25 50 75 100 125 150 175 200
Training Epoch-1.0
-0.5
0.0
0.5
1.0Trọng số Phân Phối Trọng Số của ResNet34 Tái Tham Số Hóa (b) Có weight compander

Hình 4: Phân phối trọng số của w theo epoch trong lớp tích chập đầu tiên của ResNet34 trên CIFAR10 không có weight compander 4(a) (view thu phong của Hình 3(a)) và có weight compander 4(b).

sử dụng w = Ψ(v) = v² hạn chế các trọng số chỉ dương và đạo hàm của v² tăng theo |v| tăng. Những hiệu ứng này làm hại quá trình tối ưu hóa.

Chúng tôi xem xét các hàm tái tham số hóa khác sau đây w = Ψ(v) = a·arsinh(v/b) và w = Ψ(v) = a·erf(v/b) với a, b>0. Trái ngược với arctan và erf, arsinh không bị giới hạn. Chúng tôi đưa nó vào nghiên cứu vì nó tương tự như các đường cong hình S trong vùng lân cận xung quanh số không. Tương tự với các thí nghiệm trong Phần IV trong bài báo, chúng tôi thực hiện tìm kiếm lưới cho mỗi mạng và tập dữ liệu cho các tham số a và b (tức là a, b ∈ {0.5, ..., 1.0}) cho một seed và chọn các giá trị tạo ra kết quả tốt nhất. Sau đó chúng tôi tiến hành các thí nghiệm khác cho các seed khác với các siêu tham số a và b từ seed đầu tiên. Chúng tôi sử dụng cùng cài đặt như được trình bày trong Phần IV.

Bảng VII trình bày kết quả của ResNet được huấn luyện trên CIFAR-10 và CIFAR-100, được tái tham số hóa sử dụng phương trình arsinh và phương trình erf. Tái tham số hóa ResNet với arsinh hoặc erf cải thiện độ chính xác cho tất cả ResNet so với baseline. Tuy nhiên, tái tham số hóa NN với arsinh hoạt động tệ hơn so với tái tham số hóa mạng với arctan trừ ResNet34 được huấn luyện trên CIFAR-10.

--- TRANG 8 ---
BẢNG VII: Kết quả của ResNet được huấn luyện trên CIFAR-10 và CIFAR-100 được tái tham số hóa với arsinh và erf.

Model Accuracy CIFAR-10 Accuracy CIFAR-100
ResNet18 arcsinh 94.34%±0.24% 76.59%±0.25%
ResNet18 erf 94.36%±0.26% 76.80%±0.11%
ResNet18 + WC 94.44%±0.12% 76.73%±0.09%
ResNet34 arcsinh 94.68%±0.39% 77.21%±0.23%
ResNet34 erf 94.52%±0.14% 77.26%±0.39%
ResNet34 + WC 94.43%±0.25% 77.54%±0.42%
ResNet50 arcsinh 93.82%±0.34% 76.76%±0.42%
ResNet50 erf 93.87%±0.30% 77.51%±0.37%
ResNet50 + WC 94.06%±0.27% 77.73%±0.40%

G. Gradient Biến Mất
Chúng tôi không gặp gradient biến mất khi sử dụng WC. Phương pháp của chúng tôi được áp dụng cho các trọng số. Vì hầu hết các trọng số được phân phối xung quanh số không (xem Hình 4(b)), gradient của hàm tái tham số hóa đối với hầu hết các trọng số khác số không. Hơn nữa, đối với một số trường hợp (a>b), đạo hàm của hàm tái tham số hóa được đánh giá tại 0 thậm chí lớn hơn 1. Điều quan trọng cần lưu ý là hàm tái tham số hóa phi tuyến của chúng tôi không nên được so sánh với các hàm kích hoạt phi tuyến.

V. KẾT LUẬN VÀ NGHIÊN CỨU TƯƠNG LAI
Chúng tôi đã trình bày weight compander, một tái tham số hóa đơn giản của các trọng số trong NN để giảm overfitting. Nó dựa trên hai tiền đề (i) các trọng số lớn là dấu hiệu của một mạng phức tạp hơn đã overfitted với dữ liệu huấn luyện và (ii) các mạng được điều chuẩn có xu hướng có phạm vi trọng số lớn hơn xung quanh số không với ít trọng số tập trung tại số không hơn. Hàm tái tham số hóa được đề xuất của chúng tôi ngầm hỗ trợ mạng để hạn chế độ lớn của các trọng số trong khi buộc chúng tránh xa số không cùng lúc. Do đó, phương pháp của chúng tôi thúc đẩy dân chủ trọng số, tức là ảnh hưởng của các trọng số riêng lẻ bị hạn chế và nhiều trọng số được tham gia trong quá trình dự đoán.

Sử dụng weight compander cùng với các phương pháp điều chuẩn thường dùng (tức là [1], [7], [9], [20], [44]) tăng hiệu suất của ResNet trên CIFAR-10, CIFAR-100, TinyImageNet và ImageNet và của VGG trên CIFAR-10 và CIFAR-100. Chúng tôi cải thiện độ chính xác kiểm tra của ResNet50 trên CIFAR-100 sebesar 1.56%, trên TinyImageNet sebesar 0.86% và trên ImageNet sebesar 0.22%. Hơn nữa, chúng tôi phân tích hiệu ứng của weight compander đối với phân phối trọng số. Cuối cùng, chúng tôi trình bày weight compander có thể học trong đó các siêu tham số được giới thiệu được học bằng SGD.

Chúng tôi chưa khám phá toàn bộ phạm vi khả năng của weight compander. Nghiên cứu tương lai của chúng tôi sẽ bao gồm phân tích vai trò của các hàm tái tham số hóa khác nhau và phát triển thêm weight compander có thể học. Chúng tôi đã thực hiện các thí nghiệm với arsinh và erf trong đó chúng tôi có thể đạt được những cải thiện hiệu suất tương tự như với arctan. Chúng tôi sẽ phân tích các phân phối trọng số tối ưu trong DNN để tăng lợi ích hiệu suất của các phương pháp tái tham số hóa trọng số. Hơn nữa, chúng tôi sẽ nghiên cứu cách tái tham số hóa trọng số của DNN có thể cải thiện các nhiệm vụ khác trong học sâu. Một phần mở rộng của phương pháp được giới thiệu của chúng tôi có thể là làm cho việc tái tham số hóa phức tạp hơn, tức là có sự tương quan giữa các trọng số khác nhau.

TÀI LIỆU THAM KHẢO
[1] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: a simple way to prevent neural networks from overfitting," J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958, 2014. [Online]. Available: http://dl.acm.org/citation.cfm?id=2670313

[2] L. Wan, M. D. Zeiler, S. Zhang, Y. LeCun, and R. Fergus, "Regularization of Neural Networks using DropConnect," in Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, ser. JMLR Workshop and Conference Proceedings, vol. 28. JMLR.org, 2013, pp. 1058–1066. [Online]. Available: http://proceedings.mlr.press/v28/wan13.html

[3] X. Ying, "An Overview of Overfitting and its Solutions," Journal of Physics: Conference Series, vol. 1168, p. 022022, feb 2019. [Online]. Available: https://doi.org/10.1088/1742-6596/1168/2/022022

[4] I. J. Goodfellow, Y. Bengio, and A. C. Courville, Deep Learning, ser. Adaptive computation and machine learning. MIT Press, 2016. [Online]. Available: http://www.deeplearningbook.org/

[5] Y. Yao, L. Rosasco, and A. Caponnetto, "On Early Stopping in Gradient Descent Learning," Constructive Approximation, vol. 26, no. 2, pp. 289–315, Aug. 2007. [Online]. Available: https://doi.org/10.1007/s00365-006-0663-2

[6] R. Tibshirani, "Regression Shrinkage and Selection via the Lasso," Journal of the Royal Statistical Society. Series B (Methodological), vol. 58, no. 1, pp. 267–288, 1996. [Online]. Available: http://www.jstor.org/stable/2346178

[7] A. Krogh and J. A. Hertz, "A Simple Weight Decay Can Improve Generalization," in Advances in Neural Information Processing Systems 4, [NIPS Conference, Denver, Colorado, USA, December 2-5, 1991], J. E. Moody, S. J. Hanson, and R. Lippmann, Eds. Morgan Kaufmann, 1991, pp. 950–957. [Online]. Available: http://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization

[8] S. J. Nowlan and G. E. Hinton, "Simplifying Neural Networks by Soft Weight-Sharing," Neural Comput., vol. 4, no. 4, pp. 473–493, 1992. [Online]. Available: https://doi.org/10.1162/neco.1992.4.4.473

[9] C. Shorten and T. M. Khoshgoftaar, "A survey on Image Data Augmentation for Deep Learning," J. Big Data, vol. 6, p. 60, 2019. [Online]. Available: https://doi.org/10.1186/s40537-019-0197-0

[10] D. W. Opitz and R. Maclin, "Popular Ensemble Methods: An Empirical Study," J. Artif. Intell. Res., vol. 11, pp. 169–198, 1999. [Online]. Available: https://doi.org/10.1613/jair.614

[11] K. He, X. Zhang, S. Ren, and J. Sun, "Deep Residual Learning for Image Recognition," in 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 2016, pp. 770–778. [Online]. Available: https://doi.org/10.1109/CVPR.2016.90

[12] M. Tan and Q. V. Le, "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks," in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 6105–6114. [Online]. Available: http://proceedings.mlr.press/v97/tan19a.html

[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is All you Need," in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 5998–6008. [Online]. Available: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html

[14] T. Salimans and D. P. Kingma, "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks," in Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, Eds., 2016, p.

--- TRANG 9 ---
901. [Online]. Available: https://proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html

[15] J. Schwarz, S. Jayakumar, R. Pascanu, P. E. Latham, and Y. Teh, "Powerpropagation: A sparsity inducing weight reparameterisation," in Advances in Neural Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., vol. 34. Curran Associates, Inc., 2021, pp. 28 889–28 903. [Online]. Available: https://proceedings.neurips.cc/paper/2021/file/f1e709e6aef16ba2f0cd6c7e4f52b9b6-Paper.pdf

[16] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, "Weight Uncertainty in Neural Networks," in Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ser. ICML'15. Lille, France: JMLR.org, 2015, p. 1613–1622.

[17] J. Kukačka, V. Golkov, and D. Cremers, "Regularization for Deep Learning: A Taxonomy," Oct. 2017.

[18] T. DeVries and G. W. Taylor, "Improved Regularization of Convolutional Neural Networks with Cutout," Aug. 2017.

[19] G. Ghiasi, T.-Y. Lin, and Q. V. Le, "DropBlock: A regularization method for convolutional networks," in Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31. Curran Associates, Inc., 2018. [Online]. Available: https://proceedings.neurips.cc/paper/2018/file/7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper.pdf

[20] S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift," in Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, ser. JMLR Workshop and Conference Proceedings, F. R. Bach and D. M. Blei, Eds., vol. 37. JMLR.org, 2015, pp. 448–456. [Online]. Available: http://proceedings.mlr.press/v37/ioffe15.html

[21] L. J. Ba, J. R. Kiros, and G. E. Hinton, "Layer Normalization," CoRR, vol. abs/1607.06450, 2016. [Online]. Available: http://arxiv.org/abs/1607.06450

[22] Y. Wu and K. He, "Group Normalization," Int. J. Comput. Vis., vol. 128, no. 3, pp. 742–755, 2020. [Online]. Available: https://doi.org/10.1007/s11263-019-01198-w

[23] S. Han, J. Pool, J. Tran, and W. J. Dally, "Learning both Weights and Connections for Efficient Neural Network," in Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, Eds., 2015, pp. 1135–1143. [Online]. Available: https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html

[24] P. Wimmer, J. Mehnert, and A. Condurache, "FreezeNet: Full Performance by Reduced Storage Costs," in Computer Vision - ACCV 2020 - 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 - December 4, 2020, Revised Selected Papers, Part VI, ser. Lecture Notes in Computer Science, H. Ishikawa, C. Liu, T. Pajdla, and J. Shi, Eds., vol. 12627. Springer, 2020, pp. 685–701. [Online]. Available: https://doi.org/10.1007/978-3-030-69544-6_41

[25] ——, "Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs," in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 2022, pp. 12 517–12 527. [Online]. Available: https://doi.org/10.1109/CVPR52688.2022.01220

[26] A. Brock, T. Lim, J. Ritchie, and N. Weston, "FreezeOut: Accelerate Training by Progressively Freezing Layers," Dec. 2017, NIPS 2017 Workshop on Optimization: 10th NIPS Workshop on Optimization for Machine Learning, NIPS ; Conference date: 08-12-2017.

[27] A. H. Jiang, D. L. Wong, G. Zhou, D. G. Andersen, J. Dean, G. R. Ganger, G. Joshi, M. Kaminsky, M. Kozuch, Z. C. Lipton, and P. Pillai, "Accelerating Deep Learning by Focusing on the Biggest Losers," CoRR, vol. abs/1910.00762, 2019. [Online]. Available: http://arxiv.org/abs/1910.00762

[28] R. S. Raju, K. Daruwalla, and M. H. Lipasti, "Accelerating Deep Learning with Dynamic Data Pruning," CoRR, vol. abs/2111.12621, 2021. [Online]. Available: https://arxiv.org/abs/2111.12621

[29] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, "On the importance of initialization and momentum in deep learning," in Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, ser. JMLR Workshop and Conference Proceedings, vol. 28. JMLR.org, 2013, pp. 1139–1147. [Online]. Available: http://proceedings.mlr.press/v28/sutskever13.html

[30] X. Li, S. Chen, X. Hu, and J. Yang, "Understanding the Disharmony Between Dropout and Batch Normalization by Variance Shift," in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE, 2019, pp. 2682–2690. [Online]. Available: http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.html

[31] Y. LeCun, J. S. Denker, and S. A. Solla, "Optimal Brain Damage," in Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], D. S. Touretzky, Ed. Morgan Kaufmann, 1989, pp. 598–605. [Online]. Available: http://papers.nips.cc/paper/250-optimal-brain-damage

[32] R. Ma, J. Miao, L. Niu, and P. Zhang, "Transformed ℓ1 regularization for learning sparse deep neural networks," Neural Networks, vol. 119, pp. 286–298, 2019. [Online]. Available: https://doi.org/10.1016/j.neunet.2019.08.015

[33] K. He, X. Zhang, S. Ren, and J. Sun, "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification," in 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015. IEEE Computer Society, 2015, pp. 1026–1034. [Online]. Available: https://doi.org/10.1109/ICCV.2015.123

[34] D. P. Kingma and J. Ba, "Adam: A Method for Stochastic Optimization," in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6980

[35] J. C. Duchi, E. Hazan, and Y. Singer, "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization," J. Mach. Learn. Res., vol. 12, pp. 2121–2159, 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id=2021068

[36] S. Ruder, "An overview of gradient descent optimization algorithms," CoRR, vol. abs/1609.04747, 2016. [Online]. Available: http://arxiv.org/abs/1609.04747

[37] A. Krizhevsky and G. Hinton, "Learning multiple layers of features from tiny images," Master's thesis, Department of Computer Science, University of Toronto, 2009.

[38] Y. Le and X. S. Yang, "Tiny ImageNet Visual Recognition Challenge," 2015.

[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei, "ImageNet Large Scale Visual Recognition Challenge," Int. J. Comput. Vis., vol. 115, no. 3, pp. 211–252, 2015. [Online]. Available: https://doi.org/10.1007/s11263-015-0816-y

[40] S. Zagoruyko and N. Komodakis, "Wide Residual Networks," CoRR, vol. abs/1605.07146, 2016. [Online]. Available: http://arxiv.org/abs/1605.07146

[41] K. Simonyan and A. Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition," in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1409.1556

[42] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, "PyTorch: An Imperative Style, High-Performance Deep Learning Library," in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 8024–8035. [Online]. Available: https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html

[43] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, "Visualizing the Loss Landscape of Neural Nets," in Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31. Curran Associates, Inc., 2018. [Online]. Available: https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf

[44] I. Loshchilov and F. Hutter, "Decoupled Weight Decay Regularization," in 7th International Conference on Learning Representations, ICLR

--- TRANG 10 ---
2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [Online]. Available: https://openreview.net/forum?id=Bkg6RiCqY7
