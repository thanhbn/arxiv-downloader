# 2406.06811.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/regularization/2406.06811.pdf
# Kích thước tệp: 2866169 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bản thảo
HỌC LIÊN TỤC BẰNG
CHÍNH QUY HÓA PHỔ
Alex Lewandowski†Michał Bortkiewicz¶Saurabh Kumar♯
András György‡Dale Schuurmans†‡⋆Mateusz Ostaszewski¶Marlos C. Machado†⋆
†Đại học Alberta,¶Đại học Công nghệ Warsaw,♯Đại học Stanford,
‡Google DeepMind,⋆Canada CIFAR AI Chair

TÓM TẮT
Mất tính dẻo dai là hiện tượng mà mạng nơ-ron có thể trở nên khó huấn luyện hơn trong quá trình học. Các thuật toán học liên tục tìm cách giảm thiểu tác động này bằng cách duy trì hiệu suất tốt trong khi duy trì khả năng huấn luyện mạng. Chúng tôi phát triển một kỹ thuật mới để cải thiện học liên tục được lấy cảm hứng từ quan sát rằng các giá trị đơn của tham số mạng nơ-ron tại khởi tạo là một yếu tố quan trọng cho khả năng huấn luyện trong giai đoạn đầu của việc học. Từ góc độ này, chúng tôi đưa ra một bộ chính quy hóa phổ mới cho học liên tục để duy trì tốt hơn những tính chất khởi tạo có lợi này trong suốt quá trình huấn luyện. Cụ thể, bộ chính quy hóa giữ giá trị đơn lớn nhất của mỗi lớp gần với một. Chính quy hóa phổ trực tiếp đảm bảo rằng tính đa dạng gradient được duy trì trong suốt quá trình huấn luyện, điều này thúc đẩy khả năng huấn luyện liên tục, trong khi can thiệp tối thiểu với hiệu suất trong một nhiệm vụ đơn lẻ. Chúng tôi trình bày một phân tích thực nghiệm cho thấy bộ chính quy hóa phổ được đề xuất có thể duy trì khả năng huấn luyện và hiệu suất qua một loạt các kiến trúc mô hình trong các môi trường học có giám sát liên tục và học tăng cường. Chính quy hóa phổ ít nhạy cảm với siêu tham số trong khi thể hiện việc huấn luyện tốt hơn trong các nhiệm vụ riêng lẻ, duy trì khả năng huấn luyện khi có nhiệm vụ mới đến, và đạt được hiệu suất tổng quát hóa tốt hơn.

1 GIỚI THIỆU
Một mục tiêu lâu dài của nghiên cứu máy học là phát triển các thuật toán có thể học liên tục và đối phó với những thay đổi không lường trước trong phân phối dữ liệu (Ring, 1994; Thrun, 1998). Tuy nhiên, các thuật toán học hiện tại gặp khó khăn trong việc học từ các mục tiêu thay đổi động và không thể thích ứng một cách uyển chuyển với những thay đổi không lường trước trong phân phối trong quá trình học (Abbas et al., 2023; Dohare et al., 2024; Lyle et al., 2023; Zilly et al., 2021). Những hạn chế như vậy có thể được xem như một phụ phẩm của việc giả định, theo cách này hay cách khác, rằng vấn đề là cố định. Gần đây, đã có sự nhận thức ngày càng tăng về thực tế rằng có những hạn chế về những gì có thể học được từ một tập dữ liệu cố định và không thay đổi (Hoffmann et al., 2022), rằng có những tính không cố định ngầm định trong nhiều vấn đề quan tâm (Igl et al., 2021), và rằng một số vấn đề thực tế được hưởng lợi từ việc học liên tục (Han et al., 2022; Janjua et al., 2023).

Khái niệm tính dẻo dai đã nhận được sự chú ý ngày càng tăng trong văn học học liên tục, nơi mất tính dẻo dai—hoặc là giảm khả năng huấn luyện của mạng nơ-ron (Dohare et al., 2021; Elsayed và Mahmood, 2024; Lyle et al., 2022), hoặc khả năng tổng quát hóa (Ash và Adams, 2020; Zilly et al., 2021)—đã được ghi nhận như một thiếu sót quan trọng trong các thuật toán học hiện tại. Các thuật toán học, và cụ thể hơn là mạng nơ-ron, có hiệu suất tốt trong môi trường học không liên tục, thường gặp khó khăn khi áp dụng cho các vấn đề học liên tục. Các môi trường mà mạng nơ-ron phải tiếp tục học sau khi có thay đổi trong phân phối dữ liệu thể hiện một sự mất tính dẻo dai đáng kể khiến việc học chậm lại (Lyle et al., 2023) hoặc thậm chí dừng lại sau những thay đổi liên tiếp (Abbas et al., 2023; Dohare et al., 2024; Nikishin et al., 2022).

Một số khía cạnh của thuật toán học đã được tìm thấy là góp phần hoặc giảm thiểu mất tính dẻo dai. Các ví dụ bao gồm loại tối ưu hóa (Dohare et al., 2024; Lyle et al., 2023), kích thước bước (Ash và Adams, 2020; Berariu et al., 2021), số lần lặp tối ưu hóa (Lyle et al., 2023), và việc sử dụng các bộ chính quy hóa cụ thể (Dohare et al., 2021; Kumar et al., 2023; Lewandowski et al., 2023; Lyle et al., 2022). Những yếu tố như vậy gợi ý rằng có thể có những nguyên lý tối ưu hóa cơ bản đơn giản hơn

Liên hệ với: Alex Lewandowski <lewandowski@ualberta.ca>.
1arXiv:2406.06811v2  [cs.LG]  27 Oct 2024

--- TRANG 2 ---
Bản thảo
chi phối sự mất tính dẻo dai. Ví dụ, thành công của một số phương pháp chính quy hóa mạng nơ-ron về phía các tính chất của khởi tạo gợi ý rằng một số tính chất đó giảm thiểu mất tính dẻo dai (Dohare et al., 2021; Kumar et al., 2023; Lyle et al., 2022).

Các tính chất của trọng số mạng nơ-ron tại khởi tạo được liên kết với khả năng huấn luyện trong giai đoạn đầu của việc học. Các phân tích trước đây đã chứng minh rằng khả năng huấn luyện của mạng nơ-ron sâu có thể được cải thiện bằng cách đảm bảo rằng phương sai khởi tạo của các kích hoạt ẩn và gradient vẫn đồng nhất qua các lớp khác nhau của mạng nơ-ron (Glorot và Bengio, 2010), điều này có tác dụng giữ giá trị đơn trung bình của Jacobian theo lớp gần với một. Một điều kiện mạnh hơn là đẳng cự động học, nơi tất cả các giá trị đơn gần với một, hoặc chính xác bằng một trong trường hợp khởi tạo trực giao (Pennington et al., 2017; Saxe et al., 2014; Xiao et al., 2018). Trong quá trình học, các giá trị đơn lệch khỏi khởi tạo của chúng và có xu hướng tăng theo thời gian (Martin và Mahoney, 2021), điều này có thể giảm khả năng huấn luyện và cản trở học liên tục.

Cho hiệu quả của khởi tạo trong việc đảm bảo khả năng huấn luyện của mạng nơ-ron, các thuật toán học liên tục có thể được hưởng lợi từ việc duy trì các tính chất liên quan của trọng số. Đây là động lực của một số phương pháp sử dụng khởi tạo hoặc thông qua chính quy hóa (Kumar et al., 2023) hoặc tái khởi tạo trọng số (Dohare et al., 2021; 2024; Sokar et al., 2023). Những phương pháp như vậy tận dụng khởi tạo một cách rõ ràng, hoặc bằng cách chính quy hóa tham số về phía giá trị khởi tạo của chúng hoặc bằng cách đặt lại tham số bằng cách lấy mẫu lại chúng từ phân phối khởi tạo. Trong bài báo này, chúng tôi tìm cách giải quyết trực tiếp hơn việc mất khả năng huấn luyện bằng cách duy trì các tính chất chính có mặt tại khởi tạo, từ đó tạo ra sự cân bằng tốt hơn giữa khả năng huấn luyện và hiệu suất so với các phương pháp sao chép khởi tạo một cách rõ ràng.

Chúng tôi điều tra các tính chất chính của khởi tạo đảm bảo khả năng huấn luyện, cách những tính chất này bị mất trong quá trình học, và tác động này có đối với học liên tục. Chúng tôi xác định rằng sự lệch khỏi phân phối giá trị đơn ban đầu có thể dẫn đến tính đa dạng gradient thấp, từ đó cản trở học liên tục. Dựa trên phân tích này, sau đó chúng tôi giới thiệu một bộ chính quy hóa phổ để kiểm soát sự lệch của các giá trị đơn bằng cách giữ giá trị đơn lớn nhất của mỗi lớp gần với một để trực tiếp giải quyết sự lệch của các giá trị đơn khỏi khởi tạo trong quá trình học. Các thí nghiệm của chúng tôi cho thấy chính quy hóa phổ có hiệu suất tốt hơn và ít nhạy cảm với siêu tham số hơn các bộ chính quy hóa khác qua các tập dữ liệu, tính không cố định, và kiến trúc. Trong khi các bộ chính quy hóa được tinh chỉnh tốt thường có thể giảm thiểu mất tính dẻo dai ở mức độ khác nhau, học liên tục với chính quy hóa phổ là mạnh mẽ, đạt được hiệu suất ban đầu cao và bền vững. Cụ thể, chúng tôi cho thấy chính quy hóa phổ cũng có khả năng cải thiện tổng quát hóa với cả Vision Transformer (Dosovitskiy et al., 2021) và ResNet-18 (He et al., 2016) trên các phiên bản liên tục của tiny-ImageNet (Le và Yang, 2015), CIFAR10, CIFAR100 (Krizhevsky, 2009), và SVHN2 (Netzer et al., 2011). Lưu ý rằng những tập dữ liệu và kiến trúc này bao gồm tất cả các môi trường thực nghiệm học có giám sát liên tục được xem xét trong văn học mất tính dẻo dai. Chúng tôi cũng cho thấy rằng chính quy hóa phổ có thể cải thiện hiệu suất của soft-actor critic (Haarnoja et al., 2018) trong các môi trường học tăng cường nơi mất tính dẻo dai xảy ra do thiên vị ưu tiên (Nikishin et al., 2022).

2 THIẾT LẬP VẤN ĐỀ
Chúng tôi điều tra khả năng huấn luyện của các thuật toán học mạng nơ-ron trong môi trường không phụ thuộc nhiệm vụ. Chúng tôi ký hiệu một mạng nơ-ron với L lớp, được định nghĩa đệ quy như fθ(x) := hL(x), nơi h0(x) = x là vector đầu vào, hl+1(x) = ϕ(Wlhl(x) + bl) với hàm kích hoạt theo từng phần tử, ϕ, và tham số θ = {θL, . . . , θ1} nơi θl = {Wl, bl} là tham số trọng số và độ lệch cho lớp l.¹

Chúng tôi giả định rằng fθ được huấn luyện trên một chuỗi nhiệm vụ: nhiệm vụ thứ τ được chỉ định bởi một phân phối pτ trên các cặp quan sát-mục tiêu, được ký hiệu bằng (x, y). Để đơn giản, chúng tôi giả định rằng nhiệm vụ (phân phối dữ liệu) thay đổi định kỳ sau mỗi T lần lặp. Ngoài ra, chúng tôi xem xét môi trường không phụ thuộc nhiệm vụ, nơi thuật toán học không có quyền truy cập thông tin về nhiệm vụ ngoại trừ thông qua dữ liệu mà nó lấy mẫu. Cho mỗi lần lặp trong nhiệm vụ, t ∈ ((τ−1)T, τT], thuật toán học tối ưu hóa các tham số của mạng nơ-ron để tối thiểu hóa mục tiêu

Jτ(θ) = E(x,y)∼pτ [ℓ(fθ(x), y)],

cho một hàm mất mát ℓ nào đó. Trong bài báo này, chúng tôi xem xét các phương pháp dựa trên gradient để học các tham số trọng số; phiên bản cơ bản của gradient descent sẽ cập nhật các tham số tại lần lặp t như θ(t+1) = θ(t) − α∇θJτ(t)(θ)|θ=θ(t), nơi τ(t) = ⌊t/T⌋ biểu thị số nhiệm vụ hiện tại. Trong

¹Trong ký hiệu này chúng tôi đã bỏ qua sự hiện diện của độ lệch, có thể được bao gồm hiệu quả trong các tham số.

--- TRANG 3 ---
Bản thảo
thực tế điều này được thực hiện bằng cách xem xét lỗi thực nghiệm thay vì Jτ(t) sử dụng các mẫu từ phân phối nhiệm vụ, (xt, yt) ∼ pτ, và thường là một số thuật toán tối ưu hóa phức tạp hơn.

Việc đánh giá thuật toán học liên tục được thực hiện ở cuối một nhiệm vụ (Lyle et al., 2023). Cụ thể, thuật toán học được đánh giá tại thời điểm t = τT, sau khi được cho T lần lặp để học trên nhiệm vụ τ. Một giả định hữu ích trong một số vấn đề là mỗi nhiệm vụ được lấy mẫu độc lập và giống hệt, và mỗi nhiệm vụ có độ khó như nhau, nghĩa là một mạng nơ-ron được khởi tạo ngẫu nhiên trên mỗi nhiệm vụ có thể đạt được giá trị mục tiêu tương tự cho bất kỳ nhiệm vụ cụ thể nào (Dohare et al., 2024; Elsayed và Mahmood, 2024; Lyle et al., 2023). Mặc dù giả định này hữu ích, nhưng nó không phải lúc nào cũng áp dụng được. Trong học tăng dần lớp (Van de Ven et al., 2022), việc thêm nhiều lớp hơn làm tăng độ khó của các nhiệm vụ tiếp theo. Một ví dụ khác là các nhiệm vụ tạm thời được tạo ra trong quá trình học tăng cường (Igl et al., 2021), có thể có độ khó nhiệm vụ khác nhau và dẫn đến mất tính dẻo dai do thiên vị ưu tiên (Nikishin et al., 2022). Chúng tôi cũng xem xét những môi trường như vậy ở đây.

Mất tính dẻo dai trong văn học học liên tục có thể đề cập đến mất khả năng huấn luyện (Dohare et al., 2021; Lyle et al., 2023) hoặc mất tổng quát hóa (Ash và Adams, 2020). Bởi vì khả năng huấn luyện là một yêu cầu cho việc học và tổng quát hóa, chúng tôi tập trung chủ yếu vào mất khả năng huấn luyện. Cụ thể, chúng tôi sử dụng mất khả năng huấn luyện để đề cập đến hiện tượng rằng giá trị mục tiêu, Jτ(θ(τT)), tăng như một hàm của nhiệm vụ τ. Tương đương, các thước đo hiệu suất, như độ chính xác, giảm với các nhiệm vụ mới. Dưới giả định rằng các nhiệm vụ được lấy mẫu độc lập và giống hệt, điều này sẽ gợi ý rằng khả năng huấn luyện của mạng nơ-ron giảm sút trên các nhiệm vụ mới.

3 TÍNH CHẤT PHỔ VÀ KHẢ NĂNG HUẤN LUYỆN LIÊN TỤC
Ở mức độ cao, lý do đằng sau mất tính dẻo dai là đơn giản: nghiệm của một nhiệm vụ trở thành khởi tạo cho việc học trên nhiệm vụ tiếp theo, và nếu khởi tạo này không đủ tốt để cho phép học trong nhiệm vụ mới, chúng ta phải đối mặt với vấn đề đã nêu ở trên. Để tránh vấn đề này, chúng ta cần giữ các tham số mạng trong một vùng có thể phục vụ như khởi tạo tốt ở đầu một nhiệm vụ. Cách tiếp cận này đi kèm với hai thách thức: (i) xác định một vùng phù hợp cho khởi tạo tham số trên bất kỳ nhiệm vụ nào được cho; (ii) đảm bảo rằng việc huấn luyện trong vùng khởi tạo không ngăn cản việc học hiệu quả trên nhiệm vụ hiện tại.

Dễ dàng thỏa mãn (i) và (ii) một cách riêng biệt: sử dụng thuật toán khởi tạo tiêu chuẩn, như những thuật toán của He et al. (2015); Hinton và Salakhutdinov (2006), và sau đó giữ các tham số cố định ở giá trị này thỏa mãn (i) nhưng rõ ràng không thỏa mãn (ii). Ngược lại, không hạn chế không gian tham số trong khi sử dụng thuật toán học tiêu chuẩn giải quyết (ii) nhưng không phải (i). Chúng tôi tìm kiếm sự cân bằng của hai yêu cầu. Lưu ý rằng (i) và (ii) không phải là tính chất nhị phân; do đó có thể có sự đánh đổi (ví dụ, tham số cho phép tối ưu hóa tốt hơn trên nhiệm vụ hiện tại có thể ít thuận lợi hơn như một khởi tạo tham số cho nhiệm vụ tiếp theo). Đáng chú ý rằng cách tiếp cận như vậy đã được áp dụng thành công trong bối cảnh tối ưu hóa lồi trực tuyến trong môi trường thay đổi, nơi một số thuật toán có thể được nhận ra là chạy một thuật toán học tiêu chuẩn (như mirror descent) trên một tập con được chọn cẩn thận của không gian tham số (ví dụ, György và Szepesvári, 2016; Herbster và Warmuth, 1998; Zinkevich, 2003).

Trong bài báo này, để giải quyết vấn đề (i), trước tiên chúng tôi xác định các tính chất chính mà các thuật toán khởi tạo (He et al., 2015; Hinton và Salakhutdinov, 2006) áp đặt lên các tham số bắt đầu của mạng nơ-ron. Sau đó, chúng tôi đề xuất chính quy hóa phổ để duy trì những tính chất này trong khi tạo ra sự cân bằng tốt giữa (i) và (ii).

3.1 TÍNH CHẤT PHỔ TẠI KHỞI TẠO
Khởi tạo mạng nơ-ron là chìa khóa cho khả năng huấn luyện (He et al., 2015; Hinton và Salakhutdinov, 2006). Một tính chất của khởi tạo được cho là quan trọng là ánh xạ theo lớp, hl+1 = ReLU(θlhl), có Jacobian với các giá trị đơn gần hoặc chính xác bằng một (Glorot và Bengio, 2010; Pennington et al., 2017; Saxe et al., 2014; Xiao et al., 2018). Viết Jacobian này một cách rõ ràng, chúng ta có Jl = ∂hl+1/∂hl = Dlθl nơi Dl = Diag(ReLU'([θlhl]1), . . . , ReLU'([θlhl]d)).² Chúng ta có thể thu được giới hạn trên và dưới cho các giá trị đơn của Jacobian theo lớp theo các giá trị đơn của ma trận trọng số. Ký hiệu các giá trị đơn được sắp xếp của θl và Dl bằng σd(θl) ≤ ··· ≤ σ1(θl) và σd(Dl) ≤ ··· ≤ σ1(Dl), tương ứng, chúng ta có σd(Dl)σi(θl) < σi(Jl) < σ1(Dl)σi(θl) cho tất cả i ∈ {1, . . . , d} (Zhang, 2011, Định lý 8.13). Cụ thể, nếu chuẩn phổ (giá trị đơn lớn nhất) của ma trận trọng số θl tăng, thì chuẩn phổ của Jacobian Dl cũng tăng, có khả năng ảnh hưởng đến khả năng huấn luyện. Hơn nữa, số điều kiện κ(Jl) = σ1(Jl)/σd(Jl) có thể được giới hạn bằng tích của các số điều kiện của θl và Dl, κ(θl) và κ(Dl) như κ(θl)/κ(Dl) ≤ κ(Jl) ≤ κ(θl)κ(Dl). Do đó, nếu mục tiêu của chúng ta là giữ các giá trị đơn của Jacobian gần với một bằng cách kiểm soát các giá trị đơn của ma trận trọng số, chúng ta nên đảm bảo rằng số điều kiện của ma trận sau không quá lớn.

3.2 MỘT VÍ DỤ MINH HỌA
Để làm rõ những điểm này, xem xét một mạng nơ-ron một lớp fθ(x) = θ2ReLU(θ1x) ánh xạ x ∈ R² đến R. Giả sử nhiệm vụ đầu tiên là khớp x = (1,0)⊤ và y = 0 trong lỗi bình phương trung bình. Một nghiệm tối ưu với fθ(x) = y là θ1 = [1 0; 0 a] và θ2 = (0, a) cho một giá trị nhỏ tùy ý a ≥ 0. Bây giờ xem xét nhiệm vụ tiếp theo là khớp x = (0,1)⊤ và y = 1. Các gradient của mất mát ℓ(fθ(x), y) = 1/2(y − ℓ(fθ(x))² là, cho cặp (x, y) mới được cho, ∇θ2ℓ(fθ(x), y) = (fθ(x) − y)ReLU(θ1x) = (a² − 1)(0, a)⊤ và ∇θ1ℓ(fθ(x), y) = (fθ(x) − y)xReLU'(θ1x)⊤ = (a² − 1)[0 0; 0 a]. Thực hiện cập nhật với những gradient này giữ các tham số Θ(a) ở cùng thứ tự trong khi duy trì mất mát khoảng 1. Số điều kiện của ma trận trọng số θ1 là κ = 1/a, yêu cầu Θ(1/a) = Θ(κ) bước cập nhật để đạt tiến bộ đáng kể trong quá trình học. (Một ví dụ khác cho thấy điều này một cách thực nghiệm được đưa ra trong Phụ lục A.1.)

3.3 KHẢ NĂNG HUẤN LUYỆN VÀ TÍNH ĐA DẠNG GRADIENT HIỆU QUẢ
Chúng ta có thể tổng quát hóa quan sát trên thành một vấn đề về tính đa dạng gradient giảm. Bằng tính đa dạng gradient, chúng tôi có nghĩa là sự lan rộng của các vector đơn trong ma trận của gradient ngẫu nhiên theo từng ví dụ, G = [g1, . . . , gm]. Nếu ma trận này chứa một vài giá trị đơn lớn và nhiều giá trị đơn nhỏ, thì các gradient sẽ chủ yếu nằm trong không gian của các vector đơn tương ứng với các giá trị đơn lớn. Cho phân tích này, chúng tôi tập trung vào hạng, là một thống kê tóm tắt cụ thể cho tập hợp các giá trị đơn đếm số lượng giá trị đơn khác không, rank(G) = |{i : σi(G) > 0}|. Tuy nhiên, cho mục đích thực tế, hạng có vấn đề vì nó không ổn định với nhiễu (Feng et al., 2022, Định lý 1). Trong các thí nghiệm dưới đây, chúng tôi sẽ xem xét số điều kiện, σ1(G)/σm(G) và hạng hiệu quả, erank(G) = ∑σ̄i(G) log σ̄i(G), nơi σ̄i(G) = σi(G)/∑iσi(G) (Roy và Vetterli, 2007). Những thước đo này làm rõ các vấn đề phát sinh nếu giá trị đơn lớn nhất tăng nhanh hơn các giá trị đơn nhỏ nhất (các thí nghiệm của chúng tôi cho thấy đây là trường hợp trong Phần 5.2). Trong trường hợp này, số điều kiện tăng và hạng hiệu quả giảm. Chúng tôi gọi đây là sự giảm trong tính đa dạng gradient hiệu quả.

Sự giảm trong hạng, hoặc erank, của ma trận gradient dẫn đến các gradient cùng phương trên các điểm dữ liệu khác nhau, hạn chế tính đa dạng của các hướng gradient được sử dụng trong cập nhật tham số. Điều này có thể có tác động bất lợi đến việc học. Trong trường hợp cực đoan, nơi ma trận gradient có hạng một, mỗi điểm dữ liệu cung cấp gradient theo cùng một hướng, ngay cả khi các điểm dữ liệu tương ứng với các lớp khác nhau. Xem xét gradient của mất mát trên một điểm dữ liệu cụ thể (xi, yi) đối với ma trận trọng số của một lớp ẩn, θl ∈ Rd×d, có thể được viết đệ quy cho các tham số của lớp l như Gl,i = ∇θlℓ(fθ(xi), yi) = δl,ih⊤l−1,i, nơi δl,i = θ⊤l+1δl+1,iDl,i là gradient lỗi từ lớp tiếp theo với δL,i = ∂ℓ(fθ(xi),yi)/∂fθ và Dl,i = Diag(ϕ'([θlhl,i]1), . . . , ϕ'([θlhl,i]d)). Chúng ta có thể viết lại gradient theo vector hóa của nó gl,i = vec(Gl,i) = (Id ⊗ θ⊤l+1)vl,i nơi Id là ma trận đơn vị d×d và v1,i = vec(δl+1,iDl,ih⊤l−1,i) ∈ Rd² là vector hóa của các thuật ngữ phụ thuộc dữ liệu. Ma trận gradient cho các điểm dữ liệu khác nhau là phép nối của các gradient theo từng ví dụ, Gl = [gl,1, gl,2, . . . , gl,m] = (Id ⊗ θ⊤l+1)Vl nơi Vl = [vl,1, vl,2, . . . , vl,m]. Hạng của Gl được giới hạn trên như

rank(Gl) < min{rank(Id ⊗ θ⊤l+1), rank(Vl)} = min{d rank(θ⊤l+1), rank(Vl)}.

Do đó, nếu hạng (hiệu quả) của θl+1 giảm, thì hạng (hiệu quả) của ma trận gradient cũng có thể giảm. Hạng của ma trận gradient thậm chí có thể giảm do giảm hạng trong các tham số ở các lớp khác, hoặc thông qua hạng của biểu diễn, mà những người khác đã chú ý đôi khi tương quan với mất khả năng huấn luyện (Dohare et al., 2024; Kumar et al., 2023; Lyle et al., 2023).

Tại Sao Các Tính Chất Phổ Lệch Khỏi Khởi Tạo? Trong học tự giám sát quy mô lớn, người ta đã quan sát thực nghiệm rằng chuẩn tham số tăng với tốc độ √t, nơi t là số lần lặp (Merrill et al., 2021). Những quan sát tương tự đã được thực hiện trong học liên tục, với điều kiện là mạng nơ-ron không ngừng học do mất khả năng huấn luyện (Dohare et al., 2024; Lyle et al., 2024; Nikishin et al., 2022). Một chuẩn tham số tăng có vấn đề từ góc độ tối ưu hóa. Cụ thể, chuẩn tham số mà những công trình này xem xét là chuẩn Frobenius, bằng tổng bình phương của các giá trị đơn, ∥θ∥²F = ∑i∑j[θ]²ij = ∑iσi(θ)². Do đó, một chuẩn tham số tăng tương đương với tổng bình phương của các giá trị đơn tăng. Cụ thể, lưu ý rằng ∥θl∥F ≤ √rank(θl)σ1(θl) nơi σ1(θl) biểu thị giá trị đơn lớn nhất, hoặc chuẩn phổ (Golub và Van Loan, 2013). Nếu chuẩn tham số của lớp l tăng với tốc độ √t, thì chuẩn phổ của ma trận tham số θl cũng tăng với cùng tốc độ. Điều này dẫn đến sự tăng trong chuẩn phổ của Jacobian theo lớp, σd(Dl)σ1(θl) ≤ σ1(Jl), có thể giảm tính đa dạng gradient hiệu quả và có thể gây hại cho khả năng huấn luyện.

4 CHÍNH QUY HÓA PHỔ CHO HỌC LIÊN TỤC
Nếu các tính chất quan trọng của khởi tạo bị mất trong quá trình học, việc chính quy hóa mạng nơ-ron về phía khởi tạo là tự nhiên. Đây là động lực cho chính quy hóa (Kumar et al., 2023) và tái khởi tạo trọng số (Dohare et al., 2024; Sokar et al., 2023) như các biện pháp giảm thiểu mất tính dẻo dai. Tuy nhiên, động lực của chúng tôi là nhắm mục tiêu trực tiếp hơn các tính chất chính của khởi tạo, sử dụng những hiểu biết từ Phần 3.

Chúng tôi ký hiệu một bộ chính quy hóa bằng Rτ(t)(θ, s), là một hàm của (i) các tham số của mạng nơ-ron, θ, (ii) dữ liệu, thông qua nhiệm vụ hiện tại τ(t), và của (iii) thông tin phụ trợ, như các tham số tại khởi tạo, thông qua biến trạng thái s. Tự nhiên, không phải mọi bộ chính quy hóa đều tính đến tất cả những yếu tố này. Bộ chính quy hóa được tối ưu hóa cùng với mục tiêu cơ sở, Jτ(t)(θ). Chúng tôi viết mục tiêu tổng hợp như Jλτ(t)(θ) = Jτ(t)(θ) + λRτ(t)(θ, s), nơi λ là một siêu tham số có thể điều chỉnh điều chỉnh cường độ chính quy hóa.

Trong Phần 3, chúng tôi lập luận rằng một chuẩn phổ và số điều kiện tăng có thể gây hại cho khả năng huấn luyện bằng cách giảm tính đa dạng gradient hiệu quả và sự tăng trưởng này xảy ra với tốc độ √t. Bây giờ chúng tôi điều tra chính quy hóa như một phương tiện kiểm soát chuẩn phổ để duy trì khả năng huấn luyện của mạng nơ-ron.

Một bộ chính quy hóa thường được sử dụng trong cả học liên tục và không liên tục là chính quy hóa L2, Rτ(t)(θ, ∅) = ∥θ∥², chính quy hóa các tham số về phía không. Một thay thế gần đây, chính quy hóa L2 về phía khởi tạo, Rτ(t)(θ, θ(0)) = ∥θ − θ(0)∥², được đề xuất để đối phó với độ nhạy cảm với các tham số gần không (Kumar et al., 2023). Chính quy hóa về phía các tham số cụ thể có mặt tại khởi tạo cho phép mạng nơ-ron tái tạo tham số, cung cấp một đặt lại mềm cho tham số nếu gradient trên mục tiêu cơ sở bằng không (xem Phụ lục A.2 cho một ví dụ và Phụ lục A.4 cho thêm chi tiết). Một vấn đề tiềm năng với chính quy hóa L2 về phía khởi tạo là nó có thể ngăn cản các tham số lệch khỏi giá trị được lấy mẫu cụ thể từ phân phối khởi tạo.

Bộ chính quy hóa phổ được đề xuất của chúng tôi chính quy hóa một cách rõ ràng chuẩn phổ của mỗi lớp (giá trị đơn lớn nhất) và giải quyết tính đa dạng gradient hiệu quả giảm được mô tả trong Phần 3.3. Chúng tôi chỉ tối thiểu hóa giá trị đơn lớn nhất vì giá trị đơn nhỏ nhất vẫn tương đối không đổi qua tất cả các kiến trúc chúng tôi xem xét (xem Phụ lục C.6). Ma trận tham số θl trong lớp l là một phép nối của ma trận trọng số Wl và độ lệch bl, cho ma trận tham số được mở rộng θl = [Wl|bl] và đầu vào được mở rộng hl = [hl, 1]. Với ký hiệu này, phương trình định nghĩa mạng nơ-ron fθ có thể được viết lại như hl+1(x) = ϕ(Wlhl(x) + bl) = ϕ(θlhl). Như được thảo luận trong Phần 3, mục tiêu của chúng ta là kiểm soát giá trị đơn lớn nhất của θl. Sử dụng thực tế rằng các tham số được nối có thể được giới hạn trên bằng tổng, σ1(θl) = σ1([Wl|bl]) ≤ σ1(Wl) + σ1(bl), chúng tôi đạt được điều này bằng cách chính quy hóa chuẩn phổ của mỗi tham số trong lớp một cách riêng biệt: chuẩn phổ của tham số trọng số nhân Wl được chính quy hóa về phía một và chuẩn phổ của tham số độ lệch cộng bl được chính quy hóa về phía không:

Rτ(t)(θl) = ∑l∈layers [(σ1(Wl)k − 1)² + σ1(bl)k²] = ∑l∈layers [(σ1(Wl)k − 1)² + ∥bl∥²k²].

Chúng tôi thêm số mũ k vào chuẩn phổ để phạt các chuẩn phổ lớn lệch khỏi một. Chúng tôi đặt k = 2 trong các thí nghiệm của chúng tôi và cung cấp một nghiên cứu loại bỏ trong Phụ lục C.1. Để biết chi tiết triển khai cho các lớp khác, như lớp chuẩn hóa và lớp tích chập, xem Phụ lục A.7.

--- TRANG 5 ---
Giá trị đơn lớn nhất có thể được tính hiệu quả bằng cách sử dụng lặp lũy thừa (Golub và Van der Vorst, 2000; Householder, 2013). Tương tự như công trình trước đây sử dụng chính quy hóa phổ cho tổng quát hóa trong học có giám sát, chúng tôi thấy rằng một lần lặp duy nhất là đủ cho chính quy hóa hiệu quả (Yoshida và Miyato, 2017).

Ngoài số mũ k, cách tiếp cận chính quy hóa phổ của chúng tôi khác biệt theo hai cách quan trọng so với công trình trước đây, như chính quy hóa chuẩn phổ (Yoshida và Miyato, 2017). Thứ nhất, chúng tôi chính quy hóa chuẩn phổ của mọi tham số, bao gồm các thuật ngữ độ lệch và tham số chuẩn hóa (xem Phụ lục A.7). Điều này được yêu cầu vì mọi tham số đều trải qua tăng trưởng chuẩn. Thứ hai, chúng tôi chính quy hóa chuẩn phổ của các tham số nhân về phía một thay vì không. Chính quy hóa tích cực của chuẩn phổ về phía không có thể dẫn đến các vấn đề sụp đổ tương tự như chính quy hóa L2 về phía không. Hai sự khác biệt này là rất quan trọng để cải thiện khả năng huấn luyện trong học liên tục. Cuối cùng, chính quy hóa phổ được ưa thích hơn chuẩn hóa phổ, cái mà chính quy hóa rõ ràng các trọng số trong lượt truyền thuận để chính xác bằng một, vì hai lý do: (i) chuẩn hóa phổ phụ thuộc vào dữ liệu (xem Phần 2.1 và Phương trình 12 của Miyato et al. (2018), và Phụ lục A.6), có thể có vấn đề trong học liên tục do phân phối dữ liệu thay đổi, và, (ii) các hình thức chuẩn hóa khác đã được sử dụng để huấn luyện mạng nơ-ron sâu, như LayerNorm (Ba et al., 2016), và đã được chỉ ra rằng chuẩn hóa phổ không cải thiện khả năng huấn luyện (Lyle et al., 2023). Xem Phụ lục A.5 để biết thêm chi tiết.

5 THÍ NGHIỆM
Mục tiêu của các thí nghiệm của chúng tôi là điều tra tác động của chính quy hóa phổ đối với khả năng huấn luyện trong học có giám sát liên tục, cũng như học tăng cường. Chúng tôi bao gồm một số tập dữ liệu khác nhau, loại tính không cố định, và kiến trúc. Chúng tôi so sánh các bộ chính quy hóa được đề xuất với các đường cơ sở đã được chỉ ra là cải thiện khả năng huấn luyện trong công trình trước đây, mà chúng tôi chi tiết dưới đây. Nhìn chung, các thí nghiệm của chúng tôi chứng minh rằng chính quy hóa phổ (i) liên tục giảm thiểu mất khả năng huấn luyện trên nhiều loại vấn đề học có giám sát liên tục, bao gồm huấn luyện mạng nơ-ron lớn trong hàng nghìn epoch qua một trăm nhiệm vụ, (ii) rất mạnh mẽ đối với cường độ chính quy hóa, loại tính không cố định và số epoch huấn luyện mỗi nhiệm vụ, (iii) đạt được hiệu suất tổng quát hóa tốt hơn trong quá trình học liên tục, và (iv) có thể áp dụng chung, mà chúng tôi chứng minh bằng cách áp dụng chính quy hóa phổ cho học tăng cường với các hành động liên tục.

Tập Dữ Liệu, Tính Không Cố Định, và Kiến Trúc Kết quả chính của chúng tôi sử dụng tất cả các tập dữ liệu phân loại hình ảnh thường được sử dụng cho học có giám sát liên tục: tiny-ImageNet (Le và Yang, 2015), CIFAR10, và CIFAR100 (Krizhevsky, 2009). Các thí nghiệm trong phụ lục cũng sử dụng các tập dữ liệu quy mô nhỏ hơn, như MNIST (LeCun et al., 1998), Fashion MNIST (Xiao et al., 2017), EMNIST (Cohen et al., 2017), và SVHN2 (Netzer et al., 2011). Ngoài tập dữ liệu, chúng tôi xem xét các loại tính không cố định khác nhau: (i) gán nhãn ngẫu nhiên, (ii) hoán vị pixel, (iii) lật nhãn, và (iv) học tăng dần lớp. Gán nhãn ngẫu nhiên thường được sử dụng để đánh giá khả năng huấn luyện (Lyle et al., 2023) do sự thay đổi phân phối lớn giữa các nhiệm vụ trong việc ghi nhớ các nhãn hoàn toàn mới và ngẫu nhiên. Mặt khác, hoán vị pixel chỉ yêu cầu học mặt nạ hoán vị được áp dụng cho hình ảnh; nó gây ra mất khả năng huấn luyện chậm hơn nhưng hữu ích để đánh giá tổng quát hóa (Kumar et al., 2023). Lật nhãn là việc gán lại tất cả các quan sát từ một nhãn sang nhãn khác. Không giống như hai tính không cố định kia, sự thay đổi phân phối lật nhãn không yêu cầu học một biểu diễn mới. Điều này là do chỉ lớp đầu ra của mạng nơ-ron cần được hoán vị để học việc gán lại nhãn, nhưng gradient vẫn có thể thay đổi biểu diễn một cách không cần thiết dẫn đến mất khả năng huấn luyện (Elsayed và Mahmood, 2024). Chúng tôi cũng xem xét môi trường tăng dần lớp trong đó mạng được huấn luyện trên một tập con tăng dần của các lớp từ một tập dữ liệu, bắt đầu chỉ với năm lớp trong nhiệm vụ đầu tiên và giới thiệu năm lớp mới trong các nhiệm vụ mới. Chúng tôi sử dụng cả ResNet-18 (He et al., 2016) và Vision Transformer (Dosovitskiy et al., 2021).

Các Biện Pháp Giảm Thiểu Mất Khả Năng Huấn Luyện Trong kết quả chính của chúng tôi, chúng tôi so sánh chính quy hóa phổ với chính quy hóa L2 về phía không, thu nhỏ và nhiễu (Ash và Adams, 2020), chính quy hóa L2 về phía khởi tạo (Kumar et al., 2023), tái chế nơ-ron không hoạt động (ReDO, Sokar et al., 2023), ReLU nối (Abbas et al., 2023; Shang et al., 2016), và chính quy hóa Wasserstein (Lewandowski et al., 2023). Một số bộ chính quy hóa trong văn học học liên tục mà không quên dựa vào thông tin nhiệm vụ đặc quyền, không áp dụng cho môi trường không phụ thuộc nhiệm vụ mà chúng tôi xem xét. Chúng tôi sử dụng chuyển đổi streaming (Elsayed và Mahmood, 2024) để biến đổi elastic weight consolidation (Kirkpatrick et al., 2017; Zenke et al., 2017), để nó không còn yêu cầu thông tin ranh giới nhiệm vụ, và bao gồm nó như một đường cơ sở. Chi tiết thí nghiệm bổ sung có thể được tìm thấy trong Phụ lục B.

--- TRANG 6 ---
Hình 1: Tổng quát hóa qua các loại tính không cố định khác nhau trên tiny-ImageNet sử dụng ResNet (trên) hoặc Vision Transformer (dưới). So với các đường cơ sở, chính quy hóa phổ liên tục nằm trong số các phương pháp có hiệu suất tốt nhất qua học tăng dần lớp, lật nhãn, và tính không cố định hoán vị pixel. Lưu ý rằng Vision Transformer thường đạt được hiệu suất tổng quát hóa tốt hơn kiến trúc ResNet.

5.1 ĐÁNH GIÁ SO SÁNH
Trong Hình 1, chúng tôi vẽ kết quả huấn luyện ResNet-18 và Vision Transformer trên tiny-ImageNet với các tính không cố định khác nhau. Qua các mạng, tính không cố định, và phương pháp được xem xét, chúng tôi thấy rằng chính quy hóa phổ nằm trong số các phương pháp có khả năng duy trì tính dẻo dai tốt nhất. Lợi thế của cách tiếp cận của chúng tôi, chính quy hóa phổ, đặc biệt cao trong học tăng dần lớp, nhưng hiệu suất của chính quy hóa phổ với Vision Transformer cũng là tốt nhất trong lật nhãn và hoán vị pixel. Ngược lại, hiệu suất của các đường cơ sở khác rất khác nhau đối với loại tính không cố định cụ thể. Ví dụ, thu nhỏ và nhiễu không mạnh mẽ đối với các môi trường được xem xét. Đôi khi, nó nằm trong số các phương pháp có hiệu suất tốt nhất hoặc tệ nhất.

Kết quả tương tự trên các tập dữ liệu khác và tính không cố định hoán vị pixel có thể được thấy trong Hình 2.

Hình 2: Học liên tục với các nhiệm vụ hoán vị pixel trên SVHN2, CIFAR10, CIFAR100 sử dụng ResNet-18 (trên) hoặc Vision Transformer (dưới). Qua các tập dữ liệu khác nhau, chính quy hóa phổ hiệu quả trong việc duy trì độ chính xác kiểm tra trên các nhiệm vụ mới. Không có bất kỳ biện pháp giảm thiểu nào, cả ResNet-18 và Vision Transformer đều có độ chính xác kiểm tra giảm dần, gợi ý mất tính dẻo dai.

--- TRANG 7 ---
Hình 3: Khả năng huấn luyện và tính chất mạng nơ-ron qua ImageNet, CIFAR10, và CIFAR100. Các đường cơ sở bị mất khả năng huấn luyện (trên) cũng có chuẩn phổ trung bình tăng (giữa-trên), và giảm thay đổi biểu diễn trung bình (giữa-dưới).

5.2 NHÌN VÀO BÊN TRONG MẠNG
Bây giờ chúng tôi khám phá cách các tính chất cấu trúc của mạng nơ-ron phát triển trong quá trình học liên tục, các tính chất này bị ảnh hưởng như thế nào bởi chính quy hóa phổ, và các đường cơ sở khác nhau được xem xét. Cho điều này, chúng tôi xem xét vấn đề học liên tục trong đó ResNet-18 phải ghi nhớ một tập nhãn ngẫu nhiên thay đổi từ nhiệm vụ này sang nhiệm vụ khác sử dụng các tập dữ liệu tiny-ImageNet, CIFAR10, và CIFAR100. Chúng tôi xem xét thay đổi biểu diễn trung bình để đo khoảng cách giữa các kích hoạt ẩn của mạng nơ-ron từ đầu một nhiệm vụ, đến đầu nhiệm vụ tiếp theo. Thay đổi biểu diễn trung bình là một đại diện cho tính dẻo dai, cho phép chúng ta thấy hành vi của mạng nơ-ron đã thay đổi bao nhiêu. Trong Hình 3, chúng ta thấy rằng các mạng ReLU không được chính quy hóa bị mất khả năng huấn luyện trong tất cả các vấn đề được xem xét, và điều này trùng với chuẩn phổ tăng (giữa-trên) và giảm thay đổi biểu diễn trung bình (dưới). Mặc dù chỉ có chính quy hóa phổ trực tiếp chính quy hóa chuẩn phổ của mạng, các bộ chính quy hóa khác làm như vậy gián tiếp bằng cách kiểm soát các chuẩn khác, như L2. Tuy nhiên, những bộ chính quy hóa khác này cũng chính quy hóa các tham số khác, ngăn cản chúng lệch khỏi khởi tạo và có khả năng dẫn đến việc sử dụng dung lượng không tối ưu, có thể được quan sát trong hàng dưới cùng trong Hình 3. Trong Phụ lục C.3, chúng tôi cho thấy rằng Vision Transformer không thể ghi nhớ nhãn ngẫu nhiên, gợi ý rằng khả năng tổng quát hóa của nó được bù trừ bởi khả năng huấn luyện tương đối thấp hơn.

--- TRANG 8 ---
5.3 PHÂN TÍCH ĐỘ NHẠY CẢM
Hình 4: Phân tích độ nhạy cảm về cường độ chính quy hóa. So với các bộ chính quy hóa khác, chính quy hóa phổ không nhạy cảm với cường độ chính quy hóa trong khi duy trì khả năng huấn luyện cao hơn cho bất kỳ cường độ chính quy hóa nào được cho. Hiệu quả của một bộ chính quy hóa phụ thuộc vào cường độ chính quy hóa. Quá nhiều chính quy hóa có thể làm chậm huấn luyện, dẫn đến hiệu suất không tối ưu trên bất kỳ nhiệm vụ nào được cho. Tuy nhiên, quá ít chính quy hóa có thể dẫn đến mất khả năng huấn luyện và hiệu suất không tối ưu trên các nhiệm vụ tương lai. Trong một vấn đề học liên tục, việc tinh chỉnh siêu tham số đặc biệt tốn kém (hoặc không khả thi), đặc biệt nếu phân phối trên các nhiệm vụ không được biết trước (Mesbahi et al., 2024). Do đó, chúng ta nên ưu tiên các bộ chính quy hóa ít nhạy cảm với siêu tham số. Kết quả của chúng tôi trong Hình 10 cho thấy độ nhạy cảm của chính quy hóa phổ đối với siêu tham số của nó thấp hơn nhiều so với các bộ chính quy hóa khác. Trong Phụ lục C.4, chúng tôi cũng trình bày kết quả bổ sung cho thấy tính mạnh mẽ của chính quy hóa phổ khi thay đổi (i) cường độ chính quy hóa sử dụng các kiến trúc mạng khác, (ii) loại tính không cố định, và (iii) số epoch huấn luyện mỗi nhiệm vụ.

Hình 5: Chính quy hóa phổ tăng cường tính dẻo dai trong học tăng cường trong bộ DMC. Chính quy hóa phổ cạnh tranh với đặt lại mạng + layernorm (Reset) ngay cả khi bộ đệm phát lại không giới hạn (Trên Trái). Khi kích thước bộ đệm phát lại bị giới hạn ở 250k bước, chính quy hóa phổ cải thiện so với đường cơ sở Reset (Dưới Trái). Trong cả hai trường hợp, chính quy hóa phổ vượt trội đáng kể so với layernorm (Baseline), So với siêu tham số điều chỉnh tần số đặt lại, chính quy hóa phổ ít nhạy cảm với cường độ chính quy hóa của nó. Chính quy hóa phổ cũng ngăn cản cả tham số và gradient bùng nổ, và giảm đánh giá quá cao giá trị (Phải).

5.4 TỪ HỌC CÓ GIÁM SÁT ĐẾN HỌC TĂNG CƯỜNG
Ngoài học có giám sát, chúng tôi đánh giá chính quy hóa phổ trong học tăng cường. Cụ thể, chúng tôi điều tra các nhiệm vụ điều khiển từ benchmark DMC (Tassa et al., 2020), với soft actor-critic (SAC, Haarnoja et al., 2018), trong chế độ tỷ lệ phát lại cao (RR) (D'Oro et al., 2022) với 16 cập nhật gradient cho mỗi bước môi trường. Chế độ tỷ lệ phát lại cao dẫn đến thiên vị ưu tiên (Nikishin et al., 2022), một hiện tượng liên quan đến mất tính dẻo dai. Công trình gần đây đã chứng minh rằng chỉ có việc đặt lại mạng đầy đủ với chuẩn hóa lớp (Ba et al., 2016) phục vụ như một chiến lược giảm thiểu hiệu quả trong môi trường này (Ball et al., 2023; Nauman et al., 2024).

--- TRANG 9 ---
Như được hiển thị trong Hình 5, chính quy hóa phổ mang lại lợi nhuận cao hơn đáng kể so với đường cơ sở sử dụng chuẩn hóa lớp với kích thước bộ đệm phát lại không giới hạn hoặc kích thước bộ đệm phát lại hạn chế. Tổng hợp lại, chính quy hóa phổ cạnh tranh với hiệu suất cuối cùng của đường cơ sở mạnh nhất, đặt lại mạng đầy đủ kết hợp với chuẩn hóa lớp. Tuy nhiên, trong năm trong số bảy môi trường, chính quy hóa phổ cải thiện đáng kể so với đường cơ sở đặt lại về hiệu quả mẫu (xem Phụ lục C.8). Chính quy hóa phổ cũng ít nhạy cảm với cường độ chính quy hóa của nó hơn so với đặt lại cho tần số đặt lại của chúng (Hình 5, giữa-trên và giữa-dưới). Hơn nữa, chúng tôi đã sử dụng một cường độ chính quy hóa duy nhất cho mọi mạng. Hiệu suất tốt hơn có thể đạt được bằng cách tinh chỉnh riêng lẻ mỗi cường độ chính quy hóa cho các mạng giá trị và chính sách. Ngoài ra, sự kết hợp của chuẩn hóa phổ và chuẩn norm lớp tích cực giảm đánh giá quá cao giá trị Q (Hình 5, trên-trái), là một trong những đại diện được thiết lập tốt cho sự mất ổn định huấn luyện RL (Hasselt, 2010; Nauman et al., 2024). Cuối cùng, chuẩn hóa phổ ngăn cản gradient bùng nổ và giữ chuẩn trọng số nhỏ trong quá trình huấn luyện, là một thành phần chính để học liên tục và bảo tồn tính dẻo dai của mạng nơ-ron (Lyle et al., 2022; 2023).

6 KẾT LUẬN
Trong bài báo này, chúng tôi đã điều tra mối liên hệ giữa khởi tạo, khả năng huấn luyện, và chính quy hóa. Chúng tôi xác định rằng sự lệch của giá trị đơn lớn nhất của mỗi lớp có thể dẫn đến tính đa dạng gradient thấp, ngăn cản mạng nơ-ron huấn luyện trên các nhiệm vụ mới. Để trực tiếp bảo tồn các tính chất khả năng huấn luyện có mặt tại khởi tạo, chúng tôi đề xuất chính quy hóa phổ như một cách kiểm soát giá trị đơn lớn nhất của mỗi lớp để nó không lệch đáng kể khỏi 1. Các thí nghiệm của chúng tôi cho thấy chính quy hóa phổ có hiệu suất tốt hơn và ít nhạy cảm với siêu tham số hơn các phương pháp khác qua các tập dữ liệu, tính không cố định, và kiến trúc. Trong khi một số phương pháp giảm thiểu mất khả năng huấn luyện ở mức độ khác nhau, học liên tục với chính quy hóa phổ là mạnh mẽ, đạt được hiệu suất ban đầu cao và bền vững. Chúng tôi cũng cho thấy rằng chính quy hóa phổ có khả năng cải thiện tổng quát hóa với cả Vision Transformer và ResNet-18 trên các phiên bản liên tục của tiny-ImageNet, CIFAR10, CIFAR100, trong số những cái khác. Cuối cùng, chúng tôi cho thấy rằng chính quy hóa phổ học được một chính sách hiệu quả hơn trong học tăng cường bằng cách tránh mất tính dẻo dai sớm do thiên vị ưu tiên.

TÀI LIỆU THAM KHẢO
Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, và Marlos C. Machado. Loss of plasticity in continual deep reinforcement learning. Trong Conference on Lifelong Learning Agents, 2023.

Sanjeev Arora, Nadav Cohen, Wei Hu, và Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 2019.

Jordan T. Ash và Ryan P. Adams. On Warm-Starting Neural Network Training. Trong Advances in Neural Information Processing Systems, 2020.

Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450v1, 2016.

Philip J. Ball, Laura Smith, Ilya Kostrikov, và Sergey Levine. Efficient Online Reinforcement Learning with Offline Data, 2023.

Martin Benning và Martin Burger. Modern regularization methods for inverse problems. Acta numerica, 27:1–111, 2018.

Tudor Berariu, Wojciech Czarnecki, Soham De, Jorg Bornschein, Samuel Smith, Razvan Pascanu, và Claudia Clopath. A study on the plasticity of neural networks. CoRR, abs/2106.00042, 2021.

Gregory Cohen, Saeed Afshar, Jonathan Tapson, và Andre Van Schaik. Emnist: Extending mnist to handwritten letters. Trong International Joint Conference on Neural Networks (IJCNN), 2017.

Shibhansh Dohare, Richard S. Sutton, và A. Rupam Mahmood. Continual backprop: Stochastic gradient descent with persistent randomness. CoRR, abs/2108.06325v3, 2021.

--- TRANG 10 ---
Shibhansh Dohare, J. Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, A. Rupam Mahmood, và Richard S. Sutton. Loss of plasticity in deep continual learning. Nature, 632(8026): 768–774, 2024.

Pierluca D'Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G. Bellemare, và Aaron Courville. Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier. Trong International Conference on Learning Representations, 2022.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong International Conference on Learning Representations, 2021.

Mohamed Elsayed và A. Rupam Mahmood. Addressing loss of plasticity and catastrophic forgetting in continual learning. Trong International Conference on Learning Representations, 2024.

Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, và Zheng-Jun Zha. Rank diminishing in deep neural networks. Advances in Neural Information Processing Systems, 2022.

Tomer Galanti, Zachary S. Siegel, Aparna Gupte, và Tomaso Poggio. Characterizing the Implicit Bias of Regularized SGD in Rank Minimization. CoRR, abs/2206.05794v6, 2022.

Xavier Glorot và Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. Trong International Conference on Artificial Intelligence and Statistics, 2010.

Gene H Golub và Henk A Van der Vorst. Eigenvalue computation in the 20th century. Journal of Computational and Applied Mathematics, 123(1-2):35–65, 2000.

Gene H Golub và Charles F Van Loan. Matrix computations. JHU press, 2013.

András György và Csaba Szepesvári. Shifting regret, mirror descent, and matrices. Trong Proceedings of The 33rd International Conference on Machine Learning, trang 2943–2951, 2016.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, và Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, 2018.

Benjamin Han, Hyungjun Lee, và Sébastien Martin. Real-time rideshare driver supply values using online reinforcement learning. Trong ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022.

Hado Hasselt. Double Q-learning. Trong Advances in Neural Information Processing Systems, 2010.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Trong International Conference on Computer Vision, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Conference on Computer Vision and Pattern Recognition, 2016.

M. Herbster và M. K. Warmuth. Tracking the best expert. Machine Learning, 32(2):151–178, 1998.

Geoffrey E Hinton và Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, và Laurent Sifre. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems, 2022.

Alston S Householder. The theory of matrices in numerical analysis. Courier Corporation, 2013.

--- TRANG 11 ---
Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, và Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. Trong International Conference on Learning Representations, 2021.

Sergey Ioffe và Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Trong International Conference on Machine Learning, 2015.

Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. Trong International Conference on Learning Representations, 2023.

Muhammad Kamran Janjua, Haseeb Shah, Martha White, Erfan Miahi, Marlos C Machado, và Adam White. Gvfs in the real world: making predictions online for water treatment. Machine Learning, 2023.

Hyunjik Kim, George Papamakarios, và Andriy Mnih. The lipschitz constant of self-attention. Trong International Conference on Machine Learning, 2021.

Diederik P. Kingma và Jimmy Ba. Adam: A Method for Stochastic Optimization. Trong International Conference on Learning Representations, 2015.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, và cộng sự. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Báo cáo kỹ thuật, Đại học Toronto, 2009.

Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, và Sergey Levine. Implicit under-parameterization inhibits data-efficient deep reinforcement learning. Trong International Conference on Learning Representations, 2021.

Saurabh Kumar, Henrik Marklund, và Benjamin Van Roy. Maintaining plasticity via regenerative regularization. CoRR, abs/2308.11958v1, 2023.

Ya Le và Xuan Yang. Tiny imagenet visual recognition challenge. 2015.

Yann LeCun, Corinna Cortes, và CJ Burges. MNIST handwritten digit database. ATT Labs [Trực tuyến]. Có sẵn: http://yann.lecun.com/exdb/mnist, 1998.

Alex Lewandowski, Haruto Tanaka, Dale Schuurmans, và Marlos C. Machado. Directions of Curvature as an Explanation for Loss of Plasticity. CoRR, abs/2312.00246v2, 2023.

Clare Lyle, Mark Rowland, và Will Dabney. Understanding and preventing capacity loss in reinforcement learning. Trong International Conference on Learning Representations, 2022.

Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, và Will Dabney. Understanding plasticity in neural networks. Trong International Conference on Machine Learning, 2023.

Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan Pascanu, James Martens, và Will Dabney. Disentangling the Causes of Plasticity Loss in Neural Networks. CoRR, abs/2402.18762v1, 2024.

Charles H Martin và Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165):1–73, 2021.

William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, và Noah A Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. Trong Conference on Empirical Methods in Natural Language Processing, 2021.

Golnaz Mesbahi, Olya Mastikhina, Parham Mohammad Panahi, Martha White, và Adam White. Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL. CoRR, abs/2404.02113v2, 2024.

--- TRANG 12 ---
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, và Yuichi Yoshida. Spectral normalization for generative adversarial networks. Trong International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1QRgziT-.

Michal Nauman, Michał Bortkiewicz, Piotr Miłoś, Tomasz Trzciński, Mateusz Ostaszewski, và Marek Cygan. Overestimation, Overfitting, and Plasticity in Actor-Critic: The Bitter Lesson of Reinforcement Learning, 2024.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, và cộng sự. Reading digits in natural images with unsupervised feature learning. Trong NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.

Evgenii Nikishin, Max Schwarzer, Pierluca D'Oro, Pierre-Luc Bacon, và Aaron Courville. The primacy bias in deep reinforcement learning. Trong International Conference on Machine Learning, 2022.

Jeffrey Pennington, Samuel Schoenholz, và Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. Advances in Neural Information Processing Systems, 2017.

Jeffrey Pennington, Samuel Schoenholz, và Surya Ganguli. The emergence of spectral universality in deep networks. Trong International Conference on Artificial Intelligence and Statistics, 2018.

Mark Bishop Ring. Continual learning in reinforcement environments. The University of Texas at Austin, 1994.

Olivier Roy và Martin Vetterli. The effective rank: A measure of effective dimensionality. Trong European Signal Processing Conference. IEEE, 2007.

A Saxe, J McClelland, và S Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. Trong International Conference on Learning Represenatations, 2014.

Wenling Shang, Kihyuk Sohn, Diogo Almeida, và Honglak Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. Trong international conference on machine learning, trang 2217–2225. PMLR, 2016.

Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, và Utku Evci. The dormant neuron phenomenon in deep reinforcement learning. Trong International Conference on Machine Learning, 2023.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, và Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199v4, 2013.

Yuval Tassa, Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Piotr Trochim, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, và Nicolas Heess. Dm_control: Software and Tasks for Continuous Control. Software Impacts, 6:100022, 2020.

Sebastian Thrun. Lifelong learning algorithms. Trong Learning to Learn, trang 181–209. Springer, 1998.

Nadav Timor, Gal Vardi, và Ohad Shamir. Implicit regularization towards rank minimization in relu networks. Trong International Conference on Algorithmic Learning Theory, 2023.

Yusuke Tsuzuku, Issei Sato, và Masashi Sugiyama. Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks. Advances in Neural Information Processing Systems, 2018.

Gido M Van de Ven, Tinne Tuytelaars, và Andreas S. Tolias. Three types of incremental learning. Nature Machine Intelligence, 4(12):1185–1197, 2022.

Jingfeng Wu, Difan Zou, Vladimir Braverman, và Quanquan Gu. Direction matters: On the implicit bias of stochastic gradient descent with moderate learning rate. Trong International Conference on Learning Representations, 2021.

Han Xiao, Kashif Rasul, và Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017.

--- TRANG 13 ---
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, và Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. Trong International Conference on Machine Learning, 2018.

Yuichi Yoshida và Takeru Miyato. Spectral Norm Regularization for Improving the Generalizability of Deep Learning. CoRR, abs/1705.10941v1, 2017.

Friedemann Zenke, Ben Poole, và Surya Ganguli. Continual learning through synaptic intelligence. Trong International Conference on Machine Learning, 2017.

Fuzhen Zhang. Matrix theory: basic results and techniques. Springer Science & Business Media, 2011.

Julian Zilly, Alessandro Achille, Andrea Censi, và Emilio Frazzoli. On plasticity, invariance, and mutually frozen weights in sequential task learning. Advances in Neural Information Processing Systems, 2021.

M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. Trong Proc. 20th International Conference on Machine Learning (ICML-2003), Washington, DC, 2003.

14

--- TRANG 14 ---
Bản thảo
A CHI TIẾT BỔ SUNG
A.1 VÍ DỤ MINH HỌA: CHUẨN PHỔ LỚN CÓ THỂ CẢN TRỞ KHẢ NĂNG HUẤN LUYỆN
Chúng tôi cho thấy rằng, cho một họ nghiệm trên nhiệm vụ đầu tiên, nên ưu tiên những nghiệm có chuẩn phổ gần với một hơn. Chúng tôi xem xét đầu vào nhị phân và trực giao 2 chiều, x1 = [0,1]⊤ và x2 = [1,0]⊤, với mục tiêu nhị phân tương ứng, y1 = y2 = 1. Chúng tôi xem xét mất mát bình phương, và sử dụng perceptron đa lớp với một lớp ẩn và hai đơn vị, fw2,w1(x) = w2ReLU(w1x). Một quan sát phổ biến về nghiệm được tìm thấy cho mạng nơ-ron sâu là chúng có hạng thấp (Arora et al., 2019; Galanti et al., 2022; Jacot, 2023; Timor et al., 2023). Ngoài ra, đã được quan sát rằng các ngoại lệ lớn trong phổ của mạng sâu xuất hiện trong quá trình huấn luyện, nghĩa là các giá trị đơn lớn nhất của ma trận trọng số có xu hướng lớn hơn nhiều so với các giá trị đơn nhỏ nhất của chúng (Martin và Mahoney, 2021; Pennington et al., 2018). Xem xét một nghiệm như vậy nơi w1 = [-1 c; 1/c -1], và w2 = [1/c, c] ∈ R1×2. Hàm đầu vào-đầu ra không thay đổi dưới các giá trị c khác nhau, đạt được lỗi bằng không cho bất kỳ lựa chọn cụ thể nào. Tuy nhiên, ma trận tham số cho các kích hoạt ẩn, w1, có hạng thấp vì nó chỉ có một giá trị đơn khác không duy nhất không bất biến. Đó là, giá trị đơn lớn nhất của w1 phụ thuộc vào lựa chọn c, ∥w1∥2 := σmax(w1) = c + 1/c. Ngoài ra, các gradient và cập nhật tham số phụ thuộc vào c. Do đó, nếu mục tiêu thay đổi, y'1 = 0, sự mất cân bằng của các giá trị đơn do giá trị c rất lớn hoặc nhỏ có thể cản trở khả năng huấn luyện:

w'1(x1) = [-1 c; 1/c -1] - α[0 1/c; 0 0]|{z}∇w1ℓ1, w'2(x1) = [1/c, c] - α[c,0]|{z}∇w2ℓ1, (1)

nơi ℓ1 = ℓ(fw2,w1(x1), y'1), và α là kích thước bước. Một trong những hậu quả của các giá trị đơn mất cân bằng là các gradient theo từng ví dụ có điều kiện kém (Wu et al., 2021). Đó là, khi chúng ta lấy mẫu x1, sẽ có gradient lớn cho w2, ∥∇w2ℓ(fw2,w1(x1), y'1)∥2 = c nhưng gradient nhỏ cho w1, ∥∇w1ℓ(fw2,w1(x1), y'1)∥2 = 1/c. Do đó, kích thước bước đủ nhỏ để ổn định cập nhật cho w2 dẫn đến học chậm trên w1. Kết quả tương tự xảy ra nếu mục tiêu y2 thay đổi

Trong Hình 6, chúng tôi chứng minh rằng việc học ổn định hơn với chuẩn phổ nhỏ hơn trong ví dụ này. Đối với ma trận trọng số có chiều cao hơn, chúng ta có thể mong đợi rằng sự mất cân bằng của các giá trị đơn dẫn đến tính đa dạng gradient thấp nơi các gradient theo từng ví dụ cụ thể tạo ra các ngoại lệ với độ lớn gradient lớn. Do đó, tất cả những thứ khác bằng nhau, một nghiệm với chuẩn phổ nhỏ hơn (c ≈ 1) được ưa thích cho mục đích học liên tục.

Hình 6: Ví dụ Minh họa. Chuẩn phổ thấp hơn có khả năng huấn luyện tốt hơn qua các kích thước bước khác nhau trong ví dụ minh họa đơn giản.

A.2 SỰ KHÁC BIỆT GIỮA HỌC LIÊN TỤC VÀ KHÔNG LIÊN TỤC
Một sự khác biệt quan trọng giữa học liên tục và không liên tục là, trong học liên tục, hội tụ về một điểm cố định không phải là mục tiêu. Do những thay đổi liên tục trong phân phối dữ liệu, không có điểm cố định nào tối ưu cho mọi phân phối nói chung. Do đó, hội tụ về một điểm cố định nhất thiết là không tối ưu. Nếu hội tụ về một điểm cố định xảy ra, thì chuẩn của các gradient sẽ bằng không. Các gradient hội tụ về không là một điều kiện cho mất tính dẻo dai, và có thể được thỏa mãn trên mọi phân phối tùy thuộc vào lựa chọn hàm kích hoạt. Tập trung vào trường hợp khi hàm kích hoạt là ReLU(x) = max(x,0), các gradient có thể trở thành không nếu 1) các trọng số hội tụ về không hoặc, 2) các kích hoạt hội tụ về không. Như một ví dụ chạy, chúng tôi sẽ xem xét mạng nơ-ron hai lớp không có độ lệch được ký hiệu bằng fθ(x) = θ2ϕ(θ1x), nơi θ1 ∈ Rd×din và θ2 ∈ Rdout×d là ma trận trọng số, và ϕ là hàm kích hoạt theo từng phần tử. Chúng tôi sẽ xem xét mất mát bình phương, J(θ) = 1/2E(x,y)∼p[(y − fθ(x))²], nhưng kết quả tương tự xảy ra với mất mát phân loại, như cross-entropy. Gradient cho mỗi tham số được cho bằng, ∇θ2J(θ) = (y − f)ϕ(θ1x)⊺ và ∇θ1J(θ) = θ⊺2(f − y)ϕ'(θ1x)x⊺. Mạng nơ-ron sẽ đạt điểm cố định trong học liên tục, nghĩa là các gradient bằng không cho mọi điểm dữ liệu, nếu hoặc 1) cả θ1 và θ2 đều bằng không, hoặc 2) các kích hoạt bằng không cho mọi đầu vào x, ϕ(θ1x) = 0,

A.3 CHUẨN PHỔ VÀ HẰNG SỐ LIPSCHITZ
Một lý do tiềm năng khác tại sao chuẩn phổ tăng có vấn đề là từ lý thuyết tối ưu hóa lồi. Đối với hàm kích hoạt ReLU, hằng số Lipschitz của ánh xạ lớp, hl+1 = ϕ(θlhl), bằng chuẩn phổ của ma trận trọng số. Chúng ta có thể giới hạn hằng số Lipschitz của toàn bộ mạng nơ-ron phía trên bằng tích của các hằng số Lipschitz theo lớp (Szegedy et al., 2013). Kết hợp những kết quả này, chúng ta kết luận rằng trong quá trình học liên tục, hằng số Lipschitz của mỗi lớp đang tăng và do đó hằng số Lipschitz của toàn bộ mạng cũng đang tăng. Điều này có vấn đề từ quan điểm tối ưu hóa vì gradient descent chỉ hội tụ trên các vấn đề tối ưu hóa lồi khi kích thước bước nhỏ hơn 2/L, nơi L là hằng số Lipschitz. Do đó, nếu hằng số Lipschitz tăng thì, cuối cùng, kích thước bước được chọn sẽ quá lớn và gradient descent sẽ không hội tụ cục bộ trên nhiệm vụ.

A.4 THÊM CHI TIẾT VỀ CHÍNH QUY HÓA VÀ KHẢ NĂNG HUẤN LUYỆN
Một lợi thế quan trọng khác của chính quy hóa là nó có tác động được hiểu rõ để đảm bảo khả năng huấn luyện trong các phương pháp ngoài học sâu, như hồi quy tuyến tính thoái hóa và các vấn đề nghịch đảo (Benning và Burger, 2018).

Mỗi bộ chính quy hóa thiên vị động lực tham số theo cách riêng của chúng, như giữ các tham số gần với không hoặc gần với khởi tạo. Nhưng, thuật ngữ chính quy hóa độc lập với mục tiêu cơ sở, và có khả năng với dữ liệu và/hoặc kiến trúc. Ngay cả khi mục tiêu cơ sở cung cấp gradient bằng không, chính quy hóa rõ ràng có thể cung cấp gradient cho tất cả trọng số, và đặt lại trọng số nếu đơn vị ẩn liên kết với trọng số trở nên bão hòa. Ví dụ, nếu tính phi tuyến là ReLU, thì đơn vị ẩn (i) không hoạt động nếu cho tất cả đầu vào trong tập dữ liệu (k), hli = ϕ(∑dl-1j=1θlij hl-1j,k) = 0. Sau đó biểu diễn ẩn đã sụp đổ cho đơn vị đó, và không có trọng số nào đóng góp cho đơn vị đó sẽ được cập nhật, giữ đơn vị không hoạt động. Nhưng, với chính quy hóa, các trọng số sẽ được cập nhật và đơn vị có thể trở nên hoạt động trở lại cho một số đầu vào.

A.5 CHI TIẾT VỀ CHÍNH QUY HÓA PHỔ
Chính Quy Hóa Phổ so với Chính Quy Hóa L2 Chính quy hóa L2 không hiệu quả bằng chính quy hóa phổ vì nó hạn chế độ lớn của các tham số, và thậm chí có thể gây ra sụp đổ hạng (Kumar et al., 2023). Chính quy hóa phổ chỉ hạn chế chuẩn phổ của ma trận trọng số gần với 1, σ(1)(θl) ≈ 1. Điều này có tác dụng kiểm soát độ lớn của cột tối đa của ma trận trọng số. Đó là, nếu θ ∈ Rdl×dl-1, thì 1/√dl∥θl∥1 < ∥θl∥2 nơi ∥θl∥1 = max1≤j≤dl ∑dli=1|[θl]ij|. Điều này ngụ ý rằng ∑dli=1|[θl]ij| < √dl cho mọi cột j. Điều này khác với chính quy hóa L2 theo hai cách: (i) bộ chính quy hóa không chính quy hóa tất cả các cột của ma trận trọng số, chỉ các tham số trong cột có tổng tối đa, và (ii) bộ chính quy hóa không chính quy hóa các trọng số riêng lẻ của cột, mà tổng của chúng. Điều này có nghĩa là các tham số có thể lớn hơn, và di chuyển xa hơn khỏi khởi tạo, có khả năng cho phép sử dụng dung lượng của mạng nơ-ron hiệu quả hơn. Cuối cùng, chúng tôi lưu ý rằng chính quy hóa L2 cũng có thể được xem theo chuẩn ma trận, cụ thể là chuẩn Frobenius.

Chính Quy Hóa Phổ so với Chuẩn Hóa Phổ: Chuẩn phổ cũng có thể được kiểm soát qua chuẩn hóa phổ (Miyato et al., 2018). Đối với học liên tục, chính quy hóa phổ được ưa thích hơn chuẩn hóa phổ vì hai lý do: (i) chuẩn hóa phổ phụ thuộc vào dữ liệu (xem Phần 2.1 và Phương trình 12 trong Miyato et al. (2018), và Phụ lục A.6 để biết thêm chi tiết), có thể có vấn đề trong học liên tục do phân phối dữ liệu thay đổi, và, (ii) các hình thức chuẩn hóa khác thường đã được sử dụng để huấn luyện mạng nơ-ron sâu, như LayerNorm (Ba et al., 2016). LayerNorm, cụ thể, chính quy hóa độ lớn của ánh xạ theo lớp bằng cách chia cho chuẩn của các đặc trưng trong lớp. Tuy nhiên, vì chuẩn phổ của các tham số cơ bản vẫn tăng với tốc độ √t, lớp chuẩn hóa tự nó sẽ bị mất tính đa dạng gradient. Cho mục đích đảm bảo khả năng huấn luyện liên tục, chính quy hóa kiểm soát rõ ràng chuẩn phổ của tất cả tham số và do đó được ưa thích hơn chuẩn hóa riêng lẻ. Điều đó nói rằng, chuẩn hóa vẫn có thể cải thiện hiệu suất và hiệu suất tối ưu có thể đạt được bằng sự kết hợp của chính quy hóa và chuẩn hóa, mà chúng tôi cho thấy trong Phần 5.3.

A.6 PHÂN LOẠI CÁC BỘ CHÍNH QUY HÓA CHO HỌC LIÊN TỤC
Chúng ta có thể phân loại các bộ chính quy hóa một cách rộng rãi thành các bộ chính quy hóa phụ thuộc dữ liệu và độc lập dữ liệu. Các bộ chính quy hóa phụ thuộc dữ liệu sử dụng dữ liệu theo một cách nào đó, có thể có vấn đề cho học liên tục khi phân phối dữ liệu thay đổi. Chính quy hóa trên một phân phối dữ liệu không nhất thiết duy trì các tính chất hữu ích để học trên phân phối dữ liệu tiếp theo. Ví dụ về các bộ chính quy hóa phụ thuộc dữ liệu bao gồm chính quy hóa hạng đặc trưng (Kumar et al., 2021), nhiệm vụ phụ trợ và các bộ chính quy hóa không gian đặc trưng khác (Lyle et al., 2022). Các phương pháp không tiêu chuẩn khác có tác dụng chính quy hóa phụ thuộc dữ liệu bao gồm các bộ chính quy hóa gradient như cắt gradient và tái khởi tạo trọng số dựa trên nơ-ron không hoạt động (Sokar et al., 2023). Những bộ chính quy hóa phụ thuộc dữ liệu này kiểm soát một số tính chất trên dữ liệu từ nhiệm vụ hiện tại nhưng có thể không kiểm soát tính chất trên dữ liệu từ một nhiệm vụ mới, cần thiết để duy trì tính dẻo dai.

Mặt khác, Chính Quy Hóa Độc Lập Dữ Liệu không phụ thuộc vào bất kỳ dữ liệu nào. Danh mục này chính quy hóa các tham số trực tiếp, đặc biệt hữu ích trong học liên tục khi phân phối dữ liệu đang thay đổi. Ví dụ về chính quy hóa độc lập dữ liệu bao gồm chính quy hóa L2 về phía không, giảm trọng số (cho các phương pháp gradient thích ứng), và chính quy hóa tái sinh (Kumar et al., 2023). Cách tiếp cận được đề xuất của chúng tôi về chính quy hóa phổ là độc lập dữ liệu và, hơn nữa, nhắm mục tiêu rõ ràng một điều kiện khả năng huấn luyện cho học liên tục. Do đó, chúng tôi mong đợi rằng nó đặc biệt hiệu quả trong việc duy trì khả năng huấn luyện trong học liên tục so với các bộ chính quy hóa độc lập dữ liệu khác.

A.7 CHÍNH QUY HÓA PHỔ CỦA CÁC LỚP KHÁC
Lớp Chuẩn Hóa Các lớp khác sử dụng tỷ lệ theo đơn vị, như lớp chuẩn hóa (Ba et al., 2016; Ioffe và Szegedy, 2015), cũng được chính quy hóa phổ. Ví dụ, ký hiệu γ ∈ Rd là các tham số tỷ lệ có thể huấn luyện cho LayerNorm (Ba et al., 2016), thì tích theo từng phần tử của các trọng số có thể được viết như một ma trận trọng số đường chéo, f(z) = γ ◦ z = Diag(γ)z. Giá trị đơn lớn nhất của ma trận đường chéo là mục nhập lớn nhất, σmax(Diag(γ)) = maxi|γi|. Tuy nhiên, tối ưu hóa với tối đa có vấn đề vì max không khả vi. Hơn nữa, các thay thế khả vi, như log-sum-exp ((log(∑iexp(|γi|)) − 1)²), chỉ cho giới hạn trên lỏng lẻo về tối đa có vấn đề vì chúng ta muốn chính quy hóa tối đa về phía một. Do đó, chúng tôi chính quy hóa mỗi trọng số về phía 1.

Lớp Tích Chập Tương tự như công trình khác, chúng tôi định hình lại tensor trọng số tích chập (với kích thước kernel (k×k, din bộ lọc và dout bộ lọc) thành ma trận kích thước dout×(k·k·din) (Yoshida và Miyato, 2017). Chuẩn phổ của ma trận được định hình lại này cung cấp giới hạn trên hiệu quả về chuẩn phổ của ma trận Toeplitz định nghĩa tích chập (Tsuzuku et al., 2018, Hệ quả 1).

--- TRANG 17 ---
B CHI TIẾT THÍ NGHIỆM
Tất cả các thí nghiệm của chúng tôi sử dụng Adam (Kingma và Ba, 2015) nơi kích thước bước mặc định 0.001 được chọn sau khi quét ban đầu trên [0.005, 0.001, 0.0005]. Cho tất cả kết quả của chúng tôi, chúng tôi sử dụng 10 seed ngẫu nhiên và cung cấp vùng bóng tương ứng với lỗi chuẩn của trung bình. Cho các thí nghiệm trên tiny-ImageNet SVHN2, CIFAR10 và CIFAR100, chúng tôi sử dụng 4 seed để quét qua cường độ chính quy hóa [0.01, 0.001, 0.0001], và thấy rằng 0.0001 hoạt động tốt trên tiny-ImageNet, CIFAR10 và CIFAR100 cho tất cả bộ chính quy hóa, trong khi 0.001 hoạt động tốt nhất trên SVHN2 cho tất cả bộ chính quy hóa.

Tập dữ liệu và tính không cố định:
•MNIST: Chỉ 12800 điểm dữ liệu đầu tiên được sử dụng để huấn luyện, với kích thước batch 512 và [40, 80, 120] epoch, và tổng cộng 50 nhiệm vụ.
•Fashion MNIST: Chỉ 12800 hình ảnh đầu tiên, với kích thước batch 512 và [40, 80, 120] epoch mỗi nhiệm vụ, và tổng cộng 50 nhiệm vụ.
•EMNIST: Chúng tôi sử dụng phiên bản cân bằng của tập dữ liệu, sử dụng 100000 điểm dữ liệu đầu tiên. Cho tính không cố định nhãn ngẫu nhiên, chúng tôi sử dụng kích thước batch 500 và 100 epoch mỗi nhiệm vụ, với tổng cộng 50 nhiệm vụ. Cho cả tính không cố định lật nhãn và hoán vị pixel, chúng tôi sử dụng kích thước batch 500 và 20 epoch mỗi nhiệm vụ, với tổng cộng 200 nhiệm vụ.
•SVHN2: 50000 hình ảnh đầu tiên được sử dụng để huấn luyện, 5000 hình ảnh từ tập kiểm tra được sử dụng để xác thực và phần còn lại được sử dụng để kiểm tra. Kích thước batch được sử dụng là 500, chúng tôi thấy 250 không đáng tin cậy để học do phương sai cao. Cho tính không cố định nhãn ngẫu nhiên, 20 epoch mỗi nhiệm vụ và 25 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa. Cho tính không cố định hoán vị pixel, 10 epoch và 100 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa.
•CIFAR10: Tất cả 50000 hình ảnh được sử dụng để huấn luyện, 1000 hình ảnh từ tập kiểm tra được sử dụng để xác thực và phần còn lại được sử dụng để kiểm tra. Chúng tôi sử dụng kích thước batch 250 và thấy điều này hiệu quả. Cho tính không cố định nhãn ngẫu nhiên, 20 epoch mỗi nhiệm vụ và 30 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa. Cho tính không cố định hoán vị pixel, 10 epoch và 100 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa.
•CIFAR100: Tất cả 50000 hình ảnh được sử dụng để huấn luyện, 1000 hình ảnh từ tập kiểm tra được sử dụng để xác thực và phần còn lại được sử dụng để kiểm tra. Chúng tôi sử dụng kích thước batch 250 và thấy điều này hiệu quả. Cho tính không cố định nhãn ngẫu nhiên, 20 epoch mỗi nhiệm vụ và 30 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa. Cho tính không cố định hoán vị pixel, 10 epoch và 100 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa.
•tiny-ImageNet: Tất cả 100000 hình ảnh được sử dụng để huấn luyện, 10000 hình ảnh được sử dụng để xác thực, và 10000 hình ảnh được sử dụng để kiểm tra theo phân chia được xác định trước. Chúng tôi sử dụng kích thước batch 250 và thấy điều này hiệu quả. Cho tính không cố định nhãn ngẫu nhiên, 20 epoch mỗi nhiệm vụ và 30 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa. Cho tính không cố định hoán vị pixel, 20 epoch và 100 nhiệm vụ đủ để mất khả năng huấn luyện cho mạng không được chính quy hóa.
•Học Tăng Cường: Chúng tôi đánh giá chính quy hóa phổ trong các nhiệm vụ điều khiển RL từ benchmark DMC (Tassa et al., 2020), với phương pháp SAC (Haarnoja et al., 2018) trong chế độ tỷ lệ phát lại (RR) đòi hỏi (D'Oro et al., 2022) với 16 cập nhật gradient cho mỗi bước môi trường mới. Chúng tôi chọn thiết lập này vì chế độ RR cao dẫn đến thiên vị ưu tiên đáng kể (Nikishin et al., 2022) được định nghĩa là xu hướng overfit trải nghiệm ban đầu làm hỏng phần còn lại của quá trình học.

Kiến Trúc Mạng Nơ-ron:
•MNIST, EMNIST, và Fashion MNIST: MLP 4 lớp với 256 nơ-ron mỗi lớp và kích hoạt relu. Cho các thí nghiệm sử dụng nó, LayerNorm được áp dụng sau ma trận trọng số tuyến tính và trước tính phi tuyến.
•tiny-ImageNet CIFAR10, CIFAR100, và SVHN2: ResNet-18 hiện có với batch norm, cũng như Vision Transformer (tiny) hiện có.
•Học Tăng Cường: Các nghiên cứu gần đây (Ball et al., 2023; Nauman et al., 2024) đã chứng minh rằng, trong thiết lập này, chỉ có đặt lại với chuẩn hóa lớp (Ba et al., 2016) phục vụ như một chiến lược giảm thiểu hiệu quả. Chúng tôi so sánh hiệu suất của agent SAC với cả chính quy hóa phổ và chuẩn hóa lớp với hai agent baseline: SAC chỉ với chuẩn hóa lớp, và SAC với chuẩn hóa lớp cộng đặt lại. Chúng tôi sử dụng chính quy hóa phổ với hệ số 1e−4 cho cả actor và critic. Cho mọi phương pháp, chúng tôi sử dụng một critic duy nhất và kích thước kiến trúc 2 lớp và 256 nơ-ron mỗi lớp cho cả actor và critic. Sử dụng chính sách ngẫu nhiên, chúng tôi điền trước bộ đệm phát lại với 10,000 chuyển tiếp trước khi bắt đầu huấn luyện. Kích thước tối đa của bộ đệm phát lại là 1 triệu chuyển tiếp.

--- TRANG 19 ---
C THÍ NGHIỆM BỔ SUNG
C.1 LOẠI BỎ SIÊU THAM SỐ k CHO CHÍNH QUY HÓA PHỔ
Hình 7: Đánh giá lựa chọn k cho chính quy hóa phổ cho k = 1,2,4,8 trên MNIST (Phải) và Fashion MNIST (trái). Chúng tôi thấy rằng k = 2 cân bằng sự ổn định với hiệu quả, và sử dụng giá trị này trong suốt các thí nghiệm của chúng tôi.

C.2 KẾT QUẢ TRÊN FASHION MNIST
Hình 8: Mất Tính Dẻo dai trong Fashion MNIST. Mặc dù tất cả các mạng trong thí nghiệm này sử dụng LayerNorm, mất tính dẻo dai xảy ra mà không có chính quy hóa: hiệu suất giảm như một hàm của nhiệm vụ, ngay cả với số lần lặp tăng.

--- TRANG 20 ---
C.3 VISION TRANSFORMER KHÔNG THỂ GHI NHỚ NHÃN NGẪU NHIÊN
Hình 9: ResNet và Vision Transformer trên nhiệm vụ ghi nhớ nhãn ngẫu nhiên sử dụng tiny-ImageNet. Kiến trúc Vision Transformer đặc biệt phù hợp với các nhiệm vụ có cấu trúc từ đó tổng quát hóa có thể. Tuy nhiên, chúng tôi thấy rằng khả năng huấn luyện của nó trên nhãn ngẫu nhiên thiếu. Chúng tôi không thể khiến Vision Transformer ghi nhớ nhãn ngẫu nhiên ngay cả trên một nhiệm vụ đơn trong ImageNet.

C.4 KẾT QUẢ ĐỘ NHẠY CẢM BỔ SUNG
Hình 10: Phân tích độ nhạy cảm về cường độ chính quy hóa So với các bộ chính quy hóa khác, chính quy hóa phổ không nhạy cảm với cường độ chính quy hóa trong khi duy trì khả năng huấn luyện cao hơn cho bất kỳ cường độ chính quy hóa nào được cho.

--- TRANG 21 ---
Hình 11: Độ nhạy cảm với loại tính không cố định trên EMNIST. Chính quy hóa phổ có thể duy trì khả năng huấn luyện cao một cách nhất quán qua gán nhãn ngẫu nhiên, hoán vị pixel, và lật nhãn. Khi áp dụng gán nhãn ngẫu nhiên, L2 (init) và L2 (zero) không thể đạt được khả năng huấn luyện cao so với chính quy hóa phổ.

Hình 12: Độ nhạy cảm với số epoch mỗi nhiệm vụ trên MNIST với gán nhãn ngẫu nhiên. Không có Layer Norm, chính quy hóa phổ có thể duy trì khả năng huấn luyện hơn các bộ chính quy hóa khác ngay cả khi số epoch thấp. Chính quy hóa phổ cũng có tác dụng hiệp tác với Layer Norm, cải thiện hiệu suất ban đầu trên nhiệm vụ đầu tiên, và cải thiện qua các nhiệm vụ.

Mạnh Mẽ Đối Với Tính Không Cố Định trong Fashion MNIST Tiếp theo, chúng tôi khám phá liệu các bộ chính quy hóa được đề xuất của chúng tôi có hiệu quả qua các loại tính không cố định khác nhau. Để làm điều này, chúng tôi sử dụng EMNIST vì số lượng lớp đủ lớn để lật nhãn gây ra mất khả năng huấn luyện (Elsayed và Mahmood, 2024). Trong Hình 11, chúng tôi thấy rằng chính quy hóa phổ duy trì khả năng huấn luyện tốt nhất qua các tính không cố định khác nhau.

Thay Đổi Số Epoch Mỗi Nhiệm Vụ Cho rằng vấn đề học liên tục được nêu trong Phần 2 phụ thuộc vào số lần lặp, có thể trường hợp số epoch cao hơn mỗi nhiệm vụ có thể giảm thiểu mất khả năng huấn luyện. Trong Hình 12, chúng tôi thấy rằng số epoch chỉ trì hoãn mất khả năng huấn luyện (xem Phụ lục C.2 cho kết quả trên Fashion MNIST). Ngay cả khi số epoch mỗi nhiệm vụ đủ cao để đạt 100% độ chính xác trên nhiệm vụ đầu tiên, mất khả năng huấn luyện cuối cùng vẫn xảy ra mà không có chính quy hóa. Ngược lại, khi sử dụng chính quy hóa phổ, mất khả năng huấn luyện được giảm thiểu một cách nhất quán. Chúng tôi cũng thấy rằng chính quy hóa phổ đặc biệt hiệu quả trong việc duy trì khả năng huấn luyện khi số epoch mỗi nhiệm vụ thấp. Bằng chứng cho điều này rõ ràng nhất mà không có Layer Norm, trong hàng trên, nơi chính quy hóa phổ là phương pháp duy nhất có khả năng duy trì khả năng huấn luyện của nó. Mặc dù Layer Norm kiểm soát chuẩn phổ thông qua các kích hoạt (Kim et al., 2021), việc thêm Layer Norm một mình không đủ để giảm thiểu mất khả năng huấn luyện. Tuy nhiên, Layer Norm có tác dụng hiệp tác với các hình thức chính quy hóa khác nhau, với độ chính xác được cải thiện bất kể số epoch mỗi nhiệm vụ.

--- TRANG 22 ---
C.5 TÁC ĐỘNG CỦA CHÍNH QUY HÓA ĐỐI VỚI DUNG LƯỢNG TRONG MỘT NHIỆM VỤ ĐƠN LẺ
Cường độ chính quy hóa để ngăn chặn mất khả năng huấn luyện phải đủ cao, và cường độ chính quy hóa này có thể hạn chế dung lượng của mạng nơ-ron. Bây giờ chúng tôi cho thấy việc huấn luyện mở rộng của mỗi bộ chính quy hóa sử dụng cùng cường độ chính quy hóa được sử dụng để ngăn chặn mất khả năng huấn luyện trong Hình 13. Chính quy hóa phổ không chỉ tốt nhất trong việc ngăn chặn mất khả năng huấn luyện, mà còn đạt được độ chính xác cao nhất khi huấn luyện đến hội tụ trên một nhiệm vụ đơn lẻ. Điều này có nghĩa là chính quy hóa phổ hạn chế dung lượng của mạng nơ-ron ít nhất, trong khi vẫn ngăn chặn mất khả năng huấn luyện.

Hình 13: Hiệu suất nhiệm vụ đơn lẻ với ResNet-18. Chính quy hóa phổ ít hạn chế nhất đối với dung lượng mạng nơ-ron, được chứng minh bằng khả năng khớp tốt hơn các nhãn được gán ngẫu nhiên so với các bộ chính quy hóa khác. Baseline không được chính quy hóa có thể sử dụng dung lượng của nó đầy đủ, nhưng với cái giá là giảm khả năng huấn luyện trong các nhiệm vụ sau.

--- TRANG 23 ---
C.6 ĐIỀU TRA TÁC ĐỘNG CỦA CHÍNH QUY HÓA ĐỐI VỚI TÍNH CHẤT MẠNG NƠ-RON
Hình 14: Giá trị đơn, chuẩn trọng số, hạng ổn định và tính đa dạng gradient hiệu quả: Trái: SVHN2, Giữa: CIFAR10, Phải: CIFAR100.

--- TRANG 24 ---
C.7 ĐIỀU TRA TỔNG QUÁT HÓA TRÊN CONTINUAL IMAGENET
Chúng tôi điều tra hiệu suất tổng quát hóa của chính quy hóa phổ trên Continual ImageNet. Trong Continual ImageNet, mỗi nhiệm vụ là phân biệt giữa hai lớp ImageNet. Chúng tôi sử dụng kiến trúc mạng và giao thức huấn luyện được sử dụng trong Kumar et al. (2023). Chúng tôi thấy rằng tất cả các phương pháp chính quy hóa được kiểm tra đạt được hiệu suất tổng quát hóa cao qua các nhiệm vụ. Các phương pháp chúng tôi so sánh là chính quy hóa phổ (SpectralRegAgent), L2 về phía không (L2Agent), L2 về phía khởi tạo (L2InitAgent), tái chế nơ-ron không hoạt động (ReDOAgent), và không chính quy hóa (BaseAgent).

Hình 15: Hiệu suất tổng quát hóa trên Continual ImageNet.

C.8 KẾT QUẢ TRÊN CÁC MÔI TRƯỜNG DMC RIÊNG LẺ
Chúng tôi báo cáo lợi nhuận trung bình cho các môi trường DMC trong Hình 16.

Hình 16: Trung bình và lỗi chuẩn của lợi nhuận cho 7 môi trường DMC.

--- TRANG 25 ---
