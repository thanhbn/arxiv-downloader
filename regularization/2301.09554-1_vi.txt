# 2301.09554.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/regularization/2301.09554.pdf
# Kích thước tệp: 1252721 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
1
Học Sâu Gặp Gỡ Chính Quy Hóa Thưa:
Góc Nhìn Từ Xử Lý Tín Hiệu
Rahul Parhi, Thành viên, IEEE , và Robert D. Nowak, Nghiên cứu viên cao cấp, IEEE
Tóm tắt
Học sâu đã thành công vang dội trong thực tế và hầu hết các phương pháp học máy tiên tiến
đều dựa trên mạng nơ-ron. Tuy nhiên, vẫn thiếu một lý thuyết toán học nghiêm ngặt có thể
giải thích một cách đầy đủ hiệu suất đáng kinh ngạc của mạng nơ-ron sâu. Trong bài viết này,
chúng tôi trình bày một khung toán học tương đối mới cung cấp sự hiểu biết sâu sắc hơn về
học sâu. Khung này đặc trưng hóa chính xác các tính chất hàm của mạng nơ-ron được huấn
luyện để khớp với dữ liệu. Các công cụ toán học chính hỗ trợ khung này bao gồm chính quy
hóa thưa trong miền biến đổi, biến đổi Radon của chụp cắt lớp vi tính, và lý thuyết xấp xỉ,
tất cả đều là các kỹ thuật có nguồn gốc sâu sắc trong xử lý tín hiệu. Khung này giải thích
tác động của chính quy hóa phân rã trọng số trong huấn luyện mạng nơ-ron, việc sử dụng
kết nối bỏ qua và ma trận trọng số hạng thấp trong kiến trúc mạng, vai trò của tính thưa
trong mạng nơ-ron, và giải thích tại sao mạng nơ-ron có thể hoạt động tốt trong các bài toán
chiều cao.

I. GIỚI THIỆU
Học sâu (DL) đã cách mạng hóa kỹ thuật và khoa học trong thời đại dữ liệu hiện đại.
Mục tiêu điển hình của DL là dự đoán một đầu ra y∈ Y (ví dụ, một nhãn hoặc phản hồi) từ một đầu vào
x∈ X (ví dụ, một đặc trưng hoặc ví dụ). Một mạng nơ-ron (NN) được "huấn luyện" để khớp với một tập dữ liệu
gồm các cặp {(xn, yn)}N
n=1, bằng cách tìm một tập tham số NN θ sao cho ánh xạ NN
khớp chặt chẽ với dữ liệu. NN được huấn luyện là một hàm, ký hiệu là fθ:X → Y, có thể
được sử dụng để dự đoán đầu ra y∈ Y của một đầu vào mới x∈ X. Mô hình này được gọi là

Rahul Parhi thuộc Nhóm Hình ảnh Y sinh, ´Ecole Polytechnique F ´ed´erale de Lausanne, Lausanne, Thụy Sĩ
(e-mail: rahul.parhi@epfl.ch).
Robert D. Nowak thuộc Khoa Kỹ thuật Điện và Máy tính, Đại học Wisconsin–Madison,
Madison, WI, Hoa Kỳ (e-mail: rdnowak@wisc.edu).arXiv:2301.09554v3  [stat.ML]  8 Jun 2023

--- TRANG 2 ---
2
học có giám sát, đây là trọng tâm của bài viết này. Thành công của học sâu đã tạo ra một
ngành công nghiệp phát triển mạnh mẽ liên tục phát triển các ứng dụng mới, kiến trúc NN, và thuật toán
huấn luyện. Bài viết này xem xét các phát triển gần đây trong toán học của DL, tập trung vào việc
đặc trưng hóa các loại hàm được học bởi NNs khớp với dữ liệu. Hiện tại có nhiều
lý thuyết cạnh tranh để giải thích thành công của DL. Những phát triển này là một phần của khối công trình
lý thuyết rộng lớn hơn có thể được tổ chức một cách thô lược thành ba danh mục rộng: 1) lý thuyết xấp xỉ
với NNs; 2) thiết kế và phân tích các thuật toán tối ưu hóa ("huấn luyện") cho NNs; và
3) đặc trưng hóa các tính chất của NNs đã được huấn luyện.

Bài viết này thuộc danh mục nghiên cứu thứ ba và điều tra các tính chất hàm
(tức là, tính chính quy) của các nghiệm cho bài toán huấn luyện NN với chính quy hóa Tikhonov-type rõ ràng.
Trong khi phần lớn thành công của học sâu trong thực tế đến từ các mạng có kiến trúc
có cấu trúc cao, việc thiết lập một lý thuyết nghiêm ngặt và thống nhất cho các NNs như vậy được sử dụng trong thực tế là khó khăn.
Do đó, chúng tôi chủ yếu tập trung vào NNs feedforward hoàn toàn kết nối với hàm kích hoạt
Rectified Linear Unit (ReLU) phổ biến. Bài viết này giới thiệu một khung toán học thống nhất
một dòng công trình từ nhiều tác giả trong vài năm qua đã làm sáng tỏ bản chất
và hành vi của các hàm NN được huấn luyện đến một cực tiểu toàn cục với chính quy hóa rõ ràng.
Các kết quả được trình bày chỉ là một phần của câu đố hướng tới việc phát triển một lý thuyết toán học
về học sâu. Mục đích của bài viết này, đặc biệt, là cung cấp một giới thiệu nhẹ nhàng
về khung toán học mới này, có thể tiếp cận được với độc giả có nền tảng toán học về
Tín hiệu và Hệ thống và Đại số Tuyến tính Ứng dụng. Khung này dựa trên các công cụ toán học
quen thuộc với cộng đồng xử lý tín hiệu, bao gồm chính quy hóa thưa trong miền biến đổi,
biến đổi Radon của chụp cắt lớp vi tính, và lý thuyết xấp xỉ. Nó cũng liên quan đến
các ý tưởng xử lý tín hiệu nổi tiếng như wavelets, splines, và sensing nén. Khung
này cung cấp một cách tiếp cận mới cho các câu hỏi cơ bản sau:
1) Tác động của chính quy hóa trong DL là gì?
2) NNs học các loại hàm nào?
3) Vai trò của các hàm kích hoạt NN là gì?
4) Tại sao NNs dường như phá vỡ được lời nguyền của chiều không gian?

II. MẠNG NƠ-RON VÀ HỌC TỪ DỮ LIỆU
Nhiệm vụ của DL tương ứng với việc học ánh xạ đầu vào-đầu ra từ một tập dữ liệu theo cách
phân cấp hoặc đa lớp. Mạng nơ-ron sâu (DNNs) là các ánh xạ hàm phức tạp được xây dựng
từ nhiều khối xây dựng nhỏ hơn, đơn giản hơn. Khối xây dựng đơn giản nhất của DNN là một nơ-ron (nhân tạo)

--- TRANG 3 ---
3
(a) Sigmoid
 (b) Unit Step
 (c) ReLU
Hình 1. Các hàm kích hoạt điển hình được tìm thấy trong mạng nơ-ron.

nơ-ron, được lấy cảm hứng từ các nơ-ron sinh học của não bộ [24]. Một nơ-ron là một ánh xạ hàm
Rd→R có dạng z7→σ(wTz−b), trong đó w∈Rd tương ứng với trọng số của
nơ-ron và b∈R tương ứng với bias của nơ-ron. Hàm σ:R→R được gọi
là hàm kích hoạt của nơ-ron và điều khiển phản ứng phi tuyến của nơ-ron. Một
nơ-ron "kích hoạt" khi tổ hợp có trọng số của đầu vào x của nó vượt quá một ngưỡng nhất định,
tức là, wTx> b. Do đó, các hàm kích hoạt điển hình như sigmoid, hàm bước đơn vị, hoặc
rectified linear unit (ReLU) kích hoạt khi đầu vào của chúng vượt quá 0 như thấy trong Hình 1.

Một nơ-ron được cấu thành từ một ánh xạ tuyến tính theo sau bởi một phi tuyến tính. Một dạng phổ biến (hoặc
"kiến trúc") của DNN là DNN feedforward hoàn toàn kết nối là một chuỗi các ánh xạ
tuyến tính xen kẽ và các phi tuyến tính theo thành phần. Một DNN feedforward fθ (được tham số hóa bởi
θ) có thể được biểu diễn như hợp thành hàm

fθ(x) =A(L)◦σ◦A(L−1)◦ ···σ◦A(1)(x), (1)

trong đó, với mỗi ℓ= 1, . . . , L, hàm A(ℓ)(z) =W(ℓ)z−b(ℓ) là một ánh xạ affine tuyến tính
với ma trận trọng số W(ℓ) và vector bias b(ℓ). Các hàm σ xuất hiện trong hợp thành
áp dụng hàm kích hoạt σ:R→R theo thành phần lên vector A(ℓ)(z). Trong khi hàm
kích hoạt có thể thay đổi từ nơ-ron này sang nơ-ron khác, chúng ta giả định rằng cùng một hàm kích hoạt
được sử dụng trong toàn bộ mạng trong bài viết này. Các tham số của DNN này là các trọng số
và biases, tức là, θ={(W(ℓ),b(ℓ))}L
ℓ=1. Mỗi ánh xạ A(ℓ) tương ứng với một lớp của DNN
và số lượng ánh xạ L là độ sâu của DNN. Các chiều của ma trận trọng số
W(ℓ) tương ứng với số lượng nơ-ron trong mỗi lớp (tức là, độ rộng của lớp). Các kiến trúc
DNN thay thế có thể được xây dựng với các khối xây dựng đơn giản khác, ví dụ, với các phép convolution và
các phép pooling/downsampling, tương ứng với mạng nơ-ron tích chập sâu
(DCNNs). Các kiến trúc DNN thường được mô tả bằng các sơ đồ như trong Hình 2.

--- TRANG 4 ---
4
(a) Feedforward DNN
z2 w2 Σ σ v2 Z2z1 w1
zd wdv1
vDZ1
ZD
−b......Đầu ra Lớp Trước Đầu vào Lớp Tiếp
(b) Một Nơ-ron Đơn
Hình 2. Ví dụ mô tả một mạng nơ-ron sâu và các thành phần của nó: (a) một kiến trúc DNN feedforward trong đó
các nút đại diện cho các nơ-ron và các cạnh đại diện cho các trọng số; (b) một nơ-ron đơn từ DNN trong (a) ánh xạ
một đầu vào z∈Rd thành một đầu ra Z∈RD qua Z=vσ(wTz−b).

Cho một DNN fθ được tham số hóa bởi θ∈Θ (của bất kỳ kiến trúc nào), nhiệm vụ học từ
dữ liệu {(xn, yn)}N
n=1 được công thức hóa như bài toán tối ưu

min
θ∈ΘNX
n=1L(yn, fθ(xn)), (2)

trong đó L(·,·) là một hàm mất mát (lỗi bình phương, logistic, hinge loss, v.v.). Ví dụ, mất mát
lỗi bình phương được cho bởi L(y, z) = (y−z)2. Một DNN được huấn luyện bằng cách giải bài toán tối ưu này,
thường thông qua một dạng nào đó của gradient descent. Trong các tình huống điển hình, bài toán tối ưu này là
ill-posed và do đó bài toán được chính quy hóa một cách rõ ràng thông qua việc thêm một số hạng
chính quy hóa và/hoặc một cách ngầm định bởi các ràng buộc trên kiến trúc mạng hoặc hành vi của các thủ tục
gradient descent [35]. Một hiện tượng đáng ngạc nhiên của các thuật toán huấn luyện gradient descent cho
NNs quá tham số hóa là, trong số nhiều nghiệm overfit dữ liệu, các thuật toán này

--- TRANG 5 ---
5
chọn một nghiệm thường tổng quát hóa tốt trên dữ liệu mới, ngay cả khi không có chính quy hóa rõ ràng. Điều này đã
dẫn đến các nhà nghiên cứu cố gắng hiểu vai trò của quá tham số hóa và tác động của khởi tạo ngẫu nhiên
các tham số NN trên bias ngầm của các thuật toán huấn luyện dựa trên gradient [8].

Mặt khác, chính quy hóa rõ ràng tương ứng với việc giải một bài toán tối ưu có dạng

min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λC(θ), (3)

trong đó C(θ)≥0 với mọi θ∈Θ. C(θ) là một regularizer đo lường "kích thước" (hoặc "dung lượng")
của DNN được tham số hóa bởi θ∈Θ và λ >0 là một siêu tham số có thể điều chỉnh điều khiển
sự đánh đổi giữa số hạng khớp dữ liệu và regularizer. DNNs thường được huấn luyện sử dụng các thuật toán
gradient descent với weight decay tương ứng với việc giải bài toán tối ưu

min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λCwd(θ), (4)

trong đó weight decay regularizer Cwd(θ) là chuẩn Euclidean bình phương của tất cả các trọng số
mạng. Đôi khi mục tiêu weight decay chính quy hóa tất cả các tham số, bao gồm cả biases, trong khi
đôi khi nó chỉ chính quy hóa các trọng số (để các biases không được chính quy hóa). Bài viết này
tập trung vào biến thể weight decay với biases không được chính quy hóa.

III. TÁC ĐỘNG CỦA CHÍNH QUY HÓA TRONG HỌC SÂU LÀ GÌ?
Weight decay là một dạng chính quy hóa phổ biến cho DNNs. Trên bề mặt, nó có vẻ đơn giản chỉ
là chính quy hóa Tikhonov hoặc "ridge" quen thuộc. Trong các mô hình tuyến tính tiêu chuẩn, người ta biết rõ
rằng loại chính quy hóa này có xu hướng giảm kích thước của các trọng số, nhưng không tạo ra
các trọng số thưa. Tuy nhiên, khi chính quy hóa này được sử dụng kết hợp với NNs, kết quả
hoàn toàn khác biệt. Chính quy hóa tổng bình phương các trọng số hóa ra tương đương với
chính quy hóa với một loại chính quy hóa ℓ1-norm trên các trọng số mạng, dẫn đến các nghiệm thưa
trong đó trọng số của nhiều nơ-ron bằng không [49]. Điều này do tính chất quan trọng
rằng các hàm kích hoạt được sử dụng phổ biến nhất trong DNNs là đồng nhất. Một hàm σ(t) được
gọi là đồng nhất (bậc 1) nếu σ(γt) =γσ(t) với bất kỳ γ >0. Hàm kích hoạt NN phổ biến nhất,
ReLU, là đồng nhất, cũng như leaky ReLU, kích hoạt tuyến tính,
và các đơn vị pooling/downsampling. Tính đồng nhất này dẫn đến định lý sau, được gọi là
định lý cân bằng nơ-ron (NBT).

--- TRANG 6 ---
6
Định lý Cân bằng Nơ-ron ([49, Định lý 1]): Cho fθ là một DNN của bất kỳ kiến trúc nào
được tham số hóa bởi θ∈Θ giải bài toán huấn luyện DNN với weight decay trong (4).
Khi đó, các trọng số thỏa mãn ràng buộc cân bằng sau: nếu w và v ký hiệu các trọng số đầu vào và
đầu ra của bất kỳ đơn vị đồng nhất nào trong DNN, thì ∥w∥2=∥v∥2.

Chứng minh của định lý này quy về quan sát đơn giản rằng với bất kỳ đơn vị đồng nhất nào
với trọng số đầu vào w và trọng số đầu ra v, chúng ta có thể nhân trọng số đầu vào với γ >0 và trọng số
đầu ra với 1/γ mà không thay đổi ánh xạ hàm. Ví dụ, xét nơ-ron đơn
z7→vσ(wTz−b) với hàm kích hoạt đồng nhất σ như mô tả trong Hình 2(b). Trong trường hợp
DNN như trong (1), w tương ứng với một hàng của ma trận trọng số trong ánh xạ affine của một lớp,
v tương ứng với một cột của ma trận trọng số trong lớp tiếp theo, và b tương ứng với một
mục trong vector bias. Hiển nhiên rằng (v/γ)σ((γw)Tz−γb) =vσ(wTz−b). Bằng cách lưu ý
rằng các biases không được chính quy hóa, định lý được chứng minh bằng cách nhận thấy rằng minγ>0∥γw∥2
2+∥v/γ∥2
2
xảy ra khi γ=p
∥v∥2/∥w∥2 ngụ ý rằng nghiệm chuẩn Euclidean bình phương tối thiểu
phải thỏa mãn tính chất rằng các trọng số đầu vào và đầu ra w và v được cân bằng.

A. Tính Thưa Bí Mật của Weight Decay
Tác động cân bằng của NBT có một tác động nổi bật lên các nghiệm của mục tiêu weight decay.
Đặc biệt, một hiệu ứng thúc đẩy tính thưa tương tự như chính quy hóa least absolute shrinkage and selection operator
(LASSO) [43]. Như một ví dụ minh họa, xét một NN feedforward nông (L= 2) ánh xạ Rd→RD với một hàm kích hoạt đồng nhất (ví dụ, ReLU) và K nơ-ron.
Trong trường hợp này, NN được cho bởi

fθ(x) =KX
k=1vkσ(wT
kx−bk). (5)

Ở đây, weight decay regularizer có dạng 1
2PK
k=1∥vk∥2
2+∥wk∥2
2, trong đó wk và vk là
các trọng số đầu vào và đầu ra của nơ-ron thứ k, tương ứng. Bởi NBT, điều này tương đương với
việc sử dụng regularizer PK
k=1∥vk∥2∥wk∥2. Do tính đồng nhất của hàm kích hoạt, chúng ta
có thể giả định, không mất tính tổng quát, rằng ∥wk∥2= 1 bằng cách "hấp thụ" độ lớn của trọng số
đầu vào wk vào trọng số đầu ra vk. Do đó, bằng cách ràng buộc các trọng số đầu vào có chuẩn đơn vị,
bài toán huấn luyện sau đó có thể được tái công thức hóa với regularizer PK
k=1∥vk∥2[49]. Đáng chú ý,
đây chính xác là group LASSO regularizer nổi tiếng [50], ưu tiên các nghiệm với ít
kết nối nơ-ron hoạt động (tức là, các nghiệm thường có nhiều vk chính xác bằng 0), mặc dù

--- TRANG 7 ---
7
mục tiêu huấn luyện tổng thể vẫn phi lồi. Chúng tôi cũng lưu ý rằng có một dòng công trình đã
tái công thức hóa bài toán huấn luyện phi lồi như một bài toán group LASSO lồi [33].

Tổng quát hơn, xét kiến trúc deep NN feedforward trong (1) với một hàm kích hoạt đồng nhất
và xét huấn luyện DNN với weight decay chỉ trên các trọng số mạng.
Một ứng dụng của NBT cho thấy rằng bài toán weight decay tương đương với bài toán huấn luyện DNN
được chính quy hóa với regularizer

C(θ) =1
2K(1)X
k=1∥w(1)
k∥2
2+1
2K(L)X
k=1∥v(L)
k∥2
2+LX
ℓ=1K(ℓ)X
k=1∥w(ℓ)
k∥2∥v(ℓ)
k∥2, (6)

trong đó K(ℓ) ký hiệu số lượng nơ-ron trong lớp ℓ, w(ℓ)
k ký hiệu các trọng số đầu vào cho nơ-ron thứ k
trong lớp ℓ, và v(ℓ)
k ký hiệu các trọng số đầu ra cho nơ-ron thứ k trong lớp ℓ (xem [49,
Phương trình (2)]). Các nghiệm dựa trên regularizer này cũng sẽ thưa do các 2-chuẩn xuất
hiện trong số hạng cuối trong (6) không được bình phương, tương tự như group LASSO regularizer. Đặc biệt,
regularizer này có thể được xem như một mixed ℓ2,1-chuẩn trên các ma trận trọng số. Hơn nữa, tăng
tham số chính quy hóa λ, sẽ tăng số lượng trọng số bằng không trong nghiệm.
Do đó, huấn luyện DNN với weight decay ưu tiên các nghiệm thưa, trong đó tính thưa được lượng hóa
thông qua số lượng kết nối nơ-ron hoạt động. Một phiên bản sớm của kết quả này xuất hiện năm 1998 [16],
mặc dù nó không trở nên nổi tiếng cho đến khi được tái khám phá năm 2015 [25].

IV. MẠNG NƠ-RON HỌC LOẠI HÀM NÀO?
Hiệu ứng thúc đẩy tính thưa của weight decay phát sinh từ NBT trong các kiến trúc mạng
với các hàm kích hoạt đồng nhất có một số hệ quả đối với các tính chất của NNs
được huấn luyện. Trong phần này, chúng tôi sẽ tập trung vào hàm kích hoạt ReLU phổ biến, ρ(t) = max {0, t}.
Tính thưa được áp đặt không chỉ thúc đẩy tính thưa theo nghĩa số lượng kết nối nơ-ron
hoạt động, mà còn thúc đẩy một loại tính thưa trong miền biến đổi. Tính thưa trong miền biến đổi này gợi ý
việc bao gồm kết nối bỏ qua và ma trận trọng số hạng thấp trong kiến trúc mạng.

A. Mạng Nơ-ron Nông
Trong trường hợp đơn biến, một ReLU NN feedforward nông với K nơ-ron được thực hiện bởi ánh xạ

fθ(x) =KX
k=1vkρ(wkx−bk). (7)

--- TRANG 8 ---
8
Huấn luyện NN này với weight decay tương ứng với việc giải bài toán tối ưu

min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λ
2KX
k=1|vk|2+|wk|2, (8)

Từ Phần III-A, chúng ta thấy rằng NBT ngụ ý rằng bài toán này tương đương với

min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λKX
k=1|vk||wk|. (9)

Như minh họa trong Chèn IN1, chúng ta thấy rằng (9) thực sự đang chính quy hóa tích phân của đạo hàm bậc hai
của NN, có thể được xem như một thước đo tính thưa trong miền đạo hàm bậc hai. Tích phân trong (15) phải được hiểu theo nghĩa phân phối vì xung
Dirac không phải là một hàm, mà là một hàm tổng quát hoặc phân phối. Để làm cho điều này chính xác, cho
gε(x) =e−x2/2ε/√
2πε ký hiệu mật độ Gaussian với phương sai ε >0. Như đã biết trong
xử lý tín hiệu, gε hội tụ đến xung Dirac khi ε→0. Sử dụng ý tưởng này, cho một phân phối
f, định nghĩa chuẩn

∥f∥M:= sup
ε>0∥f∗gε∥L1= sup
ε>0Z∞
−∞Z∞
−∞f(x)gε(y−x) dxdy. (16)

Định nghĩa này cung cấp một cấu trúc rõ ràng, thông qua tích chập với Gaussian, của một
dãy các hàm trơn hội tụ đến f, trong đó supremum hoạt động như giới hạn. Ví dụ, nếu f(x) =g(x) +PK
k=1vkδ(x−tk), trong đó g là một hàm khả tích tuyệt đối,
thì ∥f∥M=∥g∥L1+PK
k=1|vk|=∥g∥L1+∥v∥1, với ∥v∥1=PK
k=1|vk|. Chính theo nghĩa này
mà (15) đúng, tức là, ∥D2fθ∥M=PK
k=1|vk||wk|. Đặc biệt, M-chuẩn chính xác là
tương tự miền liên tục của ℓ1-chuẩn rời rạc thúc đẩy tính thưa. Do đó, chúng ta thấy rằng
huấn luyện một NN với weight decay như trong (8) ưu tiên các nghiệm với đạo hàm bậc hai thưa.

Hóa ra mối liên hệ giữa tính thưa trong miền đạo hàm bậc hai
và NNs thậm chí còn chặt chẽ hơn. Cho BV2(R) ký hiệu không gian các hàm ánh xạ R→R sao cho
∥D2f∥M là hữu hạn. Đây là không gian các hàm biến thiên có giới hạn bậc hai và
lượng ∥D2f∥M là total variation (TV) bậc hai¹ của f. Từ lý thuyết spline đã biết rõ [14], [23], [46] rằng các hàm khớp dữ liệu và có TV bậc hai tối thiểu là
các hàm tuyến tính từng khúc liên tục (CPwL). Vì ReLU là một hàm CPwL, ReLU NNs

¹Khái niệm TV cổ điển, thường được sử dụng trong các bài toán khử nhiễu tín hiệu [37], là TV(f):=∥Df∥M và do đó TV bậc hai của f có thể được xem như TV của đạo hàm của f: ∥D2f∥M= TV(D f).

--- TRANG 9 ---
9
[IN1] Tính Thưa ReLU trong Miền Đạo hàm Bậc hai
Cho một nơ-ron ReLU r(x) =ρ(wx−b),
đạo hàm bậc nhất của nó, Dr(x), là
Dr(x) = D ρ(wx−b)
=w u(wx−b), (10)
trong đó u là hàm bước đơn vị (Hình 1(b)).
Do đó, đạo hàm bậc hai của nó, D2r(x), là
D2r(x) = D w u(wx−b)
=w2δ(wx−b). (11)
Bởi tính chất tỷ lệ của xung Dirac [27, Bài toán 1.38(a)]
δ(γx) =1
|γ|δ(x) (12)
chúng ta có
D2r(x) =w2
|w|δ
x−b
w
=|w|δ
x−b
w
. (13)
Đạo hàm bậc hai của NN (7) sau đó là
D2fθ(x) =KX
k=1vk|wk|δ
x−bk
wk
.(14)
Do đó,
Z∞
−∞|D2fθ(x)|dx=KX
k=1|vk||wk|.(15)

b1
w1b2
w2b3
w3b4
w4b5
w5b6
w6fθ(x)
Dfθ(x)
v1|w1|v3|w3|v5|w5|
v2|w2|v4|w4|v6|w6|D2fθ(x)

Hình 3. Minh họa tính thưa trong miền đạo hàm bậc hai của một NN feedforward nông đơn biến với
6 nơ-ron.

là các hàm CPwL [3]. Thực vậy, dưới các giả định nhẹ về hàm mất mát, tập nghiệm của bài toán tối ưu

min
f∈BV2(R)NX
n=1L(yn, f(xn)) +λ∥D2f∥M (17)

được đặc trưng hóa hoàn toàn bởi NNs có dạng

fθ(x) =KX
k=1vkρ(wkx−bk) +c1x+c0, (18)

--- TRANG 10 ---
10
trong đó số lượng nơ-ron nhỏ hơn nghiêm ngặt số lượng dữ liệu ( K < N ) theo nghĩa rằng
tập nghiệm của (17) là một tập lồi đóng có các điểm cực trị có dạng (18) với
K < N [9], [29], [39]. Trong thuật ngữ mạng nơ-ron, số hạng c1x+c0 là một kết nối bỏ qua [17].
Số hạng này là một hàm affine tự nhiên phát sinh vì TV bậc hai của một hàm affine
bằng không và do đó regularizer không đặt ra hình phạt nào cho việc bao gồm số hạng này.

Trực giác đằng sau kết quả này là do thực tế rằng đạo hàm bậc hai của một hàm CPwL là
một chuỗi xung và do đó thể hiện tính thưa cực đoan trong miền đạo hàm bậc hai. Điều này được
minh họa trong Hình 3. Do đó, bài toán tối ưu (17) sẽ ưu tiên các hàm CPwL thưa
luôn chấp nhận một biểu diễn như trong (18). Trong thuật ngữ xử lý tín hiệu, "tín hiệu" thưa
trong một miền biến đổi nào đó được gọi là có tốc độ đổi mới hữu hạn [47]. Ở đây, biến đổi
liên quan là toán tử đạo hàm bậc hai và đổi mới là chuỗi xung phát sinh
sau khi áp dụng toán tử đạo hàm bậc hai cho một hàm CPwL.

Xét tối ưu hóa trên không gian tham số NN ΘK của các mạng như trong (18) với
độ rộng cố định K≥N. Từ phép dẫn xuất trong Chèn IN1, chúng ta có {fθ:θ∈ΘK} ⊂BV2(R).
Hơn nữa, từ thảo luận trên, chúng ta biết luôn tồn tại một nghiệm tối ưu cho
(17) có dạng (18) với K < N, tức là, luôn tồn tại một nghiệm cho (17) trong
{fθ:θ∈ΘK}. Do đó, từ sự tương đương của (8) và (9), chúng ta thấy rằng huấn luyện một
NN đủ rộng ( K≥N) với một kết nối bỏ qua (18) và weight decay (8) dẫn đến một nghiệm của bài toán
tối ưu (17) trên không gian hàm BV2(R). Trong khi kết quả này có thể có vẻ hiển nhiên
khi nhìn lại, nó đáng chú ý vì nó nói rằng các loại hàm mà NNs được huấn luyện với weight
decay (đến một cực tiểu toàn cục) chính xác là các hàm tối ưu trong BV2(R). Hơn nữa, kết quả này
làm sáng tỏ vai trò của quá tham số hóa vì sự tương ứng này phụ thuộc vào việc mạng
được tham số hóa tới hạn hoặc quá tham số hóa (vì K≥N).

Trong trường hợp đa biến, một NN feedforward nông có dạng

fθ(x) =KX
k=1vkρ(wT
kx−bk). (22)

Tính chất chính kết nối trường hợp đơn biến và BV2(R) là các nơ-ron ReLU được làm thưa
bởi toán tử đạo hàm bậc hai như trong (13). Một phân tích tương tự có thể được thực hiện trong trường hợp
đa biến bằng cách tìm một toán tử là biến đổi làm thưa của nơ-ron ReLU đa biến
r(x) =ρ(wTx−b). Biến đổi làm thưa được đề xuất năm 2020 trong công trình tinh hoa của Ongie
et al. [26], và dựa trên biến đổi Radon phát sinh trong hình ảnh chụp cắt lớp. Các kết nối
giữa biến đổi Radon và các nơ-ron đã được biết từ ít nhất những năm 1990, trở nên phổ biến
do đề xuất ridgelets [6] và các phiên bản sớm của biến đổi làm thưa

--- TRANG 11 ---
11
[IN2] Biến đổi Radon và Định lý Lát cắt Fourier
Biến đổi Radon, lần đầu được nghiên cứu bởi Radon năm
1917 [36], của một hàm ánh xạ Rd→R được
xác định bởi biến đổi tích phân
R{f}(α, t) =Z
Rdf(x)δ(αTx−t) dx,(19)
trong đó δ là xung Dirac đơn biến, α∈
Sd−1={u∈Rd:∥u∥2= 1} là một vector đơn vị,
và t∈R là một vô hướng. Biến đổi Radon của f
tại (α, t) là tích phân của f dọc theo siêu phẳng
{x∈Rd:αTx=t}.
Biến đổi Radon liên kết chặt chẽ với biến đổi
Fourier, được xác định bởi
bf(ω) =Z
Rdf(x)e−jωTxdx, (20)
trong đó j2=−1. Thực vậy,
\R{f}(α, ω)
=Z
RZ
Rdf(x)δ(αTx−t) dx
e−jωtdt
=Z
Rdf(x)Z
Rδ(αTx−t)e−jωtdt
dx
=Z
Rdf(x)e−j(ωα)Txdx=bf(ωα). (21)
Kết quả này được gọi là định lý lát cắt Fourier.
Nó phát biểu rằng biến đổi Fourier đơn biến trong
miền Radon tương ứng với một lát cắt của biến đổi
Fourier trong miền không gian.

cho các nơ-ron đã được nghiên cứu sớm nhất là năm 1997 [19]. Một tóm tắt về biến đổi Radon xuất hiện trong
IN2. Biến đổi làm thưa cho các nơ-ron ReLU đa biến dựa trên một kết quả liên quan đến
biến đổi Radon (được lọc), xuất hiện trong Chèn IN3.

Bộ lọc K trong (26) chính xác là bộ lọc backprojection phát sinh trong thuật toán filtered backprojection
trong tái tạo hình ảnh chụp cắt lớp và hoạt động như một bộ lọc thông cao (hoặc bộ lọc ramp) để
hiệu chỉnh sự suy giảm của tần số cao từ biến đổi Radon. Trực giác đằng sau điều này
là biến đổi Radon tích phân một hàm dọc theo các siêu phẳng. Trong trường hợp đơn biến, độ lớn
của đáp ứng tần số của một bộ tích phân hành xử như 1/|ω| và do đó làm suy giảm
tần số cao. Độ lớn của đáp ứng tần số của tích phân dọc theo một siêu phẳng,
do đó, hành xử như 1/|ω|d−1, vì các siêu phẳng có chiều (d−1). Lưu ý rằng bộ chiếu chẵn
xuất hiện trong (30) là do thực tế rằng các biến miền Radon (α, t) và
(−α,−t) tham số hóa cùng một siêu phẳng.

Từ phép dẫn xuất trong Chèn IN3, chúng ta thấy ngay rằng biến đổi làm thưa của
nơ-ron ReLU đa biến r(x) =ρ(wTx−b) với (w, b)∈Sd−1×R là toán tử D2
tKR,
trong đó D2
t=∂2/∂t2 ký hiệu đạo hàm riêng bậc hai đối với t. Chúng ta có
D2
tKR{r}(α, t) =δR((α, t)−(w, b)), (31)
trong đó δR(z−z0):= P even{δ(z−z0)}= (δ(z−z0) +δ(z+z0))/2 là sự đối xứng hóa chẵn

--- TRANG 12 ---
12
[IN3] Biến đổi Radon được lọc của Nơ-ron với Trọng số Đầu vào Chuẩn Đơn vị
Đầu tiên xét nơ-ron r(x) =σ(wTx) với
w=e1= (1,0, . . . , 0) (vector đơn vị chính tắc đầu tiên). Trong trường hợp này, r(x) =σ(x1). Bằng cách nhận thấy
rằng hàm này có thể được viết như một tích tensor,
biến đổi Fourier được cho bởi tích sau
br(ω) =bσ(ω1)dY
k=22πδ(ωk). (23)
Bởi định lý lát cắt Fourier,
\R{r}(α, ω) =bσ(ωα1)dY
k=22πδ(ωαk).(24)
Bởi tính chất tỷ lệ của xung Dirac
(12), lượng trên bằng
=bσ(ωα1)(2π)d−1
|ω|d−1δ(α2, . . . , α d). (25)
Nếu chúng ta định nghĩa bộ lọc thông qua đáp ứng tần số
dKf(ω) =|ω|d−1
2(2π)d−1bf(ω), (26)
chúng ta tìm thấy
KR{r}b (α, ω) =bσ(ωα1)
2δ(α2, . . . , α d).(27)
Lấy biến đổi Fourier ngược,
KR{r}(α, t)
=1
2|α1|σt
α1
δ(α2, . . . , α d)
=σ(t)δ(α1−1) +σ(−t)δ(α1+ 1)
2δ(α2, . . . , α d)
=σ(t)δ(α−e1) +σ(−t)δ(α+e1)
2
=:Peven{σ(t)δ(α−e1)}, (28)
trong đó Peven là bộ chiếu trích xuất phần chẵn
của đầu vào của nó (theo các biến
(α, t)). Dòng thứ hai đúng bởi tính chất giãn nở
của biến đổi Fourier [27, Phương trình
(4.34)]
1
|γ|ft
γ
F← − →bf(γω). (29)
Vì α∈Sd−1, dòng thứ ba đúng bằng cách quan
sát rằng khi α1=±1, α2, . . . , α d= 0 và
do đó dòng thứ hai là σ(±t)/2 nhân với một
xung và khi α1̸=±1, dòng thứ hai là
0, chính xác là dòng thứ ba. Bởi các tính chất
xoay của biến đổi Fourier, chúng ta có kết quả
sau cho nơ-ron r(x) =σ(wTx)
KR{r}(α, t) = P even{σ(t)δ(α−w)},(30)
trong đó w∈Sd−1.

của xung Dirac phát sinh do tính đối xứng chẵn của miền Radon. Từ tính đồng nhất
của kích hoạt ReLU, áp dụng biến đổi làm thưa này cho nơ-ron (không ràng buộc)
r(x) =ρ(wTx−b) với (w, b)∈Rd×R cho
D2
tKR{r}(α, t) =∥w∥2δR((α, t)−(ew,eb)), (32)
trong đó ew=w/∥w∥2 và eb=b/∥w∥2. Điều này tương tự như cách D2 là biến đổi làm thưa
cho các nơ-ron đơn biến như trong (13). Toán tử làm thưa đơn giản là đạo hàm bậc hai trong

--- TRANG 13 ---
13
(a) Biểu đồ bề mặt của r(x).
<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t<latexit sha1_base64="hQm0lR/mBmhjzkOfd5AoJhGlXNA=">AAACRnicbVDLSgMxFL1T3+Or6tJNsAgKpcyIr40giCC4UbG10Cklk2ZsMJkZkoxSwnyJX+NO9Av8CXciuDIdu7DWA4Fzz7mXm3vClDOlPe/NKU1MTk3PzM658wuLS8vlldWGSjJJaJ0kPJHNECvKWUzrmmlOm6mkWISc3oR3JwP/5p5KxZL4WvdT2hb4NmYRI1hbqVPeCwTWPSnMeR5UC66INFd5YGSQbwWhMAHmaQ/n6AgNqoe8ivR2p1zxal4BNE78IanAEBed8lfQTUgmaKwJx0q1fC/VbYOlZoTT3A0yRVNM7vAtbVkaY0FV2xTn5WjTKl0UJdK+WKNC/T1hsFCqL0LbWRzw1xuI/3mtTEeHbcPiNNM0Jj+LoowjnaBBVqjLJCWa9y3BRDL7V0R6WGKibaLuyJoiRRWpkVNMKHLXtWH5f6MZJ42dmr9f273crRyfDmObhXXYgC3w4QCO4QwuoA4EHuEJXuDVeXbenQ/n86e15Axn1mAEJfgGXSOyCg==</latexit>KR{r}(↵=w,t) (b) Biến đổi Radon được lọc khi α=w.
<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t

<latexit sha1_base64="6wsDouJ++1/61Ohhkr95xquTDXg=">AAACSXicbVDNSgMxGMzW//Wv6tFLsAgKpeyKqEdBBMGLim2FppRsmm2DSXZNskoJ+yo+jTfRu4/hTXoy3fZg1YHAMPN9fJmJUs60CYIPrzQzOze/sLjkL6+srq2XNzYbOskUoXWS8ETdRVhTziStG2Y4vUsVxSLitBndn4385iNVmiXy1gxS2ha4J1nMCDZO6pRPkMCmr4S9zFG14Jooe5Mjq1C+hyJhEeZpH+cQSfoAR8JTXoVmv1OuBLWgAPxLwgmpgAmuOuUh6iYkE1QawrHWrTBITdtiZRjhNPdRpmmKyT3u0ZajEguq27ZImMNdp3RhnCj3pIGF+nPDYqH1QERussjw2xuJ/3mtzMQnbctkmhkqyfhQnHFoEjiqC3aZosTwgSOYKOb+CkkfK0yMK9WfOlMUqWM9FcVGIvd9V1b4u5q/pHFQC49qh9eHldPzSW2LYBvsgD0QgmNwCi7AFagDAp7BC3gD796r9+l9ecPxaMmb7GyBKZRmvgFhzrOL</latexit>KR{r}(↵6=w,t)

(c) Biến đổi Radon được lọc khi α̸=w.
<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t(d) Biến đổi làm thưa D2
tKR{r}.

Hình 4. Sơ đồ minh họa biến đổi làm thưa của nơ-ron ReLU r(x) =ρ(wTx−b)
với (w, b)∈Sd−1×R. Bản đồ nhiệt là góc nhìn từ trên xuống của một nơ-ron ReLU được mô tả trong (a).

miền Radon được lọc. Ý tưởng chính là biến đổi Radon (được lọc) cho phép chúng ta trích xuất
hàm kích hoạt (đơn biến) từ nơ-ron đa biến và áp dụng biến đổi làm thưa đơn biến
trong biến t. Hình 4 là một sơ đồ minh họa mô tả biến đổi làm thưa của một nơ-ron ReLU.

Câu chuyện bây giờ tương tự như trường hợp đơn biến. Thực vậy, bởi NBT, huấn luyện NN trong (22)

--- TRANG 14 ---
14
với weight decay tương đương với việc giải bài toán tối ưu

min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λKX
k=1|vk|∥wk∥2. (33)

Từ (32) chúng ta thấy rằng ∥D2
tKRfθ∥M=PK
k=1|vk|∥wk∥2, và do đó huấn luyện NN (22) với
weight decay ưu tiên các nghiệm với đạo hàm bậc hai thưa trong miền Radon được lọc. Thước đo
tính thưa này có thể được xem như TV bậc hai trong miền Radon (được lọc). Cho
RBV2(Rd) ký hiệu không gian các hàm trên Rd có biến thiên có giới hạn bậc hai trong miền Radon
(được lọc) (tức là, TV bậc hai trong miền Radon (được lọc) là hữu hạn). Một kết quả chính
liên quan đến RBV2(Rd) là định lý biểu diễn sau cho mạng nơ-ron, lần đầu được chứng minh
trong [30]. Dưới các giả định nhẹ về hàm mất mát, tập nghiệm của bài toán tối ưu

min
f∈RBV2(Rd)NX
n=1L(yn, f(xn)) +λ∥D2
tKRf∥M (34)

được đặc trưng hóa hoàn toàn bởi NNs có dạng

fθ(x) =KX
k=1vkρ(wT
kx−bk) +cTx+c0, (35)

trong đó số lượng nơ-ron nhỏ hơn nghiêm ngặt số lượng dữ liệu ( K < N ) theo nghĩa rằng
tập nghiệm của (34) là một tập lồi đóng có các điểm cực trị có dạng (35) với
K < N (xem [5], [28], [45] cho các cải tiến thêm của kết quả này). Các hàm mất mát thông thường như
lỗi bình phương thỏa mãn các giả định nhẹ. Kết nối bỏ qua cTx+c0 phát sinh vì
không gian không của biến đổi làm thưa là không gian các hàm affine. Do đó, bởi cùng
lập luận được trình bày trong trường hợp đơn biến, NNs đủ rộng ( K≥N) (như trong (35)) được huấn luyện
với weight decay đến các cực tiểu toàn cục chính xác là các hàm tối ưu trong RBV2(Rd).

B. Mạng Nơ-ron Sâu
Cơ chế này đơn giản để mở rộng đến trường hợp mạng nơ-ron sâu (DNNs). Ý tưởng chính
là xét khớp dữ liệu sử dụng các hợp thành của các hàm RBV2. Được chỉ ra trong [31],
[41] rằng dưới các giả định nhẹ về hàm mất mát, một nghiệm của bài toán tối ưu

min
f(1),...,f(L)NX
n=1L(yn, f(L)◦ ··· ◦ f(1)(xn)) +λLX
ℓ=1dℓX
i=1∥D2
tKRf(ℓ)
i∥M (36)

có dạng của một DNN như trong (1), trong đó dℓ là các chiều trung gian trong các hợp thành hàm, thỏa mãn các tính chất sau:
•Số lượng lớp là L+ 1;

--- TRANG 15 ---
15
Hình 5. Một kiến trúc DNN feedforward với các nút cổ chai tuyến tính. Các nút xanh đại diện cho các nơ-ron ReLU, các
nút xám đại diện cho các nơ-ron tuyến tính, và các nút trắng mô tả các đầu vào DNN. Vì các lớp tuyến tính hẹp hơn
các lớp ReLU, kiến trúc này được gọi là DNN với các nút cổ chai tuyến tính.

•Nghiệm thưa theo nghĩa có ít kết nối nơ-ron hoạt động (độ rộng của các
lớp được giới hạn bởi N2);
•Nghiệm có kết nối bỏ qua trong tất cả các lớp;
•Kiến trúc có các nút cổ chai tuyến tính buộc các ma trận trọng số phải có hạng thấp.

Một kiến trúc như vậy được minh họa trong Hình 5. Kết quả cho thấy rằng ReLU DNNs với kết nối bỏ qua
và các nút cổ chai tuyến tính được huấn luyện với một biến thể của weight decay [31, Nhận xét 4.7]
là các nghiệm tối ưu cho việc khớp dữ liệu sử dụng các hợp thành của các hàm RBV2. Các nút cổ chai tuyến tính
có thể được viết như các ma trận trọng số được phân tích (hạng thấp) có dạng W(ℓ)=U(ℓ)V(ℓ). Những
lớp nút cổ chai này tương ứng với các lớp có hàm kích hoạt tuyến tính ( σ(t) =t). Chúng phát sinh
tự nhiên do các hợp thành hàm xuất hiện trong (36). Số lượng nơ-ron trong mỗi
lớp nút cổ chai được giới hạn bởi dℓ. Việc kết hợp các nút cổ chai tuyến tính dạng này trong DNNs
đã được chỉ ra là tăng tốc học tập [1] và tăng độ chính xác [15], tính mạnh mẽ [38], và
hiệu quả tính toán [48] của DNNs.

V. VAI TRÒ CỦA CÁC HÀM KÍCH HOẠT MẠNG NƠ-RON LÀ GÌ?
Trọng tâm chính của bài viết cho đến nay là hàm kích hoạt ReLU ρ(t) = max {0, t}.
Nhiều ý tưởng đã thảo luận trước đó có thể được mở rộng đến một lớp rộng các hàm kích hoạt.
Tính chất của ReLU được khai thác cho đến nay là nó được làm thưa bởi toán tử đạo hàm bậc hai
theo nghĩa rằng D2
tρ=δ. Thực vậy, chúng ta có thể định nghĩa một lớp rộng các không gian hàm nơ-ron
tương tự như RBV2(Rd) bằng cách định nghĩa các không gian được đặc trưng bởi các biến đổi làm thưa khác nhau được khớp

--- TRANG 16 ---
16
[Bảng 1] Các Hàm Kích hoạt Thông thường
Hàm Kích hoạt σ(t) | Đáp ứng Tần số của Biến đổi Làm thưa: bH(ω)
Rectified Linear Unit (ReLU) max{0, t} | − ω2
Truncated Power max{0, t}k/k!, k∈N | (jω)k+1
Sigmoid 1/(1 +e−t) | −1/(2j) · π/sinh(πω)
arctan arctan( t) | πjωe|ω|
Exponential e−|t| | 2/(1 +ω2)

với một hàm kích hoạt. Điều này đòi hỏi việc thay thế D2
t trong (31) bằng một biến đổi làm thưa
tổng quát H. Bảng 1 (được điều chỉnh từ [44]) cung cấp các ví dụ về các hàm kích hoạt thông thường rơi vào
khung này, trong đó mỗi biến đổi làm thưa H được định nghĩa bởi đáp ứng tần số bH(ω) của nó.
Đối với ReLU, chúng ta có H = D2
t và do đó bH(ω) = (j ω)2=−ω2.

Do đó, nhiều kết quả đã thảo luận trước đó có thể được mở rộng trực tiếp đến một lớp rộng
các hàm kích hoạt bao gồm các hàm kích hoạt sigmoid và arctan cổ điển. Chúng tôi lưu ý rằng hiệu ứng
thúc đẩy tính thưa của weight decay phụ thuộc vào tính đồng nhất của hàm kích hoạt trong DNN. Trong khi
ReLU và các hàm kích hoạt truncated power trong Bảng 1 là đồng nhất, các hàm kích hoạt khác thì không.
Điều này cung cấp bằng chứng rằng người ta nên ưu tiên các hàm kích hoạt đồng nhất như ReLU để
khai thác các kết nối chặt chẽ giữa weight decay và tính thưa. Mặc dù hiệu ứng thúc đẩy tính thưa của
weight decay không áp dụng cho các hàm kích hoạt không đồng nhất, các phát biểu tương tự như (31) vẫn
đúng bằng cách xét các nơ-ron với các trọng số đầu vào được ràng buộc có chuẩn đơn vị. Do đó, những
biến đổi làm thưa này khám phá các đổi mới của NNs độ rộng hữu hạn với trọng số đầu vào chuẩn đơn vị.
Do đó, bằng cách chỉ xét các nơ-ron với trọng số đầu vào chuẩn đơn vị, các kết quả chính đặc trưng hóa
các tập nghiệm của các bài toán tối ưu tương tự như (34) và (36) đúng, cung cấp
cái nhìn sâu sắc về các loại hàm được ưu tiên bởi DNNs sử dụng các hàm kích hoạt này.

--- TRANG 17 ---
17
VI. TẠI SAO MẠNG NƠ-RON DƯỜNG NHƯ PHÁ VỠ ĐƯỢC LỜI NGUYỀN CỦA CHIỀU KHÔNG GIAN?
Năm 1993, Barron xuất bản bài báo tinh hoa [4] về khả năng của NNs với các hàm kích hoạt sigmoid
xấp xỉ một loạt các hàm đa biến. Đáng chú ý, ông chỉ ra rằng
NNs có thể xấp xỉ các hàm thỏa mãn các điều kiện phân rã nhất định trên biến đổi Fourier của chúng
với tốc độ hoàn toàn độc lập với chiều đầu vào của các hàm. Tính chất này
đã dẫn đến nhiều người ca ngợi công trình của ông là "phá vỡ lời nguyền của chiều không gian". Ngày nay, các
không gian hàm mà ông nghiên cứu thường được gọi là các không gian Barron phổ. Hóa ra rằng
tính chất xấp xỉ đáng chú ý này của NNs là do tính thưa.

Để giải thích hiện tượng này, trước tiên chúng ta nhớ lại một bài toán "chịu lời nguyền của chiều không gian".
Một bài toán cổ điển trong xử lý tín hiệu là tái tạo một tín hiệu từ các mẫu của nó. Định lý
lấy mẫu của Shannon khẳng định rằng việc lấy mẫu một tín hiệu băng hạn trên một lưới thường xuyên với tốc độ
nhanh hơn tốc độ Nyquist đảm bảo rằng bộ nội suy sinc hoàn hảo tái tạo tín hiệu. Vì
hàm sinc và các dịch chuyển của nó tạo thành một cơ sở trực giao cho không gian các tín hiệu băng hạn, năng lượng
của tín hiệu (chuẩn L2 bình phương) tương ứng với chuẩn (rời rạc) ℓ2 bình phương của các
mẫu của nó. Các phiên bản đa biến của định lý lấy mẫu tương tự và khẳng định rằng việc lấy mẫu
tín hiệu băng hạn đa biến trên một lưới thường xuyên đủ mịn đảm bảo tái tạo hoàn hảo
với nội suy sinc (đa biến). Dễ thấy rằng kích thước lưới (và do đó số lượng
mẫu) tăng theo cấp số nhân với chiều của tín hiệu. Điều này cho thấy rằng việc lấy mẫu và
tái tạo các tín hiệu băng hạn chịu lời nguyền của chiều không gian. Lý do cơ bản
cho điều này là năng lượng hoặc "kích thước" của một tín hiệu băng hạn tương ứng với
chuẩn ℓ2 của các hệ số mở rộng của tín hiệu trong cơ sở sinc.

Hóa ra có một sự khác biệt rõ rệt nếu thay vào đó chúng ta đo "kích thước" của một hàm bằng
chuẩn ℓ1 hạn chế hơn thay vì chuẩn ℓ2, một ý tưởng được phổ biến bởi wavelets và compressed
sensing. Cho D={ψ}ψ∈D là một từ điển các nguyên tử (ví dụ, các hàm sinc, wavelets, nơ-ron, v.v.).
Xét bài toán xấp xỉ một hàm đa biến ánh xạ Rd→R chấp nhận
một phân tích f(x) =P∞
k=1vkψk(x), trong đó ψk∈ D và các hệ số mở rộng thỏa mãn
P∞
k=1|vk|=∥v∥ℓ1<∞. Hóa ra rằng tồn tại một approximant được xây dựng với
K số hạng từ từ điển D có lỗi xấp xỉ L2 ∥f−fK∥L2 phân rã với tốc độ
hoàn toàn độc lập với chiều đầu vào d.

Chúng tôi sẽ minh họa lập luận khi D={ψk}∞
k=1 là một cơ sở trực chuẩn (ví dụ, đa biến
Haar wavelets). Cho một hàm f:Rd→R chấp nhận một phân tích f(x) =P∞
k=1vkψk(x)
sao cho ∥v∥ℓ1<∞, chúng ta có thể xây dựng một approximant fK bằng một thủ tục ngưỡng đơn giản

--- TRANG 18 ---
18
giữ K hệ số lớn nhất của f và đặt tất cả các hệ số khác bằng 0. Nếu chúng ta để |v(1)| ≥
|v(2)| ≥ ··· ký hiệu các hệ số của f được sắp xếp theo độ lớn không tăng, thì lỗi
xấp xỉ bình phương được giới hạn là

∥f−fK∥2
L2=




X
k>Kv(k)ψ(k)




2
L2=X
k>K|v(k)|2, (37)

trong đó đẳng thức cuối được thực hiện bằng cách khai thác tính trực chuẩn của {ψk}∞
k=1. Cuối cùng, vì
dãy hệ số ban đầu v= (v1, v2, . . .) khả tổng tuyệt đối, |v(k)| phải phân rã
nghiêm ngặt nhanh hơn 1/k với k > K (vì đuôi của chuỗi điều hòa P
k>K1
k phân kỳ). Kết hợp
điều này với (37), lỗi xấp xỉ L2 ∥f−fK∥L2 phải phân rã như K−1/2, hoàn toàn
độc lập với chiều đầu vào d. Để có một xử lý chính xác hơn của lập luận này, chúng tôi giới thiệu
độc giả đến Định lý 9.10 trong Wavelet Tour of Signal Processing của Mallat [22]. Những loại
thủ tục ngưỡng này, đặc biệt với các cơ sở wavelet [12], đã cách mạng hóa xử lý tín hiệu và hình ảnh
và là nền tảng của compressed sensing [7], [11].

Bằng một lập luận tinh vi hơn, một hiện tượng tương tự xảy ra khi cơ sở trực chuẩn được
thay thế bằng một từ điển nguyên tử tùy ý về cơ bản. Kết quả cho các nguyên tử tổng quát dựa trên
một kỹ thuật xác suất được trình bày bởi Pisier năm 1981 tại Hội thảo Phân tích Hàm tại
´Ecole Polytechnique, Palaiseau, Pháp, ghi nhận ý tưởng cho Maurey [34]. Một hệ quả của
kỹ thuật Maurey là, cho một hàm là một kết hợp ℓ1 của các nguyên tử bị chặn từ một
từ điển, tồn tại một approximant K-số hạng chấp nhận một tốc độ xấp xỉ không phụ thuộc chiều
phân rã như K−1/2. Được thúc đẩy bởi các thảo luận với Jones về công trình của ông về xấp xỉ tham lam [18],
cung cấp một thuật toán xác định để tìm approximant chấp nhận tốc độ không phụ thuộc chiều, Barron đã sử dụng kỹ thuật của Maurey để chứng minh các tốc độ xấp xỉ không phụ thuộc chiều của ông với
NNs sigmoid. Kết quả xấp xỉ trừu tượng này hiện được gọi là bổ đề Maurey–Jones–Barron.

Đặc biệt, bổ đề Maurey–Jones–Barron có thể được áp dụng cho bất kỳ không gian hàm nào trong đó các
hàm là kết hợp ℓ1 của các nguyên tử bị chặn. Những không gian như vậy đôi khi được gọi là các không gian
biến thiên [2], [20]. Nhớ lại từ Phần IV và V rằng toán tử H KR làm thưa các nơ-ron
có dạng σ(wTx−b), trong đó (w, b)∈Sd−1×R và σ được khớp với H. Điều này ngụ ý rằng
không gian các hàm f:Rd→R sao cho ∥H KRf∥M<∞ có thể được xem như một không gian
biến thiên, trong đó từ điển tương ứng với các nơ-ron {σ(wTx−b)}(w,b)∈Sd−1×R. Do đó,
cho f:Rd→R sao cho ∥H KRf∥M<∞, tồn tại một approximant K-số hạng fK có
dạng của một NN nông với K nơ-ron sao cho lỗi xấp xỉ L2 phân rã

--- TRANG 19 ---
19
như K−1/2. Những kỹ thuật này đã được nghiên cứu và mở rộng chi tiết [42] và đã được
mở rộng đến setting của deep NNs [13] bằng cách xét các không gian hàm hợp thành tương tự như
không gian hợp thành được giới thiệu trong Phần IV-B.

Kết hợp những tốc độ xấp xỉ không phụ thuộc chiều này với hiệu ứng thúc đẩy tính thưa của
chính quy hóa weight decay cho ReLU NNs có một tác động nổi bật đến bài toán học tập. Giả sử
rằng chúng ta huấn luyện một ReLU NN nông với weight decay trên dữ liệu được tạo từ các mẫu nhiễu
yn=f(xn)+εn,n= 1, . . . , N, của f∈RBV2(Rd), trong đó xn là các biến ngẫu nhiên i.i.d. đồng nhất
trên một miền bị chặn Ω⊂Rd và εn là các biến ngẫu nhiên Gaussian i.i.d. Cho fN ký hiệu
NN được huấn luyện này. Khi đó, đã được chỉ ra [32] rằng mean integrated squared error (MISE)
E∥f−fN∥2
L2(Ω) phân rã với tốc độ N−1/2, độc lập với chiều đầu vào d. Hơn nữa, kết quả này
cũng cho thấy rằng lỗi tổng quát hóa của NN được huấn luyện trên một ví dụ mới x được tạo
đồng nhất ngẫu nhiên trên Ω cũng miễn nhiễm với lời nguyền của chiều không gian. Hơn nữa, những
ý tưởng này đã được nghiên cứu trong bối cảnh deep NNs [40], chứng minh các tốc độ MISE không phụ thuộc chiều cho
việc ước lượng các hàm H ¨older (thể hiện cấu trúc chiều thấp) với ReLU DNNs.

A. Mixed Variation và Cấu trúc Chiều thấp
Cuộc họp quốc gia của Hiệp hội Toán học Mỹ năm 2000 được tổ chức để thảo luận về các thách thức
toán học của thế kỷ 21. Ở đây, Donoho đã có một bài giảng có tiêu đề High-Dimensional
Data Analysis: The Curses and Blessings of Dimensionality [10]. Trong bài giảng này, ông đã đặt ra
thuật ngữ "mixed variation" để chỉ các loại hàm nằm trong các không gian biến thiên, trích dẫn các
không gian Barron phổ như một ví dụ. Các không gian biến thiên khác với các không gian hàm đa biến
cổ điển ở chỗ chúng ưu tiên các hàm có biến thiên yếu theo nhiều hướng (các hàm rất
trơn) cũng như các hàm có biến thiên rất mạnh theo một hoặc một vài hướng (các hàm rất
thô). Những không gian này cũng không ưu tiên các hàm có biến thiên mạnh theo nhiều
hướng. Chính thực tế này làm cho chúng khá "nhỏ" so với các không gian hàm đa biến
cổ điển, tạo ra các tốc độ xấp xỉ và MISE không phụ thuộc chiều của chúng. Các ví dụ về các hàm
với các loại biến thiên khác nhau được minh họa trong Hình 6. Các ví dụ điển hình của các hàm
nằm trong các không gian mixed variation có thể được nghĩ như các superposition của ít nơ-ron với các
hướng khác nhau hoặc các superposition của nhiều nơ-ron (thậm chí liên tục nhiều) chỉ trong một vài hướng.

Để diễn giải ý tưởng về mixed variation trong bối cảnh phân tích dữ liệu hiện đại và DL,
chúng ta chuyển sự chú ý đến Hình 6(b). Trong hình này, hàm có biến thiên mạnh, nhưng chỉ trong
một hướng duy nhất. Nói cách khác, hàm này có cấu trúc chiều thấp. Đã được quan sát
bởi một số tác giả rằng DNNs có thể tự động thích nghi với cấu trúc chiều thấp

--- TRANG 20 ---
20
(a) Biến thiên yếu theo nhiều hướng.
 (b) Biến thiên mạnh theo một hướng.
(c) Biến thiên mạnh theo nhiều hướng.
Hình 6. Ví dụ về các hàm thể hiện các loại biến thiên khác nhau.

thường phát sinh trong dữ liệu tự nhiên. Điều này có thể vì các trọng số đầu vào có thể được huấn luyện để điều chỉnh
hướng của mỗi nơ-ron. Tốc độ xấp xỉ độc lập chiều lượng hóa sức mạnh
của khả năng điều chỉnh này. Điều này giải thích tại sao DNNs giỏi học các hàm có cấu trúc chiều thấp.
Đặc biệt, hàm trong Hình 6(c) có biến thiên mạnh theo tất cả các hướng, do đó không có phương pháp nào
có thể vượt qua lời nguyền của chiều không gian trong loại tình huống này. Mặt khác, trong Hình 6(a)
hàm có biến thiên yếu theo tất cả các hướng và Hình 6(b) có biến thiên mạnh chỉ theo một
hướng, do đó đây là những hàm mà mạng nơ-ron sẽ vượt qua lời nguyền. Đối với Hình 6(b)
hiệu ứng thúc đẩy tính thưa của weight decay thúc đẩy các nghiệm DNN với các nơ-ron định hướng
theo hướng biến thiên (tức là, nó tự động học cấu trúc chiều thấp).

VII. THÔNG ĐIỆP CHÍNH VÀ HƯỚNG NGHIÊN CỨU TƯƠNG LAI
Trong bài viết này, chúng tôi đã trình bày một khung toán học để hiểu DNNs từ các nguyên lý cơ bản,
thông qua lăng kính của tính thưa và chính quy hóa thưa. Sử dụng các công cụ toán học quen thuộc từ

--- TRANG 21 ---
21
xử lý tín hiệu, chúng tôi đã cung cấp một giải thích cho hiệu ứng thúc đẩy tính thưa của sơ đồ
chính quy hóa phổ biến weight decay trong huấn luyện mạng nơ-ron, việc sử dụng kết nối bỏ qua
và ma trận trọng số hạng thấp trong kiến trúc mạng, và tại sao mạng nơ-ron dường như phá vỡ
lời nguyền của chiều không gian. Khung này cung cấp bối cảnh toán học cho nhiều hướng nghiên cứu tương lai.

Khung này gợi ý khả năng của các thuật toán huấn luyện nơ-ron mới. Sự tương đương
của các nghiệm sử dụng chính quy hóa weight decay và chính quy hóa trong (6) dẫn đến việc sử dụng
các phương pháp gradient gần đúng tương tự như các thuật toán soft-thresholding lặp để huấn luyện DNNs. Con đường
này đã bắt đầu được khám phá. Các kết quả sơ bộ trong [49] đã chỉ ra rằng các thuật toán
huấn luyện gradient gần đúng cho DNNs hoạt động tốt bằng và thường tốt hơn (đặc biệt khi
nhãn bị hỏng) so với huấn luyện dựa trên gradient tiêu chuẩn với weight decay, đồng thời
tạo ra các mạng thưa hơn.

Cũng đã có một khối lượng lớn công trình từ năm 1989 [21] về tỉa NN đã chỉ ra
một cách thực nghiệm rằng các NNs lớn có thể được nén hoặc làm thưa đến một phần nhỏ kích thước của chúng trong khi vẫn
duy trì hiệu suất dự đoán của chúng. Kết nối giữa weight decay và các regularizers thúc đẩy tính thưa
như trong (6) gợi ý các cách tiếp cận mới cho tỉa. Ví dụ, người ta có thể
áp dụng các thuật toán gradient gần đúng để dẫn xuất các xấp xỉ thưa cho các mạng nơ-ron
được pre-train lớn [41]. Có nhiều câu hỏi mở trong hướng này, cả thực nghiệm và lý thuyết,
bao gồm áp dụng các thuật toán này cho các kiến trúc DNN khác và dẫn xuất các kết quả hội tụ
cho các thuật toán này.

Khung trong bài báo này cũng chỉ ra rằng các ReLU DNNs được huấn luyện là các hợp thành của các hàm RBV2.
Như chúng ta đã thấy trong bài viết này, tại thời điểm này, chúng ta có một sự hiểu biết rõ ràng và gần như
hoàn chỉnh về RBV2(Rd). Đặc biệt, không gian RBV2 ưu tiên các hàm trơn theo hầu hết hoặc tất cả
các hướng, điều này giải thích tại sao mạng nơ-ron dường như phá vỡ lời nguyền của chiều không gian. Ít rõ ràng và
hiểu biết hơn về các hợp thành của các hàm RBV2 (đặc trưng hóa DNNs). Hiểu biết tốt hơn về các không gian hàm
hợp thành có thể cung cấp những cái nhìn sâu sắc mới về lợi ích của độ sâu trong mạng nơ-ron. Điều này
lần lượt có thể dẫn đến các hướng dẫn mới cho việc thiết kế kiến trúc NN và thuật toán huấn luyện.

LỜI CẢM ƠN
Các tác giả xin cảm ơn Rich Baraniuk, Misha Belkin, C ¸a˘gatay Candan, Ron DeVore,
Kangwook Lee, Greg Ongie, Dimitris Papailiopoulos, Tomaso Poggio, Lorenzo Rosasco, Joe

--- TRANG 22 ---
22
Shenouda, Jonathan Siegel, Ryan Tibshirani, Michael Unser, Becca Willett, Stephen Wright, Liu
Yang, và Jifan Zhang cho nhiều cuộc thảo luận sâu sắc về các chủ đề được trình bày trong bài viết này.

RP được hỗ trợ một phần bởi Chương trình Nghiên cứu sinh Tốt nghiệp của Quỹ Khoa học Quốc gia Hoa Kỳ (NSF) dưới grant DGE-1747503 và Hội đồng Nghiên cứu Châu Âu (ERC Project
FunLearn) dưới Grant 101020573. RN được hỗ trợ một phần bởi các grant NSF DMS-2134140
và DMS-2023239, grant ONR MURI N00014-20-1-2787, và grant AFOSR/AFRL
FA9550-18-1-0166, cũng như Giáo sư Keith and Jane Nosbusch.

TÀI LIỆU THAM KHẢO
[1]L. J. Ba và R. Caruana, "Do deep nets really need to be deep?" trong Proceedings of the 27th International
Conference on Neural Information Processing Systems-Volume 2, 2014, pp. 2654–2662.
[2]F. Bach, "Breaking the curse of dimensionality with convex neural networks," The Journal of Machine Learning
Research, vol. 18, no. 1, pp. 629–681, 2017.
[3]R. Balestriero và R. G. Baraniuk, "Mad max: Affine spline insights into deep learning," Proceedings of the
IEEE, vol. 109, no. 5, pp. 704–727, 2020.
[4] A. R. Barron, "Universal approximation bounds for superpositions of a sigmoidal function," IEEE Transactions
on Information theory, vol. 39, no. 3, pp. 930–945, 1993.
[5]F. Bartolucci, E. De Vito, L. Rosasco, và S. Vigogna, "Understanding neural networks with reproducing kernel
Banach spaces," Appl. Comput. Harmon. Anal., vol. 62, pp. 194–236, 2023.
[6] E. J. Cand `es, "Ridgelets: Theory and applications," Ph.D. dissertation, Stanford University, 1998.
[7]E. J. Cand `es, J. Romberg, và T. Tao, "Robust uncertainty principles: Exact signal reconstruction from highly
incomplete frequency information," IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489–509, 2006.
[8]L. Chizat và F. Bach, "Implicit bias of gradient descent for wide two-layer neural networks trained with the
logistic loss," trong Conference on Learning Theory. PMLR, 2020, pp. 1305–1338.
[9]T. Debarre, Q. Denoyelle, M. Unser, và J. Fageot, "Sparsest piecewise-linear regression of one-dimensional
data," Journal of Computational and Applied Mathematics, vol. 406, p. 114044, 2022.
[10] D. L. Donoho, "High-dimensional data analysis: The curses and blessings of dimensionality," AMS Lectures,
p. 32, 2000.
[11] D. L. Donoho, "Compressed sensing," IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289–1306,
2006.
[12] D. L. Donoho và I. M. Johnstone, "Minimax estimation via wavelet shrinkage," The Annals of Statistics, vol. 26,
no. 3, pp. 879–921, 1998.
[13] W. E và S. Wojtowytsch, "On the Banach spaces associated with multi-layer ReLU networks: Function
representation, approximation theory and gradient descent dynamics," CSIAM Transactions on Applied Mathematics,
vol. 1, no. 3, pp. 387–440, 2020.
[14] S. D. Fisher và J. W. Jerome, "Spline solutions to L1extremal problems in one and several variables," Journal
of Approximation Theory, vol. 13, no. 1, pp. 73–83, 1975.
[15] A. Golubeva, B. Neyshabur, và G. Gur-Ari, "Are wider nets better given the same number of parameters?"
International Conference on Learning Representations, 2021.

--- TRANG 23 ---
23
[16] Y. Grandvalet, "Least absolute shrinkage is equivalent to quadratic penalization," trong International Conference on
Artificial Neural Networks. Springer, 1998, pp. 201–206.
[17] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," trong Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 770–778.
[18] L. K. Jones, "A simple lemma on greedy approximation in Hilbert space and convergence rates for projection
pursuit regression and neural network training," The Annals of Statistics, pp. 608–613, 1992.
[19] V. K ˚urkov ´a, P. C. Kainen, và V. Kreinovich, "Estimates of the number of hidden units and variation with respect
to half-spaces," Neural Networks, vol. 10, no. 6, pp. 1061–1068, 1997.
[20] V. K ˚urkov ´a và M. Sanguineti, "Bounds on rates of variable-basis and neural-network approximation," IEEE
Transactions on Information Theory, vol. 47, no. 6, pp. 2659–2665, 2001.
[21] Y. LeCun, J. Denker, và S. Solla, "Optimal brain damage," Advances in neural information processing systems,
vol. 2, 1989.
[22] S. Mallat, A Wavelet Tour of Signal Processing, 3rd ed. Elsevier/Academic Press, Amsterdam, 2009.
[23] E. Mammen và S. van de Geer, "Locally adaptive regression splines," The Annals of Statistics, vol. 25, no. 1,
pp. 387–413, 1997.
[24] W. S. McCulloch và W. Pitts, "A logical calculus of the ideas immanent in nervous activity," The Bulletin of
Mathematical Biophysics, vol. 5, no. 4, pp. 115–133, 1943.
[25] B. Neyshabur, R. Tomioka, và N. Srebro, "In search of the real inductive bias: On the role of implicit regularization
in deep learning." trong International Conference on Learning Representations (Workshop), 2015.
[26] G. Ongie, R. Willett, D. Soudry, và N. Srebro, "A function space view of bounded norm infinite width ReLU
nets: The multivariate case," trong International Conference on Learning Representations, 2020.
[27] A. V. Oppenheim, A. S. Willsky, và S. H. Nawab, Signals & Systems, ser. Prentice-Hall Signal Processing Series.
Prentice Hall, 1997.
[28] R. Parhi, "On Ridge Splines, Neural Networks, and Variational Problems in Radon-Domain BV Spaces," Ph.D.
dissertation, The University of Wisconsin–Madison, 2022.
[29] R. Parhi và R. D. Nowak, "The role of neural network activation functions," IEEE Signal Processing Letters,
vol. 27, pp. 1779–1783, 2020.
[30] R. Parhi và R. D. Nowak, "Banach space representer theorems for neural networks and ridge splines." Journal
of Machine Learning Research, vol. 22, no. 43, pp. 1–40, 2021.
[31] R. Parhi và R. D. Nowak, "What kinds of functions do deep neural networks learn? Insights from variational
spline theory," SIAM Journal on Mathematics of Data Science, vol. 4, no. 2, pp. 464–489, 2022.
[32] R. Parhi và R. D. Nowak, "Near-minimax optimal estimation with shallow ReLU neural networks," IEEE
Transactions on Information Theory, vol. 69, no. 2, pp. 1125–1140, 2023.
[33] M. Pilanci và T. Ergen, "Neural networks are convex regularizers: Exact polynomial-time convex optimization
formulations for two-layer networks," trong International Conference on Machine Learning. PMLR, 2020, pp.
7695–7705.
[34] G. Pisier, "Remarques sur un r ´esultat non publi ´e de B. Maurey," S´eminaire d'Analyse Fonctionnelle (dit "Maurey-
Schwartz"), pp. 1–12, April 1981.
[35] T. Poggio, A. Banburski, và Q. Liao, "Theoretical issues in deep networks," Proceedings of the National Academy
of Sciences, vol. 117, no. 48, pp. 30 039–30 045, 2020.

--- TRANG 24 ---
24
[36] J. Radon, " ¨Uber die bestimmung von funktionen durch ihre integralwerte l ¨angs gewisser mannigfaltigkeiten," Ber.
Verh, Sachs Akad Wiss., vol. 69, pp. 262–277, 1917.
[37] L. I. Rudin, S. Osher, và E. Fatemi, "Nonlinear total variation based noise removal algorithms," Physica D:
nonlinear phenomena, vol. 60, no. 1-4, pp. 259–268, 1992.
[38] A. Sanyal, P. H. Torr, và P. K. Dokania, "Stable rank normalization for improved generalization in neural
networks and GANs," International Conference on Learning Representations, 2019.
[39] P. Savarese, I. Evron, D. Soudry, và N. Srebro, "How do infinite width bounded norm networks look in function
space?" trong Conference on Learning Theory. PMLR, 2019, pp. 2667–2690.
[40] J. Schmidt-Hieber, "Nonparametric regression using deep neural networks with ReLU activation function," The
Annals of Statistics, vol. 48, no. 4, pp. 1875–1897, 2020.
[41] J. Shenouda, R. Parhi, K. Lee, và R. D. Nowak, "Vector-valued variation spaces and width bounds for DNNs:
Insights on weight decay regularization," arXiv preprint arXiv:2305.16534, 2023.
[42] J. W. Siegel và J. Xu, "Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural
networks," Foundations of Computational Mathematics, pp. 1–57, 2022.
[43] R. Tibshirani, "Regression shrinkage and selection via the lasso," Journal of the Royal Statistical Society: Series
B (Methodological), vol. 58, no. 1, pp. 267–288, 1996.
[44] M. Unser, "From kernel methods to neural networks: A unifying variational formulation," arXiv preprint
arXiv:2206.14625, 2022.
[45] M. Unser, "Ridges, neural networks, and the Radon transform," Journal of Machine Learning Research, vol. 24,
no. 37, pp. 1–33, 2023.
[46] M. Unser, J. Fageot, và J. P. Ward, "Splines are universal solutions of linear inverse problems with generalized
TV regularization," SIAM Review, vol. 59, no. 4, pp. 769–793, 2017.
[47] M. Vetterli, P. Marziliano, và T. Blu, "Sampling signals with finite rate of innovation," IEEE Transactions on
Signal Processing, vol. 50, no. 6, pp. 1417–1428, 2002.
[48] H. Wang, S. Agarwal, và D. Papailiopoulos, "Pufferfish: Communication-efficient models at no extra cost,"
Proceedings of Machine Learning and Systems, vol. 3, pp. 365–386, 2021.
[49] L. Yang, J. Zhang, J. Shenouda, D. Papailiopoulos, K. Lee, và R. D. Nowak, "A better way to decay: Proximal
gradient training algorithms for neural nets," trong OPT 2022: Optimization for Machine Learning (NeurIPS Workshop),
2022.
[50] M. Yuan và Y. Lin, "Model selection and estimation in regression with grouped variables," Journal of the Royal
Statistical Society: Series B (Statistical Methodology), vol. 68, no. 1, pp. 49–67, 2006.

Rahul Parhi (rahul.parhi@epfl.ch) nhận bằng B.S. toán học và bằng B.S. khoa học máy tính
từ Đại học Minnesota–Twin Cities năm 2018 và bằng M.S. và Ph.D. kỹ thuật điện từ
Đại học Wisconsin–Madison năm 2019 và 2022, tương ứng. Trong quá trình học Ph.D., anh được hỗ trợ bởi NSF
Graduate Research Fellowship. Hiện tại anh là Nghiên cứu viên Sau tiến sĩ tại Nhóm Hình ảnh Y sinh,
´Ecole Polytechnique F ´ed´erale de Lausanne. Anh chủ yếu quan tâm đến các ứng dụng của phân tích hàm và điều hòa
vào các bài toán trong xử lý tín hiệu và khoa học dữ liệu. Anh là thành viên của IEEE.

--- TRANG 25 ---
25
Robert D. Nowak (rdnowak@wisc.edu) có bằng Ph.D. kỹ thuật điện từ Đại học Wisconsin–
Madison và hiện là Giáo sư Grace Wahba về Khoa học Dữ liệu và Giáo sư Keith and Jane Morgan Nosbusch trong
Kỹ thuật Điện và Máy tính tại cùng tổ chức. Với nghiên cứu tập trung vào xử lý tín hiệu, học máy,
tối ưu hóa, và thống kê, công trình của Nowak về khôi phục tín hiệu thưa và compressed sensing đã giành được nhiều
giải thưởng. Hiện tại ông phục vụ như Biên tập viên Phần cho SIAM Journal on Mathematics of Data Science và
Biên tập viên Cao cấp cho IEEE Journal on Selected Areas in Information Theory, và là IEEE Fellow.
