# 2011.09905.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/regularization/2011.09905.pdf
# Kích thước tệp: 424969 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LOss-Based SensiTivity rEgulaRization:
Hướng tới Mạng Nơ-ron Thưa Sâu
Enzo Tartaglione, Andrea Bragagnolo, Attilio Fiandrotti và Marco Grangetto
Đại học Torino, Ý
Tóm tắt
LOBSTER (LOss-Based SensiTivity rEgulaRization) là một
phương pháp để huấn luyện mạng nơ-ron có cấu trúc thưa.
Để độ nhạy của một tham số mạng là biến thiên của
hàm mất mát đối với biến thiên của tham số đó.
Các tham số có độ nhạy thấp, tức là có ít tác động
đến mất mát khi bị nhiễu loạn, được thu nhỏ và sau đó cắt tỉa để
làm thưa mạng. Phương pháp của chúng tôi cho phép huấn luyện mạng
từ đầu, tức là không cần học sơ bộ hay tua lại.
Các thí nghiệm trên nhiều kiến trúc và bộ dữ liệu cho thấy
tỷ lệ nén cạnh tranh với chi phí tính toán tối thiểu.

1 Giới thiệu
Mạng Nơ-ron Nhân tạo (ANN) đạt được hiệu suất tối ưu
trong nhiều tác vụ với chi phí là cấu trúc phức tạp
với hàng triệu tham số có thể học. Ví dụ,
ResNet (He et al. 2016) bao gồm hàng chục triệu tham số, tăng vọt lên hàng trăm triệu cho VGG-Net (Simonyan và Zisserman 2014). Tuy nhiên, số lượng tham số lớn gây cản trở khả năng triển khai mạng trên thiết bị bị hạn chế bộ nhớ (ví dụ: nhúng, di động), đòi hỏi kiến trúc tinh gọn hơn với ít tham số hơn.

Độ phức tạp của ANN có thể được giảm bằng cách áp đặt cấu trúc mạng thưa. Cụ thể, một số kết nối giữa các nơ-ron có thể được cắt tỉa bằng cách thiết lập các tham số tương ứng về không. Ngoài việc giảm tham số, một số nghiên cứu cũng gợi ý các lợi ích khác từ việc cắt tỉa ANN, như cải thiện hiệu suất trong các tình huống học chuyển giao (Liu, Wang, và Qiao 2017). Các phương pháp phổ biến như (Han et al. 2015), ví dụ, giới thiệu một thuật ngữ chính quy hóa trong hàm chi phí với mục tiêu thu nhỏ một số tham số về không. Tiếp theo, một toán tử ngưỡng xác định các tham số đã thu nhỏ về không, cuối cùng áp đặt cấu trúc thưa mong muốn. Tuy nhiên, các phương pháp như vậy yêu cầu cấu trúc được cắt tỉa phải được huấn luyện sơ bộ thông qua gradient descent chuẩn, điều này cộng thêm vào tổng thời gian học.

Nghiên cứu này đóng góp LOBSTER (LOss-Based SensiTivity rEgulaRization), một phương pháp để học các cấu trúc nơ-ron thưa. Trong bối cảnh này, hãy định nghĩa độ nhạy của tham số của ANN là đạo hàm của hàm mất mát đối với tham số. Một cách trực quan, các tham số có độ nhạy thấp có tác động không đáng kể đến hàm mất mát khi bị nhiễu loạn, và do đó phù hợp để được thu nhỏ mà không ảnh hưởng đến hiệu suất mạng. Thực tế, LOBSTER thu nhỏ về không các tham số có độ nhạy thấp với phương pháp chính quy hóa và cắt tỉa, đạt được cấu trúc mạng thưa.

Đối với tài liệu tương tự (Han, Mao, và Dally 2016; Guo, Yao, và Chen 2016; Gomez et al. 2019), LOBSTER không yêu cầu giai đoạn huấn luyện sơ bộ để học cấu trúc tham chiếu dày đặc để cắt tỉa. Hơn nữa, khác với các phương pháp dựa trên độ nhạy khác, LOBSTER tính toán độ nhạy bằng cách khai thác gradient của hàm mất mát đã có sẵn, tránh các tính toán đạo hàm bổ sung (Mozer và Smolensky 1989; Tartaglione et al. 2018), hoặc đạo hàm bậc hai (LeCun, Denker, và Solla 1990). Các thí nghiệm của chúng tôi, được thực hiện trên các cấu trúc mạng và bộ dữ liệu khác nhau, cho thấy LOBSTER vượt trội hơn một số đối thủ cạnh tranh trong nhiều tác vụ.

Phần còn lại của bài báo này được tổ chức như sau. Trong Mục 2, chúng tôi xem xét tài liệu liên quan về kiến trúc nơ-ron thưa. Tiếp theo, trong Mục 3, chúng tôi mô tả phương pháp của chúng tôi để huấn luyện mạng nơ-ron sao cho cấu trúc của nó là thưa. Chúng tôi cung cấp tổng quan chung về kỹ thuật trong Mục 4. Sau đó, trong Mục 5, chúng tôi thử nghiệm với lược đồ huấn luyện được đề xuất của chúng tôi trên một số ANN sâu trên một số bộ dữ liệu khác nhau. Cuối cùng, Mục 6 đưa ra kết luận đồng thời cung cấp các hướng nghiên cứu tương lai.

2 Các Nghiên cứu Liên quan
Đã được biết rằng nhiều ANN, được huấn luyện trên một số tác vụ, thường bị tham số hóa quá mức (Mhaskar và Poggio 2016; Brutzkus et al. 2018). Có nhiều cách để giảm kích thước của ANN. Trong nghiên cứu này, chúng tôi tập trung vào cái gọi là bài toán cắt tỉa: nó bao gồm việc phát hiện và loại bỏ các tham số khỏi ANN mà không ảnh hưởng quá mức đến hiệu suất của nó. Trong một nghiên cứu gần đây (Frankle và Carbin 2019), người ta quan sát thấy rằng chỉ một số ít tham số thực sự được cập nhật trong quá trình huấn luyện: điều này gợi ý rằng tất cả các tham số khác có thể được loại bỏ khỏi quá trình học mà không ảnh hưởng đến hiệu suất. Mặc dù các phương pháp tương tự đã được thực hiện từ nhiều năm trước (Karnin 1990), phát hiện của họ đã đánh thức sự quan tâm nghiên cứu xung quanh chủ đề như vậy. Rất nhiều nỗ lực được dành cho việc làm cho các cơ chế cắt tỉa hiệu quả hơn: ví dụ, Wang et al. cho thấy rằng một số độ thưa có thể đạt được bằng cách cắt tỉa trọng số ngay từ đầu quá trình huấn luyện (Wang et al. 2020), hoặc Lee et al., với "SNIP" của họ, có thể cắt tỉa trọng số theo cách một lần (Lee, Ajanthan, và Torr 2019). Tuy nhiên, các phương pháp này đạt được độ thưa hạn chế: chiến lược dựa trên cắt tỉa lặp đi lặp lại, khi so sánh với các phương pháp một lần hoặc vài lần, có thể đạt được độ thưa cao hơn (Tartaglione, Bragagnolo, và Grangetto 2020). Mặc dù những tiến bộ công nghệ gần đây làm cho vấn đề này trở nên thực tế và có liên quan bởi cộng đồng hướng tới tối ưu hóa kiến trúc ANN, nó sâu sắc gốc rễ của nó trong quá khứ.

Trong Le Cun et al. (LeCun, Denker, và Solla 1990), thông tin từ đạo hàm bậc hai của hàm lỗi được tận dụng để xếp hạng các tham số của mô hình đã huấn luyện trên cơ sở tính nổi bật: điều này cho phép lựa chọn sự cân bằng giữa kích thước của mạng (về số lượng tham số) và hiệu suất. Trong cùng những năm đó, Mozer và Smolensky đề xuất phương pháp tạo khung xương, một kỹ thuật để xác định, trên một mô hình đã huấn luyện, các nơ-ron ít liên quan hơn, và loại bỏ chúng (Mozer và Smolensky 1989). Điều này được thực hiện bằng cách đánh giá tác động toàn cục của việc loại bỏ một nơ-ron cho trước, được đánh giá như là hình phạt hàm lỗi từ một mô hình đã được huấn luyện trước.

Những tiến bộ công nghệ gần đây cho phép các mô hình ANN trở nên rất lớn, và đặt ra câu hỏi về hiệu quả của các thuật toán cắt tỉa: mục tiêu của kỹ thuật này là đạt được độ thưa cao nhất (tức là tỷ lệ phần trăm tối đa của các tham số được loại bỏ) với mất mát hiệu suất tối thiểu (mất mát độ chính xác từ mô hình "không cắt tỉa"). Hướng tới mục đích này, một số phương pháp khác nhau để cắt tỉa tồn tại.

Các phương pháp dựa trên Dropout tạo thành một khả năng khác để đạt được độ thưa. Ví dụ, Sparse VD dựa vào variational dropout để thúc đẩy độ thưa (Molchanov, Ashukha, và Vetrov 2017), cũng cung cấp một diễn giải Bayesian cho Gaussian dropout. Một phương pháp dựa trên dropout khác là Targeted Dropout (Gomez et al. 2019): ở đó, việc tinh chỉnh mô hình ANN tự củng cố độ thưa của nó bằng cách ngẫu nhiên loại bỏ các kết nối (hoặc toàn bộ đơn vị).

Một số phương pháp để giới thiệu độ thưa trong ANN cố gắng dựa vào bộ điều chỉnh ℓ₀ tối ưu, tuy nhiên, đây là một thước đo không khả vi. Một nghiên cứu gần đây (Louizos, Welling, và Kingma 2017) đề xuất một thước đo proxy khả vi để khắc phục vấn đề này, tuy nhiên, giới thiệu một số chi phí tính toán đáng kể. Có cách tiếp cận tổng thể tương tự, trong một nghiên cứu khác, một bộ điều chỉnh dựa trên group lasso có nhiệm vụ nhóm các bộ lọc trong các lớp tích chập được đề xuất (Wen et al. 2016). Tuy nhiên, kỹ thuật như vậy không thể tổng quát hóa trực tiếp cho các lớp fully-connected cồng kềnh, nơi hầu hết độ phức tạp (như số lượng tham số) nằm ở đó.

Một phương pháp chắc chắn hướng tới cắt tỉa tham số bao gồm việc khai thác bộ điều chỉnh ℓ₂ trong khung chính quy hóa và cắt tỉa. Cụ thể, một thuật ngữ chính quy hóa ℓ₂ chuẩn được bao gồm trong hàm chi phí được tối thiểu hóa (để phạt độ lớn của các tham số): tất cả các tham số giảm xuống dưới một ngưỡng nào đó được xác định về không, do đó học một cấu trúc thưa hơn (Han et al. 2015). Phương pháp như vậy hiệu quả vì chính quy hóa thay thế các vấn đề không ổn định (ill-posed) bằng những vấn đề gần đó và ổn định (well-posed) bằng cách giới thiệu một prior trên các tham số (Groetsch 1993). Tuy nhiên, như một nhược điểm, phương pháp này yêu cầu một quá trình huấn luyện sơ bộ để học giá trị ngưỡng; hơn nữa, tất cả các tham số đều bị phạt một cách mù quáng, bình đẳng bởi chuẩn ℓ₂ của chúng: một số tham số, có thể gây ra lỗi lớn (nếu bị loại bỏ), có thể giảm xuống dưới ngưỡng vì thuật ngữ chính quy hóa: điều này giới thiệu sự không tối ưu cũng như sự không ổn định trong quá trình cắt tỉa. Guo et al. đã cố gắng giải quyết vấn đề này với DNS của họ (Guo, Yao, và Chen 2016): họ đề xuất một quy trình thuật toán để sửa chữa việc cắt tỉa quá mức có thể xảy ra bằng cách cho phép phục hồi các kết nối bị cắt đứt. Chuyển sang các phương pháp thưa hóa không dựa trên cắt tỉa, Soft Weight Sharing (SWS) (Ullrich, Welling, và Meeds 2019) chia sẻ các tham số dư thừa giữa các lớp, dẫn đến ít tham số hơn cần được lưu trữ. Các phương pháp dựa trên chưng cất kiến thức, như Few Samples Knowledge Distillation (FSKD) (Li et al. 2020), cũng là một lựa chọn thay thế để giảm kích thước của một mô hình: có thể huấn luyện thành công một mạng học sinh nhỏ từ một giáo viên lớn hơn, đã được huấn luyện trực tiếp trên tác vụ. Lượng tử hóa cũng có thể được xem xét cho cắt tỉa: Yang et al., ví dụ, đã xem xét vấn đề ba hóa và cắt tỉa một mô hình sâu đã được huấn luyện trước (Yang, He, và Fan 2020). Các phương pháp gần đây khác chủ yếu tập trung vào việc cắt tỉa các lớp tích chập hoặc tận dụng thuật toán tối ưu hóa đàn ong nhân tạo (được gọi là ABCPruner) (Lin et al. 2020) hoặc sử dụng một tập nhỏ đầu vào để đánh giá điểm nổi bật và xây dựng một phân phối lấy mẫu (Liebenwein et al. 2020).

Trong một nghiên cứu gần đây khác (Tartaglione et al. 2018), người ta đề xuất đo lường mức độ đầu ra mạng thay đổi đối với các nhiễu loạn nhỏ của một số tham số, và lặp đi lặp lại chỉ phạt những tham số tạo ra ít hoặc không có mất mát hiệu suất. Tuy nhiên, phương pháp như vậy yêu cầu mạng đã được huấn luyện để đo biến thiên của đầu ra mạng khi một tham số bị nhiễu loạn, tăng tổng thời gian học.

Trong nghiên cứu này, chúng tôi khắc phục hạn chế cơ bản của việc huấn luyện trước mạng, giới thiệu khái niệm độ nhạy dựa trên mất mát: nó chỉ phạt các tham số mà nhiễu loạn nhỏ gây ra ít hoặc không có mất mát hiệu suất tại thời điểm huấn luyện.

3 Chính quy hóa được Đề xuất
Độ nhạy dựa trên Mất mát

ANN thường được huấn luyện thông qua tối ưu hóa dựa trên gradient descent, tức là tối thiểu hóa hàm mất mát. Các phương pháp dựa trên mini-batch mẫu đã trở nên phổ biến vì chúng cho phép tổng quát hóa tốt hơn so với học ngẫu nhiên trong khi chúng hiệu quả về bộ nhớ và thời gian. Trong khung như vậy, một tham số mạng wᵢ được cập nhật theo hướng trung bình tối thiểu hóa mất mát trung bình cho minibatch, tức là sử dụng stochastic gradient descent nổi tiếng hoặc các biến thể của nó. Nếu độ lớn gradient gần bằng không, thì tham số không được sửa đổi. Mục tiêu cuối cùng của chúng tôi là đánh giá mức độ biến thiên của giá trị wᵢ sẽ ảnh hưởng đến lỗi trên đầu ra mạng y. Chúng tôi thực hiện một nỗ lực đầu tiên hướng tới mục đích này bằng cách giới thiệu một nhiễu loạn nhỏ Δwᵢ trên wᵢ và đo biến thiên của y như

Δy = Σₖ |∂yₖ/∂wᵢ| Δwᵢ ≈ Σₖ |∂yₖ/∂wᵢ|                (1)

--- TRANG 2 ---
[Bảng với các giá trị của ∂L/∂wᵢ, sign(∂L/∂wᵢ), sign(w), và η̃ so với λ (> 0)]

Thật không may, việc đánh giá (1) là cụ thể và hạn chế đối với vùng lân cận của đầu ra mạng. Chúng tôi muốn đánh giá trực tiếp lỗi của đầu ra của mô hình ANN trên dữ liệu đã học.

Hướng tới mục đích này, chúng tôi ước tính lỗi trên đầu ra mạng gây ra bởi nhiễu loạn trên wᵢ như:

ΔL ≈ Δwᵢ ∂L/∂y ∂y/∂wᵢ = Δwᵢ ∂L/∂wᵢ                (2)

Việc sử dụng (2) thay cho (1) chuyển trọng tâm từ đầu ra sang lỗi của mạng. Cái sau là thông tin chính xác hơn để đánh giá tác động thực sự của nhiễu loạn của một tham số wᵢ cho trước. Hãy định nghĩa độ nhạy S cho một tham số wᵢ cho trước như

S(L, wᵢ) = |∂L/∂wᵢ|                (3)

Các giá trị S lớn cho thấy biến thiên lớn của hàm mất mát đối với nhiễu loạn nhỏ của wᵢ.

Với định nghĩa độ nhạy trên, chúng ta có thể thúc đẩy cấu trúc thưa bằng cách cắt tỉa các tham số có cả độ nhạy S thấp (tức là, trong một vùng phẳng của gradient hàm mất mát, nơi một nhiễu loạn nhỏ của tham số có tác động không đáng kể đến mất mát) và độ lớn thấp, giữ nguyên những tham số có S lớn. Hướng tới mục đích này, chúng tôi đề xuất quy tắc cập nhật tham số sau để thúc đẩy độ thưa:

wᵢᵗ⁺¹ := wᵢᵗ - η ∂L/∂wᵢᵗ + λ wᵢᵗ ℙ(1 - S(L, wᵢᵗ)/ΣS(L, wᵢᵗ))                (4)

trong đó

ℙ(x) = σ[1 - |x|]                (5)

σ(·) là hàm một bước và λ, α là hai siêu tham số dương.

Quy tắc Cập nhật
Trước đây chúng tôi đã giới thiệu một thước đo cho độ nhạy cũng có thể được sử dụng tại thời điểm huấn luyện. Cụ thể, khi thay (3) vào (4) chúng ta có thể viết lại quy tắc cập nhật như:

wᵢᵗ⁺¹ = wᵢᵗ - η ∂L/∂wᵢᵗ - λℛ(L, wᵢᵗ)                (6)

trong đó

ℛ(y, x) = x ℙ(∂y/∂x)                (7)

(a)
(b)
(c)
(d)

[Hình 1: Tác động của quy tắc cập nhật lên các tham số. Đường đứt nét màu đỏ là tiếp tuyến với hàm mất mát tại điểm đen, màu xanh là đóng góp SGD chuẩn, màu tím là weight decay trong khi màu cam là đóng góp LOBSTER. Ở đây chúng tôi giả định ℙ(∂L/∂wᵢ) = 1.]

Sau một số thao tác đại số, chúng ta có thể viết lại (6) như

wᵢᵗ⁺¹ = wᵢᵗ - η̃ℛ(L, wᵢᵗ) + η ∂L/∂wᵢᵗ [sign(∂L/∂wᵢᵗ) - λ̃ℛ(L, wᵢᵗ)]                (8)

Trong (8), chúng ta quan sát hai thành phần khác nhau của thuật ngữ chính quy hóa được đề xuất:

• một thuật ngữ giống weight decay λℛ(L, wᵢ) được bật/tắt bởi độ lớn của gradient trên tham số;
• một thuật ngữ sửa chỉnh cho tỷ lệ học. Cụ thể, toàn bộ quá trình học theo một tỷ lệ học tương đương

η̃ = η [sign(∂L/∂wᵢ) - λℛ(L, wᵢ)]                (9)

Hãy phân tích các sửa chỉnh trong tỷ lệ học. Nếu |∂L/∂wᵢ| ≥ α (wᵢ có độ nhạy lớn), thì ℙ(∂L/∂wᵢ) = 0 và ℛ(L, wᵢ) = 0 và đóng góp chủ đạo đến từ gradient. Trong trường hợp này quy tắc cập nhật của chúng tôi giảm về GD cổ điển:

wᵢᵗ⁺¹ = wᵢᵗ - η ∂L/∂wᵢᵗ                (10)

Khi chúng ta xem xét wᵢ ít nhạy hơn với |∂L/∂wᵢ| < α, chúng ta có ℛ(L, wᵢ) = λwᵢ (thuật ngữ weight decay) và chúng ta có thể phân biệt hai trường hợp con cho tỷ lệ học:

--- TRANG 3 ---
• nếu sign(∂L/∂wᵢ) = sign(wᵢ), thì η̃ < η (Hình 1a và Hình 1d),
• nếu sign(∂L/∂wᵢ) ≠ sign(wᵢ), thì η̃ > η (Hình 1b và Hình 1c).

Sơ đồ của tất cả các trường hợp này có thể được tìm thấy trong Bảng 1 và biểu diễn của các tác động có thể được hiển thị trong Hình 1.

Đóng góp đến từ λℛ(L, wᵢ) nhằm tối thiểu hóa độ lớn tham số, bất chấp việc tối thiểu hóa mất mát. Nếu việc tối thiểu hóa mất mát có xu hướng tối thiểu hóa độ lớn cũng như vậy, thì tỷ lệ học tương đương được giảm. Ngược lại, khi gradient descent có xu hướng tăng độ lớn, tỷ lệ học được tăng lên, để bù đắp đóng góp đến từ λℛ(L, wᵢ). Cơ chế này cho phép chúng ta thành công trong nhiệm vụ học trong khi giới thiệu độ thưa.

Trong phần tiếp theo, chúng tôi sẽ chi tiết về chiến lược huấn luyện tổng thể, bao gồm giai đoạn học và giai đoạn cắt tỉa.

4 Quy trình Huấn luyện
Phần này mô tả một quy trình để huấn luyện mạng nơ-ron thưa N tận dụng quy tắc dựa trên độ nhạy ở trên để cập nhật các tham số mạng. Chúng tôi giả định rằng các tham số đã được khởi tạo ngẫu nhiên, mặc dù quy trình cũng áp dụng nếu mạng đã được huấn luyện trước. Quy trình được minh họa trong Hình 2a và lặp lại qua hai giai đoạn như sau.

Giai đoạn Học
Trong giai đoạn học, ANN được huấn luyện lặp đi lặp lại theo quy tắc cập nhật (4) trên một tập huấn luyện nào đó. Để e biểu thị lần lặp giai đoạn học hiện tại (tức là, epoch) và Nₑ đại diện cho mạng (tức là, tập hợp các tham số có thể học) ở cuối lần lặp thứ e. Cũng để Lₑ là mất mát được đo trên một tập validation nào đó ở cuối lần lặp thứ e và L̂ là mất mát tốt nhất (thấp nhất) được đo cho đến nay trên N̂ (mạng có mất mát validation thấp nhất cho đến nay). Như điều kiện ban đầu, chúng tôi giả định, N̂ = N₀. Nếu Lₑ < L̂, tham chiếu đến mạng tốt nhất được cập nhật như N̂ = Nₑ, L̂ = Lₑ. Chúng tôi lặp lại giai đoạn học N lần cho đến khi mất mát validation tốt nhất Lₑ không giảm trong PWE lần lặp của giai đoạn học liên tiếp (chúng tôi nói bộ điều chỉnh đã đạt đến một cao nguyên hiệu suất). Tại thời điểm đó, chúng tôi chuyển sang giai đoạn cắt tỉa.

Chúng tôi cung cấp N̂ như đầu vào cho giai đoạn cắt tỉa, nơi một số tham số đã được thu nhỏ về không bởi bộ điều chỉnh dựa trên độ nhạy của chúng tôi.

Giai đoạn Cắt tỉa
Tóm lại, trong giai đoạn cắt tỉa, các tham số có độ lớn dưới giá trị ngưỡng T được xác định về không, cuối cùng làm thưa cấu trúc mạng như được hiển thị trong Hình 2b. Cụ thể, chúng tôi tìm kiếm T lớn nhất làm xấu đi mất mát phân loại L̂ nhiều nhất bằng một lượng tương đối TWT:

L_b = (1 + TWT)L̂                (11)

trong đó L_b được gọi là ranh giới mất mát. T được tìm thấy bằng cách sử dụng phương pháp chia đôi, khởi tạo T với độ lớn trung bình của các tham số khác không trong mạng. Sau đó, chúng tôi áp dụng ngưỡng T cho N̂ thu được mạng đã cắt tỉa N′ với mất mát L′ của nó trên tập validation. Tại lần lặp cắt tỉa tiếp theo, chúng tôi cập nhật T như sau:

• nếu L_b ≥ L′ mạng chịu đựng được nhiều tham số hơn bị cắt tỉa, vì vậy T được tăng lên;
• nếu L_b < L′ thì quá nhiều tham số đã bị cắt tỉa và chúng ta cần khôi phục một số: chúng ta giảm T.

Giai đoạn cắt tỉa kết thúc khi L_b = L′ và chúng ta quan sát rằng L_b < L′ cho bất kỳ ngưỡng mới T + ε với ε > 0. Một khi T được tìm thấy, tất cả các tham số có độ lớn dưới T được xác định về không, tức là chúng bị cắt tỉa vĩnh viễn. Nếu ít nhất một tham số đã bị cắt tỉa trong lần lặp cuối cùng của giai đoạn cắt tỉa, một lần lặp mới của giai đoạn chính quy hóa sẽ theo sau; nếu không, quy trình kết thúc trả về mạng đã huấn luyện, thưa.

5 Kết quả
Trong phần này, chúng tôi đánh giá thực nghiệm LOBSTER trên nhiều kiến trúc và bộ dữ liệu thường được sử dụng làm chuẩn trong tài liệu:

• LeNet-300 trên MNIST (Hình 3a),
• LeNet-5 trên MNIST (Hình 3b),
• LeNet-5 trên Fashion-MNIST (Hình 3c),
• ResNet-32 trên CIFAR-10 (Hình 3d),
• ResNet-18 trên ImageNet (Hình 3e),
• ResNet-101 trên ImageNet (Hình 3f).

Chúng tôi so sánh với các phương pháp tối ưu khác được giới thiệu trong Mục 2 bất cứ khi nào có số liệu công khai. Bên cạnh những điều này, chúng tôi cũng thực hiện một nghiên cứu loại bỏ với bộ điều chỉnh dựa trên ℓ₂ và chiến lược cắt tỉa được đề xuất của chúng tôi (như đã thảo luận trong Mục 4). Hiệu suất được đo như độ thưa mô hình đạt được so với lỗi phân loại (lỗi Top-1 hoặc Top-5). Độ thưa mạng được định nghĩa ở đây là tỷ lệ phần trăm của các tham số đã cắt tỉa trong mô hình ANN. Các thuật toán của chúng tôi được triển khai bằng Python, sử dụng PyTorch 1.2 và các mô phỏng được chạy trên GPU RTX2080 TI NVIDIA. Tất cả các siêu tham số đã được điều chỉnh thông qua grid-search. Kích thước tập validation cho tất cả các thí nghiệm là 5k.¹ Đối với tất cả các bộ dữ liệu, các giai đoạn học và cắt tỉa diễn ra trên một phần chia ngẫu nhiên của tập huấn luyện, trong khi các số được báo cáo dưới đây liên quan đến tập kiểm tra.

LeNet-300 trên MNIST
Như một thí nghiệm đầu tiên, chúng tôi huấn luyện một kiến trúc LeNet-300 thưa (LeCun et al. 1998), bao gồm ba lớp fully-connected với 300, 100 và 10 nơ-ron tương ứng. Chúng tôi huấn luyện mạng trên bộ dữ liệu MNIST, gồm 60k ảnh huấn luyện và 10k ảnh kiểm tra thang xám 28×28 pixel, mô tả các chữ số viết tay. Bắt đầu từ một mạng được khởi tạo ngẫu nhiên, chúng tôi huấn luyện LeNet-300 thông qua SGD với tỷ lệ học η = 0.1, λ = 10⁻⁴, PWE = 20 epoch và TWT = 0.05.

Tài liệu liên quan báo cáo một số kết quả nén có thể được nhóm thành hai nhóm tương ứng với tỷ lệ lỗi phân loại khoảng 1.65% và 1.95%, tương ứng. Hình 3a cung cấp kết quả cho quy trình được đề xuất. Phương pháp của chúng tôi đạt được độ thưa cao hơn so với các phương pháp được tìm thấy trong tài liệu. Điều này đặc biệt đáng chú ý xung quanh 1.65% lỗi phân loại (trái dưới trong Hình 3a), nơi chúng tôi đạt được gần như gấp đôi độ thưa của phương pháp tốt thứ hai. LOBSTER cũng đạt được độ thưa cao nhất cho dải lỗi cao hơn (phía bên phải của đồ thị), đặc biệt giành được về số lượng tham số được loại bỏ khỏi lớp fully-connected đầu tiên (lớp lớn nhất, bao gồm 235k tham số), trong đó chúng tôi quan sát chỉ 0.59% tham số sống sót.

LeNet-5 trên MNIST và Fashion-MNIST
Tiếp theo, chúng tôi thí nghiệm trên phiên bản caffe của kiến trúc LeNet-5, bao gồm hai lớp tích chập và hai lớp fully-connected. Một lần nữa, chúng tôi sử dụng một mạng được khởi tạo ngẫu nhiên, được huấn luyện thông qua SGD với tỷ lệ học η = 0.1, λ = 10⁻⁴, PWE = 20 epoch và TWT = 0.05. Kết quả được hiển thị trong Hình 3b. Ngay cả với kiến trúc tích chập, chúng tôi thu được một mạng cạnh tranh nhỏ với độ thưa 99.57%. Ở tỷ lệ nén cao hơn, Sparse VD hơi vượt trội hơn tất cả các phương pháp khác trong thí nghiệm LeNet5-MNIST. Chúng tôi quan sát rằng LOBSTER, trong thí nghiệm này, làm thưa lớp tích chập đầu tiên (≈22% độ thưa) nhiều hơn so với giải pháp Sparse VD (≈33%). Cụ thể, LOBSTER cắt tỉa 14 bộ lọc trong số 20 bộ lọc ban đầu trong lớp đầu tiên (hoặc nói cách khác, chỉ 6 bộ lọc sống sót, và chứa tất cả các tham số chưa bị cắt tỉa). Chúng tôi giả thuyết rằng, trong trường hợp Sparse VD và cho bộ dữ liệu cụ thể này, việc trích xuất một loạt các đặc trưng lớn hơn tại lớp tích chập đầu tiên, vừa làm dễ dàng nhiệm vụ phân loại (do đó lỗi Top-1 thấp hơn) và cho phép loại bỏ nhiều tham số hơn trong các lớp tiếp theo (một độ thưa được cải thiện một chút). Tuy nhiên, vì chúng tôi trên 99% độ thưa, sự khác biệt giữa hai kỹ thuật là tối thiểu.

Để tăng độ khó của nhiệm vụ huấn luyện, chúng tôi thí nghiệm trên việc phân loại bộ dữ liệu Fashion-MNIST (Xiao, Rasul, và Vollgraf 2017), một lần nữa sử dụng LeNet5. Bộ dữ liệu này có cùng kích thước và định dạng ảnh của bộ dữ liệu MNIST, nhưng nó chứa hình ảnh của các mặt hàng quần áo, dẫn đến phân phối không thưa của giá trị cường độ pixel. Vì các hình ảnh không thưa như vậy, bộ dữ liệu này nổi tiếng khó phân loại hơn MNIST. Đối với thí nghiệm này, chúng tôi huấn luyện mạng từ đầu sử dụng SGD với η = 0.1, λ = 10⁻⁴, PWE = 20 epoch và TWT = 0.1. Kết quả được hiển thị trong Hình 3c.

F-MNIST là một bộ dữ liệu thách thức hơn về bản chất so với MNIST, vì vậy độ thưa có thể đạt được thấp hơn. Tuy nhiên, phương pháp được đề xuất vẫn đạt được độ thưa cao hơn so với các phương pháp khác, loại bỏ tỷ lệ phần trăm tham số cao hơn, đặc biệt trong các lớp fully connected, trong khi duy trì khả năng tổng quát hóa tốt. Trong trường hợp này, chúng tôi quan sát rằng lớp đầu tiên là lớp ít được thưa hóa nhất: đây là một tác động của độ phức tạp cao hơn của nhiệm vụ phân loại, đòi hỏi nhiều đặc trưng hơn được trích xuất.

ResNet-32 trên CIFAR-10
Để đánh giá cách phương pháp của chúng tôi mở rộng cho các kiến trúc hiện đại, sâu hơn, chúng tôi áp dụng nó trên một triển khai PyTorch của mạng ResNet-32 (He et al. 2015) phân loại bộ dữ liệu CIFAR-10.² Bộ dữ liệu này bao gồm 60k ảnh RGB 32×32 được chia thành 10 lớp (50k ảnh huấn luyện và 10k ảnh kiểm tra). Chúng tôi huấn luyện mạng sử dụng SGD với momentum = 0.9, λ = 10⁻⁶, PWE = 10 và TWT = 0. Toàn bộ quá trình huấn luyện được thực hiện trong 11k epoch. Phương pháp của chúng tôi hoạt động tốt trong nhiệm vụ này và vượt trội hơn các kỹ thuật tối ưu khác. Hơn nữa, LOBSTER cải thiện khả năng tổng quát hóa mạng bằng cách giảm lỗi Top-1 cơ sở từ 7.37% xuống 7.33% của mạng thưa hóa trong khi loại bỏ 80.11% tham số. Hiệu ứng này rất có thể do chính kỹ thuật LOBSTER, tự điều chỉnh

² https://github.com/akamaster/pytorch_resnet_cifar10

--- TRANG 4 ---
[Các biểu đồ hiệu suất (lỗi Top-1) so với tỷ lệ tham số đã cắt tỉa cho LOBSTER và các phương pháp tối ưu khác trên các kiến trúc và bộ dữ liệu khác nhau]

--- TRANG 5 ---
[Bảng 2: So sánh giữa LOBSTER và ℓ₂+cắt tỉa như trong Hình 3 (chỉ báo cáo kết quả độ thưa tốt nhất)]

tác động chính quy hóa trên các tham số như được giải thích trong Mục 3.

ResNet trên ImageNet
Cuối cùng, chúng tôi tiếp tục mở rộng cả đầu ra và độ phức tạp của bài toán phân loại bằng cách kiểm tra phương pháp được đề xuất trên mạng trên bộ dữ liệu ImageNet nổi tiếng (ILSVRC-2012), bao gồm hơn 1.2 triệu ảnh huấn luyện, tổng cộng 1k lớp. Đối với thử nghiệm này, chúng tôi sử dụng SGD với momentum = 0.9, λ = 10⁻⁶ và TWT = 0. Toàn bộ quá trình huấn luyện kéo dài 95 epoch. Do hạn chế về thời gian, chúng tôi quyết định sử dụng mạng đã được huấn luyện trước được cung cấp bởi thư viện torchvision.³ Hình 3e hiển thị kết quả cho ResNet-18 trong khi Hình 3f hiển thị kết quả cho ResNet-101. Ngay cả trong kịch bản này, LOBSTER chứng minh đặc biệt hiệu quả: chúng tôi có thể loại bỏ, không có mất mát hiệu suất, 37.04% tham số từ ResNet-18 và 81.58% từ ResNet-101.

Nghiên cứu loại bỏ
Như một nghiên cứu loại bỏ cuối cùng, chúng tôi thay thế bộ điều chỉnh dựa trên độ nhạy của chúng tôi bằng một bộ điều chỉnh ℓ₂ đơn giản hơn trong lược đồ học của chúng tôi trong Hình 2. Lược đồ như vậy "ℓ₂+cắt tỉa" áp dụng đồng nhất hình phạt ℓ₂ cho tất cả các tham số bất kể đóng góp của chúng vào mất mát. Lược đồ này có thể so sánh với (Han et al. 2015), nhưng được tăng cường với cùng chiến lược cắt tỉa với ngưỡng thích ứng được hiển thị trong Hình 2b. So sánh giữa LOBSTER và ℓ₂+cắt tỉa được báo cáo trong Bảng 2. Trong tất cả các thí nghiệm, chúng tôi quan sát rằng việc loại bỏ bộ điều chỉnh dựa trên độ nhạy làm suy giảm hiệu suất. Thí nghiệm này xác minh vai trò của chính quy hóa dựa trên độ nhạy trong hiệu suất của lược đồ của chúng tôi. Cuối cùng, Bảng 2 cũng báo cáo độ phức tạp suy luận tương ứng tính bằng FLOP. Đối với cùng hoặc thấp hơn lỗi Top-1, LOBSTER mang lại lợi ích như ít hoạt động hơn tại thời điểm suy luận và gợi ý sự hiện diện của một số cấu trúc trong độ thưa đạt được bởi LOBSTER.

³ https://pytorch.org/docs/stable/torchvision/models.html

6 Kết luận
Chúng tôi đã trình bày LOBSTER, một phương pháp chính quy hóa phù hợp để huấn luyện mạng nơ-ron với cấu trúc thưa mà không cần huấn luyện sơ bộ. Khác với chính quy hóa ℓ₂, LOBSTER nhận thức được đóng góp toàn cục của tham số vào hàm mất mát và tự điều chỉnh tác động chính quy hóa trên tham số tùy thuộc vào các yếu tố như kiến trúc ANN hoặc bài toán huấn luyện (nói cách khác, bộ dữ liệu). Hơn nữa, việc điều chỉnh các siêu tham số của nó dễ dàng và ngưỡng tối ưu cho cắt tỉa tham số được tự xác định bởi phương pháp được đề xuất sử dụng tập validation. LOBSTER đạt được kết quả cạnh tranh từ các kiến trúc nông như LeNet-300 và LeNet-5 đến các cấu trúc sâu hơn như ResNet trên ImageNet. Trong những kịch bản này, chúng tôi đã quan sát sự thúc đẩy được cung cấp bởi phương pháp chính quy hóa được đề xuất hướng tới các phương pháp ít nhận thức hơn như chính quy hóa ℓ₂, về mặt độ thưa đạt được.

Nghiên cứu tương lai bao gồm việc mở rộng LOBSTER để đạt được độ thưa với một cấu trúc và một đánh giá kỹ lưỡng về tiết kiệm về dung lượng bộ nhớ.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo tiếng Anh được giữ nguyên như trong bản gốc]
