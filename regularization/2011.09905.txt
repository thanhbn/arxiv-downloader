# 2011.09905.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/regularization/2011.09905.pdf
# File size: 424969 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LOss-Based SensiTivity rEgulaRization:
Towards Deep Sparse Neural Networks
Enzo Tartaglione, Andrea Bragagnolo, Attilio Fiandrotti and Marco Grangetto
University of Torino, Italy
Abstract
LOBSTER (LOss-Based SensiTivity rEgulaRization) is a
method for training neural networks having a sparse topology.
Let the sensitivity of a network parameter be the variation of
the loss function with respect to the variation of the param-
eter. Parameters with low sensitivity, i.e. having little impact
on the loss when perturbed, are shrunk and then pruned to
sparsify the network. Our method allows to train a network
from scratch, i.e. without preliminary learning or rewind-
ing. Experiments on multiple architectures and datasets show
competitive compression ratios with minimal computational
overhead.
1 Introduction
Artiﬁcial Neural Networks (ANNs) achieve state-of-the-art
performance in several tasks at the price of complex topolo-
gies with millions of learnable parameters. As an example,
ResNet (He et al. 2016) includes tens of millions of pa-
rameters, soaring to hundreds of millions for VGG-Net (Si-
monyan and Zisserman 2014). A large parameter count jeop-
ardizes however the possibility to deploy a network over a
memory-constrained (e.g., embedded, mobile) device, call-
ing for leaner architectures with fewer parameters.
The complexity of ANNs can be reduced enforcing a sparse
network topology. Namely, some connections between neu-
rons can be pruned by wiring the corresponding parameters
to zero. Besides the reduction of parameters, some works
also suggested other beneﬁts coming from pruning ANNs,
like improving the performance in transfer learning sce-
narios (Liu, Wang, and Qiao 2017). Popular methods such
as (Han et al. 2015), for example, introduce a regularization
term in the cost function with the goal to shrink to zero some
parameters. Next, a threshold operator pinpoints the shrunk
parameters to zero, eventually enforcing the sought sparse
topology. However, such methods require that the topology
to be pruned has been preliminarily pruned via standard gra-
dient descent, which sums up to the total learning time.
This work contributes LOBSTER ( LOss-Based SensiTivity
rEgulaRization ), a method for learning sparse neural topolo-
gies. In this context, let us deﬁne the sensitivity of the param-
eter of an ANN as the derivative of the loss function with
respect to the parameter. Intuitively, low-sensitivity parame-
ters have a negligible impact on the loss function when per-
turbed, and so are ﬁt to be shrunk without compromisingthe network performance. Practically, LOBSTER shrinks
to zero parameters with low sensitivity with a regularize-
and-prune approach, achieving a sparse network topology.
With respect to similar literature (Han, Mao, and Dally
2016; Guo, Yao, and Chen 2016; Gomez et al. 2019), LOB-
STER does not require a preliminary training stage to learn
the dense reference topology to prune. Moreover, differ-
ently to other sensitivity-based approaches, LOBSTER com-
putes the sensitivity exploiting the already available gradi-
ent of the loss function, avoiding additional derivative com-
putations (Mozer and Smolensky 1989; Tartaglione et al.
2018), or second-order derivatives (LeCun, Denker, and
Solla 1990). Our experiments, performed over different net-
work topologies and datasets, show that LOBSTER outper-
forms several competitors in multiple tasks.
The rest of this paper is organized as follows. In Sec. 2 we
review the relevant literature concerning sparse neural archi-
tectures. Next, in Sec. 3 we describe our method for training
a neural network such that its topology is sparse. We pro-
vide a general overview on the technique in Sec. 4. Then,
in Sec. 5 we experiment with our proposed training scheme
over some deep ANNs on a number of different datasets. Fi-
nally, Sec. 6 draws the conclusions while providing further
directions for future research.
2 Related Works
It is well known that many ANNs, trained on some tasks,
are typically over-parametrized (Mhaskar and Poggio 2016;
Brutzkus et al. 2018). There are many ways to reduce the
size of an ANN. In this work we focus on the so-called
pruning problem: it consists in detecting and removing pa-
rameters from the ANN without excessively affecting its
performance. In a recent work (Frankle and Carbin 2019),
it has been observed that only a few parameters are ac-
tually updated during training: this suggests that all the
others parameters can be removed from the learning pro-
cess without affecting the performance. Despite similar ap-
proaches were already taken years earlier (Karnin 1990),
their ﬁnding woke-up the research interest around such a
topic. Lots of efforts are devoted towards making pruning
mechanisms more efﬁcient: for example, Wang et al. show
that some sparsity is achievable pruning weights at the very
beginning of the training process (Wang et al. 2020), or
Lee et al., with their “SNIP”, are able to prune weights in aarXiv:2011.09905v1  [cs.LG]  16 Nov 2020

--- PAGE 2 ---
one-shot fashion (Lee, Ajanthan, and Torr 2019). However,
these approaches achieve limited sparsity: iterative pruning-
based strategy, when compared to one-shot or few-shot ap-
proaches, are able to achieve a higher sparsity (Tartaglione,
Bragagnolo, and Grangetto 2020). Despite the recent tech-
nology advances make this problem actual and relevant by
the community towards the ANN architecture optimization,
it deepens its roots in the past.
In Le Cun et al. (LeCun, Denker, and Solla 1990), the infor-
mation from the second order derivative of the error func-
tion is leveraged to rank the the parameters of the trained
model on a saliency basis: this allows to select a trade-off
between size of the network (in terms of number of param-
eters) and and performance. In the same years, Mozer and
Smolensky proposed skeletonization , a technique to iden-
tify, on a trained model, the less relevant neurons, and to
remove them (Mozer and Smolensky 1989). This is accom-
plished evaluating the global effect of removing a given neu-
ron, evaluated as error function penalty from a pre-trained
model.
The recent technological advances let ANN models to be
very large, and pose questions about the efﬁciency of prun-
ing algorithms: the target of the technique is to achieve the
highest sparsity (ie. the maximum percentage of removed
parameters) having minimal performance loss (accuracy loss
from the “un-pruned” model). Towards this end, a number of
different approaches to pruning exists.
Dropout-based approaches constitute another possibility to
achieve sparsity. For example, Sparse VD relies on varia-
tional dropout to promote sparsity (Molchanov, Ashukha,
and Vetrov 2017), providing also a Bayesian interpretation
for Gaussian dropout. Another dropout-based approach is
Targeted Dropout (Gomez et al. 2019): there, ﬁne-tuning the
ANN model is self-reinforcing its sparsity by stochastically
dropping connections (or entire units).
Some approaches to introduce sparsity in ANNs attempt to
rely on the optimal `0regularizer which, however, is a non-
differentiable measure. A recent work (Louizos, Welling,
and Kingma 2017) proposes a differentiable proxy measure
to overcome this problem introducing, though, some rele-
vant computational overhead. Having a similar overall ap-
proach, in another work, a regularizer based on group lasso
whose task is to cluster ﬁlters in convolutional layers is pro-
posed (Wen et al. 2016). However, such a technique is not
directly generalizeable to the bulky fully-connected layers,
where most of the complexity (as number of parameters)
lies.
A sound approach towards pruning parameters consists in
exploiting a `2regularizer in a shrink-and-prune framework.
In particular, a standard `2regularization term is included
in the minimized cost function (to penalize the magnitude
of the parameters): all the parameters dropping below some
threshold are pinpointed to zero, thus learning a sparser
topology (Han et al. 2015). Such approach is effective since
regularization replaces unstable (ill-posed) problems with
nearby and stable (well-posed) ones by introducing a prior
on the parameters (Groetsch 1993). However, as a draw-
back, this method requires a preliminary training to learn the
threshold value; furthermore, all the parameters are blindly,equally-penalized by their `2norm: some parameters, which
can introduce large error (if removed), might drop below the
threshold because of the regularization term: this introduces
sub-optimalities as well as instabilities in the pruning pro-
cess. Guo et al. attempted to address this issue with their
DNS (Guo, Yao, and Chen 2016): they proposed an algo-
rithmic procedure to corrects eventual over-pruning by en-
abling the recovery of severed connections. Moving to spar-
siﬁcation methods not based on pruning, Soft Weight Shar-
ing(SWS) (Ullrich, Welling, and Meeds 2019) shares re-
dundant parameters among layers, resulting in fewer param-
eters to be stored. Approaches based on knowledge distilla-
tion, like Few Samples Knowledge Distillation (FSKD) (Li
et al. 2020), are also an alternative to reduce the size of
a model: it is possible to successfully train a small stu-
dent network from a larger teacher, which has been directly
trained on the task. Quantization can also be considered for
pruning: Yang et al., for example, considered the problem
of ternarizing and prune a pre-trained deep model (Yang,
He, and Fan 2020). Other recent approaches mainly focus
on the pruning of convolutional layers either leveraging on
the artiﬁcial bee colony optimization algorithm (dubbed as
ABCPruner) (Lin et al. 2020) or using a small set of input to
evaluate a saliency score and construct a sampling distribu-
tion (Liebenwein et al. 2020).
In another recent work (Tartaglione et al. 2018), it was pro-
posed to measure how much the network output changes for
small perturbations of some parameters, and to iteratively
penalize just those which generate little or no performance
loss. However, such method requires the network to be al-
ready trained so to measure the variation of the network
output when a parameter is perturbed, increasing the over-
all learning time.
In this work, we overcome the basic limitation of pre-
training the network, introducing the concept of loss-based
sensitivity : it only penalizes the parameters whose small per-
turbation introduces little or no performance loss at training
time.
3 Proposed Regularization
Loss-based Sensitivity
ANNs are typically trained via gradient descent based opti-
mization, i.e. minimizing the loss function . Methods based
on mini-batches of samples have gained popularity as they
allow better generalization than stochastic learning while
they are memory and time efﬁcient. In such a framework,
a network parameter wiis updated towards the averaged di-
rection which minimizes the averaged loss for the minibatch,
i.e. using the well known stochastic gradient descent or its
variations. If the gradient magnitude is close to zero, then
the parameter is not modiﬁed. Our ultimate goal is to assess
to which extent a variation of the value of wiwould affect
the error on the network output y. We make a ﬁrst attempt
towards this end introducing a small perturbation wiover
wiand measuring the variation of yas
y=X
kjykjwiX
k@yk
@wi: (1)

--- PAGE 3 ---
P
@L
@w i
sign
@L
@w i
sign(w)~

0 any any 1
1 + + 1
1 + - 1
1 - + 1
1 - - 1
Table 1: Behavior of ~compared to (>0).
Unfortunately, the evaluation of (1) is speciﬁc and restricted
to the neighborhood of the network output. We would like
to directly evaluate the error of the output of the ANN model
over the learned data.
Towards this end, we estimate the error on the network
output caused by the perturbation on wias:
Lwi@L
@y@y
@wi= wi@L
@wi: (2)
The use of (2) in place of (1) shifts the focus from the out-
put to the error of the network. The latter is a more accurate
information in order to evaluate the real effect of the pertur-
bation of a given parameter wi. Let us deﬁne the sensitivity
Sfor a given parameter wias
S(L;wi) =@L
@wi: (3)
LargeSvalues indicate large variations of the loss function
for small perturbations of wi.
Given the above sensitivity deﬁnition, we can promote
sparse topologies by pruning parameters with both low sen-
sitivityS(i.e., in a ﬂat region of the loss function gradient,
where a small perturbation of the parameter has a negligible
effect on the loss) and low magnitude, keeping unmodiﬁed
those with large S. Towards this end, we propose the fol-
lowing parameter update rule to promote sparsity:
wt+1
i:=wt
i @L
@wt
i+
 wt
i
1 S(L;wt
i)
P
S(L;wt
i)
;(4)
where
P(x) =  [1 jxj]; (5)
()is the one-step function and ; two positive hyper-
parameters.
Update Rule
Previously we have introduced a measure for the sensitivity
which can be used also at training time. In particular, plug-
ging (3) in (4) we can rewrite the update rule as:
wt+1
i=wt
i @L
@wt
i   
L;wt
i
1 @L
@wt
i
;(6)
where
  (y;x) =xP@y
@x
: (7)
(a)
 (b)
(c)
 (d)
Figure 1: Update rule effect on the parameters. The red
dashed line is the tangent to the loss function in the black
dot, in blue the standard SGD contribution, in purple the
weight decay while in orange the LOBSTER contribution.
Here we assume P(L;wi) = 1 .
After some algebraic manipulations, we can rewrite (6) as
wt+1
i=wt
i   
L;wt
i
+
 @L
@wt
i
 sign@L
@wt
i
  
L;wt
i
:(8)
In (8), we observe two different components of the proposed
regularization term:
• a weight decay-like term   (L;wi)which is en-
abled/disabled by the magnitude of the gradient on the
parameter;
• a correction term for the learning rate. In particular, the
full learning process follows an equivalent learning rate
~= sign@L
@wi
  (L;wi): (9)
Let us analyze the corrections in the learning rate. If@L
@w i
1(wihas large sensitivity), it follows that P
@L
@w i
= 0and
  (L;wi) = 0 and the dominant contribution comes from
the gradient. In this case our update rule reduces to the clas-
sical GD:
wt+1
i=wt
i @L
@wt
i: (10)
When we consider less sensitive wiwith@L
@w i<1, we get
  (L;wi) =wi(weight decay term) and we can distinguish
two sub-cases for the learning rate:

--- PAGE 4 ---
• if sign
@L
@w i
=sign(wi), then ~(Fig. 1a and
Fig. 1d),
• if sign
@L
@w i
6=sign(wi), then ~(Fig. 1b and
Fig. 1c).
A schematics of all these cases can be found in Table 1 and
the representation of the possible effects are shown in Fig. 1.
The contribution coming from   (L;wi)aims at minimizing
the parameter magnitude, disregarding the loss minimiza-
tion. If the loss minimization tends to minimize the magni-
tude as well, then the equivalent learning rate is reduced. On
the contrary, when the gradient descent tends to increase the
magnitude, the learning rate is increased, to compensate the
contribution coming from   (L;wi). This mechanism allows
us to succeed in the learning task while introducing sparsity.
In the next section we are going to detail the overall training
strategy, which cascades a learning and a pruning stage.
4 Training Procedure
This section describes a procedure to train a sparse neural
networkNleveraging the sensitivity-based rule above to up-
date the network parameters. We assume that the parameters
have been randomly initialized, albeit the procedure holds
also if the network has been pre-trained. The procedure is
illustrated in Fig. 2a and iterates over two stages as follows.
Learning Stage
During the learning stage, the ANN is iteratively trained ac-
cording to the update rule (4) on some training set. Let e
indicate the current learning stage iteration (i.e., epoch) and
Nerepresent the network (i.e., the set of learnable parame-
ters) at the end of the e-th iteration. Also let Lebe the loss
measured on some validation set at the end of the e-th iter-
ation andbLbe the best (lowest) loss measured so far on bN
(network with lowest validation loss so far). As initial con-
dition, we assume, bN=N0. IfLe<bL, the reference to the
best network is updated as bN=Ne,bL=Le. We iterate
again the learning stage Nuntil the best validation loss Le
has not decreased for PWE iterations of the learning stage
in a row (we say the regularizer has reached a performance
plateau ). At such point, we move to the pruning stage.
We providebNas input for the pruning stage, where a num-
ber of parameters have been shrunk towards zero by our
sensitivity-based regularizer.
Pruning Stage
In a nutshell, during the pruning stage parameters with mag-
nitude below a threshold value Tare pinpointed to zero,
eventually sparsifying the network topology as shown in
Fig. 2b. Namely, we look for the largest Tthat worsens the
classiﬁcation loss Lbat most by a relative quantity TWT :
Lb= (1 +TWT )bL; (11)
whereLbis called loss boundary .Tis found using the bi-
section method, initializing Twith the average magnitude of
the non-null parameters in the network. Then, we apply the
thresholdTtobNobtaining the pruned network Nwith itslossLon the validation set. At the next pruning iteration,
we updateTas follows:
• ifLbLthe network tolerates that more parameters be
pruned, soTis increased;
• ifLb< Lthen too many parameters have been pruned
and we need to restore some: we decrease T.
The pruning stage ends when Lb=Land we observe that
Lb< Lfor any new threshold T+"8" >0. OnceTis
found, all the parameters whose magnitude is below Tare
pinpointed to zero, i.e. they are pruned for good. If at leas
one parameter has been pruned during the last iteration of
the pruning stage, a new iteration of the regularization stage
follows; otherwise, the procedure ends returning the trained,
sparse network.
5 Results
In this section we experimentally evaluate LOBSTER over
multiple architectures and datasets commonly used as
benchmark in the literature:
• LeNet-300 on MNIST (Fig. 3a),
• LeNet-5 on MNIST (Fig. 3b),
• LeNet-5 on Fashion-MNIST (Fig. 3c),
• ResNet-32 on CIFAR-10 (Fig. 3d),
• ResNet-18 on ImageNet (Fig. 3e),
• ResNet-101 on ImageNet (Fig. 3f).
We compare with other state-of-the-art approaches intro-
duced in Sec. 2 wherever numbers are publicly available.
Besides these, we also perform an ablation study with a `2-
based regularizer and our proposed pruning strategy (as dis-
cussed in Sec. 4). Performance is measured as the achieved
model sparsity versus classiﬁcation error (Top-1 or Top-5 er-
ror). The network sparsity is deﬁned here ad the percentage
of pruned parameters in the ANN model. Our algorithms are
implemented in Python, using PyTorch 1.2 and simulations
are run over an RTX2080 TI NVIDIA GPU. All the hyper-
parameters have been tuned via grid-search. The validation
set size for all the experiments is 5k large.1For all datasets,
the learning and pruning stages take place on a random split
of the training set, whereas the numbers reported below are
related to the test set.
LeNet-300 on MNIST
As a ﬁrst experiment, we train a sparse LeNet-300 (Le-
Cun et al. 1998) architecture, which consists of three fully-
connected layers with 300, 100 and 10 neurons respectively.
We trained the network on the MNIST dataset, made of 60k
training images and 10k test gray-scale 28 28 pixels large
images, depicting handwritten digits. Starting from a ran-
domly initialized network, we trained LeNet-300 via SGD
with learning rate = 0:1,= 10 4,PWE = 20 epochs
andTWT = 0:05.
The related literature reports several compression results
1The source code is provided in the supplementary materials
and will be made publicly available upon acceptance of the article.

--- PAGE 5 ---
(a)
 (b)
Figure 2: The complete training procedure of LOBSTER (a) and detail of the pruning stage (b).
that can be clustered in two groups corresponding to clas-
siﬁcation error rates of about 1:65% and1:95%, respec-
tively. Fig. 3a provides results for the proposed procedure.
Our method reaches higher sparsity than the the approaches
found in literature. This is particularly noticeable around
1:65% classiﬁcation error (low left in Fig. 3a), where we
achieve almost twice the sparsity of the second best method.
LOBSTER also achieves the highest sparsity for the higher
error range (right side of the graph), gaining especially in
regards to the number of parameters removed from the ﬁrst
fully-connected layer (the largest, consisting of 235k param-
eters), in which we observe just the 0:59% of the parameters
survives.
LeNet-5 on MNIST and Fashion-MNIST
Next, we experiment on the caffe version of the LeNet-5
architecture, consisting in two convolutional and two fully-
connected layers. Again, we use a randomly-initialized net-
work, trained via SGD with learning rate = 0:1,=
10 4,PWE = 20 epochs andTWT = 0:05. The results
are shown in Fig. 3b. Even with a convolutional architecture,
we obtain a competitively small network with a sparsity of
99.57%. At higher compression rates, Sparse VD slightly
outperforms all other methods in the LeNet5-MNIST exper-
iment. We observe that LOBSTER, in this experiment, spar-
siﬁes the ﬁrst convolutional layer ( 22% sparsity) more than
Sparse VD solution ( 33%). In particular, LOBSTER prunes
14ﬁlters out of the 20original in the ﬁrst layer (or in other
words, just 6ﬁlters survive, and contain all the un-pruned
parameters). We hypothesize that, in the case of Sparse VD
and for this particular dataset, extracting a larger variety of
features at the ﬁrst convolutional layer, both eases the clas-
siﬁcation task (hence the lower Top-1 error) and allows to
drop more parameters in the next layers (a slightly improved
sparsity). However, since we are above 99% of sparsity, the
difference between the two techniques is minimal.
To scale-up the difﬁculty of the training task, we ex-perimented on the classiﬁcation of the Fashion-MNIST
dataset (Xiao, Rasul, and V ollgraf 2017), using again
LeNet5. This dataset has the same size and image format of
the MNIST dataset, yet it contains images of clothing items,
resulting in a non-sparse distribution of the pixel intensity
value. Since the images are not as sparse, such dataset is
notoriously harder to classify than MNIST. For this experi-
ment, we trained the network from scratch using SGD with
= 0:1,= 10 4,PWE = 20 epochs andTWT = 0:1.
The results are shown in Fig. 3c.
F-MNIST is an inherently more challenging dataset than
MNIST, so the achievable sparsity is lower. Nevertheless,
the proposed method still reaches higher sparsity than other
approaches, removing an higher percentage of parameters,
especially in the fully connected layers, while maintaining
good generalization. In this case, we observe that the ﬁrst
layer is the least sparsiﬁed: this is an effect of the higher
complexity of the classiﬁcation task, which requires more
features to be extracted.
ResNet-32 on CIFAR-10
To evaluate how our method scales to deeper, modern ar-
chitectures, we applied it on a PyTorch implementation of
the ResNet-32 network (He et al. 2015) that classiﬁes the
CIFAR-10 dataset.2This dataset consists of 60k 32 32
RGB images divided in 10 classes (50k training images and
10k test images). We trained the network using SGD with
momentum= 0:9,= 10 6,PWE = 10 andTWT =
0. The full training is performed for 11k epochs.Our method
performs well on this task and outperforms other state-of-
the-art techniques. Furthermore, LOBSTER improves the
network generalization ability reducing the baseline Top-1
error from 7:37% to7:33% of the sparsiﬁed network while
removing 80:11% of the parameters. This effect is most
likely due to the LOBSTER technique itself, which self-
2https://github.com/akamaster/pytorch resnet cifar10

--- PAGE 6 ---
/uni0000001b/uni0000001b /uni0000001c/uni00000013 /uni0000001c/uni00000015 /uni0000001c/uni00000017 /uni0000001c/uni00000019 /uni0000001c/uni0000001b /uni00000014/uni00000013/uni00000013
/uni00000033/uni00000055/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000014/uni00000011/uni00000019/uni00000013/uni00000014/uni00000011/uni0000001a/uni00000013/uni00000014/uni00000011/uni0000001b/uni00000013/uni00000014/uni00000011/uni0000001c/uni00000013/uni00000015/uni00000011/uni00000013/uni00000013/uni00000015/uni00000011/uni00000014/uni00000013/uni00000015/uni00000011/uni00000015/uni00000013/uni00000015/uni00000011/uni00000016/uni00000013/uni00000015/uni00000011/uni00000017/uni00000013/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002f/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000016/uni00000013/uni00000013/uni00000003/uni00000052/uni00000051/uni00000003/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni0000002f/uni00000032/uni00000025/uni00000036/uni00000037/uni00000028/uni00000035
/uni00000037/uni00000044/uni00000055/uni00000057/uni00000044/uni0000004a/uni0000004f/uni0000004c/uni00000052/uni00000051/uni00000048/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni0000001b/uni0000000c
/uni0000002f/uni00000052/uni00000058/uni0000004c/uni0000005d/uni00000052/uni00000056/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni0000001a/uni0000000c
/uni00000036/uni00000053/uni00000044/uni00000055/uni00000056/uni00000048/uni00000003/uni00000039/uni00000027
/uni00000027/uni00000031/uni00000036
ℓ2/uni00000003/uni0000000e/uni00000003/uni00000053/uni00000055/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a
/uni0000002b/uni00000044/uni00000051/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni00000018/uni0000000c
/uni00000036/uni00000031/uni0000002c/uni00000033(a)
/uni0000001c/uni00000013 /uni0000001c/uni00000015 /uni0000001c/uni00000017 /uni0000001c/uni00000019 /uni0000001c/uni0000001b /uni00000014/uni00000013/uni00000013
/uni00000033/uni00000055/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002f/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000018/uni00000003/uni00000052/uni00000051/uni00000003/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni0000002f/uni00000032/uni00000025/uni00000036/uni00000037/uni00000028/uni00000035
/uni00000037/uni00000044/uni00000055/uni00000057/uni00000044/uni0000004a/uni0000004f/uni0000004c/uni00000052/uni00000051/uni00000048/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni0000001b/uni0000000c
/uni0000002f/uni00000052/uni00000058/uni0000004c/uni0000005d/uni00000052/uni00000056/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni0000001a/uni0000000c
/uni00000036/uni00000053/uni00000044/uni00000055/uni00000056/uni00000048/uni00000003/uni00000039/uni00000027
/uni00000027/uni00000031/uni00000036
/uni0000002b/uni00000044/uni00000051/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni00000018/uni0000000c
/uni00000036/uni0000003a/uni00000036
ℓ2/uni00000003/uni0000000e/uni00000003/uni00000053/uni00000055/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a
/uni00000036/uni00000031/uni0000002c/uni00000033 (b)
/uni0000001b/uni0000001b /uni0000001c/uni00000013 /uni0000001c/uni00000015 /uni0000001c/uni00000017 /uni0000001c/uni00000019 /uni0000001c/uni0000001b
/uni00000033/uni00000055/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000001b/uni00000011/uni00000017/uni00000015/uni0000001b/uni00000011/uni00000017/uni00000017/uni0000001b/uni00000011/uni00000017/uni00000019/uni0000001b/uni00000011/uni00000017/uni0000001b/uni0000001b/uni00000011/uni00000018/uni00000013/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000002f/uni00000048/uni00000031/uni00000048/uni00000057/uni00000010/uni00000018/uni00000003/uni00000052/uni00000051/uni00000003/uni00000029/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni0000002f/uni00000032/uni00000025/uni00000036/uni00000037/uni00000028/uni00000035
/uni00000037/uni00000044/uni00000055/uni00000057/uni00000044/uni0000004a/uni0000004f/uni0000004c/uni00000052/uni00000051/uni00000048/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni0000001b/uni0000000c
ℓ2/uni00000003/uni0000000e/uni00000003/uni00000053/uni00000055/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a
/uni0000002b/uni00000044/uni00000051/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni00000018/uni0000000c
(c)
/uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013
/uni00000033/uni00000055/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000001a/uni00000011/uni00000015/uni00000018/uni0000001a/uni00000011/uni00000018/uni00000013/uni0000001a/uni00000011/uni0000001a/uni00000018/uni0000001b/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000011/uni00000015/uni00000018/uni0000001b/uni00000011/uni00000018/uni00000013/uni0000001b/uni00000011/uni0000001a/uni00000018/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000010/uni00000016/uni00000015/uni00000003/uni00000052/uni00000051/uni00000003/uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000010/uni00000014/uni00000013
/uni0000002f/uni00000032/uni00000025/uni00000036/uni00000037/uni00000028/uni00000035
/uni0000002f/uni00000052/uni00000058/uni0000004c/uni0000005d/uni00000052/uni00000056/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni0000001a/uni0000000c
/uni00000036/uni00000053/uni00000044/uni00000055/uni00000056/uni00000048/uni00000003/uni00000039/uni00000027
/uni00000037/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000057/uni00000048/uni00000047/uni00000003/uni00000027/uni00000055/uni00000052/uni00000053/uni00000052/uni00000058/uni00000057
ℓ2/uni00000003/uni0000000e/uni00000003/uni00000053/uni00000055/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a (d)
/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013
/uni00000033/uni00000055/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000016/uni00000015/uni00000011/uni00000013/uni00000013/uni00000016/uni00000017/uni00000011/uni00000013/uni00000013/uni00000016/uni00000019/uni00000011/uni00000013/uni00000013/uni00000016/uni0000001b/uni00000011/uni00000013/uni00000013/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000010/uni00000014/uni0000001b/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
/uni0000002f/uni00000032/uni00000025/uni00000036/uni00000037/uni00000028/uni00000035
/uni00000029/uni00000036/uni0000002e/uni00000027
/uni00000024/uni00000025/uni00000026/uni00000033/uni00000055/uni00000058/uni00000051/uni00000048/uni00000055
/uni0000002f/uni0000004c/uni00000048/uni00000045/uni00000048/uni00000051/uni0000005a/uni00000048/uni0000004c/uni00000051/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000015/uni00000013/uni0000000c
ℓ2/uni00000003/uni0000000e/uni00000003/uni00000053/uni00000055/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a
(e)
/uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013 /uni0000001c/uni00000013
/uni00000033/uni00000055/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000015/uni00000018/uni00000011/uni00000013/uni00000013/uni00000016/uni00000018/uni00000011/uni00000013/uni00000013/uni00000017/uni00000018/uni00000011/uni00000013/uni00000013/uni00000018/uni00000018/uni00000011/uni00000013/uni00000013/uni00000019/uni00000018/uni00000011/uni00000013/uni00000013/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000010/uni00000014/uni00000013/uni00000014/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000031/uni00000048/uni00000057
/uni0000002f/uni00000032/uni00000025/uni00000036/uni00000037/uni00000028/uni00000035
/uni00000037/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000057/uni00000048/uni00000047/uni00000003/uni00000027/uni00000055/uni00000052/uni00000053/uni00000052/uni00000058/uni00000057
/uni0000002b/uni00000044/uni00000051/uni00000003/uni00000048/uni00000057/uni00000003/uni00000044/uni0000004f/uni00000011/uni00000003/uni0000000b/uni00000015/uni00000013/uni00000014/uni00000018/uni0000000c
ℓ2/uni00000003/uni0000000e/uni00000003/uni00000053/uni00000055/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a (f)
Figure 3: Performance (Top-1 error) vs ratio of pruned parameters for LOBSTER and other state of the art methods over
different architectures and datasets.

--- PAGE 7 ---
Dataset Architecture`2+pruning LOBSTER
Top-1 (%) Sparsity (%) FLOPs Top-1 (%) Sparsity (%) FLOPs
MNISTLeNet-300 1.97 97.62 22.31k 1.95 99.13 10.63k
LeNet-5 0.80 98.62 589.75k 0.79 99.57 207.38k
F-MNIST LeNet-5 8.44 93.04 1628.39k 8.43 96.27 643.22k
CIFAR-10 ResNet-32 8.08 71.51 44.29M 7.33 80.11 32.90M
ImageNetResNet-18 31.08 25.40 2.85G 30.10 37.04 2.57G
ResNet-101 28.33 78.67 3.44G 26.44 81.58 3.00G
Table 2: Comparison between LOBSTER and with `2+pruning as in Fig. 3 (only best sparsity results are reported).
tunes the regularization on the parameters as explained in
Sec. 3.
ResNet on ImageNet
Finally, we further scale-up both the output and the com-
plexity of the classiﬁcation problem testing the proposed
method on network over the well-known ImageNet dataset
(ILSVRC-2012), composed of more than 1.2 million train
images, for a total of 1k classes. For this test we used SGD
with momentum = 0:9,= 10 6andTWT = 0. The
full training lasts 95 epochs. Due to time constraints, we de-
cided to use the pre-trained network offered by the torchvi-
sion library.3Fig. 3e shows the results for ResNet-18 while
Fig.3f shows the results for ResNet-101. Even in this sce-
nario, LOBSTER proves to be particularly efﬁcient: we are
able to remove, with no performance loss, 37:04% of the
parameters from ResNet-18 and 81:58% from ResNet-101.
Ablation study
As a ﬁnal ablation study, we replace our sensitivity-based
regularizer with a simpler `2regularizer in our leraning
scheme in Fig. 2. Such scheme “ `2+pruning” uniformly ap-
plies an`2penalty to all the parameters regardless their con-
tribution to the loss. This scheme is comparable with (Han
et al. 2015), yet enhanced with the same pruning strategy
with adaptive thresholding shown in Fig. 2b. A comparison
between LOBSTER and `2+pruning is reported in Table 2.
In all the experiments we observe that dropping the sensi-
tivity based regularizer impairs the performance. This ex-
periment veriﬁes the role of the sensitivity-based regulariza-
tion in the performance of our scheme. Finally, Table 2 also
reports the corresponding inference complexity in FLOPs.
For the same or lower Top-1 error LOBSTER yelds bene-
ﬁts as fewer operations at inference time and suggesting the
presence of some structure in the sparsity achieved by LOB-
STER.
3https://pytorch.org/docs/stable/torchvision/models.html6 Conclusion
We presented LOBSTER, a regularization method suitable
to train neural networks with a sparse topology without
a preliminary training. Differently from `2regularization,
LOBSTER is aware of the global contribution of the pa-
rameter on the loss function and self-tunes the regulariza-
tion effect on the parameter depending on factors like the
ANN architecture or the training problem itself (in other
words, the dataset). Moreover, tuning its hyper-parameters
is easy and the optimal threshold for parameter pruning is
self-determined by the proposed approach employing a val-
idation set. LOBSTER achieves competitive results from
shallow architectures like LeNet-300 and LeNet-5 to deeper
topologies like ResNet over ImageNet. In these scenarios
we have observed the boost provided by the proposed regu-
larization approach towards less-unaware approaches like `2
regularization, in terms of achieved sparsity.
Future research includes the extension of LOBSTER to
achieve sparsity with a structure and a thorough evaluation
of the savings in terms of memory footprint.
References
Brutzkus, A.; Globerson, A.; Malach, E.; and Shalev-
Shwartz, S. 2018. SGD learns over-parameterized networks
that provably generalize on linearly separable data.
Frankle, J.; and Carbin, M. 2019. The lottery ticket hypoth-
esis: Finding sparse, trainable neural networks.
Gomez, A. N.; Zhang, I.; Swersky, K.; Gal, Y .; and Hin-
ton, G. E. 2019. Learning Sparse Networks Using Targeted
Dropout. CoRR abs/1905.13678.
Groetsch, C. W. 1993. Inverse Problems in the Mathematical
Sciences . Vieweg.
Guo, Y .; Yao, A.; and Chen, Y . 2016. Dynamic network
surgery for efﬁcient dnns. In Advances In Neural Informa-
tion Processing Systems , 1379–1387.
Han, S.; Mao, H.; and Dally, W. 2016. Deep compression:
Compressing deep neural networks with pruning, trained
quantization and Huffman coding.

--- PAGE 8 ---
Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning
both weights and connections for efﬁcient neural network. In
Advances in Neural Information Processing Systems , 1135–
1143.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual
Learning for Image Recognition. CoRR abs/1512.03385.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-
ual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 770–778.
Karnin, E. D. 1990. A simple procedure for pruning back-
propagation trained neural networks. IEEE transactions on
neural networks 1(2): 239–242.
LeCun, Y .; Bottou, L.; Bengio, Y .; and Haffner, P. 1998.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE 86(11): 2278 – 2324.
LeCun, Y .; Denker, J. S.; and Solla, S. A. 1990. Optimal
brain damage. In Advances in neural information processing
systems , 598–605.
Lee, N.; Ajanthan, T.; and Torr, P. 2019. SNIP: Single-shot
Network Pruning based on Connection Sensitivity. ArXiv
abs/1810.02340.
Li, T.; Li, J.; Liu, Z.; and Zhang, C. 2020. Few sample
knowledge distillation for efﬁcient network compression. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , 14639–14647.
Liebenwein, L.; Baykal, C.; Lang, H.; Feldman, D.; and Rus,
D. 2020. Provable Filter Pruning for Efﬁcient Neural Net-
works. In International Conference on Learning Represen-
tations .
Lin, M.; Ji, R.; Zhang, Y .; Zhang, B.; Wu, Y .; and Tian, Y .
2020. Channel Pruning via Automatic Structure Search. In
Bessiere, C., ed., Proceedings of the Twenty-Ninth Interna-
tional Joint Conference on Artiﬁcial Intelligence, IJCAI-20 ,
673–679. International Joint Conferences on Artiﬁcial In-
telligence Organization. doi:10.24963/ijcai.2020/94. Main
track.
Liu, J.; Wang, Y .; and Qiao, Y . 2017. Sparse deep transfer
learning for convolutional neural network. In Proceedings of
the Thirty-First AAAI Conference on Artiﬁcial Intelligence ,
2245–2251.
Louizos, C.; Welling, M.; and Kingma, D. P. 2017. Learning
Sparse Neural Networks through L0Regularization. arXiv
preprint arXiv:1712.01312 .
Mhaskar, H. N.; and Poggio, T. 2016. Deep vs. shallow net-
works: An approximation theory perspective. Analysis and
Applications 14(06): 829–848.
Molchanov, D.; Ashukha, A.; and Vetrov, D. 2017. Varia-
tional dropout sparsiﬁes deep neural networks. volume 5,
3854–3863.
Mozer, M. C.; and Smolensky, P. 1989. Skeletonization: A
technique for trimming the fat from a network via relevance
assessment. In Advances in neural information processing
systems , 107–115.Simonyan, K.; and Zisserman, A. 2014. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 .
Tartaglione, E.; Bragagnolo, A.; and Grangetto, M. 2020.
Pruning artiﬁcial neural networks: a way to ﬁnd well-
generalizing, high-entropy sharp minima. arXiv preprint
arXiv:2004.14765 .
Tartaglione, E.; Lepsøy, S.; Fiandrotti, A.; and Francini, G.
2018. Learning sparse neural networks via sensitivity-driven
regularization. In Advances in Neural Information Process-
ing Systems , 3878–3888.
Ullrich, K.; Welling, M.; and Meeds, E. 2019. Soft weight-
sharing for neural network compression.
Wang, Y .; Zhang, X.; Xie, L.; Zhou, J.; Su, H.; Zhang, B.;
and Hu, X. 2020. Pruning from Scratch. In AAAI , 12273–
12280.
Wen, W.; Wu, C.; Wang, Y .; Chen, Y .; and Li, H. 2016.
Learning structured sparsity in deep neural networks. In
Advances in Neural Information Processing Systems , 2074–
2082.
Xiao, H.; Rasul, K.; and V ollgraf, R. 2017. Fashion-MNIST:
a Novel Image Dataset for Benchmarking Machine Learning
Algorithms. CoRR abs/1708.07747.
Yang, L.; He, Z.; and Fan, D. 2020. Harmonious Coexis-
tence of Structured Weight Pruning and Ternarization for
Deep Neural Networks. In AAAI , 6623–6630.
