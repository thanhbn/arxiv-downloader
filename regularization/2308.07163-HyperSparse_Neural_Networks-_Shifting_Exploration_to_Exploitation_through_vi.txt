# 2308.07163.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/regularization/2308.07163.pdf
# Kích thước tệp: 966831 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mạng Nơ-ron Siêu Thưa: Chuyển từ Khám phá sang Khai thác thông qua
Chính quy hóa Thích ứng
Patrick Glandorf∗, Timo Kaiser∗, Bodo Rosenhahn
Viện Xử lý Thông tin (tnt)
L3S - Leibniz Universit ¨at Hannover, Germany
{glandorf, kaiser, rosenhahn }@tnt.uni-hannover.de

Tóm tắt
Mạng nơ-ron thưa là yếu tố then chốt trong việc phát triển các ứng dụng học máy tiết kiệm tài nguyên. Chúng tôi đề xuất phương pháp học thưa mới và mạnh mẽ gọi là Huấn luyện Chính quy hóa Thích ứng (ART) để nén các mạng dày đặc thành mạng thưa. Thay vì sử dụng mặt nạ nhị phân thông thường trong quá trình huấn luyện để giảm số lượng trọng số mô hình, chúng tôi thu nhỏ trọng số gần bằng không một cách lặp đi lặp lại với việc tăng chính quy hóa trọng số. Phương pháp của chúng tôi nén "kiến thức" mô hình được huấn luyện trước vào các trọng số có độ lớn cao nhất. Do đó, chúng tôi giới thiệu một mất mát chính quy hóa mới có tên HyperSparse khai thác các trọng số cao nhất trong khi bảo toàn khả năng khám phá trọng số. Các thí nghiệm mở rộng trên CIFAR và TinyImageNet cho thấy phương pháp của chúng tôi dẫn đến những cải thiện hiệu suất đáng chú ý so với các phương pháp thưa hóa khác, đặc biệt trong các chế độ thưa cực cao lên đến 99.8% mô hình thưa. Các nghiên cứu bổ sung cung cấp những hiểu biết mới về các mẫu được mã hóa trong các trọng số có độ lớn cao.¹

1. Giới thiệu
Những năm gần đây đã cho thấy tiến bộ to lớn trong lĩnh vực học máy dựa trên việc sử dụng mạng nơ-ron (NN). Bên cạnh độ chính xác tăng lên trong gần như tất cả các nhiệm vụ, độ phức tạp tính toán của NN cũng tăng lên, ví dụ như cho Transformers [5,7] hoặc Mô hình Ngôn ngữ Lớn [2]. Độ phức tạp gây ra chi phí năng lượng cao, hạn chế khả năng áp dụng cho các hệ thống hiệu quả về chi phí [10], và phản tác dụng đối với tính công bằng và đáng tin cậy do khả năng diễn giải giảm dần [38].

Đối mặt với những vấn đề này, những năm gần đây cũng dẫn đến sự phát triển của cộng đồng nghiên cứu trong lĩnh vực NN thưa [12]. Mục tiêu là tìm các đồ thị con nhỏ (còn gọi là NN thưa) trong các NN hoạt động tốt có khả năng tương tự hoặc có thể so sánh được về các nhiệm vụ chính trong khi ít phức tạp hơn đáng kể

*Những tác giả này đóng góp ngang nhau vào công trình này
¹Mã nguồn có sẵn tại https://github.com/GreenAutoML4FAS/HyperSparse

và do đó rẻ hơn và có khả năng diễn giải tốt hơn. Các phương pháp tiêu chuẩn thường tạo NN thưa bằng cách có được mặt nạ nhị phân giới hạn số lượng trọng số được sử dụng trong NN [20, 34, 42]. Phương pháp nổi bật nhất là Cắt tỉa Độ lớn Lặp (IMP) [16] dựa trên Giả thuyết Vé số May mắn (LTH) [9]. Giả định rằng các trọng số quan trọng có độ lớn cao sau khi huấn luyện, nó huấn luyện một NN dày đặc và loại bỏ một lượng phần tử từ mặt nạ tương ứng với các trọng số thấp nhất. Sau đó, NN thưa được khởi tạo lại và huấn luyện lại từ đầu. Quá trình được lặp lại cho đến khi đạt được mức độ thưa.

Giả định của cắt tỉa độ lớn rằng các trọng số cao nhất trong NN dày đặc mã hóa các quy tắc quyết định quan trọng nhất cho một tập hợp đa dạng các lớp là có vấn đề, vì nó không được đảm bảo. Các trọng số bị loại bỏ có thể hữu ích cho dự đoán không thể được kích hoạt lại trong quá trình tinh chỉnh. Trong trường hợp xấu nhất, "sự sụp đổ lớp" có thể ngăn cản việc lan truyền thuận hữu ích [37]. Việc thiếu khả năng khám phá vẫn tồn tại trong phương pháp IMP lặp chính xác hơn nhưng tiêu tốn tài nguyên.

Phục hồi những ý tưởng chính của Han et al. [10] và Narang et al. [27] (có thể so sánh với [26]), chúng tôi giới thiệu một phương pháp nhẹ và mạnh mẽ gọi là Huấn luyện Chính quy hóa Thích ứng (ART) để có được NN rất thưa, mà ngầm "loại bỏ" trọng số với việc tăng chính quy hóa cho đến khi đạt được mức độ thưa mong muốn. ART chính quy hóa mạnh các trọng số trước khi cắt tỉa độ lớn. Đầu tiên, một NN dày đặc được huấn luyện trước cho đến khi hội tụ. Trong giai đoạn thứ hai, NN được huấn luyện với chính quy hóa tăng dần và suy giảm trọng số cho đến khi NN được cắt tỉa độ lớn giả định hoạt động ngang bằng với phiên bản dày đặc. Cuối cùng, chúng tôi áp dụng cắt tỉa độ lớn và tinh chỉnh NN mà không có chính quy hóa. Tránh mặt nạ nhị phân trong giai đoạn thứ hai cho phép khám phá và chính quy hóa buộc khai thác các trọng số còn lại trong NN thưa. Chúng tôi giới thiệu phương pháp chính quy hóa mới HyperSparse cho giai đoạn thứ hai vượt qua chính quy hóa tĩnh như Lasso [39] hoặc Weight Decay [44] và thích ứng với độ lớn trọng số bằng cách phạt các trọng số nhỏ. HyperSparse cân bằng sự đánh đổi khám phá/khai thác và do đó tăng độ chính xác trong khi dẫn đến hội tụ nhanh hơn trong giai đoạn thứ hai. Sự kết hợp của lịch trình chính quy hóa và HyperSparse cải thiện độ chính xác phân loại và thời gian tối ưu hóa đáng kể, đặc biệt trong các chế độ thưa cao với tới 99.8% trọng số bằng không. Chúng tôi đánh giá phương pháp của mình trên CIFAR-10/100 [19] và TinyImageNet [6] với ResNet-32 [11] và VGG-19 [33].

Hơn nữa, chúng tôi phân tích phân phối gradient và trọng số trong quá trình huấn luyện chính quy hóa, cho thấy HyperSparse dẫn đến hội tụ nhanh hơn đến NN thưa. Các thí nghiệm cũng cho thấy rằng khẳng định của [34], rằng NN thưa tối ưu có thể được thu được thông qua heuristic phân phối trọng số đơn giản, không đúng nói chung. Cuối cùng, chúng tôi phân tích quá trình nén NN dày đặc thành NN thưa và cho thấy rằng các trọng số cao nhất trong NN không mã hóa quy tắc quyết định cho một tập hợp đa dạng các lớp với ưu tiên bằng nhau.

Tóm lại, bài báo này
• giới thiệu HyperSparse, một mất mát chính quy hóa thích ứng vượt trội ngầm thúc đẩy độ thưa mạng có thể cấu hình bằng cách cân bằng sự đánh đổi khám phá và khai thác.
• giới thiệu khung mới ART để có được mạng thưa sử dụng chính quy hóa với đòn bẩy tăng dần, cải thiện thời gian tối ưu hóa và độ chính xác phân loại của mạng nơ-ron thưa, đặc biệt trong các chế độ thưa cao.
• phân tích quá trình liên tục nén các mẫu từ mạng nơ-ron dày đặc sang thưa.

2. Công trình Liên quan
Các phương pháp Học thưa tìm mặt nạ nhị phân để loại bỏ một lượng trọng số được xác định trước có thể được phân loại là tĩnh hoặc động (ví dụ, trong [4, 12, 14]). Theo [4], trong huấn luyện thưa động "[...] các phần tử bị loại bỏ [từ mặt nạ] có cơ hội được phát triển trở lại nếu chúng có khả năng có lợi cho dự đoán" trong khi huấn luyện tĩnh kết hợp mặt nạ cố định.

Các phương pháp tĩnh thường dựa trên Frankle et al. [9], người giới thiệu LTH và cho thấy rằng NN thưa hoạt động tốt trong NN được khởi tạo ngẫu nhiên có thể được tìm thấy sau khi huấn luyện dày đặc thông qua cắt tỉa độ lớn. Phương pháp cắt tỉa độ lớn được cải thiện bởi IMP [16] lặp lại quá trình. Thay thế quy trình huấn luyện tốn thời gian, các phương pháp như SNIP [20] hoặc GraSP [42] tìm NN thưa trong NN dày đặc được khởi tạo ngẫu nhiên sử dụng một dự đoán mạng duy nhất và gradient của nó. Để cũng giải quyết rủi ro sụp đổ lớp trong quá trình cắt tỉa, SynFlow [37] bổ sung bảo toàn tổng luồng trong mạng. Trái ngược với các công trình sau này, Su et al. [34] khẳng định rằng NN thưa phù hợp không phụ thuộc vào dữ liệu hoặc khởi tạo trọng số và cung cấp một heuristic chung cho phân phối trọng số.

Khác với các phương pháp tĩnh, các phương pháp động cắt tỉa và kích hoạt lại các phần tử bằng không trong mặt nạ nhị phân. Các trọng số được kích hoạt lại có thể được chọn ngẫu nhiên [25] hoặc xác định bởi gradient [3, 4, 8]. Ví dụ, RigL [8] lặp cắt tỉa các trọng số có độ lớn thấp và do đó kích hoạt lại các trọng số có gradient cao nhất. Ngoài ra, các phương pháp động hiện đại sử dụng mặt nạ liên tục. Ví dụ, Tai et al. [36] nới lỏng khung IMP bằng cách giới thiệu softmask có tham số để có được trung bình có trọng số giữa IMP và Top-KAST [15]. Tương tự, [24, 31] nới lỏng mặt nạ nhị phân và tối ưu hóa chuẩn L0 của nó. Cách khác là cắt tỉa mô hình một cách vốn có, ví dụ bằng cách giảm gradient của các trọng số có độ lớn nhỏ [32]. So với các phương pháp tĩnh, Liu et al. [22, 23] cho thấy các phương pháp huấn luyện thưa động vượt qua hầu hết các phương pháp tĩnh bằng cách cho phép khám phá trọng số.

Một thuộc tính khác để phân biệt các phương pháp học thưa hiện đại là độ phức tạp trong quá trình tạo mặt nạ, ví dụ như được thực hiện bởi Schwarz et al. [32]. Các phương pháp thưa→thưa tiết kiệm tài nguyên hơn duy trì NN thưa trong quá trình huấn luyện [4,8,20,32,34,37,42], trong khi các phương pháp dày đặc→thưa sử dụng tất cả các tham số trước khi tìm mặt nạ cuối cùng [9, 15, 16, 24, 31, 36].

Tuy nhiên, như được giải thích sau, phương pháp của chúng tôi thuộc về các phương pháp dày đặc→thưa vốn có giảm độ phức tạp mô hình mà không cần che mặt nạ trước khi cắt tỉa độ lớn để có được mặt nạ thưa tĩnh cho tinh chỉnh. Chúng tôi muốn đề cập đến các công trình chính của Han et al. [10], Narang et al. [27] và Molchanov et al. [26] mà sự kết hợp của chúng là mô hình cho chúng tôi. Han et al. sử dụng chính quy hóa L1 và L2 để giảm số lượng phần tử khác không trong quá trình huấn luyện. Khung sớm của họ sử dụng chính quy hóa mà không có phụ gia và không có khả năng kiểm soát mức độ thưa. Narang et al. và Molchanov et al. loại bỏ trọng số theo từng phần nhỏ với ngưỡng loại bỏ tăng dần, nhưng không kết hợp khám phá trọng số.

Khả năng Diễn giải và Hiểu biết về học máy có liên quan chặt chẽ đến học thưa và cũng được đề cập trong bài báo này. Có một số lượng ngày càng tăng các công trình trong những năm gần đây sử dụng học thưa cho các lợi ích khác, ví dụ, để tìm các mối tương quan có thể diễn giải giữa không gian đặc trưng và hình ảnh [38] hoặc để hình dung sự mơ hồ giữa các lớp [18]. Công trình của Paul et al. [28] đưa ra chi tiết về giai đoạn học sớm rất quan trọng, ví dụ, để xác định việc ghi nhớ nhiễu nhãn [17]. Họ cho thấy rằng hầu hết dữ liệu không cần thiết để có được các mạng con phù hợp. Mối quan hệ chung giữa LTH và khái quát hóa được nghiên cứu trong [30]. Varma et al. [35] cho thấy rằng NN thưa phù hợp hơn trong các chế độ dữ liệu hạn chế và nhiễu. Mặt khác Hooker et al. [13] cho thấy rằng NN thưa có tác động không tầm thường đến thiên vị đạo đức bằng cách nghiên cứu các mẫu nào được "quên" đầu tiên trong quá trình nén mạng. Câu hỏi nghiên cứu cơ bản của công trình sau này được thay đổi thành "Các mẫu nào được nén đầu tiên?" và được thảo luận trong bài báo này.

3. Phương pháp
Thưa hóa nhằm giảm số lượng trọng số khác không trong NN. Để giải quyết vấn đề này, chúng tôi sử dụng một lịch trình nhất định cho chính quy hóa sao cho các trọng số nhỏ hội tụ về không và mô hình của chúng tôi ngầm trở nên thưa.

Trong Mục 3.1, chúng tôi định nghĩa chính thức vấn đề thưa hóa. Sau đó, chúng tôi trình bày Huấn luyện Chính quy hóa Thích ứng (ART) trong Mục 3.2, tăng dần đòn bẩy chính quy hóa để tối đa hóa số lượng trọng số gần bằng không. Hơn nữa, chúng tôi giới thiệu mất mát chính quy hóa HyperSparse trong Mục 3.3 được tích hợp trong ART. Nó đồng thời cho phép khám phá các cấu trúc mới trong khi khai thác các trọng số của mạng con thưa cuối cùng.

3.1. Kiến thức Cơ bản
Chúng tôi xem xét một NN f(W, x) với cấu trúc f và trọng số W được huấn luyện để phân loại hình ảnh từ tập dữ liệu S={(xn, yn)}Nn=1, trong đó yn là lớp sự thật gốc của mẫu hình ảnh xn. Việc huấn luyện được cấu trúc theo epoch, là các tối ưu hóa lặp của trọng số W={w1, . . . , wD} trên tất cả các mẫu trong S để tối thiểu hóa mục tiêu mất mát L. Các trọng số thu được sau epoch e được ký hiệu là We, với W0 ký hiệu các trọng số trước khi tối ưu hóa. Hơn nữa, độ chính xác phân loại của NN được đo bằng hàm đánh giá ψ(W).

Mục tiêu trong thưa hóa là giảm lực lượng của W bằng cách loại bỏ một tỷ lệ trọng số được xác định trước κ, trong khi tối đa hóa ψ(W). Mạng được cắt tỉa bằng tích Hadamard m⊙W của mặt nạ nhị phân m∈[0,1]D và trọng số mô hình W. Mặt nạ thường được tạo bằng cách áp dụng cắt tỉa độ lớn m=ν(W)[3, 4, 9, 16], là một kỹ thuật đặt κ-trọng số thấp nhất bằng không.

3.2. Huấn luyện Chính quy hóa Thích ứng (ART)
Các mất mát chính quy hóa như chuẩn L1 (hồi quy Lasso) [39] hoặc chuẩn L2 [44] được sử dụng để ngăn chặn overfitting bằng cách thu nhỏ độ lớn của trọng số. Chúng tôi sử dụng hiệu ứng này trong ART cho thưa hóa, vì các trọng số có độ lớn thấp có tác động thấp đến việc thay đổi đầu ra và do đó có thể được loại bỏ với chỉ ít tác động đến ψ(W).

Chính quy hóa trong quá trình huấn luyện có thể được biểu thị như một mất mát hỗn hợp

Ltotal=Lclass+λinit·ηe· Lreg, (1)

trong đó Lclass là mất mát phân loại và Lreg là mất mát chính quy hóa. Gradient của Lreg thu nhỏ một tập hợp trọng số về xấp xỉ bằng không và tạo ra một mạng thưa vốn có với tỷ lệ cắt tỉa không xác định [39]. Tăng η đòn bẩy chính quy hóa Lreg theo cách tăng dần, nhưng các phương pháp hiện tại sử dụng tỷ lệ chính quy hóa cố định η = 1[4, 10, 26].

Sau khi huấn luyện không chính quy hóa của NN dày đặc đến hội tụ, ART sử dụng khung chính quy hóa tiêu chuẩn và sửa đổi nó bằng cách đặt η > 1 và khởi tạo thấp λinit. Sau đó, mất mát chính quy hóa Lreg gần như không có tác động đến Ltotal ban đầu, nhưng bắt đầu thu nhỏ trọng số mà không có nhiều tác động đến Lclass về không. Tuy nhiên, nó cho phép mọi trọng số wi có khả năng có độ lớn cao sao cho wi được chuyển vào NN thưa của các trọng số cao nhất (khám phá). Với việc tăng chính quy hóa, ảnh hưởng của gradient dLreg/dwi trên wi tăng lên và có nhiều khả năng vượt qua gradient dLclass/dwi. Chính quy hóa cản trở việc khám phá thích hợp của các trọng số nhỏ bằng cách kéo độ lớn về không. Mặt khác, các trọng số lớn hơn cần được khai thác để bảo toàn kết quả phân loại. Do đó, chính quy hóa tăng dần của chúng tôi liên tục chuyển đổi sự đánh đổi khám phá/khai thác từ khám phá sang khai thác. Phương pháp cho phép sắp xếp lại trọng số để tìm cấu trúc tốt hơn, nhưng buộc phải khai thác các trọng số cao nhất liên quan đến nhiệm vụ phân loại. Do số lượng trọng số xấp xỉ bằng không tăng lên, mô hình dày đặc hội tụ đến một mô hình thưa vốn có. Chúng tôi dừng huấn luyện chính quy hóa nếu NN với trọng số được cắt tỉa tốt nhất ψ(ν(Wbest)⊙Wbest) có độ chính xác cao hơn so với trọng số không được cắt tỉa mới nhất ψ(We) và chọn Wbest làm ứng viên cho tinh chỉnh.

Quy trình huấn luyện tổng thể được định nghĩa như sau:
Bước 1: Huấn luyện trước mô hình dày đặc đến hội tụ mà không có chính quy hóa.
Bước 2: Loại bỏ trọng số ngầm sử dụng ART như được mô tả trong thuật toán 1.
Bước 3: Áp dụng cắt tỉa độ lớn và tinh chỉnh mạng được cắt tỉa đến hội tụ.

ART nới lỏng phương pháp IMP lặp cắt tỉa các trọng số ít quan trọng nhất qua các lần lặp nhất định. Tương tự như tỷ lệ cắt tỉa tăng dần trong các phương pháp lặp tiêu chuẩn, chúng tôi tăng dần lượng trọng số gần bằng không và do đó xấp xỉ mặt nạ nhị phân ngầm.

3.3. Chính quy hóa HyperSparse
Mục 3.2 trước đây mô tả quá trình thu nhỏ trọng số trong W bằng cách phạt với chính quy hóa tăng dần. Một nhược điểm của quy trình này là các trọng số còn lại sau khi cắt tỉa cũng bị phạt bởi chính quy hóa. Điều này ảnh hưởng tiêu cực đến khai thác liên quan đến nhiệm vụ chính. Do đó, các trọng số còn lại không nên bị phạt. Mặt khác, nếu các trọng số nhỏ bị phạt mạnh, thuộc tính khám phá mong muốn của các phương pháp cắt tỉa động để "phát triển" trở lại các phần tử này bị hạn chế. Để giải quyết sự đánh đổi này giữa khai thác và khám phá, chúng tôi giới thiệu mất mát chính quy hóa thích ứng tạo độ thưa HyperSparse.

Kết hợp hàm Tang Hyperbolic áp dụng trên độ lớn ký hiệu là t(·) = tanh(|·|) để đơn giản, mất mát HyperSparse được định nghĩa là

LHS(W) = (1/A)|W|∑i=1 |wi||W|∑j=1 t(s·wj) - |W|∑i=1 |wi|

với A := ∑w∈W t(s·w) và ∀w∈W: dA/dw = 0, (2)

trong đó A được coi là hằng số giả trong tính toán gradient và s là hệ số căn chỉnh được mô tả sau. Chính quy hóa phạt trọng số tùy thuộc vào gradient và có thể khác nhau cho các trọng số khác nhau. Gradient của HyperSparse đối với trọng số wi xấp xỉ

dLHS(W)/dwi = sign(wi)·t'(s·wi)·(∑|W|j=1|wj|)/(∑|W|j=1t(s·wj)),

với wi, wj∈W, t'(·)∈(0,1]. (3)

Đạo hàm t'=dt/dwi hội tụ về 1 cho độ lớn nhỏ |wi| ≈ 0 và về 0 cho độ lớn lớn |wi| ≫ 0. Do đó, số hạng thứ hai trong Phương trình (3) thích ứng với trọng số và phạt mạnh độ lớn nhỏ, nhưng giảm xuống không cho độ lớn lớn. Chi tiết cho tính toán gradient và phân tích có thể tìm thấy trong tài liệu bổ sung, Mục D.

Hệ số căn chỉnh s là bắt buộc để khai thác các thuộc tính nói trên cho nhiệm vụ thưa hóa với tỷ lệ cắt tỉa cụ thể κ. Vì LHS phụ thuộc vào độ lớn trọng số, nhưng không có phạm vi giá trị xác định được cho trọng số, mất mát LHS của chúng tôi không được đảm bảo thích ứng hợp lý với W đã cho. Ví dụ, xem xét s = 1 cố định và tất cả trọng số trong W gần bằng không, gradient từ Phương trình (3) dẫn đến gần như cùng giá trị cho mọi trọng số. Do đó, chúng tôi thích ứng s với trọng số nhỏ nhất |wκ| sẽ còn lại sau khi cắt tỉa độ lớn, sao cho t'''(s·wκ) = 0, là điểm uốn của t'. Theo sự căn chỉnh này, các gradient trong Phương trình (3) của các trọng số còn lại |w| ≥ |wκ| được dịch chuyển gần hơn đến 1 và được tăng lên cho các trọng số |w| ≤ |wκ|, trong khi tuân thủ gradient mượt từ trọng số còn lại đến bị loại bỏ. Hơn nữa, mẫu số trong Phương trình (3) giảm theo thời gian, nếu nhiều trọng số trong W gần bằng không sau chính quy hóa tăng dần. Gradient cho các phân phối trọng số khác nhau của NN dựa trên HyperSparse được hiển thị trong Hình 1 và hình dung hành vi gradient được mô tả của chính quy hóa trọng số thích ứng.

4. Thí nghiệm
Phần này trình bày các thí nghiệm cho thấy phương pháp ART được đề xuất của chúng tôi vượt trội hơn các phương pháp có thể so sánh, đặc biệt trong các chế độ thưa cực cao. Thiết lập thí nghiệm của chúng tôi được mô tả trong Mục 4.1. Trong phần tiếp theo, chúng tôi cho thấy HyperSparse có tác động tích cực lớn đến thời gian tối ưu hóa và độ chính xác phân loại. Sự cải thiện này được giải thích bằng phân tích sự đánh đổi giữa khám phá và khai thác, phân phối gradient và trọng số trong Mục 4.3 và 4.4. Cuối cùng, chúng tôi phân tích và thảo luận hành vi nén trong quá trình huấn luyện chính quy hóa và rút ra những hiểu biết sâu hơn về các trọng số có độ lớn cao nhất trong Mục 4.5.

4.1. Thiết lập Thí nghiệm
Chúng tôi đánh giá ART trên các bộ dữ liệu CIFAR-10/100 [19] và TinyImageNet [6] để bao phủ các độ phức tạp khác nhau, được đưa ra bởi số lượng nhãn lớp khác nhau. Hơn nữa, chúng tôi sử dụng các độ phức tạp mô hình khác nhau, trong đó ResNet-32 [11] là mô hình đơn giản với 1.8 M tham số và VGG-19 [33] là mô hình phức tạp với 20 M tham số. Lưu ý rằng chúng tôi sử dụng triển khai được đưa ra trong [34]. Như được giải thích trong Mục 3.2, chúng tôi nhóm huấn luyện của mình thành 3 bước. Đầu tiên chúng tôi huấn luyện mô hình của mình trong 60 epoch đến hội tụ (bước 1), sử dụng tốc độ học không đổi 0.1. Trong bước chính quy hóa tiếp theo, chúng tôi khởi tạo chính quy hóa với λinit = 5·10^-6, η = 1.05, và sử dụng cùng tốc độ học như được sử dụng trong huấn luyện trước. Bước tinh chỉnh (bước 3) tương tự như [34], vì chúng tôi huấn luyện trong 160 epoch trên CIFAR-10/100 và 300 epoch trên TinyImageNet, sử dụng tốc độ học 0.1 và áp dụng phân rã nhân với 0.1 tại 2/4 và 3/4 tổng số epoch. Chúng tôi cũng thích ứng kích thước batch 64 và weight-decay 10^-4. Tất cả thí nghiệm được tính trung bình trên 5 lần chạy.

Chúng tôi so sánh phương pháp ART của mình với SNIP [20], Grasp [42], SRatio [34], và LTH [9] tương tự như được thực hiện trong [34, 41]. Ngoài ra chúng tôi đánh giá IMP [16] và RigL [8] như các phương pháp cắt tỉa động. Để có tính so sánh, tất cả đối thủ cạnh tranh trong thí nghiệm của chúng tôi được huấn luyện với cùng thiết lập như được đưa ra trong bước tinh chỉnh. Để cải thiện hiệu suất của RigL, chúng tôi mở rộng thời gian huấn luyện thêm 360 epoch. Chi tiết thêm được đưa ra trong tài liệu bổ sung, Mục A.

4.2. Mức độ Thưa
Trong phần này, chúng tôi so sánh hiệu suất của ART với các phương pháp khác trên các mức độ thưa khác nhau κ∈{90%,95%,98%,99%,99.5%,99.8%}, sử dụng các bộ dữ liệu và mô hình khác nhau. Để chứng minh ưu điểm của mất mát chính quy hóa mới của chúng tôi, chúng tôi bổ sung thay thế HyperSparse bằng L1[39] và L2[44]. Bảng 1 hiển thị độ chính xác kết quả với độ lệch chuẩn.

Phương pháp ART của chúng tôi kết hợp với HyperSparse vượt trội hơn các phương pháp SNIP [20], Grasp [42], SRatio [34], LTH [9] và RigL [8] trên tất cả mức độ thưa. Xem xét độ thưa cao 99%, 99.5% và 99.8%, tất cả đối thủ cạnh tranh giảm mạnh về độ chính xác, thậm chí đến ranh giới phân loại tối thiểu của dự đoán ngẫu nhiên cho SNIP và LTH sử dụng VGG-19. Tuy nhiên, ART có thể giữ độ chính xác cao ngay cả ở mức độ thưa cực cao. So với các mất mát chính quy hóa L1 và L2, mất mát HyperSparse của chúng tôi đạt độ chính xác cao hơn trong gần như tất cả các thiết lập và thậm chí giảm thiểu phương sai. Nếu chúng tôi bỏ qua bước Huấn luyện trước (bước 1) của ART, hiệu suất giảm nhẹ. Tuy nhiên, ART không có huấn luyện trước vẫn có kết quả tốt.

Hơn nữa, chúng tôi trình bày số epoch được huấn luyện cho giai đoạn chính quy hóa (bước 2) trong Bảng 2. Trong hầu hết các trường hợp, HyperSparse yêu cầu ít epoch hơn để kết thúc so với L1 và L2 và hội tụ nhanh hơn đến mô hình thưa hoạt động tốt. Như một khía cạnh thứ hai, ART thay đổi động độ dài huấn luyện theo mức độ thưa, mô hình và độ phức tạp dữ liệu. Do đó, ART huấn luyện lâu hơn nếu yêu cầu độ thưa cao hơn hoặc mô hình có nhiều tham số hơn và phức tạp hơn như VGG-19. So sánh hai bộ dữ liệu CIFAR-10 và CIFAR-100, có cùng số mẫu huấn luyện và do đó cùng số bước tối ưu hóa mỗi epoch, ART mở rộng độ dài huấn luyện cho vấn đề phân loại phức tạp hơn trong CIFAR-100.

ART huấn luyện mô hình trong 60 epoch trong huấn luyện trước (bước 1) và 160 epoch trong tinh chỉnh (bước 3). Xem xét độ dài huấn luyện động trong bước 2, các epoch của ART sử dụng LHS tổng cộng từ 226.2 đến 301.2 epoch trung bình. So sánh, các phương pháp cắt tỉa lặp tốn kém tính toán hơn nhiều, vì mỗi mô hình được huấn luyện nhiều lần. Ví dụ, IMP [16] yêu cầu 860 epoch trên CIFAR-10/100 trong thí nghiệm của chúng tôi.

4.3. Gradient Nhận thức Khám phá và Khai thác
Lịch trình huấn luyện của ART cho phép khám phá các cấu trúc mới của mạng thưa, trong khi nén mạng dày đặc vào các trọng số còn lại được khai thác để giảm thiểu mất mát Lclass. Để giảm sự đánh đổi giữa khám phá và khai thác, mất mát chính quy hóa HyperSparse của chúng tôi phạt các trọng số nhỏ với chính quy hóa cao hơn và buộc hầu hết trọng số gần bằng không, trong khi bảo tồn độ lớn của các trọng số còn lại sau khi cắt tỉa. Để làm nổi bật hành vi có lợi của HyperSparse, phần này hình dung và phân tích gradient. Hình 1 hiển thị các giá trị và gradient tương ứng của tất cả trọng số, được sắp xếp theo độ lớn trọng số. Lưu ý rằng chúng tôi chỉ tập trung vào bước thứ hai của ART, nơi chính quy hóa được kết hợp. Epoch 0 đại diện cho epoch đầu tiên sử dụng chính quy hóa. Trong hình con dưới, chúng ta quan sát rằng gradient của HyperSparse đối với các trọng số lớn hơn |wκ| gần hơn với 0 so với các trọng số nhỏ hơn. So sánh, L1 vẫn không đổi 1 cho tất cả trọng số. Hiệu ứng tăng chính quy hóa của các trọng số nhỏ mạnh hơn đối với mạng có nhiều trọng số gần bằng không và do đó khuếch đại theo thời gian, vì chính quy hóa tăng dần thu nhỏ độ lớn trọng số. Ví dụ, epoch 40 hiển thị gradient cao hơn cho các trọng số nhỏ so với epoch 0, trong khi có nhiều trọng số với độ lớn thấp hơn. LHS phụ thuộc tỷ lệ cắt tỉa κ tăng gradient cho các trọng số nhỏ |w|<|wκ| theo thời gian nhưng bảo tồn gradient thấp của các trọng số lớn hơn |w|>|wκ| xấp xỉ tại 0 để ưu tiên khai thác. Trong quá trình tối ưu hóa, gradient vẫn mượt và tăng từ từ cho các trọng số nhỏ hơn, nhưng gần với |wκ|. Điều này ưu tiên khám phá trong miền trọng số gần wκ. Do đó, mô hình trở nên thưa vốn có và hành vi chuyển đổi liên tục từ khám phá sang khai thác.

4.4. Sắp xếp lại Trọng số
Chúng tôi sử dụng mất mát chính quy hóa với đòn bẩy tăng dần để tìm một tập hợp trọng số hợp lý, còn lại sau khi cắt tỉa. Chúng tôi ngầm thực hiện điều này bằng cách thu nhỏ các trọng số nhỏ gần bằng không. Trong quá trình huấn luyện, trọng số được sắp xếp lại và do đó có thể thay đổi thành viên từ tập hợp bị cắt tỉa sang trọng số còn lại, và ngược lại. Chúng tôi phân tích quy trình sắp xếp lại trong Hình 2, hiển thị giao điểm của mặt nạ trung gian và cuối cùng qua tất cả epoch, sử dụng các mất mát chính quy hóa khác nhau trong ART. Mô hình được huấn luyện trước đến hội tụ mà không có chính quy hóa trong 60 epoch đầu (bước 1) và với chính quy hóa trong các epoch tiếp theo (bước 2). Tinh chỉnh không được hình dung (bước 3). Sau khi huấn luyện trước, các trọng số cao nhất chỉ giao với mặt nạ cuối cùng lên đến 20% với L1 và L2, trong khi HyperSparse dẫn đến giao điểm xấp xỉ 35%. Kết quả này cho thấy HyperSparse thay đổi ít tham số hơn trong khi sắp xếp lại trọng số, ngụ ý rằng nhiều cấu trúc từ mô hình dày đặc được khai thác. Nó cũng cho thấy HyperSparse có thời gian học ngắn hơn đáng kể so với L1 và L2. Các thanh ngang chỉ đến giao điểm trước epoch huấn luyện cuối cùng và cho thấy L1 và L2 chỉ giao với 60% và 50%, trong khi HyperSparse tiến rất gần đến mặt nạ cuối cùng với hơn 90% giao điểm. Điều này cho thấy HyperSparse tìm thấy một tập hợp trọng số có giá trị cao ổn định hơn và giảm khám phá, vì mặt nạ có ít biến thể hơn trong các epoch cuối cùng. Nhiều kết quả cho các thiết lập huấn luyện khác được hiển thị trong tài liệu bổ sung, Mục B.

Hơn nữa, chúng tôi phân tích phân phối trọng số kết quả của phương pháp của mình và so sánh nó với IMP [16] và SRatio [34]. Hình 3 hiển thị số trọng số còn lại mỗi lớp cho ResNet-32 bao gồm ba mức tỷ lệ, kết thúc với lớp tuyến tính (LL). Mỗi mức tỷ lệ bao gồm bốn khối dư (RES), được kết nối bởi khối downsampling (DS). Cấu trúc cơ bản của ART và IMP trông tương tự, vì cả hai phương pháp đều hiển thị tỷ lệ giữ không đổi qua các khối dư. Hơn nữa, ART và IMP sử dụng nhiều tham số hơn trong các lớp downsampling và tuyến tính. Chúng tôi kết luận rằng hai loại lớp này yêu cầu nhiều trọng số hơn và do đó quan trọng hơn đối với mô hình. Độ chính xác cao hơn được thảo luận trước đây gợi ý rằng phương pháp của chúng tôi khai thác các trọng số này tốt hơn. Để cho thấy rằng kết quả này cũng được thu được trên các bộ dữ liệu, mô hình và mức độ thưa khác, chúng tôi mô tả thêm phân phối trọng số trong tài liệu bổ sung, Mục C và cho thấy số tham số trong lớp tuyến tính giảm mạnh cho một tập hợp nhỏ các lớp trong CIFAR-10. Hơn nữa, phương pháp so sánh SRatio giả định rằng mạng thưa phù hợp có thể được thu được bằng tỷ lệ giữ thủ công mỗi lớp. Nó có tỷ lệ giữ giảm bậc hai có thể quan sát trong Hình 3. Như được hiển thị trong Bảng 1, phương pháp ART của chúng tôi hoạt động tốt hơn đáng kể so với SRatio và do đó chúng tôi suy luận rằng tỷ lệ giữ cố định có tác động bất lợi đến hiệu suất. Sắp xếp lại trọng số trong quá trình huấn luyện ưu tiên NN thưa hoạt động tốt, đặc biệt trong các chế độ thưa cao.

4.5. Mạng nén cái gì đầu tiên?
Cùng với việc giới thiệu ART, chúng tôi đối mặt với câu hỏi những mẫu nào được nén đầu tiên vào các trọng số lớn còn lại sau khi cắt tỉa độ lớn trong quá trình chính quy hóa. Câu hỏi này trái ngược với câu hỏi của Hooker "Mạng Nơ-ron Sâu Nén quên gì?" [13] và thách thức giả định cơ bản của cắt tỉa độ lớn, giả định các trọng số lớn là quan trọng nhất. Trong phần này, chúng tôi phân tích thứ tự thời gian của cách các mẫu được nén và giới thiệu chỉ số Vị trí Nén (CP) để xác định nó.

Theo phương pháp của chúng tôi, chính quy hóa bắt đầu tại epoch eS và kết thúc tại eE và do đó trọng số W có các trạng thái khác nhau W={We}eEe=eS trong quá trình huấn luyện. Chúng tôi đo độ chính xác cá nhân theo thời gian ψI đạt được bởi mạng thưa cho một mẫu huấn luyện (x, y)∈S, được định nghĩa bởi

ψI(x, y, f, W) = |{We∈W | f(ν(We)⊙We, x) = y}|/(eE−eS). (4)

Sau khi tính độ chính xác cá nhân cho tất cả mẫu Ψ = {ψI(xn, yn, f, W)}Nn=1 và sắp xếp Ψ theo thứ tự giảm dần, chỉ số CP(x, y, f, W) mô tả vị trí tương đối của ψI(x, y, f, W) trong sort(Ψ). Nói cách khác, các mẫu được nén sớm và phân loại đúng có được CP thấp gần 0, và những mẫu được nén sau gần hơn với 1.

Chúng tôi tính chỉ số CP cho tất cả mẫu trong CIFAR-10 trong quá trình huấn luyện NN dày đặc, thưa thấp và thưa cao. Hành vi nén cho NN dày đặc được đo trong giai đoạn huấn luyện trước (eS=0 và eE=60) và cho NN thưa trong giai đoạn chính quy hóa (eS=60 và eE=emax). Để hiển thị, mẫu nào được nén đầu tiên vào các trọng số cao nhất còn lại, 5% mẫu với CP thấp nhất được hình dung trong Hình 4 trong không gian tiềm ẩn của khung CLIP nổi tiếng [29] được ánh xạ bởi t-SNE [40]. Như thường biết, mô hình dày đặc nén các mẫu dễ của tất cả lớp trong giai đoạn đầu [17, 21], trong khi mô hình thưa thấp đã mất một số. Trong chế độ thưa cao không còn quy tắc quyết định phân biệt nào ở đầu huấn luyện, và các lớp còn lại được nén từng bước khi huấn luyện tiếp tục (xem tài liệu bổ sung, Mục E). Trong thí nghiệm của chúng tôi, chúng tôi đã thấy liên tục rằng có một thiên vị đối với lớp deer. Chúng tôi gọi hiệu ứng này là "thiên vị deer", phải được giảm bằng chính quy hóa. Thiên vị deer gợi ý rằng các trọng số lớn trong NN dày đặc không mã hóa quy tắc quyết định cho tất cả lớp.

Để định lượng kết quả trên, Bảng 3 hiển thị CP trung bình cho tất cả mẫu thuộc về một lớp cụ thể. Ngoài ra, chúng tôi chia các tập hợp lớp thành bốn tập con theo độ khó của chúng. Chúng tôi ước tính độ khó của một mẫu bằng cách đếm số lỗi nhãn của con người được tạo ra từ ba người chú thích con người được lấy từ CIFAR-N [43], ví dụ, 2 có nghĩa là hai trong ba người đã gán nhãn sai mẫu. Quan sát đầu tiên là sự tách biệt các lớp nói trên được xác nhận, vì các giá trị CP tương tự trong NN dày đặc, nhưng phân kỳ trong NN thưa. Trong các chế độ thưa cao, thiên vị deer tồn tại trước khi các mẫu đầu tiên của các lớp khác được nén. Các lớp horse và airplane chỉ được bao gồm ở cuối huấn luyện. Quan sát thứ hai là, trong một tập hợp đóng các mẫu thuộc về một lớp, các mẫu khó được nén sau. Bản chất này tương tự như quá trình huấn luyện của NN dày đặc. Chi tiết triển khai và kết quả chi tiết hơn có sẵn trong tài liệu bổ sung, Mục E.

5. Kết luận
Công trình của chúng tôi trình bày Huấn luyện Chính quy hóa Thích ứng (ART), một phương pháp sử dụng chính quy hóa để có được mạng nơ-ron thưa. Chính quy hóa được khuếch đại liên tục và được sử dụng để thu nhỏ hầu hết độ lớn trọng số gần bằng không. Chúng tôi giới thiệu mất mát chính quy hóa mới HyperSparse tạo ra độ thưa vốn có trong khi duy trì sự đánh đổi cân bằng tốt giữa khám phá các cấu trúc thưa mới và khai thác các trọng số còn lại sau khi cắt tỉa. Các thí nghiệm mở rộng trên CIFAR và TinyImageNet cho thấy khung mới của chúng tôi vượt trội hơn các đối thủ cạnh tranh học thưa. HyperSparse vượt trội hơn các mất mát chính quy hóa tiêu chuẩn và dẫn đến những cải thiện hiệu suất ấn tượng trong các chế độ thưa cực cao và nhanh hơn nhiều. Các nghiên cứu bổ sung cung cấp những hiểu biết mới về phân phối trọng số trong quá trình nén mạng và về các mẫu được mã hóa trong các trọng số có giá trị cao.

Nhìn chung, công trình này cung cấp những hiểu biết mới về mạng nơ-ron thưa và giúp phát triển học máy bền vững bằng cách giảm độ phức tạp mạng nơ-ron.

6. Lời cảm ơn
Công trình này được hỗ trợ bởi Bộ Giáo dục và Nghiên cứu Liên bang (BMBF), Đức theo dự án trung tâm dịch vụ AI KISSKI (số cấp phép 01IS22093C), Deutsche Forschungsgemeinschaft (DFG) theo Chiến lược Xuất sắc của Đức trong Cụm Xuất sắc PhoenixD (EXC 2122), và bởi Bộ Môi trường, Bảo tồn Thiên nhiên, An toàn Hạt nhân và Bảo vệ Người tiêu dùng Liên bang, Đức theo dự án GreenAutoML4FAS (số cấp phép 67KI32007A).
