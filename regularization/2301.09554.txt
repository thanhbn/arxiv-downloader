# 2301.09554.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/regularization/2301.09554.pdf
# File size: 1252721 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Deep Learning Meets Sparse Regularization:
A Signal Processing Perspective
Rahul Parhi, Member, IEEE , and Robert D. Nowak, Fellow, IEEE
Abstract
Deep learning has been wildly successful in practice and most state-of-the-art machine
learning methods are based on neural networks. Lacking, however, is a rigorous mathematical
theory that adequately explains the amazing performance of deep neural networks. In this
article, we present a relatively new mathematical framework that provides the beginning of a
deeper understanding of deep learning. This framework precisely characterizes the functional
properties of neural networks that are trained to fit to data. The key mathematical tools which
support this framework include transform-domain sparse regularization, the Radon transform of
computed tomography, and approximation theory, which are all techniques deeply rooted in signal
processing. This framework explains the effect of weight decay regularization in neural network
training, the use of skip connections and low-rank weight matrices in network architectures,
the role of sparsity in neural networks, and explains why neural networks can perform well in
high-dimensional problems.
I. I NTRODUCTION
Deep learning (DL) has revolutionized engineering and the sciences in the modern, data age.
The typical goal of DL is to predict an output y∈ Y (e.g., a label or response) from an input
x∈ X (e.g., a feature or example). A neural network (NN) is “trained” to fit to a set of data
consisting of the pairs {(xn, yn)}N
n=1, by finding a set of NN parameters θso that the NN
mapping closely matches the data. The trained NN is a function, denoted by fθ:X → Y , that
can be used to predict the output y∈ Y of a new input x∈ X. This paradigm is referred to as
Rahul Parhi is with the Biomedical Imaging Group, ´Ecole Polytechnique F ´ed´erale de Lausanne, Lausanne, Switzerland
(e-mail: rahul.parhi@epfl.ch).
Robert D. Nowak is with the Department of Electrical and Computer Engineering, University of Wisconsin–Madison,
Madison, WI, USA (e-mail: rdnowak@wisc.edu).arXiv:2301.09554v3  [stat.ML]  8 Jun 2023

--- PAGE 2 ---
2
supervised learning , which is the focus of this article. The success of deep learning has spawned a
burgeoning industry that is continually developing new applications, NN architectures, and training
algorithms. This article reviews recent developments in the mathematics of DL, focused on the
characterization of the kinds of functions learned by NNs fit to data. There are currently many
competing theories to explain the success of DL. These developments are part of a wider body
of theoretical work that can be crudely organized into three broad categories: 1) approximation
theory with NNs; 2) the design and analysis of optimization (“training”) algorithms for NNs; and
3) characterizations of the properties of trained NNs.
This article belongs to the latter category of research and investigates the functional properties
(i.e., the regularity ) of solutions to NN training problems with explicit , Tikhonov-type regularization.
While much of the success of deep learning in practice comes from networks with highly structured
architectures, it is hard to establish a rigorous and unified theory for such NNs used in practice.
Therefore, we primarily focus on fully-connected, feedforward NNs with the popular Rectified
Linear Unit (ReLU) activation function. This article introduces a mathematical framework that
unifies a line of work from several authors over the last few years that sheds light on the nature
and behavior of NN functions which are trained to a global minimizer with explicit regularization.
The presented results are just one piece of the puzzle towards developing a mathematical theory
of deep learning. The purpose of this article is, in particular, to provide a gentle introduction
to this new mathematical framework, accessible to readers with a mathematical background in
Signals and Systems andApplied Linear Algebra . The framework is based on mathematical tools
familiar to the signal processing community, including transform-domain sparse regularization,
the Radon transform of computed tomography, and approximation theory. It is also related to
well-known signal processing ideas such as wavelets, splines, and compressed sensing. This
framework provides a new take on the following fundamental questions:
1) What is the effect of regularization in DL?
2) What kinds of functions do NNs learn?
3) What is the role of NN activation functions?
4) Why do NNs seemingly break the curse of dimensionality?
II. N EURAL NETWORKS AND LEARNING FROM DATA
The task of DL corresponds to learning the input-output mapping from a data set in a hierarchical
or multi-layer manner. Deep neural networks (DNNs) are complicated function mappings built
from many smaller, simpler building blocks. The simplest building block of a DNN is an (artificial)

--- PAGE 3 ---
3
(a) Sigmoid
 (b) Unit Step
 (c) ReLU
Fig. 1. Typical activation functions found in neural networks.
neuron, inspired by the biological neurons of the brain [ 24]. A neuron is a function mapping
Rd→Rof the form z7→σ(wTz−b), where w∈Rdcorresponds to the weights of the
neuron and b∈Rcorresponds to the bias of the neuron. The function σ:R→Ris referred
to as the activation function of the neuron and controls nonlinear response of the neuron. A
neuron “activates” when the weighted combination of its input xexceeds a certain threshold,
i.e.,wTx> b. Therefore, typical activation functions such as the sigmoid, unit step function, or
rectified linear unit (ReLU) activate when their input exceeds 0as seen in Fig. 1.
A neuron is composed of a linear mapping followed by a nonlinearity. A popular form (or
“architecture”) of a DNN is a fully-connected feedforward DNN which is a cascade of alternating
linear mappings and component-wise nonlinearities. A feedforward DNN fθ(parameterized by
θ) can be represented as the function composition
fθ(x) =A(L)◦σ◦A(L−1)◦ ···σ◦A(1)(x), (1)
where, for each ℓ= 1, . . . , L , the function A(ℓ)(z) =W(ℓ)z−b(ℓ)is an affine linear mapping
with weight matrix W(ℓ)and bias vector b(ℓ). The functions σthat appear in the composition
apply the activation function σ:R→Rcomponent-wise to the vector A(ℓ)(z). While the
activation function could change from neuron to neuron, we assume that the same activation
function is used in the entire network in this article. The parameters of this DNN are the weights
and biases, i.e., θ={(W(ℓ),b(ℓ))}L
ℓ=1. Each mapping A(ℓ)corresponds to a layer of the DNN
and the number of mappings Lis the depth of the DNN. The dimensions of the weight matrices
W(ℓ)correspond to the number of neurons in each layer (i.e., the width of the layer). Alternative
DNN architectures can be built with other simple building blocks, e.g., with convolutions and
pooling/downsampling operations, which would correspond to deep convolutional neural networks
(DCNNs). DNN architectures are often depicted with diagrams as in Fig. 2.

--- PAGE 4 ---
4
(a) Feedforward DNN
z2 w2 Σ σ v2 Z2z1 w1
zd wdv1
vDZ1
ZD
−b......Previous Layer Outputs Next Layer Inputs
(b) A Single Neuron
Fig. 2. Example depiction of a deep neural network and its components: (a) a feedforward DNN architecture where
the nodes represent the neurons and the edges represent the weights; (b) a single neuron from the DNN in (a) mapping
an input z∈Rdto an output Z∈RDviaZ=vσ(wTz−b).
Given a DNN fθparameterized by θ∈Θ(of any architecture), the task of learning from the
data{(xn, yn)}N
n=1is formulated as the optimization problem
min
θ∈ΘNX
n=1L(yn, fθ(xn)), (2)
where L(·,·)is a loss function (squared error, logistic, hinge loss, etc.). For example, the squared
error loss is given by L(y, z) = (y−z)2. A DNN is trained by solving this optimization problem,
usually via some form of gradient descent. In typical scenarios, this optimization problem is
ill-posed and so the problem is regularized either explicitly through the addition of a regularization
term and/or implicitly by constraints on the network architecture or the behavior of gradient
descent procedures [ 35]. A surprising phenomenon of gradient descent training algorithms for
overparameterized NNs is that, among the many solutions which overfit the data, these algorithms

--- PAGE 5 ---
5
select one which often generalizes well on new data, even without explicit regularization. This has
led to researchers trying to understand the role of overparameterization and the effect of random
initialization of NN parameters on the implicit bias of gradient-based training algorithms [8].
On the other hand, explicit regularization corresponds to solving an optimization problem of
the form
min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λC(θ), (3)
where C(θ)≥0for all θ∈Θ.C(θ)is aregularizer which measures the “size” (or “capacity”)
of the DNN parameterized by θ∈Θandλ >0is an adjustable hyperparamter which controls a
trade-off between the data-fitting term and the regularizer. DNNs are often trained using gradient
descent algorithms with weight decay which corresponds to solving the optimization problem
min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λCwd(θ), (4)
where the weight decay regularizer Cwd(θ)is the squared Euclidean-norm of all the network
weights. Sometimes the weight decay objective regularizes all parameters, including biases, while
sometimes it only regularizes the weights (so that the biases are unregularized). This article
focuses on the variant of weight decay with unregularized biases.
III. W HAT IS THE EFFECT OF REGULARIZATION IN DEEPLEARNING ?
Weight decay is a common form of regularization for DNNs. On the surface, it appears to simply
be the familiar Tikhonov or “ridge” regularization. In standard linear models, it is well-known
that this sort of regularization tends to reduce the size of the weights, but does not produce
sparse weights. However, when this regularization is used in conjunction with NNs, the results
are strikingly different. Regularizing the sum of squared weights turns out to be equivalent to
regularization with a type of ℓ1-norm regularization on the network weights, leading to sparse
solutions in which the weights of many neurons are zero [ 49]. This is due to the key property
that the most commonly used activation functions in DNNs are homogeneous . A function σ(t)is
said to be homogeneous (of degree 1) ifσ(γt) =γσ(t)for any γ >0. The most common NN
activation function, the ReLU, is homogeneous, as well as the leaky ReLU, the linear activation,
and pooling/downsampling units. This homogeneity leads to the following theorem, referred to as
theneural balance theorem (NBT).

--- PAGE 6 ---
6
Neural Balance Theorem ([ 49, Theorem 1]) : Let fθbe a DNN of any architecture
parameterized by θ∈Θwhich solves the DNN training problem with weight decay in (4).
Then, the weights satisfy the following balance constraint: if wandvdenote the input and
output weights of any homogeneous unit in the DNN, then ∥w∥2=∥v∥2.
The proof of this theorem boils down to the simple observation that for any homogeneous unit
with input weights wand output weights v, we can scale the input weight by γ >0and the output
weight by 1/γwithout changing the function mapping. For example, consider the single neuron
z7→vσ(wTz−b)with homogeneous activation function σas depicted in Fig. 2(b). In the case
of a DNN as in (1), wcorresponds to a row of a weight matrix in the affine mapping of a layer,
vcorresponds to a column of the weight matrix in the subsequent layer, and bcorresponds to an
entry in the bias vector. It is immediate that (v/γ)σ((γw)Tz−γb) =vσ(wTz−b). By noting
that the biases are unregularized, the theorem follows by noticing that minγ>0∥γw∥2
2+∥v/γ∥2
2
occurs when γ=p
∥v∥2/∥w∥2which implies that the minimum squared Euclidean-norm solution
must satisfy the property that the input and output weights wandvare balanced.
A. The Secret Sparsity of Weight Decay
The balancing effect of the NBT has a striking effect on solutions to the weight decay objective.
In particular, a sparsity-promoting effect akin to least absolute shrinkage and selection operator
(LASSO) regularization [ 43]. As an illustrative example, consider a shallow (L= 2), feedforward
NN mapping Rd→RDwith a homogeneous activation function (e.g., the ReLU) and Kneurons.
In this case, the NN is given by
fθ(x) =KX
k=1vkσ(wT
kx−bk). (5)
Here, the weight decay regularizer is of the form1
2PK
k=1∥vk∥2
2+∥wk∥2
2, where wkandvkare
the input and output weights of the kth neuron, respectively. By the NBT, this is equivalent to
using the regularizerPK
k=1∥vk∥2∥wk∥2. Due to the homogeneity of the activation function, we
can assume, without loss of generality, that ∥wk∥2= 1 by “absorbing” the magnitude of the input
weight wkinto the output weight vk. Therefore, by constraining the input weights to be unit-norm,
the training problem can then be reformulated with the regularizerPK
k=1∥vk∥2[49]. Remarkably,
this is exactly the well-known group LASSO regularizer [ 50], which favors solutions with few
active neuron connections (i.e., solutions typically have many vkexactly equal to 0), although

--- PAGE 7 ---
7
the overall training objective remains nonconvex. We also note that there is a line of work that
has reformulated the nonconvex training problem as a convex group LASSO problem [33].
More generally, consider the feedforward deep NN architecture in (1) with a homogeneous
activation function and consider training the DNN with weight decay only on the network weights.
An application of the NBT shows that the weight decay problem is equivalent to the regularized
DNN training problem with the regularizer
C(θ) =1
2K(1)X
k=1∥w(1)
k∥2
2+1
2K(L)X
k=1∥v(L)
k∥2
2+LX
ℓ=1K(ℓ)X
k=1∥w(ℓ)
k∥2∥v(ℓ)
k∥2, (6)
where K(ℓ)denotes the number of neurons in layer ℓ,w(ℓ)
kdenotes the input weights to the kth
neuron in layer ℓ, and v(ℓ)
kdenotes the output weights to the kth neuron in layer ℓ(see [ 49,
Equation (2)]). Solutions based on this regularizer will also be sparse due to the 2-norms that
appear in the last term in (6) being not squared , akin to the group LASSO regularizer. In particular,
this regularizer can be viewed as a mixed ℓ2,1-norm on the weight matrices. Moreover, increasing
the regularization parameter λ, will increase the number of weights that are zero in the solution.
Therefore, training the DNN with weight decay favors sparse solutions, where sparsity is quantified
via the number of active neuron connections. An early version of this result appeared in 1998 [ 16],
although it did not become well-known until it was rediscovered in 2015 [25].
IV. W HAT KINDS OF FUNCTIONS DONEURAL NETWORKS LEARN ?
The sparsity-promoting effect of weight decay arising from the NBT in network architectures
with homogeneous activation functions has several consequences on the properties of trained
NNs. In this section, we will focus on the popular ReLU activation function, ρ(t) = max {0, t}.
The imposed sparsity not only promotes sparsity in the sense of the number of active neuron
connections, but also promotes a kind of transform-domain sparsity. This transform-domain
sparsity suggests the inclusion of skip connections and low-rank weight matrices in network
architectures.
A. Shallow Neural Networks
In the univariate case, a shallow, feedforward ReLU NN with Kneurons is realized by the
mapping
fθ(x) =KX
k=1vkρ(wkx−bk). (7)

--- PAGE 8 ---
8
Training this NN with weight decay corresponds to the solving the optimization problem
min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λ
2KX
k=1|vk|2+|wk|2, (8)
From Section III-A, we saw that the NBT implies that this problem is equivalent to
min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λKX
k=1|vk||wk|. (9)
As illustrated in Insert IN1, we see that (9) is actually regularizing the integral of second
derivative of the NN, which can be viewed as a measure of sparsity in the second derivative
domain. The integral in (15) must be understood in the distributional sense since the Dirac
impulse is not a function, but a generalized function or distribution . To make this precise, let
gε(x) =e−x2/2ε/√
2πεdenote the Gaussian density with variance ε >0. As is well-known in
signal processing, gεconverges to the Dirac impulse as ε→0. Using this idea, given a distribution
f, define the norm
∥f∥M:= sup
ε>0∥f∗gε∥L1= sup
ε>0Z∞
−∞Z∞
−∞f(x)gε(y−x) dxdy. (16)
This definition provides an explicit construction, via the convolution with a Gaussian, of a
sequence of smooth functions that converge to f, where the supremum acts as the limit. For
example, if f(x) =g(x) +PK
k=1vkδ(x−tk), where gis an absolutely integrable function,
then∥f∥M=∥g∥L1+PK
k=1|vk|=∥g∥L1+∥v∥1, with ∥v∥1=PK
k=1|vk|. It is in this sense
that (15) holds, i.e., ∥D2fθ∥M=PK
k=1|vk||wk|. In particular, the M-norm is precisely the
continuous-domain analogue of the sparsity-promoting discrete ℓ1-norm. Therefore, we see that
training a NN with weight decay as in (8) prefers solutions with sparse second derivatives.
It turns out that the connection between between sparsity in the second derivative domain
and NNs is even tighter. Let BV2(R)denote the space of functions mapping R→Rsuch that
∥D2f∥Mis finite. This is the space of functions of second-order bounded variation and the
quantity ∥D2f∥Mis the second-order total variation (TV)1off. It is well-known from spline
theory [ 14], [23], [46] that functions that fit data and have minimimal second-order TV are
continuous piecewise linear (CPwL) functions. Since the ReLU is a CPwL function, ReLU NNs
1The classical notion of TV , often used in signal denoising problems [ 37], isTV(f):=∥Df∥Mand so the
second-order TV of fcan be viewed as the TV of the derivative of f:∥D2f∥M= TV(D f).

--- PAGE 9 ---
9
[IN1] ReLU Sparsity in the Second Derivative Domain
Given a ReLU neuron r(x) =ρ(wx−b),
its first derivative, Dr(x), is
Dr(x) = D ρ(wx−b)
=w u(wx−b), (10)
where uis the unit step function (Fig. 1(b)).
Therefore, its second derivative, D2r(x), is
D2r(x) = D w u(wx−b)
=w2δ(wx−b). (11)
By the scaling property of the Dirac im-
pulse [27, Problem 1.38(a)]
δ(γx) =1
|γ|δ(x) (12)we have
D2r(x) =w2
|w|δ
x−b
w
=|w|δ
x−b
w
. (13)
The second derivative of the NN (7) is then
D2fθ(x) =KX
k=1vk|wk|δ
x−bk
wk
.(14)
Therefore,
Z∞
−∞|D2fθ(x)|dx=KX
k=1|vk||wk|.(15)
b1
w1b2
w2b3
w3b4
w4b5
w5b6
w6fθ(x)
Dfθ(x)
v1|w1|v3|w3|v5|w5|
v2|w2|v4|w4|v6|w6|D2fθ(x)
Fig. 3. Illustration of the sparsity in the second derivative domain of a univariate, shallow feedforward NN
with6neurons.
are CPwL functions [ 3]. In fact, under mild assumptions on the loss function, the solution set to
the optimization problem
min
f∈BV2(R)NX
n=1L(yn, f(xn)) +λ∥D2f∥M (17)
is completely characterized by NNs of the form
fθ(x) =KX
k=1vkρ(wkx−bk) +c1x+c0, (18)

--- PAGE 10 ---
10
where the number of neurons is strictly less than the number of data ( K < N ) in the sense that
the solution set to (17) is a closed convex set whose extreme points take the form of (18) with
K < N [9], [29], [39]. In neural network parlance, the c1x+c0term is a skip connection [17].
This term is an affine function that naturally arises since the second-order TV of an affine function
is zero and so the regularizer places no penalty for the inclusion of this term.
The intuition behind this result is due the fact that the second derivative of a CPwL function is
an impulse train and therefore exhibits extreme sparsity in the second derivative domain. This is
illustrated in Fig. 3. Therefore, the optimization problem (17) will favor sparse CPwL functions
which always admit a representation as in (18). In signal processing parlance, “signals” that
are sparse in some transform domain are said to have a finite rate of innovation [47]. Here, the
transform involved is the second derivative operator and the innovation is the impulse train that
arises after applying the second derivative operator to a CPwL function.
Consider the optimization over the NN parameter space ΘKof networks as in (18) with
fixed width K≥N. From the derivation in Insert IN1, we have {fθ:θ∈ΘK} ⊂BV2(R).
Furthermore, from the above discussion, we know there always exists an optimal solution to
(17) that takes the form of (18) with K < N , i.e., there always exists a solution to (17) in
{fθ:θ∈ΘK}. Therefore, from the equivalence of (8) and (9), we see that training a sufficiently
wide ( K≥N) NN with a skip connection (18) and weight decay (8) results in a solution to the
optimization problem (17) over the function space BV2(R). While this result may seem obvious
in hindsight, it is remarkable since it says that the kinds of functions that NNs trained with weight
decay (to a global minimizer) are exactly optimal functions in BV2(R). Moreover, this result
sheds light on the role of overparameterization since this correspondence hinges on the network
being critically parameterized or overparameterized (because K≥N).
In the multivariate case, a shallow feedforward NN has the form
fθ(x) =KX
k=1vkρ(wT
kx−bk). (22)
The key property connects the univariate case and BV2(R)was that ReLU neurons are sparsified
by the second derivative operator as in (13). A similar analysis can be carried out in the multivariate
case by finding an operator that is the sparsifying transform of the multivariate ReLU neuron
r(x) =ρ(wTx−b). The sparsifying transform was proposed in 2020 in the seminal work of Ongie
et al. [26], and hinges on the Radon transform that arises in tomographic imaging. Connections
between the Radon transform and neurons have been known since at least the 1990s, gaining
popularity due to the proposal of ridgelets [ 6] and early versions of the sparsifying transform

--- PAGE 11 ---
11
[IN2] The Radon Transform and Fourier Slice Theorem
The Radon transform, first studied by Radon in
1917 [ 36], of a function mapping Rd→Ris
specified by the integral transform
R{f}(α, t) =Z
Rdf(x)δ(αTx−t) dx,(19)
where δis the univariate Dirac impulse, α∈
Sd−1={u∈Rd:∥u∥2= 1}is a unit vector,
andt∈Ris a scalar. The Radon transform of f
at(α, t)is the integral of falong the hyperplane
{x∈Rd:αTx=t}.
The Radon transform is tightly linked with the
Fourier transform, specified by
bf(ω) =Z
Rdf(x)e−jωTxdx, (20)where j2=−1. Indeed,
\R{f}(α, ω)
=Z
RZ
Rdf(x)δ(αTx−t) dx
e−jωtdt
=Z
Rdf(x)Z
Rδ(αTx−t)e−jωtdt
dx
=Z
Rdf(x)e−j(ωα)Txdx=bf(ωα). (21)
This result is known as the Fourier slice theorem .
It states that the univariate Fourier transform in
the Radon domain corresponds to a slice of the
Fourier transform in the spatial domain.
for neurons were studied as early as 1997 [ 19]. A summary of the Radon transform appears in
IN2. The sparsifying transform for multivariate ReLU neurons is based on a result regarding the
(filtered) Radon transform, which appears in Insert IN3.
The filter Kin (26) is exactly the backprojection filter that arises in the filtered backprojection
algorithm in tomographic image reconstruction and acts as a high-pass filter (or ramp filter) to
correct the attenuation of high frequencies from the Radon transform. The intuition behind this
is that the Radon transform integrates a function along hyperplanes. In the univariate case, the
magnitude of the frequency response of an integrator behaves as 1/|ω|and therefore attenuates
high frequencies. The magnitude of the frequency response of integration along a hyperplane,
therefore, behaves as 1/|ω|d−1, since hyperplanes are of dimension (d−1). Note that the even
projector that appears in (30) is due to the fact that the Radon-domain variables (α, t)and
(−α,−t)parameterize the same hyperplane.
From the derivation in Insert IN3, we immediately see that the sparsifying transform of the
multivariate ReLU neuron r(x) =ρ(wTx−b)with(w, b)∈Sd−1×Ris the operator D2
tKR,
where D2
t=∂2/∂t2denotes the second-order partial derivative with respect to t. We have
D2
tKR{r}(α, t) =δR((α, t)−(w, b)), (31)
where δR(z−z0):= P even{δ(z−z0)}= (δ(z−z0) +δ(z+z0))/2is the even symmetrization

--- PAGE 12 ---
12
[IN3] Filtered Radon Transform of a Neuron with Unit-Norm Input Weights
First consider the neuron r(x) =σ(wTx)with
w=e1= (1,0, . . . , 0)(the first canonical unit
vector). In this case, r(x) =σ(x1). By noticing
that this function can be written as a tensor
product, the Fourier transform is given by the
following product
br(ω) =bσ(ω1)dY
k=22πδ(ωk). (23)
By the Fourier slice theorem,
\R{r}(α, ω) =bσ(ωα1)dY
k=22πδ(ωαk).(24)
By the scaling property of the Dirac impulse
(12), the above quantity equals
=bσ(ωα1)(2π)d−1
|ω|d−1δ(α2, . . . , α d). (25)
If we define the filter via the frequency response
dKf(ω) =|ω|d−1
2(2π)d−1bf(ω), (26)
we find
KR{r}b (α, ω) =bσ(ωα1)
2δ(α2, . . . , α d).(27)Taking the inverse Fourier transform,
KR{r}(α, t)
=1
2|α1|σt
α1
δ(α2, . . . , α d)
=σ(t)δ(α1−1) +σ(−t)δ(α1+ 1)
2δ(α2, . . . , α d)
=σ(t)δ(α−e1) +σ(−t)δ(α+e1)
2
=:Peven{σ(t)δ(α−e1)}, (28)
where Peven is the projector which extracts the
even part of its input (in terms of the variables
(α, t)). The second line holds by the dilation
property of the Fourier transform [ 27, Equation
(4.34)]
1
|γ|ft
γ
F← − →bf(γω). (29)
Since α∈Sd−1, the third line holds by observ-
ing that when α1=±1,α2, . . . , α d= 0 and
so the second line is σ(±t)/2multiplied by an
impulse and when α1̸=±1, the second line is
0, which is exactly the third line. By the rotation
properties of the Fourier transform, we have the
following result for the neuron r(x) =σ(wTx)
KR{r}(α, t) = P even{σ(t)δ(α−w)},(30)
where w∈Sd−1.
of the Dirac impulse that arises due to the even symmetry of the Radon domain. From the
homogeneity of the ReLU activation, applying this sparsifying transform to the (unconstrained)
neuron r(x) =ρ(wTx−b)with(w, b)∈Rd×Ryields
D2
tKR{r}(α, t) =∥w∥2δR((α, t)−(ew,eb)), (32)
whereew=w/∥w∥2andeb=b/∥w∥2. This is analogous to the how D2is the sparsifying transform
for univariate neurons as in (13). The sparsifying operator is simply the second derivative in the

--- PAGE 13 ---
13
(a) Surface plot of r(x).
<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t<latexit sha1_base64="hQm0lR/mBmhjzkOfd5AoJhGlXNA=">AAACRnicbVDLSgMxFL1T3+Or6tJNsAgKpcyIr40giCC4UbG10Cklk2ZsMJkZkoxSwnyJX+NO9Av8CXciuDIdu7DWA4Fzz7mXm3vClDOlPe/NKU1MTk3PzM658wuLS8vlldWGSjJJaJ0kPJHNECvKWUzrmmlOm6mkWISc3oR3JwP/5p5KxZL4WvdT2hb4NmYRI1hbqVPeCwTWPSnMeR5UC66INFd5YGSQbwWhMAHmaQ/n6AgNqoe8ivR2p1zxal4BNE78IanAEBed8lfQTUgmaKwJx0q1fC/VbYOlZoTT3A0yRVNM7vAtbVkaY0FV2xTn5WjTKl0UJdK+WKNC/T1hsFCqL0LbWRzw1xuI/3mtTEeHbcPiNNM0Jj+LoowjnaBBVqjLJCWa9y3BRDL7V0R6WGKibaLuyJoiRRWpkVNMKHLXtWH5f6MZJ42dmr9f273crRyfDmObhXXYgC3w4QCO4QwuoA4EHuEJXuDVeXbenQ/n86e15Axn1mAEJfgGXSOyCg==</latexit>KR{r}(↵=w,t) (b) Filtered Radon transform when α=w.
<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t
<latexit sha1_base64="6wsDouJ++1/61Ohhkr95xquTDXg=">AAACSXicbVDNSgMxGMzW//Wv6tFLsAgKpeyKqEdBBMGLim2FppRsmm2DSXZNskoJ+yo+jTfRu4/hTXoy3fZg1YHAMPN9fJmJUs60CYIPrzQzOze/sLjkL6+srq2XNzYbOskUoXWS8ETdRVhTziStG2Y4vUsVxSLitBndn4385iNVmiXy1gxS2ha4J1nMCDZO6pRPkMCmr4S9zFG14Jooe5Mjq1C+hyJhEeZpH+cQSfoAR8JTXoVmv1OuBLWgAPxLwgmpgAmuOuUh6iYkE1QawrHWrTBITdtiZRjhNPdRpmmKyT3u0ZajEguq27ZImMNdp3RhnCj3pIGF+nPDYqH1QERussjw2xuJ/3mtzMQnbctkmhkqyfhQnHFoEjiqC3aZosTwgSOYKOb+CkkfK0yMK9WfOlMUqWM9FcVGIvd9V1b4u5q/pHFQC49qh9eHldPzSW2LYBvsgD0QgmNwCi7AFagDAp7BC3gD796r9+l9ecPxaMmb7GyBKZRmvgFhzrOL</latexit>KR{r}(↵6=w,t)
(c) Filtered Radon transform when α̸=w.
<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t<latexit sha1_base64="gyxXXPs6abl7KvzldfK6LO78VFQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCCB5bsLXQhrLZbtq1m03YnQgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCobeJUM95isYx1J6CGS6F4CwVK3kk0p1Eg+UMwvpn5D09cGxGre5wk3I/oUIlQMIpWamK/XHGr7hxklXg5qUCORr/81RvELI24QiapMV3PTdDPqEbBJJ+WeqnhCWVjOuRdSxWNuPGz+aFTcmaVAQljbUshmau/JzIaGTOJAtsZURyZZW8m/ud1Uwyv/UyoJEWu2GJRmEqCMZl9TQZCc4ZyYgllWthbCRtRTRnabEo2BG/55VXSvqh6l9Vas1ap3+ZxFOEETuEcPLiCOtxBA1rAgMMzvMKb8+i8OO/Ox6K14OQzx/AHzucP4/6NBA==</latexit>t(d) Sparsifying transform D2
tKR{r}.
Fig. 4. Cartoon diagram illustrating the illustrating the sparsifying transform of the ReLU neuron r(x) =ρ(wTx−b)
with(w, b)∈Sd−1×R. The heatmap is a top-down view of a ReLU neuron depicted in (a).
filtered Radon domain. The key idea is that the (filtered) Radon transform allows us to extract the
(univariate) activation function from the multivariate neuron and apply the univariate sparsifying
transform in the tvariable. Figure 4 is a cartoon diagram which depicts the the sparsifying
transform of a ReLU neuron.
The story is now analogous to the univariate case. Indeed, by the NBT, training the NN in (22)

--- PAGE 14 ---
14
with weight decay is equivalent to solving the optimization problem
min
θ∈ΘNX
n=1L(yn, fθ(xn)) +λKX
k=1|vk|∥wk∥2. (33)
From (32) we see that ∥D2
tKRfθ∥M=PK
k=1|vk|∥wk∥2, and so training the NN (22) with
weight decay prefers solutions with sparse second derivatives in the filtered Radon domain. This
measure of sparsity can be viewed as the second-order TV in the (filtered) Radon domain. Let
RBV2(Rd)denote the space of functions on Rdof second-order bounded variation in the (filtered)
Radon domain (i.e., the second-order TV in the (filtered) Radon domain is finite). A key result
regarding RBV2(Rd)is the following representer theorem for neural networks, first proven
in [30]. Under mild assumptions on the loss function, the solution set to the optimization problem
min
f∈RBV2(Rd)NX
n=1L(yn, f(xn)) +λ∥D2
tKRf∥M (34)
is completely characterized by NNs of the form
fθ(x) =KX
k=1vkρ(wT
kx−bk) +cTx+c0, (35)
where the number of neurons is strictly less than the number of data ( K < N ) in the sense that
the solution set to (34) is a closed convex set whose extreme points take the form of (35) with
K < N (see [ 5], [28], [45] for further refinements of this result). Common loss functions such as
the squared-error satisfy the mild assumptions. The skip connection cTx+c0arises because the
null space of the sparsifying transform is the space of affine functions. Therefore, by the same
argument presented in the univariate case, sufficiently wide ( K≥N) NNs (as in (35)) trained
with weight decay to global minimizers are exactly optimal functions in RBV2(Rd).
B. Deep Neural Networks
The machinery is straightforward to extend to the case of deep neural networks (DNNs). The
key idea is to consider fitting data using compositions of RBV2-functions. It is shown in [ 31],
[41] that under mild assumptions on the loss function, a solution to the optimization problem
min
f(1),...,f(L)NX
n=1L(yn, f(L)◦ ··· ◦ f(1)(xn)) +λLX
ℓ=1dℓX
i=1∥D2
tKRf(ℓ)
i∥M (36)
has the form of a DNN as in (1), where dℓare the intermediary dimensions in the function
compositions, that satisfies the following properties:
•The number of layers is L+ 1;

--- PAGE 15 ---
15
Fig. 5. A feedforward DNN architecture with linear bottlenecks. The blue nodes represent ReLU neurons, the gray
nodes represent linear neurons, and the white nodes depict the DNN inputs. Since the linear layers are narrower than
the ReLU layers, this architecture is referred to as a DNN with linear bottlenecks.
•The solution is sparse in the sense of having few active neuron connections (widths of the
layers are bounded by N2);
•The solution has skip connections in all layers;
•The architecture has linear bottlenecks which forces the weight matrices to be low rank.
Such an architecture is illustrated in Fig. 5. The result shows that ReLU DNNs with skip
connections and linear bottlenecks trained with a variant of weight decay [ 31, Remark 4.7]
are optimal solutions to fitting data using compositions of RBV2-functions. Linear bottlenecks
may be written as factorized (low-rank) weight matrices of the form W(ℓ)=U(ℓ)V(ℓ). These
bottleneck layers correspond to layers with linear activation functions ( σ(t) =t). They arise
naturally due to the compositions of functions that arise in (36). The number of neurons in each
bottleneck layers is bounded by dℓ. The incorporation of linear bottlenecks of this form in DNNs
have been shown to speed up learning [ 1] and increase the accuracy [ 15], robustness [ 38], and
computational efficiency [48] of DNNs.
V. W HAT IS THE ROLE OF NEURAL NETWORK ACTIVATION FUNCTIONS ?
The primary focus of the article so far has been the ReLU activation function ρ(t) = max {0, t}.
Many of the previously discussed ideas can be extended to a broad class of activation functions.
The property of the ReLU exploited so far has been that it is sparsified by the second derivative
operator in the sense that D2
tρ=δ. Indeed, we can define a broad class of neural function spaces
akin toRBV2(Rd)by defining spaces characterized by different sparsifying transforms matched

--- PAGE 16 ---
16
[Table 1] Common Activation Functions
Activation Function σ(t)Frequency Response of
Sparsifying Transform: bH(ω)
Rectified Linear Unit (ReLU) max{0, t} − ω2
Truncated Powermax{0, t}k
k!,k∈N(jω)k+1
Sigmoid1
1 +e−t−1
2j
πsinh(πω)
arctanarctan( t)
πjωe|ω|
Exponentiale−|t|
21 +ω2
to an activation function. This entails replacing D2
tin (31) with a generic sparsifying transform
H. Table 1 (adapted from [ 44]) provides examples of common activation functions that fall into
this framework, where each sparsifying transform His defined by its frequency response bH(ω).
For the ReLU, we have H = D2
tand so bH(ω) = (j ω)2=−ω2.
Therefore, many of the previously discussed results can thus be directly extended to a broad
class of activation functions including the classical sigmoid and arctan activation functions. We
remark that the sparsity-promoting effect of weight decay hinges on the homogeneity of the
activation function in the DNN. While the ReLU and truncated power activation functions in
Table 1 are homogeneous, the other activation functions are not. This provides evidence that
one should prefer homogeneous activation functions like the ReLU in order to exploit the tight
connections between weight decay and sparsity. Although the sparsity-promoting effect of weight
decay does not apply to the non-homogeneous activation functions, statements akin to (31) do
hold by considering neurons with input weights constrained to be unit norm. Therefore, these
sparsifying transforms uncover the innovations of finite-width NNs with unit norm input weights.
Therefore, by only considering neurons with unit norm input weights, the key results which
characterize the solution sets to the optimization problems akin to (34) and (36) hold, providing
insight into the kinds of functions favored by DNNs using these activation functions.

--- PAGE 17 ---
17
VI. W HYDONEURAL NETWORKS SEEMINGLY BREAK THE CURSE OF DIMENSIONALITY ?
In 1993, Barron published his seminal paper [ 4] on the ability of NNs with sigmoid activation
functions to approximate a wide variety of multivariate functions. Remarkably, he showed that
NNs can approximate functions which satisfy certain decay conditions on their Fourier transforms
at a rate that is completely independent of the input-dimension of the functions. This property
has led to many people heralding his work as “breaking the curse of dimensionality”. Today, the
function spaces he studied are often referred to as the spectral Barron spaces . It turns out that
this remarkable approximation property of NNs is due to sparsity.
To explain this phenomenon, we first recall a problem which “suffers the curse of dimensionality”.
A classical problem in signal processing is reconstructing a signal from its samples. Shannon’s
sampling theorem asserts that sampling a bandlimited signal on a regular grid at a rate faster
than the Nyquist rate guarantees that the sinc interpolator perfectly reconstructs the signal. Since
thesinc function and its shifts form an orthobasis for the space of bandlimited signals, the
energy of the signal (squared L2-norm) corresponds to the squared (discrete) ℓ2-norm of its
samples. Multivariate versions of the sampling theorem are similar and assert that sampling
multivariate bandlimited signal on a sufficiently fine regular grid guarantees perfect reconstruction
with (multivariate) sinc interpolation. It is easy to see that the grid size (and therefore the number
of samples) grows exponentially with with the dimension of the signal. This shows that the
sampling and reconstruction of bandlimited signals suffers the curse of dimensionality. The
fundamental reason for this is that the energy or “size” of a bandlimited signal corresponds to
theℓ2-norm of the signal’s expansion coefficients in the sinc basis.
It turns out that there is a stark difference if we instead measure the “size” of a function by the
more restrictive ℓ1-norm instead of the ℓ2-norm, an idea popularized by wavelets and compressed
sensing. Let D={ψ}ψ∈Dbe a dictionary of atoms (e.g., sinc functions, wavelets, neurons, etc.).
Consider the problem of approximating a multivariate function mapping Rd→Rthat admits
a decomposition f(x) =P∞
k=1vkψk(x), where ψk∈ D and the expansion coefficients satisfy
P∞
k=1|vk|=∥v∥ℓ1<∞. It turns out that that there exists an approximant constructed with
Kterms from the dictionary Dwhose L2-approximation error ∥f−fK∥L2decays at a rate
completely independent of the input dimension d.
We will illustrate the argument when D={ψk}∞
k=1is an orthonormal basis (e.g., multivariate
Haar wavelets). Given a function f:Rd→Rthat admits a decomposition f(x) =P∞
k=1vkψk(x)
such that ∥v∥ℓ1<∞, we can construct an approximant fKby a simple thresholding procedure

--- PAGE 18 ---
18
that keeps the Klargest coefficients of fand sets all other coefficients to 0. If we let |v(1)| ≥
|v(2)| ≥ ··· denote the coefficients of fsorted in nonincreasing magnitude, then the squared
approximation error is bounded as
∥f−fK∥2
L2=X
k>Kv(k)ψ(k)2
L2=X
k>K|v(k)|2, (37)
where the last equality follows by exploiting the orthonormality of the {ψk}∞
k=1. Finally, since
the original sequence of coefficients v= (v1, v2, . . .)is absolutely summable, |v(k)|must decay
strictly faster than 1/kfork > K (since the tail of the harmonic seriesP
k>K1
kdiverges). Putting
this together with (37), the L2-approximation error ∥f−fK∥L2must decay as K−1/2, completely
independent of the input dimension d. For a more precise treatment of this argument, we refer
the reader to Theorem 9.10 in Mallat’s Wavelet Tour of Signal Processing [22]. These kinds of
thresholding procedures, particularly with wavelet bases [ 12], revolutionized signal and image
processing and were the foundations of compressed sensing [7], [11].
By a more sophisticated argument, a similar phenomenon occurs when the orthonormal basis is
replaced with an essentially arbitrary dictionary of atoms. The result for general atoms is based
on a probabilistic technique presented by Pisier in 1981 at the Functional Analysis Seminar at
´Ecole Polytechnique, Palaiseau, France, crediting the idea to Maurey [ 34]. An implication of
Maurey’s technique is that, given a function which is an ℓ1-combination of bounded atoms from a
dictionary, there exists a K-term approximant that admits a dimension-free approximation rate that
decays as K−1/2. Motivated by discussions with Jones on his work on greedy approximation [ 18],
which provides a deterministic algorithm to find the approximant that admits the dimension-free
rate, Barron used the technique of Maurey to prove his dimension-free approximation rates with
sigmoidal NNs. This abstract approximation result is now referred to as the Maurey–Jones–Barron
lemma.
In particlar, the Maurey–Jones–Barron lemma can be applied to any function space where the
functions are ℓ1-combinations of bounded atoms. Such spaces are sometimes called variation
spaces [2], [20]. Recall from Sections IV and V that the operator H KRsparsifies neurons
of the form σ(wTx−b), where (w, b)∈Sd−1×Randσis matched to H. This implies that
the space of functions f:Rd→Rsuch that ∥H KRf∥M<∞can be viewed as a variation
space, where the dictionary corresponds to the neurons {σ(wTx−b)}(w,b)∈Sd−1×R. Therefore,
given f:Rd→Rsuch that ∥H KRf∥M<∞, there exists a K-term approximant fKthat
takes the form of a shallow NN with Kneurons such that the L2-approximation error decays

--- PAGE 19 ---
19
asK−1/2. These techniques have been studied and extended in great detail [ 42] and have been
extended to the setting of deep NNs [ 13] by considering compositional function spaces akin to
the compositional space introduced in Section IV-B.
Combining these dimension-free approximation rates with the sparsity-promoting effect of
weight decay regularization for ReLU NNs has a striking effect on the learning problem. Suppose
that we train a shallow ReLU NN with weight decay on data generated from the noisy samples
yn=f(xn)+εn,n= 1, . . . , N , off∈RBV2(Rd), where xnare i.i.d. uniform random variables
on some bounded domain Ω⊂Rdandεnare i.i.d. Gaussian random variables. Let fNdenote
this trained NN. Then, it has been shown [ 32] that the mean integrated squared error (MISE)
E∥f−fN∥2
L2(Ω)decays at a rate of N−1/2, independent of the input dimension d. Moreover, this
result also shows that the generalization error of the trained NN on a new example xgenerated
uniformly at random on Ωis also immune to the curse of dimensionality. Furthermore, these
ideas have been studied in the context of deep NNs [ 40], proving dimension-free MISE rates for
estimating H ¨older functions (that exhibit low-dimensional structure) with ReLU DNNs.
A. Mixed Variation and Low-Dimensional Structure
The national meeting of the American Mathematical Society in 2000 was held to discuss the
mathematical challenges of the 21st Century. Here, Donoho gave a lecture titled High-Dimensional
Data Analysis: The Curses and Blessings of Dimensionality [10]. In this lecture, he coined the
term “mixed variation” to refer to the kinds of functions that lie in variation spaces, citing the
spectral Barron spaces as an example. The variation spaces are different from classical multivariate
function spaces in that they favor functions that have weak variation in multiple directions (very
smooth functions) as well as functions that have very strong variation in one or a few directions
(very rough functions). These spaces also disfavor functions with strong variation in multiple
directions. It is this fact that makes them quite “small” compared to classical multivariate function
spaces, giving rise to their dimension free approximation and MISE rates. Examples of functions
with different kinds of variation are illustrated in Fig. 6. The prototypical examples of functions
that lie in mixed variation spaces can be thought of as superpositions of few neurons with different
directions or superpositions of many neurons (even continuously many) in only a few directions.
In order to interpret the idea of mixed variation in the context of modern data analysis and DL,
we turn our attention to Fig. 6(b). In this figure, the function has strong variation, but only in a
single direction. In other words, this function has low-dimensional structure . It has been observed
by a number of authors that DNNs are able to automatically adapt to the low-dimensional structure

--- PAGE 20 ---
20
(a) Weak variation in multiple directions.
 (b) Strong variation in one direction.
(c) Strong variation in multiple directions.
Fig. 6. Examples of functions exhibiting different kinds of variation.
that often arises in natural data. This is possible because the input weights can be trained to adjust
orientation of each neuron. The dimension-independent approximation rate quantifies the power
of this tunability. This explains why DNNs are good at learning functions with low-dimensional
structure. In particular, the function in Fig. 6(c) has strong variation in all directions, so no method
can overcome the curse of dimensionality in this sort of situation. On the other hand, in Fig. 6(a)
the function has weak varation in all directions and Fig. 6(b) has strong variation only in one
direction, so these are functions for which neural networks will overcome the curse. For Fig. 6(b)
the the sparsity-promoting effect of weight decay promotes DNN solutions with neurons oriented
in the direction of variation (i.e., it automatically learns the low-dimensional structure).
VII. T AKEAWAY MESSAGES AND FUTURE RESEARCH DIRECTIONS
In this article we presented a mathematical framework to understand DNNs from first principles,
through the lens of sparsity and sparse regularization. Using familiar mathematical tools from

--- PAGE 21 ---
21
signal processing, we provided an explanation for the sparsity-promoting effect of the common
regularization scheme of weight decay in neural network training, the use of skip connections
and low-rank weight matrices in network architectures, and why neural networks seemingly break
the curse of dimensionality. This framework provides the mathematical setting for many future
research directions.
The framework suggests the possibility of new neural training algorithms. The equivalence
of solutions using weight decay regularization and the regularization in (6) leads to the use of
proximal gradient methods akin to iterative soft-thresholding algorithms to train DNNs. This
avenue has already begun to be explored. The preliminary results in [ 49] have shown that proximal
gradient training algorithms for DNNs perform as good as and often better (particularly when
labels are corrupted) than standard gradient-based training with weight decay, while simultaneously
producing sparser networks.
There has also been a large body of work dating back to 1989 [ 21] on NN pruning has shown
empirically that large NNs can be compressed or sparsified to a fraction of their size while still
maintaining their predictive performance. The connection between weight decay and sparsity-
promoting regularizers like in (6) suggests new approaches to pruning. For example, one could
apply proximal gradient algorithms to derive sparse approximations to large pre-trained neural
networks [ 41]. There are many open questions in this direction, both experimental and theoretical,
including applying these algorithms to other DNN architectures and deriving convergence results
for these algorithms.
The framework in this paper also shows that trained ReLU DNNs are compositions of RBV2-
functions. As we have seen in this article, at this point in time, we have a clear and nearly
complete understanding of RBV2(Rd). In particular, the RBV2-space favors functions that are
smooth in most or all directions, which explains why neural networks seemingly break the curse of
dimensionality. Less is clear and understood about the compositions of RBV2-functions (which
characterize DNNs). Better understanding of compositional function spaces could provide new
insights into the benefits of depth in neural networks. This in turn could lead to new guidelines
for designing NN architectures and training algorithms.
ACKNOWLEDGMENT
The authors would like to thank Rich Baraniuk, Misha Belkin, C ¸a˘gatay Candan, Ron DeV ore,
Kangwook Lee, Greg Ongie, Dimitris Papailiopoulos, Tomaso Poggio, Lorenzo Rosasco, Joe

--- PAGE 22 ---
22
Shenouda, Jonathan Siegel, Ryan Tibshirani, Michael Unser, Becca Willett, Stephen Wright, Liu
Yang, and Jifan Zhang for many insightful discussions on the topics presented in this article.
RP was supported by in part by the U.S. National Science Foundation (NSF) Graduate Research
Fellowship Program under grant DGE-1747503 and the European Research Council (ERC Project
FunLearn) under Grant 101020573. RN was supported in part by the NSF grants DMS-2134140
and DMS-2023239, the ONR MURI grant N00014-20-1-2787, and the AFOSR/AFRL grant
FA9550-18-1-0166, as well as the Keith and Jane Nosbusch Professorship.
REFERENCES
[1]L. J. Ba and R. Caruana, “Do deep nets really need to be deep?” in Proceedings of the 27th International
Conference on Neural Information Processing Systems-Volume 2 , 2014, pp. 2654–2662.
[2]F. Bach, “Breaking the curse of dimensionality with convex neural networks,” The Journal of Machine Learning
Research , vol. 18, no. 1, pp. 629–681, 2017.
[3]R. Balestriero and R. G. Baraniuk, “Mad max: Affine spline insights into deep learning,” Proceedings of the
IEEE , vol. 109, no. 5, pp. 704–727, 2020.
[4] A. R. Barron, “Universal approximation bounds for superpositions of a sigmoidal function,” IEEE Transactions
on Information theory , vol. 39, no. 3, pp. 930–945, 1993.
[5]F. Bartolucci, E. De Vito, L. Rosasco, and S. Vigogna, “Understanding neural networks with reproducing kernel
Banach spaces,” Appl. Comput. Harmon. Anal. , vol. 62, pp. 194–236, 2023.
[6] E. J. Cand `es, “Ridgelets: Theory and applications,” Ph.D. dissertation, Stanford University, 1998.
[7]E. J. Cand `es, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction from highly
incomplete frequency information,” IEEE Transactions on Information Theory , vol. 52, no. 2, pp. 489–509, 2006.
[8]L. Chizat and F. Bach, “Implicit bias of gradient descent for wide two-layer neural networks trained with the
logistic loss,” in Conference on Learning Theory . PMLR, 2020, pp. 1305–1338.
[9]T. Debarre, Q. Denoyelle, M. Unser, and J. Fageot, “Sparsest piecewise-linear regression of one-dimensional
data,” Journal of Computational and Applied Mathematics , vol. 406, p. 114044, 2022.
[10] D. L. Donoho, “High-dimensional data analysis: The curses and blessings of dimensionality,” AMS Lectures ,
p. 32, 2000.
[11] D. L. Donoho, “Compressed sensing,” IEEE Transactions on Information Theory , vol. 52, no. 4, pp. 1289–1306,
2006.
[12] D. L. Donoho and I. M. Johnstone, “Minimax estimation via wavelet shrinkage,” The Annals of Statistics , vol. 26,
no. 3, pp. 879–921, 1998.
[13] W. E and S. Wojtowytsch, “On the Banach spaces associated with multi-layer ReLU networks: Function
representation, approximation theory and gradient descent dynamics,” CSIAM Transactions on Applied Mathematics ,
vol. 1, no. 3, pp. 387–440, 2020.
[14] S. D. Fisher and J. W. Jerome, “Spline solutions to L1extremal problems in one and several variables,” Journal
of Approximation Theory , vol. 13, no. 1, pp. 73–83, 1975.
[15] A. Golubeva, B. Neyshabur, and G. Gur-Ari, “Are wider nets better given the same number of parameters?”
International Conference on Learning Representations , 2021.

--- PAGE 23 ---
23
[16] Y . Grandvalet, “Least absolute shrinkage is equivalent to quadratic penalization,” in International Conference on
Artificial Neural Networks . Springer, 1998, pp. 201–206.
[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition , 2016, pp. 770–778.
[18] L. K. Jones, “A simple lemma on greedy approximation in Hilbert space and convergence rates for projection
pursuit regression and neural network training,” The Annals of Statistics , pp. 608–613, 1992.
[19] V . K ˚urkov ´a, P. C. Kainen, and V . Kreinovich, “Estimates of the number of hidden units and variation with respect
to half-spaces,” Neural Networks , vol. 10, no. 6, pp. 1061–1068, 1997.
[20] V . K ˚urkov ´a and M. Sanguineti, “Bounds on rates of variable-basis and neural-network approximation,” IEEE
Transactions on Information Theory , vol. 47, no. 6, pp. 2659–2665, 2001.
[21] Y . LeCun, J. Denker, and S. Solla, “Optimal brain damage,” Advances in neural information processing systems ,
vol. 2, 1989.
[22] S. Mallat, A Wavelet Tour of Signal Processing , 3rd ed. Elsevier/Academic Press, Amsterdam, 2009.
[23] E. Mammen and S. van de Geer, “Locally adaptive regression splines,” The Annals of Statistics , vol. 25, no. 1,
pp. 387–413, 1997.
[24] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in nervous activity,” The Bulletin of
Mathematical Biophysics , vol. 5, no. 4, pp. 115–133, 1943.
[25] B. Neyshabur, R. Tomioka, and N. Srebro, “In search of the real inductive bias: On the role of implicit regularization
in deep learning.” in International Conference on Learning Representations (Workshop) , 2015.
[26] G. Ongie, R. Willett, D. Soudry, and N. Srebro, “A function space view of bounded norm infinite width ReLU
nets: The multivariate case,” in International Conference on Learning Representations , 2020.
[27] A. V . Oppenheim, A. S. Willsky, and S. H. Nawab, Signals & Systems , ser. Prentice-Hall Signal Processing Series.
Prentice Hall, 1997.
[28] R. Parhi, “On Ridge Splines, Neural Networks, and Variational Problems in Radon-Domain BV Spaces,” Ph.D.
dissertation, The University of Wisconsin–Madison, 2022.
[29] R. Parhi and R. D. Nowak, “The role of neural network activation functions,” IEEE Signal Processing Letters ,
vol. 27, pp. 1779–1783, 2020.
[30] R. Parhi and R. D. Nowak, “Banach space representer theorems for neural networks and ridge splines.” Journal
of Machine Learning Research , vol. 22, no. 43, pp. 1–40, 2021.
[31] R. Parhi and R. D. Nowak, “What kinds of functions do deep neural networks learn? Insights from variational
spline theory,” SIAM Journal on Mathematics of Data Science , vol. 4, no. 2, pp. 464–489, 2022.
[32] R. Parhi and R. D. Nowak, “Near-minimax optimal estimation with shallow ReLU neural networks,” IEEE
Transactions on Information Theory , vol. 69, no. 2, pp. 1125–1140, 2023.
[33] M. Pilanci and T. Ergen, “Neural networks are convex regularizers: Exact polynomial-time convex optimization
formulations for two-layer networks,” in International Conference on Machine Learning . PMLR, 2020, pp.
7695–7705.
[34] G. Pisier, “Remarques sur un r ´esultat non publi ´e de B. Maurey,” S´eminaire d’Analyse Fonctionnelle (dit ”Maurey-
Schwartz”) , pp. 1–12, April 1981.
[35] T. Poggio, A. Banburski, and Q. Liao, “Theoretical issues in deep networks,” Proceedings of the National Academy
of Sciences , vol. 117, no. 48, pp. 30 039–30 045, 2020.

--- PAGE 24 ---
24
[36] J. Radon, “ ¨Uber die bestimmung von funktionen durch ihre integralwerte l ¨angs gewisser mannigfaltigkeiten,” Ber.
Verh, Sachs Akad Wiss. , vol. 69, pp. 262–277, 1917.
[37] L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based noise removal algorithms,” Physica D:
nonlinear phenomena , vol. 60, no. 1-4, pp. 259–268, 1992.
[38] A. Sanyal, P. H. Torr, and P. K. Dokania, “Stable rank normalization for improved generalization in neural
networks and GANs,” International Conference on Learning Representations , 2019.
[39] P. Savarese, I. Evron, D. Soudry, and N. Srebro, “How do infinite width bounded norm networks look in function
space?” in Conference on Learning Theory . PMLR, 2019, pp. 2667–2690.
[40] J. Schmidt-Hieber, “Nonparametric regression using deep neural networks with ReLU activation function,” The
Annals of Statistics , vol. 48, no. 4, pp. 1875–1897, 2020.
[41] J. Shenouda, R. Parhi, K. Lee, and R. D. Nowak, “Vector-valued variation spaces and width bounds for DNNs:
Insights on weight decay regularization,” arXiv preprint arXiv:2305.16534 , 2023.
[42] J. W. Siegel and J. Xu, “Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural
networks,” Foundations of Computational Mathematics , pp. 1–57, 2022.
[43] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society: Series
B (Methodological) , vol. 58, no. 1, pp. 267–288, 1996.
[44] M. Unser, “From kernel methods to neural networks: A unifying variational formulation,” arXiv preprint
arXiv:2206.14625 , 2022.
[45] M. Unser, “Ridges, neural networks, and the Radon transform,” Journal of Machine Learning Research , vol. 24,
no. 37, pp. 1–33, 2023.
[46] M. Unser, J. Fageot, and J. P. Ward, “Splines are universal solutions of linear inverse problems with generalized
TV regularization,” SIAM Review , vol. 59, no. 4, pp. 769–793, 2017.
[47] M. Vetterli, P. Marziliano, and T. Blu, “Sampling signals with finite rate of innovation,” IEEE Transactions on
Signal Processing , vol. 50, no. 6, pp. 1417–1428, 2002.
[48] H. Wang, S. Agarwal, and D. Papailiopoulos, “Pufferfish: Communication-efficient models at no extra cost,”
Proceedings of Machine Learning and Systems , vol. 3, pp. 365–386, 2021.
[49] L. Yang, J. Zhang, J. Shenouda, D. Papailiopoulos, K. Lee, and R. D. Nowak, “A better way to decay: Proximal
gradient training algorithms for neural nets,” in OPT 2022: Optimization for Machine Learning (NeurIPS Workshop) ,
2022.
[50] M. Yuan and Y . Lin, “Model selection and estimation in regression with grouped variables,” Journal of the Royal
Statistical Society: Series B (Statistical Methodology) , vol. 68, no. 1, pp. 49–67, 2006.
Rahul Parhi (rahul.parhi@epfl.ch) received the B.S. degree in mathematics and the B.S. degree in computer science
from the University of Minnesota–Twin Cities in 2018 and the M.S. and Ph.D. degrees in electrical engineering from
the University of Wisconsin–Madison in 2019 and 2022, respectively. During his Ph.D., he was supported by an NSF
Graduate Research Fellowship. He is currently a Post-Doctoral Researcher with the Biomedical Imaging Group at
the´Ecole Polytechnique F ´ed´erale de Lausanne. He is primarily interested in applications of functional and harmonic
analysis to problems in signal processing and data science. He is a member of the IEEE.

--- PAGE 25 ---
25
Robert D. Nowak (rdnowak@wisc.edu) holds a Ph.D. in electrical engineering from the University of Wisconsin–
Madison and is currently the Grace Wahba Professor of Data Science and Keith and Jane Morgan Nosbusch Professor
in Electrical and Computer Engineering at the same institution. With research focusing on signal processing, machine
learning, optimization, and statistics, Nowak’s work on sparse signal recovery and compressed sensing has won several
awards. He currently serves as a Section Editor for the SIAM Journal on Mathematics of Data Science and a Senior
Editor for the IEEE Journal on Selected Areas in Information Theory, and is an IEEE Fellow.
