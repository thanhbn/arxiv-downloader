# Glot500:
# Mở rộng quy mô Corpus đa ngôn ngữ và Mô hình ngôn ngữ lên 500 ngôn ngữ

Ayyoob Imani∗1,2, Peiqin Lin∗1,2, Amir Hossein Kargaran1,2, Silvia Severini1, Masoud Jalili Sabet1, Nora Kassner1,2, Chunlan Ma1,2, Helmut Schmid1, André F. T. Martins3,4,5, François Yvon6 và Hinrich Schütze1,2

1CIS, LMU Munich, Germany
2Munich Center for Machine Learning (MCML), Germany
3Instituto Superior Técnico (Lisbon ELLIS Unit)
4Instituto de Telecomunicações
5Unbabel
6Sorbonne Université, CNRS, ISIR, France

{ayyoob, linpq, amir, silvia}@cis.lmu.de

## Tóm tắt

Cộng đồng NLP chủ yếu tập trung vào việc mở rộng quy mô các Mô hình ngôn ngữ lớn (LLM) theo chiều dọc, tức là làm cho chúng tốt hơn cho khoảng 100 ngôn ngữ. Thay vào đó, chúng tôi mở rộng quy mô LLM theo chiều ngang: chúng tôi tạo ra, thông qua việc tiếp tục huấn luyện trước, Glot500-m, một LLM bao phủ 511 ngôn ngữ chủ yếu là ít tài nguyên. Một phần quan trọng của nỗ lực này là thu thập và làm sạch Glot500-c, một corpus bao phủ 511 ngôn ngữ này và cho phép chúng tôi huấn luyện Glot500-m. Chúng tôi đánh giá Glot500-m trên năm nhiệm vụ đa dạng trên các ngôn ngữ này. Chúng tôi quan sát thấy những cải thiện lớn cho cả ngôn ngữ nhiều tài nguyên và ít tài nguyên so với baseline XLM-R. Phân tích của chúng tôi cho thấy không có yếu tố đơn lẻ nào giải thích chất lượng của các biểu diễn LLM đa ngôn ngữ. Thay vào đó, sự kết hợp của các yếu tố quyết định chất lượng bao gồm kích thước corpus, chữ viết, "sự trợ giúp" từ các ngôn ngữ liên quan và tổng dung lượng của mô hình. Công trình của chúng tôi đáp ứng một mục tiêu quan trọng của nghiên cứu NLP: chúng ta không nên giới hạn NLP ở một phần nhỏ các ngôn ngữ trên thế giới mà thay vào đó phấn đấu hỗ trợ càng nhiều ngôn ngữ càng tốt để mang lại lợi ích của công nghệ NLP cho tất cả các ngôn ngữ và văn hóa. Mã nguồn, dữ liệu và mô hình có sẵn tại https://github.com/cisnlp/Glot500.

## 1 Giới thiệu

Cộng đồng NLP chủ yếu tập trung vào việc mở rộng quy mô các Mô hình ngôn ngữ lớn (LLM) theo chiều dọc, tức là làm sâu sắc hơn sự hiểu biết của chúng về các ngôn ngữ nhiều tài nguyên bằng cách mở rộng quy mô tham số và dữ liệu huấn luyện. Mặc dù cách tiếp cận này đã cách mạng hóa NLP, những thành tựu này phần lớn chỉ giới hạn ở các ngôn ngữ nhiều tài nguyên. Các ví dụ về LLM "theo chiều dọc" là GPT3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) và Bloom (BigScience et al., 2022). Trong bài báo này, chúng tôi tạo ra Glot500-m, một mô hình thay vào đó tập trung vào việc mở rộng quy mô LLM đa ngôn ngữ theo chiều ngang, tức là mở rộng quy mô lên một số lượng lớn các ngôn ngữ mà đại đa số là ít tài nguyên. Vì LLM là thiết yếu cho tiến bộ trong NLP, việc thiếu LLM hỗ trợ các ngôn ngữ ít tài nguyên là một trở ngại nghiêm trọng trong việc đưa NLP đến tất cả các ngôn ngữ và văn hóa trên thế giới. Mục tiêu của chúng tôi là giải quyết nhu cầu này với việc tạo ra Glot500-m.

Các LLM đa ngôn ngữ hiện tại chỉ hỗ trợ khoảng 100 (Conneau et al., 2020) trong số 7000 ngôn ngữ trên thế giới. Những ngôn ngữ được hỗ trợ này là những ngôn ngữ có lượng lớn dữ liệu huấn luyện sẵn có thông qua các dự án như Oscar (Suárez et al., 2019) và các bản dump Wikipedia. Theo Siddhant et al. (2022), chúng tôi gọi 100 ngôn ngữ được XLM-R (Conneau et al., 2020) bao phủ là các ngôn ngữ đầu và những ngôn ngữ còn lại là các ngôn ngữ đuôi. Thuật ngữ này có động lực từ phân phối lệch của dữ liệu có sẵn mỗi ngôn ngữ: đối với các ngôn ngữ được tài nguyên hóa tốt nhất, có những corpus khổng lồ sẵn có, nhưng đối với đuôi dài các ngôn ngữ, chỉ tồn tại các corpus nhỏ. Đây là một vấn đề chính mà chúng tôi giải quyết: tính khả dụng của dữ liệu cho các ngôn ngữ đuôi bị hạn chế so với các ngôn ngữ đầu. Kết quả là, các ngôn ngữ đuôi thường bị bỏ qua bởi các công nghệ ngôn ngữ (Joshi et al., 2020).

Mặc dù đã có một số công trình về dịch máy cho một số lượng lớn các ngôn ngữ đuôi (Costa-jussà et al., 2022; Bapna et al., 2022), các LLM hiện tại cho các ngôn ngữ đuôi chỉ giới hạn ở một số lượng tương đối nhỏ các ngôn ngữ (Wang et al., 2019; Alabi et al., 2022; Wang et al., 2022). Trong bài báo này, chúng tôi giải quyết khoảng trống này. Công trình của chúng tôi có ba phần.

(i) Thu thập corpus. Chúng tôi thu thập Glot2000-c, một corpus bao phủ hàng nghìn ngôn ngữ đuôi. (ii) Huấn luyện mô hình. Sử dụng Glot500-c, một tập con của Glot2000-c, chúng tôi huấn luyện Glot500-m, một LLM bao phủ 511 ngôn ngữ. (iii) Xác thực. Chúng tôi thực hiện đánh giá mở rộng về chất lượng của các biểu diễn Glot500-m của các ngôn ngữ đuôi trên một bộ nhiệm vụ đa dạng.

Chi tiết hơn, thu thập corpus xem xét ba nguồn chính: các trang web được biết là xuất bản nội dung bằng các ngôn ngữ cụ thể, các corpus với nội dung đa ngôn ngữ được phân loại và các tập dữ liệu được xuất bản bằng các ngôn ngữ đuôi cụ thể. Tập dữ liệu kết quả Glot2000-c bao gồm 700GB trong 2266 ngôn ngữ được thu thập từ ≈150 nguồn. Sau khi làm sạch và loại bỏ trùng lặp, chúng tôi tạo ra tập con Glot500-c, bao gồm 511 ngôn ngữ và 534 ngôn ngữ-chữ viết (trong đó chúng tôi định nghĩa một ngôn ngữ-chữ viết là sự kết hợp của ISO639-3 và chữ viết) để huấn luyện Glot500-m. Tiêu chí của chúng tôi để bao gồm một ngôn ngữ-chữ viết trong Glot500-c là nó bao gồm hơn 30.000 câu.

Huấn luyện mô hình. Để huấn luyện Glot500-m, chúng tôi sử dụng mở rộng từ vựng và tiếp tục huấn luyện trước. Từ vựng của XLM-R được mở rộng với các token mới được huấn luyện trên Glot500-c. Sau đó chúng tôi thực hiện tiếp tục huấn luyện trước XLM-R với mục tiêu MLM (Devlin et al., 2019).

Xác thực. Chúng tôi đánh giá toàn diện Glot500-m trên một bộ nhiệm vụ hiểu ngôn ngữ tự nhiên, gán nhãn chuỗi và đa ngôn ngữ đa dạng cho hàng trăm ngôn ngữ. Kết quả cho thấy Glot500-m hoạt động tốt hơn XLM-R-B (XLM-R-base) cho các ngôn ngữ đuôi với biên độ lớn trong khi hoạt động tương đương (hoặc tốt hơn) cho các ngôn ngữ đầu.

Nghiên cứu trước đây về tính đa ngôn ngữ đã bị cản trở bởi việc thiếu LLM hỗ trợ một số lượng lớn ngôn ngữ. Hạn chế này đã dẫn đến các nghiên cứu được thực hiện trong các thiết lập khác biệt với các tình huống thế giới thực. Ví dụ, Dufter và Schütze (2020) sử dụng dữ liệu ngôn ngữ tổng hợp. Và lời nguyền của tính đa ngôn ngữ chủ yếu được nghiên cứu cho một tập hợp các ngôn ngữ nhiều tài nguyên (Conneau et al., 2020). Bằng cách tạo ra Glot500-m, chúng tôi có thể điều tra các vấn đề này trong một thiết lập thực tế hơn. Chúng tôi cung cấp mã nguồn, dữ liệu và các mô hình đã huấn luyện để thúc đẩy nghiên cứu của cộng đồng về cách bao gồm hàng trăm ngôn ngữ hiện đang được phục vụ kém bởi công nghệ NLP.

Đóng góp. (i) Chúng tôi huấn luyện mô hình đa ngôn ngữ Glot500-m trên corpus 600GB, bao phủ hơn 500 ngôn ngữ đa dạng, và công khai nó tại https://github.com/cisnlp/Glot500. (ii) Chúng tôi thu thập và làm sạch Glot500-c, một corpus bao phủ các ngôn ngữ đa dạng này và cho phép chúng tôi huấn luyện Glot500-m, và sẽ công khai càng nhiều càng tốt. (iii) Chúng tôi đánh giá Glot500-m trên pseudoperplexity và năm nhiệm vụ đa dạng trên các ngôn ngữ này. Chúng tôi quan sát thấy những cải thiện lớn cho các ngôn ngữ ít tài nguyên so với baseline XLM-R. (iv) Phân tích mở rộng của chúng tôi cho thấy không có yếu tố đơn lẻ nào giải thích chất lượng của các biểu diễn LLM đa ngôn ngữ. Thay vào đó, sự kết hợp của các yếu tố quyết định chất lượng bao gồm kích thước corpus, chữ viết, "sự trợ giúp" từ các ngôn ngữ liên quan và tổng dung lượng của mô hình. (v) Công trình của chúng tôi đáp ứng một mục tiêu quan trọng của nghiên cứu NLP: chúng ta không nên giới hạn NLP ở một số lượng tương đối nhỏ các ngôn ngữ nhiều tài nguyên mà thay vào đó phấn đấu hỗ trợ càng nhiều ngôn ngữ càng tốt để mang lại lợi ích của NLP cho tất cả các ngôn ngữ và văn hóa.

## 2 Công trình liên quan

Huấn luyện các LLM đa ngôn ngữ sử dụng mục tiêu mô hình hóa ngôn ngữ có mặt nạ (MLM) có hiệu quả để đạt được các biểu diễn liên ngôn ngữ (Devlin et al., 2019; Conneau et al., 2020). Những mô hình này có thể được cải thiện thêm bằng cách kết hợp các kỹ thuật như huấn luyện trước phân biệt (Chi et al., 2022) và việc sử dụng dữ liệu song song (Yang et al., 2020; Chi et al., 2021). Tuy nhiên, điều này chủ yếu mang lại lợi ích cho một tập hợp hạn chế các ngôn ngữ có corpus lớn.

Nghiên cứu gần đây đã cố gắng mở rộng các LLM hiện tại sang các ngôn ngữ có tài nguyên hạn chế. Wang et al. (2019) đề xuất mở rộng từ vựng; Ebrahimi và Kann (2021) điều tra các phương pháp thích ứng, bao gồm các mục tiêu MLM và Mô hình ngôn ngữ dịch thuật (TLM) và các adapter; Alabi et al. (2022) thích ứng XLM-R cho 17 ngôn ngữ châu Phi; Wang et al. (2022) mở rộng các mô hình ngôn ngữ sang các ngôn ngữ ít tài nguyên sử dụng từ điển song ngữ.

Thay vào đó, tinh chỉnh hiệu quả tham số thích ứng các mô hình được huấn luyện trước sang các ngôn ngữ mới bằng cách huấn luyện một tập nhỏ các trọng số một cách hiệu quả (Zhao et al., 2020; Pfeiffer et al., 2021; Ansell et al., 2022). Pfeiffer et al. (2022) giải quyết "lời nguyền của tính đa ngôn ngữ" bằng cách chia sẻ một phần của mô hình giữa tất cả các ngôn ngữ và có các mô-đun riêng biệt cho từng ngôn ngữ. Chúng tôi cho thấy rằng nhận thức phổ biến rằng tính đa ngôn ngữ tăng lên khi chúng ta thêm nhiều ngôn ngữ hơn, cho đến khi, từ một điểm nào đó, nó bắt đầu giảm, là ngây thơ. Lượng dữ liệu có sẵn mỗi ngôn ngữ và sự tương tự giữa các ngôn ngữ cũng đóng vai trò quan trọng (§6.8).

Một cách tiếp cận khác huấn luyện LLM từ đầu cho một số lượng hạn chế các ngôn ngữ đuôi; ví dụ, AfriBERTa (Ogueji et al., 2021a) và IndicNLP Suite (Kakwani et al., 2020) là các LLM cho 11 ngôn ngữ châu Phi và 11 ngôn ngữ Ấn Độ. Trong công trình đồng thời, Adebara et al. (2022) huấn luyện một mô hình đa ngôn ngữ cho 517 ngôn ngữ châu Phi trên corpus 42 GB, nhưng không công khai mô hình và với đánh giá trên số lượng ngôn ngữ ít hơn so với chúng tôi.

Liên quan chặt chẽ đến công trình của chúng tôi về tạo corpus, Bapna et al. (2022) và Costa-jussà et al. (2022) cũng tạo ra các tài nguyên NLP cho một số lượng lớn các ngôn ngữ đuôi. Họ huấn luyện một mô hình nhận dạng ngôn ngữ và trích xuất dữ liệu văn bản cho các ngôn ngữ đuôi từ các lần thu thập web quy mô lớn. Cách tiếp cận này hiệu quả, nhưng nó đòi hỏi tài nguyên tính toán đáng kể và người bản ngữ cho tất cả các ngôn ngữ đuôi. Điều này khó thực hiện ngoài các tập đoàn lớn. Bapna et al. (2022) không công khai dữ liệu của họ. Costa-jussà et al. (2022) chỉ phát hành một phần dữ liệu của họ trong khoảng 200 ngôn ngữ.

Một lợi ích chính của các LLM đa ngôn ngữ được mở rộng quy mô "theo chiều ngang" là chuyển giao từ các ngôn ngữ nhiều tài nguyên sang ít tài nguyên. Đánh giá của chúng tôi cho thấy Glot500-m xuất sắc trong việc này, nhưng đây không phải là trọng tâm chính của bài báo. Có một lượng lớn công trình về chuyển giao liên ngôn ngữ: (Artetxe và Schwenk, 2019; Imani-Googhari et al., 2022; Lauscher et al., 2020; Conneau et al., 2020; Turc et al., 2021; Fan et al., 2021; Severini et al., 2022; Choenni và Shutova, 2022; Wang et al., 2023), trong số các nghiên cứu khác.

## 3 Glot2000-c

### 3.1 Thu thập dữ liệu

Một trong những thách thức chính trong việc phát triển công nghệ NLP cho các ngôn ngữ đuôi là sự khan hiếm dữ liệu huấn luyện chất lượng cao. Trong công trình này, chúng tôi đề xuất một phương pháp nhẹ dễ dàng tái tạo cho các phòng thí nghiệm học thuật. Chúng tôi xác định dữ liệu ngôn ngữ đuôi đã được xuất bản trước đó bởi các nhà nghiên cứu, nhà xuất bản và dịch giả và sau đó thu thập hoặc tải xuống chúng. Bằng cách thu thập một vài trang web và biên soạn dữ liệu từ khoảng 150 tập dữ liệu khác nhau, chúng tôi tích lũy hơn 700GB văn bản trong 2266 ngôn ngữ. Chúng tôi sẽ gọi những nguồn dữ liệu này là các nguồn dữ liệu. Dữ liệu của chúng tôi bao phủ nhiều lĩnh vực, bao gồm văn bản tôn giáo, bài báo tin tức và các bài báo khoa học. Một số nguồn dữ liệu có chất lượng cao, được xác minh bởi người bản ngữ, dịch giả và nhà ngôn ngữ học. Những nguồn khác kém đáng tin cậy hơn như thu thập web và dump Wikipedia. Do đó cần thiết phải làm sạch dữ liệu. Để có danh sách các nguồn dữ liệu, xem §C.

### 3.2 Ngôn ngữ-Chữ viết

Một số ngôn ngữ được viết bằng nhiều chữ viết; ví dụ, tiếng Tajik được viết bằng cả chữ Cyrillic và chữ Ả Rập. Một số nguồn dữ liệu chỉ ra chữ viết, nhưng những nguồn khác hoặc không chỉ ra hoặc cung cấp văn bản hỗn hợp trong nhiều chữ viết. Chúng tôi phát hiện chữ viết cho mỗi câu và coi mỗi ngôn ngữ-chữ viết như một thực thể riêng biệt.

### 3.3 Mô hình ngôn ngữ N-gram và Phân kỳ ngôn ngữ

Chúng tôi huấn luyện một mô hình ngôn ngữ cấp ký tự 3-gram Mi cho mỗi ngôn ngữ-chữ viết Li, sử dụng KenLM (Heafield, 2011). Chúng tôi gọi perplexity được tính cho corpus của ngôn ngữ Li sử dụng mô hình ngôn ngữ Mj là PP(Mj,Li). Tương tự như Gamallo et al. (2017), chúng tôi định nghĩa một thước đo phân kỳ dựa trên perplexity của các ngôn ngữ Li và Lj là:

DLi,Lj = max{PP(Mj,Li), PP(Mi,Lj)}

Chúng tôi sử dụng D để lọc ra dữ liệu nhiễu trong §3.4 và nghiên cứu hiệu ứng của các ngôn ngữ tương tự trong huấn luyện LLM trong §6.7 và §6.8. Để biết thêm chi tiết, xem §A.

### 3.4 Làm sạch dữ liệu

Để loại bỏ nhiễu, chúng tôi sử dụng các bộ lọc cấp chunk và cấp corpus.

Trong khi một số nguồn được tách câu, những nguồn khác cung cấp nhiều câu (ví dụ, một đoạn văn) như một chunk. Các bộ lọc cấp chunk xử lý mỗi chunk văn bản từ nguồn dữ liệu như một đơn vị, mà không tách câu. Một số bộ lọc cấp chunk dựa trên khái niệm từ: chúng tôi sử dụng tokenization khoảng trắng khi có thể và nếu không thì dùng SentencePiece (Kudo và Richardson, 2018) được huấn luyện bởi Costa-jussà et al. (2022).

Là các bộ lọc cấp chunk, chúng tôi sử dụng các bộ lọc cấp câu SF1–SF5 từ BigScience ROOTS (Laurençon et al., 2022).

SF1 Lặp lại ký tự. Nếu tỷ lệ ký tự lặp lại quá cao, có thể câu không có đủ nội dung văn bản.

SF2 Lặp lại từ. Tỷ lệ cao từ lặp lại cho thấy nội dung lặp lại không hữu ích.

SF3 Ký tự đặc biệt. Các câu với tỷ lệ cao ký tự đặc biệt có thể là các artifact thu thập hoặc mã máy tính.

SF4 Số lượng từ không đủ. Vì huấn luyện mô hình ngôn ngữ đòi hỏi đủ ngữ cảnh, các chunk văn bản rất nhỏ không hữu ích.

SF5 Loại bỏ trùng lặp. Nếu hai câu giống hệt nhau sau khi loại bỏ dấu câu và khoảng trắng, một câu được loại bỏ.

Trong phần còn lại của bài báo, chúng tôi gọi một chunk là một câu'. Một câu' có thể bao gồm một đoạn ngắn, một câu hoàn chỉnh hoặc một chunk (tức là, nhiều câu).

Các bộ lọc cấp corpus phát hiện nếu corpus của một ngôn ngữ-chữ viết có nhiễu; ví dụ, corpus bằng ngôn ngữ khác hoặc bao gồm nội dung không có ý nghĩa như dữ liệu dạng bảng. Chúng tôi sử dụng các bộ lọc CF1 và CF2.

CF1 Trong trường hợp không khớp giữa ngôn ngữ và chữ viết, corpus được loại bỏ; ví dụ, tiếng Trung viết bằng chữ Ả Rập không có khả năng là tiếng Trung.

CF2 Không khớp perplexity. Đối với mỗi ngôn ngữ-chữ viết L1, chúng tôi tìm ngôn ngữ-chữ viết gần nhất L2: ngôn ngữ-chữ viết có phân kỳ perplexity thấp nhất (§3.3). Nếu L1 và L2 không thuộc cùng họ ngôn ngữ học, chúng tôi kiểm tra L1/L2 thủ công và thực hiện hành động thích hợp như loại bỏ corpus (ví dụ, nếu nó thực sự là tiếng Anh) hoặc sửa mã ISO được gán cho corpus.

### 3.5 Dữ liệu huấn luyện: Glot500-c

Trong số hơn 2000 ngôn ngữ-chữ viết mà chúng tôi thu thập dữ liệu, sau khi làm sạch, hầu hết có quá ít dữ liệu để huấn luyện trước LLM. Khó định lượng lượng tối thiểu cần thiết cho huấn luyện trước. Do đó, chúng tôi chọn một ngưỡng "an toàn" tương đối cao, 30.000 câu', để bao gồm các ngôn ngữ-chữ viết trong huấn luyện mô hình. Điều này cho phép chúng tôi huấn luyện mô hình hiệu quả và bao phủ nhiều ngôn ngữ ít tài nguyên.

Bảng 1 đưa ra thống kê Glot500-c. Xem §B để có danh sách các ngôn ngữ-chữ viết. Chúng tôi huấn luyện Glot500-m trên Glot500-c; lưu ý rằng trong khi Glot500-c tập trung vào các ngôn ngữ đuôi, nó chứa một số dữ liệu trong các ngôn ngữ đầu mà chúng tôi bao gồm trong huấn luyện Glot500-m để ngăn chặn quên thảm khốc.

Chúng tôi chia corpus cho mỗi ngôn ngữ thành train/dev/test, dành 1000 câu' mỗi cái cho dev và test và sử dụng phần còn lại cho train. Chúng tôi chọn 1000 câu song song nếu chúng tôi có bản dịch Kinh thánh và thêm 500 câu mỗi cái vào test và dev. Những câu song song này truyền đạt ý nghĩa giống hệt nhau và tạo điều kiện cho đánh giá liên ngôn ngữ. Chúng tôi huấn luyện trước mô hình chỉ sử dụng dữ liệu huấn luyện.

## 4 Glot500-m

### 4.1 Mở rộng từ vựng

Để mở rộng từ vựng của XLM-R, chúng tôi sử dụng SentencePiece (Kudo và Richardson, 2018) với mô hình ngôn ngữ unigram (Kudo, 2018) để huấn luyện một tokenizer với kích thước từ vựng 250K trên Glot500-c. Chúng tôi lấy mẫu dữ liệu từ các ngôn ngữ-chữ viết khác nhau theo phân phối đa thức, với α = 0.3. Lượng chúng tôi lấy mẫu cho các ngôn ngữ đầu giống với các ngôn ngữ đuôi có lượng thấp nhất; điều này ưu tiên các ngôn ngữ đuôi – các ngôn ngữ đầu đã được XLM-R học tốt. Chúng tôi hợp nhất các token thu được với từ vựng của XLM-R. Khoảng 100K token mới thực ra là token cũ, tức là đã là một phần của từ vựng XLM-R. Chúng tôi lấy xác suất của các token (thực sự) mới trực tiếp từ SentencePiece. Sau khi thêm 151K token mới vào từ vựng của XLM-R (có kích thước 250K), kích thước từ vựng của Glot500-m là 401K.

Chúng tôi cũng có thể tính toán xác suất của các token hiện có và mới trên hỗn hợp corpus huấn luyện XLM-R gốc và Glot500-c (Chung et al., 2020). Đối với các ngôn ngữ đầu, tỷ lệ phần trăm token thay đổi sử dụng tokenizer mới so với tokenizer gốc dao động từ 0.2% đến 50%. Tuy nhiên, chúng tôi không tìm thấy mối quan hệ nào giữa tỷ lệ phần trăm token thay đổi và thay đổi hiệu suất trên các nhiệm vụ downstream. Do đó, có ít hiệu ứng của tokenization trong các thí nghiệm của chúng tôi.

### 4.2 Tiếp tục huấn luyện trước

Chúng tôi tạo ra Glot500-m bằng cách tiếp tục huấn luyện trước XLM-R-B với mục tiêu MLM. Bộ tối ưu được sử dụng là Adam với beta (0.9, 0.999). Tốc độ học ban đầu: 5e-5. Mỗi bước huấn luyện chứa một batch 384 mẫu huấn luyện được chọn ngẫu nhiên từ tất cả các ngôn ngữ-chữ viết. Chiến lược lấy mẫu trên các ngôn ngữ-chữ viết giống như cho mở rộng từ vựng (§4.1). Chúng tôi lưu checkpoint mỗi 10K bước và chọn checkpoint có hiệu suất trung bình tốt nhất trên các nhiệm vụ downstream bằng early stopping. Bảng 2 liệt kê kích thước của XLM-R-B, XLM-R-L và Glot500-m. Ngoại trừ từ vựng lớn hơn (§4.1), Glot500-m có cùng kích thước với XLM-R-B. Chúng tôi huấn luyện Glot500-m trên máy chủ với tám GPU NVIDIA RTX A6000 trong hai tuần.

Tương tự như XLM-R, chúng tôi nối các câu' của một ngôn ngữ-chữ viết và đưa chúng như một luồng vào tokenizer. Đầu ra kết quả sau đó được chia thành các chunk 512 token và đưa vào mô hình.

## 5 Thiết lập thí nghiệm

Đối với hầu hết các ngôn ngữ đuôi, không có dữ liệu đánh giá được gán nhãn thủ công. Do đó chúng tôi áp dụng chiến lược đánh giá hỗn hợp: dựa một phần trên nhãn của con người, một phần trên các phương pháp đánh giá có thể áp dụng cho nhiều ngôn ngữ mà không đòi hỏi dữ liệu vàng.

Bảng 3 liệt kê tất cả các nhiệm vụ đánh giá của chúng tôi.

Perplexity Theo Salazar et al. (2020), chúng tôi tính toán pseudoperplexity (PPPL) trên tập test được giữ lại. PPPL dựa trên việc che một token từng cái một (không từ trái sang phải). Salazar et al. (2020) đưa ra bằng chứng rằng PPPL là thước đo tốt hơn về khả năng chấp nhận ngôn ngữ học so với perplexity từ trái sang phải tiêu chuẩn.

Roundtrip Alignment Để đánh giá chất lượng của các biểu diễn đa ngôn ngữ cho một phạm vi rộng các ngôn ngữ đuôi mà không có dữ liệu vàng của con người, chúng tôi áp dụng đánh giá roundtrip (Dufter et al., 2018). Đầu tiên chúng tôi căn chỉnh từ các câu' trong corpus song song dựa trên các biểu diễn đa ngôn ngữ của một LLM. Sau đó chúng tôi bắt đầu từ một từ w trong một câu' bằng ngôn ngữ-chữ viết L1, theo các liên kết căn chỉnh đến các bản dịch của nó trong ngôn ngữ-chữ viết L2, sau đó các liên kết căn chỉnh từ L2 đến L3 và cứ thế, cho đến cuối cùng chúng tôi theo các liên kết căn chỉnh trở lại L1. Nếu "roundtrip" này đưa chúng tôi trở lại w, thì nó cho thấy LLM có các biểu diễn tương tự cho ý nghĩa của w trong các ngôn ngữ-chữ viết L1, L2, L3, v.v. Nói cách khác, chất lượng liên ngôn ngữ của các biểu diễn cao. Ngược lại, thất bại trong việc trở lại w là dấu hiệu của các biểu diễn đa ngôn ngữ kém.

Chúng tôi sử dụng SimAlign (Jalili Sabet et al., 2020) và căn chỉnh ở cấp sub-word trên phần Kinh thánh của test, dựa trên các biểu diễn của LLM được tính bởi lớp transformer 8 như được đề xuất trong bài báo gốc. Chúng tôi sử dụng symmetrization giao: mỗi từ trong một câu' được căn chỉnh với nhiều nhất một từ trong câu' khác.

Như thước đo đánh giá, chúng tôi tính toán tỷ lệ phần trăm roundtrip thành công, tức là roundtrip bắt đầu từ w trong L1 và trở lại w. Đối với mỗi ngôn ngữ-chữ viết trong test, chúng tôi chọn ngẫu nhiên ba ngôn ngữ-chữ viết làm điểm trung gian L2, L3, L4. Vì các điểm trung gian ảnh hưởng đến kết quả, chúng tôi chạy thí nghiệm năm lần với các điểm trung gian khác nhau và báo cáo trung bình. Tất cả các mô hình được đánh giá với cùng năm tập ba ngôn ngữ-chữ viết trung gian.

Gán nhãn chuỗi Chúng tôi xem xét hai nhiệm vụ gán nhãn chuỗi: Nhận dạng thực thể có tên (NER) và gán nhãn từ loại (POS). Chúng tôi sử dụng tập dữ liệu WikiANN (Pan et al., 2017) cho NER và phiên bản v2.11 của Universal Dependencies (UD) (de Marneffe et al., 2021) cho POS. Vì dữ liệu huấn luyện không tồn tại cho một số ngôn ngữ, chúng tôi fine-tune trên tiếng Anh (với early stopping dựa trên dev) và đánh giá chuyển giao zero-shot trên tất cả các ngôn ngữ được bao phủ bởi WikiANN/UD. Chúng tôi đặt tốc độ học là 2e-5 với Adam.

Truy xuất câu Theo (Hu et al., 2020), chúng tôi sử dụng tối đa 1000 câu' được căn chỉnh tiếng Anh từ Tatoeba (Artetxe và Schwenk, 2019) để đánh giá SentRetr (truy xuất câu). Chúng tôi cũng sử dụng 500 câu' được căn chỉnh tiếng Anh từ phần Kinh thánh của test. Chúng tôi tìm nearest neighbor sử dụng độ tương tự cosine dựa trên trung bình word embedding ở lớp l = 8 – theo Jalili Sabet et al. (2020) – và tính toán độ chính xác top10. Để so sánh công bằng và vì các kiến trúc giống nhau, chúng tôi không tối ưu hóa siêu tham số l cho Glot500-m và XLM-R-B.

Phân loại văn bản Chúng tôi đánh giá trên Taxi1500 (Ma et al., 2023). Nó cung cấp dữ liệu vàng cho phân loại văn bản với sáu lớp trong một số lượng lớn ngôn ngữ-chữ viết mà Glot500-m hỗ trợ 354. Chúng tôi fine-tune trên tiếng Anh (với early stopping trên dev) và đánh giá zero-shot trên test của ngôn ngữ-chữ viết đích. Tốc độ học: 2e-5, kích thước batch: 16 (theo Ma et al. (2023)).

## 6 Thí nghiệm

Trong phần này, chúng tôi thảo luận về kết quả tổng hợp. Để có kết quả chi tiết, xem §D và §E.

### 6.1 Kết quả

Bảng 4 đưa ra kết quả. Glot500-m vượt trội XLM-R-B trên tất cả các nhiệm vụ cho cả ngôn ngữ-chữ viết đầu và đuôi, ngoại trừ POS trên đầu. Việc Glot500-m vượt trội XLM-R-B là dự kiến đối với các ngôn ngữ-chữ viết đuôi (tức là những ngôn ngữ không được XLM-R bao phủ). Đối với những ngôn ngữ-chữ viết này, biên độ cải thiện là lớn. Hiệu suất vượt trội có thể có vẻ phản trực giác đối với các ngôn ngữ-chữ viết đầu (những ngôn ngữ được XLM-R bao phủ) vì Glot500-m có cùng số lượng tham số (không phải embedding) với XLM-R-B. Vì số lượng ngôn ngữ được bao phủ đã tăng lên rất nhiều, để lại ít dung lượng hơn cho mỗi ngôn ngữ, chúng ta có thể mong đợi hiệu suất kém. Có một vài giải thích có thể. Thứ nhất, XLM-R có thể bị huấn luyện thiếu, và việc bao gồm thêm dữ liệu huấn luyện ngôn ngữ đầu có thể cải thiện các biểu diễn của chúng. Thứ hai, có nhiều ngôn ngữ hơn có thể cải thiện tính đa ngôn ngữ bằng cách cho phép các ngôn ngữ tương tác và tăng cường biểu diễn lẫn nhau và chuyển giao liên ngôn ngữ. Thứ ba, có những ngôn ngữ tương tự với các ngôn ngữ đầu trong số các ngôn ngữ đuôi, điều này lần lượt hỗ trợ các ngôn ngữ đầu.

Khoảng cách giữa Glot500-m và các baseline cho các ngôn ngữ-chữ viết đuôi trong gán nhãn chuỗi nhỏ hơn. Những nhiệm vụ này không đòi hỏi hiểu biết sâu sắc về ngôn ngữ và do đó chuyển giao từ các ngôn ngữ-chữ viết đầu đến đuôi dễ dàng hơn thông qua các token được chia sẻ.

Glot500-m cũng vượt trội XLM-R-L cho các ngôn ngữ-chữ viết đuôi (tất cả nhiệm vụ) và các ngôn ngữ-chữ viết đầu (3 nhiệm vụ). Điều này cho thấy rằng mở rộng quy mô kích thước không phải là cách duy nhất để cải thiện. Chúng ta cũng có thể cải thiện chất lượng của các biểu diễn LLM đa ngôn ngữ bằng cách tăng số lượng ngôn ngữ.

### 6.2 Phạm vi bao phủ ngôn ngữ

Bảng 5 so sánh Glot500-m với XLM-R-B về pseudoperplexity. Để so sánh công bằng, chúng tôi sử dụng chuẩn hóa cấp từ. Đối với 69 ngôn ngữ-chữ viết đầu, Glot500-m có hiệu suất kém hơn XLM-R-B. Điều này dự kiến vì dữ liệu huấn luyện của Glot500-m nhỏ cho những ngôn ngữ-chữ viết này. Glot500-m vượt trội XLM-R-B cho 420 ngôn ngữ-chữ viết đuôi.

Có tám ngôn ngữ-chữ viết đuôi mà Glot500-m có hiệu suất kém hơn XLM-R-B. Năm ngôn ngữ đuôi có ngôn ngữ đầu tương tự nơi hai ngôn ngữ chia sẻ một macro-ngôn ngữ: ekk/Estonian tiêu chuẩn (est/Estonian), aln/Gheg Albanian (sqi/Albanian), nob/Norwegian Bokmal (nor/Norwegian), hbs/Serbo-Croatian (srp/Serbian), lvs/Standard Latvian (lav/Latvian). Vì corpus huấn luyện trước của XLM-R-B lớn cho năm ngôn ngữ đầu, hiệu suất của nó tốt cho các ngôn ngữ đuôi gần.

Ba ngôn ngữ khác đều có chữ viết độc đáo: sat/Santali (chữ viết Ol Chiki), div/Dhivehi (chữ viết Thaana), iku/Inuktitut (âm tiết Inuktitut). Đối với những ngôn ngữ này, tokenizer của XLM-R-B trả về nhiều token UNK vì nó không được huấn luyện trên những chữ viết này, dẫn đến ước tính pseudoperplexity không hợp lý lạc quan bởi triển khai của chúng tôi.

Pseudoperplexity được chuẩn hóa cấp token của Glot500-m dao động từ 1.95 cho lhu/Lahu đến 94.4 cho tok/Toki Pona. Trung bình là 13.5, trung vị 10.6. Chúng tôi phân tích năm ngôn ngữ-chữ viết có pseudoperplexity cao nhất: tok_Latn, luo_Latn, acm_Arab, ach_Latn, và teo_Latn.

tok/Toki Pona là ngôn ngữ nhân tạo. Theo Wikipedia: "Về cơ bản các khái niệm giống hệt nhau có thể được mô tả bằng các từ khác nhau vì sự lựa chọn dựa trên nhận thức và kinh nghiệm của người nói." Tính chất này có thể dẫn đến biến động cao hơn và perplexity cao hơn.

acm/Mesopotamian Arabic chứa một số lượng lớn tweet ở dạng thô. Điều này có thể dẫn đến các token khó dự đoán trong test.

luo/Luo, ach/Acoli và teo/Teso là các ngôn ngữ Nilotic liên quan được nói ở Kenya, Tanzania, Uganda và Nam Sudan. Perplexity cao của chúng có thể liên quan đến thực tế rằng chúng là các ngôn ngữ thanh điệu, nhưng các thanh điệu không được chỉ ra bằng chữ viết. Một giải thích khả dĩ khác là dữ liệu huấn luyện bị chi phối bởi một subcorpus (Jehova's Witnesses) trong khi dữ liệu test bị chi phối bởi PBC. Có sự khác biệt chính tả giữa hai nguồn, ví dụ, "dong" (JW) so với "doŋ" (PBC) cho Acoli. Ba ngôn ngữ này cũng được nói trên khu vực rộng lớn ở các quốc gia có ngôn ngữ tiêu chuẩn khác nhau, điều này có thể tăng biến động.

Phân tích của chúng tôi không kết luận. Tuy nhiên chúng tôi lưu ý rằng khoảng cách giữa ba ngôn ngữ và các ngôn ngữ khó tiếp theo về mặt pseudoperplexity không lớn. Vậy có thể Luo, Acoli và Teso đơn giản là (vì những lý do còn phải xác định) các ngôn ngữ có perplexity cao hơn những ngôn ngữ khác.

### 6.3 Tiến trình huấn luyện

Để phân tích quá trình huấn luyện, chúng tôi đánh giá Glot500-m trên gán nhãn chuỗi và SentRetr ở các khoảng 10.000 bước. Hình 1 cho thấy hiệu suất cải thiện nhanh chóng ở đầu huấn luyện, nhưng sau đó tốc độ cải thiện chậm lại. Xu hướng này đặc biệt rõ rệt đối với các ngôn ngữ đuôi trong SentRetr. So sánh, gán nhãn chuỗi tương đối đơn giản, với baseline (XLM-R-B, epoch 0) đạt hiệu suất cao bằng cách chuyển giao đúng các lớp phổ biến như động từ và danh từ thông qua từ vựng được chia sẻ, dẫn đến cải thiện nhỏ hơn của Glot500-m so với XLM-R-B.

Đối với SentRetr, chúng tôi quan sát thấy những cải thiện lớn hơn cho Kinh thánh so với Tatoeba. Điều này có thể do tỷ lệ cao hơn của dữ liệu tôn giáo trong Glot500-c, so với dữ liệu huấn luyện của XLM-R (tức là CC100).

Hiệu suất trung bình trên các nhiệm vụ downstream đạt đỉnh ở 480K bước. Chúng tôi đã chụp ảnh Glot500-m ở giai đoạn này và phát hành nó.

### 6.4 Phân tích trên các ngôn ngữ-chữ viết

Để phân tích hiệu ứng của các ngôn ngữ-chữ viết, chúng tôi chọn năm ngôn ngữ-chữ viết đuôi mỗi cái với mức tăng lớn nhất và nhỏ nhất khi so sánh Glot500-m với XLM-R-B cho SentRetr và gán nhãn chuỗi.

Bảng 6 cho thấy Glot500-m cải thiện các ngôn ngữ với chữ viết không được XLM-R bao phủ (ví dụ, div/Dhivehi, chữ viết Thaana, xem §6.2) với biên độ lớn vì XLM-R đơn giản coi các chữ viết không được bao phủ như token không xác định và không thể tính toán các biểu diễn có ý nghĩa cho đầu vào. Lượng lớn dữ liệu chúng tôi thu thập trong Glot500-c cũng góp phần vào cải thiện cho các ngôn ngữ đuôi, ví dụ, cho tat_Cyrl (Tatar) trong SentRetr Tatoeba và mlt_Latn (Maltese) trong POS. Xem §6.7 để có phân tích chi tiết về hiệu ứng của kích thước corpus.

Mặt khác, Glot500-m đạt được kết quả chỉ tương đương hoặc thậm chí tệ hơn cho một số ngôn ngữ-chữ viết. Chúng tôi thấy ít nhất ba giải thích. (i) Như đã thảo luận trong §6.2, một số ngôn ngữ đuôi (ví dụ, nob/Norwegian Bokmal) gần với ngôn ngữ đầu (ví dụ, nor/Norwegian), vì vậy Glot500-m không có lợi thế so với XLM-R-B. (ii) Một ngôn ngữ ở cuối phạm vi kích thước corpus của chúng tôi (tức là 30.000 câu'). Ví dụ: xav_Latn, Xavánte. (iii) Một số ngôn ngữ hoàn toàn khác biệt với tất cả các ngôn ngữ khác trong Glot500-c, do đó không có sự hỗ trợ từ bất kỳ ngôn ngữ tương tự nào. Một ví dụ là mau_Latn, Huautla Mazatec. Glot500-m có thời gian khó khăn hơn nhiều trong việc học các biểu diễn tốt trong những trường hợp này.

### 6.5 Ngôn ngữ với nhiều chữ viết

Bảng 7 so sánh hiệu suất SentRetr XLM-R-B với Glot500-m cho sáu ngôn ngữ có hai chữ viết. Không có gì ngạc nhiên, XLM-R hoạt động tốt hơn nhiều cho một ngôn ngữ-chữ viết mà nó được huấn luyện trước ("đầu") so với một ngôn ngữ mà nó không được huấn luyện ("đuôi"). Chúng ta có thể cải thiện hiệu suất của một ngôn ngữ, thậm chí vượt qua ngôn ngữ-chữ viết được XLM-R bao phủ, nếu chúng ta thu thập đủ dữ liệu cho chữ viết không được XLM-R bao phủ. Đối với các ngôn ngữ có hai chữ viết không được XLM-R bao phủ, hiệu suất tốt hơn cho chữ viết mà chúng ta thu thập corpus lớn hơn. Ví dụ, kaa_Cyrl (Kara-Kalpak) có khoảng ba lần dữ liệu nhiều hơn kaa_Latn. Điều này giải thích tại sao kaa_Cyrl vượt trội kaa_Latn 30%.

Dufter và Schütze (2020) phát hiện rằng, sau khi huấn luyện một mô hình đa ngôn ngữ với hai chữ viết cho tiếng Anh (tiếng Anh tự nhiên và "tiếng Anh giả"), mô hình hoạt động tốt trong chuyển giao zero-shot nếu dung lượng của mô hình có kích thước phù hợp (tức là không quá nhỏ, không quá lớn). Các thí nghiệm của chúng tôi với dữ liệu thực cho thấy sự phức tạp của vấn đề: ngay cả khi có kích thước "đúng" cho một LLM hỗ trợ cả việc học đầy đủ các ngôn ngữ và chuyển giao đa ngôn ngữ, kích thước này khó xác định và có thể khác nhau cho các cặp ngôn ngữ khác nhau trong một mô hình được mở rộng quy mô theo chiều ngang lớn như Glot500-m.

### 6.6 Phân tích trên các họ ngôn ngữ

Bảng 8 so sánh hiệu suất SentRetr Glot500-m với XLM-R-B cho bảy họ ngôn ngữ có mười hoặc nhiều ngôn ngữ-chữ viết hơn trong Glot500-c. Chúng tôi gán ngôn ngữ vào các họ dựa trên Glottolog. Nói chung, XLM-R có hiệu suất tốt hơn khi nhiều ngôn ngữ-chữ viết từ một họ ngôn ngữ được đại diện trong dữ liệu huấn luyện của nó; ví dụ, hiệu suất tốt hơn cho indo1319 và tệ hơn cho maya1287. Kết quả cho thấy cải thiện của Glot500-m so với XLM-R-B càng lớn, phạm vi bao phủ của corpus huấn luyện Glot500-c của chúng tôi đối với một họ càng tốt.

### 6.7 Hiệu ứng của lượng dữ liệu huấn luyện

Chúng tôi kiểm tra tương quan giữa kích thước corpus huấn luyện trước và hiệu suất zero-shot của Glot500-m. Chúng tôi tập trung vào SentRetr Bible (§5) vì nó hỗ trợ nhiều ngôn ngữ đầu và đuôi nhất. Chúng tôi tìm thấy Pearson's r = 0.34, tức là kích thước corpus và hiệu suất có tương quan vừa phải, nhưng rõ ràng. Chúng tôi nghi ngờ rằng tương quan không lớn hơn vì, ngoài kích thước corpus của ngôn ngữ l chính nó, kích thước corpus của các ngôn ngữ liên quan chặt chẽ với l cũng là một yếu tố quan trọng (xem §6.4 cho phát hiện tương tự với tiếng Na Uy). Do đó chúng tôi cũng tính toán Pearson's r giữa (i) hiệu suất của ngôn ngữ l trên SentRetr Bible và (ii) kích thước corpus chung của l và k nearest neighbor của nó (theo phân kỳ perplexity, §3.3). Trong trường hợp này, Pearson's r = 0.44 (cho cả k = 3 và k = 4), cho thấy kích thước corpus của các ngôn ngữ nearest neighbor thực sự đóng vai trò.

### 6.8 Hỗ trợ thông qua các ngôn ngữ liên quan

Dựa trên §6.7, có một cách khác chúng ta có thể điều tra hiệu ứng tích cực của các ngôn ngữ liên quan chặt chẽ đối với hiệu suất: Chúng ta có thể so sánh hiệu suất (một lần nữa trên SentRetr Bible) của tiếp tục huấn luyện trước chỉ trên một ngôn ngữ (chúng tôi gọi mô hình này là Glot+1) với trên tất cả 511 ngôn ngữ được đại diện trong Glot500-c (tức là Glot500-m). Bảng 9 trình bày kết quả cho sáu ngôn ngữ-chữ viết được chọn từ các họ ngôn ngữ khác nhau và cho thấy một số ngôn ngữ không nhận được hỗ trợ từ các ngôn ngữ liên quan (ba hàng đầu). Trong trường hợp đó, Glot+1 có thể tập trung hoàn toàn vào việc học ngôn ngữ cô lập và làm tốt hơn Glot500-c. Các ngôn ngữ khác (ba hàng cuối) thực sự nhận được hỗ trợ từ các ngôn ngữ liên quan. Ví dụ, Southern Quechua (quh) dường như nhận được hỗ trợ trong Glot500-m từ Cuzco Quechua (quz) liên quan chặt chẽ, dẫn đến Glot500-m vượt trội Glot+1.

## 7 Kết luận và công trình tương lai

Chúng tôi thu thập và làm sạch dữ liệu Glot500-c, một corpus lớn của hàng trăm ngôn ngữ đuôi (tức là long-tail) thường bị bỏ qua và tạo ra Glot500-m, một LLM được huấn luyện trên Glot500-c và bao phủ những ngôn ngữ này. Chúng tôi đánh giá Glot500-m trên sáu nhiệm vụ cho phép chúng tôi đánh giá hầu như tất cả các ngôn ngữ. Chúng tôi quan sát thấy những cải thiện lớn cho cả ngôn ngữ đầu và đuôi so với XLM-R. Phân tích của chúng tôi cho thấy không có yếu tố đơn lẻ nào giải thích đầy đủ chất lượng của biểu diễn của một ngôn ngữ trong mô hình đa ngôn ngữ. Thay vào đó, sự kết hợp của các yếu tố quan trọng, bao gồm kích thước corpus, chữ viết, "sự trợ giúp" từ các ngôn ngữ liên quan và tổng dung lượng của mô hình.

Công trình này là công trình đầu tiên tạo ra một mô hình ngôn ngữ trên tập dữ liệu vài trăm gigabyte và công khai nó cho một số lượng lớn và đa dạng các ngôn ngữ ít tài nguyên như vậy. Trong nghiên cứu tương lai, chúng tôi muốn huấn luyện các mô hình lớn hơn để điều tra thêm hiệu ứng của kích thước mô hình, chưng cất các mô hình đa ngôn ngữ cao để triển khai tiết kiệm tài nguyên, khám phá các lựa chọn thay thế cho tiếp tục huấn luyện trước và sử dụng các mô hình cho nhiều nhiệm vụ downstream ngôn ngữ đuôi hơn.

## Hạn chế

(1) Chúng tôi đã không thực hiện bất kỳ tìm kiếm siêu tham số toàn diện nào, điều này sẽ củng cố thêm kết quả của chúng tôi. Quyết định này được đưa ra do chi phí cao của việc huấn luyện nhiều mô hình. (2) So với các mô hình rất lớn hiện tại, Glot500-m tương đối nhỏ. (3) Mặc dù chúng tôi đã cố gắng giảm thiểu lượng nhiễu trong dữ liệu của mình, một số nhiễu vẫn còn tồn tại.

## Tuyên bố đạo đức

Có hai vấn đề đáng đề cập liên quan đến dự án này. Thứ nhất, chúng tôi không thể kiểm tra kỹ lưỡng nội dung của dữ liệu cho tất cả các ngôn ngữ, do đó chúng tôi không thể xác nhận việc không có phân biệt đối xử dựa trên các yếu tố như chủng tộc hoặc tính dục. Dữ liệu chỉ được sử dụng như một corpus văn bản, và nội dung không nên được hiểu như sự tán thành của nhóm chúng tôi. Nếu mô hình sau đó được sử dụng để tạo sinh, có thể dữ liệu huấn luyện có thể được phản ánh trong đầu ra được tạo. Tuy nhiên, việc giải quyết các thiên kiến tiềm ẩn trong dữ liệu là một lĩnh vực cho nghiên cứu tương lai. Thứ hai, điều quan trọng cần lưu ý là trong khi các nguồn dữ liệu được sử dụng trong nghiên cứu này không cấm rõ ràng việc tái sử dụng dữ liệu cho mục đích nghiên cứu, một số nguồn có tuyên bố bản quyền chỉ ra rằng việc sử dụng như vậy được phép trong khi những nguồn khác thì không. Ngoài ra, một số nguồn cấm việc phân phối lại dữ liệu. Do đó, dữ liệu từ những nguồn này được loại bỏ khỏi phiên bản đã xuất bản của Glot2000-c.

## Lời cảm ơn

Chúng tôi muốn cảm ơn Renhao Pei, Yihong Liu, Verena Blaschke, và các reviewer ẩn danh. Công trình này được tài trợ bởi European Research Council (grant #740516 và #758969) và EU's Horizon Europe Research and Innovation Actions (UTTER, hợp đồng 101070631).
