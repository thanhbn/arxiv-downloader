# 2304.08823.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2304.08823.pdf
# File size: 246344 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Transfer to a Low-Resource Language via Close Relatives:
The Case Study on Faroese
Vésteinn Snæbjarnarson1,2Annika Simonsen3Goran Glavaš4Ivan Vuli ´c5
1University of Copenhagen2Miðeind ehf3University of Iceland
4University of Würzburg5University of Cambridge
Abstract
Multilingual language models have pushed
state-of-the-art in cross-lingual NLP trans-
fer. The majority of zero-shot cross-lingual
transfer, however, use one and the same
massively multilingual transformer (e.g.,
mBERT or XLM-R) to transfer to alltarget
languages, irrespective of their typological,
etymological, and phylogenetic relations
to other languages. In particular, readily
available data and models of resource-rich
sibling languages are often ignored. In this
work, we empirically show, in a case study
for Faroese – a low-resource language from
a high-resource language family – that by
leveraging the phylogenetic information
and departing from the ‘one-size-ﬁts-all’
paradigm, one can improve cross-lingual
transfer to low-resource languages. In par-
ticular, we leverage abundant resources of
other Scandinavian languages (i.e., Danish,
Norwegian, Swedish, and Icelandic) for the
beneﬁt of Faroese. Our evaluation results
show that we can substantially improve
the transfer performance to Faroese by ex-
ploiting data and models of closely-related
high-resource languages. Further, we re-
lease a new web corpus of Faroese and
Faroese datasets for named entity recogni-
tion (NER), semantic text similarity (STS),
and new language models trained on all
Scandinavian languages.
1 Introduction
Massively multilingual Transformer-based lan-
guage models (MMTs) such as mBERT (Devlin
et al., 2019), XLM-RoBERTa (Conneau et al.,
2020a) and mT5 (Xue et al., 2021) have been
the driving force of modern multilingual NLP, al-
lowing for rapid bootstrapping of language tech-nology for a wide range of low(er)-resource lan-
guages by means of (zero-shot or few-shot) cross-
lingual transfer from high(er)-resource languages
(Lauscher et al., 2020; Hu et al., 2020; Xu and Mur-
ray, 2022; Schmidt et al., 2022). Cross-lingual
transfer with MMTs is not without drawbacks.
MMTs’ representation spaces are heavily skewed
in favor of high-resource languages, for which they
have been exposed to much more data in pretrain-
ing (Joshi et al., 2020; Wu and Dredze, 2020); com-
bined with the ‘curse of multilinguality’ – i.e., lim-
ited per-language representation quality stemming
from a limited capacity of the model (Conneau
et al., 2020a; Pfeiffer et al., 2022) – this leads to
lower representational quality for languages under-
represented in MMTs’ pretraining. Cross-lingual
transfer with MMTs thus fails exactly in settings
in which it is needed the most: for low-resource
languages with small digital footprint (Zhao et al.,
2021). Despite these proven practical limitations,
the vast majority of work on cross-lingual transfer
still relies on MMTs due to their appealing concep-
tual generality: in theory, they support transfer be-
tween any two languages seen in their pretraining.
Such strict reliance on MMTs effectively ignores
the linguistic phylogenetics and fails to directly
leverage resources of resource-rich languages that
are closely related to a target language of interest.
In this work, we attempt to mitigate the above
limitations for a particular group of languages, de-
parting from the ‘one-size-ﬁts-all’ paradigm based
on MMTs. We focus on a frequent and realis-
tic setup in which the target language is a low-
resource language but from a high-resource lan-
guage family, i.e., with closely related resource-
rich languages. A recent comprehensive evaluation
of the languages used in Europe1scores languages
1The Digital Language Equality in Europe by 2030:
Strategic Agenda and Roadmap published by the Eu-
ropean Language Equality Programme (ELE), https://
european-language-equality.eu/agenda/ .arXiv:2304.08823v1  [cs.CL]  18 Apr 2023

--- PAGE 2 ---
based on the available resources. Languages such
as German and Spanish score at around 0.5 of
the English scores, and more than half of the lan-
guages are scored below 0.02 of the English score.
Many, including almost all regional and minority
languages such as Faroese, Scottish Gaelic, Oc-
citan, Luxembourgish, Romani languages, Sicil-
ian and Meänkieli have the score of (almost) 0.
However, what differentiates these languages from
low-resource languages from Africa (e.g., Niger-
Congo family) or indigenous languages of Latin
America (e.g., Tupian family) is the fact that they
typically have closely related high-resource lan-
guages as ‘language siblings’ . In this case, we
believe, language models (LMs) of closely related
high-resource languages promise more effective
transfer compared to using MMTs, plagued by the
‘curse of multilinguality’, as the vehicle of transfer.
In this proof-of-concept case study, we focus on
Faroese as the target language and demonstrate the
beneﬁts of linguistically informed transfer. We take
advantage of available data and resources from the
closely related but much more ‘NLP-developed’
other Scandinavian languages.2We show that us-
ing “Scandinavian” LMs brings substantial gains in
downstream transfer to Faroese compared to using
XLM-R as a widely used off-the-shelf MMT. The
gains are particularly pronounced for the task of
semantic text similarity (STS), the only high-level
semantic task in our evaluation. We further show
that adding a limited-size target-language corpus
to LM’s pretraining corpora brings further gains
in downstream transfer. As another contribution
of this work, we collect and release: (1) a corpus
of web-scraped monolingual Faroese, (2) multiple
LMs suitable for Faroese, including those trained
on all ﬁve Scandinavian languages, and (3) two
new task-speciﬁc datasets for Faroese labeled by
native speakers: for NER and STS.
2 Background and Related Work
Cross-Lingual Transfer Learning with MMTs
and Beyond. A common approach to cross-lingual
transfer learning involves pretrained MMTs (De-
vlin et al., 2019; Conneau et al., 2020a; Xue et al.,
2The Scandinavian languages are a family of Indo-
European languages that form the North Germanic branch
of the Germanic languages. The largest languages of the fam-
ily are: (1) Danish (population 5.8M), Norwegian (5.4M) and
Swedish (10.4M) – the Mainland Scandinavian languages,
and (2) Icelandic (373K) and Faroese (54K) – the Insular
Scandinavian languages.2021). These models can be further pretrained for
speciﬁc languages or directly adapted for down-
stream tasks. A major downside of the MMTs has
been dubbed the curse of multilinguality (Conneau
et al., 2020a), where the model becomes saturated
and performance can not be improved further for
one language without a sacriﬁce elsewhere, some-
thing which continued pretraining for a given lan-
guage alleviates (Pfeiffer et al., 2020). Adapter
training, such as in (Pfeiffer et al., 2020; Üstün
et al., 2022), where small adapter modules are
added to pretrained models, has also enabled cost-
efﬁcient adaptation of these models. The adapters
can then be used to ﬁne-tune for speciﬁc languages
and tasks without incurring catastrophic forgetting.
Other methods involve translation-based trans-
fer (Hu et al., 2020; Ponti et al., 2021), and trans-
fer from monolingual language models (Artetxe
et al., 2020; Gogoulou et al., 2022; Minixhofer
et al., 2022). Bilingual lexical induction (BLI) is
the method of mapping properties, in particular em-
beddings, from one language to another via some
means such as supervised embedding alignment,
unsupervised distribution matching or using an or-
thogonality constraint (Lample et al., 2018; Sø-
gaard et al., 2018; Patra et al., 2019), and has also
been used to build language tools in low-resource
languages (Wang et al., 2022).
Attempts to alleviate the abovementioned issues
have been made, such as vocabulary extension
methods (Pfeiffer et al., 2021), which add miss-
ing tokens and their conﬁgurations to the embed-
ding matrix. Phylogeny-inspired methods have also
been used where adapters have been trained for
multiple languages and stacked to align with the
language family of the language of interest (Faisal
and Anastasopoulos, 2022). Some analysis on the
effects of using pretrained MMTs has been done:
Fujinuma et al. (2022) conclude that using pre-
trained MMTs that share script and overlap in the
family with the target language is beneﬁcial. How-
ever, when adapting the model for a new language,
they claim that using as many languages as possible
(up to 100) generally yields the best performance.
Inspired by this line of research, in this work,
we focus on improving MMT-based cross-lingual
transfer for a particular group of languages, those
that have sibling languages with more abundant
data and resources.
NLP Resources in Scandinavian Languages. A
fair amount of language resources have been devel-

--- PAGE 3 ---
oped for the Scandinavian languages, particularly if
aggregated across all languages of the family. It is
also worth mentioning that Danish, Icelandic, Nor-
wegian and Swedish are represented in raw mul-
tilingual corpora such as CC100 (Conneau et al.,
2020b) or mC4 (Xue et al., 2021) as well as in paral-
lel datasets such as (Schwenk et al., 2021; Agi ´c and
Vuli´c, 2019). Large multilingual language models
have been trained on these datasets (Devlin et al.,
2019; Liu et al., 2020; Xue et al., 2021) but have
been shown to have limited capacity for languages
with smaller relative representation in pretraining
corpora. Faroese is not included (at least not cor-
rectly labelled) in these crawled corpora.This may
be in part due to the limited amount of Faroese that
can be found online, and in part due to its close
relatedness to the other languages of the Scandina-
vian family (Haas and Derczynski, 2021). A brief
overview of prior work in cross-lingual transfer to
Faroese is given in Appendix D.
In this work, we use the following open language
resources for the Scandinavian languages.
Danish: The Danish Gigaword Corpus (Strømberg-
Derczynski et al., 2021) is a billion-word corpus
containing a wide variety of text.We also use a
NER resource, the DaNE corpus (Hvingelby et al.,
2020).
Icelandic: With Icelandic as the most closely re-
lated language to Faroese, we experiment with an
Icelandic language model, IceBERT (Snæbjarnar-
son et al., 2022). For the NER experiment, we
make use of the MIM-GOLD-NER corpus (Ingólfs-
dóttir et al., 2020).
Norwegian: The Norwegian Colossal Corpus
(NCC) (Kummervold et al., 2022) contains 49GB
of clean Norwegian data from a variety of sources,
making it the largest such public collection in the
Nordics. We also make use of the NorNE (Jør-
gensen et al., 2020) NER corpus (both for Bokmål
and Nynorsk).
Swedish: The Swedish Gigaword Corpus (Eide
et al., 2016) contains text from between 1950 and
2015. The latest NER corpus for Swedish is Swe-
NERC (Ahrenberg et al., 2020), where the authors
include more modern texts than in earlier corpora.
Faroese: A POS corpus, the Sosiualurin corpus is
an annotated Newspaper corpus with 102k words
(Hansen et al., 2004). The Faroese Wikipedia
has also been used to create a tree bank (Tyers
et al., 2018), which has a Universal Dependencies(UD) mapping. We use this corpus along with the
FarPaHc (Ingason et al., 2012), which also has a
UD mapping.
3 New Faroese Datasets
3.1 Faroese Common Crawl Corpus (FC3)
Faroese monolingual data is scarce, mainly because
of the limited size of the Faroese-speaking popula-
tion. Despite this, we manage to extract a decent
amount of varied Faroese text from the Common
Crawl corpus (FC3). To this effect, we adopted
the approach of Snæbjarnarson et al. (2022) for
Icelandic, i.e., we targeted the websites with the
Faroese top-level domain ( .fo). After clean-up and
deduplication, the obtained Faroese corpus consists
of 98k paragraphs containing in total 9M word-
level tokens. Albeit relatively small compared to
corpora from other Scandinavian languages, this
Faroese corpus still drives signiﬁcant downstream
performance gains (see §5).
3.2 Named Entity Recognition (FoNE)
We annotate the Sosialurin corpus (6,286 lines,
102k words) with named entities following the
CoNLL schema using an Icelandic NER-tagger
trained using the ScandiBERT model, see §4. The
annotation was then manually reviewed. Out of the
118,533 tokens (including punctuation), 9,001 are
annotated using the Date (546), Location (1,774),
Miscellaneous (332), Money (514), Organization
(2,585), Percent (115), Person (2,947) and Time
(188) tags. We refer to this new dataset as FoNE .
3.3 Semantic Similarity (Fo-STS)
The STS Benchmark (Cer et al., 2017) measures
semantic text similarity (STS) between pairs of sen-
tences. For each pair of sentences, the annotators
assigned the score (on a Likert 1-5 scale) that in-
dicates the extent to which the two sentences are
semantically aligned. We manually translated from
English to Faroese 729 sentence pairs from the test
portion of the STS Benchmark; the translation was
carried out by a native speaker of Faroese ﬂuent in
English, who was instructed to preserve in the trans-
lation the extent of semantic alignment between the
original English sentences.
4 Model Training
We train the following new language models: (i)
ScandiBERT is trained on concatenated corpora of
all Scandinavian languages, (ii) ScandiBERT-no-fo

--- PAGE 4 ---
is trained on concatenated corpora of all Scandi-
navian languages except Faroese (i.e., without any
Faroese data, that is, no FC3, Bible or Sosialurin),
and (iii) DanskBERT which is trained only on the
Danish data; we train DanskBERT for the purposes
of comparison with IceBERT, in the setup in which
we carry out downstream transfer to Faroese by
means of a monolingual model of a closely re-
lated language (with Danish being more distant
to Faroese than Icelandic). We additionally eval-
uate transfer with models that have been further
pretrained on the FC3 corpus (indicated with the
-fcsufﬁx). We provide an overview of all training
datasets and hyperparameter conﬁgurations used in
our experiments in Appendix A.
5 Experiments
5.1 Downstream Performance for Faroese
Experimental Setup. In addition to the models
presented in §4, we make use of the monolin-
gual Icelandic model IceBERT and the massively
multilingual XLM-on-RoBERTa (XLM-R).3We
evaluate the performance of this set of pretrained
models in several downstream tasks in Faroese:
Part-of-Speech tagging (POS), Dependency Pars-
ing (DP) (UD datasets introduced in §2), Named
Entity Recognition (NER), and Semantic Text Sim-
ilarity (i.e., the new NER and STS datasets intro-
duced in §3). For all downstream tasks the task-
speciﬁc training and evaluation data span mono-
lingual Faroese data points only: we carry out the
experimentation via ten-fold cross-validation on
the respective Faroese datasets.4For each model
and downstream task, we carry out ten runs with
different random seeds (each run trains the model
for 5 epochs with batches of 16 instances) and re-
port the average performance across runs. The
exception is the STS training in which the models
were ﬁne-tuned for 3 epochs (with training batches
of size 8).5
Results and Discussion. Table 1 summarizes the
3We use the base-sized XLM-R: https://huggingface.
co/xlm-roberta-base .
4Note that our study aims to establish how different pre-
training strategies – and in particular languages included in
pretraining – affect the models’ downstream Faroese perfor-
mance, rather than to investigate the downstream cross-lingual
transfer. One could, naturally, additionally incorporate task-
speciﬁc data in other Scandinavian languages (and also in
English and other languages) in downstream training (i.e.,
perform cross-lingual transfer for the downstream task).
5Due to the limited size of the Faroese dataset, longer
training with larger batch size consistently led to overﬁtting.results across the four downstream tasks. The best-
performing model for POS, as evaluated on the
Sosialurin POS corpus, is ScandiBERT-fc3, out-
performing ScandiBERT by more than 1 point in
terms of F1. However, the ScandiBERT-no-fo-fc3
model, without any Faroese data at pretraining, ob-
tains fully on-par performance with the variant that
does include Faroese data.
The best-performing model for NER, and STS
is the ScandiBERT-no-fo-fc3 model. Somewhat
surprisingly, we get the best performance for the
model that does not include any Faroese data in
the initial pretraining, that is, it does not adjust
the tokenizer/vocabulary to Faroese. Put simply,
we observe slight gains over the ScandiBERT-fc3
model. We hypothesize that this might be due to
the fact that including Faroese in the vocabulary
results in a lower subword overlap with the other
Scandinavian languages, which in consequence,
slightly reduces the potential for transfer. While
there is only a difference of 95 tokens between the
two vocabularies, the difference yields 6% of the
words in FC3 being tokenized differently.
Finally, the results also demonstrate the impor-
tance of focusing on a smaller set of related lan-
guages rather than relying on a broader set of lan-
guages from the MMTs. Unlike the results from
Fujinuma et al. (2022), our results suggest that for
languages with higher-resource ‘siblings’ such as
Faroese, a higher-performing LM is a less general
ScandiBERT model rather than an MMT such as
XLM-R or mBERT. Different variants of ScandiB-
ERT outperform XLM-R without any Faroese data
across the board in all evaluation tasks. Another
interesting ﬁnding is that additionally ﬁne-tuning
on Faroese data (the -fc3 variants) has a much
stronger positive impact on XLM-R as the underly-
ing model than on ScandiBERT. Put simply, the im-
portance of in-target language data decreases with
the availability of more focused pretained LMs cov-
ering only languages related to the target language.
5.2 Additional Experiments
Transfer with Wechsel. To put our work in further
context, beyond comparison to MMTs, we consider
an alternative transfer learning approach, the Wech-
sel method (Minixhofer et al., 2022), a recent well-
performing method for transferring monolingual
Transformers to a new language. Further details
and results are presented in Appendix B: they all
show far worse performance than those presented

--- PAGE 5 ---
POS NER UD FP UD oft STS
Model F1 Acc. F1 Acc. F1 Acc. F1 Acc. Acc.
IceBERT 85.50.19 85.2 0.16 87.90.54 96.4 0.09 93.60.06 94.6 0.03 92.70.32 94.2 0.25 70.61.9
IceBERT-fc3 90.90.06 90.4 0.06 90.90.41 98.9 0.03 96.60.06 97.1 0.06 95.30.38 96.1 0.32 72.91.8
DanskBERT 73.40.19 74.3 0.16 85.60.44 98.4 0.06 86.20.16 87.7 0.09 84.80.57 88.7 0.44 73.21.3
DanskBERT-fc3 87.10.13 86.4 0.13 89.70.54 98.8 0.06 96.00.06 96.6 0.03 94.20.28 95.7 0.19 75.31.1
XLM-R 84.60.28 85.0 0.28 87.80.47 96.3 0.06 93.50.06 94.3 0.03 91.50.44 93.6 0.35 69.52.1
XLM-R-fc3 91.20.09 91.2 0.09 90.90.41 98.90.06 97.30.06 97.7 0.03 95.70.22 96.8 0.19 69.22.1
ScandiBERT-no-fo 88.40.09 88.1 0.09 89.90.25 96.7 0.16 95.90.06 96.4 0.06 93.80.35 95.0 0.32 75.31.5
ScandiBERT-no-fo-fc3 91.50.09 91.2 0.09 91.40.35 98.80.06 97.40.03 97.8 0.03 96.30.22 96.8 0.19 76.51.3
ScandiBERT 90.30.09 90.0 0.13 90.20.28 99.0 0.06 96.50.06 97.1 0.03 95.20.32 96.2 0.25 46.36.3
ScandiBERT-fc3 91.60.06 91.3 0.09 91.00.35 99.0 0.03 97.30.06 97.7 0.06 95.90.25 96.7 0.22 63.86.2
Table 1: Results for all downstream tasks in Faroese using the different base language models, with and
without continued Faroese pre-training. The -fc3 postﬁx indicates models that were further pretrained on
FC3. Standard error intervals are also reported.
in Table 1. We hypothesize this is due to how
closely related the languages we consider are, as
opposed to the distant languages considered in the
original Wechsel work.
Task-Speciﬁc Transfer. To explore the potential
for task-speciﬁc transfer between closely related
languages, we consider if labelled Scandinavian
datasets can be combined to beneﬁt Faroese. In
particular, we look at NER as there is an easy way
to map between labels of the different languages.
See Appendix C for more details. The best result is
achieved when training directly from the IceBERT
model, which has been trained on the large MIM-
GOLD-NER dataset, showing that given enough
resources and a close enough language model, such
a direct approach can be the most effective.
Further Discussion. Some of the results in Table 1
are as expected. Starting from the closest language
relative, the Icelandic model, IceBERT, results in
better performance for all downstream tasks than
starting with the Danish model DanskBERT. The
ScandiBERT model performs better than the mas-
sively multilingual XLM-R on all tasks, bar the
more semantic FO-STS task.
What is more interesting is that the ScandiBERT-
no-fo model that is not trained on Faroese outper-
forms the model that has Faroese included, when
ﬁne-tuned further on the FC3 dataset. In particular,
for the higher level Fo-STS task. We hypothesize
that this forces the Faroese adaptation to use the
word segmentations from the related languages for
a higher transfer beneﬁt, as the tokenizing vocabu-
lary was trained without Faroese. This is something
we hope to investigate more in future work.
6 Conclusion and Future Work
We have shown that leveraging phylogenetic in-
formation and departing from the ‘one-size-ﬁts-all’paradigm can improve cross-lingual transfer to low-
resource languages. Our evaluation results show
that we can substantially improve the transfer per-
formance to Faroese by exploiting data and models
of closely-related high-resource languages instead
of relying on MMTs. In future work, we hope to
extend the investigations and methodology beyond
Faroese, to other low-resource languages for which
higher-resource language relatives exist.
In order to boost and guide future research on
Scandinavian languages in general and Faroese
in particular, we make the models ScandiBERT6,
ScandiBERT-no-fo7,DanskBERT8andFoBERT
(ScandiBERT-no-fo-fc3)9available. As well as the
new datasets FC310,FoNE11, and Fo-STS12.
Acknowledgments
VS is supported by the Pioneer Centre for AI,
DNRF grant number P1. The work of IV has been
supported by a personal Royal Society University
Research Fellowship (no 221137; 2022-).
We would like to thank Haukur Barri Símonar-
son for his comments on the work in its early stages.
We also thank Prof. Dr.-Ing. Morris Riedel and his
team for providing access to the DEEP cluster at
Forschungszentrum Jülich.
References
Željko Agi ´c and Ivan Vuli ´c. 2019. JW300: A wide-
coverage parallel corpus for low-resource languages.
6https://huggingface.co/vesteinn/ScandiBERT
7https://huggingface.co/vesteinn/
ScandiBERT-no-faroese
8https://huggingface.co/vesteinn/DanskBERT
9https://huggingface.co/vesteinn/FoBERT
10https://huggingface.co/datasets/vesteinn/FC3
11https://huggingface.co/datasets/vesteinn/
sosialurin-faroese-ner
12https://huggingface.co/datasets/vesteinn/
faroese-sts

--- PAGE 6 ---
InProceedings of the 57th Annual Meeting of the
Association for Computational Linguistics , pages
3204–3210, Florence, Italy. Association for Compu-
tational Linguistics.
Lars Ahrenberg, Johan Frid, and Leif-Jöran Olsson.
2020. A new resource for swedish named-entity
recognition.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020. On the cross-lingual transferability of mono-
lingual representations. In Proceedings of the 58th
Annual Meeting of the Association for Computa-
tional Linguistics , pages 4623–4637, Online. Asso-
ciation for Computational Linguistics.
James Barry, Joachim Wagner, and Jennifer Foster.
2019. Cross-lingual parsing with polyglot training
and multi-treebank learning: A Faroese case study.
InProceedings of the 2nd Workshop on Deep Learn-
ing Approaches for Low-Resource NLP (DeepLo
2019) , pages 163–174, Hong Kong, China. Associ-
ation for Computational Linguistics.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-
Gazpio, and Lucia Specia. 2017. SemEval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings
of the 11th International Workshop on Semantic
Evaluation (SemEval-2017) , pages 1–14, Vancouver,
Canada. Association for Computational Linguistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020a. Unsupervised
cross-lingual representation learning at scale. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451, Online. Association for Computational Lin-
guistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Édouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020b. Unsupervised
cross-lingual representation learning at scale. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Stian Rødven Eide, Nina Tahmasebi, and Lars Borin.
2016. The swedish culturomics gigaword corpus: A
one billion word swedish reference dataset for nlp.Fahim Faisal and Antonios Anastasopoulos. 2022.
Phylogeny-inspired adaptation of multilingual mod-
els to new languages. In Proceedings of the 2nd
Conference of the Asia-Paciﬁc Chapter of the Associ-
ation for Computational Linguistics and the 12th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 434–
452, Online only. Association for Computational
Linguistics.
Mikel L. Forcada and Francis M. Tyers. 2016. Aper-
tium: a free/open source platform for machine
translation and basic language technology. In
Proceedings of the 19th Annual Conference of
the European Association for Machine Translation:
Projects/Products , Riga, Latvia. Baltic Journal of
Modern Computing.
Yoshinari Fujinuma, Jordan Boyd-Graber, and Katha-
rina Kann. 2022. Match the script, adapt if multilin-
gual: Analyzing the effect of multilingual pretrain-
ing on cross-lingual transferability. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1500–1512, Dublin, Ireland. Association for
Computational Linguistics.
Evangelia Gogoulou, Ariel Ekgren, Tim Isbister, and
Magnus Sahlgren. 2022. Cross-lingual transfer of
monolingual models. In Proceedings of the Thir-
teenth Language Resources and Evaluation Confer-
ence, pages 948–955, Marseille, France. European
Language Resources Association.
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learning
word vectors for 157 languages. In Proceedings
of the International Conference on Language Re-
sources and Evaluation (LREC 2018) .
René Haas and Leon Derczynski. 2021. Discriminat-
ing between similar nordic languages. In Proceed-
ings of the Eighth Workshop on NLP for Similar Lan-
guages, Varieties and Dialects , pages 67–75, Kiyv,
Ukraine. Association for Computational Linguistics.
Hinrik Hafsteinsson and Anton Karl Ingason. 2021. To-
wards cross-lingual application of language-speciﬁc
PoS tagging schemes. In Proceedings of the 23rd
Nordic Conference on Computational Linguistics
(NoDaLiDa) , pages 321–325, Reykjavik, Iceland
(Online). Linköping University Electronic Press,
Sweden.
Zakaris Svabo Hansen, Heini Justinussen, and Mortan
Ólason. 2004. Marking av teldutøkum tekstsavni
[tagging of a digital text corpus].
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. Xtreme: A massively multilingual multi-
task benchmark for evaluating cross-lingual gener-
alisation. In International Conference on Machine
Learning , pages 4411–4421. PMLR.

--- PAGE 7 ---
Rasmus Hvingelby, Amalie Brogaard Pauli, Maria Bar-
rett, Christina Rosted, Lasse Malm Lidegaard, and
Anders Søgaard. 2020. DaNE: A named entity re-
source for Danish. In Proceedings of the Twelfth
Language Resources and Evaluation Conference ,
pages 4597–4604, Marseille, France. European Lan-
guage Resources Association.
Anton Karl Ingason, Eiríkur Rögnvaldsson, Einar Freyr
Sigurðsson, and Joel C. Wallenberg. 2012. Faroese
parsed historical corpus (FarPaHC) 0.1. CLARIN-
IS.
Svanhvít L. Ingólfsdóttir, Ásmundur A. Guðjónsson,
and Hrafn Loftsson. 2020. Named entity recogni-
tion for icelandic: Annotated corpus and models. In
Statistical Language and Speech Processing , pages
46–57, Cham. Springer International Publishing.
Fredrik Jørgensen, Tobias Aasmoe, Anne-Stine
Ruud Husevåg, Lilja Øvrelid, and Erik Velldal.
2020. NorNE: Annotating named entities for
Norwegian. In Proceedings of the Twelfth Lan-
guage Resources and Evaluation Conference , pages
4547–4556, Marseille, France. European Language
Resources Association.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 6282–6293, Online. Association for Computa-
tional Linguistics.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.
Per Kummervold, Freddy Wetjen, and Javier de la
Rosa. 2022. The Norwegian colossal corpus: A
text corpus for training large Norwegian language
models. In Proceedings of the Thirteenth Language
Resources and Evaluation Conference , pages 3852–
3860, Marseille, France. European Language Re-
sources Association.
Guillaume Lample, Alexis Conneau, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Interna-
tional Conference on Learning Representations .
Anne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and
Goran Glavaš. 2020. From zero to hero: On the
limitations of zero-shot language transfer with mul-
tilingual Transformers. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 4483–4499, On-
line. Association for Computational Linguistics.Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726–742.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR , abs/1907.11692.
Benjamin Minixhofer, Fabian Paischer, and Navid Rek-
absaz. 2022. WECHSEL: Effective initialization of
subword embeddings for cross-lingual transfer of
monolingual language models. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies , pages 3992–
4006, Seattle, United States. Association for Com-
putational Linguistics.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics
(Demonstrations) , pages 48–53, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Barun Patra, Joel Ruben Antony Moniz, Sarthak Garg,
Matthew R. Gormley, and Graham Neubig. 2019.
Bilingual lexicon induction with semi-supervision
in non-isometric embedding spaces. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics , pages 184–193, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James
Cross, Sebastian Riedel, and Mikel Artetxe. 2022.
Lifting the curse of multilinguality by pre-training
modular transformers. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 3479–3495, Seattle,
United States. Association for Computational Lin-
guistics.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-
bastian Ruder. 2020. MAD-X: An Adapter-Based
Framework for Multi-Task Cross-Lingual Transfer.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7654–7673, Online. Association for Computa-
tional Linguistics.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebas-
tian Ruder. 2021. UNKs everywhere: Adapting mul-
tilingual language models to new scripts. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 10186–
10203, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.

--- PAGE 8 ---
Edoardo Maria Ponti, Julia Kreutzer, Ivan Vulic, and
Siva Reddy. 2021. Modelling latent translations for
cross-lingual transfer. CoRR , abs/2107.11353.
Fabian David Schmidt, Ivan Vuli ´c, and Goran Glavaš.
2022. Don’t stop ﬁne-tuning: On training regimes
for few-shot cross-lingual transfer with multilingual
language models. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 10725–10742, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Holger Schwenk, Guillaume Wenzek, Sergey Edunov,
Edouard Grave, Armand Joulin, and Angela Fan.
2021. CCMatrix: Mining billions of high-quality
parallel sentences on the web. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers) , pages 6490–6500, Online. As-
sociation for Computational Linguistics.
Vésteinn Snæbjarnarson, Haukur Barri Símonarson,
Pétur Orri Ragnarsson, Svanhvít Lilja Ingólfsdóttir,
Haukur Jónsson, Vilhjalmur Thorsteinsson, and Haf-
steinn Einarsson. 2022. A warm start and a clean
crawled corpus - a recipe for good language mod-
els. In Proceedings of the Thirteenth Language
Resources and Evaluation Conference , pages 4356–
4366, Marseille, France. European Language Re-
sources Association.
Anders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.
2018. On the limitations of unsupervised bilingual
dictionary induction. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 778–
788, Melbourne, Australia. Association for Compu-
tational Linguistics.
Leon Strømberg-Derczynski, Manuel Ciosici, Rebekah
Baglini, Morten H. Christiansen, Jacob Aarup Dals-
gaard, Riccardo Fusaroli, Peter Juel Henrichsen,
Rasmus Hvingelby, Andreas Kirkedal, Alex Speed
Kjeldsen, Claus Ladefoged, Finn Årup Nielsen, Jens
Madsen, Malte Lau Petersen, Jonathan Hvithamar
Rystrøm, and Daniel Varab. 2021. The Danish Gi-
gaword corpus. In Proceedings of the 23rd Nordic
Conference on Computational Linguistics (NoDaL-
iDa), pages 413–421, Reykjavik, Iceland (Online).
Linköping University Electronic Press, Sweden.
Francis Tyers, Mariya Sheyanova, Aleksandra Mar-
tynova, Pavel Stepachev, and Konstantin Vinogorod-
skiy. 2018. Multi-source synthetic treebank creation
for improved cross-lingual dependency parsing. In
Proceedings of the Second Workshop on Universal
Dependencies (UDW 2018) , pages 144–150, Brus-
sels, Belgium. Association for Computational Lin-
guistics.
Ahmet Üstün, Arianna Bisazza, Gosse Bouma, Gertjan
van Noord, and Sebastian Ruder. 2022. Hyper-X:
A uniﬁed hypernetwork for multi-task multilingualtransfer. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 7934–7949, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Xinyi Wang, Sebastian Ruder, and Graham Neubig.
2022. Expanding pretrained models to thousands
more languages via lexicon-based adaptation. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 863–877, Dublin, Ireland. As-
sociation for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations , pages 38–45, Online. Asso-
ciation for Computational Linguistics.
Shijie Wu and Mark Dredze. 2020. Are all languages
created equal in multilingual BERT? In Proceedings
of the 5th Workshop on Representation Learning for
NLP, pages 120–130, Online. Association for Com-
putational Linguistics.
Haoran Xu and Kenton Murray. 2022. Por qué não
utiliser alla språk? mixed training with gradient
optimization in few-shot cross-lingual transfer. In
Findings of the Association for Computational Lin-
guistics: NAACL 2022 , pages 2043–2059, Seattle,
United States. Association for Computational Lin-
guistics.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021. mT5: A massively
multilingual pre-trained text-to-text transformer. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 483–498, Online. Association for Computa-
tional Linguistics.
Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vuli ´c,
Roi Reichart, Anna Korhonen, and Hinrich Schütze.
2021. A closer look at few-shot crosslingual trans-
fer: The choice of shots matters. In Proceedings of
the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 5751–5767, Online.
Association for Computational Linguistics.
A Training of language models
We train new BPE vocabularies for all the new mod-
els we train, ScandiBERT, ScandiBERT-no-fo, and

--- PAGE 9 ---
DanskBERT. All models use the same vocabulary
size of 50k. The ScandiBERT vocabulary is trained
using all the languages, the ScandiBERT-no-fo vo-
cabulary is trained without the Faroese data, and
the DanskBERT vocabulary is only trained on the
Danish text. V ocabularies are trained using the Sen-
tencePiece software (Kudo and Richardson, 2018),
and character coverage is set to 99.995 %.
Pre-training of the new language models is
done using fairseq (Ott et al., 2019) using the
RoBERTa-base (Liu et al., 2019) conﬁguration,
ﬁne-tuning is done using the transformers (Wolf
et al., 2020) library. ScandiBERT and ScandiBERT-
no-fo were trained for 72 epochs, using a batch
size of 8.8k sequences on 24 NVIDIA V100 cards
for approximately 14 days each. Initial testing
showed that the larger batch size showed better
performance than going for around 2k sequences,
possibly due to the mixture of differing languages.
DanskBERT, on the other hand, similar to IceBERT
and RoBERTa showed better performance at the
smaller batch size. DanskBERT was trained to con-
vergence for 500k steps using 16 V100 cards for
approximately 14 days.
All-fcmodels are further trained for 50 epochs,
with an effective batch size of 100k tokens for 12k
updates, over the FC3 dataset for Faroese adapta-
tion.
An overview of the data used to train the lan-
guage models is shown in Table 2. For details on
the Icelandic data, we refer to (Snæbjarnarson et al.,
2022). For the other datasets, we refer to §2.
B Wechsel results
We compare our method to another transfer
learning approach presented by Minixhofer
et al. (2022). The FC3 dataset is used to train
fastText embeddings for Faroese, and the Icelandic
datasets are used to train fastText embeddings for
Icelandic. These embeddings are then used to
convert the multilingual models to Faroese using
Language Datasets Size
Icelandic IGC / IC3 / Skemman / Hirslan 16 GB
Danish Danish Gigaword Corpus (incl.
Twitter)4,7 GB
Norwegian NCC corpus 42 GB
Swedish Swedish Gigaword Corpus 3,4 GB
Faroese FC3 + Sosialurinn + Bible 69 MB
Table 2: Datasets used to train ScandiBERT,
ScandiBERT-no-fo and DanskBERTthe Wechsel approach. We conﬁrm the quality of
the Icelandic embeddings by running an Icelandic
semantic evaluation suite adapted from https:
//github.com/stofnun-arna-magnussonar/
ordgreypingar_embeddings , showing our
embeddings are comparable or of higher quality
than those released by Meta (Grave et al., 2018).
The experiments in Table 3 all show sub-par per-
formance compared to the results in non-Wechsel
results in Table 1. The Wechsel work considers
transfer from English-dominant models, GPT2 and
RoBERTa to French, German, Chinese, Swahili,
Sundanese, Scottish Gaelic, Uyghur and Malagasy.
None of which are closely related to English. One
reason for the discrepancy in the results could
be that the shufﬂing of the embedding matrix to
convert it is more catastrophic when considering
close languages. Another reason could be that both
Faroese and Icelandic are morphologically rich and
that all variants of the words were not properly
mapped during the conversion of the embedding
matrix.
C Mapping NER datasets
The datasets used to create a Scandinavian NER-
corpus are DaNE (Danish), FoNE (Faroese), MIM-
GOLD-NER (Icelandic), NorNE (Norwegian), and
SWE-Nerc (Swedish), presented in §2. The results
in Table 4 show that the best result is obtained when
training directly from the IceBERT model. The
ScandiBERT model has a higher variance when
pre-ﬁne-tuned on the combined NER corpora. This
approach could also be made directly for the UD
corpus, POS (in particular, using the Icelandic POS
data), and other corpora as they become available
for training or evaluation in Faroese. This demon-
strates how resources from a related language can
substantially beneﬁt a low-resource language.
To combine the NER datasets, we map the tags
to the CoNLL schema used by the Icelandic MIM-
GOLD-NER and the Faroese FoNE datasets. The
Danish DaNE dataset uses a subset of the tags used
for Icelandic and Faroese, so the mapping is purely
nominal. The mapping for Norwegian (NorNE)
and Swedish (SweNERC) datasets is shown in Ta-
ble 5.
D Prior Work on Transfer learning for
Faroese
We know of three works that consider transfer learn-
ing for Faroese from the Scandinavian languages.

--- PAGE 10 ---
POS NER UD FP UD oft STS
Model F1 Acc. F1 Acc. F1 Acc. F1 Acc. Acc.WechselIceBERT 74.40.16 75.7 0.16 67.71.2 98.7 0.06 83.60.35 84.6 0.38 66.69.01 75.7 5.88 27.33.9
IceBERT-fc3 89.00.06 89.4 0.09 88.50.47 96.4 0.09 96.30.03 96.5 0.03 95.60.28 96.2 0.25 67.72.8
XLM-R 68.90.16 73.5 0.13 59.70.92 99.0 0.06 81.00.19 84.5 0.13 71.80.73 79.7 0.44 11.44.6
XLM-R-fc3 86.80.09 88.7 0.09 88.80.41 98.4 0.06 96.30.03 96.7 0.03 95.60.25 96.5 0.19 65.72.8
ScandiBERT-no-fo 71.30.16 72.5 0.16 65.10.54 98.8 0.03 82.00.19 83.4 0.19 75.00.66 81.0 0.57 29.44.8
ScandiBERT-n.f.-fc3 89.20.06 89.6 0.06 89.20.54 99.0 0.03 96.80.06 97.1 0.06 96.10.28 96.8 0.22 74.71.0
ScandiBERT 72.60.28 73.8 0.28 65.70.54 98.8 0.03 83.00.38 84.0 0.28 76.80.51 82.5 0.35 8.75.3
ScandiBERT-fc3 89.30.09 89.7 0.09 88.80.54 98.7 0.06 96.80.03 97.1 0.03 96.00.25 96.7 0.25 53.66.0
Table 3: Results for all downstream tasks using different base language models after Wechsel adaptation,
with and without continued Faroese pre-training. The results are signiﬁcantly worse than without Wechsel
adaptations.
Model Pre-ft. Ft. F1 Acc.
SB-no-fo-fc3 None Yes 91.4 0.35 98.8 0.06
ScandiBERT Icel. Yes 92.00.32 98.80.06
ScandiBERT All No 91.5 0.51 98.9 0.06
ScandiBERT All Yes 91.8 0.51 99.0 0.06
XLM-R All No 90.6 0.19 99.0 0.03
XLM-R All Yes 90.8 0.47 99.0 0.06
Table 4: NER performance when models are pre-
ﬁnetuned on all Scandinavian datasets and then
ﬁne-tuned on FoNER.
Language Original Mapped
Norwegian O O
Norwegian PER Person
Norwegian ORG Organization
Norwegian GPE_LOC Location
Norwegian PROD Miscellaneous
Norwegian LOC Location
Norwegian GPE_ORG Organization
Norwegian DRV O
Norwegian EVT Miscellaneous
Norwegian MISC Miscellaneous
Swedish O O
Swedish EVN Miscellaneous
Swedish GRO Organization
Swedish LOC Location
Swedish MNT Miscellaneous
Swedish PRS Person
Swedish TME Time
Swedish WRK Miscellaneous
Swedish SMP Miscellaneous
Table 5: Mapping of tags to create a uniﬁed NER
dataset for the Scandinavian languages.In (Tyers et al., 2018), a rule-based translation sys-
tem (Apertium (Forcada and Tyers, 2016)) is used
to translate the Faroese Wikipedia into Swedish,
Norwegian Bokmål, and Norwegian Nynorsk. The
translations are then aligned, and the translations
dependency-parsed. The resulting trees are then
mapped to the original Faroese sentences and used
for POS-tagging and annotating morphological fea-
tures. The second work is a mapping between
Faroese and Icelandic POS-tags (Hafsteinsson and
Ingason, 2021); while not a direct application,
the authors suggest the mapping may be of use
for transfer learning between the languages. Fi-
nally, (Barry et al., 2019) use machine translation
and dependency parsing for cross-lingual syntactic
knowledge transfer from Danish, Norwegian, and
Swedish to Faroese.
