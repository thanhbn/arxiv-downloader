# 2405.19327.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2405.19327.pdf
# Kích thước tệp: 3872289 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MAP-Neo: Chuỗi Mô hình Ngôn ngữ Lớn Song ngữ
Có Khả năng Cao và Minh bạch
M-A-P, Đại học Waterloo, Wuhan AI Research, 01.AI
https://map-neo.github.io/
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) đã đạt được những bước tiến vượt bậc trong những năm gần đây để đạt được hiệu suất chưa từng có trên các nhiệm vụ khác nhau. Tuy nhiên, do lợi ích thương mại, các mô hình cạnh tranh nhất như GPT, Gemini và Claude đã bị giới hạn sau các giao diện độc quyền mà không tiết lộ chi tiết đào tạo. Gần đây, nhiều tổ chức đã mở mã nguồn một số LLMs mạnh như LLaMA-3, có thể so sánh với các LLMs mã nguồn đóng hiện có. Tuy nhiên, chỉ có trọng số của mô hình được cung cấp với hầu hết các chi tiết không được tiết lộ (ví dụ: checkpoints trung gian, corpus tiền huấn luyện và mã đào tạo, v.v.). Để cải thiện tính minh bạch của LLMs, cộng đồng nghiên cứu đã thành lập để mở mã nguồn các LLMs thực sự mở (ví dụ: Pythia, Amber, OLMo), nơi nhiều chi tiết hơn (ví dụ: corpus tiền huấn luyện và mã đào tạo) đang được cung cấp. Các mô hình này đã thúc đẩy đáng kể việc nghiên cứu khoa học về các mô hình lớn này bao gồm điểm mạnh, điểm yếu, thiên kiến và rủi ro của chúng. Tuy nhiên, chúng tôi quan sát thấy rằng các LLMs thực sự mở hiện có vẫn kém hơn các LLMs tiên tiến hiện tại với kích thước mô hình tương tự về các nhiệm vụ lý luận, kiến thức và lập trình. Để giải quyết vấn đề này, chúng tôi mở mã nguồn MAP-Neo, một mô hình ngôn ngữ song ngữ có khả năng cao và minh bạch với 7B tham số được đào tạo từ đầu trên 4.5T tokens chất lượng cao. MAP-Neo của chúng tôi là LLM song ngữ hoàn toàn mở mã nguồn đầu tiên với hiệu suất có thể so sánh với các LLMs tiên tiến hiện tại. Hơn nữa, chúng tôi mở mã nguồn tất cả chi tiết để tái tạo MAP-Neo của chúng tôi, nơi corpus tiền huấn luyện đã được làm sạch, pipeline làm sạch dữ liệu, checkpoints và framework đào tạo/đánh giá được tối ưu hóa tốt được cung cấp. Cuối cùng, chúng tôi hy vọng MAP-Neo của chúng tôi sẽ tăng cường và củng cố cộng đồng nghiên cứu mở và truyền cảm hứng cho nhiều đổi mới và sáng tạo hơn để tạo điều kiện cho các cải tiến tiếp theo của LLMs.

Hình 1: MAP-Neo cho thấy hiệu suất ấn tượng trên các mô hình cơ sở (Trái) và chat (Phải) so với cả các mô hình ngôn ngữ lớn mở trọng số phổ biến và minh bạch gần đây với kích thước tương tự.

--- TRANG 2 ---
Mục lục
1 Giới thiệu 4
2 Công trình Liên quan 5
3 Tokenizer 5
4 Matrix Data Pile 6
4.1 Pipeline Xử lý lại cho Datasets Mở . . . . . . . . . . . . . . . . . . . . . . . 7
4.1.1 Lọc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.1.2 Khử trùng lặp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Pipeline Thu thập Corpus từ Đầu . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2.1 Lọc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2.2 Khử trùng lặp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2.3 Khử trùng lặp Dòng Tương tự . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 Pipeline Chuyển đổi Tài liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.4 Thu thập Dữ liệu Bổ sung Chất lượng Cao . . . . . . . . . . . . . . . . . . . . 12
5 Mô hình 13
5.1 Kiến trúc Mô hình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.2 Hyperparameters Quy mô Mô hình . . . . . . . . . . . . . . . . . . . . . . . . 13
6 Tiền huấn luyện 13
6.1 Giai đoạn Cơ bản: Thu nhận Khả năng Tổng quát . . . . . . . . . . . . . . . . 14
6.2 Giai đoạn Suy giảm: Cải thiện và Chỉnh sửa . . . . . . . . . . . . . . . . . . . 15
7 Alignment 15
7.1 Tinh chỉnh có Giám sát . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7.1.1 Dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7.1.2 Đào tạo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7.2 DPO Lặp lại . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
8 Định luật Mở rộng của MAP-Neo 16
8.1 Định nghĩa Vấn đề . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
8.2 Định luật Mở rộng NEO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
8.3 Tổng quát hóa Định luật Mở rộng NEO . . . . . . . . . . . . . . . . . . . . . . 18
9 Hạ tầng 19
10 Đánh giá 20
10.1 Hiệu suất Mô hình Cơ sở . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
10.1.1 Kết quả Chính . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

--- TRANG 3 ---
10.1.2 Thảo luận . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
10.2 Hiệu suất Mô hình được Alignment . . . . . . . . . . . . . . . . . . . . . . . . 22
10.2.1 Kết quả Chính . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
10.2.2 Thảo luận . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
11 Tác động Xã hội 23
12 Kết luận 24
13 Đóng góp và Lời cảm ơn 24
14 Multimodal Art Projection 25
A Phụ lục 36
A.1 Chi tiết về Quy tắc Heuristic cho Văn bản Tiếng Anh . . . . . . . . . . . . . . . 36
A.2 Chi tiết về Quy tắc Heuristic cho Văn bản Tiếng Trung . . . . . . . . . . . . . . 37
A.3 Chi tiết Overflow Framework Đào tạo . . . . . . . . . . . . . . . . . . . . . . . 38
A.4 Prompts Chi tiết trong Đánh giá Checkpoints Trung gian . . . . . . . . . . . . . 38
A.5 Kết quả Chi tiết . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
A.6 Chi tiết về Datasets Mã nguồn Mở được Sử dụng trong Tiền huấn luyện . . . . . 46
A.7 Tỷ lệ Nén Chi tiết . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
A.8 Kết quả Thí nghiệm Bổ sung trong Định luật Mở rộng . . . . . . . . . . . . . . 46
A.9 Tỷ lệ Nén . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
A.10 Xử lý Hậu OCR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

--- TRANG 4 ---
1 Giới thiệu
Sự ra đời của các mô hình ngôn ngữ lớn tổng quát (LLMs) như GPT-4 [1], Claude [4] và Gemini [80] đã mở rộng đáng kể ranh giới của Xử lý Ngôn ngữ Tự nhiên (NLP) và đang mở đường hướng tới Trí tuệ Nhân tạo Tổng quát (AGI). Các mô hình này thể hiện khả năng toàn cầu, bao gồm lý luận phức tạp [116,89], nhập vai [107], viết sáng tạo [105], đánh giá tâm lý [112], giáo dục khoa học [18] và tạo nhạc [115,75,29], cùng nhiều khả năng khác. Tuy nhiên, những mô hình tiên tiến nhất vẫn giữ mã nguồn đóng do lợi ích thương mại [1,4,80]. Trong bài báo này, chúng tôi lập luận rằng các LLMs mã nguồn mở và minh bạch là thiết yếu cho cả việc dân chủ hóa LLMs và nghiên cứu học thuật tiếp theo, đặc biệt xem xét những tài nguyên đáng kể mà các mô hình này tiêu thụ.

Các công trình trước đây đã phát hành nhiều LLMs mã nguồn mở hoặc thậm chí minh bạch. Ví dụ, chuỗi LLaMA [101,102,3] đã phát hành các trọng số, từ đó thúc đẩy đáng kể sự phát triển của cộng đồng LLM mã nguồn mở. Tuy nhiên, chúng không minh bạch vì không tiết lộ chi tiết về dữ liệu đào tạo. BLOOM [86] đã đào tạo một mô hình ngôn ngữ đa ngữ với 176 tỷ tham số và mở mã nguồn trọng số mô hình, checkpoints trung gian và corpus đào tạo. Các mô hình như LLM360 [66] và Pythia [9] còn cung cấp thêm mã đào tạo, checkpoints trạng thái optimizer, mã phân tích và pipelines dữ liệu.

Các mô hình này đóng góp đáng kể vào việc xây dựng hệ sinh thái minh bạch, nhưng nói chung vẫn thua kém các LLMs cấp công nghiệp như LLaMA [3], Mistral [48] và Yi [113], v.v. OLMo [36] đã có bước tiến lớn trong việc thu hẹp khoảng cách này bằng cách cải thiện dữ liệu tiền huấn luyện và pipelines xử lý dữ liệu, và giới thiệu nhiều thành phần mã nguồn mở hơn, bao gồm logs đào tạo và ablations. Tuy nhiên, nó vẫn kém thành thạo hơn, đặc biệt trong các lĩnh vực như lập trình (HumanEval [15]), lý luận (MATH [41], GSM8K [23]), kiến thức (MMLU [40]) và đa ngữ (CMMLU [60]).

Để khắc phục những vấn đề này, chúng tôi giới thiệu MAP-Neo, một bộ LLM song ngữ hoàn toàn mã nguồn mở và minh bạch đạt hiệu suất vượt trội để thu hẹp khoảng cách với các mô hình mã nguồn đóng. Cụ thể, toàn bộ quy trình xây dựng một LLM bao gồm:

1. Pipeline Curation Dữ liệu: Chúng tôi cung cấp mã cho việc curation và làm sạch dữ liệu đào tạo (cả tiếng Anh và tiếng Trung), bao gồm hệ thống OCR ổn định, cơ chế recall dữ liệu trong DeepSeek-Math [89], tích hợp các pipelines xử lý dữ liệu mã nguồn mở trước đó, và hỗ trợ xử lý dữ liệu phân tán dựa trên Spark, cùng nhiều thứ khác.

2. Dữ liệu: Chúng tôi phát hành corpus tiền huấn luyện của chúng tôi, cụ thể là Matrix Data Pile, cùng với dữ liệu đào tạo cho tinh chỉnh có giám sát và đào tạo alignment.

3. Kiến trúc Mô hình: Chúng tôi cung cấp mã và chi tiết về kiến trúc mô hình của chúng tôi.

4. Đào tạo Mô hình: Chúng tôi cung cấp mã đào tạo cho tokenizer, mô hình cơ sở, mô hình được tinh chỉnh theo hướng dẫn và mô hình được alignment. Ngoài ra, chúng tôi giải quyết một số vấn đề của framework Megatron-LM, tăng cường hỗ trợ cho đào tạo phân tán mạnh mẽ và hiệu quả hơn. Hơn nữa, chúng tôi giới thiệu Định luật Mở rộng NEO được thiết kế để tối ưu hóa việc mở rộng LLMs bằng cách sử dụng dataset tiền huấn luyện có nguồn từ các corpus đa dạng.

5. Checkpoints Mô hình: Chúng tôi không chỉ phát hành các mô hình cuối cùng trên HuggingFace mà còn cung cấp các checkpoints trung gian để tái tạo.

6. Hạ tầng: Báo cáo này chi tiết hạ tầng cho đào tạo ổn định.

7. Đánh giá: Chúng tôi cũng cung cấp mã đánh giá chi tiết và cài đặt đánh giá toàn diện để benchmark hiệu suất của LLMs.

8. Phân tích và Bài học: Báo cáo này trình bày nhiều kỹ thuật và công thức, như các thủ thuật tối ưu hóa ở các giai đoạn khác nhau của tiền huấn luyện, và cung cấp thông tin chi tiết về việc xây dựng LLMs thông qua phân tích và ablations nghiêm ngặt.

Công trình của chúng tôi là một cột mốc hướng tới các LLMs hoàn toàn minh bạch với khả năng tiên tiến, thậm chí cạnh tranh với các LLMs mã nguồn đóng hàng đầu. Đáng chú ý, đóng góp của chúng tôi không chỉ là một mô hình nền tảng mới mà còn là một cẩm nang toàn diện để xây dựng LLMs từ đầu, bao gồm toàn bộ quy trình. Chúng tôi tin rằng mô hình của chúng tôi cung cấp một tham chiếu quan trọng cho cộng đồng, đặc biệt cho các khu vực không nói tiếng Anh trên thế giới tham gia nghiên cứu LLM.

--- TRANG 5 ---
2 Công trình Liên quan

Bảng 1: So sánh với các mô hình ngôn ngữ lớn mã nguồn mở khác (LLMs). Tất cả các metric được thu thập bằng cách đánh giá tương tự, và chi tiết được hiển thị trong Bảng 9. Các mô hình không minh bạch được liệt kê phía trên đường gạch ngang, trong khi các LLMs minh bạch được hiển thị bên dưới.

[THIS IS TABLE: Comparison table showing various LLM models with columns for Intermediate Checkpoints, Pre-training Corpus, Reproduction Code, Data Cleaning Process, C-EVAL, MMLU, GSM8K, and HumanEval scores]

Sự phát triển của các mô hình ngôn ngữ lớn mã nguồn mở (LLMs) là then chốt cho việc thúc đẩy nghiên cứu và ứng dụng trí tuệ nhân tạo. Những nỗ lực gần đây trong lĩnh vực này đã tập trung không chỉ vào việc nâng cao hiệu suất mô hình [48,3] mà còn đảm bảo tính minh bạch và khả năng tái tạo [9,66,36,128]. Mô hình MAP-Neo-7B của chúng tôi nổi lên như một bước dẫn đầu mới trong bối cảnh đang phát triển này, như được hiển thị trong Bảng 1, cân bằng giữa hiệu suất và tính minh bạch.

Chuỗi mô hình MAP-Neo đại diện cho một bước tiến trong việc nhấn mạnh tính minh bạch hoàn toàn, sắp xếp nó cùng với các mô hình đương đại khác như Mistral [48], LLaMA3 [3], Pythia [9], Amber [66] và OLMo [36]. Không giống như các mô hình này, thường thiếu checkpoints trung gian, quy trình làm sạch dữ liệu toàn diện, hoặc corpus tiền huấn luyện và mã tái tạo có thể truy cập, MAP-Neo xuất sắc bằng cách tích hợp tất cả các yếu tố này. Cam kết về tính mở của MAP-Neo này tạo điều kiện cho phân tích sâu và xác thực độc lập bởi cộng đồng nghiên cứu.

Về hiệu suất, MAP-Neo-7B thể hiện khả năng vượt trội trên phạm vi rộng các benchmark bao gồm hiểu biết tiếng Trung và tiếng Anh trên C-EVAL [46] và MMLU [20], khả năng toán học trên GSM8K [23] và MATH [41], và khả năng lập trình trên HumanEval [15]. Đáng chú ý, MAP-Neo-7B là mô hình duy nhất trong phân tích so sánh của chúng tôi đạt được tất cả các tiêu chí về tính minh bạch, cũng như điểm số cao nhất trên tất cả các bài kiểm tra so với các LLMs minh bạch khác, nhấn mạnh hiệu quả của việc đào tạo và chất lượng dữ liệu.

Công trình tương tự nhất với MAP-Neo là OLMo [36], đây là công trình tiên phong trong việc hoàn toàn mở mã nguồn LLMs. Tuy nhiên, hiệu suất của họ bị ảnh hưởng trong một số khía cạnh như kiến thức, lập trình và lý luận toán học. Hơn nữa, OLMo không thể xử lý các ngôn ngữ ngoài tiếng Anh. MAP-Neo đặt ra tiêu chuẩn mới cho tính minh bạch và hiệu suất trong lĩnh vực LLMs mã nguồn mở. Bằng cách thúc đẩy quy trình phát triển hoàn toàn minh bạch, MAP-Neo không chỉ tăng cường tính hữu ích và đáng tin cậy mà còn cung cấp framework có giá trị cho nghiên cứu tương lai, thúc đẩy những tiến bộ và nỗ lực hợp tác tiếp theo trong cộng đồng.

3 Tokenizer

Chúng tôi đào tạo tokenizer của mình sử dụng thuật toán byte-pair encoding (BPE) [88] thông qua implementation của SentencePiece [56]. Dữ liệu đào tạo bao gồm 50B mẫu từ corpus tiền huấn luyện, và độ dài tối đa được cắt ở 64K. Chúng tôi gán trọng số lấy mẫu cao hơn cho code, toán học và dữ liệu học thuật chất lượng cao. Để cân bằng hiệu quả tính toán và hiệu suất mô hình, chúng tôi đề xuất đặt kích thước từ vựng là 64000 và ràng buộc độ dài max sentence-piece là 16 để cải thiện hiệu suất tiếng Trung.

Đáng chú ý, chúng tôi cắt tất cả các số thành các chữ số riêng lẻ và fallback các ký tự UTF-8 không xác định về granularity byte. Chúng tôi không sử dụng bất kỳ chiến lược normalization nào trên các mẫu đào tạo và không thêm dummy prefixes. Tỷ lệ coverage ký tự được đặt là 0.9999. Đặc biệt, tham số remove extra whitespaces được đặt là False, được bật mặc định trong SentencePieceTrainer. Cài đặt này có thể ảnh hưởng nghiêm trọng đến hiệu suất code trong quá trình tiền huấn luyện, vì việc thụt lề code bình thường được xử lý như một khoảng trắng đơn. Chúng tôi gặp phải vấn đề cụ thể trong giai đoạn đầu của việc tiền huấn luyện mô hình. Ban đầu, chúng tôi không tắt tham số 'remove extra whitespaces', được bật mặc định trong SentencePieceTrainer. Trong quá trình đào tạo, chúng tôi quan sát thấy cải thiện ổn định trong các benchmark lý luận QA và toán học, nhưng các metric code có biến động và không cho thấy cải thiện như mong đợi. Để giải quyết vấn đề này, chúng tôi đã sửa lỗi này trong giai đoạn thứ hai của đào tạo (§6.2), giúp ổn định và cải thiện đáng kể các metric code. Hơn nữa, chúng tôi quan sát thấy rằng vấn đề này được giải quyết tốt trong các giai đoạn đào tạo giai đoạn suy giảm dưới cài đặt tokenizer mới, nơi những cải thiện nhanh chóng được đạt được.

Hơn nữa, chúng tôi cũng điều tra tỷ lệ nén trên các loại dữ liệu khác nhau, được phân loại theo cả ngôn ngữ (tiếng Trung và tiếng Anh) và chất lượng nguồn dữ liệu (chất lượng cao và từ web) như được hiển thị trong Bảng 2. Cụ thể, đầu tiên, chúng tôi quan sát thấy rằng dữ liệu chất lượng cao (HQ) bao gồm lý luận phức tạp, toán học và văn bản kiến thức tổng quát, cho thấy tỷ lệ nén khác nhau giữa tiếng Trung (HQ cn) và tiếng Anh (HQ en). Loại HQ cn có tỷ lệ nén 1.577, trong khi loại HQ en thể hiện tỷ lệ cao hơn là 3.311 ký tự trên token. Thứ hai, dữ liệu có nguồn từ web (Web) cũng bao gồm nhiều ký tự hơn so với tiếng Trung. Điều này gợi ý sự biến thiên đáng kể trong hiệu quả tokenization hoặc sử dụng ký tự giữa các ngôn ngữ, có thể do cấu trúc ngôn ngữ và phương pháp tokenization. Thứ ba, cần đề cập rằng ngay cả với tỷ lệ nén tương tự, cài đặt của tokenizer có thể gây ra biến động đáng kể trong quá trình tiền huấn luyện. Do đó, vẫn cần thiết phải điều tra thêm các chiến lược tokenization cho các tình huống sử dụng tiếp theo.

Bảng 2: Tỷ lệ Nén Trung bình theo Loại. Các tập con này không được tỷ lệ đồng đều trong tập đào tạo. Phân phối chi tiết được hiển thị trong Phụ lục 18.

[THIS IS TABLE: Shows compression rates for different categories: Code (2.951), HQ cn (1.577), HQ en (3.311), Web cn (1.418), Web en (3.699), Others (2.558)]

4 Matrix Data Pile

Hình 2: Thống kê Phân phối Dữ liệu Matrix Pile: Biểu đồ tròn bên trong đại diện cho phân phối ngôn ngữ, trong khi vòng ngoài chỉ ra tỷ lệ các meta-categories trong corpus.

Được công nhận rộng rãi rằng một corpus đào tạo được xây dựng tốt là thiết yếu cho việc đào tạo LLMs. Corpus đào tạo phục vụ như nhiên liệu thúc đẩy những tiến bộ trong mô hình hóa ngôn ngữ, như được chứng minh bởi các khả năng nổi lên của các mô hình như ChatGPT, Claude, Gemini và Llama. Tuy nhiên, do hạn chế về sở hữu trí tuệ, dữ liệu tiền huấn luyện và bộ công cụ xử lý của các LLMs (một phần) độc quyền này không được tiết lộ khi phát hành. Mặc dù cộng đồng nghiên cứu mã nguồn mở đã nỗ lực đáng kể để tăng tính minh bạch trong pipeline thu thập và xử lý dữ liệu tiền huấn luyện mô hình ngôn ngữ [9,86,95], việc phát triển các LLMs hoàn toàn mã nguồn mở vẫn thua kém các LLMs độc quyền ở một mức độ nào đó, chủ yếu do khoảng cách về số lượng và chất lượng của các datasets.

Để giải quyết nhu cầu cấp thiết về các datasets đa dạng và minh bạch hơn trong mô hình hóa ngôn ngữ, chúng tôi giới thiệu Matrix, một corpus tiền huấn luyện song ngữ gồm 4.5T tokens. Khi phát hành, Matrix có thể là corpus tiền huấn luyện LLM minh bạch lớn nhất theo hiểu biết tốt nhất của chúng tôi. Cụ thể, Matrix cung cấp chi tiết về thu thập và xử lý dữ liệu cùng với bộ công cụ hiệu suất cao. Ngoài ra, chúng tôi thiết kế Matrix dựa trên ý tưởng retrieve, lọc và làm sạch dữ liệu chất lượng cao dưới các hoàn cảnh thực tế khác nhau, được thảo luận như sau:

• Với một tập hợp các datasets tiền huấn luyện (tiếng Anh) hiện có, làm thế nào để chúng ta xử lý lại và cải thiện chất lượng? §4.1
• Làm thế nào để chúng ta xây dựng một corpus quy mô lớn, toàn diện chủ đề từ đầu, về nội dung tiếng Trung ít được khám phá? §4.2
• Nếu chúng ta có nhiều tài liệu in khổng lồ, làm thế nào để chúng ta xây dựng một hệ thống hiệu quả và hiệu quả để trích xuất nội dung văn bản khả thi? §4.3
• Khi chỉ định một lĩnh vực quan tâm, làm thế nào để chúng ta tìm dữ liệu chất lượng cao có liên quan từ nội dung web hoang dã? §4.4

Thành phần cuối cùng của corpus như sau: 52.55% từ Common Crawl, 22.29% từ mã lập trình, và phần còn lại từ các bài báo học thuật, sách và các tài liệu in khác, như được minh họa trong Hình 2. Các phương pháp chi tiết để xử lý các nguồn này được mô tả trong các phần tiếp theo, và một minh họa toàn diện về các nguồn được cung cấp trong Bảng 16.

Bảng 3: Thành phần nguồn của tập con web tiếng Anh được xử lý lại. Tỷ lệ biểu thị việc chia kích thước của dataset hiện tại cho tổng kích thước của toàn bộ dataset.

[THIS IS TABLE: Shows composition of reprocessed English web subset with datasets, parts, UTF-8 bytes, availability, and proportions]

4.1 Pipeline Xử lý lại cho Datasets Mở

Mặc dù một số corpus tiền huấn luyện đã được xử lý (chủ yếu bằng tiếng Anh) đã được phát hành bởi các công trình trước đây [95,74], chúng tôi lập luận rằng vẫn còn chỗ cho một pipeline được thiết kế tỉ mỉ hơn để cải thiện dữ liệu hiện có. Bên cạnh đó, cần đề cập rằng các LLMs hiện có có thể được cải thiện dễ dàng bằng việc tiếp tục tiền huấn luyện với dữ liệu chất lượng cao. Do đó, chúng tôi xử lý thêm các corpus dựa trên nội dung web đã chọn để tạo ra tập con tiếng Anh của hỗn hợp dữ liệu Matrix. Nguồn đến từ các phần Head và Middle của RedPajama-Data-V2 [25], phần CC của Dolma [95], phần EN của Cultrax [72], phần Refined-Web của Amber [66], SlimPajama [94] và falcon [74]. Phân phối chính xác của dataset tiếng Anh của chúng tôi được liệt kê trong Bảng 3. Quy trình bao gồm lọc và khử trùng lặp nhiều bước. Sơ đồ trong Hình 3a hiển thị thứ tự xử lý và tỷ lệ giữ lại.

4.1.1 Lọc

Để lọc thêm corpus chất lượng tương đối thấp từ các datasets mã nguồn mở, chúng tôi đề xuất sử dụng quy tắc heuristic để lọc văn bản. Các quy tắc này được thiết kế để xác định và loại bỏ dữ liệu chất lượng kém, từ đó ngăn chặn sự suy giảm hiệu suất mô hình tiềm năng do corpus tiền huấn luyện có lỗi. Vì dataset tổng hợp của chúng tôi được tạo thành từ các corpus từ nhiều nguồn, chúng tôi áp dụng các phương pháp làm sạch được thiết kế tốt [74, 14, 76, 78] và điều chỉnh quy tắc của chúng tôi cho từng nguồn để đảm bảo tính nhất quán về chất lượng.

Đối với dataset RedPajama-Data-v2 [25], cung cấp chú thích chất lượng cho mỗi văn bản, chúng tôi tích hợp quy tắc heuristic của mình với các chú thích này để tinh chỉnh đánh giá chất lượng dữ liệu và thực hiện lấy mẫu ngẫu nhiên trên dataset để xác nhận ngưỡng cho mỗi quy tắc. Đối với các datasets thiếu chú thích chất lượng, chúng tôi áp dụng các quy tắc và ngưỡng đã thiết lập được rút ra từ RedPajama-V2, đồng thời tùy chỉnh chúng để phù hợp với đặc điểm độc đáo của mỗi dataset. Ví dụ, dataset Dolma [95] bao gồm sáu tập con, cụ thể là Wikipedia, PeS2o, Stack Code, Gutenberg, C4 và CC, mỗi tập có đặc điểm dữ liệu khác nhau. Với đặc điểm độc đáo của mỗi tập con, chúng tôi tiến hành lấy mẫu và đánh giá riêng để đảm bảo rằng các sửa đổi trong quy tắc và ngưỡng phù hợp với yêu cầu lọc của chúng tôi. Cụ thể, đối với tập con CC, chúng tôi điều chỉnh ngưỡng từ duy nhất và độ dài văn bản. Đối với tập con Gutenberg, chủ yếu chứa văn bản sách, chúng tôi chỉ áp dụng một số quy tắc để tránh quá trình tốn thời gian thực hiện kiểm tra heuristic mở rộng trên văn bản dài.

Quá trình lọc bao gồm: 1) Lọc cấp tài liệu và cấp câu để đảm bảo độ dài văn bản phù hợp, tính có ý nghĩa của ký tự và tính nhất quán; 2) Loại bỏ văn bản trùng lặp, bao gồm n-grams và câu; 3) Kiểm tra từ nhạy cảm để loại bỏ văn bản chứa bất kỳ thuật ngữ nào từ blacklist.

4.1.2 Khử trùng lặp

Đã được báo cáo rằng văn bản lặp lại có thể dẫn đến sự suy giảm hiệu suất mô hình [58,51,42], làm cho khử trùng lặp trở thành bước quan trọng trong xử lý corpus. Bằng cách loại bỏ các bản sao, chúng tôi có thể giảm đáng kể tỷ lệ memorization được phát ra và làm cho việc đào tạo mô hình hiệu quả hơn [58]. Sự lặp lại có thể được phân loại thành bản sao chính xác và bản sao gần. Đối với bản sao chính xác, chúng tôi sử dụng khử trùng lặp tài liệu chính xác để loại bỏ chúng. Đối với bản sao gần, chúng tôi sử dụng khử trùng lặp Minhash LSH để loại bỏ chúng nhiều nhất có thể. Ngoài ra, có những trường hợp mà các phần của văn bản hoàn toàn trùng lặp, và trong những trường hợp này, phương pháp Minhash gặp khó khăn trong việc loại bỏ chúng. Để giải quyết điều này, chúng tôi đã áp dụng hai phương pháp để loại bỏ một phần nội dung như vậy: khử trùng lặp đoạn văn và khử trùng lặp substring chính xác.

Khử trùng lặp Tài liệu Chính xác Khử trùng lặp tài liệu chính xác là một phương pháp được sử dụng để đánh giá toàn bộ văn bản để xác định xem nó có giống hệt với văn bản khác hay không. Nếu được phát hiện là hoàn toàn giống nhau, bản sao sẽ được loại bỏ. Để xử lý dữ liệu bằng tiếng Anh, Spark được sử dụng để xử lý dataset. Do khối lượng dữ liệu khổng lồ, có thể có vấn đề về bộ nhớ không đủ. Giải pháp cho vấn đề này bao gồm việc chia dữ liệu văn bản thành các buckets riêng biệt để lưu trữ. Dữ liệu của mỗi bucket sau đó được xử lý lần lượt để loại bỏ các bản sao.

Khử trùng lặp Minhash LSH Minhash [13] là một phương pháp xuất sắc để loại bỏ các bản sao gần, đặc biệt cho dữ liệu trang web, và được sử dụng rộng rãi để tìm kiếm tương tự và phát hiện bản sao trong các datasets lớn [104,33,37]. Nó có thể xử lý các tình huống rất phổ biến nơi nội dung văn bản về cơ bản giống nhau, nhưng các khối template rải rác của các trang web khác nhau. Nguyên lý của MinHash là biểu diễn một tập hợp bằng các giá trị hash nhỏ hơn, sau đó có thể được sử dụng để ước tính độ tương tự Jaccard [47] giữa hai tập hợp: Jaccard (A, B) = (A∩B)/(A∪B).

MinHash bao gồm việc sử dụng nhiều hash functions riêng biệt ánh xạ mỗi phần tử của một tập hợp đến một domain số lớn hơn. Đối với mỗi tập hợp, những hash functions này được áp dụng cho tất cả các phần tử trong tập hợp, và giá trị hash nhỏ nhất được tạo ra bởi mỗi hash function được chọn làm giá trị hash tối thiểu của nó. Do đó, mỗi tập hợp có thể được biểu diễn bởi một vector của những giá trị hash tối thiểu này, tạo thành chữ ký MinHash của tập hợp. Đối với dữ liệu văn bản, một phương pháp n-gram có thể được sử dụng để xây dựng một tập hợp.

Sau khi thu được chữ ký của văn bản, Locality-Sensitive Hashing (LSH) [35] được sử dụng để nhanh chóng xác định các cặp tập hợp ứng viên vượt quá một ngưỡng nhất định về độ tương tự Jaccard. Điều này tăng tốc quá trình tìm kiếm các items tương tự. Phương pháp cụ thể chia chữ ký thành nhiều bands, mỗi band chứa nhiều giá trị hash. Một hash function khác sau đó được sử dụng để ánh xạ mỗi band đến một hash bucket. Tất cả các tập hợp có cùng band hash được ánh xạ đến cùng hash bucket. Tất cả các cặp tập hợp trong cùng hash bucket được xem là các cặp tương tự ứng viên mà không cần cụ thể thêm về độ tương tự của chúng. Ở đây, chúng tôi sử dụng 128 hash functions độc đáo để tạo chữ ký, chia thành 9 bands, với mỗi band chứa 13 giá trị hash. Do đó, ngưỡng Jaccard được đặt ở 0.8.

Khi xác định các cặp tương tự, các thành phần liên kết được xây dựng. Trong mỗi thành phần của các thành phần liên kết, một văn bản được giữ lại trong khi những văn bản khác bị loại bỏ. Để xử lý hiệu quả lượng lớn dữ liệu, một implementation phân tán [53] dựa trên map-reduce được áp dụng.

Khử trùng lặp Đoạn văn Khử trùng lặp đoạn văn bao gồm việc loại bỏ tất cả các đoạn văn trùng lặp trong một văn bản. Một đoạn văn được định nghĩa là một phần văn bản được phân tách bởi ký tự UTF-8 newline "\n". Khử trùng lặp đoạn văn là một phương pháp hiệu quả để loại bỏ headers điều hướng trang web, quảng cáo và các yếu tố tương tự. Vì khử trùng lặp đoạn văn bao gồm việc xóa các phần của văn bản, nó có thể gây ra một số can thiệp với phân tích nội dung.

Implementation cụ thể đầu tiên bao gồm việc chia văn bản thành nhiều đoạn văn sử dụng ký tự newline utf-8 " \n", với mỗi đoạn văn được gắn thẻ với document id tương ứng và offset trong văn bản. Sau đó, mỗi đoạn văn được hash bằng SHA256. Tiếp theo, các giá trị hash được khử trùng lặp. Sau khử trùng lặp, văn bản đã khử trùng lặp được khôi phục theo document ID và offset.

Khử trùng lặp Substring Chính xác Phương pháp này theo [58]. Với sự đa dạng của ngôn ngữ, khi độ dài của văn bản lặp lại đủ dài, rất có khả năng chúng được dẫn xuất từ nhau hoặc có nguồn từ cùng một tham chiếu. Do đó, khi hai văn bản, ti và tj chia sẻ một substring đủ dài, tức là ta..a+k i = tb..b+k j, một trong số chúng sẽ được loại bỏ. Đối với việc lựa chọn ngưỡng độ dài, chúng tôi tuân theo cài đặt trong [58], chọn k=50. Do môi trường phân tán của chúng tôi, bộ nhớ của một node đơn không đủ để chứa toàn bộ dữ liệu. Do đó, chúng tôi không áp dụng implementation trong [58]. Trong công trình của chúng tôi, chúng tôi phân đoạn mỗi văn bản thành các sliding windows 50 ký tự với kích thước bước là 1. Sau đó chúng tôi tính giá trị hash SHA256 cho mỗi window cùng với document ID và offset tương ứng. Tiếp theo, đối với các windows có giá trị hash giống hệt nhau, chúng tôi đánh dấu chúng là trùng lặp ngoại trừ cái đầu tiên. Cuối cùng, sử dụng text ID và offset, chúng tôi khôi phục các chuỗi gốc và quyết định có xóa một đoạn dựa trên marker trùng lặp hay không.

(a) Tỷ lệ giữ lại xử lý lại cho các corpus trong §4.1.
(b) Tỷ lệ giữ lại xử lý cho các corpus được crawl từ đầu trong §4.2.

Hình 3: Sơ đồ Funnel cho hai pipeline dữ liệu chính. Phần tối hơn của mỗi hàng đại diện cho tỷ lệ giữ lại cho mỗi bước xử lý và phần sáng hơn cho các corpus được lọc.

4.2 Pipeline Thu thập Corpus từ Đầu

Chúng tôi cung cấp thêm một pipeline để crawl và xử lý nội dung web từ đầu và trình bày nó với dữ liệu ngôn ngữ tiếng Trung, có thể là hướng dẫn từng bước cho nghiên cứu tiếp theo để xây dựng corpus cập nhật mới. Chúng tôi lấy corpus được tạo ra trong pipeline như vậy làm tập con tiếng Trung của Matrix, nơi 80.6% được dẫn xuất từ các trang web tiếng Trung mà chúng tôi crawl và những cái khác từ một số datasets mở, như được liệt kê trong Bảng 4. Tổng quan pipeline và chi tiết được minh họa trong Hình 3b.

Bảng 4: Thành phần nguồn của tập con web tiếng Trung.

[THIS IS TABLE: Shows composition of Chinese web subset with datasets, parts, UTF-8 bytes, availability, and proportions]

4.2.1 Lọc

Các quy tắc lọc cho datasets tiếng Trung được điều chỉnh đặc biệt để giải quyết những thách thức độc đáo của chúng, khác với những quy tắc được áp dụng cho các datasets tiếng Anh được xử lý tương đối tốt trong §4.1. Xem xét tỷ lệ lớn dữ liệu được chuyển đổi từ HTML trong các datasets tiếng Trung, chúng tôi tập trung mạnh vào việc loại bỏ các artifacts liên quan đến HTML và chỉnh sửa các inconsistencies văn bản. Hơn nữa, với sự khác biệt ngôn ngữ đáng kể giữa tiếng Trung và tiếng Anh, chúng tôi tiến hành lấy mẫu có mục tiêu các tài liệu trong datasets tiếng Trung, nhằm đánh giá lại và điều chỉnh ngưỡng và chi tiết của quy tắc lọc, đảm bảo sự phù hợp của chúng cho đặc điểm ngôn ngữ độc đáo của văn bản tiếng Trung. Ví dụ, chúng tôi tinh chỉnh các quy tắc để phân biệt giữa 'ký tự' và 'từ' trong văn bản tiếng Trung, điều chỉnh phương pháp tokenization cho phù hợp.

Các bước lọc tiếng Trung của chúng tôi tương tự như các quy tắc được điều chỉnh để lọc Massive Appropriate Pre-train Chinese Corpus (MAP-CC) [30]: 1) Thống nhất định dạng dữ liệu để tăng hiệu quả xử lý. 2) Loại bỏ URL. Bước này được tiến hành trong hai giai đoạn: đầu tiên, loại bỏ văn bản với URLs được liệt kê trong Blacklist T1; tiếp theo là quét toàn diện để loại bỏ URLs còn lại. 3) Lọc cấp câu và tài liệu để loại bỏ văn bản quá ngắn, dưới tiêu chuẩn hoặc không logic. 4). Loại bỏ trùng lặp, bao gồm n-grams và câu.

4.2.2 Khử trùng lặp

Khử trùng lặp dữ liệu tiếng Trung bao gồm Khử trùng lặp Tài liệu Chính xác, Khử trùng lặp MinHash và Khử trùng lặp Dòng Tương tự. Do khó khăn trong việc triển khai Spark trong môi trường để xử lý tiếng Trung, chúng tôi đã implement lại hai phương pháp đầu tiên. Đối với Khử trùng lặp Tài liệu Chính xác, có một số khác biệt nhỏ so với implementation cho tiếng Anh, chủ yếu để tiết kiệm bộ nhớ, nơi chúng tôi đã áp dụng phương pháp Bloom Filter và đặt tỷ lệ false positive của Bloom Filter là 0.001. Thảo luận về Khử trùng lặp Tài liệu Chính xác và MinHash LSH có thể được tìm thấy trong §4.1.2.

Chúng tôi không sử dụng khử trùng lặp substring chính xác vì khi crawl các trang web, thường crawl lặp lại cùng một nội dung nhiều lần trong một tài liệu tín hiệu. Ngoài ra, khi trích xuất văn bản chính từ HTML, thường có sự mất mát một hoặc hai từ. Sự kết hợp của hai tình huống này vi phạm giả định trong [58] rằng "hiếm khi cùng một ý tưởng được biểu đạt giống hệt nhau trong nhiều tài liệu trừ khi một biểu đạt được dẫn xuất từ cái khác, hoặc cả hai đều trích dẫn từ một nguồn chung." Do đó, sau khử trùng lặp substring chính xác, sẽ có những trường hợp từ bổ sung được giữ lại, giảm đáng kể khả năng đọc của văn bản. Do đó, chúng tôi đề xuất phương pháp khử trùng lặp Dòng Tương tự để giải quyết vấn đề này.

4.2.3 Khử trùng lặp Dòng Tương tự

Để giải quyết tình huống nội dung giống hệt nhau xuất hiện nhiều lần trong một văn bản, một phương pháp trực tiếp là chia văn bản thành các dòng sử dụng các delimiters cụ thể và sau đó so sánh độ tương tự giữa mỗi dòng. Nếu chúng tương tự, dòng tiếp theo sẽ được loại bỏ. Việc chia dòng bao gồm việc sử dụng các delimiters sau: "[", ".", "!", "?", " \", ". . . . . . ", "]". Chúng tôi sử dụng edit distance để đánh giá xem hai dòng L1 và L2 có tương tự hay không như sau:

isSimilar (L1, L2) =
True nếu min(|L1|,|L2|) ≥ 15 ∧ editDist (L1, L2) < 0.1 × min(|L1|,|L2|)
True nếu min(|L1|,|L2|) < 15 ∧ L1 = L2
False nếu không,

trong đó |L| là độ dài của dòng L và "editDist" là viết tắt của edit distance.

Do độ phức tạp tính toán của việc tính edit distance là O(len(L1) × len(L2)), để tăng tốc quá trình này, chúng tôi đề xuất thêm hai phương pháp để đánh giá sự không tương tự:

1. Sự khác biệt độ dài giữa hai dòng có lớn hơn một phần mười độ dài của dòng ngắn hơn không?
2. Tỷ lệ của giao điểm của các tập hợp ký tự và hợp của các tập hợp ký tự trong L1 và L2 có nhỏ hơn một phần ba không?

Lưu ý rằng phương pháp đầu tiên có độ phức tạp tính toán O(1), và phương pháp thứ hai có độ phức tạp O(len(L1) + len(L2)). Do đó, các phương pháp này có thể cải thiện đáng kể tốc độ tính toán. Rõ ràng, nếu một trong hai câu hỏi trên là tích cực, các dòng không thể được coi là tương tự. Nếu không, chúng tôi tính isSimilar (L1, L2) để thu được độ tương tự giữa L1 và L2.

4.3 Pipeline Chuyển đổi Tài liệu

Các tài liệu thường được định dạng tốt hơn, tập trung chủ đề và có biểu đạt nhất quán hơn so với nội dung web nhiễu. Tuy nhiên, nó có vẻ như một mỏ vàng của corpus chất lượng cao ngoại trừ việc vàng nằm sâu dưới bụi bẩn kỹ thuật số. Các tài liệu kỹ thuật số như vậy chủ yếu được lưu trữ dưới dạng PDFs tiêu chuẩn với layouts đa dạng hoặc hình ảnh quét với chất lượng không nhất quán, khiến việc xây dựng datasets trở nên thách thức. Chúng tôi quan sát hai vấn đề cốt lõi trong việc thiết kế pipeline chuyển đổi hiệu quả để trích xuất văn bản thuần từ tài liệu: i) phân tích thông tin layout và xác định các yếu tố layout khác nhau bao gồm văn bản, tiêu đề, chú thích, hình ảnh, bảng và công thức, và ii) nhận dạng mối quan hệ giữa các thành phần layout này.

Chúng tôi khảo sát các giải pháp mã nguồn mở hiện có cho chuyển đổi tài liệu và tìm thấy một số dự án xuất sắc với hiệu suất tốt: PP-StructureV2 [59], Marker, Vary [108] và Nougat [11]. Tuy nhiên, cùng với những ưu điểm, mỗi dự án đều thể hiện những hạn chế có thể được giải quyết để nâng cao hiệu suất hơn nữa: PP-StructureV2 không thể nhận dạng nội dung định dạng LaTeX và các giai đoạn hậu xử lý cần thiết; Marker và Texify hỗ trợ ít ngôn ngữ và không xử lý hiệu quả các hình ảnh; Nougat có hỗ trợ hạn chế cho dữ liệu nhiều cột và ngôn ngữ được nhận dạng; Vary và Vary-toy yêu cầu tài nguyên tính toán đáng kể. Do đó, chúng tôi đề xuất một framework bao gồm các thành phần xử lý tách rời, cho phép chúng tôi tận dụng điểm mạnh của các mô hình này cùng nhau. Ví dụ, chúng tôi sử dụng Marker để tăng cường hỗ trợ ngôn ngữ và PP-StructureV2 để phân tích layout hiệu quả. Như được minh họa trong Hình 4, framework chuyển đổi tài liệu của chúng tôi bao gồm bốn phần: Layout Detection, Element Recognition, Ordering và Post Process. Việc tách rời giữa mỗi module tăng cường khả năng diễn giải và đơn giản hóa việc nâng cấp, bổ sung và thay thế các thành phần khác nhau.

Layout Detection phân đoạn tài liệu thành nhiều phần như công thức, văn bản, headers và footers. Pipeline sử dụng mô hình phát hiện mục tiêu nhẹ được cung cấp bởi PP-StructureV2, có hiệu quả tính toán và hiệu suất vượt trội. Hiệu suất của mô hình này được nâng cao thêm bằng cách sử dụng thuật toán FGD (Feature Gradient Descent), tối ưu hóa trích xuất đặc trưng để phát hiện layout chính xác hơn.

Element Recognition kết hợp các mô hình khác nhau để xác định các yếu tố khác nhau. Đối với nhận dạng công thức, mô hình TrOCR được đào tạo thông qua Pix2Text vượt trội so với các mô hình nhận dạng công thức khác như Latex-OCR và Taxify, hỗ trợ nhận dạng các công thức được nhúng trong đoạn văn và công thức không thông thường, do đó giải quyết hiệu quả hầu hết các tình huống nhận dạng công thức. Nhận dạng văn bản sử dụng PP-OCRv4, đáng chú ý về khả năng tương thích với nhiều thiết bị tính toán và tự hào về khả năng nhận dạng mạnh; khoảng một trăm mô hình nhận dạng ngôn ngữ đã được phát hành công khai, áp dụng cho phạm vi rộng hơn các nhiệm vụ nhận dạng tài liệu. Hình ảnh được lưu dưới dạng images và chèn vào giai đoạn hợp nhất tiếp theo. Tái tạo bảng được thực hiện bằng SLANet, biểu diễn bảng ở định dạng HTML. Các vùng khác, như headers, footers và số trang, được loại bỏ và không tiến hành các giai đoạn hậu xử lý và tái tạo.

Ordering Trong các nhiệm vụ chuyển đổi tài liệu, việc xử lý đúng mối quan hệ giữa các blocks là vô cùng quan trọng. Để có được dữ liệu chuyển đổi chất lượng cao, chúng ta cần xử lý đúng các tình huống layout phức tạp như nhiều cột và điều kiện cross-page. Trong giai đoạn ordering, chúng tôi sử dụng LayoutLMv3 [45] để phát hiện cột và sắp xếp các vùng khác nhau theo quy tắc cụ thể. Chiến lược này không chỉ tăng cường độ chính xác của nhiệm vụ mà còn tối ưu hóa đáng kể khả năng đọc.

Post-processing. Các văn bản được trích xuất bởi OCR thường không thể được sử dụng trực tiếp và yêu cầu xử lý bổ sung như sau:

1. Câu bị cắt: Trong văn bản được trích xuất từ hình ảnh, câu có thể bị phân mảnh qua các dòng hoặc trang khác nhau, dẫn đến một câu đơn được chia thành nhiều đoạn. Trích xuất văn bản OCR hiệu quả đòi hỏi việc xác định và nối lại các câu phân mảnh này để tái tạo các câu mạch lạc, hoàn chỉnh.

2. Từ có dấu gạch nối: Một số từ có thể bị chia thành hai phần trong văn bản do hạn chế định dạng, được kết nối bằng dấu gạch nối (ví dụ: network-ing). Trích xuất văn bản phải nhận dạng những từ có dấu gạch nối này và hợp nhất chúng trở lại thành một từ hoàn chỉnh (ví dụ: networking).

3. Công thức toán bị hỏng: Các công thức toán học được OCR trong Markdown có thể gặp các vấn đề như thiếu yếu tố, ký hiệu không chính xác hoặc biểu thức phân mảnh. Để giải quyết vấn đề này, chúng tôi tinh chỉnh một mô hình ngôn ngữ lớn tiền huấn luyện mã nguồn mở 7 tỷ tham số [7] trên các cặp dữ liệu học tập có giám sát (xi, yi). Ở đây, xi đại diện cho hướng dẫn phát hiện và sửa lỗi trong các văn bản đã cho, và yi đại diện cho văn bản đầu ra đã được sửa. Chúng tôi áp dụng vLLM để cho phép suy luận nhanh hơn thông qua quantization và quản lý bộ nhớ hiệu quả của attention keys và values sử dụng PagedAttention, cùng các tối ưu hóa khác. Templates prompt được sử dụng để xử lý cả hai ngôn ngữ được cung cấp trong Phụ lục A.10.

Bằng cách kết hợp các chiến lược này, chúng tôi có thể cải thiện đáng kể chất lượng và tính mạch lạc của văn bản được OCR, giảm thiểu các lỗi phổ biến và nâng cao khả năng đọc và khả năng sử dụng tổng thể của nội dung được trích xuất. Chúng tôi sử dụng FastDeploy, một công cụ triển khai suy luận AI hiệu quả cao, làm codebase cho implementation của chúng tôi, có thể khai thác đầy đủ lợi thế của multithreading để tối ưu hóa tốc độ suy luận và overhead tính toán. Tổng thể, trong khi duy trì hiệu suất và hiệu quả triển khai, chúng tôi cung cấp một framework cho chuyển đổi tài liệu bao gồm các tình huống toàn diện, bao gồm nhận dạng thông tin layout, hỗ trợ tái tạo bảng và nhận dạng công thức.

4.4 Thu thập Dữ liệu Bổ sung Chất lượng Cao

Trong phần này, chúng tôi trình bày phương pháp Thu thập Dữ liệu Bổ sung Chất lượng Cao, áp dụng cho một phạm vi đa dạng các chủ đề và tăng cường tính mạnh mẽ của datasets. Lấy cảm hứng từ [89], áp dụng pipeline lặp để tạo điều kiện thu thập dữ liệu quy mô lớn, chất lượng cao từ Common Crawl, chúng tôi đề xuất chọn dữ liệu chất lượng cao cho toán học, dữ liệu tổng hợp kỳ thi khoa học và nội dung dựa trên wiki trong Matrix của chúng tôi.

Các giai đoạn quy trình của pipeline lặp được liệt kê như sau:

• Thu thập Dataset Seed: Thu thập dataset seed chất lượng cao cho lĩnh vực quan tâm, như toán học, code hoặc nội dung dựa trên wiki.
• Định nghĩa Domain và Lấy mẫu: Định nghĩa một domain như các entries dữ liệu trong dataset seed chia sẻ cùng base URL và trích xuất mẫu từ mỗi domain trong dataset seed làm mẫu tích cực để tăng cường đa dạng định dạng. Tương ứng, thu thập lượng dữ liệu bằng nhau từ Common Crawl làm mẫu tiêu cực.
• Đào tạo Mô hình: Sử dụng mô hình FastText [50] để phân loại nhị phân nhằm phân biệt tính liên quan của dữ liệu đến lĩnh vực cụ thể. Các tham số đào tạo được đặt như sau: ba epochs, learning rate 0.1, embedding dimension 256 và n-gram 3. Mô hình được quantized để tăng cường hiệu quả hoạt động trong khả năng bộ nhớ hạn chế, giảm kích thước xuống khoảng 10% so với footprint gốc.
• Đánh giá Confidence Dữ liệu: Sử dụng mô hình FastText được đào tạo để ước tính confidence của dữ liệu Common Crawl đủ điều kiện là tích cực. Giữ lại dữ liệu được sắp xếp từ confidence cao nhất đến thấp nhất. Để đơn giản hóa quá trình sắp xếp confidence, ban đầu lấy mẫu một tập con dữ liệu để thiết lập ngưỡng khả thi cân bằng nhu cầu loại trừ dữ liệu với nhu cầu giữ lại.
• Đánh giá Dữ liệu: Đánh giá dữ liệu được giữ lại thông qua ChatGPT 3.5 [1], sử dụng URL để xác định tính cụ thể của lĩnh vực. Giai đoạn này nhằm giảm thiểu tỷ lệ false positives trong khi duy trì tỷ lệ recall cần thiết.
• Recall và Chú thích Dữ liệu: Xem xét lại các domains nơi hơn 10% dữ liệu được nhận dạng là cụ thể lĩnh vực. Chú thích tập con dữ liệu này sử dụng ChatGPT 3.5 [1] thông qua URL.
• Tinh chỉnh và Lặp lại Mô hình: Tích hợp dữ liệu tích cực chưa được xác nhận từ các lần lặp trước vào các mẫu tích cực để đa dạng hóa cơ sở đào tạo của mô hình FastText. Tiếp theo, bắt đầu chu kỳ lặp mới từ giai đoạn đào tạo.

Việc lựa chọn dữ liệu cho Common Crawl tập trung vào nội dung tiếng Anh của dataset RedPajama V2 [25]. Dataset seed cho phần toán học có nguồn từ OpenWebMath [6], trong khi dataset tổng hợp khoa học từ các domains cụ thể như Chemrxiv, biorxiv và dữ liệu bài tập được crawl độc quyền từ các datasets mã nguồn mở, ví dụ wanjuan-exam [38], WebInstruct [117], Web Of Science [55]. Dữ liệu Wiki được thu thập trực tiếp từ các trang web wiki.

5 Mô hình

5.1 Kiến trúc Mô hình

Kiến trúc mô hình MAP-Neo được dựa trên transformer decoder như được nêu bởi Vaswani et al. [103]. Các tham số thiết yếu xác định kiến trúc này được chi tiết trong Bảng 5. Các mô hình được đào tạo với độ dài context 8192 tokens, kết hợp một số cải tiến được đề xuất sau khái niệm transformer gốc. Những cải tiến này được liệt kê bên dưới:

Multi-Query Attention [92]. Biến thể mô hình 7B sử dụng multi-head attention, trong khi các checkpoints mô hình 2B implement multi-query attention, sử dụng cấu hình single key-value head (numkvheads = 1). Sửa đổi này dựa trên các nghiên cứu ablation chỉ ra rằng multi-query attention đặc biệt hiệu quả ở quy mô nhỏ hơn [92].

RoPE Embeddings [97]. Thay vì embeddings vị trí tuyệt đối truyền thống, chúng tôi sử dụng rotary positional embeddings ở mỗi layer và chia sẻ những embeddings này giữa inputs và outputs, giảm thiểu kích thước mô hình tổng thể.

RMSNorm. Để đảm bảo đào tạo ổn định, mỗi sub-layer transformer—bao gồm cả attention và feedforward layers—được chuẩn hóa sử dụng RMSNorm [120].

Activation Function Chúng tôi sử dụng SwiGLU [93] làm activation function.

5.2 Hyperparameters Quy mô Mô hình

Trong công trình này, chúng tôi so sánh hai quy mô mô hình khác nhau: 2B và 7B tham số. Vì những mô hình này là Transformers dense tiêu chuẩn. Những mô hình này được xây dựng sử dụng hyperparameters trong Bảng 5. Hai mô hình được đào tạo giống hệt nhau (ngoại trừ dữ liệu đào tạo) sử dụng cùng từ vựng và batch size. Chi tiết đào tạo được hiển thị trong §3 và §5.1.

Bảng 5: Chi tiết kiến trúc mô hình. Chúng tôi liệt kê số layers, dmodel, số attention heads và kích thước attention head. Kích thước feed-forward dff luôn là 8×dmodel.

[THIS IS TABLE: Shows model architecture details with columns for Model, # Layers, # Heads, dmodel, # Feedforward dims, # KV heads for MAP-Neo 2B and 7B models]

6 Tiền huấn luyện

Trong quá trình tiền huấn luyện, chúng tôi sử dụng chiến lược tiền huấn luyện hai giai đoạn để đào tạo mô hình MAP-Neo. Giai đoạn đầu tiên được gọi là giai đoạn cơ bản, bao gồm việc đào tạo mô hình trên corpus rộng lớn về văn bản chung để phát triển khả năng tạo văn bản tổng quát. Tiếp theo, trong giai đoạn suy giảm, chúng tôi tập trung vào việc tăng cường độ tin cậy của nội dung được tạo ra bởi mô hình bằng cách kết hợp dữ liệu chất lượng cao và dữ liệu code mode. Phân phối dữ liệu được sử dụng trên các giai đoạn khác nhau được mô tả trong Hình 5. Lưu ý rằng chúng tôi tăng khối lượng dữ liệu code trong giai đoạn suy giảm. Cụ thể, trong giai đoạn cơ bản, vì Stack V2 [68] chưa có sẵn, chúng tôi đã sử dụng Stack V1 [54] và lặp lại dataset hai lần để đạt được tỷ lệ dữ liệu cân bằng. Trong giai đoạn suy giảm, với việc phát hành Stack V2 [68], chúng tôi đã kết hợp nó làm thành phần code cho đào tạo. Hơn nữa, chúng tôi thực hiện tinh chỉnh phân phối dữ liệu thêm bao gồm các nguồn dữ liệu chất lượng cao được nhân đôi, như sách, quyết định tư pháp và báo cáo chính phủ để đào tạo, nhằm cải thiện hiệu suất của mô hình. Dữ liệu mã nguồn mở được sử dụng cho tiền huấn luyện được hiển thị trong Bảng 16, chi tiết lặp lại dữ liệu được hiển thị trong Bảng 17 và hyperparameters đào tạo được hiển thị trong Bảng 6.

Hình 5: Tỷ lệ hỗn hợp dữ liệu trong giai đoạn tiền huấn luyện MAP-Neo. Bên trái là giai đoạn cơ bản và bên phải hiển thị giai đoạn suy giảm.

6.1 Giai đoạn Cơ bản: Thu nhận Khả năng Tổng quát

Trong giai đoạn cơ bản, chúng tôi sử dụng learning rate scheduler (LRS) hai giai đoạn để trang bị cho mô hình khả năng mạnh mẽ cho việc tạo văn bản tổng quát. LRS được mô hình hóa như một hàm từng đoạn, bao gồm giai đoạn warmup ban đầu nơi learning rate tăng tuyến tính từ tỷ lệ cơ sở ηa = 2×10−5 đến peak learning rate ηmax = 2×10−4 trong twarmup = 2k steps. Điều này được theo sau bởi giai đoạn cosine decay, trong đó tỷ lệ giảm dần trở lại ηb = 2×10−5 trong khoảng 365k steps. Learning rate f(t) như một hàm của thời gian t có thể được mô tả như sau:

f(t) = 
ηa + (ηmax−ηa)t/twarmup nếu t ≤ twarmup
ηb + (ηmax−ηb) × 1/2 × [1 + cos(π(t−twarmup)/(ttotal−twarmup))] nếu twarmup < t ≤ ttotal, (1)

trong đó t là timestep hiện tại, twarmup biểu thị thời lượng của giai đoạn warmup, và ttotal đại diện cho tổng số training timesteps. Giai đoạn học tập này xử lý khoảng 3,726 tỷ tokens, đảm bảo đào tạo mạnh mẽ của mô hình trên dữ liệu văn bản đa dạng. Cấu hình tỉ mỉ này của learning rates và xử lý mở rộng tối ưu hóa dynamics đào tạo và hiệu quả, thúc đẩy sự trưởng thành ổn định của khả năng mô hình.

6.2 Giai đoạn Suy giảm: Cải thiện và Chỉnh sửa

Do vấn đề trong đào tạo tokenizer như đã tuyên bố trong §3, mô hình gặp thất bại kiểm tra trong các nhiệm vụ tạo code, mặc dù khả năng hiểu ngôn ngữ mạnh mẽ được thu nhận trong giai đoạn cơ bản. Để giải quyết vấn đề này, chúng tôi đã giới thiệu giai đoạn suy giảm bổ sung được thiết kế đặc biệt để sử dụng tokenizer của phiên bản đã sửa. Learning rate trong giai đoạn suy giảm này bắt đầu ở ηc = 2×10−4 và trải qua exponential decay trong tdecay = 148 k steps, với half-life T tương ứng với nửa tdecay steps, tương tự như giai đoạn suy giảm được sử dụng bởi MiniCPM [44], có thể được công thức hóa như sau:

f(t) = ηc × 0.5^(t/T) nếu t ≤ tdecay, (2)

trong đó t là timestep hiện tại của giai đoạn suy giảm. Điều chỉnh chiến lược này không chỉ chỉnh sửa các lỗi tokenization ban đầu mà còn tăng cường hiệu suất của mô hình trên các nhiệm vụ tạo code. Trong giai đoạn này, mô hình xử lý tổng cộng khoảng 778 tỷ tokens, chủ yếu bao gồm dữ liệu hướng dẫn chất lượng cao. Chúng tôi cũng đồng thời tăng tỷ lệ code trong dữ liệu từ 14.77% lên 17.04%. Điều chỉnh này cải thiện đáng kể hiệu suất tổng thể của mô hình. Việc làm giàu có chủ ý dataset với tỷ lệ code cao hơn, kết hợp với inputs hướng dẫn, đảm bảo một mô hình mạnh mẽ và đa năng hơn, thành thạo trong việc giải quyết các nhiệm vụ lập trình phức tạp cũng như hiểu và tạo phản hồi chuyên nghiệp trong các lĩnh vực khác nhau.

7 Alignment

7.1 Tinh chỉnh có Giám sát

Để alignment với hành vi con người của LLMs, bước đầu tiên là thực hiện Supervised Fine-Tuning (SFT). SFT của chúng tôi cũng bao gồm hai giai đoạn. Trong giai đoạn đầu tiên, chúng tôi thu thập một lượng lớn dữ liệu hướng dẫn để tăng cường khả năng nền tảng của LLMs. Trong giai đoạn thứ hai, chúng tôi xây dựng dựa trên các khả năng được thiết lập trong giai đoạn đầu tiên và đề xuất cải thiện khả năng chat của MAP-Neo. Quá trình này tinh chỉnh một LLM được tiền huấn luyện trên dữ liệu kiểu chat, bao gồm cả queries và responses. Chúng tôi minh họa chi tiết về xây dựng dữ liệu và chiến lược đào tạo.

7.1.1 Dữ liệu

Giai đoạn Nền tảng: Tăng cường Khả năng Theo dõi Hướng dẫn Trong giai đoạn đầu tiên, trọng tâm của chúng tôi là tăng cường đáng kể khả năng nền tảng của mô hình (ví dụ: kỹ năng code và toán học), nơi chúng tôi sử dụng hơn 2 triệu data points hướng dẫn trong giai đoạn này. Cụ thể, giai đoạn đầu tiên bao gồm toàn bộ OpenHermes 2.5 [99], nơi chúng tôi loại trừ các phần liên quan đến benchmark TheoremQA [16] để ngăn chặn rò rỉ dữ liệu benchmark. Ngoài ra, chúng tôi kết hợp dataset Code-Feedback [125] hoàn chỉnh và một tập con dữ liệu WebInstructSub [117].

Giai đoạn Chat: Tăng cường Khả năng Chat Trong giai đoạn thứ hai, chúng tôi tập trung vào việc cải thiện khả năng chat của mô hình trong khi duy trì các kỹ năng nền tảng được thu nhận trong giai đoạn đầu tiên. Với mục đích này, chúng tôi thu thập hơn 100k dữ liệu đối thoại nhiều lượt có nguồn từ các cuộc trò chuyện người dùng thực. Để đảm bảo mô hình giữ lại khả năng nền tảng của nó, chúng tôi bao gồm 5k data points liên quan đến toán học và code được trích xuất từ giai đoạn đầu tiên. Các thí nghiệm của chúng tôi đã chứng minh rằng giai đoạn SFT bổ sung này tăng cường đáng kể hiệu suất của mô hình trên các benchmark chat, như MT-Bench [124] và AlpacaEval [62], mà không làm tổn hại khả năng nền tảng của nó.

Bằng cách tuân theo phương pháp hai giai đoạn này, chúng tôi đảm bảo rằng mô hình của chúng tôi không chỉ có thể duy trì nền tảng mạnh mẽ trong các kỹ năng thiết yếu mà còn tạo ra các phản hồi tự nhiên, hữu ích và chính xác về mặt ngữ cảnh.

7.1.2 Đào tạo

Nhất quán với tiền huấn luyện, chúng tôi cũng áp dụng mục tiêu dự đoán next-token làm nhiệm vụ đào tạo cho SFT. Lưu ý rằng chúng tôi áp dụng loss masks cho system và user inputs. Quá trình đào tạo của mô hình sử dụng optimizer AdamW với hyperparameters trong Bảng 6.

Độ dài sequence được giới hạn ở 8192, và batch size là 512. Quá trình đào tạo bao gồm hai giai đoạn sử dụng cùng hyperparameters. Trong giai đoạn đầu tiên, mô hình được đào tạo trong 3 epochs sử dụng hơn 2 triệu data points hướng dẫn, tập trung vào việc tăng cường khả năng nền tảng. Trong giai đoạn thứ hai, mô hình được đào tạo trong 1 epoch sử dụng hơn 100k dữ liệu đối thoại nhiều lượt để tăng cường khả năng chat trong khi duy trì các kỹ năng nền tảng được thu nhận trong giai đoạn đầu tiên.

7.2 DPO Lặp lại

DPO Direct Preference Optimization (DPO) [77] là một phương pháp đơn giản và hiệu quả để alignment các mô hình ngôn ngữ với phản hồi của con người. Nó chuyển đổi preference loss [12] thành loss function trên mô hình ngôn ngữ, từ đó bỏ qua nhu cầu về reward modeling rõ ràng [12] và reinforcement learning [19,87]. Bắt đầu với một mô hình ngôn ngữ được tinh chỉnh có giám sát, được ký hiệu là πsft, DPO thu thập dataset D={(x, yw, yl)i}, bao gồm preferences của con người giữa hai responses được tạo ra bởi πsft: yw (được ưa thích) và yl (không được ưa thích) cho cùng một prompt x. Sử dụng dataset này, DPO tham số hóa một mô hình ngôn ngữ πθ và trực tiếp ước tính các tham số của nó thông qua maximum likelihood estimation trên dataset preference của con người D như sau:

LDPO(πθ;πsft,D) = −E(x,yw,yl)∼D[logσ(βlogπθ(yw|x)/πsft(yw|x) − βlogπθ(yl|x)/πsft(yl|x))] (3)

DPO Lặp lại. Chúng tôi theo Storm-7B [64] để sử dụng pipeline Iterative DPO [111] để phát triển mô hình chat của chúng tôi. Cụ thể, chúng tôi sử dụng ba lần lặp, với mỗi lần lặp bao gồm ba giai đoạn: 1) tạo paired responses, 2) gắn nhãn responses sử dụng reward models, và 3) đào tạo LLM với DPO loss như được mô tả trong Eq. 3. Chúng tôi sử dụng Nectar làm prompt dataset của chúng tôi và Starling-RM-34B [126] làm reward model của chúng tôi. Mô hình này được tinh chỉnh từ Yi-34B-Chat [113] và tạo ra scalar output cho bất kỳ prompt và response nào được cho. Để bảo tồn khả năng đa ngữ của mô hình, chúng tôi cũng áp dụng preference dataset bằng tiếng Trung trong lần lặp thứ 3.

Chúng tôi báo cáo length-controlled win rate của AlpacaEval2.0 [32] để chứng minh tiến bộ hiệu suất của mô hình trong Bảng 7. Kết quả cho thấy hiệu suất cải thiện với mỗi lần lặp, chỉ ra rằng mô hình của chúng tôi ngày càng được alignment với các giá trị con người.

Bảng 7: Length-controlled win rate của MAP-Neo ở các lần lặp khác nhau trên leaderboard AlpacaEval2.0. Đối với "SFT", chúng tôi báo cáo hiệu suất của mô hình sử dụng SFT hai giai đoạn.

[THIS IS TABLE: Shows LC Win Rate (%) across different iterations: SFT (9.77), Iteration 1 (10.02), Iteration 2 (15.59), Iteration 3 (16.65)]

8 Định luật Mở rộng của MAP-Neo

8.1 Định nghĩa Vấn đề

Các định luật mở rộng có khả năng dự đoán cấu hình đào tạo cho việc đào tạo LLMs. Nguyên lý này nhấn mạnh tầm quan trọng của tỷ lệ giữa lượng dữ liệu đào tạo D (được đo bằng tokens) và kích thước mô hình N (về mặt tham số). Trong phần này, chúng tôi áp dụng Định luật Chinchilla trong Eq. 4 [43], Định luật OpenAI trong Eq. 5 [52], một dẫn xuất của định luật Symbolic Music Scaling trong Eq. 6 [75] và phương pháp được đề xuất của chúng tôi trên dataset của chúng tôi để fit các mô hình của chúng tôi, trong đó A,B,E,α,β,αc,Dc, αN,Nc và d là các hyperparameters cần được tối ưu hóa.

L(N, D) = A/N^α + B/D^β + E (4)
L(N, D) = (Nc/N)^αN + (αD + Dc/D)^αD (5)
L(N, D) = d/(N^α·D^β) + A/N^α + B/D^β + E. (6)

Định luật SMS gốc giới thiệu hai sửa đổi đối với định luật Chinchilla. Sửa đổi đầu tiên giải quyết việc lặp lại dữ liệu đào tạo, không được xem xét trong nghiên cứu của chúng tôi. Sửa đổi thứ hai liên quan đến tương tác giữa số tham số mô hình, N, và kích thước dataset, D. Cụ thể, nó đặt ra rằng đường cong loss như một hàm của D, được biểu diễn là B/D^β, bị ảnh hưởng bởi N. Tương tác giữa số tham số mô hình và kích thước dataset này cũng được phản ánh trong định luật mở rộng OpenAI. Tuy nhiên, phiên bản SMS law của chúng tôi, như được chi tiết trong Eq. 6, đơn giản hơn và mang lại kết quả vượt trội so với mô hình tương ứng trong framework OpenAI.

Động lực để fit các định luật mở rộng là tối ưu hóa loss dưới ranh giới của tài nguyên tính toán. Quá trình này được công thức hóa như việc giảm thiểu validation cross-entropy loss L, tuân theo các ràng buộc được áp đặt bởi tài nguyên tính toán có sẵn (C), cụ thể là floating-point operations per second (FLOPs), như được ký hiệu bên dưới:

arg min N,D L(N, D) s.t. FLOPs (N, D) = C (7)

Với mô hình của chúng tôi được đào tạo trên dữ liệu gần như không lặp lại và chất lượng cao, chúng tôi sử dụng training loss thay vì validation loss cho ứng dụng định luật mở rộng.

8.2 Định luật Mở rộng NEO

Chúng tôi đào tạo các mô hình với kích thước 250M, 460M và 980M tham số sử dụng 1000B tokens dữ liệu đào tạo. Những mô hình này sau đó được sử dụng để dự đoán định luật mở rộng, hướng dẫn việc đào tạo một mô hình với 7.8B tham số trên 3.07T (3065B) tokens trong giai đoạn 1. Để đánh giá fit của định luật mở rộng, chúng tôi sử dụng Huber loss (δ = 1e−3) giữa logloss thực tế và logloss được dự đoán, cùng với giá trị R2 giữa loss thực và loss được dự đoán. Tối ưu hóa định luật mở rộng được thực hiện bằng thuật toán LBFGS. Phương pháp này được áp dụng nhất quán trên định luật Chinchilla và symbolic music scaling law. Bằng cách tận dụng những phương pháp này, chúng tôi nhằm đảm bảo độ chính xác và độ tin cậy của các dự đoán định luật mở rộng, cho phép đào tạo hiệu quả các mô hình ngôn ngữ quy mô lớn.

Hình 6 minh họa các giá trị training loss cùng với dự đoán định luật Chinchilla. Mặc dù định luật Chinchilla fit tốt, với đường cong loss được dự đoán nằm trong biến động của đường cong loss thực tế, xu hướng của nó có vẻ phẳng hơn so với đường cong loss thực tế. Loss thực tế giảm nhanh hơn so với dự đoán bởi công thức Chinchilla (tức là B/D^β), gợi ý dataset của chúng tôi với các corpus chất lượng cao đa dạng có thể giảm thêm giá trị loss khi D lớn. Để giải quyết sự khác biệt giữa dự đoán Chinchilla và quan sát này, chúng tôi giới thiệu phương trình sau, được ký hiệu là định luật mở rộng NEO, bao gồm một regularization term bổ sung log(D) cho datasets chứa vài triệu tokens trên các corpus khác nhau:

L(N, D) = A/N^α + B/D^β + E − d·log(D) (8)

Lưu ý rằng mặc dù regularization term −d·log(D) về mặt lý thuyết dẫn đến không có giới hạn dưới về loss khi D tiến tới âm vô cùng, gợi ý sự không hoàn hảo tiềm năng của công thức, giá trị d thường nằm trong thí nghiệm của chúng tôi giữa 1e-2 và 3e-2. Do đó, đối với kích thước dataset nhỏ hơn hàng trăm triệu tokens, loss vẫn trong phạm vi hợp lý.

Từ Bảng 8 sau đây, chúng tôi quan sát thấy rằng phương trình định luật mở rộng NEO mang lại kết quả tốt hơn đáng kể trên tập đào tạo và tập kiểm tra.

Bảng 8: So sánh parametric fitting trên R2 và Huber Loss của các định luật mở rộng khác nhau.

[THIS IS TABLE: Shows comparison of parametric fitting with R2 Value and Huber Loss for train and test sets across different scaling laws including Chinchilla Law, OpenAI Law, SMS Law, and NEO Scaling Law]

Dưới dự đoán của định luật mở rộng NEO và ràng buộc tài nguyên tính toán của 1.5×10^23 FLOPs, cấu hình tối ưu là đào tạo một mô hình 10B tham số với 2.5T tokens, cung cấp giá trị loss được dự đoán là 0.6597. Để đảm bảo khả năng so sánh với các mô hình baseline, chúng tôi chọn giữ kích thước mô hình của chúng tôi ở 7.8B tham số, tương tự như mô hình Llama-base. Cấu hình này với mô hình 7.8B tham số với 3.07T tokens yêu cầu ít tài nguyên tính toán hơn một chút và dẫn đến giá trị loss dự đoán tương tự (0.6618). Trong khi đó, sau khi đào tạo, chúng tôi quan sát thấy rằng training loss thực trong cấu hình này là 0.6591, gần với giá trị loss dự đoán và chứng minh hiệu quả của định luật mở rộng NEO.

8.3 Tổng quát hóa Định luật Mở rộng NEO

Định luật mở rộng NEO có thể áp dụng cho phạm vi rộng hơn các mô hình ngoài MAP-Neo. Cụ thể, trong Hình 7, chúng tôi minh họa kết quả fit của định luật mở rộng Chinchilla (đường gạch ngang vàng) và định luật mở rộng NEO (đường liền đỏ) đối với DeepSeek LLM [28] với 7B và 67B tham số, cũng được tiền huấn luyện trên dataset với nhiều corpus bao gồm tiếng Trung, tiếng Anh và codes.

Chúng tôi quan sát thấy rằng đối với các kích thước mô hình lớn nhất (tức là MAP-Neo-7B và DeepSeek-67B), các dự đoán của Định luật Chinchilla có xu hướng đánh giá thấp loss thực tế khi kích thước dataset (D) nhỏ và đánh giá quá cao loss thực tế khi tham số mô hình và dữ liệu đào tạo mở rộng. Ngược lại, các dự đoán của Định luật Mở rộng NEO của chúng tôi tạo ra kết quả fitting tốt hơn khi so sánh với kết quả của Định luật Chinchilla cho MAP-Neo-7B và DeepSeek-67B.

Hình 7: Đường cong loss của dự đoán Định luật Chinchilla và dự đoán định luật Mở rộng NEO cho DeepSeek LLM. Chúng tôi sử dụng giá trị loss từ cả 7B và 67B để fitting và dự đoán.

Chúng tôi đề xuất thêm rằng định luật Mở rộng NEO có thể phù hợp hơn cho tình huống với dataset tiền huấn luyện đa dạng lớn với nhiều nguồn dataset chất lượng cao. Để thảo luận thêm về định luật mở rộng NEO trên các mô hình khác, vui lòng tham khảo Phụ lục A.8.

9 Hạ tầng

Hạ tầng tiên tiến của chúng tôi bao gồm hai thành phần chính: hệ thống xử lý dữ liệu và hệ thống đào tạo. Hệ thống đào tạo được thiết kế để hỗ trợ cả giai đoạn tiền huấn luyện và tinh chỉnh, cho phép phát triển mô hình toàn diện.

Hạ tầng của chúng tôi được thiết kế để xử lý các nhiệm vụ xử lý dữ liệu mở rộng cho cả datasets tiếng Anh và tiếng Trung. Chúng tôi sử dụng các hệ thống mạnh mẽ để đảm bảo khả năng xử lý hiệu quả và có thể mở rộng qua các ngôn ngữ khác nhau. Spark [118] được sử dụng cho distributed computing, và object storage được sử dụng để lưu dữ liệu. Mỗi máy được cấu hình với CPU 64-core, 256GB bộ nhớ và 1TB local disk. Tổng cộng có 94 máy. Đối với xử lý dữ liệu tiếng Trung, tổng cộng có 14 máy. Trong số đó, 6 máy có CPU 96-core và 180GB bộ nhớ, trong khi 8 máy còn lại có CPU 48-core và 190GB bộ nhớ. Network File System (NFS) [84] được sử dụng làm hệ thống lưu trữ tệp phân tán.

Trong giai đoạn tiền huấn luyện, bộ công cụ Megatron-Core được sử dụng vì khả năng đào tạo các mô hình ngôn ngữ quy mô lớn, có đặc trưng lên đến hàng trăm tỷ tham số. So với metric tokens per second (TPS), việc sử dụng Megatron-core đạt được tỷ lệ 7200 TPS khi đào tạo mô hình 7B, vượt trội so với hiệu suất 6400 TPS được quan sát dưới cùng cài đặt mà không sử dụng Megatron-core. Điều này được thực hiện bằng cách sử dụng cả kỹ thuật song song mô hình và dữ liệu.

Chúng tôi implement một số chiến lược để quản lý hiệu quả các datasets lớn và độ phức tạp mô hình. Đầu tiên, chúng tôi giới thiệu các chương trình để xác định và tạm thời loại bỏ các computing nodes bị nhiễm từ pool tài nguyên do lỗi phần mềm hoặc phần cứng bằng inspection, dự đoán và gắn nhãn tự động. Thứ hai, chúng tôi thực hiện sửa đổi đối với Megatron-LM để đặc biệt ngăn chặn các vấn đề overflow được chi tiết trong A.3 khi xử lý các corpus dữ liệu lớn. Cuối cùng, chúng tôi implement các cơ chế phục hồi nhiệm vụ sử dụng các checkpoint iterations được chọn chiến lược để bảo vệ chống lại các lỗi tiềm năng trong quá trình đào tạo. Những cải tiến này đảm bảo hiệu suất tối ưu và độ tin cậy trong các hoạt động đào tạo quy mô lớn của chúng tôi.

Để đảm bảo sử dụng tối ưu tài nguyên tính toán của chúng tôi, thiết kế hạ tầng của chúng tôi kết hợp topology mạng tinh vi và cấu hình phần cứng, tạo điều kiện cho phân phối workload hiệu quả và truyền dữ liệu cho các nhiệm vụ đào tạo mô hình phức tạp. Hạ tầng của chúng tôi sử dụng các kỹ thuật distributed computing để tối ưu hóa việc đào tạo các mô hình của chúng tôi. Cụ thể, mô hình 7B của chúng tôi được đào tạo sử dụng cấu hình H800 với 512 GPUs trên 64 nodes và sử dụng NCCL cho phân phối backend với ib như network interface và mlx5 của InfiniBand hardware để tăng cường giao tiếp inter-GPU. Tensor model parallelism được cấu hình để sử dụng 2 GPUs, phân phối việc thực thi một transformer module đơn qua các đơn vị này để tăng cường hiệu quả. Đối với các mô hình 2B của chúng tôi, chúng tôi sử dụng tất cả 256 GPUs với tensor model parallelism được đặt là 1 để đảm bảo data replication hiệu quả. Chúng tôi tăng cường thêm khả năng mở rộng và hiệu quả bằng cách sử dụng các kỹ thuật tương tự ZeRO-1 để sharding optimizer state. Phương pháp này cho phép quản lý các datasets mở rộng hơn và đào tạo mô hình phức tạp hơn với overhead bộ nhớ giảm đáng kể.

Cluster của chúng tôi bao gồm các máy với dual Intel Xeon CPUs và tám NVIDIA H800 GPUs. Kiến trúc tạo điều kiện cho truyền dữ liệu tốc độ cao, với mỗi CPU socket giao tiếp với hai PCIe Gen4 x16 lanes được kết nối với các PCIe switches chuyên dụng. Những switches này quản lý các kết nối đến local NVMe SSD, RDMA-capable Network Interface Card (NIC) và hai GPUs. Giao tiếp inter-CPU được tạo điều kiện bởi Intel's Ultra Path Interconnect (UPI), với cả hai CPUs được liên kết với dual-port TCP NIC hỗ trợ 100 Gbps. Cấu hình mạng của mỗi máy bao gồm bốn RDMA NICs, mỗi cái cung cấp 200 Gbps băng thông full duplex và khả năng GPU Direct RDMA tích hợp. Đáng chú ý, mảng GPU được kết nối thông qua bốn NVIDIA NVSwitches, cho phép giao tiếp intra-GPU mạnh mẽ với băng thông 400 Gbps. Cấu hình tiên tiến này nhấn mạnh khả năng của cluster để xử lý đào tạo mô hình quy mô lớn với hiệu quả và tốc độ đặc biệt.

Về các kết nối inter-machine của data center của chúng tôi, chúng tôi implement kiến trúc mạng Clos hai lớp nơi mỗi minipod chứa ít nhất 512 H800 servers được kết nối thông qua mạng RDMA tốc độ cao. Trong kiến trúc này, mỗi S0 switch được trang bị 64 ports, mỗi port hỗ trợ băng thông 400 Gbps. Sắp xếp này đảm bảo tỷ lệ convergence mạng là 1:1, một yếu tố quan trọng trong việc duy trì data flow tối ưu và giảm bottlenecks. Kết nối trong cấu trúc này được tổ chức tỉ mỉ sao cho mỗi hai S0 switches phục vụ 32 servers, với tổng cộng 32 S0 switches networking trong mỗi minipod. Thiết lập này thể hiện implementation tiên tiến được thiết kế để tối đa hóa throughput và giảm thiểu latency trong môi trường data center.

10 Đánh giá

Đánh giá kỹ lưỡng chứng minh rằng gia đình mô hình MAP-Neo đạt được hiệu suất truyền cảm hứng cả trên các benchmark tự động của mô hình cơ sở và mô hình chat. So với chuỗi LLM minh bạch trước đây, chúng tôi nhấn mạnh hiệu suất đặc biệt của MAP-Neo về khả năng code, toán học và theo dõi hướng dẫn, không chỉ trao cho MAP-Neo giá trị học thuật và thực tế.

[Continued translation of detailed evaluation results and performance comparisons across multiple benchmarks and tables...]
