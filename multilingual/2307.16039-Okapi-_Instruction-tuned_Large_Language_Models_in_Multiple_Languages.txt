# 2307.16039.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2307.16039.pdf
# File size: 388020 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Okapi: Instruction-tuned Large Language Models in Multiple Languages
with Reinforcement Learning from Human Feedback
Viet Dac Lai1, Chien Van Nguyen1, Nghia Trung Ngo1, Thuat Nguyen1,
Franck Dernoncourt2, Ryan A. Rossi2, Thien Huu Nguyen1
1Dept. of Computer Science, University of Oregon, OR, USA
2Adobe Research, USA
{vietl@cs,chienn,nghian@cs,thien@cs}@uoregon.edu
{franck.dernoncourt,ryrossi}@adobe.com
Abstract
A key technology for the development of large
language models (LLMs) involves instruction
tuning that helps align the models’ responses
with human expectations to realize impressive
learning abilities. Two major approaches for
instruction tuning characterize supervised fine-
tuning (SFT) and reinforcement learning from
human feedback (RLHF), which are currently
applied to produce the best commercial LLMs
(e.g., ChatGPT). To improve the accessibility
of LLMs for research and development efforts,
various instruction-tuned open-source LLMs
have also been introduced recently, e.g., Al-
paca, Vicuna, to name a few. However, existing
open-source LLMs have only been instruction-
tuned for English and a few popular languages,
thus hindering their impacts and accessibility
to many other languages in the world. Among a
few very recent work to explore instruction tun-
ing for LLMs in multiple languages, SFT has
been used as the only approach to instruction-
tune LLMs for multiple languages. This has
left a significant gap for fine-tuned LLMs based
on RLHF in diverse languages and raised im-
portant questions on how RLHF can boost the
performance of multilingual instruction tuning.
To overcome this issue, we present Okapi, the
first system with instruction-tuned LLMs based
on RLHF for multiple languages. Okapi intro-
duces instruction and response-ranked data in
26 diverse languages to facilitate the experi-
ments and development of future multilingual
LLM research. We also present benchmark
datasets to enable the evaluation of generative
LLMs in multiple languages. Our experiments
demonstrate the advantages of RLHF for multi-
lingual instruction over SFT for different base
models and datasets. Our framework and re-
sources are released at https://github.com/
nlp-uoregon/Okapi .
1 Introduction
Pre-trained on massive data, large language mod-
els (LLMs) with hundreds of billions of parame-ters can unlock new emergent abilities that can-
not be achieved with smaller models (Wei et al.,
2022). Large generative models such as GPT-3
(Rae et al., 2021) and OPT-175B (Zhang et al.,
2022) represent some of the most recent advances
in natural language processing (NLP), introduc-
ing a new learning paradigm to prompt LLMs to
successfully solve a range of challenging tasks
in zero-shot and few-shot fashions (Kung et al.,
2022; Choi et al., 2023; Jiao et al., 2023; Guo et al.,
2023). However, as LLMs are trained with the au-
toregressive learning objective, they might exhibit
unintended behaviours from human expectations
(Tamkin et al., 2021; Weidinger et al., 2021; Ken-
ton et al., 2021; Bommasani et al., 2021). To over-
come this issue, instruction fine-tuning has been
proposed as a prominent approach to align LLMs
with human intentions in instructions and conversa-
tions (Christiano et al., 2017; Stiennon et al., 2020;
Sanh et al., 2021; Wei et al., 2021; Ouyang et al.,
2022). Instruction-tuned LLMs can demonstrate
significantly improved capabilities in following hu-
man instructions and avoiding the production of
toxic, biased, or inaccurate texts. As such, two ma-
jor techniques for instruction tuning feature super-
vised fine-tuning (SFT) and reinforcement learning
from human feedback (RLHF) that are leveraged
by the best commercial LLMs such as ChatGPT1
and GPT-42to deliver outstanding dialog perfor-
mance.
Another issue with LLMs pertains to the mas-
sive scales and closed-source nature of the com-
mercial LLMs that greatly restrict accessibility and
the extent of interactions with the technology. To
this end, there have been growing efforts from the
open-source community to create more accessible
LLMs with affordable scales while securing com-
petitive performance as the proprietary LLMs, e.g.,
LLaMA (Touvron et al., 2023), StableLM (Stabil-
1https://openai.com/blog/chatgpt/
2https://openai.com/research/gpt-4arXiv:2307.16039v2  [cs.CL]  2 Aug 2023

--- PAGE 2 ---
ityAI, 2023), Falcon (Almazrouei et al., 2023), and
MTP (MosaicML, 2023). Instruction fine-tuning
has also been applied to these open-source lan-
guage models to improve their abilities to engage
with human, and different instruction datasets have
been collected either from human annotation or
outputs from commercial LLMs to facilitate the
tuning process, e.g., Alpaca (Taori et al., 2023), Vi-
cuna (Chiang et al., 2023), LaMini-LM (Wu et al.,
2023), and Dolly (Conover et al., 2023).
However, the instruction-following abilities of
existing open-source LLMs have been developed
mainly for English and some popular languages
(i.e., using instruction datasets for those languages),
failing to support many other languages of the
world to democratize the technologies to a broader
population (Taori et al., 2023; Chiang et al., 2023;
Wu et al., 2023). To overcome this challenge, a few
contemporary work has explored instruction tun-
ing of multilingual LLMs for multiple languages,
i.e., Phoenix (Chen et al., 2023) and Bactrian-X (Li
et al., 2023). However, their multilingual instruc-
tion tuning efforts are limited to only supervised
fine-tuning (SFT) techniques, which is unable to
examine reinforcement learning with human feed-
back (RLHF) to further boost the performance for
multilingual LLMs.
To fill in this gap, our work aims to develop
Okapi, a open-source framework with RLHF-based
instruction-tuned LLMs for multiple languages to
shed light on their performance compared to the
SFT methods in the multilingual settings. Okapi
will emphasize on less studied languages and open-
source LLMs to better democratize the benefits
of instruction-tuned LLMs and provide resources
for future research in this area. In particular, an
example in the instruction datasets involves an in-
struction, an input text, and a desired response out-
put/demonstration. In SFT, the pre-trained LLMs
are fine-tuned over the instruction triples ( instruc-
tion, input, output ) via supervised learning to pro-
mote their alignment with human expectations.
In RLHF, generated outputs from the SFT-tuned
LLMs are first ranked to provide training signals
for reward functions. Afterward, the SFT-tuned
models will be further optimized via reinforcement
learning utilizing rewards from the trained reward
models. As such, RLHF has been successfully em-
ployed to create effective commercial LLMs (e.g.,
InstructGPT, ChatGPT), owning to its ability to
learn beyond positive examples associated withonly desired demonstrations. By leveraging the
reward models, RLHF can observe lower ranking
scores for less accurate demonstrations to obtain
richer training signals for LLMs. To our knowl-
edge, Okapi is the first work to perform instruction
tuning with RLHF for open-source LLMs over mul-
tiple languages.
To develop Okapi, we need to overcome the
scarcity of necessary instruction datasets in mul-
tiple languages to train and evaluate RLHF mod-
els. Motivated by the 52K instructions from Al-
paca (Taori et al., 2023), we leverage Self-Instruct
(Wang et al., 2023) to generate 106K additional in-
structions in English, introducing a larger dataset to
facilitate RLHF evaluation. Afterward, we utilize
ChatGPT to translate the instructions into a diverse
set of 26 languages, which can handle instruction
examples with programming code via appropriate
prompts to enhance translation quality. In addition,
we introduce a translation-based prompt for Chat-
GPT to produce rankings for multiple responses
of the same instructions from the LLMs, which
will be used to train the reward models for RLHF
experiments. Finally, to measure the performance
of the fine-tuned LLMs in different languages, we
translate three benchmark datasets for LLMs in the
widely-used HuggingFace Open LLM Leaderboard
(HuggingFace, 2023; Gao et al., 2021) into 26 lan-
guages, i.e., ARC (Clark et al., 2018), HellaSwag
(Zellers et al., 2019), and MMLU (Hendrycks et al.,
2021), using ChatGPT. These datasets challenge
LLMs on diverse aspects, e.g., science reasoning,
commonsense inference, world knowledge, and
problem-solving, thus providing comprehensive
evaluations for our models. To summarize, our
contribution in this work is as follows:
•Developing RLHF-tuned LLMs in multi-
ple languages : We present Okapi, the first
instruction-tuned LLM framework, which
are RLHF-based and open-source for mul-
tiple languages. Our framework covers 26
diverse languages, including some under-
studied and low-resource languages for NLP,
e.g., Telugu, Ukrainian, Nepali, and Kan-
nada. Using BLOOM (Scao et al., 2022) and
LLaMA (Touvron et al., 2023) as the base pre-
trained LLMs, our experiments illustrate that
RLHF generally performs better than SFT for
multilingual instruction tuning. Our experi-
ments also highlight the greater challenges
of low-resource languages for multilingual

--- PAGE 3 ---
instruction-tuning of LLMs that should be bet-
ter focused in future research.
•Resource creation for instruction-tuned
LLMs in multiple languages : To cater to our
experiments with multilingual RLHF, we cre-
ate instruction resources for 26 different lan-
guages, including ChatGPT prompts, instruc-
tion datasets, response ranking data, bench-
mark datasets, and fine-tuned LLMs. We re-
lease our data, resources, and models to con-
tribute to the development and research of
multilingual instruction-tuned LLMs in the
future. The resources for our Okapi frame-
work can be found at: https://github.com/
nlp-uoregon/Okapi .
2 Data Preparation
A key requirement for our development of
instruction-tuned LLMs with RLHF involves in-
struction, ranking, and evaluation datasets in mul-
tiple languages, especially for low-resource lan-
guages. To this end, we perform a comprehen-
sive data collection process to prepare necessary
data for our multilingual framework Okapi in 26
languages, divided into four major steps: English
instruction generation, instruction translation, rank-
ing data production, and evaluation data creation.
2.1 English Instruction Generation
An instruction example to tune LLMs often has
three components: an instruction to specify the task,
an input text, and an associated output text (i.e.,
demonstration or label) (Ouyang et al., 2022). As
such, current public instruction datasets for LLMs
mainly cover English or some popular languages,
which are not suitable for our experiments. Also,
we note that a few recent instruction datasets such
as xP3 (Muennighoff et al., 2022) and Flan (Chung
et al., 2022; Longpre et al., 2023) include multilin-
gual data; however, their instructions are still writ-
ten in English. Additionally, these datasets tend to
be converted from NLP task datasets with template
instructions, which cannot reflect the flexibility of
human-written prompts to encourage effective in-
struction following in different languages (Wang
et al., 2023). Consequently, our goal is to develop
instruction datasets with instructions, inputs, and
output texts in multiple languages to better realize
general prompts from human.
To achieve this goal, our strategy is to first obtain
English instructions and then translate them intoother languages. The benefits of our approach con-
cern consistent instruction content across languages
to facilitate performance comparison while taking
advantages of translation systems to enable exam-
ination for more languages. As such, there have
been several English instruction datasets collected
by the open-source community to support instruc-
tion tuning of LLMs with different approaches, e.g.,
Alpaca (Taori et al., 2023), Dolly (Conover et al.,
2023), and LaMini-LM (Wu et al., 2023). How-
ever, to conveniently scale our data and introduce
variations of general instructions, we follow the
instruction generation method in Alpaca, which in
turn employs the Self-Instruct procedure in (Wang
et al., 2023), to produce our English dataset.
Starting with a pool of 175 human-written seed
instructions in English over different topics, at each
time, Alpaca samples several instructions from the
seeds to form an in-context example to prompt the
text-davinci-003 model of OpenAI for new instruc-
tion generation. The generated instructions are
then compared with previous instructions using the
ROUGE score, and instructions whose scores are
greater than a threshold will be retained. Overall,
Alpaca releases 52K instructions for tuning LLMs.
In this work, we apply the same Self-Instruct pro-
cedure as Alpaca to extend its 52K instructions to a
larger dataset for our RLHF-based models in Okapi.
In particular, we generate 106K additional English
instructions from Alpaca with two notable exten-
sions. First, we introduce 30 new human-created
instructions into the seed set from Alpaca to in-
crease its diversity and coverage. Among others,
our new instructions involve prompts for relation
extraction, event extraction, event summarization,
and logical questions that are not recognized in
Alpaca. Second, instead of generating the new
instructions from scratch, we condition our genera-
tion process on the 52K instructions from Alpaca
so a new instruction is only saved if it is different
enough from Alpaca’s and previous instructions
per the ROUGE score criteria. Figure 1 shows the
top 10 most common root verbs and their top direct
noun objects in the 106K generated instructions.
These verbs and nouns represent 11.4% of the en-
tire set, which exhibits diverse intents and patterns
in our instructions for Okapi.
2.2 Instruction Translation
Given the 158K English instructions from Alpaca
and our generation process, we aim to translate

--- PAGE 4 ---
generate
createrewritewriteprovidedescribeidentifysuggestfindnamelist
sentence
story
title
list
sentencestory
program
sentencetensewaytextstorysentenceparagraphfunctionexamplelistsentencetipsentencedifferenceproceswaytypesentencetextwaywaytitleactivitywordwordsentencetextnumbertypecountrywaythingFigure 1: The top 10 most frequent root verbs (inner
circle) and their top 4 direct noun objects (outer circle)
in the 106K generated instructions of Okapi. The in-
structions shown here only represent 11.4% of all the
generated instructions.
them into multiple other languages to obtain data
for our multilingual models in Okapi. Table 1
presents 26 selected languages in our framework.
Using the data ratios rof the languages in Com-
monCrawl3to classify languages as in previous
work (Bang et al., 2023; Lai et al., 2023), our study
encompasses a diverse set of languages, including
8 high-resource languages ( r >1.0), 11 medium-
resource languages ( r >0.1), and 7 low-resource
languages ( r <0.1). Notably, several of our lan-
guages, such as Marathi, Gujarati, and Kannada,
have received limited attention in NLP.
We utilize ChatGPT to translate the 158K En-
glish instructions into 26 target languages for
Okapi. Compared to traditional machine transla-
tion systems, an advantage of ChatGPT for transla-
tion is the ability to use prompts to specify different
expectations for the translated texts to facilitate di-
verse types of instructions. For example, we can
instruct ChatGPT to preserve code in the instruc-
tion examples about programming as we expect
code to be the same in the instructions of differ-
ent natural languages. In addition, as ChatGPT
has been fine-tuned on instruction-style data, we
expect that it can capture the context to better trans-
late our instructions. Figure 2 shows our prompt to
translate English instruction data with ChatGPT.
3http://commoncrawl.orgLanguage CodePop. CC SizeB L(M) (%) Cat.
English en 1,452 45.8786 H ✓ ✓
Russian ru 258 5.9692 H ✓ ✓
German de 134 5.8811 H ✓ ✓
Chinese zh 1,118 4.8747 H ✓
French fr 274 4.7254 H ✓ ✓
Spanish es 548 4.4690 H ✓ ✓
Italian it 68 2.5712 H ✓ ✓
Dutch nl 30 2.0585 H ✓ ✓
Vietnamese vi 85 1.0299 H ✓
Indonesian id 199 0.7991 M ✓
Arabic ar 274 0.6658 M ✓
Hungarian hu 17 0.6093 M ✓ ✓
Romanian ro 29 0.5637 M ✓ ✓
Danish da 6 0.4301 M ✓ ✓
Slovak sk 7 0.3777 M ✓ ✓
Ukrainian uk 33 0.3304 M ✓ ✓
Catalan ca 10 0.2314 M ✓ ✓
Serbian sr 12 0.2205 M ✓ ✓
Croatian hr 14 0.1979 M ✓ ✓
Hindi hi 602 0.1588 M ✓
Bengali bn 272 0.0930 L ✓
Tamil ta 86 0.0446 L ✓
Nepali ne 25 0.0304 L ✓
Malayalam ml 36 0.0222 L ✓
Marathi mr 99 0.0213 L ✓
Telugu te 95 0.0183 L ✓
Kannada kn 64 0.0122 L ✓
Table 1: List of 26 non-English languages in our Okapi
framework along with language codes, numbers of first
and second speakers (the “ Pop.” column), data ratios
in the CommonCrawl corpus, and language categories.
The languages are grouped into categories based on
their data ratios in the CommomCrawl corpus: High
Resource (H, >1%), Medium Resource (M, >0.1%),
and Low Resource (L, >0.01%) (Bang et al., 2023).
Columns “ B” and “ L” indicate if a language is supported
by the multilingual LLMs BLOOM and LLaMa (respec-
tively) or not.
It is important to note that we directly translate
the instruction, input text, and associated output in
each English instruction example of our data. This
is in contrast to the other multilingual instruction-
tuning approaches (Li et al., 2023) that only trans-
late instructions and input texts into a target lan-
guage (using Google Translate); ChatGPT is then
prompted to generate response outputs in the tar-
get language for the instructions and input texts.
The intuition for our approach concerns various
potential issues of ChatGPT, e.g., hallucination,
bias, mathematical reasoning, and toxic content
(Bang et al., 2023; Borji, 2023), that can be exag-
gerated if ChatGPT is used to produce responses
in non-English languages for different types of

--- PAGE 5 ---
Translation Prompt: Translate the values in
the following JSON object into <target lan-
guage> language. You must keep the keys in the
JSON object in English. If a value contains pro-
gramming code, only translate the comments
while preserving the code. Your translations
must convey all the content in the original text
and cannot involve explanations or other un-
necessary information. Please ensure that the
translated text is natural for native speakers with
correct grammar and proper word choices. Your
translation must also use exact terminology to
provide accurate information even for the ex-
perts in the related fields. Your output must only
contain a JSON object with translated text and
cannot include explanations or other informa-
tion.
Figure 2: Translation prompt for ChatGPT for multi-
ple languages in Okapi. We organize our instruction
examples into JSON objects with fields for translation
prompts, instructions, inputs, and outputs send to Chat-
GPT. <target language> is replaced with the selected
languages in our dataset.
tasks/instructions (Lai et al., 2023). The diverse
nature of the possible tasks/instructions will also
make it more challenging to devise appropriate so-
lutions for these problems in multilingual settings.
By generating the instructions and response outputs
in English, we aim to capitalize on the greater per-
formance of LLMs for different NLP tasks in En-
glish to avoid the exaggeration issues and achieve
higher quality instructions in various dimensions.
By transitioning to other languages only via the
translation task with ChatGPT, we can also ded-
icate our effort to overcome diverse multilingual
challenges for instruction tuning to the translation
task, which can allow convenient and effective solu-
tions for further improvement. Table 2 presents the
average lengths of translated prompts and response
outputs for each language in our data. Translations
from Alpaca’s original instructions and our new
generated data are shown separately for convenient
comparison.
2.3 Ranking Data Production
To perform RLHF for a LLM in Okapi, we
need to obtain ranked response outputs from the
model for the same instruction and input to train
a reward model. Concretely, given a LLM M
and a dataset S={inst k, input k}N
k=1with
Npairs of instructions inst kand input textsAlpaca Generated
Language P R P R
English 49.0 56.2 51.2 58.0
Russian 72.1 131.8 76.8 134.6
German 61.1 94.2 64.6 96.0
Chinese 47.2 47.9 49.3 48.4
French 51.6 65.5 54.1 67.0
Spanish 51.3 62.7 53.8 63.8
Italian 57.4 86.5 60.5 87.8
Dutch 60.3 94.8 63.9 96.2
Vietnamese 53.4 71.3 56.0 73.2
Indonesian 48.6 54.9 50.8 56.3
Arabic 50.0 60.5 52.3 61.6
Hungarian 68.4 117.3 72.7 120.0
Romanian 62.9 103.9 66.8 106.7
Danish 59.7 91.7 63.1 94.2
Slovak 65.1 110.2 69.1 113.3
Ukrainian 76.9 149.1 82.2 152.1
Catalan 51.8 65.1 54.3 66.7
Serbian 62.4 102.1 66.2 104.9
Croatian 63.0 102.7 66.7 104.0
Hindi 54.7 69.5 61.2 71.0
Bengali 53.8 65.7 56.7 67.3
Tamil 55.3 65.0 58.0 67.6
Nepali 53.8 65.9 56.5 67.2
Malayalam 57.8 75.1 61.0 77.0
Marathi 53.6 67.7 56.3 69.4
Telugu 57.1 74.4 60.2 75.9
Kannada 55.4 69.5 58.2 71.5
Average 57.1 80.8 60.3 82.6
Table 2: Average lengths of translated prompts (columns
“P”) and response outputs (columns “ R”) for each lan-
guage in our Okapi framework. The lengths are com-
puted according to the number of wordpieces produced
by the tokenizer of BLOOM. We separate the numbers
for the translations from the original Alpaca’s data (52K
instructions) and our new generated data (106K instruc-
tions).
Translation Prompt for Ranking: You will be
given an instruction, an input for the instruction,
and four possible responses for the instruction.
The input can be empty, shown as <empty>.
You need to translate the provided instruction,
input, and responses into English.
Instruction: . . .
Input: . . .
Response 1: . . .
Response 2: . . .
Response 3: . . .
Response 4: . . .
Figure 3: ChatGPT’s prompt to translate target language
data into English.

--- PAGE 6 ---
Ranking Prompt: Given the translated
instruction, input, and responses, you will need
to rank the responses according to three factors:
correctness with respect to the instruction and
input, coherence, and naturalness.
You will need to provide an overall rank for
each response when all the three factors are
considered. The overall rank for a response
must be an integer between 1 and 4 where
1 is for the best response and 4 is the worst
response. You cannot assign the same rank for
two different responses.
The format of your output must be: for
each response: "<Response r>: overall rank:
<1/2/3/4>". The responses must be in original
order. Do not include explanation in your
output.
An Example Output from ChatGPT:
Response 1: 3
Response 2: 1
Response 3: 4
Response 4: 2
Figure 4: ChatGPT’s prompt to rank translated data in
English.
input kfor a target language, we first prompt
Mto generate Toutput responses output k=
{output1
k, . . . , outputT
k}for each pair of instruc-
tion and input text (inst k, input k)(T >1). After-
ward, the responses in output kare ranked accord-
ing to their fitness and quality for the instruction
inst kand input text input k. This ranking data
{inst k, input k, output k}can then be leveraged to
train a reward model to compute a score for each
triple of an instruction, an input text, and a potential
response output using contrastive learning (Ouyang
et al., 2022).
In this work, we also employ ChatGPT to rank
the response outputs for multilingual LLMs. Sim-
ilar to the motivation for our translation-based
approach to obtain instruction data in multiple
languages, our ranking strategy first asks Chat-
GPT to translate the instructions and responses
{inst k, input k, output k}in a target language into
English. The ranking of the responses is then
done over the translated English data to exploit
the greater quality of ChatGPT for English and
limit different challenges associated with mul-
tilingual ranking to the translation task. To
this end, we engage with ChatGPT in a two-
turn dialog to obtain ranking for each example{inst k, input k, output k}in the target language.
The first turn is to translate the example into En-
glish using the prompt in Figure 3 while the sec-
ond turn follows up with the first turn to instruct
ChatGPT to rank the English translated responses
using the ranking prompt in Figure 4. Our two-turn
approach allows ChatGPT to condition on the trans-
lated English data in the first turn for ranking while
ensuring the same format for the ranking output in
the second turn for convenient parsing. Overall, we
obtain ranked response outputs for 42K instructions
sampled from the 106K generated instructions for
each language in Okapi.
2.4 Evaluation Data Creation
The HuggingFace Open LLM Leaderboard (Hug-
gingFace, 2023) recently adopts a suite of tasks
and datasets in the Eleuther AI Language Model
Evaluation Harness framework (Gao et al., 2021)
to facilitate performance assessment and track-
ing of newly developed LLMs. We employ three
datasets in this leaderboard i.e., AI2 Reasoning
Challenge (ARC) (Clark et al., 2018), HellaSwag
(Zellers et al., 2019), and MMLU (Hendrycks et al.,
2021), to evaluate the model performance for our
Okapi framework. All the datasets are organized as
multiple-choice question-answering tasks although
they focus on different types of knowledge and rea-
soning aspects. ARC involves 1170 grade-school
science questions; HellaSwag provides 9162 com-
monsense inference questions that are easy for hu-
mans, but difficult for many state-of-the-art models;
and MMLU assesses accuracy for 13062 questions
over various branches of knowledge (STEM, hu-
manities, social sciences, and more). Nevertheless,
although the LLM community has widely adopted
the leaderboard for performance examination, the
datasets are only provided for English, thus un-
able to evaluate LLMs for the languages in our
work. To this end, we translate the examples of
the three datasets into 26 selected languages using
ChatGPT and the translation prompt in Figure 2.
The translated datasets are then reserved to evaluate
the LLMs in our Okapi framework.
3 Reinforcement Learning with Human
Feedback
We follow three steps to develop a fine-tuned LLM
with RLHF for each target language in our Okapi
framework: supervised fine-tuning, reward model
training, and reinforcement learning.

--- PAGE 7 ---
Supervised Fine-tuning (SFT) : Starting with a
multilingual pre-trained LLM as the base, e.g.,
BLOOM (Scao et al., 2022), we fine-tune the base
model with our instruction dataset for the target
language using supervised learning. In Okapi, the
base model is fine-tuned for three epochs via the
autoregressive objective. Our training process uses
a cosine learning rate schedule with 200warm-up
steps, an initial learning rate of 2 e-5, a batch size of
128, and a weight decay of 0.05. Finally, instead of
leveraging approximation techniques for efficient
fine-tuning, we fine-tune the entire base LLM for
all of its parameters with SFT to accurately un-
derstand the model performance for multilingual
settings.
Reward Model Training : The goal of this step
is to train a reward model for the target language
that will compute reward signals for the reinforce-
ment learning frameworks to further optimize the
SFT-tuned model from the previous step. For each
pair of a prompt and potential response, our re-
ward model returns a scalar value to quantify the
appropriateness of the response with respect to the
instruction and input text in the prompt. We ex-
ploit the response-ranked datasets in Section 2.3
for this training step. Using the ranking informa-
tion, an example to train our reward model for
a language involves an instruction and an input
text (to form a prompt x) along with two sam-
pled responses ycandyrforxfrom our datasets.
Based on the ranking information, we can assume
one of the responses (i.e., yc) is more preferable
than the other (i.e., yr). In the next step, the bi-
nary ranking loss (Ouyang et al., 2022) is em-
ployed to train our reward model, aiming to assign
a higher score r(x, yc)for the preferred response
ycthan the score r(x, yr)foryr:Lreward (θ) =
−E(x,yc,yr)[logσ(rθ(x, yc)−rθ(x, yr))]. For the
training process, we initialize the reward model
for the target language from the SFT-tuned model
from previous step. We train our reward model for
2 epochs with a batch size of 64 and a learning rate
of 1e-5, using the AdamW optimizer.
Reinforcement Learning : With the reward
model established for the target language, the
SFT model undergoes additional fine-tuning
through reinforcement learning (RL) to align it
with human preferences. For this purpose, we
employ the Proximal Policy Optimization (PPO)
algorithm (Ouyang et al., 2022). Specifically,
our training process maximizes the mean rewardof the model via the objective: LRL(ϕ) =
−Ex∼DRL,y∼πϕ(y|x)[rθ(x, y)−βKL (x, y)].
Here, DRLcorresponds to the prompt dis-
tribution, and πϕ(y|x)denotes the policy
or language model with parameters ϕthat
require optimization. πϕ(y|x)is initialized
with the SFT-tuned model πϕ(y|x). Also,
KL(x, y) = DKL(πϕ(y|x)||π0(y|x))is the
Kullback–Leibler divergence to penalize large
deviation of πϕfrom the initial SFT policy π0, and
βis a penalty coefficient.
During the RL training phase, we keep the entire
LLM frozen and solely train the top four layers
for five epochs. We employ the AdamW optimizer
withβ1= 0,9,β2= 0.95, andeps= 1e−8. The
KL coefficient βis set to 0.05, while the weight
decay is 0.1, and the learning rate is 1e−6. In each
PPO iteration, we work with a batch size of 32and
a clip threshold of 0.2in Okapi.
4 Experiments
Our Okapi framework utilizes two multilingual
LLMs: BLOOM (Scao et al., 2022) and LLaMA
(Touvron et al., 2023) as the base models for
the fine-tuning processes. We focus on their 7B-
parameter versions to facilitate the computing re-
sources and achieve fairer comparison. For each
base model and target language, we carry out both
SFT-based and RLHF-based instruction-tuning for
the model in the following manners:
•SFT: The base model is fine-tuned over the
158K translated instructions (i.e., 52K from
Alpaca and 106K from our generation) in the
supervised manner.
•RLHF: The base model is first fine-tuned with
supervised training over 52K translated in-
structions from Alpaca. Afterward, a reward
model is trained to score generated responses
for input prompts using contrastive learning
over the ranked responses for the 42K trans-
lated instructions in Section 2.3. Note that the
ranked responses are sampled from the SFT-
tuned base model over 52K translated Alpaca
instructions from previous step. Finally, given
the reward model, the SFT-tuned base model
is further optimized via reinforcement learn-
ing over 64K remaining translated instructions
from our generation set (Ouyang et al., 2022).
The translated datasets ARC, HellaSwag, and
MMLU are exploited to evaluate the performance

--- PAGE 8 ---
of the models in Okapi. Following the Hugging-
Face Open LLM Leaderboard, the Eleuther AI Lan-
guage Model Evaluation Harness framework (Gao
et al., 2021) is used to compute the model per-
formance over the datasets for each language in
our framework. As a reference, we also report
the performance of the base models BLOOM and
LLaMA in the experiments. Finally, for BLOOM,
we further compare with BLOOMZ (Muennighoff
et al., 2022), which is the fine-tuned version of
BLOOM over the cross-lingual task mixture dataset
xP3 with millions of multilingual instructions to
achieve instruction-following ability.
Language BLOOM BLOOMZ SFT RLHFHigh-ResourceRussian 27.5 25.5 29.2 30.3
German 26.3 25.4 24.9 25.5
Chinese 37.3 37.0 37.9 40.0
French 36.7 37.6 37.6 41.2
Spanish 38.1 37.2 39.7 41.5
Italian 29.0 27.5 29.3 31.3
Dutch 23.1 21.5 24.8 26.1
Vietnamese 33.7 33.5 35.0 36.2
Ave Group 31.5 30.7 32.3 34.0Medium-ResourceIndonesian 36.0 35.9 37.4 38.8
Arabic 31.4 31.2 32.1 33.2
Hungarian 25.9 22.8 25.2 27.5
Romanian 26.9 23.4 27.5 30.3
Danish 24.6 24.6 23.6 25.2
Slovak 24.9 22.5 26.2 27.3
Ukrainian 22.8 23.1 23.6 25.2
Catalan 34.7 35.8 35.1 38.9
Serbian 25.1 23.6 25.6 27.8
Croatian 23.7 22.8 22.7 24.1
Hindi 29.2 28.2 28.5 29.6
Ave Group 27.7 26.7 28.0 29.8Low-ResourceBengali 26.2 25.5 26.8 28.9
Tamil 24.2 25.6 23.7 25.1
Nepali 22.3 22.7 23.4 25.7
Malayalam 26.4 25.1 24.6 24.7
Marathi 27.3 24.8 25.8 26.0
Telugu 24.3 25.8 23.9 24.5
Kannada 24.7 24.6 24.5 24.6
Ave Group 25.1 24.9 24.7 25.6
Average 28.2 27.4 28.4 30.0
Table 3: Performance of the models on the trans-
lated ARC dataset over different languages in Okapi.
BLOOM 7B is used as the base LLM.
Evaluation : Tables 3, 4, and 5 present the per-
formance of the models on the ARC, HellaSwag,
and MMLU datasets (respectively) when BLOOM
is used as the base model. Similarly, Tables 6, 7,
and 8 report the performance with the base model
LLaMA over the three datasets. In the tables, in
addition to the average scores over all languages
for the models, we also include the average scores
for each group of languages (i.e., rows “ Ave Group ”Language BLOOM BLOOMZ SFT RLHFHigh-ResourceRussian 32.5 33.1 32.9 34.2
German 32.4 33.1 34.7 35.9
Chinese 51.2 42.6 51.8 53.8
French 56.6 45.7 55.9 58.7
Spanish 56.7 48.7 56.1 59.0
Italian 40.8 40.3 43.1 44.6
Dutch 31.7 32.3 32.6 34.9
Vietnamese 48.3 40.6 49.0 51.3
Ave Group 43.8 39.6 44.5 46.6Medium-ResourceIndonesian 49.5 42.0 50.0 52.2
Arabic 43.3 39.5 44.3 47.0
Hungarian 30.1 29.8 30.8 32.7
Romanian 31.8 32.3 33.1 35.2
Danish 31.2 31.5 33.8 35.7
Slovak 29.8 29.6 31.4 32.9
Ukrainian 30.0 30.4 32.2 33.6
Catalan 51.2 40.3 50.9 53.8
Serbian 29.9 30.1 30.7 33.7
Croatian 30.0 29.4 30.5 31.6
Hindi 36.4 34.0 37.7 39.7
Ave Group 35.7 33.5 36.9 38.9Low-ResourceBengali 32.8 31.5 33.9 35.4
Tamil 29.4 29.5 30.0 30.4
Nepali 30.9 31.9 32.5 34.1
Malayalam 28.8 29.8 29.7 30.2
Marathi 31.0 31.9 31.7 32.5
Telugu 29.2 30.7 30.0 31.7
Kannada 30.3 30.9 30.7 32.1
Ave Group 30.3 30.9 31.2 32.3
Average 36.8 34.7 37.7 39.5
Table 4: Performance of the models on the translated
HellaSwag dataset over different languages in Okapi.
BLOOM 7B is used as the base LLM.
for high-, medium-, and low-resource languages) to
facilitate the comparisons. As some of our selected
languages (especially the low-resource ones) are
not supported by LLaMA, our tables for the experi-
ments with LLaMA will omit those languages (see
Table 1).
The first observation from the tables is that
RLHF is generally better than SFT for multilingual
fine-tuning of LLMs over different tasks, base mod-
els, and language groups. The improvement of av-
erage performance over all languages can go up to
2.5% on the HellaSwag dataset with LLaMA, thus
demonstrating the advantages of RLHF over SFT
for fine-tuning multilingual LLMs. It is also evi-
dent from the tables that the RLHF-tuned models
can significantly improve the performance of the
original base models (i.e., BLOOM and LLaMa)
for almost all the language groups and tasks, which
further highlights the quality of the generated in-
struction data and the effectiveness of RLHF.
Additionally, we observe that the average per-
formance improvement achieved through RLHF

--- PAGE 9 ---
Language BLOOM BLOOMZ SFT RLHFHigh-ResourceRussian 26.2 25.4 26.5 26.8
German 28.1 25.6 27.0 28.6
Chinese 29.1 27.2 27.7 28.2
French 27.4 27.7 27.7 28.4
Spanish 28.9 27.1 27.8 28.1
Italian 25.7 25.8 25.1 26.0
Dutch 26.4 26.0 26.1 26.0
Vietnamese 28.1 26.3 27.0 27.5
Ave Group 27.5 26.4 26.9 27.5Medium-ResourceIndonesian 26.9 26.3 26.8 27.5
Arabic 27.5 24.4 27.4 27.7
Hungarian 26.9 26.1 25.4 26.3
Romanian 27.4 25.9 27.6 27.4
Danish 27.1 25.2 27.2 26.9
Slovak 26.1 26.3 26.4 26.1
Ukrainian 26.6 25.8 25.9 26.4
Catalan 28.8 26.0 26.7 27.6
Serbian 27.2 25.7 27.5 27.6
Croatian 26.0 26.1 26.4 27.7
Hindi 27.5 25.9 26.8 26.5
Ave Group 27.1 25.8 26.7 27.1Low-ResourceBengali 28.2 25.9 27.1 26.8
Tamil 26.6 26.7 26.1 26.0
Nepali 26.6 25.6 25.5 25.2
Malayalam 26.4 25.2 25.8 25.8
Marathi 26.3 26.0 26.1 26.1
Telugu 26.2 25.7 25.4 25.9
Kannada 26.7 26.0 26.6 26.8
Ave Group 26.7 25.9 26.1 26.1
Average 27.1 26.0 26.6 26.9
Table 5: Performance of the models on the trans-
lated MMLU dataset over different languages in Okapi.
BLOOM 7B is used as the base LLM.
Language LLaMA SFT RLHFHigh-ResourceRussian 32.1 32.8 37.7
German 35.1 37.5 39.7
French 37.3 38.4 38.8
Spanish 36.8 38.7 39.3
Italian 35.8 36.3 39.4
Dutch 33.6 35.2 37.5
Ave Group 35.1 36.5 38.7Medium-ResourceHungarian 29.8 31.4 33.2
Romanian 32.4 33.8 37.5
Danish 32.7 35.1 36.8
Slovak 29.0 34.3 37.2
Ukrainian 32.9 35.7 36.4
Catalan 35.1 36.8 36.9
Serbian 30.8 33.5 35.8
Croatian 33.0 33.8 35.9
Ave Group 32.0 34.3 36.2
Average 33.3 35.2 37.3
Table 6: Performance of the models on the translated
ARC dataset over different languages in Okapi. LLaMA
7B is used as the base LLM.
is more substantial for the ARC and HellaSwagLanguage LLaMA SFT RLHFHigh-ResourceRussian 45.7 46.0 49.1
German 49.9 49.0 52.6
French 55.7 55.6 56.9
Spanish 56.4 55.7 56.6
Italian 52.0 52.5 55.9
Dutch 48.7 48.1 51.3
Ave Group 51.4 51.2 53.7Medium-ResourceHungarian 37.9 38.7 41.0
Romanian 44.9 45.1 48.7
Danish 46.7 47.7 51.7
Slovak 35.9 39.5 43.6
Ukrainian 44.1 46.9 47.7
Catalan 49.6 49.2 49.0
Serbian 41.1 42.6 45.0
Croatian 41.1 42.4 45.2
Ave Group 42.7 44.0 46.5
Average 46.4 47.1 49.6
Table 7: Performance of the models on the translated
HellaSwag dataset over different languages in Okapi.
LLaMA 7B is used as the base LLM.
Language LLaMA SFT RLHFHigh-ResourceRussian 30.2 30.0 30.6
German 29.9 30.4 31.7
French 30.5 31.0 30.7
Spanish 30.3 30.4 30.9
Italian 29.9 30.6 30.4
Dutch 29.8 30.0 31.1
Ave Group 30.1 30.4 30.9Medium-ResourceHungarian 29.0 29.2 30.1
Romanian 29.7 29.8 30.9
Danish 30.0 30.9 31.8
Slovak 29.4 29.6 30.2
Ukrainian 29.4 30.8 31.6
Catalan 30.2 30.3 30.5
Serbian 29.2 29.7 30.4
Croatian 29.3 29.2 30.0
Ave Group 29.5 29.9 30.7
Average 29.8 30.1 30.8
Table 8: Performance of the models on the trans-
lated MMLU dataset over different languages in Okapi.
LLaMA 7B is used as the base LLM.
datasets, while it is less pronounced for the MMLU
dataset. Based on the nature of the datasets, we at-
tribute this phenomenon to the better alignment be-
tween our instruction data for fine-tuning with the
necessary knowledge and reasoning skills in ARC
and HellaSwag than those in MMLU. In particular,
ARC and HellaSwag mainly test the abilities of the
models on basic knowledge (i.e., from 3rd grade to
9th) and commonsense inference while MMLU fo-

--- PAGE 10 ---
cuses on professional knowledge in different areas
(e.g., STEM, social sciences, humanities). As our
instructions are generated with the seeds similar
to Alpaca’s styles (Taori et al., 2023), they tend
to emphasize on general knowledge and basic in-
ference skills, thus more aligning with the ARC
and HellaSwag datasets. To this end, the generated
instructions cannot well activate/complement the
language and knowledge skills related to MMLU
from the LLMs to attain meaningful improvement
from instruction tuning.
Comparing the performance of the models across
language groups, we find that the models tend
to achieve the highest performance for the high-
resource languages, followed by the medium-
resource and low-resource languages across dif-
ferent base models. The performance improvement
of RLHF for low-resource languages is also the
least (based on the base model BLOOM), promot-
ing it a challenging area for further research. In-
terestingly, our fine-tuned BLOOM models with
158K generated instructions can significantly out-
perform BLOOMZ over almost all the languages
for the ARC, HellaSwag, and MMLU datasets us-
ing either SFT or RLHF. For example, the average
performance of RLHF is 4.8% better than those for
BLOOMZ over HellaSwag. As BLOOMZ has fine-
tuned BLOOM over more than 78M multilingual
instructions converted from NLP datasets (Muen-
nighoff et al., 2022), it demonstrates the higher
quality of our generated instructions for multilin-
gual instruction tuning of LLMs.
5 Related Work
We consider two dimensions of related work in
this study, i.e., multilingual tuning and multilingual
evaluation.
Multilingual Tuning : With the introduction of the
Transformer architecture (Vaswani et al., 2017),
various language models have been explored to
boost performance for NLP tasks, including the
encoder models BERT (Devlin et al., 2019) and
RoBERTa (Liu et al., 2019), the decoder models
GPT (Radford et al., 2019; Brown et al., 2020), and
the encoder-decoder models BART (Lewis et al.,
2020) and T5 (Raffel et al., 2020). These language
models are often trained first over English data,
and then extended to other languages in two main
approaches: monolingual and multilingual models.
In the monolingual approach, a language model
is trained specifically for a particular language,e.g., for Spanish (MMG, 2021), Japanese (Wongso,
2021), French (Martin et al., 2020; Kamal Eddine
et al., 2021), Polish (Resources and Technology In-
frastructure, 2021), Sweddish (Moell, 2021), and
Hindi (Parmar, 2021). In contrast, the multilin-
gual approach explores a single language model
that is trained on multilingual texts to serve mul-
tiple languages and achieve knowledge transfer
for lower-resource languages, e.g., the encoder-
only models mBERT (Devlin et al., 2019), XLM-
RoBERTa (Conneau et al., 2020), the decoder-only
models mBART (Liu et al., 2020) and mT5 (Xue
et al., 2021), and the decoder-only models BLOOM
(Scao et al., 2022) and LLaMA (Touvron et al.,
2023).
Based on the pre-trained language models
(PLMs), the most advanced methods for NLP in
different languages involve fine-tuning the PLMs
on training data of the downstream tasks (Min et al.,
2023), leading to state-of-the-art performance for
multilingual Sentence Splitting (Nguyen et al.,
2021a), Dependency Parsing (Kondratyuk and
Straka, 2019), Question Answering (Huang et al.,
2019), and Named Entity Recognition (Pires et al.,
2019) (among others). Additionally, fine-tuning
multilingual PLMs (such as XLM-RoBERTa) has
proven to be an effective technique to enable
zero-shot cross-lingual transfer learning across lan-
guages for various NLP tasks. This convenient
approach allows for a seamless extension of NLP
models to encompass larger sets of languages (Wu
and Dredze, 2019; Karthikeyan et al., 2020; Wu
et al., 2022; Nguyen et al., 2021b; Guzman-Nateras
et al., 2022).
Instruction tuning can be considered as a special
type of fine-tuning techniques for PLMs where gen-
erative PLMs (e.g., GPT) are further trained with
instruction data to accomplish instruction following
and response alignment with human expectations.
Supervised fine-tuning (SFT) is the most common
instruction tuning approach that is leveraged by all
of the existing LLMs, including ChatGPT, Apaca
(Taori et al., 2023), Vicuna (Chiang et al., 2023),
and LaMini-LM (Wu et al., 2023). Reinforcement
learning from human feedback can also be used to
further improve the instruction following abilities
of LLMs (Wei et al., 2021; Ouyang et al., 2022)
although this technique has been less explored by
current open-source LLMs due to the challenges
in obtaining ranking data for the reward models.
For multilingual learning, instruction tuning is only

--- PAGE 11 ---
applied in the form of SFT for non-English lan-
guages using multilingual LLMs, e.g., BLOOM
and LLaMA, in a few contemporary work (Chen
et al., 2023; Li et al., 2023; Muennighoff et al.,
2022). RLHF has not been studied for instruction
tuning for non-English languages.
Multilingual Evaluation : A major hurdle for
research in multilingual learning pertains to the
scarcity of evaluation datasets for NLP tasks in
various languages that hinders model development
and measurement. As such, prior research has in-
vested substantial efforts to tackle this challenge,
introducing multilingual datasets for a diversity of
NLP tasks. These tasks include Universal Depen-
dencies (Nivre et al., 2016), Named Entity Recog-
nition with CoNLL 2002 and 2003 (Sang and Meul-
der, 2002, 2003), Natural Language Inference with
XNLI (Conneau et al., 2018), Information Retrieval
with TyDi (Zhang et al., 2021), Question Answer-
ing with XQuAD (Artetxe et al., 2020), Summa-
rization with XWikis (Perez-Beltrachini and La-
pata, 2021), Event Extraction with MEE (Pouran
Ben Veyseh et al., 2022), and many other tasks with
XGLUE (Liang et al., 2020) and XTREME (Hu
et al., 2020). However, these multilingual datasets
are not specifically designed for evaluation of gen-
erative LLMs as our focus in this work.
To this end, the Eleuther AI Language Model
Evaluation Harness (Gao et al., 2021) provides
an unified framework to evaluate generative lan-
guage models over different knowledge and reason-
ing skills. The HuggingFace Open LLM Leader-
board (HuggingFace, 2023) leverages four key
benchmarks from this framework, i.e., ARC (Clark
et al., 2018), HellaSwag (Zellers et al., 2019), and
MMLU (Hendrycks et al., 2021), and Truthful-
lQA (Lin et al., 2022), which have been widely
adopted to measure progress of LLMs. However,
these datasets are not usable for our multilingual
framework as they only support the evaluation for
English.
6 Conclusion
We present the first framework, called Okapi, on
instruction tuning for LLMs in multiple language
using reinforcement learning from human feedback
(RLHF). To address the scarcity of necessary data
for multilingual instruction tuning, we introduce
instruction and response-ranked data in 26 diverse
languages to enable the training of supervised fine-
tuning models, reward models, and reinforcementlearning frameworks for multilingual LLMs. Our
experiments reveal the benefits of RLHF for mul-
tilingual fine-tuning of LLMs and the challenging
problems of low-resource languages in this area for
future research.
Limitations
Despite our efforts to develop and evaluate
instruction-tuned LLMs in multiple languages us-
ing reinforcement learning from human feedback,
our work suffers from several limitations that can
be improved in future work. First, although we
have attempted to cover a diverse set of 26 lan-
guages, there are still many other languages in the
world that are not considered in our work. Future
work can extend our system to include more lan-
guages, especially for low-resource languages, to
gain a more comprehensive understanding for the
language generalization of the instruction tuning
methods and better democratize the technologies.
Second, our system only leverages the base models
BLOOM and LLaMA with 7B parameters. While
this approach can facilitate the computing infras-
tructure of a larger group of institutions for fur-
ther research, it will be beneficial to support other
types of multilingual base models, e.g., the encoder-
decoder model mT5 (Xue et al., 2021), and other
model scales (e.g., the 13B and 65B models) to
strengthen the system. Third, to obtain instruction
and evaluation data for the development, we au-
tomatically generate instructions in English and
translate them into multiple languages using Chat-
GPT. We also rely on ChatGPT to obtain response-
ranked data for the reward models in RLHF. Al-
though our approach enables the extension to mul-
tiple languages with affordable development costs,
the generated and translated data might involve
unexpected noise. Additionally, they might not
perfectly reflect human-provided instruction data
in different languages. To this end, future work
can improve our system with human-generated in-
struction and evaluation data to further examine
instruction tuning for multilingual LLMs. Finally,
our evaluations only investigate the performance of
the models on benchmark datasets for generative
LLMs, which focus on testing diverse knowledge,
reasoning skills, and truthful generation. Other
important concerns of generative models such as
hallucination, toxicity, and biases are not evalu-
ated explicitly in our experiments. Future work can
study these issues to better characterize instruction

--- PAGE 12 ---
tuning methods in the multilingual settings.
References
Ebtesam Almazrouei, Hamza Alobeidli, and Abdulaziz
Alshamsi et al. 2023. Falcon-40B: an open large
language model with state-of-the-art performance.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020. On the cross-lingual transferability of mono-
lingual representations. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 4623–4637, Online. Association
for Computational Linguistics.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,
and Pascale Fung. 2023. A multitask, multilingual,
multimodal evaluation of chatgpt on reasoning, hal-
lucination, and interactivity. ArXiv , abs/2302.04023.
Rishi Bommasani, Drew A. Hudson, and Ehsan Adeli
et al. 2021. On the opportunities and risks of founda-
tion models. ArXiv , abs/2108.07258.
Ali Borji. 2023. A categorical archive of chatgpt fail-
ures. ArXiv , abs/2302.03494.
Tom Brown, Benjamin Mann, and et al. 2020.
Language models are few-shot learners. ArXiv ,
abs/2005.14165.
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan
Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao
Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang
Wan, Benyou Wang, and Haizhou Li. 2023. Phoenix:
Democratizing chatgpt across languages. ArXiv ,
abs/2304.10453.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Jonathan Choi, Kristin Hickman, Amy Monahan, and
Daniel Schwarcz. 2023. Chatgpt goes to law school.
Available at SSRN .
Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan
Martic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. In
Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS) .
Hyung Won Chung, Le Hou, S. Longpre, and Bar-
ret Zoph et al. 2022. Scaling instruction-finetuned
language models. ArXiv , abs/2210.11416.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv ,
abs/1803.05457.Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451, Online. Association for Computational Lin-
guistics.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina
Williams, Samuel Bowman, Holger Schwenk, and
Veselin Stoyanov. 2018. XNLI: Evaluating cross-
lingual sentence representations. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing , pages 2475–2485, Brus-
sels, Belgium. Association for Computational Lin-
guistics.
Mike Conover, Matt Hayes, and Ankit Mathur et al.
2023. Free dolly: Introducing the world’s first
truly open instruction-tuned llm. https://www.
databricks.com .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Leo Gao, Jonathan Tow, Stella Biderman, and et al.
2021. A framework for few-shot language model
evaluation.
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,
Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng
Wu. 2023. How close is chatgpt to human experts?
comparison corpus, evaluation, and detection. ArXiv ,
abs/2301.07597.
Luis Guzman-Nateras, Minh Van Nguyen, and Thien
Nguyen. 2022. Cross-lingual event detection via
optimized adversarial training. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 5588–5599,
Seattle, United States. Association for Computational
Linguistics.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Xiaodong Song, and
Jacob Steinhardt. 2021. Measuring massive multitask
language understanding.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. XTREME: A massively multilingual multi-
task benchmark for evaluating cross-lingual generali-
sation. In Proceedings of the International Confer-
ence on Machine Learning (ICML) .

--- PAGE 13 ---
Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,
Linjun Shou, Daxin Jiang, and Ming Zhou. 2019.
Unicoder: A universal language encoder by pre-
training with multiple cross-lingual tasks. In Pro-
ceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th In-
ternational Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 2485–2494,
Hong Kong, China. Association for Computational
Linguistics.
HuggingFace. 2023. Open llm leaderboard. https:
//github.com/tatsu-lab/stanford_alpaca .
Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing
Wang, and Zhaopeng Tu. 2023. Is chatgpt a good
translator? yes with gpt-4 as the engine. ArXiv ,
2301.08745.
Moussa Kamal Eddine, Antoine Tixier, and Michalis
Vazirgiannis. 2021. BARThez: a skilled pretrained
French sequence-to-sequence model. In Proceedings
of the 2021 Conference on Empirical Methods in Nat-
ural Language Processing , pages 9369–9390, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
K Karthikeyan, Zihan Wang, Stephen Mayhew, and
Dan Roth. 2020. Cross-lingual ability of multilin-
gual bert: An empirical study. In Proceedings of the
International Conference on Learning Representa-
tions (ICLR) .
Zachary Kenton, Tom Everitt, Laura Weidinger, Ia-
son Gabriel, Vladimir Mikulik, and Geoffrey Irv-
ing. 2021. Alignment of language agents. ArXiv ,
abs/2103.14659.
Dan Kondratyuk and Milan Straka. 2019. 75 languages,
1 model: Parsing Universal Dependencies univer-
sally. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
2779–2795, Hong Kong, China. Association for Com-
putational Linguistics.
Tiffany H Kung, Morgan Cheatham, Arielle Medenilla,
Czarina Sillos, Lorie De Leon, Camille Elepaño,
Maria Madriaga, Rimel Aggabao, Giezel Diaz-
Candido, James Maningo, et al. 2022. Performance
of chatgpt on usmle: Potential for ai-assisted medical
education using large language models. medRxiv .
Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben
Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui,
and Thien Huu Nguyen. 2023. Chatgpt beyond en-
glish: Towards a comprehensive evaluation of large
language models in multilingual learning. ArXiv ,
abs/2304.05613.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji,
and Timothy Baldwin. 2023. Bactrian-x: A multi-
lingual replicable instruction-following model with
low-rank adaptation. ArXiv , abs/2305.15011.
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei
Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin
Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,
Rahul Agrawal, Edward Cui, Sining Wei, Taroon
Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,
Shuguang Liu, Fan Yang, Daniel Campos, Rangan
Majumder, and Ming Zhou. 2020. XGLUE: A new
benchmark dataset for cross-lingual pre-training, un-
derstanding and generation. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6008–6018,
Online. Association for Computational Linguistics.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
TruthfulQA: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 3214–3252, Dublin,
Ireland. Association for Computational Linguistics.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising pre-
training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726–742.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv , abs/1907.11692.
S. Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won
Chung, Yi Tay, Denny Zhou, Quoc V . Le, Barret
Zoph, Jason Wei, and Adam Roberts. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. ArXiv , abs/2301.13688.
Louis Martin, Benjamin Muller, Pedro Javier Or-
tiz Suárez, Yoann Dupont, Laurent Romary, Éric
de la Clergerie, Djamé Seddah, and Benoît Sagot.
2020. CamemBERT: a tasty French language model.
InProceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages 7203–
7219, Online. Association for Computational Lin-
guistics.
Bonan Min, Hayley Ross, Elior Sulem, Amir
Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,
Eneko Agirre, Ilana Heintz, and Dan Roth. 2023.
Recent advances in natural language processing via
large pre-trained language models: A survey. ACM
Computing Survey .

--- PAGE 14 ---
MMG MMG. 2021. Spanish roberta.
Birger Moell. 2021. Swedish roberta.
MosaicML. 2023. Introducing mpt-7b: A new standard
for open-source, commercially usable llms. https:
//www.mosaicml.com/blog/mpt-7b .
Niklas Muennighoff, Thomas Wang, and Lintang
Sutawika et al. 2022. Crosslingual general-
ization through multitask finetuning. ArXiv ,
abs/2211.01786.
Minh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Vey-
seh, and Thien Huu Nguyen. 2021a. Trankit: A
light-weight transformer-based toolkit for multilin-
gual natural language processing. In Proceedings of
the 16th Conference of the European Chapter of the
Association for Computational Linguistics: System
Demonstrations , pages 80–90, Online. Association
for Computational Linguistics.
Minh Van Nguyen, Tuan Ngo Nguyen, Bonan Min, and
Thien Huu Nguyen. 2021b. Crosslingual transfer
learning for relation and event extraction via word
category and class alignments. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 5414–5426, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Haji ˇc, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal Dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC) .
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke E. Miller, Maddie Simens, Amanda Askell, Pe-
ter Welinder, Paul Francis Christiano, Jan Leike, and
Ryan J. Lowe. 2022. Training language models to
follow instructions with human feedback. ArXiv ,
abs/2203.02155.
Suraj Parmar. 2021. Hindi roberta.
Laura Perez-Beltrachini and Mirella Lapata. 2021. Mod-
els and datasets for cross-lingual summarisation. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pages
9408–9423, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How multilingual is multilingual BERT? In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 4996–5001, Flo-
rence, Italy. Association for Computational Linguis-
tics.Amir Pouran Ben Veyseh, Javid Ebrahimi, Franck Der-
noncourt, and Thien Nguyen. 2022. MEE: A novel
multilingual event extraction dataset. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 9603–9613,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.
Jack Rae, Sebastian Borgeaud, and et al. 2021. Scaling
language models: Methods, analysis & insights from
training gopher. ArXiv , abs/2112.11446.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. In Journal of Machine Learning Research .
Common Language Resources and Poland Technol-
ogy Infrastructure. 2021. Polish roberta.
Erik F. Tjong Kim Sang and Fien De Meulder. 2002.
Introduction to the conll-2002 shared task: Language-
independent named entity recognition. In Proceed-
ings of the Conference on Computational Natural
Language Learning (CoNLL) .
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of the Conference on Computational Natural
Language Learning (CoNLL) .
Victor Sanh, Albert Webson, and Colin Raffel et al.
2021. Multitask prompted training enables zero-shot
task generalization. ArXiv , abs/2110.08207.
Teven Scao, Angela Fan, and et al. 2022. Bloom: A
176b-parameter open-access multilingual language
model. ArXiv , abs/2211.05100.
StabilityAI. 2023. Stablelm: Stability ai language
models. https://github.com/stability-AI/
stableLM .
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
Ziegler, Ryan J. Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul Christiano. 2020. Learn-
ing to summarize from human feedback. ArXiv ,
abs/2009.01325.
Alex Tamkin, Miles Brundage, Jack Clark, and Deep
Ganguli. 2021. Understanding the capabilities, limi-
tations, and societal impact of large language models.
ArXiv , abs/2102.02503.
Rohan Taori, Ishaan Gulrajani, and Tianyi Zhang et al.
2023. Stanford alpaca: An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca .

--- PAGE 15 ---
Hugo Touvron, Thibaut Lavril, and Gautier Izacard et al.
2023. Llama: Open and efficient foundation lan-
guage models. ArXiv , abs/2302.13971.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023. Self-instruct: Aligning language
model with self generated instructions.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V . Le. 2021. Finetuned language
models are zero-shot learners. In Proceedings of the
International Conference on Learning Representa-
tions .
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Met-
zler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol
Vinyals, Percy Liang, Jeff Dean, and William Fedus.
2022. Emergent abilities of large language models.
Transactions on Machine Learning Research .
Laura Weidinger, John F. J. Mellor, and Maribeth Rauh
et al. 2021. Ethical and social risks of harm from
language models. ArXiv , abs/2112.04359.
Wilson Wongso. 2021. Japanese roberta.
Linjuan Wu, Shaojuan Wu, Xiaowang Zhang, Deyi
Xiong, Shizhan Chen, Zhiqiang Zhuang, and Zhiy-
ong Feng. 2022. Learning disentangled semantic
representations for zero-shot cross-lingual transfer
in multilingual machine reading comprehension. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 991–1000, Dublin, Ireland. As-
sociation for Computational Linguistics.
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muham-
mad Abdul-Mageed, and Alham Fikri Aji. 2023.
Lamini-lm: A diverse herd of distilled models from
large-scale instructions. ArXiv , abs/2304.14402.
Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas:
The surprising cross-lingual effectiveness of BERT.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 833–844, Hong
Kong, China. Association for Computational Linguis-
tics.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:Human Language Technologies , pages 483–498, On-
line. Association for Computational Linguistics.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-
chine really finish your sentence? In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 4791–4800, Florence,
Italy. Association for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open
pre-trained transformer language models. ArXiv ,
abs/2205.01068.
Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.
2021. Mr. TyDi: A multi-lingual benchmark for
dense retrieval. In Proceedings of the 1st Workshop
on Multilingual Representation Learning , pages 127–
137, Punta Cana, Dominican Republic. Association
for Computational Linguistics.
