# 2305.17179.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2305.17179.pdf
# Kích thước tệp: 1204365 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tác động của Tokenization đối với Mô hình Ngôn ngữ Đa ngôn ngữ:
Đánh giá Phân bổ Từ vựng và Mức độ Trùng lặp giữa các Ngôn ngữ
Tomasz Limisiewicz và Jiˇrí Balhar và David Mare ˇcek
Viện Ngôn ngữ học Hình thức và Ứng dụng, Khoa Toán và Vật lý
Đại học Charles, Prague, Cộng hòa Séc
{limisiewicz, marecek}@ufal.mff.cuni.cz

Tóm tắt
Các mô hình ngôn ngữ đa ngôn ngữ gần đây đã thu hút sự chú ý như một giải pháp đầy hứa hẹn để biểu diễn nhiều ngôn ngữ trong một mô hình duy nhất. Trong bài báo này, chúng tôi đề xuất các tiêu chí mới để đánh giá chất lượng biểu diễn từ vựng và mức độ trùng lặp từ vựng quan sát được trong các bộ tokenizer từ phụ. Các phát hiện của chúng tôi cho thấy rằng sự trùng lặp từ vựng giữa các ngôn ngữ thực sự có thể có hại cho một số nhiệm vụ downstream nhất định (POS, gán nhãn cây phụ thuộc). Ngược lại, NER và các nhiệm vụ cấp câu (truy xuất đa ngôn ngữ, NLI) được hưởng lợi từ việc chia sẻ từ vựng. Chúng tôi cũng quan sát thấy rằng phạm vi bao phủ của các token đặc thù ngôn ngữ trong từ vựng đa ngôn ngữ có tác động đáng kể đến các nhiệm vụ cấp từ. Nghiên cứu của chúng tôi cung cấp hiểu biết sâu sắc hơn về vai trò của tokenizer trong các mô hình ngôn ngữ đa ngôn ngữ và hướng dẫn cho các nhà phát triển mô hình tương lai để chọn tokenizer phù hợp nhất cho ứng dụng cụ thể của họ trước khi thực hiện việc pre-training mô hình tốn kém.¹

1 Giới thiệu
Các mô hình ngôn ngữ đa ngôn ngữ hoạt động đáng ngạc nhiên tốt trong nhiều nhiệm vụ NLP đa dạng cho các ngôn ngữ khác nhau (Devlin et al., 2019; Conneau và Lample, 2019; Conneau et al., 2019). Người ta đã quan sát thấy rằng việc biểu diễn chuỗi đầu vào có tác động đáng kể đến hiệu quả của chúng (Mielke et al., 2021). Trong các mô hình Transformer (Vaswani et al., 2017) được sử dụng rộng rãi đạt được kết quả tối ưu thông qua các nhiệm vụ đa dạng, một phần lớn các tham số được phân bổ trong lớp mã hóa đầu vào.²

Phương pháp độc lập ngôn ngữ phổ biến để biểu diễn văn bản đầu vào là học một từ vựng các chuỗi xuất hiện thường xuyên có thể bao gồm các từ hoặc các phần của từ (Sennrich et al., 2016; Song et al., 2021; Kudo và Richardson, 2018).

¹Mã nguồn có sẵn tại: github.com/tomlimi/entangled_in_scripts .
²Ví dụ, trong XLM-Roberta Base, 192M trong số 270M tham số nằm trong lớp embedding đầu vào (khoảng 70%).

[Hình 1: Biểu đồ ánh xạ tác động của phân bổ từ vựng và trùng lặp từ vựng lên hiệu suất mô hình ngôn ngữ...]

Trong công trình này, chúng tôi tập trung vào các đặc điểm của phương pháp tokenization từ phụ trong bối cảnh đa ngôn ngữ. Đóng góp chính của chúng tôi là việc giới thiệu các phương pháp đo lường xem liệu tokenizer có biểu diễn hiệu quả các token có ý nghĩa đặc thù ngôn ngữ trong từ vựng (phân bổ từ vựng) và liệu các đơn vị mà chúng học có được chia sẻ giữa các ngôn ngữ (trùng lặp từ vựng). Chúng tôi đặt ra các câu hỏi sau:

--- TRANG 2 ---
(Q1) Các tokenizer từ phụ khác nhau như thế nào về mức độ trùng lặp và phân bổ của từ vựng đã học?

Để trả lời câu hỏi này, chúng tôi áp dụng các thước đo cho tokenizer thu được bằng hai thuật toán được sử dụng rộng rãi: SentencePiece Unigram LM (Kudo và Richardson, 2018), và BPE (Sennrich et al., 2016). Hơn nữa, chúng tôi đề xuất hai phương pháp học tokenizer trên corpus đơn ngôn ngữ và sau đó kết hợp chúng để cho phép tokenization của văn bản đa ngôn ngữ.

(Q2) Những thuộc tính nào của tokenizer đa ngôn ngữ ảnh hưởng đến chất lượng biểu diễn của LM?

Chúng tôi giải quyết câu hỏi này bằng cách huấn luyện các mô hình ngôn ngữ nhỏ sử dụng các phương pháp tokenization khác nhau. Chúng tôi đánh giá các mô hình trên dự đoán từ bị che và một tập hợp đa dạng các nhiệm vụ downstream: POS, gán nhãn NER, gán nhãn cây phụ thuộc, NLI, và truy xuất câu đa ngôn ngữ.

Sơ đồ đánh giá được đề xuất cung cấp dự đoán tốt về hiệu suất của mô hình ngôn ngữ. Đáng chú ý, chúng tôi cho thấy rằng kết quả hệ thống cải thiện đáng kể khi tokenizer phân bổ nhiều đơn vị từ vựng hơn cho các ngôn ngữ cụ thể. Nghiên cứu của chúng tôi cho thấy rằng khía cạnh này có ảnh hưởng lớn hơn so với trùng lặp từ vựng đối với các nhiệm vụ cấp từ (xem Hình 1). Theo hiểu biết của chúng tôi, các tương tác giữa phân bổ từ vựng đa ngôn ngữ và trùng lặp từ vựng chưa được nghiên cứu trong các công trình trước đây.

2 Tokenization Từ phụ Đa ngôn ngữ

Phần lớn các mô hình hiện đang được triển khai sử dụng tokenization từ phụ như một cách để tiền xử lý văn bản đầu vào. Đầu vào được biểu diễn như một chuỗi các đơn vị từ một từ vựng hữu hạn, có thể được dịch thành biểu diễn số bằng một lớp embedding đầu vào.

Lợi ích của tokenization từ phụ là khả năng thu được biểu diễn số cho các từ có ý nghĩa được sử dụng thường xuyên trong tài nguyên và xử lý các từ ít thường xuyên hơn bằng cách chia chúng thành các từ phụ. Thuộc tính sau giúp giảm thiểu vấn đề từ ngoài từ vựng (OOV) bằng cách chia nhỏ chúng thành các phần nhỏ hơn (từ phụ) đã có sẵn trong từ vựng. Điều này rất quan trọng trong việc xử lý văn bản đa ngôn ngữ, đặc biệt trong các ngôn ngữ có từ vựng lớn và hình thái học phức tạp.

Trong phần tiếp theo, chúng tôi mô tả hai thuật toán tokenization từ phụ được sử dụng rộng rãi:

2.1 Nền tảng: Tokenization Từ phụ

Mã hóa cặp byte BPE: (Sennrich et al., 2016) là một phương pháp tokenization từ phụ lặp đi lặp lại thay thế cặp đơn vị từ vựng thường xuất hiện nhất trong văn bản đầu vào bằng một đơn vị duy nhất. Quá trình bắt đầu bằng việc lấy các ký tự duy nhất của văn bản huấn luyện làm từ vựng ban đầu. Sau đó, chúng ta lấy cặp đơn vị từ vựng thường xuất hiện nhất, hợp nhất cặp đó và thêm nó như một đơn vị mới vào từ vựng. Quá trình này được lặp lại cho đến khi đạt được kích thước từ vựng N được thiết lập trước.

Unigram LM: (Kudo, 2018) là phương pháp thu được từ vựng từ phụ được giới thiệu lần đầu như tokenizer cơ bản của thuật toán SentencePiece (Kudo và Richardson, 2018). Điều kiện tiên quyết là thu được một từ vựng mở rộng, ví dụ, bao gồm tất cả các chuỗi có mặt trong dữ liệu với tối đa, một số ký tự được xác định trước. Thuật toán expectation-maximization được sử dụng để ước tính xác suất của các đơn vị từ vựng. Sau khi EM hội tụ, phần các đơn vị có đóng góp thấp nhất vào khả năng của corpus huấn luyện được loại bỏ khỏi từ vựng. Thủ tục được lặp lại cho đến khi đạt được kích thước từ vựng được thiết lập trước.

2.2 Kết hợp Tokenizer Đơn ngôn ngữ

Rust et al. (2021) quan sát thấy rằng các tokenizer từ phụ được huấn luyện trên dữ liệu đơn ngôn ngữ vượt trội hơn các tokenizer đa ngôn ngữ. Tokenizer đa ngôn ngữ có thể đại diện quá mức các từ phụ đặc thù cho các ngôn ngữ chiếm một phần lớn corpus huấn luyện (ví dụ, tiếng Anh). Hơn nữa, từ vựng của chúng ít có khả năng chứa các morpheme quan trọng trong việc mô hình hóa các ngôn ngữ ít tài nguyên và thay vào đó ưu tiên các chuỗi ký tự ít có ý nghĩa xuất hiện trong các ngôn ngữ.

Để giảm thiểu vấn đề này, chúng tôi đề xuất sử dụng tokenizer đơn ngôn ngữ cho tokenization đa ngôn ngữ. Đầu tiên, các tokenizer Unigram LM được huấn luyện trên các corpus đơn ngôn ngữ riêng biệt. Sau đó, các tokenizer được kết hợp để tạo ra một tokenizer phù hợp cho dữ liệu đa ngôn ngữ. Chúng tôi đề xuất hai phương pháp để kết hợp tokenizer đơn ngôn ngữ:

Tokenization Đặc thù Ngôn ngữ NOOVERLAP: Chúng tôi huấn luyện tokenizer Unigram cho mỗi ngôn ngữ trong L ngôn ngữ được xem xét với cùng kích thước từ vựng cho mỗi ngôn ngữ N/L. Trong tokenization đa ngôn ngữ, chúng tôi áp dụng tokenizer cho một ngôn ngữ cụ thể riêng biệt và tạo ra một token với nhận dạng ngôn ngữ.³ Từ vựng bao gồm L phân đoạn với tổng kích thước N. Tự nhiên, các văn bản được tokenize trong các ngôn ngữ khác nhau sẽ bao gồm các token từ các phân đoạn từ vựng khác biệt. Đáng chú ý, cùng một chuỗi ký tự trong các ngôn ngữ khác nhau có thể được gán các id token khác nhau.

Tokenization Hỗn hợp Ngôn ngữ TOKMIX: Chúng tôi huấn luyện tokenizer Unigram LM cho mỗi ngôn ngữ trong L ngôn ngữ. Sau đó, chúng tôi tính trung bình xác suất đơn vị từ vựng qua các tokenizer, sắp xếp chúng và cắt bớt từ vựng về kích thước từ vựng N được thiết lập trước, giữ lại các đơn vị có xác suất cao nhất.⁴

ˆθ = ∑(i=1 to L) wᵢθᵢ (1)

wᵢ là trọng số được gán cho mỗi ngôn ngữ. Theo mặc định, chúng tôi đặt trọng số bằng nhau và bằng 1/L. Không giống như NOOVERLAP, các đơn vị từ vựng giống nhau đến từ các tokenizer đơn ngôn ngữ khác biệt được hợp nhất thành một đơn vị với xác suất trung bình.

2.3 Cài đặt Huấn luyện Tokenizer và Mô hình

Ban đầu chúng tôi tập trung vào một nhóm 6 ngôn ngữ khác nhau cả về chữ viết và họ ngôn ngữ: tiếng Ả Rập, Trung Quốc, Hy Lạp, Thổ Nhĩ Kỳ, Tây Ban Nha và Anh. Trong các thí nghiệm tiếp theo, chúng tôi mở rộng phương pháp cho 20 ngôn ngữ.

Chúng tôi tải xuống 10% corpus CC có sẵn tại https://data.statmt.org/cc-100/. Theo phương pháp trong (Conneau và Lample, 2019), chúng tôi lấy mẫu phụ dữ liệu của mỗi ngôn ngữ để đảm bảo rằng corpus huấn luyện được cân bằng tốt giữa các ngôn ngữ. Một phương trình xác định kích thước mẫu cₗ cho ngôn ngữ l:

cₗ,α = cₘᵢₙ · |Cₗ|/cₘᵢₙ^α (2)

Trong đó cₘᵢₙ là kích thước mẫu tối thiểu (được định nghĩa bởi ngôn ngữ nhỏ nhất), và Cₗ là tất cả dữ liệu có sẵn cho một ngôn ngữ, α là "tham số cân bằng". Trong các thí nghiệm của chúng tôi, chúng tôi đặt cₘᵢₙ là 10M ký tự, Cₗ là ví dụ 8.8B ký tự cho tiếng Anh. Chúng tôi đặt α là 0.25, tương ứng với hệ số cân bằng được chọn cho XLM-Roberta (Conneau et al., 2019). Dữ liệu huấn luyện cho tokenizer và mô hình là giống nhau. Kích thước từ vựng N được đặt là 120,000. Phụ lục A chứa chi tiết kỹ thuật về phương pháp của chúng tôi.

³Chỉ các token đặc biệt được chia sẻ giữa các ngôn ngữ, ví dụ,

--- TRANG 3 ---
phân đoạn có tổng kích thước N. Tự nhiên, các văn bản được tokenize trong các ngôn ngữ khác nhau sẽ bao gồm các token từ các phân đoạn từ vựng khác biệt. Đáng chú ý, cùng một chuỗi ký tự trong các ngôn ngữ khác nhau có thể được gán các id token khác nhau.

Tokenization Hỗn hợp Ngôn ngữ TOKMIX: Chúng tôi huấn luyện tokenizer Unigram LM cho mỗi ngôn ngữ trong L ngôn ngữ. Sau đó, chúng tôi tính trung bình xác suất đơn vị từ vựng qua các tokenizer, sắp xếp chúng và cắt bớt từ vựng về kích thước từ vựng N được thiết lập trước, giữ lại các đơn vị có xác suất cao nhất.⁴

ˆθ = ∑(i=1 to L) wᵢθᵢ (1)

wᵢ là trọng số được gán cho mỗi ngôn ngữ. Theo mặc định, chúng tôi đặt trọng số bằng nhau và bằng 1/L. Không giống như NOOVERLAP, các đơn vị từ vựng giống nhau đến từ các tokenizer đơn ngôn ngữ khác biệt được hợp nhất thành một đơn vị với xác suất trung bình.

2.3 Cài đặt Huấn luyện Tokenizer và Mô hình

Ban đầu chúng tôi tập trung vào một nhóm 6 ngôn ngữ khác nhau cả về chữ viết và họ ngôn ngữ: tiếng Ả Rập, Trung Quốc, Hy Lạp, Thổ Nhĩ Kỳ, Tây Ban Nha và Anh. Trong các thí nghiệm tiếp theo, chúng tôi mở rộng phương pháp cho 20 ngôn ngữ.

Chúng tôi tải xuống 10% corpus CC có sẵn tại https://data.statmt.org/cc-100/. Theo phương pháp trong (Conneau và Lample, 2019), chúng tôi lấy mẫu phụ dữ liệu của mỗi ngôn ngữ để đảm bảo rằng corpus huấn luyện được cân bằng tốt giữa các ngôn ngữ. Một phương trình xác định kích thước mẫu cₗ cho ngôn ngữ l:

cₗ,α = cₘᵢₙ · |Cₗ|/cₘᵢₙ^α (2)

Trong đó cₘᵢₙ là kích thước mẫu tối thiểu (được định nghĩa bởi ngôn ngữ nhỏ nhất), và Cₗ là tất cả dữ liệu có sẵn cho một ngôn ngữ, α là "tham số cân bằng". Trong các thí nghiệm của chúng tôi, chúng tôi đặt cₘᵢₙ là 10M ký tự, Cₗ là ví dụ 8.8B ký tự cho tiếng Anh. Chúng tôi đặt α là 0.25, tương ứng với hệ số cân bằng được chọn cho XLM-Roberta (Conneau et al., 2019). Dữ liệu huấn luyện cho tokenizer và mô hình là giống nhau. Kích thước từ vựng N được đặt là 120,000. Phụ lục A chứa chi tiết kỹ thuật về phương pháp của chúng tôi.

"<s>" – token bắt đầu câu.
⁴Để tính đến khả năng trùng lặp giữa từ vựng đặc thù ngôn ngữ, chúng tôi đặt kích thước của chúng trên N/L. Điều này đảm bảo rằng từ vựng chung sẽ có ít nhất N token.

3 Đo lường Thuộc tính Tokenizer

Phần này trình bày phương pháp phân tích sâu của chúng tôi để đánh giá các khía cạnh khác nhau của tokenization đa ngôn ngữ. Chúng tôi giới thiệu các thước đo phi tham số mô tả các thuộc tính chính của tokenizer đa ngôn ngữ: chất lượng biểu diễn từ vựng cho các ngôn ngữ cụ thể và sự trùng lặp từ vựng giữa các ngôn ngữ.

Chúng tôi dựa phân tích của mình trên phân phối xác suất thực nghiệm của các đơn vị từ vựng v∈V được tính toán trên corpus huấn luyện cho mỗi ngôn ngữ l:

dₗ,V(v) = f(v,Cₗ)/∑(v∈V) f(v,Cₗ) (3)

Hàm f(v,Cₗ) là số lần xuất hiện của đơn vị từ vựng v trong corpus huấn luyện đơn ngôn ngữ Cₗ.

3.1 Phân bổ Từ vựng

Chúng tôi nhằm mục đích định lượng mức độ từ vựng đa ngôn ngữ biểu diễn tốt các đơn vị từ vựng có ý nghĩa của các ngôn ngữ cụ thể. Trực giác của chúng tôi là biểu diễn từ vựng tốt được đạt được khi: 1. Nó sử dụng một phần lớn từ vựng đa ngôn ngữ, và do đó một phần lớn hơn của lớp embedding được dành riêng cho ngôn ngữ; 2. Văn bản trong ngôn ngữ được chia thành các token dài hơn và có khả năng có ý nghĩa hơn.

Phân bổ Từ vựng: Thứ hạng Trung bình Để đo lường số lượng đơn vị từ vựng có sẵn để mô hình hóa các ngôn ngữ cụ thể, chúng tôi đề xuất một ước tính về thứ hạng trung bình của các đơn vị từ vựng trong phân phối trên corpus đơn ngôn ngữ.⁵ Thước đo này biểu thị có bao nhiêu token thường được xem xét bởi một mô hình ngôn ngữ có quyền truy cập vào thông tin nhận dạng ngôn ngữ nhưng không có ngữ cảnh (LM unigram xác suất).

ARₗ,V = ∑(v∈V) rank(v,dₗ,V)dₗ,V(v) (4)

Trực giác của chúng tôi là mô hình sẽ có thông tin tốt hơn về từ vựng của ngôn ngữ khi từ vựng được phân phối trên một số lượng token lớn hơn vì nhiều tham số của lớp embedding đầu vào sẽ được phân bổ để biểu diễn các đặc trưng đặc thù ngôn ngữ. Hơn nữa, từ vựng lớn hơn có xu hướng bao phủ các đơn vị dài hơn và có ý nghĩa hơn.

⁵Trong ngữ cảnh này, thứ hạng là vị trí của đơn vị v trong từ vựng V được sắp xếp theo thứ tự giảm dần theo phân phối xác suất dₗ,V

--- TRANG 4 ---
Phân bổ Từ vựng: Ký tự trên Token
Theo trực giác trước đó, các token dài hơn có biểu diễn có ý nghĩa hơn. Do đó, chúng tôi đo lường sự phân mảnh văn bản bằng cách tính toán số ký tự trung bình cho một đơn vị từ vựng trong corpus đơn ngôn ngữ Cₗ:

CPTₗ,V = |Cₗ|/|TV(Cₗ)| (5)

TV(Cₗ) là tokenization của corpus với từ vựng V; |Cₗ| là kích thước của corpus đo bằng số ký tự. Chúng tôi chọn số ký tự làm đơn vị để tham chiếu bởi vì nó không dễ bị ảnh hưởng bởi sự khác biệt đa ngôn ngữ về ranh giới từ và độ dài trung bình của từ. Tuy nhiên, lượng thông tin được truyền tải bởi một ký tự duy nhất khác nhau rất lớn với các hệ thống chữ viết, ví dụ, văn bản được viết bằng chữ biểu tượng (ví dụ, Trung Quốc, Nhật Bản) có xu hướng ngắn hơn về số chữ cái so với các văn bản có thông tin tương tự trong chữ viết ngữ âm (ví dụ, Latin) (Perfetti và Liu, 2005).

3.2 Trùng lặp Từ vựng

Một thuộc tính quan trọng khác của từ vựng đa ngôn ngữ là chia sẻ các đơn vị từ vựng giữa các ngôn ngữ. Các công trình trước đây khẳng định rằng sự trùng lặp từ vựng cải thiện việc chuyển giao đa ngôn ngữ cho việc học các nhiệm vụ downstream (Pires et al., 2019; Wu và Dredze, 2019). Chúng tôi đo lường sự trùng lặp như sự khác biệt giữa các phân phối corpus dₗ (được định nghĩa trong phương trình 3). Chúng tôi sử dụng độ phân kỳ Jensen-Shannon.⁶

Chúng tôi áp dụng JSD vì nó đối xứng và có thể áp dụng cho phân phối với các hỗ trợ khác nhau. Điều sau thường xảy ra khi các phân phối được ước tính cho các ngôn ngữ có hệ thống chữ viết khác biệt.

JSD(dₗ₁,V||dₗ₂,V) = 
= (1/2)∑(v∈V) dₗ₁,V(v) log₂ dₗ₁,V(v)/mₗ₁,ₗ₂,V(v) +
+ (1/2)∑(v∈V) dₗ₂,V(v) log₂ dₗ₂,V(v)/mₗ₁,ₗ₂,V(v) (6)

trong đó:
mₗ₁,ₗ₂,V = (1/2)dₗ₁,V + (1/2)dₗ₂,V (7)

⁶Trong tài liệu NLP, JSD cũng được gọi là "bán kính thông tin" (Manning và Schütze, 2001).

JSD bị giới hạn trong khoảng từ 0 đến 1. Giá trị càng thấp, sự trùng lặp giữa các corpus càng lớn.

Một khả năng khác để định lượng sự trùng lặp là đếm các đơn vị từ vựng duy nhất xuất hiện trong các văn bản được tokenize giữa các ngôn ngữ. Ưu điểm của độ phân kỳ là nó phản ánh tần suất của các token được chia sẻ giữa các corpus. Nó cũng ít bị ảnh hưởng bởi việc lựa chọn kích thước dữ liệu được sử dụng để ước tính các phân phối xác suất thực nghiệm (dₗ).

4 Đánh giá Mô hình Ngôn ngữ và Nhiệm vụ Downstream

Trong phần này, chúng tôi trình bày các nhiệm vụ và thước đo để đánh giá các mô hình ngôn ngữ đa ngôn ngữ được huấn luyện với các tokenizer khác nhau.

4.1 Mô hình Ngôn ngữ

Chúng tôi đánh giá hiệu suất mô hình ngôn ngữ có che với thứ hạng nghịch đảo trung bình:

MRR = (1/N)∑(i=1 to N) 1/rank(xᵢ, P̂(·|X\xᵢ)) (8)

trong đó P̂(·|X\xᵢ) là xác suất trên từ vựng của việc dự đoán token xᵢ bởi mô hình cho ngữ cảnh của nó: X\xᵢ.

4.2 Đánh giá Downstream

Các nhiệm vụ downstream được lấy từ XTREME (Hu et al., 2020), là tập hợp các tập dữ liệu đa dạng với các phân chia được xác định trước được sử dụng để đánh giá biểu diễn của mô hình đa ngôn ngữ.

Chúng tôi khảo sát biểu diễn đầu ra của mô hình để đánh giá mức độ hữu ích của biểu diễn đã học cho các nhiệm vụ downstream. Chỉ một lớp tuyến tính bổ sung được huấn luyện cho nhiệm vụ, trong khi biểu diễn mô hình cơ sở được đóng băng. Phương pháp này phù hợp để đánh giá mức độ mô hình pre-trained mã hóa tốt các hiện tượng ngôn ngữ vì nó không thay đổi các tham số đã học trong pre-training trái ngược với fine-tuning thông thường (Conneau et al., 2018a; Belinkov, 2022).

Nhiệm vụ Cấp Từ Tập đầu tiên các nhiệm vụ bao gồm phân loại ở cấp từ đơn hoặc cặp từ. Probe là một lớp tuyến tính nhận biểu diễn từ làm đầu vào và xuất ra một trong các lớp. Đối với biểu diễn từ, chúng tôi lấy embedding đầu ra của mô hình của các từ phụ đầu tiên. Chúng tôi đánh giá kết quả với điểm F1 trung bình qua các lớp (macro-average).

--- TRANG 5 ---
[Bảng 1: Giá trị của các thước đo phân bổ từ vựng cho 4 tokenizer được huấn luyện trên tập ngôn ngữ nhỏ...]

Chúng tôi kiểm tra các nhiệm vụ cú pháp: Từ loại và Gán nhãn phụ thuộc trên Universal Dependencies (de Marneffe et al., 2021) và Nhận dạng Thực thể Có tên trên tập dữ liệu Wikiann (Pan et al., 2017). Trong gán nhãn phụ thuộc, chúng tôi sử dụng edge probe (Tenney et al., 2019) trên biểu diễn của hai từ được kết nối bởi cung phụ thuộc.

Nhiệm vụ Cấp Câu Trong tập nhiệm vụ này, chúng tôi kiểm tra xem liệu mô hình có học được biểu diễn cấp câu nắm bắt ngữ nghĩa của nó và có thể được chuyển giao giữa các ngôn ngữ. Để thu được embedding câu này, chúng tôi tính trung bình biểu diễn đầu ra của mô hình trên tất cả các token trong câu.

Chúng tôi đánh giá Suy luận Ngôn ngữ Tự nhiên trên tập dữ liệu XNLI (Conneau et al., 2018b) và Truy xuất Câu trên corpus song ngữ Tatoeba (Artetxe và Schwenk, 2019). Đối với NLI, chúng tôi sử dụng edge probing. Truy xuất câu được giải quyết bằng thuật toán không giám sát khớp câu dựa trên độ tương tự cosine của chúng. Trong Phụ lục A.3, chúng tôi cung cấp chi tiết của các tập dữ liệu và huấn luyện probe.

4.2.1 Chuyển giao Trong ngôn ngữ so với Đa ngôn ngữ

Đối với tất cả các nhiệm vụ downstream, ngoại trừ truy xuất câu, chúng tôi tính toán hiệu suất trong ngôn ngữ bằng cách huấn luyện probe và đánh giá nó trên dữ liệu kiểm tra held-out trong cùng ngôn ngữ. Chúng tôi định lượng chuyển giao đa ngôn ngữ bằng cách huấn luyện probe trên một ngôn ngữ (nguồn) và đánh giá nó trên tập kiểm tra cho ngôn ngữ khác (đích).

5 Thí nghiệm và Kết quả

Chúng tôi huấn luyện bốn tokenizer cho tập nhỏ gồm 6 ngôn ngữ đa dạng (en, es, tr, el, zh, ar) sử dụng các phương pháp hiện có: Unigram, BPE, và các phương pháp của chúng tôi để hợp nhất tokenizer đơn ngôn ngữ: NOOVERLAP, TOKMIX. Sử dụng các tokenizer này, chúng tôi sau đó huấn luyện bốn mô hình⁷ theo cài đặt của XLM-

⁷Chi tiết về quy trình pretraining và probing được mô tả trong Phụ lục A.2

[Hình 2: Thước đo trùng lặp từ vựng: Độ phân kỳ Jensen-Shannon cho bốn phương pháp tokenization...]

Roberta (Conneau et al., 2019) mà chúng tôi sau đó sử dụng cho các thí nghiệm probing.

Trong Phần 5.1, chúng tôi phân tích phân phối của các đơn vị từ vựng đã học và tính toán các thước đo phân bổ từ vựng và trùng lặp từ vựng được mô tả trong Phần 3. Sau đó trong Phần 5.2, chúng tôi đánh giá các thước đo hiệu suất của mô hình được giới thiệu trong Phần 4 và so sánh chúng với các thước đo cho tokenizer.

Tiếp theo, chúng tôi lặp lại phân tích cho tập rộng hơn gồm 20 ngôn ngữ đa dạng (bao gồm sáu ngôn ngữ đã đề cập trước đó và: he, ka, ur, hi, mr, th, ta, te, bg, ru, sw, vi, fr, de) với ba phương pháp tokenization được sử dụng trong ba mô hình pre-trained. Trong cài đặt này, chúng tôi không sử dụng tokenizer NOOVERLAP, không thể được huấn luyện hiệu quả do cần phải hạn chế từ vựng cho mỗi ngôn ngữ xuống N/L = 6,000.

5.1 Đánh giá Thuộc tính của Tokenizer

Phân bổ từ vựng khác nhau đáng kể qua các ngôn ngữ và phương pháp tokenization. Bảng 1 cho thấy rằng thứ hạng trung bình khác biệt đáng chú ý giữa các ngôn ngữ. AR cao nhất được quan sát thấy ở tiếng Trung Quốc, được gây ra bởi việc chữ viết biểu tượng yêu cầu dung lượng từ vựng mở rộng để mã hóa tất cả các ký tự.

Phân bổ từ vựng đa ngôn ngữ phụ thuộc cao vào phương pháp tokenization được sử dụng. Từ vựng học được với Unigram kém hiệu quả hơn BPE và

--- TRANG 6 ---
[Bảng 2: Kết quả đánh giá trung bình cho thuộc tính trong ngôn ngữ và nhiệm vụ...]

TOKMIX cả về thứ hạng trung bình và ký tự trên token. Bảng 7 trình bày trong Phụ lục cho thấy rằng xu hướng này tồn tại trong các ngôn ngữ ngoại trừ tiếng Trung Quốc. Điều này cho thấy rằng Unigram vanilla của chúng tôi là một bộ học từ vựng đa ngôn ngữ không tối ưu.

Điều quan trọng cần lưu ý là NOOVERLAP ghi điểm thậm chí thấp hơn Unigram trong các thước đo phân bổ từ vựng do kích thước từ vựng hạn chế cho mỗi ngôn ngữ và không cho phép trùng lặp. Tuy nhiên, như được hiển thị trong các phần tiếp theo, LM được huấn luyện với tokenizer này có thể đạt được kết quả tốt trên một số nhiệm vụ.

Việc lựa chọn phương pháp tokenization ảnh hưởng đến trùng lặp từ vựng. Hình 2 cho thấy độ phân kỳ Jensen-Shannon giữa từ vựng của sáu ngôn ngữ. Chúng tôi quan sát thấy rằng sự trùng lặp đa ngôn ngữ cao nhất xuất hiện trong từ vựng thu được bởi Unigram, tiếp theo là TOKMIX và BPE. Như mong đợi, chúng tôi không quan sát thấy sự trùng lặp cho cài đặt NOOVERLAP (JSD = 1).

Độ phân kỳ Jensen-Shannon là một dự báo tốt về việc liệu các ngôn ngữ có chia sẻ chữ viết hay không. Đối với tất cả các phương pháp tokenization, độ phân kỳ nhỏ hơn đáng kể trong nhóm góc dưới-phải của các ngôn ngữ sử dụng chữ Latin. Hiệu ứng này thậm chí còn rõ ràng hơn trong hình dung của JSD được tính toán cho hai mươi ngôn ngữ (Hình 8 trong Phụ lục C).

5.2 Thuộc tính Tokenizer Tác động đến Hiệu suất Mô hình Ngôn ngữ

Phân bổ từ vựng cao cải thiện kết quả downstream cho các nhiệm vụ cấp từ. Trong Bảng 2a, chúng tôi quan sát thấy rằng việc lựa chọn phương pháp tokenization tác động đáng kể đến kết quả cho POS, gán nhãn phụ thuộc và NER. Chúng tôi cho rằng điều này là kết quả của việc học biểu diễn từ vựng tốt trong các ngôn ngữ, ví dụ, bởi BPE và TOKMIX. Phân bổ từ vựng cao đặc biệt có lợi cho các nhiệm vụ cấp từ. Trong khi ảnh hưởng đến nhiệm vụ cấp câu (NLI) là tối thiểu.

Đáng chú ý, instance mô hình với tokenizer NOOVERLAP đạt được F1 tốt nhất trong POS và gán nhãn phụ thuộc mặc dù kém hiệu quả trong phân bổ từ vựng. Đó là kết quả của việc học biểu diễn đặc thù ngôn ngữ cho các token đặc biệt hữu ích cho các nhiệm vụ cú pháp.

Hiệu suất MLM tốt hơn không mang lại cải thiện cho các nhiệm vụ downstream. Trong Bảng 2a, chúng tôi quan sát thấy rằng các mô hình hoạt động tốt hơn trên dự đoán token bị che (MRR) có xu hướng tệ hơn trên các nhiệm vụ downstream (POS và NER). Đó là kết quả của các thứ hạng trung bình khác nhau. Càng cao, càng nhiều đơn vị từ vựng một mô hình ngôn ngữ cần xem xét để điền token bị che, làm cho

[Bảng 3: Tương quan Spearman giữa các hệ số nhiệm vụ cho kết quả trong ngôn ngữ và thước đo tokenizer...]

--- TRANG 7 ---
[Bảng 4: Kết quả đánh giá trung bình cho sự trùng lặp đa ngôn ngữ và chuyển giao...]

dự đoán từ bị che khó khăn hơn. Đồng thời, thứ hạng trung bình cao có nghĩa là từ vựng rộng hơn và chứa các đơn vị từ vựng quan trọng cho các nhiệm vụ downstream.

Một lần nữa, xu hướng này không đúng cho kết quả của cài đặt NOOVERLAP, trong đó không gian tìm kiếm cho bài toán từ bị che được hạn chế vào các token đặc thù ngôn ngữ dẫn đến hiệu suất tốt nhất trong MLM và các nhiệm vụ cú pháp (POS và dự đoán nhãn phụ thuộc).

Trong Bảng 3, chúng tôi cho thấy rằng mối quan hệ mạnh mẽ giữa phân bổ từ vựng (thứ hạng trung bình và CPT) và hiệu suất LM (MRR) được hỗ trợ thống kê. Độ dài của các đơn vị token có ảnh hưởng tích cực mạnh mẽ đến kết quả POS, gán nhãn phụ thuộc và NER (r > 0.65) và ảnh hưởng tiêu cực đến MRR (r < −0.9), trong khi nó không ảnh hưởng đáng kể đến kết quả NLI. Tương quan giữa thứ hạng trung bình và MRR, điểm NER yếu hơn nhưng vẫn đáng kể. Hơn nữa, nó tương quan đáng kể với độ chính xác XNLI với hệ số trung bình r = 0.56, mặc dù các thay đổi trong XNLI thấp giữa các tokenizer.

Tác động của trùng lặp từ vựng đến chuyển giao đa ngôn ngữ khác nhau giữa các nhiệm vụ. Chúng tôi quan sát thấy rằng phương pháp NOOVERLAP đạt được kết quả cạnh tranh cho gán nhãn POS. Đáng ngạc nhiên là không có chia sẻ từ vựng cũng cải thiện chuyển giao đa ngôn ngữ trong nhiệm vụ giữa các ngôn ngữ có chữ Latin (được hiển thị trong Bảng 4a và Hình 3b). Chúng tôi nghĩ rằng lý do đằng sau sức mạnh của phương pháp NOOVERLAP là một số token có ý nghĩa khác nhau giữa các ngôn ngữ, ví dụ, từ "a" là mạo từ không xác định trong tiếng Anh và là giới từ trong tiếng Tây Ban Nha.

Tuy nhiên, trùng lặp từ vựng rất quan trọng đối với chuyển giao đa ngôn ngữ trong một số nhiệm vụ. Đặc biệt là NER trong các ngôn ngữ có cùng chữ viết (Hình 3a) và các nhiệm vụ cấp câu. Đối với những nhiệm vụ này, NOOVERLAP kém hiệu quả đáng kể so với các phương pháp tokenization khác. Sự giảm trong các ngôn ngữ có chữ Latin nằm trong khoảng: 6.8-11.3% cho NER và 12.7-15.9% cho truy xuất câu. Trong những trường hợp này, việc sử dụng các token giống nhau có thể chỉ ra rằng các văn bản đề cập đến các thực thể giống nhau giữa các ngôn ngữ, ví dụ, tên thường là các chuỗi giống nhau trong các ngôn ngữ chia sẻ hệ thống chữ viết.

--- TRANG 8 ---
[Hình 3: Chuyển giao đa ngôn ngữ cho các nhiệm vụ POS và NER...]

[Bảng 5: Tương quan Spearman giữa kết quả chuyển giao đa ngôn ngữ và thước đo tokenization...]

Bảng 5 trình bày các tương quan cho điểm chuyển giao đa ngôn ngữ với JSD đo lường trùng lặp từ vựng. Hệ số hỗ trợ quan sát trước đó của chúng tôi rằng sự trùng lặp thấp hơn (do đó JSD cao hơn) cải thiện chuyển giao cho gán nhãn POS và phụ thuộc và làm xấu đi cho các nhiệm vụ khác. Mặc dù, tương quan cho NER không đáng kể.

Phân bổ từ vựng của ngôn ngữ nguồn và đích ảnh hưởng đáng kể đến chuyển giao đa ngôn ngữ. Tương tự như các tương quan trong ngôn ngữ, ảnh hưởng của ký tự trên token mạnh hơn đối với các nhiệm vụ cấp từ, trong khi Thứ hạng Trung bình ảnh hưởng đến các nhiệm vụ cấp câu ở mức độ lớn hơn. Quan sát này nhấn mạnh tầm quan trọng của việc phân bổ một phần đủ của từ vựng cho các ngôn ngữ ít tài nguyên để chuyển giao đa ngôn ngữ tốt hơn.⁸

Kết quả tổng quát hóa cho tập ngôn ngữ lớn hơn. Quan sát chính cho tập sáu ngôn ngữ vẫn giữ trong mô hình được huấn luyện cho hai mươi ngôn ngữ. Bảng 2b cho thấy rằng BPE và TOKMIX đạt được phân bổ từ vựng tốt hơn Unigram dẫn đến kết quả cải thiện cho các nhiệm vụ downstream cấp từ (NER, POS, Gán nhãn phụ thuộc). Do tỷ lệ kích thước vocab với số ngôn ngữ nhỏ hơn, thứ hạng trung bình giảm cho tất cả các phương pháp.

Chúng tôi quan sát trong Bảng 4b rằng trùng lặp từ vựng đa ngôn ngữ cao nhất đối với Unigram và thấp nhất đối với BPE, tương tự như cài đặt sáu ngôn ngữ. Tuy nhiên, mối liên hệ giữa trùng lặp từ vựng và chuyển giao đa ngôn ngữ ít rõ ràng hơn.

⁸Chúng tôi mô tả phân tích tương quan chi tiết trong Phụ lục C.3.

--- TRANG 9 ---
trùng lặp từ vựng cao nhất đối với Unigram và thấp nhất đối với BPE, tương tự như cài đặt sáu ngôn ngữ. Tuy nhiên, mối liên hệ giữa trùng lặp từ vựng và chuyển giao đa ngôn ngữ ít rõ ràng hơn.

6 Công trình Liên quan

Tầm quan trọng của trùng lặp từ vựng. Wu và Dredze (2019); Pires et al. (2019) khẳng định rằng sự trùng lặp đa ngôn ngữ có lợi cho chuyển giao đa ngôn ngữ. Trái ngược với công trình này, họ so sánh sự trùng lặp cho các cặp ngôn ngữ khác nhau chỉ với một tokenizer. Chúng tôi nghĩ rằng quan sát của họ có thể bị nhầm lẫn bởi sự tương đồng về mặt loại hình giữa các ngôn ngữ. Trong các công trình tiếp theo, Conneau et al. (2020) phát hiện rằng việc chia sẻ tham số trong các lớp trên cùng quan trọng hơn đối với tính đa ngôn ngữ so với embedding token giống nhau. Kết quả tương tự được thể hiện bởi Wang et al. (2021); Dufter và Schütze (2020) cho thấy rằng trong các mô hình song ngữ, việc loại bỏ nhân tạo sự trùng lặp từ vựng (tương tự như NOOVERLAP của chúng tôi) không làm xấu đi chuyển giao đa ngôn ngữ. Trái ngược với nhiều phương pháp trước đây, chúng tôi sử dụng probing để đánh giá vì phương pháp này cung cấp cái nhìn sâu sắc tốt hơn về biểu diễn đã học trong pre-training. Tương tự, kết quả của chúng tôi, Malkin et al. (2022); Limisiewicz et al. (2022) quan sát thấy rằng sự khác biệt trong chữ viết có thể, trong một số trường hợp, cải thiện chuyển giao đa ngôn ngữ trong mô hình ngôn ngữ có che và cho các nhiệm vụ downstream.

Tầm quan trọng của phân bổ từ vựng. Tác động của phân bổ từ vựng đến hiệu suất mô hình được nghiên cứu ở mức độ thấp hơn. Zheng et al. (2021) quan sát thấy rằng dung lượng từ vựng hạn chế được phân bổ cho các ngôn ngữ cụ thể cản trở hiệu suất của các nhiệm vụ downstream và do đó đề xuất một phương pháp để thu được phân bổ từ vựng cân bằng hơn trong các ngôn ngữ. Với cùng mục đích, Chung et al. (2020) đề xuất một phương pháp mới để tạo từ vựng đa ngôn ngữ dựa trên việc phân cụm các ngôn ngữ đích và hợp nhất các từ vựng riêng biệt. Gần đây, Liang et al. (2023) dựa trên các yếu tố của cả hai phương pháp và tăng từ vựng để huấn luyện mô hình XLM-V, đạt được kết quả tốt hơn so với tiền nhiệm của nó (XLM-Roberta Conneau et al. (2019)).

Trong bối cảnh đơn ngôn ngữ, Bostrom và Durrett (2020) lập luận rằng tokenization Unigram tạo ra các token từ phụ phù hợp hơn với các đơn vị hình thái học mang lại cải thiện cho các nhiệm vụ downstream. Điều này trái ngược với phát hiện của chúng tôi về hiệu suất kém của Unigram khi áp dụng cho corpus đa ngôn ngữ.

Cải thiện tokenization từ phụ đa ngôn ngữ. Patil et al. (2022) đề xuất một sửa đổi cho thuật toán BPE làm tăng sự trùng lặp giữa các ngôn ngữ tương tự và có lợi cho chuyển giao đa ngôn ngữ. Rust et al. (2021) quan sát thấy rằng các mô hình với tokenizer đơn ngôn ngữ chuyên dụng vượt trội hơn các mô hình đa ngôn ngữ. Quan sát này có thể được sử dụng bằng cách điều chỉnh lớp embedding của mô hình cho ngôn ngữ đích (Pfeiffer et al., 2020; Artetxe et al., 2020; Minixhofer et al., 2022). Tuy nhiên, các phương pháp này yêu cầu sửa đổi mô hình đặc thù ngôn ngữ, hạn chế khía cạnh đa ngôn ngữ của nó.

Các lựa chọn thay thế cho tokenization từ phụ. Có nhiều phương pháp thay thế để nhập văn bản vào các mô hình sâu, chẳng hạn như biểu diễn dựa trên ký tự (Clark et al., 2022), đầu vào byte (Xue et al., 2022), hoặc biểu diễn văn bản đầu vào dưới dạng hình ảnh (Salesky et al., 2021). Mielke et al. (2021) tóm tắt một loạt rộng các phương pháp và chỉ ra rằng chúng cung cấp sự đánh đổi và có thể phù hợp hơn cho một số nhiệm vụ hoặc ngôn ngữ nhất định.

7 Kết luận

Chúng tôi đã giới thiệu một khung mới để đánh giá các tokenizer từ phụ đa ngôn ngữ. Chúng tôi cho thấy rằng phân bổ từ vựng là một khía cạnh quan trọng ảnh hưởng đến kết quả của nhiều nhiệm vụ downstream. Cụ thể, chúng tôi đã quan sát những xu hướng sau:

1. Bao gồm các đơn vị từ vựng dài hơn và đa dạng hơn (phân bổ từ vựng cao hơn) cải thiện kết quả trong ngôn ngữ và chuyển giao đa ngôn ngữ cho các nhiệm vụ cấp từ; 2. Trùng lặp từ vựng có lợi cho chuyển giao đa ngôn ngữ trong các nhiệm vụ cấp câu; 3. Giữa các ngôn ngữ có cùng chữ viết, trùng lặp từ vựng cải thiện chuyển giao cho NER và làm xấu đi cho POS và gán nhãn phụ thuộc.

Kết luận của chúng tôi phù hợp với quan sát của Mielke et al. (2021) rằng không có "giải pháp đạn bạc" tokenizer phù hợp với tất cả mục đích.

Chúng tôi phát hành mã để đo lường thuộc tính tokenizer: github.com/tomlimi/entangled_in_scripts. Chúng tôi tin rằng nó sẽ là một công cụ đánh giá hữu ích cho các nhà phát triển mô hình có thể có cái nhìn sâu sắc tốt hơn về phương pháp tokenization trước khi huấn luyện mô hình tốn kém về mặt tính toán.

--- TRANG 10 ---
Hạn chế

Để đạt được kết quả mạnh mẽ, không thiên vị, chúng tôi quyết định huấn luyện trước trên một số lượng ngôn ngữ nhỏ hơn, cố định phương pháp của chúng tôi và sau đó xác nhận các phát hiện của chúng tôi trên tập ngôn ngữ đầy đủ. Điều này có nghĩa là hai vòng pretraining cần được thực hiện và vì điều đó, chúng tôi đã thu nhỏ mô hình của mình vì lý do hiệu quả tính toán.

Một hạn chế khác của phương pháp của chúng tôi là việc lựa chọn huấn luyện các probe tuyến tính trên biểu diễn từ ngữ cảnh thay vì phương pháp fine-tuning phổ biến hơn. Tuy nhiên, chúng tôi nghĩ rằng probing cung cấp cái nhìn sâu sắc tốt hơn về biểu diễn của mô hình pre-trained.

Tuyên bố Đạo đức

Chúng tôi không xác định được rủi ro đạo đức liên quan đến công trình này.

Lời cảm ơn

Chúng tôi cảm ơn Jindˇrich Libovický, Martin Popel, Gabriel Stanovsky, và các nhà phê bình ACL ẩn danh vì những nhận xét có giá trị và đề xuất cải thiện của họ. Công trình này đã được hỗ trợ bởi grant 338521 của Cơ quan Grant Đại học Charles. Chúng tôi đã sử dụng tài nguyên ngôn ngữ và công cụ được phát triển, lưu trữ và phân phối bởi dự án LINDAT/CLARIAH-CZ của Bộ Giáo dục, Thanh niên và Thể thao Cộng hòa Séc (dự án LM2018101).

Tài liệu tham khảo

[Danh sách tài liệu tham khảo từ trang 10-12 được duy trì nguyên văn bằng tiếng Anh như trong bản gốc]

--- TRANG 13 ---
A Chi tiết Kỹ thuật

A.1 Chi tiết huấn luyện tokenizer

Chúng tôi sử dụng thư viện Huggingface Tokenizers để huấn luyện các tokenizer Unigram và BPE. Chúng tôi giữ các giá trị mặc định cho các tham số huấn luyện. Cụ thể, đối với Unigram, chúng tôi sử dụng độ dài piece tối đa là 16 và hệ số co rút là 0.75. Đối với BPE, chúng tôi sử dụng kích thước bảng chữ cái là 1000 và tần suất hợp nhất tối thiểu là 2. Đối với tất cả các ngôn ngữ, chúng tôi sử dụng SentencePiece (Kudo và Richardson, 2018) cho các kỹ thuật phân đoạn từ thay vì tokenizer từ đặc thù ngôn ngữ.

A.2 Kiến trúc Mô hình và Pre-Training

Trong nghiên cứu này, chúng tôi sử dụng thư viện Huggingface (Wolf et al., 2020) để tiến hành tất cả các thí nghiệm. Kiến trúc mô hình dựa trên XLM-Roberta, mặc dù cho mục đích của chúng tôi, nó đã được thu nhỏ. Cụ thể, kích thước của embedding là 768, số lớp attention là 8, và số đầu attention là 6. Độ dài câu tối đa là 128, và kích thước từ vựng là 120000. Số tham số là 150M và do đó, nhỏ hơn khoảng 2 lần so với mô hình XLM-Roberta base.

Mô hình được pre-train trong 10 epoch với batch size là 1024. Learning rate là 5e-5 với suy giảm tuyến tính và weight decay và 1% warm-up steps. Trong pretraining, chúng tôi sử dụng optimizer AdamW (Loshchilov và Hutter, 2019).

Tổng cộng, chúng tôi đã pretrain 7 mô hình. Các mô hình được huấn luyện trên 3 GPU Nvidia. Các thí nghiệm probing được chạy trên 1 GPU Nvidia với 40GB bộ nhớ (Nvidia A40). Pretraining mất khoảng 17 giờ cho mỗi mô hình 6 ngôn ngữ và 60 giờ cho các mô hình được huấn luyện trên tập đầy đủ 20 ngôn ngữ.

Chúng tôi không theo đuổi bất kỳ nỗ lực tìm kiếm siêu tham số mở rộng nào vì đây không phải là trọng tâm của công việc của chúng tôi. Chúng tôi đã chọn batch size và learning rate tốt nhất cho pre-training dựa trên một vài thử nghiệm.

A.3 Dữ liệu và Huấn luyện Downstream

Các probe được huấn luyện trong 30 epoch với early stopping và batch size 16. Chúng tôi sử dụng learning rate ban đầu là 2e-5. Các tham số huấn luyện khác giống như trong pretraining. Các thí nghiệm probing mất từ 5 đến 180 phút để hoàn thành trên cùng cơ sở hạ tầng được sử dụng cho pretraining. Chúng tôi đã chạy khoảng 360 lần huấn luyện probe.

POS Chúng tôi sử dụng chú thích Từ loại từ Universal Dependencies (de Marneffe et al., 2021). Tập dữ liệu có sẵn cho 17 ngôn ngữ được chúng tôi phân tích (không bao gồm: Swahili, Thai, Georgian). Mỗi từ được gán một trong 17 thẻ POS thô.

NER Chúng tôi sử dụng tập dữ liệu Wikiann (Pan et al., 2017) bao gồm các bài viết Wikipedia với các thực thể có tên được chú thích thuộc ba loại: địa điểm, người và tổ chức trong IOB2. Theo XTREME, chúng tôi sử dụng các phân chia dữ liệu cân bằng từ (Rahimi et al., 2019).

Gán nhãn phụ thuộc Như trong Từ loại, chúng tôi sử dụng Universal Dependencies (de Marneffe et al., 2021) cho các chú thích quan hệ phụ thuộc. Chúng tôi sử dụng treebank UD lớn nhất có sẵn cho mỗi ngôn ngữ. Đối với mỗi từ, chúng tôi dự đoán một trong 37 quan hệ universal với từ đầu của nó. Vì quan hệ giữa hai từ, chúng tôi sử dụng việc nối các biểu diễn hai từ cùng với tích element-wise của chúng làm đầu vào cho probe ([hw1;hw2;hw1⊙hw2]).

NLI Chúng tôi sử dụng tập dữ liệu XNLI (Conneau et al., 2018b) cho Suy luận Ngôn ngữ Tự nhiên. Chúng tôi huấn luyện probe phân loại tuyến tính trên việc nối hai vector câu và tích element-wise của chúng: [hs1;hs2;hs1⊙hs2]. Chúng tôi dự đoán một trong hai quan hệ giữa câu đầu tiên (được gọi là premise): mâu thuẫn, kéo theo, hoặc trung tính với câu thứ hai (được gọi là hypothesis). Chúng tôi đánh giá XNLI với độ chính xác của phân loại. XNLI chứa dữ liệu cho 15 ngôn ngữ (không bao gồm: te, ta, mr, he, ka).

Truy xuất Câu Chúng tôi sử dụng tối đa 1,000 câu được căn chỉnh cho các cặp ngôn ngữ từ tập dữ liệu Tatoeba (Artetxe và Schwenk, 2019). Đối với các cặp bao gồm tiếng Anh, chúng tôi sử dụng cùng mẫu như trong bộ sưu tập dữ liệu XTREME. Đối với các cặp khác, chúng tôi tự thực hiện lấy mẫu.

Chúng tôi tính toán độ tương tự cosine giữa các biểu diễn câu qua các ngôn ngữ và tìm sự căn chỉnh tốt nhất với thuật toán Hungarian (Kuhn, 1955). Chúng tôi tính toán độ chính xác là số câu được căn chỉnh đúng chia cho tổng số câu.

B Phân tích Tokenizer Sâu

Trong Hình 4, chúng tôi trình bày xác suất của các đơn vị từ vựng, được tính toán trên corpus nối sáu ngôn ngữ, được học bởi các thuật toán tokenization khác nhau.

[Phần còn lại của tài liệu tiếp tục với các hình vẽ, bảng biểu và phân tích chi tiết khác...]
