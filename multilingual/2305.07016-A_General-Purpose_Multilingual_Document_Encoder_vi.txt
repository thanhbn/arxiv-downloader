# 2305.07016.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2305.07016.pdf
# Kích thước tệp: 520387 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Một Bộ Mã Hóa Tài Liệu Đa Ngôn Ngữ Đa Mục Đích
Onur Galo ˘glu1Robert Litschko2Goran Glavaš3
1Nhà Nghiên Cứu Độc Lập
2MaiNLP, Trung tâm Xử lý Thông tin và Ngôn ngữ (CIS), Đại học LMU Munich, Đức
3CAIDAS, Đại học Würzburg
Tóm tắt
Các transformer được tiền huấn luyện đa ngôn ngữ quy mô lớn (MMTs) đã thúc đẩy mạnh mẽ trình độ hiện đại trong NLP đa ngôn ngữ và đặc biệt là việc chuyển giao chéo ngôn ngữ của các mô hình NLP. Trong khi một lượng lớn nghiên cứu đã tận dụng MMTs để khai thác dữ liệu song song và tạo ra nhúng tài liệu song ngữ, ít nỗ lực hơn đã được dành cho việc huấn luyện bộ mã hóa tài liệu đa ngôn ngữ (quy mô lớn) đa mục đích có thể được sử dụng cho cả nhiệm vụ có giám sát và không giám sát ở cấp độ tài liệu. Trong nghiên cứu này, chúng tôi tiền huấn luyện một bộ mã hóa tài liệu đa ngôn ngữ quy mô lớn như một mô hình transformer phân cấp (HMDE) trong đó một transformer tài liệu nông tạo ngữ cảnh cho các biểu diễn câu được tạo ra bởi một bộ mã hóa câu đa ngôn ngữ tiền huấn luyện hiện đại. Chúng tôi tận dụng Wikipedia như một nguồn tài liệu có thể so sánh sẵn có để tạo dữ liệu huấn luyện, và huấn luyện HMDE bằng một mục tiêu tương phản chéo ngôn ngữ, khai thác thêm hệ thống phân cấp thể loại của Wikipedia để tạo ra các mẫu âm tính khó. Chúng tôi đánh giá hiệu quả của HMDE trong hai nhiệm vụ cấp độ tài liệu chéo ngôn ngữ phổ biến và nổi bật nhất: (1) chuyển giao chéo ngôn ngữ cho phân loại tài liệu chủ đề và (2) truy xuất tài liệu chéo ngôn ngữ. HMDE hiệu quả hơn đáng kể so với (i) các tập hợp biểu diễn dựa trên đoạn và (ii) Longformer đa ngôn ngữ. Quan trọng là, nhờ transformer cấp thấp đa ngôn ngữ quy mô lớn, HMDE thành công tổng quát hóa cho các ngôn ngữ không thấy trong tiền huấn luyện cấp độ tài liệu. Chúng tôi công khai phát hành mã và mô hình của chúng tôi.1.

1 Giới thiệu
Các Transformer Đa ngôn ngữ quy mô lớn (MMTs) như XLM-R (Conneau et al., 2020), và mT5 (Xue et al., 2021) đã thúc đẩy mạnh mẽ trình độ hiện đại trong NLP đa ngôn ngữ, đặc biệt cho các ngôn ngữ có nguồn lực trung bình được bao gồm trong tiền huấn luyện của chúng, cho phép chuyển giao chéo ngôn ngữ hiệu quả của các mô hình NLP đặc thù nhiệm vụ từ các ngôn ngữ có nhiều dữ liệu huấn luyện sang các ngôn ngữ có ít hoặc không có dữ liệu nhiệm vụ được chú thích. Là các mô hình ngôn ngữ transformer tiêu chuẩn, MMTs xử lý văn bản tuyến tính – như một chuỗi phẳng các token, điều này đã – trong ngữ cảnh đơn ngôn ngữ – được chứng minh là không tối ưu cho các nhiệm vụ cấp độ tài liệu (ví dụ: phân loại hoặc truy xuất tài liệu) vì hai lý do chính: (1) nó không tương ứng với bản chất phân cấp của tổ chức tài liệu – tài liệu là chuỗi các đoạn văn (có thể được sắp xếp có ý nghĩa), lần lượt là chuỗi các câu (Zhang et al., 2019; Glavaš and Somasundaran, 2020), và (2) biểu diễn tài liệu dài hơn độ dài đầu vào tối đa của MMTs yêu cầu hoặc cắt bớt tài liệu, dẫn đến mất thông tin có thể liên quan đến nhiệm vụ, hoặc phân đoạn, dẫn đến phân mảnh ngữ cảnh (Ding et al., 2021).

Một số mô hình tạo ra biểu diễn cấp độ tài liệu đã được đề xuất, mặc dù chủ yếu trong lĩnh vực đơn ngôn ngữ (tiếng Anh), với hai hướng nghiên cứu nổi bật. (1) Các bộ mã hóa phân cấp (Pappas and Popescu-Belis, 2017; Pappagari et al., 2019; Zhang et al., 2019; Yang et al., 2020; Glavaš and Somasundaran, 2020; Chalkidis et al., 2022) thường tạo ngữ cảnh cho các biểu diễn cấp độ câu với các tham số cấp độ tài liệu bổ sung (ví dụ: một transformer cấp độ tài liệu bổ sung). Các tham số cấp độ tài liệu này của bộ mã hóa, được thêm vào trên một mô hình ngôn ngữ tiền huấn luyện như BERT (Devlin et al., 2019), thường được huấn luyện trên các tập dữ liệu đặc thù nhiệm vụ lớn, từ phân loại tài liệu (Pappagari et al., 2019) đến tóm tắt (Zhang et al., 2019) và phân đoạn (Glavaš and Somasundaran, 2020). Huấn luyện đặc thù nhiệm vụ của các tham số cấp độ tài liệu cản trở việc chuyển giao các bộ mã hóa như vậy sang các nhiệm vụ khác. (2) Các mô hình attention thưa (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020; Tay et al., 2020) sửa đổi cơ chế attention để giảm độ phức tạp tính toán và do đó có thể mã hóa văn bản dài hơn. Mặc dù các bộ mã hóa văn bản dài phẳng không mô hình hóa bản chất phân cấp của tài liệu, chúng cho phép mã hóa phẳng các tài liệu dài hơn đáng kể.

Trong nghiên cứu này, chúng tôi chứng minh lợi ích của biểu diễn tài liệu phân cấp trong ngữ cảnh đa ngôn ngữ. Chúng tôi đề xuất huấn luyện một mô hình transformer phân cấp (HMDE), kết hợp (i) một bộ mã hóa câu đa ngôn ngữ tiền huấn luyện như một bộ mã hóa cấp thấp với (ii) một transformer cấp cao tạo ngữ cảnh cho các biểu diễn câu với nhau và từ đó chúng tôi tạo ra biểu diễn tài liệu. Không giống như trong thiết lập đơn ngôn ngữ, nơi dữ liệu đặc thù nhiệm vụ thường được sử dụng để huấn luyện các tham số của transformer cấp cao (Zhang et al., 2019; Glavaš and Somasundaran, 2020), chúng tôi khai thác thực tế rằng trong ngữ cảnh đa ngôn ngữ có thể tận dụng các căn chỉnh tài liệu chéo ngôn ngữ để hướng dẫn tiền huấn luyện của bộ mã hóa tài liệu, tức là transformer cấp cao của nó. Để làm điều này, chúng tôi tận dụng Wikipedia như một nguồn tài liệu gần như song song sẵn có, và bổ sung khai thác hệ thống phân cấp thể loại của nó để tạo ra các ví dụ âm tính cứng cho mục tiêu tiền huấn luyện tương phản của chúng tôi.

Chúng tôi đánh giá HMDE trong hai nhiệm vụ cấp độ tài liệu (chéo ngôn ngữ) nổi bật nhất: (1) chuyển giao chéo ngôn ngữ cho phân loại tài liệu (XLDC) và (2) truy xuất tài liệu chéo ngôn ngữ (CLIR). Đối với XLDC, như một nhiệm vụ có giám sát, chúng tôi tinh chỉnh HMDE trên dữ liệu đặc thù nhiệm vụ tiếng Anh; trong CLIR, ngược lại, chúng tôi tận dụng HDME theo cách không giám sát, sử dụng nó để tạo ra nhúng tài liệu tĩnh (và transformer cấp thấp của nó để tạo ra nhúng truy vấn). HDME thể hiện hiệu suất vượt trội so với các mô hình cạnh tranh – MMTs với cửa sổ trượt và Longformer đa ngôn ngữ (Yu et al., 2021; Sagen, 2021). Quan trọng là, HMDE tổng quát hóa tốt cho các ngôn ngữ không thấy trong tiền huấn luyện cấp độ tài liệu của nó. Các phân tích sâu hơn của chúng tôi cung cấp thêm những hiểu biết: (i) rằng việc cho phép các cập nhật từ huấn luyện cấp độ tài liệu lan truyền đến bộ mã hóa cấp độ câu là quan trọng (tức là không đóng băng các tham số của bộ mã hóa câu tiền huấn luyện) và (ii) rằng kích thước của corpus tiền huấn luyện cấp độ tài liệu quan trọng hơn tính đa dạng ngôn ngữ của nó (tức là số lượng ngôn ngữ mà nó bao gồm).

2 Bộ Mã Hóa Đa Ngôn Ngữ Phân Cấp
Kiến trúc HMDE, được minh họa trong Hình 1, tương tự như các bộ mã hóa tài liệu phân cấp được huấn luyện đơn ngôn ngữ trong huấn luyện đặc thù nhiệm vụ (ví dụ: (Glavaš and Somasundaran, 2020)): một bộ mã hóa cấp câu (cấp thấp) tạo ra nhúng câu từ các token, trong khi transformer cấp độ tài liệu (cấp cao) tạo ra biểu diễn tài liệu từ một chuỗi nhúng câu. Chúng tôi khởi tạo transformer cấp thấp với các trọng số tiền huấn luyện của một bộ mã hóa câu đa ngôn ngữ (Feng et al., 2022), và huấn luyện toàn bộ mô hình thông qua cấu hình bi-encoder (còn được gọi là kiến trúc Siamese) – nơi chúng tôi tính toán điểm tương tự giữa các biểu diễn của hai tài liệu được tạo ra độc lập với HDME – sử dụng mục tiêu tương phản chéo ngôn ngữ với cả âm tính trong batch và âm tính cứng (Oord et al., 2018).

--- TRANG 2 ---
để giảm độ phức tạp tính toán và do đó có thể mã hóa văn bản dài hơn. Mặc dù các bộ mã hóa văn bản dài phẳng không mô hình hóa bản chất phân cấp của tài liệu, chúng cho phép mã hóa phẳng các tài liệu dài hơn đáng kể.

Trong nghiên cứu này, chúng tôi chứng minh lợi ích của biểu diễn tài liệu phân cấp trong ngữ cảnh đa ngôn ngữ. Chúng tôi đề xuất huấn luyện một mô hình transformer phân cấp (HMDE), kết hợp (i) một bộ mã hóa câu đa ngôn ngữ tiền huấn luyện như một bộ mã hóa cấp thấp với (ii) một transformer cấp cao tạo ngữ cảnh cho các biểu diễn câu với nhau và từ đó chúng tôi tạo ra biểu diễn tài liệu. Không giống như trong thiết lập đơn ngôn ngữ, nơi dữ liệu đặc thù nhiệm vụ thường được sử dụng để huấn luyện các tham số của transformer cấp cao (Zhang et al., 2019; Glavaš and Somasundaran, 2020), chúng tôi khai thác thực tế rằng trong ngữ cảnh đa ngôn ngữ có thể tận dụng các căn chỉnh tài liệu chéo ngôn ngữ để hướng dẫn tiền huấn luyện của bộ mã hóa tài liệu, tức là transformer cấp cao của nó. Để làm điều này, chúng tôi tận dụng Wikipedia như một nguồn tài liệu gần như song song sẵn có, và bổ sung khai thác hệ thống phân cấp thể loại của nó để tạo ra các ví dụ âm tính cứng cho mục tiêu tiền huấn luyện tương phản của chúng tôi.

Chúng tôi đánh giá HMDE trong hai nhiệm vụ cấp độ tài liệu (chéo ngôn ngữ) nổi bật nhất: (1) chuyển giao chéo ngôn ngữ cho phân loại tài liệu (XLDC) và (2) truy xuất tài liệu chéo ngôn ngữ (CLIR). Đối với XLDC, như một nhiệm vụ có giám sát, chúng tôi tinh chỉnh HMDE trên dữ liệu đặc thù nhiệm vụ tiếng Anh; trong CLIR, ngược lại, chúng tôi tận dụng HDME theo cách không giám sát, sử dụng nó để tạo ra nhúng tài liệu tĩnh (và transformer cấp thấp của nó để tạo ra nhúng truy vấn). HDME thể hiện hiệu suất vượt trội so với các mô hình cạnh tranh – MMTs với cửa sổ trượt và Longformer đa ngôn ngữ (Yu et al., 2021; Sagen, 2021). Quan trọng là, HMDE tổng quát hóa tốt cho các ngôn ngữ không thấy trong tiền huấn luyện cấp độ tài liệu của nó. Các phân tích sâu hơn của chúng tôi cung cấp thêm những hiểu biết: (i) rằng việc cho phép các cập nhật từ huấn luyện cấp độ tài liệu lan truyền đến bộ mã hóa cấp độ câu là quan trọng (tức là không đóng băng các tham số của bộ mã hóa câu tiền huấn luyện) và (ii) rằng kích thước của corpus tiền huấn luyện cấp độ tài liệu quan trọng hơn tính đa dạng ngôn ngữ của nó (tức là số lượng ngôn ngữ mà nó bao gồm).

2 Bộ Mã Hóa Đa Ngôn Ngữ Phân Cấp
Kiến trúc HMDE, được minh họa trong Hình 1, tương tự như các bộ mã hóa tài liệu phân cấp

s1: [BOS] w1,1 w1,2w1,3    ...   [EOS]
s2: [BOS] w2,1w2,2w2,3    ...   [EOS]
s3: [BOS] w3,1w3,2w3,3    ...   [EOS]
...d1d2d3Transformer cấp câus1...
s2 s3...
[DBOS]Transformer cấp tài liệuPooling trung bìnhd1 d3 d2...d4
d4...
Chấm điểm theo cặp
(    vs.              ) ...Mục tiêu tương phản

Hình 1: Minh họa HDME: kiến trúc transformer phân cấp được kết hợp với mục tiêu tương phản chéo ngôn ngữ. Màu sắc tài liệu biểu thị các khái niệm Wikipedia: d1 và d2 là các trang của cùng một khái niệm (ví dụ: New York) bằng hai ngôn ngữ khác nhau, L1 và L2; tài liệu d3 và d4 là các trang của các khái niệm khác trong L1. Cặp (d1,d2) là một cặp dương tính (tức là cùng khái niệm) cho mục tiêu huấn luyện tương phản và các cặp (d1,d3) và (d1,d4) là các cặp âm tính tương ứng (tức là các khái niệm khác nhau).

được huấn luyện đơn ngôn ngữ trong huấn luyện đặc thù nhiệm vụ (ví dụ: (Glavaš and Somasundaran, 2020)): một bộ mã hóa cấp câu (cấp thấp) tạo ra nhúng câu từ các token, trong khi transformer cấp độ tài liệu (cấp cao) tạo ra biểu diễn tài liệu từ một chuỗi nhúng câu. Chúng tôi khởi tạo transformer cấp thấp với các trọng số tiền huấn luyện của một bộ mã hóa câu đa ngôn ngữ (Feng et al., 2022), và huấn luyện toàn bộ mô hình thông qua cấu hình bi-encoder (còn được gọi là kiến trúc Siamese) – nơi chúng tôi tính toán điểm tương tự giữa các biểu diễn của hai tài liệu được tạo ra độc lập với HDME – sử dụng mục tiêu tương phản chéo ngôn ngữ với cả âm tính trong batch và âm tính cứng (Oord et al., 2018).

--- TRANG 3 ---
2.1 Mã Hóa Phân Cấp
Vai trò của transformer cấp câu (cấp thấp) là tạo ra biểu diễn câu từ các chuỗi token. Vì điều này, chúng tôi khởi tạo nó với các trọng số tiền huấn luyện (bao gồm nhúng từ phụ) của LaBSE (Feng et al., 2022), một bộ mã hóa câu đa ngôn ngữ hiện đại.2 Nhúng câu là biểu diễn được biến đổi của token đặc biệt bắt đầu chuỗi (BOS). Chuỗi nhúng câu được thu thập với transformer cấp câu sau đó được chuyển tiếp đến transformer cấp độ tài liệu (cấp cao), tạo ngữ cảnh lẫn nhau cho chúng, được đặt trước với một token đặc biệt bắt đầu chuỗi cấp độ tài liệu (DBOS, với một nhúng được khởi tạo ngẫu nhiên). Chúng tôi tạo ra biểu diễn tài liệu bằng cách pooling trung bình các nhúng câu được tạo ngữ cảnh (tức là đầu ra của lớp cuối cùng của transformer cấp độ tài liệu).3

2.2 Mục Tiêu Đa Ngôn Ngữ và Chéo Ngôn Ngữ
Tập dữ liệu huấn luyện của chúng tôi bao gồm các trang Wikipedia được viết bằng một trong n ngôn ngữ (xem §3.1 để biết chi tiết về việc tạo ra các tập dữ liệu huấn luyện khác nhau): gọi L=L1;L2;:::;Ln là tập hợp các ngôn ngữ huấn luyện của chúng tôi. Trong mỗi bước huấn luyện, chúng tôi chọn một batch gồm N cặp tài liệu, f(d(1)1;d(1)2);:::; (d(N)1;d(N)2)g, trong đó d(i)1 và d(i)2 là các trang Wikipedia của cùng một khái niệm nhưng bằng hai ngôn ngữ khác nhau, Lk và Lm∈L. Mỗi tài liệu d(i)1 (tức là tài liệu đầu tiên của mỗi cặp) bổ sung được ghép cặp với một tài liệu d(i)neg – một tài liệu cùng ngôn ngữ Lk với d(i)1 và từ cùng thể loại Wikipedia – đại diện cho một âm tính cứng cho d(i)1 (xem §3.1 để biết chi tiết). Sau đó chúng tôi tính toán và tối thiểu hóa một biến thể của hàm mất InfoNCE phổ biến (Oord et al., 2018) kết hợp âm tính cứng, coi tất cả các tài liệu batch khác d(j)2 như âm tính trong batch (dễ) cho d(i)1:

L=∑(i=1 to N) 1/s(d(i)1;d(i)2) log[e^(s(d(i)1;d(i)neg)/τ) + ∑(j=1 to N)e^(s(d(i)1;d(j)2)/τ)]   (1)

2Chúng tôi tải trọng số LaBSE từ HuggingFace: https://huggingface.co/sentence-transformers/LaBSE
3Chúng tôi cũng thử nghiệm sơ bộ với vector được tạo ngữ cảnh của token DBOS như biểu diễn tài liệu, nhưng điều đó luôn dẫn đến hiệu suất thấp hơn.

với d ∈ Rh là nhúng của d, tức là đầu ra của transformer cấp độ tài liệu (và có kích thước ẩn của transformer cấp cao), s(di;dj) là hàm chấm điểm thu thập sự tương tự giữa hai nhúng tài liệu, và τ là siêu tham số (được gọi là nhiệt độ) của hàm mất InfoNCE. Theo thông lệ chung, chúng tôi sử dụng tương tự cosine làm hàm chấm điểm s.

Lưu ý rằng hàm mất chúng tôi tính toán vừa đa ngôn ngữ vừa chéo ngôn ngữ: tài liệu d(i)1 đến từ bất kỳ ngôn ngữ nào trong |L| ngôn ngữ, và các cặp dương tính (d(i)1;d(i)2) là chéo ngôn ngữ. Trong số các âm tính trong batch, sẽ có các cặp chéo ngôn ngữ cũng như đơn ngôn ngữ (khi d(i)1 và d(j)2 tình cờ là tài liệu được viết bằng cùng ngôn ngữ). Các âm tính cứng của chúng tôi, theo thiết kế, luôn là các cặp đơn ngôn ngữ. Trong khi có thể tạo ra âm tính cứng chéo ngôn ngữ theo cách tương tự (ví dụ: bằng cách ghép cặp bài viết tiếng Anh "France" với bài viết tiếng Ý "Svizzera" (Thụy Sĩ) bao quát một khái niệm khác từ cùng thể loại "Country"), âm tính cứng đơn ngôn ngữ nên khó hơn vì hai biểu diễn tài liệu sẽ có nguồn gốc từ cùng không gian con đặc thù ngôn ngữ của không gian nhúng của transformer (đa ngôn ngữ) cấp thấp (Cao et al., 2020; Wu and Dredze, 2020).

3 Thiết Lập Thí Nghiệm
Đầu tiên chúng tôi mô tả cách chúng tôi tạo ra tập dữ liệu đa ngôn ngữ cho tiền huấn luyện HMDE từ Wikipedia (§3.1). Sau đó chúng tôi mô tả ngắn gọn hai nhiệm vụ đánh giá – chuyển giao chéo ngôn ngữ cho phân loại tài liệu và truy xuất thông tin chéo ngôn ngữ – và các tập dữ liệu tương ứng của chúng (§3.2), tiếp theo là mô tả các baseline – một bộ mã hóa câu đa ngôn ngữ với cửa sổ trượt và một Longformer đa ngôn ngữ (Yu et al., 2021; Sagen, 2021) (§3.3). Chúng tôi cung cấp chi tiết huấn luyện và tối ưu hóa cho tất cả các mô hình trong Phụ lục A.1.

3.1 Tạo Dữ Liệu
Wikipedia đã được tận dụng như một nguồn phù hợp để khai thác corpus có thể so sánh và song song trong nhiều thập kỷ (Ni et al., 2009; Plamadă and Volk, 2013; Schwenk et al., 2021, và các nghiên cứu khác). Chúng tôi bổ sung vào nhóm nghiên cứu khai thác Wikipedia như một nguồn tài nguyên văn bản đa ngôn ngữ quy mô lớn bằng cách sử dụng nó để xây dựng dữ liệu tiền huấn luyện cho HMDE. Cụ thể, cho một tập hợp ngôn ngữ L={L1;L2;:::;Ln}, đầu tiên chúng tôi lấy

--- TRANG 4 ---
các phần đơn ngôn ngữ từ corpus Wiki-40B.4 Sau đó chúng tôi xác định các bài viết bằng các ngôn ngữ khác nhau về cùng một khái niệm (thông qua trường wikidata_id) và chỉ giữ lại những khái niệm có trang được tìm thấy ít nhất trong hai ngôn ngữ từ L. Đối với mỗi khái niệm như vậy với các trang p1;p2;:::;pm trong m ngôn ngữ khác nhau, chúng tôi tạo ra tất cả các cặp chéo ngôn ngữ có thể có của các bài viết (pi,pj) bao quát cùng một khái niệm. Đối với mỗi cặp (pi,pj), sau đó chúng tôi tận dụng metadata của Wikipedia – cụ thể là ánh xạ các trang Wikipedia vào hệ thống phân cấp thể loại của nó – để chọn một bài viết ni từ cùng Wikipedia đơn ngôn ngữ với pi (tức là được viết bằng cùng ngôn ngữ với pi) thuộc về (ít nhất một) cùng thể loại Wikipedia với pi. Điều này tạo ra các bộ ba (pi,pj,ni) từ đó chúng tôi tạo ra các cặp dương tính chéo ngôn ngữ (pi,pj) và các âm tính cứng đơn ngôn ngữ tương ứng (pi, ni) cho mục tiêu tương phản của chúng tôi (xem §2.2).

Một mặt, chất lượng biểu diễn của MMTs đối với một ngôn ngữ cụ thể phụ thuộc vào kích thước của corpus tiền huấn luyện của ngôn ngữ đó (Hu et al., 2020; Lauscher et al., 2020). Mặt khác, huấn luyện mô hình đa ngôn ngữ với các trường hợp từ các ngôn ngữ đa dạng về mặt ngôn ngữ học có thể tổng quát hóa tốt hơn cho các ngôn ngữ chưa thấy (Chen et al., 2019; Ansell et al., 2021). Tuy nhiên, hầu hết các ngôn ngữ có nguồn lực có xu hướng là Indo-European (Joshi et al., 2020), đặt kích thước corpus và tính đa dạng ngôn ngữ đối lập nhau. Do đó chúng tôi tạo ra hai tập dữ liệu khác nhau, mỗi tập nhấn mạnh một trong hai khía cạnh này: (1) XLW-4L được xây dựng bắt đầu từ bốn ngôn ngữ Indo-European có nguồn lực cao: tiếng Anh, Đức, Pháp và Ý; (12) XLW-12L được xây dựng bắt đầu từ một tập hợp 12 ngôn ngữ đa dạng về mặt ngôn ngữ học: tiếng Anh, Pháp, Nga, Nhật, Trung, Hungary, Phần Lan, Ả Rập, Ba Tư, Thổ Nhĩ Kỳ, Hy Lạp và Mã Lai. Với 1.1M bộ ba (pi,pj,ni), XLW-4L gần như lớn gấp đôi XLW-12L (bao gồm 592K bộ ba), mặc dù bao gồm ít hơn ba lần số ngôn ngữ: điều này chủ yếu là vì có nhiều khái niệm chung hơn giữa các Wikipedia lớn của XLW-4L (ví dụ: Đức và Ý) so với các Wikipedia nhỏ hơn của XLW-12L (ví dụ: Thổ Nhĩ Kỳ và Mã Lai).5

3.2 Nhiệm Vụ Đánh Giá và Tập Dữ Liệu
HMDE có ý nghĩa là một bộ mã hóa tài liệu đa ngôn ngữ đa mục đích. Do đó nó cần hữu ích cho cả (1) khi được tinh chỉnh cho một nhiệm vụ cấp độ tài liệu có giám sát, và (2) như một bộ mã hóa tài liệu độc lập. Do đó chúng tôi đánh giá HMDE trong (1) chuyển giao chéo ngôn ngữ zero-shot cho phân loại tài liệu có giám sát (XLDC) và (2) truy xuất tài liệu chéo ngôn ngữ không giám sát (CLIR).

4Có sẵn trong Tensorflow datasets: https://www.tensorflow.org/datasets/catalog/wikipedia
5Thống kê theo ngôn ngữ của các tập dữ liệu có trong Phụ lục.

XLDOC. Các MMT thông thường (ví dụ: mBERT hoặc XLM-R) chủ yếu được sử dụng trong chuyển giao chéo ngôn ngữ zero-shot cho các nhiệm vụ NLP có giám sát: một MMT được tinh chỉnh trên dữ liệu huấn luyện đặc thù nhiệm vụ trong một ngôn ngữ giàu nguồn lực được sử dụng để đưa ra dự đoán cho (các) ngôn ngữ không có dữ liệu nhiệm vụ. Chúng tôi đánh giá HMDE trong cùng thiết lập chuyển giao chéo ngôn ngữ zero-shot, chỉ cho một nhiệm vụ cấp độ tài liệu – phân loại tài liệu chủ đề. Chúng tôi tinh chỉnh HMDE theo cách tiêu chuẩn, bằng cách xếp chồng một bộ phân loại softmax lên trên đầu ra của bộ mã hóa cấp độ tài liệu. Với d là mã hóa HDME của tài liệu đầu vào d, dự đoán của bộ phân loại được tính như:

y=softmax (Wd+b) (2)

với W∈RCh và b∈RC là các tham số có thể huấn luyện của bộ phân loại (và C là số lớp).

Chúng tôi tinh chỉnh HMDE trên phần huấn luyện tiếng Anh của tập dữ liệu MLDOC (Schwenk and Li, 2018) và đánh giá hiệu suất của nó trên các phần test của tất cả các ngôn ngữ (đích) khác. MLDOC là một tập con của Reuters Corpus Volume 2 (RCV2), với các phần huấn luyện, phát triển và test bằng 8 ngôn ngữ (tiếng Anh, Tây Ban Nha, Đức, Pháp, Ý, Nga, Nhật và Trung), bao gồm lần lượt 1000, 1000, và 4000 tài liệu. Các tin tức được phân loại thành C= 4 lớp có liên quan chặt chẽ về mặt ngữ nghĩa (Corporate/Industrial, Economics, Government/Social, và Markets).

CLIR. Chúng tôi đánh giá hiệu quả của HMDE như một bộ mã hóa tài liệu độc lập trong một nhiệm vụ truy xuất tài liệu chéo ngôn ngữ không giám sát: các truy vấn (văn bản ngắn) bằng một ngôn ngữ được thực hiện đối với một bộ sưu tập tài liệu được viết bằng ngôn ngữ khác. Chúng tôi áp dụng một mô hình truy xuất đơn giản: chúng tôi xếp hạng tài liệu theo thứ tự giảm dần của tương tự cosine của nhúng d của chúng, được tạo ra bởi HMDE, với nhúng q của truy vấn, cos(d;q). Chúng tôi thu được nhúng truy vấn q bằng cách mã hóa truy vấn chỉ với transformer cấp thấp (cấp câu) của HMDE: q là biểu diễn được biến đổi của token bắt đầu chuỗi ([BOS]).

Chúng tôi thực hiện đánh giá trên CLEF-2003,6 một benchmark CLIR phổ biến, bao gồm các ngôn ngữ sau: tiếng Anh (EN), Đức (DE), Ý (IT), Phần Lan (FI) và Nga (RU). Theo nghiên cứu trước (Glavaš et al., 2019; Litschko et al., 2022), chúng tôi đánh giá HMDE trên 9 cặp ngôn ngữ (với ngôn ngữ đầu tiên là ngôn ngữ truy vấn): EN-FI, DE, IT, RU, DE-FI, IT, RU, FI-IT, RU. Đối với mỗi cặp ngôn ngữ chúng tôi làm việc với 60 truy vấn và các bộ sưu tập tài liệu có kích thước sau: RU – 17K, FI – 55K, IT – 158K, và DE – 295K.

6http://catalog.elra.info/en-us/repository/browse/ELRA-E0008/

3.3 Các Mô Hình Baseline
Có hai lựa chọn thay thế chính cho mã hóa tài liệu dài phân cấp. Thứ nhất là (i) phân mảnh tài liệu thành các đoạn nhỏ hơn, (ii) mã hóa mỗi đoạn với một MMT tiền huấn luyện thông thường (ví dụ: MMT vanilla như XLM-R hoặc một bộ mã hóa câu đa ngôn ngữ như LaBSE), và (iii) tập hợp biểu diễn tài liệu từ các nhúng của các đoạn. Thứ hai là huấn luyện một bộ mã hóa attention thưa đa ngôn ngữ, tương tự như (Sagen, 2021).

MMT với Cửa Sổ Trượt (LaBSE-Seg).
Để so sánh công bằng, chúng tôi sử dụng LaBSE (Feng et al., 2022) – cùng MMT tiền huấn luyện mà chúng tôi sử dụng để khởi tạo transformer cấp thấp trong HMDE – để mã hóa độc lập các đoạn chồng lấp của tài liệu đầu vào. Chúng tôi chia tài liệu thành các đoạn dài NS token. Theo Dai et al. (2022), người tìm thấy rằng các đoạn chồng lấp giảm thiểu vấn đề phân mảnh ngữ cảnh, chúng tôi làm cho các đoạn liền kề chồng lấp trong NS=3 token. Sau khi mã hóa mỗi đoạn với LaBSE, chúng tôi pooling trung bình biểu diễn tài liệu d từ tập hợp nhúng đoạn. Trong XLDX (phân loại tài liệu chủ đề) trung bình này của nhúng đoạn được đưa vào đầu phân loại. Trong CLIR, nó được so sánh với mã hóa LaBSE của truy vấn.

Longformer Đa Ngôn Ngữ (mLongformer).
Kiến trúc Longformer (Beltagy et al., 2020) kết hợp attention cửa sổ cục bộ với attention toàn cục, tạo ra một cơ chế attention lai, yêu cầu bộ nhớ tỷ lệ tuyến tính với độ dài đầu vào. Beltagy et al. (2020) bổ sung đề xuất quy trình nhiều bước để khởi tạo các tham số của Longformer dựa trên các tham số của một transformer thông thường tiền huấn luyện (ví dụ: trong trường hợp Longformer tiếng Anh đơn ngôn ngữ từ RoBERTa (Liu et al., 2019)) và sau đó huấn luyện thêm Longformer thông qua mô hình hóa ngôn ngữ có mặt nạ (MLM). Chúng tôi huấn luyện Longformer đa ngôn ngữ theo cùng quy trình: để so sánh công bằng với HMDE, chúng tôi khởi tạo các tham số của nó từ các tham số của LaBSE và thực hiện huấn luyện MLM bổ sung trên XLW-4L, cùng corpus mà chúng tôi huấn luyện HMDE.

4 Kết Quả và Thảo Luận
Đầu tiên chúng tôi báo cáo và thảo luận các kết quả chính mà chúng tôi đạt được với HMDE trên XLDC và CLIR (trong §4.1). Trong một loạt thí nghiệm tiếp theo, chúng tôi phân tích thêm các lựa chọn thiết kế chính cho HMDE (§4.2).

4.1 Kết Quả Chính
Phân Loại Tài Liệu Chéo Ngôn Ngữ. Bảng 1 so sánh HMDE được huấn luyện trên XLW-4L với một số bộ mã hóa đa ngôn ngữ tiêu chuẩn và tài liệu dài: bên cạnh các baseline được giới thiệu trong §3.3, để hoàn thiện chúng tôi thêm kết quả cho LaBSE vanilla (tức là không trượt qua tài liệu dài) và các mô hình dựa trên XLM-R và mBERT được báo cáo bởi Dong et al. (2020) và Zhao et al. (2021), tương ứng. Như mong đợi, tất cả các bộ mã hóa tài liệu dài đều vượt trội hơn tất cả MMTs tiêu chuẩn. mLongformer và HMDE nói chung thể hiện hiệu suất tương tự, vượt qua hiệu suất của LaBSE-Seg dựa trên phân đoạn cho hầu như tất cả ngôn ngữ. Hiệu suất tương đương của mLongformer và HMDE cho thấy rằng trong sự hiện diện của dữ liệu tinh chỉnh đặc thù nhiệm vụ, không thực sự quan trọng việc chúng ta tập hợp biểu diễn tài liệu theo cách phẳng hay phân cấp. Điều đặc biệt khuyến khích là cả HDME và mLongformer đều thể hiện hiệu suất mạnh cho các ngôn ngữ mà chúng không quan sát trong tiền huấn luyện cấp độ tài liệu: Tây Ban Nha, Nga, Nhật và Trung.7,8

Truy Xuất Chéo Ngôn Ngữ. Kết quả cho CLIR không giám sát được hiển thị trong Bảng 2. Giống như trong XLDC, chúng tôi bổ sung báo cáo kết quả cho LaBSE chỉ mã hóa phần đầu của tài liệu (không trượt) cũng như cho mBERT, được báo cáo bởi Litschko et al. (2022). CLIR, trong đó các transformer đa ngôn ngữ được sử dụng như các bộ mã hóa tài liệu độc lập mà không có bất kỳ tinh chỉnh đặc thù nhiệm vụ nào, kể một câu chuyện rất khác với kết quả XLDC có giám sát. HMDE vượt trội mạnh mẽ so với mLongformer, cho thấy rằng, giống như các MMT vanilla,

7LaBSE, với các tham số của nó cả HMDE và mLongformer đều được khởi tạo trước khi tiền huấn luyện cấp độ tài liệu, tuy nhiên, đã được tiếp xúc với tất cả các ngôn ngữ này trong tiền huấn luyện cấp câu của chính nó.
8Hiệu suất qua các ngôn ngữ không thể so sánh trực tiếp vì các tập test MLDOC không song song qua các ngôn ngữ.

--- TRANG 5 ---
Mô Hình En Es De Fr It Ru Ja Zh Trung Bình
Transformer Đa Ngôn Ngữ Tiêu Chuẩn
LaBSE 95.5 79.0 89.6 87.2 76.8 63.9 80.8 86.1 82.4
XLM-R (Dong et al., 2020) 93.0 84.6 92.5 87.1 73.2 68.9 78.2 85.8 83.0
mBERT (Zhao et al., 2021) 96.9 81.9 88.3 83.1 74.1 72.3 74.6 84.4 82.0
Bộ Mã Hóa Tài Liệu Dài Đa Ngôn Ngữ
LaBSE-Seg 94.0 82.9 90.2 89.9 78.1 71.9 75.5 88.4 84.0
mLongformer (XLW-4L) 95.8 87.0 93.4 91.9 80.6 71.7 79.5 88.5 86.1
HMDE (XLW-4L) 95.4 85.6 91.2 92.0 78.5 83.9 76.3 89.5 86.8

Bảng 1: Hiệu suất của HDME so sánh với MMTs tiêu chuẩn và các bộ mã hóa tài liệu dài đa ngôn ngữ baseline trên phân loại tài liệu chủ đề có giám sát (MLDOC). Hiệu suất (ngoại trừ En) cho chuyển giao chéo ngôn ngữ zero-shot: tất cả mô hình được tinh chỉnh chỉ trên dữ liệu huấn luyện tiếng Anh. Đậm: hiệu suất tốt nhất trong mỗi cột.

Mô Hình En–Fi En–It En–Ru En–De De–Fi De–It De–Ru Fi–It Fi–Ru Trung Bình
Transformer Đa Ngôn Ngữ Tiêu Chuẩn
LaBSE .247 .224 .131 .138 .247 .214 .135 .211 .125 .186
mBERT (Litschko et al., 2022) .145 .146 .167 .107 .151 .116 .149 .117 .128 .136
Bộ Mã Hóa Tài Liệu Dài Đa Ngôn Ngữ
LaBSE-Seg .243 .169 .107 .194 .268 .178 .104 .153 .014 .159
mLongformer (XLW-4L) .150 .088 .094 .082 .190 .072 .120 .097 .091 .109
HMDE (XLW-4L) .380 .282 .141 .326 .352 .259 .130 .238 .129 .249

Bảng 2: Hiệu suất của HDME so sánh với MMTs tiêu chuẩn và các bộ mã hóa tài liệu dài đa ngôn ngữ baseline trên truy xuất tài liệu chéo ngôn ngữ không giám sát (CLEF-2003). Đậm: hiệu suất tốt nhất trong mỗi cột.

mLongformer yêu cầu tinh chỉnh và không thể mã hóa đáng tin cậy các tài liệu "out of the box". HMDE cũng vượt trội đáng kể so với LaBSE-Seg, bộ mã hóa tài liệu dài dựa trên LaBSE trượt qua tài liệu. Thú vị là, LaBSE vanilla, chỉ mã hóa phần đầu của tài liệu, cũng vượt trội hơn đối tác trượt LaBSE-Seg, được tiếp xúc với toàn bộ tài liệu. Chúng tôi tin rằng điều này là vì (1) trong CLEF, thông tin liên quan đến truy xuất thường xuất hiện ở phần đầu của tài liệu và trong những trường hợp như vậy (2) pooling trung bình của LaBSE-Seg qua tất cả các đoạn tài liệu sau đó làm loãng mã hóa của nội dung liên quan đến truy vấn. Quan trọng là, HMDE trong CLIR cũng có vẻ tổng quát hóa rất tốt cho các ngôn ngữ không thấy trong tiền huấn luyện cấp độ tài liệu của nó (đặc biệt cho các tài liệu tiếng Phần Lan).

4.2 Phân Tích Sâu Hơn
Tiếp theo chúng tôi kiểm tra thực nghiệm cách các lựa chọn khác nhau trong thiết kế và tiền huấn luyện của HDME ảnh hưởng đến hiệu suất của nó, tập trung vào: (i) tính đa dạng ngôn ngữ và kích thước của corpus tiền huấn luyện (XLW-4L vs. XLW-12L), (ii) đóng băng transformer cấp thấp (tức là trọng số LaBSE) sau khởi tạo, và (iii) khởi tạo nó với trọng số của XLM-R như MMT tiêu chuẩn (vs. khởi tạo với LaBSE như bộ mã hóa câu). Chúng tôi cung cấp thêm các phân tích về phân đoạn tài liệu (câu vs. chuỗi token bỏ qua ranh giới câu) trong Phụ lục A.2.

Dữ Liệu Tiền Huấn Luyện: Tính Đa Dạng Ngôn Ngữ vs. Kích Thước.
Như đã thảo luận trong §3.1, chúng tôi chuẩn bị hai corpus khác nhau cho tiền huấn luyện HMDE: XLW-4L, lớn hơn (1.1M trường hợp) nhưng chỉ bao gồm bốn ngôn ngữ Indo-European chính và XLW-12L, nhỏ hơn (590K trường hợp) nhưng có tài liệu từ một tập hợp 12 ngôn ngữ đa dạng về mặt ngôn ngữ học. Để kiểm soát kích thước, và đánh giá hiệu ứng của tính đa dạng ngôn ngữ một mình, chúng tôi lấy mẫu ngẫu nhiên XLW-4L, tạo ra một tập dữ liệu 4 ngôn ngữ XLW-4L-S có kích thước tương đương với XLW-12L. Hình 2 hiển thị hiệu suất downstream của HMDE khi được tiền huấn luyện trên mỗi tập dữ liệu này.

--- TRANG 6 ---
XLDC CLIR0.500.751.0086.8 24.986.7
22.386.5
23.6XLW-4L XLW-4L-S XLW-12L

Hình 2: Hiệu suất của HMDE khi được tiền huấn luyện trên các tập dữ liệu khác nhau. Kết quả là trung bình qua tất cả ngôn ngữ test (XLDC) và cặp ngôn ngữ (CLIR).

So sánh giữa XLW-4L và XLW-4L-S (cùng ngôn ngữ, kích thước tập dữ liệu khác nhau) cho thấy rằng hương vị tiền huấn luyện tương phản chéo ngôn ngữ của chúng tôi (§2.2) dẫn đến tiền huấn luyện khá hiệu quả về mẫu: cắt dữ liệu huấn luyện gần như một nửa dẫn đến giảm hiệu suất nhỏ (chỉ 0.3 điểm accuracy trong XLDC; 1.3 điểm MAP trong CLIR). So sánh giữa XLW-4L-S và XLW-12L (cùng kích thước, các tập ngôn ngữ khác nhau) định lượng vai trò của tính đa dạng ngôn ngữ trong tiền huấn luyện. Hơi bất ngờ, tiền huấn luyện đa dạng ngôn ngữ hơn trên XLW-12L không mang lại hiệu suất tốt hơn so với tiền huấn luyện "chỉ Indo-European" trên XLW-4L-S: trong khi chúng hoạt động tương đương trên XLDC, tiền huấn luyện đa dạng hơn (XLW-12L) dẫn đến hiệu suất CLIR tệ hơn (-1.3 điểm MAP trung bình). Chúng tôi giả thuyết rằng điều này là do chất lượng biểu diễn cao hơn của bốn ngôn ngữ Indo-European (EN, DE, FR, IT) trong LaBSE (do sự quá đại diện của chúng trong tiền huấn luyện của LaBSE), mà chúng tôi khởi tạo transformer cấp thấp của HMDE. Chúng tôi thấy kết quả này đặc biệt khuyến khích, vì – cùng với quan sát rằng HMDE tổng quát hóa tốt cho các ngôn ngữ không thấy trong tiền huấn luyện cấp độ tài liệu của nó – nó cho thấy rằng tiền huấn luyện cấp độ tài liệu bản thân không nhất thiết cần phải đa ngôn ngữ quy mô lớn để tạo ra các bộ mã hóa tài liệu đa ngôn ngữ quy mô lớn thành công.

Transformer Cấp Thấp. Tiếp theo chúng tôi điều tra hai khía cạnh của transformer cấp thấp: (1) khởi tạo nó với trọng số nào và (2) liệu có đáng để cập nhật các tham số của nó trong quá trình tiền huấn luyện cấp độ tài liệu. Đối với khía cạnh trước, chúng tôi so sánh khởi tạo dựa trên LaBSE mặc định của chúng tôi (với LaBSE như một bộ mã hóa đa ngôn ngữ chuyên biệt câu) với khởi tạo bằng trọng số của XLM-R, như MMT đa ngôn ngữ vanilla. Để trả lời khía cạnh sau, chúng tôi bổ sung huấn luyện HMDE bằng cách đóng băng transformer cấp thấp của nó.

Mô Hình Cập Nhật XLDC CLIR
HMDE-LaBSE Được Cập Nhật 86.8 0.249
HMDE-LaBSE Đóng Băng 85.9 0.167
HMDE-XLM-R Được Cập Nhật 83.9 0.135

Bảng 3: Kết quả HMDE cho các lựa chọn khác nhau w.r.t. khởi tạo và huấn luyện của transformer cấp thấp. Huấn luyện cho tất cả ba biến thể được thực hiện trên XLW-4L. Kết quả là trung bình qua tất cả ngôn ngữ test (XLDC) và cặp ngôn ngữ (CLIR).

trong tiền huấn luyện cấp độ tài liệu. Bảng 3 tóm tắt kết quả của các phân tích này.

Trong khi đóng băng transformer cấp thấp sau khởi tạo dẫn đến huấn luyện nhanh hơn nhiều, nó dẫn đến bộ mã hóa tài liệu kém hơn, đặc biệt nếu được sử dụng để mã hóa tài liệu độc lập, mà không có tinh chỉnh đặc thù nhiệm vụ9 (HMDE-LaBSE Được Cập Nhật vs. Đóng Băng; giảm 1 điểm accuracy trong XLDC vs. giảm 8 điểm MAP trong CLIR). Khởi tạo transformer cấp thấp của HDME với trọng số LaBSE dẫn đến hiệu suất downstream tốt hơn nhiều so với khởi tạo với XLM-R không chuyên biệt cho ngữ nghĩa cấp câu.

5 Nghiên Cứu Liên Quan
Chúng tôi định vị đóng góp của chúng tôi w.r.t. ba hướng nghiên cứu liên quan: (1) tiền huấn luyện các bộ mã hóa tài liệu dài, (2) tiền huấn luyện tự giám sát cho truy xuất, và (3) khai thác tài liệu song song.

Bộ Mã Hóa Tài Liệu Dài. Các bộ mã hóa phân cấp (Zhang et al., 2019; Yang et al., 2020; Glavaš and Somasundaran, 2020) và dựa trên attention thưa (Beltagy et al., 2020; Zaheer et al., 2020; Tay et al., 2020) đã được thảo luận trong §1 chiếm phần lớn các phương pháp mã hóa tài liệu dài. Dai et al. (2022) so sánh mở rộng Longformer (Beltagy et al., 2020) với các transformer phân cấp trên các nhiệm vụ phân loại tài liệu dài khác nhau, cho thấy rằng transformer phân cấp thể hiện hiệu suất tốt hơn một chút, đặc biệt nếu bộ mã hóa cấp thấp mã hóa các đoạn chồng lấp. Ding et al. (2021) đề xuất một mô hình khác, dựa trên phân đoạn dựa trên transformer lặp lại (Dai et al., 2019), được thiết kế để khắc phục phân mảnh ngữ cảnh với cơ chế phản hồi hồi tưởng: mỗi đoạn được mã hóa hai lần – sau đoạn ban đầu từ trái sang phải với transformer lặp lại, biểu diễn đoạn được tạo ngữ cảnh lẫn nhau thêm theo hướng hai chiều. Huấn luyện của chúng kết hợp MLM-ing với mục tiêu sắp xếp lại đoạn.

Phần lớn nghiên cứu về tiền huấn luyện bộ mã hóa cho tài liệu dài tập trung vào các mô hình đơn ngôn ngữ (chủ yếu tiếng Anh). Một số ngoại lệ đa ngôn ngữ (Yu et al., 2021; Sagen, 2021) tạo ra một Longformer đa ngôn ngữ từ các MMT tiêu chuẩn (XLM-R và mBERT) theo cùng cách mà nghiên cứu gốc (Beltagy et al., 2020) tiền huấn luyện Longformer tiếng Anh sau khởi tạo từ trọng số RoBERTa. Trong nghiên cứu này, chúng tôi nhân rộng nỗ lực này, đánh giá mLongformer như baseline chính cho HMDE.

Tiền Huấn Luyện cho Truy Xuất. Các phương pháp tự giám sát và giám sát từ xa gần đây đã được đề xuất để tiền huấn luyện bộ mã hóa tài liệu cụ thể cho nhiệm vụ truy xuất tài liệu (Izacard et al., 2022; Yu et al., 2021; Gao et al., 2022). Izacard et al. (2022) tiền huấn luyện Contriever – một bộ mã hóa tài liệu dựa trên BERT với mục tiêu dựa trên nhiệm vụ cloze ngược (Lee et al., 2019): một cặp truy vấn-tài liệu dương tính được tạo ra bằng cách trích xuất một span văn bản từ tài liệu và sử dụng nó như một "truy vấn"; chúng huấn luyện với mục tiêu tương phản chấm điểm tài liệu từ đó truy vấn được trích xuất cao hơn các tài liệu khác. Gao et al. (2022) đưa truy vấn như lời nhắc cho một mô hình ngôn ngữ sinh, sau đó tạo ra tài liệu; chúng sau đó sử dụng Contriever để nhúng tài liệu tổng hợp này và tìm các tài liệu thực tương tự nhất trong bộ sưu tập, cuối cùng tinh chỉnh Contriever trên các cặp truy vấn-tài liệu thu được theo cách này. Theo cách tương tự như chúng tôi, Yu et al. (2021) tận dụng Wikipedia như một nguồn dữ liệu gần như song song: trong khi chúng tôi khai thác các căn chỉnh cấp độ tài liệu, chúng tận dụng các căn chỉnh cấp độ phần để tạo ra các trường hợp huấn luyện dương tính chéo ngôn ngữ cho truy xuất đoạn văn: một tiêu đề phần ("truy vấn") bằng một ngôn ngữ được kết hợp với nội dung phần ("tài liệu") bằng ngôn ngữ khác; chúng sau đó huấn luyện một Longformer đa ngôn ngữ được khởi tạo từ mBERT với sự kết hợp của MLM-ing truy vấn và xếp hạng liên quan tương phản. Trái ngược với những nỗ lực này, chúng tôi tạo ra một bộ mã hóa tài liệu đa ngôn ngữ đa mục đích (tức là không phụ thuộc nhiệm vụ) có thể vừa được tinh chỉnh cho các nhiệm vụ có giám sát vừa được sử dụng như một bộ nhúng tài liệu độc lập.

Khai Thác Tài Liệu Song Song. Khai thác tài liệu song song – một nhiệm vụ nhằm xác định các bản dịch lẫn nhau trong một bộ sưu tập tài liệu lớn và thường được sử dụng như bước đầu tiên trong việc trích xuất câu song song (Resnik and Smith, 2003; Uszkoreit et al., 2010; Schwenk, 2018, và các nghiên cứu khác) – là nhiệm vụ có sự tương đồng nhất với tiền huấn luyện của chúng tôi. Các phương pháp dựa trên Transformer cho nhiệm vụ này (Guo et al., 2019; El-Kishky and Guzmán, 2020; Gong et al., 2021) thường tập hợp biểu diễn cấp độ tài liệu từ nhúng câu đa ngôn ngữ. Nghiên cứu của Guo et al. (2019) có lẽ liên quan nhất đến chúng tôi: chúng huấn luyện một bộ mã hóa phân cấp với một mạng feed-forward đơn giản như bộ mã hóa cấp cao biến đổi độc lập các nhúng câu được tính toán trước: nhúng tài liệu sau đó là trung bình của các nhúng câu được biến đổi feed-forward. Mô hình được huấn luyện song ngữ (tiếng Anh-Tây Ban Nha và tiếng Anh-Pháp) với mục tiêu tương phản trên một corpus tiêu chuẩn bạc khổng lồ của các cặp tài liệu song song (lần lượt 13M và 6M cặp tài liệu) và được đánh giá trên chính nhiệm vụ khai thác tài liệu song song. Nghiên cứu của chúng tôi khác biệt trong hai khía cạnh quan trọng: (1) trong khi (Guo et al., 2019) huấn luyện các mô hình song ngữ để nhận biết tài liệu song song, chúng tôi huấn luyện một bộ mã hóa tài liệu đa ngôn ngữ quy mô lớn đa mục đích duy nhất; (2) chúng tôi huấn luyện trên một corpus nhỏ hơn nhiều của các tài liệu có thể so sánh (không song song), sẵn có từ Wikipedia. Cả hai khía cạnh đều làm cho HMDE có thể áp dụng rộng rãi hơn nhiều, cho cả nhiệm vụ tài liệu có giám sát và không giám sát và bất kỳ ngôn ngữ nào từ tiền huấn luyện của LaBSE (vì bộ mã hóa cấp thấp của HMDE được khởi tạo với trọng số của LaBSE).

6 Kết Luận
Trong nghiên cứu này, chúng tôi tiền huấn luyện một bộ mã hóa tài liệu đa ngôn ngữ dựa trên kiến trúc transformer phân cấp (HMDE), và khởi tạo bộ mã hóa cấp thấp của nó với trọng số của một bộ mã hóa câu đa ngôn ngữ hiện đại. Chúng tôi tận dụng Wikipedia như một nguồn phong phú của các tài liệu dài gần như song song và huấn luyện HDME với mục tiêu khớp tài liệu chéo ngôn ngữ tương phản. Chúng tôi cho thấy rằng mô hình thu được là một bộ mã hóa tài liệu đa ngôn ngữ đa mục đích có thể thành công vừa (1) được tinh chỉnh cho chuyển giao chéo ngôn ngữ cấp độ tài liệu vừa (2) được sử dụng như một mô hình nhúng tài liệu out of the box. Kết quả của chúng tôi làm cho HMDE hiệu quả hơn đáng kể so với cả Longformer đa ngôn ngữ và mã hóa tài liệu dựa trên phân đoạn. Quan trọng là, HMDE tổng quát hóa tốt cho các ngôn ngữ không thấy trong tiền huấn luyện cấp độ tài liệu của nó. Các thí nghiệm tiếp theo của chúng tôi tiết lộ rằng kích thước của corpus tiền huấn luyện ảnh hưởng đến hiệu suất nhiều hơn số lượng và tính đa dạng của các ngôn ngữ liên quan, cho thấy rằng các bộ mã hóa tài liệu đa ngôn ngữ quy mô lớn đáng tin cậy không nhất thiết yêu cầu tiền huấn luyện đa ngôn ngữ quy mô lớn tương đương.

--- TRANG 7 ---
9Các tham số của transformer cấp thấp luôn được cập nhật trong tinh chỉnh XLDC, ngay cả khi chúng tôi đóng băng chúng trong tiền huấn luyện cấp độ tài liệu.

Hạn Chế
Bởi vì chúng tôi khởi tạo transformer cấp thấp của HMDE với LaBSE (Feng et al., 2022), tập hợp các ngôn ngữ mà HMDE "hỗ trợ" out of the box bị ràng buộc với tập hợp 109 ngôn ngữ được bao gồm trong tiền huấn luyện của LaBSE.10 Điều này có nghĩa là HMDE sẽ, về nguyên tắc, ít hiệu quả hơn như một bộ mã hóa tài liệu cho các ngôn ngữ khác.11 HDME, giống như LaBSE, về nguyên tắc nên vô dụng cho các ngôn ngữ được viết bằng một hệ thống chữ viết mà LaBSE (hoặc thực tế là mBERT, từ đó LaBSE mượn từ vựng và nhúng từ phụ tiền huấn luyện) chưa thấy trong tiền huấn luyện của nó, vì tokenizer tương ứng sẽ tạo ra một chuỗi các token không xác định ([UNK]). Điều này có nghĩa là HMDE, giống như phần còn lại của các bộ mã hóa đa ngôn ngữ hiện có, chỉ hỗ trợ một phần nhỏ trong số 7000+ ngôn ngữ của thế giới (Joshi et al., 2020). Hơn nữa, tất cả các ngôn ngữ được bao gồm trong các tập dữ liệu đánh giá của chúng tôi – MLDOC và CLEF – đều được bao phủ bởi tập hợp 109 ngôn ngữ này, có nghĩa là hiệu suất trung bình mà chúng tôi báo cáo có thể là một ước tính quá mức đáng kể cho các ngôn ngữ không thấy trong tiền huấn luyện của LaBSE.

Hơn nữa, HMDE tận dụng Wikipedia để huấn luyện (với các tập hợp 4 hoặc 12 ngôn ngữ, xem 3.1) – số lượng trang Wikipedia (và nói chung hơn, dấu vết kỹ thuật số của một ngôn ngữ trên web) thay đổi rất lớn giữa các ngôn ngữ, hiệu quả hạn chế việc lựa chọn ngôn ngữ cho tiền huấn luyện cấp độ tài liệu của HMDE. Tuy nhiên, kết quả của chúng tôi (xem 4.1) cho thấy rằng HMDE tổng quát hóa tốt cho các ngôn ngữ không thấy trong tiền huấn luyện cấp độ tài liệu của nó.

Hơn nữa, HMDE được triển khai như một Bi-Encoder (hay còn gọi là mạng Siamese), có nghĩa là đối với một cặp tài liệu cho trước trong một ví dụ huấn luyện (cặp dương tính hoặc âm tính), nó mã hóa riêng biệt mỗi tài liệu. Kiến trúc Cross-Encoder, trong đó các tài liệu sẽ được nối trước khi mã hóa, sẽ có lợi thế cho phép bộ mã hóa tạo ngữ cảnh cho biểu diễn token/câu của một tài liệu với những biểu diễn của tài liệu khác

10Danh sách đầy đủ được cung cấp trong Bảng 10 của Phụ lục trong (Feng et al., 2022).
11Không nhất thiết là trường hợp chỉ cho các ngôn ngữ chưa thấy là họ hàng gần của một số ngôn ngữ có nguồn lực cao được thấy trong tiền huấn luyện của LaBSE.

trước khi tính toán điểm tương tự của chúng. Các kiến trúc mã hóa chéo đã được chứng minh hiệu quả, mặc dù không hiệu quả (tức là chậm) trong huấn luyện cho truy xuất tài liệu, trong đó truy vấn (ngắn) được nối với tài liệu (dài) (MacAvaney et al., 2020; Shi et al., 2020; Rosa et al., 2022). Chúng tôi không khám phá mã hóa chéo trong nghiên cứu của mình; trong trường hợp của chúng tôi, nó ngụ ý mã hóa chung của sự nối của hai tài liệu dài (bằng các ngôn ngữ khác nhau), có thể làm bùng nổ việc chiếm dụng bộ nhớ GPU và có thể ngăn chúng tôi khớp ngay cả các batch một trường hợp trên các thẻ GPU của chúng tôi.

Cân Nhắc Đạo Đức
Chúng tôi không kiểm tra HMDE một cách rõ ràng để kiểm tra xem liệu các biểu diễn mà nó tạo ra có phản ánh các thiên kiến và định kiến xã hội tiêu cực (ví dụ: phân biệt giới tính hoặc phân biệt chủng tộc), nhưng cho rằng bộ mã hóa cấp thấp của nó được khởi tạo từ trọng số của LaBSE, sẽ không đáng ngạc nhiên nếu đây là trường hợp. Nếu như vậy, nhiều kỹ thuật hiện có từ văn hệ được thiết kế để debias các mô hình ngôn ngữ tiền huấn luyện (Qian et al., 2019; Barikeri et al., 2021; Guo et al., 2022) có thể được áp dụng cho HMDE, và về nguyên tắc "as-is" (tức là không có sửa đổi đặc biệt).

Tài Liệu Tham Khảo
Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavaš, Ivan Vulić, và Anna Korhonen. 2021. Mad-g: Multilingual adapter generation for efficient cross-lingual transfer. Trong Findings of the Association for Computational Linguistics: EMNLP 2021, trang 4762–4781.

Soumya Barikeri, Anne Lauscher, Ivan Vulić, và Goran Glavaš. 2021. Redditbias: A real-world resource for bias evaluation and debiasing of conversational language models. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 1941–1955.

Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Steven Cao, Nikita Kitaev, và Dan Klein. 2020. Multilingual alignment of contextual word representations. Trong International Conference on Learning Representations.

Ilias Chalkidis, Xiang Dai, Manos Fergadiotis, Prodromos Malakasiotis, và Desmond Elliott. 2022. An exploration of hierarchical attention transformers for efficient long document classification. arXiv preprint arXiv:2210.05529.

--- TRANG 8 ---
Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang, và Claire Cardie. 2019. Multi-source cross-lingual model transfer: Learning what to share. Trong Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL).

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, và Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 8440–8451.

Xiang Dai, Ilias Chalkidis, Sune Darkner, và Desmond Elliott. 2022. Revisiting transformer-based models for long document classification. Trong Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP).

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, và Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 2978–2988.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186.

SiYu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, và Haifeng Wang. 2021. ERNIE-Doc: A retrospective long-document modeling transformer. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 2914–2927, Online. Association for Computational Linguistics.

Xin Dong, Yaxin Zhu, Yupeng Zhang, Zuohui Fu, Dongkuan Xu, Sen Yang, và Gerard de Melo. 2020. Leveraging adversarial training in self-learning for cross-lingual text classification. Trong Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, trang 1541–1544, New York, NY, USA. Association for Computing Machinery.

Ahmed El-Kishky và Francisco Guzmán. 2020. Massively multilingual document alignment with cross-lingual sentence-mover's distance. Trong Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, trang 616–625.

Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, và Wei Wang. 2022. Language-agnostic bert sentence embedding. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 878–891.

Luyu Gao, Xueguang Ma, Jimmy Lin, và Jamie Callan. 2022. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496.

Goran Glavaš, Robert Litschko, Sebastian Ruder, và Ivan Vulić. 2019. How to (properly) evaluate cross-lingual word embeddings: On strong baselines, comparative analyses, and some misconceptions. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 710–721.

Goran Glavaš và Swapna Somasundaran. 2020. Two-level transformer and auxiliary coherence modeling for improved text segmentation. Trong Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, trang 7797–7804.

Hongyu Gong, Vishrav Chaudhary, Yuqing Tang, và Francisco Guzmán. 2021. Lawdr: Language-agnostic weighted document representations from pre-trained models. arXiv preprint arXiv:2106.03379.

Mandy Guo, Yinfei Yang, Keith Stevens, Daniel Cer, Heming Ge, Yun-Hsuan Sung, Brian Strope, và Ray Kurzweil. 2019. Hierarchical document encoder for parallel corpus mining. Trong Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), trang 64–72.

Yue Guo, Yi Yang, và Ahmed Abbasi. 2022. Auto-debias: Debiasing masked language models with automated biased prompts. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 1012–1023.

Dan Hendrycks và Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.

Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, và Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. Trong International Conference on Machine Learning, trang 4411–4421. PMLR.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, và Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 8:1–22.

--- TRANG 9 ---
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, và Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the nlp world. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 6282–6293.

Anne Lauscher, Vinit Ravishankar, Ivan Vulić, và Goran Glavaš. 2020. From zero to hero: On the limitations of zero-shot language transfer with multilingual transformers. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 4483–4499.

Kenton Lee, Ming-Wei Chang, và Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 6086–6096.

Robert Litschko, Ivan Vulić, Simone Paolo Ponzetto, và Goran Glavaš. 2022. On cross-lingual retrieval with multilingual text encoders. Information Retrieval Journal, 25(2):149–183.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Ilya Loshchilov và Frank Hutter. 2019. Decoupled weight decay regularization. Trong International Conference on Learning Representations.

Sean MacAvaney, Luca Soldaini, và Nazli Goharian. 2020. Teaching a new dog old tricks: Resurrecting multilingual retrieval using zero-shot learning. Trong European Conference on Information Retrieval, trang 246–254. Springer.

Xiaochuan Ni, Jian-Tao Sun, Jian Hu, và Zheng Chen. 2009. Mining multilingual topics from wikipedia. Trong Proceedings of the 18th international conference on World wide web, trang 1155–1156.

Aaron van den Oord, Yazhe Li, và Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748.

Raghavendra Pappagari, Piotr Zelasko, Jesús Villalba, Yishay Carmiel, và Najim Dehak. 2019. Hierarchical transformers for long document classification. Trong 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), trang 838–844. IEEE.

Nikolaos Pappas và Andrei Popescu-Belis. 2017. Multilingual hierarchical attention networks for document classification. Trong Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 1015–1025.

Magdalena Plamadă và Martin Volk. 2013. Mining for domain-specific parallel text from wikipedia. Trong Proceedings of the Sixth Workshop on Building and Using Comparable Corpora, trang 112–120.

Yusu Qian, Urwa Muaz, Ben Zhang, và Jae Won Hyun. 2019. Reducing gender bias in word-level language models with a gender-equalizing loss function. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, trang 223–228.

Philip Resnik và Noah A Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.

Guilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, và Rodrigo Nogueira. 2022. In defense of cross-encoders for zero-shot retrieval. arXiv preprint arXiv:2212.06121.

Markus Sagen. 2021. Large-context question answering with cross-lingual transfer. Luận văn thạc sĩ, Đại học Uppsala, Khoa Công nghệ Thông tin.

Holger Schwenk. 2018. Filtering and mining parallel data in a joint multilingual space. Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), trang 228–234, Melbourne, Australia. Association for Computational Linguistics.

Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, và Francisco Guzmán. 2021. Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, trang 1351–1361.

Holger Schwenk và Xian Li. 2018. A corpus for multilingual document classification in eight languages. Trong Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).

Peng Shi, He Bai, và Jimmy Lin. 2020. Cross-lingual training of neural models for document ranking. Trong Findings of the Association for Computational Linguistics: EMNLP 2020, trang 2768–2773, Online. Association for Computational Linguistics.

Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, và Da-Cheng Juan. 2020. Sparse sinkhorn attention. Trong International Conference on Machine Learning, trang 9438–9447. PMLR.

Jakob Uszkoreit, Jay M Ponte, Ashok C Popat, và Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. Trong Proceedings of the 23rd International Conference on Computational Linguistics, trang 1101–1109.

--- TRANG 10 ---
Shijie Wu và Mark Dredze. 2020. Do explicit alignments robustly improve multilingual encoders? Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 4471–4482.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, và Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 483–498.

Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, và Marc Najork. 2020. Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical encoder for long-form document matching. Trong Proceedings of the 29th ACM International Conference on Information & Knowledge Management, trang 1725–1734.

Puxuan Yu, Hongliang Fei, và Ping Li. 2021. Cross-lingual language model pretraining for retrieval. Trong Proceedings of the Web Conference 2021, trang 1029–1039.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297.

Xingxing Zhang, Furu Wei, và Ming Zhou. 2019. HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 5059–5069, Florence, Italy. Association for Computational Linguistics.

Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vulić, Roi Reichart, Anna Korhonen, và Hinrich Schütze. 2021. A closer look at few-shot crosslingual transfer: The choice of shots matters. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 5751–5767, Online. Association for Computational Linguistics.

--- TRANG 11 ---
A Phụ Lục
A.1 Chi Tiết Huấn Luyện và Tối Ưu Hóa
Trong tất cả các quy trình huấn luyện, chúng tôi sử dụng AdamW (Loshchilov and Hutter, 2019) làm thuật toán tối ưu hóa.

Tiền Huấn Luyện HMDE. Chúng tôi đặt độ dài câu tối đa cho HMDE, đầu vào cho transformer cấp thấp của nó (được khởi tạo với trọng số LaBSE) là 128 token. Để so sánh công bằng, chúng tôi cũng đặt kích thước đoạn của baseline LasBSE-Seg là NS= 128 token. Để so sánh công bằng với mLongformer, chúng tôi giới hạn độ dài tài liệu tối đa cho HMDE là 32 câu, không vượt quá độ dài đầu vào tối đa của mLongformer là 4;096 token. Trong tập thí nghiệm chính của chúng tôi, transformer cấp độ tài liệu (cấp cao) bao gồm 2 lớp transformer, với kích hoạt GELU (Hendrycks and Gimpel, 2016), chuẩn hóa lớp (ε = 1e−12), và lớp con feed-forward với kích thước ẩn 2048. Tỷ lệ dropout cho transformer cấp cao được đặt là 0:1. Chúng tôi huấn luyện trong các batch có kích thước N= 2 với tích lũy gradient qua 64 batch cho 1 epoch đầy đủ,12 với tỷ lệ học ban đầu 1e−5, lập lịch tuyến tính và 1000 bước làm ấm.

Tiền Huấn Luyện mLongformer. Chúng tôi huấn luyện mô hình mLongformer (cũng được khởi tạo từ LaBSE), cũng cho 1 epoch đầy đủ thông qua MLM-ing, che giấu 15% token. Chúng tôi huấn luyện với tỷ lệ học ban đầu 1e−5 với weight decay 0:01 và 500 bước làm ấm. Chúng tôi huấn luyện trong các batch có kích thước 2, tích lũy gradient qua 32 batch.

Tinh Chỉnh XLDC. Chúng tôi tinh chỉnh cả HMDE và mLongformer cho phân loại tài liệu chủ đề với tỷ lệ học 2e−5 và không có weight decay (với 200 bước làm ấm). Chúng tôi huấn luyện trong các batch có kích thước 4 cho 50 epoch, tích lũy gradient qua 8 batch. Lựa chọn mô hình được thực hiện dựa trên hiệu suất trên phần validation tiếng Anh của tập dữ liệu MLDOC, với dừng sớm nếu validation loss không cải thiện qua 7 epoch.

A.2 Phân Tích Bổ Sung
Chúng tôi bổ sung kiểm tra quyết định thiết kế của chúng tôi về việc phân đoạn tài liệu thành câu, và mã hóa câu với transformer cấp thấp (các trọng số được khởi tạo từ LaBSE). Để làm điều này, chúng tôi so sánh chiến lược mặc định của chúng tôi về việc phân đoạn tài liệu đầu vào thành câu với phân đoạn ít thông tin hơn thành các chunk liên tiếp gồm 128 token. Bảng 4 hiển thị kết quả của so sánh này.

Mô Hình Phân Đoạn XLDC CLIR
HMDE-LaBSE Câu 86.8 0.249
HMDE-LaBSE Chunk 85.4 0.224

Bảng 4: Kết quả HMDE cho các lựa chọn khác nhau w.r.t. phân đoạn tài liệu. Huấn luyện cho cả hai biến thể được thực hiện trên XLW-4L. Kết quả là trung bình qua tất cả ngôn ngữ test (XLDC) và cặp ngôn ngữ (CLIR).

Không đáng ngạc nhiên – cho rằng bộ mã hóa cấp thấp được khởi tạo với trọng số của một bộ mã hóa câu tiền huấn luyện – phân đoạn dựa trên câu hiệu quả hơn, mặc dù chunking không thua kém nhiều.

12Lưu ý rằng kích thước batch N= 2 trong mục tiêu tương phản của chúng tôi (xem §2.2) chỉ ngụ ý một cặp âm tính trong batch (bên cạnh âm tính cứng) cho mỗi cặp dương tính.
