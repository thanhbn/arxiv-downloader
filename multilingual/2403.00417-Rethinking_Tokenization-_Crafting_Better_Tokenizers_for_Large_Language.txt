# 2403.00417.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2403.00417.pdf
# File size: 624643 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Rethinking Tokenization: Crafting Better Tokenizers for Large Language 
Models  
Jinbiao Yang  
Jinbiao.Yang@mpi.nl  
Language and Computation in Neural Systems Group, Max Planck Institute for 
Psycholinguistics  
Abstract  
Tokenization significantly influences language models(LMs) ’ performance. This paper 
traces the evolution of tokenizers from word -level to subword -level, analyzing how they 
balance tokens and types to enhance model adaptability while controlling complexity. 
Despite subword tokenizers like Byte Pair Encoding (BPE) o vercoming many word 
tokenizer limitations, they encounter difficulties  in handling non -Latin languages and 
depend heavily on extensive training data and computational resources to grasp the 
nuances of multiword expressions (MWEs). This article argues that tokenizers, more than 
mere technical tools, should drawing inspiration from the cognitive science about human 
language processing. This study then introduces the “Principle of Least Effort” from 
cognitive science, that humans naturally seek to reduce cogni tive effort, and discusses the 
benefits of this principle for tokenizer development. Based on this principle, the paper 
proposes that the Less -is-Better (LiB) model could be a new approach for LLM tokenizer. 
The LiB model can autonomously learn an integrat ed vocabulary consisting of subwords, 
words, and MWEs, which effectively reduces both the numbers of tokens and types. 
Comparative evaluations show that the LiB tokenizer outperforms existing word and BPE 
tokenizers, presenting an innovative method for tok enizer development, and hinting at the 
possibility of future cognitive science -based tokenizers being more efficient.  
Keywords : tokenizer , tokenization , language model  
  

--- PAGE 2 ---
Introduction  
When confronted with vast or intricate information, our brains typically simplify it into 
smaller, more digestible segments, thereby helping us better understand and remember. 
Language, exemplifying such complexity, often requires segmenting itself into “c hunks” 
(Isbilen & Christiansen, 2020). In the field of natural language processing (NLP), the chunks 
are often referred to as tokens  through the process known as tokenization . 
The choice of tokenizer has a crucial impact on the performance of language models. 
Especially in language models (LMs), how a tokenizer segments corpora determines the 
fundamental way the model processes language. This article investigates  the roles of 
tokens (the actual number of lexical units in a corpus) and types (the number of different 
lexical units of vocabulary) in tokenizer design, and attempts to find an ideal solution that 
optimizes the number of tokens while controlling the numb er of types. In the following 
sections, the article will explore the advantages and limitations of subword tokenizers, 
analyze the treatment of Multiword Expressions (MWE) in current large language models. 
This article also argues that tokenizers, more tha n mere technical tools, should emulate and 
learn from human language processing methods, as tokenizers deal with content generated 
by humans rather than natural phenomena like sound and images, and calls for a general 
theory to guide the development of tok enizers. The article will discuss the “Principle of 
Least Effort” as such a general theory from cognitive science, and introduce a new type of 
tokenizer model - the Less -is-Better  model, based on the Principle of Least Effort.  
From Word -level Tokenizers to Subword -level Tokenizers  
NLP applications initially relied on word -level tokenizers, which divided text into words 
using spaces and punctuation. For example, the historical development of semantic 
representations started with the Bag -of-Words model, progressed to Word2Vec by 
(Miko lov et al., 2013), and to GloVe (Global Vectors for Word Representation) by 
(Pennington et al., 2014). They all aimed at training semantic representations at the level of 
words. Word -level tokenizers are relatively effective in processing European language s, 
where spaces provide clear word boundaries. However, this method is limited in languages 
like Chinese, which do not have clear word boundaries. Moreover, the flexible 
morphological inflections in language, the constant emergence of new words, and the 
prevalence of spelling errors in corpora make it difficult for word -level vocabularies to 
generalize in practical applications.  
Subword technology can be traced back to the 1990s (Gage, 1994). Initially, these 
techniques were mainly used to compress data. With the emergence of large language 
models (LLMs), the demand for tokenizers increased. These complex models require an 
underst anding and generation of extremely rich and diverse language content, and 
traditional word -level tokenizers struggle with complex vocabularies, morphological 
inflections, and the continuous influx of new vocabularies. At this point, subword -level 
tokenizer s became the new mainstream due to their flexibility and generalization 
capabilities.  
 

--- PAGE 3 ---
 
Tokenizer  Representative Papers  Year  Used in Notable 
Models  
BPE  “Neural Machine Translation of Rare 
Words with Subword Units” (Sennrich et 
al., 2016)  2016  GPT -2&3&4  
SentencePiece  “SentencePiece: A simple and language 
independent subword tokenizer and 
detokenizer for Neural Text Processing” 
(Kudo & Richardson, 2018)  2018  ALBERT (Lan et al., 
2020), T5 (Raffel et 
al., 2019), XLNet (Z. 
Yang et al., 2020)  
Unigram  “Subword Regularization: Improving 
Neural Network Translation Models with 
Multiple Subword Candidates” (Kudo, 
2018)  2018  T5 (Raffel et al., 
2019)  
WordPiece  “Japanese and Korean Voice Search” 
(Schuster & Nakajima, 2012);“BERT: Pre -
training of Deep Bidirectional 
Transformers for Language 
Understanding” (Devlin et al., 2019)  2019  BERT (Devlin et al., 
2019), ERNIE (Sun 
et al., 2019)  
 
Table 1: Popular tokenization methods that contributed to the evolution of language models 
in recent years.  
Categories  Examples  
English text  Generative Pre -trained Transformer 4 (GPT -4) is a multimodal large 
language model created by OpenAI, and the fourth in its series of GPT 
foundation models.  
English 
subwords  Gener|ative| Pre| -trained| Transformer| |4| (|G|PT| -|4|)| is| a| 
multim|odal| large| language| model| created| by| Open|AI|,| and| the| 
fourth| in| its| series| of| G|PT| foundation| models|.  
Chinese text  您可以使用下面的工具了解语言模型如何对一段文本进行标记化，以
及这段文本中的标记总数。  
Chinese 
subwords  您|可以 |使用 |下|面|的|工|具|了|解|语|言|模|型|如|何|对|一|段|文|本|
进|行|标|记|化|，|以|及|这|段|文|本|中|的|标|记|总|数|。 
 

--- PAGE 4 ---
Table 2: Examples of text segmentation using BPE. The tokenizer used is provided officially by 
OpenAI for GPT -3.5 and GPT -4 1. 
Balancing Tokens and Types by Subwords  
In the transition from word -level to subword -level, a core consideration is how to balance 
the number of tokens and types. Word -level tokenizers, although producing fewer types, 
are unable to deal with Out -Of-Vocabulary (OOV) units. In contrast, subword -level 
tokenizers significantly reduce the occurrences of OOV, enhancing the model’s adaptability 
to new vocabularies and complex language phenomena.  
For example, BPE and WordPiece, in creating their vocabularies, effectively handle rare 
vocabularies by gradually merging frequently occurring character pairs or combinations, 
while keeping the number of tokens within a reasonable range. SentencePiece and Unigram 
models further improve adaptability to different languages, especially in languages that do 
not use spaces to separate words (like Chinese).  
Segmentation  #Tokens  #Types  
Words  100 million  1,750,000  
BPE  111 million  82,000  
Characters  550 million  3,000  
 
Table 3: An example of the number of tokens/types with different tokenizers on a German 
corpus (See Table 1 in Sennrich et al., (2016)).  
As shown in the comparison in Table 3 , the number of types in BPE is 4.7% of words, while 
the number of tokens is roughly equal (111%); the number of types in characters is 0.2% of 
words, but the number of tokens is 550%. This shows that the subword approach is high -
yield (significantly reduc ed number of types) and low -cost (slightly higher number of 
tokens).  
For languages with rich morphology (like the German corpus shown in Table 3 ), the 
significant reduction in the number of types with subword -level tokenization is mainly 
because it can break down words into frequent subunits, capturing morphological 
variations without needing separate entries for each word form. Although Chinese d oes not 
have a lot of morphological variations, the presence of numerous compound words 
(composed of two or more morphemes, like “ 关闭 ” [shut down], “ 直升机 ” [helicopter]) 
means that sub word -level tokenization can also reduce the number of types.  
The reduction in types lowers the computational complexity and memory requirements of 
LMs, having a direct positive impact on the models’ performance. Moreover, subword -level 
tokenization has a stronger generalization capability for OOV contents or spellin g errors in 
 
1 https://platform.openai.com/tokenizer  

--- PAGE 5 ---
the corpus, as shorter tokens have a higher probability to cover more of the corpus. Thus, 
LLMs generally adopt the subword approachs for alphabetic languages ( Table 1 ). 
This shift from word -level to subword -level not only marks the progress of tokenizer 
technology in the NLP field but also reflects a deeper understanding of language diversity 
and complexity. However, as seen in the examples in Table 2 , unlike languages using the 
Latin alphabet, Chinese BPE subwords are mostly single characters. An analysis2 shows 
that a sentence in Chinese may require 1.7x more tokens than a similar sentence in English, 
and Burmese or Amharic may require 10x more tokens. This indicates that for LLMs in 
various languages, especially non -Latin ones (like Chinese), BPE subwords  still have 
shortcomings. Furthermore, although subword tokenizers have made significant progress 
in handling OOV issues, they still have limitations in capturing the nuanced semantics and 
idiom implications of language. This leads to the need for direct h andling of multiword 
expressions as a complement and refinement of current tokenizer technology.  
 
Current Marginalization of Multiword Expressions (MWEs) in Language Models  
Multiword Expressions, despite playing a crucial role in everyday language, are often 
overlooked in the development of LMs. So far, only AI21 Studio’s Jurassic -X models (Lieber 
et al., 2021) has introduced multi -word tokens, including expressions, phrases, and named  
entities, into their vocabularies. This marginalization may be primarily due to several 
reasons:  
1. Performance and complexity considerations : Introducing MWEs as independent 
tokens will obviously increase the number of types. The vocabulary of the 
aforementioned Jurassic model includes about 250,000 types, much larger than 
most existing vocabularies (5 times or more)3. However, MWEs can be rare or highly 
specific to certain contexts or domains, so their introduction as independent tokens 
does not significantly reduce the total number of tokens. This somewhat contradicts 
the goal of efficient performance pursued by LMs.  Moreover, low -frequency MWEs 
lead to insufficient representation in the training data, making it difficult for the 
models to learn and accurately predict their semantics.  
2. The alternative role of big data and computational power : Current LLMs, like 
GPT -series and BERT, rely on massive training data and high computational power 
to learn the real usages of MWEs rather than their literal expressions, even though 
these expressions are not treated as whole units during training (Tian et al., 2023).  
Despite this, the direct recognition and processing of MWEs still have unique values in LMs 
and LLMs: 1. MWEs can have unique holistic semantics : Incorporating MWEs with 
 
2 https://www.artfish.ai/p/all -languages -are-not-created -tokenized  
3 https://www.ai21.com/blog/announcing -ai21 -studio -and-jurassic -1 

--- PAGE 6 ---
unique holistic semantics, like “kick the bucket” or “ 摸鱼 ” [underwork (actual meaning); 
touching fish (literal meaning)], can enrich the model’s language comprehension 
capabilities. Although this may not significantly reduce the number of tokens, it allows the 
model to capture the specific semantics of texts con taining these MWEs more directly and 
accurately. 2. Some MWEs can reduce the number of types : In some cases, by 
appropriately selecting MWEs, it might even be possible to reduce the total number of 
types. For instance, treating common fixed phrases as single units (like “ 鹦鹉 ” [parrot], “ 乒
乓” [ping pong]) might reduce the need for their individual parts.  
Due to the vast amount and diversity of MWEs, there was a scarcity of MWE lexicons. This 
scarcity consequently hindering their integration into current development of large 
language models. However, linguists and psycholinguists have long studied on MWEs ,  and 
we can rediscover their value based on human cognition:  
• Combining model performance with human language cognition : With 
technological advancements, especially when LMs reach engineering limits, LMs can 
draw more from human language cognition processes.  
• Beyond pure computational power : Although big data and powerful computing 
can solve MWE processing to a certain extent, this “brute force” method might not 
be as efficient and precise as a carefully designed LLM/tokenizer that can directly 
handle MWEs. Like the convolutional neural netw ork (Lecun et al., 1998) for deep 
learning and Reinforcement Learning from Human Feedback (RHLF)(Ouyang et al., 
2022) for LLMs, we can reconsider drawing inspiration from human language 
cognition processes when reaching engin eering limits.  
While the tokenization of subwords and MWEs can offer certain advantages for LMs, 
exploring deeper principles of language processing is still needed in understanding and 
optimizing tokenizers. This article will focus an insight of human language acquisitio n and 
use - the “Principle of Least Effort”.  
Optimizing Future Tokenizers  
In the field of engineering, particularly due to the rapid development of NLP and language 
models, there has been a growing interest in research on tokenizers. This article argue that 
tokenizers, more than mere technical tools, should emulate and learn fro m human language 
processing methods. This argument  is based on two main reasons:  
• Language as a cognitively direct product : While it may be argued that the 
invention of the aeroplane  relied on aerodynamics rather than emulating the way 
birds fly and suggesting that research on language models and tokenizers need not 
emulate human cognition, there is a fundamental difference. The aeroplane is 
designed to fly, not to mimic birds. Howeve r, tokenizers directly process content 
that originates from human cognition, such as written or spoken language. As 
evidence, BPE -family tokenizers, which are the current dominant tokenization 
algorithms, also demonstrate their superiority in terms of cogn itive plausibility 

--- PAGE 7 ---
(Beinborn & Pinter, 2023). In the quest to optimize tokenizer design, therefore, one 
cannot overlook the mechanisms of human language processing.  
• Lack of a general theory : After the decline of linguistic -based approaches in the 
LLM era, the development of tokenizers has often been aimed directly at specific, 
engineering objectives - such as splitting infrequent words into subwords (e.g., 
Sennrich et al., 2016) and improvin g the performance of LMs. This shift has 
occurred without the foundation of a new, general theoretical framework to guide 
the shift. This gap has led to reliance on trial -and-error, making the development 
time -consuming and difficul t to provide systematic guidance for subsequent 
research. In contrast, cognitive science, which has conducted extensive research on 
tokenization (e.g., Arnon & Priva, 2013; Goldwater et al., 2009; Perruchet & Vinter, 
1998; J. Yang, Cai, et al., 2020), is w ell placed to provide the general theory(s) for the 
development of NLP tokenizers.  
This article will present the “Principle of Least Effort,” a general theory from cognitive 
science that can be applied to tokenizers. It also introduces the Less -is-Better (LiB) model, 
which is based on this principle, to demonstrate how cognitive science can guide the 
development of tokenizers.  
Principle of Least Effort  
The acquisition of a linguistic token of a human means that the human is able to perceive 
and generate the token holistically (but can also decompose it if necessary), thereby 
reducing the complexity of language (Isbilen & Christiansen, 2020). Such cognitively 
holistic tokens , whether they are  words, subwords or multiword expressions  (Figure 1) , 
can be referred to as the “cognitive units ” in our mental lexicon4 (J. Yang, 2022 ). 
Furthermore, human tokenization can be described as the process of learning and 
recognizing these cognitive units. In contrast to the strict definitions of subwords, words, 
or MWEs, cognitive units are characterized by their adaptability in size and form. This 
adaptability is evident in how infants and illiterate individuals acquire language - they 
acquire and use various forms of cognitive units from their environment, even without a 
formal understanding of what a “word” is. This observation hints that the  humans are 
capable of identify and adopt suitable cognitive units from language inputs autonomously. 
Current unsupervised tokenization algorithms, which aim to capture linguistic regularities 
such as frequencies (e.g., Sennrich et al., 2016) and transitio n probabilities (e.g., Brugnara 
et al., 1993), reflect this capacity. Yet, despite knowing that the brain can learn these 
 
4 Despite the fact that an individual’s cognitive ability and linguistic experience can lead to 
individual differences in our mental lexicon, the shared language community can still 
maintain a certain degree of consistency in cognitive units. For example, t he word “apple” 
is a cognitive unit for most English speakers, and “ 苹果 ” (Chinese translation of “apple”) is a 
cognitive unit for most Chinese speakers.  

--- PAGE 8 ---
probabilistic regularities (Isbilen et al., 2020; Meltzoff et al., 2009; Schapiro et al., 2016), it 
is difficult to know the specific “algorithm” for human tokenization.  
 
  
Figure 1: Some examples of  words, adverbs or multi -word expressions  that could/are unlikely 
to become cognitive units in our mental lexicon.  
As a general theory in human cognition, George Kingsley Zipf’s Principle of Least Effort 
(PLE), articulated in his book “Human Behavior and the Principle of Least Effort” (1949), 
may bridge the gap between the end goal of tokenization and the myriad approa ches. This 
theory is fundamentally a statistical observation about language and other human behavior 
systems, stating that people tend to follow the path that minimizes effort. One may observe 
that the description of PLE is simple and the objective of PLE is to be simple, which is also 
in line with the simplicity principle  pursued by many early scholars (e.g., Aristotle, Aquinas, 
Kant William of Ockham, Newton, and Kant; see: Baker, 2022) and current cognitive 
scientists (e.g., Chater, 1999; Chater & Vitá nyi, 2003; Feldman, 2016). Applying PLE in 
language processing suggest s minimizing cognitive burden in language learning and usage. 
This involves achieving a balance where each cognitive module involved in language 
processing pursues its own minimal burden. However, language processing is a complex 
task involving multiple cognitive modules. For the sake of practicality, the process should 
be simplified.  
Zipf’s original expression of this principle is encapsulated in his book ( 1949)  that “[the 
person] will strive to solve his problems in such a way as to minimize the total work that he 
must expend in solving both his immediate problems and his probable future problems.” 
The mention of “immediate problems” and “probable future problems ” are crucial, as they 
encompass both short -term and long -term needs to reduce burden, which may be 


--- PAGE 9 ---
conflicting. Thus, following PLE means achieving a balance between short -term and long -
term cognitive burden (it is also consistent with compression theory; see the next 
paragraph). In the previous sections, we have evaluated various approaches based on th e 
number of Types and Tokens, and the benefits of fewer types and fewer tokens have also 
been demonstrated by LLM experiments (Delé tang et al., 2023; Ruoss et al., 2023). From 
the perspective of PLE: 1. Fewer tokens can lessen the burden of working memory 
storage and information decoding steps ; 2. Fewer types can alleviate the burden of 
long -term memory storage and retrieval . This principle can also help us understand the 
shift from word -level tokenizers to subword -level tokenizers, and supports the 
introduction of MWEs in the design of more effective tokenizer for LMs.  
Moreover, recent studies (Delé tang  et al., 2023; Gruver et al., 2023) proposed that LMs can 
be viewed as approximating optimal data compression. This aligns with the Principle of 
Least Effort. Using Minimum Description Length (MDL) theory (Rissanen, 1978), we can 
see that fewer types repre sent a more compressed description of the encoder model, and 
fewer tokens represent a more compressed description of the encoded data. Compression 
seeks the minimal total of the encoder model and encoded data. It is worth noting that a 
crucial difference f rom this data compression theory is that each brain module seeks its 
own minimal burden until the global balance is achieved, since the brain’s various areas 
work in coordination and competition, not necessarily managed by a single global 
controller.  
LiB Model: An Implementation of ‘Principle of Least Effort’  
In response to the limitations of existing tokenizer technologies in LMs, this section will 
introduce a new tokenizer design based on PLE. The Least Effort itself, being a principle, 
can be implemented in various ways. In previous studies (J. Yang, Frank, et al., 2020; J. Yang 
et al., 2022), the author proposed an implementation focused on reducing the burden of 
working memory (number of tokens) and long -term memory (number of types), namely 
the Less -is-Better (LiB) model. This model aims to mimics the lear ning of the flexible 
language units. It breaks through the barriers in defining various linguistic units through 
unsupervised methods, and unifies subwords, words, and multi -word expressions (MWEs) 
into the same vocabulary. In this process, it effectively balances the number of tokens and 
types to reduce the cognitive burden of using language ( Figure 2). The process is, to some 
extent, approaching to the Minimum Description Length, but with a focus on the balance 
between two individual minimization ( min(#tokens) vs. min(#types)) rather than a global 
minimization ( min(#tokens + #types )). 
Model Mechanism : The model consists of a “Memorizer” and a “Forgetter”. Initially, the 
LiB model splits the input corpus into the smallest tokens and then the “Memorizer” 
continuously merges adjacent tokens in the corpus into new (longer) units and stores them 
in the voc abulary. By using longer units, the number of unit tokens in the text decreases, 
while the number of types increases. Conversely, the “Forgetter” removes less useful “junk” 
units from the vocabulary to reduce the number of unit types. “junk” units may be t hose 
types that increase the number of unit tokens in sentences or are infrequently appearing 
types. The “Memorizer” and “Forgetter” balance each other, eventually reaching a relatively 

--- PAGE 10 ---
steady state, where the vocabulary contains units close to the goal of cognitive burden 
minimization.  See more detail  in J. Yang, Frank, et al., (2020) .  
 
Figure 2: The LiB model.  A. the overview i nformation flow , B. the strategies of text 
segmentation, C. the strategies of lexicon/vocabulary update.  
 


--- PAGE 11 ---
Results : The LiB model’s unsupervised method ignores the definition barriers between 
various traditional linguistic units, so its vocabulary also breaks through the usual 
limitations of subwords, words, and multi -word expressions. Two sentences are presented 
in Table 4 for the demonstration. On the vocabulary level , the model autonomously learns 
variform English units like “ly,” “you,” and “you can”, as well as Chinese units like “ 的” 
(English translation: “’s”), “ 孩子 ” (English translation: “kid”), and “ 新华社 ” (English 
translation: “Xinhua News Agency”) (J. Yang, Frank, et al., 2020). This fusion reflects the LiB 
model’s flexibility in learning cognitive units of different sizes and linguistic levels.  
Corpus  Type  Segmentation  
BRphono  Input  allrightwhydon’tweputhimawaynow  
Words  all|right|why|don’t|we|put|him|away|now  
LiB output  allright|whydon’t|we|puthimaway|now  
CTB8  Input  这个出口信贷项目委托中国银行为代理银行  
Words  这|个|出口 |信贷 |项目 |委托 |中国 |银行 |为|代理 |银行  
LiB output  这个 |出口信贷 |项目 |委托 |中国银行 |为|代理 |银行  
Table 4: Example segmentations of strings in the two corpora. BRphono’s results are 
transcribed into  English words for ease of presentation  (see Table 3 in J. Yang, Frank, et al., 
(2020)) . 
 
Practical Application : The units learned by LiB can be used to predict the eye fixation 
patterns of human readers, suggesting that the model’s units are consistent with human 
cognitive units (J. Yang et al., 2022). For corpora in different languages, the LiB model 
flexibly lea rns their lexicons through the unsupervised method based on PLE (J. Yang, 
Frank, et al., 2020; J. Yang et al., 2022), thereby adapting to the complexity and diversity of 
different language inputs, while balancing cognitive loads. Although LiB is only a cog nitive 
model and has not been optimized for language models, evaluations on simple language 
models show that LiB -generated units perform better in Bits -per-character scores (Table 
5). This superior performance may be attributed to that LiB learns fewer tokens and types 
than word -level tokenizers and BPE tokenizers (Table 6). This suggests the value of PLE in 
this era of LARGE language models. We may use the LiB model or other variants that also 
follow PLE as tokenizers for large language models to enhance t heir performance.  
 

--- PAGE 12 ---
Corpus  Metric  Tokenizations  
Character s BPE subword s Word s LiB unit s 
BRphono 
(English)  Average token 
length  1 2.8 2.9 3.6 
Vocabulary  
size  50 5,574  1,321  1,869  
CTB8 
(Chinese)  Average token 
length  1 1.4 1.7 1.9 
Vocabulary 
size  4,697  7,980  65,410  39,320  
Table 5: Average token lengths, lexicon sizes of different tokenizations  on the two corpora. 
The unit of Average Length is English phoneme (BRphono) or Chinese character (CTB8)  (see 
Table 4 in J. Yang, Frank, et al., (2020)) . 
 
Corpus  Model  Character  BPE subword  Word  LiB chunk  
CTB8  
 2-Gram  3.558  2.788  2.333  2.095  
3-Gram  2.025  1.193  1.163  0.903  
BRphono  
 2-Gram  2.221  1.003  0.977  0.791  
3-Gram  1.371  0.563  0.584  0.484  
Table 6: Bits per character scores on different tokenizations (see Table 5 in J. Yang, Frank, et 
al., (2020)) . 
 
This cognitive science -based approach provides a new perspective and direction for the 
future development of language models, especially when dealing with corpora in various 
languages (like Chinese, which lacks clear word boundaries).  

--- PAGE 13 ---
Summary  
This article explores the current choice and future optimization of tokenizers for large 
language models (LLMs), especially in handling complex languages like Chinese. Overall, 
subword tokenization, as a balancing technique, significantly reduces the numbe r of types 
while only slightly increasing the number of tokens compared to word tokenization, 
effectively addressing Out -Of-Vocabulary (OOV) issues and enhancing the model’s 
generalization capabilities. However, this method has limitations in controling th e number 
of tokens in some non -Latin languages (like Chinese), and also in capturing the nuanced 
semantics and idiom implications of language.  
The absence of MWEs in most LMs reflects a blind spot in the current NLP field. Although 
MWEs significantly increase the number of types, and current models can learn the 
meanings of MWEs on subwords tokenization by massive data/computational power, 
direct  recognition and processing of MWEs can still help language models improve the 
accuracy in language understanding. In future development of tokenizers, how to 
effectively select MWEs and balance the number of tokens and types could be a key area for 
tokeni zer advancement.  
To address the issues of current tokenizer technologies, this article discussed the 
importance of emulating human language processing methods in tokenizer design, and 
introduced the “Principle of Least Effort” from cognitive science, which not only reveals  
efficiency and simplicity in human language processing but also, as a general theory in 
cognitive science, can guide the design of more efficient tokenizers. Based on this principle, 
this article proposed the LiB model, a model that attempts to optimize i ts vocabulary 
through learning and forgetting mechanisms, achieving a more effective balance of tokens 
and types. It aims to simulate human language processing mechanisms, reducing cognitive 
burden, and obtaining new types of linguistic cognitive units tha t integrate subwords, 
words, and MWEs, thereby enhancing the efficiency and accuracy of language processing. 
The LiB model is not only a reflection on human language processing mechanisms but also 
provides new ideas for designing more effective tokenizers for LMs. This cognitive science -
based approach provides new perspectives and directions for the future development of 
tokenizers and language models. Incorporating insights from cognitive science with the 
design of large language models may enhance their s ynergistic evolution.  
 
Acknowledgements  
JY was supported by the Lise Meitner Research Group “Language and Computation in 
Neural Systems” of Dr. Andrea E. Martin, funded by the Max -Planck Society and the Max -
Planck -Institute for Psycholinguistics.  
 
Reference  

--- PAGE 14 ---
Arnon, I., & Priva, U. C. (2013). More than words: The effect of multi -word frequency and 
constituency on phonetic duration. Lang. Speech , 56(Pt 3), 349 –371. 
https://doi.org/10.1177/0023830913484891  
Baker, A. (2022). Simplicity. In E. N. Zalta (Ed.), The Stanford encyclopedia of philosophy  
(Summer 2022). 
https://plato.stanford.edu/archives/sum2022/entries/simplicity/ ; Metaphysics 
Research Lab, Stanford University.  
Beinborn, L., & Pinter, Y. (2023). Analyzing cognitive plausibility of subword tokenization. 
In H. Bouamor, J. Pino, & K. Bali (Eds.), Proceedings of the 2023 conference on 
empirical methods in natural language processing  (pp. 4478 –4486). Association for 
Computational Linguistics. https://doi.org/10.18653/v1/2023.emnlp -main.272  
Brugnara, F., Falavigna, D., & Omologo, M. (1993). Automatic segmentation and labeling of 
speech based on hidden markov models. Speech Commun. , 12(4), 357 –370. 
https://doi.org/10.1016/0167 -6393(93)90083 -W 
Chater, N. (1999). The search for simplicity: A fundamental cognitive principle? Q. J. Exp. 
Psychol. A , 52A (2), 273 –302. https://doi.org/10.1080/027249899391070  
Chater, N., & Vitá nyi, P. (2003). Simplicity: A unifying principle in cognitive science? Trends 
Cogn. Sci. , 7(1), 19 –22. https://doi.org/10.1016/s1364 -6613(02)00005 -0 
Delé tang, G., Ruoss, A., Duquenne, P. -A., Catt, E., Genewein, T., Mattern, C., Grau -Moya, J., 
Wenliang, L. K., Aitchison, M., Orseau, L., Hutter, M., & Veness, J. (2023). Language 
modeling is compression . http://arxiv.org/abs/2309.10668  
Devlin, J., Chang, M. -W., Lee, K., & Toutanova, K. (2019). BERT: Pre -training of deep 
bidirectional transformers for language understanding. Proceedings of the 2019 
Conference of the North American Chapter of the Association for Computational 
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , 4171 –
4186. https://doi.org/10.18653/v1/N19 -1423  
Feldman, J. (2016). The simplicity principle in perception and cognition. Wiley Interdiscip. 
Rev. Cogn. Sci. , 7(5), 330 –340. https://doi.org/10.1002/wcs.1406  
Gage, P. (1994). A new algorithm for data compression. The C Users Journal Archive . 
https://www.semanticscholar.org/paper/A -new -algorithm -for-data -compression -
Gage/1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8  
Goldwater, S., Griffiths, T. L., & Johnson, M. (2009). A bayesian framework for word 
segmentation: Exploring the effects of context. Cognition , 112 (1), 21 –54. 
https://doi.org/10.1016/j.cognition.2009.03.008  
Gruver, N., Finzi, M., Qiu, S., & Wilson, A. G. (2023). Large language models are Zero -Shot 
time series forecasters . http://arxiv.org/abs/2310.07820  

--- PAGE 15 ---
Isbilen, E. S., & Christiansen, M. H. (2020). Chunk -Based memory constraints on the cultural 
evolution of language. Top. Cogn. Sci. , 12(2), 713 –726. 
https://doi.org/10.1111/tops.12376  
Isbilen, E. S., McCauley, S. M., Kidd, E., & Christiansen, M. H. (2020). Statistically induced 
chunking recall: A Memory -Based approach to statistical learning. Cogn. Sci. , 44(7), 
e12848. https://doi.org/10.1111/cogs.12848  
Kudo, T. (2018). Subword Regularization: Improving Neural Network Translation Models 
with Multiple Subword Candidates. In I. Gurevych & Y. Miyao (Eds.), Proceedings of 
the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: 
Long Papers)  (pp. 66 –75). Association for Computational Linguistics. 
https://doi.org/10.18653/v1/P18 -1007  
Kudo, T., & Richardson, J. (2018). SentencePiece: A simple and language independent 
subword tokenizer and detokenizer for Neural Text Processing. Proceedings of the 
2018 Conference on Empirical Methods in Natural Language Processing: System 
Demonstrations , 66–71. https://doi.org/10.18653/v1/D18 -2012  
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020, February 8). 
ALBERT: A Lite BERT for Self -supervised Learning of Language Representations . 
https://doi.org/10.48550/arXiv.1909.11942  
Lecun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient -based learning applied to 
document recognition. Proc. IEEE , 86(11), 2278 –2324. 
https://doi.org/10.1109/5.726791  
Lieber, O., Sharir, O., Lenz, B., & Shoham, Y. (2021). Jurassic -1: Technical details and 
evaluation. White Paper. AI21 Labs , 1. 
Meltzoff, A. N., Kuhl, P. K., Movellan, J., & Sejnowski, T. J. (2009). Foundations for a new 
science of learning. Science , 325 (5938), 284 –288. 
https://doi.org/10.1126/science.1175626  
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word 
representations in vector space . http://arxiv.org/abs/1301.3781  
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., 
Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., 
Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022, Mar ch 4). Training language 
models to follow instructions with human feedback . 
https://doi.org/10.48550/arXiv.2203.02155  
Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global vectors for word 
representation. Proceedings of the 2014 Conference on Empirical Methods in Natural 
Language Processing (EMNLP) , 1532 –1543. https://doi.org/10.3115/v1/D14 -1162  
Perruchet, P., & Vinter, A. (1998). PARSER: A model for word segmentation. J. Mem. Lang. , 
39(2), 246 –263. https://doi.org/10.1006/jmla.1998.2576  

--- PAGE 16 ---
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. 
(2019). Exploring the limits of transfer learning with a unified Text -to-Text 
transformer . http://arxiv.org/abs/1910.10683  
Rissanen, J. (1978). Modeling by shortest data description. Automatica , 14(5), 465 –471. 
https://doi.org/10.1016/0005 -1098(78)90005 -5 
Ruoss, A., Delé tang, G., Genewein, T., Grau -Moya, J., Csordá s, R., Bennani, M., Legg, S., & 
Veness, J. (2023). Randomized positional encodings boost length generalization of 
transformers . http://arxiv.org/abs/2305.16843  
Schapiro, A. C., Turk -Browne, N. B., Norman, K. A., & Botvinick, M. M. (2016). Statistical 
learning of temporal community structure in the hippocampus. Hippocampus , 26(1), 
3–8. https://doi.org/10.1002/hipo.22523  
Schuster, M., & Nakajima, K. (2012). Japanese and Korean voice search. 2012 IEEE 
International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 5149 –
5152. https://doi.org/10.1109/ICASSP.2012.6289079  
Sennrich, R., Haddow, B., & Birch, A. (2016). Neural machine translation of rare words with 
subword units. Proceedings of the 54th Annual Meeting of the Association for 
Computational Linguistics (Volume 1: Long Papers) , 1715 –1725. 
https://doi.org/10.18653/v1/P16 -1162  
Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., Tian, X., Zhu, D., Tian, H., & Wu, H. (2019, 
April 19). ERNIE: Enhanced Representation through Knowledge Integration . 
https://doi.org/10.48550/arXiv.1904.09223  
Tian, Y., James, I., & Son, H. (2023). How Are Idioms Processed Inside Transformer 
Language Models? In A. Palmer & J. Camacho -collados (Eds.), Proceedings of the 12th 
Joint Conference on Lexical and Computational Semantics (*SEM 2023)  (pp. 174 –
179). Association for Computational Linguistics. 
https://doi.org/10.18653/v1/2023.starsem -1.16  
Yang, J. (2022). Discovering the units in language cognition: From empirical evidence to a 
computational model  [PhD thesis, Radboud University & Max Planck Institute for 
Psycholinguistics]. https://doi.org/10.13140/RG.2.2.35086.84804  
Yang, J., Cai, Q., & Tian, X. (2020). How do we segment text? Two -stage chunking operation 
in reading. eNeuro , 7(3). https://doi.org/10.1523/ENEURO.0425 -19.2020  
Yang, J., Frank, S. L., & van den Bosch, A. (2020). Less is Better: A cognitively inspired 
unsupervised model for language segmentation. Proceedings of the Workshop on the 
Cognitive Aspects of the Lexicon , 33–45. 
https://www.aclweb.org/anthology/2020.cogalex -1.4 
Yang, J., van den Bosch, A., & Frank, S. L. (2022). Unsupervised text segmentation predicts 
eye fixations during reading. Frontiers in Artificial Intelligence , 5. 
https://doi.org/10.3389/frai.2022.731615  

--- PAGE 17 ---
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2020, January 2). XLNet: 
Generalized Autoregressive Pretraining for Language Understanding . 
https://doi.org/10.48550/arXiv.1906.08237  
Zipf, G. K. (1949). Human behavior and the principle of least effort  (Vol. 573). Addison -
Wesley Press. https://psycnet.apa.org/fulltext/1950 -00412 -000.pdf  
