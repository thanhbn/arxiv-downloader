# 2305.11626.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2305.11626.pdf
# File size: 2217552 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CCT-Code: Cross-Consistency Training for Multilingual Clone Detection
and Code Search
Anton Tikhonov1, Nikita Sorokin1, Dmitry Abulkhanov1,
Irina Piontkovskaya2,Sergey Nikolenko3,Valentin Malykh1,
1MTS AI,2Huawei Noah’s Ark Lab,
3St. Petersburg Department of the Steklov Institute of Mathematics
Correspondence: valentin.malykh@phystech.edu
Abstract
We consider the well-known and important
tasks of clone detection and information re-
trieval for source code. The most standard
setup is to search clones inside the same lan-
guage code snippets. But it is also useful to
find code snippets with identical behaviour in
different programming languages. Neverthe-
less multi- and cross-lingual clone detection
has been little studied in literature. We present
a novel training procedure, cross-consistency
training (CCT) leveraging cross-lingual sim-
ilarity, that we apply to train language mod-
els on source code in various programming
languages. We show that this training is ef-
fective both for encoder- and decoder-based
models. The trained encoder-based CCT-LM
model achieves a new state of the art on POJ-
104 (monolingual C++ clone detection bench-
mark) with 96.73% MAP and AdvTest (mono-
lingual Python code search benchmark) with
47.18% MRR. The decoder-based CCT-LM
model shows comparable performance in these
tasks. In addition, we formulate the multi-
and cross-lingual clone detection problem and
present XCD, a new benchmark dataset pro-
duced from CodeForces submissions.
1 Introduction
The clone detection problem stems from an im-
portant need arising in software development prac-
tice: to identify code with the same behaviour and
effective output; this is useful, e.g., for code uni-
fication, refactoring and control of side effects.
Mou et al. (2016) formulated the clone detection
task for C/C++ source code, and later the problem
was extended to other programming languages.
The natural next step for this problem is to detect
the same behaviour for code across different pro-
gramming languages.1In this work, we formu-
1For an illustrative example, we refer here to the http:
//helloworldcollection.de/ website that contains
“Hello, world!” snippets in 603 programming languages.late the multilingual clone detection task, collect a
dataset, and establish reasonable baselines.
Various approaches for the clone detection task
have been developed, starting from algorithmic
methods (Baker, 1993; Krinke, 2001) and continu-
ing with machine learning models (Li et al., 2017;
Thaller et al., 2020; Gotmare et al., 2021). Most
machine learning approaches are based on learn-
ing the representations (embeddings) of code snip-
pets; this approach allows to find duplicate code
snippets by similarity between their embeddings
in the latent space. The performance of such sys-
tems critically depends on the quality of embed-
dings. In this work, we present a novel training
technique and pipeline called CCT for language
models that allows them to embed code snippets in
an efficient way. We demonstrate this by achiev-
ing state of the art on a preexisting clone detec-
tion dataset POJ-104 (Mou et al., 2016) and on
the newly formulated multilingual clone detection
dataset XCD. Interestingly, we also show that the
CCT technique allows a model to produce repre-
sentations useful for code search as well. Code
search, as formulated by Lu et al. (2021b), is the
task where the query is a text description and the
possible documents are code snippets.
Thus, the main contributions of our work are:
(1) a pretraining method CCT that allows a model
to align code snippets in different programming
languages; (2) a novel multilingual clone detec-
tion dataset XCD based on CodeForces submis-
sions; (3) new state of the art results obtained by
CCT-LM model trained with CCT on the clone
detection datasets POJ-104 and XCD2; (4) state
of the art results of CCT-LM for code search on
theAdvTest dataset. The paper is organized as
follows: Section 2 surveys related work, Sec-
tion 3 discusses code search datasets and intro-
2We will release the XCD dataset and the source code for
CCT and CCT-LM upon acceptance.arXiv:2305.11626v2  [cs.CL]  13 Dec 2024

--- PAGE 2 ---
duces XCD, Section 4 shows the CCT pretraining
approach, Section 5 presents our results, Section 6
analyzes them and presents an ablation study, Sec-
tion 7 concludes the paper, and Section 8 discusses
the limitations of our work.
2 Related Work
Our methods are inspired by natural language pro-
cessing, thus related work includes both pure NLP
and source code processing.
Datasets . Husain et al. (2019) presented the
CodeSearchNet dataset constructed from a GitHub
dump where the authors split method bodies into
the code itself and a description. This dataset
contains 2 million code snippet-description pairs
in 6 programming languages, including Python .
This dataset was partially used by Hasan et al.
(2021) who combined CodeSearchNet and three
other datasets into a larger one. From Code-
SearchNet they used the Java part and Python
part translated automatically into Java. The re-
sulting dataset contains 4 million code snippet-
description pairs. CodeXGLUE presented by
Lu et al. (2021b) is a machine learning bench-
mark collection of datasets for code understand-
ing and generation tasks, which includes a mod-
ification of CodeSearchNet .CodeXGLUE pro-
vides a benchmark for various code-to-code, code-
to-text, text-to-code tasks, including code search.
This benchmark includes code in 10 program-
ming languages. There are two main datasets
for clone detection: POJ-104 (Mou et al., 2016)
andBigCloneBench (Wang et al., 2020). POJ-104
represents a comparatively small corpus of C++
solutions from a student judging system. Big-
CloneBench comprises a vast dataset containing
automatically mined data in the Java language.
Code Search . Early works on code search
mostly relied on classic information re-
trieval (Bacchelli et al., 2010; Brandt et al.,
2010; Campbell and Treude, 2017; Chan et al.,
2012). Brandt et al. (2010), Barzilay et al. (2013),
and Ponzanelli et al. (2014) utilized existing
industrial Web search engines. Gu et al. (2018)
used modern dense vector representations for
information retrieval, training two recurrent
neural networks to represent source code and
text respectively. Feng et al. (2020) presented a
language model-based approach to produce these
representations. Gotmare et al. (2021) used three
Transformer-based models, two as encoders andone as a classifier, for a hierarchical representation
of code and text; although they experimented with
sharing encoder parameters, it lowered the final
quality of their model. Our model, in contrast,
uses a single decoder part of the transformer
to embed queries and documents and skips the
classifier part.
Clone Detection . To the best of our knowl-
edge, the attempt to detect similar program code
was made by Baker (Baker, 1993) who proposed
to find duplicate code in exact and parameter-
ized forms. The latter approach could be re-
formulated as code obfuscation and search for
an obfuscated exact match. Krinke (2001) pro-
posed to apply graph theory, namely to compare
shortest paths of abstract syntactic trees extracted
from the code snippets. Early work on algorith-
mic clone detection was summarized by Roy and
Cordy (2007). Last but not least, we note one of
the most recent algorithmic approaches Sourcer-
erCC (Sajnani et al., 2016), which proposes a so-
phisticated algorithm that includes the construc-
tion of a search index for candidate clone snip-
pets. Recent approaches are usually based on ma-
chine learning. Probabilistic approaches include
Alomari and Stephan (2018) with Latent Dirichlet
allocation and Thaller et al. (2020) with a novel
graph probabilistic model based on density esti-
mation. One of the first successful deep learn-
ing approaches was CClearner (Li et al., 2017)
that used text extracted from a program and its
AST features and had a simplistic multilayer per-
ceptron architecture for clone classification on a
closed code base. More recent deep learning mod-
els include graph neural networks on ASTs (Wang
et al., 2020) and employ pretrained language mod-
els Villmow et al. (2022).
Language models for source code . After the
success of BERT-like models for natural language,
these approaches were also applied to program-
ming languages. Several pre-trained programming
language models were presented recently, includ-
ing: CodeBERT (Feng et al., 2020), which is a bi-
modal pre-trained model for source code and nat-
ural language based on the RoBERTa Transformer
architecture (Liu et al., 2019), trained on masked
language modeling (MLM) and replaced token de-
tection objectives; GraphCodeBERT (Guo et al.,
2021) that uses data flow during pre-training to
solve MLM, edge prediction, and node alignment
tasks; SynCoBERT (Wang et al., 2021) that uses
multimodal contrastive learning and is pre-trained

--- PAGE 3 ---
on identifier prediction and AST edge prediction.
In recent times, autoregressive decoder models
like DeepSeek-Coder (Guo et al., 2024) have taken
over the world. They represent the decoder part
of the transformer model, which is pre-trained on
a huge corpus of source code. They are used in
source code generation tasks, such as code com-
pletion, documentation generation, and so on.
Multilingual NLP . Open-domain question an-
swering task assumes answering factoid questions
without a predefined domain (Kwiatkowski et al.,
2019). Since we propose a novel benchmark in
cross-lingual code understanding, we also review
multilingual NLP models, where recent research
has been focused on non-English question answer-
ing (QA) datasets and multilingual transfer learn-
ing, usually from English to other languages.
Until recently, appropriate training and eval-
uation datasets have been scarce, but in recent
years many works have collected loosely aligned
data obtained through automatic translation or by
parsing similar multilingual sources. Thus, Ahn
et al. (2004) presented an early attempt to multi-
lingual question answering by translating an an-
swer to the target language. Two years later, Bos
and Nissim (2006) presented another similar sys-
tem using translation. Lee and Lee (Lee and
Lee, 2019) showed successful transfer for multi-
lingual QA with training on English data and eval-
uation in Chinese. Ferr ´andez et al. (2007) pre-
sented a work on multilingual knowledge base
incorporating a system. Mitamura et al. (2006)
developed a system with the translation of key-
words extracted from question to get an answer
in the target language. Artetxe et al. (2020) stud-
ied multilingual transfer of monolingual repre-
sentations for a Transformer-based masked lan-
guage model. M’hamdi et al. (2021) examined
a multilingual optimization-based meta-learning
approach to learn to adapt to new languages for
QA. Gao and Callan (2021) proposed unsuper-
vised pretraining for dense passage retrieval, al-
though the authors concentrated on retrieval it-
self and ignored the multilingual nature of the
data. Most approaches used extractive models,
mostly due to the prevalence of datasets where,
similar to SQuAD (Rajpurkar et al., 2016), la-
beled data includes an explicitly stated question,
a passage containing an answer, and a span label-
ing for the answer. However, several works have
considered generative QA: Kumar et al. (2019)
and Chi et al. (2019) studied multilingual ques-tion generation, Riabi et al. (2020) suggested a
method to produce synthetic questions in a mul-
tilingual way by using multilingual MiniLM, and
Shakeri et al. (2020) proposed a method to gener-
ate multilingual question-answer pairs by a gener-
ative model (fine-tuned multilingual T5) based on
samples automatically translated from English to
the target language. Generative QA was mostly
considered for datasets with long answers, but
the generative model FiD (Izacard and Grave,
2021) achieved competitive results on SQuAD-
like datasets, where an answer is supposed to be
a short text span. For open domain QA, Lewis
et al. (2021) used generative models in their RAG
approach that processes top kpassages from the
retriever in the encoder simultaneously and uses
their representations in the decoder for answer
generation, in a process called fusion. Process-
ing the passages independently in the encoder al-
lows a model to scale to many contexts, as it only
runs self-attention over one context at a time. The
FiD model follows this paradigm, further improv-
ing the results in question generation. For QA
over a knowledge graph, Zhou et al. (2021) studied
unsupervised bilingual lexicon induction for zero-
shot multilingual transfer for multilingual QA in
order to map training questions in the source lan-
guage into questions in the target language for use
as augmented training data, which is important for
zero-resource languages.
3 Datasets
In this work we use two kinds of datasets, one for
clone detection and another for code search.
Code Search . For code search we use the
CodeSearchNet dataset introduced by Husain et al.
(2019). The original version of CodeSearchNet
consists of natural language queries paired with
most relevant code snippets in six programming
languages. Each snippet represents the code of a
function collected from GitHub open source code.
CodeSearchNet AdvTest is a Python-only
dataset constructed from the CodeSearchNet cor-
pus by Lu et al. (2021b). Each example, again,
includes a function paired with a text document,
and similar to Husain et al. (2019), AdvTest takes
the first paragraph of the documentation as the
query. Lu et al. (2021b) make an interesting ob-
servation: after normalizing function and variable
names with special tokens, the Mean Recipro-
cal Rank (MRR) scores of RoBERTa (Liu et al.,

--- PAGE 4 ---
2019) and CodeBERT (Feng et al., 2020) mod-
els for code search on the original CodeSearch-
Netdataset drop from 0.809 to 0.419 and from
0.869 to 0.507 respectively (in Python ). So, to im-
prove the quality of the dataset and make it harder,
they first filtered the data by removing examples
for which the code could not be parsed into an ab-
stract syntax tree, the document was shorter than 3
or longer than 256 tokens, the document contained
special tokens such as “http://”, or the document
was empty or not written in English. The filtered
dataset contains 251 820 / 9 604 / 19 210 examples
in its training/validation/test sets respectively.
Then, to better test the understanding and gen-
eralization abilities of the model, Lu et al. (2021b)
normalized function and variable names in test-
ing and development sets to nondescript tokens
such as func for the function name and argi
for the i-th variable name. The task remains to
search for source code based on a natural lan-
guage query. Moreover, in contrast to the test-
ing phase of previous works (Husain et al., 2019;
Feng et al., 2020) that only involved 1 000 can-
didates, Lu et al. (2021b) used the entire test set
for each query, which makes the AdvTest dataset
even more difficult. The training set comes from
the filtered CodeSearchNet dataset (Husain et al.,
2019) where the code is represented in a raw
form in addition to tokenization native to its pro-
gramming language. AdvTest uses MRR as the
basic evaluation metric, defined as MRR@Q =
1
QPQ
i=11
ranki,where Qis the number of queries
andrank is the position of the ground truth answer
document among ranked candidates.
Clone Detection . In the clone detection task,
the problem is to retrieve semantically similar
codes given a code as the query. To train and test
models for clone detection, we use the POJ-104
dataset introduced by Mou et al. (2016). It comes
from a pedagogical programming open judge (OJ)
system that automatically judges the validity of
submitted source code for specific problems by
running the code. The POJ-104 dataset consists
of 104 problems and includes 500 student-written
C/C++ programs for each problem. The clone de-
tection here is, given a program’s source code, to
retrieve other programs that solve the same prob-
lem. The problems are grouped into three sets
with 64/16/24 problems for training, validation,
and testing respectively. The default metric for
the POJ-104 dataset is Mean Average Precision
(MAP), where the average precision (AP) is de-fined as AP =P100
i=1(Ri−Ri−1)·Pi,where Ri
andPiare the precision and recall at threshold i,
i.e., computed taking into account only top iitems
from the candidate list. MAP is the mean AP over
all queries. It is important to mention that for POJ-
104 the maximal possible iis 499 since there are
only 500 candidates in total.
3.1 XCD Dataset
In previous works, multilingual abilities of code
language models were not sufficiently investi-
gated. To fill this gap, we introduce a new
multilingual clone detection/code retrieval dataset
XCD. The dataset is evaluated in two different
settings: clone detection approach similar to the
BUCC dataset (Xu et al., 2018), retrieval style
clone detection similar to POJ-104 (Mou et al.,
2016), and a hybrid approach. We use the Code-
Forces submissions dump as the data source, ran-
domly choosing 110 problems and 100 accepted
solutions in one language for each problem. We
chose 5 languages, namely Python, Java, C#, C++,
C, getting 55 000 code snippets in total.
Full comparison evaluation setup . Here the
task is interpreted as binary classification, similar
to Xu et al. (2018). We evaluate a model on each
pair from test set, which gives rise to n2compar-
isons. Each pair could be either positive, if the pair
consists of solutions for the same problem, or neg-
ative otherwise. We use the classic F1measure for
evaluation in this setup: F1=2·Precision ·Recall
Precision+Recall.
Retrieval style evaluation setup . For retrieval
style evaluation, we follow the design of POJ-
104. The task aims to retrieve 100 snippets per
language solving the same problem from the test
set. We evaluate ranking on 11 000 positive snip-
pets, i.e., the model should rank 11 000 docu-
ments, bringing on top the snippets solving the
same problem. Similar to Mou et al. (2016), we
use mean average precision on the top 100 results
(MAP@100) for evaluation.
Hybrid evaluation setup . We also propose a
setup where we evaluate the models not only on
positive snippets but on all snippets in the same
language. This task is harder and more similar to
AdvTest . As the metric we chose MRR@R, fol-
lowing Lu et al. (2021b).
Cross-lingual evaluation . In addition to the
multilingual setting, i.e., evaluation in the set of
solutions in a single language, we define a more
complex cross-lingual setting designed to measure
cross-lingual code understanding. To achieve this,

--- PAGE 5 ---
we extend the three setups to all languages at once,
extending the sets of both relevant and irrelevant
snippets to all programming languages.
3.2 Additional Labeling
In addition to the solution status (“Accepted” or
not), we also mined error statuses for the solutions
since the platforms used for problem solving of-
ten provide them. In total, we mined more than 97
million code snippets in more than 10 program-
ming languages. The CodeForces platform can re-
turn 15 types of verdicts for a submitted solution.
We split the verdicts into 4 groups: Defect (code
has a defect), Skip (code cannot be judged), Ac-
cepted (no defects detected), and Wrong (code that
fails some tests or constraints). Below we describe
and classify some of the most common verdicts:
(1)Memory limit exceeded : the program tries
to consume more memory than allowed ( Wrong );
(2)Time limit exceeded : the program has not ter-
minated in allotted time ( Wrong ); (3) Runtime er-
ror: the program terminated with a non-zero return
code (e.g., due to division by zero, stack overflow
etc.) ( Defect ); (4) Wrong answer on some tests
(Wrong ); (5) Idleness limit exceeded : the program
did not use the CPU for a considerable time ( De-
fect); (6) Denial of judgement : the solution was
impossible to run due to a judging error or an er-
ror in the program, e.g., using extra large arrays
(Defect ); (7) Rejected : the program does not pass
tests for an unclear reason ( Skip); (8) Accepted :
the program passed all tests.
4 Method
In this section, we introduce our pre-training ap-
proach CCT (Cross-Consistency Training). Its
goal is to robustly learn the embedding space of
code snippets and create a strong alignment be-
tween snippets solving the same problems across
programming languages. The difference between
strong and weak alignment is illustrated in Fig. 1:
in a weakly aligned embedding space, the near-
est neighbor might be a semantically similar snip-
pet from a different language but generally most
neighbors are in the same language, while in a
strongly aligned space the similarity is purely se-
mantic and does not care about the language at all.
To achieve strong alignment, we employ a con-
trastive learning objective LXCD: for a randomly
code snippet, we train the vector representations
of the source code tokens in such a way that their
querydocument
relevantnon-relevant cpp
javapythonStrong Alignment
 Weak Alignment
Figure 1: Strong and weak cross-lingual alignment.
aggregation, for example, averaging or last token,
is closer to the source code, which solves the same
problem regardless of the programming language.
This ensures that the embeddings of the source
code differentiates between related snippets and
random or similar but different (hard negative)
snippets effectively.
Noise-contrastive estimation and losses . To
learn a language-agnostic cross-lingual represen-
tation space, we propose a training procedure
based on noise contrastive estimation (NCE). Let
XandZbe some finite sets and sθ:X × Z →
Rbe a relevance score function differentiable in
θ∈Rd. The goal is to learn θsuch that the clas-
sifier x7→arg maxz∈Zsθ(x,z)has the optimal
expected loss. This leads to conditional density
estimation: for every x∈ X
pθ(z|x) =esθ(x,z)
P
z−∈Zesθ(x,z−)(1)
withθ∗= arg min
θEx,z[−logpθ(z|x)]being the
optimum. In practice, optimizing this objective di-
rectly is infeasible: if Zis large the normalization
term in (1) is intractable. Therefore, NCE uses
subsampling, so (1) becomes
πθ(z|x) =esθ(x,z)
P
z−∈Bx,zesθ(x,z−)+esθ(x,z),(2)
where Bx,z={z−
1,z−
2, . . . , z−
n}is a set of nega-
tives sampled from Zthat do not match the pos-
itive answer z+for this x. NCE also often uses
objectives similar to (2) but with πθ(ˆz|z)where z
andˆzcome from the same space, and the objective
corresponds to some similarity function.
Cross-lingual objective . Contrastive learning
frequently employs pretext tasks to learn data rep-
resentations without the need for labeled exam-
ples. In the context of learning from a multilingual

--- PAGE 6 ---
set of documents, a possible pretext task would be
to train a network to differentiate between docu-
ments with similar content but written in different
languages (positive pairs) and those with dissimi-
lar content (negative pairs). This leads to the loss
function Training with this task leads to learning a
representation space with strong alignment prop-
erties, but the IR ability of the representations is
mediocre.
LXCD(θ) =E(ˆz,z)∼WXCD[−logπθ(ˆz|z)],(3)
whereWXCD is a distribution on the set of pairs of
submissions in different programming languages
from the XCD dataset (Section 3) that shows if the
submissions are solving the same problem or not.
Hard negative mining . Previous works on con-
trastive learning show the importance of training
on hard negative samples (Qu et al., 2021; Izacard
and Grave, 2020). They used iterative training to
get hard negatives, but our data already contains
strong negative examples as preliminary solutions
from the same users that solve the same problems
but fail some tests (that is why a user would submit
an updated solution to get the “Accepted” verdict).
Thus, we mine hard negative examples as failed
solutions from the same user; if there are none we
use failed solutions from random users, and only if
there are none (e.g., for an unpopular problem) we
use a random submission for a random problem.
5 Experiments
In this section, we describe the details about data
pre-training and our CCT pipeline for multilingual
clone detection and code search tasks.
Pretraining . We train two models, one is
encoder-based, which is initialized with pre-
trained GraphCodeBERTbase model (Guo et al.,
2021); we call the resulting model CCT-LM enc.
Another one is decoder-based, which is ini-
tilized with a pretrained DeepSeek-Coder-1.3B
model (Guo et al., 2024); we call the resulting
model CCT-LM dec. Similarity scores are calcu-
lated based on dot products of the last token vector
representations, but we also researched using var-
ious types of poolings and allowing bidirectional
attention.
Hyperparameters . We use the AdamW opti-
mizer with learning rate 5e-5, weight decay 0.01,
and linear learning rate decay. We use gradient ac-
cumulation for pretraining with an effective batch
size of 2000.Clone Code
detection search
(MAP )(MRR )
Endcoder-only
RoBERTa-base (Liu et al., 2019) 76.67 18.33
CodeBERT (Feng et al., 2020) 82.67 27.19
SynCoBERT (Wang et al., 2021) 88.24 38.10
CodeRoBERTa — 42.35
GraphCodeBERT (Guo et al., 2021) 85.16 —
CasCode (Gotmare et al., 2021) — 43.98
Villmow et al. (2022) 91.34 —
CCT-LM enc 96.73 47.18
Decoder-only
CodeGen (Nijkamp et al., 2023) 89.68 —
CodeGPT (Lu et al., 2021a) 87.96 —
SantaCoder (Allal et al., 2023) 83.98 —
Phi-1 (Gunasekar et al., 2023) 92.72 —
CCT-LM dec 95.50 —
Table 1: Results on code clone detection on the POJ-
104 dataset and code search on the AdvTest dataset.
Monolingual Results . Tab. 1 presents the re-
sults of CCT-LM models compared to existing ap-
proaches, showing that CCT-LM outperform all
previous models by a large margin in this mono-
lingual setting. Thus, strong alignment enforced
by CCT pretraining is not only helpful for multi-
lingual transfer but also improves the latent space
structure in general. It is important to mention,
that CCT pretraining works for both encoder- and
decoder-based models, improving the results.
5.1 Multi- and Cross-lingual Evaluation
For these types of evaluation on XCD we use sev-
eral setups described in Sec. 3.1. Since these se-
tups are computationally intensive we work only
with encoder-based models.
Multilingual Results . Results in the multi-
lingual setting on the proposed XCD dataset are
shown in the top half of Tab. 2. In the full compar-
ison setup, it is interesting that knowledge transfer
from the POJ-104 dataset also does not help, and
the metrics remain very low. However, CCT-LM
shows much better results, obviously due to the
multilingual design of CCT pretraining. We do
not evaluate BM25 in this setup since it is not sup-
posed to compare two documents.
For retrieval style evaluation, the results also
show CCT-LM enc strongly outperforming all
baselines and actually providing a viable solution
for the problem while GraphCodeBERT does not
yield reasonable results in any programming lan-

--- PAGE 7 ---
Python Java C# Ruby JS Haskell PHP OCaml Perl Avg
Multilingual setting
Full Comparison, F1measure
GraphCodeBERTbase 0.02 0.05 0.00 0.04 0.00 0.02 0.01 0.03 0.01 0.02
GraphCodeBERTPOJ
base 0.04 0.00 0.01 0.06 0.07 0.08 0.06 0.06 0.06 0.05
CCT-LM enc 22.24 18.39 17.33 23.33 10.46 17.64 21.43 17.01 16.40 18.24
Retrieval Style, MAP@100
BM25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
GraphCodeBERTbase 7.21 9.25 1.33 4.28 1.59 5.78 6.08 2.90 10.37 5.42
GraphCodeBERTPOJ
base 30.12 24.63 23.54 32.78 36.64 24.45 37.21 33.94 45.33 32.07
CCT-LM enc 87.42 55.99 65.35 72.12 74.32 81.05 83.21 71.53 71.89 73.65
Hybrid, MRR@20
BM25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
GraphCodeBERTbase 2.08 5.42 0.22 2.59 0.80 1.99 2.90 1.40 5.23 2.51
GraphCodeBERTPOJ
base 27.10 20.04 19.44 30.98 28.37 19.70 32.89 30.08 39.98 27.62
CCT-LM enc 74.97 62.08 58.77 80.60 74.56 62.27 81.21 72.64 79.16 71.80
Cross-lingual setting
Full Comparison, F1measure
GraphCodeBERTbase 0.01 0.01 0.00 0.01 0.00 0.01 0.01 0.01 0.01 0.01
GraphCodeBERTPOJ
base 0.01 0.00 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
CCT-LM enc 8.92 9.46 4.78 6.01 7.33 5.82 6.47 5.33 3.56 6.40
Retrieval Style, MAP@100
BM25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
GraphCodeBERTbase 3.18 5.24 0.23 1.77 1.15 3.38 3.12 1.90 16.27 4.02
GraphCodeBERTPOJ
base 12.83 14.75 9.33 12.78 17.16 15.94 19.53 16.01 23.88 15.80
CCT-LM enc 44.82 20.34 23.33 35.01 32.57 40.07 43.36 36.66 37.80 34.88
Hybrid, MRR@20
BM25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
GraphCodeBERTbase 1.24 2.42 0.34 1.28 0.82 0.93 1.43 0.76 2.15 1.26
GraphCodeBERTPOJ
base 20.12 13.08 10.37 17.28 12.62 19.70 14.31 18.08 18.33 15.98
CCT-LM enc 30.83 22.77 19.32 32.66 31.64 20.80 31.59 40.42 39.40 29.93
Table 2: Multilingual clone detection in two evaluation setups on the XCD dataset.
guage. We note that BM25 (a strong baseline for
natural language IR tasks) does not work for clone
detection, which is natural since it relies on iden-
tical tokens which could be scarce even between
code snippets solving the same problem.
Results for the hybrid evaluation setup show the
same picture: BM25 still does not work, code lan-
guage models can transfer knowledge across dif-
ferent solutions to some extent, and the improve-
ment in metrics here between a fully unsupervised
method and training on POJ-104 clone detection is
noticeably greater. Still, CCT-LM encfar exceeds
every other method here and generally sets a new
baseline for multilingual code-related tasks.
Cross-lingual Results . Our results in this set-ting are presented in the bottom half of Tab. 2.
All conclusions derived for the multilingual case
(above) apply here too, but in comparison to the
multilingual setting, cross-lingual tasks are signif-
icantly harder and all values are lower. We suggest
that the difference in the results across program-
ming languages could be caused by the imbalance
in the pretraining dataset.
6 Analysis
Zero-shot Results . We investigated zero-shot
transfer from Python to Java, Ruby, PHP, Go, and
JavaScript on the CodeSearchNet dataset for pre-
viously introduced code language models and our
CCT-LM. The zero-shot results are presented in

--- PAGE 8 ---
Java Ruby PHP Go JS Avg
CodeBERT base 46.37 50.65 37.83 50.65 50.48 47.19
GraphCodeBERTbase 47.33 59.95 37.47 60.28 52.04 51.41
CCT-LM enc 48.71 62.25 42.78 61.44 51.06 53.24
Table 3: Zero-shot retrieval; F1score, CodeSearchNet .
Clone Code Defect
Detection Search detection
GraphCodeBERT (MAP )(MRR )(Acc)
Base 85.16 45.80 62.51
Base + LXCD 95.92 29.93 61.05
Base + LXCD +LLM 95.67 47.18 63.68
Base + LXCD +LLM 96.03 45.22 64.91
Base + LXCD +LLM+ SL 96.46 47.33 -
Base + LXCD +LLM+Lerr+ SL 96.73 47.57 65.58
Table 4: GraphCodeBERT variations: clone detection
on POJ-104, code search on AdvTest , defect detection
onDevign ; SL denotes the size limit.
Table 5: A comparison of DeepSeek-Coder 1.3b varia-
tions: clone detection on POJ-104, code search on Ad-
vTest
Table 3. As evidence for the power of pretrained
language models, we see that existing approaches
show rather good results even though they have
not been trained on the retrieval task. By lever-
aging its multilingual ability, CCT-LM improves
over the baselines in the zero-shot setup for all lan-
guages except JavaScript (JS).
Latent space structure . Figure 1 showed
an abstract representation of the basic CCT idea
of semantically aligned language-agnostic embed-
ding space. Figure 2 turns this theory into prac-
tice with projections of actual embeddings for
sample code snippets before and after CCT train-
ing. The snippets represent solutions for 12 sam-
ple tasks in six programming languages. We see
that after CCT, representations of code snippets
are not aligned by language but rather by prob-
lem (Fig. 2b), while their alignment had been
language-dependent before CCT (Fig. 2a).
This illustrates that CCT training significantly
improves the multilingual latent space for code
snippets, making it semantic and language-
agnostic.
Ablation Study . In this section, we study the
effects of various parts of CCT. Table 5 shows the
results of several DeepSeek-Coder-based models
on clone detection, code search tasks. We com-
pare the DeepSeek-Coder base model with differ-
ent pretraining poolings and attention types.
problem D18 G656 G784 D795 A795 K774
J795 E795 D926 C953 F953 F926CodeBER T CCT
(a) Projected embeddings of 12 coding problems.
language java cpp python ccsharp rubyCodeBER T CCT
(b) The same embeddings by programming language.
Figure 2: Sample multilingual embeddings.
7 Conclusion
Understanding semantic similarity is an important
aspect of language processing that opens up ways
to solve many different tasks, for both natural and
programming languages. In this work, we have
presented a new method CCT-LM that improves
this ability via a novel CCT pretraining approach
and demonstrated its viability for both clone de-
tection and code search. We have formulated a
novel task of multilingual clone detection and pre-
sented the XCD dataset for multilingual source
code analysis, formalized in two evaluation setups.
The proposed CCT-LM models (both encoder- and
decoder-based) outperformed strong baselines in
clone detection and code search task. CCT-LM enc
exceed other models in all the setups for multi-
and cross-lingual evaluation, proving that CCT
pretraining provides better semantic similarity un-
derstanding for a language model. We hope that
our method will be helpful in other source code
processing tasks that we have left as future work.
Moreover, we believe that modifications of our ap-
proach can be useful for NLP and perhaps other
fields of machine learning.

--- PAGE 9 ---
8 Limitations
We have studied several programming languages,
including Python and Java, in our XCD setup;
although all our methods seem to be language-
agnostic, a further study for other languages would
be interesting, especially since all considered lan-
guages are interpreted rather than compiled (like
C/C++). Many inputs exceed 512 tokens; we
used standard truncation for evaluation (taking
into consideration only the beginning of the code),
which may be suboptimal, and more suitable in-
put representations could be found. We expect
our model to improve with training on long docu-
ments. We also suppose that the model would ben-
efit from increasing the batch size by using more
powerful hardware with more memory. Note also
that while CCT-LM significantly improved state of
the art in clone detection and code search.
References
Kisuh Ahn, Beatrice Alex, Johan Bos, Tiphaine Dal-
mas, Jochen L Leidner, and Matthew B Smillie.
2004. Cross-lingual question answering using off-
the-shelf machine translation. In Workshop of the
Cross-Language Evaluation Forum for European
Languages , pages 446–457. Springer.
Loubna Ben Allal, Raymond Li, Denis Kocetkov,
Chenghao Mou, Christopher Akiki, Carlos Munoz
Ferrandis, Niklas Muennighoff, Mayank Mishra,
Alex Gu, Manan Dey, Logesh Kumar Umapathi,
Carolyn Jane Anderson, Yangtian Zi, Joel Lamy
Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry
Abulkhanov, Manuel Romero, Michael Lappert,
Francesco De Toni, Bernardo Garc ´ıa del R ´ıo,
Qian Liu, Shamik Bose, Urvashi Bhattacharyya,
Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco
Zocca, Sourab Mangrulkar, David Lansky, Huu
Nguyen, Danish Contractor, Luis Villa, Jia Li,
Dzmitry Bahdanau, Yacine Jernite, Sean Hughes,
Daniel Fried, Arjun Guha, Harm de Vries, and Le-
andro von Werra. 2023. Santacoder: don’t reach for
the stars!
Hakam W Alomari and Matthew Stephan. 2018. To-
wards slice-based semantic clone detection. In
2018 IEEE 12th International Workshop on Soft-
ware Clones (IWSC) , pages 58–59. IEEE.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020. On the cross-lingual transferability of mono-
lingual representations. In Proceedings of the 58th
Annual Meeting of the Association for Computa-
tional Linguistics , pages 4623–4637, Online. Asso-
ciation for Computational Linguistics.
Alberto Bacchelli, Michele Lanza, and Romain
Robbes. 2010. Linking e-mails and source codeartifacts. In Proceedings of the 32nd ACM/IEEE
International Conference on Software Engineering-
Volume 1 , pages 375–384.
Brenda S Baker. 1993. A program for identifying du-
plicated code. Computing Science and Statistics ,
pages 49–49.
Ohad Barzilay, Christoph Treude, and Alexey Zagal-
sky. 2013. Facilitating crowd sourced software engi-
neering via stack overflow. In Finding Source Code
on the Web for Remix and Reuse , pages 289–308.
Springer.
Johan Bos and Malvina Nissim. 2006. Cross-lingual
question answering by answer translation. In CLEF
(Working Notes) . Citeseer.
Joel Brandt, Mira Dontcheva, Marcos Weskamp, and
Scott R Klemmer. 2010. Example-centric program-
ming: integrating web search into the development
environment. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems ,
pages 513–522.
Brock Angus Campbell and Christoph Treude. 2017.
Nlp2code: Code snippet content assist via natural
language tasks. In 2017 IEEE International Confer-
ence on Software Maintenance and Evolution (IC-
SME) , pages 628–632. IEEE.
Wing-Kwan Chan, Hong Cheng, and David Lo. 2012.
Searching connected api subgraph via text phrases.
InProceedings of the ACM SIGSOFT 20th Interna-
tional Symposium on the Foundations of Software
Engineering , pages 1–11.
Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-
Ling Mao, and Heyan Huang. 2019. Cross-lingual
natural language generation via pre-training.
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
BERT: A Pre-Trained Model for Programming and
Natural Languages. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
1536–1547, Online. Association for Computational
Linguistics.
Sergio Ferr ´andez, Antonio Toral, Oscar Ferr ´andez, An-
tonio Ferr ´andez, and Rafael Munoz. 2007. Applying
wikipedia’s multilingual knowledge to cross–lingual
question answering. In International Conference
on Application of Natural Language to Information
Systems , pages 352–363. Springer.
Luyu Gao and Jamie Callan. 2021. Unsupervised cor-
pus aware language model pre-training for dense
passage retrieval.
Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, and
Steven CH Hoi. 2021. Cascaded fast and slow mod-
els for efficient semantic code search. arXiv preprint
arXiv:2110.07811 .

--- PAGE 10 ---
Xiaodong Gu, Hongyu Zhang, and Sunghun Kim.
2018. Deep code search. In 2018 IEEE/ACM 40th
International Conference on Software Engineering
(ICSE) , pages 933–944. IEEE.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
C´esar Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,
Harkirat Singh Behl, Xin Wang, S ´ebastien Bubeck,
Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and
Yuanzhi Li. 2023. Textbooks are all you need.
Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin Clement, Dawn Drain, Neel
Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.
2021. GraphCodeBERT: Pre-training Code Repre-
sentations with Data Flow. In ICLR .
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Y . Wu, Y . K. Li, Fuli Luo, Yingfei Xiong, and Wen-
feng Liang. 2024. Deepseek-coder: When the large
language model meets programming – the rise of
code intelligence.
Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ish-
tiaq, Kazi Sajeed Mehrab, Md Mahim Anjum
Haque, Tahmid Hasan, Wasi Ahmad, Anindya Iqbal,
and Rifat Shahriyar. 2021. Codesc: A large code–
description parallel dataset. In Findings of the
Association for Computational Linguistics: ACL-
IJCNLP 2021 , pages 210–218.
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
SearchNet Challenge: Evaluating the State of Se-
mantic Code Search.
Gautier Izacard and Edouard Grave. 2020. Distilling
knowledge from reader to retriever for question an-
swering.
Gautier Izacard and ´Edouard Grave. 2021. Leveraging
passage retrieval with generative models for open
domain question answering. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 874–880.
J. Krinke. 2001. Identifying similar code with program
dependence graphs. In Proceedings Eighth Work-
ing Conference on Reverse Engineering , pages 301–
309.
Vishwajeet Kumar, Nitish Joshi, Arijit Mukherjee,
Ganesh Ramakrishnan, and Preethi Jyothi. 2019.
Cross-lingual training for automatic question gen-
eration. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 4863–4872, Florence, Italy. Association
for Computational Linguistics.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et al. 2019. Natural questions: a bench-
mark for question answering research. Transactions
of the Association for Computational Linguistics ,
7:453–466.
Chia-Hsuan Lee and Hung-Yi Lee. 2019. Cross-
lingual transfer learning for question answering.
arXiv preprint arXiv:1907.06042 .
Patrick Lewis, Ethan Perez, Aleksandra Piktus,
Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich K ¨uttler, Mike Lewis, Wen tau Yih,
Tim Rockt ¨aschel, Sebastian Riedel, and Douwe
Kiela. 2021. Retrieval-augmented generation for
knowledge-intensive nlp tasks.
Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, and
Barbara Ryder. 2017. Cclearner: A deep learning-
based clone detection approach. In 2017 IEEE Inter-
national Conference on Software Maintenance and
Evolution (ICSME) , pages 249–260. IEEE.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint .
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021a. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021b.
Codexglue: A machine learning benchmark dataset
for code understanding and generation. In Thirty-
fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1) .
Meryem M’hamdi, Doo Soon Kim, Franck Der-
noncourt, Trung Bui, Xiang Ren, and Jonathan
May. 2021. X-METRA-ADA: Cross-lingual meta-
transfer learning adaptation to natural language un-
derstanding and question answering. In Proceed-
ings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
3617–3632, Online. Association for Computational
Linguistics.
Teruko Mitamura, Mengqiu Wang, Hideki Shima, and
Frank Lin. 2006. Keyword translation accuracy
and cross-lingual question answering inchinese and
japanese. In Proceedings of the Workshop on Multi-
lingual Question Answering-MLQA ‘06 .

--- PAGE 11 ---
Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.
2016. Convolutional neural networks over tree
structures for programming language processing.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,
Huan Wang, Yingbo Zhou, Silvio Savarese, and
Caiming Xiong. 2023. Codegen: An open large lan-
guage model for code with multi-turn program syn-
thesis.
Luca Ponzanelli, Gabriele Bavota, Massimiliano
Di Penta, Rocco Oliveto, and Michele Lanza. 2014.
Mining stackoverflow to turn the ide into a self-
confident programming prompter. In Proceedings
of the 11th working conference on mining software
repositories , pages 102–111.
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang
Ren, Xin Zhao, Daxiang Dong, Hua Wu, and
Haifeng Wang. 2021. Rocketqa: An opti-
mized training approach to dense passage re-
trieval for open-domain question answering. ArXiv ,
abs/2010.08191.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang. 2016. Squad: 100,000+ ques-
tions for machine comprehension of text. Preprint ,
arXiv:1606.05250.
Arij Riabi, Thomas Scialom, Rachel Keraron, Beno ˆıt
Sagot, Djam ´e Seddah, and Jacopo Staiano. 2020.
Synthetic data augmentation for zero-shot cross-
lingual question answering. arXiv preprint
arXiv:2010.12643 .
Chanchal Roy and James Cordy. 2007. A survey on
software clone detection research. School of Com-
puting TR 2007-541 .
Hitesh Sajnani, Vaibhav Saini, Jeffrey Svajlenko,
Chanchal K Roy, and Cristina V Lopes. 2016.
Sourcerercc: Scaling code clone detection to big-
code. In Proceedings of the 38th International Con-
ference on Software Engineering , pages 1157–1168.
Siamak Shakeri, Noah Constant, Mihir Sanjay Kale,
and Linting Xue. 2020. Multilingual syn-
thetic question and answer generation for cross-
lingual reading comprehension. arXiv preprint
arXiv:2010.12008v1 .
Hannes Thaller, Lukas Linsbauer, and Alexander
Egyed. 2020. Towards semantic clone detection via
probabilistic software modeling. In 2020 IEEE 14th
International Workshop on Software Clones (IWSC) ,
pages 64–69. IEEE.
Johannes Villmow, Viola Campos, Adrian Ulges, and
Ulrich Schwanecke. 2022. Addressing leakage in
self-supervised contextualized code retrieval. arXiv
preprint .
Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi
Jin. 2020. Detecting code clones with graph neu-
ral network and flow-augmented abstract syntax
tree. In 2020 IEEE 27th International Conferenceon Software Analysis, Evolution and Reengineering
(SANER) , pages 261–271. IEEE.
Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao
Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin
Jiang. 2021. SynCoBERT: Syntax-Guided Multi-
Modal Contrastive Pre-Training for Code Represen-
tation. arXiv preprint . Number: arXiv:2108.04556
arXiv:2108.04556 [cs].
Ruochen Xu, Yiming Yang, Naoki Otani, and Yuexin
Wu. 2018. Unsupervised cross-lingual transfer of
word embedding spaces. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 2465–2474, Brussels, Bel-
gium. Association for Computational Linguistics.
Yucheng Zhou, Xiubo Geng, Tao Shen, Wenqiang
Zhang, and Daxin Jiang. 2021. Improving zero-shot
cross-lingual transfer for multilingual question an-
swering over knowledge graph. In Proceedings of
the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 5822–5834,
Online. Association for Computational Linguistics.
