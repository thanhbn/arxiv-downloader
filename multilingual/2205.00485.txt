# 2205.00485.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2205.00485.pdf
# File size: 109098 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2205.00485v1  [cs.CL]  1 May 2022BILINGUAL END-TO-END ASR WITH BYTE-LEVEL SUBWORDS
Liuhui Deng, Roger Hsiao, and Arnab Ghoshal
Apple
{liuhuideng, rhsiao, aghoshal }@apple.com
ABSTRACT
In this paper, we investigate how the output representation of
an end-to-end neural network affects multilingual automat ic speech
recognition (ASR). We study different representations inc luding
character-level, byte-level, byte pair encoding (BPE), an d byte-
level byte pair encoding (BBPE) representations, and analy ze their
strengths and weaknesses. We focus on developing a single en d-to-
end model to support utterance-based bilingual ASR, where s peakers
do not alternate between two languages in a single utterance but may
change languages across utterances. We conduct our experim ents on
English and Mandarin dictation tasks, and we ﬁnd that BBPE wi th
penalty schemes can improve utterance-based bilingual ASR perfor-
mance by 2% to 5% relative even with smaller number of outputs
and fewer parameters. We conclude with analysis that indica tes di-
rections for further improving multilingual ASR.
Index Terms —Bilingual speech recognition, end-to-end neural
network, byte-level subwords
1. INTRODUCTION
End-to-end (E2E) neural network based automatic speech rec ogni-
tion (ASR) provides a degree of ﬂexibility and performance t hat
makes E2E neural models an attractive option for multilingu al ASR.
A number of studies [1] [2] [3] have focused on building a sing le E2E
model with multiple languages. The resulting E2E model can p er-
form utterance-based multilingual ASR. The works in [4] [5] [6] [7]
aim to build an E2E model that can improve code switching. Whi le
these approaches are different from each other, there are so me simi-
larities among them. First, they aim to build a single E2E mod el to
realize multilingual ASR. Second, the outputs of these mode ls are
often unions of the characters or subwords of the target lang uages.
One exception would be the work in [8], which proposes to use U TF-
8 codewords, a byte-level representation, to represent the target lan-
guages.
Byte-level models have been proposed for natural language p ro-
cessing (NLP) [9] [10] [11]. The idea is to convert text to a se quence
of variable-length UTF-8 codewords, and to have the model pr edict
one byte at each decoding step. The advantages of byte-level rep-
resentation are compactness and universality, as any combi nation of
languages may be represented with an output dimension of onl y 256.
However, a sequence represented at the byte level is always m uch
longer than its character-level counterpart for languages such as Chi-
nese and Japanese [12], which is because many characters of t hese
languages are represented by multiple bytes in UTF-8. As a re sult,
a byte-level model can be error-prone since it needs to make m ulti-
ple predictions for many single characters, and each predic tion has
a chance to make a mistake. To compensate for this drawback, [ 12]
proposes byte-level subwords for neural machine translati on. The
idea is to apply byte pair encoding (BPE) [13] to UTF-8 codewo rdsequences and as a result, an approach referred to as byte-le vel BPE
(BBPE). BBPE inherits the advantages of UTF-8 byte-level re pre-
sentation. BBPE is able to represent all languages while kee ping the
output dimension in check. At the same time, as BBPE tokens ar e
in general longer than byte-level tokens, the approach redu ces the
number of steps required by the decoding process.
In this work, we investigate bilingual (English and Mandari n)
E2E ASR models by exploring different types of output repres enta-
tions, including character-level, BPE, byte-level (UTF-8 ) and BBPE.
Similar to some of the previous work cited, we build a single E 2E
model for utterance-based bilingual speech recognition. O ur con-
tributions are threefold. First, we compare the strengths a nd weak-
nesses of different output representations in monolingual and bilin-
gual use cases. Second, we propose a method to adjust the bigr am
statistics in the BPE algorithm and show that the BBPE repres enta-
tion leads to accuracy improvements in the bilingual scenar io. Fi-
nally, we analyze different representations and show how we might
improve them for multilingual ASR.
2. OUTPUT REPRESENTATIONS FOR E2E ASR
2.1. Character-level Representation
Using a character-level representation in an E2E model mean s that
the output symbol set for the model is the set of graphemes of t he
target language. In addition to graphemes, the output repre senta-
tion may also contain punctuation marks, digits, emojis or s pecial
tokens such as begin-of-sentence (BOS) or end-of-sentence (EOS).
According to [14] [15], character-level representation is often a good
representation for Mandarin E2E models, and this serves as o ne of
the baselines in our experiments.
2.2. BPE Representation
The BPE algorithm [13] starts from the character representa tion and
iteratively merges the most frequent bigrams given a traini ng text
corpus. At the end of this process, the BPE algorithm produce s a
symbol set that consists of subwords with different lengths . This
symbol set can then be used by an E2E model as its output units. It
is common to keep the single characters in the ﬁnal symbol set , so
unseen words in the test set can still be represented by the sy mbol
set. For English, BPE is widely used in E2E ASR systems, as it i m-
proves accuracy and reduces computation due to the use of fre quent
subwords and the resulting shorter labeling sequences.
2.3. Byte-level Representation
Scalability is one of the important aspects in designing an o utput
representation for a multilingual E2E ASR model. As the mode l
supports more languages, the size of the symbol set increase s. To

--- PAGE 2 ---
tackle this problem [8] proposes a byte-level representati on based
on UTF-8. Instead of using characters or subwords as the symb ols,
byte-level model uses UTF-8 codewords as the output symbol s et.
The resulting representation is compact as each UTF-8 codew ord
only has 256 values so each symbol uses one byte. Yet, this rep re-
sentation is capable of representing any language, and addi ng more
languages does not increase the size of the symbol set, which is an
advantage compared to the character-level and BPE represen tation.
However, byte-level representation has two drawbacks, ﬁrs t, it in-
creases the length of the sequence by up to 4x [12], and it incr eases
the number of decoding steps during inference. Second, not a ll byte
sequences are valid UTF-8 sequences, which means the byte-l evel
models may generate invalid byte sequences that require spe cial han-
dling.
To repair an invalid byte sequence, [8] proposes a dynamic pr o-
gramming algorithm to recover the Unicode characters given any
byte sequence. We use this post-processing approach to reco ver
characters from byte sequences as much as possible.
2.4. Byte-level BPE Representation
To circumvent the increase of sequence length for byte-leve l repre-
sentation, [12] proposes byte-level BPE (BBPE) for neural m achine
translation, which applies BPE to byte-represented text. T he ad-
vantage of this approach is that it reduces the sequence leng th by
adopting frequent byte-level subwords and it keeps the size of the
symbol set in check. It is important to note that BBPE is equiv -
alent to BPE for many Latin-based languages, since in UTF-8, all
Latin characters are single byte units. However, for langua ges like
Chinese or Japanese, characters can use multiple bytes, so B BPE
could be helpful. Similar to BPE representation, BBPE repre sen-
tation might generate invalid byte sequences, and post-pro cessing
using dynamic programming is necessary to remedy that. Anot her
aspect is that if we keep all the single-byte UTF-8 codewords in the
symbol set after BPE, BBPE can represent all languages, as wi th the
byte-level representation.
In this paper, we propose two penalty schemes to adjust the bi -
gram statistics used by the BPE algorithm, and we only apply t he
penalty schemes to the Mandarin BBPE symbol extraction. The ﬁrst
one is length penalty. We deﬁne LPbas the length penalized number
of occurrences for bigram b
LPb(α,l,c) =/braceleftBigg
c l ≤N
(1−α)c l > N(1)
whereαis the length penalty factor (0≤α≤1);lis the length of
bigram;cis the bigram count and Nis the cutoff point determining
where to apply this penalty.
The purpose of length penalty is to penalize byte-level subw ords
longer than Nso as to encourage the BPE algorithm to form more
short subwords. For multibyte languages, many of these shor t sub-
words correspond to full characters, thus forming more shor t sub-
words leads to fewer subwords which are fractions of full cha racters.
The second penalty scheme is alphabet penalty, which penali zes
alphabetic bigrams to suppress English subwords occurring in the
Mandarin corpus. The saved space will be distributed to Mand arin
BBPE symbols. We deﬁne APbas the alphabet penalized number of
occurrences for bigram b
APb(β,LPb) =/braceleftBigg
(1−β)LPbif b is alphabetic
LPb otherwise(2)
whereβis the alphabet penalty factor 0≤β≤1.3. EXPERIMENTAL RESULTS
We evaluate our approach through two sets of experiments on p ro-
prietary English and Mandarin dictation tasks. First, we co nduct
the experiments on research purpose English and Mandarin da ta sets
with 1k hours for each language, and hence, the training data of the
bilingual system is 2k hours in total (1k hours for each langu age).
Second, we train the baseline models and BBPE-based models o n
data sets with 5k hours for each language to validate the ﬁndi ngs.
For the E2E models, we follow the procedure mentioned in [16] to
build our listen, attend and spell (LAS) models [17]. For the output
representations involving BPE, we compute the BPE symbol se ts on
the corresponding training transcripts. For evaluation, o ur models
are evaluated on 62 hours of English data and/or 35 hours of Ma n-
darin data.
In this work, we assume the character set of the target langua ge
is known and there is no unseen character during evaluation. As
a result, the symbol set of English to be used in our experimen ts
consists of 765 symbols, including English alphabets, punc tuation
marks, digits, emojis and several hundreds of Unicode chara cters
that appear in our in-house training datasets, such as cent s ign. For
Mandarin, there are 7632 symbols. In the experiments with by te-
level (UTF-8) symbol set, we add 6 additional special tokens , in-
cluding BOS and EOS, resulting in 262 symbols in total.
3.1. Monolingual models
In the English monolingual experiments, M0, M1 and M2 in Ta-
ble 1 correspond to the English LAS 1k hours models using BPE,
character-level and byte-level representation respectiv ely. The sys-
tem using BPE representation achieves 11.6% word error rate
(WER), which outperforms character-level and byte-level r epresen-
tation by 15% relative. It is important to note that characte r-level
and byte-level representations are similar for English exc ept that the
character-level representation contains special tokens l ike emojis.
For Mandarin monolingual results, since Chinese character s
use multiple bytes in UTF-8, there is a big accuracy gap be-
tween character-level (M3) and byte-level (M4) representa tions. The
character-level system has a character error rate (CER) of 1 4.6%,
where the byte-level representation is worse by 3.2% absolu te or
21.9% relative. Applying BPE on the byte-level, i.e. BBPE (M 5),
recovers some of the degradation but the gap to the character -level
system is still 1.1% absolute. We observe that the differenc e in ac-
curacy between the BBPE system and the character-level syst em is
mostly due to deletion errors. These deletion errors are cau sed by in-
valid UTF-8 codeword sequences generated by the model. Alth ough
the dynamic programming algorithm only produces valid sequ ences,
it cannot recover the correct bytes in many cases.
By using the length penalty discussed in Section 2.4 (M7), we
can shrink the gap to 0.2% absolute with a length penalty fact or of
0.99. For the length penalty, we choose the cutoff N= 3as most of
the Chinese characters consist of three bytes. The penalty s cheme
discourages the BPE algorithm from generating multi-chara cter
symbols, encourages generating more single-character sym bols, and
reduces the chance of producing invalid byte sequences. As f or the
length penalty factor α, we tried values ranging from 0.6 to 0.99, as
shown in Table 1, M7 achieves the best CER.
By further applying the alphabet penalty (M8), we ﬁnd that th e
BBPE system can be as good as the character-level system even with
smaller number of outputs and fewer parameters, as the alpha bet
penalty forces the BPE algorithm to suppress multibyte Engl ish sym-
bols. In this work, we choose the alphabet penalty factor β= 0.999

--- PAGE 3 ---
Table 1 . WER/CERs of the mono-/multilingual E2E Model using differ ent types of output representations, 1k hrs experiments
Model Exp. Output Rep. Output Dim. Params En Zh
Mono. EnM0 BPE 6917 75M 11.6 -
M1 character 765 50M 13.8 -
M2 UTF-8 262 48M 13.6 -
Mono. ZhM3 character 7632 78M - 14.6
M4 UTF-8 262 48M - 17.8
M5 BBPE 3658 62M - 15.7
M6 BBPE + LP(0.6) 3662 62M - 15.1
M7 BBPE + LP(0.99) 3661 62M - 14.8
M8 BBPE + LP(0.99) + AP(0.999) 3655 62M - 14.5
Bi. En + ZhB0 BPE(En) + character(Zh) 14414 105M 12.1 14.6
B1 character 8115 80M 13.5 14.9
B2 UTF-8 262 48M 14.0 18.1
B3 BBPE 7028 75M 11.5 15.3
B4 BBPE + LP(0.99) + AP(0.999) 7140 76M 11.5 14.3
Table 2 . WER/CERs of the best BBPE-based E2E Models and baselines, 5 k hrs experiments
Model Exp. Output Rep. Output Dim. Params En Zh
Mono. En M9 BPE 7091 75M 6.4 -
Mono. ZhM10 character 7632 78M - 9.3
M11 BBPE 3676 62M - 9.9
M12 BBPE + LP(0.99) + AP(0.999) 3674 62M - 9.4
Bi. En + ZhB5 BPE(En) + character(Zh) 14577 105M 7.2 10.2
B6 BBPE 7057 75M 7.2 10.6
B7 BBPE + LP(0.99) + AP(0.999) 7170 76M 7.0 9.9
(M8), with which there are only two multibyte English symbol s in
the symbol set, while in M7, 10% of the symbols are multibyte E n-
glish symbols. When scaled to the 5k hours training set, the c on-
clusion remains the same: the BBPE representation with leng th and
alphabet penalty can recover the degradation (M12 in Table 2 ).
3.2. Bilingual models
The last ﬁve rows of Table 1 are the results of our 1k hours bili n-
gual systems (B0 to B4). B0 is our baseline bilingual system w here
it combines the BPE symbol set from the monolingual English s ys-
tem (M0), and the character-level symbol set from the monoli ngual
Mandarin system (M3). Compared to the corresponding monoli n-
gual systems, we observe 0.5% absolute degradation on Engli sh and
similar accuracy on Mandarin. As expected, B0 has better acc uracy
on English than the character-level bilingual system (B1). The byte-
level system (B2) shows signiﬁcant degradation, 1.9% absol ute on
English and 3.5% absolute on Mandarin, as seen in the monolin gual
experiments.
Using BBPE representation recovers most of the degradation ob-
served in B2. In fact, the BBPE system (B3) is better than the b ase-
line on the English test set by 0.6% absolute, while there is s till a
0.7% gap on the Mandarin test set. Finally, by applying the le ngth
and alphabet penalty, our best BBPE system (B4) outperforms the
baseline (B0) by 0.6% absolute on English and 0.3% absolute o n
Mandarin. Besides, compared to the monolingual baselines ( M0 and
M3), B4 also shows slight improvement by 0.1% absolute and 0. 3%
absolute, respectively.
When scaled to the 5k hours training set, the results remain c on-
sistent. While the BBPE system (B6) shows some degradation i naccuracy when compared to the baseline (B5), using length an d al-
phabet penalty recovers the loss and the best BBPE system (B7 )
shows slight improvement, 3% relative on both English and Ma n-
darin test sets. The only difference compared to the 1k hours experi-
ments is that the 5k hours bilingual systems show small degra dation
when compared to their respective monolingual systems. One possi-
ble explanation is that as the amount of data increases, the b ilingual
systems might require a larger model.
4. ANALYSIS
4.1. Invalid byte sequences and the effect of penalty mechan isms
We notice that the BBPE representation may increase the dele tion
rate when the model generates invalid byte sequences. In Tab le 3,
when we compare the BBPE system (B6) with the baseline (B5), w e
can see a 19% increase in deletions. However, the penalty mec ha-
nisms fully recover the increase in deletion errors (B7). Co mparing
the symbol sets of the BBPE-based bilingual experiments B6 ( w/o
penalties) and B7 (w/ penalties), we ﬁnd that in B6 only 24% of the
symbols represent complete Mandarin characters and 20% of t hem
represent Mandarin multi-character sequences. In B7, on th e other
hand, 42% of the symbols represent complete Mandarin charac ters
and less than 2% represent Mandarin multi-character sequen ces.
4.2. Symbol sharing across languages
One motivation for using BBPE symbols is to allow more symbol s
to be shared in the multilingual scenario. Symbol sharing ra te of a
bilingual model is measured based on symbol sets, it is deﬁne d as

--- PAGE 4 ---
Table 3 . Penalty mechanisms mitigate the increase of deletion erro rs
on the Mandarin test set
Exp. penalties #del #sub #ins
B5 - 2273 19176 804
B6 No 2700 (+19%) 19609 (+2%) 774 (-4%)
B7 Yes 2164 (-5%) 18862 (-2%) 711 (-12%)
the ratio of symbols existing in both monolingual symbol set s to the
number of symbols in the combined bilingual symbol set, whic h is
simply a combination of the two monolingual symbol sets. As s hown
in Table 4, in the baseline bilingual experiment B5, only 1% o f the
symbols are shared. In our BBPE bilingual experiment B7, 2.6 %
of the symbols are shared between English and Mandarin. High er
length penalty factor alone may lead to higher sharing rate, since
multi-character Chinese symbols are suppressed and thus th ere are
more multibyte English symbols in the Mandarin symbol set. B ut
the sharing rate in B7 is not as high since higher alphabet pen alty
factor leads to lower sharing rate. We expect alternative by te-level
representations, as well as the choice of languages, can lea d to higher
sharing rates.
Table 4 . Symbol sharing between English and Mandarin in bilingual
symbol sets
Exp. #total symbols #shared symbols
B5 14577 146 (1.0%)
B7 7170 186 (2.6%)
4.3. Language confusion
In our experiments, no external language information is use d in the
bilingual models, and the bilingual models need to be able to iden-
tify the language in the audio. There is a possibility that th e model
would be confused between the two languages. For example, an En-
glish utterance could be recognized as a Mandarin utterance that has
similar pronunciation. We investigate whether the output r epresen-
tation affects the confusion rate.
Table 5 shows the percentages of utterances that are recogni zed
as the wrong language in the bilingual experiments B5 and B7. We
can see that the language confusion ratios of the two bilingu al mod-
els are close, around 0.2% of the English utterances are reco gnized
as Mandarin while 0.8% of the Mandarin utterances are recogn ized
as English. The higher confusion rate observed in Mandarin t est set
can be explained by the higher occurrence of English words in the
Mandarin training set, however, most of the confusions in Ma ndarin
test set do not come from code-switched utterances but short utter-
ances (1-2 characters).
Generally speaking, it seems the choice of output represent a-
tion does not impact the confusion rate, which might be due to the
low sharing rate of the symbols between the two languages. Fr om
the perspective of the bilingual models, both character-le vel repre-
sentation and byte-level representation provide two mostl y mutually
exclusive sets of symbols, one for each language. This could be the
reason why the confusion rates are similar. To further reduc e the
confusion rate, we may consider various language identiﬁca tion ap-
proaches [1], [2], [18].Table 5 . Percentages of utterances that are recognized as the wrong
language in bilingual models
Exp. En recognized as Zh Zh recognized as En
B5 0.17% 0.80%
B7 0.21% 0.80%
4.4. The average length of hypotheses from BBPE-based bilin -
gual models
We calculate the average length of hypotheses from the bilin gual ex-
periments. The length of a hypothesis is deﬁned as the number of
symbols. We evaluate this metric as hypothesis length corre sponds
to the number of decoding steps required for recognition, wh ich di-
rectly affects computation time. Hence, we would like to mea sure
the length of hypotheses under different output representa tions.
Table 6 shows the average length of hypotheses from ﬁve bilin -
gual models. Byte-represented hypotheses are much longer i n both
test sets, as shown in B2, which might explain why it suffers f rom
signiﬁcant accuracy degradation. On the English test set, t he best-
performing BBPE bilingual model B4, outputs longer hypothe ses
than the bilingual baseline B0, which is expected, since the English
BBPE symbols of B4 are on average shorter than those of B0. On
the Mandarin test set, however, the hypotheses from B4 are sh orter
than B0, which we attribute to the use of multi-character sym bols.
The average length of the hypotheses from B4 on the Mandarin t est
set is greater than that for B3 due to length penalty.
Table 6 . Average length of hypotheses from bilingual models
Exp. Output Rep. Output Dim. En Zh
B0 BPE(En)+ character(Zh) 14414 26.3 9.6
B1 character 8115 61.3 9.7
B2 UTF-8 262 61.3 25.7
B3 BBPE 7028 27.1 7.7
B4 BBPE(w/ penalties) 7140 27.2 8.8
5. CONCLUSIONS
In this paper, we compared different output representation s for bilin-
gual E2E ASR, including character-level, BPE, byte-level a nd BBPE
representations. We found that BBPE representation may cau se
higher deletion rate due to invalid byte sequences. To tackl e that, we
proposed penalty mechanisms and the resulting BBPE-based b ilin-
gual system is shown to outperform the baseline bilingual sy stem us-
ing a mixture of BPE and character-level representation. Ho wever,
our 5k hrs BBPE-based bilingual system still lags behind the mono-
lingual counterparts, we will try to increase the model capa city in an
attempt to close the gap in future work. In our analysis, we no ticed
that the current BBPE representation has low sharing rate be tween
the two languages which may be due to the nature of UTF-8 and in
the future, we would look into alternative byte-level repre sentations,
and we believe it might lead to better bilingual performance .
6. ACKNOWLEDGMENTS
We would like to thank Erik McDermott, Pawel Swietojanski, R uss
Webb and Manhung Siu for their support and useful discussion s.

--- PAGE 5 ---
7. REFERENCES
[1] Anjuli Kannan, Arindrima Datta, Tara Sainath, Eugene We in-
stein, Bhuvana Ramabhadran, Yonghui Wu, Ankur Bapna, and
Zhifeng Chen, “Large-scale multilingual speech recogniti on
with a streaming end-to-end model,” in Proceedings of the IN-
TERSPEECH , 2019.
[2] Surabhi Punjabi, Harish Arsikere, Zeynab Raeesy, Chand er
Chandak, Nikhil Bhave, Ankish Bansal, Markus M¨ uller, Ser-
gio Murillo, Ariya Rastrow, Sri Garimella, et al., “Streami ng
end-to-end bilingual ASR systems with joint language ident i-
ﬁcation,” in arXiv preprint arXiv:2007.03900 , 2020.
[3] Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Ha n-
nun, Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Col-
lobert, “Massively multilingual ASR: 50 languages, 1 model ,
1 billion parameters,” pp. 4751–4755, 2020.
[4] Ke Li, Jinyu Li, Guoli Ye, Rui Zhao, and Yifan Gong, “To-
wards code-switching ASR for end-to-end CTC models,” in
Proceedings of the IEEE International Conference on Acous-
tics, Speech, and Signal Processing , 2019.
[5] Changhao Shan, Chao Weng, Guangsen Wang, Dan Su, Min
Luo, Dong Yu, and Lei Xie, “Investigating end-to-end speech
recognition for mandarin-english code-switching,” in Pro-
ceedings of the IEEE International Conference on Acoustics ,
Speech, and Signal Processing , 2019.
[6] Zimeng Qiu, Yiyuan Li, Xinjian Li, Florian Metze, and
William M. Campbell, “Towards context-aware end-to-end
code-switching speech recognition,” in Proceedings of the IN-
TERSPEECH , 2020.
[7] Surabhi Punjabi, Harish Arsikere, Zeynab Raeesy, Chand er
Chandak, Nikhil Bhave, Markus Mueller, Sergio Murillo,
Ariya Rastrow, Andreas Stolcke, Jasha Droppo, Sri Garimell a,
Roland Maas, Mat Hans, Athanasios Mouchtaris, and Siegfrie d
Kunzmann, “Joint ASR and language identiﬁcation using
RNN-T: An efﬁcient approach to dynamic language switch-
ing,” in Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing , 2021.
[8] Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, and William
Chan, “Bytes are all you need: End-to-end multilingual spee ch
recognition and synthesis with bytes,” in Proceedings of the
IEEE International Conference on Acoustics, Speech, and Si g-
nal Processing , 2019, pp. 5621–5625.
[9] Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Sub -
ramanya, “Multilingual language processing from bytes,” i n
Proceedings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics - Huma n
Language Technologies , 2016, pp. 1296–1306.
[10] Marta Ruiz Costa-Juss` a, Carlos Escolano Peinado, and
Jos´ e Adri´ an Rodr´ ıguez Fonollosa, “Byte-based neural ma chine
translation,” in Proceedings of the First Workshop on Subword
and Character Level Models in NLP , 2017, pp. 154–158.
[11] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou ,
Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel,
“Byt5: Towards a token-free future with pre-trained byte-t o-
byte models,” 2021.
[12] Changhan Wang, Kyunghyun Cho, and Jiatao Gu, “Neural
machine translation with byte-level subwords,” in Proceedings
of the AAAI Conference on Artiﬁcial Intelligence , 2020, pp.
9154–9160.[13] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neu -
ral machine translation of rare words with subword units,” i n
Proceedings of the Annual Meeting of the Association for Com -
putational Linguistics , 2016, pp. 1715–1725.
[14] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu, “A compar-
ison of modeling units in sequence-to-sequence speech reco g-
nition with the transformer on mandarin chinese,” in Proceed-
ings of the International Conference on Neural Information
Processing , 2018, pp. 210–220.
[15] Wei Zou, Dongwei Jiang, Shuaijiang Zhao, Guilin Yang, a nd
Xiangang Li, “Comparable study of modeling units for end-to -
end mandarin speech recognition,” in Proceedings of the Inter-
national Symposium on Chinese Spoken Language Processing ,
2018, pp. 369–373.
[16] R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, “On-
line automatic speech recognition with listen, attend and s pell
model,” vol. 27, pp. 1889–1893, 2020.
[17] William Chan, Navdeep Jaitly, Quoc V . Le, and Oriol Viny als,
“Listen, attend and spell: A neural network for large vocabu -
lary conversational speech recognition,” in Proceedings of the
IEEE International Conference on Acoustics, Speech, and Si g-
nal Processing , 2016, pp. 4960–4964.
[18] Shinji Watanabe, Takaaki Hori, and John R Hershey, “Lan -
guage independent end-to-end architecture for joint langu age
identiﬁcation and speech recognition,” in Proceedings of the
IEEE Workshop on Automatic Speech Recognition and Under-
standing , 2017, pp. 265–271.
