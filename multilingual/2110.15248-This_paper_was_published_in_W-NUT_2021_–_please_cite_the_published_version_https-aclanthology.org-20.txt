# 2110.15248.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2110.15248.pdf
# File size: 401468 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
This paper was published in W-NUT 2021 ‚Äì please cite the published version https://aclanthology.org/2021.wnut-1.54 .
√öFAL at MultiLexNorm 2021: Improving Multilingual Lexical
Normalization by Fine-tuning ByT5
David Samuel andMilan Straka
Charles University,
Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
{samuel,straka}@ufal.mff.cuni.cz
Abstract
We present the winning entry to the Multilin-
gual Lexical Normalization (MultiLexNorm)
shared task at W-NUT 2021 (van der Goot
et al., 2021a), which evaluates lexical-
normalization systems on 12 social media
datasets in 11 languages. We base our so-
lution on a pre-trained byte-level language
model, ByT5 (Xue et al., 2021a), which we
further pre-train on synthetic data and then
Ô¨Åne-tune on authentic normalization data. Our
system achieves the best performance by a
wide margin in intrinsic evaluation, and also
the best performance in extrinsic evaluation
through dependency parsing. The source code
is released at https://github.com/ufal/
multilexnorm2021 and the Ô¨Åne-tuned mod-
els at https://huggingface.co/ufal .
1 Introduction
People produce text in natural language every
minute of every day. However, in many cases, for
example on social media like Twitter, such texts
are not conforming to a formal style. Instead, they
are in colloquial form, which is perfectly under-
standable to other people, but challenging for au-
tomatic natural language processing. To make the
processing of such texts more viable, the task of lex-
ical normalization can be used to replace the input
forms with their canonical (more formal, lexically
normalized) variants.
The aim of the W-NUT 2021: Multilingual Lex-
ical Normalization (MultiLexNorm) shared task
(van der Goot et al., 2021a) is to evaluate partic-
ipant lexical-normalization systems on 12 social
media datasets in 11 languages, including two code-
switching datasets. Both intrinsic and extrinsic
evaluation is performed, where the latter is mea-
sured through dependency parsing performed on
the normalized data.
Recently, large pre-trained models like
BERT (Devlin et al., 2019) or T5 (Raffel et al.,2020) have demonstrated superior performance
in many NLP tasks when trained in a transfer
learning setting. In line with that, we approach the
lexical normalization shared task as a Ô¨Åne-tuning
of a large pre-trained model, speciÔ¨Åcally the
multilingual byte-level generative language model
ByT5 (Xue et al., 2021a).
Our system achieves the best performance in
the shared task, both in intrinsic and extrinsic eval-
uations. In the intrinsic evaluation, our system
reaches 67.3% error reduction compared to leave-
as-is baseline; the second-best system has 53.6%.
The source code is released at https://github.
com/ufal/multilexnorm2021 and the Ô¨Åne-tuned
models are available in HuggingFace Transform-
ers (Wolf et al., 2020) at https://huggingface.
co/ufal .
2 Related Work
Lexical normalization can be considered a simpli-
Ô¨Åed variant of a well-studied problem of grammar
error correction (GEC). Grundkiewicz et al. (2019)
approach GEC as a neural machine translation task
using the Transformer architecture (Vaswani et al.,
2017), which is pre-trained using a vast amount
of synthetic data generated by character-level and
word-level edits. Recently, Rothe et al. (2021)
presented a GEC system based on multilingual
mT5 (Xue et al., 2021b), reaching state-of-the-art
results on several datasets with the gigantic xxl
model size with 13B parameters.
While the mentioned GEC systems are autore-
gressive, lexical normalization can be easily solved
in a non-autoregressive way, because the normal-
izations of different words are relatively indepen-
dent. Besides, successful non-autoregressive mod-
els have been recently proposed for general GEC
(Awasthi et al., 2019; Omelianchuk et al., 2020).
Although Ô¨Åne-tuned language models have been
successfully used in the state-of-the-art GEC sys-
tems, this has not been the case in the Ô¨Åeld of lexi-arXiv:2110.15248v2  [cs.CL]  17 Nov 2021

--- PAGE 2 ---
social
ppl
r
gr8Social
people
are
great<X>¬†s o c i a l <Y> ‚ê£ p p l ‚ê£ r ‚ê£ g r 8
s o c i a l ‚ê£ <X> p p l <Y>¬† ‚ê£ r ‚ê£ g r 8
s o c i a l  ‚ê£ p p l ‚ê£ <X> r <Y> ‚ê£ g r 8
s o c i a l ‚ê£ p p l ‚ê£ r ‚ê£ <X> g r 8 <Y>S o c i a l
p e o p l e
a r e
g r e a tData Input s OutputsFigure 1: The inputs and outputs of our lexical normalization model. The ByT5 sentinel tokens <X> and<Y>
mark the word to be normalized.
cal normalization (Muller et al., 2019; Lourentzou
et al., 2019). MoNoise (van der Goot, 2019) is
a publicly available multilingual lexical normal-
ization tool achieving competent performance (an
improved version of the system would place third
in the shared task intrinsic evaluation). It utilizes
Aspell dictionaries, FastText (Bojanowski et al.,
2017) embeddings and hand-crafted language fea-
tures, but no contextualized embeddings.
3 Model
Our model is based on a large pre-trained multilin-
gual model. In accordance with Bommasani et al.
(2021), we call such models (like BERT or T5) the
foundation models.
SpeciÔ¨Åcally, we utilize the ByT5 (Xue et al.,
2021a) foundation model. It is a byte-level gen-
erative sequence-to-sequence model, which pro-
cesses a sequence of bytes of UTF-8 encoding on
input and produces a sequence of UTF-8 encoding
bytes on output. ByT5 models were proposed as
an alternative to multilingual mT5 foundation mod-
els (Xue et al., 2021b) and have been shown to per-
form remarkably well on noisy text data (compared
to subword-based mT5), like TWEETQA (Xiong
et al., 2019).
3.1 Input and Output Format
We start by recapitulating the pre-training task of
the ByT5 model. The input sentence (including
spaces) is represented as a sequence of bytes of
UTF-8 encoding, and spans of around 20 bytes are
Social  people  are great
<X> ‚ê£ p e o p l e  ‚ê£ a <Y> e a t
<X> S o c i a l <Y> r e ‚ê£ g r <Z>Original:
Input:
Output:Figure 2: The pre-training task utilized in the ByT5
model (Xue et al., 2021a). The <X>,<Y> and<Z> are
the sentinel tokens.
masked using special sentinel tokens . The goal of
the model is to reconstruct all the masked spans.
We illustrate the task visually in Figure 2.
For lexical normalization, we could directly use
the unnormalized sentence as input and the nor-
malized sentence as output (an approach used by
Rothe et al. (2021) for GEC). However, we were
concerned that such an approach would be too dif-
ferent from the ByT5 pre-training, and furthermore,
it would not allow to reconstruct the alignment of
the normalized tokens when a word is removed
during normalization or split into several words.
Instead, we opted for a different approach illus-
trated in Figure 1. For each input word, we con-
struct a separate ByT5 input, in which we mark the
beginning and the end of the word in question us-
ing two sentinel tokens. Then we ask the model to
produce just the normalization of that word. Such
an approach normalizes each input word indepen-

--- PAGE 3 ---
self-supervised
pre-training
synthetic
pre-training
fine-tuningUnlabeled multilingual mC4
Wikipedia with synthetic noise
Hand-annotated training dataFigure 3: Three stages of training of our models: the
original ByT5 pre-training, pre-training on synthetic
data and Ô¨Ånal Ô¨Åne-tuning of authentic data.
dently, and we consider it quite similar to the orig-
inal ByT5 pre-training task. Unfortunately, it re-
quires to encode different input sequences for every
input word, which is considerably inefÔ¨Åcient, even
if we can perform normalization of all words in
parallel.
3.2 Pre-training on Synthetic Data
Fine-tuning a ByT5 model directly with supervised
training data would not deliver very high perfor-
mance, given that the normalization task is (despite
our efforts) still quite different from the pre-training
task, and that the amount of available training data
is quite low.1
Therefore, before Ô¨Åne-tuning, we Ô¨Årst pre-train
the ByT5 model using synthetic data, as illustrated
in Figure 3. Note that from now on, by ‚Äúpre-train‚Äù
we mean the training of the ByT5 foundation
model using synthetic lexical normalization data.
We construct the synthetic data by modifying
clean data in the following way:
If a word is present in the normalized output
in the training data, it is replaced by one of the
corresponding (possibly unnormalized) train-
ing data inputs, proportionally to the number
of occurrences. For example, we change ‚Äúpeo-
ple‚Äù to‚Äúppl‚Äù with 39.7% probability in the
English dataset or ‚Äúikke‚Äù to‚Äúik‚Äù with 4.97%
chance in the Danish dataset.
A large portion of the required changes can
be realized as simple character-level modi-
Ô¨Åcations. Thus, we synthesize the data by
reversing these alterations. These are 1) ac-
cent mark removal (e.g., replacing ‚Äú≈°‚Äùby‚Äús‚Äù
with 16.3% chance in Slovenian), 2) changing
1This experiment is included in ablations in Section 5.3.capitalization (e.g., lowering the Ô¨Årst capital-
ized letter with 1.0% chance in Turkish), 3)
removing apostrophes (46.8% probability in
English), and 4) other miscellaneous modiÔ¨Åca-
tions (e.g., simplifying ‚Äúqu‚Äù to‚Äúk‚Äùin Spanish
with 2.0% probability).
The training data naturally contain a large
quantity of different typographical errors, usu-
ally caused by inaccurate typing on a key-
board. To simulate this behavior, we alter the
words by skipping some characters, changing
or inserting some characters (more likely to
those close on a computer keyboard), and by
reversing two consecutive characters. To give
one example, the probability of making an
artiÔ¨Åcial typo in Italian is 0.458%.
Some modiÔ¨Åcations are unique for a speciÔ¨Åc
language. For example, the plural forms in In-
donesian can be created by duplicating the sin-
gular forms. As users want to save time when
writing in Indonesian, they sometimes indi-
cate the plural by simple appending ‚Äú2‚Äù to the
singular form. We reverse this transformation
with 33.3% probability (thus, ‚Äúlaki-lakinya‚Äù
becomes ‚Äúlaki2nya‚Äù ).
7 datasets in this shared task split or merge
words. We synthetically split/merge words
on the clean dataset to model these actions.
For example, the probability of merging two
words in Dutch is 5.99% and of splitting a
word is 0.0565%.
In order to speed up typing, the users modify
their language in various ways ‚Äì e.g. by omit-
ting vowels or shortening words to preÔ¨Åxes.
On the other hand, they repeat certain charac-
ters to make their messages more expressive.
We include these variations in the synthetic
datasets to get closer to the real data.
For more details, please consult the source code.
It is important to note that all the probabilities
were estimated from the training data. Therefore
the synthetic pre-training cannot be regarded as
completely unsupervised; one would need expert
knowledge to extend this type of pre-training on a
language without annotated data of lexical normal-
ization.
We need to make sure that most of the origi-
nal data (before the synthetic modiÔ¨Åcations) are
clean. Since we also want an ample amount of
cross-lingual text, we opted to use publicly avail-

--- PAGE 4 ---
Code Language Original Source WordsWords
split/mergedCaps% words
normedMFR
ERR
DA Danish Plank et al. (2020) 11 816 3 3 9.25 49.68
DE German Sidarenka et al. (2013) 25 157 3 3 17.96 32.09
EN English Baldwin et al. (2015) 73 806 3 7 6.90 64.93
ES Spanish Alegria et al. (2013) 13 827 7 7 7.69 25.57
HR Croatian Ljube≈°i ¬¥c et al. (2017a) 75 276 7 7 8.89 36.52
ID-EN Indonesian-English Barik et al. (2019) 23 124 3 7 12.16 61.17
IT Italian van der Goot et al. (2020) 14 641 3 3 7.32 16.83
NL Dutch Schuur (2020) 23 053 3 3 28.29 37.70
SR Serbian Ljube≈°i ¬¥c et al. (2017b) 91 738 7 7 7.65 42.62
SL Slovenian Erjavec et al. (2017) 75 276 7 7 15.62 56.71
TR Turkish √áolako Àòglu et al. (2019) 7 949 3 3 37.02 14.53
TR-DE Turkish-German van der Goot and √áetino Àòglu (2021) 16 546 3 3 24.14 22.09
Table 1: The MultiLexNorm datasets and their properties ‚Äì number of words, whether words are split/merged, if
capitalization is corrected, relative number of normalized words and performance of the most-frequent-replacement
baseline (Section 5).
able dumps of Wikipedia.2We remove any lines
shorter than 32 characters or ending with a colon
(to remove titles), segment the lines into sentences
with Stanza (Qi et al., 2020) and tokenize with
CMU-ARK tokenizer3(a tool standardly used
for tokenizing text from Twitter).
3.3 Fine-tuning
The Ô¨Ånal Ô¨Åne-tuning of the pre-trained model can
be straightforwardly performed on the training data
(so called base Ô¨Åne-tuning). However, we also
consider Ô¨Åne-tuning on a mix of both authentic
training data and the synthetic data (with 1:1 ratio)
in order to avoid overÔ¨Åtting ( mixed Ô¨Åne-tuning).
3.4 Inference
To predict the normalized sentences during infer-
ence, each word-token is processed independently,
similarly to training (Figure 1). The ByT5 decoder
autoregressively generates each token, either greed-
ily or via beam search.4When using the beam
search, we generate multiple candidate sequences,
and the beam search automatically assigns predic-
tion scores to each of them. These can then be used
to aggregate predictions from multiple models in
an ensemble.
2https://dumps.wikimedia.org/backup-index.html
3https://github.com/myleott/ark-twokenize-py
4For simplicity, we use the default settings of the
greedy_search andbeam_search procedures from the
Hugging Face library, apart from allowing longer sequences
and varying the number of beams.4 Experiments
The MultiLexNorm shared task consists of 12 so-
cial media datasets in 11 languages, including two
code-switching datasets. All these datasets are
based on Twitter data, with the Danish and the
Dutch ones also including data from other sources.
The datasets, their original sources and some of
their properties are listed in Table 1.
We train an independent model for every dataset.
The pre-training starts from the small variant of
ByT5, which contains 300M parameters. We
keep all hyperparameters of the model unchanged,
including a dropout rate of 10%. The training
employs batch size 128 and the AdaFactor opti-
mizer (Shazeer and Stern, 2018) to decrease mem-
ory usage.
The pre-training is performed using the synthetic
data described in Section 3.2, for at least 100 000
optimizer steps.5We utilize inverse square root
decay with peak learning rate of 510 4and 4 000
warm-up steps.
The Ô¨Åne-tuning phase is carried out with a con-
stant learning rate 110 4for 50 epochs. We
consider two training data conÔ¨Ågurations:
1.Using only MultiLexNorm training data, re-
specting the train/dev split (using 10% of train-
ing data as development set if there is none).
2.Because the development set, if present, is
quite large (usually circa 30% of the training
data), we also consider training on develop-
ment data. SpeciÔ¨Åcally, we concatenate train-
5We stop the training only after a full epoch, when at least
100k steps were performed.

--- PAGE 5 ---
ing and development data, if any, and take
only 3% of the data as development set (just
to detect errors, because evaluation on such
small set is considerably noisy).
Our competition model is trained on the combined
dataset (the latter option), and the synthetic data
is mixed in ( mixed Ô¨Åne-tuning from Section 3.3)
if it improves development performance over the
base Ô¨Åne-tuning.6
5 Results
The MultiLexNorm participants were allowed to
submit two runs. The Ô¨Årst run of our team √öFAL is
a single model, while the second run is an ensemble
of 4 models.7We perform ensembling by consider-
ing for each word and each model 16 replacements
including their probabilities (using a beam-search
decoder), and producing the replacement with the
highest average probability.
Apart from the participant systems, several base-
lines are also evaluated: LAI (leave-as-is), MFR
(most frequent replacement based on the training
data) and the MoNoise tool (van der Goot, 2019).
5.1 Intrinsic Evaluation
The intrinsic evaluation is performed using the Er-
ror Reduction Rate (ERR), which is word-level ac-
curacy normalized to the number of replacements
in the dataset. Formally, if we denote the system
that does not normalize any word as leave-as-is ,
we can deÔ¨Åne ERR as
ERR =accuracysystem  accuracyleave -as-is
1:0 accuracyleave -as-is:
The Ô¨Ånal ranking is determined by the ERR macro-
averaged over all datasets.
The MultiLexNorm intrinsic evaluation results
are provided in Table 2. Our system achieves the
best performance by a wide margin ‚Äì the single
model achieves 66.2% macro-averaged ERR, and
the model ensemble even a percent point more,
67.3%. That is 13.7% higher than the second-best
result of 53.6%. Our ensemble model reaches best
results on all datasets, except for Danish, where
our single model is better, and for Spanish, where
it is outperformed by other system.
6Ablations of all training data conÔ¨Ågurations are evaluated
in Section 5.3.
7The ensemble models start from a single pre-trained
checkpoint and only the Ô¨Åne-tuning is independent.5.2 Extrinsic Evaluation
To evaluate the effect of lexical normalization
on downstream applications, MultiLexNorm con-
siders dependency parsing. First, dependency
parsing models are trained using the MaChAmp
parser (van der Goot et al., 2021b) on several tree-
banks from Universal Dependencies 2.8 (Zeman
et al., 2021). Treebanks with formal style are used
(i.e., not data from social networks), speciÔ¨Åcally
German-GSD, English-EWT, Italian-ISDT and
Turkish-IMST. Then, MultiLexNorm participant
systems are used to normalize 7 social-media tree-
banks, which are then parsed using the described
parsing models and evaluated using the label at-
tachment score (LAS) metric. For details, please
see the MultiLexNorm shared task overview paper.
The results of the MultiLexNorm extrinsic eval-
uation are presented in Table 3. Our system also
achieves the best performance in the overall macro-
average and in 4 out of the 7 treebanks. Generally,
the LAS score differences are much smaller than
the ERR intrinsic evaluation metric, but the rank-
ings in both evaluations show a lot of similarity. A
notable difference is the MFR baseline, which per-
forms remarkably well in the extrinsic evaluation.
5.3 Ablation Study
To quantify the effect of various hyperparameters
of our system, Table 4 presents intrinsic evaluation
of several ablation experiments.
The Foundation Model We compare the small
variants of mT5 and ByT5 foundation models when
only Ô¨Åne-tuning (and not pre-training on synthetic
data) is used. In this setting, the ByT5 model
reaches substantially better results (59.2% average
ERR compared to 33.6%). We therefore did not
experiment further with the mT5 model.8
Pre-training and Fine-tuning Phases The pre-
training phase considerably improves results, reach-
ing 64.8% ERR compared to 59.2% ERR without
pre-training. We also evaluate the model after the
pre-training phase only ‚Äì the resulting 31.3% ERR
is quite low, worse than the MFR baseline and most
submitted systems.
Fine-tuning Data First, we consider the effect of
Ô¨Åne-tuning purely on the MultiLexNorm training
data ( base Ô¨Åne-tuning in Table 4), mixing in the
8Preliminary experiments on synthetic pre-training of En-
glish and Italian also demonstrated considerably worse results
of mT5 compared to ByT5.

--- PAGE 6 ---
Team Average DA DE EN ES HR ID-EN IT NL SL SR TR TR-DE
√öFAL (ensemble) 67.30 68.7 66.2 75.6 59.2 67.7 67.2 47.5 63.6 80.1 74.6 68.6 68.6
√öFAL (single) 66.21 70.2 65.7 73.8 55.9 67.3 66.2 42.6 62.7 79.8 73.5 68.6 68.2
HEL-LJU 53.58 56.6 59.8 62.0 35.5 56.2 55.3 35.6 45.9 67.0 66.4 51.2 51.2
MoNoise 49.02 51.3 47.0 74.3 45.5 52.6 59.8 21.8 49.5 61.9 59.6 28.2 36.7
TrinkaAI*43.75 45.9 47.3 66.0 61.3 41.3 56.4 15.8 45.7 59.5 44.5 15.5 25.8
thunderml*43.44 46.5 46.6 64.1 60.3 40.1 59.1 11.9 44.0 59.3 44.5 15.9 29.0
team 40.70 48.1 46.1 63.7 21.0 40.4 59.3 13.9 43.7 60.6 46.1 15.9 29.7
learnML 40.30 40.5 43.7 61.6 56.5 38.1 56.2 5.9 42.8 58.2 40.0 14.4 25.7
maet 40.05 48.1 46.1 63.9 21.0 40.4 59.3 5.9 43.7 60.6 46.1 15.9 29.7
MFR 38.37 49.7 32.1 64.9 25.6 36.5 61.2 16.8 37.7 56.7 42.6 14.5 22.1
CL-MoNoise 12.05 7.3 16.5 4.1 5.0 26.4 2.4 0.0 16.2 8.8 20.1 17.6 20.2
BLUE 6.73 49.7 -1.9 26.8 -9.4 -10.1 -7.2 -31.7 -2.1 -1.0 42.6 10.0 15.0
LAI 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
MaChAmp -21.25 -88.9 -93.4 51.0 25.4 42.6 39.5 -312.9 1.5 56.8 39.4 -12.7 -3.4
Table 2: The results of MultiLexNorm intrinsic evaluation. Each team could submit two systems, we show the
best of the two. More detailed results are available in the overview paper (van der Goot et al., 2021a) and in the
participant papers: HEL-LJU (Scherrer and Ljube≈°i ¬¥c, 2021), TrinkaAI (Kubal and Nagvenkar, 2021), CL-MoNoise
(van der Goot, 2021) and BLUE (Bucur et al., 2021).*denotes late submissions.
Team AverageDE
tweedeEN
aaeEN
monoiseEN
tweebank2IT
postwitaIT
twittiroTR
iwt151
√öFAL (ensemble) 64.17 73.6 62.7 58.6 59.1 68.3 72.2 54.7
√öFAL (single) 63.98 73.6 62.2 57.9 59.0 68.3 72.2 54.8
HEL-LJU 63.72 73.5 60.6 56.2 60.3 68.1 72.3 55.0
MoNoise 63.44 73.2 62.3 56.8 58.9 67.5 70.7 54.6
MFR 63.31 72.9 60.3 56.7 60.3 67.3 70.7 54.9
TrinkaAI*63.12 72.9 60.2 56.6 59.9 67.0 71.1 54.2
maet 63.09 72.8 59.4 56.6 59.8 67.4 71.1 54.5
team 63.03 72.8 59.4 56.6 59.8 67.2 70.9 54.5
thunderml*63.02 72.7 59.6 56.7 59.2 67.3 71.3 54.2
learnML 62.88 72.3 59.0 56.2 60.0 67.0 71.2 54.5
CL-MoNoise 62.71 72.7 60.9 55.3 58.5 66.5 70.1 55.0
BLUE 62.53 72.6 59.6 54.2 59.8 66.7 70.0 54.8
LAI 62.45 72.7 59.2 53.6 60.0 66.5 70.1 55.0
MaChAmp 61.89 71.3 60.8 54.6 58.0 64.7 69.8 54.1
Table 3: The results of MultiLexNorm extrinsic evaluation through dependency parsing evaluated via label attach-
ment score (LAS). Each team could submit two systems, we show the best of the two.*denotes late submissions.
synthetic data with 1:1 ratio ( mixed Ô¨Åne-tuning),
or selecting the best option according to the de-
velopment data ( best Ô¨Åne-tuning). The results
reveal that, unfortunately, our strategy of choosing
the best variant based on development performance
is actually worse than the pure base Ô¨Åne-tuning.
On the other hand, training also on the develop-
ment data improves the performance substantially
(from 64.8% to 66.2% average ERR).
Beam-search Decoding Using beam-search de-
coding with a beam size 16 has virtually no ef-fect compared to greedy decoding (66.21% average
ERR for both options). We hypothesize there is no
difference because we generate each normalization
independently, so greedy decoding on the small
target sequences recovers the optimal solution with
very high probability. In practice, it is therefore
enough to utilize greedy decoding and avoid the
higher runtime requirements of a beam search.
Ensembling Finally, utilizing an ensemble of
4 models improves the performance by a percent
point from 66.2% average ERR to 67.3%.

--- PAGE 7 ---
TreebankFoundation mT5 ByT5 ByT5 ByT5 ByT5 ByT5 ByT5 ByT5 ByT5
Pre-training 7 7 3 3 3 3 3 3 3
Fine-tuning base base 7 base mixed best best best best
Training Data train train train train train train trn+dev trn+dev trn+dev
Beam size 1 1 1 1 1 1 1 16 16
Ensemble 7 7 7 7 7 7 7 7 4
Average 33.62 59.23 31.28 64.88 63.52 64.77 66.21 66.21 67.30
Danish 31.65 67.41 49.37 65.82 67.72 65.82 70.25 70.25 68.67
German 42.91 59.35 49.10 63.40 62.50 63.40 65.77 65.65 66.22
English 61.27 70.40 40.15 73.28 72.68 73.28 73.88 73.80 75.60
Spanish -0.21 43.87 9.15 57.59 56.96 56.96 55.93 55.93 59.25
Croatian 38.11 55.15 43.31 63.18 63.03 63.03 67.29 67.29 67.74
Indonesian-English 50.86 63.75 -3.95 65.12 63.92 63.92 66.15 66.15 67.18
Italian -7.92 43.56 -12.87 46.53 35.64 46.53 42.57 42.57 47.52
Dutch 43.18 55.88 43.45 62.03 62.30 62.03 62.70 62.70 63.58
Slovenian 56.48 71.62 57.21 78.08 77.14 78.08 79.89 79.85 80.07
Serbian 43.29 60.99 59.22 71.10 71.83 71.83 73.42 73.55 74.59
Turkish 11.82 59.80 21.79 66.55 65.03 66.55 68.41 68.58 68.58
Turkish-German 31.99 58.98 19.46 65.82 63.45 65.82 68.27 68.19 68.62
Table 4: The intrinsic evaluation of the ablation experiments ‚Äì we consider various foundation models, whether pre-
training is performed, what data do we use for Ô¨Åne-tuning, the decoder beam size and Ô¨Ånally whether an ensemble
of models is used.
GPUBatch
sizeWords
per secSlowdown
GeForce RTX 3090 1 15.45 5.92 
GeForce RTX 3090 2 28.05 3.26 
GeForce RTX 3090 4 42.15 2.17 
GeForce RTX 3090 8 57.55 1.59 
GeForce RTX 3090 16 84.19 1.09 
GeForce RTX 3090 32 90.20 1.01 
GeForce RTX 3090 64 88.01 1.04 
GeForce RTX 3090 128 91.42 1 
GeForce RTX 3090 256 87.77 1.04 
GeForce RTX 3090 512 78.94 1.16 
GeForce RTX 1080 Ti 128 55.69 1.64 
GeForce RTX 2080 Ti 128 58.19 1.57 
Quadro P5000 128 43.88 2.08 
Quadro RTX 5000 128 66.81 1.37 
Table 5: Inference speed of greedy decoding, measured
on all English evaluation data (56 999 words in total) as
words per second for various GPUs and batch size.
5.4 Inference Speed
To inspect the runtime performance of our model,
we measure the inference speed of a single model
using greedy decoding on all English evaluationdata (56 999 words in total). The results for various
batch sizes and GPUs are listed in Table 5. Overall,
with a batch size of 128 our model processes 43-
91 words per second, depending on a GPU used.
For comparison, the MoNoise system is reported to
normalize 29-62 words/sec without a GPU, based
on candidate Ô¨Åltering (van der Goot, 2019, Table 3).
6 Conclusion
We presented the winning system of the W-NUT
2021: Multilingual Lexical Normalization (Multi-
LexNorm) shared task, which is based on a ByT5
foundation model. In intrinsic evaluation, the sys-
tem performance is superior by a very wide mar-
gin, and our system also delivers the best perfor-
mance in extrinsic evaluation. We release both
the source code at https://github.com/ufal/
multilexnorm2021 and the Ô¨Åne-tuned models at
https://huggingface.co/ufal .
In future work, we would like to change the
model architecture to encode input sentences only
once, either by decoding the whole sentence, or
by separating all input words with different sen-
tinel tokens and then decoding individual words
by initializing the decoder with the corresponding
sentinel tokens.

--- PAGE 8 ---
Acknowledgements
The research described herein has been sup-
ported by the Ministry of Education, Youths and
Sports of the Czech Republic, under the project
LINDAT/CLARIAH-CZ (LM2018101).
References
Inaki Alegria, Nora Aranberri, V√≠ctor Fresno, Pablo
Gamallo, Lluis Padr√≥, Inaki San Vicente, Jordi
Turmo, and Arkaitz Zubiaga. 2013. Introducci√≥n
a la tarea compartida Tweet-Norm 2013: Normal-
izaci√≥n l√©xica de tuits en Espa√±ol. In Tweet-Norm@
SEPLN , pages 1‚Äì9.
Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,
Sabyasachi Ghosh, and Vihari Piratla. 2019. Parallel
iterative edit models for local sequence transduction.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 4260‚Äì
4270, Hong Kong, China. Association for Computa-
tional Linguistics.
Timothy Baldwin, Marie Catherine de Marneffe,
Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu.
2015. Shared tasks of the 2015 workshop on noisy
user-generated text: Twitter lexical normalization
and named entity recognition. In Proceedings of the
Workshop on Noisy User-generated Text , pages 126‚Äì
135, Beijing, China. Association for Computational
Linguistics.
Anab Maulana Barik, Rahmad Mahendra, and Mirna
Adriani. 2019. Normalization of Indonesian-
English code-mixed Twitter data. In Proceedings
of the 5th Workshop on Noisy User-generated Text
(W-NUT 2019) , pages 417‚Äì424, Hong Kong, China.
Association for Computational Linguistics.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics , 5:135‚Äì146.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli,
Russ Altman, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine
Bosselut, Emma Brunskill, Erik Brynjolfsson, Shya-
mal Buch, Dallas Card, Rodrigo Castellon, Niladri
Chatterji, Annie Chen, Kathleen Creel, Jared Quincy
Davis, Dora Demszky, Chris Donahue, Moussa
Doumbouya, Esin Durmus, Stefano Ermon, John
Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea
Finn, Trevor Gale, Lauren Gillespie, Karan Goel,
Noah Goodman, Shelby Grossman, Neel Guha,
Tatsunori Hashimoto, Peter Henderson, John He-
witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing
Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,
Pratyusha Kalluri, Siddharth Karamcheti, Geoff
Keeling, Fereshte Khani, Omar Khattab, Pang WeiKohd, Mark Krass, Ranjay Krishna, Rohith Kudi-
tipudi, Ananya Kumar, Faisal Ladhak, Mina Lee,
Tony Lee, Jure Leskovec, Isabelle Levent, Xi-
ang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik,
Christopher D. Manning, Suvir Mirchandani, Eric
Mitchell, Zanele Munyikwa, Suraj Nair, Avanika
Narayan, Deepak Narayanan, Ben Newman, Allen
Nie, Juan Carlos Niebles, Hamed Nilforoshan, Ju-
lian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-
padimitriou, Joon Sung Park, Chris Piech, Eva Porte-
lance, Christopher Potts, Aditi Raghunathan, Rob
Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,
Camilo Ruiz, Jack Ryan, Christopher R√©, Dorsa
Sadigh, Shiori Sagawa, Keshav Santhanam, Andy
Shih, Krishnan Srinivasan, Alex Tamkin, Rohan
Taori, Armin W. Thomas, Florian Tram√®r, Rose E.
Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai
Wu, Sang Michael Xie, Michihiro Yasunaga, Jiax-
uan You, Matei Zaharia, Michael Zhang, Tianyi
Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng,
Kaitlyn Zhou, and Percy Liang. 2021. On the op-
portunities and risks of foundation models.
Ana-Maria Bucur, Adrian Cosma, and Liviu P. Dinu.
2021. Sequence-to-sequence lexical normalization
with multilingual transformers. In Proceedings of
the 7th Workshop on Noisy User-generated Text (W-
NUT 2021) , Punta Cana, Dominican Republic. As-
sociation for Computational Linguistics.
Talha √áolako Àòglu, Umut Sulubacak, and Ahmet C√ºneyd
Tantu Àòg. 2019. Normalizing non-canonical Turkish
texts using machine translation approaches. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics: Student Re-
search Workshop , pages 267‚Äì272, Florence, Italy.
Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171‚Äì4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Toma≈æ Erjavec, Darja Fi≈°er, Jaka ÀáCibej, ≈†pela
Arhar Holdt, Nikola Ljube≈°i ¬¥c, and Katja Zupan.
2017. CMC training corpus Janes-Tag 2.0. Slove-
nian language resource repository CLARIN.SI.
Roman Grundkiewicz, Marcin Junczys-Dowmunt, and
Kenneth HeaÔ¨Åeld. 2019. Neural grammatical error
correction systems with unsupervised pre-training
on synthetic data. In Proceedings of the Fourteenth
Workshop on Innovative Use of NLP for Building
Educational Applications , pages 252‚Äì263, Florence,
Italy. Association for Computational Linguistics.
Divesh Kubal and Apurva Nagvenkar. 2021. Multi-
lingual sequence labeling approach to solve lexical
normalization. In Proceedings of the 7th Workshop
on Noisy User-generated Text (W-NUT 2021) , Punta

--- PAGE 9 ---
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Nikola Ljube≈°i ¬¥c, Toma≈æ Erjavec, Maja Mili Àácevi¬¥c, and
Tanja Samard≈æi ¬¥c. 2017a. Croatian Twitter training
corpus ReLDI-NormTagNER-hr 2.0. Slovenian lan-
guage resource repository CLARIN.SI.
Nikola Ljube≈°i ¬¥c, Toma≈æ Erjavec, Maja Mili Àácevi¬¥c, and
Tanja Samard≈æi ¬¥c. 2017b. Serbian Twitter training
corpus ReLDI-NormTagNER-sr 2.0. Slovenian lan-
guage resource repository CLARIN.SI.
Ismini Lourentzou, Kabir Manghnani, and ChengXi-
ang Zhai. 2019. Adapting sequence to sequence
models for text normalization in social media. In
International Conference on Web and Social Media .
AAAI.
Benjamin Muller, Benoit Sagot, and Djam√© Seddah.
2019. Enhancing BERT for lexical normaliza-
tion. In Proceedings of the 5th Workshop on Noisy
User-generated Text (W-NUT 2019) , pages 297‚Äì306,
Hong Kong, China. Association for Computational
Linguistics.
Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem
Chernodub, and Oleksandr Skurzhanskyi. 2020.
GECToR ‚Äì grammatical error correction: Tag, not
rewrite. In Proceedings of the Fifteenth Workshop
on Innovative Use of NLP for Building Educational
Applications , pages 163‚Äì170, Seattle, WA, USA ‚Üí
Online. Association for Computational Linguistics.
Barbara Plank, Kristian N√∏rgaard Jensen, and Rob
van der Goot. 2020. DaN+: Danish nested named
entities and lexical normalization. In Proceedings
of the 28th International Conference on Compu-
tational Linguistics , pages 6649‚Äì6662, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,
and Christopher D. Manning. 2020. Stanza: A
Python natural language processing toolkit for many
human languages. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations .
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniÔ¨Åed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1‚Äì67.
Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebas-
tian Krause, and Aliaksei Severyn. 2021. A sim-
ple recipe for multilingual grammatical error cor-
rection. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 2: Short Papers) ,
pages 702‚Äì707, Online. Association for Computa-
tional Linguistics.Yves Scherrer and Nikola Ljube≈°i ¬¥c. 2021. Sesame
Street to Mount Sinai: BERT-constrained character-
level Moses models for multilingual lexical normal-
ization. In Proceedings of the 7th Workshop on
Noisy User-generated Text (W-NUT 2021) , Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Youri Schuur. 2020. Normalization for Dutch for im-
proved pos tagging. Master‚Äôs thesis, University of
Groningen.
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
CoRR , abs/1804.04235.
Uladzimir Sidarenka, Tatjana SchefÔ¨Çer, and Manfred
Stede. 2013. Rule-based normalization of German
Twitter messages. In Proc. of the GSCL Workshop
Verarbeitung und Annotation von Sprachdaten aus
Genres internetbasierter Kommunikation .
Rob van der Goot. 2019. MoNoise: A multi-lingual
and easy-to-use lexical normalization tool. In Pro-
ceedings of the 57th Annual Meeting of the Associa-
tion for Computational Linguistics: System Demon-
strations , pages 201‚Äì206, Florence, Italy. Associa-
tion for Computational Linguistics.
Rob van der Goot. 2021. CL-MoNoise: Cross-lingual
lexical normalization. In Proceedings of the 7th
Workshop on Noisy User-generated Text (W-NUT
2021) , Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.
Rob van der Goot and √ñzlem √áetino Àòglu. 2021. Lexical
normalization for code-switched data and its effect
on POS tagging. In Proceedings of the 16th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Papers .
Association for Computational Linguistics.
Rob van der Goot, Alan Ramponi, Tommaso Caselli,
Michele Cafagna, and Lorenzo De Mattei. 2020.
Norm it! lexical normalization for Italian and its
downstream effects for dependency parsing. In Pro-
ceedings of the 12th Language Resources and Eval-
uation Conference , pages 6272‚Äì6278, Marseille,
France. European Language Resources Association.
Rob van der Goot, Alan Ramponi, Arkaitz Zubiaga,
Barbara Plank, Benjamin Muller, I√±aki San Vi-
cente Roncal, Nikola Ljube≈°i ¬¥c, √ñzlem √áetino Àòglu,
Rahmad Mahendra, Talha √áolako Àòglu, Timothy Bald-
win, Tommaso Caselli, and Wladimir Sidorenko.
2021a. MultiLexNorm: A shared task on multilin-
gual lexical normalization. In Proceedings of the 7th
Workshop on Noisy User-generated Text (W-NUT
2021) , Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.
Rob van der Goot, Ahmet √úst√ºn, Alan Ramponi,
Ibrahim Sharaf, and Barbara Plank. 2021b. Mas-
sive choice, ample tasks (MaChAmp): A toolkit
for multi-task learning in NLP. In Proceedings of

--- PAGE 10 ---
the 16th Conference of the European Chapter of the
Association for Computational Linguistics: System
Demonstrations , pages 176‚Äì197, Online. Associa-
tion for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations , pages 38‚Äì45, Online. Asso-
ciation for Computational Linguistics.
Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-
rni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and
William Yang Wang. 2019. TWEETQA: A social
media focused question answering dataset. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5020‚Äì
5031, Florence, Italy. Association for Computa-
tional Linguistics.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. 2021a. Byt5: Towards a token-
free future with pre-trained byte-to-byte models.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021b. mT5: A massively
multilingual pre-trained text-to-text transformer. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 483‚Äì498, Online. Association for Computa-
tional Linguistics.
Daniel Zeman, Joakim Nivre, et al. 2021. Universal de-
pendencies 2.8. LINDAT/CLARIAH-CZ digital li-
brary at the Institute of Formal and Applied Linguis-
tics (√öFAL), Faculty of Mathematics and Physics,
Charles University.
