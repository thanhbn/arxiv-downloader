Okapi: Các Mô hình Ngôn ngữ Lớn được Điều chỉnh Hướng dẫn bằng Nhiều Ngôn ngữ với Học Tăng cường từ Phản hồi của Con người

Viet Dac Lai1, Chien Van Nguyen1, Nghia Trung Ngo1, Thuat Nguyen1,
Franck Dernoncourt2, Ryan A. Rossi2, Thien Huu Nguyen1
1Khoa Khoa học Máy tính, Đại học Oregon, OR, Hoa Kỳ
2Adobe Research, Hoa Kỳ
{vietl@cs,chienn,nghian@cs,thien@cs}@uoregon.edu
{franck.dernoncourt,ryrossi}@adobe.com

Tóm tắt
Một công nghệ then chốt cho việc phát triển các mô hình ngôn ngữ lớn (LLM) liên quan đến việc điều chỉnh hướng dẫn giúp căn chỉnh các phản hồi của mô hình với kỳ vọng của con người để thực hiện khả năng học tập ấn tượng. Hai phương pháp chính cho việc điều chỉnh hướng dẫn đặc trưng cho việc tinh chỉnh có giám sát (SFT) và học tăng cường từ phản hồi của con người (RLHF), hiện đang được áp dụng để sản xuất các LLM thương mại tốt nhất (ví dụ: ChatGPT). Để cải thiện khả năng tiếp cận của LLM cho các nỗ lực nghiên cứu và phát triển, nhiều LLM mã nguồn mở được điều chỉnh hướng dẫn khác nhau cũng đã được giới thiệu gần đây, ví dụ: Alpaca, Vicuna, v.v. Tuy nhiên, các LLM mã nguồn mở hiện có chỉ được điều chỉnh hướng dẫn cho tiếng Anh và một số ngôn ngữ phổ biến, do đó cản trở tác động và khả năng tiếp cận của chúng đối với nhiều ngôn ngữ khác trên thế giới. Trong số một số công trình gần đây để khám phá việc điều chỉnh hướng dẫn cho LLM bằng nhiều ngôn ngữ, SFT đã được sử dụng như phương pháp duy nhất để điều chỉnh hướng dẫn LLM cho nhiều ngôn ngữ. Điều này đã để lại một khoảng trống đáng kể cho các LLM được tinh chỉnh dựa trên RLHF trong các ngôn ngữ đa dạng và đặt ra những câu hỏi quan trọng về cách RLHF có thể thúc đẩy hiệu suất của việc điều chỉnh hướng dẫn đa ngôn ngữ. Để khắc phục vấn đề này, chúng tôi trình bày Okapi, hệ thống đầu tiên với các LLM được điều chỉnh hướng dẫn dựa trên RLHF cho nhiều ngôn ngữ. Okapi giới thiệu dữ liệu hướng dẫn và xếp hạng phản hồi bằng 26 ngôn ngữ đa dạng để hỗ trợ các thí nghiệm và phát triển nghiên cứu LLM đa ngôn ngữ trong tương lai. Chúng tôi cũng trình bày các bộ dữ liệu chuẩn để cho phép đánh giá các LLM sinh tạo bằng nhiều ngôn ngữ. Các thí nghiệm của chúng tôi chứng minh những ưu điểm của RLHF cho hướng dẫn đa ngôn ngữ so với SFT cho các mô hình cơ sở và bộ dữ liệu khác nhau. Khung và tài nguyên của chúng tôi được phát hành tại https://github.com/nlp-uoregon/Okapi.

1 Giới thiệu
Được đào tạo trước trên dữ liệu khổng lồ, các mô hình ngôn ngữ lớn (LLM) với hàng trăm tỷ tham số có thể mở khóa những khả năng mới nổi mà không thể đạt được với các mô hình nhỏ hơn (Wei et al., 2022). Các mô hình sinh tạo lớn như GPT-3 (Rae et al., 2021) và OPT-175B (Zhang et al., 2022) đại diện cho một số tiến bộ gần đây nhất trong xử lý ngôn ngữ tự nhiên (NLP), giới thiệu một mô hình học tập mới để nhắc LLM giải quyết thành công một loạt các nhiệm vụ thử thách theo cách zero-shot và few-shot (Kung et al., 2022; Choi et al., 2023; Jiao et al., 2023; Guo et al., 2023). Tuy nhiên, vì LLM được đào tạo với mục tiêu học tập tự hồi quy, chúng có thể thể hiện những hành vi không mong muốn từ kỳ vọng của con người (Tamkin et al., 2021; Weidinger et al., 2021; Kenton et al., 2021; Bommasani et al., 2021). Để khắc phục vấn đề này, việc tinh chỉnh hướng dẫn đã được đề xuất như một phương pháp nổi bật để căn chỉnh LLM với ý định của con người trong các hướng dẫn và cuộc trò chuyện (Christiano et al., 2017; Stiennon et al., 2020; Sanh et al., 2021; Wei et al., 2021; Ouyang et al., 2022). Các LLM được điều chỉnh hướng dẫn có thể chứng minh khả năng được cải thiện đáng kể trong việc tuân theo hướng dẫn của con người và tránh sản xuất các văn bản độc hại, thiên vị hoặc không chính xác. Do đó, hai kỹ thuật chính cho việc điều chỉnh hướng dẫn có đặc điểm tinh chỉnh có giám sát (SFT) và học tăng cường từ phản hồi của con người (RLHF) được tận dụng bởi các LLM thương mại tốt nhất như ChatGPT1 và GPT-42 để mang lại hiệu suất đối화 xuất sắc.

Một vấn đề khác với LLM liên quan đến quy mô khổng lồ và bản chất mã nguồn đóng của các LLM thương mại làm hạn chế nghiêm trọng khả năng tiếp cận và mức độ tương tác với công nghệ. Để giải quyết vấn đề này, đã có những nỗ lực ngày càng tăng từ cộng đồng mã nguồn mở để tạo ra những LLM dễ tiếp cận hơn với quy mô phải chăng trong khi đảm bảo hiệu suất cạnh tranh như các LLM độc quyền, ví dụ: LLaMA (Touvron et al., 2023), StableLM (StabilityAI, 2023), Falcon (Almazrouei et al., 2023), và MTP (MosaicML, 2023). Việc tinh chỉnh hướng dẫn cũng đã được áp dụng cho các mô hình ngôn ngữ mã nguồn mở này để cải thiện khả năng tương tác với con người, và các bộ dữ liệu hướng dẫn khác nhau đã được thu thập từ chú thích của con người hoặc đầu ra từ các LLM thương mại để hỗ trợ quá trình điều chỉnh, ví dụ: Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), LaMini-LM (Wu et al., 2023), và Dolly (Conover et al., 2023).

Tuy nhiên, khả năng tuân theo hướng dẫn của các LLM mã nguồn mở hiện có đã được phát triển chủ yếu cho tiếng Anh và một số ngôn ngữ phổ biến (tức là sử dụng bộ dữ liệu hướng dẫn cho những ngôn ngữ đó), không thể hỗ trợ nhiều ngôn ngữ khác trên thế giới để dân chủ hóa công nghệ cho dân số rộng hơn (Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023). Để khắc phục thách thức này, một số công trình đương đại đã khám phá việc điều chỉnh hướng dẫn của LLM đa ngôn ngữ cho nhiều ngôn ngữ, tức là Phoenix (Chen et al., 2023) và Bactrian-X (Li et al., 2023). Tuy nhiên, các nỗ lực điều chỉnh hướng dẫn đa ngôn ngữ của họ chỉ giới hạn ở các kỹ thuật tinh chỉnh có giám sát (SFT), không thể kiểm tra học tăng cường với phản hồi của con người (RLHF) để thúc đẩy thêm hiệu suất cho LLM đa ngôn ngữ.

Để lấp đầy khoảng trống này, công trình của chúng tôi nhằm phát triển Okapi, một khung mã nguồn mở với các LLM được điều chỉnh hướng dẫn dựa trên RLHF cho nhiều ngôn ngữ để làm sáng tỏ hiệu suất của chúng so với các phương pháp SFT trong cài đặt đa ngôn ngữ. Okapi sẽ nhấn mạnh vào các ngôn ngữ ít được nghiên cứu và LLM mã nguồn mở để dân chủ hóa tốt hơn lợi ích của các LLM được điều chỉnh hướng dẫn và cung cấp tài nguyên cho nghiên cứu tương lai trong lĩnh vực này. Cụ thể, một ví dụ trong bộ dữ liệu hướng dẫn bao gồm một hướng dẫn, một văn bản đầu vào và một đầu ra phản hồi mong muốn/minh họa. Trong SFT, các LLM được đào tạo trước được tinh chỉnh trên các bộ ba hướng dẫn (hướng dẫn, đầu vào, đầu ra) thông qua học có giám sát để thúc đẩy sự căn chỉnh của chúng với kỳ vọng của con người. Trong RLHF, các đầu ra được tạo ra từ các LLM được điều chỉnh SFT đầu tiên được xếp hạng để cung cấp tín hiệu đào tạo cho các hàm phần thưởng. Sau đó, các mô hình được điều chỉnh SFT sẽ được tối ưu hóa thêm thông qua học tăng cường sử dụng phần thưởng từ các mô hình phần thưởng được đào tạo. Do đó, RLHF đã được sử dụng thành công để tạo ra các LLM thương mại hiệu quả (ví dụ: InstructGPT, ChatGPT), nhờ vào khả năng học vượt ra ngoài các ví dụ tích cực chỉ liên quan đến các minh họa mong muốn. Bằng cách tận dụng các mô hình phần thưởng, RLHF có thể quan sát điểm xếp hạng thấp hơn cho các minh họa ít chính xác hơn để có được tín hiệu đào tạo phong phú hơn cho LLM. Theo hiểu biết của chúng tôi, Okapi là công trình đầu tiên thực hiện điều chỉnh hướng dẫn với RLHF cho các LLM mã nguồn mở trên nhiều ngôn ngữ.

Để phát triển Okapi, chúng tôi cần khắc phục sự khan hiếm của các bộ dữ liệu hướng dẫn cần thiết bằng nhiều ngôn ngữ để đào tạo và đánh giá các mô hình RLHF. Được thúc đẩy bởi 52K hướng dẫn từ Alpaca (Taori et al., 2023), chúng tôi tận dụng Self-Instruct (Wang et al., 2023) để tạo ra 106K hướng dẫn bổ sung bằng tiếng Anh, giới thiệu một bộ dữ liệu lớn hơn để hỗ trợ đánh giá RLHF. Sau đó, chúng tôi sử dụng ChatGPT để dịch các hướng dẫn sang một tập hợp đa dạng gồm 26 ngôn ngữ, có thể xử lý các ví dụ hướng dẫn với mã lập trình thông qua các lời nhắc phù hợp để nâng cao chất lượng dịch thuật. Ngoài ra, chúng tôi giới thiệu một lời nhắc dựa trên dịch thuật cho ChatGPT để tạo ra các xếp hạng cho nhiều phản hồi của cùng một hướng dẫn từ LLM, sẽ được sử dụng để đào tạo các mô hình phần thưởng cho các thí nghiệm RLHF. Cuối cùng, để đo lường hiệu suất của các LLM được tinh chỉnh bằng các ngôn ngữ khác nhau, chúng tôi dịch ba bộ dữ liệu chuẩn cho LLM trong Bảng xếp hạng LLM Mở HuggingFace được sử dụng rộng rãi (HuggingFace, 2023; Gao et al., 2021) sang 26 ngôn ngữ, tức là ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), và MMLU (Hendrycks et al., 2021), sử dụng ChatGPT. Các bộ dữ liệu này thách thức LLM trên các khía cạnh đa dạng, ví dụ: lý luận khoa học, suy luận thông thường, kiến thức thế giới và giải quyết vấn đề, do đó cung cấp đánh giá toàn diện cho các mô hình của chúng tôi. Tóm lại, đóng góp của chúng tôi trong công trình này như sau:

• Phát triển LLM được điều chỉnh RLHF bằng nhiều ngôn ngữ: Chúng tôi trình bày Okapi, khung LLM được điều chỉnh hướng dẫn đầu tiên, dựa trên RLHF và mã nguồn mở cho nhiều ngôn ngữ. Khung của chúng tôi bao gồm 26 ngôn ngữ đa dạng, bao gồm một số ngôn ngữ ít được nghiên cứu và tài nguyên thấp cho NLP, ví dụ: Telugu, Ukraine, Nepal và Kannada. Sử dụng BLOOM (Scao et al., 2022) và LLaMA (Touvron et al., 2023) làm LLM cơ sở được đào tạo trước, các thí nghiệm của chúng tôi minh họa rằng RLHF thường hoạt động tốt hơn SFT cho việc điều chỉnh hướng dẫn đa ngôn ngữ. Các thí nghiệm của chúng tôi cũng làm nổi bật những thách thức lớn hơn của các ngôn ngữ tài nguyên thấp cho việc điều chỉnh hướng dẫn đa ngôn ngữ của LLM mà nên được tập trung tốt hơn trong nghiên cứu tương lai.

• Tạo tài nguyên cho LLM được điều chỉnh hướng dẫn bằng nhiều ngôn ngữ: Để phục vụ cho các thí nghiệm của chúng tôi với RLHF đa ngôn ngữ, chúng tôi tạo ra tài nguyên hướng dẫn cho 26 ngôn ngữ khác nhau, bao gồm lời nhắc ChatGPT, bộ dữ liệu hướng dẫn, dữ liệu xếp hạng phản hồi, bộ dữ liệu chuẩn và LLM được tinh chỉnh. Chúng tôi phát hành dữ liệu, tài nguyên và mô hình của mình để đóng góp vào việc phát triển và nghiên cứu LLM được điều chỉnh hướng dẫn đa ngôn ngữ trong tương lai. Tài nguyên cho khung Okapi của chúng tôi có thể được tìm thấy tại: https://github.com/nlp-uoregon/Okapi.

2 Chuẩn bị Dữ liệu
Một yêu cầu chính cho việc phát triển LLM được điều chỉnh hướng dẫn của chúng tôi với RLHF liên quan đến bộ dữ liệu hướng dẫn, xếp hạng và đánh giá bằng nhiều ngôn ngữ, đặc biệt là cho các ngôn ngữ tài nguyên thấp. Để đạt được mục tiêu này, chúng tôi thực hiện một quy trình thu thập dữ liệu toàn diện để chuẩn bị dữ liệu cần thiết cho khung đa ngôn ngữ Okapi của chúng tôi bằng 26 ngôn ngữ, được chia thành bốn bước chính: tạo hướng dẫn tiếng Anh, dịch hướng dẫn, sản xuất dữ liệu xếp hạng và tạo dữ liệu đánh giá.

2.1 Tạo Hướng dẫn Tiếng Anh
Một ví dụ hướng dẫn để điều chỉnh LLM thường có ba thành phần: một hướng dẫn để chỉ định nhiệm vụ, một văn bản đầu vào và một văn bản đầu ra liên quan (tức là minh họa hoặc nhãn) (Ouyang et al., 2022). Do đó, các bộ dữ liệu hướng dẫn công khai hiện tại cho LLM chủ yếu bao gồm tiếng Anh hoặc một số ngôn ngữ phổ biến, không phù hợp cho các thí nghiệm của chúng tôi. Ngoài ra, chúng tôi lưu ý rằng một số bộ dữ liệu hướng dẫn gần đây như xP3 (Muennighoff et al., 2022) và Flan (Chung et al., 2022; Longpre et al., 2023) bao gồm dữ liệu đa ngôn ngữ; tuy nhiên, các hướng dẫn của chúng vẫn được viết bằng tiếng Anh. Ngoài ra, các bộ dữ liệu này có xu hướng được chuyển đổi từ các bộ dữ liệu nhiệm vụ NLP với các hướng dẫn mẫu, không thể phản ánh tính linh hoạt của các lời nhắc được viết bởi con người để khuyến khích việc tuân theo hướng dẫn hiệu quả bằng các ngôn ngữ khác nhau (Wang et al., 2023). Do đó, mục tiêu của chúng tôi là phát triển các bộ dữ liệu hướng dẫn với các hướng dẫn, đầu vào và văn bản đầu ra bằng nhiều ngôn ngữ để thực hiện tốt hơn các lời nhắc chung từ con người.

Để đạt được mục tiêu này, chiến lược của chúng tôi là đầu tiên có được các hướng dẫn tiếng Anh và sau đó dịch chúng sang các ngôn ngữ khác. Lợi ích của phương pháp của chúng tôi liên quan đến nội dung hướng dẫn nhất quán trên các ngôn ngữ để hỗ trợ so sánh hiệu suất trong khi tận dụng các hệ thống dịch thuật để cho phép kiểm tra cho nhiều ngôn ngữ hơn. Do đó, đã có một số bộ dữ liệu hướng dẫn tiếng Anh được thu thập bởi cộng đồng mã nguồn mở để hỗ trợ điều chỉnh hướng dẫn của LLM với các phương pháp khác nhau, ví dụ: Alpaca (Taori et al., 2023), Dolly (Conover et al., 2023), và LaMini-LM (Wu et al., 2023). Tuy nhiên, để thuận tiện mở rộng dữ liệu của chúng tôi và giới thiệu các biến thể của hướng dẫn chung, chúng tôi theo phương pháp tạo hướng dẫn trong Alpaca, mà lần lượt sử dụng quy trình Self-Instruct trong (Wang et al., 2023), để sản xuất bộ dữ liệu tiếng Anh của chúng tôi.

Bắt đầu với một nhóm 175 hướng dẫn gốc được viết bởi con người bằng tiếng Anh trên các chủ đề khác nhau, mỗi lần, Alpaca lấy mẫu một số hướng dẫn từ các hạt giống để tạo thành một ví dụ trong ngữ cảnh để nhắc mô hình text-davinci-003 của OpenAI cho việc tạo hướng dẫn mới. Các hướng dẫn được tạo ra sau đó được so sánh với các hướng dẫn trước đó bằng điểm ROUGE, và các hướng dẫn có điểm số lớn hơn ngưỡng sẽ được giữ lại. Nhìn chung, Alpaca phát hành 52K hướng dẫn để điều chỉnh LLM. Trong công trình này, chúng tôi áp dụng cùng quy trình Self-Instruct như Alpaca để mở rộng 52K hướng dẫn của nó thành một bộ dữ liệu lớn hơn cho các mô hình dựa trên RLHF của chúng tôi trong Okapi. Cụ thể, chúng tôi tạo ra 106K hướng dẫn tiếng Anh bổ sung từ Alpaca với hai mở rộng đáng chú ý. Đầu tiên, chúng tôi giới thiệu 30 hướng dẫn mới được tạo bởi con người vào tập hạt giống từ Alpaca để tăng tính đa dạng và bao phủ của nó. Trong số những cái khác, các hướng dẫn mới của chúng tôi bao gồm các lời nhắc để trích xuất quan hệ, trích xuất sự kiện, tóm tắt sự kiện và các câu hỏi logic mà không được nhận dạng trong Alpaca. Thứ hai, thay vì tạo ra các hướng dẫn mới từ đầu, chúng tôi điều kiện hóa quá trình tạo của mình trên 52K hướng dẫn từ Alpaca để một hướng dẫn mới chỉ được lưu nếu nó đủ khác so với Alpaca và các hướng dẫn trước đó theo tiêu chí điểm ROUGE. Hình 1 cho thấy 10 động từ gốc phổ biến nhất và các đối tượng danh từ trực tiếp hàng đầu của chúng trong 106K hướng dẫn được tạo ra. Các động từ và danh từ này đại diện cho 11,4% của toàn bộ tập hợp, thể hiện ý định và mô hình đa dạng trong các hướng dẫn của chúng tôi cho Okapi.

2.2 Dịch Hướng dẫn
Với 158K hướng dẫn tiếng Anh từ Alpaca và quá trình tạo của chúng tôi, chúng tôi nhằm dịch chúng sang nhiều ngôn ngữ khác để có được dữ liệu cho các mô hình đa ngôn ngữ của chúng tôi trong Okapi. Bảng 1 trình bày 26 ngôn ngữ được chọn trong khung của chúng tôi. Sử dụng tỷ lệ dữ liệu r của các ngôn ngữ trong CommonCrawl3 để phân loại ngôn ngữ như trong công trình trước đây (Bang et al., 2023; Lai et al., 2023), nghiên cứu của chúng tôi bao gồm một tập hợp đa dạng các ngôn ngữ, bao gồm 8 ngôn ngữ tài nguyên cao (r >1,0), 11 ngôn ngữ tài nguyên trung bình (r >0,1), và 7 ngôn ngữ tài nguyên thấp (r <0,1). Đáng chú ý, một số ngôn ngữ của chúng tôi, như Marathi, Gujarati và Kannada, đã nhận được sự chú ý hạn chế trong NLP.

Chúng tôi sử dụng ChatGPT để dịch 158K hướng dẫn tiếng Anh sang 26 ngôn ngữ đích cho Okapi. So với các hệ thống dịch máy truyền thống, một ưu điểm của ChatGPT cho dịch thuật là khả năng sử dụng lời nhắc để chỉ định các kỳ vọng khác nhau cho các văn bản được dịch để hỗ trợ các loại hướng dẫn đa dạng. Ví dụ, chúng tôi có thể hướng dẫn ChatGPT bảo tồn mã trong các ví dụ hướng dẫn về lập trình vì chúng tôi mong đợi mã giống nhau trong các hướng dẫn của các ngôn ngữ tự nhiên khác nhau. Ngoài ra, vì ChatGPT đã được tinh chỉnh trên dữ liệu kiểu hướng dẫn, chúng tôi mong đợi rằng nó có thể nắm bắt ngữ cảnh để dịch tốt hơn các hướng dẫn của chúng tôi. Hình 2 cho thấy lời nhắc của chúng tôi để dịch dữ liệu hướng dẫn tiếng Anh với ChatGPT.

Điều quan trọng cần lưu ý là chúng tôi trực tiếp dịch hướng dẫn, văn bản đầu vào và đầu ra liên quan trong mỗi ví dụ hướng dẫn tiếng Anh của dữ liệu chúng tôi. Điều này trái ngược với các phương pháp điều chỉnh hướng dẫn đa ngôn ngữ khác (Li et al., 2023) chỉ dịch hướng dẫn và văn bản đầu vào sang ngôn ngữ đích (sử dụng Google Translate); ChatGPT sau đó được nhắc tạo ra đầu ra phản hồi bằng ngôn ngữ đích cho các hướng dẫn và văn bản đầu vào. Trực giác cho phương pháp của chúng tôi liên quan đến các vấn đề tiềm ẩn khác nhau của ChatGPT, ví dụ: ảo giác, thiên vị, lý luận toán học và nội dung độc hại (Bang et al., 2023; Borji, 2023), có thể bị phóng đại nếu ChatGPT được sử dụng để tạo ra phản hồi bằng các ngôn ngữ không phải tiếng Anh cho các loại nhiệm vụ/hướng dẫn khác nhau (Lai et al., 2023). Bản chất đa dạng của các nhiệm vụ/hướng dẫn có thể cũng sẽ làm cho việc tạo ra các giải pháp phù hợp cho những vấn đề này trong cài đặt đa ngôn ngữ trở nên thách thức hơn. Bằng cách tạo ra các hướng dẫn và đầu ra phản hồi bằng tiếng Anh, chúng tôi nhằm tận dụng hiệu suất lớn hơn của LLM cho các nhiệm vụ NLP khác nhau bằng tiếng Anh để tránh các vấn đề phóng đại và đạt được chất lượng hướng dẫn cao hơn trong các khía cạnh khác nhau. Bằng cách chuyển sang các ngôn ngữ khác chỉ thông qua nhiệm vụ dịch thuật với ChatGPT, chúng tôi cũng có thể dành nỗ lực của mình để khắc phục các thách thức đa ngôn ngữ đa dạng cho việc điều chỉnh hướng dẫn cho nhiệm vụ dịch thuật, có thể cho phép các giải pháp thuận tiện và hiệu quả để cải thiện thêm. Bảng 2 trình bày độ dài trung bình của các lời nhắc được dịch và đầu ra phản hồi cho mỗi ngôn ngữ trong dữ liệu của chúng tôi. Các bản dịch từ hướng dẫn gốc của Alpaca và dữ liệu được tạo mới của chúng tôi được hiển thị riêng biệt để so sánh thuận tiện.

2.3 Sản xuất Dữ liệu Xếp hạng
Để thực hiện RLHF cho một LLM trong Okapi, chúng tôi cần có được các đầu ra phản hồi được xếp hạng từ mô hình cho cùng một hướng dẫn và đầu vào để đào tạo một mô hình phần thưởng. Cụ thể, cho một LLM M và một bộ dữ liệu S={inst k, input k}N k=1 với N cặp hướng dẫn inst k và văn bản đầu vào input k cho một ngôn ngữ đích, chúng tôi đầu tiên nhắc M tạo ra T đầu ra phản hồi output k = {output1 k, . . . , outputT k} cho mỗi cặp hướng dẫn và văn bản đầu vào (inst k, input k) (T >1). Sau đó, các phản hồi trong output k được xếp hạng theo độ phù hợp và chất lượng của chúng cho hướng dẫn inst k và văn bản đầu vào input k. Dữ liệu xếp hạng này {inst k, input k, output k} sau đó có thể được tận dụng để đào tạo một mô hình phần thưởng để tính điểm cho mỗi bộ ba hướng dẫn, văn bản đầu vào và đầu ra phản hồi tiềm năng sử dụng học tương phản (Ouyang et al., 2022).

Trong công trình này, chúng tôi cũng sử dụng ChatGPT để xếp hạng các đầu ra phản hồi cho LLM đa ngôn ngữ. Tương tự như động lực cho phương pháp dựa trên dịch thuật của chúng tôi để có được dữ liệu hướng dẫn bằng nhiều ngôn ngữ, chiến lược xếp hạng của chúng tôi đầu tiên yêu cầu ChatGPT dịch các hướng dẫn và phản hồi {inst k, input k, output k} bằng ngôn ngữ đích sang tiếng Anh. Việc xếp hạng các phản hồi sau đó được thực hiện trên dữ liệu tiếng Anh được dịch để khai thác chất lượng lớn hơn của ChatGPT cho tiếng Anh và giới hạn các thách thức khác nhau liên quan đến xếp hạng đa ngôn ngữ cho nhiệm vụ dịch thuật. Để đạt được mục tiêu này, chúng tôi tham gia với ChatGPT trong một cuộc đối thoại hai lượt để có được xếp hạng cho mỗi ví dụ {inst k, input k, output k} bằng ngôn ngữ đích. Lượt đầu tiên là dịch ví dụ sang tiếng Anh bằng lời nhắc trong Hình 3 trong khi lượt thứ hai tiếp tục với lượt đầu tiên để hướng dẫn ChatGPT xếp hạng các phản hồi tiếng Anh được dịch bằng lời nhắc xếp hạng trong Hình 4. Phương pháp hai lượt của chúng tôi cho phép ChatGPT điều kiện hóa trên dữ liệu tiếng Anh được dịch trong lượt đầu tiên để xếp hạng trong khi đảm bảo cùng một định dạng cho đầu ra xếp hạng trong lượt thứ hai để phân tích thuận tiện. Nhìn chung, chúng tôi có được các đầu ra phản hồi được xếp hạng cho 42K hướng dẫn được lấy mẫu từ 106K hướng dẫn được tạo ra cho mỗi ngôn ngữ trong Okapi.

2.4 Tạo Dữ liệu Đánh giá
Bảng xếp hạng LLM Mở HuggingFace (HuggingFace, 2023) gần đây áp dụng một bộ nhiệm vụ và bộ dữ liệu trong khung Eleuther AI Language Model Evaluation Harness (Gao et al., 2021) để hỗ trợ đánh giá hiệu suất và theo dõi các LLM được phát triển mới. Chúng tôi sử dụng ba bộ dữ liệu trong bảng xếp hạng này tức là AI2 Reasoning Challenge (ARC) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), và MMLU (Hendrycks et al., 2021), để đánh giá hiệu suất mô hình cho khung Okapi của chúng tôi. Tất cả các bộ dữ liệu được tổ chức như các nhiệm vụ trả lời câu hỏi trắc nghiệm mặc dù chúng tập trung vào các loại kiến thức và khía cạnh lý luận khác nhau. ARC bao gồm 1170 câu hỏi khoa học cấp học; HellaSwag cung cấp 9162 câu hỏi suy luận thông thường dễ dàng cho con người, nhưng khó khăn cho nhiều mô hình tiên tiến; và MMLU đánh giá độ chính xác cho 13062 câu hỏi trên các nhánh kiến thức khác nhau (STEM, nhân văn, khoa học xã hội, v.v.). Tuy nhiên, mặc dù cộng đồng LLM đã áp dụng rộng rãi bảng xếp hạng để kiểm tra hiệu suất, các bộ dữ liệu chỉ được cung cấp cho tiếng Anh, do đó không thể đánh giá LLM cho các ngôn ngữ trong công trình của chúng tôi. Để đạt được mục tiêu này, chúng tôi dịch các ví dụ của ba bộ dữ liệu sang 26 ngôn ngữ được chọn bằng ChatGPT và lời nhắc dịch thuật trong Hình 2. Các bộ dữ liệu được dịch sau đó được dành riêng để đánh giá các LLM trong khung Okapi của chúng tôi.

3 Học Tăng cường với Phản hồi của Con người
Chúng tôi tuân theo ba bước để phát triển một LLM được tinh chỉnh với RLHF cho mỗi ngôn ngữ đích trong khung Okapi của chúng tôi: tinh chỉnh có giám sát, đào tạo mô hình phần thưởng và học tăng cường.

Tinh chỉnh Có giám sát (SFT): Bắt đầu với một LLM đa ngôn ngữ được đào tạo trước làm cơ sở, ví dụ: BLOOM (Scao et al., 2022), chúng tôi tinh chỉnh mô hình cơ sở với bộ dữ liệu hướng dẫn của chúng tôi cho ngôn ngữ đích bằng học có giám sát. Trong Okapi, mô hình cơ sở được tinh chỉnh trong ba epoch thông qua mục tiêu tự hồi quy. Quá trình đào tạo của chúng tôi sử dụng lịch trình tốc độ học cosine với 200 bước khởi động, tốc độ học ban đầu 2e-5, kích thước batch 128 và weight decay 0,05. Cuối cùng, thay vì tận dụng các kỹ thuật xấp xỉ để tinh chỉnh hiệu quả, chúng tôi tinh chỉnh toàn bộ LLM cơ sở cho tất cả các tham số của nó với SFT để hiểu chính xác hiệu suất mô hình cho cài đặt đa ngôn ngữ.

Đào tạo Mô hình Phần thưởng: Mục tiêu của bước này là đào tạo một mô hình phần thưởng cho ngôn ngữ đích sẽ tính toán tín hiệu phần thưởng cho các khung học tăng cường để tối ưu hóa thêm mô hình được điều chỉnh SFT từ bước trước. Đối với mỗi cặp lời nhắc và phản hồi tiềm năng, mô hình phần thưởng của chúng tôi trả về một giá trị vô hướng để định lượng tính phù hợp của phản hồi đối với hướng dẫn và văn bản đầu vào trong lời nhắc. Chúng tôi khai thác các bộ dữ liệu được xếp hạng phản hồi trong Phần 2.3 cho bước đào tạo này. Sử dụng thông tin xếp hạng, một ví dụ để đào tạo mô hình phần thưởng của chúng tôi cho một ngôn ngữ bao gồm một hướng dẫn và một văn bản đầu vào (để tạo thành một lời nhắc x) cùng với hai phản hồi được lấy mẫu yc và yr cho x từ các bộ dữ liệu của chúng tôi. Dựa trên thông tin xếp hạng, chúng tôi có thể giả định một trong các phản hồi (tức là yc) được ưa thích hơn phản hồi khác (tức là yr). Ở bước tiếp theo, mất mát xếp hạng nhị phân (Ouyang et al., 2022) được sử dụng để đào tạo mô hình phần thưởng của chúng tôi, nhằm gán điểm số cao hơn r(x, yc) cho phản hồi được ưa thích yc so với điểm số r(x, yr) cho yr: Lreward (θ) = −E(x,yc,yr)[logσ(rθ(x, yc)−rθ(x, yr))]. Đối với quá trình đào tạo, chúng tôi khởi tạo mô hình phần thưởng cho ngôn ngữ đích từ mô hình được điều chỉnh SFT từ bước trước. Chúng tôi đào tạo mô hình phần thưởng của chúng tôi trong 2 epoch với kích thước batch 64 và tốc độ học 1e-5, sử dụng bộ tối ưu hóa AdamW.

Học Tăng cường: Với mô hình phần thưởng được thiết lập cho ngôn ngữ đích, mô hình SFT trải qua tinh chỉnh bổ sung thông qua học tăng cường (RL) để căn chỉnh nó với sở thích của con người. Với mục đích này, chúng tôi sử dụng thuật toán Proximal Policy Optimization (PPO) (Ouyang et al., 2022). Cụ thể, quá trình đào tạo của chúng tôi tối đa hóa phần thưởng trung bình của mô hình thông qua mục tiêu: LRL(ϕ) = −Ex∼DRL,y∼πϕ(y|x)[rθ(x, y)−βKL (x, y)]. Ở đây, DRL tương ứng với phân bố lời nhắc, và πϕ(y|x) biểu thị chính sách hoặc mô hình ngôn ngữ với các tham số ϕ yêu cầu tối ưu hóa. πϕ(y|x) được khởi tạo với mô hình được điều chỉnh SFT πϕ(y|x). Ngoài ra, KL(x, y) = DKL(πϕ(y|x)||π0(y|x)) là độ phân kỳ Kullback–Leibler để phạt độ lệch lớn của πϕ từ chính sách SFT ban đầu π0, và β là hệ số phạt.

Trong giai đoạn đào tạo RL, chúng tôi giữ nguyên toàn bộ LLM và chỉ đào tạo bốn lớp trên cùng trong năm epoch. Chúng tôi sử dụng bộ tối ưu hóa AdamW với β1= 0,9, β2= 0,95, và eps= 1e−8. Hệ số KL β được đặt thành 0,05, trong khi weight decay là 0,1, và tốc độ học là 1e−6. Trong mỗi lần lặp PPO, chúng tôi làm việc với kích thước batch 32 và ngưỡng clip 0,2 trong Okapi.

4 Thí nghiệm
Khung Okapi của chúng tôi sử dụng hai LLM đa ngôn ngữ: BLOOM (Scao et al., 2022) và LLaMA (Touvron et al., 2023) làm mô hình cơ sở cho các quá trình tinh chỉnh. Chúng tôi tập trung vào phiên bản 7B tham số của chúng để hỗ trợ tài nguyên tính toán và đạt được so sánh công bằng hơn. Đối với mỗi mô hình cơ sở và ngôn ngữ đích, chúng tôi thực hiện cả việc điều chỉnh hướng dẫn dựa trên SFT và dựa trên RLHF cho mô hình theo các cách sau:

• SFT: Mô hình cơ sở được tinh chỉnh trên 158K hướng dẫn được dịch (tức là 52K từ Alpaca và 106K từ việc tạo ra của chúng tôi) theo cách có giám sát.

• RLHF: Mô hình cơ sở đầu tiên được tinh chỉnh với đào tạo có giám sát trên 52K hướng dẫn được dịch từ Alpaca. Sau đó, một mô hình phần thưởng được đào tạo để ghi điểm các phản hồi được tạo ra cho các lời nhắc đầu vào bằng học tương phản trên các phản hồi được xếp hạng cho 42K hướng dẫn được dịch trong Phần 2.3. Lưu ý rằng các phản hồi được xếp hạng được lấy mẫu từ mô hình cơ sở được điều chỉnh SFT trên 52K hướng dẫn Alpaca được dịch từ bước trước. Cuối cùng, với mô hình phần thưởng, mô hình cơ sở được điều chỉnh SFT được tối ưu hóa thêm thông qua học tăng cường trên 64K hướng dẫn được dịch còn lại từ tập hợp tạo ra của chúng tôi (Ouyang et al., 2022).

Các bộ dữ liệu được dịch ARC, HellaSwag và MMLU được khai thác để đánh giá hiệu suất của các mô hình trong Okapi. Theo Bảng xếp hạng LLM Mở HuggingFace, khung Eleuther AI Language Model Evaluation Harness (Gao et al., 2021) được sử dụng để tính toán hiệu suất mô hình trên các bộ dữ liệu cho mỗi ngôn ngữ trong khung của chúng tôi. Như một tham chiếu, chúng tôi cũng báo cáo hiệu suất của các mô hình cơ sở BLOOM và LLaMA trong các thí nghiệm. Cuối cùng, đối với BLOOM, chúng tôi thêm so sánh với BLOOMZ (Muennighoff et al., 2022), đây là phiên bản được tinh chỉnh của BLOOM trên bộ dữ liệu hỗn hợp nhiệm vụ đa ngôn ngữ xP3 với hàng triệu hướng dẫn đa ngôn ngữ để đạt được khả năng tuân theo hướng dẫn.

Đánh giá: Bảng 3, 4 và 5 trình bày hiệu suất của các mô hình trên các bộ dữ liệu ARC, HellaSwag và MMLU (tương ứng) khi BLOOM được sử dụng làm mô hình cơ sở. Tương tự, Bảng 6, 7 và 8 báo cáo hiệu suất với mô hình cơ sở LLaMA trên ba bộ dữ liệu. Trong các bảng, ngoài điểm số trung bình trên tất cả các ngôn ngữ cho các mô hình, chúng tôi cũng bao gồm điểm số trung bình cho mỗi nhóm ngôn ngữ (tức là các hàng "Ave Group" cho các ngôn ngữ tài nguyên cao, trung bình và thấp) để hỗ trợ các so sánh. Vì một số ngôn ngữ được chọn của chúng tôi (đặc biệt là những ngôn ngữ tài nguyên thấp) không được hỗ trợ bởi LLaMA, các bảng của chúng tôi cho các thí nghiệm với LLaMA sẽ bỏ qua những ngôn ngữ đó (xem Bảng 1).

Quan sát đầu tiên từ các bảng là RLHF thường tốt hơn SFT cho việc tinh chỉnh đa ngôn ngữ của LLM trên các nhiệm vụ, mô hình cơ sở và nhóm ngôn ngữ khác nhau. Cải thiện hiệu suất trung bình trên tất cả các ngôn ngữ có thể lên đến 2,5% trên bộ dữ liệu HellaSwag với LLaMA, do đó chứng minh những ưu điểm của RLHF so với SFT cho việc tinh chỉnh LLM đa ngôn ngữ. Cũng rõ ràng từ các bảng rằng các mô hình được điều chỉnh RLHF có thể cải thiện đáng kể hiệu suất của các mô hình cơ sở ban đầu (tức là BLOOM và LLaMA) cho hầu như tất cả các nhóm ngôn ngữ và nhiệm vụ, điều này làm nổi bật thêm chất lượng của dữ liệu hướng dẫn được tạo ra và hiệu quả của RLHF.

Ngoài ra, chúng tôi quan sát thấy rằng cải thiện hiệu suất trung bình đạt được thông qua RLHF thì đáng kể hơn đối với các bộ dữ liệu ARC và HellaSwag, trong khi ít rõ ràng hơn đối với bộ dữ liệu MMLU. Dựa trên bản chất của các bộ dữ liệu, chúng tôi cho rằng hiện tượng này là do sự căn chỉnh tốt hơn giữa dữ liệu hướng dẫn của chúng tôi để tinh chỉnh với kiến thức và kỹ năng lý luận cần thiết trong ARC và HellaSwag so với những gì trong MMLU. Cụ thể, ARC và HellaSwag chủ yếu kiểm tra khả năng của các mô hình về kiến thức cơ bản (tức là từ lớp 3 đến lớp 9) và suy luận thông thường trong khi MMLU tập trung vào kiến thức chuyên môn trong các lĩnh vực khác nhau (ví dụ: STEM, khoa học xã hội, nhân văn). Vì các hướng dẫn của chúng tôi được tạo ra với các hạt giống tương tự như phong cách của Alpaca (Taori et al., 2023), chúng có xu hướng nhấn mạnh vào kiến thức chung và kỹ năng suy luận cơ bản, do đó căn chỉnh nhiều hơn với các bộ dữ liệu ARC và HellaSwag. Để đạt được mục tiêu này, các hướng dẫn được tạo ra không thể kích hoạt/bổ sung tốt các kỹ năng ngôn ngữ và kiến thức liên quan đến MMLU từ LLM để đạt được cải thiện có ý nghĩa từ việc điều chỉnh hướng dẫn.

So sánh hiệu suất của các mô hình trên các nhóm ngôn ngữ, chúng tôi thấy rằng các mô hình có xu hướng đạt được hiệu suất cao nhất cho các ngôn ngữ tài nguyên cao, tiếp theo là các ngôn ngữ tài nguyên trung bình và thấp trên các mô hình cơ sở khác nhau. Cải thiện hiệu suất của RLHF cho các ngôn ngữ tài nguyên thấp cũng là ít nhất (dựa trên mô hình cơ sở BLOOM), thúc đẩy nó trở thành một lĩnh vực thách thức cho nghiên cứu tiếp theo. Thú vị là, các mô hình BLOOM được tinh chỉnh của chúng tôi với 158K hướng dẫn được tạo ra có thể vượt trội đáng kể so với BLOOMZ trên hầu như tất cả các ngôn ngữ cho các bộ dữ liệu ARC, HellaSwag và MMLU bằng cách sử dụng SFT hoặc RLHF. Ví dụ, hiệu suất trung bình của RLHF tốt hơn 4,8% so với BLOOMZ trên HellaSwag. Vì BLOOMZ đã tinh chỉnh BLOOM trên hơn 78M hướng dẫn đa ngôn ngữ được chuyển đổi từ các bộ dữ liệu NLP (Muennighoff et al., 2022), nó chứng minh chất lượng cao hơn của các hướng dẫn được tạo ra của chúng tôi cho việc điều chỉnh hướng dẫn đa ngôn ngữ của LLM.

5 Công trình Liên quan
Chúng tôi xem xét hai khía cạnh của công trình liên quan trong nghiên cứu này, tức là điều chỉnh đa ngôn ngữ và đánh giá đa ngôn ngữ.

Điều chỉnh Đa ngôn ngữ: Với việc giới thiệu kiến trúc Transformer (Vaswani et al., 2017), nhiều mô hình ngôn ngữ khác nhau đã được khám phá để thúc đẩy hiệu suất cho các nhiệm vụ NLP, bao gồm các mô hình encoder BERT (Devlin et al., 2019) và RoBERTa (Liu et al., 2019), các mô hình decoder GPT (Radford et al., 2019; Brown et al., 2020), và các mô hình encoder-decoder BART (Lewis et al., 2020) và T5 (Raffel et al., 2020). Các mô hình ngôn ngữ này thường được đào tạo đầu tiên trên dữ liệu tiếng Anh, và sau đó được mở rộng sang các ngôn ngữ khác theo hai phương pháp chính: mô hình đơn ngôn ngữ và đa ngôn ngữ. Trong phương pháp đơn ngôn ngữ, một mô hình ngôn ngữ được đào tạo cụ thể cho một ngôn ngữ cụ thể, ví dụ: cho tiếng Tây Ban Nha (MMG, 2021), tiếng Nhật (Wongso, 2021), tiếng Pháp (Martin et al., 2020; Kamal Eddine et al., 2021), tiếng Ba Lan (Resources and Technology Infrastructure, 2021), tiếng Thụy Điển (Moell, 2021), và tiếng Hindi (Parmar, 2021). Ngược lại, phương pháp đa ngôn ngữ khám phá một mô hình ngôn ngữ duy nhất được đào tạo trên các văn bản đa ngôn ngữ để phục vụ nhiều ngôn ngữ và đạt được việc chuyển giao kiến thức cho các ngôn ngữ tài nguyên thấp hơn, ví dụ: các mô hình chỉ encoder mBERT (Devlin et al., 2019), XLM-RoBERTa (Conneau et al., 2020), các mô hình decoder-only mBART (Liu et al., 2020) và mT5 (Xue et al., 2021), và các mô hình decoder-only BLOOM (Scao et al., 2022) và LLaMA (Touvron et al., 2023).

Dựa trên các mô hình ngôn ngữ được đào tạo trước (PLM), các phương pháp tiên tiến nhất cho NLP trong các ngôn ngữ khác nhau liên quan đến việc tinh chỉnh PLM trên dữ liệu đào tạo của các nhiệm vụ downstream (Min et al., 2023), dẫn đến hiệu suất tiên tiến cho Tách câu đa ngôn ngữ (Nguyen et al., 2021a), Phân tích cú pháp phụ thuộc (Kondratyuk and Straka, 2019), Trả lời câu hỏi (Huang et al., 2019), và Nhận dạng thực thể có tên (Pires et al., 2019) (trong số những cái khác). Ngoài ra, việc tinh chỉnh PLM đa ngôn ngữ (như XLM-RoBERTa) đã được chứng minh là một kỹ thuật hiệu quả để cho phép học chuyển giao đa ngôn ngữ zero-shot trên các ngôn ngữ cho nhiều nhiệm vụ NLP khác nhau. Phương pháp thuận tiện này cho phép mở rộng liền mạch các mô hình NLP để bao gồm các tập hợp ngôn ngữ lớn hơn (Wu and Dredze, 2019; Karthikeyan et al., 2020; Wu et al., 2022; Nguyen et al., 2021b; Guzman-Nateras et al., 2022).

Điều chỉnh hướng dẫn có thể được coi là một loại kỹ thuật tinh chỉnh đặc biệt cho PLM trong đó các PLM sinh tạo (ví dụ: GPT) được đào tạo thêm với dữ liệu hướng dẫn để hoàn thành việc tuân theo hướng dẫn và căn chỉnh phản hồi với kỳ vọng của con người. Tinh chỉnh có giám sát (SFT) là phương pháp điều chỉnh hướng dẫn phổ biến nhất được tận dụng bởi tất cả các LLM hiện có, bao gồm ChatGPT, Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), và LaMini-LM (Wu et al., 2023). Học tăng cường từ phản hồi của con người cũng có thể được sử dụng để cải thiện thêm khả năng tuân theo hướng dẫn của LLM (Wei et al., 2021; Ouyang et al., 2022) mặc dù kỹ thuật này đã được khám phá ít hơn bởi các LLM mã nguồn mở hiện tại do những thách thức trong việc có được dữ liệu xếp hạng cho các mô hình phần thưởng. Đối với học đa ngôn ngữ, điều chỉnh hướng dẫn chỉ được áp dụng dưới dạng SFT cho các ngôn ngữ không phải tiếng Anh bằng cách sử dụng LLM đa ngôn ngữ, ví dụ: BLOOM và LLaMA, trong một số công trình đương đại (Chen et al., 2023; Li et al., 2023; Muennighoff et al., 2022). RLHF chưa được nghiên cứu cho việc điều chỉnh hướng dẫn cho các ngôn ngữ không phải tiếng Anh.

Đánh giá Đa ngôn ngữ: Một rào cản chính cho nghiên cứu trong học đa ngôn ngữ liên quan đến sự khan hiếm của các bộ dữ liệu đánh giá cho các nhiệm vụ NLP trong nhiều ngôn ngữ khác nhau làm cản trở việc phát triển và đo lường mô hình. Do đó, nghiên cứu trước đây đã đầu tư nỗ lực đáng kể để giải quyết thách thức này, giới thiệu các bộ dữ liệu đa ngôn ngữ cho sự đa dạng của các nhiệm vụ NLP. Các nhiệm vụ này bao gồm Universal Dependencies (Nivre et al., 2016), Nhận dạng thực thể có tên với CoNLL 2002 và 2003 (Sang and Meulder, 2002, 2003), Suy luận ngôn ngữ tự nhiên với XNLI (Conneau et al., 2018), Truy xuất thông tin với TyDi (Zhang et al., 2021), Trả lời câu hỏi với XQuAD (Artetxe et al., 2020), Tóm tắt với XWikis (Perez-Beltrachini and Lapata, 2021), Trích xuất sự kiện với MEE (Pouran Ben Veyseh et al., 2022), và nhiều nhiệm vụ khác với XGLUE (Liang et al., 2020) và XTREME (Hu et al., 2020). Tuy nhiên, các bộ dữ liệu đa ngôn ngữ này không được thiết kế cụ thể cho việc đánh giá các LLM sinh tạo như trọng tâm của chúng tôi trong công trình này.

Để đạt được mục tiêu này, Eleuther AI Language Model Evaluation Harness (Gao et al., 2021) cung cấp một khung thống nhất để đánh giá các mô hình ngôn ngữ sinh tạo trên các kỹ năng kiến thức và lý luận khác nhau. Bảng xếp hạng LLM Mở HuggingFace (HuggingFace, 2023) tận dụng bốn điểm chuẩn chính từ khung này, tức là ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), và MMLU (Hendrycks et al., 2021), và TruthfulQA (Lin et al., 2022), đã được áp dụng rộng rãi để đo lường tiến bộ của LLM. Tuy nhiên, các bộ dữ liệu này không thể sử dụng được cho khung đa ngôn ngữ của chúng tôi vì chúng chỉ hỗ trợ đánh giá cho tiếng Anh.

6 Kết luận
Chúng tôi trình bày khung đầu tiên, được gọi là Okapi, về điều chỉnh hướng dẫn cho LLM bằng nhiều ngôn ngữ sử dụng học tăng cường từ phản hồi của con người (RLHF). Để giải quyết sự khan hiếm của dữ liệu cần thiết cho việc điều chỉnh hướng dẫn đa ngôn ngữ, chúng tôi giới thiệu dữ liệu hướng dẫn và xếp hạng phản hồi bằng 26 ngôn ngữ đa dạng để cho phép đào tạo các mô hình tinh chỉnh có giám sát, mô hình phần thưởng và khung học tăng cường cho LLM đa ngôn ngữ. Các thí nghiệm của chúng tôi tiết lộ lợi ích của RLHF cho việc tinh chỉnh đa ngôn ngữ của LLM và các vấn đề thách thức của các ngôn ngữ tài nguyên thấp trong lĩnh vực này cho nghiên cứu tương lai.

Hạn chế
Mặc dù những nỗ lực của chúng tôi để phát triển và đánh giá các LLM được điều chỉnh hướng dẫn bằng nhiều ngôn ngữ sử dụng học tăng cường từ phản hồi của con người, công trình của chúng tôi gặp phải một số hạn chế có thể được cải thiện trong công trình tương lai. Đầu tiên, mặc dù chúng tôi đã cố gắng bao gồm một tập hợp đa dạng gồm 26 ngôn ngữ, vẫn còn nhiều ngôn ngữ khác trên thế giới không được xem xét trong công trình của chúng tôi. Công trình tương lai có thể mở rộng hệ thống của chúng tôi để bao gồm nhiều ngôn ngữ hơn, đặc biệt là cho các ngôn ngữ tài nguyên thấp, để có được hiểu biết toàn diện hơn về khả năng tổng quát hóa ngôn ngữ của các phương pháp điều chỉnh hướng dẫn và dân chủ hóa tốt hơn các công nghệ. Thứ hai, hệ thống của chúng tôi chỉ tận dụng các mô hình cơ sở BLOOM và LLaMA với 7B tham số. Trong khi phương pháp này có thể hỗ trợ cơ sở hạ tầng tính toán của một nhóm lớn hơn các tổ chức cho nghiên cứu tiếp theo, sẽ có lợi khi hỗ trợ các loại mô hình cơ sở đa ngôn ngữ khác, ví dụ: mô hình encoder-decoder mT5 (Xue et al., 2021), và các quy mô mô hình khác (ví dụ: các mô hình 13B và 65B) để tăng cường hệ thống. Thứ ba, để có được dữ liệu hướng dẫn và đánh giá cho việc phát triển, chúng tôi tự động tạo ra các hướng dẫn bằng tiếng Anh và dịch chúng sang nhiều ngôn ngữ bằng ChatGPT. Chúng tôi cũng dựa vào ChatGPT để có được dữ liệu xếp hạng phản hồi cho các mô hình phần thưởng trong RLHF. Mặc dù phương pháp của chúng tôi cho phép mở rộng sang nhiều ngôn ngữ với chi phí phát triển phải chăng, dữ liệu được tạo ra và dịch có thể liên quan đến tiếng ồn không mong muốn. Ngoài ra, chúng có thể không phản ánh hoàn hảo dữ liệu hướng dẫn do con người cung cấp bằng các ngôn ngữ khác nhau. Để đạt được mục tiêu này, công trình tương lai có thể cải thiện hệ thống của chúng tôi với dữ liệu hướng dẫn và đánh giá do con người tạo ra để kiểm tra thêm việc điều chỉnh hướng dẫn cho LLM đa ngôn ngữ. Cuối cùng, các đánh giá của chúng tôi chỉ điều tra hiệu suất của các mô hình trên các bộ dữ liệu chuẩn cho LLM sinh tạo, tập trung vào kiểm tra kiến thức đa dạng, kỹ năng lý luận và tạo ra sự thật. Các mối quan tâm quan trọng khác của các mô hình sinh tạo như ảo giác, độc tính và thiên vị không được đánh giá rõ ràng trong các thí nghiệm của chúng tôi. Công trình tương lai có thể nghiên cứu những vấn đề này để đặc trưng tốt hơn các phương pháp điều chỉnh hướng dẫn trong cài đặt đa ngôn ngữ.

Tài liệu tham khảo
[Phần tài liệu tham khảo được giữ nguyên như bản gốc do có nhiều trích dẫn kỹ thuật]
