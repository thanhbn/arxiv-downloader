# 2305.15183.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2305.15183.pdf
# Kích thước tệp: 586017 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Các Mô hình Ngôn ngữ Được Huấn luyện trước có Hữu ích cho Tập hợp Mô hình trong Sửa lỗi Ngữ pháp Tiếng Trung không?
Chenming Tang Xiuyu Wu Yunfang Wu∗
Phòng thí nghiệm Trọng điểm Quốc gia về Xử lý Thông tin Đa phương tiện, Đại học Bắc Kinh
Phòng thí nghiệm Trọng điểm Bộ Giáo dục về Ngôn ngữ học Tính toán, Đại học Bắc Kinh
Khoa Khoa học Máy tính, Đại học Bắc Kinh
tangchenming@stu.pku.edu.cn
{xiuyu_wu, wuyf}@pku.edu.cn
Tóm tắt
Tập hợp mô hình đã được sử dụng rộng rãi cho
Sửa lỗi Ngữ pháp (GEC), giúp tăng cường
hiệu suất mô hình. Chúng tôi giả định rằng
tập hợp mô hình dựa trên độ phức tạp
(PPL) được tính toán bởi các mô hình ngôn ngữ
được huấn luyện trước (PLM) sẽ có lợi cho
hệ thống GEC. Vì mục đích này, chúng tôi
khám phá một số chiến lược tập hợp dựa trên
các PLM mạnh với bốn mô hình đơn lẻ tinh vi.
Tuy nhiên, hiệu suất không cải thiện mà thậm chí
còn tệ hơn sau khi tập hợp dựa trên PLM. Kết quả
đáng ngạc nhiên này khiến chúng tôi thực hiện
phân tích chi tiết về dữ liệu và đưa ra một số
hiểu biết về GEC. Các tham chiếu của con người
về câu đúng còn rất thiếu trong dữ liệu thử nghiệm,
và khoảng cách giữa một câu đúng và một câu
thành ngữ đáng được chú ý. Hơn nữa, các chiến lược
tập hợp dựa trên PLM cung cấp một cách hiệu quả
để mở rộng và cải thiện dữ liệu chuẩn GEC. Mã nguồn
của chúng tôi có sẵn tại https://github.com/JamyDon/PLM-
based-CGEC-Model-Ensemble.

1 Giới thiệu
Sửa lỗi Ngữ pháp (GEC) là nhiệm vụ tự động phát hiện
và sửa chữa lỗi trong văn bản (Bryant et al., 2022).
Ngày nay, có hai phương pháp GEC chính. Phương pháp
đầu tiên là coi GEC như một nhiệm vụ dịch máy ít tài
nguyên (Yuan và Briscoe, 2016), trong đó các mô hình
chuỗi-tới-chuỗi như BART (Lewis et al., 2020) được sử dụng.
Phương pháp này đơn giản đưa văn bản không chính xác
vào bộ mã hóa và nhận kết quả được sửa chữa từ bộ giải mã.
Phương pháp thứ hai là coi GEC như một nhiệm vụ gắn thẻ
chuỗi, trong đó văn bản không chính xác vẫn được lấy làm
đầu vào, nhưng đầu ra là các thẻ chỉnh sửa (giữ, xóa, thêm,
thay thế, v.v.) cho mỗi token. Sau khi áp dụng tất cả các
chỉnh sửa vào văn bản đầu vào, kết quả được sửa chữa
sau đó được tạo ra. Mô hình được sử dụng trong phương pháp
này cũng được gọi là các mô hình chuỗi-tới-chỉnh sửa
và GECToR (Omelianchuk et al., 2020) là một ví dụ điển hình.

Tuy nhiên, hầu hết các nghiên cứu về GEC tập trung vào
tiếng Anh trong khi GEC tiếng Trung (CGEC) chỉ mới
bắt đầu. Tiếng Trung khác với tiếng Anh về nhiều mặt
và GEC của nó do đó khó hơn nhiều. Thay vì biến đổi
từ trong nhiều ngôn ngữ phương Tây, ngữ pháp tiếng Trung
được thể hiện bằng từ chức năng và thứ tự từ, làm cho
CGEC khó khăn và phức tạp hơn vì chúng ta không thể
lấy hình thức từ làm tay cầm. Ngoài ra, không giống như
tiếng Anh, chúng ta có rất ít bộ dữ liệu để huấn luyện
và thử nghiệm CGEC, điều này khiến chúng ta khám phá
các phương pháp không cần huấn luyện như tập hợp mô hình
để cải thiện thêm hiệu suất của các hệ thống CGEC.

Do bản chất của GEC là các sửa chữa có thể được biểu diễn
như một số chỉnh sửa độc lập, tập hợp mô hình đã trở thành
một cách phổ biến để cải thiện các hệ thống GEC. Trong
CGEC, Li et al. (2018), Liang et al. (2020) và Zhang et al.
(2022) tập hợp các mô hình của họ bằng cách bỏ phiếu đa số
trên các chỉnh sửa và đạt được cải thiện đáng kể. Bên cạnh đó,
Xie et al. (2016) áp dụng các mô hình ngôn ngữ để cải thiện
sửa lỗi ngôn ngữ thần kinh, theo đó Junczys-Dowmunt et al.
(2018) tập hợp các mô hình GEC của họ bằng cách sử dụng
xác suất mô hình ngôn ngữ.

Ngày nay, các Mô hình Ngôn ngữ Được huấn luyện trước
(PLM) dựa trên transformer (Vaswani et al., 2017) đã được
sử dụng chủ yếu trong NLP. Tuy nhiên, chúng tôi thấy ít
công trình về tập hợp mô hình sử dụng PLM trong CGEC.

Trong công việc này, chúng tôi giả định rằng việc chọn
đầu ra tập hợp tốt nhất với sự hỗ trợ của độ phức tạp
(PPL) được tính toán bởi PLM sẽ thúc đẩy hiệu suất
cuối cùng của CGEC. Chúng tôi thử nghiệm trên tập hợp
của bốn mô hình CGEC, bao gồm hai mô hình chuỗi-tới-chuỗi
và hai mô hình chuỗi-tới-chỉnh sửa. Chúng tôi thử bốn
chiến lược tập hợp: bỏ phiếu truyền thống, tập hợp mức câu,
tập hợp mức chỉnh sửa, và tập hợp kết hợp chỉnh sửa,
ba chiến lược cuối khai thác sức mạnh của PLM.

Đáng ngạc nhiên là kết quả của tập hợp mô hình với PLM
không vượt quá những kết quả của bỏ phiếu truyền thống
và thậm chí còn tệ hơn hầu hết các mô hình đơn lẻ.
Để tìm hiểu tại sao PPL thấp không thể dẫn đến hiệu suất
GEC tốt hơn, chúng tôi thực hiện phân tích chi tiết về
kết quả tập hợp và đưa ra một số hiểu biết về GEC:

1) Trong dữ liệu thử nghiệm, các tham chiếu của con người
không đầy đủ, trong khi các chiến lược tập hợp dựa trên PLM
tạo ra các ứng viên có giá trị, sau khi được con người kiểm tra,
có thể được coi là bổ sung cần thiết cho các tham chiếu của con người.

2) Khi đối mặt với một câu sai, một chuyên gia con người
sửa nó với nỗ lực tối thiểu, trong khi các chiến lược tập hợp
dựa trên PLM tạo ra văn bản tự nhiên và thành ngữ hơn,
điều này rất hữu ích cho những người học ngôn ngữ ở nước ngoài.

3) Với khả năng mạnh mẽ, các mô hình dựa trên PLM cố gắng
tạo ra các câu trôi chảy nhưng đôi khi bỏ qua ý nghĩa gốc
của câu nguồn, dẫn đến sửa chữa quá mức cần được giải quyết
trong công việc tương lai.

2 Các Mô hình Cơ bản

2.1 Các Mô hình CGEC Đơn lẻ
Chúng tôi triển khai bốn mô hình đơn lẻ làm đường cơ sở,
với hai mô hình seq2seq và hai mô hình seq2edit. Tất cả
các mô hình sử dụng bộ dữ liệu Lang-8 để huấn luyện.

Các Mô hình Chuỗi tới Chuỗi. Hai mô hình seq2seq đều
dựa trên BART-base-Chinese (Shao et al., 2021), và được
triển khai bằng fairseq (Ott et al., 2019). Bên cạnh Lang-8,
dữ liệu HSK cũng được sử dụng để huấn luyện. Một mô hình
seq2seq áp dụng chiến lược "dropout-src", trong đó mỗi token
trong câu đầu vào được thay thế bằng "[PAD]" với xác suất 10%.
Mô hình kia được huấn luyện trước trên dữ liệu tổng hợp
được xây dựng trên THUCNews (Sun et al., 2016) trước khi
huấn luyện bình thường.

Các Mô hình Chuỗi tới Chỉnh sửa. Chúng tôi áp dụng
GECToR-Chinese (Zhang et al., 2022) làm các mô hình
seq2edit của chúng tôi, với Structbert-large-Chinese
được huấn luyện trước (Wang et al., 2019) làm xương sống.
Hai mô hình seq2edit của chúng tôi chỉ khác nhau ở hạt giống ngẫu nhiên.

2.2 Các Mô hình Ngôn ngữ Được huấn luyện trước
Chúng tôi áp dụng ba PLM để thực hiện tập hợp mô hình.

BERT-base-Chinese. Nó được huấn luyện trước trên hai
nhiệm vụ: Mô hình Ngôn ngữ Che dấu (MLM) và Dự đoán
Câu Tiếp theo (NSP). Trong MLM, mỗi token có cơ hội 15%
được thay thế bằng "[MASK]" (80%), một từ ngẫu nhiên (10%),
hoặc chính nó (10%). Vui lòng tham khảo Devlin et al. (2019)
để biết chi tiết.

MacBERT-base-Chinese. Nó tương tự như BERT, nhưng
sử dụng che dấu toàn từ, che dấu N-gram và thay thế từ
tương tự trong MLM. Bên cạnh đó, Dự đoán Thứ tự Câu (SOP)
được khai thác thay vì NSP. Vui lòng tham khảo Cui et al.
(2020) để biết chi tiết.

GPT2-Chinese. Đây là phiên bản tiếng Trung không chính thức
của GPT-2 (Radford et al., 2019). Nó sử dụng huấn luyện trước
sinh sản, bằng cách dự đoán từ tiếp theo trong một câu chỉ
với các từ trước đó được cung cấp.

3 Chiến lược Tập hợp

Với câu nguồn và đầu ra của bốn mô hình đơn lẻ làm đầu vào,
chúng tôi trình bày bốn chiến lược tập hợp. Sơ đồ của các
chiến lược tập hợp dựa trên PLM của chúng tôi được thể hiện
trong Hình 1.

3.1 Bỏ phiếu Truyền thống
Các mô hình khác nhau bỏ phiếu cho kết quả cuối cùng.
Đối với mỗi câu, chúng tôi coi các thao tác chỉnh sửa được
đề xuất bởi không ít hơn T mô hình là đúng. Trong công việc
của chúng tôi, chúng tôi thử nghiệm T từ 2 đến 4. Chúng tôi
triển khai mã gốc do Zhang et al. (2022) cung cấp để thực hiện
chiến lược bỏ phiếu này.

3.2 Tập hợp Mức Câu
Sử dụng các PLM khác nhau, chúng tôi tính toán các độ phức tạp
(PPL) của câu nguồn và đầu ra của bốn mô hình đơn lẻ.
Cụ thể, cho một câu S = (w1, w2, ..., wn) và xác suất của
từ wi được tính toán bởi một PLM ký hiệu là pi, thì
PPL = (∏ᵢ₌₁ⁿ 1/pi)¹/ⁿ. Câu có PPL thấp nhất được chọn
làm đầu ra cuối cùng.

3.3 Tập hợp Mức Chỉnh sửa
Cho một câu nguồn S, tất cả các chỉnh sửa được đề xuất
bởi các mô hình đơn lẻ tạo thành một tập ứng viên A, và
số lượng khoảng chỉnh sửa được ký hiệu là m. Một khoảng
chỉnh sửa có nghĩa là cặp bắt đầu-kết thúc của vị trí chỉnh sửa
trong câu. Tập hợp tất cả các chỉnh sửa (từ các mô hình
đơn lẻ khác nhau) trên khoảng chỉnh sửa thứ i (bao gồm
"noop") được ký hiệu là Ai. Do đó, chúng ta có thể chia
A = ⋃ᵢ₌₁ᵐ Ai, trong đó Ai = {eⱼⁱ|j = 1,2, ..., |Ai|}, và
eⱼⁱ có nghĩa là chỉnh sửa thứ j trên khoảng chỉnh sửa thứ i.

Đối với mỗi khoảng chỉnh sửa (Ai trong A), chúng tôi tạo ra
|Ai| câu mới, mỗi câu tương ứng với một chỉnh sửa đơn lẻ
trong Ai. Sau đó chúng tôi tham khảo PLM về PPL của những
câu mới này và chấp nhận chỉnh sửa tương ứng với câu có
PPL thấp nhất, mà chúng tôi đánh dấu là e_best^i. Nói cách khác,
e_best^i là chỉnh sửa tốt nhất (được quyết định bởi PLM) trong Ai,
hoặc trên khoảng i.

Với chỉnh sửa tốt nhất của mỗi khoảng, tập chỉnh sửa cuối cùng
E_final kết hợp những chỉnh sửa tốt nhất này, được mô tả như:
E_final = {e_best^i|i ∈ {1,2, ..., m}}, (1)

Câu giả thuyết cuối cùng sau đó được tạo ra dựa trên E_final.

3.4 Tập hợp Kết hợp Chỉnh sửa
Một câu nguồn có thể chứa nhiều hơn một lỗi. Đối với mỗi câu,
chiến lược này áp dụng tất cả các kết hợp chỉnh sửa vào câu
nguồn và tạo ra nhiều câu mới.

Cụ thể, cho một câu nguồn S, các ứng viên chỉnh sửa A vẫn
được chia như A = ⋃ᵢ₌₁ᵐ Ai, và sau đó chúng ta có được tất cả
các kết hợp chỉnh sửa có thể bằng:
U = {{e₁^j₁, e₂^j₂, ..., eₘ^jₘ} |ji ∈ {1,2, ..., |Ai|}}. (2)

Do đó chúng ta tạo ra (∏ᵢ₌₁ᵐ |Ai|) câu mới, mỗi câu tương ứng
với một kết hợp chỉnh sửa trong U. Câu có PPL thấp nhất
sẽ được chấp nhận làm đầu ra cuối cùng.

Xem xét độ phức tạp tính toán, chúng tôi chỉ áp dụng chiến lược
này trên các câu có số lượng kết hợp chỉnh sửa không quá 300.
Những câu đơn giản như vậy chiếm 95,15% của MuCGEC-test
và 98,90% của NLPCC-test. Chúng tôi không làm gì với những
câu còn lại không đơn giản.

4 Thí nghiệm

4.1 Bộ dữ liệu và Thước đo Đánh giá
Chúng tôi thực hiện thí nghiệm trên dữ liệu thử nghiệm
MuCGEC (Zhang et al., 2022) và dữ liệu thử nghiệm NLPCC
(Zhao et al., 2018). MuCGEC chứa 7063 câu và mỗi câu có
tối đa ba tham chiếu, nhưng hiện tại không có sẵn. NLPCC
chứa 2000 câu, mỗi câu có một hoặc hai tham chiếu, và trung
bình khoảng 1,1 tham chiếu. Chúng tôi thực hiện phân tích
trên dữ liệu thử nghiệm NLPCC.

Trên MuCGEC, chúng tôi gửi kết quả của các hệ thống của
chúng tôi đến trang web đánh giá công khai. Trên NLPCC,
chúng tôi triển khai các công cụ do Zhang et al. (2022) cung cấp
để tính toán P (Độ chính xác), R (Độ nhớ lại), và F0.5 của
đầu ra ở mức ký tự. Ngoài ra, chúng tôi báo cáo kết quả
mức từ trên NLPCC-test để tham khảo với các công trình trước đó.

4.2 Kết quả
Bảng 1 cho thấy kết quả thí nghiệm. Chiến lược bỏ phiếu
truyền thống đạt được hiệu suất tốt nhất, với điểm F0.5
44,09 ở mức ký tự cao hơn đáng kể so với mô hình đơn lẻ
tốt nhất. Với ngưỡng T tăng, độ chính xác tăng trong khi
độ nhớ lại giảm. Khi T = 3, điểm F0.5 đạt đỉnh, phù hợp
với phát hiện của Tarnavskyi et al. (2022).

Tuy nhiên, các chiến lược tập hợp dựa trên PLM có hiệu suất
tệ hơn nhiều so với chiến lược bỏ phiếu đơn giản, và thậm chí
thấp hơn hầu hết các mô hình đơn lẻ. Về độ chính xác và
độ nhớ lại, bỏ phiếu truyền thống đạt được độ chính xác cao hơn
nhưng độ nhớ lại thấp hơn so với các mô hình đơn lẻ trong khi
các chiến lược dựa trên PLM thì ngược lại. Trong ba chiến lược
tập hợp, chiến lược mức câu hoạt động tốt nhất.

Trong các PLM khác nhau, GPT2-Chinese đạt được kết quả
tốt nhất trong cả ba chiến lược tập hợp. Điều này có thể là
do các mô hình dựa trên BERT tự nhiên giỏi dự đoán mặt nạ
hơn là tính toán PPL cho toàn bộ câu. Sau đó, chúng tôi dựa
trên GPT2-Chinese để thực hiện phân tích sâu hơn.

5 Phân tích và Thảo luận
Chúng tôi thiết kế ba chiến lược tập hợp để chọn chuỗi có
PPL thấp nhất làm đầu ra cuối cùng, nhưng tại sao điểm F0.5
lại giảm? Trong công việc của chúng tôi, tất cả các mô hình
đơn lẻ đều được tạo thành từ các PLM riêng của chúng,
có nghĩa là việc tập hợp chúng khai thác PLM khác giống như
sử dụng PLM để phán xét PLM, vì vậy hiệu suất có thể ít được
hưởng lợi. Điều này phù hợp với công việc của Junczys-Dowmunt
et al. (2018), trong đó các mô hình đơn lẻ được huấn luyện trước
ít được hưởng lợi và thậm chí có hiệu suất tệ hơn sau khi tập hợp
dựa trên PLM trong khi các mô hình đơn lẻ đơn giản khác được
hưởng lợi rất nhiều. Bên cạnh điều này, có lý do nào khác không?

5.1 Kết quả Thống kê
Để tìm hiểu nguyên nhân của hiệu suất kém của các chiến lược
tập hợp dựa trên PLM, trên dữ liệu thử nghiệm NLPCC, chúng tôi
chọn ngẫu nhiên 200 mẫu từ kết quả của cả ba chiến lược cùng
với mô hình đơn lẻ tốt nhất (seq2seq-1) để so sánh, và yêu cầu
hai sinh viên sau đại học phân tích các câu đầu ra với cách
thức đôi mắt kín. Sau đó, một chuyên gia thứ ba làm trọng tài
cho sự không nhất quán. Hướng dẫn cho người chú thích con người
được hiển thị trong Phụ lục A.

Theo phán đoán của con người, bốn loại được tóm tắt. Chính xác (E):
đầu ra trôi chảy và đúng, phù hợp với tham chiếu. Tốt (G):
đầu ra trôi chảy và đúng nhưng khác với tham chiếu, điều này
cho thấy rằng các tham chiếu không đủ. Sửa chữa quá mức (O):
đầu ra trôi chảy nhưng không đáp ứng ý nghĩa gốc của câu nguồn.
Sai (W): đầu ra có các vấn đề khác mà chúng tôi không quan tâm
trong công việc này.

Kết quả chú thích của con người được báo cáo trong Bảng 2,
và một số ví dụ về G và O được hiển thị trong Bảng 3.

5.2 Thảo luận
Sự thiếu hụt của các tham chiếu GEC. Trong các đầu ra của
các chiến lược tập hợp dựa trên PLM, khoảng 1/4 ("G") được
tự động đánh giá là sai theo các tham chiếu vàng, nhưng thực sự
đúng sau khi kiểm tra con người. Thực tế, nếu chúng ta giả định
lớp G cũng đúng, số lượng câu được sửa chữa bởi các chiến lược
tập hợp dựa trên PLM (ngoại trừ tập hợp mức chỉnh sửa) vượt quá
số lượng của seq2seq-1, mô hình đơn lẻ tốt nhất.

Điều này cho thấy rằng các tham chiếu GEC không đủ, mặc dù
các bộ dữ liệu như NLPCC cung cấp đa tham chiếu. Vì việc tạo ra
một câu đúng một cách nhân tạo khó hơn nhiều so với việc phán đoán
một chuỗi được tạo ra bởi máy là đúng hay không, việc liên tục
thêm kết quả được kiểm tra con người của các hệ thống tập hợp PLM
vào các tham chiếu có thể là một giải pháp tốt để cải thiện chất lượng
và tính đa dạng của dữ liệu thử nghiệm GEC.

Mục tiêu của GEC. Đây là một vấn đề quan trọng. Có đủ chỉ để
làm cho một câu thoát khỏi lỗi không? Lấy việc lập trình làm ví dụ,
chúng ta có thể nói một đoạn mã "tốt" khi tất cả các "lỗi" đều rõ ràng
nhưng hàng trang "cảnh báo" đang nhấp nháy không? Trong các mẫu
"Tốt", chúng tôi so sánh các tham chiếu con người và câu được tạo
tự động, và thấy nhiều tham chiếu chỉ đúng nhưng không quá thành ngữ.
Mặt khác, nhiều câu đầu ra của các chiến lược tập hợp dựa trên PLM
tự nhiên hơn và giống như người bản ngữ. Nếu một hệ thống GEC
nhằm mục đích giúp sinh viên nước ngoài học ngôn ngữ, chẳng hạn,
thì tính thành ngữ nên được xem xét.

Sửa chữa quá mức của các mô hình dựa trên PLM. Khoảng 1/10
câu được tạo ra trong tập hợp dựa trên PLM ("O") được sửa chữa
quá mức, tức là mô hình sửa chữa một token đúng và do đó tạo ra
một câu sai. PLM luôn chọn câu trôi chảy nhất với PPL thấp nhất,
đôi khi bỏ qua ý nghĩa gốc của câu nguồn. Việc sửa chữa quá mức
của các mô hình sinh sản dựa trên PLM nên được giải quyết trong
công việc tương lai.

6 Kết luận
Bài báo này giới thiệu các chiến lược tập hợp mới cho nhiệm vụ
GEC bằng cách tận dụng sức mạnh của các mô hình ngôn ngữ được
huấn luyện trước (PLM). Chúng tôi so sánh các chiến lược khác nhau
của tập hợp mô hình trong CGEC. Đáng ngạc nhiên, các chiến lược
tập hợp dựa trên PLM không có lợi cho hệ thống. Điều này cho thấy
rằng PPL và F0.5 có các mục tiêu phân kỳ. Theo phân tích của chúng tôi,
sự thiếu hụt của các tham chiếu trong GEC vẫn là một vấn đề lớn,
cần được cải thiện liên tục trong công việc tương lai.

Lời cảm ơn
Công việc này được hỗ trợ bởi Chương trình RD Công nghệ cao
Quốc gia của Trung Quốc (Số 2020AAA0106600), Quỹ Khoa học
Tự nhiên Quốc gia Trung Quốc (62076008) và Dự án Trọng điểm
của Quỹ Khoa học Tự nhiên Trung Quốc (61936012).

Hạn chế
Thứ nhất, chúng tôi không sử dụng bất kỳ mô hình đơn lẻ nào
không có PLM trong cấu trúc của chúng để thực hiện các thí nghiệm
so sánh, mặc dù ít mô hình tiên tiến ngày nay có thể thoát khỏi PLM.
Thứ hai, vì việc bao bọc của fairseq, chúng tôi không có quyền truy cập
vào tất cả các xác suất đầu ra của các mô hình đơn lẻ và do đó không thể
áp dụng chiến lược sử dụng tổng có trọng số của các mô hình đơn lẻ
và PLM được sử dụng trong Junczys-Dowmunt et al. (2018). Thứ ba,
trong khi các PLM dựa trên BERT giỏi dự đoán mặt nạ, chúng tôi chưa
tìm ra chiến lược để tận dụng khả năng đó mà không bị lúng túng
bởi xác suất có điều kiện. Thứ tư, chúng tôi chỉ thực hiện thí nghiệm
trên tiếng Trung.

Tuyên bố Đạo đức
Về Tạo phẩm Khoa học. Vì chúng tôi tập trung vào CGEC, tất cả
mã và công cụ đều dành cho tiếng Trung và tất cả dữ liệu đều bằng
tiếng Trung. Tất cả các tạo phẩm khoa học chỉ được sử dụng cho GEC.
Các tạo phẩm do Zhang et al. (2022) cung cấp có sẵn công khai dựa
trên giấy phép Apache-2.0, trên đó chúng tôi dựa mã và mô hình
của riêng mình.

Về Ngân sách Tính toán. Chúng tôi chạy tất cả các thí nghiệm
tập hợp mô hình trên CPU Intel® Xeon® Gold 5218. Thời gian xử lý
được hiển thị trong bảng 4.

Về Khả năng Tái tạo. Tất cả các thí nghiệm tập hợp mô hình
hoàn toàn có thể tái tạo khi PLM được đóng băng (tức là không quan
trọng chúng ta chạy thí nghiệm bao nhiều lần, kết quả đều giống nhau).

Về Người Chú thích Con người. Mỗi người chú thích được trả
20 đô la mỗi giờ, cao hơn mức lương tối thiểu hợp pháp. Hướng dẫn
được hiển thị trong Phụ lục A.

Tài liệu tham khảo
Christopher Bryant, Zheng Yuan, Muhammad Reza Qorib,
Hannan Cao, Hwee Tou Ng, và Ted Briscoe. 2022. Grammatical
error correction: A survey of the state of the art. arXiv preprint
arXiv:2211.05166.

Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang,
và Guoping Hu. 2020. Revisiting pre-trained models for Chinese
natural language processing. Trong Findings of the Association
for Computational Linguistics: EMNLP 2020, trang 657–668,
Trực tuyến. Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova.
2019. BERT: Pre-training of deep bidirectional transformers for
language understanding. Trong Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), trang 4171–4186, Minneapolis, Minnesota. Association
for Computational Linguistics.

Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha,
và Kenneth Heafield. 2018. Approaching neural grammatical error
correction as a low-resource machine translation task. Trong
Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), trang 595–606, New Orleans,
Louisiana. Association for Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke
Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension.
Trong Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, trang 7871–7880.

Chen Li, Junpei Zhou, Zuyi Bao, Hengyou Liu, Guangwei Xu,
và Linlin Li. 2018. A hybrid system for Chinese grammatical error
diagnosis and correction. Trong Proceedings of the 5th Workshop
on Natural Language Processing Techniques for Educational
Applications, trang 60–69, Melbourne, Australia. Association
for Computational Linguistics.

Deng Liang, Chen Zheng, Lei Guo, Xin Cui, Xiuzhang Xiong,
Hengqiao Rong, và Jinpeng Dong. 2020. BERT enhanced neural
machine translation and sequence tagging model for Chinese
grammatical error diagnosis. Trong Proceedings of the 6th Workshop
on Natural Language Processing Techniques for Educational
Applications, trang 57–66, Suzhou, China. Association for
Computational Linguistics.

Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem Chernodub,
và Oleksandr Skurzhanskyi. 2020. Gector–grammatical error
correction: Tag, not rewrite. Trong Proceedings of the Fifteenth
Workshop on Innovative Use of NLP for Building Educational
Applications, trang 163–170.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross,
Nathan Ng, David Grangier, và Michael Auli. 2019. fairseq: A fast,
extensible toolkit for sequence modeling. Trong Proceedings of
NAACL-HLT 2019: Demonstrations.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei,
và Ilya Sutskever. 2019. Language models are unsupervised
multitask learners.

Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Fei Yang,
Li Zhe, Hujun Bao, và Xipeng Qiu. 2021. Cpt: A pre-trained
unbalanced transformer for both chinese language understanding
and generation. arXiv preprint arXiv:2109.05729.

Maosong Sun, Jingyang Li, Zhipeng Guo, Zhao Yu, Y Zheng,
X Si, và Z Liu. 2016. Thuctc: an efficient chinese text classifier.
GitHub Repository.

Maksym Tarnavskyi, Artem Chernodub, và Kostiantyn Omelianchuk.
2022. Ensembling and knowledge distilling of large sequence taggers
for grammatical error correction. arXiv preprint arXiv:2203.13064.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin.
2017. Attention is all you need. Advances in neural information
processing systems, 30.

Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia,
Liwei Peng, và Luo Si. 2019. Structbert: Incorporating language
structures into pre-training for deep language understanding.
arXiv preprint arXiv:1908.04577.

Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Jurafsky,
và Andrew Y. Ng. 2016. Neural language correction with
character-based attention. CoRR, abs/1603.09727.

Zheng Yuan và Ted Briscoe. 2016. Grammatical error correction
using neural machine translation. Trong Proceedings of the 2016
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies,
trang 380–386.

Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li, Bo Zhang,
Chen Li, Fei Huang, và Min Zhang. 2022. MuCGEC: a multi-reference
multi-source evaluation dataset for Chinese grammatical error
correction. Trong Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies, trang 3118–3130, Seattle, United States.
Association for Computational Linguistics.

Yuanyuan Zhao, Nan Jiang, Weiwei Sun, và Xiaojun Wan. 2018.
Overview of the nlpcc 2018 shared task: Grammatical error correction.
Trong CCF International Conference on Natural Language Processing
and Chinese Computing, trang 439–445. Springer.

A Hướng dẫn cho Chú thích Con người
Hướng dẫn cho người chú thích con người được đề cập trong
Phần 5 như sau:

1. Bạn có thể xem dữ liệu trong "sample_200.txt", chứa kết quả
của 200 câu.

2. Mỗi mẫu chứa một số dòng, bao gồm "Input" (câu nguồn),
"seq2seq-1", "Sentence-level", "Edit-level", "Edit-combination",
và một hoặc hai dòng "Reference".

3. Bạn cần chú thích các dòng "seq2seq-1", "Sentence-level",
"Edit-level" và "Edit-combination" theo đầu vào và (các) tham chiếu.

4. Cụ thể, bạn nên chọn từ bốn loại sau. Chính xác (E): đầu ra
trôi chảy và đúng, phù hợp với tham chiếu. Tốt (G): đầu ra trôi chảy
và đúng nhưng khác với tham chiếu, điều này cho thấy rằng các tham
chiếu không đủ. Sửa chữa quá mức (O): đầu ra trôi chảy nhưng
không đáp ứng ý nghĩa gốc của câu nguồn. Sai (W): đầu ra có các
vấn đề khác mà chúng tôi không quan tâm trong công việc này.

5. Cảm ơn vì đóng góp của bạn!
