# 2406.06263.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2406.06263.pdf
# File size: 347875 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MaskLID: Code-Switching Language Identification through Iterative
Masking
Amir Hossein Kargaran♣, François Yvon♠and Hinrich Schütze♣
♣LMU Munich & Munich Center for Machine Learning, Munich, Germany
♠Sorbonne Université & CNRS, ISIR, Paris, France
amir@cis.lmu.de
Abstract
We present MaskLID , a simple, yet effective,
code-switching (CS) language identification
(LID) method. MaskLID does not require
any training and is designed to complement
current high-performance sentence-level LIDs.
Sentence-level LIDs are classifiers trained on
monolingual texts to provide single labels, typ-
ically using a softmax layer to turn scores
into probabilities. However, in cases where
a sentence is composed in both L1 and L2 lan-
guages, the LID classifier often only returns the
dominant label L1. To address this limitation,
MaskLID employs a strategy to mask text fea-
tures associated with L1, allowing the LID to
classify the text as L2 in the next round. This
method uses the LID itself to identify the fea-
tures that require masking and does not rely on
any external resource. In this work, we explore
the use of MaskLID for two open-source LIDs
(GlotLID and OpenLID), that are both based on
theFastText architecture. Code and demo are
available at github.com/cisnlp/MaskLID .
1 Introduction
Code-switching (CS), the juxtaposition of two
or more languages within a single discourse
(Gumperz, 1982), is prevalent in both written
and spoken communication (Sitaram et al., 2019;
Do˘gruöz et al., 2021). While CS has traditionally
been explored as a speech phenomenon (Milroy
and Muysken, 1995; Auer, 2013), the increasing
prevalence of CS in digital communication, such
as SMS and social media platforms (Das and Gam-
bäck, 2013; Bali et al., 2014), requires the develop-
ment of techniques to also analyze CS in written
texts. There is however a lack of CS data for re-
searchers, making it difficult to study CS and to
effectively train CS-aware models. This shortage
affects many NLP applications dealing with CS
scenarios (Solorio et al., 2021; Winata et al., 2023).
A first step towards the collection of high-qualitycorpora of CS texts is thus to identify samples of
CS in running texts.
Previous works on CS language identification
(LID) have mainly focused on building word-level
LIDs for code-switching between specific pairs of
languages, and are often limited to recognize only
two languages (Solorio et al., 2014; Nguyen and
Do˘gruöz, 2013; Elfardy et al., 2013; Barman et al.,
2014). However, such approaches are not realistic
on a larger scale, especially considering that texts
on the web typically lack prior information about
the languages that are actually being used.
More recently, Burchell et al. (2024) have inves-
tigated the use of high-quality LID at the sentence-
level to detect instances of CS. They propose to
reformulate CS LID as a sentence-level task and
to associate each segment with a set of language
labels . Their investigation reveals the difficulty
of achieving effective CS LID with existing LID
models. Furthermore, their findings indicate that
such LIDs predominantly predict only one of the
languages occurring in CS sentences.
In this work, we continue this line of reser-
ach and introduce MaskLID , a method that also
uses high-quality sentence-level LID to identify
CS segments. By masking the presence of the text
features associated with the dominant language,
MaskLID improves the ability to recognize addi-
tional language(s) as well. We explain in detail how
MaskLID works in cooperation with two existing
LIDs that are based on the FastText (Bojanowski
et al., 2017) architecture in Section 3. As we dis-
cuss, our method can identify arbitrary pairs of lan-
guages, and is also able to detect mixtures of more
than two languages in the same segment. Being
based on FastText , it is also extremely fast. This
two properties make MaskLID well suited to mine
large web corpora for examples of real-world CS
segments, that can then serve as valuable training
data for applications designed to handle CS inputs.
We evaluate MaskLID on two test datasets contain-arXiv:2406.06263v1  [cs.CL]  10 Jun 2024

--- PAGE 2 ---
ing both CS and monolingual data, showing the
benefits of using MaskLID (see Section 4).
2 One Sentence, Multiple Languages
2.1 Code-switching, Code-mixing
Code-switching (CS) can be defined as the alternate
use of two languages within the same utterance
and can happen either between sentences (inter-
sentential CS) or within a sentence (intra-sentential
CS or code-mixing ) (Gumperz, 1982). While loan-
words are often seen as a simple form of CS, their
assimilation into a foreign linguistic system some-
times yields a mixed use of languages within a sin-
gle word . For the purpose of this work, we mostly
focus on inter-sentential CS and use the terms code-
switching and code-mixing interchangeably, even
though our approach could in fact apply to longer
chunks of texts. From an abstract perspective, the
main trait of CS is thus the juxtaposition of two (or
more) languages within a single segments, a view
that is also adopted in e.g. from Bali et al. (2014).
From this perspective, CS ID can be formulated as
identifying more than one language ID in a given
text segment. We also use the fact that mixing does
not take place randomly (Myers-Scotton, 1997),
and that one language plays a dominant role and
provides the linguistic structure into which inserts
from other languages can take place.
In the next paragraph, we discuss two previous
approaches that share this view and which serve
as the foundation of MaskLID . For other related
works, refer to Appendix A.
2.2 Detecting CS with Lexical Anchors
Our work is most closely related to the research of
Mendels et al. (2018). They propose a method to
identify CS data in sentences written in two lan-
guages L1 and L2. Their approach first requires a
language identifier that is able to label the majority
language of a document as language L1, even when
the document also contains words that belong to L2.
This aligns with our setup, as sentence-level LID
models trained on monolingual texts often demon-
strate similar performance on CS data, primarily
predicting the dominant language L1 (Burchell
et al., 2024).
Mendels et al. (2018) also introduce the concept
ofanchors for each language, defining an anchor
as a word belonging to only one language within a
language pool L. The set of anchors in their work
is computed based on the analysis of monolingualcorpora, and constitutes an external resource to
their CS LID system. To relax the definition of
anchors, they also introduce the notion of weak
anchor for a language L2 relative to some other
language L1: an anchor is considered a weak an-
chor’ if it is observed in monolingual L2 corpora
but not in monolingual L1 corpora.
In their definition of CS for L1+L2 sentences, a
sentence is then considered CS if and only if it is
predicted to be in language L1 by the LID model
and contains at least one weak anchor from the
L2 anchor set (relative to L1). Our method shares
similarity with this work in that, for L1+L2 sen-
tences, the initial step consists in the identification
of L1. However, while their approach requires the
identification of sets of weak anchors for each lan-
guage pair, we identify the minority language(s)
L2 using only features that are internal to the main
LID model, dispensing from the need to compile
external resources.
2.3 CS Detection as Set Prediction Problem
Another work that is closely related to ours is the
research conducted by Burchell et al. (2024). They
use three different sentence-level LID models for
CS LID: 1) OpenLID (Burchell et al., 2023), a high-
quality LID model operating at the sentence level;
2) Multi-label OpenLID, which is similar to Open-
LID but is trained with a binary cross-entropy loss
instead of the conventional cross-entropy, and deliv-
ers Yes-No decisions for each possible language;1
and 3) Franc (Wormer, 2014), which uses trigram
distributions in the input text and a language model
to compute languages and their scores.
However, the result of these models on CS LID
are not very promising especially for the Turkish-
English CS dataset (see Section 4). One reason
is that the occurrence of one single English word
in a Turkish sentence is tagged in the gold refer-
ence as an instance of CS. Yet, one single word
may not be enough to yield large logit values for
the English label in these difficult predictions. But
this is not the only reason these models fail. Scal-
ing the baseline LID to support more languages,
which is a strong motivation behind models such
as GlotLID (Kargaran et al., 2023) and OpenLID,
makes CS LID predictions more challenging. For
instance, when the model encounters a Turkish-
English sentence and predicts Turkish as the top
1See FastText documentation:
fasttext.cc/docs/en/supervised-tutorial.html#
multi-label-classification .

--- PAGE 3 ---
language, the second best prediction may not be
English, but a language closest to Turkish instead,
such as North Azerbaijani or Turkmen, which have
more active ngram features in the CS sentence than
English. Consider, for instance, the example sen-
tence from Burchell et al. (2024, Table 9):
bir kahve dükkanında geçen film
tadında güzel bir şarkıya ayrılsın
gece falling in love at a coffee shop
OpenLID’s top 5 predictions for this sentence
are: 1) Turkish, 2) North Azerbaijani, 3) Crimean
Tatar, 4) Turkmen, 5) Tosk Albanian, with English
predicted as the 15th most likely language. Yet,
for a speaker of either Turkish or English, it is
obvious that this sentence is a mixture of just these
two languages. To solve this, MaskLID suggests
to mask the Turkish part of the sentence:
<MASK> film <MASK>
falling in love at a coffee shop.
If we now ask OpenLID to predict this masked
sentence (without the token <MASK> ), the top
prediction would be English with 0.9999 confi-
dence. MaskLID makes models such as OpenLID
much more suitable for this task. Details on how
MaskLID computes the masked parts are in Sec-
tion 3.
3 MaskLID
3.1 FastText-based LIDs
In this paper, we explore the use of MaskLID for
LIDs based on the FastText (Bojanowski et al.,
2017) architecture. However, it is also possi-
ble to apply MaskLID to other LIDs, as long as
they enable to determine how much each feature
(e.g., word) contributes to each supported language.
FastText is one of the most popular LID architec-
tures due to its open-source nature, high perfor-
mance, ease of use, and efficiency. FastText clas-
sifier is a multinomial logistic classifier that rep-
resents the input sentence as a set of feature em-
beddings, making it easy to assess each feature’s
contribution to the final prediction.
Given a sentence s, letf1, f2, . . . , f Trepresent
the features extracted from s. Note that these fea-
tures are linearly ordered, i.e., fiprecedes fi+1in
s.FastText maps these features onto vectors in
Rdvia feature embeddings x1,x2, . . . , xT. The
dimensionality of these embeddings, denoted d, is
a hyperparameter. A base LID using FastText ar-
chitecture computes the posterior probability fora language c∈[1 :N]by applying the softmax
function over logits as:
P(c|s) =exp(bc·1
TPT
t=1xt)
PN
c′=1exp(bc′·1
TPT
t=1xt).(1)
P(c|s)is the base LID probability of the input
textsbelonging to language c,bcis the weight
vector for language c, andNis the total number of
classes supported by the base LID.
To evaluate how much each feature contributes
to each supported language, we need to compute
logits separately for each feature. For simplicity
and alignment with the FastText tokenizer (which
considers white-spaces as token boundaries), we
set the level of granularity of features to be the
word level. The word-level feature embedding is
obtained as the summation of all feature embed-
dings that build each word. Noting Wthe number
of words in a sentence s, we define the N×Wma-
trixV(s), where each element Vc,t(s)represents
the logits for language cand word-level feature xt:
Vc,t(s) =bc·xt. (2)
3.2 The MaskLID Method
We define the MaskLID algorithm in alignment
with Burchell et al. (2024): given an input text, the
objective is to return a set of codes corresponding to
the language(s) it contains. However, MaskLID is
more explainable and provides insights into which
parts of the sentence contributed to its decision.
The MaskLID algorithm works as follows:
Input:
1) sentence s.
2)α, an integer parameter used to define strong
associations between words and languages:
having a language appear in the top- αlogit
values for a word is a strong cue that this word
belongs to that language.
3)β, an integer parameter used to define weak
associations between words and languages:
languages appearing in the top- βlogit values
for a word are weakly associated with that
word. βis always greater than α.
4)τ, a threshold representing the minimum size
of a sentence (in bytes) for which the LID
makes reliable decisions.
5)λ, a parameter defining the number of times
the algorithm should be repeated.

--- PAGE 4 ---
Output:
1)List of predicted languages, along with their
associated word-level features.
Procedure:
0)Take sentence sand compute V(s)using
Eq. (2). Assign sto variable u.
1)Compute the posterior probability for each
possible language using Eq. (1). Find the most
likely class ( L1 = arg max cP(c|u)) along
with its corresponding probability P(L1|u).
Assign L1 to variable L u.
2)Process column V:,t(s)for each unmasked
word tinu. If the value of VLu,t(s)is in
the top- βvalues for that column, then assign
word tto language Lu. If the value of VLu,tis
among the top- αvalues for that column, mask
word tfrom sentence u.
Masked words play here a role similar to the
anchors used in (Mendels et al., 2018): re-
call that for these authors, anchor words are
selected to uniquely identify one language –
there removal is likely to decrease the recog-
nition of L1, without impacting the ability to
recognize L2. In our approach, we identify
these pseudo-anchors on the spot, relying on
the LID internal scoring procedure.
3)check if length of u(in bytes, ignoring masked
words) is greater than τ. If not, then terminate.
This is one termination condition (for addi-
tional considerations, refer to Appendix B).
Setting τ= 0will just check that the masked
sentence is not empty, but it is better to use
a non-zero threshold, as most sentence-level
LIDs do not reliably predict short sentences
(Jauhiainen et al., 2019).
4)if the number of iterations is lower than λthen
go to back to step 1, else stop.
The complexity of this greedy procedure is
O(λ×T×Nlogβ).
4 Experiments and Results
Here, we provide an overview of our baselines
and test data. We assess the performance of the
baselines by testing them both with and without
MaskLID . Our setting of hyperparameters is ex-
plained in Appendix C.2.4.1 Baselines
Our baseline LID models are OpenLID2(support-
ing≈200 languages) and GlotLID v3.03(support-
ing≈2100 languages), two LIDs based on the
FastText architecture. For a fair comparison be-
tween these models, we limit the languages that
GlotLID supports to the same set as OpenLID (see
details in Appendix C.1). Two exceptions are ro-
manized Nepali (nep_Latn) and Hindi (hin_Latn),
which are not supported by OpenLID, but for which
we also have test data that is also used to evaluate
MaskLID with GlotLID.
4.2 Test Data
We choose Turkish-English (Yirmibe¸ so ˘glu and Ery-
i˘git, 2018), Hindi-English (Aguilar et al., 2020),
Nepali-English (Aguilar et al., 2020) and Basque-
Spanish (Aguirre et al., 2022), as our test datasets.
We have data for four CS labels and six single la-
bels (see Table 1). Details regarding these test sets,
preprocessing, their descriptions, and information
on access are in Appendix D.
4.3 Metrics
We use the number of exact (#EM) and partial
matches (#PM), along with the count of false pos-
itives (#FP) as the main metrics in our evaluation.
To ensure clarity and prevent misinterpretation of
the results, we report the absolute number of in-
stances rather than percentages.
1)#EM: This metric counts a prediction as a
match when it exactly matches the true
2)#PM: This metric counts a prediction as a
match when only part of the information is
correct: for a single label, if it is part of the
prediction; for a CS label, if part of the label
exactly matches the prediction.
3)#FP: If any label other than X is misclassified
as X, it counts as an FP for X. We do not
consider the #FP for single labels, as partial
matches of CS are counted as FP for single
labels. Therefore, we only report the FP for
CS sentences.
4.4 Results
Table 1 presents the results on the test data for two
baseline LIDs and two settings, with and without
MaskLID . The best exact match (#EM) for CS la-
bels is in boldface in the table, demonstrating that
2https://huggingface.co/laurievb/openlid
3https://huggingface.co/cis-lmu/glotlid

--- PAGE 5 ---
Baseline + MaskLID Baseline
#EM/#PM ↑ #FP↓ #EM/#PM ↑ # FP ↓
#S GlotLID OpenLID GlotLID OpenLID GlotLID OpenLID GlotLID OpenLID
CS Turkish–English 333 91/328 68 /327 0 0 4/327 4/326 0 0
CS Basque–Spanish 440 43 /430 47/426 0 0 9/426 9/424 0 3 (from Spanish)
CS Hindi–English 253 29/219 - 0 - 5 /211 - 0 -
CS Nepali–English 712 22/444 - 0 - 0 /420 - 0 -
Single Basque 357 354/354 355/355 - - 353/353 355/355 - -
Single Spanish 347 335/337 297/300 - - 337/340 287/311 - -
Single Turkish 340 333/337 329/334 - - 335/337 329/335 - -
Single Hindi 29 18/19 - - - 17/18 - - -
Single Nepali 197 63/75 - - - 68/72 - - -
Single English 508 459/490 428/469 - - 486/490 455/462 - -
Table 1: Number of exact (#EM) and partial matches (#PM) and count of false positives (#FP) calculated over CS
and single label test instances. The best exact match for CS instances is in bold, and the second is underlined. #S
reports the number of sentences for each test set.
the baseline with MaskLID achieves better perfor-
mance compared to the baseline without it. Partial
matches (#PM) in both settings (with and without
MaskLID) are quite similar.
For CS Turkish-English, MaskLID detects 91
CS at best, compared to 4 without it. For Basque-
Spanish, MaskLID detects 47 CS, versus 9 with-
out it. For Hindi-English, MaskLID detects 29
CS, compared to 5 without it. For Nepali-English,
MaskLID detects 22 CS, while none are detected
without it.
In all single-language test instances, GlotLID
outperforms OpenLID. This is also the case for
CS language instances, except for Basque-Spanish.
Considering the relatively poorer performance of
OpenLID in both single Basque and single Spanish,
overall, GlotLID proves to be the better model for
these tasks.
Additional Considerations. For CS instances:
1) The difference between #PM and #EM corre-
sponds to the number of times only one of two
mixed languages in a CS instance is predicted. 2)
The difference between number of sentences (#S)
and #PM corresponds to the number of times none
of the languages in the CS instance is predicted. In
all CS setups, the #EM and #PM value in the base-
line with MaskLID are always greater than without.
Additionally, the difference between #PM and #EM
is also smaller, which indicates a higher precision
in CS LID.
For single language instances: 1) The difference
between #PM and #EM corresponds to the number
of times the single label instance is classified as
part of a multi-label instance. 2) The difference
between #S and #PM corresponds to the number
of times a single label is never predicted, even as
part of a multi-label instance. For all single lan-guage instances, the results are quite similar except
for single English, where the number of incorrect
CS in baseline with MaskLID (#PM - #EM) is
greater than with baseline alone. To address this,
using a larger minimum length τhelps decrease
the number of CS false positives. For single En-
glish, in GlotLID with MaskLID setting, increas-
ingτfrom 20to25raises the #EM from 459 to
473; however, it reduces the #EM in GlotLID with
MaskLID setting for CS Turkish-English from 91
to 67, CS Hindi-English from 29 to 26, and CS
Nepali-English from 22 to 18. Examples of suc-
cesses and failures of MaskLID are provided in
Appendix E.
5 Conclusion
We present MaskLID , a simple, yet effective,
method for scalable code-switching (CS) language
identification (LID). MaskLID is designed to com-
plement existing high-performance sentence-level
LID models and does not require any training.
In our experiments, MaskLID increases CS LID
by a factor of 22 in Turkish-English, by 22 in
Nepali-English, by 6 in Hindi-English and by 5
in Basque–Spanish.
In future work, we aim to explore the use of
subword-level, instead of word-level features, ex-
tending the applicability of the method to languages
that do not use spaces for word separation. Addi-
tionally, we plan to generalize this method to other
LID models using techniques like LIME (Ribeiro
et al., 2016) to map features to languages. Last, we
intend to apply MaskLID on the web data, in the
hope that it will help build larger high-quality web
corpora for CS.

--- PAGE 6 ---
Limitations
The CS testsets we use in this study only repre-
sent a small subset of potential uses of CS lan-
guages. Creating additional CS datasets for more
languages would definitely be an extension of this
work. MaskLID uses hyperparameters, and chang-
ing the model and the set of languages it supports
may require adjustments to these parameters. Al-
though MaskLID detects more CS than the stan-
dalone baseline LID models, it still has a long way
to go to predict the majority of them. One im-
portant source of remaining errors is loan words,
where the L2 insert is just one word long: these
cannot be detected with out current hyperparame-
ter settings. The performance of MaskLID is also
bound by the LID it uses; it might not have good
performance for some languages, resulting e.g. in
a large number of false positives.
Ethics Statement
MaskLID uses openly available open-source LID
models and does not require any additional re-
sources except for hyperparameters. Concerning
the evaluation data, these datasets have undergone
anonymization to safeguard the privacy of all par-
ties involved. We provide links to the data and
do not host it ourselves. We provide detailed de-
scriptions of our method and evaluation process.
Additionally, we make our code openly available
to foster collaboration and reproducibility.
Acknowledgements
The authors thank the anonymous reviewers and
editors for their comments of the previous version
of this work. This research was supported by DFG
(grant SCHU 2246/14-1).
References
Ife Adebara, AbdelRahim Elmadany, Muhammad
Abdul-Mageed, and Alcides Inciarte. 2022. AfroLID:
A neural language identification tool for African lan-
guages. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 1958–1981, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Gustavo Aguilar, Sudipta Kar, and Thamar Solorio.
2020. LinCE: A centralized benchmark for linguis-
tic code-switching evaluation. In Proceedings of the
Twelfth Language Resources and Evaluation Confer-
ence, pages 1803–1813, Marseille, France. European
Language Resources Association.Maia Aguirre, Laura García-Sardiña, Manex Serras, Ar-
iane Méndez, and Jacobo López. 2022. BaSCo: An
annotated Basque-Spanish code-switching corpus for
natural language understanding. In Proceedings of
the Thirteenth Language Resources and Evaluation
Conference , pages 3158–3163, Marseille, France. Eu-
ropean Language Resources Association.
Mohamed Al-Badrashiny and Mona Diab. 2016. LILI:
A simple language independent approach for lan-
guage identification. In Proceedings of COLING
2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers , pages 1211–
1219, Osaka, Japan. The COLING 2016 Organizing
Committee.
Peter Auer. 2013. Code-switching in conversation: Lan-
guage, interaction and identity . Routledge.
Kalika Bali, Jatin Sharma, Monojit Choudhury, and Yo-
garshi Vyas. 2014. “I am borrowing ya mixing ?” an
analysis of English-Hindi code mixing in Facebook.
InProceedings of the First Workshop on Computa-
tional Approaches to Code Switching , pages 116–126,
Doha, Qatar. Association for Computational Linguis-
tics.
Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code mixing: A challenge
for language identification in the language of social
media. In Proceedings of the First Workshop on
Computational Approaches to Code Switching , pages
13–23, Doha, Qatar. Association for Computational
Linguistics.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics , 5:135–146.
Ralf D Brown. 2012. Finding and identifying text in
900+ languages. Digital Investigation , 9:S34–S43.
Laurie Burchell, Alexandra Birch, Nikolay Bogoychev,
and Kenneth Heafield. 2023. An open dataset and
model for language identification. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) ,
pages 865–879, Toronto, Canada. Association for
Computational Linguistics.
Laurie Burchell, Alexandra Birch, Robert Thompson,
and Kenneth Heafield. 2024. Code-switched lan-
guage identification is harder than you think. In
Proceedings of the 18th Conference of the European
Chapter of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 646–658,
St. Julian’s, Malta. Association for Computational
Linguistics.
Amitava Das and Björn Gambäck. 2013. Code-mixing
in social media text. Traitement Automatique des
Langues , 54(3):41–64.
Amitava Das and Björn Gambäck. 2014. Identifying
languages at the word level in code-mixed Indian

--- PAGE 7 ---
social media text. In Proceedings of the 11th Interna-
tional Conference on Natural Language Processing ,
pages 378–387, Goa, India. NLP Association of In-
dia.
A. Seza Do ˘gruöz, Sunayana Sitaram, Barbara E. Bul-
lock, and Almeida Jacqueline Toribio. 2021. A sur-
vey of code-switching: Linguistic and social per-
spectives for language technologies. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 1654–1666, Online.
Association for Computational Linguistics.
Jonathan Dunn. 2020. Mapping languages: The corpus
of global language use. Language Resources and
Evaluation , 54:999–1018.
Jonathan Dunn and Lane Edwards-Brown. 2024.
Geographically-informed language identification. In
Proceedings of the 2024 Joint International Con-
ference on Computational Linguistics, Language
Resources and Evaluation (LREC-COLING 2024) ,
pages 7672–7682, Torino, Italia. ELRA and ICCL.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona Diab.
2013. Code switch point detection in Arabic. In
Natural Language Processing and Information Sys-
tems: 18th International Conference on Applications
of Natural Language to Information Systems, NLDB
2013, Salford, UK, June 19-21, 2013. Proceedings
18, pages 412–416. Springer.
John J Gumperz. 1982. Discourse strategies . 1. Cam-
bridge University Press.
Tommi Jauhiainen, Heidi Jauhiainen, and Krister
Lindén. 2022. HeLI-OTS, off-the-shelf language
identifier for text. In Proceedings of the Thirteenth
Language Resources and Evaluation Conference ,
pages 3912–3922, Marseille, France. European Lan-
guage Resources Association.
Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timo-
thy Baldwin, and Krister Lindén. 2019. Automatic
language identification in texts: A survey. Journal of
Artificial Intelligence Research , 65:675–782.
Navya Jose, Bharathi Raja Chakravarthi, Shardul
Suryawanshi, Elizabeth Sherly, and John P Mc-
Crae. 2020. A survey of current datasets for code-
switching research. In 2020 6th international con-
ference on advanced computing and communication
systems (ICACCS) , pages 136–141. IEEE.
Amir Kargaran, Ayyoob Imani, François Yvon, and
Hinrich Schuetze. 2023. GlotLID: Language identifi-
cation for low-resource languages. In Findings of the
Association for Computational Linguistics: EMNLP
2023 , pages 6155–6218, Singapore. Association for
Computational Linguistics.
Amir Hossein Kargaran, François Yvon, and Hinrich
Schütze. 2024. GlotScript: A resource and tool forlow resource writing system identification. In Pro-
ceedings of the 2024 Joint International Conference
on Computational Linguistics, Language Resources
and Evaluation (LREC-COLING 2024) , pages 7774–
7784, Torino, Italia. ELRA and ICCL.
Laurent Kevers. 2022. CoSwID, a code switching iden-
tification method suitable for under-resourced lan-
guages. In Proceedings of the 1st Annual Meeting of
the ELRA/ISCA Special Interest Group on Under-
Resourced Languages , pages 112–121, Marseille,
France. European Language Resources Association.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1110–1119,
Atlanta, Georgia. Association for Computational Lin-
guistics.
Tom Kocmi and Ond ˇrej Bojar. 2017. LanideNN: Multi-
lingual language identification on character window.
InProceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 1, Long Papers , pages 927–936,
Valencia, Spain. Association for Computational Lin-
guistics.
Thomas Lavergne, Gilles Adda, Martine Adda-Decker,
and Lori Lamel. 2014. Automatic language identity
tagging on word and sentence-level in multilingual
text sources: a case-study on Luxembourgish. In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC’14) ,
pages 3300–3304, Reykjavik, Iceland. European Lan-
guage Resources Association (ELRA).
Holy Lovenia, Samuel Cahyawijaya, Genta Winata,
Peng Xu, Yan Xu, Zihan Liu, Rita Frieske, Tiezheng
Yu, Wenliang Dai, Elham J. Barezi, Qifeng Chen,
Xiaojuan Ma, Bertram Shi, and Pascale Fung. 2022.
ASCEND: A spontaneous Chinese-English dataset
for code-switching in multi-turn conversation. In Pro-
ceedings of the Thirteenth Language Resources and
Evaluation Conference , pages 7259–7268, Marseille,
France. European Language Resources Association.
Manuel Mager, Özlem Çetino ˘glu, and Katharina Kann.
2019. Subword-level language identification for
intra-word code-switching. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers) , pages 2005–2011, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Gideon Mendels, Victor Soto, Aaron Jaech, and Julia
Hirschberg. 2018. Collecting code-switched data
from social media. In Proceedings of the Eleventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2018) , Miyazaki, Japan. European
Language Resources Association (ELRA).

--- PAGE 8 ---
Lesley Milroy and Pieter Muysken. 1995. One speaker,
two languages: Cross-disciplinary perspectives on
code-switching , volume 10. Cambridge University
Press.
Giovanni Molina, Fahad AlGhamdi, Mahmoud
Ghoneim, Abdelati Hawwari, Nicolas Rey-
Villamizar, Mona Diab, and Thamar Solorio. 2016.
Overview for the second shared task on language
identification in code-switched data. In Proceed-
ings of the Second Workshop on Computational
Approaches to Code Switching , pages 40–49, Austin,
Texas. Association for Computational Linguistics.
Carol Myers-Scotton. 1997. Duelling languages: Gram-
matical structure in codeswitching . Oxford Univer-
sity Press.
Dong Nguyen and A. Seza Do ˘gruöz. 2013. Word level
language identification in online multilingual commu-
nication. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing ,
pages 857–862, Seattle, Washington, USA. Associa-
tion for Computational Linguistics.
NLLB Team, Marta R. Costa-jussà, James Cross, Onur
Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Rop-
ers, Safiyyah Saleem, Holger Schwenk, and Jeff
Wang. 2022. No language left behind: Scaling
human-centered machine translation. arXiv preprint
arXiv:2207.04672 .
Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. " why should i trust you?" explaining
the predictions of any classifier. In Proceedings of
the 22nd ACM SIGKDD international conference on
knowledge discovery and data mining , pages 1135–
1144.
Shruti Rijhwani, Royal Sequiera, Monojit Choudhury,
Kalika Bali, and Chandra Shekhar Maddila. 2017.
Estimating code-switching on Twitter with a novel
generalized word-level language detection technique.
InProceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 1971–1982, Vancouver, Canada.
Association for Computational Linguistics.
Sunayana Sitaram, Khyathi Raghavi Chandu, Sai Kr-
ishna Rallabandi, and Alan W. Black. 2019. A survey
of code-switched speech and language processing.
arXiv preprint arXiv:1904.00784 .
Thamar Solorio, Elizabeth Blair, Suraj Mahar-
jan, Steven Bethard, Mona Diab, MahmoudGhoneim, Abdelati Hawwari, Fahad AlGhamdi, Julia
Hirschberg, Alison Chang, and Pascale Fung. 2014.
Overview for the first shared task on language iden-
tification in code-switched data. In Proceedings of
the First Workshop on Computational Approaches to
Code Switching , pages 62–72, Doha, Qatar. Associa-
tion for Computational Linguistics.
Thamar Solorio, Shuguang Chen, Alan W. Black, Mona
Diab, Sunayana Sitaram, Victor Soto, Emre Yilmaz,
and Anirudh Srinivasan, editors. 2021. Proceedings
of the Fifth Workshop on Computational Approaches
to Linguistic Code-Switching . Association for Com-
putational Linguistics, Online.
Aleksander Stensby, B John Oommen, and Ole-
Christoffer Granmo. 2010. Language detection and
tracking in multilingual documents using weak esti-
mators. In Joint IAPR International Workshops on
Statistical Techniques in Pattern Recognition (SPR)
and Structural and Syntactic Pattern Recognition
(SSPR) , pages 600–609. Springer.
Genta Winata, Sudipta Kar, Marina Zhukova, Thamar
Solorio, Mona Diab, Sunayana Sitaram, Monojit
Choudhury, and Kalika Bali, editors. 2023. Pro-
ceedings of the 6th Workshop on Computational Ap-
proaches to Linguistic Code-Switching . Association
for Computational Linguistics, Singapore.
Titus Wormer. 2014. Franc library.
Zeynep Yirmibe¸ so ˘glu and Gül¸ sen Eryi ˘git. 2018. De-
tecting code-switching between Turkish-English lan-
guage pair. In Proceedings of the 2018 EMNLP
Workshop W-NUT: The 4th Workshop on Noisy User-
generated Text , pages 110–115, Brussels, Belgium.
Association for Computational Linguistics.
A Related Work
LID has been a longstanding and active research
area in NLP (Jauhiainen et al., 2019). Past research
in LID can be classified into two primary subcate-
gories: 1) monolingual LID; 2) CS LID.
The first category is designed under the assump-
tion that the text is entirely monolingual, or the text
contains discrete monolingual chunks (e.g., sen-
tences) in different languages. The aim of these
works is to identify the language of the whole text
or each chunk. The majority of research on this
topic has been focused on covering more languages,
with recent work claiming to cover over a thou-
sand (Kargaran et al., 2023; Adebara et al., 2022;
NLLB Team et al., 2022; Burchell et al., 2023;
Dunn, 2020; Dunn and Edwards-Brown, 2024;
Jauhiainen et al., 2022; Brown, 2012).
The second category has received less attention
than the first category. LID at either the document
or sentence level is not effective in accurately iden-
tifying CS, which may occur within a sentence.

--- PAGE 9 ---
LIDs that identify languages at the word level are
proposed to address this issue. The majority of
studies have focused on scenarios where two prede-
fined languages are looked for in the input, specif-
ically concentrating on binary language detection
at the word level (Nguyen and Do ˘gruöz, 2013; Das
and Gambäck, 2014; Elfardy et al., 2013; King
and Abney, 2013; Al-Badrashiny and Diab, 2016).
While some attempts choose sentence-level granu-
larity (Stensby et al., 2010; Lavergne et al., 2014),
most CS LIDs prefer operating at the word or token
level. Nevertheless, certain approaches broaden
the analysis to the character level (Kocmi and Bo-
jar, 2017). Among the most recent works on CS
LID, Kevers (2022) propose a method to locate
CS, primarily in multilingual documents when lan-
guage diversity is unstructured. It uses a sliding
window and determines the local language of each
token. This method requires linguistic resources
such as word lists and monolingual corpora. Rijh-
wani et al. (2017) acknowledge the challenges in
building word-level LID for CS LID. They propose
an unsupervised word-level LID approach and ap-
ply it to estimate language pairs code-switched on
Twitter. Their findings indicate that approximately
3.5% of tweets were code-switched. Mager et al.
(2019) extend the LID task from the word level to
the subword level, involving the splitting of mixed
words and tagging each part with an LID. However,
training such LID models at the subword level re-
quires CS training data, which is not practical on a
larger scale.
B Confidence in MaskLID
We discuss here additional considerations regard-
ing the design MaskLID , notably aimed the keep-
ing a good balance between over and under de-
tection of labels, which is a key aspect to reliably
detect instances of CS.
A first comment is that in our approach, the
value of parameter αis kept constant. An exten-
sion would vary this value during iterations, de-
pending on the desired level of CS-sensitive results.
However, selecting a smaller αincreases the like-
lihood of a language being chosen again in the
next round(s). In such cases, the αvalue for the
next round should be increased so that more words
belonging to L1 are masked.
To ensure that MaskLID yields a low false posi-
tive rate (FPR), the feature set assigned to language
Luin step 2 should have a minimum length (inbyte) τ. If not, we should increase the βvalue and
repeat the process again to obtain a larger feature
set, and evaluate whether the confidence probabil-
ity prediction for this set is high. If not, terminate
the procedure. It is important to note that βdoes
not play a role in masking, as only αaffects this
process. The reason for defining both αandβin-
stead of relying solely on αis to ensure a minimum
byte size so that the probability prediction for this
feature set can be trusted and to guarantee its high
confidence. Typical αvalues should thus be lower
thanβand only target the features that strongly cue
language and should accordingly be masked.
Maintaining high confidence in steps 1 and 4
is more tricky; the reason for the low confidence
probability in these steps could be the presence
of another language. However, it could also be be-
cause the text is not among the languages supported
by the LID (Kargaran et al., 2023). We suggest us-
ing a low confidence threshold for these steps or
not using one at all.
Finally, our algorithm uses two termination con-
ditions, one based on the minimum sentence length
(τ) , one based on the maximum number of lan-
guages in a given sentence ( λ): 2 or 3 is recom-
mended. In our test dataset, we know in advance
that the number of languages is at most 2.
C Experimental Settings
C.1 The Label Sets of LIDs
Following the labeling proposed by NLLB Team
et al. (2022), our two baseline LIDs use language-
scripts as labels. They define a language-script as
a combination of a ISO 639-3 language code and a
ISO 15924 script code.
We constrain GlotLID to the set of languages
supported by OpenLID. Most of the labels sup-
ported by OpenLID are supported by GlotLID. The
total number of labels is 201 for OpenLID, and
we select 200 labels for the constrained version of
GlotLID. The only difference is due to the fact that
OpenLID uses two labels for the Chinese language
(zho), written in Hans and Hant scripts, whereas
GlotLID combines both under the label Hani. Also,
GlotLID does not support acq_Arab, nor does it not
support labels pes_Arab and prs_Arab individually
(as OpenLID does) but as the merged macrolan-
guage fas_Arab. To compensate for the lack of
these two labels and to also perform experiments
for Hindi and Nepali in romanized script, we add
hin_Latn and npi_Latn to the set of labels for con-

--- PAGE 10 ---
strained GlotLID.
To restrict a FastText -based LID model to a spe-
cific subset of languages, as indicated by Eq. (1),
we only need to consider the bcvalues for lan-
guages cthat are members of the chosen set of lan-
guages. This implies that languages not included
in this set will be excluded from the softmax com-
putation. Additionally, the rows belonging to these
languages are also deleted from the matrix V(s)
(Eq. (2)).
C.2 Hyperparameters
We here explain the hyperparameters specific to
each method.
MaskLID .We generated 12 small synthetic
code-switch corpora by combining sentence parts
from French, English, Arabic, and Persian lan-
guages, ensuring a presence of at least 30% from
each of the two languages participating in the fi-
nal sentence. Subsequently, we applied MaskLID
with different hyperparameters to achieve the best
results. The hyperparameters derived from this
method, which we used for the experiments in
this paper, are as follows: α= 3 ,β= 15 ,
λ= 2, and τ= 20 . Additionally, we employed a
high-confidence threshold of 0.9for OpenLID and
GlotLID to evaluate the probability predictions for
the feature set in step 2 of the algorithm, as further
detailed in Section B.
Baseline. Following Burchell et al. (2024), we
use a threshold of 0.3 to select languages (i.e.,
among all languages supported by the model, the
languages with confidence probability greater than
0.3 are selected). However, for a fairer comparison
(since λ= 2), we only consider the top two that
pass this threshold.
D Data Selection
The CS test sets available for consideration cover
a small potential language set (Jose et al., 2020;
Aguilar et al., 2020). Accessing suitable CS test
sets for evaluating our method poses several chal-
lenges:
1) Arabic dialects, such as Standard Arabic-
Egyptian Arabic, are represented in some CS
datasets (Elfardy et al., 2013; Aguilar et al., 2020).
However, none of the baseline LID models yield
impressive results for Arabic dialects. For instance,
according to Burchell et al. (2024, Table 3), Open-
LID exhibits the worst FPR for Standard Arabic
and Egyptian Arabic among all the languages itsupports.
2) Certain datasets present unrealistic scenarios
for testing our method. For example, Mandarin-
English datasets with Mandarin written in Hani
script and English in Latin script (Lovenia et al.,
2022). Methods employing script detection can sep-
arate perfectly Hani from Latin, and perform two
separate LID predictions.4This does not showcase
the advantages of MaskLID and the performance
only is dependent to the LID performance.
3) Many accessible datasets involve CS between
one language and English.
Given these challenges, we decided to use
datasets involving English in three sets (Turkish-
English, Hindi-English, Nepali-English) and an-
other set with CS between languages without En-
glish (Basque-Spanish). The Turkish-English and
Basque-Spanish datasets are also used by Burchell
et al. (2024). We use the code provided by these
authors to label them into sentence-level tags.
Turkish-English. Yirmibe¸ so ˘glu and Eryi ˘git
(2018) developed a Turkish–English dataset for CS
as part of their work on CS LID for this langauge
pair. The dataset is sourced from Twitter and the
Ek¸ si Sözlük online forum. Labels in this dataset are
assigned at the token level, indicating whether each
token is Turkish or English. The dataset comprises
376 lines of data, and 372 of these sentences are
labeled as CS. However, for our purposes, we also
require monolingual datasets in these languages,
not just CS data. To address this, we created a
monolingual version of the CS data for the Turkish
language by removing tokens labeled as English.
A similar approach cannot be applied to create an
English monolingual dataset, as the English parts
of the data are short sentences and would adversely
impact the quality of the English monolingual data.
The original dataset can be found here: github.
com/zeynepyirmibes/code-switching-tr-en .
Basque-Spanish. The Basque–Spanish cor-
pus (Aguirre et al., 2022) comprises Spanish and
Basque sentences sourced from a collection of text
samples used in training bilingual chatbots. V ol-
unteers were presented with these sentences and
tasked with providing a realistic alternative text
with the same meaning in Basque–Spanish CS. The
dataset consists of 2304 lines of data, with 1377
sentences labeled as CS, 449 as Basque, and 478
as Spanish. The original dataset is available at:
4For example, GlotScript (Kargaran et al., 2024) provides
aseparate_script function that divides text based on differ-
ent scripts: github.com/cisnlp/GlotScript .

--- PAGE 11 ---
github.com/Vicomtech/BaSCo-Corpus .
Hindi-English & Nepali-English. Aguilar et al.
(2020) provide a benchmark for linguistic CS eval-
uation, used in previous shared tasks on CS LID
(Solorio et al., 2014; Molina et al., 2016). We test
on two of its language pairs, Hindi– English and
Nepali-English, using the validation sets since the
test sets are private. These datasets are both sourced
from Twitter and are annotated at the word level.
The Hindi-English dataset has 739 lines: 322 CS,
31 Hindi, and 386 English sentences. The Nepali-
English dataset has 1332 lines: 943 CS, 217 Nepali,
and 172 English sentences. We consider both CS
and monolingual data for experiments.
Preprocessing Sentence-level LIDs may not per-
form well on very short sentences. In the corpus
creation pipelines using these LIDs, shorter sen-
tences are typically discarded. Therefore, we filter
sentences with a length of 20 byte or fewer for
monolingual sentences and sentences with a length
of 40 byte or fewer for CS sentences. The remain-
ing number of sentences (#S) for each portion of
the data is detailed in Table 1. In addition, we
clean user tags and emojis from the datasets before
applying LIDs.
E Examples
We showcase below some failed and successful
examples of MaskLID.
Failed Example. In this example, the only En-
glish word is “status”.
yarın bir status yapıp
işlerin üstünden geçelim
As we define the minimum length for each se-
lected language to be at least τ= 20 byte, this
sentence gets classified as Turkish, which is ac-
ceptable. If, otherwise, “status” would be evalu-
ated alone, OpenLID would predict “Norwegian
Nynorsk” language, and GlotLID “Kinyarwanda”.
This is the reason why τis important to be set be-
cause otherwise the result of LID cannot be trusted.
The average length of the English part of sentences
in the CS Turkish-English getting classified solely
as Turkish by GlotLID + MaskLID is 17.858 bytes
and by OpenLID + MaskLID is 19.877 bytes. So
the main reason for failing these models here is
the English part of this sentences is short and often
does not pass the minimum length condition.
Successful Example. In this example, “dead-
line crash walking I heard it at study” are theEnglish words inserted in the Turkish sentence.
These words are not next one to the other, so meth-
ods that only consider sliding windows might fail.
MaskLID does not depend on the position of words
in a sentence and correctly classify this example as
Turkish-English CS.
ya deadline gelmişti çok büyük
bir crash olmuş arkadaşlarla
walking yaparken I heard it at
boğaziçi sesli study
However, predicting it using solely based on
OpenLID results in the top 3 labels being “Turkish”,
“Turkmen”, and “North Azerbaijani”. The average
length of the English part of sentences from CS
Turkish-English getting classified correctly as CS
Turkish-English by GlotLID + MaskLID is 42.121
bytes and by OpenLID + MaskLID is 45.294 bytes.
