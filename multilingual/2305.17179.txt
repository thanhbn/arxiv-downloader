# 2305.17179.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2305.17179.pdf
# File size: 1204365 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Tokenization Impacts Multilingual Language Modeling:
Assessing Vocabulary Allocation and Overlap Across Languages
Tomasz Limisiewicz and Jiˇrí Balhar and David Mare ˇcek
Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics
Charles University, Prague, Czech Republic
{limisiewicz, marecek}@ufal.mff.cuni.cz
Abstract
Multilingual language models have recently
gained attention as a promising solution for rep-
resenting multiple languages in a single model.
In this paper, we propose new criteria to eval-
uate the quality of lexical representation and
vocabulary overlap observed in sub-word to-
kenizers. Our findings show that the overlap
of vocabulary across languages can be actu-
ally detrimental to certain downstream tasks
(POS, dependency tree labeling). In contrast,
NER and sentence-level tasks (cross-lingual
retrieval, NLI) benefit from sharing vocabu-
lary. We also observe that the coverage of
the language-specific tokens in the multilin-
gual vocabulary significantly impacts the word-
level tasks. Our study offers a deeper under-
standing of the role of tokenizers in multilin-
gual language models and guidelines for future
model developers to choose the most suitable
tokenizer for their specific application before
undertaking costly model pre-training.1
1 Introduction
Multilingual language models perform surprisingly
well in a variety of NLP tasks for diverse languages
(Devlin et al., 2019; Conneau and Lample, 2019;
Conneau et al., 2019). It has been observed that
the representation of the input sequence has a sig-
nificant effect on their effectiveness (Mielke et al.,
2021). In the widely used Transformer (Vaswani
et al., 2017) models achieving state-of-the-art re-
sults through diverse tasks, a large fraction of pa-
rameters are allocated in the input encoding layer.2
The popular language-independent approach to rep-
resent the input texts is to learn a vocabulary of fre-
quently appearing strings that may consist of words
or parts of words (Sennrich et al., 2016; Song et al.,
2021; Kudo and Richardson, 2018).
1The code is available at: github.com/tomlimi/
entangled_in_scripts .
2For instance, in XLM-Roberta Base, 192M out of 270M
parameters are in the input embedding layer (approximately
70%).
-1.0 0.0 1.0Vocabulary Allocation
-1.00.01.0Vocabulary OverlapMLMNER
POSDependency
LabelingNLISentence
Retrieval
Task type
Pre-training
Word level
Sentence levelFigure 1: Mapping the impact of vocabulary alloca-
tionandvocabulary overlap on language model perfor-
mance. The location of points corresponds to Spearm-
nan’s correlation between vocabulary measures and the
task score (see the details in Tables 3 and 5). High
vocabulary overlap benefits NER and sentence-level
tasks (NLI, sentence retrieval) and hinders POS and
dependency labeling performance. High vocabulary
allocation improves word-level tasks but leads to a de-
crease in masked language modeling scores. Masked
language modeling is measured only in language. Thus
it’s unaffected by vocabulary overlap . Analogically,
sentence retrieval is solely cross-lingual and unaffected
byvocabulary allocation .
In this work, we focus on the characteristics of
subword tokenization methods in a multilingual
setting. Our main contribution is the introduction
of the methods for measuring whether tokenizers
effectively represent meaningful language-specific
tokens in the vocabulary ( vocabulary allocation )
and whether the units they learn are shared across
languages ( vocabulary overlap ). We posit the fol-
lowing questions:arXiv:2305.17179v1  [cs.CL]  26 May 2023

--- PAGE 2 ---
(Q1) How do sub-word tokenizers differ in
overlap andallocation of learned vocabularies?
To answer this question, we apply the metrics to to-
kenizers obtained with two widely used algorithms:
SentencePiece Unigram LM (Kudo and Richard-
son, 2018), and BPE (Sennrich et al., 2016). Fur-
thermore, we propose two methods of learning tok-
enizers on monolingual corpora and then combin-
ing them to allow the tokenization of multilingual
texts.
(Q2) Which properties of multilingual tok-
enizers affect the LM’s representation quality?
We address this question by training small language
models utilizing different tokenization methods.
We evaluate the models on masked word predic-
tion and a diverse set of downstream tasks: POS,
NER tagging, dependency tree labeling, NLI, and
cross-lingual sentence retrieval.
The proposed evaluation scheme offers a good
prediction of language models’ performance. No-
tably, we show that the system results significantly
improve when tokenizers allocate more vocabu-
lary units for specific languages. Our investiga-
tion shows that this aspect has a bigger influence
than the vocabulary overlap for word-level tasks
(see Figure 1). To the best of our knowledge, the
interactions between multilingual vocabulary al-
location andvocabulary overlap have not been
investigated in past research.
2 Multilingual Subword Tokenization
The majority of the currently deployed models use
subword tokenization as a way to pre-process the
input texts. The input is represented as a sequence
of units from a finite vocabulary, which can be
translated into numeric representation by an input
embedding layer.
The benefits of subword tokenization are the
ability to obtain numeric representation for mean-
ingful words frequently used in the resources and
handling less frequent words by splitting them into
subwords. The latter property mitigates the prob-
lem of out-of-vocabulary (OOV) words by break-
ing them down into smaller parts (sub-words) al-
ready present in the vocabulary. It is crucial in
handling multilingual texts, especially in languages
with large vocabularies and complex morphology.
In the following section, we describe two widely
used algorithms of subword tokenization:2.1 Background: Subword Tokenization
Byte-pair encoding BPE: (Sennrich et al., 2016)
is a subword tokenization method that iteratively
replaces the most frequent pair of vocabulary units
in the input text with a single unit. The process
starts with taking unique characters of the training
text as the initial vocabulary. Subsequently, we take
the most frequent pair of vocabulary units, merge
the pair, and add it as a new unit to the vocabulary.
This process is repeated until a pre-set vocabulary
sizeNis reached.
Unigram LM: (Kudo, 2018) is the method of
obtaining subword vocabulary that was first in-
troduced as the underlying tokenizer of Senten-
cePiece algorithm (Kudo and Richardson, 2018).
The prerequisite is obtaining an extensive vocabu-
lary, e.g., consisting of all strings present in data
with at most, a predefined number of characters.
The expectation-maximization algorithm is used to
estimate the probability of vocabulary units. Af-
ter EM convergence, the portion of units with the
lowest contribution to the likelihood of the training
corpus is removed from the vocabulary. The proce-
dure is repeated until the pre-set vocabulary size is
obtained.
2.2 Combining Monolingual Tokenizers
Rust et al. (2021) observed that subword tokenizers
trained on monolingual data outperform multilin-
gual ones. The latter can overrepresent the sub-
words specific to languages constituting a large por-
tion of the training corpora (e.g., English). More-
over, their vocabulary is less likely to contain mor-
phemes important in modeling low-resource lan-
guages and instead prioritizes less meaningful char-
acter sequences appearing across languages.
To alleviate this issue, we suggest utilizing
monolingual tokenizers for multilingual tokeniza-
tion. First, the Unigram LM tokenizers are trained
on separate monolingual corpora. The tokenizers
are then combined to create a tokenizer suitable
for multilingual data. We propose two methods for
combining monolingual tokenizers:
Language-specific Tokenization NOOVER-
LAP:We train Unigram tokenizers for each of
Lconsidered languages with the same vocabulary
size for each of the languagesN
L. In multilingual
tokenization, we apply the tokenizer for a specific
language separately and produce a token with lan-
guage identification.3The vocabulary consists of L
3Only the special tokens are shared across languages, e.g.,

--- PAGE 3 ---
segments of total size N. Naturally, the tokenized
texts in different languages will consist of tokens
from distinct vocabulary segments. Noticeably, the
same character sequence in different languages can
be assigned different token ids.
Language-Mixed Tokenization TOKMIX:We
train Unigram LM tokenizers for each of Llan-
guages. Subsequently, we averaged vocabulary unit
probabilities across tokenizers, sorted them, and
trimmed the vocabulary to the pre-set vocabulary
sizeNkeeping the units with the highest probabil-
ity.4
ˆθ=LX
i=1wiθi (1)
wiare weights assigned to each language. By de-
fault, we set the weights to be uniform and equal
to1
L. Unlike NOOVERLAP , the same vocabulary
units coming from distinct monolingual tokenizers
are merged into one unit with averaged probability.
2.3 Tokenizer and Model Training Setting
We initially focused on a group of 6 languages vary-
ing both in the script and language family: Arabic,
Chinese, Greek, Turkish, Spanish, and English. In
subsequent experiments, we extend the method to
20 languages.
We download 10% of CC corpus available atv
https://data.statmt.org/cc-100/ . Follow-
ing the methodology in (Conneau and Lample,
2019), we subsample each language’s data to en-
sure that the training corpus is well-balanced across
languages. An equation defines the sample size cl
for language l:
cl,α=cmin·|Cl|
cminα
(2)
Where cminis the minimal sample size (defined
by the smallest language), and Clis all data avail-
able for a language, αis the so-called “balancing
parameter”. In our experiments, we set cminto 10
M characters, Clis, e.g., 8.8 B characters for En-
glish. We set αto0.25, which corresponds to a bal-
ancing factor picked for XLM-Roberta (Conneau
et al., 2019). The training data for the tokenizer
and the model are the same. The vocabulary size N
was set to 120,000. Appendix A contains technical
details about our approach.
“<s>” – the beginning of a sentence token.
4To account for possible overlaps between language-
specific vocabularies, we set their sizes aboveN
L. It assures
that joint vocabulary will have at least Ntokens.3 Measuring Tokenizer Properties
This section presents our in-depth analytical ap-
proach to evaluate different aspects of multilingual
tokenization. We introduce non-parametric mea-
sures that describe the key properties of multilin-
gual tokenizers: quality of vocabulary representa-
tion for particular languages and lexical overlap
across languages.
We base our analysis on the empirical probability
distribution of vocabulary units v∈ V computed
on training corpus for each language l:
dl,V(v) =f(v, Cl)P
v∈Vf(v, Cl)(3)
Function f(v, Cl)is the number of occurrences
of a vocabulary unit vin monolingual training cor-
pusCl.
3.1 Vocabulary Allocation
We aim to quantify how well multilingual vocabu-
lary represents meaningful lexical units of partic-
ular languages. Our intuition is that a good lexi-
cal representation is obtained when: 1. It uses a
vast portion of multilingual vocabulary, and thus a
larger part of the embedding layer is devoted to the
language; 2. The text in the language is split into
longer and potentially more meaningful tokens.
Vocabulary Allocation: Average Rank To mea-
sure the number of vocabulary units available for
modeling specific languages, we propose an es-
timation of the average rank of vocabulary units
in distribution over a monolingual corpus.5This
measure denotes how many tokens are typically
considered by a language model that has access to
language identity information but no context (prob-
abilistic unigram LM).
ARl,V=X
v∈Vrank( v, dl,V)dl,V(v) (4)
Our intuition is that model will have better infor-
mation about the language’s lexicon when vocabu-
lary is distributed over a larger number of tokens
as more parameters of the input embedding layer
would be allocated to represent language-specific
features. Moreover, larger vocabularies tend to
cover longer and more meaningful units.
5In this context, rank is the position of unit vin the vocab-
ularyVsorted in descending order by the probability distribu-
tiondl,V

--- PAGE 4 ---
Vocabulary Allocation: Characters per Token
In line with previous intuition, longer tokens have
a more meaningful representation. Therefore, we
measure text fragmentation by computing the aver-
age number of characters for a vocabulary unit in
monolingual corpus Cl.:
CPT l,V=|Cl|
|TV(Cl)|(5)
TV(Cl)is the tokenization of the corpus with vo-
cabulary V;|Cl|is the size of the corpus measured
as the number of characters. We choose the num-
ber of characters as the unit to relate to because
it’s not susceptible to cross-lingual differences re-
garding word boundaries and the average length of
words. Still, the amount of information conveyed
by a single character varies largely with the writing
systems, e.g., texts written in logographic scripts
(e.g., Chinese, Japanese) tend to be shorter in the
number of letters than similarly informative ones
in the phonetical script (e.g., Latin) (Perfetti and
Liu, 2005).
3.2 Vocabulary Overlap
Another important property of multilingual vocab-
ulary is sharing lexical units across languages. Pre-
vious works claimed that vocabulary overlap im-
proves cross-lingual transfer for learning down-
stream tasks (Pires et al., 2019; Wu and Dredze,
2019). We measure overlap as the divergence be-
tween corpora distributions dl(defined in equa-
tion 3). We use the Jensen-Shanon divergence.6
We apply JSD because it is symmetric and appli-
cable for distribution with different supports. The
latter is often the case when distributions are esti-
mated for languages with distinct writing systems.
JSD( dl1,V||dl2,V) =
=1
2X
v∈Vdl1,V(v) log2dl1,V(v)
ml1,l2,V(v)+
+1
2X
v∈Vdl2,V(v) log2dl2,V(v)
ml1,l2,V(v)(6)
where:
ml1,l2,V=1
2dl1,V+1
2dl2,V (7)
6In NLP literature, JSD is also known as “information
radius” (Manning and Schütze, 2001).JSD is bounded in the range 0to1. The lower
the value, the larger the overlap across corpora.
Another possibility to quantify overlap is to
count unique vocabulary units appearing in tok-
enized texts across languages. The advantage of
divergence is that it reflects the frequency of shared
tokens across corpora. It is also less affected by the
choice of the data size used for estimating empiri-
cal probability distributions ( dl).
4 Evaluating Language Modeling and
Downstream Tasks
In this section, we present the tasks and measures
for evaluation of multilingual language models
trained with different tokenizers.
4.1 Language Modeling
We evaluate the masked language modeling perfor-
mance with mean reciprocal rank:
MRR =1
NNX
i=11
rank( xi,ˆP(·|X\xi))(8)
where ˆP(·|X\xi)is the probability over vocab-
ulary of predicting token xiby the model given its
context: X\xi.
4.2 Downstream Evaluation
The downstream tasks are taken from the XTREME
(Hu et al., 2020), which is the collection of diverse
datasets with predefined splits used to evaluate mul-
tilingual models’ representation.
We probe the models’ output representation to
evaluate how useful the learned representation is
for the downstream tasks. Only an additional linear
layer is trained for the task, while the base model
representation is frozen. The approach is suitable
for evaluating how well the pre-trained model en-
codes linguistic phenomena as it does not change
parameters learned in pre-training in contrast to reg-
ular fine-tuning (Conneau et al., 2018a; Belinkov,
2022).
Word-level Tasks The first set of tasks covers
classification on a single word or word pair level.
The probe is a linear layer taking word represen-
tations on input and outputting one of the classes.
For word representations, we take the model’s out-
put embedding of the first subwords. We evaluate
the results with an F1 score averaged across classes
(macro-average).

--- PAGE 5 ---
ar tr zh el es en
ARUnigram 2129 2719 5919 2070 1439 1513
BPE 2972 3226 4294 2907 2220 2143
NoOverlap 2537 2653 2090 2065 1661 1597
TokMix 3485 4167 3961 2639 1999 1898
CPTUnigram 3.16 4.01 1.84 3.5 3.88 3.91
BPE 3.7 4.19 2.03 3.97 4.34 4.22
NoOverlap 3.53 4.19 1.56 3.81 4.15 4.15
TokMix 3.7 4.45 1.73 3.9 4.24 4.18
Table 1: Values of vocabulary allocation measures for
4 tokenizers trained on the small language set. The
highest values for each language are bolded.
We test syntactic tasks: Part of Speech and
Dependency labeling on Universal Dependencies
(de Marneffe et al., 2021) and Named Entity
Recognition on Wikiann dataset (Pan et al., 2017).
In dependency labeling, we use edge probe (Ten-
ney et al., 2019) on top of the representation of two
words connected by the dependency arc.
Sentence-level Tasks In this set of tasks, we ex-
amine whether the model learns sentence-level rep-
resentations that capture its semantics and can be
transferred across languages. To obtain this sen-
tence embedding, we average the model’s output
representation across all the tokens in the sentence.
We evaluate Natural Language Inference on
XNLI dataset (Conneau et al., 2018b) and Sentence
Retrieval on Tatoeba bitext corpus (Artetxe and
Schwenk, 2019). For NLI, we use edge probing.
Sentence retrieval is solved by an unsupervised
algorithm matching sentences based on their cosine
similarity. In Appendix A.3, we provide details of
the datasets and probe training.
4.2.1 In-language vs. Cross-lingual Transfer
For all the downstream tasks, except sentence re-
trieval, we compute in-language performance by
training the probe and evaluating it on held-out
test data in the same language. We quantify cross-
lingual transfer by training a probe on one language
(source) and evaluating it on the test set for another
language (target).
5 Experiments and Results
We train four tokenizers for the smaller set of
diverse 6 languages (en, es, tr, el, zh, ar) using
existing methods: Unigram, BPE, and our meth-
ods for monolingual tokenizer merging: NOOVER-
LAP,TOKMIX. Using these tokenizers, we then
train four models7following the settings of XLM-
7Details about the pretraining and probing procedures are
described in Appendix A.2
ar zh el tr es en0.81 0.77 0.79 0.79 0.79
0.81 0.78 0.75 0.77 0.74
0.77 0.78 0.75 0.75 0.74
0.79 0.75 0.75 0.62 0.64
0.79 0.77 0.75 0.62 0.61
0.79 0.74 0.74 0.64 0.61Unigram
0.84 0.83 0.82 0.83 0.83
0.84 0.81 0.77 0.78 0.78
0.83 0.81 0.77 0.78 0.78
0.82 0.77 0.77 0.65 0.67
0.83 0.78 0.78 0.65 0.65
0.83 0.78 0.78 0.67 0.65TokMix
ar zh el tr es enar zh el tr es en1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1NoOverlap
ar zh el tr es en0.87 0.86 0.85 0.86 0.85
0.87 0.84 0.80.83 0.8
0.86 0.84 0.80.82 0.8
0.85 0.8 0.8 0.68 0.69
0.86 0.83 0.82 0.68 0.68
0.85 0.8 0.80.69 0.68BPEFigure 2: Vocabulary overlap measure: Jensen-Shanon
divergence for four tokenization methods. Orange
square in the bottom right groups the languages with the
same script (Latin).
Roberta (Conneau et al., 2019) which we then use
for the probing experiments.
In Section 5.1, we analyze the distribution of
learned vocabulary units and compute vocabulary
allocation andvocabulary overlap measures de-
scribed in Section 3. Then in Section 5.2, we eval-
uate the models’ performance measures introduced
in Section 4 and compare them with the measures
for tokenizers.
Subsequently, we repeat the analysis for the
broader set of 20 diverse languages (including six
mentioned earlier and: he, ka, ur, hi, mr, th, ta, te,
bg, ru, sw, vi, fr, de) with three tokenization meth-
ods used in three pre-trained models. In this set-
ting, we do not use NOOVERLAP tokenizer, which
cannot be trained effectively due to the necessity
of constraining vocabulary for each language to
N
L= 6,000.
5.1 Evaluation of Tokenizers’ Properties
Vocabulary allocation largely varies through-
out languages and tokenization methods. Ta-
ble 1 shows that the average rank noticeably dif-
fers across languages. The highest AR is observed
for Chinese, which is caused by the fact that lo-
gographic scripts require an extensive vocabulary
capacity to encode all characters.
Multilingual vocabulary allocation is highly de-
pendent on the tokenization method used. V ocabu-
lary learned with Unigram underperforms BPE and

--- PAGE 6 ---
V . Allocation MLM NER POS Dep. labeling NLI
(AR) (CPT) (MRR) (F1) (F1) (F1) (Acc)
Unigram 2042 3.17 42.0 62.8 ±0.157.1±0.2 48.1±0.4 53.4±0.5
BPE 2193 4.47 35.6 70.4±0.168.9±0.2 58.7±0.4 53.3±0.3
NoOverlap 1829 3.16 42.7 69.4±0.169.2±0.2 58.8±0.3 53.0±0.4
TokMix 2198 3.34 38.7 70.2±0.167.3±0.1 57.3±0.4 53.3±0.4
(a) 6 languages
V . Allocation MLM NER POS Dep. labeling NLI
(AR) (CPT) (MRR) (F1) (F1) (F1) (Acc)
Unigram 623 2.89 52.6 58.9±0.254.0±0.4 43.7±0.4 53.2±0.3
BPE 809 3.43 40.5 66.3±0.267.3±0.4 54.5±0.5 53.5±0.3
TokMix 689 3.23 44.8 65.4 ±0.366.5±0.4 53.9±0.5 52.3±0.3
(b) 20 languages
Table 2: Avearged results of evaluation for in-language properties and tasks. Each probing result is an average of 5
random seeds (for 6 languages) and 3 random seeds (for 20 languages). The best value in each metric is underlined,
and bolded results are closer than the sum of standard deviations from the optimal value.
V . Allocation MLM
(AR) (CPT) (MRR)
CPT 0.790 - -
MRR -0.723 -0.913 -
NER 0.394 0.657 -0.745
POS 0.320 0.724 -0.754
Dep l. 0.266 0.675 -0.695
NLI 0.56 0.388 -0.437
Table 3: Spearman correlations between task coeffi-
cients for in-language results and tokenizer measures.
Statistically significant correlations ( p < 0.01) are
bolded. Computed for 20 languages.
TOKMIXin both average rank and character per to-
ken. Table 7 presented in the Appendix shows that
this trend exists throughout languages except for
Chinese. This suggests that our vanilla Unigram is
a suboptimal multilingual vocabulary learner.
It is important to note that NOOVERLAP scores
even lower than Unigram in the vocabulary alloca-
tionmeasures due to the limited vocabulary size for
each language and disallowing overlap. However,
as shown in the next sections, LM trained with this
tokenizer can achieve good results on some tasks.
The choice of tokenization method affects vo-
cabulary overlap .Figure 2 shows Jensen-Shanon
divergencies between the vocabularies of six lan-
guages. We observe that the highest cross-lingual
overlaps appear in the vocabulary obtained by Uni-
gram, followed by TOKMIX, and BPE. Expectedly,
we do not observe overlaps for NOOVERLAP ’s set-
ting ( JSD = 1 ).Jensen-Shanon divergence is a good predictor of
whether the languages share the script. For all tok-
enization methods, the divergence is significantly
smaller in the bottom-right square grouping of the
languages using Latin script. This effect is even
more visible in the visualization of JSD computed
for twenty languages (Figure 8 in Appendix C).
5.2 Tokenizer Properties Impact Language
Model’s Performance
High vocabulary allocation improves down-
stream results for word-level tasks. In Table 2a,
we observe that the choice of the tokenization
method significantly impacts the results for POS,
dependency labeling, and NER. We presume it re-
sults from learning good lexical representations
throughout languages, e.g., by BPE and TOKMIX.
The higher vocabulary allocation is especially ben-
eficial for word-level tasks. Whereas the influence
on the sentence-level task (NLI) is minimal.
Notably, the model instance with NOOVERLAP
tokenizer achieves the best F1 in POS and depen-
dency labeling despite underperforming in vocabu-
lary allocation . It is the result of learning language-
specific representation for tokens that is especially
useful for syntactic tasks.
Better MLM performance doesn’t bring im-
provement to downstream tasks. In Table 2a,
we observe that the models performing better on
masked token prediction (MRR) tend to be worse
on downstream tasks (POS and NER). It is the
result of different average ranks. The higher it
is, the more vocabulary units a language model
needs to consider for masked token filling, making

--- PAGE 7 ---
Different Same All
Metric Tokenizer script script transfers
Overlap
(JSD)Unigram 0.77 0.62 0.74
BPE 0.83 0.68 0.8
NoOverlap 1.0 1.0 1.0
TokMix 0.8 0.65 0.77
NER
(F1)Unigram 31.3 ±0.455.4±0.236.1±0.4
BPE 33.5±0.5 59.9±0.2 38.7±0.4
NoOverlap 32.0 ±0.548.6±0.435.3±0.5
TokMix 31.8 ±0.458.0±0.337.0±0.4
POS
(F1)Unigram 18.1 ±0.438.3±0.422.2±0.4
BPE 25.8±0.540.8±0.428.8±0.5
NoOverlap 20.1 ±0.541.9±0.524.5±0.5
TokMix 21.9 ±0.440.4±0.325.6±0.4
Dep. labeling
(F1)Unigram 11.1 ±0.325.5±0.314.0±0.3
BPE 15.9±0.427.0±0.418.1±0.4
NoOverlap 12.8 ±0.427.8±0.515.8±0.4
TokMix 12.6 ±0.526.1±0.315.3±0.5
NLI
(Acc)Unigram 42.2±0.743.7±0.742.5±0.7
BPE 42.4±0.745.2±0.8 43.0±0.7
NoOverlap 37.3 ±0.637.1±0.537.2±0.6
TokMix 41.2±0.742.7±0.541.5±0.7
Retrieval
(Acc)Unigram 21.0 43.9 25.6
BPE 20.9 40.7 24.9
NoOverlap 12.3 28.0 15.4
TokMix 23.0 43.4 27.1
(a) 6 languagesDifferent Same All
Tokenizer script script transf
Unigram 0.75 0.58 0.73
BPE 0.83 0.67 0.81
TokMix 0.8 0.64 0.78
Unigram 33.2 ±0.550.7±0.635.4±0.5
BPE 36.6±0.654.3±0.338.8±0.5
TokMix 36.5±0.653.7±0.538.7±0.6
Unigram 23.4 ±0.532.9±0.324.6±0.5
BPE 30.5±0.640.7±0.431.8±0.6
TokMix 29.2±0.540.4±0.330.7±0.5
Unigram 13.0 ±0.615.6±0.513.4±0.6
BPE 16.5±0.619.2±0.516.9±0.5
TokMix 16.0±0.519.4±0.416.5±0.5
Unigram 37.3±0.537.5±0.437.4±0.5
BPE 36.2 ±0.538.7±0.536.7±0.5
TokMix 37.8±0.539.2±0.538.1±0.5
Unigram 44.1 44.4 44.2
BPE 44.1 49.1 45.1
TokMix 42.8 46.9 43.6
(b) 20 languages
Table 4: Averaged results of the evaluation for cross-language overlaps and transfers. Each probing result is an
average of 5 random seeds (for 6 languages) and 3 random seeds (for 20 languages). The best value in each metric
is underlined, and bolded results are closer than the sum of standard deviations from the optimal value.
masked word prediction harder. At the same time,
a high average rank means that the vocabulary is
broader and contains lexical units important for
downstream tasks.
Again, this trend does not hold for the results for
NOOVERLAP setting, in which the search space
for the masked-word problem is limited to the
language-specific tokens leading to the best per-
formance in MLM and syntactic tasks (POS and
dependency label prediction).
In Table 3, we show that the strong relation-
ship between vocabulary allocation (avg. rank and
CPT) and LM performance (MRR) is statistically
supported. The length of token units has a strong
positive influence on POS, dependency labeling,
and NER results ( r >0.65) and a negative influ-
ence on MRR ( r <−0.9), while it does not signif-
icantly affect NLI results. The correlation between
the average rank and MRR, NER scores is weaker
but still significant. Moreover, it is significantly
correlated with XNLI accuracy with a medium co-
efficient r= 0.56, even though the changes in
XNLI are low across tokenizers.Impact of vocabulary overlap on cross-lingual
transfer varies across tasks. We observed that
NOOVERLAP approach obtains competitive results
for POS tagging . Surprisingly no vocabulary shar-
ing also improves cross-lingual transfer in the task
among languages with Latin script (shown in Ta-
ble 4a and Figure 3b). We think that the reason
behind the strength of NOOVERLAP approach is
that some tokens have different meanings across
languages, e.g., the word “a” is an indefinite article
in English and a preposition in Spanish.
Nevertheless, vocabulary overlap is crucial to
cross-lingual transfer in some tasks. Especially
NER within the same script languages (Figure 3a)
and sentence-level tasks. For these tasks, NOOVER-
LAPsignificantly underperforms other tokenization
methods. The drop within Latin script languages
is in the range: 6.8-11.3%for NER and 12.7-
15.9%for sentence retrieval. In these cases, usage
of the same tokens can indicate that texts refer to
the same entities across languages, e.g., names are
usually the same strings in the languages sharing
writing system.

--- PAGE 8 ---
ar zh el tr es en1543374036
23 22232323
3013 444340
341552 5453
34145056 55
3415495756Unigram
1845364336
24 24242425
3212 474942
331351 5854
29145059 57
3117476059TokMix
ar zh el tr es enar zh el tr es en1640343936
20 25212222
3716 445245
321944 4651
41185043 52
3618424950NoOverlap
ar zh el tr es en1844364235
24 26332732
3322 424641
342148 6256
37184760 58
3219476161BPE
−20−15−10−505101520(a) NER (F1)
ar zh el tr es en1115192923
7 4658
2410 234241
14 610 1922
38 92534 61
2813243559Unigram
1017263128
13 10131113
2713 264143
18 914 2125
40123037 63
3417293760TokMix
ar zh el tr es enar zh el tr es en823202621
11 9111213
2411 244543
17 814 2224
34103038 66
2612314061NoOverlap
ar zh el tr es en1322273429
17 15212024
2622 284441
191716 2224
42203139 63
3825293662BPE
−20−15−10−505101520 (b) POS (F1)
Figure 3: Cross-lingual transfer for POS and NER tasks. The absolute values are presented for the Unigram
tokenizer. For other tokenization methods, the color scheme shows a difference from the Unigram algorithm. In the
case of NER, we observe a drop in cross-lingual transfer for NOOVERLAP tokenization, especially for the same
script pairs, suggesting that lexical overlap is an important aspect contributing to cross-lingual transfer for NER. We
don’t see similar drop in the case of Part of Speech tagging.
V . Overlap V . Allocation SRC V . Allocation TGT
(JSD) (AR) (CPT) (AR) (CPT)
NER -0.111 0.249 0.33 0.209 0.28
POS 0.395 0.365 0.547 0.489 0.653
Dep l. 0.463 0.19 0.425 0.249 0.44
NLI -0.516 0.421 0.203 0.297 0.103
Retrieval -0.648 0.235 0.082 0.238 0.085
Table 5: Spearman correlations between cross-lingual transfer results and tokenization measures. vocabulary
overlap is measured by JSD, we also measure the correlation with vocabulary allocation s of source and target
language of the transfer directions. Statistically significant correlations ( p <0.01) are bolded. Computed for six
languages.
Table 5 presents the correlations for cross-
lingual transfer scores with JSD measuring vocabu-
lary overlap . The coefficient supports our previous
observation that lower overlap (thus higher JSD)
improves transfer for POS tagging and dependency
labeling and deteriorates it for other tasks. Al-
though, the correlation for NER is not significant.
Thevocabulary allocation s of source and target
languages significantly influence the cross-lingual
transfers. Similarly to the in-language correlations,
the influence of character per token is more sub-
stantial on word-level tasks, while Average Rank
affects sentence-level tasks to a larger extent. This
observation underlines the importance of allocatinga sufficient portion of vocabulary for low-resource
for better cross-lingual transfer.8
Results generalize to the larger set of languages.
The key observation for six language sets holds
in the model trained for twenty languages. Ta-
ble 2b shows that BPE and TOKMIXobtain bet-
tervocabulary allocation than Unigram leading to
improved results for word-level downstream tasks
(NER, POS, Dependency labeling). Due to the
smaller vocab size to the language number ratio,
average ranks decrease for all methods.
We observe in Table 4b that the cross-language
8We describe the correlation analysis in detail in Ap-
pendix C.3.

--- PAGE 9 ---
vocabulary overlap is the highest for Unigram and
lowest for BPE, similar to the six languages set-
tings. However, the association between vocabu-
lary overlap and the cross-lingual transfers is less
pronounced.
6 Related Work
Importance of vocabulary overlap .Wu and
Dredze (2019); Pires et al. (2019) claimed that mul-
tilingual overlap benefits cross-lingual transfer. In
contrast to this work, they compare overlaps for dif-
ferent language pairs with only one tokenizer. We
think that their observations may be confounded by
the typological similarity between languages. In
the following works, Conneau et al. (2020) found
that sharing parameters in top layers is more impor-
tant to multilingualism than same token embedding.
Similar results were demonstrated by Wang et al.
(2021); Dufter and Schütze (2020) who show that
in bilingual models, artificially removing vocabu-
lary overlap (similarly to ours NOOVERLAP ) does
not deteriorate cross-lingual transfer. In contrast
to many previous approaches, we used probing
for evaluation because this method offers better
insight into representation learned in pre-training.
Similarly, our results, Malkin et al. (2022); Lim-
isiewicz et al. (2022) observed that differences in
scripts could, in some cases, improve the cross-
lingual transfer in masked language modeling and
for downstream tasks.
Importance of vocabulary allocation .The ef-
fect of vocabulary allocation on model perfor-
mance was studied to a lower extent. Zheng et al.
(2021) observed that limited vocabulary capacity
allocated for specific languages impedes the down-
stream tasks’ performance and thus proposed a
method to obtain more balanced vocabulary alloca-
tionthroughout languages. For the same purpose,
Chung et al. (2020) proposed a novel approach to
generating multilingual vocabulary based on clus-
tering the target languages and merging separate
vocabularies. Recently, Liang et al. (2023) based
on the elements of both approaches and increased
vocabulary to train the XLM-V model, achieving
better results than its predecessor (XLM-Roberta
Conneau et al. (2019)).
In a monolingual setting, Bostrom and Durrett
(2020) argued that Unigram tokenization produces
subword tokens that are more aligned with mor-
phological units that bring improvement for down-
stream tasks. This contrasts with our finding ofUnigram’s underperformance when applied to a
multilingual corpus.
Improving multilingual sub-word tokenization.
Patil et al. (2022) proposed a modification to BPE
algorithm that increases overlap between similar
languages and benefits cross-lingual transfer. Rust
et al. (2021) observed that models with dedicated
monolingual tokenizers outperform multilingual
ones. This observation can be utilized by adapt-
ing the embedding layer of the model for a tar-
get language (Pfeiffer et al., 2020; Artetxe et al.,
2020; Minixhofer et al., 2022). However, these
approaches require language-specific modification
of the model, limiting its multilingual aspect.
Alternatives to sub-word tokenization. There
are multiple alternative approaches for inputting
text into deep models, such as character-based rep-
resentation (Clark et al., 2022), byte input (Xue
et al., 2022), or representing the input text as im-
ages (Salesky et al., 2021). Mielke et al. (2021)
summarize a wide range of methods and point out
that they offer trade-offs and may be better suited
for certain tasks or languages.
7 Conclusions
We introduced a new framework for the evalu-
ation of multilingual subword tokenizers. We
show that vocabulary allocation is a crucial as-
pect affecting the results of many downstream tasks.
Specifically, we have observed the following trends:
1. Including longer and more diverse vocabulary
units (higher vocabulary allocation ) improves in-
language results and cross-lingual transfers for
word-level tasks; 2. vocabulary overlap is ben-
eficial for cross-lingual transfer in sentence-level
tasks; 3. Among languages with the same script,
vocabulary overlap improves transfer for NER and
deteriorates it for POS and dependency labeling.
Our conclusions are in line with the observation of
Mielke et al. (2021) that there is no “silver bullet
solution” tokenizer suiting all purposes.
We release the code for measuring tokenizer
properties: github.com/tomlimi/entangled_
in_scripts . We believe that it will be a useful
evaluation tool for the developers of models who
can get a better insight into the tokenization method
before computationally expensive model training.

--- PAGE 10 ---
Limitations
To achieve robust, unbiased results, we decided to
train first on a smaller number of languages, fix our
methodology and then confirm our findings on the
full set of languages. This meant that two rounds
of pretraining needed to be done and because of
that, we scaled our models down for computational
efficiency reasons.
Another limitation of our methodology is the
choice to train linear probes on top of the contex-
tualized word representations instead of the more
common finetuning approach. Nevertheless, we
think that probing gives better insight into the pre-
trained model’s representation.
Ethics Statement
We do not identify ethical risks connected to this
work.
Acknowledgements
We thank Jind ˇrich Libovický, Martin Popel,
Gabriel Stanovsky, and anonymous ACL review-
ers for their valuable comments and suggestions
for improvement. This work has been supported
by grant 338521 of the Charles University Grant
Agency. We have been using language resources
and tools developed, stored, and distributed by the
LINDAT/CLARIAH-CZ project of the Ministry of
Education, Youth and Sports of the Czech Republic
(project LM2018101).
References
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020. On the Cross-lingual Transferability of Mono-
lingual Representations. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 4623–4637. ArXiv:1910.11856
[cs].
Mikel Artetxe and Holger Schwenk. 2019. Massively
Multilingual Sentence Embeddings for Zero-Shot
Cross-Lingual Transfer and Beyond. Transactions of
the Association for Computational Linguistics , 7:597–
610. ArXiv:1812.10464 [cs].
Yonatan Belinkov. 2022. Probing Classifiers: Promises,
Shortcomings, and Advances. Comput. Linguistics ,
48(1):207–219.
Kaj Bostrom and Greg Durrett. 2020. Byte Pair Encod-
ing is Suboptimal for Language Model Pretraining.
InFindings of the Association for Computational Lin-
guistics: EMNLP 2020, Online Event, 16-20 Novem-
ber 2020 , volume EMNLP 2020 of Findings of ACL ,pages 4617–4624. Association for Computational
Linguistics.
Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, and
Jason Riesa. 2020. Improving Multilingual Models
with Language-Clustered V ocabularies. In Proceed-
ings of the 2020 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 4536–4546. Associa-
tion for Computational Linguistics.
Jonathan H. Clark, Dan Garrette, Iulia Turc, and John
Wieting. 2022. Canine: Pre-training an Efficient
Tokenization-Free Encoder for Language Represen-
tation. Trans. Assoc. Comput. Linguistics , 10:73–91.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsuper-
vised Cross-lingual Representation Learning at Scale.
arXiv preprint arXiv:1911.02116 .
Alexis Conneau, German Kruszewski, Guillaume Lam-
ple, Loïc Barrault, and Marco Baroni. 2018a. What
You Can Cram Into a Single $&!#* Vector: Probing
Sentence Embeddings for Linguistic Properties. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 2126–2136, Melbourne, Aus-
tralia. Association for Computational Linguistics.
Alexis Conneau and Guillaume Lample. 2019. Cross-
lingual Language Model Pretraining. In Advances
in Neural Information Processing Systems 32: An-
nual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada , pages 7057–7067.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-
ina Williams, Samuel R. Bowman, Holger Schwenk,
and Veselin Stoyanov. 2018b. XNLI: Evaluating
Cross-lingual Sentence Representations. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, Brussels, Belgium,
October 31 - November 4, 2018 , pages 2475–2485.
Association for Computational Linguistics.
Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Emerging Cross-
lingual Structure in Pretrained Language Models. In
Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2020,
Online, July 5-10, 2020 , pages 6022–6034. Associa-
tion for Computational Linguistics.
Marie-Catherine de Marneffe, Christopher D. Manning,
Joakim Nivre, and Daniel Zeman. 2021. Universal
Dependencies. Comput. Linguistics , 47(2):255–308.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for

--- PAGE 11 ---
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Philipp Dufter and Hinrich Schütze. 2020. Identifying
Elements Essential for BERT’s Multilinguality. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 4423–4437, Online. Association for Computa-
tional Linguistics.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. XTREME: A Massively Multilingual Multi-
task Benchmark for Evaluating Cross-lingual Gener-
alization. CoRR , abs/2003.11080.
Taku Kudo. 2018. Subword Regularization: Improving
Neural Network Translation Models with Multiple
Subword Candidates. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July
15-20, 2018, Volume 1: Long Papers , pages 66–75.
Association for Computational Linguistics.
Taku Kudo and John Richardson. 2018. SentencePiece:
A Simple and Language Independent Subword Tok-
enizer and Detokenizer for Neural Text Processing.
InProceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.
H. W. Kuhn. 1955. The Hungarian Method for the As-
signment Problem. Naval Research Logistics Quar-
terly, 2(1-2):83–97.
Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Na-
man Goyal, Marjan Ghazvininejad, Luke Zettle-
moyer, and Madian Khabsa. 2023. XLM-V: Over-
coming the V ocabulary Bottleneck in Multilingual
Masked Language Models. CoRR , abs/2301.10472.
Tomasz Limisiewicz, Dan Malkin, and Gabriel
Stanovsky. 2022. You Can Have Your Data and Bal-
ance It Too: Towards Balanced and Efficient Multi-
lingual Models. CoRR , abs/2210.07135.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
Weight Decay Regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 . OpenRe-
view.net.
Dan Malkin, Tomasz Limisiewicz, and Gabriel
Stanovsky. 2022. A Balanced Data Approach for
Evaluating Cross-Lingual Transfer: Mapping the Lin-
guistic Blood Bank. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL 2022, Seattle, WA,
United States, July 10-15, 2022 , pages 4903–4915.
Association for Computational Linguistics.Christopher D. Manning and Hinrich Schütze. 2001.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky,
Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja,
Chenglei Si, Wilson Y . Lee, Benoît Sagot, and Sam-
son Tan. 2021. Between Words and Characters: A
Brief History of Open-V ocabulary Modeling and To-
kenization in NLP. ArXiv , abs/2112.10508.
Benjamin Minixhofer, Fabian Paischer, and Navid Rek-
absaz. 2022. WECHSEL: Effective Initialization of
Subword Embeddings for Cross-Lingual Transfer of
Monolingual Language Models. In Proceedings of
the 2022 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL 2022, Seattle,
WA, United States, July 10-15, 2022 , pages 3992–
4006. Association for Computational Linguistics.
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-
man, Kevin Knight, and Heng Ji. 2017. Cross-lingual
Name Tagging and Linking for 282 Languages. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2017,
Vancouver, Canada, July 30 - August 4, Volume 1:
Long Papers , pages 1946–1958. Association for Com-
putational Linguistics.
Vaidehi Patil, Partha P. Talukdar, and Sunita Sarawagi.
2022. Overlap-based V ocabulary Generation Im-
proves Cross-lingual Transfer Among Related Lan-
guages. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,
May 22-27, 2022 , pages 219–233. Association for
Computational Linguistics.
Charles A. Perfetti and Ying Liu. 2005. Orthography
to Phonology and Meaning: Comparisons Across
and Within Writing Systems. Reading and Writing ,
18(3):193–210.
Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Se-
bastian Ruder. 2020. MAD-X: An Adapter-Based
Framework for Multi-Task Cross-Lingual Transfer.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2020, Online, November 16-20, 2020 , pages 7654–
7673. Association for Computational Linguistics.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How Multilingual is Multilingual BERT? In Pro-
ceedings of the 57th Conference of the Association
for Computational Linguistics, ACL 2019, Florence,
Italy, July 28- August 2, 2019, Volume 1: Long Pa-
pers, pages 4996–5001. Association for Computa-
tional Linguistics.
Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-
sively Multilingual Transfer for NER. In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 151–164, Florence,
Italy. Association for Computational Linguistics.

--- PAGE 12 ---
Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder,
and Iryna Gurevych. 2021. How Good is Your Tok-
enizer? On the Monolingual Performance of Multi-
lingual Language Models. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing, ACL/IJCNLP
2021, (Volume 1: Long Papers), Virtual Event, Au-
gust 1-6, 2021 , pages 3118–3135. Association for
Computational Linguistics.
Elizabeth Salesky, David Etter, and Matt Post. 2021. Ro-
bust Open-V ocabulary Translation from Visual Text
Representations. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2021, Virtual Event / Punta Cana,
Dominican Republic, 7-11 November, 2021 , pages
7235–7252. Association for Computational Linguis-
tics.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1715–1725,
Berlin, Germany. Association for Computational Lin-
guistics.
Xinying Song, Alex Salcianu, Yang Song, Dave Dop-
son, and Denny Zhou. 2021. Fast WordPiece Tok-
enization. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Domini-
can Republic, 7-11 November, 2021 , pages 2089–
2103. Association for Computational Linguistics.
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R. Thomas McCoy, Najoung Kim, Ben-
jamin Van Durme, Samuel R. Bowman, Dipanjan
Das, and Ellie Pavlick. 2019. What Do You Learn
From Context? Probing for Sentence Structure In
Contextualized Word Representations. In 7th Inter-
national Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998–6008.
Xinyi Wang, Sebastian Ruder, and Graham Neubig.
2021. Multi-view Subword Regularization. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL-
HLT 2021, Online, June 6-11, 2021 , pages 473–482.
Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transform-
ers: State-of-the-Art Natural Language Processing.
InProceedings of the 2020 Conference on Empiri-
cal Methods in Natural Language Processing: Sys-
tem Demonstrations, EMNLP 2020 - Demos, Online,
November 16-20, 2020 , pages 38–45. Association for
Computational Linguistics.
Shijie Wu and Mark Dredze. 2019. Beto, Bentz, Becas:
The Surprising Cross-Lingual Effectiveness of BERT.
InProceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019 , pages 833–844.
Association for Computational Linguistics.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. 2022. ByT5: Towards a Token-
Free Future With Pre-Trained Byte-to-Byte Models.
ArXiv:2105.13626 [cs].
Bo Zheng, Li Dong, Shaohan Huang, Saksham Sing-
hal, Wanxiang Che, Ting Liu, Xia Song, and Furu
Wei. 2021. Allocating Large V ocabulary Capacity
for Cross-Lingual Language Model Pre-Training. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2021, Virtual Event / Punta Cana, Dominican Repub-
lic, 7-11 November, 2021 , pages 3203–3215. Associ-
ation for Computational Linguistics.

--- PAGE 13 ---
A Technical Details
A.1 Tokenizer training details
We use the Huggingface Tokenizers library for
training the Unigram and BPE tokenizers. We
kept the default values for the training parameters.
Namely, for Unigram, we use a maximum piece
length of 16 and a shrinking factor of 0.75. For
BPE, we use alphabet size 1000 and minimum
merge frequency 2. For all languages, we use
SentencePiece (Kudo and Richardson, 2018) for
word segmentation techniques instead of language-
specific word tokenizers.
A.2 Model Architecture and Pre-Training
In this study, we employed the Huggingface li-
brary (Wolf et al., 2020) to conduct all experiments.
The model architecture is based on XLM-Roberta,
although for our purposes, it was scaled down.
Specifically, the size of the embeddings is 768,
the number of attention layers is 8, and the number
of attention heads is 6. The maximum sentence
length is 128, and the vocabulary size is 120000.
The number of parameters is 150M and, therefore,
roughly 2 times smaller than the XLM-Roberta
base model.
The model was pre-trained for 10 epochs with a
batch size of 1024. The learning rate was 5e-5 with
linear decay and weight decay and 1% warm-up
steps. In pretraining, we used AdamW optimizer
(Loshchilov and Hutter, 2019).
In total, we pretrained 7 models. The models
were trained on 3 Nvidia GPUs. The probing ex-
periments were run on 1 Nvidia GPU with 40GB
of memory (Nvidia A40). The pretraining took
about 17 hours for each 6-language model and 60
hours for the models trained on the full set of 20
languages.
We didn’t pursue any extensive hyperparameter
search efforts as this was not the focus of our work.
We selected the best batch size and learning rates
for the pre-training based on a few trials.
A.3 Downstream Data and Training
The probes were for 30 epochs with early stopping
and batch size 16. We used an initial learning rate
of 2e-5. Other training parameters were the same as
in pretraining. Probing experiments took between
5 to 180 minutes to complete on the same infras-
tructure as used for pretraining. We ran around 360
probe trainings.POS We use Part of Speech annotations from
Universal Dependencies (de Marneffe et al., 2021).
The dataset is available for 17 languages analyzed
by us (not covered: Swahili, Thai, Georgian). Each
word is assigned one of the 17 coarse POS tags.
NER We use Wikiann dataset (Pan et al., 2017)
consisting of Wikipedias article with annotated
named entities of three types: location, person, and
organization in IOB2. Following XTREME, we
use balanced data splits from (Rahimi et al., 2019).
Dependency labeling As in Part of Speech, we
use Universal Dependencies (de Marneffe et al.,
2021) for the dependency relation annotations. We
use the largest UD treebank available for each lan-
guage. For each word we predict one of the 37
universal relations to its head word. Because the
relation is between two words, we use the concate-
nation of the two word representations along with
their element-wise product as an input to the probe
([hw1;hw2;hw1⊙hw2]).
NLI We use XNLI dataset (Conneau et al.,
2018b) for Natural Language Inference. We train
the linear classification probe on top of the concate-
nation of two sentence vectors and their element-
wise product: [hs1;hs2;hs1⊙hs2]. We predict
one of two relations between the first of sentences
(called premise): contradicts, entails, or is neutral
to the second sentence (called a hypothesis). We
evaluate XNLI with the accuracy of classification.
XNLI contains data for 15 languages (not cov-
ered: te, ta, mr, he, ka).
Sentence Retrieval We use up to 1,000 sentences
aligned for pairs of languages from Tatoeba dataset
(Artetxe and Schwenk, 2019). For the pairs in-
cluding English, we use the same sample as in
XTREME data collection. For other pairs, we per-
form sampling ourselves.
We compute the cosine similarity between sen-
tence representations across languages and find the
best alignment with the Hungarian algorithm(Kuhn,
1955). We compute the accuracy as the number of
correctly aligned sentences divided by the total
number of sentences.
B In-depth Tokenizers Analysis
In Figure 4, we present the probabilities of vo-
cabulary units, computed on concatenate six lan-
guages corpora, learned by different tokenization

--- PAGE 14 ---
0 20000 40000 60000 80000 100000 120000
Vocabulary Index10−710−510−310−1Log ProbabilityUnigram
BPE
TokMix
NoOverlapFigure 4: Log-probabilites of vocabulary units in de-
creasing order for four tokenization methods.
0 1 2 3 4 5 6 7
Vocabulary Size×1050.00.51.01.52.02.53.0Avg. Rank×104
ar
trzh
eles
enUnigram BPE
Figure 5: Avearage Rank measured for vocabularies of
different sizes, obtained with BPE and Unigram algo-
rithms.
algorithms. Unigram and NOOVERLAP use a big-
ger fraction of the vocabulary for rarely appearing
tokens (with probability lower than 10−6). BPE
andTOKMIXproduce a vast set of tokens with
probabilities in the range between 10−5and10−6.
Interestingly, the former algorithm allocates about
6000 vocabulary entries to tokens not appearing in
the corpora.
BPE is better than Unigram in vocabulary allo-
cation throughout languages. To support this
claim, we train Unigram and BPE tokenizers for
different vocabulary sizes. We observe that both
the average rank (Figure 5) and CPT (Figure 6)
stop rising for vocab sizes above 250,000 (except
for Chinese). For BPE, the metrics still steadily rise
after this threshold, which makes it overperform
Unigram for most languages.
We think that the reason why Unigram does not
learn valuable tokens after this point is the way the
0 1 2 3 4 5 6 7
Vocabulary Size×1051.52.02.53.03.54.04.55.0CPT
ar
trzh
eles
enUnigram BPEFigure 6: Characters per Token measured for vocabular-
ies of different sizes, obtained with BPE and Unigram
algorithms.
English Turkish Greek
Unigrams, ing, ed,
ly, d, Ifn, a, e,
k, s, iη, ς, ο ,
α, ή, ει
BPEthe, to, of,
and, If, ao, veyaim, im
inin, ası, esiη, ο, και,
ή, να, στον
Table 6: List of units from Unigram and BPE vocabu-
lary with the highest difference in frequency between
tokenizers. The first row shows the tokens that appear
more frequently in the corpus tokenized by Unigram
and the second by the BPE tokenizer. We excluded
punctuation marks and special characters from the list.
initial vocabulary is constructed, i.e., it is the set
of all character n-grams appearing in the corpus
with n lower than 16. In contrast to BPR, Uni-
gram’s vocabulary won’t cover longer words than
16 characters, which are useful in modeling some
languages.
We believe that further work on identifying op-
timal strategies for multilingual tokenization is
needed.
Vocabulary units preferred by tokenizers. In
Table 6, we show the tokens with the highest differ-
ences in empirical probabilities obtained with BPE
and Unigram tokenizers for three languages. We
see that Unigram prefers suffixes to prefixes. Also,
it splits text more often into single, possibly due to
lower vocabulary allocation .
C Supplementary Results
C.1 Visualizations
We present the additional visualization for the re-
sults for transfers across six languages for the tasks
not presented in the main text: Dependency label-
ing 7a and NLI cross-lingual accuracy 7b, Sentence

--- PAGE 15 ---
retrieval accuracy 7c.
The results of experiments for 20 languages:
Jensen-Shanon Divergences 8, and cross-lingual
transfers for POS 10a, NER 10b, dependency tree
labeling 10c, XNLI 9a, sentence alignment 9b.
C.2 Results for All Languages
We also include detailed results for the in-language
experiments along with the proposed tokenizer met-
rics. In Table 7, we present the results for the six
languages.
C.3 Correlation Analysis
We present paired correlation plots for in-language
metrics in Figure 11. We use the results from 20
language settings to increase the number of obser-
vations. In this analysis, we focus on the differ-
ences between the tokenization methods and want
to marginalize the language-specific features (such
as the pre-training and fine-tuning data size or the
model’s preference for Indo-European languages).
Therefore, for vocabulary allocation measures (AR,
CPT) and downstream tasks, we subtract the mean
for each language. For vocabulary overlap mea-
sure (JSD) and transfer values, we subtract the
mean value for each pair of languages. In both
cases, means are computed across all tokenizers.
We present Spearman’s correlation coefficient and
associated p-value.
ar zh el tr es en03233
0 6566
17 174128
3915 1616
3104420 37
59402143Unigram
13122
1 9898
25 194332
11218 1516
5105020 40
512462144TokMix
ar zh el tr es enar zh el tr es en01121
1 91010 8
29 194432
41118 1818
775020 41
69452347NoOverlap
ar zh el tr es en13122
6 15161816
615 214433
61419 1716
7174921 40
620442245BPE
−20−15−10−505101520(a) Dependency labeling
ar zh el tr es en4043383944
42 43394243
4241 384445
384144 4242
43394741 46
4445494348Unigram
3842374241
39 40343940
4238 364643
403739 4140
44424940 48
4546484148TokMix
ar zh el tr es enar zh el tr es en3638353839
33 34343437
3637 354043
353835 3634
35384035 43
3842413639NoOverlap
ar zh el tr es en4040424339
38 39374141
4439 424746
413843 4542
44435042 49
4646494450BPE
−20−15−10−505101520
(b) Natural Language Inference
ar zh el tr es en2420 92519
24 27 61017
2027 143842
9614 2531
25103825 76
1917423176Unigram
2616 91914
26 42 71523
1642 134449
9713 2232
19154422 76
1423493276TokMix
ar zh el tr es enar zh el tr es en15 8286
16 22 235
822 73434
227 910
8334 9 65
65341065NoOverlap
ar zh el tr es en2518 72312
25 31 51414
1831 154841
7515 2426
23144824 72
1214412672BPE
−20−15−10−505101520
(c) Sentence retrieval
Figure 7: The rest of the 6-language cross-lingual trans-
fer results. The absolute values are presented for the
Unigram tokenizer. For other tokenization methods, we
show the difference from the unigram algorithm.

--- PAGE 16 ---
ar tr zh el es en All
metric tokenizer
V . Allocation
(AR)Unigram 2129 2719 5919 2070 1439 1513 2042
BPE 2972 3226 4294 2907 2220 2143 2193
NoOverlap 2537 2653 2090 2065 1661 1597 1829
TokMix 3485 4167 3961 2639 1999 1898 2198
V . Allocation
(CPT)Unigram 3.16 4.01 1.84 3.5 3.88 3.91 3.17
BPE 3.7 4.19 2.03 3.97 4.34 4.22 4.47
NoOverlap 3.53 4.19 1.56 3.81 4.15 4.15 3.16
TokMix 3.7 4.45 1.73 3.9 4.24 4.18 3.34
MLM
(MRR)Unigram 36.0 36.0 34.2 46.3 49.7 49.6 42.0
BPE 28.7 33.6 28.6 38.6 43.1 41.0 35.6
NoOverlap 38.1 39.6 41.4 42.8 47.5 46.6 42.7
TokMix 31.5 30.6 38.2 41.2 45.3 45.6 38.7
NER
(F1)Unigram 66.4 ±0.173.0±0.135.1±0.168.0±0.168.0±0.166.1±0.262.8±0.1
BPE 76.1 ±0.076.7±0.054.2±0.170.3±0.175.2±0.170.0±0.070.4±0.1
NoOverlap 76.5 ±0.172.8±0.058.4±0.169.6±0.171.6±0.167.3±0.169.4±0.1
TokMix 76.6 ±0.176.2±0.156.1±0.070.1±0.174.3±0.168.1±0.170.2±0.1
POS
(F1)Unigram 54.8 ±0.146.9±0.229.3±0.152.9±0.376.5±0.281.9±0.157.1±0.2
BPE 66.7 ±0.152.1±0.162.2±0.063.4±0.181.7±0.487.4±0.168.9±0.2
NoOverlap 66.5 ±0.152.5±0.260.6±0.167.5±0.181.3±0.686.7±0.169.2±0.2
TokMix 66.0 ±0.152.1±0.256.2±0.061.7±0.281.3±0.286.3±0.167.3±0.1
Dep. labeling
(F1)Unigram 13.5 ±0.658.6±0.820.7±0.158.4±0.471.9±0.165.7±0.248.1±0.4
BPE 13.8 ±0.063.7±1.259.5±0.168.2±0.877.0±0.270.3±0.458.7±0.4
NoOverlap 13.2 ±0.065.0±0.560.5±0.267.7±0.277.1±0.369.2±0.358.8±0.3
TokMix 14.1 ±0.062.9±1.253.8±0.167.3±0.576.5±0.169.1±0.257.3±0.4
NLI
(Acc)Unigram 52.5 ±0.352.9±0.347.5±1.455.0±0.255.3±0.357.4±0.553.4±0.5
BPE 52.2 ±0.353.6±0.545.2±0.455.6±0.355.7±0.257.8±0.253.3±0.3
NoOverlap 52.9 ±0.754.0±0.244.0±0.854.8±0.154.9±0.357.3±0.353.0±0.4
TokMix 52.0 ±0.253.6±0.546.2±1.055.4±0.355.3±0.157.5±0.253.3±0.4
Table 7: Results of evaluation for in-language properties and tasks for six diverse languages. We observe significant
changes for different tokenization methods. The results for MRR, POS, NER, XNLI are in percent. For the
downstream task, we show average and standard deviations computed for five runs of probing.

--- PAGE 17 ---
zh he ka ar ur himr ta te th elru bg sw vi tr frde es enzhhekaar urhi mrtate thel ru bg swvitrfr dees en0.780.770.820.830.790.790.770.790.770.790.790.790.780.780.760.760.750.770.74
0.78 0.720.760.780.750.740.730.740.730.740.740.740.750.730.740.720.710.740.71
0.770.72 0.770.780.750.750.730.740.720.740.740.730.740.730.730.710.70.730.7
0.820.760.77 0.570.80.790.780.790.770.780.790.780.790.780.790.770.770.790.77
0.830.780.780.57 0.80.80.80.80.780.790.80.80.810.790.810.790.790.80.79
0.790.750.750.80.8 0.30.750.760.750.770.780.770.770.770.760.750.750.770.73
0.790.740.750.790.80.3 0.740.750.750.760.770.760.770.760.750.740.740.760.73
0.770.730.730.780.80.750.74 0.730.730.750.750.740.740.740.720.720.710.740.69
0.790.740.740.790.80.760.750.73 0.750.760.770.760.760.760.750.740.730.750.72
0.770.730.720.770.780.750.750.730.75 0.750.760.750.730.730.710.70.70.720.68
0.790.740.740.780.790.770.760.750.760.75 0.760.760.760.750.750.730.730.750.72
0.790.740.740.790.80.780.770.750.770.760.76 0.320.780.760.770.740.740.760.74
0.790.740.730.780.80.770.760.740.760.750.760.32 0.760.750.750.730.730.750.73
0.780.750.740.790.810.770.770.740.760.730.760.780.76 0.630.570.620.610.590.61
0.780.730.730.780.790.770.760.740.760.730.750.760.750.63 0.650.620.640.640.64
0.760.740.730.790.810.760.750.720.750.710.750.770.750.570.65 0.580.560.570.59
0.760.720.710.770.790.750.740.720.740.70.730.740.730.620.620.58 0.560.440.51
0.750.710.70.770.790.750.740.710.730.70.730.740.730.610.640.560.56 0.580.55
0.770.740.730.790.80.770.760.740.750.720.750.760.750.590.640.570.440.58 0.54
0.740.710.70.770.790.730.730.690.720.680.720.740.730.610.640.590.510.550.54Unigram
zh he ka ar ur himr ta te th elru bg sw vi tr frde es en0.820.80.860.870.820.820.80.810.790.820.820.810.810.790.780.790.790.790.78
0.82 0.780.840.860.810.80.780.790.80.810.80.790.810.790.780.790.780.790.78
0.80.78 0.830.860.80.790.770.790.780.790.780.770.790.780.760.760.760.770.76
0.860.840.83 0.660.850.850.830.840.830.850.850.840.860.840.840.840.840.840.84
0.870.860.860.66 0.860.860.840.850.850.870.870.870.870.860.860.860.860.870.86
0.820.810.80.850.86 0.410.780.790.810.820.820.810.810.80.780.790.790.80.78
0.820.80.790.850.860.41 0.770.780.80.820.810.810.810.80.780.790.790.80.77
0.80.780.770.830.840.780.77 0.760.780.790.790.780.780.770.750.760.760.770.74
0.810.790.790.840.850.790.780.76 0.790.810.810.80.80.790.770.780.780.790.77
0.790.80.780.830.850.810.80.780.79 0.80.810.790.790.780.750.760.760.770.75
0.820.810.790.850.870.820.820.790.810.8 0.810.80.810.810.790.790.790.790.79
0.820.80.780.850.870.820.810.790.810.810.81 0.30.820.810.80.80.80.80.8
0.810.790.770.840.870.810.810.780.80.790.80.3 0.810.790.780.780.780.780.78
0.810.810.790.860.870.810.810.780.80.790.810.820.81 0.750.630.680.670.650.67
0.790.790.780.840.860.80.80.770.790.780.810.810.790.75 0.720.720.730.730.72
0.780.780.760.840.860.780.780.750.770.750.790.80.780.630.72 0.610.590.590.62
0.790.790.760.840.860.790.790.760.780.760.790.80.780.680.720.61 0.60.470.56
0.790.780.760.840.860.790.790.760.780.760.790.80.780.670.730.590.6 0.620.59
0.790.790.770.840.870.80.80.770.790.770.790.80.780.650.730.590.470.62 0.57
0.780.780.760.840.860.780.770.740.770.750.790.80.780.670.720.620.560.590.57TokMix
zh he ka ar ur himr ta te th elru bg sw vi tr frde es en0.830.830.870.880.830.830.810.830.820.840.850.840.820.820.790.810.80.820.78
0.83 0.820.860.880.830.820.80.820.840.830.830.830.840.810.810.820.810.830.8
0.830.82 0.860.880.830.820.80.820.830.830.820.820.820.810.80.810.790.820.79
0.870.860.86 0.680.870.860.840.860.870.870.870.870.880.860.860.860.860.870.86
0.880.880.880.68 0.880.880.860.880.880.890.890.890.890.880.880.880.880.890.88
0.830.830.830.870.88 0.380.80.820.830.840.850.840.830.820.810.820.810.830.8
0.830.820.820.860.880.38 0.780.810.830.830.830.830.830.810.810.810.810.830.79
0.810.80.80.840.860.80.78 0.780.810.810.810.810.810.790.780.790.780.80.76
0.830.820.820.860.880.820.810.78 0.830.840.830.830.830.810.810.820.810.830.79
0.820.840.830.870.880.830.830.810.83 0.850.850.840.820.810.790.810.80.820.78
0.840.830.830.870.890.840.830.810.840.85 0.840.840.840.820.820.820.810.830.81
0.850.830.820.870.890.850.830.810.830.850.84 0.360.850.830.830.830.820.840.82
0.840.830.820.870.890.840.830.810.830.840.840.36 0.840.820.810.820.810.830.81
0.820.840.820.880.890.830.830.810.830.820.840.850.84 0.770.630.70.680.680.69
0.820.810.810.860.880.820.810.790.810.810.820.830.820.77 0.750.760.760.760.74
0.790.810.80.860.880.810.810.780.810.790.820.830.810.630.75 0.660.620.640.65
0.810.820.810.860.880.820.810.790.820.810.820.830.820.70.760.66 0.650.530.6
0.80.810.790.860.880.810.810.780.810.80.810.820.810.680.760.620.65 0.680.62
0.820.830.820.870.890.830.830.80.830.820.830.840.830.680.760.640.530.68 0.63
0.780.80.790.860.880.80.790.760.790.780.810.820.810.690.740.650.60.620.63BPEFigure 8: Jensen-Shanon divergence for three tokenization methods, computed on 20 languages.
zh ar ur hi th el ru bg sw vi tr fr de es enzh ar ur hi th el ru bg sw vi tr fr de es en37 33 34 40 37 39 36 34 36 34 35 36 37 39
37 35 37 38 39 42 40 34 36 34 42 40 40 43
39 35 41 36 36 39 37 34 35 34 37 35 36 37
37 34 35 34 35 39 34 33 34 34 34 36 37 35
38 36 34 34 34 35 35 33 34 34 34 35 35 36
37 40 36 34 36 45 40 33 37 34 44 37 41 43
40 40 35 38 35 44 44 33 35 34 42 38 43 43
42 41 35 38 37 44 48 35 36 36 42 43 43 44
38 38 34 37 38 35 36 35 34 34 37 36 37 39
38 39 35 36 38 40 42 37 33 33 41 35 38 37
40 35 35 38 35 37 39 36 33 36 39 37 39 38
35 34 33 34 34 36 42 36 34 35 34 35 38 40
40 38 33 37 37 35 43 42 34 35 35 42 41 44
37 39 34 34 37 39 43 39 34 34 34 39 38 40
43 38 35 37 40 41 47 40 34 39 35 43 45 44Unigram
zh ar ur hi th el ru bg sw vi tr fr de es en35 33 34 36 34 37 35 33 35 33 36 34 36 36
39 35 34 38 37 43 38 34 40 37 44 41 42 38
39 40 40 35 37 40 37 35 38 35 37 41 38 36
40 37 38 36 35 37 35 34 38 37 36 36 38 35
38 37 34 36 35 34 34 33 36 34 35 34 37 36
37 42 34 34 38 39 39 34 40 35 40 36 43 37
38 45 35 35 38 41 45 34 40 35 42 40 41 38
40 44 35 35 39 40 48 34 42 36 45 38 43 42
39 41 33 35 38 38 38 34 36 36 38 37 39 39
40 42 34 35 36 42 40 38 34 37 43 38 43 39
38 40 35 35 39 36 37 35 34 37 38 37 39 36
39 40 34 34 38 39 40 38 35 39 34 37 42 40
43 43 39 37 41 45 45 42 36 41 40 43 44 45
38 43 33 34 38 37 40 39 34 38 37 43 38 37
44 44 34 37 41 43 45 42 36 43 41 45 43 45TokMix
zh ar ur hi th el ru bg sw vi tr fr de es en35 34 34 35 37 36 37 34 36 33 34 34 35 36
35 36 34 35 39 37 40 33 35 34 37 35 37 37
34 43 42 35 39 35 37 33 35 34 34 37 37 35
34 37 37 34 36 37 37 33 37 34 35 36 36 34
37 39 34 34 37 34 35 33 34 34 35 34 37 34
34 38 35 35 34 38 40 33 36 33 38 37 40 36
34 35 34 34 34 36 44 33 37 34 38 36 40 37
37 37 34 36 36 40 48 35 35 34 39 37 37 37
39 38 36 36 34 36 37 35 38 36 37 36 38 40
37 37 34 38 38 37 36 37 34 34 42 37 42 38
35 34 34 35 36 39 37 36 34 36 40 36 36 38
35 37 34 36 34 40 40 41 34 35 37 41 41 40
39 43 35 37 35 42 41 42 34 41 37 43 45 42
35 34 34 34 34 39 37 37 34 36 37 41 38 41
42 38 36 37 36 41 39 40 34 42 36 41 44 46BPE
−20−15−10−505101520
(a) Natural Language Inference
zhhekaarurhimr tateth elrubgsw vitrfrde esenzhhekaar urhi mrtate thel ru bg swvitrfr dees en96715 7 5529815 15499813
9 24 10040100 1001034344339
67 48 100387175 35
1524 100 161673 1119142118
100 82 78 782559 14
7 82 77 78 4745715519
75 100 678810016
5921
2920
55 10010029383627
29100 17 4180 1740373543
84048167878100 41 67100192037484250
15100 73 8067 2658595450
100 38462645
15100100 100 19 3519242330
41038117847 1001720263835 21242029
9347119254167 29403758461921 577561
9347514507188 3837485826242457 5373
843 21 551005929363542544523207553 66
13393518141916212027435050 3029617366Unigram
zhhekaarurhimr tateth elrubgsw vitrfrde esen76021 15 46311020 27613111423
7 20 10032100 1001133283536
60 39 1002910064 27
2120 100 161182 914141717
100 85 67 1005853 17
14 85 77 59 5047575819
77 100 536210018
8222
2919
46 10010031303624
31100 15 3172 1233313136
103239116759100 31 6464191334414342
20100 82 7264 2255545552
86 50382750
27100100 100 18 4324262843
61127910050 1001213225043 21222327
133310014584753 31333455382421 557962
11286414535762 3031415428262255 5573
1435 17 581008229363143555028237955 72
23362717171918221924364252 4327627372TokMix
zhhekaarurhimr tateth elrubgsw vitrfrde esen74721 9 3129817 145129810
7 22 10036100 1001240364633
47 48 1003210069 25
2122 100 181582 919152215
100 88 67 1005856 13
9 88 75 50 4453466417
77 100 536910013
7620
1715
31 10010033403326
29100 18 4680 1752414238
83648156750100 46 7286211948505148
17100 82 8072 2767625353
86 44422550
14100100 100 21 5429263236
51234910044 1001719274454 23262625
124010019585353 33524866422923 678464
9366915564669 4041506225262667 6276
846 22 641007617334251535032268462 70
10332515131713201526384853 3625647670BPE
−20−15−10−505101520
(b) Sentence Retrieval
Figure 9: Cross-lingual transfer for the sentence-level tasks for 20 languages. The absolute values are presented for
the Unigram tokenizer. For other tokenization methods, we show the difference from the unigram algorithm.

--- PAGE 18 ---
zh he ar ur hi mr ta te el ru bg vi tr fr de es enzh he ar ur hi mr ta te el ru bg vi tr fr de es en7544779464565545
6 352830332327173643172735324037
924 1622161416132926192022172521
52524 58353533133542143225323030
4272263 383939163846173826333136
26589 1215 588713 710 910
27791113 14 6910 814 710 99
3858111916 699716 9101112
928232325232324 3443162733363735
123438394736293819 54184142455051
6363141423728322653 203538414749
311111113121515 81314 1310121212
52016262729333711262914 18222024
11413624352426332248462041 486156
1131233542373230214455183539 4642
74336334033273422505121405447 57
10363133433627312348532038505154Unigram
zh he ar ur hi mr ta te el ru bg vi tr fr de es en1513 912122017121614141312131312
12 433936353231244351333441404345
1136 2228192114203432272527283027
113327 68464541183645254029403139
12312680 474840204352254431423341
310 71213 1316 610101014 911 911
31010141515 15 611121114 9131111
5131013142118 81213121712141314
1841293129312822 4252282938464446
214544414953362830 62304452555858
12443749453939313761 323845545555
522191920141918112023 1717202020
92622373539393814293222 21252427
14543935393835273058573245 567066
1339315047433529345564304046 5452
134741404239362929575932435958 67
16403339464737392857593142576264TokMix
zh he ar ur hi mr ta te el ru bg vi tr fr de es en23151821182217102323151717191820
21 454039353231264451323444444646
1235 2428192216213533262529293328
183031 68484643183947234031413435
15322975 454742204352264232433741
610 81414 1418 91012 915 912 910
71111151417 14 811121013 9131010
6111015142122 101212111611141113
1537313232313324 4757283242504746
224246465244393830 65284252545459
20454154474545383361 303948555555
1124221919141819142124 1719192221
142823383437424118303321 22262426
18473838463537343358583048 587067
1640325049423834335564273946 5652
165242404235342930575831426257 65
22413742473841343058613141606064BPE
−20−15−10−505101520(a) Part of Speech Tagging
zhhekaarurhimr tateth elrubgsw vitrfrde esenzhhekaar urhi mrtate thel ru bg swvitrfr dees en19181616232120181117201821242019181822
21 454526474640411348455536384141414039
1737 3430454741411140435231323936353735
144137 26464038371145425340413843424338
16263331 413736371231323534313631303230
1332373222 5245441138384341424344414241
183437303250 45461137384136384441404040
17353733215252 491142424934344035363434
121516139191916 1117181915171817181617
191315141017141515 14151414201416121614
12384131254037383311 465640354442454041
1336443318403739371149 6139343841423841
134148342343444038115552 44435352534951
12202117102322191811252326 262629272928
1439453823484441411147445457 5459495456
163952313145494445115344585251 56565754
14414839245043424111524959555457 566256
1641474127484543421154515955506058 5757
184147382247444440115249605552576556 55
17414937244846444311544660605659636058Unigram
zhhekaarurhimr tateth elrubgsw vitrfrde esen25272419332827261528273028323131282932
27 524628554543411450445639434539444340
2645 3836534850481441415241384539414140
204846 39534438381546425044494045444340
24304036 504645411634343635373934363237
2035413734 5547481138354243434642423943
203744364057 48521139354442404941444144
28384734346055 571337324436374230353133
1217171815221921 1119172017182017191817
272123231628222422 25232523252024182219
16454739324536393611 465744424745464544
1541463432444040331148 6341384040434141
204754433553454240135352 52515655555354
12222321162522202011262328 283033303131
1541494637514241361447405464 5559515560
154455383854484246115142576051 57565856
12445043335243413611514558625857 566757
1748534837554744471153476059526260 6059
164552403451464236135243596158606656 57
15445340375444414111504158666062626160TokMix
zhhekaarurhimr tateth elrubgsw vitrfrde esen25252325343028271428232526272825262727
21 554744514644401547505746424943444643
1846 3937525047441744455537354035343635
154845 40504339411546455243444146444641
23354236 494742391638373844364336373839
1834443633 5344461440364147424242423743
163746374158 46491339384443404741413839
21395036396055 531744384543404237373433
1419201517212421 1019202317172019181716
232022201624222221 20192119221919162119
20455343394541384018 516145394843464745
1838493632423837361347 6341383940423940
194858433650434539175353 54485454545353
15222519152322211912242326 302832273230
1638464029503839371544425466 5559515559
204757374352504643165246615950 58606057
15465542314840403712524760645558 596858
1947554741514341411455496163506261 6160
174451364245414137145247616457606659 58
17445640355142413814524660675962636360BPE
−20−15−10−505101520
(b) Named Entity Recognition
zh he ar ur hi mr ta te el ru bg vi tr fr de es enzh he ar ur hi mr ta te el ru bg vi tr fr de es en5133465455552644
7 313181811162728271817 3242923
13 23322222227233
310 1 602219211619171320 2191614
413 151 2524271820211522 2211817
13257 69455562554
342912 9 13 676591755
24267912 567610 2766
420 11519151516 33361519 1333526
515 1161915141423 331417 2232518
720 220231917213643 1922 2343831
310 21214121010131515 12 2151413
812 1192225242919192013 2211716
2564464765655 665
412 217191614152522241217 2 2419
824 217211915174041431922 237 35
821 220222019243937381923 23439Unigram
zh he ar ur hi mr ta te el ru bg vi tr fr de es en705688889710 70997
11 117201716163634332817 3293628
43 22222222316222
713 0 702528262021211920 1271618
815 064 2729302123222022 1262019
251810 812 666891756
571151912 1611 991113 111 78
5619111315 8881113 1988
828 01919132015 42432518 2414433
920 0192115191733 382117 1313223
924 123241519194549 2722 2414235
816 21416121516182021 14 2181918
1215 2222426303119212019 2221818
6876567910 8897 887
716 026252119163531302019 1 3125
1129 120241820185147473123 346 42
1226 123242221214544462925 14446TokMix
zh he ar ur hi mr ta te el ru bg vi tr fr de es en12 112111012 81214131613 1151211
11 620211817163434322918 6323527
12 23121222217221
714 1 682127242121211822 1271917
1016 466 2729272224242125 3282220
372911 1012 878810 1867
583151710 1510 910 913 213 87
57210 91115 9881013 2877
927 42121141917 40452321 4424332
920 3202114171732 402119 3303324
1222 423251720204650 2722 4414534
915 41516111414171920 15 4191918
1015 4232524273121222117 4232017
1563344354454 654
716 324231719143530311919 3 3124
1230 624241919195146492923 647 42
1624 626261722224845462924 54247BPE
−20−15−10−505101520
(c) Dependency labeling
Figure 10: Cross-lingual transfer for the token-level tasks on 20 languages. The absolute values are presented for
the Unigram tokenizer. For other tokenization methods, we show the difference from the unigram algorithm.

--- PAGE 19 ---
−50005001000Avg. Rankr= 0.79p= 0.000r= -0.72p= 0.000r= 0.39p= 0.002r= 0.32p= 0.022r= 0.27p= 0.059r= 0.56p= 0.000
−0.50.00.5CPTr= -0.91p= 0.000r= 0.66p= 0.000r= 0.72p= 0.000r= 0.67p= 0.000r= 0.39p= 0.015
−0.10−0.050.000.050.10MRRr= -0.74p= 0.000r= -0.75p= 0.000r= -0.70p= 0.000r= -0.44p= 0.005
−0.10.0NERr= 0.83p= 0.000r= 0.72p= 0.000r= 0.08p= 0.627
−0.2−0.10.00.1POSr= 0.87p= 0.000r= 0.16p= 0.360
−0.2−0.10.00.1UDr= 0.06p= 0.724
−1000 0 1000
Avg. Rank−0.04−0.020.000.02XNLI
−1 0
CPT−0.1 0.0 0.1
MRR−0.2 0.0
NER−0.2 0.0 0.2
POS−0.25 0.00
UD−0.05 0.00 0.05
XNLItokenizer
BPE
TokMix
UnigramFigure 11: Correlation analysis for pairs of factors: vocabulary overlap metrics, language modeling performance
(MRR), and downstream tasks. The diagonal of the figure presents the density of distribution of each feature. The
results are grouped by the type of tokenizer applied. Analysis was done in 20 language setting. In the top right
corner of each sub-plot, we show Spearman correlation coefficient and associated p-value.
