# 2310.08166.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2310.08166.pdf
# File size: 5428016 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Ziya-VL Technical Report
ZIYA-VISUAL : B ILINGUAL LARGE VISION -LANGUAGE MODEL
VIAMULTI -TASK INSTRUCTION TUNING
Junyu Lu♥∗Dixiang Zhang♥♦∗Xiaojun Wu♥∗Xinyu Gao♥
Ruyi Gan♥♣†Jiaxing Zhang♥Yan Song♣Pingjian Zhang♦
♥International Digital Economy Academy♣University of Science and Technology of China
♦South China University of Technology
{lujunyu, zhangdixiang, wuxiaojun, ganruyi, zhangjiaxing }@idea.edu.cn
clksong@gmail.com pjzhang@scut.edu.cn
ABSTRACT
Recent advancements enlarge the capabilities of large language models (LLMs) in
zero-shot image-to-text generation and understanding by integrating multi-modal
inputs. However, such success is typically limited to English scenarios due to the
lack of large-scale and high-quality non-English multi-modal resources, making it
extremely difficult to establish competitive counterparts in other languages. In this
paper, we introduce the Ziya-Visual series, a set of bilingual large-scale vision-
language models (LVLMs) designed to incorporate visual semantics into LLM for
multi-modal dialogue. Composed of Ziya-Visual-Base and Ziya-Visual-Chat, our
models adopt the Querying Transformer from BLIP-2, further exploring the as-
sistance of optimization schemes such as instruction tuning, multi-stage training
and low-rank adaptation module for visual-language alignment. In addition, we
stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating
our gathered English image-text datasets into Chinese and generating instruction-
response through the in-context learning method. The experiment results demon-
strate that compared to the existing LVLMs, Ziya-Visual achieves competitive
performance across a wide range of English-only tasks including zero-shot image-
text retrieval, image captioning, and visual question answering. The evalua-
tion leaderboard accessed by GPT-4 also indicates that our models possess sat-
isfactory image-text understanding and generation capabilities in Chinese multi-
modal scenario dialogues. Code, demo and models are available at https:
//huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1 .
1 I NTRODUCTION
Large language models (LLMs) such as GPT3 Brown et al. (2020), LLaMA Touvron et al. (2023)
and Vicuna Chiang et al. (2023) have attracted widespread attention due to their powerful text gen-
eration and understanding capabilities. These models can demonstrate powerful interactive capabil-
ities by further learning user intentions in carefully designed instruction tuning datasets Wei et al.
(2021). Recently, prominent large-scale vision-language models (LVLMs), such as BLIP-2 Li et al.
(2023c), LLaV A Liu et al. (2023a), and mPLUG-Owl Ye et al. (2023) have been developed to ex-
plore the potential of LLMs in perceiving and understanding visual signals, and they demonstrated
impressive capabilities in solving real-world multi-modal dialogues and reasoning.
However, the success of large-scale vision-language models is primarily achieved within the English
community, as a large amount of image-text captioning datasets, such as Laion Schuhmann et al.
(2022), CC12M Changpinyo et al. (2021b) and SBU Ordonez et al. (2011b), can be used to align
textual and visual representations. Furthermore, mainstream studies attempt to uniformly convert
supervised multi-modal data from several task categories into instruction tuning formats, such as
*Equal Contribution.
†Project Leader.
1arXiv:2310.08166v3  [cs.CL]  31 Oct 2023

--- PAGE 2 ---
Ziya-VL Technical Report
MULTIINSTRUCT Xu et al. (2022) and MIMIC-IT Li et al. (2023a), which comprehensively bridge
the heterogeneity of multi-modal tasks across various scenarios and types. As a result, the data
resource gap has severely hindered the development of non-English multi-modal models.
To promote the vigorous development of the multi-modal open-source community, we introduce
the open-source Ziya-Visual series. The bilingual Ziya-Visual models include Ziya-Visual-Base and
Ziya-Visual-Chat versions, both of which expand the Ziya-LLaMA-13B language model through
the Q-Former and visual encoder, endowing it with visual comprehension capabilities. Specifically,
Ziya-Visual-Base continues the breakpoints of BLIP-2 Li et al. (2023c), aligning vision-language
representations under the premise of freezing the LLM and visual encoder. After the pre-training
stage and two-stage instruction tuning, it uses a spot of image captioning and instruction-tuning data
to stimulate the LLM’s understanding and generation capabilities of visual information. Addition-
ally, Ziya-Visual-Chat is an interactive vision-language model based on the pre-trained instructBLIP
framework Dai et al. (2023), which adapts several training strategies, including instruction tuning
and low-rank adaptation, to improve bilingual alignment. After multi-stage pre-training and in-
struction tuning, it can more flexibly extract informative features from images and instructions, and
supports complex multi-modal scenarios such as multi-turn dialogues, situational question answer-
ing, logical reasoning. To achieve this, we leverage substantial multi-modal English resource as a
pivot to present a Bilingual Multi-Modal In-Context (BMMIC) dataset, consisting of over 5 milion
image-text pairs, which utilizes GPT-4 OpenAI (2023) for automated translation and generation of
Chinese vision-language question-answer pairs. Specifically, the features of the Ziya-Visual series
models include:
•Competitive Performance: It is on par with the state-of-the-art monolingual open-source
Large Vision Language Models under the same-level model scale on several evaluation
benchmarks, including zero-shot image captioning, visual question answering and visual
reasoning.
•Bilingual capability supporting text recognition and visual reasoning: The Ziya-Visual nat-
urally supports multi-modal dialogue in both English and Chinese scenarios. We have
explored the impact of various training strategies on model performance, serving as a valu-
able reference for fellow researchers.
•Open-Source bilingual multi-modal data: We provide a multi-modal in-context instruction-
response dataset encompassing various real-world dialogue scenarios, along with an au-
tomation annotation pipeline for translation and generation.
2 R ELATED WORK
2.1 M ULTI -MODAL LARGE LANGUAGE MODEL
Due to the scaling up of training data and model size, the expansion of large language models to
multi-modal learning has attracted widespread attention. Flamingo Alayrac et al. (2022), LLaMA-
Adapter Zhang et al. (2023a); Gao et al. (2023) and Otter Li et al. (2023b) integrate learnable cross-
attention layers into the pre-trained LLMs to perceive visual knowledge, and train on a large-scaled
interleaved image-text dataset. To more efficiently and effectively boost vision-and-language pre-
training, BLIP-2 Li et al. (2023c) uses a Q-Former as perceivers to align queried visual feature
with text, which pre-trains the representation learning via multiple vision-language losses. Instruct-
BLIP Dai et al. (2023) further proposes instruction-aware visual feature extraction, which enables
flexible and informative feature extraction according to the given instructions. Additionally, some
methods transmit the features extracted from visual encoder to the LLM input through linear projec-
tion, such as LLaV A Liu et al. (2023a) and Shikra Chen et al. (2023), which utilize visual information
in a more direct and intuitive way.
2.2 M ULTI -MODAL INSTRUCTION TUNING DATASET
To explore instruction tuning for multi-modal learning, MULTIINSTRUCT Xu et al. (2022) first
proposes a multi-modal instruction tuning benchmark dataset, converting 62 different multi-modal
tasks into a unified seq-to-seq format. Due to GPT4’s demonstrated strong understanding of multi-
modal textual representations, LLaV A Liu et al. (2023a) introduces a data reformation perspective
2

--- PAGE 3 ---
Ziya-VL Technical Report
and pipeline to convert image-text pairs into the appropriate instruction-following format by utilizing
ChatGPT/GPT-4 OpenAI (2023). Furthermore, MIMIC-IT Li et al. (2023a) specializes the gener-
ation of instruction-response pairs through multi-modal in-context information and diverse visual
scenes. M3IT Li et al. (2023d) converts classical vision-language tasks into a unified vision-to-
text schema via manual instruction writing and dataset pre-processing, including captioning, visual
question answering, visual conditioned generation, reasoning and classification.
3 M ETHODOLOGY
3.1 I NSTRUCTION -RESPONSE CONSTRUCTION
Figure 1: The overview of our bilingual instruction-response pairs construction. We use human
annotation and a warm-up stage with GPT-4 OpenAI (2023) to generate system messages and in-
context examples. Subsequently, we leverage GPT-4’s multi-modal understanding capability to gen-
erate instruction-response pairs and the pre-prepared filters to select high-quality data.
In this section, We aim to build a bilingual multi-modal in-context (BMMIC) dataset to support
more LVLMs in enhancing the ability to comprehend the real-world dialogue. As illustrated in Fig-
ure 1, we provide an overview of the BMMIC dataset, including the data preparation and automated
instruction-response generation/translation pipeline.
The community has witnessed the emergence of publicly available multi-modal data such as image-
text pairs, encompassing referential question answering, contextual dialogues and caption descrip-
tions about images. However, the availability of multi-modal instruction-following data is limited,
primarily because the annotation process is time-consuming and annotation targets are less well-
defined. Taking inspiration from the impressive performance of GPT-4 in multi-modal associa-
tion and in-context learning, we select the COCO Lin et al. (2014), Flickr30k Young et al. (2014)
and AI Challenger 2017 Wu et al. (2019) datasets for multi-modal generation and the LLaV A Liu
et al. (2023a), M3IT Li et al. (2023d) datasets for multi-modal translation through an instruction-
following approach. We mainly consider the design of system messages and in-context learning Liu
et al. (2023a); Dong et al. (2022) when invoking the GPT-4 interface. Further, to mitigate the occur-
rence of erroneous instruction-response pairs due to cognitive bias in GPT-4, we develop a rule-based
detector to filter out data that are out of expectations.
Input Format: We obtain two types of symbolic representations from annotated multi-modal
datasets: (1) Captions typically describe the visual scene from various perspectives. (2) Bounding
boxes localize visual objects within visual scene, with each bounding box encoding the semantics
and spatial position of the object. Here, we leverage the Grounded-SAM pipeline, consisting of the
Grounding-DINO Liu et al. (2023b), SAM Kirillov et al. (2023) and RAM Zhang et al. (2023b)
modules, to extract the fine-grained visual feature from images which are further transformed into
boundary boxes and visual tags as symbolic representations.
3

--- PAGE 4 ---
Ziya-VL Technical Report
System Messages: We customize the GPT-4 role for instruction generation and translation based
on the composition of instruction types and rewriting requirements, including input format, response
types, style constraints and expansion paths. Following LLaV A, we denote the Conversation ,Detail
description andComplex reasoning to generate instruction data and Translation to translate available
English instruction-response pairs. We further provide Deepening (increase the depth and breadth
of the instruction-reponse pairs), Concretizing (complex the instruction-reponse pairs by replacing
general concepts with more specific concepts, Increasing Reasoning (rewrite the responses to ex-
plicitly request multiple-step reasoning) and Adding Constraints (constraining and specializing the
format and content of instruction-response pairs), four rewriting schemes to improve the diversity
and richness of instructions and responses.
In-Context Learning: For the p-th image, we take several associated captions XC
qalong with
tagged bounding boxes XB
q, and manually annotate 50 instruction-response pairs as query examples,
each of which is denoted as IqandRq. Further, we could define an in-context function Cψ: 
User [XC
q, XB
q], Assistant [Iq, Rq]
7→pθ 
Assistant [Ik, Rk]|User [XC
k, XB
k]
to represent the
multi-modal in-context generation with current query example. Similarly, we simply switch to the
corresponding system instructions for translating the English instruction-response pairs.
3.2 M ODEL ARCHITECTURE
The overall architecture of Ziya-Visual series consists of three components:
Large Language Model: Ziya-Visual adopt a bilingual large language model as the foundation
backbone, which is initialized with pre-trained Ziya-LLaMA-13B Wang et al. (2022). Ziya-LLaMA-
13B inherites the breakpoints from the initial LLaMA, adapting the tokenizer, training strategy and
model weights for bilingual version.
Visual Encoder: We adopt Vision Transformer (ViT) Dosovitskiy et al. (2020) as the visual en-
coder to extract a fixed number of output features from the images, initialized with pre-trained
weights from ViT/G-14.
Querying Transformer: To alleviate the issues of information redundancy and inefficiency
caused by long image feature sequences, Ziya-Visual uses the Q-Former Li et al. (2023c) to bridge
the gap between the pre-trained image encoder and LLM. As shown in Figure 3.3, Q-Former consists
of two transformer submodules that share the same self-attention layers: (1) an image transformer
that interacts with the visual encoder for image features compression. The module uses a group
of learnable visual query embeddings as input to the image transformer, which interact with each
other through self-attention layers and compress image feature from 256 ×768 to 64 through cross-
attention layers. (2) a text transformer that encodes the text presentations paired with input images.
The Q-Former controls the visibility between quries and text through the self-attention mask ma-
trix, enabling the execution of various pre-training tasks to align vision-language features. During
instruction-tuning stage, the compressed visual feature sequence is subsequently fed into the large
language model.
3.3 T RAINING
The training process of the Ziya-Visual models consist of three stages: a first stage of pre-training
and two stage of instruction-tuning training.
Pre-training: As illustrated in Figure 3.3, during the pre-training stage, we mainly collect several
large-scale, high-noise datasets of web-crawled image-text pairs. We leverage an in-house bilingual
CLIP-v2 Radford et al. (2021) model to clean up and accumulate a portion of high-quality Chinese-
English image captioning data based on publicly available open-source datasets, which contributes
to improving the overall data quality. As summarized in Table 1, we prepare a total of 13 million
English and 17 million Chinese image-text pairs for pre-training.
We connect Q-Former to a frozen image encoder and perform pre-training stage with image-text
pairs, aiming to guide the learned queries to extract the most informative visual representations from
4

--- PAGE 5 ---
Ziya-VL Technical Report
Table 1: Details of Ziya-Visual pre-training data. CC12M Changpinyo et al. (2021a), CC3M Sharma
et al. (2018), SBU Ordonez et al. (2011a) and COCO Caption are academic caption datasets.
Laion High is a Chinese language subset of LAION Schuhmann et al. (2022) with high resolution,
and Wukong Gu et al. (2022) dataset is a large-scale multi-modality Chinese dataset, provided by
Huawei Noah Lab. AI Challenger Wu et al. (2019) is Chinese competition dataset.
Language Dataset Original Cleaned Remaining %
EnglishCC12M 10.5M 9M 85 %
CC3M 2.8M 2.8M 100 %
SBU 0.88M 0.7M 80 %
COCO Caption 0.6M 0.6M 100 %
ChineseAI Challenger 3M 3M 100 %
Wukong 100M 12M 12 %
Laion High 2.3M 1.8M 75 %
Figure 2: (Left) Model architecture of Q-Former and pre-training objectives. We jointly optimize
three objectives that incentivize the queries (a set of trainable embeddings) to extract the most per-
tinent visual representations in alignment with the text. (Right) The self-attention masking strategy
for each objective to control query-text interaction. The model architecture and training design are
referred from BLIP2 Li et al. (2023c).
the text through representation aligning. Following the practices of BLIP2 Li et al. (2023c) and AL-
BEF Li et al. (2021), we also jointly optimize three pre-training objectives, Image-Text Contrastive
Learning (ITC), Image-grounded Text Generation (ITG) and Image-Text Matching (ITM), which
share the same input format and model parameters. Specially, for Ziya-Visual-Chat, We further use
the contrastive similarity to sample one in-batch hard negative image-text pair for the ITM task.
The model is trained using AdamW Kingma & Ba (2014) optimizer with learning rate lr= 1e−5,
β1= 0.9,β2= 0.9andeps= 1e−8. We use a weight decay of 5e−2and a gradient clipping of 2.0.
The training process uses a batch size of 2048 for the image-text pairs, and the entire pre-training
stage lasts for 90000 steps.
Instruction-Tuning: In the first stage of instruction-tuning (Multi-task Representation Learning),
to ensure diversity of the instruction-tuning data, we collect a wide range of publicly available vision-
language fine-tuning datasets and transform them into instruction-tuning format using manually de-
vised rules. As summarized in Table 3, we train Ziya-Visual on 4 tasks simultaneously. For image
captioning, We collect a set of high-quality bilingual academic and competitive fine-tuning datasets,
including COCO Lin et al. (2014), COCO-CN Li et al. (2019), Flickr30k Young et al. (2014), Flickr-
30k-CNA (consisting of ICM, IQM, ICR, IQR series) Xie et al. (2022), AI Challenger 2017 Wu
et al. (2019) and TextCaps Sidorov et al. (2020). We use a mixture of publicly available data for
visual question answering that includes GQA Hudson & Manning (2019), VQAv2 Goyal et al.
(2017), OCR-VQA Mishra et al. (2019), TextVQA Singh et al. (2019) and CLEVR Johnson et al.
(2017), where the VQAv2 and GQA datasets are further translated into Chinese through a trans-
lation pipeline, named as VQAv2 (T) and GQA (T). Furthermore, for the reference grounding
and grounded captioning duality tasks, we construct training instruction-response pairs from Vi-
5

--- PAGE 6 ---
Ziya-VL Technical Report
Table 2: Details of Ziya-Visual instruction-tuning data in Multi-task Representation Learning stage.
The label (T) means that the dataset is initially translated from the original data into Chinese, while
the datasets highlighted in red are exclusive to Ziya-Visual-Chat.
Task # Samples Dataset
Image Captioning 8.8MCC3M, COCO, COCO-CN, Flickr30k,
Flickr30k-CNA, AI Challenger 2017, TextCaps
Visual Question Answering 4.5MVQAv2, VQAv2 (T), GQA,
OCR-VQA, GQA (T), TextVQA, CLEVR
Reference Grounding 7.0MVisual Genome, RefCOCO,
RefCOCO+, RefCOCOg, VCR
Grounded Capation 7.0MVisual Genome, RefCOCO,
RefCOCO+, RefCOCOg, VCR
Table 3: Details of Ziya-Visual instruction-tuning data in Scece-aware Knowledge Learning stage.
The label (T) means that the dataset is initially translated from the original data into Chinese, while
the datasets highlighted in red are exclusive to Ziya-Visual-Chat.
Dataset Data Description # Samples
LLaV ACaptioning 595k
Detail Description 23k
Complex Reasoning 77k
Conversation 58k
LLaV A (T) Total 753k
M3ITImage Captioning 748k
Classification 360k
Visual Question Answering 235k
Knowledgeable Visual QA 57k
Reasoning 121k
Generation 174k
Chinese 273k
M3IT (T) Total 1.98M
SVITConversation 1.6M
Complex Reasoning 1.6M
Detail Description 106k
Referring QAs 1.0M
In-House Total 300k
sual Genome Krishna et al. (2017), RefCOCO, RefCOCO+, RefCOCOg Kazemzadeh et al. (2014)
and VCR Zellers et al. (2019).
As shown in the left part of Figure 3.3, for Ziya-Visual-Base, we freeze the Q-Former and connect
it to the LLM to harvest the LLM’s generative language capability, where the queries transform the
image information into understandable text-like representations. A trainable fully-connected (FC)
layer is used for linearly projecting the output query tokens into the LLM as instruction prefixes. In
Ziya-Visual-Chat, the instruction text is not only given as input to the LLM, but also to the text trans-
former in Q-Former. The instruction interacts with the queries through self-attention layers of the
Q-Former, which influences the queries towards extracting image features that are more informative
of the task as described by the instruction Dai et al. (2023).
In the second stage of instruction-tuning (Scene-aware Knowledge Learning), we gather a set of
high-quality and informative instruction fine-tuning datasets to enhance the instruction following
and dialogue capabilities of Ziya-Visual models, encompassing multi-turn conversations, logical
reasoning, situational descriptions and detailed depiction in instruction-response scenarios. There-
fore, we collect a set of the latest open-source instruction-response datasets, including LLaV A Liu
6

--- PAGE 7 ---
Ziya-VL Technical Report
et al. (2023a), M3IT Li et al. (2023d) and SVIT Zhao et al. (2023), which cover a wide range of
multi-modal scenarios in a conversational tone to closely resemble real-world dialogues. Similarly,
we also construct a Chinese version of LLaV A (T) and M3IT (T) using the translation pipeline and
use the generation pipeline to self-annotate the In-House data.
During training, Ziya-Visual-Base opens up the Q-Former for fine-tuning on top of the existing
framework. As for Ziya-Visual-Chat, we consider that the model can learn visual concepts and
knowledge depicted in images through visual knowledge learning. Therefore, in addition to open-
ing up the Q-Former and projection layer, we apply low-rank adaptation (LoRA) in the ViT and
LLM to adapt fVandfLby training multiple low-rank matrices for efficient alignment with human
instructions.
Figure 3: The two-stage instruction-tuning pipeline of Ziya-Visual. (Top) The multi-task repre-
sentation learning and scene-aware knowledge learning for Ziya-Visual-Base. (Bottom The Ziya-
Visual-Chat with instruction-aware visual feature extraction and low-rank adaption module.
4 E XPERIMENT ANDANALYSIS
In this section, we evaluate Ziya-Visual series on various traditional vision-language tasks, primarily
covering image captioning and general visual question answering (VQA) tasks as presented in Ta-
ble 4. Additionally, we assess the multilingual multi-modal conversation capability of Ziya-Visual
in both Chinese and English on the LLaV A benchmark and its Chinese translated version.
4.1 I MAGE CAPTIONING AND GENERAL VISUAL QUESTION ANSWERING
Image captioning task requires the model to generate descriptions for a given image. We use No-
caps Agrawal et al. (2019) and Flickr30k Young et al. (2014) as the English evaluation datasets, and
report the CIDEr score as metric. As for Chinese version, we choose the common translated subset
COCO-CN and Flickr30k-CNA, and extra report the image-text retrieval scores. General VQA task
requires model to generate answer for given image-question pair. We utilize five English benchmark
for evaluation, including VQAv2 Goyal et al. (2017), OKVQA Marino et al. (2019), GQA Hudson &
Manning (2019), ScienceQA (Image Set) Lu et al. (2022) and VizWiz VQA Gurari et al. (2018). In
general scenarios, we employ open-ended answer generation, while in cases with provided options,
7

--- PAGE 8 ---
Ziya-VL Technical Report
Table 4: Summary of the evaluation benchmarks.
Task Dataset Description Split Metric
Image
CaptionNocaps Captioning of natural images val CIDEr(↑)
COCO-CN Captioning of natural images karpathy-test CIDEr(↑)
Flickr30k-CNA Captioning of natural images karpathy-test CIDEr(↑)
Flickr30K Captioning of natural images karpathy-test CIDEr(↑)
General
VQAVQAv2 VQA on natural images test-dev VQA Score( ↑)
OKVQA VQA on natural images requiring outside knowledge val VQA Score( ↑)
GQA VQA on scene understanding and reasoning test-balanced EM(↑)
ScienceQA Multi-choice VQA on a diverse set of science topics test Accuracy( ↑)
VizWiz VQA on photos taken by people who are blind test-dev VQA Score( ↑)
Table 5: Experiment results on English Image Captioning and General VQA.
ModelImage Captioning General VQA
Nocaps
(0-shot)Flickr30K
(0-shot)VQAv2 OKVQA GQASciQA-Img
(0-shot)VizWiz
(0-shot)
Flamingo-9B - 61.5 51.8 44.7 - - 28.8
Flamingo-80B - 67.2 56.3 50.6 - - 31.6
Unified-IO-XL 100.0 - 77.9 54.0 - - -
Kosmos-1 - 67.1 51.0 - - - 29.2
Kosmos-2 - 80.5 51.1 - - - -
BLIP-2 (Vicuna-13B) 103.9 71.6 65.0 45.9 32.3 61.0 19.6
InstructBLIP (Vicuna-13B) 121.9 82.8 - - 49.5 63.1 33.4
Shikra (Vicuna-13B) - 73.9 77.4 47.2 - - -
Ziya-Visual-Base (Ziya-LLaMA-13B) 108.2 74.7 69.2 50.3 39.8 59.8 26.6
Ziya-Visual-Chat (Ziya-LLaMA-13B) 119.6 83.4 75.5 47.9 51.1 65.9 34.0
we constrain the model’s output to the possible choices. We evaluate in a Chinese environment using
the GQA (T) and VQAv2 (T) test sets.
The overall performance on English image captioning and general VQA tasks are reported in Ta-
ble 5. We can observe that Ziya-Visual-Base, adapted for bilingual multi-modal alignment and
further instruction-tuning on top of BLIP-2, shows improvement across various English vision-
language tasks. Meanwhile, with minimal adjustments using a small amount of Chinese instruction-
response pairs, Ziya-Visual-Base demonstrates commendable comprehension and generation abil-
ities in Chinese multi-modal tasks. Next, we report the overall performance on Chinese image
captioning and general VQA tasks in Table 6. With the support of more instruction-tuning datasets
and comprehensive training strategies, Ziya-Visual-Chat achieves performance on par with English
state-of-the-art LVLMs (InstructBLIP Dai et al. (2023) and Shikra Chen et al. (2023), and outper-
forms most existing multilingual LVLMs in Chinese scenarios. Specifically, compared to models
with outstanding performance in the open-source community, such as mPLUG-Owl Ye et al. (2023)
and VISCPM-Chat Hu et al. (2023), we achieve comparable performance in the image-text retrieval
task. Moreover, on the CIDEr metric for the COCO-CN, Flickr30k-CNA and GQA (T) datasets,
Ziya-Visual-Chat achieves an average score of 110.4, surpassing mPLUG-Owl’s 96.5 and VISCPM-
Chat’s 108.6. We also observe that on the accuracy metric for GQA (T) and VQAv2 (T), we achieve
an average score of 51.8%, surpassing mPLUG-Owl and VISCPM-Chat by 6.15% and 1.75%, re-
spectively. The improvement indicates that Ziya-Visual-Chat can more accurately capture the visual
objects within the images via instruction-aware fine-tuning, and low-rank adaption on LLM and ViT
modules.
4.2 LL AVA B ENCHMARK
LLaV A benchmark consists of 90 instances, each containing an image with symbolic represen-
tations, a question and an answer, which comprehensively evaluates the model’s performance in
conversation, detailed description and complex reasoning through GPT-4 scoring. We compare
Ziya-Visual with existing multi-modal conversation models, which include the English-only mod-
els: MiniGPT-4 Zhu et al. (2023), InstructBLIP Dai et al. (2023), and LLaV A Liu et al. (2023a), as
8

--- PAGE 9 ---
Ziya-VL Technical Report
Table 6: Experiment results on Chinese Image Captioning and General VQA.
ModelImage Captioning General VQA
COCO-CN Flickr30K GQA (T) VQAv2 (T)
CIDEr IR@1 TR@1 CIDEr IR@1 TR@1 ACC CIDEr ACC
VisualGLM 87.8 85.5 94.6 86.2 82.9 89.5 32.3 93.9 45.8
mPLUG-Owl 93.7 93.3 96.4 94.5 89.3 93.2 37.7 101.2 53.6
VISCPM-Chat 108.2 91.4 97.7 104.5 88.5 93.3 41.2 113.0 58.9
Ziya-Visual-Base 95.4 89.1 94.2 91.6 86.9 90.2 36.6 104.5 54.1
Ziya-Visual-Chat 111.4 91.0 96.1 103.7 91.4 92.7 43.4 116.2 60.2
Table 7: Experimental results on LLaV A benchmark accessed by GPT-4. Con: Conversation, DD:
Detailed Description, CR: Complex Reasoning, A VG: the average score of three tasks. The best
results are marked in bold .
ModelLLM
BackboneEnglish Chinese
Con DD CR A VG Con DD CR A VG
English
ModelMiniGPT-4 Vicuna-13B 65.0 67.3 76.6 69.7 - - - -
InstructBLIP Vicuna-13B 81.9 68.0 91.2 80.5 - - - -
LLaV A Vicuna-13B 89.5 70.4 96.2 85.6 - - - -
En-Zh
Bilingual
ModelmPLUG-Owl LLaMA-7B 64.6 47.7 80.1 64.2 76.3 61.2 77.8 72.0
VisualGLM ChatGLM-6B 62.4 63.0 80.6 68.7 76.6 87.8 83.6 82.7
VISCPM-Chat CPM-Bee-10B 81.4 69.2 93.1 81.4 90.0 87.4 95.0 90.9
Ziya-Visual-Base Ziya-LLaMA-13B 82.7 69.9 92.1 81.7 85.0 74.7 82.4 80.8
Ziya-Visual-Chat Ziya-LLaMA-13B 84.2 71.4 96.6 84.1 90.9 81.3 88.0 86.7
well as Chinese-English bilingual models: mPLUG-Owl (LLaMA-7B) Ye et al. (2023), VisualGLM
(ChatGLM-6B) Du et al. (2022) and VISCPM-Chat (CPM-Bee-10B) Hu et al. (2023).
As shown in Figure 7, Ziya-Visual-Base demonstrates a balanced performance across various met-
rics, indicating that the model achieves a preliminary multi-modal understanding capability. With
the introduction of more bilingual instruction-response pairs, Ziya-Visual-Chat achieves an average
GPT-4 score of 84.1 in English and 86.7 in Chinese, surpassing Ziya-Visual-Base by 2.4 and 5.9,
respectively. This significant improvement highlights the effectiveness of instruction-aware visual
feature extraction and low-rank adaptation strategies in promoting LLM cognition and aligning vi-
sual representation, which enables Ziya-Visual-Chat to understand complex instructions and diverse
scenarios and provide accurate responses.
4.3 C ASE STUDY
To intuitively observe the multi-modal understanding and generation capabilities of the Ziya-Visual
series, in Figure 4.3, we present dialogues involving Ziya-Visual-Base and Ziya-Visual-Chat in a
selection of qualitative examples.
From the prediction results of case 1 complex reasoning and case 2 detail description, we can observe
that Ziya-Visual-Base demonstrate initial multi-modal understanding and generation capabilities.
It enables the model to comprehensively perceive visual objects and global semantic in images,
further providing corresponding responses to instructions. However, Ziya-Visual-Base still exhibits
significant deficiencies in visual hallucination and expression fluency. Specifically, in the responses
for case 1 and case 2, it erroneously mentioned ’ 马(horse)’ and ’ 卡车(truck)’ respectively, which
are not visual objects present in the images. Furthermore, even though the model can generate
answers with long context, it still misses many elements in the images.
Excitingly, Ziya-Visual-Chat significantly addresses the aforementioned shortcomings, which can
accurately capture image details and provide a well-structured analysis and reasoning chain based
on instructions. We can observe that in case 1, Ziya-Visual-Chat can provide a well-founded analysis
9

--- PAGE 10 ---
Ziya-VL Technical Report
of the ’What challenges might the woman on the motorcycle face in this situation?’ from various
aspects. In case 2, although the model’s answer is concise, it accurately covers visual objects such
as ’城市街道(city street)’, ’ 大型时钟(large clock)’, ’ 树(tree)’ and ’ 汽车(car)’.
Ziya-VL-Chat
在这种情况下 ，骑摩托车的妇女可能会面临一些挑战 。首先 ，
她必须小心地操纵摩托车以避免与母牛或任何其他动物碰撞 。
由于道路狭窄和有人行走 ，她需要保持警惕并谨慎驾驶 ，因为
不注意可能导致事故或伤害到周围的人 。此外 ，如果母牛被吓
到或受惊吓 ，它们可能会突然改变方向 ，使骑手难以控制自己
的摩托车 ，从而增加了危险性 。Ziya-VL-Base
在这种情况下 , 骑摩托车的妇女可能会面临一些挑战 。首先, 
她必须保持对马和牛的注意力 , 以确保安全 。由于道路狭窄且
有动物 , 她需要谨慎驾驶并避免与其他人或动物发生碰撞 。此
外, 由于道路条件较差 , 她可能需要更多的技巧来控制摩托车 , 
例如平衡和操纵 。问题 ：在这种情况下 ，骑摩托车的妇女可能会面临什么挑战 ？
Ziya-VL-Chat
In the image, a woman is riding her motorcycle down a 
dirt road with cows walking in front of her. The 
challenges she might face include navigating around 
the cows and avoiding any sudden movements or 
changes in direction that could startle them. 
Additionally, the presence of cows on the road may 
cause disruption to traffic flow as they move slowly and 
unpredictably. She needs to be aware of their behavior 
and maintain a safe distance from them while driving. 
Furthermore, the dusty conditions caused by the 
passing vehicles can make it difficult for her to see 
clearly ahead, increasing the risk of accidents if she fails 
to anticipate potential hazards such as potholes or 
uneven surfaces.Ziya-VL-Base
In the image, a woman on a motorcycle is navigating 
through a dirt road with cows and other people walking 
around. She might face challenges such as maintaining 
control of her vehicle while maneuvering around 
obstacles like cows or pedestrians. Additionally, she may 
need to be extra vigilant when riding near others who 
are not necessarily familiar with traffic rules or 
regulations in this rural setting. This could lead to 
potential accidents if she fails to anticipate their 
movements or actions. Furthermore, the presence of 
animals and unpaved roads can make it more difficult 
for the woman to ride smoothly and safely along the 
path.Question: What challenges might the woman on the 
motorcycle face in this situation?问题 ：在这种情况下 ，骑摩托车的妇女可能会面临什么挑战 ？
Question: What challenges might the woman on the 
motorcycle face in this situation?
Ziya-VL-Chat
图像描绘了一个城市街道的场景 ，其中有一个大型时钟安装在人行道
上。它位于路边或人行道旁边 ，可能是公共区域的一部分 。时钟附近还
有几棵树和停放的汽车 ，为该地区增添了活力 。问题 ：描述图像的内容 。
Ziya-VL-Chat
The image features a city street with a large clock mounted on a 
pole in the middle of the sidewalk. The clock is positioned near 
some trees and a building, adding to the urban atmosphere. 
There are several cars parked along the road, including one close 
to the left edge of the scene and another further back towards 
the right side. A bicycle can also be seen leaning against the curb 
nearby.In addition to the vehicles, there are various potted plants 
scattered around the area, contributing to the lively ambiance of 
this city street.Question: Describe the following image.Ziya-VL-Base
该图像以一个带有时钟的柱子为特色 。它位于人行道上  ， 可能是在商
业区或住宅区附近 。这座城市中心的时钟吸引了许多人  ， 他们正在周
围散步和享受阳光明媚的天气 。街道两侧都有停车场  ， 其中一些停车
场靠近路边  ， 而另一些则更远离路边 。此外 ， 还有几辆汽车停在不同
的地方  ， 包括一辆卡车和一辆自行车  ， 表明了城市的繁荣 。问题 ：描述图像的内容 。
Ziya-VL-Base
The image features a large clock tower located in the middle of a 
city street. It is surrounded by trees and flowers, giving it an 
urban park -like atmosphere. There are several cars parked along 
the sidewalk near the clock tower, indicating that people have 
stopped to admire or use its services.In addition to the clock 
tower, there are also benches placed around the area for 
pedestrians to rest on while enjoying their surroundings. A 
bicycle can be seen leaning against one of these benches, 
suggesting that some individuals may prefer using this mode of 
transportation instead of driving a car.Question: Describe the following image.Case 1
Case 2
Figure 4: Case Study of Ziya-Visual-Base and Ziya-Visual-Chat on bilingual multi-modal genera-
tion.
5 C ONCLUSION AND FUTURE WORK
We release the Ziya-Visual series, a comprehensive set of large-scale bilingual vision-language mod-
els and instruction-response pair construction pipelines, which are designed to foster development
within the multi-modal open-source community. The latest Ziya-Visual-Chat achieves comparable
or slightly superior performance to mainstream English-only and bilingual LVLMs across various
benchmarks, demonstrating significant multi-modal understanding and generation capabilities in
multi-turn conversation, complex reasoning and detail descriptions. Looking forward, we are com-
mitted to further enhancing the capabilities of Ziya-Visual in several key aspects:
•Developing the capabilities of Ziya-Visual in multi-modal tasks such as referential dia-
logues, general scene differentiation, video comprehension.
•Enhancing the fine-grained visual object understanding of Ziya-Visual by extending the
model structure and training data, enabling it to handle detail portrait and action recognition
within complex images.
•Extending Ziya-Visual’s functionality in multi-modal generation, particularly in the inte-
gration of diffusion to achieve text-to-image and image-to-image generation.
10

--- PAGE 11 ---
Ziya-VL Technical Report
REFERENCES
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra,
Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In
Proceedings of the IEEE/CVF international conference on computer vision , pp. 8948–8957, 2019.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716–
23736, 2022.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. In Proceedings of the 34th International Conference on Neural Information
Processing Systems , pp. 1877–1901, 2020.
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-
scale image-text pre-training to recognize long-tail visual concepts. In CVPR , 2021a.
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 3558–3568, 2021b.
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing
multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 , 2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language
models with instruction tuning, 2023.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu,
and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234 , 2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-
age is worth 16x16 words: Transformers for image recognition at scale. In International Confer-
ence on Learning Representations , 2020.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:
General language model pretraining with autoregressive blank infilling. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pp. 320–335, 2022.
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,
Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model.
arXiv preprint arXiv:2304.15010 , 2023.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pp. 6904–6913, 2017.
Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui
Huang, Wei Zhang, Xin Jiang, et al. Wukong: A 100 million large-scale chinese cross-modal
pre-training benchmark. In Thirty-sixth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track , 2022.
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and
Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 3608–3617,
2018.
11

--- PAGE 12 ---
Ziya-VL Technical Report
Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu,
Yue Zhao, Haoye Zhang, et al. Large multilingual models pivot zero-shot multimodal learning
across languages. arXiv preprint arXiv:2308.12038 , 2023.
Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pp. 6700–6709, 2019.
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
1988–1997. IEEE, 2017.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to
objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP) , pp. 787–798, 2014.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv e-prints ,
pp. arXiv–1412, 2014.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and Ross Girshick.
Segment anything. arXiv:2304.02643 , 2023.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language
and vision using crowdsourced dense image annotations. International Journal of Computer
Vision , 123(1):32–73, 2017.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan
Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint
arXiv:2306.05425 , 2023a.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A
multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023b.
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven
Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum
distillation. Advances in Neural Information Processing Systems , 34:9694–9705, 2021.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 , 2023c.
Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,
Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual in-
struction tuning. arXiv e-prints , pp. arXiv–2306, 2023d.
Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu.
Coco-cn for cross-lingual image tagging, captioning, and retrieval. IEEE Transactions on Multi-
media , 21(9):2347–2360, 2019.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13 , pp. 740–755. Springer, 2014.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485 , 2023a.
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei
Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for
open-set object detection. arXiv preprint arXiv:2303.05499 , 2023b.
12

--- PAGE 13 ---
Ziya-VL Technical Report
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,
Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for
science question answering. Advances in Neural Information Processing Systems , 35:2507–2521,
2022.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual
question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf
conference on computer vision and pattern recognition , pp. 3195–3204, 2019.
A Mishra, S Shekhar, AK Singh, and A Chakraborty. Ocr-vqa: Visual question answering by
reading text in images. In Proceedings of the International Conference on Document Analysis
and Recognition, ICDAR , pp. 947–952. IEEE Computer Society, 2019.
OpenAI. Gpt-4 technical report, 2023.
Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million
captioned photographs. NeurIPS , 2011a.
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing images using 1 million
captioned photographs. In Proceedings of the 24th International Conference on Neural Informa-
tion Processing Systems , pp. 1143–1151, 2011b.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning , pp.
8748–8763. PMLR, 2021.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman,
Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-
5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth
Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022.
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: A dataset for
image captioning with reading comprehension. In European Conference on Computer Vision , pp.
742–758, 2020.
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. Towards vqa models that can read. In 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pp. 8309–8318. IEEE, 2019.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing
He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu,
Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng,
Chongpei Chen, Ruyi Gan, and Jiaxing Zhang. Fengshenbang 1.0: Being the foundation of
chinese cognitive intelligence. CoRR , abs/2209.02970, 2022.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International
Conference on Learning Representations , 2021.
Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei Zhou,
Guosen Lin, Yanwei Fu, Yizhou Wang, and Yonggang Wang. Large-scale datasets for going
deeper in image understanding. In 2019 IEEE International Conference on Multimedia and
Expo (ICME) . IEEE, jul 2019. doi: 10.1109/icme.2019.00256. URL https://doi.org/
10.1109%2Ficme.2019.00256 .
13

--- PAGE 14 ---
Ziya-VL Technical Report
Chunyu Xie, Jincheng Li, Heng Cai, Fanjing Kong, Xiaoyu Wu, Jianfei Song, Henrique Morimitsu,
Lin Yao, Dexin Wang, Dawei Leng, et al. Zero and r2d2: A large-scale chinese cross-modal
benchmark and a vision-language framework. arXiv preprint arXiv:2205.03860 , 2022.
Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning
via instruction tuning. arXiv preprint arXiv:2212.10773 , 2022.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen
Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models
with multimodality. arXiv preprint arXiv:2304.14178 , 2023.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual
denotations: New similarity metrics for semantic inference over event descriptions. Transactions
of the Association for Computational Linguistics , 2:67–78, 2014.
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Vi-
sual commonsense reasoning. In 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 6713–6724. IEEE, 2019.
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng
Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init atten-
tion. arXiv preprint arXiv:2303.16199 , 2023a.
Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin,
Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: A strong image tagging model.
arXiv preprint arXiv:2306.03514 , 2023b.
Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv e-prints ,
pp. arXiv–2307, 2023.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 , 2023.
14

--- PAGE 15 ---
Ziya-VL Technical Report
Visual input example, suitcases:
User What is unusual about this image?
Ziya-Visual-V1 The colors of the two suitcases in the image are different. One is brown, and the other is
black.
VisualGLM The two suitcases in the picture are black and brown. They have a white label indicating
that they are antiques or vintage items. These boxes may be antique suitcases, travel
cases, or part of antique furniture. Their design and decoration may suggest their use in
various historical periods.
Table 8: Case Study of Ziya-Visual-V1 and VisualGLM.
.1 C ASE STUDY
Using Ziya-Visual-V1 (Old name of Ziya-Visual-Base) and VisualGLM as examples and through
case analysis, we observed that Ziya-Visual-V1 tends to provide concise and accurate answers,
whereas VisualGLM’s responses are more abundant and divergent. This observation may explain
why Ziya-Visual-V1 outperforms VisualGLM in simple and detailed conversations but slightly falls
behind in complex reasoning tasks. Although we haven’t conducted detailed ablation experiments,
we hypothesize that the primary cause of these differences lies in the variation of image question-
answering training data rather than the differences in the employed LLM.
We illustrate our analysis with an example 8 from coco test set: Given the question ”What are the
colors of the two suitcases in the picture?” The responses from the two models are as follows:
GPT-4 provides a rating of 9 for Ziya-Visual-V1 (Assistant 1) and 7 for VisualGLM (Assistant 2),
with the explanation: Assistant 1’s response is concise and straightforward, directly answering the
question by providing the colors of the two suitcases as black and brown. Assistant 2’s response
also mentions the colors of the suitcases but includes unnecessary information such as the white
label and the possible uses of the suitcases as antiques. While this additional information might be
interesting, it does not directly address the question. As a result, Assistant 1’s response is considered
more useful, relevant, and accurate by GPT-4.
Also, let’s now take a look at some examples from Ziya-Visual-V1 to gain an intuitive understanding
of its bilingual visual question-answering capabilities.
This example 5 showcases the model’s image recognition, knowledge, and creative abilities. In the
first question, the model identifies the scene from the movie ”Titanic” in the image and provides in-
formation about the film’s director, release date, and achievements in awards. In the second question,
the model creatively generates a modern love poem based on the user’s request.
This example 6 illustrates Ziya-Visual-V1’s recognition and understanding of traditional Chinese
culture. The model identifies information from a Chinese painting and, upon receiving the clue
”Qingming Riverside Picture,” provides details about the painter Zhang Zeduan and the historical
background of the Northern Song Dynasty.
Ziya-Visual-V1’s competence extends to handling multiple images and conducting multi-turn in-
teractions. In this example 7, Ziya-Visual-V1 demonstrates its powerful capabilities by narrating a
story based on three images provided by the user. Thanks to the creativity of the generative model,
the storylines in the generated Chinese and English texts are indeed different. The English story
depicts a night outing of a group of friends, while the Chinese story is completely distinct. The Chi-
15

--- PAGE 16 ---
Ziya-VL Technical Report
nese story revolves around a lady encountering a mother cat and her kittens amidst the city’s night
scenery. The story involves interactions and conversations between the lady and the feline family,
leading to heartwarming separations.
Which movie is this scene from, and who is the director?
The scene is from the movie “T itanic” and the director of this
particular scene is James Cameron. In the image, a man and
woman are standing on a ship’ s deck with their arms around each
other while looking out at the ocean.
Write a poem describing this scene.这是哪部电影的场景，导演是谁
图像中的场景是电影《泰坦尼克号》。男人和女人站在船上  ，  背对着
海洋 ，  他们似乎正在进行一个戏剧性的表演或姿势。这部电影由詹姆
斯. 卡梅隆执导  ，  于  1997 年首次发布。该片以其视觉效果、音乐和故
事情节而闻名  ，  并赢得了多项奥斯卡奖。
写一首诗描述这个场景
在海洋的边缘  ，  他们站着  ，
背对着大海  ，  彼此拥抱。
他们的身体似乎在空气中漂浮  ，
就像泰坦尼克号上的船员一样  ，
他们的故事将永远停留在那里  ，
就像泰坦尼克号上的船员一样  ，
他们的故事将永远停留在那里A man and woman stand on a ship’ s deck,
With arms entwined in the setting sun’ s glow .
Their eyes meet as they gaze out at sea,
A moment of love shared between them so free.
The wind whips through their hair ,
As waves crash against the hull with care.
They hold each other close,
In this timeless embrace that will never be lost.
The sun sets behind them, painting the sky red,
As memories are made to last for all time.
This scene from “T itanic” is etched in our hearts,
A story of love and loss, forever parted by the tides..
Figure 5: Visual Dialogue Example 1: Titanic Movie Clip. Note that the text on the left and right
are generated independently, and the Chinese text on the right is not a translation of the English text
on the left.
16

--- PAGE 17 ---
Ziya-VL Technical Report
Please describe the scene in this picture.
The image depicts a large crowd of people gathered in an Asian
city, likely during a celebration or event. There are many
individuals walking around and interacting with each other ,
creating a lively atmosphere. Some people can be seen carrying
umbrellas to protect themselves from the sun or rain. The scene is
set against a background that resembles an old painting, giving it
a historical feel. The overall composition showcases the vibrant
energy of the gathering while also highlighting the cultural
heritage of the location through its artistic style.
This is the famous Chinese painting "Along the River During the
Qingming Festival." Do you know who painted it?请描述一下这幅图的场景
该图描绘了一个古老的城市场景 , 其中有许多人和建筑物。在画面的左
侧, 可以看到一座大型的建筑物 , 它似乎是一座寺庙或宫殿。这座建筑物
周围环绕着一些小房子和树木 , 形成了一个独特而美丽的风景。在右侧 ,
可以看到更多的建筑物和人群 , 他们似乎正在参加某种活动或庆祝活
动。
这是中国的名画 “ 清明上河图 ” ，你知道是谁画的吗？
是的, “ 清明上河图 ” 是中国北宋时期画家张择端所绘制的一幅著名的风
景画。这幅画描述了 10 世纪末至 12 世纪初的中国城市生活场景 , 以及人
们在其中进行日常活动和庆祝活动的情况。The famous Chinese painting “Along the River During the
Qingming Festival” was painted by Zhang Zeduan in 1085 AD. It is
considered one of China’ s greatest artistic achievements and a
masterpiece of the Song Dynasty (960-1279).
Figure 6: Visual Dialogue Example 2: Ancient Chinese Masterpiece: ”Qingming Shanghe Tu”
(Along the River During the Qingming Festival).
17

--- PAGE 18 ---
Ziya-VL Technical Report
Please describe the scene in this picture.
The image features a large, old building with a clock tower and a
lighted sign. It is located in front of a tree-lined park or garden
area. A full moon can be seen above the scene, adding to its
beauty and serenity .
In this context, what kind of story might unfold? W rite a
beginning.图中的内容是什么？
图像中有一座大型的建筑物和一个月亮。这座建筑物可能是教堂或其他
宗教场所 , 因为它具有尖顶和钟楼。在夜晚 , 月亮照耀着整个区域 , 使景
象更加神秘而美丽。
在这个情境下可能发生什么样的故事？写一个开头。
在这个夜晚 , 一位年轻的女士独自漫步在城市中。她沿着街道走来 , 经过
了一座大型建筑物 , 它有尖顶和钟楼 , 看起来像是教堂或其他宗教场所。
当她靠近时 , 她注意到月亮照耀着整个区域 , 使景象更加神秘而美丽。As the full moon illuminated the night sky , a group of friends
decided to take a walk in the nearby park. They had heard about
an old building with a clock tower and a lighted sign that was
located near the center of the park. The friends were intrigued by
this mysterious structure and wanted to explore it further .
Continuing the story to match the current picture:
The friends continue their exploration of City Hall and discover
two cats sitting on a bench in the building’ s courtyard. They
approach the animals, who seem to be enjoying the warm night
air and the company of each other ......
As the friends spend more time with the cats, they can’t help but
smile at their cuteness and the joy they bring to those around
them. In this moment, they realize that even a place like City Hall -
which holds so much history and significance - can still offer
unexpected moments of happiness and delight.
Integrating the above story , let the ending naturally transition to
this image.继续写这个故事，使内容符合现在这幅图的内容。
当她靠近时 , 她注意到一对母子猫坐在花园里的草地上。这两只猫是一
对可爱的橘色和白色的猫 , 它们似乎正在享受夜晚的宁静。女士感到惊
讶并被这个场景所吸引 , 因为她从未见过如此温柔而友好的动物。
她决定走近 , 以便更好地观察这些可爱的动物。当她接近时 , 猫妈妈向她
示意, 让她能够与自己的孩子分享他们的空间。女士很高兴地同意了 , 并
开始与这对可爱的猫咪交谈。
整合上面的故事，让结尾自然过渡到这张图。
女士与这对可爱的猫咪交谈了一段时间 , 然后决定离开。在她离开之前 ,
她向母亲和孩子道别并感谢他们让她有机会与他们共度美好的夜晚。当
她走出花园时 , 她回头看着月亮照耀下的城市 , 感到自己被这个神秘而美
丽的场景所吸引。As the friends said their goodbyes to City Hall and made their way
back home, they couldn’t help but feel grateful for the experience.
The building had exceeded all of their expectations, providing
them with a glimpse into its rich history while also introducing
them to two adorable cats who brought joy to everyone around
them......As they walked through the park on their way out, they
noticed that the moon was still high in the sky , casting its soft
light over everything below . They stopped momentarily to take it
all in - the beauty of the night, the memories from their visit to
City Hall, and the thoughtfulness of those who took care of the
cats.
Figure 7: Visual Dialogue Example 3: Multiple Images and Extended Dialogue.
18

--- PAGE 19 ---
Ziya-VL Technical Report
Figure 8: This is an overview of our Ziya-Visual-Base. The basic model structure is a multi-modal
Vision Transformer model interacted with Large Language Model. In the process, images are fed
into the Vision Transformer (ViT) and Q-Former to extract semantic features (instruction are fed
into the Q-former for Ziya-Visual-Chat). These features are then aligned in dimension with the
token embeddings of the Large Language Model (LLM) using a projection layer, resulting in Visual
Tokens. The Visual Tokens are combined with Instruction and jointly input into the LLM for text
generation. This approach enables the model to effectively utilize both visual and textual information
in generating coherent and relevant text outputs.
19
