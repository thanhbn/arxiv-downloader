# 2401.07037.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2401.07037.pdf
# Kích thước tệp: 1011370 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
XCOT: Điều chỉnh Hướng dẫn Đa ngôn ngữ cho Lý luận Chuỗi Suy nghĩ Đa ngôn ngữ
Linzheng Chai1, Jian Yang1∗, Tao Sun1, Hongcheng Guo1, Jiaheng Liu1,
Bing Wang1,Xiannian Liang1,Jiaqi Bai1,Tongliang Li3,Qiyao Peng2,Zhoujun Li1
1Phòng thí nghiệm trọng điểm nhà nước về Môi trường Phát triển Phần mềm, Đại học Beihang
2Trường Truyền thông và Phương tiện Mới, Đại học Thiên Tân
3Đại học Khoa học và Công nghệ Thông tin Bắc Kinh
{challenging, jiaya, buaast, hongchengguo, liujiaheng, bingwang, xnliang}@buaa.edu.cn;
{bjq, lizj}@buaa.edu.cm; qypeng@tju.edu.cn; tonyliangli@bistu.edu.cn;

Tóm tắt
Chuỗi suy nghĩ (CoT) đã nổi lên như một kỹ thuật mạnh mẽ để gợi ra khả năng lý luận trong các mô hình ngôn ngữ lớn và cải thiện nhiều tác vụ downstream khác nhau. CoT chủ yếu thể hiện hiệu suất xuất sắc trong tiếng Anh, nhưng việc sử dụng nó trong các ngôn ngữ có ít tài nguyên bị hạn chế do khả năng tổng quát hóa ngôn ngữ kém. Để thu hẹp khoảng cách giữa các ngôn ngữ khác nhau, chúng tôi đề xuất một khung điều chỉnh tinh hướng dẫn đa ngôn ngữ (XCOT) để chuyển giao kiến thức từ các ngôn ngữ có nhiều tài nguyên sang các ngôn ngữ có ít tài nguyên. Cụ thể, dữ liệu huấn luyện hướng dẫn đa ngôn ngữ (XCOT-INSTRUCT) được tạo ra để khuyến khích sự liên kết ngữ nghĩa của nhiều ngôn ngữ. Chúng tôi giới thiệu học tập few-shot trong ngữ cảnh đa ngôn ngữ (xICL) để tăng tốc thỏa thuận đa ngôn ngữ trong điều chỉnh hướng dẫn, trong đó một số đoạn của ngôn ngữ nguồn trong các ví dụ được thay thế ngẫu nhiên bằng các bản dịch tương ứng của ngôn ngữ đích. Trong quá trình điều chỉnh hướng dẫn đa ngôn ngữ, chúng tôi áp dụng chiến lược CoT trực tuyến ngẫu nhiên để tăng cường khả năng lý luận đa ngôn ngữ của mô hình ngôn ngữ lớn bằng cách đầu tiên dịch truy vấn sang ngôn ngữ khác và sau đó trả lời bằng tiếng Anh. Để tiếp tục tạo điều kiện cho việc chuyển giao ngôn ngữ, chúng tôi tận dụng CoT có nhiều tài nguyên để giám sát việc huấn luyện các ngôn ngữ có ít tài nguyên với chưng cất đa ngôn ngữ. Kết quả thực nghiệm trên các benchmark trước đây chứng minh hiệu suất vượt trội của XCOT trong việc giảm khoảng cách giữa các ngôn ngữ khác nhau, làm nổi bật tiềm năng của nó trong việc giảm khoảng cách đa ngôn ngữ1.

1 Giới thiệu
Những tiến bộ gần đây trong Mô hình Ngôn ngữ Lớn (LLMs) (Touvron et al., 2023a,b; Patel et al., 2023; OpenAI, 2023; Bai et al., 2023) trong xử lý ngôn ngữ tự nhiên (NLP) đã thu hút sự quan tâm mạnh mẽ của các nhà nghiên cứu. LLMs (Wei et al., 2022c; Zhang et al., 2023; Kojima et al., 2022a) được trang bị thêm kỹ thuật chuỗi suy nghĩ (CoT) để đạt được hiệu suất ấn tượng trong các tác vụ lý luận phức tạp, nơi LLMs đầu tiên tạo ra các bước lý luận trung gian và suy ra câu trả lời cuối cùng.

Tuy nhiên, các nghiên cứu hiện tại liên quan đến các phương pháp CoT chủ yếu bị giới hạn trong các ngôn ngữ có nhiều tài nguyên (ví dụ tiếng Anh) và ít quan tâm đến các tình huống đa ngôn ngữ. Các công trình gần đây (Shi et al., 2023; Qin et al., 2023; Chen et al., 2023a) nỗ lực đơn giản sử dụng kỹ thuật prompt để cải thiện khả năng tổng quát hóa ngôn ngữ của mô hình mà không cần điều chỉnh tinh nào. Các phương pháp dựa trên prompt này bỏ qua tiềm năng của sự liên kết đa ngôn ngữ dựa trên biểu diễn có được từ điều chỉnh tinh có giám sát đa ngôn ngữ (cross-lingual SFT). Điều chỉnh tinh có giám sát đã được chứng minh hoạt động ở mức độ thỏa đáng trên nhiều tác vụ khác nhau, như FLAN (Wei et al., 2022a) và InstructGPT (Ouyang et al., 2022). Do đó, cách khuyến khích sự liên kết đa ngôn ngữ trong điều chỉnh tinh có giám sát vẫn cần được khám phá thêm.

Để giảm thiểu khoảng cách giữa các ngôn ngữ khác nhau, chúng tôi đề xuất một khung lý luận Chuỗi Suy nghĩ Đa ngôn ngữ (XCOT) sử dụng điều chỉnh tinh hướng dẫn có giám sát đa ngôn ngữ. Cụ thể, chúng tôi đầu tiên xây dựng dữ liệu huấn luyện hướng dẫn đa ngôn ngữ

∗Tác giả liên hệ.
1Dataset và mã sẽ được phát hành.

Câu hỏi Đa ngôn ngữ LLM Th En Zh De Fr Suy nghĩ bằng tiếng Anh Trả lời Ngữ cảnh Chuyển đổi mã Câu hỏi Đa ngôn ngữ Căn chỉnh

Hình 1: Minh họa về XCOT. Điều chỉnh hướng dẫn đa ngôn ngữ được sử dụng để căn chỉnh biểu diễn của các ngôn ngữ khác nhau.

--- TRANG 2 ---
En Zh De Sp Chuyển đổi mã cấp độ span... Tiếng Anh: John viết 20 trang mỗi ngày. Anh ấy sẽ mất bao lâu để viết 3 cuốn sách mỗi cuốn 400 trang? Tiếng Đức: John schreibt 20 Seiten am Tag. Wie lange wird er brauchen, um drei Bücher mit jeweils 400 Seiten zu schreiben?

Mô hình Ngôn ngữ Lớn Chuyển đổi mã: John write 20 Seiten am Tag. Wie lange wird er brauchen, um drei Bücher mit are 400 pages each? Ngữ cảnh Chuyển đổi mã Câu hỏi tiếng Anh CoT tiếng Anh Trả lời Ngữ cảnh Chuyển đổi mã Câu hỏi tiếng Trung CoT tiếng Trung Trả lời

Kho dữ liệu Đa ngôn ngữ Kho dữ liệu Chuyển đổi mã

CoT Trực tuyến Ngẫu nhiên SFT Đa ngôn ngữ SFT trên tiếng Anh

Epoch 1−n

Lấy mẫu Từ chối Chuyển đổi mã

Kho dữ liệu Chuyển đổi mã Nâng cao

Epoch n SFT Chuyển đổi mã

Chưng cất Đa ngôn ngữ Dịch thuật

Fr Khung của CoT Đa ngôn ngữ

Hình 2: Tổng quan về XCOT. Học tập few-shot trong ngữ cảnh đa ngôn ngữ (xICL) khuyến khích sự căn chỉnh đa ngôn ngữ trong điều chỉnh hướng dẫn, nơi truy vấn trong ví dụ được trộn với các token ngôn ngữ khác nhau. Trong quá trình điều chỉnh hướng dẫn đa ngôn ngữ, chiến lược CoT trực tuyến ngẫu nhiên (Random-CoT) được sử dụng để thúc đẩy khả năng lý luận đa ngôn ngữ của LLM và sau đó trả lời bằng tiếng Anh. Cuối cùng, chúng tôi tận dụng CoT có nhiều tài nguyên để giám sát việc huấn luyện các ngôn ngữ có ít tài nguyên với chưng cất đa ngôn ngữ.

(XCOT-INSTRUCT) bằng cách dịch tiếng Anh sang các ngôn ngữ khác. Sau đó, chúng tôi thay thế ngẫu nhiên một số đoạn của ngôn ngữ nguồn trong các ví dụ bằng các bản dịch tương ứng của ngôn ngữ đích. Để chuyển giao các ngôn ngữ có nhiều tài nguyên sang các ngôn ngữ có ít tài nguyên, chúng tôi trộn các token của ngôn ngữ nguồn và ngôn ngữ đích trong cùng một truy vấn để cho phép LLMs xử lý các ngôn ngữ khác nhau. Các ví dụ chuyển đổi mã và truy vấn có thể được áp dụng cho học tập trong ngữ cảnh đa ngôn ngữ trong điều chỉnh hướng dẫn có giám sát. Trong quá trình điều chỉnh hướng dẫn đa ngôn ngữ, chúng tôi áp dụng chiến lược CoT trực tuyến ngẫu nhiên để tăng cường khả năng lý luận đa ngôn ngữ của mô hình ngôn ngữ lớn bằng cách đầu tiên dịch truy vấn sang ngôn ngữ khác và sau đó trả lời bằng tiếng Anh. Để tiếp tục tạo điều kiện cho việc chuyển giao ngôn ngữ, chúng tôi tận dụng CoT có nhiều tài nguyên để giám sát việc huấn luyện các ngôn ngữ có ít tài nguyên với chưng cất đa ngôn ngữ. Kết quả thực nghiệm trên các benchmark trước đây chứng minh hiệu suất vượt trội của XCOT trong việc giảm khoảng cách giữa các ngôn ngữ khác nhau, làm nổi bật tiềm năng của nó trong việc giảm khoảng cách đa ngôn ngữ.

Các thực nghiệm mở rộng của XCOT được đánh giá trên các benchmark đa ngôn ngữ MGSM của 11 ngôn ngữ và MSVAMP của 10 ngôn ngữ. Kết quả cho thấy phương pháp đề xuất của chúng tôi liên tục đạt được hiệu suất tiên tiến nhất trên tất cả các ngôn ngữ, vượt trội hơn baseline mạnh với biên độ trung bình là 15%. Các đóng góp trong công trình này được tóm tắt như sau: (1) Chúng tôi xây dựng dữ liệu hướng dẫn đa ngôn ngữ để chuyển giao kiến thức từ các ngôn ngữ có nhiều tài nguyên sang các ngôn ngữ có ít tài nguyên. Dữ liệu huấn luyện được tăng cường thêm bằng học tập trong ngữ cảnh đa ngôn ngữ, nơi một đoạn ngữ cảnh minh họa chuyển đổi mã và truy vấn hiện tại được nối với nhau làm đầu vào cho LLM. (2) Trong quá trình huấn luyện, chúng tôi đề xuất CoT trực tuyến ngẫu nhiên (Random-CoT), trước tiên dịch ngẫu nhiên truy vấn sang các ngôn ngữ khác và sau đó trả lời bằng tiếng Anh. (3) Để căn chỉnh biểu diễn của các ngôn ngữ khác nhau, chúng tôi đề xuất kiến thức đa ngôn ngữ để căn chỉnh phân phối đầu ra cho các truy vấn của các ngôn ngữ khác nhau sử dụng phân kỳ Kullback–Leibler.

2 Lý luận CoT Đa ngôn ngữ
Cho truy vấn q = (q1...qn) của ngôn ngữ Li, mô hình ngôn ngữ lớn (LLM) M xuất ra câu trả lời tương ứng a = (a1,...,an) của ngôn ngữ Lj, trong đó m và n là độ dài của prompt và câu trả lời trong một mẫu (q,a). Li và Lj là ngôn ngữ nguồn và ngôn ngữ đích, trong đó Lall = {Lk}K k=1 và K là số lượng ngôn ngữ. LLM tiếp tục tăng cường hiệu suất tác vụ bằng lý luận chuỗi suy nghĩ, nơi các ví dụ chuỗi suy nghĩa của các chuỗi c = (c1,...,ct) được thêm vào các exemplar của prompting. Các lý lẽ chất lượng cao c bao gồm một loạt các bước lý luận ngôn ngữ tự nhiên trung gian cung cấp các gợi ý hữu ích cho đầu ra cuối cùng. Cho nhiều ví dụ chuỗi suy nghĩ làm minh họa và prompt gốc

--- TRANG 3 ---
q của ngôn ngữ đích như một tổng thể, định nghĩa bài toán của CoT đa ngôn ngữ được mô tả như sau:
P(a|q, c) = ∏ⁿⱼ₌₁ P(aⱼ|a<ⱼ; q, c, M) (1)
trong đó q (câu hỏi) và c (các exemplar tương ứng) được nối với nhau thành một tổng thể p để dự đoán câu trả lời được ký hiệu là P(a|p). Được thúc đẩy bởi các minh họa CoT c, LLM đầu tiên tạo ra các bước trung gian và sau đó xuất ra câu trả lời cuối cùng a.

3 XCOT
3.1 Tổng quan Mô hình
Hình 2 mô tả khung tổng thể của phương pháp XCOT của chúng tôi. Cụ thể, học tập few-shot trong ngữ cảnh đa ngôn ngữ (xICL) khuyến khích căn chỉnh đa ngôn ngữ trong điều chỉnh hướng dẫn, nơi truy vấn trong ví dụ được trộn với các token ngôn ngữ khác nhau. Trong quá trình điều chỉnh hướng dẫn đa ngôn ngữ, chiến lược CoT trực tuyến ngẫu nhiên (Random-CoT) được sử dụng để thúc đẩy khả năng lý luận đa ngôn ngữ của LLM và sau đó trả lời bằng tiếng Anh. Cuối cùng, chúng tôi tận dụng CoT có nhiều tài nguyên để giám sát việc huấn luyện các ngôn ngữ có ít tài nguyên với chưng cất đa ngôn ngữ.

3.2 XCOT-INSTRUCT
Xây dựng Dữ liệu Chúng tôi tạo một bộ dữ liệu hướng dẫn đa ngôn ngữ mới (XCOT-INSTRUCT) cho lý luận chuỗi suy nghĩ đa ngôn ngữ, có thể được sử dụng làm kho dữ liệu huấn luyện cho các benchmark đa ngôn ngữ, như MGSM (Shi et al., 2023) và MSVAMP (Chen et al., 2023a). Chúng tôi sử dụng bộ dịch đa ngôn ngữ để mở rộng dữ liệu hướng dẫn tiếng Anh² thành 10 ngôn ngữ khác, bao gồm tiếng Đức, tiếng Pháp, tiếng Tây Ban Nha, tiếng Nga, tiếng Trung, tiếng Nhật, tiếng Thái, tiếng Telugu, tiếng Bengali và tiếng Swahili. Bộ dữ liệu hướng dẫn của mỗi ngôn ngữ chứa 7.4K mẫu, trong đó chúng tôi chỉ dịch truy vấn sang các ngôn ngữ khác và giữ nguyên phản hồi bằng tiếng Anh để tạo điều kiện cho việc chuyển giao đa ngôn ngữ. Cuối cùng, chúng tôi có được dữ liệu hướng dẫn đa ngôn ngữ D = {DLₖ}ᴷₖ₌₁ và (qLᵢ, cLᵢ, aLⱼ) ∈ DLᵢ, trong đó DLᵢ là dữ liệu huấn luyện SFT của ngôn ngữ Lᵢ và số lượng ngôn ngữ là K. DLᵢ chứa truy vấn qLᵢ và phản hồi aLⱼ với ngữ cảnh tương ứng cLᵢ. qLᵢ là truy vấn của ngôn ngữ nguồn và aLⱼ là phản hồi của ngôn ngữ có nhiều tài nguyên (Lⱼ là tiếng Anh trong công trình của chúng tôi). cLᵢ = {qLᵢᵦ, aLⱼᵦ}ᴮᵦ₌₁ là ngữ cảnh

²https://github.com/openai/grade-school-math

Thuật toán 1: CoT Trực tuyến Ngẫu nhiên
Đầu vào: Bộ dữ liệu Hướng dẫn Đa ngôn ngữ: D;
LLM Đa ngôn ngữ: M;
Bước điều chỉnh tinh có giám sát tối đa: T;
Kích thước batch: B;
Tập ngôn ngữ đích: Lall = {Lₖ}ᴷₖ₌₁;
Đầu ra: LLM đã điều chỉnh tinh: M
1 t ← 0
2 while t ≤ T; do
3   Batch B được lấy mẫu ngẫu nhiên ∈ D
4   for k ← 1 to B; do
5     (cLᵢ,ⱼ, qLᵢ, aLⱼ) ← B
6     Lₖ ∼ U(Lall) (Lₖ ≠ Lⱼ)
7     qLₖ ← M([cLᵢ,ⱼ, qLᵢ, yₖ])
       // Dịch qLᵢ → qLₖ
8     aLⱼ ← M([cLᵢ,ⱼ, qLᵢ, qLₖ, yₖ])
       // Trả lời bằng ngôn ngữ Lⱼ
9     B ← B ∪ (x'ₖ, yₖ, tₖ)
10  Tối ưu hóa M với B
11  i ← i + 1
12 return M

minh họa bao gồm B truy vấn của ngôn ngữ Lᵢ và các phản hồi của Lⱼ. Đối với mỗi ngôn ngữ, chúng tôi xây dựng khoảng 22K mẫu ngữ cảnh minh họa dữ liệu.

Điều chỉnh Hướng dẫn Đa ngôn ngữ Cho kho dữ liệu hướng dẫn đa ngôn ngữ D = {DLₖ}ᴷₖ₌₁, trong đó D chứa K ngôn ngữ và Lall = {Lₖ}ᴷₖ₌₁. LLM được huấn luyện chung trên hợp của kho dữ liệu đa ngôn ngữ D:

Lₓ = -∑ᴷᵢ₌₁ E(cLᵢ,qLᵢ,aLⱼ∼DLᵢ)[log P(aLⱼ|qLᵢ, cLᵢ; M)] (2)

trong đó qLᵢ là truy vấn của ngôn ngữ Lᵢ và aLⱼ là phản hồi của ngôn ngữ Lⱼ.

3.3 Học tập Trong ngữ cảnh Đa ngôn ngữ
Để khuyến khích căn chỉnh đa ngôn ngữ giữa các ngôn ngữ khác nhau, chúng tôi xây dựng truy vấn chuyển đổi mã bằng cách thay thế các span của truy vấn nguồn bằng các đối tác của ngôn ngữ đích.

Chuỗi Chuyển đổi Mã Cho một truy vấn song ngữ (qLᵢ, qLⱼ) với truy vấn ngôn ngữ nguồn qLᵢ = {qLᵢ₁,..., qLᵢₘ} có m token và bản dịch đích qLⱼ = {yLⱼ₁,..., yLⱼₙ} có n token, chúng tôi tạo chuỗi chuyển đổi mã qLᵢ,ⱼ bằng cách

--- TRANG 4 ---
thay thế cụm từ qLᵢⱼ bằng bản dịch tương đương qLⱼᵥ₁:ᵥ₂, trong đó qLᵢᵥ₁:ᵥ₂ là bản dịch đích của đoạn nguồn qLⱼᵤ₁:ᵤ₂. qLᵢᵤ₁:ᵤ₂ biểu thị cụm từ trong qLᵢ từ token thứ u1 đến token thứ u2 và qLⱼᵥ₁:ᵥ₂ biểu thị cụm từ trong qLⱼ từ token thứ v1 đến token thứ v2 (1 ≤ v1 ≤ v2 ≤ n). Đối với mỗi cụm từ trong chuyển đổi mã trong qLᵢ,ⱼ, nó đến từ cụm từ nguồn qLᵢᵤ₁:ᵤ₂ hoặc cụm từ đích qLⱼᵥ₁:ᵥ₂. Tỷ lệ của các từ nguồn trong chuỗi chuyển đổi mã qLᵢ,ⱼ được ký hiệu là α. Lᵢ,ⱼ chứa qLᵢ/ⱼ (câu nguồn với token đích) và qLᵢ/ⱼ (câu đích với token đích).

Cụ thể, chuỗi chuyển đổi mã có thể được tạo theo hai cách: (1) qLᵢ/ⱼ (câu nguồn với token đích): hầu hết các token trong qLᵢ/ⱼ xuất phát từ qLᵢ, trong đó một số cụm từ nguồn qLᵢᵤ₁:ᵤ₂ được thay thế bằng các cụm từ đích tương ứng qLᵢ₂ᵥ₁:ᵥ₂ (α ≥ 0.5). (2) qLⱼ/ᵢ (câu đích với token nguồn): hầu hết các token trong qLⱼ,ᵢ xuất phát từ qLⱼ trong đó một số cụm từ đích qLⱼᵥ₁,ᵥ₂ được thay thế bằng các cụm từ nguồn tương ứng qLᵢᵤ₁,ᵤ₂ (α < 0.5).

3.4 CoT Trực tuyến Ngẫu nhiên
Để buộc mô hình hiểu các truy vấn đa ngôn ngữ, chúng tôi giới thiệu CoT trực tuyến ngẫu nhiên (Random-CoT), trước tiên nhắc LLM dịch truy vấn qLᵢ₁ sang ngôn ngữ khác qLᵢ₂ và sau đó trả lời bằng aLⱼ trong quá trình điều chỉnh LLM.

CoT Trực tuyến Ngẫu nhiên Để mở rộng quy mô lên CoT đa ngôn ngữ, chúng tôi thực hiện CoT trực tuyến bằng cách lấy mẫu ngẫu nhiên các ngôn ngữ trung gian Lᵢ₂. Thuật toán 1 mô tả chi tiết của Random-CoT, trong đó cho thể hiện huấn luyện (cLᵢ₁,ⱼ, qLᵢ₁, aLⱼ) ∈ D, chúng tôi lấy mẫu đồng nhất một ngôn ngữ trung gian Lᵢ₂ (Lᵢ₂ ≠ Lᵢ₁) và nhắc LLM đầu tiên dịch qLᵢ₁ thành qLᵢ₂. Mặc dù Lᵢ₂ có thể thuộc về các ngôn ngữ có ít tài nguyên và chất lượng của qLᵢ₂ có thể kém ban đầu, phương pháp của chúng tôi vẫn hưởng lợi từ tín hiệu dịch thuật của qLᵢ₁ → qLᵢ₂ bằng cách căn chỉnh biểu diễn của các ngôn ngữ khác nhau.

3.5 Chưng cất Đa ngôn ngữ
Để tiếp tục tăng cường điều chỉnh hướng dẫn đa ngôn ngữ, chúng tôi sử dụng LLM đã điều chỉnh tinh M để tạo ra phản hồi tổng hợp của các truy vấn đa ngôn ngữ và sau đó chọn các đường dẫn lý luận chính xác làm bộ dữ liệu tăng cường D'. Cuối cùng, mô hình của chúng tôi được huấn luyện trên bộ dữ liệu gốc và bộ dữ liệu tăng cường D ∪ D'.

Sau đó, chúng tôi sử dụng mẫu có nhiều tài nguyên để giám sát mẫu có ít tài nguyên để chuyển giao kiến thức từ ngôn ngữ có nhiều tài nguyên sang ngôn ngữ có ít tài nguyên. Cho mẫu có nhiều tài nguyên song song (cLᵢ,ⱼ, qLᵢ, aLⱼ) và mẫu có ít tài nguyên (cLₖ,ⱼ, qLₖ, aLⱼ), mô hình riêng biệt dự đoán phân phối đích p(aLⱼ|cLᵢ,ⱼ, qLᵢ) và p(aLⱼ|cLₖ,ⱼ, qLₖ). Vì qLᵢ và qLₖ có ý nghĩa tương đương về mặt ngữ nghĩa, chúng tôi có thể tận dụng phân phối Pₕᵢgₕ = p(aLⱼ|cLᵢ,ⱼ, qLᵢ) để giám sát Pₗₒw = p(aLⱼ|cLₖ,ⱼ, qLₖ) ở cấp độ token:

Lₐ = -1/n ∑ⁿₜ₌₁ Pᵗₕᵢgₕ log Pᵗₗₒw (3)

trong đó Pᵗₕᵢgₕ và Pᵗₗₒw là phân phối token thứ t trong câu trả lời. Thông qua phân phối đa ngôn ngữ cấp độ token, chúng tôi chuyển giao kiến thức có nhiều tài nguyên sang các ngôn ngữ có ít tài nguyên.

4 Thực nghiệm
4.1 Điều chỉnh Tinh Có giám sát Đa ngôn ngữ
Đối với mỗi câu hỏi trong bộ dữ liệu, chúng tôi chọn ngẫu nhiên 2 câu hỏi khác và các câu trả lời tương ứng làm ngữ cảnh. Chúng tôi đặt 0 < α < 1 với ngưỡng thay thế 0.8 để thực hiện thao tác chuyển đổi mã trên câu hỏi trong ngữ cảnh. Cụ thể, chúng tôi sử dụng tiếng Anh làm ngôn ngữ nguồn, và ngôn ngữ tương ứng với câu hỏi được sử dụng làm ngôn ngữ đích. Đối với câu hỏi tiếng Anh, chúng tôi chọn ngẫu nhiên một ngôn ngữ khác làm ngôn ngữ đích. Chúng tôi triển khai mô hình của mình dựa trên Llama-2-7B, Llama-2-13B và Bloom-7b1. Chúng tôi điều chỉnh tinh các mô hình này với 3 epoch và sử dụng bộ lập lịch cosine với tốc độ học 2e-5 và đặt 3% warm up. Đối với chưng cất đa ngôn ngữ, chúng tôi đặt β = 0.3.

4.2 Đánh giá
Để đánh giá toàn diện khả năng thành thạo đa ngôn ngữ của XCOT, chúng tôi đánh giá phương pháp này sử dụng benchmark MGSM (Shi et al., 2023), mở rộng bộ dữ liệu GSM8K (Cobbe et al., 2021) tiếng Anh thành mười ngôn ngữ đa dạng về mặt loại hình học thông qua việc dịch thủ công các bài toán. Để tiến hành đánh giá kỹ lưỡng và rộng rãi về kỹ năng giải toán đa ngôn ngữ, chúng tôi cũng đã tạo ra một bộ dữ liệu kiểm tra ngoài domain bổ sung gọi là MSVAMP (Chen et al., 2023a), xuất phát từ bộ dữ liệu SVAMP (Patel et al., 2021). Bộ dữ liệu này kết hợp các bài toán toán học trong 10 ngôn ngữ khác nhau, ban đầu được dịch bằng dịch máy và sau đó được tinh chỉnh thông qua việc xem xét và sửa chữa cẩn thận của con người để đảm bảo độ chính xác và sắc thái. Cuối cùng, phương pháp của chúng tôi

--- TRANG 5 ---
[Bảng hiệu suất đa ngôn ngữ trên benchmark MGSM và MSVAMP với các kết quả chi tiết cho từng ngôn ngữ và mô hình]

được đánh giá trên MGSM (Shi et al., 2023) và MSVAMP (Chen et al., 2023a) với chỉ số độ chính xác. Trong các thực nghiệm, chúng tôi báo cáo độ chính xác của tất cả các phương pháp.

4.3 Baseline
XCOT chủ yếu được so sánh với: (1) LLM nguồn đóng GPT-3.5, GPT-4; (2) các mô hình nguồn mở Llama-2 và Bloom. XCOT chủ yếu tiến hành thực nghiệm dựa trên Llama-2 và so sánh với các phương pháp dựa trên Llama-2 khác như RFT, MathOctopus, MAmmoTH, WizardMath, v.v. Hơn nữa, chúng tôi chọn Bloom làm mô hình cơ sở để khám phá hiệu suất của XCOT khi kết hợp với LLM đa ngôn ngữ.

4.4 Kết quả Chính
MGSM Bảng 1 trình bày kết quả của phương pháp chúng tôi và các baseline trước đây trên MGSM của 11 ngôn ngữ, bao gồm En, De, En, Fr, Es, Ru, Zh, Ja, Th, Te, Bn, Sw. So với baseline nguồn mở Llama-2, MAmmoTH (Yue et al., 2023) được huấn luyện với sự kết hợp của chuỗi suy nghĩ (CoT) và chương trình suy nghĩ (PoT) đạt được cải thiện mạnh mẽ. Phương pháp của chúng tôi vượt trội đáng kể so với baseline mạnh trước đây MAmmoTH với trung bình điểm. Điều này có thể chứng minh rằng phương pháp của chúng tôi có thể tận dụng học tập trong ngữ cảnh đa ngôn ngữ (xICL) và CoT trực tuyến ngẫu nhiên (Random-CoT) để khuyến khích căn chỉnh giữa các ngôn ngữ khác nhau.

MSVAMP Bảng 2 so sánh hiệu suất của phương pháp chúng tôi với các phương pháp liên quan trước đây trên MSVAMP của 10 ngôn ngữ. Baseline đa ngôn ngữ mạnh gần đây MathOctopus đánh bại các baseline trước đây MAmmoTH và WizardMath với sự hỗ trợ của bộ dữ liệu hướng dẫn đa ngôn ngữ MGSM8KInstruct. Hơn nữa, phương pháp đề xuất của chúng tôi đạt được hiệu suất tốt nhất 42.9 điểm ở mức 7B trên tất cả các ngôn ngữ, chứng minh rằng khung đề xuất của chúng tôi tăng cường khả năng chuyển giao từ các ngôn ngữ có nhiều tài nguyên sang tất cả các ngôn ngữ khác.

5 Phân tích
Nghiên cứu Loại bỏ Để xác minh tính hiệu quả của từng module trong phương pháp của chúng tôi, chúng tôi tiến hành nghiên cứu loại bỏ bằng cách thêm các module dần dần. LLM đa ngôn ngữ Llama-7B đầu tiên được huấn luyện trên kho dữ liệu đa ngôn ngữ XCOT-INSTRUCT, trong đó mô hình được ký hiệu là ①. So với mô hình ban đầu

--- TRANG 6 ---
[Các bảng phân tích chi tiết về số lượng đường dẫn lý luận khác nhau, nghiên cứu loại bỏ, và so sánh các phương pháp prompting đa ngôn ngữ]

①, mô hình ② với ngữ cảnh chuyển đổi mã trong điều chỉnh đa ngôn ngữ đạt được cải thiện +4.7 điểm trung bình, cho thấy việc sử dụng xICL trong việc khuyến khích căn chỉnh giữa các ngôn ngữ khác nhau. Sau đó, mô hình ③ được tăng cường thêm với mSampling với biên độ lớn +5.8 điểm, trong đó mô hình tạo ra các phản hồi đa ngôn ngữ và chọn các đường dẫn lý luận chính xác làm bộ dữ liệu tăng cường. Trong quá trình điều chỉnh đa ngôn ngữ, phương pháp của chúng tôi áp dụng Random-CoT để đầu tiên dịch truy vấn sang ngôn ngữ khác và sau đó trả lời bằng tiếng Anh. Đối với phân phối đầu ra, phân phối có nhiều tài nguyên được sử dụng để giám sát phân phối có ít tài nguyên (xDistill). Kết hợp tất cả lại với nhau, chúng tôi có được mô hình cuối cùng XCOT (⑤) với 47.7 điểm. Bảng 4 tóm tắt kết quả của nghiên cứu loại bỏ về chuyển giao đa ngôn ngữ trong các phần khác nhau, nhấn mạnh tính hiệu quả của chuyển giao đa ngôn ngữ có thể cải thiện dần hiệu suất trong các khía cạnh khác nhau.

Prompting Đa ngôn ngữ Để kích hoạt khả năng tiềm năng đa ngôn ngữ của LLM, chúng tôi giới thiệu xICL để buộc mô hình hiểu các truy vấn đa ngôn ngữ và căn chỉnh biểu diễn của chúng. Để thúc đẩy thỏa thuận đa ngôn ngữ trong điều chỉnh hướng dẫn, chúng tôi thay thế ngẫu nhiên một số đoạn của ngôn ngữ nguồn trong các ví dụ bằng các bản dịch tương ứng của ngôn ngữ đích cho học tập few-shot trong ngữ cảnh đa ngôn ngữ (xICL). Bảng 6 cho thấy kết quả của XCOT với ngữ cảnh tiếng Anh, ngữ cảnh gốc và ngữ cảnh chuyển đổi mã trên các backbone khác nhau. Truy vấn được trộn với các token ngôn ngữ khác nhau mang lại cải thiện đáng kể trong các ngôn ngữ khác nhau.

Nghiên cứu Ví dụ Cho các truy vấn của các ngôn ngữ khác nhau, phương pháp của chúng tôi nhắc LLM đầu tiên xem xét truy vấn đa ngôn ngữ bằng tiếng Anh và sau đó trả lời bằng tiếng Anh. Bảng 7 cho thấy các ví dụ về baseline tiếng Tây Ban Nha, tiếng Trung và tiếng Đức. Chúng tôi quan sát thấy Llama-2 có xu hướng tạo ra các câu trả lời không chính xác cho các truy vấn không phải tiếng Anh. Đối với câu hỏi tiếng Đức "Jimmy hat 2 $ mehr als doppelt so viel Geld wie Ethel. Wenn Ethal 8 $ hat, wie viel Geld hat dann Jimmy im Moment?", phương pháp của chúng tôi đầu tiên suy nghĩ về truy vấn không phải tiếng Anh bằng tiếng Anh "Question: Jimmy has $2 more than twice the money Ethel has. If Ethel has $8, how much money does Jimmy have now?". và sau đó trả lời bằng tiếng Anh. Điều này chứng minh rằng phương pháp của chúng tôi có thể căn chỉnh cả truy vấn và phản hồi giữa các ngôn ngữ khác nhau.

Đường dẫn Lý luận Đa ngôn ngữ Dữ liệu hướng dẫn đa ngôn ngữ của chúng tôi được tăng cường bằng lấy mẫu đa ngôn ngữ, trong đó LLM đã điều chỉnh tinh tạo ra phản hồi và chọn đường dẫn chính xác. Bảng 3 cho thấy các ngôn ngữ khác nhau có số lượng đường dẫn lý luận tương tự, chứng minh rằng việc sử dụng CoT đa ngôn ngữ chuyển giao thành công các mẫu lý luận từ ngôn ngữ này sang ngôn ngữ khác. XCOT có thể tích lũy tất cả các đường dẫn lý luận để cải thiện hiệu suất mô hình.

Biểu diễn Đa ngôn ngữ Chúng tôi chọn ngẫu nhiên 250 truy vấn song song với các ví dụ 2-shot của mỗi ngôn ngữ trong XCOT-INSTRUCT và trực quan hóa biểu diễn của chúng (Maaten và Hinton, 2008) của các lớp decoder Llama cuối cùng trong Hình 3 sử dụng mô hình đa ngôn ngữ của chúng tôi được điều chỉnh tinh trên XCOT-INSTRUCT và baseline đa ngôn ngữ. Trạng thái ẩn đầu tiên của encoder được áp dụng làm biểu diễn câu. So với Hình 3(a) của baseline, các ngôn ngữ khác nhau trở nên gần nhau hơn và có nhiều khả năng chồng chéo với nhau trong Hình 3(b) của phương pháp chúng tôi, chứng minh rằng phương pháp của chúng tôi căn chỉnh hiệu quả biểu diễn của các ngôn ngữ khác nhau vào không gian chung.

Hiểu và Lý luận bằng tiếng Anh Sau SFT đa ngôn ngữ với Random-CoT, XCOT chọn ngôn ngữ có nhiều tài nguyên (tiếng Anh) làm ngôn ngữ phụ trợ để hiểu và trả lời câu hỏi không phải tiếng Anh. Trong Hình 4, phương pháp của chúng tôi

--- TRANG 7 ---
[Tiếp tục các bảng phân tích và ví dụ so sánh]

sử dụng "Let's think step by step in English" để trả lời câu hỏi tiếng Anh. Đối với câu hỏi không phải tiếng Anh, chúng tôi áp dụng "Let's think the question in {Language} and then think step by step in English", trong đó {Language} có thể là các ngôn ngữ có nhiều tài nguyên trong điều chỉnh SFT nhưng chúng tôi đặt {Language} là tiếng Anh trong giai đoạn suy luận. Để chuyển giao hiệu quả kiến thức từ ngôn ngữ có nhiều tài nguyên sang ngôn ngữ có ít tài nguyên, chúng tôi buộc LLM hiểu truy vấn bằng tiếng Anh và sau đó suy nghĩ bằng tiếng Anh.

Phân tích trong Random-CoT Để tạo điều kiện cho việc căn chỉnh giữa các ngôn ngữ khác nhau, câu hỏi của ngôn ngữ Li1 đầu tiên được dịch sang ngôn ngữ khác Li2 trong điều chỉnh SFT. Cho truy vấn của ngôn ngữ Li1, chúng ta có thể dịch sang ngôn ngữ khác Li2 (Li1 ≠ Li2). Chiến lược "Lall → Le" biểu thị rằng Li1 ∈ Lall ∧ Li2 = Le. Bảng 5 cho thấy kết quả của phương pháp chúng tôi với các chiến lược Random-CoT khác nhau và chiến lược "Lall → Lhigh" đạt được hiệu suất tốt nhất, có thể được quy cho việc chuyển giao ngôn ngữ từ ngôn ngữ có nhiều tài nguyên sang ngôn ngữ có ít tài nguyên.

Cài đặt Ít tài nguyên Hình 5 vẽ kết quả đánh giá đa ngôn ngữ của XCOT với các kích thước dữ liệu SFT khác nhau. Chúng tôi quan sát thấy phương pháp của chúng tôi với gần 20% dữ liệu SFT vẫn có thể đánh bại baseline mạnh Llama-7B, có thể quy cho sự tăng cường lẫn nhau của nhiều ngôn ngữ.

6 Công trình Liên quan
Mô hình Ngôn ngữ Lớn Các mô hình ngôn ngữ lớn (LLMs) đã cho thấy sức mạnh to lớn trong nhiều tác vụ NLP, và khi quy mô của mô hình trở nên lớn hơn, LLMs nổi lên với những khả năng đáng ngạc nhiên (Touvron et al., 2023c; Wei et al., 2022b; Du et al., 2022; Guo et al., 2023), như tuân theo hướng dẫn của con người, học tập trong ngữ cảnh, và lý luận các tác vụ phức tạp. Wei et al. (2022d) phát hiện rằng LLM có thể giải quyết các vấn đề phức tạp một cách hiệu quả bằng chiến lược prompting chuỗi suy nghĩ (cung cấp một số exemplar chứa các bước lý luận để hướng dẫn mô hình tạo ra các bước lý luận trung gian). Hơn nữa, Kojima et al. (2022b) phát hiện rằng LLMs có thể giải quyết các vấn đề phức tạp bằng CoT ngay cả khi không

--- TRANG 8 ---
cung cấp exemplar. Tuy nhiên, khả năng CoT thường yêu cầu mô hình có số lượng tham số đặc biệt lớn và đòi hỏi tài nguyên tính toán khổng lồ. Cũng có một số công trình (Ho et al., 2022; Zhu et al., 2023) khám phá khả năng CoT của các LLM nhỏ hơn. Trong bài báo này, chúng tôi tập trung vào khả năng CoT cho các LLM nhỏ hơn và tiếp tục di chuyển nó sang lý luận đa ngôn ngữ.

Chuyển giao Đa ngôn ngữ Chuyển giao đa ngôn ngữ liên quan đến việc sử dụng dữ liệu có nhãn từ ngôn ngữ tài nguyên để giải quyết thách thức của dữ liệu có nhãn không đủ trong ngôn ngữ đích. Các công trình trước đây (Conneau và Lample, 2019; Conneau et al., 2020; Yang et al., 2020; Ma et al., 2020; Yang et al., 2023) chứng minh rằng các mô hình được huấn luyện trước trên dữ liệu đa ngôn ngữ thực hiện thành thạo các tác vụ chuyển giao đa ngôn ngữ. Các mô hình được huấn luyện trước đa ngôn ngữ này đã được ứng dụng rộng rãi trên nhiều tác vụ NLP downstream khác nhau, như dịch đa ngôn ngữ (Tan et al., 2019; Yang et al., 2022b; Gaschi et al., 2023; Yang et al., 2022c), tóm tắt đa ngôn ngữ (Bhattacharjee et al., 2023; Wang et al., 2023), trích xuất thông tin đa ngôn ngữ (Zhou et al., 2022; Yang et al., 2022a; Wu et al., 2020). Nhiều LLMs được huấn luyện trên dữ liệu đa ngôn ngữ, ban cho chúng khả năng đa ngôn ngữ mạnh mẽ (Scao et al., 2022; Muennighoff et al., 2022). Tuy nhiên, khả năng đa ngôn ngữ trong LLM nhỏ hơn không đáng kể, vì vậy chúng tôi tăng cường tiềm năng lý luận đa ngôn ngữ của LLMs bằng cách sử dụng dữ liệu huấn luyện giả có được từ các bộ dữ liệu ngôn ngữ nguồn có nhãn.

7 Kết luận
Trong công trình này, chúng tôi đề xuất một khung điều chỉnh tinh hướng dẫn đa ngôn ngữ (XCOT) để giải quyết sự chênh lệch bằng cách khuyến khích căn chỉnh giữa các ngôn ngữ khác nhau. Một bộ dữ liệu hướng dẫn đa ngôn ngữ (XCOT-INSTRUCT) đầu tiên được tạo ra để căn chỉnh về mặt ngữ nghĩa khả năng lý luận giữa các ngôn ngữ khác nhau. Sau đó, phương pháp của chúng tôi kết hợp Học tập Trong ngữ cảnh Đa ngôn ngữ (xICL) để kích hoạt căn chỉnh đa ngôn ngữ. Trong quá trình điều chỉnh hướng dẫn, chúng tôi áp dụng CoT trực tuyến ngẫu nhiên (Random-CoT), nhắc LLM dịch truy vấn sang các ngôn ngữ khác nhau và sau đó cung cấp phản hồi tiếng Anh. Để tiếp tục thúc đẩy chuyển giao ngôn ngữ, chúng tôi tận dụng CoT có nhiều tài nguyên để hướng dẫn việc huấn luyện CoT có ít tài nguyên với chưng cất đa ngôn ngữ (xDistill). Đánh giá toàn diện của chúng tôi trên các benchmark đã thiết lập thể hiện tính hiệu quả của XCOT trong việc thu hẹp khoảng cách ngôn ngữ đa ngôn ngữ. Kết quả làm nổi bật tiềm năng của nó như một giải pháp mạnh mẽ để giảm phân chia đa ngôn ngữ, tạo ra một tiền lệ mới cho hiệu suất mô hình ngôn ngữ đa ngôn ngữ.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ với các chi tiết về tác giả, tiêu đề, và thông tin xuất bản cho tất cả các công trình được trích dẫn trong bài báo]
