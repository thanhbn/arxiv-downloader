AutoMoE: Hỗn hợp Chuyên gia Dị thể với Tính toán Thích ứng cho Dịch máy Thần kinh Hiệu quả

Ganesh Jawahar∗ ♣, Subhabrata Mukherjee♠
Xiaodong Liu♠, Young Jin Kim♠, Muhammad Abdul-Mageed♣♢, Laks V .S. Lakshmanan♣
Ahmed Hassan Awadallah♠, Sebastien Bubeck♠, Jianfeng Gao♠
♣Đại học British Columbia,♠Microsoft Research,♢MBZUAI

Tóm tắt
Các mô hình Mixture-of-Expert (MoE) đã đạt được hiệu suất tối ưu trong các nhiệm vụ Dịch máy Thần kinh (NMT). Các nghiên cứu hiện tại về MoE chủ yếu xem xét thiết kế đồng nhất nơi cùng một số lượng chuyên gia có cùng kích thước được đặt đồng đều trong toàn bộ mạng. Hơn nữa, các nghiên cứu MoE hiện tại không xem xét các ràng buộc tính toán (ví dụ: FLOPs, độ trễ) để hướng dẫn thiết kế của chúng. Để giải quyết vấn đề này, chúng tôi phát triển AutoMoE – một khung để thiết kế MoE dị thể dưới các ràng buộc tính toán. AutoMoE tận dụng Neural Architecture Search (NAS) để có được các sub-transformer MoE thưa thớt hiệu quả với tăng tốc suy luận 4× (CPU) và giảm FLOPs so với các Transformer được thiết kế thủ công, với hiệu suất tương đương về điểm BLEU so với Transformer dày đặc và trong vòng 1 điểm BLEU của MoE SwitchTransformer, trên tổng hợp các tập dữ liệu chuẩn cho NMT. Không gian tìm kiếm dị thể với các mô-đun Transformer dày đặc và được kích hoạt thưa thớt (ví dụ: có bao nhiêu chuyên gia? đặt chúng ở đâu? kích thước của chúng nên như thế nào?) cho phép tính toán thích ứng – nơi các lượng tính toán khác nhau được sử dụng cho các token khác nhau trong đầu vào. Tính thích ứng đến một cách tự nhiên từ các quyết định định tuyến gửi token đến các chuyên gia có kích thước khác nhau. Mã nguồn, dữ liệu và các mô hình đã được huấn luyện của AutoMoE có sẵn tại https://aka.ms/AutoMoE.

1 Giới thiệu
Các mô hình được kích hoạt thưa thớt như Mixture-of-Experts (MoE) (Fedus và cộng sự, 2022b) thực hiện tính toán có điều kiện trong đó chỉ một tập con các trọng số của mạng được kích hoạt cho mỗi đầu vào. Tính toán có chọn lọc cho phép chúng ta thiết kế các mạng thần kinh với số lượng lớn các tham số mô hình mà không tăng đáng kể chi phí tính toán. Với khả năng tăng cường, các mô hình thưa thớt này đã chứng minh hiệu suất tối ưu trong các nhiệm vụ ngôn ngữ tự nhiên như dịch máy thần kinh (NMT) (Kim và cộng sự, 2021; Kudugunta và cộng sự, 2021; Zuo và cộng sự, 2022).

Các kiến trúc MoE yêu cầu một số lựa chọn thiết kế: (a) Vị trí đặt chuyên gia: Xác định các lớp Transformer để giới thiệu các mạng con chuyên gia. (b) Số lượng chuyên gia: Có bao nhiêu chuyên gia cần đặt trong các lớp khác nhau? (c) Kích thước FFN chuyên gia: Kích thước của mạng feedforward (FFN) cho mỗi chuyên gia nên như thế nào? Với không gian tìm kiếm lớn của các kiến trúc tiềm năng và chi phí tính toán cực kỳ cao để huấn luyện và đánh giá chúng, các phương pháp hiện tại thiết kế thủ công các kiến trúc MoE từ một không gian đồng nhất bị hạn chế cao. Ví dụ, chúng sử dụng cùng một số lượng chuyên gia có cùng khả năng trong các lớp khác nhau và đưa ra các quyết định tùy ý như giới thiệu chuyên gia trong mỗi lớp khác (Fedus và cộng sự, 2022b; Kim và cộng sự, 2021; Zuo và cộng sự, 2022; Du và cộng sự, 2022; Artetxe và cộng sự, 2021) hoặc mỗi bốn lớp (Zoph và cộng sự, 2022).

Mặc dù các MoE này hỗ trợ tính toán có điều kiện, tính đồng nhất (cụ thể là các chuyên gia có kích thước cố định) dẫn đến cùng một lượng (mặc dù là các tập con khác nhau) trọng số được áp dụng cho mỗi đầu vào. Chúng tôi đưa ra giả thuyết rằng đây không phải là giải pháp tối ưu và chúng ta có thể giảm số lượng chuyên gia (trong một số lớp) để giảm chi phí giao tiếp, và kích thước (của một số chuyên gia) để giảm chi phí tính toán dẫn đến giảm kích thước mô hình, FLOPs và độ trễ mà không suy giảm chất lượng nhiều.

Điều này tự nhiên mở rộng MoE để trở thành các mô hình tính toán thích ứng (tương tự như công việc về thoát sớm (Schuster và cộng sự, 2022)) nơi các lượng tính toán khác nhau được sử dụng cho các đầu vào khác nhau. Tính thích ứng đến một cách tự nhiên từ các quyết định định tuyến sẽ gửi token đến các chuyên gia có kích thước khác nhau.

Các quan sát trên được mô tả trong Bảng 1, cho thấy các ví dụ minh họa về MoE được thiết kế thủ công so với những cái được thiết kế bởi khung AutoMoE của chúng tôi. Chúng tôi so sánh các kiến trúc này với các chỉ số tính toán khác nhau (ví dụ: độ trễ, FLOPs, tham số MoE hoạt động), cấu hình kiến trúc và hiệu suất nhiệm vụ. Đối với cấu hình hiệu quả nhất (hàng cuối cùng trong bảng), AutoMoE giảm số lượng lớp decoder, bù đắp khả năng với tăng chuyên gia ở lớp dưới cùng, và đặt hầu hết các chuyên gia trong encoder. Nhìn chung, AutoMoE giới thiệu các thành phần và đóng góp sau:

• Thiết kế dị thể với tính toán thích ứng cho MoE với số lượng, kích thước và vị trí đặt chuyên gia thay đổi trong cả encoder và decoder.

• Mở rộng huấn luyện Supernet và tìm kiếm tiến hóa từ công việc trước đây về Transformer dày đặc đến không gian tìm kiếm mới của MoE thưa thớt. Điều này kết hợp tất cả các kiến trúc con MoE có thể trong một đồ thị duy nhất; huấn luyện chúng cùng nhau thông qua chia sẻ trọng số; và tìm kiếm cái tối ưu với hiệu suất tốt nhất có thể trên một nhiệm vụ downstream thỏa mãn ràng buộc tính toán do người dùng chỉ định.

• Các thí nghiệm trên các chuẩn NMT chứng minh AutoMoE-designed MoE đạt được tăng tốc suy luận 4× trên CPU và giảm FLOPs tương đương so với Transformer được thiết kế thủ công, với hiệu suất tương đương về BLEU với Transformer dày đặc và trong vòng 1 điểm BLEU của MoE SwitchTransformer. Hơn nữa, nó vượt trội hơn các phương pháp NAS trong không gian tìm kiếm dày đặc (ví dụ: giảm FLOPs 1.3× và 2.4× và tăng tốc suy luận so với HAT (Wang và cộng sự, 2020) và Evolved Transformer (So và cộng sự, 2019)).

2 Kiến thức nền tảng
Mixture-of-Experts: MoE có một văn học phong phú trong học máy từ đầu những năm 90 (Yuksel và cộng sự, 2012). Chúng đã nhận được sự chú ý đáng kể với các nghiên cứu như (Shazeer và cộng sự, 2017), Switch Transformers (Fedus và cộng sự, 2022b), GShard (Lepikhin và cộng sự, 2020), BASE (Lewis và cộng sự, 2021), Hash (Roller và cộng sự, 2021), GLaM (Du và cộng sự, 2022), Stochastic Experts (Zuo và cộng sự, 2022), Gating Dropout (Liu và cộng sự, 2022) và ST-MoE (Zoph và cộng sự, 2022). Một số khác biệt quan trọng trong các nghiên cứu này bao gồm lựa chọn hàm định tuyến chuyên gia, kỹ thuật đặt chuyên gia, kỹ thuật nâng cao ổn định/hiệu suất và bản chất của nhiệm vụ (huấn luyện trước so với tinh chỉnh). Một số thách thức trong việc xây dựng các mô hình chuyên gia thưa thớt bao gồm: (i) thiếu đa dạng trong thiết kế chuyên gia (lựa chọn lớp chuyên gia, số lượng chuyên gia, kích thước chuyên gia, v.v.), (ii) bất ổn định huấn luyện, (iii) khả năng tổng quát hóa kém ngoài phân phối, (iv) thích ứng chéo nhiệm vụ của các mô hình được huấn luyện trước, (v) nút thắt cổ chai giao tiếp, (vi) bộ nhớ cao và (vii) vấn đề cân bằng tải chuyên gia, để kể một số. Một đánh giá toàn diện về các mô hình chuyên gia thưa thớt gần đây có thể tìm thấy tại (Fedus và cộng sự, 2022a).

Thiết kế MoE: Hầu hết các nghiên cứu về MoE dựa vào các lựa chọn thủ công tùy ý cho vị trí đặt chuyên gia, số lượng chuyên gia và kích thước của chúng. Các phương pháp hiện tại chủ yếu sử dụng thiết kế thủ công, nơi chúng thêm chuyên gia vào (i) các lớp xen kẽ (Fedus và cộng sự, 2022b; Kim và cộng sự, 2021; Zuo và cộng sự, 2022; Du và cộng sự, 2022; Artetxe và cộng sự, 2021), (ii) mỗi bốn lớp (Zoph và cộng sự, 2022), hoặc (iii) một vài lớp cuối cùng (Rajbhandari và cộng sự, 2022). Mặc dù các MoE này hỗ trợ tính toán có điều kiện, chúng thường không hỗ trợ tính toán thích ứng vì cùng một số lượng tham số chuyên gia áp dụng cho mỗi đầu vào, phần lớn được đưa ra bởi thiết kế đồng nhất của chúng (ví dụ: tất cả chuyên gia cùng kích thước). Hơn nữa, thiết kế MoE thường bất khả tri với các ràng buộc tính toán (ví dụ: độ trễ, bộ nhớ) của phần cứng mà mô hình MoE phải được triển khai.

Neural Architecture Search (NAS): Với một không gian tìm kiếm kiến trúc và các ràng buộc hiệu quả (ví dụ: kích thước mô hình, độ trễ), NAS thường nhằm mục đích xác định kiến trúc tối ưu tối đa hóa hiệu suất nhiệm vụ, trong khi thỏa mãn các ràng buộc hiệu quả. NAS gần đây đã được sử dụng cho các nhiệm vụ hiểu ngôn ngữ tự nhiên để xây dựng các mô hình ngôn ngữ được huấn luyện trước dựa trên BERT (Devlin và cộng sự, 2019) và GPT (Brown và cộng sự, 2020) hiệu quả (Xu và cộng sự, 2021; Yin và cộng sự, 2021; Xu và cộng sự, 2022a,b; Gao và cộng sự, 2022; Dong và cộng sự, 2021; So và cộng sự, 2021; Javaheripi và cộng sự, 2022) cũng như cho các nhiệm vụ dịch máy (So và cộng sự, 2019; Wang và cộng sự, 2020). Hardware aware transformers (HAT) (Wang và cộng sự, 2020) là một khung NAS tối ưu với Transformer dày đặc cho MT sử dụng độ trễ phần cứng làm phản hồi để tối ưu hóa.

Tuy nhiên, tất cả các nghiên cứu NAS trên xem xét một không gian tìm kiếm với các Transformer được kích hoạt dày đặc và kiến trúc không phải MoE. Chúng chủ yếu tìm kiếm các siêu tham số kiến trúc Transformer điển hình như số lượng lớp, đầu chú ý và kích thước ẩn. Ngược lại, chúng tôi đề xuất khung NAS đầu tiên tìm kiếm các mô-đun Mixture-of-Expert được kích hoạt thưa thớt hiệu quả trong Transformer. Khung AutoMoE dị thể của chúng tôi giải quyết một số lựa chọn thiết kế lâu dài cho MoE như có bao nhiêu chuyên gia? đặt chúng ở lớp nào? kích thước của chúng nên như thế nào? và nhiều hơn nữa.

3 Thiết kế Mixture-of-Experts Dị thể
Giờ đây chúng tôi trình bày các thành phần của khung AutoMoE (được minh họa trong Hình 1) để thiết kế MoE hiệu quả dưới các ràng buộc tính toán.

3.1 Không gian Tìm kiếm MoE Dị thể
Các phương pháp MoE hiện tại hạn chế không gian thiết kế của chúng bằng cách xem xét phân phối đồng đều về kích thước và số lượng chuyên gia được đặt trong các lớp Transformer khác nhau. Ví dụ, thiết kế MoE tiêu chuẩn (Fedus và cộng sự, 2022b) cho một Transformer L-lớp với M chuyên gia được đặt trong các lớp xen kẽ chỉ có hai cấu hình có thể là {1-M-1-···}, {M-1-M-···}. (a) Không gian thiết kế của chúng tôi cho phép số lượng chuyên gia thay đổi trong mỗi lớp dẫn đến M^L cấu hình có thể. (b) Hơn nữa, không gian thiết kế của chúng tôi cũng cho phép kích thước chuyên gia thay đổi, ví dụ: bằng cách điều chỉnh chiều rộng của các mạng con feedforward (FFN) cho các chuyên gia khác nhau. Xem xét N kích thước FFN có thể cho mỗi chuyên gia dẫn đến N^(ML) cấu hình có thể để thiết kế không gian chuyên gia. (c) Cuối cùng, với bản chất tự hồi quy của các nhiệm vụ như dịch máy thần kinh, chi phí suy luận bị chi phối bởi decoder (Kasai và cộng sự, 2021). Ví dụ, đối với MoE dựa trên token, decoder mất 200× thời gian mỗi bước so với encoder ở thông lượng đỉnh (Kudugunta và cộng sự, 2021). Do đó, chúng tôi tiếp tục xem xét số lượng lớp decoder thay đổi cùng với các lựa chọn trên cho vị trí đặt chuyên gia và khả năng chuyên gia. Theo hiểu biết tốt nhất của chúng tôi, nghiên cứu của chúng tôi là đầu tiên nghiên cứu không gian thiết kế linh hoạt và toàn diện như vậy cho kiến trúc MoE.

Ngoài các chuyên gia dị thể, chúng tôi cho phép thiết kế linh hoạt cho các mô-đun Transformer không phải chuyên gia như số lượng đầu chú ý, kích thước ẩn và chiều feedforward trung gian. Thiết kế dị thể này của các mô-đun Transformer dày đặc không phải MoE đã được khám phá trong các nghiên cứu trước đây như HAT (Wang và cộng sự, 2020) cho các nhiệm vụ tạo như NMT và AutoDistil (Xu và cộng sự, 2022a) cho các nhiệm vụ hiểu như những cái trong chuẩn GLUE (Wang và cộng sự, 2018). Bảng 2 cho thấy không gian tìm kiếm của chúng tôi. Chúng tôi chứng minh tìm kiếm MoE dị thể của chúng tôi hoạt động tốt hơn cả kiến trúc được thiết kế thủ công và được tìm kiếm NAS trong không gian dày đặc.

3.2 Huấn luyện Supernet cho MoE
AutoMoE tận dụng ý tưởng huấn luyện Supernet từ các nghiên cứu trước đây (Cai và cộng sự, 2020; Xu và cộng sự, 2022a; Wang và cộng sự, 2020) trong Neural Architecture Search được phát triển cho các kiến trúc không phải MoE tiêu chuẩn. Chúng tôi mở rộng huấn luyện Supernet đến không gian tìm kiếm cho MoE bằng cách kết hợp các chuyên gia, cổng và giao thức định tuyến. Thông thường, một Supernet bao gồm hàng nghìn mạng con được huấn luyện cùng nhau thông qua chia sẻ trọng số. Supernet cho AutoMoE là MoE được kích hoạt thưa thớt lớn nhất trong không gian tìm kiếm. Nó bao gồm số lượng chuyên gia tối đa (M) được đặt trong mỗi lớp của Transformer trong cả encoder và decoder. Mỗi FFN chuyên gia có kích thước ẩn trung gian tối đa trong không gian tìm kiếm. Các nguyên tắc tương tự áp dụng cho các mô-đun dày đặc không phải chuyên gia được khởi tạo với kích thước đầy đủ tương ứng.

Supernet được huấn luyện với các bước sau: (i) lấy mẫu một kiến trúc ứng viên ngẫu nhiên từ không gian tìm kiếm (Guo và cộng sự, 2020); (ii) huấn luyện kiến trúc được lấy mẫu bằng cách trích xuất phần trọng số chung từ các lớp khác nhau trong Supernet (tức là bằng chia sẻ trọng số) cho một bước huấn luyện trên nhiệm vụ; (iii) lặp lại các bước (i) và (ii) cho đến khi ngân sách huấn luyện cạn kiệt. Một khi huấn luyện Supernet hội tụ, chúng ta có thể có được ước tính độ chính xác nhanh cho một kiến trúc ứng viên (tức là mạng con) bằng cách trích xuất trọng số được chia sẻ của nó từ Supernet và đánh giá trên tập validation.

Thách thức chính ở đây là xây dựng các kỹ thuật chia sẻ trọng số cho các thành phần MoE, bao gồm: (i) router: một mạng thần kinh được huấn luyện để định tuyến mỗi token (của 'kích thước nhúng') trong một ví dụ đến chính xác một chuyên gia (trong số M chuyên gia) cho định tuyến top-1; (ii) FFN expert: một khối FFN Transformer tiêu chuẩn có trọng số độc nhất và được học độc lập. Các lớp chuyên gia của AutoMoE tuân theo đặc tả Switch Transformer (Fedus và cộng sự, 2022b). Để trích xuất mạng con từ Supernet, AutoMoE trích xuất các hàng đầu và cột đầu của ma trận trọng số router của Supernet, tương ứng với thiết kế subnet. Ví dụ, xem xét router của Supernet được thiết kế cho 4 chuyên gia và kích thước nhúng 640 với hình dạng của ma trận trọng số router là 4×640. Xem xét một subnet được lấy mẫu trong quá trình huấn luyện Supernet bao gồm 3<4 chuyên gia và kích thước nhúng 512<640 với ma trận router của subnet là 3×512. Để điền vào ma trận này, chúng tôi trích xuất 3 hàng đầu và 512 cột đầu từ ma trận trọng số của Supernet (như được minh họa trong Hình 2 (a)). Kỹ thuật chia sẻ trọng số như vậy cho phép chúng tôi thiết kế các kiến trúc MoE dị thể với số lượng chuyên gia thay đổi trong mỗi lớp Transformer.

AutoMoE cũng trích xuất các hàng đầu và cột đầu từ ma trận trọng số của mỗi FFN expert từ Supernet, tương ứng với thiết kế subnet. Đối với ví dụ trước, giả sử kích thước FFN trung gian của mỗi chuyên gia trong Supernet là 3072 (hình dạng ma trận trọng số cho lớp FFN đầu tiên là 3072×640 và lớp FFN thứ hai là 640×3072). Giả sử subnet được lấy mẫu được thiết kế cho 2 chuyên gia với kích thước FFN trung gian của một chuyên gia là 2048 trong khi cái khác là 1024. Đối với chuyên gia đầu tiên, các ma trận trọng số của subnet có hình dạng 2048×512 (Input) và 512×2048 (Output) được trích xuất từ 2048 hàng đầu, 512 cột (Input) và 512 hàng đầu, 2048 cột (Output) của trọng số Supernet tương ứng. Đối với chuyên gia thứ hai, các ma trận trọng số có hình dạng 1024×512 (Input) và 512×1024 (Output) được trích xuất từ 1024 hàng đầu, 512 cột (Input) và 512 hàng đầu, 1024 cột (Output) của trọng số Supernet tương ứng. Ví dụ này được minh họa trong Hình 2 (b). Kỹ thuật trích xuất subnet không trích xuất trọng số từ chuyên gia thứ ba và thứ tư của Supernet vì subnet được thiết kế chỉ có hai chuyên gia (không hiển thị trong hình). Kỹ thuật chia sẻ trọng số như vậy cho phép chúng tôi thiết kế kiến trúc với kích thước FFN trung gian thay đổi cho mỗi chuyên gia. Các kỹ thuật bổ sung để cải thiện khả năng chuyên gia như xếp chồng FFN và các kỹ thuật cải thiện hiệu suất Supernet với lấy mẫu sandwich (Yu và cộng sự, 2019), chưng cất kiến thức tại chỗ (Yu và cộng sự, 2019), giảm xung đột gradient (Gong và cộng sự, 2022) được để lại cho công việc tương lai.

3.3 Tìm kiếm Mạng con MoE Hiệu quả với Ràng buộc Tính toán
Tìm kiếm AutoMoE dựa trên một thuật toán tiến hóa nhận ràng buộc tính toán phần cứng (ví dụ: độ trễ CPU ≤600ms) làm đầu vào và nhằm xác định mạng con MoE từ Supernet đạt được độ chính xác tối đa cho nhiệm vụ trong khi thỏa mãn ràng buộc. Thuật toán hoạt động bằng cách lấy mẫu một tập hợp ban đầu các kiến trúc MoE ứng viên ngẫu nhiên từ Supernet; phát triển các kiến trúc hàng đầu một cách lặp đi lặp lại bằng đột biến; theo sau bằng lai tạo; cho đến khi các lần lặp tìm kiếm cạn kiệt. Các kiến trúc MoE ứng viên dễ dàng được xếp hạng bởi bộ ước tính hiệu suất Supernet dựa trên điểm validation cho nhiệm vụ. Ước tính độ trễ cho mỗi kiến trúc được thu được bằng cách đo độ trễ trực tiếp trên thiết bị đích. Phương pháp tiêu chuẩn đo độ trễ vàng cho lan truyền xuôi của một batch ví dụ cho một số lượng lớn (ví dụ: 300) lần chạy và sau đó tính trung bình cắt ngắn (sau khi loại bỏ 10% độ trễ ngoại lệ dưới và trên). Ước tính độ trễ này có thể tốn kém với không gian lớn của các kiến trúc ứng viên. Để vượt qua thách thức này, AutoMoE sử dụng độ trễ vàng một phần, được thu được bằng lan truyền xuôi của một batch ví dụ cho một số lượng nhỏ (ví dụ: 100) lần chạy và sau đó tính trung bình cắt ngắn. Sau khi tìm kiếm hoàn thành, kiến trúc MoE với hiệu suất cao nhất được chọn làm kiến trúc tối ưu.

3.4 Huấn luyện Sub-Transformer MoE Hiệu quả
Một khi kiến trúc MoE tối ưu được xác định, chúng tôi huấn luyện trọng số mô hình cho kiến trúc cuối cùng để hội tụ cho cùng số bước huấn luyện như các mô hình cơ sở của chúng tôi để so sánh công bằng.

4 Thí nghiệm
Tập dữ liệu và chỉ số đánh giá.
Chúng tôi đánh giá AutoMoE trên các chuẩn dịch máy tiêu chuẩn: WMT'14 En-De, WMT'14 En-Fr và WMT'19 En-De với thống kê tập dữ liệu trong Bảng 3. Chúng tôi sử dụng tập dữ liệu đã được tiền xử lý và thiết lập đánh giá từ (Wang và cộng sự, 2020). Chúng tôi báo cáo điểm BLEU (Papineni và cộng sự, 2002) làm chỉ số hiệu suất với beam kích thước 5 và phạt độ dài 0.6 (cho WMT).

Cơ sở. Chúng tôi so sánh AutoMoE với cả kiến trúc được thiết kế thủ công và được tìm kiếm NAS. Đối với cơ sở thủ công, chúng tôi xem xét: (a) Transformer được kích hoạt dày đặc (Vaswani và cộng sự, 2017) không có chuyên gia; (b) MoE được kích hoạt thưa thớt với các chuyên gia đồng nhất (tức là cùng số lượng và kích thước FFN) được đặt trong mỗi lớp khác (Fedus và cộng sự, 2022b; Kim và cộng sự, 2021; Zuo và cộng sự, 2022; Du và cộng sự, 2022; Artetxe và cộng sự, 2021).

Đối với cơ sở NAS, chúng tôi xem xét (c) HAT (Wang và cộng sự, 2020), là một khung NAS tối ưu dựa trên Supernet để xác định sub-Transformer dày đặc hiệu quả cho dịch máy thần kinh (cùng thiết lập nhiệm vụ như chúng tôi); và (d) Evolved Transformer (So và cộng sự, 2019) là một trong những nghiên cứu sớm về tìm kiếm sub-Transformer dày đặc hiệu quả với tìm kiếm kiến trúc dựa trên tiến hóa. Lưu ý rằng cả hai cơ sở NAS chỉ áp dụng cho các transformer dày đặc không phải MoE, và AutoMoE là nghiên cứu đầu tiên tận dụng NAS để xác định các sub-transformer MoE thưa thớt hiệu quả. Cuối cùng, chúng tôi xem xét (e) AutoMoE với Random Search (thường được coi là cơ sở mạnh cho NAS) lấy mẫu một mạng con MoE (với ràng buộc độ trễ) ngẫu nhiên từ không gian tìm kiếm AutoMoE và huấn luyện nó đến hội tụ.

Cấu hình huấn luyện và không gian tìm kiếm. Tất cả cơ sở và AutoMoE bao gồm Supernet và mô hình cuối cùng được huấn luyện với cùng thiết lập để so sánh công bằng. Tất cả mô hình được huấn luyện trong 40K bước, với khởi động 10K bước từ 10^-7 đến 10^-3 và sử dụng cosine annealing đến 10^-7 cho phần còn lại của các bước. Tất cả mô hình được huấn luyện bằng toolkit fairseq (Ott và cộng sự, 2019) với kích thước batch hiệu quả 524K token trên 16 GPU V100. Tất cả cơ sở NAS có cùng không gian tìm kiếm cho các mô-đun Transformer dày đặc (ví dụ: số lượng lớp decoder, chiều q-k-v, đầu chú ý, v.v.) với AutoMoE tiếp tục kết hợp các khía cạnh liên quan MoE (ví dụ: chuyên gia, cổng, định tuyến, v.v.) trong không gian tìm kiếm. Số lượng lớp encoder được giữ cố định cho tất cả cơ sở NAS bao gồm AutoMoE vì độ trễ chủ yếu được xác định bởi các decoder cho tạo tự hồi quy (như chúng tôi thảo luận trong Phần 5.2).

Thiết lập tìm kiếm tiến hóa. Để ước tính hiệu suất, chúng tôi theo dõi loss validation của subnet trên nhiệm vụ NMT. Chúng tôi tính độ trễ bằng cách đo thời gian thực hiện dịch từ câu nguồn sang câu đích với cùng độ dài đầu vào/đầu ra mong muốn (30 cho WMT) và thiết lập beam gốc (xem Phần 4) trên thiết bị đích (Intel Xeon CPU). Chúng tôi đo độ trễ 300 lần cho vàng (để báo cáo chỉ số cuối cùng) và 100 lần cho vàng một phần (trong quá trình tìm kiếm tiến hóa) tương ứng; loại bỏ 10% trên và dưới (độ trễ ngoại lệ) và tính trung bình của phần còn lại. Thiết lập siêu tham số cho tìm kiếm tiến hóa bao gồm: 15 như các lần lặp, 125 như kích thước quần thể, 25 như kích thước cha mẹ, 50 như kích thước quần thể đột biến với xác suất đột biến 0.3 và 50 như kích thước quần thể lai tạo. Trừ khi được nêu khác, ràng buộc độ trễ cho tất cả thí nghiệm được đặt thành 600ms.

5 Kết quả
5.1 Hiệu suất AutoMoE so với Cơ sở
Bảng 4 trình bày so sánh AutoMoE với cơ sở trên nhiều chỉ số tính toán và hiệu suất nhiệm vụ. Chúng tôi báo cáo số lượng tham số không có trọng số nhúng và FLOPs không có lớp giải mã cuối cùng cho tất cả mô hình, nhất quán với đánh giá (Wang và cộng sự, 2020). Các sub-Transformer MoE thưa thớt được tạo bởi AutoMoE đạt được giảm 4× FLOPs so với cả Transformer-Big được thiết kế thủ công (được kích hoạt dày đặc) và MoE SwitchTransformer-Big (được kích hoạt thưa thớt) với chuyên gia trong mỗi lớp, và tăng tốc suy luận tương đương trên CPU. So với cơ sở NAS như Evolved Transformer (So và cộng sự, 2019) và HAT (Wang và cộng sự, 2020) tạo ra các sub-Transformer được kích hoạt dày đặc, AutoMoE cải thiện FLOPs và độ trễ lần lượt 2.4× và 1.3× với hiệu suất tương đương về điểm BLEU trên tổng hợp. Đáng chú ý, AutoMoE dựa trên Supernet và HAT có chi phí huấn luyện phân bổ giảm mạnh (giờ GPU) so với Evolved Transformer với tìm kiếm tiến hóa tiến bộ. AutoMoE với Random Search, một cơ sở NAS mạnh, đạt được tăng tốc tốt nhất nhưng với suy thoái hiệu suất đáng kể.

So với tất cả các mô hình khác (cả dày đặc và thưa thớt), chúng tôi quan sát AutoMoE tạo ra các mạng với độ thưa thớt cao dẫn đến giảm mạnh các tham số hoạt động và FLOPs. Đối với các mô hình NAS, chúng tôi huấn luyện top-2 sub-Transformer trong Pareto và báo cáo cái với sự đánh đổi tốt nhất về BLEU vs. FLOPs trên tập validation. Số chuyên gia tối đa cho hiệu suất tốt nhất thay đổi cho các nhiệm vụ khác nhau, với 6 chuyên gia cho WMT'14 En-De, 16 chuyên gia cho WMT'14 En-Fr và WMT'19 En-De – với hai tập dữ liệu sau lớn gấp 10 lần so với tập trước.

5.2 Phân tích
Lớp decoder vs. FLOPs. Hình 3 (a) cho thấy FLOPs trung bình cho nhiều kiến trúc AutoMoE với các lớp decoder khác nhau như thu được trong quá trình tìm kiếm của chúng tôi (thay đổi từ 3 đến 6) từ Pareto và các mô hình cơ sở. Chú ý rằng FLOPs tăng theo sự tăng của lớp decoder, với bản chất tự hồi quy của các nhiệm vụ NMT yêu cầu tạo token tuần tự. Trái ngược với các Transformer được thiết kế thủ công với 6 lớp decoder (cả biến thể dày đặc và MoE được kích hoạt thưa thớt), các kiến trúc được tìm kiếm bởi AutoMoE và HAT giảm số lượng lớp decoder với kết quả giảm cả FLOPs và độ trễ. Điều này cũng rõ ràng trong Hình 3 (e) cho thấy độ trễ decoder chi phối tổng độ trễ suy luận cho tất cả các mô hình hơn 90%.

Phân bố chuyên gia trong encoder vs. decoder. Hình 3 (b) vẽ số lượng chuyên gia encoder như tỷ lệ của tổng số chuyên gia cho các sub-Transformer được tạo bởi AutoMoE. Chúng tôi quan sát rằng AutoMoE gán số lượng lớn chuyên gia cho encoder so với decoder. Kết quả là, encoder có khả năng cao hơn nhiều (tức là tham số encoder như tỷ lệ tham số tổng thể) so với decoder. Điều này tương quan với quan sát trước đó rằng các mô hình với số lượng lớp encoder cao hơn so với lớp decoder có sự đánh đổi độ trễ-hiệu suất tốt hơn (Kasai và cộng sự, 2021). Những phát hiện của chúng tôi từ các kiến trúc được thiết kế bởi AutoMoE chỉ ra rằng số lượng lớp và chuyên gia là hai núm điều chỉnh cùng nhau giúp điều chỉnh khả năng encoder và độ trễ decoder để thiết kế MoE hiệu quả.

Phân bố chuyên gia trong các lớp khác nhau. Hình 3 (c) và (d) cho thấy phần trăm chuyên gia được phân bổ cho các lớp khác nhau cho encoder và decoder – trung bình trên nhiều kiến trúc được lấy mẫu từ Supernet AutoMoE. Chú ý rằng các lớp encoder giữa (thứ 3, thứ 5) được phân bổ số lượng chuyên gia tối đa, trong khi lớp đầu tiên nhận ít nhất. Xu hướng ngược lại cho decoder, với lớp đầu tiên nhận hầu hết chuyên gia với giảm dần trong phân bổ chuyên gia. Điều này cũng nhất quán với việc giữ decoder nhẹ bằng cách bỏ các lớp để giảm độ trễ; trong khi bù đắp cho khả năng giảm với tăng chuyên gia trong vài lớp đầu tiên.

Độ trễ vs. FLOPs làm ràng buộc cho tìm kiếm. Bảng 6 trình bày tác động của độ trễ và FLOPs làm ràng buộc tính toán trên sự đánh đổi hiệu suất-hiệu quả. Ràng buộc FLOPs dẫn đến các mô hình hoàn toàn sử dụng hết ngân sách FLOPs; trong khi dẫn đến độ trễ cao hơn. Mặt khác, ràng buộc độ trễ có xu hướng không sử dụng hết ngân sách dẫn đến FLOPs và độ trễ tương đối vượt trội, cung cấp kiểm soát nghiêm ngặt hơn.

Các kiến trúc MoE được tạo bởi AutoMoE tối ưu Pareto. Bảng 5 cho thấy các kiến trúc MoE được kích hoạt thưa thớt được thiết kế bởi hai biến thể của AutoMoE ('std-expert': kích thước FFN chuyên gia giống nhau trong mỗi lớp và thay đổi qua; 'fract-expert': kích thước chuyên gia hoàn toàn dị thể) cho các tập dữ liệu khác nhau với sự đánh đổi tốt nhất về BLEU vs. độ trễ. Trên tổng hợp, 71% chuyên gia được phân bổ cho encoder so với decoder. Trong khi đó, 70% lớp chuyên gia trong kiến trúc 'fract-expert' có 2 hoặc nhiều chuyên gia hơn, trong đó hơn 75% lớp chuyên gia có khả năng thay đổi (tức là chuyên gia với kích thước FFN trung gian khác nhau). Hình 4, 5, 6 trong Phụ lục cho thấy kiến trúc đầy đủ (kích thước nhúng, lớp, đầu, chuyên gia, vị trí, kích thước, v.v.) của subnet AutoMoE trên WMT14 En-De, WMT14 En-Fr và WMT19 EN-De tương ứng.

Biến thể không gian tìm kiếm MoE. Bảng 7 trình bày tác động của các lựa chọn không gian tìm kiếm trên sự đánh đổi hiệu quả và hiệu suất MoE. Biến thể đầu tiên là làm cho '#Encoder Layers' trở thành một chiều tìm kiếm đàn hồi. Lưu ý rằng cả HAT và AutoMoE đều xem xét số lượng lớp encoder là cố định (tham khảo Bảng 2). Chúng tôi quan sát rằng thay đổi lớp encoder có sự đánh đổi tương đối kém về hiệu suất mô hình vs. hiệu quả so với thay đổi lớp decoder, củng cố các quan sát trước đây của chúng tôi về tầm quan trọng của khả năng và độ sâu encoder.

Trong biến thể thứ hai (xem hàng chính thứ ba), chúng tôi cố định kiến trúc chuyên gia (với 2 chuyên gia được đặt thủ công đồng đều) trong không gian tìm kiếm và chỉ tìm kiếm các siêu tham số Transformer tiêu chuẩn. Quan sát rằng các mô hình được thiết kế bởi AutoMoE có FLOPs tốt hơn so với những cái được thiết kế thủ công như vậy.

Biến thể cuối cùng giới thiệu chuyên gia đồng nhất hoặc giả (tức là chuyên gia với kích thước FFN trung gian 0, tương đương với phép toán đồng nhất). Điều này khám phá ý tưởng rằng chúng ta có thể bỏ qua tính toán cho một số token dựa trên ngữ cảnh thay vì luôn luôn buộc chúng qua FFN. Chúng tôi quan sát các chuyên gia đồng nhất làm tổn hại hiệu suất một cách nhẹ nhàng nhưng giảm đáng kể FLOPs (xem hàng chính cuối cùng).

6 Kết luận
AutoMoE là khung đầu tiên thiết kế MoE dị thể dưới các ràng buộc tính toán. Nó hỗ trợ tính toán thích ứng tức là tính toán thay đổi cho các đầu vào khác nhau với các chuyên gia có kích thước thay đổi. Nó tận dụng NAS để khám phá một không gian tìm kiếm dị thể với số lượng chuyên gia, kích thước và lựa chọn vị trí thay đổi; cùng với các siêu tham số kiến trúc Transformer tiêu chuẩn khác. Các mạng con MoE được tạo bởi AutoMoE giảm FLOPs và độ trễ so với cả kiến trúc được thiết kế thủ công và được tìm kiếm NAS trên các nhiệm vụ MT chuẩn.

7 Hạn chế
Với trọng tâm của chúng tôi là tìm các mô hình MoE hiệu quả dưới các ràng buộc tính toán, không gian tìm kiếm và đánh giá AutoMoE đã bị hạn chế về quy mô đối với các mô hình Transformer kích thước lớn cho các nhiệm vụ MT chuẩn. Một mở rộng tự nhiên của công việc này là khám phá các giới hạn của các mô hình MoE như SwitchTransformers (Fedus và cộng sự, 2022b) và GShard (Lepikhin và cộng sự, 2020) lớn hơn đáng kể chứa hàng tỷ đến hàng nghìn tỷ tham số; cũng như thiết kế các mô hình chuyên gia hiệu quả thưa thớt và có thể chuyển giao (Zoph và cộng sự, 2022) cho các loại nhiệm vụ đa dạng như lý luận, tóm tắt và hiểu biết.

Các hạn chế của công việc này như sau:
1. Sandwich sampling (Yu và cộng sự, 2019), chưng cất kiến thức tại chỗ (Yu và cộng sự, 2019) và giảm xung đột gradient (Gong và cộng sự, 2022) là các kỹ thuật phổ biến để cải thiện quy trình huấn luyện supernet. Sẽ thú vị khi nghiên cứu tác động của các kỹ thuật này để cải thiện supernet của AutoMoE.

2. AutoMoE sử dụng chiều ẩn của mạng feedforward trung gian (FFN) để điều chỉnh khả năng của mỗi chuyên gia. Sẽ thú vị khi nghiên cứu các kỹ thuật khác để điều chỉnh khả năng chuyên gia như xếp chồng số lượng lớp ẩn thay đổi trong FFN.

3. Xương sống của supernet của AutoMoE sử dụng Switch Transformer, thêm các lớp chuyên gia dựa trên FFN và định tuyến mỗi token đến chính xác một chuyên gia (định tuyến top-1). Sẽ thú vị khi: (i) tìm kiếm số lượng token để định tuyến, và (ii) tìm kiếm thành phần Transformer (ví dụ: FFN, các lớp chiếu self-attention, LayerNorm) để thêm các lớp chuyên gia.

4. Không gian tìm kiếm của AutoMoE chứa các thành phần Transformer cổ điển như multi-head attention và các lớp FFN. Sẽ thú vị khi thêm các thành phần hiệu quả theo thiết kế như lớp convolutional, FLASH (Hua và cộng sự, 2022) và g-MLP (Liu và cộng sự, 2021).

Lời cảm ơn
MAM nhận sự hỗ trợ từ Canada Research Chairs (CRC), Hội đồng Nghiên cứu Khoa học và Kỹ thuật Tự nhiên Canada (NSERC; RGPIN-2018-04267), Quỹ Đổi mới Canada (CFI; 37771) và Liên minh Nghiên cứu Kỹ thuật số Canada. Nghiên cứu của Lakshmanan được hỗ trợ một phần bởi khoản tài trợ từ NSERC (Canada).
