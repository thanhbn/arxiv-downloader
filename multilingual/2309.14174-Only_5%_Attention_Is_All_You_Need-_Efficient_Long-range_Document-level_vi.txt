# 2309.14174.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2309.14174.pdf
# Kích thước tệp: 1273178 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Chỉ cần 5% Attention: Dịch thuật thần kinh hiệu quả tầm xa cấp tài liệu
Zihan Liu1,2∗, Zewei Sun2, Shanbo Cheng2, Shujian Huang1, Mingxuan Wang2
1National Key Laboratory for Novel Software Technology, Nanjing University
2ByteDance
liuzh@smail.nju.edu.cn ,huangsj@nju.edu.cn
{sunzewei.v,chengshanbo,wangmingxuan.89}@bytedance.com

Tóm tắt
Dịch thuật thần kinh cấp tài liệu (DocNMT) đã được chứng minh là quan trọng trong việc xử lý các hiện tượng diễn ngôn bằng cách đưa vào thông tin ngữ cảnh cấp tài liệu. Một trong những hướng quan trọng nhất là nhập toàn bộ tài liệu trực tiếp vào mô hình Transformer tiêu chuẩn. Trong trường hợp này, hiệu quả trở thành mối quan tâm quan trọng do độ phức tạp bậc hai của module attention. Các nghiên cứu hiện tại hoặc tập trung vào phần encoder, không thể được triển khai trên các tác vụ sinh chuỗi-tới-chuỗi như Dịch thuật máy (MT), hoặc gặp phải sự sụt giảm hiệu suất đáng kể. Trong công việc này, chúng tôi duy trì hiệu suất dịch thuật trong khi đạt được tăng tốc 20% bằng cách đưa vào lớp lựa chọn bổ sung dựa trên attention nhẹ để chọn một phần nhỏ các token cần được chú ý. Phương pháp này tận dụng attention gốc để đảm bảo hiệu suất và giảm chiều để tăng tốc suy luận. Kết quả thực nghiệm cho thấy phương pháp của chúng tôi có thể đạt tới 95% độ thưa (chỉ 5% token được chú ý) và tiết kiệm 93% chi phí tính toán trên module attention so với Transformer gốc, đồng thời duy trì hiệu suất.

1 Giới thiệu
Các phát triển gần đây trong dịch thuật thần kinh máy đã tập trung vào việc dịch các câu riêng lẻ, nhưng nghiên cứu đã chỉ ra rằng thông tin cấp tài liệu là quan trọng để xử lý các hiện tượng diễn ngôn như tính nhất quán từ vựng và coreference đại từ, phụ thuộc vào ngữ cảnh tầm xa. Do đó, các cơ chế attention khác nhau (Zhang et al., 2018; Maruf et al., 2019; Zheng et al., 2020; Bao et al., 2021) mã hóa thông tin ngữ cảnh cấp tài liệu đã được đề xuất.

Tuy nhiên, chi phí tính toán của các cơ chế attention này tăng theo bậc hai với độ dài của chuỗi đầu vào. Để giải quyết vấn đề này, các nhà nghiên cứu đã đề xuất các mô hình transformer hiệu quả (Tay et al., 2020b) nhằm giảm chi phí tính toán của attention thông qua các kỹ thuật như mẫu thưa (Tay et al., 2020a; Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020) giới hạn số lượng token cần chú ý, token bộ nhớ hoặc toàn cục nén các token ngữ cảnh thành một biểu diễn duy nhất (Lee et al., 2019; Ma et al., 2021), xấp xỉ softmax với phương pháp kernel (Choromanski et al., 2020; Qin et al., 2022; Peng et al., 2021), hoặc kết hợp các phương pháp trên (Tay et al., 2021a; Zhu et al., 2021).

Mặc dù sự xuất hiện của các mô hình transformer hiệu quả khác nhau, các tác vụ chuỗi-tới-chuỗi tầm xa như dịch thuật máy cấp tài liệu vẫn cần khám phá thêm.

Một mặt, một số mô hình hiệu quả hiện tại (Wang et al., 2020; Zaheer et al., 2020; Lee-Thorp et al., 2022) tập trung vào phần encoder và không thể được sử dụng để sinh do tính chất tự hồi quy. Một số (Tay et al., 2021b; Child et al., 2019; Beltagy et al., 2020) có mối quan hệ mạnh với vị trí của các token nên không thể áp dụng cho cross attention khi không có sự căn chỉnh rõ ràng giữa query và key.

Mặt khác, các nghiên cứu nhắm vào sinh chuỗi-tới-chuỗi hiệu quả chỉ xác minh phương pháp của họ trên các benchmark dịch thuật cấp câu bình thường như bộ test WMT EN-DE (Peng et al., 2021; Petrick et al., 2022; Ma et al., 2021). Trong các thực nghiệm sơ bộ, chúng tôi thấy rằng hầu hết các công trình đều giảm nghiêm trọng về BLEU khi xử lý các tác vụ dịch thuật tài liệu thực tế.

Để giải quyết vấn đề này, chúng tôi cố gắng giảm chi phí tính toán đồng thời đảm bảo hiệu suất dịch thuật. Trong bài báo này, chúng tôi chủ yếu tập trung vào cơ chế attention theo các mô hình transformer hiệu quả khác.

Cụ thể, chúng tôi muốn chọn các token quan trọng (Sun et al., 2020, 2022a) và chỉ thực hiện attention với chúng. Các nghiên cứu trước đây có động lực tương tự hoặc thiết kế mẫu thưa với prior con người như cửa sổ trượt cố định (Beltagy et al., 2020; Tay et al., 2020a; Zaheer et al., 2020) thiếu tính linh hoạt, hoặc cố gắng học mẫu thưa bằng phương pháp clustering. Tuy nhiên, hiệu suất kém của các phương pháp mẫu có thể học được trên DocNMT phản ánh rằng query không chú ý đến các key như mong đợi trong attention gốc.

Để đảm bảo hiệu suất, chúng tôi tận dụng attention gốc và đề xuất Lightweight Attention Selection Transformer (Lasformer). Lasformer kết hợp các lớp lựa chọn sử dụng attention nhẹ, có phân phối được hướng dẫn bởi giám sát từ attention gốc. Việc đạt được xử lý nhẹ được thực hiện bằng cách giảm chiều ẩn, trong khi quá trình lựa chọn bao gồm việc giữ lại các token có điểm attention cao nhất, một chiến lược được xác nhận hiệu quả bởi (Zhao et al., 2019). Bằng cách sử dụng các cơ chế này, chúng tôi có thể lọc hiệu quả các token không quan trọng với chi phí tương đối thấp, dẫn đến việc giảm gánh nặng tính toán tổng thể, đặc biệt khi một tỷ lệ lớn các token có thể được lọc ra.

Xác định số lượng token phù hợp để giữ lại là cực kỳ quan trọng, vì chúng phải đóng góp đủ thông tin để đảm bảo hiệu suất tối ưu, đồng thời giảm thiểu số lượng để nâng cao hiệu quả. Trong phương pháp của chúng tôi, độ thưa được học một cách thích ứng, tăng dần trong quá trình huấn luyện cho đến khi đạt mức tối ưu cân bằng giữa hiệu suất và hiệu quả cho mỗi lớp lựa chọn.

Thực nghiệm cho thấy Lasformer có thể giảm hiệu quả tính toán attention. Chỉ 5% token được sử dụng trong attention và hiệu suất dịch thuật gần như không thay đổi. Đối với chuỗi dài hàng nghìn từ, phương pháp của chúng tôi có thể giảm chi phí attention xuống 7%. Và tốc độ suy luận end-to-end có thể được tăng cường lên 1.2x.

2 Công trình liên quan
2.1 Dịch thuật máy cấp tài liệu
Dịch thuật máy cấp tài liệu bao gồm ngữ cảnh nguồn và đích bổ sung để cải thiện việc dịch thuật về mặt tính mạch lạc và nhất quán (Voita et al., 2019; Müller et al., 2018; Lopes et al., 2020; Bawden et al., 2018). Có hai dòng phương pháp để sử dụng ngữ cảnh. Một dòng đưa vào encoder bổ sung để mã hóa ngữ cảnh và tích hợp nó vào câu hiện tại (Zhang et al., 2018; Maruf et al., 2019). Hạn chế là cùng một câu có thể được mã hóa nhiều lần do đó tăng độ phức tạp. Điều này được giải quyết bởi các công trình gần đây bằng cách chia sẻ tham số của context encoder và current sentence encoder (Zheng et al., 2020; Ma et al., 2020).

Dòng công trình khác nối ngữ cảnh và câu hiện tại và dịch nó như thể đó là một câu duy nhất (Tiedemann and Scherrer, 2017; Sun et al., 2022b). Tuy nhiên, việc nối tạo ra chuỗi đầu vào dài và khiến việc huấn luyện mô hình trở nên khó khăn vì entropy cao của phân phối attention. Để giảm bớt vấn đề, locality bias được đưa vào, trong đó thông tin cấp câu được tăng cường (Bao et al., 2021).

Tóm lại, phương pháp trước dựa trên dịch thuật câu đồng thời tích hợp ngữ cảnh. Phương pháp sau cố gắng dịch toàn bộ tài liệu đồng thời đưa vào tính địa phương cấp câu. Và chúng dường như đạt đến cùng một sơ đồ sử dụng cả local attention và global attention.

Local attention ngụ ý mẫu thưa được thiết kế bởi con người và việc đưa vào mẫu thưa có thể học được cho global attention trong dịch thuật máy cấp tài liệu là tự nhiên.

2.2 Transformer hiệu quả
Đã có một số phương pháp trước đây cho Transformer hiệu quả tập trung vào các thuộc tính của attention, cụ thể là tính thưa và low rank để giảm chi phí tính toán.

Tính thưa đề cập đến ý tưởng rằng chỉ một vài token nhận được lượng attention đáng kể, trong khi phần còn lại đóng góp ít cho đầu ra. Một số phương pháp (Tay et al., 2021a; Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) đã đề xuất các mẫu thủ công như cửa sổ trượt hoặc cửa sổ giãn, được lấy cảm hứng từ kiến thức prior con người rằng các token gần đóng góp attention nhiều nhất. Các phương pháp khác (Kitaev et al., 2020; Wang et al., 2022; Tay et al., 2020a; Roy et al., 2021) đã cố gắng làm cho mẫu thưa có thể học được với chi phí thấp hơn bằng cách sử dụng các kỹ thuật như clustering, dựa trên ý tưởng rằng các token tương tự dự kiến sẽ chú ý đến nhau và thuộc về cùng một cluster. Các phương pháp clustering này có thể bao gồm các kỹ thuật như locality sensitive hashing (Kitaev et al., 2020), K-means (Roy et al., 2021), hoặc learnable sorting networks (Tay et al., 2020a).

Mặt khác, các phương pháp low-rank dựa trên ý tưởng rằng các đặc trưng N chiều có thể được nén thành ít chiều hơn. Một số công trình (Lee et al., 2019; Ma et al., 2021; Jaegle et al., 2021) đã sử dụng token toàn cục hoặc bộ nhớ để nén thông tin tầm xa thành số lượng embedding hạn chế hoặc đã sử dụng phương pháp kernel (Peng et al., 2021; Qin et al., 2022) để xấp xỉ điểm softmax, cho phép tính toán key và value trước và giảm độ phức tạp từ O(N²d) xuống O(Nd²) (trong đó d là chiều của self-attention).

Trong khi các phương pháp thưa duy trì cấu trúc attention token-to-token, các phương pháp low-rank sử dụng embedding nén cho attention. Phương pháp token-to-token dễ hiểu hơn nhưng có thể mất một số thông tin, trong khi phương pháp kia có thể chứa nhiều thông tin hơn nhưng cũng có thể nhiễu hơn. Vì thông tin trong DocNMT thưa (Lupo et al., 2022), nhiễu của các phương pháp low-rank có thể nghiêm trọng hơn nhiều và do đó chúng tôi khai thác các phương pháp thưa.

3 Phương pháp
Dịch thuật cấp tài liệu chuỗi-tới-chuỗi nhằm nắm bắt hoàn toàn ngữ cảnh xa. Điều này được thực hiện bằng cơ chế attention, cho phép mỗi query chú ý đến tất cả các key, dẫn đến chi phí tính toán tăng theo bậc hai với độ dài chuỗi. Tuy nhiên, chỉ một phần khá nhỏ các token thực sự liên quan. Do đó, việc chọn những token quan trọng và lọc ra những token khác để giảm số lượng đối tượng được căn chỉnh là quan trọng.

Cụ thể, chúng tôi tận dụng cơ chế attention gốc và chưng cất nó thành attention nhẹ với chiều ẩn thấp hơn để chọn các token quan trọng, như thể hiện trong Hình 1. Nó vẫn tính toán O(N²) nhưng với chi phí tính toán ít hơn nhiều. Sau khi lọc, chỉ các token còn lại sẽ được chú ý. Mặc dù việc lựa chọn đưa vào chi phí bổ sung, tổng hiệu quả có thể được cải thiện miễn là phạm vi căn chỉnh được giới hạn đủ.

Về cơ bản, chúng tôi chia phương pháp thành bốn phần: lightweight attention, attention supervision, adaptive sparsity, và layer sharing, sẽ được giới thiệu trong các phần sau.

3.1 Lightweight Attention
Giả sử chuỗi có tổng cộng N token và chúng ta cần chọn kN token quan trọng cho token hiện tại. k là tỷ lệ lựa chọn. Vì việc lựa chọn chỉ là một quá trình sơ bộ và chỉ nên tính toán rất ít, chúng tôi chiếu trạng thái ẩn của tất cả các token từ d xuống ds (ví dụ từ 512 xuống 64). Sau đó, một phiên bản nhẹ của attention được thực hiện với những trạng thái ẩn chiều thấp đó:

As = softmax(QsKsᵀ/√ds) (1)

trong đó Qs, Ks, và As đại diện cho query, key và attention được chiếu. Qs = XWQ, Ks = XWK, và WQ ∈ ℝd×ds, WK ∈ ℝd×ds.

Sau khi sắp xếp tất cả các logit, chúng tôi chỉ giữ lại k key hàng đầu cho mỗi query token và che các token khác:

mask = top-k(As) (2)

Rõ ràng, hàm top-k không khả vi. Để huấn luyện mạng lựa chọn, chúng tôi sử dụng trick tái tham số hóa từ Gumbel Softmax (Jang et al., 2017) để làm cho các tham số có thể học được:

mask = mask + As - SG(As) (3)

trong đó SG đề cập đến stop-gradient. Sau đó gradient có thể được truyền đến As trong khi duy trì giá trị của mask.

3.2 Attention Supervision
Theo trực quan, phân phối của lightweight attention nên nhất quán với lớp attention gốc để đảm bảo hiệu suất. Do đó, chúng tôi kéo phân phối trước về phân phối sau trong quá trình huấn luyện bằng KL loss bổ sung. Quá trình chưng cất như vậy không yêu cầu mô hình Transformer được huấn luyện trước, nhưng các lớp chiều thấp và cao được huấn luyện với ràng buộc tính nhất quán. Tuy nhiên, việc sử dụng attention gốc ngăn cản tăng tốc tại thời gian huấn luyện, vì vậy chúng tôi chỉ tập trung vào hiệu quả suy luận.

A = softmax(QKᵀ/√d) (4)
Ls = kl_div(As, A) (5)

trong đó Q và K là các trạng thái ẩn được chiếu chiều cao từ lớp attention gốc. kl_div là Kullback-Leibler Divergence. Loss được thêm vào NMT loss với một siêu tham số α:

Loss = Lnmt + α * Ls (6)

3.3 Adaptive Sparsity
k đại diện cho mức độ thưa và quan trọng trong toàn bộ quy trình lựa chọn. Tuy nhiên, lựa chọn tối ưu của k không rõ ràng. Chúng tôi đề xuất một thuật toán thích ứng để tìm kiếm nó.

Cụ thể, chúng tôi đặt ngưỡng t cho tổng attention. Trực quan là, vì một lượng nhỏ token đóng góp cho phần lớn trọng số attention, "phần lớn trọng số" có thể được định lượng là ngưỡng t. Nếu tổng attention hiện tại dưới t, một số token quan trọng có thể bị lọc, vì vậy chúng tôi tăng nhẹ k một bước nhỏ, và ngược lại:

k = {k - step if sum(topk) > t
     k + step else} (7)

Chúng tôi coi k là tỷ lệ phần trăm, nên k nằm trong khoảng [0,1], và step là hằng số nhỏ như 0.001. Chúng tôi khởi tạo k là 1 và giới hạn k lớn hơn hoặc bằng 1%. Đối với tài liệu có ít câu, ít nhất 10 token được chú ý để tránh hiệu suất kém.

Trong khi k giảm dần và hội tụ trong quá trình huấn luyện, mô hình được khuyến khích học phân phối attention tập trung và loại bỏ thông tin không liên quan. Trong một số lớp, đặc biệt là lớp encoder, k có thể bị kẹt tại một điểm nào đó và đôi khi không bao giờ giảm, vì vậy chúng tôi thủ công vô hiệu hóa k + step khi k lớn.

3.4 Layer Sharing
Hơn nữa, chúng tôi chia sẻ các mẫu thưa đã học qua các lớp như (Xiao et al., 2019) đã chứng minh rằng trọng số attention có thể được tái sử dụng trực tiếp vì theo trực quan, mỗi query trong các lớp khác nhau thường chú ý đến cùng các key. Vì vậy chi phí lựa chọn bổ sung có thể được giảm thêm trong khi duy trì hiệu suất dịch thuật.

Về cơ bản, chúng tôi chia tất cả các lớp lựa chọn thành m nhóm và mỗi nhóm có r = n/m lớp, trong đó n là số lớp gốc. Chúng tôi chỉ tính toán attention của lớp lựa chọn thấp nhất trong mỗi nhóm. Sau đó các lớp lựa chọn khác chia sẻ cùng attention như lớp thấp nhất:

Asi = As⌊i/r⌋*r (8)

Bằng cách này, chúng tôi có thể tiết kiệm m*(r-1) tính toán attention selection.

3.5 Tiết kiệm chi phí
Cuối cùng, chúng tôi cố gắng chính thức hóa chi phí attention với các thuật toán và tham số này. Chi phí attention của Transformer attention gốc (Tay et al., 2020b):

CTransformer = 2nN²d (9)

trong đó n là số lớp, N là độ dài chuỗi, và d là chiều của trạng thái ẩn. "2" có nghĩa là tích vô hướng (A = QK) và tổng có trọng số (AV). Và Lasformer có thể đạt được độ phức tạp như sau:

CLasformer = (1/r) * nN²ds + 2knN²d (10)

trong đó r là số lớp trong mỗi nhóm, ds là chiều của lớp lựa chọn, và k là tỷ lệ lựa chọn. Mục đầu tiên có nghĩa là lựa chọn thô chiều ds. Mục thứ hai có nghĩa là masked attention.

Với chiều nhỏ cho lựa chọn (ds), độ thưa cao cho attention (k), và kích thước nhóm lớp lớn (r), chúng tôi có thể giảm đáng kể tổng chi phí tính toán. Nếu chúng tôi đặt n = 6, d = 512 như Transformer base, và ds = 64, t = 0.95 (k = 0.05), r = 3, chi phí attention chỉ có thể là 7% so với Transformer gốc. Kết quả chi tiết được liệt kê trong các phần sau.

4 Thực nghiệm
4.1 Bộ dữ liệu
Chúng tôi tiến hành thực nghiệm trên ba bộ dữ liệu Anh-Đức và một bộ dữ liệu Trung-Anh. Các bộ dữ liệu Anh-Đức bao gồm TED, News, và Europarl, theo Maruf et al. (2019). Corpus TED từ IWSLT 2017, và chúng tôi sử dụng tst2016-2017 làm test set và phần còn lại được sử dụng để phát triển. News là corpus News Commentary-v11 được căn chỉnh theo tài liệu, và WMT'16 newstest2015 và newstest2016 được sử dụng để phát triển và kiểm tra, tương ứng. Europarl được trích xuất như đề xuất trong Maruf et al. (2019). Đối với bộ dữ liệu Trung-Anh, chúng tôi theo Sun et al. (2022b), sử dụng PDC là corpus tin tức song ngữ được thu thập với các miền đa dạng.

Dữ liệu huấn luyện ở trên được tổ chức thành một hỗn hợp dữ liệu cấp câu và dữ liệu cấp tài liệu như được sử dụng trong Sun et al. (2022b). Tất cả dữ liệu được cắt thành sub-word sử dụng BPE với 32k thao tác merge.

4.2 Cài đặt mô hình
Chúng tôi xây dựng mô hình dịch thuật dựa trên Transformer base (Vaswani et al., 2017) sử dụng fairseq (Ott et al., 2019), bao gồm 6 lớp, 512 chiều, 8 head, 2048 kích thước ẩn feed-forward, cho cả encoder và decoder. Chúng tôi sử dụng dropout nhỏ 0.1, cũng như word dropout, trên các bộ dữ liệu lớn như Europarl và PDC, và dropout lớn 0.3 trên các bộ dữ liệu nhỏ như TED và News.

Đối với lớp lựa chọn đề xuất, chúng tôi sử dụng ds = 64 chiều, m = 2 nhóm, và r = 3 lớp. Hệ số α được đặt là 0.01 và ngưỡng t cho dynamic top-k được đặt là 0.95.

Chúng tôi áp dụng sacreBLEU không phân biệt chữ hoa thường (Post, 2018) trên toàn bộ tài liệu, theo tất cả các nghiên cứu NMT cấp tài liệu trước đây.

4.3 Công trình so sánh
Chúng tôi so sánh kết quả với ba Transformer hiệu quả điển hình từ các lớp phương pháp khác nhau và trực tiếp sử dụng mã nguồn mở của họ để tiến hành thực nghiệm trên các bộ dữ liệu:

• LSH-trans (Petrick et al., 2022) dựa trên Reformer và sử dụng locality sensitive hashing để có được một cluster các token được chú ý đến nhau trong đó.

• Luna (Ma et al., 2021) là mô hình dựa trên low-rank nén chuỗi dài thành số lượng token toàn cục cố định sử dụng cơ chế attention.

• RFA-trans (Wu et al., 2022) mở rộng RFA (Peng et al., 2021) với cơ chế gating cấp câu để tăng cường tính địa phương.

Có nhiều nghiên cứu Transformer hiệu quả khác (Beltagy et al., 2020; Zaheer et al., 2020; Tay et al., 2020a, 2021a). Tuy nhiên, vì chúng bỏ qua các tác vụ sinh chuỗi-tới-chuỗi và chỉ tập trung vào tác vụ encoder-only hoặc decoder-only, chúng tôi không đưa chúng vào đây.

4.4 Kết quả
Bảng 1 hiển thị kết quả dịch thuật so sánh với các mô hình dịch thuật cấp tài liệu trước đây. Về hiệu quả, tất cả các nghiên cứu liên quan đều đạt được tiết kiệm chi phí ở mức độ khác nhau. Chúng cho kết quả tốt hơn về mặt chi phí hoặc tốc độ. Tuy nhiên, chúng gặp phải sự sụt giảm chất lượng nghiêm trọng khi xử lý các tài liệu tầm xa thực tế. Chúng tôi thấy rằng mặc dù chúng báo cáo kết quả tương đương trên WMT hoặc IWSLT (với ngữ cảnh rất hạn chế, khoảng 30 token mỗi câu), có sự giảm hiệu suất lớn trên các tài liệu dài như TED, Europarl, và PDC. Những kết quả này được thu được bởi mã nguồn mở của họ. Chúng tôi đề xuất rằng tất cả các nghiên cứu liên quan đến hiệu quả nên được xác minh trên các chuỗi tầm xa thực tế. Nếu không, một số rủi ro tiềm ẩn có thể bị bỏ qua.

Tổng thể, Lasformer đạt được kết quả tốt nhất, không chỉ giảm tính toán attention và tăng tốc độ suy luận end-to-end hiệu quả mà còn duy trì chất lượng dịch thuật. Đáng chú ý, chúng tôi cắt giảm chi phí attention xuống 7%, điều này quan trọng cho sự tăng trưởng bậc hai với độ dài chuỗi.

Trong khi đó, ngoài BLEU, chúng tôi tiến hành thực nghiệm trên test set cấp tài liệu để đánh giá khả năng sử dụng ngữ cảnh tài liệu. Chúng tôi không sử dụng contrastive test set (Voita et al., 2019; Bawden et al., 2018) vì instance của chúng chỉ chứa tối đa 5 câu. Thay vào đó, chúng tôi kiểm tra mô hình trên PDC (Sun et al., 2022b), bao gồm Tense Consistency (TC), Conjunction Presence (CP), Pronoun Translation (PT) và điểm tổng thể TCP là trung bình hình học của các điểm trên. Bảng 2 cho thấy mô hình của chúng tôi đạt được kết quả tương đương với baseline Transformer và chiến lược lựa chọn của chúng tôi giữ lại các token quan trọng để xử lý tính mạch lạc diễn ngôn.

5 Phân tích
Trong phần này, chúng tôi sẽ đi sâu vào phương pháp và phân tích một số phần quan trọng và hiện tượng thú vị. Trừ khi có giải thích bổ sung, cài đặt cơ bản của tất cả các thực nghiệm như sau: t = 0.95, ds = 64, r = 2. Bộ dữ liệu là PDC.

5.1 Phân phối độ thưa
Vì hiệu quả của mô hình hoàn toàn dựa vào mẫu thưa topk đã học, đó là mối quan tâm chính của chúng tôi rằng độ thưa có thể đạt đến mức nào. Như thể hiện trong Bảng 3, Lasformer cho kết quả attention rất thưa.

Chúng tôi cũng thấy mức độ thưa giữa các module khác nhau là khác nhau. Decoder self-attention có thể đạt độ thưa cực kỳ 2%, cho thấy phần lớn ngữ cảnh quá khứ không quan trọng đối với mô hình ngôn ngữ. Trong khi encoder self-attention chỉ cho thấy độ thưa 10%. Chúng tôi đề xuất rằng phân phối attention ở phía nguồn tương đối phẳng nên mô hình cần nhiều token hơn. Xét encoder không tự hồi quy, việc giảm mạnh phía decoder, bao gồm cross-attention và self-attention, có thể tăng đáng kể hiệu quả. Và ngay cả dưới độ thưa lớn như vậy, Lasformer vẫn có thể đạt kết quả dịch thuật tương đương.

5.2 Nghiên cứu loại bỏ
Bảng 4 cho thấy hiệu quả của các module khác nhau chúng tôi đề xuất.

"- Top-k Selection" có nghĩa là chúng tôi từ bỏ Top-k selection. Thay vào đó, chúng tôi giới hạn phạm vi attention trong cửa sổ cố định có tâm là query và độ dài là 20. Mặc dù có chi phí attention thấp hơn, sự suy giảm chất lượng cho thấy prior con người ngây thơ không mạnh mẽ và dẫn đến sụt giảm chất lượng.

"- Attention Supervision" có nghĩa là chúng tôi đặt α trong công thức 6 là 0, do đó không ràng buộc tính nhất quán giữa attention của lớp lựa chọn và lớp gốc. Do đó, điểm BLEU có sự sụt giảm lớn, cho thấy tầm quan trọng của attention supervision. Và thiếu giám sát có thể gây ra sự thất bại của các transformer hiệu quả dựa trên độ thưa trước đây.

"- Re-parameter trick" có nghĩa là chúng tôi không sử dụng công thức 3 để các tham số của lớp lựa chọn chỉ được huấn luyện bởi attention supervision loss và không đóng góp vào NMT loss. Điểm BLEU có sự sụt giảm nhỏ, cho thấy re-parameter trick hữu ích.

Nó đạt kết quả tương đương nhưng tăng đáng kể chi phí tính toán.

5.3 t và k: Hiệu ứng độ thưa
Mức độ thưa k cũng là chỉ số quan trọng. Chúng tôi tiến hành một số thực nghiệm để kiểm tra mối quan hệ giữa tổng attention t và độ thưa k. Như thể hiện trong Bảng 5, yêu cầu tổng attention thấp hơn mang lại độ thưa nhiều hơn nhưng cũng BLEU thấp hơn một chút. Chúng tôi chọn t = 0.95 như một sự cân bằng.

5.4 N: Càng dài, càng hiệu quả
Vì phương pháp của chúng tôi nhắm vào các chuỗi tầm xa, việc xem xét hiệu ứng của độ dài chuỗi là cần thiết. Chúng tôi tính toán tổng chi phí attention (bao gồm chiếu tuyến tính QKV) với độ dài chuỗi.

Như thể hiện trong Hình 2, khi chuỗi dài hơn, tỷ lệ chi phí attention giảm dần, có nghĩa là chúng tôi đạt được hiệu quả ngày càng cao. Đối với chuỗi cực dài như 8K token, chúng tôi có thể giảm chi phí attention xuống 15% (7% nếu không bao gồm chiếu tuyến tính QKV). Điều này cho thấy tiềm năng phi thường của phương pháp chúng tôi. Khi phạm vi dịch thuật ngày càng rộng (ví dụ một cuốn sách hoặc phim hoàn chỉnh), Lasformer có thể đạt được hiệu quả cao.

5.5 ds: Đường cong hình U với chi phí
Rõ ràng, chiều thấp hy sinh độ chính xác của mô hình để giảm chi phí tính toán. Do đó, đây là sự cân bằng giữa hiệu quả và hiệu suất. Chúng tôi tiến hành một loạt thực nghiệm. Bảng 6 cho thấy hiệu quả và hiệu suất dưới các chiều khác nhau.

Chúng tôi thấy rằng chiều thấp 32 là đủ cho lựa chọn thô, trong khi chiều 16 làm hại hiệu suất. Ngoài ra, chiều thấp hơn của lớp lựa chọn có thể mang lại độ thưa k cao hơn, ngược lại làm tăng chi phí tính toán. Ngay cả khi chúng tôi chỉ tập trung vào hiệu quả, chiều thấp nhất không có nghĩa là chi phí thấp nhất. Chi phí attention giảm rồi tăng khi ds giảm. Do đó, chúng tôi chọn ds = 64 làm cài đặt cuối cùng.

5.6 r: Chia sẻ lớp giúp ích
Điểm khác là mẫu thưa được thu được trong một lớp lựa chọn và áp dụng cho tất cả các lớp trong một nhóm lớp. Chúng tôi đề xuất rằng một số lớp liền kề chia sẻ cùng chức năng nên attention của chúng có thể được chia sẻ cùng nhau. Ví dụ, các lớp thấp hơn được kỳ vọng học thông tin cú pháp trong khi các lớp cao hơn được kỳ vọng học thông tin ngữ nghĩa. Vì vậy một số phân phối attention có thể được chia sẻ qua các lớp. Như thể hiện trong Bảng 7, chia sẻ lớp hơi tăng k và giảm BLEU. Chúng tôi đề xuất rằng chia sẻ quá nhiều lớp giới hạn khả năng mô hình trong khi không chia sẻ dẫn đến một số dư thừa. Lấy r = 3 cho kết quả tốt nhất.

5.7 Trực quan hóa: Mẫu Attention
Hình 3 hiển thị các mẫu thưa trên encoder self-attention, cross-attention, và decoder self-attention.

Một mặt, tồn tại một số đặc điểm chung, như: 1) Hầu hết các token thích chú ý đến các token gần đó. 2) Một số token phục vụ như token toàn cục mà hầu hết tất cả các token chú ý đến nó, có thể là một số dấu câu. Những đặc điểm này chia sẻ cùng ý tưởng với prior con người (Child et al., 2019; Beltagy et al., 2020).

Mặt khác, cũng có nhiều phân phối không theo quy tắc, bao gồm các token rất xa. Chúng tôi đề xuất rằng ngữ cảnh tầm xa có thể đóng góp cho token hiện tại như thì hoặc đại từ (Sun et al., 2022b). Những attention trôi dạt này không thể được xử lý bởi prior con người trong khi Lasformer có thể xử lý tốt.

6 Kết luận
Trong bài báo này, chúng tôi tập trung vào hiệu quả dịch thuật cấp tài liệu tầm xa do chi phí tăng trưởng bậc hai với độ dài. Tuy nhiên, các nghiên cứu trước đây gặp phải sự sụt giảm hiệu suất nghiêm trọng khi suy luận các chuỗi dài thực tế. Để giải quyết vấn đề này, chúng tôi đề xuất chọn các token quan trọng với lightweight attention, được giám sát bởi attention gốc. Lasformer đề xuất hiệu quả giảm chi phí attention đồng thời duy trì thành công chất lượng dịch thuật. Hóa ra chỉ khoảng 5% attention là cần thiết và chi phí attention có thể được giảm xuống 7%. Cuối cùng, chúng tôi đạt được tăng tốc tổng thể 20%.

Hạn chế
Hạn chế chính của công trình này là việc giảm chi phí không phản ánh tăng tốc thực tế, bị ảnh hưởng bởi các module tuyến tính và tối ưu hóa GPU.

Các module tuyến tính bao gồm các lớp embedding, chiếu query, key, value, và mạng feed-forward. Thực tế, chúng là nút thắt cổ chai chính khi độ dài chuỗi ngắn. Chúng tôi kiểm tra chi phí thời gian cho các module khác nhau của độ dài đầu vào khác nhau và thấy rằng các module attention trở thành nút thắt cổ chai (trên 50%) chỉ khi độ dài đầu vào trên 1500 token. Do đó, tăng tốc tương đối nhỏ khi đầu vào ngắn.

Tối ưu hóa GPU là mối quan tâm quan trọng khác. Đầu tiên, do thuộc tính tính toán song song, lớp tuyến tính 512 x 32 không nhanh hơn 8 lần so với lớp tuyến tính 512 x 512. Nó phụ thuộc vào kiến trúc GPU và thậm chí kích thước batch. Nhiều lõi GPU hơn và kích thước batch nhỏ dẫn đến sử dụng GPU thấp hơn và tăng tốc nhỏ. Thứ hai, mô hình thưa không nhanh bằng mô hình dày đặc về mặt truy cập bộ nhớ GPU và pre-fetching, vì vậy chi phí đọc bộ nhớ nhiều hơn là không thể tránh khỏi, làm hại tăng tốc end-to-end cuối cùng.

Lời cảm ơn
Chúng tôi muốn cảm ơn các nhà đánh giá ẩn danh vì những bình luận sâu sắc của họ. Một phần của công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62376116, 62176120), Quỹ Nghiên cứu Tỉnh Liêu Ninh cho Nghiên cứu Cơ bản (Số 2022-KF-26-02).
