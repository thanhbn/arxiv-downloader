# Sabi´ a: Các Mô hình Ngôn ngữ Lớn Tiếng Bồ Đào Nha
Ramon Pires, Hugo Abonizio, Thales Sales Almeida, và Rodrigo Nogueira
Maritaca AI
{ramon,hugo,thales,rodrigo }@maritaca.ai
Tóm tắt. Khi khả năng của các mô hình ngôn ngữ tiếp tục phát triển, có thể hình dung rằng mô hình "một kích cỡ phù hợp với tất cả" sẽ vẫn là mô hình chủ đạo. Ví dụ, với số lượng lớn các ngôn ngữ trên toàn thế giới, nhiều trong số đó là ngôn ngữ có ít tài nguyên, thực hành phổ biến là huấn luyện trước một mô hình duy nhất trên nhiều ngôn ngữ. Trong bài báo này, chúng tôi bổ sung vào bằng chứng ngày càng tăng thách thức thực hành này, chứng minh rằng việc huấn luyện trước đơn ngữ trên ngôn ngữ mục tiêu cải thiện đáng kể các mô hình đã được huấn luyện rộng rãi trên các corpus đa dạng. Cụ thể hơn, chúng tôi tiếp tục huấn luyện trước các mô hình GPT-J và LLaMA trên các văn bản tiếng Bồ Đào Nha sử dụng 3% hoặc ít hơn ngân sách huấn luyện trước ban đầu của chúng. Các đánh giá few-shot trên Poeta, một bộ gồm 14 tập dữ liệu tiếng Bồ Đào Nha, cho thấy các mô hình của chúng tôi vượt trội hơn các đối tác tập trung vào tiếng Anh và đa ngữ với một khoảng cách đáng kể. Mô hình tốt nhất của chúng tôi, Sabi´ a-65B, hoạt động ngang bằng với GPT-3.5-turbo. Bằng cách đánh giá trên các tập dữ liệu ban đầu được tạo ra bằng ngôn ngữ mục tiêu cũng như các tập được dịch, chúng tôi nghiên cứu tác động của việc huấn luyện trước đặc thù ngôn ngữ về mặt 1) nắm bắt các sắc thái và cấu trúc ngôn ngữ vốn có của ngôn ngữ mục tiêu, và 2) làm phong phú kiến thức của mô hình về một lĩnh vực hoặc văn hóa. Kết quả của chúng tôi chỉ ra rằng hầu hết lợi ích xuất phát từ kiến thức đặc thù lĩnh vực có được thông qua việc huấn luyện trước đơn ngữ. Cuối cùng, chúng tôi cho thấy mô hình tiếng Bồ Đào Nha của chúng tôi có hiệu suất thấp hơn trong các nhiệm vụ tiếng Anh, từ đó củng cố sự đánh đổi vốn có trong việc tinh chỉnh các mô hình cho các lĩnh vực cụ thể. Sabi´ a-7B có sẵn tại HuggingFace: https://huggingface.co/maritaca-ai/sabia-7b

1 Giới thiệu
Các Mô hình Ngôn ngữ đã cách mạng hóa lĩnh vực xử lý ngôn ngữ tự nhiên với khả năng đặc biệt trong việc thực hiện các nhiệm vụ với sự giám sát tối thiểu. Mặc dù chủ yếu được huấn luyện trước trên các corpus tập trung vào tiếng Anh, các mô hình đã cho thấy khả năng đa ngữ ấn tượng [10]. Với sự phong phú của các ngôn ngữ trên toàn thế giới, đa số trong số đó là ngôn ngữ có ít tài nguyên, việc huấn luyện trước các mô hình đơn lẻ trên nhiều ngôn ngữ đồng thời đã trở thành một thực hành phổ biến. Các mô hình như XLM-R [12], mBART [28], mT5 [70], và BLOOM [54] minh họa cho cách tiếp cận này.

Bất chấp thành công của các mô hình đa ngữ này, chúng tôi cho rằng chúng có thể không phải là cách tiếp cận tối ưu để nắm bắt sự phong phú văn hóa và kiến thức vốn có trong từng ngôn ngữ riêng lẻ. Khi có sẵn một corpus đặc thù ngôn ngữ có quy mô vừa phải, việc tiếp tục huấn luyện trước có thể tích hợp kiến thức còn thiếu vào mô hình, nâng cao hiệu suất của nó trong các nhiệm vụ được nhắm đến. Để kiểm tra giả thuyết này, chúng tôi mở rộng việc huấn luyện trước của các mô hình tập trung vào tiếng Anh bằng cách sử dụng các corpus tiếng Bồ Đào Nha và đánh giá hiệu suất của chúng trên một phạm vi rộng các tập dữ liệu tiếng Bồ Đào Nha sử dụng phương pháp học few-shot. Kết quả của chúng tôi chỉ ra rằng, ngay cả đối với các mô hình được huấn luyện vượt quá các khuyến nghị của Hoffmann et al [18], việc huấn luyện trước bổ sung này cải thiện đáng kể hiệu suất so với các mô hình đa ngữ.

Chúng tôi đánh giá các mô hình của mình trên các tập dữ liệu bao gồm các văn bản ban đầu được tạo ra bởi những người nói tiếng Bồ Đào Nha Brazil bản địa, cũng như các tập dữ liệu được dịch từ tiếng Anh sang tiếng Bồ Đào Nha. Chúng tôi quan sát thấy cải thiện trên tất cả các tập dữ liệu do việc huấn luyện trước tiếng Bồ Đào Nha, với những cải thiện đặc biệt rõ rệt đối với các tập dữ liệu được tạo ra bởi những người nói Brazil. Một trong những cải thiện lớn nhất được quan sát thấy trên tập dữ liệu ENEM [57], có nguồn gốc từ các kỳ thi đầu vào được sử dụng bởi các trường đại học Brazil và đòi hỏi kiến thức rộng rãi về lịch sử, địa lý và văn học của đất nước. Kết quả này cung cấp bằng chứng rằng đóng góp chính của việc huấn luyện trước đặc thù ngôn ngữ của chúng tôi là tiêm kiến thức đặc thù lĩnh vực về một nền văn hóa cụ thể thay vì chỉ đơn thuần nâng cao thành thạo ngôn ngữ.

2 Công trình liên quan
Thành công của việc huấn luyện trước đa ngữ đã được ghi nhận rõ ràng trong tài liệu, với các mô hình như ByT5 [69], mT5 [70], XLM-R [12], XGLM [27] và mGPT [56] mở đường cho việc hiểu và tạo sinh ngôn ngữ toàn diện hơn bằng cách tận dụng kiến thức được chia sẻ qua nhiều ngôn ngữ. Tuy nhiên, có những hạn chế đối với cách tiếp cận này.

BLOOM, một mô hình 175B tham số được huấn luyện trước trên 46 ngôn ngữ, hoạt động kém hơn trong các nhiệm vụ tiếng Anh so với OPT [74], một mô hình có kích thước tương tự được huấn luyện trước trên các corpus tập trung vào tiếng Anh sử dụng tài nguyên tính toán và kích thước dữ liệu có thể so sánh. Chúng tôi phỏng đoán rằng hiệu suất kém của BLOOM có thể do việc tiếp xúc tương đối hạn chế với các token tiếng Anh trong giai đoạn huấn luyện trước. Do đó, quan sát này gợi ý rằng việc huấn luyện trước đơn ngữ có thể mang lại những lợi thế bổ sung.

Để hỗ trợ giả thuyết này, các mô hình với hàng trăm triệu tham số được huấn luyện trước trên các văn bản đơn ngữ đã chứng minh được những cải thiện so với các đối tác đa ngữ [7,59,6,52,32,24,8,2,25,36,21]. Ngoài ra, nghiên cứu đã chỉ ra rằng việc thích ứng ngôn ngữ có lợi ngay cả đối với các ngôn ngữ có ít tài nguyên [38,13,4,72]. Tuy nhiên, có một số lượng hạn chế các bài báo nghiên cứu được công bố với các đánh giá toàn diện về lợi ích của việc tiếp tục huấn luyện trước ở quy mô đa tỷ tham số [50,73,22]. Thông qua nghiên cứu này, chúng tôi đóng góp vào tài liệu bằng cách chứng minh hiệu quả của việc tiếp tục huấn luyện trước đặc thù ngôn ngữ cho các mô hình ngôn ngữ tiếng Bồ Đào Nha lên đến quy mô 65B tham số.

Câu hỏi về việc liệu có lợi khi huấn luyện các mô hình cho các ngôn ngữ cụ thể hay không có liên quan chặt chẽ với câu hỏi về việc liệu có lợi khi huấn luyện các mô hình cho các lĩnh vực kiến thức cụ thể hay không. Các nghiên cứu gần đây, như Minerva [26] và Galactica [62], đã cho thấy rằng việc huấn luyện trước đặc thù lĩnh vực có thể dẫn đến những cải thiện đáng kể, ngay cả với một corpus huấn luyện trước nhỏ hơn so với các corpus huấn luyện trước quy mô lớn, đa mục đích. Tương tự, Fu et al. [15] đã chứng minh tính khả thi của việc chuyên môn hóa các mô hình nhỏ hơn để thực hiện lý luận đa bước, một khả năng thường chỉ dành cho các mô hình có ít nhất 50B tham số, với cái giá là hiệu suất giảm sút trong các nhiệm vụ khác, tổng quát hơn.

Huấn luyện trước với sự kết hợp của các corpus tổng quát và đặc thù lĩnh vực có thể nâng cao hiệu suất trong các nhiệm vụ chuyên môn mà không làm giảm hiệu quả trong các nhiệm vụ đa mục đích, mặc dù với chi phí tăng yêu cầu tính toán. Ví dụ, BloombergGPT [68], một mô hình 50B tham số được huấn luyện trước trên corpus không đồng nhất trong đó hơn một nửa các văn bản đến từ lĩnh vực tài chính, thể hiện hiệu suất tương đương với OPT-66B trong các nhiệm vụ tổng quát. Tuy nhiên, tập dữ liệu huấn luyện trước của BloombergGPT lớn gấp ba lần, và do đó sử dụng nhiều tài nguyên tính toán hơn.

Thay vì theo đuổi một mô hình duy nhất hoạt động tốt qua nhiều lĩnh vực, Gururangan et al. [17] đề xuất một cách tiếp cận thay thế: sử dụng nhiều mô hình chuyên gia, mỗi mô hình được huấn luyện trên một tập con đặc thù lĩnh vực trong một tập dữ liệu rộng lớn, đa dạng hơn, để hoạt động như một mô hình đa mục đích duy nhất. Các mô hình của họ vượt trội hơn các mô hình dày đặc qua nhiều nhiệm vụ đặc thư lĩnh vực, với cái giá là số lượng tham số tăng, do đó dẫn đến yêu cầu bộ nhớ lớn hơn để suy luận hiệu quả.

3 Phương pháp luận
Trong phần này, chúng tôi nêu ra dữ liệu huấn luyện trước và chi tiết huấn luyện được sử dụng để xây dựng các mô hình của chúng tôi, bao gồm các nguồn dữ liệu, kỹ thuật tiền xử lý, kiến trúc, siêu tham số và phương pháp tối ưu hóa.

3.1 Dữ liệu huấn luyện trước
Dữ liệu huấn luyện trước được lấy từ tập con tiếng Bồ Đào Nha của tập dữ liệu ClueWeb 2022 [40,41]. Để tăng chất lượng của tập dữ liệu, chúng tôi áp dụng các bộ lọc chất lượng từ MassiveText [45], sửa đổi chúng để phù hợp với các yêu cầu cụ thể của ngôn ngữ tiếng Bồ Đào Nha. Chúng tôi chuẩn hóa văn bản với ftfy, chuyển đổi wikitexts thành các văn bản có thể đọc được của con người, và loại trừ các tài liệu chứa ít hơn 200 token duy nhất.

Các bộ lọc chất lượng này chủ yếu được thiết kế cho các trang web và có thể không chuyển tiếp một cách liền mạch sang các lĩnh vực khác. Có tiềm năng cải thiện bằng cách sử dụng các phương pháp tự động hóa hơn; tuy nhiên, nghiên cứu này không khám phá các cách tiếp cận như vậy do bản chất tốn tài nguyên của các thí nghiệm huấn luyện trước.

Sau quá trình làm sạch, tất cả các tài liệu được nối với nhau bằng cách sử dụng một token kết thúc chuỗi làm dấu phân cách, và sau đó được token hóa. Tokenizer GPT-J, giống hệt với tokenizer GPT-2 [44], tạo ra 7.8 tỷ token, trong khi tokenizer LLaMA tạo ra 7.3 tỷ token. Sự khác biệt trong tổng số token chủ yếu do các chiến lược token hóa khác nhau mà mỗi mô hình sử dụng, byte-level BPE và BPE dựa trên sentencepiece [23], tương ứng cùng với sự biến đổi của các từ vựng được sử dụng bởi mỗi tokenizer.

Chúng tôi mở rộng việc huấn luyện của ba mô hình — LLaMA 7B và 65B [63] cũng như GPT-J [66] — ban đầu được huấn luyện trên các corpus tập trung vào tiếng Anh, trên các văn bản tiếng Bồ Đào Nha; các mô hình được huấn luyện trước thêm từ LLaMA được ký hiệu là Sabi´ a, trong khi mô hình có nguồn gốc từ GPT-J được gọi là Sabi´ a-J.

3.2 Các mô hình Sabi´ a
Các mô hình LLaMA 7B và 65B là các mô hình Transformer chỉ có bộ giải mã [64] với kiến trúc tương tự như PALM [10]. Các mô hình được huấn luyện sử dụng mục tiêu mô hình hóa ngôn ngữ nhân quả trên một tập dữ liệu khổng lồ có nguồn từ các trang web, mã nguồn, sách và bài báo khoa học. Mô hình 7B được huấn luyện trên 1 nghìn tỷ token và mô hình 65B được huấn luyện trên 1.4 nghìn tỷ token. Trong khi phần lớn corpus là tiếng Anh, nó cũng bao gồm một lượng văn bản tiếng Bồ Đào Nha không được chỉ định.

Bắt đầu từ các trọng số LLaMA, chúng tôi huấn luyện các mô hình Sabi´ a trên tập dữ liệu tiếng Bồ Đào Nha của chúng tôi (xem Phần 3.1) sử dụng các framework t5x và seqio [48]. Tuân thủ chặt chẽ các siêu tham số được sử dụng bởi PALM, chúng tôi sử dụng trình tối ưu AdaFactor [55] không có phân tích nhân tử, động lượng bậc nhất β1= 0.9, và động lượng bậc hai β2= 1−k−0.8, trong đó k đại diện cho số bước.

Chúng tôi áp dụng cắt chuẩn tắc toàn cục tại 1.0 và suy giảm trọng số động của lr2, với lr biểu thị tốc độ học hiện tại.

Bên cạnh hàm mất mô hình hóa ngôn ngữ nhân quả tiêu chuẩn, chúng tôi sử dụng một hàm mất phụ trợ của 10−4log2(∑iezi), trong đó z là các logit, để giảm khả năng xảy ra các đỉnh mất mát ở quy mô 65B tham số. Tốc độ học được tăng tuyến tính từ 0 đến 1e-3 trong 1,000 bước đầu, theo sau là tốc độ học không đổi 1e-3 trong thêm 9,000 bước.

Các mô hình được huấn luyện trên TPU v2-512, sử dụng các batch 512 chuỗi, mỗi chuỗi chứa 2048 token. Chúng tôi sử dụng gradient checkpointing, còn được gọi là rematerialization, để cho phép sử dụng các batch lớn hơn, từ đó tăng việc sử dụng TPU. Đối với mô hình 7B, cấu hình này dẫn đến thông lượng 124,000 token/giây, tương ứng với Model FLOPs Utilization (MFU) [10] là 45.2%, loại trừ các phép toán tự chú ý. Đối với mô hình 65B, chúng tôi đạt được thông lượng 14,000 token/giây, dẫn đến MFU là 47.4%.

Các mô hình kết quả được huấn luyện trên tổng cộng 10.4 tỷ token, hoặc 1.52 epoch của tập dữ liệu tiếng Bồ Đào Nha. Điều này tương đương với 10,000 bước huấn luyện. Chúng tôi nhận thấy các cải thiện trong các nhiệm vụ few-shot vượt quá một epoch, điều này củng cố kết quả từ Taylor et al. [62]. Tuy nhiên, do chi phí cao của việc huấn luyện trước, chúng tôi đã không tiếp tục huấn luyện.

3.3 Sabi´ a-J
Mô hình GPT-J là một mô hình Transformer chỉ có bộ giải mã 6B tham số có kiến trúc và siêu tham số huấn luyện tuân theo chặt chẽ GPT-3 6.7B. Những khác biệt chính nằm ở việc tính toán MLP và tự chú ý song song, áp dụng đầu chú ý với chiều 256 (lớn gấp đôi so với GPT-3 6.7B), và sử dụng Rotary Positional Embedding (RoPE) [61]. GPT-J được huấn luyện trên 400B token từ tập dữ liệu The Pile [16], trong đó 97.4% token là tiếng Anh.

Chúng tôi bắt đầu huấn luyện Sabi´ a-J từ checkpoint GPT-J được phát hành, sử dụng framework mesh-transformer-jax [65] và trình tối ưu AdamW [30] với suy giảm trọng số 0.1. Chúng tôi bắt đầu huấn luyện trước bằng cách làm nóng tốc độ học đến 1.2e-5 trong 13,500 bước, theo sau là suy giảm cosine annealing trong 135,518 bước đến tốc độ học cuối 2.4e-6, và giữ nó không đổi từ đó trở đi. Chúng tôi huấn luyện trên TPU v3-8 sử dụng kích thước batch hiệu quả 32 chuỗi của 2048 token. Điều này dẫn đến thông lượng 5,200 token/giây, tương ứng với MFU 44.5% không có tự chú ý. Mô hình được huấn luyện trong 18 ngày trên 7.8B token, hoặc một epoch của tập dữ liệu tiếng Bồ Đào Nha.

4 Đánh giá trên Poeta
Chúng tôi đánh giá các mô hình Sabi´ a trên benchmark Portuguese Evaluation Tasks (Poeta), bao gồm 14 tập dữ liệu NLP downstream bằng tiếng Bồ Đào Nha: ASSIN 2 RTE và STS [47], ENEM Challenge [57], ENEM 2022 [37], FaQuAD [53], TweetSentBr [5], AG News [75], IMDB [31], MASSIVE [14], MKQA [29], BoolQ [11], SST2 [58], WSC [33], và BLUEX [1]. Một nửa trong số đó (ASSIN 2 RTE và STS, BLUEX, ENEM Challenge, ENEM 2022, FaQuAD, và TweetSentBr) ban đầu được viết bằng tiếng Bồ Đào Nha, và phần còn lại được dịch thủ công hoặc tự động sang tiếng Bồ Đào Nha từ bản gốc tiếng Anh của chúng. Chúng tôi gọi nhóm đầu tiên là tập dữ liệu "Bản địa" và nhóm thứ hai là tập dữ liệu "Đã dịch".

Các mô hình được đánh giá theo cách few-shot sử dụng số lượng ví dụ tối đa phù hợp với ngữ cảnh 2048 token cho mỗi nhiệm vụ. Chúng tôi sử dụng tokenizer GPT-2 làm tham chiếu vì nó tạo ra nhiều token hơn. Điều này cho phép chúng tôi thoải mái phù hợp với các prompt được token hóa với các tokenizer khác.

Để đánh giá các mô hình, chúng tôi chọn thủ công một tập hợp các ví dụ few-shot cho mỗi tập dữ liệu trên Poeta. Tùy thuộc vào tập dữ liệu, các ví dụ này được cân bằng theo lớp (ngoại trừ FaQuAD, BLUEX, ENEM Challenge, ENEM 2022, MKQA, và WSC). Đối với mỗi ví dụ kiểm tra, các prompt được xây dựng với các ví dụ few-shot đã chọn theo thứ tự luân phiên. Mỗi nhiệm vụ trên Poeta có một hướng dẫn cụ thể được đặt ở đầu prompt.

Theo Srivastava et al [60], chúng tôi áp dụng Normalized Preferred Metric (NPM) làm thước đo đánh giá chính:

NPM=1/N ∑(i=1 to N) 100×([raw preferred metric]i−[random score]i)/([high score]i−[random score]i)

trong đó N là số lượng tập dữ liệu đánh giá, [raw preferred metric]i là điểm số thu được bởi mô hình trên tập dữ liệu thứ i, [random score]i là điểm số của một mô hình ngẫu nhiên (ví dụ, 50% cho một nhiệm vụ phân loại nhị phân) và [high score]i là điểm số cao nhất có thể trên tập dữ liệu đó, là 1 hoặc 100. Thước đo ưa thích và điểm số ngẫu nhiên cho mỗi tập dữ liệu được trình bày trong Bảng 1. Lý do đằng sau việc sử dụng NPM thay vì một trung bình đơn giản qua tất cả các tập dữ liệu là để giảm thiểu ảnh hưởng không đáng có của các tập dữ liệu với điểm số vốn cao, như các tập dữ liệu phân loại nhị phân, có thể làm lu mờ các tập dữ liệu được đặc trưng bởi điểm số thấp hơn.

5 Kết quả
Các kết quả chính có thể được tìm thấy trong Bảng 2. Các mô hình như BLOOMZ, XGLM và Bertin-GPT gặp khó khăn trong việc tạo ra câu trả lời bằng tiếng Bồ Đào Nha. Để giải quyết vấn đề này, chúng tôi áp dụng một cách tiếp cận tương tự như được sử dụng bởi các tác giả XGLM: bằng cách tính toán khả năng của mỗi chuỗi câu trả lời ứng viên dựa trên văn bản đầu vào và sau đó chọn lớp có xác suất cao nhất. Đối với FaQuAD, tập dữ liệu duy nhất trong benchmark không có câu trả lời ứng viên được xác định trước, chúng tôi cho phép các mô hình tạo ra câu trả lời ở định dạng gốc của chúng.

Chúng tôi quan sát thấy rằng các baseline LLaMA vượt trội đáng kể so với các mô hình có kích thước tương đương được huấn luyện với ít token hơn, như Galactica và OPT. Hơn nữa, mặc dù được huấn luyện trên các corpus tập trung vào tiếng Anh, LLaMA-7B vượt qua các BLOOM và XGLM đa ngữ có kích thước tương tự. Các mô hình Sabi´ a cho thấy cải thiện đáng kể trong NPM so với các mô hình baseline tương ứng của chúng. Những cải thiện NPM này đáng kể hơn đối với các mô hình Sabi´ a-J và Sabi´ a-7B nhỏ hơn. Đáng chú ý, Sabi´ a-65B có hiệu suất vượt trội một chút so với GPT-3.5-turbo của OpenAI, mô hình này phục vụ như mô hình cơ sở cho ChatGPT.

Thông qua việc huấn luyện trước tiếng Bồ Đào Nha của chúng tôi, chúng tôi quan sát thấy rằng cải thiện trong NPM cao hơn trong các tập dữ liệu bản địa so với các tập dữ liệu đã dịch. Đối với Sabi´ a-65B, các cải thiện so với LLaMA-65B chủ yếu đến từ tập con bản địa. Chúng tôi đưa ra giả thuyết rằng điều này là do bản chất "cơ học" của các tập dữ liệu đã dịch: vì chúng được dịch từ tiếng Anh, mô hình baseline đã sở hữu kiến thức cần thiết để giải quyết chúng và ít được lợi từ việc học kiến thức ngôn ngữ học, cú pháp và ngữ pháp của ngôn ngữ mục tiêu. Ví dụ, để trả lời câu hỏi "does p o box come before street address" (tập dữ liệu BoolQ), mô hình ít được lợi từ việc huấn luyện trước bổ sung trên corpus tiếng Bồ Đào Nha vì không có khả năng corpus sẽ cung cấp thông tin mới về định dạng địa chỉ gửi thư Mỹ mà mô hình chưa gặp phải trong quá trình huấn luyện trước tập trung vào tiếng Anh ban đầu. Ngược lại, việc huấn luyện trước đặc thù ngôn ngữ giới thiệu kiến thức cụ thể cần thiết để giải quyết các nhiệm vụ trong tập con bản địa.

Mặc dù GPT-J thể hiện hiệu suất few-shot thấp hơn trong các nhiệm vụ tiếng Anh so với LLaMA, chúng tôi sử dụng nó trong nghiên cứu này để minh họa rằng không chỉ các mô hình được tối ưu hóa cao như LLaMA mới có thể hưởng lợi từ việc mở rộng huấn luyện trước. Chúng tôi chọn không sử dụng BLOOM-7.1B làm checkpoint ban đầu cho việc huấn luyện trước do hiệu suất kém hơn so với GPT-J trong các thí nghiệm few-shot sơ bộ trên ba tập dữ liệu tiếng Bồ Đào Nha. Tuy nhiên, chúng tôi sau đó phát hiện ra rằng hiệu suất của nó trên Poeta vượt qua GPT-J. Tuy vậy, BLOOM vẫn thể hiện hiệu suất thấp hơn so với LLaMA.

Tương tự như Sabi´ a-J, BERTIN-GPT là một mô hình được huấn luyện trước trên văn bản tiếng Tây Ban Nha bắt đầu từ các trọng số GPT-J. Vì tiếng Tây Ban Nha và tiếng Bồ Đào Nha là những ngôn ngữ tương tự, hợp lý khi mong đợi rằng BERTIN-GPT sẽ hoạt động tốt hơn so với mô hình baseline của nó. Tuy nhiên, NPM quan sát được cho BERTIN-GPT chỉ cao hơn một chút so với GPT-J.

Một so sánh đáng chú ý liên quan đến Galactica, một mô hình được huấn luyện trước trên văn bản khoa học, chủ yếu bằng tiếng Anh, và một mô hình OPT có kích thước tương tự, sử dụng tính toán huấn luyện trước có thể so sánh nhưng được huấn luyện trước trên một corpus tiếng Anh tập trung lớn hơn và đa dạng hơn. Trong nghiên cứu của họ, các tác giả chứng minh rằng Galactica hoạt động ngang bằng với OPT trong các nhiệm vụ tiếng Anh và vượt trội hơn OPT trong các nhiệm vụ liên quan đến khoa học. Ngược lại, OPT vượt trội đáng kể so với Galactica trong các nhiệm vụ tiếng Bồ Đào Nha. Kết quả này nhấn mạnh các đánh đổi liên quan đến việc chuyên môn hóa đặc thù lĩnh vực, thường kéo theo hiệu suất giảm sút trong các nhiệm vụ khác.

BLOOMZ [35], một mô hình được điều chỉnh hướng dẫn đa ngữ, cho thấy hiệu suất vượt trội so với mô hình BLOOM baseline của nó, sánh ngang với LLaMA có kích thước tương đương. Tuy nhiên, cách tiếp cận huấn luyện trước bằng tiếng Bồ Đào Nha của chúng tôi dường như mang lại kết quả vượt trội, vì Sabi´ a-J vượt qua BLOOMZ mặc dù có nguồn gốc từ một mô hình baseline có hiệu suất thấp hơn. Chúng tôi hình dung việc tiếp tục huấn luyện trước và điều chỉnh hướng dẫn như các kỹ thuật bổ sung được kết hợp trong nghiên cứu tương lai.

5.1 Kết quả theo từng tập dữ liệu
Bảng 3 trình bày kết quả theo từng tập dữ liệu Poeta cho các mô hình Sabi´ a, baseline của chúng, và cho state-of-the-art được giám sát. Các kết quả SOTA được báo cáo cho các tập dữ liệu đã dịch được thu thập bằng cách sử dụng các phiên bản tiếng Anh gốc của chúng [71,76,46,51]. Vì benchmark Poeta loại trừ các ví dụ không thể trả lời của tập dữ liệu MKQA, chúng tôi quyết định không bao gồm kết quả SOTA cho tập dữ liệu này.

Trong các tập dữ liệu thách thức hơn, như ENEM Challenge, ENEM 2022, và BLUEX, có nguồn gốc từ các kỳ thi tuyển sinh vào các trường đại học Brazil, chúng tôi thấy những cải thiện đáng kể nhất do việc huấn luyện trước đặc thù ngôn ngữ. Những cải thiện đáng kể cũng được quan sát thấy trong TweetSentBr, một tập dữ liệu chứa các tweet với nhiều tiếng lóng và tham chiếu đến văn hóa đại chúng Brazil. Chúng tôi đưa ra giả thuyết rằng việc huấn luyện trước này truyền đạt kiến thức cụ thể về văn hóa, văn học và địa lý của đất nước ít được gặp và học hỏi thường xuyên hơn trong quá trình huấn luyện trước ban đầu với các văn bản đa dạng hơn.

Một số khả năng chỉ xuất hiện ở quy mô, như được chứng minh bởi [67]. Ví dụ, các mô hình 6-7B hoạt động gần với baseline ngẫu nhiên trong các tập dữ liệu như ASSIN 2 RTE và STS, và WSC. Tuy nhiên, ở quy mô 65B, chúng tôi quan sát những cải thiện đáng kể, tiếp cận hoặc vượt qua các mô hình được giám sát state-of-the-art trên các tập dữ liệu ASSIN 2 RTE và FaQuAD.

Kết quả GPT-4 [39] chỉ ra rằng vẫn còn chỗ để cải thiện cho Sabi´ a-65B trong phần lớn các tập dữ liệu được đánh giá trong công việc này. Tuy nhiên, Sabi´ a-65B hoạt động ngang bằng với GPT-4 trong các tập dữ liệu như ASSIN 2 RTE, ENEM Challenge, và FaQuAD.

5.2 Ô nhiễm dữ liệu
Dữ liệu huấn luyện trước cho các mô hình Sabi´ a được thu thập đến tháng 2 năm 2022. Vì ENEM 2022 được công bố công khai vào tháng 11 năm 2022, mô hình không thể có quyền truy cập vào các câu trả lời cho các câu hỏi có trong dữ liệu huấn luyện trước của nó. Do đó, những cải thiện quan sát thấy ít nhất đối với ENEM 2022, cao hơn trung bình của các tập dữ liệu, không thể được quy cho ô nhiễm dữ liệu. Tuy nhiên, đối với các tập dữ liệu khác, khả năng ô nhiễm dữ liệu không thể được loại trừ.

5.3 Nghiên cứu loại bỏ: Các tập dữ liệu tiếng Anh
Trong nghiên cứu loại bỏ này, chúng tôi điều tra tác động tiềm năng của việc huấn luyện trước tiếng Bồ Đào Nha đối với hiệu suất của mô hình trong các tập dữ liệu tiếng Anh. Chúng tôi đánh giá các mô hình LLaMA-7B và Sabi´ a-7B trong các nhiệm vụ trắc nghiệm tiếng Anh. Để đơn giản, chúng tôi sử dụng một thiết lập đánh giá few-shot với 10 ví dụ được chọn ngẫu nhiên (prompt được lấy mẫu động). Quan trọng là, chúng tôi không kết hợp bất kỳ mô tả nào hoặc bao gồm các từ khóa tiếng Bồ Đào Nha để phân định các ví dụ few-shot. Chúng tôi cũng hạn chế tất cả các tập dữ liệu xuống 350 ví dụ kiểm tra.

Theo cách tiếp cận của LLaMA [63], với ngữ cảnh được cung cấp, chúng tôi chọn câu trả lời có khả năng cao nhất được chuẩn hóa theo số lượng ký tự. Kết quả trong Bảng 4 chỉ ra rằng mô hình Sabi´ a-7B thể hiện hiệu suất giảm một chút trong các nhiệm vụ tiếng Anh so với baseline. Kết quả này củng cố tiền đề của chúng tôi rằng việc chuyên môn hóa mô hình luôn kéo theo một cuộc cân bằng, trong đó những cải thiện trong một lĩnh vực thường xuyên trùng hợp với sự suy giảm trong lĩnh vực khác.

6 Hạn chế
Do các ràng buộc tài chính liên quan đến việc huấn luyện trước và, quan trọng hơn, lao động thủ công liên quan đến việc thu thập và tuyển chọn các tập dữ liệu đánh giá, các thí nghiệm được tiến hành độc quyền bằng tiếng Bồ Đào Nha. Với việc các mô hình của chúng tôi bắt đầu huấn luyện trước từ các mô hình được huấn luyện trước tiếng Anh và tiếng Bồ Đào Nha và tiếng Anh thể hiện sự gần gũi ngôn ngữ tương đối, chúng tôi dự đoán rằng các nhà nghiên cứu khác tiến hành huấn luyện trước tiếp theo trên các ngôn ngữ có liên quan chặt chẽ với tiếng Anh sẽ quan sát những cải thiện tương đương trong các nhiệm vụ mục tiêu của họ. Tuy nhiên, việc xác định liệu lợi ích của phương pháp này có tồn tại cho các ngôn ngữ xa hơn với tiếng Anh hay không vẫn là một câu hỏi nghiên cứu mở.

Tiếng Bồ Đào Nha là một ngôn ngữ có sự phong phú của các văn bản chất lượng cao dựa trên web. Do đó, những cải thiện quan sát được với phương pháp được đề xuất có thể không nhất thiết mở rộng đến các ngôn ngữ có ít tài nguyên với tính khả dụng hạn chế của các văn bản chất lượng. Trong những trường hợp như vậy, các phương pháp hiệu quả tham số [19,43,42] có thể có lợi, như được chứng minh bởi Yong et al. [72]. Chúng tôi không sử dụng các kỹ thuật này trong nghiên cứu này do chi phí huấn luyện, tương đương với việc huấn luyện toàn bộ mô hình.

7 Kết luận
Trong nghiên cứu này, chúng tôi đóng góp vào bằng chứng khoa học mở rộng rằng việc chuyên môn hóa các mô hình cho từng ngôn ngữ riêng lẻ dẫn đến các cải thiện, ngay cả khi mô hình baseline lớn và được huấn luyện rộng rãi. Chúng tôi đạt được điều này cho ngôn ngữ tiếng Bồ Đào Nha sử dụng một mô hình gần như state-of-the-art với 65 tỷ tham số. Với chi phí huấn luyện trước tương đối thấp và những cải thiện hiệu suất đáng kể quan sát được, chúng tôi dự thấy một cảnh quan tương lai bao gồm một mảng đa dạng các mô hình, mỗi mô hình được điều chỉnh cho một lĩnh vực cụ thể, thay vì một mô hình duy nhất, bao trùm tất cả.

8 Lời cảm ơn
Chúng tôi cảm ơn Google Cloud vì khoản tài trợ TPU hào phóng.
