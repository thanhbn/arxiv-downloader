# Dịch máy đồng thời với các mô hình ngôn ngữ lớn

Minghan Wang, Thuy-Trang Vu, Jinming Zhao,
Fatemeh Shiri, Ehsan Shareghi, Gholamreza Haffari
Khoa Khoa học Dữ liệu & AI, Đại học Monash
{minghan.wang,trang.vu1,jinming.zhao,
fatemeh.shiri, ehsan.shareghi, gholamreza.haffari} @monash.edu

Tóm tắt

Các hệ thống dịch máy đồng thời (SimulMT) trong thế giới thực đối mặt với nhiều thách thức hơn là chỉ sự đánh đổi giữa chất lượng và độ trễ. Chúng cũng cần giải quyết các vấn đề liên quan đến tính bền vững với đầu vào nhiễu, xử lý ngữ cảnh dài và tính linh hoạt cho việc tiêm kiến thức. Những thách thức này đòi hỏi các mô hình có khả năng hiểu và tạo sinh ngôn ngữ mạnh mẽ mà các mô hình MT chuyên dụng thường không được trang bị. Trong bài báo này, chúng tôi nghiên cứu khả năng áp dụng Mô hình Ngôn ngữ Lớn (LLM) cho các tác vụ SimulMT bằng cách sử dụng các phương pháp giải mã tăng dần hiện có với thuật toán RALCP mới được đề xuất để giảm độ trễ. Chúng tôi đã tiến hành thí nghiệm sử dụng mô hình Llama2-7b-chat trên chín ngôn ngữ khác nhau từ bộ dữ liệu MUST-C. Kết quả cho thấy LLM vượt trội hơn các mô hình MT chuyên dụng về các chỉ số BLEU và LAAL. Phân tích sâu hơn cho thấy LLM có lợi thế về hiệu quả điều chỉnh và tính bền vững. Tuy nhiên, cần lưu ý rằng chi phí tính toán của LLM vẫn là một trở ngại đáng kể đối với việc áp dụng trong SimulMT.

1 Giới thiệu

Dịch máy đồng thời (SimulMT) là một tác vụ rất thách thức, đòi hỏi cả chất lượng cao và độ trễ thấp (Gu et al., 2017a), đồng thời cũng đối mặt với các thách thức khác nhau trong thế giới thực. Vì các hệ thống SimulMT thường là một phần của hệ thống Dịch giọng nói đồng thời (SimulST) được nối tiếp với mô-đun Nhận dạng giọng nói tự động (ASR), những thách thức này bao gồm, nhưng không giới hạn ở: (i) Đầu ra ASR thường chứa lỗi, đòi hỏi một mức độ khả năng chịu lỗi trong mô hình SimulMT (Ruiz và Federico, 2014; Hu và Li, 2022); (ii) SimulMT thường được áp dụng cho các luồng đầu vào gần như vô tận, đòi hỏi nội dung dịch phải duy trì tính nhất quán ngữ cảnh tốt (Radford et al., 2023); (iii) Hệ thống cần dễ dàng tích hợp kiến thức bên ngoài để can thiệp vào nội dung dịch, chẳng hạn như danh sách đen từ nhạy cảm hoặc bản dịch tên cụ thể.

Hầu hết các công trình hiện tại chủ yếu tập trung vào việc xây dựng các mô hình và chính sách SimulMT chuyên dụng để tìm ra sự cân bằng tối ưu giữa chất lượng và độ trễ (Ma et al., 2019a; Chiu và Raffel, 2017; Arivazhagan et al., 2019; Raffel et al., 2017; Gu et al., 2017a; Arthur et al., 2021a; Wang et al., 2022). Một số nỗ lực đã thành công trong việc chuyển đổi các mô hình NMT ngoại tuyến thành các mô hình SimulMT để tránh chi phí đào tạo cao từ đầu (Liu et al., 2020; Nguyen et al., 2021a; Guo et al., 2023; Arivazhagan et al., 2020; Papi et al., 2022a), nhưng họ chưa khám phá đầy đủ những thách thức được đề cập ở trên.

Gần đây, sự phát triển nhanh chóng của các mô hình ngôn ngữ lớn (LLM) đã chứng minh khả năng đa tác vụ và đa ngôn ngữ của chúng, mang lại giải pháp mới cho nhiều tác vụ NLP phức tạp (OpenAI, 2023; Touvron et al., 2023a,b; Bang et al., 2023). Nghiên cứu cho thấy chúng cũng có những lợi thế nhất định trong các tác vụ dịch ngoại tuyến, cụ thể là đối với các ngôn ngữ có tài nguyên cao (Hendy et al., 2023; Zhu et al., 2023; Robinson et al., 2023; Yang et al., 2023). Do đó, việc xem xét liệu khả năng hiểu và tạo sinh mạnh mẽ của LLM có thể được tận dụng để giải quyết các thách thức trong SimulMT hay không là điều tự nhiên.

Tuy nhiên, việc áp dụng LLM vào SimulMT tự nó cũng có những thách thức, chẳng hạn như thiết kế các chính sách đọc-viết phù hợp cho LLM và xử lý hiệu quả các trạng thái nguồn và đích tăng dần, cùng với lợi ích hoặc chi phí của chúng. Do đó, trong bài báo này, chúng tôi đặt ra hai câu hỏi nghiên cứu: (1) liệu chúng ta có thể hiệu quả chuyển đổi các LLM mã nguồn mở sẵn có với những điều chỉnh nhẹ thành các mô hình SimulMT hay không? và (2) liệu việc áp dụng LLM trong SimulMT có giải quyết được một số thách thức nêu trên hay không, và trong quá trình đó, có những hạn chế nào?

Để giải quyết những câu hỏi này, trước tiên chúng tôi chọn Llama2-7b-chat (Touvron et al., 2023b) làm LLM cốt lõi. Sau đó, xem xét chi phí đào tạo đắt đỏ của LLM, chúng tôi chọn tìm một phương pháp có thể trang bị cho LLM khả năng giải mã đồng thời mà không cần đào tạo. Vì vậy, chúng tôi thiết kế chính sách "đọc-n & giải mã tăng dần" dựa trên phương pháp được đề xuất trong (Liu et al., 2020; Nguyen et al., 2021a), cụ thể là giải mã tăng dần với thỏa thuận cục bộ (LA), có thể biến một mô hình chuỗi-tới-chuỗi được đào tạo cụ thể cho giải mã ngoại tuyến thành mô hình hỗ trợ giải mã đồng thời. Hơn nữa, để giải quyết vấn đề độ trễ cao do thuật toán Tiền tố chung dài nhất (LCP) được sử dụng trong giải mã tăng dần, chúng tôi đề xuất thuật toán Tiền tố chung dài nhất thỏa thuận thư giãn (RALCP) để cải thiện việc lựa chọn ứng cử viên để viết trong quá trình giải mã tăng dần, dẫn đến việc giảm đáng kể độ trễ. Sau đó, chúng tôi tiến hành thí nghiệm trên chín cặp ngôn ngữ từ bộ dữ liệu MUST-C (Gangi et al., 2019), so sánh LLM của chúng tôi với các mô hình NMT chuyên dụng như Transformer (Vaswani et al., 2017). Phát hiện của chúng tôi cho thấy LLM có thể vượt trội hơn các mô hình MT chuyên dụng khi sử dụng chính xác cùng một chính sách giải mã. Cuối cùng, chúng tôi tiến hành phân tích chi tiết về các yếu tố khác nhau ảnh hưởng đến việc sử dụng LLM cho SimulMT, bao gồm các lợi thế tiềm năng (ví dụ như cải thiện hiệu quả sử dụng dữ liệu, tính bền vững của đầu vào nhiễu) và hạn chế (ví dụ như vấn đề hiệu quả).

Đóng góp của chúng tôi có thể được tóm tắt như sau:
• Trong bài báo này, chúng tôi sử dụng khung giải mã tăng dần để biến LLM thành mô hình SimulMT và đề xuất RALCP để giải quyết vấn đề độ trễ cao do thuật toán LCP gây ra.
• Chúng tôi chứng minh tiềm năng áp dụng LLM cho các tác vụ SimulMT và minh chứng rằng LLM, sau khi trải qua điều chỉnh tế có giám sát, có thể đạt hiệu suất tương đương với các hệ thống SimulMT chuyên dụng.
• Thông qua phân tích của chúng tôi, chúng tôi phát hiện ra rằng kiến thức trước của LLM có lợi cho việc cải thiện hiệu quả điều chỉnh tế có giám sát trên một số ngôn ngữ nhất định, và cho tính bền vững của đầu vào nhiễu.
• Chúng tôi xác định rằng chi phí tính toán của LLM trong quá trình suy luận là một vấn đề tiềm tàng hạn chế việc áp dụng chúng trong SimulMT.

2 Kiến thức nền tảng

Dịch máy đồng thời (SimulMT) là một tác vụ đòi hỏi mô hình MT phải trả về nội dung dịch với ngữ cảnh nguồn tăng dần theo thời gian thực. Nó có thể được hình thức hóa như một Quá trình quyết định Markov (MDP), trong đó mô hình có thể được coi là hàm chính sách π. Nó nhận trạng thái hiện tại S_t tại thời điểm cụ thể t và trả về một hành động: A_t = π(S_t), trong đó A_t ∈ {R, W}. Ở đây, R đại diện cho việc tiếp tục ĐỌC ngữ cảnh nguồn, và W biểu thị hành động VIẾT phân đoạn dịch gần nhất. Trạng thái S_t thường bao gồm lịch sử của văn bản nguồn đã được đọc và văn bản đích đã được dịch S_t = ⟨S^t_i, T^t_j⟩, trong đó i và j là độ dài của lịch sử nguồn và đích. Do đó, chúng ta có thể sử dụng R(i + 1) để biểu thị hành động đọc thêm một token nguồn và sử dụng W(w, j + 1) để biểu thị việc viết một token w. Việc cập nhật trạng thái S_t theo hành động A_t có thể được biểu thị như:

S_{t+1} = {
    ⟨S^t_i ∪ {w}, T^t_j⟩ nếu A_t = R(i + 1)
    ⟨S^t_i, T^t_j ∪ {w}⟩ nếu A_t = W(w, j + 1)
}

trong đó w đại diện cho bất kỳ từ nguồn hoặc đích nào.

Việc đánh giá các hệ thống SimulMT không chỉ xem xét chất lượng dịch mà còn tính đến độ trễ, đo lường sự chậm trễ giữa quỹ đạo đích và nguồn. Các chỉ số được sử dụng để đo độ trễ bao gồm Độ trễ trung bình (AL) (Ma et al., 2020), Tỷ lệ trung bình (AP) (Cho và Esipova, 2016) hoặc Độ trễ trung bình thích ứng độ dài (LAAL) (Papi et al., 2022b). Trong bài báo này, chúng tôi sử dụng LAAL (Xem Phụ lục C.1 để biết định nghĩa) vì nó có độ hiệu chỉnh tốt hơn về sự khác biệt độ dài giữa giả thuyết và tham chiếu.

Mô hình Ngôn ngữ Lớn (LLM) tận dụng giải mã tự hồi quy để thực hiện mô hình hóa ngôn ngữ không giám sát trên các tập dữ liệu văn bản rộng lớn, trang bị cho chúng khả năng hiểu và tạo sinh ngôn ngữ. Hầu hết các LLM ngày nay đều sử dụng kiến trúc Transformer chỉ có bộ giải mã (Vaswani et al., 2017) bao gồm các lớp self-attention và khối feed-forward. Ngoài đào tạo không giám sát, các LLM gần đây trải qua điều chỉnh tế có giám sát (SFT) và học tăng cường từ phản hồi con người (RLHF) để điều chỉnh hành vi của chúng với sở thích con người (Ouyang et al., 2022). Điều này cho phép các mô hình này thực hiện nhiều tác vụ NLP khác nhau thông qua các tương tác đối thoại. Cụ thể hơn, người dùng xây dựng các prompt bao gồm hướng dẫn và ngữ cảnh và nhắc nhở mô hình tạo ra phản hồi chứa kết quả mong muốn. Trong bài báo của chúng tôi, chúng tôi chủ yếu sử dụng tìm kiếm chùm thay vì lấy mẫu top-p để có được các bản dịch ổn định hơn. Do đó, chúng tôi coi việc gọi LLM là một hàm tạo sinh g_θ với chuỗi prompt X và kích thước chùm B làm đầu vào và các chuỗi phản hồi Y (cho tất cả ứng cử viên chùm) cũng như xác suất của chúng Pr là giá trị trả về: Y, Pr = g_θ(X, B).

3 Điều chỉnh LLM cho SimulMT

3.1 Thiết kế Prompt cho các Trạng thái Tăng dần

Mặc dù có những khác biệt đáng kể trong quá trình giải mã giữa các mô hình SimulMT và các mô hình MT ngoại tuyến, phương pháp cơ bản để hướng dẫn LLM trong dịch thuật vẫn nhất quán. Phương pháp này tiếp tục dựa vào việc xây dựng các prompt bao gồm hướng dẫn + ngữ cảnh làm đầu vào, nhắc nhở LLM thực hiện hoàn thiện văn bản. Để trình bày chi tiết hơn, trong dịch ngoại tuyến, chúng ta thường xây dựng prompt như sau: "[INST] Dịch câu sau từ tiếng Anh sang tiếng Đức: S [/INST]", trong đó S là câu nguồn. LLM sau đó cung cấp bản dịch trong nội dung được hoàn thiện sau "[/INST]". Bản dịch hoàn thiện có thể được biểu thị là T.

Trong SimulMT, chúng tôi giữ nguyên hướng dẫn và coi văn bản nguồn là một chuỗi có độ dài thay đổi phụ thuộc vào thời gian S^t_i cho biết tại thời điểm t, đã đọc được i token nguồn. Ngoài ra, chúng tôi coi nội dung dịch tích lũy là một chuỗi có độ dài thay đổi khác T^t_j, cho biết đã viết j token đích tại thời điểm t. Tại thời điểm này, đầu vào của mô hình cũng phụ thuộc vào thời gian, và chúng tôi định nghĩa X^t là đầu vào cho mô hình tại thời điểm t. X^t có thể được lấy thông qua hàm tạo prompt X^t = create_prompt(S^t_i, T^t_j), đặt S^t_i và T^t_j trong cùng một chuỗi bắt đầu với hướng dẫn: "[INST] Dịch câu sau từ tiếng Anh sang tiếng Đức: S^t_i [/INST] T^t_j". Bằng cách sử dụng phương pháp này, chúng ta có thể quản lý hiệu quả nội dung nguồn và đích đang diễn ra một cách riêng biệt và cấu trúc chúng thành các prompt tiêu chuẩn (dòng 6 trong Thuật toán 1).

3.2 Chính sách Đọc-n & Giải mã tăng dần

Với mục tiêu khám phá việc áp dụng thực tế LLM trong các tác vụ SimulMT theo cách đơn giản và hiệu quả, thiết kế chính sách của chúng tôi tuân theo hai nguyên tắc chính. Thứ nhất, chúng tôi nhằm mục đích để chính sách chủ yếu dựa vào khả năng tạo sinh văn bản vốn có của LLM, tránh việc giới thiệu các tham số bổ sung cho việc học chính sách. Thứ hai, nhận thức rằng việc gọi LLM thường tốn chi phí tính toán đáng kể và có thể dẫn đến độ trễ xử lý bổ sung, chúng tôi tìm cách cung cấp cho người dùng khả năng kiểm soát thuận tiện tần suất gọi LLM.

Dựa trên những nguyên tắc này, chúng tôi giới thiệu chính sách Đọc-n & giải mã tăng dần. Để xác định thời điểm thực hiện hành động ĐỌC, chúng tôi sử dụng một phương pháp đơn giản: sau mỗi hành động VIẾT, một số cố định n token được đọc (dòng 2 trong Thuật toán 1). Phương pháp này cung cấp một cách thuận tiện để kiểm soát tần suất gọi LLM, vì quá trình ra quyết định không yêu cầu sự tham gia của LLM. Ngoài ra, phương pháp này phù hợp với chế độ hoạt động của nhiều hệ thống ASR streaming như U2++ (Wu et al., 2021), đọc các đoạn speech theo khoảng thời gian cố định và dự đoán nhiều token transcript để đưa vào hệ thống SimulMT để dịch.

Đối với quyết định hành động VIẾT, chúng tôi trực tiếp sử dụng phương pháp giải mã tăng dần được đề xuất trong (Liu et al., 2020; Nguyen et al., 2021a). Điều này bao gồm việc gọi LLM dựa trên trạng thái tăng dần hiện tại để thực hiện giải mã tìm kiếm chùm hoàn chỉnh (dòng 8 trong Thuật toán 1). Sau đó, chúng tôi sử dụng thuật toán tiền tố chung dài nhất (LCP) để xác định một tiền tố (dòng 11 trong Thuật toán 1) với thỏa thuận cục bộ (LA) ở mức độ từ (§3.3). Nếu tìm thấy tiền tố như vậy, chính sách kích hoạt hành động VIẾT; ngược lại, nó tiếp tục đọc n token liên tiếp (dòng 17 trong Thuật toán 1).

3.3 Giảm độ trễ với RALCP

Mặc dù thuật toán giải mã tăng dần đã trang bị cho LLM khả năng thực hiện SimulMT, nhưng có một thách thức khi xử lý các ứng cử viên tìm kiếm chùm thể hiện sự đa dạng đáng kể (Xem Hình 2 để biết ví dụ). Trong những trường hợp như vậy, thuật toán LCP ban đầu có thể gặp khó khăn trong việc kịp thời cung cấp tiền tố dài nhất phù hợp để viết ra. Do đó, việc gọi LLM liên quan đến trạng thái tăng dần hiện tại bị lãng phí, dẫn đến sự gia tăng đáng kể về độ trễ. Để giải quyết vấn đề này, chúng tôi tối ưu hóa thuật toán LCP và giới thiệu thuật toán Tiền tố chung dài nhất thỏa thuận thư giãn (RALCP).

RALCP sử dụng cơ chế bỏ phiếu để thư giãn các ràng buộc trong việc xác định tiền tố chung. Ví dụ, nếu 80% ứng cử viên có thể đề xuất cùng một token, thì token đó được chấp nhận là một phần của tiền tố. Chúng tôi ký hiệu γ là ngưỡng thỏa thuận, được coi là ngưỡng chấp nhận token xuất hiện thường xuyên nhất tại vị trí nhất định. Cụ thể, trong LCP thông thường, tiền tố với thỏa thuận cục bộ được xác định bằng cách khớp token tại cùng vị trí i cho tất cả chuỗi ứng cử viên, nếu chúng giữ cùng một token, token sẽ được thu thập vào tiền tố. Trong RALCP, chúng tôi thư giãn tiêu chí lựa chọn token bằng cách sử dụng cơ chế bỏ phiếu, tức là nếu token tại i có số phiếu chuẩn hóa (tần suất) lớn hơn γ, nó sẽ được chấp nhận trong tiền tố. Trong các thí nghiệm của chúng tôi, chúng tôi khám phá γ từ 0.1 đến 1.0 và thấy rằng 0.6 là giá trị cân bằng thực nghiệm đối với hiệu suất và độ trễ (Xem C.4 để biết chi tiết).

3.4 SFT và Đào tạo Tiền tố

Do thực tế rằng 89.7% corpus tiền huấn luyện của Llama2 bao gồm tiếng Anh, chúng tôi đã quan sát thấy một hạn chế đáng kể trong khả năng dịch đa ngôn ngữ của nó trong các thí nghiệm (§4.2). Trong cài đặt một lần, nó vẫn thể hiện khoảng cách hiệu suất đáng kể so với các baseline khác. Để giải quyết bất lợi vốn có này do độ phủ thấp của các ngôn ngữ không phải tiếng Anh trong dữ liệu tiền huấn luyện, chúng tôi tiếp tục khám phá việc sử dụng điều chỉnh tế có giám sát (SFT) để khám phá mức độ cải thiện có thể đạt được.

Tuy nhiên, do chi phí tính toán cao liên quan đến việc điều chỉnh tế trên tập dữ liệu lớn với tất cả tham số, điều này là không khả thi và không phù hợp với các nguyên tắc nêu trên trong §3.2. Chúng tôi đặt ra các hạn chế cho phương pháp SFT để kiểm soát chi phí. Cụ thể, chúng tôi đã sử dụng LoRA (Hu et al., 2022) cho việc điều chỉnh tế hiệu quả, và đóng băng các tham số LLM ban đầu. Hơn nữa, chúng tôi tiến hành đào tạo chỉ một epoch trên tập điều chỉnh tế trong thí nghiệm chính.

Chúng tôi đã khám phá tổng cộng hai chiến lược SFT: (i) SFT Ngoại tuyến thuần túy, trong đó chúng tôi sử dụng các cặp nguồn-đích câu đầy đủ để xây dựng prompt và phản hồi cho việc đào tạo, và (ii) ngoại tuyến + Tiền tố, trong đó chúng tôi trộn các cặp nguồn-đích câu đầy đủ với một số lượng nhỏ các cặp tiền tố-tiền tố (sẽ được giới thiệu ngay) và tiến hành điều chỉnh tế trên tập dữ liệu kết hợp này.

SFT Ngoại tuyến thuần túy Chúng tôi trộn tất cả dữ liệu đào tạo của bộ dữ liệu MUST-C cho mỗi cặp ngôn ngữ được chọn thành một tập dữ liệu kết hợp. Đối với mỗi mẫu, để đạt được khả năng tổng quát tốt hơn, trước tiên chúng tôi lấy mẫu một template từ danh sách 10 template được định nghĩa trước để xây dựng đầu vào prompt như trong phần §3.1. Các template được định nghĩa trước được thể hiện trong Phụ lục B. Trong quá trình điều chỉnh tế, chúng tôi chỉ tính toán loss trên phản hồi đích để tránh quên thảm khốc như được đề xuất trong (Touvron et al., 2023b).

SFT Ngoại tuyến + Tiền tố Được truyền cảm hứng từ phương pháp điều chỉnh mô hình trên dữ liệu tiền tố-tiền tố được mô tả trong (Niehues et al., 2018; Liu et al., 2020), nhằm giải quyết vấn đề "tưởng tượng" (bản dịch thường được mô hình tưởng tượng thành một câu đầy đủ), chúng tôi tạo ra tập dữ liệu tiền tố-tiền tố của mình. Tuy nhiên, thay vì tạo ra một tập dữ liệu tiền tố nhân tạo có kích thước 1:1 với việc cắt ngắn dựa trên tỷ lệ, chúng tôi chọn sử dụng ChatGPT để tạo ra một tập dữ liệu nhỏ hơn nhiều để thuận tiện. Cụ thể, chúng tôi lấy mẫu ngẫu nhiên 1000 câu nguồn từ tập đào tạo của mỗi cặp ngôn ngữ và cắt ngắn chúng thành 20% đến 80% độ dài đầy đủ một cách đồng đều, dẫn đến 9000 tiền tố nguồn. Sau đó, chúng tôi sử dụng ChatGPT (gpt-3.5-turbo) để dịch các tiền tố nguồn này thành tiền tố đích. Chúng tôi kiểm tra chất lượng của các tiền tố được tạo ra bằng phân tích định lượng để đảm bảo chất lượng hợp lý. Chi tiết thêm được cung cấp trong Phụ lục A. Những cặp tiền tố này được trộn lẫn với tập dữ liệu câu đầy đủ được sử dụng trong chiến lược SFT ngoại tuyến thuần túy cho SFT theo cách tương tự.

4 Thí nghiệm

4.1 Thiết lập Thí nghiệm

Dữ liệu và Đánh giá Chúng tôi đã chọn chín cặp ngôn ngữ từ bộ dữ liệu MUST-C (Gangi et al., 2019), được sử dụng phổ biến trong việc đánh giá hiệu suất của các hệ thống dịch giọng nói và văn bản. Chín cặp ngôn ngữ này đều có tiếng Anh làm ngôn ngữ nguồn và bao gồm các phát ngôn từ các bài nói chuyện TED. Thống kê chi tiết của mỗi cặp ngôn ngữ có thể được tìm thấy trong Bảng 1. Trong quá trình đào tạo, tập đào tạo kết hợp có tổng số 2 triệu mẫu với 9000 mẫu tiền tố-tiền tố bổ sung (§3.4) cho việc đào tạo SFT+tiền tố. Chúng tôi sử dụng tập kiểm tra tst-COMMON để đánh giá. Đối với các chỉ số đánh giá, BLEU (Papineni et al., 2002) được sử dụng để đánh giá chất lượng, và LAAL (Papi et al., 2022b) được sử dụng để đánh giá độ trễ. Tất cả các đánh giá được thực hiện với bộ công cụ SimulEval (Ma et al., 2020), tuân theo hạn chế của đánh giá IWSLT (Agrawal et al., 2023) rằng các phân đoạn dịch được cam kết không được phép cập nhật.

LLM Chúng tôi sử dụng Llama2-7B-chat làm LLM (Touvron et al., 2023b) trong các thí nghiệm. Nó đã được tiền huấn luyện trên 2 tỷ token, và có độ dài ngữ cảnh 4K. Lý do chọn phiên bản 7B trong thí nghiệm là mô hình có kích thước tham số này có thể thực hiện suy luận trên một GPU đơn, làm cho nó phù hợp hơn cho các trường hợp sử dụng thực tế.

Trong quá trình SFT, chúng tôi sử dụng LoRA (Hu et al., 2022) để giảm chi phí tính toán, các adapter LoRA được cấu hình với r = 64 và α = 16, do đó có tổng tham số có thể huấn luyện là 33M. Chúng tôi đặt tốc độ học là 2e-4, kích thước batch là 48, và sử dụng lượng tử hóa 4-bit. Đối với tất cả các thí nghiệm liên quan đến LLM, một GPU A100 đơn được sử dụng. SFT chỉ được thực hiện trong một epoch, trừ khi được nêu khác.

Baseline Chúng tôi thiết lập một mô hình baseline tức là một NMT ngoại tuyến là một Transformer encoder-decoder 6 lớp (Vaswani et al., 2017) được đào tạo trên dữ liệu song song câu đầy đủ (nhưng với các câu nguồn được thêm tiền tố thẻ ngôn ngữ cho đào tạo đa ngôn ngữ) từ đầu trong 300K bước với 16k token mỗi batch trên 4 GPU A40, kích thước tham số của nó là 48M. Nó sử dụng cùng chính sách giải mã như LLM, nhưng xử lý văn bản nguồn và đích tăng dần với encoder và decoder riêng biệt, tương tự như việc thực hiện của (Polák et al., 2022; Guo et al., 2023).

4.2 Kết quả Thí nghiệm

Bảng 2 trình bày kết quả thí nghiệm chính của chúng tôi. Các thí nghiệm được chia thành hai kịch bản và 5 nhóm, tức là ngoại tuyến (nhóm I và II) và đồng thời (nhóm III-V). Đối với mỗi kịch bản, chúng tôi đánh giá hiệu suất của các mô hình baseline, và LLM trong cài đặt một lần và SFT (chúng tôi thấy rằng LLM trong cài đặt không lần nào thường tạo ra định dạng không mong muốn trong phản hồi, chi tiết của cài đặt một lần có thể được tìm thấy trong Phụ lục C.2). Đối với mỗi mô hình trong kịch bản đồng thời, chúng tôi đánh giá chúng với cả LCP (γ = 1.0) và RALCP (γ = 0.6, được chú thích với ⋆), lý do chọn γ = 0.6 được thảo luận trong Phụ lục C.4. Chúng tôi đặt n = 6 cho tất cả các mô hình đồng thời vì độ trễ vừa phải mà nó dẫn đến. Đối với tất cả các mô hình trong cả hai kịch bản được báo cáo trong Bảng 2, chúng tôi đặt kích thước chùm là 10. Thêm kết quả sử dụng cấu hình tham số siêu khác nhau và các chỉ số đánh giá như COMET (Rei et al., 2020) được báo cáo trong Phụ lục C.5. Những phát hiện sau đây có thể được tóm tắt trong Bảng 2.

Kịch bản ngoại tuyến Chúng tôi quan sát thấy khoảng cách hiệu suất đáng kể giữa cài đặt một lần của LLM và mô hình baseline (chênh lệch trung bình 10 điểm). Tuy nhiên, đối với LLM-PFX-SFT, khoảng cách này được thu hẹp, chứng tỏ tiềm năng đáng kể của Llama2 trong các tác vụ dịch.

Kịch bản đồng thời Chúng tôi thấy rằng cả LLM-One-Shot và LLM-PFX-SFT đều duy trì ngang bằng với kết quả kịch bản ngoại tuyến của chúng, cho thấy tính bền vững của phương pháp đọc-n & giải mã tăng dần trên LLM.

Lợi ích của RALCP Tất cả kết quả đồng thời đều chứng minh rằng RALCP giảm hiệu quả độ trễ (khoảng 45%). Trong trường hợp các mô hình baseline, RALCP có tác động tiêu cực đáng chú ý đến BLEU. Tuy nhiên, đối với LLM, nó có thể giữ BLEU không đổi. Chúng tôi suy đoán điều này là do cấu trúc chỉ có decoder của LLM đảm bảo phụ thuộc đơn điệu vào ngữ cảnh nguồn, đảm bảo tính nhất quán cao hơn trong các ứng cử viên chùm. Do đó, RALCP giảm hiệu quả độ trễ trong khi duy trì chất lượng tiền tố. Đối với các mô hình baseline, việc sử dụng RALCP dẫn đến lỗi do tính chất không đơn điệu vốn có của các encoder hai chiều, dẫn đến độ không chắc chắn cao hơn và sự đa dạng trong các ứng cử viên chùm. Vấn đề này cũng được thảo luận trong (Liu et al., 2020). Kết luận, kết quả của chúng tôi cho thấy rằng RALCP phù hợp hơn cho các mô hình có phụ thuộc đơn điệu vào ngữ cảnh nguồn.

5 Phân tích

5.1 Hiệu quả Sử dụng Dữ liệu

Hình 3 trình bày tỷ lệ phần trăm hiệu suất được giữ lại sau SFT sử dụng các kích thước dữ liệu khác nhau từ 1k đến 100k, so với hiệu suất đạt được với dữ liệu đầy đủ (ký hiệu là all) trên ba cặp ngôn ngữ đại diện (en-de, en-ro, en-ru). Chúng tôi cũng cung cấp hiệu suất một lần làm baseline và hiệu suất tốt nhất thu được bằng SFT đa ngôn ngữ (được mô tả trong §3.4) ký hiệu là multi-L.

Chúng ta có thể quan sát mối tương quan cao giữa độ phủ ngôn ngữ (xem Bảng 1, cột "Pretraining Coverage") trong corpus tiền huấn luyện của Llama2 và hiệu suất dịch được giữ lại trong cài đặt một lần. Có 2 quan sát thú vị mà chúng tôi có thể đề cập ở đây để nhấn mạnh lợi ích của LLM: (i) 1k mẫu có thể cung cấp cải thiện đáng kể so với giải mã một lần, nhưng vẫn chưa đủ cho ngôn ngữ tài nguyên thấp. (ii) Chỉ với 10k mẫu, nó giữ lại 90% hiệu suất và thu hẹp khoảng cách giữa ngôn ngữ tài nguyên thấp và cao. Thiết lập thí nghiệm chi tiết và kết quả được thể hiện trong Phụ lục C.3.

5.2 Tính Bền vững của Đầu vào Nhiễu

Để nghiên cứu sâu hơn về các lợi thế tiềm năng của LLM trong tác vụ SimulMT, chúng tôi đánh giá hiệu suất của LLM khi sử dụng transcript ASR làm đầu vào. Để đảm bảo tính nhất quán trong đầu vào cho các phương pháp khác nhau, chúng tôi không trực tiếp sử dụng hệ thống ASR streaming trong quá trình suy luận. Thay vào đó, chúng tôi đầu tiên sử dụng Whisper-base (Radford et al., 2023) để tạo transcript (với WER trung bình 17.31) cho các tập kiểm tra của tất cả 9 cặp ngôn ngữ, sau đó được sử dụng làm đầu vào cho SimulMT, thay thế các đầu vào ground-truth trước đó.

Đối với thí nghiệm này, chúng tôi sử dụng cả BLEU và COMET (Rei et al., 2020) làm chỉ số đánh giá. Chúng tôi bao gồm COMET vì việc đánh giá tính bền vững của mô hình trong các tình huống đầu vào nhiễu đòi hỏi nhiều hơn là chỉ khớp n-gram trong BLEU. Hình 4 hiển thị điểm BLEU và COMET trung bình cho tất cả 9 cặp ngôn ngữ sử dụng ba mô hình với ground truth và ASR làm đầu vào. Đối với cả điểm BLEU và COMET, LLM vượt trội hơn các mô hình NMT chuyên dụng một cách đáng kể, cho thấy rằng LLM có tính bền vững tốt hơn trên đầu vào nhiễu.

5.3 Hiệu quả Suy luận

So với baseline Transformer, LLM có số lượng tham số lớn hơn, thường phải chịu chi phí suy luận cao hơn. Hình 5 minh họa thời gian trung bình cần thiết để dự đoán một token đơn trong cả kịch bản ngoại tuyến và đồng thời. Thời gian này được tính bằng cách lấy trung bình thời gian thực tế trên tất cả độ dài giả thuyết cho ba tập kiểm tra (en-de, en-ro, en-ru), cũng tính đến thời gian dành cho các lần gọi mô hình bị lãng phí do RALCP không thành công trong việc lựa chọn tiền tố trong quá trình giải mã tăng dần. Như thể hiện trong hình, LLM tiêu tốn nhiều thời gian hơn trong cả hai kịch bản so với các phương pháp baseline khác. Điều này cho thấy rằng trong việc sử dụng thực tế, LLM phải xem xét độ trễ bổ sung do chi phí tính toán mang lại.

6 Các Công trình Liên quan

Dịch máy đồng thời (SimulMT) là tác vụ cung cấp bản dịch thời gian thực của luồng câu nguồn với mục tiêu là giảm thiểu độ trễ trong khi tối đa hóa chất lượng dịch. Một phương pháp phổ biến là đào tạo mô hình MT trên tập dữ liệu tiền tố-tiền tố để trực tiếp dự đoán token đích dựa trên token nguồn một phần (Ma et al., 2019b). Thay vào đó, Liu et al. (2020) đề xuất khung giải mã tăng dần để tận dụng NMT ngoại tuyến được tiền huấn luyện và biến nó thành mô hình SimulMT mà không cần đào tạo thêm. Một thành phần cốt lõi của SimulMT là chính sách đọc-viết để quyết định tại mỗi bước có nên đợi thêm một token nguồn (ĐỌC) hay tạo ra một token đích (VIẾT). Các phương pháp trước đây đã khám phá chính sách cố định, luôn đợi k token trước khi tạo ra (Ma et al., 2019b; Zhang et al., 2022) và chính sách thích ứng, đào tạo một agent thông qua học tăng cường (Gu et al., 2017b; Arthur et al., 2021b). Dịch lại (Arivazhagan et al., 2019) từ đầu câu nguồn tại bước VIẾT sẽ phát sinh độ trễ dịch cao. Các phương pháp phát hiện giả thuyết ổn định như Thỏa thuận Cục bộ (Liu et al., 2020), hold-n (Liu et al., 2020) và Share prefix SP-n (Nguyen et al., 2021b) được sử dụng để cam kết giả thuyết ổn định và chỉ tái tạo một chuỗi con của câu nguồn. Mục tiêu là giảm độ trễ và giảm thiểu tiềm năng lỗi từ câu nguồn không hoàn chỉnh (Polák et al., 2022).

LLM cho NMT Nghiên cứu gần đây đã đi sâu vào việc sử dụng tiềm năng của LLM trong MT (Hendy et al., 2023; Zhu et al., 2023; Robinson et al., 2023). Mặc dù LLM có thể hiển thị một mức độ khả năng dịch nhất định, nghiên cứu trước đây đã xác định rằng chúng vẫn tụt hậu so với các mô hình NMT thông thường, đặc biệt đối với các ngôn ngữ tài nguyên thấp (Robinson et al., 2023). Ngoài ra, hiệu suất dịch khác nhau tùy thuộc vào chiến lược nhắc nhở (Zhang et al., 2023). Các nỗ lực đã được thực hiện để nâng cao hiệu suất dịch của LLM bằng cách tích hợp hướng dẫn từ từ điển (Lu et al., 2023), điều chỉnh tế thêm (Zeng et al., 2023; Xu et al., 2023) và tăng cường với bộ nhớ dịch (Mu et al., 2023). Tuy nhiên, theo hiểu biết của chúng tôi, thiếu nghiên cứu khám phá khả năng dịch đồng thời của LLM.

7 Kết luận

Trong bài báo này, chúng tôi tập trung vào việc khám phá khả năng áp dụng LLM cho SimulMT. Chúng tôi ban đầu đã biến đổi Llama2-7B-chat thành mô hình hỗ trợ dịch đồng thời bằng cách sử dụng phương pháp giải mã tăng dần hiện có. Sau đó, chúng tôi giới thiệu thuật toán RALCP để giảm độ trễ suy luận. Trong các thí nghiệm, chúng tôi thấy rằng LLM sau SFT có thể vượt trội hơn mô hình NMT chuyên dụng sử dụng cùng chính sách giải mã, chứng tỏ tiềm năng của LLM trong tác vụ này. Ngoài ra, chúng tôi quan sát thấy LLM thể hiện một mức độ tính bền vững chống lại đầu vào nhiễu và có thể mang lại cải thiện hiệu quả thông qua điều chỉnh tế có giám sát với dữ liệu hạn chế. Tuy nhiên, chúng tôi cũng xác định rằng chi phí tính toán của LLM là một thách thức đáng kể. Trong công việc tương lai, chúng tôi dự định đề xuất các chính sách phù hợp hơn cho LLM và khám phá sâu hơn các ứng dụng có thể của các khả năng LLM khác nhau trong các tác vụ SimulMT.

Hạn chế

Chúng tôi tóm tắt các hạn chế của nghiên cứu này trong ba khía cạnh:

Chính sách Trong bài báo này, chúng tôi chỉ khám phá một chính sách tương đối đơn giản, tức là "đọc-n & giải mã tăng dần". Đặc biệt, quá trình ra quyết định cho hành động ĐỌC gần như ngây thơ. Chúng tôi nhận thức rằng việc gọi LLM thường xuyên để tạo ra hoàn toàn do tính không hiệu quả của chính sách là một yếu tố chính cho chi phí tính toán cao. Trong công việc tương lai, chúng tôi nhằm khám phá các chính sách thích ứng và hiệu quả hơn.

Dữ liệu Đánh giá của chúng tôi chỉ được thực hiện trên bộ dữ liệu MUST-C, điều này đã hạn chế sự đa dạng về lĩnh vực và phong cách. Chúng tôi tin rằng các bộ dữ liệu phong phú hơn nên được xem xét để cho phép đánh giá toàn diện hơn về phương pháp.

Sử dụng LLM Hiện tại, chúng tôi chỉ nghiên cứu khả năng sử dụng LLM như một mô hình dịch trong toàn bộ pipeline SimulMT. Tuy nhiên, LLM có khả năng vượt ra ngoài dịch. Trong công việc tương lai, chúng tôi kế hoạch tận dụng đầy đủ khả năng đa tác vụ của LLM và khám phá các mô hình sử dụng đa dạng hơn trong pipeline.

Những hạn chế này cung cấp hướng cho nghiên cứu tương lai để nâng cao hơn nữa khả năng áp dụng và hiệu suất của LLM trong tác vụ SimulMT.
