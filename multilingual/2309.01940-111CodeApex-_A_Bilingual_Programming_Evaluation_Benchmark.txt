# 2309.01940.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2309.01940.pdf
# File size: 1703988 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
111CodeApex: A Bilingual Programming Evaluation Benchmark
for Large Language Models
LINGYUE FU, HUACAN CHAI, KOUNIANHUA DU, WEIMING ZHANG, SHUANG LUO,
JIANGHAO LIN, YUCHEN FANG, RENTING RUI, HAO GUAN, JIANXING LIU, SIYUAN
QI, LONGTENG FAN, JIAYI LEI, YIFAN LIU, JINGKUAN WANG, KANGNING ZHANG,
WEINAN ZHANG, and YONG YUâˆ—,Apex Data & Knowledge Management Lab, Shanghai Jiao Tong
University, China
https://apex.sjtu.edu.cn/codeapex/
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the pro-
gramming capabilities of models, attracting growing attention from researchers. Evaluating the programming
capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream
applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming
comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task
tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning,
and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based
on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code
segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose
and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of
69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant
room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating
the coding capabilities of LLMs, further promoting their development and growth.
CCS Concepts: â€¢Computing methodologies â†’Natural language processing .
Additional Key Words and Phrases: Large Language Model, Benchmark
ACM Reference Format:
Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting
Rui, Hao Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang,
Weinan Zhang, and Yong Yu. 2018. CodeApex: A Bilingual Programming Evaluation Benchmark for Large
Language Models. J. ACM 37, 4, Article 111 (August 2018), 32 pages. https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Due to the widespread adaption of the Transformer [ 51] architecture and advancements in com-
putational power, Large Language Models (LLMs) have been widely employed in various tasks,
âˆ—The leading author is Lingyue Fu <fulingyue@sjtu.edu.cn>. Shuang Luo has graduated from SJTU and is now at KTH.
Correspondence to Weinan Zhang <wnzhang@sjtu.edu.cn> and Yong Yu <yyu@sjtu.edu.cn>.
Authorsâ€™ address: Lingyue Fu; Huacan Chai; Kounianhua Du; Weiming Zhang; Shuang Luo; Jianghao Lin; Yuchen Fang;
Renting Rui; Hao Guan; Jianxing Liu; Siyuan Qi; Longteng Fan; Jiayi Lei; Yifan Liu; Jingkuan Wang; Kangning Zhang;
Weinan Zhang; Yong Yu, Apex Data & Knowledge Management Lab, Shanghai Jiao Tong University, Shanghai, China
https://apex.sjtu.edu.cn/codeapex/ .
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
0004-5411/2018/8-ART111 $15.00
https://doi.org/XXXXXXX.XXXXXXX
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.arXiv:2309.01940v4  [cs.CL]  11 Mar 2024

--- PAGE 2 ---
111:2Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 1. Comparison between commonly used programming benchmarks in the era of LLMs and CodeApex.
P.C. stands for programming comprehension. C.G. stands for code generation. C.C. stands for code correction.
The number of plus symbols (i.e., +) in the Data Scale column represents the order of magnitude of the test
samples.
BenchmarkProgramming Task Fine-grained
CategorizationData Scale Bilingual Human ExpertP.C. C.G. C.C.
MBPP [4] % ! % % +++ % %
CoderEval [59] % ! % % ++ % %
HumanEval [9] % ! % % + % %
HumanEval-X [65] % ! % % ++ % %
DebugBench [50] % % ! ! +++ % !
CodeApex (Ours) ! ! ! ! +++ ! !
including recommender systems [ 36,37], dialogue systems [ 1,63], summarization [ 39,62], sen-
timent analysis [ 6,21], etc. This trend highlights the need for robust evaluation frameworks to
measure the effectiveness of LLMs in these diverse applications such as education [ 10,17,23], social
science [ 18,20] and medical [ 22,31]. Establishing these benchmarks can provide quantitatively
thorough assessments for the capabilities of LLMs from various perspectives, thereby guiding
model optimization, validating model performance, and analyzing model differences. Among these
diverse benchmarks, we argue that the code and programming capability is one of the most crucial
aspects for the evaluation of LLMs for the following two reasons.
Firstly , the performance of LLMs on code and programming tasks largely reflects their inherent
capabilities of logical reasoning, instruction following, and structural data understanding. Code
and programming serve as the bridge between humans and computers, and translate high-level
human need into executable steps, logical consistency, featuring standard syntax, and functional
modularity [ 57]. Various works [ 41,43,57] have identified the importance of code data for model
tuning to help unlock the reasoning ability of LLMs and steer LLMs to produce structured and
precise intermediate steps. As a result, various benchmarks and leaderboards [ 27] that focus on
general reasoning ability evaluation of LLMs would include code-related tasks in addition to
traditional tasks like arithmetic [ 3,14] and symbolic reasoning [ 55], which draws a thorough
assessment over the fundamental capabilities of large language models.
Secondly , apart from the potential relationship with basic reasoning abilities of LLMs, code-related
tasks also play crucial roles in a wide range of downstream applications, such as education [ 16]
and software engineering [ 7]. A variety of code foundation models [ 7,35,53,65] have been
proposed for different programming tasks. Copilot [ 7] achieves code auto-completion and auto-
correction during the programming process, therefore greatly promoting the coding experience of
human programmers. CodeGeeX [ 65], as an interactive programming assistant, can solve various
programming problems, including code interpretation, code translation, code correction, document
generation, etc. Google DeepMind team proposes AlphaCode [ 35], which aims to solve programming
problems that require deep reasoning abilities to find novel solutions. CodeT5 [ 53] employs a unified
framework to seamlessly support both code understanding and generation tasks, with the allowance
of multi-task learning. Such a emergence of code foundation models with different programming
goals (e.g., code completion, code debugging) draws demand on the corresponding evaluation to
ensure their performance in real-world production scenarios.
These factors discussed above yield an urgent need for a comprehensive benchmark to thoroughly
evaluate the code and programming capabilities of large language models. In Table 1, we illustrate the
important properties of the commonly used programming evaluation benchmarks [ 4,9,50,59,65],
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 3 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:3
and discuss their potential limitations as follows: (1) Existing benchmarks only focus on a single
programming task for LLMs within uni-lingual test cases (i.e., English), which makes the evaluation
incomprehensive. (2) Existing benchmarks, excluding DebugBench [ 50], generally lack fine-grained
categorization over the test data and human-expert evaluation, which are crucial to derive deeper
insights and analysis for different aspects of programming, as well as the thorough comparison
between LLMs and human-level abilities.
To this end, we propose the CodeApex benchmark, which gives a comprehensive code and
programming evaluation of large language models. As shown in Table 1, CodeApex is a pioneering
bilingual (English and Chinese) programming benchmark over three different code-related tasks
(i.e., programming comprehension, code generation, and code correction) with fine-grained cate-
gorization, large test data scale, as well as human-expert evaluation. We have comprehensively
evaluated 12 different LLMs using CodeApex, including both API-based and open-source models.
Our work analyzes the overall code capabilities of LLMs by comparing their performance across
different tasks. Fine-grained categorization experiments provide an analysis of LLMs across different
strategies and data types. Experimental results demonstrate the varying performance of different
models in code-related tasks, with GPT models showcasing exceptional competitiveness and dis-
tinct advantages. Additionally, the experiment compares the performance of LLMs in bilingual and
different prompt strategy scenarios. We also organize human testing on code comprehension and
code generation tasks, comparing performance between humans and LLMs. Overall, within the
leaderboard of the CodeApex, there remains significant room for improvement in LLM accuracy,
indicating an untapped potential for LLMs in code-related tasks.
The rest of the paper is organized as follows. In Section 2, we review previous work on evaluating
the code-related capabilities of LLMs. We present the evaluation protocol for three programming
tasks in Section 3. We present and discuss the evaluation results across three tasks and multiple
categorizations in Section 4. Finally, we conclude this paper and discuss future work in Section 5.
2 RELATED WORK
2.1 Code Foundation Models
The implementation of programming comprehension heavily relies on the alignment of code space
and natural language space through encoding. Graph2Code [ 58] employs graph neural networks
to convert source code into a graph structure, capturing structural information within the code
and thereby enhancing the accuracy of programming comprehension. Code2Vec [ 2] represents the
Abstract Syntax Tree (AST) as token sequences along paths, enabling code transformation into
fixed-length vector representations and facilitating the learning of code semantics. The introduction
of the Transformer architecture [ 52] has provided novel approaches for code comprehension tasks.
One prominent model in this regard is CodeT5 [ 53], an extension of the Text-to-Text Transfer
Transformer (T5) [ 45] specifically designed for natural language processing tasks on source code.
CodeBERT [ 25] maps both natural language and code into a shared vector space and leverages
attention mechanisms to capture semantic relationships between them. Furthermore, there are
models [ 42,56] that focus on specific programming tasks such as code comment generation and
API documentation generation within the source code context.
The task of code generation has garnered significant attention after the emergence of LLMs.
These language models are pre-trained on massive text datasets, enabling them to learn rich
language representations. General-purpose LLMs, such as GPT and Llama [ 46], have a certain
ability to generate code. Some LLMs are specifically designed training schemes for the programming
tasks, aimed at improving their coding performance. One common approach to code generation is
fine-tuning existing large-scale language models. In this method, a pre-trained language model
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 4 ---
111:4Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
is used as the initial model and further trained on specific code generation datasets. Codex [ 9],
PaLMCoder[ 11], CodeGeeX2[ 65] are further trained on GPT-3, PaLM[ 11], ChatGLM2[ 19,61] with
extensive public code datasets, demonstrating better comprehension and generation abilities on
programming languages. With the development of instruction tuning techniques, a series of models,
like WizardCoder[ 40], Code Llama-Instruct[ 47], PanguCoder[ 12] demonstrate powerful capabilities
in multilingual code generation and debugging tasks. Another prevalent approach is prompt
engineering, which involves designing suitable prompts or guiding statements to direct the model
in completing specific code generation tasks. By providing insightful inputs, models can generate
more accurate and expected code outputs. CodeGen [ 44] converts natural language questions
into code generation tasks and improves generation effectiveness through appropriate prompt
engineering techniques. DocPrompting [ 66] leverages keyword retrieval to integrate information
from code repositories into prompt phrases, guiding the LLMs to enhance the performance of LLMs
in code generation tasks.
2.2 Programming Evaluation
Evaluating large language modelsâ€™ (LLMs) capabilities can be effectively achieved through multi-
choice formats, where models are tasked with selecting the correct answer from given options.
Multi-choice RACE transforms the original reading comprehension dataset RACE [ 32] into a set
of questions with answer choices, facilitating the evaluation of the modelâ€™s score. AI2 Reasoning
Challenge (ARC) [ 13] serves as a benchmark to measure how well language models can reason and
draw conclusions within the domain of science. C-Eval [ 27] provides multiple-choice questions
from multiple subjects, enabling us to evaluate the competency of LLMs in different academic
domains. CodeApex employs multiple choice question formats to evaluate LLMâ€™s programming
comprehension ability.
Another crucial dimension in assessing LLMsâ€™ capabilities involves their performance on code-
related tasks. The development of specialized benchmark datasets offers nuanced insights into LLMsâ€™
programming comprehension and generation skills. In the earlier days, FlashFill++ [ 8] provides
a benchmark dataset for inferring string transformation programs from input-output examples.
CodeSearchNet [ 28] serves code search tasks by offering a vast collection of code snippets for code
retrieval and documentation generation. With the development of deep learning, NLP tasks have
become more diverse. Benchmark datasets have started combining code-related tasks with natural
language. CodeXGLUE [ 26] is a benchmark that encompasses natural language understanding
of code, covering various tasks such as code summarization, code translation, code completion,
and more. APPS [ 24] is a benchmark designed to evaluate program synthesis systems, with a
focus on probabilistic programs and their ability to generalize to new tasks. With the emergence
of LLMs, [ 9] proposed the HumanEval benchmark, specifically designed to assess the quality
of generated code. HumanEval-X[ 65] benchmark introduces support for multiple programming
languages, including C++, Python, Go, and Java, aiming to facilitate research in cross-language
code generation and understanding. CoderEval[ 60] benchmark specifically focuses on evaluating
large language modelsâ€™ abilities to tackle complex, industrial-level coding challenges through a
series of practical programming tasks. DebugBench [ 50] implants bugs into standard code by GPT4
and evaluates five LLMs in a zero-shot scenario. CodeApex focuses on evaluating the reasoning
capabilities of LLMs by presenting them with algorithmic challenges and associated test cases.
3 EVALUATION PROTOCOL
In this section, we introduce the evaluation framework of CodeApex for three code tasks: (1)
programming comprehension, (2) code generation, and (3) code correction. The overall experiment
scenarios of CodeApex are listed in Figure 1.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 5 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:5
Multi-hop ReasoningFunction-with-ContextFunction-onlyAnswer-onlyChain-of-Thought2-shotConceptual UnderstandingCommonsense ReasoningQuestion CategoryProgramming Comprehension
Code GenerationPrompt Strategy
Prompt StrategyIn-context Learning0-shot5-shot
Code CorrectionCodeApex
Prompt StrategyNarrative QuestionsExplicit QuestionsQuestion CategoryCode-with-Error MessageCode-onlyWrong AnswerCode-with-Standard CodeTime Limit ExceededRuntime ErrorError Type
Fig. 1. Overview diagram of CodeApex benchmark.
â€¢The first task is programming comprehension , whose test set includes 250multiple-choice exam
questions, which are categorized into conceptual understanding, commonsense reasoning,
and multi-hop reasoning questions. The questions are selected from the final exams of
different classes (Programming, Data Structure, Algorithm) at the university, which reduces
the risk that the test data is already in the training corpus of LLMs. LLMs are asked to choose
the correct option under the 0-shot, 2-shot, and 5-shot in-context learning scenarios. Due to
the effective application of Chain-of-Thought (CoT), we also compare the performances of
LLMs under the answer-only and CoT settings.
â€¢The second task is code generation , whose test set includes 476C++-based coding questions,
covering explicit questions and narrative questions. The description of the question and
the prototype of the function to implement the question are given, and LLMs are asked to
complete the main part of the function. We also provide both the function-only and function-
with-context scenarios, which indicate whether the calling code of the objective function is
given in addition to the description of the objective function (i.e., the context information of
the code).
â€¢The third task is code correction , whose test set includes 1330 erroneous code segments with
three types of error messages (Wrong Answer, Time Limit Exceeded, and Runtime Error).
We introduce three types of prompts to simulate code-only, code-with-error message, and
code-with-standard code scenarios.
For each task, we first provide an overview and then discuss about the data processing, prompting
strategy, and evaluation metric. To facilitate a fine-grained comparison of the differences in the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 6 ---
111:6Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
The storage space of the circular queue is an array A[21], where â€œfrontâ€ points to the position before the front element, and â€œrearâ€ points to the rear element. Assuming the current values of â€œfrontâ€ and â€œrearâ€ are 8 and 3 respectively, the length of the queue is ____.A.5B.6C.16D.17If there are definitions: char *p = "h12\0ab", str[] = "h12\0ab";, the values of sizeof(p), sizeof(str), and strlen(p) are____.A.4ï¼Œ4ï¼Œ3B.7ï¼Œ7ï¼Œ7C.4ï¼Œ8ï¼Œ7D.4ï¼Œ7ï¼Œ3If the most frequently used operations for a linear table are to access the value of the i-thelement and its predecessor and successor, in order to improve efficiency, the ____ storage method should be adopted.A.Singly linked listB. Doubly linked listC.Circular linked listD.Sequential listConceptual UnderstandingCommonsense ReasoningMulti-hop Reasoning
Fig. 2. Examples of the three categories of questions in the programming comprehension task.
abilities of LLMs across different natural languages, we provide aligned Chinese and English versions
for all test data in the three tasks.
3.1 Programming Comprehension Task
3.1.1 Overview. In the programming comprehension task, LLMs have to answer multiple-choice
questions that are closely related to code & programming topics. This evaluates the basic capabilities
of LLMs in understanding the code from various dimensions such as syntax, execution flow and code
structure. The ability of programming comprehension lays the foundation for LLMs to accomplish
other code-related tasks, and is therefore indispensable to thoroughly integrate LLMs into real-world
code production scenarios.
3.1.2 Data Processing. We evaluate the programming comprehension capability of LLMs via
multiple-choice questions, which generally cover the key knowledge points of programming, data
structure and algorithms. The multiple-choice question data is obtained from the final exams of
real-world college courses under strict confidentiality, greatly decreasing the possibility of the test
data overlapping with the training data of LLMs crawled online (i.e., test data leakage). Questions
with multiple correct answers are manually modified into single-choice questions to facilitate
testing consistency. Similar to C-Eval [ 27], we format the options of the multiple-choice questions
for alignment. The question prompts are provided in Markdown format, i.e., code blocks are marked
using triple quotes. All questions are manually cross-validated to ensure the unambiguity of the
question descriptions and the correctness of the answers.
In order to facilitate further exploration of the fine-grained abilities of LLMs, we manually classify
the questions into three categories: (1) conceptual understanding, (2) commonsense reasoning,
and (3) multi-hop reasoning. Example questions of three categories are demonstrated in Figure
2. Conceptual understanding questions assess LLMsâ€™ basic understanding of programming con-
cepts, typically involving selecting the correct concept or natural language description without
requiring specific code writing or comprehension. Commonsense Reasoning questions primarily
evaluate LLMsâ€™ ability to read and understand code. These two types of questions focus on a
single knowledge point, with relatively simple problem-solving steps that can be deduced through
single-step reasoning. Multi-hop reasoning questions evaluate LLMsâ€™ ability to reason about code.
These questions often involve chains of thought and require multiple steps of inference to get
the correct answer. These three categories of questions correspond to LLMsâ€™ elementary grasp
of programming concepts, basic understanding of code, and advanced comprehension of code,
respectively. The categorized results could provide us with deeper insights into the fine-grained
levels of programming comprehension capabilities of LLMs.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 7 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:7
ä»¥ä¸‹æ˜¯ä¸€æ¬¡ç¼–ç¨‹è€ƒè¯•çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·é€‰å‡ºå…¶ä¸­çš„æ­£ç¡®ç­”æ¡ˆã€‚è‹¥æ£®æ—Fæœ‰15æ¡è¾¹ã€25ä¸ªç»“ç‚¹ï¼Œåˆ™FåŒ…å«æ ‘çš„ä¸ªæ•°æ˜¯____A. 8B. 9C. 10D. 11ç­”æ¡ˆï¼šCâ€¦[x-shots]è‹¥æœ‰å®šä¹‰ï¼š```char str[] = {â€˜hâ€™,â€˜1â€™,â€˜2â€™,â€˜0â€™,â€˜aâ€™,â€˜bâ€™};const char *p = str;``` ä¸‹åˆ—è¯­å¥æ­£ç¡®çš„æ˜¯ï¼š____ã€‚A. p[2] = 's'B. strcpy(str,"123456")C. strcpy(p,"abc")D. strcpy(str,"abc")ç­”æ¡ˆ:DAnswer-only Scenario (Chinese Version)
(a) Answer-only setting (Chinese).
The following is a single choice question about a programming exam, please select the correct answer among them, please output only one answer.If forest F has 15 edges and 25 nodes, the number of trees contained in F is ____.A. 8B. 9C. 10D. 11Answer: Câ€¦[x-shots]If there is a definition: ```char str[] = {â€˜hâ€™,â€˜1â€™,â€˜2â€™,â€˜0â€™,â€˜aâ€™,â€˜bâ€™}; const char *p = str;```. Which of the following statements is correct:____.A. p[2] = 's'B. strcpy(str,"123456")C. strcpy(p,"abc")D. strcpy(str,"abc")Answer: DAnswer-only Scenario (English Version) (b) Answer-only setting (English).
Chain-of-Thought Scenario  (Chinese Version)ä»¥ä¸‹æ˜¯ä¸€æ¬¡ç¼–ç¨‹è€ƒè¯•çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·é€‰å‡ºå…¶ä¸­çš„æ­£ç¡®ç­”æ¡ˆã€‚è‹¥æ£®æ—Fæœ‰15æ¡è¾¹ã€25ä¸ªç»“ç‚¹ï¼Œåˆ™FåŒ…å«æ ‘çš„ä¸ªæ•°æ˜¯____A. 8B. 9C. 10D. 11ç­”æ¡ˆï¼šè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ€è€ƒã€‚1ï¼Œ15æ¡è¾¹ã€25ä¸ªç»“ç‚¹ï¼Œå³æœ‰25ä¸ªç‚¹æ„æˆ15æ¡è¾¹ï¼Œä¸”25ä¸ªç‚¹åªèƒ½å›´æˆæœ‰é™çš„æ£®æ—2ï¼Œå› æ­¤æ±‚æ£®æ—ä¸­æ ‘çš„ä¸ªæ•°ç­‰ä»·äºæ±‚è¿™25ä¸ªç‚¹å¯ä»¥ç»„æˆå¤šå°‘ä¸ªå­¤ç«‹çš„ç»„ï¼Œå³æœ‰å¤šå°‘é¢—ç‹¬ç«‹çš„æ ‘ã€‚3ï¼Œæ£®æ—ä¸­æœ‰mé¢—æ ‘ï¼Œåˆ™ç»“ç‚¹æ•°é‡ä¸º25-mï¼Œè¾¹æ•°ä¸º15-mï¼Œä»è€Œå¯å¾—ï¼š25 -m = 2 Ã—(15 -m),å³ m = 10ï¼Œå› æ­¤æ£®æ—FåŒ…å«10æ£µæ ‘ï¼Œç­”æ¡ˆä¸ºCã€‚å› æ­¤è¿™é“é¢˜çš„æ­£ç¡®ç­”æ¡ˆæ˜¯Cã€‚â€¦[x-shots]è‹¥æœ‰å®šä¹‰ï¼š```char str[] = {â€˜hâ€™,â€˜1â€™,â€˜2â€™,â€˜0â€™,â€˜aâ€™,â€˜bâ€™};const char *p = str;``` ä¸‹åˆ—è¯­å¥æ­£ç¡®çš„æ˜¯ï¼š____ã€‚A. p[2] = 's'B. strcpy(str,"123456")C. strcpy(p,"abc")D. strcpy(str,"abc")ç­”æ¡ˆ:è®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ€è€ƒã€‚1ï¼Œâ€¦â€¦2ï¼Œâ€¦â€¦å› æ­¤è¿™é“é¢˜æ­£ç¡®ç­”æ¡ˆæ˜¯Dã€‚
(c) Chain-of-Thought setting (Chinese).
The following is a single choice question about aprogramming exam, please select the correct answer among them.If forest F has 15 edges and 25 nodes, the number of trees contained in F is ____.A. 8B. 9C. 10D. 11Answer: Let's think step by step.1, 15 edges and 25 nodes, that is, 25 points form 15 edges and 25 points can only form a limited forest.2, â€¦3, â€¦[explanations to the example question]Therefore, the correct answer to this question is C.â€¦[x-shots]If there is a definition: ```char str[] = {â€˜hâ€™,â€˜1â€™,â€˜2â€™,â€˜0â€™,â€˜aâ€™,â€˜bâ€™}; const char *p = str;```. Which of the following statements is correct:____.A. p[2] = 's'B. strcpy(str,"123456")C. strcpy(p,"abc")D. strcpy(str,"abc")Answer: Letâ€™s think step by step.1,â€¦â€¦2, â€¦â€¦Therefore, the correct answer to this question is D.Chain-of-Thought Scenario  (English Version) (d) Chain-of-Thought setting (English).
Fig. 3. Examples illustrating the programming comprehension task in answer-only and chain-of-thought
scenarios, presented in both English and Chinese versions. The completed responses from LLMs are highlighted
in red, while the input prompts are shown in black text.
3.1.3 Prompting Strategy. We evaluate the code comprehension abilities of LLMs under two dif-
ferent prompting strategies, i.e., answer-only and chain-of-thought. In the answer-only setting,
LLMs are required to generate only a single option (i.e., the selected choice), and the correctness of
the generated option contributes to the final accuracy score. In the chain-of-thought setting, LLMs
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 8 ---
111:8Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Problem:Given an integer array numswhose size is n and a target integer, write a code to determine if there exists a contiguous subarray in numsthat sums up to the target. If such a subarray exists, return True; otherwise, return False.  The first number of input is n, and then input n numbers consecutively to represent array[1] to array[n]. Here1â‰¤#â‰¤1$7.Exampleinput:```3 123 5```Exampleoutput:```True```Testcase 1:  3 1 2 3 5â€¦Testcase 5: 1000 â€¦ â€¦Testcase 6:10000â€¦â€¦â€¦Testcase 9: 10000000 â€¦ â€¦Testcase 10: 10000000 â€¦ â€¦
Fig. 4. Illustration of testcases of different difficulty levels. If the complexity of the code is ğ‘‚(ğ‘›2), it could
pass at least the first six testcases. If the tester uses the prefix sum algorithms with ğ‘‚(ğ‘›)complexity, he can
pass all test points.
would first generate a piece of analytical text followed by a final answer, and the accuracy of the
final answer serves as the measure of model performance. We illustrate the prompt examples under
both settings in Figure 3.
3.1.4 Evaluation Metric. We adopt the accuracy (ACC) as the evaluation metric for the program-
ming comprehension task. After LLMs provide a response under either answer-only or chain-of-
thought settings, we extract the final choice using regular expressions. If the choice cannot be
extracted via regular expressions, it is regarded as an incorrect answer. The final accuracy of the
LLM is calculated as:
ACC =#Correct Answers
#Questions.
3.2 Code Generation Task
3.2.1 Overview. In the code generation task, LLMs have to extract relevant algorithm requirements
from natural language descriptions, and then, in conjunction with a given code function framework,
generate executable and accurate codes to pass the test cases. Such a task demands that large
language models possess the comprehension ability for both natural languages and code structures,
as well as the capability to analyze, design, and ultimately generate functional code snippets.
3.2.2 Data Processing. The test data is sourced from basic exercises on an internal Online Judge
platform, which provides question descriptions, sample inputs/outputs, code templates, and test
cases of varying difficulty levels. The question descriptions are typically provided in natural
language, some of which also contain scenario-based descriptions. The sample inputs/outputs
illustrate the format of the test case inputs and outputs, represented in Markdown format. The
code templates represent fully functional code that passes all the tests and is used to generate test
examples for CodeApex. The difficulty levels of the test cases are illustrated in Figure 4. Taking
binary search as an example, the first five test cases have small input sizes ğ‘›and can be solved
using a regular loop-based search. However, the last five test cases have larger input sizes, and if a
regular loop-based search is used, it will exceed the time limit. The privacy of data sources and the
diversity of question descriptions decrease the possibility of test data appearing in the training
corpus of LLMs. This promotes the fairness of evaluation and contributes to rational cognition of
the programming abilities of different LLMs.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 9 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:9
Code Generation (Question-Description-Only)Prompt: I will provide you with a specific question description. Your task is to generate the code that accomplishes this target task. You only need to provide the C++ code; please do not include additional explanatory text. Problem: input integer a and b, output the value of a+b.Time Limit: 1000 msSpace Limit: 1 MBGPT3.5-turbo:#include<iostream>intmain() {inta, b;std::cout<< "Enter the value of a: â€;std::cin>> a;std::cout<< "Enter the value of b: ";std::cin>> b;intsum = a + b;std::cout<< "The sum of a and b is: " << sum << std::endl;return0;}
Fig. 5. Illustration of the Question-Description-Only mode, where only the code question description is fed
into LLMs for generation. The generated code by GPT3.5-turbo is demonstrated below. The code contains
additional boot outputs, causing this logically correct code to fail the testcases.
Most LLMs are unable to generate code that meets the given input-output conditions based
solely on question descriptions. When we directly input the original question description into
LLMs, it is highly likely that the generated code will contain additional prompt-related output.
Although the logic of the code itself may be correct, these extra outputs are not included in the
original test cases, causing the generated code to fail the tests. Figure 5 illustrates an example of
this situation, which occurs during code generation with GPT-3.5-turbo. In such cases, in order
to more accurately evaluate code generation capability of LLMs itself, we utilize GPT-4 to assist
in generating functional form test examples. With an in-context learning approach, we guide the
LLMs in rewritting the template code and dividing it into a main function and some auxiliary
functions. The main function includes (i) library references, (ii) the main function itself (with input
and output code), and (iii) calls to auxiliary functions. The auxiliary functions contain the main
algorithmic logic with input parameters and return the output to the main function.
To further explore the different types of code generation abilities of LLMs, we divide the questions
into two categories: explicit questions and narrative questions. Explicit questions directly specify the
functionality that the program needs to implement, such as "Implement a quicksort" or "Calculate
the number of characters in the input." Narrative questions consist of more complex natural language
descriptions, with the code requirements hidden within a scenario. For example, given a class of
students, you need to inquire about the grade of the k-th student. This question requires storing
the grades of the students in an array, sorting them, and then outputting the k-th largest element.
Previous benchmarks [ 9] focus on explicit questions, and do not pay attention to the importance of
evaluating narrative questions.
3.2.3 Prompting Strategy. We describe our approach for generating target functions that adhere to
function templates by designing prompts. These prompts consist of three components: question
description, function description, and data range specification. In the â€™function-onlyâ€™ scenario,
no additional information is included in the prompts. In this case, LLMs are asked to generate a
runnable function code that conforms to the function prototype and the question description. In the
function-with-context scenario, we incorporate the context of the main function as an additional
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 10 ---
111:10Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Function-only Scenario (Chinese) æˆ‘ä¼šç»™ä½ ä¸€ä¸ªéœ€è¦ç”¨C++ç¼–ç¨‹è§£å†³çš„é—®é¢˜å’Œä¸€ä¸ªC++çš„å‡½æ•°å£°æ˜ï¼Œä½ çš„ä»»åŠ¡æ˜¯æŒ‰ç…§è¿™ä¸ªå‡½æ•°å£°æ˜ç”Ÿæˆå¯¹åº”çš„C++å‡½æ•°å»è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ åªéœ€è¦è¿”å›C++å‡½æ•°,ä¸è¦è¿”å›é¢å¤–çš„è¯´æ˜æ–‡å­—ã€‚é—®é¢˜ï¼šè¾“å…¥ä¸¤ä¸ªæ•´æ•°aï¼Œbï¼Œè¾“å‡ºa+bçš„å€¼ã€‚å‡½æ•°å£°æ˜ï¼šint sum_a_b(int a, int b);
(a) Function-only scenario (Chinese).
Function-only Scenario (English) I will provide you with a problem that needs to be solved using C++ programming, along with a C++ function declaration. Your task is to generate the corresponding C++ function based on this function declaration to solve the given problem. You only need to return the C++ function, without any additional explanatory text.Problem: input integers a and bm output the value of a+b.Function Declaration:int sum_a_b(int a, int b); (b) Function-only scenario (English).
Function-with-context Scenario (Chinese)æˆ‘ä¼šç»™ä½ ä¸€ä¸ªç”¨äºè§£å†³æŸä¸ªé—®é¢˜ä½†ç¼ºå°‘äº†å‡½æ•°å®šä¹‰çš„C++ä»£ç ï¼Œè¿™ä¸ªç¼ºå¤±å‡½æ•°å®šä¹‰çš„å‡½æ•°å£°æ˜ï¼Œè¿™ä¸ªC++ä»£ç è§£å†³çš„é—®é¢˜åŠè¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯ç”¨è¿™ä¸ªå‡½æ•°å£°æ˜ç”Ÿæˆå‡½æ•°å®šä¹‰å»å¡«è¡¥æ‰€ç»™çš„C++ä»£ç ç¼ºå°‘çš„éƒ¨åˆ†ï¼Œå¹¶æ»¡è¶³æ—¶é—´é™åˆ¶å’Œç©ºé—´é™åˆ¶ã€‚ä½ åªéœ€è¦è¿”å›C++ä»£ç çš„å‡½æ•°å®šä¹‰,ä¸è¦è¿”å›é¢å¤–çš„è¯´æ˜æ–‡å­—ã€‚é—®é¢˜ï¼šè¾“å…¥ä¸¤ä¸ªæ•´æ•°aï¼Œbï¼Œè¾“å‡ºa+bçš„å€¼ã€‚å‡½æ•°å£°æ˜ï¼šint sum_a_b(int a, int b);ä»£ç ï¼š#include <iostream>//function start//function endint main() {int a, b;std::cin >> a>> b;//calling startint sum = sum_a_b(int a, int b);//calling endstd::cout << sum << std::endl;return 0;}è¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼šè¾“å…¥ï¼š```1 2```ï¼Œè¾“å‡ºï¼š```3```æ—¶é—´é™åˆ¶ï¼š1000msç©ºé—´é™åˆ¶ï¼š1MB
(c) Function-with-Context scenario (Chinese).
Function-with-context Scenario (English)I will give you a C++ code that solves a problem but is missing a function definition, a function declaration that is missing a function definition, a problem that the C++ code solves and an input/output example. Your task is to use this function declaration to generate a function definition that fills in the missing parts of the given C++ code and satisfies the time and space constraints. You only need to return the C++ function, without any additional explanatory text.Problem: input integers a and bm output the value of a+b.Function Declaration:int sum_a_b(int a, int b);Code: #include <iostream>//function start//function endint main() {int a, b;std::cin >> a>> b;//calling startint sum = sum_a_b(int a, int b);//calling endstd::cout << sum << std::endl;return 0;}Input-Output Examples:Input: ```1 2```,Output:```3```Time Limit: 1000msSpace Limit:1MB (d) Function-with-Context scenario (English).
Fig. 6. Examples of specific design of the prompts for code generation task. The model generates the text
highlighted in yellow.
input to guide the LLM to generate the target function. The specific design of the prompts is
illustrated in Figure 6.
3.2.4 Evaluation Metric. To evaluate the effectiveness of the code, we use the original questionâ€™s
test cases to measure its correctness. Each question provides 5-10 test cases that cover various
possible input scenarios and edge conditions. CodeApex benchmark aligns with real-world scenarios
of human programming by using the number of passed test cases as the modelâ€™s score. All test case
scores are equally weighted, regardless of their difficulty.
CodeApex introduces an evaluation metric called accept rate as the evaluation function for the
generated code by the model. When the language model generates a response, we extract the
runnable C++ code function from it and concatenate it into the main function in the dataset. The
concatenated code remains executable, and the target input-output pairs are aligned with the test
cases in the original dataset. By comparing the output of the test cases with the actual output, we
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 11 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:11
can get the number of the test cases the generated code passes, i.e., #passes(ğ‘). We calculate the
ğ´ğ¶@ğ‘˜, which represents the quality of the model-generated code
ğ´ğ¶@ğ‘˜=1
|ğ‘„|âˆ‘ï¸
ğ‘âˆˆğ‘„[I(#passes(ğ‘)â‰¥ğ‘˜)],
whereğ‘„is the question set, and the indicator function Ipresents
I(condition)=1,if condition is true,
0,otherwise.
Whenğ‘˜is set to 1, the code can pass at least one test case. ğ´ğ¶@1reflects the functional correctness
of the model-generated code. In this case, some test cases may not pass, which may result from
inappropriate algorithm selection in the generation process or exceeding the time/space limits of
the question itself. When ğ‘˜is set toğ‘ğ‘™ğ‘™,ğ´ğ¶@ğ‘ğ‘™ğ‘™represents the proportion of questions that have
passed all test cases. This is a relatively strict metric for evaluating performance. In addition, we
introduce the Accept Rate (AC Rate) metric to measure how well the generated code meets. The
AC Rate is calculated as follows:
AC Rate =1
|ğ‘„|âˆ‘ï¸
ğ‘âˆˆğ‘„h#passes(ğ‘)
#testcases(ğ‘)i
.
It reflects the overall performance of LLMs in the code generation task, including both algorithm
selection and implementation.
3.3 Code Correction Task
3.3.1 Overview. In code correction task, LLMs need to revise the generated code with the question
description, error message and standard code. Code correction is an essential stage in the program-
ming procedure, the mastering of which could offer practical assistance in software engineering. In
addition, utilizing the self-inspiration ability of LLMs to correct codes has emerged as a principle
way for improving the accuracy of code generation [ 10]. Therefore, we include the evaluation for
code correction, which is a fundamental ability of programming by LLMs.
3.3.2 Data Processing. The test data is originated from real studentsâ€™ submission of exercises on
the internal Online Judge platform. Each submission contains the exercise ID, submission code,
error type, and submission time. The types of error include: Wrong Answer (WA), Time Limit
Exceeded (TLE), and Runtime Error (RE). If the submission answers all testcases correctly, then the
error type is empty.
To ensure that erroneous code can be corrected within a certain number of steps rather than
rewritten, we sample examples from submissions of the same users, where both the incorrect
version and the correct version of the same problem exist. The incorrect version is then sampled
to be the test sample, where we restrict the edit distance [ 33] between the incorrect version and
the correct version to be less or equal to 50. From the generated code pairs, we select code pairs
for each error type. Due to varying frequencies of difference error types, the resulting error codes
primarily consist of WA error, along with a portion of other error types. We also ensure that the
number of code pairs for each question is roughly equal.
3.3.3 Prompting Strategy. We design three types of prompts for simulating different code correction
scenarios. The first type of prompt, Code-only, contains only the erroneous code and the question
description without any additional information. This prompt is used to evaluate code correction
capability of LLMs, simulating humans directly reading code and identifying errors. The second
type of prompt, Code-with-Error Message, includes the erroneous code, task description, and the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 12 ---
111:12Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
ä¸‹é¢ç»™ä½ ä¸€é“é¢˜ç›®æè¿°ä»¥åŠå­¦ç”Ÿçš„é”™è¯¯ä»£ç ï¼Œè¯·ä½ å°†åœ¨ç°æœ‰ä»£ç çš„åŸºç¡€ä¸Šä¿®æ­£ï¼Œä½¿å…¶å¯ä»¥é€šè¿‡åŸæœ‰é¢˜ç›®çš„æµ‹è¯•ç‚¹ã€‚é¢˜ç›®æè¿°ï¼šè¾“å…¥ä¸€ä¸ªå­—ç¬¦ï¼Œåˆ¤æ–­è¯¥å­—ç¬¦æ˜¯å¤§å†™æˆ–å°å†™å­—æ¯ã€‚é”™è¯¯ä»£ç ä¸ºï¼š```#include <bits/stdc++.h>using namespace std; int main() {char aa;if( ( aa>='a' && aa<='z' ) || ( aa>='A' && aa<='Z' ) )cout<<"yes";else cout<<"no";return 0;}```è¯·ä»…ä»…è¾“å‡ºä¿®æ­£ä¹‹åçš„æ­£ç¡®ä»£ç ï¼Œä¸è¦è¾“å‡ºå¤šä½™ä¿¡æ¯ã€‚Code-only Scenario (Chinese)
(a) Code-only scenario (Chinese).
Below you are given a task description and the studentâ€™s erroneous code, please correct the existing code so that it can pass the test cases of the task.Task description:Enter a character, check if the character is an uppercase or lowercase letter. Erroneous code isï¼š```#include <bits/stdc++.h>using namespace std; int main() {char aa;if( ( aa>='a' && aa<='z' ) || ( aa>='A' && aa<='Z' ) )cout<<"yes";else cout<<"no";return 0;}```Please output only the correct code after the correction and do not output redundant information.Code-only Scenario (English) (b) Code-only scenario (English).
ä¸‹é¢ç»™ä½ ä¸€é“é¢˜ç›®æè¿°ä»¥åŠå­¦ç”Ÿçš„é”™è¯¯ä»£ç ï¼Œè¯·ä½ å°†åœ¨ç°æœ‰ä»£ç çš„åŸºç¡€ä¸Šä¿®æ­£ï¼Œä½¿å…¶å¯ä»¥é€šè¿‡åŸæœ‰é¢˜ç›®çš„æµ‹è¯•ç‚¹ã€‚é¢˜ç›®æè¿°ï¼šè¾“å…¥ä¸€ä¸ªå­—ç¬¦ï¼Œåˆ¤æ–­è¯¥å­—ç¬¦æ˜¯å¤§å†™æˆ–å°å†™å­—æ¯ã€‚é”™è¯¯åŸå› ä¸ºï¼šPartially Acceptedé”™è¯¯ä»£ç ä¸ºï¼šâ€¦â€¦è¯·ä»…ä»…è¾“å‡ºä¿®æ­£ä¹‹åçš„æ­£ç¡®ä»£ç ï¼Œä¸è¦è¾“å‡ºå¤šä½™ä¿¡æ¯ã€‚Code-with-Error Message Scenario (Chinese)
(c) Code-with-Standard Code scenario (Chinese).
Below you are given a task description and the studentâ€™s erroneous code, please correct the existing code so that it can pass the test cases of the task.Task description: Enter a character, check if the character is an uppercase or lowercase letter. Error type isï¼šPartially AcceptedErroneous code is:â€¦â€¦Please output only the correct code after the correction and do not output redundant information.Code-with-Error Message (Chinese)Code-with-Error Message Scenario (English) (d) Code-with-Error Message scenario (English).
ä¸‹é¢ç»™ä½ ä¸€é“é¢˜ç›®æè¿°ä»¥åŠå­¦ç”Ÿçš„é”™è¯¯ä»£ç ï¼Œè¯·ä½ å°†åœ¨ç°æœ‰ä»£ç çš„åŸºç¡€ä¸Šä¿®æ­£ï¼Œä½¿å…¶å¯ä»¥é€šè¿‡åŸæœ‰é¢˜ç›®çš„æµ‹è¯•ç‚¹ã€‚é¢˜ç›®æè¿°ï¼šè¾“å…¥ä¸€ä¸ªå­—ç¬¦ï¼Œåˆ¤æ–­è¯¥å­—ç¬¦æ˜¯å¤§å†™æˆ–å°å†™å­—æ¯ã€‚é”™è¯¯åŸå› ä¸ºï¼šPartially Acceptedé”™è¯¯ä»£ç ä¸ºï¼šâ€¦â€¦æ ‡å‡†ä»£ç ä¸ºï¼š```#include <bits/stdc++.h>using namespace std; int main() {char aa;if( ( aa>='a' && aa<='z' ) || ( aa>='A' && aa<='Z' ) )cout<<"yes";else cout<<"no";return 0;}```è¯·ä»…ä»…è¾“å‡ºä¿®æ­£ä¹‹åçš„æ­£ç¡®ä»£ç ï¼Œä¸è¦è¾“å‡ºå¤šä½™ä¿¡æ¯ã€‚Code-with-Standard Code Scenario (Chinese)
(e) Code-with-Error Message scenario (Chinese).
Below you are given a task description and the studentâ€™s erroneous code, please correct the existing code so that it can pass the test cases of the task.Task description: Enter a character, check if the character is an uppercase or lowercase letter. Error type isï¼šPartially AcceptedErroneous code is:â€¦â€¦Standardcodeis:```#include <bits/stdc++.h>using namespace std; int main() {char aa;if( ( aa>='a' && aa<='z' ) || ( aa>='A' && aa<='Z' ) )cout<<"yes";else cout<<"no";return 0;}```Please output only the correct code after the correction and do not output redundant information.Code-with-Standard Code (English)Code-with-Standard Code Scenario (English) (f) Code-with-Standard Code scenario (English).
Fig. 7. Examples of specific design of the prompts for code correction task.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 13 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:13
type of error. This prompt simulates engineers debugging their code based on actual feedback from
the compiler, aligning with real-world development scenarios. The third type of prompt is called
Code-with-Standard Code. It builds upon the Code-with-Error Message prompt by adding standard
code as additional input. In the development process, engineers can refer to existing code for
correction. Evaluating code correction ability of LLMs in this scenario can assess its comprehensive
programming capabilities, providing a guidance for LLM-assisted programming. The specific design
of three types of prompts is demonstrated in Figure 7.
3.3.4 Evaluation Metric. Similar to code generation, we use the same test cases and same metrics
(AC@1, AC@all, AC Rate) to measure the correctness of modified code. LLMs should realize code
correction based on the input erroneous code. If the difference between the input and output code
is too large (edit distance greater than 50), the output code is considered invalid. The score for
invalid code is recorded as 0, regardless of whether it passes the test cases. In code-with-standard
code scenario, in order to prevent the LLM from directly outputting standard code, we validate the
generated code in the Code-with-Standard Code scenario. Any output code that is identical to the
standard code is directly marked as an incorrect answer.
4 EVALUATION RESULTS
4.1 Experimental Setup
4.1.1 Dataset Statistics. In the programming comprehension task, CodeApex releases a total of
250 multiple-choice questions, which are divided into three categories: conceptual understanding
(90 questions), commonsense reasoning (99 questions), and multi-hop reasoning (61 questions). In
the code generation task, CodeApex releases 476 algorithm questions in total. These questions are
divided into 258 functional questions and 218 descriptive questions. In the code correction task,
CodeApex releases 1330 segments of erroneous code, corresponding to solving 116 questions in
the code generation task. Each erroneous code has an error message, with a total of three types of
error messages: Wrong Answer (1175 code segments), Time Limit Exceeded (69 code segments)
and Runtime Error (86 code segments).
The test data is available on GitHub1for users to download and run. The standard answers are not
publicly disclosed to ensure the appropriate usage of CodeApex, as its data might unintentionally
include web-crawled content. Users are required to submit their model predictions to the provided
webpage2to obtain an automated accuracy assessment, and a public leaderboard is maintained on
the webpage. Users can choose whether to publish their modelsâ€™ scores on the leaderboard.
4.1.2 LLMs to Evaluate. CodeApex totally evaluates 12 LLMs, comprising a total of 41 variants.
The tested LLMs and their variants are listed in Table 2. The variants of the models encompass
the chatbot mode and the completion mode, as well as different parameter sizes and versions
within the same base model. In the programming comprehension task, we conduct experiments
on 10 LLMs, including both open-source models and API-based models. In the code generation
task, we evaluate 11 accessible LLMs, including both general-purpose and code-specialized models.
In the code correction task, due to the inability of some LLMs to output recognizable code, our
experiments showcase the results of 7 models. The largest open-source models have 13 billion
parameters, while the largest parameterized model based on API is GPT4.
Parameter Settings. When LLMs generate responses, we choose two distinct temperatures to
cater to different application scenarios. For the programming comprehension task, we set tempera-
ture=0.01 to generate more conservative results, preventing the model from producing responses
1https://github.com/APEXLAB/CodeApex.git
2https://apex.sjtu.edu.cn/codeapex/
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 14 ---
111:14Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 2. CodeApex evaluates 14 LLMs along with their variants.
Model Model Size Form Creator
ChatGLM[38] 6B open, general-purpose
ChatGLM2[38] 6B open, general-purpose
ChatGLM3[38] 6B open, general-purposeTsinghua & Zhipu
MOSS[48] 16B open, general-purpose Fudan University
Chinese-Alpaca[15] 7B open, general-purpose Cui et al. [15]
BELLE[29, 30] 7B open, general-purpose Beike
BELLE-LLaMA[29, 30] 7B, 13B open, general-purpose Beike
InternLM-Chat[49] 7B open, general-purpose Shanghai AI Lab & Sense Time
Baichuan[5] 7B open, general-purpose Baichuan Inc.
EduChat-base[16] 7B, 13B open, Edu-Use
EduChat-sft[16] 7B open, Edu-UseEast China Normal University
CodeT5-plus[53] 6B, 16B open, code-specialized Salesforce AI
WizardCoder[40] 15B open, code-specialized Microsoft
StarCoder[34] undisclosed api, code-specialized BigCode
Vicuna[64] 13B open, general-purpose LM-SYS
GPT-3.5-turbo[63] undisclosed api, general-purpose
GPT-4 undisclosed api, general-purposeOpenAI
that do not align with the desired answers. For the code generation and code correction task, we
opted for a temperature of 0.7 and a top-p value of 0.8 to encourage the model to generate a broader
range of possible codes, introducing more randomness in each code generation instance.
4.2 Programming Comprehension Results
The overall performance of the programming comprehension task in the answer-only and CoT
scenarios are presented in Table 3 and Table 4. The tables display the best results achieved in
the 0-shot, 2-shot, and 5-shot settings. Detailed results for the N-shot ( ğ‘=0,2,5) settings are
provided in the Appendix A. GPT4 obtains the highest average accuracy among all the models,
followed by GPT3.5-turbo. The third best model in Answer-only scenario is InternLM-Chat-7B,
which has an accuracy of 37%, which still have a huge gap between GPT. It is noteworthy that the
accuracy of most LLMs is below 50%, highlighting that CodeApex Benchmark is challenging in the
programming comprehension task.
Bilingual Accuracy Comparison. The Chinese version scored higher than the English ver-
sion. There are two main reasons: (1) The source question descriptions are obtained from final
exams in Chinese universities, and thus the exam questions are initially presented in Chinese. Even
after translating them into English version, they still contain some language habits specific to
Chinese speakers. As a result, when inputting these English questions with biases into the LLM,
some noise might be introduced into the modelâ€™s encoding results. (2) Most evaluated models are
primarily trained on Chinese data, which leads to a poor understanding of English. In Table 3
and 4, LLM trained primarily on English corpus, such as codeT5 and GPT3.5-turbo, tend to have
approximate performance on Chinese and English versions.
Accuracy Comparison on Different Question Categories. Among the three question categories,
approximately half of the models perform best in conceptual comprehension, followed
by commonsense reasoning, with the lowest accuracy on multi-hop reasoning. This result
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 15 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:15
Table 3. Accuracy results of Programming Comprehension Task in the answer-only setting. C.U., C.R., and
M.H.R. indicate conceptual understanding, commonsense reasoning, and multi-hop reasoning questions,
respectively. Each column displays the best accuracy among 5-shot, 2-shot, and 0-shot scenarios. The best
performance of LLMs is in bold, the second-best performance is underlined, and the third-best performance
is wave underlined. * represents chat-bot mode of the LLM. ZH and EN represent the results of Chinese and
English test versions.
Programming Comprehension (Answer-only)
ZH EN Model
C.U. C.R. M.H.R. Total C.U. C.R. M.H.R. Total
ChatGLM-6B 0.3222 0.2626 0.3115 0.2960 0.3556 0.2424 0.3115 0.2960
ChatGLM-6B* 0.3333 0.2727 0.3115 0.2960 0.3926 0.2727:::::0.3770 0.3200
ChatGLM2-6B 0.3667 0.2727 0.3388 0.2987 0.4111 0.3165 0.2951 0.2987
ChatGLM2-6B* 0.3444 0.2626 0.3115 0.3040 0.3556 0.2828 0.3115 0.3040
ChatGLM3-6B 0.3770 0.2626 0.3770 0.3013 0.3667 0.3300 0.2951 0.3240
ChatGLM3-6B* 0.4593 0.2727 0.2789 0.3080 0.3407 0.2256 0.2951 0.2733
MOSS-16B* 0.3148 0.3401 0.3279 0.3120 0.2630 0.3468 0.3333 0.3120
Chinese-Alpaca-7B 0.3481 0.2929 0.2678 0.2880 0.3111 0.2727 0.3279 0.2947
Chinese-Alpaca-7B* 0.3000 0.2896 0.2787 0.2840 0.2926 0.2896 0.2350 0.2840
Chinese-Alpaca-plus-7B 0.2333 0.3367 0.2623 0.2653 0.3407 0.2963 0.3279 0.2987
Chinese-Alpaca-plus-7B* 0.3259 0.3030 0.2459 0.2853 0.2630 0.2929 0.2295 0.2573
Chinese-Alpaca-13B 0.2593 0.2929 0.2623 0.2733 0.3111 0.2323 0.2131 0.2733
Chinese-Alpaca-13B* 0.2741 0.2929 0.2077 0.2627 0.2778 0.2795 0.2514 0.2547
Chinese-Alpaca-plus-13B 0.2667 0.3401 0.2131 0.2827 0.2444 0.3569 0.3060 0.2827
Chinese-Alpaca-plus-13B* 0.2741 0.2997 0.1967 0.2573 0.2889 0.3569 0.2459 0.2973
BELLE-7B-1M 0.3333 0.3199 0.2787 0.2947 0.3333 0.3199 0.2787 0.3080
BELLE-7B-1M* 0.3481 0.3131 0.2459 0.3013 0.3889 0.3131 0.2404 0.3040
BELLE-7B-2M 0.2444 0.3367 0.2842 0.2760 0.2556 0.3165 0.2842 0.2613
BELLE-7B-2M* 0.2111 0.3165 0.2787 0.2413 0.2667 0.3165 0.2459 0.2400
BELLE-LLaMA-7B-0.6M 0.2926 0.3401 0.3169 0.3053 0.3222 0.3333 0.3279 0.2880
BELLE-LLaMA-7B-0.6M* 0.2778 0.3131 0.3115 0.3000 0.3444 0.2963 0.3115 0.3000
BELLE-LLaMA-7B-2M 0.2222 0.2963 0.1967 0.2387 0.3000 0.3333 0.2623 0.2680
BELLE-LLaMA-7B-2M* 0.2778 0.3064 0.2623 0.2840 0.2778 0.3232 0.2623 0.2840
BELLE-LLaMA-13B-2M 0.2889 0.3333 0.1967 0.2840 0.2222 0.3131 0.2131 0.2840
BELLE-LLaMA-13B-2M* 0.3444 0.2929 0.2186 0.2827 0.2667 0.2660 0.2186 0.2693
InternLM-Chat-7B 0.4556 0.3434 0.3443 0.3720 0.4444 0.3636 0.3443 0.3733
Baichuan-7B 0.3407 0.3367 0.2623 0.3147 0.2926 0.3064 0.2951 0.3147
EduChat-base-002-7B* 0.2259 0.3131 0.2295 0.2480 0.3296 0.3434 0.3279 0.3147
EduChat-base-002-13B* 0.2815 0.3535 0.2459 0.3013 0.3667:::::0.3636 0.2459 0.3267
EduChat-sft-002-7B* 0.2444 0.2424 0.2951 0.2560 0.2889 0.3434 0.2459 0.2920
CodeT5-plus-16B 0.2556 0.3030 0.3224 0.2640 0.2074 0.2997 0.3224 0.2640
CodeT5-plus-16B* 0.2815 0.3737 0.2732 0.3160 0.1963 0.3737 0.2787 0.2467
CodeT5-plus-6B 0.2333 0.3535 0.2404 0.2693 0.2778 0.3535 0.2896 0.3173
CodeT5-plus-6B* 0.2296 0.2828 0.2568 0.2573 0.2704 0.2828 0.2568 0.3040
GPT-3.5-turbo:::::0.6074 0.3300 0.6066:::::0.4893 0.5222 0.3468 0.6066 0.4893
GPT-3.5-turbo* 0.5963:::::0.3603:::::0.6393:::::0.5053:::::0.5333 0.3603 0.6066 0.4413
GPT4 0.6741 0.5455 0.8360 0.6600 0.6444 0.4848 0.8033 0.6160
GPT4* 0.7000 0.5455 0.8197 0.6680 0.6222 0.4848 0.8033 0.6120
Human (closed-book) 0.5818 0.6599 0.6683 0.6343 - - - -
Human (open-book) 0.8596 0.6821 0.8878 0.7960 - - - -
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 16 ---
111:16Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 4. Accuracy results of Programming Comprehension Task in CoT setting. C.U., C.R., and M.H.R. indicate
conceptual understanding, commonsense reasoning, and multi-hop reasoning questions, respectively. Each
column displays the best accuracy among 5-shot, 2-shot, and 0-shot scenarios. The best performance of LLMs
is in bold, the second-best performance is underlined, and the third-best performance is wave underlined. *
represents chat-bot mode of the LLM. ZH and EN represent the results of Chinese and English test versions.
Since CodeT5 fails to generate a response with chain-of-thought, we exclude it in this setting.
Programming Comprehension (Chain-of-Thought)
ZH EN Model
C.U. C.R. M.H.R. Total C.U. C.R. M.H.R. Total
ChatGLM-6B 0.3111 0.2121 0.3770 0.2880 0.3444 0.1818 0.3115 0.2640
ChatGLM-6B* 0.3333 0.2222 0.2787 0.2480 0.2889 0.2525 0.3115 0.2760
ChatGLM2-6B 0.4778 0.2525 0.3115 0.3240 0.3667 0.2525 0.2951 0.2960
ChatGLM2-6B* 0.4111 0.2929 0.3770 0.3360 0.4000 0.2525 0.2787 0.2960
ChatGLM3-6B 0.3630 0.2626 0.2514 0.2827 0.3963 0.2963 0.2841 0.2867
ChatGLM3-6B* 0.3519 0.2256 0.3497 0.2733 0.3370 0.2054 0.2842 0.2693
MOSS-16B* 0.2667 0.1717 0.3279 0.2240 0.2667 0.2020 0.2787 0.2360
Chinese-Alpaca-7B 0.2889 0.3030 0.2131 0.2680 0.2556 0.3333 0.2131 0.2640
Chinese-Alpaca-7B* 0.2778 0.3232 0.2623 0.2640 0.2667 0.3232 0.2295 0.2800
Chinese-Alpaca-plus-7B 0.3667 0.3131 0.2787 0.3240 0.2778 0.3030 0.2951 0.2880
Chinese-Alpaca-plus-7B* 0.2667 0.2626 0.2295 0.2400 0.2778 0.2626 0.2623 0.2480
Chinese-Alpaca-13B 0.2889 0.2929 0.2459 0.2640 0.3111 0.2323 0.2295 0.2520
Chinese-Alpaca-13B* 0.3111 0.2727 0.2295 0.2640 0.2444 0.2121 0.2623 0.2160
Chinese-Alpaca-plus-13B 0.3111 0.2929 0.2131 0.2600 0.3111 0.2525 0.2951 0.2560
Chinese-Alpaca-plus-13B* 0.3556 0.2727 0.2295 0.2920 0.3000 0.2222 0.2951 0.2560
BELLE-7B-1M 0.3333 0.2929 0.2459 0.2720 0.2444 0.2222 0.2131 0.2120
BELLE-7B-1M* 0.3333 0.2525 0.2131 0.2600 0.2111 0.2020 0.1475 0.1840
BELLE-7B-2M 0.2889 0.2626 0.2295 0.2240 0.2556 0.2222 0.1311 0.2040
BELLE-7B-2M* 0.2667 0.2424 0.2131 0.2400 0.1889 0.2222 0.1311 0.1880
BELLE-LLaMA-7B-0.6M 0.2667 0.3030 0.2951 0.2760 0.2222 0.2323 0.2623 0.2320
BELLE-LLaMA-7B-0.6M* 0.3444 0.3636 0.3115 0.3200 0.3333 0.2424 0.2623 0.2600
BELLE-LLaMA-7B-2M 0.2444 0.3030 0.2295 0.2640 0.2222 0.2121 0.1311 0.1880
BELLE-LLaMA-7B-2M* 0.2778 0.3030 0.2459 0.2800 0.2111 0.2020 0.1967 0.1880
BELLE-LLaMA-13B-2M 0.2889 0.2727 0.1967 0.2560 0.2556 0.2222 0.1967 0.2120
BELLE-LLaMA-13B-2M* 0.2667 0.2828 0.2131 0.2600 0.2778 0.1717 0.1803 0.2120
InternLM-Chat-7B 0.3556 0.1818 0.3607 0.2880 0.4222 0.2525 0.2623 0.3160
Baichuan-7B 0.0667 0.1111 0.0656 0.0720 0.1222 0.1111 0.2960 0.1000
EduChat-base-002-7B* 0.2778 0.2323 0.2623 0.2480 0.2556 0.2424 0.2623 0.2360
EduChat-base-002-13B* 0.3000 0.2424 0.3279 0.2800 0.3111 0.2424 0.2787 0.2680
EduChat-sft-002-7B* 0.3111 0.2121 0.2623 0.2520 0.3111 0.2222 0.2623 0.2560
GPT-3.5-turbo:::::0.5944 0.4141:::::0.6803:::::0.5260:::::0.5278 0.3687:::::0.6393 0.4740
GPT-3.5-turbo* 0.5667:::::0.4242 0.6503 0.5187 0.5074:::::0.3737 0.6339:::::0.4853
GPT4 0.7111 0.5152 0.8361 0.6640 0.6444 0.5051 0.8689 0.6320
GPT4* 0.7222 0.5859 0.7869 0.6960 0.6556 0.5152 0.8361 0.6280
Human (closed-book) 0.5818 0.6599 0.6683 0.6343 - - - -
Human (open-book) 0.8596 0.6821 0.8878 0.7960 - - - -
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 17 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:17
indicates that LLMs are likely to incorporate knowledge of programming concepts during training.
Most models scored higher in commonsense reasoning compared to multi-hop reasoning, suggesting
that the capability of LLMs significantly decreases with an increase in the number of reasoning
steps.
Effects of Chain-of-Thought Prompting. Most of the models achieve approximate or lower
accuracy than the answer-only setting. The accuracy results of the CoT setting are depicted in
Table 4. The reasons for this observation are two folds: (1) Models we evaluate do not reach model
sizes that have the emergent ability of CoT. According to [ 54], the emergence of CoT requires
LLM to have at least 60B parameters. When the parameter number is not enough, the CoT setting
might introduce additional noise, and the generated response of LLM would be unstable. Thatâ€™s
why GPT3.5-turbo, which has reached the emergence point, achieves higher accuracy in the CoT
setting. (2) When answering conceptual understanding and commonsense reasoning questions, we
do not require multi-step reasoning. Thus, the CoT ability of LLMs does not provide assistance for
these categories of questions. However, for multi-hop reasoning questions, there is a noticeable
improvement in accuracy in the CoT scenario for some models (such as ChatGLM2, educhat, and
GPT3.5-turbo). Since CodeT5 fails to generate a response with chain-of-thought, we exclude it in
the CoT setting.
Human Performance. Novice programmers perform similarly to GPT-4 in closed-book
tests after learning, while human performance in open-book exams is significantly better
than all LLMs. The performance of human testers is shown in Table 3. Note that programming
comprehension tasks in CodeApex is considered a semi-open-book exam for LLMs, i.e., they have
limited offline knowledge base.
4.3 Code Generation Results
Code generation task results of all the models are shown in Table 5 and Table 6. Two prompt
strategies (function-only and function-with-context) are employed for each language version. The
evaluation metrics include AC@1, AC@all, and AC Rate. GPT outperforms the other LLMs, with
the best accepted rate over 66% (GPT4). WizardCoder and StarCoder ranked second and third,
highlighting the significant improvement in code generation capability through code-based fine-
tuning. There is no noticeable performance difference between the Chinese and English versions.
Effects of Contexts in Prompt. As shown in Table 5 and Table 6, providing the context of
function calls for LLM can effectively enhance the accuracy of generating target function
code. Meanwhile, Table 7 shows the proportion of compilable code in each scenario. The majority
of models are capable of generating over 50% of compilable code, which demonstrates the ability
of LLM to adhere to the function prototype. After concatenating the generated function and the
main function, the code that could be compiled can be checked by testcases. Generally, providing
contextual information about the function can assist LLMs in generating compilable code .
Question Category Comparison. Table 8 shows the performance on explicit and narrative questions.
The data is selected from function-only and function-with-context scenarios, with the better-
performing data chosen. The table demonstrates that LLMs perform better on explicit questions
than narrative questions . This is because explicit questions only require atomic code generation
ability of LLMs, while narrative questions require LLM to first understand the question description,
convert natural language into program logic, and finally generate code. It is indeed a significant
challenge to extract program logic from natural language, requiring a deep understanding of
language semantics and context. Meanwhile, three metrics of LLM on narrative questions are
mostly consistent, indicating that the generated code is either entirely correct or entirely incorrect.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 18 ---
111:18Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 5. Performance on AC@1, AC@all, and AC Rate metrics of code generation task in Chinese version.
The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best
performance is wave underlined. General-purpose LLMs that are not able to generate code are removed from
the table.
Code Generation (Chinese)
Function-only Function-with-Context Model
AC@1 AC@all AC Rate AC@1 AC@all AC Rate
ChatGLM-6B 0.2143 0.0924 0.1371 0.1576 0.0693 0.1063
ChatGLM2-6B 0.2143 0.1197 0.1560 0.1996 0.0777 0.1274
ChatGLM3-6B 0.1387 0.1092 0.1330 0.0903 0.0651 0.0854
MOSS-16B* 0.2374 0.1492 0.1879 0.2458 0.1282 0.1770
Chinese-Alpaca-plus-13B 0.2773 0.1387 0.1867 0.2710 0.1366 0.1886
BELLE-7B-1M 0.1639 0.0588 0.0954 0.1513 0.0651 0.0941
BELLE-LLaMA-13B-2M 0.1912 0.0903 0.1283 0.1996 0.0840 0.1274
InternLM-Chat-7B 0.1450 0.0735 0.1039 0.1513 0.0924 0.1128
Baichuan-Chat-13B 0.3130 0.1786 0.2303 0.1723 0.0987 0.1263
WizardCoder-15B:::::0.4496::::::0.2773:::::0.3468:::::0.4328:::::0.2668:::::0.3329
StarCoder 0.2227 0.1366 0.1679 0.1870 0.0924 0.1340
Vicuna-13B 0.2689 0.1261 0.1790 0.3046 0.1492 0.2045
GPT3.5-turbo 0.6429 0.4265 0.5240 0.6660 0.4853 0.5644
GPT4 0.6216 0.4968 0.5411 0.7672 0.6336 0.6659
Human (one-submission) - - - 0.7099 0.7099 0.7099
Human (three-submission) - - - 0.9288 0.9288 0.9288
Human (five-submission) - - - 0.9766 0.9766 0.9766
This may be due to the fact that narrative questions in the dataset are designed for real-world
problem-solving, with relatively low requirements for code time and space complexity.
Human Performance. We extract real student learning records from online platforms to evaluate
human performance in code generation tasks. We calculated the pass rates for human submissions
in n times (n=1, 3, 5), and the average results are shown in the Table 5, Table 6 and Table 8.
Experimental results suggest that human performance can outperform all LLMs, and they
have a better performance on narrative questions. This is mainly due to the ease with which
human programmers can translate between natural language and code languages.
4.4 Code Correction Results
The overall performance of LLMs in the code correction task is shown in Table 9 and 10. There is a
significant difference in debugging ability among different models. GPT-4 performs the best among
all LLMs with the best accuracy of 66%, followed by GPT3.5-turbo, StarCoder and WizardCoder.
Baichuan and ChatGLM3 could correct over 25% erroneous code, which is equipped with code
correction capability. Other models perform poorly in code correction tasks, with a success rate of
less than 20%. The code correction capability of LLMs still has a lot of room for improvement.
Feedback Analysis. Providing error types does not significantly help LLMs in the code
correction task. This may be because the online judge platform only provides three types of
error without specific line numbers or other detailed error messages, which is not sufficient to
assist LLMs in code correction. LLMs that are unable to complete code correction tasks can
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 19 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:19
Table 6. Performance on AC@1, AC@all, and AC Rate metrics of code generation task in English version.
The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best
performance is wave underlined. General-purpose LLMs that are not able to generate code are removed from
the table.
Code Generation (English)
Function-only Function-with-Context Model
AC@1 AC@all AC Rate AC@1 AC@all AC Rate
ChatGLM-6B 0.2080 0.0693 0.1203 0.1828 0.0672 0.1031
ChatGLM2-6B 0.1891 0.0819 0.1243 0.1029 0.0462 0.0668
ChatGLM3-6B 0.1450 0.1176 0.1363 0.1324 0.0987 0.1232
MOSS-16B* 0.2626 0.1513 0.2002 0.1534 0.0882 0.1178
Chinese-Alpaca-plus-13B 0.2878 0.1345 0.1963 0.2542 0.1092 0.1682
BELLE-7B-1M 0.2038 0.0651 0.1161 0.1618 0.0525 0.0863
BELLE-LLaMA-13B-2M 0.1870 0.0777 0.1197 0.2227 0.0861 0.1434
InternLM-Chat-7B 0.0945 0.0525 0.0707 0.3025 0.1597 0.2126
Baichuan-Chat-13B 0.3298 0.1618 0.2310 0.3319 0.1681 0.2273
WizardCoder-15B:::::0.4244::::::0.2500:::::0.3248:::::0.4391:::::0.2752:::::0.3444
StarCoder 0.3382 0.1891 0.2494 0.2794 0.1723 0.2116
Vicuna-13B 0.1870 0.0861 0.1236 0.2983 0.1218 0.1861
GPT3.5-turbo 0.6134 0.4286 0.5105 0.6597 0.4832 0.5606
GPT4 0.5839 0.4623 0.5176 0.7032 0.5705 0.5992
Human (one-submission) - - - 0.7099 0.7099 0.7099
Human (three-submission) - - - 0.9288 0.9288 0.9288
Human (five-submission) - - - 0.9766 0.9766 0.9766
Table 7. Compilable code proportion on each scenario. All models except MOSS-16B* use the completion
pattern to generate code. The best performance of LLMs is in bold, the second-best performance is underlined,
and the third-best performance is wave underlined.
Compilable Code Proportion
ZH EN Model
Function-only Function-with-Context Function-only Function-with-Context
ChatGLM-6B 0.5693 0.5231 0.6429 0.5336
ChatGLM2-6B 0.5105 0.5399 0.5399 0.4034
ChatGLM3-6B 0.6828 0.4601 0.7353 0.6261
MOSS-16B* 0.5231 0.5147 0.6092 0.3739
Chinese-Alpaca-plus-13B 0.7164 0.6744 0.7017 0.6723
BELLE-7B-1M 0.4244 0.4244 0.5273 0.4307
BELLE-LLaMA-13B-2M 0.5105 0.4601 0.4580 0.5357
InternLM-Chat-7B 0.3466 0.4265 0.2017 0.7626
Baichuan-Chat-13B 0.6218 0.3908 0.7605 0.7437
WizardCoder-15B 0.8529:::::0.8634 0.8361:::::0.7899
StarCoder 0.4685 0.4853 0.6765 0.5945
Vicuna-13B 0.7122 0.7983 0.4496 0.7815
GPT3.5-turbo 0.9118 0.8950 0.8929 0.8929
GPT4:::::0.7865 0.9159:::::0.7883 0.8674
exhibit code correction capability after being provided with standard code. This is because
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 20 ---
111:20Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 8. Best performance comparison between explicit and narrative questions on AC@1, AC@all and AC
Rate metric of code generation in English version. The best performance of LLMs is in bold, the second-best
performance is underlined, and the third-best performance is wave underlined.
Code Generation (English)
Explicit Questions Narrative Questions Model
AC@1 AC@all AC Rate AC@1 AC@all AC Rate
ChatGLM-6B 0.2558 0.1008 0.1240 0.1560 0.1560 0.1560
ChatGLM2-6B 0.2326 0.1202 0.1153 0.1376 0.1376 0.1376
ChatGLM3-6B 0.1387 0.1092 0.1330 0.0903 0.0651 0.0854
MOSS-16B* 0.3488 0.2403 0.2562 0.1606 0.1606 0.1606
Chinese-Alpaca-plus-13B 0.3876 0.2054 0.2384 0.1835 0.1835 0.1835
BELLE-7B-1M 0.2674 0.0930 0.1335 0.1330 0.1330 0.1330
BELLE-LLaMA-13B-2M 0.2946 0.1395 0.1933 0.1422 0.1422 0.1422
InternLM-Chat-7B 0.3992 0.2287 0.2502 0.1927 0.1927 0.1927
Baichuan-Chat-13B 0.4147 0.2481 0.2680 0.2523 0.2523 0.2523
Wizard-Coder-15B:::::0.5388:::::0.3488:::::0.3943:::::0.3440:::::0.3440:::::0.3440
StarCoder 0.4264 0.2597 0.2744 0.2385 0.2385 0.2385
Vicuna-13B 0.3953 0.1822 0.2393 0.1881 0.1881 0.1881
GPT3.5-turbo 0.7054 0.5271 0.5770 0.6193 0.6193 0.6193
GPT4 0.7276 0.5837 0.6025 0.6743 0.5550 0.5959
Human (one-submission) 0.6970 0.6970 0.6970 0.7457 0.7457 0.7457
Human (three-submissions) 0.9169 0.9169 0.9169 0.9618 0.9618 0.9618
Human (five-submissions) 0.9731 0.9731 0.9731 0.9865 0.9865 0.9865
Table 9. Performance on AC@1, AC@all, and AC Rate metrics of code generation task in Chinese version.
The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best
performance is wave underlined.
Code Correction (Chinese)
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.1323 0.0519 0.0899 0.1286 0.0511 0.0851 0.1699 0.1566 0.1086
ChatGLM2-6B 0.2549 0.0902 0.1574 0.2316 0.0887 0.1402 0.2022 0.1654 0.1476
ChatGLM3-6B 0.4293 0.2436 0.2944 0.4414 0.2376 0.2918 0.2917 0.2805 0.2220
BELLE-7B-1M 0.0526 0.0226 0.0345 0.0256 0.0060 0.0156 0.0880 0.0496 0.0724
BELLE-LLaMA-13B-2M 0.0647 0.0218 0.0386 0.0496 0.0165 0.0276 0.0429 0.0398 0.0241
InternLM-Chat-7B 0.1150 0.0677 0.0785 0.1271 0.0662 0.0831 0.1474 0.1338 0.1334
Baichuan-Chat-13B 0.3654 0.2534 0.2887 0.3782 0.2556 0.2762 0.2376 0.2083 0.1803
WizardCoder-15B 0.5767::::::0.4331 0.4568 0.5556::::::0.4120 0.4490 0.2556 0.2429 0.1963
StarCoder 0.6639 0.4090:::::0.4869 0.6421 0.3767:::::0.4638 0.4737:::::0.3030 0.3452
GPT3.5-turbo:::::0.5699 0.5466 0.4909:::::0.5722 0.5481 0.5015:::::0.3436 0.3316:::::0.2908
GPT4 0.6970 0.6594 0.6453 0.7143 0.6767 0.6677 0.6038 0.5940 0.5269
the standard code serves as a reference point or template, guiding the model to make corrections
based on the provided code structure and logic.
Error Type Analysis. We compare the performance on three types of error (WA, TLE, and RE) in
Table 11 and Table 12. Results shows correcting these three types of errors has different emergence
ability of LLMs. Correcting wrong answer codes is the hardest task for LLMs since errors might
occur everywhere. TLE usually occurs in closed loops or too many loops, which is relatively easy
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 21 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:21
Table 10. Performance on AC@1, AC@all, and AC Rate metrics of code generation task in English version.
The best performance of LLMs is in bold, the second-best performance is underlined, and the third-best
performance is wave underlined.
Code Correction (English)
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.1489 0.0579 0.0950 0.0128 0.0030 0.0088 0.1241 0.1113 0.1056
ChatGLM2-6B 0.2323 0.0850 0.1284 0.1835 0.0797 0.1023 0.2504 0.2113 0.1943
ChatGLM3-6B 0.3692 0.2248 0.2769 0.3022 0.1609 0.2163 0.2105 0.2075 0.1612
BELLE-7B-1M 0.1609 0.0549 0.0959 0.0023 0.0008 0.0007 0.1158 0.1008 0.0914
BELLE-LLaMA-13B-2M 0.1105 0.0571 0.0795 0.2150 0.0579 0.1307 0.1692 0.1466 0.1142
InternLM-Chat-7B 0.1850 0.1113 0.1289 0.1150 0.0609 0.0757:::::0.2632 0.2436 0.1978
Baichuan-Chat-13B 0.2414 0.1737 0.1963 0.0955 0.0549 0.0582 0.1842 0.1805 0.1398
WizardCoder-15B 0.4504 0.3331 0.3347 0.2917 0.2316 0.2443 0.1571 0.1391 0.1052
StarCoder 0.5992::::::0.3602:::::0.4326 0.5586::::::0.3113 0.4017 0.3602 0.2782 0.2694
GPT3.5-turbo:::::0.5083 0.4925 0.4332:::::0.3632 0.3459:::::0.3161 0.2617:::::0.2594:::::0.2003
GPT4 0.6436 0.6143 0.5775 0.6617 0.6323 0.6022 0.4812 0.4752 0.4032
Table 11. Best performance comparison between correcting three types of code error (WA, TLE, RE) on AC@1,
AC@all and AC Rate metric of code correction task in Chinese version. The best performance of LLMs is in
bold, the second-best performance is underlined, and the third-best performance is wave underlined.
ModelsCode Correction (Chinese)
WA TLE RE
AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate
ChatGLM-6B 0.1736 0.1591 0.1148 0.2609 0.2609 0.1988 0.1512 0.1163 0.0941
ChatGLM2-6B 0.2493 0.1711 0.1525 0.3623 0.1884 0.3854 0.2442 0.1744 0.1487
ChatGLM3-6B 0.4468 0.2805 0.2826 0.4348 0.3768 0.5030 0.3837 0.3372 0.3383
BELLE-7B-1M 0.0894 0.0519 0.0742 0.1159 0.0435 0.1055 0.0465 0.0348 0.0218
BELLE-LLaMA-13B-2M 0.0655 0.0357 0.0379 0.0870 0.0580 0.1034 0.1279 0.1279 0.0750
InternLM-Chat-7B 0.1498 0.1362 0.1371 0.1594 0.1594 0.1116 0.1046 0.0814 0.1214
Baichuan-13B-Chat 0.3728 0.2477 0.2837 0.5362 0.4348 0.5639 0.3256 0.2674 0.2606
WizardCoder-15B:::::0.5864:::::0.4264 0.4487 0.4493:::::0.4058:::::0.5497 0.5698:::::0.5465:::::0.5498
StarCoder 0.6843 0.4162 0.4934:::::0.4783 0.2319 0.4361:::::0.5349 0.4535 0.4188
GPT-3.5-turbo 0.5668 0.5405:::::0.4932 0.4493 0.4348 0.4077 0.7791 0.7791 0.7162
GPT-4 0.6996 0.6612 0.6546 0.8261 0.7391 0.7667 0.8605 0.8488 0.8240
to locate. RE is also easy to locate, usually because the divisor is 0, the subscript is out of bounds,
and so on. Thus when parameter size is limited (StarCoder, GPT3.5), RE and TLE are easy to find,
while the ability to find WA error emergents until the parameter size of GPT4.
5 CONCLUSION
We present CodeApex, a bilingual benchmark focusing on the programming capabilities of LLMs.
First, we evaluate LLMs in the programming comprehension task by assessing their accuracy in
three categories of multiple-choice questions. We design two types of prompts, answer-only and
chain-of-thought, with three in-context learning scenarios, to compare LLMsâ€™ performance. Second,
we evaluate LLMsâ€™ code generation capabilities by testing the acceptance rate of test cases. The test
set includes 476 C++-based coding questions, categorized as either explicit questions or narrative
questions. We also provide a comparison on function-only and function-with-context scenarios, in
order to compare the impact of context information of the target code. Third, we introduce 1332
real-world erroneous code segments to evaluate LLMsâ€™ code correction capability. We compare
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 22 ---
111:22Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 12. Best performance comparison between correcting three types of code error (WA, TLE, RE) on AC@1,
AC@all and AC Rate metric of code correction task in English version. The best performance of LLMs is in
bold, the second-best performance is underlined, and the third-best performance is wave underlined.
ModelsCode Correction (English)
WA TLE RE
AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate
ChatGLM-6B 0.1506 0.1038 0.1078 0.2319 0.2319 0.1379 0.1395 0.1163 0.0941
ChatGLM2-6B 0.2647 0.2213 0.2011 0.2609 0.1159 0.2840 0.2093 0.2093 0.1910
ChatGLM3-6B 0.3745 0.2187 0.2726 0.3188 0.2464 0.3773 0.3372 0.2907 0.2783
BELLE-7B-1M 0.1591 0.1089 0.0968 0.1739 0.0725 0.1318 0.1744 0.1512 0.1596
BELLE-LLaMA-13B-2M 0.2204 0.1404 0.1300 0.2754 0.2464 0.2738 0.1511 0.1511 0.1091
InternLM-Chat-7B 0.2570 0.2383 0.2010 0.3623:::::0.3623 0.1318 0.2674 0.2209 0.1924
Baichuan-13B-Chat 0.2289 0.1821 0.1844 0.3913 0.2754:::::0.4118 0.2907 0.2326 0.2374
WizardCoder 0.4553 0.3277 0.3241 0.3913 0.2754:::::0.4118 0.4302 0.4070 0.3861
StarCoder 0.6136:::::0.3617 0.4349 0.5217 0.2899 0.4178:::::0.5233:::::0.4535:::::0.4052
GPT-3.5-turbo:::::0.4945 0.4783:::::0.4171:::::0.4638 0.4348 0.4178 0.7326 0.7326 0.6958
GPT-4 0.6460 0.6170 0.5837 0.7391 0.6667 0.7079 0.8372 0.8372 0.8213
the performance in code-only, code-with-error message, and code-with-standard code scenarios,
and analysis the result of different code error types. We organize experiments on 12 famous LLMs,
including general-purpose LLMs and specialized models based on code fine-tuning. Currently, GPT
achieves top-tier performance in programming abilities, but human tests indicate that there is still
significant room for improvement. Furthermore, specialized LLMs demonstrate competitiveness
and show their potential for further enhancement.
We also incorporate human performance into the leaderboard, comparing the coding abilities of
the CodeApex with human-level performance. The experiments indicate that the programming
capabilities of LLMs still have some distance from human performance. In the future, the test set
within CodeApex will undergo continuous updates, encompassing an expanded range of question
types and additional programming languages. We hope that CodeApex could provide a reference for
later study on the programming capabilities of LLMs, fostering their development and prosperity
together.
REFERENCES
[1]Daniel K Adiwardana, Minh-Thang Lu, Robert Fergus, David Chen, et al .2020. Meena: Open-domain conversational
agent. arXiv preprint arXiv:2001.09977 (2020).
[2]Uri Alon, Meital Zilberstein, and Omer Levy. 2018. Code2Vec: Learning Distributed Representations of Code. arXiv
preprint arXiv:1803.09473 (2018).
[3]Daman Arora, Himanshu Gaurav Singh, and Mausam. 2023. Have LLMs Advanced Enough? A Challenging Problem
Solving Benchmark For Large Language Models. arXiv:2305.15074 [cs.CL]
[4]Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,
Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models.
CoRR abs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732
[5] B. Inc. Baichuan. 2023. Baichuan 7b & 13b. https://github.com/baichuan-inc.
[6]Mohammad Ehsan Basiri, Shahla Nemati, Moloud Abdar, Erik Cambria, and U Rajendra Acharya. 2021. ABCDM: An
attention-based bidirectional CNN-RNN deep model for sentiment analysis. Future Generation Computer Systems
115 (2021), 279â€“294.
[7]Ananda Basu and et al. 2004. Copilotâ€“a hard real-time runtime monitor. In International Conference onEmbedded
Software. Springer, 213â€“222.
[8]JosÃ© Cambronero, Sumit Gulwani, Vu Le, Daniel Perelman, Arjun Radhakrishna, Clint Simon, and Ashish Tiwari. 2023.
FlashFill++: Scaling Programming by Example by Cutting to the Chase. Proc. ACM Program. Lang. 7, POPL, Article
33 (jan 2023), 30 pages. https://doi.org/10.1145/3571226
[9]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 23 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:23
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,
Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,
Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak,
Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan
Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie
Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.
Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs.LG]
[10] Xinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and Denny Zhou. 2023. Teaching Large Language Models to Self-Debug.
arXiv:2304.05128 [cs.CL]
[11] A Chowdhery, S Narang, J Devlin, et al .2023. Palm: Scaling language modeling with pathways. Journal ofMachine
Learning Research 24, 240 (2023), 1â€“113.
[12] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng
Xiao, Bo Shen, Lin Li, et al .2022. Pangu-coder: Program synthesis with function-level language modeling. arXiv
preprint arXiv:2207.11280 (2022).
[13] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457 [cs.AI]
[14] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry
Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve
Math Word Problems. CoRR abs/2110.14168 (2021). arXiv:2110.14168 https://arxiv.org/abs/2110.14168
[15] Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.
arXiv preprint arXiv:2304.08177 (2023). https://arxiv.org/abs/2304.08177
[16] Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tiea, Yougen Zhou, Yilei Wang,
Aimin Zhou, Ze Zhouand Qin Chen, Jie Zhou, Liang He, and Xipeng Qiu. 2023. EduChat: A Large-Scale Language
Model-based Chatbot System for Intelligent Education. arXiv preprint arXiv:2308.02773 (2023).
[17] Joost CF de Winter. 2023. Can ChatGPT pass high school exams on English language comprehension. Researchgate.
Preprint (2023).
[18] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023. How Ready are Pre-trained Abstractive Models and
LLMs for Legal Case Judgement Summarization? arXiv:2306.01248 [cs.CL]
[19] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language
Model Pretraining with Autoregressive Blank Infilling. In Proceedings ofthe60th Annual Meeting oftheAssociation
forComputational Linguistics (Volume 1:Long Papers). 320â€“335.
[20] Michael C Frank. 2023. Baby steps in evaluating the capacities of large language models. Nature Reviews Psychology
2, 8 (2023), 451â€“452.
[21] Deepayan Ghosal, Koustav Ghosh, Abir Chakraborty, Asif Ekbal, and Pushpak Bhattacharyya. 2020. SAIL: A Situated
Artificial Intelligence Language Model for Sentiment Analysis in Literary Texts. In Proceedings ofthe58th Annual
Meeting oftheAssociation forComputational Linguistics: System Demonstrations. 97â€“104.
[22] Aidan Gilson, Conrad Safraneck, Thomas Huang, Vimig Socrates, Ling Chi, Richard Taylor, and David Chartash.
2023. How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large
Language Models for Medical Education and Knowledge Assessment. JMIR medical education 9 (02 2023), e45312.
https://doi.org/10.2196/45312
[23] Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja KujanpÃ¤Ã¤, and Juha Sorva. 2023. Exploring the
Responses of Large Language Models to Beginner Programmersâ€™ Help Requests. arXiv preprint arXiv:2306.05715
(2023).
[24] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir
Puranik, Horace He, Dawn Song, et al .2021. Measuring coding challenge competence with apps. arXiv preprint
arXiv:2105.09938 (2021).
[25] Linqing Huang, Pengcheng Yin, Wenpeng Guo, Xiaoya Zhang, Mo Yu, and Yinxuan Wang. 2020. CodeBERT: A Pre-
trained Model for Programming and Natural Languages. In Proceedings ofthe58th Annual Meeting oftheAssociation
forComputational Linguistics (ACL).
[26] Rui Huang, Xiaoya Zhang, and Furu Wei. Accessed: 2021. CodeXGLUE. https://github.com/microsoft/CodeXGLUE.
[27] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,
Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese
Evaluation Suite for Foundation Models. arXiv preprint arXiv:2305.08322 (2023).
[28] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2020. CodeSearchNet
Challenge: Evaluating the State of Semantic Code Search. arXiv:1909.09436 [cs.LG]
[29] Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and Xiangang Li. 2023. BELLE: Be Everyoneâ€™s
Large Language model Engine. https://github.com/LianjiaTech/BELLE.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 24 ---
111:24Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
[30] Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. 2023. Exploring
the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases. arXiv
preprint arXiv:2303.14742 (2023).
[31] Adi Lahat, Eyal Shachar, Benjamin Avidan, Zina Shatz, Benjamin S Glicksberg, and Eyal Klang. 2023. Evaluating the
use of large language model in identifying top research questions in gastroenterology. Scientific reports 13, 1 (2023),
4164.
[32] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding Comprehension
Dataset From Examinations. arXiv:1704.04683 [cs.CL]
[33] Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics
doklady 10, 8 (1966), 707â€“710.
[34] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,
Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier
Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, JoÃ£o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel
Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,
Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang,
Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov,
Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan
Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish
Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Sean Hughes,
Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you!
arXiv:2305.06161 [cs.CL]
[35] Y Li, D Choi, J Chung, et al. 2022. Competition-level code generation with alphacode. Science (2022).
[36] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming
Tang, et al .2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint
arXiv:2306.05817 (2023).
[37] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan
Zhang. 2023. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in
recommendation. arXiv preprint arXiv:2308.11131 (2023).
[38] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt
tuning can be comparable to fine-tuning across scales and tasks. In Proceedings ofthe60th Annual Meeting ofthe
Association forComputational Linguistics (Volume 2:Short Papers). 61â€“68.
[39] Yang Liu and Mirella Lapata. 2019. Fine-tune BERT for extractive summarization. In Association forComputational
Linguistics (ACL).
[40] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and
Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct.
[41] Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2023. At Which Training
Stage Does Code Data Help LLMs Reasoning? arXiv preprint arXiv:2309.16298 (2023).
[42] Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016. Convolutional Neural Networks over Tree Structures
for Programming Language Processing. In Proceedings oftheThirtieth AAAI Conference onArtificial Intelligence
(AAAI).
[43] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an LLM to Help
With Code Understanding. arXiv:2307.08177 [cs.SE]
[44] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.
Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474
(2022).
[45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of
Machine Learning Research (JMLR) 21, 140 (2020), 1â€“67.
[46] Michael Robb et al .2021. LLAMA: Leveraging Language Models for Zero-shot Semantic Parsing with Utterance
Re-Writing. arXiv preprint arXiv:2109.02645 (2021).
[47] B Roziere, J Gehring, F Gloeckle, et al .2023. Code llama: Open foundation models for code. arXiv preprint
arXiv:2308.12950 (2023).
[48] Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong
Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui
Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023. MOSS: Training Conversational Language
Models from Synthetic Data. (2023).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 25 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:25
[49] InternLM Team. 2023. InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities. https:
//github.com/InternLM/InternLM.
[50] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Zhiyuan Liu, and Maosong Sun. 2024.
DebugBench: Evaluating Debugging Capability of Large Language Models. arXiv:2401.04621 [cs.SE]
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. Advances inneural information processing systems 30 (2017).
[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. In Advances inNeural Information Processing Systems (NeurIPS).
[53] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained
Encoder-Decoder Models for Code Understanding and Generation. arXiv:2109.00859 [cs.CL]
[54] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,
Denny Zhou, Donald Metzler, et al .2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682
(2022).
[55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al .2022.
Chain-of-thought prompting elicits reasoning in large language models. Advances inNeural Information Processing
Systems 35 (2022), 24824â€“24837.
[56] Kui Xu, Lei Wu, Zhiguang Wang, Fei Tian, Tong Liu, and Compling Pu. 2019. AttentionXML: Label Tree-based
Attention-Aware Deep Model for High-Performance Extreme Multi-label Text Classification. In Proceedings ofthe
25th ACM SIGKDD International Conference onKnowledge Discovery &Data Mining (KDD).
[57] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang,
et al.2024. If llm is the wizard, then code is the wand: A survey on how code empowers large language models to
serve as intelligent agents. arXiv preprint arXiv:2401.00812 (2024).
[58] Pengcheng Yao, Yiding Mao, Zijun Luo, Xiwei Li, and Ling Qin. 2020. Graph2Code: Generating Code Snippets from
Graphical User Interfaces Using Graph Neural Networks. In Proceedings ofthe28th ACM International Conference
onMultimedia (MM).
[59] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie.
2024. CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models. In Proceedings
ofthe46th IEEE/ACM International Conference onSoftware Engineering (<conf-loc>, <city>Lisbon</city>, <coun-
try>Portugal</country>, </conf-loc>) (ICSE â€™24) . Association for Computing Machinery, New York, NY, USA, Article
37, 12 pages. https://doi.org/10.1145/3597503.3623316
[60] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao
Xie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings
ofthe46th IEEE/ACM International Conference onSoftware Engineering. 1â€“12.
[61] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,
Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).
[62] Jingqing Zhang, Yaoqing Yang, Chris Tar, and Jason Baldridge. 2020. PEGASUS: Pre-training with extracted gap-
sentences for abstractive summarization. arXiv preprint arXiv:1912.08777 (2020).
[63] Yizhe Zhang, Eric Sun, Zhiyuan Liu, Xin Li, Minlie Deng, and Yang Ji. 2019. DialoGPT: Large-scale generative
pre-training for conversational response generation. arXiv preprint arXiv:1911.00536 (2019).
[64] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with
MT-Bench and Chatbot Arena. arXiv:2306.05685 [cs.CL]
[65] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li,
Teng Su, Zhilin Yang, and Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual
Evaluations on HumanEval-X. arXiv:2303.17568 [cs.LG]
[66] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. 2022. Docprompting: Generating code by
retrieving the docs. In TheEleventh International Conference onLearning Representations.
ACKNOWLEDGMENT
We would like to thank Qinxiang Cao and Pengfei Liu for helpful discussions. We also thank
Shanghai Boyu Education Technology Co., Ltd for granting us data authorization and providing
valuable support in establishing the testing framework. The work is partially supported by National
Natural Science Foundation of China (62177033).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 26 ---
111:26Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
A BREAKDOWN PERFORMANCE IN PROGRAMMING COMPREHENSION
Table 1, Table 2, Table 3, and Table 4 present the breakdown of performance of LLMs in programming
comprehension task in the 5-shot, 2-shot, and 0-shot scenarios.
Table 1. 5-shot, 2-shot, and 0-shot accuracy results of Chinese version Programming Comprehension Task in
the answer-only setting. C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning,
and multi-hop reasoning questions, respectively. * represents chat-bot mode of the LLM. - indicates that the
model does not have the ability to complete the test in that setting.
Programming Comprehension (Answer-only)-ZH
C.U. C.R. M.H.R. Total Model
5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot
ChatGLM-6B 0.32 0.30 0.32 0.16 0.12 0.26 0.30 0.26 0.31 0.25 0.22 0.30
ChatGLM-6B* 0.31 0.33 0.31 0.27 0.25 0.16 0.25 0.31 0.23 0.28 0.30 0.23
ChatGLM2-6B 0.32 0.33 0.37 0.25 0.27 0.21 0.30 0.28 0.34 0.29 0.30 0.30
ChatGLM2-6B* 0.34 0.31 0.32 0.26 0.26 0.19 0.31 0.31 0.23 0.30 0.29 0.25
ChatGLM3-6B 0.29 0.30 0.34 0.24 0.26 0.24 0.38 0.34 0.34 0.29 0.30 0.30
ChatGLM3-6B* 0.23 0.35 0.46 0.21 0.27 0.19 0.25 0.17 0.28 0.23 0.28 0.31
MOSS-16B* 0.31 0.30 0.28 0.24 0.28 0.34 0.33 0.25 0.32 0.29 0.28 0.31
Chinese-Alpaca-7B 0.21 0.26 0.35 0.27 0.29 0.25 0.18 0.27 0.26 0.23 0.27 0.29
Chinese-Alpaca-7B* 0.24 0.24 0.30 0.29 0.29 0.27 0.21 0.15 0.28 0.25 0.24 0.28
Chinese-Alpaca-plus-7B 0.21 0.19 0.23 0.30 0.34 0.30 0.14 0.16 0.26 0.23 0.24 0.27
Chinese-Alpaca-plus-7B* 0.33 0.20 0.28 0.29 0.30 0.16 0.21 0.25 0.25 0.29 0.25 0.22
Chinese-Alpaca-13B 0.24 0.21 0.26 0.23 0.20 0.29 0.15 0.20 0.26 0.22 0.20 0.27
Chinese-Alpaca-13B* 0.23 0.27 0.27 0.23 0.29 0.24 0.21 0.20 0.18 0.23 0.26 0.24
Chinese-Alpaca-plus-13B 0.27 0.24 0.24 0.34 0.28 0.20 0.21 0.20 0.19 0.28 0.25 0.21
Chinese-Alpaca-plus-13B* 0.27 0.23 0.27 0.27 0.30 0.28 0.13 0.20 0.20 0.23 0.25 0.26
BELLE-7B-1M 0.27 0.26 0.33 0.32 0.25 0.28 0.28 0.25 0.26 0.29 0.25 0.29
BELLE-7B-1M* 0.35 0.31 0.26 0.31 0.29 0.26 0.21 0.21 0.25 0.30 0.28 0.26
BELLE-7B-2M 0.20 0.22 0.24 0.32 0.34 0.26 0.28 0.26 0.19 0.27 0.28 0.24
BELLE-7B-2M* 0.20 0.21 0.20 0.32 0.30 0.25 0.18 0.15 0.28 0.24 0.23 0.24
BELLE-LLaMA-7B-0.6M 0.29 0.24 0.14 0.33 0.34 0.27 0.28 0.32 0.21 0.31 0.30 0.21
BELLE-LLaMA-7B-0.6M* 0.28 0.28 0.28 0.30 0.31 0.31 0.30 0.31 0.28 0.29 0.30 0.29
BELLE-LLaMA-7B-2M 0.17 0.22 0.20 0.30 0.29 0.28 0.16 0.17 0.20 0.22 0.24 0.23
BELLE-LLaMA-7B-2M* 0.19 0.28 0.23 0.21 0.30 0.31 0.09 0.26 0.20 0.17 0.28 0.25
BELLE-LLaMA-13B-2M 0.19 0.21 0.29 0.31 0.31 0.33 0.18 0.19 0.20 0.24 0.25 0.28
BELLE-LLaMA-13B-2M* 0.34 0.24 0.19 0.27 0.29 0.27 0.22 0.21 0.21 0.28 0.25 0.23
InternLM-Chat-7B 0.46 0.42 0.44 0.22 0.27 0.34 0.34 0.33 0.31 0.34 0.34 0.37
Baichuan-7B 0.34 0.24 0.34 0.26 0.24 0.34 0.26 0.25 0.24 0.29 0.25 0.31
EduChat-base-002-7B* 0.19 0.23 0.18 0.31 0.22 0.14 0.23 0.20 0.16 0.25 0.22 0.16
EduChat-base-002-13B* 0.28 0.25 0.17 0.35 0.34 0.03 0.25 0.22 0.13 0.30 0.28 0.10
EduChat-sft-002-7B* 0.22 0.24 0.17 0.19 0.24 0.15 0.25 0.30 0.27 0.21 0.26 0.19
CodeT5-plus-16B 0.16 0.17 0.26 0.28 0.29 0.30 0.32 0.26 0.21 0.25 0.24 0.26
CodeT5-plus-16B* 0.28 0.18 - 0.37 0.26 - 0.27 0.23 - 0.32 0.22 -
CodeT5-plus-6B 0.20 0.19 0.23 0.35 0.35 0.19 0.24 0.23 0.18 0.27 0.26 0.20
CodeT5-plus-6B* 0.23 0.21 - 0.28 0.23 - 0.26 0.18 - 0.26 0.21 -
GPT-3.5-turbo 0.58 0.56 0.61 0.30 0.31 0.33 0.61 0.59 0.57 0.47 0.47 0.49
GPT-3.5-turbo* 0.60 0.57 0.57 0.36 0.35 0.27 0.61 0.64 0.54 0.51 0.50 0.44
GPT-4 0.67 0.64 0.63 0.55 0.54 0.49 0.84 0.82 0.80 0.66 0.64 0.62
GPT-4* 0.70 0.67 0.66 0.55 0.48 0.55 0.82 0.82 0.77 0.67 0.63 0.64
B BREAKDOWN PERFORMANCE IN CODE GENERATION
Table 5 and Table 6 present the breakdown of performance of LLMs in code generation task in the
code-only and code-with-context scenarios. The breakdown performance of explicit questions and
narrative questions is also shown in the table.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 27 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:27
Table 2. 5-shot, 2-shot, and 0-shot accuracy results of Chinese version Programming Comprehension Task in
CoT setting. C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning, and multi-hop
reasoning questions, respectively. * represents chat-bot mode of the LLM. - indicates that the model does not
have the ability to complete the test in that setting.
Programming Comprehension (Chain-of-Thought)-ZH
C.U. C.R. M.H.R. Total Model
5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot
ChatGLM-6B 0.31 0.27 0.22 0.21 0.16 0.18 0.38 0.26 0.23 0.29 0.22 0.21
ChatGLM-6B* 0.33 0.26 0.27 0.20 0.22 0.17 0.18 0.28 0.28 0.24 0.25 0.23
ChatGLM2-6B 0.48 0.36 0.30 0.23 0.25 0.20 0.25 0.31 0.28 0.32 0.30 0.26
ChatGLM2-6B* 0.41 0.32 0.30 0.24 0.29 0.26 0.38 0.23 0.21 0.34 0.29 0.26
ChatGLM3-6B 0.33 0.34 0.36 0.26 0.34 0.24 0.25 0.24 0.22 0.28 0.28 0.28
ChatGLM3-6B* 0.34 0.26 0.35 0.22 0.26 0.23 0.27 0.35 0.22 0.27 0.25 0.27
MOSS-16B* 0.09 0.27 0.27 0.09 0.12 0.17 0.23 0.33 0.20 0.12 0.22 0.21
Chinese-Alpaca-7B 0.28 0.29 0.21 0.25 0.30 0.10 0.20 0.18 0.21 0.25 0.27 0.17
Chinese-Alpaca-7B* 0.28 0.24 0.27 0.24 0.32 0.12 0.16 0.20 0.26 0.24 0.26 0.21
Chinese-Alpaca-plus-7B 0.20 0.37 0.23 0.27 0.31 0.20 0.18 0.28 0.18 0.22 0.32 0.21
Chinese-Alpaca-plus-7B* 0.27 0.27 0.22 0.26 0.24 0.24 0.15 0.20 0.23 0.24 0.24 0.23
Chinese-Alpaca-13B 0.29 0.23 0.27 0.29 0.20 0.20 0.18 0.23 0.25 0.26 0.22 0.24
Chinese-Alpaca-13B* 0.28 0.27 0.31 0.27 0.23 0.17 0.23 0.16 0.13 0.26 0.23 0.21
Chinese-Alpaca-plus-13B 0.24 0.26 0.31 0.25 0.29 0.18 0.13 0.21 0.18 0.22 0.26 0.23
Chinese-Alpaca-plus-13B* 0.29 0.24 0.36 0.25 0.24 0.27 0.15 0.23 0.23 0.24 0.24 0.29
BELLE-7B-1M 0.33 0.30 0.26 0.23 0.29 0.19 0.25 0.20 0.18 0.27 0.27 0.21
BELLE-7B-1M* 0.33 0.28 0.18 0.22 0.25 0.10 0.21 0.16 0.15 0.26 0.24 0.14
BELLE-7B-2M 0.20 0.20 0.29 0.24 0.26 0.18 0.23 0.18 0.11 0.22 0.22 0.20
BELLE-7B-2M* 0.26 0.20 0.27 0.23 0.24 0.23 0.15 0.21 0.21 0.22 0.22 0.24
BELLE-LLaMA-7B-0.6M 0.27 0.23 0.23 0.21 0.30 0.28 0.16 0.30 0.18 0.22 0.28 0.24
BELLE-LLaMA-7B-0.6M* 0.28 0.28 0.34 0.20 0.36 0.28 0.21 0.31 0.20 0.23 0.32 0.28
BELLE-LLaMA-7B-2M 0.24 0.19 0.12 0.30 0.19 0.14 0.23 0.21 0.15 0.26 0.20 0.14
BELLE-LLaMA-7B-2M* 0.28 0.28 0.17 0.26 0.30 0.12 0.25 0.25 0.13 0.26 0.28 0.14
BELLE-LLaMA-13B-2M 0.22 0.29 0.20 0.19 0.27 0.19 0.20 0.18 0.16 0.20 0.26 0.19
BELLE-LLaMA-13B-2M* 0.21 0.27 0.17 0.22 0.28 0.15 0.18 0.21 0.11 0.21 0.26 0.15
InternLM-Chat-7B 0.36 0.32 0.34 0.18 0.18 0.14 0.36 0.20 0.23 0.29 0.24 0.24
Baichuan-7B 0.04 0.06 0.07 0.11 0.02 0.04 0.05 0.02 0.07 0.07 0.03 0.06
EduChat-base-002-7B* 0.28 0.27 0.11 0.23 0.22 0.21 0.18 0.26 0.15 0.24 0.25 0.16
EduChat-base-002-13B* 0.30 0.29 0.18 0.23 0.24 0.18 0.18 0.33 0.16 0.24 0.28 0.18
EduChat-sft-002-7B* 0.31 0.24 0.27 0.20 0.21 0.19 0.25 0.23 0.26 0.25 0.23 0.24
CodeT5-plus-16B - - - - - - - - - - - -
CodeT5-plus-16B* - - - - - - - - - - - -
CodeT5-plus-6B - - - - - - - - - - - -
CodeT5-plus-6B* - - - - - - - - - - - -
GPT-3.5-turbo 0.59 0.55 0.50 0.37 0.41 0.35 0.68 0.63 0.52 0.53 0.52 0.45
GPT-3.5-turbo* 0.57 0.56 0.54 0.39 0.42 0.33 0.65 0.57 0.56 0.52 0.51 0.46
GPT-4 0.71 0.71 0.63 0.52 0.52 0.51 0.66 0.66 0.62
GPT-4* 0.72 0.68 0.57 0.59 0.52 0.45 0.84 0.85 0.79 0.70 0.66 0.58
C BREAKDOWN PERFORMANCE IN CODE CORRECTION
Table 7, 9, 11, 8, 10, and 12 present the breakdown of performance of LLMs in code correction task
with wrong answer error, time limited error and runtime error. The breakdown performance of
code-only, code-with-error message and code-with-standard code scenarios are also shown in the
table.
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 28 ---
111:28Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 3. 5-shot, 2-shot, and 0-shot accuracy results of English version Programming Comprehension Task in
the answer-only setting. C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning,
and multi-hop reasoning questions, respectively. * represents chat-bot mode of the LLM. - indicates that the
model does not have the ability to complete the test in that setting.
Programming Comprehension (Answer-Only)-EN
C.U. C.R. M.H.R. Total Model
5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot
ChatGLM-6B 0.32 0.34 0.36 0.16 0.21 0.24 0.30 0.31 0.30 0.29 0.28 0.30
ChatGLM-6B* 0.32 0.25 0.39 0.27 0.24 0.23 0.25 0.38 0.30 0.32 0.28 0.23
ChatGLM2-6B 0.34 0.33 0.41 0.25 0.26 0.32 0.30 0.30 0.23 0.29 0.30 0.30
ChatGLM2-6B* 0.36 0.32 0.28 0.26 0.28 0.23 0.31 0.28 0.21 0.30 0.30 0.25
ChatGLM3-6B 0.31 0.31 0.37 0.33 0.32 0.30 0.28 0.30 0.30 0.31 0.31 0.32
ChatGLM3-6B* 0.24 0.30 0.34 0.23 0.17 0.20 0.23 0.19 0.30 0.23 0.22 0.27
MOSS-16B* 0.26 0.25 0.26 0.24 0.27 0.35 0.33 0.33 0.15 0.28 0.28 0.31
Chinese-Alpaca-7B 0.31 0.30 0.28 0.27 0.26 0.25 0.18 0.21 0.33 0.29 0.26 0.29
Chinese-Alpaca-7B* 0.17 0.26 0.29 0.29 0.29 0.19 0.21 0.16 0.23 0.17 0.25 0.28
Chinese-Alpaca-plus-7B 0.34 0.24 0.28 0.30 0.29 0.28 0.14 0.22 0.33 0.30 0.26 0.27
Chinese-Alpaca-plus-7B* 0.24 0.26 0.21 0.29 0.21 0.20 0.21 0.20 0.23 0.26 0.23 0.22
Chinese-Alpaca-13B 0.26 0.20 0.31 0.23 0.21 0.23 0.15 0.16 0.21 0.23 0.20 0.27
Chinese-Alpaca-13B* 0.10 0.23 0.28 0.23 0.28 0.21 0.21 0.25 0.16 0.10 0.25 0.24
Chinese-Alpaca-plus-13B 0.24 0.21 0.24 0.34 0.36 0.27 0.21 0.26 0.31 0.28 0.28 0.21
Chinese-Alpaca-plus-13B* 0.21 0.27 0.29 0.27 0.36 0.19 0.13 0.25 0.22 0.26 0.30 0.26
BELLE-7B-1M 0.33 0.32 0.33 0.32 0.31 0.26 0.28 0.28 0.23 0.31 0.31 0.29
BELLE-7B-1M* 0.39 0.36 0.33 0.31 0.27 0.28 0.21 0.23 0.24 0.30 0.29 0.26
BELLE-7B-2M 0.26 0.24 0.24 0.32 0.26 0.30 0.28 0.20 0.26 0.26 0.24 0.24
BELLE-7B-2M* 0.23 0.21 0.27 0.32 0.23 0.19 0.18 0.20 0.25 0.23 0.22 0.24
BELLE-LLaMA-7B-0.6M 0.24 0.32 0.29 0.33 0.23 0.27 0.28 0.33 0.20 0.26 0.29 0.21
BELLE-LLaMA-7B-0.6M* 0.27 0.31 0.34 0.30 0.28 0.27 0.30 0.31 0.25 0.29 0.30 0.29
BELLE-LLaMA-7B-2M 0.30 0.28 0.21 0.30 0.27 0.33 0.16 0.14 0.26 0.27 0.24 0.23
BELLE-LLaMA-7B-2M* 0.28 0.26 0.27 0.21 0.32 0.29 0.09 0.26 0.25 0.26 0.28 0.25
BELLE-LLaMA-13B-2M 0.18 0.20 0.22 0.31 0.22 0.25 0.18 0.18 0.21 0.24 0.20 0.28
BELLE-LLaMA-13B-2M* 0.24 0.27 0.26 0.27 0.27 0.23 0.22 0.20 0.15 0.27 0.25 0.23
InternLM-Chat-7B 0.43 0.41 0.44 0.22 0.33 0.36 0.34 0.26 0.31 0.37 0.34 0.37
Baichuan-7B 0.26 0.29 0.27 0.26 0.27 0.31 0.26 0.27 0.30 0.22 0.28 0.31
EduChat-base-002-7B* 0.33 0.24 0.26 0.31 0.34 0.25 0.23 0.31 0.33 0.31 0.30 0.16
EduChat-base-002-13B* 0.37 0.30 0.21 0.35 0.36 0.12 0.25 0.25 0.18 0.33 0.31 0.10
EduChat-sft-002-7B* 0.29 0.26 0.23 0.19 0.34 0.25 0.25 0.25 0.15 0.29 0.29 0.19
CodeT5-plus-16B 0.19 0.20 0.21 0.28 0.30 0.29 0.32 0.24 0.22 0.25 0.25 0.26
CodeT5-plus-16B* 0.20 0.20 - 0.37 0.27 - 0.27 0.28 - 0.24 0.25 -
CodeT5-plus-6B 0.28 0.27 0.24 0.35 0.29 0.19 0.24 0.29 0.16 0.32 0.28 0.20
CodeT5-plus-6B* 0.26 0.27 - 0.28 0.28 - 0.26 0.25 - 0.30 0.27 -
GPT3.5-turbo 0.51 0.51 0.52 0.30 0.30 0.35 0.61 0.55 0.51 0.42 0.44 0.49
GPT3.5-turbo* 0.53 0.52 0.53 0.36 0.32 0.25 0.61 0.50 0.55 0.44 0.44 0.44
GPT-4 0.62 0.64 0.63 0.48 0.44 0.48 0.75 0.75 0.80 0.60 0.59 0.62
GPT-4* 0.61 0.62 0.62 0.48 0.42 0.48 0.82 0.77 0.80 0.61 0.58 0.61
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 29 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:29
Table 4. 5-shot, 2-shot, and 0-shot accuracy results of English version Programming Comprehension Task in
CoT setting. C.U., C.R, and M.H.R indicate conceptual understanding, commonsense reasoning, and multi-hop
reasoning questions, respectively. * represents chat-bot mode of the LLM. - indicates that the model does not
have the ability to complete the test in that setting.
Programming Comprehension(Chain-of-Thought)-EN
C.U. C.R. M.H.R. Total Model
5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot 5-shot 2-shot 0-shot
ChatGLM-6B 0.31 0.34 0.23 0.16 0.16 0.18 0.25 0.31 0.26 0.24 0.26 0.22
ChatGLM-6B* 0.28 0.29 0.20 0.25 0.17 0.25 0.31 0.28 0.28 0.28 0.24 0.24
ChatGLM2-6B 0.37 0.36 0.33 0.25 0.24 0.15 0.23 0.30 0.21 0.29 0.30 0.23
ChatGLM2-6B* 0.30 0.29 0.40 0.25 0.23 0.25 0.23 0.28 0.21 0.26 0.26 0.30
ChatGLM3-6B 0.35 0.33 0.40 0.20 0.30 0.17 0.28 0.21 0.27 0.27 0.29 0.27
ChatGLM3-6B* 0.34 0.28 0.31 0.21 0.21 0.20 0.27 0.19 0.28 0.27 0.23 0.26
MOSS-16B* 0.13 0.27 0.26 0.10 0.19 0.20 0.28 0.26 0.18 0.16 0.24 0.22
Chinese-Alpaca-7B 0.23 0.26 0.23 0.33 0.30 0.14 0.13 0.21 0.13 0.25 0.26 0.17
Chinese-Alpaca-7B* 0.23 0.27 0.22 0.30 0.32 0.14 0.11 0.23 0.10 0.23 0.28 0.16
Chinese-Alpaca-plus-7B 0.20 0.27 0.28 0.23 0.30 0.09 0.28 0.30 0.13 0.23 0.29 0.17
Chinese-Alpaca-plus-7B* 0.22 0.26 0.28 0.26 0.18 0.20 0.26 0.21 0.16 0.25 0.22 0.22
Chinese-Alpaca-13B 0.31 0.26 0.26 0.23 0.21 0.14 0.20 0.23 0.20 0.25 0.23 0.20
Chinese-Alpaca-13B* 0.20 0.24 0.22 0.21 0.16 0.19 0.18 0.26 0.21 0.20 0.22 0.21
Chinese-Alpaca-plus-13B 0.24 0.31 0.24 0.22 0.19 0.25 0.30 0.28 0.10 0.25 0.26 0.21
Chinese-Alpaca-plus-13B* 0.30 0.30 0.21 0.22 0.15 0.20 0.25 0.30 0.18 0.26 0.24 0.20
BELLE-7B-1M 0.23 0.17 0.24 0.22 0.16 0.18 0.16 0.08 0.21 0.21 0.14 0.21
BELLE-7B-1M* 0.21 0.19 0.12 0.19 0.20 0.11 0.11 0.15 0.07 0.18 0.18 0.10
BELLE-7B-2M 0.13 0.14 0.26 0.22 0.19 0.20 0.11 0.13 0.13 0.16 0.16 0.20
BELLE-7B-2M* 0.19 0.19 0.14 0.22 0.22 0.08 0.13 0.07 0.07 0.19 0.17 0.10
BELLE-LLaMA-7B-0.6M 0.22 0.20 0.08 0.22 0.23 0.13 0.26 0.18 0.03 0.23 0.21 0.09
BELLE-LLaMA-7B-0.6M* 0.33 0.22 0.20 0.19 0.21 0.24 0.26 0.21 0.25 0.26 0.22 0.23
BELLE-LLaMA-7B-2M 0.22 0.07 0.09 0.21 0.08 0.10 0.10 0.05 0.13 0.19 0.07 0.10
BELLE-LLaMA-7B-2M* 0.21 0.11 0.21 0.20 0.11 0.15 0.13 0.03 0.20 0.19 0.09 0.18
BELLE-LLaMA-13B-2M 0.26 0.21 0.11 0.21 0.22 0.13 0.15 0.13 0.20 0.21 0.20 0.14
BELLE-LLaMA-13B-2M* 0.28 0.21 0.16 0.17 0.10 0.09 0.18 0.11 0.10 0.21 0.14 0.12
InternLM-Chat-7B 0.42 0.32 0.37 0.25 0.20 0.21 0.26 0.25 0.25 0.32 0.26 0.28
Baichuan-7B 0.12 0.10 0.08 0.09 0.11 0.02 0.08 0.02 0.30 0.10 0.08 0.04
EduChat-base-002-7B* 0.24 0.26 0.13 0.19 0.24 0.12 0.26 0.20 0.15 0.23 0.24 0.13
EduChat-base-002-13B* 0.31 0.24 0.12 0.24 0.22 0.12 0.25 0.28 0.13 0.27 0.24 0.12
EduChat-sft-002-7B* 0.29 0.31 0.24 0.22 0.20 0.17 0.26 0.25 0.15 0.26 0.25 0.19
CodeT5-plus-16B - - - - - - - - - - - -
CodeT5-plus-16B* - - - - - - - - - - - -
CodeT5-plus-6B - - - - - - - - - - - -
CodeT5-plus-6B* - - - - - - - - - - - -
GPT3.5-turbo 0.44 0.49 0.53 0.34 0.35 0.37 0.64 0.61 0.57 0.45 0.46 0.47
GPT3.5-turbo* 0.51 0.50 0.47 0.37 0.34 0.33 0.63 0.61 0.50 0.49 0.47 0.42
GPT-4 0.60 0.64 0.62 0.51 0.51 0.51 0.87 0.82 0.75 0.63 0.63 0.61
GPT-4* 0.61 0.63 0.66 0.52 0.52 0.49 0.84 0.79 0.75 0.63 0.62 0.62
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 30 ---
111:30Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 5. Best performance comparison between explicit questions and narrative questions on AC@1, AC@all
and AC Rate metric of code generation in Chinese version.
Code Generation-ZH
Code-only Code-with-Context
Explicit Questions Narrative Questions Explicit Questions Narrative Questions Model
AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate
baichuan 0.42 0.26 0.29 0.18 0.18 0.18 0.24 0.16 0.16 0.09 0.09 0.09
BELLE 0.27 0.15 0.16 0.10 0.10 0.10 0.27 0.14 0.15 0.12 0.12 0.12
BELLE-7B-1M 0.20 0.09 0.10 0.12 0.12 0.12 0.19 0.10 0.11 0.10 0.10 0.10
chatglm 0.27 0.14 0.17 0.15 0.15 0.15 0.21 0.11 0.12 0.10 0.10 0.10
chatglm2 0.29 0.17 0.20 0.12 0.12 0.12 0.27 0.12 0.20 0.12 0.12 0.12
cn-alpaca-plus-13B 0.36 0.20 0.22 0.19 0.19 0.19 0.37 0.21 0.24 0.16 0.16 0.16
gpt3.5-turbo 0.70 0.48 0.57 0.58 0.58 0.58 0.72 0.52 0.60 0.61 0.61 0.61
gpt-4 0.66 0.53 0.57 0.57 0.46 0.51 0.77 0.64 0.67 0.77 0.62 0.66
intern 0.19 0.11 0.11 0.09 0.09 0.09 0.21 0.14 0.13 0.08 0.08 0.08
moss 0.34 0.22 0.23 0.11 0.11 0.11 0.34 0.20 0.24 0.14 0.14 0.14
starcoder 0.28 0.18 0.21 0.16 0.16 0.16 0.23 0.12 0.15 0.15 0.15 0.15
vicuna 0.35 0.18 0.23 0.17 0.17 0.17 0.37 0.21 0.26 0.23 0.23 0.23
wizard 0.54 0.35 0.37 0.34 0.34 0.34 0.53 0.36 0.39 0.32 0.32 0.32
Table 6. Best performance comparison between explicit questions and narrative questions on AC@1, AC@all
and AC Rate metric of code generation in English version.
Code Generation-EN
Code-only Code-with-Context
Explicit Questions Narrative Questions Explicit Questions Narrative Questions Model
AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate AC@1 AC@all AC Rate
baichuan 0.41 0.23 0.27 0.24 0.24 0.24 0.40 0.25 0.25 0.25 0.25 0.25
BELLE 0.28 0.13 0.16 0.08 0.08 0.08 0.29 0.14 0.19 0.14 0.14 0.14
BELLE-7B-1M 0.27 0.09 0.13 0.13 0.13 0.13 0.19 0.08 0.10 0.12 0.12 0.12
chatglm 0.26 0.10 0.12 0.16 0.16 0.16 0.22 0.09 0.11 0.14 0.14 0.14
chatglm2 0.23 0.12 0.12 0.14 0.14 0.14 0.14 0.07 0.07 0.06 0.06 0.06
cn-alpaca-plus-13B 0.39 0.21 0.24 0.17 0.17 0.17 0.32 0.17 0.20 0.18 0.18 0.18
gpt3.5-turbo 0.69 0.50 0.56 0.53 0.53 0.53 0.71 0.53 0.58 0.62 0.62 0.62
gpt-4 0.60 0.47 0.52 0.56 0.45 0.51 0.73 0.58 0.60 0.67 0.56 0.60
intern 0.14 0.09 0.08 0.04 0.04 0.04 0.40 0.23 0.25 0.19 0.19 0.19
moss 0.35 0.24 0.26 0.16 0.16 0.16 0.22 0.13 0.15 0.08 0.08 0.08
starcoder 0.43 0.26 0.27 0.24 0.24 0.24 0.35 0.23 0.24 0.19 0.19 0.19
vicuna 0.25 0.12 0.14 0.11 0.11 0.11 0.40 0.18 0.24 0.19 0.19 0.19
wizard 0.54 0.34 0.39 0.29 0.29 0.29 0.52 0.35 0.39 0.34 0.34 0.34
Table 7. Performance on AC@1, AC@all, and AC Rate metrics of code correction task (Wrong Answer Error)
in Chinese version.
Code Correction (Wrong Answer)-ZH
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.1362 0.0511 0.0906 0.1243 0.0468 0.0814 0.1736 0.1591 0.1148
ChatGLM2-6B 0.2493 0.0791 0.1482 0.2315 0.0851 0.1361 0.2119 0.1711 0.1525
ChatGLM3-6B 0.4323 0.2298 0.2826 0.4468 0.2230 0.2798 0.2917 0.2805 0.2220
BELLE-7B-1M 0.0553 0.0221 0.0346 0.0272 0.0051 0.0155 0.0894 0.0519 0.0742
BELLE-LLaMA-13B-2M 0.0655 0.0196 0.0379 0.0494 0.0136 0.0253 0.0391 0.0357 0.0219
InternLM-Chat-7B 0.1234 0.0706 0.0832 0.1362 0.0681 0.0859 0.1498 0.1362 0.1371
Baichuan-Chat-13B 0.3643 0.2477 0.2837 0.3728 0.2443 0.2649 0.2400 0.2102 0.1890
WizardCoder-15B 0.5864 0.4264 0.4487 0.5626 0.4034 0.4387 0.2511 0.2383 0.2000
StarCoder 0.6843 0.4162 0.4934 0.6621 0.3821 0.4691 0.4757 0.3047 0.3447
GPT3.5-turbo 0.5643 0.5396 0.4818 0.5668 0.5405 0.4932 0.3404 0.3268 0.2916
GPT4 0.6774 0.6417 0.6287 0.6996 0.6612 0.6546 0.5838 0.5728 0.5152
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 31 ---
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models 111:31
Table 8. Performance on AC@1, AC@all, and AC Rate metrics of code correction task (Wrong Answer Error)
in English version.
Code Correction (Wrong Answer)-EN
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.1506 0.0562 0.0950 0.0136 0.0034 0.0093 0.1174 0.1038 0.1078
ChatGLM2-6B 0.2332 0.0791 0.1196 0.1872 0.0765 0.1018 0.2647 0.2213 0.2011
ChatGLM3-6B 0.3745 0.2187 0.2726 0.3123 0.1583 0.2161 0.2145 0.2111 0.1622
BELLE-7B-1M 0.1591 0.0468 0.0903 0.0026 0.0009 0.0008 0.1260 0.1089 0.0968
BELLE-LLaMA-13B-2M 0.1123 0.0553 0.0785 0.2204 0.0553 0.1300 0.1660 0.1404 0.1161
InternLM-Chat-7B 0.1864 0.1038 0.1269 0.1209 0.0604 0.0768 0.2570 0.2383 0.2010
Baichuan-Chat-13B 0.2289 0.1634 0.1844 0.0970 0.0553 0.0587 0.1864 0.1821 0.1451
WizardCoder-15B 0.4553 0.3277 0.3241 0.3081 0.2417 0.2535 0.1498 0.1311 0.1018
StarCoder 0.6136 0.3617 0.4349 0.5770 0.3183 0.4079 0.3566 0.2774 0.2660
GPT3.5-turbo 0.4945 0.4783 0.4171 0.3609 0.3430 0.3144 0.2536 0.2511 0.2020
GPT4 0.6247 0.5957 0.5581 0.6460 0.6170 0.5837 0.4664 0.4596 0.3949
Table 9. Performance on AC@1, AC@all, and AC Rate metrics of code correction task (Time Limit Exceeded
Error) in Chinese version.
Code Correction (CPU Timeout)-ZH
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.1159 0.0870 0.1359 0.1739 0.1304 0.1988 0.2609 0.2609 0.0913
ChatGLM2-6B 0.3623 0.1884 0.3854 0.3043 0.1449 0.2941 0.0580 0.0580 0.0487
ChatGLM3-6B 0.4348 0.3623 0.5030 0.4203 0.3623 0.5010 0.3768 0.3768 0.1136
BELLE-7B-1M 0.0580 0.0435 0.0730 0.0145 0.0145 0.0203 0.1159 0.0290 0.1055
BELLE-LLaMA-13B-2M 0.0870 0.0580 0.0913 0.0870 0.0580 0.1034 0.0000 0.00000 0.0000
InternLM-Chat-7B 0.0725 0.0580 0.0345 0.1014 0.0870 0.1116 0.1594 0.1594 0.0649
Baichuan-Chat-13B 0.4783 0.3768 0.4665 0.5362 0.4348 0.5639 0.3188 0.2899 0.1075
WizardCoder-15B 0.4203 0.4058 0.5071 0.4493 0.4058 0.5497 0.3188 0.3043 0.0933
StarCoder 0.4783 0.2319 0.4361 0.4638 0.2173 0.4260 0.4783 0.1739 0.3327
GPT3.5-turbo 0.4058 0.3768 0.3692 0.4493 0.4348 0.4077 0.3913 0.3913 0.2252
GPT4 0.8261 0.7246 0.7667 0.8116 0.7391 0.7565 0.7391 0.7391 0.5761
Table 10. Performance on AC@1, AC@all, and AC Rate metrics of code correction task (Time Limit Exceeded
Error) in English version.
Code Correction(CPU Timeout)-EN
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.1304 0.0870 0.1379 0.0000 0.0000 0.0000 0.2319 0.2319 0.0730
ChatGLM2-6B 0.2609 0.1159 0.2840 0.0870 0.0290 0.0446 0.0580 0.0435 0.0406
ChatGLM3-6B 0.3188 0.2464 0.3773 0.2319 0.1884 0.2515 0.1304 0.1304 0.0933
BELLE-7B-1M 0.1739 0.0725 0.1318 0.0000 0.0000 0.0000 0.0290 0.0290 0.0406
BELLE-LLaMA-13B-2M 0.1449 0.1014 0.1643 0.2754 0.1159 0.2738 0.2464 0.2464 0.0771
InternLM-Chat-7B 0.1304 0.1159 0.1318 0.0435 0.0290 0.0730 0.3623 0.3623 0.1318
Baichuan-Chat-13B 0.3913 0.2754 0.4118 0.1159 0.0870 0.0994 0.1884 0.1884 0.0325
WizardCoder-15B 0.3913 0.3333 0.5051 0.1304 0.1304 0.1846 0.2319 0.2319 0.0913
StarCoder 0.4493 0.2174 0.4178 0.4058 0.1304 0.3448 0.5217 0.2899 0.3124
GPT3.5-turbo 0.4638 0.4348 0.4178 0.4058 0.3768 0.2860 0.3623 0.3623 0.0933
GPT4 0.7391 0.6667 0.6815 0.7101 0.6377 0.7079 0.5072 0.5072 0.3144
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 32 ---
111:32Lingyue Fu, Huacan Chai, Kounianhua Du, Weiming Zhang, Shuang Luo, Jianghao Lin, Yuchen Fang, Renting Rui, Hao
Guan, Jianxing Liu, Siyuan Qi, Longteng Fan, Jiayi Lei, Yifan Liu, Jingkuan Wang, Kangning Zhang, Weinan Zhang,
and Yong Yu
Table 11. Performance on AC@1, AC@all, and AC Rate metrics of code correction task (Runtime Error) in
Chinese version.
Code Correction(Runtime Error)-ZH
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.0930 0.0347 0.0477 0.1512 0.0465 0.0682 0.1279 0.1163 0.0941
ChatGLM2-6B 0.2442 0.1628 0.1487 0.1744 0.0930 0.1010 0.1860 0.1744 0.1378
ChatGLM3-6B 0.3837 0.3372 0.3383 0.3837 0.3372 0.3383 0.2907 0.2791 0.2278
BELLE-7B-1M 0.0116 0.0116 0.0068 0.0116 0.0116 0.0136 0.0465 0.0348 0.0218
BELLE-LLaMA-13B-2M 0.0349 0.0233 0.0150 0.0233 0.0233 0.0136 0.1279 0.1279 0.0750
InternLM-Chat-7B 0.0349 0.0349 0.0341 0.0233 0.0233 0.0205 0.1046 0.0814 0.1214
Baichuan-Chat-13B 0.2907 0.2326 0.2483 0.3256 0.2674 0.2606 0.1395 0.1163 0.0928
WizardCoder-15B 0.5698 0.5465 0.5498 0.5465 0.5349 0.5416 0.2674 0.2558 0.2087
StarCoder 0.5349 0.4535 0.4188 0.5116 0.4302 0.4052 0.4419 0.3837 0.3615
GPT3.5-turbo 0.7791 0.7791 0.7162 0.7442 0.7442 0.6958 0.3488 0.3488 0.3220
GPT4 0.8605 0.8488 0.8240 0.8372 0.8372 0.8131 0.7674 0.7674 0.6767
Table 12. Performance on AC@1, AC@all, and AC Rate metrics of code correction task (Runtime Error) in
English version.
Code Correction(Runtime Error)-EN
Code-only Code-with-Error Message Code-with-Standard Code Model
AC@1 AC@n AC Rate AC@1 AC@n AC Rate AC@1 AC@n AC Rate
ChatGLM-6B 0.1395 0.0581 0.0655 0.0116 0.0000 0.0068 0.1279 0.1163 0.0941
ChatGLM2-6B 0.1977 0.1395 0.1610 0.2093 0.1628 0.1487 0.2093 0.2093 0.1910
ChatGLM3-6B 0.3372 0.2907 0.2783 0.2209 0.1744 0.1951 0.2209 0.2209 0.1910
BELLE-7B-1M 0.1744 0.1512 0.1596 0.0000 0.0000 0.0000 0.0465 0.0465 0.0409
BELLE-LLaMA-13B-2M 0.0581 0.0465 0.0382 0.0930 0.0465 0.0450 0.1511 0.1511 0.1091
InternLM-Chat-7B 0.2093 0.2093 0.1569 0.0930 0.0930 0.0614 0.2674 0.2209 0.1924
Baichuan-Chat-13B 0.2907 0.2326 0.2374 0.0581 0.0233 0.0231 0.1512 0.1512 0.1296
WizardCoder-15B 0.4302 0.4070 0.3861 0.1977 0.1744 0.1405 0.1977 0.1744 0.1692
StarCoder 0.5233 0.4535 0.4052 0.4302 0.3605 0.3424 0.2791 0.2791 0.2933
GPT3.5-turbo 0.7326 0.7326 0.6958 0.3605 0.3605 0.3629 0.2907 0.2906 0.2469
GPT4 0.8256 0.8256 0.8117 0.8372 0.8372 0.8213 0.6628 0.6628 0.5935
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
