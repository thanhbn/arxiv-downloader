# PersianMind: Mô hình Ngôn ngữ Lớn Đa ngôn ngữ Ba Tư-Anh

Pedram Rostami, Ali Salemi, và Mohammad Javad Dousti
{pedram.rostami, alisalemi, mjdousti}@ut.ac.ir
Đại học Tehran

## Tóm tắt

Các mô hình ngôn ngữ lớn thể hiện khả năng đáng chú ý trong nhiều tác vụ ngôn ngữ khác nhau và có kiến thức rộng rãi trên các lĩnh vực khác nhau. Mặc dù chúng hoạt động tốt nhất bằng tiếng Anh, khả năng của chúng trong các ngôn ngữ khác cũng đáng chú ý. Ngược lại, các mô hình mã nguồn mở như LLaMa chủ yếu được huấn luyện trên các tập dữ liệu tiếng Anh, dẫn đến hiệu suất kém trong các ngôn ngữ khác không phải tiếng Anh. Trong bài báo này, chúng tôi giới thiệu PersianMind, một mô hình ngôn ngữ lớn song ngữ mã nguồn mở thể hiện hiệu suất tương đương với GPT-3.5-turbo mã nguồn đóng trong tiếng Ba Tư. Bằng cách mở rộng từ vựng của LLaMa2 với 10.000 token tiếng Ba Tư và huấn luyện nó trên tập dữ liệu gồm gần 2 tỷ token tiếng Ba Tư, chúng tôi chứng minh rằng phương pháp của chúng tôi bảo tồn kiến thức tiếng Anh của mô hình và sử dụng học chuyển giao để xuất sắc trong việc chuyển giao kiến thức tác vụ từ ngôn ngữ này sang ngôn ngữ khác.

**Từ khóa**: Mô hình Ngôn ngữ Lớn, LLaMa, Ngôn ngữ Ba Tư

## 1. Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) đã là phát triển quan trọng nhất trong lĩnh vực xử lý ngôn ngữ tự nhiên trong những năm gần đây, tái giới thiệu khái niệm sử dụng một mô hình như trí tuệ nhân tạo tổng quát (AGI) (Bubeck et al., 2023). Các mô hình chỉ giải mã dựa trên transformer này (Vaswani et al., 2017) được phân biệt bởi quy mô đáng kể và việc huấn luyện trên các tập dữ liệu văn bản rộng lớn. LLM là công cụ đa năng cho các tác vụ ngôn ngữ khác nhau trong xử lý ngôn ngữ tự nhiên. Hơn nữa, có những trường hợp chatbot dựa trên LLM có thể thay thế các hệ thống truy xuất thông tin truyền thống (Zhao et al., 2023). Khi kết hợp với khả năng đa phương thức, chúng cũng có thể được sử dụng trong thị giác máy tính, như được chứng minh bởi sự xuất hiện của các chatbot trực quan (C. Wu et al., 2023).

Các LLM thương mại nổi bật như ChatGPT (OpenAI, 2022), GPT-4 (OpenAI et al., 2023), PaLM2 (Anil et al., 2023), và Claude (Anthropic, 2023) thể hiện hiệu suất xuất sắc trên nhiều tác vụ đa dạng, bao gồm tạo văn bản, tóm tắt, và tạo mã. Ngoài ra, các mô hình này thể hiện kết quả hứa hẹn khi áp dụng cho các ngôn ngữ khác không phải tiếng Anh. Tuy nhiên, điều quan trọng cần lưu ý là các LLM này là độc quyền và có những hạn chế nhất định về tinh chỉnh và việc truy cập vào các mô hình gốc của chúng bị hạn chế. Mặt khác, mặc dù các LLM mã nguồn mở như LLaMa2 đã thể hiện kết quả ấn tượng trong tiếng Anh, hiệu suất của chúng giảm đáng kể khi áp dụng cho các ngôn ngữ khác (Touvron, Martin, et al., 2023). Sự khác biệt này có thể được quy cho việc tập dữ liệu huấn luyện của chúng chủ yếu bao gồm các văn bản tiếng Anh, điều này hạn chế khả năng hiểu và tạo nội dung của chúng trong các ngôn ngữ khác.

Để giải quyết hiệu suất kém của các LLM mã nguồn mở trong tiếng Ba Tư, chúng tôi giới thiệu PersianMind, một LLM Ba Tư-Anh mã nguồn mở đạt được kết quả tương đương với GPT-3.5-turbo (OpenAI, 2022) trong benchmark đọc hiểu. Trong bài báo này, chúng tôi sử dụng một tokenizer Byte-Pair Encoding tiếng Ba Tư gồm 10.000 token, được huấn luyện trên corpus Wikipedia tiếng Ba Tư đã làm sạch. Các token này được thêm vào từ vựng của LLaMa2, và các embedding của mô hình được mở rộng tương ứng. Chúng tôi sử dụng kỹ thuật LoRA để huấn luyện mô hình của chúng tôi trên corpus tiếng Ba Tư 2 tỷ token và sau đó tinh chỉnh mô hình bằng cách sử dụng các tập dữ liệu điều chỉnh hướng dẫn khác nhau để nâng cao hiệu suất của nó trên các tác vụ xử lý ngôn ngữ tự nhiên. Do các tập dữ liệu điều chỉnh hướng dẫn cho tiếng Ba Tư có hạn, chúng tôi tinh chỉnh mô hình bằng cách tinh chỉnh nó trên các tập dữ liệu dịch máy tiếng Ba Tư chất lượng cao.

Các đóng góp chính của chúng tôi trong bài báo này như sau:

• Giới thiệu PersianMind, một mô hình ngôn ngữ lớn Ba Tư-Anh mã nguồn mở được huấn luyện bằng phương pháp tính toán chi phí sử dụng kỹ thuật LoRA và song song hóa dữ liệu.

• Đạt được kết quả tốt nhất trên tập con tiếng Ba Tư của benchmark Belebele và tác vụ QA trắc nghiệm ParsiNLU.

• Đạt được hiệu suất tương đương với GPT-3.5-turbo trong tác vụ đọc hiểu tiếng Ba Tư.

• Giảm thiểu quên thảm khốc do huấn luyện mở rộng trên các tập dữ liệu tiếng Ba Tư bằng cách huấn luyện trên các tập dữ liệu song song Ba Tư-Anh và sử dụng kỹ thuật LoRA.

• Chứng minh rằng PersianMind có thể tạo ra các embedding câu chất lượng cao, vượt trội hơn hiệu suất của các mô hình ngôn ngữ có mặt nạ trước đó. Ngoài ra, chúng tôi đã chỉ ra rằng các embedding câu được tạo bởi PersianMind thể hiện tính đa ngôn ngữ.

• Chỉ ra hiệu quả của học chuyển giao đa ngôn ngữ trên PersianMind, chứng minh rằng việc tinh chỉnh mô hình với dữ liệu tiếng Ba Tư nâng cao đáng kể hiệu suất của nó trên tác vụ tiếng Anh tương ứng.

Phần còn lại của bài báo này được tổ chức như sau: Phần 2 xem xét các LLM mã nguồn mở và các phương pháp tinh chỉnh hiệu quả tham số. Phần 3 chi tiết phương pháp huấn luyện của chúng tôi, trong khi Phần 4 so sánh hiệu suất của PersianMind trên các tác vụ khác nhau với các đối thủ cạnh tranh khác. Trong Phần 5, chúng tôi thảo luận về dấu chân carbon liên quan đến việc huấn luyện PersianMind. Cuối cùng, Phần 6 kết luận bài báo.

## 2. Công trình liên quan

### 2.1 LLM mở

Trong khi GPT-3.5-turbo thể hiện thành thạo xuất sắc trong tạo ngôn ngữ tự nhiên, mô hình LLaMa (Touvron, Lavril, et al., 2023) là LLM đầu tiên tuyên bố đạt được hiệu suất tương tự trong các tác vụ tiếng Anh khác nhau. Họ mô hình LLaMa là một trong những mô hình ngôn ngữ lớn nền tảng mở phổ biến nhất, có quy mô từ 7B đến 65B tham số. Các mô hình LLaMa nhỏ hơn được huấn luyện trên 1T token, trong khi các mô hình lớn hơn được huấn luyện trên 1.4T token. Trong cả hai trường hợp, dữ liệu huấn luyện chủ yếu bao gồm văn bản và mã tiếng Anh. Chỉ 4.5% tập dữ liệu của họ là đa ngôn ngữ, bao gồm chữ Latin hoặc chữ Cyrillic. Mô hình Mosaic Pretrained Transformers (MPT) (MosaicML, 2023) có 7B tham số và được huấn luyện trên 1T token văn bản và mã tiếng Anh. Mô hình này tăng độ dài ngữ cảnh của đầu vào từ 2k lên 65k trong mô hình storyteller của nó. Các mô hình LLaMa2 (Touvron, Martin, et al., 2023) là phiên bản cập nhật của các mô hình LLaMa, bao gồm một tập hợp các LLM với 7B, 13B, và 70B tham số. Các mô hình này được tiền huấn luyện trên tập dữ liệu lớn hơn và chất lượng cao hơn so với phiên bản đầu tiên của các mô hình LLaMa. Tập dữ liệu tiền huấn luyện của họ chủ yếu bao gồm văn bản và mã tiếng Anh, với ít hơn 2% văn bản trong các ngôn ngữ khác.

Các mô hình Falcon (Tii, 2023) là một tập hợp các LLM với 1.3B, 7.5B, 40B, và (mã nguồn đóng) 180B tham số, được huấn luyện trên tập dữ liệu RefinedWeb (Penedo et al., 2023) - một tập dữ liệu dựa trên web được tuyển chọn, chất lượng cao. Mặc dù tập dữ liệu RefinedWeb là đa ngôn ngữ và bao gồm nhiều ngôn ngữ như tiếng Ba Tư, các mô hình Falcon mã nguồn mở được huấn luyện trên các ngôn ngữ châu Âu, đặc biệt là tiếng Anh. Các mô hình Yi (Yi, 2023) là một tập hợp các LLM Trung-Anh với 6B và 34B tham số. Các mô hình này được huấn luyện trên tập dữ liệu 3T token tiếng Anh và tiếng Trung và vượt trội hơn các mô hình trước đó trên các benchmark tiếng Anh và đặc biệt là tiếng Trung.

Trong khi các mô hình mới hơn như Falcon và Yi thể hiện hiệu suất tốt hơn trên các benchmark tiếng Anh, chúng tôi quyết định sử dụng mô hình Llama2 do được huấn luyện trên các tập dữ liệu đa ngôn ngữ hơn. Chúng tôi đặc biệt chọn tinh chỉnh biến thể LLaMa2-7B-chat vì việc tải nó với kiểu dữ liệu fp16 chỉ yêu cầu 14GB bộ nhớ GPU, làm cho nó dễ dàng tải trên GPU tiêu dùng với 24GB bộ nhớ. Việc tải mô hình của chúng tôi trên một GPU duy nhất cho phép chúng tôi tránh được chi phí của song song mô hình. Hơn nữa, trong thiết lập đa GPU, phương pháp này dẫn đến việc huấn luyện nhanh hơn bằng cách tận dụng song song hóa dữ liệu.

### 2.2 Tinh chỉnh Hiệu quả Tham số

Trong khi việc tinh chỉnh các LLM với hàng tỷ tham số có thể là một nhiệm vụ đắt đỏ, các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT) nhằm giảm chi phí huấn luyện bằng cách tinh chỉnh một số lượng nhỏ tham số. Điều chỉnh adapter (Houlsby et al., 2019) là một trong những kỹ thuật PEFT đầu tiên. Trong điều chỉnh adapter, các lớp adapter nhỏ được chèn sau các lớp attention đa đầu và feed forward của mỗi khối transformer, chỉ có các lớp này trải qua huấn luyện. Adamix (Y. Wang et al., 2022) đề xuất tận dụng adapter theo cách hỗn hợp chuyên gia, trong khi SparseAdapters (He et al., 2022) thừa nhận sự dư thừa trong nhiều tham số adapter. Bằng cách loại bỏ các tham số dư thừa này trong quá trình khởi tạo và sau đó tinh chỉnh, SparseAdapter đạt được kết quả tốt hơn.

Không giống như các phương pháp dựa trên adapter, thường liên quan đến việc tinh chỉnh các mô hình lớn với việc thêm các tham số có thể huấn luyện nhỏ, một tập hợp các phương pháp đề xuất một cách tiếp cận khác - tinh chỉnh chỉ một tập con nhỏ của mô hình hiện có. BitFit (Ben Zaken et al., 2022) đề xuất tinh chỉnh chỉ các bias của mô hình. FishMask (Sung et al., 2021) khuyến nghị lựa chọn các tham số có giá trị thông tin Fisher cao nhất để huấn luyện. Trong khi Freeze and Reconfigure (FAR) (Vucetic et al., 2022) đề xuất đóng băng các cột ít quan trọng hơn và chỉ huấn luyện những cột quan trọng.

Thích ứng thứ hạng thấp (LoRA) (Hu et al., 2022) đại diện cho một danh mục khác của các phương pháp PEFT. LoRA giới thiệu khái niệm đóng băng mô hình tiền huấn luyện và thêm các trọng số nhỏ, có thể điều chỉnh vào các lớp cụ thể. Các trọng số này có dạng ma trận phân tách thứ hạng được dẫn xuất từ các trọng số mô hình tiền huấn luyện. Chúng tôi giới thiệu độc giả quan tâm đến (Lialin et al., 2023) cung cấp một khảo sát toàn diện về các phương pháp PEFT.

Với số lượng nhỏ các tham số có thể huấn luyện và hiệu suất tuyệt vời của nó trên các mô hình có kích thước khác nhau (từ 125M đến 175B), chúng tôi chọn tinh chỉnh mô hình của chúng tôi bằng kỹ thuật LoRA. Hơn nữa, các mô hình Bacterian-X (H. Li et al., 2023) là một ví dụ đáng chú ý về hiệu quả của LoRA trong việc tinh chỉnh LLM để học một ngôn ngữ bổ sung.

## 3. Mô hình PersianMind

### 3.1 Tokenizer Song ngữ và Mở rộng Embedding

Tokenizer của LLaMa2 có 32.000 token, chỉ bao gồm 55 token tiếng Ả Rập và Ba Tư. Nó đặc biệt bao gồm các chữ cái Ba Tư và không bao gồm bất kỳ từ con tiếng Ba Tư bổ sung nào. Trong tình huống này, việc tinh chỉnh mô hình với corpus tiếng Ba Tư có thể mất thời gian dài vì mỗi từ được tokenize thành các chữ cái. Do đó, chúng tôi quyết định huấn luyện một tokenizer tiếng Ba Tư mới. Việc bổ sung tokenizer của LLaMa2 với các từ con tiếng Ba Tư cho phép tinh chỉnh mô hình của chúng tôi trên các corpus rộng lớn hơn trong giới hạn của cùng một ngân sách tính toán.

Do đó, phương pháp của chúng tôi liên quan đến việc huấn luyện một tokenizer mã hóa cặp byte (BPE) (Sennrich et al., 2016) với 10.000 token trên corpus Wikipedia tiếng Ba Tư 1GB. Hơn nữa, chúng tôi cải tiến tokenizer LLaMa2 bằng cách kết hợp các từ con tiếng Ba Tư. Tokenizer Ba Tư-Anh kết hợp có 41.510 token. Mặc dù việc huấn luyện tokenizer BPE lớn hơn có thể nắm bắt được nhiều token thực thể có tên hơn, nó sẽ tăng kích thước của các embedding đầu vào và đầu ra, dẫn đến nhiều tham số có thể huấn luyện hơn, điều này đòi hỏi nhiều tài nguyên tính toán hơn. Hơn nữa, chúng tôi mở rộng các embedding đầu vào và đầu ra theo kích thước của tokenizer của chúng tôi. Các embedding mới được thêm vào được khởi tạo ngẫu nhiên trong không gian của các embedding của LLaMa2.

### 3.2 Chi tiết Huấn luyện

Phương pháp huấn luyện mô hình của chúng tôi tập trung vào việc tối ưu hóa huấn luyện trên ngân sách tính toán bị hạn chế. Để đạt được điều này, chúng tôi tận dụng biến thể 7B-chat của các mô hình LLaMa2. Chúng tôi tránh song song mô hình do chi phí đáng kể trong bản phát hành đầu tiên của PersianMind và sẽ xem xét nó cho các bản phát hành trong tương lai. Thay vào đó, chúng tôi áp dụng song song hóa dữ liệu để giảm thời gian huấn luyện. Do đó, kỹ thuật LoRA được triển khai bằng cách kết hợp các trọng số LoRA trên tất cả các lớp của kiến trúc transformer, sử dụng thứ hạng LoRA là 8 và tỷ lệ dropout là 0.05. Do đó, các tham số huấn luyện của chúng tôi bao gồm các trọng số LoRA, embedding đầu vào và đầu ra.

Phương pháp huấn luyện của chúng tôi bao gồm hai giai đoạn chính. Đầu tiên, chúng tôi tinh chỉnh các embedding và trọng số LoRA của mô hình thông qua việc huấn luyện trên corpus văn bản tiếng Ba Tư thuần túy. Tiếp theo, chúng tôi thực hiện tinh chỉnh có giám sát (SFT) của mô hình trên các tập dữ liệu hướng dẫn. Trong cả hai giai đoạn, mục tiêu huấn luyện của chúng tôi là mô hình ngôn ngữ nhân quả.

Để đảm bảo đánh giá kỹ lưỡng, chúng tôi chia việc huấn luyện thành ba bước riêng biệt.

**Bước 1**: Trong bước đầu tiên, chúng tôi tinh chỉnh mô hình của chúng tôi trong hai epoch trên corpus Wikipedia tiếng Ba Tư, chứa 111 triệu token. Trong giai đoạn này, chúng tôi cố ý bỏ qua việc tinh chỉnh trên các hướng dẫn để khám phá khả năng của mô hình. Mặc dù perplexity không lý tưởng, mô hình vẫn có thể tạo ra kết quả chấp nhận được.

**Bước 2**: Trong bước thứ hai, chúng tôi tinh chỉnh mô hình của chúng tôi trên tập con 10GB của tập dữ liệu CC100 (Conneau et al., 2020), bao gồm 1.147 tỷ token. Trong giai đoạn này, chúng tôi tinh chỉnh mô hình của chúng tôi trên các tập dữ liệu PersianQA (Ayoubi và Davoodeh, 2021) và ParsQuad (Abadani et al., 2021) để cải thiện hiệu suất của nó trong tác vụ hỏi đáp. Ngoài ra, chúng tôi thực hiện tinh chỉnh trên một phần của tập dữ liệu song song TED2020 Anh-Ba Tư (Reimers và Gurevych, 2020) để có bản dịch tốt hơn, và tinh chỉnh trên tập dữ liệu Alpaca được dịch máy (Taori et al., 2023), một tập dữ liệu hướng dẫn toàn diện bao gồm các tác vụ và câu hỏi khác nhau.

**Bước 3**: Trong bước thứ ba, chúng tôi tinh chỉnh mô hình của chúng tôi trên tập con 5GB bổ sung của tập dữ liệu CC100, bao gồm 600 triệu token. Giai đoạn này ưu tiên tinh chỉnh trên nhiều tập dữ liệu hướng dẫn khác nhau. Đáng chú ý, để tăng cường khả năng dịch thuật của mô hình, chúng tôi tinh chỉnh trên toàn bộ tập dữ liệu song song TED2020 Anh-Ba Tư. PersianQA và ParsQuad một lần nữa được sử dụng để tăng cường khả năng của mô hình trong tác vụ hỏi đáp. Để cải thiện thành thạo trong QA trắc nghiệm và tác vụ đọc hiểu, chúng tôi tinh chỉnh mô hình của chúng tôi trên QA trắc nghiệm ParsiNLU (Khashabi et al., 2021), đọc hiểu ParsiNLU (Khashabi et al., 2021), và tập dữ liệu phụ trợ MMLU được dịch máy (Hendrycks et al., 2021). Ngoài ra, 20.000 hướng dẫn từ tập dữ liệu tóm tắt PN (Mehrdad Farahani, Gharachorloo, và Manthouri, 2021) được kết hợp để tăng cường khả năng tóm tắt của mô hình. Khả năng lý luận ngôn ngữ tự nhiên được tinh chỉnh bằng cách tinh chỉnh trên tập dữ liệu Chain of Thought của ORCA được dịch máy (Mukherjee et al., 2023). Hơn nữa, một số hướng dẫn được tạo thủ công liên quan đến công thức nấu ăn và tục ngữ Ba Tư được giới thiệu để đa dạng hóa hiểu biết của mô hình về các ngữ cảnh ngôn ngữ tự nhiên.

Mô hình được huấn luyện trong bước này thể hiện thành thạo trong việc tạo văn bản tiếng Ba Tư, với điểm perplexity vượt trội hơn mô hình bước thứ hai. Ngoài ra, có một cải thiện nhẹ trong điểm perplexity của mô hình khi áp dụng cho văn bản tiếng Anh. Đáng chú ý, việc tinh chỉnh trên tập dữ liệu Chain of Thought của ORCA đã góp phần nâng cao khả năng phân tích và giải quyết vấn đề của mô hình một cách có hệ thống và từng bước.

Hình 1 cung cấp tổng quan về phương pháp huấn luyện của chúng tôi, trong khi Bảng 1 trình bày các tập dữ liệu được sử dụng để huấn luyện, cùng với số lượng token tương ứng. Ngoài ra, nó cung cấp thông tin về các tập dữ liệu được sử dụng cho điều chỉnh hướng dẫn trong mỗi bước, cùng với số lượng hướng dẫn tương ứng.

Bảng 2 hiển thị các điểm perplexity được quan sát trong các tập dữ liệu Ba Tư và Anh sau mỗi bước huấn luyện của mô hình, trước và sau SFT. Điểm perplexity tiếng Anh được tính toán trên corpus tiếng Anh gồm 24 bài báo từ The New York Times và The New Yorker, bao gồm các chủ đề khác nhau và chứa 54.000 token. Đối với điểm perplexity tiếng Ba Tư, chúng tôi chọn chín bài báo từ Tarjomaan và Faradars, trải rộng trên các lĩnh vực khác nhau và chứa 58.000 token. Đối với cả tập dữ liệu tiếng Anh và Ba Tư, chúng tôi chọn các bài báo được xuất bản gần đây để đảm bảo rằng mô hình của chúng tôi không được huấn luyện trên chúng.

Kết quả cho thấy SFT có tác động đáng kể đến điểm perplexity cho cả tiếng Ba Tư và tiếng Anh. Trong bước thứ hai, việc huấn luyện mô hình của chúng tôi trên hơn 1 tỷ văn bản tiếng Ba Tư thuần túy chỉ dẫn đến cải thiện 8 điểm trong điểm perplexity. Tuy nhiên, việc thực hiện SFT trên 170.000 hướng dẫn đã dẫn đến cải thiện 10 điểm.

Đáng chú ý, các điểm perplexity cho thấy việc huấn luyện trên văn bản tiếng Ba Tư thuần túy có thể dẫn đến quên thảm khốc kiến thức tiếng Anh của mô hình. Tuy nhiên, việc thực hiện SFT trên tập dữ liệu song song Ba Tư-Anh có thể giúp giảm thiểu tác động này.

## 4. Đánh giá

### 4.1 QA Trắc nghiệm và Đọc hiểu

Chúng tôi đánh giá mô hình của chúng tôi bằng cách sử dụng tập con kiểm tra của QA trắc nghiệm ParsiNLU và tập dữ liệu đọc hiểu trắc nghiệm Belebele (Bandarkar et al., 2023). Tập dữ liệu ParsiNLU bao gồm các câu hỏi tiếng Ba Tư với nhiều ứng viên, trong đó một trong số chúng là câu trả lời đúng. Đáng chú ý, không có đoạn văn ngữ cảnh cho những câu hỏi này. Ngược lại, tập dữ liệu Belebele trình bày các câu hỏi được trích xuất từ các đoạn văn được cung cấp, với bốn câu trả lời ứng viên, với một câu trả lời đúng.

Bảng 3 hiển thị kết quả đánh giá trên tập dữ liệu QA trắc nghiệm ParsiNLU, được phân loại thành các loại câu hỏi văn học, kiến thức thường thức, và toán học & logic. Kết quả cũng liên quan đến việc so sánh mô hình của chúng tôi với các biến thể lớn và x-lớn của các mô hình mT5 (Xue et al., 2021), mỗi mô hình được huấn luyện trên các tập dữ liệu huấn luyện tiếng Ba Tư, tiếng Anh, hoặc hỗn hợp của cả hai. Việc đánh giá bao gồm cả mô hình PersianMind và phiên bản tinh chỉnh của nó. PersianMind được tinh chỉnh trong một epoch trên tập dữ liệu huấn luyện, trong khi mô hình mT5 được tinh chỉnh trong 20k bước trên tập huấn luyện trong ít nhất 15 epoch (Khashabi et al., 2021).

Như có thể thấy, PersianMind đạt được kết quả tương đương với mô hình mT5 x-large được huấn luyện trên cả tập dữ liệu tiếng Ba Tư và tiếng Anh. Đáng chú ý, mô hình PersianMind tinh chỉnh đã vượt trội hơn mô hình hoạt động tốt nhất gần 5%. Trong khi nó thể hiện hiệu suất tương đối yếu hơn trên các câu hỏi toán học & logic, nó xuất sắc trong danh mục kiến thức thường thức, vượt trội hơn các mô hình khác trong lĩnh vực cụ thể này.

Chúng tôi đánh giá PersianMind trên cả tập con tiếng Ba Tư và tiếng Anh của tập dữ liệu Belebele. Trong Bảng 4, chúng tôi so sánh kết quả của mô hình với mô hình ngôn ngữ có mặt nạ đa ngôn ngữ XLM-V được phát hành gần đây (Liang et al., 2023), LLaMa2-7B-chat gốc, và GPT-3.5-turbo. Kết quả cho thấy PersianMind đã nâng cao hiệu suất của LLaMa2-7B-chat lên 43% và 48% trên tập con tiếng Anh và Ba Tư, tương ứng. Ngoài ra, PersianMind vượt trội hơn mô hình XLM-V. Mặc dù hiệu suất của nó trong tiếng Anh thấp hơn GPT-3.5-turbo 1%, nó vượt trội hơn GPT-3.5-turbo 12% trong tập con tiếng Ba Tư.

Mặc dù PersianMind được tinh chỉnh độc quyền trên các tập dữ liệu QA trắc nghiệm tiếng Ba Tư trong bước 3, hiệu suất của nó trong tiếng Anh cũng được cải thiện. Quan sát này làm nổi bật tiềm năng cho học chuyển giao đa ngôn ngữ cho LLM.

### 4.2 Dịch thuật

Chúng tôi đánh giá PersianMind trên tập dữ liệu Flores-200 (Costa-jussà et al., 2022) cho các hướng dịch Fa→En và En→Fa. Đánh giá mô hình được thực hiện bằng cách sử dụng cả điểm BLEU (Post, 2018) và Comet (Rei et al., 2022), và kết quả được trình bày trong Bảng 5. Mô hình của chúng tôi được so sánh trong các thiết lập khác nhau, bao gồm zero-shot, 2-shot, và 4-shot. Ngoài ra, chúng tôi tinh chỉnh PersianMind trên 5.000 trường hợp dữ liệu song song từ WikiMatrix (Schwenk et al., 2021) và so sánh hiệu suất của nó với các mô hình khác. Mô hình ParsiNLU mT5-large được tinh chỉnh trong 200k bước trên các tập dữ liệu song song Ba Tư-Anh (Khashabi et al., 2021). Hơn nữa, chúng tôi đánh giá mô hình của chúng tôi so với GPT-3.5-turbo và NLLB-MoE (Costa-jussà et al., 2022), một mô hình dịch đa ngôn ngữ với 54B tham số, hỗ trợ 200 ngôn ngữ.

Kết quả cho thấy trong khi bản dịch Fa→En của PersianMind được cải thiện trong thiết lập few-shot, chất lượng dịch thuật En→Fa giảm. Quan sát này cho thấy học trong ngữ cảnh không phải là phương pháp hiệu quả cho tác vụ dịch thuật. Tiếp theo, chúng tôi tinh chỉnh PersianMind trên 5.000 hướng dẫn dịch thuật. Kết quả cho thấy PersianMind tinh chỉnh có thể tạo ra kết quả ngang bằng với mô hình mT5-large được tinh chỉnh trên tập dữ liệu song song Ba Tư-Anh 200.000. Tuy nhiên, PersianMind tinh chỉnh vẫn tạo ra kết quả yếu hơn so với GPT-3.5-turbo.

Trong so sánh của chúng tôi, chúng tôi cũng đánh giá kết quả LLM so với mô hình dịch NLLB-MoE. Mặc dù có khoảng cách đáng kể giữa kết quả LLM và NLLB-MoE về điểm BLEU, khoảng cách nhỏ hơn nhiều trong điểm Comet. Điều này truyền đạt thực tế rằng trong khi bản dịch LLM có ý nghĩa rất tương tự với bản dịch tham chiếu, không có sự trùng lặp cao giữa n-gram của bản dịch mô hình và bản dịch tham chiếu.

### 4.3 Tương đồng Ngữ nghĩa Văn bản

Để đánh giá chất lượng của các embedding câu được tạo bởi mô hình của chúng tôi, chúng tôi đã tiến hành đánh giá trên các benchmark Tương đồng Ngữ nghĩa Văn bản (STS) (Corley và Mihalcea, 2005). Ban đầu, chúng tôi đánh giá các embedding câu của mô hình trên cả tập dữ liệu STS tiếng Ba Tư và tiếng Anh một cách độc lập. Tiếp theo, chúng tôi đánh giá hiệu suất đa ngôn ngữ của các câu tương đồng ngữ nghĩa trong ngữ cảnh Ba Tư-Anh.

Đối với tiếng Ba Tư, chúng tôi sử dụng tập dữ liệu FarSick (Ghasemi và Keyvanrad, 2021), cung cấp các cặp câu với điểm liên quan từ 1.0 đến 5.0. Đối với tiếng Anh, chúng tôi sử dụng tập dữ liệu benchmark STS của MTEB (Muennighoff et al., 2023), có cấu trúc tương tự như tập dữ liệu FarSick. Trong quá trình đánh giá, chúng tôi tạo embedding cho từng câu trong một cặp câu riêng biệt và tính toán mức độ liên quan của chúng bằng cách sử dụng tương tự cosine. Tiếp theo, chúng tôi so sánh điểm tương tự của mô hình với điểm vàng bằng cách sử dụng metric tương quan Spearman.

Chúng tôi tạo embedding câu với mô hình của chúng tôi bằng cách sử dụng phương pháp AnglE (X. Li và Jing Li, 2023), sử dụng embedding của token padding trong mẫu prompt sau: "Tóm tắt câu 'text' trong một từ:". Kết quả của chúng tôi sau đó được so sánh với các mô hình ngôn ngữ nền tảng đa ngôn ngữ khác, bao gồm mBERT (Devlin et al., 2019), ParsBERT (Mehrdad Farahani, Gharachorloo, Marzieh Farahani, et al., 2021), và XLM-RoBERTa (Conneau et al., 2020). Việc tính toán embedding câu từ các mô hình này liên quan đến các phương pháp khác nhau, chẳng hạn như sử dụng embedding của token [CLS], mean pooling, và phương pháp AnglE-BERT. Tiếp theo, chúng tôi so sánh kết quả tốt nhất từ các mô hình khác với kết quả thu được từ mô hình của chúng tôi. Ngoài ra, chúng tôi đánh giá kết quả của mô hình so với các mô hình LaBSE (Feng et al., 2022) và LASER3 (Heffernan et al., 2022), thường được sử dụng cho mục đích khai thác bitext.

Trong Bảng 6, chúng tôi trình bày kết quả điểm tương tự ngữ nghĩa cho PersianMind và các mô hình khác. Các phát hiện cho thấy PersianMind đạt được điểm tương quan Spearman cao nhất trong tất cả các mô hình ngôn ngữ nền tảng, cả trong tiếng Anh và tiếng Ba Tư. Nó thể hiện cải thiện so với điểm tương quan LLaMa2-7B-chat 9% trong tiếng Ba Tư và 2% trong tiếng Anh. Hơn nữa, PersianMind vượt trội hơn các embedding câu LASER3 và đạt được kết quả trung bình tương đương với LaBSE.

Để đánh giá tính đa ngôn ngữ của embedding câu của mô hình, chúng tôi sử dụng tập con Ba Tư-Anh của tập dữ liệu Flores-200. Để đánh giá tương tự ngữ nghĩa của các câu song song Anh-Ba Tư, chúng tôi tính toán tương tự cosine của embedding câu tiếng Ba Tư và tiếng Anh và sau đó lấy trung bình trên tất cả các cặp. Bảng 7 so sánh PersianMind với các mô hình ngôn ngữ nền tảng và khai thác bitext khác. Trong khi PersianMind không đạt được điểm tương tự cosine trung bình của LaBSE, một mô hình chuyên dụng cho tác vụ này, nó vẫn đạt được 72% đáng chú ý, vượt trội hơn mô hình gốc 14% và nổi lên như mô hình ngôn ngữ hoạt động tốt nhất trong số các mô hình khác.

## 5. Dấu chân Carbon

PersianMind được huấn luyện trên bốn GPU NVIDIA RTX 3090. Việc huấn luyện trên văn bản tiếng Ba Tư thuần túy cần 9 ngày, trong khi việc thực hiện SFT trên các hướng dẫn mất 1 ngày. Do đó, việc huấn luyện PersianMind tiêu thụ tổng cộng 960 giờ GPU. Mức tiêu thụ điện năng của mỗi GPU NVIDIA RTX 3090 là 350W. Ngoài ra, xem xét Hiệu suất Sử dụng Điện năng (PUE) là 1.4 cho máy chủ của chúng tôi do hiệu quả thiết bị không tối ưu, mức tiêu thụ điện năng tổng thể để huấn luyện PersianMind, dựa trên công thức (C.-J. Wu et al., 2022), là 470kWh.

Để tính toán lượng khí thải carbon, chúng tôi cần ước tính cường độ phát thải CO2, được liên kết với vị trí trung tâm dữ liệu của chúng tôi. Chúng tôi sử dụng dữ liệu điện toàn cầu (Ember, 2023) về cường độ carbon của điện ở Iran, đo được 494 gram CO2 tương đương phát thải trên mỗi kilowatt-giờ điện. Do đó, việc huấn luyện PersianMind dẫn đến phát thải 232.38 kCO2eq.

## 6. Kết luận

Trong bài báo này, chúng tôi giới thiệu PersianMind, một mô hình ngôn ngữ lớn được xây dựng dựa trên LLaMa2-7B-chat như mô hình nền tảng, kết hợp thêm 10.000 từ con tiếng Ba Tư và được huấn luyện trên tập dữ liệu rộng lớn gần 2 tỷ token tiếng Ba Tư. Bằng cách sử dụng LoRA trong quá trình huấn luyện, mục tiêu của chúng tôi là đạt được việc huấn luyện hiệu quả về chi phí. Chúng tôi đã chứng minh rằng việc sử dụng kỹ thuật LoRA và tiến hành SFT trên các tập dữ liệu song song Anh-Ba Tư cho phép chúng tôi giảm quên thảm khốc kiến thức tiếng Anh mặc dù mô hình của chúng tôi được huấn luyện trên tập hợp lớn các tập dữ liệu tiếng Ba Tư. Các phát hiện của chúng tôi cho thấy PersianMind đạt được kết quả tương đương trong các tập dữ liệu QA trắc nghiệm đọc hiểu với GPT-3.5-turbo. Bằng cách tinh chỉnh với 5.000 câu song song, nó vượt trội hơn hiệu suất của mô hình mT5-large, được tinh chỉnh với 200.000 mẫu, trong các tác vụ dịch thuật. Ngoài ra, chúng tôi đã chứng minh rằng PersianMind tạo ra embedding câu hiệu quả cao cho cả câu tiếng Anh và tiếng Ba Tư. Như một mô hình ngôn ngữ, nó vượt trội hơn các mô hình ngôn ngữ có mặt nạ trước đó, thể hiện hiệu quả của nó trong các tác vụ xử lý ngôn ngữ tự nhiên khác nhau.

## Tài liệu tham khảo

[Các tài liệu tham khảo được giữ nguyên như trong bản gốc]

## Phụ lục A: Mẫu sinh từ PersianMind

Chúng tôi đã tạo ra cuộc trò chuyện sau thảo luận về việc nâng cao kỹ năng toán học của trẻ em. Để đảm bảo kết quả có tính xác định, chúng tôi cũng đặt giá trị seed là 1442.

**Lời nhắc:**
Bạn là một trợ lý trí tuệ nhân tạo tư vấn về học tập các chủ đề khác nhau. Một đứa trẻ tiểu học gặp khó khăn trong việc học toán. Làm thế nào có thể giúp em tiến bộ trong môn toán?

**Phản hồi:**
Để giúp con bạn cải thiện kỹ năng toán học, bạn có thể sử dụng một số phương pháp sau:

1. Trò chơi giáo dục: Chơi các trò chơi giáo dục như câu đố và đố vui với con bạn. Điều này không chỉ thú vị mà còn dạy chúng nhìn thấy và hiểu số dưới dạng hình ảnh.

2. Sách truyện: Đọc những cuốn sách truyện có chứa các bài toán có thể làm cho trẻ em quan tâm và khuyến khích chúng tham gia nhiều hơn vào toán học.

3. Luyện tập giải quyết vấn đề: Cung cấp bài tập giải quyết vấn đề cho trẻ em cho phép chúng phát triển kỹ năng giải quyết vấn đề và đồng thời hiểu rõ hơn các khái niệm toán học.

4. Gia sư tư: Nếu con bạn vẫn gặp khó khăn trong lớp học ở trường, việc thuê gia sư tư có thể giúp em tăng cường kỹ năng toán học.

5. Lập kế hoạch đều đặn: Tạo một chương trình học đều đặn cho con bạn có thể động viên em và cho phép em có tiến bộ đáng kể trong các bài học toán.

6. Khuyến khích: Khuyến khích con bạn nỗ lực trong môn toán rất quan trọng. Hãy cho chúng thấy chủ đề này quan trọng như thế nào đối với sự phát triển của chúng và chúng đã cố gắng chăm chỉ đến mức nào.

**Lời nhắc:**
Bạn có thể nói thêm về điểm số 3 không?

**Phản hồi:**
Về điểm số 3, luyện tập giải quyết vấn đề, đề xuất của tôi là cả hai phụ huynh nên hợp tác để cung cấp cho con mình nhiều cơ hội hơn để giải quyết vấn đề. Điều này cho phép chúng mở rộng kỹ năng giải quyết vấn đề và hiểu sâu hơn về các khái niệm toán học. Ngoài ra, họ nên tìm kiếm các nguồn tài liệu phù hợp có thể trả lời hiệu quả các câu hỏi toán học.

Cuối cùng, điều quan trọng cần lưu ý là việc giải quyết vấn đề tự nó là không đủ. Học sinh cần học và luyện tập tài liệu theo nhiều cách khác nhau để ghi nhớ tốt. Do đó, hãy cố gắng thử nghiệm các phương pháp khác nhau để giúp đỡ chúng một cách tốt nhất có thể.

**Lời nhắc:**
Nếu phụ huynh không thể dành nhiều thời gian cho việc luyện tập với con, họ có thể sử dụng những giải pháp nào?

**Phản hồi:**
Nếu phụ huynh không thể dành nhiều thời gian cho việc luyện tập, họ có thể sử dụng các công cụ trực tuyến hoặc phần mềm. Nhiều trang web và ứng dụng có sẵn trên internet cho phép bạn dễ dàng giải quyết các câu hỏi toán và dần dần nâng cao kỹ năng giải quyết vấn đề. Ngoài ra, một số trường học và cơ sở giáo dục tư cũng tổ chức các khóa học trực tuyến mà bạn có thể tận dụng.

**Lời nhắc:**
Bạn có thể nêu một vài ví dụ cụ thể không?

**Phản hồi:**
Một trong những trang web toán học nổi tiếng nhất là Learning Games for Kids có bộ sưu tập rộng lớn các trò chơi giáo dục. Ngoài ra, có nhiều nền tảng như Khan Academy và Duolingo cho phép bạn nâng cao kỹ năng ngôn ngữ và toán học.

**Lời nhắc:**
Bạn có thể giải thích thêm về khuyến khích không?

**Phản hồi:**
Khuyến khích con bạn nỗ lực trong môn toán rất quan trọng. Hãy cho chúng thấy chủ đề này quan trọng như thế nào đối với sự phát triển của chúng và chúng đã cố gắng chăm chỉ đến mức nào. Việc khuyến khích chúng nỗ lực trong lĩnh vực này mang lại cho chúng cảm giác thành công và khiến chúng háo hức đối mặt với môn toán thay vì chạy trốn khỏi nó.

**Lời nhắc:**
Những loại khuyến khích nào có thể phù hợp với trẻ em tiểu học?

**Phản hồi:**
Những lời khuyến khích ngắn gọn thường hiệu quả hơn vì chúng cho phép trẻ em tự hào về bản thân mà không gây áp lực quá mức. Ví dụ, bạn có thể nói "Điều này rất tuyệt vời!" hoặc "Rất tốt!"

**Lời nhắc:**
Những trò chơi giáo dục nào có thể hiệu quả? Hãy nêu 5 ví dụ.

**Phản hồi:**
1. Câu đố: Câu đố là một loại trò chơi cho phép người chơi đạt được mục tiêu cụ thể bằng cách di chuyển các mảnh khác nhau theo thứ tự đúng. Loại trò chơi này giúp trẻ em nhận biết tốt hơn các mẫu và mối quan hệ giữa các số.

2. Đố vui: Đố vui là một loại trò chơi cho phép người chơi đạt được mục tiêu cụ thể bằng cách giải một câu đố. Loại trò chơi này giúp trẻ em mở rộng kỹ năng giải quyết vấn đề và hiểu sâu hơn về các khái niệm toán học.

3. Trò chơi tư duy: Trò chơi tư duy có nhiều loại khác nhau nhưng tất cả đều cho phép người chơi tìm ra câu trả lời bằng cách suy nghĩ về một vấn đề. Loại trò chơi này giúp trẻ em mở rộng kỹ năng tư duy phản biện và hiểu sâu hơn về các khái niệm toán học.

4. Trò chơi chiến lược: Trò chơi chiến lược có nhiều loại khác nhau nhưng tất cả đều cho phép người chơi đạt được mục tiêu cụ thể bằng cách đưa ra quyết định thông minh. Loại trò chơi này giúp trẻ em mở rộng kỹ năng ra quyết định và hiểu sâu hơn về các khái niệm toán học.

5. Trò chơi trên bàn: Trò chơi trên bàn có nhiều loại khác nhau nhưng tất cả đều cho phép người chơi tạo ra tình huống cụ thể bằng cách di chuyển các quân cờ. Loại trò chơi này giúp trẻ em mở rộng kỹ năng giải quyết vấn đề và hiểu sâu hơn về các khái niệm toán học.
