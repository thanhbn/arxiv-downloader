# 2309.17061.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2309.17061.pdf
# File size: 801242 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SCALE: S YNERGIZED COLLABORATION OF ASYM -
METRIC LANGUAGE TRANSLATION ENGINES
Xin Cheng1Xun Wang2Tao Ge2
Si-Qing Chen2Furu Wei2Dongyan Zhao1Rui Yan3
1Peking University2Microsoft3Renmin University of China
ABSTRACT
In this paper, we introduce SCALE, a collaborative framework that connects
compact Specialized Translation Models (STMs) and general-purpose Large Lan-
guage Models (LLMs) as one unified translation engine. By introducing transla-
tion from STM into the triplet in-context demonstrations, SCALE unlocks refine-
ment and pivoting ability of LLM, thus mitigating language bias of LLM and par-
allel data bias of STM, enhancing LLM speciality without sacrificing generality,
and facilitating continual learning without expensive LLM fine-tuning. Our com-
prehensive experiments show that SCALE significantly outperforms both few-shot
LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource set-
tings. Moreover, in Xhosa to English translation, SCALE experiences consistent
improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot
GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a com-
pact model consisting of merely 600M parameters. SCALE could also effectively
exploit the existing language bias of LLMs by using an English-centric STM as a
pivot for translation between any language pairs, outperforming few-shot GPT-4
by an average of 6 COMET points across eight translation directions. Furthermore
we provide an in-depth analysis of SCALE’s robustness, translation characteris-
tics, and latency costs, providing solid foundation for future studies exploring the
potential synergy between LLMs and more specialized, task-specific models1.
1 I NTRODUCTION
Large Language Models (LLMs) have recently revolutionized the field of natural language pro-
cessing (OpenAI, 2023; Touvron et al., 2023; Peng et al., 2023), significantly influencing machine
translation (MT) by delivering exceptional performance without requiring a bilingual corpus, par-
ticularly in high-resource languages (Brown et al., 2020; Garcia et al., 2023). Moreover, as a unified
multi-task learner, LLMs represent a substantial step towards artificial general intelligence (Bubeck
et al., 2023), with the potential to overcome not only language barriers but also cultural boundaries
simultaneously through a simple “translate and explain” prompt.
Despite their advancements, LLM-based translation systems still confront several challenges.
Firstly, there exists a significant language bias towards English (e.g., 92.1% of the GPT-3 pre-
training corpus is English, while French, the second largest, represents only 1.8%2), which signifi-
cantly constraints multilingual translation performance, especially for those low-resource languages
(Scao et al., 2022; Hendy et al., 2023). Secondly, as a practical approach for system improvement,
fine-tuning LLM poses great challenges. These include (1) the trade-off between speciality and
generality (Cheng et al., 2023a; Lin et al., 2023), and (2) the prohibitively high cost associated with
tuning large-scale models (Hu et al., 2021; Dettmers et al., 2023). In contrast, traditional Specialized
Translation Models (STMs)—those based on encoder-decoder architecture, trained with supervision
and significantly smaller in size (Sutskever et al., 2014; Vaswani et al., 2017)—serve as specialists
for specific translation tasks and could be efficiently fine-tuned. However, these models lack general
1Code available at: https://github.com/Hannibal046/SCALE
2https://github.com/openai/gpt-3/blob/master/dataset_statistics/
languages_by_character_count.csv
1arXiv:2309.17061v1  [cs.CL]  29 Sep 2023

--- PAGE 2 ---
COMETAssameseArmenianAmharicXhosaUyghurKhmer83.986.283.278.883.784.685.688.386.980.785.486.186.688.888.082.186.487.1BLEURT Score  
AssameseArmenianAmharic
Xhosa
UyghurKhmer
707478
GPT-4
NLLB
SCALE
( b )BLEURTAssameseArmenianAmharicXhosaUyghurKhmer69.173.167.370.867.469.972.177.073.674.070.472.273.877.875.775.772.273.9COMET Score  
AssameseArmenianAmharic
Xhosa
UyghurKhmer
828589
GPT-4
NLLB
SCALE
( a )
2Figure 1: Translation results of few-shot LLM (GPT-4), STM (NLLB) and SCALE (ours) for six
low-resource languages measured by COMET and BLEURT.
language capabilities and are potentially susceptible to parallel data bias, such as the memorization
of low-quality samples (Raunak et al., 2022).
In this paper, we demonstrate for the first time the possibility to unify these two asymmetric transla-
tion engines in a single framework. Our work, SCALE, connects LLMs and STMS by utilizing the
LLM’s most enigmatic capability: in-context learning. Rather than employing source-target pairs as
in conventional few-shot translation (Garcia et al., 2023; Vilar et al., 2023), SCALE would first sam-
ple translations from a STM and then use triplets consisting of a source sentence, an STM-generated
set and a target sentence as in-context demonstrations to unlock the refinement and pivoting ability
of LLMs. With SCALE, we could (1) mitigate both language bias of LLMs by utilizing an STM that
concentrates on a specific language pair, and parallel data bias of STMs by using a general-purpose
LLM as the main body of the system; (2) enhance the speciality of LLMs without compromising
generality; (3) facilitate continual learning within the framework by updating only the lightweight
STM, thus avoiding expensive LLM fine-tuning. By employing SCALE, we create a more efficient
and effective system that combines the best of both translation engines.
Our comprehensive experiments reveal that SCALE considerably outperforms few-shot LLMs (e.g.,
GPT-4) and specialized models (e.g., NLLB) in the challenging low-resource setting, as depicted in
Figure 1. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement
by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and
3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters.
Remarkably, SCALE can effectively exploit the existing language bias of LLMs by using an English-
centric STM as a pivot for translation between any language pairs, outperforming few-shot GPT-4
by an average of 6 COMET points across eight translation directions. Furthermore, we conduct
an in-depth analysis of the robustness, translation characteristics, and latency costs associated with
SCALE. Our findings provide valuable insights and encourage further research in this field.
2 T HESCALE F RAMEWORK
In this section, we present the proposed SCALE method and provide an overview illustrated in
Figure 2. Popularized by GPT-3 (Brown et al., 2020), In-context Learning (ICL) allows LLMs to
perform a wide variety of tasks, even newly created ones (Bills et al., 2023), by leveraging few-shot
learning with a limited number of demonstrations. For a translation task from a source language X
to a target language Y, an LLM with parameters θcarries out ICL by conditioning on ksource-target
paired examples E= (x1, y1)⊕(x2, y2)⊕...(xk, yk)and the test source sentence x, generating
the target yin an auto-regressive manner as yt∼pθ(yt|E, x, y <t). In this scenario, the LLM
must analyze the provided examples to discern the input distribution, output distribution, input-
output mapping, and formatting to successfully complete the task (Press et al., 2022; Wei et al.,
2023). Different from conventional ICL, SCALE introduces an intermediate variable Zas reference
2

--- PAGE 3 ---
....  Specialized Model
        Large Language Model
Corpus
X ...
Z
...
Y ...
X...
Z
...Translate X to Y, conditioning
on potentially useful  Z
....
Update
source
 target
reference
trainable
frozen
x NTrainFigure 2: The SCALE framework, comprised of a lightweight specialized model and a frozen large
language model with triplet in-context demonstrations.
between source xand target y, transforming each demonstration example into a triplet (x,Z, y).
The variable Zis a generation set sampled from a specialized translation model MX7→Y trained on
a labeled dataset. The final input to the LLM consists of the instruction, demonstrations, and source
sentence combined in a prompt template: T((x1,Z1, y1)⊕(x2,Z2, y2)...⊕(xk,Zk, yk)),(x,Z)).
Unlike language understanding tasks that have fixed label set (Xu et al., 2023), the hypothesis space
of translation model is actually infinite, so we could sample multiple generation paths from STM for
one single source sentence to provide a more comprehensive generation guide for LLM. The SCALE
framework, though conceptually straightforward, demonstrates several advantages over STMs and
LLMs, as highlighted below:
Refinement ForXtoYtranslation task, when the intermediate variable Zis fromMX7→Y (x),
SCALE essentially conduct few-shot learning in a multi-task way by introducing an additional re-
finement task. Refinement has long been proved effective in MT (Xia et al., 2017; Cheng et al.,
2022). And this also holds true for LLM-based translation. In this refinement process, we pass sam-
pled sentences and their confidence score (probability score) from STM to an LLM. The LLM then
digests the information carried by the sampled set and infers the generation space of the STM, which
guides the LLM to generate the output that is more consistent with the local data distribution (Xu
et al., 2023). And since the final translation is delivered by an LLM, SCALE could also mitigate
the parallel data bias from STMs and exhibit robustness by not merely copying and pasting the draft
translation from STMs as shown in §5.3.
Pivoting Considering the predominantly English-centric nature of most LLMs (Brown et al., 2020;
Touvron et al., 2023), SCALE could employ an intermediate variable ZfromMX7→ English (x)where
the target language Yis not necessarily English. And here Zserves as a pivot point for LLMs
to enhance their understanding of the source sentence and yield improved translations. This can
also be regarded as a form of knowledge transfer from high-resource languages to low-resource
languages (Chen et al., 2017; Kim et al., 2019; Jiao et al., 2023).
Updating A significant limitation of the existing LLM-based translation systems is the inherent
complexity of LLM continual learning. This complexity arises from several factors, including the
delicate balance between speciality and generality (Lin et al., 2023), the catastrophic forgetting
problem (Yong et al., 2023), and the substantial computational demands (Dettmers et al., 2023).
In contrast, the SCALE framework offers a more efficient and streamlined approach to continuous
updating. By exclusively and effectively updating the lightweight MX7→· component, the framework
ensures that the LLM remains untouched, thus preserving its general language capabilities. This
selective updating process not only mitigates the issue of catastrophic forgetting but also reduces the
computational burden of fine-tuning associated with LLM-based translation systems.
3 E XPERIMENTAL SETUP
3.1 D ATASET
Our evaluation datasets encompass a diverse set of languages, spanning both low- and high-resource
settings and deriving from various language families. To facilitate reproducibility and data sharing,
3

--- PAGE 4 ---
all our evaluation datasets come from the devtest split of Flores-200 (NLLB Team et al., 2022), a
publicly available many-to-many evaluation data set covering 200 languages from all over the world.
3.2 T RANSLATION SYSTEMS
We compare our approach with cutting-edge academic systems including both specialized models
and LLMs, as well as one commercial system, Microsoft Translator3.
We have two strong specialized models:
•M2M100 (Fan et al., 2021) is the first multilingual encoder-decoder translation model that
can translate between any pair of 100 languages without relying on English data.
•NLLB (NLLB Team et al., 2022) is a supervised translation model suite covering from
169M to 54.5B (MOE) parameters with encoder-decoder architecture and capable of deliv-
ering high-quality translations directly between 200 languages.
For few-shot LLMs, we consider:
•XGLM (Lin et al., 2022) is a multilingual generative language models trained on a cor-
pus covering a diverse set of languages and the largest XGLM-7.5B model outperforms
comparable sized GPT-3 model in multilingual setting.
•GPT-3.54is a GPT model specially optimized for conversational purpose and shows re-
markable performance in machine translation tasks (Jiao et al., 2023).
•GPT-4 (OpenAI, 2023) is the latest and the most powerful version of GPT-series.
We use both GPT-3.5 and GPT-4 from Microsoft Azure OpenAI Service5. Without further notice,
the number of few-shot samples in LLM and SCALE are set to 10 and the sample selection strategy
follows Agrawal et al. (2022). The prompt we use could be found in the Appendix A.1.
3.3 E VALUATION METRICS
Because neural metrics have shown higher correlation with human preference (Freitag et al., 2022;
Rei et al., 2020) and are widely adopted by recent literatures (Hendy et al., 2023; Garcia et al.,
2023), we mainly evaluate our system with (1) COMET-226, a reference-based neural metric (Rei
et al., 2022a) combining direct assessments, sentence-level score, and word-level tags from multidi-
mensional quality metrics error annotations, (2) COMETKiwi7, a refrence-free quality estimation
model from Rei et al. (2022b), and (3) BLEURT (Sellam et al., 2020), a learnable evaluation metric
with a regression model trained on ratings data. For completeness, we also include the results of
lexical metrics such as spBLEU (NLLB Team et al., 2022) and chrF++ (Popovic, 2017).
4 E XPERIMENTAL RESULTS
In this section, we conduct various experiments to show the effectiveness of our framework. In §4.1,
we verify the effectiveness of the refinement ability within SCALE by comparing with STMs and
few-shot LLMs. In §4.2, we focus on non-English pairs to test the pivoting ability of SCALE. In
§4.3, we show the continual learning results of SCALE with a fixed LLM and an evolving STM.
4.1 SCALE R EFINEMENT
To evaluate the refinement capabilities of SCALE, this section primarily concentrates on low-
resource languages, which currently pose significant challenges for few-shot LLMs. Our approach
3https://azure.microsoft.com/en-us/products/cognitive-services/
translator
4https://platform.openai.com/docs/models/gpt-3-5
5https://azure.microsoft.com/en-us/products/ai-services/openai-service
6https://huggingface.co/Unbabel/wmt22-comet-da
7https://huggingface.co/Unbabel/wmt22-cometkiwi-da
4

--- PAGE 5 ---
showcases its versatility by incorporating languages from diverse families and scripts, including
Assamese (asm Beng), Armenian (hye Armn), Amharic (amh Ethi), Xhosa (xho Latn), Uyghur
(uig Arab), Khmer (khm Khmr), Nepali (npi Deva), and Sindhi (snd Arab). For additional data
details, please refer to the Appendix A.2.
COMET-22 COMETKiwi BLEURT spBLEU COMET-22 COMETKiwi BLEURT spBLEU
asm Beng hye Armn
NLLB 85.6 82.8 72.1 33.9 88.3 87.5 77.0 43.0
M2M100 n/a n/a n/a n/a 75.9 76.5 58.9 23.7
Microsoft 83.5 81.7 68.8 29.6 85.2 85.0 71.5 34.6
XGLM 62.7 57.8 38.8 3.7 43.9 50.2 20.5 0.2
GPT-3.5 78.6 76.7 61.0 18.1 77.0 77.2 60.5 19.4
GPT-4 83.9 80.9 69.1 27.9 86.2 86.0 73.1 35.6
SCALE-refine 86.6 83.2 73.8 34.1 88.8 88.0 77.8 42.3
amh Ethi xho Latn
NLLB 86.9 84.5 73.6 36.4 80.7 65.8 74.0 40.1
M2M100 72.3 72.0 54.8 18.5 68.0 62.1 59.0 25.7
Microsoft 87.5 84.6 74.7 41.9 n/a n/a n/a n/a
XGLM 50.2 43.9 17.8 0.1 39.6 41.7 37.1 1.6
GPT-3.5 58.8 54.2 31.7 3.4 69.1 65.5 58.3 21.9
GPT-4 83.2 81.9 67.3 27.1 78.8 67.1 70.8 34.5
SCALE-refine 88.0 85.3 75.7 37.6 82.1 67.3 75.7 40.0
uigArab khm Khmr
NLLB 85.4 84.4 70.4 27.5 86.1 85.4 72.2 35.4
M2M100 n/a n/a n/a n/a 69.6 71.6 54.0 17.6
Microsoft 82.7 81.7 66.2 21.6 80.2 80.5 63.3 25.6
XGLM 37.1 52.8 16.9 0.2 48.6 53.7 21.6 0.7
GPT-3.5 73.7 74.2 53.0 11.6 73.3 73.0 53.2 13.9
GPT-4 83.7 82.8 67.4 23.1 84.6 84.0 69.9 29.1
SCALE-refine 86.4 85.0 72.2 27.9 87.1 85.9 73.9 34.7
npiDeva snd Arab
NLLB 90.4 88.3 77.1 45.0 86.9 79.5 75.5 44.4
M2M100 75.2 73.6 55.1 21.2 49.8 47.2 39.2 6.4
Microsoft 89.8 88.2 75.3 42.8 83.6 77.4 70.4 38.5
XGLM 72.9 67.0 48.8 8.3 53.8 45.1 29.8 1.8
GPT-3.5 87.2 85.4 69.9 29.3 75.6 68.1 58.8 17.3
GPT-4 90.2 88.1 76.3 40.8 83.2 75.3 69.9 32.3
SCALE-refine 91.1 88.8 78.1 44.0 87.5 79.5 76.6 42.9
Table 1: Translation results of eight low-resource languages to English. The best results are in bold
and the second best are with underscore . SCALE-refine is compared with specialized model (NLLB,
M2M), commercial system (MS Translator) and few-shot LLM (XGLM, GPT-3.5, GPT-4).
We adopt three kinds of baseline systems as described in §3.2. For supervised NLLB model suite,
we choose the NLLB-3.3B version, and for SCALE-refine, the LLM is GPT-4 the STM is also
NLLB-3.3B for fair comparison.
The results are displayed in Table 1. As observed, few-shot LLMs, including GPT-4, significantly
trail behind specialized models in all translation directions. Even with Xhosa belonging to the same
language family as English, the GPT-4 model fails to deliver comparable results to NLLB model.
In contrast, our framework, by combining LLMs and STMs, demonstrates superior performance
over few-shot GPT-4 by an averaged 2.96 COMET scores and 5 BLEURT scores, and surpasses
the strong NLLB model in 8/8 directions. Interestingly, when the performance gap is substantial
(e.g., SCALE-refine over GPT-4), the lexical metric spBLEU aligns with COMET and BLEURT.
However, when comparing SCALE-refine with NLLB, although COMET-22, COMETkiwi, and
BLEURT exhibit consistent patterns, spBLEU displays degradation with the GPT-based system in 4
out of 8 directions. Similar findings are also reported in Vilar et al. (2023); Hendy et al. (2023).
4.2 SCALE P IVOTING
In this section, we demonstrate the performance of SCALE-pivot, in which the variable Zis not di-
rectly pertinent to the current translation directions but functions as a pivot. Specifically, we examine
the performance of few-shot GPT-4 and SCALE-pivot on Lao →Ytranslations, where Yrepresents
a language set encompassing both low-resource and high-resource languages. For the low-resource
languages, we include Assamese (asm Beng), Armenian (hye Armn), Amharic (amh Ethi), Xhosa
(xho Latn), and we have German (deu Latn), Czech (ces Latn), Bulgarian (bul Cyrl) and Greek
(ellGrek) for the high-resource setting.
5

--- PAGE 6 ---
Armenian Assamese Amharic Xhosa German Czech Bulgarian Greek7075808590COMET
69.5
67.768.470.776.281.1
79.679.2
77.1
73.473.872.883.287.9
86.7
85.6Lao => X
GPT-4 SCALE-PivotFigure 3: Translation results from Lao to both low- and high-resource languages, where GPT-4 uses
few-shot prompting and SCALE-pivot uses English as the pivot language.
The results are presented in Figure 3. Firstly, with GPT-4 results alone, we could observe that the
language bias of LLM heavily affects translation performance. The few-shot GPT-4 model typically
excels in the high-resource setting but struggles in low-resource one. Furthermore, it is evident that
SCALE-pivot can enhance the performance of GPT-4 in both low- and high-resource settings, while
the performance gain is more significant in high-resource setting (an averaged 6.8 COMET-22 score
improvement for high-resource versus 5.2 for low-resource).
4.3 SCALE U PDATING
788082
78.8 - GPT-4
M2M NLLB-600M NLLB-1.3B NLLB-3.3B68COMET Score
STM
SCALE
70727476
70.8 - GPT-4
M2M NLLB-600M NLLB-1.3B NLLB-3.3B5860BLEURT Score
STM
SCALE
34363840
34.5 - GPT-4
M2M NLLB-600M NLLB-1.3B NLLB-3.3B2426
spBLEU Score
STM
SCALE
52.555.057.560.0
53.3 - GPT-4
M2M NLLB-600M NLLB-1.3B NLLB-3.3B45.047.5
chrF++ Score
STM
SCALE
Figure 4: Translation results from Xhosa to English with evolving STMs in the SCALE framework.
In this section, we explore the potential enhancement of our framework by keeping the LLM fixed
and solely updating the STM. Specifically, we use M2M100-12B and NLLB model suite ranging
from 600M to 3.3B as our evolving STM. We conduct experiments on the Xhosa →English direction
and adopt the prompt format of SCALE-refine. The experimental results are displayed in Figure 4,
leading to the following observations:
(1) The overall framework can be consistently improved with a fixed LLM and a continuously evolv-
ing STM; (2) SCALE, when equipped with a small model containing only 600M parameters, can
outperform GPT-4 with an absolute 2.5 COMET-22 score and a 3.8 BLEURT score; (3) Equipped
6

--- PAGE 7 ---
Assamese Armenian Amharic Xhosa Uyghur Khmer Nepali Sindhi30354045PPLFluency measured by PPL
SCALE GPT-4 NLLBFigure 5: Perplexity score from X→English translation measured by GPT2-XL.
with an STM (M2M100) of relatively lower performance than original few-shot GPT-4 , SCALE
demonstrates strong robustness by not merely copying and pasting the less satisfactory reference
answer provided by M2M100, which we detailedly investigated in §5.3.
Interestingly, we also observe that the growth patterns exhibited by lexical metrics and neural se-
mantic metrics differ. For M2M100 and NLLB-600M as STM, both metrics experience substantial
improvement, while for NLLB-1.3B and 3.3B as STM, SCALE maintains the same lexical accuracy
while continually enhancing translation performance as measured by neural semantic metrics.
5 F URTHER ANALYSIS
5.1 T RANSLATION CHARACTERISTICS
To gain a deeper understanding of the translation characteristics of different systems (few-shot
LLMs, STMs, and SCALE) beyond overall translation quality, we employ the following measure-
ments, as suggested by Hendy et al. (2023):
1.Translation Fluency: Since LLMs are optimized by predicting the next token, their trans-
lations tend to display a language modeling bias that favors fluency over adequacy. To
investigate this, we utilize an independently trained open-source language model (GPT2-
XL (Radford et al., 2019)) to measure the perplexity score of the translation output.
2.Translation Non-Monotonicity: This metric evaluates the extent to which a translation
adheres to the source sentence’s structure, calculating the deviation from the diagonal in
the word-to-word alignment. Translations that are more paraphrastic or less literal tend to
deviate from closely tracking the source word order across language pairs (Hendy et al.,
2023). We apply the non-monotonicity metric proposed by Schioppa et al. (2021).
3.Unaligned Source Words: Another measure of literalness is the count of unaligned source
words (Hendy et al., 2023; Raunak et al., 2023a). When accounting for quality, less literal
translations are likely to include more words that do not align with those in the source
sentence.
We present the Translation Fluency results of X→English translation in Figure 5, where Xre-
mains the same as used in Section 4.1. It is evident that regardless of the translation quality deliv-
ered by the LLM, whether superior (SCALE) or inferior (GPT-4) compared to the STM (NLLB),
the LLM translation generally demonstrates higher fluency than the STM. Additionally, in 6 out of
the 8 languages examined, SCALE produces lower perplexity scores than the original GPT-4 out-
put. This suggests that the STM-generated variable Zcan effectively aid the GPT-4 model in further
decreasing its generation uncertainty.
ForNon-Monotonicity andUnaligned Source Words8, we choose Xhosa →English translation
with different STMs, and the results are shown in Figure 6. We also include PPL score for com-
pleteness. We find that both the USW and NM scores for STM are higher than those of GPT-4. This
8We use this implementation: https://github.com/vyraun/literalness with xlmr branch in
https://github.com/neulab/awesome-align/tree/xlmr
7

--- PAGE 8 ---
# Path COMET-22 BLEURT spBLEU
1 80.4 73.2 35.6
2 81.2 74.3 37.1
3 81.4 74.7 38.0
4 81.5 74.8 38.3
5 81.4 74.9 38.4
Table 2: Translation results from Xhosa to English with multi-path sampling. All the experiments
are conducted by one-shot SCALE-refine and only differ in the number of sampled paths from STM.
indicates that even though STM provides higher translation quality, it results in less literal transla-
tions. However, for SCALE, it effectively reduces GPT-4’s NM score while maintaining a moderate
USW score. This suggests that during the SCALE refinement process, the model primarily adheres
to the original LLM output structure while taking cues from STM’s word selection. We show several
concrete cases in Appendix A.3.
4243444546
NLLB
600MNLLB
1.3BNLLB
3.3B33343536
34.7 - GPT-4PPL Score
STM
SCALE
NLLB
600MNLLB
1.3BNLLB
3.3B53.554.054.555.055.5
53.8 - GPT-4USW Score
STM
SCALE
NLLB
600MNLLB
1.3BNLLB
3.3B12.0012.2512.5012.7513.0013.2513.5013.7514.00
12.79 - GPT-4NM Score
STM
SCALE
Figure 6: Perplexity, Unaligned Source Words percentage and Non-Monotonicity score from
Xhosa→English translation.
5.2 M ULTIPATH SAMPLING
In this section, We list the results of multiple path sampling strategy in Table 2. We test with
Xhosa→English with one-shot SCALE-refine. The results show that without increasing the shot
number in the few-shot learning, using STM to generate more generation paths could consistently
improve the overall performance, which could be useful in the extremely low-resource setting where
demonstration samples are hard to acquire.
5.3 A BLATION
In this section, we conduct an ablation study for each key design in our framework. We examine
the following variants: (1) without confidence: This model follows the same setting as the SCALE-
refine in §4.1, except that we do not pass the confidence score of each token as input. (2) zero-shot:
This variant removes all in-context-learning examples, keeping only the translation instruction and
the reference answer from STM. (3) one-shot: This model utilizes only one-shot, in contrast to
the ten-shot results presented in §4.1. (4) zero-shot-M2M: This model also implements zero-shot,
but the STM used is M2M100, a less performant model than the original few-shot GPT-4. This is
employed to assess the robustness of our framework.
The outcomes of our ablation study are showcased in Table 3. It is evident that each component in
our framework perform effectively, with the in-context-learning setting providing the most perfor-
mance gain. This indicates that simply offering a reference answer to the LLM without in-context
samples does not adequately guide the model in utilizing those references effectively. Furthermore,
the number of ICL examples is also an essential factor in the process.
Regarding the SCALE zero-shot-M2M variant, its performance is significantly inferior to that of
the few-shot LLM due to the poor quality of the M2M100 output. From this observation, we can
8

--- PAGE 9 ---
conclude that the robustness of SCALE, as illustrated in Figure 4, primarily stems from the power
of in-context learning. This learning approach informs the LLM about which elements to trust and
which to disregard, ultimately improving the overall translation performance and robustness.
COMET-22 COMETKiwi BLEURT
M2M100 68.0 62.1 59.0
NLLB 80.7 65.8 74.0
GPT-4 78.8 67.1 70.8
SCALE 82.1 67.3 75.7
w/o confidence 81.6 67.6 74.9
zero-shot 81.4 66.4 74.8
one-shot 81.7 66.7 75.3
zero-shot-M2M 76.4 66.8 68.2
Table 3: Ablation study for SCALE with Xhosa →English translation.
5.4 G ENERATION LATENCY
few-shot LLM SCALE
avg. #length total avg. #length STM LLM total
0-shot 101.37 7.19 161.13 1.87 7.43 9.3
1-shot 198.00 7.46 516.92 1.87 8.33 10.2
10-shot 951.91 9.52 2489.72 1.87 14.17 16.04
Table 4: Generation latency results of LLM (BLOOM-175B) and SCALE (BLOOM-175B + NLLB-
3.3B) measured in seconds (s).
In this section, we conduct a detailed evaluation of the overhead introduced by SCALE in compar-
ison to conventional few-shot LLM. The additional latency arises from two factors: first, the time
required to generate the variable Zfor the current source sentence xusing STM, and second, the
increased latency caused by the LLM due to the extended context. Since the response time from the
GPT API may not accurately represent the actual latency of the LLM, we utilize one of the largest
open-source LLMs (BLOOM-176B) for this analysis. As shown in Table 4, we observe that the in-
curred latency can be primarily attributed to the extended context window due to the quadratic time
complexity of the transformer architecture. Exploring methods to accelerate this process based on
STM-generated output using speculative decoding techniques remains a topic for future work (Xia
et al., 2022; Chen et al., 2023a; Yang et al., 2023).
6 R ELATED WORK
The use of LLM for translation tasks has garnered significant interest in recent times. Brown et al.
(2020) initially demonstrated the efficacy of prompting an LLM with a few examples to achieve note-
worthy results, particularly in high-resource languages (Vilar et al., 2023; Lin et al., 2022). Follow-
ing the release of ChatGPT, several studies have examined its overall translation performance(Jiao
et al., 2023; Hendy et al., 2023), along with works focusing on the issue of hallucination (Guerreiro
et al., 2023) , literalness (Raunak et al., 2023a), multilinguality (Zhu et al., 2023) and incidental
bilingualism problem (Briakou et al., 2023). A comprehensive analysis conducted by Garcia et al.
(2023) revealed the unreasonable effectiveness of few-shot LLMs. Furthermore, a diverse range of
research has attempted to enhance LLM-based translation systems through cultural awareness (Yao
et al., 2023), refinement (Chen et al., 2023b; Cheng et al., 2023b), retrieval-augmentation (Cheng
et al., 2023b), post-editing (Raunak et al., 2023b), and comparison (Zeng et al., 2023).
Our work also shares similarities with a series of studies that aim to build collaboration between
LLMs and other systems. Luo et al. (2023) propose equipping LLMs with a knowledge-guiding
module to access relevant information without altering the LLMs’ parameters. Hendy et al. (2023)
propose to use Microsoft Translator system as the primary translation system, and then use GPT as
9

--- PAGE 10 ---
a fallback system when the quality of MS-Translator is unsatisfactory measured by reference-free
metrics. Xu et al. (2023) introduce SuperICL and achieve significant improvements in various lan-
guage understanding tasks. Ge et al. (2023) employ a trainable LoRA-based encoder as an additional
model for LLM context compression.
7 C ONCLUSION
In this paper, we present a novel collaborative framework SCALE, which effectively combines the
strengths of Large Language Models (LLMs) and compact Specialized Translation Models (STMs)
through an in-context learning approach. By providing triplet in-context demonstrations, our frame-
work successfully unlocks the refinement and pivoting capabilities of LLMs. SCALE demonstrates
its superiority in many scenarios including low-resource setting, multilingual translation and model
continual learning setting. Our results offer crucial understanding and a robust basis for subsequent
research investigating the possible synergistic effects between LLMs and more specialized models
tailored for specific tasks.
ACKNOWLEDGMENTS
We would like to acknowledge Jiduan Liu and Lemao Liu for the helpful discussions and valuable
suggestions.
REFERENCES
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-
context examples selection for machine translation, 2022.
Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever,
Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in lan-
guage models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index.
html.(Date accessed: 14.05. 2023) , 2023.
Eleftheria Briakou, Colin Cherry, and George Foster. Searching for needles in a haystack: On the
role of incidental bilingualism in palm’s translation capability. arXiv preprint arXiv:2305.10266 ,
2023.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR ,
abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165 .
S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-
mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 , 2023a.
Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield. Iterative translation refinement
with large language models, 2023b.
Yun Chen, Yang Liu, Yong Cheng, and Victor OK Li. A teacher-student framework for zero-resource
neural machine translation. arXiv preprint arXiv:1705.00753 , 2017.
Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with
contrastive translation memories. arXiv preprint arXiv:2212.03140 , 2022.
10

--- PAGE 11 ---
Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge
from paramters for plug-and-play language modeling. In Findings of the Association for Com-
putational Linguistics: ACL 2023 , pp. 14288–14308, Toronto, Canada, July 2023a. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.901. URL https:
//aclanthology.org/2023.findings-acl.901 .
Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up:
Retrieval-augmented text generation with self memory, 2023b.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314 , 2023.
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal,
Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom
Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. Beyond english-
centric multilingual machine translation. J. Mach. Learn. Res. , 22:107:1–107:48, 2021. URL
http://jmlr.org/papers/v22/20-1307.html .
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis,
Tom Kocmi, George F. Foster, Alon Lavie, and Andr ´e F. T. Martins. Results of WMT22 metrics
shared task: Stop using BLEU - neural metrics are better and more robust. In Philipp Koehn,
Lo¨ıc Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-juss `a, Christian
Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grund-
kiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Tom Kocmi,
Andr ´e Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Mat-
teo Negri, Aur ´elie N ´ev´eol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri
(eds.), Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi,
United Arab Emirates (Hybrid), December 7-8, 2022 , pp. 46–68. Association for Computational
Linguistics, 2022. URL https://aclanthology.org/2022.wmt-1.2 .
Xavier Garcia, Yamini Bansal, Colin Cherry, George F. Foster, Maxim Krikun, Fangxiaoyu Feng,
Melvin Johnson, and Orhan Firat. The unreasonable effectiveness of few-shot learning for
machine translation. CoRR , abs/2302.01398, 2023. doi: 10.48550/arXiv.2302.01398. URL
https://doi.org/10.48550/arXiv.2302.01398 .
Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context
compression in a large language model. arXiv preprint arXiv:2307.06945 , 2023.
Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre
Colombo, and Andr ´e F. T. Martins. Hallucinations in large multilingual translation models, 2023.
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Mat-
sushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How good are GPT mod-
els at machine translation? A comprehensive evaluation. CoRR , abs/2302.09210, 2023. doi:
10.48550/arXiv.2302.09210. URL https://doi.org/10.48550/arXiv.2302.09210 .
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt a good
translator? a preliminary study. arXiv preprint arXiv:2301.08745 , 2023.
Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. Pivot-based
transfer learning for neural machine translation between non-english languages. arXiv preprint
arXiv:1909.09524 , 2019.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle
Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh
Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva,
Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative
language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of
the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu
Dhabi, United Arab Emirates, December 7-11, 2022 , pp. 9019–9052. Association for Computa-
tional Linguistics, 2022. URL https://aclanthology.org/2022.emnlp-main.616 .
11

--- PAGE 12 ---
Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang
Wang, Han Zhao, Yuan Yao, and Tong Zhang. Speciality vs generality: An empirical study on
catastrophic forgetting in fine-tuning foundation models, 2023.
Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin
Jiang. Augmented large language models with parametric knowledge guiding, 2023.
NLLB Team, Marta R. Costa-juss `a, James Cross, Onur C ¸ elebi, Maha Elbayad, Kenneth Heafield,
Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler
Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez,
Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shan-
non Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela
Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm ´an, Philipp Koehn, Alexandre Mourachko,
Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind:
Scaling human-centered machine translation. 2022.
OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. URL https://api.
semanticscholar.org/CorpusID:257532815 .
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin
Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , et al. Rwkv: Reinventing rnns for
the transformer era. arXiv preprint arXiv:2305.13048 , 2023.
Maja Popovic. chrf++: words helping character n-grams. In Ondrej Bojar, Christian Buck, Ra-
jen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio
Jimeno-Yepes, Philipp Koehn, and Julia Kreutzer (eds.), Proceedings of the Second Confer-
ence on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017 , pp.
612–618. Association for Computational Linguistics, 2017. doi: 10.18653/v1/w17-4770. URL
https://doi.org/10.18653/v1/w17-4770 .
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring
and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 ,
2022.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
Language models are unsupervised multitask learners. 2019. URL https://api.
semanticscholar.org/CorpusID:160025533 .
Vikas Raunak, Matt Post, and Arul Menezes. SALTED: A framework for SAlient long-tail
translation error detection. In Findings of the Association for Computational Linguistics:
EMNLP 2022 , pp. 5163–5179, Abu Dhabi, United Arab Emirates, December 2022. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.379. URL https:
//aclanthology.org/2022.findings-emnlp.379 .
Vikas Raunak, Arul Menezes, Matt Post, and Hany Hassan Awadalla. Do gpts produce less literal
translations?, 2023a.
Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah, and Arul Menezes. Leveraging gpt-4 for
automatic translation post-editing, 2023b.
Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. COMET: A neural framework for
MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,
Online, November 16-20, 2020 , pp. 2685–2702. Association for Computational Linguistics, 2020.
doi: 10.18653/v1/2020.emnlp-main.213. URL https://doi.org/10.18653/v1/2020.
emnlp-main.213 .
Ricardo Rei, Jos ´e G. C. de Souza, Duarte M. Alves, Chrysoula Zerva, Ana C. Farinha, Taisiya
Glushkova, Alon Lavie, Lu ´ısa Coheur, and Andr ´e F. T. Martins. COMET-22: unbabel-ist 2022
submission for the metrics shared task. In Philipp Koehn, Lo ¨ıc Barrault, Ondrej Bojar, Fethi
Bougares, Rajen Chatterjee, Marta R. Costa-juss `a, Christian Federmann, Mark Fishel, Alexan-
der Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Had-
dow, Matthias Huck, Antonio Jimeno-Yepes, Tom Kocmi, Andr ´e Martins, Makoto Morishita,
12

--- PAGE 13 ---
Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aur ´elie N ´ev´eol, Mariana
Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (eds.), Proceedings of the Seventh
Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid),
December 7-8, 2022 , pp. 578–585. Association for Computational Linguistics, 2022a. URL
https://aclanthology.org/2022.wmt-1.52 .
Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine
Maroti, Jos ´e G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and
Andr ´e F. T. Martins. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared
task. In Proceedings of the Seventh Conference on Machine Translation (WMT) , pp. 634–645,
Abu Dhabi, United Arab Emirates (Hybrid), December 2022b. Association for Computational
Linguistics. URL https://aclanthology.org/2022.wmt-1.60 .
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagn ´e, Alexandra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall ´e, et al. Bloom: A 176b-
parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.
Andrea Schioppa, David Vilar, Artem Sokolov, and Katja Filippova. Controlling machine trans-
lation for multiple attributes with additive interventions. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language Processing , pp. 6676–6696, Online and
Punta Cana, Dominican Republic, November 2021. Association for Computational Linguis-
tics. doi: 10.18653/v1/2021.emnlp-main.535. URL https://aclanthology.org/2021.
emnlp-main.535 .
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. BLEURT: learning robust metrics for text
generation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Pro-
ceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020 , pp. 7881–7892. Association for Computational Linguistics, 2020.
doi: 10.18653/v1/2020.acl-main.704. URL https://doi.org/10.18653/v1/2020.
acl-main.704 .
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
Advances in neural information processing systems , 27, 2014.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster.
Prompting palm for translation: Assessing strategies and performance, 2023.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv
preprint arXiv:2303.03846 , 2023.
Heming Xia, Tao Ge, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Lossless
speedup of autoregressive translation. 2022. URL https://openreview.net/pdf?id=
H-VlwsYvVi .
Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Delibera-
tion networks: Sequence generation beyond one-pass decoding. Advances in neural information
processing systems , 30, 2017.
Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. Small
models are valuable plug-ins for large language models. arXiv preprint arXiv:2305.08848 , 2023.
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and
Furu Wei. Inference with reference: Lossless acceleration of large language models, 2023.
13

--- PAGE 14 ---
Binwei Yao, Ming Jiang, Diyi Yang, and Junjie Hu. Empowering llm-based machine translation
with cultural awareness. arXiv preprint arXiv:2305.14328 , 2023.
Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa
Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa,
Genta Indra Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vassilina Nikoulina.
Bloom+1: Adding language support to bloom for zero-shot prompting, 2023.
Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. Tim: Teaching large language models to
translate with comparison. arXiv preprint arXiv:2307.04408 , 2023.
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen,
and Lei Li. Multilingual machine translation with large language models: Empirical results and
analysis, 2023.
14

--- PAGE 15 ---
A A PPENDIX
A.1 P ROMPT EXAMPLE
In Table 5, we list the prompt we use for few-shot LLM and in Table 6, for our SCALE framework.
We use Chat Markup Language version from Azure to format our prompt9.
Instruction<|imstart|>system
Assistant is an intelligent chatbot designed
to help users translate from $ {source language }to ${target language }
<|imend|>
Examples<|imstart|>user
Source: $ {source 1}
Target: $ {target 1}
...
Source: $ {source n}
Target: $ {target n}
InputSource: $ {source}
<|imend|>
<|imstart|>assistant
Target:
Table 5: Prompt of Chat Markup Language format for few-shot LLM.
Instruction<|imstart|>system
Assistant is an intelligent chatbot designed
to help users translate from $ {source language }to ${target language }
Context:
·Assistant would would be given a potentially useful reference answer
from a fine-tuned model
·The number in brackets denotes the confidence score of a fine-tuned model
to generate the token.
<|imend|>
Examples<|imstart|>user
Source: $ {source 1}
Potentially useful reference answer 1: $ {reference 1}
Potentially useful reference answer 2: $ {reference 2}
Target: $ {target 1}
...
Source: $ {source n}
Potentially useful reference answer 1: $ {reference 1}
Potentially useful reference answer 2: $ {reference 2}
Target: $ {target n}
InputSource: $ {source}
Potentially useful reference answer 1: $ {reference 1}
Potentially useful reference answer 2: $ {reference 2}
<|imend|>
<|imstart|>assistant
Target:
Table 6: Prompt of Chat Markup Language format for SCALE.
9https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/
chatgpt?pivots=programming-language-chat-ml
15

--- PAGE 16 ---
A.2 D ATA STATISTICS
We list the detailed data information for SCALE-refine and SCALE-Pivot experiments in Table A.2.
The number of dev set is 997 and 1012 for devtest set in flores-200 (NLLB Team et al., 2022).
code language # dev length # devtest length script family resource
asm Beng Assamese 40.55 41.67 Bengali Indo-European low
hyeArmn Armenian 43.91 45.31 Armenian Indo-European low
amh Ethi Amharic 38.87 39.64 Ge’ez Afro-Asiatic low
xhoLatn Xhosa 35.31 36.37 Latin Atlantic-Congo low
uigArab Uyghur 40.77 42.41 Arabic Turkic low
khm Khmr Khmer 52.77 53.79 Khmer Austroasiatic low
npiDeva Nepali 34.36 35.48 Devanagari Indo-European low
engLatn English 28.99 30.28 Latin Indo-European high
deuLatn German 37.57 39.16 Latin Indo-European high
cesLatn Czech 36.63 38.10 Latin Indo-European high
bulCyrl Bulgarian 37.99 39.45 Cyrillic Indo-European high
rusCyrl Russian 39.42 40.21 Cyrillic Indo-European high
Table 7: Data statistics for all the tested languages in the paper.
A.3 T RANSLATION CASES
In this section, we list several translation cases from different languages.
Under review as a conference paper at ICLR 2024
SOURCEየዳይነሶር ላባዎች የዳበረ ራቺስ የሚባል ዘንግ ስለሌለው፣ ነገር ግን ሌሎች የላባ
ባህርያት — ባርብስ እና ባርቡልስ — ስላለው ተመራማሪዎች ራቺስ ከእነዚህ ሌሎች
ባህርያት የቆየ ዝግመተ ለውጥ ውጤት እንደሆነ ይላሉ።
TARGET Because the dinosaur feathers do not have a well-developed shaft,
called a rachis, but do have other features of feathers — barbs and
barbules — the researchers inferred the rachis was likely a later
evolutionary development that these other features.
MS Translator Dinosaur feathers developed because it doesn’t have a rod called
rachis, but has other feather traits — barbs and barbules — that
researchers say is the result of older evolution of rachis from these
other traits.
NLLB dinosaur feathers did not develop a shaft called the rachis, but
other feather features, such as barbs and barbels, suggest that
the rachis was the result of an earlier evolution of these other
features.
GPT-4 As there is no known population of the extinct Laysan Rail on
Laysan Island, researchers suggest that the presence of rails on
the other islands—Barbados and Barbuda—indicates a prolonged
period of isolation and change.
SCALE Dinosaur feathers did not develop a shaft called the rachis, how-
ever, other feather features such as barbs and barbules suggest
that the rachis was the result of an earlier evolution of these other
features.
Table 9: Translation case from Amharic to English.
SOURCE बाइसन, एल्क, मूस, भालु र लगभग सबै ठूला जनावरहरूले जस्ता नरम देखि ए पनि
आक्रमण गर्न सक्छन्।
TARGET No matter how docile they may look, bison, elk, moose, bears,
and nearly all large animals can attack.
MS Translator Bison, elk, moose, bears, and almost all large animals can attack
even if they look soft.
NLLB The Bible says: ”The one who is walking with wise persons will
become wise, but the one who is having dealings with the stupid
ones will fare badly”.
GPT-4 Bison, elk, moose, bears, and nearly all large animals, despite
appearing gentle, can be aggressive.
SCALE Bison, elk, moose, bears and nearly all large animals can attack
even though they appear docile.
Table 10: Translation case from Nepali to English.
17
Figure 7: Translation case from Nepali to English.
16

--- PAGE 17 ---
Under review as a conference paper at ICLR 2024
A.2 Data Statistics
We list the detailed data information for SCALE-refine and SCALE-Pivot experiments in
Table A.2. The number of dev set is 997 and 1012 for devtest set in flores-200 (NLLB Team
et al., 2022).
code language # dev length # devtest length script family resource
asm_Beng Assamese 40.55 41.67 Bengali Indo-European low
hye_Armn Armenian 43.91 45.31 Armenian Indo-European low
amh_Ethi Amharic 38.87 39.64 Ge’ez Afro-Asiatic low
xho_Latn Xhosa 35.31 36.37 Latin Atlantic-Congo low
uig_Arab Uyghur 40.77 42.41 Arabic Turkic low
khm_Khmr Khmer 52.77 53.79 Khmer Austroasiatic low
npi_Deva Nepali 34.36 35.48 Devanagari Indo-European low
eng_Latn English 28.99 30.28 Latin Indo-European high
deu_Latn German 37.57 39.16 Latin Indo-European high
ces_Latn Czech 36.63 38.10 Latin Indo-European high
bul_Cyrl Bulgarian 37.99 39.45 Cyrillic Indo-European high
rus_Cyrl Russian 39.42 40.21 Cyrillic Indo-European high
Table 7: Data statistics for all the tested languages in the paper.
A.3 Translation Cases
In this section, we list several translation cases from different languages.
SOURCE ভ�ৰ�থোৱো�ৰকো��চলোওঁতোজনৰ ভ�ৰৰখোত সহোয় ক�ৰ����ো��োঁৰোৰ গো-
দীৰ দু�য়োফো�লতল�লওল��থো�ক।
TARGET Stirrups are supports for the rider’s feet that hang down on either
side of the saddle.
MS Translator The legged rickshaw helps to keep the driver’s leg which hangs
down on either side of the horse’s mattress.
NLLB The foot rest helps to keep the rider’s feet which are sloping down-
wards on both sides of the horse’s saddle.
GPT-4 A heavily loaded Rickshaw helps balance the load by tilting to
both sides when going over bumps.
SCALE The stirrup helps to support the rider’s feet, which are sloping
downwards on both sides of the horse’s saddle.
Table 8: Translation case from Assamese to English.
Under review as a conference paper at ICLR 2024
SOURCE बाइसन, एल्क, मूस, भालु र लगभग सबै ठूला जनावरहरूले जस्ता नरम देखि ए पनि
आक्रमण गर्न सक्छन्।
TARGET No matter how docile they may look, bison, elk, moose, bears,
and nearly all large animals can attack.
MS Translator Bison, elk, moose, bears, and almost all large animals can attack
even if they look soft.
NLLB The Bible says: ”The one who is walking with wise persons will
become wise, but the one who is having dealings with the stupid
ones will fare badly”.
GPT-4 Bison, elk, moose, bears, and nearly all large animals, despite
appearing gentle, can be aggressive.
SCALE Bison, elk, moose, bears and nearly all large animals can attack
even though they appear docile.
Table 11: Translation case from Nepali to English.
GPT-4
SCALE
Table 12: Translation case from Nepali to English.
18Es gibt eine Chance, dass es genauso verschwindet, wie es 
aussieht, als ob es einfach verschwindet.
Es ist auch nützlich, einen Ring zu tragen, nur scheint der Ring 
zu teuer zu sein.SOURCE बाइसन, एल्क, मूस, भालु र लगभग सबै ठूला जनावरहरूले जस्ता नरम देखि ए पनि
आक्रमण गर्न सक्छन्।
TARGET
 Auch das Tragen eines Rings ist hilfreich (nur keinen, der zu 
         teuer aussieht
Figure 7: Translation case from Lao to German.
16Figure 8: Translation case from Assamese to English.
Under review as a conference paper at ICLR 2024
SOURCEየዳይነሶር ላባዎች የዳበረ ራቺስ የሚባል ዘንግ ስለሌለው፣ ነገር ግን ሌሎች የላባ
ባህርያት — ባርብስ እና ባርቡልስ — ስላለው ተመራማሪዎች ራቺስ ከእነዚህ ሌሎች
ባህርያት የቆየ ዝግመተ ለውጥ ውጤት እንደሆነ ይላሉ።
TARGET Because the dinosaur feathers do not have a well-developed shaft,
called a rachis, but do have other features of feathers — barbs and
barbules — the researchers inferred the rachis was likely a later
evolutionary development that these other features.
MS Translator Dinosaur feathers developed because it doesn’t have a rod called
rachis, but has other feather traits — barbs and barbules — that
researchers say is the result of older evolution of rachis from these
other traits.
NLLB dinosaur feathers did not develop a shaft called the rachis, but
other feather features, such as barbs and barbels, suggest that
the rachis was the result of an earlier evolution of these other
features.
GPT-4 As there is no known population of the extinct Laysan Rail on
Laysan Island, researchers suggest that the presence of rails on
the other islands—Barbados and Barbuda—indicates a prolonged
period of isolation and change.
SCALE Dinosaur feathers did not develop a shaft called the rachis, how-
ever, other feather features such as barbs and barbules suggest
that the rachis was the result of an earlier evolution of these other
features.
Table 9: Translation case from Amharic to English.
SOURCE बाइसन, एल्क, मूस, भालु र लगभग सबै ठूला जनावरहरूले जस्ता नरम देखि ए पनि
आक्रमण गर्न सक्छन्।
TARGET No matter how docile they may look, bison, elk, moose, bears,
and nearly all large animals can attack.
MS Translator Bison, elk, moose, bears, and almost all large animals can attack
even if they look soft.
NLLB The Bible says: ”The one who is walking with wise persons will
become wise, but the one who is having dealings with the stupid
ones will fare badly”.
GPT-4 Bison, elk, moose, bears, and nearly all large animals, despite
appearing gentle, can be aggressive.
SCALE Bison, elk, moose, bears and nearly all large animals can attack
even though they appear docile.
Table 10: Translation case from Nepali to English.
17
Figure 9: Translation case from Amharic to English.
17

--- PAGE 18 ---
Under review as a conference paper at ICLR 2024
SOURCE बाइसन, एल्क, मूस, भालु र लगभग सबै ठूला जनावरहरूले जस्ता नरम देखि ए पनि
आक्रमण गर्न सक्छन्।
TARGET No matter how docile they may look, bison, elk, moose, bears,
and nearly all large animals can attack.
MS Translator Bison, elk, moose, bears, and almost all large animals can attack
even if they look soft.
NLLB The Bible says: ”The one who is walking with wise persons will
become wise, but the one who is having dealings with the stupid
ones will fare badly”.
GPT-4 Bison, elk, moose, bears, and nearly all large animals, despite
appearing gentle, can be aggressive.
SCALE Bison, elk, moose, bears and nearly all large animals can attack
even though they appear docile.
Table 11: Translation case from Nepali to English.
GPT-4
SCALE
Table 12: Translation case from Nepali to English.
18Es gibt eine Chance, dass es genauso verschwindet, wie es 
aussieht, als ob es einfach verschwindet.
Es ist auch nützlich, einen Ring zu tragen, nur scheint der Ring 
zu teuer zu sein.SOURCE बाइसन, एल्क, मूस, भालु र लगभग सबै ठूला जनावरहरूले जस्ता नरम देखि ए पनि
आक्रमण गर्न सक्छन्।
TARGET
 Auch das Tragen eines Rings ist hilfreich (nur keinen, der zu 
         teuer aussiehtFigure 10: Translation case from Lao to German.
18
