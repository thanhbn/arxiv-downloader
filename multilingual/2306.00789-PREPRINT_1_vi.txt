# 2306.00789.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2306.00789.pdf
# Kích thước file: 580637 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
BẢN THẢO 1
Cải thiện Học chuyển giao Đa ngôn ngữ cho
Dịch thuật Tự động Giọng nói
Sameer Khurana, Nauman Dawalatabad, Antoine Laurent, Luis Vicente, Pablo Gimeno, Victoria Mingote,
James Glass
Tóm tắt —Nghiên cứu về dịch thuật đa ngôn ngữ từ giọng nói sang văn bản là vấn đề đang được quan tâm. Việc có một mô hình duy nhất hỗ trợ nhiều tác vụ dịch thuật là điều mong muốn. Mục tiêu của công trình này là cải thiện học chuyển giao đa ngôn ngữ trong dịch thuật đa ngôn ngữ từ giọng nói sang văn bản thông qua chưng cất kiến thức ngữ nghĩa. Chúng tôi chỉ ra rằng bằng cách khởi tạo bộ mã hóa của mô hình dịch thuật chuỗi-sang-chuỗi encoder-decoder với SAMU-XLS-R, một bộ mã hóa transformer giọng nói đa ngôn ngữ được huấn luyện bằng chưng cất kiến thức ngữ nghĩa đa phương thức (giọng nói-văn bản), chúng tôi đạt được chuyển giao kiến thức tác vụ đa ngôn ngữ tốt hơn đáng kể so với XLS-R cơ sở, một bộ mã hóa transformer giọng nói đa ngôn ngữ được huấn luyện thông qua học tự giám sát. Chúng tôi chứng minh hiệu quả của phương pháp trên hai tập dữ liệu phổ biến, cụ thể là CoVoST-2 và Europarl. Trên 21 tác vụ dịch thuật của bộ đánh giá CoVoST-2, chúng tôi đạt được cải thiện trung bình 12.8 điểm BLEU so với các mô hình cơ sở. Trong kịch bản dịch thuật zero-shot, chúng tôi đạt được mức tăng trung bình 18.8 và 11.9 điểm BLEU trung bình trên các ngôn ngữ tài nguyên trung bình và thấp chưa được thấy. Chúng tôi quan sát tương tự trên bộ đánh giá dịch thuật giọng nói Europarl.
Từ khóa Chỉ mục —Học Chuyển giao Đa ngôn ngữ, Dịch thuật Tự động Giọng nói, Biểu diễn Giọng nói Đa ngôn ngữ Được căn chỉnh Ngữ nghĩa

I. GIỚI THIỆU
Học Biểu diễn Tự giám sát (SSRL) từ giọng nói [1]–[18] đã cải thiện rất nhiều trong vài năm qua do việc giới thiệu Mã hóa Dự đoán Tương phản (CPC) [19], một phương pháp học biểu diễn tự giám sát được áp dụng cho dữ liệu giọng nói, văn bản và hình ảnh. Việc giới thiệu ý tưởng cốt lõi của ước lượng tương phản nhiễu [20] trong CPC đã dẫn đến một loạt các bài báo trong SSRL giọng nói, như Wav2Vec [5], VQ-Wav2Vec [6], Wav2Vec-2.0 [9], Multilingual Wav2Vec-2.0 [11], XLS-R (XLS-R, một phiên bản lớn hơn của multilingual wav2vec-2.0) [15]. Các bộ mã hóa giọng nói SSRL được huấn luyện trước như XLS-R được coi là "mô hình nền tảng" [21] cho các ứng dụng xử lý giọng nói đa ngôn ngữ downstream như Nhận dạng Giọng nói Tự động Đa ngôn ngữ [11], [15], [22], Dịch thuật Giọng nói Đa ngôn ngữ [15], [18], [23], và các tác vụ dự đoán thuộc tính para-ngôn ngữ khác [24], [25]. Công trình này tập trung vào Dịch thuật Giọng nói Đa ngôn ngữ.

Dịch thuật Giọng nói Đa ngôn ngữ (MST) đề cập đến việc dịch giọng nói trong tất cả các ngôn ngữ nguồn trong tập X sang văn bản trong tất cả các ngôn ngữ đích trong tập Y, điều này ngụ ý tổng cộng |T| = |X|×|Y| tác vụ dịch thuật. Trong MST, chúng tôi huấn luyện một mô hình duy nhất cho tất cả các tác vụ dịch thuật được đưa ra bởi tập T. Lợi ích của việc có một mô hình duy nhất thay vì mô hình riêng lẻ cho mỗi tác vụ t∈T có hai mặt: Thứ nhất, việc duy trì và chia sẻ một mô hình duy nhất có thể thực hiện nhiều tác vụ thuận tiện hơn việc có |T| mô hình riêng biệt, và thứ hai, việc chia sẻ tham số mô hình giữa |T| tác vụ dịch thuật có thể dẫn đến chuyển giao kiến thức giữa các tác vụ, đặc biệt từ tài nguyên cao sang tài nguyên thấp.

Kiến trúc mạng neural tiêu chuẩn được sử dụng cho MST là mô hình encoder-decoder [26], [27]. Gần đây, MST đã thấy những cải thiện đáng kể nhờ; (i) khởi tạo tốt hơn cho encoder và decoder của mô hình dịch thuật với các bộ mã hóa giọng nói được huấn luyện trước, như XLS-R[15], và các bộ giải mã văn bản, như MBART [28], (ii) các chiến lược tinh chỉnh tốt hơn [23], và (iii) các corpus dịch thuật giọng nói-văn bản song song [29], [30]. Tuy nhiên, như chúng tôi chứng minh trong Phần II, hiệu suất trên các tác vụ tài nguyên thấp vẫn còn kém, và đặc biệt, khoảng cách hiệu suất (khoảng cách chuyển giao đa ngôn ngữ) giữa các ngôn ngữ tài nguyên cao và thấp vẫn còn lớn. Chúng tôi giả thuyết rằng điều này là do bộ mã hóa giọng nói XLS-R học các đặc trưng bề mặt không bền vững từ dữ liệu giọng nói không gán nhãn, thay vì kiến thức ngôn ngữ học cấp cao về ngữ nghĩa.

Để tiêm kiến thức ngữ nghĩa vào các biểu diễn XLS-R đã học, chúng tôi chuyển sang khung Học Biểu diễn Đa ngôn ngữ Đa phương thức Được căn chỉnh Ngữ nghĩa được giới thiệu gần đây, SAMU-XLS-R[17]. SAMU-XLS-R (Phần III-B) là một khung chưng cất kiến thức (KD) chưng cất kiến thức ngữ nghĩa từ một mô hình nhúng văn bản được huấn luyện trước vào bộ mã hóa giọng nói đa ngôn ngữ XLS-R được huấn luyện trước. Khung KD xuất ra bộ mã hóa giọng nói SAMU-XLS-R học một đa tạp giọng nói đa ngôn ngữ có cấu trúc ngữ nghĩa, nơi một phát ngôn được nói gần với các bản dịch được nói của nó trong một số ngôn ngữ khác có mặt trong pool huấn luyện của SAMU-XLS-R (Chi tiết trong Phần III-B). Chúng tôi khẳng định rằng việc xây dựng mô hình MST sử dụng các biểu diễn ngữ nghĩa được học bởi SAMU-XLS-R sẽ dẫn đến chuyển giao đa ngôn ngữ tốt hơn từ các tác vụ tài nguyên cao sang thấp so với XLS-R và các baseline bộ mã hóa giọng nói đa ngôn ngữ không ngữ nghĩa khác. Chúng tôi xác minh khẳng định của mình thông qua một số thí nghiệm (Phần VI). Thông qua công trình này, chúng tôi đóng góp những điều sau:

• Chúng tôi đã tăng gấp đôi số lượng ngôn ngữ được hỗ trợ trước đây bởi bộ mã hóa SAMU-XLS-R từ 25 lên 53 (Phần IV-A).

• Bộ mã hóa SAMU-XLS-R cung cấp nhúng giọng nói ở cấp độ phát ngôn. Các công trình trước [31] đã khám phá việc sử dụng nhúng SAMU-XLS-R cho hiểu ngôn ngữ nói tự động và truy xuất ngữ nghĩa giọng nói-văn bản và giọng nói-giọng nói [17]. Khác biệt, trong công trình này, lần đầu tiên, chúng tôi chỉ ra rằng các nhúng ngữ cảnh chi tiết (tương ứng với đoạn giọng nói dài 20ms) đi trước biểu diễn cấp độ phát ngôn thô (đoạn giọng nói dài 3-10s) được học bởi SAMU-XLS-R rất phù hợp cho tác vụ sinh chuỗi của dịch thuật giọng nói đa ngôn ngữ. Chúng tôi chứng minh thực nghiệm, thông qua một số thí nghiệm, tính ưu việt của các biểu diễn SAMU-XLS-R so với XLS-R và các bộ mã hóa giọng nói đa ngôn ngữ khác (Phần VI).

• Trên bộ đánh giá MST X → English CoVoST-2 công khai [30], chúng tôi chỉ ra rằng bằng cách chuyển bộ mã hóa XLS-R trong mô hình MST sang bộ mã hóa SAMU-XLS-R, hiệu suất cải thiện đáng kể trên nhóm ngôn ngữ X → English tài nguyên trung bình 15.6 điểm BLEU, và trên nhóm ngôn ngữ tài nguyên thấp 18.9 điểm BLEU, và tổng thể 13.8 điểm BLEU (Phần VI-A).

• Chúng tôi cũng chỉ ra hiệu quả của bộ mã hóa giọng nói SAMU-XLS-R trong các thiết lập dịch thuật Zero-Shot. Trong kịch bản MST Zero-Shot, nơi chúng tôi huấn luyện mô hình MST dựa trên SAMU-XLS-R chỉ trên các tác vụ X→English tài nguyên cao trong CoVoST-2, chúng tôi quan sát được cải thiện trên các tác vụ tài nguyên trung bình và thấp lần lượt là 18.8 và 11.9 điểm BLEU so với baseline XLS-R (Phần VI-B).

• Cuối cùng, chúng tôi mở rộng các nghiên cứu của mình trên một bộ đánh giá MST khác, cụ thể là Europarl [29], bao gồm một số tác vụ dịch thuật X→Y. Trong kịch bản MST zero-shot, chúng tôi quan sát được cải thiện tổng thể 8.5 điểm BLEU trung bình với bộ mã hóa SAMU-XLS-R so với XLS-R, nhờ vào sự gia tăng đáng kể trong hiệu suất trên các ngôn ngữ nguồn chưa được thấy (trong quá trình huấn luyện) là 17 điểm BLEU (Phần VI-C).

II. ĐỘNG LỰC: KHOẢNG CÁCH CHUYỂN GIAO ĐA NGÔN NGỮ

Để tạo động lực cho công việc của chúng tôi, chúng tôi chỉ ra hiệu suất của bộ mã hóa giọng nói đa ngôn ngữ XLS-R trên bộ đánh giá dịch thuật giọng nói sang văn bản CoVoST-2 [30]. CoVoST-2 bao gồm 21 tác vụ dịch thuật giọng nói sang văn bản X→EN, nơi X đề cập đến ngôn ngữ của phát ngôn giọng nói, và EN đề cập đến bản dịch văn bản tiếng Anh tương ứng. Bộ mã hóa giọng nói XLS-R được huấn luyện trước thông qua Học Tự giám sát sử dụng dữ liệu giọng nói không gán nhãn trong 128 ngôn ngữ. Babu et al. [15] tinh chỉnh bộ mã hóa XLS-R được huấn luyện trước kết hợp với bộ giải mã văn bản MBART được huấn luyện trước [28] đồng thời trên 21 tác vụ dịch thuật X→EN (học đa tác vụ) trong bộ đánh giá CoVoST-2. Chúng tôi phân loại 21 tác vụ dịch thuật thành các nhóm tài nguyên cao, trung bình và thấp. Một tác vụ được phân loại là cao nếu nó có hơn 100 giờ dữ liệu huấn luyện dịch thuật giọng nói(X)-văn bản(EN) được ghép cặp, trung bình nếu dữ liệu huấn luyện từ 10 đến 100 giờ, và thấp nếu dữ liệu huấn luyện ít hơn 10 giờ. Có bốn tác vụ tài nguyên cao, năm tác vụ tài nguyên trung bình, và 12 tác vụ tài nguyên thấp.

Chúng tôi báo cáo hiệu suất của mô hình transformer XLS-R(bộ mã hóa giọng nói) →MBART (bộ giải mã văn bản) trên các nhóm dịch thuật tài nguyên cao, trung bình và thấp trong Hình 1. Chúng tôi báo cáo điểm BLEU-4 trung bình trên mỗi nhóm dịch thuật. Điều quan trọng cần quan sát là khoảng cách hiệu suất (khoảng cách chuyển giao đa ngôn ngữ) giữa các nhóm dịch thuật tài nguyên cao và thấp cho các bộ mã hóa XLS-R có kích thước khác nhau từ 300M đến 2B

--- TRANG 2 ---
BẢN THẢO 2

tham số. Vậy, việc tăng kích thước mô hình từ 300M lên 2B, tăng hơn 500% chỉ dẫn đến giảm 16% khoảng cách chuyển giao. Vì mô hình dịch thuật được xây dựng dựa trên các biểu diễn của bộ mã hóa XLS-R được huấn luyện trước, có một thành phần bị thiếu trong các biểu diễn được huấn luyện trước dẫn đến chuyển giao đa ngôn ngữ kém từ các tác vụ tài nguyên cao sang thấp. Điều này ngụ ý rằng kiến thức mà mô hình dịch thuật dựa trên XLS-R thu được trong khi học thực hiện các tác vụ dịch thuật X→EN tài nguyên cao không hữu ích (hoặc có thể chuyển giao) cho việc học các tác vụ tài nguyên thấp.

Chúng tôi khẳng định rằng phần bị thiếu là kiến thức ngữ nghĩa. Chúng tôi giả thuyết rằng vì SAMU-XLS-R được huấn luyện đặc biệt để mã hóa thông tin ngữ nghĩa trong các biểu diễn nội bộ của nó, việc xây dựng mô hình MST sử dụng các biểu diễn ngữ nghĩa được học bởi SAMU-XLS-R sẽ dẫn đến chuyển giao đa ngôn ngữ tốt hơn từ các tác vụ dịch thuật tài nguyên cao sang thấp, do đó giảm khoảng cách chuyển giao đa ngôn ngữ được đề cập ở trên.

III. KIẾN THỨC CƠ BẢN

A. XLS-R (XLS-R)

Phần này thảo luận về kiến trúc của bộ mã hóa XLS-R. Bộ mã hóa XLS-R bao gồm một bộ trích xuất đặc trưng Mạng Neural Tích chập (CNN) [33], ký hiệu là h, và một bộ mã hóa transformer [27], ký hiệu là g.

1) Bộ Trích xuất Đặc trưng CNN: h: a1:S → f1:T ánh xạ dạng sóng giọng nói (a1:S|as ∈ R) thành một biểu diễn trung gian (f1:T|ft ∈ Rd), nơi T = S/r, với r là hệ số mà h lấy mẫu xuống a1:S, và d là kích thước của chiều đặc trưng. Đối với bộ mã hóa XLS-R, r = 320, và d = 1024. Bộ trích xuất đặc trưng h bao gồm bảy lớp Tích chập thời gian (CONV). Mỗi lớp sử dụng hàm kích hoạt GeLU [34]. Các bản đồ đặc trưng được xuất ra bởi mỗi lớp có

--- TRANG 3 ---
BẢN THẢO 3

512 kênh. Đầu ra của lớp CONV cuối cùng trong h được chiếu lên bằng một lớp kết nối đầy đủ (FC) để vector đặc trưng ft có cùng chiều với bộ mã hóa transformer. Lưu ý rằng trường tiếp nhận của mỗi vector đặc trưng là 20ms của dạng sóng giọng nói đầu vào. Bộ trích xuất đặc trưng được theo sau bởi một lớp CONV khác mã hóa vị trí tương đối của chuỗi đặc trưng f1:T, được sử dụng như đầu vào cho bộ mã hóa transformer g.

2) Bộ Mã hóa Transformer: g: f1:T → c1:T ánh xạ (f1:T|ft ∈ Rd) thành biểu diễn có ngữ cảnh (c1:T|ct ∈ Rd). Trường tiếp nhận của mỗi vector ngữ cảnh ct bằng độ dài của dạng sóng giọng nói đầu vào a1:S, thường là 5-10s. Bộ mã hóa transformer bao gồm 24 lớp. Mỗi lớp transformer bao gồm Multi-Headed Self-Attention (MHSA), theo sau là hai lớp FC. Layer Normalization (LN) được sử dụng ở đầu vào của các khối MHSA và FC. Kết nối dư cộng đầu vào của MHSA với đầu ra của nó, và đầu vào của khối FC với đầu ra của nó. Chiều lớp bộ mã hóa transformer là 1024 (kích thước của vector đặc trưng được xuất ra bởi một lớp transformer), và chiều lớp FC là 3072. Mỗi khối MHSA bao gồm 16 đầu attention. Đầu ra của lớp FC đầu tiên được xử lý bởi phi tuyến ReLU [35].

3) Chi tiết Huấn luyện: XLS-R được huấn luyện trước trên khoảng 400K giờ dữ liệu giọng nói đa ngôn ngữ không gán nhãn được phân đoạn thành các phát ngôn có kích thước 3-10s. Dữ liệu huấn luyện bao gồm giọng nói không gán nhãn trong 128 ngôn ngữ đến từ các corpus sau: VoxPopuli [36], CommonVoice (CoVo) [37], Multilingual Speech (MLS) [38], BABEL, và Voxlingua [39]. Để biết chi tiết huấn luyện đầy đủ, xem [15]. Chúng tôi sử dụng các checkpoint XLS-R được huấn luyện trước được phát hành công khai¹.

Như baseline cho các thí nghiệm dịch thuật giọng nói đa ngôn ngữ X→EN (Phần VI-A), chúng tôi sử dụng các mô hình dịch thuật XLS-R→MBART được huấn luyện trên bộ đánh giá MST X→EN CoVoST-2, được phát hành chính thức tại đây². Phiên bản phát hành chính thức bao gồm ba mô hình dịch thuật tương ứng với các bộ mã hóa giọng nói XLS-R 0.3B, 1B, và

¹https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec/xlsr
²https://huggingface.co/facebook/wav2vec2-xls-r-300m-21-to-en

2B tham số. Bộ giải mã văn bản trong cả ba trường hợp là bộ giải mã MBART 400M tham số.

B. SAMU-XLS-R (SAMU-XLS-R)

Đa tạp biểu diễn của bộ mã hóa XLS-R mã hóa kiến thức ngôn ngữ học cấp thấp, như được chứng minh bởi khả năng truy xuất giọng nói ngữ nghĩa đa ngôn ngữ không tồn tại khi sử dụng các biểu diễn đã học của XLS-R trong [17]. SAMU-XLS-R là một khung chưng cất kiến thức (KD) song phương thức (giọng nói & văn bản) để mở rộng không gian biểu diễn của bộ mã hóa giọng nói XLS-R cũng để mã hóa kiến thức ngữ nghĩa cấp cao. Khung học SAMU-XLS-R sử dụng tập giọng nói đa ngôn ngữ có phiên âm D = {aⁿ₁:S, yⁿ₁:L}ᴺᵢ₌₁ bao gồm giọng nói, aⁱ₁:S được ghép cặp với phiên âm văn bản yⁱ₁:L của nó. Lưu ý rằng giọng nói và phiên âm của nó cùng ngôn ngữ. Khung KD SAMU-XLS-R bao gồm một nhánh xử lý giọng nói và văn bản như được minh họa trong Hình 2, và được mô tả chi tiết dưới đây.

1) Nhánh Giọng nói: Nhánh giọng nói ánh xạ dạng sóng giọng nói a₁:S có độ dài 3-10s thành một vector nhúng duy nhất e ∈ R⁷⁶⁸. Việc ánh xạ được thực hiện trong hai bước sau: (i) Bộ mã hóa giọng nói XLS-R được huấn luyện trước ánh xạ a₁:S thành biểu diễn có ngữ cảnh c₁:T, và (ii) Một pooling thời gian dựa trên attention [40], và một lớp chiếu xuống phi tuyến (tanh) biến đổi c₁:T thành một nhúng duy nhất e có kích thước 768.

2) Nhánh Văn bản: Nhánh văn bản bao gồm một bộ mã hóa nhúng câu LaBSE được huấn luyện trước [32] biến đổi phiên âm văn bản y₁:L thành một nhúng ngữ nghĩa z ∈ Rᵈ, có kích thước d = 768. LaBSE hỗ trợ 109 ngôn ngữ viết. Nó nhúng các câu từ các ngôn ngữ khác nhau trong một không gian nhúng ngữ nghĩa được chia sẻ, tức là, một câu và bản dịch của nó nằm gần nhau trong không gian nhúng LaBSE. Bằng cách hồi quy trên nhúng ngữ nghĩa LaBSE, bộ mã hóa giọng nói XLS-R học mã hóa ngữ nghĩa ẩn trong phát ngôn được nói trong các biểu diễn nội bộ của nó. Bộ mã hóa giọng nói nhận thức ngữ nghĩa mới này được gọi là bộ mã hóa SAMU-XLS-R.

3) Chi tiết Huấn luyện.: Bộ mã hóa SAMU-XLS-R được huấn luyện để tối thiểu hóa khoảng cách cosine giữa nhúng giọng nói và

--- TRANG 4 ---
BẢN THẢO 4

văn bản e, và z tương ứng (xem Hình 2). Hàm mất mát được cho bởi:

L = β * (1.0 - e·z / (||e|| ||z||))     (1)

Trong đó β được sử dụng để tăng quy mô độ lớn của mất mát khoảng cách cosine, một độ lớn mất mát nhỏ có thể dẫn đến các giá trị cập nhật gradient cực kỳ nhỏ và thời gian huấn luyện dài hơn đáng kể. Việc tăng quy mô mất mát bằng một hằng số β có thể giảm thiểu vấn đề này. Các tham số của nhánh giọng nói được điều chỉnh để tối thiểu hóa mất mát L đã đề cập ở trên. Bộ mã hóa văn bản LaBSE vẫn cố định trong quá trình huấn luyện cùng với bộ trích xuất đặc trưng h của bộ mã hóa XLS-R được huấn luyện trước (Xem Hình 2 và [17] để biết chi tiết huấn luyện).

IV. PHƯƠNG PHÁP

A. Mở rộng SAMU-XLS-R

Chúng tôi tăng gấp đôi số lượng ngôn ngữ được hỗ trợ bởi SAMU-XLS-R lên 53 so với 25 được hỗ trợ trước đây. Để huấn luyện SAMU-XLS-R, chúng tôi sử dụng các phát ngôn giọng nói từ nhiều ngôn ngữ được chú thích với phiên âm văn bản của chúng trong cùng ngôn ngữ. Chúng tôi gọi pool dữ liệu huấn luyện của SAMU-XLS-R là dữ liệu Nền (BKG). Dữ liệu BKG phụ thuộc vào miền tác vụ MST downstream. Lý tưởng nhất, chúng tôi muốn dữ liệu BKG từ cùng miền với tác vụ MST (trong miền). Chúng tôi sử dụng giọng nói có phiên âm đa ngôn ngữ từ corpus CommonVoice-Version8 (CoVo-V8) [37]. CoVo-V8 bao gồm giọng nói có phiên âm trong 87 ngôn ngữ (26 họ ngôn ngữ). Khoảng 53 ngôn ngữ trùng lặp với tập ngôn ngữ được hỗ trợ bởi Language-Agnostic BERT Sentence Encoder (LaBSE), cung cấp giám sát ngữ nghĩa để huấn luyện bộ mã hóa giọng nói SAMU-XLS-R. 53 ngôn ngữ là: Tiếng Ả Rập (ar), Tiếng Belarus (be), Tiếng Bulgaria (bg), Tiếng Catalan (ca), Tiếng Séc (cs), Tiếng Wales (cy), Tiếng Đan Mạch (da), Tiếng Đức (de), Tiếng Hy Lạp (el), Tiếng Anh (en), Tiếng Estonia (eo), Tiếng Tây Ban Nha (es), Tiếng Estonia (et), Tiếng Basque (eu), Tiếng Ba Tư (fa), Tiếng Phần Lan (fi), Tiếng Pháp (fr), Tiếng Frisia Tây (fy-NL), Tiếng Gaelic (ga-IE), Tiếng Galician (gl), Tiếng Hausa (ha), Tiếng Hindi (hi), Tiếng Hungary (hu), Tiếng Indonesia (id), Tiếng Ý (it), Tiếng Nhật (ja), Tiếng Georgia (ka), Tiếng Khmer (kmr), Tiếng Kyrgyz (ky), Tiếng Lithuania (lt), Tiếng Latvia (lv), Tiếng Mông Cổ (mn), Tiếng Malta (mt), Tiếng Hà Lan (nl), Tiếng Ba Lan (pl), Tiếng Bồ Đào Nha (pt), Tiếng Romania (ro), Tiếng Nga (ru), Tiếng Kinyarwanda (rw), Tiếng Slovak (sk), Tiếng Slovenia (sl), Tiếng Thụy Điển (sv-SE), Tiếng Swahili (sw), Tiếng Tamil (ta), Tiếng Thái (th), Tiếng Thổ Nhĩ Kỳ (tr), Tiếng Tatar (tt), Tiếng Uyghur (ug), Tiếng Ukraine (uk), Tiếng Uzbek (uz), Tiếng Việt (vi), Tiếng Trung-Quan Thoại (zh-CN), Tiếng Trung-Hồng Kông (zh-HK), Tiếng Trung-Đài Loan (zh-TW). Bảng I trình bày số giờ giọng nói có phiên âm có sẵn trong mỗi 53 ngôn ngữ để huấn luyện SAMU-XLS-R. Tổng số giờ huấn luyện là 12.7K. Dữ liệu BKG rất mất cân bằng, nơi phần lớn dữ liệu đến từ một vài ngôn ngữ tài nguyên cao. Theo [17, Phương trình 3], chúng tôi cân bằng dữ liệu BKG để có đại diện tỷ lệ thuận của các ngôn ngữ khác nhau trong một mini-batch huấn luyện, tránh việc under-fitting với các ngôn ngữ tài nguyên thấp. Quá trình up-sampling liên quan đến việc lặp lại các phát ngôn trong các ngôn ngữ tài nguyên thấp. Không giống như công việc trước đây, chúng tôi sử dụng tăng cường dữ liệu offline, cụ thể là nhiễu loạn tốc độ [41], để cải thiện tính bền vững của các biểu diễn đã học đối với sự thay đổi phân phối dữ liệu.

B. Mô hình Dịch thuật

1) Tổng quan: Chúng tôi sử dụng kiến trúc encoder-decoder tiêu chuẩn cho mô hình dịch thuật của mình. Chúng tôi khởi tạo encoder bằng SAMU-XLS-R được huấn luyện trước. Theo [23], decoder được khởi tạo với decoder của một mô hình dịch thuật văn bản-sang-văn bản được huấn luyện trước, cụ thể là MBART³. Mô hình encoder-decoder được huấn luyện sử dụng các corpus bao gồm các tuple (a₁:S, y₁:L), nơi y₁:L là chuỗi dịch thuật văn bản của chuỗi giọng nói a₁:S.

2) Huấn luyện: Cho một cặp dịch thuật giọng nói-văn bản (a₁:S, y₁:L), chúng tôi điều chỉnh các tham số của mô hình dịch thuật để tối đa hóa log-xác suất của y₁:L có điều kiện trên phát ngôn giọng nói a₁:S (log-likelihood). Ngoài ra, chúng tôi sử dụng làm mịn nhãn [42], [43], nơi một nhãn sự thật nền yl được thay thế ngẫu nhiên bằng nhãn được dự đoán bởi mô hình ŷl. Chúng tôi đặt xác suất thay thế token là 0.3. Chúng tôi sử dụng bộ tối ưu Adam [44] để tối đa hóa log-likelihood với

³https://github.com/facebookresearch/fairseq/tree/main/examples/multilingual#mbart50-models

--- TRANG 5 ---
BẢN THẢO 5

tốc độ học (lr) tối ưu (theo tập phát triển) là 5e-4. Chúng tôi sử dụng bộ lập lịch tốc độ học ba pha [9] như sau; (i) khởi động lr lên 5e-4 cho 10% đầu tiên của các lần lặp huấn luyện, (ii) giữ lr không đổi cho 40% lần lặp tiếp theo, và (iii) giảm lr tuyến tính cho phần còn lại của huấn luyện. Chúng tôi sử dụng 28K lần lặp huấn luyện và huấn luyện mô hình trên 8 GPU A100. Một lần lặp huấn luyện duy nhất sử dụng batch size là 10 phút các phát ngôn giọng nói được ghép cặp với bản dịch văn bản của chúng. Chúng tôi sử dụng kiểu huấn luyện mixed-precision [45]; hầu hết các tính toán được thực hiện trên số dấu phẩy động nửa độ chính xác ngoại trừ tính toán mất mát cuối cùng. Chúng tôi sử dụng toolkit fairseq [46] để huấn luyện mô hình.

Theo [9], chúng tôi che (chiều thời gian và đặc trưng) chuỗi đặc trưng f₁:T, đầu ra bởi bộ trích xuất đặc trưng CNN của SAMU-XLS-R. Quá trình che được thực hiện trong hai bước sau: (i) với một xác suất nào đó, được gọi là xác suất che, chọn một chỉ số che, và (ii) che M chỉ số liên tiếp bắt đầu từ chỉ số được chọn. M được biết đến như độ dài che. Đối với chiều thời gian, chúng tôi đặt 0.3 như xác suất che và độ dài che là sáu. Đối với chiều đặc trưng, chúng tôi đặt xác suất che là 0.5 và 64 như độ dài che. Các tham số che thời gian và đặc trưng được chọn theo tập phát triển. Tăng cường dữ liệu đã đề cập ở trên tương tự như SpecAugment [47], một phương pháp tăng cường dữ liệu phổ đồ giọng nói thường được sử dụng để huấn luyện các mô hình sinh văn bản từ giọng nói.

Mô hình dịch thuật bao gồm 700 triệu tham số có thể huấn luyện (300M tham số encoder và 400M tham số decoder). Chúng tôi chỉ tinh chỉnh 75 triệu tham số. Hầu hết các tham số encoder được cố định ở các giá trị SAMU-XLS-R được huấn luyện trước, và các tham số decoder được cố định ở bộ giải mã MBART được huấn luyện trước. Dưới đây chúng tôi đưa ra chi tiết về tinh chỉnh encoder và decoder.

3) Tinh chỉnh Encoder: Chúng tôi giữ tất cả các tham số của bộ mã hóa giọng nói cố định ở các giá trị SAMU-XLS-R được huấn luyện trước của chúng. Thay vào đó, chúng tôi thêm một số lượng nhỏ các tham số đặc thù cho tác vụ downstream vào mỗi lớp transformer của bộ mã hóa giọng nói theo [48]. Chúng tôi chèn các adapter, một lớp Feed-Forward thắt cổ chai, sau các khối Multi-Headed Self-Attention (MHSA) và Fully-Connected (FC) trong mỗi lớp transformer. Một adapter bao gồm một lớp ẩn duy nhất với kích hoạt ReLU. Các lớp đầu vào và đầu ra có cùng kích thước. Ngược lại, lớp ẩn là một phần của kích thước lớp đầu vào. Chúng tôi tìm thấy kích thước tối ưu (theo tập phát triển) của lớp ẩn là một phần tư của kích thước lớp đầu vào.

Động lực để sử dụng adapter có hai mặt: (i) Hiệu quả Tham số: Chúng tôi chỉ điều chỉnh các tham số lớp adapter, một phần của tổng số tham số bao gồm bộ mã hóa giọng nói. Trong trường hợp của chúng tôi, bộ mã hóa SAMU-XLS-R bao gồm 300M tham số, và chúng tôi thêm 75M tham số adapter đặc thù cho tác vụ. Do đó, chúng tôi chỉ tinh chỉnh 75M tham số trên tác vụ downstream. (ii) Tránh Quên Kiến thức: Bằng cách đóng băng các tham số của encoder ở các giá trị được huấn luyện trước, chúng tôi bảo tồn kiến thức ngữ nghĩa cần thiết được mã hóa trong bộ mã hóa SAMU-XLS-R mà nó đã thu được trong giai đoạn huấn luyện trước chưng cất kiến thức ngữ nghĩa. Chúng tôi sau đó chỉ ra (Phần VI-D2) rằng việc bảo tồn kiến thức ngữ nghĩa này là cần thiết để đạt được chuyển giao đa ngôn ngữ tốt. Do đó, các adapter tạo thành một phần quan trọng trong chiến lược học chuyển giao đa ngôn ngữ của chúng tôi. Cần lưu ý rằng nghiên cứu về adapter đang mở rộng liên tục. Một phân tích sâu về các lựa chọn adapter cụ thể về hiệu suất tác vụ downstream nằm ngoài phạm vi của bài báo này.

4) Tinh chỉnh Decoder: Giống như encoder, chúng tôi giữ hầu hết các tham số cố định ở các giá trị được huấn luyện trước của chúng được thu được trong tác vụ dịch thuật văn bản-sang-văn bản đa ngôn ngữ. Không giống như encoder, chúng tôi không giới thiệu bất kỳ tham số đặc thù cho tác vụ mới nào. Thay vào đó, chúng tôi điều chỉnh một phần các tham số của decoder. Mỗi lớp trong transformer decoder bao gồm Masked (causal) Multi-Headed Self-Attention (MMHSA), Encoder-Decoder Cross-Attention (CA), và hai khối Fully-Connected (FC). Các đầu vào cho các khối MMHSA, CA, và FC được chuẩn hóa thông qua phương pháp Layer Normalization (LN) được trình bày trong [49]. Theo [23], chúng tôi chỉ điều chỉnh các tham số của các khối LN và CA. Chúng tôi tinh chỉnh CA bởi vì, trước đây, nó được huấn luyện như một phần của decoder trong một pipeline dịch thuật văn bản-sang-văn bản. Do đó, mô-đun CA cần được tinh chỉnh lại cho tác vụ MST downstream để làm cho nó phù hợp với đầu vào từ bộ mã hóa giọng nói. Hơn nữa, chúng tôi tinh chỉnh LN bởi vì nó là đặc thù cho tác vụ và tập dữ liệu và cải thiện hiệu suất của MST một cách thực nghiệm [23].

5) Suy luận: Chúng tôi sử dụng tìm kiếm chùm tia, với kích thước chùm tia là 5, để tạo ra bản dịch văn bản cho một phát ngôn giọng nói đã cho. Quá trình suy luận là offline, tức là, decoder xem xét toàn bộ phát ngôn giọng nói đầu vào để tạo ra bản dịch đầu ra. Decoder tạo ra bản dịch theo cách tự hồi quy. Chúng tôi không sử dụng bất kỳ mô hình ngôn ngữ bên ngoài nào trong quá trình suy luận.

V. ĐÁNH GIÁ

A. Các Kịch bản Dịch thuật

Chúng tôi giải quyết các kịch bản dịch thuật sau trong công trình này.

1) Dịch thuật Đa ngôn ngữ: Chúng tôi đồng thời huấn luyện một mô hình dịch thuật trên một số tác vụ dịch thuật giọng nói-sang-văn bản trong kịch bản này. Ví dụ, chúng tôi huấn luyện một mô hình duy nhất cho tất cả 21 tác vụ dịch thuật X→EN trong bộ đánh giá CoVoST-2. Hầu hết dữ liệu huấn luyện đến từ một vài tác vụ dịch thuật tài nguyên cao như FR→EN và DE→EN, trong khi hầu hết các tác vụ như ID→EN đều tài nguyên thấp. Trong kịch bản này, chúng tôi so sánh các mô hình dịch thuật khác nhau để kiểm tra chuyển giao tác vụ dịch thuật đa ngôn ngữ từ các tác vụ dịch thuật tài nguyên cao sang thấp.

2) Dịch thuật Đa ngôn ngữ Zero-Shot: Cho một tập các tác vụ dịch thuật, chúng tôi huấn luyện một mô hình dịch thuật trên một tập con của các tác vụ trong khi giữ phần còn lại ẩn trong quá trình huấn luyện trong kịch bản dịch thuật này. Ví dụ, chúng tôi huấn luyện một mô hình dịch thuật giọng nói-sang-văn bản X→EN sử dụng các tác vụ dịch thuật tài nguyên cao trong bộ đánh giá CoVoST-2 trong khi giữ các tác vụ tài nguyên trung bình và thấp không được thấy trong quá trình huấn luyện. Chúng tôi so sánh các mô hình dịch thuật cho chuyển giao tác vụ đa ngôn ngữ zero-shot trong kịch bản này từ các tác vụ dịch thuật X→EN tài nguyên cao sang trung bình và thấp.

B. Các Tác vụ Dịch thuật

Chúng tôi xây dựng các mô hình dịch thuật để giải quyết các tác vụ dịch thuật sau trong công trình này.

--- TRANG 6 ---
BẢN THẢO 6

1) Dịch thuật Giọng nói-sang-Văn bản X→EN: Chúng tôi xây dựng các mô hình dịch thuật cho 21 tác vụ dịch thuật X(Giọng nói)→EN(Văn bản) trong bộ đánh giá dịch thuật CoVoST-2 [30]. 21 ngôn ngữ nói trong CoVoST-2 là Tiếng Pháp (fr), Tiếng Đức (de), Tiếng Tây Ban Nha (es), Tiếng Catalan (ca), Tiếng Ý (it), Tiếng Nga (ru), Tiếng Trung (zh), Tiếng Bồ Đào Nha (pt), Tiếng Ba Tư (fa), Tiếng Estonia (et), Tiếng Mông Cổ (mn), Tiếng Hà Lan (nl), Tiếng Thổ Nhĩ Kỳ (tr), Tiếng Ả Rập (ar), Tiếng Thụy Điển (sv), Tiếng Latvia (lv), Tiếng Slovenia (sl), Tiếng Tamil (ta), Tiếng Nhật (ja), Tiếng Indonesia (id), và Tiếng Wales (cy). 21 tác vụ dịch thuật được chia thành các nhóm tài nguyên cao, trung bình và thấp, tùy thuộc vào lượng dữ liệu có nhãn có sẵn cho một tác vụ dịch thuật. Các tác vụ tài nguyên cao có hơn 100 giờ dữ liệu huấn luyện có nhãn, các tác vụ tài nguyên trung bình có từ 10 đến 100 giờ, và các tác vụ tài nguyên thấp có ít hơn mười giờ dữ liệu huấn luyện có nhãn. CoVoST-2 có bốn tác vụ dịch thuật tài nguyên cao tương ứng với các ngôn ngữ nguồn fr, de, es, và ca và năm tác vụ tài nguyên trung bình tương ứng với các ngôn ngữ it, ru, zh, pt, và fa. Phần còn lại của các tác vụ là tài nguyên thấp. Hình 3 trình bày dữ liệu huấn luyện cho mỗi trong số 21 tác vụ dịch thuật. Chú ý sự mất cân bằng dữ liệu giữa các tác vụ khác nhau.

Trong kịch bản dịch thuật đa ngôn ngữ, chúng tôi đồng thời huấn luyện các mô hình dịch thuật trên tất cả 21 tác vụ đã đề cập ở trên. Chúng tôi chỉ huấn luyện các mô hình dịch thuật trên bốn tác vụ tài nguyên cao trong kịch bản zero-shot.

2) Dịch thuật Giọng nói-sang-Văn bản X→Y: Chúng tôi phát triển các mô hình dịch thuật cho 72 tác vụ dịch thuật X→Y trong bộ đánh giá Europarl [29]. Có chín ngôn ngữ nói trong Europarl cụ thể là, en, fr, de, it, es, pt, pl, ro, và nl. Các phát ngôn giọng nói trong mỗi ngôn ngữ nói được ghép cặp với bản dịch văn bản tương ứng của chúng trong tám ngôn ngữ khác. Trong kịch bản zero-shot, chúng tôi huấn luyện các mô hình dịch thuật trên 32 tác vụ tương ứng với bốn ngôn ngữ nguồn sau fr, de, es, và it. Mỗi trong số bốn ngôn ngữ nguồn được ghép cặp với tám ngôn ngữ đích. Bảng II hiển thị dữ liệu huấn luyện có nhãn của mỗi tác vụ dịch thuật.

C. Các Mô hình Dịch thuật

1) SAMU-XLS-R-300M: Chúng tôi đề xuất mô hình transformer SAMU-XLS-R-300M cho dịch thuật trong công trình này. Encoder được khởi tạo bằng bộ mã hóa giọng nói SAMU-XLS-R được huấn luyện trước (Phần IV-A), và decoder được khởi tạo bằng bộ giải mã văn bản MBART được huấn luyện trước. Hậu tố 300M trong SAMU-XLS-R-300M đề cập đến kích thước của mô hình là 300M tham số.

2) XLS-R-(300M, 1B, 2B): Chúng tôi so sánh SAMU-XLS-R-300M với ba mô hình dịch thuật dựa trên bộ mã hóa giọng nói XLS-R cụ thể là, XLS-R-300M, XLS-R-1B, và XLS-R-2B. Ba mô hình dịch thuật khác biệt với mô hình SAMU-XLS-R-300M ở chỗ encoder của mô hình dịch thuật được khởi tạo bằng các bộ mã hóa giọng nói XLS-R được huấn luyện trước có kích thước khác nhau từ 300M đến 2B tham số. Decoder được khởi tạo bằng bộ giải mã MBART được huấn luyện trước. Không giống như bộ mã hóa giọng nói đa phương thức SAMU-XLS-R của chúng tôi, XLS-R chỉ được huấn luyện bằng dữ liệu giọng nói không gán nhãn. Ngoài ra, SAMU-XLS-R được huấn luyện đặc biệt để học các biểu diễn ngữ nghĩa, trong khi XLS-R không có ràng buộc nào được áp đặt trong giai đoạn huấn luyện của nó để mã hóa kiến thức ngữ nghĩa.

3) mSLAM: Chúng tôi so sánh mô hình dịch thuật SAMU-XLS-R-300M với hai mô hình dịch thuật dựa trên bộ mã hóa giọng nói mSLAM [18] cụ thể là, mSLAM-600M, và mSLAM-2B. Giống như SAMU-XLS-R, bộ mã hóa giọng nói mSLAM là một bộ mã hóa giọng nói-văn bản đa phương thức. Không giống như SAMU-XLS-R, được huấn luyện bằng giám sát ngữ nghĩa từ một bộ mã hóa văn bản ngữ nghĩa được huấn luyện trước, mSLAM không được huấn luyện với giám sát ngữ nghĩa rõ ràng.

4) Dịch thuật Cascade: Chúng tôi so sánh SAMU-XLS-R-300M với một hệ thống dịch thuật cascade mạnh. Chúng tôi thực hiện dịch thuật trong hai bước: (i) Phiên âm phát ngôn giọng nói bằng một mô hình ASR, và (ii) Sử dụng một mô hình dịch thuật văn bản-sang-văn bản để dịch phiên âm ASR sang văn bản trong ngôn ngữ đích. Chúng tôi sử dụng whisper-large-v2⁴[50] như mô hình ASR trong bước đầu tiên và mô hình dịch thuật văn bản-sang-văn bản MBART [28] cho bước thứ hai trong cascade. Đối với cascade X→EN, chúng tôi sử dụng mô hình dịch thuật văn bản-sang-văn bản MBART-many-to-English⁵. Mô hình ASR Whisper là đa ngôn ngữ hỗ trợ phiên âm khoảng 93 ngôn ngữ. Vì [50] chỉ ra rằng whisper đạt hiệu suất ASR tối tân trên một số bộ đánh giá công khai, chúng tôi chọn Whisper để tự động phiên âm giọng nói. Mô hình dịch thuật MBART-many-to-English có thể dịch văn bản từ 50 ngôn ngữ sang tiếng Anh.

5) Phiên âm: Như một topline, chúng tôi sử dụng các phiên âm văn bản sự thật nền tương ứng với các phát ngôn giọng nói và sử dụng MBART-many-to-English để dịch sang tiếng Anh.

⁴https://huggingface.co/openai/whisper-large-v2
⁵https://huggingface.co/facebook/mbart-large-50-many-to-one-mmt

--- TRANG 7 ---
BẢN THẢO 7

--- TRANG 8 ---
BẢN THẢO 8

--- TRANG 9 ---
BẢN THẢO 9

--- TRANG 10 ---
BẢN THẢO 10

--- TRANG 11 ---
BẢN THẢO 11

--- TRANG 12 ---
BẢN THẢO 12
