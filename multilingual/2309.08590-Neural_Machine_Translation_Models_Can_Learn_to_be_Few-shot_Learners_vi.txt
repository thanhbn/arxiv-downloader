# 2309.08590.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2309.08590.pdf
# Kích thước tệp: 268255 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Các Mô hình Dịch máy Thần kinh Có thể Học được Cách trở thành Người học Few-shot
Raphael Reinauer†và Patrick Simianer†và Kaden Uhlig và Johannes E. M. Mosig
và Joern Wuebker
Lilt
{raphael.reinauer,patrick}@lilt.com
Tóm tắt
Khả năng nổi bật của các Mô hình Ngôn ngữ Lớn trong việc sử dụng một số lượng nhỏ các ví dụ để học thực hiện trong các lĩnh vực và nhiệm vụ mới, còn được gọi là học trong ngữ cảnh (ICL). Trong công trình này, chúng tôi chỉ ra rằng một mô hình nhỏ hơn nhiều có thể được huấn luyện để thực hiện ICL bằng cách tinh chỉnh hướng tới một mục tiêu huấn luyện chuyên biệt, được minh họa trên nhiệm vụ thích ứng lĩnh vực cho dịch máy thần kinh. Với khả năng ICL này, mô hình có thể tận dụng các ví dụ few-shot liên quan để thích ứng đầu ra của nó hướng tới lĩnh vực. Chúng tôi so sánh chất lượng của việc thích ứng lĩnh vực này với các kỹ thuật giám sát truyền thống và ICL với một Mô hình Ngôn ngữ Lớn 40B-tham số. Phương pháp của chúng tôi cho phép suy luận theo lô hiệu quả trên hỗn hợp các lĩnh vực và vượt trội hơn các baseline tiên tiến về cả chất lượng dịch thuật và tỷ lệ thích ứng tức thời, tức là khả năng tái tạo một thuật ngữ cụ thể sau khi được hiển thị một ví dụ duy nhất.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) đã thể hiện khả năng học few-shot trên nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau, như được nêu bật bởi Brown et al. (2020) hoặc Garcia et al. (2023). Khi được nhắc với các bản dịch ví dụ phù hợp, chúng có thể cạnh tranh với các mô hình dịch máy thần kinh (NMT), được xây dựng và huấn luyện đặc biệt để dịch giữa các ngôn ngữ (Vilar et al., 2023). Thật thú vị, người ta có thể thích ứng LLMs với các lĩnh vực cụ thể chỉ bằng cách thêm các bản dịch ví dụ vào lời nhắc của chúng tại thời điểm suy luận (Moslem et al., 2023). Khả năng thích ứng với các nhiệm vụ và lĩnh vực cụ thể này được gọi là học trong ngữ cảnh (ICL). Trái ngược với các phương pháp tinh chỉnh truyền thống, ICL không yêu cầu một tập hợp các tham số tùy chỉnh riêng biệt cho mỗi lĩnh vực, điều này ngụ ý những lợi ích hiệu quả lớn thông qua suy luận theo lô.

†Đóng góp ngang nhau. Trong bài báo này, chúng tôi tích hợp ICL cho thích ứng lĩnh vực vào các hệ thống NMT theo nhiều bước. Chúng tôi so sánh phương pháp của chúng tôi để thích ứng các hệ thống NMT với các phương pháp tinh chỉnh truyền thống, cũng như với khả năng thích ứng lĩnh vực của một LLM mã nguồn mở. Cụ thể, các đóng góp chính của chúng tôi là:

1. Chúng tôi đánh giá khả năng ICL của một hệ thống NMT không được sửa đổi cho thích ứng lĩnh vực và chứng minh những hạn chế của nó.
2. Chúng tôi đề xuất một sơ đồ huấn luyện để cải thiện khả năng ICL của mô hình NMT.
3. Chúng tôi chỉ ra rằng ICL có thể được kết hợp với các phương pháp thích ứng truyền thống hơn để cải thiện hơn nữa hiệu suất thích ứng lĩnh vực.
4. Chúng tôi so sánh phương pháp của chúng tôi với hiệu suất của LLM mã nguồn mở FALCON-40B (Penedo et al., 2023) trên một nhiệm vụ dịch máy với ICL cho thích ứng lĩnh vực.

2 Công trình Liên quan
Bulte và Tezcan (2019) cải thiện hiệu suất dịch của một mô hình NMT bằng cách tích hợp các cặp khớp mờ dịch từ bộ nhớ dịch làm đầu vào cho mô hình NMT. Ý tưởng này được mở rộng thêm bởi Pham et al. (2020) và Xu et al. (2020), những người đối với một đoạn nguồn đã cho sử dụng nhúng câu để truy xuất các ví dụ tương tự và so sánh các sơ đồ khác nhau để tích hợp những ví dụ đó làm gợi ý vào mạng NMT.

Phương pháp của chúng tôi khác ở chỗ chúng tôi chỉ huấn luyện trên các token thuộc về bản dịch và không trên các token được cung cấp làm ngữ cảnh, điều mà chúng tôi chỉ ra là hoạt động tốt hơn. Ngoài ra, quy trình huấn luyện của Pham et al. (2020) khác biệt, khi họ huấn luyện mô hình của họ từ đầu, sử dụng dữ liệu huấn luyện từ nhiều lĩnh vực và đánh giá trên chính những lĩnh vực đó, trong khi chúng tôi huấn luyện trên dữ liệu lĩnh vực chung và đánh giá trên một lĩnh vực mới không có trong dữ liệu huấn luyện. Hơn nữa,

--- TRANG 2 ---
chúng tôi tập trung vào nhiệm vụ thích ứng đa lĩnh vực sử dụng các adapter nhẹ. Phương pháp này không chỉ cho phép chúng tôi mở rộng sang các lĩnh vực mới mà không cần huấn luyện lại toàn bộ mô hình, mà còn cung cấp một chiến lược thực tế và hiệu quả hơn cho các ứng dụng thế giới thực.

Các tác giả của (Moslem et al., 2023) đã điều tra khả năng của một LLM độc quyền, cụ thể là GPT-3.5, cho dịch máy thích ứng sử dụng ICL. Các thí nghiệm rộng rãi của họ cho thấy GPT-3.5 có thể thích ứng tốt với các cặp câu trong lĩnh vực và/hoặc thuật ngữ.

3 Thí nghiệm
Chúng tôi thực hiện một loạt thí nghiệm để phát triển các hệ thống NMT xuất sắc trong thích ứng lĩnh vực ICL few-shot. Ở đây chúng tôi trình bày các thí nghiệm theo thứ tự logic, trong đó chúng tôi bắt đầu với các mô hình baseline được mô tả trong Phần 3.1 và sau đó giới thiệu một số giai đoạn phát triển. Trong giai đoạn 0 và 1, chúng tôi thử ICL với các mô hình baseline không được sửa đổi và được tinh chỉnh lĩnh vực, tương ứng. Sau đó, trong GIAI ĐOẠN 2, chúng tôi tinh chỉnh mô hình baseline cho nhiệm vụ ICL lĩnh vực, thay vì một lĩnh vực cụ thể. Cuối cùng, chúng tôi kết hợp ICL và thích ứng lĩnh vực thông qua tinh chỉnh trong GIAI ĐOẠN 3. Tiến trình thí nghiệm của chúng tôi được hướng dẫn bởi các số liệu và bộ dữ liệu mà chúng tôi giới thiệu trong Phần 3.5 và 3.6, tương ứng.

3.1 Mô hình
Trong suốt bài báo này, chúng tôi làm việc với một hệ thống NMT và LLM FALCON-40B, cả hai đều được mô tả ở đây.

3.1.1 FALCON LLM
Để cung cấp một so sánh trực tiếp với LLMs và khả năng ICL của chúng, chúng tôi thực hiện thí nghiệm với mô hình ngôn ngữ Transformer chỉ có bộ giải mã FALCON-40B (Penedo et al., 2023), cụ thể là biến thể không được điều chỉnh theo hướng dẫn¹. Suy luận được thực hiện với giải mã tham lam. Theo công trình trước đây (Bawden và Yvon, 2023; Garcia et al., 2023; Hendy et al., 2023) (trong số những người khác), mô hình được nhắc thực hiện dịch mà không có tinh chỉnh cụ thể hướng tới nhiệm vụ dịch máy. Một mẫu nhắc đơn giản được sử dụng cho tất cả các thí nghiệm k-shot với FALCON-40B, xem Hình 1.

Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng k = 0

¹Mô hình có sẵn từ nền tảng huggingface:
https://huggingface.co/tiiuae/falcon-40b

English: <câu nguồn>\n
German: <câu đích>\n
English: [...]

Hình 1: Mẫu nhắc cho LLM.

không hoạt động tốt với mô hình cụ thể này² - các đầu ra có xu hướng hoàn toàn được tưởng tượng.

3.1.2 Hệ thống NMT
Mô hình baseline mà chúng tôi sử dụng làm điểm khởi đầu cho tất cả các thí nghiệm tiếp theo là một mô hình Transformer (Vaswani et al., 2017) với 12 lớp bộ mã hóa và hai lớp bộ giải mã, được triển khai với bộ công cụ NVIDIA NeMo (Kuchaiev et al., 2019). Kích thước nhúng là 1.024 với chiều mạng feed-forward là 4.096. Mô hình có một từ vựng chung gồm 32.768 token, trong khi các ma trận nhúng là cụ thể cho các mô-đun bộ mã hóa, bộ giải mã và phép chiếu đầu ra, tức là các tham số không được chia sẻ giữa chúng. Mô hình được huấn luyện để hỗ trợ kích thước đầu vào tối đa 1.536 token bằng cách tăng cường dữ liệu huấn luyện với các phép nối ngẫu nhiên của các câu song song. Chúng tôi đánh giá mô hình bằng giải mã tham lam.

Đối với các thí nghiệm được trình bày ở đây, mô hình baseline được tinh chỉnh đầy đủ (GIAI ĐOẠN 2A và GIAI ĐOẠN 2B), hoặc các adapter nhẹ (Bapna và Firat, 2019) được thêm vào mô hình (GIAI ĐOẠN 1 và GIAI ĐOẠN 3). Chúng tôi chọn tinh chỉnh mô hình đầy đủ trên dữ liệu ngoài lĩnh vực để thích ứng mô hình NMT với một nhiệm vụ mới - dịch với ngữ cảnh tăng cường của các ví dụ liên quan - và các lớp adapter để học từ dữ liệu trong lĩnh vực.

Các adapter mà chúng tôi sử dụng theo công thức của Bapna et al. (2019), nhưng với chuẩn hóa lớp được áp dụng sau nút cổ chai thay vì trước nó. Chúng tôi sử dụng độ rộng nút cổ chai là 256 và chèn adapter vào mỗi lớp của bộ giải mã và mỗi lớp khác của bộ mã hóa.

Chúng tôi luôn tinh chỉnh với bộ tối ưu hóa ADAM (Kingma và Ba, 2014) và dừng sớm dựa trên loss xác thực.

3.2 GIAI ĐOẠN 0 & GIAI ĐOẠN 1: ICL với Mô hình NMT Tiêu chuẩn
Được thúc đẩy bởi khả năng học few-shot của LLMs, chúng tôi kiểm tra khả năng của một mô hình NMT tiếng Anh-tiếng Đức tiêu chuẩn để thích ứng với một lĩnh vực chỉ được đưa ra các cặp dịch tương tự và liên quan làm ngữ cảnh bổ sung, tức là không thay đổi các tham số của mô hình.

Để tìm các đoạn nguồn tương tự trong bộ nhớ dịch, chúng tôi tìm kiếm các láng giềng gần nhất trong không gian nhúng. Chúng tôi sử dụng mô hình nhúng câu đa ngôn ngữ³ từ thư viện sentence transformer (Reimers và Gurevych, 2020) để nhúng các phía nguồn của tất cả các cặp đoạn. Sau đó chúng tôi sử dụng hnswlib (Malkov và Yashunin, 2020) để tìm các láng giềng gần nhất xấp xỉ: Mỗi câu nguồn trong các bộ dữ liệu cụ thể lĩnh vực đầu tiên được mã hóa với mô hình nhúng câu và sau đó được thêm vào một chỉ mục. Vì sự đơn giản trong bài báo này, chúng tôi sẽ gọi các láng giềng gần nhất xấp xỉ đơn giản là láng giềng gần nhất. Để đo độ tương tự giữa một cặp đoạn s và s', chúng tôi sử dụng khoảng cách cosine của các vectơ nhúng tương ứng v_s và v_s', tức là,

d(s,s') := 1 − v_s·v_s' / (||v_s||_2·||v_s'||_2).

Đối với một đoạn nguồn s và đoạn đích t đã cho, chúng tôi xác định các láng giềng gần nhất s₁, s₂, ..., sₖ của nó, sử dụng khoảng cách cosine ở trên. Mỗi câu nguồn sᵢ được ghép đôi với một bản dịch tham chiếu tᵢ cho i = 1, ..., k. Chúng tôi sắp xếp các cặp theo khoảng cách của chúng đến s trong không gian nhúng, tức là,

d(s,s₁) ≤ d(s,s₂) ≤ ... ≤ d(s,sₖ).

Giả định của chúng tôi là các đoạn tương tự nên có các bản dịch tương tự. Đối với GIAI ĐOẠN 0 của các thí nghiệm, chúng tôi coi các câu ngữ cảnh và văn bản nguồn thực tế như một khối văn bản, chỉ được phân tách bởi một khoảng trắng duy nhất, sắp xếp các đoạn từ ít tương tự nhất đến tương tự nhất, với đoạn nguồn hiện tại s ở cuối. Kết quả là, đầu vào của bộ mã hóa là

<bos> sₖ sₖ₋₁ ... s₁ s <eos>

trong khi đối với bộ giải mã, chúng tôi sử dụng tiền tố:

<bos> tₖ tₖ₋₁ ... t₁

trong đó <bos> và <eos> biểu thị các token bắt đầu câu và kết thúc câu, tương ứng. Nhiệm vụ của mô hình sau đó là tiếp tục từ tiền tố đích bằng cách tạo ra một bản dịch của đoạn nguồn s.

Trong các thí nghiệm của chúng tôi, chúng tôi đánh giá hiệu suất dịch sử dụng một số k láng giềng gần nhất khác nhau, cụ thể k ∈ {1,2,5}.

³Tên mô hình trên https://www.sbert.net/:
all-MiniLM-L6-v2

Trong GIAI ĐOẠN 1 chúng tôi chạy các thí nghiệm bổ sung trong đó chúng tôi tinh chỉnh mô hình cho mỗi lĩnh vực, sử dụng dữ liệu huấn luyện trong lĩnh vực ở định dạng gốc. Việc tinh chỉnh cụ thể lĩnh vực này được thực hiện bằng cách tiêm các lớp adapter (Bapna và Firat, 2019) vào mạng trong khi đóng băng phần còn lại của mô hình, và tận dụng loss log-likelihood âm tiêu chuẩn (NLL) để huấn luyện. Đối với mỗi lĩnh vực, chúng tôi sau đó kiểm tra mô hình được tinh chỉnh trực tiếp (0-shot trong Bảng 3 và 4) cũng như với ICL (k-shot với k ≠ 0).

Các adapter được huấn luyện đến hội tụ, tức là cho đến khi không còn cải thiện thêm về loss xác thực.

3.3 GIAI ĐOẠN 2A & GIAI ĐOẠN 2B: Tinh chỉnh hướng tới ICL
Để cải thiện khả năng của mô hình trong việc sử dụng các ví dụ láng giềng gần nhất trong ngữ cảnh, chúng tôi tinh chỉnh thêm toàn bộ mô hình trên dữ liệu ngoài lĩnh vực, cụ thể là News-Commentary⁴ (Kocmi et al., 2022), chứa khoảng 450K đoạn song song. Để xác thực chúng tôi sử dụng một mẫu 2K đoạn song song từ EuroParl⁵ (Koehn, 2005). Đối với việc tinh chỉnh mô hình đầy đủ này, chúng tôi không huấn luyện đến hội tụ, mà áp dụng dừng sớm tích cực: Huấn luyện được dừng khi loss xác thực không giảm ít nhất 0.1 hai lần liên tiếp, xác thực cho mỗi 1% của một epoch. Điều này để khuyến khích mô hình chỉ học nhiệm vụ mới và định dạng dữ liệu, nhưng không thích ứng với phân phối dữ liệu mới.

Thay vì nối trực tiếp các láng giềng gần nhất vào các ví dụ huấn luyện, chúng tôi thêm một token phân tách đặc biệt - <sep> - để phân tách các đoạn nguồn và đích. Sau đó chúng tôi xây dựng các trường hợp huấn luyện cho bộ mã hóa như:

<bos> sₖ <sep> sₖ₋₁ <sep> ... <sep> s₁ <sep> s <eos>

và cho bộ giải mã như:

<bos> tₖ <sep> tₖ₋₁ <sep> ... <sep> t₁ <sep> t <eos>    (1)

và tính toán loss NLL trên tất cả các token của (1). Loss huấn luyện này giống hệt với loss được sử dụng trong Pham et al. (2020). Chúng tôi ký hiệu quy trình này là GIAI ĐOẠN 2A.

Đối với GIAI ĐOẠN 2B, ý tưởng là mô hình nên học dự đoán đoạn đích từ đoạn nguồn sử dụng các bản dịch láng giềng gần nhất nhưng không học dự đoán tₖ, ..., t₁ như trong (Pham et al., 2020). Do đó chúng tôi che loss huấn luyện NLL sao cho nó chỉ được tính toán trên các token thuộc về đoạn đích t, loại trừ tất cả các token ngữ cảnh, do đó hoàn toàn tập trung tín hiệu huấn luyện vào việc dịch t trong ngữ cảnh của k láng giềng gần nhất của nó.

Sau đó chúng tôi sử dụng cùng định dạng như trong GIAI ĐOẠN 2A để huấn luyện, trong khi tại thời điểm suy luận chúng tôi cung cấp cho bộ giải mã một tiền tố chứa các ví dụ ICL:

<bos> tₖ <sep> tₖ₋₁ <sep> ... <sep> t₁ <sep>

Cuối cùng, chúng tôi đo chất lượng của bản dịch dự đoán t̂ bằng cách tính toán điểm BLEU và COMET với đoạn đích t làm tham chiếu.

Đối với cả GIAI ĐOẠN 2A và GIAI ĐOẠN 2B, các k-láng giềng gần nhất cho mỗi đoạn trong dữ liệu huấn luyện và dữ liệu xác thực được trích xuất từ toàn bộ bộ dữ liệu News-Commentary như được mô tả trong Phần 3.2.

3.4 GIAI ĐOẠN 3: Kết hợp ICL và Thích ứng Lĩnh vực
Để kết hợp khả năng ICL của GIAI ĐOẠN 2B với thích ứng lĩnh vực dựa trên adapter, chúng tôi thêm adapter vào mô hình từ GIAI ĐOẠN 2B sử dụng cùng cấu hình như cho các thí nghiệm GIAI ĐOẠN 1. Một lần nữa, chúng tôi huấn luyện các lớp adapter riêng biệt cho mỗi lĩnh vực.

Mỗi ví dụ từ tập huấn luyện được chú thích với các láng giềng gần nhất của nó từ cùng tập huấn luyện, loại trừ chính nó.

3.5 Số liệu
Để đánh giá chất lượng dịch thuật, chúng tôi sử dụng framework SacreBLEU (Post, 2018) triển khai số liệu BLEU (Papineni et al., 2002). Chúng tôi cũng đánh giá với COMET dựa trên tham chiếu (Rei et al., 2022) để so sánh các đầu ra mô hình với các bản dịch tham chiếu trong dữ liệu kiểm tra.

3.6 Bộ dữ liệu
Chúng tôi chạy các thí nghiệm của chúng tôi với cặp ngôn ngữ tiếng Anh-tiếng Đức trên 8 lĩnh vực từ các bộ sưu tập corpus ACED- và MDNS, mà chúng tôi mô tả trong phần này. Thống kê cho tất cả các bộ dữ liệu được cung cấp trong Bảng 1.

3.6.1 Corpus ACED
Corpus ACED (Lin et al., 2022) bao gồm ba bộ dữ liệu riêng biệt, cụ thể là Asics, Emerson, và Digitalocean, mỗi bộ gồm các câu tiếng Anh-tiếng Đức được trích xuất từ các lĩnh vực khác nhau. ACED là một benchmark thế giới thực chứa dữ liệu bắt nguồn từ các bản dịch được thực hiện bởi con người.

--- TRANG 4 ---

                    Huấn luyện    Xác thực    Kiểm tra
Asics                1.4          0.5         0.6
Digitalocean         11.8         2.0         7.6
Emerson              4.3          1.3         1.7
IT                   223          2.0         2.0
Koran                17.9         2.0         2.0
Law                  467          2.0         2.0
Medical              248          2.0         2.0
Subtitles            500          2.0         2.0

Bảng 1: Số lượng đoạn cho các phần chia bộ dữ liệu cụ thể lĩnh vực được sử dụng cho thí nghiệm, tính bằng nghìn.

3.6.2 Corpus MDNS
Corpus MDNS (Aharoni và Goldberg, 2020) là một corpus đa lĩnh vực chứa văn bản song song tiếng Anh-tiếng Đức từ năm lĩnh vực đa dạng (IT, Koran, Law, Medical, Subtitles). Nó được tạo ra đặc biệt để đánh giá thích ứng lĩnh vực.

4 Kết quả
Ở đây chúng tôi thảo luận về kết quả thí nghiệm, tiến triển từ GIAI ĐOẠN 0 đến GIAI ĐOẠN 3. Tất cả kết quả được mô tả riêng biệt cho các corpus ACED- và MDNS trong Bảng 3 và 4 tương ứng.

4.1 GIAI ĐOẠN 0: ICL với Mô hình NMT Baseline
Khi chúng tôi thêm các láng giềng gần nhất vào các đầu vào và tiền tố đích, đầu tiên chúng tôi quan sát thấy rằng các số liệu tự động hầu hết được cải thiện trên tất cả các bộ dữ liệu. Đáng chú ý, kết quả với 1-shot láng giềng gần nhất là tốt nhất trong nhóm thí nghiệm này. Ngoài ra chúng tôi thấy rằng kết quả 5-shot thường xuống dưới baseline.

Cụ thể đối với các corpus Medical và Subtitles của MDNS, chúng tôi thấy rằng mô hình không thể cải thiện so với baseline cho tất cả k.

Khoảng cách cosine của các láng giềng gần nhất dường như là một chỉ số khả thi của hiệu suất trong tập hợp thí nghiệm này, ví dụ khi so sánh kết quả cho ACED Emerson & Digitalocean, nơi khoảng cách cosine trung bình (xem Bảng 2) cho k = 1 thấp hơn nhiều cho Emerson ở 0.13, so với 0.3 cho Digitalocean. Chúng tôi tìm thấy mối tương quan Pearson âm vừa phải, không có ý nghĩa thống kê (r = -0.43) giữa khoảng cách cosine trung bình cho k = 1 và sự khác biệt về điểm BLEU giữa thí nghiệm GIAI ĐOẠN 0 1-shot và baseline.

--- TRANG 5 ---

                    ACED                                          MDNS
           Asics    Digitalocean   Emerson    IT     Koran    Law    Medical   Subtitles
k = 1      0.19     0.30          0.13       0.15   0.18     0.13   0.12      0.24
k = 2      0.21     0.31          0.14       0.17   0.20     0.15   0.14      0.25
k = 5      0.23     0.34          0.16       0.21   0.24     0.17   0.17      0.27

Bảng 2: Khoảng cách cosine trung bình trong không gian nhúng của các nguồn tập kiểm tra đến k-láng giềng gần nhất từ tập huấn luyện, cho k ∈ {1,2,5}.

                     Asics                  Digitalocean              Emerson                   Trung bình
             BLEU    COMET         BLEU    COMET         BLEU    COMET         BLEU    COMET
Baseline     34.5    0.8624        53.3    0.9043        44.9    0.9108        44.2    0.8925
GIAI ĐOẠN 0
1-shot       43.7    0.8578        54.4    0.8982        72.1    0.9213        56.7    0.8924
2-shot       44.5    0.8525        54.5    0.8967        67.2    0.9137        55.4    0.8876
5-shot       41.0    0.8420        53.9    0.8955        28.7    0.8705        41.2    0.8693
GIAI ĐOẠN 1
0-shot       41.2    0.8780        60.1    0.9152        79.2    0.944         60.2    0.9124
1-shot       46.4    0.8657        59.6    0.9099        78.1    0.9378        61.4    0.9045
2-shot       46.2    0.8628        59.0    0.9090        66.3    0.9275        57.2    0.8998
5-shot       44.2    0.8500        57.3    0.9038        32.2    0.893         44.6    0.8823
GIAI ĐOẠN 2A
1-shot       43.0    0.8765        55.0    0.9073        73.1    0.9382        57.0    0.9073
2-shot       43.5    0.8785        54.4    0.9072        71.6    0.9392        56.5    0.9083
5-shot       42.3    0.8662        54.4    0.9066        73.4    0.9347        56.7    0.9025
GIAI ĐOẠN 2B
1-shot       44.5    0.8766        54.9    0.9046        73.1    0.9391        57.5    0.9068
2-shot       44.5    0.8777        55.4    0.9080        74.3    0.939         58.1    0.9082
5-shot       44.7    0.8734        55.0    0.9072        70.0    0.9363        56.6    0.9056
GIAI ĐOẠN 3
1-shot       48.8    0.8896        60.5    0.9141        78.9    0.9480        62.7    0.9172
2-shot       48.5    0.8914        60.1    0.9132        80.7    0.9456        63.1    0.9167
5-shot       47.6    0.8837        59.0    0.9095        80.2    0.9437        62.3    0.9123
Falcon
1-shot       31.8    0.8588        40.0    0.8677        71.6    0.9380        47.8    0.8882
2-shot       34.5    0.8671        44.8    0.8876        76.9    0.9416        52.1    0.8988
5-shot       40.8    0.8789        X       X             78.5    0.9434        X       X

Bảng 3: Kết quả cho corpus ACED của đánh giá đa giai đoạn cho số lượng k-láng giềng gần nhất khác nhau, sử dụng số liệu BLEU và COMET. Điểm "Baseline" dành cho hệ thống NMT tiếng Anh-tiếng Đức được mô tả trong Phần 3.1. Chúng tôi bỏ qua bộ dữ liệu Digitalocean cho đánh giá FALCON-40B 5-shot.

Trong khi BLEU chỉ ra sự cải thiện (COMET chỉ giảm cho k > 1), chúng tôi thấy rằng hành vi của mô hình thực tế là thoái hóa. Cụ thể, mô hình thường không thể tạo ra bất kỳ đầu ra nào sau tiền tố đã cho và thay vào đó dự đoán <eos> ngay lập tức, dẫn đến các bản dịch trống. Chúng tôi thấy rằng tỷ lệ bản dịch trống là 8.5%, 8.1%, và 9.1% cho k = 1, 2, và 5 tương ứng. Ngược lại, hệ thống baseline có tỷ lệ đầu ra trống là 0%. Điều này bất chấp mô hình được huấn luyện đặc biệt để hỗ trợ các đầu vào bao phủ toàn bộ độ rộng ngữ cảnh trong quá trình tiền huấn luyện.

4.2 GIAI ĐOẠN 1: Kết hợp ICL với Tinh chỉnh Lĩnh vực
Đối với GIAI ĐOẠN 1, đầu tiên chúng tôi quan sát thấy rằng mô hình có thể được thích ứng hiệu quả cho mỗi lĩnh vực bằng cách huấn luyện adapter (xem kết quả GIAI ĐOẠN 1, 0-shot trong Bảng 3 và 4). Một ngoại lệ đáng chú ý là MDNS Subtitles, nơi thích ứng chỉ cải thiện nhẹ so với baseline. Kết quả này, tuy nhiên, phù hợp với công trình khác (Aharoni và Goldberg, 2020).

Khi chúng tôi kết hợp các adapter được huấn luyện với ICL, chúng tôi không thấy sự cải thiện so với kết quả 0-shot của GIAI ĐOẠN 1, ngoại trừ ACED Asics.

Hiệu suất giảm thảm hại đối với các corpus MDNS Medical & Subtitles. Tỷ lệ

--- TRANG 6 ---

                IT                  Koran               Law                 Medical             Subtitles           Trung bình
        BLEU    COMET      BLEU    COMET      BLEU    COMET      BLEU    COMET      BLEU    COMET      BLEU    COMET
Baseline 34.3   0.8153     14.7    0.7229     44.7    0.8696     43.5    0.8406     27.7    0.7891     33.0    0.8075
GIAI ĐOẠN 0
1-shot   35.9   0.7698     17.2    0.6580     51.6    0.853      42.3    0.7964     17.5    0.6358     32.9    0.7426
2-shot   35.9   0.7433     17.2    0.6346     49.9    0.8467     38.2    0.7810     22.4    0.7024     32.7    0.7416
5-shot   31.9   0.7196     14.5    0.6000     42.3    0.8287     30.5    0.7505     24.4    0.7400     28.7    0.7278
GIAI ĐOẠN 1
0-shot   39.6   0.8403     22.6    0.7274     50.7    0.8824     47.8    0.8429     28.1    0.7879     37.8    0.8162
1-shot   36.7   0.7620     21.1    0.6434     51.1    0.8228     7.1     0.5078     0.0     0.4306     23.2    0.6333
2-shot   35.6   0.7436     20.5    0.6152     48.9    0.8019     15.9    0.5441     0.0     0.4208     24.2    0.6251
5-shot   32.8   0.7296     18.4    0.5980     44.9    0.7940     23.4    0.5854     16.8    0.6388     27.3    0.6692
GIAI ĐOẠN 2A
1-shot   34.3   0.8277     15.5    0.7222     49.5    0.8739     43.6    0.8380     25.7    0.7838     33.7    0.8091
2-shot   35.8   0.8244     16.4    0.7154     49.6    0.8739     44.6    0.8362     24.1    0.7810     34.1    0.8062
5-shot   34.3   0.8203     15.9    0.7083     48.1    0.8659     40.7    0.8220     24.0    0.7712     32.6    0.7975
GIAI ĐOẠN 2B
1-shot   34.6   0.8269     16.0    0.7217     50.4    0.8752     44.2    0.8405     25.9    0.7830     34.2    0.8095
2-shot   35.5   0.8182     16.5    0.7150     49.9    0.8747     43.4    0.8349     24.5    0.7774     34.0    0.8040
5-shot   33.5   0.8103     16.6    0.7070     48.2    0.8696     37.5    0.8274     25.2    0.7782     32.2    0.7985
GIAI ĐOẠN 3
1-shot   41.4   0.8423     28.8    0.7235     58.1    0.8862     52.9    0.8488     27.0    0.7846     41.6    0.8171
2-shot   41.7   0.8401     29.6    0.7225     57.3    0.8850     51.2    0.8480     27.6    0.7850     41.5    0.8161
5-shot   40.9   0.8296     29.2    0.7249     55.8    0.8804     48.7    0.8413     27.5    0.7876     40.4    0.8128
Falcon
1-shot   31.5   0.7985     17.9    0.7081     45.4    0.8538     42.4    0.8035     21.7    0.7586     31.8    0.7845
2-shot   35.5   0.8202     22.4    0.7263     49.5    0.8680     47.5    0.8288     21.4    0.7605     35.3    0.8008
5-shot   40.1   0.8377     24.5    0.7358     50.5    0.8749     50.1    0.8401     22.6    0.7776     37.6    0.8132

Bảng 4: Kết quả cho corpus MDNS của đánh giá đa giai đoạn cho số lượng k-láng giềng gần nhất khác nhau sử dụng số liệu BLEU và COMET. Điểm "Baseline" dành cho hệ thống NMT tiếng Anh-tiếng Đức được mô tả trong Phần 3.1.

bản dịch trống cũng tăng lên đáng kể⁶, với tỷ lệ lên đến 63.1% cho kết quả 1-shot trên MDNS Medical (tăng từ 8.0% ở GIAI ĐOẠN 0).

4.3 GIAI ĐOẠN 2A & GIAI ĐOẠN 2B: Tinh chỉnh hướng tới ICL
Khi chúng tôi so sánh GIAI ĐOẠN 2B (tinh chỉnh với loss che mặt như được mô tả trong Phần 3.3) với kết quả GIAI ĐOẠN 0, chúng tôi thấy rằng việc thêm bộ phân tách và tinh chỉnh mô hình dẫn đến điểm số cải thiện nói chung trên các corpus ACED cho tất cả k. Kết quả BLEU trên các corpus MDNS cho thấy hiệu suất hơi kém hơn so với kết quả GIAI ĐOẠN 0 trong 3 trên 5 corpus cho k = 1, nhưng trung bình vẫn được cải thiện. Tuy nhiên, điểm COMET được cải thiện nhất quán cho so sánh này. Chúng tôi cũng thấy rằng điểm cho k = 2 và k = 1 rất gần nhau, với 2-shot dẫn trước 1-shot 0.6% điểm BLEU trung bình trên dữ liệu ACED, và 1-shot dẫn trước 2-shot 0.2 điểm BLEU trên MDNS. Điều này trái ngược với những gì chúng tôi đã quan sát trong GIAI ĐOẠN 0. k = 5 vẫn hoạt động kém hơn,

⁶Tỷ lệ bản dịch trống của GIAI ĐOẠN 1 cho mỗi k trên tất cả corpus: 1-shot: 20.0%, 2-shot: 20.6%, 5-shot: 13.6%.

nhưng chúng tôi quan sát thấy lợi ích tương đối cao so với kết quả GIAI ĐOẠN 0 5-shot.

Khi so sánh GIAI ĐOẠN 2A và GIAI ĐOẠN 2B, tức là loss che mặt và loss NLL tiêu chuẩn, kết quả không rõ ràng.

Chúng tôi quan sát thêm rằng GIAI ĐOẠN 2B thể hiện tỷ lệ tạo ra bản dịch trống gần như không đáng kể, ở 0.3%, 0.8%, và 1.2% cho k = 1, 2, 5 tương ứng.

4.4 GIAI ĐOẠN 3: Kết hợp ICL và Thích ứng Lĩnh vực
Khi kết hợp ICL với các adapter được huấn luyện với dữ liệu chú thích láng giềng gần nhất, chúng tôi quan sát thấy kết quả tốt nhất toàn cầu cho các mô hình NMT. So với GIAI ĐOẠN 1, cũng được tinh chỉnh hướng tới mỗi lĩnh vực, chúng tôi quan sát thấy kết quả cải thiện lớn trên tất cả các số liệu tự động. GIAI ĐOẠN 3 2-shot mang lại kết quả tốt nhất trên ACED, với sự cải thiện 2.5 điểm BLEU so với người về nhì về BLEU trung bình GIAI ĐOẠN 1 1-shot. Trên MDNS, GIAI ĐOẠN 3 1-shot cải thiện so với người về nhì GIAI ĐOẠN 1 0-shot 3.8 điểm.

Đặc biệt là điểm cho MDNS Koran cải thiện rõ rệt trên tất cả các mô hình trước đây, với sự cải thiện tương đối 101% so với baseline. Các mô hình dường như có thể sử dụng tốt hơn các láng giềng gần nhất gần trong bộ dữ liệu này, thường là chuỗi con của nhau. Xem Phần 4.6 để phân tích chi tiết về hành vi sao chép trên bộ dữ liệu ACED Asics.

Tỷ lệ bản dịch trống được giảm xuống 0.0% cho tất cả k.

Chúng tôi quan sát thêm rằng kết quả cho ICL 1- và 2-shot rất tương tự, và điểm cho 5-shot cũng được cải thiện.

4.5 FALCON: Thích ứng Cả Nhiệm vụ và Lĩnh vực Cùng lúc
LLM FALCON-40B chứng tỏ xuất sắc trong ICL, học một nhiệm vụ và thích ứng với một lĩnh vực cùng lúc. Đáng chú ý, điểm cải thiện với giá trị k cao hơn, đây là hành vi ngược lại với những gì chúng tôi đã quan sát với các mô hình NMT. Khi các láng giềng gần nhất gần với dữ liệu kiểm tra, như chúng đối với các bộ dữ liệu ACED Emerson và MDNS IT, chúng tôi thấy kết quả gần với kết quả GIAI ĐOẠN 3 tốt nhất.

Tuy nhiên, tốc độ tạo của FALCON-40B rất chậm ở mức trung bình 2.6 token mỗi giây trong thiết lập 1-shot.

Cũng lưu ý rằng chúng tôi không có phương tiện tại thời điểm này để kiểm tra xem các phần của dữ liệu kiểm tra có được chứa trong dữ liệu huấn luyện của FALCON hay không.

4.6 Phân tích Định tính
Duy trì tính nhất quán trong các bản dịch là một tiêu chí chất lượng quan trọng trong ngành bản địa hóa, và là động lực chính trong việc sử dụng bộ nhớ dịch, giúp đảm bảo rằng các tài liệu tiếp thị, ví dụ, đồng nhất trong các tính năng và chức năng được hứa hẹn của các sản phẩm được quảng cáo (Emery et al., 2011). Trong các mô hình NMT, tính nhất quán này truyền thống được tăng cường bằng cách tinh chỉnh một mô hình dịch cho một lĩnh vực cụ thể, mà chúng tôi ký hiệu bằng "GIAI ĐOẠN 1 với 0-shot". Trong phần này, chúng tôi so sánh phương pháp tinh chỉnh với ICL của chúng tôi, cụ thể là "GIAI ĐOẠN 3 với 1-shot". Chúng tôi đánh giá tính nhất quán dịch trên bộ dữ liệu Asics.

Vì mục đích đó, chúng tôi chọn các đoạn s trong dữ liệu kiểm tra mà láng giềng gần nhất nguồn s' trong dữ liệu huấn luyện Asics khác biệt chính xác một từ. Các đoạn s này được ký hiệu là các đoạn thay thế từ. Đối với mỗi cặp (s, s'), chúng tôi sau đó sử dụng hai nguồn và một đích t' trong lời nhắc ICL và đích khác t làm tham chiếu để so sánh bản dịch được tạo ra. Chúng tôi định nghĩa phần của các cặp mà bản dịch được tạo ra khớp chính xác với tham chiếu là độ chính xác thay thế từ (WSA). Kết quả trong Bảng 6.

Bản dịch cho GIAI ĐOẠN 3 1-shot đạt điểm WSA 74.6%, so với 57.14% cho phương pháp tinh chỉnh (GIAI ĐOẠN 1 0-shot), trong khi mô hình không thích ứng chỉ tạo ra bản dịch tham chiếu chính xác trong 1.7% trường hợp.

5 Kết luận
Chúng tôi đã chỉ ra rằng một hệ thống NMT tiêu chuẩn có thể được huấn luyện để trở thành người học trong ngữ cảnh hiệu quả trong các nhiệm vụ thích ứng lĩnh vực. Chúng tôi thấy rằng điều này hiệu quả nhất khi chúng tôi kết hợp tinh chỉnh chung hướng tới nhiệm vụ ICL và huấn luyện các lớp adapter cho một lĩnh vực cụ thể với dữ liệu chú thích láng giềng gần nhất.

Khi mô hình không được tinh chỉnh hướng tới nhiệm vụ, chúng tôi thấy rằng ICL hoạt động ở một mức độ nào đó, nhưng cho thấy hành vi thoái hóa.

Trong khi các LLM như FALCON-40B có thể thích ứng với nhiệm vụ MT với ICL, điều này đi kèm với chi phí tính toán tăng lên. Nói chung, kết quả với LLM vẫn hoạt động kém hơn các mô hình MT chuyên dụng của chúng tôi.

Tài liệu tham khảo
Roee Aharoni và Yoav Goldberg. 2020. Các cụm lĩnh vực không giám sát trong các mô hình ngôn ngữ được tiền huấn luyện. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.

Ankur Bapna, Naveen Arivazhagan, và Orhan Firat. 2019. Thích ứng đơn giản, có thể mở rộng cho dịch máy thần kinh. CoRR, abs/1909.08478.

Ankur Bapna và Orhan Firat. 2019. Thích ứng đơn giản, có thể mở rộng cho dịch máy thần kinh. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), trang 1538–1548, Hong Kong, China. Association for Computational Linguistics.

Rachel Bawden và François Yvon. 2023. Điều tra hiệu suất dịch của một mô hình ngôn ngữ đa ngôn ngữ lớn: trường hợp của BLOOM. Trong Proceedings of the 24th Annual Conference of the European Association for Machine Translation, trang 157–170, Tampere, Finland. European Association for Machine Translation.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind

--- TRANG 8 ---

Nguồn: Strive for every point in the women's GEL-DEDICATE ™ 6 CLAY tennis shoe by ASICS.
Bản dịch Tham chiếu: Strebe nach jedem Punkt in dem GEL-DEDICATE ™ 6 CLAY Tennisschuh für Damen von ASICS.
BASELINE: Mit dem GEL-DEDICATE ™ 6 CLAY Damen-Tennisschuh von ASICS kannst du jeden Punkt erreichen.
GIAI ĐOẠN 1 với 0-shot: Mit dem ASICS GEL-DEDICATE ™ 6 CLAY Tennisschuh für Damen kannst du jeden Punkt erreichen.
GIAI ĐOẠN 3 với 1-shot: Strebe nach jedem Punkt in dem GEL-DEDICATE ™ 6 CLAY Tennisschuh für Damen von ASICS.

Bảng 5: So sánh các đầu ra dịch ví dụ từ các mô hình khác nhau và bản dịch tham chiếu. Các từ khác với bản dịch tham chiếu được tô sáng bằng màu xanh dương. Láng giềng nguồn gần nhất là "Strive for every point in the men's GEL-DEDICATE ™6 CLAY tennis shoe by ASICS." với bản dịch tham chiếu "Strebe nach jedem Punkt in dem GEL-DEDICATE ™6 CLAY Tennisschuh für Herren von ASICS.". Lưu ý rằng láng giềng gần nhất chỉ khác một từ trong mỗi ngôn ngữ.

                    GIAI ĐOẠN 3 với 1-shot    GIAI ĐOẠN 1 với 0-shot    Mô hình Không Thích ứng
Các đoạn thay thế từ        74.60%                   57.14%                   1.7%

Bảng 6: Kết quả cho độ chính xác thay thế từ (WSA, xem tiểu mục 4.6) cho các mô hình thích ứng và không thích ứng khác nhau cho các đoạn thay thế từ.

Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Các mô hình ngôn ngữ là người học few-shot. Advances in Neural Information Processing Systems, 33:1877–1901.

Bram Bulte và Arda Tezcan. 2019. Sửa chữa mờ thần kinh: Tích hợp các khớp mờ vào dịch máy thần kinh. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 1800–1809, Florence, Italy. Association for Computational Linguistics.

Vince Emery, Karl Kadie, và Mary Laplante. 2011. Nội dung Tiếp thị Đa ngôn ngữ: Phát triển Kinh doanh Quốc tế với Chuỗi Giá trị Nội dung Toàn cầu. Outsell.

Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, và Orhan Firat. 2023. Hiệu quả không hợp lý của học few-shot cho dịch máy. Trong Proceedings of the 40th International Conference on Machine Learning, tập 202 của Proceedings of Machine Learning Research, trang 10867–10878. PMLR.

Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, và Hany Hassan Awadalla. 2023. Các mô hình GPT tốt như thế nào trong dịch máy? Một đánh giá toàn diện. arXiv preprint arXiv:2302.09210.

Diederik Kingma và Jimmy Ba. 2014. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. International Conference on Learning Representations.

Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, và Maja Popović. 2022. Những phát hiện của hội nghị dịch máy năm 2022 (WMT22). Trong Proceedings of the Seventh Conference on Machine Translation (WMT), trang 1–45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Philipp Koehn. 2005. Europarl: Một corpus song song cho dịch máy thống kê. Trong Proceedings of machine translation summit x: papers, trang 79–86.

Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, Patrice Castonguay, Mariya Popova, Jocelyn Huang, và Jonathan M. Cohen. 2019. Nemo: một bộ công cụ để xây dựng các ứng dụng AI sử dụng các mô-đun thần kinh.

Jessy Lin, Geza Kovacs, Aditya Shastry, Joern Wuebker, và John DeNero. 2022. Sửa chữa tự động các bản dịch của con người. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 494–507, Seattle, United States. Association for Computational Linguistics.

Yu A Malkov và Dmitry A Yashunin. 2020. Tìm kiếm láng giềng gần nhất xấp xỉ hiệu quả và mạnh mẽ sử dụng đồ thị thế giới nhỏ điều hướng phân cấp. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(4):824–836.

Yasmin Moslem, Rejwanul Haque, John D. Kelleher, và Andy Way. 2023. Dịch máy thích ứng với các mô hình ngôn ngữ lớn. Trong Proceedings of the 24th Annual Conference of the European Association for Machine Translation, trang 227–237, Tampere, Finland. European Association for Machine Translation.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. BLEU: Một phương pháp cho đánh giá tự động dịch máy. Trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, trang 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. 2023. Bộ dữ liệu RefinedWeb cho Falcon LLM: Vượt trội hơn các corpus được tuyển chọn với dữ liệu web, và chỉ dữ liệu web.

M. Pham, Jitao Xu, Josep Maria Crego, François Yvon, và Jean Senellart. 2020. Mồi dịch máy thần kinh. Trong Conference on Machine Translation.

Matt Post. 2018. Một lời kêu gọi rõ ràng trong báo cáo điểm BLEU. Trong Proceedings of the Third Conference on Machine Translation: Research Papers, trang 186–191, Brussels, Belgium. Association for Computational Linguistics.

Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, và André F. T. Martins. 2022. COMET-22: Bài nộp Unbabel-IST 2022 cho nhiệm vụ chia sẻ số liệu. Trong Proceedings of the Seventh Conference on Machine Translation (WMT), trang 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Nils Reimers và Iryna Gurevych. 2020. Làm cho các nhúng câu đơn ngôn ngữ trở thành đa ngôn ngữ sử dụng chưng cất tri thức. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in Neural Information Processing Systems, trang 5998–6008.

David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, và George Foster. 2023. Nhắc PaLM để dịch: Đánh giá các chiến lược và hiệu suất. Trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 15406–15427, Toronto, Canada. Association for Computational Linguistics.

Jitao Xu, Josep Crego, và Jean Senellart. 2020. Tăng cường dịch máy thần kinh với các bản dịch tương tự. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 1580–1590, Online. Association for Computational Linguistics.
