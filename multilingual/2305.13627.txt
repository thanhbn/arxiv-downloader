# 2305.13627.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2305.13627.pdf
# File size: 950835 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
InstructAlign: High-and-Low Resource Language Alignment
via Continual Crosslingual Instruction Tuning
Samuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, Pascale Fung
Hong Kong University of Science and Technology
Clear Water Bay, Hong Kong
scahyawijaya@connect.ust.hk
Abstract
Large language models (LLMs) that are tuned
with instructions have demonstrated remark-
able capabilities in various tasks and languages.
However, their ability to generalize to underrep-
resented languages is limited due to the scarcity
of available data. Additionally, directly adapt-
ing new languages to instruction-tuned LLMs
can result in catastrophic forgetting, which
leads to the loss of multitasking ability. To
address this issue, we propose InstructAlign
which uses continual crosslingual instruction
tuning to enable LLMs to align new unseen lan-
guages with previously learned high-resource
languages. Our results demonstrate the ef-
fectiveness of InstructAlign in enabling the
model to understand low-resource languages
with limited parallel data while preventing
catastrophic forgetting. Our work contributes
to the advancement of language adaptation
methods, particularly for adapting instruction-
tuned LLMs to underrepresented languages.
Our code is released on https://github.
com/HLTCHKUST/InstructAlign .
1 Introduction
Instruction-tuned Large language models (LLMs)
have demonstrated their generalization capability
of solving various tasks expressed in natural lan-
guage without requiring any explicit training on
the corresponding task (Brown et al., 2020; Smith
et al., 2022; Rae et al., 2022; Thoppilan et al.,
2022; Chowdhery et al., 2022; Scao et al., 2022;
Zeng et al., 2022). This generalization capability
is further improved with various tuning methods,
such as instruction tuning (Sanh et al., 2022; Wei
et al., 2022a; Chung et al., 2022; Muennighoff et al.,
2022). However, LLMs and their instruction-tuned
variants face difficulties in generalizing across var-
ious languages, leading to a disparity in perfor-
mances(Xue et al., 2021; Gehrmann et al., 2022;
Scao et al., 2022; Chowdhery et al., 2022; Yong
et al., 2023; Zhang et al., 2023; Asai et al., 2023;Kabra et al., 2023). Moreover, these models have
limited language coverage, mostly in the Indo-
European language family as indicated in Figure 1.
For instance, BLOOM (Scao et al., 2022) and
BLOOMZ (Muennighoff et al., 2022), the largest
community-driven open-source multilingual pre-
trained LLM, only covers 46 languages during pre-
training, excluding some high-resource languages
with hundreds of millions of speakers, such as Ger-
man, Japanese, Korean, and Russian, as well as
many more low-resource languages with millions
of speakers, such as Serbian, Finnish, Amharic,
Sinhala, Lao, Javanese, Sundanese, etc.
Expanding the language repertoire of LLMs is
essential for promoting inclusivity and diversity
in Natural Language Processing (NLP) technol-
ogy, particularly for languages that are underrepre-
sented and low-resource. Recent studies, including
Wilie et al. (2020); Cahyawijaya et al. (2021); Aji
et al. (2022); Adelani et al. (2021, 2022); Kak-
wani et al. (2020); Kumar et al. (2022); Ebrahimi
et al. (2022); Adilazuarda et al. (2022); Cahyawi-
jaya et al. (2023b,a); Song et al. (2023) have em-
phasized the importance of this issue. To address
this concern, previous research (Yong et al., 2022)
has demonstrated that continual pretraining (Chau
et al., 2020; Muller et al., 2021; Ebrahimi and Kann,
2021) and parameter-efficient fine-tuning (PEFT)
methods, like MAD-X (Pfeiffer et al., 2020) and
(IA)3(Liu et al., 2022), can be utilized to swiftly
integrate the knowledge of unseen languages into
LLMs using monolingual corpora of the new lan-
guages by conducting masked language modeling
(MLM) (Devlin et al., 2019). However, these meth-
ods become ineffective when applied directly to
instruction-tuned LLMs due to catastrophic for-
getting (French, 1993) which prevents them from
solving general natural language tasks after the lan-
guage adaptation phase (Yong et al., 2022). More-
over, adapter-based approaches, such as MAD-
X (Pfeiffer et al., 2020), result in the loss of multi-arXiv:2305.13627v2  [cs.CL]  24 Oct 2023

--- PAGE 2 ---
Figure 1: The number of languages supported by existing LLMs (green region) per language family1. Existing
LLMs only support a fraction of languages around the globe. Most of them are within the Indo-European language
family, while most other language families are underrepresented or even unexplored.
lingual inference capability due to modularity (Adi-
lazuarda et al., 2023).
To solve this problem, we introduce InstructAl-
ign, a continual instruction tuning framework to
seamlessly align newly adapted low-resource lan-
guages (L2) with the pre-trained high-resource lan-
guages (L1) of an instruction-tuned LLM through
crosslingual alignment. InstructAlign compels
LLMs to perform crosslingual alignments between
pre-trained and novel languages through alignment-
based crosslingual instruction tuning, enabling the
model to grasp L2 with only a limited amount of
parallel data. To further prevent catastrophic for-
getting, InstructAlign incorporates experience re-
play (Chaudhry et al., 2019b; Rolnick et al., 2019),
which adds past data during the instruction tuning.
In summary, our work presents the following
major contributions:
•We propose InstructAlign, a crosslingual con-
tinual instruction tuning method that allows
instruction-tuned LLMs to understand L2 with
minimal degradation on L1 while retaining
their zero-shot prompting capability.
•We propose alignment-based crosslingual in-
struction tuning, which enables LLMs to align
L2 to L1 allowing better L2 acquisition with
only a limited amount of parallel data.
•We evaluate the effectiveness of InstructAl-
ign on Indonesian local languages datasets,
and demonstrate that InstructAlign can sig-
nificantly improve the performance on L2 by
5-10% F1 while maintaining the original per-
1We gather the language and language family information
from URIEL (Littell et al., 2017; Malaviya et al., 2017).formance on L1 and its multitask capability.
•We analyze the correlation between the per-
formance of L2 and other unseen languages
(L3), suggesting the zero-shot generalization
of InstructAlign to L3 particularly when the
languages are related.2
2 Related Work
2.1 Instruction Tuning in LLMs
Early works (Wei et al., 2022a; Chung et al.,
2022; Sanh et al., 2022; Ouyang et al., 2022) have
shown the effectiveness of instruction-tuned LLMs,
which significantly improves the zero-shot gener-
alization capability over the corresponding non-
instruction-tuned LLMs by a huge margin. Since
then, various instruction-tuned LLMs have been
released, including T0 (Sanh et al., 2022), In-
structGPT (Ouyang et al., 2022), FLAN-GPT (Wei
et al., 2022a), FLAN-T5 (Chung et al., 2022),
FLAN-PaLM (Chung et al., 2022), mT0 (Muen-
nighoff et al., 2022), BLOOMZ (Muennighoff
et al., 2022), Alpaca (Taori et al., 2023), etc. How-
ever, most of these models are only pre-trained on
a single or few languages, with the exception of
mT0 and BLOOMZ which are adapted from mod-
els pre-trained on 101 languages, i.e., mT5 (Xue
et al., 2021), and pre-trained on 46 languages, i.e.,
BLOOM (Scao et al., 2022), respectively. In this
work, we utilize BLOOMZ (Muennighoff et al.,
2022) as the backbone in of InstructAlign.
2We use the terms L1, L2, and L3 to denote the first,
second, and third language acquisition (Hammarberg, 2001,
2014). In our context, L1 denotes the pre-trained languages
in LLMs, L2 denotes the newly adapted languages, and
L3denotes other languages that have not been seen after tuning
with InstructAlign, which are only used in the evaluation.

--- PAGE 3 ---
Figure 2: Example of the alignment-based crosslingual instruction prompts, i.e., bilingual denoising (TLM), machine
translation (MT), and crosslingual semantic similarity (XSS) in comparison to the monolingual denoising (MLM).
2.2 Crosslingual Alignment
Crosslingual alignment is a widely explored con-
cept that allows language models (LMs) to align,
commonly at a word/sentence level, across dif-
ferent languages. Crosslingual alignment allows
the models to perform crosslingual inference with-
out requiring any tuning on the target task. Fung
(1997, 1998) a bilingual lexicon extraction method
through word-to-word alignment from word rela-
tion matrix. Fung and Cheung (2004) introduces
a bilingual lexicon and parallel sentence extrac-
tion method from aligning sentences from non-
parallel data via Bootstrapping and EM. Lample
et al. (2018b); Cao et al. (2020) introduces align
bilingual lexicon method that requires no parallel
data by performing embedding alignment across
different languages. This is then utilized to deal
with unsupervised machine translation Lample et al.
(2018a). A crosslingual pre-training objective for
building LMs, namely translation language model-
ing (TLM) (Conneau and Lample, 2019), has also
been explored which enforces token-level align-
ment between languages allowing the model to
learn aligned representation across multiple lan-
guages. In this work, we perform crosslingual
alignment through instruction by introducing bilin-
gual denoising instruction which is equivalent to
token-level alignment in TLM, and translation in-
struction which serves as sentence-level alignment
across different languages.
2.3 Continual Learning for Language Models
Continual learning is a paradigm to learn various
tasks gradually allowing the model to acquire new
knowledge over time(Delange et al., 2021). Usinga naive fine-tuning approach for continual learn-
ing causes the model to suffer from catastrophic
forgetting (CF) (French, 1999). Therefore, vari-
ous methods have been introduced to prevent CF.
Regularization-based methods (Kirkpatrick et al.,
2017; Liu et al., 2018; Aljundi et al., 2018) add a
regularization in the loss function to prevent the
model to be updated into a direction that causes CF.
Replay-based methods (Rolnick et al., 2019; Lopez-
Paz and Ranzato, 2017; Chaudhry et al., 2019a) add
samples from previous tasks to be incorporated dur-
ing learning the new task, which helps regularize
the model to avoid CF. Parameter isolation meth-
ods (Aljundi et al., 2017; Serrà et al., 2018; Mallya
and Lazebnik, 2018) prevent the model from CF
by learning new tasks using a new set of param-
eters while keeping the other parameters frozen
during fine-tuning. In this work, we apply expe-
rience replay (Rolnick et al., 2019), which is a
simple replay-based method by adding tasks from
previously learned languages when training new
languages without any loss modification.
3 Methodology
InstructAlign is a continual crosslingual instruction
tuning framework that allows the model to align
high-to-low resource languages through instruction
tuning. InstructAlign introduces two components,
i.e., 1) crosslingual alignment through instruction
tuning, which allows the model to align pre-trained
languages with the new languages through crosslin-
gual alignment, and 2) continual instruction tuning,
which applies continual learning into instruction
tuning to avoid catastrophic forgetting.

--- PAGE 4 ---
Dataset Task #Lang. #L1 #L2 #L3 #Test
NusaX Sentiment Analysis 12 2 7 3 4400
NusaTranslation Sentiment Analysis 11+1 1 3 8 10400
NusaParagraph Emotion Recognition 10 0 4 6 5700
NusaParagraph Topic Classification 10 0 4 6 6250
Table 1: Statistics of all datasets used in the experiments. #Lang. denotes the #languages in each dataset.
3.1 Crosslingual Alignment through
Instruction
Given a parallel text pair (x, y)from two lan-
guages, the goal of crosslingual alignment is to
learn a mapping function f(.)parameterized by θ
such that f(x, θ) =f(y, θ). The (x, y)text pair
commonly comes in the form of a word pair or a
phrase pair (Lample et al., 2018b,a), but in theory,
it should be able to generalize to a sentence pair or
even a paragraph. With the goal of aligning two
parallel texts from two different languages, Instruc-
tAlign defines a set of alignment-based crosslin-
gual instructions by exploiting multiple alignment
objectives that can be achieved through a parallel
sentence. Specifically, we explore three different
objectives, i.e., bilingual denoising / translation lan-
guage modeling ( TLM ), machine translation ( MT)
and crosslingual semantic similarity ( XSS).
We first define a parallel sentence pair (X=
{x1, x2, . . . , x m}, Y={y1, y2, . . . , y n}), where
xiandyidenote the i-th token of the sentence
XandY, respectively. For bilingual denoising
(TLM ), we model the problem as a conditional
denoising task. InstructAlign first applies a per-
turbation function gtlm(.)to the target sentence
Ythat masks out part of the tokens in order to
get˜Y=gtlm(Y). The pair (X,˜Y)is then used
to generate a prompt using h(X,˜Y , Ttlm), result-
ing in an input-output data pair for prompting
(htlm(X,˜Y , Ttlm), Y), where htlm(.)denotes a
bilingual denoising prompt generator and Ttlmthe
prompt template.
For the machine translation ( MT) objec-
tive, we define the input-output data pair as
(hmt(X, Tmt), Y), where hmt(.)denotes a ma-
chine translation prompt generator and Tmtde-
notes a machine translation prompt template. As
for the crosslingual semantic similarity ( XSS) ob-
jective, we models the problem as an inference task
to predict whether two parallel sentences X and Y
are semantically similar. Specifically, we define
the input-output data pair as (hxss(X, Y, Txss), l)
where hxss(.)is a semantic similarity prompt gen-erator, Txssdenotes a semantic similarity prompt
template and lthe binary label regarding whether
the sentences are semantically related or not. The
examples of the crosslingual alignment objectives
are shown in Figure 2.
3.2 Continual Instruction Tuning through
Experience Replay
Within the continual instruction tuning phase of
InstructAlign, experience replay (Rolnick et al.,
2019) is employed to minimize the catastrophic
forgetting problem. mhamdi-etal-2023-cross Ex-
perience replay works by storing some of the past
training data and using them during the optimiza-
tion step of the new data. These past data serve
as a regularization term that prevents the models
to forget past knowledge when learning from the
new data. The past data is collected from the in-
struction tuning data used when developing the
corresponding instruction-tuned model, which are
all supervised.
During the continual instruction tuning, In-
structAlign takes only rrandomly sampled data
from the past instruction tuning data. The sam-
pled past data is used during continual-instruction
tuning with a balanced sampling between the
past data and new data. More formally, we de-
fine a past dataset Doldand a newly generated
crosslingual instruction dataset Dcli. On each
optimization step, InstructAlign samples data in
an interleaving manner resulting in a batch data
B={sDold
1, sDcli
1, sDold
2, sDcli
2, . . . , sDold
n, sDcli
n}
with2nsamples, where sDold
i andsDcli
idenote a
sample that is taken randomly from DoldandDcli,
respectively. Since the samples are all supervised,
the optimization can be done by optimizing the
cross-entropy loss (Good, 1952) from all the sam-
ples in the batch.
4 Experiment Setting
4.1 Continual-Instruction Tuning Dataset
During the InstructAlign tuning, we train the model
on 7 L2 languages from Malayo-Polynesian lan-

--- PAGE 5 ---
MethodL2 Weighted F1 (%) L1 Weighted F1 (%)
NT-S NX-S NP-E NP-T Avg. NX-S En NX-S Id NT-S Id Avg.
BLOOM & BLOOMZ Baseline
BLOOM-560M 57.62 21.80 2.80 5.34 21.89 29.26 21.13 61.47 37.29
BLOOM-1.1B 59.18 22.02 2.80 5.35 22.34 22.02 22.54 58.81 34.46
BLOOM-3B 44.98 21.21 2.80 5.35 18.59 24.03 21.17 58.30 34.50
BLOOMZ-560M 46.83 33.73 2.80 5.35 22.18 58.24 55.59 69.81 61.21
BLOOMZ-1.1B 64.01 41.50 2.80 5.35 28.42 57.41 58.58 80.40 65.46
BLOOMZ-3B 69.41 45.82 2.80 5.73 30.94 62.65 63.21 81.38 69.08
InstructAlign-Tuned BLOOMZ-560M
MLM r=100k 66.51 42.51 2.80 5.52 29.34 60.97 60.01 70.93 63.97
MT Obj. r=100k 66.42 41.20 2.82 5.40 28.96 60.96 58.09 64.18 61.08
TLM r=100k 69.24 42.91 2.87 5.43 30.11 61.65 58.52 72.40 64.19
XSS r=100k 68.10 45.83 2.84 5.53 30.58 61.89 58.22 71.27 63.79
InstructAlign-Tuned BLOOMZ-1.1B
MLM r=100k 71.46 45.73 2.84 5.49 31.38 61.30 60.83 73.25 65.13
MT Obj. r=100k 66.15 44.93 2.84 5.40 29.91 61.68 59.18 65.28 62.05
TLM r=100k 70.29 49.25 3.17 6.34 32.26 63.26 60.54 74.66 66.15
XSS r=100k 71.89 49.23 3.08 5.81 32.50 63.78 59.34 75.74 66.29
Table 2: Evaluation results of InstructAlign with BLOOMZ-560M and BLOOMZ-1.1B backbones. Compared to
BLOOM and BLOOMZ baselines, All InstructAlign-tuned models improve the zero-shot crosslingual performance
in L2 while also retaining the performance in L1.
guage family group, i.e., Sundanese (sun), Javanese
(jav), Balinese (ban), Minangkabau (min), Bugi-
nese (bug), Acehnese (ace), and Banjarese (bjn).
For the L1 languages, we utilize English (eng),
as English covers the majority of the pre-training
data in most LLMs, and Indonesian (ind), as the
language is closely related to the target L2 lan-
guages. For the dataset, we utilize FLORES-200
dataset (Goyal et al., 2021; Team et al., 2022) as
the source of the parallel data where we combine
the validation and the test set producing a total of
∼2000 parallel sentences for each language pair
which is orders of magnitude smaller compared
the data size used for language adaptation used
in prior works(Pfeiffer et al., 2020; Cahyawijaya
et al., 2021; Alabi et al., 2022; Yong et al., 2022).
4.2 Models & Hyperparameters
We utilize BLOOMZ (Muennighoff et al., 2022)
as the backbone model. Specifically, we explore
InstructAlign on two model size, i.e., BLOOMZ-
560M and BLOOMZ-1.1B. For InstructAlign, we
evaluate three crosslingual alignment objectives,
i.e.,TLM ,XSS, and MT. The list of prompts used
for instruction tuning is described in Appendix A.
We use English prompts in all experiments. Werun all experiments with an initial learning rate
of 1e-5 with a linear learning rate decay and a
batch size of 32 for a fixed optimization step of
50,000. We run the InstructAlign on a single
RTX3090 GPU (24GB) using the AdamW opti-
mizer (Loshchilov and Hutter, 2019) and mixed-
precision training (Micikevicius et al., 2018). We
use a fixed number of replay samples r= 100000.
4.3 Evaluation Setting
After tuning with InstructAlign, the model is then
evaluated in a zero-shot crosslingual inference set-
ting, in which the model has never seen the task on
the target languages, but might have seen the task
on other seen languages. To retrieve the classifi-
cation label, we compute the joint probability of
the prompt with each label in the dataset and pick
the label which prompt the highest joint probability.
We consider 3 different prompts in English for the
zero-shot inference and take the average accuracy
and weighted F1 scores as the evaluation metrics.
The list of the prompts used in our evaluation is
shown in Appendix A. We use a single RTX1080Ti
GPU (11GB) to run the evaluation for all models.
To reduce the memory bottleneck during inference,
we run the evaluations using 8-bit inference via

--- PAGE 6 ---
Figure 3: Average performance of various models across
different model scales on the L1 and L2 languages sub-
sets of the NT-S andNX-S datasets.
LLM.int8() (Dettmers et al., 2022). We provide the
performance comparison between 8-bit and 32-bit
evaluation in Appendix B.
Zero-Shot Evaluation Datasets For evaluating
the effectiveness of InstructAlign, we utilize four
multilingual Indonesian local languages datasets,
i.e., the sentiment analysis task from NusaX ( NX-
S) (Winata et al., 2022), the sentiment analysis task
from NusaTranslation ( NT-S ) (Cahyawijaya et al.,
2023a), the topic classification task from NusaPara-
graph ( NP-T ) (Cahyawijaya et al., 2023a), and
the paragraph-level emotion recognition task from
NusaParagraph ( NP-E ) (Cahyawijaya et al., 2023a).
The detailed per-dataset statistics are shown in
Table 1. NusaX covers 12 languages including
2 L1 languages: English (eng) and Indonesian
(ind), 7 L2 languages: Acehnese (ace), Balinese
(ban), Buginese (bug), Banjarese (ban), Javanese
(jav), Minangkabau (min), and Sundanese (sun),
and 3 L3 languages: Toba Batak (bbc), Madurese
(mad), and Ngaju (nij). While NusaTranslation cov-
ers 11 languages, which includes 3 L2 languages:
Javanese (jav), Sundanese (sun), and Minangk-
abau (min), and 8 L3 languages: Ambon (abs),
Batak (btk), Betawi (bew), Bima (bhp), Madurese
(mad), Makassarese (mak), Musi (mui), and Re-
jang (rej). NusaParagraph covers 10 languages,
which includes 4 L2 languages: Sundanese, Ja-
vanese (jav), Minangkabau (min), and, and 6 L3
languages: Batak (btk), Betawi (bew), Madurese
(mad), Makassarese (mak), Musi (mui), Rejang
(rej). To expand the evaluation dataset for L1,
we add the Indonesian sentiment analaysis data
from IndoLEM (Koto et al., 2020)3as the Indone-
3The source translation data of NusaTranslationMethod L2 L1
Baselines
Random 40.28 30.88
Majority 32.34 21.17
BLOOMZ-560M 37.66 61.21
Single Objective
Monolingual Denoising (MLM) 36.71 53.14
Machine Translation (MT) 35.43 47.95
Bilingual Denoising (TLM) 45.48 53.28
Crosslingual Semantic Similarity (XSS) 44.55 54.05
Multi Objectives
MLM + MT 40.09 47.67
TLM + MT 42.93 48.75
XSS + MT 43.32 50.66
MLM + TLM 43.46 53.16
MLM + XSS 42.82 53.90
TLM + XSS 45.83 54.01
Table 3: Averaged Weighted F1 scores from various
InstructAlign objectives in the NT-S andNX-S datasets.
We use BLOOMZ-560M as the backbone.
sian (ind) subset of NT-S . More details about each
dataset can be found in Appendix C.
4.4 Baselines
For our baselines, we conduct zero-shot prompt-
ing using four different sizes of BLOOMZ, i.e.,
BLOOMZ-560M, BLOOMZ-1.1B, BLOOMZ-
1.7B, and BLOOMZ-3B, without any additional
language adaptation phase. In addition, to compare
the effectiveness of the crosslingual alignment, we
add continual instruction-tuned baselines that in-
corporate only monolingual denoising instructions,
which is equivalent to performing language adapta-
tion using MLM (Devlin et al., 2019).
5 Experiment Result
Effectiveness of InstructAlign Table 2 shows
the result of InstructAlign on both L1 and L2
languages. InstructAlign-tuned models with MT,
TLM, and XSS objectives significantly outperform
the comparable-sized BLOOM and BLOOMZ
baselines on L2 languages while retaining a similar
performance level as the original BLOOMZ mod-
els on L1 languages. Surprisingly, InstructAlign
with MLM objectives is also effective, yielding a
similar performance on L2 languages compared
to the crosslingual objectives. In §6.1, we show
that this improvement only occurs after combin-
ing the MLM objective with the experience replay,
demonstrating the importance of continual instruc-

--- PAGE 7 ---
Figure 4: ∆weighted F1 of InstructAlign tuned BLOOMZ-560M with (left) TLM and (right) XSS objectives
various continual instruction-tuned approaches compared to the original BLOOMZ-560M baseline. Negative scores
indicate that the model performs worse compared to the baseline.
tion tuning during language adaption. While in the
NusaParagraph emotion recognition ( NP-E ) and
topic classification ( NP-T ) tasks, all baselines yield
a very low score, suggesting that the ability to solve
long text classification tasks do not emerge on that
scale (Wei et al., 2022b). Interestingly, InstructAl-
ign tuned models indicate consistent improvement,
although marginal, on these tasks, demonstrating
that an early emergence in L2 languages is possible
through InstructAlign.
Effect of Model Scaling As shown in Figure 3,
we observe that scaling increases the zero-shot per-
formance of BLOOMZ on both L1 and L2, but
the same does not apply to BLOOM, suggesting
the benefit of instruction tuning for better general-
ization to unseen tasks and languages. Moreover,
applying InstructAlign on larger BLOOMZ results
in higher overall zero-shot performance on both
L1 and L2. Specifically, InstructAlign-tuned mod-
els with 1.1 billion parameters yield ∼2% higher
performance compared to the 560 million parame-
ters InstructAlign-tuned models and even perform
competitively with the original 3 billion parameters
BLOOMZ model. This suggests that the scaling
law of language models (Kaplan et al., 2020; Hoff-
mann et al., 2022) also apply after InstructAlign
where larger-sized models tend to perform better
compared to their smaller counterpart. Detailed
experiment results are described in Appendix D.
6 Analysis and Discussion
6.1 Alignment Objectives
To better understand the effectiveness of each align-
ment objective, we conduct experiments by us-
ing a single objective, i.e., monolingual denois-ing (MLM), machine translation (MT), bilingual
denoising (TLM) and crosslingual semantic simi-
larity (XSS), as well as multi objectives on various
combinations. We also test zero-shot prompting
without any additional language adaption phase
as a baseline for comparison. Note that contin-
ual instruction tuning through experience replay is
not applied ( r=0) in these experiments since we
focused on the effect of alignment objectives.
As shown in Table 3, BLOOMZ 560M zero-shot
performs better than the random baseline on L1
while achieving a lower score on L2, showing that
BLOOMZ 560M is unable to be directly applied
to these L2 languages. For InstructAlign with a
single objective, similar to the result from prior
work (Yong et al., 2022), applying the MLM ob-
jective decays the performance of the model. Sim-
ilarly, using MT objective also decreases the per-
formance of both L1 and L2. Nevertheless, as
shown in Table 2, this problem can be mitigated
by applying continual learning. On the other hand,
both TLM and XSS help improve the model on
L2, indicating that these objectives are effective for
aligning L1 and L2 languages. Additionally, the
performance in L1 languages is also retained the
most when using the TLM and XSS objectives.
When combining multiple objectives during In-
structAlign, we observe the highest score when
combining TLM and XSS. Interestingly, adding
the MLM and MT objectives during InstructAlign
consistently yields a lower score compared to the
single TLM and XSS objectives for both L2 and L1
languages. These facts suggest that cross-lingual
objectives such as XSS and TLM, are effective
for learning new languages through cross-lingual
instruction-tuning with limited data.

--- PAGE 8 ---
Figure 5: Correlation of ∆weighted F1 from the Instruc-
tAlign tuned models to the corresponding BLOOMZ
backbone models on novel and unseen languages. R
denotes the Pearson correlation coefficient.
6.2 Continual Instruction Tuning
In order to assess the effectiveness of continual in-
struction tuning through experience replay, we con-
duct an experiment exploring the effect of different
numbers of replay samples rused in continual in-
struction tuning. Specifically, we explore 4 settings
ofr, i.e., r= [0,1000,10000 ,100000] . Figure 4
shows the performance of the InstructAlign tuned
models across different ranges of replay examples
r. When using no experience replay ( r=0), the
performance of the pre-trained languages drops
significantly, and even further, the performances
on the novel languages also drop which suggests
that the multitask prompting capability for both
of these methods are degraded (Yong et al., 2022).
When rincreases, a much smaller performance
degradation is observed on the L1 languages. In-
terestingly, the performance on novel languages
also improved when rincreases which in the end,
increases the performance of the model across all
languages. These facts demonstrate the importance
of the experience replays for avoiding catastrophic
forgetting in continual instruction tuning.
6.3 Impact of InstructAlign on L3 Languages
We further assess the impact of aligning L2 lan-
guages through InstructAlign to other unseen In-
donesian languages which are within the same lan-
guage family group (L3). To assess the effective-
ness of transferability from the L2 languages to
L3 languages, we compute the correlation coeffi-
cient between ∆weighted F1 score on the L2 and
L3 languages for each model compared to the cor-
Figure 6: Per language results of InstructAlign tuned
models in NusaX. red denotes L1 languages. teal de-
notes L2 languages. purple denotes L3 languages
responding baseline, and measure the Pearson’s
correlation coefficient (Rodgers and Nicewander,
1988; Freedman et al., 2007).
As shown in Figure 5, the correlation coefficient
between the performance improvement of L2 and
L3 languages is high with a Pearson’s correlation
coefficient of 0.96. This indicates the effective-
ness of the InstructAlign approach for not only
adapting to L2 languages but also to related L3 lan-
guages. Nevertheless, the improvement for unseen
language still depends on the language distances
as shown in Figure 6, where performance on Toba
Batak (bbc) and Buginese (bug) yield much lower
scores compares the other languages. This result
aligns with the analysis from NusaX (Winata et al.,
2022) which shows that the performances of Bug-
inese (bug) and Toba Batak (bbc) are the lowest
for both the multitask and zero-shot crosslingual
settings due to the relatively low vocabulary over-
lapping compared to other languages in NusaX.
This suggests that by performing , the model can
also understand unseen languages that are related to
the novel-adapted language, indicating the general-
ization of the crosslingual transfer from pre-trained
languages to novel and unseen languages
6.4 Conclusion
In this work, we address the challenge of increasing
the language coverage of instruction-tuned LLMs
by introducing a crosslingual continual instruc-
tion tuning method, InstructAlign. We demon-
strate that InstructAlign allows an instruction-tuned
LLM to effectively learn novel languages through

--- PAGE 9 ---
alignment-based crosslingual instruction tuning ob-
jectives while retaining the existing multitask and
multilingual abilities. Based on our experiment re-
sults on four Indonesian local languages datasets,
InstructAlign effectively improves the understand-
ing of novel Indonesian local languages, improving
the language understanding performance on novel
languages by ∼5-10% weighted F1 score and also
demonstrates a better forward transfer performance
to other unseen Indonesian local languages by a
significant margin. In addition, we analyze vari-
ous objectives of InstructAlign and demonstrate
the effectiveness of alignment-based crosslingual
instruction tuning objectives compared to the tra-
ditional masked language modeling (MLM) for
learning novel languages with a limited amount of
data. Our work contributes to the advancement of
language adaptation methods for instruction-tuned
LLMs, especially for underrepresented languages.
7 Limitation and Future Works
7.1 Other Model Architectures
Despite the effectiveness of InstructAlign on
BLOOMZ, its effectiveness has not been explored
for different model architectures such as encoder-
decoder or other model architectures. Due to the
limited computing budget, we can only run the In-
structAlign experiment on a decoder-only model,
i.e., BLOOMZ, We encourage future works to ex-
plore the experiment in other model architectures.
7.2 Scaling to Larger LLMs
As described in §5, we hypothesize that
InstructAlign-tuned models follow the scaling laws
of language models (Kaplan et al., 2020; Hoffmann
et al., 2022). Nevertheless, we can only empiri-
cally show this scaling effect of InstructAlign in
BLOOMZ-560M and BLOOMZ-1.1B due to the
limited computing budget. We expect future works
to expand the exploration to larger-scale models.
7.3 Other Continual Learning Methods
In terms of continual learning methods, we only ex-
plore a single approach, i.e., experience replay (Rol-
nick et al., 2019), due to the efficient memory re-
quirement of this method. Further analysis and ex-
amination of other potential continual learning ap-
proaches, such as A-GEM (Chaudhry et al., 2019a)
and EWC (Liu et al., 2018), is another potential
research direction to be explored in future works.7.4 Underrepresented Languages from Other
Language Family
There are many other underrepresented languages
such as indigenous languages of the Ameri-
cas (Ebrahimi et al., 2022), African (Adelani et al.,
2021, 2022), Indic (Kakwani et al., 2020; Kumar
et al., 2022), Austronesian (Winata et al., 2022;
Cahyawijaya et al., 2023b), and many others all
around the world. In this work, we only explore
InstructAlign for Malayo-Polynesian language fam-
ily group under the Austronesian language family,
specifically for Indonesian local languages. For
future work, we are eager to explore the general-
ization of InstructAlign and other language adap-
tation methods on other underrepresented and low-
resource languages.
Ethical Consideration
Our work highlights the importance of inclusivity
in LLM technology for underrepresented and ex-
tremely low-resource languages. During our study,
we are well aware of the ethical responsibility asso-
ciated with language research and the potential im-
pact it can have on communities. Our ultimate goal
is to promote linguistic diversity and contribute to
a more inclusive NLP landscape. We encourage
further collaboration and engagement with under-
represented and low-resource language communi-
ties to ensure that their voices are heard and their
needs are addressed in future language technology
development. We remain committed to the princi-
ples of ethical research, diversity, inclusivity, and
fairness, striving to mitigate biases and promote
social good through our work in the field of NLP.
8 Acknowledgements
We thank Bryan Wilie and Yong Zheng Xin for the
fruitful discussions and suggestions. This work has
been partially funded by PhD Fellowship Award,
the Hong Kong University of Science and Technol-
ogy; and PF20-43679 Hong Kong PhD Fellowship
Scheme, Research Grant Council, Hong Kong.

--- PAGE 10 ---
References
David Adelani, Graham Neubig, Sebastian Ruder,
Shruti Rijhwani, Michael Beukman, Chester Palen-
Michel, Constantine Lignos, Jesujoba Alabi, Sham-
suddeen Muhammad, Peter Nabende, Cheikh
M. Bamba Dione, Andiswa Bukula, Rooweither
Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda,
Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe,
Derguene Mbaye, Amelia Taylor, Fatoumata Ka-
bore, Chris Chinenye Emezue, Anuoluwapo Aremu,
Perez Ogayo, Catherine Gitau, Edwin Munkoh-
Buabeng, Victoire Memdjokam Koagne, Allah-
sera Auguste Tapo, Tebogo Macucwa, Vukosi Mari-
vate, Mboning Tchiaze Elvis, Tajuddeen Gwad-
abe, Tosin Adewumi, Orevaoghene Ahia, Joyce
Nakatumba-Nabende, Neo Lerato Mokono, Ig-
natius Ezeani, Chiamaka Chukwuneke, Mofetoluwa
Oluwaseun Adeyemi, Gilles Quentin Hacheme,
Idris Abdulmumin, Odunayo Ogundepo, Oreen
Yousuf, Tatiana Moteu, and Dietrich Klakow. 2022.
MasakhaNER 2.0: Africa-centric transfer learning
for named entity recognition. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 4488–4508, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
David Ifeoluwa Adelani, Jade Abbott, Graham Neu-
big, Daniel D’souza, Julia Kreutzer, Constantine Lig-
nos, Chester Palen-Michel, Happy Buzaaba, Shruti
Rijhwani, Sebastian Ruder, Stephen Mayhew, Is-
rael Abebe Azime, Shamsuddeen H. Muhammad,
Chris Chinenye Emezue, Joyce Nakatumba-Nabende,
Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau,
Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-
mam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,
Rubungo Andre Niyongabo, Jonathan Mukiibi, Ver-
rah Otiende, Iroro Orife, Davis David, Samba Ngom,
Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,
Gerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-
wuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel
Oyerinde, Clemencia Siro, Tobius Saul Bateesa,
Temilola Oloyede, Yvonne Wambui, Victor Akin-
ode, Deborah Nabagereka, Maurice Katusiime, Ayo-
dele Awokoya, Mouhamadane MBOUP, Dibora Ge-
breyohannes, Henok Tilaye, Kelechi Nwaike, De-
gaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-
vaoghene Ahia, Bonaventure F. P. Dossou, Kelechi
Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,
Adewale Akinfaderin, Tendai Marengereke, and Sa-
lomey Osei. 2021. MasakhaNER: Named Entity
Recognition for African Languages. Transactions
of the Association for Computational Linguistics ,
9:1116–1131.
Muhammad Farid Adilazuarda, Samuel Cahyawijaya,
and Ayu Purwarianti. 2023. The obscure limitation
of modular multilingual language models.
Muhammad Farid Adilazuarda, Samuel Cahyawijaya,
Genta Indra Winata, Pascale Fung, and Ayu Purwari-
anti. 2022. IndoRobusta: Towards robustness against
diverse code-mixed Indonesian local languages. In
Proceedings of the First Workshop on Scaling UpMultilingual Evaluation , pages 25–34, Online. Asso-
ciation for Computational Linguistics.
Alham Fikri Aji, Genta Indra Winata, Fajri Koto,
Samuel Cahyawijaya, Ade Romadhony, Rahmad Ma-
hendra, Kemal Kurniawan, David Moeljadi, Radi-
tyo Eko Prasojo, Timothy Baldwin, Jey Han Lau,
and Sebastian Ruder. 2022. One country, 700+ lan-
guages: NLP challenges for underrepresented lan-
guages and dialects in Indonesia. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 7226–7249, Dublin, Ireland. Association for
Computational Linguistics.
Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius
Mosbach, and Dietrich Klakow. 2022. Adapting pre-
trained language models to African languages via
multilingual adaptive fine-tuning. In Proceedings of
the 29th International Conference on Computational
Linguistics , pages 4336–4349, Gyeongju, Republic
of Korea. International Committee on Computational
Linguistics.
Rahaf Aljundi, Francesca Babiloni, Mohamed Elho-
seiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018.
Memory aware synapses: Learning what (not) to
forget. In Computer Vision – ECCV 2018 , pages
144–161. Springer International Publishing.
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuyte-
laars. 2017. Expert gate: Lifelong learning with a
network of experts. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) .
IEEE.
Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu,
Terra Blevins, Hila Gonen, Machel Reid, Yulia
Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi.
2023. Buffet: Benchmarking large language models
for few-shot cross-lingual transfer.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Samuel Cahyawijaya, Fajri Koto Holy Lovenia,
Dea Adhista, Emmanuel Dave, Sarah Oktavianti,
Salsabil Maulana Akbar, Jhonson Lee, Nuur Shadieq,
Tjeng Wawan Cenggoro, Hanung Linuwih, Bryan
Wilie, Galih Pradipta Muridan, Alham Fikri Aji,
Genta Indra Winata, David Moeljadi, Ayu Purwari-
anti, and Pascale Fung. 2023a. Nusawrites: Con-
structing high-quality corpora for underrepresented

--- PAGE 11 ---
and extremely low-resource languages. Anonymous
preprint under review.
Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji,
Genta Indra Winata, Bryan Wilie, Rahmad Mahendra,
Christian Wibisono, Ade Romadhony, Karissa Vin-
centio, Fajri Koto, Jennifer Santoso, David Moeljadi,
Cahya Wirawan, Frederikus Hudi, Ivan Halim Parmo-
nangan, Ika Alfina, Muhammad Satrio Wicaksono, Il-
ham Firdausi Putra, Samsul Rahmadani, Yulianti Oe-
nang, Ali Akbar Septiandri, James Jaya, Kaustubh D.
Dhole, Arie Ardiyanti Suryani, Rifki Afina Putri,
Dan Su, Keith Stevens, Made Nindyatama Nityasya,
Muhammad Farid Adilazuarda, Ryan Ignatius, Ryan-
dito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang
Dai, Yan Xu, Dyah Damapuspita, Cuk Tho, Ichwanul
Muslim Karo Karo, Tirana Noor Fatyanosa, Ziwei
Ji, Pascale Fung, Graham Neubig, Timothy Baldwin,
Sebastian Ruder, Herry Sujaini, Sakriani Sakti, and
Ayu Purwarianti. 2023b. Nusacrowd: Open source
initiative for indonesian nlp resources.
Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie,
Karissa Vincentio, Xiaohong Li, Adhiguna Kun-
coro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba-
har, Masayu Khodra, Ayu Purwarianti, and Pascale
Fung. 2021. IndoNLG: Benchmark and resources for
evaluating Indonesian natural language generation.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
8875–8898, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Steven Cao, Nikita Kitaev, and Dan Klein. 2020. Multi-
lingual alignment of contextual word representations.
InInternational Conference on Learning Representa-
tions .
Ethan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.
Parsing with multilingual BERT, a small corpus, and
a small treebank. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
1324–1334, Online. Association for Computational
Linguistics.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus
Rohrbach, and Mohamed Elhoseiny. 2019a. Efficient
lifelong learning with a-gem. In ICLR .
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elho-
seiny, Thalaiyasingam Ajanthan, Puneet K. Dokania,
Philip H. S. Torr, and Marc’Aurelio Ranzato. 2019b.
On tiny episodic memories in continual learning.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.
Alexis Conneau and Guillaume Lample. 2019. Cross-
Lingual Language Model Pretraining . Curran Asso-
ciates Inc., Red Hook, NY , USA.
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and
Tinne Tuytelaars. 2021. A continual learning sur-
vey: Defying forgetting in classification tasks. IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence , pages 1–1.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-
cation for transformers at scale.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Abteen Ebrahimi and Katharina Kann. 2021. How to
adapt your pretrained multilingual model to 1600
languages. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 4555–4567, Online. Association for Computa-
tional Linguistics.
Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,
Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John
Ortega, Ricardo Ramos, Annette Rios, Ivan Vladimir
Meza Ruiz, Gustavo Giménez-Lugo, Elisabeth
Mager, Graham Neubig, Alexis Palmer, Rolando

--- PAGE 12 ---
Coto-Solano, Thang Vu, and Katharina Kann. 2022.
AmericasNLI: Evaluating zero-shot natural language
understanding of pretrained multilingual models in
truly low-resource languages. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
6279–6299, Dublin, Ireland. Association for Compu-
tational Linguistics.
David Freedman, Robert Pisani, and Roger Purves.
2007. Statistics (international student edition).
Pisani, R. Purves, 4th edn. WW Norton & Company,
New York .
R French. 1999. Catastrophic forgetting in connection-
ist networks. Trends in Cognitive Sciences , 3(4):128–
135.
Robert M. French. 1993. Catastrophic interference in
connectionist networks: Can it be predicted, can it be
prevented? In Proceedings of the 6th International
Conference on Neural Information Processing Sys-
tems, NIPS’93, page 1176–1177, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Pascale Fung. 1997. Finding terminology translations
from non-parallel corpora. In Fifth Workshop on Very
Large Corpora, VLC 1997, Beijing, China and Hong
Kong, August 18 and August 20, 1997 .
Pascale Fung. 1998. A statistical view on bilingual lexi-
con extraction: From parallel corpora to non-parallel
corpora. In Machine Translation and the Informa-
tion Soup, Third Conference of the Association for
Machine Translation in the Americas, AMTA ’98,
Langhorne, PA, USA, October 28-31, 1998, Proceed-
ings, volume 1529 of Lecture Notes in Computer
Science , pages 1–17. Springer.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and E. In Proceedings
of the 2004 Conference on Empirical Methods in Nat-
ural Language Processing , EMNLP 2004, A meet-
ing of SIGDAT, a Special Interest Group of the ACL,
held in conjunction with ACL 2004, 25-26 July 2004,
Barcelona, Spain , pages 57–63. ACL.
Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya
Mahendiran, Alex Wang, Alexandros Papangelis,
Aman Madaan, Angelina Mcmillan-major, Anna
Shvets, Ashish Upadhyay, Bernd Bohnet, Bingsheng
Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin
You, Craig Thomson, Cristina Garbacea, Dakuo
Wang, Daniel Deutsch, Deyi Xiong, Di Jin, Dimi-
tra Gkatzia, Dragomir Radev, Elizabeth Clark, Esin
Durmus, Faisal Ladhak, Filip Ginter, Genta Indra
Winata, Hendrik Strobelt, Hiroaki Hayashi, Jekate-
rina Novikova, Jenna Kanerva, Jenny Chim, Jiawei
Zhou, Jordan Clive, Joshua Maynez, João Sedoc,
Juraj Juraska, Kaustubh Dhole, Khyathi Raghavi
Chandu, Laura Perez Beltrachini, Leonardo F . R.
Ribeiro, Lewis Tunstall, Li Zhang, Mahim Pushkarna,
Mathias Creutz, Michael White, Mihir Sanjay Kale,Moussa Kamal Eddine, Nico Daheim, Nishant Subra-
mani, Ondrej Dusek, Paul Pu Liang, Pawan Sasanka
Ammanamanchi, Qi Zhu, Ratish Puduppully, Reno
Kriz, Rifat Shahriyar, Ronald Cardenas, Saad Ma-
hamood, Salomey Osei, Samuel Cahyawijaya, Sanja
Štajner, Sebastien Montella, Shailza Jolly, Simon
Mille, Tahmid Hasan, Tianhao Shen, Tosin Adewumi,
Vikas Raunak, Vipul Raheja, Vitaly Nikolaev, Vivian
Tsai, Yacine Jernite, Ying Xu, Yisi Sang, Yixin Liu,
and Yufang Hou. 2022. GEMv2: Multilingual NLG
benchmarking in a single line of code. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing: System Demonstra-
tions , pages 266–281, Abu Dhabi, UAE. Association
for Computational Linguistics.
I. J. Good. 1952. Rational decisions. Journal of the
Royal Statistical Society. Series B (Methodological) ,
14(1):107–114.
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-
ishnan, Marc’Aurelio Ranzato, Francisco Guzmán,
and Angela Fan. 2021. The flores-101 evaluation
benchmark for low-resource and multilingual ma-
chine translation.
Björn Hammarberg. 2001. Chapter 2. roles of l1 and l2
in l3 production and acquisition. In Cross-Linguistic
Influence in Third Language Acquisition , pages 21–
41. Multilingual Matters.
Björn Hammarberg. 2014. 1. problems in defining the
concepts of l1, l2 and l3. In Teaching and Learning
in Multilingual Contexts , pages 3–18. Multilingual
Matters.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katherine Millican, George van den Driessche, Bog-
dan Damoc, Aurelia Guy, Simon Osindero, Karen
Simonyan, Erich Elsen, Oriol Vinyals, Jack William
Rae, and Laurent Sifre. 2022. An empirical analysis
of compute-optimal large language model training.
InAdvances in Neural Information Processing Sys-
tems.
Arfinda Ilmania, Abdurrahman, Samuel Cahyawijaya,
and Ayu Purwarianti. 2018. Aspect detection and
sentiment classification using deep neural network
for indonesian aspect-based sentiment analysis. In
2018 International Conference on Asian Language
Processing (IALP) , pages 62–67.
Anubha Kabra, Emmy Liu, Simran Khanuja, Al-
ham Fikri Aji, Genta Winata, Samuel Cahyawijaya,
Anuoluwapo Aremu, Perez Ogayo, and Graham Neu-
big. 2023. Multi-lingual and multi-cultural figurative
language understanding. In Findings of the Asso-
ciation for Computational Linguistics: ACL 2023 ,
pages 8269–8284, Toronto, Canada. Association for
Computational Linguistics.

--- PAGE 13 ---
Divyanshu Kakwani, Anoop Kunchukuttan, Satish
Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M.
Khapra, and Pratyush Kumar. 2020. IndicNLPSuite:
Monolingual corpora, evaluation benchmarks and
pre-trained multilingual language models for Indian
languages. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 4948–
4961, Online. Association for Computational Lin-
guistics.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
Joel Veness, Guillaume Desjardins, Andrei A. Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell.
2017. Overcoming catastrophic forgetting in neural
networks. Proceedings of the National Academy of
Sciences , 114(13):3521–3526.
Fajri Koto, Afshin Rahimi, Jey Han Lau, and Timo-
thy Baldwin. 2020. IndoLEM and IndoBERT: A
benchmark dataset and pre-trained language model
for Indonesian NLP. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics ,
pages 757–770, Barcelona, Spain (Online). Interna-
tional Committee on Computational Linguistics.
Aman Kumar, Himani Shrotriya, Prachi Sahu, Amogh
Mishra, Raj Dabre, Ratish Puduppully, Anoop
Kunchukuttan, Mitesh M. Khapra, and Pratyush Ku-
mar. 2022. IndicNLG benchmark: Multilingual
datasets for diverse NLG tasks in Indic languages.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
5363–5394, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
InInternational Conference on Learning Representa-
tions .
Guillaume Lample, Alexis Conneau, Marc’Aurelio Ran-
zato, Ludovic Denoyer, and Hervé Jégou. 2018b.
Word translation without parallel data. In Interna-
tional Conference on Learning Representations .
Patrick Littell, David R Mortensen, Ke Lin, Katherine
Kairis, Carlisle Turner, and Lori Levin. 2017. Uriel
and lang2vec: Representing languages as typological,
geographical, and phylogenetic vectors. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers , volume 2, pages 8–14.
Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin Raffel.
2022. Few-shot parameter-efficient fine-tuning is bet-
ter and cheaper than in-context learning. In Advances
in Neural Information Processing Systems .Xialei Liu, Marc Masana, Luis Herranz, Joost Van de
Weijer, Antonio M. López, and Andrew D. Bagdanov.
2018. Rotate your networks: Better weight con-
solidation and less catastrophic forgetting. In 2018
24th International Conference on Pattern Recogni-
tion (ICPR) , pages 2262–2268.
David Lopez-Paz and Marc’Aurelio Ranzato. 2017.
Gradient episodic memory for continual learning. In
Proceedings of the 31st International Conference on
Neural Information Processing Systems , NIPS’17,
page 6470–6479, Red Hook, NY , USA. Curran Asso-
ciates Inc.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .
Rahmad Mahendra, Alham Fikri Aji, Samuel Louvan,
Fahrurrozi Rahman, and Clara Vania. 2021. IndoNLI:
A natural language inference dataset for Indonesian.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
10511–10527, Online and Punta Cana, Dominican
Republic. Association for Computational Linguistics.
Chaitanya Malaviya, Graham Neubig, and Patrick Lit-
tell. 2017. Learning language representations for
typology prediction. In Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
Copenhagen, Denmark.
Arun Mallya and Svetlana Lazebnik. 2018. Packnet:
Adding multiple tasks to a single network by iterative
pruning. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition , pages
7765–7773.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gre-
gory Diamos, Erich Elsen, David Garcia, Boris Gins-
burg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, and Hao Wu. 2018. Mixed precision train-
ing.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-
ley Schoelkopf, Xiangru Tang, Dragomir Radev, Al-
ham Fikri Aji, Khalid Almubarak, Samuel Albanie,
Zaid Alyafeai, Albert Webson, Edward Raff, and
Colin Raffel. 2022. Crosslingual generalization
through multitask finetuning.
Benjamin Muller, Antonios Anastasopoulos, Benoît
Sagot, and Djamé Seddah. 2021. When being un-
seen from mBERT is just the beginning: Handling
new languages with multilingual language models.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 448–462, Online. Association for Computa-
tional Linguistics.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,

--- PAGE 14 ---
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-
bastian Ruder. 2020. MAD-X: An Adapter-Based
Framework for Multi-Task Cross-Lingual Transfer.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7654–7673, Online. Association for Computa-
tional Linguistics.
Ayu Purwarianti and Ida Ayu Putu Ari Crisdayanti. 2019.
Improving bi-lstm performance for indonesian senti-
ment analysis using paragraph vector. In 2019 Inter-
national Conference of Advanced Informatics: Con-
cepts, Theory and Applications (ICAICTA) , pages
1–5.
Oddy Virgantara Putra, Fathin Muhammad Wasman-
son, Triana Harmini, and Shoffin Nahwa Utama.
2020. Sundanese twitter dataset for emotion clas-
sification. In 2020 International Conference on Com-
puter Engineering, Network, and Intelligent Multime-
dia (CENIM) , pages 391–395.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cob Menick, Albin Cassirer, Richard Powell, George
van den Driessche, Lisa Anne Hendricks, Mari-
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
hannes Welbl, Sumanth Dathathri, Saffron Huang,
Jonathan Uesato, John Mellor, Irina Higgins, Anto-
nia Creswell, Nat McAleese, Amy Wu, Erich Elsen,
Siddhant Jayakumar, Elena Buchatskaya, David Bud-
den, Esme Sutherland, Karen Simonyan, Michela Pa-
ganini, Laurent Sifre, Lena Martens, Xiang Lorraine
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
Gribovskaya, Domenic Donato, Angeliki Lazaridou,
Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-
poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-
tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,
Daniel Toyama, Cyprien de Masson d’Autume, Yujia
Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,
Aidan Clark, Diego de Las Casas, Aurelia Guy,
Chris Jones, James Bradbury, Matthew Johnson,
Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed Lockhart, Simon Osindero, Laura
Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,
Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-
ray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling
language models: Methods, analysis & insights from
training gopher.
Joseph Lee Rodgers and W. Alan Nicewander. 1988.
Thirteen ways to look at the correlation coefficient.
The American Statistician , 42(1):59–66.
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timo-
thy Lillicrap, and Gregory Wayne. 2019. Experiencereplay for continual learning. In Advances in Neural
Information Processing Systems , volume 32. Curran
Associates, Inc.
Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault Fevry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. In International Conference on Learning
Representations .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, Jonathan Tow, Alexander M. Rush,
Stella Biderman, Albert Webson, Pawan Sasanka Am-
manamanchi, Thomas Wang, Benoît Sagot, Niklas
Muennighoff, Albert Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina
McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile
Saulnier, Samson Tan, Pedro Ortiz Suarez, Vic-
tor Sanh, Hugo Laurençon, Yacine Jernite, Julien
Launay, Margaret Mitchell, Colin Raffel, Aaron
Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri
Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue,
Christopher Klamm, Colin Leong, Daniel van Strien,
David Ifeoluwa Adelani, Dragomir Radev, Ed-
uardo González Ponferrada, Efrat Levkovizh, Ethan
Kim, Eyal Bar Natan, Francesco De Toni, Gérard
Dupont, Germán Kruszewski, Giada Pistilli, Hady
Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris
Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,
Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,
Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joy-
deep Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro V on Werra, Leon Weber, Long
Phan, Loubna Ben allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
María Grandury, Mario Šaško, Max Huang, Max-
imin Coavoux, Mayank Singh, Mike Tian-Jian Jiang,
Minh Chien Vu, Mohammad A. Jauhar, Mustafa
Ghaleb, Nishant Subramani, Nora Kassner, Nuru-
laqilla Khamis, Olivier Nguyen, Omar Espejel, Ona
de Gibert, Paulo Villegas, Peter Henderson, Pierre
Colombo, Priscilla Amuok, Quentin Lhoest, Rheza
Harliman, Rishi Bommasani, Roberto Luis López,
Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Se-
bastian Nagel, Shamik Bose, Shamsuddeen Hassan
Muhammad, Shanya Sharma, Shayne Longpre, So-
maieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-
ney Zink, Tiago Timponi Torrent, Timo Schick, Tris-
tan Thrush, Valentin Danchev, Vassilina Nikoulina,
Veronika Laippala, Violette Lepercq, Vrinda Prabhu,

--- PAGE 15 ---
Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin
Heinzerling, Chenglei Si, Elizabeth Salesky, Sab-
rina J. Mielke, Wilson Y . Lee, Abheesht Sharma, An-
drea Santilli, Antoine Chaffin, Arnaud Stiegler, Deba-
jyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han
Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan
Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-
ful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-
hal Nayak, Ryan Teehan, Samuel Albanie, Sheng
Shen, Srulik Ben-David, Stephen H. Bach, Taewoon
Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-
mish Thakker, Vikas Raunak, Xiangru Tang, Zheng-
Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,
Hadar Tojarieh, Adam Roberts, Hyung Won Chung,
Jaesung Tae, Jason Phang, Ofir Press, Conglong Li,
Deepak Narayanan, Hatim Bourfoune, Jared Casper,
Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia
Zhang, Mohammad Shoeybi, Myriam Peyrounette,
Nicolas Patry, Nouamane Tazi, Omar Sanseviero,
Patrick von Platen, Pierre Cornette, Pierre François
Lavallée, Rémi Lacroix, Samyam Rajbhandari, San-
chit Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aurélie Névéol, Charles Lover-
ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,
Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog-
danov, Genta Indra Winata, Hailey Schoelkopf, Jan-
Christoph Kalo, Jekaterina Novikova, Jessica Zosa
Forde, Jordan Clive, Jungo Kasai, Ken Kawamura,
Liam Hazan, Marine Carpuat, Miruna Clinciu, Na-
joung Kim, Newton Cheng, Oleg Serikov, Omer
Antverg, Oskar van der Wal, Rui Zhang, Ruochen
Zhang, Sebastian Gehrmann, Shani Pais, Tatiana
Shavrina, Thomas Scialom, Tian Yun, Tomasz Lim-
isiewicz, Verena Rieser, Vitaly Protasov, Vladislav
Mikhailov, Yada Pruksachatkun, Yonatan Belinkov,
Zachary Bamberger, Zden ˇek Kasner, Alice Rueda,
Amanda Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ana Santos, Anthony Hevia, Antigona Unl-
dreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tam-
mour, Azadeh HajiHosseini, Bahareh Behroozi, Ben-
jamin Ajibade, Bharat Saxena, Carlos Muñoz Ferran-
dis, Danish Contractor, David Lansky, Davis David,
Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi
Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline
Ononiwu, Habib Rezanejad, Hessie Jones, Indrani
Bhattacharya, Irene Solaiman, Irina Sedenko, Isar
Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bo-
nis Sanz, Karen Fort, Livia Dutra, Mairon Sama-
gaio, Maraim Elbadri, Margot Mieskes, Marissa Ger-
chick, Martha Akinlolu, Michael McKenna, Mike
Qiu, Muhammed Ghauri, Mykola Burynok, Nafis
Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy,
Olanrewaju Samuel, Ran An, Rasmus Kromann,
Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas
Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi
Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Ab-
hinav Ramesh Kashyap, Alfredo Palasciano, Al-
ison Callahan, Anima Shukla, Antonio Miranda-
Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang,
Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin
Xu, Clémentine Fourrier, Daniel León Periñán,Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio
Barth, Florian Fuhrimann, Gabriel Altay, Giyased-
din Bayrak, Gully Burns, Helena U. Vrabec, Imane
Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas
Golde, Jose David Posada, Karthik Rangasai Sivara-
man, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,
Madeleine Hahn de Bykhovetz, Maiko Takeuchi,
Marc Pàmies, Maria A Castillo, Marianna Nezhurina,
Mario Sänger, Matthias Samwald, Michael Cullan,
Michael Weinberg, Michiel De Wolf, Mina Mihalj-
cic, Minna Liu, Moritz Freidank, Myungsun Kang,
Natasha Seelam, Nathan Dahlberg, Nicholas Michio
Broad, Nikolaus Muellner, Pascale Fung, Patrick
Haller, Ramya Chandrasekhar, Renata Eisenberg,
Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi
Su, Samuel Cahyawijaya, Samuele Garda, Shlok S
Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Si-
mon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Ste-
fan Schweter, Sushil Bharati, Tanmay Laud, Théo
Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis
Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yi-
fan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zi-
fan Ye, Mathilde Bras, Younes Belkada, and Thomas
Wolf. 2022. Bloom: A 176b-parameter open-access
multilingual language model.
Joan Serrà, Dídac Surís, Marius Miron, and Alexan-
dros Karatzoglou. 2018. Overcoming catastrophic
forgetting with hard attention to the task. In ICML .
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
Zerveas, Vijay Korthikanti, Elton Zhang, Rewon
Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia
Song, Mohammad Shoeybi, Yuxiong He, Michael
Houston, Saurabh Tiwary, and Bryan Catanzaro.
2022. Using deepspeed and megatron to train
megatron-turing nlg 530b, a large-scale generative
language model.
Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei
Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra
Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yu-
lia Tsvetkov, Antonios Anastasopoulos, and Graham
Neubig. 2023. Globalbench: A benchmark for global
progress in natural language processing.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stan-
ford alpaca: An instruction-following llama
model. https://github.com/tatsu-lab/
stanford_alpaca .
NLLB Team, Marta R. Costa-jussà, James Cross, Onur
Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia

--- PAGE 16 ---
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022. No language left behind: Scaling human-
centered machine translation.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,
Amin Ghafouri, Marcelo Menegali, Yanping Huang,
Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,
Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-
Ching Chang, Igor Krivokon, Will Rusch, Marc
Pickett, Pranesh Srinivasan, Laichee Man, Kathleen
Meier-Hellstern, Meredith Ringel Morris, Tulsee
Doshi, Renelito Delos Santos, Toju Duke, Johnny So-
raker, Ben Zevenbergen, Vinodkumar Prabhakaran,
Mark Diaz, Ben Hutchinson, Kristen Olson, Ale-
jandra Molina, Erin Hoffman-John, Josh Lee, Lora
Aroyo, Ravi Rajakumar, Alena Butryna, Matthew
Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-
hen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-
Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc
Le. 2022. Lamda: Language models for dialog appli-
cations.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022a. Finetuned
language models are zero-shot learners.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022b. Emer-
gent abilities of large language models. Transactions
on Machine Learning Research . Survey Certifica-
tion.
Bryan Wilie, Karissa Vincentio, Genta Indra Winata,
Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,
Sidik Soleman, Rahmad Mahendra, Pascale Fung,
Syafri Bahar, and Ayu Purwarianti. 2020. IndoNLU:
Benchmark and resources for evaluating Indonesian
natural language understanding. In Proceedings of
the 1st Conference of the Asia-Pacific Chapter of the
Association for Computational Linguistics and the
10th International Joint Conference on Natural Lan-
guage Processing , pages 843–857, Suzhou, China.
Association for Computational Linguistics.
Genta Indra Winata, Alham Fikri Aji, Samuel Cahyaw-
ijaya, Rahmad Mahendra, Fajri Koto, Ade Ro-
madhony, Kemal Kurniawan, David Moeljadi, Ra-
dityo Eko Prasojo, Pascale Fung, et al. 2022.
Nusax: Multilingual parallel sentiment dataset for
10 indonesian local languages. arXiv preprint
arXiv:2205.15960 .
Wilson Wongso, David Samuel Setiawan, and Derwin
Suhartono. 2021. Causal and masked language mod-
eling of javanese language using transformer-basedarchitectures. In 2021 International Conference on
Advanced Computer Science and Information Sys-
tems (ICACSIS) , pages 1–7.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 483–498, On-
line. Association for Computational Linguistics.
Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muen-
nighoff, Alham Fikri Aji, David Ifeoluwa Adelani,
Khalid Almubarak, M Saiful Bari, Lintang Sutawika,
Jungo Kasai, Ahmed Baruwa, Genta Indra Winata,
Stella Biderman, Dragomir Radev, and Vassilina
Nikoulina. 2022. Bloom+1: Adding language sup-
port to bloom for zero-shot prompting.
Zheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde,
Skyler Wang, Samuel Cahyawijaya, Holy Lovenia,
Genta Indra Winata, Lintang Sutawika, Jan Christian
Blaise Cruz, Long Phan, et al. 2023. Prompting
multilingual large language models to generate code-
mixed texts: The case of south east asian languages.
arXiv preprint arXiv:2303.13592 .
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan
Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng
Zhang, Yuxiao Dong, and Jie Tang. 2022. Glm-130b:
An open bilingual pre-trained model.
Ruochen Zhang, Samuel Cahyawijaya, Jan Chris-
tian Blaise Cruz, and Alham Fikri Aji. 2023. Mul-
tilingual large language models are not (yet) code-
switchers.

--- PAGE 17 ---
A Prompt List
In this section, we provide the list of the prompt
used in our experiment. For InstructAlign, we use
6 prompts for each objective. The prompt list for
bilingual denoising ( TLM ), machine translation
(MT), crosslingual semantic similarity ( XSS), and
monolingual denoising ( MLM ) are shown in Ta-
ble 4, Table 5, Table 6, and Table 7, respectively.
For the evaluation, we employ 3 English prompts
for each task. The prompt list for sentiment anal-
ysis, emotion recognition, and topic classification
tasks are described in Table 8, Table 9, and Ta-
ble 10, respectively.
B Comparison Between LLM-int8() and
Full Precision Inference
We run all inference within our experiment with
8-bit quantization using LLM.int8() (Dettmers
et al., 2022). To the best of our knowl-
edge, the effectiveness of LLM.int8() (Dettmers
et al., 2022) has never been evaluated on
zero-shot prompting in low-resource language
cases. We evaluate datasets from various In-
donesian and local languages spoken in In-
donesian which are listed in IndoNLU (Wilie
et al., 2020) and NusaCrowd (Cahyawijaya et al.,
2023b). Specifically, we evaluate on 10 lan-
guages in NusaX (Winata et al., 2022), Javanese
IMDB (Wongso et al., 2021), IndoLEM Senti-
ment (Koto et al., 2020), IndoNLI (Mahendra et al.,
2021), SmSA (Purwarianti and Crisdayanti, 2019),
CASA (Ilmania et al., 2018), and Sundanese Twit-
ter Dataset for Emotion (Putra et al., 2020) datasets.
Based on the result shown in Table 11, there is
only a marginal performance different between 8-
bit quantization with LLM.int8() compared to the
full precision models, which suggests the general-
ization of LLM.int8() (Dettmers et al., 2022) for
zero-shot prompting in low-resource languages.
C Datasets Details
In this section, we describe the statistics for
each dataset use in the experiment. Table 12
shows the statistics for the sentiment analysis task
of NusaTranslation (Cahyawijaya et al., 2023a).
For the Indonesian subset, we take the first fold
of the IndoLEM sentiment (Koto et al., 2020),
which is the Indonesian sentiment analysis dataset
used as the source sentences in the NusaTransla-
tion (Cahyawijaya et al., 2023a). Table 13 showsthe statistics for the sentiment analysis task of
NusaX (Winata et al., 2022). Table 14 and Ta-
ble 15 display the statistics for the emotion recog-
nition and topic classification tasks of NusaPara-
graph (Cahyawijaya et al., 2023a), respectively.
D Detailed Experiment Results
In this section, we provide the complete experi-
mental result per dataset. Table 16 shows the ex-
periment results on the sentiment analysis task
of NusaTranslation. Table 17 shows the exper-
iment results on the sentiment analysis task of
NusaX (Winata et al., 2022). Table 18 and Ta-
ble 19 show the experiment results on the emo-
tion recognition and topic classification tasks of
NusaParagraph, respectively.

--- PAGE 18 ---
Prompt in Bilingual Denoising (TLM) Task
[INPUT_TEXT]. Denoise the previous [INPUT_LANG] text to its
equivalent sentence in [CONTEXT_LANG]: [CONTEXT]\n[LABEL_TEXT]
Context in [CONTEXT_LANG]: [CONTEXT]\nFix the following [INPUT_LANG]
text "[INPUT_TEXT]" ensuring the meaning is equivalent with the
context. [LABEL_TEXT]
Context in [CONTEXT_LANG]: [CONTEXT]\nNoisy text in [INPUT_LANG]:
[INPUT_TEXT]\nHow would you fix the [INPUT_LANG] sentence to make the
meaning the same as the context? [LABEL_TEXT]
[INPUT_TEXT]. Denoise the previous [INPUT_LANG] sentence to it
equivalent sentence: [CONTEXT]\n[LABEL_TEXT]
Context: [CONTEXT]\nFix the following [INPUT_LANG] text
"[INPUT_TEXT]" ensuring the meaning is equivalent with the context.
[LABEL_TEXT]
Context: [CONTEXT]\nNoisy text in [INPUT_LANG]: [INPUT_TEXT]\nHow
would you fix the [INPUT_LANG] sentence to make the meaning the same
as the [CONTEXT_LANG] sentence? [LABEL_TEXT]
Table 4: Prompt used for Bilingual Denoising ( TLM ) task
Prompt in Machine Translation (MT) Task
Translate the following text from [SOURCE_LANG] to
[TARGET_LANG].\nText: [SOURCE_TEXT]\nTranslation: [TARGET_TEXT]
[SOURCE_TEXT]\nTranslate the text above from [SOURCE_LANG] to
[TARGET_LANG]. [TARGET_TEXT]
Text in [SOURCE_LANG]: [SOURCE_TEXT]\nHow would you translate that in
[TARGET_LANG]? [TARGET_TEXT]
Translate the following text to [TARGET_LANG].\nText:
[SOURCE_TEXT]\nTranslation: [TARGET_TEXT]
[SOURCE_TEXT]\nTranslate the text above to [TARGET_LANG].
[TARGET_TEXT]
Input text: [SOURCE_TEXT]\nHow would you translate that into
[TARGET_LANG]? [TARGET_TEXT]
Table 5: Prompt used for Machine Translation ( MT) task
Prompt in Crosslingual Semantic Similarity (XSS) Task
[SOURCE_LANG] sentence: [SOURCE_TEXT]\n[TARGET_LANG] sentence:
[TARGET_TEXT]\nDo the two sentences have the same meaning? [LABEL]
Sentence A: [SOURCE_TEXT]\nSentence B: [TARGET_TEXT]\nDo sentence A
and sentence B have the same meaning? [LABEL]
[SOURCE_LANG] sentence: [SOURCE_TEXT]\n[TARGET_LANG] sentence:
[TARGET_TEXT]\nAre the two sentences equivalent? [LABEL]
Sentence A: [SOURCE_TEXT]\nSentence B: [TARGET_TEXT]\nAre sentence A
and sentence B equivalent? [LABEL]
Is the [SOURCE_LANG] sentence "[SOURCE_TEXT]" equivalent to the
[TARGET_LANG] sentence "[TARGET_TEXT]"? [LABEL]
Is the sentence "[SOURCE_TEXT]" equivalent to the sentence
"[TARGET_TEXT]"? [LABEL]
Table 6: Prompt used for Crosslingual Semantic Similarity ( XSS) task

--- PAGE 19 ---
Prompt in Monolingual Denoising (MLM) Task
Denoise the following noisy [SOURCE_LANG] text: "[SOURCE_TEXT]", to
make a correct sentence. [TARGET_TEXT]
Fix and complete the following [SOURCE_LANG] sentence:
[SOURCE_TEXT]\n[TARGET_TEXT]
Sentence in [SOURCE_LANG]: [SOURCE_TEXT]\nHow would you fix the
sentence to make a correct sentence? [TARGET_TEXT]
Denoise the following noisy text "[SOURCE_TEXT]" to make a correct
[SOURCE_LANG] sentence. [TARGET_TEXT]
Fix and complete the following sentence: [SOURCE_TEXT]\n[TARGET_TEXT]
Input text: [SOURCE_TEXT]\nHow would you fix the sentence to make a
correct [SOURCE_LANG] sentence? [TARGET_TEXT]
Table 7: Prompt used for Monolingual Denoising ( MLM ) task
Prompt in Sentiment Analysis Task
[INPUT]\nWhat would be the sentiment of the text above? [OPTIONS]?
[LABELS_CHOICE]
What is the sentiment of this text?\nText: [INPUT]\nAnswer with
[OPTIONS]: [LABELS_CHOICE]
Text: [INPUT]\n\nPlease classify the sentiment of above text.
Answer with [OPTIONS]: [LABELS_CHOICE]
Table 8: Prompt used for Sentiment Analysis task
Prompt in Emotion Recognition Task
[INPUT]\nWhat would be the emotion of the text above? [OPTIONS]?
[LABELS_CHOICE]
What is the emotion of this text?\nText: [INPUT]\nAnswer with
[OPTIONS]: [LABELS_CHOICE]
Text: [INPUT]\n\nPlease classify the emotion of above text. Answer
with [OPTIONS]: [LABELS_CHOICE]
Table 9: Prompt used for Emotion Recognition task
Prompt in Topic Classification Task
[INPUT]\nWhat would be the topic of the text above? [OPTIONS]?
[LABELS_CHOICE]
What is the topic of this text?\nText: [INPUT]\nAnswer with
[OPTIONS]: [LABELS_CHOICE]
Text: [INPUT]\n\nPlease classify the topic of above text. Answer
with [OPTIONS]: [LABELS_CHOICE]
Table 10: Prompt used for the Topic Classification task

--- PAGE 20 ---
Model Prompt Lang. Acc Macro F1 Macro Prec. Macro Rec.
Full Precision
BLOOMZ-560M EN 47.58 33.25 37.97 43.11
BLOOMZ-560M ID 44.37 29.78 37.79 40.28
BLOOMZ-1B1 EN 52.26 37.90 40.48 45.79
BLOOMZ-1B1 ID 52.88 39.28 46.42 46.67
BLOOMZ-1B7 EN 51.44 36.90 41.90 45.10
BLOOMZ-1B7 ID 52.68 41.20 50.81 48.03
8-Bit Quantization
BLOOMZ-560M EN 47.56 34.67 40.94 42.97
BLOOMZ-560M ID 43.64 33.30 42.90 39.68
BLOOMZ-1B1 EN 50.68 37.52 40.37 44.56
BLOOMZ-1B1 ID 51.23 38.69 43.53 45.34
BLOOMZ-1B7 EN 49.71 35.05 42.11 43.57
BLOOMZ-1B7 ID 52.61 41.87 51.74 48.15
BLOOMZ-3B EN 54.80 40.78 46.59 48.24
BLOOMZ-3B ID 56.75 44.34 45.16 51.12
Table 11: Evaluation of full precision and 8-bit quantization on various Indonesian local languages datasets.

--- PAGE 21 ---
Status Language Train Valid. Test
Pre-trained Indonesian (ind) 3638 399 1011
SeenJavanese (jav) 3400 448 1200
Sundanese (sun) 3400 448 1200
Minangkabau (min) 3400 448 1200
UnseenAmbon (abs) 250 98 500
Batak (btk) 3400 448 1200
Betawi (bew) 3400 448 1200
Bima (bhp) 260 100 500
Madurese (mad) 3400 448 1200
Makassarese (mak) 3400 448 1200
Musi (mui) 250 91 500
Rejang (rej) 250 78 500
Table 12: Statistics of NusaTranslation sentiment anal-
ysis dataset. Pre-trained denotes languages that are
already seen before the InstructAlign tuning. Seen
denotes languages that are seen during the InstructAl-
ign.Unseen denotes languages that are still unseen after
the InstructAlign.
Status Language Train Valid. Test
Pre-trainedEnglish (eng) 500 100 400
Indonesia (ind) 500 100 400
SeenAceh (ace) 500 100 400
Bali (ban) 500 100 400
Banjar (bjn) 500 100 400
Bugis (bug) 500 100 400
Minang (min) 500 100 400
Javanese(jav) 500 100 400
Sunda (sun) 500 100 400
UnseenMadura (mad) 500 100 400
Ngaju (nij) 500 100 400
Bataknese (bbc) 500 100 400
Table 13: Statistics of NusaX sentiment analysis dataset.
Pre-trained denotes languages that are already seen
before the InstructAlign. Seen denotes languages that
are seen during the InstructAlign. Unseen denotes lan-
guages that are still unseen after the InstructAlign.Status Language Train Valid. Test
UnseenJavanese (jav) 2800 440 800
Minangkabau (min) 2000 357 800
Sundanese (sun) 2400 400 800
Buginese (bug) 87 50 300
SeenBatak (btk) 1150 292 500
Betawi (bew) 2700 430 800
Madurese (mad) 1000 263 500
Makassarese(mak) 1500 304 500
Musi (mui) 200 75 400
Rejang (rej) 136 50 300
Table 14: Statistics of NusaParagraph emotion recog-
nition dataset. Pre-trained denotes languages that are
already seen before InstructAlign. Seen denotes lan-
guages that are seen during InstructAlign. Unseen de-
notes languages that are still unseen after InstructAlign.
Status Language Train Valid. Test
UnseenJavanese (jav) 2650 448 800
Minangkabau (min) 2400 399 800
Sundanese (sun) 2800 468 900
Buginese (bug) 93 50 300
SeenBatak (btk) 1350 275 500
Betawi (bew) 2650 435 800
Madurese (mad) 1800 367 700
Makassarese(mak) 1500 376 700
Musi (mui) 168 80 400
Rejang (rej) 105 50 350
Table 15: Statistics of NusaParagraph topic classifica-
tion dataset. Pre-trained denotes languages that are
already seen before InstructAlign. Seen denotes lan-
guages that are seen during InstructAlign. Unseen de-
notes languages that are still unseen after InstructAlign.

--- PAGE 22 ---
ModelL1 L2 L3
ind jav min sun abs bew bhp btk mad mak mui rej
BLOOM 560m 61.47 56.09 58.13 58.63 62.53 58.42 49.72 56.05 54.02 53.97 60.27 55.55
BLOOM 1b1 58.81 59.05 59.33 59.16 47.87 58.23 60.85 58.95 58.79 58.83 54.92 57.73
BLOOM 3b 58.30 44.84 45.48 44.61 46.08 45.61 44.54 43.62 43.36 44.03 45.05 43.15
BLOOMZ 560m 69.81 43.00 50.97 46.51 45.23 47.87 33.13 36.69 36.84 35.30 61.42 36.21
BLOOMZ 1b1 80.40 61.32 68.95 61.75 61.07 66.94 46.18 50.20 49.71 50.66 70.31 52.01
BLOOMZ 3b 81.38 68.05 71.76 68.43 69.57 69.76 67.73 65.09 64.37 63.14 69.08 64.05
MLM BLOOMZ 560m 65.68 23.29 21.11 22.31 20.86 22.00 20.40 19.04 21.10 20.59 28.82 19.50
MLM BLOOMZ 560m-r=100k 71.93 63.89 69.37 66.27 64.46 65.38 56.12 62.68 58.20 56.51 67.73 58.12
MLM BLOOMZ 1b1-r=100k 73.25 71.18 72.24 70.95 62.67 67.65 56.07 59.22 58.60 60.38 68.10 60.27
MT BLOOMZ 560m 55.20 41.87 39.00 38.16 36.88 39.29 36.07 34.74 36.97 33.81 41.72 37.70
MT BLOOMZ 560m-r=100k 74.46 70.73 69.94 70.00 66.81 67.65 64.58 66.53 65.10 61.35 68.43 63.23
MT BLOOMZ 1b1-r=100k 70.86 59.62 62.22 61.63 54.37 57.97 49.95 50.22 51.31 52.22 60.25 50.30
TLM BLOOMZ 560m 71.57 66.74 66.05 66.94 63.06 65.64 59.00 61.07 61.31 61.13 65.30 63.17
TLM BLOOMZ 560m-r=1k 70.52 61.73 62.76 62.01 54.34 56.31 48.52 49.44 49.21 47.95 61.11 49.22
TLM BLOOMZ 560m-r=10k 72.82 66.27 66.22 66.76 62.29 63.32 59.61 61.27 60.35 60.32 63.60 60.49
TLM BLOOMZ 560m-r=100k 72.40 61.05 59.43 62.11 54.51 56.44 46.68 50.72 50.56 45.02 63.27 48.39
TLM BLOOMZ 1b1-r=100k 75.66 70.05 70.12 70.70 64.47 67.07 62.92 61.87 60.96 61.86 68.11 61.53
XSS BLOOMZ 560m 64.48 57.65 52.18 54.13 52.40 53.59 48.55 48.06 49.59 44.01 58.03 49.43
XSS BLOOMZ 560m-r=1k 69.34 63.55 62.84 65.45 65.20 64.15 59.11 60.53 62.34 61.58 63.51 58.36
XSS BLOOMZ 560m-r=10k 72.22 67.89 67.81 67.25 62.76 64.42 62.83 61.98 62.02 62.21 65.38 59.27
XSS BLOOMZ 560m-r=100k 71.27 68.34 67.89 68.07 61.58 68.69 62.66 65.73 63.44 58.24 70.24 64.92
XSS BLOOMZ 1b1-r=100k 76.75 72.40 71.40 71.87 63.75 65.45 60.27 61.27 60.10 63.21 66.16 60.49
Table 16: Experiment result on the sentiment analysis task of the NusaTranslation dataset
ModelL1 L2 L3
eng ind ace ban bjn bug jav min sun bbc mad nij
BLOOM 560m 29.26 21.13 21.35 21.93 21.35 23.21 21.86 21.82 21.04 22.28 22.13 21.11
BLOOM 1b1 22.02 22.54 21.47 22.62 22.27 21.34 22.97 21.92 21.55 22.10 21.65 21.53
BLOOM 3b 24.03 21.17 21.31 21.17 21.18 21.35 21.17 21.17 21.17 21.19 21.20 21.17
BLOOMZ 560m 58.24 55.59 31.18 32.40 37.17 27.79 35.86 39.29 32.44 29.49 32.80 38.15
BLOOMZ 1b1 57.41 58.58 43.31 43.02 44.72 31.12 46.52 42.59 39.20 26.82 41.92 40.76
BLOOMZ 3b 62.65 63.21 48.81 48.40 55.27 23.47 54.26 51.11 39.41 32.42 38.88 41.68
MLM BLOOMZ 560m 49.99 49.33 31.74 28.37 34.32 25.76 33.89 31.27 29.20 28.43 32.08 30.98
MLM BLOOMZ 560m-R-100000 61.32 60.01 42.69 41.69 50.95 31.53 44.28 44.30 42.11 33.18 41.05 40.15
MLM BLOOMZ 1b1-R-100000 61.30 59.73 43.11 43.02 50.71 31.31 53.66 51.05 47.27 31.13 42.02 39.83
MT BLOOMZ 560m 47.24 41.41 31.78 33.78 34.69 28.44 35.47 35.15 36.01 26.86 26.69 27.49
MT BLOOMZ 560m-R-100000 60.09 54.18 39.11 42.59 46.22 34.50 43.37 41.31 41.31 35.95 38.54 39.84
MT BLOOMZ 1b1-R-100000 59.18 53.69 43.97 45.40 50.16 38.65 48.37 45.97 41.98 37.97 40.90 40.60
TLM BLOOMZ 560m 44.72 46.02 33.59 34.26 41.16 25.36 41.76 38.72 37.40 25.67 30.88 29.98
TLM BLOOMZ 560m-R-1000 58.05 54.59 43.03 37.06 46.55 34.02 43.21 43.24 39.59 33.99 38.16 37.39
TLM BLOOMZ 560m-R-10000 57.38 57.73 43.43 36.76 45.99 35.06 44.38 43.30 40.83 34.06 42.46 40.00
TLM BLOOMZ 560m-R-100000 61.65 56.50 41.78 41.36 48.15 31.19 48.89 44.12 44.90 33.78 41.51 37.90
TLM BLOOMZ 1b1-R-100000 64.26 63.54 52.22 51.35 58.19 41.87 59.48 59.67 56.99 38.26 48.11 48.01
XSS BLOOMZ 560m 53.93 53.19 43.60 41.73 47.09 37.79 47.29 45.36 43.42 32.59 41.66 40.79
XSS BLOOMZ 560m-R-1000 56.57 54.90 36.78 40.28 42.20 28.56 45.67 41.33 39.80 27.30 31.67 32.20
XSS BLOOMZ 560m-R-10000 55.62 57.84 44.24 44.03 50.04 32.87 48.92 45.55 45.64 36.38 40.36 43.12
XSS BLOOMZ 560m-R-100000 59.89 58.22 45.53 39.57 52.68 36.15 49.83 50.61 46.45 35.27 42.40 43.39
XSS BLOOMZ 1b1-R-100000 60.78 59.34 45.83 45.45 53.08 36.24 52.24 50.54 47.20 33.81 40.99 41.08
Table 17: Experiment result on the sentiment analysis task of the NusaX dataset

--- PAGE 23 ---
ModelL2 L3
bug jav min sun bew btk mad mak mui rej
BLOOM 560m 1.19 2.42 4.54 3.05 4.37 2.56 0.59 1.42 1.11 2.66
BLOOM 1b1 1.19 2.42 4.54 3.05 4.29 2.57 0.59 1.42 1.11 2.44
BLOOM 3b 1.19 2.42 4.54 3.05 4.29 2.57 0.59 1.42 1.11 2.44
BLOOMZ 560m 2.36 2.93 4.71 3.52 4.35 3.33 1.41 3.09 1.28 4.10
BLOOMZ 1b1 1.19 2.42 4.54 3.05 4.29 2.57 0.59 1.42 1.11 2.44
BLOOMZ 3b 1.19 2.42 4.54 3.05 4.29 2.57 0.59 1.42 1.11 2.44
MLM BLOOMZ 560m 1.19 2.51 4.63 3.04 4.29 2.57 0.59 1.42 1.11 2.44
MLM BLOOMZ 560m-R-100000 1.19 2.41 4.54 3.05 4.29 2.57 0.59 1.42 1.11 2.44
MLM BLOOMZ 1b1-R-100000 1.19 2.42 4.71 3.05 4.29 2.57 0.59 1.42 1.11 2.44
MT BLOOMZ 1b1-R-100000 1.60 2.76 4.77 3.54 4.27 2.56 0.59 1.42 1.12 2.44
MT BLOOMZ 560m 1.41 2.58 9.14 5.63 4.45 2.57 0.59 1.56 2.10 2.44
MT BLOOMZ 560m-R-100000 1.19 2.51 4.54 3.04 4.29 2.70 0.59 1.42 1.11 2.44
TLM BLOOMZ 560m 1.19 2.58 4.88 3.54 4.29 2.57 0.59 1.42 1.11 2.44
TLM BLOOMZ 560m-R-1000 1.19 2.50 5.10 3.14 4.29 2.57 0.59 1.42 1.12 2.44
TLM BLOOMZ 560m-R-10000 1.19 2.67 5.12 4.34 4.29 2.57 0.73 1.42 1.11 2.66
TLM BLOOMZ 560m-R-100000 1.40 2.42 4.54 3.13 4.29 2.71 0.73 1.42 1.11 2.44
TLM BLOOMZ 1b1-R-100000 1.19 2.41 4.63 3.13 4.29 2.57 0.59 1.42 1.12 2.44
XSS BLOOMZ 560m 1.54 3.12 4.65 3.77 4.29 2.71 0.59 1.42 1.11 2.44
XSS BLOOMZ 560m-R-1000 1.56 2.82 5.16 3.54 4.37 2.69 0.73 1.42 1.11 2.44
XSS BLOOMZ 560m-R-10000 1.39 2.84 5.56 4.10 4.29 2.57 0.59 1.43 1.28 2.44
XSS BLOOMZ 560m-R-100000 1.19 2.42 4.54 3.05 4.29 2.57 0.59 1.42 1.11 2.42
XSS BLOOMZ 1b1-R-100000 1.19 2.67 4.63 3.84 4.29 2.57 0.59 1.42 1.11 2.44
Table 18: Experiment result on the emotion recognition task of the NusaParagraph dataset

--- PAGE 24 ---
ModelL2 L3
bug jav min sun bew btk mad mak mui rej
BLOOM-560m 7.68 3.50 6.36 3.80 5.42 7.92 11.25 9.07 3.91 5.80
BLOOM-1b1 7.72 3.50 6.36 3.81 5.42 7.93 11.26 9.09 3.91 5.82
BLOOM-3b 7.72 3.50 6.36 3.81 5.42 7.93 11.26 9.09 3.91 5.82
BLOOMZ-560m 9.13 4.10 6.86 4.29 6.07 8.75 11.71 9.45 4.09 6.04
BLOOMZ-1b1 7.72 3.50 6.36 3.81 5.51 7.93 11.26 9.09 3.91 5.82
BLOOMZ-3b 7.72 4.21 6.70 4.30 7.55 8.33 11.28 9.19 7.30 5.82
MLM BLOOMZ-560m 8.15 3.50 6.36 3.81 5.42 7.93 11.26 9.09 3.91 5.82
MLM BLOOMZ-560m r=100000 7.72 3.49 6.52 4.36 5.51 7.93 11.27 9.09 4.08 5.82
MLM BLOOMZ-1b1 r=100000 7.72 3.50 6.36 3.81 5.42 7.93 11.26 9.09 3.91 5.82
MT BLOOMZ-560m 7.72 3.50 6.36 3.81 5.42 7.93 11.26 9.09 3.92 5.82
MT BLOOMZ-560m r=100000 7.72 3.58 6.37 3.93 5.52 7.94 11.22 9.19 3.92 5.82
MT BLOOMZ-1b1 r=100000 8.61 4.59 7.08 5.08 5.71 8.20 11.52 9.29 4.63 6.01
TLM BLOOMZ-560m 9.43 3.83 7.27 7.11 5.42 7.93 11.28 9.18 3.92 5.82
TLM BLOOMZ-560m r=1000 14.08 11.46 17.31 16.55 10.35 12.61 11.92 12.04 9.34 5.96
TLM BLOOMZ-560m r=10000 8.37 4.23 7.66 5.40 5.43 8.05 11.20 9.28 4.25 5.96
TLM BLOOMZ-560m r=100000 7.75 3.50 6.34 3.80 5.51 7.93 11.35 9.18 4.23 5.78
TLM BLOOMZ-1b1 r=100000 7.71 3.67 6.55 4.05 5.42 7.93 11.27 9.08 3.91 5.82
XSS BLOOMZ-560m 8.38 3.57 6.46 3.88 5.42 7.94 11.26 9.09 3.91 5.82
XSS BLOOMZ-560m r=1000 6.14 4.21 4.34 6.14 4.38 7.29 11.21 8.39 5.52 6.63
XSS BLOOMZ-560m r=10000 8.06 4.24 7.46 5.07 5.41 8.23 11.32 9.10 4.08 5.85
XSS BLOOMZ-560m r=100000 7.73 3.50 6.67 4.23 5.50 7.93 11.26 9.09 3.92 5.82
XSS BLOOMZ-1b1 r=100000 8.00 4.05 7.40 4.62 5.67 8.08 11.55 9.19 4.08 5.83
Table 19: Experiment result on the topic classification task of the NusaParagraph dataset
