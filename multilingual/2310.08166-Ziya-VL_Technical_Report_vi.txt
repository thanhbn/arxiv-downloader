# Báo cáo kỹ thuật Ziya-VL
# ZIYA-VISUAL : MÔ HÌNH NGÔN NGỮ-THỊ GIÁC LỚN SONG NGỮ
# THÔNG QUA ĐIỀU CHỈNH HƯỚNG DẪN ĐA NHIỆM

Junyu Lu♥∗Dixiang Zhang♥♦∗Xiaojun Wu♥∗Xinyu Gao♥
Ruyi Gan♥♣†Jiaxing Zhang♥Yan Song♣Pingjian Zhang♦
♥Viện Kinh tế Số Quốc tế♣Đại học Khoa học và Công nghệ Trung Quốc
♦Đại học Công nghệ Nam Trung Quốc
{lujunyu, zhangdixiang, wuxiaojun, ganruyi, zhangjiaxing }@idea.edu.cn
clksong@gmail.com pjzhang@scut.edu.cn

TÓM TẮT
Những tiến bộ gần đây mở rộng khả năng của các mô hình ngôn ngữ lớn (LLM) trong việc tạo và hiểu văn bản từ hình ảnh zero-shot bằng cách tích hợp đầu vào đa phương thức. Tuy nhiên, thành công như vậy thường bị hạn chế trong các kịch bản tiếng Anh do thiếu tài nguyên đa phương thức phi tiếng Anh quy mô lớn và chất lượng cao, khiến việc thiết lập các đối tác cạnh tranh bằng các ngôn ngữ khác trở nên cực kỳ khó khăn. Trong bài báo này, chúng tôi giới thiệu loạt Ziya-Visual, một bộ mô hình ngôn ngữ-thị giác lớn song ngữ (LVLM) được thiết kế để kết hợp ngữ nghĩa thị giác vào LLM cho đối thoại đa phương thức. Bao gồm Ziya-Visual-Base và Ziya-Visual-Chat, các mô hình của chúng tôi áp dụng Querying Transformer từ BLIP-2, tiếp tục khám phá sự hỗ trợ của các sơ đồ tối ưu hóa như điều chỉnh hướng dẫn, huấn luyện đa giai đoạn và mô-đun thích ứng low-rank cho việc liên kết thị giác-ngôn ngữ. Ngoài ra, chúng tôi kích thích khả năng hiểu của GPT-4 trong các kịch bản đa phương thức, dịch các bộ dữ liệu hình ảnh-văn bản tiếng Anh thu thập được sang tiếng Trung và tạo ra hướng dẫn-phản hồi thông qua phương pháp học trong ngữ cảnh. Kết quả thử nghiệm chứng minh rằng so với các LVLM hiện có, Ziya-Visual đạt hiệu suất cạnh tranh trên nhiều nhiệm vụ chỉ dành cho tiếng Anh bao gồm truy xuất hình ảnh-văn bản zero-shot, mô tả hình ảnh và trả lời câu hỏi thị giác. Bảng xếp hạng đánh giá được GPT-4 truy cập cũng chỉ ra rằng các mô hình của chúng tôi sở hữu khả năng hiểu và tạo văn bản-hình ảnh thỏa đáng trong các cuộc đối thoại kịch bản đa phương thức tiếng Trung. Mã nguồn, demo và mô hình có sẵn tại https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1.

1 GIỚI THIỆU

Các mô hình ngôn ngữ lớn (LLM) như GPT3 Brown et al. (2020), LLaMA Touvron et al. (2023) và Vicuna Chiang et al. (2023) đã thu hút sự chú ý rộng rãi do khả năng tạo và hiểu văn bản mạnh mẽ. Các mô hình này có thể thể hiện khả năng tương tác mạnh mẽ bằng cách học thêm ý định của người dùng trong các bộ dữ liệu điều chỉnh hướng dẫn được thiết kế cẩn thận Wei et al. (2021). Gần đây, các mô hình ngôn ngữ-thị giác quy mô lớn (LVLM) nổi bật như BLIP-2 Li et al. (2023c), LLaVA Liu et al. (2023a) và mPLUG-Owl Ye et al. (2023) đã được phát triển để khám phá tiềm năng của LLM trong việc nhận thức và hiểu tín hiệu thị giác, và chúng đã thể hiện khả năng ấn tượng trong việc giải quyết các cuộc đối thoại và suy luận đa phương thức trong thế giới thực.

Tuy nhiên, thành công của các mô hình ngôn ngữ-thị giác quy mô lớn chủ yếu đạt được trong cộng đồng tiếng Anh, vì một lượng lớn bộ dữ liệu mô tả hình ảnh-văn bản, như Laion Schuhmann et al. (2022), CC12M Changpinyo et al. (2021b) và SBU Ordonez et al. (2011b), có thể được sử dụng để liên kết các biểu diễn văn bản và thị giác. Hơn nữa, các nghiên cứu chính thống cố gắng chuyển đổi đồng nhất dữ liệu đa phương thức có giám sát từ một số danh mục nhiệm vụ thành các định dạng điều chỉnh hướng dẫn, như MULTIINSTRUCT Xu et al. (2022) và MIMIC-IT Li et al. (2023a), kết nối toàn diện tính không đồng nhất của các nhiệm vụ đa phương thức qua các kịch bản và loại khác nhau. Kết quả là, khoảng cách tài nguyên dữ liệu đã cản trở nghiêm trọng sự phát triển của các mô hình đa phương thức phi tiếng Anh.

Để thúc đẩy sự phát triển mạnh mẽ của cộng đồng mã nguồn mở đa phương thức, chúng tôi giới thiệu loạt Ziya-Visual mã nguồn mở. Các mô hình Ziya-Visual song ngữ bao gồm các phiên bản Ziya-Visual-Base và Ziya-Visual-Chat, cả hai đều mở rộng mô hình ngôn ngữ Ziya-LLaMA-13B thông qua Q-Former và bộ mã hóa thị giác, trang bị cho nó khả năng hiểu thị giác. Cụ thể, Ziya-Visual-Base tiếp tục các điểm ngắt của BLIP-2 Li et al. (2023c), liên kết các biểu diễn thị giác-ngôn ngữ dưới tiền đề đóng băng LLM và bộ mã hóa thị giác. Sau giai đoạn tiền huấn luyện và điều chỉnh hướng dẫn hai giai đoạn, nó sử dụng một chút dữ liệu mô tả hình ảnh và điều chỉnh hướng dẫn để kích thích khả năng hiểu và tạo thông tin thị giác của LLM. Ngoài ra, Ziya-Visual-Chat là một mô hình ngôn ngữ-thị giác tương tác dựa trên khung instructBLIP được tiền huấn luyện Dai et al. (2023), áp dụng một số chiến lược huấn luyện, bao gồm điều chỉnh hướng dẫn và thích ứng low-rank, để cải thiện liên kết song ngữ. Sau tiền huấn luyện và điều chỉnh hướng dẫn đa giai đoạn, nó có thể trích xuất các đặc trung thông tin từ hình ảnh và hướng dẫn một cách linh hoạt hơn, và hỗ trợ các kịch bản đa phương thức phức tạp như đối thoại đa lượt, trả lời câu hỏi tình huống, suy luận logic. Để đạt được điều này, chúng tôi tận dụng tài nguyên đa phương thức tiếng Anh đáng kể như một trục để trình bày bộ dữ liệu Bilingual Multi-Modal In-Context (BMMIC), bao gồm hơn 5 triệu cặp hình ảnh-văn bản, sử dụng GPT-4 OpenAI (2023) để dịch tự động và tạo các cặp câu hỏi-trả lời ngôn ngữ-thị giác tiếng Trung. Cụ thể, các đặc điểm của các mô hình loạt Ziya-Visual bao gồm:

• Hiệu suất cạnh tranh: Ngang bằng với các Mô hình Ngôn ngữ Thị giác Lớn mã nguồn mở đơn ngữ hiện đại nhất dưới cùng quy mô mô hình trên một số điểm chuẩn đánh giá, bao gồm mô tả hình ảnh zero-shot, trả lời câu hỏi thị giác và suy luận thị giác.

• Khả năng song ngữ hỗ trợ nhận dạng văn bản và suy luận thị giác: Ziya-Visual tự nhiên hỗ trợ đối thoại đa phương thức trong cả kịch bản tiếng Anh và tiếng Trung. Chúng tôi đã khám phá tác động của các chiến lược huấn luyện khác nhau đối với hiệu suất mô hình, phục vụ như một tài liệu tham khảo có giá trị cho các nhà nghiên cứu đồng nghiệp.

• Dữ liệu đa phương thức song ngữ mã nguồn mở: Chúng tôi cung cấp bộ dữ liệu phản hồi-hướng dẫn trong ngữ cảnh đa phương thức bao gồm các kịch bản đối thoại thế giới thực khác nhau, cùng với một đường ống chú thích tự động cho dịch thuật và tạo sinh.

2 CÔNG TRÌNH LIÊN QUAN

2.1 MÔ HÌNH NGÔN NGỮ LỚN ĐA PHƯƠNG THỨC

Do việc mở rộng quy mô dữ liệu huấn luyện và kích thước mô hình, việc mở rộng các mô hình ngôn ngữ lớn sang học đa phương thức đã thu hút sự chú ý rộng rãi. Flamingo Alayrac et al. (2022), LLaMA-Adapter Zhang et al. (2023a); Gao et al. (2023) và Otter Li et al. (2023b) tích hợp các lớp cross-attention có thể học vào LLM được tiền huấn luyện để nhận thức kiến thức thị giác, và huấn luyện trên bộ dữ liệu hình ảnh-văn bản xen kẽ quy mô lớn. Để thúc đẩy tiền huấn luyện thị giác-và-ngôn ngữ hiệu quả và hữu hiệu hơn, BLIP-2 Li et al. (2023c) sử dụng Q-Former như các bộ nhận thức để liên kết đặc trưng thị giác được truy vấn với văn bản, tiền huấn luyện học biểu diễn thông qua nhiều mất mát ngôn ngữ-thị giác. InstructBLIP Dai et al. (2023) tiếp tục đề xuất trích xuất đặc trưng thị giác nhận biết hướng dẫn, cho phép trích xuất đặc trưng linh hoạt và thông tin theo hướng dẫn đã cho. Ngoài ra, một số phương pháp truyền các đặc trưng được trích xuất từ bộ mã hóa thị giác đến đầu vào LLM thông qua phép chiếu tuyến tính, như LLaVA Liu et al. (2023a) và Shikra Chen et al. (2023), sử dụng thông tin thị giác theo cách trực tiếp và trực quan hơn.

2.2 BỘ DỮ LIỆU ĐIỀU CHỈNH HƯỚNG DẪN ĐA PHƯƠNG THỨC

Để khám phá điều chỉnh hướng dẫn cho học đa phương thức, MULTIINSTRUCT Xu et al. (2022) đầu tiên đề xuất bộ dữ liệu điểm chuẩn điều chỉnh hướng dẫn đa phương thức, chuyển đổi 62 nhiệm vụ đa phương thức khác nhau thành định dạng seq-to-seq thống nhất. Do GPT4 đã chứng minh khả năng hiểu mạnh mẽ về các biểu diễn văn bản đa phương thức, LLaVA Liu et al. (2023a) giới thiệu một góc nhìn và đường ống cải cách dữ liệu để chuyển đổi các cặp hình ảnh-văn bản thành định dạng tuân theo hướng dẫn phù hợp bằng cách sử dụng ChatGPT/GPT-4 OpenAI (2023). Hơn nữa, MIMIC-IT Li et al. (2023a) chuyên môn hóa việc tạo các cặp hướng dẫn-phản hồi thông qua thông tin trong ngữ cảnh đa phương thức và các cảnh thị giác đa dạng. M3IT Li et al. (2023d) chuyển đổi các nhiệm vụ ngôn ngữ-thị giác cổ điển thành lược đồ thị giác-thành-văn bản thống nhất thông qua viết hướng dẫn thủ công và tiền xử lý bộ dữ liệu, bao gồm mô tả, trả lời câu hỏi thị giác, tạo sinh có điều kiện thị giác, suy luận và phân loại.

3 PHƯƠNG PHÁP LUẬN

3.1 XÂY DỰNG HƯỚNG DẪN-PHẢN HỒI

Hình 1: Tổng quan về việc xây dựng các cặp hướng dẫn-phản hồi song ngữ của chúng tôi. Chúng tôi sử dụng chú thích của con người và giai đoạn khởi động với GPT-4 OpenAI (2023) để tạo thông điệp hệ thống và các ví dụ trong ngữ cảnh. Tiếp theo, chúng tôi tận dụng khả năng hiểu đa phương thức của GPT-4 để tạo các cặp hướng dẫn-phản hồi và các bộ lọc được chuẩn bị trước để chọn dữ liệu chất lượng cao.

Trong phần này, chúng tôi nhằm xây dựng bộ dữ liệu trong ngữ cảnh đa phương thức song ngữ (BMMIC) để hỗ trợ nhiều LVLM hơn trong việc nâng cao khả năng hiểu đối thoại thế giới thực. Như được minh họa trong Hình 1, chúng tôi cung cấp tổng quan về bộ dữ liệu BMMIC, bao gồm chuẩn bị dữ liệu và đường ống tạo/dịch hướng dẫn-phản hồi tự động.

Cộng đồng đã chứng kiến sự xuất hiện của dữ liệu đa phương thức có sẵn công khai như các cặp hình ảnh-văn bản, bao gồm trả lời câu hỏi tham khảo, đối thoại theo ngữ cảnh và mô tả chú thích về hình ảnh. Tuy nhiên, tính khả dụng của dữ liệu tuân theo hướng dẫn đa phương thức bị hạn chế, chủ yếu vì quá trình chú thích tốn thời gian và các mục tiêu chú thích ít được định nghĩa rõ ràng. Lấy cảm hứng từ hiệu suất ấn tượng của GPT-4 trong liên kết đa phương thức và học trong ngữ cảnh, chúng tôi chọn các bộ dữ liệu COCO Lin et al. (2014), Flickr30k Young et al. (2014) và AI Challenger 2017 Wu et al. (2019) cho tạo sinh đa phương thức và các bộ dữ liệu LLaVA Liu et al. (2023a), M3IT Li et al. (2023d) cho dịch thuật đa phương thức thông qua cách tiếp cận tuân theo hướng dẫn. Chúng tôi chủ yếu xem xét thiết kế thông điệp hệ thống và học trong ngữ cảnh Liu et al. (2023a); Dong et al. (2022) khi gọi giao diện GPT-4. Hơn nữa, để giảm thiểu việc xảy ra các cặp hướng dẫn-phản hồi sai lệch do thiên kiến nhận thức trong GPT-4, chúng tôi phát triển một bộ phát hiện dựa trên quy tắc để lọc bỏ dữ liệu vượt quá kỳ vọng.

Định dạng đầu vào: Chúng tôi thu được hai loại biểu diễn ký hiệu từ các bộ dữ liệu đa phương thức được chú thích: (1) Chú thích thường mô tả cảnh thị giác từ các góc độ khác nhau. (2) Hộp giới hạn định vị các đối tượng thị giác trong cảnh thị giác, với mỗi hộp giới hạn mã hóa ngữ nghĩa và vị trí không gian của đối tượng. Ở đây, chúng tôi tận dụng đường ống Grounded-SAM, bao gồm các mô-đun Grounding-DINO Liu et al. (2023b), SAM Kirillov et al. (2023) và RAM Zhang et al. (2023b), để trích xuất đặc trưng thị giác tỉ mỉ từ hình ảnh được chuyển đổi thành các hộp biên giới và thẻ thị giác như biểu diễn ký hiệu.

Thông điệp hệ thống: Chúng tôi tùy chỉnh vai trò GPT-4 cho tạo hướng dẫn và dịch thuật dựa trên thành phần của các loại hướng dẫn và yêu cầu viết lại, bao gồm định dạng đầu vào, loại phản hồi, ràng buộc phong cách và đường dẫn mở rộng. Theo LLaVA, chúng tôi ký hiệu Conversation, Detail description và Complex reasoning để tạo dữ liệu hướng dẫn và Translation để dịch các cặp hướng dẫn-phản hồi tiếng Anh có sẵn. Chúng tôi tiếp tục cung cấp Deepening (tăng độ sâu và rộng của các cặp hướng dẫn-phản hồi), Concretizing (phức tạp hóa các cặp hướng dẫn-phản hồi bằng cách thay thế các khái niệm chung bằng các khái niệm cụ thể hơn), Increasing Reasoning (viết lại các phản hồi để yêu cầu rõ ràng suy luận đa bước) và Adding Constraints (ràng buộc và chuyên môn hóa định dạng và nội dung của các cặp hướng dẫn-phản hồi), bốn sơ đồ viết lại để cải thiện tính đa dạng và phong phú của hướng dẫn và phản hồi.

Học trong ngữ cảnh: Đối với hình ảnh thứ p, chúng tôi lấy một số chú thích liên quan XC_q cùng với các hộp giới hạn được gắn thẻ XB_q, và chú thích thủ công 50 cặp hướng dẫn-phản hồi như các ví dụ truy vấn, mỗi cặp được ký hiệu là Iq và Rq. Hơn nữa, chúng tôi có thể định nghĩa một hàm trong ngữ cảnh Cψ:
User [XC_q, XB_q], Assistant [Iq, Rq] 7→pθ Assistant [Ik, Rk]|User [XC_k, XB_k]
để biểu diễn việc tạo sinh trong ngữ cảnh đa phương thức với ví dụ truy vấn hiện tại. Tương tự, chúng tôi đơn giản chuyển sang các hướng dẫn hệ thống tương ứng để dịch các cặp hướng dẫn-phản hồi tiếng Anh.

3.2 KIẾN TRÚC MÔ HÌNH

Kiến trúc tổng thể của loạt Ziya-Visual bao gồm ba thành phần:

Mô hình ngôn ngữ lớn: Ziya-Visual áp dụng mô hình ngôn ngữ lớn song ngữ như xương sống nền tảng, được khởi tạo với Ziya-LLaMA-13B được tiền huấn luyện Wang et al. (2022). Ziya-LLaMA-13B kế thừa các điểm ngắt từ LLaMA ban đầu, thích ứng tokenizer, chiến lược huấn luyện và trọng số mô hình cho phiên bản song ngữ.

Bộ mã hóa thị giác: Chúng tôi áp dụng Vision Transformer (ViT) Dosovitskiy et al. (2020) như bộ mã hóa thị giác để trích xuất một số lượng đặc trưng đầu ra cố định từ hình ảnh, được khởi tạo với trọng số được tiền huấn luyện từ ViT/G-14.

Querying Transformer: Để giảm thiểu các vấn đề dư thừa thông tin và không hiệu quả gây ra bởi các chuỗi đặc trưng hình ảnh dài, Ziya-Visual sử dụng Q-Former Li et al. (2023c) để kết nối khoảng cách giữa bộ mã hóa hình ảnh được tiền huấn luyện và LLM. Như được hiển thị trong Hình 3.3, Q-Former bao gồm hai mô-đun transformer con chia sẻ cùng các lớp self-attention: (1) một transformer hình ảnh tương tác với bộ mã hóa thị giác để nén đặc trưng hình ảnh. Mô-đun sử dụng một nhóm embedding truy vấn thị giác có thể học như đầu vào cho transformer hình ảnh, tương tác với nhau thông qua các lớp self-attention và nén đặc trưng hình ảnh từ 256 × 768 xuống 64 thông qua các lớp cross-attention. (2) một transformer văn bản mã hóa các bản trình bày văn bản được ghép nối với hình ảnh đầu vào. Q-Former kiểm soát khả năng hiển thị giữa các truy vấn và văn bản thông qua ma trận mặt nạ self-attention, cho phép thực hiện các nhiệm vụ tiền huấn luyện khác nhau để liên kết các đặc trưng thị giác-ngôn ngữ. Trong giai đoạn điều chỉnh hướng dẫn, chuỗi đặc trưng thị giác được nén tiếp tục được đưa vào mô hình ngôn ngữ lớn.

3.3 HUẤN LUYỆN

Quá trình huấn luyện của các mô hình Ziya-Visual bao gồm ba giai đoạn: giai đoạn đầu tiên của tiền huấn luyện và hai giai đoạn huấn luyện điều chỉnh hướng dẫn.

Tiền huấn luyện: Như được minh họa trong Hình 3.3, trong giai đoạn tiền huấn luyện, chúng tôi chủ yếu thu thập một số bộ dữ liệu quy mô lớn, nhiều noise của các cặp hình ảnh-văn bản được thu thập từ web. Chúng tôi tận dụng mô hình CLIP-v2 song ngữ nội bộ Radford et al. (2021) để làm sạch và tích lũy một phần dữ liệu mô tả hình ảnh tiếng Trung-Anh chất lượng cao dựa trên các bộ dữ liệu mã nguồn mở có sẵn công khai, góp phần cải thiện chất lượng dữ liệu tổng thể. Như được tóm tắt trong Bảng 1, chúng tôi chuẩn bị tổng cộng 13 triệu cặp hình ảnh-văn bản tiếng Anh và 17 triệu cặp tiếng Trung cho tiền huấn luyện.

Chúng tôi kết nối Q-Former với một bộ mã hóa hình ảnh bị đóng băng và thực hiện giai đoạn tiền huấn luyện với các cặp hình ảnh-văn bản, nhằm hướng dẫn các truy vấn đã học để trích xuất các biểu diễn thị giác thông tin nhất từ văn bản thông qua liên kết biểu diễn. Theo thực hành của BLIP2 Li et al. (2023c) và ALBEF Li et al. (2021), chúng tôi cũng tối ưu hóa đồng thời ba mục tiêu tiền huấn luyện, Image-Text Contrastive Learning (ITC), Image-grounded Text Generation (ITG) và Image-Text Matching (ITM), chia sẻ cùng định dạng đầu vào và tham số mô hình. Đặc biệt, đối với Ziya-Visual-Chat, chúng tôi tiếp tục sử dụng độ tương tự contrastive để lấy mẫu một cặp hình ảnh-văn bản âm khó trong batch cho nhiệm vụ ITM.

Mô hình được huấn luyện sử dụng bộ tối ưu hóa AdamW Kingma & Ba (2014) với tốc độ học lr = 1e−5, β1 = 0.9, β2 = 0.9 và eps = 1e−8. Chúng tôi sử dụng weight decay 5e−2 và gradient clipping 2.0. Quá trình huấn luyện sử dụng batch size 2048 cho các cặp hình ảnh-văn bản, và toàn bộ giai đoạn tiền huấn luyện kéo dài 90000 bước.

Điều chỉnh hướng dẫn: Trong giai đoạn đầu tiên của điều chỉnh hướng dẫn (Multi-task Representation Learning), để đảm bảo tính đa dạng của dữ liệu điều chỉnh hướng dẫn, chúng tôi thu thập một loạt các bộ dữ liệu điều chỉnh tỉ mỉ ngôn ngữ-thị giác có sẵn công khai và chuyển đổi chúng thành định dạng điều chỉnh hướng dẫn sử dụng các quy tắc được thiết kế thủ công. Như được tóm tắt trong Bảng 3, chúng tôi huấn luyện Ziya-Visual trên 4 nhiệm vụ đồng thời. Đối với mô tả hình ảnh, chúng tôi thu thập một bộ các bộ dữ liệu điều chỉnh tỉ mỉ học thuật và cạnh tranh song ngữ chất lượng cao, bao gồm COCO Lin et al. (2014), COCO-CN Li et al. (2019), Flickr30k Young et al. (2014), Flickr30k-CNA (bao gồm chuỗi ICM, IQM, ICR, IQR) Xie et al. (2022), AI Challenger 2017 Wu et al. (2019) và TextCaps Sidorov et al. (2020). Chúng tôi sử dụng hỗn hợp dữ liệu có sẵn công khai cho trả lời câu hỏi thị giác bao gồm GQA Hudson & Manning (2019), VQAv2 Goyal et al. (2017), OCR-VQA Mishra et al. (2019), TextVQA Singh et al. (2019) và CLEVR Johnson et al. (2017), trong đó các bộ dữ liệu VQAv2 và GQA được dịch thêm sang tiếng Trung thông qua đường ống dịch thuật, được đặt tên là VQAv2 (T) và GQA (T). Hơn nữa, đối với các nhiệm vụ nhị nguyên grounding tham chiếu và mô tả grounded, chúng tôi xây dựng các cặp hướng dẫn-phản hồi huấn luyện từ Visual Genome Krishna et al. (2017), RefCOCO, RefCOCO+, RefCOCOg Kazemzadeh et al. (2014) và VCR Zellers et al. (2019).

Như được hiển thị trong phần bên trái của Hình 3.3, đối với Ziya-Visual-Base, chúng tôi đóng băng Q-Former và kết nối nó với LLM để khai thác khả năng ngôn ngữ tạo sinh của LLM, trong đó các truy vấn chuyển đổi thông tin hình ảnh thành các biểu diễn giống văn bản có thể hiểu được. Một lớp fully-connected (FC) có thể huấn luyện được sử dụng để chiếu tuyến tính các token truy vấn đầu ra vào LLM như tiền tố hướng dẫn. Trong Ziya-Visual-Chat, văn bản hướng dẫn không chỉ được đưa vào LLM mà còn vào transformer văn bản trong Q-Former. Hướng dẫn tương tác với các truy vấn thông qua các lớp self-attention của Q-Former, ảnh hưởng đến các truy vấn hướng tới trích xuất các đặc trưng hình ảnh thông tin hơn về nhiệm vụ như được mô tả bởi hướng dẫn Dai et al. (2023).

Trong giai đoạn thứ hai của điều chỉnh hướng dẫn (Scene-aware Knowledge Learning), chúng tôi thu thập một bộ các bộ dữ liệu điều chỉnh tỉ mỉ hướng dẫn chất lượng cao và thông tin để nâng cao khả năng tuân theo hướng dẫn và đối thoại của các mô hình Ziya-Visual, bao gồm các cuộc hội thoại đa lượt, suy luận logic, mô tả tình huống và miêu tả chi tiết trong các kịch bản hướng dẫn-phản hồi. Do đó, chúng tôi thu thập một bộ các bộ dữ liệu hướng dẫn-phản hồi mã nguồn mở mới nhất, bao gồm LLaVA Liu et al. (2023a), M3IT Li et al. (2023d) và SVIT Zhao et al. (2023), bao gồm một loạt các kịch bản đa phương thức trong giọng điệu hội thoại để sát với các cuộc đối thoại thế giới thực. Tương tự, chúng tôi cũng xây dựng phiên bản tiếng Trung của LLaVA (T) và M3IT (T) sử dụng đường ống dịch thuật và sử dụng đường ống tạo sinh để tự chú thích dữ liệu In-House.

Trong quá trình huấn luyện, Ziya-Visual-Base mở Q-Former để điều chỉnh tỉ mỉ trên khung hiện có. Đối với Ziya-Visual-Chat, chúng tôi xem xét rằng mô hình có thể học các khái niệm và kiến thức thị giác được mô tả trong hình ảnh thông qua học kiến thức thị giác. Do đó, ngoài việc mở Q-Former và lớp chiếu, chúng tôi áp dụng thích ứng low-rank (LoRA) trong ViT và LLM để thích ứng fV và fL bằng cách huấn luyện nhiều ma trận low-rank cho liên kết hiệu quả với hướng dẫn của con người.

Hình 3: Đường ống điều chỉnh hướng dẫn hai giai đoạn của Ziya-Visual. (Trên) Học biểu diễn đa nhiệm vụ và học kiến thức nhận biết cảnh cho Ziya-Visual-Base. (Dưới) Ziya-Visual-Chat với trích xuất đặc trưng thị giác nhận biết hướng dẫn và mô-đun thích ứng low-rank.

4 THỬ NGHIỆM VÀ PHÂN TÍCH

Trong phần này, chúng tôi đánh giá loạt Ziya-Visual trên các nhiệm vụ ngôn ngữ-thị giác truyền thống khác nhau, chủ yếu bao gồm các nhiệm vụ mô tả hình ảnh và trả lời câu hỏi thị giác tổng quát (VQA) như được trình bày trong Bảng 4. Ngoài ra, chúng tôi đánh giá khả năng đối thoại đa phương thức đa ngôn ngữ của Ziya-Visual trong cả tiếng Trung và tiếng Anh trên điểm chuẩn LLaVA và phiên bản dịch tiếng Trung của nó.

4.1 MÔ TẢ HÌNH ẢNH VÀ TRẢ LỜI CÂU HỎI THỊ GIÁC TỔNG QUÁT

Nhiệm vụ mô tả hình ảnh yêu cầu mô hình tạo mô tả cho một hình ảnh đã cho. Chúng tôi sử dụng Nocaps Agrawal et al. (2019) và Flickr30k Young et al. (2014) như các bộ dữ liệu đánh giá tiếng Anh, và báo cáo điểm CIDEr làm thước đo. Đối với phiên bản tiếng Trung, chúng tôi chọn các tập con dịch phổ biến COCO-CN và Flickr30k-CNA, và báo cáo thêm điểm truy xuất hình ảnh-văn bản. Nhiệm vụ VQA tổng quát yêu cầu mô hình tạo câu trả lời cho cặp hình ảnh-câu hỏi đã cho. Chúng tôi sử dụng năm điểm chuẩn tiếng Anh để đánh giá, bao gồm VQAv2 Goyal et al. (2017), OKVQA Marino et al. (2019), GQA Hudson & Manning (2019), ScienceQA (Image Set) Lu et al. (2022) và VizWiz VQA Gurari et al. (2018). Trong các kịch bản chung, chúng tôi sử dụng tạo câu trả lời mở, trong khi trong các trường hợp có tùy chọn được cung cấp, chúng tôi ràng buộc đầu ra của mô hình theo các lựa chọn có thể. Chúng tôi đánh giá trong môi trường tiếng Trung sử dụng các bộ kiểm tra GQA (T) và VQAv2 (T).

Hiệu suất tổng thể trên các nhiệm vụ mô tả hình ảnh và VQA tổng quát tiếng Anh được báo cáo trong Bảng 5. Chúng tôi có thể quan sát rằng Ziya-Visual-Base, được thích ứng cho liên kết đa phương thức song ngữ và điều chỉnh hướng dẫn thêm trên BLIP-2, cho thấy cải thiện trên các nhiệm vụ ngôn ngữ-thị giác tiếng Anh khác nhau. Trong khi đó, với các điều chỉnh tối thiểu sử dụng một lượng nhỏ cặp hướng dẫn-phản hồi tiếng Trung, Ziya-Visual-Base thể hiện khả năng hiểu và tạo sinh đáng khen ngợi trong các nhiệm vụ đa phương thức tiếng Trung. Tiếp theo, chúng tôi báo cáo hiệu suất tổng thể trên các nhiệm vụ mô tả hình ảnh và VQA tổng quát tiếng Trung trong Bảng 6. Với sự hỗ trợ của nhiều bộ dữ liệu điều chỉnh hướng dẫn hơn và các chiến lược huấn luyện toàn diện, Ziya-Visual-Chat đạt hiệu suất ngang bằng với các LVLM tiếng Anh hiện đại (InstructBLIP Dai et al. (2023) và Shikra Chen et al. (2023)), và vượt trội hơn hầu hết các LVLM đa ngôn ngữ hiện có trong các kịch bản tiếng Trung. Cụ thể, so với các mô hình có hiệu suất xuất sắc trong cộng đồng mã nguồn mở, như mPLUG-Owl Ye et al. (2023) và VISCPM-Chat Hu et al. (2023), chúng tôi đạt hiệu suất tương đương trong nhiệm vụ truy xuất hình ảnh-văn bản. Hơn nữa, trên thước đo CIDEr cho các bộ dữ liệu COCO-CN, Flickr30k-CNA và GQA (T), Ziya-Visual-Chat đạt điểm trung bình 110.4, vượt trội hơn 96.5 của mPLUG-Owl và 108.6 của VISCPM-Chat. Chúng tôi cũng quan sát rằng trên thước đo độ chính xác cho GQA (T) và VQAv2 (T), chúng tôi đạt điểm trung bình 51.8%, vượt trội hơn mPLUG-Owl và VISCPM-Chat lần lượt 6.15% và 1.75%. Sự cải thiện cho thấy rằng Ziya-Visual-Chat có thể nắm bắt chính xác hơn các đối tượng thị giác trong hình ảnh thông qua điều chỉnh tỉ mỉ nhận biết hướng dẫn, và thích ứng low-rank trên các mô-đun LLM và ViT.

4.2 ĐIỂM CHUẨN LLAVA

Điểm chuẩn LLaVA bao gồm 90 trường hợp, mỗi trường hợp chứa một hình ảnh với các biểu diễn ký hiệu, một câu hỏi và một câu trả lời, đánh giá toàn diện hiệu suất của mô hình trong hội thoại, mô tả chi tiết và suy luận phức tạp thông qua chấm điểm GPT-4. Chúng tôi so sánh Ziya-Visual với các mô hình hội thoại đa phương thức hiện có, bao gồm các mô hình chỉ tiếng Anh: MiniGPT-4 Zhu et al. (2023), InstructBLIP Dai et al. (2023), và LLaVA Liu et al. (2023a), cũng như các mô hình song ngữ Trung-Anh: mPLUG-Owl (LLaMA-7B) Ye et al. (2023), VisualGLM (ChatGLM-6B) Du et al. (2022) và VISCPM-Chat (CPM-Bee-10B) Hu et al. (2023).

Như được hiển thị trong Hình 7, Ziya-Visual-Base thể hiện hiệu suất cân bằng trên các thước đo khác nhau, cho thấy rằng mô hình đạt được khả năng hiểu đa phương thức sơ bộ. Với việc giới thiệu nhiều cặp hướng dẫn-phản hồi song ngữ hơn, Ziya-Visual-Chat đạt điểm GPT-4 trung bình 84.1 trong tiếng Anh và 86.7 trong tiếng Trung, vượt trội hơn Ziya-Visual-Base lần lượt 2.4 và 5.9. Sự cải thiện đáng kể này làm nổi bật hiệu quả của các chiến lược trích xuất đặc trưng thị giác nhận biết hướng dẫn và thích ứng low-rank trong việc thúc đẩy nhận thức LLM và liên kết biểu diễn thị giác, cho phép Ziya-Visual-Chat hiểu các hướng dẫn phức tạp và các kịch bản đa dạng và cung cấp các phản hồi chính xác.

4.3 NGHIÊN CỨU TRƯỜNG HỢP

Để quan sát trực quan khả năng hiểu và tạo sinh đa phương thức của loạt Ziya-Visual, trong Hình 4.3, chúng tôi trình bày các cuộc đối thoại liên quan đến Ziya-Visual-Base và Ziya-Visual-Chat trong một lựa chọn các ví dụ định tính.

Từ kết quả dự đoán của trường hợp 1 suy luận phức tạp và trường hợp 2 mô tả chi tiết, chúng tôi có thể quan sát rằng Ziya-Visual-Base thể hiện khả năng hiểu và tạo sinh đa phương thức ban đầu. Nó cho phép mô hình nhận thức toàn diện các đối tượng thị giác và ngữ nghĩa toàn cầu trong hình ảnh, tiếp tục cung cấp các phản hồi tương ứng với hướng dẫn. Tuy nhiên, Ziya-Visual-Base vẫn thể hiện những thiếu sót đáng kể trong ảo giác thị giác và tính trôi chảy của biểu đạt. Cụ thể, trong các phản hồi cho trường hợp 1 và trường hợp 2, nó đề cập sai lầm đến '马(horse)' và '卡车(truck)', không phải là các đối tượng thị giác có mặt trong hình ảnh. Hơn nữa, mặc dù mô hình có thể tạo ra các câu trả lời với ngữ cảnh dài, nó vẫn bỏ lỡ nhiều yếu tố trong hình ảnh.

Thú vị là, Ziya-Visual-Chat giải quyết đáng kể các thiếu sót nêu trên, có thể nắm bắt chính xác các chi tiết hình ảnh và cung cấp phân tích có cấu trúc tốt và chuỗi suy luận dựa trên hướng dẫn. Chúng tôi có thể quan sát rằng trong trường hợp 1, Ziya-Visual-Chat có thể cung cấp phân tích có cơ sở về 'Những thách thức nào mà người phụ nữ đi xe máy có thể gặp phải trong tình huống này?' từ các khía cạnh khác nhau. Trong trường hợp 2, mặc dù câu trả lời của mô hình ngắn gọn, nó bao gồm chính xác các đối tượng thị giác như '城市街道(city street)', '大型时钟(large clock)', '树(tree)' và '汽车(car)'.

5 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi phát hành loạt Ziya-Visual, một bộ toàn diện các mô hình ngôn ngữ-thị giác song ngữ quy mô lớn và đường ống xây dựng cặp hướng dẫn-phản hồi, được thiết kế để thúc đẩy phát triển trong cộng đồng mã nguồn mở đa phương thức. Ziya-Visual-Chat mới nhất đạt hiệu suất tương đương hoặc vượt trội một chút so với các LVLM chỉ tiếng Anh và song ngữ chính thống trên các điểm chuẩn khác nhau, thể hiện khả năng hiểu và tạo sinh đa phương thức đáng kể trong hội thoại đa lượt, suy luận phức tạp và mô tả chi tiết. Nhìn về tương lai, chúng tôi cam kết tiếp tục nâng cao khả năng của Ziya-Visual trong một số khía cạnh chính:

• Phát triển khả năng của Ziya-Visual trong các nhiệm vụ đa phương thức như đối thoại tham chiếu, phân biệt cảnh chung, hiểu video.

• Nâng cao khả năng hiểu đối tượng thị giác tỉ mỉ của Ziya-Visual bằng cách mở rộng cấu trúc mô hình và dữ liệu huấn luyện, cho phép nó xử lý chân dung chi tiết và nhận dạng hành động trong hình ảnh phức tạp.

• Mở rộng chức năng của Ziya-Visual trong tạo sinh đa phương thức, đặc biệt trong việc tích hợp diffusion để đạt được tạo sinh text-to-image và image-to-image.
