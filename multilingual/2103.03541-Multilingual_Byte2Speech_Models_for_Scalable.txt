# 2103.03541.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2103.03541.pdf
# File size: 1371078 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Multilingual Byte2Speech Models for Scalable
Low-resource Speech Synthesis
Mutian He1Jingzhou Yang2Lei He2Frank K. Soong2
1The Hong Kong University of Science and Technology
2Microsoft China
mhear@cse.ust.hk
{jingy,helei,frankkps}@microsoft.com
Abstract
To scale neural speech synthesis to various real-world languages, we present a multi-
lingual end-to-end framework that maps byte inputs to spectrograms, thus allowing
arbitrary input scripts. Besides strong results on 40+ languages, the framework
demonstrates capabilities to adapt to new languages under extreme low-resource
and even few-shot scenarios of merely 40s transcribed recording, without the need
of per-language resources like lexicon, extra corpus, auxiliary models, or linguistic
expertise, thus ensuring scalability. While it retains satisfactory intelligibility and
naturalness matching rich-resource models. Exhaustive comparative and ablation
studies are performed to reveal the potential of the framework for low-resource
languages. Furthermore, we propose a novel method to extract language-speciﬁc
sub-networks in a multilingual model for a better understanding of its mechanism.
1 Introduction
Recent years witnessed the magniﬁcent triumph of end-to-end deep learning. Particularly for speech
synthesis (or text-to-speech, TTS), pipelines and handcrafted features are substituted by end-to-end
neural models (Shen et al., 2018). But such a success relies on a rich resource of high-quality data,
and data requirements have thus become a bottleneck of research and application on neural TTS.
Various researches have been proposed for the issue. However, most of them require extra resources.
For instance, phoneme-input models reduce the modeling complexity and reach better performance
(Yasuda et al., 2021), but lexicons and/or grapheme-to-phoneme (G2P) rules of the target language
must be given. While character-input methods require knowledge of the script to deﬁne model
inputs on non-alphabetic scripts such as Indic and Chinese ones. Some methods leverage phonology
knowledge to reduce data requirements (Cai et al., 2020; Chen et al., 2019; Demirsahin et al., 2018),
thus linguistic expertise on the target is essential. Besides, a speech chain of synthesis and recognition
plays a key role in the low-resource TTS by Ren et al. (2019) and Xu et al. (2020), which relies on an
additional large paired corpus to train a recognizer. To conclude, all such methods depend on extra
resources: data, knowledge, or developer’s efforts for each target language.
It is often costly to prepare the resources and build a complex system on a language, not to say to
repeat this on thousands of languages in the world. Therefore, we propose a novel scalability-centered
task: We must refrain from using extra per-language resources. Developers should not put efforts on
any language, but rely on paired TTS data which often could be collected in a standardized process.
For such a challenge, we highlight transfer learning from rich resource languages. Performances
of low-resource languages are improved in a multilingual model on a cohort of languages (Li and
Zen, 2016; Demirsahin et al., 2018; de Korte et al., 2020; Yang and He, 2020). Despite the immense
diversity of languages, identical or similar writing systems, G2P rules, phoneme inventory, or at least
Preprint. Under review.arXiv:2103.03541v2  [cs.CL]  9 Jul 2021

--- PAGE 2 ---
226.5429
428.11650
255.32330
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%#Hours (Total 909.9)#Languages (Total 43)#Speakers (Total 109) T1
T2
T3Figure 1: Statistics of the training corpus, as for amounts of data, languages, and speakers per tier.
the pattern to learn sequential mapping in TTS can be leveraged for transfer learning. Particularly,
we extend Li et al. (2019a)’s method. By leveraging the existent text encodings of UTF-8, arbitrary
scripts covered by Unicode are supported without our need to study individual writing systems. We
build a strong multilingual multi-speaker transformer on a 900-hour corpus of 43 languages by 109
speakers written in various scripts. Using a tier-wise progressive and language-balanced strategy,
such a model learns to produce correct and natural speech similar to phoneme-based methods.
We then evaluate its capability to adapt to selected low-resource target languages upon a linguistic
basis. Through a co-training strategy, it can learn a brand new language such as Romanian and
Greek in a few-shot regime of 10 samples or less than 1 minute of audio, and to reach topline
performance with much fewer data. In this way, more marginalized, endangered or underrepresented
languages may beneﬁt from neural TTS. Also, we investigate the contribution of various factors in our
framework by exhaustive empirical studies. Furthermore, to better understand the mechanism of the
model, we propose a novel approach to interpret a multilingual model by language-speciﬁc pruning
and show that a multilingual model can be viewed as a fusion of monolingual models sharing part of
parameters between each other, which emerges from training. To facilitate reproduction, we present a
pipeline based on open resources.1To conclude, the contributions of the paper are three-fold:
•We build a multilingual Byte2Speech framework for a scalability-centered TTS task. With well-
designed training strategies, it matches performance of phoneme models on a variety of languages.
•We investigate its low-resource capabilities to adapt to new languages in few shots and to reach
results similar to baseline models using data reduced by an order of magnitude.
• We deepen our understandings of multilingual model mechanisms by a novel interpretation.
2 Methods
2.1 Framework
We adopt the multilingual and multispeaker transformer TTS framework in Yang and He (2020) and
extend it to byte inputs. In detail, we train a 12-layer transformer to predict mel-spectrograms, with
language and speaker embeddings concatenated by encoder outputs, while we ﬁnd no improvements
if we feed language ID to inputs. Inputs texts are encoded in UTF-8, each byte as a token, and fed
into the model along with [BOS] and [EOS], so the vocabulary sizes 256 except for special tokens
like [BOS]. Hence, in our inputs, a character can be represented in one or more tokens. We directly
use encoded texts for training, with only basic pre-processings like rule-based transformations of
digits and Chinese word segmentation. Besides, Unicode characters with separable parts can often
be represented in multiple ways. For example, each precomposed Korean character stands for a
syllable, but it is equivalent to record it by a series of Jamo (letters). Similar cases include ligatures
and letters with diacritic marks. Although using non-precomposed tokens reveals more about a
character, following our principle for using minimal language-speciﬁc knowledge, we do not apply
any normalization on characters and allow precomposed characters to present in data as-is.
2.2 Training corpus
We use 43 languages as our source languages, with 32 distinct languages in ISO-639-1, plus their
regional variants.2The corpus thus covers a majority of human scripts and phonemes. Particularly,
besides Latin alphabets, there are also Cyrillic alphabets, Chinese characters, syllabic scripts (like
1The pipeline and audio demos are available at github.com/mutiann/few-shot-transformer-tts .
2For simplicity we use the term “language” for all of them. See Appendix Section C for dataset details.
2

--- PAGE 3 ---
Korean), abugida (like Hindi), and abjad (like Arabics) . With such diversity, relationships between
text and speech are highly disparate. Languages like Spanish use a phonemic orthography, while
English G2P is far from regular. Alphabets record all phonemes, while abjads typically omit vowels.
Chinese logograms do not record pronunciations, and in Japanese, a Chinese character can have 10
different readings. Making it worse, as in the real world, language resources are highly imbalanced.
We split the languages into three tiers by number of samples. As shown in Figure 1, T1 languages like
English (US) possess a disproportional number of samples and speakers, while only <10k samples
are available for each T3 language. Therefore, despite the large corpus, obtaining a multilingual
source model remains challenging.
2.3 Adaptation targets
To best reveal the adaptation capabilities, we inspect the linguistic similarity between each adaptation
target and each tier of source languages, including aspects of writing systems, pronunciation or G2P
rules, lexicon, and phonetic traits. Based on this, we pick ﬁve particular targets:
•Indian English (en-in), which, as for our data, is mostly mutual-intelligible with other English
variants as for writings, G2P, lexicon, and phoneme inventory. However, there are many phonetic
shifts, such as pronouncing “th” in “then” as voiced dental plosive similar to /d/.
•Romanian (ro-ro), which uses the Latin alphabet with ﬁve extra letters, and has a close-to-phonemic
orthography that G2P is mostly one-to-one. The script, G2P, lexicon, and phoneme inventory are
close to other Romance languages like Italian and French in T1, thus allowing an effective transfer.
•Greek (el-gr), which uses the Greek alphabet unseen in sources, thus the model needs to learn a new
alphabet with each letter in two bytes. Greek orthography is of intermediate-depth with one-to-N
relations from phoneme to letters, but G2P is mostly regular using rules similar to other alphabets.
Also, there are lexical and phonetic links with other European languages, giving chance to transfer.
•Thai (th-th), which is written in 3-byte abugida unseen in sources. Abugidas, unlike alphabets, ﬁrst
record a syllable and then modify its vowel by diacritics. Only in T3 there are Indic languages using
abugidas, but with different encodings. Furthermore, Thai is tonal, uses less phonemic G2P with rich
irregularities, and has no particularly similar source language. All of these hinder adaptation.
•Mandarin Chinese (zh-cn) is a tonal language written in 3-byte simpliﬁed Chinese logograms, hence
a byte-based model must memorize the reading of each character. Though some characters overlap
with Japanese in T1 and Cantonese in T2 (in traditional Chinese), the spoken forms are systematically
different. Thus it is particularly challenging and the help of source languages is dubious.
Besides, we analyze the phoneme inventory involved. The training corpus covers all phonemes used
in target languages since T2, showing the capability of the source model to articulate the targets. Thus
the key for adaptation is to learn to handle input scripts and pronunciation rules. While for lower tiers
like T1, there is one phoneme from ro-ro absent, one from el-gr, two from th-th, and four from zh-cn.
Those phonemes rare in source languages add to difﬁculties. To conclude, from en-in to th-th/zh-cn,
the task becomes more challenging and similar source languages are present only in higher tiers.
2.4 Training strategy
Transformer TTS suffers from unstable training, often requiring tricks such as alignment constraints
(Chen et al., 2020). Byte inputs and multilingualism add to the complexity, making our tier-wise
progressive training essential: We initialize all models by training on selected short en-us samples
for 30k steps. Starting from it, T1 data are added, and then T2 data at total 350k steps, and T3 at
650k. Languages are exponentially balanced: For language iwithNisamples, we compute
ci=Ni=X
jNj
pi=c
i=X
jc
j
During training, we ﬁrst sample a language iwith probability piusing= 0:2, and then a training
example from it (Yang and He, 2020). To improve efﬁciency, we use 4 GPUs with dynamic batching
(Li et al., 2019b). Alternative training settings lead to suboptimal performance in experiments.
3

--- PAGE 4 ---
Table 1: CER (%) on sources with data size labeled, using best results of each model
LANGUAGE EN -US(150 H) DE-DE(30H) ZH-HK(30H) TE-IN(5H)
PHONEME 3.23 2.13 13.16 11.35
BYTE (T3) 2.43 1.18 15.67 9.58
As for low resource adaptation, we discover that merely training by the target results in overﬁtting,
and rich source languages can serve as regularization. Therefore, we adopt a multitask or co-training
strategy. Instead of ﬁne-tuning a well-pretrained model, we add the target language with pi= 0:25to
train with a mixture of sources and the target. To identify the impact of source languages, we attempt
to perform adaptation after each tier transition plus using en-us only, that is to co-train with en-us
from 30k steps, T1 from 350k, added by T2 from 500k, and T3 from 700k. Exponential learning rate
decay is applied in a period of 850k steps, with the step counter reset at adaptation and tier transitions.
2.5 Evaluation metrics
For each language, we generate mel-spectrograms on a held-out set of 100 samples (utterances),
in comparison with mels from recordings. Waveforms produced by Grifﬁn-Lim are sent to Azure
Speech-To-Text to get character error rate (CER) as a large-scale intelligibility metric. Also, we
evaluate the quality by mean square error (MSE) with ground truth mels, both after collapsing
unvoiced parts and dynamic time warping using FastDTW (Salvador and Chan, 2007). Besides,
we collect an extra set of news scripts of much more ( 1000) samples per language with longer
and more complicated sentences. We compute CER upon these scripts to build a CER-Ex metric.
We further perform subjective tests on selected models. For intelligibility, we invite ﬁve judges per
sample to annotate word errors on audios from the 100-sample held-out set. For mean opinion score
(MOS), we invite 20 judges per sample on a 20-sample evaluation set selected from the held-out set,
on which none of the models have mispronunciations, in order to best compare the naturalness. We
use a pretrained WaveNet to generate waveform for subjective tests. Results are also compared with
phoneme-based multilingual models by Yang and He (2020) trained on the same dataset as a topline.
3 Experiments
3.1 Source languages
We ﬁrst demonstrate intelligibility by CER as in Table 1. The Byte (T3) model achieves CER
comparable to the phoneme-based model on languages using phonetic scripts, including en-us from
T1, German (de-de) from T2, and Telugu (te-in) from T3. These results show that a sophisticated
byte-based model sufﬁces to learn various phonetic scripts at the same time, and may even produce
fewer errors than a phoneme-based model, possibly thanks to our training strategy and representation
sharing of input tokens. This also applies to Telugu with fewer data and a unique abugida. As for the
logographic Cantonese (zh-hk), CERs are both high due to rich homophones, but the gap between
models is limited, showing that the byte model may memorize and reproduce the pronunciations of
thousands of Chinese characters sparsely scattered in data. Therefore, our Byte2Speech model may
reach competitive intelligibility on rich-resource languages without prior G2P knowledge.
3.2 Adaptation
We perform adaptation on targets with randomly sampled sub-datasets of different sizes, co-trained
from different sources, and compare them with single-language models and recording mel CERs.
Romanian results in Figure 2 are the most representative: as shown by the T3 curve, with merely
10 samples or 39s recordings, the model acquires a brand new language of Romanian with high
intelligibility of 2.3% CER, entering the regime of few-shot learning. Besides, with 1k samples,
adapted models reach CER close to ground truth and the full-data ( >7k samples) single-language
model, 30% better than the 1k-sample single-language model. Thanks to the co-training strategy, this
even applies to adaptation from en-us on 1k samples. But with fewer data, multilingualism becomes
4

--- PAGE 5 ---
10
(39s)30 100
(8.2 min)300 1k
(85.6 min)1.01.52.02.53.03.54.0
Single (7k): 1.2Ground Truth: 1.2Single (1k): 1.9
1.41.52.73.8
1.42.3
1.32.3
1.8
1.7T3 (0.1): 2.02.32.9Romanian (ro-ro)
CER (%)
EN-US
IT-IT
T1
T2
T3
10
(59s)15 30
(191s)100 200
(21.8min)300 500
(54.2min)24681012
Single (7k): 1.4Ground Truth: 1.7Single (500): 3.4
1.44.410.2
1.32.0
1.35.3 5.76.68.111.6
2.12.73.8
1.511.212.2Greek (el-gr)
CER (%)
EN-US
T1
T2
T2-
T3
T3-
10
(45s)30 100
(438s)300 1k
(70.7 min)2345678
Single (9k): 2.5Ground Truth: 3.6Single (1k): 5.4
4.97.6
3.95.1
2.73.4
2.42.63.4
2.22.62.9Indian English (en-in)
CER (%)
EN-US
T1
T2
T3
1.23.45.3
4.1
2.8 T3 (0.1): 1.6Figure 2: CER for Indian English (en-in), Romanian (ro-ro), and Greek (el-gr), with x-axis the
number of samples or lengths of audio. For clarity, only one number is labeled for overlapping
datapoints, and those too large are omitted. Crosses ( ×) represent failed models. Horizontal lines
show ground truth and single-language model results with dataset sizes labeled. Isolated datapoints
and curves labeled as IT-IT, T2-, and T3- will be discussed in Section 3.3.
10 15 30 100 200 300 5000.180.200.220.240.260.280.300.32
Single (7k): 0.178Single (500): 0.211
0.1950.2210.259
0.1910.2020.1990.2310.3030.329
0.2710.278
0.2290.239
0.2030.207Greek (el-gr)
MSE
EN-US
T1
T2
T3
10 30 100 300 1k51015202530
Single (9k): 21.4Single (1k): 28.7
8.312.214.423.9
7.711.418.8
6.18.614.7
5.78.012.5
5.27.011.6Indian English (en-in)
CER-Ex (%)
EN-US
T1
T2
T3
10 15 30 100 200 300 5002.55.07.510.012.515.017.520.022.525.0
Single (7k): 5.8Single (500): 16.4
3.49.911.5
4.022.1
3.420.1
16.024.4
10.111.5
6.27.5
3.6Greek (el-gr)
CER-Ex (%)
EN-US
T1
T2
T3
0.1948.3
4.3 4.0
Figure 3: MSE for Greek (el-gr), and CER-Ex for Indian English (en-in) and Greek (el-gr).
essential: With higher tiers or more source languages, all the metrics get constantly improved,
especially with <300 samples.
More remarkably, even for a brand new alphabet of Greek, the model learns most pronunciations with
10 samples; and it easily reaches CER of 2.1%, close to ground truth and the full-data model, using
30 samples. Besides, on Greek, the model may produce mels close to ground truth using much fewer
data, as indicated by MSE in Figure 3. A diverse source corpus proves critical as en-us and T1 models
fail to produce any intelligible speech when the data size is reduced to 200 or 100, respectively.
Indian English is simpler and closer to an English dialect, so all models reach a low CER even with
few shots, and differences between T1/T2/T3 are limited. However, the CER gap between en-us
and T1 still shows the impact of multilingualism. More, as shown in Figure 3, it is found in all
languages but most remarkable in en-in that with a higher tier, adapted models have better CER-Ex,
often beyond full-data (9k) models. On complicated texts in the CER-Ex test set, single-language
models often produce mispronunciations and misalignments. Thanks to adaptation from a rich source
corpus with more diverse scripts, the adapted model may generalize better to those difﬁcult inputs.
The task is harder for Thai and Mandarin Chinese with less phonemic scripts. 100 Thai samples are
required to reach 17% CER, and 1k samples (71.4 minutes) to match the ground truth CER of 2.3%.
Mandarin Chinese is the most difﬁcult, but a 2k-sample adaptation still reaches 9.2% CER. Although
it seems formidable to acquire Thai or Mandarin Chinese in few shots, multilingual adaptation is
nevertheless effective, and MSE and CER of a 2k-sample single-language model are achieved using
10% or 15% data on th-th and zh-cn respectively. To conclude, though insufﬁcient for few-shot
learning on these languages, our approach is still powerful.
With all these results, we show that the general capability of the TTS task can be acquired by a
single multilingual Byte2Speech model, and might be easily applied to a new language with few data,
making a few-shot spoken language learner. More complete results are given in Appendix Section B.
3.3 Comparative studies
Diversity or Quantity By adding extra tiers of sources, the training data expand. We question if this
matters. Therefore, we experiment with downsampled data in each tier: we create a T2-dataset with
5

--- PAGE 6 ---
10
(59s)15 30
(191s)60 100
(10.8 min)5000510152025303540
1.38.0
1.217.921.132.3
7.937.3
3.14.8
2.0Greek (el-gr), T2 variants
CER (%)
T2 (350k)
T2 (400k)
T2 (500k)
T2 (600k)
10
(59s)15 30
(191s)60 100
(10.8 min)24681012
11.2
Pan: 9.112.4
6.9
Pan: 5.8
Pan (0.1): 4.67.2
2.13.1
1.5
1.3Greek (el-gr), T3 variants
CER (%)
T3 (650k)
T3 (700k)
T3 (700k, 0.1)
T3 (800k)
T3D
10
(38s)30 100
(388s)300 1k
(63.0 min)010203040506070
Single (18k): 5.1Ground Truth: 5.3Single (1k): 5.9VietP: 14.220.622.430.864.0
6.28.118.0
6.6
4.8VietP: 32.350.353.063.5Mandarin Chinese, Pinyin
CER (%)
EN-US
T1
T2
T3
Pan (0.1): 7.18.6Figure 4: CER for alternative settings in Greek (el-gr) and Mandarin Chinese (zh-cn).
T1 and T2 languages but only with the size of T1, and a T3-dataset with T2 and T3 languages but of
T2 size. We use T2- or T3- as the source dataset, and then adapt the model to Greek. As shown in
Figure 2, T2- adaptation has a performance close to T2 while much better than T1, and it is similar
for T3-. Therefore, for adaptation the diversity of the source corpus outweighs the data quantity.
Diversity or Similarity By intuition, adaptation to a target can be best aided by a similar source.
This is also supported by the fact that T2 (with zh-hk) greatly helps zh-cn adaptation. Therefore, we
examine it by co-training targets only using a closely related source, that is en-us for en-in, and Italian
(it-it) for ro-ro. As indicated by the corresponding curves in Figure 2, with only similar sources,
results are close to T1. Cases are similar when co-training zh-cn with zh-hk. Therefore, the key to
adaptation is not only leveraging a similar language but a combination of a diverse set of languages.
Progressive or Direct Tier-wise progressive training plays an important role: we carry on T3D
experiments that directly use full corpus. As a result, the model has worse source performance and
fails on zh-hk. More, T3D source models show inferior Greek adaptation, as shown in Figure 4.
When and How to Adapt Instead of ﬁne-tuning a well-trained base model, we start adaptation far
before convergence on source languages. Starting from a semi-mature model (such as T2 500k
steps or T3 700k steps) is beneﬁcial: As shown in T3 (650k) results of Figure 4, directly adding
Greek to T3 results in a performance drop. While a mature network like T3 (800k) may lose its
ﬂexibility and fail on 10 samples. This is more phenomenal on T2, with up to 29% absolute CER
gap between different times to adapt. Therefore, a semi-mature network is beneﬁcial for adaptation
in our setting. Besides, we notice that at a few-shot regime the model tends to overﬁt. Therefore,
we tune the proportion of the target by setting pi= 0:1, enforcing extra regularization from other
languages. As demonstrated by the T3 (0.1) points for ro-ro and el-gr in Figure 2 and Figure 4,
few-shot performances are improved.
Additional details and omitted ﬁgures on ablation studies are available at Appendix Section B.
3.4 Extensibility to language expertise
Although we aim at using minimal language-speciﬁc resources, the framework can be extended. For
example, considering that under few-shot regimes some pronunciagtion rules never appear in the
samples, we investigate Greek orthography and deﬁne a minimal set of 41 graphemes to represent
Greek G2P rules. We then create pangram training sets sized 10 or 15 covering the set. As in results
labeled as Pan in Figure 4, signiﬁcant beneﬁts on few-shot cases are shown, particularly on 10-sample
cases with 19% relative CER reduction. Combined with pi= 0:1, thePan (0.1) model reaches 37%
reduction. Another approach is to augment inputs. We transform zh-cn texts into the Romanized
Pinyin. As shown in Figure 4, zh-cn (Pinyin) obtains performance similar to other phonetic scripts,
with CER close to toplines and the ground truth in 100 samples. If we further transform the script to
mimic the Romanization of vi-vn (Vietnamese) in T3 to aid knowledge transfer, the VietP model
gets extra improvements on few-shot regimes. To conclude, the framework is ﬂexible to improve on a
particular language if we know more about it. See Appendix Section A for details.
3.5 Subjective evaluations
We perform subjective tests to evaluate our methods more accurately. As for naturalness, mean opinion
score (MOS) results are given in Table 2. For source languages, byte models show comparable or
6

--- PAGE 7 ---
Table 2: Mean opinion score with 95% conﬁdence interval on different voices. For adaptation to target
languages, we report low and extremely low (XLow) resource results, with the number of samples of
each condition indicated in the following #line. We also give results on full-data single-language
models, and the native name of languages to give an impression on input scripts.
LANGUAGE EN -US DE -DE ZH -HK VI -VN TE -IN
NATIVE NAME ENGLISH (US) D EUTSCH
廣東話 
TIẾNG VIỆT 
తెలుగ  
RECORDING 4.270.13 3.95 0.13 4.01 0.13 4.59 0.08 4.52 0.11
PHONEME 3.540.10 3.67 0.09 3.83 0.10 4.18 0.08 4.31 0.08
BYTE 3.690.10 3.44 0.10 3.76 0.10 4.15 0.08 4.30 0.09
EN-IN RO -RO EL -GR TH -TH ZH -CN(PINYIN )
INDIAN ENGLISH ROMÂN ˘A
ΕΛΛΗΝΙΚΆ  
ไทย HAN 4-YÜ3
RECORDING 4.720.08 4.71 0.09 4.48 0.11 4.65 0.09 3.82 0.12
PHONEME 4.480.08 4.16 0.10 4.20 0.10 4.08 0.09 3.34 0.09
BYTE SINGLE 4.560.08 4.38 0.08 4.41 0.08 4.18 0.10 3.58 0.09
# 9K 7K 7K 7K 19K
BYTE
LOW 4.590.06 4.37 0.09 4.29 0.08 4.24 0.08 3.46 0.09
# 1K 1K 500 2 K 1K
XLOW 4.370.09 3.74 0.10 3.30 0.12 3.75 0.11 2.75 0.09
# 30 30 30 500 100
Table 3: Word level intelligibility (%) on Romanian and Greek.
LANGUAGE PHONEME BYTE (10 SAMPLES ) B YTE (30 SAMPLES )
RO-RO 99.9 99.4 99.3
EL-GR 98.9 93.5 96.8
only slightly worse results from the phoneme-based model. Hence multilingual byte models may
produce natural speech on rich-resource source languages. On target languages, we choose to test
performances on a low-resource case ( BYTE LOW), along with an extremely low or few-shot case
(BYTE XLOW) that shows good intelligibility, in comparison with single-language full-data byte
models. We report Pinyin result as low-resource zh-cn are full of errors and thus less meaningful. Byte
XLow has some gap compared to Byte Low models, while the Byte Low models show comparable
MOS on en-in, ro-ro, and th-th, and only slight regression on el-gr and zh-cn, compared to BYTE
SINGLE , using only 5% to 29% data. Furthermore, both Byte Low and Byte Single models have
MOS comparable to phoneme models on all target languages. Results are consistent with objective
metrics, showing our exceptional performance on low-resource adaptation.
We also test subjective intelligibility on Romanian and Greek, using the 30-sample T3 adaptation
model and the best 10-sample model, that is Pan (0.1) for Greek and T3 (0.1) for Romanian. As
in Table 3, few-shot Greek models show good intelligibility with >90% words correct, and for
Romanian few-shot models are sufﬁcient to produce close to 100% intelligibility in tests. The results
are highly consistent with CER, indicating that CER is a reliable metric for perceptual intelligibility.
4 Model mechanisms
From Google’s multilingual translation to multilingual BERT, ML and NLP researchers have discov-
ered that various languages can be well handled in a single shared model, which is counter-intuitive
and different from typical multi-task learning using separate task-speciﬁc modules. Hence, the topic
is valuable for general ML researchers: how a single model reaches such versatility on diverse tasks?
Inspired by the ﬁndings that there are both language-speciﬁc and -agnostic components in multilingual
BERT embeddings (Libovický et al., 2019), we believe that the answer may lie in the relationship
between the multilingual model and the language-speciﬁc models. Therefore, we attempt to identify
parameters or sub-models in the multilingual network that are important for each single language,
and to explore the relationship between these particular sub-models.
7

--- PAGE 8 ---
bg ru hr sk da nb sv de ur hi vi zh-cn zh-hk
bg99.0 46.0 44.1 43.1 41.6 41.7 41.9 41.7 41.6 41.8 40.9 39.7 40.1
ru 46.0 98.5 43.1 42.8 41.1 41.3 41.6 41.4 41.0 41.6 40.4 39.6 39.5
hr 44.1 43.1 98.2 45.1 42.1 42.3 43.1 41.9 41.7 41.5 41.6 40.6 40.4
sk 43.1 42.8 45.1 98.1 40.8 41.0 41.6 41.7 41.3 41.2 41.7 40.6 40.5
da 41.6 41.1 42.1 40.8 96.6 47.6 46.1 43.9 41.0 40.6 40.6 39.7 40.4
nb 41.7 41.3 42.3 41.0 47.6 98.0 46.7 43.7 40.7 40.8 40.6 39.5 40.2
sv 41.9 41.6 43.1 41.6 46.1 46.7 98.1 43.8 40.8 41.0 40.9 40.0 40.3
de 41.7 41.4 41.9 41.7 43.9 43.7 43.8 97.9 40.4 40.6 40.2 39.9 40.4
ur 41.6 41.0 41.7 41.3 41.0 40.7 40.8 40.4 98.3 44.5 41.3 40.5 40.6
hi 41.8 41.6 41.5 41.2 40.6 40.8 41.0 40.6 44.5 98.6 41.5 40.1 40.3
vi 40.9 40.4 41.6 41.7 40.6 40.6 40.9 40.2 41.3 41.5 98.2 42.5 42.8
zh-cn39.7 39.6 40.6 40.6 39.7 39.5 40.0 39.9 40.5 40.1 42.5 97.2 45.1
zh-hk40.1 39.5 40.4 40.5 40.4 40.2 40.3 40.4 40.6 40.3 42.8 45.1 97.2Figure 5: Percentage of overlapping neurons between selected language-speciﬁc models from post-
ReLU activations in encoder layer 5. Diagonal values show intra-language overlappings.
We adopt data-driven pruning with Taylor criterions (Molchanov et al., 2017), intending to identify
salient neurons given input data. A neuron that is salient on data of a language might be important for
the model to handle the language. The criterion is to consider the impact on the loss Lwhen zeroing
each neuron hiunder an independent assumption, given training examples Dof the language:
L(hi) =jL(hi= 0;D) L(hi;D)j
, which is estimated using the Taylor expansion of L(hi)nearhi= 0. With a ﬁrst-order expansion,
L(hi= 0;D) =L(hi;D) @L
@hihi+R1(hi= 0)
IgnoringR1, the impact or saliency is approximated as
(hi) =jL(hi)j=j@L
@hihij
, which could be calculated in standard backpropagation.
We randomly sample 1k training examples on each language for saliency computation, on top of the
T3 adaptation model to zh-cn with 2k samples. As we use a transformer, we compute saliency after
each feedforward layer (plus non-linearity if present). Therefore, the impact of zeroing a neuron is
equivalent to zeroing the corresponding parameters. Since layers are shared between steps in the
input/output sequence, the saliency of each neuron is obtained by max-pooling across steps and
averaging between samples. We then sort neurons for each layer and prune the lower half, assuming
the remained network of 50% parameters to be a language-speciﬁc model for each language. Next,
for each pair of languages, we compare their speciﬁc sub-models by inspecting the proportion of
neurons overlapped in both of them. Besides, we randomly split data of each language into halves
and perform pruning separately, and then determine their overlappings in the same way.
As shown in Figure 5, for each pair around 40% neurons are salient on both languages, and the
ratios correlate with language similarities. While intra-language overlappings are above 90%, hence
differences of salient neurons are due to differences of languages but not samples. Besides, as results
are from a deep encoder layer, it not a direct result of distinct language IDs or character sets. Although
we present those from encoder layer 5, such patterns appear in most layers.
Intuitively, languages with shared scripts have signiﬁcant overlappings, such as bg/Bulgarian and
ru/Russian from different branches of Slavic languages but both using Cyrillic alphabets. While the
spoken form similarity matters and those with greater lexical, phonetic, or phylogenetic similarity
are more overlapped, as shown by the closely related Germanic languages, especially the Nordic
branch (da/Danish, nb/Norwegian, sv/Swedish), while de/German is farther. This even applies to
languages with different scripts, such as ur/Urdu and hi/Hindi, the same spoken language in two
different scripts. It is similar on distinct languages, such as vi (which is not phylogenetically related
to but has rich loanwords from Chinese), zh-hk, and the adaptation target zh-cn. Hence, during
adaptation, the structures and parameters from similar source languages might be employed by the
target. The phylogenetic relations can also be observed in the overlappings from bg to other Slavic
languages, like hr/Croatian and sk/Slovak both using the Latin alphabet: hr has more overlapping as
bg and hr are both South Slavic languages while sk is a more distant West Slavic language.
8

--- PAGE 9 ---
Table 4: MSE, CER, and the corresponding relative drop when pruning with German or the target
language itself (S ELF), and then retraining on the target language.
TARGET DUTCH POLISH RUSSIAN KOREAN ARABICS CANTONESE
MSESELF 0.458 0.510 0.466 0.451 0.520 0.541
GERMAN 0.459 0.512 0.474 0.470 0.544 0.837
DROP (%) 0.3% 0.4% 1.8% 4.2% 4.6% 54.9%
CERSELF 3.0% 2.1% 5.2% 13.2% 7.4% 26.5%
GERMAN 3.0% 2.2% 6.3% 17.3% 10.8% 67.4%
DROP (%) 0.5% 5.8% 21.4% 30.9% 47.3% 154.2%
We further verify our ﬁndings by actually pruning a T3 model with saliency computed from German,
and then retraining it to monolingual models of T2 languages under low-resource settings, and next
comparing the results of pruning with the language itself. As shown by Table 4, the more a language
is different from German, the greater performance drop by German-pruning can be observed. While if
the model is randomly pruned, it will mostly fail. This indicates that our criterion correctly identiﬁes
neurons speciﬁc to German, and a similar language may better utilize the German sub-model.
All these evidences support that our model captures various high-level relations between languages
just like multilingual BERT (Rama et al., 2020). Therefore, unlike previous works that require the
manual design of language-speciﬁc and -agnostic modules such as per-language encoders (Nachmani
and Wolf, 2019; Nekvinda and Dusek, 2020), we show that an architecture of language-speciﬁc
sub-networks with partially shared parameters emerges from the multilingual training of a single
model, and the model may discover and utilize language similarities for both source languages and
the transfer to targets to handle the task of multilingual TTS and low-resource adaptation.
5 Related work
Various prior works attempt to build multilingual neural TTS models. Li et al. (2019a) is a closely
related work that proposed byte-based speech recognition and synthesis model. However, the work
was done only on few languages and did not touch the low-resource scenarios. Other multilingual
methods introduced adversarial training (Zhang et al., 2019), or per-language encoder (Nachmani
and Wolf, 2019; de Korte et al., 2020), possibly with parameters predicted from language IDs
(Nekvinda and Dusek, 2020). Multilingual TTS helps low-resource languages to a great extent, both
in multilingual training and adaptation (Li and Zen, 2016; Demirsahin et al., 2018; Baljekar et al.,
2018; de Korte et al., 2020). Our framework is developed upon the phoneme-based language-balanced
multilingual transformer TTS (Yang and He, 2020) which extend the method to 40+ languages, while
we remove the needs of phoneme-inputs, outperform their performance on low-resource adaptation,
and explore the few-shot regime. Various more complex methods were proposed to better leverage
transfer learning, pretraining, and semi-supervised learning, such as using pretrained text embeddings
and self-supervised decoder pretraining (Chung et al., 2019), and ﬁne-tuning from an autoencoder
with discrete latents trained on unpaired target speeches (Zhang and Lin, 2020; Liu et al., 2020a).
Ren et al. (2019) applies dual learning between recognition and synthesis, along with denoising
autoencoding, and Xu et al. (2020) further added rich-resource pretraining and knowledge distillation.
Data augmentation with injected noise (Liu et al., 2020b) also helps. Besides, transfer from other
languages could be obtained by language expertise such as designing a uniﬁed set of phonemes
(Cai et al., 2020), often derived from IPA (Chen et al., 2019; Demirsahin et al., 2018), or creating
feature vectors by phonology (Staib et al., 2020). While our method eliminates the need of these
language-speciﬁc expertise and complicated pipelines using auxilliary models and data.
6 Conclusions and future work
We present a systematic approach to build a multilingual Byte2Speech TTS model and show that
it is capable to match phoneme-based performance on both standard and low-resource adaptation
scenarios. We also deepen our understanding of the mechanism by a novel interpretation. Future
work will focus on improving few-shot performances and further exploring the model mechanism.
9

--- PAGE 10 ---
References
Pallavi Baljekar, Sai Krishna Rallabandi, and Alan W. Black. An investigation of convolution attention
based models for multilingual speech synthesis of indian languages. In B. Yegnanarayana, editor,
Interspeech 2018, 19th Annual Conference of the International Speech Communication Association,
Hyderabad, India, 2-6 September 2018 , pages 2474–2478. ISCA, 2018. doi: 10.21437/Interspeech.
2018-1869. URL https://doi.org/10.21437/Interspeech.2018-1869 .
Zexin Cai, Yaogen Yang, and Ming Li. Cross-lingual multispeaker text-to-speech under limited-data
scenario. CoRR , abs/2005.10441, 2020.
Mingjian Chen, Xu Tan, Yi Ren, Jin Xu, Hao Sun, Sheng Zhao, and Tao Qin. Multispeech:
Multi-speaker text to speech with transformer. In Helen Meng, Bo Xu, and Thomas Fang
Zheng, editors, Interspeech 2020, 21st Annual Conference of the International Speech Com-
munication Association, Virtual Event, Shanghai, China, 25-29 October 2020 , pages 4024–
4028. ISCA, 2020. doi: 10.21437/Interspeech.2020-3139. URL https://doi.org/10.21437/
Interspeech.2020-3139 .
Yuan-Jui Chen, Tao Tu, Cheng-chieh Yeh, and Hung-yi Lee. End-to-end text-to-speech for low-
resource languages by cross-lingual transfer learning. In Gernot Kubin and Zdravko Kacic, editors,
Interspeech 2019, 20th Annual Conference of the International Speech Communication Association,
Graz, Austria, 15-19 September 2019 , pages 2075–2079. ISCA, 2019. doi: 10.21437/Interspeech.
2019-2730. URL https://doi.org/10.21437/Interspeech.2019-2730 .
Yu-An Chung, Yuxuan Wang, Wei-Ning Hsu, Yu Zhang, and R. J. Skerry-Ryan. Semi-supervised
training for improving data efﬁciency in end-to-end speech synthesis. In IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom,
May 12-17, 2019 , pages 6940–6944. IEEE, 2019. doi: 10.1109/ICASSP.2019.8683862. URL
https://doi.org/10.1109/ICASSP.2019.8683862 .
Marcel de Korte, Jaebok Kim, and Esther Klabbers. Efﬁcient neural speech synthesis for low-
resource languages through multilingual modeling. In Helen Meng, Bo Xu, and Thomas Fang
Zheng, editors, Interspeech 2020, 21st Annual Conference of the International Speech Com-
munication Association, Virtual Event, Shanghai, China, 25-29 October 2020 , pages 2967–
2971. ISCA, 2020. doi: 10.21437/Interspeech.2020-2664. URL https://doi.org/10.21437/
Interspeech.2020-2664 .
Isin Demirsahin, Martin Jansche, and Alexander Gutkin. A uniﬁed phonological representation
of south asian languages for multilingual text-to-speech. In Shyam S. Agrawal, editor, 6th Intl.
Workshop on Spoken Language Technologies for Under-Resourced Languages, SLTU 2018, 29-31
August 2018, Gurugram, India , pages 80–84. ISCA, 2018. doi: 10.21437/SLTU.2018-17. URL
https://doi.org/10.21437/SLTU.2018-17 .
Fei He, Shan-Hui Cathy Chu, Oddur Kjartansson, Clara Rivera, Anna Katanova, Alexander Gutkin,
Isin Demirsahin, Cibu Johny, Martin Jansche, Supheakmungkol Sarin, and Knot Pipatsrisawat.
Open-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi,
tamil and telugu speech synthesis systems. In Nicoletta Calzolari, Frédéric Béchet, Philippe
Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente
Maegaard, Joseph Mariani, Hélène Mazo, Asunción Moreno, Jan Odijk, and Stelios Piperidis,
editors, Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020,
Marseille, France, May 11-16, 2020 , pages 6494–6503. European Language Resources Association,
2020. URL https://www.aclweb.org/anthology/2020.lrec-1.800/ .
Melvin Johnson, Mike Schuster, Quoc V . Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda B. Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean.
Google’s multilingual neural machine translation system: Enabling zero-shot translation. Trans.
Assoc. Comput. Linguistics , 5:339–351, 2017. URL https://transacl.org/ojs/index.php/
tacl/article/view/1081 .
Bo Li and Heiga Zen. Multi-language multi-speaker acoustic modeling for LSTM-RNN based
statistical parametric speech synthesis. In Nelson Morgan, editor, Interspeech 2016, 17th Annual
Conference of the International Speech Communication Association, San Francisco, CA, USA,
10

--- PAGE 11 ---
September 8-12, 2016 , pages 2468–2472. ISCA, 2016. doi: 10.21437/Interspeech.2016-172. URL
https://doi.org/10.21437/Interspeech.2016-172 .
Bo Li, Yu Zhang, Tara N. Sainath, Yonghui Wu, and William Chan. Bytes are all you need:
End-to-end multilingual speech recognition and synthesis with bytes. In IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom,
May 12-17, 2019 , pages 5621–5625. IEEE, 2019a. doi: 10.1109/ICASSP.2019.8682674. URL
https://doi.org/10.1109/ICASSP.2019.8682674 .
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with
transformer network. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019,
Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 6706–6713. AAAI Press, 2019b. doi:
10.1609/aaai.v33i01.33016706. URL https://doi.org/10.1609/aaai.v33i01.33016706 .
Jindrich Libovický, Rudolf Rosa, and Alexander Fraser. How language-neutral is multilingual BERT?
CoRR , abs/1911.03310, 2019.
Alexander H. Liu, Tao Tu, Hung-yi Lee, and Lin-Shan Lee. Towards unsupervised speech recog-
nition and synthesis with quantized speech representation learning. In 2020 IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May
4-8, 2020 , pages 7259–7263. IEEE, 2020a. doi: 10.1109/ICASSP40776.2020.9053571. URL
https://doi.org/10.1109/ICASSP40776.2020.9053571 .
Ruolan Liu, Xue Wen, Chunhui Lu, and Xiao Chen. Tone learning in low-resource bilingual TTS. In
Helen Meng, Bo Xu, and Thomas Fang Zheng, editors, Interspeech 2020, 21st Annual Conference
of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29
October 2020 , pages 2952–2956. ISCA, 2020b. doi: 10.21437/Interspeech.2020-2180. URL
https://doi.org/10.21437/Interspeech.2020-2180 .
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efﬁcient inference. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings .
OpenReview.net, 2017.
Eliya Nachmani and Lior Wolf. Unsupervised polyglot text-to-speech. In IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom,
May 12-17, 2019 , pages 7055–7059. IEEE, 2019. doi: 10.1109/ICASSP.2019.8683519. URL
https://doi.org/10.1109/ICASSP.2019.8683519 .
Tomás Nekvinda and Ondrej Dusek. One model, many languages: Meta-learning for multilingual text-
to-speech. In Helen Meng, Bo Xu, and Thomas Fang Zheng, editors, Interspeech 2020, 21st Annual
Conference of the International Speech Communication Association, Virtual Event, Shanghai,
China, 25-29 October 2020 , pages 2972–2976. ISCA, 2020. doi: 10.21437/Interspeech.2020-2679.
URL https://doi.org/10.21437/Interspeech.2020-2679 .
Taraka Rama, Lisa Beinborn, and Steffen Eger. Probing multilingual BERT for genetic and typological
signals. In Donia Scott, Núria Bel, and Chengqing Zong, editors, Proceedings of the 28th
International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online),
December 8-13, 2020 , pages 1214–1228. International Committee on Computational Linguistics,
2020. doi: 10.18653/v1/2020.coling-main.105. URL https://doi.org/10.18653/v1/2020.
coling-main.105 .
Yi Ren, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Almost unsupervised text
to speech and automatic speech recognition. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15
June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research ,
pages 5410–5419. PMLR, 2019. URL http://proceedings.mlr.press/v97/ren19a.html .
Stan Salvador and Philip Chan. Toward accurate dynamic time warping in linear time and space.
Intelligent Data Analysis , 11(5):561–580, 2007.
11

--- PAGE 12 ---
Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng
Chen, Yu Zhang, Yuxuan Wang, RJ-Skerrv Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and
Yonghui Wu. Natural TTS synthesis by conditioning wavenet on MEL spectrogram predictions. In
2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018,
Calgary, AB, Canada, April 15-20, 2018 , pages 4779–4783. IEEE, 2018. doi: 10.1109/ICASSP.
2018.8461368. URL https://doi.org/10.1109/ICASSP.2018.8461368 .
Marlene Staib, Tian Huey Teh, Alexandra Torresquintero, Devang S. Ram Mohan, Lorenzo Foglianti,
Raphael Lenain, and Jiameng Gao. Phonological features for 0-shot multilingual speech synthesis.
In Helen Meng, Bo Xu, and Thomas Fang Zheng, editors, Interspeech 2020, 21st Annual Con-
ference of the International Speech Communication Association, Virtual Event, Shanghai, China,
25-29 October 2020 , pages 2942–2946. ISCA, 2020. doi: 10.21437/Interspeech.2020-1821. URL
https://doi.org/10.21437/Interspeech.2020-1821 .
Jin Xu, Xu Tan, Yi Ren, Tao Qin, Jian Li, Sheng Zhao, and Tie-Yan Liu. LRSpeech: Extremely low-
resource speech synthesis and recognition. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya
Prakash, editors, KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining, Virtual Event, CA, USA, August 23-27, 2020 , pages 2802–2812. ACM, 2020. URL
https://dl.acm.org/doi/10.1145/3394486.3403331 .
Jingzhou Yang and Lei He. Towards universal text-to-speech. In Helen Meng, Bo Xu, and
Thomas Fang Zheng, editors, Interspeech 2020, 21st Annual Conference of the International
Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020 , pages
3171–3175. ISCA, 2020. doi: 10.21437/Interspeech.2020-1590. URL https://doi.org/10.
21437/Interspeech.2020-1590 .
Yusuke Yasuda, Xin Wang, and Junichi Yamagishi. Investigation of learning abilities on linguistic
features in sequence-to-sequence text-to-speech synthesis. Comput. Speech Lang. , 67:101183, 2021.
doi: 10.1016/j.csl.2020.101183. URL https://doi.org/10.1016/j.csl.2020.101183 .
Haitong Zhang and Yue Lin. Unsupervised learning for sequence-to-sequence text-to-speech for
low-resource languages. In Helen Meng, Bo Xu, and Thomas Fang Zheng, editors, Interspeech
2020, 21st Annual Conference of the International Speech Communication Association, Virtual
Event, Shanghai, China, 25-29 October 2020 , pages 3161–3165. ISCA, 2020. doi: 10.21437/
Interspeech.2020-1403. URL https://doi.org/10.21437/Interspeech.2020-1403 .
Yu Zhang, Ron J. Weiss, Heiga Zen, Yonghui Wu, Zhifeng Chen, R. J. Skerry-Ryan, Ye Jia, Andrew
Rosenberg, and Bhuvana Ramabhadran. Learning to speak ﬂuently in a foreign language: Multi-
lingual speech synthesis and cross-language voice cloning. In Gernot Kubin and Zdravko Kacic,
editors, Interspeech 2019, 20th Annual Conference of the International Speech Communication
Association, Graz, Austria, 15-19 September 2019 , pages 2080–2084. ISCA, 2019. doi: 10.21437/
Interspeech.2019-2668. URL https://doi.org/10.21437/Interspeech.2019-2668 .
A Implementation details
A.1 Model
We follow the best setting of Li et al. (2019b) to use a transformer with scaled sinusoidal position
encoding, 6 encoder layers, 6 decoder layers, and 8 heads, along with a 5-layer convolutional post-net,
except that encoder pre-net is not adopted. We follow the parameters used in Tacotron2 (Shen et al.,
2018) to create acoustic features of mel spectrograms. The processed mel spectrograms are used to
train a WaveNet under the setting of Li et al. (2019b), to generate waveforms used in subjective tests.
As for training, we apply Adam optimizer with exponential learning rate decay from 1e-3 to 1e-5 in
850k steps using a decay rate of 0.01, which is reset at tier transitions and the beginning of adaptations.
We use dynamic batching with at most 8000 output frames in each batch (Li et al., 2019b), and train
with data parallelism on four V100 GPUs. The timings for tier transitions and adaptation are found
by grid search according to the results on source languages and the target language respectively. On
the source languages, the model uses total 1M steps to reach convergence, taking 6.3 days. The best
result among the training steps are reported for each metric. On target languages the time varies,
12

--- PAGE 13 ---
depending on the difﬁculty of the language and the number of samples. Starting from T3, a 10-sample
ro-ro model takes 100k steps to reach best CER, while a 1k-sample ro-ro model takes 300k steps, and
a 1k-sample zh-cn one using characters takes around 400k steps.
We follow the approach in Yang and He (2020) to add language embeddings and speaker embeddings
from the corresponding MLPs after the encoder. Therefore, with byte inputs the language identity is
not directly given to the encoder. We also attempted to inject the language embeddings at the input
side, either by concatenating with input embeddings, or by special tokens (Johnson et al., 2017). We
also attempted to use deeper (e.g. 16-layer) transformers. However, in experiments we found that it
leads to similar or even worse performance on source languages.
To facilitate reproduction, an implementation based on open codes and datasets will be available at
https://github.com/mutiann/few-shot-transformer-tts , although due to differences of
dataset the results are not identical to what we report in the paper on proprietary datasets.
A.2 Comparative studies
In comparative studies, we determine the training steps for alternative training settings by comparing
the training loss to our standard setting. Starting from 30k steps we train the model with T1 languages
in T2- until it reaches the loss of 350k steps of the standard model, and then add the rest languages
in T2- until it reaches the loss of 500k steps. Similarly, we perform experiments on T3- dataset
starting from the standard T1 at 350k steps. For T3D adaptation, we start from the step when reaching
training loss identical to the standard T3 at 700k steps.
A.3 Extensibility to language expertise
As for Greek, we ﬁnd a minimal set of 41 graphemes of letters and multigraphs to cover major G2P
rules. In the randomly selected 10 and 15 samples, seven rarest elements of the minimal set never
appear, while all of them appear in the evaluation set, making errors inevitable. Therefore, by using
such language-speciﬁc expertise, we train on an alternative set of 10 and 15 samples, each forming a
pangram when viewed together, covering all the elements in the minimal set. Other conditions are set
identical to the standard T3. We pick the pangram sets with total script lengths close to the original
10- and 15-sample-set (with difference <10%), to avoid the impact of different sample lengths.
As for Mandarin Chinese, we transform scripts into Pinyin by lexicon, with words segmented and
tones labeled by numbers. We obtain performance similar to other phonetic scripts. However, the gap
is large on few-shot scenarios, potentially due to phonetic differences to most source languages. We
observe that errors mostly occur on tones and pronunciation of the vowel /ü/ absent in the standard
Latin alphabet. Therefore, we further turn to encourage transfer from Vietnamese in T3, which has
similar tones to Chinese but uses a Latin alphabet with particular diacritics to denote tones. We adopt
the diacritics and substitute /ü/ with /
ư / in Vietnamese to create the VietP model, and reach better
performance. The full results of the experiments are given in Figure 7.
A.4 Model mechanisms
We choose to inspect the language-speciﬁc sub-models with 50% neurons pruned. On some layers
(such as the ReLU outputs on the last encoder layer) due to the sparsity of ReLU activation only less
than half of the neurons are used, making all the overlapping proportions 100%. This can be avoided
if we choose a larger pruning ratio such as 75%. However under such a large ratio, the pruned model
is too small to capture many complicated TTS tasks in the retraining experiment, leading to failures
of retraining on zh-hk and ko-kr. Hence we choose a smaller ratio of 50%.
In retraining experiments, since a 50% pruned model still possess the ﬂexibility to learn a language if
sufﬁcient data are given, we use a low-resource scenario to better analyze the result. All except zh-hk
use 500 samples, while zh-hk uses 2k samples, which is the minimum size we found to produce an
intelligible self-pruning model. Experiments are performed on a converged T3 model of 1M steps.
13

--- PAGE 14 ---
10
(45s)30 100
(438s)300 1k
(70.7 min)0.600.620.640.660.680.700.720.740.76
Single (9k): 0.579Single (1k): 0.6320.7300.7380.7450.754
0.6990.706
0.6510.6630.667
0.6350.641
0.614MSE
10
(45s)30 100
(438s)300 1k
(70.7 min)2345678
Single (9k): 2.5Ground Truth: 3.6Single (1k): 5.4
4.95.15.37.6
3.94.15.1
2.73.4
2.42.63.4
2.22.62.9Indian English (en-in)
CER (%)
10
(45s)30 100
(438s)300 1k
(70.7 min)51015202530
Single (9k): 21.4Single (1k): 28.7
8.312.214.423.9
7.711.418.8
6.18.614.7
5.78.012.5
5.27.011.6CER-Ex (%)
EN-US
T1
T2
T3
10
(39s)30 100
(8.2 min)300 1k
(85.6 min)0.3750.4000.4250.4500.4750.5000.5250.5500.575
Single (7k): 0.366Single (1k): 0.3870.422T3 (0.1): 0.4280.4290.4660.485
0.4030.4080.4210.427
0.3960.404
0.3890.460T3 (0.1): 0.4670.4730.4860.560MSE
10
(39s)30 100
(8.2 min)300 1k
(85.6 min)1.01.52.02.53.03.54.0
Single (7k): 1.2Ground Truth: 1.2Single (1k): 1.9
1.41.52.73.8
1.42.3
1.3T3 (0.1): 1.61.71.82.3
T3 (0.1): 2.02.32.9Romanian (ro-ro)
CER (%)
10
(39s)30 100
(8.2 min)300 1k
(85.6 min)05101520
Single (7k): 3.3Single (1k): 7.5
T3 (0.1): 2.12.53.16.318.222.0
2.04.49.212.0
1.53.45.67.7
1.32.93.54.0
T3 (0.1): 2.95.313.7CER-Ex (%)
EN-US
IT-IT
T1
T2
T3
10
(59s)15 30
(191s)60 100
(10.8 min)200 300
(32.5 min)5000.180.200.220.240.260.280.300.32
Single (7k): 0.178Single (500): 0.211
0.1950.1980.2210.259
0.1910.1940.2020.1990.2020.2310.3030.3210.329
0.2710.2780.300
0.2290.2330.2390.249
0.2100.2160.221
0.2030.2070.214MSE
10
(59s)15 30
(191s)60 100
(10.8 min)200 300
(32.5 min)50024681012
Single (7k): 1.4Ground Truth: 1.7Single (500): 3.4
1.44.410.2
1.32.01.35.35.76.68.111.6
2.12.73.43.8
1.51.81.511.212.2Greek (el-gr)
CER (%)
10
(59s)15 30
(191s)60 100
(10.8 min)200 300
(32.5 min)5002.55.07.510.012.515.017.520.022.525.0
Single (7k): 5.8Single (500): 16.4
3.43.89.911.5
4.022.1
3.44.020.1
16.017.324.4
10.111.517.9
6.27.58.5
4.25.16.3
3.64.35.0CER-Ex (%)
EN-US
T1
T2
T2-
T3
T3-
100
(412s)200 300
(21.2 min)500 1k
(71.4 min)2k0.300.350.400.450.500.55
Single (7k): 0.300Single (2k): 0.391
0.3170.3220.3540.436
0.3100.3270.3370.3370.3430.527
0.3640.381T3 (0.1): 0.4470.457
0.379MSE
100
(412s)200 300
(21.2 min)500 1k
(71.4 min)2k0510152025
Ground Truth: 2.3Single (7k): 2.6Single (2k): 15.3
2.53.26.720.0
2.34.2 5.09.1
3.74.5T3 (0.1): 17.023.5
7.5Thai (th-th)
CER (%)
100
(412s)200 300
(21.2 min)500 1k
(71.4 min)2k102030405060
Single (7k): 26.3Single (2k): 52.9
13.616.140.157.7
13.928.131.6
20.830.5
16.020.2T3 (0.1): 37.242.7
24.9CER-Ex (%)
EN-US
T1
T2
T3
60
(227s)100 300
(19.4 min)500 1k
(63.0 min)2k0.40.50.60.70.8
Single (18k): 0.226Single (2k): 0.500
0.4370.4960.5180.714
0.3870.4130.4250.4690.5700.7170.732
0.4890.5010.6060.6390.833
0.7520.771MSE
60
(227s)100 300
(19.4 min)500 1k
(63.0 min)2k01020304050607080
Ground Truth: 5.3Single (18k): 5.5Single (2k): 30.3
14.624.629.161.1
9.213.915.823.231.633.965.7
22.024.245.549.173.475.1
59.365.2Mandarin Chinese (zh-cn)
CER (%)
60
(227s)100 300
(19.4 min)500 1k
(63.0 min)2k102030405060708090
Single (18k): 35.6Single (2k): 73.9
17.319.538.557.777.1
14.825.444.848.6
37.039.977.686.7
24.728.959.175.580.784.3
66.871.4CER-Ex (%)
EN-US
T1
T2
T3
ZH-HK8.3
2.80.646
0.618Figure 6: Objective results for Indian English (en-in), Romanian (ro-ro), Greek (el-gr), Thai (th-th),
and Mandarin Chinese (zh-cn).
B Additional experimental results
B.1 Adaptation
The full results of MSE, CER, and CER-Ex for en-in, ro-ro, and el-gr are given in Figure 6. which
are consistent with our ﬁndings given in the paper:
•MSE, CER, and CER-Ex similar to single-language models can be reached with much fewer
data.
• Good intelligibility can be achieved in few shots.
14

--- PAGE 15 ---
10 15 30 60 100 5000.200.250.300.350.40
Single (7k): 0.178Single (500): 0.2110.2090.259
0.1920.3290.3490.395
0.2780.390
0.238
0.215MSE
10 15 30 60 100 5000510152025303540
Single (7k): 1.4Ground Truth: 1.7Single (500): 3.4
1.38.0
1.217.921.132.3
7.937.3
3.14.82.0Greek (el-gr), T2 variants
CER (%)
10 15 30 60 100 50051015202530
Single (7k): 5.8Single (500): 16.7
4.46.123.5
3.54.224.829.9
11.515.2
7.9
5.4CER-Ex (%)
T2 (350k)
T2 (400k)
T2 (500k)
T2 (600k)
10 15 30 60 1000.200.220.240.260.280.300.32
Single (7k): 0.178Single (500): 0.2110.2270.2330.237
0.2100.217
0.204MSE
10 15 30 60 10024681012
Single (7k): 1.4Ground Truth: 1.7Single (500): 3.4Pan (0.1): 7.18.6Pan: 9.111.212.4
7.2
2.12.43.1
1.51.3Greek (el-gr), T3 variants
CER (%)
10 15 30 60 10046810121416
Single (7k): 5.8Single (500): 16.7
8.8Pan (0.1): 9.010.3Pan: 10.911.112.5
5.46.27.17.6
4.34.75.3
3.8Pan (0.1): 12.1Pan: 12.813.516.2CER-Ex (%)
T3 (650k)
T3 (700k)
T3 (700k, 0.1)
T3 (800k)
T3D
10
(38s)30 100
(388s)300 1k
(63.0 min)0.350.400.450.500.550.600.65
Single (18k): 0.222Single (1k): 0.370VietP: 0.4610.4790.4910.5100.585
0.4110.452
0.3810.369VietP: 0.5440.6000.628MSE
10
(38s)30 100
(388s)300 1k
(63.0 min)010203040506070
Single (18k): 5.1Ground Truth: 5.3Single (1k): 5.9VietP: 14.220.622.430.864.0
6.28.118.0
6.6
4.8VietP: 32.350.353.063.5Mandarin Chinese, Pinyin
CER (%)
10
(38s)30 100
(388s)300 1k
(63.0 min)20406080
Single (18k): 33 .5Single (1k): 49.5
VietP: 26.535.139.355.589.3
11.914.134.260.1
9.411.826.746.1
8.723.632.4VietP: 48.265.267.782.1CER-Ex (%)
EN-US
T1
T2
T38.60.284
Pan: 0.3050.312
Pan (0.1): 0.295
0.284
Pan: 0.27 7
Pan (0.1): 0.2630.271
0.2076.9
Pan (0.1): 4.6Pan: 5.8
0.419
0.389Figure 7: Objective results for alternative settings in Greek (el-gr) and Mandarin Chinese (zh-cn)
using Pinyin.
•The more source languages, the better performance, especially when there are limited data.
• Better robustness than single-language models as shown by CER-Ex.
• Results on different metrics are consistent to each other.
The results for th-th and zh-cn are also given in Figure 6. zh-cn is more difﬁcult and even with 2k
samples the best model could still not reach the ground truth CER, while T2 with zh-hk helps a lot.
Nevertheless, the results are consistent with our ﬁndings, except for few-shot language acquisition.
B.2 Comparative studies
The full results for diversity or quantity on Greek are given in Figure 6, showing consistency with
our ﬁndings, so are the results for diversity of similarity (added by the zh-hk curve on zh-cn), and
forwhen and how to adapt (as supported by Figure 7).
As for the question of progressive or direct , the T3D source model has similar or worse source CER:
2.46% on en-us, 1.26% on de-de, 47.82% on zh-hk, and 9.65% on te-in; and it has much worse Greek
adaptation results, as in Figure 7. Besides, the full results of Pangram and Pinyin experiments are
consistent with ﬁndings in the paper as well. Details for implementation are mentioned in Section A.
B.3 Cross-lingual speaker transfer
Although not our focus, our model also supports cross-lingual speaker transfer similar to previous
works Yang and He (2020), that is to transfer a speaker’s voice to another language. Audio samples
are available on our webpage, showing the cross-language transfer on a Japanese female speaker and
an American male speaker. As shown by the results, the model could generate English speech in the
Japanese speaker’s voice, and Japanese speech in the American speaker’s voice, even though such
cases are unseen during training. We further compare with the recordings for corresponding English
15

--- PAGE 16 ---
English:
en-us , en-gb,
en-ca , en-au ,
en-inSpanish:
es-mx
Portuguese:
pt-br , pt-pt
Japanese:
ja-jpFrench:
fr-ca , fr-fr,
fr-ch, fr-beGerman:
de-de , de-at
Dutch:
nl-nl , nl-be
Cantonese:
zh-hkNorwegian:
nb-no
Korean:
ko-krSwedish:
sv-seRussian:
ru-ru
Polish:
pl-pl
Turkish:
tr-trCatalan:
ca-es
Hindi:
hi-inDanish:
da-dk
Thai:
th-thCzech:
cs-czSlovak:
sk-skBulgarian:
bg-bg
Slovenian:
sl-si
Hebrew:
he-il
Telugu:
te-inTamil:
ta-inCroatian:
hr-hrUkrainian:
uk-ua
Urdu:
ur-pk
Mandarin:
zh-cnRomanian:
ro-roItalian:
it-itRomance GermanicSlavic
ChineseAfro-asiatic Family
Arabics:
ar-egIndo-Iranian
Dravidian FamilyGreek Alphabet
Cyrillic Alphabet
Latin AlphabetIndo-European Family
Finnish:
fi-fi
Hungarian:
hu-huUralic Family
Malay:
ms-my
Vietnamese:
vi-vn
Abjad
Brahmic Abugida
Chinese
SyllabicGreek:
el-grFigure 8: Linguistic traits of the involved languages. T1 languages are given in green bold codes in
green boxes, T2 in bold codes in yellow boxes, target languages in red codes in red boxes, and the
rest are T3 languages. Writing systems of the languages are indicated. Language families and their
direct branches are shown in rectangular boxes to group the languages. Languages with additional
similarity are grouped in more ﬁne-grained rounded boxes.
utterances by the Japanese speaker, and ﬁnd that by our model more natural English speech with less
Japanese accent could be produced.
B.4 Multi-speaker target language data
All adaptation targets in our experiments are datasets with thousands of samples from a single
professional speaker. However, in real world scenarios it is often more economical to collect a
dataset on the target language with multiple non-professional speakers, each contributing hundreds of
samples, as in the settings and datasets of He et al. (2020). Therefore we also perform low-resource
adaptation on their open Gujarati (gu-in) datasets released under CC-BY-SA 4.0, with 4k samples
from 35 speakers. Under the settings identical to our previous T3 adaptation experiments, we could
reach a CER of 6.84%, showing that our method is applicable to the scenario as well.
C Dataset details
Details of our dataset is given in Table 5. Besides, We demonstrate the linguistic traits of languages
involved in Figure 8, including their writing systems and phylogenetical relationships. As shown
in the ﬁgure, our full dataset cover a wide variety of spoken languages and writing systems, but the
lower tiers (such as T1 marked in green) are much narrower, and our target languages (marked in
red) are selected that each of them shares different level of similarity with the source languages when
adapted from each tier.
16

--- PAGE 17 ---
Table 5: List of languages and their codes of each tier in our experiment.
Code Language Code Language
TIER1 da-dk Danish (Denmark)
en-us English (US) de-at German (Austria)
es-mx Spanish (Mexico) de-ch German (Swiss)
fr-ca French (Canada) ﬁ-ﬁ Finnish (Finland)
ja-jp Japanese (Japan) fr-be French (Belgium)
TIER2 fr-ch French (Swiss)
ar-eg Arabics (Egypt) he-il Hebrew (Israel)
de-de German (Germany) hi-in Hindi (India)
en-au English (Australia) hr-hr Croatian (Croatia)
en-ca English (Canada) hu-hu Hungarian (Hungary)
en-gb English (UK) ms-my Malay (Malaysia)
fr-fr French (France) nl-be Dutch (Belgium)
it-it Italian (Italy) sk-sk Slovak (Slovak)
ko-kr Korean (Korea) sl-si Slovenian (Slovenia)
nb-no Norwegian Bokmal (Norway) ta-in Tamil (India)
nl-nl Dutch (Netherlands) te-in Telugu (India)
pl-pl Polish (Poland) tr-tr Turkish (Turkey)
pt-br Portuguese (Brazil) uk-ua Ukrainian (Ukraine)
pt-pt Portuguese (Portugal) ur-pk Urdu (Pakistan)
ru-ru Russian (Russia) vi-vn Vietnamese (Vietnam)
sv-se Swedish (Sweden) T ARGETS
zh-hk Cantonese (Hong Kong) el-gr Greek (Greece)
TIER3 en-in English (India)
bg-bg Bulgarian (Bulgaria) ro-ro Romanian (Romania)
ca-es Catalan (Spain) th-th Thai (Thailand)
cs-cz Czech (Czech) zh-cn Mandarin Chinese (Mainland China)
17
