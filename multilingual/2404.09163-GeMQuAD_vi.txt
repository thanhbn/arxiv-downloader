# GeMQuAD: Tạo ra các Tập dữ liệu Hỏi đáp Đa ngữ từ các Mô hình Ngôn ngữ Lớn sử dụng Học ít mẫu

Amani Namboori∗
Amazon, Alexa International
anamburi@amazon.com

Shivam Sadashiv Mangale∗
Amazon, Alexa International
mangsh@amazon.com

Andy Rosenbaum†
Amazon, Alexa AI
andros@amazon.com

Saleh Soltan†
Amazon, Alexa AI
ssoltan@amazon.com

## Tóm tắt

Sự xuất hiện của các Mô hình Ngôn ngữ Lớn (LLMs) với khả năng như Học trong Ngữ cảnh (ICL) đã mở ra những khả năng mới cho việc tạo dữ liệu trên nhiều lĩnh vực khác nhau trong khi giảm thiểu nhu cầu về các kỹ thuật thu thập và mô hình hóa dữ liệu rộng rãi. Các nhà nghiên cứu đã khám phá các cách để sử dụng dữ liệu tổng hợp được tạo ra này để tối ưu hóa các mô hình học sinh nhỏ hơn nhằm giảm chi phí triển khai và độ trễ thấp hơn trong các nhiệm vụ hạ nguồn. Tuy nhiên, dữ liệu được tạo ra bởi ICL thường gặp phải chất lượng thấp do tính cụ thể của nhiệm vụ bị hạn chế với ít ví dụ được sử dụng trong ICL. Trong bài báo này, chúng tôi đề xuất GeMQuAD - một phương pháp học bán giám sát, mở rộng khung WeakDAP, được áp dụng cho một tập dữ liệu được tạo ra thông qua ICL với chỉ một ví dụ trong ngôn ngữ đích sử dụng AlexaTM 20B Seq2Seq LLM. Thông qua phương pháp của chúng tôi, chúng tôi lặp lại việc xác định dữ liệu chất lượng cao để nâng cao hiệu suất mô hình, đặc biệt cho môi trường đa ngữ tài nguyên thấp trong bối cảnh nhiệm vụ Trả lời Câu hỏi Trích xuất. Khung của chúng tôi vượt trội hơn mô hình được tăng cường bằng dịch máy 0.22/1.68 điểm F1/EM (Khớp Chính xác) cho tiếng Hindi và 0.82/1.37 điểm F1/EM cho tiếng Tây Ban Nha trên tập dữ liệu MLQA, và nó vượt qua hiệu suất của mô hình được huấn luyện trên tập dữ liệu chỉ có tiếng Anh 5.05/6.50 điểm F1/EM cho tiếng Hindi và 3.81/3.69 điểm F1/EM cho tiếng Tây Ban Nha trên cùng tập dữ liệu. Đáng chú ý, phương pháp của chúng tôi sử dụng một LLM được tiền huấn luyện để tạo ra mà không cần tinh chỉnh (FT), chỉ sử dụng một ví dụ được chú thích duy nhất trong ICL để tạo dữ liệu, cung cấp một quy trình phát triển hiệu quả về chi phí.

## 1 Giới thiệu

Trong khi các LLM như ChatGPT (OpenAI, 2023) có thể trả lời câu hỏi từ văn bản, chúng tốn kém về mặt tính toán và phát sinh chi phí lớn để chạy với độ trễ thấp và thông lượng cao. Một lựa chọn thay thế phổ biến là sử dụng các mô hình chỉ mã hóa nhỏ như BERT (Devlin et al., 2019) hoặc XLM-R (Conneau et al., 2019) cho QA trích xuất. Tuy nhiên, những mô hình nhỏ hơn này dựa vào một lượng lớn dữ liệu được chú thích, điều này khan hiếm và khó có được, đặc biệt cho các môi trường đa ngữ và cụ thể theo lĩnh vực. Trong trường hợp thiếu dữ liệu được chú thích, việc tạo dữ liệu tổng hợp gần đây đã có hiệu quả trong việc vượt qua tình trạng khan hiếm dữ liệu được gán nhãn cho các ứng dụng bao gồm QA trích xuất (Alberti et al., 2019).

Dạy thông qua Dữ liệu (TvD) (Rosenbaum et al., 2023) liên quan đến việc sử dụng mô hình giáo viên dựa trên LLM để tạo ra dữ liệu huấn luyện tổng hợp cho một nhiệm vụ cụ thể, được sử dụng để tinh chỉnh một mô hình học sinh nhỏ hơn. Thách thức là các LLM, trong khi có khả năng tạo ra dữ liệu, thiếu huấn luyện cụ thể theo lĩnh vực, và việc đại diện nhiệm vụ vẫn không đầy đủ do các ví dụ hạn chế trong ICL. Điều này dẫn đến chất lượng dữ liệu thấp hơn so với dữ liệu được chú thích. Như đã được chứng minh trong (Shakeri et al., 2021), các mô hình sinh cho phép tạo ra dữ liệu cụ thể theo nhiệm vụ, đa ngữ thông qua tinh chỉnh trên một ngôn ngữ duy nhất. Tuy nhiên, quá trình này tốn kém do số lượng lớn tham số liên quan đến việc tinh chỉnh LLM.

Trong bài báo này, chúng tôi tập trung vào việc tạo ra dữ liệu tổng hợp chất lượng cao trong các kịch bản tài nguyên thấp mà không cần tinh chỉnh mô hình giáo viên. Chúng tôi đề xuất áp dụng AlexaTM 20B (Soltan et al., 2022) để tạo ra dữ liệu trả lời câu hỏi tổng hợp bằng cách nhắc mô hình với một ví dụ trong ngữ cảnh 1 lần, và chúng tôi áp dụng một phương pháp bán giám sát dựa trên WeakDAP (Chen et al., 2022) trên mô hình học sinh XLM-R-Base để xác định các ví dụ chất lượng cao từ dữ liệu được tạo ra và cải thiện hiệu suất một cách tăng dần. Trong khi chúng tôi sử dụng AlexaTM 20B làm giáo viên, đây là một khung tổng quát có thể được áp dụng cho dữ liệu được tạo ra từ bất kỳ LLM nào và cho bất kỳ loại nhiệm vụ nào.

## 2 Các Công trình Liên quan

Việc tạo ra cặp Câu hỏi-Trả lời (QAG) chủ yếu được coi là hai nhiệm vụ độc lập: cho một ngữ cảnh để thực hiện Tạo Câu hỏi (QG) và sau đó một nhiệm vụ Trả lời Câu hỏi (QA). Ngữ cảnh được sử dụng cho QG được sử dụng trong các giải pháp đa ngữ như tạo ra bằng tiếng Anh trước rồi sau đó dịch bằng dịch máy (MT) sang ngôn ngữ yêu cầu, trong đó (Li và Callison-Burch, 2023) đã thực hiện điều này bằng cách sử dụng căn chỉnh, trong khi (Fabbri et al., 2020) cũng đã xem xét việc tạo ra dựa trên mẫu cho các lĩnh vực cụ thể. Mặt khác (Riabi et al., 2021) đã xem xét việc tạo ra dữ liệu tổng hợp trực tiếp trong ngôn ngữ đích. Một sự khác biệt với các công trình như vậy là GeMQuAD không dựa vào MT và kho ngữ liệu song song vì chúng tôi đang sử dụng các mô hình đa ngữ. QA có thể được xem là đối tác của bài toán QG, như đã được chứng minh bởi (Kramchaninova và Defauw, 2022) trong việc sử dụng các câu trả lời ứng cử viên dựa trên ngữ cảnh để tạo ra câu hỏi. Công trình trước đây (Shakeri et al., 2021) đánh giá chất lượng Câu hỏi & Trả lời (Q&A) thông qua các đánh giá tự động, trong khi (Riabi et al., 2021) và (Agrawal et al., 2023) đã xem xét các nhiệm vụ QA hạ nguồn để định lượng hiệu suất của chúng.

QAMELEON (Agrawal et al., 2023) là công trình gần đây nhất liên quan đến chúng tôi, cũng đã giải quyết vấn đề này bằng cách coi việc tạo ra Q&A là một nhiệm vụ duy nhất. Chúng tôi tin rằng phương pháp này có tiềm năng phát triển tốt hơn so với việc coi nó là hai nhiệm vụ riêng biệt, vì nó bảo tồn sự phụ thuộc giữa câu hỏi và câu trả lời. Tương tự như chúng tôi, QAMELEON đề xuất sử dụng LLM để tạo ra các cặp Q&A tổng hợp thông qua ICL với 5 mẫu vàng, sau đó họ sử dụng để tinh chỉnh một mô hình QA để đánh giá các đóng góp của họ. Khái niệm cốt lõi của QAMELEON liên quan đến Điều chỉnh Nhắc (PT) LLM của họ (PaLM-540B Chowdhery et al. (2022)) cho nhiệm vụ QA để tạo ra dữ liệu chất lượng cao và đánh giá phương pháp của họ trên mt5-XL Xue et al. (2021). Điều chỉnh Nhắc (PT) là một kỹ thuật tinh chỉnh hiệu quả về tham số học và cập nhật các tham số mô hình cụ thể cho nhắc đầu vào chỉ thay vì cập nhật toàn bộ tham số LLM. Từ góc độ tính toán, trong khi PT hiệu quả hơn FT, nó vẫn liên quan đến việc điều chỉnh LLM như PaLM 540B có chi phí tính toán cao. QAMELEON cũng tuyên bố hiệu suất đạt được của họ gắn liền với mô hình LLM cơ sở được sử dụng tức là PaLM 540B. Tuy nhiên, chúng tôi tin rằng việc sử dụng một LLM lớn như PaLM không khả thi trong các kịch bản ngân sách thấp. Phương pháp GeMQuAD của chúng tôi là một khung tổng quát có thể được áp dụng cho bất kỳ loại dữ liệu tổng hợp nào, để có được các cặp chất lượng cao từ tập dữ liệu được tạo ra tổng thể. Chúng tôi thử nghiệm với các mô hình nhỏ hơn với việc xem xét thực thi chi phí thấp, không có tinh chỉnh của các mô hình tạo ra, và chúng tôi đang sử dụng một phương pháp lặp để phát triển trên dữ liệu tổng hợp của chúng tôi so với việc sử dụng tất cả dữ liệu tổng hợp được tạo ra. Để so sánh kết quả của chúng tôi với QAMELEON, chúng tôi không thể tái tạo chiến lược tạo ra vì PaLM-540B là một mô hình đóng, và các ngôn ngữ trong tập dữ liệu QAMELEON được phát hành không trùng lặp với các ngôn ngữ mục tiêu của chúng tôi.

## 3 Phương pháp

Để cải thiện hiệu suất đa ngữ của nhiệm vụ QA Trích xuất hạ nguồn, phương pháp của chúng tôi bao gồm 3 bước chính. 1) Tạo ra dữ liệu trả lời câu hỏi tổng hợp sử dụng ICL 1-shot trên mô hình AlexaTM 20B (giáo viên); 2) Áp dụng phương pháp học bán giám sát của chúng tôi để lọc các bản ghi chất lượng tốt; 3) tinh chỉnh mô hình học sinh XLM-R-Base với dữ liệu được lọc từ bước 2. Chúng tôi áp dụng các bước 2 & 3 lặp lại cho đến khi hiệu suất mô hình ngừng cải thiện trong tối đa k=2 vòng.

### 3.1 Tạo Dữ liệu Tổng hợp sử dụng ICL

Hình 1 cho thấy thiết kế nhắc được sử dụng để tạo dữ liệu trên AlexaTM 20B sử dụng ICL. [CLM] là một token đặc biệt mà mô hình mong đợi trong quá trình học trong ngữ cảnh. Hướng dẫn cung cấp định hướng cho mô hình để hiểu rằng Câu hỏi và Trả lời nên được tạo ra từ Ngữ cảnh được thể hiện trong ví dụ 1-shot. Ngữ cảnh kiểm tra được thêm vào sau ví dụ. Mô hình học cách tạo ra câu hỏi và câu trả lời từ ngữ cảnh kiểm tra, theo cùng định dạng được chứng minh trong ví dụ. Thêm chi tiết về cấu hình ICL được sử dụng được đề cập trong phần Thiết lập Thí nghiệm.

### 3.2 Phương pháp học bán giám sát

Chúng tôi đang tạo ra dữ liệu trong một môi trường tài nguyên thấp, tức là học 1-shot. Dữ liệu tổng hợp được tạo ra bởi mô hình sẽ có chất lượng thấp hơn so với dữ liệu được chú thích. Trong khi các kiểm tra chất lượng có thể được thêm vào để loại trừ các cặp chất lượng thấp như một phần của bước xử lý hậu kỳ, những kiểm tra này chỉ giới hạn ở các xác thực xác định và cấu trúc. Như một phần của xử lý hậu kỳ, chúng tôi xác minh rằng câu trả lời là một phần của ngữ cảnh, và loại bỏ bất kỳ cặp Q&A trùng lặp nào khỏi tập dữ liệu.

Để xác định các cặp chức năng chất lượng tốt, chúng tôi mở rộng phương pháp WeakDAP với tùy chỉnh của chúng tôi để tinh chỉnh mô hình học sinh XLM-R-base để lọc ra dữ liệu tổng hợp được tạo ra một cách chính xác. Như được hiển thị trong Hình 2, quy trình làm việc bắt đầu với việc tạo ra dữ liệu tổng hợp trong ngôn ngữ đích từ mô hình giáo viên thông qua ICL sử dụng định dạng được hiển thị trong Hình 1. Khi việc tạo dữ liệu hoàn tất, một bộ gán nhãn yếu đánh giá dữ liệu tổng hợp cho nhiệm vụ QA Trích xuất. Cho một bộ (ngữ cảnh, câu hỏi) làm đầu vào, bộ gán nhãn dự đoán khoảng trả lời trong ngữ cảnh tương ứng.

Trong các thí nghiệm của chúng tôi, ban đầu chúng tôi sử dụng mô hình cơ sở của chúng tôi làm bộ gán nhãn yếu, được huấn luyện trên kho ngữ liệu dữ liệu được chú thích tiếng Anh (dữ liệu Vàng) cho nhiệm vụ QA Trích xuất. Vì bộ gán nhãn chưa từng thấy bất kỳ dữ liệu nào trong ngôn ngữ đích trong quá trình huấn luyện, hiệu suất QA Trích xuất của mô hình trên ngôn ngữ đích bị hạn chế, và chủ yếu dựa vào khả năng chuyển giao đa ngữ của mô hình cơ sở mà nó được tinh chỉnh. Từ tập dữ liệu tổng hợp, bất kỳ câu hỏi nào mà bộ gán nhãn của chúng tôi có thể cung cấp cùng câu trả lời như được tạo ra bởi mô hình giáo viên có thể được coi là có chất lượng cao và liên quan đến ngữ cảnh. Do đó, tất cả các cặp Q&A mà bộ gán nhãn tạo ra một cách chính xác dựa trên nhãn trả lời dữ liệu tổng hợp được lọc và thêm vào tập dữ liệu bạc.

### 3.3 Tinh chỉnh mô hình học sinh

Mô hình học sinh được tinh chỉnh đầu tiên trên dữ liệu được lọc từ phương pháp bán giám sát (dữ liệu bạc) và sau đó trên dữ liệu vàng. Điều này nhằm ưu tiên dữ liệu chất lượng cao hơn trong phần sau của FT mà chúng tôi thấy cải thiện hiệu suất mô hình. Phân tích tương tự cũng được báo cáo bởi (Riabi et al., 2021) trong các công trình của họ. Hiệu suất mô hình học sinh được cải thiện do việc thêm dữ liệu bạc mới trong ngôn ngữ đích, giờ đây chúng tôi thay thế bộ gán nhãn yếu bằng mô hình cải tiến, và thực hiện đánh giá dữ liệu tổng hợp lần nữa. Vì bộ gán nhãn là một phiên bản cải tiến so với các lần lặp trước đó, nó sẽ có thể xác định thêm một vài cặp Q&A chất lượng tốt hơn từ tập dữ liệu tổng hợp, điều này làm tăng khối lượng dữ liệu bạc. Quá trình này tiếp tục lặp lại cho đến khi hiệu suất mô hình không cải thiện trong k vòng bởi tối thiểu ngưỡng e (hoặc) khối lượng dữ liệu bạc mới trong lần lặp đó nhỏ hơn v% của tổng dữ liệu tổng hợp được tạo ra. Các giá trị k, e, v có thể điều chỉnh.

## 4 Thí nghiệm

### 4.1 Tập dữ liệu

Chúng tôi đã sử dụng nhiều tập dữ liệu như một phần của phân tích thí nghiệm. Trong việc tạo dữ liệu tổng hợp sử dụng ICL, chúng tôi đã sử dụng mười bản ghi đầu tiên từ tập dữ liệu XQUAD (Artetxe et al., 2020) cho các ví dụ được gán nhãn tiếng Hindi và tiếng Tây Ban Nha trong các nhắc ICL 1-shot của chúng tôi. Để tạo ra các cặp Q&A tổng hợp, chúng tôi đã sử dụng ngữ cảnh từ phân chia 'translate-dev' của XTREME (Hu et al., 2020). Mô hình học sinh của chúng tôi được huấn luyện và xác thực sử dụng dữ liệu QA trích xuất tiếng Anh từ SQUAD v1.1 (Rajpurkar et al., 2016). Để đánh giá hiệu suất của mô hình học sinh trên các nhiệm vụ QA đa ngữ, chúng tôi sử dụng hai tập dữ liệu đánh giá: tập dữ liệu MLQA (Lewis et al., 2019) và 1180 bản ghi còn lại từ tập dữ liệu XQUAD, loại trừ 10 bản ghi đầu tiên. Tất cả các tập dữ liệu này được chú thích bởi con người trừ XTREME, dựa vào dịch máy. Thêm chi tiết về các tập dữ liệu có thể được tìm thấy trong Phụ lục.

### 4.2 Thiết lập Thí nghiệm

#### 4.2.1 Cấu hình ICL

Để thực hiện học 1-shot trên AlexaTM, chúng tôi đã sử dụng 10 ví dụ được chú thích từ tập dữ liệu XQUAD cho cả việc tạo dữ liệu tiếng Hindi và tiếng Tây Ban Nha, và tạo ra các cặp Q&A cho các ngữ cảnh từ tập dữ liệu XTREME. Chúng tôi chọn ngẫu nhiên 1 ví dụ để tạo nhắc đầu vào để đảm bảo quá trình tạo ra của chúng tôi được ngẫu nhiên hóa và đa dạng hóa. Về cấu hình ICL, chúng tôi đã sử dụng phương pháp lấy mẫu (do_sample=True), với nhiệt độ được đặt ở 0.9, và chọn ngẫu nhiên top_k & top_p trong khoảng từ 50 đến 100 và 0.5 đến 0.95 tương ứng cho mỗi ví dụ để tăng sự khái quát hóa trong các bản ghi được tạo ra. max_length được đặt ở 50 token để kiểm soát độ dài tạo ra.

#### 4.2.2 Cấu hình Tinh chỉnh

Chúng tôi sử dụng XLM-R-base được tiền huấn luyện (Conneau et al., 2019) làm mô hình học sinh của chúng tôi, tinh chỉnh nó trên nhiệm vụ QA với dữ liệu tổng hợp được tạo ra & đánh giá hiệu suất của nó trên cả tập dữ liệu MLQA & XQUAD. Tập dữ liệu MLQA và XQUAD đều là tập dữ liệu QA miền mở. Do đó, chúng tôi tạo ra dữ liệu tổng hợp chỉ một lần sử dụng các ngữ cảnh kiểm tra từ XTREME và không tạo ra bất kỳ dữ liệu tổng hợp nào cụ thể cho các tập dữ liệu này. Trên tất cả các thí nghiệm, chúng tôi nhất quán sử dụng một tập con mẫu cố định gồm 10.000 bản ghi từ tập dữ liệu SQUAD làm dữ liệu vàng, mà chúng tôi gọi là 'SQUAD en10k', để duy trì tính nhất quán trong kích thước tập dữ liệu được chú thích tiếng Anh với các tập dữ liệu tổng hợp được tạo ra.

Cùng với phương pháp bán giám sát (XLMRb gemquad), chúng tôi thử nghiệm trên các cấu hình sử dụng dữ liệu tổng hợp trong việc tinh chỉnh mô hình học sinh, với XLMRb combined là mô hình mà chúng tôi tinh chỉnh chỉ một lần trên tập dữ liệu kết hợp của dữ liệu tổng hợp và tập con SQUAD en10k, trong khi XLMRb sequential là mô hình được tinh chỉnh đầu tiên trên dữ liệu tổng hợp hoàn chỉnh và sau đó tinh chỉnh trên SQUAD en10k (tương tự như phương pháp tinh chỉnh của chúng tôi nhưng không có lọc bán giám sát). Để so sánh cơ sở, chúng tôi đánh giá phương pháp của chúng tôi so với hai mô hình. XLMRb baseline được tinh chỉnh chỉ trên tập con SQUAD en10k. Chúng tôi so sánh phương pháp của chúng tôi với tăng cường dữ liệu sử dụng dịch máy (MT) (Hu et al., 2020), XLMRb MT được tinh chỉnh trên một tập dữ liệu kết hợp bao gồm SQUAD en10k, cũng như phiên bản được dịch máy của nó cho dữ liệu tiếng Hindi và tiếng Tây Ban Nha. XLMRb baseline cũng là phiên bản ban đầu của bộ gán nhãn yếu của chúng tôi trong phương pháp lọc bán giám sát.

Chúng tôi chạy các vòng lặp, trong đó mỗi lần lặp chúng tôi tinh chỉnh mô hình học sinh trên các lô dữ liệu bạc được lọc bởi bộ gán nhãn và SQUAD en10k. Mô hình cải tiến trong lần lặp hiện tại đánh giá dữ liệu tổng hợp để dự đoán câu trả lời cho các cặp (ngữ cảnh, câu hỏi) đã cho, và phân loại mẫu tổng hợp tương ứng như một ví dụ được tạo ra chính xác nếu các câu trả lời khớp hoặc ngược lại. Lô mẫu tổng hợp được phân loại là được tạo ra chính xác trong lần lặp hiện tại bởi mô hình học sinh của chúng tôi, được bao gồm trong dữ liệu bạc để tinh chỉnh mô hình học sinh trong lần lặp tiếp theo. Các lần lặp được dừng lại nếu cải thiện trong hiệu suất nhiệm vụ QA mô hình cho k=2 vòng bởi ngưỡng e<0.005 hoặc khối lượng lô mẫu mới được thêm vào dữ liệu bạc v<1% của tổng dữ liệu tổng hợp được tạo ra bởi ICL.

Để tinh chỉnh, chúng tôi đã sử dụng một bộ lập lịch tuyến tính với AdamW (Loshchilov và Hutter, 2017) làm bộ tối ưu hóa, với learning_rate=2e-5 & batch_size=8. Để duy trì tính nhất quán của kết quả của chúng tôi trên các thí nghiệm, chúng tôi thay đổi các epoch cho mỗi thí nghiệm để chúng tôi có số bước cập nhật huấn luyện tương tự để đảm bảo các mô hình trong mỗi thí nghiệm trải qua lượng huấn luyện tương tự trong tất cả các lần lặp/thí nghiệm. Do đó, bất kỳ thay đổi hiệu suất nào (có tính đến biến động nhỏ) đều liên quan đến dữ liệu, xác thực việc sử dụng dữ liệu tổng hợp, bất kể kích thước của nó. Chúng tôi sử dụng epoch hoạt động tốt nhất theo tập xác thực trong huấn luyện làm mô hình cuối cùng của chúng tôi tại mỗi lần lặp.

## 5 Kết quả

| Mô hình | Tiếng Anh | Tiếng Hindi | Tiếng Tây Ban Nha | Trung bình |
|---------|-----------|-------------|-------------------|------------|
| XLMRb baseline | 74.73 / 61.04 | 55.04 / 37.09 | 61.04 / 40.00 | 55.99 / 40.74 |
| XLMRb MT | 75.85 / 62.12 | 59.87 / 41.91 | 64.03/ 42.32 | 58.48 / 42.91 |
| XLMRb combined | 75.17 / 61.51 | 51.27 / 37.17 | 58.53 / 39.52 | 54.95 / 41.17 |
| XLMRb sequential | 76.17 / 62.40 | 59.67 / 43.51 | 64.73 / 43.44 | 59.38 / 44.36 |
| XLMRb gemquad | 76.33 / 62.66 | 60.09 / 43.59 | 64.85 / 43.69 | 59.81 / 44.63 |

Bảng 1: Hiệu suất trong nhiệm vụ QA cho FT với dữ liệu tổng hợp tiếng Hindi & tiếng Tây Ban Nha cùng với tập con SQUAD en10k. Điểm được báo cáo là F1/Khớp Chính xác trên tập dữ liệu MLQA cho 3 ngôn ngữ là một phần của việc huấn luyện và cũng là trung bình trên các ngôn ngữ en, hi, es, de, ar, vi và zh trong tập dữ liệu MLQA.

| Mô hình | Tiếng Anh | Tiếng Hindi | Tiếng Tây Ban Nha | Trung bình |
|---------|-----------|-------------|-------------------|------------|
| XLMRb baseline | 78.56 / 66.69 | 61.18 / 45.00 | 71.39 / 51.53 | 67.39 / 51.15 |
| XLMRb MT | 79.63 / 67.12 | 67.79 / 50.76 | 74.22 / 54.15 | 69.39 / 52.96 |
| XLMRb combined | 78.95 / 66.44 | 57.85 / 42.88 | 63.05 / 45.17 | 64.67 / 49.41 |
| XLMRb sequential | 79.36 / 67.54 | 67.18 / 50.76 | 72.79 / 54.24 | 69.97 / 54.36 |
| XLMRb gemquad | 79.59 / 67.71 | 67.79 / 52.03 | 73.89 / 55.00 | 70.49 / 55.10 |

Bảng 2: Hiệu suất trong nhiệm vụ QA cho FT với dữ liệu tổng hợp tiếng Hindi & tiếng Tây Ban Nha cùng với tập con SQUAD en10k. Điểm được báo cáo là F1/Khớp Chính xác trên tập dữ liệu XQUAD cho 3 ngôn ngữ là một phần của việc huấn luyện và cũng là trung bình trên các ngôn ngữ en, hi, es, de, ar, vi và zh trong tập dữ liệu XQUAD.

Bảng 1 & 2 mô tả hiệu suất của chúng tôi khi sử dụng dữ liệu tổng hợp được tạo ra bằng tiếng Hindi và tiếng Tây Ban Nha cùng với SQUAD en10k trên các tập dữ liệu MLQA và XQUAD. Chúng tôi đang sử dụng cùng một mô hình để đánh giá trên cả hai tập dữ liệu. Trên XQUAD, chúng tôi thực hiện đánh giá chỉ cho các ngôn ngữ chung giữa MLQA & XQUAD để nhất quán về hiệu suất tổng thể được tính trung bình trên các ngôn ngữ.

Chúng tôi tạo ra gần 19,5K cặp Q&A cho tiếng Hindi và khoảng 15,5K cặp cho tiếng Tây Ban Nha. Bảng 3 hiển thị các con số này cùng với khối lượng tập dữ liệu khác được sử dụng trong phân tích của chúng tôi. Hình 4 mô tả số lượng mẫu dữ liệu tổng hợp được phân loại là đúng trong mỗi lần lặp của cả thí nghiệm tiếng Hindi và tiếng Tây Ban Nha. Chúng tôi thấy rằng chúng tôi bắt đầu với một phần lớn của các mẫu, khoảng 30% tổng dữ liệu, nhưng chỉ tăng dần khi chúng tôi tiến hành trong các lần lặp. Điều này được mong đợi vì chúng tôi chỉ muốn các mẫu tốt nhất, vì vậy chúng dễ bị chọn hơn trong các lần lặp sớm hơn. Cuối cùng chúng tôi có thể sử dụng khoảng 45% dữ liệu được tạo ra cho trường hợp sử dụng của chúng tôi. Chúng tôi đạt được hiệu suất tốt nhất trong lần lặp thứ 3. Không có cải thiện thêm trong lần lặp thứ 4 và 5, khiến chúng tôi dừng các lần lặp với tiêu chí k=2. Thêm chi tiết về hiệu suất theo từng lần lặp được cung cấp trong Phụ lục.

Dữ liệu tổng hợp giúp cải thiện hiệu suất Với phương pháp được đề xuất của chúng tôi (XLMRb gemquad), chúng tôi quan sát thấy những cải thiện tương tự trên cả hai tập dữ liệu. Trên MLQA, chúng tôi thấy 3.82/3.89 (F1/EM) điểm cải thiện so với XLMRb baseline, với cải thiện 5.05/6.50 & 3.81/3.69 điểm trong tiếng Hindi & tiếng Tây Ban Nha tương ứng. Trên XQUAD, chúng tôi đạt được cải thiện tổng thể 3.1/3.95 điểm, với cải thiện 6.61/7.03 & 2.50/3.47 điểm trong tiếng Hindi & tiếng Tây Ban Nha tương ứng, kết luận rằng việc có các ví dụ tổng hợp mới cải thiện hiệu suất.

Tinh chỉnh tuần tự trên các tập dữ liệu riêng biệt hoạt động tốt hơn Cấu hình tinh chỉnh XLMRb combined cho kết quả hỗn hợp. Trong khi tiếng Anh thấy một chút tăng hiệu suất, nhìn chung, hiệu suất tiếng Hindi và tiếng Tây Ban Nha giảm. Chúng tôi nghi ngờ rằng bằng cách tinh chỉnh trên cả dữ liệu tổng hợp và SQUAD en10k cùng nhau, chúng tôi có thể vô tình định hướng sai mô hình bằng cách cho tầm quan trọng như nhau cho tất cả các mẫu. Do đó, chúng tôi thử nghiệm với cấu hình XLMRb sequential, tinh chỉnh mỗi tập dữ liệu riêng biệt, ưu tiên SQUAD en10k do việc tinh chỉnh sau này hoạt động tốt hơn.

Dữ liệu chất lượng cao, kích thước nhỏ hơn vượt trội hơn dữ liệu chất lượng thấp, khối lượng lớn XLMRb gemquad vượt trội hơn XLMRb MT liên quan đến nhiều mẫu dữ liệu hơn so với phương pháp bán giám sát. Ngoài ra, bằng cách chọn các cặp dữ liệu chất lượng cao từ tập dữ liệu tổng thể hoạt động tốt hơn so với việc sử dụng toàn bộ tập dữ liệu tổng hợp được tạo ra thông qua ICL (XLMRb sequential).

Từ đánh giá của chúng tôi ở trên, tổng thể GeMQuAD vượt trội hơn XLMRb combined với cải thiện 5.3/4.6 F1/EM (Hiệu suất trung bình trên tất cả các ngôn ngữ trong cả tập dữ liệu MLQA & XQUAD) và XLMRb sequential với 0.5/0.5 F1/EM. Trên các đường cơ sở, phương pháp của chúng tôi hoạt động tốt hơn dịch máy XLMRb MT với 1.2/1.9 F1/EM, trong khi chỉ sử dụng dữ liệu được chú thích tiếng Anh XLMRb baseline với 3.5/3.9 F1/EM. Tóm lại, GeMQuAD > sequential (0.5/0.5) > MT (1.2/1.9) > baseline (3.5/3.9) > combined (5.3/4.6).

Chúng tôi quan sát thấy những cải thiện tốt trong các ngôn ngữ không được bao gồm trong dữ liệu tinh chỉnh của mô hình học sinh, như tiếng Đức, tiếng Ả Rập, tiếng Việt và tiếng Trung. Thông tin chi tiết về hiệu suất của các ngôn ngữ này được cung cấp trong Phụ lục. Chúng tôi cho rằng khả năng đa ngữ tăng cường này là do việc chọn lọc các mẫu chất lượng cao hơn từ dữ liệu tổng hợp được tạo ra của chúng tôi.

## 6 Kết luận & Các bước tiếp theo

Trong bài báo này, chúng tôi đã giới thiệu GeMQuAD, một phương pháp hiệu quả về chi phí để tạo ra dữ liệu tổng hợp bằng tiếng Hindi và tiếng Tây Ban Nha sử dụng ICL trên AlexaTM. Chúng tôi đã chứng minh hiệu quả của phương pháp bán giám sát để chọn lọc các cặp Q&A tổng hợp chất lượng cao, dẫn đến cải thiện 0.22/1.68 F1/EM cho tiếng Hindi và 0.82/1.37 F1/EM cho tiếng Tây Ban Nha so với mô hình được huấn luyện trên dữ liệu tiếng Anh, được tăng cường bằng dịch máy. Phương pháp của chúng tôi vượt trội hơn mô hình được huấn luyện chỉ trên tập dữ liệu tiếng Anh với 5.05/6.50 F1/EM cho tiếng Hindi và 3.81/3.69 F1/EM cho tiếng Tây Ban Nha trên tập dữ liệu MLQA, tất cả đều đạt được mà không cần tinh chỉnh LLM. Các bước tiếp theo của chúng tôi liên quan đến việc mở rộng phân tích này cho các ngôn ngữ tài nguyên thấp khác, thực hiện QA cụ thể theo lĩnh vực, và mở rộng chức năng để bao gồm việc tạo ra QA trừu tượng.

## Tài liệu tham khảo

Priyanka Agrawal, Chris Alberti, Fantine Huot, Joshua Maynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev, Dipanjan Das, và Mirella Lapata. 2023. Qameleon: Multilingual qa with only 5 examples.

Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, và Michael Collins. 2019. Synthetic qa corpora generation with roundtrip consistency.

Mikel Artetxe, Sebastian Ruder, và Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.

Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Andy Rosenbaum, Seokhwan Kim, Yang Liu, Zhou Yu, và Dilek Hakkani-Tur. 2022. Weakly supervised data augmentation through prompting for dialogue understanding.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. CoRR, abs/1911.02116.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Alexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, và Bing Xiang. 2020. Template-based question generation from retrieved sentences for improved unsupervised question answering. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 4508–4513, Online. Association for Computational Linguistics.

Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, và Melvin Johnson. 2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. Trong Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, trang 4411–4421. PMLR.

Alina Kramchaninova và Arne Defauw. 2022. Synthetic data generation for multilingual domain-adaptable question answering systems. Trong Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, trang 151–160, Ghent, Belgium. European Association for Machine Translation.

Patrick Lewis, Barlas Oğuz, Ruty Rinott, Sebastian Riedel, và Holger Schwenk. 2019. Mlqa: Evaluating cross-lingual extractive question answering. arXiv preprint arXiv:1910.07475.

Bryan Li và Chris Callison-Burch. 2023. Paxqa: Generating cross-lingual question answering examples at training scale.

Ilya Loshchilov và Frank Hutter. 2017. Fixing weight decay regularization in adam. CoRR, abs/1711.05101.

OpenAI. 2023. GPT-3.5 (ChatGPT). https://openai.com.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.

Arij Riabi, Thomas Scialom, Rachel Keraron, Benoît Sagot, Djamé Seddah, và Jacopo Staiano. 2021. Synthetic data augmentation for zero-shot cross-lingual question answering. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 7016–7030, Online và Punta Cana, Dominican Republic. Association for Computational Linguistics.

Andy Rosenbaum, Saleh Soltan, và Wael Hamza. 2023. Using large language models (llms) to synthesize training data. Amazon Science.

Siamak Shakeri, Noah Constant, Mihir Kale, và Linting Xue. 2021. Towards zero-shot multilingual synthetic question and answer generation for cross-lingual reading comprehension. Trong Proceedings of the 14th International Conference on Natural Language Generation, trang 35–45, Aberdeen, Scotland, UK. Association for Computational Linguistics.

Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, và Prem Natarajan. 2022. Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, và Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer.

## Phụ lục

### A Hiệu suất mô hình trên các ngôn ngữ khác

Bảng 3 và 4 trình bày hiệu suất mô hình và so sánh cho các ngôn ngữ trong tập dữ liệu MLQA và XQUAD, loại trừ tiếng Anh, tiếng Hindi và tiếng Tây Ban Nha. Mặc dù không phải là một phần của việc tinh chỉnh, chúng tôi quan sát thấy cải thiện đáng chú ý trong các ngôn ngữ này. Điều này cho thấy rằng khi hiệu suất tổng thể của mô hình cải thiện với dữ liệu bổ sung trong các ngôn ngữ đích, khả năng chuyển giao đa ngữ của nó cũng tăng lên, dẫn đến hiệu suất được nâng cao trong các ngôn ngữ này. Chúng tôi giới hạn đánh giá ở 7 ngôn ngữ chung với MLQA để duy trì tính nhất quán trong các so sánh đa ngữ, mặc dù XQUAD hỗ trợ 12 ngôn ngữ.

| Mô hình | Tiếng Đức | Tiếng Ả Rập | Tiếng Việt | Tiếng Trung |
|---------|-----------|-------------|------------|-------------|
| XLMRb baseline | 56.23 / 41.18 | 49.56 / 31.10 | 61.13 / 41.18 | 34.21 / 33.62 |
| XLMRb MT | 58.00 / 42.02 | 50.57 / 32.15 | 63.22 / 42.58 | 37.83 / 37.26 |
| XLMRb combined | 56.06 / 41.89 | 46.94 / 31.08 | 60.69 / 41.64 | 35.96 / 35.39 |
| XLMRb sequential | 59.26 / 44.12 | 52.53 / 34.43 | 64.60 / 44.59 | 38.67 / 38.06 |
| XLMRb gemquad | 59.32 / 43.57 | 52.56 / 34.41 | 66.13 / 45.77 | 39.37 / 38.74 |

Bảng 3: Hiệu suất trong nhiệm vụ QA cho FT với dữ liệu tổng hợp tiếng Hindi & tiếng Tây Ban Nha cùng với tập con SQUAD en10k. Điểm được báo cáo là F1/Khớp Chính xác trên tập dữ liệu MLQA cho các ngôn ngữ không phải là một phần của FT, như tiếng Đức, tiếng Ả Rập, tiếng Việt và tiếng Trung.

| Mô hình | Tiếng Đức | Tiếng Ả Rập | Tiếng Việt | Tiếng Trung |
|---------|-----------|-------------|------------|-------------|
| XLMRb baseline | 69.09 / 52.29 | 62.49 / 45.76 | 69.10 / 49.07 | 59.95 / 47.71 |
| XLMRb MT | 69.33 / 51.95 | 62.41 / 44.66 | 71.18 / 50.67 | 61.19 / 51.44 |
| XLMRb combined | 67.50 / 51.95 | 57.42 / 40.51 | 66.74 / 46.95 | 61.16 / 51.95 |
| XLMRb sequential | 71.64 / 55.34 | 64.08 / 47.20 | 70.76 / 50.93 | 63.96 / 54.49 |
| XLMRb gemquad | 70.92 / 55.17 | 64.96 / 48.22 | 72.03 / 51.86 | 64.23 / 55.68 |

Bảng 4: Hiệu suất trong nhiệm vụ QA cho FT với dữ liệu tổng hợp tiếng Hindi & tiếng Tây Ban Nha cùng với tập con SQUAD en10k. Điểm được báo cáo là F1/Khớp Chính xác trên tập dữ liệu XQUAD cho các ngôn ngữ không phải là một phần của FT, như tiếng Đức, tiếng Ả Rập, tiếng Việt và tiếng Trung.

### B Chi tiết lần lặp phương pháp Bán giám sát

Phần này cung cấp những hiểu biết chi tiết về hiệu suất tăng dần & dữ liệu được bao gồm tinh chỉnh qua các lần lặp của phương pháp bán giám sát được đề xuất trong bài báo này.

Đối với thí nghiệm với việc tinh chỉnh mô hình học sinh XLM-R-Base với dữ liệu tổng hợp được tạo ra từ AlexaTM 20B bằng tiếng Hindi & tiếng Tây Ban Nha, phương pháp lặp bán giám sát của chúng tôi kết thúc sau 5 lần lặp, với lần lặp thứ 3 là tốt nhất về hiệu suất vì hiệu suất mô hình không cải thiện trong lần lặp thứ 4 và 5 (tức là k=2).

Bảng 5 đại diện cho phân phối dữ liệu tổng hợp được bao gồm trong các lần lặp bán giám sát. Tổng số mẫu được bao gồm trong mỗi vòng bao gồm dữ liệu tổng hợp tiếng Hindi, dữ liệu tổng hợp tiếng Tây Ban Nha và SQUAD en10k. Ví dụ cho vòng 1, tổng dữ liệu được bao gồm là 4528+5407+10000 = 19935. Trong quá trình huấn luyện, hiệu suất mô hình liên tục cải thiện cho đến vòng 3 trên xác thực SQUAD (tập dữ liệu xác thực được sử dụng trong việc tinh chỉnh mô hình học sinh), sau đó có sự suy giảm nhẹ trong 2 vòng. Theo tiêu chí dừng, chúng tôi dừng quá trình lặp, và chọn vòng 3 là mô hình hoạt động tốt nhất từ phương pháp bán giám sát. Cùng với hiệu suất mô hình trên tập xác thực, chúng tôi cũng đại diện cho hiệu suất mô hình trên các tập dữ liệu đánh giá MLQA và XQUAD để tham khảo. Cùng một mô hình hiệu suất cũng có thể nhìn thấy trên cả hai tập dữ liệu đánh giá.

Cải thiện hiệu suất trong các lần lặp Hình 5 và 6 cho thấy hiệu suất xác thực (xác thực SQUAD) của mô hình trong quá trình tinh chỉnh qua các lần lặp. Như đã giải thích trong bài báo, khung tinh chỉnh của chúng tôi là một quy trình nhiều bước. Mô hình được huấn luyện trên dữ liệu tổng hợp tiếng Hindi trước, dữ liệu tổng hợp tiếng Tây Ban Nha thứ hai và dữ liệu được chú thích tiếng Anh như bước cuối cùng để giữ dữ liệu chất lượng cao trong các giai đoạn sau. Hai biểu đồ trên cho thấy hiệu suất xác thực mà mô hình đạt được tại mỗi bước. Một mô hình thú vị ở đây là khi hiệu suất tổng thể của mô hình tăng từ vòng 1 đến 3, nó cũng được phản ánh trong các bước riêng lẻ. Ví dụ, hiệu suất mô hình tăng trong tiếng Hindi từ vòng 1 (53.98) đến vòng 2 (56.18) cho Khớp Chính xác. Một mô hình tương tự có thể được quan sát cho các bước tiếng Tây Ban Nha và tiếng Anh cũng như cho cả biểu đồ EM và F1. Với vòng 3 là cao nhất cho tiếng Tây Ban Nha, hiệu suất trong vòng 4 và 5 kém hơn 3. Cùng một mô hình được quan sát trên hiệu suất tập kiểm tra MLQA & XQUAD của chúng tôi. Điều này cho phép hiểu rằng hiệu suất tại mỗi bước cải thiện hiệu suất của bước tiếp theo và sau đó là hiệu suất tổng thể. Vì vậy, việc có các mẫu tốt như một phần của mỗi bước là cần thiết để có được hiệu suất tối ưu.

### C Tập dữ liệu

SQUAD v1.1 (Rajpurkar et al., 2016) là một tập dữ liệu QA trích xuất bằng tiếng Anh đã được tạo ra từ hơn 107k bộ ba ngữ cảnh-câu hỏi-câu trả lời trên 536 bài báo. Những bài báo này đã được lấy từ Wikipedia và tập dữ liệu được chú thích bằng MechanicalTurk. Đây là tập dữ liệu phổ biến nhất được sử dụng bởi nhiều nghiên cứu liên quan đến các nhiệm vụ QA. Tập dữ liệu SQUAD có 2 tập con, trong đó tập con huấn luyện được sử dụng để huấn luyện mô hình cơ sở trên dữ liệu tiếng Anh được gán nhãn. Tập con xác thực được sử dụng như tập dữ liệu đánh giá cho quá trình huấn luyện mô hình XLM-R-Base (mô hình học sinh).

XQUAD (Artetxe et al., 2020) là một tập dữ liệu QA trích xuất là phiên bản đa ngữ của SQUAD. Tập dữ liệu này chỉ có 1 tập con, một tập xác thực được chú thích bởi con người gồm 1190 ví dụ trong 12 ngôn ngữ được hỗ trợ. Chúng tôi đã sử dụng 10 bản ghi đầu tiên của tập con XQUAD tiếng Hindi và tiếng Tây Ban Nha làm ví dụ được chú thích trong các nhắc ICL 1-shot của chúng tôi. 1180 bản ghi còn lại được sử dụng để so sánh hiệu suất của nhiệm vụ hạ nguồn của QA trích xuất trên các ngôn ngữ trên mô hình học sinh. Mặc dù chúng tôi chỉ sử dụng 1 ví dụ tại một thời điểm để tạo ra dữ liệu, chúng tôi đã ngẫu nhiên hóa nhắc đầu vào từ những 10 ví dụ này để thêm sự đa dạng.

XTREME (Hu et al., 2020) là một tập dữ liệu QA trích xuất đa ngữ được tạo ra bằng cách dịch SQUADv1.1 sử dụng dịch máy. Để tạo ra các cặp Q&A tổng hợp cho ngôn ngữ hi (Hindi) và es (Spanish), chúng tôi sử dụng thông tin ngữ cảnh từ phân chia translate-dev của phiên bản tập dữ liệu Xquad-Xtreme như các ngữ cảnh kiểm tra của chúng tôi, tức là các ngữ cảnh mà các cặp Q&A được tạo ra. Chúng tôi đã không sử dụng các cặp Q&A được gán nhãn có sẵn trong tập dữ liệu này cho bất kỳ mục đích huấn luyện nào của chúng tôi.

MLQA (Lewis et al., 2019) là một tập dữ liệu điểm chuẩn QA đa ngữ trích xuất đã được tạo ra thủ công. Từ các tập con cụ thể theo ngôn ngữ, chúng tôi sử dụng phân chia kiểm tra để hiểu và báo cáo hiệu suất của chúng tôi trên nhiệm vụ hạ nguồn của QA trích xuất trên các ngôn ngữ trên mô hình học sinh.
