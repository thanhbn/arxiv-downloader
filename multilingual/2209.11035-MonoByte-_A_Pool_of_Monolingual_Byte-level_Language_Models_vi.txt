# MonoByte: Một Nhóm Các Mô Hình Ngôn Ngữ Đơn Ngữ Cấp Byte

Hugo Abonizio
FEEC, UNICAMP, Brazil
NeuralMind, Brazil
hugo.abonizio@gmail.com

Leandro Rodrigues de Souza
FEEC, UNICAMP, Brazil
l231250@g.unicamp.br

Roberto Lotufo
FEEC, UNICAMP, Brazil
NeuralMind, Brazil
lotufo@unicamp.br

Rodrigo Nogueira
FEEC, UNICAMP, Brazil
NeuralMind, Brazil
rfn@unicamp.br

## Tóm tắt

Khả năng chuyển giao đa ngôn ngữ không cần giám sát của các mô hình được tiền huấn luyện trên các corpus đa ngôn ngữ và thậm chí đơn ngữ đã thúc đẩy nhiều giả thuyết để giải thích kết quả thực nghiệm hấp dẫn này. Tuy nhiên, do chi phí tiền huấn luyện cao, hầu hết các nghiên cứu sử dụng các mô hình công khai có phương pháp tiền huấn luyện như lựa chọn tokenization, kích thước corpus và ngân sách tính toán có thể khác biệt đáng kể. Khi các nhà nghiên cứu tiền huấn luyện mô hình riêng, họ thường làm điều này dưới ngân sách hạn chế, và các mô hình kết quả có thể hoạt động kém hơn đáng kể so với các mô hình SOTA. Những khác biệt thực nghiệm này đã dẫn đến các kết luận không nhất quán khác nhau về bản chất của khả năng đa ngôn ngữ của các mô hình này. Để hỗ trợ nghiên cứu thêm về chủ đề này, chúng tôi đã phát hành 10 mô hình đơn ngữ cấp byte được tiền huấn luyện nghiêm ngặt với cùng cấu hình với ngân sách tính toán lớn (tương đương 420 ngày trên V100) và các corpus lớn gấp 4 lần so với BERT gốc. Vì chúng không cần tokenizer, vấn đề về embeddings token chưa thấy được loại bỏ, do đó cho phép các nhà nghiên cứu thử nghiệm các thí nghiệm đa ngôn ngữ rộng hơn trên các ngôn ngữ có chữ viết khác nhau. Ngoài ra, chúng tôi phát hành hai mô hình được tiền huấn luyện trên văn bản không phải ngôn ngữ tự nhiên có thể được sử dụng trong các thí nghiệm kiểm tra tính hợp lý. Các thí nghiệm trên các tác vụ QA và NLI cho thấy các mô hình đơn ngữ của chúng tôi đạt được hiệu suất cạnh tranh với mô hình đa ngôn ngữ, và do đó có thể được sử dụng để tăng cường hiểu biết về khả năng chuyển giao đa ngôn ngữ trong các mô hình ngôn ngữ.

## 1 Giới thiệu

Ngay sau khi BERT (Devlin et al., 2019) được công bố, các nhà nghiên cứu đã chỉ ra rằng các mô hình đa ngôn ngữ được tiền huấn luyện trên mục tiêu mô hình hóa ngôn ngữ có mặt nạ có thể đạt được hiệu suất đa ngôn ngữ không cần giám sát đáng kể trên các tác vụ NLP khác nhau (tức là, một mô hình đa ngôn ngữ được tinh chỉnh trên ngôn ngữ có tài nguyên cao và được đánh giá trực tiếp trên các ngôn ngữ khác) (Conneau et al., 2019; Hu et al., 2020).

Những kết quả thực nghiệm này đã kích hoạt một làn sóng nghiên cứu nhằm giải thích hành vi này. Pires et al. (2019) đã đưa ra giả thuyết "anchor tokens", tức là các token được chia sẻ giữa hai ngôn ngữ hoạt động như điểm tham chiếu để các mô hình có thể học các khái niệm tương tự. Artetxe et al. (2020) và Conneau et al. (2020) đã đặt câu hỏi về những phát hiện này và chỉ ra rằng, thậm chí không có từ vựng chung, các mô hình đạt được hiệu suất chuyển giao đa ngôn ngữ tuyệt vời bằng cách chỉ tận dụng các tham số được chia sẻ.

Có lẽ thậm chí còn bất ngờ hơn là hiệu suất của các mô hình đơn ngữ trong bối cảnh đa ngôn ngữ. Ví dụ, các mô hình được tiền huấn luyện và tinh chỉnh chỉ trên tiếng Anh có thể hoạt động tốt trong các tác vụ tiếng Pháp (Oladipo et al., 2022). de Souza et al. (2021) cũng cho thấy rằng các mô hình đơn ngữ được tinh chỉnh trên ngôn ngữ nước ngoài (tức là, ngôn ngữ khác với ngôn ngữ tiền huấn luyện) đạt được kết quả tương đương với các mô hình được tinh chỉnh trên ngôn ngữ của chúng.

Nghiên cứu gần đây điều tra các thuộc tính của các corpus tiền huấn luyện góp phần vào hiệu suất của mô hình trên các tác vụ ngôn ngữ tự nhiên. Các mô hình được tiền huấn luyện trên các corpus không phải ngôn ngữ tự nhiên, như mã, âm nhạc, protein và ngôn ngữ nhân tạo và hiệu suất của chúng được so sánh với các mô hình ngôn ngữ tự nhiên (Papadimitriou and Jurafsky, 2020; Chiang and yi Lee, 2020; Lu et al., 2021; Ri and Tsuruoka, 2022). Bằng chứng cho thấy rằng tiền huấn luyện trên corpus chứa cấu trúc đệ quy hoặc phân cấp nhân tạo giữa các token dẫn đến hiệu suất tương tự khi so sánh với các mô hình được tiền huấn luyện trên ngôn ngữ tự nhiên.

Tuy nhiên, một vấn đề nổi bật là những thí nghiệm này được thực hiện hoặc 1) sử dụng các mô hình được tiền huấn luyện bởi các nhóm nghiên cứu khác nhau, có cấu hình tiền huấn luyện khác biệt rộng rãi, hoặc 2) sử dụng các mô hình được tiền huấn luyện dưới ngân sách tính toán hoặc corpus tiền huấn luyện nhỏ hơn hàng bậc độ lớn so với những mô hình được sử dụng bởi các mô hình SOTA. Ví dụ, Chiang and yi Lee (2020) tiền huấn luyện các mô hình của họ trên tập dữ liệu nhỏ hơn 200 lần so với BERT. Điều này có vấn đề vì các kỹ năng nhất định chỉ được học khi kích thước corpus và ngân sách huấn luyện đủ lớn (Zhang et al., 2020).

Những khác biệt này trong phương pháp tiền huấn luyện và các mô hình được huấn luyện thiếu làm cho việc rút ra kết luận về bản chất của khả năng đa ngôn ngữ của các mô hình như vậy trở nên khó khăn. Ngoài ra, vì hầu hết các mô hình sử dụng tokenization subword, rất khó để thí nghiệm với các ngôn ngữ không có sự chồng lấp từ vựng subword đáng kể. Ví dụ, nếu một mô hình được tiền huấn luyện và tinh chỉnh trên tiếng Anh, nó không thể được kiểm tra bằng tiếng Trung vì có rất ít sự chồng lấp token trong từ vựng của chúng, và do đó, các embeddings token tiếng Trung không được học. Đây là vấn đề thậm chí đối với các ngôn ngữ có cùng chữ viết. Ví dụ, nhiều embeddings tiếng Tây Ban Nha không được học trong tiền huấn luyện chỉ tiếng Anh, và do đó khó để biết liệu việc mô hình không thể học các biểu diễn đa ngôn ngữ chỉ là do các vấn đề tokenization (Rust et al., 2021).

Theo Bommasani et al. (2021), những người ủng hộ việc phát hành các mô hình tiền huấn luyện, corpus và script như một cách để tăng cường lĩnh vực này, chúng tôi phát hành các mô hình cấp byte được tiền huấn luyện trên các corpus lớn từ cùng một domain và sử dụng chính xác cùng thiết lập huấn luyện và ngân sách tính toán. Vì chúng chỉ dựa vào byte để biểu diễn chuỗi, chúng có thể được sử dụng để so sánh các ngôn ngữ sử dụng chữ viết khác nhau.

Mỗi mô hình mất khoảng 210 giờ tiền huấn luyện, tương đương với hơn ba tháng tính toán TPU. Các mô hình có sẵn tại https://huggingface.co/monobyte và mã được sử dụng để tinh chỉnh có thể được tìm thấy tại https://github.com/lersouza/lang-agnostic.

## 2 Nghiên cứu liên quan

Trong những năm gần đây, nhiều phiên bản đơn ngữ của BERT (Devlin et al., 2019; Souza et al., 2020; Chan et al., 2020; Martin et al., 2020; Antoun et al., 2020; Lee et al., 2021; Nguyen and Tuan Nguyen, 2020; Cañete et al., 2020; Bhattacharjee et al., 2021) và T5 (Raffel et al., 2020; Carmo et al., 2020; Sarti and Nissim, 2022) đã được phát hành. Các tác giả thường tuyên bố rằng các phiên bản này vượt trội hơn các mô hình đa ngôn ngữ trong ngôn ngữ của chúng. Rust et al. (2021) đã chỉ ra rằng tokenizer đóng vai trò quan trọng trong việc đạt được những kết quả đó. Gần đây hơn, Xue et al. (2021a) đã phát hành mô hình ByT5, có thể khắc phục vấn đề này bằng cách tận dụng từ vựng cấp byte. Mô hình này được tiền huấn luyện trên nhiều ngôn ngữ và đạt được kết quả tuyệt vời khi so sánh với mT5 (Xue et al., 2021b). Tuy nhiên, không có phiên bản đơn ngữ nào của ByT5 được phát hành, điều này làm cho việc tiến hành điều tra thêm về tiền huấn luyện và hiệu suất đa ngôn ngữ của mô hình không bị ảnh hưởng bởi tokenization trong các ngôn ngữ khác nhau trở nên khó khăn.

Nghiên cứu hiện tại về các thuộc tính của tiền huấn luyện thường dựa vào các mô hình đơn ngữ hoặc các mô hình được tiền huấn luyện trên ngôn ngữ nhân tạo. Các nhà nghiên cứu thường dựa vào các mô hình được phát hành bởi các nhóm khác (de Souza et al., 2021) hoặc tiền huấn luyện các mô hình trong thiết lập được kiểm soát rất chặt chẽ (Artetxe et al., 2020; Papadimitriou and Jurafsky, 2020; Fujinuma et al., 2022). Trong cả hai trường hợp, không rõ liệu kết quả có bị ảnh hưởng bởi những phương pháp khác biệt này hay các mô hình được huấn luyện thiếu. Blevins and Zettlemoyer (2022), chẳng hạn, gợi ý rằng ô nhiễm ngôn ngữ (tức là, corpus đơn ngữ bao gồm các câu trong nhiều ngôn ngữ hơn) là lý do cho hiệu quả không cần giám sát của đơn ngữ. Điều tương tự xảy ra với các mô hình được tiền huấn luyện trên corpus ngôn ngữ không tự nhiên (Chiang and yi Lee, 2020; Lu et al., 2021; Ri and Tsuruoka, 2022), nơi kích thước corpus khác với những mô hình được huấn luyện trên ngôn ngữ tự nhiên, có thể ảnh hưởng đến các kết luận rút ra từ các thí nghiệm. Chúng tôi hy vọng thu hẹp khoảng cách này bằng cách phát hành bộ các mô hình đơn ngữ được trình bày trong bài báo này.

## 3 Tiền huấn luyện

Để huấn luyện các mô hình đơn ngữ của chúng tôi, chúng tôi đã sử dụng mC4 (Xue et al., 2021b), cùng corpus được sử dụng để huấn luyện mT5 và ByT5, bao gồm 101 ngôn ngữ tự nhiên được trích xuất từ Common Crawl web scrape. Đối với mỗi mô hình, chúng tôi chỉ chọn các tài liệu được viết bằng ngôn ngữ cụ thể, ban đầu được xác định bằng cld3 bởi Xue et al. (2021b). mC4 là một corpus lớn với các phân phối ngôn ngữ khác nhau, với một số được đại diện nhiều hơn những ngôn ngữ khác. Do đó, chúng tôi đã cắt mỗi corpus tiền huấn luyện ở khoảng 65 tỷ byte UTF-8 cho tất cả các ngôn ngữ. Ngoại lệ duy nhất là tiếng Bengali, được cắt ở 32 tỷ byte, tương ứng với tổng kích thước của nó.

Tiền huấn luyện được thực hiện trên TPU VM v3-8 sử dụng thư viện Flax (Heek et al., 2020) và script tiền huấn luyện cho mô hình hóa ngôn ngữ có mặt nạ span giống T5 có sẵn trên thư viện HuggingFace Transformers. Chúng tôi chọn kiến trúc nhỏ hơn của ByT5, với 300 triệu tham số, và sử dụng các siêu tham số tương tự như được báo cáo trong các bài báo ByT5 (Xue et al., 2021a) và mT5 (Xue et al., 2021b). Chúng tôi không thí nghiệm với các mô hình lớn hơn do chi phí tính toán của chúng. Chúng tôi đặt độ dài chuỗi là 1024 token (byte UTF-8) và huấn luyện trong 1 triệu bước với batch 2^16 byte, dẫn đến khoảng 65 tỷ byte. Vì chúng tôi chỉ có thể vừa batch 8 ví dụ trên mỗi thiết bị, chúng tôi đã sử dụng tích lũy gradient của 128 bước để đạt được kích thước batch lớn hơn. Tốc độ học bắt đầu ở 10^-3 và giảm tuyến tính trong suốt quá trình tiền huấn luyện.

So với ByT5 gốc, các mô hình của chúng tôi được tiền huấn luyện trên corpus nhỏ hơn 16 lần. Tuy nhiên, corpus tiền huấn luyện của chúng tôi lớn hơn 4 lần so với BERT, được huấn luyện trên 3,300M từ (Devlin et al., 2019) — tức là, khoảng 16 tỷ ký tự, xem xét trung bình 5 ký tự cho mỗi từ trong tiếng Anh (Bochkarev et al., 2015). So với GPT-2, corpus của chúng tôi lớn hơn 1.6 lần (Radford et al., 2019).

So với nghiên cứu mô hình đơn ngữ khác, Rust et al. (2021) tiền huấn luyện trên corpus khoảng 77GB, theo số bước và token trên batch được báo cáo. Ri and Tsuruoka (2022) tiền huấn luyện trên corpus 1.2GB văn bản được tạo nhân tạo. Một so sánh mở rộng hơn với các công trình khác được trình bày trong Bảng 1 và chứng minh rằng tiền huấn luyện của chúng tôi có thể so sánh với các mô hình đơn ngữ SOTA và lớn hơn nhiều so với các corpus được sử dụng trong phần lớn các nghiên cứu đơn ngữ.

Để kiểm tra tính hợp lý và xác thực hiệu suất của các mô hình tiền huấn luyện của chúng tôi, chúng tôi đã so sánh kết quả của chúng với các baseline khác nhau được đề xuất trong tài liệu. Chiến lược tiền huấn luyện tổng hợp đầu tiên, được gọi là nesting parentheses, được đề xuất bởi Papadimitriou and Jurafsky (2020), tạo ra đệ quy các ký hiệu ngẫu nhiên tôn trọng cấu trúc phân cấp. Điều này được đề xuất để đánh giá cách đệ quy này chuyển giao vào việc mô hình hóa ngôn ngữ thực. Công trình của họ cho thấy rằng các mô hình được tiền huấn luyện trên corpus phân cấp tổng hợp có thể dự đoán ngôn ngữ con người tốt hơn nhiều so với các baseline khác (ví dụ, ngẫu nhiên, âm nhạc và mã), với perplexity có thể so sánh với ngôn ngữ thực.

Chúng tôi đã sử dụng mã được phát hành bởi các tác giả, với từ vựng là 50,000 từ thường xuyên nhất dựa trên một triệu tài liệu đầu tiên của corpus mC4 tiếng Tây Ban Nha, và xác suất lấy mẫu của chúng dựa trên phân phối Zipf trên cùng corpus tiếng Tây Ban Nha. Chúng tôi đã tạo ra các ví dụ cho đến khi đạt được cùng kích thước với kích thước corpus ngôn ngữ tự nhiên của chúng tôi để thực hiện so sánh công bằng.

Baseline tiền huấn luyện thứ hai được đề xuất bởi Krishna et al. (2021), được gọi là nonsense, nơi các tác giả khám phá giả thuyết chuyển giao kiến thức của cải thiện hiệu suất downstream của transfer learning. Các tác giả tiền huấn luyện trên các tài liệu bao gồm n-gram ngẫu nhiên và đạt được hiệu suất tương tự trong tóm tắt trên các tập dữ liệu khác nhau khi so sánh với tiền huấn luyện trên ngôn ngữ thực. Chúng tôi đã tạo ra các ví dụ sử dụng mã gốc để phù hợp với cùng kích thước corpus của các mô hình ngôn ngữ tự nhiên của chúng tôi.

## 4 Thí nghiệm

Các mô hình của chúng tôi được đánh giá trên hai tác vụ downstream: Natural Language Inference (NLI) và Question Answering (QA). Chúng tôi sử dụng thiết lập tương tự như In-language model, được mô tả bởi Hu et al. (2020), nơi mô hình được tinh chỉnh và đánh giá trong tập con của tác vụ tương ứng với ngôn ngữ tiền huấn luyện của nó. Các phần sau cung cấp chi tiết về các tập dữ liệu được sử dụng và kết quả. Các mô hình đơn ngữ được so sánh với checkpoint ByT5 Small. Checkpoint sau được mong đợi hoạt động tốt hơn vì nó được tiền huấn luyện trên một tập hợp lớn hơn nhiều của corpus đa dạng hơn, như được điều tra bởi Fujinuma et al. (2022). Để biết thêm thông tin về quy trình tinh chỉnh, vui lòng tham khảo Phụ lục A.

**Natural Language Inference.** Tập dữ liệu XNLI (Conneau et al., 2018) được sử dụng để tinh chỉnh và đánh giá các mô hình tiếng Đức, tiếng Anh, tiếng Tây Ban Nha, tiếng Nga, tiếng Việt và tiếng Trung của chúng tôi. Đối với mô hình tiếng Bồ Đào Nha, chúng tôi chọn tác vụ Recognizing Textual Entailment (RTE) của tập dữ liệu ASSIN2 (Real et al., 2020). Hiệu suất được đo bằng metric độ chính xác.

Kết quả được báo cáo trong Bảng 2. So với ByT5 đa ngôn ngữ, các mô hình của chúng tôi đạt được hiệu suất cạnh tranh. Sự khác biệt về độ chính xác là khoảng 1.28 trung bình. Kết quả ở tiếng Nga thể hiện khoảng cách lớn nhất (6.19 điểm), trong khi ở tiếng Việt thể hiện khoảng cách nhỏ nhất (2.64 điểm). Nonsense đạt được hiệu suất gần ngẫu nhiên, trong khi Hierarchical tiếp cận các mô hình được tiền huấn luyện trên ngôn ngữ tự nhiên, như cũng được chứng minh bởi Papadimitriou and Jurafsky (2020) và Chiang and yi Lee (2020).

**Question Answering.** Phiên bản gold passage của tập dữ liệu TydiQA (Clark et al., 2020) (TydiQA-GoldP) được chọn để tinh chỉnh và đánh giá các mô hình tiếng Ả Rập, tiếng Bengali, tiếng Anh, tiếng Hàn và tiếng Nga. Để tinh chỉnh mô hình tiếng Bồ Đào Nha, chúng tôi sử dụng FaQuAD (Sayama et al., 2019). Hiệu suất được đo bằng metric F1 Score như được mô tả bởi Rajpurkar et al. (2016).

Kết quả được báo cáo trong Bảng 3. Đối với tiếng Bồ Đào Nha, mô hình của chúng tôi vượt trội hơn checkpoint Đa ngôn ngữ 22.51 điểm, trong khi, đối với tiếng Bengali, nó tụt lại 16.39 điểm. Các mô hình đơn ngữ của chúng tôi tụt lại, trung bình, 4 điểm so với mô hình đa ngôn ngữ được phát hành bởi Google. Nhìn vào sự khác biệt giữa các mô hình đơn ngữ và đa ngôn ngữ cho mỗi ngôn ngữ, chúng ta thấy biến động cao hơn. Kết quả Nonsense và Hierarchical rất xa. Question answering là một tác vụ khó hơn (Vania et al., 2021) và là một benchmark tốt cho chất lượng mô hình. Chúng tôi giả thuyết rằng sự khác biệt về độ phức tạp của tác vụ này đòi hỏi nhiều hơn từ mô hình. Tiền huấn luyện chỉ với việc nhấn mạnh cấu trúc token có thể không cung cấp kiến thức cần thiết cho hiệu suất tốt.

## 5 Kết luận

Trong công trình này, chúng tôi đã giới thiệu một nhóm gồm 10 mô hình ngôn ngữ không cần tokenizer được tiền huấn luyện trên các corpus đơn ngữ lớn. Chúng tôi đã chứng minh rằng các mô hình đơn ngữ của chúng tôi có thể đạt được kết quả cạnh tranh với ByT5 đa ngôn ngữ, mặc dù có tiền huấn luyện nhỏ hơn và ít đa dạng hơn — đơn ngữ, so với tất cả các ngôn ngữ được nối với nhau.

Mục tiêu chính của công trình của chúng tôi là phát hành một bộ các mô hình đơn ngữ tuân thủ cẩn thận cùng phương pháp tiền huấn luyện để hỗ trợ nghiên cứu về các mô hình ngôn ngữ đơn ngữ, thuộc tính tiền huấn luyện và tính đa ngôn ngữ. Chúng tôi hy vọng rằng bằng cách phát hành các mô hình của mình, chúng tôi thu hẹp khoảng cách này của việc có thiết lập được kiểm soát và có thể tái tạo hơn cho các thí nghiệm nghiêm ngặt.

## Lời cảm ơn

Nghiên cứu này được tài trợ một phần bởi các grant 2020/09753-5 và 2022/01640-2 từ Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP). Chúng tôi cũng muốn cảm ơn Google Cloud vì các khoản tín dụng để hỗ trợ công trình này.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được duy trì như bản gốc]

## Phụ lục A: Chi tiết Tinh chỉnh

Các thí nghiệm được thực hiện trên một GPU NVIDIA A100 80GB duy nhất. Chúng tôi sử dụng optimizer Adafactor (Shazeer and Stern, 2018) với tốc độ học không đổi 10^-4. Các mô hình được tinh chỉnh bằng ba seed khác nhau, và chúng tôi báo cáo trung bình của các kết quả. Chúng tôi đã chọn siêu tham số dựa trên các thí nghiệm sơ bộ với checkpoint ByT5 Small trên cả tập dữ liệu XNLI và TydiQA. Chúng tôi không thực hiện tìm kiếm siêu tham số toàn diện, và sử dụng cùng cài đặt (theo tác vụ) cho tất cả các mô hình.

**Natural Language Inference.** Đối với tất cả các thí nghiệm NLI, chúng tôi sử dụng kích thước batch 16, tích lũy gradient cho 4 bước. Độ dài đầu vào tối đa là 1024, được cắt theo batch. Chúng tôi huấn luyện mô hình để xuất ra bộ nhận diện lớp (một số). Chúng tôi huấn luyện trong 10 epoch và đánh giá mỗi 0.2 epoch. Chúng tôi cũng thực hiện early stopping với patience 5 lần đánh giá và chọn mô hình tốt nhất trên tập validation của mỗi tác vụ. Các kết quả được báo cáo trên tập test của mỗi tập dữ liệu.

**Question Answering.** Đối với QA, kích thước batch được chọn là 6, tích lũy gradient cho 4 bước. Độ dài đầu vào tối đa là 2048 (câu hỏi và context được nối), được cắt theo batch. Chúng tôi huấn luyện mô hình để xuất ra câu trả lời cho câu hỏi với độ dài tối đa 768 byte. Chúng tôi huấn luyện trong 10 epoch và đánh giá ở cuối mỗi epoch. Chúng tôi cũng thực hiện early stopping với patience 3 và chọn mô hình tốt nhất trên tập validation của mỗi tác vụ.
