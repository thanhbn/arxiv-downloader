# 2309.10654.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2309.10654.pdf
# File size: 1405718 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CFGPT: Chinese Financial Assistant with Large Language Model
Jiangtong Li1, Yuxuan Bian1, Guoxuan Wang1, Yang Lei1, Dawei Cheng1,2,
Zhijun Ding1,2, Changjun Jiang1,2
1Department of Computer Science and Technology, Tongji University, Shanghai, China.
2Shanghai Artificial Intelligence Laboratory, Shanghai, China.
Abstract
Large language models (LLMs) have demonstrated great po-
tential in natural language processing tasks within the fi-
nancial domain. In this work, we present a Chinese Finan-
cial Generative Pre-trained Transformer framework, named
CFGPT, which includes a dataset (CFData) for pre-training
and supervised fine-tuning, a financial LLM (CFLLM) to
adeptly manage financial texts, and a deployment frame-
work (CFAPP) designed to navigate real-world financial ap-
plications. The CFData comprising both a pre-training dataset
and a supervised fine-tuning dataset, where the pre-training
dataset collates Chinese financial data and analytics, along-
side a smaller subset of general-purpose text with 584M doc-
uments and 141B tokens in total, and the supervised fine-
tuning dataset is tailored for six distinct financial tasks, em-
bodying various facets of financial analysis and decision-
making with 1.5M instruction pairs and 1.5B tokens in to-
tal. The CFLLM, which is based on InternLM-7B to balance
the model capability and size, is trained on CFData in two
stage, continued pre-training and supervised fine-tuning. The
CFAPP is centered on large language models (LLMs) and
augmented with additional modules to ensure multifaceted
functionality in real-world application. Our codes are re-
leased at https://github.com/TongjiFinLab/CFGPT1.
1 Introduction
In recent years, pre-trained language models (PLMs) have
garnered significant attention in both industrial and research
arenas, emerging as a key in natural language processing and
interactive artificial intelligence (Devlin et al. 2018; Radford
et al. 2018; Brown et al. 2020; Raffel et al. 2020; Ouyang
et al. 2022). The public unveiling of GPT-3.5 and GPT-4
in 2022 marked a significant advancement, as these large
language models (LLMs) showcased unparalleled perfor-
mance in a range of tasks, from reading comprehension and
open-ended question answering to code generation (OpenAI
2021, 2023). Notably, these LLMs (OpenAI 2021; Du et al.
2022; OpenAI 2023) exhibit profound competencies in nat-
ural language understanding (NLU) and can execute a vari-
ety of tasks by following natural language instructions with-
out the need for training data. Despite these successes, the
intricacies of financial texts pose challenges that demand
1Correspondence to dcheng@tongji.edu.cndomain-specific LLMs for effective comprehension of so-
phisticated financial language and concepts. To this end, sev-
eral financial large language models (FinLLMs) have been
developed, including FinGPT (Yang, Liu, and Wang 2023;
Zhang, Yang, and Liu 2023), PIXIU (Xie et al. 2023), and
BloombeergGPT (Wu et al. 2023).
There are also some early endeavors toward the Chi-
nese financial large language model. (Zhang and Yang 2023;
Zhang, Yang, and Xu 2023; Lu et al. 2023; Yu 2023). For
instance, BBT-FinT5 (Lu et al. 2023) fine-tuned T5 (Raffel
et al. 2020) using the masked language model task (MLM)
and the knowledge-enhanced triple mask task (KETM). Xu-
anYuan2.0 (Zhang and Yang 2023; Zhang, Yang, and Xu
2023), on the other hand, effectively fine-tuned BLOOM-
176B (Scao et al. 2022) with both general and domain-
specific datasets, showcasing prowess in general and finan-
cial question-answering tasks. Cornucopia (Yu 2023) further
innovated by supervised fine-tuning LLaMA (Touvron et al.
2023a,b), enhancing its reasoning abilities within the Chi-
nese financial domain. Despite the substantial achievements
of LLMs in the financial domain, their application to the
Chinese financial context not abundantly studied.
In this work, we introduce a Chinese Financial Genera-
tive Pre-trained Transformer framework, named CFGPT, in-
cluding a dataset for pre-training and supervised fine-tuning,
a financial LLM to adeptly manage financial texts, and a
framework designed to navigate real-world financial appli-
cations. We begin by constructing the CFData, comprising
both a pre-training dataset and an supervised fine-tuning
dataset. The pre-training dataset collates an extensive as-
sortment of Chinese financial data and analytics, alongside a
smaller subset of general-purpose text. In aggregate, it con-
tains 584M documents and 141B tokens, encompassing an-
nouncements, research reports, social media content, finan-
cial news articles, and entries from Wikipedia. In contrast,
the supervised fine-tuning dataset is tailored for six distinct
financial tasks, embodying various facets of financial analy-
sis and decision-making. It consists of 1.5M instruction pairs
and 1.5B tokens, covering areas such as sentiment analysis,
event detection, report summarization, topic decomposition,
question answering, and stock movement prediction. We
chose InternLM-7B as our base model in initial phrase. Our
framework also support most of LLM as base model. Lever-
aging CFData, our CFLLM undergoes a two-stage trainingarXiv:2309.10654v2  [cs.CL]  22 Sep 2023

--- PAGE 2 ---
regimen: continued pre-training and supervised fine-tuning.
This approach aims to amplify its zero-shot and few-shot
performance in Chinese financial tasks.
We conducted an exhaustive analysis of the critical needs
in the Chinese financial domain, leading to the creation of
our CFAPP framework. Designed for real-world applica-
tions, this framework is centered on large language models
(LLMs) and augmented with additional modules to ensure
multifaceted functionality. A standout feature of CFAPP is
its adaptability to diverse input formats—ranging from text
and audio to PDF files, enabling users to engage with the
system in their preferred manner and enhancing its versa-
tility and user-centricity. To guarantee the veracity and pre-
cision of responses, our framework incorporates several in-
teraction modules: vector databases, a chain-of-thought sys-
tem, and domain-specific model. Beyond its input flexibility,
CFAPP also offers varied output formats, such as raw text,
templated text, and mind maps. Collectively, these integra-
tive features position our framework as a pioneering solution
in the Chinese financial sector, adeptly catering to its unique
demands.
2 Related Work
In this section, we will discuss related works from three as-
pects, the financial dataset, the financial language models,
the financial evaluation benchmarks.
2.1 Financial Dataset
Financial datasets typically encompass two primary cate-
gories: text datasets and structured datasets. In this section,
our attention is centered on the text datasets, specifically,
those comprising textual content from the financial sector,
devoid of any accompanying tables or figures. Historically,
the primary focus of such datasets has been on news and
social media text, offering valuable insights for informed
decision-making. For instance, Zhang and Skiena (2010)
capitalized on the sentiment discernible in blogs and news
articles to predict stock prices. Similarly, both Ding et al.
(2014) and Liang et al. (2020) harnessed news articles, trans-
forming them into structured event relations as a means to
predict stock movements. Further explorations by Liu et al.
(2021) and Cheng et al. (2022) delved into extracting latent
features from raw textual data, using this auxiliary informa-
tion to bolster the training of trading systems.
With the popularity of LLMs, there has been a no-
table shift in research focus, from being model-centric
to data-centric, which magnified the importance of tex-
tual data within the financial domain. For instance,
BloombergGPT (Wu et al. 2023) integrates both general
and financial textual data to train FinLLMs from the ground
up. Similarly, models such as PIXIU (Xie et al. 2023) and
FinGPT (Yang, Liu, and Wang 2023; Zhang, Yang, and
Liu 2023) curate instruction datasets. These datasets re-
frame tasks like sentiment analysis, named entity detec-
tion, and stock price prediction within a question-answering
paradigm, facilitating supervised fine-tuning for financial
applications. Focusing on the Chinese financial domain,
BBT-Fin (Lu et al. 2023) introduced the BBT-FinCorpus,which aggregates roughly 3B tokens from sources like cor-
porate reports, research reports, social media, and finan-
cial news. Meanwhile, XuanYuan 2.0 (Zhang, Yang, and Xu
2023; Zhang and Yang 2023) offers a hybrid-tuning dataset
combining 380 tokens from both general and financial do-
mains for pre-training and supervised fine-tuning, although
this dataset remains proprietary. Cornucopia (Yu 2023) em-
phasizes supervised fine-tuning, constructing a dataset with
26M instruction pairs covering areas such as insurance, in-
vestment, stocks, funds, loans, and credit cards. Recognizing
the crucial role of data quality in crafting efficient FinLLMs,
we introduce the CFData corpus. It encompasses 584M doc-
uments with 141B tokens for pre-training and an additional
1.5M instruction pairs with 1.5B tokens explicitly designed
for supervised fine-tuning.
2.2 Financial Language Models
Before the integration of LLMs into the financial sector,
the focus of pre-trained language models largely revolved
around continued pre-training using financial texts. The
adaptability of BERT (Devlin et al. 2018) to the financial
domain was demonstrated when Araci (2019) and Yang, Uy,
and Huang (2020) pre-trained it on English finance news
and communications. The results significantly surpassed
competitive baselines in financial sentiment analysis tasks.
As for Chinese, Mengzi-fin (Zhang et al. 2021) and BBT-
FinT5 (Lu et al. 2023) were trained with analogous tasks
and achieved improvements in multiple financial tasks.
The advent of GPT (OpenAI 2021, 2023) spurred a
heightened interest in FinLLMs. BloombergGPT (Wu et al.
2023) stands out as a pioneering FinLLM, boasting 50 bil-
lion parameters and trained entirely from scratch. Its per-
formance metrics, particularly under zero-shot and few-shot
scenarios, further underscored the need for domain-specific
training. Successor models like FinGPT (Yang, Liu, and
Wang 2023; Zhang, Yang, and Liu 2023) and PIXIU (Xie
et al. 2023) delved deeper into supervised fine-tuning, and
their outcomes surpassed BloombergGPT and other generic
LLMs (Black et al. 2022; Du et al. 2022) in zero-shot or few-
shot configurations, thereby accentuating the efficacy of su-
pervised fine-tuning in enhancing in-context learning. Turn-
ing our attention to the Chinese financial domain, models
such as XuanYuan2.0 (Zhang and Yang 2023; Zhang, Yang,
and Xu 2023) showcased prowess in both generic and finan-
cial question-answering after fine-tuning on domain-specific
datasets. Cornucopia (Yu 2023) advanced supervised fine-
tuning techniques with LLaMA (Touvron et al. 2023a,b),
elevating reasoning capabilities of LLaMA in the Chinese
financial domain.
2.3 Financial Evaluation Benchmarks
The inception of financial evaluation benchmarks, FLUE,
was heralded by Shah et al. (2022). This benchmark com-
prised five tasks: sentiment analysis (Malo et al. 2014),
news headline classification (Sinha and Khandait 2021),
named entity recognition (Alvarado, Verspoor, and Baldwin
2015), structure boundary detection, and question answer-
ing (Maia et al. 2018). Recognizing a gap for more com-
prehensive financial applications within FLUE, Xie et al.

--- PAGE 3 ---
Dataset # Docs ×103# Chars ×106# Tokens ×106Chars/Doc Tokens/Doc % Token Storage (GB)
Pretraining 583,978 206,665 140,609 354 241 100.00 573.2
CFData-CP 39.1 13,357 8,788 341,423 225,330 6.24 37.0
CFData-CA 6,193 31,120 17,272 5025 2789 12.28 86.3
CFData-RR 392 5,027 3,529 12,826 9,003 2.51 13.9
CFData-FN 82,438 37,262 26,297 452 319 18.70 103.3
CFData-SM 494,661 119,708 84,587 242 171 60.15 332.0
CFData-Wiki 255 191 137 750 537 0.09 0.5
supervised fine-tuning 1,572 2,042 1,512 1,299 962 100.00 5.66
CFData-SA 120 118 86 982 711 5.69 0.33
CFData-ED 490 461 343 941 701 22.69 1.28
CFData-TD 369 266 187 721 507 12.37 0.74
CFData-RS 369 1,014 765 2,751 2,076 50.60 2.81
CFData-QA 12 8 6 648 470 0.39 0.02
CFData-SP 212 175 125 822 588 8.27 0.48
Table 1: Detailed statistics about the pretraining dataset and the supervised fine-tuning dataset. # Docs, # Chars, and # Tokens
indicate the number of documents, characters and tokens in each sub-dataset. Chars/Doc, Tokens/Doc indicate the number of
characters and tokens per document in each sub-dataset. % Token indicates the percentage of the overall tokens of each sub-
dataset. Storage (GB) indicates the storage occupation of each sub-dataset in terms of gigabyte (GB).
(2023) augmented the benchmark by adding a stock move-
ment prediction task, resulting in the FLARE benchmark.
As for the Chinese financial domain, Lu et al. (2023) pi-
oneered the BBT-CFLEB benchmark, encompassing six
tasks: news classification, summarization, relation extrac-
tion, question answering, negative news determination, and
sentiment analysis. Recently, Zhang et al. (2023) presented
FinEval, a benchmark specifically designed for the financial
domain knowledge in the LLMs, including 4,661 multiple-
choice questions covering finance, economy, accounting,
and certificate.
3 Datasets
In this section, we present the CFData datasets for pre-
training and supervised fine-tuning.
3.1 Overview
In this section, we present the CFData for pre-training and
supervised fine-tuning. The CFData can be described from
two perspectives: 1) Pre-training Dataset: The dataset en-
compasses a large amount of Chinese financial data and
analytics and a small amount of general-purpose text for
pre-training, such as announcements, research reports, so-
cial media content, financial news articles, and Wikipedia.
2) supervised fine-tuning Dataset: The dataset includes six
financial tasks to reflect different aspects of financial anal-
ysis and decision-making, which include sentiment analy-
sis, event detection, report summarization, topic decompo-
sition, question answering, and stock movement prediction.
CFData provides much text information in the financial do-
main, allowing a FinLLM to learn from different of sources.
In Table 1, we provide the detailed statistics about our CF-
Data.
The pre-training dataset consists of 591 million docu-
ments and 193 billion tokens, including six sub-datasets, 1)
CFData-CP (6.24%): 39 thousand corporate prospectus with13 billion tokens; 2) CFData-CA (12.28%): 6 million cor-
porate announcements with 17 billion tokens; 3) CFData-
RR (2.51% ): 392 thousand research reports with 3 billion
tokens; 4) CFData-FN (18.70%): 82 million financial news
with 26 billion tokens; 5) CFData-SM (60.15%): 495 mil-
lion social medias and 84 billion tokens; 6) CFData-Wiki
(0.09%): 255 thousand Wikipedia content with 137 mil-
lion tokens. More details about the data source and the pre-
processing of each sub-dataset are in Section 3.2 and Fig-
ure 1.
The supervised fine-tuning dataset consist 1.6 million in-
structions pairs and 1.5 billion tokens, including six finan-
cial tasks, 1) CFData-SA (5.69% ): 120 thousand instances
with 86 million tokens for sentiment analysis; 2) CFData-
RS (50.60%): 369 thousand instances and 765 million to-
kens for report summary; 3) CFData-ED (22.69% ): 490
thousand instances with 343 million tokens for event de-
tection; 4) CFData-TD (12.37%): 369 thousand instances
and 187 million tokens for topic decomposition; 5) CFData-
QA (0.39%): 12 thousand instances and 6 million tokens for
question-answering; 6) CFData-SP (8.27%): 212 thousand
instances and 125 million tokens for stock moving predic-
tion. More details about the data source and the task con-
struction of each task are in Section 3.3.
3.2 Data for Pre-training
In this section, we have gathered additional Chinese finan-
cial documents to facilitate pre-training. This dataset com-
prises of financial documents, including corporate prospec-
tuses, corporate announcements, research reports, social me-
dia content, financial news articles, and the Wikipedia con-
tent. We used a proxy-based distributed crawler to crawl
public web pages to get these documents through the
API from CFData2. Moreover, we follow the similar pre-
processing steps in (Zeng et al. 2021) and (Lu et al. 2023)
2https://github.com/TongjiFinLab/CFGPT-dataset

--- PAGE 4 ---
CFData-CP&CA&RR
PDF Documents
PDFMiner
Table, Figure, Garbled
Characters Filter
Raw Text Documents
Length Selector
Final DocumentsCFData-FN
HTML  Pages
HTML  Extractor
Raw Text Articles
LSH Filter
Length Selector
Final ArticlesCFData-SM
HTML  Pages
HTML  Extractor
LSH Filter
Length Selector
Final PostsBanned Words Filter
Garbled Characters Filter
Raw Text PostsCFData-W iki
Dumped Package
HTML  Extractor
Length Selector
Final DocumentsTraditional Chinese Converter
Simplified Chinese DocumentsRaw Text DocumentsFigure 1: The preprocess steps of each sub-dataset in CFData pre-training dataset.
to clean the dataset. The inclusion of these documents aims
enabling the LLMs to better grasp the intricacies of financial
concepts and language within the Chinese context. Com-
pared with BloombergGPT (Wu et al. 2023), CFData also
consists more long (tokens pre document) financial docu-
ments for pre-training, especially in corporate prospectus,
corporate announcements, and research report.
CFData-CP Corporate prospectuses are critical legal doc-
uments prepared by companies to provide information to po-
tential investors. They typically include details about the fi-
nancial status, business model, strategies, risks, management
team, legal proceedings, regulatory compliance, and the uti-
lization of funds by the respective company. Considered the
most important and official document in the financial mar-
ket, corporate prospectuses play a significant role in invest-
ment decisions. To construct this sub-dataset, we crawl the
corporate prospectuses from Shanghai Stock Exchange web-
site3and Shenzhen Stock Exchange website4spanning the
years from 2012 to 2022, where we got 43,371 documents
in PDF format initially. To process the corporate prospec-
tuses for pre-training, we first convert the PDF documents
into raw text format through the PDFMiner package5. Then,
we remove all figures, tables, and garbled characters present
in the documents through regular expression. Additionally,
we filter out documents with a length smaller than 10,000
characters. After these preprocessing steps, the CFData-CP
sub-dataset consists of 39.1 thousand corporate prospectuses
with a total of 8.79 million tokens.
CFData-CA Corporate announcements are official finan-
cial statements released by listed companies to provide in-
formation to the general public. These announcements can
3http://www.sse.com.cn/disclosure/overview/
4http://www.szse.cn/disclosure
5https://pypi.org/project/pdfminer/be categorized into two types: regular announcements, such
as annual and quarterly reports, and irregular announce-
ments that address or disclose unexpected events. To con-
struct this sub-dataset, we crawl the corporate prospectuses
from listed companies in Shanghai Stock Exchange website
and Shenzhen Stock Exchange website spanning the years
from 2012 to 2022, where we got 9,389,193 documents in
PDF format initially. We following the similar operation
as CFData-CP to process the corporate announcements, in-
cluding, converting the PDF documents into raw text format
through the PDFMiner, removing all figures, tables, and gar-
bled characters present in the documents through regular ex-
pression, and filtering out documents with a length smaller
than 1,000 characters. Finally, the CFData-CA sub-dataset
consists of 6.19 million corporate announcements with a to-
tal of 17.3 billion tokens.
CFData-RR Research reports are specialized documents
that concentrate on macroeconomic issues, sectors, indus-
tries, and stocks. They are typically issued by investment
institutions and aim to analyze the current status and fu-
ture development trends of the aforementioned topics. These
reports not only contains the unique perspectives and in-
sights of the analysts but also provide the analyzing step
toward their perspectives and insights, making them im-
portant to reflect the logic of analysts in financial domain.
To construct this sub-dataset, we crawl the research report
from Eastmoney website6spanning the years from 2016 to
2022, where we got 683,294 documents in PDF format ini-
tially. We following the similar operation as CFData-CP and
CFData-CA to process the research report, including, con-
verting the PDF documents into raw text format through the
PDFMiner, removing all figures, tables, and garbled char-
acters present in the documents through regular expression,
and filtering out documents with a length smaller than 2,000
6https://data.eastmoney.com/report/

--- PAGE 5 ---
characters. Finally, the CFData-RR sub-dataset consists of
392 thousand research report with a total of 3.53 billion to-
kens.
CFData-FN Financial news are the news articles related
to financial market with financial jargon and acronyms,
ranging from stock market updates, company earnings re-
ports, mergers and acquisitions, economic data releases,
central bank decisions, government policies, exchange rates,
interest rates, and so on. To construct the sub-dataset with
high-quality financial news in terms of factual and unbiased
information, we crawl the financial news from reputable
Chinese financial websites, including Sina Finance7, Ten-
cent Finance8, Phoenix Finance9, 36Kr10, and Cailianshe11,
spanning the years from 2018 to 2022. We crawl a total
of 121,273,632 Chinese financial news in HTML initially,
where there are about 61, 66, 63, 68, 71 thousand news pre
day in 2018, 2019, 2020, 2021 and 2022, respectively. To
process the financial news for pre-training, we first extract
the news articles into raw text through regular expression.
Then we implement the locality-sensitive hashing (Datar
et al. 2004) (LSH) algorithm to filter out the redundant or
duplicated news across multiple sources, resulting in a di-
verse and non-repetitive dataset. Moreover, we remove the
financial news whose length are smaller than 100 characters.
Finally, the CFData-FN sub-dataset consists of 82.4 million
news articles, comprising a total of 26.3 billion tokens.
CFData-SM Social media contents are usually posted by
individuals to represent their viewpoints, which is quite im-
portant for a FinLLM to comprehend the elementary in-
vestors in financial market. There are various platforms for
individual investors to discuss the financial market or release
the posts, with Xueqiu12, Guba13, and Weibo14being the
most active ones. To construct this sub-dataset, we crawl the
posts from these three platforms spanning the years from
2018 to 2022, where only the posts mentioned sectors, in-
dustries, stocks, futures, and options will be crawled. We
crawl a total of 20,838,204,735 posts in HTML initially,
where there are about 11 millions news pre day. However,
it is important to note that the posts from social media plat-
forms may not always represent reputable sources and could
potentially contain biased perspectives or garbled charac-
ters. To address these concerns, we implement certain mea-
sures in our data filter process.
• We extract posts into raw text through regular expression;
• We utilize banned word dictionary from (Zeng et al.
2021) to filter out the posts that exhibit toxicity, identity
attacks, insults, threats, profanity, and sexually explicit;
• We remove posts with garbled characters that comprised
7https://finance.sina.com.cn/
8https://new.qq.com/ch/finance/
9https://finance.ifeng.com/
10https://36kr.com/
11https://www.cls.cn
12https://xueqiu.com/
13https://guba.eastmoney.com/
14http://weibo.commore than 30% of the total characters and eliminate all
garbled characters present in the remaining posts;
• We implement the locality-sensitive hashing (Datar et al.
2004) (LSH) algorithm to filter out the duplicated posts.
• We filter out posts with a length of less than 50 characters
to ensure a minimum level of meaningful content.
By applying these filtering criteria, we construct the
CFData-SM sub-dataset with 495 million social media
posts, comprising a total of 84.6 billion tokens.
CFData-Wiki Wikipedia is a online encyclopedia, con-
taining multiple subjects in multiple areas, which is impor-
tant to introduce general topics over the work. To address
maintain the generalization ability of FinLLMs during pre-
training, we incorporate data from the Chinese Wikipedia
page. To accomplish this, we obtained a dump of the Chi-
nese Wikipedia until July 20, 2023 from the Wikipedia15.
To ensure the cleanliness of the dataset for pre-training, we
performed several preprocessing steps. Firstly, we extracted
the Chinese documents from the dumped file through reg-
ular expression. Additionally, we converted traditional Chi-
nese characters into simplified Chinese characters to main-
tain consistency within CFData. Furthermore, we remove all
garbled characters present in each document. By applying
these preprocessing steps, we obtained a final dataset con-
sisting of 255 thousand documents with a total of 137 mil-
lion tokens.
3.3 Data for Supervised Fine-tuning
Derived from real-world finance applications, our super-
vised fine-tuning dataset is constructed using open-sourced
data from six distinct tasks: financial sentiment analysis,
financial event detection, financial report summary, finan-
cial topic decomposition, financial question answering, and
stock movement prediction. The specific prompts for each
task can be found in Table 2.
CFData-SA Financial sentiment analysis is a crucial task
in the financial domain. While there are existing datasets
available, most of them are in English. To enhance the Fin-
LLM specifically for financial sentiment analysis in Chi-
nese market, we construct the supervised fine-tuning dataset
in two aspects, 1) label the social media posts from the
CFData-SM by the GPT-4 API; 2) correlate the content and
invest rating in the research report from the CFData-RR.
For the first approach, we first random select 60 thousand
social media posts with the length more than 100 characters,
where we select 12 thousand social media posts each year
from 2018 to 2022. Then we explore the GPT-4 API to la-
bel the sentiment of each post with “Positive”, “Negative”,
and “Neutral”, where the prompt can be found in Table 2.
For the second approach, we first random select 60 thou-
sand research reports with the length between 2000 to 3000
characters, where we select 8.57 thousand research reports
each year from 2016 to 2022. Then we extract the content
and the invest rating from each research report, where the
15https://dumps.wikimedia.org/zhwiki/latest/

--- PAGE 6 ---
Task Instruction Pair
CFData-SAPlease analyze the sentiment of the following financial paragraph.
The answer should be choose from “Positive”, “Negative”, “Neutral”.
The paragraph is “[paragraph]”.
CFData-EDPlease detect the “[event category]” from the following financial paragraph.
If the “[event category]” exists, find all the event, otherwise, return None.
The paragraph is “[paragraph]”.
CFData-RS Please summarize the following financial report. The report is “[report]”.
CFData-TDPlease decompose the following financial topic from multiple small aspects.
The topic is “[topic]”.
CFData-QAPlease answer the questions based on given financial paragraph and conversation history.
The financial paragraph is “[paragraph]”. The conversation history is “[history]”.
The question is “[question]”.
CFData-SPPlease analyze the text information and price information of “[stock name]”,
and determine how will the price change.
The answer should be choose from “Positive”, “Negative”, “Neutral”.
The text information is “[text]”. The price information is “[price]”.
Table 2: The instruction pair for each sub-dataset. The corresponding content are filled into “[]” during supervised fine-tuning.
content is regarded as the text condition, and the invest rat-
ing is mapped to “Positive”, “Negative”, and “Neutral” and
regarded as the target. This approach allows us to capture
the sentiment within the specific content of the report, con-
sidering the unique characteristics of the Chinese financial
market. By combining these two approaches, we construct
the CFData-ED task comprising 120 thousand items with a
total of 118 million tokens.
CFData-ED Financial event detection aims to categorize
the financial documents with a hierarchy is helpful for peo-
ple to make informed investment decisions in the financial
market (Liang et al. 2020). Given a financial document, e.g.,
news, research reports and announcements of listed compa-
nies, the financial event is detected based on a tree-structured
event scheme, which contains 98 event categories spread-
ing across the event category nodes of the constructed event
category tree of depth 7. Specifically, we explore the CN-
Fin (Liang et al. 2020) dataset to construct the financial
event detection task, where we randomly select 490 thou-
sand instances out of all 500 thousand instance to construct
CFData-ED dataset, and leave 10 thousand instances for fu-
ture benchmark construction. By regarding the event detec-
tion as multi-label classification with the prompt in Table 2,
we construct the CFData-ED task comprising 490 thousand
items with a total of 461 million tokens.
CFData-TD In real-world financial applications, decom-
posing a general question into multiple simple or concrete
questions is helpful to proceed the search and summary to-
wards the original question, which is also important for re-
trieval augmented generation (RAG) chatbot to answer the
question thoroughly with more evidence. For example, one
might inquire about the development of electric vehicle in-
dustry in China. However, answering such questions re-
quires breaking them down into multiple aspects to gather
qualitative and quantitative information, which can then be
used to address the original questions effectively. The logical
decomposition of the topic into different aspects is crucialfor constructing a dataset that supports this type of analysis.
To accomplish this without extensive manual labor, we
propose leveraging research reports, as their titles typically
contain the topic of interest, while their outlines reflect the
analytical approach used to study that topic. To enhance a
FinLLM in the context of financial topic decomposition, we
propose constructing the dataset by using the title of each
research report as the source topic and the outline of each
research report as the decomposed target. This dataset will
enable the model to learn how to decompose financial topics
into their constituent aspects. By filtering the research report
without outlines or titles from CFData-RR, we get 379 thou-
sand research reports in total, where we randomly select 369
thousand research to construct CFData-TD dataset and leave
10 thousand instances for future use. Then we split the titles
and outlines of each research report to construct this dataset.
Finally, we construct the CFData-TD task comprising 369
thousand items with a total of 187 million tokens.
CFData-RS Report summary holds significant impor-
tance in financial applications. Given that financial reports
have specific focus areas distinct from general documents,
enhancing a FinLLM for financial report summary is of ut-
most importance. To accomplish this without extensive man-
ual labor, we propose to leveraging the research report it-
self to provide supervision. In detail, we construct a this
dataset by considering the content of the research report as
the source and utilizing the conclusion and abstract of the
research report as the target. This approach allows us to cap-
ture the key information and main points from the research
reports, facilitating the generation of concise and informa-
tive summaries. Following the similar split in CFData-TD,
we select 369 thousand research to construct CFData-RS
dataset and leave 10 thousand instances for future use. By
implementing this methodology, we construct the CFData-
RS task comprising 369 thousand items with a total of 765
million tokens.

--- PAGE 7 ---
User Input
Conversation
HistoryTask 
Classiﬁer
Task
Selection
LLMs  Output ResponseTask Speciﬁc
Control ModuleFigure 2: The working pipeline of CFAPP framework
LLMs
GPT 4.0
LLaMAOur Model
CFLLM-
ins-7BLLM APIsHistory Recorder Causal Reasoning
Price PredictionRisk Management
Input Parser
Whisper + 
TokenizerTokenizerPyPDF
+
Tokenizer
PDF
 T ext
 Audio
Graph Neural Network
Risk Contagion DetectorContent Summary
XMindTemplate
Repertory
Institution Relation Detector
Vector Database (Report)
Balance Sheet DatabaseVector Database (Report)
XMind ReAct
Price-V olume Sheet
Alpha Prediction Model
Factor Repertory
Vector Database (News)CFAPP
Figure 3: The CFAPP framework
CFData-QA Question answering is an important task that
involves automatically providing answers to financial ques-
tions based on the conditional information. However, the
majority of existing financial document question-answering
datasets are in English (Chen et al. 2021, 2022). To ad-
dress this limitation and enhance the performance of our
CFGPT model in financial question answering, we pro-
pose translating two existing English financial question-
answering datasets: FinQA (Chen et al. 2021) and Con-
vFinQA (Chen et al. 2022), into Chinese. FinQA comprises
question-answering pairs annotated by experts, along with
corresponding earnings reports from S&P 500 companies.
ConvFinQA builds upon FinQA by introducing multi-turn
conversations for question-answering tasks using earnings
reports. By merging these two translated datasets, we suc-
cessfully construct the CFData-QA task comprising 12 thou-
sand items with a total of 6 million tokens.
CFData-SP Stock moving prediction is a fundamental fi-
nancial task with significant potential value in real-world
applications, particularly in quantitative investment. In line
with the formulation in BigData22 (Soun et al. 2022), we
construct the CFData-SP dataset by combining the most
popular social media posts, financial news, and historical
stock prices to forecast future stock price movements. In de-
tail, we first select the mostly viewed related financial newsand related posts, historical close price in past five day, and
close price changing rate in next day for each stock in CSI
Smallcap 500 Index and each trading day from 2021 to 2022.
Then we frame the task as a classification problem, with the
goal of predicting whether the stock close price in next day
will ascend, descend, or hold.
Specifically, we employ the following classification cri-
teria: 1) If the close price changing rate exceeds 0.50%,
the sample is classified as “Ascend”; 2) If the close price
changing rate falls below -0.50%, the sample is classified as
“Descend”; 3) Otherwise, samples are categorized as “hold”.
Therefore, we can capture the different directions and mag-
nitudes of stock price movements in our dataset. Finally,
we construct the CFData-SP task comprising 212 thousand
items with a total of 175 tokens.
4 Model and Training
Our CFLLM model is built based on the base model,
InternLM-chat-7b. To enhance the understanding and rea-
soning abilities of base model within the financial domain,
and align the base model to real-world financial application,
we formulate the fine-tuning process into two stage: contin-
ued pre-training and supervised fine-tuning.
4.1 Continued Pre-training
In the first stage, we explore the InternLM-chat-7b as the
base model, which is fine-tuned with a standard left-to-right
causal language modeling objective on the our pre-training
dataset in Sec.3.2. To maximize GPU utilization, we follow
(Brown et al. 2020) and cut all our training sequences to
be the same length, in our case 1,024 tokens. To achieve
this, we first concatenate all our tokenized training docu-
ments with an “ <EOS>” token as a document separator.
Then we generate the the training instance with sample gap
and sample length as 512 and 1,024, respectively. Note that,
the InternLM-chat-7b model explore the relative positional
encoding and FlashAttention, our model can also be ap-
plied to sequence longer than 1,024 during the inference.
During the continued pre-training, we use the AdamW op-
timizer (Loshchilov and Hutter 2017), with the β1,β2, and
weight decay as 0.9, 0.95, and 1e-5 respectively. The batch
size is set as 512, learning rate is set as 1 ×10−5, and we use
the cosine decay learning rate scheduler with linear warmup
in the first 1000 steps. The continued pre-training is execute
on 8 pieces of A800 80GB GPUs. After the continued pre-
training, we get our CFLLM-pt-7B model.
4.2 Supervised Fine-tuning
In the second stage, we explore the continued pretrained
model in Sec 4.1 as the base model, which is fine-tuned
with QLoRA (Hu et al. 2022) in supervised fine-tuning
data covering 6 financial tasks in Sec. 3.3 and Moss-03-sft
dataset (Sun et al. 2023) to balance the general response abil-
ity and domain specific ability of our model. To further en-
hance the ability of model dealing with long financial doc-
uments, we set the maximum length of input texts as 2048.
To balance the GPU utilization and training instance length,
we sorted all training instance to make the training instance

--- PAGE 8 ---
within each batch have the similar sequence, and the batch
size is between 64 to 512, which is decided by the sequence
length of each batch. During the supervised fine-tuning, we
use the AdamW optimizer (Loshchilov and Hutter 2017),
with the β1,β2, and weight decay as 0.9, 0.95, and 1e-5 re-
spectively. The learning rate is set as 2 ×10−4, and we use
the cosine decay learning rate scheduler with linear warmup
in the first 500 steps. For the QLoRA setting, we set the
LoRA rank as 64, the LoRA Alpha as 16 and the dropout
rate of the LoRA linear function was set to 0.05. The su-
pervised fine-tuning is execute on 8 pieces of A800 80GB
GPUs. After the supervised fine-tuning, we get our CFLLM-
ins-7B model.
5 Application
In this section we will introduce our CFAPP framework and
provide examples to show the capability our demonstration.
5.1 Systemic Description
In Figure 2, we introduce the working pipeline of our
CFAPP framework. Specifically, given a user input, we first
explore a task classifier to identify the the functional task
of the input, then we seed the user input along with the
conversation history to the task specific control module,
where we designed different working procedure to each of
the task. For each task specific control module, it will in-
teract with the LLMs multiple times to get the output re-
sponse. In Figure 3, we introduce all the components of our
CFAPP framework, where the LLMs provide multiple choice
to different large language models, the Input Parser provides
three different apis for different kind of inputs, the History
Recorder stores the conversation history in previous itera-
tion, the Content Summary ,Causal Reasoning ,Price Pre-
diction , and Risk Management contains the model, database,
and tools for their corresponding requirements. Note that
theContent Summary andCausal Reasoning functionalities
mainly focus on enhance the representation and reasoning
ability of the LLMs in financial field, while the Price Pre-
diction andRisk Management functionalities pay more at-
tention to incorporate the task-specific model with LLMs to
enhance the justification and decision ability of our system.
In the following sections, we will introduce the detailed im-
plementation of each functionality.
Content Summary The content summary feature of our
system allows users to emphasize specific documents or
paragraphs of interest. Our system supports multiple out-
put formats, including template summaries, mind map sum-
maries, and text summaries. When interacting with our sys-
tem, users can indicate their desired output format, and we
provide support for generating summaries in these multiple
formats. For content summarization, we utilize the conver-
sation history and the user input to seed the large language
model (LLM) and generate item-wise summaries and the re-
sponse types. If the response type is “template”, we retrieve
the corresponding template from our template repository and
combine it with the item-wise summary to generate the re-
sponse. If the response type is “mind map”, we directly gen-
erate a mind map based on the item-wise summary usingthe graphviz package. If the response type is “raw text”, we
seed the item-wise summary back into the LLM to generate
a coherent and comprehensive paragraph as the response.
By offering these different response types, we provide users
with flexibility in selecting the output format that best suits
their needs and preferences. This allows users to obtain sum-
maries in the format that they find most useful and conve-
nient.
Causal Reasoning Our framework includes a causal rea-
soning component that enables users to answer complex
questions with the support of evidence. It is important to
note that our framework supports both conditional question-
answering and open-domain question-answering. For open-
domain question-answering, the user inputs the question
along with ReAct format prompts (which involve the loop
of Thought, Action, and Observation) to the large language
model (LLM). The ReAct prompts guide our framework
to retrieve relevant documents from the vector database
as supporting evidence. The ReAct prompts allow for two
types of actions: “[search]” and “[response]”. If the ac-
tion is “[search]”, our framework retrieves the top-5 rel-
evant documents from the vector database. On the other
hand, if the action is “[response]”, the LLM directly gen-
erates the answer based on the user input, conversation his-
tory, and the retrieved documents. After generating the re-
sponse, we present all the “Thought” and the answer to the
users, demonstrating how our framework decomposes and
solves the question. For conditional question-answering, the
user provides both the question and the condition to the
LLM, along with revised ReAct prompts. The revised Re-
Act prompts include three types of actions: “[search]”, “[re-
sponse]”, and “[pass]”. The “[pass]” action is used when the
Thought can be captured within the given condition. The re-
vised ReAct prompts allow the LLM to determine whether
the condition is sufficient to generate the answer. If the con-
dition is enough to answer the question, our framework di-
rectly generates the answers. However, if the condition is
not sufficient, a similar procedure is executed as in the open-
domain question-answering scenario. Through these mech-
anisms, our framework enables users to address both condi-
tional and open-domain questions, leveraging evidence from
the vector database and the reasoning capabilities of the
LLM. This provides users with a robust and comprehensive
approach to answering complex questions.
5.2 Demonstration Example
In Figures 4 and 5, we show two examples of our framework
to reveals the functionality of Content Summary andCausal
Reasoning .
In the first example from Figure 4, we provide the frame-
work with a PDF document related to the industrial struc-
tural of a listed company “Kweichow Moutai Company
Limited (SH:600519)”, and then the users asks to summary
the shareholding relationship and executive chart of Kwe-
ichow Moutai Company with mind map, where our model
can accomplish this work precisely with the mind map as
output. The detailed conversation is translated as follows:
• User: Hello, I will upload a PDF file, can your help to

--- PAGE 9 ---
CFAPPFigure 4: The example of our Content Summary to summarize the shareholding relationship and executive chart based on the
corporate announcements.
summary the top-5 shareholding relationship and execu-
tive chart of Kweichow Moutai (600519)?
• CFAPP: Yes, please upload the PDF file, I will try to gen-
erate the shareholding relationship and executive chart of
Kweichow Moutai (600519).
• User: Upload a PDF file, named as “ Kweichow Moutai
Co., Ltd. First Quarter Report for the year 2023 ”
• CFAPP: A mind map about the top-5 shareholding re-
lationship and top-6 executive chart of the Kweichow
Moutai (600519).
In the second example from Figures 5, we ask a open-
domain question, “”, and our framework can return the ques-
tion decomposition procedure and the answer together as
the response to show how our framework analyze this open-
domain question and then interact with the vector dataset to
answer question with evidence. The detailed conversation is
translated as follows:
• User: How did China’s lithium battery market develop in
2020?
• CFAPP: The question should be considered in the fol-
lowing aspects: 1. Market Size and Growth Rate: Re-
search the overall scale of China’s lithium battery market
in 2020 and its growth rate compared to previous years.
This can help understand the overall market situation and
development trend; 2. Industry Chain Analysis: Analyze
the production, distribution, and sales of lithium batter-
ies at each stage. This helps understand the operation of
the entire industry chain and identify potential issues at
each stage; 3. Key Participants and Competitive Land-
scape: Research the major companies in the market, in-
cluding their market share, product lines, and research
and development investments. This helps understand the
market’s competitive landscape and the strategies of key
participants; 4. Technological Advancements: Research
the development of lithium battery technology, includingnew materials, processes, and innovations. Technological
advancements directly impact product performance and
costs, which in turn influence market competitiveness; 5.
Policy Environment: Analyze the impact of government
policies on the lithium battery industry. For example,
government support policies for electric vehicles may in-
fluence the demand for lithium batteries; 6. Market De-
mand: Analyze the demand for lithium batteries in the
Chinese market, including the total demand, growth rate,
and future demand forecasts. Demand is the most direct
driving force for market development; 7. International
Impact: Study the influence of the international market
and international relations on the Chinese lithium bat-
tery market. For instance, the development of the global
electric vehicle market may have an impact on China’s
lithium battery market; 8. Environmental Impact: Inves-
tigate the environmental effects of the lithium battery in-
dustry, including raw material extraction and waste bat-
tery disposal. This is closely related to sustainable devel-
opment and corporate social responsibility.
Based on the search results towards these question, our
answer can be organized as In terms of market size and
growth rate, the Chinese lithium battery market reached
approximately 65 billion yuan in 2020, experiencing a
slight decline compared to the previous year. However,
the shipment volume increased by 13% year-on-year,
indicating that market demand remained robust. This
can be attributed to the rapid expansion of the new en-
ergy vehicle market and the continuous growth in de-
mand for energy storage batteries. Notably, the produc-
tion of lithium iron phosphate batteries increased by
24.7% in 2020, accounting for 41.4% of the total pro-
duction, which reflects the increasing demand for high
safety, long-life, and high-capacity lithium batteries in
the market.Regarding the industry chain, the lithium bat-
tery industry covers multiple segments, including raw

--- PAGE 10 ---
CFAPPFigure 5: The example of our Causal Reasoning to answer the open-domain question.
material supply, battery equipment manufacturing, bat-
tery production, and end applications. With China’s con-
tinuous innovation in lithium battery technology and ap-
plications, as well as the government’s active promo-
tion in the new energy field, cooperation across vari-
ous segments of the industry chain has become increas-
ingly closer, creating a positive cycle within the indus-
try chain.In terms of technological development, lithium
battery technology is progressing towards higher specific
energy and safety. Particularly, solid-state lithium batter-
ies are considered a key technology for future battery
development. Although currently in the stage of tech-
nology promotion and scaled production verification, it
is expected that by 2025, solid-state battery technology
will be commercialized and begin to move towards more
advanced lithium battery technologies, such as lithium-
metal, lithium-air, and lithium-sulfur batteries.Regarding
the policy environment, the Chinese government has pro-
vided strong support for lithium battery manufacturing
technology and has implemented a series of policies to
drive the development of this field. These policies aim to
improve battery energy density and range, thereby pro-
moting the development of new battery technologies like
solid-state batteries. In the future, with further develop-
ment in the new energy vehicle market, the lithium bat-
tery market will benefit from policy dividends, leading
to further expansion in scale and enhanced competitive-
ness.Regarding market demand, with the steady increase
in demand for lithium batteries from 3C products (com-
puters, communications, and consumer electronics) and
the gradual expansion of the new energy vehicle market,the demand for lithium batteries will continue to grow.
Consumer-grade lithium batteries account for 43.16% of
the total lithium-ion battery shipments, highlighting the
significant demand from the consumer electronics mar-
ket.Overall, the Chinese lithium battery market showed
strong vitality and potential in 2020 across various as-
pects such as market size, technological innovation, pol-
icy environment, and market demand. However, the mar-
ket still faces some challenges, such as improving bat-
tery performance, reducing costs, and ensuring safety.
These challenges require collaborative efforts and con-
tinuous technological innovation across the entire indus-
try chain. Nevertheless, with China’s ongoing efforts in
battery technology research and industry chain integra-
tion, we can expect a promising and broader future for
the Chinese lithium battery market.
6 Conclusion
In this study, we introduced an open-sourced Chinese finan-
cial assistant with large language model, named CFGPT,
for applicable LLM requirements in the financial domain.
CFGPT comprises a deployment framework to cater to real-
world applications, a two-stage fine-tuned open-source LLM
on Chinese financial datasets, and a benchmark for evalu-
ating model performance in the Chinese financial domain.
Through initial evaluation, we showcased the effectiveness
of our CFLLM-ins-7B model across financial tasks, un-
derscoring the potential of domain-specific continued pre-
training and supervised fine-tuning of large language mod-
els in the financial domain. Nonetheless, challenges such as
enhancing performance on complex tasks and addressing re-

--- PAGE 11 ---
source constraints still exist. Our open-source contribution
aims to foster further research and innovation in financial
large language model literature, promoting the development
of more valuable and applicable LLMs in the finance sector.
References
Alvarado, J. C. S.; Verspoor, K.; and Baldwin, T. 2015. Do-
main adaption of named entity recognition to support credit
risk assessment. In Proceedings of the Australasian Lan-
guage Technology Association Workshop 2015 , 84–90.
Araci, D. 2019. Finbert: Financial sentiment analy-
sis with pre-trained language models. arXiv preprint
arXiv:1908.10063 .
Black, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.;
Golding, L.; He, H.; Leahy, C.; McDonell, K.; Phang, J.;
et al. 2022. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745 .
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. In
NeurIPS .
Chen, Z.; Chen, W.; Smiley, C.; Shah, S.; Borova, I.; Lang-
don, D.; Moussa, R.; Beane, M.; Huang, T.-H.; Routledge,
B.; et al. 2021. Finqa: A dataset of numerical reasoning over
financial data. arXiv preprint arXiv:2109.00122 .
Chen, Z.; Li, S.; Smiley, C.; Ma, Z.; Shah, S.; and Wang,
W. Y . 2022. Convfinqa: Exploring the chain of numeri-
cal reasoning in conversational finance question answering.
arXiv preprint arXiv:2210.03849 .
Cheng, D.; Yang, F.; Xiang, S.; and Liu, J. 2022. Financial
time series forecasting with multi-modality graph neural net-
work. Pattern Recognition , 121: 108218.
Datar, M.; Immorlica, N.; Indyk, P.; and Mirrokni, V . S.
2004. Locality-sensitive hashing scheme based on p-stable
distributions. In Proceedings of the twentieth annual sym-
posium on Computational geometry , 253–262.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. In NAACL .
Ding, X.; Zhang, Y .; Liu, T.; and Duan, J. 2014. Using struc-
tured events to predict stock price movement: An empirical
investigation. In Proceedings of the 2014 conference on em-
pirical methods in natural language processing (EMNLP) ,
1415–1425.
Du, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
Tang, J. 2022. GLM: General Language Model Pretraining
with Autoregressive Blank Infilling. In ACL.
Hu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,
S.; Wang, L.; and Chen, W. 2022. Lora: Low-rank adaptation
of large language models. In ICLR .
Liang, X.; Cheng, D.; Yang, F.; Luo, Y .; Qian, W.; and Zhou,
A. 2020. F-HMTC: Detecting Financial Events for Invest-
ment Decisions Based on Neural Hierarchical Multi-Label
Text Classification. In IJCAI , 4490–4496.
Liu, X.-Y .; Yang, H.; Gao, J.; and Wang, C. D. 2021. FinRL:
Deep reinforcement learning framework to automate tradingin quantitative finance. In Proceedings of the second ACM
international conference on AI in finance , 1–9.
Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 .
Lu, D.; Liang, J.; Xu, Y .; He, Q.; Geng, Y .; Han, M.; Xin,
Y .; Wu, H.; and Xiao, Y . 2023. BBT-Fin: Comprehen-
sive Construction of Chinese Financial Domain Pre-trained
Language Model, Corpus and Benchmark. arXiv preprint
arXiv:2302.09432 .
Maia, M.; Handschuh, S.; Freitas, A.; Davis, B.; McDer-
mott, R.; Zarrouk, M.; and Balahur, A. 2018. Www’18 open
challenge: financial opinion mining and question answering.
InCompanion proceedings of the the web conference 2018 ,
1941–1942.
Malo, P.; Sinha, A.; Korhonen, P.; Wallenius, J.; and Takala,
P. 2014. Good debt or bad debt: Detecting semantic ori-
entations in economic texts. Journal of the Association for
Information Science and Technology , 65(4): 782–796.
OpenAI. 2021. GPT-3.5. https://www.openai.com/chatgpt/.
OpenAI. 2023. ChatGPT-4. https://www.openai.com/
chatgpt/.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
et al. 2022. Training language models to follow instructions
with human feedback. In NeurIPS .
Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;
et al. 2018. Improving language understanding by gener-
ative pre-training. In ICLR . OpenAI.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-
ing the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research ,
21(1): 5485–5551.
Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili ´c, S.; Hesslow,
D.; Castagn ´e, R.; Luccioni, A. S.; Yvon, F.; Gall ´e, M.; et al.
2022. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 .
Shah, R. S.; Chawla, K.; Eidnani, D.; Shah, A.; Du, W.;
Chava, S.; Raman, N.; Smiley, C.; Chen, J.; and Yang, D.
2022. When flue meets flang: Benchmarks and large pre-
trained language model for financial domain. arXiv preprint
arXiv:2211.00083 .
Sinha, A.; and Khandait, T. 2021. Impact of news on the
commodity market: Dataset and results. In Advances in In-
formation and Communication: Proceedings of the 2021 Fu-
ture of Information and Communication Conference (FICC),
Volume 2 , 589–601. Springer.
Soun, Y .; Yoo, J.; Cho, M.; Jeon, J.; and Kang, U. 2022.
Accurate Stock Movement Prediction with Self-supervised
Learning from Sparse Noisy Tweets. In 2022 IEEE Inter-
national Conference on Big Data (Big Data) , 1691–1700.
IEEE.
Sun, T.; Zhang, X.; He, Z.; Li, P.; Cheng, Q.; Yan, H.; Liu,
X.; Shao, Y .; Tang, Q.; Zhao, X.; Chen, K.; Zheng, Y .; Zhou,
Z.; Li, R.; Zhan, J.; Zhou, Y .; Li, L.; Yang, X.; Wu, L.; Yin,
Z.; Huang, X.; and Qiu, X. 2023. MOSS: Training Conver-
sational Language Models from Synthetic Data.

--- PAGE 12 ---
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; et al. 2023a. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971 .
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023b. Llama 2: Open Foundation and Fine-Tuned
Chat Models. arXiv preprint arXiv:2307.09288 .
Wu, S.; Irsoy, O.; Lu, S.; Dabravolski, V .; Dredze, M.;
Gehrmann, S.; Kambadur, P.; Rosenberg, D.; and Mann, G.
2023. Bloomberggpt: A large language model for finance.
arXiv preprint arXiv:2303.17564 .
Xie, Q.; Han, W.; Zhang, X.; Lai, Y .; Peng, M.; Lopez-
Lira, A.; and Huang, J. 2023. PIXIU: A Large Language
Model, Instruction Data and Evaluation Benchmark for Fi-
nance. arXiv preprint arXiv:2306.05443 .
Yang, H.; Liu, X.-Y .; and Wang, C. D. 2023. FinGPT: Open-
Source Financial Large Language Models. FinLLM at IJ-
CAI.
Yang, Y .; Uy, M. C. S.; and Huang, A. 2020. Finbert: A pre-
trained language model for financial communications. arXiv
preprint arXiv:2006.08097 .
Yu, Y . 2023. Cornucopia-LLaMA-Fin-Chinese.
https://github.com/jerry1993-tech/Cornucopia-LLaMA-
Fin-Chinese.
Zeng, W.; Ren, X.; Su, T.; Wang, H.; Liao, Y .; Wang, Z.;
Jiang, X.; Yang, Z.; Wang, K.; Zhang, X.; et al. 2021.
Pangu- alpha : Large-scale autoregressive pretrained Chi-
nese language models with auto-parallel computation. arXiv
preprint arXiv:2104.12369 .
Zhang, B.; Yang, H.; and Liu, X.-Y . 2023. Instruct-
FinGPT: Financial Sentiment Analysis by Instruction Tun-
ing of General-Purpose Large Language Models. FinLLM
at IJCAI .
Zhang, L.; Cai, W.; Liu, Z.; Yang, Z.; Dai, W.; Liao, Y .; Qin,
Q.; Li, Y .; Liu, X.; Liu, Z.; et al. 2023. FinEval: A Chinese
Financial Domain Knowledge Evaluation Benchmark for
Large Language Models. arXiv preprint arXiv:2308.09975 .
Zhang, W.; and Skiena, S. 2010. Trading strategies to exploit
blog and news sentiment. In Proceedings of the interna-
tional AAAI conference on web and social media , volume 4,
375–378.
Zhang, X.; and Yang, Q. 2023. Self-QA: Unsupervised
Knowledge Guided Language Model Alignment. arXiv
preprint arXiv:2305.11952 .
Zhang, X.; Yang, Q.; and Xu, D. 2023. XuanYuan 2.0: A
Large Chinese Financial Chat Model with Hundreds of Bil-
lions Parameters. arXiv preprint arXiv:2305.12002 .
Zhang, Z.; Zhang, H.; Chen, K.; Guo, Y .; Hua, J.; Wang,
Y .; and Zhou, M. 2021. Mengzi: Towards lightweight yet
ingenious pre-trained models for chinese. arXiv preprint
arXiv:2110.06696 .
