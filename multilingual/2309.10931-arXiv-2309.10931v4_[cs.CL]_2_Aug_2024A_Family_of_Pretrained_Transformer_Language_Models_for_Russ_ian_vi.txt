# 2309.10931.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2309.10931.pdf
# Kích thước tệp: 309910 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2309.10931v4 [cs.CL] 2 Aug 2024 Một Họ Mô Hình Ngôn Ngữ Transformer Được Tiền Huấn Luyện cho Tiếng Nga

Dmitry Zmitrovich1, Alexander Abramov1, Andrey Kalmykov1,
Maria Tikhonova1, Ekaterina Taktasheva2∗, Danil Astafurov1,
Mark Baushenko1, Artem Snegirev1, Vitalii Kadulin1, Sergey Markov1,
Tatiana Shavrina3∗, Vladislav Mikhailov4∗, and Alena Fenogenova1
1SaluteDevices, 2Đại học Edinburgh, 3Viện Ngôn ngữ học, RAS, 4Đại học Oslo
Liên hệ: alenush93@gmail.com

Tóm tắt
Các mô hình ngôn ngữ Transformer (LM) là nền tảng của các phương pháp nghiên cứu và ứng dụng NLP trong nhiều ngôn ngữ khác nhau. Tuy nhiên, việc phát triển những mô hình như vậy đặc biệt cho tiếng Nga đã nhận được ít sự chú ý. Bài báo này giới thiệu một bộ sưu tập gồm 13 mô hình ngôn ngữ Transformer tiếng Nga, bao gồm các kiến trúc encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), và encoder-decoder (ruT5, FRED-T5). Chúng tôi cung cấp một báo cáo về thiết kế kiến trúc mô hình và tiền huấn luyện, cũng như kết quả đánh giá khả năng khái quát hóa của chúng trên các tập dữ liệu và benchmark hiểu biết và sinh văn bản tiếng Nga. Bằng cách tiền huấn luyện và phát hành những mô hình Transformer LM chuyên biệt này, chúng tôi nhằm mở rộng phạm vi các hướng nghiên cứu NLP và cho phép phát triển các giải pháp công nghiệp cho tiếng Nga.

Từ khóa: Mô hình ngôn ngữ tiếng Nga, Hiểu biết ngôn ngữ tiếng Nga, Sinh văn bản tiếng Nga

1. Giới thiệu
Các mô hình ngôn ngữ Transformer (LM; Vaswani et al., 2017) đã trở thành một thành phần thiết yếu của các phương pháp tiên tiến cho nhiều tác vụ hiểu biết và sinh ngôn ngữ tự nhiên. Những LM này trải qua quá trình tiền huấn luyện theo cách tự giám sát ở quy mô lớn trên các kho văn bản lớn trước khi được thích ứng với tác vụ downstream thông qua fine-tuning, học few-shot, và instruction tuning (Ruder et al., 2019; Bommasani et al., 2022; Chowdhery et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Quyền truy cập mở vào các trọng số của mô hình được tiền huấn luyện cho phép cộng đồng đẩy nhanh nghiên cứu và phát triển các giải pháp công nghiệp hiệu quả (Wolf et al., 2020). Tuy nhiên, hầu hết những LM này được phát triển cho tiếng Anh, điều này áp đặt những hạn chế đáng kể lên tiềm năng của các công nghệ ngôn ngữ.

Cộng đồng đã giải quyết vấn đề này bằng cách phát hành các LM đa ngôn ngữ lớn (ví dụ, Conneau và Lample, 2019; Conneau et al., 2020; Liu et al., 2020b; Xue et al., 2021; Scao et al., 2023) và các LM đơn ngôn ngữ cho các ngôn ngữ đa dạng về mặt loại hình học (ví dụ, Polignano et al., 2019; Le et al., 2020; Delobelle et al., 2020; Cui et al., 2020; Kutuzov et al., 2021). Ngày nay, vẫn còn thiếu các mô hình Transformer LM được phát triển đặc biệt cho tiếng Nga.

Bài báo này giới thiệu một họ các mô hình Transformer LM được tiền huấn luyện cho tiếng Nga, bao gồm một bộ đa dạng các kiến trúc mô hình. Chúng tôi cung cấp các phiên bản tiếng Nga của các mô hình BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2019), GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), và UL2 (Tay et al., 2022) với nhiều kích thước khác nhau. Chúng tôi báo cáo việc phát triển các LM của chúng tôi và tập trung vào việc đánh giá chúng trên một bộ các tập dữ liệu và benchmark hiểu biết và sinh văn bản tiếng Nga tiêu chuẩn. Kết quả cho thấy các LM của chúng tôi vượt trội so với các đối tác đa ngôn ngữ và các mô hình Transformer tiếng Nga liên quan trên hầu hết các tác vụ, đạt được hiệu suất tốt nhất. Những đóng góp chính như sau:

1. Chúng tôi tiền huấn luyện và phát hành 13 mô hình LM dựa trên Transformer cho tiếng Nga: ruBERT-base1, ruBERT-large2, ruRoBERTa-large3, ruELECTRA-small4, ruELECTRA-medium5, ruELECTRA-large6, ruGPT-3-small7, ruGPT-3-medium8, ruGPT-3-large9, ruT5-base10, ruT5-large11, FRED-T5-large12, và FRED-T5-XL13. Các LM đã được phát hành trong những năm gần đây dưới giấy phép MIT.

2. Chúng tôi thực hiện một loạt thí nghiệm để đánh giá khả năng khái quát hóa của các LM của chúng tôi

1hf.co/ai-forever/ruBERT-base
2hf.co/ai-forever/ruBERT-large
3hf.co/ai-forever/ruRoBERTa-large
4hf.co/ai-forever/ruELECTRA-small
5hf.co/ai-forever/ruELECTRA-medium
6hf.co/ai-forever/ruELECTRA-large
7hf.co/ai-forever/ruGPT-3-small
8hf.co/ai-forever/ruGPT-3-medium
9hf.co/ai-forever/ruGPT-3-large
10hf.co/ai-forever/ruT5-base
11hf.co/ai-forever/ruT5-large
12hf.co/ai-forever/FRED-T5-large
13hf.co/ai-forever/FRED-T5-XL

--- TRANG 2 ---
trên một phạm vi rộng các tác vụ, bao gồm đọc hiểu máy, suy luận ngôn ngữ tự nhiên, định nghĩa nghĩa từ, giải quyết đồng tham chiếu, phân loại tính chấp nhận được, nhận dạng tính không phù hợp, đơn giản hóa văn bản, tóm tắt văn bản, và khử độc văn bản. Cơ sở mã đánh giá được công khai14.

2. Công trình liên quan
2.1. Mô hình ngôn ngữ đa ngôn ngữ
Tiếng Nga được đại diện tốt trong kho văn bản tiền huấn luyện của nhiều mô hình LM đa ngôn ngữ lớn, như mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), RemBERT (Chung et al., 2021), mBART (Liu et al., 2020b), mT5 (Xue et al., 2021), XGLM (Lin et al., 2022), mGPT (Shliazhko et al., 2022), BLOOM (Scao et al., 2023), và mDeBERTa (He et al., 2023), trong số những cái khác. Các mô hình LM đa ngôn ngữ đã đóng góp đáng kể vào việc đạt được kết quả đáng chú ý trong các tác vụ NLP tiêu chuẩn cho tiếng Nga và các ngôn ngữ liên quan (Arkhipov et al., 2019). Tuy nhiên, với sự phát triển của các đối tác đơn ngôn ngữ (xem § 2.2), những mô hình LM này chủ yếu đóng vai trò là đường cơ sở mạnh cho các tác vụ hiểu biết và sinh ngôn ngữ tiếng Nga phức tạp hơn (ví dụ, Shavrina et al., 2020; Sakhovskiy et al., 2021; Mikhailov et al., 2022).

2.2. Mô hình ngôn ngữ tiếng Nga
DeepPavlov (Burtsev et al., 2018) đã tiền huấn luyện một trong những mô hình LM đơn ngôn ngữ dựa trên BERT đầu tiên cho tiếng Nga. Cấu hình mô hình bao gồm (i) mô hình RuBERT-base được tiền huấn luyện trên kho văn bản Wikipedia tiếng Nga và tin tức (Kuratov và Arkhipov, 2019), (ii) mô hình RuBERT-base-conversational15 được tiền huấn luyện trên OpenSubtitles (Lison và Tiedemann, 2016) và văn bản mạng xã hội, và (iii) một phiên bản chưng cất của RuBERT-base-conversational (Kolesnikova et al., 2022).

Yandex đã phát hành RuLeanALBERT16, một phiên bản tiếng Nga của mô hình ALBERT (Lan et al., 2020), và YaLM-100B17, mô hình LM tiếng Nga lớn nhất có sẵn công khai. Các mô hình LM được tiền huấn luyện trên kho văn bản web, các bài viết Wikipedia, văn bản từ kho Taiga (Shavrina và Shapovalova, 2017), và nhiều nguồn khác.

Theo các công trình này, chúng tôi đã đóng góp vào việc phát triển các mô hình LM tiếng Nga mã nguồn mở với nhiều

14github.com/aiforever/russianlm-evaluation
15hf.co/DeepPavlov/rubert-base-conversational
16hf.co/yandex/RuLeanALBERT
17hf.co/yandex/YaLM-100B

[THIS IS TABLE: Bảng 1 showing pretraining corpus statistics with columns for Model, Wikipedia (ru/en), News, Books, C4, OpenSubtitles, and Size]

kiến trúc mô hình khác nhau, được sử dụng rộng rãi trong cộng đồng NLP tiếng Nga cho mục đích nghiên cứu và phát triển (ví dụ, Dementieva et al., 2022; Artemova et al., 2022; Shamardina et al., 2022).

3. Mô hình
Phần này mô tả kho văn bản tiền huấn luyện mô hình, thiết kế kiến trúc, và chi tiết tiền huấn luyện.

3.1. Kho văn bản tiền huấn luyện
Thu thập dữ liệu Bảng 1 tóm tắt thống kê chung của kho văn bản tiền huấn luyện của chúng tôi. Kho văn bản bao gồm văn bản từ nhiều nguồn có sẵn công khai, đại diện cho các lĩnh vực đa dạng:

• Wikipedia — một bộ sưu tập các văn bản lĩnh vực chung từ kho văn bản Wikipedia tiếng Nga và tiếng Anh. Các bài viết Wikipedia được trích xuất từ các dump tương ứng với sự trợ giúp của công cụ WikiExtractor (Attardi, 2015).

• Tin tức — một bộ sưu tập các bài báo tin tức từ kho Taiga và các nguồn tin tức Lenta, Gazeta, và Interfax từ thư viện corus18.

• Sách — một bộ sưu tập các văn bản văn học từ kho librusec (Panchenko et al., 2017) và văn bản thơ ca từ kho Taiga. Các văn bản được tải xuống qua thư viện corus.

• Colossal Clean Crawled Corpus (C4; Raffel et al., 2020) — một bộ sưu tập các văn bản web bằng tiếng Nga. Dữ liệu C4 được tải xuống bằng cách sử dụng Tensorflow datasets (Paper, 2021).

• OpenSubtitles — một bộ sưu tập phụ đề phim và truyền hình được trích xuất từ kho song song.

Nhìn chung, các lĩnh vực và kích thước khác nhau của các kho con được bao gồm trong kho văn bản tiền huấn luyện kết quả của các mô hình LM của chúng tôi, dao động từ 30GB (ruBERT) đến 450GB (ruGPT-3). Sự biến đổi này chủ yếu do nhiều yếu tố. Thứ nhất, các mô hình của chúng tôi đã trải qua tiền huấn luyện trong vài năm dựa trên những tiến bộ phương pháp trong việc phát triển LM và tạo kho văn bản tiền huấn luyện. Ví dụ, kho con C4 của ruGPT-3 khác với kho của ruT5 và FRED-T5 ở chỗ nó được lọc theo quy trình được mô tả trong Ortiz Suárez et al. (2019). Thứ hai, lượng dữ liệu văn bản trong các nguồn có sẵn công khai đã tăng lên

18github.com/natasha/corus

--- TRANG 3 ---
theo thời gian, thúc đẩy cải thiện phạm vi bao phủ các thay đổi của thế giới và đại diện lĩnh vực.

3.2. Kiến trúc & Chi tiết tiền huấn luyện
Các mục tiêu tiền huấn luyện, kiến trúc mô hình, chiến lược mở rộng, và các lựa chọn thiết kế khác cho các mô hình LM của chúng tôi được tóm tắt trong Bảng 2. Các lựa chọn cấu hình mô hình dựa trên các nghiên cứu thực nghiệm rộng rãi được mô tả chi tiết trong Devlin et al. (2019); Liu et al. (2019); Clark et al. (2020); Brown et al. (2020); Tay et al. (2022), và các yếu tố khác, như tính khả dụng của dữ liệu và tài nguyên tính toán, tiêu chuẩn LM, và trạng thái lĩnh vực tại một thời điểm cụ thể, bắt đầu từ kiến trúc mô hình BERT.

3.2.1. ruBERT
Kiến trúc ruBERT dựa trên BERT (Devlin et al., 2019) và được tiền huấn luyện trên (i) mục tiêu mô hình hóa ngôn ngữ có mặt nạ (MLM) để dự đoán các token bị che trong đầu vào và (ii) mục tiêu dự đoán câu tiếp theo (NSP) để dự đoán liệu hai câu có theo sau nhau hay không. Chúng tôi sử dụng hai phiên bản BERT (BERT-base và BERT-large) và mã hóa Byte-pair Encoding (BPE; Wang et al., 2020), với kích thước từ vựng là 12·104 token. Những khác biệt chính giữa ruBERT của DeepPavlov và các mô hình ruBERT LM của chúng tôi như sau. Thứ nhất, chúng tôi tiền huấn luyện và phát hành mô hình ruBERT-large đầu tiên. Thứ hai, các mô hình ruBERT của DeepPavlov được tiền huấn luyện với kích thước batch nhỏ trên số lượng GPU hạn chế. Ngược lại, chúng tôi tiền huấn luyện các mô hình ruBERT LM của chúng tôi trên kho văn bản tiền huấn luyện tương tự bằng cách sử dụng kích thước batch lớn hơn và nhiều tài nguyên tính toán hơn, dẫn đến cải thiện hiệu suất mô hình (xem § 4).

Chi tiết tiền huấn luyện Chúng tôi tiền huấn luyện ruBERT-base và ruBERT-large với độ dài chuỗi tối đa là 512 token bằng cách sử dụng bộ lập lịch tuyến tính với tốc độ học ban đầu 1e−4 và trình tối ưu hóa Adam (Kingma và Ba, 2017) với β1 = 0.9, β2 = 0.99, và ε = 1e−8. Xác suất che mặt nạ là 0.15. Tổng số bước tiền huấn luyện là 106. ruBERT-base được tiền huấn luyện trong 8 ngày trên 16 GPU V100, và ruBERT-large được tiền huấn luyện trong 20 ngày trên 16 GPU V100.

3.2.2. ruRoBERTa
Kiến trúc Chúng tôi sử dụng cấu hình RoBERTa-large (Liu et al., 2019) cho ruRoBERTa-large. Mục tiêu tiền huấn luyện là MLM, phương pháp mã hóa là Byte-level BPE (BBPE; Wang et al., 2020), và từ vựng đếm 5·104 token.

Chi tiết tiền huấn luyện Chúng tôi tiền huấn luyện mô hình với tổng kích thước batch là 4096, độ dài chuỗi tối đa là 512 token, bộ lập lịch tuyến tính với tốc độ học ban đầu 1e−4, và trình tối ưu hóa Adam với β1 = 0.9, β2 = 0.99, và ε = 1e−8. Xác suất che mặt nạ là 0.15. Mô hình đã thấy 2T token trong quá trình tiền huấn luyện, mất 21 ngày trên 64 GPU V100.

3.2.3. ruELECTRA
Kiến trúc Chúng tôi sử dụng cấu hình kiến trúc ELECTRA và tuân theo quy trình tiền huấn luyện được mô tả trong Clark et al. (2020). Các mô hình được tiền huấn luyện với mục tiêu phát hiện token thay thế (RTD) để dự đoán token đầu vào nào bị che bởi "generator" dựa trên MLM. Chúng tôi sử dụng BPE với kích thước từ vựng là 256·103, 64·103, và 120·103 token tương ứng cho ruELECTRA-small, ruELECTRA-medium, và ruELECTRA-large.

Chi tiết tiền huấn luyện Chúng tôi tiền huấn luyện các mô hình ruELECTRA bằng cách sử dụng tốc độ học 2e−4, xác suất che mặt nạ 0.25, trình tối ưu hóa Adam với β1 = 0.9, β2 = 0.99, và ε = 1e−6, và độ dài chuỗi tối đa là 512 token. ruELECTRA-small, ruELECTRA-medium, và ruELECTRA-large được tiền huấn luyện với kích thước batch lần lượt là 128, 64, và 48 trong 7, 8, và 10 ngày trên 4 GPU V100 với tổng số bước lần lượt là 1·106, 1·106, và 4·105.

3.2.4. ruGPT-3
Kiến trúc ruGPT-3 là phiên bản tiếng Nga của GPT-3 (Brown et al., 2020). Chúng tôi sử dụng mô tả kiến trúc mô hình của Brown et al. (2020) và cơ sở mã GPT-2 (Radford et al., 2019) từ thư viện Transformers (Wolf et al., 2020). ruGPT-3 được tiền huấn luyện trên mục tiêu mô hình hóa ngôn ngữ. Chúng tôi sử dụng mã hóa BBPE với kích thước từ vựng là 5·104 token.

Chi tiết tiền huấn luyện Các mô hình ruGPT-3 được tiền huấn luyện với độ dài chuỗi tối đa là 1024 token trong ba epoch và 2048 token trong một epoch. Chúng tôi sử dụng tốc độ học ban đầu 1e−4 và trình tối ưu hóa Adam với β1 = 0.9, β2 = 0.99, và ε = 1e−8. Tổng số token được thấy trong quá trình tiền huấn luyện là 80B. Việc tiền huấn luyện ruGPT3-small, ruGPT3-medium, và ruGPT3-large mất lần lượt 7, 16, và 16 ngày trên 32, 64, và 128 GPU V100-SXM3.

3.2.5. ruT5
Kiến trúc ruT5 là một trong những mô hình LM encoder-decoder đầu tiên được tiền huấn luyện chỉ trên dữ liệu văn bản tiếng Nga.

--- TRANG 4 ---
[THIS IS TABLE: Bảng 2 showing model architecture configurations with columns for Model, Encoder, Decoder, Objective, Parameters, # Layers, dmodel, dff, Tokenizer, # Heads]

ruT5 được thiết kế tương tự như T5 (Raffel et al., 2020) và có sẵn trong hai cấu hình mô hình: ruT5-base và ruT5-large. Các mô hình được tiền huấn luyện trên mục tiêu MLM span corruption, trong đó các span liên tiếp của token đầu vào bị che, và mô hình được huấn luyện để tái tạo lại các token bị che. Chúng tôi sử dụng mã hóa SentencePiece (Kudo và Richardson, 2018) với kích thước từ vựng là 32·103 token.

Chi tiết tiền huấn luyện Các mô hình ruT5 được tiền huấn luyện bằng cách sử dụng bộ lập lịch tuyến tính với tốc độ học 1e−4 và trình tối ưu hóa Adam với β1 = 0.9, β2 = 0.99, và ε = 1e−8. Độ dài chuỗi được đặt là 512/512 cho đầu vào và mục tiêu. Các mô hình ruT5-base và ruT5-large được tiền huấn luyện với tổng kích thước batch là 2048 trong 14 ngày trên 32 GPU V100 và 21 ngày trên 64 GPU V100 tương ứng.

3.2.6. FRED-T5
Kiến trúc FRED-T5 (Full-scale Russian Enhanced Denoisers) là mô hình encoder-decoder dựa trên T5 và UL2 (Tay et al., 2022), có sẵn trong hai cấu hình: FRED-T5-large và FRED-T5-XL. Khác với ruT5, FRED-T5 sử dụng hàm gated GELU thay vì ReLU. Lấy cảm hứng từ (Tay et al., 2022), chúng tôi tiền huấn luyện FRED-T5 trên một hỗn hợp các denoiser, một tập hợp các mục tiêu tiền huấn luyện đa dạng. R-Denoiser là mục tiêu MLM span corruption được sử dụng trong T5. S-Denoiser tuân theo mục tiêu mô hình hóa ngôn ngữ, trong đó chuỗi đầu vào được chia thành các token ngữ cảnh và mục tiêu sao cho các mục tiêu không phụ thuộc vào thông tin tương lai. X-Denoiser nhằm khôi phục phần lớn đầu vào dựa trên các mục tiêu span corruption và mô hình hóa ngôn ngữ.

Những khác biệt chính trong các phương pháp tiền huấn luyện giữa UL2 và FRED-T5 như sau: (i) chúng tôi sử dụng bảy denoiser với phân phối đồng nhất của các siêu tham số μ (độ dài span trung bình), r (tỷ lệ corruption), và n (số lượng span bị hỏng) thay vì phân phối chuẩn, và (ii) chúng tôi sử dụng BBPE thay vì SentencePiece, với kích thước từ vựng là 5·104 token.

Chúng tôi sử dụng các token đặc biệt và siêu tham số sau cho các denoiser FRED-T5: <LM> (μ=L/4, r=0.25, n=1), <SC1> (μ=3, r=0.15, n=1), <SC2> (μ=8, r=0.15, n=1), <SC3> (μ=64, r=0.15, n=1), <SC4> (μ=3, r=0.5, n=1), <SC5> (μ=8, r=0.5, n=1), <SC6> (μ=64, r=0.5, n=1), trong đó L là độ dài đầu vào. Token <LM> tương ứng với S-Denoiser.

Chi tiết tiền huấn luyện FRED-T5 được tiền huấn luyện bằng cách sử dụng bộ lập lịch tuyến tính với tốc độ học ban đầu 1e−4 và trình tối ưu hóa Adafactor (Shazeer và Stern, 2018) với β1 = 0.9, β2 = 0.99, và ε = 1e−8. Độ dài chuỗi được đặt là 512/512 cho đầu vào và mục tiêu. Các mô hình FRED-T5-large và FRED-T5-XL được tiền huấn luyện với tổng kích thước batch là 2048 trong 35 ngày trên 160 GPU V100, tiếp theo là 5 ngày trên 80 GPU A100, và trong 45 ngày trên 112 GPU A100 tương ứng.

4. Đánh giá thực nghiệm
Phần này mô tả thiết lập thí nghiệm và trình bày các kết quả chính của việc đánh giá các mô hình LM của chúng tôi trên một bộ benchmark và tập dữ liệu tiêu chuẩn cho tiếng Nga. Các siêu tham số kết quả tối ưu được tóm tắt trong Bảng 10 (xem § 10.1).

4.1. Hiểu biết ngôn ngữ tự nhiên
4.1.1. Hiểu biết ngôn ngữ chung
Tác vụ Russian SuperGLUE (Shavrina et al., 2020) bao gồm chín tác vụ về hiểu biết thông thức (RUSSE, PARus), suy luận ngôn ngữ tự nhiên (TERRa, RCB), suy luận (RWSD), đọc hiểu máy (MuSeRC, RuCoS; Fenogenova et al., 2020) và kiến thức thế giới (DaNetQA; Glushkova et al., 2021), và một bộ kiểm tra chẩn đoán phạm vi rộng (LiDiRus). Các metric hiệu suất là điểm số accuracy (Acc.; PARus, TERRa, RUSSE, RWSD, RCB, và DaNetQA), exact match (EM; MuSeRC, RuCoS), F1-score (F1; RCB, RuCoS), macro-average F1-score (F1a; MuSeRC), và Matthews Correlation Coefficient (MCC; LiDiRus).

Phương pháp Chúng tôi ước tính hiệu suất mô hình thông qua fine-tuning và đánh giá zero-shot. Các mô hình LM encoder và encoder-decoder được fine-tune tối đa 40 epoch với early stopping dựa trên metric hiệu suất cụ thể theo tác vụ hoặc trung bình của chúng trên tập validation. Các mẫu template tác vụ được trình bày trong Bảng 11 (xem § 10.2).

• Encoder LM: chúng tôi fine-tune các encoder thông qua thư viện Transformers bằng cách sử dụng trình tối ưu hóa AdamW (Loshchilov và Hutter, 2019), tốc độ học 1·10−5, weight decay 0.01, và kích thước batch 32.

• Decoder LM: các mô hình decoder-only được đánh giá trong thiết lập zero-shot, trong đó nhãn mục tiêu được chọn dựa trên perplexity thấp nhất của các mẫu template prompt kết quả. Kết quả ruGPT-3 được lấy từ bảng xếp hạng chính thức tính đến tháng 9 năm 2023: russian-superglue.com/leaderboard.

• Encoder-decoder LM: chúng tôi công thức hóa các tác vụ ở định dạng text-to-text và tuân theo quy trình fine-tuning hai giai đoạn (Raffel et al., 2020). Giai đoạn đầu tiên là tiền huấn luyện đa tác vụ, trong đó mô hình được tiếp tục tiền huấn luyện trên một kết hợp các tác vụ. Mỗi đầu vào bắt đầu với một tiền tố cụ thể theo tác vụ. Tiếp theo, mô hình được fine-tune trên mỗi tác vụ riêng lẻ bằng cách sử dụng độ chính xác bf16. Chúng tôi thí nghiệm với việc sử dụng các kết hợp Adam & linear scheduler với tốc độ học 1·10−5, và Adafactor & constant scheduler với tốc độ học 1·10−3.

Baseline Chúng tôi fine-tune ruBERT-base của DeepPavlov, mBERT, mT5-base, mT5-large và XLM-R-large như được mô tả ở trên. Chúng tôi cũng so sánh các mô hình LM của chúng tôi với các kết quả bảng xếp hạng chính thức sau: người chú thích, ruBERT-base-conversational của DeepPavlov (ruBERT-base-conv), YaLM 3.3B & P-tuning (YaLM P-tune), RuLeanALBERT, và FRED-T5-XL encoder-only được fine-tune trên mỗi tác vụ RSG độc lập.

Kết quả Các kết quả được hiển thị trong Bảng 3. FRED-T5-XL thể hiện tốt nhất trên hầu hết các tác vụ, với điểm số tổng thể là 75.2. Fine-tuning chỉ encoder FRED-T5-XL dẫn đến kết quả mạnh trên PARus, MuSeRC, TERRa, RUSSE, và RuCoS. ruRoBERTa-large nhận được hiệu suất tốt nhất tổng thể trong số các mô hình LM encoder được đề xuất (68.1),

--- TRANG 5 ---
[THIS IS TABLE: Bảng 3 showing results on Russian SuperGLUE with multiple models and metrics]

thực hiện tương đương với ruT5-large. So sánh kết quả với encoder có hiệu suất tốt nhất, chúng tôi thấy rằng ruRoBERTa-large vượt trội hơn RuLeanALBERT trên RCB và DaNetQA. Chúng tôi cũng thấy rằng các mô hình LM dựa trên ruBERT của chúng tôi vượt trội hơn các mô hình ruBERT của DeepPavlov. ruELECTRA thể hiện kém hơn trên các tác vụ đọc hiểu máy, dẫn đến điểm số tổng thể thấp hơn. Hiệu suất zero-shot tổng thể của các mô hình LM decoder-only tương tự như ruBERT-base-conv và các mô hình LM dựa trên ruELECTRA. Các phiên bản lớn hơn của các mô hình LM dựa trên ruGPT vượt trội hơn các encoder trên RCB, PARus, và MuSeRC (ví dụ, mBERT, XLM-R-large, và ruELECTRA).

Các mô hình LM của chúng tôi đã thúc đẩy kết quả hiện đại mới trên hầu hết các tác vụ Russian SuperGLUE, và khoảng cách hiệu suất tổng thể giữa con người và các mô hình LM đã được thu hẹp lên đến 4.9. Tuy nhiên, vẫn còn chỗ cho cải thiện mô hình trên các tác vụ RWSD, RCB, TERRa, và PARus.

4.1.2. Phân loại tính chấp nhận được
Tác vụ RuCoLA (Mikhailov et al., 2022) bao gồm các câu trong lĩnh vực từ các ấn phẩm ngôn ngữ học và các câu ngoài lĩnh vực được tạo ra bởi các mô hình LM sinh. Tác vụ là dự đoán xem một câu cho trước có chấp nhận được hay không. Các metric hiệu suất là điểm số accuracy (Acc.) và MCC.

Phương pháp Chúng tôi tuân theo quy trình fine-tuning và đánh giá được mô tả trong Mikhailov et al. (2022). Chúng tôi sử dụng kết quả ruRoBERTa-large, ruGPT-3-medium, và ruT5-base từ Mikhailov et al. (2022). Cấu hình mô hình tốt nhất được chọn dựa trên MCC trên tập validation.

• Encoder LM: các encoder (ruBERT, ruELECTRA) được fine-tune trong 5 epoch bằng cách sử dụng trình tối ưu hóa AdamW thông qua tìm kiếm lưới trên một tập hợp các siêu tham số: tốc độ học {10−5, 3·10−5, 5·10−5} và các giá trị weight decay {10−4, 10−2, 0.1}. Kết quả được tính trung bình trên 10 lần chạy thí nghiệm với các seed ngẫu nhiên khác nhau.

• Decoder LM: các mô hình ruGPT-3-small và ruGPT-3-large được đánh giá bằng cách sử dụng phương pháp phân loại dựa trên ngưỡng cho thang đo tính chấp nhận được PenLP (Lau et al., 2020). Ngưỡng được chọn trên tập huấn luyện thông qua cross-validation 10-fold để tối đa hóa MCC trên tập validation: −19.65 (ruGPT-3-small), −20.91 (ruGPT-3-medium), và −19.39 (ruGPT-3-large).

• Encoder-decoder LM: ruT5-large được fine-tune trong 20 epoch, với không gian tìm kiếm {10−4, 10−3} cho tốc độ học và {0, 10−4} cho weight decay. Chúng tôi fine-tune các mô hình FRED-T5 trong 20 epoch bằng cách sử dụng trình tối ưu hóa Adafactor, tốc độ học 5·10−4, weight decay 0.0, và kích thước batch 16. Chúng tôi báo cáo kết quả cho chỉ một lần chạy thí nghiệm.

Baseline Chúng tôi fine-tune ruBERT-base của DeepPavlov, ruBERT-base-conv, và mBERT như được mô tả ở trên. Ngưỡng PenLP cho mGPT-

--- TRANG 6 ---
[THIS IS TABLE: Bảng 4 showing results for acceptability classification on RuCoLA test set with multiple models and metrics]

XL19 là −54.37. Chúng tôi sử dụng kết quả cho người chú thích, XLM-R, và RemBERT từ Mikhailov et al. (2022). Kết quả cho RuLeanALBERT từ bảng xếp hạng RuCoLA tính đến tháng 9 năm 2023: rucola-benchmark.com/leaderboard.

Kết quả Kết quả cho phân loại tính chấp nhận được được trình bày trong Bảng 4. Nhìn chung, các mô hình LM của chúng tôi vượt trội hơn các đối tác đơn ngôn ngữ và đa ngôn ngữ của chúng. ruRoBERTa-large nhận được hiệu suất tốt nhất trong số các mô hình LM, tuy nhiên vẫn thua kém người chú thích chuyên gia. Thứ hai tốt nhất là RuLeanALBERT, tiếp theo là FRED-T5-XL và RemBERT. Đồng thời, ruELECTRA vượt trội hơn mBERT và XLM-R. Chúng tôi quan sát thấy rằng ruGPT-3-large thể hiện tốt nhất trong số các bộ phân loại dựa trên ngưỡng, và hiệu suất ruGPT-3-medium tương tự như mGPT 1.3B. Các mô hình LM của chúng tôi khái quát hóa tốt với các câu được tạo ra bởi máy, cho thấy sự khác biệt hiệu suất nhỏ giữa các tập trong và ngoài lĩnh vực.

4.1.3. Nhận dạng tính không phù hợp
Tác vụ Chúng tôi sử dụng tập dữ liệu của Babakov et al. (2021) để đánh giá khả năng của mô hình trong việc nhận dạng các thông điệp không phù hợp, có thể bao gồm chủ đề nhạy cảm (ví dụ, tội phạm, chê bai cơ thể, và phân biệt giới tính) và gây hại cho danh tiếng của người dùng. Metric hiệu suất mục tiêu là macro-average F1-score.

19hf.co/ai-forever/mGPT

Phương pháp Chúng tôi fine-tune và đánh giá các mô hình LM encoder, decoder, và encoder-decoder như được mô tả trong §4.1.2. Các ngưỡng PenLP là −37.66 (ruGPT-3-small), −35.82 (ruGPT-3-medium), và −35.39 (ruGPT-3-large).

Baseline Chúng tôi fine-tune và đánh giá mBERT, ruBERT-base của DeepPavlov, ruBERT-base-conv, mT5-base, và mT5-large như được mô tả trong § 4.1.2. Ngưỡng PenLP cho mGPT-XL là −32.54.

Kết quả Kết quả cho nhận dạng tính không phù hợp được trình bày trong Bảng 5. Nhìn chung, tất cả các mô hình nhận được hiệu suất mạnh, và các mô hình LM encoder và decoder-only thể hiện tương đương. Hiệu suất cải thiện với việc mở rộng mô hình, ngoại trừ các mô hình decoder-only và ruT5. FRED-T5-XL cho thấy kết quả tốt nhất trong số các mô hình LM, tiếp theo là ruRoBERTa-large và FRED-T5-large.

--- TRANG 7 ---
[THIS IS TABLE: Bảng 5 showing results for inappropriateness identification with F1-scores for different models]

4.2. Sinh ngôn ngữ tự nhiên
4.2.1. Đơn giản hóa văn bản
Tác vụ RuSimpleSentEval-2021 (Sakhovskiy et al., 2021) là một kho văn bản các cặp câu bao gồm các câu phức tạp và các phiên bản đơn giản hóa của chúng. Tác vụ là viết lại câu đầu vào theo cách ít phức tạp hơn. Các metric hiệu suất là SARI (Xu et al., 2015) và BERTScore (Zhang et al., 2020) được tính toán giữa đầu vào và đầu ra bằng cách sử dụng mBERT.

Phương pháp Chúng tôi fine-tune các mô hình LM decoder và encoder-decoder bằng cách sử dụng trình tối ưu hóa AdamW, tốc độ học 5·10−5, và kích thước batch 2 trong 3 và 10 epoch tương ứng. Chiến lược giải mã và các siêu tham số để suy luận được chọn dựa trên hiệu suất validation và phân tích thủ công các đầu ra mô hình. Chiến lược kết quả là beam search với 5 beam cho tất cả các mô hình.

Baseline Chúng tôi báo cáo điểm số tham chiếu của con người và một baseline phi neural của câu đầu vào không có thay đổi nào (Câu đầu vào). Sau đó, tuân theo quy trình được mô tả ở trên, chúng tôi fine-tune mBART-large-50 (Tang et al., 2021), mGPT-XL, mT5-base, và mT5-large.

Kết quả Kết quả cho tác vụ đơn giản hóa văn bản được trình bày trong Bảng 6. Đối với tất cả các mô hình được kiểm tra ngoại trừ ruGPT3-small, BERTScore vượt quá 0.9, có nghĩa là các dự đoán đơn giản hóa rất gần với câu đầu vào với những đơn giản hóa nhẹ, chủ yếu ở mức từ. Nhìn chung, phân tích thủ công của chúng tôi về các đầu ra mô hình cho thấy metric mục tiêu (SARI) không chỉ ra

--- TRANG 8 ---
[THIS IS TABLE: Bảng 6 showing results for text simplification on RuSimpleSentEval-2021 test sets with various models and metrics]

hiệu suất dự định. Ví dụ, các mô hình LM đa ngôn ngữ (mT5 và mBART-large-50) có xu hướng sao chép hầu hết các phần của đầu vào, dẫn đến BERTScore cao (trên 0.96) và điểm số SARI mạnh. Đồng thời, SARI không phải lúc nào cũng cải thiện với việc mở rộng mô hình. Chúng tôi cũng thấy rằng các mô hình LM encoder-decoder vượt trội hơn các mô hình LM decoder-only, và ruT5-base để lại câu đầu vào không thay đổi, tương tự như mT5 và mBART-large-50. Kết quả cho thấy cần thiết phải tiến hành đánh giá dựa trên con người để có được bức tranh đầy đủ hơn về hiệu suất mô hình.

4.2.2. Tóm tắt văn bản
Tác vụ Gazeta (Gusev, 2020) là một kho văn bản các bài báo tin tức và tóm tắt của chúng cho việc tóm tắt trừu tượng. Các metric hiệu suất là các metric đánh giá tóm tắt tiêu chuẩn: ROUGE-L (Lin, 2004), BERTScore, BLEU (Papineni et al., 2002), METEOR (Banerjee và Lavie, 2005), và ChrF1 (Popović, 2015).

Phương pháp Chúng tôi fine-tune các mô hình decoder-only trong 3 epoch bằng cách sử dụng trình tối ưu hóa AdamW, bộ lập lịch tuyến tính với warmup, và tốc độ học 5·10−5. Các mô hình encoder-decoder được fine-tune với Adafactor với tốc độ học hằng số 1·10−3. Chúng tôi kiểm tra các chiến lược sinh và siêu tham số khác nhau trên tập validation. Chiến lược kết quả là beam search với 5 beam cho tất cả các mô hình LM.

Baseline Chúng tôi fine-tune mBART-large-50, mT5-base, và mT5-large như được mô tả ở trên.

[THIS IS TABLE: Bảng 7 showing results for text summarization on Gazeta with various models and metrics]

Kết quả Kết quả cho tóm tắt văn bản được hiển thị trong Bảng 7. Các điểm số cho thấy rằng hiệu suất cải thiện khi kích thước mô hình tăng lên. ruGPT-3-large đạt được điểm số cao nhất trong số các mô hình LM decoder, và FRED-T5-XL nhận được hiệu suất tốt nhất trong số các mô hình LM encoder-decoder. Phân tích thủ công các đầu ra mô hình cho thấy các mô hình ruGPT-3 có xu hướng sao chép các phần của đầu vào, trong khi các mô hình ruT5 và FRED-T5 tạo ra những tóm tắt hợp lý hơn. Nhìn chung, các mô hình LM của chúng tôi cho thấy điểm số cao hơn so với các đối tác đa ngôn ngữ của chúng.

4.2.3. Khử độc văn bản
Tác vụ Kho văn bản Khử độc RUSSE (Dementieva et al., 2022) kiểm tra khả năng của mô hình trong việc sinh ra một phiên bản khử độc của văn bản độc hại. Các metric hiệu suất dựa trên Dementieva et al. (2022): điểm số ChrF1, độ chính xác chuyển đổi phong cách, độ tương tự nội dung, tính trôi chảy, và điểm số "Joint" (tích của ba metric cuối cùng).

Phương pháp Chúng tôi tiến hành fine-tuning các mô hình LM trong năm epoch bằng cách sử dụng AdamW cho các mô hình dựa trên rGPT và Adafactor cho các mô hình dựa trên ruT5. Chúng tôi thí nghiệm với nhiều chiến lược giải mã trên tập validation, phân tích các metric hiệu suất và tiến hành phân tích thủ công các đầu ra. Chúng tôi sử dụng beam search với 5 beam và repetition penalty 1.05 ở giai đoạn suy luận.

Baseline Chúng tôi báo cáo điểm số tham chiếu của con người và kết quả baseline do Dementieva et al. (2022) cung cấp: (i) baseline tầm thường "Duplicate", để lại văn bản gốc nguyên vẹn và đóng vai trò là ngưỡng hiệu suất thấp hơn; (ii) baseline "Delete", loại bỏ các từ độc hại dựa trên từ vựng được định nghĩa trước. Ngoài ra, chúng tôi fine-tune và đánh giá mBART-large-50, mT5-base, và mT5-large với các tham số giống như các mô hình LM ở trên.

--- TRANG 9 ---
[THIS IS TABLE: Bảng 8 showing results for detoxification with performance metrics including STA, SIM, FL, Joint, and ChrF1 scores for different models]

Kết quả Kết quả khử độc văn bản được trình bày trong Bảng 8. Các điểm số cho thấy rằng các mô hình LM thể hiện cải thiện hiệu suất đáng kể so với các baseline khi xem xét điểm số "Joint" và vượt qua hiệu suất của con người về độ tương tự văn bản và tính trôi chảy. Sự khác biệt hiệu suất giữa các mô hình LM decoder-only và encoder-decoder không đáng kể. Tuy nhiên, các mô hình LM encoder-decoder thể hiện tốt hơn, với FRED-T5-XL đạt được điểm số Joint cao nhất (58.5) và điểm số ChrF1 mô hình tốt nhất (58.1).

5. Kết luận
Bài báo này giới thiệu 13 mô hình LM Transformer tiếng Nga với nhiều kiến trúc mô hình, mục tiêu tiền huấn luyện, và kích thước mô hình khác nhau. Chúng tôi đã phát hành các mô hình LM của chúng tôi trong những năm gần đây, tạo điều kiện cho những tiến bộ nghiên cứu và phát triển các giải pháp downstream chuyên biệt cho tiếng Nga. Chúng tôi cung cấp một báo cáo về thiết kế kiến trúc mô hình, kho văn bản tiền huấn luyện, và tiền huấn luyện. Chúng tôi đánh giá thực nghiệm các mô hình LM của chúng tôi, các đối tác đa ngôn ngữ của chúng, và các mô hình LM tiếng Nga mã nguồn mở khác trên các benchmark và tập dữ liệu NLP tiếng Nga tiêu chuẩn. Kết quả cho thấy các mô hình LM của chúng tôi thúc đẩy hiệu suất hiện đại trên Russian SuperGLUE và RuCoLA và khớp với hiệu suất của con người trên các tác vụ đọc hiểu máy và khử độc văn bản. Chúng tôi phác thảo các hướng nghiên cứu công việc tương lai sau đây nằm ngoài phạm vi của bài báo này: (i) phân tích hiệu suất mô hình khi dữ liệu fine-tuning bị hạn chế, (ii) khám phá tác động của thành phần kho văn bản tiền huấn luyện, (iii) các kỹ thuật khác để thích ứng các mô hình ngôn ngữ với tiếng Nga, như khởi tạo từ một mô hình LM đa ngôn ngữ, (iv) tiến hành tìm kiếm siêu tham số tối ưu hơn, và (v) thực hiện đánh giá sinh dựa trên con người. Chúng tôi nhằm tiếp tục phát triển các mô hình LM tiếng Nga mới trong tương lai.

6. Hạn chế
Kích thước ngữ cảnh hạn chế Mặc dù các mô hình LM sinh của chúng tôi đạt được kết quả mạnh và thúc đẩy hiệu suất hiện đại trên nhiều tác vụ khác nhau, kích thước cửa sổ ngữ cảnh của chúng (tối đa 2048 token) hạn chế ứng dụng mô hình trên các tác vụ ngữ cảnh dài. Chúng tôi để lại thí nghiệm với các phương pháp fine-tuning hiệu quả để mở rộng kích thước ngữ cảnh cho công việc tương lai (ví dụ, Chen et al., 2023).

Đánh giá thiên vị xã hội Các thí nghiệm đánh giá được tiến hành trong bài báo này không thể - và thực tế không thể - giải quyết tất cả các tình huống có thể xảy ra. Chúng tôi nhằm đánh giá khả năng khái quát hóa mô hình của chúng tôi trên các tập dữ liệu và benchmark học thuật tiêu chuẩn, bao gồm nhiều tác vụ hiểu biết và sinh ngôn ngữ tự nhiên khác nhau. Tuy nhiên, thiết lập thí nghiệm của chúng tôi bị hạn chế do thiếu các tài nguyên được đánh giá đồng đẳng cho các trường hợp đánh giá cụ thể, như phát hiện thiên vị xã hội, định kiến, và lời nói thù địch. Do đó, trước khi triển khai các mô hình LM của chúng tôi, các nhà phát triển nên thực hiện đánh giá an toàn cho các tình huống ứng dụng mô hình cụ thể của họ.

Đánh giá sinh ngôn ngữ Các metric hiệu suất cho các tác vụ sinh ngôn ngữ tự nhiên không phải lúc nào cũng nắm bắt được các thuộc tính cụ thể theo tác vụ (ví dụ, Fomicheva và Specia, 2019; Colombo et al., 2022; Chhun et al., 2022). Phân tích thủ công của chúng tôi về các đầu ra mô hình xác nhận những phát hiện này cho tác vụ đơn giản hóa văn bản (xem § 4.2.1). Trong khi chúng tôi tuân theo phương pháp đánh giá dựa trên sự kết hợp các metric hiệu suất tiêu chuẩn của các loại khác nhau, những metric này có thể không đánh giá toàn diện khả năng sinh của mô hình. Chúng tôi đề xuất rằng đánh giá mô hình side-by-side dựa trên con người có thể giúp có được bức tranh đầy đủ về hiệu suất.

Chuyển đổi lĩnh vực Kho văn bản tiền huấn luyện của các mô hình LM của chúng tôi có nhiều lĩnh vực khác nhau, bao gồm lĩnh vực chung, tin tức, sách, văn bản web, và phụ đề. Tuy nhiên, tiền huấn luyện các mô hình LM20 trên các kho con khác nhau có thể cản trở hiệu suất của chúng trong các ứng dụng cụ thể theo lĩnh vực và trên dữ liệu ngoài lĩnh vực. Tuy nhiên, chúng tôi thực nghiệm cho thấy các mô hình LM của chúng tôi nhận được hiệu suất mạnh trên các lĩnh vực không được đại diện tốt trong kho văn bản tiền huấn luyện, từ các ấn phẩm ngôn ngữ học (§ 4.1.2) đến thông điệp người dùng (§ 4.1.3).

20Nhớ lại rằng các mô hình LM của chúng tôi đã được tiền huấn luyện trong vài năm qua, và sự lựa chọn lĩnh vực và kích thước kho con dựa trên nhiều yếu tố (xem § 3.1).

--- TRANG 10 ---
7. Cân nhắc đạo đức
Việc phát triển các mô hình LM mới được chi tiết trong bài báo này tuân thủ các hướng dẫn đạo đức tiêu chuẩn. Chúng tôi ủng hộ việc sử dụng có trách nhiệm và công bằng những mô hình này, cân nhắc cẩn thận các tác động xã hội tiềm năng của chúng. Sự chú ý đặc biệt được dành cho việc lọc nội dung có hại và đảm bảo một phạm vi đa dạng các quan điểm và nguồn được bao gồm trong kho văn bản tiền huấn luyện mô hình. Hơn nữa, chúng tôi nhận ra tầm quan trọng của việc giám sát liên tục trong việc theo dõi và giải quyết các hậu quả không mong muốn của việc triển khai những mô hình này trong các ứng dụng thực tế.

Khả năng lạm dụng Chúng tôi tin rằng nghiên cứu của chúng tôi không nên tham gia vào việc tạo ra nội dung có thể ảnh hưởng đến sức khỏe cá nhân hoặc cộng đồng, bao gồm (i) ứng dụng lập pháp hoặc kiểm duyệt, (ii) thông tin sai lệch, xâm phạm quyền tiếp cận thông tin, (iii) phi nhân hóa, thể hiện sai lệch, hoặc các đại diện có hại khác của con người hoặc tôn giáo, văn hóa, niềm tin của họ, (iv) thúc đẩy nội dung có hại hoặc phân biệt đối xử.

Thiên vị và chất lượng dữ liệu Dữ liệu tiền huấn luyện cho một số mô hình được trình bày bao gồm các phân đoạn lớn từ lĩnh vực internet và do đó chứa nhiều định kiến và thiên vị khác nhau. Do đó, vẫn cần đánh giá mô hình phù hợp để khám phá các lỗ hổng có thể có của chúng trong việc khái quát hóa với dữ liệu ngoài lĩnh vực.

Hiệu quả năng lượng và sử dụng Chúng tôi tính toán phát thải CO2 từ tiền huấn luyện các mô hình LM của chúng tôi theo Phương trình 1 (Strubell et al., 2019):

CO2 = PUE * kWh * ICO2 / 1000 (1)

Hiệu quả sử dụng năng lượng (PUE) của các trung tâm dữ liệu của chúng tôi là 1.3. Phát thải CO2 tính bằng kg được trình bày trong Bảng 9. Các kỹ thuật nén mô hình và phương pháp fine-tuning hiệu quả tham số có thể giảm chi phí tính toán liên quan đến suy luận mô hình. Lưu ý rằng trong khi các mô hình ruELECTRA kém hiệu suất hơn baseline trên một số tác vụ hiểu biết ngôn ngữ tự nhiên (ví dụ, đọc hiểu máy), những mô hình LM này rất hiệu quả do kích thước của chúng (ví dụ, các phiên bản nhỏ và trung bình có lần lượt 42M và 85M). Chúng tôi khuyến nghị người dùng tiến hành đánh giá riêng cho tác vụ downstream quan tâm, tính đến cả hiệu suất và hiệu quả.

[THIS IS TABLE: Bảng 9 showing CO2 emissions of pretraining models with values in kg for different model types]

8. Tài liệu tham khảo thư mục
[Extensive bibliography follows with multiple academic references in standard format]

--- TRANG 11 ---
[Bibliography continues with more academic references]

--- TRANG 12 ---
[Bibliography continues with more academic references]

--- TRANG 13 ---
[Bibliography continues with more academic references]

--- TRANG 14 ---
[Bibliography continues with more academic references]

--- TRANG 15 ---
[Bibliography continues with more academic references]

--- TRANG 16 ---
9. Tài liệu tham khảo nguồn ngôn ngữ
[Language resource references section with multiple citations]

--- TRANG 17 ---
10. Phụ lục
10.1. Giá trị siêu tham số
[THIS IS TABLE: Bảng 10 showing optimal hyperparameter values found in experiments, with columns for Model, Optimizer, Learning Rate, Weight Decay, and Batch Size across different tasks]

--- TRANG 18 ---
10.2. Mẫu Russian SuperGLUE
[THIS IS TABLE: Bảng 11 showing example templates for Russian SuperGLUE tasks with Model, Format, and Labels columns for different tasks like LiDiRus, RCB, PARus, MuSeRC, TERRa, RUSSE, RWSD, DaNetQA, and RuCoS]
