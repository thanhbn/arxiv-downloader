# GECTurk: Bộ dữ liệu Phát hiện và Sửa lỗi Ngữ pháp cho Tiếng Thổ Nhĩ Kỳ

Atakan Kara, Farrin Marouf Safian, Andrew Bond, Gözde Gül Şahin
Khoa Kỹ thuật Máy tính
Đại học Koç, Istanbul, Thổ Nhĩ Kỳ
https://gglab-ku.github.io/

## Tóm tắt
Các công cụ Phát hiện và Sửa lỗi Ngữ pháp (GEC) đã được chứng minh là hữu ích cho người bản xứ và những người học ngôn ngữ thứ hai. Việc phát triển các công cụ như vậy đòi hỏi một lượng lớn dữ liệu song song, có chú thích, điều này không có sẵn cho hầu hết các ngôn ngữ. Việc tạo dữ liệu tổng hợp là một thực tiễn phổ biến để khắc phục tình trạng khan hiếm dữ liệu như vậy. Tuy nhiên, điều này không đơn giản đối với các ngôn ngữ có hình thái phong phú như tiếng Thổ Nhĩ Kỳ do các quy tắc viết phức tạp đòi hỏi thông tin ngữ âm học, hình thái học và cú pháp. Trong công trình này, chúng tôi trình bày một pipeline tạo dữ liệu tổng hợp linh hoạt và có thể mở rộng cho tiếng Thổ Nhĩ Kỳ bao gồm hơn 20 quy tắc ngữ pháp và chính tả được chuyên gia biên soạn (tức là các quy tắc viết) được triển khai thông qua các hàm biến đổi phức tạp. Sử dụng pipeline này, chúng tôi tạo ra 130.000 câu song song chất lượng cao từ các bài báo được biên tập chuyên nghiệp. Ngoài ra, chúng tôi tạo ra một bộ kiểm tra thực tế hơn bằng cách chú thích thủ công một tập hợp các đánh giá phim. Chúng tôi triển khai ba baseline công thức hóa nhiệm vụ như i) dịch máy thần kinh, ii) gắn thẻ chuỗi, và iii) điều chỉnh tiền tố với mô hình decoder-only được huấn luyện trước, đạt được kết quả mạnh mẽ. Hơn nữa, chúng tôi thực hiện các thí nghiệm toàn diện trên các tập dữ liệu ngoài miền để có được hiểu biết về khả năng chuyển giao và độ bền vững của các phương pháp được đề xuất. Kết quả của chúng tôi cho thấy rằng corpus của chúng tôi, GECTurk, có chất lượng cao và cho phép chuyển giao kiến thức cho bối cảnh ngoài miền. Để khuyến khích nghiên cứu thêm về Turkish GEC, chúng tôi phát hành các tập dữ liệu, mô hình baseline và pipeline tạo dữ liệu tổng hợp tại https://github.com/GGLAB-KU/gecturk.

## 1 Giới thiệu
Sửa lỗi Ngữ pháp (GEC) là một trong những nhiệm vụ NLP được thiết lập tốt với các nhiệm vụ chia sẻ chuyên dụng (ví dụ: BEA (Bryant et al., 2019)), các tiêu chuẩn đánh giá, và thậm chí các thước đo đánh giá cụ thể. Với sự quan tâm ngày càng tăng từ cộng đồng, lĩnh vực này liên tục cần các công cụ viết mới, phương pháp mới, và quan trọng hơn, các mở rộng cho các ngôn ngữ khác.

Gần đây, đã có sự bùng nổ nghiên cứu về GEC cho các ngôn ngữ có nhiều tài nguyên, đặc biệt là tiếng Anh (Rothe et al., 2021; Omelianchuk et al., 2020; Bryant et al., 2019). Các kỹ thuật gần đây này sử dụng hai phương pháp chính: công thức hóa nhiệm vụ như i) dịch máy thần kinh, tức là sinh ra (Rothe et al., 2021) và ii) phân loại token để phát hiện các token lỗi (Omelianchuk et al., 2020). Tập hợp phương pháp đầu tiên chủ yếu sử dụng và kỹ thuật hóa Transformers vanilla để sinh ra văn bản đã sửa, trong khi tập hợp thứ hai tập trung vào việc kỹ thuật hóa một tập hợp các lỗi và quy tắc biến đổi. Tuy nhiên, cả hai công thức đều đòi hỏi một tập hợp lớn các corpus song song chứa các cặp câu đúng và sai ngữ pháp. Hơn nữa, phương pháp thứ hai còn đòi hỏi một tập dữ liệu được biên soạn cẩn thận với các chú thích để sửa lỗi (tức là vị trí và loại lỗi). Tuy nhiên, việc xây dựng một corpus song song như vậy với các chú thích lỗi là không tầm thường—đặc biệt đối với các ngôn ngữ ít tài nguyên với hình thái phong phú như tiếng Thổ Nhĩ Kỳ. Thách thức này là do các quy tắc ngữ pháp, tức là các lỗi viết bị vướng vào trong nhiều lớp, như ngữ âm học, hình thái học, cú pháp và ngữ nghĩa. Tính đến hôm nay, không có tập dữ liệu lỗi chính tả hoặc ngữ pháp nào, như được đề cập bởi Çöltekin et al. (2023), ngoại trừ tập dữ liệu được giới thiệu bởi Arikan et al. (2019).

Để giải quyết vấn đề này, chúng tôi tập trung vào Tiếng Thổ Nhĩ Kỳ và sử dụng các quy tắc viết chính thức được thiết lập bởi Hiệp hội Ngôn ngữ Thổ Nhĩ Kỳ¹. Chúng tôi triển khai các hàm tham nhũng, tức là biến đổi, để tạo ra các trường hợp vi phạm một quy tắc cụ thể, điều này đòi hỏi phân tích thách thức các câu trên nhiều cấp độ ngôn ngữ học, cũng như việc biên soạn các từ điển chuyên biệt. Sau đó, chúng tôi tạo ra một corpus chú thích tổng hợp, chất lượng cao lớn bằng cách áp dụng các hàm biến đổi cho các bài báo tiếng Thổ Nhĩ Kỳ hiện đại được biên tập chuyên nghiệp. Ngoài các hàm biến đổi được sử dụng để tạo dữ liệu, chúng tôi triển khai và chia sẻ các hàm biến đổi ngược để xác thực các tập dữ liệu được tạo ra và phát triển các mô hình sequence tagger, điều này đạt được kết quả tiên tiến nhất trong tiếng Anh.

Ngoài ra, chúng tôi biên soạn một corpus các đánh giá phim và chú thích thủ công 300 câu với các loại lỗi được đề xuất để đánh giá các mô hình trong bối cảnh thực tế. Hơn nữa, chúng tôi thiết kế và triển khai một số baseline sử dụng dịch máy thần kinh tiêu chuẩn (NMT), gắn thẻ chuỗi và điều chỉnh tiền tố. Trong khi các mô hình NMT chỉ được huấn luyện để sinh ra các câu đã sửa, các mô hình gắn thẻ chuỗi được huấn luyện để gắn thẻ các token với loại lỗi (nếu có) và sau đó thực hiện hàm biến đổi ngược liên quan trên lỗi được phát hiện để tạo ra văn bản đúng. Cuối cùng, chúng tôi thực hiện điều chỉnh tiền tố (Li và Liang, 2021) trên mô hình ngôn ngữ đa ngữ lớn, mGPT Shliazhko et al. (2022) cho cả nhiệm vụ phát hiện và sửa để kiểm tra khả năng của các kỹ thuật gần đây hơn.

Các phát hiện của chúng tôi chỉ ra rằng phương pháp pipeline của chúng tôi sử dụng các mô hình nhỏ hơn hoạt động tốt hơn so với việc sử dụng các mô hình được huấn luyện trước lớn hơn theo cách end-to-end khi nói đến cả tập dữ liệu tổng hợp và thực tế—đặc biệt là đối với nhiệm vụ phát hiện lỗi ngữ pháp. Ngược lại, chúng tôi quan sát thấy rằng việc huấn luyện trước có lợi cho các mô hình trong việc xử lý các trường hợp thực tế hơn, mặc dù các mô hình lớn hơn vẫn tụt hậu so với các đối tác đơn giản hơn của chúng. Kết quả của chúng tôi từ các thử nghiệm ngoài miền ngụ ý rằng việc huấn luyện trên tập dữ liệu tổng hợp mang lại một tiên nghiệm mạnh mẽ cho cả mô hình nhỏ hơn và lớn hơn.

Các đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi đề xuất lược đồ lỗi ngữ pháp toàn diện, được chuyên gia biên soạn đầu tiên cho tiếng Thổ Nhĩ Kỳ bao gồm 25 loại lỗi.
• Chúng tôi trình bày một pipeline tạo dữ liệu tổng hợp có thể được sử dụng để tạo ra các tập dữ liệu có kích thước tùy ý, và có thể dễ dàng mở rộng để bao gồm các loại lỗi ngữ pháp mới hoặc từ điển, và có thể dễ dàng sửa đổi để bao gồm các công cụ tùy chỉnh (ví dụ: bộ phân tích hình thái và bộ khử nhập nhằng).
• Chúng tôi trình bày tập dữ liệu công khai quy mô lớn, chi tiết đầu tiên cho việc sửa và phát hiện ngữ pháp tiếng Thổ Nhĩ Kỳ, cùng với một tập kiểm tra thực tế được chú thích thủ công và các mô hình baseline mạnh mẽ.

Chúng tôi công khai các tập dữ liệu, mô hình baseline và pipeline tạo dữ liệu tổng hợp tại https://github.com/GGLAB-KU/gecturk.

## 2 Công trình liên quan
**English GEC** Mặc dù có lịch sử lâu dài, với BEA-2019 Shared Task on Grammatical Error Correction (Bryant et al., 2019), cộng đồng GEC bắt đầu sử dụng các mô hình thần kinh và công thức hóa GEC như một nhiệm vụ dịch máy thần kinh (tức là dịch từ các câu sai ngữ pháp sang câu đúng), điều này đã trở thành phương pháp thống trị. Một phương pháp gần đây khác, GECToR (Omelianchuk et al., 2020), sử dụng ý tưởng của các biến đổi ngược, có thể được áp dụng cho một danh sách các token nguồn [x1, . . . , xn], để tạo ra ngữ pháp đúng mong muốn. Mô hình của họ là một sequence tagger với một bộ mã hóa BERT. Mỗi thẻ tương ứng với một biến đổi trong đó các biến đổi được áp dụng sau khi việc gắn thẻ chuỗi kết thúc. Ngược lại, mô hình gT5 được phát hành bởi Xue et al. (2021) là một mô hình mT5 đa ngữ được tinh chỉnh trên các câu bị tham nhũng nhân tạo từ corpus mC4 và sử dụng một nhiệm vụ dự đoán và phân loại span để sửa lỗi ngữ pháp (Rothe et al., 2021). Điều này đòi hỏi rất nhiều thời gian huấn luyện bổ sung, vì mô hình mT5 ban đầu không được chuẩn bị cho một nhiệm vụ tương tự. Mô hình của họ đạt được kết quả SOTA ở 4 ngôn ngữ trong khi chỉ huấn luyện một lần.

**Turkish GEC** Trước đây, Arikan et al. (2019) đã đề xuất một mô hình sequence tagger thần kinh và một tập dữ liệu được tạo ra tổng hợp để sửa lỗi clitic "de/da". Trong ngữ pháp tiếng Thổ Nhĩ Kỳ, "de/da" được sử dụng cả như một hậu tố chỉ nơi chốn và một liên từ có nghĩa là cũng, too được viết riêng biệt. Ví dụ, "-de" là một hậu tố chỉ nơi chốn trong câu "Ev de (Ở nhà)"; trong khi được sử dụng như một liên từ ở đây: "Ben de geliyorum (Tôi cũng đang đến)". Lỗi trong việc sử dụng các clitic này là phổ biến trong số người bản xứ, thường là do một số sắc thái ngữ cảnh và phương ngữ nói ảnh hưởng đến ngôn ngữ viết. Öztürk et al. (2020) đã kết hợp một mô hình word embedding ngữ cảnh, cụ thể là BERT, với một sequence tagger để sửa lỗi clitic "de/da". Mặc dù những lỗi này là phổ biến, chúng chỉ chiếm một phần nhỏ các lỗi ngữ pháp được thực hiện bởi người bản xứ. Ngoài ra, trong khi có nhiều hình thức khác nhau của lỗi này, công trình trước đây chỉ xem xét một vài. Chiến lược tạo dữ liệu của chúng tôi xem xét nhiều phiên bản của lỗi clitic "de/da" và nhiều lỗi ngữ pháp phổ biến khác.

**Phương pháp dựa trên Phân tích cú pháp** Trong khi hầu hết các phương pháp hiện tại tập trung vào các biến đổi ngược và gắn thẻ chuỗi, có một số nghiên cứu liên quan đến việc sử dụng các kỹ thuật phân tích cú pháp. Flickinger và Yu (2013) tạo ra một cây phân tích cú pháp, và xác định các phần bị biến dạng của cây để phát hiện lỗi ngữ pháp. da Costa (2021) sử dụng các bộ phân tích cú pháp tượng trưng và ngữ pháp tính toán cho GEC và GED. Trên cùng một hướng nghiên cứu, Flickinger và Packard sử dụng các phân tích cầu nối kết hợp với phân tích cú pháp để kết nối tốt hơn hai cụm từ trong Head-driven Phrase Structure Grammar (HPSG).

## 3 Tạo Dữ liệu Tổng hợp
Quá trình tạo tổng thể được đưa ra trong Hình 1. Đầu tiên, chúng tôi lấy mẫu ngẫu nhiên từ các corpus tiếng Thổ Nhĩ Kỳ được biên tập chuyên nghiệp (§3.1). Sau đó, các câu bị tham nhũng—nếu có thể—theo các quy tắc biến đổi được chuyên gia biên soạn được giải thích trong §3.2, cũng như việc sử dụng bộ phân tích hình thái. Cuối cùng, các cặp câu đúng ngữ pháp và bị tham nhũng được thêm vào corpus Turkish GEC cuối cùng theo định dạng dữ liệu M2scorer (Dahlmeier và Ng, 2012) (MaxMatch).

### 3.1 Corpus
Pipeline tạo dữ liệu được đề xuất của chúng tôi được xây dựng dựa trên giả định rằng tất cả các câu đầu vào đều đúng ngữ pháp. Do đó, chúng tôi dựa nghiên cứu của mình trên các corpus báo chí được biên soạn trước đây (Diri và Amasyali, 2003; Amasyalı và Diri, 2006; Can và Amasyalı, 2016; Kemik NLP Group, 2022) được đọc hiệu đính và trải qua một quá trình biên tập chuyên nghiệp. Các bài báo về nhiều chủ đề khác nhau, bao gồm chính trị, thể thao và y học, và đã được viết bởi hơn 95 tác giả cho ba tờ báo khác nhau; tổng cộng, hơn 7000 tài liệu do một tác giả duy nhất viết đã được thu thập từ 2004-2012. Khi chúng tôi có được các câu nguồn đúng ngữ pháp, chúng tôi thực hiện một số bước tiền xử lý, chẳng hạn như loại bỏ các bản sao (2,9% của tập dữ liệu kết hợp), kết thúc với 138K câu độc đáo.

### 3.2 Biến đổi
Hiệp hội Ngôn ngữ Thổ Nhĩ Kỳ (TDK)², một cơ quan chính phủ được thành lập năm 1932, chịu trách nhiệm cung cấp các tài nguyên để tiến hành nghiên cứu khoa học về các nguồn viết và nói của tiếng Thổ Nhĩ Kỳ. Trong phạm vi này, họ chỉ định và duy trì một danh sách toàn diện các quy tắc viết có sẵn công khai³. Chúng tôi dựa vào danh sách được chuyên gia biên soạn này để tạo ra các quy tắc biến đổi tiến và lùi, mà chúng tôi gọi là f và f^(-1) tương ứng. Tuy nhiên, danh sách này dài, và một số quy tắc viết là trực quan đối với người bản xứ, vì vậy bất kỳ lỗi nào được thực hiện trên các quy tắc này đều nghe có vẻ bất thường đối với họ. Chúng tôi chọn các quy tắc ngữ pháp được sử dụng sai phổ biến nhất bởi người bản xứ, được xác định bằng cách tham khảo với các chuyên gia ngôn ngữ Thổ Nhĩ Kỳ và lọc danh sách từ TDK bằng cách sử dụng phản hồi của họ. Chúng tôi không bao gồm bất kỳ quy tắc nào phổ biến đối với người học tiếng Thổ Nhĩ Kỳ nhưng hiếm khi được thực hiện bởi người bản xứ. Bảng 1 cung cấp danh sách đầy đủ các quy tắc biến đổi được sản xuất bởi công trình này. Các biến đổi dựa vào một bộ phân tích hình thái, điều này là cần thiết để có được các biến đổi đúng cho một ngôn ngữ có hình thái phong phú như tiếng Thổ Nhĩ Kỳ. Để biết thêm thông tin về Hình thái học Thổ Nhĩ Kỳ, chúng tôi tham khảo Oflazer (2014) và Lewis (1985).

**Áp dụng f** Đối với mỗi câu được lấy mẫu, đầu tiên, chúng tôi xáo trộn danh sách các f. Điều này đảm bảo rằng các hàm biến đổi loại trừ lẫn nhau được áp dụng với tần suất mong muốn. Sau đó, chúng tôi áp dụng lặp đi lặp lại từng f trên câu được đưa ra với mã giả trong Thuật toán 1. Ở đây, f nhận một câu đầu vào s, phân tích hình thái của câu Ms, một mảng các chỉ số cho biết liệu bất kỳ biến đổi nào đã được áp dụng cho từ— flags, và tham số p ∈ (0,1). Thuật toán, sau đó, lặp qua các token (hoặc cặp) và kiểm tra xem token đã được biến đổi chưa. Nếu không, nó kiểm tra xem token(s) có đủ điều kiện cho f không. Nếu đủ điều kiện, chúng tôi áp dụng f với xác suất p, vì không phải tất cả các lỗi đều được thực hiện với cùng một tần suất bởi người bản xứ.⁴

**Kiểm tra Đủ điều kiện** Một số quy tắc viết chính thức đòi hỏi phân tích cú pháp ở cấp độ token và câu. Ví dụ, để áp dụng hàm biến đổi CONJ_DE_SEP, người ta phải thực hiện phân tích hình thái và khử nhập nhằng để phân tích các thẻ part-of-speech ở cấp độ morpheme. Tức là, biến đổi CONJ_DE_SEP chỉ có thể được áp dụng nếu một morpheme "-de/da" với thẻ part-of-speech CONJUNCTION được tìm thấy. Ngoài ra, một tập hợp nhỏ các quy tắc đòi hỏi các từ điển chuyên biệt, ví dụ: một danh sách các từ ngoại lai đặc biệt cho FOREIGN_R2_EXC. Để giải quyết vấn đề trước, chúng tôi sử dụng một bộ phân tích hình thái tiên tiến Dayanik et al. (2018), và các từ điển được lấy từ các danh sách chính thức được cung cấp bởi TDK².

⁴Chúng tôi chọn các xác suất một cách trực quan sau một phân tích ban đầu trên corpus web và tiểu luận của sinh viên.

[Phần còn lại của tài liệu được dịch theo cách tương tự...]
