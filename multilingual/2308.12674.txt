# 2308.12674.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2308.12674.pdf
# File size: 991484 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Improving Translation Faithfulness of Large Language Models via
Augmenting Instructions
Yijie Chen1, Yijin Liu2, Fandong Meng2, Yufeng Chen1, Jinan Xu1, Jie Zhou2
1Beijing Jiaotong University, Beijing, China
2Pattern Recognition Center, WeChat AI, Tencent Inc, China
{22120354, chenyf, jaxu}@bjtu.edu.cn
{yijinliu, fandongmeng, withtomzhou}@tencent.com
Abstract
Large Language Models (LLMs) present strong
general capabilities, and a current compelling
challenge is stimulating their specialized capa-
bilities, such as machine translation, through
low-cost instruction tuning. The standard
instruction-following data is sequentially or-
ganized as the concatenation of an instruction,
an input, and a response. As the attention mech-
anism of LLMs has limitations on local fo-
cus, LLMs tend to focus more on the words
or sentences nearby at each position. This
leads to a high risk of instruction forgetting
during decoding. To alleviate the above is-
sues, We propose SWIE (Segment- Weighted
Instruction Embedding) and an instruction-
following dataset OVERMISS.SWIE im-
proves the model instruction understanding by
adding a global instruction representation on
the following input and response representa-
tions. OVERMISSimproves model faithful-
ness by comparing over-translation and miss-
translation results with the correct translation.
We apply our methods to two mainstream open-
source LLMs, BLOOM and LLaMA. The ex-
perimental results demonstrate significant im-
provements in translation performance with
SWIE based on BLOOMZ-3b, particularly in
zero-shot and long text translations due to re-
duced instruction forgetting risk. Additionally,
OVERMISS outperforms the baseline in trans-
lation performance ( e.g.an increase in BLEU
scores from 0.69 to 3.12 and an average im-
provement of 0.48 percentage comet scores for
LLaMA-7b) with further enhancements seen
in models combining OVERMISS and SWIE
(e.g.the BLUE scores increase up to 0.56 from
English to German across three different back-
bones), and both exhibit improvements in the
faithfulness metric based on word alignment.1
1Our code and datasets are released in Github:
https://github.com/pppa2019/swie_overmiss_llm4mt1 Introduction
In recent years, pre-trained language models
(PLMs) have experienced a burgeoning growth and
have been extensively investigated and employed
in downstream tasks. However, large language
models (LLMs) exhibit surprising emergent abili-
ties (Wei et al., 2022) which have not been observed
in small PLMs, and LLMs have shown significant
ability on general tasks and zero-shot or few-shot
settings, even including symbolic reasoning, com-
monsense, algorithm, and so on.
Super LLMs like GPT-4 and ChatGPT, which
can only be used via API, have demonstrated re-
markable translation performance without fine-
tuning (Jiao et al., 2023b; Hendy et al., 2023). For
general LMs, fine-tuning is a prevailing approach to
adapt to specific downstream tasks. Consequently,
the fine-tuning of relatively smaller open-source
LLMs presents an attractive alternative, given that
it can augment the model’s translation capabili-
ties without imposing significant computational
costs (Jiao et al., 2023a). Nonetheless, instruction
tuning on LLMs in machine translation remains a
field that has not been fully explored.
Although the de facto architecture of state-of-the-
art models in machine translation remains encoder-
decoder (Bahdanau et al., 2015; Gao et al., 2022),
the majority of the open-source LLMs adopt the
causal language model (causal LM) architecture.
However, the core limitation of causal LM is the
local focus (Liu et al., 2023) of its attention mech-
anism, which leads to the model’s tendency to fo-
cus on nearby words or sentences at each position.
Consequently, in the instruction fine-tuning data,
the instruction text is further away from the output
compared to the input text, increasing the risk of
instruction forgetting during decoding. In machine
translation, ignoring instructions can lead to issues
such as hallucinations or unfaithfulness, ultimately
reducing the quality and credibility of the models.arXiv:2308.12674v1  [cs.CL]  24 Aug 2023

--- PAGE 2 ---
This paper introduces a novel method for im-
proving instruction tuning named SWIE (Segment-
Weighted Instruction Embedding), which utilizes
parameterized adapters to encode instruction and
introduces segmented weight to enable a natural in-
tegration of instruction representations and global
representations. In order to further improve the
model translation faithfulness, we present OVER-
MISS, an instruction dataset that utilizes our pro-
posed framework to collect contrastive negative
samples that specifically target over-translation and
miss-translation issues.
We evaluate our methods on different machine
translation benchmarks and various backbone mod-
els (including BLOOMZ-3b, BLOOMZ-7b1-mt,
and LLaMA-7b). Results on BLOOMZ-3b show
SWIE has improved from 0.19 to 0.51 BLEU
scores on four translation directions of WMT22
test sets, from 0.20 to 0.58 BLEU scores on six
zero-shot translation directions, and 0.67 BLEU
scores in average on long sentence test sets. Ad-
ditionally, OVERMISSalso leads to significant im-
provements ( e.g.for WMT22 test sets, an increase
in BLEU scores from 0.69 to 3.12 and an increase
in 0.48 percentage COMET score on average on
LLaMA-7b on WMT). The combination of SWIE
andOVERMISSachieves a further improvement
up to 0.56 BLEU scores on three backbone models
from English to German.
In summary, our contributions are as follows:
•We propose SWIE , a novel segment-weighted
instruction embedding method, which effec-
tively improves translation performance and
faithfulness, and its effectiveness is more sig-
nificant in the zero-shot and longer text set-
tings since the strengthening of instruction-
following ability.
•We propose a translation faithfulness con-
trastive instruction-tuning data construction
method and construct OVERMISS. We demon-
strate that OVERMISSconsistently improves
the translation performance on three backbone
models and two test sets ( e.g.on LLaMA-7b,
increase up to 3.12 BLEU score on WMT22
test sets and up to 3.03 BLEU score on FLO-
RES test sets.)
•By examining the internal attention scores of
the models, we discovered that SWIE leads to
a higher attention ratio for instructions com-
pared with baseline, thereby validating ourhypothesis and effectively substantiating its
efficacy in mitigating the instruction forget-
ting problem.
Figure 1: The model structure of SWIE.
2 Related work
Our work is closely related to machine translation,
the variants of instruction tuning for LLMs, and
hallucination in text generation. We will provide a
brief overview of these areas in this section.
2.1 Machine Translation based on LLMs
Owing to the strong zero-shot and instruction-
following abilities of LLMs, super LLMs like
GPT-4 have achieved comparable translation per-
formance to the best system on WMT system in
high-resources translation direction on translation
and relevant tasks like post-editing (Raunak et al.,
2023; He et al., 2023).
The aforementioned study exclusively employs
models that are only accessed via API, thereby
limiting its applicability. Consequently, numerous
studies have been conducted to investigate the po-
tential of fine-tuning open-source LLMs. In the
context of instruction tuning LLMs for machine
translation, (Jiao et al., 2023a; Zhang et al., 2023)
have proposed multi-task instruction data construc-
tion frameworks for instruction tuning open-source
LLMs on machine translation. (Zeng et al., 2023)
proposed a contrastive learning loss in order to train
the model to learn contrastive sample pairs.
2.2 Instruction Tuning
The first work on instruction tuning is FLAN (Wei
et al., 2021), which shows a surprising result on

--- PAGE 3 ---
Figure 2: An instance of translation instruction and an instance of O VERMISS.
zero-shot and few-shot settings. There are a lot
of follow-up works proposed to construct larger-
scale instruction datasets. The instruction tuning
datasets adopt different instruction and language
styles: FLAN (Longpre et al., 2023) use “input”
and “target”; unnatural instruction(Honovich et al.,
2022) use “instruction”, “input”, “constrain” and
“output”; Super-NaturalInstructions(Wang et al.,
2022) constructs positive and negative sample for
each task. As a unified and scaling-up dataset, OPT-
IML casts all the above datasets to “instruction”
and “output” segments.
Due to the fact that instructions serve as the de-
finetion of tasks and are typically located at the
beginning of samples, the representation of instruc-
tions in causal LMs face a higher risk of being
forggoten during decoding. To allevate this is-
sue, there are currently some efforts proposing
improved methods that differ from the standard
fine-tuning approaches, in order to enhance the
learning in instruction components.
(Ye et al., 2022) models the instruction in the
condition given input and target, thereby alleviating
the demands of long context modeling. (Choi et al.,
2022) proposed a distilling-based context injection
method to preserve the long context information in
the fixed model when the model is used in static
long prompts situations.
As the above methods require higher demands
for data and task scenarios, such as fixed instruc-
tions as conditions. They cannot meet the condition
of machine translation, which typically only con-
tains short instructions that indicate the translation
direction.2.3 Hallucinations in Language Models
Hallucinations in neural machine translation have
been discussed for a long time (Lee et al., 2018;
Müller et al., 2020), and it has the same mean as un-
faithfulness. It is widely observed that the sources
of hallucination or unfaithfulness can be the lack
of knowledge or inadequate attention to the source
(Ferrando et al., 2022; Raunak et al., 2021).
On machine translation hallucination detection
benchmarks, we found that existing datasets are
constructed by humans or perturbing the transla-
tion model (Raunak et al., 2021). Human-making
datasets like HalOmi (Dale et al., 2023) are highly
cost and hard to scale up. Datasets generated
by the model perturbing method have low qual-
ity because the sentences generated are far from
both the natural distribution and the distribution of
modern LLMs. Thus, our proposed hallucination-
mimicking dataset construction method can fill the
gap with high-quality fluent negative samples.
3 Method
In this section, we propose a contrastive faithful-
ness translation instruction dataset OVERMISSand
a global instruction fusion method SWIE . Before
introducing the proposed method, the necessary
background will be formulated first.
3.1 Backgound
3.1.1 Instruction Tuning Formalization
Instruction tuning is one of the alignment methods
to make language models meet human preferences.
To formulate instruction tuning, we define s,x, and

--- PAGE 4 ---
yas the instruction, the input, and the target, respec-
tively. Noting that the input is not necessary but
the instruction is needed all the time. The standard
instruction tuning is trained with maximum likeli-
hood estimation (MLE), and the training objection
can be calculated by Equation (1). Furthermore,
due to the attention mechanism tends to pay more
attention to the text nearby, the instruction part
faces a higher risk to be forgotten in the generation
process.
LMLE =−TX
t=1logP(yt|y<t;x;s) (1)
3.1.2 Causal Language Model
Decoder-only architecture is designed for unified
text generation tasks, including prefix decoder and
causal decoder (Raffel et al., 2020). Most of the
LLMs use the causal decoder architecture because
of the wide observation of scaling law on the causal
decoder. However, a more comprehensive inves-
tigation of other architectures’ performance at a
large scale is still lacking. (Zhao et al., 2023)
A causal language model is composed of a stack
of causal decoder layers. The function of the multi-
head attention mechanism is to combine the hid-
den representation of each position with contextual
information. With a causal attention mask, text
generation tasks in any format can be unified in
training and decoding states. In detail, let m,n
be the position indexes of two tokens, and q,k,v,
oare, respectively, query, key, value, and output
representation, and the length of input tokens is
N. In a causal language model, when m > n , the
attention score am,nwill be masked.
am,n=exp
q⊤
mkn√
d
PN
j=1expq⊤mkj√
d (2)
om=NX
n=1am,nvn (3)
3.2 SWIE: Segment-weighted Instruction
Embedding
We propose segment-weighted instruction embed-
ding in order to strengthen global instruction at-
tention for decoders, and the details are described
as follows. Instruction tuning can be divided into
several segments, including instruction, input, re-
sponse, and so on. The sentence will be converted
Figure 3: An illustration of segmented-weight
to a list of tokens after the tokenizer. We define
a segment ID for each segment and then map the
segment index to every token of the tokens list.
Assuming a sentence tokens list is represented as
S= [s1,s2,···,sl]∈R1×N, and its segment ids
list is Is, which is also an array with length N. Let
thecbe the ID of the instruction span, and the bbe
the array record of the beginning indexes of each
span. The encoded instruction representation can
be obtained in the output of each decoder layer,
we use an instruction adapter to re-parameterize in-
struction. We set a segment weight to constrain the
fusion of instruction representation on input and re-
sponse segments. Let Lbe the length of the tokens
list of the input span, Bbe the array that records
the beginning position index of each segment, and
we use Lto standardize the slope. The Hlrepre-
sents the hidden output of lthlayer and the Hinsl
represents the max pool result of the instruction
part in Hl. We use a down-sampling linear layer,
an activation layer, and an up-sampling linear layer
as the adapter.
On implementation details, we selected the mid-
dle three layers of the language model to fuse the
extracted instruction feature with the global hidden
representation. The selection principle is based
on our analysis of the attention score distribution
of each layer, and the detailed analysis process is
shown in Section. Visualize Inadequate Attention
on Instruction . The model structure is visualized
in Figure.1, and the process is described as Equa-
tion (4-6), and the illustration of segmented-weight
is shown in Figure.3.
Hl:=Hl+Wseg·f(Hinsl) (4)
Wsegi=0 Is[i] =c
i−B[Is[i]]
LIs[i]̸=c(5)
f(Hinsl) =Lup(σ(Ldown (Hinsl))) (6)

--- PAGE 5 ---
3.3 O VERMISS: A Natural Hallucination
Dataset
In the machine translation task, the most usual
taxonomy of model hallucination or unfaithful-
ness for fluent output is over-translation and miss-
translation. Over-translation refers to the situ-
ation in which the translated sentence contains
words irrelevant to the source sentence, and miss-
translation refers to the situation in which the trans-
lation sentence lacks part of the information from
the source sentence. Thus, we prompt gpt-3.5-
torbo to mimic the two typical error types, and
the prompts are appended in Table.1.
To qualify the extent of miss-translation or over-
translation errors of generated sentences, we use
awesome-align2to evaluate the word-level cross-
lingual alignment rate, and the statistic result is
shown in Table.2, which indicates the generated
data satisfied the requirement of the negative sam-
ples while preserving the meaning of most of the
source sentences.
Instruction-tuning datasets can be organized flex-
ibly, and the standard format contains instruction,
input, and response. After we constructed the over-
translation and miss-translation contrastive samples
based on WMT17-20 with the proposed automatic
pipeline, we organized the final instruction data as
Figure.2. And the total number of samples in the
dataset is 54420.
4 Emprical Experiments
We choose BLOOM and LLaMA as the backbone
models. There are 4 translation directions included,
De⇒En, En ⇒De, En ⇒Zh, and Zh ⇒En.
4.1 Training Setting
4.1.1 Alpaca
Alpaca Dataset3is a high-quality multi-task
instruction-following dataset that contains 52K
items. We use Alpaca Dataset to finetune the pre-
trained language models as our baseline.
4.1.2 Parrot-hint
Following (Jiao et al., 2023a), we set Parrot-
hint as our strong baseline. The Parrot-hint4
dataset includes 3 sub-datasets, Alpaca Dataset,
the WMT17-20 dataset, and the MQM instruction
dataset. Parrot-hint contains 200K data in total.
2https://github.com/neulab/awesome-align
3https://github.com/tatsu-lab/stanford_alpaca
4https://github.com/wxjiao/ParroT4.1.3 O VERMISS
In the training process, we utilize the Parrot-hint
dataset to ensure the basic ability of the fine-tuned
models. As the mixup dataset contains instruction-
following data without a hint and with a hint, and
data with a hint both have an auxiliary task based
on translation. So we use a curriculum learning
strategy to fine-tune the data in two stages.
4.2 Evaluation
This section introduces the test sets and the evalua-
tion metrics we use.
4.2.1 WMT22 Test Sets
WMT22 test sets come from the news translation
track of WMT22 competition5. The test sets in-
clude 1984, 2037, 2037, and 1875 samples for De
⇒En, En ⇒De, En ⇒Zh, and Zh ⇒En, respec-
tively.
4.2.2 Flores-200 Dev-test
Flores-200 is a multi-language translation bench-
mark. We use the dev-test split as our test set to
enrich our experiments, and there are 1012 samples
for each translation direction.
4.2.3 Automatic Evaluation
For lexical evaluation, we use BLEU (Papineni
et al., 2002); for semantic evaluation, we use
COMET with reference. Both of them are widely
used metrics in machine translation, and we use
ScareBLEU6and Unbabel/wmt22-comet-da in the
evaluation implementation.
4.3 Implement details
We use the transformers and DeepSpeed frame-
work for model training and inference. The train-
ing hyper-parameters follow the setting of (Jiao
et al., 2023a). We uniformly set the dim of the
instruction adapter to 32. The 3B size models are
trained on 8 V100 GPUs, and the 7B size models
are trained on 4 A100(40G) GPUs. In order to
reduce the memory requirement and prevent the
models from over-fitting, we train all models with
freezing embedding layers in DeepSpeed stage 1.
4.4 Main Results
The main results are shown in Table.3. For LLaMA
fine-tuned by Alpaca, the model performs well
5https://github.com/wmt-conference/wmt22-news-
systems
6https://github.com/mjpost/sacrebleu

--- PAGE 6 ---
type prompt
miss-translation You are an unprofessional [source language] to [target language] translator
who is not fully faithful to the original text in the translation process there is a
problem of omission, i.e. the translation leaves out parts of the original text.
Please translate the following [source language] sentence:
[source sentence]
If the following is a high-quality human [target language] translation:
[target sentence]
Please give a direct low-quality [target language] translation with omission
problems, noting that you are not simply rewriting the previous translation, but
need to emulate a translator that may have omissions, i.e. omitting parts of the
original text.
over-translation You are an [source language] to [target language] translator, but your translation
is not professional. In the translation process, you have not been completely
faithful to the original text, resulting in a translation that is not in the original
text.
This is a translation illusion problem and you need to give a translation that has
the illusion problem. Please translate the following [source language] sentence:
[source sentence]
If the following is a high-quality human [target language]translation:
[target sentence]
Please give a straightforward low-quality [target language] translation that has
an additive translation problem or a translation illusion problem. Please note
that you need to simulate a translator with possible translation enhancement
problems and translate what is not in the original text, rather than simply
rewriting the previous translation.
Table 1: The prompts for producing the O VERMISSdataset.
data source coverage target coverage
reference 0.8845 0.8699
miss data 0.5800 0.7180
over data 0.6958 0.5771
Table 2: Data statistics of generated over-translation and
miss-translation data.
when translating En ⇔De, while when translating
En⇔Zh, it often confuses the target language, re-
sulting in code-mixing or out-of-target translation.
For BLOOM fine-tuned by Alpaca, the model trans-
lates En ⇔Zh better while translating worse in
En⇔De when comparing with LLaMA-Alpaca, in-
dicating the difference between the basic language
translation capacity of models. Overall, we have
three main observations during the experiment as
follows.
Firstly, according to the comparison between
OVERMISSand Parrot-hint, we found that OVER-
MISSnotably led to performance enhancement.
For example, based on LLaMA-7b, the model
trained with OVERMISShas an improvement of
1.02, 1.25. 3.12 and 0.69 BLEU scores on fourtranslation directions respectively, and an improve-
ment of 0.48 percentage comet scores on average.
Although the Flores dataset has a different distri-
bution from the WMT training data, we found that
theOVERMISSstill increases 0.46 BLEU score on
En⇒De and 3.03 BLEU score on Zh ⇒En.
Secondly, according to the comparison between
SWIE and Parrot-hint, we found that this method
has an obvious improvement on some of the set-
tings and has a stable slight improvement on other
settings. For example, on BLOOMZ-3b, SWIE
outperforms Parrot-hint from 0.19 to 0.51 BLEU
scores.
Thirdly, by combining the OVERMISSand
SWIE , a further improvement can be seen in all of
the backbones in the En ⇒De translation direction
from 0.05 to 0.56 BLEU scores, and in BLOOMZ-
7b-mt in three of the four translation directions.
Since both methods aim to improve faithfulness,
their combination is not orthogonal.
4.5 Long Sentence Translation
To assess the efficacy of SWIE in the context of
long text translation, we employed a concatena-
tion approach to merge the adjacent 3-5 sentences

--- PAGE 7 ---
ModelDe⇒En En ⇒De En ⇒Zh Zh ⇒En
bleu comet bleu comet bleu comet bleu comet
WMT22 Winners
33.7 85.46 38.4 88.09 33.5 87.84 54.3 81.12
BLOOMZ-3b WMT22
Alpaca 14.68 68.49 5.55 49.10 20.20 81.46 11.65 75.38
Parrot 22.05 75.59 17.80 67.64 33.95 83.70 21.33 78.19
w/ SWIE 22.56 75.59 18.17 67.64 34.14 83.64 21.71 78.58
w/ O VERMISS 24.00 76.66 19.24 70.44 35.35 83.51 21.93 78.08
w/ O VERMISSw/ SWIE 24.05 76.40 19.03 70.38 35.48 83.34 21.73 78.06
BLOOMZ-7b1-mt WMT22
Alpaca 18.64 73.37 9.97 61.65 25.52 82.31 15.07 77.79
Parrot 23.80 77.77 20.58 73.63 35.49 84.61 22.58 78.93
w/ SWIE 24.34 77.90 20.19 73.17 35.99 85.02 22.28 79.22
w/ O VERMISS 25.84 78.79 22.15 75.01 36.61 84.43 23.40 79.36
w/ O VERMISSw/ SWIE 25.95 78.80 21.83 75.17 36.88 84.53 23.33 79.15
LLaMA-7b WMT22
Alpaca 28.92 82.77 21.72 79.70 17.72 71.96 15.95 74.95
Parrot 28.90 82.84 25.96 82.78 28.12 79.84 20.61 75.61
w/ SWIE 28.56 82.97 25.70 82.11 29.03 79.68 20.33 75.48
w/ O VERMISS 29.92 83.50 27.21 82.36 31.24 80.63 21.30 76.48
w/ O VERMISSw/ SWIE 30.48 82.97 27.10 81.89 31.08 80.14 21.19 76.14
LLaMA-7b Flores
Parrot-hint 40.83 88.50 31.14 85.73 26.96 80.08 22.48 83.62
w/ SWIE 40.92 88.51 30.82 85.52 27.34 79.86 22.23 83.44
w/ O VERMISS 40.35 88.55 31.60 85.59 29.99 81.95 21.68 83.64
w/ O VERMISSw/ SWIE 40.20 88.39 31.41 85.21 29.07 81.14 21.59 83.50
Table 3: Translation performance of LLMs on WMT22 and Flores test sets. The bolded scores refer to the best
performance under the same or comparable settings.
model setting De ⇒En En ⇒De En ⇒Zh Zh ⇒En
Parrot-hint 23.73 17.11 34.70 19.11
w/ SWIE 24.02 17.79 34.94 20.59
Table 4: The comparison between baseline and SWIE
on WMT22-concat dataset.
from the WMT22 test sets, thereby creating the
WMT22-concat test set for long text translation.
Subsequently, we conducted an ablation experi-
ment on SWIE using the WMT22-concat test set
on BLOOMZ-3b. The results are presented in Ta-
ble.4 demonstrate that SWIE yields an average im-
provement of 0.6725 BLEU scores, with a notable
increase of 1.49 BLEU score observed in the Chi-
nese to English translation. These findings suggest
that instruction augmenting is better suited to long
text scenarios and can lead to further enhancements
in performance compared to the original WMT22
test sets.4.6 Zero-shot Performance
Using zero-shot translation directions, the
instruction-following ability can be effectively
evaluated. We select 6 zero-shot directions from
WMT22 test sets, including Uk ⇒En, Fr ⇒De,
Cs⇔En, and Ru ⇔En. We observe that SWIE
leads to 0.20 to 0.58 BLEU scores. The experiment
results are as our expectation since the SWIE
enhances the instruction-following ability of the
model, and the instruction needs more attention in
the zero-shot translation direction scenario.
4.7 The Impact of Inference Instruction
We test the impact of inference prompts. As
the auxiliary task instruction dataset provides the
model with typical translation quality information,
we can use more detailed prefixes during the in-
ference stage to guide the model’s translation pro-
cess with an awareness of certain principles. In
Table.6, the basic setting means the briefest instruc-
tion, that is, “translate the following sentences from
[source language] to [target language]". According

--- PAGE 8 ---
setting Uk ⇒En Fr ⇒De Cs ⇒En En ⇒Cs Ru ⇒En En ⇒Ru
Parrot-hint 6.77 19.58 4.97 2.69 17.59 4.35
w/ SWIE 7.33 19.73 5.14 2.85 17.79 4.74
Table 5: Zero-shot BLEU scores performance based on BLOOMZ-3b.
to the training task contained in the datasets, we
use some extra guides to provide a more detailed
request to models, such as output with no error or
no over/miss-translation problems. Conflicting to
the findings in (Jiao et al., 2023a), the “no-error"
hint does not yield positive benefits in the situa-
tion where fine-tuned model on OVERMISS, while
the “no-over", “no-miss" and “no-over/miss" can
improve model performance furthermore.
setting Overall BLEU
basic 25.13
w/ no-error 24.76
w/ no-over 25.13
w/ no-miss 25.29
w/ no-over/miss 25.24
Table 6: The comparison between inference prompts.
4.8 Faithfulness Quantification
On the qualification of word-level machine transla-
tion faithfulness, there is no widely-used standard
toolkit yet. The same method as Section. Natural
Hallucination Data Construction , we use word
alignment tools to match the source sentences and
the inference sentences word by word, then calcu-
late the recall of source words matching rate and
hypothesis words matching rate, and then the ratio
can reflect the absence and the redundancy extent.
The final scores are derived by averaging the source
and target coverage rate on our WMT22 test sets.
The result shows in Table.7 that both SWIE and
OVERMISScan improve the faithfulness of results,
showing the effectiveness of our proposed method.
setting score
Parrot-hint 87.94
w/ SWIE 88.28
w/ O VERMISS 88.84
w/ SWIE w/ O VERMISS 88.80
Table 7: The ablation study of faithfulness score on
SWIE and O VERMISS.5 Visualize Inadequate Attention on
Instruction
Our standard instruction-following data item is or-
ganized as instruction, input, and output sequen-
tially. The attention score in transformers can show
the positions the model addresses more. We divide
a random translation sample from test sets into 3
spans, including instruction, input, and response.
Subsequently, we calculate the accumulation atten-
tion scores for each span on each token. Assuming
ais the attention score matrix, the sidis the index
of the end of the span, we use Sspan to represent
the accumulated attention score in a position as
shown in Equation.7.
As depicted in Figure.4, it is evident that the
middle layers of the model manifest a considerably
higher attention accumulation score on the input
spans, whereas the bottom and top layers exhibit
more uniform attention distributions. This obser-
vation suggests that attention inadequacy of the in-
struction arises in the middle layers. Accordingly,
in our experimental settings, we opt to incorporate
SWIE into the middle three layers.
We compute the ratio of the attention score at
the ending position of the instruction and the at-
tention score at the ending position of the input.
As illustrated in Figure.5, our method leads to a
lower attention rate, especially for the middle lay-
ers, which implies that the attention on the instruc-
tion is relatively higher than that of the baseline
model.
Sspan=TX
i=sid+1a[i][sid] (7)
6 Conclusion
We proposed SWIE andOVERMISS, a novel ad-
ditional model structure for strengthening the at-
tention of the model to instruction, and an effec-
tive data construction method for machine trans-
lation faithfulness. The experiment results show
that our methods outperform the strong baselines
on widely used machine translation metrics like

--- PAGE 9 ---
Figure 4: Accumulative attention score on the post-
instruction and post-input positions for each layer. This
figure is based on BLOOMZ-3b fine-tuned by the Parrot-
hint dataset in the origin model structure.
Figure 5: The comparison between models with
and without SWIE on attention ratio between post-
instruction and post-input position. This experiment
is based on BLOOMZ-3b.
BLEU and COMET, and SWIE improves the trans-
lation performance more significantly in long text
and zero-shot scenarios. To evaluate the transla-
tion faithfulness, we employ a cross-lingual word
alignment metric, and the result further illustrates
the effectiveness of our method on faithful transla-
tion. Through the internal attention scores of the
models, we visualize the attention distribution on
the original model and the attention shift induced
bySWIE , thereby corroborating our assumption
regarding the necessity for increased attention on
instruction.
In the future, the following aspects can be ex-
plored based on our work: (1) investigating explain-
able and trainable methodologies for constructing
segment weights; (2) extending the data construc-
tion method to other tasks; (3) exploring methods
to reduce inference latency.References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate.
Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo.
2022. Prompt injection: Parameterization of fixed
inputs. arXiv preprint arXiv:2206.11349 .
David Dale, Elena V oita, Janice Lam, Prangthip
Hansanti, Christophe Ropers, Elahe Kalbassi, Cyn-
thia Gao, Loïc Barrault, and Marta R Costa-jussà.
2023. Halomi: A manually annotated bench-
mark for multilingual hallucination and omission
detection in machine translation. arXiv preprint
arXiv:2305.11746 .
Javier Ferrando, Gerard I Gállego, Belen Alastruey, Car-
los Escolano, and Marta R Costa-jussà. 2022. To-
wards opening the black box of neural machine trans-
lation: Source and target interpretations of the trans-
former. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 8756–8769.
Yingbo Gao, Christian Herold, Zijian Yang, and Her-
mann Ney. 2022. Is encoder-decoder redundant for
neural machine translation? In Proceedings of the
2nd Conference of the Asia-Pacific Chapter of the As-
sociation for Computational Linguistics and the 12th
International Joint Conference on Natural Language
Processing , pages 562–574.
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng
Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shum-
ing Shi, and Xing Wang. 2023. Exploring human-
like translation strategy with large language models.
arXiv preprint arXiv:2305.04118 .
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,
Young Jin Kim, Mohamed Afify, and Hany Hassan
Awadalla. 2023. How good are gpt models at ma-
chine translation? a comprehensive evaluation. arXiv
preprint arXiv:2302.09210 .
Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2022. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor. arXiv
preprint arXiv:2212.09689 .
Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Xing
Wang, Shuming Shi, and Zhaopeng Tu. 2023a. Par-
rot: Translating during chat using large language
models. arXiv preprint arXiv:2304.02426 .
Wenxiang Jiao, Wenxuan Wang, JT Huang, Xing Wang,
and ZP Tu. 2023b. Is chatgpt a good transla-
tor? yes with gpt-4 as the engine. arXiv preprint
arXiv:2301.08745 .
Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fan-
njiang, and David Sussillo. 2018. Hallucinations in
neural machine translation.

--- PAGE 10 ---
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023. Lost in the middle: How lan-
guage models use long contexts. arXiv preprint
arXiv:2307.03172 .
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688 .
Mathias Müller, Annette Rios, and Rico Sennrich. 2020.
Domain robustness in neural machine translation.
pages 151–164.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Vikas Raunak, Arul Menezes, and Marcin Junczys-
Dowmunt. 2021. The curious case of hallucinations
in neural machine translation. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 1172–1183.
Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah,
and Arul Menezes. 2023. Leveraging gpt-4 for
automatic translation post-editing. arXiv preprint
arXiv:2305.14878 .
Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran, An-
jana Arunkumar, David Stap, et al. 2022. Super-
naturalinstructions: Generalization via declarative
instructions on 1600+ nlp tasks. pages 5085–5109.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M
Dai, and Quoc V Le. 2021. Finetuned language
models are zero-shot learners.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 .
Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo
Shin, and Minjoon Seo. 2022. Guess the instruction!
flipped learning makes language models stronger
zero-shot learners. In The Eleventh International
Conference on Learning Representations .Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie
Zhou. 2023. Tim: Teaching large language mod-
els to translate with comparison. arXiv preprint
arXiv:2307.04408 .
Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhen-
grui Ma, Yan Zhou, Langlin Huang, Mengyu Bu,
Shangtong Gui, Yunji Chen, Xilin Chen, et al.
2023. Bayling: Bridging cross-lingual alignment
and instruction following through interactive trans-
lation for large language models. arXiv preprint
arXiv:2306.10968 .
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models. arXiv preprint
arXiv:2303.18223 .
