# 2308.14508.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2308.14508.pdf
# Kích thước tập tin: 11630690 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================


--- TRANG 1 ---
LongBench: Một Benchmark Song ngữ, Đa nhiệm vụ
cho Hiểu biết Ngữ cảnh Dài
Yushi Bai12, Xin Lv2, Jiajie Zhang12, Hongchang Lyu3, Jiankai Tang1,
Zhidian Huang1, Zhengxiao Du12, Xiao Liu12, Aohan Zeng12,
Lei Hou1, Yuxiao Dong1†, Jie Tang1, Juanzi Li1†
1Đại học Tsinghua2Zhipu.AI3Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc
Tóm tắt
Mặc dù các mô hình ngôn ngữ lớn (LLM)
thể hiện hiệu suất ấn tượng cho nhiều
nhiệm vụ ngôn ngữ, hầu hết chúng chỉ có thể xử
lý các văn bản dài vài nghìn token, hạn
chế ứng dụng của chúng trên các đầu vào chuỗi
dài hơn, như sách, báo cáo và cơ sở mã.
Các nghiên cứu gần đây đã đề xuất các phương pháp cải
thiện khả năng ngữ cảnh dài của LLM bằng cách mở
rộng cửa sổ ngữ cảnh và các cơ chế bộ nhớ tinh
vi hơn. Tuy nhiên, các benchmark toàn diện
được thiết kế riêng để đánh giá hiểu biết ngữ cảnh
dài còn thiếu. Trong bài báo này, chúng tôi giới thiệu LongBench, benchmark song ngữ,
đa nhiệm vụ đầu tiên cho hiểu biết ngữ cảnh
dài, cho phép đánh giá nghiêm ngặt hơn về hiểu biết ngữ cảnh dài. Long-
Bench bao gồm 21 bộ dữ liệu trên 6 danh mục
nhiệm vụ bằng cả tiếng Anh và tiếng Trung, với độ
dài trung bình 6.711 từ (tiếng Anh) và
13.386 ký tự (tiếng Trung). Các nhiệm vụ này bao gồm
các lĩnh vực ứng dụng văn bản dài chính bao gồm hỏi đáp
tài liệu đơn, hỏi đáp đa tài liệu, tóm tắt, học
ít mẫu, nhiệm vụ tổng hợp và hoàn thành mã.
Tất cả các bộ dữ liệu trong LongBench được chuẩn
hóa thành định dạng thống nhất, cho phép
đánh giá tự động dễ dàng các LLM. Sau
đánh giá toàn diện 8 LLM trên Long-
Bench, chúng tôi phát hiện rằng: (1) Mô hình thương mại
(GPT-3.5-Turbo-16k) vượt trội hơn các mô
hình mã nguồn mở khác, nhưng vẫn gặp khó khăn trên
ngữ cảnh dài hơn. (2) Nhúng vị trí có tỷ lệ và
tinh chỉnh trên các chuỗi dài hơn dẫn đến cải thiện đáng kể về hiểu biết ngữ cảnh
dài. (3) Kỹ thuật nén ngữ cảnh như
truy xuất mang lại cải thiện cho mô hình có
khả năng yếu trên ngữ cảnh dài, nhưng hiệu suất vẫn tụt hậu so với các mô hình có khả năng hiểu biết ngữ cảnh dài mạnh.

1 Giới thiệu
Lĩnh vực NLP từ lâu đã tìm cách trao cho máy
móc khả năng hiểu và lập luận
†Tác giả liên hệtrên một ngữ cảnh dài. Các nhiệm vụ như tóm tắt
và hỏi đáp dựa trên sách, báo cáo,
và tài liệu, và tạo mã ở cấp độ kho
lưu trữ đòi hỏi khả năng mô hình hóa các chuỗi ngữ cảnh dài kéo dài hàng nghìn hoặc thậm chí hàng chục
nghìn token. Tuy nhiên, nhiều mô hình ngôn ngữ lớn ngày nay chỉ có thể hiểu
và tạo ra các văn bản dài vài nghìn token,
để lại chỗ cho các cải thiện tiềm năng trong việc xử
lý ngữ cảnh dài hơn. Gần đây hơn, đã có
nỗ lực ngày càng tăng để cải thiện khả năng của các mô hình ngôn ngữ lớn về hiểu biết ngữ cảnh dài.
Các phương pháp này bao gồm mở rộng cửa sổ ngữ
cảnh (Press et al., 2022; Chen et al., 2023), sử dụng
bộ nhớ tuần hoàn (Dai et al., 2019; Bulatov et al.,
2023), sử dụng attention thưa thớt (Ding et al., 2023;
Mohtashami and Jaggi, 2023), và tăng cường
với bộ nhớ ngoài (Liang et al., 2023; Zhou
et al., 2023). Tuy nhiên, không giống như trong ngữ cảnh ngắn,
nơi có nhiều benchmark đa nhiệm vụ
sẵn có cho đánh giá đa khía cạnh (Hendrycks
et al., 2021; Srivastava et al., 2023), không có
benchmark như vậy trên ngữ cảnh dài hơn.

Để tạo điều kiện cho nghiên cứu sâu hơn theo hướng này,
chúng tôi đề xuất LongBench, benchmark song ngữ, đa
nhiệm vụ đầu tiên được thiết kế riêng cho hiểu biết ngữ cảnh dài. LongBench bao gồm 6 danh mục
nhiệm vụ chính và 21 nhiệm vụ khác nhau, bao gồm
các tình huống ứng dụng văn bản dài chính bao gồm hỏi đáp đa
tài liệu, hỏi đáp tài liệu đơn, tóm tắt,
học ít mẫu, hoàn thành mã và các
nhiệm vụ tổng hợp. Ngoài ra, LongBench bao gồm các
ngôn ngữ khác nhau (tiếng Trung và tiếng Anh) để cung
cấp đánh giá toàn diện hơn về khả năng song ngữ của các mô
hình lớn trên ngữ cảnh dài.
Với thống kê tổng quan được hiển thị trong Hình 1, Long-
Bench chứa 4.750 trường hợp thử nghiệm, với độ
dài trung bình 6.711 từ và 13.386 ký tự
cho các trường hợp tiếng Anh và tiếng Trung, tương ứng.
Tất cả 21 bộ dữ liệu trong LongBench được chuẩn
hóa thành định dạng thống nhất, trong đó 6 được trích xuất trực tiếparXiv:2308.14508v2  [cs.CL]  19 Jun 2024

--- TRANG 2 ---
1
2
3
4
5
6Hỏi đáp Tài liệu Đơn: 750Hỏi đáp Đa Tài liệu: 800Tóm tắt: 600Ít mẫu: 800Tổng hợp: 600Mã: 1000Hình 1: Trái: Số lượng dữ liệu trong mỗi loại nhiệm vụ trong LongBench. Phải: Phân phối độ dài cho dữ liệu tiếng Anh và
tiếng Trung trong LongBench, đo bằng số từ và ký tự.

từ các bộ dữ liệu gốc được cung cấp bởi
các nghiên cứu trước đó, 10 được xây dựng dựa trên các bộ dữ liệu gốc và xử lý để phù hợp với đánh giá ngữ cảnh dài, và 5 được tạo và chú thích
bởi chúng tôi. Chúng tôi hoàn toàn nhận thức được chi phí có thể cao liên quan đến quá trình đánh giá mô hình, đặc
biệt trong bối cảnh các tình huống ngữ cảnh dài
(như chi phí chú thích thủ công hoặc chi phí gọi API).
Do đó, chúng tôi áp dụng phương pháp đánh giá hoàn toàn tự động, trong đó chúng tôi sử dụng các chỉ số tự động như
ROUGE-L và F1 để đo độ tương tự của
các đầu ra với câu trả lời đúng.

Chúng tôi tiến hành đánh giá toàn diện 8
mô hình trên LongBench. Các kết quả thực nghiệm cung
cấp kết luận sâu sắc về khả năng đa nhiệm vụ của các mô hình hiện tại về hiểu biết ngữ cảnh dài. Ngoài ra, để tách biệt tốt hơn khả năng ngữ cảnh dài của mô hình khỏi khả năng nhiệm vụ của chúng, chúng tôi xây dựng LongBench-E có
phân phối độ dài đều hơn, do đó phù hợp
để đánh giá khả năng của mỗi mô hình trên các
độ dài ngữ cảnh khác nhau. Kết quả trên LongBench-E tiết lộ
rằng mặc dù một số mô hình được huấn luyện hoặc tinh chỉnh
trên ngữ cảnh dài hơn, chúng vẫn trải qua sự suy
giảm đáng kể về hiệu suất khi độ dài ngữ cảnh tăng. Chúng tôi cũng điều tra hiệu ứng của các kỹ thuật nén ngữ cảnh dựa trên truy xuất
và tóm tắt. Kết quả của chúng tôi chứng minh rằng các
phương pháp này chỉ có lợi cho các mô hình thể
hiện khả năng yếu hơn trên ngữ cảnh dài.

2 Nghiên cứu Liên quan

Kỹ thuật Mô hình hóa Ngữ cảnh Dài. Chúng tôi trước tiên
thảo luận về một số dòng phương pháp phổ biến nhằm
giải quyết hiểu biết ngữ cảnh dài. Các nghiên cứu này
chủ yếu nhằm giải quyết hai thách thức chínhtrong mô hình hóa văn bản dài, bao gồm chi phí thời gian chạy cao
trên ngữ cảnh dài hơn, và hiện tượng quên thảm khốc khi xử lý chuỗi dài. Một loạt các nghiên cứu tập trung vào cách làm cho
Transformer hiệu quả và không quên hơn (Tay
et al., 2022), với các thiết kế như tính toán thưa thớt và hiệu
quả (Child et al., 2019; Kitaev et al.,
2020; Beltagy et al., 2020; Zaheer et al., 2020;
Wang et al., 2020; Fedus et al., 2022; Ding et al.,
2023), các mô-đun tuần hoàn và bộ nhớ (Dai et al.,
2019; Rae et al., 2020; Wu et al., 2022; Martins
et al., 2022; Bulatov et al., 2022; Orvieto et al.,
2023; Liang et al., 2023; Zhou et al., 2023). Gần
đây hơn, một số phương pháp (Press et al., 2022; Sun
et al., 2022; Chen et al., 2023) đã được đề xuất
để cho phép ngoại suy độ dài của Transformer, và
đã được áp dụng trong quá trình huấn luyện của các
LLM ngữ cảnh dài như ChatGLM2-32k (Zeng et al.,
2023) và LongChat-32k (Li et al., 2023).

Đánh giá cho Hiểu biết Ngữ cảnh Dài.
Nhiều nghiên cứu trước đây về mô hình hóa văn bản dài dựa
vào chỉ số perplexity để đánh giá (Beltagy
et al., 2020; Roy et al., 2021; Press et al., 2022).
Tuy nhiên, như được đề xuất trong (Sun et al., 2021),
chỉ số perplexity có thể không nhất thiết phản ánh
hiệu suất của mô hình trên các nhiệm vụ cấp độ chuỗi trong
các ứng dụng thực tế Trong khi đó, một số nghiên cứu đánh giá
mô hình hóa văn bản dài thông qua các nhiệm vụ nhân tạo như
truy xuất (Tay et al., 2021; Chen et al., 2023; Li
et al., 2023), điều này cũng có thể không đáp ứng trong việc phản ánh
các tình huống thế giới thực.

Đồng thời, ZeroSCROLLS (Shaham et al.,
2022, 2023) và L-Eval (An et al., 2023) được đề
xuất như các benchmark đánh giá cho mô hình hóa văn bản dài. Tuy nhiên, chúng bao gồm một
phạm vi hạn chế các loại nhiệm vụ, do đó hạn chế sự đa dạng của
các mẫu mô hình hóa văn bản dài cần thiết trong bench-

--- TRANG 3 ---
mark, và do đó, tính toàn diện của
kết quả đánh giá. Gần đây, AgentBench (Liu
et al., 2023c) cũng đề cập đến thách thức về việc xử lý quỹ đạo tương tác dài của LLM-
as-Agent, nhưng không thành công trong việc kết hợp nó như một chiều đánh giá chuyên dụng. Ngược lại, LongBench bao gồm sáu
danh mục nhiệm vụ chính, với mỗi danh mục có
các chuỗi có độ dài, ngôn ngữ và
lĩnh vực khác nhau. Chúng tôi tin rằng nó cung cấp đánh giá toàn diện hơn về khả năng mô hình hóa văn bản dài của các mô hình ngôn ngữ lớn trên một phổ độ dài, phân
phối, cũng như các mẫu phụ thuộc dài.

3 LongBench: Nhiệm vụ và Xây dựng

3.1 Định nghĩa Vấn đề

Chúng tôi hình thức hóa vấn đề hiểu biết ngữ cảnh dài
như sau: Cho các chuỗi đầu vào và ngữ cảnh
(I, C), mô hình được kỳ vọng đưa ra
câu trả lời A. Ví dụ, trong một nhiệm vụ QA, đầu vào
I sẽ là câu hỏi, ngữ cảnh C đề cập đến
tài liệu, và A biểu thị câu trả lời cho câu hỏi. Nói chung, trong LongBench, I và A có xu hướng
ngắn, trong khi C đại diện cho một chuỗi dài lên đến
hàng nghìn token. Việc khởi tạo của
(I, C, A) cho mỗi nhiệm vụ được liệt kê trong Bảng 7.

3.2 Xây dựng Bộ dữ liệu

Trong phần này, chúng tôi sẽ cung cấp giới thiệu chi tiết về
quá trình thu thập, chú thích và tổ
chức dữ liệu cho mỗi bộ dữ liệu trong LongBench
(LongBench-E), theo các nhiệm vụ cụ thể. Đối với
thống kê dữ liệu tổng thể của LongBench, chúng tôi tham khảo
Bảng 1.

3.2.1 Thu thập và Chú thích Dữ liệu

Hỏi đáp Tài liệu Đơn. Đối với hỏi đáp tài liệu đơn, chúng tôi tập
trung vào các trường hợp có tài liệu dài hơn. Chúng tôi trích xuất
NarrativeQA từ bộ dữ liệu gốc trong Ko ˇcisk`y
et al. (2018), bao gồm các câu chuyện dài cùng
với các câu hỏi được đặt ra để kiểm tra khả năng hiểu
đọc. Chúng tôi cũng lấy mẫu từ Qasper (Dasigi et al.,
2021), có tính năng QA về các bài báo NLP và được
chú thích bởi các nhà thực hành NLP.

Để kiểm tra tốt hơn khả năng hiểu ngữ cảnh dài
của mô hình trên các lĩnh vực đa dạng, chúng tôi thủ công
tuyển chọn các bộ dữ liệu MultiFieldQA bằng cả tiếng Anh
và tiếng Trung. Chúng tôi trước tiên thu thập các tài liệu và bài
báo từ nhiều nguồn, bao gồm các tài liệu pháp lý, báo cáo chính phủ, bách khoa toàn thư, bài
báo học thuật, v.v. (các nguồn được chi tiết trong Phụ lục).
Chúng tôi mời ba sinh viên Tiến sĩ chú thích câu hỏi và câu trả lời cho mỗi bài báo, với các câu trả lời xác định càng nhiều càng tốt để dễ dàng đánh giá tự động. Trong quá trình chú thích, chúng tôi đảm bảo rằng
các câu trả lời có thể được suy ra từ các tài liệu,
và vị trí của bằng chứng khá ngẫu nhiên để
tránh các thiên vị có thể xảy ra nếu, chẳng hạn, các
câu lệnh liên quan đến câu trả lời thường xuyên được tìm thấy ở
đầu hoặc cuối, như đề cập trong (Liu et al.,
2023a).

Hỏi đáp Đa Tài liệu. Hỏi đáp đa tài liệu yêu cầu các mô
hình trích xuất và kết hợp thông tin từ nhiều
tài liệu để có được câu trả lời, thường khó khăn hơn
hỏi đáp tài liệu đơn. Các mẫu thử nghiệm tiếng Anh
được xây dựng từ ba bộ dữ liệu QA đa bước dựa trên Wikipedia: HotpotQA (Yang et al.,
2018), 2WikiMultihopQA (Ho et al., 2020), và
MuSiQue (Trivedi et al., 2022). HotpotQA bao gồm
một số câu hỏi 2 bước được viết trực tiếp bởi người
bản ngữ với hai đoạn văn liên quan. 2Wiki-
MultihopQA bao gồm các câu hỏi lên đến 5 bước được
tổng hợp thông qua các mẫu được thiết kế thủ công
để đảm bảo rằng chúng không thể được giải quyết thông qua
các lối tắt. Các câu hỏi trong MuSiQue được sáng tác cẩn thận từ các câu hỏi đơn giản liên quan đến lập luận lên đến
4 bước, và sau đó được diễn giải lại bởi các người chú
thích để vừa tránh lối tắt vừa đảm bảo tính tự nhiên về mặt ngôn ngữ. Mỗi câu hỏi trong các bộ dữ liệu gốc
được bổ sung bởi 2-4 đoạn văn hỗ trợ cung
cấp bằng chứng lập luận một bước và một số
đoạn văn gây nhiễu.

Để điều chỉnh dữ liệu cho đánh giá ngữ cảnh dài, chúng tôi
sử dụng các đoạn văn Wikipedia hoàn chỉnh bao
gồm các đoạn văn hỗ trợ hoặc gây nhiễu làm
ngữ cảnh. Ban đầu, các đoạn văn hỗ trợ được bao gồm
trong ngữ cảnh, và sau đó càng nhiều đoạn văn gây nhiễu
được thêm vào cho đến khi tổng độ dài đạt
độ dài tối đa. Cuối cùng, các đoạn văn này được sắp
xếp ngẫu nhiên để tạo thành ngữ cảnh đa tài liệu.
Ngoài ba bộ dữ liệu tiếng Anh này, chúng tôi cũng xây
dựng một bộ dữ liệu tiếng Trung dựa trên DuReader (He
et al., 2018), được phát triển dựa trên Tìm kiếm Baidu
và Baidu Zhidao, bao gồm 200K câu hỏi và 1M tài liệu liên quan. Để điều chỉnh nó cho
việc đánh giá khả năng ngữ cảnh dài, đối với mỗi câu hỏi,
chúng tôi không chỉ cung cấp nhiều tài liệu liên quan đến
câu hỏi mà còn tùy ý chọn một số từ
tập hợp tổng thể các tài liệu làm chất gây nhiễu, cho đến khi mỗi
câu hỏi được liên kết với 20 tài liệu.

Tóm tắt. So với các nhiệm vụ QA, thường
có thể được giải quyết bằng cách sử dụng thông tin cục bộ trong
ngữ cảnh, tóm tắt đòi hỏi một sự hiểu biết toàn cầu hơn

--- TRANG 4 ---
ID Bộ dữ liệu Nguồn Độ dài TB Chỉ số Ngôn ngữ #dữ liệu
Hỏi đáp Tài liệu Đơn
NarrativeQA 1-1 Văn học, Phim 18,409 F1 Tiếng Anh 200
Qasper 1-2 Khoa học 3,619 F1 Tiếng Anh 200
MultiFieldQA-en 1-3 Đa lĩnh vực 4,559 F1 Tiếng Anh 150
MultiFieldQA-zh 1-4 Đa lĩnh vực 6,701 F1 Tiếng Trung 200
Hỏi đáp Đa Tài liệu
HotpotQA 2-1 Wikipedia 9,151 F1 Tiếng Anh 200
2WikiMultihopQA 2-2 Wikipedia 4,887 F1 Tiếng Anh 200
MuSiQue 2-3 Wikipedia 11,214 F1 Tiếng Anh 200
DuReader 2-4 Tìm kiếm Baidu 15,768 Rouge-L Tiếng Trung 200
Tóm tắt
GovReport 3-1 Báo cáo chính phủ 8,734 Rouge-L Tiếng Anh 200
QMSum 3-2 Cuộc họp 10,614 Rouge-L Tiếng Anh 200
MultiNews 3-3 Tin tức 2,113 Rouge-L Tiếng Anh 200
VCSUM 3-4 Cuộc họp 15,380 Rouge-L Tiếng Trung 200
Học Ít mẫu
TREC 4-1 Câu hỏi web 5,177 Độ chính xác (CLS) Tiếng Anh 200
TriviaQA 4-2 Wikipedia, Web 8,209 F1 Tiếng Anh 200
SAMSum 4-3 Đối thoại 6,258 Rouge-L Tiếng Anh 200
LSHT 4-4 Tin tức 22,337 Độ chính xác (CLS) Tiếng Trung 200
Nhiệm vụ Tổng hợp
PassageCount 5-1 Wikipedia 11,141 Độ chính xác (EM) Tiếng Anh 200
PassageRetrieval-en 5-2 Wikipedia 9,289 Độ chính xác (EM) Tiếng Anh 200
PassageRetrieval-zh 5-3 Bộ dữ liệu C4 6,745 Độ chính xác (EM) Tiếng Trung 200
Hoàn thành Mã
LCC 6-1 Github 1,235 Edit Sim Python/C#/Java 500
RepoBench-P 6-2 Kho lưu trữ Github 4,206 Edit Sim Python/Java 500

Bảng 1: Tổng quan về thống kê bộ dữ liệu trong LongBench. Các bộ dữ liệu tiếng Trung được tô sáng. 'Nguồn' biểu thị
nguồn gốc của ngữ cảnh. 'Độ dài TB' (độ dài trung bình) được tính bằng số từ cho các bộ dữ liệu tiếng Anh (mã)
và số ký tự cho các bộ dữ liệu tiếng Trung. 'Độ chính xác (CLS)' đề cập đến độ chính xác phân loại,
trong khi 'Độ chính xác (EM)' đề cập đến độ chính xác khớp chính xác.

về toàn bộ ngữ cảnh. Chúng tôi trích xuất
GovReport từ bộ dữ liệu gốc (Huang et al.,
2021). Bộ dữ liệu GovReport gốc là một bộ sưu tập quy mô lớn các báo cáo chi tiết từ Văn phòng Trách nhiệm Chính phủ Hoa Kỳ và Dịch vụ Nghiên cứu Quốc hội, mỗi báo cáo đi kèm với một
bản tóm tắt do con người viết, bao trùm một loạt rộng các
vấn đề chính sách quốc gia. Chúng tôi cũng lấy mẫu từ
QMSum (Zhong et al., 2021), bao gồm
các cặp truy vấn-tóm tắt được chú thích trên 232 cuộc họp
trên nhiều lĩnh vực, bao gồm sản phẩm, học
thuật và các cuộc họp ủy ban. Chúng tôi coi truy vấn
là đầu vào I, nội dung cuộc họp là ngữ cảnh C, và
bản tóm tắt là câu trả lời A. MultiNews được lấy
từ bộ dữ liệu tóm tắt đa tài liệu gốc trong (Fabbri et al., 2019). Bộ dữ liệu MultiNews
có các cụm từ 2-10 bài báo tin tức thảo
luận về cùng một sự kiện hoặc chủ đề, mỗi cụm được ghép đôi với một
bản tóm tắt do con người viết tóm tắt thông tin chínhtừ nhiều bài báo nguồn. Trong
LongBench, chúng tôi bao gồm "Tài liệu i" trước bài
báo tin tức thứ i, và nối chúng thành ngữ
cảnh C. VCSUM (Wu et al., 2023) là một bộ dữ liệu tóm tắt cuộc họp tiếng Trung quy mô lớn bao
gồm 239 cuộc họp thực tế với hơn 230 giờ
thời lượng, với các chú thích đa dạng để hỗ trợ
nhiều nhiệm vụ tóm tắt. Trong LongBench, chúng tôi
chọn các phân đoạn dài từ VCSUM để tạo thành
các mẫu đánh giá của chúng tôi.

Học Ít mẫu. Chúng tôi xác định học trong ngữ cảnh ít mẫu là một thiết lập thực tế yêu cầu
hiểu biết ngữ cảnh dài, đặc biệt khi số
lượng ví dụ tăng lên (Ainslie et al., 2023).
Để đảm bảo sự đa dạng của các nhiệm vụ, chúng tôi kết
hợp các nhiệm vụ phân loại, tóm tắt và hiểu
đọc trong kịch bản học ít mẫu. Chúng tôi bao gồm 2 bộ dữ liệu phân loại
với các nhãn lớp chi tiết, bao gồm TREC (Li

--- TRANG 5 ---
and Roth, 2002), một nhiệm vụ phân loại câu hỏi
liên quan đến 50 lớp chi tiết, và LSHT (NLPCC,
2014), một nhiệm vụ phân loại tin tức tiếng Trung với
24 lớp. Đối với nhiệm vụ tóm tắt, chúng tôi sử dụng
bộ dữ liệu SAMSum (Gliwa et al., 2019), chứa
các cuộc trò chuyện giống như tin nhắn với bản tóm tắt được chú thích bởi con người. TriviaQA (Joshi et al., 2017)
chứa các cặp câu hỏi-câu trả lời được gắn nhãn với các
đoạn văn bằng chứng, và chúng tôi sử dụng nó như một
nhiệm vụ hiểu đọc. Chúng tôi lọc các đoạn văn trong TriviaQA
có ít hơn 1.000 từ làm ví dụ.

Trong mỗi bộ dữ liệu được điều chỉnh cho Long-
Bench ở trên, đối với mỗi dữ liệu thử nghiệm, chúng tôi trước tiên ngẫu nhiên chọn một số nguyên trong một phạm vi làm số lượng ví
dụ, sau đó ngẫu nhiên lấy mẫu số
lượng mẫu tương ứng từ tập huấn luyện, và nối
chúng để tạo thành ngữ cảnh C. Đối với TREC,
LSHT, SAMSum và TriviaQA, các phạm vi lần lượt là
[100,600],[10,40],[10,100],[2,24].

Nhiệm vụ Tổng hợp. Không giống như các nhiệm vụ tiêu chuẩn giống nhau hơn
về mẫu phụ thuộc dài cần thiết,
các nhiệm vụ tổng hợp có thể được thiết kế tỉ mỉ để
kiểm tra khả năng của mô hình trong các tình huống và
mẫu cụ thể. Trong LongBench, chúng tôi thiết kế ba nhiệm vụ tổng hợp. PassageRetrieval-en và PassageRetrieval-
zh được xây dựng dựa trên Wikipedia tiếng Anh và
các phần tiếng Trung của bộ dữ liệu C4 (Raffel et al.,
2020). Đối với mỗi mục dữ liệu, chúng tôi ngẫu nhiên lấy mẫu 30
đoạn văn và chọn một trong số chúng để tóm tắt
bằng GPT-3.5-Turbo. Nhiệm vụ yêu cầu mô hình
xác định đoạn văn gốc mà bản tóm tắt được tạo
tương ứng.

PassageCount tìm cách tạo ra một tình huống khắt khe hơn
trong đó mô hình được yêu cầu sử dụng
toàn bộ ngữ cảnh để giải quyết nhiệm vụ. Đối với mỗi phần
dữ liệu, chúng tôi ngẫu nhiên chọn một số đoạn văn từ Wikipedia tiếng Anh, lặp lại mỗi đoạn văn ngẫu nhiên
một số lần, và cuối cùng xáo trộn các đoạn văn.
Nhiệm vụ yêu cầu mô hình xác định số lượng
đoạn văn duy nhất trong tập hợp đã cho. Cụ thể,
chúng tôi ngẫu nhiên chọn M từ [17,50] làm giới
hạn trên cho số lượng đoạn văn. Sau đó, số
lượng N đoạn văn duy nhất được chọn ngẫu nhiên
từ phạm vi [2, M]. Chúng tôi tiến hành lấy mẫu ngẫu nhiên
có thay thế từ tập hợp N đoạn văn duy nhất
để có được M đoạn văn cuối cùng.

Hoàn thành Mã. Hoàn thành mã là một nhiệm vụ quan trọng
được sử dụng bởi các hệ thống tự động hoàn thành để hỗ trợ
người dùng bằng cách hoàn thành mã dựa trên đầu vào mã
trước đó và ngữ cảnh (Chen et al., 2021; Zheng et al.,
2023b). Nhiệm vụ này có thể đặt ra thách thức đáng kể cho các mô hình, đặc biệt khi xử lý các đầu vào mã
dài hoặc thậm chí dữ liệu cấp độ kho lưu trữ. Điều này
chủ yếu là do các mô hình cần thiết lập sự chú ý trên các chuỗi tầm xa theo mối quan hệ trong các phần tử mã, chẳng hạn như giữa
định nghĩa lớp và hàm. Do đó chúng tôi nhận thức
điều này như một nhiệm vụ phù hợp để đánh giá khả năng mô hình hóa ngữ cảnh dài của mô hình.

Bộ dữ liệu LCC được lấy mẫu từ bộ dữ liệu
Hoàn thành Mã Dài gốc (Guo et al., 2023).
Bộ dữ liệu gốc được xây dựng bằng cách lọc mã
trong một tập tin từ GitHub dựa trên độ dài. Dữ liệu này
bao gồm một phần dài của các dòng mã trước đó
làm ngữ cảnh, và dòng mã tiếp theo làm
câu trả lời. Chúng tôi cũng xem xét thiết lập hoàn thành mã cấp độ kho lưu trữ, yêu cầu tổng hợp
thông tin từ mã trên các tập tin. Đối với nhiệm vụ này,
chúng tôi điều chỉnh bộ dữ liệu RepoBench-P từ (Liu et al.,
2023b). RepoBench-P được thu thập từ các kho lưu trữ Github, và được xây dựng bằng cách trước tiên truy xuất
các đoạn mã liên quan từ các tập tin khác dựa trên
các câu lệnh nhập mô-đun. Các đoạn này sau đó
được nối với các dòng mã trước đó
trong tập tin hiện tại làm ngữ cảnh, và được sử dụng để
dự đoán dòng mã tiếp theo. Chúng tôi chọn thiết lập
XF-F (Cross-File-First) khó khăn nhất từ
bộ dữ liệu gốc, trong đó ngữ cảnh trong tập tin không đưa ra
việc sử dụng trước đó của mô-đun để hỗ trợ dự đoán.
Đối với mỗi phần dữ liệu gốc, chúng tôi xáo trộn các
đoạn mã khác tập tin bao gồm đoạn mã khác tập tin vàng
(được chú thích thủ công như ngữ cảnh tối ưu cho dự đoán), và kết hợp chúng thành ngữ
cảnh C. Các dòng mã trước đó được lấy làm
đầu vào I, và dòng mã tiếp theo làm câu trả lời A.

3.2.2 Trích xuất Dữ liệu

Vì các LLM có thể đã được huấn luyện trên
tập huấn luyện của một số bộ dữ liệu công cộng mà chúng tôi thu thập,
để tránh rò rỉ thử nghiệm, chúng tôi trích xuất dữ liệu
từ các tập thử nghiệm của những bộ dữ liệu công cộng này, ngoại trừ VCSUM do dữ liệu không đủ
trong tập thử nghiệm của nó. Chúng tôi sử dụng hai chiến lược trích xuất: lấy mẫu ngẫu nhiên và lấy mẫu đồng đều.
Thông qua lấy mẫu ngẫu nhiên, chúng tôi duy trì một phân
phối độ dài tự nhiên để mô phỏng chính xác hơn
các tình huống thực tế, và có được LongBench. Thay vào đó, chúng tôi thực hiện lấy mẫu đồng đều dựa trên
độ dài của dữ liệu với trọng tâm vào việc nghiên cứu
khả năng của mô hình trên các độ dài ngữ cảnh khác nhau
trong chính mỗi nhiệm vụ. Phương pháp này cung cấp những hiểu biết về khả năng thực sự của mô hình hiểu
ngữ cảnh dài độc lập với khả năng nhiệm vụ. Chúng tôi

--- TRANG 6 ---
Mô hìnhHỏi đáp Tài liệu Đơn Hỏi đáp Đa Tài liệu Tóm tắt
1-1 1-2 1-3 1-4 TB 2-1 2-2 2-3 2-4 TB 3-1 3-2 3-3 3-4 TB
GPT-3.5-Turbo-16k 23.6 43.3 52.3 61.2 45.1 51.6 37.7 26.9 28.7 36.2 29.5 23.4 26.7 16.0 23.9
Llama2-7B-chat-4k 18.7 19.2 36.8 11.9 21.7 25.4 32.8 9.4 5.2 18.2 27.3 20.8 25.8 0.2 18.5
LongChat-v1.5-7B-32k 16.9 27.7 41.4 29.1 28.8 31.5 20.6 9.7 19.5 20.3 30.8 22.7 26.4 9.9 22.5
XGen-7B-8k 18.0 18.1 37.7 14.8 22.1 29.7 21.1 10.3 11.0 18.0 27.3 20.5 26.2 2.2 19.0
InternLM-7B-8k 12.1 16.7 23.4 33.6 21.4 28.7 22.8 9.0 11.1 17.9 9.7 15.9 22.8 12.4 15.2
ChatGLM2-6B 11.8 22.5 35.0 33.2 25.6 22.4 20.1 6.1 16.3 16.2 23.2 21.1 25.2 14.5 21.0
ChatGLM2-6B-32k 21.1 31.5 46.2 51.6 37.6 45.1 34.0 21.9 37.6 34.7 32.4 24.0 26.5 16.2 24.8
Vicuna-v1.5-7B-16k 19.4 26.1 38.5 43.0 31.8 25.3 20.8 9.8 19.3 18.8 27.9 22.8 27.2 15.1 23.2

Bảng 2: Kết quả (%) trên các nhiệm vụ hỏi đáp tài liệu đơn, hỏi đáp đa tài liệu và tóm tắt.

Mô hìnhHọc Ít mẫu Tổng hợp Mã Tổng thể
4-1 4-2 4-3 4-4 TB 5-1 5-2 5-3 TB 6-1 6-2 TB EN ZH Tất cả
GPT-3.5-Turbo-16k 68.0 91.4 41.7 29.2 57.6 4.5 71.0 77.5 51.0 54.7 53.6 54.1 44.0 44.5 44.7
Llama2-7B-chat-4k 61.5 77.8 40.7 19.8 49.9 2.1 9.8 0.5 4.1 52.4 43.8 48.1 31.0 14.3 26.8
LongChat-v1.5-7B-32k 63.5 82.3 34.2 23.2 50.8 1.0 30.5 7.6 13.0 53.0 55.3 54.1 34.3 23.9 31.6
XGen-7B-8k 65.5 77.8 25.3 20.5 47.3 2.1 8.5 3.5 4.7 38.6 38.6 38.6 28.3 15.1 25.0
InternLM-7B-8k 52.0 77.8 21.2 15.2 41.6 3.0 6.0 0.9 3.3 44.1 28.8 36.4 24.2 18.3 22.6
ChatGLM2-6B 44.5 70.6 29.5 20.8 41.3 2.5 3.0 6.5 4.0 49.0 43.2 46.1 26.6 22.9 25.7
ChatGLM2-6B-32k 62.5 78.7 36.3 27.7 51.3 1.5 77.0 64.5 47.7 55.6 49.9 52.7 40.9 41.7 41.4
Vicuna-v1.5-7B-16k 71.5 86.2 40.8 28.8 56.8 6.5 4.5 5.0 5.3 51.0 43.5 47.3 31.9 26.4 30.5

Bảng 3: Kết quả (%) trên học ít mẫu, tổng hợp và các nhiệm vụ mã. 'Tổng thể' được tính bằng trung bình vĩ mô
(trung bình của 'TB') trên các danh mục nhiệm vụ chính. Điều này được tính trên các nhiệm vụ tiếng Anh (EN), nhiệm vụ tiếng Trung (ZH) và tất cả
(Tất cả) các nhiệm vụ, các nhiệm vụ mã được bao gồm trong cả hai ngôn ngữ.

chọn 13 trong số các bộ dữ liệu tiếng Anh, bao gồm Qasper,
MultiFieldQA-en, HotpotQA, 2WikiMultihopQA,
GovReport, Multi-news, TREC, TriviaQA, SAM-
Sum, PassageCount, PassageRetrieval-en, LCC,
và RepoBench-P, cung cấp phạm vi bao phủ rộng hơn
về độ dài dữ liệu. Trong quá trình lấy mẫu đồng đều, chúng tôi sử dụng số từ làm độ dài và lấy mẫu một
số lượng dữ liệu tương đương từ các phạm vi độ dài
0-4k, 4k-8k và 8k+. Dữ liệu kết quả được biên
soạn thành LongBench-E (thống kê trong Bảng 8).

4 Thí nghiệm

4.1 Kết quả Benchmark trên LongBench và
LongBench-E

Thiết lập Thí nghiệm. Chúng tôi đánh giá 8 LLM phổ biến
có tính năng khả năng ngữ cảnh dài, bao gồm GPT-
3.5-Turbo-16k (OpenAI, 2022a), Llama2-7B-chat-
4k (Touvron et al., 2023), LongChat-v1.5-7B-
32k (Li et al., 2023), XGen-7B-8k (Nijkamp et al.,
2023), InternLM-7B-8k (Team, 2023), ChatGLM2-
6B, ChatGLM2-6B-32k (Du et al., 2022; Zeng
et al., 2023), và Vicuna-v1.5-7B-16k (Zheng et al.,
2023a). ChatGLM2-6B-32k được huấn luyện dựa trên
ChatGLM2-6B, với độ dài ngữ cảnh 32k trong
quá trình liên kết và nội suy vị trí (Chen et al.,
2023). LongChat-v1.5-7B-32k và Vicuna-v1.5-7B-16k được tinh chỉnh từ Llama2-7B, với tinh chỉnh có giám sát và tỷ lệ RoPE tuyến tính.

Chúng tôi tiến hành đánh giá trong thiết lập zero-shot,
ngoại trừ các nhiệm vụ học ít mẫu trong đó
các ví dụ ít mẫu được cung cấp như một phần của
ngữ cảnh dài. Lời nhắc định dạng đầu vào và
độ dài đầu ra tối đa mà chúng tôi sử dụng trong quá trình đánh giá
có thể được tìm thấy trong Phụ lục. Khi độ dài đầu vào
L vượt quá độ dài ngữ cảnh tối đa M của một
mô hình (được chỉ ra bởi hậu tố của tên nó), chúng tôi cắt
chuỗi đầu vào S từ giữa vì
phần đầu và cuối của chuỗi có thể chứa thông
tin quan trọng như hướng dẫn hoặc câu hỏi:
S1:L→[S1:⌊M/2⌋;SL−⌊M/2⌋−1:L]. Trong quá trình tạo
sinh, chúng tôi sử dụng giải mã tham lam để có tính tái tạo.
Đáng chú ý là các mô hình trò chuyện được đánh giá thường
có các lời nhắc cụ thể khiến các mô
hình tạo ra các phản hồi giống như đối thoại. Trong
quá trình đánh giá, chúng tôi tránh thêm những lời nhắc này trong học ít mẫu và các nhiệm vụ hoàn thành mã, vì các câu trả lời cho những nhiệm vụ này nên được tạo ra theo
kiểu hoàn thành thay vì kiểu trò chuyện.

Chỉ số cho mỗi bộ dữ liệu được hiển thị trong Bảng 1.
Đối với các nhiệm vụ được xây dựng dựa trên các bộ dữ liệu trước đó, các chỉ
số chúng tôi sử dụng phù hợp với những chỉ số được sử dụng trong
nghiên cứu gốc. F1 và ROUGE-L (Lin, 2004) là

--- TRANG 7 ---
GPT-3.5-Turbo-16k ChatGLM2-6B-32k Vicuna-v1.5-7B-16k2530354045Điểm TB44.7
41.5
30.544.2
39.3
30.539.1
35.4
30.2Độ dài tối đa
Cắt 8k
Cắt 4kHình 2: Điểm trung bình (%) dưới kích thước cắt khác nhau.

hai chỉ số dựa trên N-gram phổ biến được áp dụng rộng rãi
trong các nhiệm vụ QA và tóm tắt. Edit Sim (khoảng cách Levenshtein) được sử dụng phổ biến trong đánh giá tạo mã (Svyatkovskiy et al., 2020). Đối với
các nhiệm vụ học ít mẫu, chúng tôi trích xuất dòng đầu tiên
của phản hồi. Đối với hai nhiệm vụ hoàn thành mã, chúng tôi trích xuất dòng đầu tiên của việc tạo ra mô hình
không phải là bình luận. Mã và bộ dữ liệu có sẵn tại https://github.com/THUDM/
LongBench.

Kết quả trên LongBench. Bảng 2, 3 báo cáo
hiệu suất (%) trên tất cả các bộ dữ liệu trong LongBench.
Ngoài ra, Hình 4 trình bày một biểu đồ radar mô
tả khả năng của các mô hình trên 6 nhiệm vụ chính.
Để hình dung tốt hơn, chúng tôi chia tỷ lệ điểm số
tối đa trên tất cả các mô hình trên mỗi nhiệm vụ thành 100 trong
biểu đồ radar. Chúng tôi tóm tắt các phát hiện chính
từ kết quả thí nghiệm: (1) Vẫn còn
khoảng cách hiệu suất trên các nhiệm vụ ngữ cảnh dài giữa
các mô hình mã nguồn mở kích thước nhỏ hơn và mô
hình thương mại (GPT-3.5-Turbo-16k). (2) Các mô hình được
hưởng lợi từ nhúng vị trí có tỷ lệ và tiếp tục
huấn luyện trên ngữ cảnh dài hơn, vì ChatGLM2-6B-
32k và LongChat-v1.5-7B-32k đạt được các cải thiện tương đối
lần lượt là 62% và 19%. Chúng tôi phân
tích thêm tính chất đa nhiệm vụ của LongBench
bằng mối tương quan giữa các nhiệm vụ trong và giữa mỗi
danh mục nhiệm vụ trong Phụ lục D. Chúng tôi tìm thấy các mối tương quan cao hơn cho hiệu suất trên các nhiệm vụ cùng
danh mục hoặc ngôn ngữ.

Để nghiên cứu liệu các mô hình có độ dài tối đa dài hơn
có thực sự được hưởng lợi từ việc sử dụng ngữ cảnh dài hơn hay không,
chúng tôi tiến hành thí nghiệm với GPT-Turbo-
3.5-16k, ChatGLM2-6B-32k và Vicuna-v1.5-7B-
16k với kích thước cắt 4k và 8k trên Long-
Bench. Điểm số trung bình vĩ mô trên tất cả các nhiệm vụ
với các kích thước cắt khác nhau được mô tả trong Hình 2. Ở đây, 'độ dài tối đa' biểu thị việc cắt
Hình 3: Điểm trung bình (%) dưới độ dài ngữ cảnh khác nhau
trên LongBench-E.

ở cấu hình độ dài tối đa của mô hình. Chúng tôi
quan sát thấy GPT-Turbo-3.5-16k và ChatGLM2-
6B-32k đạt điểm số cao hơn dưới kích thước cắt
lớn hơn, cho thấy chúng có thể tận dụng tốt hơn
ngữ cảnh dài hơn. Hơn nữa, điều này xác nhận rằng
benchmark của chúng tôi thực sự cần thiết cho mô hình hóa ngữ cảnh dài — chỉ sử dụng thông tin bị cắt là không đủ để hoàn thành thành công các nhiệm vụ trong
LongBench. Mặt khác, hiệu suất của
các LLM trên LongBench có thể được cải thiện thêm bằng cách
tăng cường khả năng mô hình hóa ngữ cảnh dài của chúng.

Kết quả trên LongBench-E. Trong khi LongBench tạo điều kiện
cho việc đo lường khả năng đa nhiệm vụ tổng thể trên các nhiệm vụ yêu cầu hiểu biết ngữ cảnh dài,
LongBench-E tập trung nhiều hơn vào việc đo lường hiệu suất của mô hình thay đổi như thế nào dưới các
độ dài ngữ cảnh khác nhau trong cùng một nhiệm vụ. Như được giới thiệu
trong Mục 3.2.2, LongBench-E chứa một tập con
các bộ dữ liệu được bao gồm trong LongBench, có
độ dài ngữ cảnh được phân phối đều hơn. Hình 3 báo cáo
điểm số trung bình vĩ mô (%) trên dữ liệu trong các phạm vi độ dài
0-4k, 4k-8k và 8k+ (Xem kết quả trên
tất cả các bộ dữ liệu trong Bảng 9). Có thể suy ra khả năng ngữ cảnh dài của mô hình từ độ dốc của đường cong —
sự sụt giảm đáng kể về hiệu suất trên dữ liệu có
độ dài lớn hơn, như được chỉ ra bởi một đường cong dốc hơn, chỉ ra
những hạn chế của mô hình trong việc xử lý hiệu quả mô hình hóa văn bản dài. Từ kết quả trên LongBench-E, chúng tôi
quan sát thấy ChatGLM2-6B-32k và LongChat-
v1.5-7B-32k mạnh mẽ hơn đối với độ dài ngữ cảnh dài hơn, với sự sụt giảm tương đối lần lượt là 4% và 7% từ
0-4k đến 8k+. Hơn nữa, mặc dù GPT-
3.5-Turbo-16k thể hiện hiệu suất tổng thể ấn tượng
trên tất cả các nhiệm vụ, chúng tôi phát hiện rằng nó vẫn
gặp khó khăn trên ngữ cảnh dài hơn (-17% từ 0-4k đến
8k+), để lại chỗ cho sự phát triển trong tương lai về ngữ cảnh dài

--- TRANG 8 ---
Bộ truy xuấtHỏi đáp Tài liệu Đơn Hỏi đáp Đa Tài liệuTB
1-1 1-2 1-3 1-4 2-1 2-2 2-3 2-4
GPT-3.5-Turbo-16k
không truy xuất 23.6 43.3 52.3 61.2 51.6 37.7 26.9 28.7 40.7
E-200×7 21.8 38.1 52.8 53.6 46.6 44.9 30.4 30.7 39.9
E-500×3 21.8 39.6 50.3 55.9 49.3 38.6 23.3 30.4 38.6
C-200×7 18.3 35.6 54.3 52.4 47.0 39.5 25.2 30.5 37.8
C-500×3 20.3 35.7 48.7 51.2 47.7 39.1 21.9 30.7 36.9
B-200×7 14.1 28.6 30.1 55.0 38.3 29.0 18.1 29.6 30.3
B-500×3 14.5 30.4 31.3 55.1 37.2 35.1 11.7 29.9 30.6
Llama2-7B-chat-4k
không truy xuất 18.7 19.2 36.8 11.9 25.4 32.8 9.4 5.2 19.9
E-200×7 20.0 25.7 40.3 13.9 34.7 34.4 17.3 5.5 24.0
E-500×3 17.7 25.2 38.9 12.0 34.9 32.8 15.5 5.0 22.7
C-200×7 18.3 23.8 41.8 10.8 33.6 34.5 17.2 5.0 23.1
C-500×3 17.1 22.5 39.5 9.9 34.6 35.0 14.1 4.7 22.2
B-200×7 12.3 19.6 25.9 13.1 29.2 25.9 9.1 5.1 17.5
B-500×3 14.7 20.4 26.2 13.5 23.1 29.7 7.9 5.0 17.6
ChatGLM2-6B-32k
không truy xuất 21.1 31.5 46.2 51.7 45.1 34.0 21.9 37.6 36.1
E-200×7 19.4 33.3 40.9 48.3 41.2 32.9 22.8 36.7 34.4
E-500×3 14.6 31.2 40.5 46.3 39.4 31.5 20.2 38.1 32.7
C-200×7 15.1 32.9 43.1 45.8 38.3 32.3 16.9 35.5 32.5
C-500×3 12.9 29.6 41.1 49.2 38.1 33.2 17.5 37.8 32.4
B-200×7 12.5 20.1 23.8 50.2 28.7 24.3 10.9 35.0 25.7
B-500×3 11.2 20.5 25.4 51.9 27.7 27.6 12.2 35.6 26.5

Bảng 4: Kết quả nén ngữ cảnh dựa trên truy xuất
(%) trên LongBench. E, C, B biểu thị các phương pháp truy xuất khác nhau,
cụ thể là text-embedding-ada-002, Contriever,
và BM25. M×N chỉ ra việc truy xuất top-N
phân đoạn khi chia thành các khối bằng M từ. Đối với mọi
mô hình và mọi bộ dữ liệu, hiệu suất tốt nhất trên tất cả
các phương pháp truy xuất được in đậm.

Mô hình 3-1 3-2 3-3 3-4 TB
GPT-3.5-Turbo-16k 29.5 23.4 26.7 16.0 23.9
GPT-3.5-Turbo-16k+Tóm tắt 17.9 16.6 17.9 19.7 18.0
Llama2-7B-chat-4k 27.3 20.8 25.8 0.2 18.5
Llama2-7B-chat-4k+Tóm tắt 12.8 16.6 4.6 0.6 8.6
ChatGLM2-6B-32k 32.4 24.0 26.5 16.2 24.8
ChatGLM2-6B-32k+Tóm tắt 17.6 15.9 14.9 17.2 16.4

Bảng 5: Kết quả nén ngữ cảnh dựa trên tóm tắt
(%) trên LongBench.

mô hình hóa.

4.2 Tác động của Kỹ thuật Nén Ngữ cảnh

Chúng tôi khám phá thêm tác động của các kỹ thuật nén ngữ cảnh
trên LongBench, bao gồm nén ngữ cảnh dựa trên truy xuất
và nén ngữ cảnh dựa trên tóm tắt. Truy xuất được sử dụng rộng rãi
trong việc tăng cường các mô hình ngôn ngữ với bộ
nhớ ngoài (Khandelwal et al., 2020; Borgeaud et al.,
2022; Izacard et al., 2022b). Ứng dụng này có thể
được mở rộng để xem xét các ngữ cảnh dài hơn, như
tài liệu hoặc sách, như các dạng bộ nhớ ngoài,
từ đó thông tin liên quan có thể được truy xuất
bằng một truy vấn cụ thể. Cho một ngữ cảnh dài, chúng tôi
trước tiên chia nó thành các khối với kích thước mặc định M
từ (hoặc ký tự trên các bộ dữ liệu tiếng Trung), sau đó sử dụngmột bộ truy xuất cụ thể để tính toán nhúng của các
khối văn bản và truy vấn, và chỉ nối các
khối top-N theo độ tương tự cosine của
nhúng của chúng với nhúng truy vấn. Các
khối top-N làm ngữ cảnh nén, cùng với
truy vấn, sau đó được đưa vào mô hình để tạo ra
câu trả lời. Một pipeline tương tự cũng được triển khai
trong LangChain. Chúng tôi thí nghiệm với ba bộ truy
xuất — OpenAI Embedding (text-embedding-ada-
002 (OpenAI, 2022b)), Contriever (Izacard et al.,
2022a), và BM25 — cùng với hai kích thước khối
200 và 500. Để tiến hành so sánh công bằng hơn
dưới cùng độ dài ngữ cảnh, chúng tôi lấy
top-7 và top-3 khối tương ứng khi kích thước khối
lần lượt là 200 và 500. Bảng 4 báo cáo
kết quả trên các nhiệm vụ QA trong LongBench. Chúng tôi tóm
tắt các phát hiện của chúng tôi: (1) text-embedding-ada-002 thực
hiện tốt nhất trong ba bộ truy xuất, trong khi
kết quả Contriever mã nguồn mở gần với
text-embedding-ada-002 và vượt trội hơn BM25. (2)
Nói chung, chia ngữ cảnh dài thành các khối ngắn hơn
và truy xuất nhiều khối hơn dẫn đến hiệu suất tốt hơn. (3) Dưới phương pháp truy xuất tốt nhất,
các cải thiện cho ba mô hình lần lượt là -2%,
21% và -5%. Hơn nữa, ngay cả sau
truy xuất, hiệu suất của Llama2-7B-chat-4k
vẫn tụt hậu so với hai mô hình khác. Kết quả
cho thấy kỹ thuật truy xuất chỉ có thể phục vụ
như một sự bù đắp hiệu suất cho các mô hình không thể
mô hình hóa ngữ cảnh dài tốt, và không phải là lối tắt
để giải quyết các nhiệm vụ hiểu biết ngữ cảnh dài.

Chúng tôi cũng nghiên cứu hiệu ứng của việc sử dụng bản tóm tắt
do mô hình tạo ra như một kỹ thuật nén ngữ cảnh. Cụ thể, chúng tôi trước tiên sử dụng mô hình để
tạo ra một bản tóm tắt ngắn gọn cho mỗi khối văn bản, và
nối các bản tóm tắt lại với nhau làm
ngữ cảnh nén. Chúng tôi thí nghiệm trên các
nhiệm vụ tóm tắt trong LongBench, và kết quả như
được hiển thị trong Bảng 5. Chúng tôi phát hiện rằng phương pháp nén này
cải thiện hiệu suất của các mô hình
chỉ trên nhiệm vụ VCSUM (3-4), vì dữ liệu trong
VCSUM dài hơn so với ba bộ dữ liệu khác.

4.3 Hiểu biết Ngữ cảnh hay Ghi nhớ?

Vì mô hình có thể đã gặp
ngữ cảnh dài trong quá trình tiền huấn luyện, nó có thể dựa vào
ghi nhớ thay vì hiểu biết ngữ cảnh để
trả lời câu hỏi. Chúng tôi tiến hành một thí nghiệm
để đánh giá mức độ mà các nhiệm vụ này dựa vào
ghi nhớ thay vì khả năng hiểu biết ngữ cảnh dài. Cụ thể, chúng tôi giữ lại ngữ cảnh dài

--- TRANG 9 ---
Mô hình NarrativeQA Qasper MultiFieldQA-en MultiFieldQA-zh HotpotQA 2WikiMQA MuSiQue DuReader
GPT-3.5-Turbo-16k (không có ngữ cảnh) 4.7 12.4 15.7 10.9 31.7 28.9 15.0 17.1
GPT-3.5-Turbo-16k 23.6 (+18.9) 43.3 (+30.9) 52.3 (+36.6) 61.2 (+50.3) 51.6 (+19.9) 37.7 (+8.8) 26.9 (+11.9) 28.7 (+11.6)
Llama2-7B-chat-4k (không có ngữ cảnh) 7.9 12.5 16.4 5.1 25.5 28.2 9.8 2.6
Llama2-7B-chat-4k 18.7 (+10.8) 19.2 (+6.7) 36.8 (+20.4) 11.9 (+6.8) 25.4 (-0.1) 32.8 (+4.6) 9.4 (-0.4) 5.2 (+2.6)
ChatGLM2-6B-32k (không có ngữ cảnh) 8.9 14.2 12.5 20.4 17.0 19.9 7.7 16.9
ChatGLM2-6B-32k 21.1 (+12.2) 31.5 (+17.3) 46.2 (+33.7) 51.6 (+31.2) 45.1 (+28.1) 34.0 (+14.1) 21.9 (+14.2) 37.6 (+20.7)

Bảng 6: Đánh giá hiểu biết ngữ cảnh so với ghi nhớ.

từ mô hình, chỉ đặt câu hỏi cho
nó, và đánh giá hiệu suất của nó. Bảng 6 hiển thị
kết quả cho GPT-3.5-Turbo-16k, Llama2-7B-chat-
4k và ChatGLM2-6B-32k trên các bộ dữ liệu Hỏi đáp Tài liệu Đơn
và Hỏi đáp Đa Tài liệu trong LongBench.

Chúng tôi quan sát rằng hiệu suất ghi nhớ
(không có ngữ cảnh) trên HotpotQA, 2WikiMultihopQA,
và MusiQue tương đối cao. Điều này có thể
là do các bộ dữ liệu này được lấy từ Wikipedia, một
nguồn phổ biến để huấn luyện các LLM phổ biến. Điểm số
∆ (điểm số gốc trừ điểm số khi không có ngữ
cảnh) giải quyết hiện tượng ghi nhớ
(Yu et al., 2024), và cũng có thể phục vụ như một
chỉ báo quan trọng cho khả năng hiểu biết ngữ cảnh dài của mô hình.

5 Kết luận

Trong bài báo này, chúng tôi giới thiệu LongBench, một benchmark đa
nhiệm vụ song ngữ được thiết kế riêng để đánh giá khả năng hiểu biết ngữ cảnh dài của các LLM. Long-
Bench bao gồm sáu danh mục chính và tổng cộng 21
nhiệm vụ, với độ dài dữ liệu kéo dài từ hàng nghìn
token lên đến hàng chục nghìn token. Chúng tôi cũng phát triển LongBench-E có
phân phối độ dài dữ liệu đều hơn. Chúng tôi tiến hành các
thí nghiệm mở rộng trên LongBench và LongBench-E,
đưa ra các kết luận sâu sắc về khả
năng của các LLM hiện tại về hiểu biết ngữ cảnh dài.
Hơn nữa, phân tích của chúng tôi cho thấy LongBench
và LongBench-E phục vụ như các bệ thử nghiệm lý tưởng cho nghiên cứu trong tương lai về mô hình hóa ngữ cảnh dài.

6 Hạn chế

Mặc dù LongBench cung cấp một bệ thử nghiệm toàn diện hơn
cho hiểu biết ngữ cảnh dài, nó vẫn có
những thiếu sót, như chúng tôi tóm tắt dưới đây. (1)
Các chỉ số tự động có thể không đáng tin cậy: Như các
nghiên cứu trước đây cho thấy (Bai et al., 2023), các
chỉ số đánh giá tự động (ROUGE-L, F1) có thể
không phản ánh chính xác chất lượng của phản hồi.
Đặc biệt, kết quả trên các chỉ số này có thể bị
đánh giá thấp đối với các mô hình được sử dụng để tạo ra các phản hồi dài hơn. Mặc dù việc sử dụng LLM làm
người kiểm tra có thể giảm vấn đề này (Bai et al., 2023;
An et al., 2023), chi phí thời gian chạy cho đánh
giá có thể cao, và LLM cũng có thiên vị khi
được sử dụng như một chỉ số đánh giá (Zheng et al., 2023a).
(2) Kết hợp với khả năng làm theo hướng dẫn: Mục tiêu chính của chúng tôi là đánh giá khả năng
thành thạo của các mô hình trong mô hình hóa ngữ cảnh dài bất kể
khả năng làm theo hướng dẫn của chúng. Tuy nhiên,
vì các nhiệm vụ trong LongBench gần hơn với các ứng dụng thế giới thực, việc làm chủ chúng không thể tránh khỏi đòi hỏi
một mức độ nhất định của khả năng làm theo hướng dẫn.
Do đó, hiệu suất trên LongBench được
kết hợp với khả năng làm theo hướng dẫn của các mô hình.

Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi một khoản tài trợ từ
Viện Guo Qiang, Đại học Tsinghua
(2019GQB0003), Zhipu AI và Chương trình Nghiên cứu Khoa học Sáng kiến của Đại học Tsinghua. Chúng tôi
cũng muốn cảm ơn các nhà đánh giá ẩn danh
vì những đề xuất của họ.

Tài liệu tham khảo

Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago
Ontañón, Siddhartha Brahma, Yury Zemlyanskiy,
David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay,
et al. 2023. Colt5: Faster long-range transform-
ers with conditional computation. arXiv preprint
arXiv:2303.09752.

Chenxin An, Shansan Gong, Ming Zhong, Mukai
Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.
2023. L-eval: Instituting standardized evaluation
for long context language models. arXiv preprint
arXiv:2307.11088.

Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He,
Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao,
Haozhe Lyu, et al. 2023. Benchmarking foundation
models with language-model-as-an-examiner. arXiv
preprint arXiv:2306.04181.

Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150.

--- TRANG 10 ---
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning, pages 2206–2240. PMLR.

Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev.
2023. Scaling transformer to 1m tokens and beyond
with rmt. arXiv preprint arXiv:2304.11062.

Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
2022. Recurrent memory transformer. Advances
in Neural Information Processing Systems, 35:11079–
11091.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
arXiv preprint arXiv:2306.15595.

Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-xl: Attentive language models beyond
a fixed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics, pages 2978–2988.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
Noah A Smith, and Matt Gardner. 2021. A dataset
of information-seeking questions and answers an-
chored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 4599–4610.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,
Shaohan Huang, Wenhui Wang, and Furu Wei. 2023.
Longnet: Scaling transformers to 1,000,000,000 to-
kens. arXiv preprint arXiv:2307.02486.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 320–335.

Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi
Li, and Dragomir Radev. 2019. Multi-news: A large-
scale multi-document summarization dataset and ab-
stractive hierarchical model. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1074–1084.

William Fedus, Barret Zoph, and Noam Shazeer. 2022.
Switch transformers: Scaling to trillion parame-
ter models with simple and efficient sparsity. The
Journal of Machine Learning Research, 23(1):5232–
5270.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-
sander Wawer. 2019. Samsum corpus: A human-
annotated dialogue dataset for abstractive summa-
rization. EMNLP-IJCNLP 2019, page 70.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Ju-
lian McAuley. 2023. Longcoder: A long-range pre-
trained language model for code completion. arXiv
preprint arXiv:2306.14893.

Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao,
Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu,
Qiaoqiao She, et al. 2018. Dureader: a chinese ma-
chine reading comprehension dataset from real-world
applications. ACL 2018, page 37.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language under-
standing. In International Conference on Learning
Representations.

Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing a multi-hop
qa dataset for comprehensive evaluation of reasoning
steps. In Proceedings of the 28th International Con-
ference on Computational Linguistics, pages 6609–
6625.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, and Lu Wang. 2021. Efficient attentions for long
document summarization. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1419–1436.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022a. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022b. Few-shot learning with re-
trieval augmented language models. arXiv preprint
arXiv:2208.03299.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1601–1611.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations.

--- TRANG 11 ---
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. In Inter-
national Conference on Learning Representations.

Tomáš Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris
Dyer, Karl Moritz Hermann, Gábor Melis, and Ed-
ward Grefenstette. 2018. The narrativeqa reading
comprehension challenge. Transactions of the Asso-
ciation for Computational Linguistics, 6:317–328.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-
min Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe
Ma, and Hao Zhang. 2023. How long can open-
source llms truly promise on context length?

Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In COLING 2002: The 19th International
Conference on Computational Linguistics.

Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu,
Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023.
Unleashing infinite-length input capacity for large-
scale language models with self-controlled memory
system. arXiv preprint arXiv:2304.13343.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out, pages 74–81.

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023a. Lost in the middle: How lan-
guage models use long contexts. arXiv preprint
arXiv:2307.03172.

Tianyang Liu, Canwen Xu, and Julian McAuley.
2023b. Repobench: Benchmarking repository-
level code auto-completion systems. arXiv preprint
arXiv:2306.03091.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-
anyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,
Kaiwen Men, Kejuan Yang, et al. 2023c. Agent-
bench: Evaluating llms as agents. arXiv preprint
arXiv:2308.03688.

Pedro Henrique Martins, Zita Marinho, and André FT
Martins. 2022. ∞-former: Infinite memory trans-
former. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 5468–5485.

Amirkeivan Mohtashami and Martin Jaggi. 2023.
Landmark attention: Random-access infinite con-
text length for transformers. arXiv preprint
arXiv:2305.16300.

Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang,
Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz,
Philippe Laban, et al. 2023. Long sequence modeling
with xgen: A 7b llm trained on 8k input sequence
length. Salesforce AI Research Blog.

NLPCC. 2014. Task definition for large scale text cate-
gorization at nlpcc 2014.

OpenAI. 2022a. Introducing chatgpt.

OpenAI. 2022b. Openai: New and improved embed-
ding model.

Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan
Fernando, Caglar Gulcehre, Razvan Pascanu, and
Soham De. 2023. Resurrecting recurrent neu-
ral networks for long sequences. arXiv preprint
arXiv:2303.06349.

Ofir Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Confer-
ence on Learning Representations.

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
Chloe Hillier, and Timothy P Lillicrap. 2020. Com-
pressive transformers for long-range sequence mod-
elling. In International Conference on Learning Rep-
resentations.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research,
21(1):5485–5551.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier. 2021. Efficient content-based sparse
attention with routing transformers. Transactions of
the Association for Computational Linguistics, 9:53–
68.

Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-
rant, and Omer Levy. 2023. Zeroscrolls: A zero-
shot benchmark for long text understanding. arXiv
preprint arXiv:2305.14196.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori
Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor
Geva, Jonathan Berant, et al. 2022. Scrolls: Stan-
dardized comparison over long language sequences.
In Proceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing, pages
12007–12021.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta, Adrià
Garriga-Alonso, et al. 2023. Beyond the imitation
game: Quantifying and extrapolating the capabili-
ties of language models. Transactions on Machine
Learning Research.

Simeng Sun, Kalpesh Krishna, Andrew Mattarella-
Micke, and Mohit Iyyer. 2021. Do long-range lan-
guage models actually use long-range context? In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pages 807–
822.

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2022. A length-extrapolatable
transformer. arXiv preprint arXiv:2212.10554.

--- TRANG 12 ---
Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
and Neel Sundaresan. 2020. Intellicode compose:
Code generation using transformer. In Proceedings
of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foun-
dations of Software Engineering, pages 1433–1443.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. 2021. Long
range arena: A benchmark for efficient transformers.
In International Conference on Learning Representa-
tions.

Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-
zler. 2022. Efficient transformers: A survey. ACM
Comput. Surv., 55(6).

InternLM Team. 2023. Internlm: A multilingual
language model with progressively enhanced capa-
bilities. https://github.com/InternLM/
InternLM.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. musique: Multi-
hop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics, 10:539–554.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768.

Han Wu, Mingjie Zhan, Haochen Tan, Zhaohui Hou,
Ding Liang, and Linqi Song. 2023. VCSUM: A ver-
satile Chinese meeting summarization dataset. In
Findings of the Association for Computational Lin-
guistics: ACL 2023, pages 6065–6079.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,
and Christian Szegedy. 2022. Memorizing transform-
ers. In International Conference on Learning Repre-
sentations.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D Manning. 2018. Hotpotqa: A dataset for
diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2369–2380.

Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao,
Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiao-
han Zhang, Hanming Li, et al. 2024. Kola: Carefully
benchmarking world knowledge of large language
models. In The Twelfth International Conference on
Learning Representations.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. Advances in neural information
processing systems, 33:17283–17297.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2023. Glm-130b: An
open bilingual pre-trained model. In The Eleventh In-
ternational Conference on Learning Representations.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023a.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXiv preprint arXiv:2306.05685.

Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan
Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang,
Yang Li, et al. 2023b. Codegeex: A pre-trained
model for code generation with multilingual bench-
marking on humaneval-x. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 5673–5684.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia
Mutuma, Rahul Jha, Ahmed Hassan, Asli Celikyil-
maz, Yang Liu, Xipeng Qiu, et al. 2021. Qmsum: A
new benchmark for query-based multi-domain meet-
ing summarization. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 5905–5921.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui,
Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cot-
terell, and Mrinmaya Sachan. 2023. Recurrentgpt:
Interactive generation of (arbitrarily) long text. arXiv
preprint arXiv:2305.13304.

--- TRANG 13 ---
A Chi tiết Bộ dữ liệu

Bảng 7 liệt kê việc khởi tạo (I, C, A) cho mỗi bộ dữ liệu trong LongBench. Bảng 8 báo cáo số lượng
dữ liệu trên mỗi nhiệm vụ rơi vào phạm vi độ dài 0-4k, 4k-8k và 8k+ trong LongBench-E.

Bộ dữ liệu Đầu vào I Ngữ cảnh C Câu trả lời A
Hỏi đáp Tài liệu Đơn
NarrativeQA Câu hỏi Tài liệu Câu trả lời
Qasper Câu hỏi Tài liệu Câu trả lời
MultiFieldQA-en Câu hỏi Tài liệu Câu trả lời
MultiFieldQA-zh Câu hỏi Tài liệu Câu trả lời
Hỏi đáp Đa Tài liệu
HotpotQA Câu hỏi Nhiều tài liệu Câu trả lời
2WikiMultihopQA Câu hỏi Nhiều tài liệu Câu trả lời
MuSiQue Câu hỏi Nhiều tài liệu Câu trả lời
DuReader Câu hỏi Nhiều tài liệu Câu trả lời
Tóm tắt
GovReport - Tài liệu Tóm tắt
QMSum Truy vấn Tài liệu Tóm tắt
MultiNews - Tài liệu Tóm tắt
VCSUM - Tài liệu Tóm tắt
Học Ít mẫu
TREC Câu hỏi Ví dụ ít mẫu Nhãn lớp
TriviaQA Đoạn văn&Câu hỏi Ví dụ ít mẫu Câu trả lời
SAMSum Đối thoại Ví dụ ít mẫu Tóm tắt
LSHT Tài liệu tin tức Ví dụ ít mẫu Nhãn lớp
Nhiệm vụ Tổng hợp
PassageCount - Nhiều đoạn văn Số đếm
PassageRetrieval-en Tóm tắt Nhiều đoạn văn Tiêu đề của đoạn văn
PassageRetrieval-zh Tóm tắt Nhiều đoạn văn Tiêu đề của đoạn văn
Hoàn thành Mã
LCC - Các dòng mã trước đó Dòng mã tiếp theo
RepoBench-P Các dòng mã trước đó Đoạn mã khác tập tin Dòng mã tiếp theo

Bảng 7: Khởi tạo (I, C, A) cho mỗi nhiệm vụ trong LongBench.

Chi tiết về nguồn tài liệu MultiFieldQA. Nguồn của các tài liệu trong MultiFieldQA bao gồm:
• Arxiv (cho các bài báo học thuật): truy cập mở và có thể được tải xuống miễn phí bởi bất kỳ ai.
• Bộ dữ liệu C4: bộ dữ liệu có sẵn công khai với giấy phép ODC-BY.
• WuDaoCorpora: bộ dữ liệu truy cập mở.
• Chinese Judgements Online (cho các tài liệu pháp lý tiếng Trung): trang web tải xuống bản án tiếng Trung mở.
• Wikipedia (cho bách khoa toàn thư): cấp quyền truy cập miễn phí và được cấp phép theo CC BY-SA.
• Chinese Government Website (cho báo cáo chính phủ tiếng Trung): trang web tải xuống báo cáo chính phủ tiếng Trung mở.

Hướng dẫn chú thích cho MultiFieldQA. Ở đây chúng tôi cung cấp các hướng dẫn chú thích: "Vui lòng đề xuất
một câu hỏi và một câu trả lời đúng cho mỗi tài liệu sau. Yêu cầu: 1. Các câu hỏi
nên càng rõ ràng càng tốt và có các câu trả lời xác định và tương đối ngắn. 2. Sự phân phối của các
đoạn văn bằng chứng nên càng ngẫu nhiên càng tốt trong suốt bài báo. 3. Đảm bảo rằng các câu hỏi
có các loại đa dạng, bao gồm, nhưng không giới hạn ở, các câu hỏi trích xuất thông tin (ví dụ: thời gian của một

--- TRANG 14 ---
Bộ dữ liệu #dữ liệu trong 0-4k #dữ liệu trong 4k-8k #dữ liệu trong 8k+
Hỏi đáp Tài liệu Đơn
Qasper 100 100 24
MultiFieldQA-en 67 70 13
Hỏi đáp Đa Tài liệu
HotpotQA 100 100 100
2WikiMultihopQA 100 100 100
Tóm tắt
GovReport 100 100 100
MultiNews 100 100 94
Học Ít mẫu
TREC 100 100 100
TriviaQA 100 100 100
SAMSum 100 100 100
Nhiệm vụ Tổng hợp
PassageCount 100 100 100
PassageRetrieval-en 100 100 100
Hoàn thành Mã
LCC 100 100 100
RepoBench-P 100 100 100

Bảng 8: Phân phối độ dài dữ liệu trong LongBench-E.

sự kiện, ngày sinh của một người, v.v.), các câu hỏi tóm tắt (ví dụ: bài báo chủ yếu mô tả những người nào), và các câu hỏi lập luận đa bước."

Thời gian trung bình để chú thích mỗi mẫu dữ liệu trong MultiFieldQA là khoảng 5 phút. Các người chú thích tham gia vào quá trình này là các sinh viên Tiến sĩ có kinh nghiệm nghiên cứu sâu rộng trong lĩnh vực
NLP, định vị họ như những người chú thích chuyên gia. Xác thực chéo giữa các người chú thích cho thấy tỷ lệ chính xác 100% của các câu trả lời được chú thích.

B Thiết lập Đánh giá

Lời nhắc Đánh giá. Trong phần này, chúng tôi trình bày một bộ sưu tập các mẫu lời nhắc tùy chỉnh được thiết kế
cho mỗi bộ dữ liệu trong LongBench, được sử dụng trong quá trình đánh giá của chúng tôi. Nhớ lại rằng mỗi trường hợp dữ liệu đi kèm với một đầu vào I cũng như một ngữ cảnh C. Chúng tôi đặt hướng dẫn cả ở đầu và cuối
của lời nhắc, đảm bảo các mô hình hiểu đầy đủ những gì cần làm.

NarrativeQA: Bạn được đưa cho một câu chuyện, có thể là một tiểu thuyết hoặc kịch bản phim, và một câu hỏi.
Trả lời câu hỏi một cách ngắn gọn nhất có thể, sử dụng một cụm từ đơn nếu có thể. Không cung cấp bất kỳ
giải thích nào.
Câu chuyện: {context}
Bây giờ, trả lời câu hỏi dựa trên câu chuyện một cách ngắn gọn nhất có thể, sử dụng một cụm từ đơn nếu có thể.
Không cung cấp bất kỳ giải thích nào.
Câu hỏi: {input}
Câu trả lời:

Qasper: Bạn được đưa cho một bài báo khoa học và một câu hỏi. Trả lời câu hỏi một cách ngắn gọn nhất
có thể, sử dụng một cụm từ hoặc câu đơn nếu có thể. Nếu câu hỏi không thể được trả lời dựa trên thông
tin trong bài báo, viết "không thể trả lời". Nếu câu hỏi là câu hỏi có/không, trả lời "có",

--- TRANG 15 ---
"không", hoặc "không thể trả lời". Không cung cấp bất kỳ giải thích nào.
Bài báo: {context}
Trả lời câu hỏi dựa trên bài báo trên một cách ngắn gọn nhất có thể, sử dụng một cụm từ hoặc
câu đơn nếu có thể. Nếu câu hỏi không thể được trả lời dựa trên thông tin trong bài báo, viết
"không thể trả lời". Nếu câu hỏi là câu hỏi có/không, trả lời "có", "không", hoặc "không thể trả lời". Không
cung cấp bất kỳ giải thích nào.
Câu hỏi: {input}
Câu trả lời:

MultiField-en: Đọc văn bản sau và trả lời ngắn gọn.
{context}
Bây giờ, trả lời câu hỏi sau dựa trên văn bản trên, chỉ đưa ra câu trả lời và không xuất ra
bất kỳ từ nào khác.
Câu hỏi: {input}
Câu trả lời:

MultiField-zh: 阅读以下文字并用中文简短回答：
{context}
现在请基于上面的文章回答下面的问题，只告诉我答案，不要输出任何其他字词。
问题：{input}
回答：

HotpotQA: Trả lời câu hỏi dựa trên các đoạn văn đã cho. Chỉ đưa ra câu trả lời và không xuất ra
bất kỳ từ nào khác.
Sau đây là các đoạn văn đã cho.
{context}
Trả lời câu hỏi dựa trên các đoạn văn đã cho. Chỉ đưa ra câu trả lời và không xuất ra
bất kỳ từ nào khác.
Câu hỏi: {input}
Câu trả lời:

2WikiMultihopQA: Trả lời câu hỏi dựa trên các đoạn văn đã cho. Chỉ đưa ra câu trả lời và không xuất ra
bất kỳ từ nào khác.
Sau đây là các đoạn văn đã cho.
{context}
Trả lời câu hỏi dựa trên các đoạn văn đã cho. Chỉ đưa ra câu trả lời và không xuất ra
bất kỳ từ nào khác.
Câu hỏi: {input}
Câu trả lời:

MuSiQue: Trả lời câu hỏi dựa trên các đoạn văn đã cho. Chỉ đưa ra câu trả lời và không xuất ra
bất kỳ từ nào khác.
Sau đây là các đoạn văn đã cho.
{context}
Trả lời câu hỏi dựa trên các đoạn văn đã cho. Chỉ đưa ra câu trả lời và không xuất ra
bất kỳ từ nào khác.
Câu hỏi: {input}
Câu trả lời:

DuReader: 请基于给定的文章回答下述问题。
文章：{context}
请基于上述文章回答下面的问题。

--- TRANG 16 ---
问题：{input}
回答：

GovReport: Bạn được đưa cho một báo cáo của một cơ quan chính phủ. Viết một bản tóm tắt một trang của báo cáo.
Báo cáo:
{context}
Bây giờ, viết một bản tóm tắt một trang của báo cáo.
Tóm tắt:

QMSum: Bạn được đưa cho một bản ghi cuộc họp và một truy vấn chứa một câu hỏi hoặc hướng dẫn. Trả lời
truy vấn trong một hoặc nhiều câu.
Bản ghi:
{context}
Bây giờ, trả lời truy vấn dựa trên bản ghi cuộc họp trên trong một hoặc nhiều câu.
Truy vấn: {input}
Câu trả lời:

MultiNews: Bạn được đưa cho một số đoạn tin tức. Viết một bản tóm tắt một trang của tất cả tin tức.
Tin tức:
{context}
Bây giờ, viết một bản tóm tắt một trang của tất cả tin tức.
Tóm tắt:

VCSUM: 下面有一段会议记录，请你阅读后，写一段总结，总结会议的内容。
会议记录：
{context}
会议总结：

TREC: Vui lòng xác định loại của câu hỏi dưới đây. Đây là một số ví dụ về câu hỏi.
{context}
{input}

TriviaQA: Trả lời câu hỏi dựa trên đoạn văn đã cho. Chỉ đưa ra câu trả lời và không
xuất ra bất kỳ từ nào khác. Sau đây là một số ví dụ.
{context}
{input}

SAMSum: Tóm tắt cuộc đối thoại thành một vài câu ngắn. Sau đây là một số ví dụ.
{context}
{input}

LSHT: 请判断给定新闻的类别，下面是一些例子。
{context}
{input}

PassageCount: Có một số đoạn văn dưới đây có nguồn từ Wikipedia. Một số trong số chúng có thể
là bản sao. Vui lòng đọc cẩn thận các đoạn văn này và xác định có bao nhiêu đoạn văn duy nhất
sau khi loại bỏ các bản sao. Nói cách khác, có bao nhiêu đoạn văn không lặp lại tổng cộng?
{context}
Vui lòng nhập số đếm cuối cùng của các đoạn văn duy nhất sau khi loại bỏ các bản sao. Định dạng đầu ra chỉ nên

--- TRANG 17 ---
chứa số, chẳng hạn như 1, 2, 3, và tiếp tục.
Câu trả lời cuối cùng là:

PassageRetrieval-en: Đây là 30 đoạn văn từ Wikipedia, cùng với một bản tóm tắt. Vui lòng
xác định đoạn văn nào mà bản tóm tắt đến từ.
{context}
Sau đây là một bản tóm tắt.
{input}
Vui lòng nhập số của đoạn văn mà bản tóm tắt đến từ. Định dạng câu trả lời phải như
"Đoạn văn 1", "Đoạn văn 2", v.v.
Câu trả lời là:

PassageRetrieval-zh: 以下是若干段落文字，以及其中一个段落的摘要。请确定给定的摘要
出自哪一段。
{context}
下面是一个摘要
{input}
请输入摘要所属段落的编号。答案格式必须是"段落1"，"段落2"等格式
答案是：

LCC: Vui lòng hoàn thành mã được đưa ra dưới đây.
{context}Dòng mã tiếp theo:

RepoBench-P: Vui lòng hoàn thành mã được đưa ra dưới đây.
{context}{input}Dòng mã tiếp theo:

Độ dài Đầu ra Tối đa. Chúng tôi đặt độ dài đầu ra tối đa trên mỗi bộ dữ liệu trong quá trình đánh giá để ngăn
các mô hình tạo ra không ngừng.

1-1 1-2 1-3 1-4 2-1 2-2 2-3 2-4 3-1 3-2 3-3 3-4 4-1 4-2 4-3 4-4 5-1 5-2 5-3 6-1 6-2
128 128 64 64 32 32 32 128 512 512 512 512 64 32 128 64 32 32 32 64 64

C Biểu đồ Radar và Phân tích

Tiếng AnhTiếng Trung
Hỏi đáp Tài liệu ĐơnHoàn thành Mã
Nhiệm vụ Tổng hợpTóm tắtHỏi đáp Đa Tài liệu
Học Ít mẫuHỏi đáp Tài liệu ĐơnHoàn thành Mã
Nhiệm vụ Tổng hợpHọc Ít mẫuTóm tắtHỏi đáp Đa Tài liệu

Hình 4: Điểm trung bình trên 6 nhiệm vụ chính, trên các bộ dữ liệu tiếng Anh và tiếng Trung, tương ứng.

Trong số 6 nhiệm vụ chính, tóm tắt và hoàn thành mã có xu hướng không đủ phân biệt.
Điều này có thể do các chỉ số dựa trên độ tương tự (ROUGE-L, Edit Sim) trên các nhiệm vụ này không đủ nhạy cảm để phân biệt tốt giữa các mô hình mạnh và yếu. Trong khi đó, chúng tôi thấy rằng
các nhiệm vụ tổng hợp có xu hướng cung cấp mức độ phân biệt cao hơn, nơi các mô hình hoặc đạt điểm cao
hoặc hiển thị hiệu suất gần như bằng không. Những phát hiện này khiến chúng tôi tin rằng có thể không phải là ý tưởng tốt
để chỉ đơn giản lấy trung bình trên tất cả các nhiệm vụ như dấu hiệu của khả năng ngữ cảnh dài của mô hình, như được sử dụng trong
benchmark trước đó (Shaham et al., 2023) — vì hiệu suất trên các nhiệm vụ phân biệt hơn, chẳng hạn như
các nhiệm vụ tổng hợp trong benchmark của chúng tôi, có thể chi phối thứ hạng cuối cùng. Điều này đòi hỏi một
chiến lược đánh giá như chúng tôi sử dụng trong LongBench đánh giá riêng biệt từng danh mục nhiệm vụ, có khả năng dẫn đến
kết quả benchmark có ý nghĩa hơn.

D Phân tích về Mối tương quan Giữa các Nhiệm vụ trên LongBench

1-11-21-31-42-12-22-32-43-13-23-33-44-14-24-34-45-15-25-36-16-21-11-21-31-42-12-22-32-43-13-23-33-44-14-24-34-45-15-25-36-16-21.00.80.60.40.20.0-0.2-0.4

Hình 5: Mối tương quan Spearman giữa mỗi cặp nhiệm vụ trong LongBench.

Chúng tôi phân tích tính chất đa nhiệm vụ của LongBench bằng mối tương quan giữa các nhiệm vụ trong và giữa mỗi
danh mục nhiệm vụ, như được hiển thị trong Hình 5. Chúng tôi quan sát rằng hầu hết các nhiệm vụ trong cùng danh mục nhiệm vụ có
mối tương quan cao, ngoại trừ PassageCount (5-1), thể hiện mối tương quan thấp với hầu như tất cả các nhiệm vụ
vì các mô hình thực hiện kém (gần như ngẫu nhiên) trên nhiệm vụ khó khăn này. Trong khi đó, chúng tôi chú ý rằng các
mối tương quan giữa Qasper (1-2), RepoBench-P (6-2) và các nhiệm vụ khác cũng thấp hơn, điều này ngụ ý
rằng các nhiệm vụ này có khả năng yêu cầu một mẫu attention khác với các nhiệm vụ khác. Đáng chú ý, các nhiệm vụ trong
cùng ngôn ngữ có mối tương quan cao hơn với nhau, ví dụ: mối tương quan cao giữa các nhiệm vụ tiếng Trung
(1-4, 2-4, 3-4, 4-4, 5-3). Những quan sát này cho thấy LongBench cung cấp kết quả đánh giá toàn diện hơn bằng cách tích hợp các loại nhiệm vụ và ngôn ngữ khác nhau.

E Kết quả đầy đủ trên LongBench-E

Chúng tôi hiển thị kết quả đầy đủ trên LongBench-E trong Bảng 9.

--- TRANG 19 ---
Mô hình Độ dài TBHỏi đáp-Doc Hỏi đáp-Đa Doc Tóm tắt Học ít mẫu Tổng hợp Mã
1-2 1-3 2-1 2-2 3-1 3-3 4-1 4-2 4-3 5-1 5-2 6-1 6-2
GPT-3.5-Turbo-16k0-4k 51.5 45.8 57.4 64.6 49.8 31.3 26.9 57.7 88.1 38.1 9.8 99.0 58.8 52.0
4k-8k 47.4 41.1 43.0 53.0 45.1 29.6 23.4 71.7 91.6 37.1 9.5 90.7 52.2 46.9
8k+ 42.4 27.9 61.8 50.9 23.6 28.4 22.6 75.3 87.4 40.6 1.1 66.7 47.8 42.4
Llama2-7B-chat-4k0-4k 35.9 20.9 43.5 36.8 33.3 31.7 27.1 52.0 81.9 40.6 8.3 17.0 57.5 38.8
4k-8k 30.5 18.0 31.5 29.2 22.5 27.8 22.9 58.0 80.4 37.0 1.9 4.0 49.2 41.8
8k+ 29.6 21.1 31.1 24.4 21.5 25.6 22.0 58.0 83.4 42.1 2.8 9.0 35.6 40.4
LongChat-v1.5-7B-32k0-4k 36.9 28.4 44.1 30.8 26.0 34.0 27.1 50.0 81.0 38.6 0.0 35.0 50.8 54.0
4k-8k 35.3 27.5 37.5 34.6 18.8 30.7 23.1 65.0 81.5 31.7 0.1 22.0 60.7 50.3
8k+ 34.5 14.0 48.6 25.2 19.1 28.4 22.3 61.0 86.6 32.2 0.0 25.0 60.8 50.4
XGen-7B-8k0-4k 32.6 19.4 49.9 34.0 21.9 31.0 27.7 59.0 83.7 25.0 8.0 7.8 37.1 42.4
4k-8k 27.5 17.9 27.5 23.5 19.4 28.0 21.9 70.0 67.9 25.1 4.1 8.0 36.3 35.1
8k+ 27.4 16.7 29.6 26.2 13.6 26.5 21.0 68.0 81.0 25.6 1.0 8.0 30.4 38.8
InternLM-7B-8k0-4k 30.4 19.7 32.0 43.3 24.4 18.0 21.3 50.0 80.0 21.2 8.0 18.0 47.4 32.3
4k-8k 23.0 13.7 16.5 17.5 28.6 9.4 17.4 46.0 77.5 21.4 7.7 7.0 36.0 25.4
8k+ 23.2 26.2 16.0 24.9 15.0 6.6 15.9 36.0 80.5 20.0 4.5 10.0 39.1 28.8
ChatGLM2-6B0-4k 33.1 19.6 45.5 27.8 31.3 29.6 25.6 36.0 76.9 32.8 6.5 22.2 51.3 41.2
4k-8k 28.5 21.1 28.0 19.2 24.6 23.4 21.9 47.0 72.5 29.0 6.0 8.0 49.5 40.9
8k+ 25.5 16.0 19.4 21.7 15.8 20.1 20.4 46.0 69.9 28.2 2.3 5.0 49.0 40.5
ChatGLM2-6B-32k0-4k 44.2 33.9 45.0 47.5 39.9 34.9 27.1 56.0 77.0 33.2 3.0 85.0 55.1 48.3
4k-8k 43.4 33.4 44.8 45.2 38.0 33.2 22.0 68.0 74.7 32.1 4.0 79.0 58.7 45.4
8k+ 43.1 23.4 57.4 42.2 26.4 31.5 21.3 71.0 81.8 33.6 5.0 81.0 55.4 49.3
Vicuna-v1.5-7B-16k0-4k 37.3 29.2 46.4 38.2 30.8 34.1 28.0 56.0 84.2 39.7 7.0 18.0 56.1 40.2
4k-8k 32.3 20.1 32.9 23.9 17.4 30.4 23.7 73.0 85.1 37.3 3.0 7.0 59.5 39.5
8k+ 29.6 21.8 28.1 19.7 12.3 24.4 21.5 68.0 89.9 39.2 1.0 7.0 46.5 41.4

Bảng 9: Kết quả (%) trên LongBench-E.
