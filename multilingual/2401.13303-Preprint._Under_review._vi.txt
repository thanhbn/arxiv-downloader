# 2401.13303.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2401.13303.pdf
# Kích thước tệp: 644169 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bản thảo. Đang được đánh giá.
MaLA-500: Thích Ứng Ngôn Ngữ Quy Mô Lớn của Các Mô Hình Ngôn Ngữ Lớn
Peiqin Lin∗1,2, Shaoxiong Ji∗3, Jörg Tiedemann3, André F. T. Martins4,5,6, Hinrich Schütze1,2
1Trung tâm Xử lý Thông tin và Ngôn ngữ, Đại học LMU Munich
2Trung tâm Học máy Munich3Đại học Helsinki
4Instituto Superior Técnico (Đơn vị ELLIS Lisbon)
5Instituto de Telecomunicações6Unbabel
linpq@cis.lmu.de, shaoxiong.ji@helsinki.fi
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã thúc đẩy tình trạng nghệ thuật trong xử lý ngôn ngữ tự nhiên. Tuy nhiên, thiết kế chủ yếu dành cho tiếng Anh hoặc một tập hợp hạn chế các ngôn ngữ tạo ra khoảng cách đáng kể trong hiệu quả của chúng đối với các ngôn ngữ có ít tài nguyên. Để thu hẹp khoảng cách này, chúng tôi giới thiệu MaLA-500, một mô hình ngôn ngữ lớn mới được thiết kế để bao phủ một phạm vi rộng lớn gồm 534 ngôn ngữ. Để huấn luyện MaLA-500, chúng tôi sử dụng mở rộng từ vựng và tiếp tục huấn luyện trước trên LLaMA 2 với Glot500-c. Đánh giá nội tại của chúng tôi chứng minh rằng MaLA-500 tốt hơn trong việc dự đoán các văn bản đã cho của các ngôn ngữ có ít tài nguyên so với các LLM đa ngôn ngữ hiện có. Hơn nữa, đánh giá ngoại tại của học trong ngữ cảnh cho thấy MaLA-500 vượt trội hơn các LLM trước đó trên SIB200 và Taxi1500 với biên độ đáng kể, tức là 11,68% và 4,82% độ chính xác trung bình marco qua các ngôn ngữ. Chúng tôi phát hành MaLA-500 tại https://huggingface.co/MaLA-LM .

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM), ví dụ như LLaMA (Touvron et al., 2023a;b), Mistral (Jiang et al., 2023; 2024), và ChatGPT,1 đã cho thấy hiệu suất đáng chú ý trong hiểu và tạo sinh ngôn ngữ tự nhiên. Các nghiên cứu tiếp theo (Bang et al., 2023; Lai et al., 2023; Ahuja et al., 2023a;b) quan sát thấy rằng những LLM tập trung vào tiếng Anh này, như LLaMA với chủ yếu là tiếng Anh làm dữ liệu huấn luyện, có khả năng xử lý một số ngôn ngữ phi tiếng Anh có nhiều tài nguyên, được hưởng lợi từ việc bao gồm dữ liệu ngôn ngữ phi tiếng Anh trong quá trình huấn luyện trước. Tuy nhiên, khả năng áp dụng của chúng đối với các ngôn ngữ có ít tài nguyên vẫn còn hạn chế do sự khan hiếm dữ liệu.

Các nghiên cứu trước đây đã phát hành các mô hình đa ngôn ngữ được huấn luyện trước với chủ yếu là kiến trúc transformer chỉ mã hóa, ví dụ như multilingual BERT (Devlin et al., 2019) và XLM-R (Conneau et al., 2020), cho khoảng 100 ngôn ngữ. Sự chuyển đổi mô hình từ chỉ mã hóa sang chỉ giải mã đạt được khả năng mở rộng cho các mô hình ngôn ngữ lớn với hàng tỷ tham số mô hình, dẫn đến sự phát triển của các mô hình đa ngôn ngữ mở. Gần đây, một số LLM đa ngôn ngữ tạo sinh, như XGLM (Lin et al., 2021), mGPT (Shliazhko et al., 2022), và BLOOM (Scao et al., 2022), đã xuất hiện. Đáng chú ý, phạm vi bao phủ ngôn ngữ hiện tại cho những LLM tạo sinh này bị hạn chế lên đến 60 ngôn ngữ, làm nổi bật nhu cầu còn lại cho công việc tiếp theo về các LLM đa ngôn ngữ quy mô lớn cho nhiều ngôn ngữ tự nhiên.

ImaniGooghari et al. (2023) đã đạt được một cột mốc quan trọng trong lĩnh vực thích ứng ngôn ngữ quy mô lớn bằng cách mở rộng phạm vi bao phủ ngôn ngữ của một mô hình ngôn ngữ đa ngôn ngữ quy mô nhỏ, XLM-R (Conneau et al., 2020) - một mô hình tự mã hóa với 278M tham số, từ 100 ngôn ngữ lên một số lượng ấn tượng 534 ngôn ngữ, và giới thiệu một mô hình mở rộng, Glot500-m với 395M tham số. ImaniGooghari et al. (2023) giới thiệu bộ dữ liệu Glot500-c trải rộng 534 ngôn ngữ từ 47 họ ngôn ngữ, và sau đó áp dụng

*Đóng góp bằng nhau.
1https://openai.com/blog/chatgpt

--- TRANG 2 ---
Bản thảo. Đang được đánh giá.
mở rộng từ vựng và tiếp tục huấn luyện trước để tạo ra Glot500-m. Việc giới thiệu Glot500-c giảm thiểu thách thức về sự khan hiếm dữ liệu cho các ngôn ngữ có ít tài nguyên. Hơn nữa, phương pháp thích ứng thuận lợi hơn so với huấn luyện từ đầu, vì nó yêu cầu ít tài nguyên tính toán hơn và tạo ra dấu chân carbon nhỏ hơn. Thành công này đóng vai trò như một động lực mạnh mẽ cho việc khám phá của chúng tôi về thích ứng ngôn ngữ quy mô lớn của các LLM.

Công trình này nhằm mở rộng khả năng của các LLM để bao gồm một phạm vi rộng hơn các ngôn ngữ. Các công trình hiện có như ImaniGooghari et al. (2023) về thích ứng ngôn ngữ của các mô hình được huấn luyện trước cung cấp phạm vi bao phủ mở rộng qua một phổ ngôn ngữ học rộng lớn nhưng bị hạn chế đến kích thước mô hình tương đối nhỏ - chủ yếu ở quy mô hàng trăm triệu, trong khi các công trình khác như Yong et al. (2022) mở rộng các LLM tạo sinh nhưng bị hạn chế đến một số lượng nhỏ ngôn ngữ. Nghiên cứu của chúng tôi đẩy ranh giới bằng cách khám phá các kỹ thuật thích ứng ngôn ngữ cho các LLM với tham số mô hình tăng lên đến 10 tỷ cho 534 ngôn ngữ. Điều tra của chúng tôi đi sâu vào các LLM tạo sinh với sự gia tăng đáng kể về tham số mô hình và khả năng học trong ngữ cảnh của chúng trong các ngôn ngữ đa dạng, đặc biệt là các ngôn ngữ có ít tài nguyên. Sự tăng cường này cho phép chúng tôi nâng cao mức độ liên quan về ngữ cảnh và ngôn ngữ học qua một phạm vi đa dạng các ngôn ngữ.

Chúng tôi giải quyết các thách thức của việc thích ứng các LLM với các ngôn ngữ có ít tài nguyên, như sự thưa thớt dữ liệu, từ vựng đặc thù miền, và sự đa dạng ngôn ngữ học. Cụ thể, chúng tôi nghiên cứu tiếp tục huấn luyện trước của LLM mở, tức là LLaMA 2 (Touvron et al., 2023b), mở rộng từ vựng, và các kỹ thuật thích ứng, tức là tái tham số hóa thứ hạng thấp LoRA (Hu et al., 2022). Chúng tôi triển khai huấn luyện phân tán và phát hành MaLA-500 bao phủ hơn 500 ngôn ngữ trong các miền khác nhau. Chúng tôi đánh giá MaLA-500 sử dụng các biện pháp nội tại trên tập kiểm tra Glot500-c được giữ lại và dữ liệu song song và các thước đo ngoại tại trên các điểm chuẩn hạ lưu: SIB200 và Taxi1500. Kết quả cho thấy MaLA-500 vượt trội hơn các LLM mở hiện có với kích thước mô hình gần bằng hoặc lớn hơn một chút. Công trình này mở rộng khả năng tiếp cận của các LLM, làm cho chúng có giá trị cho một tập hợp đa dạng hơn các trường hợp sử dụng đặc thù ngôn ngữ, đặc biệt là cho những ngôn ngữ có ít tài nguyên, và giải quyết vấn đề bình đẳng bằng cách loại bỏ rào cản ngôn ngữ cho người nói nhiều ngôn ngữ, đặc biệt là những ngôn ngữ thiếu đại diện được bao phủ bởi các LLM hiện có.

2 Thích Ứng Ngôn Ngữ Quy Mô Lớn
Nguyên tắc thích ứng ngôn ngữ quy mô lớn của các mô hình ngôn ngữ lớn chứa việc sử dụng một bộ dữ liệu đa ngôn ngữ quy mô lớn (Phần 2.1), LLM cơ sở mạnh mẽ (Phần 2.2), và kỹ thuật thích ứng ngôn ngữ hiệu quả: mở rộng từ vựng (Phần 2.3) và tiếp tục huấn luyện trước (Phần 2.4).

2.1 Dữ liệu
Chúng tôi sử dụng Glot500-c (ImaniGooghari et al., 2023) bao phủ 534 ngôn ngữ2 làm dữ liệu huấn luyện của MaLA-500. Xem §A cho danh sách các ngôn ngữ với lượng dữ liệu của chúng. Số lượng câu gốc dao động từ 10 nghìn đến 63 triệu. Lưu ý rằng Glot500-c không nỗ lực hết mình để thu thập dữ liệu cho các ngôn ngữ có nhiều tài nguyên mà tập trung vào các ngôn ngữ có ít tài nguyên. Chúng tôi lấy mẫu các ngôn ngữ từ bộ dữ liệu mất cân bằng theo phân phối đa thức, với α=0,3 cho mở rộng từ vựng và tiếp tục huấn luyện trước. Chúng tôi sử dụng các quy mô khác nhau để lấy mẫu dữ liệu được sử dụng trong huấn luyện mô hình và xây dựng từ vựng. Sau khi lấy mẫu, số lượng câu để huấn luyện dao động từ 600 nghìn đến 8 triệu mỗi ngôn ngữ, dẫn đến tổng cộng 1 tỷ câu. Số lượng câu để xây dựng từ vựng dao động từ 30 nghìn đến 400 nghìn, tạo tổng cộng 50 triệu câu.

2.2 Mô hình
Chúng tôi chọn LLaMA 2 (Touvron et al., 2023b) để bắt đầu huấn luyện liên tục. Các mô hình dòng LLaMA (Touvron et al., 2023a), với trọng số mô hình được phát hành công khai, đã trở nên phổ biến trong cộng đồng nghiên cứu. Mặc dù tập trung vào tiếng Anh so với các đối tác đa ngôn ngữ của chúng, chúng đã cho thấy khả năng đáng chú ý cho nhiều ngôn ngữ (Ahuja et al., 2023b). Chúng tôi chọn LLaMA 2 mới nhất, được huấn luyện trên 2 nghìn tỷ token, làm mô hình cơ sở của chúng tôi để hưởng lợi từ khả năng ngôn ngữ xuất sắc của nó. Nghiên cứu của chúng tôi chọn mô hình 7B với 32 lớp transformer, và để lại việc mở rộng các LLM với kích thước lớn hơn như công việc tương lai.

2.3 Mở rộng Từ vựng
Tokenizer 32.000 gốc của LLaMA 2 bao phủ tiếng Anh và một phần nhỏ các ngôn ngữ châu Âu khác sử dụng chữ viết Latin hoặc Cyrillic. Để tăng cường khả năng và hiệu quả mã hóa của nó cho một phạm vi rộng hơn các ngôn ngữ, chúng tôi mở rộng từ vựng với Glot500-c. Cụ thể, chúng tôi ban đầu huấn luyện một tokenizer đa ngôn ngữ với SentencePiece (Kudo & Richardson, 2018) trên Glot500-c được lấy mẫu với từ vựng 250.000. Sau đó, chúng tôi hợp nhất tokenizer đã huấn luyện với tokenizer LLaMA 2 gốc bằng cách lấy hợp của từ vựng của chúng. Kết quả là, chúng tôi có được tokenizer của MaLA-500 với kích thước từ vựng 260.164. Sau khi mở rộng từ vựng và thay đổi kích thước lớp nhúng, kích thước mô hình trở thành 8,6B.

Chúng tôi đo lường tác động của mở rộng từ vựng trên tập phát triển của Glot500-c bằng cách phân tích việc giảm độ dài phân đoạn cho mỗi ngôn ngữ. Kết quả cho thấy hiệu ứng của mở rộng từ vựng thay đổi, dao động từ 8% (tiếng Anh, engLatn) đến 88% (Oriya, oriOrya). Không có gì đáng ngạc nhiên, mở rộng từ vựng có tác động lớn hơn đối với các ngôn ngữ được viết bằng chữ viết phi Latin so với những ngôn ngữ sử dụng chữ viết Latin. Tuy nhiên, đối với một số ngôn ngữ có ít tài nguyên được viết bằng chữ viết Latin, ví dụ như Kabiyè (kbpLatn) và tiếng Việt (vieLatn), độ dài phân đoạn được rút ngắn khoảng 50%.

2.4 Tiếp tục Huấn luyện Trước
Chúng tôi sử dụng tiếp tục huấn luyện trước cho thích ứng ngôn ngữ với thích ứng thứ hạng thấp (LoRA, Hu et al., 2022) để cho phép huấn luyện tiết kiệm tham số, với hạn chế về tài nguyên tính toán của chúng tôi. LoRA tiêm các ma trận phân tách thứ hạng có thể huấn luyện, xấp xỉ các ma trận trọng số lớn với thứ hạng thấp hơn, vào trọng số mô hình được huấn luyện trước. Nó giảm độ phức tạp tính toán và do đó tiết kiệm chi phí huấn luyện trong khi duy trì chất lượng mô hình cao (Hu et al., 2022). Chúng tôi liên tục huấn luyện mô hình ngôn ngữ nhân quả để cập nhật các ma trận phân tách thứ hạng, lớp nhúng, và đầu mô hình hóa ngôn ngữ trong khi đóng băng trọng số transformer của các mô hình được huấn luyện trước, cho phép mô hình ngôn ngữ được huấn luyện liên tục học từ dữ liệu mới trong các ngôn ngữ mới mà không hoàn toàn mất khả năng ngôn ngữ trước đó của nó. Huấn luyện liên tục các mô hình ngôn ngữ lớn yêu cầu tài nguyên tính toán đáng kể. Chúng tôi áp dụng thiết lập huấn luyện phân tán hiệu quả trên siêu máy tính để làm cho quá trình huấn luyện khả thi.

2.5 Huấn luyện
Phần cứng và Phần mềm Chúng tôi huấn luyện mô hình của chúng tôi trên cụm tính toán với hiệu suất đỉnh lý thuyết 2 petaflops trên các nút GPU. Chúng tôi triển khai huấn luyện phân tán trên 24 GPU Nvidia Ampere A100. Về phần mềm, chúng tôi sử dụng Huggingface Transformers (Wolf et al., 2020), PEFT (Parameter-Efficient Fine-Tuning),3 và DeepSpeed (Rasley et al., 2020). Chúng tôi sử dụng trình tối ưu hóa dư thừa ZeRO (Rajbhandari et al., 2020) và tối đa hóa kích thước batch phù hợp với bộ nhớ của mỗi GPU. Chúng tôi sử dụng huấn luyện độ chính xác hỗn hợp sử dụng định dạng bfloat16.

Siêu tham số Tốc độ học được đặt ở 3e-4. Suy giảm trọng số 0,01 được áp dụng để phạt các trọng số lớn và giảm thiểu quá khớp. Mô-đun LoRA có thể huấn luyện nhắm vào các ma trận truy vấn và giá trị. Đầu mô hình ngôn ngữ không được phân tách bởi mô-đun LoRA mà được huấn luyện theo cách tham số đầy đủ. Trong thiết lập của chúng tôi, mô hình cuối cùng có tổng cộng 10B tham số, trong đó 2B tham số có thể huấn luyện. Mô-đun LoRA được kết hợp với thứ hạng 8, giá trị alpha 32, và tỷ lệ dropout 0,05, góp phần vào khả năng thích ứng và điều chuẩn của mô hình trong quá trình huấn luyện. Cửa sổ ngữ cảnh là 4k. Chúng tôi tối đa hóa kích thước batch để phù hợp với bộ nhớ, tạo kích thước batch toàn cục 384. Mô hình trải qua ba epoch huấn luyện. Các checkpoint được lưu mỗi 500 bước, và chúng tôi sử dụng dừng sớm để chọn checkpoint thể hiện hiệu suất trung bình tốt nhất trên các tác vụ hạ lưu.

Tác động Môi trường Chúng tôi huấn luyện mô hình của chúng tôi trên một trung tâm dữ liệu trung tính carbon, với toàn bộ điện được tạo ra bằng thủy điện tái tạo, và nhiệt thải được sử dụng trong hệ thống sưởi khu vực để giảm thêm dấu chân CO2.4

3 Đánh giá
3.1 Điểm chuẩn và Thiết lập
Chúng tôi xem xét cả các biện pháp nội tại và ngoại tại để đánh giá. Thống kê bộ dữ liệu đánh giá được hiển thị trong Bảng 1.

Bộ dữ liệu Thước đo ∥Dữ liệu∥ ∥Ngôn ngữ∥ Miền
Nội tại Glot500-c test (ImaniGooghari et al., 2023) NLL 1000 534 Khác
PBC (Mayer & Cysouw, 2014) NLL 500 370 Kinh Thánh
Ngoại tại SIB200 (Adelani et al., 2023) ACC 204 177 Khác
Taxi1500 (Ma et al., 2023) ACC 111 351 Kinh Thánh
Bảng 1: Thống kê bộ dữ liệu đánh giá. ∥Dữ liệu∥: kích thước tập kiểm tra mỗi ngôn ngữ. ∥Ngôn ngữ∥: số lượng ngôn ngữ được đánh giá. NLL: log-likelihood âm. ACC: Độ chính xác.

Đối với đánh giá nội tại, perplexity không thể so sánh được qua các mô hình và ngôn ngữ do các phân đoạn văn bản khác nhau. Được truyền cảm hứng bởi Xue et al. (2022); Yu et al. (2023), thay vào đó chúng tôi đo lường log-likelihood âm (NLL) của văn bản sử dụng các LLM đã cho. Chúng tôi nối bộ dữ liệu làm văn bản đầu vào và áp dụng chiến lược cửa sổ trượt.5 Việc đánh giá các LLM khác nhau sử dụng cùng dữ liệu với việc nối các câu mỗi ngôn ngữ, do đó làm cho NLL có thể so sánh được giữa các mô hình. Ngoài ra, chúng tôi xem xét NLL có thể so sánh ngôn ngữ bằng cách đo lường NLL trên dữ liệu song song, trong đó mỗi mẫu trong các ngôn ngữ khác nhau chứa cùng thông tin ngữ nghĩa. Chúng tôi báo cáo NLL có thể so sánh mô hình của tập kiểm tra Glot500-c bao phủ tất cả 534 ngôn ngữ được xem xét (§3.2), và NLL có thể so sánh ngôn ngữ trên Parallel Bible Corpus (PBC, Mayer & Cysouw, 2014), bao phủ 370 ngôn ngữ (§3.3).

Đối với đánh giá ngoại tại, chúng tôi đánh giá khả năng học ít shot của MaLA-500 và so sánh nó với các LLM khác trên SIB200 (Adelani et al., 2023) và Taxi1500 (Ma et al., 2023). SIB200 là một bộ dữ liệu phân loại chủ đề. Tác vụ phân loại bao gồm bảy lớp, cụ thể là khoa học/công nghệ, du lịch, chính trị, thể thao, sức khỏe, giải trí, và địa lý. Đánh giá của chúng tôi trải rộng một tập hợp đa dạng gồm 177 ngôn ngữ, thu được bằng cách giao của các tập hợp ngôn ngữ của SIB200 và Glot500-c. Lưu ý rằng tập đánh giá SIB200 dựa trên flores200 được bao gồm trong dữ liệu huấn luyện vì Glot500-c bao gồm flores200, nhưng nhãn phân loại không được cung cấp.

Taxi1500 là một bộ dữ liệu phân loại văn bản khác trải rộng 351 ngôn ngữ. Nó bao gồm sáu lớp, cụ thể là Khuyến nghị, Đức tin, Mô tả, Tội lỗi, Ân điển, và Bạo lực. Nỗ lực đánh giá của chúng tôi nhằm bao phủ càng nhiều ngôn ngữ càng tốt. Tuy nhiên, việc đánh giá các mô hình ngôn ngữ đa ngôn ngữ quy mô lớn là một tác vụ thách thức. Do thiếu các điểm chuẩn đánh giá đa ngôn ngữ thực tế, chúng tôi sử dụng điểm chuẩn này chứa nội dung tôn giáo.

Đối với đánh giá học trong ngữ cảnh, LLM được đánh giá nhận một prompt có cấu trúc, là sự nối của các ví dụ ít shot và mẫu dự định để dự đoán. Định dạng cho cả ví dụ ít shot và mẫu để dự đoán được định nghĩa như sau:

Mẫu cho SIB200:

4https://www.csc.fi/sustainable-development
5https://huggingface.co/docs/transformers/en/perplexity

--- TRANG 5 ---
Bản thảo. Đang được đánh giá.
Chủ đề của tin tức [sent] là [label]
Mẫu cho Taxi1500:
Chủ đề của câu [sent] là [label]
trong đó [sent] là câu để phân loại, và [label] là sự thật cơ bản. [label] được bao gồm khi mẫu đóng vai trò như một ví dụ ít shot nhưng được bỏ qua khi dự đoán mẫu. Prompt được xây dựng sau đó được sử dụng làm đầu vào cho LLM. Tiếp theo, LLM được đánh giá được nhắc để ước tính xác suất của nhãn trên tập nhãn dựa trên prompt được cung cấp.

Đối với SIB200, các ví dụ ít shot được lấy mẫu ngẫu nhiên từ các tập huấn luyện trong ngôn ngữ. Vì việc chọn ngẫu nhiên các ví dụ ít shot cho học trong ngữ cảnh mang lại kết quả ngẫu nhiên cho cả MaLA-500 và các LLM trước đó trên Taxi1500, chúng tôi xem xét học trong ngữ cảnh dựa trên retriever (Liu et al., 2022). Cụ thể, chúng tôi sử dụng nhúng từ trung bình trong lớp 8 của Glot500 (ImaniGooghari et al., 2023) để truy xuất các mẫu tương tự ngữ nghĩa như được đề xuất trong công trình trước đây (Sabet et al., 2020) cho tất cả các mô hình được so sánh. Quá trình đánh giá được thực hiện sử dụng lm-evaluation-harness,6 và chúng tôi sử dụng độ chính xác (ACC) để đo lường hiệu suất phân loại.

3.2 So sánh qua các LLM
Chúng tôi so sánh MaLA-500 với LLaMA 2-7B, mGPT-13B, BLOOM-7B1, và XGLM-7.5B trên tập kiểm tra Glot500-c, SIB200, Taxi1500 bằng cách tính toán hiệu suất trung bình qua các ngôn ngữ, và kết quả được đưa ra trong Bảng 2. Trong số các LLM được đánh giá, LLaMA 2-7B hoạt động tốt thứ hai, cho thấy rằng LLaMA 2-7B có khả năng đa ngôn ngữ mạnh mẽ và việc chọn nó làm mô hình cơ sở là hợp lý. MaLA-500 vượt trội hơn tất cả các LLM được so sánh với kích thước mô hình gần bằng hoặc lớn hơn một chút qua tất cả các tác vụ được đánh giá. Đáng chú ý, so với LLaMA 2-7B, MaLA-500 đạt được NLL thấp hơn trên tập kiểm tra Glot500-c là 39,33, và có cải thiện 14,94% và 4,82% trên SIB200 và Taxi1500, tương ứng. Điều này làm nổi bật đóng góp đáng kể của MaLA-500 trong việc tăng cường khả năng đa ngôn ngữ của các LLM.

Mô hình Glot500-c test (NLL↓) SIB200 (ACC ↑) Taxi1500 (ACC ↑)
LLaMA 2-7B 190,58 42,08 44,07
mGPT-13B 282,46 45,34 40,98
BLOOM-7B1 202,95 44,63 43,98
XGLM-7.5B 205,07 34,36 43,24
MaLA-500 151,25 57,02 48,89
Bảng 2: Kết quả trung bình qua các ngôn ngữ trên tập kiểm tra Glot500-c (đo bằng NLL), SIB200, và Taxi1500 (đo bằng độ chính xác (%)) của các LLM khác nhau. mGPT không có mô hình với khoảng 7B tham số, vì vậy chúng tôi chọn một mô hình lớn hơn với 13B tham số. ↓ biểu thị càng thấp càng tốt. ↑ biểu thị càng cao càng tốt. Kết quả tốt nhất được in đậm.

Hình 1 đến 3 cung cấp phân tích hiệu suất chi tiết qua các ngôn ngữ trên tập kiểm tra Glot500-c, SIB200, và Taxi1500. Trong những hình đó, chúng tôi nhóm điểm số vào các khoảng hiệu suất khác nhau và hiển thị chúng bằng các màu khác nhau. Đối với tập kiểm tra Glot500-c, MaLA-500 có nhiều ngôn ngữ đạt được NLL tốt hơn, tức là 61 ngôn ngữ với NLL nhỏ hơn 100 và 171 ngôn ngữ với NLL giữa 100 và 150. Bên cạnh đó, MaLA-500 có 54 (10%) ngôn ngữ đạt được NLL lớn hơn 200, có thể cho thấy các ngôn ngữ không được bao phủ tốt bởi LLM được đo. Tuy nhiên, số lượng này ít hơn nhiều so với các LLM khác. Ví dụ, LLM tốt thứ hai, LLaMA 2-7B, có 231 (43%) ngôn ngữ đạt được NLL lớn hơn 200. Đối với cả SIB200 và Taxi1500, MaLA-500 vượt trội hơn các LLM trước đó theo nghĩa nó có được kết quả ngẫu nhiên trong ít ngôn ngữ hơn và đạt được hiệu suất ấn tượng trong nhiều ngôn ngữ hơn so với các đối tác của nó.

6https://github.com/EleutherAI/lm-evaluation-harness

--- TRANG 6 ---
Bản thảo. Đang được đánh giá.
LLaMA 2-7B
mGPT-13B
BLOOM-7B1
XGLM-7.5B
MaLA-500
0 200 400 600 200- 150-200 100-150 0-100
Hình 1: NLL (thấp hơn là tốt hơn) trên tập kiểm tra Glot500-c với điểm số được nhóm thành bốn khoảng hiển thị bằng các màu khác nhau. Trục X: số lượng ngôn ngữ trong các phạm vi hiệu suất.

LLaMA 2-7B
mGPT-13B
BLOOM-7B1
XGLM-7.5B
MaLA-500
0 50 100 150 200 0-20 20-40 40-60 60-80
Hình 2: Độ chính xác (cao hơn là tốt hơn) trên SIB200 với điểm số được nhóm thành bốn khoảng hiển thị bằng các màu khác nhau. Trục X: số lượng ngôn ngữ trong các phạm vi hiệu suất (%).

3.3 So sánh qua các Ngôn ngữ
Để kiểm tra chi tiết cách MaLA-500 hoạt động qua các ngôn ngữ, chúng tôi kiểm tra hiệu suất qua các họ ngôn ngữ7 được hiển thị trong Bảng 3. Chúng tôi quan sát thấy rằng các họ ngôn ngữ có nhiều tài nguyên hơn, ví dụ như Indo-European (indo1319) và Dravidian (drav1251), đạt được hiệu suất tốt hơn một chút so với các họ ngôn ngữ có ít tài nguyên, ví dụ như Sino-Tibetan (sino1245).

Trong Bảng 4, chúng tôi trình bày một phân tích toàn diện về 5 cải thiện và suy giảm hiệu suất hàng đầu qua các ngôn ngữ trên SIB200 từ MaLA-500 so với LLaMA 2-7B. Chúng tôi quan sát thấy rằng MaLA-500 có cải thiện đáng kể trên các chữ viết có ít tài nguyên, ví dụ như Kannada (kanKnda), trong khi có hiệu suất tệ hơn trên các ngôn ngữ có nhiều tài nguyên, ví dụ như tiếng Thụy Điển (sweLatn), đã được bao phủ tốt bởi LLaMA 2-7B.

Trong phân tích toàn diện của chúng tôi về các yếu tố đóng góp trên SIB200, chúng tôi lưu ý rằng kích thước bộ dữ liệu của một ngôn ngữ thể hiện mối tương quan yếu 0,13 với mức tăng hiệu suất của nó. Ngược lại, kích thước bộ dữ liệu của họ ngôn ngữ mà một ngôn ngữ thuộc về thể hiện mối tương quan vừa phải 0,40. Một mối tương quan Pearson vừa phải cao 0,53 được quan sát giữa hiệu ứng của mở rộng từ vựng, tức là việc giảm độ dài phân đoạn, và mức tăng hiệu suất. Quan sát này đúng cho các ngôn ngữ với cả chữ viết phi Latin, như Kannada (kanKnda), Malayalam (malMlym), và Tigrinya (tirEthi), cũng như chữ viết Latin, như Igbo (iboLatn) và Yoruba (yorLatn). Điều này chứng minh hiệu quả của mở rộng từ vựng.

7Chúng tôi gán các ngôn ngữ cho các họ dựa trên Glottolog: https://glottolog.org/glottolog/family.

--- TRANG 7 ---
Bản thảo. Đang được đánh giá.
LLaMA 2-7B
mGPT-13B
BLOOM-7B1
XGLM-7.5B
MaLA-500
0 100 200 300 400 0-40 40-55 50-80
Hình 3: Độ chính xác (cao hơn là tốt hơn) trên Taxi1500 với điểm số được nhóm thành bốn khoảng hiển thị bằng các màu khác nhau. Trục X: số lượng ngôn ngữ trong các phạm vi hiệu suất (%).

họ ∥Câu∥ PBC (NLL↓) SIB200 (ACC ↑) Taxi1500 (ACC ↑)
indo1319 988M 145,35 63,53 53,03
drav1251 135M 131,29 56,25 54,65
aust1307 113M 147,37 62,83 49,69
turk1311 109M 161,71 57,08 52,55
afro1255 100M 165,46 52,00 43,74
atla1278 57M 141,92 42,90 45,52
ural1272 50M 137,52 66,67 48,58
sino1245 29M 155,64 49,30 49,31
khác 60M 167,69 55,74 46,67
Bảng 3: So sánh hiệu suất qua các họ ngôn ngữ trên PBC, SIB200, và Taxi1500. ∥Câu∥: số lượng câu được sử dụng để tiếp tục huấn luyện trước tổng cộng. ↓ biểu thị càng thấp càng tốt. ↑ biểu thị càng cao càng tốt.

3.4 Hiệu ứng của Số lượng Shot
Hình 4 minh họa mối quan hệ giữa độ chính xác và số lượng ví dụ trong ngữ cảnh (tức là shots) trên SIB200. Khi số lượng shots trong ngữ cảnh tăng lên, có sự gia tăng tương ứng về độ chính xác. Đáng chú ý, với chỉ 1-shot, độ chính xác thể hiện tính ngẫu nhiên ở 30,88%, cho thấy 1-shot cung cấp thông tin hạn chế cho việc học tác vụ. Sự chuyển đổi từ 1 shot sang 2 shots/3 shots dẫn đến cải thiện đáng chú ý, với hiệu suất được tăng cường 19,83% và 26,14%, tương ứng. Điều này làm nổi bật hiệu quả của việc tăng số lượng shots. MaLA-500 đạt được hiệu suất đỉnh ở khoảng 65% độ chính xác với 6-10 shots trong ngữ cảnh. Điều này có thể được quy cho tính chất đa lớp của bộ dữ liệu SIB200, đòi hỏi nhiều shots hơn để học các ánh xạ đầu vào-nhãn phức tạp.

Trong Hình 5, một mô tả tinh tế hơn của kết quả phù hợp với các quan sát được thực hiện trong Hình 4. Trong lĩnh vực học trong ngữ cảnh 1-shot, khoảng 50 ngôn ngữ thể hiện kết quả bất thường. Khi số lượng shots tăng lên, có sự giảm số lượng ngôn ngữ đạt được độ chính xác thấp (25-50%), kết hợp với một nhóm ngày càng tăng đạt được độ chính xác cao (75-100%).

Kiểm tra sâu hơn vào các xu hướng ngôn ngữ cá nhân cho thấy một số ngôn ngữ có ít tài nguyên yêu cầu nhiều shots hơn để đạt được hiệu suất tốt hơn (ví dụ, pesArab cho tiếng Ba Tư) hoặc thậm chí thể hiện hiệu suất kém với 10 shots (ví dụ, dzoTibt cho Dzongkha và ayrLatn cho Central Aymara). Ngược lại, các ngôn ngữ có nhiều tài nguyên, như fraLatn cho tiếng Pháp, thể hiện hiệu suất ấn tượng ngay cả với ít shots hơn, và việc tăng số lượng shots chỉ dẫn đến cải thiện nhỏ.

--- TRANG 8 ---
Bản thảo. Đang được đánh giá.
mức cao mức thấp
Ngôn ngữ LLaMA 2-7B MaLA-500 ∆ Ngôn ngữ LLaMA 2-7B MaLA-500 ∆
kan Knda 17,16 57,35 40,19 swe Latn 71,08 60,29 -10,79
ckb Arab 19,61 60,29 40,68 rusCyrl 71,57 65,20 -06,37
asm Beng 17,16 58,82 41,66 dan Latn 69,12 63,24 -05,88
pan Guru 14,22 58,82 44,60 polLatn 74,51 68,63 -05,88
sinSinh 15,20 60,29 45,09 ukr Cyrl 71,57 65,69 -05,88
Bảng 4: Kết quả cho năm ngôn ngữ mỗi loại với mức tăng lớn nhất (mức cao) và nhỏ nhất (mức thấp) từ MaLA-500 so với LLaMA 2-7B cho SIB200. ∆ biểu thị sự khác biệt giữa điểm số của MaLA-500 và LLaMA 2-7B. Xem §B cho kết quả chi tiết cho mỗi tác vụ.

Số lượng shots Độ chính xác
0 0.00 20.00 40.00 60.00 80.00
1 2 3 4 5 6 7 8 9 10
Hình 4: Độ chính xác trung bình marco học trong ngữ cảnh (%) trên SIB200 với số lượng shots khác nhau sử dụng MaLA-500.

4 Công trình Liên quan
4.1 Mô hình Ngôn ngữ Đa ngôn ngữ
Phát triển mô hình ngôn ngữ đã nỗ lực mở rộng phạm vi các ngôn ngữ huấn luyện trước để giải quyết các tình huống đa ngôn ngữ. Các mô hình đa ngôn ngữ được huấn luyện trước đã có thể chứa lên đến hàng trăm hoặc nhiều ngôn ngữ hơn. Các ví dụ đáng chú ý bao gồm mBERT Devlin et al. (2019), hỗ trợ 104 ngôn ngữ, XLM-R (Conneau et al., 2020) bao phủ 100 ngôn ngữ, mBART (Liu et al., 2020) được thiết kế cho 25 ngôn ngữ, mT5 (Xue et al., 2021) trải rộng 101 ngôn ngữ, XGLM (Lin et al., 2021) qua 30 ngôn ngữ, GPT-3 bao phủ 118 ngôn ngữ (93% tiếng Anh), mGPT (Shliazhko et al., 2022) chứa 60 ngôn ngữ, và BLOOM (Scao et al., 2022) hỗ trợ 46 ngôn ngữ và 13 ngôn ngữ lập trình.

Đáng ngạc nhiên, hai mô hình ngôn ngữ đa ngôn ngữ gần đây đã vượt qua giới hạn thông thường bằng cách hỗ trợ hơn 400 ngôn ngữ. Glot500-m (ImaniGooghari et al., 2023) trải rộng 511 ngôn ngữ thông qua mở rộng từ vựng và huấn luyện tiếp tục dựa trên XLM-R. SERENGETI (Adebara et al., 2022) đi xa hơn nữa bằng cách hỗ trợ 517 ngôn ngữ và biến thể ngôn ngữ châu Phi, được viết bằng năm chữ viết khác nhau, sử dụng các mô hình được truyền cảm hứng từ cả ELECTRA (Clark et al., 2020) và XLM-R. MADLAD (Kudugunta et al., 2023) bao phủ 419 ngôn ngữ và huấn luyện một mô hình ngôn ngữ 8B từ đầu với mục tiêu UL2 thích ứng (Tay et al., 2022). Công trình của chúng tôi đồng thời với mô hình ngôn ngữ MADLAD-400. Chúng tôi phân biệt nó bằng: 1) bao phủ ngôn ngữ. Công trình của chúng tôi bao phủ hơn 500 ngôn ngữ, một số lượng có thể so sánh với các mô hình chỉ mã hóa và vượt trội hơn MADLAD-400 thêm 100 ngôn ngữ. 2) phương pháp huấn luyện. Chúng tôi xem xét huấn luyện liên tục để hưởng lợi từ kiến thức đã học của các mô hình gốc. 3) kiến trúc mô hình. Chúng tôi áp dụng kiến trúc mô hình mở, tức là LLaMA, trong khi MADLAD sử dụng kiến trúc T5 chỉ giải mã, chưa được hỗ trợ bởi hệ sinh thái HuggingFace tại thời điểm viết, do đó dẫn đến khó khăn bổ sung trong sử dụng.

--- TRANG 9 ---
Bản thảo. Đang được đánh giá.
1
2
3
4
5
6
7
8
9
10
0 50 100 150 200 0-25 25-50 50-75 75-100
Hình 5: Kết quả chi tiết của học trong ngữ cảnh trên SIB200 sử dụng MaLA-500. Trục X: số lượng ngôn ngữ trong các phạm vi độ chính xác khác nhau (%). Trục Y: số lượng shots.

4.2 Thích Ứng Ngôn ngữ
Trước sự xuất hiện của các LLM, các phương pháp đa dạng được sử dụng để thích ứng các mô hình ngôn ngữ đa ngôn ngữ quy mô nhỏ với các ngôn ngữ mới. Các phương pháp này bao gồm sử dụng adapters (Pfeiffer et al., 2020; Üstün et al., 2020; Pfeiffer et al., 2020; Nguyen et al., 2021; Faisal & Anastasopoulos, 2022; Yong et al., 2022), mở rộng và thay thế từ vựng (Chau et al., 2020; Wang et al., 2020; Müller et al., 2020; 2021; Pfeiffer et al., 2021; Chen et al., 2023; Downey et al., 2023), tận dụng bộ dữ liệu đơn ngôn ngữ (Ebrahimi & Kann, 2021; Alabi et al., 2022), và sử dụng từ điển song ngữ (Wang et al., 2022).

Trong khi các mô hình ngôn ngữ đã được mở rộng quy mô đáng chú ý, phạm vi bao phủ của chúng bị hạn chế đến một tập hợp cụ thể các ngôn ngữ. Để giải quyết hạn chế này, các phương pháp khác nhau đã được đề xuất để mở rộng khả năng áp dụng của những mô hình ngôn ngữ lớn này qua một phạm vi rộng hơn các ngôn ngữ, phục vụ cho cả các tác vụ mục đích chung và các ứng dụng cụ thể như dịch máy. Các phương pháp này cũng bao gồm mở rộng từ vựng (Cui et al., 2023), tiếp tục huấn luyện trước và instruction-tuning (Yong et al., 2022; Cui et al., 2023; Chen et al., 2024; Zhao et al., 2024), và khai thác bộ dữ liệu song song (Cahyawijaya et al., 2023; Yang et al., 2023; Zhu et al., 2023; Xu et al., 2023). Mặc dù có những nỗ lực này, thích ứng ngôn ngữ quy mô lớn của các LLM cho các tác vụ mục đích chung qua các ngôn ngữ đa dạng, ví dụ như bao phủ nhiều họ ngôn ngữ và hơn một trăm ngôn ngữ, vẫn là một lĩnh vực chưa được khám phá kỹ lưỡng.

5 Kết luận và Công việc Tương lai
Chúng tôi trình bày một nỗ lực tiên phong trong thích ứng ngôn ngữ quy mô lớn trên các LLM, tập trung vào việc mở rộng LLaMA 7B thành mô hình của chúng tôi, MaLA-500. Sự thích ứng này bao gồm mở rộng từ vựng và tiếp tục huấn luyện trước với LoRA. Phương pháp của chúng tôi dẫn đến MaLA-500 đạt được khả năng học trong ngữ cảnh tiên tiến, như được chứng minh trên các điểm chuẩn SIB200 và Taxi1500. Chúng tôi phát hành các script huấn luyện và trọng số mô hình công khai để tạo điều kiện cho nghiên cứu tương lai. Công trình này đánh dấu một tiến bộ đáng kể trong việc áp dụng các LLM cho một phạm vi đa dạng các ngôn ngữ.

Công việc tương lai của chúng tôi sẽ tập trung vào việc cải thiện thêm khả năng mô hình, ví dụ như về dịch máy qua nhiều cặp ngôn ngữ. Alves et al. (2023) cho thấy rằng các LLM (LLaMA-7B và LLaMA-13B) thể hiện hiệu suất kém ngay cả trên các cặp ngôn ngữ có nhiều tài nguyên tập trung vào tiếng Anh trong một số trường hợp. Dịch thuật với các LLM trên các ngôn ngữ có ít tài nguyên thách thức hơn. Mô hình LLaMA-7B hoạt động kém trong các thí nghiệm sơ bộ của chúng tôi. Bên cạnh đó, bộ dữ liệu huấn luyện trước của chúng tôi không cố ý bao gồm các văn bản song ngữ, và mô hình MaLA-500 của chúng tôi không được instruction-tuned với dữ liệu dịch thuật. Chúng tôi để lại việc bao gồm văn bản song ngữ trong quá trình tiếp tục huấn luyện trước, instruction fine-tuning với dữ liệu dịch thuật, và đánh giá về dịch máy như các công việc tương lai.

--- TRANG 10 ---
Bản thảo. Đang được đánh giá.
Tuyên bố Đạo đức
Các LLM đã được biết đến là thể hiện các thiên kiến có mặt trong dữ liệu huấn luyện của chúng. Khi mở rộng các LLM đến các ngôn ngữ có ít tài nguyên, có nguy cơ truyền bá thiên kiến từ các ngôn ngữ có nhiều tài nguyên đến những ngôn ngữ thiếu đại diện. Sự chú ý cẩn thận phải được dành để giảm thiểu thiên kiến và đảm bảo công bằng trong thu thập dữ liệu và huấn luyện mô hình. Bài báo nhằm làm cho các LLM dễ tiếp cận hơn cho các ngôn ngữ thiếu đại diện. Tuy nhiên, vẫn có nguy cơ tạo ra sự phân chia ngôn ngữ số nếu một số cộng đồng bị bỏ lại phía sau do khả năng tiếp cận công nghệ hạn chế. Công việc tương lai sẽ giải quyết các thiên kiến bằng cách tiến hành kiểm tra thiên kiến trên dữ liệu huấn luyện, loại bỏ thiên kiến trong các mô hình trong quá trình tạo sinh, và liên tục giám sát đầu ra mô hình.

Tuyên bố Tái tạo
Chúng tôi thực hiện các nỗ lực sau để đảm bảo nghiên cứu có thể tái tạo. Chúng tôi phát hành trọng số mô hình (https://huggingface.co/MaLA-LM) và mã code cho huấn luyện và đánh giá (https://github.com/MaLA-LM/mala-500). Chúng tôi sử dụng các điểm chuẩn đánh giá có sẵn công khai có thể được lấy miễn phí hoặc theo yêu cầu. Kết quả có thể tái tạo với trọng số mô hình và script đánh giá đã phát hành của chúng tôi,

Lời cảm ơn
Chúng tôi cảm ơn José Pombal vì các gợi ý xây dựng về huấn luyện. Công trình này được tài trợ bởi Hội đồng Nghiên cứu Châu Âu (các khoản tài trợ #740516, #771113 và #758969), Các Hành động Nghiên cứu và Đổi mới Horizon Europe của EU (UTTER, hợp đồng 101070631), và chương trình nghiên cứu và đổi mới Horizon Europe của Liên minh Châu Âu theo thỏa thuận tài trợ số 101070350 và từ UK Research and Innovation (UKRI) theo bảo đảm tài trợ Horizon Europe của chính phủ Anh [khoản tài trợ #10052546]. Các tác giả muốn ghi nhận CSC – IT Center for Science, Finland, về các tài nguyên tính toán hào phóng trên siêu máy tính Mahti và siêu máy tính LUMI thông qua quyền truy cập quy mô cực lớn LUMI (MOOMIN và LumiNMT). Shaoxiong Ji và Peiqin Lin ghi nhận hỗ trợ đi lại từ ELISE (GA no 951847).

Tài liệu tham khảo
[Danh sách tài liệu tham khảo tiếp tục...]
