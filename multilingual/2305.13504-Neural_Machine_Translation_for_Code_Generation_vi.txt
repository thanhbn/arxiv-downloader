# Dịch máy thần kinh cho việc tạo mã

Dharma KC
Đại học Arizona
kcdharma@arizona.edu

Clayton T. Morrison
Đại học Arizona
claytonm@arizona.edu

Tóm tắt

Các phương pháp dịch máy thần kinh (NMT) được phát triển cho xử lý ngôn ngữ tự nhiên đã được chứng minh là rất thành công trong việc tự động hóa dịch thuật từ ngôn ngữ tự nhiên này sang ngôn ngữ tự nhiên khác. Gần đây, các phương pháp NMT này đã được điều chỉnh để áp dụng cho việc tạo mã chương trình. Trong NMT cho việc tạo mã, nhiệm vụ là tạo ra mã nguồn đầu ra thỏa mãn các ràng buộc được biểu diễn trong đầu vào. Trong tài liệu, nhiều kịch bản đầu vào khác nhau đã được khám phá, bao gồm tạo mã dựa trên mô tả ngôn ngữ tự nhiên, các biểu diễn cấp thấp hơn như nhị phân hoặc assembly (dịch ngược thần kinh), các biểu diễn một phần của mã nguồn (hoàn thiện và sửa chữa mã), và mã nguồn trong ngôn ngữ khác (dịch mã). Trong bài báo này, chúng tôi khảo sát tài liệu về NMT cho việc tạo mã, phân loại các phương pháp đa dạng đã được khám phá theo các biểu diễn đầu vào và đầu ra, kiến trúc mô hình, kỹ thuật tối ưu hóa được sử dụng, tập dữ liệu, và phương pháp đánh giá. Chúng tôi thảo luận về các hạn chế của các phương pháp hiện có và hướng nghiên cứu trong tương lai.

1 Giới thiệu

Dịch máy thần kinh (NMT) đã được sử dụng rộng rãi trong xử lý ngôn ngữ tự nhiên, nơi nhiệm vụ là dịch các câu từ ngôn ngữ này sang ngôn ngữ khác (chẳng hạn từ tiếng Anh sang tiếng Đức). Hầu hết các framework NMT đều tuân theo kiến trúc Encoder-Decoder được tiên phong bởi Sutskever et al. [162]. Trong kiến trúc này, các chuỗi đầu vào của các token (đại diện cho từ hoặc ký tự trong ngôn ngữ nguồn) được đưa vào một mạng nơ-ron encoder để tính toán một biểu diễn nội bộ, sau đó một mạng nơ-ron decoder nhận biểu diễn nội bộ đó và giải mã nó thành chuỗi đầu ra của các token tương ứng với ngôn ngữ đích. Các nghiên cứu gần đây đã bắt đầu khám phá việc áp dụng ý tưởng này và các ý tưởng liên quan cho nhiệm vụ tạo mã nguồn, dẫn đến một lĩnh vực con mới của NMT cho việc tạo mã. Đây hiện là một lĩnh vực nghiên cứu tích cực và phát triển nhanh chóng với nhiều ứng dụng [6,101,69]. Ví dụ, việc tạo mã từ ngôn ngữ tự nhiên có thể giúp các nhà phát triển mới bắt đầu viết mã và các lập trình viên có kinh nghiệm làm việc hiệu quả hơn [108,10]. Trong cái gọi là dịch ngược thần kinh, NMT cho việc tạo mã đã được áp dụng để dịch từ assembly hoặc nhị phân sang mã nguồn. Điều này có thể giúp các nhà nghiên cứu bảo mật với việc nhận dạng malware, hiểu chức năng của các chương trình, so sánh chương trình, và nhiều hơn nữa [57,109]. Việc tạo mã từ mã nguồn một phần có thể giúp trong việc hoàn thiện chương trình, sửa chữa, và cung cấp phản hồi tự động [19,40]. Và cuối cùng, trong bối cảnh tương tự trực tiếp với NMT cho dịch ngôn ngữ tự nhiên, các phương pháp NMT có thể được sử dụng để dịch từ mã nguồn trong một ngôn ngữ lập trình sang ngôn ngữ khác [33]. Trong tài liệu, các nhà nghiên cứu đang sử dụng các phương pháp khác nhau trong mỗi lĩnh vực ứng dụng này, nhưng nói chung, lĩnh vực này đang phát triển rất nhanh, với các kết quả quan trọng bị cô lập trong các cộng đồng ứng dụng cụ thể, và những hiểu biết quan trọng có thể chuyển giao sang ứng dụng khác rất khó theo dõi. Việc tóm tắt các bài báo này dựa trên các phương pháp và ý tưởng mà chúng sử dụng và xác định các hạn chế của chúng là quan trọng để thông báo cho việc phát triển các kiến trúc và giải pháp mới. Trong khảo sát được trình bày ở đây, chúng tôi phát triển một số chiều để phân biệt các cách tiếp cận khác nhau. Một chiều tự nhiên là xem xét lớp kiến trúc mạng nơ-ron được sử dụng; điều này bao gồm các biến thể của mạng nơ-ron tái phát (RNN), transformer và các mô hình ngôn ngữ lớn (LLM), decoder cây, mạng nơ-ron đồ thị (GNN), các phương pháp neuro-symbolic, và kết hợp học tăng cường (RL). Chúng tôi xem xét từng cái về các ưu điểm và hạn chế của chúng. Theo một chiều khác, chúng tôi đặc trưng hóa các cách tiếp cận theo biểu diễn đầu ra mà chúng tạo ra: chuỗi hoặc đồ thị (cây cú pháp trừu tượng (AST)) với các ưu điểm và hạn chế của mỗi phương pháp. Chúng tôi hy vọng nội dung của bài báo này sẽ hữu ích cho các nhà nghiên cứu mới để có cái nhìn tổng quan về lĩnh vực này, và cho các chuyên gia để thiết kế các kiến trúc và giải pháp mới phù hợp với nhiệm vụ và yêu cầu của họ. Khi ranh giới giữa các phương pháp không rõ ràng hoặc khi các bài báo sử dụng các phương pháp lai, chúng tôi đã nhóm chúng dựa trên thành phần chính của chúng.

Tổng hợp chương trình: nền tảng và xu hướng [69] cung cấp phân tích chi tiết về các phương pháp truyền thống như tổng hợp chương trình quy nạp (tìm kiếm liệt kê hoặc các giải pháp dựa trên sketch [155]), và các kỹ thuật tổng hợp chương trình suy diễn. Tổng hợp chương trình suy diễn hoạt động bằng cách dịch các mô tả từ ngôn ngữ hình thức (đặc tả) sang mã nguồn sử dụng chứng minh định lý để tìm một chứng minh thỏa mãn tất cả các ràng buộc và trích xuất chương trình từ các chứng minh [63,121]. Do đó, nó yêu cầu các mô tả chương trình rõ ràng trong ngôn ngữ hình thức để bắt đầu. Các phương pháp quy nạp [68] có thể tạo mã nguồn từ các cặp ví dụ đầu vào-đầu ra, nhưng chúng thực hiện tìm kiếm trên tất cả các quy tắc ngôn ngữ và khó mở rộng cho việc tạo mã nguồn thực tế. Chúng chủ yếu làm việc với các ngôn ngữ đặc thù cho lĩnh vực (DSL) với ít quy tắc so với ngữ pháp của các ngôn ngữ lập trình chung như C, C++, và Python. Các bài báo khảo sát hiện có [6,101] cũng cung cấp khảo sát về các phương pháp xác suất, các mô hình được hướng dẫn bởi ngôn ngữ đặc thù cho lĩnh vực (DSL), và các mô hình ngôn ngữ n-gram [76] cùng với các ứng dụng của kỹ thuật học máy trên mã như tóm tắt mã, sửa lỗi, và nhiều hơn nữa. Bài báo gần đây [101] bao gồm các phương pháp dựa trên học sâu cho việc tạo mã nguồn nhưng không bao gồm các kỹ thuật mới nhất như các phương pháp dựa trên RL, các bài báo từ lĩnh vực dịch ngược thần kinh, các mô hình ngôn ngữ lớn gần đây như CODEX và ALPHA CODE, và các kỹ thuật biểu diễn mã. Một hướng nghiên cứu khác phổ biến cho tổng hợp chương trình dựa trên các trình thông dịch khả vi. Các phương pháp dựa trên trình thông dịch khả vi [23,113,61,97,88,142,62] tạo mã nguồn từ các ví dụ đầu vào đầu ra. Chúng định nghĩa một ánh xạ khả vi từ đầu vào và mã nguồn đến đầu ra và sử dụng gradient descent để tìm kiếm chương trình tốt nhất thỏa mãn các ràng buộc. Nhược điểm chính của các phương pháp dựa trên trình thông dịch khả vi là mỗi vấn đề được giải quyết độc lập, và các phương pháp hiện tại dựa trên ý tưởng này chưa mở rộng được cho việc tạo mã trong ngôn ngữ lập trình chung như C, C++, và Python. Chúng tôi không bao gồm các phương pháp chưa mở rộng được cho việc tạo mã trong các ngôn ngữ lập trình chung như C, C++, và Python và đề xuất độc giả tham khảo các bài báo khảo sát. Phân tích ngữ nghĩa và tạo mã là những nhiệm vụ khá liên quan nơi các ý tưởng từ một lĩnh vực có thể được chuyển giao sang lĩnh vực khác, chúng tôi giới thiệu độc giả đến bài báo khảo sát này [103] về phân tích ngữ nghĩa và tạo mã. Tương tự, [186] tập trung nhiều hơn vào hiểu mã trong khi chúng tôi tập trung vào tạo mã.

Bài báo khảo sát này được tổ chức như sau: phần tổng quan [2] cung cấp tóm tắt ngắn gọn về các thành phần được sử dụng bởi các thuật toán dựa trên NMT, phần NMT [3] giới thiệu framework dịch máy thần kinh theo sau bởi các phương pháp được sử dụng trong NLP cho dịch ngôn ngữ làm cơ sở cho các kỹ thuật tạo mã hiện tại, phần NMT4Code [4] cung cấp mô tả về các phương pháp được sử dụng bởi các bài báo tạo mã hiện có, phần cơ chế sao chép [refsection:copy] giới thiệu cơ chế sao chép sẽ giúp các mô hình NMT sao chép một số token từ chuỗi đầu vào sang chuỗi đầu ra, phần học biểu diễn [6] cung cấp các kỹ thuật học biểu diễn hiện tại, phần tập dữ liệu [8] cung cấp danh sách các tập dữ liệu được sử dụng cho việc tạo mã nguồn, phần đánh giá [9] cung cấp các kỹ thuật đánh giá, phần vấn đề mở [7] cung cấp các vấn đề mở và hướng nghiên cứu trong tương lai. Cuối cùng, trong phần kết luận [10] chúng tôi kết luận bài báo của mình.

2 Tổng quan

Trong phần này, chúng tôi cung cấp tóm tắt về các thành phần quan trọng được sử dụng bởi các bài báo tạo mã này.

2.1 Mạng nơ-ron tái phát (RNN)

Mạng nơ-ron tái phát [146,87,154] là phần mở rộng của các mô hình feedforward để học từ một chuỗi. Tại bất kỳ bước thời gian t nào, cho token đầu vào hiện tại (xt) và trạng thái ẩn trước đó (xt−1), đơn vị RNN tạo ra một trạng thái ẩn (ht: tóm tắt của chuỗi đến bước thời gian hiện tại) cùng với một số đầu ra (ot). Chúng ta có thể mô hình hóa ot và ht với các phương trình sau:

ht=tanh (Wi∗xt+Wh∗ht−1) (1)
ot=Wo∗ht (2)

Cho một chuỗi từ (x1, x2, ..., xt), RNN hoạt động theo cách tuần tự tạo ra đầu ra (ot) và trạng thái ẩn (ht) tại mỗi bước thời gian t. Ưu điểm của RNN là nó có thể xử lý chuỗi có độ dài bất kỳ. Nhược điểm của RNN là nó không thể xử lý các phụ thuộc tầm xa, và khó huấn luyện vì vấn đề biến mất gradient [133,77]. Bộ nhớ dài-ngắn hạn (LSTM) [78] và các đơn vị tái phát có cổng (GRU) [35] là các biến thể của RNN với cơ chế có cổng với hai trạng thái: trạng thái ẩn cho bộ nhớ ngắn hạn và trạng thái tế bào cho bộ nhớ dài hạn, và được chứng minh xử lý các phụ thuộc tầm xa tốt hơn RNN vanilla. Chúng ta có thể sử dụng RNN và các biến thể của chúng để tạo ra biểu diễn cho dữ liệu tuần tự như chuỗi mã nguồn, dạng tuyến tính hóa của cây cú pháp trừu tượng (AST), hoặc mô tả ngôn ngữ tự nhiên của mã nguồn.

2.2 Transformer và các mô hình ngôn ngữ lớn (LLM)

Transformer [173] loại bỏ cơ chế mã hóa tuần tự của RNN và thay vào đó mã hóa các token trong mạng feedforward với các lớp attention đa đầu tích vô hướng. Ưu điểm của transformer là chúng có thể xử lý các phụ thuộc tầm xa tốt hơn RNN và dễ song song hóa. Cho các vector truy vấn (Q), giá trị (V), và khóa (K), Attention sau đó được tính như sau:

MultiHeadAttention (Q, K, V) =concat ([head 1, head 2, ...., head k]W0 (3)
head i=Attention (QWQ_i, KWK_i, V WV_i) (4)
Attention (Q, K, V) =softmax (QKT/√dk)V (5)

Trong đó √dk là hệ số tỷ lệ bằng với chiều của vector đặc trưng. Đầu vào được chuyển đổi thành vector khóa, truy vấn, và giá trị bằng cách sử dụng toán tử tuyến tính và attention được tính bằng cách sử dụng nhiều đầu như được hiển thị bởi các phương trình trên. Vector đặc trưng cho token truy vấn được cập nhật với tổ hợp tuyến tính của vector đặc trưng của các token khác. Vector đặc trưng này sau đó đi qua một lớp feedforward khác với kết nối dư và quá trình này được lặp lại. Cho rằng mã nguồn có các phụ thuộc tầm xa [101] và transformer có thể xử lý các phụ thuộc tầm xa, transformer là kiến trúc quan trọng cho việc tạo mã nguồn. Các mô hình ngôn ngữ lớn dựa trên Transformer như GPT-3 [25] và BERT [41] đã thống trị lĩnh vực NLP cho việc tạo chuỗi và học biểu diễn tương ứng, và đang được sử dụng ngày càng nhiều cho việc tạo mã [179,53]. Chúng ta có thể sử dụng transformer hoặc LLM để tạo ra biểu diễn tuần tự của mã nguồn, hoặc dạng tuyến tính hóa của cây cú pháp trừu tượng (AST).

2.3 TreeDecoder

Các phương pháp dựa trên giải mã cây như dịch Tree-to-Tree [33] tạo ra một cây bằng cách giải mã từng nút một với một số thuật toán duyệt cây như tìm kiếm theo chiều sâu (DFS) hoặc tìm kiếm theo chiều rộng (BFS). Các decoder dựa trên cây thường sử dụng attention [12] trên chuỗi đầu vào để tạo ra một nút mới. Các decoder dựa trên cây cũng sử dụng parent feeding (sử dụng trạng thái ẩn của nút cha) [33] để cải thiện việc giải mã của một nút mới. Hệ thống có thể được mở rộng để chú ý đến cây được tạo ra một phần cùng với attention đến chuỗi đầu vào. Các thành phần cốt lõi của các decoder này là RNN (LSTM, và GRU). Ví dụ, Tree-to-Tree [33] sử dụng hai LSTM, một cho dự đoán con trái và một cho dự đoán con phải. Hầu hết các phương pháp tạo mã dựa trên tree decoder đầu tiên tạo ra cây cú pháp trừu tượng (AST) sau đó có thể được chuyển đổi thành mã nguồn.

2.4 GNN

Mạng nơ-ron đồ thị [187,92,174,43] là các phương pháp khá mạnh mẽ cho dự đoán và học có cấu trúc đồ thị. Hầu hết chúng đều hoạt động dựa trên nguyên lý truyền thông điệp nơi một nút nhận một số thông tin về các nút láng giềng của nó và cập nhật trạng thái của nó. Hãy xem xét AST một phần của chúng ta như một đồ thị G= (V,E), trong đó V là tập hợp các nút và E là tập hợp các cạnh. Gọi X ∈R|v|∗d là tập hợp các đặc trưng nút trong đó mỗi nút v∈ V có một đặc trưng d chiều. Lần lặp truyền thông điệp thứ k của một GNN có thể được mô hình hóa như một biến thể của phương trình sau [73]:

h(k+1)_v =update(k)(h(k)_v,aggregate(k)(h(k)_u,∀u∈ N(v)) (6)
=update(k)(h(k)_v,m(k)_N(v)) (7)

Trong đó N(v) biểu thị các láng giềng của nút v. Tại bất kỳ lần lặp nào của GNN, hàm aggregate lấy embedding của các láng giềng của nút v và kết hợp chúng thành một vector embedding. Hàm update lấy vector embedding của nút v tại bước thời gian trước đó và vector embedding đầu ra của hàm aggregate để cho chúng ta một embedding mới cho nút v. Ở đây, update và aggregate có thể là bất kỳ hàm khả vi nào. Cho rằng hàm aggregate nhận một tập hợp đầu vào, GNN được định nghĩa theo cách này là bất biến hoán vị. Hơn nữa, các phương trình có thể dễ dàng được sửa đổi để bao gồm thông tin vector embedding cạnh nếu có sẵn. Lưu ý rằng một lần lặp của GNN thu thập thông tin từ vùng láng giềng bước đầu. Nếu chúng ta muốn thu thập thông tin từ vùng láng giềng k bước, chúng ta có thể chạy GNN k lần [k lớp]. Chúng ta có thể sử dụng GNN để tạo ra biểu diễn cây của mã nguồn tức là cây cú pháp trừu tượng (AST).

2.5 Neuro-symbolic

Các phương pháp neuro-symbolic [59,28] là các phương pháp kết hợp mạng nơ-ron với logic symbolic. Trong bài báo khảo sát này, chúng tôi bao gồm các bài báo có thành phần neural và thành phần symbolic như DeepCoder [13] và DreamCoder [49] như các phương pháp neuro-symbolic. Một mô hình khác của các phương pháp neuro-symbolic tạo ra một chương trình P mà khi áp dụng cho đầu vào sẽ tạo ra đầu ra mong muốn. Ví dụ, Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar [80] sử dụng transformer [173] và pointer network [175] để dự đoán một chuỗi các quy tắc ngữ pháp chỉnh sửa (quy tắc DSL như chèn và xóa từ một vị trí). Hãy gọi các chuỗi hành động này là một chương trình (P). Chương trình này (P) khi áp dụng cho mã nguồn có lỗi (x), xuất ra chương trình không có lỗi (y). Ưu điểm của các phương pháp neuro-symbolic là các quy tắc DSL có thể hiểu được bởi con người, và phương pháp này cũng có thể tổng quát hóa tốt hơn khi ngoại suy so với các cách tiếp cận chỉ neural. Các phương pháp neuro-symbolic được chia thành nhiều nhóm dựa trên thành phần chính của chúng là module neural hay symbolic [59]. Phương pháp sử dụng càng nhiều thành phần symbolic thì càng có thể diễn giải được nhưng khó mở rộng hơn cho các vấn đề tạo mã lớn.

2.6 RL

Tất cả các phương pháp trên không tận dụng các unit test và thông báo lỗi compiler vì chúng không khả vi. Các thuật toán như REINFORCE [185] có thể được sử dụng để huấn luyện hệ thống từ các giá trị không khả vi như vậy. Các phương pháp dựa trên học tăng cường chuyển đổi kết quả của unit test và đầu ra compiler thành giá trị reward và chủ yếu sử dụng các phương pháp policy gradient [163,42] để huấn luyện mạng dựa trên các tín hiệu này [100]. Một khi chúng ta có cơ chế để tính reward dựa trên các tín hiệu này, chúng ta có thể sử dụng nó như một hàm loss nơi mục tiêu của chúng ta là tối đa hóa tổng reward. Do đó loss trở thành:

loss_rl =−Eˆy∼Pθ[r(ˆy)], (8)

trong đó ˆy= [ ˆy1, ...ˆyT] là chuỗi được tạo bởi mô hình (transformer, GNN, v.v.), Pθ và r(ˆy) là reward cho chuỗi được tạo đó. Sau đó chúng ta có thể ước tính gradient của hàm reward không khả vi như sau:

∇θloss_rl ≈ −Eˆy∼pθ[r(ˆy)∗ ∇θlogp θ(ˆy|x)] (9)
≈ −Eˆy∼pθ[r(ˆy)∗∑_t∇θlogp θ( ˆyt|ˆy1, ..., ˆyt−1,x)], (10)

trong đó ˆy là chuỗi đầu ra được tạo bởi mô hình từ chuỗi đầu vào x và ˆyt là token được dự đoán trong chuỗi tại bước thời gian t.

3 Dịch máy thần kinh: NMT

Trong phần này, chúng tôi cung cấp một framework toán học cho hệ thống dịch máy thần kinh. Sau đó chúng tôi mô tả các bài báo và ý tưởng quan trọng mà quan trọng cho các hệ thống tạo mã dựa trên NMT. Các hệ thống dịch này thường tuân theo một framework toán học được mô tả dưới đây. Chủ yếu, chúng giả định có một corpus song song để huấn luyện mô hình chứa các cặp chuỗi đầu vào và đầu ra. Gọi đầu vào là x chứa một chuỗi các token x1, x2, x3, ..., xn. Gọi đích ground truth cho chuỗi đầu vào x là y chứa một chuỗi các token đầu ra y1, y2, y3, ..., ym. Gọi framework dịch được tham số hóa bởi θ. Khi đó mục tiêu tổng thể là tối đa hóa xác suất có điều kiện của y cho x. Trong đó ˆyi là dự đoán của token tại bước thời gian i và yi là token ground truth tại bước thời gian i. Như chúng ta có thể thấy từ phương trình dưới đây, việc tối đa hóa xác suất có điều kiện này tương đương với việc tối thiểu hóa tổng cross-entropy loss tại mỗi bước thời gian có thể được tối thiểu hóa bằng cách sử dụng backpropagation through time (BPTT) [184]. Đối với các nhiệm vụ dịch ngôn ngữ, x là câu đầu vào trong một ngôn ngữ và y là câu đầu ra trong ngôn ngữ khác. Đối với việc tạo mã, y là mã nguồn, trong khi x có thể là bất kỳ biểu diễn nào của mã nguồn như mã assembly hoặc giải thích tự nhiên của mã. Hầu hết các mô hình NMT khác nhau trong cách chúng tính P(yi|y<i,x;θ). Chúng ta có thể mô hình P(yi|y<i,x;θ) bằng cách sử dụng các mô hình như RNN, transformer, tree decoder, hoặc GNN. Đối với các mô hình NMT, yi là token ground truth tại bước giải mã i. Đối với tree decoder và GNN, yi là token ground truth tại nút i. Do đó, đối với các mô hình NMT, tổng loss của một mẫu là tổng cross-entropy loss tại mỗi bước của bước giải mã. Đối với tree decoder và mô hình GNN, tổng loss là tổng cross-entropy loss tại mỗi nút của bước giải mã.

P(y|x;θ) = arg max_θ P(y1, y2, y3, ..., ym|x;θ)
= arg max_θ P(y1|x)∗P(y2|y1,x;θ)∗...∗P(ym|ym−1, ...y1,x;θ) ∵chain rule
= arg max_θ ∏^m_{i=1} P(yi|y<i,x;θ) where: y<i=y1, ..., yi−1

tương đương với việc tối đa hóa log likelihood
= arg max_θ log ∏^m_{i=1} P(yi|y<i,x;θ)
= arg max_θ ∑^m_{i=1} log P(yi|y<i,x;θ)

tương đương với việc tối thiểu hóa negative log likelihood
= arg min_θ −∑^m_{i=1} log P(yi|y<i,x;θ)

let P(yi|y<i,x;θ)∼Categorical(p)
then, using the pmf of a categorical distribution
= arg min_θ −∑^m_{i=1} log ∏^K_{k=1} p^{I(ˆy = k)}_k
= arg min_θ ∑^m_{i=1} ∑^K_{k=1} −log p^{I(ˆy = k)}_k
= arg min_θ ∑^m_{i=1} −yi∗log p(ˆyi)

Ở đây, Categorical(p) là phân phối categorical trên từ vựng đầu ra. Tất cả các mô hình này cố gắng tối đa hóa likelihood của token tiếp theo cho các token đã được tạo cho đến nay. Do đó, các token được tạo có thể không đúng về mặt cú pháp. Một số phương pháp neuro-symbolic thường đặt ràng buộc về ngữ pháp của ngôn ngữ đầu ra tại mỗi bước dự đoán token, sao cho đầu ra luôn đúng về mặt cú pháp [192]. Trong phần sau, chúng tôi tóm tắt các bài báo quan trọng xây dựng nền tảng cho các kỹ thuật NMT hiện tại cho việc tạo mã nguồn.

3.1 Học chuỗi đến chuỗi với mạng nơ-ron

Mạng chuỗi đến chuỗi (kiến trúc Encoder-Decoder) được đề xuất bởi [162] là một trong những framework được sử dụng rộng rãi nhất cho dịch máy thần kinh. Hình sau đây hiển thị kiến trúc của họ trong đó sos biểu thị token bắt đầu chuỗi, và eos biểu thị token kết thúc chuỗi. ht biểu thị trạng thái ẩn của LSTM tại bước thời gian t. EMB-E biểu thị lớp embedding cho encoder và EMB-D biểu thị lớp embedding cho decoder. LIN biểu thị lớp tuyến tính chuyển đổi trạng thái ẩn đầu ra của decoder thành xác suất từ đầu ra. z là vector bối cảnh từ bước thời gian cuối cùng của encoder nắm bắt tất cả thông tin về chuỗi đầu vào và được truyền cho decoder để đưa ra dự đoán.

[Hình 1: Học chuỗi đến chuỗi]

Tại một bước thời gian duy nhất, LSTM nhận vector embedding của đầu vào xt và trạng thái ẩn trước đó ht−1 để có trạng thái ẩn hiện tại ht. Vấn đề chính với cơ chế này là nó cần nén tất cả thông tin về chuỗi đầu vào vào vector bối cảnh z duy nhất, điều này nhanh chóng trở thành nút thắt cổ chai cho các chuỗi dài hơn. Tác động đối với việc tạo mã là: mặc dù đây là phương pháp baseline tốt, nó không mạnh mẽ trong việc xử lý các phụ thuộc tầm xa thường xuyên trong nhiệm vụ tạo mã.

3.2 Dịch máy thần kinh bằng cách học đồng thời căn chỉnh và dịch

Kiến trúc của chuỗi đến chuỗi với attention [12] này tương tự như kiến trúc chuỗi đến chuỗi trên (encoder-decoder) [162] với các khác biệt sau: đầu tiên, biểu diễn token đầu vào (embedding) tại mỗi bước thời gian (ht) kết hợp biểu diễn (embedding) của một từ được tính bằng cách sử dụng bối cảnh từ cả hai bên của từ. Bối cảnh từ bên trái (h→_t) được tính bằng cách sử dụng GRU [35] nhận các token từ chuỗi đầu vào theo cách từ trái sang phải. Bối cảnh từ bên phải (h←_t) được tính bằng cách sử dụng GRU [35] nhận các token từ chuỗi đầu vào theo cách từ phải sang trái. Cuối cùng, hai biểu diễn này: (h→_t) và (h←_t) được nối lại để có biểu diễn của một từ. Điều này giúp mô hình nắm bắt bối cảnh của một từ từ cả hai bên. Thứ hai, nó giảm vấn đề nút thắt cổ chai của kiến trúc trước bằng cách sử dụng cơ chế attention. Kiến trúc này cho phép mô hình nhìn vào biểu diễn ẩn của tất cả các bước thời gian của chuỗi đầu vào trong khi giải mã ˆyt. Do đó mô hình không cần nén tất cả thông tin vào vector bối cảnh z. Tại mỗi bước giải mã ˆyt, mô hình này nhận dự đoán trước đó ˆyt−1 và một vector ci. Vector bối cảnh ci khác nhau cho mỗi bước giải mã. Đầu tiên, nó tính các giá trị α, là trọng số (vô hướng) mà mô hình nên dành cho từ đầu vào, tức là αt,1 biểu thị trọng số mà mô hình nên dành cho từ đầu vào x1 trong khi giải mã yt. Sau đó vector bối cảnh cuối cùng ci được tính như sau:

ci=∑^T_{j=1} αi,j hj (11)

Trong đó hj=h→_j;h←_j và ; biểu thị toán tử nối. Họ sử dụng một perceptron đa lớp (MLP) để tính trọng số αi,j. Các trọng số này sau đó được chuyển đổi sang phạm vi [0, 1] bằng cách sử dụng hàm softmax. Tác động đối với việc tạo mã là: cơ chế attention giúp mô hình này xử lý các phụ thuộc tầm xa so với kiến trúc trước.

3.3 Attention là tất cả những gì bạn cần

Vấn đề chính với hai kiến trúc trước [162,12] là chúng có tính tuần tự, nghĩa là chúng nhận một từ mỗi lần làm tăng thời gian huấn luyện và khó song song hóa. Transformer [173] giải quyết vấn đề này bằng cách loại bỏ kiến trúc mạng nơ-ron tái phát nhận một token mỗi lần và sử dụng các lớp attention và các lớp kết nối đầy đủ có thể nhận toàn bộ chuỗi mỗi lần. Do đó, mô hình dễ song song hóa trên GPU. Hơn nữa, nó khắc phục vấn đề biến mất gradient và bùng nổ gradient của hai kiến trúc trên vì nó không có phép nhân chuỗi dài xảy ra trong backpropagation through time (BPTT). Chúng ta có thể xem multi-head attention như việc tạo ra embedding bối cảnh hóa của token đầu vào. Vì mô hình nhận toàn bộ chuỗi mỗi lần làm đầu vào, nó không bảo toàn tính chất tuần tự của đầu vào và đầu ra. Họ sử dụng mã hóa vị trí để giải quyết vấn đề này trong đó embedding của từ đầu vào được cộng với embedding của vị trí nó xuất hiện. Tác động đối với việc tạo mã là: nó có thể xử lý các phụ thuộc dài hạn dễ dàng hơn hai kiến trúc trước vì mỗi token đầu vào được kết nối với token khác thông qua một đơn vị multi-head attention. Nhược điểm chính của mô hình này là độ phức tạp thời gian chạy và độ phức tạp bộ nhớ của hệ thống tăng bậc hai theo độ dài đầu vào. Điều này có thể là vấn đề lớn cho việc tạo mã vì đầu vào cho việc tạo mã (như assembly) có thể khá lớn. Trong những trường hợp như vậy, các phương pháp cải thiện độ phức tạp này thành tuyến tính như BigBird [196], và LongFormer [16] có thể cung cấp giải pháp thay thế tốt. Hơn nữa, việc làm cho cơ chế attention nhận biết IO và thưa thớt [38] là một hướng thay thế khác để xử lý các chuỗi tầm xa.

Bảng 1 sau đây hiển thị phân tích độ phức tạp cho một bước giải mã duy nhất của decoder đối với độ dài của chuỗi đầu vào (n).

Bảng 1: So sánh giữa các mô hình NMT
Mô hình | độ phức tạp thời gian chạy | độ phức tạp bộ nhớ | độ dài đường dẫn | dễ song song hóa
SEQ2SEQ | O(n) | O(1) | O(n) | ×
ATTENTION | O(n) | O(n) | O(n) | ×
TRANSFORMER | O(n²) | O(n²) | O(1) | ✓

Hơn nữa, các mô hình không gian trạng thái [64,66,65,39] đang trở nên phổ biến trong việc xử lý các phụ thuộc tầm xa tốt hơn kiến trúc transformer và việc khám phá hướng này cho việc tạo mã là một hướng nghiên cứu thú vị và hứng thú.

3.4 Hướng tới dịch máy thần kinh chuỗi đến cây

Bài báo này [2] đưa kiến thức cú pháp về ngôn ngữ đích vào mô hình bằng cách dịch chuỗi đích thành cây cấu tạo. Họ tuyến tính hóa cây này thành một chuỗi và sử dụng chuỗi đến chuỗi với attention [12] để tạo ra chuỗi đích. Họ chỉ ra rằng điều này cải thiện dự đoán và căn chỉnh giữa chuỗi đầu vào và chuỗi đích so với dự đoán chuỗi đầu ra trực tiếp mà không có kiến thức cú pháp. Tác động của điều này đối với việc tạo mã là: chúng ta có thể chuyển đổi mã nguồn thành cây cú pháp trừu tượng (AST) chứa thông tin cú pháp và cố gắng dự đoán AST này thay vì mã nguồn để có được mô hình tốt hơn. Dự đoán cây trực tiếp khó song song hóa, nhưng bước tuyến tính hóa cho phép chúng ta sử dụng các mô hình chuỗi đến chuỗi như transformer [173] để đưa ra dự đoán, và nó dễ song song hóa. Hơn nữa, có số lượng ngày càng tăng các bài báo cố gắng đưa kiến thức cú pháp bằng cách pre-train trên AST [177], và kiến thức ngữ nghĩa bằng cách pre-train trên data flow graph [70]. Mô hình có thể nắm bắt cấu trúc cú pháp và cấu trúc ngữ nghĩa của mã tốt hơn mô hình không có.

3.5 Tokenization

Tokenization của mã nguồn là một thành phần quan trọng của dịch máy thần kinh cho việc tạo mã nguồn. Các nhà phát triển liên tục tạo ra các từ mới để đặt tên cho biến và hàm của họ, do đó làm tăng kích thước từ vựng đầu ra. Độ phức tạp của việc huấn luyện và giải mã hệ thống dựa trên NMT tăng theo kích thước từ vựng đầu ra [84]. Tokenization của mã nguồn hoặc ngôn ngữ tự nhiên có thể được nhóm thành các nhóm cấp cao sau:

3.5.1 Tokenization dựa trên từ:

Tokenization dựa trên từ tokenize mã nguồn hoặc ngôn ngữ tự nhiên dựa trên các từ hoàn chỉnh cho các từ thường xuyên và sử dụng token "<unk>" (không xác định) cho các từ hiếm. Nhưng điều này làm giảm chất lượng dịch thuật nếu số lượng từ không xác định tăng [12]. Cho rằng mã nguồn có phân phối đuôi dài thực sự [101], việc chăm sóc các token này là quan trọng. Có nhiều giải pháp cho vấn đề này trong tài liệu, ví dụ, [118] thực hiện căn chỉnh token "<unk>" với chuỗi đầu vào và thay thế token "<unk>" bằng từ được căn chỉnh từ chuỗi đầu vào. Trong trường hợp mã nguồn, một giải pháp khả thi khác là thay thế các biến bằng các token cụ thể như "var0, var1, ..." (và tương tự cho các hàm). Đây là giải pháp tốt để giảm kích thước từ vựng đầu ra, nhưng nó làm cho mã được tạo khó đọc. Tóm lại, tokenization dựa trên từ này tăng kích thước từ vựng và có vấn đề cho việc học từ vựng mở. Việc tăng kích thước từ vựng làm tăng độ phức tạp thời gian và yêu cầu bộ nhớ cho các giải pháp dựa trên NMT.

3.5.2 Tokenization dựa trên ký tự:

Mã hóa cấp ký tự [36,112] sử dụng một ký tự duy nhất làm token. Ưu điểm của tokenization cấp ký tự là từ vựng đầu ra nhỏ, nhưng nó tăng độ dài chuỗi một lượng lớn khó dự đoán cho mô hình NMT. Hơn nữa, khó học biểu diễn ngữ nghĩa của một ký tự ('c') so với một từ ('computer'). Do đó, điều này tốt cho việc giảm kích thước từ vựng nhưng làm cho mô hình khó hội tụ hơn vì nó tăng độ dài chuỗi một lượng lớn.

3.5.3 Tokenization dựa trên từ con:

Giải pháp lai giữa tokenization cấp ký tự và tokenization cấp từ là tokenization dựa trên từ con. Nó dựa trên ý tưởng rằng các từ thường xuyên nên được tokenize như từ và các từ hiếm nên được tokenize như các từ con có ý nghĩa. Điều này thú vị trong trường hợp mã nguồn vì các nhà phát triển thường tạo ra tên mới bằng cách sử dụng camel-case và snake-case khá hiếm và kỹ thuật này cho phép chúng ta chia các từ mới này thành các từ con có ý nghĩa cấu thành. Điều này cho phép tokenization dựa trên từ con có từ vựng nhỏ hơn và vẫn có thể học các biểu diễn có ý nghĩa. Có nhiều cách để thực hiện tokenization từ con như byte pair encoding (BPE) [150], WordPiece [147], và SentencePiece [95]. Ngoài ra, mã nguồn chứa các giá trị số lớn và việc tokenize từng số riêng biệt là không thể. Trong những trường hợp như vậy, tốt hơn là tokenize các số này thành chuỗi các chữ số [90].

Chúng tôi giới thiệu đến [122, 159, 180, 138] để phân tích thêm về các kỹ thuật tokenization.

3.6 Tăng cường dữ liệu:

Tăng cường dữ liệu là kỹ thuật quan trọng để cải thiện khả năng tổng quát hóa của mạng nơ-ron sâu [153]. Một số kỹ thuật như xoay, lật, cắt ngẫu nhiên, v.v. tồn tại cho tăng cường dữ liệu trong lĩnh vực thị giác máy tính [153]. Một số kỹ thuật như thêm ngẫu nhiên, xóa ngẫu nhiên, thay thế từ đồng nghĩa, v.v. tồn tại cho lĩnh vực NLP [52]. Chúng ta có thể áp dụng một số kỹ thuật tăng cường dữ liệu từ NLP cho việc tạo mã nguồn. Gần đây, NEUTRON đã thử các kỹ thuật tăng cường dữ liệu như random masking và cho thấy cải thiện hiệu suất ấn tượng từ tăng cường dữ liệu. Hơn nữa, Exploring Data Augmentation for Code Generation Tasks [31] thử nghiệm với các kỹ thuật tăng cường dữ liệu cho việc tạo mã nguồn như sử dụng dữ liệu đơn ngữ bằng back translation [149,37], cải thiện nhận thức số và cho thấy cải thiện trong việc tạo mã. Nhưng, nghiên cứu về các kỹ thuật tăng cường dữ liệu cho việc tạo mã vẫn còn sơ khai và là hướng nghiên cứu quan trọng.

3.7 Chiến lược giải mã

Hầu hết các giải pháp dựa trên NMT tạo mã nguồn theo cách tự hồi quy. Token tiếp theo được dự đoán dựa trên likelihood tối đa của token cho chuỗi đầu vào và các token đã được tạo. Đã được chỉ ra rằng cách tiếp cận tham lam này có nhiều vấn đề như giải pháp suy thoái [182] và thiếu tính nhất quán ngữ nghĩa [15]. Cách tiếp cận tham lam có hiệu quả tính toán và là lựa chọn tối ưu cho bước thời gian hiện tại, nhưng khi chúng ta tạo toàn bộ chuỗi, nó có thể là lựa chọn không tối ưu. Một lựa chọn khác sẽ là lưu trữ mọi dự đoán tại mọi bước giải mã và khám phá tất cả các chuỗi có thể (tìm kiếm toàn diện) từ các dự đoán này, điều này sẽ cho chúng ta chuỗi chính xác, nhưng điều này sẽ dẫn chúng ta đến thuật toán mũ. Giải mã beam search đứng ở giữa hai cực này bằng cách giữ một số chuỗi cố định tại mỗi bước thời gian dựa trên xác suất chung của chúng. Do đó, nó cung cấp sự cân bằng tốt giữa độ chính xác và chi phí tính toán. Tác động của điều này đối với việc tạo mã là: beam search là phương pháp mạnh mẽ có thể cải thiện độ chính xác của các mô hình tạo mã nguồn. Beam search giữ một số cố định (beam size) các dự đoán tại mỗi bước thời gian, nhưng beam search với kích thước thích ứng thậm chí có thể cải thiện kết quả tốt hơn [55]. Hơn nữa, beam search có thể được mở rộng để tạo mã nguồn đúng cú pháp với một số chi phí tính toán. Các phương pháp dựa trên tối đa hóa này (tham lam và beam search) đã được chỉ ra tạo ra các giải pháp với sự lặp lại không mong muốn [51] trong lĩnh vực NLP. Gần đây, [48] đã chỉ ra rằng sequential monte carlo (SMC) với một số hàm giá trị hoạt động tốt hơn beam search cho việc tạo chương trình đồ họa từ hình ảnh. Sequential monte carlo giữ K số lượng hạt (chương trình) và tái trọng số chúng trên một số hàm giá trị. Một nhược điểm của SMC là nó tốn kém tính toán hơn beam search. Một tập hợp phương pháp khác thường được sử dụng cho giải mã là các phương pháp ngẫu nhiên như top-k sampling [51], và nucleus sampling [79]. Các phương pháp ngẫu nhiên này được chỉ ra tạo ra văn bản không nhất quán về mặt ngữ nghĩa với tiền tố [157] trong lĩnh vực NLP. Một hướng chiến lược giải mã khác là sử dụng một mô hình khác làm ranker. Ví dụ, LEVER: Learning to Verify Language-to-Code Generation with Execution [126] xếp hạng lại các chương trình được tạo dựa trên ranker hoặc verifier dựa trên LM nhận mô tả đầu vào, mã được lấy mẫu và kết quả thực thi và cho thấy kết quả cải thiện. Coder Reviewer Reranking for Code Generation [201] lấy mẫu nhiều giải pháp sử dụng tham số nhiệt độ và sử dụng hai mô hình: mô hình coder đánh giá p(y|x) và mô hình reviewer đánh giá p(x|y) và sử dụng tích của hai điểm này để chọn chương trình được tạo. Họ cho thấy kết quả ấn tượng và tuyên bố kết quả state-of-the-art. Gần đây, các chiến lược giải mã dựa trên tìm kiếm tương phản [157,156] đã được chỉ ra là chiến lược giải mã tốt cho việc tạo văn bản thần kinh. Nhưng, nghiên cứu của họ về việc tạo mã nguồn còn hạn chế và là hướng nghiên cứu quan trọng. NeuroLogic Decoding [116] giới thiệu các ràng buộc logic trong quá trình giải mã cho việc tạo văn bản. NeuroLogic A∗ [117] giải mã mở rộng giải mã neurologic với các heuristic nhìn trước để tạo ra văn bản thỏa mãn các ràng buộc. Các kỹ thuật tương tự có thể hữu ích cho việc tạo mã dưới các ràng buộc. Gần đây, Planning With Large Language Models for Code Generation [199] đề xuất giải mã transformer được hướng dẫn bởi kế hoạch trong đó ý tưởng là sử dụng mô hình đã được pre-train để tạo chuỗi hoàn chỉnh từ một token đã cho (tìm kiếm nhìn trước) và đánh giá nó bằng cách sử dụng test case. Phương pháp này tạo ra các chương trình tốt hơn nhưng tốn kém tính toán.

4 Dịch máy thần kinh cho mã: NMT4Code

Trong phần này, chúng tôi tóm tắt các bài báo sử dụng dịch máy thần kinh cho việc tạo mã dựa trên biểu diễn đầu ra được tạo bởi các phương pháp này và các phương pháp họ sử dụng cho việc tạo mã. Các bài báo này có thể được tóm tắt thành hai nhóm tức là Chuỗi và Đồ thị dựa trên biểu diễn đầu ra mà chúng tạo ra cho mã nguồn.

Các phương pháp chuỗi biểu diễn mã nguồn đầu ra trong biểu diễn tuần tự. Biểu diễn tuần tự này có thể là dạng tuần tự của mã nguồn gốc hoặc dạng tuyến tính hóa của biểu diễn AST của mã nguồn. Ưu điểm của biểu diễn tuần tự là chúng ta có thể sử dụng các mô hình chuỗi đến chuỗi [162,12,173] ở dạng thô của chúng cho việc tạo mã nguồn.

4.1 SourceCode

Các phương pháp này tạo mã nguồn trực tiếp từ đầu vào đã cho. Chúng chủ yếu sử dụng các kỹ thuật RNN, LSTM, và Transformer cho việc tạo mã nguồn. Ưu điểm của việc tạo mã nguồn trực tiếp thay vì AST là chúng ta không cần module để chuyển đổi từ AST sang mã nguồn. Một ưu điểm khác của việc tạo mã nguồn trực tiếp là cùng một phương pháp có thể dễ dàng được chuyển giao cho việc tạo mã nguồn trong nhiều ngôn ngữ. Nhược điểm của việc tạo mã nguồn trực tiếp thay vì tạo AST là nó không chứa thông tin cú pháp một cách rõ ràng, làm cho mô hình khó học chúng một mình, và cũng các biểu diễn được học cụ thể hơn cho ngôn ngữ đích [8,7]. Trong phần sau, chúng tôi tóm tắt ngắn gọn các bài báo tạo mã nguồn dựa trên các phương pháp họ sử dụng.

4.1.1 RNN

: Các phương pháp này tạo mã nguồn sử dụng mạng nơ-ron tái phát (RNN) và các biến thể của chúng như LSTM và GRU. Vấn đề với các mô hình dựa trên RNN là chúng không thể xử lý các phụ thuộc tầm xa khá tốt. Ví dụ, RNN DECOMPILATION [90] tạo mã nguồn C từ chuỗi nhị phân sử dụng RNN. Họ tokenize các chuỗi nhị phân đầu vào ở cấp byte. Họ không sử dụng các disassembler hiện có tốt trong việc dịch từ nhị phân đầu vào sang assembly, có thể cung cấp bias quy nạp tốt cho mạng nơ-ron vì nó chứa ngữ nghĩa phong phú hơn và thông tin cấu trúc hơn chuỗi nhị phân. NEURAL DECOMPILATION [91] sử dụng LSTM với attention để tạo mã nguồn (C) từ biểu diễn assembly đầu vào. Họ sử dụng phản hồi compiler để huấn luyện lại hệ thống với một số dữ liệu bổ sung nếu hầu hết mã được tạo không compile được. Ý tưởng sử dụng phản hồi compiler để cải thiện hệ thống (hoặc bằng cách huấn luyện lại như được thực hiện ở đây hoặc sử dụng chúng như tín hiệu reward trong các hệ thống dựa trên RL) là hướng nghiên cứu quan trọng cho việc tạo mã nguồn có thể compile được. NEUTRON: [109] sử dụng mô hình LSTM với attention [119] để tạo mã nguồn (C) từ ngôn ngữ assembly. Họ đầu tiên phân đoạn assembly đầu vào thành các khối khác nhau sử dụng encoder LSTM và dịch từng đơn vị do đó tạo ra các đoạn mã trong ngôn ngữ cấp cao. Họ sử dụng phân tích data flow và control flow của assembly đầu vào để kết hợp các đoạn mã này thành một hàm. Hơn nữa, họ sử dụng phản hồi từ syntax checker và cải thiện hệ thống hơn nữa bằng cách huấn luyện lại và sử dụng các kỹ thuật sửa lỗi dựa trên quy tắc. LPN: Latent Predictor Networks for Code Generation [113] giải quyết vấn đề tạo mã nguồn trong ngôn ngữ cấp cao như Python và Java từ mô tả đầu vào sử dụng LSTM được sửa đổi với cơ chế attention [12]. Họ đề xuất cách tiếp cận lai dựa trên pointer network [175] và character RNN (predictor). Mô hình đầu tiên chọn predictor (latent) có điều kiện trên đầu vào và sử dụng nó để tạo ra một ký tự hoặc sao chép từ đầu vào. Do đó cùng một mạng có thể tạo ra ký tự và sao chép từ chuỗi đầu vào cùng lúc. SYNFIX: Automated correction for syntax errors in programming assignments using recurrent neural networks [19] sử dụng RNN để sửa lỗi cú pháp (ví dụ: thiếu dấu ngoặc) trong các khóa học trực tuyến mở đại chúng (MOOC) cho mã có lỗi sử dụng mô hình được học từ bài nộp đúng cho vấn đề đã cho. Họ sử dụng mô hình chuỗi để dự đoán thay thế hoặc chèn token tại vị trí được cung cấp bởi compiler. Vì compiler không phải lúc nào cũng cung cấp vị trí chính xác của lỗi, DEEPFIX: Fixing Common C Language Errors by Deep Learning [72] đề xuất RNN với attention [12] để dự đoán dòng có lỗi và mã để thay thế nó. Họ sử dụng phản hồi compiler để sửa lỗi lặp đi lặp lại trong chương trình cho đến khi chúng được sửa hoàn toàn. Các phương pháp này [19,72] chủ yếu sửa các lỗi liên quan đến cú pháp nhưng việc cố gắng giải quyết các lỗi liên quan đến ngữ nghĩa (ví dụ: thay thế biến) cũng là hướng thú vị [40]. An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation [170] thực hiện nghiên cứu thực nghiệm quy mô lớn về khả năng tồn tại của các mô hình NMT dựa trên RNN này để sửa lỗi thực tế và đưa ra câu trả lời tích cực. PIX2CODE: Generating Code from a Graphical User Interface Screenshot [17] tạo mã nguồn từ giao diện người dùng đồ họa. Họ sử dụng CNN để xử lý GUI đầu vào và tạo mã nguồn sử dụng LSTM. SPARSE POINTER NETWORK: Learning Python Code Suggestion with a Sparse Pointer Network [20] sử dụng LSTM với attention để tạo token mới cho việc hoàn thiện mã. Họ cũng áp dụng pointer network [175] với attention đến các định danh hiện có để chọn chúng cho việc hoàn thiện mã. Do đó, mô hình có thể tạo token mới và cũng có thể chọn một từ tập hợp hiện có để hoàn thiện. NL2BASH: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System [111] và Program Synthesis from Natural Language Using Recurrent Neural Networks [110] sử dụng RNN với attention để tạo script bash từ giải thích ngôn ngữ tự nhiên. Một hạn chế chung của việc tạo mã dựa trên RNN là hiệu suất của các decoder dựa trên RNN này giảm khi độ dài của chuỗi tăng.

4.1.2 Transformer và LLM

: Việc tạo mã sử dụng transformer [173] và các mô hình ngôn ngữ lớn [136,25,41] là phương pháp state-of-the-art hiện tại. CODEX [30] fine-tune một mô hình ngôn ngữ gọi là GPT [136] trên mã nguồn công cộng có sẵn trong GitHub cho việc tạo mã nguồn từ giải thích ngôn ngữ tự nhiên. Họ tạo nhiều mẫu và chọn mẫu tốt nhất dựa trên tiêu chí như unit test hoặc mean log probability, và điều này cải thiện hiệu suất của hệ thống một cách đáng kể. Hạn chế của mô hình này là hiệu suất mô hình giảm khi độ dài chuỗi tăng. CODEX cũng đã được triển khai trong các toolkit thực tế như GitHub Copilot để hỗ trợ các nhà phát triển bằng cách tạo mã nguồn từ các mô tả tự nhiên. ALPHA CODE [108] trình bày giải pháp có thể tạo mã nguồn cấp độ thi đấu (Python và C++) từ ví dụ ngôn ngữ tự nhiên và unit test. Họ pre-train mô hình của họ trên nhiều ngôn ngữ lập trình sử dụng masked language modeling như BERT [41] cho encoder và dự đoán token tiếp theo cho decoder [108]. Sau đó họ fine-tune mô hình của họ trên tập dữ liệu CodeContests. Trong quá trình suy luận, họ tạo hàng triệu mẫu cho mỗi vấn đề song song sử dụng nhiệt độ cao (ví dụ: T=0.25). Một khi họ tạo nhiều mẫu, họ lọc những mã có thể vượt qua các unit test ví dụ đã cho. Họ cũng nhóm các giải pháp dựa trên phản hồi của chúng đối với test case đầu vào. Các bước lấy mẫu, lọc, và nhóm này là các bước quan trọng cho việc tạo mã nguồn cấp độ thi đấu [108]. LLM: Program synthesis with large language models [10] thực hiện nghiên cứu quy mô lớn về các mô hình ngôn ngữ lớn (LLM) cho việc tạo mã nguồn (Python) từ mô tả ngôn ngữ tự nhiên và unit test. Họ chỉ ra rằng các LLM này khá hiệu quả trong các nhiệm vụ few-shot learning về việc tạo mã. INCODER: A Generative Model for Code Infilling and Synthesis [56] trình bày framework thống nhất cho việc tạo mã và chỉnh sửa mã dựa trên LLM. Họ huấn luyện các mô hình tự hồi quy để dự đoán một tài liệu bị che (một tài liệu nơi một số span được thay thế bằng <mask> và di chuyển đến cuối). Điều này cho phép họ tạo mã tại thời điểm test và cũng cho phép chỉnh sửa mã, đổi tên biến, dự đoán loại, và tạo comment sử dụng một mô hình duy nhất. POLYCODER A systematic evaluation of large language models of code [189] trình bày đánh giá hệ thống của các LLM tự hồi quy như Codex [30], GPT-Neo [21], GPT-J [176], GPT-NeoX [22], CodeParrot [172] trên các ngôn ngữ lập trình khác nhau. Họ cũng đề xuất một LLM cỡ trung bình gọi là PolyCoder được huấn luyện trong nhiều ngôn ngữ lập trình. Họ quan sát hiệu suất của LLM tăng bằng cách huấn luyện sử dụng nhiều ngôn ngữ, tăng kích thước mô hình (trừ CodeParrot), và huấn luyện lâu hơn. NL2CODE: A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages [27] trình bày benchmark tạo mã song song trên nhiều ngôn ngữ và đánh giá hai mô hình dựa trên LLM phổ biến CODEX và INCODER cho việc tạo mã. Generating Bug-Fixes Using Pretrained Transformers [47] chỉ ra rằng các mô hình dựa trên transformer [173] cũng khá hiệu quả trong việc sửa lỗi. Learning Autocompletion from Real-World Datasets [11] và IntelliCode Compose: Code Generation using Transformer [165] sử dụng các mô hình dựa trên transformer này cho việc hoàn thiện mã thực tế. Learning Performance-Improving Code Edits [120] sử dụng mô hình ngôn ngữ để dự đoán mã được tối ưu hóa (thay vì mã có độ phức tạp thời gian O(n²), đề xuất mã làm điều đó trong O(n)). CONVERSATION: A Conversational Paradigm for Program Synthesis [128] và The Programmer's Assistant [144] trình bày cách tiếp cận hội thoại cho việc tạo mã nguồn (Python) từ giải thích ngôn ngữ tự nhiên sử dụng mô hình dựa trên LLM nơi mô hình tương tác với người dùng để tạo các chương trình con nhiều lần dẫn đến giải pháp hoàn chỉnh. Họ nối các prompt trước đó với các chương trình con trước đó cho dự đoán chương trình con tiếp theo sử dụng LLM. Việc làm cho hệ thống có khả năng tạo mã nguồn trên mô hình hội thoại là hướng nghiên cứu quan trọng. Điều này cho phép mô hình hiểu ý định của người dùng tốt hơn. Việc hiểu ý định của người dùng cho việc tạo mã là bước đầu quan trọng hướng tới việc tạo mã [60]. Conversational Automated Program Repair [188] sử dụng mô hình hội thoại để sửa mã có lỗi sử dụng các mô hình LLM như codex và ChatGPT [131]. Gần đây, prompt-based engineering [183,181] đã khá phổ biến như một cách để hiểu ý định của người dùng cho việc tạo văn bản tự nhiên. Evaluating the Text-to-SQL Capabilities of Large Language Models [139] sử dụng đánh giá dựa trên prompt của codex trên chuyển đổi text-to-SQL và cho thấy kết quả cải thiện. DocPrompting [203] sử dụng prompting để thu thập kiến thức từ tài liệu và API thay đổi (của các hàm thư viện) để tạo mã. Các mô hình ngôn ngữ dựa trên tăng cường truy xuất này tốt cho việc kết hợp kiến thức từ tài liệu và API thay đổi liên tục. Asking Clarification Questions for Code Generation in General-Purpose Programming Language [105] sử dụng câu hỏi làm rõ (CQ) để giải quyết sự mơ hồ trong việc hiểu ý định của người dùng bằng cách sử dụng LLM như một bộ tạo CQ và bộ tạo mã. Mô hình này khá thú vị vì nó có thể đặt câu hỏi cho người dùng khi mô tả tự nhiên mơ hồ. Một mô hình quan trọng khác cho việc tạo mã là việc chia nhỏ một vấn đề lớn thành các vấn đề con và tạo chương trình để giải quyết từng vấn đề con riêng biệt. Điều này có liên quan chặt chẽ đến cách con người tạo chương trình. Parsel: A (De-)compositional Framework for Algorithmic Reasoning with Language Models [197] chuyển đổi mô tả tự nhiên thành các hàm parsel (vấn đề con) với các ràng buộc từ đầu vào của con người, sau đó họ giải quyết các vấn đề con này với các triển khai thực tế sử dụng mô hình ngôn ngữ (Codex) và cũng sử dụng solver để thỏa mãn các ràng buộc. Sẽ thực sự thú vị nếu mô hình có thể phân tách một vấn đề thành các vấn đề con. Một phương pháp cố gắng ý tưởng này là Self-planning Code Generation with Large Language Model [86]. Nó sử dụng LLM cho self-planning. LLM tạo kế hoạch để giải quyết một nhiệm vụ sử dụng prompt engineering và giải quyết vấn đề theo cách từng bước. Một mô hình liên quan chặt chẽ khác cho việc tạo mã là tạo sketch. Chúng ta có thể nghĩ về sketch như một hướng dẫn cấp cao để giải quyết việc tạo chương trình. Ý tưởng này cũng được lấy cảm hứng từ cách con người viết mã nguồn. Chúng ta đầu tiên tạo một sketch thô và sau đó điền vào các chi tiết nhỏ. Coarse-to-Fine Decoding for Neural Semantic Parsing [44] đầu tiên giải mã sketch của chương trình (với các token thô) và thứ hai giải mã các chi tiết cấp thấp như tên biến dựa trên đầu vào và sketch. Một ý tưởng tương tự đã được sử dụng trong [130]. SKCODER: A Sketch-based Approach for Automatic Code Generation [106] kết hợp ý tưởng của mô hình tăng cường truy xuất với tạo dựa trên sketch. Cho một mô tả NL, nó chọn một đoạn mã tương tự từ corpus truy xuất, dựa trên mô tả NL, nó sử dụng một sketcher để trích xuất sketch mã từ mã tương tự, và sử dụng một editor để chỉnh sửa sketch dựa trên mô tả NL và có được mã đích. Gần đây, các mô hình dựa trên transformer này cũng đã cho thấy hiệu suất ấn tượng của chúng trong việc giải quyết các vấn đề toán học bằng cách đóng khung chúng như một vấn đề tổng hợp chương trình từ mô tả tự nhiên [167]. Hơn nữa, tổng hợp chương trình dựa trên LLM đang được sử dụng để tạo các chương trình có thể hoạt động trên hình ảnh và ngôn ngữ tự nhiên để tạo ra câu trả lời. Cơ chế này cung cấp framework neuro-symbolic cho các giải pháp cho các vấn đề trong thị giác máy tính và xử lý ngôn ngữ tự nhiên. Ví dụ, PAL: Program-aided Language Models [58] sử dụng việc tạo mã như một bước trung gian để giải quyết lý luận symbolic và số học. Code as Policies: Language Model Programs for Embodied Control [181] sử dụng việc tạo mã như một bước trung gian có thể được thực thi như một policy cho việc thao tác robot. Binding Language Models in Symbolic Languages [34] nhận đầu vào, tạo chương trình (chương trình binder: như python với pandas) sử dụng mô hình ngôn ngữ (Codex) sau đó có thể được thực thi bởi trình thông dịch binder (SQL + python + mô hình ngôn ngữ) để tạo ra câu trả lời cho các nhiệm vụ trả lời câu hỏi ngôn ngữ tự nhiên. Hiệu suất của các LLM này tăng với việc tăng kích thước mô hình của chúng [10,30], nhưng kích thước lớn làm cho chúng tốn kém để huấn luyện và đánh giá.

4.1.3 RL

: Các mô hình tạo mã dựa trên RL có thể sử dụng các tín hiệu từ unit test hoặc compiler để cải thiện hệ thống tạo mã không giống như các mô hình dựa trên RNN và LLM trước đó (ALPHA CODE sử dụng unit test để lọc các mẫu nhưng không học từ phản hồi của nó). Ví dụ, CODERL [100] sử dụng framework actor-critic [94,163] nơi actor (LLM [179]) tạo mã và critic (transformer) tạo tín hiệu phản hồi cho actor. Tương tự, Execution-based Code Generation using Deep Reinforcement Learning [152] thực thi mã được tạo và sử dụng kết quả từ việc thực thi như reward để cập nhật mô hình cùng với phạt KL divergence để giảm ghi nhớ, điểm số khớp AST và DFG cho kiến thức cú pháp và ngữ nghĩa. REPL: Write, Execute, Assess: Program Synthesis with a REPL [48] sử dụng mô hình học dựa trên RL để sử dụng các tín hiệu từ REPL (read-eval-print-loop) để tạo mã nguồn (chương trình đồ họa) từ hình ảnh. Bài báo này dựa trên ý tưởng của execution-guided program synthesis [32,207] nơi ý tưởng là sử dụng các trạng thái thực thi của chương trình con hiện tại để tạo chương trình tốt hơn. Đây là mô hình mạnh mẽ có thể được kết hợp với bất kỳ kiến trúc encoder-decoder nào [32]. Bài báo này sử dụng mạng policy (π) để chọn một hành động (quy tắc sản xuất từ ngữ pháp: quy tắc thêm circle/thêm rectangle cho đồ họa 2D) và tạo chương trình con, REPL để thực thi chương trình con hiện tại và tạo đầu ra hiện tại (render chương trình hiện tại để tạo hình ảnh), và hàm giá trị (v) để đánh giá khả năng chương trình hiện tại sẽ giúp hướng tới mục tiêu cuối cùng (spec: hình ảnh cuối cùng được render). Một phần thú vị khác của bài báo này là framework đánh giá nơi họ tạo chương trình sử dụng sequential monte carlo [46] được hướng dẫn bởi hàm giá trị (v) và cho thấy nó hoạt động tốt hơn beam search. Điều này tương tự như cách con người viết chương trình: viết gì đó, đánh giá nó, và sửa nó cho đến khi nó hoạt động. Hướng nghiên cứu này là hướng nghiên cứu tốt. Tương tự, SEQ2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning [202] sử dụng mạng policy để tạo truy vấn SQL từ mô tả ngôn ngữ tự nhiên và sử dụng các tín hiệu từ việc thực thi truy vấn được tạo. Framework reinforcement cho phép chúng ta huấn luyện hệ thống nơi chúng ta có thể có nhiều truy vấn SQL tạo ra cùng kết quả sử dụng reward, mà nếu không sẽ bị phạt bởi cross-entropy loss trừ truy vấn ground truth. RLC ORRECTION: Deep reinforcement learning for programming language correction sử dụng framework actor-critic [124] để sửa các lỗi phổ biến trong mã nguồn C. Họ chỉ ra rằng hệ thống có thể được huấn luyện nhanh hơn với expert demonstration và cuối cùng đánh bại DEEPFIX. COMP CODER: Compilable Neural Code Generation with Compiler Feedback [178] sử dụng phản hồi compiler như tín hiệu reward để cải thiện LLM cho việc hoàn thiện và tạo mã nguồn (Python) từ mô tả ngôn ngữ tự nhiên. Do đó, LLM với framework RL là các mô hình khá mạnh mẽ cho việc tạo mã.

4.1.4 NeuroSymbolic

: Các phương pháp neuro-symbolic [59] chứa một module neural và một module symbolic. Mạng nơ-ron tuyệt vời trong việc học từ dữ liệu nhiễu nhưng không thể diễn giải và thiếu khả năng lý luận rõ ràng. Các mô hình symbolic tuyệt vời trong lý luận nhưng khá không hiệu quả trong việc học từ dữ liệu nhiễu. Do đó chúng bổ sung cho nhau để học từ dữ liệu nhiễu với lý luận. Do đó, các phương pháp neuro-symbolic đang trở nên khá phổ biến. Trong phần này, chúng tôi sẽ bao gồm các phương pháp neuro-symbolic cho việc tạo chương trình. Chúng tôi cũng bao gồm các phương pháp học thư viện (các phương pháp xây dựng thư viện đã học) bên trong các phương pháp neuro-symbolic coi thư viện như một module symbolic. Trong bối cảnh tạo mã, có hai ý tưởng chính: một là sử dụng các kỹ thuật tìm kiếm dựa trên classical và một là sử dụng mạng nơ-ron. Các kỹ thuật classical liệt kê trên không gian chương trình như tìm kiếm liệt kê, hoặc thêm một số heuristic và ràng buộc để tăng tốc như các giải pháp dựa trên sketch và các giải pháp dựa trên SMT solver. Nhược điểm chính của các phương pháp này là chúng ta cần thực hiện tìm kiếm tổ hợp trong không gian chương trình rất khó và các mô hình này chỉ hoạt động với DSL (không mở rộng được cho các ngôn ngữ lập trình thực tế như Python và C++). Các phương pháp dựa trên mạng nơ-ron tạo mã nguồn dựa trên việc dự đoán token có khả năng tiếp theo mỗi lần và do đó có thể thất bại nếu một token sai trong chuỗi. Các phương pháp như DeepCoder: Learning to Write Programs [13] sử dụng cả hai kỹ thuật cho việc tạo mã. Nó sử dụng mạng nơ-ron để dự đoán sự hiện diện hoặc vắng mặt của các hàm cấp cao (ví dụ: sort, max, min, reverse) dựa trên các ví dụ đầu vào-đầu ra và để kết quả này hướng dẫn quá trình tìm kiếm của các kỹ thuật quy nạp chương trình classical như giải pháp dựa trên sketch [155] và λ2 [54]. Họ chỉ ra rằng giải pháp của họ cải thiện thời gian tìm kiếm một cách đáng kể. Hạn chế của bài báo là giải pháp đã được thử cho một DSL nhỏ và việc áp dụng ý tưởng tương tự cho việc tạo mã nguồn trong các ngôn ngữ lập trình thực tế như Python, C, và C++ là hướng nghiên cứu thú vị. Library Learning for Neurally-Guided Bayesian Program Induction [50] và Dreamcoder [49] đề xuất thuật toán quy nạp chương trình học DSL (trừu tượng cao hơn) trong khi đồng thời huấn luyện mạng nơ-ron để dự đoán thuộc tính chương trình như DeepCoder. Điều này liên quan đến mô hình lập trình nơi lập trình viên xây dựng thư viện của các subroutine có thể tái sử dụng được chia sẻ qua các nhiệm vụ lập trình liên quan và có thể được kết hợp để tạo ra các subroutine ngày càng phức tạp và mạnh mẽ. Hệ thống trích xuất các subroutine DSL mới từ cấu trúc chung được tìm thấy qua các syntax tree của các chương trình được tạo giải quyết tập hợp nhiệm vụ đã cho. Các phương pháp này là hướng hứa hẹn cho việc tạo mã giống con người nhưng khả năng áp dụng của chúng cho việc tạo mã nguồn trong các ngôn ngữ lập trình thực tế như C, C++, và Python vẫn chưa được thấy vì không gian tìm kiếm của các ngôn ngữ lập trình thực tế này cực kỳ lớn so với không gian tìm kiếm được định nghĩa bởi DSL mà các phương pháp này sử dụng.

4.2 ASTSequence

Các bài báo này đầu tiên biểu diễn chương trình dưới dạng AST, nhưng tại thời điểm dự đoán, họ tuyến tính hóa AST thành một chuỗi và đưa ra dự đoán. Ưu điểm của hệ thống như vậy là hệ thống tuyến tính hóa dễ song song hóa và biểu diễn AST mang thông tin cú pháp rõ ràng giúp mô hình học ngữ pháp của ngôn ngữ dễ dàng. Nhược điểm của phương pháp này là chúng ta cần giới thiệu các token mới (ví dụ: dấu ngoặc) cho việc tuyến tính hóa AST làm tăng các phụ thuộc tầm xa trong dự đoán. Do đó, nếu chúng ta thiếu một token tương ứng, đầu ra không thể được chuyển đổi thành AST bằng mã tự động và do đó chúng ta không thể tạo mã nguồn. Nhưng các ràng buộc này có thể được thỏa mãn trong quá trình giải mã. Ví dụ, POINTER MIXTURE: Code completion with neural attention and pointer networks [107] sử dụng framework này cho việc hoàn thiện mã trong ngôn ngữ typed động như Python. Họ đề xuất mạng Pointer mixture nơi mô hình có thể tạo token mới từ từ vựng đầu ra bằng LSTM và attention [12], và cũng có thể sao chép token từ mã một phần bằng cách sử dụng pointer network [175]. Họ sử dụng duyệt theo chiều sâu in-order để làm phẳng biểu diễn AST của mã nguồn. Họ cũng đề xuất parent attention cho việc hoàn thiện mã dựa trên AST nơi ý tưởng là nút cha nên có mức độ liên quan cao cho nút con so với các nút khác. Việc biểu diễn mã nguồn bằng AST và để mô hình dự đoán AST thay vì mã nguồn làm cho mô hình dễ hội tụ hơn vì cú pháp rõ ràng. Mô hình cũng học các biểu diễn có thể được chia sẻ qua nhiều ngôn ngữ [8,7] so với mô hình dự đoán mã nguồn trực tiếp. Nhưng, việc chuyển đổi mã nguồn thành AST và làm phẳng chúng làm tăng kích thước và giới thiệu các phụ thuộc tầm xa giữa các token [198]. Các AST sâu hơn cũng làm yếu khả năng của các mô hình nắm bắt ngữ nghĩa phức tạp [206]. Giải pháp cho vấn đề này là tăng cường AST với cấu trúc data flow và control flow [5]. Một giải pháp khác sẽ là phân tách các AST lớn thành chuỗi các cây statement nhỏ [198]. Các phương pháp sử dụng AST để đưa cấu trúc cú pháp vào mô hình và data flow graph để đưa nhận thức ngữ nghĩa vào mô hình đang tăng lên. Ngoài ra, việc so sánh các mô hình dựa trên nhiều biểu diễn đầu ra và tác động của biểu diễn đầu ra đối với việc tạo mã là hướng nghiên cứu quan trọng.

4.3 Graph

Các phương pháp này tạo ra biểu diễn đồ thị (AST) của mã nguồn. Ưu điểm của các phương pháp như vậy là AST nắm bắt cú pháp của ngôn ngữ đã cho một cách rõ ràng. Nhược điểm chính của các hệ thống như vậy là không dễ dàng song song hóa các hệ thống như vậy và chúng ta cần một module bổ sung khác tạo ra mã nguồn từ biểu diễn AST.

4.3.1 BinaryTree

Các phương pháp này đầu tiên mã hóa cây AST Nary thành cây nhị phân sử dụng thuật toán được định nghĩa trước như thuật toán left child right sibling và tạo ra cây nhị phân. Nhược điểm của ý tưởng này là chúng ta lại cần một module khác chuyển đổi từ cây nhị phân sang cây AST Nary. Việc chuyển đổi từ cây AST sang cây nhị phân cũng tăng phụ thuộc tầm xa giữa các token và có thể làm cho việc huấn luyện khó khăn hơn. Neural Code Completion [114] chuyển đổi mã nguồn thành AST. Sau đó họ chuyển đổi AST thành cây nhị phân sử dụng thuật toán left-child right-sibling. Họ sử dụng LSTM để dự đoán nút tiếp theo cho việc hoàn thiện mã. TREE2TREE [33] dịch các chương trình được viết trong một ngôn ngữ (Java/Coffeescript) sang ngôn ngữ khác (C#/Javascript). Họ tạo ra AST của ngôn ngữ đích cho AST của ngôn ngữ đầu vào sử dụng tree decoder. Họ sử dụng tree-lstm [166] để mã hóa biểu diễn AST đầu vào của mã nguồn. Khi decoder mở rộng một non-terminal, nó định vị sub-tree tương ứng trong source tree sử dụng cơ chế attention và sử dụng thông tin của subtree để hướng dẫn việc mở rộng non-terminal. Họ cũng sử dụng ý tưởng parent attention feeding (nếu nút đích t phụ thuộc vào nút nguồn s, có khả năng con của t phụ thuộc vào con của s) để cải thiện kết quả dự đoán. CODA: An End-to-End Neural Program Decompiler [57] tạo ra biểu diễn AST của mã nguồn từ assembly đầu vào. Họ sử dụng instruction type aware encoder (RNN riêng biệt cho các loại statement instruction khác nhau: memory, arithmetic, và branch operation) để mã hóa assembly đầu vào. Họ tạo ra cây AST nhị phân bằng cách chuyển đổi AST gốc thành cây nhị phân sử dụng biểu diễn left child right sibling và sử dụng AST tree decoder với cơ chế attention để giải mã tương tự như TREE2TREE. Tree decoder bao gồm hai LSTM, một cho dự đoán left-child và một cho dự đoán right-child.

4.3.2 NaryTree

Các phương pháp này trực tiếp dự đoán cây AST Nary. Ưu điểm của ý tưởng này là chúng ta không cần module khác chuyển đổi từ cây nhị phân sang cây AST Nary so với việc tạo cây nhị phân. Nhược điểm của các phương pháp như vậy là không dễ dàng song song hóa các mô hình này so với các mô hình chuỗi đến chuỗi. Trong phần sau, chúng tôi chia các phương pháp dự đoán cây AST Nary dựa trên các phương pháp họ sử dụng như RNN, transformer, GNN, và neuro-symbolic.

[RNN và CNN]: SNM: A syntactic neural model for general-purpose code generation [192] tạo ra cây cú pháp trừu tượng và sau đó sử dụng chúng để tạo mã ngôn ngữ lập trình cấp cao như Python từ mô tả ngôn ngữ tự nhiên của mã nguồn. Họ sử dụng mô hình encoder-decoder nơi encoder là mạng LSTM đơn giản mã hóa mô tả của mã. Decoder tạo ra AST từ đầu vào đã mã hóa và sử dụng ngữ pháp của AST. Tại mỗi bước của việc tạo, nó chọn một quy tắc sản xuất (function call, if-statement) và thêm nó vào AST một phần nếu bước tạo là non-terminal. Nếu bước tạo tương ứng với terminal, nó tạo ra tên biến và giá trị sử dụng cơ chế sao chép bằng pointer network [175]. Họ cũng sử dụng parent feeding trong khi tạo subtree (truyền embedding của parent đã tạo children và sử dụng đó cho dự đoán). LCPC: Mapping Language to Code in Programmatic Context [82] sử dụng kiến trúc encoder-decoder để chuyển đổi mô tả ngôn ngữ tự nhiên của tên phương thức để tạo mã cho phương thức đó. Bài báo này chọn quy tắc sử dụng cơ chế attention và sử dụng các quy tắc đó để giải mã cấu trúc chương trình (ATS). Họ cũng sử dụng cơ chế sao chép có giám sát từ việc sử dụng CopyNet [67]. ASNS: Abstract Syntax Networks for Code Generation and Semantic Parsing [135] đề xuất abstract syntax network (ASN) tạo ra cây cú pháp trừu tượng mã nguồn [AST] (Python) theo cách top-down từ mô tả tự nhiên được trích xuất từ các trò chơi thẻ bài như HearthStone. Họ sử dụng cơ chế encoder-decoder nơi encoder mã hóa ngôn ngữ tự nhiên [được trích xuất từ hình ảnh] và decoder xây dựng AST cho mã nguồn theo cách top-down. Họ sử dụng LSTM dọc và ngang với attention để chuyển thông tin theo hướng top-down và horizontal tương ứng. Họ sử dụng các module khác nhau tương ứng với construct trong ngữ pháp. Ưu điểm là mạng luôn tôn trọng ngữ pháp của ngôn ngữ được tạo. Mạng đầu tiên dự đoán module và module tương ứng thực hiện việc mở rộng. Họ cũng sử dụng supervised loss cho việc căn chỉnh (attention) của các token đích với một số token đầu vào thay vì sử dụng attention như bước hậu xử lý cho cơ chế sao chép. Program Synthesis and Semantic Parsing with Learned Code Idioms [151] khai thác code idiom (đoạn mã đại diện cho trừu tượng cấp cao hơn: tổng của hai số) từ tập dữ liệu bằng cách sử dụng AST của chúng và nhìn vào các mẫu lặp lại. Trong quá trình tổng hợp chương trình, nó tạo ra AST: nhưng cho mỗi việc tạo nút, nó có thể tạo ra một token hoặc một code idiom (subgraph của AST). Việc khai thác code idiom này là ý tưởng quan trọng để xây dựng subroutine có thể được sử dụng qua nhiều nhiệm vụ cho việc tạo mã nguồn. CNNDECODER A grammar-based structural CNN decoder for code generation [160] sử dụng mạng nơ-ron tích chập (CNN) [102] thay vì RNN để dự đoán các quy tắc ngữ pháp theo cách tương tự như ASNS. Họ chỉ ra rằng CNN có thể xử lý các phụ thuộc tầm xa tốt hơn RNN.

[Transformer và LLM]: SLM: structural language models [9] sử dụng cây cú pháp trừu tượng (AST) cho việc hoàn thiện mã. Cho mã nguồn của chương trình với một số dòng bị bỏ trống, mô hình có thể dự đoán mã bị bỏ trống. Ý tưởng chính là học đồng thời encoder và decoder. Cho cả nguồn và đích đều là mã, họ học đồng thời xác suất của AST của chương trình. Xác suất của một AST được tính như xác suất có điều kiện của mỗi nút cho các nút khác đã quan sát cho đến nay. Để thu thập thông tin của mỗi nút để dự đoán nút tiếp theo sẽ là gì, họ sử dụng đường dẫn từ gốc đến nút đã cho và đường dẫn từ mọi lá terminal đến nút đã cho (được mã hóa bằng LSTM). Sau đó họ sử dụng transformer [173] để tổng hợp thông tin từ nhiều đường dẫn. Biểu diễn mã nguồn dựa trên AST Path đã khá mạnh mẽ cho biểu diễn mã nguồn [8,7]. TREEGEN: A Tree-Based Transformer Architecture for Code Generation [161] sử dụng transformer [173] để tạo ra biểu diễn AST của mã nguồn bằng cách đưa ra dự đoán về các quy tắc ngữ pháp từ giải thích tự nhiên. Ưu điểm của phương pháp này là transformer có thể xử lý các phụ thuộc tầm xa tốt so với RNN và việc dự đoán về quy tắc ngữ pháp đảm bảo rằng mã được tạo đúng về mặt cú pháp.

[GNN]: GMG: Generative Code Modeling with Graphs [24] cố gắng thực hiện việc hoàn thiện mã cho một số mã bối cảnh sử dụng mạng nơ-ron đồ thị. Ý tưởng chính là sử dụng AST để biểu diễn mã một phần đã cho và tăng cường nó bằng cách sử dụng một số loại cạnh giữa các nút trong AST một phần như parent of edge, child of edge, next sibling edge, v.v. (tăng cường data flow và control flow). Sau đó họ áp dụng mạng nơ-ron đồ thị để có được biểu diễn của chương trình một phần. Trong quá trình mỗi lần mở rộng của nút, họ thực hiện phân loại trên các quy tắc ngữ pháp và chọn một trong các quy tắc ngữ pháp có thể sao cho các subtree được tạo luôn đúng về mặt cú pháp.

[NeuroSymbolic:] SEMFIX: Semantic Code Repair using Neuro-Symbolic Transformation Networks [40] phát triển hệ thống để dự đoán vị trí của các lỗi ngữ nghĩa đơn giản (biến không chính xác, toán tử so sánh không chính xác, thiếu toán tử not, thiếu self) cùng với việc sửa chữa thực tế mà không sử dụng unit test cho Python. Họ chuyển đổi mã nguồn thành AST, nhúng nút AST với thông tin như dạng chuỗi của nút, vị trí của nút, mối quan hệ với parent, và loại của nút, và mã hóa AST sử dụng LSTM hai chiều. Sau đó họ sử dụng module khác để chọn các quy tắc ứng viên sửa chữa (ví dụ: thay thế == bằng != tại nút n). Họ sử dụng MLP để chọn các quy tắc ứng viên sửa chữa vì chúng cố định và pointer network [175] để thay thế biến. Hạn chế của bài báo này là các nhà phát triển cần viết các quy tắc ứng viên sửa chữa này. Nghiên cứu về việc sử dụng các phương pháp neuro-symbolic cho việc tạo mã còn hạn chế và là hướng nghiên cứu quan trọng.

5 Cơ chế sao chép

Cơ chế sao chép: hệ thống sao chép các token từ đầu vào sang đầu ra khá hữu ích cho việc tạo mã. Ví dụ, chúng có thể cực kỳ hữu ích cho dịch ngược thần kinh nơi chúng ta cần sao chép các giá trị từ assembly đầu vào sang mã đầu ra. Chúng cũng quan trọng như nhau cho việc tạo mã từ ngôn ngữ tự nhiên, dịch chương trình, và nhiều hơn nữa. Có nhiều giải pháp cho vấn đề này. PTRNET: Pointer Networks [175] đề xuất ý tưởng sử dụng attention như một con trỏ để chọn một thành viên của chuỗi đầu vào làm token đầu ra. Điều này cho phép mô hình chọn một phần của đầu vào tại thời điểm tạo. Ý tưởng này đã khá mạnh mẽ và nhiều bài báo trong dịch máy và tạo mã sử dụng ý tưởng này hoặc các biến thể của nó để sao chép một số token từ chuỗi đầu vào. Nó cũng cho phép chúng ta giới hạn từ vựng đầu ra trong quá trình tạo mã vì chúng ta có thể sao chép một số token từ đầu vào trực tiếp (ngay cả khi chúng là token ngoài từ vựng cho ngôn ngữ đầu ra). PTRGENNET: Get To The Point: Summarization with Pointer-Generator Networks [148] sử dụng mạng chuỗi đến chuỗi cho việc tóm tắt văn bản trừu tượng với cách tiếp cận lai của việc tạo token và sao chép giá trị từ đầu vào sử dụng pointer network. Họ cũng sử dụng một thuật ngữ phạt trong hàm loss phạt attention lặp lại vào cùng vị trí. COPYNET: Incorporating Copying Mechanism in Sequence-to-Sequence Learning [67]. Nó giới thiệu cơ chế sao chép giá trị từ chuỗi đầu vào sang chuỗi đầu ra trong các kịch bản học chuỗi đến chuỗi. Phương pháp của họ quyết định khi nào sao chép từ đầu vào hoặc khi nào dự đoán giá trị đích sử dụng hai hàm. Hàm tính attention giữa trạng thái ẩn hiện tại của decoder đến mọi từ trong từ vựng và đến mọi từ trong chuỗi đầu vào và quyết định sao chép hoặc tạo token đích. Hơn nữa, hệ thống có thể được huấn luyện theo cách end-to-end từ dữ liệu. Do đó cơ chế sao chép là một phần quan trọng của các hệ thống dịch máy thần kinh cho việc tạo mã. Nhưng, trong một số trường hợp, việc sao chép trực tiếp từ chuỗi đầu vào có thể không phải là lựa chọn tốt vì token đầu ra phụ thuộc vào một số hàm của token đầu vào. Ví dụ, khi chúng ta cố gắng tạo mã nguồn từ chuỗi assembly nơi phép nhân với hai trong mã nguồn được triển khai bằng shift operation, việc sao chép trực tiếp có thể cho chúng ta kết quả sai. Trong những trường hợp như vậy, việc bổ sung cơ chế sao chép với các hàm đơn giản như multi-layer perceptron có thể cho kết quả tốt hơn.

6 Học biểu diễn và pre-training

Việc học một cách tốt để biểu diễn mã nguồn hoặc văn bản là một phần quan trọng của việc tạo mã nguồn. Các embedding được học từ các kỹ thuật này có thể được sử dụng cho các nhiệm vụ downstream khác nhau như tạo mã, tóm tắt mã, và hoàn thiện mã. Trong phần này, chúng tôi tóm tắt các ý tưởng khác nhau có thể được sử dụng cho học biểu diễn và pre-training. Công việc ban đầu về biểu diễn mã được lấy cảm hứng từ các công việc trong NLP như WORD2VEC: Efficient Estimation of Word Representations in Vector Space và Distributed Representations of Words and Phrases and their Compositionality [123]. Nó sử dụng giả thuyết phân phối nói rằng các từ xuất hiện trong bối cảnh tương tự có ý nghĩa tương tự. Các chiến lược pre-training khá phổ biến trong NLP và CV như một cách để sử dụng dữ liệu không nhãn để học các biểu diễn. Trong các phần sau, chúng tôi sẽ mô tả một số chiến lược pre-training và học biểu diễn quan trọng cho mã nguồn. Việc tạo mã dựa trên NMT sử dụng mô hình dựa trên encoder-decoder cho việc tạo mã. Dựa trên thành phần (encoder, decoder, cả hai) họ sử dụng để pre-train, các phương pháp này có thể được chia thành các nhóm sau:

6.1 Chỉ encoder:

Các phương pháp này pre-train chỉ encoder. Ví dụ, CUBERT: Learning and Evaluating Contextual Embedding of Source Code [89] sử dụng mục tiêu masked language modeling của BERT [41] cho pre-training tạo mã. CODEBERT: A Pre-Trained Model for Programming and Natural Languages [53] sử dụng masked language modeling cùng với replaced token detection cho việc tạo mã. GRAPH CODEBERT [70] sử dụng thông tin data flow được trích xuất từ mã kết hợp với CODEBERT. DOBF [145] sử dụng pre-training dựa trên deobfuscation để đưa thông tin lĩnh vực ngôn ngữ lập trình. DOBF và GRAPH CODEBERT chỉ tập trung vào encoder đặc thù cho mã. Nhược điểm của các phương pháp này là chúng không sử dụng decoder cho pre-training.

6.2 Chỉ decoder:

Các phương pháp này chỉ pre-train decoder. Ví dụ, INTELLICODE [165] sử dụng GPT cho nhiệm vụ hoàn thiện mã và được huấn luyện cho dự đoán từ tiếp theo. Nhược điểm của các mô hình này là chúng không sử dụng encoder trong giai đoạn pre-training.

6.3 Encoder-Decoder:

Các phương pháp này pre-train cả encoder và decoder cùng lúc. Ví dụ, CODET5 [179] tăng cường mô hình T5 [137] sử dụng denoising sequence to sequence pre-training (làm hỏng đầu vào và để decoder giải mã nó) và thêm identifier tagging, masked identifier prediction, masked span prediction, và dual generation để cải thiện hệ thống. Vì mô hình này có thể pre-train cả encoder và decoder cho các nhiệm vụ tương ứng của chúng, phương pháp này có ưu thế so với các phương pháp chỉ encoder và chỉ decoder cho việc tạo mã nguồn [179]. PLBART [3]: huấn luyện với các mô hình denoising sequence to sequence như BART [104] nhưng không đưa thông tin lĩnh vực từ lĩnh vực ngôn ngữ lập trình. LANG AGNOSTIC [208] đề xuất nắm bắt khoảng cách tương đối giữa các token mã trên mã.

Dựa trên loại thông tin mà các phương pháp này cố gắng nắm bắt, chúng có thể được chia thành các nhóm sau:

6.4 dựa trên văn bản

Các mô hình học biểu diễn chỉ văn bản như [179] chỉ nắm bắt biểu diễn dựa trên văn bản của mã nguồn. Nhưng, mã nguồn có cấu trúc phong phú hơn có thể được định nghĩa bằng cây cú pháp trừu tượng (AST) và ngữ nghĩa có thể được nắm bắt bằng data flow graph (DFG) và program dependence graph (PDG). Việc học biểu diễn chỉ văn bản buộc mô hình phải học thông tin cú pháp và thông tin ngữ nghĩa này một mình và do đó có thể không phải là cách tối ưu cho việc học biểu diễn của mã nguồn.

6.5 dựa trên cú pháp

Các phương pháp này cố gắng đưa kiến thức cú pháp của mã nguồn vào mục tiêu học biểu diễn của chúng. Ví dụ, CODE2VEC: Learning Distributed Representations of Code [8] và CODE2SEQ: Generating Sequences from Structured Representations of Code [7] tạo ra biểu diễn vector của đoạn mã đã cho bằng cách tạo biểu diễn vector cho các AST path (đường dẫn giữa các nút lá trong AST) và tổng hợp chúng bằng module attention. Các biểu diễn này có thể hữu ích cho các nhiệm vụ như tóm tắt mã, tài liệu, truy xuất, hoàn thiện mã, và nhiều hơn nữa. CODE2SEQ sử dụng LSTM để nhúng đường dẫn thành vector có độ dài cố định trong khi CODE2VEC sử dụng lớp tuyến tính. Language-Agnostic Representation Learning of Source Code from Structure and Context [208] tận dụng bối cảnh (mã nguồn) và cấu trúc (AST) để học biểu diễn tốt của mã nguồn. UniXcoder: Unified Cross-Modal Pre-training for Code Representation [71] đề xuất pre-training đa phương thức sử dụng comment mã và AST tuyến tính hóa cùng với mã nguồn. Nó sử dụng masked language modeling, dự đoán token tiếp theo, mục tiêu denoising cùng với mục tiêu học tương phản đa phương thức (embedding tương tự cho các ví dụ tương tự và khác nhau cho khác nhau), và mục tiêu tạo cross-modal (tạo comment từ AST và ngược lại) như các chiến lược pre-training. Theo cách tương tự, TreeBERT [85] sử dụng framework transformer encoder-decoder và sử dụng thông tin cấu trúc cây bằng cách mô hình hóa các AST path (tree-based masked language modeling và dự đoán thứ tự nút). SYNCOBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation [177] kết hợp cây cú pháp trừu tượng cho pre-training. Nó sử dụng dự đoán Identifier và dự đoán cạnh AST như hai chiến lược pre-training và cố gắng tối đa hóa thông tin tương hỗ giữa các đầu vào đa phương thức như mã, comment, và AST sử dụng học tương phản. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations [129] sử dụng cả văn bản và AST tuyến tính hóa của mã với framework encoder-decoder cho việc học biểu diễn mã.

6.6 dựa trên ngữ nghĩa

Các phương pháp này cố gắng đưa không chỉ thông tin dựa trên văn bản và cú pháp, mà còn thông tin ngữ nghĩa vào các mục tiêu học của chúng. Cho rằng, các phương pháp học biểu diễn dựa trên mã nguồn nắm bắt bối cảnh và các kỹ thuật dựa trên AST nắm bắt cú pháp, các phương pháp lai cố gắng học bối cảnh và cú pháp đang tăng lên. Ví dụ, LRPG: Learning to Represent Programs with Graphs [5] tăng cường AST với các cạnh data flow và control flow. Họ sử dụng mạng nơ-ron đồ thị trên các đồ thị này để học biểu diễn ngữ nghĩa của mã nguồn. Contrastive code representation learning [83] sử dụng học tương phản cho việc học biểu diễn mã nguồn. Ý tưởng là tối đa hóa độ tương tự với các chương trình tương đương và tối thiểu hóa độ tương tự với các chương trình khác nhau về chức năng. Họ tuyên bố rằng nó tốt hơn các mô hình ngôn ngữ có che (MLM) vì chúng (MLM) chỉ học lý luận ngôn ngữ cục bộ có thể không phải là cách tốt nhất để tóm tắt chức năng của chương trình. StructCoder: Structure-Aware Transformer for Code Generation [168] mã hóa thông tin cú pháp và ngữ nghĩa cho encoder và decoder cho việc tạo mã sử dụng dự đoán AST path cho thông tin cú pháp và data flow cho thông tin ngữ nghĩa. Gần đây, Flow2Vec: Value-Flow-Based Precise Code Embedding [158] sử dụng value flow cho việc học biểu diễn mã và cho thấy kết quả ấn tượng.

7 Vấn đề mở và hướng nghiên cứu

Việc tạo mã nguồn là nhiệm vụ đầy thử thách. Có nhiều vấn đề mở và hướng nghiên cứu. Trong phần này, chúng tôi sẽ cố gắng làm nổi bật một số vấn đề mở và hướng nghiên cứu tiềm năng. Mã nguồn có các phụ thuộc dài ở nhiều nơi [101]. Ví dụ, một statement mở file có thể ở dòng 3, trong khi lệnh đóng cùng file đó có thể ở dòng 100. Phụ thuộc rất dài này là vấn đề cho các kỹ thuật hiện có. Đã được chỉ ra rằng hiệu suất của các mô hình encoder-decoder này giảm khi độ dài chuỗi tăng. Do đó việc giải quyết các phụ thuộc tầm xa là hướng nghiên cứu quan trọng cho việc tạo mã nguồn. Hầu hết các phương pháp hiện có hoạt động trên dự đoán token tiếp theo (NTP) và tối ưu hóa likelihood của token tiếp theo cho các token trước đó và đầu vào và điều này dẫn đến tích lũy lỗi [18,140]. Các hệ thống dựa trên NMT hiện tại hoạt động trên dự đoán token tiếp theo theo cách tuần tự. Mặc dù các chiến lược giải mã khác nhau giúp tránh những cạm bẫy này, chúng tốn kém tính toán. Con người không tạo chương trình hoàn chỉnh token by token cho một nhiệm vụ hoàn chỉnh như các phương pháp tạo mã hiện tại [108]. Chúng ta chia nhỏ một vấn đề thành các vấn đề con và test chúng lặp đi lặp lại. Các phương pháp có thể chia nhỏ một vấn đề thành các vấn đề nhỏ, tạo mã cho các chương trình con như vậy, và đánh giá chúng như [48,197] là các hướng nghiên cứu tiềm năng tốt. Hầu hết các hệ thống tạo mã hiện tại không kết hợp các trừu tượng mã được tạo thành các trừu tượng cấp cao hơn như con người làm. Con người giữ một bộ sưu tập các subroutine và kết hợp chúng để thực hiện các nhiệm vụ cấp cao hơn. DREAM CODER: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning [49] thực hiện bước đầu tiên theo hướng này. Hầu hết các phương pháp hoạt động tốt nhất trong việc tạo mã nguồn hiện tại đều dựa trên các mô hình ngôn ngữ lớn. Nhưng, các LLM này yêu cầu rất nhiều dữ liệu huấn luyện và tài nguyên tính toán. Do đó, các giải pháp hiện tại không hiệu quả về mẫu. Việc làm cho việc tạo mã nguồn này hiệu quả về mẫu là hướng nghiên cứu thú vị. Phương pháp được hướng dẫn bởi cú pháp chăm sóc cú pháp nhưng không phải ngữ nghĩa. Đặc biệt, quy trình huấn luyện có giám sát tiêu chuẩn có thể gặp phải program aliasing: cho cùng các ví dụ đầu vào-đầu ra, có nhiều chương trình tương đương về mặt ngữ nghĩa, nhưng tất cả ngoại trừ cái được cung cấp trong dữ liệu huấn luyện sẽ bị phạt như các chương trình sai [32]. Để giảm thiểu điều này [26] đề xuất huấn luyện với học tăng cường để nó thưởng tất cả các chương trình đúng về mặt ngữ nghĩa một khi chúng được tạo ra đầy đủ. Nhưng [32] chỉ ra chúng ta có thể sử dụng execution-guided synthesis cho nhiệm vụ này. Execution-guided synthesis [32,207] hiện tại hoạt động với DSL, nhưng việc mở rộng chúng cho việc tạo mã nguồn thực tế cũng là hướng nghiên cứu thú vị. Con người không viết mọi thứ từ đầu cho việc tạo mã từ mô tả: A Retrieve-and-Edit Framework for Predicting Structured Outputs [74] truy xuất ví dụ gần nhất từ tập dữ liệu huấn luyện và sử dụng mạng editor để dự đoán các chỉnh sửa trên mã được truy xuất để nó thực hiện công việc dự định. Việc sử dụng các ý tưởng của mã và tài liệu đã tồn tại để cải thiện việc tạo mã tốt hơn là hướng thú vị. Việc sử dụng các ý tưởng như Unsupervised Translation of Programming Languages [98] khi chúng ta không có tập dữ liệu được căn chỉnh song song cũng là hướng nghiên cứu thú vị cho việc tạo mã nguồn. Khi ngày càng nhiều bài báo sử dụng phản hồi từ unit test để cải thiện các mô hình tạo mã, đáng để khám phá các bộ tạo unit test tự động dựa trên deep learning như Unit Test Case Generation with Transformers and Focal Context [171]. Điều này cho phép chúng ta cải thiện cả hai mô hình, tức là bộ tạo mã và bộ tạo unit test đồng thời. Hầu hết các hệ thống hiện có hoạt động trên các chương trình nhỏ với các hàm đơn lẻ và việc mở rộng sang chương trình với nhiều hàm là hướng nghiên cứu đầy thử thách và thú vị. Module tạo mã có thể chỉ ghi nhớ tập dữ liệu huấn luyện dẫn đến các vấn đề bảo mật và bản quyền: WhyGen: Explaining ML-powered Code Generation by Referring to Training Examples [190] sử dụng fingerprint để tìm ví dụ huấn luyện gần nhất. Nhưng giải pháp có thể mở rộng cho huấn luyện quy mô internet vẫn là vấn đề nghiên cứu mở. Reinforcement learning from human feedback (RLHF) đã khá phổ biến trong việc fine-tune các mô hình ngôn ngữ lớn với phản hồi từ con người. Việc áp dụng các ý tưởng tương tự để cải thiện việc tạo mã là hướng nghiên cứu quan trọng. Gần đây, Improving Code Generation by Training with Natural Language Feedback [29] cho thấy kết quả ấn tượng ban đầu trong hướng nghiên cứu này.

8 Tập dữ liệu

Trong phần này, chúng tôi sẽ giới thiệu một số tập dữ liệu cho các nhiệm vụ tạo mã. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation: [115][https://github.com/microsoft/CodeXGLUE] bao gồm 14 tập dữ liệu cho 10 nhiệm vụ code intelligence đa dạng bao gồm dịch code-to-code, code-to-text, text-to-code, và text-to-text (tài liệu). Tập dữ liệu bao gồm nhiều ngôn ngữ như C, C++, Java, Python, và nhiều hơn nữa. CodeXGLUE cũng bao gồm tám tập dữ liệu được đề xuất trước đó — BigCloneBench [164], POJ-104 [125], Devign [205], PY150 [141], Github Java Corpus [4], Bugs2Fix [170], CONCODE [82], và CodeSearchNet [81]. APPS: Measuring Coding Challenge Competence With APPS [75][https://github.com/hendrycks/apps] bao gồm tập dữ liệu cho việc tạo mã Python từ giải thích ngôn ngữ tự nhiên. SPoC: Search-based Pseudocode to Code [96][https://github.com/Sumith1896/spoc] là tập dữ liệu cho việc tạo mã C++ từ pseudocode. Concode: Mapping Language to Code in a Programmatic Context [82][https://github.com/sriniiyer/concode] là tập dữ liệu cho việc tạo mã Java từ docstring. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search [81][https://github.com/github/CodeSearchNet] bao gồm tập dữ liệu cho việc truy xuất mã từ ngôn ngữ tự nhiên cho nhiều ngôn ngữ như Go, Java, JavaScript, PHP, Python, và Ruby. Nó cũng có thể được sử dụng cho các nhiệm vụ tạo mã. A parallel corpus of Python functions and documentation strings for automated code documentation and code generation [14][https://github.com/EdinburghNLP/code-docstring-corpus] bao gồm tập dữ liệu cho việc tạo mã trong Python từ docstring và ngược lại. StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow [191][https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset] bao gồm tập dữ liệu cho các cặp question-to-code cho python và SQL được khai thác từ stack-overflow. CoNaLa: The Code/Natural Language Challenge [193][https://conala-corpus.github.io/] bao gồm tập dữ liệu cho việc tạo mã python từ ngôn ngữ tự nhiên. HumanEval: Hand-Written Evaluation Set [30][https://github.com/openai/human-eval] bao gồm tập dữ liệu cho việc tạo chương trình Python viết tay từ function signature, docstring, và body với trung bình 7.7 unit test cho mỗi chương trình. CodeContests: competitive programming dataset [108][https://github.com/deepmind/code_contests] bao gồm tập dữ liệu CodeNet cũng như tập dữ liệu cấp độ thi đấu ALPHA CODE cho việc tạo mã Python và C++ từ ngôn ngữ tự nhiên với unit test. MBPP (mostly basic programming problems) dataset [10][https://huggingface.co/datasets/mbpp] bao gồm tập dữ liệu cho việc tạo mã Python từ giải thích ngôn ngữ tự nhiên cùng với test case. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task [195][https://yale-lily.github.io/spider] bao gồm tập dữ liệu cho việc tạo mã SQL từ ngôn ngữ tự nhiên. NL2Bash: Generating bash command from natural language [111][https://github.com/TellinaTool/nl2bash] bao gồm tập dữ liệu cho việc tạo script bash từ giải thích ngôn ngữ tự nhiên. POLYCODER: A Systematic evaluation of large language models of code [https://github.com/VHellendoorn/Code-LMs] bao gồm tập dữ liệu cho việc tạo mã trong 12 ngôn ngữ lập trình từ giải thích ngôn ngữ tự nhiên. Pix2code: Generating Code from a Graphical User Interface Screenshot [17][https://github.com/tonybeltramelli/pix2code] bao gồm tập dữ liệu cho việc tạo mã (Android, IOS, Web Technologies) từ giao diện người dùng đồ họa. WikiSQL: Generating Structured Queries from Natural Language using Reinforcement Learning [202][https://github.com/salesforce/WikiSQL] bao gồm tập dữ liệu cho việc tạo truy vấn SQL từ giải thích tự nhiên. MultiPL-E: A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages [27][https://github.com/nuprl/multipl-e] bao gồm tập dữ liệu cho việc tạo mã nguồn trong 18 ngôn ngữ lập trình từ giải thích ngôn ngữ tự nhiên. CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks [134][https://github.com/IBM/Project_CodeNet] bao gồm tập dữ liệu cho nhiều nhiệm vụ như tạo mã nguồn, dịch mã, v.v. trong 55 ngôn ngữ lập trình khác nhau. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation [99][https://ds1000-code-gen.github.io/] bao gồm tập dữ liệu cho các vấn đề tạo mã trong data science từ mô tả tự nhiên. The Stack: 3 TB of permissively licensed source code [93][https://huggingface.co/datasets/bigcode/the-stack] là tập dữ liệu 3.1 TB bao gồm mã nguồn được cấp phép permissively trong 30 ngôn ngữ lập trình. CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models [194] bao gồm tập dữ liệu cho việc tạo mã từ ngôn ngữ tự nhiên được thu thập từ các dự án open-source thực tế. Multi-Turn Programming Benchmark (MTPB) [127] bao gồm tập dữ liệu cho việc tạo mã hội thoại nơi người dùng chỉ định các nhiệm vụ con và mô hình hoàn thành chúng.

9 Đánh giá

Trong phần này, chúng tôi sẽ giới thiệu các metric phổ biến hiện đang được sử dụng để đánh giá mã được tạo.

• BLEU Score: BLEU score [132] và exact match accuracy. Exact match accuracy là phần mẫu test mà mô hình dự đoán toàn bộ chuỗi một cách chính xác. BLEU score đã là tiêu chuẩn để đánh giá dịch máy so với dịch của con người trong xử lý ngôn ngữ tự nhiên. BLEU score được định nghĩa như sau:

BLEU =BP.exp∑^N_{n=1} w_n log p_n

Trong đó, w_n là vô hướng sao cho ∑^N_{n=1} w_n = 1, p_n là precision được sửa đổi cho n-gram được định nghĩa như sau (ŷ là dự đoán):

p_n = (∑_{n−gram∈ŷ} Count_{clip}(n−gram)) / (∑_{n−gram∈ŷ} Count(n−gram))

Trong đó, Count(n−gram) là số lượng n-gram trong dự đoán, và Count_{clip}(n−gram) là số lượng các n-gram này trong dự đoán có mặt trong câu đích nhưng được clip bởi tần suất tối đa của n-gram đó trong câu đích. BP là brevity penalty phạt các bản dịch ngắn và được định nghĩa như sau:

BP = 1, if c > r
     e^{(1−r/c)}, otherwise

Trong đó c là độ dài của dự đoán và r là độ dài của đích ground truth (tham chiếu). Điều này cho chúng ta một giá trị giữa [0, 1] cho mỗi bản dịch. Thường thì N được đặt thành 4, và w_n được đặt thành 1/N.

Vấn đề với bleu score là nó không thể đo lường tính chính xác chức năng của các chương trình và không thể nắm bắt các đặc trưng ngữ nghĩa đặc thù cho mã: [100,75,30,143]. Ngoài ra, [30] cho thấy một số ví dụ nơi các mã tương đương về chức năng có điểm bleu thấp hơn các mã không tương đương về chức năng. Điều này có thể được giải thích bởi thực tế rằng các chương trình giống hệt về mặt ngữ nghĩa có thể có sự chồng chéo n-gram rất thấp; ví dụ, vì việc đổi tên identifier [10,75,30]. Mặc dù BLEU score đã được sử dụng trong nhiều bài báo để đánh giá, BLEU score không phải là metric tốt để đánh giá mã nguồn được tạo. Does BLEU Score Work for Code Migration [169] chỉ ra rằng BLEU không phải là thước đo tốt cho đánh giá mã và đề xuất phương pháp gọi là RUBY dựa trên string edit distance (text: độ tương tự mã), tree edit distance (AST: độ tương tự cú pháp) và graph edit distance (PDG: độ tương tự ngữ nghĩa).

• CodeBLEU: A Method for Automatic Evaluation of Code Synthesis [143]: sử dụng tổ hợp có trọng số của BLEU (4-gram), syntactic matching (khớp subtree trong AST), và semantic matching (sử dụng cấu trúc dataflow). Họ chứng minh điều này có mối tương quan tốt hơn với đánh giá của con người so với BLEU.

CodeBLEU = α∗BLEU + β∗BLEU_{weight} + γ∗Match_{ast} + δ∗Match_{df} (12)

Trong đó BLEU là BLEU score tiêu chuẩn [132], BLEU_weight là weighted n-gram match (tầm quan trọng cao cho từ khóa), Match_ast là syntactic AST match, và Match_df là semantic data flow match [143] Mặc dù codeBLEU nắm bắt nhiều khía cạnh của mã so với BLEU score, nó có thể đo lường tính chính xác chức năng của mã. Do đó, codeBLEU một mình không phải là thước đo tốt để đánh giá mã nguồn được tạo.

• Exact Match (EM): Exact match so sánh toàn bộ chuỗi với ground truth. Cho rằng, các chương trình không có nhiều sự chồng chéo có thể tạo ra cùng kết quả, EM là thước đo khắt khe để đánh giá mã nguồn. Các metric dựa trên exact match không thể tính đến không gian lớn và phức tạp của các hàm chương trình tương đương với giải pháp tham chiếu [30]

• Edit distance: Khoảng cách Levenschtein (chi phí tối thiểu của chuỗi các phép toán chỉnh sửa chuỗi để chuyển đổi chuỗi được tạo thành chuỗi ground truth) trong trường hợp chuỗi mã nguồn và graph edit distance [1] (chi phí tối thiểu của chuỗi các phép toán chỉnh sửa nút và cạnh để chuyển đổi đồ thị được tạo thành đồ thị ground truth) đã được sử dụng trong tài liệu để đánh giá việc tạo mã dựa trên AST. Như đã lưu ý trong các metric trên, nó gặp phải các vấn đề tương tự và không thể đo lường tính chính xác chức năng của chương trình.

• Unit test: Unit test là tập hợp các điều kiện cần được thỏa mãn bởi mã được tạo. Metric này được hỗ trợ bởi mô hình phát triển test-driven nơi các nhà phát triển đầu tiên viết unit test trước khi viết phần mềm [108]. Cho một số lượng đủ unit test bao gồm tất cả các đường dẫn thực thi của mã tham chiếu, chúng ta có thể xác minh rằng hai chương trình tương đương về chức năng. Nhưng, các chương trình test hạn chế có thể sai lầm tuyên bố rằng chương trình đúng khi nó không đúng. Do đó, unit test đầy đủ là phương pháp thực sự tốt để đánh giá mã nguồn được tạo. Một vấn đề với unit test là mã được tạo có thể độc hại. Trong trường hợp như vậy, tốt hơn là chạy mã được tạo trong môi trường sandbox.

• pass@k metric [96,30]: Metric này được sử dụng với unit test, nơi ý tưởng là tạo k mẫu cho mỗi chương trình và vấn đề được giải quyết nếu bất kỳ một trong k giải pháp nào vượt qua unit test. Nhưng cái này có phương sai cao [30], và do đó một sửa đổi được đề xuất bởi [30] để tạo n > k giải pháp và đếm các giải pháp vượt qua unit test là c. Khi đó pass@k được định nghĩa là:

pass@k = E_{problems}[1 − \binom{n−c}{k} / \binom{n}{k}] (13)

cũng như, đánh giá dựa trên timeout để ước tính độ phức tạp thuật toán của giải pháp được tạo [108] đang được sử dụng kết hợp với metric pass@k.

• CodeBERTScore và CodeScore: Evaluating Code Generation With Pretrained Models of Code [204] và CodeScore: Evaluating Code Generation by Learning Code Execution [45] đề xuất sử dụng LLM để đánh giá mã được tạo tương tự như BERTScore [200] được sử dụng để đánh giá ngôn ngữ tự nhiên được tạo. Họ tuyên bố rằng metric này có mối tương quan cao hơn với sở thích của con người và với tính chính xác chức năng hơn tất cả các metric đánh giá mã hiện có. Nó hoạt động bằng cách tính độ tương tự của mã được tạo và mã tham chiếu trong không gian embedding nơi các embedding được tạo bằng cách sử dụng mô hình đã được pre-train gọi là CodeBERT [53].

10 Kết luận

Các kiến trúc dựa trên dịch máy thần kinh đang trở nên khá phổ biến cho việc tạo nguồn từ các đầu vào khác nhau. Việc tạo mã dựa trên NMT hữu ích trong nhiều lĩnh vực như tạo mã từ giải thích tự nhiên, tạo mã từ nhị phân hoặc assembly đầu vào (decompilation), dịch code-to-code, sửa chữa mã, sửa lỗi, và nhiều hơn nữa. Trong bài báo khảo sát này, chúng tôi bao gồm các kỹ thuật mới nhất được sử dụng cho việc tạo mã nguồn từ nhiều loại đầu vào. Chúng tôi cũng làm nổi bật các kỹ thuật quan trọng hữu ích cho việc tạo mã nguồn cùng với việc xác định các thách thức hiện tại và hướng nghiên cứu tiềm năng. Chúng tôi trình bày các phương pháp đánh giá phổ biến với ưu điểm và nhược điểm của chúng. Chúng tôi nghĩ các bài báo khảo sát của chúng tôi đặt nền tảng cho các nhà nghiên cứu mới bắt đầu làm việc trong lĩnh vực này với tập hợp kỹ thuật hiện tại tổng thể và thúc đẩy các nhà nghiên cứu có kinh nghiệm phát triển các giải pháp mới có thể giải quyết các thách thức được xác định trong bài báo và nhiều hơn nữa.

[Tài liệu tham khảo sẽ được dịch tiếp theo nếu cần]
