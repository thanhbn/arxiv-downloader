# Hỗn hợp Chuyên gia Định tuyến Ngôn ngữ cho Nhận dạng Giọng nói Đa ngôn ngữ và Chuyển đổi Mã

Wenxuan Wang, Guodong Ma, Yuke Li∗, Binbin Du
NetEase Yidun AI Lab, Hangzhou, Trung Quốc
{wangwenxuan, maguodong, liyuke, dubinbin }@corp.netease.com

## Tóm tắt
Nhận dạng giọng nói đa ngôn ngữ cho cả giọng nói đơn ngôn ngữ và chuyển đổi mã là một nhiệm vụ đầy thách thức. Gần đây, dựa trên Hỗn hợp Chuyên gia (MoE), nhiều nghiên cứu đã đạt được tiến bộ tốt trong ASR đa ngôn ngữ và chuyển đổi mã, nhưng có độ phức tạp tính toán lớn khi số lượng ngôn ngữ được hỗ trợ tăng lên. Trong nghiên cứu này, chúng tôi đề xuất một mạng hiệu quả về mặt tính toán có tên là Hỗn hợp Chuyên gia Định tuyến Ngôn ngữ (LR-MoE) cho ASR đa ngôn ngữ và chuyển đổi mã. LR-MoE trích xuất các biểu diễn đặc thù ngôn ngữ thông qua Hỗn hợp Chuyên gia Ngôn ngữ (MLE), được hướng dẫn học bởi cơ chế định tuyến ngôn ngữ theo khung. Mạng nhận dạng ngôn ngữ cấp khung (LID) chia sẻ trọng số được huấn luyện chung như bộ định tuyến trước được chia sẻ của mỗi lớp MoE. Các thí nghiệm cho thấy phương pháp đề xuất cải thiện đáng kể hiệu suất nhận dạng giọng nói đa ngôn ngữ và chuyển đổi mã so với cơ sở với hiệu quả tính toán tương đương.

**Từ khóa chỉ mục**: hỗn hợp chuyên gia, nhận dạng ngôn ngữ, đa ngôn ngữ, chuyển đổi mã, nhận dạng giọng nói

## 1. Giới thiệu
Đa ngôn ngữ là một hiện tượng phổ biến trên thế giới. Những người nói đa ngôn ngữ thường giao tiếp bằng nhiều ngôn ngữ đồng thời, chẳng hạn như xen lẫn tiếng Anh trong tiếng Quan Thoại. Do đó, một hệ thống nhận dạng giọng nói đa ngôn ngữ thực tế cần hỗ trợ nhận dạng các phát ngôn đơn ngôn ngữ và chuyển đổi mã trong nhiều ngôn ngữ.

Các hệ thống ASR từ đầu đến cuối (E2E) [1–9] đã trở nên ngày càng phổ biến gần đây do quy trình đơn giản, hiệu suất xuất sắc và ít phụ thuộc vào kiến thức ngôn ngữ học so với các phương pháp kết hợp truyền thống [10]. Các nghiên cứu trước đây dựa trên mô hình E2E cũng đã đạt được tiến bộ tốt trong lĩnh vực ASR đa ngôn ngữ, bao gồm tổng hợp tập dữ liệu chuyển đổi mã [11–13], huấn luyện đa nhiệm vụ với nhận dạng ngôn ngữ kết hợp [14, 15], học biểu diễn giọng nói tự giám sát [16–18], học chuyển giao đa ngôn ngữ [15, 19], v.v. Kiến trúc MoE là một phương pháp hiệu quả để cải thiện hiệu suất nhận dạng giọng nói đa ngôn ngữ trong cả tình huống đơn ngôn ngữ và chuyển đổi mã, được quan tâm rộng rãi và nghiên cứu gần đây. Các phương pháp dựa trên MoE hiện tại [20–26] trích xuất các biểu diễn đặc thù ngôn ngữ riêng biệt bằng các bộ mã hóa độc lập và kết hợp chúng để giải mã. Phần lớn, độ phức tạp tính toán của các mô hình sẽ tăng đáng kể theo số lượng ngôn ngữ được hỗ trợ.

Trong nghiên cứu này, chúng tôi đề xuất một mạng hiệu quả về mặt tính toán có tên là Hỗn hợp Chuyên gia Định tuyến Ngôn ngữ (LR-MoE) để cải thiện hiệu suất của nhiệm vụ ASR đa ngôn ngữ và chuyển đổi mã. Kiến trúc LR-MoE bao gồm một khối chia sẻ và một khối Hỗn hợp-Chuyên gia-Ngôn ngữ (MLE). Không giống như hỗn hợp chuyên gia có cổng thưa thớt (sMoE) [27–30], các lớp chuyên gia trong khối MLE phụ thuộc vào ngôn ngữ, được gọi là Chuyên gia Đặc thù Ngôn ngữ (LSE). Khối chia sẻ tạo ra biểu diễn toàn cục, trong khi LSE của khối MLE trích xuất các biểu diễn đặc thù ngôn ngữ. Trong khối MLE, chúng tôi thiết kế cơ chế Định tuyến Ngôn ngữ theo Khung (FLR), hướng dẫn các lớp chuyên gia học chuyên môn hóa ngôn ngữ ở giai đoạn huấn luyện. Một mạng nhận dạng ngôn ngữ cấp khung (LID) chia sẻ trọng số được huấn luyện chung như bộ định tuyến trước được chia sẻ của mỗi lớp LSE, và việc căn chỉnh LID theo khung sẽ được sử dụng làm đường dẫn định tuyến của các lớp LSE. Chúng tôi cũng so sánh định tuyến ngôn ngữ theo phát ngôn và theo khung cho LR-MoE trong thí nghiệm đa ngôn ngữ và chuyển đổi mã. Để phân biệt chúng, chúng tôi sẽ đặt tên hai mạng với định tuyến khác nhau là ULR-MoE và FLR-MoE, tương ứng. Các đóng góp của chúng tôi được tóm tắt như sau:

• Chúng tôi đề xuất kiến trúc LR-MoE hiệu quả về mặt tính toán, phù hợp để áp dụng trong nhiều ngôn ngữ hơn với ít tăng độ phức tạp tính toán.

• Chúng tôi nghiên cứu nhiều chiến lược định tuyến của MoE và đề xuất cơ chế FLR để hướng dẫn các lớp chuyên gia học chuyên môn hóa ngôn ngữ, tương thích với cả ASR đa ngôn ngữ đơn lẻ và chuyển đổi mã.

• Trong các thí nghiệm chuyển đổi mã Quan Thoại-Tiếng Anh và đa ngôn ngữ, phương pháp đề xuất cải thiện đáng kể hiệu suất nhận dạng giọng nói đa ngôn ngữ và chuyển đổi mã so với cơ sở với hiệu quả tính toán tương đương và vượt trội hơn các phương pháp dựa trên MoE trước đây với độ phức tạp tính toán ít hơn.

## 2. Các Nghiên cứu Liên quan và Động lực

### 2.1. Các nghiên cứu dựa trên MoE trước đây
Gần đây, nhiều nghiên cứu [20–24] tập trung vào khám phá các kiến trúc MoE để nhận dạng giọng nói đơn ngôn ngữ và chuyển đổi mã trong câu. Các phương pháp dựa trên MoE chủ yếu sử dụng các bộ mã hóa chuyên gia đặc thù ngôn ngữ để tạo ra các biểu diễn đặc thù ngôn ngữ song song và kết hợp chúng, với sự khác biệt chính yếu trong chế độ kết hợp các bộ mã hóa chuyên gia và chiến lược huấn luyện. Ví dụ, mạng transformer bộ mã hóa kép [21] sử dụng mạng có cổng để xuất động các hệ số nội suy MoE để trộn hai biểu diễn mã hóa. Trọng số của các bộ mã hóa chuyên gia được khởi tạo với mô hình đơn ngôn ngữ được huấn luyện trước, tương ứng. Bộ chuyển đổi thần kinh có thừa số có điều kiện [22] định nghĩa các nhiệm vụ con đơn ngôn ngữ với đồng bộ hóa nhãn-khung để đạt được mô hình hóa thống nhất của ASR chuyển đổi mã và đơn ngôn ngữ. Bộ mã hóa nhận biết ngôn ngữ [23,24] học các biểu diễn đặc thù ngôn ngữ thông qua huấn luyện nhận biết ngôn ngữ với tổn thất phụ trợ đặc thù ngôn ngữ thay vì huấn luyện trước đơn ngôn ngữ và sử dụng phép cộng theo khung để kết hợp chúng.

### 2.2. Động lực
Như đã đề cập ở trên, các nghiên cứu dựa trên MoE trước đây đã đạt được cải thiện đáng kể trong ASR đơn ngôn ngữ và chuyển đổi mã, nhưng vẫn còn những vấn đề sau:

• Phương pháp cần tính toán tất cả các khối đặc thù ngôn ngữ. Tuy nhiên, chỉ một khối hoạt động trong tình huống đơn ngôn ngữ. Điều này có nghĩa là một lượng lớn chi phí tính toán dư thừa. Và càng nhiều ngôn ngữ được hỗ trợ, càng nhiều chi phí tính toán dư thừa.

• Các khối đặc thù ngôn ngữ bị cô lập với nhau và thiếu tương tác. Kết quả là, thông tin ngữ cảnh đa ngôn ngữ dễ bị mất trong giọng nói chuyển đổi mã.

Để giảm thiểu hai vấn đề trên, chúng tôi đề xuất kiến trúc LR-MoE được lấy cảm hứng từ hỗn hợp chuyên gia có cổng thưa thớt [27–30]. Vui lòng tham khảo Phần 3 để biết thêm chi tiết.

[Hình 1: Sơ đồ nguyên lý của các mô-đun MoE. (a) Hỗn hợp Chuyên gia Có cổng Thưa thớt (sMoE), (b) Hỗn hợp Chuyên gia Ngôn ngữ (MLE).]

## 3. Phương pháp Đề xuất

### 3.1. Hỗn hợp Chuyên gia Có cổng Thưa thớt
Mô-đun sMoE được hiển thị trong Hình 1(a). Như một đại diện, Switch Transformer [28] áp dụng chiến lược định tuyến chuyên gia top-1 trong kiến trúc MoE để định tuyến các mẫu dữ liệu đến mô hình chuyên gia có xác suất cao nhất trong mạng có cổng. Độ phức tạp tính toán của toàn bộ mạng tăng nhẹ khi số lượng chuyên gia tăng, và chi phí tính toán thêm chỉ đến từ mạng có cổng. Đầu vào của lớp chuyên gia và mạng có cổng là đầu ra của lớp không phải chuyên gia trước đó. Xác suất định tuyến p có thể được biểu diễn như sau:

p = Softmax(Wr · one + br)                (1)

trong đó Wr và br là trọng số và bias của bộ định tuyến tương ứng. Một tổn thất phụ trợ được thêm vào để đảm bảo cân bằng tải giữa các chuyên gia trong quá trình huấn luyện. Tổn thất cân bằng được biểu diễn:

Lb = n · Σ(i=1 to n) fi · pi              (2)

trong đó fi là phần mẫu được gửi đến chuyên gia thứ i, n là số lượng chuyên gia.

### 3.2. Kiến trúc của LR-MoE
Để tăng cường tương tác thông tin ngữ cảnh đa ngôn ngữ, chúng tôi mở rộng thêm các phần chia sẻ, chẳng hạn như các lớp chú ý trong mạng Transformer [31]. Điều này khác với bộ mã hóa đặc thù ngôn ngữ riêng biệt [20–23]. Hình 2 hiển thị mô hình Transformer dựa trên LR-MoE. Khối chia sẻ được xếp chồng bởi các khối transformer tiêu chuẩn. Trái ngược với khối transformer tiêu chuẩn, chúng tôi giới thiệu mô-đun MLE FFN như được hiển thị trong Hình 1(b) để tăng cường các biểu diễn đặc thù ngôn ngữ trong khối MLE. Tất cả các mô-đun MLE chia sẻ cùng một bộ định tuyến ngôn ngữ, là một mạng có cổng LID cấp khung ở phía trước. Theo ngôn ngữ top-1 được dự đoán bởi FLR, các mẫu dữ liệu được định tuyến đến lớp LSE tương ứng. Đối với mỗi khung thời gian, chỉ một LSE sẽ được định tuyến, vì vậy độ phức tạp tính toán của mô hình sẽ không tăng với nhiều ngôn ngữ hơn.

[Hình 2: Cấu trúc của Mô hình Transformer LR-MoE. N và (L−N) là số lượng lớp của khối chia sẻ và khối MLE, tương ứng.]

### 3.3. Định tuyến Ngôn ngữ

#### 3.3.1. Mạng Có cổng LID
Xem xét rằng LID có thể được coi là một nhiệm vụ con chiều thấp của ASR và đầu ra của lớp không phải chuyên gia one đã chứa thông tin ngôn ngữ học chiều cao phong phú, chúng tôi mô hình hóa nhiệm vụ LID cấp khung với một lớp tuyến tính như sau:

r = Wr · one + br                          (3)

Một tổn thất phụ trợ LID cấp khung được thêm vào để huấn luyện chung các nhiệm vụ LID và ASR ở giai đoạn huấn luyện. Chúng tôi lấy các nhãn phụ trợ Ylid bằng cách thay thế các token trong nhãn văn bản bằng ID ngôn ngữ tương ứng. Sau đó, dựa trên r, tổn thất LID-CTC được áp dụng để có được căn chỉnh token-khung, như được hiển thị trong Eq. (4).

Llid−ctc = −log PCTC(Ylid|r)              (4)

Do tính chất đinh thưa của Connectionist Temporal Classification (CTC), kết quả giải mã tham lam của đầu ra LID zt sẽ chứa một lượng lớn blank. Do đó, chúng tôi áp dụng một chiến lược căn chỉnh đơn giản để có được thông tin định tuyến ngôn ngữ cấp khung dày đặc như sau:

zt = {
  zf,      nếu t = 0
  zt,      nếu zt ≠ φ
  zt−1,    nếu zt = φ
}                                          (5)

trong đó zt ∈ {language ids} ∪ φ, t = 0,1,2, ..., T, zf là phần tử không blank đầu tiên.

Ngoài ra, chúng tôi cũng sử dụng mạng có cổng LID theo phát ngôn với tổn thất cross entropy (CE) để so sánh. Tổn thất LID theo phát ngôn như sau:

Llid−utt = CE(ru, Ulid)                    (6)

trong đó ru là pooling trung bình toàn cục theo chiều thời gian của r, Ulid là ID ngôn ngữ của phát ngôn.

Hàm tổn thất mục tiêu cuối cùng được hiển thị trong Eq. (7):

Lmtl = Lasr + λlid Llid                    (7)

trong đó λlid được chọn thủ công, Llid của ULR và FLR tương ứng với Llid−utt và Llid−ctc.

#### 3.3.2. Bộ Định tuyến Chia sẻ
Không giống như sMoE [27–30], chúng tôi sử dụng bộ định tuyến chia sẻ thay vì bộ định tuyến độc lập cho mỗi lớp MLE, chủ yếu do những cân nhắc sau: Bộ định tuyến độc lập của mỗi lớp MoE trong sMoE hữu ích trong việc có được các đường dẫn định tuyến đa dạng hơn và khả năng mô hình lớn hơn. Tuy nhiên, các lớp chuyên gia là đặc thù ngôn ngữ và các đường dẫn định tuyến mong muốn được xác định với tiền nghiệm trong LR-MoE. Do đó, bộ định tuyến LID chia sẻ có thể hữu ích để giảm tính toán bổ sung và tích lũy lỗi đa cấp do trôi căn chỉnh của định tuyến ngôn ngữ.

### 3.4. Khối Chia sẻ Được Huấn luyện Trước
Các đặc trưng bottleneck của ASR có hiệu quả trong học chuyển giao cho LID [32]. Do đó, chúng tôi sử dụng khối chia sẻ được huấn luyện trước để tăng tốc độ hội tụ của mạng có cổng LID và giảm lan truyền gradient xấu do các đường dẫn định tuyến sai lệch, đặc biệt ở giai đoạn huấn luyện đầu.

## 4. Thí nghiệm

### 4.1. Tập dữ liệu
Các thí nghiệm của chúng tôi được thực hiện trên tập dữ liệu ASRU 2019 Mandarin-English code-switching Challenge [33] và tập dữ liệu bốn ngôn ngữ bao gồm Aishell-1 (CN) [34], tập con train-clean-100 của Librispeech [35] (EN), Japanese (JA), Zeroth-Korean (KR) và dữ liệu chuyển đổi mã Mandarin-English (CN-EN). JA và CN-EN được thu thập từ Datatang. Bảng 1 và 2 hiển thị chi tiết của tất cả các tập dữ liệu thí nghiệm.

[Bảng 1: Chi tiết của Tập dữ liệu Chuyển đổi Mã Mandarin-English]
[Bảng 2: Chi tiết của Tập dữ liệu Đa ngôn ngữ và Chuyển đổi Mã]

Đối với tất cả các thí nghiệm, các đặc trưng âm thanh là năng lượng filter-bank log 80 chiều được trích xuất với kích thước bước 10ms và kích thước cửa sổ 25ms. SpecAugment [36] được áp dụng trong tất cả các giai đoạn huấn luyện. Từ vựng Mandarin-English và từ vựng của 4 ngôn ngữ bao gồm 12064 và 15492 ký tự duy nhất và token BPE [37].

### 4.2. Thiết lập thí nghiệm
Các thí nghiệm được thực hiện trên toolkit ESPnet [38]. Chúng tôi sử dụng mô hình Transformer CTC/Attention với bộ mã hóa 12 lớp và bộ giải mã 6 lớp làm cơ sở, được gọi là mô hình Vallina. Bộ mã hóa LR-MoE bao gồm khối chia sẻ 6 lớp và khối MLE 6 lớp trong các thí nghiệm. Chúng tôi cũng so sánh các bộ mã hóa dựa trên MoE khác nhau với phương pháp đề xuất của chúng tôi, bao gồm Bi-Encoder [21], LAE [24]. Bi-Encoder chứa bộ mã hóa 12 lớp cho mỗi ngôn ngữ. LAE chứa khối chia sẻ 9 lớp và khối đặc thù ngôn ngữ 3 lớp cho mỗi ngôn ngữ. Ngoài ra, chúng tôi triển khai sMoE 12 lớp [30] với 4 chuyên gia trong mỗi khối MoE để so sánh trong thí nghiệm đa ngôn ngữ và chuyển đổi mã. Tất cả các bộ mã hóa và giải mã được xếp chồng các khối dựa trên transformer với chiều chú ý 256, 4 đầu chú ý và chiều feed-forward 2048. Chúng tôi triển khai học đa nhiệm vụ với λctc = 0.3 và λlid = 0.3 cho ASR và LID ở giai đoạn huấn luyện của mô hình LR-MoE. Chúng tôi sử dụng trình tối ưu Adam với tỉ lệ transformer-lr là 1 và các bước khởi động 25k để huấn luyện 100 epoch trên 8 GPU Tesla V100. Quá trình huấn luyện áp dụng chiến lược kích thước batch động với kích thước batch tối đa 128. Chúng tôi huấn luyện mô hình ngôn ngữ 4-gram với tất cả các bản ghi huấn luyện và áp dụng tìm kiếm beam tiền tố CTC cho bộ giải mã với kích thước beam cố định 10.

### 4.3. Kết quả Thí nghiệm

#### 4.3.1. Kết quả trên ASR Mandarin-English
[Bảng 3: Hiệu suất của các mô hình trong hệ thống ASR Mandarin-English dựa trên CTC và AED]

Như được hiển thị trong Bảng 3, phương pháp đề xuất của chúng tôi vượt trội hơn các phương pháp dựa trên MoE trước đây với số tham số tương đương hoặc ít hơn trong hệ thống ASR Mandarin-English, bao gồm các mô hình CTC và Attention-based Encoder-Decoder (AED). Phương pháp đề xuất đạt được cải thiện hiệu suất đáng kể so với cơ sở. Các cải thiện tương đối trên các tập đánh giá mono-Mandarin, mono-English và code-switch là 28.2%, 18.5% và 13.9% trong mô hình dựa trên CTC và 25.4%, 17.9% và 13.4% trong mô hình dựa trên AED, tương ứng.

#### 4.3.2. Kết quả trên ASR đa ngôn ngữ
[Bảng 4: Hiệu suất của các mô hình trong hệ thống ASR đa ngôn ngữ dựa trên CTC]

Bảng 4 hiển thị kết quả của các mô hình trong hệ thống ASR đa ngôn ngữ dựa trên CTC. So với các phương pháp dựa trên MoE trước đây, phương pháp đề xuất của chúng tôi đạt được cải thiện hiệu suất đáng kể trong cả tình huống đơn ngôn ngữ và chuyển đổi mã. Về FLOPs, độ phức tạp tính toán của kiến trúc đề xuất tăng ít với sự gia tăng của các ngôn ngữ, và dễ dàng mở rộng trong ASR đa ngôn ngữ.

Phương pháp đề xuất cải thiện đáng kể hiệu suất so với cơ sở với độ phức tạp tính toán tương đương. Các cải thiện tương đối trung bình trên các tập đánh giá đơn ngôn ngữ và chuyển đổi mã là 28.4% và 26.8%, tương ứng.

Chúng tôi cũng so sánh các chiến lược định tuyến khác nhau của LR-MoE. Các thí nghiệm cho thấy FLR-MoE đạt được cải thiện hiệu suất nhẹ so với ULR-MoE, đặc biệt trong các tình huống chuyển đổi mã. Chiến lược định tuyến ngôn ngữ chia sẻ cho FLR-MoE vượt trội hơn chiến lược định tuyến ngôn ngữ theo lớp trên cả tập đánh giá đơn ngôn ngữ và chuyển đổi mã.

### 4.4. Nghiên cứu Ablation và phân tích

#### 4.4.1. Vị trí của MLE
[Bảng 5: Nghiên cứu ablation về vị trí của các lớp MLE FFN]

Như được hiển thị trong Bảng 5, chúng tôi khám phá ảnh hưởng của vị trí các lớp MLE đối với hiệu suất. Theo phân tích của chúng tôi, khối chia sẻ càng sâu thì LID càng chính xác và biểu diễn đặc thù ngôn ngữ càng yếu trong mô hình FLR-MoE có độ sâu cố định, đây là sự đánh đổi của thiết kế mô hình. Các thí nghiệm cho thấy phương pháp đề xuất có khả năng mạnh để phân biệt ngôn ngữ, và chúng tôi đạt được kết quả tốt nhất với bộ định tuyến ngôn ngữ ở vị trí giữa.

#### 4.4.2. Phân tích LID và Định tuyến
Chúng tôi tóm tắt kết quả phân loại ngôn ngữ cấp phát ngôn dựa trên thông tin định tuyến LID cấp khung và kết quả của ASR. Độ chính xác phân loại ngôn ngữ và ma trận nhầm lẫn của phương pháp đề xuất được hiển thị trong Hình 3. So với độ chính xác LID trung bình 98.8% của cơ sở, độ chính xác LID trung bình của phương pháp đề xuất là 99.4%. Điều này cho thấy phương pháp đề xuất có thể hiệu quả giảm sự nhầm lẫn giữa các ngôn ngữ.

Như được hiển thị trong Hình 4, đối với đầu vào chuyển đổi mã, phương pháp đề xuất có được định tuyến của chuyên gia ngôn ngữ bằng căn chỉnh LID token-khung và định tuyến các đầu vào lớp của các phân đoạn ngôn ngữ đến các chuyên gia ngôn ngữ tương ứng, điều này chứng minh tính hiệu quả của phương pháp đề xuất cho ASR chuyển đổi mã.

[Hình 3: Ma trận nhầm lẫn ngôn ngữ trên các tập đánh giá đa ngôn ngữ. trái: Cơ sở, phải: FLR-MoE.]

[Hình 4: Trực quan hóa phân phối ASR-CTC, LID-CTC và định tuyến ngôn ngữ cho giọng nói chuyển đổi mã Mandarin-English.]

## 5. Kết luận
Bài báo này đề xuất kiến trúc LR-MoE để cải thiện hệ thống ASR đa ngôn ngữ cho các tình huống đơn ngôn ngữ và chuyển đổi mã. Dựa trên cơ chế định tuyến ngôn ngữ theo khung (FLR), LR-MoE đề xuất có thể chuyển đổi khối chuyên gia ngôn ngữ tương ứng để trích xuất các biểu diễn đặc thù ngôn ngữ một cách thích ứng và hiệu quả. Các thí nghiệm cho thấy LR-MoE cải thiện đáng kể hệ thống ASR đa ngôn ngữ và chuyển đổi mã so với mô hình Transformer tiêu chuẩn với độ phức tạp tính toán tương đương và vượt trội hơn các phương pháp dựa trên MoE trước đây với độ phức tạp tính toán ít hơn. Trong tương lai, chúng tôi sẽ khám phá các cơ chế định tuyến MoE hiệu quả hơn cho nhận dạng giọng nói đa ngôn ngữ và chuyển đổi mã.
