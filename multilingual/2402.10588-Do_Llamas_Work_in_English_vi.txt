# 2402.10588.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2402.10588.pdf
# Kích thước tệp: 3583624 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Liệu Llama Có Hoạt Động Bằng Tiếng Anh?
Về Ngôn Ngữ Tiềm Ẩn của Các Transformer Đa Ngôn Ngữ
Chris Wendler*, Veniamin Veselovsky*, Giovanni Monea*, Robert West*
EPFL
{chris.wendler, veniamin.veselovsky, giovanni.monea, robert.west}@epfl.ch
Tóm tắt
Chúng tôi đặt câu hỏi liệu các mô hình ngôn ngữ đa ngôn ngữ được huấn luyện trên corpus không cân bằng, với sự thống trị của tiếng Anh, có sử dụng tiếng Anh làm ngôn ngữ trục nội bộ hay không - một câu hỏi có tầm quan trọng then chốt để hiểu cách các mô hình ngôn ngữ hoạt động và nguồn gốc của thiên lệch ngôn ngữ. Tập trung vào họ mô hình transformer Llama-2, nghiên cứu của chúng tôi sử dụng các prompt không phải tiếng Anh được xây dựng cẩn thận với một phần tiếp theo đúng duy nhất là single-token. Từ lớp này đến lớp khác, các transformer dần dần ánh xạ một embedding đầu vào của token prompt cuối cùng thành một embedding đầu ra từ đó các xác suất token tiếp theo được tính toán. Theo dõi các embedding trung gian qua không gian nhiều chiều của chúng tiết lộ ba giai đoạn riêng biệt, trong đó các embedding trung gian (1) bắt đầu xa các embedding token đầu ra; (2) đã cho phép giải mã một token tiếp theo đúng về mặt ngữ nghĩa ở các lớp giữa, nhưng cho xác suất cao hơn cho phiên bản tiếng Anh của nó so với ngôn ngữ đầu vào; (3) cuối cùng di chuyển vào một vùng cụ thể theo ngôn ngữ đầu vào của không gian embedding. Chúng tôi đưa các kết quả này vào một mô hình khái niệm trong đó ba giai đoạn hoạt động lần lượt trong "không gian đầu vào", "không gian khái niệm" và "không gian đầu ra". Quan trọng là, bằng chứng của chúng tôi cho thấy rằng "không gian khái niệm" trừu tượng nằm gần tiếng Anh hơn so với các ngôn ngữ khác, điều này có thể có hệ quả quan trọng liên quan đến các thiên lệch được giữ bởi các mô hình ngôn ngữ đa ngôn ngữ. Mã nguồn và dữ liệu được cung cấp tại đây: https://github.com/epfl-dlab/llm-latent-language .

1 Giới thiệu
Hầu hết các mô hình ngôn ngữ lớn (LLM) hiện đại được huấn luyện trên các corpus khổng lồ chủ yếu là văn bản tiếng Anh (Touvron et al., 2023; OpenAI, 2023). Mặc dù vậy, chúng đạt được hiệu suất mạnh mẽ trên một loạt các tác vụ downstream, thậm chí trong các ngôn ngữ không phải tiếng Anh (Shi et al., 2022). Điều này đặt ra một câu hỏi hấp dẫn: Làm thế nào các LLM có thể tổng quát hóa tốt từ dữ liệu huấn luyện chủ yếu là tiếng Anh sang các ngôn ngữ khác?

Một cách trực quan, một phương pháp để đạt được hiệu suất mạnh mẽ trên dữ liệu không phải tiếng Anh một cách hiệu quả về dữ liệu là sử dụng tiếng Anh làm ngôn ngữ trục, bằng cách đầu tiên dịch đầu vào sang tiếng Anh, xử lý nó bằng tiếng Anh, và sau đó dịch câu trả lời trở lại ngôn ngữ đầu vào. Phương pháp này đã được chứng minh là dẫn đến hiệu suất cao khi được thực hiện một cách rõ ràng (Shi et al., 2022; Ahuja et al., 2023; Huang et al., 2023). Câu hỏi hướng dẫn của chúng tôi trong công việc này là liệu việc chuyển sang tiếng Anh cũng xảy ra ngầm khi các LLM được prompt bằng ngôn ngữ không phải tiếng Anh.

Trong cộng đồng nghiên cứu cũng như báo chí đại chúng, nhiều người dường như giả định rằng câu trả lời là có, được thể hiện bằng những tuyên bố như "Máy móc, có thể nói, suy nghĩ bằng tiếng Anh và dịch cuộc trò chuyện vào phút cuối sang tiếng Estonia" (Piir, 2023). Trong công việc này, chúng tôi đặt mục tiêu vượt qua những suy đoán như vậy và nghiên cứu câu hỏi một cách thực nghiệm.

Câu hỏi có tầm quan trọng lớn. Một mặt, việc ngầm sử dụng tiếng Anh làm trục nội bộ có thể làm thiên lệch các LLM theo các mẫu hình Anglocentric có thể khiến mô hình có khuynh hướng đối với các yếu tố ngôn ngữ nhất định (từ vựng, ngữ pháp, ẩn dụ, v.v.), đồng thời cũng định hình các hành vi sâu sắc hơn liên quan đến, ví dụ, lập trường cảm xúc (Boroditsky et al., 2003) hoặc lý luận thời gian (Núñez and Sweetser, 2006). Mặt khác, nếu các LLM không sử dụng tiếng Anh làm trục, điều này đặt ra câu hỏi về cách khác chúng quản lý để hoạt động tốt đến vậy thậm chí trong các ngôn ngữ ít tài nguyên. Nhìn chung, việc tìm kiếm một ngôn ngữ trục nội bộ có triển vọng thúc đẩy sự hiểu biết của chúng ta về cách các LLM hoạt động bất kể chúng ta có thành công hay không.

Việc nghiên cứu sự tồn tại của một ngôn ngữ LLM nội bộ được phức tạp hóa bởi quy mô và bản chất khó hiểu nổi tiếng của các mạng neural đằng sau các LLM, mà sau lớp đầu vào không hoạt động trên các token rời rạc, mà trên các vector điểm nổi nhiều chiều. Làm thế nào để hiểu liệu những vector đó tương ứng với tiếng Anh, tiếng Estonia, tiếng Trung, v.v. - hoặc không tương ứng với ngôn ngữ nào cả - là một vấn đề mở, và câu hỏi liệu các LLM có sử dụng ngôn ngữ trục nội bộ hay không do đó, theo hiểu biết tốt nhất của chúng tôi, chưa được giải quyết thực nghiệm trước đây.

Tóm tắt đóng góp. Để vượt qua những rào cản này, chúng tôi dựa vào và đóng góp vào lĩnh vực mới nổi về khả năng diễn giải cơ học (cf. Sec. 2). Trong một transformer, vector embedding của mỗi token đầu vào được biến đổi dần dần từ lớp này sang lớp khác mà không thay đổi hình dạng của nó. Sau lớp cuối cùng, một thao tác "unembedding" biến vector thành một phân phối token tiếp theo. Tập trung vào họ mô hình Llama-2 (Touvron et al., 2023) - trong số các LLM mã nguồn mở lớn nhất hiện nay - chúng tôi phát hiện rằng việc áp dụng thao tác "unembedding" một cách sớm trong các lớp trung gian, không phải lớp cuối - một kỹ thuật được gọi là logit lens (Nostalgebraist, 2020) - đã giải mã một token phù hợp ngữ cảnh từ sớm (Hình 1), cho chúng ta một cái nhìn thoáng qua (có hạn chế) về trạng thái nội bộ số khó diễn giải của mô hình.

Khai thác sự thật này, chúng tôi cẩn thận thiết kế các prompt cho phép chúng tôi xác định liệu một token được giải mã bằng logit-lens có đúng về mặt ngữ nghĩa hay không và nó thuộc về ngôn ngữ nào (ví dụ, một prompt yêu cầu mô hình dịch từ tiếng Pháp "fleur" ["flower"] sang tiếng Trung "花"; cf. Hình 1). Theo dõi các xác suất ngôn ngữ qua các lớp, chúng tôi quan sát thấy rằng không có token phù hợp ngữ cảnh nào được giải mã trong nửa đầu của các lớp, theo sau bởi một sự chuyển đổi đột ngột của khối lượng xác suất lên phiên bản tiếng Anh ("flower") của token tiếp theo đúng, và cuối cùng là một sự chuyển đổi sang token tiếp theo đúng trong ngôn ngữ đích ("花").

Mở rộng bằng chứng đầu tiên này về tiếng Anh như một ngôn ngữ trục nội bộ, chúng tôi phân tích các embedding tiềm ẩn trực tiếp như các điểm Euclidean nhiều chiều, thay vì qua logit lens. Điều này cho phép chúng tôi vẽ một bức tranh tinh tế hơn về cấu trúc của forward pass của Llama-2, cho thấy rằng, trong các lớp giữa, transformer hoạt động trong một "không gian khái niệm" trừu tượng mà một phần trực giao với "không gian token" cụ thể theo ngôn ngữ, chỉ được đạt đến trong các lớp cuối cùng. Trong cách diễn giải này, sự gần gũi của các embedding tiềm ẩn với các token tiếng Anh được quan sát thông qua logit lens là do một thiên lệch tiếng Anh trong không gian khái niệm, chrather than từ việc mô hình đầu tiên dịch sang tiếng Anh và sau đó "khởi động lại" forward pass từ đó.

Chúng tôi kết luận bằng cách thảo luận về các tác động và hướng tương lai để nghiên cứu các thiên lệch tiềm ẩn và tác động của chúng - một bước quan trọng hướng tới AI đáng tin cậy.

2 Công trình liên quan
Các mô hình ngôn ngữ đa ngôn ngữ. Các mô hình ngôn ngữ đa ngôn ngữ (LM) được huấn luyện để đồng thời xử lý nhiều ngôn ngữ đầu vào. Các ví dụ bao gồm mBERT (Devlin et al., 2018), mBART (Liu et al., 2020), XLM-R (Conneau et al., 2020a), mT5 (Xue et al., 2021), XGLM (Lin et al., 2022), mGPT (Shliazhko et al., 2022), BLOOM (Scao et al., 2022), và PolyLM (Wei et al., 2023). Các mô hình hàng đầu hiện tại như GPT-4, PaLM, và Llama-2, mặc dù hoạt động tốt hơn bằng tiếng Anh do dữ liệu huấn luyện Anglocentric của chúng (Huang et al., 2023; Bang et al., 2023; Zhang et al., 2023), vẫn hoạt động tốt qua các ngôn ngữ (Shi et al., 2022).

Các nhà nghiên cứu đã đưa ra nhiều phương pháp để chuyển giao hiệu quả các khả năng LM qua các ngôn ngữ, ví dụ, bằng cách căn chỉnh các embedding ngữ cảnh (Schuster et al., 2019; Cao et al., 2020), học lại các ma trận embedding trong quá trình tinh chỉnh trên một ngôn ngữ mới (Artetxe et al., 2020), hoặc làm như vậy một cách lặp lại trong quá trình tiền huấn luyện (Chen et al., 2023).

Một số phương pháp tận dụng tiếng Anh làm ngôn ngữ trục. Ví dụ, Zhu et al. (2023) cho thấy rằng Llama có thể được tăng cường hiệu quả với khả năng tuân theo hướng dẫn đa ngôn ngữ nhờ vào các biểu diễn tiếng Anh của nó. Tương tự, Zhu et al. (2024) chứng minh tính khả thi của việc tận dụng thành thạo tiếng Anh của các mô hình ngôn ngữ cho các ngữ cảnh không phải tiếng Anh bằng cách tinh chỉnh chúng trên dữ liệu dịch thuật và dữ liệu hướng dẫn chỉ tiếng Anh. Họ sử dụng thành công phương pháp này để tăng cường khả năng lý luận đa ngôn ngữ của Llama-2. Về các ngôn ngữ ít tài nguyên không phải Latin, Husain et al. (2024) minh họa rằng việc tận dụng cả dữ liệu La tinh hóa và tiếng Anh chứng tỏ là một chiến lược hiệu quả để cải thiện hiệu suất nhiệm vụ đa ngôn ngữ một cách hiệu quả. Các chiến lược prompting cũng có thể cải thiện hiệu suất đa ngôn ngữ bằng cách tận dụng tiếng Anh làm ngôn ngữ trục, ví dụ, bằng cách đơn giản đầu tiên dịch các prompt sang tiếng Anh (Shi et al., 2022; Ahuja et al., 2023; Etxaniz et al., 2023) hoặc bằng cách hướng dẫn các LM thực hiện lý luận chuỗi suy nghĩ (Wei et al., 2022) bằng tiếng Anh (Huang et al., 2023).

Mặc dù việc sử dụng các ngôn ngữ có nhiều tài nguyên có thể tăng cường hiệu suất trên các ngôn ngữ ít tài nguyên, nhưng nó cũng có thể làm thiên lệch việc tạo ra đầu ra trong các ngôn ngữ ít tài nguyên, ví dụ, về mặt ngữ pháp (Papadimitriou et al., 2022).

Các nhà nghiên cứu cũng đã điều tra cách các biểu diễn tiềm ẩn khác nhau qua các ngôn ngữ trong các mô hình đa ngôn ngữ. Trong trường hợp các mô hình chỉ có encoder như mBERT, bằng chứng hội tụ cho thấy sự tồn tại của một không gian bất khả tri ngôn ngữ trong các lớp sau theo sau các lớp đầu cụ thể theo ngôn ngữ (Libovický et al., 2020; Conneau et al., 2020b; Muller et al., 2021; Choenni and Shutova, 2020).

Khả năng diễn giải cơ học. Lĩnh vực mới nổi về khả năng diễn giải cơ học (MI) nhằm mục đích thiết kế ngược và từ đó hiểu các mạng neural, sử dụng các kỹ thuật như khám phá mạch (Nanda et al., 2023; Conmy et al., 2023), huấn luyện có kiểm soát cụ thể theo nhiệm vụ (Li et al., 2022; Marks and Tegmark, 2023), và truy vết nhân quả (Meng et al., 2022; Monea et al., 2023).

Đối với các mô hình nhỏ hơn, ví dụ, GPT-2 (Radford et al., 2019) và Pythia (Biderman et al., 2023), các phương pháp MI như thăm dò thưa thớt (Gurnee et al., 2023) đã tiết lộ các neuron ngôn ngữ tiếng Pháp (Gurnee et al., 2023) và tiếng Đức (Quirke et al., 2023) đơn nghĩa và các mạch n-gram tiếng Đức phụ thuộc ngữ cảnh (các mạng con để tăng cường xác suất của các n-gram tiếng Đức khi neuron ngữ cảnh tiếng Đức đơn nghĩa đang hoạt động) (Quirke et al., 2023).

Các công cụ liên quan nhất từ kho MI trong bối cảnh công việc này là logit lens (Nostalgebraist, 2020), tuned lens (Belrose et al., 2023), và direct logit attribution (Elhage et al., 2021), mà giải mã các biểu diễn token trung gian từ các mô hình transformer theo các cách khác nhau. Logit lens làm như vậy bằng cách sử dụng đầu mô hình hóa ngôn ngữ, thường chỉ được áp dụng trong lớp cuối cùng, một cách sớm trong các lớp trước đó, mà không có bất kỳ huấn luyện bổ sung nào. Tuned lens tinh vi hơn bổ sung huấn luyện một ánh xạ affine để biến đổi một trạng thái tiềm ẩn trung gian sao cho nó bắt chước các dự đoán token được thực hiện bởi trạng thái tiềm ẩn cuối cùng. Cuối cùng, direct logit attribution tổng quát hóa logit lens bằng cách xem xét đóng góp logit của mỗi đầu attention riêng lẻ.

Trong công việc này, chúng tôi phụ thuộc nhiều vào logit lens, được mô tả thêm trong Sec. 3.2, thay vì tuned lens. Cái sau sẽ làm hỏng mục đích của chúng tôi là hiểu liệu Llama-2, khi được prompt bằng ngôn ngữ không phải tiếng Anh, có đi vòng qua các trạng thái nội bộ tiếng Anh trước khi xuất ra văn bản không phải tiếng Anh hay không. Vì tuned lens được huấn luyện cụ thể để ánh xạ các trạng thái nội bộ - thậm chí nếu tương ứng với tiếng Anh - với dự đoán token tiếp theo cuối cùng, không phải tiếng Anh, tiêu chí tối ưu hóa sẽ "tối ưu hóa bỏ" tín hiệu mà chúng tôi quan tâm.

3 Vật liệu và phương pháp
3.1 Các mô hình ngôn ngữ: Llama-2
Chúng tôi tập trung vào họ mô hình ngôn ngữ Llama-2 (Touvron et al., 2023), một số mô hình mã nguồn mở lớn nhất và được sử dụng rộng rái nhất. Các mô hình được huấn luyện trên một corpus đa ngôn ngữ mà phần lớn bị thống trị bởi tiếng Anh, chiếm 89,70% corpus. Tuy nhiên, do kích thước của dữ liệu huấn luyện (hai nghìn tỷ token), thậm chí một tỷ lệ phần trăm nhỏ của dữ liệu huấn luyện không phải tiếng Anh vẫn tạo thành một số lượng lớn token về mặt tuyệt đối (ví dụ, 0,17% = 3,4B token tiếng Đức, 0,13% = 2,6B token tiếng Trung). Do đó, Llama-2 được coi là một mô hình đa ngôn ngữ, mặc dù có thiên lệch tiếng Anh.

Các phiên bản. Llama-2 có ba kích thước mô hình, với 7B/13B/70B tham số, 32/40/80 lớp, và chiều embedding d=4096/5120/8192, tương ứng. Qua tất cả các kích thước mô hình, từ vựng V chứa v=32.000 token. Ở đây chúng tôi nghiên cứu tất cả các kích thước mô hình, sử dụng lượng tử hóa 8-bit (Dettmers et al., 2022) trong các thí nghiệm của chúng tôi.

Kiến trúc. Llama-2 là một transformer tự hồi quy, chỉ decoder, dựa trên residual. Các mô hình như vậy duy trì hình dạng của dữ liệu đầu vào trong suốt quá trình tính toán trong một forward pass: một vector embedding, được gọi là tiềm ẩn, cho mỗi token đầu vào x1,...,xn∈V, trong đó n là độ dài chuỗi đầu vào. Các tiềm ẩn ban đầu h(0)1,...,h(0)n∈Rd được thu được từ một từ điển embedding đã học chứa một vector cố định cho mỗi token từ vựng. Mỗi tiềm ẩn này được cập nhật từng bước qua từng lớp bằng cách thêm một residual. Residual được thêm vào tiềm ẩn tại vị trí i trong lớp j là một hàm fj của tất cả các tiềm ẩn token trước đó h(j-1)1,...,h(j-1)i:

h(j)i = h(j-1)i + fj(h(j-1)1,...,h(j-1)i), (1)

trong đó vector kết quả h(j)i vẫn có chiều d. Hàm fj chính nó, được gọi là khối transformer, bao gồm một lớp self-attention có mặt nạ theo sau bởi một lớp feed-forward với một kết nối residual và chuẩn hóa root mean square (RMS) ở giữa (Vaswani et al., 2017; Touvron et al., 2023). Do chuẩn hóa RMS, tất cả các tiềm ẩn nằm trên một siêu cầu d-chiều có bán kính √d.

Trong tiền huấn luyện, tất cả các khối transformer f1,...,fm (với m là số lớp) được điều chỉnh sao cho tiềm ẩn cuối cùng h(m)i cho vị trí i phù hợp để dự đoán token tại vị trí i+1. Để dự đoán, vector embedding cuối cùng được nhân với một ma trận gọi là unembedding U∈Rv×d, mà tạo ra một vector thực zi=Uh(m)i∈Rv chứa điểm logit được gọi là zit cho mỗi token từ vựng t∈V. Các điểm này sau đó được chuyển đổi thành xác suất P(xi+1=t|x1,...,xi)∝ezit thông qua thao tác softmax.

3.2 Diễn giải các embedding tiềm ẩn: Logit lens
Khi các transformer được triển khai trong thực tế, chỉ các vector tiềm ẩn cuối cùng sau khối transformer cuối cùng được chuyển thành phân phối token bằng cách nhân chúng với U và lấy softmax. Tuy nhiên, vì các tiềm ẩn có cùng hình dạng trong tất cả các lớp, bất kỳ tiềm ẩn nào về nguyên tắc có thể được chuyển thành phân phối token, bằng cách đối xử với nó như thể nó là một tiềm ẩn lớp cuối cùng. Việc giải mã token sớm từ các tiềm ẩn theo cách này, một phương pháp được gọi là logit lens (cf. Sec. 2), có thể tạo điều kiện cho việc kiểm tra và diễn giải trạng thái nội bộ của các transformer. Sử dụng logit lens, chúng tôi thu được một phân phối token tiếp theo P(xi+1|h(j)i) cho mỗi vị trí i và lớp j.

Chúng tôi minh họa logit lens trong Hình 1, trong đó mỗi ô cho thấy token có khả năng nhất khi áp dụng logit lens cho tiềm ẩn ở vị trí và lớp đó. Như thấy, logit lens giải mã các token phù hợp ngữ cảnh đã từ các lớp trung gian.

3.3 Dữ liệu: Các nhiệm vụ để gợi ra ngôn ngữ tiềm ẩn
Mục tiêu của chúng tôi là khám phá liệu các trạng thái tiềm ẩn, nội bộ của Llama-2 có tương ứng với các ngôn ngữ tự nhiên cụ thể hay không. Mặc dù logit lens cho phép chúng tôi ánh xạ các vector tiềm ẩn thành phân phối token, chúng tôi vẫn cần một ánh xạ từ phân phối token sang ngôn ngữ. Việc làm như vậy nói chung là khó khăn vì nhiều token mơ hồ về mặt ngôn ngữ; ví dụ, token "an" được sử dụng phổ biến trong tiếng Anh, tiếng Pháp và tiếng Đức, trong số những ngôn ngữ khác. Để vượt qua vấn đề này, chúng tôi xây dựng các prompt x1...xn trong đó token tiếp theo đúng xn+1 là (1) rõ ràng và (2) có thể được gán rõ ràng cho một ngôn ngữ.

Thiết kế prompt. Để đảm bảo rằng token tiếp theo là rõ ràng (tiêu chí 1), chúng tôi thiết kế ba nhiệm vụ hoàn thành văn bản trong đó token tiếp theo xn+1 có thể dễ dàng suy ra từ prompt x1...xn. Trong việc mô tả các nhiệm vụ, chúng tôi sử dụng tiếng Trung như một ngôn ngữ ví dụ.

Nhiệm vụ dịch thuật. Ở đây nhiệm vụ là dịch từ không phải tiếng Anh (ví dụ, tiếng Pháp) trước đó sang tiếng Trung. Chúng tôi cho mô hình thấy bốn từ với bản dịch đúng của chúng, theo sau bởi từ thứ năm mà không có bản dịch, và để mô hình dự đoán token tiếp theo ("中文" có nghĩa là "tiếng Trung" bên dưới):

Français: "vertu" - 中文: "德"
Français: "siège" - 中文: "座" 
Français: "neige" - 中文: "雪"
Français: "montagne" - 中文: "山"
Français: "fleur" - 中文: "

Với một prompt như vậy, Llama-2 có thể dễ dàng suy ra rằng nó nên dịch từ tiếng Pháp thứ năm. Chúng tôi cẩn thận lựa chọn các từ như được mô tả bên dưới và xây dựng một prompt cho mỗi từ bằng cách lấy mẫu ngẫu nhiên các demonstration từ các từ còn lại.

Nhiệm vụ lặp lại. Tương tự, chúng tôi giao cho mô hình nhiệm vụ đơn giản lặp lại từ cuối cùng, thay vì dịch nó, bằng cách prompting như sau:

中文: "德" - 中文: "德"
中文: "座" - 中文: "座"
中文: "雪" - 中文: "雪"
中文: "山" - 中文: "山"
中文: "花" - 中文: "

Nhiệm vụ cloze. Như một nhiệm vụ hơi khó hơn, chúng tôi xem xét một bài kiểm tra cloze, trong đó mô hình phải dự đoán một từ bị che trong một câu. Cho một từ đích, chúng tôi xây dựng một câu tiếng Anh bắt đầu bằng từ đó bằng cách prompting GPT-4, che từ đích, và dịch câu sang các ngôn ngữ khác. Để xây dựng các prompt, chúng tôi lấy mẫu hai demonstration từ các từ còn lại. Một ví dụ tiếng Anh trước khi dịch sang các ngôn ngữ khác như sau:

A "___" is used to play sports like soccer and basketball. Answer: "ball".
A "___" is a solid mineral material forming part of the surface of the earth. Answer: "rock".
A "___" is often given as a gift and can be found in gardens. Answer: "

Lựa chọn từ. Để cho phép gán ngôn ngữ rõ ràng (tiêu chí 2), chúng tôi xây dựng một tập hợp từ đóng cho mỗi ngôn ngữ. Như một trường hợp đặc biệt sạch sẽ, chúng tôi tập trung vào tiếng Trung, có nhiều từ single-token và không sử dụng dấu cách. Chúng tôi quét từ vựng của Llama-2 để tìm các từ tiếng Trung single-token (chủ yếu là danh từ) có bản dịch tiếng Anh single-token. Bằng cách này, các xác suất của Llama-2 cho từ tiếng Trung tiếp theo đúng và cho tương tự tiếng Anh của nó có thể được đọc trực tiếp từ các xác suất token tiếp theo.

Để tăng tính mạnh mẽ, chúng tôi cũng chạy tất cả các thí nghiệm trên tiếng Đức, tiếng Pháp và tiếng Nga. Để làm điều này, chúng tôi dịch các từ tiếng Trung/tiếng Anh đã chọn và, đối với mỗi ngôn ngữ, loại bỏ các từ chia sẻ tiền tố token với phiên bản tiếng Anh, vì điều này sẽ làm cho việc phát hiện ngôn ngữ (cf. Sec. 3.4) trở nên mơ hồ.

Chúng tôi làm việc với 139 từ tiếng Trung, 104 từ tiếng Đức, 56 từ tiếng Pháp và 115 từ tiếng Nga (cf. Phụ lục A.1).

3.4 Đo các xác suất ngôn ngữ tiềm ẩn
Để điều tra một ngôn ngữ trục giả định bên trong Llama-2, chúng tôi áp dụng logit lens cho các tiềm ẩn h(j)n tương ứng với token đầu vào cuối cùng xn cho mỗi lớp j, thu được một phân phối token tiếp theo P(xn+1|h(j)n) cho mỗi lớp. Các prompt của chúng tôi (cf. Sec. 3.3) được thiết kế cụ thể sao cho một phân phối token tiếp theo trung gian cho phép chúng tôi ước tính xác suất của từ tiếp theo đúng trong ngôn ngữ đầu vào cũng như tiếng Anh. Vì chúng tôi cụ thể lựa chọn các từ single-token trong tiếng Trung (ZH) cũng như tiếng Anh (EN), chúng tôi có thể đơn giản định nghĩa xác suất của ngôn ngữ ℓ∈{ZH,EN} như xác suất của token tiếp theo là phiên bản tℓ của ℓ của từ single-token đúng: P(lang=ℓ|h(j)n):=P(xn+1=tℓ|h(j)n). (Để dễ đọc, chúng tôi cũng đơn giản viết P(lang=ℓ).)

Lưu ý rằng điều này không định nghĩa một phân phối trên các ngôn ngữ, vì nói chung ∑ℓP(lang=ℓ)<1.

Trong các ngôn ngữ khác (và trong các trường hợp góc trong tiếng Trung và tiếng Anh), chúng tôi phải tính đến nhiều tokenization và dấu cách (cf. Phụ lục A.2).

4 Kết quả
Khi trình bày kết quả, đầu tiên chúng tôi (Sec. 4.1) lấy một quan điểm xác suất thông qua logit lens (Sec. 3.2), cho tất cả các nhiệm vụ và tất cả các kích thước mô hình. (Vì các kết quả nhất quán qua các ngôn ngữ, chúng tôi tập trung vào tiếng Trung ở đây và tham khảo Phụ lục B cho tiếng Pháp, tiếng Đức và tiếng Nga.) Sau đó (Sec. 4.2) chúng tôi khoan sâu hơn bằng cách lấy một quan điểm hình học về cách các embedding token trôi dạt khi transformer tính toán từ lớp này sang lớp khác.

4.1 Quan điểm xác suất: Logit lens
Logit lens cho chúng tôi một tập hợp các xác suất ngôn ngữ (cf. Sec. 3.4) cho mỗi prompt đầu vào và lớp. Hình 2 theo dõi sự phát triển của các xác suất ngôn ngữ từ lớp này sang lớp khác, với một biểu đồ cho mỗi tổ hợp của kích thước mô hình (cột) và nhiệm vụ (hàng). Trục x cho thấy các chỉ số lớp, và trục y là các xác suất ngôn ngữ P(lang=ZH) và P(lang=EN) được tính trung bình trên các prompt đầu vào.

Trên các nhiệm vụ dịch thuật và cloze, một bức tranh nhất quán xuất hiện qua các kích thước mô hình. Cả token tiếng Trung đúng và tương tự tiếng Anh của nó đều không thu được bất kỳ khối lượng xác suất đáng chú ý nào trong nửa đầu của các lớp. Sau đó, xung quanh lớp giữa, tiếng Anh bắt đầu tăng mạnh theo sau bởi một sự suy giảm, trong khi tiếng Trung từ từ tăng và, sau một sự giao nhau với tiếng Anh, tăng vọt trên năm lớp cuối cùng. Trên nhiệm vụ lặp lại, tiếng Trung đã tăng cùng với tiếng Anh (được thảo luận trong Sec. 6). Điều này tương phản với tất cả các ngôn ngữ khác, trong đó tiếng Anh tăng trước (Phụ lục B).

Trên đỉnh của các xác suất ngôn ngữ (Sec. 3.4), entropy của toàn bộ phân phối token tiếp theo được hiển thị như một heatmap phía trên các biểu đồ. Chúng tôi một lần nữa quan sát một mẫu nhất quán qua các nhiệm vụ và kích thước mô hình: entropy cao trong nửa đầu của các lớp, trong khi cả P(lang=ZH) và P(lang=EN) đều gần bằng không, theo sau bởi một sự giảm mạnh cùng lúc P(lang=EN) tăng. Từ đó trở đi, entropy vẫn thấp, với một sự phục hồi nhẹ khi khối lượng xác suất chuyển từ tiếng Anh sang tiếng Trung.

Với 32.000≈2^15 token trong từ vựng, entropy sớm khoảng 14 bit ngụ ý một phân phối token tiếp theo gần đồng đều (khoảng 15 bit).

Trực quan hóa đường dẫn. Các biểu đồ của Hình 2 chỉ xem xét xác suất của token tiếng Trung tiếp theo đúng và tương tự tiếng Anh của nó, mà không nói gì về các token còn lại. Để hình thành trực giác về toàn bộ phân phối, chúng tôi sử dụng giảm chiều để trực quan hóa dữ liệu. Đầu tiên, chúng tôi định nghĩa khoảng cách giữa một tiềm ẩn hn tại vị trí n và một token t thông qua negative log-likelihood của t cho hn, như được tính toán bởi logit lens (cf. Sec. 3.4): d(hn,t) = -log P(xn+1=t|hn). Sau đó, chúng tôi sử dụng classical multidimensional scaling để nhúng các token và tiềm ẩn trong một không gian 2D chung gần như bảo toàn khoảng cách. (Các khoảng cách intra-token và intra-latent được đặt thành max h,t d(h,t), phục vụ như một "lực lò xo" đẩy các điểm 2D ra xa.)

Phép tính toán forward của transformer cho một token đầu vào cuối cùng xn có thể được trực quan hóa bằng cách kết nối các embedding 2D của các tiềm ẩn h(j)n trong các lớp tiếp theo j, như được trình bày và giải thích trong Hình 3 (dịch từ tiếng Đức sang tiếng Trung, 70B). Chúng tôi tạo hai quan sát: (1) Một cụm token tiếng Anh và tiếng Trung xuất hiện, cho thấy rằng cùng một tiềm ẩn cũng cho xác suất cao cho toàn bộ ngôn ngữ, bên cạnh phiên bản cụ thể theo ngôn ngữ của token tiếp theo đúng. (2) Các đường dẫn đầu tiên đi qua cụm tiếng Anh, và chỉ sau đó mới đến cụm tiếng Trung. Được lấy cùng nhau, bức tranh đang nổi lên là khi dịch một từ tiếng Đức sang tiếng Trung, Llama-2 thực hiện một "đường vòng" qua một không gian con tiếng Anh.

Cho đến nay, chúng tôi đã đặc trưng hóa các trạng thái tiềm ẩn trung gian của transformer từ một góc độ xác suất, bằng cách nghiên cứu các phân phối token tiếp theo thu được thông qua logit lens. Để hiểu sâu hơn, chúng tôi tiếp theo lấy một góc độ hình học và phân tích các tiềm ẩn trực tiếp như các điểm trong không gian Euclidean, tức là trước khi ánh xạ chúng thành xác suất token.

4.2 Quan điểm hình học: Một cuộc phiêu lưu không gian 8192D
Một cách đơn giản, nhiệm vụ được giải quyết bởi một transformer tự hồi quy là ánh xạ embedding đầu vào của token hiện tại thành embedding đầu ra của token tiếp theo. Nhiệm vụ được giải quyết từng bước, mỗi lớp sửa đổi (bằng cách thêm residual) vector tiềm ẩn được tạo ra bởi lớp trước đó, một quá trình mô tả về mặt hình học một đường dẫn qua không gian Euclidean d-chiều. Bây giờ chúng tôi đặt mục tiêu đặc trưng hóa đường dẫn này. Vì quan điểm xác suất (Hình 2) đã cho kết quả nhất quán qua các nhiệm vụ và kích thước mô hình, chúng tôi tập trung vào một nhiệm vụ (dịch thuật) và một kích thước mô hình (70B, tức là d=8192).

Các cầu embedding. Các embedding token đầu ra (các hàng của ma trận unembedding U) và các tiềm ẩn h cùng cư trú trong cùng một không gian Euclidean d-chiều. Thực tế, do chuẩn hóa RMS (Sec. 3.1), các tiềm ẩn theo thiết kế sống trên một siêu cầu có bán kính √d≈90.1. Ngoài ra, bằng cách phân tích 2-norm của các embedding token đầu ra (trung bình 1.52, SD 0.23), chúng tôi phát hiện rằng cái sau cũng gần như nằm trên một cầu, có bán kính 1.52.

Năng lượng token. Quan trọng là, các embedding token chiếm cầu của chúng không đồng đều; ví dụ, 25% đầu tiên của các thành phần chính chiếm 50% tổng phương sai, và 54% đầu tiên chiếm 80%. Để xây dựng trực giác, đầu tiên xem xét một trường hợp cực đoan giả định trong đó các token nằm trong một không gian con thích hợp ("không gian con token") của không gian d-chiều đầy đủ (mặc dù, thực nghiệm, U có rank d, vì vậy các embedding đầu ra của các token trải rộng toàn bộ Rd). Nếu một tiềm ẩn h có một thành phần trực giao với không gian con token, nó bao gồm thông tin không liên quan để dự đoán token tiếp theo dựa trên chỉ h (vì các logit là tích vô hướng của các vector tiềm ẩn và token). Thành phần trực giao vẫn có thể quan trọng cho các phép tính được thực hiện bởi các lớp sau và để dự đoán token tiếp theo trong các lớp đó. Nhưng logit lens, giải mã các tiềm ẩn thành các token một cách sớm trong các lớp trung gian, sẽ mù với thành phần trực giao.

Góc của một tiềm ẩn h với "không gian con token" do đó đo lường bao nhiều của h không liên quan để dự đoán token tiếp theo ngay lập tức. Cụ thể, chúng tôi xem xét mean squared cosine giữa h và các embedding token (các hàng của U) để nắm bắt bao nhiều "năng lượng" của h dịch thành điểm logit. Để diễn giải, chúng tôi chuẩn hóa bằng mean squared cosine giữa chính các embedding token, thu được cái mà chúng tôi gọi là squared token energy của h:

E(h)² = (1/v)||Ûh||²₂/||h||²₂ / (1/v²)||ÛÛᵀ||²_F = (v/d)||Ûh||²₂ / ||ÛÛᵀ||²_F    (2)

(Û là U với các hàng được chuẩn hóa 2), mà nắm bắt sự gần gũi của h với "không gian con token", so với sự gần gũi của một token ngẫu nhiên với "không gian con token".

Chúng tôi trực quan hóa token energy và mối quan hệ của nó với các đại lượng chính khác trong Hình 4. Như một hàm của lớp (Hình 4(b)), root mean squared token energy thấp (khoảng 20%) và chủ yếu phẳng trước lớp 70, khi nó đột nhiên tăng vọt - đúng khi các dự đoán token tiếp theo chuyển từ tiếng Anh sang tiếng Trung (Hình 4(c)).

Tóm lại, Hình 4(a–c) tiết lộ ba giai đoạn:

1. Giai đoạn 1 (lớp 1–40): Entropy cao (14 bit, gần như đồng đều), token energy thấp, không ngôn ngữ nào thống trị.

2. Giai đoạn 2 (lớp 41–70): Entropy thấp (1–2 bit), token energy thấp, tiếng Anh thống trị.

3. Giai đoạn 3 (lớp 71–80): Entropy thấp, token energy cao (tăng từ 20% lên 30%), tiếng Trung thống trị.

5 Mô hình khái niệm
Tiếp theo, chúng tôi xây dựng một mô hình khái niệm nhất quán với các quan sát trên.

Để dự đoán token tiếp theo, công việc của transformer về cơ bản bao gồm việc ánh xạ embedding đầu vào của token hiện tại thành embedding đầu ra của token tiếp theo. Giai đoạn 1 tập trung vào việc xây dựng một biểu diễn đặc trưng tốt hơn cho token hiện tại từ embedding đầu vào của nó, bằng cách xử lý các vấn đề tokenization (ví dụ, tích hợp các token trước đó thuộc về cùng một từ), tích hợp các từ thành các đơn vị ngữ nghĩa lớn hơn, v.v. Giai đoạn này chưa trực tiếp quan tâm đến việc dự đoán token tiếp theo, với các tiềm ẩn vẫn phần lớn trực giao với không gian token đầu ra (token energy thấp), dẫn đến tích vô hướng nhỏ giữa các tiềm ẩn và các embedding token đầu ra, và do đó đến entropy cao.

Trong Giai đoạn 2, các tiềm ẩn sống trong một "không gian khái niệm" trừu tượng, mà, không giống như trong Giai đoạn 1, không còn trực giao với không gian token đầu ra. Thay vào đó, các "embedding khái niệm" tiềm ẩn gần hơn với các embedding token đầu ra có thể diễn đạt khái niệm tương ứng (qua các ngôn ngữ, từ đồng nghĩa, v.v.), dẫn đến entropy thấp. Trong số các token liên quan đến khái niệm, các biến thể tiếng Anh nằm gần embedding khái niệm hơn so với các biến thể không phải tiếng Anh (do việc tiếp xúc áp đảo của mô hình với tiếng Anh trong quá trình huấn luyện), dẫn đến xác suất cao hơn cho các token tiếng Anh so với tiếng Trung. Mặc dù có mối tương quan giữa khái niệm và các embedding token, các embedding khái niệm cũng mang nhiều thông tin vượt ra ngoài các token đầu ra (bao gồm thông tin ngữ cảnh cụ thể theo đầu vào và thông tin về ngôn ngữ đích), dẫn đến token energy vẫn thấp.

Trong Giai đoạn 3, mô hình ánh xạ các khái niệm trừu tượng thành các từ/token cụ thể trong ngôn ngữ đích. Thông tin không liên quan cho việc dự đoán token tiếp theo được loại bỏ, dẫn đến một sự tăng vọt trong token energy.

Phác thảo. Mô hình này được minh họa - với một bản phác thảo được đơn giản hóa mạnh mẽ giống như đồ chơi - trong Hình 4(d). Trong bức tranh này, mô hình hoạt động trong không gian 3D (thay vì 8192D thực tế). Tất cả các embedding (token đầu ra và tiềm ẩn) nằm trên một cầu xung quanh gốc. Các embedding token nằm trên xích đạo và chủ yếu trải rộng dọc theo trục x (trái/phải), nắm bắt ngôn ngữ (tiếng Anh bên trái, tiếng Trung bên phải). Trục y (trước/sau) nắm bắt các khái niệm, trong bức tranh đồ chơi này dọc theo một thang đo "độ ngọt" 1D. Trục z (dưới/trên) cung cấp một mức độ tự do bổ sung có thể được sử dụng để lưu trữ thông tin về ngữ cảnh, ngôn ngữ, v.v. Một forward pass của transformer di chuyển dọc theo bề mặt của cầu. Trong Giai đoạn 1, tiềm ẩn bắt đầu ở cực bắc, trực giao với cả embedding token đầu ra và khái niệm. Giai đoạn 2 xoay tiềm ẩn vào không gian khái niệm; các token tiếng Anh có khả năng cao hơn vì các embedding của chúng có một thành phần khái niệm y mạnh hơn. Cuối cùng, Giai đoạn 3 xoay tiềm ẩn dọc theo xích đạo vào bán cầu của ngôn ngữ đích, lên token đầu ra nắm bắt tốt nhất khái niệm đang hoạt động trong ngôn ngữ đó.

6 Thảo luận
Trong nỗ lực trả lời liệu các mô hình Llama-2 có sử dụng tiếng Anh làm ngôn ngữ trục nội bộ hay không, chúng tôi phát hiện rằng các embedding tiềm ẩn thực sự nằm xa token tiếp theo đúng trong ngôn ngữ đầu vào hơn so với tương tự tiếng Anh của nó, dẫn đến các biểu diễn nội bộ chủ yếu là tiếng Anh như thấy qua logit lens. Do đó có thể hấp dẫn để kết luận rằng, vâng, Llama-2 sử dụng tiếng Anh như một trục ngầm, tương tự như việc sử dụng tiếng Anh làm trục rõ ràng trước đây của các nhà nghiên cứu (Shi et al., 2022; Ahuja et al., 2023; Huang et al., 2023). Nhưng câu trả lời của chúng tôi phải tinh tế hơn, vì phần lớn "năng lượng" của các tiềm ẩn chỉ theo các hướng phần lớn trực giao với các embedding token đầu ra và do đó không quan trọng cho việc dự đoán token tiếp theo. Mô hình có thể sử dụng các hướng này như các mức độ tự do bổ sung để xây dựng các biểu diễn đặc trưng phong phú từ các đầu vào thô của nó (Yosinski et al., 2014, 2015; Geva et al., 2022), có thể được xem như việc hình thành một "không gian khái niệm" trừu tượng. Trong cách diễn giải này, ngôn ngữ chung nội bộ của mô hình không phải là tiếng Anh mà là các khái niệm - các khái niệm bị thiên lệch hướng về tiếng Anh. Do đó, tiếng Anh vẫn có thể được xem như một ngôn ngữ trục, nhưng theo nghĩa ngữ nghĩa, chứ không phải hoàn toàn theo nghĩa từ vựng.

Các thí nghiệm của chúng tôi liên quan đến ba nhiệm vụ hoàn thành văn bản. Các nhiệm vụ dịch thuật và cloze hoạt động ở mức ngữ nghĩa, trong khi nhiệm vụ lặp lại từ hoàn toàn cú pháp. Tuy nhiên, trong hầu hết các ngôn ngữ (Hình 7) mẫu tương tự như hai nhiệm vụ kia, với các token đầu tiên đi qua một "giai đoạn tiếng Anh" - có thể bởi vì nhận biết rằng nhiệm vụ là đơn giản sao chép một token đòi hỏi hiểu biết ngữ nghĩa, được đạt được chỉ trong không gian khái niệm, mà lần lượt gần hơn với các embedding token tiếng Anh.

Điều này nói rằng, lưu ý rằng mẫu tiếng Anh-đầu tiên ít rõ ràng hơn trên nhiệm vụ lặp lại (Hình 7), trong đó ngôn ngữ đầu vào tăng sớm hơn so với các nhiệm vụ khác hoặc, đối với tiếng Trung (Hình 7(e)) thậm chí đồng thời với, hoặc nhanh hơn, tiếng Anh. Điều này có thể do tokenization: đối với tiếng Trung chúng tôi rõ ràng chọn 100% từ single-token, trái ngược với chỉ 13% cho tiếng Nga, 43% cho tiếng Đức, và 55% cho tiếng Pháp (Bảng 1). Khi các token cụ thể theo ngôn ngữ có sẵn, đường vòng qua tiếng Anh dường như ít rõ ràng hơn. Điều này hỗ trợ những lo ngại trước đây về tầm quan trọng của tokenization, không chỉ gây gánh nặng cho các ngôn ngữ thiểu số với nhiều token hơn mỗi từ (Artetxe et al., 2020), mà, như chúng tôi cho thấy, cũng buộc các tiềm ẩn phải đi qua một không gian ngữ nghĩa thiên lệch tiếng Anh.

Công việc tương lai nên điều tra theo những cách nào một thiên lệch tiếng Anh trong không gian tiềm ẩn có thể có vấn đề, ví dụ, bằng cách làm thiên lệch hành vi mô hình downstream. Chúng tôi thấy triển vọng trong việc thiết kế các thí nghiệm dựa trên công việc từ tâm lý ngôn ngữ học, đã cho thấy rằng các khái niệm có thể mang các giá trị cảm xúc khác nhau trong các ngôn ngữ khác nhau (Boroditsky et al., 2003) và việc sử dụng một từ cho hai khái niệm (colexification) có thể ảnh hưởng đến nhận thức (Di Natale et al., 2021).

Công việc tương lai cũng nên nghiên cứu cách thiên lệch tiếng Anh thay đổi khi giảm sự thống trị của tiếng Anh trong quá trình huấn luyện, ví dụ, bằng cách áp dụng phương pháp của chúng tôi lên các dẫn xuất Llama-2 với một hỗn hợp ngôn ngữ khác nhau (Goddard, 2023; Plüster, 2023; Huang, 2023; Kim, 2023), hoặc bằng cách sử dụng các tokenizer ít Anglocentric hơn. Công việc như vậy sẽ đưa ra những manh mối quan trọng để giảm thiên lệch tiếng Anh và cho phép AI công bằng hơn.

Hạn chế
Trong bài báo này, chúng tôi tập trung vào họ mô hình ngôn ngữ Llama-2, điều này hạn chế các tuyên bố mà chúng tôi có thể đưa ra về các mô hình bị thống trị bởi tiếng Anh khác (nhưng xem Phụ lục B.2 để biết bằng chứng ban đầu rằng Mistral-7B hoạt động giống hệt). Hơn nữa, vì phương pháp đề xuất dựa trên các tham số mô hình, rất ít có thể nói về các mô hình mã nguồn đóng được sử dụng rộng rãi hơn. Tuy nhiên, các phương pháp được nêu trong bài báo này có thể được áp dụng một cách đơn giản cho các transformer tự hồi quy khác và tổng quát hóa cho các transformer không tự hồi quy (cho rằng các tham số của chúng có sẵn), một hướng đáng được khám phá trong tương lai.

Ngoài ra, các nhiệm vụ được nêu trong bài báo đơn giản và cung cấp một bối cảnh có kiểm soát cao, nhưng giống như đồ chơi, để nghiên cứu ngôn ngữ nội bộ của các LLM. Điều này rất cần thiết như một bước đầu tiên để minh họa sự tồn tại, nhưng công việc tương lai nên mở rộng đến một loạt các nhiệm vụ rộng hơn; chúng có thể bao gồm các vấn đề nhạy cảm về văn hóa hơn, các trường hợp sử dụng phổ biến (cf. Sec. 6), và các phân tích kỹ thuật vượt ra ngoài các token đơn.

Mặc dù chúng tôi tìm thấy bằng chứng về một "không gian khái niệm" trong cách diễn giải của chúng tôi (Sec. 5), chúng tôi có hiểu biết hạn chế về cấu trúc của không gian này ở dạng nhiều chiều gốc của nó. Chúng tôi tin rằng việc hiểu rõ hơn và lập bản đồ không gian khái niệm này là một hướng tương lai quan trọng và sẽ dẫn đến một nền tảng mạnh mẽ hơn cho mô hình khái niệm được trình bày.

Cuối cùng, mặc dù logit lens cấp cho chúng tôi quyền truy cập gần đúng vào các niềm tin nội bộ về những gì nên là đầu ra tại một vị trí chuỗi đã cho, mọi thứ khác có trong các biểu diễn trung gian (ví dụ, thông tin để xây dựng các khóa, truy vấn, giá trị, hoặc để thực hiện các phép tính trung gian không đóng góp trực tiếp vào niềm tin đầu ra) vẫn bị ẩn và chỉ đi vào phần phân tích dựa trên logit lens của chúng tôi như nhiễu.

Lời cảm ơn
Chúng tôi cảm ơn Nina Rimsky (2023) đã chia sẻ wrapper Llama-2 và triển khai logit lens của cô ấy; Lucia Quirke về các đầu vào về khả năng diễn giải cơ học, về thiết lập thí nghiệm của chúng tôi, và cho một cuộc thảo luận có kết quả; Saibo Geng đã giúp chúng tôi với bộ dữ liệu tiếng Trung; Nicola Cancedda, David Garcia, Eric Horvitz, Manoel Horta Ribeiro, Maxime Peyrard, Saibo Geng, Tim Davidson, Valentin Hartmann, và Zachary Horvitz cho các cuộc thảo luận và phản hồi sâu sắc; và Meta đã mở nguồn Llama-2 và từ đó giúp dân chủ hóa nghiên cứu LLM. Cuối cùng, chúng tôi cảm ơn các người đánh giá đồng đẳng ẩn danh về đầu vào tích cực của họ, điều này đã dẫn đến, trong số những cái khác, Phụ lục B.1 và B.2. Phòng thí nghiệm của West được hỗ trợ một phần bởi các grant từ Swiss National Science Foundation (200021_185043, TMSGI2_211379), Swiss Data Science Center (P22_08), H2020 (952215), và bởi những món quà hào phóng từ Meta, Google, và Microsoft.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch sang tiếng Việt...]
