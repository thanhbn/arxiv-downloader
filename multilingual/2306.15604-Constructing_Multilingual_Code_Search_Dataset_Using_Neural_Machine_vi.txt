Xây dựng Tập dữ liệu Tìm kiếm Mã đa ngôn ngữ Sử dụng Dịch máy Thần kinh

Ryo Sekizawa1Nan Duan2Shuai Lu2Hitomi Yanaka1
1Đại học Tokyo2Microsoft Research Asia
{ryosekizawa,hyanaka}@is.s.u-tokyo.ac.jp
{nanduan,shuailu}@microsoft.com

Tóm tắt

Tìm kiếm mã là một nhiệm vụ tìm các mã lập trình khớp về mặt ngữ nghĩa với các truy vấn ngôn ngữ tự nhiên đã cho. Mặc dù một số tập dữ liệu hiện có cho nhiệm vụ này có tính đa ngôn ngữ về mặt ngôn ngữ lập trình, dữ liệu truy vấn của chúng chỉ có bằng tiếng Anh. Trong nghiên cứu này, chúng tôi tạo ra một tập dữ liệu tìm kiếm mã đa ngôn ngữ bằng bốn ngôn ngữ tự nhiên và bốn ngôn ngữ lập trình sử dụng mô hình dịch máy thần kinh. Sử dụng tập dữ liệu của chúng tôi, chúng tôi tiền huấn luyện và tinh chỉnh các mô hình dựa trên Transformer và sau đó đánh giá chúng trên nhiều bộ test tìm kiếm mã. Kết quả của chúng tôi cho thấy mô hình được tiền huấn luyện với tất cả dữ liệu ngôn ngữ tự nhiên và ngôn ngữ lập trình đã thực hiện tốt nhất trong hầu hết các trường hợp. Bằng cách áp dụng lọc dữ liệu dịch ngược vào tập dữ liệu của chúng tôi, chúng tôi chứng minh rằng chất lượng dịch thuật ảnh hưởng đến hiệu suất của mô hình ở một mức độ nhất định, nhưng kích thước dữ liệu quan trọng hơn.

1 Giới thiệu

Tìm kiếm mã là nhiệm vụ tìm mã ngôn ngữ lập trình tương ứng về mặt ngữ nghĩa cho một truy vấn ngôn ngữ tự nhiên bằng cách tính toán độ tương tự của chúng. Với sự lan rộng của các kho lưu trữ chia sẻ mã quy mô lớn và sự phát triển của các công cụ tìm kiếm tiên tiến, tìm kiếm mã hiệu suất cao là một công nghệ quan trọng để hỗ trợ các nhà phát triển phần mềm. Vì các nhà phát triển phần mềm trên toàn thế giới tìm kiếm mã bằng ngôn ngữ mẹ đẻ của họ, chúng tôi kỳ vọng các mô hình tìm kiếm mã phải đa ngôn ngữ. Mặc dù nhiều nghiên cứu trước đây tập trung vào các nhiệm vụ mã đa ngôn ngữ khác ngoài tìm kiếm mã (ví dụ: tạo mã, giải thích mã) (Wang et al., 2021; Ahmad et al., 2021; Fried et al., 2023; Zheng et al., 2023), các tập dữ liệu tìm kiếm mã hiện có (Husain et al., 2020; Huang et al., 2021; Shuai et al., 2021) chỉ chứa dữ liệu đơn ngôn ngữ cho các truy vấn tìm kiếm.

Trong nghiên cứu này, chúng tôi xây dựng một tập dữ liệu tìm kiếm mã đa ngôn ngữ mới bằng cách dịch dữ liệu ngôn ngữ tự nhiên của tập dữ liệu quy mô lớn hiện có sử dụng mô hình dịch máy thần kinh. Chúng tôi cũng sử dụng tập dữ liệu của chúng tôi để tiền huấn luyện và tinh chỉnh mô hình dựa trên Transformer (Vaswani et al., 2017) và đánh giá nó trên các bộ test tìm kiếm mã đa ngôn ngữ mà chúng tôi tạo ra. Chúng tôi cho thấy rằng mô hình được tiền huấn luyện với tất cả dữ liệu ngôn ngữ tự nhiên và ngôn ngữ lập trình thực hiện tốt nhất trong hầu như tất cả các cài đặt. Chúng tôi cũng phân tích mối quan hệ giữa chất lượng dịch thuật của tập dữ liệu và hiệu suất của mô hình bằng cách lọc tập dữ liệu tinh chỉnh sử dụng dịch ngược. Mô hình và tập dữ liệu của chúng tôi sẽ được công khai tại https://github.com/ynklab/XCodeSearchNet. Các đóng góp của nghiên cứu này như sau:

1. Xây dựng tập dữ liệu tìm kiếm mã lớn bao gồm các truy vấn ngôn ngữ tự nhiên đa ngôn ngữ và mã sử dụng dịch máy.
2. Xây dựng mô hình tìm kiếm mã đa ngôn ngữ và đánh giá nó trên nhiệm vụ tìm kiếm mã sử dụng tập dữ liệu của chúng tôi.
3. Phân tích mối tương quan giữa chất lượng dịch thuật và hiệu suất mô hình trên nhiệm vụ tìm kiếm mã.

2 Bối cảnh

2.1 Tập dữ liệu Tìm kiếm Mã

CodeSearchNet Corpus1(CSN; Husain et al., 2020) là một tập dữ liệu mã (code) bằng sáu ngôn ngữ lập trình: Go, Python, Java, PHP, Ruby, và Javascript, và dữ liệu ngôn ngữ tự nhiên mô tả chúng (docstring). CSN được tạo ra bằng cách tự động thu thập các cặp mã hàm và tài liệu của nó có sẵn công khai trên GitHub và được phép phân phối lại. Corpus này chứa khoảng 2,3 triệu cặp dữ liệu và 4 triệu dữ liệu chỉ mã. Dữ liệu ngôn ngữ tự nhiên trong CSN là tài liệu hàm, đây là dữ liệu giả của các văn bản con người sử dụng để tìm kiếm mã.

Ngược lại, một số tập dữ liệu được tạo ra dựa trên các truy vấn ngôn ngữ tự nhiên được sử dụng để tìm kiếm mã bởi con người. CodeXGLUE (Shuai et al., 2021), một điểm chuẩn cho các nhiệm vụ hiểu mã khác nhau, bao gồm hai tập dữ liệu tìm kiếm mã: WebQueryTest (WQT) và CoSQA (Huang et al., 2021). Dữ liệu truy vấn của các tập dữ liệu này được thu thập từ các nhật ký tìm kiếm của người dùng Microsoft Bing và mã từ CSN. Với những dữ liệu được thu thập riêng biệt này, các nhà chú thích có kiến thức lập trình thủ công ánh xạ truy vấn và mã tương ứng để xây dựng tập dữ liệu. Đặc điểm chung của các tập dữ liệu này là tất cả dữ liệu ngôn ngữ tự nhiên, chẳng hạn như docstring và truy vấn, đều giới hạn ở tiếng Anh và không hỗ trợ nhiều ngôn ngữ.

2.2 CodeBERT

CodeBERT (Feng et al., 2020) là một mô hình được tiền huấn luyện và tinh chỉnh với CSN và dựa trên kiến trúc của RoBERTa (Liu et al., 2019). CodeBERT sử dụng Masked Language Modeling (MLM; Devlin et al., 2019; Lample và Conneau, 2019) và Replaced Token Detection (RTD; Clark et al., 2020) làm các nhiệm vụ tiền huấn luyện. Cả dữ liệu docstring và mã trong CSN đều được sử dụng trong MLM, trong khi chỉ dữ liệu mã được sử dụng trong RTD. CodeBERT chỉ được huấn luyện với dữ liệu tiếng Anh, do đó không khả dụng cho nhiệm vụ tìm kiếm mã với các truy vấn đa ngôn ngữ.

3 Xây dựng Tập dữ liệu Sử dụng Dịch máy

Một cách có thể để xây dựng tập dữ liệu tìm kiếm mã cho nhiều ngôn ngữ là dịch một tập dữ liệu đơn ngôn ngữ hiện có. Tuy nhiên, kích thước dữ liệu lớn của CSN khiến việc dịch thủ công tất cả docstring của nó trở nên khó khăn. Bảng 1 cho thấy số lượng cặp dữ liệu CSN được sử dụng để tiền huấn luyện (MLM) và tinh chỉnh CodeBERT.

Do đó, chúng tôi sử dụng mô hình dịch máy để dịch dữ liệu chỉ tiếng Anh để tạo ra dữ liệu đa ngôn ngữ một cách hiệu quả. Bằng cách dịch các docstring CSN, chúng tôi tạo ra một tập dữ liệu đa ngôn ngữ bao gồm bốn ngôn ngữ tự nhiên (tiếng Anh, tiếng Pháp, tiếng Nhật và tiếng Trung) và bốn ngôn ngữ lập trình (Go, Python, Java và PHP). Chúng tôi cũng dịch các truy vấn trong các tập dữ liệu mà Feng et al. (2020) sử dụng để tinh chỉnh và đánh giá CodeBERT cho các thí nghiệm của chúng tôi trong Phần 4.1 và Phần 4.2. Trong dữ liệu tinh chỉnh của họ, số lượng nhãn tích cực và tiêu cực được cân bằng. Lưu ý rằng chúng tôi không sử dụng dữ liệu JavaScript và Ruby, có kích thước nhỏ hơn nhiều so với các ngôn ngữ lập trình khác.

Như một mô hình dịch thuật, chúng tôi sử dụng M2M-100 (Fan et al., 2022), hỗ trợ dịch thuật trong 100 ngôn ngữ. M2M-100 đạt được độ chính xác cao trong việc dịch các ngôn ngữ ít tài nguyên bằng cách phân loại 100 ngôn ngữ thành 14 họ từ và tạo dữ liệu huấn luyện song ngữ trong các họ đó. Chúng tôi sử dụng mô hình m2m_100_1.2B, được cung cấp bởi EasyNMT, một framework công khai của các mô hình dịch máy. Chúng tôi đặt kích thước beam của mô hình là 3.

Chúng tôi chú thích thủ công các nhãn cho một số dữ liệu của tập dữ liệu tinh chỉnh để kiểm tra mối tương quan với các nhãn gốc, được tìm thấy là 0,911 (xem Phụ lục B để biết chi tiết).

4 Thí nghiệm Cơ bản

Chúng tôi tiến hành các thí nghiệm cơ bản, trong đó chúng tôi huấn luyện mô hình dựa trên Transformer với tập dữ liệu đa ngôn ngữ của chúng tôi dưới các cài đặt khác nhau về kích thước dữ liệu và đánh giá nó trên nhiều bộ test tìm kiếm mã.

4.1 Huấn luyện

Chúng tôi thực hiện tiền huấn luyện và tinh chỉnh trên một mô hình được khởi tạo với kiến trúc và tham số XLM-R (Conneau et al., 2019). XLM-R là một mô hình được tiền huấn luyện bằng MLM với corpus Wikipedia và Common Crawl cho 100 ngôn ngữ sử dụng Transformer (Vaswani et al., 2017) và đạt hiệu suất cao trên các nhiệm vụ đa ngôn ngữ, chẳng hạn như trả lời câu hỏi. Lưu ý rằng chúng tôi sử dụng thuật ngữ "tiền huấn luyện" để chỉ việc huấn luyện thêm XLM-R với tập dữ liệu của chúng tôi. Trong bài báo này, chúng tôi sử dụng MLM làm mục tiêu học tập để tiền huấn luyện XLM-R và sau đó tinh chỉnh nó sử dụng các cặp dữ liệu mà ngôn ngữ truy vấn và mã là đơn ngôn ngữ. Chúng tôi sử dụng các cặp dữ liệu đơn ngôn ngữ để tinh chỉnh thay vì kết hợp đa ngôn ngữ, vì Feng et al. (2020) làm rõ rằng tinh chỉnh CodeBERT với sáu ngôn ngữ lập trình cùng nhau "thực hiện tệ hơn so với tinh chỉnh một mô hình cụ thể cho từng ngôn ngữ lập trình." Dữ liệu truy vấn và mã được nối với nhau để đưa vào mô hình, và nó dự đoán độ tương tự của chúng dựa trên biểu diễn vector của các token đầu ra [CLS]. Xem Phụ lục C để biết thêm chi tiết về cài đặt huấn luyện, bao gồm các siêu tham số.

4.2 Đánh giá

Như với Feng et al. (2020), chúng tôi sử dụng Mean Reciprocal Rank (MRR) làm thước đo đánh giá.

MRR = 1/|Q| Σ(i=1 đến |Q|) 1/rank_i

|Q| đề cập đến tổng số truy vấn. Khi một bộ test có 1.000 cặp dữ liệu, cho một truy vấn ngôn ngữ tự nhiên i, mô hình tính toán độ tương tự với mã tương ứng i và 999 mã phân tâm. Nếu điểm tương tự được đưa ra cho mã i là cao thứ 2 trong số 1.000 mã, rank_i bằng 2. Sau đó, trung bình của nghịch đảo của rank_i trên tất cả các truy vấn và mã được tính toán là MRR.

Bảng 2 cho thấy kích thước của CSN chúng tôi sử dụng trong các thí nghiệm của chúng tôi. Mỗi bộ test của CSN để đánh giá MRR chứa 1.000 cặp dữ liệu được lấy mẫu ngẫu nhiên từ các bộ test gốc. Chúng tôi sử dụng CoSQA và WQT làm bộ test ngoài CSN. Cũng như CSN, chúng tôi tạo các bộ test CoSQA từ 20.604 cặp dữ liệu gốc. Chúng tôi tính trung bình điểm MRR trên ba bộ test khác nhau cho CSN và CoSQA. Bộ test WQT gốc có 422 cặp dữ liệu, vì vậy chúng tôi sử dụng nó như vậy mà không lấy mẫu dữ liệu như CoSQA.

Chúng tôi dịch các truy vấn ngôn ngữ tự nhiên trong các bộ test này sử dụng cùng mô hình dịch máy và cài đặt tham số như việc dịch dữ liệu huấn luyện.

4.3 Cài đặt Mô hình

Chúng tôi chuẩn bị ba cài đặt mô hình khác nhau về lượng và mẫu dữ liệu huấn luyện.

No-pre-training: Một mô hình XLM-R không có huấn luyện thêm nào được áp dụng và các tham số ban đầu của nó được sử dụng.

All-to-One: Một mô hình sử dụng các cặp dữ liệu của truy vấn đa ngôn ngữ và mã đơn ngôn ngữ để tiền huấn luyện. Kích thước dữ liệu tiền huấn luyện dao động từ 1,2 triệu đến 2,7 triệu, tùy thuộc vào ngôn ngữ lập trình.

All-to-All: Một mô hình sử dụng các cặp dữ liệu của truy vấn đa ngôn ngữ và mã đa ngôn ngữ để tiền huấn luyện. Kích thước dữ liệu tiền huấn luyện là hơn 7,6 triệu.

4.4 Kết quả

Bảng 3 cho thấy điểm số của đánh giá MRR dưới tất cả các cài đặt. Điểm số với CSN cho thấy All-to-All thực hiện tốt nhất trong Go, Java và PHP ở hầu hết tất cả các ngôn ngữ tự nhiên. Mặt khác, All-to-One cho thấy điểm số tốt hơn All-to-All trên bộ test Python. Có thể hiệu suất đã đạt đỉnh ở All-to-One trên bộ test Python, vì sự khác biệt trong điểm số giữa All-to-One và All-to-All tương đối nhỏ (<0,1). Trên CoSQA và WQT, cũng có các trường hợp mà các cài đặt mô hình khác ngoài All-to-All thực hiện tốt hơn.

Hiệu suất của CodeBERT gốc trên nhiệm vụ tìm kiếm mã được hiển thị trong Bảng 4. Nhìn chung, All-to-All ngang bằng với hiệu suất của CodeBERT trong dữ liệu tiếng Anh. Đặc biệt, All-to-All đạt điểm số tốt hơn trong Java và PHP so với CodeBERT. Lưu ý rằng các thí nghiệm của chúng tôi và của CodeBERT khác nhau về số lượng bộ test được sử dụng. Do đó, khó có thể so sánh trực tiếp các điểm số này để thảo luận về ưu thế của mô hình.

Chúng tôi quan sát một xu hướng dần dần rằng điểm số giảm ở tiếng Anh và tiếng Pháp và tăng ở tiếng Nhật và tiếng Trung khi chúng tôi tăng kích thước dữ liệu tiền huấn luyện. Hiện tượng này có thể do sự khác biệt trong kiến thức về các ngôn ngữ này được thu được trong quá trình tiền huấn luyện XLM-R. Dữ liệu tiền huấn luyện XLM-R chứa khoảng 350 GiB cho tiếng Anh và tiếng Pháp và khoảng 69 GiB và 46 GiB cho tiếng Nhật và tiếng Trung, tương ứng. Khi các tham số của XLM-R được cập nhật trong quá trình tiền huấn luyện của chúng tôi, kiến thức về tiếng Anh và tiếng Pháp mà mô hình ban đầu có đã bị mất. Mặt khác, điểm số của tiếng Nhật và tiếng Trung, trong đó mô hình sở hữu một lượng nhỏ dữ liệu, đã được cải thiện bằng cách tăng kích thước dữ liệu.

5 Phân tích về Chất lượng Dịch thuật

5.1 Lọc Dịch ngược

Chất lượng dịch thuật của tập dữ liệu của chúng tôi phải ảnh hưởng đến hiệu suất nhiệm vụ của mô hình. Do đó, chúng tôi điều tra xem có sự khác biệt trong điểm số của nhiệm vụ tìm kiếm mã hay không khi chúng tôi lọc bỏ dữ liệu chất lượng thấp từ tập dữ liệu tinh chỉnh.

Chúng tôi áp dụng phương pháp lọc dịch ngược dựa trên các nghiên cứu trước đây đã sử dụng dịch máy để tự động xây dựng tập dữ liệu đa ngôn ngữ chất lượng cao từ tập dữ liệu tiếng Anh (Sobrevilla Cabezudo et al., 2019; Dou et al., 2020; Yoshikoshi et al., 2020). Đầu tiên, chúng tôi áp dụng dịch ngược cho các docstring tiếng Pháp, tiếng Nhật và tiếng Trung. Sau đó, chúng tôi tính điểm BLEU đơn ngữ (Papineni et al., 2002) giữa các docstring được dịch ngược và các docstring tiếng Anh gốc và chỉ thu thập dữ liệu có điểm số cao hơn các ngưỡng nhất định. Trong các thí nghiệm của chúng tôi, chúng tôi tiến hành lọc đối với tập dữ liệu tinh chỉnh của Go. Bảng 5 cho thấy kích thước dữ liệu sau khi lọc dịch ngược. Chúng tôi đặt ngưỡng từ 0,2 đến 0,7 theo bước tăng 0,1 và so sánh hiệu suất của mô hình với mỗi ngưỡng. Chúng tôi chọn các giá trị này vì kích thước của các tập dữ liệu thay đổi tương đối lớn khi được lọc với ngưỡng 0,3 đến 0,6 (Phụ lục D).

5.2 Kết quả

Bảng 6 cho thấy điểm MRR của các mô hình có dữ liệu tinh chỉnh được lọc với các ngưỡng khác nhau. Trong mọi ngôn ngữ, điểm số đạt đỉnh khi chúng tôi đặt ngưỡng từ 0,2 đến 0,5 và sau đó giảm với các ngưỡng lớn hơn lên đến 0,7. Kết quả này ngụ ý rằng việc lọc đã loại bỏ thành công dữ liệu chất lượng thấp trong khi duy trì số lượng dữ liệu huấn luyện và dẫn đến điểm MRR tốt hơn. Chúng tôi cho rằng sự thay đổi về kích thước từ tập dữ liệu gốc trở nên nổi bật hơn với các ngưỡng từ 0,5 đến 0,7 (khoảng 100K-400K), do đó cuối cùng dẫn đến việc giảm điểm số tổng thể.

Tuy nhiên, những thay đổi điểm số có vẻ không đáng kể (±0,02) giữa các ngưỡng này. Một lý do có thể là kích thước dữ liệu vẫn còn hơn 250K ngay cả sau khi lọc, điều này đã đủ cho việc tinh chỉnh nói chung.

Tóm lại, kết quả cho thấy việc lọc bỏ một số dữ liệu chất lượng thấp cải thiện hiệu suất của mô hình trên nhiệm vụ tìm kiếm mã, nhưng việc loại bỏ hơn 150K dữ liệu làm xấu đi điểm số test.

6 Kết luận

Chúng tôi đã tạo ra một tập dữ liệu tìm kiếm mã đa ngôn ngữ lớn bằng mô hình dịch máy thần kinh. Sau đó, chúng tôi xây dựng một mô hình tìm kiếm mã đa ngôn ngữ sử dụng tập dữ liệu của chúng tôi. Chúng tôi phát hiện ra rằng các mô hình được tiền huấn luyện với tất cả dữ liệu ngôn ngữ tự nhiên và ngôn ngữ lập trình đa ngôn ngữ đạt hiệu suất tốt nhất trên nhiệm vụ tìm kiếm mã hầu như mọi lúc. Chúng tôi cũng điều tra mối quan hệ giữa chất lượng dịch thuật của tập dữ liệu và hiệu suất của mô hình. Kết quả cho thấy kích thước dữ liệu đóng góp nhiều hơn cho hiệu suất tìm kiếm mã của mô hình so với chất lượng dịch thuật dữ liệu.

Nhìn chung, nghiên cứu này giới thiệu rằng việc sử dụng mô hình dịch máy có sẵn công khai giúp dịch văn bản trong lĩnh vực lập trình. Chúng tôi có thể áp dụng phương pháp của chúng tôi để mở rộng tập dữ liệu cho các ngôn ngữ khác ngoài tiếng Pháp, tiếng Nhật và tiếng Trung để xây dựng mô hình cho các ngôn ngữ tự nhiên khác nhau.

Hạn chế

Chúng tôi đã sử dụng XLM-R cho mô hình cơ bản để huấn luyện với tập dữ liệu của chúng tôi trong các thí nghiệm vì chúng tôi muốn làm cho các cài đặt thí nghiệm gần với nghiên cứu trước đây của CodeBERT nhưng cho dữ liệu đa ngôn ngữ. Vì CodeBERT dựa trên RoBERTa, chúng tôi đã chọn XLM-R, cũng dựa trên RoBERTa và đã được huấn luyện với dữ liệu đa ngôn ngữ.

Lời cảm ơn

Chúng tôi cảm ơn hai người đánh giá ẩn danh vì những bình luận và đề xuất hữu ích của họ, điều này đã cải thiện bài báo này. Nghiên cứu này được hỗ trợ bởi JSPS KAKENHI Grant Number JP20K19868 và một phần bởi Microsoft Research Asia (Collaborative Research Sponsorship).

Tài liệu tham khảo

Wasi Ahmad et al. 2021. Unified Pre-training for Program Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655–2668, Online. Association for Computational Linguistics.

Kevin Clark et al. 2020. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In International Conference on Learning Representations.

Alexis Conneau et al. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.

Jacob Devlin et al. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Zi-Yi Dou, Antonios Anastasopoulos, and Graham Neubig. 2020. Dynamic data selection and weighting for iterative back-translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5894–5904, Online. Association for Computational Linguistics.

Angela Fan et al. 2022. Beyond english-centric multilingual machine translation. The Journal of Machine Learning Research, 22(1):107:4839–107:4886.

Zhangyin Feng et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1536–1547, Online. Association for Computational Linguistics.

Daniel Fried et al. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh International Conference on Learning Representations.

Junjie Huang et al. 2021. CoSQA: 20,000+ Web Queries for Code Search and Question Answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5690–5700. Association for Computational Linguistics.

Hamel Husain et al. 2020. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. arXiv preprint arXiv:1909.09436.

Guillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291.

Yinhan Liu et al. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.

Kishore Papineni et al. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, page 311–318, USA. Association for Computational Linguistics.

Lu Shuai et al. 2021. CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664.

Marco Antonio Sobrevilla Cabezudo, Simon Mille, and Thiago Pardo. 2019. Back-translation as strategy to tackle the lack of corpus in natural language generation from semantic representations. In Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 94–103, Hong Kong, China. Association for Computational Linguistics.

Ashish Vaswani et al. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

Yue Wang et al. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696–8708, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Takumi Yoshikoshi et al. 2020. Multilingualization of a natural language inference dataset using machine translation. The 244th meeting of IPSJ Natural Language Processing, 2020(6):1–8.

Qinkai Zheng et al. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. arXiv preprint arXiv:2303.17568.

A CodeSearchNet

Bảng 1 cho thấy kích thước của CSN cho mỗi ngôn ngữ lập trình được sử dụng để tiền huấn luyện CodeBERT với MLM và tinh chỉnh trên nhiệm vụ tìm kiếm mã. Số lượng dữ liệu để tinh chỉnh trong Go được liệt kê là 635,635 trong Feng et al. (2020), nhưng tập dữ liệu được cung cấp công khai chứa 635,652 dữ liệu.

B Dịch thuật Tập dữ liệu

Chúng tôi đánh giá thủ công chất lượng dịch thuật của tập dữ liệu. Bảng 7 cho thấy các ví dụ về dịch thuật dữ liệu truy vấn từ tiếng Anh sang tiếng Nhật sử dụng M2M-100. Vì các truy vấn của CSN dựa trên mô tả mã nguồn, một số trong số chúng chứa các chuỗi không nhất thiết cần được dịch, chẳng hạn như tên biến, tên hàm và thuật ngữ kỹ thuật (ví dụ: SetStatus, retrieveCoinSupply). M2M-100 dịch thành công toàn bộ câu, để lại các chuỗi cụ thể miền như cần thiết.

Mặt khác, chúng tôi quan sát một số lỗi, chẳng hạn như dịch thành các từ không xác định (ví dụ: "alphanumeric" thành "アルファナウマリ") hoặc bỏ qua một số văn bản khỏi bản dịch.

Chúng tôi cũng chú thích thủ công các nhãn của 45 cặp dữ liệu được lấy mẫu từ tập dữ liệu tinh chỉnh của các truy vấn tiếng Nhật và mã Go và tính toán mức độ chúng khớp với các nhãn gốc. 45 cặp dữ liệu này không chứa các truy vấn không được dịch thành công và vẫn bằng tiếng Anh. Trong số 45 cặp dữ liệu, 28 trong số chúng có "1" làm nhãn và 17 cho "0". Chúng tôi tính toán mối tương quan với độ chính xác, và điểm số là 0,911.

C Cài đặt Huấn luyện

Như các siêu tham số để tiền huấn luyện mô hình, chúng tôi đặt kích thước batch là 64, độ dài đầu vào tối đa là 256, và tốc độ học là 2e-4. Như các siêu tham số cho việc tinh chỉnh mô hình, chúng tôi đặt kích thước batch là 16, tốc độ học là 1e-5, và số epoch huấn luyện tối đa là 3. Trong cả hai trường hợp, chúng tôi sử dụng Adam làm bộ tối ưu hóa.

D Lọc Dịch ngược

Bảng 8 cho thấy một ví dụ về dữ liệu bị loại bỏ bằng việc lọc. Bảng 9 cho thấy kích thước dữ liệu của mỗi ngưỡng lọc.
