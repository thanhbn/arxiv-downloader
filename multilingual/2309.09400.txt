# 2309.09400.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2309.09400.pdf
# File size: 323868 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large
Language Models in 167 Languages
Thuat Nguyen1, Chien Van Nguyen1, Viet Dac Lai1, Hieu Man1, Nghia Trung Ngo1
Franck Dernoncourt2, Ryan A. Rossi2, Thien Huu Nguyen1
1Dept. of Computer Science, University of Oregon, OR, USA
2Adobe Research, USA
nguyenhuuthuat09@gmail.com
{chienn,vietl@cs,hieum,nghian,thien@cs}@uoregon.edu
{franck.dernoncourt,ryrossi}@adobe.com
Abstract
The driving factors behind the development
of large language models (LLMs) with im-
pressive learning capabilities are their colos-
sal model sizes and extensive training datasets.
Along with the progress in natural language
processing, LLMs have been frequently made
accessible to the public to foster deeper inves-
tigation and applications. However, when it
comes to training datasets for these LLMs, es-
pecially the recent state-of-the-art models, they
are often not fully disclosed. Creating training
data for high-performing LLMs involves exten-
sive cleaning and deduplication to ensure the
necessary level of quality. The lack of trans-
parency for training data has thus hampered
research on attributing and addressing hallu-
cination and bias issues in LLMs, hindering
replication efforts and further advancements
in the community. These challenges become
even more pronounced in multilingual learn-
ing scenarios, where the available multilingual
text datasets are often inadequately collected
and cleaned. Consequently, there is a lack of
open-source and readily usable dataset to effec-
tively train LLMs in multiple languages. To
overcome this issue, we present CulturaX, a
substantial multilingual dataset with 6.3 tril-
lion tokens in 167 languages, tailored for LLM
development. Our dataset undergoes meticu-
lous cleaning and deduplication through a rig-
orous pipeline of multiple stages to accom-
plish the best quality for model training, in-
cluding language identification, URL-based
filtering, metric-based cleaning, document re-
finement, and data deduplication. CulturaX is
fully released to the public in HuggingFace to
facilitate research and advancements in mul-
tilingual LLMs: https://huggingface.co/
datasets/uonlp/CulturaX .
1 Introduction
Large language models (LLMs) have fundamen-
tally transformed research and applications of nat-
ural language processing (NLP), significantly ad-vancing the state-of-the-art performance for nu-
merous tasks and revealing new emergent abilities
(Brown et al., 2020; Wei et al., 2022). Based on
the transformer architecture (Vaswani et al., 2017),
three major variants of LLMs have been explored
in the literature: the encoder-only models to en-
code input texts into representation vectors, e.g.,
BERT (Devlin et al., 2019) and RoBERTa (Liu
et al., 2019); the decoder-only models to generate
texts, e.g., GPT (Radford et al., 2019; Brown et al.,
2020); and the encoder-decoder models to per-
form sequence-to-sequence generation, e.g., BART
(Lewis et al., 2020) and T5 (Raffel et al., 2020).
The remarkable capabilities of LLMs have pri-
marily been propelled by the ever-expanding scale
of model sizes and training datasets, which have
been deemed essential for achieving optimal perfor-
mance by the scaling laws (Hernandez et al., 2022).
For instance, beginning with the BERT model,
which had a mere few hundred million parame-
ters (Devlin et al., 2019), recent GPT-based models
have been expanded to encompass hundreds of bil-
lions of parameters (Shoeybi et al., 2019; Scao
et al., 2022; Lieber et al., 2021; Chowdhery et al.,
2022). Similarly, the training datasets for LLMs
have grown exponentially, evolving from a modest
13GB of text data from Wikipedia and books used
for BERT (Devlin et al., 2019; Liu et al., 2019)
to consume terabytes of data for the latest mod-
els, such as Falcon (Penedo et al., 2023), MPT
(MosaicML, 2023), LLaMa (Touvron et al., 2023),
PolyLM (Wei et al., 2023) and ChatGPT1.
As the field keeps progressing rapidly, pre-
trained LLMs have typically been released to the
public to foster further research and advancements.
These models are obtainable either through com-
mercial APIs, as illustrated by ChatGPT and GPT-
4, or via open-source initiatives, exemplified by
Falcon and LLaMa. Nevertheless, in contrast
to the public accessibility of LLMs, the training
1https://openai.com/blog/chatgptarXiv:2309.09400v1  [cs.CL]  17 Sep 2023

--- PAGE 2 ---
datasets that underpin the state-of-the-art mod-
els have mostly remained closely guarded secrets,
even in the case of open-source LLMs such as
BLOOM, LLaMa, MPT, and Falcon. For exam-
ple, Falcon (Penedo et al., 2023) and BLOOM
(Scao et al., 2022) only provide a glimpse of their
complete training data, whereas MPT’s, LLaMa’s
and PolyLM’s datasets (Touvron et al., 2023; Wei
et al., 2023) remain inaccessible to the public. On
one hand, the lack of transparency has impeded in-
depth analysis and comprehension of LLMs, hin-
dering crucial research into attributing and address-
ing fundamental issues stemming from the training
data, such as hallucinations, biases, and toxic con-
tent (Tamkin et al., 2021; Weidinger et al., 2021;
Kenton et al., 2021; Bommasani et al., 2021). On
the other hand, concealing the training data restricts
the development of LLMs to a select few stakehold-
ers with ample resources, thereby constraining the
democratization and benefits of the technology and
exacerbating its biases within broader society.
To attain transparency and democratization for
LLMs, it is thus crucial to create large-scale and
high-quality datasets for training high-performing
LLMs while ensuring their public accessibility to
foster deeper research and advancements. In the
realm of LLMs, high-quality training datasets are
often crafted through the application of extensive
data cleaning and deduplication processes, aimed at
eliminating noisy and redundant content from vast
text collections (Allamanis, 2018; Penedo et al.,
2023). To this end, there have been recent efforts
from the community to develop such open-source
datasets for LLMs, such as RedPajama with 1.21T
tokens (Computer, 2023), SlimPajama2with 627B
tokens, and AI2 Dolma3with 3T tokens. How-
ever, most of the existing open-source datasets for
LLMs are tailored for the English language, which
hinders the utilization and performance of the re-
sulting LLMs when applied to non-English lan-
guages, particularly those with limited linguistic
resources (Bang et al., 2023; Lai et al., 2023). This
emphasis on English also restricts the capacity of
open-source datasets to comprehensively tackle the
research challenges and democratization concerns
of LLMs across the diverse spectrum of over 7,000
languages spoken worldwide.
2https://www.cerebras.net/blog/slimpajama-a-6
27b-token-cleaned-and-deduplicated-version-of-r
edpajama
3https://blog.allenai.org/dolma-3-trillion-to
kens-open-llm-corpus-9a0ff4b8da64Simultaneously, some multilingual datasets have
been developed and made available, providing text
data for multiple languages. Nevertheless, their
quality and scale fall short of meeting the require-
ments for training high-performing LLMs. Specif-
ically, the multilingual text dataset sourced from
Wikipedia, while of high quality, is regarded as
relatively small when it comes to training LLMs
(Conneau et al., 2020). The OSCAR datasets (Or-
tiz Suárez et al., 2019; Ortiz Suárez et al., 2020;
Abadji et al., 2021, 2022)4extract text data from
CommonCrawl (CC) for more than 160 languages.
However, these datasets lack document-level dedu-
plication (i.e., removing similar documents in the
dataset), leading to the inclusion of redundant in-
formation and impairing the performance of gener-
ative LLMs (Lee et al., 2022). Similarly, the mC4
(Xue et al., 2021), CCAligned (Conneau et al.,
2020), WikiMatrix (Schwenk et al., 2021), and
ParaCrawl (Bañón et al., 2020) datasets altogether
support over 100 languages but suffers from less
accurate language identification, introducing noise
into the data (Kreutzer et al., 2022). These datasets
are also not deduplicated at fuzzy and document
levels, e.g., via MinHash (Broder, 1997). Addi-
tionally, the CC100 dataset (Wenzek et al., 2020;
Conneau et al., 2020), employed in training the
multilingual XLM-RoBERTa model across 100 lan-
guages, only considers the snapshots of CC in 2018,
constraining its size and the availability of up-to-
date information to train high-performing LLMs.
To address the aforementioned issues for open-
source datasets, our work introduces a novel multi-
lingual dataset, called CulturaX, for training LLMs
in 167 languages. CulturaX merges the latest it-
eration of mC4 (version 3.1.0) with all available
OSCAR corpora up to the current year, encompass-
ing distributions 20.19, 21.09, 22.01, and 23.01.
This amalgamation results in a large multilingual
dataset, comprising 27 TB of text data with 6.3
trillion tokens and offering the most up-to-date
data for LLM development. More than half of
our dataset is dedicated to non-English languages
to significantly boost the data size and enhance
the feasibility of training models in multilingual
scenarios. Importantly, CulturaX is extensively
cleaned and deduplicated at the document level
to produce the highest quality to train LLMs for
multiple languages. In particular, our data clean-
ing process includes a comprehensive pipeline de-
4https://oscar-project.org

--- PAGE 3 ---
signed to eliminate low-quality data. This involves
removing noisy text, non-linguistic content, toxic
data, incorrect language identification, and more.
Our data cleaning pipeline employs a variant of
the Interquartile Range (IQR) method (Dekking
et al., 2007) to select appropriate thresholds for var-
ious dataset metrics (e.g., stopword ratios, data per-
plexity, and language identification scores), which
can be used to filter noisy outliers for the dataset.
As such, we leverage the percentiles of the distri-
butions computed over large samples of data to
effectively guide the threshold selection process
for each filtering metric and language. Finally,
we perform extensive deduplication for the data
of the languages within our datasets based on the
near deduplication method MinHashLSH (Broder,
1997; Leskovec et al., 2020) and URLs, leading
to high-quality data to train multilingual LLMs.
Our dataset will be fully available to the public to
promote further research and development for mul-
tilingual learning. To our knowledge, CulturaX is
the largest open-source multilingual dataset to date
that is deeply cleaned and deduplicated for LLM
and NLP applications.
2 Multilingual Dataset Creation
To develop a multilingual public dataset for LLMs,
our strategy is to combine mC4 (Xue et al., 2021)
and OSCAR (Ortiz Suárez et al., 2019; Abadji et al.,
2021, 2022), two largest multilingual datasets at
our disposal. We then process the data with an ex-
tensive pipeline, involving two major steps of clean-
ing and deduplication, to produce an enormous and
high-quality dataset for multilingual LLMs.
mC4 is a multilingual document-level dataset,
originally created to train the multilingual encoder-
decoder model mT5 (Xue et al., 2021) for 101 lan-
guages. This dataset is extracted from 71 monthly
snapshots from CC by removing pages with less
than three long lines (line length filter), pages
with bad words, and duplicated lines across doc-
uments. Language identification for the pages in
mC4 is done by the cld3 tool (Botha et al., 2017)5,
which is a small feed-forward network (Xue et al.,
2021). Any pages with a language confidence be-
low 0.95% are excluded. mC4 is deduplicated with
exact match at the document level; however, fuzzy
document-level deduplication is not performed. We
utilize the latest version of mC4 (version 3.1.0)6
5https://github.com/google/cld3
6https://huggingface.co/datasets/mc4
mC466%OSCAR 20.197%OSCAR 21.099%OSCAR 22.017%OSCAR 23.0111%Figure 1: Distribution of document counts from mC4
and OSCAR in our initial dataset.
prepared by AllenAI in this work.
A notable aspect of our dataset pertains to the
web-based origin of our selected datasets, mC4
and OSCAR, extracted from CC. This differs
from certain previous work (Radford et al., 2019;
MosaicML, 2023; Touvron et al., 2023) that has
also relied on curated datasets like The Pile (Gao
et al., 2020) and BookCorpus (Zhu et al., 2015) to
train LLMs, presuming their higher overall qual-
ity. However, in the context of multilingual set-
tings, we argue that web-scraped datasets can be
a more suitable approach, as curated datasets of
superior quality might not be available for various
languages. Our strategy of using web-scraped data
facilitates efficient data collection across multiple
languages, contributing to enhanced training data
scales. Furthermore, recent studies have demon-
strated the effectiveness of cleaning web-scraped
data to yield state-of-the-art LLMs (Raffel et al.,
2020; Almazrouei et al., 2023). In total, the com-
bination of mC4 and OSCAR provides us 13.5B
documents for further processing. Figure 1 illus-
trates the distribution of the document counts for
mC4 and the four available versions of OSCAR in
our initial dataset.
2.1 Data Cleaning
Given the combination of the mC4 and OSCAR
datasets, we first perform a comprehensive data
cleaning procedure to remove noisy and bad con-
tent from the data, including language identifica-
tion, ULR-based filtering, metric-based cleaning,
and document refinement.

--- PAGE 4 ---
Language Identification : A particular issue
concerns the use of two different language iden-
tification tools, i.e., cld3 and FastText, for mC4
and OSCAR (respectively). It has been shown in
previous studies that cld3 is significantly worse
than FastText, causing substantially more language
detection errors for mC4 (Kreutzer et al., 2022). In
fact, compared to several other language detectors,
FastText has demonstrated state-of-the-art perfor-
mance over benchmark datasets7. To this end, our
first data cleaning step involves applying FastText
to re-predict the languages for the documents in
mC4. Documents whose predicted languages are
different from the provided ones in mC4 will be
removed from the dataset. The rationale is to avoid
documents that are confusing for the language de-
tectors cld3 and FastText, thus potentially intro-
ducing noise for the data. Finally, to ensure the
highest quality, we remove data for any language
found in mC4 but not supported by FastText.
URL-based Filtering : In the next step, we aim
to eliminate pages from the known toxic and harm-
ful sources to reduce relevant risks from our data.
In particular, we leverage the latest UT1 blacklist of
URLs and domains provided by the University of
Toulouse to support Internet use regulation for ad-
ministrators at schools. This list involves sites from
different topics, including pornography, grumbling,
and hacking, that should be discarded for LLM
training. Updated twice to thrice per week, the
blacklist involves more than 3.7M records that are
contributed by both human and robots (e.g., search
engines, known addresses and indexes) (Abadji
et al., 2022). As such, we remove any page from
our dataset whose associated URL matches a site
in the blacklist. This step is helpful for our dataset
as the blacklist is not employed before for the mC4
dataset. In addition, although OSCAR has already
used this blacklist for data cleaning, our approach
incorporates the most up-to-date information from
the list, which might not be available for the current
distributions of OSCAR.
Metric-based Cleaning : To enhance the
dataset’s quality, motivated by the data process-
ing pipeline from the BigScience’s ROOTS corpus
for BLOOM (Laurençon et al., 2022; Scao et al.,
2022), we further utilize the distributions for var-
ious dataset metrics to identify and filter outlying
documents. Each metric provides a singular value
7https://modelpredict.com/
language-identification-surveyfor every document within the dataset, quantify-
ing specific attributes such as number_words ,stop-
word_ratios , and perplexity_score for each doc-
ument. For each metric and its range of possi-
ble values within the dataset, a threshold will be
determined to partition the range into two zones:
a normal range and an abnormal range. The ab-
normal range is designated for documents exhibit-
ing metric values significantly deviating from the
norm, classifying them as outliers/noises, and con-
sequently, these outliers are removed from our
dataset. As such, we employ a comprehensive ar-
ray of dataset metrics, which will be collectively
employed to refine our dataset, as outlined below:
• Number of words
• Character repetition ratio
• Word repetition ratio
• Special character ratio
• Stop word ratio
• Flagged word ratio
• Language identification confidence
• Perplexity score
• Document length (number of characters)
• Number of lines
• Short line length ratio
• Short line ratio
The last four metrics are suggested by the OS-
CAR dataset while the others are inherited from the
BigScience ROOTS corpus’s pipeline to process
OSCAR data. For the perplexity score, following
the BigScience ROOTS corpus, we train a Senten-
cePiece tokenizer (Kudo, 2018) and 5-gram Kneser-
Ney language models as provided in the KenLM
library (Heafield, 2011) using the 20230501 dumps
of Wikipedia. Documents displaying high perplex-
ity scores based on these KenLM models are con-
sidered notably different from Wikipedia articles.
This indicates a level of noise that will be excluded
from our dataset (Wenzek et al., 2020). The tok-
enizer will also be used to obtain the number of
words/tokens in the documents for our metrics. We
publicly release our KenLM models in Hugging-
Face8to faciliate future exploration.
Repeated information (e.g., words, paragraphs)
can appear in the web-curated data due to crawling
errors and low-quality sources, causing detrimental
consequences for training LLMs (Holtzman et al.,
2019). The character and word repetition ratios
are thus designed to avoid documents with exces-
8https://huggingface.co/uonlp/kenlm

--- PAGE 5 ---
sively repeated information. High frequencies of
special characters, stop words, or flagged words
can indicate noisy and low-quality documents. We
thus utilize the stop word and flagged word lists for
different languages to compute their ratios for doc-
ument removal. In addition to the stop word and
flagged word lists provided by BigScience ROOTS
for their 13 languages, we further collect dictionar-
ies for these types of words for other languages.
We prioritize the lists that have been shared on
personal GitHub accounts for various languages,
as these are often crafted by native speakers and
exhibit higher quality. Moreover, lower language
identification confidence might also suggest noisy
language structures for the data. For each document
in the dataset, we thus obtain a language identifi-
cation confidence via the probability that FastText
assigns to its corresponding language to aid data
filtering. Finally, for the short line-based criteria,
we implement a threshold of 100 characters to clas-
sify lines as short, as used by OSCAR. Documents
with excessive occurrence of short lines will not be
retained in our dataset.
Threshold Selection : Given the set of dataset
metrics, an important question concerns the selec-
tion of appropriate thresholds for each metric and
language to generate high-quality multilingual data.
In the BigScience ROOTS project (Laurençon et al.,
2022), this selection process is carried out by native
speakers of 13 languages. The resulting thresholds
are employed for the rest of their 46 languages.
The project offers a visualization interface that in-
dexes a sample of a few thousand documents per
language, enabling users to monitor data statistics
as they adjust thresholds for the metrics. However,
this process cannot be easily extended to different
languages due to the requirement of experienced
native speakers, which incurs significant costs. Fur-
thermore, the limited sample sizes hinder the repre-
sentativeness of the chosen thresholds for the full
datasets. In our analysis, we observe that some
selected thresholds for certain languages within
BigScience ROOTS almost fall outside the value
ranges for the entire dataset, leading to the deacti-
vation of the corresponding metrics.
To address these issues, we leverage a variant
of the Interquartile Range (IQR) method (Dekking
et al., 2007) to select appropriate thresholds for
the filtering metrics for our dataset. For each met-
ric and language, we generate a distribution of its
possible values across the entire dataset for the lan-guage. There is an exception for languages with
substantial amounts of data, such as Spanish and
Russian, where only 25% of the data is used to cal-
culate these distributions. Afterward, we compute
theQ1-th and Q3-th percentiles of the distribution
(Q1< Q 3) and use them for the thresholds for
our filtering metrics. In particular, the lower Q1-
th percentile will be chosen for the metrics that
favor high values (e.g., language identification con-
fidence), while metrics favoring low values (e.g.,
perplexity scores and document length) will uti-
lize the upper Q3-th percentile. We investigate
different values for (Q1, Q3), considering (25,75),
(20,80),(15,85),(10,90), and (5,95). The selec-
tion of Q1= 10 andQ2= 90 has achieved the
best data quality for a sample of languages in our
examination.
It is worth emphasizing that the utilization of
percentiles for threshold selection enables our ap-
proach to efficiently draw upon more extensive data
samples for each language compared to those em-
ployed in the BigScience ROOTS project. This re-
sults in more reliable thresholds for the full datasets
over different languages. Specifically, concerning
the large languages where only a 25% data sample
is employed to compute the value distribution for a
metric, we observe that the proportion of discarded
data to the entire dataset closely aligns with that of
the data sample when applying the same selected
filtering threshold. This underscores the represen-
tativeness of the thresholds selected through our
methodology. Finally, once the thresholds for the
metrics in a given language have been determined,
we will eliminate any document that surpasses a
metric’s threshold and enters the unfavorable range
of the data.
Document Refinement : The previous cleaning
steps are done at the dataset level, aiming to remove
low-quality documents from the dataset. In this
step, we further clean the retained documents to
improve the quality. It is important to note that our
prior metric-based filtering step plays a vital role in
eliminating highly noisy documents, which, in turn,
streamlines the process of developing effective doc-
ument cleaning rules during this step. Notably,
since the documents from mC4 and OSCAR are
extracted from HTML pages crawled from the Inter-
net, a significant portion of them may carry crawl-
ing and extraction errors, including long JavaScript
lines and extraneous content. Consequently, filter-
ing out these documents greatly simplifies our task

--- PAGE 6 ---
of designing rules to clean the documents within
our dataset.
As such, for each document, we eliminate its
noisy or irrelevant portions via a series of oper-
ations. First, we remove any short lines located
at the end of each document, as these lines typi-
cally contain footer details or unhelpful informa-
tion from the websites. Second, we eliminate the
lines that contain words from our list of JavaScript
(JS) keywords (e.g., “ <script ”) to avoid irrelevant
and non-linguistic information. Here, we exclu-
sively remove JS lines if the document contains
just one line with JS keywords, and this particular
line must also feature at least two different types
of JS keywords. We adopt this approach as docu-
ments with more than two JS lines are likely coding
tutorials in our data, which should be preserved to
improve diversity. In addition, certain JS keywords
are used in natural language, e.g., “ var”. By re-
quiring at least two different types of JS keywords,
we reduce the risk of inadvertently omitting helpful
content and disrupting the document’s structure.
2.2 Data Deduplication
Despite thorough data cleaning, the remaining
dataset might still contain a substantial amount
of repeated data due to various reasons, including
information being reposted on the web, multiple
references to the same articles, boilerplate content,
and plagiarism. The duplicated data can thus cause
memorization and significantly hinder generaliza-
tion for LLMs (Lee et al., 2022; Hernandez et al.,
2022). Although expensive, data deduplication
is thus considered as a crucial step to guarantee
the highest quality of data for training LLMs. To
this end, we undertake a comprehensive deduplica-
tion procedure for our dataset, utilizing MinHash
(Broder, 1997) and URLs. This deduplication pro-
cess is carried out independently for each language.
Furthermore, we restrict deduplication to languages
that retain over 100K documents following our data
cleaning procedures (i.e., 51.5% of our languages),
aiming to promote smaller languages within our
dataset.
MinHash Deduplication : For each language’s
dataset, we first apply the MinHashLSH method
(Leskovec et al., 2020) to filter similar documents
in the dataset. MinHashLSH is a near deduplication
technique based on MinHash (Broder, 1997) with
multiple hash functions for n-grams and the Jac-
card similarity. Locality-Sensitive Hashing (LSH)is incorporated to improve efficiency by focusing
on document pairs that are most likely similar. We
leverage a variant of the Spark implementation of
MinHashLSH in the text-dedup repo9, employing
5-grams and a threshold of 0.8to determine similar
documents for the Jaccard similarity. Running Min-
HashLSH for each language’s dataset, especially
for languages with the largest data volumes like
English, Russian, Spanish, and Chinese, represents
the most computationally expensive operation in
our dataset creation effort.
URL-based Deduplication : Finally, we elimi-
nate all documents that share identical URLs with
other documents in the dataset. This step is neces-
sary to address situations where various versions of
the same articles are linked to identical URLs but
have been updated or modified during the publica-
tion process, effectively bypassing the near dedu-
plication step. Some URLs for the articles in CC
might only display their general domains due to
crawling errors. To enhance accuracy, we refrain
from removing URLs that only include their gen-
eral domains.
We utilize 600 AWS c5.24xlarge EC2 instances
to preprocess and deduplicate our multilingual
dataset. Each instance is equipped with 96 CPU
cores, 192GB of memory, and 1TB of disk space.
The disk space can be used to replace memory
when necessary (e.g., for data deduplication).
3 Data Analysis and Experiments
After completing all the cleaning and deduplication
steps, our ultimate dataset comprises 6.3 trillion to-
kens spanning 167 languages. Table 1 provides an
overview of the number of documents and tokens
for the top 42 languages in CulturaX following
each processing stage. As can be seen, our data-
cleaning pipeline can substantially reduce the num-
ber of documents in the original mC4 and OSCAR
datasets for each language. The total number of
removed documents accounts for 46.48% of our
initial documents, suggesting the the effectiveness
of our approaches to filter noisy information for
multilingual datasets.
4 Related Work
Compared to other NLP tasks, language models
can be trained with unlabeled data, enabling effi-
cient data collection to produce gigantic scales for
9https://github.com/ChenghaoMou/text-dedup/
tree/main

--- PAGE 7 ---
Code Language#Documents (M) #Tokens
InitialURL Metric MinHash URL Filtering (B) (%)
Filtering Filtering Dedup Dedup Rate (%)
en English 5783.24 5766.08 3586.85 3308.30 3241.07 43.96 2846.97 45.13
ru Russian 1431.35 1429.05 922.34 845.64 799.31 44.16 737.20 11.69
es Spanish 844.48 842.75 530.01 479.65 450.94 46.60 373.85 5.93
de German 863.18 861.46 515.83 447.06 420.02 51.34 357.03 5.66
fr French 711.64 709.48 439.69 387.37 363.75 48.89 319.33 5.06
zh Chinese 444.37 444.03 258.35 222.37 218.62 50.80 227.06 3.60
it Italian 406.87 406.04 254.72 226.42 211.31 48.06 165.45 2.62
pt Portuguese 347.47 346.76 217.21 200.11 190.29 45.24 136.94 2.17
pl Polish 270.12 269.73 170.86 151.71 142.17 47.37 117.27 1.86
ja Japanese 247.67 247.19 137.88 114.64 111.19 55.11 107.87 1.71
vi Vietnamese 182.88 182.72 118.67 108.77 102.41 44.00 98.45 1.56
nl Dutch 238.92 238.56 148.19 125.51 117.39 50.87 80.03 1.27
ar Arabic 132.88 132.65 84.84 77.65 74.03 44.29 69.35 1.10
tr Turkish 183.65 183.47 109.94 99.18 94.21 48.70 64.29 1.02
cs Czech 136.91 136.44 80.38 69.01 65.35 52.27 56.91 0.90
fa Persian 118.55 118.50 70.26 62.42 59.53 49.78 45.95 0.73
hu Hungarian 88.59 88.21 53.29 46.89 44.13 50.19 43.42 0.69
el Greek 100.77 100.68 61.43 54.33 51.43 48.96 43.15 0.68
ro Romanian 89.37 89.25 45.99 42.8 40.33 54.87 39.65 0.63
sv Swedish 103.04 102.76 58.67 52.09 49.71 51.76 38.49 0.61
uk Ukrainian 81.50 81.44 50.95 47.12 44.74 45.10 38.23 0.61
fi Finnish 59.85 59.80 36.69 32.15 30.47 49.09 28.93 0.46
ko Korean 46.09 45.85 25.19 21.17 20.56 55.39 24.77 0.39
da Danish 53.16 52.99 28.67 26.48 25.43 52.16 22.92 0.36
bg Bulgarian 47.01 46.90 28.09 25.45 24.13 48.67 22.92 0.36
no Norwegian 40.07 40.01 20.69 19.49 18.91 52.81 18.43 0.29
hi Hindi 35.59 35.50 22.01 20.77 19.67 44.73 16.79 0.27
sk Slovak 40.13 39.95 22.20 19.56 18.58 53.70 16.44 0.26
th Thai 49.04 48.96 26.20 21.93 20.96 57.26 15.72 0.25
lt Lithuanian 27.08 27.01 15.87 14.25 13.34 50.74 14.25 0.23
ca Catalan 31.13 31.12 18.99 16.46 15.53 50.11 12.53 0.20
id Indonesian 48.08 48.05 25.79 23.74 23.25 51.64 12.06 0.19
bn Bangla 20.90 20.85 13.82 13.22 12.44 40.48 9.57 0.15
et Estonian 16.20 16.15 9.69 8.45 8.00 50.62 8.81 0.14
sl Slovenian 15.46 15.39 8.00 7.60 7.34 52.52 8.01 0.13
lv Latvian 14.14 14.09 8.37 7.48 7.14 49.50 7.85 0.12
he Hebrew 10.78 10.77 5.90 4.77 4.65 56.86 4.94 0.08
sr Serbian 7.80 7.75 4.80 4.25 4.05 48.08 4.62 0.07
ta Tamil 8.77 8.75 5.27 4.94 4.73 46.07 4.38 0.07
sq Albanian 9.40 9.38 5.96 5.04 5.21 44.57 3.65 0.06
az Azerbaijani 9.66 9.65 5.73 5.24 5.08 47.41 3.51 0.06
Total (42 languages) 13397.79 13366.17 8254.28 7471.48 7181.40 46.40 6267.99 99.37
Total (167 languages) 13506.76 13474.94 8308.74 7521.23 7228.91 46.48 6308.42 100.00
Table 1: Data statistics for 42 languages with the percentages of tokens greater than 0.05% in our dataset. Columns
grouped with the “#Documents (M)” label indicate the number of documents for each language after the correspond-
ing cleaning and reduplication steps. The token counts are based on our final dataset (i.e., after all the cleaning and
deduplication steps).

--- PAGE 8 ---
the training data. There are two primary types of
data commonly used for training LLMs: curated
data and web crawl data. Curated data typically
consists of well-written and well-formatted text
from targeted sources and domains, e.g., Wikipedia
articles, books, newswire articles, and scientific
papers, as used for the “The Pile” (Gao et al., 2020)
and “BookCorpus” (Zhu et al., 2015) datasets. In
contrast, web crawl data encompasses text gathered
from a wide array of sources across the internet,
varying significantly in terms of format and writing
styles, e.g., blogs, social media posts, news arti-
cles, and advertisements. CommonCrawl (CC) is a
widely-used web crawl repository that has collected
petabytes of data over the Internet for 12 years. To
this end, curated data is frequently considered to
possess higher quality, which has resulted in its
preference for training early LLMs, e.g., BERT
(Devlin et al., 2019) and GPT-2 (Radford et al.,
2019). However, as the demand for larger models
has grown, web crawl data has gained more atten-
tion as it contributes a substantial portion to the
training data of recent LLMs, e.g., RoBERTa (Liu
et al., 2019), BART (Lewis et al., 2020), T5 (Raf-
fel et al., 2020), GPT-3 (Rae et al., 2021), LLaMa
(Touvron et al., 2023), MPT (MosaicML, 2023),
and Falcon (Almazrouei et al., 2023). As such,
different extractions of CC has been produced to
train such LLMs, including C4 (Raffel et al., 2020),
CC-News (Nagel), and STORIES (Trinh and Le,
2018).
Regarding the accessibility of training data,
datasets used to train early LLMs are often made
available to the public (Devlin et al., 2019; Raffel
et al., 2020). However, in the case of the most
recent state-of-the-art (SOTA) generative LLMs,
their training datasets are not released fully, po-
tentially due to commercial interests. This applies
not only to proprietary models like ChatGPT and
GPT-4 but also to models that claim to be open-
source models such as LLaMa, MPT, Falcon, and
BLOOM (Scao et al., 2022). To address the trans-
parency issue with existing LLMs, recent efforts
have been made to replicate and release the train-
ing datasets for the state-of-the-art LLMs, i.e., Red-
Pajama (Computer, 2023), SlimPajama, and AI2
Dolma. The key distinctions for these datasets
concern their large-scale text data that has been
meticulously cleaned and document-level dedupli-
cated to ensure high quality for training LLMs.
Nonetheless, a common drawback of these open-source datasets is that they remain predominantly
focused on English data, offering limited data for
other languages.
To obtain a multilingual large-scale dataset for
training LLMs, it is more convenient to exploit
web-scrape datasets such as CC to enable efficient
data collection with up-to-date information in mul-
tiple languages. In addition, to ensure high quality
for high-performing LLMs, it is necessary to ex-
tensively clean and deduplicate the multilingual
data to avoid noisy and irrelevant content, e.g.,
low-quality machine-generated text and adult con-
tent (Trinh and Le, 2018; Kreutzer et al., 2022;
Raffel et al., 2020). As such, a typical data pro-
cessing pipeline to generate high-quality datasets
can involve multiple steps, as demonstrated by
FastText (Joulin et al., 2016), CC-Net (Wenzek
et al., 2020), the BigScience ROOTS corpus for
the BLOOM models (Laurençon et al., 2022; Scao
et al., 2022), the RefinedWeb dataset for the Fal-
con model (Penedo et al., 2023; Almazrouei et al.,
2023), and the dataset to train the LLaMa models
(Touvron et al., 2023). The first step necessitates in
such pipelines language identification to appropri-
ately assign data to their corresponding languages
(Joulin et al., 2016). The next steps features various
dataset-specific rules and heuristics to filter undesir-
able content according to the ratios of special char-
acters, short lines, bad words, among others (Grave
et al., 2018; Laurençon et al., 2022). The data can
also be filtered via lightweight models, e.g., via
the KenLM language models (Heafield, 2011), to
avoid noisy documents (Wenzek et al., 2020). Fi-
nally, data deduplication should be performed to
remove similar or repeated information (Laurençon
et al., 2022; Penedo et al., 2023). An important
step in this regard involves fuzzy deduplication at
document level, e.g., via MinHash (Broder, 1997),
to eliminate similar documents, thus mitigating
memorization and improving the generalization for
resulting LLMs (Lee et al., 2022).
To this end, while there are multilingual open-
source datasets with text data in multiple languages,
such as mC4 (Xue et al., 2021), OSCAR (Ortiz
Suárez et al., 2019), CC100 (Wenzek et al., 2020;
Conneau et al., 2020), and the BigScience ROOT
corpus (Laurençon et al., 2022), their quality and
scale do not meet the requirements for effectively
training LLMs, particularly generative models such
as GPT. For example, as highlighted in the intro-
duction, both mC4 and OSCAR lack fuzzy dedu-

--- PAGE 9 ---
plication for the data at the document level. mC4
also suffers from its poorer language identifica-
tion due to the use of cld3 . BigScience ROOTS
only provides a small sample data for 46 languages
while CC100 does not have information beyond
2018. Our dataset CulturaX thus comprehensively
addresses the issues for the existing datasets, of-
fering a multilingual, open-source, and large-scale
dataset with readily usable and high-quality data to
train LLMs.
5 Conclusion
We present CulturaX, a novel multilingual dataset
with text data for 167 languages. Our dataset
is cleaned and deduplicated via a comprehensive
pipeline, producing 6.3 trillion tokens. CulturaX is
thus a large-scale and high-quality dataset, which
can be readily used to train high-performing LLMs
for multiple languages. Our data is openly acces-
sible to the public to promote further research and
applications of multilingual learning.
References
Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and
Benoît Sagot. 2022. Towards a cleaner document-
oriented multilingual crawled corpus. In Proceedings
of the Thirteenth Language Resources and Evalua-
tion Conference , pages 4344–4355, Marseille, France.
European Language Resources Association.
Julien Abadji, Pedro Javier Ortiz Suárez, Laurent Ro-
mary, and Benoît Sagot. 2021. Ungoliant: An op-
timized pipeline for the generation of a very large-
scale multilingual web corpus. In Proceedings of
the Workshop on Challenges in the Management of
Large Corpora (CMLC-9) 2021. Limerick, 12 July
2021 (Online-Event) .
Miltiadis Allamanis. 2018. The adverse effects of code
duplication in machine learning models of code. Pro-
ceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Re-
flections on Programming and Software .
Ebtesam Almazrouei, Hamza Alobeidli, and Abdulaziz
Alshamsi et al. 2023. Falcon-40B: an open large
language model with state-of-the-art performance.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,
and Pascale Fung. 2023. A multitask, multilingual,
multimodal evaluation of chatgpt on reasoning, hal-
lucination, and interactivity. ArXiv , abs/2302.04023.
Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth
Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L.
Forcada, Amir Kamran, Faheem Kirefu, PhilippKoehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere,
Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec,
Brian Thompson, William Waites, Dion Wiggins, and
Jaume Zaragoza. 2020. ParaCrawl: Web-scale acqui-
sition of parallel corpora. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 4555–4567, Online. Association
for Computational Linguistics.
Rishi Bommasani, Drew A. Hudson, and Ehsan Adeli
et al. 2021. On the opportunities and risks of founda-
tion models. ArXiv , abs/2108.07258.
Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex
Salcianu, David Weiss, Ryan McDonald, and Slav
Petrov. 2017. Natural language processing with small
feed-forward networks. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 2879–2885, Copenhagen,
Denmark. Association for Computational Linguis-
tics.
A. Broder. 1997. On the resemblance and containment
of documents. In Proceedings of the Compression
and Complexity of Sequences .
Tom Brown, Benjamin Mann, and et al. 2020.
Language models are few-shot learners. ArXiv ,
abs/2005.14165.
Aakanksha Chowdhery, Sharan Narang, and Jacob De-
vlin et al. 2022. Palm: Scaling language modeling
with pathways. ArXiv , abs/2204.02311.
Together Computer. 2023. Redpajama: An open source
recipe to reproduce llama training dataset.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451, Online. Association for Computational Lin-
guistics.
Michel Dekking, Cornelis Kraaikamp, Hendrik Paul,
and Ludolf Erwin Meester. 2007. A modern intro-
duction to probability and statistics: Understanding
why and how. In Springer Texts in Statistics .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Leo Gao, Stella Rose Biderman, Sid Black, Laurence
Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The pile: An

--- PAGE 10 ---
800gb dataset of diverse text for language modeling.
ArXiv , abs/2101.00027.
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learning
word vectors for 157 languages. In Proceedings of
the Eleventh International Conference on Language
Resources and Evaluation (LREC 2018) , Miyazaki,
Japan. European Language Resources Association
(ELRA).
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation , pages
187–197, Edinburgh, Scotland. Association for Com-
putational Linguistics.
Danny Hernandez, Tom B. Brown, Tom Conerly, Nova
DasSarma, Dawn Drain, Sheer El-Showk, Nelson
Elhage, Zac Hatfield-Dodds, T. J. Henighan, Tristan
Hume, Scott Johnston, Benjamin Mann, Christopher
Olah, Catherine Olsson, Dario Amodei, Nicholas
Joseph, Jared Kaplan, and Sam McCandlish. 2022.
Scaling laws and interpretability of learning from
repeated data. ArXiv , abs/2205.10487.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. ArXiv , abs/1904.09751.
Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, Hervé Jégou, and Tomas Mikolov.
2016. Fasttext.zip: Compressing text classification
models. ArXiv , abs/1612.03651.
Zachary Kenton, Tom Everitt, Laura Weidinger, Ia-
son Gabriel, Vladimir Mikulik, and Geoffrey Irv-
ing. 2021. Alignment of language agents. ArXiv ,
abs/2103.14659.
Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,
Daan van Esch, Nasanbayar Ulzii-Orshikh, Allah-
sera Tapo, Nishant Subramani, Artem Sokolov, Clay-
tone Sikasote, Monang Setyawan, Supheakmungkol
Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera, An-
nette Rios, Isabel Papadimitriou, Salomey Osei, Pe-
dro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-
dre Niyongabo Rubungo, Toan Q. Nguyen, Math-
ias Müller, André Müller, Shamsuddeen Hassan
Muhammad, Nanda Muhammad, Ayanda Mnyak-
eni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-
gira, Colin Leong, Nze Lawson, Sneha Kudugunta,
Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-
ture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,
Sakine Çabuk Ballı, Stella Biderman, Alessia Bat-
tisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,
Israel Abebe Azime, Ayodele Awokoya, Duygu Ata-
man, Orevaoghene Ahia, Oghenefego Ahia, Sweta
Agrawal, and Mofetoluwa Adeyemi. 2022. Quality
at a glance: An audit of web-crawled multilingual
datasets. Transactions of the Association for Compu-
tational Linguistics , 10:50–72.
Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiplesubword candidates. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 66–75,
Melbourne, Australia. Association for Computational
Linguistics.
Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben
Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui,
and Thien Huu Nguyen. 2023. Chatgpt beyond en-
glish: Towards a comprehensive evaluation of large
language models in multilingual learning. ArXiv ,
abs/2304.05613.
Hugo Laurençon, Lucile Saulnier, Thomas Wang,
Christopher Akiki, Albert Villanova del Moral,
Teven Le Scao, Leandro V on Werra, Chenghao Mou,
Eduardo González Ponferrada, Huu Nguyen, Jörg
Frohberg, Mario Šaško, Quentin Lhoest, Angelina
McMillan-Major, Gérard Dupont, Stella Biderman,
Anna Rogers, Loubna Ben allal, Francesco De Toni,
Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor,
Maraim Masoud, Pierre Colombo, Javier de la Rosa,
Paulo Villegas, Tristan Thrush, Shayne Longpre, Se-
bastian Nagel, Leon Weber, Manuel Romero Muñoz,
Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid
Almubarak, Vu Minh Chien, Itziar Gonzalez-Dios,
Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz
Suarez, Aaron Gokaslan, Shamik Bose, David Ife-
oluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas
Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Mar-
garet Mitchell, Sasha Luccioni, and Yacine Jernite.
2022. The bigscience ROOTS corpus: A 1.6TB
composite multilingual dataset. In Thirty-sixth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track .
Katherine Lee, Daphne Ippolito, Andrew Nystrom,
Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,
and Nicholas Carlini. 2022. Deduplicating training
data makes language models better. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 8424–8445, Dublin, Ireland. Association for
Computational Linguistics.
Jure Leskovec, Anand Rajaraman, and Jeffrey David
Ullman. 2020. Mining of massive datasets. In Cam-
bridge University Press .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.
2021. Jurassic-1: Technical details and evaluation.
White Paper. AI21 Labs.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,

--- PAGE 11 ---
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv , abs/1907.11692.
MosaicML. 2023. Introducing mpt-7b: A new standard
for open-source, commercially usable llms. https:
//www.mosaicml.com/blog/mpt-7b .
Sebastian Nagel. Cc-news.
http: //web.archive.org/save/http:
//commoncrawl.org/2016/10/news- dataset-available.
Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît
Sagot. 2020. A monolingual approach to contextual-
ized word embeddings for mid-resource languages.
InProceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics , pages 1703–
1714, Online. Association for Computational Linguis-
tics.
Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent
Romary. 2019. Asynchronous pipelines for process-
ing huge corpora on medium to low resource infras-
tructures. In Proceedings of the Workshop on Chal-
lenges in the Management of Large Corpora (CMLC-
7) 2019. Cardiff, 22nd July 2019 .
Guilherme Penedo, Quentin Malartic, Daniel Hess-
low, Ruxandra-Aimée Cojocaru, Alessandro Cap-
pelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam
Almazrouei, and Julien Launay. 2023. The refined-
web dataset for falcon llm: Outperforming curated
corpora with web data, and web data only. ArXiv ,
abs/2306.01116.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.
Jack Rae, Sebastian Borgeaud, and et al. 2021. Scaling
language models: Methods, analysis & insights from
training gopher. ArXiv , abs/2112.11446.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. In Journal of Machine Learning Research .
Teven Scao, Angela Fan, and et al. 2022. Bloom: A
176b-parameter open-access multilingual language
model. ArXiv , abs/2211.05100.
Holger Schwenk, Vishrav Chaudhary, Shuo Sun,
Hongyu Gong, and Francisco Guzmán. 2021. Wiki-
Matrix: Mining 135M parallel sentences in 1620 lan-
guage pairs from Wikipedia. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume ,
pages 1351–1361, Online. Association for Computa-
tional Linguistics.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2019. Megatron-lm: Training multi-billionparameter language models using model parallelism.
ArXiv , abs/1909.08053.
Alex Tamkin, Miles Brundage, Jack Clark, and Deep
Ganguli. 2021. Understanding the capabilities, limi-
tations, and societal impact of large language models.
ArXiv , abs/2102.02503.
Hugo Touvron, Thibaut Lavril, and Gautier Izacard et al.
2023. Llama: Open and efficient foundation lan-
guage models. ArXiv , abs/2302.13971.
Trieu H. Trinh and Quoc V . Le. 2018. A simple method
for commonsense reasoning. ArXiv , abs/1806.02847.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems .
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Met-
zler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol
Vinyals, Percy Liang, Jeff Dean, and William Fedus.
2022. Emergent abilities of large language models.
Transactions on Machine Learning Research .
Xiangpeng Wei, Hao-Ran Wei, Huan Lin, Tianhao Li,
Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhi-
wei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li,
Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong
Yang, Fei Huang, and Jun Xie. 2023. Polylm: An
open source polyglot large language model. ArXiv ,
abs/2307.06018.
Laura Weidinger, John F. J. Mellor, and Maribeth Rauh
et al. 2021. Ethical and social risks of harm from
language models. ArXiv , abs/2112.04359.
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Con-
neau, Vishrav Chaudhary, Francisco Guzmán, Ar-
mand Joulin, and Edouard Grave. 2020. CCNet:
Extracting high quality monolingual datasets from
web crawl data. In Proceedings of the Twelfth Lan-
guage Resources and Evaluation Conference , pages
4003–4012, Marseille, France. European Language
Resources Association.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 483–498, On-
line. Association for Computational Linguistics.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watch-
ing movies and reading books. Proceedings of the
IEEE International Conference on Computer Vision
(ICCV) .
