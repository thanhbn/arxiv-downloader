# CulturaX: Bộ Dữ Liệu Đa Ngôn Ngữ Khổng Lồ và Được Làm Sạch 
# cho Các Mô Hình Ngôn Ngữ Lớn trong 167 Ngôn Ngữ

Thuat Nguyen1, Chien Van Nguyen1, Viet Dac Lai1, Hieu Man1, Nghia Trung Ngo1
Franck Dernoncourt2, Ryan A. Rossi2, Thien Huu Nguyen1
1Khoa Khoa học Máy tính, Đại học Oregon, Oregon, Hoa Kỳ
2Adobe Research, Hoa Kỳ
nguyenhuuthuat09@gmail.com
{chienn,vietl@cs,hieum,nghian,thien@cs}@uoregon.edu
{franck.dernoncourt,ryrossi}@adobe.com

## Tóm tắt

Các yếu tố thúc đẩy sự phát triển của các mô hình ngôn ngữ lớn (LLM) với khả năng học tập ấn tượng là quy mô mô hình khổng lồ và bộ dữ liệu huấn luyện rộng lớn. Cùng với tiến bộ trong xử lý ngôn ngữ tự nhiên, các LLM thường được công khai để thúc đẩy nghiên cứu và ứng dụng sâu hơn. Tuy nhiên, khi nói đến bộ dữ liệu huấn luyện cho các LLM này, đặc biệt là các mô hình tiên tiến gần đây, chúng thường không được công bố đầy đủ. Việc tạo dữ liệu huấn luyện cho các LLM hiệu suất cao đòi hỏi việc làm sạch và khử trùng lặp rộng rãi để đảm bảo chất lượng cần thiết. Việc thiếu minh bạch đối với dữ liệu huấn luyện đã cản trở nghiên cứu về việc quy kết và giải quyết các vấn đề ảo giác và thiên lệch trong LLM, làm cản trở nỗ lực sao chép và tiến bộ hơn nữa trong cộng đồng. Những thách thức này trở nên rõ rệt hơn trong các tình huống học đa ngôn ngữ, nơi các bộ dữ liệu văn bản đa ngôn ngữ có sẵn thường được thu thập và làm sạch không đầy đủ. Do đó, thiếu bộ dữ liệu mã nguồn mở và có thể sử dụng ngay để huấn luyện hiệu quả các LLM trong nhiều ngôn ngữ. Để khắc phục vấn đề này, chúng tôi giới thiệu CulturaX, một bộ dữ liệu đa ngôn ngữ đáng kể với 6.3 nghìn tỷ token trong 167 ngôn ngữ, được thiết kế riêng cho phát triển LLM. Bộ dữ liệu của chúng tôi trải qua quá trình làm sạch và khử trùng lặp tỉ mỉ thông qua pipeline nghiêm ngặt với nhiều giai đoạn để đạt được chất lượng tốt nhất cho huấn luyện mô hình, bao gồm nhận dạng ngôn ngữ, lọc dựa trên URL, làm sạch dựa trên metric, tinh chỉnh tài liệu và khử trùng lặp dữ liệu. CulturaX được phát hành đầy đủ công khai trên HuggingFace để hỗ trợ nghiên cứu và tiến bộ trong các LLM đa ngôn ngữ: https://huggingface.co/datasets/uonlp/CulturaX.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) đã biến đổi cơ bản nghiên cứu và ứng dụng của xử lý ngôn ngữ tự nhiên (NLP), đáng kể thúc đẩy hiệu suất tiên tiến cho nhiều tác vụ và tiết lộ những khả năng mới nổi (Brown et al., 2020; Wei et al., 2022). Dựa trên kiến trúc transformer (Vaswani et al., 2017), ba biến thể chính của LLM đã được khám phá trong tài liệu: các mô hình chỉ mã hóa để mã hóa văn bản đầu vào thành vectơ biểu diễn, ví dụ, BERT (Devlin et al., 2019) và RoBERTa (Liu et al., 2019); các mô hình chỉ giải mã để tạo văn bản, ví dụ, GPT (Radford et al., 2019; Brown et al., 2020); và các mô hình mã hóa-giải mã để thực hiện tạo chuỗi-đến-chuỗi, ví dụ, BART (Lewis et al., 2020) và T5 (Raffel et al., 2020).

Khả năng đáng chú ý của LLM chủ yếu được thúc đẩy bởi quy mô không ngừng mở rộng của kích thước mô hình và bộ dữ liệu huấn luyện, được coi là cần thiết để đạt hiệu suất tối ưu theo các quy luật tỷ lệ (Hernandez et al., 2022). Ví dụ, bắt đầu với mô hình BERT, chỉ có vài trăm triệu tham số (Devlin et al., 2019), các mô hình dựa trên GPT gần đây đã được mở rộng để bao gồm hàng trăm tỷ tham số (Shoeybi et al., 2019; Scao et al., 2022; Lieber et al., 2021; Chowdhery et al., 2022). Tương tự, các bộ dữ liệu huấn luyện cho LLM đã tăng trưởng theo cấp số nhân, phát triển từ 13GB dữ liệu văn bản khiêm tốn từ Wikipedia và sách được sử dụng cho BERT (Devlin et al., 2019; Liu et al., 2019) đến tiêu thụ terabyte dữ liệu cho các mô hình mới nhất, chẳng hạn như Falcon (Penedo et al., 2023), MPT (MosaicML, 2023), LLaMa (Touvron et al., 2023), PolyLM (Wei et al., 2023) và ChatGPT.

Khi lĩnh vực tiếp tục tiến bộ nhanh chóng, các LLM được huấn luyện trước thường được phát hành công khai để thúc đẩy nghiên cứu và tiến bộ hơn nữa. Các mô hình này có thể có được thông qua API thương mại, như được minh họa bởi ChatGPT và GPT-4, hoặc thông qua các sáng kiến mã nguồn mở, được thể hiện bởi Falcon và LLaMa. Tuy nhiên, trái ngược với khả năng truy cập công khai của LLM, các bộ dữ liệu huấn luyện làm nền tảng cho các mô hình tiên tiến hầu hết vẫn là bí mật được giữ kín, ngay cả trong trường hợp các LLM mã nguồn mở như BLOOM, LLaMa, MPT và Falcon. Ví dụ, Falcon (Penedo et al., 2023) và BLOOM (Scao et al., 2022) chỉ cung cấp một cái nhìn thoáng qua về dữ liệu huấn luyện đầy đủ của họ, trong khi các bộ dữ liệu của MPT, LLaMa và PolyLM (Touvron et al., 2023; Wei et al., 2023) vẫn không thể truy cập được đối với công chúng. Một mặt, việc thiếu minh bạch đã cản trở phân tích và hiểu biết sâu sắc về LLM, làm cản trở nghiên cứu quan trọng về việc quy kết và giải quyết các vấn đề cơ bản xuất phát từ dữ liệu huấn luyện, chẳng hạn như ảo giác, thiên lệch và nội dung độc hại (Tamkin et al., 2021; Weidinger et al., 2021; Kenton et al., 2021; Bommasani et al., 2021). Mặt khác, việc che giấu dữ liệu huấn luyện hạn chế sự phát triển LLM cho một số ít bên liên quan có nguồn lực dồi dào, từ đó hạn chế dân chủ hóa và lợi ích của công nghệ và làm trầm trọng thêm các thiên lệch trong xã hội rộng lớn hơn.

Để đạt được minh bạch và dân chủ hóa cho LLM, việc tạo ra các bộ dữ liệu quy mô lớn và chất lượng cao để huấn luyện LLM hiệu suất cao trong khi đảm bảo khả năng truy cập công khai để thúc đẩy nghiên cứu và tiến bộ sâu hơn là rất quan trọng. Trong lĩnh vực LLM, các bộ dữ liệu huấn luyện chất lượng cao thường được tạo ra thông qua việc áp dụng các quy trình làm sạch và khử trùng lặp dữ liệu rộng rãi, nhằm loại bỏ nội dung nhiễu và dư thừa từ các bộ sưu tập văn bản khổng lồ (Allamanis, 2018; Penedo et al., 2023). Với mục đích này, đã có những nỗ lực gần đây từ cộng đồng để phát triển các bộ dữ liệu mã nguồn mở như vậy cho LLM, chẳng hạn như RedPajama với 1.21T token (Computer, 2023), SlimPajama với 627B token, và AI2 Dolma với 3T token. Tuy nhiên, hầu hết các bộ dữ liệu mã nguồn mở hiện có cho LLM được thiết kế riêng cho tiếng Anh, điều này cản trở việc sử dụng và hiệu suất của các LLM kết quả khi áp dụng cho các ngôn ngữ không phải tiếng Anh, đặc biệt là những ngôn ngữ có nguồn lực ngôn ngữ học hạn chế (Bang et al., 2023; Lai et al., 2023). Sự nhấn mạnh vào tiếng Anh này cũng hạn chế khả năng của các bộ dữ liệu mã nguồn mở để giải quyết toàn diện các thách thức nghiên cứu và mối quan tâm dân chủ hóa của LLM trên phổ đa dạng của hơn 7.000 ngôn ngữ được nói trên toàn thế giới.

Đồng thời, một số bộ dữ liệu đa ngôn ngữ đã được phát triển và cung cấp, cung cấp dữ liệu văn bản cho nhiều ngôn ngữ. Tuy nhiên, chất lượng và quy mô của chúng không đáp ứng được các yêu cầu để huấn luyện LLM hiệu suất cao. Cụ thể, bộ dữ liệu văn bản đa ngôn ngữ có nguồn gốc từ Wikipedia, mặc dù có chất lượng cao, được coi là tương đối nhỏ khi nói đến huấn luyện LLM (Conneau et al., 2020). Các bộ dữ liệu OSCAR (Ortiz Suárez et al., 2019; Ortiz Suárez et al., 2020; Abadji et al., 2021, 2022) trích xuất dữ liệu văn bản từ CommonCrawl (CC) cho hơn 160 ngôn ngữ. Tuy nhiên, các bộ dữ liệu này thiếu khử trùng lặp ở cấp độ tài liệu (tức là loại bỏ các tài liệu tương tự trong bộ dữ liệu), dẫn đến việc bao gồm thông tin dư thừa và làm suy giảm hiệu suất của LLM tạo sinh (Lee et al., 2022). Tương tự, các bộ dữ liệu mC4 (Xue et al., 2021), CCAligned (Conneau et al., 2020), WikiMatrix (Schwenk et al., 2021) và ParaCrawl (Bañón et al., 2020) cùng nhau hỗ trợ hơn 100 ngôn ngữ nhưng bị ảnh hưởng bởi việc nhận dạng ngôn ngữ kém chính xác hơn, tạo ra nhiễu trong dữ liệu (Kreutzer et al., 2022). Các bộ dữ liệu này cũng không được khử trùng lặp ở cấp độ mờ và tài liệu, ví dụ, thông qua MinHash (Broder, 1997). Ngoài ra, bộ dữ liệu CC100 (Wenzek et al., 2020; Conneau et al., 2020), được sử dụng để huấn luyện mô hình XLM-RoBERTa đa ngôn ngữ trên 100 ngôn ngữ, chỉ xem xét các bản chụp của CC vào năm 2018, hạn chế quy mô và tính sẵn có của thông tin cập nhật để huấn luyện LLM hiệu suất cao.

Để giải quyết các vấn đề nêu trên cho các bộ dữ liệu mã nguồn mở, công trình của chúng tôi giới thiệu một bộ dữ liệu đa ngôn ngữ mới, được gọi là CulturaX, để huấn luyện LLM trong 167 ngôn ngữ. CulturaX kết hợp phiên bản mới nhất của mC4 (phiên bản 3.1.0) với tất cả các kho ngữ liệu OSCAR có sẵn cho đến năm hiện tại, bao gồm các bản phân phối 20.19, 21.09, 22.01 và 23.01. Sự kết hợp này tạo ra một bộ dữ liệu đa ngôn ngữ lớn, bao gồm 27 TB dữ liệu văn bản với 6.3 nghìn tỷ token và cung cấp dữ liệu cập nhật nhất cho phát triển LLM. Hơn một nửa bộ dữ liệu của chúng tôi dành cho các ngôn ngữ không phải tiếng Anh để đáng kể tăng kích thước dữ liệu và nâng cao tính khả thi của việc huấn luyện mô hình trong các tình huống đa ngôn ngữ. Quan trọng, CulturaX được làm sạch và khử trùng lặp rộng rãi ở cấp độ tài liệu để tạo ra chất lượng cao nhất để huấn luyện LLM cho nhiều ngôn ngữ. Cụ thể, quy trình làm sạch dữ liệu của chúng tôi bao gồm một pipeline toàn diện được thiết kế để loại bỏ dữ liệu chất lượng thấp. Điều này bao gồm loại bỏ văn bản nhiễu, nội dung không ngôn ngữ, dữ liệu độc hại, nhận dạng ngôn ngữ không chính xác, và nhiều hơn nữa. Pipeline làm sạch dữ liệu của chúng tôi sử dụng một biến thể của phương pháp Khoảng tứ phân vị (IQR) (Dekking et al., 2007) để chọn ngưỡng phù hợp cho các metric khác nhau của bộ dữ liệu (ví dụ, tỷ lệ từ dừng, độ phức tạp dữ liệu và điểm nhận dạng ngôn ngữ), có thể được sử dụng để lọc các ngoại lệ nhiễu cho bộ dữ liệu. Như vậy, chúng tôi tận dụng các phân vị của các phân phối được tính toán trên các mẫu dữ liệu lớn để hướng dẫn hiệu quả quá trình lựa chọn ngưỡng cho mỗi metric lọc và ngôn ngữ. Cuối cùng, chúng tôi thực hiện khử trùng lặp rộng rãi cho dữ liệu của các ngôn ngữ trong bộ dữ liệu của chúng tôi dựa trên phương pháp khử trùng lặp gần MinHashLSH (Broder, 1997; Leskovec et al., 2020) và URL, dẫn đến dữ liệu chất lượng cao để huấn luyện LLM đa ngôn ngữ. Bộ dữ liệu của chúng tôi sẽ được cung cấp đầy đủ cho công chúng để thúc đẩy nghiên cứu và phát triển hơn nữa cho việc học đa ngôn ngữ. Theo hiểu biết của chúng tôi, CulturaX là bộ dữ liệu đa ngôn ngữ mã nguồn mở lớn nhất cho đến nay được làm sạch sâu và khử trùng lặp cho các ứng dụng LLM và NLP.

## 2 Tạo Bộ Dữ Liệu Đa Ngôn Ngữ

Để phát triển một bộ dữ liệu đa ngôn ngữ công khai cho LLM, chiến lược của chúng tôi là kết hợp mC4 (Xue et al., 2021) và OSCAR (Ortiz Suárez et al., 2019; Abadji et al., 2021, 2022), hai bộ dữ liệu đa ngôn ngữ lớn nhất mà chúng tôi có. Sau đó, chúng tôi xử lý dữ liệu với một pipeline rộng rãi, bao gồm hai bước chính là làm sạch và khử trùng lặp, để tạo ra một bộ dữ liệu khổng lồ và chất lượng cao cho LLM đa ngôn ngữ.

mC4 là một bộ dữ liệu đa ngôn ngữ ở cấp độ tài liệu, ban đầu được tạo ra để huấn luyện mô hình mã hóa-giải mã đa ngôn ngữ mT5 (Xue et al., 2021) cho 101 ngôn ngữ. Bộ dữ liệu này được trích xuất từ 71 bản chụp hàng tháng từ CC bằng cách loại bỏ các trang có ít hơn ba dòng dài (bộ lọc độ dài dòng), các trang có từ xấu và các dòng trùng lặp giữa các tài liệu. Nhận dạng ngôn ngữ cho các trang trong mC4 được thực hiện bởi công cụ cld3 (Botha et al., 2017), là một mạng nơ-ron nhỏ (Xue et al., 2021). Bất kỳ trang nào có độ tin cậy ngôn ngữ dưới 0.95% đều bị loại trừ. mC4 được khử trùng lặp với khớp chính xác ở cấp độ tài liệu; tuy nhiên, khử trùng lặp mờ ở cấp độ tài liệu không được thực hiện. Chúng tôi sử dụng phiên bản mới nhất của mC4 (phiên bản 3.1.0) được chuẩn bị bởi AllenAI trong công trình này.

Một khía cạnh đáng chú ý của bộ dữ liệu chúng tôi liên quan đến nguồn gốc dựa trên web của các bộ dữ liệu được chọn, mC4 và OSCAR, được trích xuất từ CC. Điều này khác với một số công trình trước đây (Radford et al., 2019; MosaicML, 2023; Touvron et al., 2023) cũng đã dựa vào các bộ dữ liệu được tuyển chọn như The Pile (Gao et al., 2020) và BookCorpus (Zhu et al., 2015) để huấn luyện LLM, giả định chất lượng tổng thể cao hơn của chúng. Tuy nhiên, trong bối cảnh cài đặt đa ngôn ngữ, chúng tôi lập luận rằng các bộ dữ liệu được thu thập từ web có thể là một cách tiếp cận phù hợp hơn, vì các bộ dữ liệu được tuyển chọn có chất lượng cao hơn có thể không có sẵn cho các ngôn ngữ khác nhau. Chiến lược sử dụng dữ liệu được thu thập từ web của chúng tôi tạo điều kiện thu thập dữ liệu hiệu quả trên nhiều ngôn ngữ, đóng góp vào việc nâng cao quy mô dữ liệu huấn luyện. Hơn nữa, các nghiên cứu gần đây đã chứng minh tính hiệu quả của việc làm sạch dữ liệu được thu thập từ web để tạo ra các LLM tiên tiến (Raffel et al., 2020; Almazrouei et al., 2023). Tổng cộng, sự kết hợp của mC4 và OSCAR cung cấp cho chúng tôi 13.5B tài liệu để xử lý tiếp theo. Hình 1 minh họa phân phối số lượng tài liệu cho mC4 và bốn phiên bản có sẵn của OSCAR trong bộ dữ liệu ban đầu của chúng tôi.

### 2.1 Làm Sạch Dữ Liệu

Với sự kết hợp của các bộ dữ liệu mC4 và OSCAR, đầu tiên chúng tôi thực hiện một quy trình làm sạch dữ liệu toàn diện để loại bỏ nội dung nhiễu và xấu khỏi dữ liệu, bao gồm nhận dạng ngôn ngữ, lọc dựa trên URL, làm sạch dựa trên metric và tinh chỉnh tài liệu.

**Nhận dạng Ngôn ngữ**: Một vấn đề cụ thể liên quan đến việc sử dụng hai công cụ nhận dạng ngôn ngữ khác nhau, tức là cld3 và FastText, cho mC4 và OSCAR (tương ứng). Đã được chỉ ra trong các nghiên cứu trước đây rằng cld3 tệ hơn đáng kể so với FastText, gây ra nhiều lỗi phát hiện ngôn ngữ hơn cho mC4 (Kreutzer et al., 2022). Trên thực tế, so với một số bộ phát hiện ngôn ngữ khác, FastText đã chứng minh hiệu suất tiên tiến trên các bộ dữ liệu benchmark. Với mục đích này, bước làm sạch dữ liệu đầu tiên của chúng tôi bao gồm việc áp dụng FastText để dự đoán lại các ngôn ngữ cho các tài liệu trong mC4. Các tài liệu có ngôn ngữ được dự đoán khác với những ngôn ngữ được cung cấp trong mC4 sẽ bị loại bỏ khỏi bộ dữ liệu. Lý do là để tránh các tài liệu gây nhầm lẫn cho các bộ phát hiện ngôn ngữ cld3 và FastText, do đó có thể tạo ra nhiễu cho dữ liệu. Cuối cùng, để đảm bảo chất lượng cao nhất, chúng tôi loại bỏ dữ liệu cho bất kỳ ngôn ngữ nào được tìm thấy trong mC4 nhưng không được hỗ trợ bởi FastText.

**Lọc Dựa Trên URL**: Trong bước tiếp theo, chúng tôi nhằm loại bỏ các trang từ các nguồn độc hại và có hại đã biết để giảm các rủi ro liên quan từ dữ liệu của chúng tôi. Cụ thể, chúng tôi tận dụng danh sách đen UT1 mới nhất về URL và tên miền được cung cấp bởi Đại học Toulouse để hỗ trợ việc điều chỉnh sử dụng Internet cho các quản trị viên tại trường học. Danh sách này bao gồm các trang web từ các chủ đề khác nhau, bao gồm khiêu dâm, phàn nàn và hacking, nên được loại bỏ cho việc huấn luyện LLM. Được cập nhật hai đến ba lần mỗi tuần, danh sách đen bao gồm hơn 3.7M bản ghi được đóng góp bởi cả con người và robot (ví dụ, công cụ tìm kiếm, địa chỉ và chỉ mục đã biết) (Abadji et al., 2022). Như vậy, chúng tôi loại bỏ bất kỳ trang nào từ bộ dữ liệu của chúng tôi có URL liên kết khớp với một trang web trong danh sách đen. Bước này hữu ích cho bộ dữ liệu của chúng tôi vì danh sách đen không được sử dụng trước đây cho bộ dữ liệu mC4. Ngoài ra, mặc dù OSCAR đã sử dụng danh sách đen này để làm sạch dữ liệu, cách tiếp cận của chúng tôi kết hợp thông tin cập nhật nhất từ danh sách, có thể không có sẵn cho các bản phân phối hiện tại của OSCAR.

**Làm Sạch Dựa Trên Metric**: Để nâng cao chất lượng của bộ dữ liệu, được thúc đẩy bởi pipeline xử lý dữ liệu từ kho ngữ liệu ROOTS của BigScience cho BLOOM (Laurençon et al., 2022; Scao et al., 2022), chúng tôi tiếp tục sử dụng các phân phối cho các metric khác nhau của bộ dữ liệu để xác định và lọc các tài liệu ngoại lệ. Mỗi metric cung cấp một giá trị đơn lẻ cho mọi tài liệu trong bộ dữ liệu, định lượng các thuộc tính cụ thể như số_từ, tỷ_lệ_từ_dừng và điểm_độ_phức_tạp cho mỗi tài liệu. Đối với mỗi metric và phạm vi giá trị có thể có của nó trong bộ dữ liệu, một ngưỡng sẽ được xác định để phân vùng phạm vi thành hai vùng: một phạm vi bình thường và một phạm vi bất thường. Phạm vi bất thường được chỉ định cho các tài liệu có giá trị metric lệch đáng kể so với chuẩn, phân loại chúng là ngoại lệ/nhiễu, và do đó, các ngoại lệ này bị loại bỏ khỏi bộ dữ liệu của chúng tôi. Như vậy, chúng tôi sử dụng một mảng toàn diện các metric bộ dữ liệu, sẽ được sử dụng chung để tinh chỉnh bộ dữ liệu của chúng tôi, như được nêu dưới đây:

• Số lượng từ
• Tỷ lệ lặp lại ký tự
• Tỷ lệ lặp lại từ
• Tỷ lệ ký tự đặc biệt
• Tỷ lệ từ dừng
• Tỷ lệ từ được gắn cờ
• Độ tin cậy nhận dạng ngôn ngữ
• Điểm độ phức tạp
• Độ dài tài liệu (số ký tự)
• Số dòng
• Tỷ lệ độ dài dòng ngắn
• Tỷ lệ dòng ngắn

Bốn metric cuối cùng được đề xuất bởi bộ dữ liệu OSCAR trong khi các metric khác được kế thừa từ pipeline của kho ngữ liệu BigScience ROOTS để xử lý dữ liệu OSCAR. Đối với điểm độ phức tạp, theo kho ngữ liệu BigScience ROOTS, chúng tôi huấn luyện một tokenizer SentencePiece (Kudo, 2018) và các mô hình ngôn ngữ 5-gram Kneser-Ney như được cung cấp trong thư viện KenLM (Heafield, 2011) sử dụng các dump Wikipedia ngày 20230501. Các tài liệu hiển thị điểm độ phức tạp cao dựa trên các mô hình KenLM này được coi là khác biệt đáng kể so với các bài viết Wikipedia. Điều này chỉ ra một mức độ nhiễu sẽ được loại trừ khỏi bộ dữ liệu của chúng tôi (Wenzek et al., 2020). Tokenizer cũng sẽ được sử dụng để có được số lượng từ/token trong các tài liệu cho các metric của chúng tôi. Chúng tôi công khai phát hành các mô hình KenLM của chúng tôi trên HuggingFace để tạo điều kiện cho việc khám phá trong tương lai.

Thông tin lặp lại (ví dụ, từ, đoạn văn) có thể xuất hiện trong dữ liệu được tuyển chọn từ web do lỗi thu thập và nguồn chất lượng thấp, gây ra hậu quả có hại cho việc huấn luyện LLM (Holtzman et al., 2019). Tỷ lệ lặp lại ký tự và từ do đó được thiết kế để tránh các tài liệu với thông tin lặp lại quá mức. Tần suất cao của ký tự đặc biệt, từ dừng hoặc từ được gắn cờ có thể chỉ ra các tài liệu nhiễu và chất lượng thấp. Do đó, chúng tôi sử dụng các danh sách từ dừng và từ được gắn cờ cho các ngôn ngữ khác nhau để tính toán tỷ lệ của chúng để loại bỏ tài liệu. Ngoài các danh sách từ dừng và từ được gắn cờ được cung cấp bởi BigScience ROOTS cho 13 ngôn ngữ của họ, chúng tôi tiếp tục thu thập từ điển cho các loại từ này cho các ngôn ngữ khác. Chúng tôi ưu tiên các danh sách đã được chia sẻ trên các tài khoản GitHub cá nhân cho các ngôn ngữ khác nhau, vì chúng thường được tạo ra bởi người bản xứ và thể hiện chất lượng cao hơn. Hơn nữa, độ tin cậy nhận dạng ngôn ngữ thấp hơn cũng có thể gợi ý cấu trúc ngôn ngữ nhiễu cho dữ liệu. Đối với mỗi tài liệu trong bộ dữ liệu, chúng tôi do đó có được độ tin cậy nhận dạng ngôn ngữ thông qua xác suất mà FastText gán cho ngôn ngữ tương ứng của nó để hỗ trợ lọc dữ liệu. Cuối cùng, đối với các tiêu chí dựa trên dòng ngắn, chúng tôi triển khai một ngưỡng 100 ký tự để phân loại các dòng là ngắn, như được sử dụng bởi OSCAR. Các tài liệu có sự xuất hiện quá mức của các dòng ngắn sẽ không được giữ lại trong bộ dữ liệu của chúng tôi.

**Lựa Chọn Ngưỡng**: Với tập hợp các metric bộ dữ liệu, một câu hỏi quan trọng liên quan đến việc lựa chọn các ngưỡng phù hợp cho mỗi metric và ngôn ngữ để tạo ra dữ liệu đa ngôn ngữ chất lượng cao. Trong dự án BigScience ROOTS (Laurençon et al., 2022), quá trình lựa chọn này được thực hiện bởi người bản xứ của 13 ngôn ngữ. Các ngưỡng kết quả được sử dụng cho 46 ngôn ngữ còn lại của họ. Dự án cung cấp một giao diện trực quan hóa lập chỉ mục một mẫu vài nghìn tài liệu mỗi ngôn ngữ, cho phép người dùng theo dõi thống kê dữ liệu khi họ điều chỉnh ngưỡng cho các metric. Tuy nhiên, quá trình này không thể dễ dàng mở rộng cho các ngôn ngữ khác nhau do yêu cầu về người bản xứ có kinh nghiệm, điều này phát sinh chi phí đáng kể. Hơn nữa, kích thước mẫu hạn chế cản trở tính đại diện của các ngưỡng được chọn cho toàn bộ bộ dữ liệu. Trong phân tích của chúng tôi, chúng tôi quan sát thấy rằng một số ngưỡng được chọn cho một số ngôn ngữ nhất định trong BigScience ROOTS gần như nằm ngoài phạm vi giá trị cho toàn bộ bộ dữ liệu, dẫn đến việc vô hiệu hóa các metric tương ứng.

Để giải quyết những vấn đề này, chúng tôi tận dụng một biến thể của phương pháp Khoảng tứ phân vị (IQR) (Dekking et al., 2007) để chọn các ngưỡng phù hợp cho các metric lọc cho bộ dữ liệu của chúng tôi. Đối với mỗi metric và ngôn ngữ, chúng tôi tạo ra một phân phối các giá trị có thể có của nó trên toàn bộ bộ dữ liệu cho ngôn ngữ đó. Có một ngoại lệ đối với các ngôn ngữ có lượng dữ liệu lớn, chẳng hạn như tiếng Tây Ban Nha và tiếng Nga, nơi chỉ 25% dữ liệu được sử dụng để tính toán các phân phối này. Sau đó, chúng tôi tính toán phân vị thứ Q1 và Q3 của phân phối (Q1 < Q3) và sử dụng chúng cho các ngưỡng cho các metric lọc của chúng tôi. Cụ thể, phân vị thứ Q1 thấp hơn sẽ được chọn cho các metric ưa thích giá trị cao (ví dụ, độ tin cậy nhận dạng ngôn ngữ), trong khi các metric ưa thích giá trị thấp (ví dụ, điểm độ phức tạp và độ dài tài liệu) sẽ sử dụng phân vị thứ Q3 cao hơn. Chúng tôi điều tra các giá trị khác nhau cho (Q1, Q3), xem xét (25,75), (20,80), (15,85), (10,90) và (5,95). Việc chọn Q1 = 10 và Q3 = 90 đã đạt được chất lượng dữ liệu tốt nhất cho một mẫu các ngôn ngữ trong kiểm tra của chúng tôi.

Điều đáng nhấn mạnh là việc sử dụng phân vị để lựa chọn ngưỡng cho phép cách tiếp cận của chúng tôi hiệu quả tận dụng các mẫu dữ liệu rộng lớn hơn cho mỗi ngôn ngữ so với những mẫu được sử dụng trong dự án BigScience ROOTS. Điều này dẫn đến các ngưỡng đáng tin cậy hơn cho toàn bộ bộ dữ liệu trên các ngôn ngữ khác nhau. Cụ thể, liên quan đến các ngôn ngữ lớn nơi chỉ một mẫu dữ liệu 25% được sử dụng để tính toán phân phối giá trị cho một metric, chúng tôi quan sát thấy rằng tỷ lệ dữ liệu bị loại bỏ so với toàn bộ bộ dữ liệu gần như phù hợp với tỷ lệ của mẫu dữ liệu khi áp dụng cùng một ngưỡng lọc được chọn. Điều này nhấn mạnh tính đại diện của các ngưỡng được chọn thông qua phương pháp của chúng tôi. Cuối cùng, một khi các ngưỡng cho các metric trong một ngôn ngữ nhất định đã được xác định, chúng tôi sẽ loại bỏ bất kỳ tài liệu nào vượt quá ngưỡng của một metric và đi vào phạm vi bất lợi của dữ liệu.

**Tinh Chỉnh Tài Liệu**: Các bước làm sạch trước đây được thực hiện ở cấp độ bộ dữ liệu, nhằm loại bỏ các tài liệu chất lượng thấp khỏi bộ dữ liệu. Trong bước này, chúng tôi tiếp tục làm sạch các tài liệu được giữ lại để cải thiện chất lượng. Điều quan trọng cần lưu ý là bước lọc dựa trên metric trước đây của chúng tôi đóng vai trò quan trọng trong việc loại bỏ các tài liệu cực kỳ nhiễu, điều này lần lượt hợp lý hóa quá trình phát triển các quy tắc làm sạch tài liệu hiệu quả trong bước này. Đáng chú ý, vì các tài liệu từ mC4 và OSCAR được trích xuất từ các trang HTML được thu thập từ Internet, một phần đáng kể trong số chúng có thể mang lỗi thu thập và trích xuất, bao gồm các dòng JavaScript dài và nội dung không liên quan. Do đó, việc lọc ra các tài liệu này làm đơn giản hóa đáng kể nhiệm vụ của chúng tôi trong việc thiết kế các quy tắc để làm sạch các tài liệu trong bộ dữ liệu của chúng tôi.

Như vậy, đối với mỗi tài liệu, chúng tôi loại bỏ các phần nhiễu hoặc không liên quan của nó thông qua một loạt các hoạt động. Đầu tiên, chúng tôi loại bỏ bất kỳ dòng ngắn nào nằm ở cuối mỗi tài liệu, vì các dòng này thường chứa chi tiết chân trang hoặc thông tin không hữu ích từ các trang web. Thứ hai, chúng tôi loại bỏ các dòng chứa từ từ danh sách từ khóa JavaScript (JS) của chúng tôi (ví dụ, "<script") để tránh thông tin không liên quan và không ngôn ngữ. Ở đây, chúng tôi chỉ loại bỏ các dòng JS nếu tài liệu chỉ chứa một dòng với từ khóa JS, và dòng cụ thể này cũng phải có ít nhất hai loại từ khóa JS khác nhau. Chúng tôi áp dụng cách tiếp cận này vì các tài liệu có nhiều hơn hai dòng JS có thể là hướng dẫn lập trình trong dữ liệu của chúng tôi, nên được bảo tồn để cải thiện tính đa dạng. Ngoài ra, một số từ khóa JS được sử dụng trong ngôn ngữ tự nhiên, ví dụ, "var". Bằng cách yêu cầu ít nhất hai loại từ khóa JS khác nhau, chúng tôi giảm nguy cơ vô tình bỏ sót nội dung hữu ích và làm gián đoạn cấu trúc của tài liệu.

### 2.2 Khử Trùng Lặp Dữ Liệu

Mặc dù làm sạch dữ liệu kỹ lưỡng, bộ dữ liệu còn lại vẫn có thể chứa một lượng lớn dữ liệu lặp lại do nhiều lý do khác nhau, bao gồm thông tin được đăng lại trên web, nhiều tham chiếu đến cùng một bài viết, nội dung mẫu và đạo văn. Dữ liệu trùng lặp do đó có thể gây ra việc ghi nhớ và cản trở đáng kể khả năng tổng quát hóa cho LLM (Lee et al., 2022; Hernandez et al., 2022). Mặc dù tốn kém, khử trùng lặp dữ liệu do đó được coi là một bước quan trọng để đảm bảo chất lượng cao nhất của dữ liệu để huấn luyện LLM. Với mục đích này, chúng tôi thực hiện một quy trình khử trùng lặp toàn diện cho bộ dữ liệu của chúng tôi, sử dụng MinHash (Broder, 1997) và URL. Quá trình khử trùng lặp này được thực hiện độc lập cho mỗi ngôn ngữ. Hơn nữa, chúng tôi hạn chế khử trùng lặp cho các ngôn ngữ có hơn 100K tài liệu sau các quy trình làm sạch dữ liệu của chúng tôi (tức là 51.5% ngôn ngữ của chúng tôi), nhằm thúc đẩy các ngôn ngữ nhỏ hơn trong bộ dữ liệu của chúng tôi.

**Khử Trùng Lặp MinHash**: Đối với bộ dữ liệu của mỗi ngôn ngữ, đầu tiên chúng tôi áp dụng phương pháp MinHashLSH (Leskovec et al., 2020) để lọc các tài liệu tương tự trong bộ dữ liệu. MinHashLSH là một kỹ thuật khử trùng lặp gần dựa trên MinHash (Broder, 1997) với nhiều hàm băm cho n-gram và độ tương tự Jaccard. Locality-Sensitive Hashing (LSH) được kết hợp để cải thiện hiệu quả bằng cách tập trung vào các cặp tài liệu có khả năng tương tự nhất. Chúng tôi tận dụng một biến thể của việc triển khai Spark của MinHashLSH trong repo text-dedup, sử dụng 5-gram và ngưỡng 0.8 để xác định các tài liệu tương tự cho độ tương tự Jaccard. Chạy MinHashLSH cho bộ dữ liệu của mỗi ngôn ngữ, đặc biệt là đối với các ngôn ngữ có khối lượng dữ liệu lớn nhất như tiếng Anh, tiếng Nga, tiếng Tây Ban Nha và tiếng Trung, đại diện cho hoạt động tốn kém tính toán nhất trong nỗ lực tạo bộ dữ liệu của chúng tôi.

**Khử Trùng Lặp Dựa Trên URL**: Cuối cùng, chúng tôi loại bỏ tất cả các tài liệu có chung URL giống hệt với các tài liệu khác trong bộ dữ liệu. Bước này cần thiết để giải quyết các tình huống nơi các phiên bản khác nhau của cùng một bài viết được liên kết với các URL giống hệt nhau nhưng đã được cập nhật hoặc sửa đổi trong quá trình xuất bản, hiệu quả bỏ qua bước khử trùng lặp gần. Một số URL cho các bài viết trong CC có thể chỉ hiển thị các tên miền chung của chúng do lỗi thu thập. Để nâng cao độ chính xác, chúng tôi tránh loại bỏ các URL chỉ bao gồm các tên miền chung của chúng.

Chúng tôi sử dụng 600 phiên bản AWS c5.24xlarge EC2 để tiền xử lý và khử trùng lặp bộ dữ liệu đa ngôn ngữ của chúng tôi. Mỗi phiên bản được trang bị 96 lõi CPU, 192GB bộ nhớ và 1TB dung lượng đĩa. Dung lượng đĩa có thể được sử dụng để thay thế bộ nhớ khi cần thiết (ví dụ, để khử trùng lặp dữ liệu).

## 3 Phân Tích Dữ Liệu và Thí Nghiệm

Sau khi hoàn thành tất cả các bước làm sạch và khử trùng lặp, bộ dữ liệu cuối cùng của chúng tôi bao gồm 6.3 nghìn tỷ token trải rộng 167 ngôn ngữ. Bảng 1 cung cấp tổng quan về số lượng tài liệu và token cho 42 ngôn ngữ hàng đầu trong CulturaX sau mỗi giai đoạn xử lý. Như có thể thấy, pipeline làm sạch dữ liệu của chúng tôi có thể giảm đáng kể số lượng tài liệu trong các bộ dữ liệu mC4 và OSCAR ban đầu cho mỗi ngôn ngữ. Tổng số tài liệu bị loại bỏ chiếm 46.48% số tài liệu ban đầu của chúng tôi, cho thấy tính hiệu quả của các phương pháp của chúng tôi để lọc thông tin nhiễu cho các bộ dữ liệu đa ngôn ngữ.

## 4 Công Trình Liên Quan

So với các tác vụ NLP khác, các mô hình ngôn ngữ có thể được huấn luyện với dữ liệu không có nhãn, cho phép thu thập dữ liệu hiệu quả để tạo ra quy mô khổng lồ cho dữ liệu huấn luyện. Có hai loại dữ liệu chính thường được sử dụng để huấn luyện LLM: dữ liệu được tuyển chọn và dữ liệu thu thập web. Dữ liệu được tuyển chọn thường bao gồm văn bản được viết tốt và được định dạng tốt từ các nguồn và lĩnh vực có mục tiêu, ví dụ, các bài viết Wikipedia, sách, bài viết báo chí và tài liệu khoa học, như được sử dụng cho các bộ dữ liệu "The Pile" (Gao et al., 2020) và "BookCorpus" (Zhu et al., 2015). Ngược lại, dữ liệu thu thập web bao gồm văn bản được thu thập từ một loạt các nguồn trên internet, khác nhau đáng kể về định dạng và phong cách viết, ví dụ, blog, bài đăng trên mạng xã hội, bài viết tin tức và quảng cáo. CommonCrawl (CC) là một kho lưu trữ thu thập web được sử dụng rộng rãi đã thu thập petabyte dữ liệu trên Internet trong 12 năm. Với mục đích này, dữ liệu được tuyển chọn thường được coi là có chất lượng cao hơn, điều này đã dẫn đến việc ưu tiên nó để huấn luyện các LLM sớm, ví dụ, BERT (Devlin et al., 2019) và GPT-2 (Radford et al., 2019). Tuy nhiên, khi nhu cầu về các mô hình lớn hơn đã tăng lên, dữ liệu thu thập web đã nhận được nhiều sự chú ý hơn vì nó đóng góp một phần đáng kể vào dữ liệu huấn luyện của các LLM gần đây, ví dụ, RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), T5 (Raffel et al., 2020), GPT-3 (Rae et al., 2021), LLaMa (Touvron et al., 2023), MPT (MosaicML, 2023) và Falcon (Almazrouei et al., 2023). Như vậy, các trích xuất khác nhau của CC đã được sản xuất để huấn luyện các LLM như vậy, bao gồm C4 (Raffel et al., 2020), CC-News (Nagel) và STORIES (Trinh and Le, 2018).

Về khả năng truy cập của dữ liệu huấn luyện, các bộ dữ liệu được sử dụng để huấn luyện các LLM sớm thường được cung cấp công khai (Devlin et al., 2019; Raffel et al., 2020). Tuy nhiên, trong trường hợp các LLM tạo sinh tiên tiến (SOTA) gần đây nhất, các bộ dữ liệu huấn luyện của chúng không được phát hành đầy đủ, có thể do lợi ích thương mại. Điều này áp dụng không chỉ cho các mô hình độc quyền như ChatGPT và GPT-4 mà còn cho các mô hình tuyên bố là mô hình mã nguồn mở như LLaMa, MPT, Falcon và BLOOM (Scao et al., 2022). Để giải quyết vấn đề minh bạch với các LLM hiện có, các nỗ lực gần đây đã được thực hiện để sao chép và phát hành các bộ dữ liệu huấn luyện cho các LLM tiên tiến, tức là RedPajama (Computer, 2023), SlimPajama và AI2 Dolma. Sự khác biệt chính đối với các bộ dữ liệu này liên quan đến dữ liệu văn bản quy mô lớn của chúng đã được làm sạch tỉ mỉ và khử trùng lặp ở cấp độ tài liệu để đảm bảo chất lượng cao cho việc huấn luyện LLM. Tuy nhiên, một nhược điểm chung của các bộ dữ liệu mã nguồn mở này là chúng vẫn tập trung chủ yếu vào dữ liệu tiếng Anh, cung cấp dữ liệu hạn chế cho các ngôn ngữ khác.

Để có được một bộ dữ liệu đa ngôn ngữ quy mô lớn để huấn luyện LLM, việc khai thác các bộ dữ liệu thu thập web như CC để cho phép thu thập dữ liệu hiệu quả với thông tin cập nhật trong nhiều ngôn ngữ là thuận tiện hơn. Ngoài ra, để đảm bảo chất lượng cao cho các LLM hiệu suất cao, cần phải làm sạch và khử trùng lặp rộng rãi dữ liệu đa ngôn ngữ để tránh nội dung nhiễu và không liên quan, ví dụ, văn bản được tạo bởi máy chất lượng thấp và nội dung người lớn (Trinh and Le, 2018; Kreutzer et al., 2022; Raffel et al., 2020). Như vậy, một pipeline xử lý dữ liệu điển hình để tạo ra các bộ dữ liệu chất lượng cao có thể bao gồm nhiều bước, như được chứng minh bởi FastText (Joulin et al., 2016), CC-Net (Wenzek et al., 2020), kho ngữ liệu BigScience ROOTS cho các mô hình BLOOM (Laurençon et al., 2022; Scao et al., 2022), bộ dữ liệu RefinedWeb cho mô hình Falcon (Penedo et al., 2023; Almazrouei et al., 2023) và bộ dữ liệu để huấn luyện các mô hình LLaMa (Touvron et al., 2023). Bước đầu tiên cần thiết trong các pipeline như vậy là nhận dạng ngôn ngữ để phân bổ dữ liệu một cách thích hợp cho các ngôn ngữ tương ứng của chúng (Joulin et al., 2016). Các bước tiếp theo đặc trưng cho nhiều quy tắc và phương pháp heuristic cụ thể của bộ dữ liệu để lọc nội dung không mong muốn theo tỷ lệ của các ký tự đặc biệt, dòng ngắn, từ xấu, trong số những thứ khác (Grave et al., 2018; Laurençon et al., 2022). Dữ liệu cũng có thể được lọc thông qua các mô hình nhẹ, ví dụ, thông qua các mô hình ngôn ngữ KenLM (Heafield, 2011), để tránh các tài liệu nhiễu (Wenzek et al., 2020). Cuối cùng, khử trùng lặp dữ liệu nên được thực hiện để loại bỏ thông tin tương tự hoặc lặp lại (Laurençon et al., 2022; Penedo et al., 2023). Một bước quan trọng trong vấn đề này bao gồm khử trùng lặp mờ ở cấp độ tài liệu, ví dụ, thông qua MinHash (Broder, 1997), để loại bỏ các tài liệu tương tự, do đó giảm thiểu việc ghi nhớ và cải thiện khả năng tổng quát hóa cho các LLM kết quả (Lee et al., 2022).

Với mục đích này, trong khi có các bộ dữ liệu đa ngôn ngữ mã nguồn mở với dữ liệu văn bản trong nhiều ngôn ngữ, chẳng hạn như mC4 (Xue et al., 2021), OSCAR (Ortiz Suárez et al., 2019), CC100 (Wenzek et al., 2020; Conneau et al., 2020) và kho ngữ liệu BigScience ROOT (Laurençon et al., 2022), chất lượng và quy mô của chúng không đáp ứng các yêu cầu để huấn luyện hiệu quả các LLM, đặc biệt là các mô hình tạo sinh như GPT. Ví dụ, như được nêu bật trong phần giới thiệu, cả mC4 và OSCAR đều thiếu khử trùng lặp mờ cho dữ liệu ở cấp độ tài liệu. mC4 cũng bị ảnh hưởng bởi việc nhận dạng ngôn ngữ kém hơn do sử dụng cld3. BigScience ROOTS chỉ cung cấp một mẫu dữ liệu nhỏ cho 46 ngôn ngữ trong khi CC100 không có thông tin ngoài năm 2018. Bộ dữ liệu CulturaX của chúng tôi do đó giải quyết toàn diện các vấn đề cho các bộ dữ liệu hiện có, cung cấp một bộ dữ liệu đa ngôn ngữ, mã nguồn mở và quy mô lớn với dữ liệu có thể sử dụng ngay và chất lượng cao để huấn luyện LLM.

## 5 Kết Luận

Chúng tôi trình bày CulturaX, một bộ dữ liệu đa ngôn ngữ mới với dữ liệu văn bản cho 167 ngôn ngữ. Bộ dữ liệu của chúng tôi được làm sạch và khử trùng lặp thông qua một pipeline toàn diện, tạo ra 6.3 nghìn tỷ token. CulturaX do đó là một bộ dữ liệu quy mô lớn và chất lượng cao, có thể được sử dụng ngay để huấn luyện các LLM hiệu suất cao cho nhiều ngôn ngữ. Dữ liệu của chúng tôi được truy cập mở cho công chúng để thúc đẩy nghiên cứu và ứng dụng hơn nữa của việc học đa ngôn ngữ.
