# 2306.00789.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2306.00789.pdf
# File size: 580637 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PREPRINT 1
Improved Cross-Lingual Transfer Learning For
Automatic Speech Translation
Sameer Khurana, Nauman Dawalatabad, Antoine Laurent, Luis Vicente, Pablo Gimeno, Victoria Mingote,
James Glass
Abstract ‚ÄîResearch in multilingual speech-to-text translation is
topical. Having a single model that supports multiple translation
tasks is desirable. The goal of this work it to improve cross-
lingual transfer learning in multilingual speech-to-text translation
via semantic knowledge distillation. We show that by initializing
the encoder of the encoder-decoder sequence-to-sequence trans-
lation model with SAMU -XLS-R, a multilingual speech transformer
encoder trained using multi-modal (speech-text) semantic knowl-
edge distillation, we achieve significantly better cross-lingual task
knowledge transfer than the baseline XLS-R, a multilingual speech
transformer encoder trained via self-supervised learning. We
demonstrate the effectiveness of our approach on two popular
datasets, namely, CoVoST-2 and Europarl. On the 21 translation
tasks of the CoVoST-2 benchmark, we achieve an average
improvement of 12.8 BLEU points over the baselines. In the
zero-shot translation scenario, we achieve an average gain of
18.8 and 11.9 average BLEU points on unseen medium and low-
resource languages. We make similar observations on Europarl
speech translation benchmark.
Index Terms ‚ÄîCross-Lingual Transfer Learning, Automatic
Speech Translation, Semantically Aligned Cross-Lingual Speech
Representations
I. I NTRODUCTION
Self-Supervised Representation Learning (SSRL) from
speech [1]‚Äì[18] has improved tremendously over the past few
years due to the introduction of Contrastive Predictive Coding
(CPC) [19], a self-supervised representation learning method
applied to speech, text, and visual data. The introduction of the
core idea of noise contrastive estimation [20] in CPC has led
to a series of papers in speech SSRL, such as Wav2Vec [5],
VQ-Wav2Vec [6], Wav2Vec-2.0 [9], Multilingual Wav2Vec-
2.0 [11], XLS-R ( XLS-R, a bigger version of the multilingual
wav2vec-2.0) [15]. Pre-trained SSRL speech encoders like
XLS-Rare considered "foundation models" [21] for down-
stream multilingual speech processing applications such as
Multilingual Automatic Speech Recognition [11], [15], [22],
Multilingual Speech Translation [15], [18], [23], and other
para-linguistic property prediction tasks [24], [25]. This work
focuses on Multilingual Speech Translation.
Multilingual Speech Translation (MST) refers to translating
speech in all the source languages in set Xto text in all the
target languages in set Y, which implies a total of |T |=
Sameer Khurana (email: skhurana@mit.edu), Nauman Dawalatabad, and
James Glass are with MIT Computer Science and Artificial Intelligence
Laboratory, USA. Luis Vicente, Pablo Gimeno, and Victoria Mingote are
with the University of Zaragoza, Spain. Antoine Laurent is with LIUM
University, France. This work was partially performed using HPC resources
from GENCI‚ÄìIDRIS, grants AD011012527 and AD011012565. This work
has received funding from the European Union‚Äôs Horizon 2020 research and
innovation programme under the Marie Sk≈Çodowska-Curie grant agreement
No 101007666.|X|√ó|Y| translation tasks. In MST, we train a single model for
all the translation tasks given by set T. The benefits of having a
single model instead of individual model for each task t‚àà T
are two-fold: First, it is convenient to maintain and share a
single model that can perform multiple tasks rather than having
|T |separate models, and second, sharing model parameters
amongst |T |translation tasks could lead to knowledge transfer
across tasks, especially from high-resource to low-resource.
The standard neural network architecture used for MST is
theencoder-decoder model [26], [27]. Recently, MST has seen
significant improvements owing to; (i) better initialization of
the translation model‚Äôs encoder and decoder with pre-trained
speech encoders, like XLS-R[15], and text decoders, like
MBART [28], (ii) better fine-tuning strategies [23], and (iii)
parallel speech-text translation corpora [29], [30]. However,
as we demonstrate in Section II, the performance on low-
resource tasks remains poor, and in particular, the performance
gap (cross-lingual transfer gap) between high and low-resource
languages remains large. We hypothesize that this is because
theXLS-Rspeech encoder learns non-robust surface-level
features from unlabeled speech data, rather than the high-level
linguistic knowledge about semantics .
To inject semantic knowledge into the learned XLS-Rrep-
resentations, we turn to the recently introduced Semantically-
Aligned Multimodal Cross-Lingual Representation Learning
framework, SAMU -XLS-R[17]. SAMU -XLS-R(Section III-B) is a
knowledge-distillation framework that distills semantic knowl-
edge from a pre-trained text embedding model into the pre-
trained multilingual XLS-Rspeech encoder. The KD framework
outputs the SAMU -XLS-Rspeech encoder that learns a semanti-
cally structured multilingual speech manifold, where a spoken
utterance is close to its spoken translations in several other
languages that are present in the SAMU -XLS-R‚Äôs training pool
(Details in Section III-B). We claim that building MST model
using the semantic representations learned by SAMU -XLS-R
would lead to better cross-lingual transfer from high to low-
resource tasks compared to the XLS-Rand other non-semantic
multilingual speech encoder baselines. We verify our claim
through several experiments (Section VI). Through this work,
we make the following contributions :
‚Ä¢We doubled the number of languages previously sup-
ported by SAMU -XLS-Rencoder from 25 to 53 (Sec-
tion IV-A).
‚Ä¢SAMU -XLS-Rencoder provides speech embedding at the
utterance level. Previous works [31] have explored the use
SAMU -XLS-Rembedding for automatic spoken language
understanding and semantic speech-text and speech-
speech retrieval [17]. Differently, in this work, for the firstarXiv:2306.00789v4  [cs.CL]  25 Jan 2024

--- PAGE 2 ---
PREPRINT 2
time, we show that the fine-grained contextual embed-
dings (corresponding to 20ms duration speech segment)
that precede the coarse utterance level representation (3-
10s long speech segment) learned by SAMU -XLS-Ris well-
suited for the sequence generation task of multilingual
speech translation. We empirically demonstrate, through
several experiments, the superiority of SAMU -XLS-Rrep-
resentations over XLS-Rand other multilingual speech
encoders (Section VI).
‚Ä¢On the public CoV oST-2 X ‚Üí English MST benchmark
[30], we show that by switching the XLS-Rencoder in the
MST model to the SAMU -XLS-Rencoder, the performance
improves significantly on medium-resource X ‚Üí English
language group by 15.6 BLEU points, and on low-
resource language group by 18.9 BLEU points, and
overall by 13.8 BLEU points (Section VI-A).
‚Ä¢We also show the efficacy of the SAMU -XLS-Rspeech en-
coder in Zero-Shot translation settings. In the Zero-Shot
MST scenario, where we train the SAMU -XLS-R-based
MST model only on high-resource X‚ÜíEnglish tasks in
CoV oST-2, we observe an improvement on medium and
low-resource tasks of 18.8 and 11.9 BLEU points over
theXLS-Rbaseline (Section VI-B).
‚Ä¢Finally, we extend our studies on another MST bench-
mark, namely Europarl [29], which consists of several
X‚ÜíY translation tasks. In the zero-shot MST scenario,
we observe an overall improvement of 8.5 BLEU points
average with SAMU -XLS-Rencoder over XLS-R, owing
to the significant increase in performance on unseen
(during training) source languages of 17 BLEU points
(Section VI-C).
II. M OTIVATION : CROSS -LINGUAL TRANSFER GAP
To motivate our work, we show the performance of the
multilingual XLS-Rspeech encoder on the CoV oST-2 speech-
to-text translation benchmark [30]. CoV oST-2 comprises 21
X‚ÜíEN speech-to-text translation tasks, where X refers to the
language of speech utterance, and EN refers to the corre-
sponding English text translation. XLS-Rspeech encoder is pre-
trained via Self-Supervised Learning using unlabeled speech
data in 128 languages. Babu et al. [15] fine-tunes the pre-
trained XLS-Rencoder combined with a pre-trained MBART text
decoder [28] simultaneously on 21 X ‚ÜíEN translation tasks
(multi-task learning) in CoV oST-2 benchmark. We categorize
the 21 translation tasks into high, mid, and low-resource
groups. A task is classified as high if it has more than 100
hours of paired speech(X)-text(EN) translation training data,
mid if training data is between 10 and 100 hours, and low if
training data is less than 10 hours. There are four high, five
mid, and 12 low-resource tasks.
We report the XLS-R(speech encoder) ‚ÜíMBART (text de-
coder) transformer model‚Äôs performance on the high, mid,
and low-resource translation groups in Fig. 1. We report the
average BLEU-4 score on each translation group. The vital
thing to observe is the performance gap (cross-lingual transfer
gap) between high and low-resource translation groups for
different-sized XLS-Rencoders ranging from 300M to 2BFig. 1: We report translation performance on 21 X ‚ÜíEN
speech-to-text translation tasks in CoV oST-2 benchmark with
different sized pre-trained XLS-R encoders fine-tuned on la-
beled speech translation data. The 21 tasks are categorized into
high, mid, and low resource tasks depending on the available
labeled training data for a task. We report average BLEU-4
scores in the three categories. The important thing to consider
is the performance gap or cross-lingual transfer gap between
high and low-resource translation tasks. We address this large
gap in this paper.
30.634.336.118.925.527.75.111.715.10510152025303540
XLS-R-300MXLS-R-1BXLS-R-2BBLEU-4HighMidLow25.122.621‚Ä¢566% increase in model parameters leads to only a 16% reduction in transfer gap‚Ä¢Simply scaling up models is not going to improve transferTransfer gap
parameters. So, increasing the model size from 300M to 2B,
a more than 500% increase leads to only a 16% reduction in
the cross-lingual transfer gap. Since the translation model is
built on top of the pre-trained XLS-Rencoder‚Äôs representations,
there is some missing ingredient in the pre-trained represen-
tations that leads to a poor cross-lingual transfer from high
to low-resource tasks. This implies that the knowledge that
theXLS-R-based translation model acquires while learning to
perform high-resource X ‚ÜíEN translation tasks is not useful
(or transferable) for learning the low-resource tasks.
Weclaim that the missing piece is semantic knowledge.
We hypothesize that since SAMU -XLS-Ris specifically trained
to encode semantic information in its internal representations,
building a translation model on top of SAMU -XLS-Rwould
lead to better cross-lingual transfer from high to low-resource
translation tasks, thus reducing the cross-lingual transfer gap
mentioned above.
III. P RELIMINARIES
A. XLS-R ( XLS-R)
This section discusses the architecture of the XLS-Ren-
coder. The XLS-Rencoder consists of a Convolutional Neural
Network (CNN) [33] feature extractor, denoted by h, and a
transformer encoder [27], denoted by g.
1) CNN Feature Extractor: h:a1:S‚Üíf1:Tmaps the
speech waveform (a1:S|as‚ààR)to an intermediate rep-
resentation (f1:T|ft‚ààRd), where T=S/r, where ris the
factor by which hdownsamples a1:S, and dis the size of
the feature dimension. For the XLS-Rencoder, r= 320 , and
d= 1024 . The feature extractor hconsists of seven temporal
Convolution layers ( CONV ). Each layer uses GeLU activation
function [34]. The feature maps outputted by each layer have

--- PAGE 3 ---
PREPRINT 3
Fig. 2: SAMU -XLS-Rsemantic knowledge-distillation framework. The learning framework comprises a speech and a text encoder.
The speech encoder transforms a raw speech waveform into an embedding vector. The text encoder transforms the transcript
corresponding to the speech utterance into an embedding. The text encoder is initialized using the pre-trained Language-
Agnostic BERT Sentence Embedding model LaBSE [32]. The speech encoder below the pooling layer is initialized using the
pre-trained XLS-Rspeech encoder [15].
CONVCONVTRANSFTRANSFAttentive Pooling
768 dimensionsFC √†TanhLaBSE Text EncoderAll‚Äôswell, that ends wellSpeechTranscriptSpeechEmbedding
768 dimensionsTranscript EmbeddingKnowledge Distillation LossInitialized using pre-trained XLS-Rùëì!:#ùëê!:#ùëé!:$ùë¶!:%ezMASK%ùëì!:#Frozen During TrainingFrozen During Training
512 channels. The output of the last CONV layer in his up-
projected using a fully-connected layer ( FC) so that the feature
vector fthas the same dimension as the transformer encoder.
Note that the receptive field of each feature vector is 20ms of
the input speech waveform. The feature extractor is followed
by another CONV layer that encodes the relative position of the
feature sequence f1:T, which is used as input to the transformer
encoder g.
2) Transformer Encoder: g:f1:T‚Üíc1:Tmaps (f1:T|ft‚àà
Rd)to contextualized representation (c1:T|ct‚ààRd). The
receptive field of each context vector ctis equal to the length
of the input speech waveform a1:S, which is usually 5-10s.
The transformer encoder consists of 24 layers. Each trans-
former layer consists of Multi-Headed Self-Attention ( MHSA ),
followed by two FClayers. Layer Normalization ( LN) is used
at the input of MHSA andFCblocks. Residual connections adds
the input of MHSA , with its output, and the input of FCblock
with its output. The transformer encoder layer dimension is
1024 (size of the feature vector outputted by a transformer
layer), and the FClayer dimension is 3072. Each MHSA block
consists of 16 attention heads. The output of the first FClayer
is processed by a ReLU non-linearity [35].
3) Training Details: XLS-Ris pre-trained on approximately
400K hours of multilingual unlabeled speech data segmented
into utterances of size 3-10s. The training data consists of
unlabeled speech in 128 languages coming from the following
corpora: V oxPopuli [36], CommonV oice (CoV o) [37], Multi-
lingual Speech (MLS) [38], BABEL, and V oxlingua [39]. For
full training details, see [15]. We use the pre-trained XLS-R
checkpoints publicly released1.
As baselines for multilingual X ‚ÜíEN speech translation
experiments (Section VI-A), we use the XLS-R‚ÜíMBART trans-
lation models that are trained on CoV oST-2 X ‚ÜíEN MST
benchmark, officially released here2. The official release com-
prises three translation models corresponding to 0.3B, 1B, and
1https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec/
xlsr
2https://huggingface.co/facebook/wav2vec2-xls-r-300m-21-to-en2B parameter XLS-Rspeech encoders. The text decoder in all
three cases in the 400M parameter MBART decoder.
B. SAMU-XLS-R ( SAMU -XLS-R)
XLS-Rencoder‚Äôs representation manifold encodes low-
level linguistic knowledge, as evident by the non-existent
cross-lingual semantic speech retrieval capability when using
XLS-R‚Äôs learned representations in [17]. SAMU -XLS-Ris a bi-
modal (speech & text) knowledge distillation (KD) frame-
work for expanding the representation space of XLS-Rspeech
encoder also to encode high-level semantic knowledge. The
SAMU -XLS-Rlearning framework uses multilingual transcribed
speech set D={an
1:S,yn
1:L}N
i=1consisting of speech, ai
1:S
paired with its text transcript yi
1:L. Note that speech and its
transcript are in the same language. The SAMU -XLS-RKD
framework consists of a speech and a text processing branch
as illustrated in Figure 2, and detailed below.
1) The Speech Branch: The speech branch maps the speech
waveform a1:Sof duration 3-10s to a single embedding vector
e‚ààR768. The mapping is performed in the following two
steps: (i) The pre-trained XLS-Rspeech encoder maps a1:S
to the contextual representation c1:T, and (ii) An attention-
based temporal pooling [40], and a non-linear ( tanh ) down-
projection layer transforms c1:Tto a single embedding eof
size 768.
2) The Text Branch: The text branch consists of a pre-
trained LaBSE sentence embedding encoder [32] that trans-
forms the text transcript y1:Lto a semantic embedding z‚ààRd,
of size d= 768 .LaBSE supports 109 written languages. It
embeds sentences from different languages in a shared se-
mantic embedding space, i.e., a sentence and its translation lie
close together in the LaBSE embedding space. By regressing
on the LaBSE semantic embedding, the XLS-Rspeech encoder
learns to encode semantics hidden in the spoken utterance in
its internal representations. This new semantics-aware speech
encoder is referred to as SAMU -XLS-Rencoder.
3) Training Details.: The SAMU -XLS-Rencoder is trained
to minimize the cosine distance between the speech and the

--- PAGE 4 ---
PREPRINT 4
TABLE I: The number of hours of transcribed speech data available for training SAMU -XLS-Rin each of the 53 languages from
the CommonV oice-Version8 corpus.
ar be bg ca cs cy da de el en eo
85.2 903.9 8.2 916.8 54.9 116.3 6.6 1062.8 15.9 2185.8 1407.9
es et eu fa fi fr fy-NL ga-IE gl ha hi
404.6 33.0 98.9 317.3 8.5 826.1 49.6 4.3 10.2 3.4 11.7
hu id it ja ka kmr ky lt lv mn mt
19.9 25.8 310.6 40.8 7.6 47.0 37.2 17.4 7.1 12.4 8.3
nl pl pt ro ru rw sk sl sv-SE sw ta
98.0 142.2 112.0 15.8 162.6 2000.7 17.7 9.6 40.8 146.8 217.7
th tr tt ug uk uz vi zh-CN zh-HK zh-TW
142.1 65.1 29.2 59.8 63.4 81.0 4.5 68.0 99.7 62.6
text embedding e, and zrespectively (see Figure 2). The loss
function is given by:
L=Œ≤‚àó
1.0‚àíe¬∑z
||e|| ||z||
(1)
Where Œ≤is used to scale up the magnitude of the cosine
distance loss, a small loss magnitude could lead to extremely
small gradient update values and significantly longer training
times. Upscaling the loss by a constant Œ≤could alleviate this
problem. The parameters of the speech branch are tuned to
minimize the above-mentioned loss L. The text encoder LaBSE
remains fixed during training along with the feature extractor
hof the pre-trained XLS-Rencoder (See Figure 2 and [17] for
training details).
IV. M ETHOD
A. Expanding SAMU-XLS-R
We double the number of languages supported by
SAMU -XLS-Rto 53 than the 25 supported previously. For
training SAMU -XLS-R, we use speech utterances from multiple
languages annotated with their text transcripts in the same
language. We refer to SAMU -XLS-R‚Äôs training data pool as
Background (BKG) data. BKG data depends on the down-
stream MST task domain. Ideally, we would like BKG data
from the same domain as the MST task (in-domain). We use
the multilingual transcribed speech from the CommonV oice-
Version8 (CoV o-V8) corpus [37]. CoV o-V8 consists of tran-
scribed speech in 87 languages (26 language families). Around
53 languages overlap with the language set supported by
Language-Agnostic BERT Sentence Encoder (LaBSE), which
provides semantic supervision for training the SAMU-XLS-R
speech encoder. The 53 languages are: Arabic (ar), Belarusian
(be), Bulgarian (bg), Catalan (ca), Czech (cs), Welsh (cy),
Danish (da), German (de), Greek (el), English (en), Estonian
(eo), Spanish (es), Estonian (et), Basque (eu), Persian (fa),
Finnish (fi), French (fr), Western Frisian (fy-NL), Gaelic (ga-
IE), Galician (gl), Hausa (ha), Hindi (hi), Hungarian (hu), In-
donesian (id), Italian (it), Japanese (ja), Georgian (ka), Khmer
(kmr), Kyrgyz (ky), Lithuanian (lt), Latvian (lv), Mongolian(mn), Maltese (mt), Dutch (nl), Polish (pl), Portuguese (pt),
Romanian (ro), Russian (ru), Kinyarwanda (rw), Slovak (sk),
Slovenian (sl), Swedish (sv-SE), Swahili (sw), Tamil (ta),
Thai (th), Turkish (tr), Tatar (tt), Uyghur (ug), Ukaranian
(uk), Uzbek (uz), Vietnamese (vi), Chinese-Mandarin (zh-
CN), Chinese-HongKong (zh-HK), Chinese-Taiwanese (zh-
TW). Table I presents the number of hours of transcribed
speech available in each of the 53 languages for SAMU -XLS-R
training. The total training hours is 12.7K. BKG data is highly
imbalanced, where the majority of the data comes from a
few high-resource languages. Following [17, Equation 3], we
balance the BKG data to have a proportionate representation
of different languages in a training mini-batch, which avoids
under-fitting to low-resource languages. The up-sampling pro-
cess involves repeating utterances in low-resource languages.
Unlike previous work, we use offline data augmentation,
namely speed perturbation [41], to improve the robustness of
the learned representations to the data distribution shifts.
B. Translation Model
1) Overview: We use the standard encoder-decoder archi-
tecture for our translation model. We initialize the encoder
using pre-trained SAMU -XLS-R. Following [23], the decoder is
initialized with the decoder of a pre-trained text-to-text trans-
lation model, namely MBART3. The encoder-decoder model is
trained using corpora that consist of tuples (a1:S,y1:L), where
y1:Lis the text translation sequence of the speech sequence
a1:S.
2) Training: Given a speech-text translation pair
(a1:S,y1:L), we tune the parameters of the translation
model to maximize the log-probability of y1:Lconditioned
on speech utterance a1:S(log-likelihood). Also, we employ
label-smoothing [42], [43], where a ground-truth yllabel is
randomly replaced with the label predicted by the model ÀÜyl.
We set the probability of token replacement as 0.3. We use
the Adam optimizer [44] to maximize the log-likelihood with
3https://github.com/facebookresearch/fairseq/tree/main/examples/
multilingual#mbart50-models

--- PAGE 5 ---
PREPRINT 5
the optimal (according to a development set) learning rate
(lr) of 5e-4. We use a three-phase learning rate scheduler [9]
as follows; (i) warm-up lr to 5e-4 for the first 10% of the
training iterations, (ii) keep the lr constant for the next 40%
of iterations, and (iii) decay the lr linearly for the rest of
training. We use 28K training iterations and train the model
on 8 A100 GPUs. A single training iteration uses a batch
size of 10 minutes of speech utterances paired with their text
translations. We use mixed-precision training style [45]; most
computations are performed on half-precision floating point
numbers except the final loss computation. We use the fairseq
toolkit [46] for model training.
Following [9], we mask (time and feature dimension) the
feature sequence f1:T, output by the CNN feature extractor
ofSAMU -XLS-R. The masking process is performed in the
following two steps: (i) with some probability, referred to
as the masking probability , choose a masking index, and (ii)
mask Mconsecutive indices starting from the chosen index.
Mis known as the masking span . For the time dimension, we
set 0.3 as the masking probability and the mask span of six.
For the feature dimension, we set the masking probability to
0.5 and 64 as the mask span. The time and feature masking
parameters are chosen according to a development set. The
above-mentioned data augmentation is akin to SpecAugment
[47], a commonly used speech spectrogram data augmentation
method for training speech-to-text generation models.
The translation model comprises 700 million trainable pa-
rameters (300M encoder and 400M decoder parameters). We
fine-tune only 75 million parameters. Most encoder parameters
are fixed to the pre-trained SAMU -XLS-Rparameters, and the
decoder parameters are fixed to the pre-trained MBART decoder.
Below we give details about encoder and decoder fine-tuning.
3) Encoder Fine-Tuning: We keep all the parameters of the
speech encoder fixed to their pre-trained SAMU -XLS-Rvalues.
Instead, we add a small number of downstream task-specific
parameters to each transformer layer of the speech encoder
following [48]. We insert adapters, a bottleneck Feed-Forward
layer, after the Multi-Headed Self-Attention ( MHSA ) and Fully-
Connected ( FC) blocks in each transformer layer. An adapter
comprises a single hidden layer with ReLU activation. The
input and the output layers have the same size. In contrast,
the hidden layer is a fraction of the input layer size. We found
the optimal (according to a development set) size of the hidden
layer to be a fourth of the input layer size.
The motivation for using adapters is two-fold: (i) Parameter
Efficiency: We only tune the adapter layer parameters, a frac-
tion of the total parameters that comprise the speech encoder.
In our case, the SAMU -XLS-Rencoder consists of 300M parame-
ters, and we add 75M task-specific adapter parameters. Hence,
we only fine-tune the 75M parameters on the downstream task.
(ii)Avoids Knowledge Forgetting: By freezing the encoder‚Äôs
parameters to pre-trained values, we preserve the essential
semantic knowledge encoded in the SAMU -XLS-Rencoder that
it acquired during the semantic knowledge-distillation pre-
training phase. We later show (Section VI-D2) that preserving
this semantic knowledge is essential to achieving good cross-
lingual transfer. Hence, adapters form a crucial part of our
cross-lingual transfer learning strategy. It must be noted thatresearch on adapters is ever-expanding. An in-depth analysis
of specific adapter choices on downstream task performance
is out-of-scope for this paper.
4) Decoder Fine-Tuning: Like the encoder, we keep most
parameters fixed to their pre-trained values acquired during the
multilingual text-to-text translation task. Unlike the encoder,
we do not introduce any new task-specific parameters. Instead,
we tune a fraction of the decoder‚Äôs parameters. Each layer in
the transformer decoder consists of the Masked (causal) Multi-
Headed Self-Attention ( MMHSA ), Encoder-Decoder Cross-
Attention ( CA), and two Fully-Connected ( FC) blocks. The
inputs to the MMHSA ,CA, and FCblocks are normalized via
Layer Normalization ( LN) method presented in [49]. Following
[23], we only tune the parameters of the LNandCAblocks.
We fine-tune CAbecause, previously, it is trained as part of
a decoder in a text-to-text translation pipeline. Hence, the CA
module needs to be fine-tuned again for the downstream MST
task to make it amenable to the input from the speech encoder.
Moreover, we fine-tune LNbecause it is task and dataset-
specific and empirically improves the performance of MST
[23].
5) Inference: We use beam search , with a beam size of 5,
to generate text translations for a given speech utterance. The
inference process is offline , i.e., the decoder takes the full input
speech utterance into account to generate the output transla-
tion. The decoder generates translation in an autoregressive
manner. We do not use any external language model during
inference.
V. E VALUATION
A. Translation Scenarios
We tackle the following translation scenarios in this work.
1) Multilingual Translation: We simultaneously train a
translation model on several speech-to-text translation tasks
in this scenario. E.g., we train a single model for all 21
X‚ÜíEN translation tasks in the CoV oST-2 benchmark. Most
of the training data come from a few high-resource translation
tasks such as FR ‚ÜíEN and DE ‚ÜíEN, while most tasks such
as ID‚ÜíEN are low-resource. In this scenario, we compare
different translation models to test for cross-lingual translation
task transfer from high to low-resource translation tasks.
2) Zero-Shot Multilingual Translation: Given a set of trans-
lation tasks, we train a translation model on a subset of the
tasks while keeping the rest hidden during training in this
translation scenario. E.g., we train an X ‚ÜíEN speech-to-text
translation model using high-resource translation tasks in the
CoV oST-2 benchmark while keeping the mid and low-resource
tasks unseen during training. We compare translation models
for zero-shot cross-lingual task transfer in this scenario from
high to mid and low-resource X ‚ÜíEN translation tasks.
B. Translation Tasks
We build translation models to tackle the following transla-
tion tasks in this work.

--- PAGE 6 ---
PREPRINT 6
Fig. 3: Number of hours of labeled training data (Y-Axis)
for all the 21 X ‚ÜíEN translation tasks in the CoV oST-2
benchmark.
2641841361134944181010743322222222050100150200250300
FRDECAESFAITRUPTZHNLTRETMNARSVLVSLTAJAIDCYHoursSource Spoken Language (X)Translation Model Training in the CoVoST-2 X √†EN Benchmark
TABLE II: Training data (hours) for the 72 translation tasks
X‚ÜíY in the Europarl Speech-to-Text translation benchmark.
SRC/TGT FR DE IT ES PT PL RO NL EN
FR - 21 20 21 22 20 18 22 32
DE 18 - 17 18 18 17 17 18 30
IT 21 21 - 21 21 21 19 20 37
ES 14 14 14 - 14 13 12 13 22
PT 10 10 10 10 - 9 9 9 15
PL 18 18 17 18 18 - 16 18 28
RO 12 12 12 12 12 12 - 12 24
NL 5 5 4 5 4 4 4 - 7
EN 81 83 80 81 81 79 72 80 -
1) X‚ÜíEN Speech-to-Text Translation: We build translation
models for the 21 X(Speech) ‚ÜíEN(Text) translation tasks in
the CoV oST-2 translation benchmark [30]. The 21 spoken
languages in CoV oST-2 are French (fr), German (de), Spanish
(es), Catalan (ca), Italian (it), Russian (ru), Chinese (zh),
Portuguese (pt), Persian (fa), Estonian (et), Mongolian (mn),
Dutch (nl), Turkish (tr), Arabic (ar), Swedish (sv), Latvian (lv),
Slovenian (sl), Tamil (ta), Japanese (ja), Indonesian (id), and
Welsh (cy). The 21 translation tasks are divided into high, mid,
and low-resource groups, depending on the amount of labeled
data available for a translation task. High-resource tasks have
more than 100 hours of labeled training data, mid-resource
tasks have between 10 and 100 hours, and low-resource have
less than ten hours of labeled training data. CoV oST-2 has four
high-resource translation tasks corresponding to fr, de, es, and
ca source languages and five mid-resource tasks corresponding
to it, ru, zh, pt, and fa languages. The rest of the tasks are low-
resource. Fig. 3 presents the training data for each of the 21
translation tasks. Notice the data imbalance among different
tasks.
In the multilingual translation scenario, we simultaneously
train translation models on all 21 tasks mentioned above. We
only train translation models on the four high-resource tasks
in the zero-shot scenario.
2) X‚ÜíY Speech-to-Text Translation: We develop transla-
tion models for the 72 X ‚ÜíY translation tasks in the Europarlbenchmark [29]. There are nine spoken languages in Europarl
namely, en, fr, de, it, es, pt, pl, ro, and nl. Speech utterances in
each spoken language are paired with their corresponding text
translations in eight other languages. In the zero-shot scenario,
we train the translation models on 32 tasks corresponding to
the following four source languages fr, de, es, and it. Each of
the four source languages is paired with eight target languages.
Table II shows each translation task‚Äôs labeled training data.
C. Translation Models
1)SAMU -XLS-R-300M: We propose SAMU -XLS-R-300M
transformer model for translation in this work. The encoder
is initialized using the pre-trained SAMU -XLS-Rspeech encoder
(Section IV-A), and the decoder is initialized using the pre-
trained MBART text decoder. The suffix 300M in SAMU -XLS-R-
300M refers to the model‚Äôs size of 300M parameters.
2)XLS-R-(300M, 1B, 2B): We compare SAMU -XLS-R-300M
with three XLS-Rspeech encoder based translation models
namely, XLS-R-300M, XLS-R-1B, and XLS-R-2B. The three
translation models differ from SAMU -XLS-R-300M model in
that the encoder of the translation model is initialized using
pre-trained XLS-Rspeech encoders of different sizes ranging
from 300M to 2B parameters. The decoder is initialized
using the pre-trained MBART decoder. Unlike our multimodal
SAMU -XLS-Rspeech encoder, XLS-Ris only trained using
unlabeled speech data. Also, SAMU -XLS-Ris specifically trained
to learn semantic representations, while XLS-Rhas no con-
straints imposed during its training phase to encode semantic
knowledge.
3) mSLAM: We compare SAMU -XLS-R-300M translation
model against two mSLAM [18] speech encoder based trans-
lation models namely, mSLAM -600M, and mSLAM -2B. Like
SAMU -XLS-R,mSLAM speech encoder is a multimodal speech-
text encoder. Unlike SAMU -XLS-R, which is trained using
semantic supervision from a pre-trained semantic text encoder,
mSLAM is not trained with explicit semantic supervision.
4) Cascaded Translation: We compare SAMU -XLS-R-300M
with a strong cascaded translation system. We perform the
translation in two steps: (i) Transcribe the speech utterance us-
ing an ASR model, and (ii) Use a text-to-text translation model
to translate the ASR transcript to text in a target language. We
use whisper-large-v24[50] as the ASR model in the first step
andMBART [28] text-to-text translation model for the second
step in the cascade. For X ‚ÜíEN cascade, we use MBART -many-
to-English text-to-text translation model5. The Whisper ASR
model is multilingual that supports transcription of around 93
languages. Since [50] shows that whisper achieves state-of-
the-art ASR performance on several public benchmarks, we
choose Whisper for automatically transcribing speech. MBART -
many-to-English translation model can translate text from 50
languages to English.
5) Transcripts: As a topline, we use the ground-truth text
transcripts corresponding to speech utterances and use MBART -
many-to-English to translate to English.
4https://huggingface.co/openai/whisper-large-v2
5https://huggingface.co/facebook/mbart-large-50-many-to-one-mmt

--- PAGE 7 ---
PREPRINT 7
TABLE III: We compare our proposed SAMU -XLS-R-300M translation model with several other translation models, whose
encoders are initialized using differently sized pre-trained XLS-Rmultilingual unimodal speech encoders. The performance is
measured using BLEU-4, Google-BLEU, ROUGE-L, METEOR, BERTScore, and NIST translation metrics.
BLEU-4 Google-BLEU
Model High Mid Low TRFGap High Mid Low TRFGap
XLS-R-300M 30.6 18.9 5.1 25.1 0.36 0.24 0.10 0.26
XLS-R-1B 34.3 25.5 11.7 22.6 0.38 0.29 0.16 0.22
XLS-R-2B 36.1 27.7 15.1 21.0 0.39 0.31 0.20 0.19
SAMU -XLS-R-300M 34.4 31.1 20.3 14.1 0.38 0.34 0.24 0.13
Cascaded 32.6 29.7 22.5 10.1 0.36 0.33 0.26 0.10
Transcripts 36.4 34.2 27.9 8.5 0.39 0.37 0.32 0.08
ROUGE-L METEOR
Model High Mid Low TRFGap High Mid Low TRFGap
XLS-R-300M 0.60 0.44 0.23 0.37 0.62 0.45 0.24 0.38
XLS-R-1B 0.61 0.49 0.31 0.30 0.63 0.50 0.32 0.31
XLS-R-2B 0.63 0.53 0.38 0.25 0.65 0.53 0.39 0.26
SAMU -XLS-R-300M 0.62 0.58 0.42 0.19 0.64 0.59 0.45 0.19
Cascaded 0.60 0.56 0.45 0.14 0.61 0.56 0.46 0.15
Transcripts 0.64 0.61 0.52 0.11 0.66 0.62 0.54 0.12
BERTScore NIST
Model High Mid Low TRFGap High Mid Low TRFGap
XLS-R-300M 0.56 0.33 0.04 0.52 8.0 5.0 1.9 6.1
XLS-R-1B 0.58 0.41 0.16 0.42 8.3 5.9 3.0 5.3
XLS-R-2B 0.61 0.45 0.25 0.36 8.6 6.3 3.7 4.9
SAMU -XLS-R-300M 0.59 0.54 0.34 0.25 8.3 7.1 4.4 4.0
Cascaded 0.56 0.50 0.37 0.20 8.2 6.9 4.9 3.3
Transcripts 0.62 0.58 0.47 0.14 8.7 7.7 5.7 3.0
TABLE IV: We compare our proposed SAMU -XLS-R-300M
translation model with mSLAM translation models, whose en-
coders are initialized using differently sized pre-trained mSLAM
multilingual multimodal speech encoders. The performance is
measured using the BLEU-4 translation metric.
BLEU-4
Model High Mid Low TRFGap
mSLAM -600M 37.6 27.8 15.1 22.5
mSLAM -2B 37.8 29.6 18.5 19.3
SAMU -XLS-R-300M 34.4 31.1 20.3 14.1
VI. R ESULTS
A. Multilingual X ‚ÜíEN Translation
Table III shows the performance of different translation
models on the high, mid, and low-resource translation groups
in the CoV oST-2 speech-to-text translation benchmark. We
compare our proposed SAMU -XLS-R-300M translation model
against XLS-R-300M, XLS-R-1B, and XLS-R-2B translation
models. CoV oST-2 comprises 21 X ‚ÜíEN translation tasks,
and the translation models are trained simultaneously on all
translation tasks. See Section V-B for details about the trans-lation tasks, and Section V-C for details about the different
translation models.
The model performance is measured using the standard
translation metrics, namely BLEU-4 [51], Google-BLEU [52],
ROUGE-L [53], METEOR [54], BERTScore [55], and NIST
[56]. We make the following observations : (i) On High
resource tasks , the XLS-R-2B model performs the best, with
SAMU-XLS-R-300M lagging a couple of points behind. Com-
pared to the similar-sized XLS-R-300M model, SAMU -XLS-R-
300M performs 4 BLEU points better. (ii) On Mid resource
tasks ,SAMU -XLS-R-300M outperforms all the models achiev-
ing a BLEU score of 31.1, which is significantly better than
XLS-R-300M model‚Äôs BLEU score of 5.1. SAMU -XLS-R-300M
even outperforms the much larger XLS-R-2B speech encoder by
3.3 BLEU points. (iii) On Low resource tasks ,SAMU -XLS-R-
300M performs the best. Compared to the similar-sized XLS-R-
300M model, SAMU -XLS-R-300M does better by 15 BLEU
points. It also outperforms the much larger XLS-R-2B by
5.2 BLEU points. The cross-lingual transfer gap (TRFGap),
which is the difference in performance between high and low
resource task groups, is significantly less (14.1 BLEU) for
SAMU -XLS-R-300M model compared to other models. Second
toSAMU -XLS-R-300M is XLS-R-2B, which has a TRFGap of 21
BLEU points while having 500% more parameters. Similar
arguments can be made using metrics other than BLEU-4.

--- PAGE 8 ---
PREPRINT 8
TABLE V: We compare our proposed SAMU -XLS-R-300M translation model with several other translation models, whose
encoders are initialized using differently sized pre-trained XLS-Rmultilingual unimodal speech encoders. The performance is
measured using the BLEU-4 translation metric.
Model fr de es ca it fa ru zh pt
XLS-R-300M 34.9 29.3 35.9 30.6 30.9 6.3 30.0 5.2 30.8
XLS-R-1B 36.3 31.4 37.8 32.1 33.5 9.1 35.7 6.9 41.4
XLS-R-2B 37.6 33.6 39.1 33.9 35.0 13.0 39.5 9.4 41.8
SAMU -XLS-R-300M 36.1 31.7 37.9 31.9 34.0 22.0 42.1 13.1 44.2
Cascaded 32.7 30.9 36.4 30.3 32.5 14.3 42.9 14.4 44.2
Transcripts 38.4 34.1 41.1 32.1 36.4 24.2 46.0 17.4 46.9
Model nl tr et mn ar sv lv sl ta
XLS-R-300M 25.1 6.7 4.0 0.2 3.9 11.3 7.4 8.3 0.0
XLS-R-1B 29.6 11.1 8.0 0.6 9.3 24.5 15.7 16.8 0.1
XLS-R-2B 31.6 16.9 11.2 1.5 17.1 29.7 19.7 19.0 0.5
SAMU -XLS-R-300M 34.9 28.4 11.6 3.4 36.6 28.5 1.9 12.9 4.0
Cascaded 33.0 25.1 17.4 0.0 35.7 39.6 20.3 27.8 1.6
Transcripts 36.3 28.8 25.6 3.8 44.6 46.4 29.2 38.4 2.2
Model cy ja id
XLS-R-300M 2.8 0.6 1.2
XLS-R-1B 6.61 1.3 7.8
XLS-R-2B 14.2 3.5 16.4
SAMU -XLS-R-300M 34.1 13.1 34.4
Cascaded 6.4 19.4 43.6
Transcripts 9.0 20.8 49.8
On all metrics, SAMU -XLS-Ris significantly better than the
XLS-Rbaselines and comparable to the strong cascaded system,
while significantly worse than the model which uses the human
transcripts corresponding to speech utterances for translation
into English using the MBART -many-to-English text translation
model. For the subsequent experiments, we only report BLEU-
4 scores.
Table IV compares SAMU -XLS-R-300M translation model
with mSLAM -600M, and mSLAM -2B models that use differently
sized pre-trained multimodal (speech-text) multilingual mSLAM
speech encoder. SAMU -XLS-R-300M performs better on mid-
and low-resource translation tasks. Importantly, SAMU -XLS-R-
300M has a lower cross-lingual transfer gap (TRFGap) be-
tween high and low resource groups of 14.1 BLEU points
compared to 22.5 for mSLAM -600M, and 19.3 for mSLAM -2B.
The BLEU scores for mSLAM models are lifted from [18].
Since mSLAM [18] report only BLEU scores on the CoV oST-
2 benchmark, and we do not have access to mSLAM models, we
can not evaluate the model using metrics other than BLEU-4.
The above observations validate our claims that building
translation technology on top of semantic speech representa-
tions would increase the cross-lingual task knowledge transfer
from high to low-resource languages. Similar inferences can
be reached by using metrics other than BLEU-4.
Table V shows the performance of different translation
models on each of the 21 X ‚ÜíEN speech-to-text transla-
tion tasks in the CoV oST-2 benchmark. We observe that
SAMU -XLS-R-300M significantly outperforms the similarly
sized XLS-R-300M translation model on several mid and low-resource languages. Some notable improvements are for the
following source languages: id (34.4 vs. 1.2 BLEU), cy (34.1
vs. 2.8 BLEU), ja (13.1 vs 0.6 BLEU), sv (28.5 vs 11.3
BLEU), tr (28.4 vs 6.7 BLEU), ar (36.6 vs. 3.9 BLEU),
fa (22.0 vs. 6.3 BLEU), and pt (44.2 vs. 30.8 BLEU). For
some source languages SAMU -XLS-R-300M does not do well,
such as lv (1.9 BLEU), sl (12.9 BLEU), mn (3.4 BLEU), et
(11.6 BLEU), ta (4.0), ja (13.1 BLEU), and zh (13.1 BLEU).
For ta, ja, mn, and zh, the topline ( Transcripts ) model
also performs poorly. For sl, and lv, the topline text-to-text
translation and cascaded models perform significantly better
than the SAMU -XLS-R-300M model.
Poor performance of SAMU -XLS-R-300M on lv, mn, and
sl can be explained by the lack of available transcribed
speech data in these languages for multimodal pre-training of
SAMU -XLS-R(Section IV-A). We have 7 hours for lv, 12 hours
for mn, and 9 hours of transcribed speech for sl, compared
to 317 hours for fa, 85 hours for ar, 116 hours for cy, 98
hours for nl, 40 hours for sv, 25.8 hours for id, 162 hours for
ru, 400 hours for es, and close to 1K hours for fr, de, and
ca. However, for ta, zh, and ja we have a decent amount of
transcribed speech, but still, the performance is relatively poor.
This can be explained away by observing the performance
of the topline ( Transcripts ), where we use a pre-trained
MBART -many-to-English text-to-text translation model (Details
in Section V-C) that translates the ground-truth transcripts
corresponding to speech utterances in language X to text in
English. The topline performance for zh, ja, and ta is relatively
poor. Since we use the decoder of MBART -many-to-English

--- PAGE 9 ---
PREPRINT 9
model to initialize the decoder of our speech-to-text translation
model SAMU -XLS-R-300M, we also observe poor speech-text-
translation performance on these tasks.
B. Zero-Shot X ‚ÜíEN Speech-to-Text Translation.
Next, we train the translation models on four high-resource
X‚ÜíEN translation tasks in the CoV oST-2 benchmark (See
Section V-B for details). We evaluate the X ‚ÜíEN transla-
tion models on the high, mid, and low task groups to test
for zero-shot cross-lingual transfer capability of SAMU -XLS-R-
300M from high to mid and low-resource X ‚ÜíEN tasks. We
compare SAMU -XLS-R-300 with XLS-R-300M translation model.
Figure 4 presents the results. We observe that SAMU -XLS-R-
300M performs on average 18.8 BLEU points better in the
mid-resource and 11.9 BLEU points in the low-resource group.
The cross-lingual transfer gap between the high & mid and
high & low groups is significantly smaller for SAMU -XLS-R-
300M (9.0, and 20.8) than XLS-R-300M (25.2, and 30.1).
Fig. 4: We report average BLEU-4 for the zero-shot X ‚ÜíEN
multilingual speech-to-text translation scenario on the high,
mid, and low resource task groups in the CoV oST-2 bench-
mark. We compare our translation model SAMU -XLS-R-300M
with the similarly sized XLS-R-300M translation model. The
translation models are only trained on high-resource groups,
while the mid and low-resource groups are unseen during
training.
3133.6
5.824.6
0.912.80510152025303540
XLS-R-300MSAMU-XLS-R-300MBLEU-4
HighMidLowUnseen Groups25.230.19.020.8Unseen Groups
These results strengthen our claims that building speech
translation technology with semantic speech representations
would improve cross-lingual transfer across languages. Note
that zero-shot implies that the translation model during its
training does not see any paired X ‚ÜíEN translation data for
mid and low-resource languages X. Transcribed speech data
was available for these languages during multimodal pre-
training of SAMU -XLS-Rspeech encoder which is used to ini-
tialize the encoder of the SAMU -XLS-R-300M translation model.
C. Zero-Shot X ‚ÜíY Speech-to-Text Translation.
Finally, to further bolster our claims about the usefulness of
semantic speech representations for translation, we compare
translation models on the 72 X ‚ÜíY translation tasks in the
Europarl Benchmark. See Section V-B for details about trans-
lation tasks and available data for training translation models.We train translation models SAMU -XLS-R-300M and XLS-R-
300M on a 32-task subset out of 72 tasks. The 32 tasks
correspond to four source spoken languages: fr, de, es, and
it, while the remaining 40 tasks correspond to five source
languages: pl, pt, ro, nl, and en. Speech utterances in each
source language are paired with text translations in eight other
languages. Unlike X ‚ÜíEN translation models discussed above,
we initialize the decoder of X ‚ÜíY translation models with the
decoder of MBART -many-to-many6model instead of MBART -
many-to-English, since we have to generate translations in
multiple target languages.
Figure 5 compares the performance of XLS-R-300M, and
SAMU -XLS-R-300M on the 72 translation tasks. We report the
absolute BLEU-4 score improvement that SAMU -XLS-R-300M
achieves over XLS-R-300M baseline translation model. The
darker the cell in the figure, the greater the improvement in
the BLEU score. We observe that SAMU -XLS-R-300M performs
drastically better than XLS-R-300M on the 40 unseen (during
translation model training) translation tasks and at par on the
32 seen (during translation model training) translation tasks.
We observe the biggest improvements in unseen tasks such
as pl‚Üíen (23.8), ro ‚Üíen (21.9), pl ‚Üíen (19.9), pl ‚Üífr (19.6),
ro‚Üíes (17.2), etc. Overall, SAMU -XLS-R-300M improves over
XLS-R-300M baseline model by an average of 12 BLEU-4
points.
TABLE VI: We compare the translation model‚Äôs performance
when using Adapter-based fine-tuning vs. fine-tuning all the
encoder parameters. The performance is measured using the
BLEU-4 translation metric.
Model fr de es it pl pt ro nl
XLS-R-F 33.0 25.4 29.4 30.3 2.4 12.7 7.6 5.2
XLS-R-A 29.8 22.4 25.5 27.2 3.2 15.6 9.1 8.2
SAMU -XLS-R-F 33.2 26.3 29.7 30.7 3.0 12.2 9.8 7.0
SAMU -XLS-R-A 32.3 25.5 29.3 28.7 24.3 18.4 29.9 24.1
D. Analysis
1)XLS-Rvs.XLS-R-CTC: The experiments above compare
SAMU -XLS-Rwith XLS-Rmultilingual speech encoder. XLS-R
is trained using multilingual unlabeled speech data in 128
languages via self-supervised learning, while SAMU -XLS-Rfine-
tunes pre-trained XLS-Rusing multilingual transcribed speech
data via semantic knowledge distillation. But, what if instead
of fine-tuning pre-trained XLS-Rwith semantic supervision
from text transcriptions like SAMU -XLS-Rdoes, we fine-tune
XLS-Ron the task of Automatic Speech Recognition using the
same multilingual transcribed speech data that was used for
SAMU -XLS-Rtraining? We compare the translation performance
ofXLS-RandXLS-R-CTC encoders in Fig. 6. XLS-R-CTC refers
to the encoder we get after supervised CTC-based fine-tuning
of pre-trained XLS-Rusing multilingual transcribed speech
data. CTC [57] is a standard framework for training ASR
models end-to-end . We observe that the pre-trained XLS-R
encoder performs better than XLS-R-CTC encoder. XLS-R-CTC
6https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt

--- PAGE 10 ---
PREPRINT 10
Fig. 5: Absolute BLEU score improvements using SAMU -XLS-R-300M over XLS-R-300M baseline on the 72 X ‚ÜíY translation
tasks in the Europarl benchmark. The translation models are trained on a subset of 32 translation tasks, corresponding to four
source languages, while 40 tasks are unseen during training corresponding to five source languages.
SRC/TGTFRDEESITPLPTRONLENFRNA0.80-0.4-0.1-1.50.7-0.11.2DE2.3NA1.51.310.91.6-0.12ES0.90.8NA0.31.70.31.412.9IT-0.8-0.10.1NA-0.8-1.60.4-0.41.3PL19.613.719.914.5NA16.116.115.523.8PT1.41.11.60.30.5NA1.90.42RO18.411.517.213.711.214.8NA12.721.9NL11.59.58.997.410.910.1NA13.3EN17.713.519.614.19.716.617.513.9NAUnseen During TrainingTraining Data
Fig. 6: We compare on the Europarl X ‚ÜíEN benchmark
XLS-RandXLS-R-CTC initialization of the translation model‚Äôs
encoder. XLS-Ris pre-trained using unlabeled speech via self-
supervised learning, while XLS-R-CTC refers to the XLS-R
encoder that is fine-tuned (after self-supervised pre-training)
using transcribed speech data. We report the BLEU-4 score
for eight source spoken languages. Speech utterances in each
source language are paired with its English text translations.
332525.417.129.421.330.322.5
2.43.812.711.27.69.65.27.405101520253035
XLS-RXLS-R-CTCBLEU-4
FRDEESITPLPTRONL
encoder is slightly better on some translation tasks. Still, the
difference is so small that our previous observations on the
efficacy of our proposed semantic speech encoder SAMU -XLS-R
are not impacted, even though we compared SAMU -XLS-R
with the XLS-Rencoder, which is trained using unlabeled
multilingual speech.
2) Adapter vs. Full Encoder Fine-Tuning: As mentioned in
Section IV-B3, we perform Adapter-based fine-tuning of the
translation model‚Äôs encoder, where we insert new task-specific
parameters in the form of adapters in each encoder layer. We
only fine-tune adapter layers while keeping the rest of the
layers frozen to their pre-trained values. Table VI compares
the translation model‚Äôs performance when using Adapter-
based fine-tuning vs. fine-tuning all the encoder parameters.
We observe that using adapter fine-tuning with XLS-R-300M
(XLS-R-A) translation model brings performance gains on low-
resource tasks such as ro ‚Üíen, nl‚Üíen, pt‚Üíen, and pl ‚Üíen.
In contrast, the performance degrades significantly for higher-
resource tasks compared to full encoder fine-tuning ( XLS-R-F).
We observe a similar trend with SAMU -XLS-R-300M translation
model. But, the performance gains for low-resource tasks
are drastic with adapter-based fine-tuning of the encoder(SAMU -XLS-R-A). Again, this is due to our proposed semantic
speech encoder SAMU -XLS-R, which results in a significant
cross-lingual transfer from high to low-resource translation
tasks. This result also shows that preserving semantic knowl-
edge during training for downstream translation tasks, learned
by the SAMU -XLS-Rencoder as a result of our multimodal
learning framework, is essential.
VII. C ONCLUSION
This paper addresses the central question of cross-lingual
transfer learning in Natural Language Processing. We focus
on the problem of multilingual spoken language translation,
which we model using the standard encoder-decoder model.
We analyze the impact of different encoder initializations on
the downstream translation task performance. We show that
by initializing the encoder with an encoder that we pre-train
using the newly introduced semantic knowledge distillation
framework SAMU -XLS-R, we achieve significantly better cross-
lingual transfer in the downstream speech-to-text translation
task than the baselines. The baseline translation models use the
state-of-the-art multilingual pre-trained speech encoder XLS-R
and others for initialization.
To substantiate our claims, we perform multilingual trans-
lation on two public benchmarks, CoV oST-2 and Europarl.
On the 21 X ‚ÜíEnglish CoV oST-2 speech translation tasks,
we achieve an average improvement of 12.8 BLEU points. In
the zero-shot scenario, where we train the translation model
only on the four high-resource languages while keeping the
rest 17 languages unseen (during training), we achieve an
average improvement of 11.8 BLEU points over the baseline
XLS-Rencoder initialization. In particular, we achieve drastic
improvements of 18.8 and 11.9 average BLEU points on
medium and low-resource languages, respectively. We made
similar observations on the Europarl X ‚ÜíY speech-to-text
translation benchmark.
Our work has limitations. Currently, training SAMU -XLS-R
requires access to multilingual transcribed data, which could
be hard for many spoken languages. Also, the dependence on
a pre-trained text encoder hinders expanding SAMU -XLS-Rto
more languages. Hence, future work should focus on injecting
semantic information via weakly supervised learning using
unaligned speech and text data and without using the LaBSE
text encoder.

--- PAGE 11 ---
PREPRINT 11
REFERENCES
[1] D. Harwath, A. Torralba, and J. Glass, ‚ÄúUnsupervised learning of
spoken language with visual context,‚Äù Advances in Neural Information
Processing Systems , vol. 29, 2016.
[2] W.-N. Hsu, Y . Zhang, and J. Glass, ‚ÄúUnsupervised learning of disentan-
gled and interpretable representations from sequential data,‚Äù Advances
in neural information processing systems , vol. 30, 2017.
[3] S. Khurana, S. R. Joty, A. Ali, and J. Glass, ‚ÄúA factorial deep
markov model for unsupervised disentangled representation learning
from speech,‚Äù in ICASSP 2019 - 2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , 2019, pp. 6540‚Äì
6544.
[4] S. Pascual, M. Ravanelli, J. Serr√†, A. Bonafonte, and Y . Bengio,
‚ÄúLearning problem-agnostic speech representations from multiple self-
supervised tasks,‚Äù arXiv:1904.03416 , 2019.
[5] S. Schneider, A. Baevski, R. Collobert, and M. Auli, ‚Äúwav2vec:
Unsupervised pre-training for speech recognition,‚Äù arXiv:1904.05862 ,
2019. [Online]. Available: https://arxiv.org/abs/1904.05862
[6] A. Baevski, S. Schneider, and M. Auli, ‚Äúvq-wav2vec: Self-supervised
learning of discrete speech representations,‚Äù in International Conference
on Learning Representations , 2020. [Online]. Available: https:
//openreview.net/forum?id=rylwJxrYDS
[7] Y .-A. Chung and J. Glass, ‚ÄúGenerative pre-training for speech with
autoregressive predictive coding,‚Äù in ICASSP 2020-2020 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2020, pp. 3497‚Äì3501.
[8] S. Khurana, A. Laurent, W.-N. Hsu, J. Chorowski, A. Lancucki,
R. Marxer, and J. Glass, ‚ÄúA convolutional deep markov model for
unsupervised speech representation learning,‚Äù arXiv:2006.02547 , 2020.
[Online]. Available: https://arxiv.org/abs/2006.02547
[9] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, ‚Äúwav2vec 2.0:
A framework for self-supervised learning of speech representations,‚Äù
arXiv:2006.11477 , 2020.
[10] D. Harwath, W.-N. Hsu, and J. Glass, ‚ÄúLearning hierarchical discrete
linguistic units from visually-grounded speech,‚Äù in International
Conference on Learning Representations , 2020. [Online]. Available:
https://openreview.net/forum?id=B1elCp4KwH
[11] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and
M. Auli, ‚ÄúUnsupervised cross-lingual representation learning for
speech recognition,‚Äù arXiv:2006.13979 , 2020. [Online]. Available:
https://arxiv.org/abs/2006.13979
[12] A. T. Liu, S.-W. Li, and H. yi Lee, ‚ÄúTERA: Self-supervised
learning of transformer encoder representation for speech,‚Äù IEEE/ACM
Transactions on Audio, Speech, and Language Processing , vol. 29, pp.
2351‚Äì2366, 2021. [Online]. Available: https://doi.org/10.1109%2Ftaslp.
2021.3095662
[13] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and
A. Mohamed, ‚ÄúHubert: Self-supervised speech representation learning
by masked prediction of hidden units,‚Äù IEEE/ACM Transactions on
Audio, Speech, and Language Processing , vol. 29, pp. 3451‚Äì3460, 2021.
[14] Y .-A. Chung, Y . Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and
Y . Wu, ‚ÄúW2v-bert: Combining contrastive learning and masked language
modeling for self-supervised speech pre-training,‚Äù arXiv:2108.06209 ,
2021. [Online]. Available: https://arxiv.org/abs/2108.06209
[15] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal,
K. Singh, P. von Platen, Y . Saraf, J. Pino, A. Baevski, A. Conneau, and
M. Auli, ‚ÄúXls-r: Self-supervised cross-lingual speech representation
learning at scale,‚Äù arXiv:2111.09296 , 2021. [Online]. Available:
https://arxiv.org/abs/2111.09296
[16] A. H. Liu, S. Jin, C.-I. J. Lai, A. Rouditchenko, A. Oliva, and J. Glass,
‚ÄúCross-modal discrete representation learning,‚Äù arXiv:2106.05438 ,
2021. [Online]. Available: https://arxiv.org/abs/2106.05438
[17] S. Khurana, A. Laurent, and J. Glass, ‚ÄúSamu-xlsr: Semantically-aligned
multimodal utterance-level cross-lingual speech representation,‚Äù IEEE
Journal of Selected Topics in Signal Processing , pp. 1‚Äì13, 2022.
[18] A. Bapna, C. Cherry, Y . Zhang, Y . Jia, M. Johnson, Y . Cheng,
S. Khanuja, J. Riesa, and A. Conneau, ‚ÄúmSLAM: Massively
multilingual joint pre-training for speech and text,‚Äù arXiv:2202.01374 ,
2022. [Online]. Available: https://arxiv.org/abs/2202.01374
[19] A. v. d. Oord, Y . Li, and O. Vinyals, ‚ÄúRepresentation learning
with contrastive predictive coding,‚Äù arXiv:1807.03748 , 2018. [Online].
Available: https://arxiv.org/abs/1807.03748
[20] M. Gutmann and A. Hyv√§rinen, ‚ÄúNoise-contrastive estimation: A new
estimation principle for unnormalized statistical models,‚Äù in Proceedings
of the Thirteenth International Conference on Artificial Intelligence
and Statistics , ser. Proceedings of Machine Learning Research, Y . W.Teh and M. Titterington, Eds., vol. 9. Chia Laguna Resort, Sardinia,
Italy: PMLR, 13‚Äì15 May 2010, pp. 297‚Äì304. [Online]. Available:
https://proceedings.mlr.press/v9/gutmann10a.html
[21] R. Bommasani and et. al., ‚ÄúOn the opportunities and risks of
foundation models,‚Äù arXiv:2108.07258 , 2021. [Online]. Available:
https://arxiv.org/abs/2108.07258
[22] M. Rivi√®re, A. Joulin, P.-E. Mazar√©, and E. Dupoux, ‚ÄúUnsupervised
pretraining transfers well across languages,‚Äù arXiv:2002.02848 , 2020.
[23] X. Li, C. Wang, Y . Tang, C. Tran, Y . Tang, J. Pino, A. Baevski,
A. Conneau, and M. Auli, ‚ÄúMultilingual speech translation with
efficient finetuning of pretrained models,‚Äù arXiv:2010.12829 , 2020.
[Online]. Available: https://arxiv.org/abs/2010.12829
[24] J. Shor, A. Jansen, W. Han, D. Park, and Y . Zhang, ‚ÄúUniversal
paralinguistic speech representations using self-supervised conformers,‚Äù
arXiv:2110.04621 , 2021. [Online]. Available: https://arxiv.org/abs/2110.
04621
[25] S. wen Yang, P.-H. Chi, Y .-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y . Y .
Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng,
K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe,
A. Mohamed, and H. yi Lee, ‚ÄúSuperb: Speech processing universal
performance benchmark,‚Äù arXiv:2105.01051 , 2021.
[26] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence learning
with neural networks,‚Äù arXiv:1409.3215 , 2014. [Online]. Available:
https://arxiv.org/abs/1409.3215
[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all
you need,‚Äù arXiv:1706.03762 , 2017. [Online]. Available: https:
//arxiv.org/abs/1706.03762
[28] Y . Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad,
M. Lewis, and L. Zettlemoyer, ‚ÄúMultilingual denoising pre-training
for neural machine translation,‚Äù Transactions of the Association for
Computational Linguistics , vol. 8, pp. 726‚Äì742, 2020. [Online].
Available: https://aclanthology.org/2020.tacl-1.47
[29] J. Iranzo-S√°nchez, J. A. Silvestre-Cerd√†, J. Jorge, N. Rosell√≥,
A. Gim√©nez, A. Sanchis, J. Civera, and A. Juan, ‚ÄúEuroparl-st: A
multilingual corpus for speech translation of parliamentary debates,‚Äù
arXiv:1911.03167 , 2019. [Online]. Available: https://arxiv.org/abs/1911.
03167
[30] C. Wang, J. Pino, A. Wu, and J. Gu, ‚ÄúCovost: A diverse multilingual
speech-to-text translation corpus,‚Äù arXiv:2002.01320 , 2020. [Online].
Available: https://arxiv.org/abs/2002.01320
[31] G. Laperri√®re, V . Pelloin, M. Rouvier, T. Stafylakis, and Y . Est√®ve,
‚ÄúOn the use of semantically-aligned speech representations for spoken
language understanding,‚Äù arXiv:2210.05291 , 2022. [Online]. Available:
https://arxiv.org/abs/2210.05291
[32] F. Feng, Y . Yang, D. Cer, N. Arivazhagan, and W. Wang, ‚ÄúLanguage-
agnostic bert sentence embedding,‚Äù arXiv:2007.01852 , 2020. [Online].
Available: https://arxiv.org/abs/2007.01852
[33] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner, ‚ÄúGradient-based learning
applied to document recognition,‚Äù Proceedings of the IEEE , vol. 86,
no. 11, pp. 2278‚Äì2324, 1998.
[34] D. Hendrycks and K. Gimpel, ‚ÄúGaussian error linear units (gelus),‚Äù
arXiv:1606.08415 , 2016. [Online]. Available: https://arxiv.org/abs/1606.
08415
[35] A. F. Agarap, ‚ÄúDeep learning using rectified linear units (ReLU),‚Äù
arXiv:1803.08375 , 2018. [Online]. Available: https://arxiv.org/abs/1803.
08375
[36] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza,
M. Williamson, J. Pino, and E. Dupoux, ‚ÄúVoxPopuli: A large-scale
multilingual speech corpus for representation learning, semi-supervised
learning and interpretation,‚Äù in Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) . Online: Association for
Computational Linguistics, Aug. 2021, pp. 993‚Äì1003. [Online].
Available: https://aclanthology.org/2021.acl-long.80
[37] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer,
R. Morais, L. Saunders, F. M. Tyers, and G. Weber, ‚ÄúCommon Voice:
A massively-multilingual speech corpus,‚Äù arXiv:1912.06670 , 2020.
[38] V . Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, ‚ÄúMls: A
large-scale multilingual dataset for speech research,‚Äù arXiv:2012.03411 ,
2020.
[39] J. Valk and T. Alum√§e, ‚ÄúV oxlingua107: a dataset for spoken
language recognition,‚Äù arXiv:2011.12998 , 2020. [Online]. Available:
https://arxiv.org/abs/2011.12998
[40] P. Safari, M. India, and J. Hernando, ‚ÄúSelf-attention encoding and
pooling for speaker recognition,‚Äù arXiv:2008.01077 , 2020.

--- PAGE 12 ---
PREPRINT 12
[41] T. Ko, V . Peddinti, D. Povey, and S. Khudanpur, ‚ÄúAudio augmentation
for speech recognition,‚Äù in Proc. Interspeech 2015 , 2015, pp. 3586‚Äì3589.
[42] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄúRethinking
the inception architecture for computer vision,‚Äù in 2016 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR) , 2016, pp.
2818‚Äì2826.
[43] R. M√ºller, S. Kornblith, and G. Hinton, ‚ÄúWhen does label
smoothing help?‚Äù arXiv:1906.02629 , 2019. [Online]. Available: https:
//arxiv.org/abs/1906.02629
[44] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic
optimization,‚Äù arXiv:1412.6980 , 2014. [Online]. Available: https:
//arxiv.org/abs/1412.6980
[45] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen,
D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and
H. Wu, ‚ÄúMixed precision training,‚Äù arXiv:1710.03740 , 2017. [Online].
Available: https://arxiv.org/abs/1710.03740
[46] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng,
D. Grangier, and M. Auli, ‚Äúfairseq: A fast, extensible toolkit for
sequence modeling,‚Äù arXiv:1904.01038 , 2019. [Online]. Available:
https://arxiv.org/abs/1904.01038
[47] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
and Q. V . Le, ‚ÄúSpecAugment: A simple data augmentation method for
automatic speech recognition,‚Äù arXiv:1904.08779 , 2019.
[48] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe,
A. Gesmundo, M. Attariyan, and S. Gelly, ‚ÄúParameter-efficient transfer
learning for nlp,‚Äù in Proc. ICML , 2019.
[49] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normalization,‚Äù
arXiv:1607.06450 , 2016.
[50] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, ‚ÄúRobust speech recognition via large-scale weak super-
vision.‚Äù
[51] M. Post, ‚ÄúA call for clarity in reporting BLEU scores,‚Äù in
Proceedings of the Third Conference on Machine Translation:
Research Papers . Belgium, Brussels: Association for Computational
Linguistics, Oct. 2018, pp. 186‚Äì191. [Online]. Available: https:
//www.aclweb.org/anthology/W18-6319
[52] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,
M. Krikun, Y . Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,
M. Johnson, X. Liu, ≈Åukasz Kaiser, S. Gouws, Y . Kato, T. Kudo,
H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young,
J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes,
and J. Dean, ‚ÄúGoogle‚Äôs neural machine translation system: Bridging the
gap between human and machine translation,‚Äù 2016.
[53] C.-Y . Lin, ‚ÄúROUGE: A package for automatic evaluation of summaries,‚Äù
inText Summarization Branches Out . Barcelona, Spain: Association for
Computational Linguistics, Jul. 2004, pp. 74‚Äì81. [Online]. Available:
https://aclanthology.org/W04-1013
[54] S. Banerjee and A. Lavie, ‚ÄúMETEOR: An automatic metric for
MT evaluation with improved correlation with human judgments,‚Äù in
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization . Ann Arbor,
Michigan: Association for Computational Linguistics, Jun. 2005, pp. 65‚Äì
72. [Online]. Available: https://www.aclweb.org/anthology/W05-0909
[55] M. K. Eddine, G. Shang, A. J.-P. Tixier, and M. Vazirgiannis, ‚ÄúFru-
galscore: Learning cheaper, lighter and faster evaluation metrics for
automatic text generation,‚Äù arXiv preprint arXiv:2110.08559 , 2021.
[56] G. Doddington, ‚ÄúAutomatic evaluation of machine translation quality
using n-gram co-occurrence statistics,‚Äù in Proceedings of the Second
International Conference on Human Language Technology Research ,
ser. HLT ‚Äô02. San Francisco, CA, USA: Morgan Kaufmann Publishers
Inc., 2002, p. 138‚Äì145.
[57] A. Graves, S. Fern√°ndez, F. J. Gomez, and J. Schmidhuber, ‚ÄúConnection-
ist temporal classification: Labelling unsegmented sequence data with
recurrent neural networks,‚Äù in Proc. ICML , Jun. 2006.
