# BÁO CÁO KỸ THUẬT QWEN

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu.

Đội Qwen, Tập đoàn Alibaba∗

TÓM TẮT

Các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa lĩnh vực trí tuệ nhân tạo, cho phép các tác vụ xử lý ngôn ngữ tự nhiên mà trước đây được cho là chỉ dành riêng cho con người. Trong công trình này, chúng tôi giới thiệu QWEN¹, phần đầu tiên của chuỗi mô hình ngôn ngữ lớn của chúng tôi. QWEN là một chuỗi mô hình ngôn ngữ toàn diện bao gồm các mô hình riêng biệt với số lượng tham số khác nhau. Nó bao gồm QWEN, các mô hình ngôn ngữ cơ sở được tiền huấn luyện, và QWEN-CHAT, các mô hình trò chuyện được tinh chỉnh bằng các kỹ thuật căn chỉnh con người. Các mô hình ngôn ngữ cơ sở liên tục chứng minh hiệu suất vượt trội trên nhiều tác vụ hạ nguồn, và các mô hình trò chuyện, đặc biệt là những mô hình được huấn luyện bằng Học tăng cường từ phản hồi của con người (RLHF), có tính cạnh tranh cao. Các mô hình trò chuyện sở hữu khả năng sử dụng công cụ và lập kế hoạch tiên tiến để tạo ra các ứng dụng tác nhân, thể hiện hiệu suất ấn tượng ngay cả khi so sánh với các mô hình lớn hơn trên các tác vụ phức tạp như sử dụng trình thông dịch mã. Hơn nữa, chúng tôi đã phát triển các mô hình chuyên biệt cho lập trình, CODE-QWEN và CODE-QWEN-CHAT, cũng như các mô hình tập trung vào toán học, MATH-QWEN-CHAT, được xây dựng dựa trên các mô hình ngôn ngữ cơ sở. Các mô hình này chứng minh hiệu suất được cải thiện đáng kể so với các mô hình mã nguồn mở và chỉ hơi kém so với các mô hình độc quyền.

∗Các tác giả được sắp xếp theo thứ tự bảng chữ cái theo họ. Liên hệ: ericzhou.zc@alibaba-inc.com.
¹QWEN là tên gọi tắt của Qianwen, có nghĩa là "hàng nghìn câu hỏi" trong tiếng Trung. Cách phát âm của "QWEN" có thể thay đổi tùy thuộc vào ngữ cảnh và người nói. Đây là một cách phát âm có thể: /kwɛn/.

Mục lục

1 Giới thiệu 3
2 Tiền huấn luyện 4
2.1 Dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Kiến trúc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.4 Huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.5 Mở rộng độ dài ngữ cảnh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.6 Kết quả thí nghiệm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3 Căn chỉnh 9
3.1 Tinh chỉnh có giám sát . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.1.1 Dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.2 Huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Học tăng cường từ phản hồi của con người . . . . . . . . . . . . . . . . . . . . . 10
3.2.1 Mô hình phần thưởng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2.2 Học tăng cường . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.3 Đánh giá tự động và con người của các mô hình đã căn chỉnh . . . . . . . . . . . . 11
3.4 Sử dụng công cụ, trình thông dịch mã và tác nhân . . . . . . . . . . . . . . . . . . 13
4 CODE-QWEN: Mô hình chuyên biệt cho lập trình 16
4.1 Tiền huấn luyện mã . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.2 Tinh chỉnh có giám sát mã . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.3 Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5 MATH-QWEN: Mô hình chuyên biệt cho lập luận toán học 17
5.1 Huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5.2 Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6 Nghiên cứu liên quan 20
6.1 Mô hình ngôn ngữ lớn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.2 Căn chỉnh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.3 Sử dụng công cụ và tác nhân . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.4 LLM cho lập trình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.5 LLM cho toán học . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
7 Kết luận 22

1 GIỚI THIỆU

Các mô hình ngôn ngữ lớn (LLM) (Radford et al., 2018; Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Anil et al., 2023; Thoppilan et al., 2022; Touvron et al., 2023a;b) đã cách mạng hóa lĩnh vực trí tuệ nhân tạo (AI) bằng cách cung cấp một nền tảng mạnh mẽ cho các tác vụ lập luận phức tạp và giải quyết vấn đề. Những mô hình này có khả năng nén kiến thức rộng lớn vào các mạng nơ-ron, khiến chúng trở thành các tác nhân cực kỳ đa năng. Với giao diện trò chuyện, LLM có thể thực hiện các tác vụ mà trước đây được cho là lĩnh vực riêng của con người, đặc biệt là những tác vụ liên quan đến sáng tạo và chuyên môn (OpenAI, 2022; Ouyang et al., 2022; Anil et al., 2023; Google, 2023; Anthropic, 2023a;b). Chúng có thể tham gia vào các cuộc trò chuyện bằng ngôn ngữ tự nhiên với con người, trả lời câu hỏi, cung cấp thông tin và thậm chí tạo ra nội dung sáng tạo như câu chuyện, thơ và âm nhạc. Điều này đã dẫn đến sự phát triển của một loạt các ứng dụng, từ chatbot và trợ lý ảo đến các công cụ dịch thuật ngôn ngữ và tóm tắt.

LLM không chỉ giới hạn ở các tác vụ ngôn ngữ. Chúng cũng có thể hoạt động như một tác nhân tổng quát (Reed et al., 2022; Bai et al., 2022a; Wang et al., 2023a; AutoGPT, 2023; Hong et al., 2023), hợp tác với các hệ thống, công cụ và mô hình bên ngoài để đạt được các mục tiêu do con người đặt ra. Ví dụ, LLM có thể hiểu các hướng dẫn đa phương thức (OpenAI, 2023; Bai et al., 2023; Liu et al., 2023a; Ye et al., 2023; Dai et al., 2023; Peng et al., 2023b), thực thi mã (Chen et al., 2021; Zheng et al., 2023; Li et al., 2023d), sử dụng công cụ (Schick et al., 2023; LangChain, Inc., 2023; AutoGPT, 2023), và nhiều hơn nữa. Điều này mở ra một thế giới hoàn toàn mới của các khả năng cho các ứng dụng AI, từ xe tự động và robot đến chăm sóc sức khỏe và tài chính. Khi những mô hình này tiếp tục phát triển và cải thiện, chúng ta có thể mong đợi thấy nhiều ứng dụng đổi mới và thú vị hơn trong những năm tới. Cho dù đó là giúp chúng ta giải quyết các vấn đề phức tạp, tạo ra các hình thức giải trí mới, hay chuyển đổi cách chúng ta sống và làm việc, LLM được định vị để đóng vai trò trung tâm trong việc định hình tương lai của AI.

Hình 1: Dòng mô hình của chuỗi Qwen. Chúng tôi đã tiền huấn luyện các mô hình ngôn ngữ, cụ thể là QWEN, trên các bộ dữ liệu khổng lồ chứa hàng nghìn tỷ token. Sau đó chúng tôi sử dụng SFT và RLHF để căn chỉnh QWEN với sở thích của con người và do đó chúng tôi có QWEN-CHAT và cụ thể là phiên bản cải tiến QWEN-CHAT-RLHF. Ngoài ra, chúng tôi cũng phát triển các mô hình chuyên biệt cho lập trình và toán học, như CODE-QWEN, CODE-QWEN-CHAT, và MATH-QWEN-CHAT dựa trên QWEN với các kỹ thuật tương tự. Lưu ý rằng chúng tôi trước đó đã phát hành LLM đa phương thức, QWEN-VL và QWEN-VL-CHAT (Bai et al., 2023), cũng dựa trên các mô hình cơ sở QWEN của chúng tôi.

Mặc dù có những khả năng ấn tượng, LLM thường bị chỉ trích vì thiếu khả năng tái tạo, điều khiển và khả năng tiếp cận đối với các nhà cung cấp dịch vụ. Trong công trình này, chúng tôi hân hạnh trình bày và phát hành phiên bản đầu tiên của chuỗi LLM của chúng tôi, QWEN. QWEN là một tên gọi tắt xuất phát từ cụm từ tiếng Trung Qianwen, có nghĩa là "hàng nghìn câu hỏi" và truyền tải khái niệm chấp nhận một loạt các câu hỏi. QWEN là một chuỗi mô hình ngôn ngữ toàn diện bao gồm các mô hình riêng biệt với số lượng tham số khác nhau. Chuỗi mô hình bao gồm các mô hình ngôn ngữ cơ sở được tiền huấn luyện, các mô hình trò chuyện được tinh chỉnh bằng các kỹ thuật căn chỉnh con người, tức là tinh chỉnh có giám sát (SFT), học tăng cường với phản hồi con người (RLHF), v.v., cũng như các mô hình chuyên biệt trong lập trình và toán học. Chi tiết được nêu dưới đây:

1. Các mô hình ngôn ngữ cơ sở, cụ thể là QWEN, đã trải qua quá trình huấn luyện rộng rãi sử dụng lên đến 3 nghìn tỷ token của các văn bản và mã đa dạng, bao gồm nhiều lĩnh vực. Những mô hình này đã liên tục chứng minh hiệu suất vượt trội trên nhiều tác vụ hạ nguồn, ngay cả khi so sánh với các đối tác lớn hơn đáng kể.

2. Các mô hình QWEN-CHAT đã được tinh chỉnh cẩn thận trên một bộ dữ liệu được tuyển chọn liên quan đến thực hiện tác vụ, trò chuyện, sử dụng công cụ, tác nhân, an toàn, v.v. Đánh giá điểm chuẩn chứng minh rằng các mô hình SFT có thể đạt được hiệu suất vượt trội. Hơn nữa, chúng tôi đã huấn luyện các mô hình phần thưởng để bắt chước sở thích của con người và áp dụng chúng trong RLHF cho các mô hình trò chuyện có thể tạo ra các phản hồi được con người ưa thích. Thông qua đánh giá con người của một thử nghiệm đầy thử thách, chúng tôi thấy rằng các mô hình QWEN-CHAT được huấn luyện bằng RLHF có tính cạnh tranh cao, vẫn còn kém GPT-4 trên điểm chuẩn của chúng tôi.

3. Ngoài ra, chúng tôi trình bày các mô hình chuyên biệt được gọi là CODE-QWEN, bao gồm CODE-QWEN-7B và CODE-QWEN-14B, cũng như các mô hình trò chuyện của chúng, CODE-QWEN-14B-CHAT và CODE-QWEN-7B-CHAT. Cụ thể, CODE-QWEN đã được tiền huấn luyện trên các bộ dữ liệu mã rộng lớn và được tinh chỉnh thêm để xử lý các cuộc trò chuyện liên quan đến tạo mã, gỡ lỗi và thông dịch. Kết quả của các thí nghiệm được tiến hành trên các bộ dữ liệu điểm chuẩn, như HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), và HumanEvalPack (Muennighoff et al., 2023), chứng minh mức độ thành thạo cao của CODE-QWEN trong hiểu và tạo mã.

4. Nghiên cứu này cũng giới thiệu MATH-QWEN-CHAT được thiết kế đặc biệt để giải quyết các bài toán. Kết quả của chúng tôi cho thấy rằng cả MATH-QWEN-7B-CHAT và MATH-QWEN-14B-CHAT đều vượt trội hơn các mô hình mã nguồn mở cùng kích thước với biên độ lớn và đang tiến gần đến GPT-3.5 trên các bộ dữ liệu điểm chuẩn liên quan đến toán học như GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al., 2021).

5. Bên cạnh đó, chúng tôi đã mở mã nguồn QWEN-VL và QWEN-VL-CHAT, có khả năng đa năng để hiểu các hướng dẫn thị giác và ngôn ngữ. Những mô hình này vượt trội hơn các mô hình ngôn ngữ-thị giác mã nguồn mở hiện tại trên các điểm chuẩn đánh giá khác nhau và hỗ trợ nhận dạng văn bản và nền tảng thị giác trong cả tiếng Trung và tiếng Anh. Hơn nữa, những mô hình này cho phép trò chuyện nhiều hình ảnh và kể chuyện. Chi tiết thêm có thể được tìm thấy trong Bai et al. (2023).

Bây giờ, chúng tôi chính thức mở mã nguồn các mô hình tiền huấn luyện cơ sở 14B-tham số và 7B-tham số QWEN và các mô hình trò chuyện đã căn chỉnh QWEN-CHAT². Việc phát hành này nhằm cung cấp các LLM toàn diện và mạnh mẽ hơn ở quy mô thân thiện với nhà phát triển hoặc ứng dụng.

Cấu trúc của báo cáo này như sau: Phần 2 mô tả phương pháp tiền huấn luyện và kết quả của QWEN. Phần 3 bao gồm phương pháp căn chỉnh và báo cáo kết quả của cả đánh giá tự động và đánh giá con người. Ngoài ra, phần này mô tả chi tiết về nỗ lực của chúng tôi trong việc xây dựng các mô hình trò chuyện có khả năng sử dụng công cụ, trình thông dịch mã và tác nhân. Trong Phần 4 và 5, chúng tôi đi sâu vào các mô hình chuyên biệt về lập trình và toán học và hiệu suất của chúng. Phần 6 cung cấp tổng quan về các nghiên cứu liên quan, và Phần 7 kết luận bài báo này và chỉ ra công việc tương lai của chúng tôi.

2 TIỀN HUẤN LUYỆN

Giai đoạn tiền huấn luyện liên quan đến việc học một lượng lớn dữ liệu để có được sự hiểu biết toàn diện về thế giới và các phức tạp khác nhau của nó. Điều này không chỉ bao gồm các khả năng ngôn ngữ cơ bản mà còn cả các kỹ năng tiên tiến như số học, lập trình và lập luận logic. Trong phần này, chúng tôi giới thiệu dữ liệu, thiết kế mô hình và quy mô, cũng như kết quả đánh giá toàn diện trên các bộ dữ liệu điểm chuẩn.

2.1 DỮ LIỆU

Kích thước dữ liệu đã được chứng minh là một yếu tố quan trọng trong việc phát triển một mô hình ngôn ngữ lớn mạnh mẽ, như được nêu bật trong nghiên cứu trước đây (Hoffmann et al., 2022; Touvron et al., 2023b). Để tạo ra một bộ dữ liệu tiền huấn luyện hiệu quả, điều cần thiết là đảm bảo rằng dữ liệu đa dạng và bao gồm nhiều loại, lĩnh vực và tác vụ khác nhau. Bộ dữ liệu của chúng tôi được thiết kế để đáp ứng những yêu cầu này và bao gồm các tài liệu web công cộng, bách khoa toàn thư, sách, mã, v.v. Ngoài ra, bộ dữ liệu của chúng tôi là đa ngôn ngữ, với một phần đáng kể dữ liệu bằng tiếng Anh và tiếng Trung.

Hình 2: Hiệu suất của GPT-4, GPT-3.5, SOTA 13B trước đây, cũng như QWEN-14B. Chúng tôi trình bày kết quả trên 12 bộ dữ liệu bao gồm nhiều lĩnh vực, bao gồm hiểu ngôn ngữ, kiến thức, lập luận, v.v. QWEN vượt trội đáng kể so với SOTA trước đây của các mô hình có kích thước tương tự, nhưng vẫn thua cả GPT-3.5 và GPT-4.

Để đảm bảo chất lượng dữ liệu tiền huấn luyện của chúng tôi, chúng tôi đã phát triển một quy trình tiền xử lý dữ liệu toàn diện. Đối với dữ liệu web công cộng, chúng tôi trích xuất văn bản từ HTML và sử dụng các công cụ nhận dạng ngôn ngữ để xác định ngôn ngữ. Để tăng tính đa dạng của dữ liệu, chúng tôi sử dụng các kỹ thuật khử trùng lặp, bao gồm khử trùng lặp khớp chính xác sau chuẩn hóa và khử trùng lặp mờ sử dụng thuật toán MinHash và LSH. Để lọc ra dữ liệu chất lượng thấp, chúng tôi sử dụng kết hợp các phương pháp dựa trên quy tắc và máy học. Cụ thể, chúng tôi sử dụng nhiều mô hình để chấm điểm nội dung, bao gồm các mô hình ngôn ngữ, mô hình chấm điểm chất lượng văn bản và mô hình để nhận dạng nội dung có thể xúc phạm hoặc không phù hợp. Chúng tôi cũng lấy mẫu thủ công các văn bản từ nhiều nguồn khác nhau và xem xét chúng để đảm bảo chất lượng. Để nâng cao thêm chất lượng dữ liệu, chúng tôi chọn lọc tăng mẫu dữ liệu từ các nguồn nhất định, để đảm bảo rằng các mô hình của chúng tôi được huấn luyện trên một loạt nội dung chất lượng cao đa dạng. Trong các nghiên cứu gần đây (Zeng et al., 2022; Aribandi et al., 2021; Raffel et al., 2020), đã được chứng minh rằng tiền huấn luyện các mô hình ngôn ngữ với các hướng dẫn đa tác vụ có thể nâng cao hiệu suất zero-shot và few-shot của chúng. Để nâng cao thêm hiệu suất của mô hình, chúng tôi đã tích hợp dữ liệu hướng dẫn chất lượng cao vào quá trình tiền huấn luyện. Để bảo vệ tính toàn vẹn của đánh giá điểm chuẩn, chúng tôi đã áp dụng một phương pháp tương tự như Brown et al. (2020) và loại bỏ cẩn thận bất kỳ mẫu hướng dẫn nào có sự chồng chéo 13-gram với bất kỳ dữ liệu nào có trong các bộ kiểm tra được sử dụng trong đánh giá của chúng tôi. Do số lượng lớn các tác vụ hạ nguồn, không khả thi để lặp lại quá trình lọc này cho tất cả các tác vụ. Thay vào đó, chúng tôi đã đảm bảo rằng dữ liệu hướng dẫn cho các tác vụ được báo cáo đã trải qua quy trình lọc của chúng tôi để đảm bảo độ chính xác và tin cậy. Cuối cùng, chúng tôi đã xây dựng một bộ dữ liệu lên đến 3 nghìn tỷ token.

2.2 TOKENIZATION

Thiết kế từ vựng có tác động đáng kể đến hiệu quả huấn luyện và hiệu suất tác vụ hạ nguồn. Trong nghiên cứu này, chúng tôi sử dụng mã hóa cặp byte (BPE) làm phương pháp tokenization, theo GPT-3.5 và GPT-4. Chúng tôi bắt đầu với tokenizer BPE nhanh mã nguồn mở, tiktoken (Jain, 2022), và chọn từ vựng cl100k_base làm điểm khởi đầu. Để nâng cao hiệu suất của mô hình trên các tác vụ hạ nguồn đa ngôn ngữ, đặc biệt là tiếng Trung, chúng tôi bổ sung từ vựng với các ký tự và từ tiếng Trung thường dùng, cũng như các ngôn ngữ khác. Ngoài ra, theo Touvron et al. (2023a;b), chúng tôi đã tách các số thành các chữ số đơn. Kích thước từ vựng cuối cùng là khoảng 152K.

Hiệu suất của tokenizer QWEN về mặt nén được mô tả trong Hình 3. Trong so sánh này, chúng tôi đã đánh giá QWEN với một số tokenizer khác, bao gồm XLM-R (Conneau et al., 2019), LLaMA (Touvron et al., 2023a), Baichuan (Inc., 2023a), và InternLM (InternLM Team, 2023). Phát hiện của chúng tôi cho thấy QWEN đạt được hiệu quả nén cao hơn so với các đối thủ cạnh tranh trong hầu hết các ngôn ngữ. Điều này có nghĩa là chi phí phục vụ có thể được giảm đáng kể vì một số lượng nhỏ hơn các token từ QWEN có thể truyền tải nhiều thông tin hơn so với các đối thủ cạnh tranh. Hơn nữa, chúng tôi đã tiến hành các thí nghiệm sơ bộ để đảm bảo rằng việc mở rộng kích thước từ vựng của QWEN không ảnh hưởng tiêu cực đến hiệu suất hạ nguồn của mô hình tiền huấn luyện. Mặc dù có sự gia tăng kích thước từ vựng, các thí nghiệm của chúng tôi đã cho thấy QWEN duy trì mức hiệu suất trong đánh giá hạ nguồn.

Hình 3: Tỷ lệ nén mã hóa của các mô hình khác nhau. Chúng tôi ngẫu nhiên chọn 1 triệu tài liệu corpus của mỗi ngôn ngữ để kiểm tra và so sánh tỷ lệ nén mã hóa của các mô hình khác nhau (với XLM-R (Conneau et al., 2019), hỗ trợ 100 ngôn ngữ, làm giá trị cơ sở 1, không hiển thị trong hình). Như có thể thấy, trong khi đảm bảo giải mã hiệu quả của tiếng Trung, tiếng Anh và mã, QWEN cũng đạt được tỷ lệ nén cao cho nhiều ngôn ngữ khác (như th, he, ar, ko, vi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr, v.v.), trang bị cho mô hình khả năng mở rộng mạnh mẽ cũng như hiệu quả huấn luyện và suy luận cao trong các ngôn ngữ này.

2.3 KIẾN TRÚC

QWEN được thiết kế sử dụng một phiên bản cải tiến của kiến trúc Transformer. Cụ thể, chúng tôi đã áp dụng phương pháp mã nguồn mở gần đây để huấn luyện các mô hình ngôn ngữ lớn, LLaMA (Touvron et al., 2023a), được coi rộng rãi là LLM mã nguồn mở hàng đầu. Các sửa đổi của chúng tôi đối với kiến trúc bao gồm:

Bảng 1: Kích thước mô hình, kiến trúc và siêu tham số tối ưu hóa.

| # Tham số | Kích thước ẩn | Đầu | Lớp | Tỷ lệ học | Kích thước batch | Token huấn luyện |
|-----------|---------------|-----|-----|-----------|------------------|------------------|
| 1.8B      | 2048          | 16  | 24  | 3.0×10⁻⁴ | 4M               | 2.2T             |
| 7B        | 4096          | 32  | 32  | 3.0×10⁻⁴ | 4M               | 2.4T             |
| 14B       | 5120          | 40  | 40  | 3.0×10⁻⁴ | 4M               | 3.0T             |

• Embedding và chiếu đầu ra. Dựa trên các phát hiện thí nghiệm sơ bộ, chúng tôi đã chọn phương pháp embedding không ràng buộc thay vì ràng buộc các trọng số của embedding đầu vào và chiếu đầu ra. Quyết định này được đưa ra để đạt được hiệu suất tốt hơn với giá của chi phí bộ nhớ.

• Embedding vị trí. Chúng tôi đã chọn RoPE (Rotary Positional Embedding) (Su et al., 2021) làm lựa chọn ưa thích để tích hợp thông tin vị trí vào mô hình. RoPE đã được áp dụng rộng rãi và đã chứng minh thành công trong các mô hình ngôn ngữ lớn đương đại, đáng chú ý là PaLM (Chowdhery et al., 2022; Anil et al., 2023) và LLaMA (Touvron et al., 2023a;b). Đặc biệt, chúng tôi đã chọn sử dụng độ chính xác FP32 cho ma trận tần số nghịch đảo, thay vì BF16 hoặc FP16, để ưu tiên hiệu suất mô hình và đạt được độ chính xác cao hơn.

• Bias. Đối với hầu hết các lớp, chúng tôi loại bỏ bias theo Chowdhery et al. (2022), nhưng chúng tôi thêm bias trong lớp QKV của attention để nâng cao khả năng ngoại suy của mô hình (Su, 2023b).

• Pre-Norm & RMSNorm. Trong các mô hình Transformer hiện đại, pre-normalization là phương pháp được sử dụng rộng rãi nhất, đã được chứng minh là cải thiện tính ổn định huấn luyện so với post-normalization. Nghiên cứu gần đây đã đề xuất các phương pháp thay thế cho tính ổn định huấn luyện tốt hơn, mà chúng tôi dự định khám phá trong các phiên bản tương lai của mô hình. Ngoài ra, chúng tôi đã thay thế kỹ thuật chuẩn hóa lớp truyền thống được mô tả trong (Ba et al., 2016) bằng RMSNorm (Jiang et al., 2023). Thay đổi này đã mang lại hiệu suất tương đương trong khi cũng cải thiện hiệu quả.

• Hàm kích hoạt. Chúng tôi đã chọn SwiGLU (Shazeer, 2020) làm hàm kích hoạt, một sự kết hợp của Swish (Ramachandran et al., 2017) và Gated Linear Unit (Dauphin et al., 2017). Các thí nghiệm ban đầu của chúng tôi đã cho thấy rằng các hàm kích hoạt dựa trên GLU nói chung vượt trội hơn các lựa chọn cơ sở khác, như GeLU (Hendrycks & Gimpel, 2016). Như là thực hành phổ biến trong nghiên cứu trước đây, chúng tôi đã giảm chiều của mạng feed-forward (FFN) từ 4 lần kích thước ẩn xuống 8/3 lần kích thước ẩn.

2.4 HUẤN LUYỆN

Để huấn luyện QWEN, chúng tôi theo phương pháp tiêu chuẩn của mô hình ngôn ngữ tự hồi quy, như được mô tả trong Radford et al. (2018). Điều này liên quan đến việc huấn luyện mô hình để dự đoán token tiếp theo dựa trên ngữ cảnh được cung cấp bởi các token trước đó. Chúng tôi huấn luyện các mô hình với độ dài ngữ cảnh là 2048. Để tạo batch dữ liệu, chúng tôi xáo trộn và hợp nhất các tài liệu, sau đó cắt chúng theo độ dài ngữ cảnh được chỉ định. Để cải thiện hiệu quả tính toán và giảm sử dụng bộ nhớ, chúng tôi sử dụng Flash Attention trong các mô-đun attention (Dao et al., 2022). Chúng tôi áp dụng bộ tối ưu hóa tiêu chuẩn AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) cho tối ưu hóa tiền huấn luyện. Chúng tôi thiết lập các siêu tham số β₁ = 0.9, β₂ = 0.95, và ε = 10⁻⁸. Chúng tôi sử dụng lịch trình tỷ lệ học cosine với tỷ lệ học đỉnh được chỉ định cho mỗi kích thước mô hình. Tỷ lệ học được giảm xuống tỷ lệ học tối thiểu là 10% của tỷ lệ học đỉnh. Tất cả các mô hình được huấn luyện với độ chính xác hỗn hợp BFloat16 để đảm bảo tính ổn định huấn luyện.

2.5 MỞ RỘNG ĐỘ DÀI NGỮ CẢNH

Các mô hình Transformer có một hạn chế đáng kể về độ dài ngữ cảnh cho cơ chế attention của chúng. Khi độ dài ngữ cảnh tăng lên, việc tính toán có độ phức tạp bậc hai dẫn đến sự gia tăng mạnh mẽ về cả chi phí tính toán và bộ nhớ. Trong công trình này, chúng tôi đã triển khai các kỹ thuật đơn giản không cần huấn luyện chỉ được áp dụng trong quá trình suy luận để mở rộng độ dài ngữ cảnh của mô hình. Một trong những kỹ thuật chính chúng tôi đã sử dụng là nội suy nhận biết NTK (bloc97, 2023). Khác với nội suy vị trí (PI) (Chen et al., 2023a) mà mở rộng mỗi chiều của RoPE một cách bằng nhau, nội suy nhận biết NTK điều chỉnh cơ sở của RoPE để ngăn ngừa mất thông tin tần số cao một cách không cần huấn luyện. Để cải thiện thêm hiệu suất, chúng tôi cũng đã triển khai một phần mở rộng đơn giản được gọi là nội suy nhận biết NTK động, sau này được thảo luận chính thức trong (Peng et al., 2023a). Nó thay đổi tỷ lệ động theo từng đoạn, tránh suy giảm hiệu suất nghiêm trọng. Các kỹ thuật này cho phép chúng tôi mở rộng hiệu quả độ dài ngữ cảnh của các mô hình Transformer mà không làm giảm hiệu quả tính toán hoặc độ chính xác của chúng.

QWEN ngoài ra còn tích hợp hai cơ chế attention: LogN-Scaling (Chiang & Cholak, 2022; Su, 2023a) và window attention (Beltagy et al., 2020). LogN-Scaling điều chỉnh lại tích vô hướng của query và value bằng một hệ số phụ thuộc vào tỷ lệ của độ dài ngữ cảnh so với độ dài huấn luyện, đảm bảo rằng entropy của giá trị attention vẫn ổn định khi độ dài ngữ cảnh tăng. Window attention hạn chế attention trong một cửa sổ ngữ cảnh giới hạn, ngăn mô hình chú ý đến các token quá xa.

Chúng tôi cũng quan sát thấy rằng khả năng mô hình hóa ngữ cảnh dài của mô hình thay đổi giữa các lớp, với các lớp thấp hơn nhạy cảm hơn trong việc mở rộng độ dài ngữ cảnh so với các lớp cao hơn. Để tận dụng quan sát này, chúng tôi gán các kích thước cửa sổ khác nhau cho mỗi lớp, sử dụng cửa sổ ngắn hơn cho các lớp thấp hơn và cửa sổ dài hơn cho các lớp cao hơn.

2.6 KẾT QUẢ THÍ NGHIỆM

Để đánh giá khả năng học zero-shot và few-shot của các mô hình, chúng tôi tiến hành đánh giá điểm chuẩn kỹ lưỡng sử dụng một loạt các bộ dữ liệu. Chúng tôi so sánh QWEN với các mô hình cơ sở mã nguồn mở mới nhất, bao gồm LLaMA (Touvron et al., 2023a), LLAMA 2 (Touvron et al., 2023b), MPT (Mosaic ML, 2023), Falcon (Almazrouei et al., 2023), Baichuan2 (Yang et al., 2023), ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM Team, 2023), XVERSE (Inc., 2023b), và StableBeluga2 (Stability AI, 2023). Đánh giá của chúng tôi bao gồm tổng cộng 7 điểm chuẩn phổ biến, đó là MMLU (5-shot) (Hendrycks et al., 2020), C-Eval (5-shot) (Huang et al., 2023), GSM8K (8-shot) (Cobbe et al., 2021), MATH (4-shot) (Hendrycks et al., 2021), HumanEval (0-shot) (Chen et al., 2021), MBPP (0-shot) (Austin et al., 2021), và BBH (Big Bench Hard) (3 shot) (Suzgun et al., 2022). Chúng tôi nhằm cung cấp một tóm tắt toàn diện về hiệu suất tổng thể của các mô hình trên các điểm chuẩn này.

Trong đánh giá này, chúng tôi tập trung vào các mô hình ngôn ngữ cơ sở không có căn chỉnh và thu thập điểm số tốt nhất của các baseline từ kết quả chính thức và OpenCompass (OpenCompass Team, 2023). Kết quả được trình bày trong Bảng 2.

Bảng 2: Hiệu suất tổng thể trên các điểm chuẩn được sử dụng rộng rãi so với các mô hình cơ sở mã nguồn mở. Mô hình QWEN lớn nhất với 14 tỷ tham số vượt trội hơn các mô hình SoTA 13B trước đây trên tất cả các bộ dữ liệu.

| Mô hình           | Tham số | MMLU  | C-Eval | GSM8K | MATH | HumanEval | MBPP | BBH  |
|-------------------|---------|-------|--------|-------|------|-----------|------|------|
|                   |         | 5-shot| 5-shot | 8-shot|4-shot| 0-shot    |3-shot|3-shot|
| MPT               | 7B      | 30.8  | 23.5   | 9.1   | 3.0  | 18.3      | 22.8 | 35.6 |
|                   | 30B     | 47.9  | -      | 15.2  | 3.1  | 25.0      | 32.8 | 38.0 |
| Falcon            | 7B      | 27.8  | -      | 6.8   | 2.3  | -         | 11.2 | 28.0 |
|                   | 40B     | 57.0  | -      | 19.6  | 5.5  | -         | 29.8 | 37.1 |
| ChatGLM2          | 6B      | 47.9  | 51.7   | 32.4  | 6.5  | -         | -    | 33.7 |
| InternLM          | 7B      | 51.0  | 53.4   | 31.2  | 6.3  | 10.4      | 14.0 | 37.0 |
|                   | 20B     | 62.1  | 58.8   | 52.6  | 7.9  | 25.6      | 35.6 | 52.5 |
| Baichuan2         | 7B      | 54.7  | 56.3   | 24.6  | 5.6  | 18.3      | 24.2 | 41.6 |
|                   | 13B     | 59.5  | 59.0   | 52.8  | 10.1 | 17.1      | 30.2 | 49.0 |
| LLaMA             | 7B      | 35.6  | 27.3   | 11.0  | 2.9  | 12.8      | 17.7 | 33.5 |
|                   | 13B     | 47.7  | 31.8   | 20.3  | 4.2  | 15.8      | 22.0 | 37.9 |
|                   | 33B     | 58.7  | 37.5   | 42.3  | 7.1  | 21.7      | 30.2 | 50.0 |
|                   | 65B     | 63.7  | 40.4   | 54.4  | 10.6 | 23.7      | 37.7 | 58.4 |
| LLAMA 2           | 7B      | 46.8  | 32.5   | 16.7  | 3.3  | 12.8      | 20.8 | 38.2 |
|                   | 13B     | 55.0  | 41.4   | 29.6  | 5.0  | 18.9      | 30.3 | 45.6 |
|                   | 34B     | 62.6  | -      | 42.2  | 6.2  | 22.6      | 33.0 | 44.1 |
|                   | 70B     | 69.8  | 50.1   | 63.3  | 13.5 | 29.9      | 45.0 | 64.9 |
| StableBeluga2     | 70B     | 68.6  | 51.4   | 69.6  | 14.6 | 28.0      | 11.4 | 69.3 |
| QWEN              | 1.8B    | 44.6  | 54.7   | 21.2  | 5.6  | 17.1      | 14.8 | 28.2 |
|                   | 7B      | 58.2  | 63.5   | 51.7  | 11.6 | 29.9      | 31.6 | 45.0 |
|                   | 14B     | 66.3  | 72.1   | 61.3  | 24.8 | 32.3      | 40.8 | 53.4 |

Kết quả thí nghiệm của chúng tôi chứng minh rằng ba mô hình QWEN thể hiện hiệu suất đặc biệt trên tất cả các tác vụ hạ nguồn. Đáng chú ý là ngay cả các mô hình lớn hơn, như LLaMA2-70B, cũng bị QWEN-14B vượt qua trong 3 tác vụ. QWEN-7B cũng hoạt động đáng ngưỡng mộ, vượt trội hơn LLaMA2-13B và đạt kết quả tương đương với Baichuan2-13B. Đáng chú ý, mặc dù có số lượng tham số tương đối nhỏ, QWEN-1.8B có khả năng có hiệu suất cạnh tranh trên một số tác vụ nhất định và thậm chí vượt trội hơn các mô hình lớn hơn trong một số trường hợp. Các phát hiện này nêu bật khả năng ấn tượng của các mô hình QWEN, đặc biệt là QWEN-14B, và cho thấy rằng các mô hình nhỏ hơn, như QWEN-1.8B, vẫn có thể đạt được hiệu suất mạnh trong một số ứng dụng nhất định.

Để đánh giá hiệu quả của việc mở rộng độ dài ngữ cảnh, Bảng 3 trình bày kết quả kiểm tra trên arXiv³ về mặt perplexity (PPL). Những kết quả này chứng minh rằng bằng cách kết hợp nội suy nhận biết NTK, LogN-Scaling và phân công cửa sổ theo lớp, chúng tôi có thể duy trì hiệu quả hiệu suất của các mô hình trong ngữ cảnh hơn 8192 token.

Bảng 3: Kết quả của QWEN về suy luận ngữ cảnh dài sử dụng các kỹ thuật khác nhau. Các phát hiện thí nghiệm của chúng tôi cho thấy rằng việc áp dụng các kỹ thuật quan trọng của chúng tôi cho phép mô hình liên tục đạt được perplexity thấp khi độ dài ngữ cảnh tăng. Điều này cho thấy rằng những kỹ thuật này đóng vai trò quan trọng trong việc nâng cao khả năng của mô hình để hiểu và tạo ra các văn bản dài.

| Mô hình      | Độ dài chuỗi                    |
|              | 1024  | 2048  | 4096  | 8192   | 16384   |
|--------------|-------|-------|-------|--------|---------|
| QWEN-7B      | 4.23  | 3.78  | 39.35 | 469.81 | 2645.09 |
| + dynamic ntk| 4.23  | 3.78  | 3.59  | 3.66   | 5.71    |
| + dynamic ntk + logn | 4.23 | 3.78 | 3.58 | 3.56 | 4.62 |
| + dynamic ntk + logn + window attn | 4.23 | 3.78 | 3.58 | 3.49 | 4.32 |
| QWEN-14B     | -     | 3.46  | 22.79 | 334.65 | 3168.35 |
| + dynamic ntk + logn + window attn | - | 3.46 | 3.29 | 3.18 | 3.42 |

3 CĂN CHỈNH

Các mô hình ngôn ngữ lớn tiền huấn luyện đã được phát hiện là không căn chỉnh với hành vi con người, khiến chúng không phù hợp để phục vụ như các trợ lý AI trong hầu hết các trường hợp. Nghiên cứu gần đây đã cho thấy rằng việc sử dụng các kỹ thuật căn chỉnh, như tinh chỉnh có giám sát (SFT) và học tăng cường từ phản hồi của con người (RLHF), có thể cải thiện đáng kể khả năng của các mô hình ngôn ngữ tham gia vào cuộc trò chuyện tự nhiên. Trong phần này, chúng tôi sẽ đi sâu vào chi tiết về cách các mô hình QWEN đã được huấn luyện bằng SFT và RLHF, và đánh giá hiệu suất của chúng trong bối cảnh hỗ trợ dựa trên trò chuyện.

3.1 TINH CHỈNH CÓ GIÁM SÁT

Để có được sự hiểu biết về hành vi con người, bước đầu tiên là thực hiện SFT, tinh chỉnh một LLM tiền huấn luyện trên dữ liệu kiểu trò chuyện, bao gồm cả câu hỏi và phản hồi. Trong các phần sau, chúng tôi sẽ đi sâu vào chi tiết về xây dựng dữ liệu và phương pháp huấn luyện.

³Bộ dữ liệu chứa các bài báo học thuật từ https://arxiv.org.

3.1.1 DỮ LIỆU

Để nâng cao khả năng của các bộ dữ liệu tinh chỉnh có giám sát, chúng tôi đã chú thích các cuộc trò chuyện theo nhiều phong cách. Trong khi các bộ dữ liệu thông thường (Wei et al., 2022a) chứa một lượng lớn dữ liệu được khuyến khích bằng câu hỏi, hướng dẫn và câu trả lời bằng ngôn ngữ tự nhiên, phương pháp của chúng tôi đi xa hơn bằng cách chú thích các cuộc trò chuyện kiểu con người. Thực hành này, được lấy cảm hứng từ Ouyang et al. (2022), nhằm cải thiện tính hữu ích của mô hình bằng cách tập trung vào tạo ngôn ngữ tự nhiên cho các tác vụ đa dạng. Để đảm bảo khả năng tổng quát hóa của mô hình đối với nhiều tình huống khác nhau, chúng tôi đặc biệt loại trừ dữ liệu được định dạng trong các mẫu prompt có thể hạn chế khả năng của nó. Hơn nữa, chúng tôi đã ưu tiên an toàn của mô hình ngôn ngữ bằng cách chú thích dữ liệu liên quan đến các mối quan tâm an toàn như bạo lực, thiên vị và khiêu dâm.

Ngoài chất lượng dữ liệu, chúng tôi đã quan sát thấy rằng phương pháp huấn luyện có thể tác động đáng kể đến hiệu suất cuối cùng của mô hình. Để đạt được điều này, chúng tôi đã sử dụng định dạng kiểu ChatML (OpenAI, 2022), đây là một siêu ngôn ngữ đa năng có khả năng mô tả cả siêu dữ liệu (như vai trò) và nội dung của một lượt. Định dạng này cho phép mô hình phân biệt hiệu quả giữa các loại thông tin khác nhau, bao gồm thiết lập hệ thống, đầu vào người dùng và đầu ra trợ lý, trong số những thứ khác. Bằng cách tận dụng phương pháp này, chúng tôi có thể nâng cao khả năng của mô hình để xử lý và phân tích chính xác dữ liệu trò chuyện phức tạp.

3.1.2 HUẤN LUYỆN

Phù hợp với tiền huấn luyện, chúng tôi cũng áp dụng dự đoán token tiếp theo làm tác vụ huấn luyện cho SFT. Chúng tôi áp dụng mặt nạ mất mát cho đầu vào hệ thống và người dùng. Chi tiết thêm được trình bày trong Phần A.1.1. Quá trình huấn luyện của mô hình sử dụng bộ tối ưu hóa AdamW, với các siêu tham số sau: β₁ được đặt thành 0.9, β₂ được đặt thành 0.95, và ε được đặt thành 10⁻⁸. Độ dài chuỗi được giới hạn ở 2048, và kích thước batch là 128. Mô hình trải qua tổng cộng 4000 bước, với tỷ lệ học được tăng dần trong 1430 bước đầu tiên, đạt đỉnh ở 2×10⁻⁶. Để ngăn ngừa overfitting, weight decay được áp dụng với giá trị 0.1, dropout được đặt thành 0.1, và gradient clipping được thực thi với giới hạn 1.0.

3.2 HỌC TĂNG CƯỜNG TỪ PHẢN HỒI CỦA CON NGƯỜI

Mặc dù SFT đã được chứng minh là hiệu quả, chúng tôi thừa nhận rằng khả năng tổng quát hóa và sáng tạo của nó có thể bị hạn chế, và nó dễ bị overfitting. Để giải quyết vấn đề này, chúng tôi đã triển khai Học tăng cường từ phản hồi của con người (RLHF) để căn chỉnh thêm các mô hình SFT với sở thích của con người, theo các phương pháp của Ouyang et al. (2022); Christiano et al. (2017). Quá trình này liên quan đến việc huấn luyện một mô hình phần thưởng và sử dụng Proximal Policy Optimization (PPO) (Schulman et al., 2017) để tiến hành huấn luyện chính sách.

3.2.1 MÔ HÌNH PHẦN THƯỞNG

Để tạo ra một mô hình phần thưởng thành công, giống như xây dựng một mô hình ngôn ngữ lớn (LLM), điều quan trọng là phải trải qua tiền huấn luyện trước và sau đó tinh chỉnh. Quá trình tiền huấn luyện này, còn được gọi là tiền huấn luyện mô hình sở thích (PMP) (Bai et al., 2022b), đòi hỏi một bộ dữ liệu lớn về dữ liệu so sánh. Bộ dữ liệu này bao gồm các cặp mẫu, mỗi cặp chứa hai phản hồi khác biệt cho một câu hỏi duy nhất và sở thích tương ứng của chúng. Tương tự, tinh chỉnh cũng được tiến hành trên loại dữ liệu so sánh này, nhưng với chất lượng cao hơn do có sự chú thích chất lượng.

Trong giai đoạn tinh chỉnh, chúng tôi thu thập nhiều prompt khác nhau và điều chỉnh mô hình phần thưởng dựa trên phản hồi của con người cho các phản hồi từ các mô hình QWEN. Để đảm bảo tính đa dạng và phức tạp của prompt người dùng được tính đến đúng cách, chúng tôi đã tạo ra một hệ thống phân loại với khoảng 6600 thẻ chi tiết và triển khai một thuật toán lấy mẫu cân bằng xem xét cả tính đa dạng và phức tạp khi chọn prompt để chú thích bởi mô hình phần thưởng (Lu et al., 2023). Để tạo ra một loạt phản hồi đa dạng, chúng tôi đã sử dụng các mô hình QWEN có kích thước khác nhau và các chiến lược lấy mẫu, vì các phản hồi đa dạng có thể giúp giảm khó khăn chú thích và nâng cao hiệu suất của mô hình phần thưởng. Những phản hồi này sau đó được đánh giá bởi các chú thích viên theo một hướng dẫn chú thích tiêu chuẩn, và các cặp so sánh được hình thành dựa trên điểm số của chúng.

Trong việc tạo ra mô hình phần thưởng, chúng tôi sử dụng mô hình ngôn ngữ tiền huấn luyện cùng kích thước QWEN để khởi tạo quá trình. Điều quan trọng cần đề cập là chúng tôi đã tích hợp một lớp pooling vào mô hình QWEN gốc để trích xuất phần thưởng cho một câu dựa trên một token kết thúc cụ thể. Tỷ lệ học cho quá trình này đã được đặt thành giá trị không đổi 3×10⁻⁶, và kích thước batch là 64. Ngoài ra, độ dài chuỗi được đặt thành 2048, và quá trình huấn luyện kéo dài trong một epoch duy nhất.

Chúng tôi áp dụng độ chính xác trên bộ dữ liệu kiểm tra làm một chỉ số đánh giá quan trọng nhưng không độc quyền cho mô hình phần thưởng. Trong Bảng 4, chúng tôi báo cáo độ chính xác cặp kiểm tra của các mô hình PMP và phần thưởng trên các bộ dữ liệu điểm chuẩn sở thích con người đa dạng (Bai et al., 2022b; Stiennon et al., 2020; Ethayarajh et al., 2022; Lightman et al., 2023). Cụ thể, QWEN Helpful-base và QWEN Helpful-online là các bộ dữ liệu độc quyền của chúng tôi. Các phản hồi trong QWEN Helpful-base được tạo ra từ QWEN không có RLHF, trong khi QWEN Helpful-online bao gồm các phản hồi từ QWEN có RLHF. Kết quả cho thấy mô hình PMP chứng minh khả năng tổng quát hóa cao trên dữ liệu ngoài phân phối, và mô hình phần thưởng chứng minh sự cải thiện đáng kể trên các bộ dữ liệu phần thưởng QWEN của chúng tôi.

Bảng 4: Độ chính xác kiểm tra của tiền huấn luyện mô hình sở thích QWEN (PMP) và mô hình phần thưởng (RM) trên các bộ dữ liệu điểm chuẩn sở thích con người đa dạng.

| Bộ dữ liệu      | QWEN     | QWEN     | Anthropic | Anthropic | OpenAI  | Stanford | OpenAI    |
|                 | Helpful- | Helpful- | Helpful-  | Helpful-  | Summ.   | SHP      | PRM800K   |
|                 | base     | online   | base      | online    |         |          |           |
|-----------------|----------|----------|-----------|-----------|---------|----------|-----------|
| PMP             | 62.68    | 61.62    | 76.52     | 65.43     | 69.60   | 60.05    | 70.59     |
| RM              | 74.78    | 69.71    | 73.98     | 64.57     | 69.99   | 60.10    | 70.52     |

3.2.2 HỌC TĂNG CƯỜNG

Quá trình Proximal Policy Optimization (PPO) của chúng tôi liên quan đến bốn mô hình: mô hình chính sách, mô hình giá trị, mô hình tham chiếu và mô hình phần thưởng. Trước khi bắt đầu quy trình PPO, chúng tôi tạm dừng cập nhật của mô hình chính sách và tập trung hoàn toàn vào việc cập nhật mô hình giá trị trong 50 bước. Phương pháp này đảm bảo rằng mô hình giá trị có thể thích ứng với các mô hình phần thưởng khác nhau một cách hiệu quả.

Trong quá trình vận hành PPO, chúng tôi sử dụng chiến lược lấy mẫu hai phản hồi cho mỗi câu hỏi đồng thời. Chiến lược này đã được chứng minh là hiệu quả hơn dựa trên các đánh giá điểm chuẩn nội bộ của chúng tôi. Chúng tôi đặt hệ số phân kỳ KL thành 0.04 và chuẩn hóa phần thưởng dựa trên trung bình chạy.

Các mô hình chính sách và giá trị có tỷ lệ học lần lượt là 1×10⁻⁶ và 5×10⁻⁶. Để tăng cường tính ổn định huấn luyện, chúng tôi sử dụng value loss clipping với giá trị clip là 0.15. Đối với suy luận, top-p của chính sách được đặt thành 0.9. Phát hiện của chúng tôi chỉ ra rằng mặc dù entropy thấp hơn một chút so với khi top-p được đặt thành 1.0, có sự gia tăng nhanh hơn về phần thưởng, cuối cùng dẫn đến phần thưởng đánh giá cao hơn một cách nhất quán trong các điều kiện tương tự.

Ngoài ra, chúng tôi đã triển khai một gradient tiền huấn luyện để giảm thiểu thuế căn chỉnh. Các phát hiện thực nghiệm chỉ ra rằng, với mô hình phần thưởng cụ thể này, hình phạt KL đủ mạnh mẽ để chống lại thuế căn chỉnh trong các điểm chuẩn không nghiêm ngặt về mã hoặc toán học, như những điểm kiểm tra kiến thức thông thường và hiểu đọc. Điều cấp thiết là sử dụng một khối lượng đáng kể dữ liệu tiền huấn luyện so với dữ liệu PPO để đảm bảo hiệu quả của gradient tiền huấn luyện. Ngoài ra, nghiên cứu thực nghiệm của chúng tôi cho thấy rằng một giá trị quá lớn cho hệ số này có thể cản trở đáng kể việc căn chỉnh với mô hình phần thưởng, cuối cùng làm tổn hại đến việc căn chỉnh cuối cùng, trong khi một giá trị quá nhỏ sẽ chỉ có tác động nhỏ đến việc giảm thuế căn chỉnh.

3.3 ĐÁNH GIÁ TỰ ĐỘNG VÀ CON NGƯỜI CỦA CÁC MÔ HÌNH ĐÃ CĂN CHỈNH

Để thể hiện hiệu quả của các mô hình đã căn chỉnh, chúng tôi tiến hành so sánh với các mô hình đã căn chỉnh khác trên các điểm chuẩn được thiết lập tốt, bao gồm MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), và BBH (Suzgun et al., 2022). Bên cạnh cài đặt few-shot được sử dụng rộng rãi, chúng tôi kiểm tra các mô hình đã căn chỉnh trong cài đặt zero-shot để chứng minh mức độ mô hình tuân theo hướng dẫn. Prompt trong cài đặt zero-shot bao gồm một hướng dẫn và một câu hỏi mà không có bất kỳ ví dụ trước đó nào trong ngữ cảnh. Kết quả của các baseline được thu thập từ báo cáo chính thức và OpenCompass (OpenCompass Team, 2023).

Kết quả trong Bảng 5 chứng minh hiệu quả của các mô hình đã căn chỉnh trong việc hiểu hướng dẫn của con người và tạo ra các phản hồi phù hợp. QWEN-14B-Chat vượt trội hơn tất cả các mô hình khác ngoại trừ ChatGPT (OpenAI, 2022) và LLAMA 2-CHAT-70B (Touvron et al., 2023b) trong tất cả các bộ dữ liệu, bao gồm MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), và BBH (Suzgun et al., 2022). Đặc biệt, hiệu suất của QWEN trong HumanEval, đo lường chất lượng của mã được tạo ra, cao hơn đáng kể so với các mô hình mã nguồn mở khác.

Bảng 5: Hiệu suất của các mô hình đã căn chỉnh trên các điểm chuẩn được sử dụng rộng rãi. Chúng tôi báo cáo cả hiệu suất zero-shot và few-shot của các mô hình.

| Mô hình              | Tham số | MMLU           | C-Eval         | GSM8K          | HumanEval | BBH            |
|                      |         | 0-shot/5-shot  | 0-shot/5-shot  | 0-shot/8-shot  | 0-shot    | 0-shot/3-shot  |
|----------------------|---------|----------------|----------------|----------------|-----------|----------------|
| **Mô hình độc quyền**|         |                |                |                |           |                |
| GPT-3.5              | -       | -/69.1         | -/52.5         | -/78.2         | 73.2      | -/70.1         |
| GPT-4                | -       | -/83.0         | -/69.9         | -/91.4         | 86.6      | -/86.7         |
| **Mô hình mã nguồn mở**|       |                |                |                |           |                |
| ChatGLM2             | 6B      | 45.5/46.0      | 50.1/52.6      | -/28.8         | 11.0      | -/32.7         |
| InternLM-Chat        | 7B      | -/51.1         | -/53.6         | -/33.0         | 14.6      | -/32.5         |
| Baichuan2-Chat       | 7B      | -/52.9         | -/55.6         | -/32.8         | 13.4      | -/35.8         |
|                      | 13B     | -/57.3         | -/56.7         | -/55.3         | 17.7      | -/49.9         |
| LLAMA 2-CHAT         | 7B      | -/46.2         | -/31.9         | -/26.3         | 12.2      | -/35.6         |
|                      | 13B     | -/54.6         | -/36.2         | -/37.1         | 18.9      | -/40.1         |
|                      | 70B     | -/63.8         | -/44.3         | -/59.3         | 32.3      | -/60.8         |
| QWEN-CHAT            | 1.8B    | 42.4/43.9      | 50.7/50.3      | 27.8/19.5      | 14.6      | 27.1/25.0      |
|                      | 7B      | 55.8/57.0      | 59.7/59.3      | 50.3/54.1      | 37.2      | 39.6/46.7      |
|                      | 14B     | 64.6/66.5      | 69.8/71.7      | 60.1/59.3      | 43.9      | 46.9/58.7      |

Hơn nữa, hiệu suất của QWEN liên tục tốt hơn so với các mô hình mã nguồn mở có kích thước tương tự, như LLaMA2 (Touvron et al., 2023b), ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM Team, 2023), và Baichuan2 (Yang et al., 2023). Điều này cho thấy rằng phương pháp căn chỉnh của chúng tôi, bao gồm việc tinh chỉnh mô hình trên một bộ dữ liệu lớn các cuộc trò chuyện của con người, đã hiệu quả trong việc cải thiện khả năng của mô hình để hiểu và tạo ra ngôn ngữ giống con người.

Mặc dù vậy, chúng tôi có những dè dặt về khả năng của đánh giá điểm chuẩn truyền thống để đo lường chính xác hiệu suất và tiềm năng của các mô hình trò chuyện được huấn luyện bằng các kỹ thuật căn chỉnh trong bối cảnh ngày nay. Các kết quả đã đề cập trước đây cung cấp một số bằng chứng về vị thế cạnh tranh của chúng tôi, nhưng chúng tôi tin rằng điều quan trọng là phát triển các phương pháp đánh giá mới được thiết kế riêng cho các mô hình đã căn chỉnh. Chúng tôi tin rằng đánh giá con người là quan trọng, đó là lý do tại sao chúng tôi đã tạo ra một bộ dữ liệu được tuyển chọn cẩn thận cho mục đích này. Quá trình của chúng tôi liên quan đến việc thu thập 300 hướng dẫn bằng tiếng Trung bao gồm một loạt các chủ đề, bao gồm kiến thức, hiểu ngôn ngữ, viết sáng tạo, lập trình và toán học. Để đánh giá hiệu suất của các mô hình khác nhau, chúng tôi đã chọn phiên bản SFT của QWEN-CHAT-7B và các phiên bản SFT và RLHF của QWEN-CHAT-14B, và thêm hai baseline mạnh, GPT-3.5 và GPT-4⁴, để so sánh. Đối với mỗi hướng dẫn, chúng tôi yêu cầu ba chú thích viên xếp hạng các phản hồi mô hình theo điểm số tổng thể về tính hữu ích, thông tin, tính hợp lệ và các yếu tố liên quan khác. Bộ dữ liệu và phương pháp đánh giá của chúng tôi cung cấp một đánh giá toàn diện và nghiêm ngặt về khả năng của các mô hình ngôn ngữ khác nhau trong nhiều lĩnh vực.

Hình 4 minh họa tỷ lệ thắng của các mô hình khác nhau. Đối với mỗi mô hình, chúng tôi báo cáo tỷ lệ thắng, hòa và thua so với GPT-3.5, với các phân đoạn của mỗi thanh từ dưới lên trên đại diện cho các thống kê này. Kết quả thí nghiệm rõ ràng chứng minh rằng mô hình RLHF vượt trội hơn các mô hình SFT với biên độ đáng kể, cho thấy rằng RLHF có thể khuyến khích mô hình tạo ra các phản hồi được con người ưa thích hơn. Về hiệu suất tổng thể, chúng tôi thấy rằng mô hình RLHF vượt trội đáng kể so với các mô hình SFT, thua GPT-4. Điều này cho thấy hiệu quả của RLHF để căn chỉnh với sở thích con người. Để cung cấp sự hiểu biết toàn diện hơn về hiệu suất của các mô hình, chúng tôi bao gồm một nghiên cứu trường hợp với các ví dụ từ các mô hình khác nhau trong Phụ lục A.2.2. Tuy nhiên, vẫn khó để nắm bắt chính xác khoảng cách giữa các mô hình của chúng tôi và các mô hình độc quyền. Do đó, cần có một đánh giá rộng lớn và nghiêm ngặt hơn cho các mô hình trò chuyện.

Hình 4: Kết quả đánh giá con người cho các mô hình trò chuyện. Chúng tôi so sánh Qwen-7B (SFT), Qwen-14B (SFT), Qwen-14B (RLHF), cũng như GPT-4 với GPT-3.5. Mỗi phân đoạn thanh đại diện cho tỷ lệ thắng, hòa và thua, từ dưới lên trên. Trung bình, mô hình RLHF vượt trội hơn mô hình SFT. Bộ dữ liệu bao gồm 300 hướng dẫn tiếng Trung.

3.4 SỬ DỤNG CÔNG CỤ, TRÌNH THÔNG DỊCH MÃ VÀ TÁC NHÂN

Bảng 6: Hiệu suất của QWEN trên điểm chuẩn tiếng Trung nội bộ đánh giá khả năng sử dụng các công cụ chưa thấy thông qua ReAct prompting.

| Mô hình     | Tham số | Lựa chọn công cụ (Acc. ↑) | Đầu vào công cụ (Rouge-L ↑) | Lỗi dương tính giả (%) ↓ |
|-------------|---------|----------------------------|------------------------------|---------------------------|
| GPT-4       | -       | 95                         | 90                           | 15.0                      |
| GPT-3.5     | -       | 85                         | 88                           | 75.0                      |
| QWEN-CHAT   | 1.8B    | 92                         | 89                           | 19.3                      |
|             | 7B      | 98                         | 91                           | 7.3                       |
|             | 14B     | 98                         | 93                           | 2.4                       |

Các mô hình QWEN, được thiết kế để đa năng, có khả năng đáng kể trong việc hỗ trợ (bán) tự động hóa các tác vụ hàng ngày bằng cách tận dụng kỹ năng sử dụng công cụ và lập kế hoạch của chúng. Do đó, chúng có thể phục vụ như tác nhân hoặc copilot để giúp hợp lý hóa các tác vụ khác nhau. Chúng tôi khám phá khả năng thành thạo của QWEN trong các lĩnh vực sau:

• Sử dụng các công cụ chưa thấy thông qua ReAct prompting (Yao et al., 2022) (xem Bảng 6).
• Sử dụng trình thông dịch mã Python để tăng cường lập luận toán học, phân tích dữ liệu và nhiều hơn nữa (xem Bảng 7 và Bảng 8).
• Hoạt động như một tác nhân có quyền truy cập vào bộ sưu tập mô hình đa phương thức rộng lớn của Hugging Face trong khi tương tác với con người (xem Bảng 9).

⁴Để có được kết quả từ các mô hình, chúng tôi sử dụng API OpenAI của GPT-3.5-turbo-0613 và GPT-4-0613.

Bảng 7: Tỷ lệ mã được tạo bởi QWEN có thể thực thi trên điểm chuẩn đánh giá nội bộ cho Code Interpreter. Điểm chuẩn này kiểm tra khả năng lập trình của QWEN trong giải quyết bài toán, trực quan hóa dữ liệu và mục đích chung. CODE LLAMA hoạt động kém trên các tác vụ trực quan hóa vì nó tạo ảo giác các cột không tồn tại chỉ dựa trên tên tệp CSV (xem Hình 5).

| Mô hình               | Tham số | Thể loại            |                    |              |          |
|                       |         | Toán (%) | Trực quan (%) | Chung (%) | Tất cả (%) |
|-----------------------|---------|----------|---------------|-----------|------------|
| GPT-4                 | -       | 91.9     | 85.9          | 82.8      | 86.8       |
| GPT-3.5               | -       | 89.2     | 65.0          | 74.1      | 72.9       |
| LLAMA 2-CHAT          | 7B      | 41.9     | 33.1          | 24.1      | 33.6       |
|                       | 13B     | 50.0     | 40.5          | 48.3      | 44.4       |
| CODE LLAMA-INSTRUCT   | 7B      | 85.1     | 54.0          | 70.7      | 65.1       |
|                       | 13B     | 93.2     | 55.8          | 74.1      | 68.8       |
| InternLM-Chat         | 7B v1.1 | 78.4     | 44.2          | 62.1      | 56.3       |
|                       | 20B     | 70.3     | 44.2          | 65.5      | 54.9       |
| QWEN-CHAT             | 1.8B    | 33.8     | 30.1          | 8.6       | 26.8       |
|                       | 7B      | 82.4     | 64.4          | 67.2      | 70.2       |
|                       | 14B     | 89.2     | 84.1          | 65.5      | 81.7       |

Bảng 8: Tính đúng đắn của phản hồi cuối cùng trên điểm chuẩn đánh giá nội bộ cho Code Interpreter. Các tác vụ Trực quan-Khó liên quan đến việc lập kế hoạch nhiều bước, trong khi các tác vụ Trực quan-Dễ thì không. Trực quan-Tất cả đo lường cả hai loại tác vụ. CODE LLAMA xuất sắc trong việc thực hiện các tác vụ Trực quan-Dễ nhưng có xu hướng hoạt động kém trong các tác vụ Trực quan-Khó, do xu hướng tạo ảo giác các cột không tồn tại dựa trên tên của tệp CSV (xem Hình 5).

| Mô hình               | Tham số | Thể loại                    |                      |                     |                      |
|                       |         | Toán (%) | Trực quan-Khó (%) | Trực quan-Dễ (%) | Trực quan-Tất cả (%) |
|-----------------------|---------|----------|-------------------|------------------|----------------------|
| GPT-4                 | -       | 82.8     | 66.7              | 60.8             | 63.8                 |
| GPT-3.5               | -       | 47.3     | 33.3              | 55.7             | 44.2                 |
| LLAMA 2-CHAT          | 7B      | 3.9      | 14.3              | 39.2             | 26.4                 |
|                       | 13B     | 8.3      | 8.3               | 40.5             | 23.9                 |
| CODE LLAMA-INSTRUCT   | 7B      | 14.3     | 26.2              | 60.8             | 42.9                 |
|                       | 13B     | 28.2     | 27.4              | 62.0             | 44.2                 |
| InternLM-Chat         | 7B v1.1 | 28.5     | 4.8               | 40.5             | 22.1                 |
|                       | 20B     | 34.6     | 21.4              | 45.6             | 33.1                 |
| QWEN-CHAT             | 1.8B    | 14.7     | 3.6               | 20.3             | 11.7                 |
|                       | 7B      | 41.9     | 40.5              | 54.4             | 47.2                 |
|                       | 14B     | 58.4     | 53.6              | 59.5             | 56.4                 |

Bảng 9: Kết quả của QWEN-Chat trên điểm chuẩn Hugging Face Agent.

| Tác vụ     | Mô hình         | Tham số | Chỉ số                      |                  |                    |
|            |                 |         | Lựa chọn công cụ ↑ | Công cụ sử dụng ↑ | Tính đúng mã ↑ |
|------------|-----------------|---------|-------------------|------------------|-------------------|
| Chế độ chạy| GPT-4           | -       | 100               | 100              | 97.4              |
|            | GPT-3.5         | -       | 95.4              | 96.3             | 87.0              |
|            | Starcoder-Base  | 15B     | 86.1              | 87.0             | 68.9              |
|            | Starcoder       | 15B     | 87.0              | 88.0             | 68.9              |
|            | QWEN-CHAT       | 1.8B    | 85.2              | 84.3             | 61.1              |
|            |                 | 7B      | 87.0              | 87.0             | 71.5              |
|            |                 | 14B     | 93.5              | 94.4             | 87.0              |
| Chế độ trò chuyện| GPT-4     | -       | 97.9              | 97.9             | 98.5              |
|            | GPT-3.5         | -       | 97.3              | 96.8             | 89.6              |
|            | Starcoder-Base  | 15B     | 97.9              | 97.9             | 91.1              |
|            | Starcoder       | 15B     | 97.9              | 97.9             | 89.6              |
|            | QWEN-CHAT       | 1.8B    | 93.6              | 93.6             | 73.2              |
|            |                 | 7B      | 94.7              | 94.7             | 85.1              |
|            |                 | 14B     | 97.9              | 97.9             | 95.5              |

Để nâng cao khả năng của QWEN như một tác nhân hoặc copilot, chúng tôi sử dụng chiến lược self-instruct (Wang et al., 2023c) cho SFT. Cụ thể, chúng tôi sử dụng khả năng học trong ngữ cảnh của QWEN để tự hướng dẫn. Bằng cách cung cấp một số ví dụ, chúng tôi có thể khuyến khích QWEN tạo ra nhiều truy vấn liên quan hơn và tạo ra các đầu ra theo một định dạng cụ thể, như ReAct (Yao et al., 2022). Sau đó chúng tôi áp dụng các quy tắc và có sự tham gia của các chú thích viên con người để lọc ra bất kỳ mẫu nhiễu nào. Sau đó, các mẫu được tích hợp vào dữ liệu huấn luyện của QWEN, dẫn đến một phiên bản cập nhật của QWEN đáng tin cậy hơn để tự hướng dẫn. Chúng tôi lặp lại quá trình này nhiều lần cho đến khi chúng tôi thu thập được một số lượng đủ các mẫu có cả chất lượng đặc biệt và một loạt đa dạng. Kết quả, bộ sưu tập cuối cùng của chúng tôi bao gồm khoảng 2000 mẫu chất lượng cao.

Trong quá trình tinh chỉnh, chúng tôi trộn những mẫu chất lượng cao này với tất cả các mẫu SFT mục đích chung khác, thay vì giới thiệu một giai đoạn huấn luyện bổ sung. Bằng cách làm như vậy, chúng tôi có thể giữ lại các khả năng mục đích chung thiết yếu cũng liên quan đến việc xây dựng các ứng dụng tác nhân.

**Sử dụng công cụ thông qua ReAct Prompting** Chúng tôi đã tạo ra và công khai một điểm chuẩn để đánh giá khả năng của QWEN gọi plugin, công cụ, hàm hoặc API bằng ReAct Prompting (xem Qwen Team, Alibaba Group, 2023b). Để đảm bảo đánh giá công bằng, chúng tôi đã loại trừ bất kỳ plugin nào được bao gồm trong tập huấn luyện của QWEN khỏi tập đánh giá. Điểm chuẩn đánh giá độ chính xác của mô hình trong việc chọn plugin đúng từ một nhóm lên đến năm ứng cử viên, cũng như tính hợp lý của các tham số được truyền vào plugin và tần suất của các dương tính giả. Trong đánh giá này, một dương tính giả xảy ra khi mô hình gọi sai plugin để phản hồi một truy vấn, mặc dù không được yêu cầu làm như vậy.

Kết quả được trình bày trong Bảng 6 chứng minh rằng QWEN liên tục đạt được độ chính xác cao hơn trong việc xác định tính liên quan của một truy vấn đối với các công cụ có sẵn khi kích thước mô hình tăng. Tuy nhiên, bảng cũng làm nổi bật rằng vượt quá một điểm nhất định, có ít cải thiện hiệu suất khi nói đến việc chọn công cụ phù hợp và cung cấp các đối số liên quan. Điều này cho thấy rằng điểm chuẩn sơ bộ hiện tại có thể tương đối dễ và có thể yêu cầu cải tiến thêm trong các phiên bản tương lai. Đáng chú ý rằng GPT-3.5 nổi bật như một ngoại lệ, hiển thị hiệu suất dưới mức tối ưu trên điểm chuẩn cụ thể này. Điều này có thể là do thực tế rằng điểm chuẩn chủ yếu tập trung vào tiếng Trung, có thể không phù hợp tốt với khả năng của GPT-3.5. Ngoài ra, chúng tôi quan sát thấy rằng GPT-3.5 có xu hướng cố gắng sử dụng ít nhất một công cụ, ngay cả khi truy vấn không thể được giải quyết hiệu quả bằng các công cụ được cung cấp.

**Sử dụng Code Interpreter cho lập luận toán học và phân tích dữ liệu** Trình thông dịch mã Python được coi rộng rãi là một công cụ mạnh mẽ để tăng cường khả năng của một tác nhân LLM. Đáng để điều tra liệu QWEN có thể khai thác toàn bộ tiềm năng của trình thông dịch này để nâng cao hiệu suất trong các lĩnh vực đa dạng, như lập luận toán học và phân tích dữ liệu. Để tạo điều kiện cho cuộc khám phá này, chúng tôi đã phát triển và công khai một điểm chuẩn được thiết kế riêng cho mục đích này (xem Qwen Team, Alibaba Group, 2023a).

Điểm chuẩn bao gồm ba danh mục tác vụ chính: giải quyết bài toán, trực quan hóa dữ liệu và các tác vụ mục đích chung khác như xử lý tệp hậu kỳ và thu thập dữ liệu web. Trong các tác vụ trực quan hóa, chúng tôi phân biệt giữa hai mức độ khó khăn. Mức độ dễ hơn có thể đạt được bằng cách đơn giản viết và thực thi một đoạn mã duy nhất mà không cần kỹ năng lập kế hoạch tiên tiến. Tuy nhiên, mức độ thách thức hơn đòi hỏi lập kế hoạch chiến lược và thực thi nhiều đoạn mã theo cách tuần tự. Điều này là do mã tiếp theo phải được viết dựa trên đầu ra của mã trước đó. Ví dụ, một tác nhân có thể cần kiểm tra cấu trúc của tệp CSV bằng một đoạn mã trước khi tiến hành viết và thực thi mã bổ sung để tạo biểu đồ.

Về các chỉ số đánh giá, chúng tôi xem xét cả khả năng thực thi và tính đúng đắn của mã được tạo ra. Để làm rõ về các chỉ số tính đúng đắn, đối với các bài toán, chúng tôi đo độ chính xác bằng cách xác minh liệu câu trả lời số chính xác có mặt trong cả kết quả thực thi mã và phản hồi cuối cùng. Khi nói đến trực quan hóa dữ liệu, chúng tôi đánh giá độ chính xác bằng cách sử dụng QWEN-VL (Bai et al., 2023), một mô hình ngôn ngữ đa phương thức mạnh mẽ. QWEN-VL có khả năng trả lời các câu hỏi văn bản được ghép nối với hình ảnh, và chúng tôi dựa vào nó để xác nhận liệu hình ảnh được tạo ra bởi mã có đáp ứng yêu cầu của người dùng hay không.

Kết quả về khả năng thực thi và tính đúng đắn được trình bày trong Bảng 7 và Bảng 8, tương ứng. Rõ ràng là CODE LLAMA nói chung vượt trội hơn LLAMA 2, đối tác tổng quát của nó, điều này không gây ngạc nhiên vì điểm chuẩn này cụ thể yêu cầu kỹ năng lập trình. Tuy nhiên, đáng chú ý rằng các mô hình chuyên biệt được tối ưu hóa cho tổng hợp mã không nhất thiết vượt trội hơn các mô hình tổng quát. Điều này là do thực tế rằng điểm chuẩn này bao gồm nhiều kỹ năng khác ngoài lập trình, như trừu tượng hóa các bài toán thành phương trình, hiểu các ràng buộc được chỉ định bằng ngôn ngữ, và phản hồi theo định dạng được chỉ định như ReAct. Đáng chú ý, QWEN-7B-CHAT và QWEN-14B-CHAT vượt trội hơn tất cả các lựa chọn thay thế mã nguồn mở khác có quy mô tương tự một cách đáng kể, mặc dù là các mô hình tổng quát.

**Phục vụ như một Hugging Face Agent** Hugging Face cung cấp một framework được gọi là Hugging Face Agent hoặc Transformers Agent (Hugging Face, 2023), trao quyền cho các tác nhân LLM với một bộ công cụ đa phương thức được tuyển chọn, bao gồm nhận dạng giọng nói và tổng hợp hình ảnh. Framework này cho phép một tác nhân LLM tương tác với con người, diễn giải các lệnh ngôn ngữ tự nhiên và sử dụng các công cụ được cung cấp khi cần thiết.

Để đánh giá hiệu quả của QWEN như một Hugging Face agent, chúng tôi đã sử dụng các điểm chuẩn đánh giá được cung cấp bởi Hugging Face. Kết quả được trình bày trong Bảng 9. Kết quả đánh giá cho thấy QWEN hoạt động khá tốt so với các lựa chọn thay thế mã nguồn mở khác, chỉ hơi kém so với GPT-4 độc quyền, chứng minh khả năng cạnh tranh của QWEN.

4 CODE-QWEN: MÔ HÌNH CHUYÊN BIỆT CHO LẬP TRÌNH

Huấn luyện trên dữ liệu cụ thể theo lĩnh vực đã được chứng minh là rất hiệu quả, đặc biệt trong trường hợp tiền huấn luyện và tinh chỉnh mã. Một mô hình ngôn ngữ được tăng cường bằng việc huấn luyện trên dữ liệu mã có thể phục vụ như một công cụ có giá trị cho lập trình, gỡ lỗi và thông dịch, trong số các tác vụ khác. Trong công trình này, chúng tôi đã phát triển một loạt mô hình tổng quát sử dụng các kỹ thuật tiền huấn luyện và căn chỉnh. Dựa trên nền tảng này, chúng tôi đã tạo ra các mô hình cụ thể theo lĩnh vực cho lập trình bằng cách tận dụng các mô hình ngôn ngữ cơ sở của QWEN, bao gồm mô hình tiền huấn luyện tiếp tục, CODE-QWEN và mô hình tinh chỉnh có giám sát, CODE-QWEN-CHAT. Cả hai mô hình đều có phiên bản 14 tỷ và 7 tỷ tham số.

4.1 TIỀN HUẤN LUYỆN MÃ

Chúng tôi tin rằng việc chỉ dựa vào dữ liệu mã để tiền huấn luyện có thể dẫn đến mất đáng kể khả năng hoạt động như một trợ lý đa năng. Khác với các phương pháp trước đây chỉ tập trung vào tiền huấn luyện trên dữ liệu mã (Li et al., 2022; 2023d), chúng tôi áp dụng một phương pháp khác (Rozière et al., 2023) bằng cách bắt đầu với các mô hình cơ sở QWEN được huấn luyện trên sự kết hợp của dữ liệu văn bản và mã, sau đó tiếp tục tiền huấn luyện trên dữ liệu mã. Chúng tôi tiếp tục tiền huấn luyện các mô hình trên tổng cộng khoảng 90 tỷ token. Trong giai đoạn tiền huấn luyện, chúng tôi khởi tạo mô hình bằng các mô hình ngôn ngữ cơ sở QWEN. Nhiều ứng dụng dựa vào các mô hình chuyên biệt cho lập trình có thể gặp phải các tình huống ngữ cảnh dài, như sử dụng công cụ và thông dịch mã, như được đề cập trong Phần 3.4. Để giải quyết vấn đề này, chúng tôi huấn luyện các mô hình với độ dài ngữ cảnh lên đến 8192. Tương tự như huấn luyện mô hình cơ sở trong Phần 2.4, chúng tôi sử dụng Flash Attention (Dao et al., 2022) trong các mô-đun attention, và áp dụng bộ tối ưu hóa tiêu chuẩn AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017), thiết lập β₁ = 0.9, β₂ = 0.95, và ε = 10⁻⁸. Chúng tôi đặt tỷ lệ học là 6.0×10⁻⁵ cho CODE-QWEN-14B và 3.0×10⁻⁵ cho CODE-QWEN-7B, với 3% bước khởi động và không giảm tỷ lệ học.

4.2 TINH CHỈNH CÓ GIÁM SÁT MÃ

Sau khi tiến hành một loạt thí nghiệm thực nghiệm, chúng tôi đã xác định rằng chiến lược SFT đa giai đoạn mang lại hiệu suất tốt nhất so với các phương pháp khác. Trong giai đoạn tinh chỉnh có giám sát, mô hình CODE-QWEN-CHAT được khởi tạo bởi mô hình nền tảng mã CODE-QWEN được tối ưu hóa bởi bộ tối ưu hóa AdamW (Kingma & Ba, 2014; Loshchilov & Hutter, 2017) (β₁ = 0.9, β₂ = 0.95, ε = 10⁻⁸) với tỷ lệ học 2.0×10⁻⁶ và 1.0×10⁻⁵ cho mô hình 14B và 7B tương ứng. Tỷ lệ học tăng đến giá trị đỉnh với lịch trình tỷ lệ học cosine (3% bước khởi động) và sau đó vẫn không đổi.

4.3 ĐÁNH GIÁ

Các mô hình CODE-QWEN của chúng tôi đã được so sánh với cả các mô hình ngôn ngữ độc quyền và mã nguồn mở, như được hiển thị trong Bảng 10 và 11. Những bảng này trình bày kết quả đánh giá của chúng tôi trên các tập kiểm tra của Humaneval (Chen et al., 2021), MBPP (Austin et al., 2021), và điểm chuẩn tạo mã đa ngôn ngữ HUMAN EVALPACK (Muennighoff et al., 2023). So sánh dựa trên hiệu suất pass@1 của các mô hình trên các bộ dữ liệu điểm chuẩn này. Kết quả của so sánh này được thể hiện rõ ràng trong Bảng 10 và 11.

Phân tích của chúng tôi cho thấy rằng các mô hình chuyên biệt, cụ thể là CODE-QWEN và CODE-QWEN-CHAT, vượt trội đáng kể so với các baseline trước đây có số lượng tham số tương tự, như OCTOGEEX (Muennighoff et al., 2023), InstructCodeT5+ (Wang et al., 2023d), và CodeGeeX2 (Zheng et al., 2023). Thực tế, những mô hình này thậm chí còn sánh ngang với hiệu suất của các mô hình lớn hơn như Starcoder (Li et al., 2023d).

Khi so sánh với một số mô hình nguồn đóng quy mô cực lớn, CODE-QWEN và CODE-QWEN-CHAT chứng minh lợi thế rõ ràng về pass@1. Tuy nhiên, điều quan trọng cần lưu ý là những mô hình này vẫn thua các phương pháp tiên tiến, như GPT-4, nói chung. Tuy nhiên, với việc tiếp tục mở rộng cả kích thước mô hình và kích thước dữ liệu, chúng tôi tin rằng khoảng cách này có thể được thu hẹp trong tương lai gần.

Điều quan trọng cần nhấn mạnh là các đánh giá đã đề cập trước đây không đủ để nắm bắt toàn bộ điểm mạnh và điểm yếu của các mô hình. Theo ý kiến của chúng tôi, cần thiết phải phát triển các bài kiểm tra nghiêm ngặt hơn để cho phép chúng tôi đánh giá chính xác hiệu suất tương đối so với GPT-4.

Bảng 10: Kết quả của pass@1 (%) trên HumanEval và MBPP. Hầu hết điểm số được trích xuất từ các bài báo của StarCoder (Li et al., 2023d), CodeT5+ (Wang et al., 2023d), WizardCoder (Luo et al., 2023b) và CODE LLAMA (Rozière et al., 2023).

| Mô hình                      | Tham số | HumanEval | MBPP |
|------------------------------|---------|-----------|------|
| **Mô hình độc quyền**        |         |           |      |
| PaLM                         | 540B    | 26.2      | 36.8 |
| PaLM-Coder                   | 540B    | 36.0      | 47.0 |
| PaLM 2-S                     | -       | 37.6      | 50.0 |
| Code-Cushman-001             | -       | 33.5      | 45.9 |
| Code-Davinci-002             | -       | 47.0      | 58.1 |
| GPT-3.5                      | -       | 73.2      | -    |
| GPT-4                        | -       | 86.6      | -    |
| **Mô hình mã nguồn mở**      |         |           |      |
| LLAMA 2                      | 7B      | 12.2      | 20.8 |
|                              | 13B     | 20.1      | 27.6 |
|                              | 34B     | 22.6      | 33.8 |
|                              | 70B     | 30.5      | 45.4 |
| CodeGen-Multi                | 16B     | 18.3      | 20.9 |
| CodeGen-Mono                 | 16B     | 29.3      | 35.3 |
| CodeGeeX2                    | 6B      | 35.9      | -    |
| StarCoder-Prompted           | 15B     | 40.8      | 49.5 |
| CodeT5+                      | 16B     | 30.9      | -    |
| InstructCodeT5+              | 16B     | 35.0      | -    |
| CODE LLAMA                   | 7B      | 33.5      | 41.4 |
|                              | 13B     | 36.0      | 47.0 |
|                              | 34B     | 48.8      | 55.0 |
| CODE LLAMA-INSTRUCT          | 7B      | 34.8      | 44.4 |
|                              | 13B     | 42.7      | 49.4 |
|                              | 34B     | 41.5      | 57.0 |
| CODE LLAMA-PYTHON            | 7B      | 38.4      | 47.6 |
|                              | 13B     | 43.3      | 49.0 |
|                              | 34B     | 53.7      | 56.2 |
| UNNATURAL CODE LLAMA         | 34B     | 62.2      | 61.2 |
| WizardCoder-Python           | 13B     | 64.0      | 55.6 |
|                              | 34B     | 73.2      | 61.2 |
| QWEN-CHAT                    | 7B      | 37.2      | 35.8 |
|                              | 14B     | 43.9      | 46.4 |
| CODE-QWEN                    | 7B      | 40.2      | 41.8 |
|                              | 14B     | 45.1      | 51.4 |
| CODE-QWEN-CHAT               | 7B      | 43.3      | 44.2 |
|                              | 14B     | 66.4      | 52.4 |

5 MATH-QWEN: MÔ HÌNH CHUYÊN BIỆT CHO LẬP LUẬN TOÁN HỌC

Chúng tôi đã tạo ra một chuỗi mô hình chuyên biệt về toán học được gọi là MATH-QWEN-CHAT, được xây dựng dựa trên các mô hình ngôn ngữ tiền huấn luyện QWEN. Cụ thể, chúng tôi đã phát triển các mô hình trợ lý được thiết kế đặc biệt để xuất sắc trong số học và toán học và được căn chỉnh với hành vi con người. Chúng tôi phát hành hai phiên bản của chuỗi mô hình này, MATH-QWEN-14B-CHAT và MATH-QWEN-7B-CHAT, có lần lượt 14 tỷ và 7 tỷ tham số.

5.1 HUẤN LUYỆN

Chúng tôi thực hiện SFT toán học trên bộ dữ liệu hướng dẫn toán học được tăng cường cho lập luận toán học, và do đó chúng tôi có được mô hình trò chuyện, MATH-QWEN-CHAT, trực tiếp. Do độ dài trung bình ngắn hơn của dữ liệu SFT toán học, chúng tôi sử dụng độ dài chuỗi 1024 để huấn luyện nhanh hơn. Hầu hết đầu vào người dùng trong bộ dữ liệu SFT toán học là các câu hỏi thi cử, và dễ dàng cho mô hình dự đoán định dạng đầu vào và số có thể là ngẫu nhiên. Do đó, chúng tôi che các đầu vào của hệ thống và người dùng để tránh tính toán mất mát trên chúng và thấy rằng việc che chúng tăng tốc sự hội tụ trong các thí nghiệm sơ bộ của chúng tôi. Để tối ưu hóa, chúng tôi sử dụng bộ tối ưu hóa AdamW với cùng siêu tham số của SFT ngoại trừ chúng tôi sử dụng tỷ lệ học đỉnh 2×10⁻⁵ và 50 000 bước huấn luyện.

Bảng 11: Hiệu suất pass@1 zero-shot (%) trên điểm chuẩn HUMAN EVALPACK (synthesize). Kết quả baseline một phần từ OCTOPACK (Muennighoff et al., 2023).

| Mô hình                | Tham số | Ngôn ngữ lập trình                    |               |          |          |          |          |          |
|                        |         | Python | JavaScript | Java     | Go       | C++      | Rust     | Trung bình |
|------------------------|---------|--------|------------|----------|----------|----------|----------|-----------|
| **Mô hình độc quyền**  |         |        |            |          |          |          |          |           |
| GPT-4                  | -       | 86.6   | 82.9       | 81.7     | 72.6     | 78.7     | 67.1     | 78.3      |
| **Mô hình mã nguồn mở**|         |        |            |          |          |          |          |           |
| InstructCodeT5+        | 16B     | 37.0   | 18.9       | 17.4     | 9.5      | 19.8     | 0.3      | 17.1      |
| StarChat-β             | 15B     | 33.5   | 31.4       | 26.7     | 25.5     | 26.6     | 14.0     | 26.3      |
| StarCoder              | 15B     | 33.6   | 30.8       | 30.2     | 17.6     | 31.6     | 21.8     | 27.6      |
| CodeGeeX2              | 6B      | 35.9   | 32.2       | 30.8     | 22.5     | 29.3     | 18.1     | 28.1      |
| OCTOGEEX               | 6B      | 44.7   | 33.8       | 36.9     | 21.9     | 32.3     | 15.7     | 30.9      |
| OCTOCODER              | 15B     | 46.2   | 39.2       | 38.2     | 30.4     | 35.6     | 23.4     | 35.5      |
| WizardCoder            | 15B     | 59.8   | 49.5       | 36.1     | 36.4     | 40.9     | 20.2     | 40.5      |
| QWEN-CHAT              | 7B      | 37.2   | 23.2       | 32.9     | 20.7     | 22.0     | 9.1      | 24.2      |
|                        | 14B     | 43.9   | 38.4       | 42.7     | 34.1     | 24.4     | 18.9     | 33.7      |
| CODE-QWEN              | 7B      | 40.2   | 40.4       | 40.2     | 26.2     | 20.7     | 15.8     | 30.6      |
|                        | 14B     | 45.1   | 51.8       | 57.3     | 39.6     | 18.2     | 20.7     | 38.8      |
| CODE-QWEN-CHAT         | 7B      | 43.3   | 41.5       | 49.4     | 29.3     | 32.9     | 20.1     | 36.1      |
|                        | 14B     | 66.4   | 58.5       | 56.1     | 47.6     | 54.2     | 28.7     | 51.9      |

5.2 ĐÁNH GIÁ

Chúng tôi đánh giá các mô hình trên các tập kiểm tra của GSM8K (Grade school math) (Cobbe et al., 2021), MATH (Challenging competition math problems) (Hendrycks et al., 2021), Math401 (Arithmetic ability) (Yuan et al., 2023b), và Math23K (Chinese grade school math) (Wang et al., 2017). Chúng tôi so sánh MATH-QWEN-CHAT với các mô hình độc quyền ChatGPT và Minerva (Lewkowycz et al., 2022) và mô hình chuyên biệt toán học mã nguồn mở RFT (Yuan et al., 2023a), WizardMath (Luo et al., 2023a), và GAIRMath-Abel (Chern et al., 2023a) trong Bảng 12. Các mô hình MATH-QWEN-CHAT cho thấy khả năng lập luận toán học và số học tốt hơn so với các mô hình mã nguồn mở và các mô hình QWEN-CHAT có kích thước tương tự. So với các mô hình độc quyền, MATH-QWEN-7B-CHAT vượt trội hơn Minerva-8B trong MATH. MATH-QWEN-14B-CHAT đang theo đuổi Minerva-62B và GPT-3.5 trong GSM8K và MATH và mang lại hiệu suất tốt hơn về khả năng số học và các bài toán toán học tiếng Trung.

Bảng 12: Kết quả của các mô hình về lập luận toán học. Chúng tôi báo cáo độ chính xác của QWEN cho tất cả các điểm chuẩn sử dụng giải mã tham lam. Đối với MATH, chúng tôi báo cáo hiệu suất của QWEN trên tập kiểm tra từ Lightman et al. (2023).

| Mô hình                | Tham số | GSM8K | MATH | Math401 | Math23K |
|------------------------|---------|-------|------|---------|---------|
| **Mô hình độc quyền**  |         |       |      |         |         |
| GPT-4                  | -       | 92.0  | 42.5 | 83.5    | 74.0    |
| GPT-3.5                | -       | 80.8  | 34.1 | 75.1    | 60.0    |
| Minerva                | 8B      | 16.2  | 14.1 | -       | -       |
|                        | 62B     | 52.4  | 27.6 | -       | -       |
|                        | 540B    | 58.8  | 33.6 | -       | -       |
| **Mô hình mã nguồn mở**|         |       |      |         |         |
| LLaMA-1 RFT            | 7B      | 46.5  | 5.2  | -       | -       |
|                        | 13B     | 52.1  | 5.1  | -       | -       |
| WizardMath             | 7B      | 54.9  | 10.7 | -       | -       |
|                        | 13B     | 63.9  | 14.0 | -       | -       |
|                        | 70B     | 81.6  | 22.7 | -       | -       |
| GAIRMath-Abel          | 7B      | 59.7  | 13.0 | -       | -       |
|                        | 13B     | 66.4  | 17.3 | -       | -       |
|                        | 70B     | 83.6  | 28.3 | -       | -       |
| QWEN-CHAT              | 7B      | 50.3  | 6.8  | 57.4    | 51.2    |
|                        | 14B     | 60.1  | 18.4 | 70.1    | 67.0    |
| MATH-QWEN-CHAT         | 7B      | 62.5  | 17.2 | 80.8    | 75.4    |
|                        | 14B     | 69.8  | 24.2 | 85.0    | 78.4    |

6 NGHIÊN CỨU LIÊN QUAN

6.1 MÔ HÌNH NGÔN NGỮ LỚN

Sự phấn khích về LLM bắt đầu với việc giới thiệu kiến trúc Transformer (Vaswani et al., 2017), sau đó được áp dụng để tiền huấn luyện dữ liệu quy mô lớn bởi các nhà nghiên cứu như Radford et al. (2018); Devlin et al. (2018); Liu et al. (2019). Những nỗ lực này đã dẫn đến thành công đáng kể trong transfer learning, với kích thước mô hình tăng từ 100 triệu lên hơn 10 tỷ tham số (Raffel et al., 2020; Shoeybi et al., 2019).

Năm 2020, việc phát hành GPT-3, một mô hình ngôn ngữ khổng lồ gấp 10 lần T5, đã chứng minh tiềm năng đáng kinh ngạc của học few-shot và zero-shot thông qua prompt engineering và in-context learning, và sau đó là chain-of-thought prompting (Wei et al., 2022c). Thành công này đã dẫn đến một số nghiên cứu khám phá khả năng mở rộng thêm các mô hình này (Scao et al., 2022; Zhang et al., 2022; Du et al., 2021; Zeng et al., 2022; Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022; Black et al., 2022; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Thoppilan et al., 2022). Kết quả là, cộng đồng đã đến với quan điểm coi những mô hình ngôn ngữ lớn này như những nền tảng thiết yếu cho các mô hình hạ nguồn (Bommasani et al., 2021).

Sự ra đời của ChatGPT (OpenAI, 2022) và việc ra mắt GPT-4 tiếp theo (OpenAI, 2023) đánh dấu hai khoảnh khắc lịch sử trong lĩnh vực trí tuệ nhân tạo, chứng minh rằng các mô hình ngôn ngữ lớn (LLM) có thể phục vụ như các trợ lý AI hiệu quả có khả năng giao tiếp với con người. Những sự kiện này đã khơi dậy sự quan tâm của các nhà nghiên cứu và nhà phát triển trong việc xây dựng các mô hình ngôn ngữ phù hợp với các giá trị con người và có khả năng đạt được trí tuệ nhân tạo tổng quát (AGI) (Anil et al., 2023; Anthropic, 2023a;b).

Một sự phát triển đáng chú ý trong lĩnh vực này là sự xuất hiện của các LLM mã nguồn mở, cụ thể là LLaMA (Touvron et al., 2023a) và LLAMA 2 (Touvron et al., 2023b), được công nhận là những mô hình ngôn ngữ mã nguồn mở mạnh mẽ nhất từng được tạo ra. Điều này đã dẫn đến một làn sóng hoạt động trong cộng đồng mã nguồn mở (Wolf et al., 2019), với một loạt các mô hình ngôn ngữ lớn được phát triển cộng tác để xây dựng dựa trên tiến bộ này (Mosaic ML, 2023; Almazrouei et al., 2023; ChatGLM2 Team, 2023; Yang et al., 2023; InternLM Team, 2023).

6.2 CĂN CHỈNH

Cộng đồng đã ấn tượng bởi hiệu quả đáng ngạc nhiên của căn chỉnh trên LLM. Trước đây, LLM không có căn chỉnh thường gặp phải các vấn đề như tạo lặp lại, ảo giác và sai lệch khỏi sở thích con người. Từ năm 2021, các nhà nghiên cứu đã miệt mài làm việc để phát triển các phương pháp nâng cao hiệu suất của LLM trong các tác vụ hạ nguồn (Wei et al., 2022a; Sanh et al., 2021; Longpre et al., 2023; Chung et al., 2022; Muennighoff et al., 2022). Hơn nữa, các nhà nghiên cứu đã tích cực khám phá cách căn chỉnh LLM với hướng dẫn của con người (Ouyang et al., 2022; Askell et al., 2021; Bai et al., 2022b;c). Một thách thức lớn trong nghiên cứu căn chỉnh là khó khăn trong việc thu thập dữ liệu. Trong khi OpenAI đã sử dụng nền tảng của mình để thu thập prompt hoặc hướng dẫn của con người, điều này không khả thi đối với những người khác để thu thập dữ liệu như vậy.

Tuy nhiên, đã có một số tiến bộ trong lĩnh vực này, chẳng hạn như phương pháp self-instruct được đề xuất trong Wang et al. (2023c). Tác phẩm sáng tạo này cung cấp một giải pháp tiềm năng cho vấn đề thu thập dữ liệu trong nghiên cứu căn chỉnh. Kết quả là, đã có sự gia tăng trong dữ liệu trò chuyện mã nguồn mở, bao gồm Alpaca (Taori et al., 2023), MOSS (Sun et al., 2023a), Dolly (Conover et al., 2023), Evol-Instruct (Xu et al., 2023b), và những cái khác (Sun et al., 2023b; Xu et al., 2023a;c; Chen et al., 2023c; Ding et al., 2023; Ji et al., 2023; Yang, 2023). Tương tự, đã có sự gia tăng trong các mô hình trò chuyện mã nguồn mở, như Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Guanaco (Dettmers et al., 2023), MOSS (Sun et al., 2023a), WizardLM (Xu et al., 2023b), và những cái khác (Xu et al., 2023c; Chen et al., 2023c; Ding et al., 2023; Wang et al., 2023b).

Để huấn luyện một mô hình trò chuyện hiệu quả, các giải pháp có sẵn chủ yếu dựa trên SFT và RLHF (Ouyang et al., 2022). Trong khi SFT tương tự như tiền huấn luyện, nó tập trung vào việc tuân theo hướng dẫn sử dụng dữ liệu đã đề cập trước đó. Tuy nhiên, đối với nhiều nhà phát triển, dung lượng bộ nhớ hạn chế là một trở ngại lớn cho nghiên cứu thêm trong SFT. Kết quả là, các phương pháp tinh chỉnh hiệu quả tham số, như LoRA (Hu et al., 2021) và Q-LoRA (Dettmers et al., 2023), đã trở nên phổ biến trong cộng đồng. LoRA chỉ tinh chỉnh các adapter thứ hạng thấp, trong khi Q-LoRA xây dựng dựa trên LoRA và sử dụng LLM được lượng tử hóa 4-bit và paged attention (Dettmers et al., 2022; Frantar et al., 2022; Kwon et al., 2023). Về RLHF, các phương pháp gần đây như PPO (Schulman et al., 2017; Touvron et al., 2023b) đã được áp dụng, nhưng cũng có các kỹ thuật thay thế nhằm giải quyết sự phức tạp của tối ưu hóa, như RRHF (Yuan et al., 2023c), DPO (Rafailov et al., 2023), và PRO (Song et al., 2023). Mặc dù có cuộc tranh luận đang diễn ra về hiệu quả của RLHF, cần có nhiều bằng chứng hơn để hiểu cách nó nâng cao trí thông minh của LLM và những nhược điểm tiềm ẩn mà nó có thể có.

6.3 SỬ DỤNG CÔNG CỤ VÀ TÁC NHÂN

Chức năng lập kế hoạch của LLM cho phép gọi các công cụ, như API hoặc khả năng tác nhân, thông qua in-context learning, như được chứng minh bởi Schick et al. (2023). Yao et al. (2022) đã giới thiệu ReAct, một định dạng tạo cho phép mô hình tạo ra các suy nghĩ về công cụ nào cần sử dụng, chấp nhận đầu vào từ các quan sát API, và tạo ra một phản hồi. GPT-3.5 và GPT-4, khi được khuyến khích với few shot, đã cho thấy hiệu suất nhất quán và ấn tượng. Ngoài việc sử dụng công cụ, LLM có thể sử dụng các nguồn bộ nhớ bên ngoài như cơ sở kiến thức (Hu et al., 2023; Zhong et al., 2023b) hoặc công cụ tìm kiếm (Nakano et al., 2021; Liu et al., 2023b) để tạo ra các câu trả lời chính xác và nhiều thông tin hơn. Điều này đã dẫn đến sự phổ biến của các framework như LangChain (LangChain, Inc., 2023). Nghiên cứu về LLM cho việc sử dụng công cụ cũng đã khơi dậy sự quan tâm trong việc xây dựng các tác nhân với khả năng LLM, như các tác nhân có thể gọi các mô hình AI khác nhau (Shen et al., 2023; Li et al., 2023a), các tác nhân học suốt đời có thân thể hoặc đa phương thức (Wang et al., 2023a; Driess et al., 2023), và nhiều tác nhân tương tác với nhau và thậm chí xây dựng một vi xã hội (Chen et al., 2023b; Li et al., 2023b; Xu et al., 2023d; Hong et al., 2023).

6.4 LLM CHO LẬP TRÌNH

Nghiên cứu trước đây đã chứng minh rằng LLM sở hữu khả năng đáng kể trong hiểu và tạo mã, đặc biệt là những mô hình có số lượng tham số khổng lồ (Chowdhery et al., 2022; Anil et al., 2023; Rae et al., 2021; Hoffmann et al., 2022). Hơn nữa, một số LLM đã được tiền huấn luyện, tiếp tục tiền huấn luyện hoặc tinh chỉnh trên dữ liệu liên quan đến lập trình, điều này đã dẫn đến hiệu suất được cải thiện đáng kể so với LLM mục đích chung. Những mô hình này bao gồm Codex Chen et al. (2021), AlphaCode (Li et al., 2022), SantaCoder (Allal et al., 2023), Starcoder-Base (Li et al., 2023d), InCoder (Fried et al., 2022), CodeT5 (Wang et al., 2021), CodeGeeX (Zheng et al., 2023), và CODE LLAMA (Rozière et al., 2023). Ngoài những mô hình này, các nghiên cứu gần đây đã tập trung vào việc phát triển các kỹ thuật căn chỉnh chuyên biệt cho lập trình, như Code Llama-Instruct (Rozière et al., 2023) và StarCoder (Li et al., 2023d). Những mô hình này có thể hỗ trợ các nhà phát triển trong nhiều tác vụ liên quan đến mã, bao gồm tạo mã (Chen et al., 2021; Austin et al., 2021), hoàn thành mã (Zhang et al., 2023a), dịch mã (Szafraniec et al., 2023), sửa lỗi (Muennighoff et al., 2023), tinh chỉnh mã (Liu et al., 2023c), và trả lời câu hỏi về mã (Liu & Wan, 2021). Nói một cách ngắn gọn, LLM có tiềm năng cách mạng hóa lĩnh vực lập trình bằng cách cung cấp cho các nhà phát triển những công cụ mạnh mẽ để hiểu, tạo mã và các tác vụ liên quan.

6.5 LLM CHO TOÁN HỌC

LLM với một quy mô mô hình nhất định đã được phát hiện sở hữu khả năng thực hiện lập luận toán học (Wei et al., 2022b; Suzgun et al., 2022). Để khuyến khích LLM đạt được hiệu suất tốt hơn trong các tác vụ liên quan đến toán học, các nhà nghiên cứu đã sử dụng các kỹ thuật như chain-of-thought prompting (Wei et al., 2022c) và scratchpad (Nye et al., 2021), đã cho thấy kết quả đầy hứa hẹn. Ngoài ra, self-consistency (Wang et al., 2022) và least-to-most prompting (Zhou et al., 2022) đã cải thiện thêm hiệu suất của những mô hình này trên các tác vụ này. Tuy nhiên, prompt engineering là một quá trình tốn thời gian đòi hỏi nhiều thử nghiệm và sai sót, và vẫn khó cho LLM thực hiện tốt một cách nhất quán hoặc đạt được kết quả thỏa đáng trong việc giải quyết các vấn đề toán học. Hơn nữa, việc đơn giản mở rộng dữ liệu và kích thước mô hình không phải là một cách hiệu quả để cải thiện khả năng lập luận toán học của mô hình. Thay vào đó, tiền huấn luyện trên các corpus liên quan đến toán học đã được chứng minh là liên tục nâng cao những khả năng này (Hendrycks et al., 2021; Lewkowycz et al., 2022; Taylor et al., 2022; Lightman et al., 2023). Ngoài ra, tinh chỉnh trên các bộ dữ liệu tuân theo hướng dẫn liên quan đến toán học (Si et al., 2023; Yuan et al., 2023a; Luo et al., 2023a; Yue et al., 2023; Chern et al., 2023a; Yu et al., 2023), cũng đã hiệu quả và tiết kiệm chi phí hơn so với tiền huấn luyện cụ thể cho toán học. Mặc dù có những hạn chế về độ chính xác, LLM vẫn có tiềm năng đáng kể để hỗ trợ người dùng với các vấn đề toán học thực tế. Vẫn còn nhiều chỗ cho sự phát triển thêm trong lĩnh vực này.

7 KẾT LUẬN

Trong báo cáo này, chúng tôi trình bày chuỗi mô hình ngôn ngữ lớn QWEN, thể hiện những tiến bộ mới nhất trong xử lý ngôn ngữ tự nhiên. Với 14B, 7B và 1.8B tham số, những mô hình này đã được tiền huấn luyện trên lượng dữ liệu khổng lồ, bao gồm hàng nghìn tỷ token, và được tinh chỉnh bằng các kỹ thuật tiên tiến như SFT và RLHF. Ngoài ra, chuỗi QWEN bao gồm các mô hình chuyên biệt cho lập trình và toán học, như CODE-QWEN, CODE-QWEN-CHAT, và MATH-QWEN-CHAT, đã được huấn luyện trên dữ liệu cụ thể theo lĩnh vực để xuất sắc trong các lĩnh vực tương ứng. Kết quả của chúng tôi chứng minh rằng chuỗi QWEN có tính cạnh tranh với các mô hình mã nguồn mở hiện có và thậm chí sánh ngang với hiệu suất của một số mô hình độc quyền trên các điểm chuẩn toàn diện và đánh giá con người.

Chúng tôi tin rằng việc mở cửa QWEN sẽ thúc đẩy sự hợp tác và đổi mới trong cộng đồng, cho phép các nhà nghiên cứu và nhà phát triển xây dựng dựa trên công việc của chúng tôi và đẩy ranh giới của những gì có thể với các mô hình ngôn ngữ. Bằng cách cung cấp những mô hình này cho công chúng, chúng tôi hy vọng sẽ truyền cảm hứng cho nghiên cứu và ứng dụng mới sẽ tiếp tục thúc đẩy lĩnh vực này và đóng góp vào sự hiểu biết của chúng ta về các biến số và kỹ thuật được giới thiệu trong các cài đặt thực tế. Nói tóm lại, chuỗi QWEN đại diện cho một cột mốc quan trọng trong sự phát triển của chúng tôi về các mô hình ngôn ngữ lớn, và chúng tôi hào hứng khi thấy cách nó sẽ được sử dụng để thúc đẩy tiến bộ và đổi mới trong những năm tới.
