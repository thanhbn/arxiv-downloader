# 2306.01709.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2306.01709.pdf
# File size: 480310 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Distilling Efficient Language-Specific Models for Cross-Lingual Transfer
Alan Ansell1Edoardo Maria Ponti2,1Anna Korhonen1Ivan Vuli ´c1
1Language Technology Lab, University of Cambridge
2University of Edinburgh
aja63@cam.ac.uk
Abstract
Massively multilingual Transformers (MMTs),
such as mBERT and XLM-R, are widely used
for cross-lingual transfer learning. While these
are pretrained to represent hundreds of lan-
guages, end users of NLP systems are often
interested only in individual languages. For
such purposes, the MMTs’ language coverage
makes them unnecessarily expensive to deploy
in terms of model size, inference time, en-
ergy, and hardware cost. We thus propose to
extract compressed, language-specific models
from MMTs which retain the capacity of the
original MMTs for cross-lingual transfer. This
is achieved by distilling the MMT bilingually ,
i.e., using data from only the source and tar-
get language of interest. Specifically, we use
a two-phase distillation approach, termed BIS-
TILLATION : (i) the first phase distils a general
bilingual model from the MMT, while (ii) the
second, task-specific phase sparsely fine-tunes
the bilingual ‘student’ model using a task-tuned
variant of the original MMT as its ‘teacher’.
We evaluate this distillation technique in zero-
shot cross-lingual transfer across a number of
standard cross-lingual benchmarks. The key
results indicate that the distilled models ex-
hibit minimal degradation in target language
performance relative to the base MMT despite
being significantly smaller and faster. Further-
more, we find that they outperform multilin-
gually distilled models such as DistilmBERT
and MiniLMv2 while having a very modest
training budget in comparison, even on a per-
language basis. We also show that bilingual
models distilled from MMTs greatly outper-
form bilingual models trained from scratch.
1 Introduction
Massively multilingual Transformers (MMTs), pre-
trained on unlabelled data from hundreds of lan-
guages, are a highly effective tool for cross-lingual
transfer (Devlin et al., 2019; Conneau et al., 2020;
Chung et al., 2020; He et al., 2021). However,
they suffer from several limitations as a result of
3032343638Performance ()
DP (LAS)
6869707172NER (F1)
0.0 0.5 1.0
Relative FLOPS ()
49.049.550.050.551.051.5Performance ()
NLI (ACC)
0.0 0.5 1.0
Relative FLOPS ()
65.067.570.072.575.077.5QA (F1)
model
LtSft
DistilmBERT
BiStil, lrf = 2
BiStil, lrf = 3
MiniLMv2parameters
80M
120M
160M
200M
240MFigure 1: Tradeoff between parameter count, inference
FLOPs and averaged performance for our BISTILmod-
els for cross-lingual transfer and several baselines.
their ample language coverage. Firstly, aiming to
represent many languages within their parameter
budget and dealing with the training signals from
different languages might result in negative inter-
ference. This is known as the “curse of multilin-
guality” (Conneau et al., 2020), which impairs the
MMT’s transfer capabilities (Pfeiffer et al., 2022).
Secondly, in practice people are often interested in
using or researching NLP systems in just a single
language. This makes the MMTs unnecessarily
costly in terms of storage, memory, and compute
and thus hard to deploy. This especially impacts
communities which speak low-resource languages,
which are more likely to have limited access to
computational resources (Alabi et al., 2022).arXiv:2306.01709v1  [cs.CL]  2 Jun 2023

--- PAGE 2 ---
In this work, we address the question: can we
increase the time-efficiency and space-efficiency of
MMTs while retaining their performance in cross-
lingual transfer? Knowledge distillation (Hinton
et al., 2015) is a family of general methods to
achieve the first goal by producing smaller, faster
models (Sanh et al., 2019; Jiao et al., 2020, inter
alia) and has also been applied to MMTs specif-
ically. However, when the distilled MMT is re-
quired to cover the same number of languages as
the original model, whose capacity is already thinly
stretched over hundreds of languages, the “curse of
multilinguality” asserts itself, resulting in a signifi-
cant loss in performance (Sanh et al., 2019).
As a consequence, to achieve the best possible
performance with reduced capacity, we depart from
the practice of retaining all the languages from the
original MMT in the distilled model. Instead, we
argue, we should cover only twolanguages, namely
the source language and the target language of inter-
est. In fact, distilling just onelanguage would fall
short of the second goal stated above, namely fa-
cilitating cross-lingual transfer, as a monolingually
distilled model would be unable to learn from a
distinct source language during task-specific fine-
tuning. Maintaining cross-lingual transfer capa-
bilities, however, is crucial due to the paucity of
labelled task data in many of the world’s languages
in most tasks (Ponti et al., 2019; Joshi et al., 2020).
In particular, we propose a method for bilingual
distillation of MMTs, termed BISTILLATION , in-
spired by the two-phase recipe of Jiao et al. (2020).
We start from a “student” model, initialized by dis-
carding a subset of layers of the original “teacher”
MMT, as well as the irrelevant part of its vocabu-
lary. In the first, “general” phase of distillation,
unlabelled data is used to align the the hidden rep-
resentations and attention distributions of the stu-
dent with those of the teacher. In the second, task-
specific phase, the student is fine-tuned for the task
of interest through guidance from a task-adapted
variant of the teacher. Rather than fully fine-tuning
the student during this second phase, we instead use
the parameter-efficient Lottery-Ticket Sparse Fine-
Tuning (LT-SFT) method of Ansell et al. (2022).
Parameter-efficient task fine-tuning enables a sys-
tem to support multiple tasks with the same dis-
tilled compact model, without unnecessarily creat-
ing full model copies per each task.
We evaluate our efficient “bistilled” models on
a range of downstream tasks from several bench-marks for multilingual NLP, including dependency
parsing from Universal Dependencies (UD; Ze-
man et al., 2020), named entity recognition from
MasakhaNER (Adelani et al., 2021), natural lan-
guage inference from AmericasNLI (Ebrahimi
et al., 2022), and QA from XQuAD (Artetxe et al.,
2020). We evaluate the model performance as well
as its space efficiency (measured in terms of pa-
rameter count) and time efficiency (measured in
terms of FLOPs and inference time). We com-
pare it against highly relevant baselines: bilingual
models pretrained from scratch and two existing
multilingual distilled models, DistilmBERT (Sanh
et al., 2019) and MiniLMv2 (Wang et al., 2021a).
We find that while our bilingually distilled mod-
els are twice or thrice smaller and faster than the
original MMT, their performance is only slightly
degraded, as illustrated in Figure 1. Our method
outperforms the baselines by sizable margins,
showing the advantages of (i) bilingual as opposed
to multilingual distillation, and (ii) distilling mod-
els from MMTs rather than training them from
scratch. We hope that our endeavour will bene-
fit end-users of multilingual models, and poten-
tial users under-served by currently available tech-
nologies, by making NLP systems more accessi-
ble. The code and models are publicly available at
https://github.com/AlanAnsell/bistil .
2 Background
2.1 Cross-Lingual Transfer with MMTs
Prominent examples of MMTs include mBERT
(Devlin et al., 2019), XLM-R (Conneau et al., 2020)
and mDeBERTa (He et al., 2021), among others.
Pires et al. (2019) and Wu and Dredze (2019)
showed that mBERT is surprisingly effective at
zero-shot cross-lingual transfer . Zero-shot cross-
lingual transfer is a useful paradigm when there is
little or no training data available for the task of
interest in the target language, but there is training
data available in some other source language. In the
simplest form of zero-shot cross-lingual transfer,
the model is trained on source language data and
is then used without modification for inference on
target language data. While this generally works
quite well for high-resource languages, transfer
performance degrades for low-resource languages,
especially those under-represented or fully unseen
by the MMT during its pretraining (Lauscher et al.,
2020; Pfeiffer et al., 2020; Ansell et al., 2021; Ade-
lani et al., 2021; Ebrahimi et al., 2022).

--- PAGE 3 ---
2.2 Modular Adaptation of MMTs
Because MMTs divide their capacity among many
languages, they may often perform sub-optimally
with respect to a single source or target language.
Furthermore, we are sometimes interested in a tar-
get language not covered by the MMT. A naive
solution to these problems is to prepare the MMT
with continued pretraining on the target language
before proceeding to task fine-tuning. While this
can improve performance, Pfeiffer et al. (2020)
show that a more effective approach is to perform
this continued pretraining in a parameter-efficient
manner, specifically with the use of adapters (Re-
buffi et al., 2017; Houlsby et al., 2019). The re-
sulting language-specific adapter is known as a lan-
guage adapter . When the task fine-tuning is also
learned in the form of an adapter ( task adapter ),
Pfeiffer et al. demonstrate that zero-shot transfer
can be achieved by composing arbitrary language
and task adapter pairs.
Ansell et al. (2022) extend this idea to a new
parameter-fine tuning method, sparse fine-tuning
(SFT). An SFT of a model is where only a sparse
subset of its pre-trained parameters are fine-tuned,
i.e. an SFT of a pretrained model Fwith param-
etersθcan be written as F(·;θ+ϕ), where the
difference vector ϕis sparse (Sung et al., 2021).
Language and task SFTs with difference vectors
ϕLandϕTrespectively are composed through ad-
dition, i.e. yielding F(·;θ+ϕL+ϕT). SFTs are
learned through a procedure called “Lottery Ticket
Sparse Fine-Tuning” (LT-SFT), based on the Lot-
tery Ticket algorithm of Frankle and Carbin (2019).
Thek% of parameters which undergo the greatest
absolute change during an initial full fine-tuning
phase are selected as tunable parameters during the
second “sparse” phase which yields the final SFT.
As SFT composition exhibited somewhat bet-
ter zero-shot cross-lingual transfer performance
across a range of tasks than adapter composition,
and SFTs avoid the inference time slow-down in-
curred by adapters at inference time, we adopt
this parameter-efficient approach throughout this
work. However, we note that other modular and
parameter-efficient architectures can also be tried
in future work (Pfeiffer et al., 2023).
Multi-Source Training. Ansell et al. (2021) show
that multi-source task adapter training, where a task
adapter is trained using data from several source
languages simultaneously, yields large gains in
cross-lingual transfer performance as a result ofthe task adapter learning more language-agnostic
representations. Ansell et al. (2022) find similarly
large gains from multi-source training of task SFTs.
An important aspect of cross-lingual transfer with
SFTs is that the source language SFT is applied
during task SFT training. This requires each batch
during multi-source training to consist of exam-
ples from a single source language, for which the
relevant language SFT is applied during the corre-
sponding training step.
2.3 Distilling Pretrained Language Models
Knowledge distillation (Bucilu ˘a et al., 2006; Hin-
ton et al., 2015) is a technique for compressing
a pretrained large “teacher” model into a smaller
“student” model by training the student to copy the
behavior of the teacher. Whereas during standard
pretraining, the model receives a single “hard” la-
bel per training example, during distillation the stu-
dent benefits from the enriched signal provided by
the full label distribution predicted by the teacher
model. Sanh et al. (2019) use this technique to pro-
duce DistilBERT , a distilled version of BERT base
(Devlin et al., 2019) with 6 instead of the origi-
nal 12 layers, and DistilmBERT , a corresponding
distilled version of multilingual BERT. There has
been extensive subsequent work on distillation of
pretrained language models, but with less focus on
distilling MMTs in particular.
3 B ISTILLATION : Methodology
Overview. We are interested in providing NLP ca-
pabilities with limited computational resources in a
specific target language Twhich lacks training data
in the tasks of interest. A common paradigm in pre-
vious work (Pfeiffer et al., 2020; Ansell et al., 2022)
is to use cross-lingual transfer with an MMT in con-
junction with parameter-efficient task and language
adaptation to support multiple tasks without adding
a large number of additional parameters per task,
see §2.2. Our goal in this work is to replace the
highly general MMT, plus optional language adap-
tation, with a target language-specific model which
maintains the benefits of cross-lingual transfer.
An obvious first attempt would be to simply dis-
til the MMT into a smaller model using only text in
the target language. However, this monolingual dis-
tillation approach is insufficient, as during task fine-
tuning, the monolingually distilled student model
no longer “understands” the source language. In-
deed, our preliminary experiments confirmed the

--- PAGE 4 ---
intuition that this approach is inadequate. This
problem can be overcome through bilingual distil-
lation, where text from both the source and target
language is used to train the student model.1
Therefore, our aim is to devise a method for de-
riving from an MMT Ma smaller model M′
S,T,τ
to perform a given task τin the target language T
given only training data in the source language S.
Our approach is inspired by the two-stage distil-
lation paradigm of Jiao et al. (2020). In the first,
“general” phase, a bilingual student model M′
S,Tis
distilled from Musing the same unsupervised task
(e.g., masked language modeling) that was used
forM’s pretraining. In the second, “task-specific”
phase, M′
S,T,τ is produced by fine-tuning M′
S,Tus-
ingMτas its teacher, where Mτis derived from M
by fine-tuning it for task τ. The following sections
explain the details of these phases.
3.1 Distillation Method
LetLTbe the number of Transformer layers in the
teacher model, indexed from 1 to LT. The number
of student model layers LSis required to evenly
divide LT. We define the downscaling stride as
s=LT
LS.
Following Jiao et al. (2020), the loss functions
of the two distillation phases make use of three
components, (i) attention-based , (ii) hidden state-
based , and (iii) prediction-based . Attention-based
loss is defined as follows:
Lattn=1
LSLSX
i=1MSE(AS
i, AT
i·s). (1)
Here, AS
iandAT
i∈Rl×lrefer to the attention
distribution2of Transformer layer iof the student
and teacher model, respectively; lrefers to the input
sequence length; MSE() denotes mean squared error
loss.
Hidden state-base loss is defined as follows:
Lhidden =1
LS+ 1LSX
i=0MSE(HS
i, HT
i·s),(2)
1This is similar to the idea of bilingual language adapters
proposed by Parovi ´c et al. (2022), which obtain superior trans-
fer performance by adapting the MMT to both source and
target language simultaneously, removing the need to use dif-
ferent and possibly incompatible language adapters during
training and inference.
2Here, for ease of implementation within the Huggingface
Transformers library (Wolf et al., 2020), we differ from Jiao
et al. (2020), who use raw attention scores.where HS
iandHT
i∈Rl×drefer to the hidden
representations output by Transformer layer iof
the student and teacher model, respectively, or the
output of the embedding layer when i= 0. Note
that we assume that the student and teacher share
the same hidden dimensionality d.
Finally, the prediction-based loss is defined as
Lpred=CE(zS,zT), (3)
where zSandzTare the label distributions pre-
dicted by the student and teacher model, respec-
tively, and CEdenotes cross-entropy loss.
The intuition behind using attention-based and
hidden state-based loss for our purposes is as fol-
lows. We (i) require good monolingual perfor-
mance in the source and target language, but we
also (ii) must preserve the existing alignment be-
tween these languages in the MMT which would
consequently facilitate transfer between them. The
intuition is that encouraging the student’s interme-
diate representations to match those of the teacher
will help to preserve this alignment.
We next describe how these loss components are
employed in each phase of B ISTILLATION .
3.2 Stage 1: General Bilingual Distillation
Initialization. We initialize all parameters of the
student model by copying those of the teacher
model, but retaining only the Transformer layers
whose indices are multiples of s.
Vocabulary Reduction. Our distilled models can
dispose of the many irrelevant tokens in the base
MMT’s vocabulary, i.e. those which are not fre-
quently used in either the source or target language
of interest, an idea previously proposed by Abdaoui
et al. (2020). During initialization, the vocabu-
lary of the student model is selected by retaining
only the tokens of the teacher’s vocabulary whose
unigram probability in either the source or target
language corpus is ≥10−6.
Teacher Language Adaptation. As we wish to be
able to produce distilled models for languages not
covered in the base MMT, and to obtain the best
possible performance for languages which are cov-
ered, we employ language adaptation of the teacher
MMT with language-specific SFTs (Ansell et al.,
2022) applied on top of the original MMT during
distillation.3Since it draws examples from two lan-
guages, each with its own language SFT, bilingual
3Put simply, additionally applying language-specific SFTs
‘skews’ the MMT towards those particular languages.

--- PAGE 5 ---
distillation becomes a special case of multi-source
training as described in §2.2. At each training step,
either the source or target language is selected at
random with equal probability; the batch is com-
posed of sequences drawn from the training corpus
of the chosen language, and a pretrained SFT for
that language is applied to the teacher MMT.
Objective. The overall loss function for this phase
is given by the sum of the attention-based and hid-
den state-based loss. Omitting the prediction-based
loss here has the advantage of avoiding the need to
evaluate the distribution of tokens predicted by the
MLM head, which is costly because of the consid-
erable size of MMTs’ embedding matrices.
3.3 Stage 2: Task-Specific Distillation
After a general bilingual model has been distilled
from the teacher MMT in Stage 1, it can be fine-
tuned for a specific task. We first obtain the
teacher for task-specific distillation by applying
task-specific LT-SFT to fine-tune the base MMT
(i.e., the teacher in the general distillation phase)
for the task in question. This teacher’s outputs and
representations are then used to fine-tune the bilin-
gual student model, again using task LT-SFT at the
student’s end. The use of parameter-efficient task
adaptation here avoids adding a large number of
parameters to the system for each task. The objec-
tive during this task-specific fine-tuning consists of
the sum of all three losses from §3.1: Lattn,Lhidden ,
andLpred.
4 Experimental Setup
We largely adopt the evaluation framework of
Ansell et al. (2022) for direct comparability with
their LT-SFT method, which they apply to undis-
tilled MMTs, and which we apply for task-specific
fine-tuning of bilingually distilled MMTs. Specif-
ically, we evaluate zero-shot cross-lingual trans-
fer performance on four representative tasks: de-
pendency parsing, named entity recognition, nat-
ural language inference, and QA. While the prior
work focused only on low-resource languages, our
method is also highly relevant to high-resource lan-
guages: the XQuAD QA task (Artetxe et al., 2020)
provides additional insight into high-resource tar-
get language performance. Table 1 summarizes the
experimental setup, including the datasets and lan-
guages considered in our experiments. In total, we
cover a set of 44 typologically and geographically
diverse languages, which makes them representa-tive of cross-lingual variation (Ponti et al., 2020).
We experiment with three different MMTs
as shown in Table 1: mBERT (Devlin et al.,
2019), XLM-R base(Conneau et al., 2020), and
mDeBERTa base(He et al., 2021).
4.1 Baselines and Model Variants
We refer to our main method as BISTIL. We com-
pare it with several relevant approaches. First, the
LTSFTmethod (Ansell et al., 2022), a state-of-the-
art cross-lingual transfer approach, uses LT-SFT
with language adaptation on the base MMT. LTSFT
can be seen as an upper bound for BISTIL, allowing
us to measure how much the performance suffers as
a result of replacing the MMT with its bilingually
distilled variant.
For each task except NLI,4we also compare
against a multilingually distilled MMT, i.e. with all
pretraining languages used for distillation as well.
For DP and NER, where mBERT is the base MMT,
the distilled MMT is DISTILM BERT (Sanh et al.,
2019), which is similarly based on mBERT. For
QA, where BISTILuses mDeBERTa as the base
MMT, no directly comparable multilingually dis-
tilled MMT is available, so we opt for a loose com-
parison with MINILM V2(Wang et al., 2021a), dis-
tilled from XLM-R large, which has achieved strong
results on cross-lingual transfer in high-resource
languages. We perform task-specific fine-tuning
with LT-SFT on DistilmBERT and MiniLMv2 in
the same way as for the the undistilled MMTs in the
LTSFTsetting. For DP and NER we also perform
language adaptation of DistilmBERT.5
We also consider SCRATCH , a setting where we
train bilingual models from scratch instead of dis-
tilling them from a pretrained MMT. We then apply
the same LT-SFT task fine-tuning method as for
the other baselines. This comparison allows us to
evaluate the benefit of distilling efficient bilingual
models from the MMT rather than pretraining the
same-sized bilingual models from scratch.
We refer to our main method, with the task-
specific distillation stage as described in §3.3, as
4There does not seem to be a multilingually distilled MMT
that would provide a suitable comparison on the AmericasNLI
task, as MiniLMv2 and other models distilled without an
MLM head do not support adaptation to unseen languages
through continued pretraining. DistilmBERT on the other
hand is based on a generally weaker MMT than XLM-R,
which is used as B ISTIL’s base MMT for NLI.
5MiniLMv2 does not support language adaptation (see
Footnote 4), but this is not as important for the high-resource
languages used in XQuAD (Ansell et al., 2022).

--- PAGE 6 ---
Task Target Dataset Source Dataset MMT Target Languages
Dependency Parsing
(DP)Universal Dependencies
2.7 (Zeman et al., 2020)Universal Dependencies
2.7 (Zeman et al., 2020)mBERTArabic†, Bambara, Buryat, Cantonese, Chinese†, Erzya,
Faroese, Japanese†, Livvi, Maltese, Manx, North Sami,
Komi Zyrian, Sanskrit, Upper Sorbian, Uyghur
Named Entity Recogni-
tion (NER)MasakhaNER (Adelani
et al., 2021)CoNLL 2003 (Tjong
Kim Sang and De Meul-
der, 2003)mBERTHausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian-
Pidgin, Swahili∗, Wolof, Yorùbá∗
Natural Language Infer-
ence (NLI)AmericasNLI (Ebrahimi
et al., 2022)MultiNLI (Williams
et al., 2018)XLM-RAymara, Asháninka, Bribri, Guarani, Náhuatl, Otomí,
Quechua, Rarámuri, Shipibo-Konibo, Wixarika
Question Answering
(QA)XQuAD (Artetxe et al.,
2020)SQuAD v1.1 (Rajpurkar
et al., 2016)mDeBERTaArabic†, Chinese†, German†, Greek†, Hindi†,
Romanian†, Russian†, Spanish†, Thai†, Turkish†,
Vietnamese†
Table 1: Details of the tasks, datasets, MMTs and languages involved in our zero-shot cross-lingual transfer
evaluation.∗denotes low-resource languages and†high-resource languages seen during MMT pretraining; all other
languages are low-resource and unseen. The source language is always English. We ‘bistil’ the MMT listed per
each task and target language. Further details of all the language and data sources used are provided in Appendix B.
MMT Distillation LRF DRF #L D #V #P
mBERTnone - - 12 768 120K 178M
D’MBERT 2 - 6 768 120K 135M
BISTIL∗ 2 - 6 768 31K 67M
3 - 4 768 31K 53M
XLM-R basenone - - 12 768 250K 278M
BISTIL∗ 2 - 6 768 28K 65M
3 - 4 768 28K 51M
XLM-R largenone - - 24 1024 250K 560M
MINILM V2 2 2.67 12 384 250K 118M
mDeBERTanone - - 12 768 250K 278M
BISTIL∗ 2 - 6 768 41K 75M
3 - 4 768 41K 60M
Table 2: Dimensions of models before andafter distilla-
tion. LRF = Layer Reduction Factor; DRF = hidden Di-
mension Reduction Factor; #L = number of Transformer
Layers; D = hidden Dimension; #V = V ocabulary size;
#P = total number of model Parameters; D’MBERT =
DISTILM BERT .∗- because of its vocabulary reduction
procedure, BISTILcan produce models of slightly dif-
ferent sizes for different languages; the vocabulary sizes
and numbers of parameters shown are averages over all
BISTILmodels trained.
BISTIL-TF (TF = teacher forcing ). We also carry
out an ablation focused on the second phase of
BISTILLATION : here, we consider performing
task-specific fine-tuning without the assistance of a
teacher, i.e. in the same manner as LTSFT. We refer
to this variant as B ISTIL-ST (ST = self-taught ).
Table 2 provides details of the model sizes, be-
fore and after distillation using the above methods,
demonstrating the benefits of BISTILLATION with
respect to model compactness.
4.2 Distillation/Adaptation Training Setup
We always perform language adaptation of the
teacher model during both phases of BISTILLA -
TION and during LTSFTexcept for mDeBERTa andMiniLMv26. For language adaptation of MMTs
we use the pretrained language SFTs of Ansell
et al. (2022), and we train our own for Distilm-
BERT. Similarly, for the LTSFTbaseline, and for
task adaptation of the teacher in the BISTIL-TF
configuration, we use their pretrained single-source
task SFTs or train our own when necessary. When
training/distilling our own models or SFTs, we gen-
erally choose hyperparameters which match those
used to train their SFTs in the original work. See
Appendix A for full training details and hyperpa-
rameters of all models in our comparison, and Ap-
pendix B for details of the training corpora.
We experiment with two layer reduction factors
(LRF) for BISTILLATION , 2 (a reduction from 12
to 6 layers) and 3 (12 to 4 layers). Whereas the BIS-
TILsetting initializes the model from the teacher
(see §3.2), the SCRATCH setting initializes it ran-
domly.
5 Results and Discussion
The results in terms of task performance are sum-
marized in Tables 3-6. As expected, LTSFTon the
undistilled MMTs performs best across all tasks.
However, BISTIL-TF with reduction factor 2 is not
much worse, with a degradation in performance
not exceeding 1.3 points relative to LTSFTon DP,
NER and NLI. The larger gap of 3.4 EM points
on QA is likely a result of the fact that the base
MMT is much more thoroughly pretrained on the
high-resource languages found in XQuAD than on
the lower-resource languages found in the datasets
for the other tasks. It is therefore harder for BIDIS-
6See Footnote 4 for MiniLMv2; mDeBERTa could in the-
ory support language adaptation but its pretraining code was
not made publicly available in time to be used in this work.

--- PAGE 7 ---
ar bm bxr fo gv hsb ja kpv mt myv olo sa sme ug yue zh avg avg ∆
LTSFT 53.6 16.5 25.9 55.5 42.4 60.5 19.7 27.2 55.4 45.3 47.8 25.2 42.1 16.7 34.0 37.0 37.8 -
DISTILM BERT 47.7 9.9 19.5 49.1 31.7 53.2 16.2 20.0 43.0 34.9 37.6 17.7 31.4 11.4 28.9 33.9 30.4 -7.4
SCRATCH ,LRF= 2 16.9 4.9 6.7 27.8 9.1 15.2 6.7 5.6 16.1 12.7 11.1 3.5 9.3 3.9 11.5 14.6 11.0 -26.8
BISTIL-ST, LRF= 2 50.9 15.8 24.1 53.7 38.3 57.1 18.7 23.9 52.2 43.7 46.5 25.2 39.8 13.3 31.8 34.8 35.6 -2.2
BISTIL-ST, LRF= 3 48.2 16.1 23.4 52.1 35.0 55.1 18.1 22.2 49.9 40.3 41.3 22.2 37.6 13.3 30.7 33.4 33.7 -4.1
BISTIL-TF, LRF= 2 53.2 16.4 24.6 54.8 39.1 59.0 19.0 23.8 54.1 43.5 46.0 26.9 40.7 13.1 32.7 36.4 36.5 -1.3
BISTIL-TF, LRF= 3 49.7 16.4 24.4 52.7 36.8 57.1 18.2 21.0 52.2 41.0 43.3 25.1 38.1 14.5 31.3 34.9 34.8 -3.0
Table 3: DP; LAS scores. The results with the smallest gap to the upper-bound L TSFTmodel are in bold .
hau ibo kin lug luo pcm swa wol yor avg avg ∆
LTSFT 83.5 76.7 67.4 67.9 54.7 74.6 79.4 66.3 74.8 71.7 -
DISTILM BERT 81.1 73.2 65.3 63.4 50.0 69.2 77.7 64.4 71.2 68.4 -3.3
BISTIL-ST, LRF= 2 81.3 74.1 65.9 66.7 53.5 72.1 77.1 64.6 72.8 69.8 -1.9
BISTIL-ST, LRF= 3 80.3 74.0 63.1 64.6 54.7 69.6 76.9 68.0 70.5 69.1 -2.6
BISTIL-TF, LRF= 2 81.0 74.8 67.5 67.3 55.0 72.9 78.4 69.0 75.7 71.3 -0.4
BISTIL-TF, LRF= 3 79.6 74.8 64.6 64.5 56.7 70.6 77.2 66.1 72.8 69.6 -2.1
Table 4: NER; F1 scores.
aym bzd cni gn hch nah oto quy shp tar avg avg ∆
LTSFT 58.1 44.4 47.9 63.5 42.8 52.4 48.5 62.0 50.3 43.3 51.3 -
BISTIL-TF, LRF= 2 58.9 45.7 46.4 62.9 44.3 50.8 44.0 58.7 47.2 43.1 50.2 -1.1
BISTIL-TF, LRF= 3 57.7 43.6 48.1 60.9 41.3 51.4 42.6 59.9 45.5 40.3 49.1 -2.2
Table 5: NLI accuracy (%)
ar de el es hi ro ru th tr vi zh avg avg ∆
LTSFT 56.5 64.7 61.2 62.4 57.8 69.0 61.8 56.0 56.4 57.1 60.8 60.3 -
MINILM V2 50.4 59.4 54.4 57.9 52.9 64.5 57.6 50.3 51.3 53.8 55.0 55.2 -5.1
BISTIL-TF, LRF= 2 53.5 62.2 55.4 59.8 54.5 66.2 58.3 54.4 53.1 53.4 55.7 57.0 -3.4
BISTIL-TF, LRF= 3 44.3 55.0 44.1 55.2 46.1 59.5 51.3 42.4 48.3 44.6 50.9 49.3 -11.1
Table 6: XQuAD; Exact Match scores.
TILto achieve the base MMT’s depth of knowl-
edge of the target language during its relatively
short distillation training time. BISTIL-TF, LRF=
2nevertheless outperforms MiniLMv2 on QA by
1.7 EM points, despite MiniLMv2 receiving 320
times more training than each BIDISTIL model, or
roughly 6 times more per language7.
Furthermore, BISTIL-TF, LRF= 2significantly
outperforms DISTILM BERT , with a 6.1 LAS gap
on DP and 2.9 F1 gap on NER. BISTIL,LRF = 2
produces models roughly half the size of DISTILM -
BERT and that, once again, are trained for vastly
less time8.
Training bilingual models from SCRATCH per-
forms poorly, lagging behind the other methods
7MiniLMv2 is trained for 1M steps with a batch size of
256 and max sequence length of 512; BIDISTIL for 200K
steps with a batch size of 8 and max sequence length of 256.
8Sanh et al. (2019) note that their monolingual DistilBERT
model was trained on 8 16GB V100 GPUs for approximately
90 hours. Our BISTILmodels are trained on a single 10GB
RTX 3080 GPU for approximately 9 hours.by more than 20 points on DP.9One crucial weak-
ness of SCRATCH , besides its reduced monolingual
performance, is a lack of alignment between its
representations of the source and target languages,
severely impairing cross-lingual transfer. This high-
lights the advantage of distilling a bilingual model
from an MMT within which cross-lingual align-
ment is already present.
Interestingly, when we evaluate the SCRATCH
models on their English DP performance, we obtain
an average UAS/LAS score of 81.8/77.1, which
is much more competitive in relative terms with
theBISTIL-TF, LRF = 2 English DP score of
91.0/88.2 than the corresponding comparison in
average target language DP scores of 29.9/11.0 to
55.5/36.5. This suggests that an even larger factor
inSCRATCH ’s weakness than its poor monolingual
performance is a lack of alignment between its
representations of the source and target languages,
9As this method is clearly inferior, we opted to reduce
computational expense by not repeating it for other tasks.

--- PAGE 8 ---
cpu ↑gpu ↑ flops ↓
DISTILM BERT 1.41x 1.03x 0.61x
BISTIL,LRF= 2 1.44x 1.25x 0.61x
BISTIL,LRF= 3 1.71x 1.36x 0.48x
(a) DP efficiency
cpu ↑gpu ↑ flops ↓
DISTILM BERT 1.93x 1.94x 0.50x
BISTIL,LRF= 2 1.97x 1.98x 0.50x
BISTIL,LRF= 3 2.97x 2.78x 0.33x
(b) NER efficiency
cpu ↑gpu ↑ flops ↓
BISTIL,LRF= 2 2.02x 1.97x 0.50x
BISTIL,LRF= 3 2.89x 2.85x 0.33x
(c) NLI efficiency
cpu ↑gpu ↑ flops ↓
MINILM V2 4.25x 3.44x 0.21x
BISTIL,LRF= 2 1.99x 1.85x 0.50x
BISTIL,LRF= 3 2.85x 2.42x 0.33x
(d) QA efficiency
Table 7: Relative inference speed and FLOP cost. Val-
ues are given relative to LTSFTwithout distillation, i.e.
a speed reading of “2.00x” means the distilled model can
on average process twice as many inference examples
per second as the undistilled MMT. Likewise a FLOPs
reading of “0.50x” would mean that the distilled model
on average requires half as many FLOPs to process an
inference example as the undistilled MMT does.
severely impairing cross-lingual transfer. This high-
lights the advantage of distilling a bilingual model
from an MMT within which cross-lingual align-
ment is already present.
As expected, the performance of BISTILis some-
what weaker with a larger layer reduction factor
of 3, though this is heavily task-dependent. With
an LRF of 3, BISTIL-TF still comfortably outper-
forms DISTILM BERT on DP and NER, and does
not fall much behind LRF = 2 for NLI. However,
we observe a considerable degradation in perfor-
mance for LRF = 3 for QA; this may indicate that
a 4-layer Transformer struggles to adapt to this
particular task, or that for this architecture the mod-
est training time is not sufficient to approach the
base MMT’s understanding of the source and target
languages.
Table 7 presents an analysis of the inference time
efficiency. We measure the inference speed both
on CPU with batch size 1 and GPU with the same
batch size as during task-specific training. We also
calculate the number of floating-point operations
(FLOPs) per example using fvcore, measured dur-ing an inference pass over the test set of the first
language in each task.
For NER, NLI and QA, the efficiency results
conform quite closely to the intuitive expectation
that a model’s inference time should scale linearly
with its number of layers; that is, BIDISTIL with
LRF = 2 is generally around twice as fast as the
base MMT. For DP, we observe a seemingly sub-
linear scaling which is caused by the very large
biaffine parsing head, consisting of ∼23M param-
eters. The significant cost of applying the model
head contributes equally to all models regardless of
their degree of distillation. Despite having a mod-
erate LRF of 2, MINILM V2exhibits impressive
speed as a result of the fact that it additionally has
a smaller hidden dimension than its teacher (see
Table 2), a technique which we do not consider
forBIDISTIL , but may be a promising avenue for
future work.
We argue that BIDISTIL accomplishes its aim by
achieving two- to three-fold reductions in inference
time and model size without sacrificing much in
the way of raw performance. Its superior perfor-
mance relative to multilingually distilled models
despite its comparatively very modest training bud-
get supports the assertion that specializing multi-
lingual models for a specific transfer pair during
distillation helps to avoid performance degradation
resulting from the curse of multilinguality.
6 Related Work
One strand of prior work focuses on parameter-
efficient adaptation of pretrained MMTs, i.e. adap-
tation by adding/modifying a small subset of pa-
rameters. Adapters (Rebuffi et al., 2017; Houlsby
et al., 2019) have been used extensively for this
purpose (Üstün et al., 2020), with the MAD-X
framework of Pfeiffer et al. (2020) becoming a
starting point for several further developments (Vi-
doni et al., 2020; Wang et al., 2021b; Parovi ´c et al.,
2022), where a notable theme is adapting MMTs to
unseen languages (Ansell et al., 2021; Pfeiffer et al.,
2021). Ansell et al. (2022) propose composable
sparse fine-tunings as an alternative to adapters.
Pfeiffer et al. (2022) create a modular MMT
from scratch, where some parameters are shared
among all languages and others are language-
specific. This allows the model to dedicate con-
siderable capacity to every language without each
language-specific model becoming overly large;
thus it is quite similar in its aims to this work.

--- PAGE 9 ---
A variety of approaches have been proposed for
general distillation of pretrained language mod-
els. The simplest form uses only soft target prob-
abilities predicted by the teacher model as the
training signal for the student (Sanh et al., 2019).
Other approaches try to align the hidden states
and self-attention distributions of the student and
teacher (Sun et al., 2020; Jiao et al., 2020) and/or
finer-grained aspects of the self-attention mecha-
nism (Wang et al., 2020, 2021a). Mukherjee et al.
(2021) initialize the student’s embedding matrix
with a factorization of the teacher’s for better per-
formance when their hidden dimensions differ. Of
these, Sanh et al. (2019); Wang et al. (2020, 2021a);
Mukherjee et al. (2021) apply their methods to pro-
duce distilled versions of MMTs.
Parovi ´c et al. (2022) adapt pretrained MMTs
to specific transfer pairs with adapters; this ap-
proach is similar to ours in spirit, but it is aimed
towards improving performance rather than effi-
ciency. Minixhofer et al. (2022) learn to transfer
full monolingual models across languages. The
only work prior we are aware of which creates
purely bilingual models for cross-lingual transfer
is that of Tran (2020). This approach starts with
a monolingual pretrained source language model,
initializes target language embeddings via an align-
ment procedure, and then continues training the
model with the added target embeddings on both
languages.
7 Conclusions
While MMTs are an effective tool for cross-lingual
transfer, their broad language coverage makes them
unnecessarily costly to deploy in the frequently-
encountered situation where capability is required
in only a single, often low-resource, language. We
have proposed BISTILLATION , a method of train-
ing more efficient models suited to this scenario
which works by distilling an MMT using only the
source-target language pair of interest. We show
that this approach produces models that offer an
excellent trade off between target language perfor-
mance, efficiency, and model compactness. The
‘bistilled’ models exhibit only a slight decrease in
performance relative to their base MMTs whilst
achieving considerable reduction in both model
size and inference time. Their results also compare
favorably to those of multilingually distilled MMTs
despite receiving substantially less training even on
a per-language basis.Limitations
While the results of our experiments seem suffi-
cient to validate the concept and our general ap-
proach to bilingual distillation, we have not car-
ried out a detailed systematic analysis of alterna-
tive implementations of the various aspects of our
methods, such as different student model initial-
izations, distillation objectives and hyperparame-
ter settings. Furthermore, our BISTILmodels are
likely undertrained due to limited computational
resources. Consequently, we do not claim our spe-
cific implementation of bilingual distillation to be
optimal or even close to optimal. Areas that war-
rant further investigation toward realizing the full
potential of this approach include the use of hid-
den dimension reduction, which yielded impressive
speed gains for MiniLMv2 in our experiments, and
other innovations in distillation such as progressive
knowledge transfer (Mukherjee et al., 2021).
With the exception of improved efficiency, our
BISTILmodels inherit the limitations of the MMTs
from which they are distilled; notably, there is a
discrepancy between the performance on high- and
low-resource languages resulting from the distribu-
tion of data used during MMT pretraining.
In this work, we have only considered English
as the source language; some target languages may
benefit from other transfer sources. Future work
may also consider the use of multi-source transfer,
which would entail distilling with more than two
languages. Here the challenge would be optimizing
the balance of model capacity allocated to source
languages versus the target language.
Acknowledgements
Alan wishes to thank David and Claudia Harding
for their generous support via the Harding Dis-
tinguished Postgraduate Scholarship Programme.
Ivan Vuli ´c is supported by a personal Royal So-
ciety University Research Fellowship ‘Inclusive
and Sustainable Language Technology for a Truly
Multilingual World’ (no 221137; 2022–).
References
Amine Abdaoui, Camille Pradel, and Grégoire Sigel.
2020. Load what you need: Smaller versions of
mutililingual BERT. In Proceedings of SustaiNLP:
Workshop on Simple and Efficient Natural Language
Processing , pages 119–123, Online. Association for
Computational Linguistics.

--- PAGE 10 ---
David Ifeoluwa Adelani, Jade Abbott, Graham Neu-
big, Daniel D’souza, Julia Kreutzer, Constantine Lig-
nos, Chester Palen-Michel, Happy Buzaaba, Shruti
Rijhwani, Sebastian Ruder, Stephen Mayhew, Is-
rael Abebe Azime, Shamsuddeen H. Muhammad,
Chris Chinenye Emezue, Joyce Nakatumba-Nabende,
Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau,
Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-
mam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,
Rubungo Andre Niyongabo, Jonathan Mukiibi, Ver-
rah Otiende, Iroro Orife, Davis David, Samba Ngom,
Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,
Gerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-
wuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel
Oyerinde, Clemencia Siro, Tobius Saul Bateesa,
Temilola Oloyede, Yvonne Wambui, Victor Akin-
ode, Deborah Nabagereka, Maurice Katusiime, Ayo-
dele Awokoya, Mouhamadane MBOUP, Dibora Ge-
breyohannes, Henok Tilaye, Kelechi Nwaike, De-
gaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-
vaoghene Ahia, Bonaventure F. P. Dossou, Kelechi
Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,
Adewale Akinfaderin, Tendai Marengereke, and Sa-
lomey Osei. 2021. MasakhaNER: Named entity
recognition for African languages. Transactions
of the Association for Computational Linguistics ,
9:1116–1131.
Željko Agi ´c and Ivan Vuli ´c. 2019. JW300: A wide-
coverage parallel corpus for low-resource languages.
InProceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics , pages 3204–
3210, Florence, Italy. Association for Computational
Linguistics.
Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius
Mosbach, and Dietrich Klakow. 2022. Adapting pre-
trained language models to African languages via
multilingual adaptive fine-tuning. In Proceedings of
the 29th International Conference on Computational
Linguistics , pages 4336–4349, Gyeongju, Republic
of Korea. International Committee on Computational
Linguistics.
Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan
Vuli´c. 2022. Composable sparse fine-tuning for cross-
lingual transfer. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1778–1796,
Dublin, Ireland. Association for Computational Lin-
guistics.
Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-
bastian Ruder, Goran Glavaš, Ivan Vuli ´c, and Anna
Korhonen. 2021. MAD-G: Multilingual adapter gen-
eration for efficient cross-lingual transfer. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2021 , pages 4762–4781, Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020. On the cross-lingual transferability of mono-
lingual representations. In Proceedings of the 58th
Annual Meeting of the Association for ComputationalLinguistics , pages 4623–4637, Online. Association
for Computational Linguistics.
David Brambila. 1976. Diccionario Raramuri-
Castellano: Tarahumar .
Cristian Bucilu ˘a, Rich Caruana, and Alexandru
Niculescu-Mizil. 2006. Model compression. In Pro-
ceedings of the 12th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ’06, page 535–541, New York, NY , USA.
Association for Computing Machinery.
Gina Bustamante, Arturo Oncevay, and Roberto
Zariquiey. 2020. No data to crawl? monolingual
corpus creation from PDF files of truly low-resource
languages in Peru. In Proceedings of the 12th Lan-
guage Resources and Evaluation Conference , pages
2914–2923, Marseille, France. European Language
Resources Association.
Luis Chiruzzo, Pedro Amarilla, Adolfo Ríos, and
Gustavo Giménez Lugo. 2020. Development of a
Guarani - Spanish parallel corpus. In Proceedings of
the 12th Language Resources and Evaluation Confer-
ence, pages 2629–2633, Marseille, France. European
Language Resources Association.
Hyung Won Chung, Thibault Fevry, Henry Tsai, Melvin
Johnson, and Sebastian Ruder. 2020. Rethinking em-
bedding coupling in pre-trained language models. In
International Conference on Learning Representa-
tions .
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451, Online. Association for Computational Lin-
guistics.
Rubén Cushimariano Romano and Richer C. Se-
bastián Q. 2008. Ñaantsipeta asháninkaki bi-
rakochaki. diccionario asháninka-castellano. ver-
sión preliminar. http://www.lengamer.org/
publicaciones/diccionarios/ .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . Open-
Review.net.

--- PAGE 11 ---
Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,
Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John
Ortega, Ricardo Ramos, Annette Rios, Ivan Vladimir
Meza Ruiz, Gustavo Giménez-Lugo, Elisabeth
Mager, Graham Neubig, Alexis Palmer, Rolando
Coto-Solano, Thang Vu, and Katharina Kann. 2022.
AmericasNLI: Evaluating zero-shot natural language
understanding of pretrained multilingual models in
truly low-resource languages. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
6279–6299, Dublin, Ireland. Association for Compu-
tational Linguistics.
Isaac Feldman and Rolando Coto-Solano. 2020. Neu-
ral machine translation models with back-translation
for the extremely low-resource indigenous language
Bribri. In Proceedings of the 28th International Con-
ference on Computational Linguistics , pages 3965–
3976, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Jonathan Frankle and Michael Carbin. 2019. The lottery
ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning
Representations .
Ana-Paula Galarreta, Andrés Melgar, and Arturo On-
cevay. 2017. Corpus creation and initial SMT ex-
periments between Spanish and Shipibo-konibo. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing, RANLP
2017 , pages 238–244, Varna, Bulgaria. INCOMA
Ltd.
Goran Glavaš and Ivan Vuli ´c. 2021. Is supervised syn-
tactic parsing beneficial for language understanding
tasks? an empirical investigation. In Proceedings
of the 16th Conference of the European Chapter of
the Association for Computational Linguistics: Main
Volume , pages 3090–3104, Online. Association for
Computational Linguistics.
Ximena Gutierrez-Vasques, Gerardo Sierra, and
Isaac Hernandez Pompa. 2016. Axolotl: a web
accessible parallel corpus for Spanish-Nahuatl. In
Proceedings of the Tenth International Conference
on Language Resources and Evaluation (LREC’16) ,
pages 4210–4214, Portorož, Slovenia. European Lan-
guage Resources Association (ELRA).
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.
Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding shar-
ing.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedingsof Machine Learning Research , pages 2790–2799.
PMLR.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.
TinyBERT: Distilling BERT for natural language un-
derstanding. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 4163–
4174, Online. Association for Computational Lin-
guistics.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
6282–6293, Online. Association for Computational
Linguistics.
Anne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and
Goran Glavaš. 2020. From zero to hero: On the
limitations of zero-shot language transfer with mul-
tilingual Transformers. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 4483–4499, On-
line. Association for Computational Linguistics.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .
Manuel Mager, Diónico Carrillo, and Ivan Meza. 2018.
Probabilistic finite-state morphological segmenter for
wixarika (huichol) language. Journal of Intelligent
& Fuzzy Systems , 34(5):3081–3087.
Elena Mihas. 2011. Añaani katonkosatzi parenini, El id-
ioma del alto Perené . Milwaukee, WI: Clarks Graph-
ics.
Benjamin Minixhofer, Fabian Paischer, and Navid Rek-
absaz. 2022. WECHSEL: Effective initialization of
subword embeddings for cross-lingual transfer of
monolingual language models. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 3992–4006,
Seattle, United States. Association for Computational
Linguistics.
Subhabrata Mukherjee, Ahmed Hassan Awadallah, and
Jianfeng Gao. 2021. Xtremedistiltransformers: Task
transfer for task-agnostic distillation.
John E Ortega, Richard Alexander Castro-Mamani, and
Jaime Rafael Montoya Samame. 2020. Overcoming
resistance: The normalization of an Amazonian tribal
language. In Proceedings of the 3rd Workshop on
Technologies for MT of Low Resource Languages ,
pages 1–13, Suzhou, China. Association for Compu-
tational Linguistics.
Marinela Parovi ´c, Goran Glavaš, Ivan Vuli ´c, and Anna
Korhonen. 2022. BAD-X: Bilingual adapters im-
prove zero-shot cross-lingual transfer. In Proceed-
ings of the 2022 Conference of the North Ameri-
can Chapter of the Association for Computational

--- PAGE 12 ---
Linguistics: Human Language Technologies , pages
1791–1799, Seattle, United States. Association for
Computational Linguistics.
Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James
Cross, Sebastian Riedel, and Mikel Artetxe. 2022.
Lifting the curse of multilinguality by pre-training
modular transformers. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 3479–3495, Seattle,
United States. Association for Computational Lin-
guistics.
Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli ´c, and
Edoardo Maria Ponti. 2023. Modular deep learning.
CoRR , abs/2302.11529.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-
bastian Ruder. 2020. MAD-X: An Adapter-Based
Framework for Multi-Task Cross-Lingual Transfer.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7654–7673, Online. Association for Computa-
tional Linguistics.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebas-
tian Ruder. 2021. UNKs everywhere: Adapting mul-
tilingual language models to new scripts. In Proceed-
ings of the 2021 Conference on Empirical Methods in
Natural Language Processing , pages 10186–10203,
Online and Punta Cana, Dominican Republic. Asso-
ciation for Computational Linguistics.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How multilingual is multilingual BERT? In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 4996–5001, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska,
Qianchu Liu, Ivan Vuli ´c, and Anna Korhonen. 2020.
XCOPA: A multilingual dataset for causal common-
sense reasoning. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 2362–2376, Online. As-
sociation for Computational Linguistics.
Edoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak,
Ivan Vuli ´c, Roi Reichart, Thierry Poibeau, Ekate-
rina Shutova, and Anna Korhonen. 2019. Modeling
language variation and universals: A survey on ty-
pological linguistics for natural language processing.
Computational Linguistics , 45(3):559–601.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea
Vedaldi. 2017. Learning multiple visual domainswith residual adapters. In Advances in Neural In-
formation Processing Systems , volume 30. Curran
Associates, Inc.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020. MobileBERT:
a compact task-agnostic BERT for resource-limited
devices. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 2158–2170, Online. Association for Computa-
tional Linguistics.
Yi-Lin Sung, Varun Nair, and Colin Raffel. 2021. Train-
ing neural networks with fixed sparse masks. In Ad-
vances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Process-
ing Systems 2021, NeurIPS 2021, December 6-14,
2021, virtual , pages 24193–24205.
Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC’12) , pages 2214–2218, Istanbul,
Turkey. European Language Resources Association
(ELRA).
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Natural
Language Learning at HLT-NAACL 2003 , pages 142–
147.
Ke Tran. 2020. From english to foreign languages:
Transferring pre-trained language models.
Ahmet Üstün, Arianna Bisazza, Gosse Bouma, and Gert-
jan van Noord. 2020. UDapter: Language adaptation
for truly Universal Dependency parsing. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
2302–2315, Online. Association for Computational
Linguistics.
Marko Vidoni, Ivan Vuli ´c, and Goran Glavaš. 2020.
Orthogonal language and task adapters in zero-shot
cross-lingual transfer.
Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,
and Furu Wei. 2021a. MiniLMv2: Multi-head self-
attention relation distillation for compressing pre-
trained transformers. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 2140–2151, Online. Association for Computa-
tional Linguistics.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. In Advances in Neural
Information Processing Systems , volume 33, pages
5776–5788. Curran Associates, Inc.

--- PAGE 13 ---
Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Gra-
ham Neubig. 2021b. Efficient test time adapter en-
sembling for low-resource language varieties. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 730–737, Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas:
The surprising cross-lingual effectiveness of BERT.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 833–844, Hong
Kong, China. Association for Computational Linguis-
tics.
Daniel Zeman, Joakim Nivre, Mitchell Abrams, Elia
Ackermann, Noëmi Aepli, Hamid Aghaei, Željko
Agi´c, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy
Ajede, Gabriel ˙e Aleksandravi ˇci¯ut˙e, Ika Alfina, Lene
Antonsen, Katya Aplonova, Angelina Aquino, Car-
olina Aragon, Maria Jesus Aranzabe, Hórunn
Arnardóttir, Gashaw Arutie, Jessica Naraiswari Ar-
widarasti, Masayuki Asahara, Luma Ateyah, Furkan
Atmaca, Mohammed Attia, Aitziber Atutxa, Liesbeth
Augustinus, Elena Badmaeva, Keerthana Balasubra-
mani, Miguel Ballesteros, Esha Banerjee, Sebastian
Bank, Verginica Barbu Mititelu, Victoria Basmov,
Colin Batchelor, John Bauer, Seyyit Talha Bedir,
Kepa Bengoetxea, Gözde Berk, Yevgeni Berzak, Ir-
shad Ahmad Bhat, Riyaz Ahmad Bhat, Erica Biagetti,
Eckhard Bick, Agn ˙e Bielinskien ˙e, Kristín Bjarnadót-
tir, Rogier Blokland, Victoria Bobicev, Loïc Boizou,
Emanuel Borges Völker, Carl Börstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Adriane Boyd,
Kristina Brokait ˙e, Aljoscha Burchardt, Marie Can-
dito, Bernard Caron, Gauthier Caron, Tatiana Cav-
alcanti, Gül¸ sen Cebiro ˘glu Eryi ˘git, Flavio Massimil-
iano Cecchini, Giuseppe G. A. Celano, Slavomír ˇCé-
plö, Savas Cetin, Özlem Çetino ˘glu, Fabricio Chalub,Ethan Chi, Yongseok Cho, Jinho Choi, Jayeol Chun,
Alessandra T. Cignarella, Silvie Cinková, Aurélie
Collomb, Ça ˘grı Çöltekin, Miriam Connor, Ma-
rine Courtin, Elizabeth Davidson, Marie-Catherine
de Marneffe, Valeria de Paiva, Mehmet Oguz De-
rin, Elvis de Souza, Arantza Diaz de Ilarraza,
Carly Dickerson, Arawinda Dinakaramani, Bamba
Dione, Peter Dirix, Kaja Dobrovoljc, Timothy Dozat,
Kira Droganova, Puneet Dwivedi, Hanne Eckhoff,
Marhaba Eli, Ali Elkahky, Binyam Ephrem, Olga
Erina, Tomaž Erjavec, Aline Etienne, Wograine
Evelyn, Sidney Facundes, Richárd Farkas, Marília
Fernanda, Hector Fernandez Alcalde, Jennifer Fos-
ter, Cláudia Freitas, Kazunori Fujita, Katarína Gaj-
došová, Daniel Galbraith, Marcos Garcia, Moa
Gärdenfors, Sebastian Garza, Fabrício Ferraz Ger-
ardi, Kim Gerdes, Filip Ginter, Iakes Goenaga,
Koldo Gojenola, Memduh Gökırmak, Yoav Goldberg,
Xavier Gómez Guinovart, Berta González Saavedra,
Bernadeta Grici ¯ut˙e, Matias Grioni, Loïc Grobol, Nor-
munds Gr ¯uz¯ıtis, Bruno Guillaume, Céline Guillot-
Barbance, Tunga Güngör, Nizar Habash, Hinrik Haf-
steinsson, Jan Haji ˇc, Jan Haji ˇc jr., Mika Hämäläi-
nen, Linh Hà M ˜y, Na-Rae Han, Muhammad Yudi-
stira Hanifmuti, Sam Hardwick, Kim Harris, Dag
Haug, Johannes Heinecke, Oliver Hellwig, Fe-
lix Hennig, Barbora Hladká, Jaroslava Hlavá ˇcová,
Florinel Hociung, Petter Hohle, Eva Huber, Jena
Hwang, Takumi Ikeda, Anton Karl Ingason, Radu
Ion, Elena Irimia, O.lájídé Ishola, Tomáš Jelínek,
Anders Johannsen, Hildur Jónsdóttir, Fredrik Jør-
gensen, Markus Juutinen, Sarveswaran K, Hüner
Ka¸ sıkara, Andre Kaasen, Nadezhda Kabaeva, Syl-
vain Kahane, Hiroshi Kanayama, Jenna Kanerva,
Boris Katz, Tolga Kayadelen, Jessica Kenney, Vá-
clava Kettnerová, Jesse Kirchner, Elena Klementieva,
Arne Köhn, Abdullatif Köksal, Kamil Kopacewicz,
Timo Korkiakangas, Natalia Kotsyba, Jolanta Ko-
valevskait ˙e, Simon Krek, Parameswari Krishna-
murthy, Sookyoung Kwak, Veronika Laippala, Lu-
cia Lam, Lorenzo Lambertino, Tatiana Lando,
Septina Dian Larasati, Alexei Lavrentiev, John Lee,
Phương Lê H `ông, Alessandro Lenci, Saran Lert-
pradit, Herman Leung, Maria Levina, Cheuk Ying
Li, Josie Li, Keying Li, Yuan Li, KyungTae Lim,
Krister Lindén, Nikola Ljubeši ´c, Olga Loginova,
Andry Luthfi, Mikko Luukko, Olga Lyashevskaya,
Teresa Lynn, Vivien Macketanz, Aibek Makazhanov,
Michael Mandl, Christopher Manning, Ruli Manu-
rung, C ˘at˘alina M ˘ar˘anduc, David Mare ˇcek, Katrin
Marheinecke, Héctor Martínez Alonso, André Mar-
tins, Jan Mašek, Hiroshi Matsuda, Yuji Matsumoto,
Ryan McDonald, Sarah McGuinness, Gustavo Men-
donça, Niko Miekka, Karina Mischenkova, Mar-
garita Misirpashayeva, Anna Missilä, C ˘at˘alin Mi-
titelu, Maria Mitrofan, Yusuke Miyao, AmirHossein
Mojiri Foroushani, Amirsaeid Moloodi, Simonetta
Montemagni, Amir More, Laura Moreno Romero,
Keiko Sophie Mori, Shinsuke Mori, Tomohiko
Morioka, Shigeki Moro, Bjartur Mortensen, Bohdan
Moskalevskyi, Kadri Muischnek, Robert Munro,
Yugo Murawaki, Kaili Müürisep, Pinkey Nainwani,
Mariam Nakhlé, Juan Ignacio Navarro Horñiacek,

--- PAGE 14 ---
Anna Nedoluzhko, Gunta Nešpore-B ¯erzkalne, L ương
Nguy ˜ên Th i., Huy `ên Nguy ˜ên Th i.Minh, Yoshihiro
Nikaido, Vitaly Nikolaev, Rattima Nitisaroj, Alireza
Nourian, Hanna Nurmi, Stina Ojala, Atul Kr. Ojha,
Adéday o.‘ Olúòkun, Mai Omura, Emeka Onwueg-
buzia, Petya Osenova, Robert Östling, Lilja Øvre-
lid, ¸ Saziye Betül Özate¸ s, Arzucan Özgür, Balkız
Öztürk Ba¸ saran, Niko Partanen, Elena Pascual,
Marco Passarotti, Agnieszka Patejuk, Guilherme
Paulino-Passos, Angelika Peljak-Łapi ´nska, Siyao
Peng, Cenel-Augusto Perez, Natalia Perkova, Guy
Perrier, Slav Petrov, Daria Petrova, Jason Phelan,
Jussi Piitulainen, Tommi A Pirinen, Emily Pitler,
Barbara Plank, Thierry Poibeau, Larisa Ponomareva,
Martin Popel, Lauma Pretkalni n,a, Sophie Prévost,
Prokopis Prokopidis, Adam Przepiórkowski, Tiina
Puolakainen, Sampo Pyysalo, Peng Qi, Andriela
Rääbis, Alexandre Rademaker, Taraka Rama, Lo-
ganathan Ramasamy, Carlos Ramisch, Fam Rashel,
Mohammad Sadegh Rasooli, Vinit Ravishankar, Livy
Real, Petru Rebeja, Siva Reddy, Georg Rehm, Ivan
Riabov, Michael Rießler, Erika Rimkut ˙e, Larissa Ri-
naldi, Laura Rituma, Luisa Rocha, Eiríkur Rögnvalds-
son, Mykhailo Romanenko, Rudolf Rosa, Valentin
Ros,ca, Davide Rovati, Olga Rudina, Jack Rueter,
Kristján Rúnarsson, Shoval Sadde, Pegah Safari,
Benoît Sagot, Aleksi Sahala, Shadi Saleh, Alessio
Salomoni, Tanja Samardži ´c, Stephanie Samson,
Manuela Sanguinetti, Dage Särg, Baiba Saul ¯ıte,
Yanin Sawanakunanon, Kevin Scannell, Salvatore
Scarlata, Nathan Schneider, Sebastian Schuster,
Djamé Seddah, Wolfgang Seeker, Mojgan Seraji,
Mo Shen, Atsuko Shimada, Hiroyuki Shirasu, Muh
Shohibussirri, Dmitry Sichinava, Einar Freyr Sig-
urðsson, Aline Silveira, Natalia Silveira, Maria Simi,
Radu Simionescu, Katalin Simkó, Mária Šimková,
Kiril Simov, Maria Skachedubova, Aaron Smith, Is-
abela Soares-Bastos, Carolyn Spadine, Stein hór Ste-
ingrímsson, Antonio Stella, Milan Straka, Emmett
Strickland, Jana Strnadová, Alane Suhr, Yogi Les-
mana Sulestio, Umut Sulubacak, Shingo Suzuki,
Zsolt Szántó, Dima Taji, Yuta Takahashi, Fabio Tam-
burini, Mary Ann C. Tan, Takaaki Tanaka, Sam-
son Tella, Isabelle Tellier, Guillaume Thomas, Li-
isi Torga, Marsida Toska, Trond Trosterud, Anna
Trukhina, Reut Tsarfaty, Utku Türk, Francis Ty-
ers, Sumire Uematsu, Roman Untilov, Zde ˇnka Ure-
šová, Larraitz Uria, Hans Uszkoreit, Andrius Utka,
Sowmya Vajjala, Daniel van Niekerk, Gertjan van
Noord, Viktor Varga, Eric Villemonte de la Clerg-
erie, Veronika Vincze, Aya Wakasa, Joel C. Wallen-
berg, Lars Wallin, Abigail Walsh, Jing Xian Wang,
Jonathan North Washington, Maximilan Wendt,
Paul Widmer, Seyi Williams, Mats Wirén, Chris-
tian Wittern, Tsegay Woldemariam, Tak-sum Wong,
Alina Wróblewska, Mary Yako, Kayo Yamashita,
Naoki Yamazaki, Chunxiao Yan, Koichi Yasuoka,
Marat M. Yavrumyan, Zhuoran Yu, Zden ˇek Žabokrt-
ský, Shorouq Zahra, Amir Zeldes, Hanzhi Zhu, and
Anna Zhuravleva. 2020. Universal Dependencies
2.7. LINDAT/CLARIAH-CZ digital library at the
Institute of Formal and Applied Linguistics (ÚFAL),
Faculty of Mathematics and Physics, Charles Univer-sity.
ATraining Details and Hyperparameters
As we evaluate over many languages and tasks, we
carry out a single run per (task, language, configu-
ration) triple.
A.1 Language Distillation/Adaptation
The following are constant across all language dis-
tillation/SFT training: we use a batch size of 8 and
a maximum sequence length of 256; model check-
points are evaluated every 1,000 steps (5,000 for
high-resource languages) on a held-out set of 5%
of the corpus (1% for high-resource languages),
and the one with the smallest loss is selected at
the end of training; we use the AdamW optimizer
(Loshchilov and Hutter, 2019) with linear decay
without any warm-up.
During LT-SFT training of DistilmBERT’s lan-
guage SFTs, the dense and sparse fine-tuning
phases each last the lesser of 100,000 steps or 200
epochs, but at least 30,000 steps if 200 epochs is
less. The initial learning rate is 5·10−5. The SFT
density is set to 4%.10
When distilling bilingual models or learning
them from scratch, training lasts 200,000 steps (to
equal the total length of the two phases of LT-SFT
training). The initial learning rate is 10−4. The
model architecture and hyperparameters are iden-
tical to the teacher MMT’s other than a reduction
in the number of layers and the use of vocabulary
reduction as described in §3.2.
A.2 Task Distillation/Adaptation
For DP and NER, we train task SFTs for 3 epochs
in the dense phase of LT-SFT and 10 epochs in
the sparse phase, evaluating the model checkpoint
on the validation set at the end of each epoch, and
taking the best checkpoint at the end of training.
The selection metric is labeled attachment score for
DP and F1-score for NER. The initial learning rate
is5·10−5with linear decay. For NER, we use the
standard token-level single-layer multi-class model
head. For DP, we use the shallow variant (Glavaš
and Vuli ´c, 2021) of the biaffine dependency parser
of Dozat and Manning (2017). For NLI, we train
for 5 epochs with batch size 32, with checkpoint
evaluation on the validation set every 625 steps,
10This is similar but not identical to the density used by
Ansell et al. (2022), who use a very specific number of train-
able parameters for comparability to their baseline; we prefer
to use a round number.

--- PAGE 15 ---
and an initial learning rate of 2·10−5. We apply a
two-layer multi-class classification head atop the
model output corresponding to the [CLS] token.
For QA, we train for 5 epochs with a batch size
of 12, with checkpoint evaluation every 2000 steps
and an initial learning rate of 3·10−5. The single-
layer model head independently predicts the start
and end positions of the answer span, and at in-
ference time the span whose endpoints have the
largest sum of logits is selected.
We set the density of our task SFTs to 8%, which
Ansell et al. (2022) found to offer the best task
performance in all their experiments.

--- PAGE 16 ---
B Languages
Task Language ISO Code Family UD Treebank Corpus source(s)
Source English en Indo-European, Germanic EWT Wikipedia
DPArabic ar Afro-Asiatic, Semitic PUD
WikipediaBambara bm Mande CRB
Buryat bxr Mongolic BDT
Cantonese yue Sino-Tibetan HK
Chinese zh Sino-Tibetan GSD
Erzya myv Uralic, Mordvin JR
Faroese fo Indo-European, Germanic FarPaHC
Japanese ja Japanese GSD
Livvi olo Uralic, Finnic KKPP
Maltese mt Afro-Asiatic, Semitic MUDT
Manx gv Indo-European, Celtic Cadhan
North Sami sme Uralic, Sami Giella
Komi Zyrian kpv Uralic, Permic Lattice
Sanskrit sa Indo-European, Indic UFAL
Upper Sorbian hsb Indo-European, Slavic UFAL
Uyghur ug Turkic, Southeastern UDT
NERHausa hau Afro-Asiatic, Chadic
N/AWikipedia
Igbo ibo Niger-Congo, V olta-Niger Wikipedia
Kinyarwanda kin Niger-Congo, Bantu Wikipedia
Luganda lug Niger-Congo, Bantu Wikipedia
Luo luo Nilo-Saharan Luo News Dataset (Adelani et al., 2021)
Nigerian-Pidgin pcm English Creole JW300 (Agi ´c and Vuli ´c, 2019)
Swahili swa Niger-Congo, Bantu Wikipedia
Wolof wol Niger-Congo, Senegambian Wikipedia
Yorùbá yor Niger-Congo, V olta-Niger Wikipedia
NLIAymara aym Aymaran
N/ATiedemann (2012); Wikipedia
Asháninka cni ArawakanOrtega et al. (2020); Cushimariano Romano and
Sebastián Q. (2008); Mihas (2011); Bustamante
et al. (2020)
Bribri bzd Chibchan, Talamanca Feldman and Coto-Solano (2020)
Guarani gn Tupian, Tupi-Guarani Chiruzzo et al. (2020); Wikipedia
Náhuatl nah Uto-Aztecan, Aztecan Gutierrez-Vasques et al. (2016); Wikipedia
Otomí oto Oto-Manguean, Otomian Hñähñu Online Corpus
Quechua quy Quechuan Agi ´c and Vuli ´c (2019); Wikipedia
Rarámuri tar Uto-Aztecan, Tarahumaran Brambila (1976)
Shipibo-Konibo shp Panoan Galarreta et al. (2017); Bustamante et al. (2020)
Wixarika hch Uto-Aztecan, Corachol Mager et al. (2018)
QAArabic ar Afro-Asiatic, Semitic
N/A WikipediaChinese zh Sino-Tibetan
German de Indo-European, Germanic
Greek el Indo-European, Greek
Hindi hi Indo-European, Indic
Romanian ro Indo-European, Romance
Russian ru Indo-European, Slavic
Thai th Tai-Kadai, Kam-Tai
Turkish tr Turkic, Southwestern
Vietnamese vi Austro-Asiatic, Viet-Muong
Table 8: Details of the languages and data used for training and evaluation of SFTs and adapters. The corpora
of Bustamante et al. (2020) are available at https://github.com/iapucp/multilingual-data-peru ; all other
NLI corpora mentioned are available at https://github.com/AmericasNLP/americasnlp2021 .
C Additional Results
ar bm bxr fo gv hsb ja kpv mt myv olo sa sme ug yue zh avg avg ∆
LTSFT 70.8 43.1 49.2 68.2 60.0 73.7 36.9 50.5 74.6 65.9 66.4 49.5 58.0 36.4 51.1 59.8 57.1 -
DISTILM BERT 65.7 34.4 42.3 63.0 52.8 67.6 32.1 42.2 65.4 58.6 59.6 44.1 51.2 29.2 47.0 56.1 50.7 -6.4
SCRATCH ,LRF= 2 38.5 26.6 24.8 44.9 35.4 33.5 18.6 23.4 42.9 31.5 30.2 23.0 26.1 12.3 30.8 35.6 29.9 -27.2
BISTIL-ST, LRF= 2 68.0 41.6 45.7 66.3 56.6 70.9 34.1 48.2 71.0 64.5 64.3 48.9 57.6 34.5 49.4 56.7 54.9 -2.2
BISTIL-ST, LRF= 3 65.5 42.5 45.9 64.1 52.7 68.1 33.2 46.5 68.0 62.0 61.5 46.9 55.1 32.4 48.6 55.3 53.0 -4.1
BISTIL-TF, LRF= 2 70.3 43.4 46.8 67.1 57.7 72.4 34.5 47.6 72.7 64.2 62.6 50.5 57.4 32.3 49.8 58.6 55.5 -1.6
BISTIL-TF, LRF= 3 67.0 43.9 47.6 65.1 54.2 70.0 33.4 44.2 69.7 62.3 61.8 49.2 55.1 33.3 48.9 56.5 53.9 -3.3
Table 9: DP UAS score

--- PAGE 17 ---
ar de el es hi ro ru th tr vi zh avg avg ∆
LTSFT 73.0 80.5 78.6 80.6 74.3 82.4 77.8 69.7 72.2 76.5 68.9 75.9 -
MINILM V2 66.4 75.5 72.4 76.6 69.6 78.3 74.0 63.8 67.6 73.3 64.6 71.1 -4.8
BISTIL-TF, LRF= 2 69.4 77.4 73.8 77.6 69.7 79.1 75.0 66.7 68.8 72.8 64.5 72.3 -3.6
BISTIL-TF, LRF= 3 62.4 70.7 63.3 74.7 61.4 73.4 68.7 54.3 62.9 63.0 60.4 65.0 -10.9
Table 10: XQuAD F1 score
