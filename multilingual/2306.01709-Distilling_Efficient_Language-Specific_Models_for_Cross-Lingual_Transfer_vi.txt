# Chưng cất Các Mô hình Ngôn ngữ Cụ thể Hiệu quả cho Chuyển giao Đa ngôn ngữ
Alan Ansell1Edoardo Maria Ponti2,1Anna Korhonen1Ivan Vuli ́ć1
1Phòng thí nghiệm Công nghệ Ngôn ngữ, Đại học Cambridge
2Đại học Edinburgh
aja63@cam.ac.uk
Tóm tắt
Các Transformer đa ngôn ngữ lớn (MMT),
như mBERT và XLM-R, được sử dụng rộng rãi
cho việc học chuyển giao đa ngôn ngữ. Trong khi
những mô hình này được huấn luyện trước để biểu
diễn hàng trăm ngôn ngữ, người dùng cuối của các
hệ thống NLP thường chỉ quan tâm đến từng ngôn
ngữ riêng lẻ. Đối với những mục đích như vậy, phạm
vi bao phủ ngôn ngữ của MMT khiến chúng trở nên
đắt đỏ không cần thiết để triển khai về mặt kích
thước mô hình, thời gian suy luận, năng lượng và
chi phí phần cứng. Do đó chúng tôi đề xuất trích
xuất các mô hình nén, cụ thể theo ngôn ngữ từ MMT
nhưng vẫn giữ lại khả năng của MMT gốc cho việc
chuyển giao đa ngôn ngữ. Điều này được thực hiện
bằng cách chưng cất MMT theo cách song ngữ, tức
là chỉ sử dụng dữ liệu từ ngôn ngữ nguồn và ngôn
ngữ đích quan tâm. Cụ thể, chúng tôi sử dụng
phương pháp chưng cất hai giai đoạn, được gọi là
BISTILLATION: (i) giai đoạn đầu chưng cất một
mô hình song ngữ chung từ MMT, trong khi (ii) giai
đoạn thứ hai, cụ thể cho tác vụ, tinh chỉnh thưa thớt
mô hình 'học sinh' song ngữ sử dụng biến thể được
tinh chỉnh tác vụ của MMT gốc làm 'giáo viên'.
Chúng tôi đánh giá kỹ thuật chưng cất này trong
việc chuyển giao đa ngôn ngữ zero-shot trên một số
chuẩn đánh giá đa ngôn ngữ tiêu chuẩn. Kết quả
chính cho thấy các mô hình đã chưng cất thể hiện
sự suy giảm tối thiểu trong hiệu suất ngôn ngữ đích
so với MMT cơ sở mặc dù nhỏ hơn và nhanh hơn
đáng kể. Hơn nữa, chúng tôi thấy rằng chúng vượt
trội hơn các mô hình chưng cất đa ngôn ngữ như
DistilmBERT và MiniLMv2 trong khi có ngân sách
huấn luyện rất khiêm tốn trong so sánh, thậm chí
trên cơ sở từng ngôn ngữ. Chúng tôi cũng chỉ ra
rằng các mô hình song ngữ được chưng cất từ MMT
vượt trội hơn nhiều so với các mô hình song ngữ
được huấn luyện từ đầu.

1 Giới thiệu
Các Transformer đa ngôn ngữ lớn (MMT), được
huấn luyện trước trên dữ liệu không nhãn từ hàng
trăm ngôn ngữ, là một công cụ rất hiệu quả cho
việc chuyển giao đa ngôn ngữ (Devlin et al., 2019;
Conneau et al., 2020; Chung et al., 2020; He et al.,
2021). Tuy nhiên, chúng gặp phải một số hạn chế
do phạm vi bao phủ ngôn ngữ rộng lớn. Thứ nhất,
việc nhắm đến biểu diễn nhiều ngôn ngữ trong
ngân sách tham số và xử lý các tín hiệu huấn luyện
từ các ngôn ngữ khác nhau có thể dẫn đến sự can
thiệp tiêu cực. Điều này được gọi là "lời nguyền của
tính đa ngôn ngữ" (Conneau et al., 2020), làm suy
yếu khả năng chuyển giao của MMT (Pfeiffer et al.,
2022). Thứ hai, trong thực tế, mọi người thường
chỉ quan tâm đến việc sử dụng hoặc nghiên cứu
các hệ thống NLP trong một ngôn ngữ duy nhất.
Điều này khiến MMT trở nên đắt đỏ không cần
thiết về mặt lưu trữ, bộ nhớ và tính toán và do đó
khó triển khai. Điều này đặc biệt ảnh hưởng đến
các cộng đồng nói các ngôn ngữ ít tài nguyên, vốn
có nhiều khả năng bị hạn chế về tiếp cận tài nguyên
tính toán (Alabi et al., 2022).

Trong công trình này, chúng tôi giải quyết câu hỏi:
liệu chúng ta có thể tăng hiệu quả thời gian và hiệu
quả không gian của MMT trong khi vẫn giữ được
hiệu suất của chúng trong việc chuyển giao đa ngôn
ngữ? Chưng cất kiến thức (Hinton et al., 2015) là
một họ các phương pháp chung để đạt được mục
tiêu đầu tiên bằng cách tạo ra các mô hình nhỏ hơn,
nhanh hơn (Sanh et al., 2019; Jiao et al., 2020, trong
số khác) và cũng đã được áp dụng cụ thể cho MMT.
Tuy nhiên, khi MMT đã chưng cất được yêu cầu bao
phủ cùng một số lượng ngôn ngữ như mô hình gốc,
mà khả năng của nó đã bị căng mỏng trên hàng trăm
ngôn ngữ, "lời nguyền của tính đa ngôn ngữ" khẳng
định bản thân, dẫn đến mất mát đáng kể trong hiệu
suất (Sanh et al., 2019).

Do đó, để đạt được hiệu suất tốt nhất có thể với
khả năng giảm, chúng tôi rời khỏi thông lệ giữ lại
tất cả các ngôn ngữ từ MMT gốc trong mô hình đã
chưng cất. Thay vào đó, chúng tôi lập luận, chúng ta
nên chỉ bao phủ hai ngôn ngữ, cụ thể là ngôn ngữ
nguồn và ngôn ngữ đích quan tâm. Thực tế, việc
chưng cất chỉ một ngôn ngữ sẽ không đạt được mục
tiêu thứ hai nêu trên, cụ thể là tạo điều kiện cho việc
chuyển giao đa ngôn ngữ, vì một mô hình được
chưng cất đơn ngôn ngữ sẽ không thể học từ một
ngôn ngữ nguồn khác biệt trong quá trình tinh chỉnh
cụ thể tác vụ. Tuy nhiên, việc duy trì khả năng
chuyển giao đa ngôn ngữ là rất quan trọng do sự
khan hiếm dữ liệu tác vụ có nhãn trong nhiều ngôn
ngữ trên thế giới trong hầu hết các tác vụ (Ponti et
al., 2019; Joshi et al., 2020).

Cụ thể, chúng tôi đề xuất một phương pháp cho
việc chưng cất song ngữ của MMT, được gọi là
BISTILLATION, được lấy cảm hứng từ công thức
hai giai đoạn của Jiao et al. (2020). Chúng tôi bắt
đầu từ một mô hình "học sinh", được khởi tạo bằng
cách loại bỏ một tập con các lớp của MMT "giáo
viên" gốc, cũng như phần không liên quan của từ
vựng của nó. Trong giai đoạn "chung" đầu tiên của
chưng cất, dữ liệu không nhãn được sử dụng để
căn chỉnh các biểu diễn ẩn và phân phối attention
của học sinh với những của giáo viên. Trong giai
đoạn thứ hai, cụ thể tác vụ, học sinh được tinh chỉnh
cho tác vụ quan tâm thông qua hướng dẫn từ một
biến thể được thích ứng tác vụ của giáo viên. Thay
vì tinh chỉnh hoàn toàn học sinh trong giai đoạn thứ
hai này, chúng tôi thay vào đó sử dụng phương pháp
Lottery-Ticket Sparse Fine-Tuning (LT-SFT) hiệu
quả tham số của Ansell et al. (2022). Tinh chỉnh
tác vụ hiệu quả tham số cho phép một hệ thống hỗ
trợ nhiều tác vụ với cùng một mô hình compact đã
chưng cất, mà không tạo ra các bản sao mô hình
đầy đủ không cần thiết cho mỗi tác vụ.

Chúng tôi đánh giá các mô hình "bistilled" hiệu
quả của chúng tôi trên một loạt các tác vụ downstream
từ một số benchmark cho NLP đa ngôn ngữ, bao
gồm phân tích cú pháp phụ thuộc từ Universal
Dependencies (UD; Zeman et al., 2020), nhận dạng
thực thể có tên từ MasakhaNER (Adelani et al.,
2021), suy luận ngôn ngữ tự nhiên từ AmericasNLI
(Ebrahimi et al., 2022), và QA từ XQuAD (Artetxe
et al., 2020). Chúng tôi đánh giá hiệu suất mô hình
cũng như hiệu quả không gian của nó (được đo
bằng số lượng tham số) và hiệu quả thời gian (được
đo bằng FLOP và thời gian suy luận). Chúng tôi so
sánh nó với các baseline có liên quan cao: các mô
hình song ngữ được huấn luyện trước từ đầu và hai
mô hình chưng cất đa ngôn ngữ hiện có, DistilmBERT
(Sanh et al., 2019) và MiniLMv2 (Wang et al.,
2021a).

Chúng tôi thấy rằng trong khi các mô hình được
chưng cất song ngữ của chúng tôi nhỏ hơn và nhanh
hơn gấp hai hoặc ba lần so với MMT gốc, hiệu suất
của chúng chỉ bị suy giảm nhẹ, như được minh họa
trong Hình 1. Phương pháp của chúng tôi vượt trội
hơn các baseline với lề đáng kể, cho thấy ưu điểm
của (i) chưng cất song ngữ thay vì đa ngôn ngữ, và
(ii) chưng cất các mô hình từ MMT thay vì huấn
luyện chúng từ đầu. Chúng tôi hy vọng rằng nỗ lực
của chúng tôi sẽ có lợi cho người dùng cuối của các
mô hình đa ngôn ngữ, và những người dùng tiềm
năng được phục vụ dưới mức bởi các công nghệ
hiện có, bằng cách làm cho các hệ thống NLP dễ
tiếp cận hơn. Mã và mô hình được công bố tại
https://github.com/AlanAnsell/bistil.

2 Bối cảnh

2.1 Chuyển giao Đa ngôn ngữ với MMT
Các ví dụ nổi bật của MMT bao gồm mBERT (Devlin
et al., 2019), XLM-R (Conneau et al., 2020) và
mDeBERTa (He et al., 2021), trong số những cái
khác. Pires et al. (2019) và Wu và Dredze (2019)
đã chỉ ra rằng mBERT cực kỳ hiệu quả trong việc
chuyển giao đa ngôn ngữ zero-shot. Chuyển giao
đa ngôn ngữ zero-shot là một mô hình hữu ích khi
có ít hoặc không có dữ liệu huấn luyện có sẵn cho
tác vụ quan tâm trong ngôn ngữ đích, nhưng có dữ
liệu huấn luyện có sẵn trong một số ngôn ngữ nguồn
khác. Trong hình thức đơn giản nhất của chuyển
giao đa ngôn ngữ zero-shot, mô hình được huấn
luyện trên dữ liệu ngôn ngữ nguồn và sau đó được
sử dụng mà không sửa đổi để suy luận trên dữ liệu
ngôn ngữ đích. Mặc dù điều này thường hoạt động
khá tốt cho các ngôn ngữ có nhiều tài nguyên, hiệu
suất chuyển giao suy giảm đối với các ngôn ngữ ít
tài nguyên, đặc biệt là những ngôn ngữ bị thiếu
đại diện hoặc hoàn toàn không được nhìn thấy bởi
MMT trong quá trình huấn luyện trước của nó
(Lauscher et al., 2020; Pfeiffer et al., 2020; Ansell
et al., 2021; Adelani et al., 2021; Ebrahimi et al.,
2022).

2.2 Thích ứng Mô-đun của MMT
Bởi vì MMT chia khả năng của chúng giữa nhiều
ngôn ngữ, chúng thường có thể thực hiện dưới tối
ưu đối với một ngôn ngữ nguồn hoặc đích duy nhất.
Hơn nữa, đôi khi chúng ta quan tâm đến một ngôn
ngữ đích không được bao phủ bởi MMT. Một giải
pháp ngây thơ cho những vấn đề này là chuẩn bị
MMT với việc tiếp tục huấn luyện trước trên ngôn
ngữ đích trước khi tiến hành tinh chỉnh tác vụ. Mặc
dù điều này có thể cải thiện hiệu suất, Pfeiffer et al.
(2020) cho thấy rằng một cách tiếp cận hiệu quả
hơn là thực hiện việc tiếp tục huấn luyện trước này
theo cách hiệu quả tham số, cụ thể với việc sử dụng
adapter (Rebuffi et al., 2017; Houlsby et al., 2019).
Adapter cụ thể ngôn ngữ kết quả được gọi là
language adapter. Khi tinh chỉnh tác vụ cũng được
học dưới dạng một adapter (task adapter), Pfeiffer
et al. chứng minh rằng chuyển giao zero-shot có thể
được thực hiện bằng cách kết hợp các cặp language
và task adapter tùy ý.

Ansell et al. (2022) mở rộng ý tưởng này thành
một phương pháp tinh chỉnh tham số mới, tinh chỉnh
thưa thớt (SFT). Một SFT của một mô hình là nơi
chỉ một tập con thưa thớt của các tham số được
huấn luyện trước của nó được tinh chỉnh, tức là một
SFT của một mô hình được huấn luyện trước F với
các tham số θ có thể được viết là F(·;θ+ϕ), trong
đó vector khác biệt ϕ là thưa thớt (Sung et al.,
2021). Language và task SFT với các vector khác
biệt ϕL và ϕT tương ứng được kết hợp thông qua
phép cộng, tức là cho ra F(·;θ+ϕL+ϕT). SFT được
học thông qua một quy trình được gọi là "Lottery
Ticket Sparse Fine-Tuning" (LT-SFT), dựa trên
thuật toán Lottery Ticket của Frankle và Carbin
(2019). k% tham số trải qua thay đổi tuyệt đối lớn
nhất trong giai đoạn tinh chỉnh đầy đủ ban đầu được
chọn làm tham số có thể điều chỉnh trong giai đoạn
"thưa thớt" thứ hai mang lại SFT cuối cùng.

Vì thành phần SFT thể hiện hiệu suất chuyển giao
đa ngôn ngữ zero-shot hơi tốt hơn so với thành phần
adapter trên một loạt các tác vụ, và SFT tránh được
việc chậm lại thời gian suy luận do adapter gây ra
tại thời gian suy luận, chúng tôi áp dụng cách tiếp
cận hiệu quả tham số này trong toàn bộ công trình
này. Tuy nhiên, chúng tôi lưu ý rằng các kiến trúc
mô-đun và hiệu quả tham số khác cũng có thể được
thử trong công việc tương lai (Pfeiffer et al., 2023).

Huấn luyện Đa nguồn. Ansell et al. (2021) cho thấy
rằng huấn luyện task adapter đa nguồn, trong đó
một task adapter được huấn luyện sử dụng dữ liệu
từ một số ngôn ngữ nguồn đồng thời, mang lại
những lợi ích lớn trong hiệu suất chuyển giao đa
ngôn ngữ do task adapter học được các biểu diễn
bất khả tri ngôn ngữ hơn. Ansell et al. (2022) tìm
thấy những lợi ích lớn tương tự từ việc huấn luyện
đa nguồn của task SFT. Một khía cạnh quan trọng
của chuyển giao đa ngôn ngữ với SFT là source
language SFT được áp dụng trong quá trình huấn
luyện task SFT. Điều này yêu cầu mỗi batch trong
quá trình huấn luyện đa nguồn phải bao gồm các
ví dụ từ một ngôn ngữ nguồn duy nhất, mà language
SFT liên quan được áp dụng trong bước huấn luyện
tương ứng.

2.3 Chưng cất Mô hình Ngôn ngữ Được huấn luyện trước
Chưng cất kiến thức (Bucilua et al., 2006; Hinton
et al., 2015) là một kỹ thuật để nén một mô hình
"giáo viên" lớn được huấn luyện trước thành một
mô hình "học sinh" nhỏ hơn bằng cách huấn luyện
học sinh sao chép hành vi của giáo viên. Trong khi
trong quá trình huấn luyện trước tiêu chuẩn, mô
hình nhận một nhãn "cứng" duy nhất cho mỗi ví
dụ huấn luyện, trong quá trình chưng cất học sinh
được hưởng lợi từ tín hiệu phong phú được cung
cấp bởi phân phối nhãn đầy đủ được dự đoán bởi
mô hình giáo viên. Sanh et al. (2019) sử dụng kỹ
thuật này để tạo ra DistilBERT, một phiên bản đã
chưng cất của BERT base (Devlin et al., 2019) với
6 thay vì 12 lớp gốc, và DistilmBERT, một phiên
bản đã chưng cất tương ứng của multilingual BERT.
Đã có công việc tiếp theo rộng rãi về chưng cất các
mô hình ngôn ngữ được huấn luyện trước, nhưng
ít tập trung hơn vào việc chưng cất MMT cụ thể.

3 BISTILLATION: Phương pháp luận

Tổng quan. Chúng tôi quan tâm đến việc cung cấp
khả năng NLP với tài nguyên tính toán hạn chế
trong một ngôn ngữ đích cụ thể T thiếu dữ liệu
huấn luyện trong các tác vụ quan tâm. Một mô hình
phổ biến trong công việc trước đây (Pfeiffer et al.,
2020; Ansell et al., 2022) là sử dụng chuyển giao
đa ngôn ngữ với một MMT kết hợp với thích ứng
tác vụ và ngôn ngữ hiệu quả tham số để hỗ trợ nhiều
tác vụ mà không thêm một số lượng lớn tham số bổ
sung cho mỗi tác vụ, xem §2.2. Mục tiêu của chúng
tôi trong công việc này là thay thế MMT rất chung,
cộng với thích ứng ngôn ngữ tùy chọn, bằng một
mô hình cụ thể ngôn ngữ đích duy trì các lợi ích
của chuyển giao đa ngôn ngữ.

Một nỗ lực đầu tiên rõ ràng sẽ là đơn giản chưng
cất MMT thành một mô hình nhỏ hơn chỉ sử dụng
văn bản trong ngôn ngữ đích. Tuy nhiên, cách tiếp
cận chưng cất đơn ngôn ngữ này là không đủ, vì
trong quá trình tinh chỉnh tác vụ, mô hình học sinh
được chưng cất đơn ngôn ngữ không còn "hiểu"
ngôn ngữ nguồn. Thực vậy, các thí nghiệm sơ bộ
của chúng tôi đã xác nhận trực giác rằng cách tiếp
cận này là không đủ. Vấn đề này có thể được khắc
phục thông qua chưng cất song ngữ, trong đó văn
bản từ cả ngôn ngữ nguồn và đích được sử dụng
để huấn luyện mô hình học sinh.

Do đó, mục tiêu của chúng tôi là nghiên cứu một
phương pháp để tạo ra từ một MMT M một mô hình
nhỏ hơn M'S,T,τ để thực hiện một tác vụ τ cho trước
trong ngôn ngữ đích T chỉ cho dữ liệu huấn luyện
trong ngôn ngữ nguồn S. Cách tiếp cận của chúng
tôi được lấy cảm hứng từ mô hình chưng cất hai
giai đoạn của Jiao et al. (2020). Trong giai đoạn
"chung" đầu tiên, một mô hình học sinh song ngữ
M'S,T được chưng cất từ M sử dụng cùng một tác
vụ không giám sát (ví dụ: mô hình hóa ngôn ngữ
có mặt nạ) đã được sử dụng cho việc huấn luyện
trước của M. Trong giai đoạn "cụ thể tác vụ" thứ
hai, M'S,T,τ được tạo ra bằng cách tinh chỉnh M'S,T
sử dụng Mτ làm giáo viên của nó, trong đó Mτ được
tạo ra từ M bằng cách tinh chỉnh nó cho tác vụ τ.
Các phần tiếp theo giải thích chi tiết của những
giai đoạn này.

3.1 Phương pháp Chưng cất
Gọi LT là số lớp Transformer trong mô hình giáo
viên, được chỉ số từ 1 đến LT. Số lớp mô hình học
sinh LS được yêu cầu chia đều cho LT. Chúng tôi
định nghĩa stride downscaling là s=LT/LS.

Theo Jiao et al. (2020), các hàm mất mát của hai
giai đoạn chưng cất sử dụng ba thành phần, (i) dựa
trên attention, (ii) dựa trên trạng thái ẩn, và (iii)
dựa trên dự đoán. Mất mát dựa trên attention được
định nghĩa như sau:

Lattn=1/LS ∑i=1^LS MSE(AS_i, AT_i·s).    (1)

Ở đây, AS_i và AT_i ∈ R^l×l đề cập đến phân phối
attention của lớp Transformer i của mô hình học
sinh và giáo viên, tương ứng; l đề cập đến độ dài
chuỗi đầu vào; MSE() biểu thị mất mát lỗi bình
phương trung bình.

Mất mát dựa trên trạng thái ẩn được định nghĩa
như sau:
Lhidden = 1/(LS+1) ∑i=0^LS MSE(HS_i, HT_i·s),    (2)

trong đó HS_i và HT_i ∈ R^l×d đề cập đến các biểu
diễn ẩn được xuất ra bởi lớp Transformer i của mô
hình học sinh và giáo viên, tương ứng, hoặc đầu ra
của lớp embedding khi i=0. Lưu ý rằng chúng tôi
giả định rằng học sinh và giáo viên chia sẻ cùng
một chiều ẩn d.

Cuối cùng, mất mát dựa trên dự đoán được định
nghĩa là
Lpred=CE(zS,zT),    (3)

trong đó zS và zT là các phân phối nhãn được dự
đoán bởi mô hình học sinh và giáo viên, tương ứng,
và CE biểu thị mất mát cross-entropy.

Trực giác đằng sau việc sử dụng mất mát dựa trên
attention và dựa trên trạng thái ẩn cho mục đích
của chúng tôi như sau. Chúng tôi (i) yêu cầu hiệu
suất đơn ngôn ngữ tốt trong ngôn ngữ nguồn và
đích, nhưng chúng tôi cũng (ii) phải bảo tồn sự
căn chỉnh hiện có giữa những ngôn ngữ này trong
MMT mà sẽ do đó tạo điều kiện cho việc chuyển
giao giữa chúng. Trực giác là việc khuyến khích
các biểu diễn trung gian của học sinh khớp với
những của giáo viên sẽ giúp bảo tồn sự căn chỉnh
này.

Chúng tôi tiếp theo mô tả cách những thành phần
mất mát này được sử dụng trong mỗi giai đoạn
của BISTILLATION.

3.2 Giai đoạn 1: Chưng cất Song ngữ Chung

Khởi tạo. Chúng tôi khởi tạo tất cả các tham số của
mô hình học sinh bằng cách sao chép những của
mô hình giáo viên, nhưng chỉ giữ lại các lớp
Transformer có chỉ số là bội số của s.

Giảm Từ vựng. Các mô hình đã chưng cất của chúng
tôi có thể loại bỏ nhiều token không liên quan trong
từ vựng của MMT cơ sở, tức là những token không
được sử dụng thường xuyên trong ngôn ngữ nguồn
hoặc đích quan tâm, một ý tưởng đã được đề xuất
trước đó bởi Abdaoui et al. (2020). Trong quá trình
khởi tạo, từ vựng của mô hình học sinh được chọn
bằng cách chỉ giữ lại các token của từ vựng giáo
viên có xác suất unigram trong corpus ngôn ngữ
nguồn hoặc đích ≥10^-6.

Thích ứng Ngôn ngữ Giáo viên. Vì chúng tôi muốn
có thể tạo ra các mô hình đã chưng cất cho các ngôn
ngữ không được bao phủ trong MMT cơ sở, và để
có được hiệu suất tốt nhất có thể cho các ngôn ngữ
được bao phủ, chúng tôi sử dụng thích ứng ngôn
ngữ của MMT giáo viên với các SFT cụ thể ngôn
ngữ (Ansell et al., 2022) được áp dụng lên trên
MMT gốc trong quá trình chưng cất. Vì nó lấy các
ví dụ từ hai ngôn ngữ, mỗi ngôn ngữ có SFT ngôn
ngữ riêng, việc chưng cất song ngữ trở thành một
trường hợp đặc biệt của huấn luyện đa nguồn như
được mô tả trong §2.2. Tại mỗi bước huấn luyện,
ngôn ngữ nguồn hoặc đích được chọn ngẫu nhiên
với xác suất bằng nhau; batch được tạo thành từ
các chuỗi được lấy từ corpus huấn luyện của ngôn
ngữ được chọn, và một SFT được huấn luyện trước
cho ngôn ngữ đó được áp dụng cho MMT giáo viên.

Mục tiêu. Hàm mất mát tổng thể cho giai đoạn này
được cho bởi tổng của mất mát dựa trên attention
và dựa trên trạng thái ẩn. Việc bỏ qua mất mát dựa
trên dự đoán ở đây có ưu điểm là tránh được nhu
cầu đánh giá phân phối của các token được dự đoán
bởi đầu MLM, điều này tốn kém do kích thước đáng
kể của ma trận embedding của MMT.

3.3 Giai đoạn 2: Chưng cất Cụ thể Tác vụ
Sau khi một mô hình song ngữ chung đã được chưng
cất từ MMT giáo viên trong Giai đoạn 1, nó có thể
được tinh chỉnh cho một tác vụ cụ thể. Đầu tiên
chúng tôi có được giáo viên cho chưng cất cụ thể
tác vụ bằng cách áp dụng LT-SFT cụ thể tác vụ để
tinh chỉnh MMT cơ sở (tức là giáo viên trong giai
đoạn chưng cất chung) cho tác vụ đang xem xét.
Các đầu ra và biểu diễn của giáo viên này sau đó
được sử dụng để tinh chỉnh mô hình học sinh song
ngữ, một lần nữa sử dụng task LT-SFT ở phía học
sinh. Việc sử dụng thích ứng tác vụ hiệu quả tham
số ở đây tránh việc thêm một số lượng lớn tham số
vào hệ thống cho mỗi tác vụ. Mục tiêu trong quá
trình tinh chỉnh cụ thể tác vụ này bao gồm tổng của
tất cả ba mất mát từ §3.1: Lattn, Lhidden, và Lpred.

4 Thiết lập Thí nghiệm
Chúng tôi phần lớn áp dụng khung đánh giá của
Ansell et al. (2022) để có thể so sánh trực tiếp với
phương pháp LT-SFT của họ, mà họ áp dụng cho
các MMT chưa chưng cất, và mà chúng tôi áp dụng
cho việc tinh chỉnh cụ thể tác vụ của các MMT đã
chưng cất song ngữ. Cụ thể, chúng tôi đánh giá
hiệu suất chuyển giao đa ngôn ngữ zero-shot trên
bốn tác vụ đại diện: phân tích cú pháp phụ thuộc,
nhận dạng thực thể có tên, suy luận ngôn ngữ tự
nhiên, và QA. Trong khi công việc trước chỉ tập
trung vào các ngôn ngữ ít tài nguyên, phương pháp
của chúng tôi cũng rất liên quan đến các ngôn ngữ
có nhiều tài nguyên: tác vụ XQuAD QA (Artetxe
et al., 2020) cung cấp thêm thông tin chi tiết về
hiệu suất ngôn ngữ đích có nhiều tài nguyên. Bảng
1 tóm tắt thiết lập thí nghiệm, bao gồm các tập dữ
liệu và ngôn ngữ được xem xét trong các thí nghiệm
của chúng tôi. Tổng cộng, chúng tôi bao phủ một
tập hợp 44 ngôn ngữ đa dạng về loại hình và địa
lý, điều này làm cho chúng có tính đại diện cho sự
biến đổi đa ngôn ngữ (Ponti et al., 2020).

Chúng tôi thí nghiệm với ba MMT khác nhau như
được hiển thị trong Bảng 1: mBERT (Devlin et al.,
2019), XLM-R base (Conneau et al., 2020), và
mDeBERTa base (He et al., 2021).

4.1 Baseline và Biến thể Mô hình
Chúng tôi gọi phương pháp chính của chúng tôi là
BISTIL. Chúng tôi so sánh nó với một số cách tiếp
cận có liên quan. Đầu tiên, phương pháp LTSFT
(Ansell et al., 2022), một cách tiếp cận chuyển giao
đa ngôn ngữ tiên tiến, sử dụng LT-SFT với thích
ứng ngôn ngữ trên MMT cơ sở. LTSFT có thể được
xem như một cận trên cho BISTIL, cho phép chúng
tôi đo được hiệu suất bị suy giảm bao nhiều do việc
thay thế MMT bằng biến thể đã chưng cất song ngữ
của nó.

Đối với mỗi tác vụ ngoại trừ NLI, chúng tôi cũng
so sánh với một MMT đã chưng cất đa ngôn ngữ,
tức là với tất cả các ngôn ngữ huấn luyện trước
được sử dụng cho chưng cất. Đối với DP và NER,
trong đó mBERT là MMT cơ sở, MMT đã chưng
cất là DISTILMBERT (Sanh et al., 2019), tương tự
dựa trên mBERT. Đối với QA, trong đó BISTIL sử
dụng mDeBERTa làm MMT cơ sở, không có MMT
đã chưng cất đa ngôn ngữ có thể so sánh trực tiếp
nào có sẵn, vì vậy chúng tôi lựa chọn so sánh lỏng
lẻo với MINILMV2 (Wang et al., 2021a), được
chưng cất từ XLM-R large, đã đạt được kết quả
mạnh mẽ trong chuyển giao đa ngôn ngữ ở các ngôn
ngữ có nhiều tài nguyên. Chúng tôi thực hiện tinh
chỉnh cụ thể tác vụ với LT-SFT trên DistilmBERT
và MiniLMv2 theo cùng cách như đối với các MMT
chưa chưng cất trong thiết lập LTSFT. Đối với DP
và NER chúng tôi cũng thực hiện thích ứng ngôn
ngữ của DistilmBERT.

Chúng tôi cũng xem xét SCRATCH, một thiết lập
trong đó chúng tôi huấn luyện các mô hình song
ngữ từ đầu thay vì chưng cất chúng từ một MMT
được huấn luyện trước. Sau đó chúng tôi áp dụng
cùng phương pháp tinh chỉnh LT-SFT như đối với
các baseline khác. So sánh này cho phép chúng tôi
đánh giá lợi ích của việc chưng cất các mô hình
song ngữ hiệu quả từ MMT thay vì huấn luyện trước
các mô hình song ngữ cùng kích thước từ đầu.

Chúng tôi gọi phương pháp chính của chúng tôi,
với giai đoạn chưng cất cụ thể tác vụ như được mô
tả trong §3.3, là BISTIL-TF (TF = teacher forcing).
Chúng tôi cũng thực hiện một ablation tập trung
vào giai đoạn thứ hai của BISTILLATION: ở đây,
chúng tôi xem xét việc thực hiện tinh chỉnh cụ thể
tác vụ mà không có sự hỗ trợ của giáo viên, tức là
theo cùng cách như LTSFT. Chúng tôi gọi biến thể
này là BISTIL-ST (ST = self-taught).

Bảng 2 cung cấp chi tiết về kích thước mô hình,
trước và sau chưng cất sử dụng các phương pháp
trên, chứng minh lợi ích của BISTILLATION đối
với tính compact của mô hình.

4.2 Thiết lập Huấn luyện Chưng cất/Thích ứng
Chúng tôi luôn thực hiện thích ứng ngôn ngữ của
mô hình giáo viên trong cả hai giai đoạn của
BISTILLATION và trong LTSFT ngoại trừ đối với
mDeBERTa và MiniLMv2. Đối với thích ứng ngôn
ngữ của MMT chúng tôi sử dụng các language SFT
được huấn luyện trước của Ansell et al. (2022), và
chúng tôi huấn luyện của riêng chúng tôi cho
DistilmBERT. Tương tự, đối với baseline LTSFT,
và cho thích ứng tác vụ của giáo viên trong cấu
hình BISTIL-TF, chúng tôi sử dụng các task SFT
nguồn đơn được huấn luyện trước của họ hoặc
huấn luyện của riêng chúng tôi khi cần thiết. Khi
huấn luyện/chưng cất các mô hình hoặc SFT của
riêng chúng tôi, chúng tôi thường chọn siêu tham
số khớp với những được sử dụng để huấn luyện
SFT của họ trong công việc gốc. Xem Phụ lục A
cho chi tiết huấn luyện đầy đủ và siêu tham số của
tất cả các mô hình trong so sánh của chúng tôi, và
Phụ lục B cho chi tiết của các corpus huấn luyện.

Chúng tôi thí nghiệm với hai hệ số giảm lớp (LRF)
cho BISTILLATION, 2 (giảm từ 12 xuống 6 lớp)
và 3 (12 xuống 4 lớp). Trong khi thiết lập BISTIL
khởi tạo mô hình từ giáo viên (xem §3.2), thiết lập
SCRATCH khởi tạo nó ngẫu nhiên.

5 Kết quả và Thảo luận
Kết quả về hiệu suất tác vụ được tóm tắt trong Bảng
3-6. Như mong đợi, LTSFT trên các MMT chưa
chưng cất thực hiện tốt nhất trên tất cả các tác vụ.
Tuy nhiên, BISTIL-TF với hệ số giảm 2 không tệ
hơn nhiều, với sự suy giảm hiệu suất không vượt
quá 1,3 điểm so với LTSFT trên DP, NER và NLI.
Khoảng cách lớn hơn 3,4 điểm EM trên QA có thể
là kết quả của việc MMT cơ sở được huấn luyện
trước kỹ lưỡng hơn nhiều trên các ngôn ngữ có
nhiều tài nguyên được tìm thấy trong XQuAD so
với các ngôn ngữ ít tài nguyên hơn được tìm thấy
trong các tập dữ liệu cho các tác vụ khác. Do đó
việc BIDISTIL đạt được độ sâu kiến thức của MMT
cơ sở về ngôn ngữ đích trong thời gian huấn luyện
chưng cất tương đối ngắn của nó là khó hơn.
BISTIL-TF, LRF=2 tuy nhiên vẫn vượt trội hơn
MiniLMv2 trên QA với 1,7 điểm EM, mặc dù
MiniLMv2 nhận được gấp 320 lần nhiều huấn
luyện hơn mỗi mô hình BIDISTIL, hoặc khoảng
6 lần nhiều hơn mỗi ngôn ngữ.

Hơn nữa, BISTIL-TF, LRF=2 vượt trội đáng kể
hơn DISTILMBERT, với khoảng cách 6,1 LAS trên
DP và 2,9 F1 trên NER. BISTIL, LRF=2 tạo ra các
mô hình khoảng một nửa kích thước của
DISTILMBERT và một lần nữa, được huấn luyện
ít thời gian hơn rất nhiều.

Huấn luyện các mô hình song ngữ từ SCRATCH
thực hiện kém, tụt lại phía sau các phương pháp
khác hơn 20 điểm trên DP. Một điểm yếu quan
trọng của SCRATCH, bên cạnh hiệu suất đơn ngôn
ngữ giảm, là thiếu sự căn chỉnh giữa các biểu diễn
của ngôn ngữ nguồn và đích, làm suy yếu nghiêm
trọng việc chuyển giao đa ngôn ngữ. Điều này nổi
bật ưu điểm của việc chưng cất một mô hình song
ngữ từ một MMT trong đó sự căn chỉnh đa ngôn
ngữ đã có sẵn.

Thú vị, khi chúng tôi đánh giá các mô hình
SCRATCH trên hiệu suất DP tiếng Anh của chúng,
chúng tôi có được điểm UAS/LAS trung bình là
81,8/77,1, cạnh tranh hơn nhiều về mặt tương đối
với điểm DP tiếng Anh BISTIL-TF, LRF=2 là
91,0/88,2 so với so sánh tương ứng trong điểm DP
ngôn ngữ đích trung bình là 29,9/11,0 với 55,5/36,5.
Điều này gợi ý rằng một yếu tố thậm chí lớn hơn
trong điểm yếu của SCRATCH so với hiệu suất đơn
ngôn ngữ kém của nó là thiếu sự căn chỉnh giữa
các biểu diễn của ngôn ngữ nguồn và đích, làm
suy yếu nghiêm trọng việc chuyển giao đa ngôn
ngữ. Điều này nổi bật ưu điểm của việc chưng cất
một mô hình song ngữ từ một MMT trong đó sự
căn chỉnh đa ngôn ngữ đã có sẵn.

Như mong đợi, hiệu suất của BISTIL hơi yếu hơn
với hệ số giảm lớp lớn hơn là 3, mặc dù điều này
phụ thuộc nhiều vào tác vụ. Với LRF là 3, BISTIL-TF
vẫn vượt trội thoải mái hơn DISTILMBERT trên
DP và NER, và không tụt lại nhiều so với LRF=2
cho NLI. Tuy nhiên, chúng tôi quan sát thấy sự
suy giảm đáng kể trong hiệu suất đối với LRF=3
cho QA; điều này có thể chỉ ra rằng một Transformer
4 lớp gặp khó khăn để thích ứng với tác vụ cụ thể
này, hoặc với kiến trúc này thời gian huấn luyện
khiêm tốn không đủ để tiến gần đến sự hiểu biết
của MMT cơ sở về ngôn ngữ nguồn và đích.

Bảng 7 trình bày phân tích về hiệu quả thời gian
suy luận. Chúng tôi đo tốc độ suy luận cả trên CPU
với kích thước batch 1 và GPU với cùng kích thước
batch như trong quá trình huấn luyện cụ thể tác vụ.
Chúng tôi cũng tính toán số lượng phép toán dấu
phẩy động (FLOP) trên mỗi ví dụ sử dụng fvcore,
được đo trong quá trình chạy suy luận trên tập test
của ngôn ngữ đầu tiên trong mỗi tác vụ.

Đối với NER, NLI và QA, kết quả hiệu quả phù
hợp khá gần với kỳ vọng trực quan rằng thời gian
suy luận của một mô hình nên tỷ lệ tuyến tính với
số lớp của nó; tức là BIDISTIL với LRF=2 thường
nhanh hơn khoảng hai lần so với MMT cơ sở. Đối
với DP, chúng tôi quan sát được việc mở rộng có
vẻ dưới tuyến tính được gây ra bởi đầu phân tích
biaffine rất lớn, bao gồm ~23M tham số. Chi phí
đáng kể của việc áp dụng đầu mô hình đóng góp
như nhau cho tất cả các mô hình bất kể mức độ
chưng cất của chúng. Mặc dù có LRF khiêm tốn là
2, MINILMV2 thể hiện tốc độ ấn tượng do việc nó
bổ sung có chiều ẩn nhỏ hơn so với giáo viên của
nó (xem Bảng 2), một kỹ thuật mà chúng tôi không
xem xét cho BIDISTIL, nhưng có thể là một hướng
đầy hứa hẹn cho công việc tương lai.

Chúng tôi lập luận rằng BIDISTIL hoàn thành mục
tiêu của nó bằng cách đạt được việc giảm hai đến
ba lần trong thời gian suy luận và kích thước mô
hình mà không hy sinh nhiều về hiệu suất thô. Hiệu
suất vượt trội của nó so với các mô hình đã chưng
cất đa ngôn ngữ mặc dù ngân sách huấn luyện rất
khiêm tốn của nó hỗ trợ khẳng định rằng việc chuyên
môn hóa các mô hình đa ngôn ngữ cho một cặp
chuyển giao cụ thể trong quá trình chưng cất giúp
tránh sự suy giảm hiệu suất do lời nguyền của tính
đa ngôn ngữ.

6 Công việc Liên quan
Một dòng công việc trước tập trung vào thích ứng
hiệu quả tham số của các MMT được huấn luyện
trước, tức là thích ứng bằng cách thêm/sửa đổi một
tập con nhỏ tham số. Adapter (Rebuffi et al., 2017;
Houlsby et al., 2019) đã được sử dụng rộng rãi cho
mục đích này (Üstün et al., 2020), với khung MAD-X
của Pfeiffer et al. (2020) trở thành điểm khởi đầu
cho một số phát triển tiếp theo (Vidoni et al., 2020;
Wang et al., 2021b; Parović et al., 2022), trong đó
một chủ đề đáng chú ý là thích ứng MMT với các
ngôn ngữ chưa được nhìn thấy (Ansell et al., 2021;
Pfeiffer et al., 2021). Ansell et al. (2022) đề xuất
các tinh chỉnh thưa thớt có thể kết hợp như một
thay thế cho adapter.

Pfeiffer et al. (2022) tạo ra một MMT mô-đun từ
đầu, trong đó một số tham số được chia sẻ giữa tất
cả các ngôn ngữ và những tham số khác là cụ thể
ngôn ngữ. Điều này cho phép mô hình dành khả
năng đáng kể cho mỗi ngôn ngữ mà không làm cho
mỗi mô hình cụ thể ngôn ngữ trở nên quá lớn; do
đó nó khá tương tự về mục tiêu với công việc này.

Nhiều cách tiếp cận đã được đề xuất cho việc
chưng cất chung các mô hình ngôn ngữ được huấn
luyện trước. Hình thức đơn giản nhất chỉ sử dụng
các xác suất đích mềm được dự đoán bởi mô hình
giáo viên làm tín hiệu huấn luyện cho học sinh
(Sanh et al., 2019). Các cách tiếp cận khác cố gắng
căn chỉnh các trạng thái ẩn và phân phối self-attention
của học sinh và giáo viên (Sun et al., 2020; Jiao
et al., 2020) và/hoặc các khía cạnh chi tiết hơn của
cơ chế self-attention (Wang et al., 2020, 2021a).
Mukherjee et al. (2021) khởi tạo ma trận embedding
của học sinh với một phân tích nhân tử của giáo
viên để có hiệu suất tốt hơn khi các chiều ẩn của
chúng khác nhau. Trong số này, Sanh et al. (2019);
Wang et al. (2020, 2021a); Mukherjee et al. (2021)
áp dụng phương pháp của họ để tạo ra các phiên
bản đã chưng cất của MMT.

Parović et al. (2022) thích ứng các MMT được
huấn luyện trước với các cặp chuyển giao cụ thể
với adapter; cách tiếp cận này tương tự với của
chúng tôi về tinh thần, nhưng nó nhắm đến cải thiện
hiệu suất thay vì hiệu quả. Minixhofer et al. (2022)
học cách chuyển giao các mô hình đơn ngôn ngữ
đầy đủ qua các ngôn ngữ. Công việc trước duy nhất
mà chúng tôi biết tạo ra các mô hình hoàn toàn song
ngữ cho chuyển giao đa ngôn ngữ là của Tran
(2020). Cách tiếp cận này bắt đầu với một mô hình
ngôn ngữ nguồn đơn ngôn ngữ được huấn luyện
trước, khởi tạo các embedding ngôn ngữ đích thông
qua một quy trình căn chỉnh, và sau đó tiếp tục
huấn luyện mô hình với các embedding đích được
thêm vào trên cả hai ngôn ngữ.

7 Kết luận
Mặc dù MMT là một công cụ hiệu quả cho chuyển
giao đa ngôn ngữ, phạm vi bao phủ ngôn ngữ rộng
của chúng khiến chúng trở nên đắt đỏ không cần
thiết để triển khai trong tình huống thường gặp
trong đó khả năng được yêu cầu chỉ trong một ngôn
ngữ duy nhất, thường là ít tài nguyên. Chúng tôi
đã đề xuất BISTILLATION, một phương pháp huấn
luyện các mô hình hiệu quả hơn phù hợp với tình
huống này hoạt động bằng cách chưng cất một
MMT chỉ sử dụng cặp ngôn ngữ nguồn-đích quan
tâm. Chúng tôi chỉ ra rằng cách tiếp cận này tạo ra
các mô hình cung cấp sự cân bằng tuyệt vời giữa
hiệu suất ngôn ngữ đích, hiệu quả và tính compact
của mô hình. Các mô hình 'bistilled' thể hiện chỉ
một sự giảm nhẹ trong hiệu suất so với MMT cơ
sở của chúng trong khi đạt được sự giảm đáng kể
trong cả kích thước mô hình và thời gian suy luận.
Kết quả của chúng cũng so sánh thuận lợi với những
của các MMT đã chưng cất đa ngôn ngữ mặc dù
nhận được ít huấn luyện hơn đáng kể thậm chí trên
cơ sở từng ngôn ngữ.

Hạn chế
Mặc dù kết quả của các thí nghiệm của chúng tôi
có vẻ đủ để xác thực khái niệm và cách tiếp cận
chung của chúng tôi đối với chưng cất song ngữ,
chúng tôi đã không thực hiện phân tích hệ thống
chi tiết về các triển khai thay thế của các khía cạnh
khác nhau trong phương pháp của chúng tôi, như
các khởi tạo mô hình học sinh khác nhau, mục tiêu
chưng cất và thiết lập siêu tham số. Hơn nữa, các
mô hình BISTIL của chúng tôi có thể bị huấn luyện
dưới mức do tài nguyên tính toán hạn chế. Do đó,
chúng tôi không tuyên bố triển khai cụ thể của
chưng cất song ngữ của chúng tôi là tối ưu hoặc
thậm chí gần tối ưu. Các lĩnh vực cần điều tra thêm
hướng tới việc nhận ra tiềm năng đầy đủ của cách
tiếp cận này bao gồm việc sử dụng giảm chiều ẩn,
mang lại lợi ích tốc độ ấn tượng cho MiniLMv2
trong các thí nghiệm của chúng tôi, và các đổi mới
khác trong chưng cất như chuyển giao kiến thức
tiến bộ (Mukherjee et al., 2021).

Ngoại trừ hiệu quả được cải thiện, các mô hình
BISTIL của chúng tôi kế thừa các hạn chế của các
MMT mà chúng được chưng cất từ đó; đáng chú ý,
có sự khác biệt giữa hiệu suất trên các ngôn ngữ
có nhiều và ít tài nguyên do phân phối dữ liệu được
sử dụng trong quá trình huấn luyện trước MMT.

Trong công việc này, chúng tôi chỉ xem xét tiếng
Anh làm ngôn ngữ nguồn; một số ngôn ngữ đích
có thể hưởng lợi từ các nguồn chuyển giao khác.
Công việc tương lai cũng có thể xem xét việc sử
dụng chuyển giao đa nguồn, điều này sẽ đòi hỏi
chưng cất với hơn hai ngôn ngữ. Ở đây thách thức
sẽ là tối ưu hóa sự cân bằng của khả năng mô hình
được phân bổ cho các ngôn ngữ nguồn so với ngôn
ngữ đích.

Lời cảm ơn
Alan muốn cảm ơn David và Claudia Harding vì
sự hỗ trợ hào phóng của họ thông qua Chương trình
Học bổng Sau đại học Xuất sắc Harding. Ivan Vulić
được hỗ trợ bởi một Royal Society University
Research Fellowship cá nhân 'Công nghệ Ngôn
ngữ Bao trùm và Bền vững cho một Thế giới Thực
sự Đa ngôn ngữ' (số 221137; 2022–).
