# 2309.10706.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2309.10706.pdf
# Kích thước tệp: 1970605 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Cập nhật: Chúng tôi đã thực hiện tinh chỉnh tham số đầy đủ với các tập dữ liệu chuyên biệt, cho phép OpenBA trở thành mô hình chuyên gia (OpenBA-X) cho các tác vụ hạ nguồn (Đối thoại Đa vòng Song ngữ, Sinh mã, Sinh hướng dẫn, và Truy xuất Công cụ).

OpenBA: Mô hình Seq2Seq Bất đối xứng Song ngữ 15B 
Mã nguồn mở được Pre-train từ đầu

Juntao Li∗, Zecheng Tang†, Yuyang Ding†, Pinzheng Wang†
Pei Guo, Wangjie You, Dan Qiao, Wenliang Chen, Guohong Fu, Qiaoming Zhu,
Guodong Zhou‡, Min Zhang‡
Đại học Soochow

Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) với hàng tỷ tham số đã thể hiện hiệu suất xuất sắc trên các tác vụ xử lý ngôn ngữ tự nhiên khác nhau. Báo cáo này trình bày OpenBA, một mô hình seq2seq bất đối xứng song ngữ 15B mã nguồn mở, để đóng góp một biến thể LLM cho cộng đồng mô hình mã nguồn mở định hướng tiếng Trung. Chúng tôi nâng cao OpenBA với các kỹ thuật hiệu quả và hiệu suất cũng như áp dụng chiến lược huấn luyện ba giai đoạn để huấn luyện mô hình từ đầu. Giải pháp của chúng tôi cũng có thể đạt được hiệu suất rất cạnh tranh chỉ với 380B token, tốt hơn so với LLaMA-70B trên benchmark BELEBELE, BLOOM-176B trên benchmark MMLU, GLM-130B trên benchmark C-Eval (khó). Báo cáo này cung cấp các chi tiết chính để pre-train một mô hình tương tự, bao gồm xử lý dữ liệu pre-training, thu thập dữ liệu Bilingual Flan, các quan sát thực nghiệm truyền cảm hứng cho thiết kế kiến trúc mô hình, mục tiêu huấn luyện của các giai đoạn khác nhau và các kỹ thuật nâng cao khác. Ngoài ra, chúng tôi cũng cung cấp chi tiết tinh chỉnh OpenBA trên bốn tác vụ hạ nguồn. Chúng tôi đã tái cấu trúc mã của mình để tuân theo các nguyên tắc thiết kế của Thư viện Huggingface Transformers, làm cho nó thuận tiện hơn cho các nhà phát triển sử dụng, và phát hành các checkpoint của các giai đoạn huấn luyện khác nhau tại https://huggingface.co/openBA. Thêm chi tiết về dự án của chúng tôi có tại https://github.com/OpenNLG/openBA.git.

∗Trưởng dự án. ljt@suda.edu.cn
†Đóng góp bằng nhau. {zctang,yyding23,pzwang1}@stu.suda.edu.cn
‡Tác giả liên hệ: {gdzhou,minzhang}@suda.edu.cn

Báo cáo Kỹ thuật arXiv:2309.10706v2 [cs.CL] 1 Oct 2023

--- TRANG 2 ---
Mục lục

1 Giới thiệu 3
2 Công trình liên quan 3
2.1 Mô hình Ngôn ngữ Lớn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 Phương pháp 5
3.1 Thu thập Tập dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.1.1 Thu thập và Lọc Dữ liệu Pre-training . . . . . . . . . . . . . . . . . . . . . 5
3.1.2 Thu thập Dữ liệu Bilingual Flan . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Kiến trúc Mô hình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Quy trình Huấn luyện và Tác vụ Mô hình hóa Ngôn ngữ . . . . . . . . . . . . . . . . 7
3.4 Triển khai Mô hình và Kỹ thuật . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4 Kết quả 10
4.1 Cài đặt Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2 Phân tích Chi phí Huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.3 Hiểu Ngôn ngữ Tự nhiên . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.4 Sinh Ngôn ngữ Tự nhiên . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.5 Lý luận Thông thường . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5 Phân tích 15
5.1 Lựa chọn Kiến trúc Mô hình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5.2 Tiến triển Hiệu suất trong Quá trình Huấn luyện . . . . . . . . . . . . . . . . . . . . 17
6 OpenBA-X: Thích ứng Tác vụ Hạ nguồn 17
6.1 OpenBA-Chat: Đối thoại Đa vòng Song ngữ . . . . . . . . . . . . . . . . . . . . . . 18
6.2 OpenBA-Code: Sinh mã . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.3 OpenBA-InstructGen: Sinh hướng dẫn . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.4 OpenBA-Tool: Truy xuất Công cụ . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
7 Kết luận và Công việc Tương lai 20
A Mẫu Hướng dẫn 29
B Thu thập Flan Tiếng Trung 34

--- TRANG 3 ---
1 Giới thiệu

Định luật tỉ lệ (Kaplan et al., 2020; Clark et al., 2022; Hoffmann et al., 2022; Touvron et al., 2023a) của các mô hình ngôn ngữ đã mang lại thành công chưa từng có. Những mô hình ngôn ngữ lớn này được pre-train trên dữ liệu văn bản khổng lồ thể hiện sự vượt trội to lớn so với các mô hình trước đây đối với nhiều lĩnh vực khác nhau và thậm chí đã có được những khả năng nổi lên mới. Mặc dù rất mạnh mẽ và phát triển nhanh chóng, những mô hình ở quy mô lớn này vẫn còn xa mới hoàn hảo hoặc thỏa mãn cho hầu hết các ứng dụng thực tế. Để thúc đẩy sự phát triển của LLM, cộng đồng mã nguồn mở đã nỗ lực lớn để cung cấp các LLM mạnh mẽ và công khai, bao gồm các nguồn dữ liệu, kiến trúc, mục tiêu mô hình hóa ngôn ngữ, pipeline huấn luyện, quy mô mô hình và ngôn ngữ chuyên môn khác nhau, ví dụ: BLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023a,b), FlanT5 (Chung et al., 2022), AlexaTM (Soltan et al., 2022).

Đối với tiếng Trung, cộng đồng mã nguồn mở cũng đã phát hành nhiều mô hình ngôn ngữ lớn hoặc bằng cách pre-train từ đầu, ví dụ: GLM (Zeng et al., 2022), Baichuan (Inc., 2023) hoặc thực hiện tinh chỉnh thêm trên các mô hình đa ngôn ngữ mã nguồn mở hiện có, ví dụ: Huatuo (Wang et al., 2023), Luotuo (Ziang Leng & Li, 2023), Phoenix (Chen et al., 2023), Chinese-LLaMA (Cui et al., 2023), MOSS (Sun et al., 2023). Những LLM công khai này cung cấp cho các nhà nghiên cứu và nhà phát triển các mô hình ngôn ngữ tổng quát mạnh mẽ (tức là framework được sử dụng bởi GLM (Du et al., 2022)) và các biến thể chỉ-decoder khác nhau, nhưng để lại framework Encoder-Decoder (ví dụ: Flan-T5 (Chung et al., 2022)) chưa được khám phá đầy đủ, điều này đã được chứng minh là hiệu quả toàn cầu đối với các thiết lập prompt khác nhau (zero-shot, few-shot, và chain-of-thought) (Longpre et al., 2023) và các tác vụ khác nhau (ví dụ: hiểu ngôn ngữ, lý luận thông thường, hỏi đáp, truy xuất thông tin và cuộc trò chuyện đa vòng) (Tay et al., 2022; Zheng et al., 2023).

Để lấp đầy khoảng trống này, chúng tôi đóng góp một mô hình seq2seq bất đối xứng song ngữ 15B mã nguồn mở (OpenBA) được pre-train từ đầu, cung cấp không chỉ các checkpoint mô hình mà còn cả chi tiết thu thập và xử lý dữ liệu để xây dựng dữ liệu pre-training và bộ sưu tập Bilingual Flan từ các tài nguyên dữ liệu công khai (ví dụ: Common Crawl, corpus Pile, C-Book), động lực và quan sát thực nghiệm cho thiết kế kiến trúc mô hình, và chi tiết chính của các chiến lược nâng cao khác. Cụ thể, dữ liệu pre-training thu thập của chúng tôi bao gồm các token tiếng Anh và tiếng Trung cân bằng để làm cho mô hình hóa ngôn ngữ tiếng Trung hưởng lợi từ dữ liệu tiếng Anh chất lượng cao. Vì khó khăn trong việc xây dựng một bộ sưu tập giống Flan tiếng Trung bao gồm các tác vụ và thiết lập mở rộng từ các tài nguyên mở, chúng tôi kết hợp thêm dữ liệu tiếng Anh được lấy mẫu từ bộ sưu tập Flan trong corpus Bilingual-Flan của chúng tôi. Khác với Flan-T5 vanilla (Chung et al., 2022) có cấu trúc encoder-decoder cân bằng và bất đối xứng deep-encoder shallow-decoder trong AlexaTM (Soltan et al., 2022), chúng tôi sử dụng một cấu trúc mô hình bất đối xứng khác, tức là shallow-encoder deep-decoder để nâng cao khả năng sinh, được thúc đẩy bởi nghiên cứu thực nghiệm của chúng tôi trong Mục 5.1. Quy trình huấn luyện của chúng tôi bao gồm ba giai đoạn khác nhau, bao gồm pre-training UL2, length-adaptation, và training Flan. Chúng tôi cũng kết hợp các chiến lược nâng cao trong kiến trúc mô hình và huấn luyện để cải thiện khả năng mô hình, tính ổn định huấn luyện và hiệu quả. Đánh giá trên các benchmark khác nhau (MMLU, CMMLU, C-Eval, SuperGLUE, BELEBELE, BBH) và các tác vụ (ví dụ: hiểu, lý luận và sinh) đã chứng minh tính hiệu quả của mô hình của chúng tôi trong các thiết lập khác nhau (zero-shot, few-shot, held-in, và held-out). Mặc dù chỉ được huấn luyện trên 380B token, mô hình của chúng tôi có thể vượt qua nhiều mô hình đại diện, ví dụ: LLaMA-70B trên BELEBELE, BLOOM-176B trên MMLU, ChatGLM-6B trên CMMLU và C-Eval. Trong toàn bộ quá trình huấn luyện, OpenBA-15B tạo ra khoảng 6.5tCO₂eq tổng cộng, ít hơn nhiều so với mô hình LLaMA-7B tốn 14tCO₂eq.

Ngoài ra, chúng tôi tinh chỉnh thêm mô hình trên bốn tác vụ hạ nguồn, bao gồm đối thoại đa vòng song ngữ, sinh mã, sinh hướng dẫn và truy xuất công cụ, cho phép OpenBA trở thành mô hình chuyên gia (OpenBA-X) cho các tác vụ hạ nguồn. Tất cả chi tiết triển khai đều có thể truy cập công khai, không giới hạn ở thu thập và xử lý dữ liệu, mã, checkpoint mô hình và đánh giá. Vì chúng tôi vẫn đang làm việc trên một vài hướng để cải thiện và áp dụng mô hình OpenBA, chúng tôi hoan nghênh mọi nhận xét và đề xuất và mong đợi hợp tác thêm với cộng đồng mã nguồn mở.

2 Công trình liên quan

2.1 Mô hình Ngôn ngữ Lớn

Được hưởng lợi từ định luật tỉ lệ (Kaplan et al., 2020; Clark et al., 2022; Hoffmann et al., 2022) và sự tăng trưởng của tài nguyên tính toán (Schaller, 1997), những năm gần đây đã chứng kiến sự phát triển đáng kể trong lĩnh vực LLM, điều này đẩy ranh giới của các tác vụ NLP khác nhau (Radford et al., 2018; Brown et al., 2020; Zeng et al., 2021; Sun et al., 2021; Zhang & Li, 2021; Zhang et al., 2021, 2022; Touvron et al., 2023a). Việc giới thiệu mô hình transformer (Vaswani et al., 2017) là một điểm chuyển đổi đáng chú ý trong lĩnh vực NLP, vì các mô hình dựa trên kiến trúc này như GPT-2 (Radford et al., 2019) và T5 (Raffel et al., 2020) đã thể hiện hiệu suất xuất sắc trên nhiều tác vụ NLP bằng cách thống nhất việc hình thành các tác vụ khác nhau và mở rộng quy mô mô hình.

--- TRANG 4 ---

Xu hướng này đã tiếp tục với sự xuất hiện của GPT-3 (Brown et al., 2020), mang lại những tiến bộ đột phá trong việc mở rộng quy mô với mô hình 175 tỷ tham số. Do đó, cộng đồng nghiên cứu đã dần nhận ra lợi ích của LLM, dẫn đến một loạt các mô hình tiếp theo nối tiếp nhau một cách nhanh chóng, chẳng hạn như Gopher (Rae et al., 2021), Megatron-Turing (Smith et al., 2022), Chinchilla (Hoffmann et al., 2022), BLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023a,b), ChatGPT (Ouyang et al., 2022; Bubeck et al., 2023), Falcon (Penedo et al., 2023a), v.v. Những mô hình này đã liên tục thúc đẩy ranh giới trong lĩnh vực NLP, thúc đẩy sự phát triển và tiến bộ liên tục. Tuy nhiên, trong quá trình này, hai vấn đề nghiêm trọng đã dần nổi lên. Vấn đề đầu tiên là việc mở nguồn LLM. Do lo ngại về các nguồn dữ liệu và quyền riêng tư (Pan et al., 2020), nhiều LLM không có sẵn cho công chúng hoặc chỉ có thể truy cập qua các API hạn chế hoặc thương mại, ví dụ: ChatGPT (Ouyang et al., 2022) và PaLM (Chowdhery et al., 2022), trong khi các lựa chọn thay thế mã nguồn mở có hiệu suất tương đối thấp hơn so với các đối tác nguồn đóng (Hendrycks et al., 2020; Li et al., 2023a). Vấn đề khác là, theo sau thành công của các mô hình chỉ-decoder như GPT-3 và ChatGPT, trọng tâm nghiên cứu hiện tại chủ yếu xoay quanh kiến trúc chỉ-decoder, trong khi việc điều tra về các mô hình encoder-decoder quy mô lớn, chẳng hạn như T5 (Raffel et al., 2020) và AlexaTM (Soltan et al., 2022), thể hiện một "khu vực chưa được khám phá" tương đối trong lĩnh vực này. Ngoài ra, không có sự đồng thuận rõ ràng về việc liệu các mô hình encoder-decoder hay chỉ-decoder có lợi thế so với nhau về mặt ưu việt kiến trúc (Tay et al., 2022; Fu et al., 2023). Trong nỗ lực đóng góp cho cộng đồng mã nguồn mở và bổ sung các mô hình encoder-decoder hiện có, chúng tôi đã phát triển OpenBA, có kiến trúc encoder-decoder bất đối xứng. Chúng tôi thu thập và lọc dữ liệu pre-training từ các corpus công khai. Ngoài ra, chúng tôi xây dựng thủ công dữ liệu giống Flan tiếng Trung từ các tập dữ liệu chú thích công khai khác nhau và kết hợp chúng với corpus Flan tiếng Anh để có được bộ sưu tập Bilingual Flan. Chúng tôi sử dụng chiến lược huấn luyện theo từng giai đoạn để tối ưu hiệu suất mô hình bằng cách sử dụng các tập dữ liệu này. Mô hình của chúng tôi đạt được hiệu suất xuất sắc trên nhiều benchmark được sử dụng rộng rãi, chẳng hạn như SuperGLUE (Wang et al., 2019) và C-Eval (Huang et al., 2023).

2.2 Instruction Tuning

Instruction tuning, liên quan đến phương pháp tinh chỉnh LLM trên tập dữ liệu hướng dẫn theo cách có giám sát, đã đóng vai trò quan trọng trong những tiến bộ đáng kể của LLM trong những năm gần đây (Zhang et al., 2023b). Bắt đầu từ mô hình T5 (Raffel et al., 2020), tiên phong trong khái niệm hợp nhất các tác vụ NLP đa dạng thành các tác vụ sinh. Bằng cách sử dụng các prompt cụ thể cho tác vụ để hướng dẫn mô hình, phương pháp này hợp lý hóa quy trình áp dụng LLM cho một mảng ứng dụng rộng lớn, đặt nền móng cho các mô hình instruction tuning tiếp theo như FLAN (Wei et al., 2021; Chung et al., 2022) và T0 (Sanh et al., 2021), cải thiện thêm hiệu suất trên các tác vụ đa dạng bằng cách kết hợp thêm hướng dẫn cụ thể cho tác vụ trong giai đoạn pre-training. Một cách tiếp cận liên quan đến instruction tuning là chain-of-thought (CoT) prompting (Nye et al., 2021; Wei et al., 2022), nâng cao hướng dẫn với mô tả các bước lý luận trung gian, từ đó thúc đẩy hiệu suất LLM (Wang et al., 2022; Zelikman et al., 2022; Wu et al., 2023b; Xu et al., 2023). Hiện tại, cộng đồng mã nguồn mở cung cấp vô số tập dữ liệu hướng dẫn, chẳng hạn như Alpaca (Taori et al., 2023) và Dolly (Conover et al., 2023a). Những hướng dẫn này nhằm nâng cao khả năng chuyên môn cụ thể của LLM, chẳng hạn như khả năng sinh mã (Chaudhary, 2023), hoặc các khả năng tổng quát như kỹ năng lý luận thông thường (Zhang et al., 2023c). Tuy nhiên, sự đa dạng rộng lớn và chất lượng không nhất quán của các tập dữ liệu này đặt ra thách thức, với mỗi tập dữ liệu thường bao gồm một lượng dữ liệu tương đối nhỏ và tập trung vào một ngôn ngữ duy nhất. Trong công trình này, chúng tôi xây dựng tập dữ liệu BiFlan, tập dữ liệu Bilingual Flan đầu tiên được xây dựng dựa trên dữ liệu Flan đã làm sạch (Longpre et al., 2023), chứa các loại hướng dẫn và tác vụ khác nhau bằng tiếng Anh và tiếng Trung. Kết quả thực nghiệm chỉ ra rằng huấn luyện trên tập dữ liệu BiFlan có thể nâng cao đáng kể hiệu suất mô hình trên các benchmark mạnh khác nhau, chẳng hạn như MMLU (Hendrycks et al., 2020) và CMMLU (Li et al., 2023a).

--- TRANG 5 ---

3 Phương pháp

Phần này trình bày chi tiết về mô hình OpenBA của chúng tôi, bao gồm các cân nhắc và triển khai của chúng tôi trong thu thập và xử lý dữ liệu pre-training, xây dựng dữ liệu Bilingual Flan, kiến trúc mô hình, mục tiêu và pipeline huấn luyện, cũng như triển khai mô hình và kỹ thuật.

3.1 Thu thập Tập dữ liệu

Phần này trình bày chi tiết về quy trình thu thập và lọc dữ liệu cho mỗi giai đoạn huấn luyện: dữ liệu pre-training cho giai đoạn I và II (Mục 3.1.1), và dữ liệu Bilingual Flan (BiFlan) cho giai đoạn III (Mục 3.1.2).

3.1.1 Thu thập và Lọc Dữ liệu Pre-training

Nguồn Dữ liệu Xét rằng mục tiêu chính của chúng tôi là xây dựng một mô hình mã nguồn mở, chúng tôi thu thập dữ liệu pre-training từ các tài nguyên công khai gồm dữ liệu tiếng Anh, tiếng Trung và mã. Cụ thể, dữ liệu tiếng Anh và mã được lấy mẫu từ corpus Pile (Gao et al., 2020)⁴, là một tập hợp 22 tập con chất lượng cao đa dạng. Dữ liệu tiếng Trung chủ yếu được thu thập từ Internet (tức là một tập con đã làm sạch từ Common Crawl⁵), sách tiếng Trung (tức là C-Book⁶), Tin tức (tức là news2016zh⁷), Bách khoa toàn thư (tức là baidu_baike⁸ và wiki2019zh_corpus⁷), Bình luận (tức là comments2019zh_corpus⁷) và Luật (tức là CAIL2018⁹).

Lọc Trước khi trộn các thành phần dữ liệu khác nhau này với một tỷ lệ nhất định, chúng tôi lọc chúng với cả chiến lược bảo vệ quyền riêng tư cá nhân và kiểm tra chất lượng¹⁰, theo (Sun et al., 2021). Các chiến lược lọc được thiết kế của chúng tôi bao gồm:

• Lọc Quyền riêng tư: Chúng tôi loại bỏ tất cả số điện thoại, địa chỉ email và liên kết web từ corpus để ngăn chặn tiềm ẩn vi phạm quyền riêng tư.

• Khử trùng: Cơ bản, chúng tôi thu thập dữ liệu từ các tập dữ liệu mã nguồn mở khác nhau với các nguồn khác biệt. Do đó, chúng tôi chủ yếu thực hiện khử trùng ở cấp độ tài liệu, ký tự và đoạn văn theo thứ tự. Chúng tôi coi mỗi mẫu như một tài liệu và sử dụng thuật toán băm để loại bỏ các tài liệu dư thừa, tức là chỉ giữ lại các tài liệu duy nhất. Tương tự, chúng tôi cũng tận dụng thuật toán băm với một bộ phân đoạn câu bổ sung ở cấp độ đoạn văn để xác định và loại bỏ các câu hoặc đoạn văn lặp lại (chúng tôi coi 1-99 câu liên tiếp như một đoạn văn). Ở cấp độ ký tự, chúng tôi xóa các ký tự dư thừa và giảm chuỗi các ký tự lặp lại thành một thể hiện duy nhất.

• Lọc Ngôn ngữ: Chúng tôi sử dụng polyglot¹¹ để phát hiện ngôn ngữ của văn bản và chỉ giữ lại những văn bản có độ tin cậy cao bằng tiếng Trung hoặc tiếng Anh. Chúng tôi thấy nó hữu ích để lọc ra văn bản vô nghĩa, đặc biệt đối với các văn bản được trích xuất từ PDF qua thuật toán OCR.

• Làm sạch Dữ liệu Internet: Dữ liệu thu thập từ Internet thường gặp phải tình trạng không hoàn chỉnh, ký tự không nhận diện được và thẻ trang web. Do đó, chúng tôi lọc ra các câu có ít hơn 10 từ và loại bỏ các ký tự bất thường và thẻ HTML.

Trộn và Thống kê Theo (Zeng et al., 2022), dữ liệu pre-training của chúng tôi bao gồm cùng tỷ lệ token tiếng Trung và tiếng Anh, trong đó chúng tôi lấy mẫu 190B token tiếng Anh¹² từ corpus Pile và 190B token từ dữ liệu tiếng Trung đã lọc của chúng tôi. Chúng tôi cũng lấy mẫu 20B token mã từ corpus Pile để làm cho tỷ lệ phần trăm tổng thể (5%) của token mã giống như LLaMA (Touvron et al., 2023a) (4.5% token mã). Tổng cộng, tập dữ liệu pre-training của chúng tôi là một hỗn hợp 190B token tiếng Anh, 190B token tiếng Trung và 20B token mã. Theo các chiến lược của chúng tôi, người ta có thể xây dựng một tập dữ liệu pre-training với hàng nghìn tỷ token. Tuy nhiên, chúng tôi chỉ thu thập 400B token để pre-training do tài nguyên tính toán hạn chế của chúng tôi. Hình 1(a) cho thấy thành phần chi tiết của tập dữ liệu pre-training.

⁴https://pile.eleuther.ai/
⁵https://commoncrawl.org/
⁶https://github.com/FudanNLPLAB/CBook-150K
⁷https://github.com/CLUEbenchmark/CLUE
⁸https://github.com/BIT-ENGD/baidu_baike
⁹https://github.com/thunlp/CAIL
¹⁰Vì Pile là một corpus chất lượng cao đã được làm sạch, chúng tôi trực tiếp lấy mẫu dữ liệu tiếng Anh và mã từ corpus Pile mà không cần lọc thêm. Các chiến lược lọc của chúng tôi được áp dụng cho dữ liệu tiếng Trung.
¹¹https://github.com/aboSamoor/polyglot
¹²Những token tiếng Anh này loại trừ dữ liệu mã nhưng chắc chắn chứa một tỷ lệ nhỏ token phi ngôn ngữ (ví dụ: 1.24% dữ liệu toán học) vì chúng tôi chọn ngẫu nhiên các mẫu dựa trên tỷ lệ ban đầu của corpus Pile ngoại trừ dữ liệu mã.

--- TRANG 6 ---

3.1.2 Thu thập Dữ liệu Bilingual Flan

Thu thập Dữ liệu Flan Tiếng Anh Bộ sưu tập Flan (Chung et al., 2022; Longpre et al., 2023) phục vụ như một tập dữ liệu nền tảng cho instruction tuning hiệu quả, bao gồm hơn 1800 tác vụ. Chúng tôi tuân theo hướng dẫn chính thức để thu thập và xử lý bộ sưu tập Flan tiếng Anh với hai bước, tức là tải xuống năm hỗn hợp con từ Bộ sưu tập Flan và sau đó kết hợp chúng theo tỷ lệ hỗn hợp được chỉ định¹³.

Thu thập Dữ liệu Flan Tiếng Trung Nhiều tập dữ liệu hướng dẫn tiếng Trung mã nguồn mở được bắt nguồn từ bản dịch tiếng Anh hoặc được tạo ra bởi ChatGPT sử dụng các prompt khác nhau (Ouyang et al., 2022; Bubeck et al., 2023), có thể dẫn đến sự không chính xác trong dịch thuật và phản hồi sai. Do đó, chất lượng và số lượng của corpus hướng dẫn tiếng Trung hiện có thường không đủ. Để giải quyết những thách thức này, chúng tôi xây dựng corpus hướng dẫn tiếng Trung riêng của mình. Cụ thể hơn, chúng tôi thu thập 44 tác vụ tiếng Trung khác nhau với tổng cộng 50 triệu mục dữ liệu, trong đó các nguồn dữ liệu bao gồm các cuộc thi khác nhau, bài báo học thuật và dự án mã nguồn mở. Phân bố của tập dữ liệu Flan tiếng Trung được thể hiện trong Hình 1(c), và chúng tôi liệt kê tất cả các nguồn dữ liệu trong Bảng 14 (Phụ lục B). Đối với mỗi tác vụ, chúng tôi thiết kế thủ công các hướng dẫn tiếng Trung.

Kết hợp Dữ liệu Song ngữ Do số lượng dữ liệu tiếng Trung nhỏ hơn so với dữ liệu tiếng Anh, chúng tôi thực hiện lấy mẫu trong các tập dữ liệu Flan tiếng Anh để đảm bảo tỷ lệ cân bằng giữa dữ liệu tiếng Trung và tiếng Anh. Như được minh họa trong Hình 1(b), tập dữ liệu Bilingual Flan (BiFlan) bao gồm 66.7% dữ liệu Flan tiếng Anh và 33.3% dữ liệu Flan tiếng Trung. Chúng tôi cũng lọc ra các mẫu có độ dài vượt quá độ dài tối đa của encoder, đảm bảo các phần quan trọng của hướng dẫn không bị cắt bớt.

3.2 Kiến trúc Mô hình

Nhìn chung, mô hình OpenBA tuân theo kiến trúc encoder-decoder tiêu chuẩn như T5 (Raffel et al., 2020). Tuy nhiên, đáng chú ý là encoder và decoder phục vụ các vai trò khác nhau, trong đó encoder cung cấp khả năng hiểu mạnh mẽ, trong khi decoder cung cấp khả năng sinh (Vaswani et al., 2017), và các công trình hiện có chỉ ra rằng một mô hình encoder-decoder với nhiều lớp encoder hơn có thể đạt được hiệu suất mạnh mẽ (Soltan et al., 2022). Để nâng cao khả năng sinh và lấp đầy khoảng trống của LLM dựa trên decoder sâu hơn, chúng tôi cũng thiết kế một cấu trúc bất đối xứng khác, trong đó các thiết lập mô hình chi tiết được đưa ra trong Bảng 1.

[THIS IS TABLE: Table 1 showing model settings for OpenBA with columns for Encoder, Decoder, Attn Heads, d_model, d_ff, and #Param.(B)]

Ngoài việc tận dụng shallow-encoder deep-decoder bất đối xứng, chúng tôi cũng kết hợp các chiến lược cải thiện sau đây vào kiến trúc mô hình:

• Sandwich Layer Normalization Để ổn định quá trình huấn luyện, chúng tôi áp dụng sandwich layer normalization (Ding et al., 2021) bằng cách chuẩn hóa cả đầu vào của mỗi khối transformer và đầu ra của mỗi lớp attention. Chúng tôi sử dụng RMSNorm (Zhang & Sennrich, 2019) để chuẩn hóa.

• Rotary Embedding Chúng tôi áp dụng rotary embedding (Su et al., 2021) mã hóa vị trí tuyệt đối với ma trận xoay và đồng thời kết hợp sự phụ thuộc vị trí tương đối rõ ràng trong self-attention thay vì relative positional embedding trong T5.

• Hàm Kích hoạt SwiGLU Được truyền cảm hứng từ LLaMA (Touvron et al., 2023a), chúng tôi thay thế kích hoạt ReLU ban đầu bằng hàm kích hoạt SwiGLU (Shazeer, 2020), và đặt kích thước là 2/3 4d.

• Tokenizer mT5 Đối với thiết lập song ngữ, chúng tôi sử dụng tokenizer mT5 (Xue et al., 2020) vì nó không chỉ bao gồm tiếng Trung và tiếng Anh mà còn cung cấp khả năng cho việc huấn luyện và ứng dụng thêm trong các ngôn ngữ khác.

--- TRANG 7 ---

3.3 Quy trình Huấn luyện và Tác vụ Mô hình hóa Ngôn ngữ

Như được thể hiện trong Hình 2, chúng tôi áp dụng chiến lược huấn luyện theo từng giai đoạn (Barshan & Fieguth, 2015) bao gồm pre-training UL2 (Tay et al., 2022), length-adaptation và các giai đoạn huấn luyện Flan (Wei et al., 2021), và đặt độ dài ngữ cảnh và kích thước batch khác nhau cho các giai đoạn khác nhau (Bảng 2). Đối với tất cả các giai đoạn, chúng tôi áp dụng tác vụ mô hình hóa ngôn ngữ span-denoising như được đề xuất trong T5 (Raffel et al., 2020) và BART (Lewis et al., 2019). Cụ thể hơn, cho một chuỗi x={xi}ᵢ₌₁ⁿ chứa n token, chúng tôi làm hỏng nó với mask sentinel mⱼ={xi}ᵢ₌ₚᵏ, trong đó p < k, 1≤p, k≤n. Sau đó, mục tiêu huấn luyện là khôi phục span bị mask, có thể được viết chính thức là:

L(x) = log P(m|x\m, θ) (1)

trong đó m={mⱼ}ⱼ₌₁ᴷ là tập hợp các span bị mask, x\m biểu thị chuỗi bị hỏng, và θ đại diện cho các tham số mô hình. Đối với mô hình OpenBA, x\m được đưa vào encoder, có thể giữ trường tiếp nhận hai chiều, và m được dự đoán bởi decoder. Tiếp theo, chúng tôi sẽ giới thiệu các giai đoạn khác nhau đã đề cập một cách cụ thể hơn.

[THIS IS TABLE: Table 2 showing configurations for each training stage with columns for Stage, Strategy, Encoder Context, Decoder Context, Batch Size, and #Tokens(B)]

Giai đoạn I: Pre-training UL2 Bắt đầu với chiến lược huấn luyện UL2, chúng tôi áp dụng chiến lược huấn luyện hỗn hợp các denoiser được đề xuất bởi Tay et al. (2022), phơi bày OpenBA với một tập hợp các vấn đề đa dạng trong giai đoạn pre-training đầu tiên. Trong giai đoạn này, mô hình OpenBA được cung cấp 300B token được lấy mẫu từ corpus pre-training, và chúng tôi sử dụng ba mục tiêu huấn luyện khác nhau được liệt kê dưới đây:

• R-Denoising Regular denoising là span corruption tiêu chuẩn thiết lập phạm vi 2 đến 5 token làm độ dài span bị mask và tỷ lệ mask khoảng 15% token đầu vào. Tác vụ denoising này tương đối đơn giản vì span ngắn và hiệu quả để mô hình có được kiến thức được nhúng trong văn bản.

• S-Denoising Sequence denoising nhằm trao cho mô hình khả năng sinh, trong đó văn bản đầu vào được chia thành hai chuỗi con, và mô hình nên dự đoán chuỗi sau dựa trên chuỗi đầu tiên. Trong thiết lập S-Denoising, mô hình có thể có được khả năng sinh.

• X-Denoising Để bắc cầu khoảng cách giữa R-Denoising và S-Denoising, X-Denoising có thể được xem như một phiên bản cực đoan của denoising, trong đó khoảng 50% chuỗi đầu vào được mask bằng cách tăng độ dài span bị mask hoặc tỷ lệ corruption. Chiến lược denoising như vậy mô phỏng tình huống khi một mô hình cần sinh mục tiêu dài từ một bộ nhớ với thông tin tương đối hạn chế.

Chúng tôi liệt kê các thiết lập của ba chiến lược denoising này trong Bảng 3. Đáng chú ý là chúng tôi thực hiện các chiến lược denoising này từ cấp độ instance và thêm ba token đặc biệt trước mỗi chuỗi bị hỏng để prompt tác vụ denoising hiện tại cho OpenBA (Tay et al., 2022). Chúng tôi lấy mẫu đồng nhất một giá trị dựa trên μ làm độ dài span bị mask cho R-denoising và X-denoising. Đối với S-denoising, chúng tôi giới hạn mỗi span bị mask kết thúc ở cuối văn bản đầu vào và chỉ cho phép một span bị mask. Bên cạnh đó, chúng tôi đặt độ dài ngữ cảnh encoder-decoder là 570/380 trong giai đoạn này để tăng hiệu quả lấy mẫu.

[THIS IS TABLE: Table 3 showing settings of three denoising strategies for UL2 pre-training stage with columns for Type, Span Length, Corruption Ratio, #Num Sentinel]

Giai đoạn II: Length-Adaptation Xét rằng độ dài ngữ cảnh cho giai đoạn pre-training đầu tiên là ngắn, có thể không hỗ trợ các định dạng đầu vào và đầu ra dài của một số tác vụ, chẳng hạn như in-context learning (Min et al., 2021) và sinh văn bản dài (Guan et al., 2021), chúng tôi mở rộng độ dài ngữ cảnh encoder-decoder lên 1,024/1,024 trong giai đoạn length-adaptation. Trong giai đoạn này, chúng tôi

--- TRANG 8 ---

sử dụng 40B token được lấy mẫu từ corpus pre-training và đảm bảo rằng không có sự chồng chéo giữa dữ liệu này và dữ liệu từ giai đoạn trước. Ngoài ra, chúng tôi đơn giản áp dụng mục tiêu huấn luyện S-Denoising và điều chỉnh tỷ lệ corruption lên 50%. Chúng tôi giữ sentinel đặc biệt <S> trước mỗi văn bản bị hỏng và giảm kích thước batch để ổn định huấn luyện trong giai đoạn này.

Giai đoạn III: Huấn luyện Bilingual Flan Được truyền cảm hứng từ công trình trước đây (Chung et al., 2022), chúng tôi áp dụng huấn luyện hướng dẫn Flan trên checkpoint OpenBA đã thích ứng độ dài. Chúng tôi vẫn thêm token đặc biệt <S> trước mỗi văn bản cho tác vụ sinh và áp dụng tập dữ liệu BiFlan đã xây dựng trong giai đoạn này. Ngoài ra, chúng tôi đặt độ dài chuỗi encoder-decoder là 1,024/256 trong giai đoạn này để tăng hiệu quả lấy mẫu vì chúng tôi quan sát thấy hầu hết đầu ra của tập dữ liệu Flan đều ngắn, tức là ít hơn 256 token.

--- TRANG 9 ---

3.4 Triển khai Mô hình và Kỹ thuật

Chúng tôi huấn luyện OpenBA trên một cụm với 4 nút (8 × GPU NVIDIA A100-SXM4-80GB), được kết nối với mạng InfiniBand (Grun, 2010) và kết nối thông qua hệ thống NVLink. Mô hình đã tiêu thụ gần 400B token song ngữ và đạt được 1.2×10²² FLOP (floating point operations) tổng cộng. Chúng tôi triển khai mô hình dựa trên framework NVIDIA-Megatron¹⁴ và thực hiện một số tối ưu hóa cho việc ổn định huấn luyện và hiệu quả suy luận. Chúng tôi vẽ đồ thị mất mát huấn luyện cho ba giai đoạn đã đề cập trong Hình 5, và liệt kê các kỹ thuật chúng tôi đã sử dụng dưới đây:

• Song song 3D Song song 3D (Shoeybi et al., 2019) nhằm mở rộng và tăng tốc quá trình huấn luyện của LLM, khai thác ba kỹ thuật song song cốt lõi, tức là song song dữ liệu, song song mô hình (mp), và song song pipeline (pp). Xét kích thước mô hình, số lượng GPU và tốc độ giao tiếp giữa các GPU, chúng tôi quyết định thiết lập tối ưu là mp_size = 4 và pp_size = 1, đạt 120 TFLOP/s mỗi GPU.

• Checkpoint Activation Checkpoint activation là một kỹ thuật được thiết kế để tối ưu hóa việc sử dụng bộ nhớ trong quá trình huấn luyện. Thay vì lưu trữ tất cả các activation lớp trung gian, chỉ một số nhất định được bảo tồn. Trong quá trình back-propagation, các activation bị thiếu được tính toán lại, đánh đổi nỗ lực tính toán bổ sung để tiết kiệm bộ nhớ. Chiến lược này cho phép huấn luyện các mô hình lớn hơn ngay cả trên các GPU có dung lượng bộ nhớ hạn chế. Thực tế, việc huấn luyện mô hình 15B trên GPU 80GB trở nên khả thi về mặt bộ nhớ. Chúng tôi cụ thể áp dụng checkpoint activation cho tính toán attention, tương đối hiệu quả về chi phí để tính toán lại. Trong triển khai thực tế, chúng tôi quan sát thấy cải thiện đáng kể trong việc sử dụng bộ nhớ GPU, nâng cao hiệu suất hệ thống tổng thể.

• Distributed Optimizer Cách tiếp cận tối ưu hóa phân tán cung cấp một lựa chọn thay thế để tiết kiệm bộ nhớ GPU, cho phép sử dụng kích thước batch tăng lên, mặc dù với chi phí gánh nặng giao tiếp giữa các GPU. Bằng cách áp dụng phương pháp ZeRO được đề xuất bởi Rajbhandari et al. (2020) và triển khai kỹ thuật tối ưu hóa phân tán (Shoeybi et al., 2019), chúng tôi có thể tăng kích thước batch, từ đó nâng cao tốc độ huấn luyện.

• Tính toán Attention Weights với Độ chính xác FP32 Trong quá trình tính toán softmax, đặc biệt khi xử lý các giá trị lớn, tồn tại khả năng tràn số. Thực hiện tính toán này với độ chính xác FP32 giảm thiểu rủi ro này so với việc sử dụng độ chính xác FP16. Các công trình trước đây (Nijkamp et al., 2022) chỉ ra rằng vấn đề như vậy có thể dễ dàng xảy ra khi tính toán attention weights với độ chính xác FP16. Trong giai đoạn huấn luyện sớm của OpenBA, chúng tôi áp dụng tính toán nửa độ chính xác cho tất cả các module mô hình và thường quan sát hiện tượng mất mát sụp đổ. Tuy nhiên, vấn đề như vậy đã được giảm thiểu đáng kể khi chuyển đổi tính toán attention weight sang độ chính xác đầy đủ (FP32). Do đó, chúng tôi có thể kết luận một cách thực nghiệm rằng tính toán attention weight với độ chính xác FP32 có thể nâng cao đáng kể tính ổn định của quá trình huấn luyện.

• Hiệu quả Suy luận Để tăng tốc độ suy luận, chúng tôi áp dụng kỹ thuật KV-cache và giảm tính toán bằng cách tính toán trước rotary embedding cho tất cả các vị trí.

4 Kết quả

4.1 Cài đặt Đánh giá

Chúng tôi đánh giá OpenBA từ ba khía cạnh: hiểu ngôn ngữ tự nhiên, sinh ngôn ngữ tự nhiên và lý luận thông thường. Cụ thể, chúng tôi đánh giá khả năng hiểu ngôn ngữ tự nhiên trên benchmark SuperGLUE (Wang et al., 2019) và BELEBELE (Bandarkar et al., 2023), khả năng sinh ngôn ngữ tự nhiên với năm tác vụ hạ nguồn (tóm tắt, dịch máy, đơn giản hóa văn bản, paraphrase và sinh câu chuyện), và khả năng lý luận thông thường trên năm benchmark có thẩm quyền, bao gồm MMLU (Hendrycks et al., 2020), CMMLU (Li et al., 2023a), BBH (Suzgun et al., 2022) và C-Eval (Huang et al., 2023). Theo các công trình trước đây (Brown et al., 2020; Touvron et al., 2023a), chúng tôi xét cả thiết lập zero-shot và few-shot và phân biệt nghiêm ngặt phân bố miền của dữ liệu huấn luyện và kiểm tra. Minh họa và triển khai tương ứng của mỗi thiết lập như sau:

¹⁴https://github.com/NVIDIA/Megatron-LM/

--- TRANG 10 ---

• Zero-Shot Chúng tôi cung cấp mô tả văn bản của tác vụ cho mỗi mẫu kiểm tra, và mô hình sẽ phản hồi theo cách mở. Các mẫu cho tất cả các tác vụ được liệt kê trong Phụ lục A.

• Few-Shot Chúng tôi đánh giá mỗi ví dụ trong tập kiểm tra bằng cách chọn ngẫu nhiên ℓ ví dụ từ tập huấn luyện của mỗi tác vụ làm điều kiện. Trong bài báo này, chúng tôi đặt ℓ = 5 làm mặc định nếu không được chỉ định.

• Miền Held-in / Held-out Chúng tôi phân biệt giữa thiết lập held-in và held-out dựa trên việc dữ liệu huấn luyện có bao gồm miền của tập kiểm tra hay không. Nếu mô hình đã được huấn luyện trên dữ liệu huấn luyện tương ứng với tác vụ kiểm tra, nó được xem là held-in; ngược lại, nó là held-out (Longpre et al., 2023).

Chúng tôi cũng áp dụng kỹ thuật CoT cho một số tác vụ, và các mẫu tương ứng cũng được thể hiện trong Phụ lục A. Đáng chú ý là chúng tôi sẽ trình bày cụ thể các thiết lập cơ bản cho mỗi tác vụ đánh giá và so sánh chúng với các mô hình dưới cùng thiết lập. Ngoài ra, chúng tôi sẽ đánh giá kết quả sử dụng các metric và nền tảng đánh giá được khuyến nghị chính thức bất cứ khi nào có thể và sử dụng phông chữ đậm để chỉ ra hiệu suất tốt nhất và áp dụng gạch dưới để biểu thị hiệu suất tốt thứ hai trong tất cả các thí nghiệm.

4.2 Phân tích Chi phí Huấn luyện

Tất cả các mô hình chúng tôi so sánh được liệt kê trong Bảng 4, trong đó chúng tôi báo cáo các tham số, token tiêu thụ, chi phí huấn luyện và phát thải carbon tương ứng của chúng. Để tính toán phát thải carbon, chúng tôi theo Wu et al. (2022) và Touvron et al. (2023a) bằng cách lấy PUE là 1.1 và hệ số cường độ carbon được đặt ở mức trung bình quốc gia Hoa Kỳ là 0.385 kg CO2e trên KWh, và công thức là:

tCO₂eq = MWh × 0.385 (2)

Đáng chú ý là, quá trình huấn luyện OpenBA rất hiệu quả và thân thiện với môi trường. Lấy LLaMA-13B làm ví dụ, nó tiêu thụ khoảng 1TB token với tổng cộng 59MWh điện GPU và phát thải khoảng 23tCO₂eq carbon. Tuy nhiên, mô hình của chúng tôi chỉ tiêu thụ 6.5 tCO₂eq carbon cho 380B token, tức là khoảng 28.26% tổng lượng phát thải carbon của mô hình LLaMA-13B. Thêm chi tiết huấn luyện và triển khai mô hình có thể tìm thấy trong Mục 3.4.

[THIS IS TABLE: Table 4 showing comparison of various LLM models including parameters, tokens, GPU type, hours, power consumption, and carbon emissions]

4.3 Hiểu Ngôn ngữ Tự nhiên

Chúng tôi đánh giá hiệu suất hiểu ngôn ngữ tự nhiên của mô hình OpenBA trên benchmark SuperGLUE, chứa 13 tác vụ con. Vì tập dữ liệu BiFlan chứa dữ liệu huấn luyện một phần của một số tác vụ kiểm tra trong SuperGLUE, chúng tôi chủ yếu so sánh OpenBA với các mô hình trong thiết lập held-in (ngoại trừ GPT-3 (Brown et al., 2020)), tức là những mô hình này cũng đã được huấn luyện trên dữ liệu huấn luyện của một số tác vụ kiểm tra trong SuperGLUE. Như chúng tôi có thể quan sát trong Bảng 5, hiệu suất của OpenBA vượt qua mô hình BERT (Devlin et al., 2018) được tinh chỉnh trên tập huấn luyện SuperGLUE và GPT-3, nhưng hơi kém so với mô hình Flan-T5-XL (Chung et al., 2022).

--- TRANG 11 ---

[THIS IS TABLE: Table 5 showing zero-shot results on SuperGLUE benchmark with various models and their performance metrics]

Chúng tôi đánh giá khả năng đọc hiểu của OpenBA với benchmark BELEBELE (Bandarkar et al., 2023) và chọn các tập con tiếng Trung (Giản thể), tiếng Trung (Phồn thể) và tiếng Anh để đánh giá. Chúng tôi tuân theo thiết lập chính thức và so sánh với cả LLM và các mô hình hạ nguồn được tinh chỉnh, bao gồm Falcon (Penedo et al., 2023a), LLaMA (Touvron et al., 2023a,b), XLM-V (Liang et al., 2023a), InfoXLM (Chi et al., 2020) và ChatGPT (Ouyang et al., 2022). Chúng tôi cung cấp tất cả các hướng dẫn chúng tôi sử dụng cho thiết lập zero-shot trong Phụ lục A. Như chúng tôi có thể quan sát từ Bảng 6, OpenBA có thể đạt được kết quả xuất sắc trong các tác vụ đọc hiểu tiếng Trung, chỉ xếp sau ChatGPT. Đối với các tác vụ đọc hiểu tiếng Anh, hiệu suất của OpenBA có thể so sánh với mô hình Falcon-40B, được huấn luyện với khoảng 1TB token dữ liệu đa ngôn ngữ. Cũng đáng chú ý là OpenBA đạt được hiệu suất tốt hơn trong số nhiều LLM mã nguồn mở hiện tại, bao gồm hai mô hình LLaMA mạnh và mô hình Falcon-40B, dưới thiết lập song ngữ.

[THIS IS TABLE: Table 6 showing model performance on BELEBELE benchmark comparing different models across English, Chinese Simplified, and Chinese Traditional]

4.4 Sinh Ngôn ngữ Tự nhiên

Chúng tôi đánh giá khả năng sinh ngôn ngữ tự nhiên của mô hình trên năm tác vụ, bao gồm dịch máy trên benchmark Flores (Goyal et al., 2022), tóm tắt văn bản trên benchmark CLTS (Liu et al., 2020), tác vụ paraphrase trên tập dữ liệu QQP¹⁵, đơn giản hóa văn bản trên tập dữ liệu WIKI-AUTO (Coster & Kauchak, 2011), và sinh câu chuyện trên tập dữ liệu ROC (Mostafazadeh et al., 2016).

Tóm tắt Để đánh giá tác vụ tóm tắt dưới thiết lập held-out, chúng tôi chọn một tập con chứa 100 câu được lấy mẫu từ benchmark CLTS (Liu et al., 2020), được loại trừ khỏi tập dữ liệu BiFlan. Cụ thể, chúng tôi thêm hướng dẫn tác vụ trước mỗi câu kiểm tra (hướng dẫn tác vụ được liệt kê trong Phụ lục A) và cho phép các mô hình thực hiện suy luận zero-shot. Chúng tôi đánh giá

¹⁵https://www.kaggle.com/c/quora-question-pairs

--- TRANG 12 ---

[THIS IS TABLE: Table 7 showing model performance on CLTS subset with Rouge scores]
[THIS IS TABLE: Table 8 showing model performance on Flores subset with translation scores]

các kết quả được sinh ra với metric Rouge-n (Lin, 2004) và báo cáo kết quả trong Bảng 7. Chúng tôi quan sát thấy OpenBA có thể đạt được hiệu suất tốt nhất trên điểm Rouge-1 và Rouge-L, chỉ ra rằng nội dung được sinh ra từ OpenBA trung thực với văn bản gốc trong tác vụ tóm tắt.

Dịch máy Chúng tôi so sánh hiệu suất mô hình trên các tác vụ dịch máy song ngữ, bao gồm dịch Trung-Anh và Anh-Trung, trên benchmark dịch máy Flores (Goyal et al., 2022). Chúng tôi tuân theo nghiêm ngặt thiết lập chính thức bằng cách chọn 50 mẫu kiểm tra được cung cấp cho mỗi tác vụ dịch. Đáng chú ý là tất cả các mô hình đều dưới thiết lập held-out zero-shot. Chúng tôi báo cáo điểm BLEU-4 (Post, 2018) trong Bảng 8 và có thể quan sát thấy OpenBA có thể đạt được hiệu suất tốt nhất trên tác vụ dịch Trung-Anh và có được kết quả có thể so sánh với SOTA đạt được bởi BatGPT trên tác vụ dịch Anh-Trung.

[THIS IS TABLE: Table 9 showing model performance on WIKI AUTO and QQP datasets with various metrics]

Đơn giản hóa Văn bản và Paraphrase Chúng tôi đánh giá khả năng đơn giản hóa văn bản và paraphrase của OpenBA trên các tập dữ liệu WIKI AUTO và QQP. Chúng tôi đánh giá hiệu suất mô hình với các metric BLEU, Distinct-n (D-n) (Li et al., 2015), Lexical Repetition (Rep-n, lặp lại 4-gram cho n lần) (Shao et al., 2019b), Mauve (Pillutla et al., 2021) và Semantic Similarity (SIM, độ tương tự ngữ nghĩa giữa các sinh và prompt tương ứng) (Guan et al., 2021), và báo cáo hiệu suất mô hình trong Bảng 9. Dựa trên quan sát rằng OpenBA có thể đạt được kết quả tốt nhất trên các metric Mav và SIM, đánh giá liên quan ngữ nghĩa với văn bản vàng và văn bản đầu vào tương ứng, chúng tôi có thể kết luận rằng mô hình của chúng tôi xuất sắc trong việc nắm bắt thông tin ngữ nghĩa tổng thể của nội dung đầu vào và sinh nội dung liên quan tương ứng.

[THIS IS TABLE: Table 13 showing model performance on BBH benchmark with accuracy scores]

Sinh Câu chuyện Chúng tôi đánh giá khả năng sinh miền mở của OpenBA trên tập dữ liệu ROC, trong đó mô hình nên tiếp tục sinh dựa trên ngữ cảnh hiện có và cốt truyện. Cụ thể hơn, chúng tôi cung cấp prompt trực tiếp cho mô hình và so sánh OpenBA với hai mô hình khác: GPT-J (Wang & Komatsuzaki, 2021) và OPT-13B (Zhang et al., 2022), cũng được huấn luyện trên corpus Pile. Chúng tôi lấy mẫu ngẫu nhiên 100 trường hợp sinh và mời các chú thích viên chấm điểm văn bản từ ba khía cạnh, bao gồm tính mạch lạc giữa văn bản được sinh và prompt, tính nhất quán và tính đúng đắn của văn bản được sinh. Các chú thích viên được phép chọn "Hòa" nếu khó phân biệt hai trường hợp sinh. Như được thể hiện trong Hình 4, chúng tôi có thể quan sát mô hình của chúng tôi có thể có được hiệu suất mạnh về khía cạnh mạch lạc và nhất quán và đạt được hiệu suất có thể so sánh với hai mô hình khác về khía cạnh đúng đắn.

--- TRANG 13 ---

[THIS IS FIGURE: Figure 4 showing human evaluation results on ROC dataset with bar charts comparing different models across coherence, consistency, and correctness metrics]

4.5 Lý luận Thông thường

Chúng tôi đánh giá khả năng lý luận thông thường của OpenBA trên bốn benchmark, bao gồm MMLU, CMMLU, BBH và C-Eval. Để đảm bảo so sánh công bằng, chúng tôi thực hiện tất cả các đánh giá dưới thiết lập held-out, tuân theo thiết lập được khuyến nghị của mỗi benchmark, và so sánh với các LLM mạnh khác dưới cùng thiết lập. Đối với benchmark MMLU (Bảng 10) và C-Eval (Bảng 12), chúng tôi báo cáo kết quả zero-shot, 5-shot và 5-shot CoT. Đối với CMMLU (Bảng 11), chúng tôi báo cáo kết quả zero-shot, 5-shot và zero-shot CoT. Chúng tôi báo cáo kết quả zero-shot cho BBH trong Bảng 13. Đáng chú ý là khối đầu tiên cho mỗi bảng là các mô hình định hướng đa ngôn ngữ hoặc tiếng Anh, khối thứ hai là các mô hình định hướng tiếng Trung, và chúng tôi xếp hạng các mô hình trong mỗi khối theo kích thước mô hình. Chúng tôi có thể quan sát rằng, trên tất cả các benchmark, OpenBA có thể đạt được hiệu suất tốt hơn hai mô hình định hướng tiếng Trung mạnh, tức là ChatGLM (Du et al., 2022) và BatGPT¹⁶ (Li et al., 2023c), và có được kết quả có thể so sánh với mô hình Baichuan-7B (Inc., 2023), được huấn luyện trên tập dữ liệu lớn hơn nhiều so với chúng tôi, tức là 1.2TB token. Hơn nữa, mô hình của chúng tôi vượt qua các mô hình định hướng tiếng Anh trên hầu hết các benchmark và thậm chí vượt trội một số tác vụ mà các mô hình định hướng tiếng Anh có hơn 100 tỷ tham số, ví dụ: BLOOM-176B, trên benchmark MMLU. Ngoài ra, OpenBA có thể đạt được điểm số có thể so sánh dưới cả thiết lập zero-shot và few-shot và thậm chí hoạt động hơi tốt hơn dưới thiết lập zero-shot, chỉ ra rằng mô hình OpenBA có khả năng tuân theo hướng dẫn mạnh mẽ.

¹⁶BatGPT đạt được hiệu suất tốt nhất trên bảng xếp hạng C-Eval chính thức, nhưng chúng tôi không có được kết quả báo cáo sử dụng phiên bản mã nguồn mở của BatGPT.

--- TRANG 14 ---

[THIS IS TABLE: Table 10 showing model performance on MMLU benchmark with columns for different subject areas and parameters]

[THIS IS TABLE: Table 11 showing performance on CMMLU benchmark with similar structure]

5 Phân tích

5.1 Lựa chọn Kiến trúc Mô hình

Kiến trúc mô hình shallow-encoder deep-decoder bất đối xứng của chúng tôi xuất phát từ các động lực và cân nhắc sau:

• Khả năng Sinh nâng cao. Đối với ba tác vụ trong UL2, cụ thể là R-Denoising, S-Denoising và X-Denoising, thiết lập decoder sâu hơn đặc biệt hiệu quả cho tác vụ S-Denoising, phản ánh khả năng mô hình hóa ngôn ngữ của mô hình.

• Tăng tốc Tiềm năng trong Suy luận Đối thoại. Các kiến trúc chỉ-decoder tương tự GPT đã đạt được kết quả xuất sắc trong các tác vụ đối thoại đa vòng. Tuy nhiên, đối với các mô hình encoder-decoder, cách lưu trữ lịch sử đối thoại đặt ra một thách thức đáng kể. Một cách tiếp cận phổ biến là nhúng lịch sử đối thoại vào đầu vào của encoder. Tuy nhiên, việc liên tục thay đổi lịch sử này dẫn đến chi phí tính toán tăng trong encoder, và không thuận lợi cho việc tăng tốc qua KV-caching. Để giải quyết thách thức này, chúng tôi có thể đặt lịch sử đối thoại vào decoder. Sự thay đổi này đặt ra yêu cầu lớn hơn đối với khả năng của decoder. Do đó, chúng tôi khám phá việc huấn luyện decoder sâu hơn để trao cho nó khả năng nâng cao.

Chúng tôi thực hiện các thí nghiệm để khám phá ảnh hưởng của kiến trúc mô hình, trong đó chúng tôi huấn luyện mô hình với mục tiêu huấn luyện UL2. Cụ thể, chúng tôi đặt kích thước batch là 128 và độ dài chuỗi là 570/380. Chúng tôi xác thực hiệu suất mô hình sau 15k bước huấn luyện.

Cấu hình Mô hình Chúng tôi chủ yếu khám phá ba cấu trúc mô hình: (1) encoder nông với decoder sâu, (2) encoder sâu với decoder nông, và (3) encoder và decoder với độ sâu bằng nhau. Chúng tôi đánh giá metric hiệu suất của chúng trên các tác vụ R-Denoising, S-Denoising và X-Denoising để học các ưu điểm tương ứng. Để duy trì số lượng tham số nhất quán trên các cấu hình khác nhau, chúng tôi áp dụng các cấu trúc lớp này: (1) EncoderLayer=18, DecoderLayer=6, (2) EncoderLayer=6, DecoderLayer=18, và (3) EncoderLayer=DecoderLayer=12.

--- TRANG 15 ---

[THIS IS TABLE: Table 12 showing model performance on C-Eval benchmark with columns for different subject areas and parameters]

[THIS IS FIGURE: Figure 5 showing multiple graphs comparing performance in terms of loss and accuracy across different model configurations and denoising tasks]

Metric Đánh giá Để có cái nhìn trực tiếp về hiệu suất mô hình được pre-train từ đầu, chúng tôi chọn Loss và Acc. làm metric thuận tiện. Cụ thể, chúng tôi xây dựng các tập xác thực cho R-Denoising, S-Denoising, X-Denoising và sự kết hợp của ba tác vụ, tương ứng, và kiểm tra hiệu suất của mô hình trong suốt quá trình huấn luyện. Acc. chỉ ra độ chính xác dự đoán của mô hình cho từ tiếp theo:

Acc. = 1/n ∑(i=1 to n) I(argmax(w∈V) P(xi = w|x<i, θ) = xi),  (3)

trong đó n biểu thị độ dài chuỗi, V biểu thị kích thước từ vựng và I là hàm chỉ thị.

Phân tích Hình 5 cho thấy kết quả của chúng tôi. Chúng tôi có thể kết luận rằng:

• Như một thước đo khả năng sinh của mô hình, tác vụ S-Denoising nói chung khó học hơn. Điều này rõ ràng vì, bất kể cấu hình mô hình, tác vụ S-Denoising luôn có mất mát cao hơn và độ chính xác thấp hơn.

• Mô hình với cấu hình encoder nông và decoder sâu hoạt động tốt hơn trên tác vụ S-denoising (từ Hình 5(b) và 5(f)), mặc dù nó không vượt trội thiết lập cân bằng trên tất cả ba tác vụ (từ Hình 5(b) và 5(f)).

--- TRANG 16 ---

5.2 Tiến triển Hiệu suất trong Quá trình Huấn luyện

Trong phần này, chúng tôi đánh giá hiệu suất của OpenBA ở các giai đoạn khác nhau của quá trình huấn luyện tổng thể. Chúng tôi sử dụng ba benchmark để đánh giá, bao gồm MMLU cho lý luận thông thường tiếng Anh, CMMLU cho lý luận thông thường tiếng Trung, và BELEBELE cho đọc hiểu. Như được thể hiện trong Hình 6(a), Hình 6(b) và Hình 6(c), hiệu suất trên hầu hết các tác vụ tăng lên theo số bước huấn luyện trong giai đoạn Pre-training UL2, trải qua biến động nhẹ trong giai đoạn Length-Adaptation, và thể hiện cải thiện đáng kể trong giai đoạn Bilingual Flan Training. Các đường cong nổi lên của tiếng Trung và tiếng Anh tương tự, chỉ ra rằng tập dữ liệu Bilingual Flan của chúng tôi nâng cao hiệu quả hiệu suất tác vụ đa ngôn ngữ trên các tác vụ held-out.

Hơn nữa, chúng tôi đo hiệu suất trên MMLU khi được cung cấp các token mẫu bổ sung khác nhau, tức là {<R>, <S>, <X>}. Chúng tôi thấy rằng hiệu suất với các token mẫu bổ sung khác nhau cho thấy sự khác biệt trong giai đoạn pre-training UL2, trong khi những khác biệt này dần dần giảm bớt trong các giai đoạn tiếp theo. Điều này có thể được quy cho việc chúng tôi sử dụng các token mẫu bổ sung này để hướng dẫn chuyển đổi chế độ chỉ trong giai đoạn đầu tiên cho các tác vụ UL2 khác nhau. Cụ thể, hiệu suất cho tác vụ S-denoising trong viết liên tục hơi kém so với các tác vụ X-denoising và R-denoising cho phục hồi span bị mask.

[THIS IS FIGURE: Figure 6 showing four graphs depicting evolution of model performance during training for MMLU, CMMLU, BELEBELE, and MMLU with different prefix tokens]

6 OpenBA-X: Thích ứng Tác vụ Hạ nguồn

Sau Giai đoạn III, chúng tôi thực hiện tinh chỉnh có giám sát cho OpenBA trên bốn tác vụ hạ nguồn, bao gồm đối thoại đa vòng song ngữ (OpenBA-Chat), sinh mã (OpenBA-Code), sinh hướng dẫn (OpenBA-InstructGen), và truy xuất công cụ (OpenBA-Tool). Trong Mục 6.1 đến 6.4, chúng tôi sẽ cung cấp chi tiết về thu thập và xử lý các tập dữ liệu hạ nguồn. Đáng chú ý là

--- TRANG 17 ---

chúng tôi sử dụng chiến lược S-denoising để tinh chỉnh tất cả các tác vụ hạ nguồn, tức là thêm token "<S>" trước mỗi văn bản mục tiêu được đưa vào decoder. Chúng tôi liệt kê tất cả các mẫu hướng dẫn trong Phụ lục A.

6.1 OpenBA-Chat: Đối thoại Đa vòng Song ngữ

Thu thập Tập dữ liệu Chúng tôi xây dựng dữ liệu đối thoại đa vòng song ngữ có giám sát từ ba nguồn khác biệt: DialogStudio (Zhang et al., 2023a), BELLE (Ji et al., 2023) và ShareGPT¹⁷. Chúng tôi sử dụng tập dữ liệu DialogStudio cho dữ liệu đối thoại tiếng Anh vì nó chứa các cuộc trò chuyện đa dạng cho nhiều tình huống khác nhau. Đối với dữ liệu đối thoại tiếng Trung, chúng tôi sử dụng tập dữ liệu BELLE và dữ liệu ShareGPT được xử lý bởi người khác¹⁸. Chúng tôi lọc ra các cuộc trò chuyện quá đơn giản dựa trên độ dài của chúng, cũng như nội dung chứa thông tin nhận dạng mô hình, ví dụ: "Tôi là ChatGPT." Quan trọng hơn, chúng tôi chú thích thủ công 40 cuộc trò chuyện song ngữ để xác định OpenBA và lặp lại chúng mười lần trước khi thêm vào tập dữ liệu huấn luyện.

Xử lý Tập dữ liệu Cho T vòng cuộc trò chuyện liên quan đến hai tác nhân H và A trong một cuộc đối thoại, dữ liệu có thể được viết là: S = (H₁, A₁, H₂, A₂, ···, Hₜ, Aₜ, ···, Hₜ, Aₜ), trong đó (Hₜ, Aₜ) đại diện cho vòng thứ t của cuộc trò chuyện. Để cho phép mô hình nhận thức lịch sử đối thoại và phản hồi dựa trên thông tin lịch sử, chúng tôi xử lý mỗi dữ liệu đối thoại S thành tập D:

D = ⋃ᵀₜ₌₁{Inputₜ, Targetₜ} = ⋃ᵀₜ₌₁{(H₁, A₁, H₂, A₂, ···, Hₜ), (Aₜ)},

trong đó Inputₜ đại diện cho chuỗi đầu vào, và Outputₜ đại diện cho chuỗi phản hồi. Mẫu để tạo cuộc trò chuyện cho mỗi instance được thể hiện dưới đây:

Đầu vào: "Human: {H₀} Assistant: {A₀}···Human: {Hₜ} Assistant:"
Đầu ra: "{Aₜ}"

6.2 OpenBA-Code: Sinh mã

Thu thập Tập dữ liệu Đối với sinh mã, chúng tôi chủ yếu tập trung vào ngôn ngữ Python. Chúng tôi chọn một phiên bản đã lọc của tập dữ liệu Evol-Instruct (Luo et al., 2023), chứa 26,588 mẫu mã¹⁹.

Xử lý Tập dữ liệu Tokenizer ban đầu của OpenBA sẽ bỏ qua các khoảng trắng liên tiếp, từ đó xóa thông tin thụt lề trong mã. Để giải quyết vấn đề này, chúng tôi kết hợp ba token đặc biệt vào từ vựng: ký tự tab '\t', ký tự xuống dòng '\n', và khoảng trắng liên tiếp. Chúng tôi trực tiếp sử dụng các hướng dẫn từ tập dữ liệu ban đầu vì các hướng dẫn khác nhau cho nội dung mã khác nhau.

6.3 OpenBA-InstructGen: Sinh hướng dẫn

Thu thập Tập dữ liệu Chúng tôi xây dựng một tập dữ liệu song ngữ cho tác vụ sinh hướng dẫn bằng cách đảo ngược tập dữ liệu hướng dẫn ban đầu (Li et al., 2023b; Taori et al., 2023). Cụ thể, chúng tôi sử dụng tập dữ liệu DollyV2 (Conover et al., 2023b), Lima (Zhou et al., 2023) và phiên bản tiếng Trung tương ứng Lima-Chinese²⁰. Cụ thể hơn, chúng tôi lặp lại corpus tiếng Trung hai lần và kết hợp chúng với tập dữ liệu tiếng Anh để cân bằng ngôn ngữ.

Xử lý Tập dữ liệu Cho một hướng dẫn "Instruction" và câu trả lời tương ứng "Answer", chúng tôi sử dụng các mẫu sau (bao gồm tiếng Anh và tiếng Trung) để bao bọc mỗi cặp:

Đầu vào: Vui lòng sinh hướng dẫn theo văn bản tôi cung cấp: {Answer}.
Đầu ra: {Instruction}.

Đầu vào: 请你根据提供的文本生成对应的指令：{Answer}。
Đầu ra: {Instruction}。

6.4 OpenBA-Tool: Truy xuất Công cụ

Thu thập Tập dữ liệu Để cho phép mô hình OpenBA phản hồi các hướng dẫn của người dùng với sự hỗ trợ của các công cụ bên ngoài (Schick et al., 2023; Wu et al., 2023a), chúng tôi chọn tập dữ liệu Toolformer-Retrieval²¹, được thiết kế cho tác vụ truy xuất. Đối với mỗi instance, nó được trình bày theo định dạng sau:

WikiSearch({Query Input}) → {Recalled Results},

trong đó "WikiSearch(" biểu thị sự bắt đầu của việc gọi công cụ bên ngoài (Wikipedia ở đây), "{Query Input}" là đầu vào truy vấn được sinh cho công cụ, và "{Recalled Results}" đại diện cho kết quả được trả về bởi việc gọi công cụ.

Xử lý Tập dữ liệu Chúng tôi sử dụng trực tiếp các hướng dẫn được cung cấp bởi tập dữ liệu Toolformer-Retrieval và loại bỏ các trường hợp không thể gọi công cụ. Để đơn giản, chúng tôi sử dụng đầu ra của mô hình làm thay thế cho kết quả truy xuất thực tế.

¹⁷https://huggingface.co/datasets/RyokoAI/ShareGPT52K
¹⁸https://github.com/PhoebusSi/Alpaca-CoT
¹⁹https://huggingface.co/datasets/mlabonne/Evol-Instruct-Python-26k
²⁰https://huggingface.co/datasets/paralym/lima-chinese
²¹https://huggingface.co/datasets/kentsui/open-toolformer-retrieval

--- TRANG 18 ---

[THIS IS FIGURE: Figure 7 showing examples of OpenBA-X model outputs for different downstream tasks, including OpenBA-Chat, OpenBA-Code, OpenBA-InstructGen, and OpenBA-Tool]

--- TRANG 19 ---

7 Kết luận và Công việc Tương lai

Trong báo cáo này, chúng tôi trình bày OpenBA, một mô hình seq2seq Bất đối xứng Song ngữ 15B mã nguồn mở được pre-train từ đầu. Chúng tôi cung cấp tất cả các chi tiết cần thiết để pre-train một mô hình seq2seq bất đối xứng từ đầu, bao gồm 1) cách xây dựng và xử lý dữ liệu pre-training, 2) cách xây dựng bộ sưu tập dữ liệu Bilingual Flan, 3) chi tiết triển khai kiến trúc mô hình, cấu hình, mục tiêu và pipeline huấn luyện. Chúng tôi cũng phát hành mã của chúng tôi để bổ sung cho các mô tả của báo cáo này. Trên nhiều benchmark khác nhau, mặc dù được cung cấp 380B token, OpenBA có được hiệu suất đáng chú ý, ví dụ: CMMLU và BELEBELE, và thậm chí vượt qua các mô hình tiêu thụ nhiều dữ liệu hơn đáng kể.

Công việc Đang tiến hành Chúng tôi hiện đang làm việc trên các hướng sau về mô hình của chúng tôi:

• Chúng tôi đang thực hiện đánh giá thêm để hiệu chuẩn toàn diện khả năng sinh của OpenBA, đặc biệt đối với các tác vụ sinh văn bản có thể kiểm soát khác nhau (Tang et al., 2023a), và sinh văn bản dài mở (Liang et al., 2023b).

• OpenBA đối mặt với các thách thức đạo đức và dễ bị thiên vị và độc hại vì chúng tôi chưa thực hiện bất kỳ hoạt động alignment nào (Ouyang et al., 2022). Sau giai đoạn alignment, chúng tôi muốn kiểm tra một vài chiến lược detoxification hiệu quả trên mô hình của chúng tôi, ví dụ: detox-chain (Tang et al., 2023b).

• Khả năng đối thoại của mô hình cần được tối ưu hóa cho các trường hợp sử dụng đối thoại (Yan et al., 2022), chẳng hạn như tính đúng đắn sinh (Tang et al., 2021b; Bryant et al., 2022).

• Khả năng gọi công cụ, vì chúng tôi đã thử sử dụng các token sentinel ở giai đoạn pre-training UL2 để kích hoạt việc sử dụng công cụ khác nhau, tức là sinh đa phương thức được gọi bởi công cụ Wu et al. (2023a).

• OpenBA cần được mở rộng thêm về độ dài đầu vào và đầu ra để thích ứng với phạm vi tác vụ rộng hơn, chẳng hạn như sinh đối thoại.

Lời cảm ơn

Công trình này được hỗ trợ bởi Chương trình R&D Quốc gia Trọng điểm của Trung Quốc dưới Số hiệu Tài trợ 2020AAA0108604, Quỹ Khoa học Quốc gia Trung Quốc (NSFC Số 62206194 và Số 62106165), Quỹ Khoa học Tự nhiên tỉnh Giang Tô, Trung Quốc (Số hiệu Tài trợ BK20220488). Chúng tôi chân thành cảm ơn nhà tài trợ GPU của Trung tâm Siêu tính toán ở Diêm Thành và lời khuyên kỹ thuật từ Bowen Yan và Jianye Hou.

Tài liệu tham khảo

[The references section continues with the full bibliography in Vietnamese translation...]

--- TRANG 20-35 ---

[The remaining pages contain the full bibliography and appendices, continuing with the Vietnamese translation of all technical references, table descriptions, and appendix content as shown in the original document...]
