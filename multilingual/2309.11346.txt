# 2309.11346.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2309.11346.pdf
# File size: 1089669 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GECTurk: Grammatical Error Correction and Detection Dataset for
Turkish
Atakan Kara, Farrin Marouf Safian, Andrew Bond, Gözde Gül ¸ Sahin
Computer Engineering Department
Koç University, Istanbul, Turkey
https://gglab-ku.github.io/
Abstract
Grammatical Error Detection and Correction
(GEC) tools have proven useful for native
speakers and second language learners. De-
veloping such tools requires a large amount of
parallel, annotated data, which is unavailable
for most languages. Synthetic data generation
is a common practice to overcome the scarcity
of such data. However, it is not straightfor-
ward for morphologically rich languages like
Turkish due to complex writing rules that re-
quire phonological, morphological, and syn-
tactic information. In this work, we present a
flexible and extensible synthetic data genera-
tion pipeline for Turkish covering more than
20 expert-curated grammar and spelling rules
(a.k.a., writing rules) implemented through
complex transformation functions. Using this
pipeline, we derive 130,000 high-quality par-
allel sentences from professionally edited arti-
cles. Additionally, we create a more realistic
test set by manually annotating a set of movie
reviews. We implement three baselines for-
mulating the task as i) neural machine trans-
lation, ii) sequence tagging, and iii) prefix
tuning with a pretrained decoder-only model,
achieving strong results. Furthermore, we per-
form exhaustive experiments on out-of-domain
datasets to gain insights on the transferabil-
ity and robustness of the proposed approaches.
Our results suggest that our corpus, GECTurk,
is high-quality and allows knowledge trans-
fer for the out-of-domain setting. To encour-
age further research on Turkish GEC, we re-
lease our datasets, baseline models, and the
synthetic data generation pipeline at https:
//github.com/GGLAB-KU/gecturk .
1 Introduction
Grammatical Error Correction (GEC) is among the
well-established NLP tasks with dedicated shared
tasks (e.g., BEA (Bryant et al., 2019)), benchmarks,
and even specific evaluation measures. With the
increasing interest from the community, the field isin constant need of novel writing tools, and meth-
ods, and more importantly, extensions to other lan-
guages.
Recently, there has been an explosion of research
about GEC for high-resource languages, especially
for English (Rothe et al., 2021; Omelianchuk et al.,
2020; Bryant et al., 2019). These recent tech-
niques use two main approaches: formulating the
task as i) neural machine translation, i.e., gener-
ation (Rothe et al., 2021) and ii) token classifi-
cation to detect erroneous tokens (Omelianchuk
et al., 2020). First set of approaches mainly utilize
and engineer vanilla Transformers to generate the
corrected text, while, the second set focuses on en-
gineering a set of errors and transformation rules.
Nonetheless, both formulations require a large set
of parallel corpora containing grammatically cor-
rect and incorrect sentence pairs. Furthermore, the
latter approach additionally requires a highly cu-
rated dataset with annotations for correcting errors
(i.e., location and type of the error). However, con-
structing such a parallel corpus with error annota-
tions is nontrivial—especially for low-resourced
languages with rich morphology like Turkish. The
challenge is due to grammar rules, a.k.a., writing
errors being entangled in several layers, such as
phonology, morphology, syntax, and semantics. As
of today, there are no spelling or grammatical error
datasets, as mentioned by Çöltekin et al. (2023),
with the exception of the dataset introduced by
Arikan et al. (2019).
To address this, we focus on the Turkish Lan-
guage and utilize the official writing rules estab-
lished by the Turkish Language Association1. We
implement corruption, a.k.a. transformation, func-
tions to generate instances that violate a specific
rule, which requires challenging analysis of sen-
tences on several linguistic levels, as well as cura-
tion of specialized lexicons. Then, we generate a
large, synthetic, high-quality annotated corpus by
1https://tdk.gov.tr/arXiv:2309.11346v1  [cs.CL]  20 Sep 2023

--- PAGE 2 ---
applying transformation functions to professionally
edited, modern Turkish articles. In addition to the
transformation functions used for data generation,
we implement and share the reverse-transformation
functions for validating the generated datasets and
developing sequence tagger models, which achieve
state-of-the-art in English.
In addition, we compile a corpus of movie re-
views and manually annotate 300 sentences with
the proposed error types to evaluate the models
in a real-life setting. Furthermore, we design and
implement several baselines using standard neural
machine translation (NMT), sequence tagging and
prefix-tuning. While NMT models are only trained
to generate the corrected sentences, sequence tag-
ging models are trained to tag the tokens with the
error type (if any) and then perform the associated
reverse transformation function on the detected er-
ror to generate the correct text. Finally, we perform
prefix-tuning (Li and Liang, 2021) on large mul-
tilingual language model, mGPT Shliazhko et al.
(2022) for both detection and correction tasks to
test the capacity of more recent techniques.
Our findings indicate that our pipeline approach
using smaller models perform better than employ-
ing larger pretrained models in an end-to-end fash-
ion when it comes to both synthetic and real-world
datasets—particularly for the grammatical error
detection task. Conversely, we observe that pre-
training benefits the models in dealing with more
realistic cases, despite the larger models still falling
behind their simpler counterparts. Our results from
the out-of-domain tests imply that training on the
synthetic dataset gives a strong prior to both smaller
and larger models.
Our contributions can be summarized as follows:
•We propose the first comprehensive, expert-
curated grammatical error schema for Turkish
that covers 25 error types.
•We present a synthetic data generation
pipeline that can be used to create arbitrary
sized datasets, and can be easily extended to
include new grammatical error types or lex-
icons, and can easily be modified to include
custom tools (e.g., morphological analyzer
and disambiguator).
•We present the first large-scale, fine-grained
public dataset for Turkish grammatical cor-
rection and detection, along with a manuallyannotated realistic test set and strong baseline
models.
We make our datasets, baseline models, and
synthetic data generation pipeline publicly avail-
able at https://github.com/GGLAB-KU/
gecturk .
2 Related Work
English GEC Despite having a long history, with
the BEA-2019 Shared Task on Grammatical Error
Correction (Bryant et al., 2019), the GEC com-
munity started employing neural models and for-
mulating GEC as a neural machine translation
task (i.e., translate from grammatically incorrect
to correct sentences), which has become the dom-
inant approach. Another recent approach, GEC-
ToR (Omelianchuk et al., 2020), uses the idea of
reverse transformations, which can be applied to a
list of source tokens [x1, . . . , x n], in order to pro-
duce the desired correct grammar. Their model
is a sequence tagger with a BERT encoder. Each
tag corresponds to a transformation where trans-
formations are applied after the sequence tagging
finishes. In contrast, the gT5 model released by
Xue et al. (2021) is a multilingual mT5 model fine-
tuned on artificially corrupted sentences from the
mC4 corpus and uses a span prediction and classi-
fication task to fix grammatical errors (Rothe et al.,
2021). This does require a lot of additional training
time, since the original mT5 model is not initially
prepared for a similar task. Their model achieves
SOTA results in 4 languages while only training
once.
Turkish GEC Previously, Arikan et al. (2019)
proposed a neural sequence tagger model and a
synthetically generated dataset to correct “de/da”
clitic errors. In Turkish grammar, “de/da” is used
both as a locative suffix and a conjuction meaning
also,toothat is written separately. For instance,
“-de” is a locative suffix in the sentence “Ev de(At
home)”; while used as a conjuction here: “Ben de
geliyorum (I’m coming too)”. Mistakes in using
these clitics are common among native speakers,
often due to some contextual subtleties and oral di-
alect influencing the written language. Öztürk et al.
(2020) combined a contextual word embedding
model, namely BERT, with a sequence tagger to
correct “de/da” clitic errors. Although these errors
are common, they constitute only a small portion
of grammatical errors made by native speakers. In

--- PAGE 3 ---
Uyuyakaldığı için hem
işe gitmedi 
hemde akşamki
yemeğe  gelemeyecekGrammatically
correct
corpus
LexiconsSample
sentenceTransformation
Functions
3. Use lexicons if needed2. Obtain part of speech
informationIf valid, Analyzing validity1. Get correct
sentence
Morphological
Analysis and
Disambiguation4. Extract tokens to modifyTransform
sentence with
specified
probability
Transformed
tokens5. Undo using 
reverse
transformationOutput SentenceFor each transformation,
6. If equal,
keepCompare with 
original sentence
Corpus for
Turkish Grammatical
 Error CorrectionUyuyakaldığı için hem
işe gitmedi hem
de akşamki
yemeğe  gelemeyecek
de
(CONJ)No
lexicons
neededhem de
hemde7. Add to
corpusCombine all
transformations
Uyuya kaldığı için
hem işe
gitmedi hemde
akşamki
yemeğe  gelemiyecekFigure 1: Data generation pipeline. 1) First, a correct sentence is obtained from the grammatically correct corpus.
2) Then, morphological analysis is performed. 3) The validity of the sentence for the transformation function is
checked. If the sentence is eligible, the transformation is applied with some probability p. 4)First, selecting tokens
to modify, 5) then, checking if the reverse transformations can recover the original tokens. 6) If original cannot be
recovered, the sentence is removed. If can be recovered 7) the transformed sentence is added to the corpus.
addition, while there are various forms of this error,
the previous work only considers a few. Our data
generation strategy considers multiple versions of
the “de/da” clitic errors and many more common
grammatical errors.
Parsing-based Approaches While most of the
current approaches focus on reverse transforma-
tions and sequence tagging, there are several stud-
ies that involve the use of parsing techniques.
Flickinger and Yu (2013) create a parsing tree,
and identify malformed parts of the tree to detect
grammatical errors. da Costa (2021) use symbolic
parsers and computational grammars for GEC and
GED. On the same research line, Flickinger and
Packard use bridged analyses combined with pars-
ing to better allow for connecting two phrases in
Head-driven Phrase Structure Grammar (HPSG).
3 Synthetic Data Generation
The overall generation process is given in Fig. 1.
First, we randomly sample from professionally
edited Turkish corpora ( §3.1). Then, sentences
are corrupted—if possible—following the expert-
curated transformation rules explained in §3.2, as
well as the use of a morphological analyzer. Fi-
nally, pairs of grammatically correct and corrupted
sentences are added to the final Turkish GEC cor-
pus following the M2scorer (Dahlmeier and Ng,
2012) (MaxMatch) data format.3.1 Corpus
Our proposed data generation pipeline is built upon
the assumption that all input sentences are gram-
matically correct. Hence, we base our study on
previously compiled newspaper corpora (Diri and
Amasyali, 2003; Amasyalı and Diri, 2006; Can and
Amasyalı, 2016; Kemik NLP Group, 2022) that are
proofread and went through a professional editing
process. The articles are on various topics, includ-
ing politics, sports, and medicine, and have been
written by more than 95 authors for three differ-
ent newspapers; in total, more than 7000 singly
authored documents were collected between 2004-
2012. Once we obtained grammatically correct
source sentences, we performed several preprocess-
ing steps, such as removing duplicates (2.9% of
the combined dataset), ending with 138K unique
sentences.
3.2 Transformation
The Turkish Language Association (TDK)2, a gov-
ernment agency founded in 1932, is responsible for
providing resources to conduct scientific research
on written and oral sources of Turkish. Within this
scope, they specify and maintain a comprehensive
list of publicly available writing rules3. We rely
on this expert-curated list to generate forward and
backward transformation rules, which we refer to
2https://www.tdk.gov.tr/
3https://www.tdk.gov.tr/kategori/
icerik/yazim-kurallari/

--- PAGE 4 ---
asfandf−1respectively. However, the list is long,
and some writing rules are intuitive to native speak-
ers, so that any errors made on these rules sound
abnormal to them. We select the grammar rules
that are most commonly used incorrectly by native
speakers, determined by consulation with Turkish
language experts and filtering the list from TDK
using their feedback. We do not include any rules
that are common for Turkish language learners but
rarely made by native speakers. Table 1 provides
the full list of the transformation rules produced
by this work. The transformations rely on a mor-
phological analyzer, which was essential to get the
transformations right for a morphologically rich
language like Turkish. For more information on
Turkish Morphology, we refer to Oflazer (2014)
and Lewis (1985).
Applying fFor each sampled sentence, first, we
shuffle the list of fs. This ensures that mutually
exclusive transformation functions are applied with
desired frequencies. Then, we iteratively apply
eachfon the sentence given with the pseudo-code
in Algorithm 1. Here, fgets an input sentence
s, morphological analysis of the sentence Ms, an
array of indicators for whether any transformation
has been applied to the word— flags , and param-
eterp∈(0,1). The algorithm, then, iterates over
tokens (or pairs) and checks whether the token has
been transformed. If not, it checks whether the
token(s) are eligible for f. If eligible, we apply f
with the probability p, since not all errors are made
with the same frequency by native speakers.4
Eligibility Check Some official writing rules re-
quire syntactic analysis at the token and sentence
levels. For instance, to apply the transformation
function CONJ_DE_SEP, one must perform mor-
phological analysis and disambiguation to analyze
the part-of-speech tags at the morpheme level. That
is, CONJ_DE_SEP transformation can be applied
only if a “-de/da” morpheme with a CONJUC -
TION part-of-speech tag is found. Additionally,
a small set of rules requires specialized lexicons,
e.g., a list of exceptional foreign words for FOR-
EIGN_R2_EXC. To address the former, we use
a state-of-the-art morphological analyzer Dayanik
et al. (2018), and the lexicons are taken from the
official lists provided by TDK2.
4We choose the probabilities intuitively after an initial
analysis on web corpus and student essays.Uyuyakaldı ˘gı için hem i¸ se gitmedi hem de ak¸ samki
yeme ˘gegelemeyecek .
(Because they overslept, they didn’t go to work and won’t be able to come to
dinner tonight.)
(a)
Uyuya kaldı ˘gı için hem i¸ se gitmedi hemde ak¸ samki
yeme ˘gegelemiyecek .
(b)
S Uyuya kaldı ˘gı için hem i¸ se gitmedi hemde ak¸ samki yeme ˘ge gelemiyecek.
A 0 2|||COMP_VERB_ADJ|||Uyuyakaldı ˘gı|||REQUIRED|||-NONE-|||0
A 6 7|||CONJ_DE_SEP|||hem de|||REQUIRED|||-NONE-|||0
A 9 10|||PRONOUNC_EXC|||gelemeyecek|||REQUIRED|||-NONE-|||0
(c)
Figure 2: The grammatically correct sentence is given
in (a), the transformed version is given in (b), and the
annotation format is given in (c).
Annotation Format We use the standard GEC
annotation format following Ng et al. (2013) and
Bryant et al. (2019). An example annotation is
given in Fig. 2. Here, S and A refer to the ungram-
matical sentence and edit annotations respectively.
Each A contains starting and ending indices, the
error type, the corrected phrase, and the id of the
annotator.
Postprocessing Despite the use of professionally
edited source sentences, there are still some gram-
matically incorrect sentences that slip through. We
detect these cases by taking advantage of a key
property of our reverse transformations: since each
fis reversible, we should see S=f−1(f(S))for
each sentence S. Therefore, at the end of the trans-
formation process, we perform this check on every
generated, grammatically incorrect sentence. If a
sentence fails this check, then we know it is prob-
lematic, and we remove it from the corpus. The
final sentences are thus properly modified in the
desired way, with no unintentional side effects.
3.3 Annotated Corpus
The annotated corpus includes more than 138K
sentences, with 104K error annotations belonging
to 25 error types given in Table 1. In this corpus,
50% of sentences are error free, in order for mod-
els to learn how to detect/correct sentences that
are already grammatically correct. The generative
pipeline controls the frequency of those error types,
aiming to mimic the human error frequencies (see
App. B). As in the dataset of CoNLL-2014 shared
task (Ng et al., 2014), some error types appear
more frequently than others. These frequencies

--- PAGE 5 ---
Algorithm 1 Apply f
Require: s:=sentence, Ms:=morphological analysis, flags, p
Ensure: tags
tags←[ ]
n←Number of tokens in s
fori= 1→ndo
ifflags[i]andis_eligible (s, M s)andflipCoin (p)then
tags.insert("A i{i+ 1}|||ruleID |||sentence [i]|||REQUIRED |||-NONE- |||0")
sentence [i]←transformed token at index i
flags[i]←False
are by the probability parameters p; therefore, the
difference between frequencies of error types is an
intended result. Our dataset is finally split into a
train/val/test set of 70%/15%/15%.
3.4 Curated Test Corpus
For a more realistic test setting, we use movie re-
views from a popular website5shared by Altinok
(2023). After performing sentence tokenization
and deduplication, we use sentences that contain
grammatical errors. To do so, we employ various
techniques: for each rule, we utilize available dic-
tionaries, incorporate regular expressions to iden-
tify specific morphemes that may exhibit errors,
and employ models trained on a synthetic dataset.
Then, our domain expert annotates the sentences
for the proposed error types, following the standard
GEC annotation format. By adhering to this pro-
cedure, we successfully produce a test dataset of
300 sentences, wherein half of the sentences were
grammatically correct and the other half contained
errors.
4 Tasks and Models
In this paper, we consider two tasks: Grammatical
Error Correction (GEC) and Grammatical Error
Detection (GED).
Grammatical Error Correction (GEC) takes
as input a grammatically incorrect sentence and
outputs the corrected version of the sentence. For-
mally, given an input sentence x= (x1,···, xT)
which may contain some grammar mistakes, the
aim is to produce an output sentence y=
(y1,···, yT′)which contains no grammatical er-
rors. Conditions are not imposed on how the model
produces grammatically correct sentences.
Grammatical Error Detection (GED) takes a
slightly different approach to this problem, with the
goal of producing detailed information about the
5www.beyazperde.comerrors in the source sentence. This includes details
about the type of error and the location of the error
in the sentence. Formally, given an input sentence
x= (x1,···, xT), we can represent the problem as
a token-level classification task, where the output is
c= (c1,···, cT), and cirepresents the error type
of token i. Given the knowledge that an error of
typeckoccurred at the location from mton, it is
then possible to apply the corresponding reverse
transformation f−1, and fix the error.
4.1 Models
We introduce three models to evaluate the perfor-
mance of GECTurk: An NMT baseline, a sequence
tagger using BERT (Devlin et al., 2019) pretrained
on Turkish, and mGPT using prefix-tuning. All
models are trained using 1 Nvidia V100 GPU. We
only provide the essential information about the
models here. More details are available in Ap-
pendix A.
NMT Baseline: We train a vanilla transformer
model (Vaswani et al., 2017) for GEC. This choice
is inspired by the most recent shared task on gram-
matical correction (Bryant et al., 2019), where
many of the winning teams used transformer-based
models and modeled the problem as a Neural Ma-
chine Translation (NMT). The training dataset con-
sists of triples {(xi, yi, ai)}N
i=1, where xiis the i-th
input sentence, yiis the corresponding ground truth
corrected sentence, and aiare the annotations. Dur-
ing training, the model receives xias input. Due
to the nature of the formulation, NMT is only used
for correction.
Sequence Tagger: Similar to recent
work (Omelianchuk et al., 2020), we train a
sequence tagging model using a cased BERT en-
coder, pretrained on Turkish text (Schweter, 2020)
with default configurations and additional linear
and softmax layers on the top. The BERT model
uses the WordPiece tokenizer (Wu et al., 2016)
that segments tokens into subwords. Therefore,

--- PAGE 6 ---
Category Rule ID Description f Frequency
-DE/-DA1. CONJ_DE_SEP Conjunction “-de/-da” is writ-
ten separately.Durumu [o ˘gluna da -> o ˘glu-
nada] bildirdi.12962
2. CONJ_DE_VH Conjunction “-de/-da” must fol-
low the vowel harmonyÇok [da -> de] iyi olmu¸ s. 101
3. CONJ_DE_AR Conjunction “-de/-da” does not
follow phonetic assimilation
rules.Sınıf [da -> ta] temizlendi. 99
4. YADA “-de/-da” written together with
the word “ya” is always written
separately.Sen [ya da -> yada] o buradan
gidecek.472
5. CONJ_DE_APOS Conjunction “-de/-da” cannot
be used with an apostrophe.[Ay¸ se de -> Ay¸ se’de] geldi. 10859
6. CASE_DE Suffix “-de/-da” is written adja-
cent.[Evde -> Ev de] hiç süt
kalmamı¸ stı.37462-KI7. CONJ_KI_SEP Conjunction “-ki” is written sep-
arately.Bugün öyle çok [yorulmu¸ s ki ->
yorulmu¸ ski] hemen yattı.1817
8. CONJ_KI_EXC On some exceptional instances,
by convention, the conjunction
“-ki” is written adjacent.[Belki -> Bel ki], [oysaki ->
oysa ki], [çünkü -> çünki]1395FOREIGN9. FOREIGN_R1 Words that start with double
consonants of foreign origin are
written without adding an “-i”
between the letters.[gram -> gıram] 307
15. FOREIGN_R2 Some foreign origin words un-
dergo consonant assimilation[sebebi -> sebepi] 327
16. FOREIGN_R2_EXC Exceptions to the FOR-
EIGN_R2 rule[evrakı -> evra ˘gı] 422BISYL13. BISYLL_HAPL_VOW Some bisyllabic words undergo
haplology when they get a suf-
fix starting with a vowel.[a˘gzı -> a ˘gızı] 1567
14. BISYLL_HAPL_VOW_EXC Exception to previous rule [içeride -> içerde] 866LIGHT VERB17. LIGHT_VERB_SEP Light verbs such as “etmek,
edilmek, eylemek, olmak, olun-
mak” are written separately in
case of no phonological assimi-
lation[arz etmek -> arzetmek] 460
18. LIGHT_VERB_ADJ Light verbs are written adjacent
in case of phonological assimi-
lation e.g., liaison[emretti -> emir etti] 547COMPOUND20. COMP_VERB_ADJ Compound words formed by
knowing, giving, staying, stop-
ping, coming, and writing are
written adjacent if they have a
suffix starting with -a, -e, -ı, -i,
-u, -ü.[uyuyakalma -> uyuya kalmak],
[gidedurmak -> gide durmak],
[çıka gelmek -> çıka gelmek]7840SINGLE22. PRONOUN_EXC Traditionally, some pronouns
are written adjacent.[hiçbir -> hiç bir], [herhangi ->
her hangi]3867
23. SENT_CAP The first letter of the sentence is
capitalized.[Onlar -> onlar] geldi. 2613
24. CAPPED Some Arabic and Persian orig-
inated words are written with
capped letters.[kâ˘gıt -> ka ˘gıt], [karargâh ->
karargah]834
25. ABBREV Grammatical rules for abbrevi-
ations, such as adding suffixes
to abbreviations, punctuations
with abbreviations etc.[Alm. -> Alm], [THY’de ->
THY’da], [cm’yi -> cm’ye]359
12. PRONOUNC_EXC Unlike its pronunciation, verbs
ending with “-a/-e” do not mu-
tate when they get a suffix other
than “-yor”[ba¸ slayaca ˘gım -> ba¸ slıyaca ˘gım] 12750
Table 1: Selected list of writing rules introduced by Turkish Language Association. f[arg1−> arg 2]refers to the
transformation function where the correct and corrupted surface forms are given with arg1andarg2respectively.

--- PAGE 7 ---
each sentence in the dataset is first tokenized into
subwords and passed into the BERT encoder. We
only hold the first subword’s representation for
words with multiple subword tokens. Then, the
encoder’s representations are linearly transformed
and passed to the softmax layer to classify into
possible error types described in Table 1 or no error.
The model is finetuned for token classification
objective using cross-entropy loss. The loss is
similar to Eq. 1 located in the Appendix, except
ranging over the number of possible error types.
The advantage of this model is the ability to per-
form error detection easily, as opposed to simply
error correction. Correction is simply performed
with reverse transformations as described previ-
ously.
Prefix Tuning: Inspired by the recent successes
of prefix tuning (Li and Liang, 2021) as an al-
ternative to model fine-tuning, we use Open-
Prompt (Ding et al., 2022) to perform prefix tuning
on mGPT (Shliazhko et al., 2022). Despite be-
ing multilingual and primarily focused on other
languages, mGPT achieves encouraging results on
morphologically rich languages (Acikgoz et al.,
2022). In prefix tuning, we append Ntrainable
(soft) tokens to the front of each input. Therefore,
given input x= (x1,···, xT), the new input be-
comes x= (s1,···, sN, x1,···, xT), where the
si’s are the added artificial tokens. We then opti-
mize only these tokens during training, while leav-
ing the original model frozen. We primarily utilize
mGPT for sequence generation, where the model
simply outputs the grammatically correct sentence.
We use the standard sequence generation prompt
provided by OpenPrompt, due to its recent suc-
cess (Acikgoz et al., 2022), and use teacher forcing
during training. When the model detects multiple
types of errors in the same sentence, all the detec-
tion information is generated on the same line.
Here, we model both correction and detection
tasks in the same sequence generation approach,
where the corrected sentence is first generated, and
then information about the violated rule, and the
location of this error is generated at the end of the
sentence. This allows for one trained model to
output both results. In order to train this correctly,
the target sentence was appended with the details
of the error type and location, and used for loss
calculations. An example is provided in Fig 3.
Ne yani Atatürk kadına 
ilgi duymuyor muydu? 
;11;5;6 S Ne yani Atatürk kadına ilgi 
duymuyormuydu? 
A 5 6|||rule_11|||duymuyor 
muydu?|||REQUIRED|||-NONE-||
|0M-GPT Figure 3: Example output of the mGPT model. The
first line performs the grammatical error correction, and
subsequent lines allow for detection. The annotations
are included for comparison with model outputs, but are
not actually provided to the model.
Dataset #s #a %e #e
(Ours) GECTurk 138K 104K 49.7% 25
(Ours) CuratedTest 300 227 50% 25
BOUN 10K 6K 50.0% 2
BOUN complex 102 105 100.0% 2
Table 2: Datasets used for training and evaluation. #s:
sentences, #a: annotations, %e: percentage of erroneous
sentences, #e: number of errors types
5 Experimental Setup
5.1 Datasets
The list of datasets and their statistics are given
in Table 2. GECTurk and MovieReview datasets
are already described in §3.3 and §3.4 accordingly.
The BOUN dataset (Arikan et al., 2019) is a rela-
tively smaller dataset of 15K training and 2K test
sentences, containing only 2error types. It also
includes a complex split, a list of 100sentences
that are mentioned to be extra challenging by the
authors.
5.2 Evaluation
Grammatical Error Correction Following the
Omelianchuk et al. (2020) and Bryant et al.
(2019), we report Precision ( P), Recall ( R),
andF0.5scores using the M2(MaxMatch)
scorer (Dahlmeier and Ng, 2012).
Grammatical Error Detection To allow for
a fair comparison with the BOUN dataset from
Arikan et al. (2019), we use the same metrics,
namely Precision ( P), Recall ( R), and F1. Since
the task is modeled as a sequence tagging prob-
lem, this aligns with the standard evaluation for se-
quence tagging, such as in Huang et al. (2015). For
all GED results, we report macro metrics, which are
computed by taking unweighted average of each
classes’ result. We use macro metrics over micro
ones since distribution of grammatical errors types
made by humans are imbalanced. To calculate
these scores, we use SeqEval (Nakayama, 2018),

--- PAGE 8 ---
a common library for evaluating sequence tagging
tasks and use one tag for each error type.
6 Experiments and Results
We conduct a set of experiments to investigate the
performance of the proposed baselines and the dif-
ficulty level of the introduced dataset: GECTurk.
To do so, we train the baseline models using the
experimental setup explained in Section §5 using
three different fixed seeds. The mean and standard
deviation of their performances on both GEC and
GED (if applicable) are given in Table 3. As can
be seen, both SeqTag and mGPT provide excep-
tionally strong results over 0.94F0.5score for the
GEC task, compared to the NMT baseline model.
On the other hand, the detection task is performed
more competently by SeqTag—as expected—than
mGPT, which again achieves around 0.90F1score.
Moreover, the experiment on the effect of dataset
sizes shows that the proposed dataset is indeed
more challenging than existing ones, as the mod-
els demonstrate a steeper learning curve due to the
larger number of error types. More information
on the dataset size experiments can be found in
Appendix C.
The reason why successful detection does not al-
ways translate into successful correction is because
erroneous edits undermine the grammatical accu-
racy of the sentence, even when the grammatical
error is successfully detected. Here, the M2scorer
effectively identifies this anomaly and explains the
decrease in the correction scores. It is also worth
noting that GED and GEC are two separate tasks,
and models handle them differently. For exam-
ple, generative models, such as mGPT, generate
both the corrections and detections by predicting
the next tokens, so there isn’t necessarily a strong
correlation between what is generated for each of
them. On the other hand, SeqTag uses a pipeline ap-
proach, where it detects the errors first, then applies
reverse transformations to fix them. Hence there
is a stronger correlation between the detection and
correction performances for SeqTag, as expected.
In order to test whether the performance of our
models would transfer to different domains, we
perform zero-shot experiments on the curated test
set from the movie domain. We use the check-
points that performed highest on our synthetic test
set, and evaluate without any additional training
on the hand-annotated corpus as given in Table 3,
second row. For the detection task, SeqTag per-forms similarly to synthetic setting, while mGPT’s
performance increases dramatically , proving the
importance of being exposed to real-life data dur-
ing pretraining. However, SeqTag still outperforms
mGPT by a large margin due to its classification
objective. On the other hand, both models perform
significantly worse on the correction task for the
currated test data compared to the synthetic setting,
suggesting a larger room for improvement on this
more challenging test set.
6.1 Knowledge Transfer
Next, we investigate the transfer capacity of our
models on unseen datasets using a different set of
errors (i.e., mostly a subset) originally introduced
to our models. We first evaluate our pretrained
models on the BOUN (Arikan et al., 2019) stan-
dard and complex test splits to gain insights into
their zero-shot ability, given with Table 4, first row.
Surprisingly, our best model, SeqTag, achieves 0,80
F1that is on-par with state-of-the-art for the stan-
dard split. It also surpasses state-of-the-art accu-
racy scores on the complex split by a large mar-
gin together with the mGPT model. This result
suggests that, the error type knowledge is mostly
transferrable to other domains. Similar to our re-
sults on GECTurk, mGPT scores considerably low
on detection compared to SeqTag. However, the
performance of mGPT is higher on BOUN dataset
due to the small number of error types that are rel-
atively more balanced. We note that despite the
claims made by the authors of the BOUN dataset,
our results suggest no additional complexity in the
“complex” split as shown in Table 4, second row.
Finally, we investigate the effectiveness of our
general approach by training our proposed models
from scratch on the BOUN (Arikan et al., 2019)
training split, given in Table 4, BOUN Full Train-
ing. For this setup, the NMT model was not able to
fully converge, and just produced unhelpful noise,
hence, shown as 0.
Following our previous results, SeqTag achieves
F1score of 0.91, surpassing the state-of-the-art by
0.04pp, and its zero-shot performance by a large
margin ( 0.11pp). This suggests that, pipeline ap-
proach is able to transfer a considerable amount of
knowledge, however, there is still a large gap that
can be compensated by directly training on the ac-
tual domain and error types. On the other hand, the
high scores provide cues for the strength of the pro-
posed model. Surprisingly, prefix tuning of mGPT

--- PAGE 9 ---
GECTurk
Detection Correction
P R F1 P R F0.5
NMT - - - 0.50 ±0.01 0.84 ±0.01 0.55 ±0.01
SeqTag 0.90 ±0.003 0.90 ±0.013 0.90 ±0.006 0.98 ±0.001 0.98 ±0.001 0.98 ±0.001
mGPT 0.52 ±0.02 0.38 ±0.004 0.41 ±0.01 0.95 ±0.01 0.92 ±0.03 0.94 ±0.01
Curated Test Data
NMT - - - 0.31 0.62 0.35
SeqTag 0.94 0.87 0.89 0.85 0.80 0.84
mGPT 0.73 0.52 0.59 0.75 0.61 0.72
Table 3: Detection and Correction results of the baselines on GECTurk (in-domain) and curated test dataset (out-of-
domain).
BOUN Zero-Shot
Detection Correction
P R F1 Acc P R F0.5
NMT - - - - 0.19 0.56 0.22
SeqTag 0.91 0.72 0.80 0.99 0.81 0.63 0.77
mGPT 0.60 0.56 0.58 0.97 0.75 0.67 0.73
BOUN Complex Split Zero-Shot
NMT - - - - 0.71 0.73 0.72
SeqTag 0.99 0.93 0.96 0.99 0.99 0.93 0.98
mGPT 0.61 0.96 0.90 0.93 0.79 0.75 0.78
BOUN Full Training
NMT - - - - 0 0 0
SeqTag 0.97 0.86 0.91 0.99 0.97 0.86 0.94
mGPT 0.61 0.48 0.53 0.97 0.85 0.73 0.82
BOUN Results 0.92 0.82 0.87 0.71*- - -
*This result is for BOUN Complex Split Zero-Shot task.
Table 4: Performance metrics of various models on the
BOUN dataset. The table is divided into three sections:
models trained on the GECTurk dataset and evaluated
zero-shot on two different BOUN splits, and models
exclusively trained and evaluated on BOUN.
model directly on the BOUN training dataset does
not increase the performance compared to the zero-
shot setting. This suggests two things: i) synthetic
dataset such as ours, GECTurk, provides quality
prior knowledge on the Turkish grammatical error
types and ii) pretrained models have a considerably
larger transfer capacity compared to training from
scratch, as expected. Furthermore, for languages
where the error types are mostly identified and can
be fixed by a set of rules, a pipeline approach such
as SeqTag proves more effective, efficient and ro-
bust.
7 Conclusion and Future Work
In this work, we have presented an annotated
dataset for Grammatical Error Correction (GEC)
and Detection (GED), GECTurk, containing more
than 20 Turkish writing error types proposed byTurkish language experts. We have also intro-
duced a flexible and extensible data generation
pipeline that can be used to create a synthetic
dataset from grammatically correct sentences. We
used this pipeline to create a large-scale dataset us-
ing multiple opinion columns from Turkish news-
papers. In addition, we have manually constructed
a more challenging test set by annotating the movie-
reviews with the proposed error types.
Finally, we implemented a diverse set of strong
baseline models, by training from scratch, fine-
tuning, or if applicable, using prefix tuning. Our
results show that simpler models focusing on the
smaller problem of detecting the error types outper-
form large pretrained models on both the synthetic
and real-life datasets, especially for the detection
task. On the other hand, we observe that pretrain-
ing helps the models to handle more realistic cases,
even though they still lag behind the simpler mod-
els. Our out-of-domain results suggest that training
on the synthetic data gives a strong prior to both
smaller and larger models.
Acknowledgements
This work has been supported by the Scien-
tific and Technological Research Council of
Türkiye (TÜB ˙ITAK) as part of the project “Auto-
matic Learning of Procedural Language from Natu-
ral Language Instructions for Intelligent Assistance”
with the number 121C132. We also gratefully ac-
knowledge KUIS AI Lab for providing computa-
tional support. We thank our anonymous reviewers
and the members of GGLab who helped us improve
this paper.
Limitations
There are a few key limitations to our work. One
key issue is that mGPT is very computationally

--- PAGE 10 ---
intensive to work with, even when only doing pre-
fix tuning. This prevented us from training un-
til fully converged, and instead only opted for 1
epoch. Another limitation is that the data genera-
tion pipeline is very time-consuming as it requires
the use of a morphological analyzer. This prevents
the pipeline from being used for very large-scale
datasets. Additionally, the requirements for hand-
crafted rules and reverse transformations slows the
speed at which new rules can be added, and which
rules can even be added.
Another important limitation is the necessity of
dictionaries to handle exceptions to grammatical
rules. Words in Turkish that have been borrowed
from other languages (notably Persian, French, and
Arabic) tend to not align with the normal grammar
rules, and thus require special lists of exceptions.
While we aimed to include as many as possible, it
is definitely possible that we missed some, which
can lead to rare edge cases where our pipeline
fails. While dictionaries do allow for incorporat-
ing learned knowledge directly into the process,
and is certainly an invaluable part of our pipeline,
these edge cases can cause problems during dataset
generation, so should be considered a limitation.
Ethical Considerations
Transformer-based large language models such as
(Vaswani et al., 2017; Shliazhko et al., 2022; Rothe
et al., 2021) are very successful, but they also have
some ethical concerns. First, the model is highly
dependent on the dataset it was pre-trained on. It
is possible that the dataset contained certain biases,
such as racism or sexism, which will later be passed
to the model outputs. While this is an important
detail to focus on, our work does not focus much on
actually training these models. We perform training
using GECTurk, which is collected from opinion
columns, as well as the BOUN dataset, which is
already publicly available. Due to the publication
of these columns in reputable Turkish newspapers,
they contain less bias than the average document
collected online. While the pre-training procedure
itself may have introduced some biases, we are
unable to handle these in our work. While these
problems are inherent to all deep learning models,
we emphasize transformer models here due to our
model choices.
Another possible ethical issue is the misuse of
grammatical correction models for cheating. By
having a model that can automatically detect andcorrect grammatical errors, students can more eas-
ily use these to score better than normal on assign-
ments and exams, with less time invested. Not only
is this bad for the student’s learning, but it also
affects others who can be negatively impacted by
the student’s artificial success.
Despite the concerns about misuse or inherent
biases in the model, we believe that grammatical
error correction models are more beneficial than
harmful. Many people, from authors and writ-
ers, to language learners, can benefit from having
grammar corrections. By introducing a dataset and
demonstrating models on Turkish, an under-served
language in the NLP community, more people will
be able to take advantage of this, similar to the
many existing tools for English.
References
Emre Can Acikgoz, Tilek Chubakov, Muge Kural,
Gözde ¸ Sahin, and Deniz Yuret. 2022. Transformers
on multilingual clause-level morphology. In Proceed-
ings of the The 2nd Workshop on Multi-lingual Rep-
resentation Learning (MRL) , pages 100–105, Abu
Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.
Duygu Altinok. 2023. A diverse set of freely avail-
able linguistic resources for Turkish. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 13739–13750, Toronto, Canada. Association
for Computational Linguistics.
M. Fatih Amasyalı and Banu Diri. 2006. Automatic
Turkish text categorization in terms of author, genre
and gender. In Natural Language Processing and
Information Systems , pages 221–226, Berlin, Heidel-
berg. Springer Berlin Heidelberg.
Ugurcan Arikan, Onur Güngör, and Suzan Uskudarli.
2019. Detecting clitics related orthographic errors
in Turkish. In Proceedings of the International Con-
ference on Recent Advances in Natural Language
Processing, RANLP 2019, Varna, Bulgaria, Septem-
ber 2-4, 2019 , pages 71–76. INCOMA Ltd.
Christopher Bryant, Mariano Felice, Øistein E. Ander-
sen, and Ted Briscoe. 2019. The BEA-2019 shared
task on grammatical error correction. In Proceedings
of the Fourteenth Workshop on Innovative Use of NLP
for Building Educational Applications , pages 52–75,
Florence, Italy. Association for Computational Lin-
guistics.
Ender Can and Mehmet Fatih Amasyalı. 2016.
Text2arff: A text representation library. In 2016 24th
Signal Processing and Communication Application
Conference (SIU) , pages 197–200.

--- PAGE 11 ---
Çagri Çöltekin, A. Seza Do ˘gruöz, and Özlem Çetinoglu.
2023. Correction to: Resources for Turkish natural
language processing: A critical survey. Lang. Resour.
Evaluation , 57(1):489.
Luís Morgado da Costa. 2021. Using rich models of lan-
guage in grammatical error detection . Ph.D. thesis,
Nanyang Technological University.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
568–572, Montréal, Canada. Association for Compu-
tational Linguistics.
Erenay Dayanik, Ekin Akyürek, and Deniz Yuret. 2018.
Morphnet: A sequence-to-sequence model that com-
bines morphological analysis and disambiguation.
CoRR , abs/1805.07946.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers) ,
pages 4171–4186. Association for Computational
Linguistics.
Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen,
Zhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022.
Openprompt: An open-source framework for prompt-
learning. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2022 - System Demonstrations, Dublin, Ireland,
May 22-27, 2022 , pages 105–113. Association for
Computational Linguistics.
Banu Diri and Mehmet Fatih Amasyali. 2003. Auto-
matic author detection for Turkish texts.
Dan Flickinger and Woodley Packard. Robust parsing in
hpsg: Bridging the coverage chasm. Poster presented
at the 22nd International Conference on HPSG. 2015.
Dan Flickinger and Jiye Yu. 2013. Toward more preci-
sion in correction of grammatical errors. In CoNLL
Shared Task .
Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR , abs/1508.01991.
Kemik NLP Group. 2022. Our datasets.
http://www.kemik.yildiz.edu.tr/
data/File/2500koseyazisi.rar . Online;
accessed 26-November-2022]".
Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR ,
abs/1412.6980.G. Lewis. 1985. Turkish Grammar . Oxford University
Press.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th Inter-
national Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , abs/2101.00190.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 . OpenRe-
view.net.
Hiroki Nakayama. 2018. seqeval: A python framework
for sequence labeling evaluation. Software available
from https://github.com/chakki-works/seqeval.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task , pages 1–14,
Baltimore, Maryland. Association for Computational
Linguistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
InProceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1–12, Sofia, Bulgaria. Association for
Computational Linguistics.
Kemal Oflazer. 2014. Turkish and its challenges for
language processing. Lang. Resour. Evaluation ,
48(4):639–653.
Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem
Chernodub, and Oleksandr Skurzhanskyi. 2020.
GECToR – grammatical error correction: Tag, not
rewrite. In Proceedings of the Fifteenth Workshop
on Innovative Use of NLP for Building Educational
Applications , pages 163–170, Seattle, WA, USA →
Online. Association for Computational Linguistics.
Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebas-
tian Krause, and Aliaksei Severyn. 2021. A simple
recipe for multilingual grammatical error correction.
InProceedings of the 59th Annual Meeting of the As-
sociation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 2: Short Papers) , pages 702–707,
Online. Association for Computational Linguistics.
Stefan Schweter. 2020. Berturk - bert models for Turk-
ish.
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova,
Vladislav Mikhailov, Anastasia Kozlova, and Tatiana
Shavrina. 2022. mGPT: Few-shot learners go multi-
lingual. CoRR , abs/2204.07580.

--- PAGE 12 ---
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,
Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner, Apurva Shah, Melvin Johnson, Xiaobing
Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,
Taku Kudo, Hideto Kazawa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason
Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
Greg Corrado, Macduff Hughes, and Jeffrey Dean.
2016. Google’s neural machine translation system:
Bridging the gap between human and machine trans-
lation. CoRR , abs/1609.08144.
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and
Colin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 483–498, On-
line. Association for Computational Linguistics.
Hasan Öztürk, Alperen De ˘girmenci, Onur Güngör, and
Suzan Uskudarli. 2020. The role of contextual word
embeddings in correcting the ‘de/da’ clitic errors in
Turkish. In 2020 28th Signal Processing and Com-
munications Applications Conference (SIU) , pages
1–4.

--- PAGE 13 ---
A Model Details
A.1 NMT Baseline
For tokenization, we used BerTurk-cased
(Schweter, 2020) tokenizer, passed to the NMT
model. The transformer model has 6 encoders
with embedding size 512, 6 decoder layers, and
8 heads. A dropout of 0.1 is used directly after
the positional embeddings. For training, an Adam
(Kingma and Ba, 2014) optimizer with β1= 0.9,
β2= 0.98, and ε= 1e−9, and a learning rate
of1e−4is used. We used batch size of 32, and
trained the model for 100 epochs on a single
V100. We use a standard cross-entropy loss during
training, as follows:
LGEC(ˆy, y) =−NX
n=1VX
c=1logexp(ˆyn,c)yn,cPV
i=1exp(ˆyn,i)
(1)
Here, Nis the batch size, Vis the number of error
classes, xis the model output, and yis the target.
For the data size experiments, we used the same
architecture but with slightly different hyperparam-
eters. For both the 75% and 100% experiments, the
model was trained for 100 epochs. For the 50%
experiment, we only trained for 50 epochs. When
training 10% and 25%, the Adam optimizer is used
with the same βvalues, a learning rate of 5e−4,
and a weight decay of 1e−4, for 100 epochs. The
zero-shot testing on the BOUN (Arikan et al., 2019)
dataset is tokenized with the same tokenizer, and
the best pre-trained model from GECTurk is used
for evaluation.
A.2 Sequence Tagger
For training, we used the AdamW (Loshchilov and
Hutter, 2019) optimizer for 3epochs, using batch
size16, learning rate 2e−5, weight decay 0.01,
β1= 0.9, and β2= 0.999.
A.3 Prefix Tuning
We used the standard mGPT tokenizer and the
OpenPrompt prefix tuning template. All experi-
ments use 5 soft tokens at the beginning. Teacher
forcing is used during training, and both the cor-
rection and detection tasks are formulated as a se-
quence generation problem. Following the settings
from Acikgoz et al. (2022), we don’t use weight
decay for the bias and LayerNorm weights. The
AdamW optimizer is used, with an initial learning
rate of 5e−5, linearly decaying to 0over the entire
training. We clip the norm of the gradient at 1.0.
Figure 4: Number of sentences with each writing rule
type.
Due to the computational requirements of mGPT,
we only train on GECTurk for a single epoch on
all experiments. However, on the smaller BOUN
dataset, we train for 5 epochs. For inference, we
also follow the hyperparameters from Acikgoz et al.
(2022), using a temperature of 1.0, top p of 0.9, no
repetition penalty, and a beam search of 5 beams.
For all experiments, a batch size of 3was used. The
max sequence length, including soft tokens, is set
to 512.
B GECTurk Error Frequencies
Fig. 4 shows the frequencies of each error type in
GECTurk dataset.
C Effect of Dataset Size
In order to obtain a better understanding of how
important the dataset size is for this task, we con-
ducted training on 1,10,25,50,75, and 100per-
cent of GECTurk and evaluated each model using
the same evaluation measures. Fig. 5 shows how
the performance of the models vary with more train-
ing data. NMT reaches its top point with around
75% of the training data, while SeqTag and mGPT
achieve high F0.5scores with 25% of the training
split. However, as discussed before, correction
scores can be misleadingly high, since high fre-
quency and easy to correct errors will push the re-
sults much higher. Hence we also plot the F1 scores
for the detection task both on TurkishGEC and
BOUN datasets in Fig. 6. The plot shows that the
GECTurk dataset is richer than the BOUN, since
SeqTag and mGPT F1 performances are much
steeper on the former.

--- PAGE 14 ---
Figure 5: Correction performance of each model trained
on varying sizes of GECTurk
Figure 6: F1score performance of SeqTag and mGPT
on GECTurk and Boun training splits.
