# 2212.09811.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2212.09811.pdf
# File size: 740137 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Memory-efficient NLLB-200: Language-specific Expert Pruning of a
Massively Multilingual Machine Translation Model
Yeskendir Koishekenov∗1,2Alexandre Berard1Vassilina Nikoulina1
1NA VER LABS Europe
2University of Amsterdam
{first.last}@naverlabs.com
yeskendir.koishekenov@student.uva.nl
Abstract
The recently released NLLB-200 is a set of
multilingual Neural Machine Translation mod-
els that cover 202 languages. The largest model
is based on a Mixture of Experts architecture
and achieves SoTA results across many lan-
guage pairs. It contains 54.5B parameters and
requires at least four 32GB GPUs just for in-
ference. In this work, we propose a pruning
method that enables the removal of up to 80%
of experts without further finetuning and with
a negligible loss in translation quality, which
makes it feasible to run the model on a single
32GB GPU. Further analysis suggests that our
pruning metrics can identify language-specific
experts.
1 Introduction
The Transformer (Vaswani et al., 2017) has be-
come the dominant modeling paradigm in Natural
Language Processing tasks. Many subsequent ad-
vances in the field came from increasing the com-
putational budget, training data, and model size.
Neural Machine Translation was not an exception,
where massively multilingual NMT (Aharoni et al.,
2019; Fan et al., 2021; Tang et al., 2020; Zhang
et al., 2020) demonstrated promising results, while
attempting to overcome the curse of multilinguality
(Conneau et al., 2019) by scaling up model size.
However, increasing the parameter size exacer-
bates the cost of training (Yang et al., 2019; Strubell
et al., 2019; Patterson et al., 2021) and hurts the
memory footprint and inference latency (Dai et al.,
2019; Fan et al., 2021; Wang et al., 2022). Sparsely-
gated Mixture-of-Experts (MoE) models are an ef-
ficient alternative to dense models (Lepikhin et al.,
2020; Fedus et al., 2021; Riquelme et al., 2021).
For example, Du et al. (2022) demonstrates that an
MoE language model results in a 7x larger model
compared to GPT-3, but requires only 30% of its
*Work done during an internship at NA VER LABS Europeenergy for training and half of its FLOPs at infer-
ence.
Mixture-of-Experts models are neural networks
whose set of parameters is partitioned into experts.
Contrary to dense models, where all network pa-
rameters are used for every input, an MoE model
activates different parts of the network, the experts,
depending on the input, which is typically done
by a gating mechanism at the token level. MoE
models are computationally efficient due to expert
parallelism (Fedus et al., 2021) across a large num-
ber of GPUs, by having each GPU hold a subset of
all experts and communicate with the other GPUs
when it needs expert outputs for its local batch.
In NLLB-2001(Costa-jussà et al., 2022), a load
balancing regularizer in the objective function
(Shazeer et al., 2017) promotes equal distribution
of the tokens across experts. This encourages the
model to use all the experts and ensures that all
GPUs are used equally for the sake of computa-
tional efficiency. However, considering a large
number of experts, it does not guarantee that all
experts will be equally activated for a particular
pair of languages at inference. It raises a research
question: are there language-specific experts in
multilingual MoE models? If this is the case, we
may be able to prune such models without loss of
translation quality for the language pairs of our
interest. Reducing memory usage would be use-
ful for a model like NLLB-200, which normally
requires at least four 32GB GPUs at inference.
In this work, we define metrics to assess the
importance of each expert and prune the least im-
portant experts at inference. We aim to avoid fine-
tuning because of its computational cost. In an
ideal scenario, we would like to be able to iden-
tify the important experts in an MoE model so
that practitioners can deploy large models, such as
NLLB-200, on a single GPU. We summarize our
1In what follows, NLLB-200 refers to the 54.5B-parameter
MoE NLLB model, unless specified otherwise.arXiv:2212.09811v3  [cs.CL]  7 Jul 2023

--- PAGE 2 ---
main contributions as follows:
•We propose a pruning strategy that can remove
80% of experts in the NLLB-200 model with-
out further finetuning and with a negligible
loss in translation quality;
•We find that the decoder experts can be pruned
more aggressively than the encoder experts;
•We show the emergence of language-specific
experts in the NLLB-200 model;
•We demonstrate that the important language-
specific experts in the decoder are shared be-
tween linguistically related languages;
•We release the ids of the pruned experts, along
with other experts’ gathered statistics so that
anyone with a single 32GB GPU can use
NLLB-200 at inference.2
2 Related work
The concept of Mixture-of-Experts models in ma-
chine learning dates back to the works of Jacobs
et al. (1991); Jordan and Jacobs (1994). Most re-
cent versions were inspired by Shazeer et al. (2017),
who achieved state-of-the-art language modeling
and translation results with the largest model at
that time. Combined with the Transformer model,
MoE models grew in popularity (Lepikhin et al.,
2020; Fedus et al., 2021). Beyond natural language
processing, MoE models showed a large success in
computer vision (Puigcerver et al., 2020), speech
recognition (You et al., 2021), multi-modal learn-
ing (Mustafa et al., 2022), and diffusion models
(Feng et al., 2022; Balaji et al., 2022) to name a
few. For a more detailed survey of MoE models,
we refer readers to Yuksel et al. (2012) and Fedus
et al. (2022).
Despite the recent successes, large MoE mod-
els require a lot of memory and the contribution
(or roles) of experts is under-explored. Chen et al.
(2022) showed that the contributions of experts of
a pre-trained MoE model in different tasks such
as MNLI, CoLA, and SQuAD are quite differ-
ent. Moreover, they converted a large sparse MoE
model pre-trained on a general task to a single-
expert dense model by fine-tuning the most ‘pro-
fessional’ expert and dropping the other experts. It
demonstrates that experts do not contribute equally
to the performance and some are more important
than others. Zoph et al. (2022) also studied dif-
2https://europe.naverlabs.com/
research/natural-language-processing/
nllb-200-expert-pruningferent expert specializations such as sentinel to-
kens, punctuation, conjunctions and articles, and
even languages. They concluded that experts in
the encoder exhibit specialization, in contrast to
the decoder, but not by language. According to the
authors, their mechanism of token routing and load
balancing prevents language specialization.
Kudugunta et al. (2021) train study routing
mechanisms at different levels of granularity and
show that task-level experts (i.e., per language)
can achieve similar performance as token-level ex-
perts. However, this work assumes that the model
is trained this way, while our own work attempts to
prune an existing token-level MoE model at infer-
ence without re-training it.
There have been a number of attempts to com-
press existing massively multilingual NMT models
(Costa-jussà et al., 2022; Mohammadshahi et al.,
2022b,a). However, to the best of our knowledge,
none of them explicitly studied expert pruning and
the emergence of language-specific experts in a
large MoE model like we do. There has been a
related line of works on pruning attention heads in
transformer models (Michel et al., 2019; V oita et al.,
2019), demonstrating linguistically-interpretable
roles of attention heads (V oita et al., 2019; Jo and
Myaeng, 2020) and the emergence of language-
specific attention heads (Kim et al., 2021b; Held
and Yang, 2022). Understanding the role of atten-
tion heads helps carefully remove the least impor-
tant ones without damage to translation quality.
Closest to our work, Kim et al. (2021a) tried to
prune a machine translation MoE model by keeping
the most activated experts,3but did not manage to
preserve performance without further fine-tuning.
Even though it has been shown that multilin-
gual NMT models benefit from a larger number
of experts (Costa-jussà et al., 2022), to the best
of our knowledge, our work is the first to study
whether any language-specific experts emerge in
a massively multilingual Mixture-of-Expert model
for NMT, and how can redundant (or non-relevant)
experts be pruned.
3 Background
3.1 Mixture-of-Experts models
Sparsely-gated Mixture-of-Experts (MoE) models
activate a subset of their parameters per input to-
ken, contrary to dense models, where the entire
network is used for each input token. Therefore,
3Equivalent to our activity pruning metric.

--- PAGE 3 ---
the total amount of parameters can be significantly
increased because the computation cost per token
becomes only proportional to the size of the acti-
vated sub-network, not the total model size. An
increased number of parameters unlocks signifi-
cant representational capacity. Allocating different
devices for different experts and running them in
parallel (i.e., expert parallelism, Fedus et al., 2021),
in combination with data parallelism makes MoE
computationally efficient and highly scalable (Fe-
dus et al., 2021; Lepikhin et al., 2020).
In the MoE Transformer models proposed by
Lepikhin et al. (2020), the FFN sublayers in the
dense model are replaced with MoE layers. An
MoE layer takes an input token representation xt
and then routes it to the top- kexperts selected from
a set{Ei}N
i=1ofNexperts thanks to a gating net-
work:
Gt=softmax (Wg·xt) (1)
Where Wg∈RN×dis a learned parameter. The
output of the MoE layer is a weighted sum of the
outputs of the kselected experts E:
yt=1P
i∈EGt,iX
i∈EGt,iEi(xt) (2)
3.2 NLLB-200
No Language Left Behind (NLLB-200) is a set
of massively multilingual NMT models that can
translate to and from 202 languages (Costa-jussà
et al., 2022), including many very low resources
languages. Models of varying sizes have been re-
leased. The largest one is a Mixture-of-Experts
model and has 54.5B parameters. A dense model
of 3.3B models is also available, which has the
same architecture as the 54.5B MoE model without
the experts. In this work, we will attempt to prune
the experts from the 54.5B model while using the
3.3B variant as a lower-bound baseline.4
In the 54.5B MoE model, every 4thFFN sublayer
– in both the encoder and decoder – is replaced by
an MoE layer, starting at the 4thlayer (this makes
12 layers with experts). Each MoE layer consists
of 128 experts (1536 experts in total) with the same
architecture as an FFN sublayer, and has its own
gating network, following the top- kgating algo-
rithm of Lepikhin et al. (2020) and selecting the
4If the pruned models’ performance is worse than the 3.3B
baseline, there is no point in using the MoE model, which is
larger and more cumbersome to use.top-2experts per token without any randomiza-
tion. The model was trained with a linear combi-
nation of label-smoothed cross-entropy (Szegedy
et al., 2016) with an auxiliary load balancing loss
(Shazeer et al., 2017), which encourages tokens to
be uniformly distributed across experts.
Memory usage. The 3.3B and 54.5B models
are Transformers with an embedding dimension
of 2048, an FFN dimension of 8192, 16 attention
heads, 24 encoder layers, and 24 decoder layers.
When storing their parameters in half precision, the
3.3B dense model and 54.5B MoE model take re-
spectively 6.2GiB and 101.5GiB of memory. Each
expert has 33.6M parameters, representing 51.6B
parameters in total or 96GiB of memory. While
the 3.3B model can easily run on a single GPU,
the 54.5B model requires at the very least 4 32GB
GPUs to run. To maximize efficiency, decoding
with the MoE model has to be done with expert par-
allelism (Fedus et al., 2021), with each GPU hold-
ing a full copy of the “dense” parameters (2.9B or
5.5GiB) and 1/Nthof the experts per layer, where
Nis the number of GPUs.5Because of the mem-
ory usage of beam search decoding and memory
fragmentation, batched decoding actually requires
more GPUs in practice (e.g., 6 or 8), or to offload
the encoder and decoder to the CPU when they are
not used.6
4 Our Approach
We experiment with different experts’ pruning met-
rics and strategies that allow us to select the most
relevant experts per language or language pair, and
thus significantly reduce the memory usage at in-
ference time of NLLB-200.
4.1 Expert pruning metrics
The pruning metric should quantify the contribu-
tion of a given expert to the translation. Intu-
itively, experts that were more involved in transla-
tion should be considered more important.
Activity. We define the Top 1 activity ,top1(e),
of an expert eas the fraction of tokens routed to
this expert as the first choice (i.e., the frequency
at which this expert was ranked first by the gating
mechanism). We also consider the Top 2 activity
5This brings the memory usage to 118GiB (or 29.5GiB per
GPU) when decoding on 4 GPUs.
6Memory usage can be divided by almost two by encoding
the full test set with the encoder and then moving the encoder
to CPU and decoder to GPU.

--- PAGE 4 ---
variant, top2(e), with the fraction of tokens routed
to this expert as their first or second choice.
Using only activity as an importance metric can
be sub-optimal as it does not take into account the
gating value assigned to this expert by the model.
Load Balancing. We experiment with the load
balancing pruning metric, similar to the load bal-
ancing loss used by Costa-jussà et al. (2022) to
train the MoE model. It is defined as the prod-
uct of the activity and the average gate value:
LB(e) =top1(e)×mean (e).
Importance. Following the definition of atten-
tion head confidence by V oita et al. (2019), we
define the confidence of an expert, conf (e), as its
average gate value when it is ranked first . Then, we
can define the “vanilla” importance of an expert as
the product of its’ activity andconfidence .7
impvanilla (e) =top1(e)×conf (e) (3)
We define importance as an improved version of
vanilla importance with an exponential to smooth
the confidence values:
imp(e) =top1(e)×exp (conf (e)) (4)
4.2 Expert statistics granularity
To compute the pruning metrics defined above, for
each expert e∈ {1, . . . , 1536}8we collect the gate
statistics, top1(e),top2(e),mean (e)andconf (e),
by decoding the validation sets for all language
directions.9However, these statistics can be aggre-
gated at different granularity levels. Depending on
how these statistics are aggregated, we hope to see
language-specific experts emerge. In our experi-
ments, we consider three different granularities:
•global : we aggregate the statistics across all
language pairs to keep the overall best experts;
•language-pair : we collect gate statistics for
each language pair and thus keep a (poten-
tially) different set of experts for each lan-
guage pair;
•language-specific : we aggregate encoder-side
statistics per source language and decoder-
side statistics per target language, which will
let us keep a single set of encoder/decoder
experts per source/target language.
7Using confidence alone as a pruning metric has demon-
strated very poor performance in our preliminary experiments,
and therefore was not retained for the follow up study.
812 layers with 128 experts each = 1536 experts
9We always use beam search with a beam size of 4.4.3 Expert pruning algorithm
Using the pruning metrics defined in Section 4.1,
there are different expert pruning strategies that we
can adopt. The pruning metric values are normal-
ized to sum to one in each layer, and experts are
sorted from most important to least important.
Fixed per layer. First, the simplest way is to re-
tain a fixed amount of top experts in each layer.
For example, 75% pruning retains 384 out of 1536
experts, which corresponds to 32 experts per layer.
In the balanced setting, the number of experts per
layer is the same in the encoder and decoder (e.g.,
32 per layer). In the unbalanced setting, we keep a
different number of experts in the encoder and de-
coder (e.g., 40 per encoder layer and 24 per decoder
layer).
Global threshold. The pruning metrics we de-
fined let us easily prune experts per layer, but not
globally. To select globally best experts (with no
a priori on the number of experts per layer) we
search for a global threshold θsuch that:
12X
k=1min(nk|nkX
i=1ϕ(ek
i)≥θ) =count (5)
Where ϕis the pruning metric; kthe layer id
(out of 12 layers with experts); ek
itheithexpert in
the sorted list of experts for that layer; and count
the desired total number of experts to retain (e.g.,
384 for 75% pruning). Experts {ek
i}nk
i=1are then
retained and the rest are pruned.10In our experi-
ments, we make sure to keep at least 4 experts per
layer.11
Our intuition behind this pruning method is to
define a constant probability mass (or “importance”
mass) each layer should have. Keeping only a
couple of experts in a layer is fine if they are col-
lectively used a majority of the time. Conversely,
some layers may need more experts if expert usage
is more uniformly distributed.
Figure 1 illustrates how experts are distributed
among layers with this approach at 75% pruning
and with the top1metric. We see that the decoder
requires much fewer experts per layer than the en-
coder to reach the same activity threshold.
10We iterate from 0 to 1 by increments of 0.001until we
find a value of θwhich satisfies this equation.
11To be able to decode on up to 4 GPUs and to limit the
risk of degenerate behavior because some layers have too few
experts. Also since NLLB-200 uses top-2 gating, we need at
least 2 experts per layer.

--- PAGE 5 ---
enc-4 enc-8enc-12 enc-16 enc-20 enc-24dec-4 dec-8dec-12 dec-16 dec-20 dec-24010203040506070Experts per layerFigure 1: Average number of experts per layer after
pruning 75% of experts with the global threshold algo-
rithm (average activity threshold: 0.69). Pruning is done
per language direction and the values are averaged over
the 870 directions of the valid set.
We also experiment with a variant of this method,
which we call Enc/Dec thresholds , with a fixed
amount in the encoder and decoder (e.g., 192 and
192) and thresholds that are defined independently
in the encoder and decoder.
5 Experiments
5.1 Evaluation settings
In our experiments, we use the FLORES-200
benchmark (Costa-jussà et al., 2022), which con-
sists of translations of 3001 English sentences
(from 842 distinct Wikipedia articles) to all other
201 languages. The multi-parallel nature of this
dataset makes it possible to evaluate performance
in all 40 602 language directions. As our final test
benchmark, we take a representative subsample of
53 languages out of 202, which were also used
as an ablation dataset by Costa-jussà et al. (2022).
In our intermediate experiments, we work with a
smaller subset of 30 out of 53 languages, with 10
languages per resource type (high, low, very low)
and covering the same fourteen language families
as the full subset of 53 languages. More details
on the languages considered in our experiments as
well as the amount of resources available per cate-
gory are provided in Tables 8 and 14 in Appendix.
To evaluate translation quality we use two met-
rics: chrF++12(Popovi ´c, 2015) and spBLEU13
(Costa-jussà et al., 2022). BLEU is heavily
12SacreBLEU signature for chrF++: nrefs:1|case:mixed
|eff:yes|nc:6|nw:2|space:no|version:2.3.1
13SacreBLEU signature for spBLEU: nrefs:1|case:mixed
|eff:no|tok:flores200|smooth:exp|version:2.3.1tokenization-dependant and its implementations do
not include tokenizers for most of the NLLB-200
languages. spBLEU overcomes this issue by tok-
enizing the references and model outputs with a
multilingual SentencePiece tokenizer (SPM-200,
Costa-jussà et al., 2022). We report chrF++ results
in the main paper and spBLEU results in Appendix.
We use FLORES-200 dev (which we call valid ) for
collecting MoE gate statistics and comparing dif-
ferent pruning algorithms and rates, and FLORES-
200 devtest (which we call test) for reporting final
results and comparing with the 3.3B and 54.5B
baselines.
5.2 Results
In the first set of experiments, we work with a sub-
set of 30 languages. Table 1 compares different
expert pruning metrics and strategies under a 75%
pruning rate. The experts are selected per language
pair, and the scores are averaged per resource type
(high, low, very low). The first part of the table
reports two baselines: an upper bound correspond-
ing to the full (unpruned) 54.5B MoE model, and
a lower bound being the 3.3B dense model (same
architecture without experts).
Pruning metric The second part of Table 1 com-
pares the chrF++ performance of different pruning
metrics (spBLEU score are reported in Appendix
Table 9). From these results, we can see that the
top-1 activity andimportance metrics are the most
effective at identifying important experts. Further
experiments with global threshold pruning (third
part of Table 1) confirm the slightly better perfor-
mance of the importance metric which we keep as
the default for the next experiments.
Pruning algorithm Table 1 also compares the
pruning algorithms described in Section 4.3 ( fixed
per layer andglobal threshold ). Note that with
fixed per layer , we can either allocate the same ex-
pert budget in the encoder and decoder (balanced
setting) or have more experts in the encoder (unbal-
anced setting).
First, we see that the global threshold strategy
gives the best results overall, with the same aver-
age chrF++ as the full unpruned model. However,
global threshold is not very practical for several
reasons. First, it identifies a different amount of ex-
perts per layer for each language pair, which leads
to variable memory usage across language pairs. It
also requires recreating and reloading the model
when decoding multiple directions, which is very

--- PAGE 6 ---
Method Metric High→High High→Low High→V . low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model (Costa-jussà et al., 2022) 44.54 38.20 30.08 40.49 35.19 27.61 35.27 30.68 24.75 34.06
54.5B MoE model (Costa-jussà et al., 2022) 45.90 39.19 30.24 42.29 36.35 28.18 36.55 32.16 24.93 35.07
Fixed per layer (balanced)Top 1 45.52 38.75 30.13 41.51 35.50 27.92 36.09 31.68 24.90 34.64
Top 2 44.38 37.92 29.60 40.56 34.86 27.48 35.24 30.97 24.54 33.93
Load balancing 44.48 38.06 29.64 40.67 34.95 27.56 35.29 31.04 24.59 34.01
Importance (vanilla) 42.87 34.73 28.40 40.92 34.17 27.46 34.96 29.71 23.99 33.00
Importance 45.59 38.76 30.18 41.50 35.41 27.87 36.15 31.69 24.96 34.66
Global thresholdTop 1 46.01 39.28 30.44 41.91 36.18 28.21 36.40 31.97 25.06 35.03
Importance 46.10 39.31 30.46 41.99 36.25 28.29 36.47 32.09 25.10 35.09
Fixed per layer (unbalanced)
Importance45.79 39.00 30.33 41.80 35.76 28.12 36.36 31.93 25.10 34.89
Enc/Dec thresholds (balanced) 45.57 38.73 30.07 41.52 35.36 27.81 36.13 31.62 24.88 34.61
Enc/Dec thresholds (unbalanced) 45.88 38.97 30.28 41.92 35.85 28.10 36.39 31.84 25.06 34.90
Table 1: chrF++ valid scores on 30 languages of different pruning algorithms and metrics, with 75% pruning (i.e.,
384 experts are kept in total). The unbalanced approaches keep 240 encoder experts and 144 decoder experts.
0 10 20 30 40 50
Percentage of experts retained33.033.534.034.535.0Average valid chrF++ (870 directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
0 10 20 30 40 50
Percentage of experts retained14.014.515.015.516.016.517.017.518.0Average valid spBLEU (870 directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
0 10 20 30 40 50
Percentage of experts retained43.043.544.044.545.045.546.0Average valid chrF++ (90 high-resource directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
0 10 20 30 40 50
Percentage of experts retained36.036.537.037.538.038.539.039.5Average valid chrF++ (290 low-resource directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
0 10 20 30 40 50
Percentage of experts retained28.7529.0029.2529.5029.7530.0030.2530.5030.75Average valid chrF++ (490 very low resource directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
Figure 2: chrF++ and spBLEU valid scores on 30 languages for different resource types as a function of the
percentage of experts retained. Pruning is done per language pair with the importance metric and with a fixed
number of experts per layer.
slow. Finally, we found that it was more sensitive
to over-generation and hallucinations (which we
elaborate on in Section A in Appendix) at higher
pruning rates. The enc/dec thresholds approach
does not suffer from all the limitations of global
threshold , but it is not better than fixed per layer
either. Therefore, for simplicity, we pick the fixed
per layer approach for our next experiments.
Balanced versus unbalanced pruning When re-
taining 25% of experts (384 out of 12 ×128), global
threshold keeps on average 335 encoder experts
and 49 decoder experts. The number of selected
experts in the encoder and decoder for different
language resource types is shown in Table 16 in
Appendix. Following this observation that encoder
experts seem more important than decoder ones, we
experiment with different encoder/decoder ratios.
1:1is the balanced setting. 2:1and3:1are unbal-anced with respectively twice and three times as
many encoder experts as decoder experts. Figure 2
shows that 3:1performs the best across almost all
pruning rates and resource types.
Pruning with global statistics. Figure 2 and Fig-
ure 4 in Appendix also show that the same experts
can be pruned across all language pairs (with statis-
tics aggregated over all directions) with no loss
in performance at 50% pruning. Statistics at the
language-direction granularity let us safely prune
up to 80% of the experts (in the unbalanced set-
ting), which makes the model small enough to fit
on a single GPU.
Test results and language-specific pruning. Fi-
nally, we validate our results over the test set on
53 languages (2 756 directions). We use the fixed
per layer approach with a 3:1ratio, which showed

--- PAGE 7 ---
Method Enc experts Dec experts High→High High→Low High→V . Low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model 6 6 44.18 38.30 31.45 38.24 34.60 27.93 35.93 32.02 26.47 35.81
54.5B MoE model 768 768 45.41 38.98 31.89 39.72 35.40 28.83 37.29 33.23 26.95 36.81
Fixed per layer (lang-pair) 216 72 45.37 39.06 31.79 39.20 35.03 28.47 37.05 33.16 26.63 36.59
Fixed per layer (global) 216 72 43.20 37.60 31.68 37.37 33.94 28.40 35.38 31.97 26.84 35.34
Fixed per layer (lang) 216 72 45.35 39.10 31.82 39.18 35.10 28.51 37.02 33.19 26.62 36.61
Table 2: chrF++ test scores on 53 languages, with the importance metric for 80% pruning (1-GPU decoding).
Method Enc experts Dec experts High→High High→Low High→V . low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model 6 6 45.54 38.84 32.72 39.18 34.87 29.07 38.39 34.11 29.21 34.64
54.5B MoE model 768 768 46.68 39.36 33.56 40.53 35.49 30.07 40.46 35.49 30.16 35.74
Fixed per layer (lang) 216 72 46.67 39.59 33.33 40.19 35.50 29.67 39.94 35.29 29.50 35.46
Table 3: chrF++ test scores on all 202 languages, with the importance metric for 80% pruning (1-GPU decoding).
EncoderDecoderEn→Fr En→Ur Ast→Ur Ur→Fr Ur→Ast Fr→Ast Ast→Ko Fr→Ko Ko→En Ko→Ast Fr→En Ast→En
En→Fr NA 0.18 0.17 0.71 0.31 0.32 0.17 0.17 0.19 0.31 0.17 0.17
En→Ur 1.00 NA 0.68 0.20 0.24 0.22 0.31 0.30 0.26 0.23 0.21 0.23
Ast→Ur 0.35 0.35 NA 0.22 0.25 0.22 0.37 0.35 0.21 0.22 0.18 0.20
Ur→Fr 0.36 0.36 0.40 NA 0.39 0.38 0.19 0.19 0.22 0.39 0.16 0.19
Ur→Ast 0.36 0.36 0.40 1.00 NA 0.83 0.27 0.25 0.24 0.87 0.17 0.22
Fr→Ast 0.42 0.42 0.45 0.44 0.44 NA 0.25 0.24 0.24 0.80 0.20 0.22
Ast→Ko 0.35 0.35 1.00 0.40 0.40 0.45 NA 0.78 0.15 0.24 0.15 0.16
Fr→Ko 0.42 0.42 0.45 0.44 0.44 1.00 0.45 NA 0.14 0.23 0.15 0.13
Ko→En 0.33 0.33 0.41 0.48 0.48 0.44 0.41 0.44 NA 0.26 0.55 0.70
Ko→Ast 0.33 0.33 0.41 0.48 0.48 0.44 0.41 0.44 1.00 NA 0.17 0.22
Fr→En 0.42 0.42 0.45 0.44 0.44 1.00 0.45 1.00 0.44 0.44 NA 0.61
Ast→En 0.35 0.35 1.00 0.40 0.40 0.45 1.00 0.45 0.41 0.41 0.45 NA
Table 4: The Jaccard similarity of selected 25% important experts between different language pairs in the encoder
(lower triangle) and decoder (upper triangle). Pruning is done per language pair with the importance metric. The
same number of experts were chosen for the encoder and decoder with thresholding.
the best results on the validation set at 80% (min-
imum rate for 1-GPU decoding). Tables 2 and 11
report these test scores with three different levels
of granularity: global ,language-pair-specific or
language-specific (as described in Section 4.2). Ta-
ble 10 in the Appendix reports valid scores with
the same settings.
Pruning important experts chosen per language
pair gives 0.8 chrF++ more on average than the
3.3B dense model, and 0.2 chrF++ less than the
full MoE model. Global pruning on the other hand
performs worse than both the MoE and dense mod-
els, which confirms the importance of having a
language-specific pruning strategy.
While choosing important experts for each lan-
guage pair is effective, it is not very practical: with
Llanguages, this generates L×(L−1)differ-
ent configurations. A more practical approach is
to prune encoder experts per source language and
decoder experts per target language (i.e., language-
specific pruning). This pruning strategy performs
exactly as well as pruning per language direction
and is more convenient. Following this observation,
we extract per-language gate statistics on all 202
languages.14Then, we apply 80% per-layer prun-
14By decoding 25 random line pairs per language direction,ing with the importance metric (at the language
granularity) and decode the test set in all 40 602 di-
rections. Tables 3 and 12 report the chrF++ and sp-
BLEU scores. Table 13 reports average score deltas
with the unpruned model (and standard deviation
per resource type). To facilitate future research
and give the opportunity for anyone with a 32GB
GPU to run the NLLB-200 model, we release the
detailed gate statistics and the ids of the selected
experts. We also share the scores for each direc-
tion and the decoding outputs of our best pruning
approaches.
6 Discussion
6.1 Inference speed and compute budget
Table 5 reports the inference speed of different
models: the 3.3B dense model, the full MoE model,
and the MoE model with 80% pruning. We see that
with 80% pruning, the MoE model requires a single
32GB V100 and performs approximately as fast as
the full model on 4 GPUs. If 4 GPUs are available,
80% pruning can double the inference speed of the
resulting in 5 025 lines per source language and per target
language. To speed up this process, we do teacher forcing
instead of beam-search decoding, which we found to perform
as well.

--- PAGE 8 ---
Model Batch size GPUs WPS Time (s)
54.5B 16k8 195 105
4 156 131
80% pruning16k 4 299 79
4k 1 172 135
3.3B 4k 1 246 86
Table 5: Inference speed benchmark for the 3.3B dense
baseline model, the full MoE model, and its pruned
version with 36 experts per encoder layer and 12 per
decoder layer. We decode the FLORES valid set from
29 languages into English and average the decoding
time or words per second.
MoE model.
Table 15 in Appendix gives a breakdown of the
number of GPU hours used for this work.
6.2 Similarity of selected experts
Section 5.2 shows that only a fraction of all ex-
perts is necessary to translate between two given
languages. We analyze the experts selected by our
pruning method, to verify whether we can claim
that there are indeed language-specific experts. In
order to do so, we select experts with our proposed
importance metric and prune them per language
pair at a 75% rate with the Enc/dec thresholds
method, so that both the encoder and decoder have
the same number of experts. We then compute the
Jaccard similarity of selected encoder/decoder ex-
perts between different language pairs sharing the
same source or target language. The lower and up-
per triangles of Table 4 show this similarity in the
encoder and decoder respectively. We see that the
encoder experts are independent of the target lan-
guage (even though pruning is based on statistics
collected at the lang-pair granularity level). This is
an expected result, and it is due to the model design,
where the target language code is introduced on the
decoder side only: the encoder representation is
not impacted by the target language. We note that
the similarity between different source languages is
also quite high (30-50%). The similarity between
important decoder experts for the same target lan-
guage is in the 68-87% range; and in the 13-39%
range for different target languages. These obser-
vations combined with the results in Section 5.2
suggest the emergence of language-specific experts
in the NLLB-200 model.
6.3 Similarity of languages based on the
importance metric
Finally, we compare expert statistics across differ-
ent languages, to better understand whether knowl-
Figure 3: Hierarchical clustering of languages based on
theimportance metric of experts in the decoder. Differ-
ent colors represent different language subgroupings.
edge transfer happens at the expert level between
similar languages. We gather importance metrics
for each expert in the decoder for each language
and concatenate the values of all MoE layers to
have one feature vector of dimension 768. Then
we do hierarchical clustering and show it as a den-
drogram in Figure 3, where we highlight different
language subgroupings with different colors. We
can see that some clusters contain linguistically re-
lated languages, such as Yue Chinese, Korean and
Japanese; Russian and Belarussian; or Portuguese,
Asturian, and French. We run a similar analysis
on the encoder experts and also observe meaning-
ful language clustering, but less clear (Appendix
Figure 7).
6.4 Discrepancy between chrF++ and
spBLEU scores
We observed that our pruning method results in
slightly higher performance drop according to sp-
BLEU, than with chrF++. We hypothesize that it

--- PAGE 9 ---
is due to a rare but visible phenomenon of over-
generation (and sometimes hallucinations). In the
majority of cases, the translation is accurate ini-
tially but subsequently includes repetitions, para-
phrasing, or slight hallucinations. The spBLEU
metric penalizes this behavior more than chrF++,
which could account for the variation in scores ob-
served. More details on this are in Section A in
Appendix.
7 Conclusion
In this paper, we study expert pruning in the NLLB-
200 Mixture-of-Experts MT model. We propose
expert pruning metrics based on gate statistics col-
lected while decoding. We study several pruning
strategies and demonstrate that it is possible to
prune up to 80% of experts with a negligible loss
in performance, which makes it possible to decode
on a single 32GB GPU. We compare pruning at
three levels of granularity: per language direction,
per language, or global. Language-specific and
language-pair pruning perform the same but the
former is the most convenient. Global pruning
(i.e., pruning always the same experts regardless of
the source and target languages) performs surpris-
ingly well but worse than language-specific prun-
ing, which suggests that there are indeed some
language-specific experts. This latter hypothesis is
confirmed by our analysis of the selected experts.
8 Risks and Limitations
In our work, we rely on a single Mixture-of-Experts
NMT model which is NLLB-200. There is a risk
that our conclusions may only hold for this particu-
lar model and are specific to the way this model was
trained. We believe that our findings still can be
of interest to any person willing to use the NLLB-
200 model because: (1) It was the only publicly-
available MoE NMT model at the time of submis-
sion; (2) It is the only model covering 202 lan-
guages and reaching SoTA results for most of those
languages.
Moreover, we did not try to finetune the pruned
model, which could potentially improve the results
(but requires a large number of GPUs) and there-
fore change some of our conclusions.
This work has similar risks as the original NLLB-
200 models regarding the misuse of potentially
wrong translations. Note that, as observed by Mo-
hammadshahi et al. (2022b), pruning could amplify
the biases already present in the full model.Acknowledgement
This work was completed during a research in-
ternship at NA VER LABS Europe. Yeskendir
Koishekenov is also supported by ELLIS Amster-
dam and Qualcomm AI Research.
References
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.
Massively multilingual neural machine translation.
arXiv preprint arXiv:1903.00089 .
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vah-
dat, Jiaming Song, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, et al.
2022. ediffi: Text-to-image diffusion models with
an ensemble of expert denoisers. arXiv preprint
arXiv:2211.01324 .
Tianyu Chen, Shaohan Huang, Yuan Xie, Binx-
ing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li,
and Furu Wei. 2022. Task-specific expert prun-
ing for sparse mixture-of-experts. arXiv preprint
arXiv:2206.00277 .
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale. arXiv
preprint arXiv:1911.02116 .
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe
Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,
et al. 2022. No language left behind: Scaling
human-centered machine translation. arXiv preprint
arXiv:2207.04672 .
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language mod-
els beyond a fixed-length context. arXiv preprint
arXiv:1901.02860 .
Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret
Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou,
Tao Wang, Emma Wang, Kellie Webster, Marie Pel-
lat, Kevin Robinson, Kathleen Meier-Hellstern, Toju
Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui
Wu, Zhifeng Chen, and Claire Cui. 2022. GLaM:
Efficient scaling of language models with mixture-
of-experts. In Proceedings of the 39th International
Conference on Machine Learning , volume 162 of
Proceedings of Machine Learning Research , pages
5547–5569. PMLR.
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi
Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep
Baines, Onur Celebi, Guillaume Wenzek, Vishrav

--- PAGE 10 ---
Chaudhary, et al. 2021. Beyond english-centric mul-
tilingual machine translation. J. Mach. Learn. Res. ,
22(107):1–48.
William Fedus, Jeff Dean, and Barret Zoph. 2022. A re-
view of sparse expert models in deep learning. arXiv
preprint arXiv:2209.01667 .
William Fedus, Barret Zoph, and Noam Shazeer. 2021.
Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity.
Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang,
Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu,
Weichong Yin, Shikun Feng, et al. 2022. Ernie-vilg
2.0: Improving text-to-image diffusion model with
knowledge-enhanced mixture-of-denoising-experts.
arXiv preprint arXiv:2210.15257 .
William Held and Diyi Yang. 2022. Shapley
head pruning: Identifying and removing interfer-
ence in multilingual transformers. arXiv preprint
arXiv:2210.05709 .
Robert A Jacobs, Michael I Jordan, Steven J Nowlan,
and Geoffrey E Hinton. 1991. Adaptive mixtures of
local experts. Neural computation , 3(1):79–87.
Jae-young Jo and Sung-Hyon Myaeng. 2020. Roles and
utilization of attention heads in transformer-based
neural language models. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 3404–3417.
Michael I Jordan and Robert A Jacobs. 1994. Hierarchi-
cal mixtures of experts and the em algorithm. Neural
computation , 6(2):181–214.
Young Jin Kim, Ammar Ahmad Awan, Alexandre
Muzio, Andres Felipe Cruz Salinas, Liyang Lu,
Amr Hendy, Samyam Rajbhandari, Yuxiong He, and
Hany Hassan Awadalla. 2021a. Scalable and effi-
cient moe training for multitask multilingual models.
arXiv preprint arXiv:2109.10465 .
Zae Myung Kim, Laurent Besacier, Vassilina Nikoulina,
and Didier Schwab. 2021b. Do multilingual neu-
ral machine translation models contain language
pair specific attention heads? arXiv preprint
arXiv:2105.14940 .
Sneha Kudugunta, Yanping Huang, Ankur Bapna,
Maxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-
ong, and Orhan Firat. 2021. Beyond distillation:
Task-level mixture-of-experts for efficient inference.
arXiv preprint arXiv:2110.03742 .
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,
Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. 2020.
Gshard: Scaling giant models with conditional com-
putation and automatic sharding. arXiv preprint
arXiv:2006.16668 .Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? In Ad-
vances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc.
Ali Mohammadshahi, Vassilina Nikoulina, Alexandre
Berard, Caroline De Brun, James Henderson, and
Laurent Besacier. 2022a. Small-100: Introducing
shallow multilingual machine translation model for
low-resource languages. ArXiv , abs/2210.11621.
Ali Mohammadshahi, Vassilina Nikoulina, Alexandre
Berard, Caroline De Brun, James Henderson, and
Laurent Besacier. 2022b. What do compressed mul-
tilingual machine translation models forget? ArXiv ,
abs/2205.10828.
Basil Mustafa, Carlos Riquelme, Joan Puigcerver,
Rodolphe Jenatton, and Neil Houlsby. 2022. Mul-
timodal contrastive learning with limoe: the
language-image mixture of experts. arXiv preprint
arXiv:2206.02770 .
David Patterson, Joseph Gonzalez, Quoc Le, Chen
Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, and Jeff Dean. 2021. Carbon
emissions and large neural network training. arXiv
preprint arXiv:2104.10350 .
Maja Popovi ´c. 2015. chrf: character n-gram f-score for
automatic mt evaluation. In Proceedings of the Tenth
Workshop on Statistical Machine Translation , pages
392–395.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Joan Puigcerver, Carlos Riquelme, Basil Mustafa,
Cedric Renggli, André Susano Pinto, Sylvain Gelly,
Daniel Keysers, and Neil Houlsby. 2020. Scalable
transfer learning with expert models. arXiv preprint
arXiv:2009.13239 .
Carlos Riquelme, Joan Puigcerver, Basil Mustafa,
Maxim Neumann, Rodolphe Jenatton, André Su-
sano Pinto, Daniel Keysers, and Neil Houlsby. 2021.
Scaling vision with sparse mixture of experts. Ad-
vances in Neural Information Processing Systems ,
34:8583–8595.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538 .
Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. 2019. Energy and policy considerations for deep
learning in nlp. arXiv preprint arXiv:1906.02243 .
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision. In

--- PAGE 11 ---
Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2818–2826.
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-
man Goyal, Vishrav Chaudhary, Jiatao Gu, and An-
gela Fan. 2020. Multilingual translation with exten-
sible multilingual pretraining and finetuning. arXiv
preprint arXiv:2008.00401 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Elena V oita, David Talbot, Fedor Moiseev, Rico Sen-
nrich, and Ivan Titov. 2019. Analyzing multi-
head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned. arXiv preprint
arXiv:1905.09418 .
Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang,
Dongdong Zhang, and Furu Wei. 2022. Deepnet:
Scaling transformers to 1,000 layers. arXiv preprint
arXiv:2203.00555 .
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. Advances in neural informa-
tion processing systems , 32.
Zhao You, Shulin Feng, Dan Su, and Dong Yu. 2021.
Speechmoe: Scaling to large acoustic models with
dynamic routing mixture of experts. arXiv preprint
arXiv:2105.03036 .
Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader.
2012. Twenty years of mixture of experts. IEEE
transactions on neural networks and learning sys-
tems, 23(8):1177–1193.
Biao Zhang, Philip Williams, Ivan Titov, and Rico
Sennrich. 2020. Improving massively multilingual
neural machine translation and zero-shot translation.
arXiv preprint arXiv:2004.11867 .
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-
ping Huang, Jeff Dean, Noam Shazeer, and William
Fedus. 2022. St-moe: Designing stable and transfer-
able sparse expert models, 2022. URL https://arxiv.
org/abs/2202.08906 .

--- PAGE 12 ---
A Discrepancy between chrF++ and spBLEU scores
The spBLEU scores (Figure 2 top right, or Figure 4 and Tables 9 and 11) do not show exactly the same
trend as chrF++. The gap between the full models and their pruned versions is slightly higher. This
is likely caused by a rare but visible phenomenon of over-generation (and sometimes hallucinations).
Table 7 shows some examples of such over-generation (with 3:1fixed per layer lang-pair pruning at 80%).
Most of the time, the translation is correct, but then continues with repetitions of itself, paraphrasing, or
slight hallucinations. This behavior is more penalized by spBLEU than chrF++, which may explain the
difference in scores. For instance, when duplicating the FLORES valid English-French translation output
of the 54.5B model (i.e., concatenating each output sentence with itself), we see a spBLEU drop of 47%
and a chrF++ drop of only 13%. The global threshold method is more sensitive to this phenomenon. For
instance, 80% pruning leads to a 1.75 spBLEU drop (vs 0.53 for the fixed per layer method). We report in
Table 6 the difference in length ratio (reported by SacreBLEU, Post, 2018) between the pruned models
and the full model. We observe that global threshold at 80% has an average length ratio delta with the full
model of 0.16 (meaning it generates longer outputs), while fixed per layer has 0.04. We hypothesize that
this over-generation issue may be mitigated by identifying experts that are specialized in generating the
end-of-sequence symbol, but this is the subject of future work.
Method Enc experts Dec experts High→High High→Low High→V . Low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model 6 6 0.02±0.02 0.04±0.03 0.09±0.06 0.02±0.03 0.04±0.04 0.11±0.06 0.06±0.05 0.07±0.07 0.15±0.08 0.06±0.07
Fixed per layer288 96 0.03±0.03 0.01±0.02 0.04±0.03 0.04±0.06 0.01±0.03 0.04±0.03 0.06±0.06 0.03±0.03 0.06±0.04 0.04±0.04
216 72 0.04±0.03 0.02±0.02 0.05±0.04 0.05±0.07 0.01±0.04 0.05±0.05 0.07±0.07 0.04±0.04 0.07±0.04 0.04±0.05
144 48 0.05±0.05 0.03±0.04 0.07±0.05 0.07±0.09 0.05±0.10 0.08±0.06 0.09±0.07 0.07±0.09 0.10±0.06 0.07±0.07
Global threshold384 0.07±0.07 0.07±0.10 0.10±0.07 0.13±0.11 0.11±0.17 0.14±0.09 0.13±0.12 0.11±0.14 0.14±0.09 0.11±0.11
288 0.10±0.10 0.12±0.17 0.15±0.20 0.19±0.22 0.15±0.21 0.18±0.24 0.20±0.25 0.13±0.15 0.19±0.22 0.16±0.20
192 0.10±0.10 0.12±0.15 0.12±0.09 0.17±0.15 0.17±0.23 0.16±0.12 0.16±0.14 0.17±0.19 0.15±0.11 0.15±0.15
Enc/dec thresholds 216 72 0.05±0.04 0.03±0.03 0.06±0.04 0.08±0.09 0.03±0.04 0.06±0.05 0.09±0.07 0.06±0.06 0.08±0.04 0.06±0.06
Table 6: Valid length ratio statistics on 30 languages. We compute the length ratio deltas w.r.t. the full unpruned
model and report the mean and standard deviation in each language category. Values smaller than zero mean that the
model generates longer sequences on average. We prune with the importance metric, using statistics at the language
pair granularity. Length ratios are obtained with SacreBLEU and the ‘flores200’ tokenization.
Reference However, very few Royal Navy ships were based near the likely invasion routes as the
admirals were afraid they would be sunk by German air attack.
Translation However, the Royal Navy had few ships in the vicinity of the invasion routes, as the
admirals feared that they would be sunk by German air attack. The Royal Navy’s
ships were too small to be able to operate in the vicinity of the invasion routes.
Reference However, the discovery of his tomb in 1922 made him a celebrity. While many tombs
of the past were robbed, this tomb was left virtually undisturbed.
Translation However, his tomb became famous when it was discovered in 1922. Although many
tombs in the past have been excavated, this one has remained largely undisturbed. The
tomb was built in the middle of the 19th century. The tomb was built in the middle of
the 20th century. The tomb was built in the middle of the 20th century
Reference The translation engines have improved dramatically, and now often give more or less
correct translations (and more seldom gibberish), but some care is due, as they still
may have gotten it all wrong.
Translation Translation engines have improved dramatically, and now often produce more or less
accurate translations (as well as more or less uncommon ones), but some errors can
still be corrected, so some caution is warranted. For example, in the case of the English
language, the use of the word "translate" in the context of a translation of a document,
such as a translation of a document into a foreign language, is not recommended. The
use of the word "translate" in the context of a translation of a document is discouraged.
Table 7: Examples of over-generation when translating from Tigrinya ( tir_Ethi ) to English ( eng_Latn ) with
fixed per layer (3:1) 80% pruning at the language direction granularity.

--- PAGE 13 ---
Code Language Resource Type Family Subgrouping
tsn_Latn Tswana High Atlantic-Congo Benue-Congo
vie_Latn Vietnamese High Austroasiatic Vietic
rus_Cyrl Russian High Indo-European Balto-Slavic
eng_Latn English High Indo-European Germanic
fra_Latn French High Indo-European Italic
por_Latn Portuguese High Indo-European Italic
jpn_Jpan Japanese High Japonic Japanesic
kor_Hang Korean High Koreanic Korean
fin_Latn Finnish High Uralic Finnic
tir_Ethi Tigrinya Low Afro-Asiatic Semitic
nso_Latn Northern Sotho Low Atlantic-Congo Benue-Congo
yor_Latn Yoruba Low Atlantic-Congo Benue-Congo
mal_Mlym Malayalam Low Dravidian South Dravidian
tam_Taml Tamil Low Dravidian South Dravidian
bel_Cyrl Belarusian Low Indo-European Balto-Slavic
cym_Latn Welsh Low Indo-European Celtic
urd_Arab Urdu Low Indo-European Indo-Aryan
luo_Latn Luo Low Nilotic Western Nilotic
tat_Cyrl Tatar Low Turkic Common Turkic
cjk_Latn Chokwe Very low Atlantic-Congo Benue-Congo
kik_Latn Kikuyu Very low Atlantic-Congo Benue-Congo
fuv_Latn Nigerian Fulfulde Very low Atlantic-Congo North-Central Atlantic
wol_Latn Wolof Very low Atlantic-Congo North-Central Atlantic
ace_Latn Acehnese Very low Austronesian Malayo-Polynesian
ayr_Latn Central Aymara Very low Aymaran Central Southern Aymara
snd_Arab Sindhi Very low Indo-European Indo-Aryan
ast_Latn Asturian Very low Indo-European Italic
kea_Latn Kabuverdianu Very low Indo-European Italic
yue_Hant Yue Chinese Very low Sino-Tibetan Sinitic
arb_Arab Modern Standard Arabic High Afro-Asiatic Semitic
swh_Latn Swahili High Atlantic-Congo Benue-Congo
eus_Latn Basque High Basque Basque
bul_Cyrl Bulgarian High Indo-European Balto-Slavic
lvs_Latn Standard Latvian High Indo-European Balto-Slavic
afr_Latn Afrikaans High Indo-European Germanic
isl_Latn Icelandic High Indo-European Germanic
hin_Deva Hindi High Indo-European Indo-Aryan
pes_Arab Western Persian High Indo-European Iranian
ita_Latn Italian High Indo-European Italic
zho_Hans Chinese High Sino-Tibetan Sinitic
hau_Latn Hausa Low Afro-Asiatic Chadic
kin_Latn Kinyarwanda Low Atlantic-Congo Benue-Congo
kon_Latn Kikongo Low Atlantic-Congo Benue-Congo
lin_Latn Lingala Low Atlantic-Congo Benue-Congo
run_Latn Rundi Low Atlantic-Congo Benue-Congo
tso_Latn Tsonga Low Atlantic-Congo Benue-Congo
ewe_Latn Ewe Low Atlantic-Congo Kwa V olta-Congo
fon_Latn Fon Low Atlantic-Congo Kwa V olta-Congo
twi_Latn Twi Low Atlantic-Congo Kwa V olta-Congo
tel_Telu Telugu Low Dravidian South
mar_Deva Marathi Low Indo-European Indo-Aryan
sin_Sinh Sinhala Low Indo-European Indo-Aryan
oci_Latn Occitan Very low Indo-European Italic
Table 8: Set of 53 languages used in the experiments. We show the lang code, name, resource type, family, and
subgrouping of each language. The 30 languages used in the intermediate experiments are in the top half of the
table.

--- PAGE 14 ---
Method Metric High→High High→Low High→V . Low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model (Costa-jussà et al., 2022) 27.22 20.77 11.29 23.10 17.85 9.44 19.07 14.65 7.84 16.78
54.5B MoE model (Costa-jussà et al., 2022) 28.98 22.29 11.87 25.19 19.49 10.24 20.79 16.55 8.36 18.17
Fixed per layer (balanced)Top 1 28.39 21.82 11.64 24.22 18.67 9.92 20.05 15.98 8.13 17.62
Top 2 27.06 20.89 11.20 23.08 17.87 9.48 19.08 15.18 7.83 16.82
Load balancing 27.16 21.04 11.30 23.17 17.98 9.60 19.14 15.24 7.88 16.92
Importance (vanilla) 25.92 18.27 10.51 23.78 17.64 9.69 19.20 14.43 7.73 16.33
Importance 28.45 21.86 11.66 24.25 18.62 9.90 20.13 16.02 8.19 17.65
Global thresholdTop 1 28.33 21.50 11.26 23.54 18.16 9.26 19.72 15.45 7.69 17.18
Importance 28.43 21.56 11.28 23.52 18.37 9.40 19.74 15.54 7.69 17.25
Fixed per layer (unbalanced)
Importance28.63 22.08 11.76 24.47 18.94 10.03 20.19 16.21 8.23 17.81
Enc/Dec thresholds (balanced) 28.47 21.87 11.65 24.25 18.61 9.90 20.11 15.97 8.15 17.64
Enc/Dec thresholds (unbalanced) 28.72 22.08 11.74 24.57 18.99 10.01 20.26 16.14 8.20 17.83
Table 9: spBLEU valid scores on 30 languages of different pruning algorithms and metrics, with 75% pruning (i.e.,
384 experts are kept in total). The unbalanced approaches keep 240 encoder experts and 144 decoder experts.
0 10 20 30 40 50
Percentage of experts retained242526272829Average valid chrF++ (90 high-resource directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
0 10 20 30 40 50
Percentage of experts retained171819202122Average valid chrF++ (290 low-resource directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
0 10 20 30 40 50
Percentage of experts retained11.011.512.012.513.013.5Average valid chrF++ (490 very low resource directions)
54B model (all experts)
3.3B model (no expert)
per lang pair pruning (1:1)
per lang pair pruning (2:1)
per lang pair pruning (3:1)
global pruning (1:1)
global pruning (2:1)
Figure 4: spBLEU valid scores on 30 languages for different resource types as a function of the percentage of
experts retained. Pruning is done at the language pair granularity with the importance metric and with a fixed
number of experts per layer.
Method Enc experts Dec experts High→High High→Low High→V . Low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model 6 6 44.54 38.20 30.08 40.49 35.19 27.61 35.27 30.68 24.75 34.06
54.5B MoE model 768 768 45.90 39.19 30.24 42.29 36.35 28.18 36.55 32.16 24.93 35.07
Fixed per layer (lang-pair) 216 72 45.87 39.16 30.41 41.75 35.89 28.11 36.34 31.97 25.08 34.93
Fixed per layer (global) 216 72 44.56 37.80 29.91 40.61 34.78 27.85 35.51 31.04 25.09 34.10
Fixed per layer (lang) 216 72 45.84 39.22 30.46 41.72 35.96 28.17 36.29 32.03 25.11 34.96
Enc/dec thresholds (lang-pair) 216 72 45.89 39.19 30.39 41.77 36.02 28.21 36.28 31.97 25.07 34.95
Global threshold (lang-pair) 288 45.82 38.96 30.06 41.44 35.98 27.92 35.90 31.83 24.72 34.71
Table 10: chrF++ valid scores on 30 languages, with the importance metric for 80% pruning (1-GPU decoding) at
three different levels of granularity (global, per language or per language direction).
Method Enc experts Dec experts High→High High→Low High→V . Low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model 6 6 26.72 18.69 12.62 21.08 15.65 10.12 19.52 14.03 9.27 17.71
54.5B MoE model 768 768 28.42 20.11 13.31 22.81 16.93 10.99 21.35 15.75 9.91 19.12
Fixed per layer (lang-pair) 216 72 28.01 19.81 12.81 21.94 16.33 10.37 20.63 15.27 9.27 18.56
Fixed per layer (global) 216 72 24.15 17.15 12.26 18.34 13.89 9.77 17.41 12.78 8.88 16.03
Fixed per layer (lang) 216 72 27.87 19.82 12.78 21.79 16.37 10.35 20.51 15.28 9.19 18.50
Table 11: spBLEU test scores on 53 languages, with the importance metric for 80% pruning (1-GPU decoding) at
three different levels of granularity (global, per language or per language direction).
Method Enc experts Dec experts High→High High→Low High→V . Low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model 6 6 26.86 18.35 14.18 20.91 15.15 11.48 20.09 14.38 11.42 15.95
54.5B MoE model 768 768 28.61 19.49 15.41 22.66 16.22 12.69 22.71 16.18 12.71 17.48
Fixed per layer (lang) 216 72 28.27 19.26 15.08 22.02 15.84 12.24 21.90 15.62 12.05 16.97
Table 12: spBLEU test scores on all 202 languages, with the importance metric for 80% pruning (1-GPU decoding)
at the language granularity.
Method Enc experts Dec experts High→High High→Low High→V . Low Low→High Low→Low Low→V . low V . low→High V . low→Low V . low→V . low Average
3.3B dense model 6 6 1.14±1.23 0.52±1.15 0.84±2.17 1.34±1.30 0.62±1.23 1.00±2.24 2.07±1.46 1.38±1.62 0.95±2.23 1.10±1.83
Fixed per layer (lang) 216 72 0.01±0.58 -0.23±0.55 0.23±1.55 0.34±0.58 -0.01±0.60 0.39±1.61 0.53±0.60 0.19±0.57 0.66±1.68 0.29±1.17
3.3B dense model 6 6 1.75±1.35 1.13±1.25 1.24±2.07 1.76±1.42 1.07±1.21 1.21±1.89 2.62±1.67 1.80±1.58 1.29±1.95 1.53±1.74
Fixed per layer (lang) 216 72 0.33±0.92 0.22±0.56 0.34±1.25 0.65±1.08 0.38±0.53 0.44±1.18 0.81±1.00 0.56±0.52 0.66±1.16 0.51±0.99
Table 13: Test chrF++ deltas (first part) and spBLEU deltas (second part) with the unpruned MoE model on all 202
languages. The pruned version uses the importance metric with 80% pruning at the language granularity. Each
column reports the average score for a given language category, as well as the standard deviation. A positive value
means that this model is worse than the full 54.5B model. The last column reports the average score and standard
deviation over all 202 ×201 directions.

--- PAGE 15 ---
HausaTigrinya
Modern Standard ArabicYorubaKikongo
KinyarwandaSwahiliT songa
Northern SothoRundi
ChokweLingalaKikuyuT swanaEweT wiFonWolof
Nigerian FulfuldeVietnameseAcehnese
Central AymaraBasqueT eluguT amil
MalayalamBelarusianRussian
Standard LatvianBulgarianWelsh
IcelandicAfrikaansEnglishHindiSindhiSinhalaMarathiUrdu
Western PersianItalianFrenchOccitan
Portuguese
KabuverdianuAsturianJapaneseKoreanLuo
Yue ChineseChineseT atarFinnishHausa
Tigrinya
Modern Standard Arabic
Yoruba
Kikongo
Kinyarwanda
Swahili
T songa
Northern Sotho
Rundi
Chokwe
Lingala
Kikuyu
T swana
Ewe
T wi
Fon
Wolof
Nigerian Fulfulde
Vietnamese
Acehnese
Central Aymara
Basque
T elugu
T amil
Malayalam
Belarusian
Russian
Standard Latvian
Bulgarian
Welsh
Icelandic
Afrikaans
English
Hindi
Sindhi
Sinhala
Marathi
Urdu
Western Persian
Italian
French
Occitan
Portuguese
Kabuverdianu
Asturian
Japanese
Korean
Luo
Yue Chinese
Chinese
T atar
Finnish1.00.20.190.190.210.490.38 0.50.360.46 0.40.460.240.210.240.51 0.20.240.360.22 0.30.470.320.420.45 0.20.390.360.470.43 0.50.450.470.14 0.40.160.440.430.380.210.230.22 0.20.150.270.150.440.450.250.230.240.430.43
0.21.00.680.190.170.130.180.140.180.110.410.170.170.180.17 0.20.16 0.20.240.180.230.140.230.180.19 0.20.130.17 0.20.170.230.260.170.160.140.150.150.140.13 0.20.410.190.190.170.190.180.160.150.19 0.20.190.220.35
0.190.68 1.00.170.150.120.160.130.17 0.10.410.160.140.160.140.170.130.170.210.15 0.20.130.210.140.140.160.110.17 0.20.160.210.240.160.150.110.140.120.110.11 0.20.430.210.180.170.160.160.130.130.150.160.150.180.33
0.190.190.17 1.00.570.130.380.120.370.120.190.330.610.58 0.60.38 0.60.630.440.590.490.14 0.40.230.240.570.110.160.120.170.170.250.190.170.13 0.60.170.14 0.10.610.180.190.590.510.590.590.140.14 0.60.620.61 0.30.2
0.210.170.150.57 1.00.160.440.190.410.15 0.20.440.640.660.61 0.40.620.650.450.620.510.160.420.260.28 0.60.140.180.130.190.160.27 0.20.130.170.580.210.190.140.590.160.180.590.510.630.570.180.160.660.610.610.32 0.2
0.490.130.120.130.16 1.00.310.550.280.890.320.350.220.14 0.20.390.170.190.280.170.220.710.250.480.480.170.570.380.570.440.570.380.480.140.540.120.540.570.550.160.180.190.15 0.10.20.110.580.570.220.170.180.390.33
0.380.180.160.380.440.31 1.00.280.470.290.320.590.480.380.440.520.410.420.510.410.460.280.460.360.38 0.40.240.240.250.29 0.30.380.320.120.260.350.310.290.23 0.40.20.210.360.330.420.320.270.280.450.410.43 0.40.32
0.50.140.130.120.190.550.28 1.00.320.520.330.360.170.170.190.360.150.160.240.160.190.540.260.440.440.150.480.380.520.430.490.380.470.170.490.090.48 0.50.50.140.160.160.130.130.190.110.530.520.180.160.170.370.31
0.360.180.170.370.410.280.470.32 1.00.270.280.520.430.470.450.540.390.390.470.39 0.40.280.440.390.440.380.280.280.280.320.29 0.40.320.170.280.340.330.320.260.390.19 0.20.350.330.430.330.29 0.30.43 0.40.410.410.29
0.460.11 0.10.120.150.890.290.520.27 1.00.30.33 0.20.120.180.360.160.170.250.150.19 0.70.220.440.450.140.590.350.510.410.510.350.440.130.550.110.530.580.550.150.150.170.13 0.10.180.090.580.580.190.150.160.36 0.3
0.40.410.410.19 0.20.320.320.330.28 0.31.00.340.240.170.230.350.180.250.38 0.20.280.330.320.280.320.190.270.310.370.320.430.410.330.130.250.120.270.260.240.170.34 0.20.190.160.230.160.280.280.25 0.20.220.340.49
0.460.170.160.330.440.350.590.360.520.330.34 1.00.460.380.430.580.360.410.51 0.40.430.350.460.460.490.390.310.320.320.360.340.460.420.150.340.320.410.380.310.370.190.210.340.320.420.310.360.350.44 0.40.420.490.37
0.240.170.140.610.640.220.480.170.43 0.20.240.46 1.00.630.710.450.680.710.550.650.520.220.470.320.320.620.180.210.190.230.220.320.230.120.210.550.260.220.170.610.160.170.580.480.680.550.21 0.20.720.670.680.380.23
0.210.180.160.580.660.140.380.170.470.120.170.380.63 1.00.610.390.630.630.44 0.60.490.14 0.40.250.25 0.60.120.150.130.160.130.250.170.140.150.57 0.20.150.120.570.160.180.570.510.630.550.140.140.64 0.60.59 0.30.17
0.240.170.14 0.60.61 0.20.440.190.450.180.230.430.710.61 1.00.450.660.670.490.650.52 0.20.450.310.320.630.170.190.170.22 0.20.310.220.14 0.20.580.260.220.160.630.16 0.20.590.480.680.55 0.20.20.690.640.660.360.23
0.51 0.20.170.38 0.40.390.520.360.540.360.350.580.450.390.45 1.00.40.440.57 0.40.47 0.40.480.470.510.410.320.320.360.38 0.40.52 0.40.160.330.340.410.370.290.410.220.210.370.330.480.350.380.390.480.440.450.510.38
0.20.160.13 0.60.620.170.410.150.390.160.180.360.680.630.66 0.41.00.660.470.61 0.50.180.430.270.280.580.140.150.150.190.170.270.180.140.160.580.210.170.130.590.170.21 0.60.490.630.550.170.160.640.61 0.60.30.2
0.24 0.20.170.630.650.190.420.160.390.170.250.410.710.630.670.440.66 1.00.590.680.630.220.520.280.310.660.16 0.20.180.220.220.310.230.140.170.570.220.180.140.640.190.210.63 0.50.70.60.20.190.740.690.680.380.26
0.360.240.210.440.450.280.510.240.470.250.380.510.550.440.490.570.470.59 1.00.470.59 0.30.550.370.380.480.220.270.28 0.30.320.430.310.160.240.370.280.240.180.450.240.230.47 0.40.550.430.260.260.570.520.520.440.38
0.220.180.150.590.620.170.410.160.390.15 0.20.40.65 0.60.65 0.40.610.680.47 1.00.550.190.480.28 0.30.640.150.210.160.23 0.20.310.230.160.180.580.230.190.130.66 0.20.210.610.530.670.590.190.190.670.680.680.360.24
0.30.23 0.20.490.510.220.460.19 0.40.190.280.430.520.490.520.47 0.50.630.590.55 1.00.230.540.310.310.520.170.230.230.250.240.360.250.150.190.450.23 0.20.150.520.220.220.490.430.570.440.210.210.570.520.540.390.32
0.470.140.130.140.160.710.280.540.28 0.70.330.350.220.14 0.20.40.180.22 0.30.190.23 1.00.280.490.510.170.610.380.570.480.580.43 0.50.150.590.110.550.630.580.170.170.180.170.120.220.140.680.670.230.19 0.20.420.35
0.320.230.21 0.40.420.250.460.260.440.220.320.460.47 0.40.450.480.430.520.550.480.540.28 1.00.350.390.460.220.290.290.310.340.450.340.180.220.370.270.240.190.440.280.260.480.430.510.440.250.250.480.470.480.430.41
0.420.180.140.230.260.480.360.440.390.440.280.460.320.250.310.470.270.280.370.280.310.490.35 1.00.68 0.30.48 0.40.450.490.490.470.470.160.520.230.610.570.48 0.30.160.170.250.220.330.210.540.550.31 0.30.330.510.32
0.450.190.140.240.280.480.380.440.440.450.320.490.320.250.320.510.280.310.38 0.30.310.510.390.68 1.00.320.480.430.460.48 0.50.510.490.170.490.230.570.550.450.290.180.170.270.220.340.230.550.540.320.320.340.580.36
0.20.20.160.57 0.60.17 0.40.150.380.140.190.390.62 0.60.630.410.580.660.480.640.520.170.46 0.30.32 1.00.160.220.170.220.190.310.220.150.17 0.60.230.190.140.690.180.190.63 0.50.66 0.60.190.180.660.640.670.370.23
0.390.130.110.110.140.570.240.480.280.590.270.310.180.120.170.320.140.160.220.150.170.610.220.480.480.16 1.00.490.540.540.470.370.440.150.57 0.10.540.610.610.160.140.120.13 0.10.19 0.10.61 0.60.180.160.180.430.28
0.360.170.170.160.180.380.240.380.280.350.310.320.210.150.190.320.15 0.20.270.210.230.380.29 0.40.430.220.49 1.00.45 0.60.410.380.410.170.360.120.410.390.360.190.230.190.150.150.210.140.41 0.40.210.220.230.490.37
0.47 0.20.20.120.130.570.250.520.280.510.370.320.190.130.170.360.150.180.280.160.230.570.290.450.460.170.540.45 1.00.490.650.490.510.160.460.090.480.480.490.150.260.210.150.13 0.20.120.540.520.190.170.180.410.45
0.430.170.160.170.190.440.290.430.320.410.320.360.230.160.220.380.190.22 0.30.230.250.480.310.490.480.220.54 0.60.49 1.00.510.450.480.190.420.170.480.460.420.220.220.210.190.170.240.160.470.470.240.220.24 0.50.39
0.50.230.210.170.160.57 0.30.490.290.510.430.340.220.13 0.20.40.170.220.32 0.20.240.580.340.49 0.50.190.470.410.650.51 1.00.550.550.180.430.120.490.470.440.170.290.22 0.20.140.230.150.52 0.50.22 0.20.210.440.47
0.450.260.240.250.270.380.380.38 0.40.350.410.460.320.250.310.520.270.310.430.310.360.430.450.470.510.310.370.380.490.450.55 1.00.530.180.330.220.380.370.320.28 0.30.24 0.30.250.340.24 0.40.40.320.320.330.520.51
0.470.170.160.19 0.20.480.320.470.320.440.330.420.230.170.22 0.40.180.230.310.230.25 0.50.340.470.490.220.440.410.510.480.550.53 1.00.20.410.170.460.470.430.190.220.22 0.20.170.250.170.480.470.250.230.240.450.41
0.140.160.150.170.130.140.120.170.170.130.130.150.120.140.140.160.140.140.160.160.150.150.180.160.170.150.150.170.160.190.180.18 0.21.00.140.150.160.140.130.150.250.220.160.220.15 0.20.130.130.150.140.140.160.17
0.40.140.110.130.170.540.260.490.280.550.250.340.210.15 0.20.330.160.170.240.180.190.590.220.520.490.170.570.360.460.420.430.330.410.14 1.00.150.57 0.70.70.180.11 0.10.140.11 0.20.110.610.630.190.19 0.20.380.25
0.160.150.14 0.60.580.120.350.090.340.110.120.320.550.570.580.340.580.570.370.580.450.110.370.230.23 0.60.10.120.090.170.120.220.170.150.15 1.00.180.170.140.680.120.180.55 0.50.570.550.140.130.570.59 0.60.260.15
0.440.150.120.170.210.540.310.480.330.530.270.410.26 0.20.260.410.210.220.280.230.230.550.270.610.570.230.540.410.480.480.490.380.460.160.570.18 1.00.590.570.230.150.140.180.150.250.150.610.610.240.240.260.440.28
0.430.140.110.140.190.570.29 0.50.320.580.260.380.220.150.220.370.170.180.240.19 0.20.630.240.570.550.190.610.390.480.460.470.370.470.14 0.70.170.59 1.00.650.190.130.130.140.12 0.20.12 0.70.70.210.190.220.420.29
0.380.130.11 0.10.140.550.23 0.50.260.550.240.310.170.120.160.290.130.140.180.130.150.580.190.480.450.140.610.360.490.420.440.320.430.13 0.70.140.570.65 1.00.170.11 0.10.10.090.170.080.650.610.160.160.170.340.24
0.21 0.20.20.610.590.16 0.40.140.390.150.170.370.610.570.630.410.590.640.450.660.520.170.44 0.30.290.690.160.190.150.220.170.280.190.150.180.680.230.190.17 1.00.18 0.20.60.520.650.570.21 0.20.630.68 0.70.340.21
0.230.410.430.180.160.18 0.20.160.190.150.340.190.160.160.160.220.170.190.24 0.20.220.170.280.160.180.180.140.230.260.220.29 0.30.220.250.110.120.150.130.110.18 1.00.330.250.260.210.230.150.140.180.190.17 0.20.42
0.220.190.210.190.180.190.210.16 0.20.17 0.20.210.170.18 0.20.210.210.210.230.210.220.180.260.170.170.190.120.190.210.210.220.240.220.22 0.10.180.140.13 0.10.20.33 1.00.260.26 0.20.230.130.150.190.19 0.20.190.28
0.20.190.180.590.590.150.360.130.350.130.190.340.580.570.590.37 0.60.630.470.610.490.170.480.250.270.630.130.150.150.19 0.20.30.20.160.140.550.180.14 0.10.60.250.26 1.00.590.710.750.150.150.63 0.60.610.310.23
0.150.170.170.510.51 0.10.330.130.33 0.10.160.320.480.510.480.330.49 0.50.40.530.430.120.430.220.22 0.50.10.150.130.170.140.250.170.220.11 0.50.150.120.090.520.260.260.59 1.00.540.680.120.120.510.51 0.50.27 0.2
0.270.190.160.590.63 0.20.420.190.430.180.230.420.680.630.680.480.63 0.70.550.670.570.220.510.330.340.660.190.21 0.20.240.230.340.250.15 0.20.570.25 0.20.170.650.21 0.20.710.54 1.00.650.220.220.710.710.710.380.26
0.150.180.160.590.570.110.320.110.330.090.160.310.550.550.550.350.55 0.60.430.590.440.140.440.210.23 0.60.10.140.120.160.150.240.17 0.20.110.550.150.120.080.570.230.230.750.680.65 1.00.130.120.590.590.580.27 0.2
0.440.160.130.140.180.580.270.530.290.580.280.360.210.14 0.20.380.17 0.20.260.190.210.680.250.540.550.190.610.410.540.470.52 0.40.480.130.610.140.61 0.70.650.210.150.130.150.120.220.13 1.00.790.220.240.250.450.31
0.450.150.130.140.160.570.280.52 0.30.580.280.35 0.20.14 0.20.390.160.190.260.190.210.670.250.550.540.18 0.60.40.520.47 0.50.40.470.130.630.130.61 0.70.61 0.20.140.150.150.120.220.120.79 1.00.20.230.250.450.31
0.250.190.15 0.60.660.220.450.180.430.190.250.440.720.640.690.480.640.740.570.670.570.230.480.310.320.660.180.210.190.240.220.320.250.150.190.570.240.210.160.630.180.190.630.510.710.590.22 0.21.00.670.670.390.25
0.23 0.20.160.620.610.170.410.16 0.40.15 0.20.40.67 0.60.640.440.610.690.520.680.520.190.47 0.30.320.640.160.220.170.22 0.20.320.230.140.190.590.240.190.160.680.190.19 0.60.510.710.590.240.230.67 1.00.90.370.23
0.240.190.150.610.610.180.430.170.410.160.220.420.680.590.660.45 0.60.680.520.680.54 0.20.480.330.340.670.180.230.180.240.210.330.240.14 0.20.60.260.220.17 0.70.17 0.20.61 0.50.710.580.250.250.67 0.91.00.40.24
0.430.220.18 0.30.320.39 0.40.370.410.360.340.490.38 0.30.360.51 0.30.380.440.360.390.420.430.510.580.370.430.490.41 0.50.440.520.450.160.380.260.440.420.340.34 0.20.190.310.270.380.270.450.450.390.37 0.41.00.41
0.430.350.33 0.20.20.330.320.310.29 0.30.490.370.230.170.230.38 0.20.260.380.240.320.350.410.320.360.230.280.370.450.390.470.510.410.170.250.150.280.290.240.210.420.280.23 0.20.26 0.20.310.310.250.230.240.41 1.0Figure 5: Jaccard similarity of selected 25% decoder experts for different languages. Pruning was done per language
with the importance metric and enc/dec threshold pruning. Languages are sorted by language family.
Resource Type Criterion Language count
Very low |L| ≤100k 11
Low 100k≤ |L| ≤1m 22
High 1m≤ |L| 20
Table 14: Distribution of languages in the 53-language subset, based on the amount of available data |L|. The
30-language subset has 10 languages of each resource type. Line counts are published by Costa-jussà et al. (2022)
here: https://tinyurl.com/535f7ust

--- PAGE 16 ---
HausaTigrinya
Modern Standard ArabicYorubaKikongo
KinyarwandaSwahiliT songa
Northern SothoRundi
ChokweLingalaKikuyuT swanaEweT wiFonWolof
Nigerian FulfuldeVietnameseAcehnese
Central AymaraBasqueT eluguT amil
MalayalamBelarusianRussian
Standard LatvianBulgarianWelsh
IcelandicAfrikaansEnglishHindiSindhiSinhalaMarathiUrdu
Western PersianItalianFrenchOccitan
Portuguese
KabuverdianuAsturianJapaneseKoreanLuo
Yue ChineseChineseT atarFinnishHausa
Tigrinya
Modern Standard Arabic
Yoruba
Kikongo
Kinyarwanda
Swahili
T songa
Northern Sotho
Rundi
Chokwe
Lingala
Kikuyu
T swana
Ewe
T wi
Fon
Wolof
Nigerian Fulfulde
Vietnamese
Acehnese
Central Aymara
Basque
T elugu
T amil
Malayalam
Belarusian
Russian
Standard Latvian
Bulgarian
Welsh
Icelandic
Afrikaans
English
Hindi
Sindhi
Sinhala
Marathi
Urdu
Western Persian
Italian
French
Occitan
Portuguese
Kabuverdianu
Asturian
Japanese
Korean
Luo
Yue Chinese
Chinese
T atar
Finnish1.00.43 0.40.470.470.450.480.510.490.480.320.480.390.470.450.440.370.340.310.450.380.340.410.360.390.380.360.410.410.380.420.480.350.290.350.380.410.370.370.380.38 0.40.340.410.440.350.28 0.40.51 0.40.320.480.45
0.43 1.00.480.420.380.450.480.430.440.450.270.420.34 0.40.370.370.31 0.30.30.420.340.310.350.490.520.51 0.40.430.430.450.420.410.390.320.430.49 0.50.50.540.51 0.40.40.360.420.410.360.380.420.350.460.430.450.45
0.40.48 1.00.380.380.380.49 0.40.410.410.280.410.340.380.380.370.280.310.270.470.390.32 0.40.450.450.480.430.480.450.520.440.410.440.320.440.430.480.450.450.480.440.46 0.40.450.410.370.360.430.340.420.38 0.50.47
0.470.420.38 1.00.470.420.430.440.470.470.350.480.420.440.470.490.470.370.310.420.370.360.330.350.390.360.330.370.350.350.430.430.360.290.350.330.430.360.33 0.40.340.370.340.370.380.32 0.30.350.470.410.340.420.43
0.470.380.380.47 1.00.50.510.550.470.530.390.660.470.51 0.50.490.390.330.28 0.40.380.380.350.340.380.370.350.370.360.340.370.420.350.260.350.34 0.40.360.360.390.360.390.360.350.430.320.280.38 0.50.410.310.440.43
0.450.450.380.42 0.51.00.540.490.470.740.32 0.50.48 0.50.430.460.360.320.280.410.340.350.420.370.450.390.37 0.40.370.370.390.470.360.280.360.350.430.390.360.380.360.410.380.370.390.320.290.35 0.40.390.320.430.38
0.480.480.490.430.510.54 1.00.490.520.550.320.540.460.510.420.460.310.330.280.520.380.31 0.40.430.490.47 0.40.440.450.420.440.480.420.330.41 0.40.460.430.440.450.430.450.380.430.460.390.340.440.410.470.370.540.47
0.510.43 0.40.440.550.490.49 1.00.540.530.380.520.460.570.490.440.340.33 0.30.430.390.350.420.380.390.370.370.440.41 0.40.40.440.390.310.380.37 0.40.40.380.410.390.450.350.420.390.360.280.380.440.390.330.470.45
0.490.440.410.470.470.470.520.54 1.00.50.330.460.450.640.470.450.360.340.310.43 0.40.370.390.380.41 0.40.360.420.430.380.430.440.38 0.30.390.390.390.390.390.420.380.410.360.390.430.350.310.370.42 0.40.330.460.46
0.480.450.410.470.530.740.550.53 0.51.00.370.520.530.540.440.470.380.330.280.410.350.35 0.40.350.44 0.40.38 0.40.370.36 0.40.460.380.280.380.360.420.390.380.380.380.430.360.380.410.32 0.30.370.44 0.40.340.44 0.4
0.320.270.280.350.390.320.320.380.330.37 1.00.35 0.40.36 0.40.380.35 0.40.380.290.380.350.250.250.260.260.240.290.310.250.280.330.280.230.270.270.240.260.260.250.250.260.260.290.320.270.230.260.390.290.260.320.32
0.480.420.410.480.66 0.50.540.520.460.520.35 1.00.44 0.50.460.470.390.340.290.420.410.360.390.380.38 0.40.340.410.410.380.410.440.360.280.390.37 0.40.38 0.40.42 0.40.44 0.40.420.440.37 0.30.410.460.440.340.470.45
0.390.340.340.420.470.480.460.460.450.53 0.40.44 1.00.470.410.470.360.360.310.340.370.360.320.290.340.310.320.340.320.340.360.370.290.280.310.290.340.310.310.310.320.370.290.320.32 0.30.250.310.410.310.250.380.35
0.47 0.40.380.440.51 0.50.510.570.640.540.36 0.50.47 1.00.460.450.360.320.290.420.370.380.380.360.380.370.350.390.390.370.360.470.340.290.350.330.360.380.370.410.37 0.40.340.380.420.320.290.380.44 0.40.30.450.44
0.450.370.380.47 0.50.430.420.490.470.44 0.40.460.410.46 1.00.570.440.380.320.410.410.360.350.350.380.350.360.350.350.34 0.40.40.360.260.370.320.340.380.350.360.340.340.350.350.390.320.310.350.440.410.320.430.39
0.440.370.370.490.490.460.460.440.450.470.380.470.470.450.57 1.00.450.370.350.410.390.360.380.340.380.350.320.370.370.330.410.410.380.270.350.320.360.390.350.370.320.370.340.350.410.320.320.370.430.390.320.460.41
0.370.310.280.470.390.360.310.340.360.380.350.390.360.360.440.45 1.00.370.280.340.350.370.310.280.280.31 0.30.29 0.30.280.370.330.310.220.320.28 0.30.320.310.290.270.290.290.29 0.30.250.280.280.390.350.280.340.31
0.34 0.30.310.370.330.320.330.330.340.33 0.40.340.360.320.380.370.37 1.00.430.330.430.370.290.28 0.30.280.270.310.320.290.380.320.290.280.310.340.290.290.32 0.30.30.31 0.30.30.340.320.28 0.30.380.330.340.330.33
0.31 0.30.270.310.280.280.28 0.30.310.280.380.290.310.290.320.350.280.43 1.00.270.450.330.240.270.290.240.240.270.230.23 0.30.29 0.30.230.260.250.250.250.270.250.240.240.250.240.260.260.230.260.320.240.250.280.27
0.450.420.470.42 0.40.410.520.430.430.410.290.420.340.420.410.410.340.330.27 1.00.390.32 0.40.420.440.460.420.440.440.450.490.510.430.320.440.420.470.450.480.460.460.45 0.40.470.430.370.370.460.36 0.50.450.510.49
0.380.340.390.370.380.340.380.39 0.40.350.380.410.370.370.410.390.350.430.450.39 1.00.420.360.330.330.320.320.350.370.320.370.360.360.270.380.330.310.360.340.350.320.340.340.340.36 0.30.320.340.370.340.320.410.41
0.340.310.320.360.380.350.310.350.370.350.350.360.360.380.360.360.370.370.330.320.42 1.00.330.270.280.280.290.270.320.280.320.310.270.230.280.320.310.310.29 0.30.280.27 0.30.28 0.30.270.240.280.380.310.280.360.34
0.410.35 0.40.330.350.42 0.40.420.39 0.40.250.390.320.380.350.380.310.290.24 0.40.360.33 1.00.350.330.350.470.410.450.350.39 0.40.370.270.360.350.410.380.340.360.330.410.360.390.350.310.270.320.34 0.40.30.50.43
0.360.490.450.350.340.370.430.380.380.350.250.380.290.360.350.340.280.280.270.420.330.270.35 1.00.60.610.370.510.450.450.430.410.430.370.570.540.490.620.550.510.430.440.410.470.410.430.430.490.290.420.430.510.47
0.390.520.450.390.380.450.490.390.410.440.260.380.340.380.380.380.28 0.30.290.440.330.280.33 0.61.00.610.370.440.410.420.390.410.430.360.510.490.540.570.520.47 0.40.420.38 0.40.430.370.410.470.320.470.440.480.44
0.380.510.480.360.370.390.470.37 0.40.40.26 0.40.310.370.350.350.310.280.240.460.320.280.350.610.61 1.00.370.470.450.440.410.410.420.350.520.520.520.620.580.510.440.45 0.40.450.420.380.430.510.320.470.450.540.45
0.36 0.40.430.330.350.37 0.40.370.360.380.240.340.320.350.360.32 0.30.270.240.420.320.290.470.370.370.37 1.00.44 0.40.470.39 0.40.380.270.350.380.440.39 0.40.40.380.410.39 0.40.390.320.310.36 0.30.460.320.420.37
0.410.430.480.370.37 0.40.440.440.42 0.40.290.410.340.390.350.370.290.310.270.440.350.270.410.510.440.470.44 1.00.540.580.420.450.410.410.510.440.390.460.470.490.530.550.410.560.420.420.390.460.350.420.420.55 0.5
0.410.430.450.350.360.370.450.410.430.370.310.410.320.390.350.37 0.30.320.230.440.370.320.450.450.410.45 0.40.54 1.00.540.440.480.390.340.450.45 0.40.450.450.490.440.470.390.49 0.40.390.390.480.360.42 0.40.520.54
0.380.450.520.350.340.370.42 0.40.380.360.250.380.340.370.340.330.280.290.230.450.320.280.350.450.420.440.470.580.54 1.00.430.47 0.40.420.430.44 0.40.420.47 0.50.560.53 0.40.530.420.41 0.40.440.32 0.40.410.480.47
0.420.420.440.430.370.390.44 0.40.43 0.40.280.410.360.36 0.40.410.370.38 0.30.490.370.320.390.430.390.410.390.420.440.43 1.00.450.440.360.420.390.420.430.430.450.420.44 0.40.440.420.380.340.390.390.390.370.450.47
0.480.410.410.430.420.470.480.440.440.460.330.440.370.47 0.40.410.330.320.290.510.360.31 0.40.410.410.41 0.40.450.480.470.45 1.00.40.32 0.40.380.440.41 0.40.460.420.450.390.450.460.360.32 0.40.40.40.360.490.51
0.350.390.440.360.350.360.420.390.380.380.280.360.290.340.360.380.310.29 0.30.430.360.270.370.430.430.420.380.410.39 0.40.44 0.41.00.330.440.420.420.49 0.40.410.440.460.440.440.410.410.360.390.310.360.350.450.41
0.290.320.320.290.260.280.330.31 0.30.280.230.280.280.290.260.270.220.280.230.320.270.230.270.370.360.350.270.410.340.420.360.320.33 1.00.370.340.280.350.350.34 0.40.390.320.420.320.370.370.340.26 0.30.360.340.35
0.350.430.440.350.350.360.410.380.390.380.270.390.310.350.370.350.320.310.260.440.380.280.360.570.510.520.350.510.450.430.42 0.40.440.37 1.00.570.430.580.560.490.410.450.350.440.390.410.440.470.310.410.440.510.46
0.380.490.430.330.340.35 0.40.370.390.360.270.370.290.330.320.320.280.340.250.420.330.320.350.540.490.520.380.440.450.440.390.380.420.340.57 1.00.490.570.630.510.410.420.430.440.47 0.40.410.440.320.420.470.480.44
0.41 0.50.480.43 0.40.430.46 0.40.390.420.24 0.40.340.360.340.36 0.30.290.250.470.310.310.410.490.540.520.440.39 0.40.40.420.440.420.280.430.49 1.00.490.460.470.360.440.390.380.390.320.340.420.350.490.390.460.42
0.37 0.50.450.360.360.390.43 0.40.390.390.260.380.310.380.380.390.320.290.250.450.360.310.380.620.570.620.390.460.450.420.430.410.490.350.580.570.49 1.00.620.520.410.44 0.40.430.420.370.430.46 0.30.440.430.540.47
0.370.540.450.330.360.360.440.380.390.380.26 0.40.310.370.350.350.310.320.270.480.340.290.340.550.520.58 0.40.470.450.470.43 0.40.40.350.560.630.460.62 1.00.540.420.44 0.40.460.43 0.40.420.49 0.30.440.420.490.44
0.380.510.48 0.40.390.380.450.410.420.380.250.420.310.410.360.370.29 0.30.250.460.35 0.30.360.510.470.51 0.40.490.49 0.50.450.460.410.340.490.510.470.520.54 1.00.450.450.390.450.440.390.420.520.330.480.460.510.48
0.38 0.40.440.340.360.360.430.390.380.380.25 0.40.320.370.340.320.27 0.30.240.460.320.280.330.43 0.40.440.380.530.440.560.420.420.44 0.40.410.410.360.410.420.45 1.00.590.480.640.470.520.360.420.340.360.380.440.48
0.40.40.460.370.390.410.450.450.410.430.260.440.37 0.40.340.370.290.310.240.450.340.270.410.440.420.450.410.550.470.530.440.450.460.390.450.420.440.440.440.450.59 1.00.48 0.60.470.480.340.410.360.380.350.510.49
0.340.36 0.40.340.360.380.380.350.360.360.26 0.40.290.340.350.340.29 0.30.25 0.40.34 0.30.360.410.38 0.40.390.410.39 0.40.40.390.440.320.350.430.39 0.40.40.390.480.48 1.00.470.480.540.310.390.320.330.340.420.39
0.410.420.450.370.350.370.430.420.390.380.290.420.320.380.350.350.29 0.30.240.470.340.280.390.47 0.40.45 0.40.560.490.530.440.450.440.420.440.440.380.430.460.450.64 0.60.47 1.00.480.520.360.440.370.380.38 0.50.5
0.440.410.410.380.430.390.460.390.430.410.320.440.320.420.390.41 0.30.340.260.430.36 0.30.350.410.430.420.390.42 0.40.420.420.460.410.320.390.470.390.420.430.440.470.470.480.48 1.00.470.34 0.40.410.42 0.40.460.47
0.350.360.370.320.320.320.390.360.350.320.270.37 0.30.320.320.320.250.320.260.37 0.30.270.310.430.370.380.320.420.390.410.380.360.410.370.41 0.40.320.37 0.40.390.520.480.540.520.47 1.00.33 0.40.330.310.36 0.40.41
0.280.380.36 0.30.280.290.340.280.31 0.30.23 0.30.250.290.310.320.280.280.230.370.320.240.270.430.410.430.310.390.39 0.40.340.320.360.370.440.410.340.430.420.420.360.340.310.360.340.33 1.00.520.250.420.540.390.36
0.40.420.430.350.380.350.440.380.370.370.260.410.310.380.350.370.28 0.30.260.460.340.280.320.490.470.510.360.460.480.440.39 0.40.390.340.470.440.420.460.490.520.420.410.390.44 0.40.40.52 1.00.330.44 0.50.50.45
0.510.350.340.47 0.50.40.410.440.420.440.390.460.410.440.440.430.390.380.320.360.370.380.340.290.320.32 0.30.350.360.320.39 0.40.310.260.310.320.35 0.30.30.330.340.360.320.370.410.330.250.33 1.00.35 0.30.390.43
0.40.460.420.410.410.390.470.39 0.40.40.290.440.31 0.40.410.390.350.330.24 0.50.340.31 0.40.420.470.470.460.420.42 0.40.39 0.40.36 0.30.410.420.490.440.440.480.360.380.330.380.420.310.420.440.35 1.00.570.480.41
0.320.430.380.340.310.320.370.330.330.340.260.340.25 0.30.320.320.280.340.250.450.320.28 0.30.430.440.450.320.42 0.40.410.370.360.350.360.440.470.390.430.420.460.380.350.340.38 0.40.360.54 0.50.30.57 1.00.42 0.4
0.480.45 0.50.420.440.430.540.470.460.440.320.470.380.450.430.460.340.330.280.510.410.36 0.50.510.480.540.420.550.520.480.450.490.450.340.510.480.460.540.490.510.440.510.42 0.50.46 0.40.39 0.50.390.480.42 1.00.52
0.450.450.470.430.430.380.470.450.46 0.40.320.450.350.440.390.410.310.330.270.490.410.340.430.470.440.450.37 0.50.540.470.470.510.410.350.460.440.420.470.440.480.480.490.39 0.50.470.410.360.450.430.41 0.40.52 1.0Figure 6: Jaccard similarity of selected 25% encoder experts for different languages. Pruning was done per language
with the importance metric and enc/Dec threshold pruning. Languages are sorted by language family.
Model Hours GPU hours
3.3B 480 440
54.5B (full) 4 740 3 840
54.5B (pruned) 15 900 5 700
Total 21 120 9 980
Table 15: Time spent decoding with each type of model in this work. This includes failed or non-discussed
experiments. The “hours” column measures the total time spent by the decoding script, including model creation
and loading (note that the GPUs were reserved but idle during that time). “GPU hours” measures the time actually
spent decoding (i.e., with the GPU active).

--- PAGE 17 ---
Language pair resource type Encoder Decoder
High→High 320 64
High→Low 344 44
High→V . low 348 36
Low→High 319 65
Low→Low 343 41
Low→V . low 346 38
V . low→High 314 70
V . low→Low 338 46
V . low→V . low 340 44
Average 335 49
Table 16: Average number of experts in the encoder and
decoder for different language resource type language
pairs with global threshold 75% pruning and the impor-
tance metric.
Figure 7: Hierarchical clustering of languages based
on the importance metric of encoder experts. Different
colors represent different language subgroupings.
Figure 8: Hierarchical clustering of languages based
on the importance metric of encoder experts. Different
colors represent different language families.
Figure 9: Hierarchical clustering of languages based
on the importance metric of decoder experts. Different
colors represent different language families.
