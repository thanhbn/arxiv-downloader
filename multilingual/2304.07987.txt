# 2304.07987.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2304.07987.pdf
# File size: 774857 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CHINESE OPEN INSTRUCTION GENERALIST :
A P RELIMINARY RELEASE
Ge Zhang1 3, Yemin Shi1, Ruibo Liu1 4, Ruibin Yuan1 7, Yizhi Li1 2, Siwei Dong1, Yu Shu1, Zhaoqun Li1 5, Zekun Wang1 6,
Chenghua Lin2, Wenhao Huang1y, Jie Fu1y
1Beijing Academy of Artiﬁcial Intelligence, China
2Department of Computer Science, The University of Shefﬁeld, UK
3University of Michigan Ann Arbor, USA
4Dartmouth University, USA
5Zhejiang University, China
6Beihang University, China
7Carnegie Mellon University, USA
{ymshi, whhuang, fujie}@baai.ac.cn1,
gezhang@umich.edu1
ABSTRACT
Instruction tuning is widely recognized as a key technique for building generalist language models,
which has attracted the attention of researchers and the public with the release of InstructGPT (Ouyang
et al., 2022) and ChatGPT3. Despite impressive progress in English-oriented large-scale language
models (LLMs), it is still under-explored whether English-based foundation LLMs can perform
similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and
how we can construct the corpora needed for the tuning.
To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset
by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k
Chinese instruction tuning samples, which have been manually checked to guarantee high quality.
We also summarize the existing English and Chinese instruction corpora and brieﬂy describe some
potential applications of the newly constructed Chinese instruction corpora. The resulting Chinese
OpenInstruction Generalist ( COIG ) corpora are available in Huggingface4and Github5, and will be
continuously updated.
1 Introduction
Pre-trained large-scale language models (LLMs) have shown revolutionary performance in many downstream tasks (Guo
et al., 2023; Wei et al., 2021). One crucial ability of LLMs is called instruction following. That is, models can complete
the tasks described by instructions given as input. This ability is based on a specialized training stage called instruction
tuning . Compared to unlabeled data used for pre-training, the data for instruction tuning is typically more goal-oriented,
and it should explicitly demonstrate how a response follows its corresponding instruction with a given input .
There are many instruction tuning datasets in English. For example, the FLAN collection (Longpre et al., 2023) contains
15M examples covering 1836 tasks, and OPT-IML (Iyer et al., 2022b) claims to have 18M examples for more than 2000
tasks (although it is still not publicly available). In contrast, existing data resources for Chinese instruction tuning are
either small in scale or have questionable quality. For example, Ziang Leng and Li (2023) directly translate English
instruction tuning data into Chinese, but do not consider mitigating translation errors or potential cultural gaps, e.g.
Chinese prefer self-sacriﬁce spirit while most Western countries prefer self-expression and individualistic heroism,
The two authors contributed equally to this work.
yCorresponding authors.
3https://chat.openai.com/
4https://huggingface.co/datasets/BAAI/COIG
5https://github.com/BAAI-Zlab/COIGarXiv:2304.07987v4  [cs.CL]  25 Apr 2023

--- PAGE 2 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Veriﬁcation Format Culture Scaling
General Purpose
Academic Exams
Value Alignment
Counterfactual
Code
Table 1: The four dimensions we consider when constructing COIG instruction following data. Veriﬁcation : whether
the response can be veriﬁed. Format : whether the format is crucial. Culture : whether the response will depend on a
certain culture. Scale : whether scaling is important. The number of ﬁlled stars presents the importance of a certain
factor.
between languages. For datasets that are mostly constructed through generations using Chinese LLMs (Yunjie et al.,
2023; Xu et al., 2023; Chenghao Fan and Tian, 2023), they typically lacks a thorough data veriﬁcation process for
quality assurance. Therefore, we are motivated to develop new instruction-tuning corpora, Chinese OpenInstruction
Generalist ( COIG ), that is larger, more diverse, and manually veriﬁed by humans. This ensures its quality, which
echoes the ﬁndings of Alpaca (Taori et al., 2023) that high-quality and diverse demonstrations are crucial for good
instruction-following performance.
We highlight some unique features of COIG:
•Domain Adaption : As shown in Tab. 1, we consider four dimensions of instruction-tuning datasets (Veriﬁca-
tion, Format, Culture, Scaling). For each domain, we adapt our data collection pipeline to better reﬂect the
domain specialty.
•Diversity : We consider a variety of tasks, including common sense reasoning, human value alignment,
code generation, and hallucination correction, while very few Chinese instruction tuning data is deliberately
designed for such a complete spectrum.
•Quality Check by Humans : Compared to existing model-generated Chinese instruction corpora, including
(Ziang Leng and Li, 2023; Yunjie et al., 2023; Xue et al., 2023; JosephusCheung, 2021), COIG translated
corpus is carefully veriﬁed by human annotators. Moreover, since COIG translated corpus is translated from
English instruction corpora (Wang et al., 2022b; Honovich et al., 2022; Wang et al., 2022a) with diverse tasks,
it is much more diverse than Chinese instruction corpora built by adapting prompt engineering on existing
Chinese datasets, e.g. (Zeng et al., 2023; Yang, 2023; Guo et al., 2023).
The main portion of COIG data is actual data that already exists on the Web, and we convert it into the proper
instruction-following manner in terms of their characteristics. For example, for the academic exams domain, we crawled
and manually annotated 63.5k instructions from the Chinese National College Entrance Examination, Civil Servant
Examination, etc. COIG also features in including data on human value alignment in the Chinese-speaking world, and
leetcode-based instruction following samples for programming. To ensure the ﬁnal data quality, we hired 223 Chinese
college students as quality checkers, to help us with data ﬁltering, correction, and ratings. The resulting COIG corpus is
a comprehensive set that can equip Chinese LLMs with strong instruction-following abilities in many domains.
In addition, we provide insights into the data construction pipeline based on empirical observations. We demonstrate that
selecting the proper pipeline for different domains is crucial, and we have suggested the best practice for constructing
instruction-tuning data in the domains COIG covers ( x3), which can be used as a reference for future instruction corpus
construction workﬂow design.
The paper’s contributions are as follows:
•To the best of our knowledge, this is one of the very ﬁrst research works speciﬁcally summarizing the existing
Chinese instruction tuning corpora and providing insights about how future Chinese instruction tuning corpora
can be constructed.
•We construct 5 open-source high-quality Chinese instruction corpora, including a 68k general Chinese
instruction corpus, a 62k Chinese exam instruction corpus, a 3k Chinese human-value alignment corpus, and
a 13k Chinese Counterfactual Correction Multi-round Chat corpus, as samples of constructing new Chinese
instruction corpora along the research directions pointed out.
•We construct a manually veriﬁed general high-quality Chinese instruction tuning corpus which can be directly
used for Chinese LLMs’ instruction tuning, both commercial and non-commercial.
2

--- PAGE 3 ---
Chinese Open Instruction Generalist:
A Preliminary Release
2 Existing Instruction Corpora
Table 2: Note that the number of tasks, task types, instructions, and samples are not equivalent to one another. Only the
number of tasks and instructions are reported. If the instructional data was obtained from existing public datasets and
the data processing pipeline is publicly available, it is considered open-sourced. The ﬁeld “Veriﬁed?” refers to whether
the data has been manually veriﬁed.
Dataset # Tasks # Instructions Lan Collection
MethodUsage Access Veriﬁed?
PromptSource (Bach et al., 2022) 180 2,085 English Mixed Instruct.
TuningOpen Yes
P3 (Sanh et al., 2021) 270 2,073 English Mixed Instruct.
TuningOpen Yes
xP3 (Muennighoff et al., 2022) 83 - Multilingual Mixed Instruct.
TuningOpen No
Natural Instruct v1 (Mishra et al.,
2022)61 61 English Existing Instruct.
TuningOpen Yes
Super-Natural-Instruct v2 (Wang
et al., 2022b)1,616 1,616 Multilingual Mixed Instruct.
TuningOpen Yes
CrossFit (Ye et al., 2021) 160 - English Existing Instruct.
TuningOpen Yes
FLAN (Wei et al., 2021) 2021 62 620 English Existing Instruct.
TuningOpen Yes
ExMix (Aribandi et al., 2021) 107 107 English Existing Instruct.
TuningOpen -
UniﬁedSKG (Xie et al., 2022) 21 21 English Existing Instruct.
TuningOpen Yes
MetaICL (Min et al., 2021) 142 - English Existing Instruct.
TuningOpen Yes
InstructGPT (Ouyang et al., 2022) - 112,801 English Human
AnnotatedRLHF,
In-
struct.
TuningClosed Yes
FLAN Collection 2022 (Chung
et al., 2022; Longpre et al., 2023)1,836 18,360 English Existing Instruct.
TuningOpen No
OPT-IML Bench (Iyer et al., 2022a) 1,667 3,128 English Existing Instruct.
TuningOpen Yes
GLM-130B (Zeng et al., 2023) 74 - Multilingual Existing Instruct.
TuningOpen Yes
Self-Instruct (Wang et al., 2022a) 175 52,445 English Model
GeneratedInstruct.
TuningOpen No
Unnatural Instructions (Honovich
et al., 2022)- 240,000 English Model
GeneratedInstruct.
TuningOpen No
Alpaca (Taori et al., 2023) 175 51,942 English Model
GeneratedInstruct.
TuningOpen No
Large language models (LLMs) ﬁne-tuned to respond to speciﬁc instructions have demonstrated a remarkable zero-shot
ability to generalize to new tasks. One key ingredient is the curation of the instruction data, for which the research
community has developed various strategies for dataset construction. In this section, we provide a comprehensive
summary of the English and Chinese instruction corpora in Tab. 2, 3 and Tab. 4, respectively. We also describe the
mainstream approaches for constructing the instruction-tuning datasets below.
Human annotation. Early attempts to construct instruction data is typically through human annotation (Mishra et al.,
2022; Wang et al., 2022b; Databricks, 2022). Representative works include PromptSource (Bach et al., 2022) and Super-
Natural-Instructions (Wang et al., 2022b), both instructional datasets that require extensive manual/expert annotations
to collect instructions that can train models to follow various in-context instructions. Although human-annotated
instruction data is generally of high quality, they are also limited in quantity, diversity, and creativity. This limitation
has a signiﬁcant impact. Previous research has demonstrated a direct correlation between the size and diversity of
instructional data and the generalizability of the resultant models to previously unseen tasks (Wang et al., 2022a).
Semi- and automatic construction. To address this bottleneck and reduce dependence on human annotators, re-
searchers have proposed various methods, ranging from semi-automatic (Wang et al., 2022a), to fully automatic
instruction generation (Honovich et al., 2022). Self-Instruct (Wang et al., 2022a) is a bootstrapping framework that
3

--- PAGE 4 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Table 3: English Instruction Data (Continued from Table 2)
Dataset # Tasks # Instructions Lan Collection
MethodUsage Access Human
Veri-
ﬁed?
OIG (AI, 2021) 30 43M English Mixed Instruct.
TuningOpen No
Baize (Xu et al., 2023) 3 100K+ English Model
GeneratedChat Open No
Camel (Guohao et al., 2023) - 115K English Model
GeneratedInstruct.
Tuning,
ChatOpen No
UltraChat (Ding et al., 2023) - 675K English Model
GeneratedChat Open No
Dolly (Databricks, 2022) 7 15,000 English Human
AnnotatedInstruct.
TuningOpen Yes
Guanaco-Dataset (JosephusCheung,
2021)175 534,530 Multilingual Mixed Instruct.
TuningOpen No
ChatLLaMA Chinese-ChatLLaMA
(YDli-ai, 2021)- - Multilingual Mixed Instruct.
TuningOpen No
GPT-4-LLM (Peng et al., 2023) 175 165K Multilingual Model
GeneratedRLHF,
Instruct.
TuningOpen No
ShareGPT (ShareGPT, 2021) - - Multilingual Model
GeneratedInstruct.
Tuning,
ChatClosed Yes
SHP (Ethayarajh et al., 2023) 18 385K English Existing,
Human
AnnotatedRLHF,
Instruct.
TuningOpen Yes
HH-RLHF (Bai et al., 2022; An-
thropic, 2022; Ganguli et al., 2022)- 169,550 English Mixed RLHF,
Instruct.
TuningOpen Yes
HC3 (Guo et al., 2023) 12 37,175 Multilingual Mixed Instruct.
TuningOpen Yes
Stack-Exchange-Preferences (Lam-
bert et al., 2023)- 10M English Existing RLHF,
Instruct.
TuningOpen Yes
InstructWild (Xue et al., 2023) 429 104K Multilingual Model
GeneratedInstruct.
TuningOpen No
utilizes an initial set of manually-written instructions to guide the expansion of the instructions. The framework
generates its instructions and aligns its outputs with them, resulting in enhanced instruction-following abilities of LLMs.
Motivated by recent research on leveraging language models for data generation, Honovich et al. (2022) propose to
collect instructions by prompting an LLM. This involves eliciting additional instruction examples using a limited
number of seed instructions, and further expanding the dataset by soliciting the model to rephrase each instruction. To
promote creativity, stochastic decoding is utilized to generate diverse example inputs, while deterministic decoding
is employed for output generation to ensure accuracy. Although automated or semi-automated methods for data
construction signiﬁcantly reduce the need for human labor, they may also result in a substantial amount of noise in the
generated samples. For instance, Unnatural Instructions can exhibit up to 50% noisy samples. Therefore, it is crucial to
implement mechanisms (e.g., data pruning) that can mitigate this challenge and improve the usability of the data.
LLM Society. Given a set of manual conﬁguration, communicative or generative agents represent a promising
alternative approach to generate instruction or chat corpora (Guohao et al., 2023; Xu et al., 2023; Park et al., 2023).
Guohao et al. (2023) claim that the instruction and chat corpora generated by communicative agents can retain many
useful characteristics of CoTs and self-reﬁnement. Xu et al. (2023) validate that self-chat corpus generated by only
giving ChatGPT manual conﬁgurations is helpful for aligning LLM with human preferences.
Translation. In addition to the aforementioned methods, translation is also a primary method for constructing Chinese
instruction tuning corpora, that is, translating English corpora into Chinese. Representative datasets constructed through
translation include Luotuo (Ziang Leng and Li, 2023), BELLE (Yunjie et al., 2023), and Alpaca (Liu et al., 2023),
which are constructed by machine translation engines with little or no manual veriﬁcation. Furthermore, the popular
Alpaca (Taori et al., 2023; Liu et al., 2023) instruction corpus is limited to noncommercial use only. Nevertheless, the
Chinese instruction corpora are much scarcer, compared to the English corpora available.
4

--- PAGE 5 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Table 4: Chinese Instruction Data (Continued from Table 3)
Dataset #Tasks # Instructions Language Collection
MethodUsage Access Human
Veriﬁed?
xP3 (Muennighoff
et al., 2022)83 - Multilingual Mixed Instruct.
TuningOpen No
Super-Natural-
Instructions
(v2) (Wang et al.,
2022b)1,616 1,616 Multilingual Mixed Instruct.
TuningOpen Yes
ZeroPrompt (Xu
et al., 2022)1,110 - Chinese Human
Annotated,
Existing
DatasetInstruct.
TuningClosed Yes
GLM-130B (Zeng
et al., 2023)74 - Multilingual
(eng, zh)Existing
DatasetInstruct.
TuningOpen Yes
pCLUE (CLUEbench-
mark, 2021)9 73 Chinese Existing
DatasetInstruct.
TuningOpen Yes
Belle-1.5M (Yunjie
et al., 2023)175 1.5M Chinese Model
GeneratedInstruct.
TuningOpen No
Guanaco-Dataset
(JosephusCheung,
2021)175 534,530 Multilingual Mixed Instruct.
TuningOpen No
CSL (Li et al.,
2022)4 396,209 Chinese Existing
DatasetInstruct.
TuningOpen Yes
Chinese-
ChatLLaMA
(YDli-ai, 2021)- - Multilingual Mixed Instruct.
TuningOpen No
Fireﬂy (Yang, 2023) 23 1.1M Chinese Mixed Instruct.
TuningOpen Yes
Luotuo (Ziang Leng
and Li, 2023)175 51,672 Chinese Mixed Instruct.
TuningOpen No
Chinese-Alpaca
(Liu et al., 2023)- - Chinese Existing
Dataset,
Human
AnnotatedInstruct.
TuningOpen No
GPT-4-LLM (Peng
et al., 2023)175 165K Multilingual
(eng, zh)Model
GeneratedRLHF,
Instruct.
TuningOpen No
ShareGPT
(ShareGPT, 2021)- - Multilingual Model
GeneratedInstruct.
Tuning,
ChatClosed Yes
Chinese-Vicuna
(Chenghao Fan and
Tian, 2023)- 1M Chinese Model
Generated,
Existing
DatasetInstruct.
TuningOpen No
CUGE (Yao et al.,
2021)18 - Chinese Existing
DatasetInstruct.
TuningOpen Yes
HC3 (Guo et al.,
2023)12 37,175 Multilingual
(eng, zh)Mixed Instruct.
TuningOpen Yes
InstructWild (Xue
et al., 2023)429 104K Multilingual
(eng, zh)Model
GeneratedInstruct.
TuningOpen No
Our Translated Cor-
pus2k 67,798 Chinese Mixed Instruct.
TuningOpen Yes
5

--- PAGE 6 ---
Chinese Open Instruction Generalist:
A Preliminary Release
3 COIG: Chinese Open Instruction Generalist
To address the scarcity of instruction corpora, we propose the Chinese Open Instruction Generalist ( COIG ) project
to maintain a harmless, helpful, and diverse set of Chinese instruction corpora. We welcome all researchers in the
community to contribute to the corpus set and collaborate with us. We only release the ﬁrst chip of COIG to help the
Chinese LLMs’ development in the exploration stage and appeal to more researchers joining us in building COIG. We
separately introduce a manually veriﬁed translated general instruction corpus in § 3.1, a manually annotated exam
instruction corpus in § 3.2, a human value alignment instruction corpus in § 3.3, a multi-round counterfactual correction
chat corpus in § 3.3, and a leetcode instruction corpus in § 3.5. We provide these new instruction corpora to assist the
community with instruction tuning on Chinese LLMs. These instruction corpora are also template workﬂows for how
new Chinese instruction corpora can be built and expanded effectively.
3.1 Translation-based General Instruction Corpus
To enable the corpus for commercial and non-commercial use, we carefully select the core data of unnatural instruc-
tions (Honovich et al., 2022), the seed instruction set of self-instruct (Wang et al., 2022a), and task descriptions of
supernatural instructions (Wang et al., 2022b) as the English instruction source. These source instructions are not
generated by any OpenAI API, and are therefore available for commercial and non-commercial use (Wang et al., 2022a;
Honovich et al., 2022; Wang et al., 2022b). There are 67798 instructions in total, which are composed of 1616 task
descriptions in (Wang et al., 2022b) along with a single instance for each of them, 175 seed tasks in (Wang et al., 2022a),
and 66007 instructions from (Honovich et al., 2022).
To reduce the cost and further improve the quality of the instruction corpus, we separate the translation procedure
into three phases: automatic translation, manual veriﬁcation, and manual correction. First , during the automatic
translation phase, we concatenate the instruction with the input and output of the instances and feed them into DeepL6
for translation.
Second , during the manual veriﬁcation phase, we deﬁne 4 labels for the annotators to select for each instruction. Each
instruction is (i) directly usable; (ii) usable but with the source input and output of the instance; (iii) usable but with
manual correction, and (iv) not usable. There are fewer than twenty cases where it is not usable. We adopt a two-phase
quality veriﬁcation for the manual veriﬁcation phase: In the ﬁrst phase, each case is veriﬁed by an industrial experienced
quality inspector with more than 5 years of work experience after being annotated by the annotator. The entire corpus
can be passed into the second quality veriﬁcation phase if and only if the correctness rate exceeds 95%. The corpus got
a96:63% correctness rate in the ﬁrst quality veriﬁcation phase in the ﬁnal. Our expert quality inspectors (namely, our
coauthors) are in charge of the second quality veriﬁcation phase and only randomly sample 200 cases from the total
corpus for quality veriﬁcation. If and only if all sampled cases are classiﬁed correctly, the corpus is able to be passed
into the manual correction phase.
Third , during the manual correction phase, the annotators are asked to correct translated instructions and instances
into correct Chinese {instruction, input, output} triplets instead of just keeping the translation correct. The annotators
are asked to do it because there exist factual errors in source unnatural instructions which might lead to LLMs’
hallucinations. There are 18074 instructions fed into the manual correction phase in total. We use the same two-phase
quality veriﬁcation procedure as the manual veriﬁcation phase. The corpus got a 97:24% correctness rate in the ﬁrst
quality veriﬁcation phase of the manual correction phase.
These strict quality veriﬁcation procedures assure the reliability of the translated corpus.
3.2 Exam Instructions
The Chinese National College Entrance Examination, Middle School Entrance Examinations, and Civil Servant
Examination are the main Chinese commonsense tests. These exams contain various question formats and detailed
analysis that can be used as the Chain-of-Thought ( CoT) corpus. We use potato (Pei et al., 2022), an active learning
powered open-source annotation website template, for manual annotation, which extracts six informative elements from
original exam questions, including instruction, question context, question, answer, answer analysis, and coarse-grained
subject. There are many reading comprehension questions in these exams, and the question context means the reading
material of these reading comprehension questions. There are six main coarse-grained subjects: Chinese, English,
Politics, Biology, History, and Geology. There are very few Math, Physics, and Chemistry questions in the corpus
because these questions are often with complex symbols which are hard to annotate. We illustrate the question format
percentage in Fig. 1 and the major subject percentage in Fig. 2. For many choice questions, we recommend that the
6https://www.deepl.com/translator
6

--- PAGE 7 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Geograhpy
1.6%
History
39.6%
Biology
4.5%Chinese
15.2%
English
9.7%
Politics
29.4%
Figure 1: The percentage of instructions of different coarse-grained subjects.
Blank-Filling
4.4%
Choice
63.1%Material Analysis
19.5%
True or False
1.6%
Short Answer
0.9%
Reading
10.5%
Figure 2: The percentage of instructions in different question formats.
researchers utilize this corpus to further post-process it using prompts or post-process it to blank-ﬁlling questions to
increase the instructions’ diversity further.
3.3 Human Value Alignment Instructions
Many existing human value alignment datasets can serve instruction tuning (Anthropic, 2022; Forbes et al., 2020;
Emelin et al., 2021); however, these datasets are in English, and we ﬁnd that simply translating them into Chinese
cannot produce high-quality alignment data that matches the unique culture rooted in the Chinese-speaking world. For
example, in Western English-speaking countries, people are often encouraged to move out when they hit adulthood,
while in many Chinese-speaking communities or broader East Asian countries, it is acceptable or even encouraged for
the youth to still live with and accompany their family even though they have grown up.
To respect and reﬂect this major difference caused by different cultural backgrounds, different from other tasks in
COIG that leverage one uniﬁed collection of instruction-following samples, we categorize the value alignment data into
two separate sets: 1) a set of samples that present shared human values in the Chinese-speaking world, and 2) some
additional sets of samples that present regional-culture or country-speciﬁc human values. For the ﬁrst shared set, we
choose self-instruct (Wang et al., 2022a) as the main method to augment a set of seed instruction-following samples.
For the additional sets, to guarantee that the data genuinely reﬂect the local values, we mainly rely on web crawlers to
collect the data in original forms.
The seed instructions for shared human values are manually picked from Chinese textbooks and exams on ethics
education, since we believe most of the content in these materials has already considered the common ground of
different communities (e.g., there are 56 minorities in China). We deliberately consider the following three principles
when ﬁltering the data:
•It should present shared human values widely accepted in the Chinese-speaking world, rather than regional
ones.
• It should not include political propaganda or religious beliefs and should not be related to disputed claims.
•It should not just explain proverbs or quotes, since they will likely be covered in the knowledge retrieval
instructions-following data.
In total, we choose 50 instructions as the augmentation seeds, and produce 3k resulting instructions following samples
for general-purpose value alignment in the Chinese-speaking world. Meanwhile, we also collect 19,470 samples
as a regional addition, which is speciﬁc to users in China (including many terms that are only used in the Chinese
community). See A.1 for examples.
7

--- PAGE 8 ---
Chinese Open Instruction Generalist:
A Preliminary Release
3.4 Counterfactual Correction Multi-round Chat
LLMs have become ubiquitous in a variety of NLP applications. However, these models often generate responses that
are not truthful, and in some cases can even propagate misinformation or hallucination. The models may falsely and
repeatedly insist a claim with no sign of internal awareness that the claim was a product of their own imagination.
To mitigate above issues, and enhance the truthfulness of the model’s responses, we build the Counterfactual Correction
Multi-round Chat dataset (CCMC). It is constructed based on the CN-DBpedia knowledge base (Xu et al., 2017) with
the aim of alleviating and resolving the pain points of hallucination and factual inconsistency in current LLMs. The
original knowledge base consists of 5634k entities with their corresponding attribute-value pairs and original text.
The CCMC dataset includes 5 rounds of role-playing chat between a student and a teacher, and the corresponding
knowledge they refer to. The teacher generates responses based on ground-truth knowledge and corrects factual errors
or inconsistencies in the student’s questions or statements in each round. In the ﬁnal round, the teacher will summarize
the chat and review the confusing terms, i.e. the factual errors or inconsistencies in the student’s questions or statements.
The dataset contains 13,653 dialogues and resulting in 68,265 rounds of chat. See A.2 for example.
We outline the workﬂow for creating the CCMC dataset. The workﬂow consists of three main parts: entity selection,
information extraction, and chat generation.
We ﬁrst perform entity selection by ranking entities based on entity tag frequency and choosing the top 200. We prioritize
entities with summaries and aim to retain factual/knowledge-based content, such as well-established, historically tested
concepts, and entities related to various academic disciplines, historical events, and social events. Tags like organizations,
companies, foods, and games are excluded.
Next, we extract information from the knowledge base using a chat LLM. We ﬁrst obtain a source entity by randomly
sampling an entity from the high-priority categories, returning triplets, content summaries, and content section titles.
Then we ask a chat LLM to summarize all the information into a better summary and also extract attribute-value pairs
from the input. This can ﬁlter out some of the false tags in the Baidu tags, and also take information in the unstructured
content into consideration. For the confusion entity, we use a prompt-based method to extract a list of confusing terms
based on the input information. Then we match the terms with the knowledge base. If the term exists in the base, we
keep the term and use the same method to extract better summary and attribute-value pairs.
We employ a teacher-student question-and-answer approach for chat generation to generate attack and defense scenarios
gradually. We provide the extracted original entity summary and confusing entity summary. Then, we let the student
ask the teacher about the original concept while mistakenly mixing it up with the confusing one. The teacher would
then clarify and differentiate the concepts in a JSON format. The conversation would continue for multiple rounds,
each time with the student challenging the teacher based on previous dialogues, and the teacher providing clariﬁcations
and distinctions. In the ﬁnal round, the teacher would reintroduce the original concept and summarize the concepts that
were easily confused, emphasizing and differentiating the concepts the student had previously mixed up. All chats are
generated by prompting a chat LLM.
3.5 Leetcode Instructions
Table 5: Statistics of Leetcode Instructions. Task types C2T and T2C refer to code-to-text and text-to-code, respectively.
And e.is the abbreviation for “explanation”. Programming languages with less than 50 instructions are merged into the
“Others” class.
Programming
LanguageTask Type
C2T w/o e. C2T w/ e. T2C w/o e. T2C w/ e. All
C 8 76 12 89 185
C# 8 56 6 58 128
C++ 168 943 180 963 2254
Go 175 1008 164 899 2246
Java 213 989 193 983 2378
JavaScript 16 172 29 153 370
Python3 198 995 208 981 2382
Rust 46 252 39 252 589
SQL 35 6 30 5 76
TypeScript 98 454 82 450 1084
Others 2 20 3 20 45
All 967 4971 946 4853 11737
8

--- PAGE 9 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Given that the code-related tasks potentially contribute to the ability emergence of LLMs (Ouyang et al., 2022), we
argue that code-related tasks aligned with the Chinese natural language should be considered in our datasets. Therefore,
we build the Leetcode instructions from a CC-BY-SA-4.0 license collection7of2;589programming questions. The
questions contain problem descriptions, multiple programming languages, and explanations8.
We categorize the instruction tasks into two classes considering the input and output: code-to-text and text-to-code.
The code-to-text task requires producing function descriptions given programming codes, whereas the text-to-code
task requires output codes from the question. Depending on whether the program question has a corresponding
explanation, the task instruction will be distinguished by with/without explanation . We prepare 38types of descriptions
to generate the Leetcode instructions. We iterate through the available programming language implementations for
each programming question, randomly sample the task as code-to-text or text-to-code, and then randomly select a
corresponding instruction description.
The statistics of the Leetcode instructions are shown in Tab. 5. We derive 11;737code-related instructions in four types
of tasks in more than 10programming languages from the collection. The statistics show that the constructed dataset is
diversiﬁed and may beneﬁt LLM instruction tuning.
3.6 Empirical Validation of Instruction Corpora Construction Workﬂow
This section summarizes reasonable empirical conclusions and lessons about the Chinese instruction corpora construction
workﬂow.
First , adopting In-Context-Learning ( ICL) for generating new instructions (Wang et al., 2022a; Honovich et al., 2022)
is a key contributing factor when we want to expand the size of the instruction corpus. Taking the general-purpose
instruction corpora (Yunjie et al., 2023; Taori et al., 2023) in Tab. 1 as examples, generating these instructions is more
realistic using the ICL ability of existing LLMs instead of relying on manual annotation or other methods9. LLM
developers should carefully decide which LLMs and seed instruction corpora they prefer based on the license of the
source, the relationship of the source with OpenAI10, and their needs.
Second , human annotation or veriﬁcation is needed when there is a cultural difference between the targeted language
and the language of source instruction corpora. As in § 3.3, we must carefully select the seed in manual instruction to
ensure that the seed instructions align well with Chinese culture and do not include political propaganda or regional
beliefs. We also recommend using existing corpora, such as the method introduced in (Ethayarajh et al., 2023) when
building human value alignment instructions, where one crawls corpus from forums and post-processes it to make it
harmless.
Third , model-generated corpora need more detailed manual quality veriﬁcation, especially in cases where the output
format is crucial. During the translation and veriﬁcation procedure of the unnatural instructions (Honovich et al., 2022)
explained in § 3.1, we notice many instances that do not follow the model-generated instructions and a considerable
number of imperfect model-generated instructions. Another concern is that the diversity and distribution of model-
generated instructions are highly dependent on seed instructions. Manual selection and veriﬁcation may help sample an
instruction corpus from a large raw instruction corpus with a more balanced distribution and better diversity than the
large raw instruction corpus itself, as indicated in (Geng et al., 2023).
4 Conclusion and Discussion
We have described how we build the most comprehensive Chinese instruction dataset with careful human veriﬁcation.
Since our aim is to build a community based on the continuous update philosophy, this early-phase release is a solid
foundation and momentum for future evolution and improvement. Note that “early-phase” does not imply that the
current version is highly incomplete, but emphasizes that we commit to updating the corpora and welcome contributions
from the community. Our next major release will focus on building better instructions for collective LLMs (Park et al.,
2023; Guohao et al., 2023; Xu et al., 2023) and how to use them to improve the training of LLMs in return.
7https://github.com/doocs/leetcode
8834questions do not have explanations.
9GLM-130B (Zeng et al., 2023), T5 (Raffel et al., 2020), and various other LLMs are also capable of performing the ICL
procedure needed to generate new instructions (Honovich et al., 2022; Wang et al., 2022a) but with a relatively higher rate of error
compared to ChatGPT.
10OpenAI does not allow the content generated from their applications to be used to improve model performance without
permission. For details, see https://openai.com/policies/terms-of-use .
9

--- PAGE 10 ---
Chinese Open Instruction Generalist:
A Preliminary Release
In addition to our efforts in constructing the instruction corpora, we also want to share our thoughts on several potential
algorithmic improvements that warrant investigation. For example, it is well-known that not every training sample is
equally useful when training machine learning models. Thus, it is reasonable to conjecture that some instructions may
be more important than others when ﬁne-tuning LLMs. In practice, we are always constrained by the limited resources
for a speciﬁc training task. Fine-tuning LLMs with redundant instructions may give little beneﬁt, and constructing
meaningful and high-quality instructions is difﬁcult and costly.
Furthermore, inspired by the detrimental gradient interference phenomena studied in (Yu et al., 2020), we hypothesize
that the diverse nature of instructions could make ﬁne-tuning challenging as the gradients11obtained by various
instructions might even conﬂict with each other, making the optimization process difﬁcult to converge to good solutions.
One solution is to adopt active learning approaches to proactively and iteratively construct the most informative
instructions that beneﬁt LLM the most, rather than creating them simultaneously and treating them equally important.
This active learning-based approach requires a set of comprehensive evaluation and diagnosis toolkits to guide the
active construction procedure of instructions. Additionally, humans learn much better and faster if the training samples
are presented in a meaningful order which usually exposes the learners to gradually increasing complexity (Bengio
et al., 2009). It makes sense to incorporate this curriculum learning principle (Wang et al., 2021) into the active learning
framework (Jafarpour et al., 2021). Alternatively, after generating these instructions, we can learn to reweight the
instructions within a mini-batch or across multiple mini-batches, using meta learning (Ren et al., 2018) to mitigate the
issue of conﬂicting gradients.
5 Acknowledgement
We would like to express our gratitude to the student volunteers from Zhejiang University, China University of
Geosciences (Beijing), Beijing Language and Culture University, Zhengzhou University, Beijing University of Posts
and Telecommunications, Beihang University, and Hangzhou Dianzi University for their manual annotation and quality
control efforts. We also appreciate the non-proﬁt annotation services provided by Stardust.ai in the construction of the
translation dataset.
References
AI, L. (2021). Oig dataset. https://laion.ai/blog/oig-dataset/ . [Online; accessed 14-April-2023].
Anthropic (2022). Huggingface datasets: hh-rlhf. https://huggingface.co/datasets/Anthropic/
hh-rlhf/tree/main . [Online; accessed 14-April-2023].
Aribandi, V ., Tay, Y ., Schuster, T., Rao, J., Zheng, H., Mehta, S. V ., Zhuang, H., Tran, V ., Bahri, D., Ni, J., Gupta, J., Hui,
K., Ruder, S., and Metzler, D. (2021). Ext5: Towards extreme multi-task scaling for transfer learning. International
Conference On Learning Representations .
Bach, S. H., Sanh, V ., Yong, Z. X., Webson, A., Raffel, C., Nayak, N. V ., Sharma, A., Kim, T., Bari, M. S., Févry, T.,
Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David, S., Xu, C., Chhablani, G., Wang, H., Fries, J. A., Al-shaibani,
M. S., Sharma, S., Thakker, U., Almubarak, K., Tang, X., Jiang, M. T.-J., and Rush, A. M. (2022). Promptsource:
An integrated development environment and repository for natural language prompts. Annual Meeting Of The
Association For Computational Linguistics .
Bai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,
Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatﬁeld-Dodds, Z., Hernandez, D.,
Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish,
S., Olah, C., Mann, B., and Kaplan, J. (2022). Training a helpful and harmless assistant with reinforcement learning
from human feedback. ARXIV .ORG .
Bengio, Y ., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In Proceedings of the 26th annual
international conference on machine learning , pages 41–48.
Chenghao Fan, Z. L. and Tian, J. (2023). Chinese-vicuna: A chinese instruction-following llama-based model.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson,
A., Gu, S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V .,
11It becomes even more tricky when we pack these instructions into a mini-batch, usually constrained by the on-device memory
capacity. One remedy is to design the corresponding gradient accumulation scheme with care.
10

--- PAGE 11 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Huang, Y ., Dai, A. M., Yu, H., Petrov, S., Chi, E., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V ., and Wei, J.
(2022). Scaling instruction-ﬁnetuned language models. ARXIV .ORG .
CLUEbenchmark (2021). pclue: A parallel training framework for chinese language understanding evaluation.
https://github.com/CLUEbenchmark/pCLUE . [Online; accessed 14-April-2023].
Databricks (2022). Dolly: A high-performance deep learning library for large-scale nlp. https://github.com/
databrickslabs/dolly . [Online; accessed 14-April-2023].
Ding, N., Chen, Y ., Xu, B., Hu, S., Qin, Y ., Liu, Z., Sun, M., and Zhou, B. (2023). Ultrachat: A large-scale
auto-generated multi-round dialogue data. https://github.com/thunlp/ultrachat .
Emelin, D., Le Bras, R., Hwang, J. D., Forbes, M., and Choi, Y . (2021). Moral stories: Situated reasoning about norms,
intents, actions, and their consequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing , pages 698–718, Online and Punta Cana, Dominican Republic. Association for Computational
Linguistics.
Ethayarajh, K., Zhang, H., Wang, Y ., and Jurafsky, D. (2023). Stanford human preferences dataset.
Forbes, M., Hwang, J. D., Shwartz, V ., Sap, M., and Choi, Y . (2020). Social chemistry 101: Learning to reason about
social and moral norms. arXiv preprint arXiv:2011.00620 .
Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y ., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse,
K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S.,
Hatﬁeld-Dodds, Z., Henighan, T., Hernandez, D., Hume, T., Jacobson, J., Johnston, S., Kravec, S., Olsson, C., Ringer,
S., Tran-Johnson, E., Amodei, D., Brown, T., Joseph, N., McCandlish, S., Olah, C., Kaplan, J., and Clark, J. (2022).
Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint
arXiv: Arxiv-2209.07858 .
Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D. (2023). Koala: A dialogue model
for academic research. Blog post.
Guo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y ., Yue, J., and Wu, Y . (2023). How close is chatgpt to human
experts? comparison corpus, evaluation, and detection. arXiv preprint arxiv:2301.07597 .
Guohao, L., Hasan Abed Al, K. H., Hani, I., Dmitrii, K., and Ghanem, B. (2023). Camel: Communicative agents for
“mind” exploration of large scale language model society.
Honovich, O., Scialom, T., Levy, O., and Schick, T. (2022). Unnatural instructions: Tuning language models with
(almost) no human labor.
Iyer, S., Lin, X., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S., Li, X.,
O’Horo, B., Pereyra, G., Wang, J., Dewan, C., Celikyilmaz, A., Zettlemoyer, L., and Stoyanov, V . (2022a). Opt-iml:
Scaling language model instruction meta learning through the lens of generalization. ARXIV .ORG .
Iyer, S., Lin, X. V ., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S., et al.
(2022b). Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv
preprint arXiv:2212.12017 .
Jafarpour, B., Sepehr, D., and Pogrebnyakov, N. (2021). Active curriculum learning. In Proceedings of the First
Workshop on Interactive Learning for Natural Language Processing , pages 40–45.
JosephusCheung (2021). Guanacodataset. https://huggingface.co/datasets/JosephusCheung/
GuanacoDataset . [Online; accessed 14-April-2023].
Lambert, N., Tunstall, L., Rajani, N., and Thrush, T. (2023). Huggingface h4 stack exchange preference dataset.
Li, Y ., Zhang, Y ., Zhao, Z., Shen, L., Liu, W., Mao, W., and Zhang, H. (2022). CSL: A large-scale Chinese scientiﬁc
literature dataset. In Proceedings of the 29th International Conference on Computational Linguistics , pages 3917–
3923, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.
Liu, B., Huang, K., Jiao, L., He, Y ., Zhang, R., Liang, Y ., and Wang, Y . (2023). Chinese alpaca dataset. https:
//github.com/hikariming/alpaca_chinese_dataset .
11

--- PAGE 12 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y ., Zhou, D., Le, Q. V ., Zoph, B., Wei, J., and Roberts, A.
(2023). The ﬂan collection: Designing data and methods for effective instruction tuning. ARXIV .ORG .
Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. (2021). Metaicl: Learning to learn in context. North American
Chapter Of The Association For Computational Linguistics .
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. (2022). Cross-task generalization via natural language
crowdsourcing instructions. In ACL.
Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X.,
Schoelkopf, H., Tang, X., Radev, D., Aji, A. F., Almubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff, E., and
Raffel, C. (2022). Crosslingual generalization through multitask ﬁnetuning.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,
et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information
Processing Systems , 35:27730–27744.
Park, J. S., O’Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents:
Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442 .
Pei, J., Ananthasubramaniam, A., Wang, X., Zhou, N., Dedeloudis, A., Sargent, J., and Jurgens, D. (2022). Potato: The
portable text annotation tool. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations .
Peng, B., Li, C., He, P., Galley, M., and Gao, J. (2023). Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 .
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. (2020). Exploring
the limits of transfer learning with a uniﬁed text-to-text transformer. The Journal of Machine Learning Research ,
21(1):5485–5551.
Ren, M., Zeng, W., Yang, B., and Urtasun, R. (2018). Learning to reweight examples for robust deep learning. In
International conference on machine learning , pages 4334–4343. PMLR.
Sanh, V ., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chafﬁn, A., Stiegler, A., Scao, T. L., Raja, A.,
Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D.,
Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj,
T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Biderman, S., Gao, L., Bers, T., Wolf, T.,
and Rush, A. M. (2021). Multitask prompted training enables zero-shot task generalization.
ShareGPT (2021). Sharegpt. https://sharegpt.com/ . [Online; accessed 14-April-2023].
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford
alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca .
Wang, X., Chen, Y ., and Zhu, W. (2021). A survey on curriculum learning. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 44(9):4555–4576.
Wang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2022a). Self-instruct: Aligning
language model with self generated instructions.
Wang, Y ., Mishra, S., Alipoormolabashi, P., Kordi, Y ., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S.,
Naik, A., Stap, D., et al. (2022b). Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks.
InEMNLP .
Wei, J., Bosma, M., Zhao, V ., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V . (2021). Finetuned
language models are zero-shot learners. In International Conference on Learning Representations .
Xie, T., Wu, C. H., Shi, P., Zhong, R., Scholak, T., Yasunaga, M., Wu, C., Zhong, M., Yin, P., Wang, S. I., Zhong, V .,
Wang, B., Li, C., Boyle, C., Ni, A., Yao, Z., Radev, D., Xiong, C., Kong, L., Zhang, R., Smith, N. A., Zettlemoyer,
L., and Yu, T. (2022). Uniﬁedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text
language models. In Goldberg, Y ., Kozareva, Z., and Zhang, Y ., editors, Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December
7-11, 2022 , pages 602–631. Association for Computational Linguistics.
12

--- PAGE 13 ---
Chinese Open Instruction Generalist:
A Preliminary Release
Xu, B., Xu, Y ., Liang, J., Xie, C., Liang, B., Cui, W., and Xiao, Y . (2017). Cn-dbpedia: A never-ending chinese
knowledge extraction system. In Advances in Artiﬁcial Intelligence: From Theory to Practice: 30th International
Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2017, Arras,
France, June 27-30, 2017, Proceedings, Part II , pages 428–438. Springer.
Xu, C., Guo, D., Duan, N., and McAuley, J. (2023). Baize: An open-source chat model with parameter-efﬁcient tuning
on self-chat data. arXiv preprint arXiv: Arxiv-2304.01196 .
Xu, H., Chen, Y ., Du, Y ., Shao, N., Yanggang, W., Li, H., and Yang, Z. (2022). ZeroPrompt: Scaling prompt-based
pretraining to 1,000 tasks improves zero-shot generalization. In Findings of the Association for Computational
Linguistics: EMNLP 2022 , pages 4235–4252, Abu Dhabi, United Arab Emirates. Association for Computational
Linguistics.
Xue, F., Zheng, Z., and You, Y . (2023). Instruction in the wild: A user-based instruction dataset. https://github.
com/XueFuzhao/InstructionWild .
Yang, J. (2023). Fireﬂy. https://github.com/yangjianxin1/Firefly .
Yao, Y ., Dong, Q., Guan, J., Cao, B., Zhang, Z., Xiao, C., Wang, X., Qi, F., Bao, J., Nie, J., Zeng, Z., Gu, Y ., Zhou, K.,
Huang, X., Li, W., Ren, S., Lu, J., Xu, C., Wang, H., Zeng, G., Zhou, Z., Zhang, J., Li, J., Huang, M., Yan, R., He,
X., Wan, X., Zhao, X., Sun, X., Liu, Y ., Liu, Z., Han, X., Yang, E., Sui, Z., and Sun, M. (2021). Cuge: A chinese
language understanding and generation evaluation benchmark. ARXIV .ORG .
YDli-ai (2021). Chinese-chatllama. https://github.com/ydli-ai/Chinese-ChatLLaMA . [Online; ac-
cessed 14-April-2023].
Ye, Q., Lin, B. Y ., and Ren, X. (2021). CrossFit: A few-shot learning challenge for cross-task generalization in NLP.
InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 7163–7189,
Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. (2020). Gradient surgery for multi-task learning.
Advances in Neural Information Processing Systems , 33:5824–5836.
Yunjie, J., Yong, D., Yan, G., Yiping, P., Qiang, N., Baochang, M., and Xiangang, L. (2023). Belle: Be everyone’s large
language model engine. https://github.com/LianjiaTech/BELLE .
Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y ., Zheng, W., Xia, X., Tam, W. L., Ma, Z., Xue,
Y ., Zhai, J., Chen, W., Liu, Z., Zhang, P., Dong, Y ., and Tang, J. (2023). GLM-130b: An open bilingual pre-trained
model. In The Eleventh International Conference on Learning Representations (ICLR) .
Ziang Leng, Q. C. and Li, C. (2023). Luotuo: An instruction-following chinese language model, lora tuning on llama.
https://github.com/LC1332/Chinese-alpaca-lora .
13

--- PAGE 14 ---
Chinese Open Instruction Generalist:
A Preliminary Release
A Appendix
A.1 Human Value Alignment Instructions
Instructions Sample
Instruction:
假如你是一名校长，最近接到不少学生和家长反映，学校操场上某些同学总是进行踢足球、打篮球等活动，严重
影响了其他同学的学习和休息。请你制定一份公告，呼吁全体学生遵守校规，有序使用学校场地。
Input:
公告起草委员会已经开会商讨该公告的内容，加入你的建议，你需要在公告中提供具体制度，呼吁全体学生遵守
校规，保证课堂和自习室的安静和干净。
Output:
全校同学在使用学校的场地设施，应自觉遵守学校有关规定和管理制度，保证场地的有序和安全，维护学校正常
的教学、体育训练、课外活动等各项工作的进行。学校操场是同学们锻炼身体的场所，但也是学习的场所，应该
保持安静，不影响其他同学的学习和休息。进入球场、相邻广场玩耍一律需老师主持，并根据实际情况合理规划
使用时间；球鞋等装备必须符合安全要求。严禁未经校方批准私自使用学校未开放的区域及相关设施；公共体育
设施的使用，要坚持先预约后使用的原则。我们希望同学们在学校的各个场地内，能够自律自觉，有序文明的使
用，并与教职工一起共同创建良好的校园环境。
Table 6: Instructions for aligning AI with human value.
Instructions Sample
Instruction:
假如您在繁忙的工作中感到压力，下列哪些做法是正确的？
Input:
A．乐观开朗心态，正确认识工作
B．工作之余利用假期适当休息
C．用唾液测试压力荷尔蒙
D．抱怨工作压力
Output:
压力是生活、工作中难以避免的，如何正确应对压力是每一个人都要面对的现实问题。
AB:首先，要保持乐观的心态，正确对待工作压力，认真思考解决问题的方法，适当调整工作、生活的节奏和方
式，保持身心愉悦；当然，也需要适当的休息，放松身心，以便更好地应对工作和压力。因此，AB是正确的做
法，符合题意；
C：常规唾液测试并不能测试压力，不符合题意；
D：抱怨只会让负面情绪更加放大，不利于缓解压力。所以，D不符合题意。
故本题选AB。
Table 7: Instructions for aligning AI with human value.
14

--- PAGE 15 ---
Chinese Open Instruction Generalist:
A Preliminary Release
A.2 Counterfactual Correction Multi-round Chat
Multi-round Chat Sample
Premise:
刻点小食蚜蝇（Paragus tibialis Fallen ）是双翅目昆虫纲食蚜蝇科的一种捕食性天敌，属于动物界节肢动物门。它会对棉花等作物造
成危害，以蚜虫为寄主。该蚜蝇的成虫在4月份开始在绿肥、蚕豆和榆、槐、柳、木槿以及杂草等植物上产卵繁殖，并于5月上旬
迁入棉田，发生数量较少。卵期约3-4天，幼虫期8-9天，蛹期12-14天，由卵到成虫历期23-27天，在6-7月期间卵期一般2-3天，幼虫
期6-7天，蛹期6-7天，由卵到成虫历期14-17天。
Confusion:
食蚜蝇科，英文俗称syrphidﬂies 或ﬂowerﬂies 。形似蜂，腹节上常有黄黑相间的斑纹。R脉与M脉间有一游离的伪脉
（spuriousvein ）。幼虫体平滑或有圆锥突起及刚毛；捕食性种类的体前端尖，后端平截；粪食性种类的腹末有长的呼吸管。成
虫通常在阳光下取食花蜜和花粉；飞翔时能在空中静止不移又忽然突进。腐食性和粪食性的幼虫生活在木、粪便和腐败动植物体
中；捕食性种类取食蚜虫、介壳虫、粉虱和叶蝉等。常见种类：纤腰巴食蚜蝇Baccha Maculata Walker 。
Round 1:
Q:什么是刻点小食蚜蝇？
A:刻点小食蚜蝇是双翅目昆虫纲食蚜蝇科的一种捕食性天敌，以蚜虫为寄主，会对棉花等作物造成危害。
Round 2:
Q:刚刚您不是说刻点小食蚜蝇是属于食蚜蝇科的吗？那syrphid ﬂies 或ﬂower ﬂies 难道不是刻点小食蚜蝇的英文名吗？
A: syrphid ﬂies 或ﬂower ﬂies是食蚜蝇科的名字，而不是刻点小食蚜蝇的英文名。虽然刻点小食蚜蝇也属于食蚜蝇科，但是不同品种的
昆虫是有不同的学名和英文名的。
Round 3:
Q:您刚才所讲的关于食蚜蝇科的形态特征，如形似蜂，腹节上常有黄黑相间的斑纹；幼虫体平滑或有圆锥突起及刚毛；捕食性种类
的体前端尖，后端平截；粪食性种类的腹末有长的呼吸管等是否也与刻点小食蚜蝇的形态特征相似？
A:食蚜蝇科的形态特征您说得没错，但是刻点小食蚜蝇的形态特征可能与食蚜蝇科其他成员有所不同，因为每个物种都有其独特的
形态特征，这是我们进行它们的科学分类和分类学研究时所依据的标准。刻点小食蚜蝇的形态特征可以根据其学名或者其他资料来
具体分析。
Round 4:
Q:您刚才所讲的关于食蚜蝇科的生活习性，如成虫通常在阳光下取食花蜜和花粉；飞翔时能在空中静止不移又忽然突进；腐食性和
粪食性的幼虫生活在木、粪便和腐败动植物体中；捕食性种类取食蚜虫、介壳虫、粉虱和叶蝉等是否也与刻点小食蚜蝇的生活习性
相似？
A:您所说的食蚜蝇科的生活习性均是该科中许多物种的表现，并不是所有食蚜蝇科中的成员都会表现这些特征。刻点小食蚜蝇在食
性和习性方面可能与其他食蚜蝇科的成员有所不同。因此，要了解刻点小食蚜蝇的生活习性，需要查阅具体的资料。
Round 5:
Q:能否请您重新给出一下刻点小食蚜蝇的介绍？
A:刻点小食蚜蝇是一种捕食性天敌，属于双翅目昆虫纲食蚜蝇科中的一种。其幼虫以蚜虫为寄主，能对棉花等作物造成危害。与食
蚜蝇科中的其他成员相比，刻点小食蚜蝇的特点可能在于它在祸害棉田过程中的生命周期以及其形态特征等方面。注意要区分刻点
小食蚜蝇和食蚜蝇科，这两者是不同的概念。此外，刻点小食蚜蝇与syrphid ﬂies 或ﬂower ﬂies 这些名称也不是等同的关系。
Table 8: Instructions for aligning AI with human value.
15
