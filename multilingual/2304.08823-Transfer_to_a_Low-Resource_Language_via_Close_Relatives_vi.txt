# Chuyển giao sang Ngôn ngữ Ít Tài nguyên thông qua Các Ngôn ngữ Họ hàng Gần: 
Nghiên cứu Trường hợp về Tiếng Faroe

## Tóm tắt

Các mô hình ngôn ngữ đa ngôn ngữ đã đẩy mạnh hiệu suất tiên tiến trong chuyển giao NLP đa ngôn ngữ. Tuy nhiên, phần lớn chuyển giao đa ngôn ngữ zero-shot sử dụng cùng một transformer đa ngôn ngữ quy mô lớn (ví dụ: mBERT hoặc XLM-R) để chuyển giao cho tất cả các ngôn ngữ đích, bất kể mối quan hệ loại hình học, từ nguyên học và phát sinh học của chúng với các ngôn ngữ khác. Đặc biệt, dữ liệu và mô hình có sẵn của các ngôn ngữ họ hàng giàu tài nguyên thường bị bỏ qua. Trong nghiên cứu này, chúng tôi chứng minh thực nghiệm trong một nghiên cứu trường hợp cho tiếng Faroe - một ngôn ngữ ít tài nguyên từ một gia đình ngôn ngữ giàu tài nguyên - rằng bằng cách tận dụng thông tin phát sinh học và rời bỏ mô hình 'một kích cỡ phù hợp tất cả', người ta có thể cải thiện chuyển giao đa ngôn ngữ cho các ngôn ngữ ít tài nguyên. Cụ thể, chúng tôi tận dụng nguồn tài nguyên phong phú của các ngôn ngữ Scandinavia khác (tức là tiếng Đan Mạch, Na Uy, Thụy Điển và Iceland) để mang lại lợi ích cho tiếng Faroe. Kết quả đánh giá của chúng tôi cho thấy chúng ta có thể cải thiện đáng kể hiệu suất chuyển giao cho tiếng Faroe bằng cách khai thác dữ liệu và mô hình của các ngôn ngữ giàu tài nguyên có quan hệ gần gũi. Hơn nữa, chúng tôi phát hành một kho dữ liệu web mới của tiếng Faroe và các bộ dữ liệu Faroe cho nhận dạng thực thể có tên (NER), độ tương tự văn bản ngữ nghĩa (STS), và các mô hình ngôn ngữ mới được huấn luyện trên tất cả các ngôn ngữ Scandinavia.

## 1 Giới thiệu

Các mô hình ngôn ngữ đa ngôn ngữ quy mô lớn dựa trên Transformer (MMT) như mBERT (Devlin et al., 2019), XLM-RoBERTa (Conneau et al., 2020a) và mT5 (Xue et al., 2021) đã là động lực thúc đẩy của NLP đa ngôn ngữ hiện đại, cho phép khởi động nhanh chóng công nghệ ngôn ngữ cho nhiều ngôn ngữ ít tài nguyên thông qua chuyển giao đa ngôn ngữ (zero-shot hoặc few-shot) từ các ngôn ngữ giàu tài nguyên (Lauscher et al., 2020; Hu et al., 2020; Xu và Murray, 2022; Schmidt et al., 2022). Chuyển giao đa ngôn ngữ với MMT không phải không có nhược điểm. Không gian biểu diễn của MMT bị lệch nặng về phía các ngôn ngữ giàu tài nguyên, mà chúng đã được tiếp xúc với nhiều dữ liệu hơn trong tiền huấn luyện (Joshi et al., 2020; Wu và Dredze, 2020); kết hợp với 'lời nguyền đa ngôn ngữ' - tức là chất lượng biểu diễn trên mỗi ngôn ngữ bị hạn chế xuất phát từ khả năng hạn chế của mô hình (Conneau et al., 2020a; Pfeiffer et al., 2022) - điều này dẫn đến chất lượng biểu diễn thấp hơn cho các ngôn ngữ ít được đại diện trong tiền huấn luyện MMT. Do đó, chuyển giao đa ngôn ngữ với MMT thất bại chính xác trong các tình huống cần thiết nhất: đối với các ngôn ngữ ít tài nguyên với dấu chân số hóa nhỏ (Zhao et al., 2021). Mặc dù có những hạn chế thực tế đã được chứng minh này, phần lớn các nghiên cứu về chuyển giao đa ngôn ngữ vẫn dựa vào MMT do tính tổng quát khái niệm hấp dẫn của chúng: về lý thuyết, chúng hỗ trợ chuyển giao giữa bất kỳ hai ngôn ngữ nào được thấy trong tiền huấn luyện. Sự phụ thuộc nghiêm ngặt như vậy vào MMT thực sự bỏ qua ngữ học phát sinh và không tận dụng trực tiếp các tài nguyên của các ngôn ngữ giàu tài nguyên có quan hệ gần gũi với ngôn ngữ đích quan tâm.

Trong nghiên cứu này, chúng tôi cố gắng giảm thiểu những hạn chế trên cho một nhóm ngôn ngữ cụ thể, rời bỏ mô hình 'một kích cỡ phù hợp tất cả' dựa trên MMT. Chúng tôi tập trung vào một thiết lập thường gặp và thực tế trong đó ngôn ngữ đích là một ngôn ngữ ít tài nguyên nhưng từ một gia đình ngôn ngữ giàu tài nguyên, tức là với các ngôn ngữ giàu tài nguyên có quan hệ gần gũi. Một đánh giá toàn diện gần đây về các ngôn ngữ được sử dụng ở Châu Âu cho điểm các ngôn ngữ dựa trên các tài nguyên có sẵn. Các ngôn ngữ như tiếng Đức và Tây Ban Nha đạt khoảng 0,5 điểm so với tiếng Anh, và hơn một nửa số ngôn ngữ được chấm điểm dưới 0,02 so với điểm tiếng Anh. Nhiều ngôn ngữ, bao gồm gần như tất cả các ngôn ngữ vùng và thiểu số như tiếng Faroe, Gaelic Scotland, Occitan, Luxembourg, các ngôn ngữ Romani, Sicily và Meänkieli có điểm (gần như) bằng 0. Tuy nhiên, điều khiến những ngôn ngữ này khác biệt với các ngôn ngữ ít tài nguyên từ Châu Phi (ví dụ: gia đình Niger-Congo) hoặc các ngôn ngữ bản địa của Mỹ Latin (ví dụ: gia đình Tupian) là thực tế rằng chúng thường có các ngôn ngữ giàu tài nguyên có quan hệ gần gũi như 'ngôn ngữ họ hàng'. Trong trường hợp này, chúng tôi tin rằng các mô hình ngôn ngữ (LM) của các ngôn ngữ giàu tài nguyên có quan hệ gần gũi hứa hẹn chuyển giao hiệu quả hơn so với việc sử dụng MMT, bị ảnh hưởng bởi 'lời nguyền đa ngôn ngữ', như phương tiện chuyển giao.

Trong nghiên cứu trường hợp bằng chứng khái niệm này, chúng tôi tập trung vào tiếng Faroe như ngôn ngữ đích và chứng minh lợi ích của chuyển giao được thông báo ngôn ngữ học. Chúng tôi tận dụng dữ liệu và tài nguyên có sẵn từ các ngôn ngữ Scandinavia khác có quan hệ gần gũi nhưng 'phát triển NLP' hơn nhiều. Chúng tôi cho thấy rằng việc sử dụng LM "Scandinavia" mang lại lợi ích đáng kể trong chuyển giao downstream cho tiếng Faroe so với việc sử dụng XLM-R như một MMT được sử dụng rộng rãi. Lợi ích đặc biệt rõ ràng đối với nhiệm vụ độ tương tự văn bản ngữ nghĩa (STS), nhiệm vụ ngữ nghĩa cấp cao duy nhất trong đánh giá của chúng tôi. Chúng tôi còn cho thấy rằng việc thêm một kho dữ liệu ngôn ngữ đích kích thước hạn chế vào kho dữ liệu tiền huấn luyện của LM mang lại lợi ích tiếp theo trong chuyển giao downstream. Như một đóng góp khác của nghiên cứu này, chúng tôi thu thập và phát hành: (1) một kho dữ liệu đơn ngôn ngữ tiếng Faroe được thu thập từ web, (2) nhiều LM phù hợp cho tiếng Faroe, bao gồm những mô hình được huấn luyện trên tất cả năm ngôn ngữ Scandinavia, và (3) hai bộ dữ liệu nhiệm vụ cụ thể mới cho tiếng Faroe được gán nhãn bởi người bản ngữ: cho NER và STS.

## 2 Bối cảnh và Nghiên cứu Liên quan

**Học chuyển giao đa ngôn ngữ với MMT và xa hơn nữa.** Một cách tiếp cận phổ biến đối với học chuyển giao đa ngôn ngữ liên quan đến MMT được tiền huấn luyện (Devlin et al., 2019; Conneau et al., 2020a; Xue et al., 2021). Các mô hình này có thể được tiền huấn luyện thêm cho các ngôn ngữ cụ thể hoặc được thích ứng trực tiếp cho các nhiệm vụ downstream. Một nhược điểm chính của MMT đã được gọi là lời nguyền đa ngôn ngữ (Conneau et al., 2020a), trong đó mô hình trở nên bão hòa và hiệu suất không thể được cải thiện thêm cho một ngôn ngữ mà không hy sinh ở nơi khác, điều mà việc tiếp tục tiền huấn luyện cho một ngôn ngữ nhất định có thể giảm bớt (Pfeiffer et al., 2020). Huấn luyện adapter, như trong (Pfeiffer et al., 2020; Üstün et al., 2022), trong đó các mô-đun adapter nhỏ được thêm vào các mô hình được tiền huấn luyện, cũng đã cho phép thích ứng hiệu quả về chi phí của các mô hình này. Các adapter sau đó có thể được sử dụng để tinh chỉnh cho các ngôn ngữ và nhiệm vụ cụ thể mà không gây ra quên lãng thảm khốc.

Các phương pháp khác liên quan đến chuyển giao dựa trên dịch thuật (Hu et al., 2020; Ponti et al., 2021), và chuyển giao từ các mô hình ngôn ngữ đơn ngôn ngữ (Artetxe et al., 2020; Gogoulou et al., 2022; Minixhofer et al., 2022). Cảm ứng từ vựng song ngữ (BLI) là phương pháp ánh xạ các thuộc tính, đặc biệt là embedding, từ ngôn ngữ này sang ngôn ngữ khác thông qua một số phương tiện như căn chỉnh embedding có giám sát, khớp phân phối không giám sát hoặc sử dụng ràng buộc trực giao (Lample et al., 2018; Søgaard et al., 2018; Patra et al., 2019), và cũng đã được sử dụng để xây dựng các công cụ ngôn ngữ trong các ngôn ngữ ít tài nguyên (Wang et al., 2022).

Đã có những nỗ lực để giảm bớt các vấn đề nêu trên, như các phương pháp mở rộng từ vựng (Pfeiffer et al., 2021), thêm các token thiếu và cấu hình của chúng vào ma trận embedding. Các phương pháp lấy cảm hứng từ phát sinh học cũng đã được sử dụng trong đó các adapter được huấn luyện cho nhiều ngôn ngữ và xếp chồng để căn chỉnh với gia đình ngôn ngữ của ngôn ngữ quan tâm (Faisal và Anastasopoulos, 2022). Một số phân tích về tác động của việc sử dụng MMT được tiền huấn luyện đã được thực hiện: Fujinuma et al. (2022) kết luận rằng việc sử dụng MMT được tiền huấn luyện chia sẻ chữ viết và chồng chéo trong gia đình với ngôn ngữ đích là có lợi. Tuy nhiên, khi thích ứng mô hình cho một ngôn ngữ mới, họ tuyên bố rằng việc sử dụng càng nhiều ngôn ngữ càng tốt (lên đến 100) thường mang lại hiệu suất tốt nhất.

Được truyền cảm hứng từ dòng nghiên cứu này, trong nghiên cứu này, chúng tôi tập trung vào việc cải thiện chuyển giao đa ngôn ngữ dựa trên MMT cho một nhóm ngôn ngữ cụ thể, những ngôn ngữ có các ngôn ngữ họ hàng với dữ liệu và tài nguyên phong phú hơn.

**Tài nguyên NLP trong các Ngôn ngữ Scandinavia.** Một lượng khá lớn tài nguyên ngôn ngữ đã được phát triển cho các ngôn ngữ Scandinavia, đặc biệt nếu tổng hợp trên tất cả các ngôn ngữ của gia đình. Cũng đáng đề cập rằng tiếng Đan Mạch, Iceland, Na Uy và Thụy Điển được đại diện trong các kho dữ liệu đa ngôn ngữ thô như CC100 (Conneau et al., 2020b) hoặc mC4 (Xue et al., 2021) cũng như trong các bộ dữ liệu song song như (Schwenk et al., 2021; Agić và Vulić, 2019). Các mô hình ngôn ngữ đa ngôn ngữ lớn đã được huấn luyện trên các bộ dữ liệu này (Devlin et al., 2019; Liu et al., 2020; Xue et al., 2021) nhưng đã được chứng minh là có khả năng hạn chế đối với các ngôn ngữ có đại diện tương đối nhỏ hơn trong kho dữ liệu tiền huấn luyện. Tiếng Faroe không được bao gồm (ít nhất không được gán nhãn chính xác) trong các kho dữ liệu thu thập này. Điều này có thể một phần do lượng tiếng Faroe hạn chế có thể tìm thấy trực tuyến, và một phần do mối quan hệ gần gũi của nó với các ngôn ngữ khác trong gia đình Scandinavia (Haas và Derczynski, 2021). Một tổng quan ngắn gọn về các nghiên cứu trước đây về chuyển giao đa ngôn ngữ cho tiếng Faroe được đưa ra trong Phụ lục D.

Trong nghiên cứu này, chúng tôi sử dụng các tài nguyên ngôn ngữ mở sau đây cho các ngôn ngữ Scandinavia.

**Tiếng Đan Mạch:** Kho dữ liệu Danish Gigaword (Strømberg-Derczynski et al., 2021) là một kho dữ liệu tỷ từ chứa nhiều loại văn bản khác nhau. Chúng tôi cũng sử dụng một tài nguyên NER, kho dữ liệu DaNE (Hvingelby et al., 2020).

**Tiếng Iceland:** Với tiếng Iceland là ngôn ngữ có quan hệ gần nhất với tiếng Faroe, chúng tôi thí nghiệm với một mô hình ngôn ngữ Iceland, IceBERT (Snæbjarnarson et al., 2022). Đối với thí nghiệm NER, chúng tôi sử dụng kho dữ liệu MIM-GOLD-NER (Ingólfsdóttir et al., 2020).

**Tiếng Na Uy:** Kho dữ liệu Norwegian Colossal Corpus (NCC) (Kummervold et al., 2022) chứa 49GB dữ liệu Na Uy sạch từ nhiều nguồn khác nhau, làm cho nó trở thành bộ sưu tập công khai lớn nhất như vậy ở vùng Bắc Âu. Chúng tôi cũng sử dụng kho dữ liệu NER NorNE (Jørgensen et al., 2020) (cả cho Bokmål và Nynorsk).

**Tiếng Thụy Điển:** Kho dữ liệu Swedish Gigaword (Eide et al., 2016) chứa văn bản từ năm 1950 đến 2015. Kho dữ liệu NER mới nhất cho tiếng Thụy Điển là SweNERC (Ahrenberg et al., 2020), nơi các tác giả bao gồm các văn bản hiện đại hơn so với các kho dữ liệu trước đó.

**Tiếng Faroe:** Một kho dữ liệu POS, kho dữ liệu Sosiualurin là một kho dữ liệu báo được chú thích với 102k từ (Hansen et al., 2004). Wikipedia tiếng Faroe cũng đã được sử dụng để tạo một ngân hàng cây (Tyers et al., 2018), có ánh xạ Universal Dependencies (UD). Chúng tôi sử dụng kho dữ liệu này cùng với FarPaHc (Ingason et al., 2012), cũng có ánh xạ UD.

## 3 Bộ dữ liệu Faroe mới

### 3.1 Kho dữ liệu Common Crawl tiếng Faroe (FC3)

Dữ liệu đơn ngôn ngữ tiếng Faroe rất khan hiếm, chủ yếu do quy mô hạn chế của dân số nói tiếng Faroe. Mặc dù vậy, chúng tôi quản lý để trích xuất một lượng văn bản tiếng Faroe đa dạng khá tốt từ kho dữ liệu Common Crawl (FC3). Để thực hiện điều này, chúng tôi đã áp dụng cách tiếp cận của Snæbjarnarson et al. (2022) cho tiếng Iceland, tức là chúng tôi nhắm mục tiêu các trang web với tên miền cấp cao tiếng Faroe (.fo). Sau khi làm sạch và loại bỏ trùng lặp, kho dữ liệu tiếng Faroe thu được bao gồm 98k đoạn văn chứa tổng cộng 9M token cấp từ. Mặc dù tương đối nhỏ so với kho dữ liệu từ các ngôn ngữ Scandinavia khác, kho dữ liệu tiếng Faroe này vẫn mang lại lợi ích hiệu suất downstream đáng kể (xem §5).

### 3.2 Nhận dạng Thực thể Có tên (FoNE)

Chúng tôi chú thích kho dữ liệu Sosialurin (6.286 dòng, 102k từ) với các thực thể có tên theo lược đồ CoNLL sử dụng một bộ gắn thẻ NER Iceland được huấn luyện bằng mô hình ScandiBERT, xem §4. Chú thích sau đó được xem xét thủ công. Trong số 118.533 token (bao gồm dấu câu), 9.001 được chú thích bằng các thẻ Ngày (546), Địa điểm (1.774), Khác (332), Tiền (514), Tổ chức (2.585), Phần trăm (115), Người (2.947) và Thời gian (188). Chúng tôi gọi bộ dữ liệu mới này là FoNE.

### 3.3 Độ tương tự Ngữ nghĩa (Fo-STS)

STS Benchmark (Cer et al., 2017) đo độ tương tự văn bản ngữ nghĩa (STS) giữa các cặp câu. Đối với mỗi cặp câu, người chú thích gán điểm (trên thang Likert 1-5) cho biết mức độ mà hai câu được căn chỉnh về mặt ngữ nghĩa. Chúng tôi đã dịch thủ công từ tiếng Anh sang tiếng Faroe 729 cặp câu từ phần kiểm tra của STS Benchmark; việc dịch được thực hiện bởi một người bản ngữ tiếng Faroe thông thạo tiếng Anh, người được hướng dẫn bảo tồn trong bản dịch mức độ căn chỉnh ngữ nghĩa giữa các câu tiếng Anh gốc.

## 4 Huấn luyện Mô hình

Chúng tôi huấn luyện các mô hình ngôn ngữ mới sau đây: (i) ScandiBERT được huấn luyện trên kho dữ liệu kết hợp của tất cả các ngôn ngữ Scandinavia, (ii) ScandiBERT-no-fo được huấn luyện trên kho dữ liệu kết hợp của tất cả các ngôn ngữ Scandinavia ngoại trừ tiếng Faroe (tức là không có dữ liệu tiếng Faroe nào, nghĩa là không có FC3, Kinh thánh hoặc Sosialurin), và (iii) DanskBERT được huấn luyện chỉ trên dữ liệu tiếng Đan Mạch; chúng tôi huấn luyện DanskBERT cho mục đích so sánh với IceBERT, trong thiết lập mà chúng tôi thực hiện chuyển giao downstream cho tiếng Faroe bằng một mô hình đơn ngôn ngữ của một ngôn ngữ có quan hệ gần gũi (với tiếng Đan Mạch xa hơn với tiếng Faroe so với tiếng Iceland). Chúng tôi cũng đánh giá chuyển giao với các mô hình đã được tiền huấn luyện thêm trên kho dữ liệu FC3 (được chỉ ra bằng hậu tố -fc). Chúng tôi cung cấp tổng quan về tất cả các bộ dữ liệu huấn luyện và cấu hình siêu tham số được sử dụng trong các thí nghiệm của chúng tôi trong Phụ lục A.

## 5 Thí nghiệm

### 5.1 Hiệu suất Downstream cho tiếng Faroe

**Thiết lập Thí nghiệm.** Ngoài các mô hình được trình bày trong §4, chúng tôi sử dụng mô hình Iceland đơn ngôn ngữ IceBERT và XLM-on-RoBERTa đa ngôn ngữ quy mô lớn (XLM-R). Chúng tôi đánh giá hiệu suất của tập hợp các mô hình được tiền huấn luyện này trong một số nhiệm vụ downstream trong tiếng Faroe: Gắn thẻ Từ loại (POS), Phân tích cú pháp Phụ thuộc (DP) (bộ dữ liệu UD được giới thiệu trong §2), Nhận dạng Thực thể Có tên (NER), và Độ tương tự Văn bản Ngữ nghĩa (tức là các bộ dữ liệu NER và STS mới được giới thiệu trong §3). Đối với tất cả các nhiệm vụ downstream, dữ liệu huấn luyện và đánh giá nhiệm vụ cụ thể chỉ bao gồm các điểm dữ liệu tiếng Faroe đơn ngôn ngữ: chúng tôi thực hiện thí nghiệm thông qua xác thực chéo mười lần trên các bộ dữ liệu tiếng Faroe tương ứng. Đối với mỗi mô hình và nhiệm vụ downstream, chúng tôi thực hiện mười lần chạy với các hạt giống ngẫu nhiên khác nhau (mỗi lần chạy huấn luyện mô hình trong 5 epoch với batch 16 instance) và báo cáo hiệu suất trung bình qua các lần chạy. Ngoại lệ là huấn luyện STS trong đó các mô hình được tinh chỉnh trong 3 epoch (với batch huấn luyện kích thước 8).

**Kết quả và Thảo luận.** Bảng 1 tóm tắt kết quả trên bốn nhiệm vụ downstream. Mô hình hoạt động tốt nhất cho POS, được đánh giá trên kho dữ liệu POS Sosialurin, là ScandiBERT-fc3, vượt trội ScandiBERT hơn 1 điểm về F1. Tuy nhiên, mô hình ScandiBERT-no-fo-fc3, không có dữ liệu tiếng Faroe nào trong tiền huấn luyện, đạt hiệu suất tương đương hoàn toàn với biến thể bao gồm dữ liệu tiếng Faroe.

Mô hình hoạt động tốt nhất cho NER và STS là mô hình ScandiBERT-no-fo-fc3. Hơi bất ngờ, chúng tôi đạt được hiệu suất tốt nhất cho mô hình không bao gồm dữ liệu tiếng Faroe nào trong tiền huấn luyện ban đầu, tức là không điều chỉnh tokenizer/từ vựng cho tiếng Faroe. Nói đơn giản, chúng tôi quan sát thấy lợi ích nhẹ so với mô hình ScandiBERT-fc3. Chúng tôi giả định rằng điều này có thể do việc bao gồm tiếng Faroe trong từ vựng dẫn đến sự chồng chéo subword thấp hơn với các ngôn ngữ Scandinavia khác, điều này có thể làm giảm nhẹ tiềm năng chuyển giao. Mặc dù chỉ có sự khác biệt 95 token giữa hai từ vựng, sự khác biệt tạo ra 6% từ trong FC3 được token hóa khác nhau.

Cuối cùng, kết quả cũng chứng minh tầm quan trọng của việc tập trung vào một tập hợp nhỏ hơn các ngôn ngữ có liên quan thay vì dựa vào một tập hợp rộng hơn các ngôn ngữ từ MMT. Không giống như kết quả từ Fujinuma et al. (2022), kết quả của chúng tôi cho thấy rằng đối với các ngôn ngữ có 'anh chị em' có tài nguyên cao hơn như tiếng Faroe, một LM hoạt động tốt hơn là mô hình ScandiBERT ít tổng quát hơn thay vì một MMT như XLM-R hoặc mBERT. Các biến thể khác nhau của ScandiBERT vượt trội XLM-R không có dữ liệu tiếng Faroe nào trên tất cả các nhiệm vụ đánh giá. Một phát hiện thú vị khác là việc tinh chỉnh thêm trên dữ liệu tiếng Faroe (các biến thể -fc3) có tác động tích cực mạnh hơn nhiều đối với XLM-R như mô hình cơ bản so với ScandiBERT. Nói đơn giản, tầm quan trọng của dữ liệu ngôn ngữ đích giảm với tính khả dụng của các LM được tiền huấn luyện tập trung hơn chỉ bao gồm các ngôn ngữ liên quan đến ngôn ngữ đích.

### 5.2 Thí nghiệm bổ sung

**Chuyển giao với Wechsel.** Để đặt nghiên cứu của chúng tôi trong bối cảnh rộng hơn, ngoài so sánh với MMT, chúng tôi xem xét một cách tiếp cận học chuyển giao thay thế, phương pháp Wechsel (Minixhofer et al., 2022), một phương pháp gần đây hoạt động tốt để chuyển giao Transformer đơn ngôn ngữ sang ngôn ngữ mới. Chi tiết và kết quả thêm được trình bày trong Phụ lục B: tất cả đều cho thấy hiệu suất tệ hơn nhiều so với những kết quả được trình bày trong Bảng 1. Chúng tôi giả định điều này là do mức độ quan hệ gần gũi của các ngôn ngữ chúng tôi xem xét, trái ngược với các ngôn ngữ xa xôi được xem xét trong nghiên cứu Wechsel gốc.

**Chuyển giao Nhiệm vụ cụ thể.** Để khám phá tiềm năng chuyển giao nhiệm vụ cụ thể giữa các ngôn ngữ có quan hệ gần gũi, chúng tôi xem xét liệu các bộ dữ liệu Scandinavia được gán nhãn có thể được kết hợp để mang lại lợi ích cho tiếng Faroe. Cụ thể, chúng tôi xem xét NER vì có cách dễ dàng để ánh xạ giữa các nhãn của các ngôn ngữ khác nhau. Xem Phụ lục C để biết chi tiết. Kết quả tốt nhất đạt được khi huấn luyện trực tiếp từ mô hình IceBERT, đã được huấn luyện trên bộ dữ liệu MIM-GOLD-NER lớn, cho thấy rằng với đủ tài nguyên và mô hình ngôn ngữ đủ gần, cách tiếp cận trực tiếp như vậy có thể hiệu quả nhất.

**Thảo luận thêm.** Một số kết quả trong Bảng 1 là như mong đợi. Bắt đầu từ người họ hàng ngôn ngữ gần nhất, mô hình Iceland, IceBERT, cho kết quả tốt hơn cho tất cả các nhiệm vụ downstream so với bắt đầu với mô hình Đan Mạch DanskBERT. Mô hình ScandiBERT hoạt động tốt hơn XLM-R đa ngôn ngữ quy mô lớn trên tất cả các nhiệm vụ, trừ nhiệm vụ FO-STS ngữ nghĩa hơn.

Điều thú vị hơn là mô hình ScandiBERT-no-fo không được huấn luyện trên tiếng Faroe vượt trội mô hình bao gồm tiếng Faroe, khi được tinh chỉnh thêm trên bộ dữ liệu FC3. Đặc biệt, đối với nhiệm vụ Fo-STS cấp cao hơn. Chúng tôi giả định rằng điều này buộc việc thích ứng tiếng Faroe sử dụng phân đoạn từ từ các ngôn ngữ liên quan để có lợi ích chuyển giao cao hơn, vì từ vựng token hóa được huấn luyện không có tiếng Faroe. Đây là điều chúng tôi hy vọng sẽ điều tra thêm trong nghiên cứu tương lai.

## 6 Kết luận và Nghiên cứu Tương lai

Chúng tôi đã chỉ ra rằng tận dụng thông tin phát sinh học và rời bỏ mô hình 'một kích cỡ phù hợp tất cả' có thể cải thiện chuyển giao đa ngôn ngữ cho các ngôn ngữ ít tài nguyên. Kết quả đánh giá của chúng tôi cho thấy chúng ta có thể cải thiện đáng kể hiệu suất chuyển giao cho tiếng Faroe bằng cách khai thác dữ liệu và mô hình của các ngôn ngữ giàu tài nguyên có quan hệ gần gũi thay vì dựa vào MMT. Trong nghiên cứu tương lai, chúng tôi hy vọng mở rộng các điều tra và phương pháp ra ngoài tiếng Faroe, đến các ngôn ngữ ít tài nguyên khác mà có tồn tại các họ hàng ngôn ngữ có tài nguyên cao hơn.

Để thúc đẩy và hướng dẫn nghiên cứu tương lai về các ngôn ngữ Scandinavia nói chung và tiếng Faroe nói riêng, chúng tôi cung cấp các mô hình ScandiBERT, ScandiBERT-no-fo, DanskBERT và FoBERT (ScandiBERT-no-fo-fc3). Cũng như các bộ dữ liệu mới FC3, FoNE và Fo-STS.
