# M3IT: Một Tập Dữ Liệu Quy Mô Lớn hướng tới
Tinh Chỉnh Hướng Dẫn Đa Phương Thức Đa Ngôn Ngữ

Lei Li†, Yuwei Yin†, Shicheng Li§, Liang Chen§, Peiyi Wang§, Shuhuai Ren§, Mukai Li‡
Yazheng Yang†, Jingjing Xu‡, Xu Sun§, Lingpeng Kong†, Qi Liu†
†Đại học Hồng Kông
§Phòng thí nghiệm Quốc gia Trọng điểm về Xử lý Thông tin Đa phương tiện,
Khoa Khoa học Máy tính, Đại học Bắc Kinh
‡Phòng thí nghiệm AI Thượng Hải
nlp.lilei@gmail.com
jingjingxu@pku.edu.cn {lpk, liuqi}@cs.hku.hk

## Tóm tắt

Tinh chỉnh hướng dẫn đã thúc đẩy đáng kể các mô hình ngôn ngữ lớn (LLM) như ChatGPT, cho phép chúng phù hợp với các hướng dẫn của con người qua nhiều nhiệm vụ đa dạng. Tuy nhiên, tiến bộ trong các mô hình thị giác-ngôn ngữ mở (VLM) đã bị hạn chế do sự khan hiếm của các tập dữ liệu hướng dẫn chất lượng cao. Để giải quyết thách thức này và thúc đẩy nghiên cứu trong lĩnh vực thị giác-ngôn ngữ, chúng tôi giới thiệu tập dữ liệu Tinh chỉnh Hướng dẫn Đa phương thức, Đa ngôn ngữ (M3IT), được thiết kế để tối ưu hóa việc căn chỉnh VLM với các hướng dẫn của con người. Tập dữ liệu M3IT của chúng tôi bao gồm 40 tập dữ liệu được tuyển chọn cẩn thận, bao gồm 2,4 triệu thể hiện và 400 hướng dẫn nhiệm vụ được viết thủ công, được định dạng lại thành cấu trúc thị giác-sang-văn bản. Các nhiệm vụ chính được dịch sang 80 ngôn ngữ với một hệ thống dịch thuật tiên tiến, đảm bảo khả năng tiếp cận rộng hơn. M3IT vượt trội so với các tập dữ liệu trước đây về phạm vi nhiệm vụ, số lượng hướng dẫn và quy mô thể hiện. Hơn nữa, chúng tôi phát triển Ying-VLM, một mô hình VLM được huấn luyện trên tập dữ liệu M3IT của chúng tôi, thể hiện tiềm năng trả lời các câu hỏi phức tạp đòi hỏi kiến thức thế giới, khái quát hóa cho các nhiệm vụ video chưa thấy, và hiểu các hướng dẫn chưa thấy bằng tiếng Trung. Chúng tôi đã mở mã nguồn tập dữ liệu để khuyến khích nghiên cứu thêm.

## 1 Giới thiệu

Đã có xu hướng tăng liên tục để phát triển các trợ lý thông minh có thể tuân theo hướng dẫn của con người [3,36,37]. Trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP), tinh chỉnh hướng dẫn [35,53] là một mô hình thành công tận dụng các thể hiện được định dạng tốt quy mô lớn để căn chỉnh các mô hình ngôn ngữ lớn (LLM) với hướng dẫn của con người. Bằng cách tinh chỉnh trên các thể hiện với mô tả nhiệm vụ cụ thể, LLM học cách tuân theo hướng dẫn để thực hiện nhiều nhiệm vụ khác nhau, và thể hiện khả năng khái quát hóa mạnh mẽ trên các nhiệm vụ chưa thấy [29]. Mở rộng ra ngoài NLP, một tác nhân thông minh đa mục đích phải bao gồm nhiều phương thức khác nhau, chẳng hạn như thị giác, thúc đẩy các nỗ lực gần đây để điều tra tinh chỉnh hướng dẫn trong các lĩnh vực thị giác-ngôn ngữ [63,28,7]. Để phát triển các mô hình thị giác-ngôn ngữ (VLM) mạnh mẽ, điều cần thiết là phải có một tập dữ liệu được xây dựng tốt bao gồm các nhiệm vụ thị giác-ngôn ngữ đa dạng và phù hợp với hướng dẫn của con người. Tuy nhiên, dữ liệu hướng dẫn hỗ trợ các VLM hiện có hoặc không được công khai (ví dụ: GPT-4) hoặc cung cấp phạm vi nhiệm vụ và ngôn ngữ hạn chế (ví dụ: chỉ xem xét các nhiệm vụ bằng tiếng Anh). Sự khan hiếm của các tập dữ liệu toàn diện này đã cản trở tiến bộ của các mô hình thị giác-ngôn ngữ mở, làm nổi bật tầm quan trọng của tinh chỉnh hướng dẫn đa phương thức và nhu cầu về các tập dữ liệu chất lượng cao.

Trong bài báo này, chúng tôi nhằm thúc đẩy nghiên cứu tinh chỉnh hướng dẫn trong lĩnh vực đa phương thức bằng cách giới thiệu một tập dữ liệu mở M3IT, một tập dữ liệu Tinh chỉnh Hướng dẫn Đa phương thức Đa ngôn ngữ, như một bước thiết yếu hướng tới việc xây dựng một trợ lý đa mục đích linh hoạt. Chúng tôi xây dựng tập dữ liệu này bằng cách chuyển đổi các tập dữ liệu hiện có thành một lược đồ thị giác-sang-văn bản thống nhất với bốn giai đoạn: (1) viết hướng dẫn thủ công, (2) tiền xử lý tập dữ liệu, (3) kiểm tra chất lượng cẩn thận và (4) dịch tập dữ liệu cho các nhiệm vụ chính. Tập dữ liệu của chúng tôi bao gồm một loạt các nhiệm vụ, bao gồm các nhiệm vụ hình ảnh-văn bản cổ điển như phân loại hình ảnh, trả lời câu hỏi thị giác và mô tả hình ảnh. Các nhiệm vụ liên quan đến video, chẳng hạn như trả lời câu hỏi video, cũng được kết hợp để đảm bảo phạm vi bao quát toàn diện qua nhiều phương thức. Chúng tôi tiếp tục tích hợp các tập dữ liệu thị giác-ngôn ngữ tiếng Trung với các hướng dẫn tiếng Trung tương ứng. Tập dữ liệu kết quả biên soạn 40 nhiệm vụ đa dạng và 400 hướng dẫn. Cuối cùng, các nhiệm vụ thị giác-ngôn ngữ chính được dịch sang 80 ngôn ngữ với một hệ thống dịch thuật mạnh mẽ, để hỗ trợ các nghiên cứu đa ngôn ngữ.

Để đánh giá hiệu quả của tập dữ liệu được đề xuất, chúng tôi phát triển một mô hình thị giác-ngôn ngữ, Ying-VLM, bằng cách tích hợp một bộ mã hóa thị giác mạnh mẽ, BLIP-2 [23] với một mô hình ngôn ngữ lớn, Ziya-13B [61], được phát triển từ LLaMA [49]. Dựa trên phương pháp thành công của việc kết hợp các token thị giác như các lời nhắc văn bản trong LLM [7,63,28], chúng tôi sử dụng quy trình huấn luyện hai giai đoạn: (1) giai đoạn ban đầu căn chỉnh các đặc trưng thị giác với nhúng văn bản thông qua mô tả hình ảnh trên LAION400M [41], và (2) giai đoạn thứ hai tăng cường mô hình bằng cách tiến hành tinh chỉnh hướng dẫn trên các nhiệm vụ được chọn của tập dữ liệu của chúng tôi.

Kết quả thực nghiệm cho thấy Ying-VLM vượt trội so với các mô hình cơ sở mạnh mẽ trong các nhiệm vụ VQA có kiến thức và thể hiện hiệu suất khái quát hóa được cải thiện cho các nhiệm vụ video và đa ngôn ngữ chưa thấy. Phân tích sâu hơn cho thấy hiệu suất được cải thiện tương ứng với số lượng nhiệm vụ tăng lên cho tinh chỉnh hướng dẫn, trong khi tính đa dạng của các hướng dẫn cũng ảnh hưởng đến kết quả.

Bài báo này trình bày hai đóng góp chính: (1) Chúng tôi giới thiệu tập dữ liệu Tinh chỉnh Hướng dẫn Đa phương thức, đa ngôn ngữ (M3IT) mã nguồn mở, quy mô lớn, được thiết kế để hỗ trợ phát triển các tác nhân đa phương thức đa mục đích. (2) Chúng tôi phát triển Ying-VLM, một trợ lý thị giác xuất sắc trong các nhiệm vụ VQA có kiến thức, thể hiện khả năng khái quát hóa mạnh mẽ cho các nhiệm vụ video QA và đa phương thức tiếng Trung chưa thấy, và cung cấp những hiểu biết có giá trị cho nghiên cứu tương lai.

## 2 Công trình liên quan

Công trình của chúng tôi lấy cảm hứng từ các tiêu chuẩn tinh chỉnh hướng dẫn ngôn ngữ gần đây [53,35], đã được chứng minh hiệu quả trong việc cải thiện các mô hình ngôn ngữ để có được khả năng khái quát hóa đa nhiệm vụ [29,52]. Trong bài báo này, chúng tôi tập trung vào việc khám phá mô hình tinh chỉnh hướng dẫn từ LLM đến các tác nhân đa phương thức. Không giống như các nhiệm vụ chỉ có văn bản, các nhiệm vụ thị giác-ngôn ngữ thường có các định dạng đa dạng hơn, điều này đặt ra những thách thức mới đối với các tiêu chuẩn tinh chỉnh hướng dẫn thị giác-ngôn ngữ.

Để phát triển một mô hình thị giác-ngôn ngữ đa mục đích, điều quan trọng là tạo ra các tập dữ liệu tinh chỉnh hướng dẫn đa phương thức chất lượng cao bao gồm các nhiệm vụ, ngôn ngữ và hướng dẫn đa dạng. Một số nghiên cứu đã điều tra tinh chỉnh hướng dẫn đa phương thức cho VLM. LLaVA [28] và MiniGPT-4 [63] tạo ra cuộc đối thoại liên quan đến nội dung thị giác bằng cách kết hợp dữ liệu mô tả hình ảnh vào các mô hình GPT-4/ChatGPT. MultiInstruct [56] định dạng lại một loạt các nhiệm vụ phân loại thị giác thành định dạng tinh chỉnh-hướng dẫn, trong khi InstructBLIP [7] điều chỉnh 28 nhiệm vụ hình ảnh-sang-văn bản hiện có. Tuy nhiên, các tập dữ liệu này không cung cấp một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức lý tưởng do (1) phạm vi bao quát hạn chế của các loại nhiệm vụ khác nhau trong các lĩnh vực đa phương thức, (2) tính đa dạng và chất lượng của các thể hiện, và (3) việc bao gồm nhiều ngôn ngữ cho sự đa dạng ngôn ngữ rộng. Trong bài báo này, chúng tôi xây dựng một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức được cải thiện bằng cách mở rộng phạm vi nhiệm vụ lên 40 tập dữ liệu, bổ sung các thể hiện với 10 hướng dẫn nhiệm vụ được viết thủ công, và bao gồm các nhiệm vụ bằng các ngôn ngữ khác nhau.

## 3 M3IT: Một Tập Dữ Liệu Tinh Chỉnh Hướng Dẫn Đa Phương Thức Đa Ngôn Ngữ

Trong phần này, chúng tôi giới thiệu tập dữ liệu M3IT được đề xuất bằng cách đầu tiên trình bày chi tiết về phạm vi bao quát của tập dữ liệu (§ 3.1), tiếp theo là các chi tiết của quy trình chú thích (§ 3.2). Cuối cùng, chúng tôi trình bày định dạng tập dữ liệu và cung cấp thống kê của các hướng dẫn tập dữ liệu được tạo ra (§ 3.3).

### 3.1 Phạm Vi Nhiệm Vụ

Tập dữ liệu của chúng tôi biên soạn các nhiệm vụ đa dạng của các nhiệm vụ thị giác-ngôn ngữ cổ điển, bao gồm mô tả, trả lời câu hỏi thị giác (VQA), tạo ra có điều kiện thị giác, lý luận và phân loại.

**Mô tả** Nhiệm vụ này nhằm tạo ra các mô tả về các hình ảnh được cho theo các nhu cầu khác nhau. Chúng tôi bao gồm MS COCO [27] (phân chia Karpathy) cho các mô tả hình ảnh chung. TextCaps [44] yêu cầu các mô hình nắm bắt văn bản được trình bày trong hình ảnh và tạo ra mô tả tương ứng. Image-Paragraph-Captioning [21] tập trung vào việc tạo ra các mô tả chi tiết cho hình ảnh.

**Lý luận** Nhiệm vụ này đánh giá các khả năng lý luận cụ thể. Chúng tôi kết hợp CLEVR [19] và NLVR [46] cho lý luận không gian, Visual Commonsense Reasoning (VCR) [60] cho lý luận thường thức, Visual MRC [47] cho đọc hiểu toàn diện trên hình ảnh, và Winoground [48] cho lý luận ngữ nghĩa tinh tế trên mô tả văn bản và nội dung hình ảnh.

**Trả lời Câu hỏi Thị giác (VQA)** Đây là nhiệm vụ đa phương thức được nghiên cứu rộng rãi nhất, yêu cầu mô hình trả lời đúng một câu hỏi được cho dựa trên hình ảnh. Các nhiệm vụ bao gồm VQA v2 [15], Shapes VQA [1], DocVQA [33], OCR-VQA [34], ST-VQA [2], Text-VQA [45], và GQA [18].

**Trả lời Câu hỏi Thị giác Có Kiến thức** Không giống như các nhiệm vụ VQA truyền thống tập trung vào câu hỏi liên quan đến nội dung hình ảnh, trả lời câu hỏi thị giác có kiến thức (KVQA) yêu cầu mô hình tận dụng kiến thức bên ngoài để trả lời câu hỏi. Chúng tôi kết hợp hai tập dữ liệu VQA kiến thức bên ngoài: OK-VQA [32] và A-OK-VQA [42], ScienceQA [31] chứa các câu hỏi khoa học đa phương thức, và ViQuAE [22] tập trung vào các sự kiện kiến thức của các thực thể có tên trong hình ảnh.

**Phân loại** Nhiệm vụ này bao gồm việc phân loại một hình ảnh dựa trên một tập hợp các nhãn ứng viên được cho. ImageNet [40], Grounded Object Identification (COCO-GOI) [27], COCO-Text [50], Image Text Matching (COCO-ITM) [27], e-SNLI-VE [20], Multi-modal Fact Checking (Mocheg) [58], và IQA [9] được bao gồm. Do các ràng buộc về độ dài đầu vào của mô hình ngôn ngữ, chúng tôi giảm số lượng tùy chọn trong một số tập dữ liệu với nhãn ứng viên rộng, chẳng hạn như ImageNet.

**Tạo ra** Tạo ra có điều kiện thị giác yêu cầu các mô hình hiểu nội dung thị giác và tạo ra một tác phẩm đáp ứng yêu cầu nhiệm vụ. Chúng tôi có Visual Storytelling (VIST) [17], Visual Dialog (VisDial) [8], và dịch máy đa phương thức Multi30k [10] trong danh mục này.

**Nhiệm vụ Thị giác-Ngôn ngữ Tiếng Trung và Đa ngôn ngữ** Để kiểm tra hiệu quả của tinh chỉnh hướng dẫn trên các ngôn ngữ khác nhau, chúng tôi kết hợp một số nhiệm vụ thị giác-ngôn ngữ tiếng Trung bao gồm FM-IQA [11] cho VQA, COCO-CN [25] và Flickr8k-CN [24] cho mô tả, Chinese Food Net [4] cho phân loại, và MMChat [62] cho tạo ra.

**Nhiệm vụ Video-Ngôn ngữ** Ngoài các hình ảnh tĩnh, chúng tôi quan tâm đến việc liệu tinh chỉnh hướng dẫn cũng có thể được áp dụng cho các nhiệm vụ video-văn bản. Chúng tôi bao gồm các tập dữ liệu MSR-VTT cổ điển [55] cho mô tả video, MSRVTT-QA [54], ActivityNet-QA [59], iVQA [57] và MSVD-QA [54] cho trả lời câu hỏi video, Something-Something [14] cho phân loại hành động video.

Như được hiển thị trong Hình 1, tập dữ liệu của chúng tôi có phạm vi bao quát rộng của các tiêu chuẩn thị giác-ngôn ngữ và video-ngôn ngữ hiện có, cho phép các bộ kỹ năng khác nhau cho các mô hình ngôn ngữ, từ mô tả hình ảnh đơn giản đến lý luận phức tạp dựa trên hình ảnh thậm chí vượt ra ngoài nội dung thị giác.

### 3.2 Quy Trình Chú Thích

Để xây dựng các tập dữ liệu hướng dẫn đa phương thức chất lượng cao, chúng tôi viết lại các tập dữ liệu khác nhau thành định dạng thị giác-sang-văn bản. Quy trình chú thích bao gồm bốn bước: (1) viết hướng dẫn cho mỗi nhiệm vụ, (2) cấu trúc hình ảnh và văn bản thành một lược đồ thống nhất, (3) kiểm tra chất lượng tập dữ liệu tổng thể, và (4) xây dựng các bộ đa ngôn ngữ. Tám tác giả của công trình này được tuyển dụng làm người chú thích con người, mỗi người là một sinh viên sau đại học quen thuộc với tài liệu liên quan.

**Giai đoạn I: Viết Hướng dẫn** Để xây dựng các hướng dẫn chất lượng cao, đầu tiên chúng tôi yêu cầu các người chú thích đọc cẩn thận bài báo tập dữ liệu và kiểm tra tập dữ liệu gốc với một số thể hiện để có được hiểu biết rõ ràng về nhiệm vụ. Sau đó, họ được yêu cầu viết thủ công 10 hướng dẫn nhiệm vụ đa dạng, bao gồm các đặc điểm chính của nhiệm vụ. Bảng 2 hiển thị thống kê của các hướng dẫn được viết cho mỗi nhiệm vụ. Tổng cộng, chúng tôi chú thích 400 hướng dẫn cho tất cả các nhiệm vụ. Độ dài trung bình mỗi hướng dẫn là 24,4. Để đánh giá tính đa dạng của các hướng dẫn được chú thích, chúng tôi sử dụng khoảng cách chỉnh sửa trung bình để đo độ tương tự giữa hai chuỗi. Khoảng cách chỉnh sửa trung bình trong cùng một nhiệm vụ là 76,6, cho thấy phạm vi đa dạng hướng dẫn tốt.

**Giai đoạn II: Thống nhất Định dạng Dữ liệu** Sau khi hướng dẫn đã được viết theo đặc điểm nhiệm vụ, chúng tôi tiếp tục xử lý hình ảnh và văn bản tương ứng cho một lược đồ thể hiện thống nhất. Đối với hầu hết các tập dữ liệu, chúng tôi giữ nguyên hình ảnh và văn bản gốc, trong đó hình ảnh được chuyển đổi thành các chuỗi được mã hóa base64 tương ứng để dễ dàng tải dữ liệu. Chúng tôi thực hiện hai sửa đổi trên các ví dụ tiềm năng:

(1) **Thêm Hộp Giới hạn vào Hình ảnh.** Đối với các nhiệm vụ được thiết kế cho các vùng cụ thể trong hình ảnh, một giải pháp đơn giản là cung cấp thông tin hộp giới hạn bằng ngôn ngữ tự nhiên để thông báo cho các mô hình ngôn ngữ về các vùng quan tâm. Tuy nhiên, các kỹ thuật tiền xử lý hình ảnh được áp dụng bởi các bộ mã hóa thị giác khác nhau có thể thay đổi kích thước hình ảnh gốc, và chú thích hộp giới hạn gốc do đó cần điều chỉnh thêm. Lấy cảm hứng từ quan sát gần đây rằng các bộ mã hóa thị giác phổ biến như CLIP [39] nhạy cảm với lời nhắc thị giác [43], chúng tôi trực tiếp gắn thẻ hộp giới hạn như một hình chữ nhật màu đỏ vào hình ảnh, phục vụ như một gợi ý cho VLM tập trung vào vùng mục tiêu.

(2) **Diễn giải Câu trả lời Ngắn.** Như các nghiên cứu gần đây đã chỉ ra rằng các câu trả lời ngắn gọn và ngắn gọn gốc trong tập dữ liệu VQA phổ biến có thể ảnh hưởng tiêu cực đến hiệu suất tạo ra của mô hình [7], chúng tôi đề xuất sử dụng mô hình ChatGPT [36] để diễn giải các câu trả lời gốc, bằng cách cung cấp câu hỏi và câu trả lời gốc với thông tin ngữ cảnh tiềm năng bổ sung. Thông tin ngữ cảnh bao gồm mô tả của hình ảnh gốc và token OCR cho câu hỏi liên quan đến cảnh. Lời nhắc được sử dụng để diễn giải câu trả lời có thể được tìm thấy trong Phụ lục. Hình 2 minh họa các sửa đổi dữ liệu chúng tôi đã thực hiện trên tập dữ liệu của chúng tôi.

**Giai đoạn III: Kiểm tra Chất lượng** Trong giai đoạn này, chúng tôi chỉ định một người chú thích khác cho mỗi nhiệm vụ để xem xét 10 ví dụ từ mỗi phân chia. Trong giai đoạn này, chúng tôi xác định các sự không nhất quán định dạng nhỏ giữa các nhiệm vụ và giải quyết chúng bằng cách chuẩn hóa các định dạng nhiệm vụ. Chúng tôi cũng quan sát thấy rằng một số câu trả lời (ít hơn 3% của các thể hiện được kiểm tra) không được diễn giải hiệu quả bởi ChatGPT do thông tin hình ảnh không đủ. Chúng tôi sử dụng heuristics đơn giản để lọc các câu trả lời được diễn giải này và sử dụng một mẫu cơ bản để chuyển đổi câu trả lời gốc thành một câu. Chúng tôi thấy rằng phần nhỏ này của các câu trả lời được diễn giải không thành công có tác động không đáng kể. Cuối cùng, tập dữ liệu nhiệm vụ được coi là hoàn thành khi người chú thích có thể tải thành công và kiểm tra lại tính chính xác của các hướng dẫn, đầu vào và đầu ra cho mỗi thể hiện được kiểm tra.

**Giai đoạn IV: Dịch Tập Dữ liệu Chính** Để tăng cường đa dạng ngôn ngữ và hỗ trợ đánh giá qua các ngôn ngữ khác nhau, chúng tôi chọn một tập con của các tập dữ liệu (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT và MSRVTT-QA) bao gồm các nhiệm vụ khác nhau và dịch dữ liệu đánh giá của chúng sang 100 ngôn ngữ theo FLORES-101 [13]. Chúng tôi dịch 500 mẫu cho mỗi phân chia của mỗi nhiệm vụ trong phiên bản đầu tiên của chúng tôi. Nhiều mẫu đa ngôn ngữ hơn sẽ được hỗ trợ trong tương lai. Chúng tôi áp dụng phiên bản chưng cất NLLB-1.3B [6] để dịch, một trong những mô hình dịch đa ngôn ngữ mở tiên tiến nhất. Vì không có người bản ngữ cho các ngôn ngữ khác nhau, chúng tôi áp dụng một cơ chế lọc tự động để đảm bảo chất lượng dịch thuật, trong đó các ngôn ngữ có điểm BLEU dịch từ tiếng Anh lớn hơn 20 dựa trên kết quả FLORES-101 được giữ lại. Sau bước này, chỉ có 80 ngôn ngữ được giữ lại (xem Phụ lục cho tên ngôn ngữ chi tiết).

### 3.3 Định Dạng Tập Dữ Liệu

Thể hiện trong tập dữ liệu của chúng tôi bao gồm năm trường: (1) **Hình ảnh**: chúng tôi biểu diễn các hình ảnh với hộp giới hạn có thể được thêm vào bằng một chuỗi base64. (2) **Hướng dẫn**: chúng tôi chọn ngẫu nhiên một hướng dẫn từ nhóm hướng dẫn nhiệm vụ cho mỗi thể hiện. (3) **Đầu vào**: chúng tôi phân bổ trường này để cung cấp đầu vào cụ thể của nhiệm vụ cho mô hình, ví dụ: câu hỏi trong các nhiệm vụ VQA. Đối với các nhiệm vụ như mô tả, không có đầu vào bổ sung nên trường tương ứng được để trống. (4) **Đầu ra**: đầu ra yêu cầu cho các nhiệm vụ cụ thể, chẳng hạn như mô tả của hình ảnh cho các nhiệm vụ mô tả và câu trả lời cho câu hỏi liên quan đến hình ảnh. (5) **Dữ liệu Meta**: chúng tôi cung cấp trường này để bảo toàn thông tin quan trọng như id hình ảnh để tham chiếu tập dữ liệu gốc. Hình 3 minh họa một thể hiện trong định dạng thống nhất. Với sự phân biệt rõ ràng của các trường này, người dùng tiêu chuẩn của chúng tôi có thể linh hoạt xây dựng các thể hiện huấn luyện cần thiết và đánh giá các mô hình một cách thuận tiện. Bảng 3 đưa ra thống kê được tổng hợp theo nhiệm vụ, và chúng tôi giới thiệu độc giả đến Phụ lục để biết thống kê chi tiết và giấy phép của mỗi tập dữ liệu.

## 4 Thực nghiệm

Trong phần này, chúng tôi xây dựng một VLM để xác nhận hiệu quả của tập dữ liệu M3IT được đề xuất cho các tác nhân đa phương thức. Chúng tôi đầu tiên giới thiệu các thiết lập thực nghiệm (§ 4.1), sau đó báo cáo và thảo luận kết quả (§ 4.2). Cuối cùng, chúng tôi phân tích ảnh hưởng của số lượng nhiệm vụ và tính đa dạng hướng dẫn, và cung cấp một kết quả định tính (§ 4.3).

### 4.1 Thiết Lập Thực Nghiệm

**Chi tiết Triển khai** Lấy cảm hứng từ thành công gần đây của BLIP [23], chúng tôi áp dụng bộ mã hóa thị giác và kiến trúc Q-former trong mô hình BLIP2-OPT-2.7B [23] để trích xuất các đặc trưng thị giác liên quan từ hình ảnh. Đối với các mô hình ngôn ngữ lớn, chúng tôi sử dụng Ziya-13B [61] được phát triển từ LLaMA [49] với khả năng song ngữ (tiếng Anh và tiếng Trung). Chúng tôi sử dụng huấn luyện hai giai đoạn.

**Giai đoạn I Căn chỉnh Thị giác-Văn bản**: Để căn chỉnh không gian đặc trưng thị giác và văn bản, chúng tôi sử dụng các hướng dẫn trong mô tả coco và thực hiện huấn luyện căn chỉnh ban đầu trên LAION 400M [41]. Chúng tôi huấn luyện Q-former và phép chiếu ngôn ngữ, dẫn đến tổng cộng 130M tham số để tối ưu hóa với AdamW [30]. Kích thước batch được đặt thành 256 để tối đa hóa việc sử dụng GPU và mô hình được huấn luyện với 300k bước. Tốc độ học tăng tuyến tính đến giá trị đỉnh 5e-5 trong 2000 bước đầu tiên và theo bộ lập lịch suy giảm cosine. Trọng số suy giảm được đặt thành 0.05.

**Giai đoạn II Tinh chỉnh Hướng dẫn Đa phương thức**: Chúng tôi tiếp tục thực hiện tinh chỉnh hướng dẫn đa phương thức trong tiêu chuẩn của chúng tôi để kích hoạt tiềm năng to lớn của LLM. Chúng tôi huấn luyện mô hình sau huấn luyện căn chỉnh trong 3 epoch và với tốc độ học thấp hơn là 1e-5 và giai đoạn khởi động 1000 bước. Lấy cảm hứng từ điều chỉnh LoRa [16], các trọng số để ánh xạ các vector truy vấn và giá trị trong lớp attention của LLM có thể học được trong giai đoạn này để thích nghi tốt hơn với tập dữ liệu tinh chỉnh hướng dẫn. Các tham số huấn luyện khác nhất quán với Giai đoạn I. Tất cả các thí nghiệm được tiến hành với 8 GPU NVIDIA 80GB A100. Phải mất khoảng 10 ngày cho Giai đoạn I và Giai đoạn II có thể hoàn thành trong một ngày.

**Thiết lập Đánh giá** Để kiểm tra khả năng khái quát hóa của tinh chỉnh hướng dẫn, một số nhiệm vụ được giữ lại để đánh giá (xem Hình 1 cho các nhiệm vụ held-in/out). Chúng tôi quan tâm đến các câu hỏi nghiên cứu sau: (RQ1) Tinh chỉnh hướng dẫn đa phương thức có thể kích thích kiến thức thế giới từ LLM không? (RQ2) Tinh chỉnh hướng dẫn chỉ bằng tiếng Anh có thể khái quát hóa cho các ngôn ngữ khác như tiếng Trung không? và (RQ3) Tinh chỉnh hướng dẫn đa phương thức chỉ hình ảnh có thể khái quát hóa cho các nhiệm vụ video-ngôn ngữ không? Đối với RQ1, chúng tôi đánh giá các mô hình của chúng tôi trên ba nhiệm vụ KVQA trong tập dữ liệu của chúng tôi, tức là OK-VQA [32], A-OKVQA [42] và ViQuAE. Đối với RQ2 và RQ3, chúng tôi thực hiện đánh giá chuyển giao zero-shot trên các tập dữ liệu thị giác-ngôn ngữ tiếng Trung và video-ngôn ngữ, tương ứng. Chúng tôi sử dụng giải mã tham lam trong suy luận nếu không được chỉ định khác.

**Số liệu** Chúng tôi áp dụng ROUGE-L [26] làm số liệu tự động để đánh giá tính nhất quán giữa dự đoán và câu trả lời sự thật cơ sở, tập trung vào việc đánh giá khả năng đối thoại của mô hình. Vì số liệu tự động có thể không nắm bắt đầy đủ các sắc thái của chất lượng đối thoại, chúng tôi tiếp tục giới thiệu GPT-4 như một đại diện của các đánh giá viên con người (§ 4.2).

**Đường cơ sở** Chúng tôi so sánh các mô hình của chúng tôi với các tác nhân đa phương thức mạnh mẽ được đề xuất gần đây, bao gồm (1) BLIP-2-Flan-T5-XXL [23] nơi một Flan-T5 được tinh chỉnh hướng dẫn [53] được kết nối với một bộ mã hóa thị giác mạnh mẽ để thực hiện một loạt các nhiệm vụ đa phương thức; (2) MiniGPT-4 căn chỉnh một bộ mã hóa thị giác CLIP với một Vicuna đông lạnh [5] với tập dữ liệu đối thoại được thu thập nhân tạo; và (3) InstructBLIP, một tác nhân đa phương thức được tăng cường tinh chỉnh hướng dẫn được đề xuất gần đây với Vicuna-13B với các tập dữ liệu đa mô hình được chuyển đổi và tập dữ liệu LLaVA [28] được tạo ra bởi GPT-4.

### 4.2 Kết Quả Chính

**RQ1: Đánh giá Trả lời Câu hỏi Thị giác Có Kiến thức** Kết quả trên các tiêu chuẩn KVQA được hiển thị trong Bảng 4. So với đường cơ sở mạnh nhất, mô hình của chúng tôi đạt được cải thiện 3,2 và 2,7 điểm ROUGE-L cho OK-VQA và A-OKVQA, tương ứng. Ngoài ra, Ying-VLM mang lại hiệu suất tốt nhất trên tập dữ liệu ViQuAE được giữ lại. Những phát hiện này cho thấy rằng tinh chỉnh hướng dẫn trên M3IT hiệu quả khai thác kiến thức từ LLM và nâng cao chất lượng phản hồi.

**RQ2: Chuyển giao Zero-Shot cho Các Nhiệm vụ Thị giác-Ngôn ngữ Tiếng Trung** Chúng tôi đánh giá các mô hình trên ba nhiệm vụ thị giác-ngôn ngữ tiếng Trung chưa thấy để điều tra khả năng khái quát hóa đa ngôn ngữ của tinh chỉnh hướng dẫn. BLIP-2 không được xem xét, vì Flan-T5 không hỗ trợ tiếng Trung. Như được minh họa trong Bảng 5, mô hình của chúng tôi vượt trội so với MiniGPT4 và InstructBLIP trên tất cả các nhiệm vụ được đánh giá, thể hiện cải thiện đáng chú ý. Những phát hiện này cho thấy rằng tinh chỉnh hướng dẫn với các tập dữ liệu tiếng Anh có thể hiệu quả khái quát hóa cho các ngôn ngữ khác nhau, thể hiện tiềm năng đầy hứa hẹn có thể được khám phá thêm.

**RQ3: Chuyển giao Zero-Shot cho Các Nhiệm vụ Video-Ngôn ngữ** Để đánh giá hiệu suất trên các nhiệm vụ video-ngôn ngữ, chúng tôi lấy mẫu đồng đều 8 khung hình từ mỗi video. Một so sánh với MiniGPT4 bị loại trừ, vì nó không hỗ trợ đầu vào video. Theo phương pháp của InstructBLIP [7], chúng tôi nối nhúng thị giác được trích xuất từ Q-former của mỗi khung hình như một nhúng tiền tố cho mô hình ngôn ngữ. Như được thể hiện trong Bảng 6, mô hình của chúng tôi xuất sắc trong những thiết lập thách thức này, vượt trội đáng kể so với các đường cơ sở chuỗi BLIP. Đáng chú ý là tập dữ liệu huấn luyện không bao gồm bất kỳ đầu vào thị giác nào như video, ngụ ý rằng tinh chỉnh hướng dẫn của chúng tôi hiệu quả hỗ trợ mô hình trong việc khái quát hóa cho đầu vào video với chiều thời gian.

**Kết quả Đánh giá GPT-4** Để xác nhận thêm chất lượng của phản hồi được tạo ra, chúng tôi đề xuất sử dụng mô hình GPT-4 mạnh mẽ như một đại diện của các đánh giá viên con người [38,12]. Cụ thể, theo Vicuna [5], chúng tôi sử dụng GPT-4 để đánh giá hiệu suất của các mô hình khác nhau so với Ying-VLM của chúng tôi. Xem xét chi phí API của GPT-4, 300 ví dụ được lấy mẫu ngẫu nhiên từ các tập dữ liệu OK-VQA, A-OKVQA và ViQuAE làm tập con để đánh giá. Đối với mỗi mẫu, chúng tôi xây dựng một lời nhắc bao gồm câu hỏi gốc, câu trả lời tham chiếu tương ứng, phản hồi được tạo ra bởi Ying-VLM của chúng tôi, và đầu ra hệ thống cơ sở. GPT-4 được truy vấn với lời nhắc để đánh giá cả hai phản hồi trên thang điểm mười dựa trên câu hỏi và câu trả lời tham chiếu được cho. Các đánh giá chủ yếu dựa trên tính chính xác, liên quan và tự nhiên của phản hồi để đáp ứng các yêu cầu khi con người tương tác với các tác nhân đa phương thức (xem Phụ lục cho mẫu đánh giá chi tiết). Chúng tôi sử dụng chiến lược được đề xuất bởi Wang et al. [51] để giảm thiểu các thiên vị đánh giá tiềm năng liên quan đến thứ tự phản hồi. Hình 4 cho thấy Ying-VLM của chúng tôi vượt trội so với tất cả các mô hình cơ sở trong hầu hết các mẫu. Đáng chú ý, Ying-VLM đánh bại đường cơ sở mạnh nhất MiniGPT4 trên 167 trong số 300 mẫu được thử nghiệm. Nhất quán với đánh giá ROUGE-L trước đó, kết quả này cho thấy mô hình được tinh chỉnh trên tập dữ liệu hướng dẫn của chúng tôi có thể tạo ra các phản hồi chính xác và hấp dẫn hơn trên các nhiệm vụ KVQA thách thức.

### 4.3 Phân Tích

Chúng tôi điều tra hiệu ứng của số lượng nhiệm vụ và tính đa dạng hướng dẫn trên hiệu suất của các mô hình đã học, cung cấp những hiểu biết cho các nghiên cứu tương lai để sử dụng tiêu chuẩn của chúng tôi tốt hơn.

**Số lượng Nhiệm vụ** Chúng tôi điều tra ảnh hưởng của số lượng nhiệm vụ bằng cách xáo trộn ngẫu nhiên các nhiệm vụ của chúng tôi và sau đó chọn một tập con để huấn luyện mô hình trong giai đoạn tinh chỉnh hướng dẫn. Do hạn chế tài nguyên tính toán, chúng tôi thiết lập tối đa 5k ví dụ cho mỗi nhiệm vụ và huấn luyện tất cả các mô hình trong 5k bước với kích thước batch là 64. Chúng tôi chọn 0, 4, 8, 16 và tất cả 27 nhiệm vụ để huấn luyện, và báo cáo điểm ROUGE-L cá nhân và điểm trung bình. Như được minh họa trong Hình 5, việc tăng số lượng nhiệm vụ cải thiện đáng kể kết quả của hiệu suất khái quát hóa. Bên cạnh đó, lợi ích hiệu suất không giảm khi số lượng nhiệm vụ tăng. Điều này đầy hứa hẹn vì nó cho thấy chúng ta có thể liên tục cải thiện hiệu suất bằng cách đưa thêm nhiệm vụ vào huấn luyện. Sẽ thú vị khi điều tra ảnh hưởng của các cụm nhiệm vụ khác nhau, điều mà chúng tôi để dành cho các nghiên cứu tương lai.

**Tính Đa dạng Hướng dẫn** Để điều tra ảnh hưởng của tính đa dạng hướng dẫn, chúng tôi giới hạn số lượng hướng dẫn được sử dụng trong mỗi tập dữ liệu thành 1, 2, 4 và 8, dẫn đến các mức độ đa dạng khác nhau cho mỗi nhiệm vụ. Các tham số huấn luyện khác nhất quán với những tham số được sử dụng trong các thí nghiệm trước đó về điều tra số lượng nhiệm vụ. Hình 6 cho thấy hiệu suất thay đổi theo mức độ đa dạng. Cụ thể, kết quả của chúng tôi cho thấy sử dụng bốn hướng dẫn mỗi nhiệm vụ là đủ để đạt được hiệu suất khá tốt. Chúng tôi để lại phân tích sâu hơn về tính đa dạng hướng dẫn cho công việc tương lai.

**Kết quả Định tính** Chúng tôi tiến hành một nghiên cứu trường hợp để cung cấp hiểu biết trực tiếp hơn về các mô hình được tinh chỉnh hướng dẫn. Các trường hợp được chọn từ các tập dữ liệu ViQuAE và ChineseFoodNet được giữ lại. Như được hiển thị trong Hình 7, mô hình của chúng tôi tạo ra các phản hồi chính xác cho tất cả các câu hỏi. Ngược lại, MiniGPT4 tạo ra một câu trả lời không chính xác cho câu hỏi sân vận động ở bên trái và không tuân theo hướng dẫn trong các trường hợp tiếp theo, cung cấp mô tả hình ảnh chung thay vì. Ngoài ra, so với InstructBLIP, cung cấp các câu trả lời ngắn gọn nhưng ít hấp dẫn hơn cho hai câu hỏi yêu cầu kiến thức bên ngoài, mô hình của chúng tôi phản hồi tự nhiên và hấp dẫn hơn, nhấn mạnh giá trị của tập dữ liệu của chúng tôi. Mô hình của chúng tôi cũng thành công khái quát hóa cho đầu vào tiếng Trung, phân loại chính xác hình ảnh thực phẩm dựa trên hướng dẫn. Những trường hợp này nhấn mạnh tầm quan trọng của tinh chỉnh hướng dẫn và chứng minh rằng tập dữ liệu của chúng tôi có thể hiệu quả tăng cường khả năng của VLM.

## 5 Kết luận

Trong bài báo này, chúng tôi trình bày M3IT, một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức đa ngôn ngữ để hỗ trợ phát triển các mô hình ngôn ngữ lớn đa phương thức. Tập dữ liệu bao gồm 2,4 triệu thể hiện được tuyển chọn cẩn thận và 400 hướng dẫn nhiệm vụ được viết thủ công qua 40 nhiệm vụ. Chúng tôi xây dựng Ying-VLM để xác nhận hiệu quả của tập dữ liệu của chúng tôi. Kết quả định lượng và định tính chứng minh rằng các mô hình được huấn luyện với tập dữ liệu của chúng tôi thành công tuân theo hướng dẫn của con người, cung cấp các phản hồi hấp dẫn hơn, và đạt được hiệu suất khái quát hóa mạnh mẽ trên các nhiệm vụ video và tiếng Trung chưa thấy. Phân tích sâu hơn cho thấy số lượng nhiệm vụ tăng lên có thể liên tục thúc đẩy hiệu suất, và tính đa dạng hướng dẫn có thể ảnh hưởng đến kết quả. Chúng tôi hy vọng tiêu chuẩn được đề xuất, các mô hình được huấn luyện và các phát hiện thực nghiệm của chúng tôi có thể tạo điều kiện cho các nghiên cứu tương lai hướng tới việc xây dựng các tác nhân thông minh đa phương thức mạnh mẽ.
