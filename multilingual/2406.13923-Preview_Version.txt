# 2406.13923.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2406.13923.pdf
# File size: 2061022 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preview Version
PIN: A Knowledge-Intensive Dataset for
Paired and Interleaved Multimodal Documents
Junjie Wang*,1,3, Yin Zhang1,8, Yatai Ji5, Yuxiang Zhang1,3, Chunyang Jiang8, Yubo Wang4, Kang Zhu6, Zekun
Wang6, Tiezhen Wang7, Wenhao Huang6, Jie Fu*,8, Bei Chen*,6, Qunshu Lin*,2, Minghao Liu*,2, Ge
Zhang*,1,4,6and Wenhu Chen*,1,4
*Corresponding Authors,1Multimodal Art Projection,22077AI,3Waseda University,4University of Waterloo,5Tsinghua University,
601.AI,7Hugging Face,8Independent Researcher
RecentadvancementsinLargeMultimodalModels(LMMs)haveleveragedextensivemultimodaldatasets
to enhance capabilities in complex knowledge-driven tasks. However, persistent challenges in perceptual
and reasoning errors limit their efficacy, particularly in interpreting intricate visual data and deducing
multimodal relationships. Addressing these issues, we introduce a novel dataset format, PIN (Paired
and INterleaved multimodal documents), designed to significantly improve both the depth and breadth
of multimodal training. The PIN format is built on three foundational principles: knowledge intensity,
scalability, and support for diverse training modalities. This innovative format combines markdown
files and comprehensive images to enrich training data with a dense knowledge structure and versatile
training strategies. We present PIN-14M , an open-source dataset comprising 14million samples derived
from a diverse range of Chinese and English sources, tailored to include complex web and scientific
content. This dataset is constructed meticulously to ensure data quality and ethical integrity, aiming
to facilitate advanced training strategies and improve model robustness against common multimodal
training pitfalls. Our initial results, forming the basis of this technical report, suggest significant
potential for the PIN format in refining LMM performance, with plans for future expansions and detailed
evaluations of its impact on model capabilities.
1 Introduction 1
2 Related Work 3
2.1 Formats of Multimodal Data . . . 3
2.2 Pre-training Strategies on LMMs . 4
3 Data Curation 4
3.1 PIN format . . . . . . . . . . . . 4
3.2 Data Process . . . . . . . . . . . . 6
3.3 Quality Control . . . . . . . . . . 10
3.4 Ethical Considerations . . . . . . 114 Analysis of PIN 11
4.1 General statistics . . . . . . . . . 11
4.2 Topic Modeling . . . . . . . . . . 12
5 Training Strategies 13
5.1 Based on Image-text Pairs . . . . 13
5.2 Based on Interleaved Documents 14
5.3 Potential Strategies . . . . . . . . 14
6 Discussion 15
7 Conclusion 16
1. Introduction
Recent advances in Large Multimodal Models (LMMs) have enabled their successful applications in a
variety of knowledge-driven tasks such as chart reasoning and phenomenon understanding through
the learning of large-scale multimodal datasets [ 1,2]. However, recent benchmark studies [ 3,4] have
highlighted two primary types of errors: perceptual errors and reasoning errors. Perceptual errors
include difficulties in interpreting tables and graphs, especially those that are professionally complex.
Moreover, reasoning errors often occur when the model fails to deduce relationships between images
and text, particularly in scenarios involving sequential states. In response to these challenges and
Corresponding author(s): wjj1020181822@toki.waseda.jp, chenbei@01.ai, wenhu.chen@uwaterloo.ca
©2024 PIN team. All rights reservedarXiv:2406.13923v1  [cs.AI]  20 Jun 2024

--- PAGE 2 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
(a) Previous Multimodal Formats(a.1) Image -Text Pairs
Dangerously Set innerHTML
An error report
(a.2) Interleaved Documents
(b.1) Markdown Files
(b) PIN Format (ours)(b.2) Overall Images
Paired
Knowledge intensity Scalability Support for diverse training strategiesInterleaved Interleaved
Figure1|ComparativeanalysisoftraditionalmultimodalformatsversustheproposedPINformat. The
PIN format preserves rich knowledge attributes (e.g., bolding, highlighting, code), supports semantic
interplay between images and text in markdown files, and enhances knowledge representation
through an overall image.
with the goal of training a knowledge-intensive LMM, we propose a solution from a data perspective:
the creation of a knowledge-intensive multimodal dataset .
As illustrated in Figure 1 (a), current mainstream multimodal training datasets primarily fall into two
categories: (a.1) image-text pairs and (a.2) interleaved documents. In image-text pairs [ 5,6], the
text corresponds to the image, allowing models to train perceptual abilities, although with limited
inferential knowledge. Several works have shifted focus towards academic documents, treating the
content of papers as text and pages as images to create text-image datasets [ 7,8]. While these
datasets show strong performance on scientific benchmarks, they face several limitations. First, the
exclusion of images and related content fails to acknowledge the vital interaction between text and
visuals within papers. Furthermore, the segmentation of each page disrupts the natural continuity of
the documents, impeding models from learning the comprehensive knowledge of the entire paper.
Additionally, the absence of open-source datasets presents significant challenges to replication efforts.
To include the rich interleaved information of the images and text, the interleaved document format
has been introduced, enhancing both perceptual and inferential capabilities of models. However, this
format currently faces three challenges: a scarcity of datasets (only MMC4 [ 9] and OBELICS [ 10]
available), a lack of specialized data (only web pages) and a lack of overall information. We consider
what data format could address these issues. The ideal format should exhibit three key characteristics:
(1)knowledgeintensity ,(2)scalability , and(3)supportfordiversetrainingstrategies . Therefore,
as shown in Figure 1 (b), we propose a novel data format: PIN, or Paired and INterleaved multimodal
documents. Specifically, the PIN format consists of two main components: (b.1) markdown files
and (b.2) overall images. The markdown files contain knowledge-intensive interleaved documents,
formatted with simple markup syntax (like bold, italics, and headers) to facilitate understanding of
knowledge, such as the article structure and key points. Moreover, it supports embedding links and
images, which is invaluable for creating multimedia-rich documents, thus preserving the potential for
2

--- PAGE 3 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
supporting additional modalities in the future, such as audio and video. On the other hand, the setup
of the overall image allows models to learn rich information such as the layout, further analyzing
the connections between text and images, such as overall and parts, sequence, etc. Therefore, the
setup of this data format meets the requirements for (1) knowledge intensity. Moreover, we set
PIN format as a unified format, providing processes and methods for other datasets to convert to
the PIN format. By transforming existing mainstream datasets into the PIN format, we also achieve
(2) scalability. Additionally, the PIN format is compatible with popular training strategies, such
as image-text pair training and interleaved multimodal training. Further, we could develop new
pre-training methods based on the PIN format, such as predicting images from markdown text or
extracting textual knowledge from images. Rich training strategies enable models trained on PIN
documents to learn from various perspectives, fulfilling different needs. Therefore, the PIN format
meets the requirements for (3) supporting diverse training strategies.
In this work, we develop several datasets following the PIN format. We introduce an open-source
version, PIN-14M1, which comprises 14million samples extracted from various Chinese and English
data sources. Our data not only includes common web pages but also scientific documents featuring
images that require inference and understanding, such as diagrams and charts. We detail the
creation process of this dataset, encompassing collection methods, filtering processes, and ethical
considerations. Additionally, we embed quality signals within the dataset, allowing researchers to
selectively utilize the data based on specific needs.
Thistechnicalreportisapreliminaryversion. Infuturework,weplantoprovideamorecomprehensive
paper that offers deeper analyses of the PIN format. We will also release larger datasets; PIN-14M is
merely an initial dataset. Furthermore, to explore the effectiveness of our datasets, we will conduct
experiments and provide detailed analyses in the future.
The contributions of this technical report (preview version) are:
•We introduce a new multimodal dataset format, PIN, for Paired and Interleaved multimodal doc-
uments. It supports training of knowledge-intensive large models and is scalable for converting
current datasets to this format.
•The data processing workflows and methods will be open-sourced, along with several dataset
conversion techniques.
•We release an open-source dataset, PIN-14M .
2. Related Work
2.1. Formats of Multimodal Data
Multimodalpre-trainingdatasetsareprimarilyformattedintwoways: image-textpairsandinterleaved
documents. The image-text pair format is currently the most utilized, involving the generation of
large data volumes through extensive web crawling for alt-text descriptions [ 5,6,11]. Although
these datasets cover a broad scope, they exhibit inherent constraints. The dependency on alt-text
frequently results in concise and simplistic texts that provide mere snapshots of the image content,
often lacking depth in contextual richness and grammatical details. In certain cases, the alt-text
comprises just a few rudimentary words, and may not constitute complete sentences. To address this
challenge, NOUGAT [ 7] and KOSMOS-2.5 [ 8] suggest image-text pairs on academic documents such
as academic papers and scientific journals. PDF pages and paper text are treated as image-text pairs,
enriching the semantic information of the text. However, this approach excludes images, overlooking
1PIN-14M :https://huggingface.co/datasets/m-a-p/PIN-14M
3

--- PAGE 4 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
the rich interactive information between text and visuals. Moreover, segmenting the article into
pages prevents the use of information across pages. The interleaved document format has emerged
as a recent studies [ 1,9,10]. For example, OpenFlamingo [ 9] and IDEFICS [ 10] have explored
this technique of interspersing images with text in their pre-training data, thereby enhancing the
multimodal recognition and reasoning capabilities of LMMs. These methodologies entail extracting
coherent images and text from extensive web content and intricately weaving them together. However,
these strategies primarily target web content and overlook more enriched sources of knowledge,
like academic papers. Furthermore, rigorous cleaning processes tend to erase substantial contextual
information, ignoring crucial expression of knowledge, such as markers. These procedures also do
not capture the interlaced information from the full images, such as layout and the specific locations
of information appearance. To counter these shortcomings, we propose the PIN format, designed to
maximize the extraction and presentation of both visual and textual information, thereby facilitating
a more comprehensive learning environment for LMMs.
2.2. Pre-training Strategies on LMMs
The motivation behind multimodal pre-training is to train basic abilities in models by leveraging
the intrinsic properties of the corpus. In contrast to unimodal datasets, multimodal image-text
datasets are designed with enriched intrinsic attributes from the outset. These include alignment
between images and text, interrelations within images, and continuity within the text. Mainstream
pre-training strategies for LMMs are specifically tailored to the current formats of these multimodal
datasets. Strategies such as Contrastive Learning (CL), Image-Text Matching (ITM), Masked Language
Modeling (MLM), and Masked Vision Modeling (MVM) are commonly employed in image-text paired
datasets [ 12–15]. Recently introduced interleaved datasets offer a perspective where multimodal
models learn to predict subsequent words by processing interwoven information from images and
text [1]. The PIN format, which incorporates both paired and interleaved characteristics, seamlessly
supports all the aforementioned training strategies. Moreover, we will discuss potential pre-training
strategies based on the new features of our dataset in section 5.
3. Data Curation
In this section, we describe our format and explain the processing of raw data into this format.
Additionally, we show methods for quality control and discuss ethical considerations.
3.1. PIN format
3.1.1. Philosophy
In a variety of scientific and engineering disciplines, high-quality datasets are essential, facilitating
progress from fundamental research to industrial implementations. This report aims to design
a dataset that is not only responsive to current technological needs but also flexible enough to
accommodate future advancements. Our design principles include:
•Knowledge intensity
•Scalability
•Supports diverse training strategies
Knowledge intensity. Inspired by NOUGAT [ 7], our approach enhances the knowledge richness
of our dataset in three main ways. First, unlike web pages, we extract multimodal information
4

--- PAGE 5 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
from academical documents. In addition, we transition from purely text-based markdown files
to interleaved formats with content images and provide an “overall image”, which recaptures the
multimodal information lost in the NOUGAT dataset construction. Second, recognizing the scarcity of
academic documents, we incorporate other sources such as books and code repositories, preserving
their markup notions. Third, for data originating from web pages, we introduce an “overall image” to
enrich the visual interleaved information. Through these methods, our dataset encapsulates varied
levels of knowledge intensity, ensuring a rich informational depth.
Scalability. To ensure our dataset can be scaled up for production, our designed format must fulfill
two key criteria: compatibility with existing multimodal datasets and support for various data formats.
For pre-existing multimodal datasets such as OBELICS [ 10] and MMC4 [ 9], we generate an “overall
image” for each document through straightforward processing and convert the original text-based list
structures into a unified interleaved markdown format. Similarly, for datasets based on image-text
pairs, we can easily adapt them to our format using designed templates. Moreover, our approach
includeshandlingvariousmultimodalstyles, suchaswebpages, academicpapers, andPDFdocuments.
We are also exploring additional formats, such as the purely textual ones, which are indispensable for
training large language models. For instance, RedPajama-Data-v2 [ 16] has reached an astonishing
30trillion tokens. Therefore, we will detail how we mass-produce data in our format in section 3.2.
Supports diverse training strategies. First, our dataset format is designed to encompass all existing
pre-training methodologies. Therefore, it adopts a paired and interleaved format, incorporating
sections for text (markdown files) and images (overall images). This division into text and image areas
enables seamless application of all current image-text pair-based pre-training objectives. Additionally,
considering the interleaved format within the text sections, our dataset can also accommodate newly
designed pre-training objectives from models like Flamingo [ 1]. Furthermore, we are equipped
to support more diverse training strategies, such as image-based knowledge extraction. Detailed
explanations will be provided in Section 5.
3.1.2. Paired and Interleaved Structure
To align with our philosophy, we design a paired and interleaved structure, as depicted in Figure 1
(b). We now show how this dataset is stored and explain the meanings of various annotation tags.
example_dataset/
|
|-- content_image/
| |-- 1.png
| |-- 2.png
| |-- 3.png
| ...
|-- overall_image/
| |-- 1.png
| |-- 2.png
| |-- 3.png
| ...
\-- example_dataset.jsonl
(a) Structure of the example dataset.example_dataset/
|
|-- part00/ - The first part.
| |-- content_image/
| |-- overall_image/
| \-- part00.jsonl
|
|-- part01/ - The second part.
| |-- content_image/
| |-- overall_image/
| \-- part01.jsonl
|
... - More similar parts.
(b) Segmented structure of the example dataset.
Figure 2|The file tree structure of an example dataset in PIN format.
The architecture of our proposed data structure is depicted in Figure 2. The directory “content images”
holds the images mentioned within the markdown text, and “overall images” display the overall visual
representation of the markdown files. Moreover, the JSONL file encapsulate the textual content along
5

--- PAGE 6 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
1{
2 "id": 1919,
3 "meta": {
4 "language": "en",
5 "oi_exist": true,
6 "oi_source": "compiling",
7 "source_dateset": "example_source (e.g. OBELICS)",
8 "ori_meta": {
9 "document_url": "https://www.example.com/2022/02/21/example/",
10 ...
11 }
12 },
13 "doc_id": 1997,
14 "page_id": 0,
15 "date_download": "2024-03-01"
16 },
17 "license": "CC-BY-4.0",
18 "quality_signals": {
19 "doc_length": 100,
20 ...
21 },
22 "content_image": [
23 "content_image/1997-0.png",
24 "content_image/1997-1.png"
25 ],
26 "md": "<img src= 'content_image/1997-0.png '>\n\nThis is a fake sample data line, just
↩→for show.\n\nThis is a fake sample data line, just for show.\n\n<img src= '
↩→content_image/1997-1.png '>\n\nThis is a fake sample data line, just for show.",
27 "overall_image": "overall_image/1997.png"
28}
Figure 3|An example data sample of JSONL files.
with associated data details. In particular, if the subset is large, we consider a subpart structure,
which is shown in Figure 2b.
As illustrated in Figure 3, we provide a detailed example of the annotations included with each data
entry. Specifically, metadata for each multimodal document is detailed in the metafield. This includes
thelanguage , denoted as Chinese (zh) and English (en). For entries derived from existing datasets,
the original metadata is retained under ori_meta . The source of the overall image is specified in
oi_source , with the origin indicated as either from the original dataset (ori) or compiled using
code (compiling). Furthermore, we introduce quality_signals tag, which stores indicators used
for quality control, such as document length. These signals allow for the dataset to be segmented
according to specific operational needs, enabling rapid filtering to isolate the required data subsets.
3.2. Data Process
Our PIN dataset comprises two main components: datasets we have constructed and existing datasets
that we have transformed. In detail, our constructed datasets currently include PIN-webpage, PIN-
PMC, and PIN-arXiv. Moreover, the transformed existing datasets are: DocLayNet, Linux-CN, chinese-
markdown, OBELICS, MMC4, leetcode, and PG19. However, the released PIN-14M dataset does
not include two of the subsets we collected: PIN-webpage and arXiv. As illustrated in Figure 4, we
present one sample from each subset of the PIN-14M dataset. As shown in Figure 5, we process the
collected data to the designed PIN format. Initial steps involve processing raw text and images from
6

--- PAGE 7 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
PIN-arXiv PIN-PMC
chinese -markdown OBELICS
DocLayNet
Markdown Markdown
Linux -CN
MMC4 PG19
Markdown
 Markdown
Markdown
 Markdown
 Markdown
 Markdown
Figure 4|Samples from various subsets of the PIN-14M dataset. For each subset, one entry is
extracted, showcasing both its markdown file section and the corresponding overall image.
Figure 5|The overview of our process workflow.
the dataset, which includes cleaning the data and downloading images. Subsequently, this content is
compiled into markdown files. The next phase centers on extracting from the multimodal documents
or compiling overall images based on the characteristics of the datasets. Finally, the markdown files
and overall images are combined into our PIN format. In practice, each subset is tailored with specific
processing workflows designed to enhance and leverage unique data characteristics. These workflows
are detailed as follows:
•Multimodal scientific documents: markup text, graphical data and PDF files.
•Annotated multimodal documents: annotated information, images and PDF files.
•Web pages: interleaved text and images.
•Text-only documents: only text.
3.2.1. Multimodal Scientific Documents
PIN-arXiv. ArXiv2is a popular electronic preprint platform encompassing a broad spectrum of
disciplines including physics, mathematics, computer science, quantitative biology, quantitative
finance, statistics, electrical engineering, systems science, and economics. Therefore, we collect the
knowledge-intensive multimodal documents from it. The data processing workflow on this platform
is depicted in Figure 6, and involves the following steps:
2https://arxiv.org/
7

--- PAGE 8 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
Figure 6|Data processing workflow of PIN-arXiv subset.
Figure 7|Data processing workflow of PIN-PMC subset.
1.Data Collection: In the initial stage, we collect source code and PDF documents from the
websites, setting the foundation for further processing.
2.Document Conversion: By utilizing the Engrafo3converter, LaTeX documents are transformed
into beautifully formatted, responsive web pages in HTML format, enhancing accessibility and
visual appeal.
3.Content Parsing: The HTML outputs are then processed through the parser in NOUGAT [ 7],
converting it into text-only markdown format.
4.Multimodal Information Recovery: To address the loss of multimodal data in the text-only
output, a matching algorithm is applied to reintegrate vital visual and textual information from
the HTML back into the markdown files.
5.Overall Image Processing: Each page of the original PDF documents is converted into an image
by utilizing pdf2image library4.
6.Dataset Compilation: The markdown files and images are compiled into a dataset in PIN format.
Specifically, each sample within the dataset includes a markdown file accompanied by several
overall images.
Following the data processing, we create a large-scale multimodal dataset, the PIN-arXiv subset,
consisting of over one million samples. The dataset has not been paginated due to the extensive
manual effort required by designing text-based pagination algorithms. For instance, dual-column
layouts in several PDF files could hinder the alignment between overall images and markdown content.
However, we plan to implement pagination after developing a suitable AI-based model.
PIN-PMC. PubMed Central (PMC)5is a free digital archive that houses open access scholarly articles
3https://github.com/arxiv-vanity/engrafo
4https://github.com/Belval/pdf2image
5https://www.ncbi.nlm.nih.gov/pmc/
8

--- PAGE 9 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
from biomedical and life sciences journals. All articles are stored in a machine-readable XML format,
with corresponding PDF versions available for access. Given the rich content of XML, including styles
and other elements, our focus remains solely on the embedded knowledge. As illustrated in Figure 7,
we design a series of algorithms and processing steps. Initially, we adapt the s2orc-doc2json
library6for the conversion from XML to JSON, ensuring comprehensive extraction of necessary
knowledge. Specifically, we improve the original code by handling of references, images, and tables.
Subsequently, we parse the JSON files into markdown files, integrating disparate knowledge snippets
into a unified document. Finally, we transform the collected PDF files into overall images and integrate
these with the processed markdown text into the PIN format. In the preview release of the open-
source PIN-14M dataset, we provide 99,157public samples. The remaining data will be progressively
released in subsequent versions of the PIN dataset.
3.2.2. Annotated Multimodal Documents
DocLayNet. General PDF files in the DocLayNet dataset [ 17] contain a wealth of digital information,
particularly rich in text and style details. This dataset includes numerous expert annotations, with
the JSON files providing bounding-box positions and the content of these boxes (such as images
and text) on the pages. Inspired by the sequence in which humans read PDF files, we develop a
straightforward JSON-to-markdown parser and converted the corresponding PDF pages into images.
Finally, we integrate these into the DocLayNet subset with 68,757samples.
3.2.3. Web pages
Linux-CN. The Internet hosts numerous technical and academic forums where people share a wide
range of technologies and experiences, embodying vast knowledge. Therefore, the Linux-CN commu-
nity has been included in our dataset. Given the availability of their archives7, it is only necessary
to reorganize all articles and file structures to create markdown files in GitHub Flavored Markdown
(GFM) format. These files are then rendered using a browser with GFM light style, followed by taking
screenshots to produce overall images. Lastly, formatting this content into the PIN format completes
the Linux-CN subset with 9,564documents.
Chinese-markdown. Web pages often contain documents that are inherently structured using
Markdown syntax. These documents are primarily composed of extensive marked text and images,
as seen in blogs. To address this common scenario, we utilize the chinese-markdown dataset8, which
gathers markup-based text content from web pages. Initially, we extract image links for downloading.
Subsequently, we perform basic cleaning of the text. Considering the complexity of paginating
markup-based text, such as encountering code blocks that span two pages, this characteristic might
disrupt content organization. Therefore, we employ a browser to render the content using GFM light
style and then capture screenshots as overall images. Finally, we combine the markdown documents
and overall images to create the PIN format, forming the chinese-markdown subset with 168,323
samples.
OBELICS. The OBELICS dataset [ 10] processes web data into multimodal documents with interleaved
text and images, and shows the effectiveness of its data structure. Considering the dataset’s inherent
interleaved feature, we only need to reorganize the dataset structure to generate markdown files.
Since all texts are plain without complex markup, we employ a heuristic pagination algorithm to
paginate the markdown content ( input), leading to the generation of overall images. Specifically, the
heuristic function, 𝑓page, estimates three key parameters:
6https://github.com/allenai/s2orc-doc2json
7https://huggingface.co/datasets/linux-cn/archive
8https://huggingface.co/datasets/rojas-diego/chinese-markdown
9

--- PAGE 10 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
•Maximum number of lines per page, 𝑛line
•Maximum number of characters per line, 𝑛text
•Number of lines an image occupies, 𝑛image
This function is computed as:
page list =𝑓page(input,𝑛line,𝑛text,𝑛image), (1)
where the page list consists of segmented markdown files. Each file from the page list is then compiled
into a single-page PDF using pandoc9, subsequently transformed into an image via pdf2image
library. Finally, each markdown page and its corresponding overall image are compiled into the PIN
format, thereby creating an entry in our OBELICS subset.
MMC4. The MMC4 dataset [ 9] represents one of the key datasets in interleaved format. Consequently,
we apply a process similar to that used for our OBELICS subset. In the current open-source PIN-14M
release, we only process the mmc4-core-ff segment, resulting in our MMC4 subset, which comprises
approximately 5,351,628records.
PIN-webpage. We crawl web pages from several publicly accessible websites and adhere to a data
acquisition, cleaning, and filtering process akin to that used in OBELICS [ 10]. We then apply a similar
pagination algorithm and codes in the data process of the OBELICS subset. As a result, we create a
subset termed PIN-webpage.
3.2.4. Text-only documents
Leetcode. Textual data, devoid of visual content, can still encapsulate extensive knowledge. Conse-
quently, scenarios involving solely text are incorporated into our dataset. We employ the leetcode
dataset10as an exemplar due to its comprehensive use of enriched textual elements such as code
snippets, emphasis through bolding, and underlining, beyond mere plain text. In detail, the dataset
undergoes a systematic reorganization into markdown documents. Then, we apply the similar pro-
cessing method in Linux-CN subset to create overall images. The final step involves formatting these
materials into the leetcode subset with 2,360samples.
PG19.The PG19 dataset [ 18] consists of books, formatted in plain text without any images, and
is characterized by unusually long texts, with an average length of nearly 400,000characters. To
facilitate model training and enable efficient learning of pagination techniques, we segment the
extensive texts into manageable pages. Similar to the process of the OBELICS subset, We estimate the
number of characters each page can accommodate and divide the entire document into numerous
pages, some extending beyond a hundred pages. Each page, along with its corresponding text, is
then treated as a separate entry. In summary, we collect 2,612,285samples to form the PG19 subset.
3.3. Quality Control
Drawing on the RedPajama-Data-v2 dataset [ 16] design, we introduce quality signals into the PIN
format. Despite the extensive scale of multimodal datasets, often exceeding billions of entries,
users typically possess limited insight into their intrinsic attributes. Extensive pre-cleaning and pre-
processing are generally required before these datasets can be applied in model training. To alleviate
this repetitive task for each user, we implement quality signals to quickly familiarize researchers
with our data. These signals serve as effective tools for identifying and filtering out low-quality
9https://pandoc.org/
10https://huggingface.co/datasets/greengerong/leetcode
10

--- PAGE 11 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
2612285 , 19%
5795198 , 41%5351628 , 38%168323 , 1%
2360 , 0%
9564 , 0%
68757 , 0%99157 , 1%pg19
OBELICS
mmc4-core-ff
chinese-markdown
leetcode
linux-cn
DocLayNet
PIN-PMC2608029 , 17%
5770432 , 39%5277983 , 35%167989 , 1%
2360 , 0%9564 , 0%
69375 , 1%
1074799 , 7%
0, 0%
5840658 , 37%9014579 , 58%106768 , 1%
0, 0% 38960 , 0%
90259 , 1%
454482 , 3%
Documents: 14,107,272 Overall images: 14,980,531 Content images: 15,545,706 
Figure 8|General statisitcs of our PIN-14M dataset.
or irrelevant data. By minimizing noise, they enhance the overall quality of the dataset, which is
crucial for training robust machine learning models. Furthermore, explicit quality indicators enhance
transparency in data processing. They allow users to easily comprehend the dataset’s composition and
limitations, potentially assisting researchers and developers in making more informed decisions. In
practical applications, users can stratify or group data based on these quality signals. They prioritize
high-quality data while adopting distinct strategies for low-quality data, such as additional cleaning
or exclusion. In the preliminary release, PIN-14M , we have incorporated several basic quality signals,
with plans to expand these in future updates.
3.4. Ethical Considerations
Given the diverse sources of our dataset and the complex processes involved, each sample within our
dataset is accompanied by a license field that specifies the licensing terms of the data. If the data
is produced internally or comprises components generated by us, such as compiled overall images, it
is subject to the Apache 2.0 license11.
Furthermore, we remove NSFW images from our self-collected datasets. However, given the vast
volume of our datasets, we encourage the community to conduct more detailed inspections when
using them to ensure compliance with their respective legal requirements.
If users have further questions or suggestions, they can start a discussion on our Hugging Face page12.
This will allow us to delve deeper into the issues and enhance the dataset content accordingly.
4. Analysis of PIN
In this section, we perform a preliminary analysis of our open-source PIN-14M dataset.
4.1. General statistics
As shown in Figure 8, the number of documents, overall images, and content images per subset
is provided, alongside each subset’s contribution to the total. The discrepancy between document
and overall image counts stems from two factors. Firstly, in the PIN-PMC dataset, the absence of
pagination results in multiple overall images corresponding to a single document. Secondly, some
subsets, post-pagination, convert markdown files into overall images; this process can introduce errors,
such as failed image generation, leading to a higher document count compared to overall images,
11https://www.apache.org/licenses/LICENSE-2.0
12https://huggingface.co/datasets/m-a-p/PIN-14M/discussions
11

--- PAGE 12 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
notably in subsets like OBELICS and mmc4-core-ff. Moreover, we observe that on average, OBELICS
features approximately 1.01images per overall image, while mmc4-ff-core averages about 1.71
images per overall image. The denser text in OBELICS likely results in fewer images occupying the
same space compared to mmc4-ff-core, which may have less text and consequently lower knowledge
density. A more comprehensive analysis will be provided in the subsequent version.
4.2. Topic Modeling
AsshowninTable1, weconductexperimentsusingLatentDirichletAllocation(LDA)[ 19]bysampling
100,000 documents to illustrate the thematic distribution within our dataset, alongside estimated
proportions and commonly associated terms. In detail, we list 20topics in LDA results, facilitating
both a high-level and granular analysis of the content. Prominent themes such as “Technology and
Quality”, “Digital Technology”, and “Design and Aesthetics” dominate the data. Moreover, our dataset
encompasses a diverse array of topics, including “Music and Celebrations”, and “Cooking and Recipes”.
Topic Name RatioKeywords
Technology and Quality 12.54new, system, use, used, quality, power, design, high, time,
range
Digital Technology 8.60 new, use, data, time, game, using, like, click, app, need
Urban Life 8.87 new, first, year, two, city, home, time, team, company, years
Design and Aesthetics 8.06 design, made, make, new, black, look, like, white, room,
use
Politics and Society 7.26 said, people, new, government, state, us, time, two, year,
could
Entertainment and Media 6.92 time, like, film, new, first, love, book, life, two, show
General Interaction 5.75 like, time, get, people, really, even, could, new, know, see
Health and Research 5.28 water, may, many, new, research, time, health, well, used,
people
Cooking and Recipes 4.34 make, add, like, time, recipe, made, use, food, minutes,
water
Online Activities 3.15 get, new, free, online, like, use, make, time, game, best
Travel and Hospitality 3.10 park, time, new, like, hotel, get, first, take, great, said
Historical Events 2.27 new, general, people, war, two, time, years, first, state, said
Daily Activities 1.95 time, get, like, food, good, great, make, first, day, go
Personal Care 1.45 skin, like, wine, time, new, love, day, first, make, get
Art and Museums 1.50 art, work, hair, museum, first, like, time, two, new, painting
Cleaning and Services 0.88 cleaning, car, get, services, time, new, us, said, carpet, need
Narratives and Dialogue 0.84 said, man, could, time, upon, little, like, see, two, well
General Opinions 0.55 may, time, like, little, many, great, see, two, first, well
Music and Celebrations 0.55 music, wedding, like, said, get, time, good, make, know,
bass
Unclassified 0.10 said, time, see, like, could, make, get, little, us, well
Table 1|LDA results with 20 topics with their proportion (%), trained on 100,000 random sampled
documents. Each topic is characterized by concepts derived from its associated keywords.
12

--- PAGE 13 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
Page 1
Overall imageABSTRACT: Diclofenac 
sodium is a nonsteroidal 
anti-inflammatory drug 
(NSAIDs) with analgesic 
and antipyretic properties. 
Because of widespread …It inhibits the prostaglandin 
synthesis 8. It is well 
absorbed orally, 99% 
protein bound, metabolized 
and excreted both in urine 
and bile. The plasma t½ …Drug Content of Different 
Brands of Diclofenac 
Sodium 50 mg Tablets: 
Determination of the drug 
content was performed 
according to USP …
Markdown filePage 2
…
Text Content image Text Content image Text
𝐼1o𝐼2o…𝑇11𝑇21…𝐼1c1𝐼2c1…𝑇12𝑇22…𝐼1c2𝐼2c2…𝑇13𝑇23…Tokenization
Figure 9|Samples from our PIN-14M dataset. Moreover, the raw data might be transformed to
tokens after tokenization.
5. Training Strategies
In this section, we discuss potential training strategies, detailing how to adapt current methods and
explore several new possible strategies. Our dataset consists of multimodal documents that can be
represented either as single or multiple entries. For multiple entries, integration is achieved through
doc_id andpage_id fields, ensuring no loss of overall document information. As shown in Figure 9,
we can extract markdown files ( 𝑆𝑜𝑖) and overall images ( 𝑆𝑚𝑑) from these entries. After tokenization,
they can be represented as:
𝑆𝑜𝑖={𝐼𝑜
𝑖}|𝑆𝑜𝑖|
𝑖=1(2)
𝑆𝑚𝑑={𝑇1
𝑖}|𝑇1|
𝑖=1{𝐼𝑐1
𝑖}|𝐼𝑐1|
𝑖=1{𝑇2
𝑖}|𝑇2|
𝑖=1{𝐼𝑐2
𝑖}|𝐼𝑐2|
𝑖=1{𝑇2
𝑖}|𝑇2|
𝑖=1... (3)
To explain training strategies effectively, we employ the widely-used tokenization method. However,
this is not necessarily the best approach for pre-processing. Alternatively, direct processing of image
pixels could be considered, which would eliminate the need for image tokens.
5.1. Based on Image-text Pairs
Contrastive Learning (CL). The core concept is optimizing the model, allowing it to understand and
align different modalities [ 13,20]. Specifically, corresponding image-text pairs are drawn closer in a
shared embedding space, whereas non-corresponding pairs are pushed further apart. For instance,
CLIP employs a contrastive loss function that integrates information from both image-to-text ( 𝑖2𝑡)
and text-to-image ( 𝑡2𝑖) pairs. The specific loss function can be represented as follows:
L𝑖2𝑡=−1
𝑁𝑁∑︁
𝑖=1logexp(𝑥𝑇
𝑖𝑦𝑖/𝜎)
Í𝑁
𝑗=1exp(𝑥𝑇
𝑖𝑦𝑗/𝜎), (4)
L𝑡2𝑖=−1
𝑁𝑁∑︁
𝑖=1logexp(𝑦𝑇
𝑖𝑥𝑖/𝜎)
Í𝑁
𝑗=1exp(𝑦𝑇
𝑖𝑥𝑗/𝜎), (5)
where𝑁is the number of image-text pairs, 𝑥𝑖and𝑥𝑗are the feature vectors of the 𝑖-th and𝑗-th
images, respectively, 𝑦𝑖and𝑦𝑗are the feature vectors of the corresponding texts. Moreover, 𝜎is the
temperature parameter. The expdenotes the exponential function, and logdenotes the logarithm
function. These loss functions aim to maximize the similarity between matching image-text pairs
(𝑥𝑖,𝑦𝑖)while minimizing the similarity between non-matching pairs (𝑥𝑖,𝑦𝑗)and(𝑦𝑖,𝑥𝑗). Furthermore,
the overall loss is:
L𝐶𝐿=L𝑖2𝑡+L𝑡2𝑖 (6)
13

--- PAGE 14 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
In the PIN format, we can replace the 𝑦vector with the overall multimodal vector from 𝑆𝑚𝑑, and
the𝑥is the overall feature of 𝑆𝑜𝑖. This enables the model to learn deeper multimodal connections by
considering the relationships between overall image vectors and mixed image-text vectors. Moreover,
when obtaining the overall vector 𝑆𝑚𝑑is challenging, we can consider two sets of contrastive learning:
(overall image, markup-based text) and (overall image, content images).
Image-Text Matching (ITM). Similar to CL, ITM leverages the inherent alignment of multimodal
data for pre-training [ 21]. The key difference is that ITM employs cross-entropy loss to determine
whether a given image and text pair are aligned.
In the PIN format, we can use a pair of 𝑆𝑚𝑑and𝑆𝑜𝑖. Since images usually occupy a large number of
tokens, we can remove the image component of 𝑆𝑚𝑑to increase the difficulty of ITM task.
Masked Language Modeling (MLM) and Masked Vision Modeling (MVM). Both tasks involve
maskingsometokensandusingtheremaininginformationtoreconstructthemaskedportions[ 15,22].
For MLM, different segments or continuous sections of 𝑆𝑚𝑑can be masked. For MVM, in 𝑆𝑜𝑖, we can
randomly mask various patches, regions, or detected objects. To prevent information leakage, it is
essential to either synchronize or remove the image components from 𝑆𝑚𝑑.
5.2. Based on Interleaved Documents
Flamingo models the likelihood of text conditioned on interleaved sequences of text tokens and visual
inputs (images/videos) [ 1]. It employs a cross-modal generation objective, which is to train the model
to predict the next text token given the preceding tokens and visual context. The training objective
can be expressed as:
Lcross-modal =−𝑇∑︁
𝑡=1log𝑃(𝑤𝑡|𝑤<𝑡,𝑉), (7)
where𝑤𝑡represents the 𝑡-th token in the text sequence, and 𝑤<𝑡represents all preceding tokens in
the text sequence. 𝑉represents the visual inputs (features extracted from images or videos). In the
PIN format, We can just train the models directly utilizing the interleaved part ( 𝑆𝑚𝑑).
5.3. Potential Strategies
Sinceourformatincludesrichinformation,wemightconsiderusingonlyaportionofitforpre-training.
For example, we could pre-train a robust model that understands text-rich images by focusing solely
on the overall image section. Additionally, we could utilize the interleaved markdown file section
(𝑆𝑚𝑑) for the subsequent pre-training tasks such as modal prediction and multimodal next token
prediction.
Modal Prediction. It involves determining whether the next segment in an interleaved sequence
of text and images should be text or image, based on the preceding content. This task leverages
the known context to make accurate predictions. A practical application involves using multimodal
dialogue data, which inherently includes both text and images. The pre-training task focuses on
predicting the content and format of subsequent dialogues.
Multimodal Next Token Prediction (MNTP). The objective is to treat all modal data, including
images and text, as tokens, such as 𝑆𝑚𝑑. This approach allows the next predicted token to be either
text or image, enhancing the diversity of predictions.
Pagination Prediction (PP). We can use the doc_id andpage_id to determine the position of each
page within the overall document. This allows us to assign special tokens to data subsets during
14

--- PAGE 15 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
pagination, thereby combining multiple pieces of data. For instance, a multimodal document ( 𝑆𝑐𝑜𝑛𝑡𝑒𝑛𝑡)
with two pages can be represented as follows:
𝑆𝑐𝑜𝑛𝑡𝑒𝑛𝑡 =[BOD][BOP] 𝑆𝑝𝑎𝑔𝑒1
𝑚𝑑[EOP][BOP] 𝑆𝑝𝑎𝑔𝑒2
𝑚𝑑[EOP][EOD] , (8)
where [BOD]and[EOD]indicate the beginning and end of the document, respectively. Similarly,
[BOP]and[EOP]denote the beginning and end of each page. The PP task requires the model to
predict the positions of these special tokens in conjunction with the overall images.
Multimodal Document Rendering (MDR). This task is similar to the Text-to-Image Generation
(TIG) tasks commonly used in models like stable diffusion [ 23]. In detail, the model predicts 𝑆𝑜𝑖by
learning information from 𝑆𝑚𝑑. However, our situation is more challenging. The model not only needs
to understand the text content but also to arrange the images and text appropriately. Additionally, it
must render specific expressions of knowledge attributes, such as bold text. We can further increase
the difficulty of this task by removing all image tokens from 𝑆𝑚𝑑. This forces the model to generate
suitable content images and place them in the appropriate position within the overall images.
Knowledge Extraction (KE). This task is analogous to Image-to-Text Generation (ITG) [ 15] and
Optical Character Recognition (OCR) tasks. ITG requires models to observe natural images and
generatedescriptivetexts,whileOCRfocusesonextractingtextfromimagesalongwiththeirpositional
information. In our task, the input images are text-rich article images ( 𝑆𝑜𝑖), and the output is the
extraction of knowledge information ( 𝑆𝑚𝑑) from these images. This approach ensures more natural
training with reduced complexity and noise. Additionally, models trained using this method can
seamlessly convert extensive collections of documents into interleaved multimodal formats. This
facilitates the creation of self-iterative processes, allowing the model to generate data and continue
learning autonomously.
6. Discussion
We propose a unified data format that seamlessly integrates various tasks and training processes. Our
data processing workflow accounts for different modalities and can easily incorporate high-quality
unimodal data. The uniformity of our data format facilitates easier analysis of scaling laws for
researchers. During the supervised training phase that follows pre-training, our interleaved text and
image arrangement allows for the straightforward inclusion of instructions and auxiliary information.
This design ensures consistency between upstream and downstream tasks and enables our model to
handle zero-shot tasks immediately after pre-training.
In this technical report, our data pipelines cover a diverse array of scenarios, catering to complex
scientific articles, multimodal PDF files, common web pages, and even text-only contexts. By making
the processing approaches available to the public, we hope to enable the multimodal community to
quickly understand and utilize the dataset while ensuring data transparency.
Furthermore, we will open source the code for compiling the overall image. This allows users to easily
enhance the data to meet their needs. For example, they can adjust font size, format, line spacing,
and styles.
We will now present some potential questions:
Why do we not opt for OCR formats?
Our objective is for the model to focus solely on the knowledge itself, such as the meaning represented
by images and the capacity for deep reasoning based on textual knowledge attributes. We prefer
to avoid burdening the model with the complex positional information required by OCR, such as
15

--- PAGE 16 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
understanding combinations of character boundaries within an image. For instance, in an image
containing the text “APPLE”, the model should only need to comprehend that there is a word “APPLE”
and not expend parameters and reasoning abilities on recognizing “A”, “AP”, “APP”, “APPL”, or “APPLE”,
and dealing with the positions and boundaries these combinations represent. Such a format allows
the model to concentrate solely on knowledge, understanding, and reasoning.
Do PIN format markdown files have a uniform style?
Given the popularity of GitHub Flavored Markdown (GFM), which is widely supported by browsers
and applications, we choose GFM as our primary style. Specifically, we use the GFM light style for
generating overall images. However, for documents containing a significant amount of formulas,
such as those in PIN-arXiv, we opt for the Mathpix markdown format, due to its robust support for
academic notations.
How to handle tables?
In addressing table data, we apply specific treatments based on their complexity and style. For tables
with complex and specialized designs, we opt for HTML representation due to its rich expressive
capabilities and broad application support for rendering. Notably, styles akin to LaTeX tables found in
Mathpix markdown within academic papers are transformed into HTML. Simple table formats, such
as those in GFM, are maintained in their original form. Tables presented as images are preserved
in their original format, enabling the model to discern the relationships between tabular data and
textual content. These varied approaches enhance the diversity of our dataset.
What is the next plan?
The forthcoming phase involves open-sourcing a larger scale dataset within the next few months.
Recognizing the limitations of our current technical report, we are committed to delivering a more
comprehensive elaboration in its subsequent edition.
7. Conclusion
In this paper, we introduce a new multimodal data format, the Paired and INterleaved Multimodal
Document (PIN). We thoroughly outline our design philosophy and step-by-step develop the PIN
format based on these principles. Additionally, we describe our data processing workflow, which
encompasses four different data types in the fields of NLP and multimodal. We have also made
thePIN-14M dataset available, which includes 8subsets and their processing code (coming soon).
Althoughwedonotpresentcorrespondingexperiments, wedetailpotentialtrainingstrategiesenabled
by our new data structure. Finally, we address frequently asked questions and considerations. We
plan to release an updated version of the technical report and a larger dataset soon.
References
[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford,
Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick,
Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski,
RicardoBarreira, OriolVinyals, AndrewZisserman, andKarénSimonyan. Flamingo: avisuallan-
guagemodelforfew-shotlearning. InSanmiKoyejo, S.Mohamed, A.Agarwal, DanielleBelgrave,
K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Confer-
ence on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Novem-
16

--- PAGE 17 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
ber 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/paper/
2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html .
[2]AlexYoung,BeiChen,ChaoLi,ChengenHuang,GeZhang,GuanweiZhang,HengLi,Jiangcheng
Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang,
ShimingYang, TaoYu, WenXie, WenhaoHuang, XiaohuiHu, XiaoyiRen, XinyaoNiu, Pengcheng
Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.
Yi: Open foundation models by 01.ai. CoRR, abs/2403.04652, 2024. doi: 10.48550/ARXIV.
2403.04652. URL https://doi.org/10.48550/arXiv.2403.04652 .
[3]Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,
Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun,
Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and
Wenhu Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning
benchmark for expert AGI. CoRR, abs/2311.16502, 2023. doi: 10.48550/ARXIV.2311.16502.
URLhttps://doi.org/10.48550/arXiv.2311.16502 .
[4]Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang
Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan,
Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao
Huang, Wenhu Chen, and Jie Fu. CMMMU: A chinese massive multi-discipline multimodal
understanding benchmark. CoRR, abs/2401.11944, 2024. doi: 10.48550/ARXIV.2401.11944.
URLhttps://doi.org/10.48550/arXiv.2401.11944 .
[5]GitHub - kakaobrain/coyo-dataset: COYO-700M: Large-scale Image-Text Pair Dataset —
github.com. https://github.com/kakaobrain/coyo-dataset . [Accessed 05-05-2024].
[6]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick
Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk,
and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation
image-text models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,
and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,
November 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/
paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_
and_Benchmarks.html .
[7]Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical
understanding for academic documents. CoRR, abs/2308.13418, 2023.
[8]Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang,
Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, and Furu Wei.
Kosmos-2.5: A multimodal literate model. CoRR, abs/2309.11419, 2023.
[9]Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex
Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal
C4: an open, billion-scale corpus of images interleaved with text. In Alice Oh, Tristan
Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,
Advances in Neural Information Processing Systems 36: Annual Conference on Neural In-
formation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10
- 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
1c6bed78d3813886d3d72595dbecb80b-Abstract-Datasets_and_Benchmarks.
html.
17

--- PAGE 18 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
[10]Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton
Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu
Cord, and Victor Sanh. OBELICS: an open web-scale filtered dataset of interleaved image-text
documents. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,
and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper_files/
paper/2023/hash/e2cfb719f58585f779d0a4f9f07bd618-Abstract-Datasets_
and_Benchmarks.html .
[11]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pages 3558–
3568. Computer Vision Foundation / IEEE, 2021.
[12]Yatai Ji, Junjie Wang, Yuan Gong, Lin Zhang, Yanru Zhu, Hongfa Wang, Jiaxing Zhang, Tetsuya
Sakai, and Yujiu Yang. MAP: multimodal uncertainty-aware vision-language pre-training model.
InCVPR, pages 23262–23271. IEEE, 2023.
[13]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
Learning transferable visual models from natural language supervision. In ICML, volume 139 of
Proceedings of Machine Learning Research , pages 8748–8763. PMLR, 2021.
[14]Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, and Stefano
Soatto. Masked vision and language modeling for multi-modal representation learning. In ICLR.
OpenReview.net, 2023.
[15]Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang.
E2E-VLP: end-to-end vision-language pre-training enhanced by visual learning. In ACL/IJCNLP
(1), pages 503–513. Association for Computational Linguistics, 2021.
[16]Together Computer. Redpajama: an open dataset for training large language models. https:
//github.com/togethercomputer/RedPajama-Data , October 2023.
[17]Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter W. J. Staar. Do-
claynet: Alargehuman-annotateddatasetfordocument-layoutanalysis. CoRR,abs/2206.01062,
2022.
[18]Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.
Compressive transformers for long-range sequence modelling. In ICLR. OpenReview.net, 2020.
[19]David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, 2003.
[20]Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. In ICML, volume 139 of Proceedings of Machine Learning Research ,
pages 4904–4916. PMLR, 2021.
[21]Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. Interbert:
Vision-and-language interaction for multi-modal pretraining. CoRR, abs/2003.13198, 2020.
[22]Hao Tan and Mohit Bansal. LXMERT: learning cross-modality encoder representations from
transformers. In EMNLP/IJCNLP (1) , pages 5099–5110. Association for Computational Linguis-
tics, 2019.
[23]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
18

--- PAGE 19 ---
PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents(Preview Version)
resolution image synthesis with latent diffusion models. In CVPR, pages 10674–10685. IEEE,
2022.
19
