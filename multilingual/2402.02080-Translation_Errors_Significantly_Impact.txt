# 2402.02080.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2402.02080.pdf
# File size: 292266 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Translation Errors Significantly Impact
Low-Resource Languages in Cross-Lingual Learning
Ashish Sunil Agrawal∗1,Barah Fazili∗1,Preethi Jyothi1
1Indian Institute of Technology Bombay, Mumbai, India
{ashishagrawal,barah,pjyothi}@cse.iitb.ac.in
Abstract
Popular benchmarks (e.g., XNLI) used to eval-
uate cross-lingual language understanding con-
sist of parallel versions of English evaluation
sets in multiple target languages created with
the help of professional translators. When cre-
ating such parallel data, it is critical to en-
sure high-quality translations for all target lan-
guages for an accurate characterization of cross-
lingual transfer. In this work, we find that trans-
lation inconsistencies do exist and interestingly
theydisproportionally impact low-resource lan-
guages in XNLI. To identify such inconsisten-
cies, we propose measuring the gap in perfor-
mance between zero-shot evaluations on the
human-translated and machine-translated tar-
get text across multiple target languages; rel-
atively large gaps are indicative of translation
errors. We also corroborate that translation
errors exist for two target languages, namely
Hindi and Urdu, by doing a manual reannota-
tion of human-translated test instances in these
two languages and finding poor agreement with
the original English labels these instances were
supposed to inherit.1
1 Introduction
Multilingual benchmarks, such as XNLI,
XTREME, play a vital role in assessing the
cross-lingual generalization of multilingual
pretrained models (Conneau et al., 2018; Hu
et al., 2020). Typically, these benchmarks involve
translating development and test sets from English
into different target languages using professional
human translators. However, such a translation
process is susceptible to human errors and could
lead to incorrect estimates of cross-lingual transfer
to target languages. We find translation errors
do emerge and they disproportionately affect
*These authors contributed equally to this work.
1Our code is available at https://github.com/translation-
errors
7075808590
03.5710.514
Human Translations from English to target language, translated back to EnglishMachine Translations from English to target language, translated back to English
SpanishFrenchGermanGreekBulgarianRussianVietnameseThaiTurkishArabicChineseHindiSwahiliUrduEnglishFigure 1: XNLI performance gap by evaluating on trans-
lations of human-annotated data in target languages
versus paraphrases of the original English data via back-
translations pivoted on each target language.
translations in certain low-resource languages such
as Hindi and Urdu.2
Consider the well-known Cross-Lingual Natural
Language Inference (XNLI) benchmark (Conneau
et al., 2018) that contains human translations of
English premise-hypothesis pairs (with the labels
reproduced from English) into 14 typologically-
diverse target languages. Prior work raised con-
cerns about whether the semantic relationships be-
tween premise and hypothesis are preserved in such
human translations, but did not probe into this is-
sue further (Artetxe et al., 2020a, 2023). We find
that there are indeed errors introduced in the hu-
man translations leading to label inconsistencies
and that this issue disproportionately affects low-
resource languages.
To visualize the impact of low-quality transla-
tions on low-resource languages, Figure 1 com-
pares zero-shot XNLI performance on all 14 target
languages using the XLMR model (Conneau et al.,
2020) finetuned on English NLI with the following
2In the context of multilingual models, we refer to a lan-
guage as low (or high)-resource based on the proportion of
its data used in model pretraining. XLMR (Conneau et al.,
2020) is pretrained on the CC-100 corpus that includes roughly
50GB each of data from high-resource languages such as
French, Greek and Bulgarian, and only 20.2GB, 5.7GB and
1.6GB of data in low-resource languages such as Hindi, Urdu
and Swahili, respectively.arXiv:2402.02080v1  [cs.CL]  3 Feb 2024

--- PAGE 2 ---
two input types: 1. Human translations of the orig-
inal English NLI instances to the target language
from XNLI, translated back to English. 2. Machine
translations of the original English NLI instances
to the target language, translated back to English.
We see a clear differential trend with larger gaps
between the (scores over the) two input types for
low-resource languages such as Swahili, Urdu and
Turkish (appearing on the right) and smaller gaps
for high-resource languages such as Spanish, Ger-
man and French (appearing on the left). We also
observe that the cross-lingual transfer gap when
comparing the performance of human-translations
for each target language with that of English (the
latter shown as a dotted line) is largely overesti-
mated for low-resource languages.
To summarize, our main contributions are:
1We highlight the problem of translation er-
rors in XNLI disproportionately affecting low-
resource languages, and propose a practical
way of identifying low-quality human trans-
lations by comparing their performance with
machine translations derived from the original
English sentences.
2We find that the translation errors persist under
various train/test settings, including training
data derived from machine-translations and
paraphrases via backtranslations.
3For two low-resource languages Hindi and
Urdu, we manually annotate a subset of NLI
data and find large discrepancies in the newly
annotated labels when compared to the labels
projected from the original English sentences.
2 Experimental Setup
2.1 Tasks and Models
Our main focus is on the popular XNLI (Conneau
et al., 2018) benchmark, which is a three-way clas-
sification task to check whether a premise entails,
contradicts or is neutral to a hypothesis. Parallel
to English NLI (Bowman et al., 2015; Williams
et al., 2018), XNLI consists of development sets
(2490 instances) and test sets (5010 instances) in
14 typologically-diverse languages3Translation-
based gap analysis on two other multilingual tasks
(MLQA and PAWSX) is included in Appendix A.
3Languages include French (fr), Spanish (es), German
(de), Greek (el), Bulgarian (bg), Russian (ru), Turkish (tr),
Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi
(hi), Swahili (sw) and Urdu (ur).We use XLM-Roberta (XLMR) (Conneau et al.,
2020) as the pretrained multilingual model in all
our experiments. (Appendix B reports scores using
mBERT (Devlin et al., 2019) for XNLI that follow
the same trends.)
2.2 Training and Test Variants
(Artetxe et al., 2020a) showed that using machine-
translated data to finetune the pretrained model
helps it generalize better to both machine and
human-translated test data. Motivated by this find-
ing, we construct the following training variants:
1ORIG: Original English training data.
2Backtranslated-train ( B-T RAIN ): English
paraphrases of the original English data via
backtranslations, with Spanish as a pivot.
B-T RAIN is a training variant introduced
in (Artetxe et al., 2020a) that we adopt in our work.
We also evaluate on the following four variants of
test data:
1Zero-shot ( ZS): Human-translated dev/test
sets in the target languages.
2Translate-test ( TT): Machine translations of
target language dev/test sets to English.
3Translate-from-English ( TE): Machine trans-
lations of original English to the target lan-
guages.
4Backtranslation-via-target ( BT): Machine
translations of original English to the target
language and back to English.
We use two translation systems to create the
above variants: 1) A state-of-the-art open-source
multilingual translation model from the No Lan-
guage Left Behind (NLLB) project (NLLB Team
et al., 2022), and 2) Google’s Cloud Translate API.4
Due to the prohibitive cost of the latter for the cre-
ation of training data, we use NLLB to create all our
training variants (unless specified otherwise).5Test
variants were created using both translation sys-
tems. More implementation details and translation-
related details are provided in Appendix D and
4https://cloud.google.com/translate
5We found NLLB to be poor in quality when translating
from English to Chinese. We used the M2M translation sys-
tem (Fan et al., 2020) for English-to-Chinese that was far
superior.

--- PAGE 3 ---
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 89.3 83.5 84.8 83.4 82.4 83.7 80.5 79.4 79.2 79.9 78.3 79.4 77.2 72.7 74.0 79.9
TT-g - 83.7 84.4 83.0 83.4 84.2 80.9 75.8 80.5 80.6 77.9 80.6 79.2 71.9 73.6 79.9
TE-g - 85.3 85.9 85.9 84.8 86.1 84.9 83.8 82.7 84.0 82.0 84.3 82.1 77.3 81.8 83.6
BT-g - 86.6 86.8 86.5 85.9 86.7 85.8 85.4 85.1 85.4 82.7 84.9 85.1 83.6 84.8 85.4
∆-g 2.9 2 3.1 2.5 2.5 4.9 6 4.6 4.8 4.4 4.3 5.9 10.9 10.8 4.9
Table 1: Results of ORIG(model trained on original English data) evaluated on different test set variants described
in Section 2.2. -g refers to using Google-translate as the translator. Highest scores in each column are shown in bold
and next highest is underlined.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 89.2 84.5 85.9 84.6 84.3 85.5 82.9 81.0 81.8 82.6 79.8 80.9 79.6 74.7 75.6 81.7
TT-g - 84.8 86.5 84.1 85.1 85.9 82.7 78.9 83.1 82.7 80.4 82.6 81.4 74.9 76.9 82.1
TE-g - 86.6 87.0 86.9 85.5 86.4 86.4 84.3 84.6 84.9 83.3 84.6 83.5 78.9 82.9 84.7
BT-g - 88.0 87.7 87.6 86.7 87.5 87.1 85.9 86.4 86.2 84.2 85.9 85.9 85.4 86.1 86.5
∆-g 3.2 1.2 2.5 1.6 1.6 4.2 4.9 3.3 3.5 3.8 3.3 4.5 10.5 9.2 4.3
Table 2: Results of B-T RAIN on different test set variants described in Section 2.2. -g refers to using Google-
translate as the translator.
Appendix E. Some of the types of translation errors
in the human-translated dev/test sets in ZSandTT
are illustrated in Appendix 6.
3 Cross-lingual Transfer Gap in XNLI
3.1 Using Original English NLI Train Set
Table 1 presents XNLI accuracy scores for all four
test variants using ORIGtraining data. Test trans-
lations are generated using both NLLB (-n) and
Google Translate (-g) (Numbers for NLLB trans-
lations are present in Appendix C). ∆-g in Table 1
refers to the performance gap when using human
vs. machine translations. It is the difference be-
tween the accuracy for BT-g (machine-translated
target language text) and the best accuracy among
ZSandTT-g (human-translated target language
text). It is striking that ∆-g values for low-resource
languages like Urdu and Swahili are as high as 10.8
and10.9, respectively, and as low as 2.9and2for
high-resource languages like French and Spanish,
respectively.
3.2 Using Translated Train Sets
Table 2 shows test accuracies using an XLMR
model finetuned on B-T RAIN . Across all target
languages and all test set variants, we see consis-
tent improvements in performance compared to
ORIGin Table 1. This is consistent with the ob-
servation in (Artetxe et al., 2020a) that finetuning
on backtranslation-driven paraphrases helps gener-alize better to both human and machine translated
test sets. Interestingly, even with the overall im-
provements using B-T RAIN , the large performance
gap between ZSandTE(and TTandBT) for low-
resource languages like Urdu and Swahili persists.6
Overestimated Cross-lingual Gap. Based on
Hu et al. (2020), we compute cross-lingual trans-
fer gap as the difference between English accuracy
and the average of accuracy scores across all other
languages. From Table 2, the previously reported
cross-lingual gap was 7 using ZS, which reduces to
2.7 using BT-g. The largest gaps for an individual
language were previously 14.5 and 13.6 for Swahili
and Urdu (the delta of their zero-shot scores wrt
English test set scores) and have now reduced to
3.8 and 3.1 with BT-g, respectively. This suggests
a quick recipe for a quality check of human transla-
tions. For target languages supported by machine-
translation systems, the performance gap between
either ZSandTEor between TTandBTcould be
a quick way to check whether the human transla-
tions might have issues during the data collection
phase (thus yielding large gap values).
6We ran a Wilcoxon signed-rank test comparing accuracies
from the ORIG model between the ZS test sets and BT-g
test sets across all 14 languages. Performance on BT-g is
significantly better (at p < 0.001) than on ZS test sets. We
similarly found that the accuracies from the superior B-Train
model is also significantly better (at p < 0.001) on the BT-g
test sets compared to the ZS test sets.

--- PAGE 4 ---
4 Human Evaluation
For two low-resource languages Hindi and Urdu,
we reannotate a subset of the human-translations
with NLI labels and check how well they match
the labels inherited from the original English text.
We pick random, non-overlapping sets of 200 in-
stances each in English, Hindi and Urdu and get
them relabelled by native speakers. (Appendix F
provides more annotation details.) The new labels
matched the original labels 90.5%,66.5%and60%
of the time for English, Hindi and Urdu, respec-
tively. This clearly highlights the large drop in
label agreement for Hindi and Urdu compared to
English, with relative reductions of 24% and 30.5%
for Hindi and Urdu, respectively. In Conneau et al.
(2018), the same experiment was conducted using
English and French and the original labels were
recovered 85% and83% of the time, respectively.
The authors concluded there was no loss of infor-
mation in the translations. However, we find there
to be a significant loss of information in transla-
tions for languages such as Hindi and Urdu.
To verify if machine translations ( TE) (rather
than XNLI’s human translations ( ORIG)) align bet-
ter with the labels from the original English, we re-
label 200 instances translated from English to Hindi
and Urdu (via Google Translate). The annotators
recovered the ground-truth labels 80% and71%
of the time for Hindi and Urdu, respectively, high-
lighting that label inconsistencies in Hindi/Urdu
human translations ( ORIG) are significantly worse
than with machine translations (TE).
5 Attention-based Analysis
We assess how the attention distributions learned
for XNLI over the English test instances correlate
with the attention distributions learned for human-
annotated Hindi/Urdu/Swahili test instances and
Google-translated (English to) Hindi/Urdu/Swahili
test instances. For each correctly predicted En-
glish instance, we consider both human-translated
(HT) and machine-translated (MT) target language
translations and compute word alignments between
English and these translations using awesome-
align (Dou and Neubig, 2021). Aligned words
whose attention score is greater than the mean at-
tention score for the sequence are counted and nor-
malized by the total number of such words in a
sequence. Finally, we compute an average over
all these overlap fractions across instances in the
dataset. These mean overlap scores shown in Ta-text/lang ur hi sw fr
HT 0.375 0.392 0.396 0.594
MT 0.428 0.42 0.422 0.611
Table 3: Aggregate attention scores over aligned words
in Human Translated (HT) and Machine Translated
(MT) XNLI test instances with parallel English data.
ble 3 are computed separately using the human
translations (HT) and machine translations (MT).
For all three languages, we find the overlap fraction
to be higher for the Google-translated sentences
compared to the human-translated sentences. This
suggests that MT aligns better with the original
English text compared to HT.
Since MT is typically more literal than human
translations, thus resulting in more one-to-one
aligned word pairs across the MT translations, it
is not entirely surprising that we would see larger
overlap fractions using MT translations in Table
3. We were also interested in the gap between the
overlap fractions across MT and human translations
across different languages. We observe that the gap
between human and MT translations in terms of
the overlap fractions is smaller for a high-resource
language like French (1.7%), as opposed to Urdu
(5.3%), Hindi (2.8%) or Swahili (2.6%).
6 Impact of Using Translations for
Multilingual Datasets
Table 4 highlights a few examples of premise-
hypothesis pairs in XNLI’s Hindi and Urdu that
are no longer semantically consistent with the orig-
inal labels (copied from English) after translation.
These examples would be flagged as having predic-
tion errors when in fact the predictions are reason-
able given the semantic deviations in the human-
translated Hindi/Urdu sentences from the original
English sentences.
While Table 4 shows examples of errors, trans-
lation issues might not always be errors and could
just be deviations due to unfamiliar phrases or
English-specific nuances that do not get adequately
captured in the translations. For example, we show
a snippet of a premise below:
English premise : “but no . . . is what you see down
here so it’s nice with me working at home because
i can wear pants”
Google translated premise : lekin nahi . . . jo ap
yahan neeche dekh rahe hain isliye mere saath ghar
par kaam karna accha hai kyonki main pants pehen

--- PAGE 5 ---
Premise Hypothesis En-Premise En-Hypothesis Label/Pred Comment
Aise hi choti si
baatein bhane
mera karm par
ek bada antar
bana diyaMei kuch
hasil karne ki
koshish kar
raha tha.Little things
like that made
a big difference
in what I was
trying to do.I was trying
to accomplish
something.E/N Incorrect translation of premise
changes the relationship between the
label and the premise-hypothesis pair.
Mei tumhe ek
ghante mei wa-
pas phone karta
hoo, ve kehte
hai.Usne kaha ki
ve bol rahe the.I’ll call you
back in about
an hour, he
says.He said they
were done
speaking.C/E Hypothesis is incorrectly translated
leading to a change in meaning (i.e
"they were done speaking" is trans-
lated to "they were speaking").
Wo qaed nahin
rehna chahte
theyUnhe kuch
mawaqe par
pakda ja sakta
tha lekin wo
is se bachna
chahte theyThey didn’t
want to stay
captive.They had been
captured at some
point but wanted
to escape.N/C Tense is incorrect in the translation of
the hypothesis. The premise implies
that they have already been captured
while the incorrect translation implies
that they did not want to get caught,
hence predicting a contradiction.
Ye tha, ye ek
khoobsoorat
din thaAj ek aramdah
din thaThat was, that
was a pretty
scary day.It was a relaxing
day.C/N Tense is incorrectly altered to present
and "pretty scary" is translated to sim-
ply "khoobsoorat"(pretty), thus invert-
ing the overall sentiment.
Table 4: Semantically incorrect examples of premise-hypothesis pairs in Hindi (first two) and Urdu (latter two). E,
N and C implies entailment, neutral and contradiction labels.
sakti hun
Human translated premise : lekin nahi . . . jo ki
ap neeche dekhte hi hain, isliye mere saath ghar
par kaam karna accha hai kyonki main pants pehen
sakti hun
The phrase "nice with me working at home" was
incorrectly translated as "mere saath ghar par kaam
karna," which back-translates to “work at home
with me.” This misinterpretation may stem from
the unfamiliar phrase in English.
As NLP systems improve, high-quality manual
annotations are critical. With existing NLP systems
already showing differential trends on high- versus
low-resource languages (Robinson et al., 2023), it
is increasingly important to insulate against trans-
lation inadequacies leading to label errors that pre-
dominantly affect low-resource languages.
7 Related Work
There is growing interest in building multilin-
gual benchmarks for the evaluation of cross-
lingual transfer. E.g., XTREME (Conneau et al.,
2019) covering a wide range of languages and
tasks including XNLI (Conneau et al., 2018),
XQuAD (Artetxe et al., 2020b), PAWS-X (Yang
et al., 2019) and MLQA (Lewis et al., 2019).
Recently, many extensions of XTREME: IndX-
TREME (Doddapaneni et al., 2022) focusing on
18 Indian languages, XTREME-R (Ruder et al.,
2021) and XTREME-UP (Ruder et al., 2023) have
also been released. Translation artifacts haveonly been studied in select prior works. (Moham-
mad et al., 2016) study how translations can al-
ter sentiment labels in Arabic text. In very recent
work, (Artetxe et al., 2023) advocate for the use of
English-only finetuning using machine-translation
systems. However, this relies on high-quality hu-
man translations in the target languages which we
highlight needs to be carefully examined especially
for low-resource languages.
8 Conclusions
This work studies the problem of translation irreg-
ularities in evaluation sets of multilingual bench-
marks like XNLI that are created by translating En-
glish into multiple target languages. We find that
the translation sets of low-resource languages like
Urdu, Swahili exhibit most inconsistencies while
translations of high-resource languages like French,
German are more immune to this problem. We sug-
gest an effective way to check the quality of human
translations by comparing performance with ma-
chine translations, and show how the cross-lingual
transfer estimates can significantly vary with im-
proved translations.
9 Acknowledgements
The last author would like to gratefully acknowl-
edge a faculty grant from Google Research India
supporting her research on multilingual models.
The authors are also thankful to the anonymous
reviewers for very constructive feedback.

--- PAGE 6 ---
10 Limitations
For tasks that have output labels directly corre-
sponding to the input text (e.g., sequence labeling
tasks like POS-tagging, question answering, etc.), it
would be trickier to use our technique since transla-
tions could change the word order and subsequently
affect the output labels as well.
We highlight the problem of the cross-lingual
transfer gap for low-resource languages being mis-
characterized due to poor performance on these
languages stemming from poor-quality translations
and not necessarily because the model has difficulty
with the given target languages. We do not offer a
solution to deal with translation errors. Rather, we
ask for additional checks when collecting transla-
tions for low-resource languages.
We identify that the existing translation datasets
for low-resource languages in XNLI have incon-
sistencies. While we did not create manually-
corrected versions of these translation sets, we will
be releasing the machine-translated text from En-
glish to these target languages upon publication.
Ethics Statement
We would like to emphasize our commitment to
upholding ethical practices throughout this work.
We aimed to ensure that human annotators received
a fair compensation for their annotation efforts and
was commensurate with the time and effort invested
in their work. For translations using Google Trans-
late, we used the paid Cloud API service in accor-
dance with the terms and conditions of usage.
References
Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, An-
gela Fan, and Luke Zettlemoyer. 2023. Revisiting
machine translation for cross-lingual classification.
arXiv preprint arXiv:2305.14240 .
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020a.
Translation artifacts in cross-lingual transfer learning.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7674–7684, Online. Association for Computa-
tional Linguistics.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020b. On the cross-lingual transferability of mono-
lingual representations. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics . Association for Computational Linguis-
tics.Irshad Ahmad Bhat, Vandan Mujadia, Aniruddha Tam-
mewar, Riyaz Ahmad Bhat, and Manish Shrivastava.
2015. Iiit-h system submission for fire2014 shared
task on transliterated search. In Proceedings of the
Forum for Information Retrieval Evaluation , FIRE
’14, pages 48–53, New York, NY , USA. ACM.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
InProceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing , pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451, Online. Association for Computational Lin-
guistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina
Williams, Samuel Bowman, Holger Schwenk, and
Veselin Stoyanov. 2018. XNLI: Evaluating cross-
lingual sentence representations. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing , pages 2475–2485, Brus-
sels, Belgium. Association for Computational Lin-
guistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.
Sumanth Doddapaneni, Rahul Aralikatte, Gowtham
Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop
Kunchukuttan, and Pratyush Kumar. 2022. Indicx-
treme: A multi-task benchmark for evaluating indic
languages.
Zi-Yi Dou and Graham Neubig. 2021. Word alignment
by fine-tuning embeddings on parallel corpora. In
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL) .
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi
Ma, Ahmed El-Kishky, Siddharth Goyal, Man-
deep Baines, Onur Celebi, Guillaume Wenzek,
Vishrav Chaudhary, Naman Goyal, Tom Birch, Vi-
taliy Liptchinsky, Sergey Edunov, Edouard Grave,
Michael Auli, and Armand Joulin. 2020. Beyond
english-centric multilingual machine translation.

--- PAGE 7 ---
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-
vazhagan, and Wei Wang. 2020. Language-agnostic
BERT sentence embedding. CoRR , abs/2007.01852.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. Xtreme: A massively multilingual multi-task
benchmark for evaluating cross-lingual generaliza-
tion. CoRR , abs/2003.11080.
Patrick S. H. Lewis, Barlas Oguz, Ruty Rinott, Sebas-
tian Riedel, and Holger Schwenk. 2019. MLQA:
evaluating cross-lingual extractive question answer-
ing.CoRR , abs/1910.07475.
Saif M. Mohammad, Mohammad Salameh, and Svet-
lana Kiritchenko. 2016. How translation alters senti-
ment. J. Artif. Int. Res. , 55(1):95–130.
NLLB Team, Marta R. Costa-jussà, James Cross, Onur
Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco Guzmán, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2022. No language left behind: Scaling human-
centered machine translation.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang. 2016. Squad: 100, 000+ ques-
tions for machine comprehension of text. CoRR ,
abs/1606.05250.
Nathaniel Robinson, Perez Ogayo, David R. Mortensen,
and Graham Neubig. 2023. ChatGPT MT: Competi-
tive for high- (but not low-) resource languages. In
Proceedings of the Eighth Conference on Machine
Translation , pages 392–418, Singapore. Association
for Computational Linguistics.
Sebastian Ruder, Jonathan H. Clark, Alexander Gutkin,
Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijh-
wani, Parker Riley, Jean-Michel A. Sarr, Xinyi Wang,
John Wieting, Nitish Gupta, Anna Katanova, Christo
Kirov, Dana L. Dickinson, Brian Roark, Bidisha
Samanta, Connie Tao, David I. Adelani, Vera Ax-
elrod, Isaac Caswell, Colin Cherry, Dan Garrette,
Reeve Ingle, Melvin Johnson, Dmitry Panteleev, and
Partha Talukdar. 2023. Xtreme-up: A user-centric
scarce-data benchmark for under-represented lan-
guages.
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-
dhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie
Hu, Dan Garrette, Graham Neubig, and Melvin John-
son. 2021. XTREME-R: Towards more challenging
and nuanced multilingual evaluation. In Proceedings
of the 2021 Conference on Empirical Methods inF1/EM en hi en vi
(# sents) (4918) (4918 ) (5495 ) (5495)
ZS 83.2/69.8 70.6/52.9 83.4/70.6 74.0/52.7
TT-n - 78.4/64.5 - 74.9/61.3
BT-n - 78.4/64.7 - 76.7/63.2
Table 5: Results on TT-n and BT-n MLQA test sets.
BT-n Hi indicates backtranslated data pivoted through
Hindi, TT-n Hi indicates test set in Hi translated to En.
(Note that for MLQA only questions are translated.)
Natural Language Processing , pages 10215–10245,
Online and Punta Cana, Dominican Republic. Asso-
ciation for Computational Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason
Baldridge. 2019. Paws-x: A cross-lingual adversarial
dataset for paraphrase identification.
A Performance Gap Analysis for MLQA,
PA WS-X
Multilingual (Extractive) Question Answering
(Lewis et al. (2019), MLQA) consists of ques-
tions in English translated to six different lan-
guages including Arabic (ar), German (de), Spanish
(es), Hindi (hi), Vietnamese (vi) and Chinese (zh)
amounting to 5K instances in each target language.
PAWS-X: A Cross-lingual Adversarial Dataset for
Paraphrase Identification (Yang et al., 2019) con-
sists of dev/test paraphrases in English translated
to six different languages: French(fr), Spanish(es),
German (de), Chinese (zh), Japanese (ja), and Ko-
rean (ko) with the help of human translators.
MLQA. For MLQA, we translate questions in
the two low-resource languages, Hindi and Viet-
namese, to English using NLLB ( TT). We also
create a BTversion of the original English ques-
tions (2.2) using Hindi and Vietnamese as pivots.
Table 5 shows TT and BT scores for Hindi are
nearly identical and there is a small improvement
using BT for Vietnamese compared to TT. This
indicates that the professional annotators did not
introduce semantic inconsistencies during transla-
tion for MLQA. In general, classification tasks like

--- PAGE 8 ---
Instructions
Given premise and hypothesis, label each pair as "entail-
ment", "contradiction" or "neutral" as follows:
1. if hypothesis is entailed by the premise, it’s an "entail-
ment" ,
2. if the hypothesis contradicts the premise (hypothesis
cannot be True given the premise), it’s a "contradiction",
3. if the hypothesis is independent of the premise (hy-
pothesis may or may not be True given the premise), it’s
a "neutral" relationship.
Table 6: Task description shared with the annotators
for the NLI task
XNLI appear to be more susceptible to translation
inconsistencies since the annotators are not made
aware of the ground-truth labels during translation
and are only asked to independently translate the
premise/hypothesis pairs.
PA WS-X. Table 7 shows the results of the dif-
ferent settings ZS, TE, TT, and BT for the six
languages. The model used for inference is xlm-
roberta-large trained on the English train set. TE
is better than ZS mainly for Korean (by 4.9% in
test set) and Chinese (4.9% in dev set) and is nearly
equal for other languages. BT is better than TT
again for Korean and Chinese and nearly equal for
other languages. This indicates the presence of
human translation inconsistency for the two lan-
guages.
B Comparing the Performance of mBert
and XLMR
As can be seen in Table 8, XLMR outperforms
mBert by a huge margin on every language. Thus,
we used XLMR for evaluating all our experiments.
C Performance of models using NLLB as
the translator
Tables 9, 10 show the results of the models trained
using ORIGandB-T RAIN training data. Transla-
tion has been done using the NLLB translator. ∆-n
denotes the difference between max( BT-n,TE-n)
and max( ZS,TT). The results are similar to what
we observe in Tables 1, 2. ∆-n is particularly high
for low-resource languages like Hindi, Swahili, and
Urdu. Also, the delta decreases for the B-T RAIN
model.
D Details of Model Training
The models mBert and XLMR were trained using
the same setting as mentioned in the XTREMErepository.7
XNLI. mBert is trained for 2 epochs with a learn-
ing rate of 2e-5, with a batch size of 8 and gradient
accumulation of 4 (i.e an effective batch size of 32).
XLMR is trained for 2 epochs with a learning rate
of 5e-6, batch size of 5 and gradient accumulation
steps of 6 (i.e effective batch size of 30). The final
model is selected from the best checkpoint, which
is based on the model’s performance on the English
dev set. For training the different variants of the
model ( ORIG,T-T RAIN ,B-T RAIN , BT-enes, MT-
hi-g, MT-hi-n) we use the same hyperparameter
setting as mentioned above.
We use xlm-roberta-large for all our experiments.
Model training was done on a single Nvidia
Geforce GTX 1080 Ti GPU, which has a RAM
of 12GB. It took us around one day to train a sin-
gle model for 2 epochs. For data translation using
NLLB(3.3B parameter model), we made use of the
NVIDIA A100-SXM4-80GB gpu for faster pro-
cessing. Translating the test sets took couple of
hours(1-1.5).
MLQA. To evaluate the performance on MLQA
dataset, we trained XLMR on the SQUAD
dataset (Rajpurkar et al., 2016). The model is
trained for 3 epochs with a learning rate of 3e-5,
batch size of 1 and gradient accumulation of 32 (i.e
an effective batch size of 32).
PA WS-X. We trained xlm-roberta-large model
on the English train set. The model is trained for 5
epochs with a learning rate of 2e-5, batch size of
2 and gradient accumulation of 16 (i.e an effective
batch size of 32).
E Details of Train and Test Translations
To train the model on back-translated (using Span-
ish as the pivot) and machine-translated(translated
to Hindi and Spanish) data, we made use of the
open-source 3.3B parameter NLLB model hosted
on Hugging-Face8. We found that the English to
Chinese translation using NLLB is of lower qual-
ity, so we tried the open source 1.2B parameter
M2M (Fan et al., 2020) model9and it performed
better compared to the NLLB translator.
7https://github.com/google-research/xtreme
8https://huggingface.co/facebook/nllb-200-3.3B
9https://huggingface.co/facebook/m2m100_1.2B

--- PAGE 9 ---
dev/test en de es fr ja ko zh avg
sents (2000/2000) (2000/2000) (2000/2000) (2000/2000) (2000/2000) (2000/2000) (2000/2000) -
ZS 95/95.9 89/90.9 90.4/90.4 91.4/91.6 82.9/80.5 83.6/80.8 83.9/84.2 86.9/86.4
TT-n - 88.9/89.9 89.8/91 90.4/91.6 83/79 82.2/80.4 81.6/80.9 86.0/85.5
TE-n - 91.2/92.3 92.1/92.3 90.9/91.2 83.7/83.4 86.8/85.7 88.8/88.6 88.9/88.9
BT-n - 90.6/91.5 91.6/92.2 90.8/90.8 81.9/80.6 84/84.4 89/88.2 88.0/88.0
Table 7: Results on ZS, TE, TT, and BT PAWS-X.
dev en fr es de el bg ru tr ar vi th zh hi sw ur avg
XLMR 89.9 84.2 85.0 84.3 81.8 83.2 79.7 79.9 79.2 81.6 78.0 80.0 78.3 72.1 74.6 80.8
mBert 83.0 74.9 74.8 72.2 67.8 68.2 68.4 63.4 65.4 69.8 54.8 70.6 61.5 52.4 53.3 66.7
Table 8: Zero shot performance of O RIGmBert and XLMR models on the XNLI target dev sets.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 89.3 83.5 84.8 83.4 82.4 83.7 80.5 79.4 79.2 79.9 78.3 79.4 77.2 72.7 74.0 79.9
TT-n - 82.1 83.1 80.7 82.3 82.6 79.3 75.9 78.0 78.7 73.8 77.6 77.7 70.5 71.3 78.1
BT-n - 84.5 84.9 83.5 82.9 82.7 82.3 81.1 81.4 82.4 76.4 79.6 82.9 79.4 80.8 81.8
TE-n - 84.4 85.5 83.9 83.6 83.9 83.4 81.7 81.5 81.9 78.7 81.0 82.1 77.0 80.3 82.1
∆-n 1 0.7 0.5 1.2 0.2 2.9 2.3 2.3 2.5 0.4 1.6 5.2 6.7 6.8 2.2
Table 9: Results of ORIG(model trained on original English data) evaluated on different test set variants described
in Section 2.2. -n refers to using NLLB as the translator. Highest scores in each column are shown in bold and next
highest is underlined.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 89.2 84.5 85.9 84.6 84.3 85.5 82.9 81.0 81.8 82.6 79.8 80.9 79.6 74.7 75.6 81.7
TT-n - 84.0 85.7 82.4 84.4 84.4 81.8 78.9 81.0 80.9 77.4 80.5 80.5 73.6 74.4 80.7
BT-n - 85.9 86.8 85.1 84.8 84.6 84.3 82.8 83.5 84.2 79.3 81.4 84.8 81.9 82.5 83.7
TE-n - 85.8 86.8 85.2 84.9 85.2 84.6 83.0 83.5 83.6 80.6 82.0 83.4 79.1 81.4 83.5
∆-n 1.4 0.9 0.6 0.5 -0.3 1.7 2 1.7 1.6 1.6 1.1 4.3 7.2 6.9 2
Table 10: Results of B-T RAIN on different test set variants described in Section 2.2. -n refers to using NLLB as the
translator.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 88.9 84.8 85.7 84.8 84.4 85.0 82.9 80.9 81.2 81.9 78.9 80.7 79.6 74.9 75.9 81.7
TT-n - 83.2 84.5 82.4 83.9 84.1 81.3 78.4 80.6 80.7 76.6 79.7 80.1 73.1 74.2 80.2
TT-g - 84.3 85.9 84.2 84.8 85.2 82.8 77.8 82.5 81.9 79.9 82.2 81.1 74.3 76.0 81.6
BT-n - 85.2 86.2 84.6 84.8 84.2 83.9 82.3 83.3 83.9 79.2 81.6 84.4 81.4 81.9 83.4
TE-n - 85.3 86.3 85.1 84.4 84.9 84.7 82.5 83.1 83.9 79.9 81.8 83.0 79.0 81.4 83.2
TE-g - 86.2 86.6 86.5 85.1 86.8 86.0 83.9 84.1 85.0 82.7 84.5 83.4 79.4 82.8 84.5
BT-g - 87.0 87.3 87.3 86.7 87.0 86.7 85.7 86.0 86.1 83.8 85.5 85.8 84.6 85.5 86.1
∆-g 2.2 1.4 2.5 1.9 1.8 3.8 4.8 3.5 4.2 4.1 3.3 4.7 9.7 9.5 4.1
Table 11: Results of T-T RAIN on different test set variants described in Section 2.2.
F Details of Human Annotations
Each task (set of random 200 sentences) is anno-
tated independently by two annotators. The task
description shared with the annotators is includedin Table 6. The sentences in agreement between the
two annotators are reviewed and approved for the
dataset by the final annotator. If there is a mismatch,
it is sent to the two annotators for review and pos-
sible corrections. If the mismatch persists, a third

--- PAGE 10 ---
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 89.8 85.1 86.2 84.6 84.1 85.2 82.4 81.3 81.2 81.9 79.3 80.9 78.6 74.9 76.0 82.1
TT-n - 84.2 85.2 82.6 84.8 84.8 81.9 78.8 81.7 81.1 78.2 80.3 80.7 73.8 75.1 80.9
BT-n - 85.9 86.6 85.0 85.0 85.2 84.2 83.2 83.6 84.8 79.4 81.9 85.2 82.1 82.8 83.9
TE-n - 85.9 87.0 85.2 84.5 85.3 84.6 83.1 83.6 84.2 80.1 82.7 82.9 78.7 80.8 83.5
Table 12: Results of BT-enes (model trained on back-translated(en →es→en) + original English train set) on
different test set data settings 2.2, -nrefers to using NLLB translator.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 87.4 82.9 84.2 82.7 83.4 83.4 81.1 80.8 79.9 80.4 78.1 79.9 78.8 74.1 75.3 80.8
TT-n - 81.7 82.6 80.1 82.2 82.3 80.3 76.2 79.4 79.3 75.8 77.9 78.5 72.2 72.5 78.6
BT-n - 83.9 84.4 83.4 82.7 81.8 82.3 80.1 81.5 82.2 77.5 80.0 83.3 79.9 81.0 81.7
TE-n - 83.7 84.9 83.6 83.0 83.5 82.8 81.5 82.0 82.3 79.4 81.1 82.7 78.2 81.4 82.1
Table 13: Results of MT-hi-g (model trained on data translated to Hindi (en →hi) using google-translate) on different
test set data settings 2.2.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ZS 87.2 83.4 83.6 82.9 82.7 83.4 81.8 79.9 79.9 80.1 78.7 80.6 78.4 73.6 74.9 80.7
TT-n - 82.2 83.6 80.6 82.6 82.6 80.38 76.4 79.6 79.5 76.9 78.8 79.4 72.73 73.2 79.2
BT-n - 83.7 84.7 83.4 83.0 82.7 82.3 80.6 81.9 82.9 78.2 80.7 83.4 80.2 81.6 82.1
TE-n - 83.8 84.8 83.5 82.9 83.7 82.6 81.2 82.1 81.9 79.2 81.3 82.6 78.1 80.9 82.0
Table 14: Results of MT-hi-n (model trained on data translated to Hindi (en →hi) using NLLB-translate) using
different data settings 2.2.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ORIG 89.3 83.5 84.8 83.4 82.4 83.7 80.5 79.4 79.2 79.9 78.3 79.4 77.2 72.7 74.0 80.5
B-train 89.2 84.5 85.9 84.6 84.3 85.6 82.9 81.0 81.8 82.6 79.8 80.9 79.6 74.7 75.6 82.2
BT-enes 89.8 85.1 86.2 84.6 84.1 85.2 82.4 81.3 81.2 81.9 79.3 80.9 78.6 74.9 76.1 82.1
T-T RAIN 88.9 84.8 85.7 84.8 84.4 85.0 82.2 80.9 81.2 81.9 78.9 80.7 79.6 74.9 75.9 81.9
Table 15: Comparing zero-shot test set results of different trained models (translations performed using NLLB).
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
ORIG - 82.1 83.1 80.7 82.3 82.6 79.3 75.9 78.0 78.7 73.8 77.6 77.7 70.5 71.3 78.1
B-T RAIN - 84.0 85.7 82.4 84.4 84.4 81.8 78.9 81.0 80.9 77.4 80.5 80.5 73.6 74.4 80.7
BT-enes - 84.2 85.2 82.6 84.8 84.8 81.9 78.8 81.7 81.1 78.2 80.3 80.7 73.8 75.1 80.9
T-T RAIN - 83.2 84.5 82.4 83.9 84.1 81.3 78.4 80.6 80.7 76.6 79.7 80.1 73.1 74.2 80.2
Table 16: Comparing translate-test (using NLLB translator) test set results of different trained models.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
MT-hi-g 87.4 82.9 84.2 82.7 83.4 83.4 81.1 80.8 79.9 80.4 78.1 79.9 78.8 74.1 75.3 80.8
MT-hi-n 87.2 83.4 83.6 82.9 82.7 83.4 81.8 79.9 79.9 80.1 78.7 81.2 78.4 73.6 74.9 80.7
Table 17: Comparing zero-shot test set results of models trained on machine-translated Hindi (1/3rd of training
data), hi-g implies using google translator and hi-n implies using NLLB translator.
test en fr es de el bg ru tr ar vi th zh hi sw ur avg
MT-hi-g - 81.7 82.6 80.1 82.2 82.3 80.3 76.2 79.4 79.3 77.9 76.5 78.5 72.2 72.5 78.7
MT-hi-n - 82.2 83.6 80.6 82.6 82.6 80.4 76.4 79.6 79.5 76.9 78.8 79.4 72.7 73.2 79.2
Table 18: Comparing translate-test (using NLLB translator) test set results of models trained on machine-translated
Hindi(1/3rd of training data), hig implies using google translator and hin implies using NLLB translator.

--- PAGE 11 ---
annotator performs a fresh annotation. The final
annotator reviews the 3 answers and submits the
final answer for the dataset. We also computed the
Cohen’s Kappa score between the two annotators
and found them to be: 0.64 for English sentences,
0.43 for Hindi sentences, and 0.37 for Urdu sen-
tences. Although the agreement scores are lower
for Hindi and Urdu, for the machine-translated text
they are still higher than human annotated text, es-
pecially for Urdu (0.41 for MT sentences vs. 0.37
for human translations). For the instances with
conflicting labels from the two annotators, most
of these instances were marked as neutral by one
annotator and as entailment or contradiction by the
other. A noticeable pattern for "neutral” versus “en-
tailment" emerged: the hypothesis often included
extra details or claims not explicitly stated in the
premise. This tends to be labeled as neutral by the
more meticulous annotator and as entailment when
adopting a more flexible approach.
G Tools and Libraries
We made use of awesome-align (Dou and Neubig,
2021) to align words between English and any tar-
get language. The model used by awesome-align
was bert-base-multilingual-cased. We used the Py-
torch framework10and Hugging-face library11for
all our model training and inferencing tasks. To
integrate Labse (Feng et al., 2020), we made use
of the Sentence-transformers library12. To convert
the transliterated sentences to the original scripts,
we made use of both google-translate and Indic-
trans (Bhat et al., 2015) (for Indian languages). We
made use of the google-cloud-translate api to use
the google-translate services.
H More Trained Models
We trained a few more models in different settings
to check their impact on the cross-lingual perfor-
mance despite presence of semantic irregularities.
The additional models we trained include:
1.T-T RAIN is the model trained on English train
set machine translated to Spanish. (See Ta-
ble 11.)
2.BT-enes, i.e train the model on backtranslated
english (using Spanish as a pivot) + the origi-
nal English.
10https://pytorch.org/
11https://huggingface.co/
12https://www.sbert.net/3.MT-hi-g, i.e train the model on machine-
translated train set where the train set is trans-
lated to Hindi using google-translate. Here
we used only 1/3rd of training data to train the
model(to incur low costs of translation).
4.MT-hi-n, this is the same as above, except
that the translation is performed using NLLB
translator.
Using T-T RAIN is more effective in improving test
performance across all target languages compared
to using O RIG
Tables 12, 13, 14 shows the results of the trained
models across different test settings (test sets
translated using NLLB). The figures highlight the
potential semantic gap that exists between BT
andTT(also ZSandTE) across all the models
which increases more towards the low resource
languages.
In Table 15 and 16, we compare the zero shot
and translate-test results of all the trained models
across different languages. B-T RAIN and BT-enes
performs the best across majority of the languages.
Table 17, 18 compares the zero-shot and translate-
test results of the MT-hi models, it can be seen
that both the models perform equally across the
languages, also because of training on less amount
of data, their zero-shot performance is very slightly
inferior to the O RIGmodel.
