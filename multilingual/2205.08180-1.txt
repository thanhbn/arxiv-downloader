# 2205.08180.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2205.08180.pdf
# File size: 1915343 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
SAMU-XLSR: Semantically-Aligned Multimodal
Utterance-level Cross-Lingual Speech
Representation
Sameer Khurana1, Antoine Laurent2, James Glass1
1MIT Computer Science and ArtiÔ¨Åcial Intelligence Laboratory, Cambridge, MA, USA
2LIUM - Le Mans University, France
fskhurana, glassg@mit.edu
Abstract ‚ÄîWe propose the SAMU -XLSR : S emantically-
Aligned M ultimodal U tterance-level Cross -Lingual S peech
Representation learning framework. Unlike previous works
on speech representation learning, which learns multilingual
contextual speech embedding at the resolution of an acoustic
frame (10-20ms), this work focuses on learning multimodal
(speech-text) multilingual speech embedding at the resolution
of a sentence (5-10s) such that the embedding vector space is
semantically aligned across different languages. We combine
state-of-the-art multilingual acoustic frame-level speech
representation learning model XLS-Rwith the Language
Agnostic BERT Sentence Embedding ( LaBSE ) model to create
an utterance-level multimodal multilingual speech encoder
SAMU -XLSR . Although we train SAMU -XLSR with only multilingual
transcribed speech data, cross-lingual speech-text and speech-
speech associations emerge in its learned representation space.
To substantiate our claims, we use SAMU -XLSR speech encoder in
combination with a pre-trained LaBSE text sentence encoder for
cross-lingual speech-to-text translation retrieval, and SAMU -XLSR
alone for cross-lingual speech-to-speech translation retrieval. We
highlight these applications by performing several cross-lingual
text and speech translation retrieval tasks across several datasets.
Index Terms ‚ÄîCross-lingual speech representation learning,
Language-agnostic speech embedding, Zero-shot speech-to-text
translation retrieval, Zero-shot speech-to-speech translation re-
trieval
I. I NTRODUCTION
Recently, self-supervised pre-training of large transformer
encoders on massive amounts of unlabeled audio data fol-
lowed by task-speciÔ¨Åc Ô¨Åne-tuning has emerged as the de-facto
approach for achieving state-of-the-art performance on several
tasks in spoken language processing. However, popular self-
supervised representation learning (SSL) approaches such as
Wav2vec-2.0 [1] and others [2]‚Äì[12] learn speech embedding
at acoustic frame-level, i.e., for short speech segments of
duration 10 to 20 milliseconds.
Unlike previous works mentioned above, this work fo-
cuses on learning semantically-aligned multimodal utterance-
level cross-lingual speech representations ( SAMU -XLSR ). The
SAMU -XLSR ‚Äôs embedding vector space is multimodal since it
is shared between the speech and the text modalities. It is
cross-lingual since various languages share it. Furthermore, it‚Äôs
Preprint. Under Review.Fig. 1: An illustration of the cross-lingual multimodal embed-
ding space.
The bird is bathing in the sink (en)?????? ????? ?? ????? (ar)L'oiseau se baigne dans l'√©vie (fr)????????????(ja)??????????????? ??????Monsieur le Pr√©sidentMr President(ar)(en)(fr)
semantically aligned since, in the SAMU -XLSR ‚Äôs vector space, a
spoken utterance is clustered together with its speech and text
translations. We show a two-dimensional illustration of the
desired embedding vector space in Figure 1. As an example,
consider the English phrase A bird is bathing in the sink .
Now, in SAMU -XLSR ‚Äôs embedding space, the written form of
the above phrase should be clustered together with its written
and spoken forms in various languages (Japanese, French,
and Arabic in the Ô¨Ågure). And, in some other regions of the
embedding space, the phrase Mr President is clustered with its
written and spoken form in several languages. Unfortunately,
the acoustic frame-level unimodal contextual representation
learning frameworks like Wav2vec-2.0 [1] or the multilingual
XLS-R[7], [9] do not learn an embedding space with the
same properties. We believe that encoding semantics is one
of the many missing pieces in the self-supervised speech
representation learning puzzle.
On the other hand, several transformer encoders for
text have been proposed in recent years that go beyond
token-level contextual representations and learn cross-lingual
semantically-aligned sentence embedding vector spaces across
several languages [13]‚Äì[15]. These models have found use in
bi-text data mining. The task is to retrieve the text translation
in a target language for a given sentence query in a source
language by matching the query sentence embedding with
those of sentences in the target language search database
[16]‚Äì[18]. Given that text encoders can successfully learnarXiv:2205.08180v1  [cs.CL]  17 May 2022

--- PAGE 2 ---
2
Fig. 2: A pedagogical description of how learning with transcribed speech data using LaBSE as the teacher could lead to the
emergence of cross-lingual speech and text associations. In this illustration, we use English speech x(EN)and its transcription
y(EN)for training. SAMU -XLSR ‚Äôs parameters are tuned to close the distance between the speech embedding given by SAMU -XLSR
in orange and LaBSE ‚Äôs embedding (Anchor) of the corresponding text transcript in green. Since LaBSE ‚Äôs text embedding
space is semantically-aligned across various languages, by pulling the speech embedding towards the anchor embedding,
we automatically learn cross-lingual speech-text alignments without ever seeing cross-lingual associations during training. In
practice, we train SAMU -XLSR with multilingual transcribed speech, not just English.
LaBSE 
Embedding 
SpaceAll's 
well 
that 
ends 
well
Bien 
est√° 
lo 
que 
bien 
acaba
Tout 
est 
bien 
qui 
finit 
bien
Anchor
Pull
semantically aligned cross-lingual sentence embedding spaces,
we ask whether it is possible to make these text embedding
spaces multimodal by learning to map speech utterances in the
semantically-aligned cross-lingual text embedding space.
To that end, we propose a multimodal learning framework
for Ô¨Åne-tuning the pre-trained multilingual XLS-Rspeech en-
coder via knowledge distillation from the pre-trained language-
agnostic BERT sentence encoder LaBSE [15]. Also, we append
a pooling mechanism and a non-linear projection layer after
the last layer of the pre-trained XLS-Rencoder to transform
the frame-level contextual representations into a single ut-
terance level embedding vector. Then, we train the speech
encoder using transcribed speech; given a speech utterance,
the parameters of the speech encoder are tuned to accurately
predict the text embedding provided by the LaBSE encoder
of its corresponding transcript. Because LaBSE ‚Äôs embedding
vector space is semantically-aligned across various languages,
the text transcript would be clustered together with its text
translations. Hence, we get cross-lingual speech-to-text asso-
ciations for free by simply using transcribed speech to train
the speech encoder via the proposed knowledge distillation
framework. For a pedagogical description, see Figure 2.
One of the use cases of the SAMU -XLSR embedding space
described above is for data mining. Recent years have seen
remarkable progress in Automatic Speech Recognition across
several domains and languages. The next frontier in spoken
language processing is automatic speech to text and speech
to speech machine translation. Developing speech-based MT
systems would require massive amounts of parallel translated
speech data in several languages, which could be highly costly
to collect. But, the multimodal cross-lingual embedding space
illustrated in Fig. 1 could address this issue. We could build
a cross-lingual speech to text and speech to speech retrieval
pipeline, which could entirely or, in some cases, partially auto-
mate the process of collecting either text or speech translations
corresponding to a spoken utterance. We advise the reader
to look at papers in Natural Language Processing that use
multilingual sentence encoders to perform cross-lingual text
mining, such as [15], [19]‚Äì[21].Cross-lingual speech-to-text mining to create parallel
speech-text translation datasets is just one possible applica-
tion of SAMU -XLSR . But, what motivates us to work on this
problem is the potential application in zero-shot speech-to-text
translation. The success of zero-shot translation depends on
learning a semantically-aligned language invariant embedding
vector space or an interlingua for different spoken languages,
where speech utterances and their speech translations are
clustered together. We show that this is an emergent property
inSAMU -XLSR ‚Äôs embedding vector space as a result of training
SAMU -XLSR using the proposed multimodal learning frame-
work (Section IV-E). Some of the text machine translation
papers that inspire us in the Ô¨Åeld of zero-shot translation are
[22], [23].
Through this work, we make the following contributions :
We propose a simple yet effective multimodal learn-
ing framework for semantically-aligned multimodal
(joint speech-text) utterance-level speech representation
(SAMU -XLSR ) shared across multiple languages (Sec-
tion II).
First, we demonstrate the effectiveness of our models on
several zero-shot cross-lingual speech-to-text and speech-
to-speech translation retrieval tasks (Section IV).
Second, we show that SAMU -XLSR could be used for
sequence-to-sequence modeling tasks such as phoneme
recognition and Automatic Speech Recognition (ASR)
(Section V).
Finally, we conduct analysis to understand better the
various design decisions that went into constructing
SAMU -XLSR (Section VI).
A work that is similar to ours is presented in [24]. Unlike the
previous work, we evaluate our model on multiple datasets
across many languages with a special emphasis on low-
resource languages.
Furthermore, unlike the multimodal speech encoder pre-
sented in [24], we show that SAMU -XLSR performs at par or
better than XLS-Ron the downstream ASR task across different
languages. We recommend the reader to read [24] along with
this paper to get a holistic understanding of this Ô¨Åeld.

--- PAGE 3 ---
3
Fig. 3: An illustration of the multimodal training framework
Pre-Trained XLS-REncoder (Frame Level)Linear ProjectionAttention PoolingTanh Act. FunctionTransformer EncoderAll's well that ends well.
Cosine Distance LossStop Grad.CLS PoolingLinear ProjectionTanh Act. Functionùë≥ùíÇùë©ùë∫ùë¨ùë∫ùë®ùë¥ùëº-ùëøùë≥ùë∫ùëπ
II. M ETHODOLOGY
A. Problem Formulation
We train SAMU -XLSR using a multilingual set Dof paired
examples (x(l);y(l)), wherex(l)is the speech waveform, and
y(l)is its text transcript in language l. Given a training
example, (x(l);y(l)), we transform the sequence of discrete
tokensy(l)to a dense embedding vector zT2Rdusing a
text encoder g, and the series of speech samples x(l)into a
dense embedding vector zS2Rdusing a speech encoder f.
Then, we update the parameters of the speech encoder fso
that the distance between the speech embedding zSand the
text embedding zTis minimized. The training loss for a single
example is given by the following equation:
J(;) =distance (zS;zT) (1)
We use the pre-trained Language-agnostic BERT Sentence
Encoder ( LaBSE ) as the text encoder gandSAMU -XLSR as the
speech encoder f. The parameters of the speech encoder
are updated during training, while the parameters of the
text encoder remain Ô¨Åxed. An illustration of the multimodal
learning framework is shown in Figure 3.
B.SAMU -XLSR Speech Encoder, f
SAMU -XLSR consists of a pre-trained frame-level XLS-R
speech encoder [9] followed by a mechanism for pooling the
frame-level contextual representations into a single embedding
vector.
TheXLS-Rspeech encoder consists of a deep convolutional
neural network that maps 1D time series representing the
sample values of the speech waveform into a 2D sequence
of feature vectors H2RT512. Each feature vector ht2H
represents 20ms of the speech signal. The time resolution of
htis similar to that of an acoustic frame. Therefore, we refer
toHas frame-level representations. Next, the feature sequence
His transformed into contextual representations C2RT1024
by a deep transformer encoder [25]. The transformer encoder
consists of 24 Multi-Headed Self-Attention (MHSA) trans-
former blocks. The attention vector size is 1024, and there
are 16 attention heads in each block. We use the publicly
available pre-trained XLS-Rcheckpoint1which was trained on
400k hours of unlabeled speech data in 128 languages.
1https://huggingface.co/facebook/wav2vec2-xls-r-300mNext, we use Self-Attention pooling [26] strategy to get a
single utterance-level embedding vector e2R1024. In this
pooling strategy, we take a weighted combinationTP
t=1vtctof
contextual vectors ct2 C, where v= (v1;:::;vT)is the
attention vector, given by the following equation:
v=softmax (Cw) (2)
where, w2R1024, which gives v2RT, such thatP
tvt= 1.
The weight vector wis learned during training.
Finally, we take a non-linear projection of the embed-
ding vector eto get the speech embedding zS. Overall, the
SAMU -XLSR speech encoder consists of approximately 300
million trainable parameters (weights and biases).
C. LaBSE Text Encoder, g
The key ingredient in our proposed multimodal learning
framework is the LaBSE text encoder g, which allows us to
learn a joint speech-text embedding space that is semantically
aligned and shared across different languages. LaBSE is a
language-agnostic text encoder for text with an architecture
similar to the BERT transformer encoder [27]. However,
unlike BERT, LaBSE is a sentence embedding model, which
is trained using both masked [27] and translation language
modeling [28] objective functions. LaBSE consists of a token
level transformer encoder with 12 MHSA layers, followed
by a pooling mechanism to construct a dense sentence-level
embedding vector.
TheLaBSE ‚Äôs transformer encoder takes as input text that is
tokenized into ‚Äùwordpieces‚Äù [29], [30] and outputs a sequence
of contextual token embedding W 2RL768. A non-linear
projection of the CLS token embedding is used as the sen-
tence embedding zT2R768, which is used as the training
target for SAMU -XLSR training. We use the pre-trained LaBSE
model checkpoint2hosted on the Huggingface [31] models3
platform. We refer to the use of CLS token embedding for
sentence representation as CLS pooling to conform with the
terminology used in the Huggingface hosted LaBSE encoder.
LaBSE embeds sentences from 109 languages into a shared
semantically-aligned embedding vector space. Unlike LaBSE ,
2https://huggingface.co/sentence-transformers/LaBSE
3https://huggingface.co/models

--- PAGE 4 ---
4
other multilingual text encoders such as XLM-R [32] do
not learn an aligned sentence embedding space. Therefore,
to achieve our goal of embedding speech in a semantically
aligned vector space, we use LaBSE as the teacher for training
SAMU -XLSR .
D.SAMU -XLSR Training Details
1) Training Data, D:We train SAMU -XLSR on transcribed
speech in 25 languages derived from the publicly avail-
able CommonV oice-v7 (CoV o) dataset. The 25 languages are
namely, English (EN), French (FR), German (DE), Spanish
(ES), Catalan (CA), Italian (IT), Welsh (CY), Russian (RU),
Chinese (China) (ZH CN), Chinese (Taiwan) (ZH TW), Chi-
nese (Hong Kong) (ZH HK), Portuguese (PT), Polish (PL),
Persian (FA), Estonian (ET), Mongolian (MN), Dutch (NL),
Turkish (TR), Arabic (AR), Swedish (SV SE), Latvian (LV),
Slovenian (SL), Tamil (TA), Japanese (JA) and Indonesian
(ID). Table I shows the per-language transcribed data available
in CoV o. The total training data size is 6.8K hours.
Clearly, the data is highly imbalanced. The top 5 high-
resource languages make up 72% of the training data, while
the bottom 14 low-resource languages make up just 10% of
the training data. The above mentioned problem could lead to
SAMU -XLSR severely under-Ô¨Åtting on low-resource languages,
because SAMU -XLSR , during its training lifetime, might en-
counter transcribed speech data from low-resource languages
in its train mini-batch only a few times. Following [33], [34]
we re-balance the training set Dby up/down-sampling data
from each language lwith a ratio l:
l=1
plp
lP
lp
lwithpl=nl
LP
l=1nl(3)
where,is the smoothing parameter, nlis the number of
utterances for language lin the training set. Figure 4, shows
how varying between 1.0 and 0.05 re-balances the training
set. As we make smaller, observe that the share of low-
resource languages in the training set becomes approximately
same as that of high-resource languages. It is important to note
that when we up-sample data from low-resource languages,
we simply repeat the utterances from those languages, and,
down-sampling data from high-resource languages involve
picking random utterances according to the ratio l. Hence,
training with a re-balanced training set that is created using
a small value of could result in a drop in performance
on high-resource languages as compared to the model that
is trained with the original unbalanced training set. We study
the effect that the smoothing parameter has on the model‚Äôs
downstream task performance in Section VI-B.
2) Optimization Settings: We train SAMU -XLSR for 400K
training iterations, on 32 V100-32gb GPUs, with a per-
GPU mini-batch size of approximately 2 hours of transcribed
speech. Following [7], we use the Adam optimizer for up-
dating the model parameters with a three phase learning rate
scheduler; Warm-up the learning rate to a maximum value
of 1e-4 for the Ô¨Årst 10% of the training iterations, then
the learning rate remains constant for the next 40% of theFig. 4: Re-balancing the training set with different values of
the smoothing parameter 
high resource low resource024#Utterances1e6 Alpha=1.0
high resource low resource021e6 Alpha=0.7
high resource low resource01#Utterances1e6 Alpha=0.1
high resource low resource011e6 Alpha=0.05
TABLE I: Amount of per language transcribed speech data in
the CommonV oice-v7 dataset
Lang EN DE CA FR ES
Dur [Hrs] 2K 960 790 740 380
Lang FA IT CY TA RU
Dur [Hrs] 290 290 220 200 150
Lang PL ZH HK NL PT AR
Dur [Hrs] 130 96 93 85 84
Lang ZH CN ZH TW SV SE ET TR
Dur [Hrs] 63 59 34 32 32
Lang JA ID MN SL LV
Dur [Hrs] 27 25 12 9 7
training iterations, and Ô¨Ånally decays linearly for the rest of
the iterations. For the Ô¨Årst 10K training iterations, only the
projection layer of SAMU -XLSR encoder is trained while the
pre-trained frame-level XLS-Rspeech encoder remains Ô¨Åxed.
We do not update the weights of the XLS-R‚Äôs convolutional
feature extractor throughout the training process. Also, we
use a modiÔ¨Åed version of SpecAugment [35] on the feature
sequenceH(Section II-B) to mask the input to the XLS-R‚Äôs
transformer encoder, which leads to better performance on
downstream tasks. The above mentioned training settings are
the standard for Ô¨Åne-tuning the pre-trained XLS-Ror wav2vec-
2.0 speech encoders on downstream ASR tasks [1], [7].
We use the cosine distance between the speech and the
text embedding as the training loss (Equation 1). We do not
update the weights of the LaBSE text encoder throughout
training. The reason for this design choice is straightforward.
LaBSE ‚Äôs sentence embedding space is already semantically
aligned across 109 languages. By Ô¨Åne-tuning LaBSE along
with SAMU -XLSR on transcribed speech data D, we run the
risk of destroying this alignment. In fact, LaBSE will have no
incentive to maintain an aligned embedding space. Instead,
our learning framework simply attempts to embed speech
utterances in the LaBSE ‚Äôs sentence embedding space to make
it multimodal. By simply forcing the speech embeddings
outputted by SAMU -XLSR to be closer to LaBSE text embedding,
we get the cross-lingual semantic alignments between speech
utterances in different languages and text in 109 languages

--- PAGE 5 ---
5
TABLE II: SAMU -XLSR model card
Parameters Value
Training Data CoV o 25
Smoothing factor ( ) for data re-balancing 0.05
Training updates 200K
Freeze Fine-tune updates 10k
CNN Feature Extractor Frozen
Optimizer Adam
max learning rate (LR) 1e-4
LR scheduler 10-40-50
batch size / GPU 2Hrs
Data Augmentation SpecAugment on H
Training Objf. Cosine Distance
Training Teacher LaBSE
Pooling Fn. Self-Attention
Model init. XLSR Pre-Trained checkpoint
Num. GPUs 32
Supported Spoken Langs 22
Supported text Langs 109
without ever encountering cross-lingual associations during
the model‚Äôs training. Having said that, it might be possible to
train the LaBSE text encoder along with SAMU -XLSR and still
maintain the LaBSE ‚Äôs semantically aligned embedding space.
But, it is out-of-scope of this paper.
E.SAMU -XLSR Model Card
Table II summarizes the best conÔ¨Åguration of different
hyperparameters for training SAMU -XLSR encoder. Next, we
explain what some parameters in the table mean. CoV o 25
refers to the multilingual transcribed speech data used for
training the model. We use data in 25 languages from the
CoV o dataset. CNN Feature Extractor refers to the pre-trained
XLS-R‚Äôs convolutional encoder that maps the 1D speech wave-
form to a 2D feature representation that is used as input to
the transformer encoder. We keep its weights Ô¨Åxed to the pre-
trained value. Freeze Fine-tune updates refer to the number
of training iterations up to which we only train the projection
layer of SAMU -XLSR . See Equation 3 and the text above it for
details on the smoothing factor . The learning rate scheduler
(LR scheduler) has a value of 10-40-50 refers to the learning
rate scheduler mentioned in Section II-D. Training teacher is
LaBSE which refers to the fact that the training targets for
SAMU -XLSR are the embedding vectors corresponding to the
text transcripts provided by LaBSE . The model supports 25
spoken languages and 109 written languages since SAMU -XLSR
is trained on the transcribed speech from 25 languages and
LaBSE can encode text in 109 languages in its semantically
aligned cross-lingual vector space.
III. D OWNSTREAM EVALUATION TASKS & M ETRICS
A. Overview
Retrieval : We evaluate our multimodal framework (Fig. 3)
that consists of SAMU -XLSR , a speech embedding model, and
LaBSE , a text embedding model, on several downstream trans-
lation retrieval tasks. Retrieval is a common way to evaluatemultilingual semantically aligned sentence embedding vector
spaces in Natural language processing [15], [19].
As mentioned before, our work aims to learn a semantically
aligned cross-lingual multimodal (joint speech-text) embed-
ding space. Hence, if successful at achieving our desired goal,
theSAMU -XLSR -LaBSE combination should give good per-
formance on cross-lingual speech-to-text translation retrieval
tasks. Also, SAMU -XLSR alone should be able to perform well
on cross-lingual speech-to-speech translation retrieval tasks.
Sequence Generation : Furthermore, we perform sequence-
to-sequence modeling tasks, namely the Connectionist Tem-
poral ClassiÔ¨Åcation (CTC) [36] based Phoneme Recognition
(generating the underlying phoneme sequence correspond-
ing to an input speech sequence) and Automatic Speech
Recognition (ASR) (generating the underlying word sequence
corresponding to an input speech sequence) using SAMU -XLSR .
B. Translation Retrieval Tasks
Here, we summarize the retrieval process, evaluation met-
rics and the speech-to-text and speech-to-speech translation
retrieval tasks we use to evaluate the SAMU -XLSR ‚Äôs multimodal
semantic embedding space.
Retrieval process and Evaluation Metrics : We construct
two databases (DB), query and search, to perform translation
retrieval. The query DB consists of speech utterances in a
language X, and in the case of text translation retrieval tasks,
the search DB consists of text sentences in a language Y . The
task is to retrieve the correct text translation from the search
DB corresponding to each speech query in the query DB. To
that end, we transform the speech utterances in the query DB
through SAMU -XLSR to query speech embedding matrix Q2
RN768, whereNis the number of speech queries in the
query DB. Also, we transform the sentences in the search DB
through the LaBSE encoder to search text embedding matrix
S2RM768, whereMis the number of sentences in the
search DB. Given that the vectors are normalized, we could
retrieve the text translations for the speech queries as follows:
A=QST
r=argmaxjA:;j
where,A2RNMis the cosine similarity matrix, whose
(i;j)thelementAi;jis the cosine similarity between the
speech query embedding qi2Qand the sentence embedding
sj2S, and r2RNis the index vector, such that its each
componentri2ris the index of the closest match in the text
translation search DB. Also, given the index vector u, where
each component uj2uis the index of the ground-truth text
translation in the search DB, we compute the model‚Äôs retrieval
accuracy as follows:
ACC = 100NP
i=11fri=uig
N(4)
where, the function 1fri=uigreturns one when ri=ui, the
predicted translation index matches the ground-truth transla-
tion index, otherwise it outputs zero. Hence, the numerator
is the number of queries for which the model retrieved the

--- PAGE 6 ---
6
correct translations from the search DB and the denominator
is the total number of queries in the query DB.
We refer to the retrieval accuracy in Equation 4 as Recall@1
or R@1, which contrasts with another similar metric, R@5,
where the indicator function returns one if any of the top Ô¨Åve
retrieved search DB indices matches with the correct index.
We report R@5 for speech retrieval evaluation tasks. The
recall is commonly used to evaluate audio-visual multimodal
representation learning models [37]‚Äì[39].
In addition to R@1, for text translation retrieval tasks, we
also report the Word Error Rate (WER) [40] between the
retrieved and the ground-truth text translation. The reason is
that it is hard to interpret retrieval accuracies. For example,
WER for model A with a retrieval accuracy of 70% might not
be much worse than the WER for model B with a retrieval
accuracy of 80% because model A might be worse than model
B in retrieving the exact translations. However, it might still
recover translations with a signiÔ¨Åcant string overlap with the
actual translation. The retrieval accuracy will fail to capture
this.
X!EN Text Translation Retrieval : We use the CoV oST-2
[41] X-EN speech-translation dataset for this evaluation task.
The speech query DB is in a language X 2fRU, IT, FR, ES,
TR, DE, ET, CY , NL, ID, CA, FA, AR, ZH, SV , MN, SL, JA,
TA, LVgand the search DB consists of English sentences. To
construct the speech query DB for each language X, we use
the combined testing and development sets (henceforth, eval
set) from CoV oST-2. To construct the search DB, we combine
the English text translation from all the 22 X !EN eval sets
in CoV oST-2, which we refer to as Sa. In addition, we create
a search DB Sb, that contains approximately 1.4M English
sentences from the CoV o English transcribed speech data. We
use the combined search DB S=Sa[Sbfor all the 22 X!EN
text translation retrieval tasks. We add SbtoSato make the
retrieval task harder than if we just search over Sa.
EN!Y Text Translation Retrieval : We use the the pub-
licly available CoV oST-2 corpora [41] for this evaluation task,
which consists of English speech queries paired with their text
translations. The speech query DB is in English and search DB
is in a language Y2fDE, CA, ZH, FA, ET, MN, TR, AR, SV ,
LV , SL, TA, JA, ID, CY g. For each EN!Y retrieval task,
the query DB consist of speech utterances in the combined
development and testing sets. The search DB consists of
the true text translations in language Y . corresponding to
the speech queries. In addition, we add the Y language text
translations available in the EN !Y CoV oST-2 training set to
make the retrieval task harder. Similarly, we create a search DB
for each of the 15 languages Y for the EN !Y text translation
retrieval task.
For this evaluation scenario, we also perform text translation
retrieval on the MUST-C [42] EN !Y corpora. In MUST-C,
we have English speech queries paired with their true text
translation in a language Y 2fES, PT, FR, DE, Romanian
(RO), NL, IT, Czech (CS), Vietnamese (VI), FA, TR, AR,
RU, ZHg. We create an eval set, a union of MUST-C dev,
tst-COMMON and tst-HE data splits. The speech query DB
consists of speech utterances in the eval set. The search DB for
a language Y consists of sentences from the EN !Y MUST-Ceval set combined with sentences from the EN !Y training
set.
X!Y Text Translation Retrieval : We use the MTEDx
[43] speech-translation corpora, which consists of speech
queries in language X paired with their ground-truth text
translation. For this evaluation task, we have the translation
pairs X Y2fITES, IT EN, ES FR, ES IT, FR PT, ES PT,
FREN, PT ES, ES EN, PT EN, RU ENg. For a translation
pair X Y , we have speech queries in language X and the text
search DB in language Y . For a retrieval X !Y , the query
DB consists of speech utterances in the MTEDx X !Y eval
set (dev+test), and the text search DB in language consists
of the ground-truth text translations from the X !Y eval set
and the X!Y training set. The reader might observe that the
search DB is more signiÔ¨Åcant than the query DB for all the
text translation retrieval tasks and consists of the actual text
translations and random sentences to make the retrieval task
harder.
We consider MTEDx X !Y translation retrieval evaluation
tasks as out-of-domain because we train SAMU -XLSR on tran-
scribed read speech from the CoV o dataset. At the same time,
MTEDx consists of oratory speech collected from TED talks.
X!EN Speech Translation Retrieval : Finally, we evaluate
our model on speech translation retrieval tasks. We get the
parallel X!EN speech-speech translation data from the pub-
licly available V oxPopuli corpora [44]. For this task, speech
queries are in a language X 2fES, FR, PL, NL, DE, RO,
Croatian (HR), CS gand the search DB consists of English
speech translations corresponding to the queries. Unlike the
text translation retrieval tasks, the search DB is the same size
as the query DB and consists of only actual speech translations
corresponding to the queries.
C. Sequence-to-Sequence Modeling Tasks
Phoneme Recognition : Phoneme recognition refers to the
task of automatically decoding the underlying phoneme se-
quenceycorresponding to a speech sequence ( x). We Fine-
tune the Pre-trained SAMU -XLSR using paired (x;y)examples
drawn from the CoV o dataset. Following [7], [45], we build a
phoneme recognizer for nine different languages, namely ES,
FR, IT, Kabyle (KY), NL, RU, SV , TR, and Tatar (TT). We use
one hour of transcribed data for training, 20mins for validation
(model selection), and one hour for testing. The data splits are
the same ones proposed in [45] and used in [7] for evaluating
XLS-Ron the phoneme recognition task. Our Fine-tuning setup
matches the XLS-RFine-tuning setup used in [7].
Automatic Speech Recognition : ASR refers to the task
of automatically decoding the underlying word sequence cor-
responding to a speech utterance. The Fine-tuning setup is
the same as that for Phoneme Recognition. However, instead
of phoneme sequence as the target for training, we have
character sequences. To generate the word sequence from
decoded character sequence, we use CTC beam search with a
character-level N-gram language model.
We use the Espnet speech recognition toolkit [46], [47] for
Fine-tuning the Pre-trained SAMU -XLSR andXLS-Rmodels for
sequence-to-sequence modeling tasks.

--- PAGE 7 ---
7
TABLE III: We perform zero-shot X!EN text translation retrieval on In-domain CoV oST-2 dataset. The search database for
all X!EN retrieval tasks consists of 1.6 million English sentences. We give the number of speech utterances in the query
database for each retrieval task below. The task is to retrieve the correct text translation for the speech queries in language
X. We report the Retrieval accuracy (R@1) and the Word Error Rate between the ground-truth and retrieved text translations.
We compare our retrieval pipeline SAMU -XLSR -LaBSE , with ASR-LaBSE and the Topline retrieval model. The SAMU -XLSR -LaBSE
retrieval pipeline transforms speech queries to embedding vectors using our SAMU -XLSR speech encoder. Then, we match the
query embedding vectors with the LaBSE text embeddings of the sentences in the search DB to retrieve the translation. The
ASR-LaBSE retrieval pipeline Ô¨Årst uses an ASR for language X to transcribe speech queries and then uses LaBSE to perform
text-to-text translation retrieval. The Topline model uses the ground-truth text transcripts for the speech queries and performs
text-to-text translation retrieval tasks using LaBSE .
X RU IT FR ES TR DE ET CY NL ID CA FA AR ZH SV MN SL JA TA Avg.
Query DB 12K 18K 30K 26K 3.3K 27K 3.1K 1.4K 3.4K 1.6K 25K 6.8K 3.5K 9.7K 2.9K 3.5K 870 1.3K 1.2K -
SAMU -XLSR -LaBSE Speech(X) !Text(EN) Retrieval
R@1[%] 93.5 92.9 92.5 92.9 93.4 90.9 91.5 84.6 89.7 84.4 82.1 83.6 73.7 78.6 72.4 68.2 52.1 48.9 42.4 76.8
WER[%] 2.6 3.0 3.5 3.6 3.7 4.7 4.8 5.1 4.9 9.5 11.0 10.2 13.8 15.2 19.0 26.0 32.4 44.7 57.7 17.2
ASR-LaBSE Speech(X) !Text(EN) Retrieval
R@1[%] 92.7 90.1 90.4 91.3 90.9 88.2 94.8 81.7 89.3 65.6 80.6 76.1 54.0 55.4 63.9 53.9 64.0 23.6 26.5 71.7
WER[%] 3.0 4.8 5.0 4.6 5.8 6.5 2.1 7.6 5.3 23.4 11.5 16.8 34.3 36.0 17.2 41.3 16.7 72.9 75.0 20.9
Topline LaBSE Text(X) !Text(EN) Retrieval
R@1[%] 94.4 94.0 94.8 94.3 94.2 93.2 97.5 86.2 90.8 91.3 83.8 85.1 74.5 81.4 87.0 81.3 70.9 83.1 49.2 85.2
WER[%] 2.0 2.5 1.9 2.6 2.9 2.8 0.4 4.1 4.2 2.5 9.9 8.7 13.5 12.8 4.7 14.4 10.2 9.4 51.7 8.7
TABLE IV: We perform zero-shot EN!Y text translation retrieval on In-domain CoV oST-2 dataset. The search database for
each EN!Y retrieval task consists of 320K sentences in language Y , and the query database consists of 31K English speech
utterances. The task is to retrieve the correct text translation for the English speech queries. We report the Retrieval accuracy
(R@1) and the Word Error Rate between the ground-truth and retrieved text translations. We compare our retrieval pipeline
SAMU -XLSR -LaBSE , with ASR-LaBSE and the Topline retrieval model. The SAMU -XLSR -LaBSE retrieval pipeline transforms speech
queries to embedding vectors using our SAMU -XLSR speech encoder. Then, we match the query embedding vectors with the
LaBSE text embeddings of the sentences in the search DB to retrieve the translation. The ASR-LaBSE retrieval pipeline Ô¨Årst
uses an English language ASR to transcribe speech queries and then uses LaBSE to perform text-to-text translation retrieval.
The Topline model uses the ground-truth text transcripts for the speech queries and performs text-to-text translation retrieval
tasks using LaBSE .
Y ZH SL TR LV CY ID DE CA AR SV ET TA FA JA MN Avg.
SAMU -XLSR -LaBSE Speech(EN) !Text(Y) Retrieval
R@1[%] 87.2 90.5 89.4 89.9 90.8 91.0 91.5 91.4 88.3 91.7 90.4 90.5 89.0 88.1 86.2 89.7
WER[%] 11.2 6.3 7.4 7.2 6.2 5.9 5.8 5.5 8.5 5.5 6.6 7.3 8.4 11.9 10.9 7.6
ASR-LaBSE Speech(EN) !Text(Y) Retrieval
R@1[%] 87.9 90.6 89.8 90.2 90.7 91.2 91.4 91.6 89.0 91.7 90.5 91.2 89.6 88.4 87.3 90.1
WER[%] 10.7 6.2 7.1 6.9 6.2 5.7 5.8 5.3 7.8 5.4 6.5 6.5 7.7 11.5 9.8 7.3
Topline LaBSE Text(EN) !Text(Y) Retrieval
R@1[%] 95.8 97.1 96.2 96.6 96.7 96.8 97.1 96.9 95.7 97.3 96.7 97.0 95.4 95.5 94.5 96.4
WER[%] 2.7 1.3 1.9 1.8 1.7 1.5 1.5 1.3 2.3 1.3 1.6 1.8 2.8 4.2 3.5 2.1

--- PAGE 8 ---
8
We believe that evaluating SAMU -XLSR on sequence genera-
tion tasks mentioned above is interesting because it would be
good to know whether SAMU -XLSR , a speech encoder that we
train using an utterance-level objective function (See Fig. ??),
could also be used for tasks other than the utterance-level text
and speech translation retrieval.
Another thing to note is that for sequence generation tasks,
we dissect SAMU -XLSR before the attention pooling layer (See
Fig. 3 to look at SAMU -XLSR ‚Äôs architecture) and use the
computational modules before the pooling layer because for
sequence generation tasks, we want a representation at the
acoustic frame-level instead of the utterance level embedding
outputted by SAMU -XLSR .
IV. D OWNSTREAM TASKS : ZERO-SHOT TRANSLATION
RETRIEVAL
A. Additional Retrieval Models for comparison with
SAMU -XLSR
ASR-LaBSE retrieval pipeline : We also perform translation
retrieval tasks using an ASR-LaBSE combination, where we
convert the speech queries into text transcripts in the same
language as the queries using an ASR model. Then, we
perform ASR transcript to text translation retrieval using
LaBSE . We build 25 language-speciÔ¨Åc ASR models to cover
all the spoken languages in our text translation retrieval tasks.
To construct the ASR models, we Ô¨Åne-tune the pre-trained
XLS-Rcheckpoint on the downstream ASR task using the
transcribed speech data in the target language available from
the CoV o dataset (See Table I for the amount of per language
transcribed speech data). We use the standard Connectionist
temporal ClassiÔ¨Åcation [48] based optimization setup for Ô¨Åne-
tuning the XLS-Rmodel for the ASR task detailed in [7]. We
use a beam size of 20 and a tri-gram character-level language
model for decoding speech queries to text. We use the ESPnet
speech recognition toolkit [46], [49] for constructing the ASR
models and decoding.
Topline : As a topline, we use the ground-truth transcrip-
tions corresponding to speech queries and perform ground-
truth transcription to text translation retrieval using LaBSE .
Our SAMU -XLSR -LaBSE retrieval framework cannot perform
better than the topline. Because the best we can do with
our proposed multimodal learning framework is to match the
LaBSE embedding vectors perfectly.
B. Results on X!EN text translation retrieval tasks
Table III shows the results on X !EN translation retrieval
tasks using SAMU -XLSR -LaBSE ,ASR-LaBSE and Topline LaBSE
retrieval pipelines. We report the retrieval accuracy (R@1)
and WERs for different spoken languages X. The task is to
retrieve the English text translation for a given speech query
(X). The table shows the number of speech queries per spoken
language X. The number of speech queries in the evaluation set
varies across languages, with more queries for high-resource
languages and less for low-resource languages. It is a function
of the evaluation set available for different languages in the
CoV oST-2 eval set. The search for the English translation is
over a text database that consists of 1.6M English sentences.The text DB contains the actual English translations and the
text transcriptions from the CommonV oice English dataset.
We added the extra English sentences to make the translation
retrieval task harder than searching over a small database of
only true English translations. See Section III-B for more
details on X!EN retrieval tasks.
Interestingly, ASR-LaBSE is signiÔ¨Åcantly worse than
SAMU -XLSR -LaBSE retrieval model on retrieval tasks where
the speech queries are in non-European languages. For ex-
ample, on ID!EN, FA!EN, AR!EN, ZH!EN, MN!EN,
JA!EN and TA!EN retrieval tasks, SAMU -XLSR -LaBSE
achieves a WER of 9.5%, 10.2%, 13.8%, 15.2%, 26.0%,
44.7% and 57.7% respectively compared to 23.4%, 16.8%,
34.3%, 36.0%, 41.3%, 72.9%, 75.0% respectively by ASR-
LaBSE . On average SAMU -XLSR -LaBSE achieves an average
WER of 22.6% compared to 33.7% with ASR-LaBSE on
non-European spoken languages (X) !EN translation retrieval
tasks. On retrieval tasks, where speech queries are in Euro-
pean languages, SAMU -XLSR -LaBSE performs at par with ASR-
LaBSE retrieval pipeline. For example, on RU !EN, IT!EN,
FR!EN, ES!EN, DE!EN, ET!EN, CY!EN, NL!EN,
CA!EN, SV!EN, SL!EN and LV!EN translation re-
trieval tasks, SAMU -XLSR -LaBSE achieves an average WER of
13.6% compared to 10.2% with ASR-LaBSE retrieval pipeline.
These results are not surprising given the fact that for Eu-
ropean languages (high and low-resource), the ASR system
is generally better than for the non-European languages. This
is due to the fact that the XLSR speech encoder, which we
Ô¨Åne-tune on downstream ASR task using language-speciÔ¨Åc
transcribed data, is pre-trained on majority European language
speech data.
Finally, the topline model uses the ground-truth text tran-
scriptions corresponding to the speech queries (X) to retrieve
the English text translations. This model uses only LaBSE
to perform the text(X) !text(EN) retrieval task. The topline
achieves an average WER of 14.5% on non-European lan-
guages X and 4.9% on European languages, which implies
that we could not quite reach the topline performance with
ourSAMU -XLSR -LaBSE retrieval pipeline and there is room
for improvement. We believe that increasing the scale of the
training data and using contrastive loss for training SAMU -XLSR
could result in improved performance. However, a training
setup with a contrastive loss would require considerable engi-
neering effort because of the engineering complexity involved
in mining negative samples across GPUs as done for training
LaBSE [15]. Drawing negative samples from the same GPU
device would not be sufÔ¨Åcient because of the small per
GPU batch size owing to the large speech encoder size and
long speech waveforms. Hence, we leave the exploration of
contrastive learning for future work.
C. Results on EN!Y text translation retrieval tasks
Table IV and V shows the results on EN !Y speech!text
retrieval tasks using SAMU -XLSR -LaBSE ,ASR-LaBSE and
Topline LaBSE retrieval pipelines. We retrieve the text trans-
lation in a language Y for a given speech query in English for
the EN!Y retrieval tasks. In the results table, Ô¨Årst, we show

--- PAGE 9 ---
9
TABLE V: We perform zero-shot EN!Y text translation retrieval on Out-of-domain MUST-C dataset. The search database
for each EN!Y retrieval task consists of approximately 200K sentences in language Y , and the query database consists of
about 4K English speech utterances. The task is to retrieve the correct text translation for the English speech queries. We
report the Retrieval accuracy (R@1) and the Word Error Rate between the ground-truth and retrieved text translations. We
compare our retrieval pipeline SAMU -XLSR -LaBSE , with ASR-LaBSE and the Topline retrieval model. The SAMU -XLSR -LaBSE
retrieval pipeline transforms speech queries to embedding vectors using our SAMU -XLSR speech encoder. Then, we match the
query embedding vectors with the LaBSE text embeddings of the sentences in the search DB to retrieve the translation. The
ASR-LaBSE retrieval pipeline Ô¨Årst uses an English language ASR to transcribe speech queries and then uses LaBSE to perform
text-to-text translation retrieval. The Topline model uses the ground-truth text transcripts for the speech queries and performs
text-to-text translation retrieval tasks using LaBSE .
Y DE PT FR DE RO NL IT CS VI FA TR AR RU ZH Avg.
SAMU -XLSR -LaBSE Speech(EN) !Text(Y) Retrieval
R@1[%] 87.4 88.2 87.1 86.8 87.3 86.3 85.6 85.1 82.4 82.5 84.1 83.2 81.3 77.8 84.6
WER[%] 7.0 6.8 7.3 7.4 7.5 7.8 8.5 10.1 10.2 12.3 11.7 13.8 13.4 21.0 10.3
ASR-LaBSE Speech(EN) !Text(Y) Retrieval
R@1[%] 88.8 88.6 88.4 87.9 87.5 87.0 86.6 86.4 83.0 83.8 84.5 83.8 82.7 79.0 85.6
WER[%] 6.2 6.5 6.4 6.7 7.4 7.2 7.7 8.9 9.8 10.3 11.1 13.2 12.3 20.6 9.6
Topline LaBSE Text(EN) !Text(Y) Retrieval
R@1[%] 96.1 96.0 96.1 95.9 95.7 95.3 95.1 95.1 92.9 91.4 92.7 92.4 92.3 87.6 93.9
WER[%] 1.8 1.9 1.8 1.8 2.2 2.1 2.5 3.2 3.1 6.1 5.2 6.5 5.0 10.0 3.8
TABLE VI: We present results on Out-of-domain MTEDx X!Y text translation retrieval tasks. For a retrieval task X Y , the
speech queries are in language X, and the search DB consists of sentences in language Y . The task is to retrieve the correct text
translation for each speech query. We report the Retrieval accuracy (R@1) and the Word Error Rate between the ground-truth
and retrieved text translations. We compare our retrieval pipeline SAMU -XLSR -LaBSE , with ASR-LaBSE and the Topline retrieval
model. The SAMU -XLSR -LaBSE retrieval pipeline transforms speech queries to embedding vectors using our SAMU -XLSR speech
encoder. Then, we match the query embedding vectors with the LaBSE text embeddings of the sentences in the search DB to
retrieve the translation. The ASR-LaBSE retrieval pipeline Ô¨Årst uses an ASR model for language X to transcribe speech queries
and then use LaBSE to perform text-to-text translation retrieval. The Topline model uses the ground-truth text transcripts for
the speech queries and performs text-to-text translation retrieval tasks using LaBSE .
XY IT ES IT EN ES FR ES IT FR PT ES PT FR EN PT ES ES EN PT EN RU EN Avg.
Query DB 1.8K 2K 1.8K 270 2K 1.8K 2K 2K 1.8K 2K 1.8K -
Search DB 1.6M 270K 220K 250K 270K 210K 1.6M 1.6M 1.6M 210K 270K -
SAMU -XLSR -LaBSE Speech(X) !Text(Y) Retrieval
R@1[%] 92.2 87.0 87.5 84.5 86.7 85.9 81.0 80.8 78.6 74.6 61.2 81.8
WER[%] 2.8 5.7 6.2 6.3 6.3 6.4 8.3 9.4 9.6 12.4 26.0 9.0
ASR-LaBSE Speech(X) !Text(Y) Retrieval
R@1[%] 92.5 88.2 90.2 85.7 87.1 88.3 82.9 84.7 82.2 80.1 69.9 84.7
WER[%] 2.4 4.4 4.2 6.1 5.5 4.5 6.9 7.1 7.4 8.8 17.0 6.8
Topline LaBSE Text(X) !Text(Y) Retrieval
R@1[%] 96.1 93.3 94.4 91.7 93.5 94.1 90.9 94.8 90.5 91.7 87.6 92.6
WER [%] 1.0 2.3 1.8 2.9 2.0 1.7 2.9 1.3 3.2 2.6 5.4 2.5

--- PAGE 10 ---
10
TABLE VII: We perform zero-shot X!EN speech translation retrieval on the V oxPopuli dataset. The speech queries are in a
language X, and the search database consists of speech utterances that are translations of speech queries. Unlike text translation
retrieval tasks, where the search DB is much bigger than the query DB, here, the search and the query DB have the same
size. During its training, SAMU -XLSR did not have access to any cross-lingual speech-to-speech associations. Hence, semantic
alignment among speech utterances in different languages is an emergent property of the embedding vector space learned by
SAMU -XLSR via our proposed multimodal learning framework. We compare SAMU -XLSR ‚Äôs vector space with XLS-R.
X ES FR PL NL DE RO HR CS Avg.
SAMU -XLSR Speech(X) !Speech(EN) Retrieval
Query & Search DB 36K 50K 19K 11K 60K 16K 8K 11K -
R@1[%] 97.9 97.8 97.7 97.5 96.0 76.0 53.3 52.8 83.6
R@5[%] 98.5 98.4 98.4 98.0 97.1 80.9 59.5 58.2 86.1
XLS-RSpeech(X) !Speech(EN) Retrieval
R@1[%] - - - - 0.0 - - - 0.0
the number of English speech queries and the sentences in the
search database for each language, Y .
For the CoV oST-2 EN !Y retrieval tasks, we have 32K
English speech queries in the query DB and 320K sentences
in the search DB in language Y for each EN !Y retrieval task.
See Section III-B for more details on the EN !Y CoV oST-2
retrieval tasks.
Table IV shows results on CoV oST-2 EN !Y retrieval tasks.
We have 32K English speech queries in the query DB and
320K sentences in the search DB in language Y for each
EN!Y retrieval task. See Section III for more details on the
EN!Y CoV oST-2 retrieval tasks. We observe that SAMU -XLSR -
LaBSE and ASR-LaBSE retrieval pipelines perform at par
achieving a retrieval WER of 7.6% and 7.3% respectively,
while the Topline LaBSE text(EN)!text(Y) retrieval pipeline
achieves an average WER of 2.1% across the 15 retrieval tasks.
There is room for improvement. In particular, for retrieving
text translations in non-European languages such as ZH, MN,
JA, FA, AR, and TA, for which the average WER achieved
by our proposed SAMU -XLSR -LaBSE retrieval pipeline is 9.7%
compared to 2.8% with the topline LaBSE text(EN)!text(Y)
retrieval. For European languages, our retrieval model achieves
a WER of 6.1% compared to 1.7% for the topline model. Our
model performs better in European languages (6.1% WER)
than non-European languages (9.7% WER).
Table V shows EN!Y retrieval results on the out-of-domain
MUST-C evaluation corpus. We have the same number of
4K speech utterances in the query DB and 200K sentences
in the search DB for all text translation retrieval tasks. We
observe that SAMU -XLSR -LaBSE perform at par with ASR-
LaBSE retrieval pipeline, achieving an average of 10.3%
WER compared to 9.6% achieved by the ASR-LaBSE retrieval
pipeline on the 14 EN !Y retrieval tasks. Our model achieves
a WER of less than 10% for most languages except TR,
AR, RU, and ZH, for which the model achieves a WER of
11.1%, 13.2%, 12.3%, and 20.6% respectively. These WERs
are approximately double the WERs, achieved by the topline
LaBSE text(EN)!text(Y) retrieval model. However, the WERsare at a respectable less than 20% mark.
D. Results on X!Y text translation retrieval tasks
Table VI shows results on out-of-domain MTEDx X !Y
text translation retrieval tasks using SAMU -XLSR -LaBSE ,ASR-
LaBSE and topline LaBSE retrieval pipelines. The table shows
the speech queries and text search database combination for
each pair X Y . We observe that SAMU -XLSR -LaBSE achieves
an average retrieval WER of 9% compared to 6.8% with
ASR-LaBSE and 2.5% with topline LaBSE on the 11 text
translation retrieval tasks. It is unsurprising that ASR-LaBSE
retrieval pipeline performs better than the SAMU -XLSR -LaBSE
model. Because, the speech queries for X !Y retrieval tasks
are in European languages and our European language ASR
models are quite good. The results reported here conÔ¨Årm with
the observation we made for X !EN CoV oST-2 translation
retrieval tasks, where SAMU -XLSR -LaBSE performed better than
ASR-LaBSE for non-European languages but not for the Eu-
ropean languages. Note that if we had an ASR model that
generated text transcripts that exactly matched the ground-
truth transcripts, then the performance of ASR-LaBSE would
be same as that of the topline model.
E. Results on X!EN speech translation retrieval tasks
We observe that the SAMU -XLSR speech encoder learns
a semantically aligned vector space across several spoken
languages. The model can retrieve the correct English speech
translations corresponding to speech queries in a language X
with above 96% accuracy for X 2fES, FR, PL, NL, DE g. For
X2fRO, HR, CSg,SAMU -XLSR ‚Äôs speech translation retrieval
performance is lagging behind other languages. This result is
not surprising because SAMU -XLSR did not see any transcribed
data from these three languages during training. SAMU -XLSR
achieves an average retrieval R@1 accuracy of 83.6% across
the 8 X!EN speech translation retrieval tasks. On the other
hand, XLS-Rfails on this retrieval task. To get an utterance
level speech embedding from XLS-R, we perform temporal

--- PAGE 11 ---
11
TABLE VIII: We present Phoneme Error Rates, PER[%], achieved by Ô¨Åne-tuning SAMU -XLSR andXLS-Ron the downstream
phoneme recognition task across different languages. We use one hour of labeled training data for Ô¨Åne-tuning and twenty
minutes of development data for model selection. We evaluate the models using one hour of testing data. The test data is
unseen and only used after ASR Ô¨Åne-tuning for model evaluation. The train, dev, and test data splits are provided by [45] and
used in previous works for Ô¨Åne-tuning XLS-Rfor phoneme recognition [7].
Model ES FR IT KY NL RU SV TR TT Avg.
XLS-R[7] 2.9 5.0 5.7 6.1 5.8 8.1 12.2 7.1 5.1 6.4
XLS-R(Our Fine-tuning setup) 3.5 5.6 5.9 5.7 6.8 9.5 8.1 7.8 4.5 6.4
SAMU -XLSR (This work) 4.4 5.4 5.4 7.7 6.0 6.3 7.7 6.5 6.9 6.2
TABLE IX: We present Word Error Rates, WER[%], achieved by Ô¨Åne-tuning SAMU -XLSR andXLS-Ron the downstream speech
recognition task across different languages.
Model ES FR IT KY NL RU SV TR TT AR Avg.
XLS-R(Our Fine-tuning setup) 16.2 31.9 16.2 24.2 19.8 28.5 27.2 26.7 21.1 46.5 25.8
SAMU -XLSR (This work) 16.4 29.4 18.2 31.5 17.8 25.6 18.6 21.4 30.1 43.9 24.3
mean pooling of the contextual frame-wise embeddings from
the last layer of the model. From the poor retrieval results, it is
evident that the XLS-Rrepresentation space is not semantically
aligned across different languages. We achieve similarly poor
results with representations from different XLS-Rlayers.
V. D OWNSTREAM TASKS : SEQUENCE -TO-SEQUENCE
MODELING
A. Phoneme Recognition
Table VIII shows the phoneme error rates (PER) achieved by
SAMU -XLSR andXLS-Ron nine Commonvoice languages. We
observe that SAMU -XLSR is comparable with XLS-Ron phoneme
recognition task achieving an average PER of 6.2% compared
to 6.4% achieved by XLS-Racross the nine target languages,
namely. See Section III-C for details about the task and the
data used for Fine-tuning SAMU -XLSR andXLS-R.
B. Automatic Speech Recognition
Table IX shows the Word Error Rates (WER) achieved
by Fine-tuning SAMU -XLSR andXLS-Ron nine languages. We
observe that SAMU -XLSR performs at par with XLS-Rachieving
an average WER of 24.3% compared to 25.8% achieved
byXLS-R. Interestingly, on the out-of-domain Arabic (AR)
language, which is drawn from the MGB2 [50] news broadcast
corpus (different from the read speech CoV o corpus used to
Pre-train SAMU -XLSR ),SAMU -XLSR performs better that XLS-R.
The fact that sequence-to-sequence modeling results (ASR
& Phoneme Recognition) are at par with XLS-Rimplies that
SAMU -XLSR in addition to being useful for zero-shot cross-
lingual text and speech translation retrieval (Section IV) can
also be used for sequence generation tasks like ASR.
VI. E MPIRICAL ANALYSIS OF VARIOUS DESIGN CHOICES
In this section, we study various design decisions that went
into creating the SAMU -XLSR speech encoder.A. Loss and pooling functions
While detailing SAMU -XLSR in Section II-B, we mentioned
that we use the Self-Attention pooling method to construct
an utterance-level speech embedding from acoustic frame-
level contextual embedding vectors. Also, we use the cosine
distance loss for training SAMU -XLSR . Table X shows that
combining cosine distance loss and the Self-Attention pooling
method is better than combining other loss functions and
pooling methods. We train SAMU -XLSR with L1, L2, and
cosine distance losses and compare its average text translation
retrieval performance across the 21 X !EN CoV oST-2 retrieval
tasks. Also, we compare the retrieval performance with Mean,
Max, and Self-Attention pooling strategies. Three loss func-
tions with three pooling strategies lead to nine possible training
conÔ¨Ågurations. For quick analysis, we train SAMU -XLSR on 8
V100-32GB GPUs for 100K iterations on a subset DSof
the complete multilingual transcribed training data D.DSis
constructed by randomly sampling 400K training examples
fromD.SAMU -XLSR with Self-Attention pooling method and
trained with cosine distance loss reaches an average retrieval
R@1 accuracy of 48.8%, which is better than the other 8
training conÔ¨Ågurations.
B. Data Re-balancing Smoothing parameter 
This section studies the effect on the model‚Äôs average
retrieval performance across 21 X !EN retrieval tasks when
we train the model with re-balanced training data accord-
ing to Equation 3. The smoothing parameter is the only
hyper-parameter in the data re-balancing equation. First, we
construct several re-balanced multilingual transcribed speech
datasets corresponding to different values of . Then, we
randomly sample 400K utterances from re-balanced datasets
forSAMU -XLSR model training. We train SAMU -XLSR using
cosine distance loss function for 100K iterations on 8 V100-
32GB GPUs.

--- PAGE 12 ---
12
TABLE X: Avg. retrieval Performance on 21 X !EN text
translation retrieval tasks for different combinations of loss
and pooling functions
Loss Pooling R@5 [%] R@1 [%] WER [%]
L1 Max 52.2 44.0 50.9
L1 Mean 52.9 44.6 49.9
L1 Att. 54.0 45.6 48.8
Cos Max 55.4 46.6 47.5
L2 Max 55.6 46.8 47.3
Cos Mean 56.3 47.6 46.2
L2 Mean 57.2 48.2 45.4
L2 Att. 57.6 48.6 45.3
Cos Att. 58.0 48.8 44.6
TABLE XI: Avg. retrieval performance on 21 X !EN text
translation retrieval tasks for different values of 
 R@5 [%] R@1 [%] WER [%]
1.00 58.0 48.8 44.6
0.70 70.3 60.5 32.2
0.30 79.3 69.5 22.8
0.10 81.6 71.7 20.5
0.01 81.9 72.0 19.9
0.05 82.2 72.4 19.6
We observe in Table XI that the models trained with re-
balanced data (  < 1:0) achieve signiÔ¨Åcantly better aver-
age retrieval accuracy across the 21 X !EN text translation
retrieval tasks than the model trained with no re-balancing
(= 1:0). We achieve the best performance with = 0:05,
where the model‚Äôs average retrieval accuracy R@1 is 72.4%
compared to 48.8% achieved by SAMU -XLSR trained on the
original dataset without any re-balancing. The massive boost
in retrieval performance is due to the model doing much better
on X!EN retrieval tasks where speech queries are in low-
resource languages, which implies that the model was indeed
under-Ô¨Åtting on low-resource languages due to the data imbal-
TABLE XII: Avg. retrieval performance on 7 X !EN text
translation retrieval tasks for different s. The speech queries
are in low-resource languages
 R@5 [%] R@1 [%] WER [%]
1.00 32.1 23.8 72.1
0.05 71.9 61.4 29.7
TABLE XIII: Avg. retrieval performance on 5 X !EN text
translation retrieval tasks for different s. The speech queries
are in high-resource languages
 R@5 [%] R@1 [%] WER [%]
0.05 92.0 85.0 9.4
1.00 93.8 87.5 7.3TABLE XIV: Avg. retrieval performance on 21 X !EN text
translation retrieval tasks for different training data
Model R@5 [%] R@1 [%] WER [%]
SAMU -XLSR T2 49.9 41.3 54.6
SAMU -XLSR T3 79.7 69.5 22.7
SAMU -XLSR T1 82.2 72.4 19.6
TABLE XV: Avg. retrieval performance on 7 X !EN text
translation retrieval tasks for different training data. The
speech queries are in low-resource languages
Model R@5 [%] R@1 [%] WER [%]
SAMU -XLSR T2 15.5 9.2 91.4
SAMU -XLSR T3 67.3 55.7 36.1
SAMU -XLSR T1 71.9 61.4 29.7
ance in the training set of SAMU -XLSR . Table XII shows that
SAMU -XLSR trained with data re-balancing ( = 0:05) achieves
an average retrieval R@1 accuracy of 61.4% compared to
23.8% achieved by SAMU -XLSR trained on unbalanced training
set (= 1:0). Also, Table XIII shows that there is a negligible
performance difference for different s on X!EN tasks when
speech queries are in high-resource languages.
C. Training Data
In Section II-D1, we mention that we train SAMU -XLSR
with multilingual transcribed speech data collected from the
CoV o dataset. In this section, we study the effect of train-
ingSAMU -XLSR with paired speech-translation data. We train
SAMU -XLSR using three different training datasets: 1) Tran-
scribed multilingual speech in 25 languages from the CoV o
dataset, which we refer to as the training setup T1, and the
model trained with this setup as SAMU -XLSR T1, 2) The 22
X!EN CoV oST-2 [41] speech-translation training sets, where
speech utterances are paired with their corresponding English
text translations. We refer to that as the training setup T2,
and the model trained with this setup as SAMU -XLSR T2. 3) A
combination of both T1 and T2. We refer to the model trained
with this setup as SAMU -XLSR T3. Also, we re-balance the
different training datasets using = 0:05and then randomly
pick 400K examples for training. Finally, we train the model
for 100K iterations on 8 V100-32GB GPUs.
Table XIV shows average retrieval performance on 21
X!EN retrieval tasks achieved by SAMU -XLSR trained with the
three different training setups mentioned above. We observe
TABLE XVI: Avg. retrieval performance on 5 X !EN text
translation retrieval tasks for different training data. The
speech queries are in high-resource languages
Model R@5 [%] R@1 [%] WER [%]
SAMU -XLSR T1 92.0 85.0 9.4
SAMU -XLSR T2 91.9 84.9 9.2
SAMU -XLSR T3 92.3 85.7 8.7

--- PAGE 13 ---
13
that SAMU -XLSR T1 achieves the best retrieval performance
out of the three models, which implies that we can train
SAMU -XLSR with just multilingual transcribed speech. Further-
more, table XV shows that SAMU -XLSR T1 is notably better
for X!EN tasks when speech queries are in low-resource
languages. For speech queries in high-resource languages, the
performance difference among the three models is negligible.
See Table XVI for X !EN retrieval tasks, when speech queries
are in high-resource languages.
VII. C ONCLUSION
We proposed a semantically-aligned multimodal (joint
speech-text) utterance-level cross-lingual speech representa-
tion ( SAMU -XLSR ) learning framework in this work. We show
that just by using multilingual transcribed speech to train the
proposed representation learning model, cross-lingual align-
ments between speech utterances and their text and speech
translations emerge in the model‚Äôs learned embedding vector
space.
We show that unlike XLS-R(a speech-only multilingual
speech encoder), SAMU -XLSR in combination with language-
agnostic BERT sentence encoder LaBSE can perform zero-shot
speech-to-text and speech-to-speech translation retrieval across
several spoken and written languages. Furthermore, we show
thatSAMU -XLSR performs at par with XLS-Ron sequence-to-
sequence modeling tasks such as ASR and Phoneme Recog-
nition. In the future, we will extend our multimodal learn-
ing framework for the task of zero-shot speech translation
and large-scale speech-to-text data mining to create parallel
speech-text translation datasets for training speech translation
models.
ACKNOWLEDGMENTS
This work uses HPC resources of IDRIS under the allo-
cation AD011012527 made by GENCI. We thank Nauman
Dawalatabad and Yuan Gong from MIT CSAIL spoken lan-
guage systems lab for reviewing the paper and provide helpful
comments.
REFERENCES
[1] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, ‚Äúwav2vec 2.0: A
framework for self-supervised learning of speech representations,‚Äù arXiv
preprint arXiv:abs/2006.11477 , 2020.
[2] Y .-A. Chung and J. Glass, ‚ÄúGenerative pre-training for speech with
autoregressive predictive coding,‚Äù in ICASSP 2020-2020 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2020, pp. 3497‚Äì3501.
[3] A. H. Liu, Y .-A. Chung, and J. Glass, ‚ÄúNon-autoregressive predictive
coding for learning speech representations from local dependencies,‚Äù
2020. [Online]. Available: https://arxiv.org/abs/2011.00406
[4] S. Pascual, M. Ravanelli, J. Serr `a, A. Bonafonte, and Y . Bengio,
‚ÄúLearning problem-agnostic speech representations from multiple self-
supervised tasks,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1904.
03416
[5] S. Schneider, A. Baevski, R. Collobert, and M. Auli, ‚Äúwav2vec:
Unsupervised pre-training for speech recognition,‚Äù 2019. [Online].
Available: https://arxiv.org/abs/1904.05862
[6] S. Khurana, A. Laurent, W.-N. Hsu, J. Chorowski, A. Lancucki,
R. Marxer, and J. Glass, ‚ÄúA convolutional deep markov model for
unsupervised speech representation learning,‚Äù 2020. [Online]. Available:
https://arxiv.org/abs/2006.02547[7] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and
M. Auli, ‚ÄúUnsupervised cross-lingual representation learning for speech
recognition,‚Äù 2020. [Online]. Available: https://arxiv.org/abs/2006.13979
[8] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and
A. Mohamed, ‚ÄúHubert: Self-supervised speech representation learning
by masked prediction of hidden units,‚Äù IEEE/ACM Transactions on
Audio, Speech, and Language Processing , vol. 29, pp. 3451‚Äì3460, 2021.
[9] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh,
P. von Platen, Y . Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli,
‚ÄúXls-r: Self-supervised cross-lingual speech representation learning at
scale,‚Äù 2021. [Online]. Available: https://arxiv.org/abs/2111.09296
[10] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen, J. Li,
N. Kanda, T. Yoshioka, X. Xiao et al. , ‚ÄúWavlm: Large-scale self-
supervised pre-training for full stack speech processing,‚Äù arXiv preprint
arXiv:2110.13900 , 2021.
[11] Y .-A. Chung, Y . Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang,
and Y . Wu, ‚ÄúW2v-bert: Combining contrastive learning and masked
language modeling for self-supervised speech pre-training,‚Äù 2021.
[Online]. Available: https://arxiv.org/abs/2108.06209
[12] A. Bapna, C. Cherry, Y . Zhang, Y . Jia, M. Johnson, Y . Cheng,
S. Khanuja, J. Riesa, and A. Conneau, ‚Äúmslam: Massively multilingual
joint pre-training for speech and text,‚Äù arXiv preprint arXiv:2202.01374 ,
2022.
[13] H. Schwenk and M. Douze, ‚ÄúLearning joint multilingual sentence
representations with neural machine translation,‚Äù 2017. [Online].
Available: https://arxiv.org/abs/1704.04154
[14] M. Artetxe and H. Schwenk, ‚ÄúMassively multilingual sentence embed-
dings for zero-shot cross-lingual transfer and beyond,‚Äù Transactions of
the Association for Computational Linguistics , vol. 7, pp. 597‚Äì610,
nov 2019. [Online]. Available: https://doi.org/10.1162%2Ftacl a00288
[15] F. Feng, Y . Yang, D. Cer, N. Arivazhagan, and W. Wang, ‚ÄúLanguage-
agnostic bert sentence embedding,‚Äù 2020. [Online]. Available: https:
//arxiv.org/abs/2007.01852
[16] H. Schwenk, ‚ÄúFiltering and mining parallel data in a joint multilingual
space,‚Äù 2018. [Online]. Available: https://arxiv.org/abs/1805.09822
[17] H. Schwenk, V . Chaudhary, S. Sun, H. Gong, and F. Guzm ¬¥an,
‚ÄúWikimatrix: Mining 135m parallel sentences in 1620 language pairs
from wikipedia,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1907.
05791
[18] H. Schwenk, G. Wenzek, S. Edunov, E. Grave, and A. Joulin,
‚ÄúCcmatrix: Mining billions of high-quality parallel sentences on the
web,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1911.04944
[19] H. Schwenk and M. Douze, ‚ÄúLearning joint multilingual sentence
representations with neural machine translation,‚Äù 2017. [Online].
Available: https://arxiv.org/abs/1704.04154
[20] M. Artetxe and H. Schwenk, ‚ÄúMassively multilingual sentence embed-
dings for zero-shot cross-lingual transfer and beyond,‚Äù Transactions of
the Association for Computational Linguistics , vol. 7, p. 597‚Äì610, Nov
2019. [Online]. Available: http://dx.doi.org/10.1162/tacl a00288
[21] H. Schwenk, G. Wenzek, S. Edunov, E. Grave, and A. Joulin,
‚ÄúCcmatrix: Mining billions of high-quality parallel sentences on the
web,‚Äù 2019. [Online]. Available: https://arxiv.org/abs/1911.04944
[22] J. Gu, Y . Wang, K. Cho, and V . O. Li, ‚ÄúImproved zero-
shot neural machine translation via ignoring spurious correlations,‚Äù
inProceedings of the 57th Annual Meeting of the Association
for Computational Linguistics . Florence, Italy: Association for
Computational Linguistics, Jul. 2019, pp. 1258‚Äì1268. [Online].
Available: https://aclanthology.org/P19-1121
[23] N. Arivazhagan, A. Bapna, O. Firat, R. Aharoni, M. Johnson, and
W. Macherey, ‚ÄúThe missing ingredient in zero-shot neural machine
translation,‚Äù arXiv preprint arXiv:1903.07091 , 2019.
[24] P.-A. Duquenne, H. Gong, and H. Schwenk, ‚ÄúMultimodal and multi-
lingual embeddings for large-scale speech mining,‚Äù Advances in Neural
Information Processing Systems , vol. 34, 2021.
[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù 2017.
[Online]. Available: https://arxiv.org/abs/1706.03762
[26] P. Safari, M. India, and J. Hernando, ‚ÄúSelf-attention encoding and
pooling for speaker recognition,‚Äù arXiv preprint arXiv:2008.01077 ,
2020.
[27] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training
of deep bidirectional transformers for language understanding,‚Äù 2018.
[Online]. Available: https://arxiv.org/abs/1810.04805
[28] G. Lample and A. Conneau, ‚ÄúCross-lingual language model pretraining,‚Äù
arXiv preprint arXiv:1901.07291 , 2019.

--- PAGE 14 ---
14
[29] M. Schuster and K. Nakajima, ‚ÄúJapanese and korean voice search,‚Äù in
2012 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , 2012, pp. 5149‚Äì5152.
[30] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,
M. Krikun, Y . Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,
M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y . Kato, T. Kudo,
H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young,
J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes,
and J. Dean, ‚ÄúGoogle‚Äôs neural machine translation system: Bridging
the gap between human and machine translation,‚Äù 2016. [Online].
Available: https://arxiv.org/abs/1609.08144
[31] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,
P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer,
P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,
M. Drame, Q. Lhoest, and A. M. Rush, ‚ÄúHuggingface‚Äôs transformers:
State-of-the-art natural language processing,‚Äù 2019. [Online]. Available:
https://arxiv.org/abs/1910.03771
[32] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,
F. Guzm ¬¥an, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,
‚ÄúUnsupervised cross-lingual representation learning at scale,‚Äù 2019.
[Online]. Available: https://arxiv.org/abs/1911.02116
[33] G. Lample and A. Conneau, ‚ÄúCross-lingual language model pretraining,‚Äù
2019. [Online]. Available: https://arxiv.org/abs/1901.07291
[34] Y . Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad,
M. Lewis, and L. Zettlemoyer, ‚ÄúMultilingual denoising pre-training
for neural machine translation,‚Äù 2020. [Online]. Available: https:
//arxiv.org/abs/2001.08210
[35] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
and Q. V . Le, ‚ÄúSpecAugment: A simple data augmentation method for
automatic speech recognition,‚Äù arXiv preprint arXiv:1904.08779 , 2019.
[36] A. Graves, ‚ÄúSequence transduction with recurrent neural networks,‚Äù
arXiv preprint arXiv:abs/1211.3711 , 2012.
[37] D. Harwath, A. Torralba, and J. Glass, ‚ÄúUnsupervised learning of
spoken language with visual context,‚Äù Advances in Neural Information
Processing Systems , vol. 29, 2016.
[38] D. Harwath, W.-N. Hsu, and J. Glass, ‚ÄúLearning hierarchical discrete
linguistic units from visually-grounded speech,‚Äù in International
Conference on Learning Representations , 2020. [Online]. Available:
https://openreview.net/forum?id=B1elCp4KwH
[39] A. Rouditchenko, A. Boggust, D. Harwath, B. Chen, D. Joshi,
S. Thomas, K. Audhkhasi, H. Kuehne, R. Panda, R. Feris,
B. Kingsbury, M. Picheny, A. Torralba, and J. Glass, ‚ÄúAvlnet: Learning
audio-visual language representations from instructional videos,‚Äù 2020.
[Online]. Available: https://arxiv.org/abs/2006.09199
[40] Wikipedia contributors, ‚ÄúWord error rate ‚Äî Wikipedia, the free
encyclopedia,‚Äù 2020, [Online; accessed 23-April-2022]. [Online].
Available: https://en.wikipedia.org/w/index.php?title=Word error rate&
oldid=939575741
[41] C. Wang, J. Pino, A. Wu, and J. Gu, ‚ÄúCovost: A diverse multilingual
speech-to-text translation corpus,‚Äù 2020. [Online]. Available: https:
//arxiv.org/abs/2002.01320
[42] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi,
‚ÄúMuST-C: a Multilingual Speech Translation Corpus,‚Äù in Proceedings of
the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers) . Minneapolis, Minnesota: Association
for Computational Linguistics, Jun. 2019, pp. 2012‚Äì2017. [Online].
Available: https://aclanthology.org/N19-1202
[43] E. Salesky, M. Wiesner, J. Bremerman, R. Cattoni, M. Negri,
M. Turchi, D. W. Oard, and M. Post, ‚ÄúThe multilingual tedx corpus
for speech recognition and translation,‚Äù 2021. [Online]. Available:
https://arxiv.org/abs/2102.01757
[44] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza,
M. Williamson, J. Pino, and E. Dupoux, ‚ÄúVoxPopuli: A large-scale
multilingual speech corpus for representation learning, semi-supervised
learning and interpretation,‚Äù in Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) . Online: Association for
Computational Linguistics, Aug. 2021, pp. 993‚Äì1003. [Online].
Available: https://aclanthology.org/2021.acl-long.80
[45] M. Rivi `ere, A. Joulin, P.-E. Mazar ¬¥e, and E. Dupoux, ‚ÄúUnsupervised
pretraining transfers well across languages,‚Äù 2020. [Online]. Available:
https://arxiv.org/abs/2002.02848
[46] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y . Unno,
N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduch-intala, and T. Ochiai, ‚ÄúESPnet: End-to-end speech processing toolkit,‚Äù
inProc. Interspeech , Sep. 2018, pp. 2207‚Äì2211.
[47] S. Arora, S. Dalmia, P. Denisov, X. Chang, Y . Ueda, Y . Peng, Y . Zhang,
S. Kumar, K. Ganesan, B. Yan et al. , ‚ÄúEspnet-slu: Advancing spoken
language understanding through espnet,‚Äù in ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2022, pp. 7167‚Äì7171.
[48] A. Graves, S. Fern ¬¥andez, F. J. Gomez, and J. Schmidhuber, ‚ÄúConnection-
ist temporal classiÔ¨Åcation: Labelling unsegmented sequence data with
recurrent neural networks,‚Äù in Proc. ICML , Jun. 2006.
[49] S. Watanabe, F. Boyer, X. Chang, P. Guo, T. Hayashi, Y . Higuchi,
T. Hori, W.-C. Huang, H. Inaguma, N. Kamo et al. , ‚ÄúThe 2020 espnet up-
date: new features, broadened applications, performance improvements,
and future plans,‚Äù in 2021 IEEE Data Science and Learning Workshop
(DSLW) . IEEE, 2021, pp. 1‚Äì6.
[50] A. Ali, P. Bell, J. Glass, Y . Messaoui, H. Mubarak, S. Renals, and
Y . Zhang, ‚ÄúThe mgb-2 challenge: Arabic multi-dialect broadcast media
recognition,‚Äù 2016. [Online]. Available: https://arxiv.org/abs/1609.05625

--- PAGE 15 ---
15
TABLE XVII: Given a speech query in language X, we search over a large English database of 1.6M sentences to retrieve
the top-5 translations using our proposed SAMU -XLSR -LaBSE retrieval pipeline. We randomly pick Ô¨Åve speech queries from the
CoV oST-2 eval set, two in French, and one each in German, Arabic and Spanish. For each speech query, we retrieve the top-5
English translations.
Speech Query Query Lang. Top-5 Retrieved EN Translations
La chute de la cit ¬¥e est difÔ¨Åcile `a expliquer. FR 1) The fall of the city is difÔ¨Åcult to explain
2) The origin of the town name is unclear.
3) It‚Äôs not easy to describe why it happened.
4) Further history of the village is unclear.
5) The origin of the town is not completely clear.
Elle est le chef-lieu du d ¬¥epartement de l‚ÄôOkano. FR 1) It is the seat of Okanogan County.
2)It is the main city of the Okano District.
3) It is the county seat of Macon County.
4) It is the capital of Otwock County.
5) Its county seat is Oconto.
Die Bl ¬®utezeit reicht von M ¬®arz und April DE 1) The Ô¨Çowering season lasts from March
vor der Bildung der Laubbl ¬®atter. until April, just before foliage develops.
2) The Ô¨Çowering period extends from April through June.
3) Flowering occurs from April through July.
4) Its Ô¨Çowering season is around February to April.
5) The blooming starts in the middle of April
and goes almost until mid May.
√±K
Y¬™K.A√ì√±K
BA√îg.X@X	QK. AR 1) She‚Äôs getting worse every day.
2) It is getting better every day.
3) It‚Äôs getting warmer day after day.
4)She gets prettier every day.
5) It‚Äôs getting colder day after day.
Fue enfermera voluntaria en la I Guerra Mundial. ES 1) She was a volunteer nurse on World War I.
2) Her mother was a nurse during World War One.
3) During World War One he served as a paramedic.
4) During World War One he was a medical sergeant
5) In World War One, she was a Red Cross nurse.

--- PAGE 16 ---
16
Fig. 5: We extract the representation sequence from a Pre-trained SAMU -XLSR (our proposed model) from before the attention
pooling layer. Next, we compute the cosine similarity between the adjacent feature vectors to compute a sequence of distances
and use a peak Ô¨Ånding algorithm to detect the local peaks. After tuning the peak threshold in the peak Ô¨Ånding algorithm, we
observe that the peaks correspond to the underlying word boundaries.

