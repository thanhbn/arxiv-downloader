Ngữ nghĩa nhận thức chú ý cải thiện dịch máy thần kinh
Aviv Slobodkin Leshem Choshen Omri Abend
Trường Khoa học máy tính và Kỹ thuật
Đại học Hebrew Jerusalem
{aviv.slobodkin,leshem.choshen,omri.abend}@mail.huji.ac.il

Tóm tắt
Việc tích hợp cấu trúc cú pháp vào dịch máy Transformer đã cho thấy kết quả tích cực, nhưng theo hiểu biết của chúng tôi, chưa có công trình nào thử nghiệm với cấu trúc ngữ nghĩa. Trong nghiên cứu này, chúng tôi đề xuất hai phương pháp mới không có tham số để tiêm thông tin ngữ nghĩa vào Transformers, cả hai đều dựa trên việc che chắn nhận thức ngữ nghĩa của (một số) đầu chú ý. Một phương pháp như vậy hoạt động trên bộ mã hóa, thông qua đầu Chú ý Tự động Nhận thức Cảnh (SASA). Phương pháp khác trên bộ giải mã, thông qua đầu Chú ý Chéo Nhận thức Cảnh (SACrA). Chúng tôi cho thấy sự cải thiện nhất quán so với Transformer thuần túy và các mô hình nhận thức cú pháp cho bốn cặp ngôn ngữ. Chúng tôi cũng cho thấy lợi ích bổ sung khi sử dụng cả cấu trúc ngữ nghĩa và cú pháp trong một số cặp ngôn ngữ.

1 Giới thiệu
Từ lâu, người ta đã lập luận rằng biểu diễn ngữ nghĩa có thể có lợi cho dịch máy (Weaver, 1955; Bar-Hillel, 1960). Hơn nữa, dịch máy thần kinh (NMT) dựa trên RNN đã được chứng minh là có lợi từ việc tiêm cấu trúc ngữ nghĩa (Song et al., 2019; Marcheggiani et al., 2018). Bất chấp những lợi ích này, theo hiểu biết của chúng tôi, chưa có nỗ lực nào để tích hợp cấu trúc ngữ nghĩa vào NMT Transformers (Vaswani et al., 2017). Chúng tôi giải quyết khoảng trống này, tập trung vào các sự kiện chính trong văn bản, được biểu diễn bởi UCCA (Chú thích Khái niệm Nhận thức Phổ quát; Abend và Rappoport, 2013), cụ thể là các cảnh.

UCCA là một khung ngữ nghĩa có nguồn gốc từ các lý thuyết loại hình học và ngôn ngữ học nhận thức (Dixon, 2009, 2010, 2012). Mục tiêu chính của nó là biểu diễn một số yếu tố chính của cấu trúc ngữ nghĩa của câu trong khi bỏ qua cú pháp của nó. Chính thức, biểu diễn UCCA của một đoạn văn là một đồ thị có hướng không chu trình trong đó các lá tương ứng với các từ của câu và các nút tương ứng với các đơn vị ngữ nghĩa. Các cạnh được gán nhãn bằng vai trò của điểm cuối trong quan hệ tương ứng với điểm khởi đầu của chúng (xem Hình 1). Một trong những động lực sử dụng UCCA là khả năng tách câu thành "Cảnh", tương tự như các sự kiện (xem Hình 1). Mỗi Cảnh như vậy bao gồm một quan hệ chính, có thể là Quá trình (tức là hành động), ký hiệu bằng P, hoặc Trạng thái (tức là trạng thái liên tục), ký hiệu bằng S. Các cảnh cũng chứa ít nhất một Tham gia (tức là thực thể), ký hiệu bằng A. Ví dụ, câu trong Hình 1a bao gồm hai cảnh: cảnh đầu tiên có Quá trình "saw" và hai Tham gia – "I" và "the dog"; cảnh thứ hai có Quá trình "barked" và một Tham gia – "dog".

Cho đến nay, theo hiểu biết của chúng tôi, công trình nhận thức cấu trúc duy nhất đã tích hợp kiến thức ngôn ngữ học và cấu trúc đồ thị vào Transformers sử dụng cấu trúc cú pháp (Strubell et al., 2018; Bugliarello và Okazaki, 2020; Akoury et al., 2019; Sundararaman et al., 2019; Choshen và Abend, 2021, và các tác giả khác). Phương pháp được trình bày dựa trên phương pháp do Bugliarello và Okazaki (2020) đề xuất, sử dụng đồ thị Universal Dependencies (UD; Nivre et al., 2016) của câu nguồn để tập trung chú ý của bộ mã hóa vào cha của mỗi token, cụ thể là tổ tiên trực tiếp của token trong đồ thị UD. Tương tự, chúng tôi sử dụng đồ thị UCCA của câu nguồn để tạo mặt nạ nhận thức cảnh cho các đầu tự chú ý của bộ mã hóa. Chúng tôi gọi phương pháp này là SASA (xem §2.1).

Chúng tôi thử nghiệm mô hình (§2) trên việc dịch tiếng Anh sang bốn ngôn ngữ. Hai ngôn ngữ có cú pháp tương tự hơn với tiếng Anh (Nikolaev et al., 2020; Dryer và Haspelmath, 2013): tiếng Đức (En-De), tiếng Nga (En-Ru), và hai ngôn ngữ ít tương tự hơn: tiếng Thổ Nhĩ Kỳ (En-Tr) và tiếng Phần Lan (En-Fi). Chúng tôi chọn các cặp ngôn ngữ này vì thuộc tính ngữ pháp đa dạng và sự sẵn có của bộ dữ liệu song song đáng tin cậy cho mỗi ngôn ngữ trong benchmark WMT. Chúng tôi tìm thấy sự cải thiện nhất quán trên nhiều bộ thử nghiệm cho cả bốn trường hợp.

Ngoài ra, chúng tôi tạo ra một biến thể cú pháp của mô hình ngữ nghĩa để so sánh tốt hơn. Chúng tôi quan sát thấy rằng trung bình, mô hình nhận thức ngữ nghĩa vượt trội hơn các mô hình cú pháp. Hơn nữa, đối với hai ngôn ngữ ít tương tự với tiếng Anh (En-Tr và En-Fi), việc kết hợp cả dữ liệu ngữ nghĩa và cú pháp mang lại lợi ích thêm. Mặc dù các cải thiện thường nhỏ, đôi khi phiên bản kết hợp vượt trội SASA và UDISCAL (biến thể cú pháp của chúng tôi, xem §3) lần lượt 0.52 và 0.69 điểm BLEU (hoặc 0.46 và 0.43 chrF).

Chúng tôi cũng đề xuất một phương pháp mới để đưa thông tin đồ thị nguồn vào giai đoạn giải mã, cụ thể là thông qua lớp chú ý chéo trong bộ giải mã (xem §2.2). Chúng tôi tìm thấy rằng nó cải thiện so với các mô hình cơ sở và cú pháp, mặc dù SASA thường tốt hơn. Thú vị là, đối với En-Fi, mô hình này cũng vượt trội SASA, cho thấy một số cặp ngôn ngữ có thể hưởng lợi nhiều hơn từ việc tiêm ngữ nghĩa vào bộ giải mã.

Nhìn chung, thông qua một loạt thí nghiệm (xem §4), chúng tôi cho thấy tiềm năng của ngữ nghĩa như một hỗ trợ cho NMT. Chúng tôi thử nghiệm với một tập hợp lớn các biến thể của phương pháp, để xem chúng giúp ích tốt nhất ở đâu và bằng phương pháp tích hợp nào. Cuối cùng, chúng tôi cho thấy các mô hình ngữ nghĩa vượt trội các cơ sở UD và có thể bổ sung cho chúng trong các ngôn ngữ xa, thể hiện cải thiện khi kết hợp.

2 Các mô hình
Transformers đã được chứng minh là gặp khó khăn khi dịch một số loại phụ thuộc tầm xa (Choshen và Abend, 2019; Bisazza et al., 2021a), và khi đối mặt với thứ tự từ bất thường (Bisazza et al., 2021b). Sulem et al. (2018a) đề xuất tiền xử lý dựa trên UCCA tại thời điểm suy luận, tách câu thành các cảnh khác nhau. Họ giả thuyết rằng các mô hình cần phân tách đầu vào thành các cảnh một cách ngầm định, và cung cấp cho chúng sự phân tách như vậy, cũng như câu gốc. Họ cho thấy rằng điều này có thể tạo thuận lợi cho dịch máy (Sulem et al., 2020) và đơn giản hóa câu (Sulem et al., 2018b) trong một số trường hợp.

Được thúc đẩy bởi những tiến bộ này, chúng tôi tích hợp UCCA để tách nguồn thành các cảnh. Tuy nhiên, không giống như Sulem et al., chúng tôi không thay đổi độ dài câu trong tiền xử lý, vì phương pháp này cho phép ít linh hoạt hơn trong cách thông tin được truyền, và vì các kết quả sơ bộ trong việc tái triển khai phương pháp này mang lại kết quả kém hơn (xem §A.5). Thay vào đó, chúng tôi điều tra các cách để tích hợp sự phân tách vào kiến trúc chú ý.

Chúng tôi theo công trình trước đây (Bugliarello và Okazaki, 2020) trong cách tích hợp thông tin ngữ nghĩa. Trong bài báo của họ, Bugliarello và Okazaki (2020) đã giới thiệu cú pháp dưới dạng mặt nạ nhận thức cha, được áp dụng trước lớp softmax trong tự chú ý của bộ mã hóa. Chúng tôi che mặt nạ theo phương pháp tương tự để giới thiệu ngữ nghĩa. Tuy nhiên, cha trong khung UCCA là một khái niệm khó nắm bắt, vì các nút có thể có nhiều cha. Do đó, chúng tôi sử dụng một cách khác để biểu đạt thông tin ngữ nghĩa trong mặt nạ, tức là chúng tôi làm cho nó nhận thức cảnh, thay vì nhận thức cha.

Theo Sulem et al. (2018b), chúng tôi chia câu nguồn thành các cảnh, sử dụng phân tích UCCA của câu. Sau đó chúng tôi định nghĩa mặt nạ Nhận thức Cảnh:

MC[i;j] = {1, nếu i,j trong cùng một cảnh; 0, ngược lại} (1)

Trực quan, một đầu chú ý được che mặt nạ theo cách này được phép chú ý đến các token khác, miễn là chúng chia sẻ một cảnh với token hiện tại. Hình 1 minh họa hai ví dụ về những mặt nạ như vậy, đi kèm với đồ thị phân tích UCCA và sự phân đoạn thành Cảnh mà từ đó những mặt nạ này được tạo ra.

Mô hình cơ sở của chúng tôi là Transformer (Vaswani et al., 2017), mà chúng tôi cải thiện bằng cách làm cho các lớp chú ý nhận thức cảnh hơn. Chúng tôi buộc một trong những đầu chú ý đến các từ trong cùng một cảnh mà chúng tôi giả định có nhiều khả năng liên quan hơn so với các từ từ các cảnh khác nhau. Khi chúng tôi thay thế các đầu tự chú ý thường xuyên bằng các đầu nhận thức cảnh, chúng tôi duy trì cùng số lượng đầu và lớp như trong cơ sở.

2.1 Chú ý Tự động Nhận thức Cảnh (SASA)
Hình 2 trình bày kiến trúc của mô hình. Đối với một câu nguồn có độ dài L, chúng tôi thu được các ma trận khóa, truy vấn và giá trị ký hiệu bằng Ki;Qi;Vi∈RL×d, tương ứng. Sau đó, để có được ma trận đầu ra Oi∈RL×d, chúng tôi thực hiện các tính toán sau:

Si = Softmax(Qi(Ki)T/√dk) (2)
Oi = Si⊙MiS⊙Vi (3)

Trong đó 1/√dk là hệ số tỷ lệ, softmax trong phương trình 2 được thực hiện theo từng phần tử, MiS∈{0,1}L×L là mặt nạ nhận thức cảnh được tạo trước và ⊙ trong phương trình 3 biểu thị phép nhân theo từng phần tử. Sự khác biệt giữa phương pháp của chúng tôi và Transformer thuần túy (Vaswani et al., 2017) nằm ở phương trình 3, với phép nhân theo từng phần tử giữa MiS và Si, vốn không có trong Transformer thuần túy (phần còn lại giống nhau).

2.2 Chú ý Chéo Nhận thức Cảnh (SACrA)
Tiếp theo, chúng tôi thiết kế một mô hình trong đó chúng tôi tích hợp thông tin về cấu trúc cảnh thông qua lớp chú ý chéo trong bộ giải mã (xem Hình 3). Do đó, thay vì ảnh hưởng đến việc mã hóa tổng thể của nguồn, chúng tôi đưa các sự phân tách lên trước để hỗ trợ việc chọn token tiếp theo.

Chính thức, đối với một câu nguồn có độ dài Lsrc và câu đích có độ dài Ltrg, chúng tôi tính toán cho mỗi đầu các ma trận truy vấn và giá trị, ký hiệu bằng Qi∈RLtrg×dmodel và Vi∈RLsrc×d, tương ứng. Về giá trị khóa, ký hiệu bằng K̃i∈RLsrc×Ltrg, chúng tôi tính toán chúng như sau:

K̃i = (Xienc)T⊙MiS⊙1/Lsrc (4)

trong đó Xienc∈RLsrc×dmodel là đầu ra của bộ mã hóa và MS∈{0,1}Lsrc×Lsrc là mặt nạ được tạo trước.

Cuối cùng, chúng tôi truyền Vi;Qi và K̃i qua một lớp chú ý thường xuyên, như với kiến trúc Transformer tiêu chuẩn.

Ma trận Khóa Nhận thức Cảnh. Lý do đằng sau cách chúng tôi tính toán ma trận khóa nhận thức cảnh nằm ở vai trò của ma trận khóa trong một lớp chú ý. Trong lớp chú ý chéo, các truy vấn đến từ bộ giải mã. Thông tin ngữ cảnh phía nguồn được mã hóa trong các khóa, đến từ bộ mã hóa. Do đó, khi chúng tôi gán cùng mặt nạ cảnh cho tất cả các từ được bao gồm trong cùng một tập hợp cảnh, các giá trị khóa cho những từ này sẽ giống nhau, và do đó chúng sẽ được truy vấn xử lý tương tự. Kết quả là, truy vấn sẽ cho trọng số giống nhau cho các token nguồn chia sẻ cùng tập hợp cảnh. Do đó, một cảnh hoàn chỉnh (hoặc một vài cảnh), thay vì các token cụ thể (như với Transformer thuần túy), sẽ ảnh hưởng đến token được tạo tiếp theo, điều này sẽ mang lại quá trình giải mã nhận thức cảnh hơn.

3 Thiết lập thí nghiệm
Chuẩn bị dữ liệu. Đầu tiên, chúng tôi bỏ escape các ký tự HTML và tokenize tất cả các corpus song song (Koehn et al., 2007). Tiếp theo, chúng tôi loại bỏ các câu trống, câu dài hơn 100 token (hoặc phía nguồn hoặc phía đích), câu có tỷ lệ nguồn-đích lớn hơn 1.5, câu không khớp với ngôn ngữ của corpus như được xác định bởi langid Lui và Baldwin, 2012, và câu mà fast align (Dyer et al., 2013) coi là không có khả năng căn chỉnh (điểm căn chỉnh tối thiểu là -180). Sau đó, đối với các ngôn ngữ có viết hoa, chúng tôi huấn luyện các mô hình true-casing trên tập huấn luyện (Koehn et al., 2007) và áp dụng chúng cho tất cả đầu vào của mạng. Cuối cùng, chúng tôi huấn luyện một mô hình BPE (Sennrich et al., 2016), chung cho các cặp ngôn ngữ có hệ thống viết tương tự (ví dụ: Latin, Cyrillic, v.v.) và riêng biệt ngược lại, rồi áp dụng chúng tương ứng.

Chúng tôi huấn luyện mô hình trên toàn bộ bộ dữ liệu WMT16 cho nhiệm vụ tiếng Anh→tiếng Đức (En-De), sử dụng WMT newstest2013 như tập phát triển. Chúng tôi cũng huấn luyện các mô hình trên tập huấn luyện bao gồm Yandex Corpus, News Commentary v15, và Wikititles v2 cho nhiệm vụ tiếng Anh→tiếng Nga (En-Ru). Ngoài ra, chúng tôi huấn luyện các mô hình trên toàn bộ bộ dữ liệu WMT19 (loại trừ ParaCrawl, để tránh nhiễu trong dữ liệu) cho tiếng Anh→tiếng Phần Lan (En-Fi). Cuối cùng, chúng tôi huấn luyện trên toàn bộ bộ dữ liệu WMT18 cho nhiệm vụ tiếng Anh→tiếng Thổ Nhĩ Kỳ (En-Tr). Đối với các tập thử nghiệm, chúng tôi sử dụng tất cả newstests có sẵn cho mỗi cặp ngôn ngữ từ năm 2012, loại trừ tập được chỉ định cho phát triển.

Các mô hình. Các siêu tham số được chia sẻ bởi tất cả các mô hình được mô tả trong §3. Chúng tôi điều chỉnh số lượng đầu mà chúng tôi áp dụng mặt nạ (#heads) và các lớp của bộ mã hóa mà chúng tôi áp dụng SASA (layer), sử dụng tập phát triển En-De. Chúng tôi bắt đầu với điều chỉnh các lớp cho SASA, mà chúng tôi tìm thấy là layer = 4, và sau đó chúng tôi điều chỉnh #heads (trong khi cố định layer = 4), và nhận được #head = 1. Chúng tôi cũng sử dụng tập phát triển En-De để điều chỉnh #heads và các lớp của mô hình SACrA theo cách tương tự, cụ thể là đầu tiên các lớp và sau đó #heads (với các lớp được điều chỉnh cố định). Chúng tôi tìm thấy các siêu tham số tốt nhất là #heads = 1 và layers = 2&3. Đối với cả hai mô hình, chúng tôi áp dụng các siêu tham số đã điều chỉnh cho tất cả các cặp ngôn ngữ khác. Thú vị là, mặc dù việc thay đổi tất cả các lớp của mô hình là thực hành phổ biến, chúng tôi thấy nó không tối ưu. Hơn nữa, thực tế là thông tin ngữ nghĩa có lợi hơn ở các lớp cao hơn, trái ngược với thông tin cú pháp hữu ích nhất khi được giới thiệu ở các lớp thấp hơn (xem §3) có thể gợi ý rằng ngữ nghĩa liên quan đến khái quát hóa phức tạp hơn, điều này gợi nhớ đến các phát hiện của công trình trước đây (Tenney et al., 2019a; Belinkov, 2018; Tenney et al., 2019b; Peters et al., 2018; Blevins et al., 2018; Slobodkin et al., 2021).

Các phân tích UCCA được trích xuất sử dụng mô hình TUPA dựa trên BERT được huấn luyện trước, đã được huấn luyện trên các câu tiếng Anh, Đức và Pháp (Hershcovich et al., 2017).

Mặt nạ nhị phân. Đối với mô hình SASA, chúng tôi thử nghiệm với hai loại mặt nạ: mặt nạ nhị phân, như được mô tả trong §2, và mặt nạ tỷ lệ, tức là

MC[i;j] = {1, nếu i,j trong cùng một cảnh; C, ngược lại} (5)

trong đó C∈(0;1). Bằng cách đó, chúng tôi cho phép một số thông tin ngoài cảnh truyền qua, trong khi vẫn nhấn mạnh thông tin trong cảnh (bằng cách giữ giá trị M cho các token cùng cảnh ở 1). Để điều chỉnh C, chúng tôi thực hiện tìm kiếm lưới nhỏ trên C∈{0:05;0:1;0:15;0:2;0:3;0:5}.

Ngoài ra, tương tự như Bugliarello và Okazaki (2020), chúng tôi thử nghiệm mặt nạ phân phối chuẩn, theo phương trình sau:

Mi,j = fnorm(x=C×dist(i;j)) (6)

trong đó fnorm là hàm mật độ của phân phối chuẩn:

fnorm(x) = 1/√(2π) × e^(-x²/2) (7)

Chúng tôi định nghĩa một đồ thị cảnh trong đó các nút là các cảnh và các cạnh được vẽ giữa các cảnh có từ trùng lặp. dist(i;j) là khoảng cách ngắn nhất giữa các token i và j. σ=1/√2, để đảm bảo giá trị của M là 1 cho các từ chia sẻ một cảnh (dist(i;j)=0), và C là một siêu tham số, được xác định thông qua tìm kiếm lưới trên C∈{0:1;0:2;0:5;√0:5}. Đối với mỗi phiên bản tỷ lệ của mặt nạ, chúng tôi chọn mặt nạ có hiệu suất tốt nhất và so sánh nó với mặt nạ nhị phân (xem 1). Chúng tôi thấy rằng không có phiên bản nào vượt trội mặt nạ nhị phân. Do đó, chúng tôi báo cáo phần còn lại của các thí nghiệm với mặt nạ nhị phân.

Các cơ sở. Chúng tôi so sánh mô hình với một vài mô hình khác:
• Transformer. Mô hình NMT dựa trên Transformer tiêu chuẩn, sử dụng các siêu tham số tiêu chuẩn, như được mô tả trong §3.
• PASCAL. Theo Bugliarello và Okazaki (2020), chúng tôi tạo mặt nạ cú pháp cho lớp tự chú ý trong bộ mã hóa. Chúng tôi trích xuất đồ thị UD (Nivre et al., 2016) với udpipe (Straka và Straková, 2017). Giá trị của các mục của mặt nạ bằng (xem phương trình 7):

Mpt;j = fnorm(x=σ×(j-pt)) (8)

với σ=1 và pt là vị trí giữa của cha của token thứ t trong đồ thị UD của câu.

Chúng tôi sử dụng cùng siêu tham số chung như trong cơ sở Transformer. Ngoài ra, theo việc điều chỉnh của Bugliarello và Okazaki (2020), chúng tôi áp dụng mặt nạ PASCAL cho năm đầu của lớp chú ý đầu tiên của bộ mã hóa, nhưng không giống như bài báo gốc, chúng tôi áp dụng nó sau softmax của lớp, vì nó mang lại kết quả tốt hơn và cũng giống với hành động của mô hình chúng tôi.

• UDISCAL. Trong nỗ lực cải thiện mô hình PASCAL, chúng tôi tạo mặt nạ thay vì chỉ nhạy cảm với cha phụ thuộc, nhạy cảm với tất cả các quan hệ UD trong câu. Chúng tôi ký hiệu nó là mặt nạ UD-Distance-Scaled (UDISCAL). Cụ thể, để tính toán mặt nạ, chúng tôi sử dụng phương trình tương tự như của PASCAL, với một sự thay đổi nhỏ:

Mi;j = fnorm(x=σ×dist(i;j)) (9)

Trong đó σ=1, và dist(i;j) được định nghĩa là khoảng cách giữa token i và token j trong đồ thị UD của câu trong khi coi đồ thị là không có hướng. Như với lớp PASCAL, chúng tôi áp dụng mặt nạ UD-scaled sau lớp softmax. Nhưng, không giống như đầu PASCAL, chúng tôi điều chỉnh các siêu tham số của kiến trúc chỉ là một đầu của lớp đầu tiên, sau khi thực hiện tìm kiếm lưới nhỏ, cụ thể là thử nghiệm với tất cả các lớp l∈[1;4], và sau đó với #head∈[1;5].

Chi tiết huấn luyện. Tất cả các mô hình của chúng tôi đều dựa trên mô hình NMT dựa trên Transformer tiêu chuẩn (Vaswani et al., 2017), với 4000 bước khởi động. Ngoài ra, chúng tôi sử dụng biểu diễn token nội bộ kích thước 256, hàm mất mát cross-entropy theo token, làm mềm nhãn với ls=0:1 (Szegedy et al., 2016), bộ tối ưu hóa Adam, hệ số Adam β1=0:9 và β2=0:98, và Adam ε=1e-8. Hơn nữa, chúng tôi kết hợp 4 lớp trong bộ mã hóa và 4 trong bộ giải mã, và chúng tôi sử dụng tìm kiếm chùm trong quá trình suy luận, với kích thước chùm 4 và hệ số chuẩn hóa α=0:6. Ngoài ra, chúng tôi sử dụng kích thước lô 128 câu cho việc huấn luyện. Chúng tôi sử dụng chrF++.py với 1 từ và beta của 3 để có được điểm chrF+ (Popovic, 2017) như trong WMT19 (Ma et al., 2019) và BLEU detokenized (Papineni et al., 2002) được triển khai trong Moses. Chúng tôi sử dụng bộ công cụ Nematus (Sennrich et al., 2017), và chúng tôi huấn luyện tất cả các mô hình trên 4 GPU NVIDIA trong 150K bước. Thời gian huấn luyện trung bình cho Transformer thuần túy là 21.8 giờ, và thời gian huấn luyện trung bình cho mô hình SASA là 26.5 giờ.

4 Các thí nghiệm
Chúng tôi giả thuyết rằng các mô hình NMT có thể được hưởng lợi từ việc giới thiệu cấu trúc ngữ nghĩa, và trình bày một tập hợp các thí nghiệm hỗ trợ giả thuyết này sử dụng các phương pháp được trình bày ở trên.

4.1 Chú ý Tự động Nhận thức Cảnh
Chúng tôi thấy rằng trung bình, SASA vượt trội Transformer cho tất cả bốn cặp ngôn ngữ (xem 3), đôi khi có lợi ích lớn hơn 1 điểm BLEU. Hơn nữa, chúng tôi đánh giá tính nhất quán của lợi ích của SASA, sử dụng kiểm tra dấu, và nhận được giá trị p nhỏ hơn 0.01, do đó thể hiện sự cải thiện có ý nghĩa thống kê (xem §A.4). Chúng tôi thấy xu hướng tương tự khi đánh giá hiệu suất sử dụng metric chrF (xem §A.2), điều này càng làm nổi bật lợi ích nhất quán của mô hình.

Chúng tôi cũng đánh giá hiệu suất của mô hình trên các câu có phụ thuộc dài (xem A.3), được tìm thấy là thách thức đối với Transformers (Choshen và Abend, 2019). Chúng tôi giả định rằng những trường hợp như vậy có thể được hưởng lợi rất nhiều từ việc giới thiệu ngữ nghĩa. Trái ngược với giả thuyết, chúng tôi thấy lợi ích chỉ lớn hơn một chút so với trường hợp chung, điều này dẫn chúng tôi kết luận rằng những cải thiện mà chúng tôi thấy không đặc biệt bắt nguồn từ thách thức cú pháp. Tuy nhiên, chúng tôi vẫn quan sát thấy sự cải thiện nhất quán, với lợi ích lên đến 1.41 điểm BLEU, điều này càng nhấn mạnh tính ưu việt của mô hình so với mô hình cơ sở.

Phân tích định tính. Bảng 2 trình bày một vài ví dụ trong đó Transformer cơ sở sai, trong khi mô hình của chúng tôi dịch đúng (xem §A.6 cho các phân tích UCCA của các ví dụ). Trong ví dụ đầu tiên, Transformer dịch từ "show" như một động từ, tức là to show, thay vì như một danh từ. Trong ví dụ thứ hai, mô hình cơ sở mắc hai lỗi: nó hiểu sai từ "look forward to" thành "look at", và nó cũng dịch nó như một động từ thì hiện tại thay vì quá khứ. Ví dụ thứ ba đặc biệt thú vị, vì nó làm nổi bật sức mạnh của mô hình. Trong ví dụ này, Transformer mắc hai lỗi: đầu tiên, nó dịch phần "play with (someone) in the yard" thành "play with the yard". Tiếp theo, nó gán mệnh đề mô tả "which never got out" cho yard, thay vì children. Có vẻ như việc giới thiệu thông tin về cấu trúc cảnh vào mô hình tạo thuận lợi cho việc dịch, vì nó cả nhóm từ "kids" với cụm từ "I used to play with in the yard", và nó cũng tách "never got out" khỏi từ "yard". Thay vào đó, nó nhóm từ sau với "kids", do đó làm nổi bật các quan hệ giữa các từ trong câu. Nói chung, tất cả những ví dụ này là các trường hợp mà mạng thành công trong việc phân biệt một từ trong ngữ cảnh của nó.

4.2 So sánh với Mặt nạ Cú pháp
Tiếp theo, chúng tôi muốn so sánh mô hình với các cơ sở khác. Vì đây là công trình đầu tiên tích hợp thông tin ngữ nghĩa vào mô hình NMT dựa trên Transformer, chúng tôi so sánh công trình với các mô hình được tiêm cú pháp (như được mô tả trong §3): một là mô hình PASCAL (Bugliarello và Okazaki, 2020), và mô hình khác là sự thích ứng của chúng tôi với PASCAL, mô hình UD-Distance-Scaled (UDISCAL), giống với mặt nạ SASA của chúng tôi hơn. Chúng tôi thấy (Bảng 3) rằng trung bình, SASA vượt trội cả PASCAL và UDISCAL. Chúng tôi cũng so sánh SASA với mỗi mô hình cú pháp, thấy rằng nó có ý nghĩa tốt hơn (kiểm tra dấu p<0:01; xem §A.4). Điều này gợi ý rằng ngữ nghĩa có thể có lợi hơn cho Transformers so với cú pháp.

4.3 Kết hợp Cú pháp và Ngữ nghĩa
Đương nhiên, câu hỏi tiếp theo của chúng tôi là liệu việc kết hợp cả đầu ngữ nghĩa và cú pháp có tiếp tục cải thiện hiệu suất của mô hình hay không. Do đó, chúng tôi thử nghiệm sự kết hợp của SASA với hoặc PASCAL hoặc UDISCAL, giữ nguyên các siêu tham số được sử dụng cho các mô hình riêng biệt. Chúng tôi thấy rằng việc kết hợp với UDISCAL vượt trội hơn, và do đó chúng tôi tiếp tục với nó. Thú vị là, En-De và En-Ru hầu như không được hưởng lợi từ sự kết hợp so với chỉ mô hình SASA. Chúng tôi giả thuyết rằng điều này có thể do thực tế là cú pháp của mỗi cặp ngôn ngữ đã khá tương tự, và do đó mô hình chủ yếu dựa vào nó để tách câu mà UCCA cũng cung cấp. Mặt khác, En-Fi và En-Tr được hưởng lợi từ sự kết hợp, cả trung bình và trong hầu hết các tập thử nghiệm. Đánh giá hiệu suất sử dụng metric chrF (xem §A.2) mang lại hành vi tương tự, điều này càng khẳng định tính hợp lệ của nó. Điều này dẫn chúng tôi giả thuyết rằng các cặp ngôn ngữ có khoảng cách loại hình học xa hơn với nhau có thể được hưởng lợi nhiều hơn từ cả ngữ nghĩa và cú pháp; chúng tôi hoãn một cuộc thảo luận đầy đủ hơn về điểm này cho công trình tương lai. Để xác nhận rằng phiên bản kết hợp liên tục vượt trội mỗi phiên bản riêng biệt cho các ngôn ngữ có khoảng cách loại hình học xa, chúng tôi so sánh mỗi cặp sử dụng kiểm tra dấu (chỉ trên các tập thử nghiệm của En-Fi và En-Tr). Chúng tôi nhận được giá trị p là 0.02 cho so sánh với SASA và 0.0008 cho so sánh với UDISCAL. Điều này gợi ý rằng đối với những cặp ngôn ngữ này, thực sự có lợi ích đáng kể, mặc dù nhỏ, từ việc tiêm cả ngữ nghĩa và cú pháp.

4.4 Chú ý Chéo Nhận thức Cảnh
Sau phân tích về chú ý tự động nhận thức cảnh, chúng tôi muốn kiểm tra xem Transformers có thể được hưởng lợi từ việc tiêm ngữ nghĩa phía nguồn vào bộ giải mã hay không. Vì vậy, chúng tôi phát triển mô hình Chú ý Chéo Nhận thức Cảnh (SACrA), như được mô tả trong §2.2. Bảng 3 trình bày kết quả của SACrA, so sánh với cơ sở Transformer và SASA. Chúng tôi thấy rằng nói chung SASA vượt trội SACrA, gợi ý rằng ngữ nghĩa có lợi hơn trong quá trình mã hóa. Với việc đó, đối với ba trong bốn cặp ngôn ngữ, SACrA thực sự mang lại lợi ích so với Transformer, mặc dù nhỏ, và đối với một cặp ngôn ngữ (En-Fi) nó thậm chí vượt trội SASA trung bình. Hơn nữa, so sánh SACrA với Transformer sử dụng kiểm tra dấu (xem §A.4) cho thấy cải thiện có ý nghĩa (p=0:047).

Đáng ngạc nhiên, không giống như đối tác tự chú ý, việc kết hợp mô hình SACrA với UDISCAL dường như không có lợi chút nào, và trong hầu hết các trường hợp thậm chí bị vượt trội bởi Transformer cơ sở. Chúng tôi giả thuyết rằng điều này xảy ra vì việc chỉ định quá nhiều đầu cho việc tiêm ngôn ngữ học là không hiệu quả khi những đầu đó không thể tương tác trực tiếp với nhau, vì thông tin từ đầu UDISCAL chỉ đến đầu SACrA sau khi quá trình mã hóa hoàn thành. Một hướng có thể cho công trình tương lai là tìm cách làm giàu cú pháp cho bộ giải mã, và sau đó kết hợp nó với mô hình SACrA của chúng tôi.

5 Kết luận
Trong nghiên cứu này, chúng tôi đề xuất hai phương pháp mới để tiêm thông tin ngữ nghĩa vào mô hình NMT Transformer – một thông qua bộ mã hóa (tức là SASA) và một thông qua bộ giải mã (tức là SACrA). Sức mạnh của cả hai phương pháp là chúng không giới thiệu thêm tham số vào mô hình, và chỉ dựa vào các phân tích UCCA của các câu nguồn, được tạo ra trước bằng cách sử dụng một parser có sẵn, và do đó không tăng độ phức tạp của mô hình. Chúng tôi so sánh các phương pháp với các phương pháp tiêm cú pháp được phát triển trước đây, và với sự thích ứng của chúng tôi với những phương pháp này, và thấy rằng thông tin ngữ nghĩa có xu hướng có lợi hơn đáng kể so với thông tin cú pháp, chủ yếu khi được tiêm vào bộ mã hóa (SASA), nhưng đôi khi cũng trong quá trình giải mã (SACrA). Hơn nữa, chúng tôi thấy rằng đối với các ngôn ngữ khác biệt đủ, như tiếng Anh và tiếng Phần Lan hoặc tiếng Anh và tiếng Thổ Nhĩ Kỳ, việc kết hợp cả cấu trúc cú pháp và ngữ nghĩa tiếp tục cải thiện hiệu suất của các mô hình dịch. Công trình tương lai sẽ điều tra thêm về lợi ích của cấu trúc ngữ nghĩa trong Transformers, một mình và kết hợp với cấu trúc cú pháp.

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Israel (grant số 2424/21), và bởi Chương trình Nghiên cứu Ứng dụng trong Học viện của Cơ quan Đổi mới Israel.
