# Dịch máy thần kinh với từ phụ cấp byte

Changhan Wangy, Kyunghyun Choyz?và Jiatao Guy
yFacebook AI Research;zĐại học New York;?CIFAR Global Scholar
fchanghan, kyunghyuncho, jgu g@fb.com

## Tóm tắt

Gần như tất cả các mô hình dịch máy hiện tại đều được xây dựng dựa trên các từ vựng dựa trên ký tự: ký tự, từ phụ hoặc từ. Tuy nhiên, các ký tự hiếm từ văn bản nhiễu hoặc các ngôn ngữ giàu ký tự như tiếng Nhật và tiếng Trung có thể không cần thiết chiếm các vị trí từ vựng và hạn chế tính gọn gàng của nó. Việc biểu diễn văn bản ở cấp độ byte và sử dụng bộ 256 byte làm từ vựng là một giải pháp tiềm năng cho vấn đề này. Tuy nhiên, chi phí tính toán cao đã ngăn cản việc triển khai rộng rãi hoặc sử dụng trong thực tế. Trong bài báo này, chúng tôi nghiên cứu các từ phụ cấp byte, cụ thể là BPE cấp byte (BBPE), gọn gàng hơn từ vựng ký tự và không có token ngoài từ vựng, nhưng hiệu quả hơn việc chỉ sử dụng byte thuần túy. Chúng tôi cho rằng việc ngữ cảnh hóa các embedding BBPE là cần thiết, có thể được thực hiện bằng một lớp tích chập hoặc lớp hồi quy. Các thí nghiệm của chúng tôi cho thấy BBPE có hiệu suất tương đương với BPE trong khi kích thước của nó chỉ bằng 1/8 so với BPE. Trong bối cảnh đa ngôn ngữ, BBPE tối đa hóa việc chia sẻ từ vựng giữa nhiều ngôn ngữ và đạt được chất lượng dịch tốt hơn. Hơn nữa, chúng tôi chỉ ra rằng BBPE cho phép chuyển giao mô hình giữa các ngôn ngữ có bộ ký tự không trùng lặp.

## Giới thiệu

Việc sử dụng mã hóa cặp byte (BPE) để xây dựng từ vựng trong dịch máy thần kinh (NMT) đã trở thành một thực hành tiêu chuẩn. Trong thực hành này, chúng ta nhận thấy rằng BPE được sử dụng ở cấp độ ký tự thay vì ở cấp độ byte, điều này phổ biến hơn trong nén dữ liệu. Chúng tôi nghi ngờ điều này là do văn bản thường được biểu diễn một cách tự nhiên như một chuỗi ký tự, mặc dù gần đây người ta đã nhận thấy rằng biểu diễn byte của văn bản có những ưu điểm riêng, như tính gọn gàng (tối đa 256 giá trị có thể) và không phụ thuộc vào ngôn ngữ.

Trong bài báo này, chúng tôi xem xét các "từ phụ" cấp byte được sử dụng để tokenize văn bản thành các n-gram byte có độ dài biến đổi, trái ngược với các từ phụ cấp ký tự trong đó chúng ta biểu diễn văn bản như một chuỗi các n-gram ký tự. Chúng tôi tập trung cụ thể vào BPE cấp byte (BBPE), kiểm tra các từ vựng BBPE gọn gàng trong cả bối cảnh song ngữ và đa ngôn ngữ cũng như trong một thiết lập mới của học chuyển giao sang một ngôn ngữ mới với bộ ký tự không trùng lặp.

## Biểu diễn văn bản cấp byte

### Mã hóa biểu diễn cấp byte
Chúng tôi xem xét mã hóa UTF-8 của văn bản, mã hóa mỗi ký tự Unicode thành 1 đến 4 byte. Điều này cho phép chúng ta mô hình hóa một câu như một chuỗi byte thay vì ký tự. Trong khi có 138K ký tự Unicode bao phủ hơn 150 ngôn ngữ, chúng ta biểu diễn một câu trong bất kỳ ngôn ngữ nào như một chuỗi các byte UTF-8 (248 trong số 256 byte có thể).

Biểu diễn chuỗi byte của văn bản thường dài hơn nhiều (tối đa 4x) so với biểu diễn chuỗi ký tự, điều này làm cho việc sử dụng byte như chúng vốn có trở nên đòi hỏi tính toán cao. Như một giải pháp thay thế, chúng tôi xem xét việc phân đoạn một chuỗi byte thành các n-gram có độ dài biến đổi ("từ phụ" cấp byte). Cụ thể, chúng tôi học từ vựng BPE trên biểu diễn cấp byte mở rộng bộ byte UTF-8 với các n-gram byte. Chúng tôi ký hiệu loại từ vựng này là B(yte-level)BPE trong phần còn lại của bài báo. Hình 1 cho thấy một ví dụ về tokenization BBPE.

Các ký hiệu BBPE có thể là các ký tự một phần được chia sẻ bởi các ký tự khác nhau hoặc sự kết hợp của các ký tự hoàn chỉnh và một phần. Tính tùy ý này có thể đòi hỏi việc kết hợp một ngữ cảnh lớn hơn xung quanh mỗi ký hiệu để phân biệt và học các ranh giới ký tự. Trong công việc này, chúng tôi dựa các thí nghiệm của mình trên các mô hình Transformer. Chúng tôi đề xuất sử dụng một lớp tích chập theo chiều sâu hoặc một lớp hồi quy hai chiều với các đơn vị hồi quy có cổng (GRU) để ngữ cảnh hóa các embedding BBPE trước khi đưa chúng vào mô hình:

xctxemb=DepthWiseConv (xemb)
hoặc
xctxemb=BiGRU (xemb)

### Giải mã với từ phụ cấp byte
Trong khi bất kỳ câu nào cũng có thể được biểu diễn như một chuỗi byte, điều ngược lại tuy nhiên không nhất thiết đúng ở chỗ có những chuỗi byte không dịch thành các chuỗi ký tự hợp lệ. Về mặt thực nghiệm, chúng tôi thấy rằng các đầu ra không hợp lệ từ các mô hình đã được huấn luyện rất hiếm. Chúng tôi không quan sát thấy bất kỳ cái nào trong các thí nghiệm được mô tả dưới đây (lưu ý rằng một trong số chúng có một tập kiểm tra lớn gồm 165K ví dụ). Và một mẫu lỗi phổ biến trong các mô hình được huấn luyện một nửa là byte lặp lại dư thừa. Trong hệ thống của chúng tôi, chúng tôi cố gắng phục hồi càng nhiều ký tự Unicode càng tốt từ mẫu lỗi này một cách hiệu quả trong thời gian tuyến tính. Thuật toán như sau: Đối với một chuỗi byte cho trước {B_k}^N_{k=1}, chúng tôi ký hiệu số lượng ký tự tối đa mà chúng ta có thể phục hồi từ nó là f(k). Sau đó f(k) có cấu trúc phụ tối ưu và có thể được giải quyết bằng quy hoạch động:

f(k) = max_{t=1,2,3,4}{f(k-t) + g(k-t+1, k)} (1)

trong đó g(i, j) = 1 nếu {B_k}^j_{k=i} tương ứng với một ký tự hợp lệ, ngược lại là 0. Khi f(k) được tính toán đệ quy, chúng tôi cũng ghi lại các lựa chọn tại mỗi vị trí k để chúng ta có thể phục hồi giải pháp thông qua backtracking. Thiết kế của mã hóa UTF-8 đảm bảo tính duy nhất của quá trình phục hồi này: đối với một ký tự UTF-8 được mã hóa với nhiều byte, các byte theo sau sẽ không tạo thành một ký tự UTF-8 được mã hóa hợp lệ. Sau đó, lựa chọn tốt nhất trong Eq. 1 là duy nhất và giải pháp cuối cùng cũng vậy.

## Cài đặt thí nghiệm

### Bộ dữ liệu
Chúng tôi chạy thí nghiệm trên ba tập dữ liệu song ngữ cũng như một tập dữ liệu đa ngôn ngữ nhiều-sang-tiếng Anh:

**Tiếng Anh-Tiếng Đức (En-De)**: chúng tôi sao chép cùng cài đặt của Vaswani et al. (2017) sử dụng dữ liệu WMT 2014 (newstest13 cho validation và newstest14 cho testing)

**Tiếng Nhật-Tiếng Anh (Ja-En)**: chúng tôi theo Michel và Neubig (2018) và nối KFTT, TED và JESC để xây dựng tập huấn luyện, validation và test.

**Tiếng Sinhala-Tiếng Anh (Si-En)**: chúng tôi sử dụng dữ liệu từ FLoRes (Guzmán et al. 2019).

**Nhiều-sang-tiếng Anh (X-En)**: chúng tôi áp dụng tập dữ liệu TED Talks được biên dịch bởi Ye et al. (2018), bao gồm dữ liệu song song cho 59 ngôn ngữ. Cho các thí nghiệm của chúng tôi, chúng tôi sử dụng tiếng Anh làm target và 58 ngôn ngữ khác làm source. Chúng tôi lấy mẫu 22K ví dụ từ tập development 135K cho validation.

Bảng 1 cho thấy thống kê tổng quan của các tập dữ liệu này. Chúng tôi học từ vựng (B)BPE chung trên các câu source và target bằng SentencePiece.

### Mô hình và học tập
Chúng tôi sử dụng Fairseq để huấn luyện Transformers với cùng lịch trình tỷ lệ học như trong bài báo gốc. Tất cả các cấu hình mô hình được liệt kê trong bảng 2. Chúng tôi đặt attention và ReLU dropout thành 0.1, ngoại trừ Si-En mà chúng tôi sử dụng 0.2. Chúng tôi sử dụng 0.2 residual dropout cho các mô hình T_base trong X-En. Chúng tôi sử dụng kernel size 5 và padding 2 ở cả hai bên cho tất cả các lớp tích chập.

### Suy luận và đánh giá
Chúng tôi đặt beam width thành 4 cho En-De và 5 cho các tập khác và sử dụng checkpoint tốt nhất theo validation loss để tạo ra các dự đoán. Chúng tôi tính toán BLEU tokenized phân biệt chữ hoa chữ thường làm chỉ số bằng sacreBLEU.

## Kết quả và phân tích

### So sánh định tính: BPE vs. BBPE

#### Phân phối tần suất ký hiệu
Vì việc xây dựng từ vựng BBPE bắt đầu từ bộ byte UTF-8, nó có tính linh hoạt trong việc phân tách các ký tự hiếm thành các n-gram byte từ từ vựng thay vì bao gồm chúng trực tiếp. Điều này giải phóng các vị trí từ vựng cho các ký hiệu tần suất khác. Hình 2 so sánh phân phối tần suất ký hiệu của BPE và BBPE. Chúng ta có thể thấy rằng các ký hiệu BBPE được phân phối đều hơn so với BPE, ngay cả khi ký hiệu sau đã được phân phối đều hơn nhiều so với ký tự thuần túy.

Bằng cách đặt các kích thước từ vựng BBPE khác nhau, chúng ta có thể kiểm soát mức độ phân tách ký tự hiếm và chia sẻ ký hiệu giữa các ký tự khác nhau. Bảng 3 cho thấy tỷ lệ token BBPE với ký tự một phần. Chúng ta có thể thấy rằng một phần lớn ký tự hiếm được phân tách trên Ja-En và X-En, có bộ ký tự lớn lần lượt là 8K và 11K.

#### Chia sẻ đa ngôn ngữ
Trong bối cảnh đa ngôn ngữ, việc chia sẻ ký hiệu cũng xảy ra giữa các ngôn ngữ khác nhau bất chấp các hệ thống viết khác nhau. Điều này cho phép tối đa hóa việc chia sẻ tham số không chỉ cho phần mô hình mà còn cho phần từ vựng trong một mô hình chung. Hình 3 minh họa mức độ chia sẻ ký hiệu BBPE giữa top 5 ngôn ngữ (theo số lượng ví dụ huấn luyện) trong X-En có hệ thống viết khác nhau với nhau.

#### Tác động đến độ dài chuỗi
So với BPE, các ký hiệu BBPE thường có độ hạt mịn hơn với độ dài cấp byte ngắn hơn, dẫn đến các chuỗi được tokenize dài hơn cũng như thời gian huấn luyện và suy luận dài hơn. Tuy nhiên, BBPE được tối ưu hóa cho mục tiêu dựa trên nén (giống như BPE), và vẫn hiệu quả hơn từ vựng ký tự. Bảng 4 liệt kê độ dài trung bình của các câu huấn luyện được tokenize với các từ vựng khác nhau. Chúng ta có thể quan sát thấy rằng các câu được tokenize với BBPE có độ dài ngắn hơn đáng kể so với ký tự, ngay cả khi từ vựng BBPE nhỏ hơn nhiều (ví dụ chỉ 1/5 kích thước bộ ký tự trên X-En). Một quan sát khác là tỷ lệ độ dài source-target cho BBPE có xu hướng lớn hơn nhiều khi bộ ký tự source và bộ ký tự target có kích thước rất khác nhau (ví dụ 11K cho phía source X-En và 0.1K cho phía target).

### Tầm quan trọng của ngữ cảnh hóa
Chúng tôi so sánh ba cách khác nhau để ngữ cảnh hóa token embeddings: không có, tích chập 1 lớp và bi-GRU 1 lớp, trên X-En với mô hình T_base. Chúng tôi quan sát từ Hình 4 rằng tất cả các loại từ vựng đều có thể hưởng lợi từ ngữ cảnh hóa embedding. Tăng hiệu suất đáng kể hơn trên các từ vựng hạt mịn: byte, ký tự và BBPE. Đối với BBPE, thông tin ngữ cảnh tầm xa từ Bi-GRU mang lại hơn 4% tăng trên validation BLEU trong tất cả các trường hợp. Mã hóa ngữ cảnh trong token embeddings giảm khó khăn của việc học attention trên nhiều token source và làm cho huấn luyện mô hình dễ dàng hơn.

### BBPE trên các bộ ký tự nhiễu
Tập huấn luyện En-De có khá nhiều cặp câu nhiễu thường chứa một số bảng chữ cái không phải Latin do lỗi căn chỉnh và các câu code-switched. Điều này dẫn đến một bộ ký tự 3.4K, trong khi ngược lại, cả tiếng Anh và tiếng Đức đều có ít hơn 30 bảng chữ cái. Vì BPE bao gồm tất cả ký tự, những ký tự hiếm đó sẽ lãng phí khá nhiều vị trí từ vựng BPE. Để so sánh, chúng tôi thử với từ vựng BBPE nhỏ 2K và 4K nơi các ký tự hiếm bị loại trừ. Chúng tôi thấy rằng hiệu suất của chúng tương đương với baseline BPE 32K trong khi có dung lượng mô hình nhỏ hơn.

### BBPE trên các ngôn ngữ giàu ký tự
Các ngôn ngữ sử dụng hệ thống viết logographic, như tiếng Trung và tiếng Nhật, có thể có hơn 50K ký tự, trong khi chỉ một phần nhỏ trong số chúng được sử dụng thường xuyên. Bộ dữ liệu Ja-En của chúng tôi có một bộ 7.9K ký tự, trong đó 99.99% token trong tập huấn luyện được bao phủ bởi 2.4K ký tự hàng đầu. Với quan sát này, chúng tôi thí nghiệm với BBPE 4K là khoảng 50% kích thước bộ ký tự. Chúng tôi thấy rằng BBPE có thể so sánh với BPE và thậm chí vượt trội hơn BPE khi sử dụng mô hình T_big lớn hơn.

### BBPE trong dịch nhiều-sang-En
Bộ dữ liệu nhiều-sang-En của chúng tôi chứa 58 ngôn ngữ (song song với tiếng Anh) và 10.8K ký tự từ các hệ thống viết khác nhau, giữa chúng các ký tự không nhất thiết được chia sẻ. Tuy nhiên, các ký tự chia sẻ các n-gram byte. Chúng tôi thí nghiệm với BBPE 2K và 4K có kích thước 12.5% và 25% của từ vựng BPE baseline. Như được hiển thị trong Bảng 7, cả hai đều đánh bại baseline BPE trên BLEU tổng thể cũng như trên hầu hết các ngôn ngữ cả tài nguyên cao và thấp.

### Học chuyển giao trên các ký tự chưa thấy
Vì BBPE chứa tất cả byte UTF-8 và không có token ngoài từ vựng, các mô hình dựa trên BBPE có thể được chuyển giao giữa các ngôn ngữ với các bộ ký tự không trùng lặp. Ngược lại, điều này là không thể với các từ vựng dựa trên ký tự mà không thay thế từ vựng và huấn luyện lại embeddings từ đầu. Bộ dữ liệu Si-En của chúng tôi có 77 chữ viết Sinhala không giao nhau với bộ ký tự X-En. Chúng tôi thí nghiệm chuyển giao một mô hình BBPE 4K T_flores đã được huấn luyện trước (trên X-En) sang bộ dữ liệu này trong khi tái sử dụng từ vựng gốc. Như được hiển thị trong bảng 8, mô hình được chuyển giao tăng 0.9-1.8 điểm BLEU so với các baseline, cho thấy tính tổng quát của các embedding BBPE đã được huấn luyện trước và khả năng thích ứng với các ngôn ngữ khác nhau với các ký tự chưa thấy.

## Công trình liên quan

### Từ vựng từ phụ
Các công trình trước đây đã cho thấy rằng các từ vựng hạt mịn hơn liên tục vượt trội hơn các từ vựng cấp từ trong nhiều bối cảnh, ví dụ, các từ vựng dựa trên phân đoạn hình thái học, mã hóa cặp byte và các từ vựng từ mô hình ngôn ngữ unigram. Các từ vựng từ phụ cấp byte của chúng tôi dựa trên mã hóa cặp byte, trong khi chúng tôi sử dụng byte làm đơn vị cơ bản để tạo thành từ phụ.

### Từ vựng ký tự
Các công trình hiện tại cũng đã khám phá từ vựng ký tự thuần túy cho dịch máy. Kim et al. (2016) đề xuất xây dựng biểu diễn từ từ ký tự; Chung, Cho, và Bengio (2016) loại bỏ hạn chế của ranh giới từ và trực tiếp học giải mã ở cấp độ ký tự; Lee, Cho, và Hofmann (2017) tiếp tục mở rộng nó thành một mô hình hoàn toàn cấp ký tự trong bối cảnh đa ngôn ngữ; Cherry et al. (2018) cho thấy rằng các mô hình cấp ký tự thường vượt trội hơn các mô hình cấp từ phụ khi có đủ dung lượng mô hình.

### Từ vựng cấp byte
Công trình gần nhất với chúng tôi là từ vựng BPE cấp byte được sử dụng trong GPT-2, một mô hình ngôn ngữ tiếng Anh quy mô lớn. Tuy nhiên, họ phụ thuộc rất nhiều vào các quy tắc hợp nhất được mã hóa cứng và chưa tiến hành bất kỳ phân tích nào về cách BPE cấp byte của họ tác động đến chất lượng mô hình hóa ngôn ngữ. Một từ vựng bao gồm hoàn toàn các byte đã được sử dụng trước đây trong một số nhiệm vụ xử lý ngôn ngữ tự nhiên: gắn thẻ từ loại và nhận dạng thực thể có tên, dịch thuật, đọc máy và nhận dạng giọng nói.

### Transformer với tích chập hoặc RNN
Có bằng chứng về tăng hiệu suất từ việc kết hợp Transformer với các lớp tích chập hoặc hồi quy trong lĩnh vực NMT, nhận dạng giọng nói và mô hình hóa ngôn ngữ.

## Kết luận

Chúng tôi đã đề xuất BBPE xây dựng từ vựng từ phụ cấp byte cho dịch máy. Nó dẫn đến một từ vựng gọn gàng hơn nhiều so với các từ vựng dựa trên ký tự mà không mất hiệu suất. Trong bối cảnh đa ngôn ngữ, cái trước thường vượt trội hơn cái sau. BBPE không có bất kỳ token ngoài từ vựng nào, cho phép chúng ta chuyển giao một mô hình sử dụng BBPE giữa các ngôn ngữ với các từ vựng không trùng lặp. Mô hình học chuyển giao này thực sự rất tổng quát và có thể được áp dụng cho bất kỳ ngôn ngữ và bộ dữ liệu nào để tăng hiệu suất hoặc tăng tốc huấn luyện. Với cùng kích thước từ vựng, BBPE phân đoạn câu thành các chuỗi ngắn hơn so với các phương pháp dựa trên ký tự, dẫn đến huấn luyện và suy luận nhanh hơn. Công việc tương lai của chúng tôi bao gồm: loại bỏ sự mất cân bằng độ dài câu source-target; đánh giá BBPE trong các bối cảnh dịch một-sang-nhiều và nhiều-sang-nhiều; khám phá các thuật toán phân đoạn tốt hơn cho từ phụ cấp byte.
