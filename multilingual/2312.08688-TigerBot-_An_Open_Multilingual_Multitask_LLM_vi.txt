# 2312.08688.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2312.08688.pdf
# Kích thước file: 1406831 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
TigerBot: Một Mô hình Ngôn ngữ Lớn Đa nhiệm Đa ngôn ngữ Mở
Ye Chen
Tiger Research
Shanghai, Trung Quốc
yechen@tigerbot.com Wei Cai
Tiger Research
Shanghai, Trung Quốc
wei.cai@tigerbot.com Liangmin Wu
Tiger Research
Shanghai, Trung Quốc
liangmin.wu@tigerbot.com
Xiaowei Li
Tiger Research
Shanghai, Trung Quốc
xiaowei.li@tigerbot.com Zhanxuan Xin
Tiger Research
Shanghai, Trung Quốc
zhanxuan.xin@tigerbot.com Cong Fu
Tiger Research
Shanghai, Trung Quốc
cong.fu@tigerbot.com

Tóm tắt
Chúng tôi phát hành và giới thiệu gia đình mô hình ngôn ngữ lớn TigerBot (LLM)¹, bao gồm các mô hình cơ sở và chat, với kích thước từ 7, 13, 70 và 180 tỷ tham số. Chúng tôi phát triển các mô hình của mình khởi đầu từ Llama-2 và BLOOM, và đẩy ranh giới xa hơn trong dữ liệu, thuật toán huấn luyện, hạ tầng và công cụ ứng dụng. Các mô hình của chúng tôi mang lại sự cải thiện hiệu suất có ý nghĩa so với các mô hình nguồn mở SOTA, ví dụ: Llama-2, cụ thể là cải thiện 6% trong tiếng Anh và 20% trong tiếng Trung. Gia đình mô hình TigerBot cũng đạt được hiệu suất hàng đầu trong các benchmark và bảng xếp hạng học thuật và công nghiệp chính². Chúng tôi tin rằng TigerBot chỉ đại diện cho một bức ảnh chụp nhanh về tiến trình nhanh như chớp trong cộng đồng nguồn mở LLM. Do đó, chúng tôi hào hứng đóng góp lại bằng cách công bố công khai các mô hình của chúng tôi và báo cáo phương pháp tiếp cận của chúng tôi, với trọng tâm bổ sung về việc xây dựng LLM SOTA theo cách dân chủ hóa và làm cho LLM có ích trong các ứng dụng thực tế.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện triển vọng chưa từng có trong một loạt các nhiệm vụ. Kể từ khi ra mắt ngoạn mục của ChatGPT, đã có sự phát triển ngoạn mục trong cộng đồng, chủ yếu theo ba chủ đề:

1. Khả năng cơ bản, được thúc đẩy bởi cả các mô hình độc quyền (GPT [4, 19], BARD [22], Claude [2]) và các mô hình nguồn mở (BLOOM [27], Llama [32, 33]).
2. Kinh tế học tính toán, từ thu thập dữ liệu (ví dụ: Alpaca [31]), huấn luyện (ví dụ: LoRA [11]), lượng tử hóa (ExLlama [34]), và suy luận (ví dụ: TGI [12] và TensorRT [18]).
3. Sẵn sàng ứng dụng, từ API, plugin, gọi hàm và agent, sinh tăng cường truy xuất (RAG), cửa sổ ngữ cảnh dài, đến gần đây là đa phương thức và đóng vai.

Phương pháp chính để xây dựng LLM là huấn luyện trước các transformer chỉ giải mã [35] trên một kho dữ liệu văn bản không giám sát rộng lớn, sau đó là căn chỉnh với sở thích của con người với dữ liệu minh họa có nhãn hoặc dữ liệu so sánh, sử dụng tinh chỉnh có giám sát (SFT) hoặc học tăng cường với phản hồi của con người (RLHF). Chúng tôi đã tuân theo cùng một phương pháp, mặc dù đã có những đóng góp sau:

¹web: https://www.tigerbot.com/chat ; github: https://github.com/TigerResearch/TigerBot
²Tại thời điểm viết bài này, TigerBot xếp hạng các mô hình nguồn mở hàng đầu trong Bảng xếp hạng LLM OpenCompass, và bảng xếp hạng benchmark LLM Trung Quốc CLiB.

Bản thảo. Đang được xem xét.arXiv:2312.08688v2 [cs.CL] 15 Tháng 12 2023

--- TRANG 2 ---
1. Một hỗn hợp dữ liệu huấn luyện mới với đánh giá và làm sạch thử nghiệm kỹ lưỡng.
2. Một ngăn xếp các triển khai thuật toán và hạ tầng mới để làm cho các mô hình của chúng tôi trở thành tối tân (SOTA) về cả hiệu suất và hiệu quả tính toán.
3. Một mô tả kỹ lưỡng về các triển khai và quan sát của chúng tôi từ thực địa, trong việc triển khai các mô hình của chúng tôi vào các ứng dụng thực tế, điều này giúp chúng tôi ưu tiên các nỗ lực nghiên cứu.

Bên cạnh việc đạt được khả năng cơ bản vượt trội, chúng tôi cam kết dân chủ hóa việc phát triển LLM. Theo hiểu biết của chúng tôi, TigerBot chỉ phát sinh chi phí tính toán ít nhất (ít hơn hai triệu đô la trong khoảng thời gian tháng 4 - tháng 12 năm 2023) và dấu chân carbon để có thể sản xuất một trong những gia đình mô hình toàn diện nhất (từ 7B đến 180B, cơ sở và chat, với ngăn xếp đầy đủ các công cụ). Điều này chỉ có thể xảy ra với tinh thần nguồn mở, do đó chúng tôi đóng góp một sự elaboration chi tiết về phương pháp và kinh nghiệm của chúng tôi bằng cùng một cách. Hơn nữa, chúng tôi đã thực hiện các biện pháp để đảm bảo an toàn cho các mô hình của chúng tôi.

2 Các mô hình TigerBot
Chúng tôi phát hành nguồn mở các mô hình của chúng tôi để nghiên cứu và sử dụng thương mại miễn phí³, như được tóm tắt trong Bảng 1, cùng với một bộ công cụ phát triển. Hình 1 hiển thị loss huấn luyện cho pretraining.

Bảng 1: Gia đình mô hình Tigerbot
Mô hình | Cơ sở | Chat | API | Plug-in | Đa phương thức | Độ dài ngữ cảnh
7B | ✓ | ✓ | chat, fine-tune | tìm kiếm | hình ảnh ra | 2k
13B | ✓ | ✓ | chat, fine-tune | tìm kiếm, doc | hình ảnh vào/ra | 32k
70B | ✓ | ✓ | chat, fine-tune | tìm kiếm, doc | hình ảnh vào/ra | 32k
180B | ✓ | ✓ | chat | tìm kiếm, doc | hình ảnh vào/ra | 2k

(a) Loss huấn luyện cho Tigerbot-70b v.v.
(b) Loss huấn luyện cho Tigerbot-180b

Hình 1: Loss huấn luyện cho các mô hình Tigerbot

Mô hình cơ sở là một biểu hiện của kiến thức thế giới, phục vụ như một nền tảng cho việc tinh chỉnh và ứng dụng downstream. Mô hình chat được tinh chỉnh để thực hiện các nhiệm vụ đa mục đích như chat, hỏi đáp (QA), sinh tạo, và như vậy. API là một cách nhanh chóng để khai thác khả năng mô hình TigerBot SOTA từ cloud với vài dòng mã. Hơn nữa, các plugin cho phép nhà phát triển và người dùng tận dụng toàn bộ internet thông qua các công cụ tìm kiếm hiện đại (tìm kiếm), hoặc cơ sở tri thức độc quyền của họ (tài liệu).

2.1 Dữ liệu huấn luyện
Trong giai đoạn pretraining, dữ liệu huấn luyện của chúng tôi bao gồm khoảng 500 tỷ token, hoặc 1.8TB dữ liệu văn bản thô, được làm sạch, loại bỏ trùng lặp và lấy mẫu xuống từ 5.6TB dữ liệu. Dữ liệu của chúng tôi được chọn từ 25 bộ dữ liệu công cộng và độc quyền, dựa trên các cân nhắc thiết kế sau: (1) chất lượng tốt, về mặt tính factual, đa dạng và định dạng, dựa trên các lần lặp lại trong nhiều tháng và phản hồi người dùng; (2) phủ sóng đa ngôn ngữ, đặc biệt là tiếng Trung (ví dụ: WuDao và kho dữ liệu WanJuan) và các ngôn ngữ châu Á phía đông chính ngoài tiếng Anh (với zh:en khoảng 5:5); và (3) phủ sóng đa nhiệm vụ, như web (C4 và RefinedWeb), sách (BookCorpus và Clibrary), wikipedia, mã (GitHub và Stack Overflow), học thuật (arXiv), và dữ liệu lĩnh vực (ví dụ: pháp lý và bằng sáng chế). Bảng 2 hiển thị hỗn hợp dữ liệu huấn luyện và nguồn của chúng, và Hình 2 minh họa tỷ lệ các bộ dữ liệu.

Bảng 2: Dữ liệu huấn luyện Tigerbot
Bộ dữ liệu | Kích thước (GB) | Token (B) | Nguồn
Sách | en-books | 100.00 | 25.06 | BookCorpus
| zh-books | 154.00 | 39.23 | Clibrary
| zh-textbook | 2.20 | 0.74 | WanJuan
WebTexts | en-c4 | 80.30 | 19.92 | C4
| en-refinedweb | 345.15 | 86.80 | RefinedWeb
| en-webtext | 39.00 | 10.14 | OpenWebText
| zh-news | 121.00 | 27.38 | Tigerbot và WanJuan
| zh-webtext | 614.00 | 147.59 | WuDao và WanJuan
Papers | en-arxiv | 38.00 | 12.52 | arXiv
Codes | en-github | 117.13 | 42.84 | Github
| en-stackoverflow | 24.80 | 7.97 | Stack Overflow
Wiki | en-wiki | 21.00 | 6.68 | English wikipedia
| zh-wiki | 2.79 | 1.72 | Chinese wikipedia
| zh-baike | 87.50 | 23.00 | Tigerbot và WuDao
| ja-wiki | 6.80 | 2.00 | Japanese wikipedia
| ko-wiki | 1.50 | 0.53 | Korean wikipedia
Domain | en-stackexchange | 6.80 | 1.91 | Stack Exchange
| zh-law | 35.03 | 9.42 | Tigerbot và WanJuan
| zh-patent | 17.00 | 4.66 | WanJuan
| zh-sentiment | 0.02 | 0.01 | Cantonese sentiment
Tổng cộng | 1,814.02 | 470.12⁴

Trong giai đoạn học căn chỉnh, chúng tôi thu thập 5 triệu dữ liệu hoàn thành hướng dẫn cho SFT, và 15k dữ liệu so sánh có chú thích của con người cho RLHF, trong đó 10k ví dụ cho lấy mẫu từ chối và 5k cho tối ưu hóa sở thích trực tiếp (DPO). Dữ liệu huấn luyện fine-tuning của chúng tôi được chọn từ khoảng 30 bộ dữ liệu mở và độc quyền, và nhiều tháng chú thích của con người từ prompt người dùng thực tế. Hỗn hợp dữ liệu được thiết kế để đại diện cho một loạt rộng các nhiệm vụ đa mục đích (ví dụ: sinh tạo, QA, và brainstorming), theo một danh mục tương tự trong [19] và chúng tôi mở rộng thêm khoảng 100 danh mục con (ví dụ: trích xuất - từ văn bản đến json, phân loại - phân loại tình cảm).

Chúng tôi tin rằng chất lượng dữ liệu có vai trò then chốt trong chất lượng mô hình, do đó chúng tôi đã áp dụng một cách tiếp cận có hệ thống để đảm bảo chất lượng dữ liệu. Dữ liệu từ web thường bị lệch về chất lượng, thậm chí còn hơn thế đối với kho dữ liệu ngoài tiếng Anh. Mặt khác, cơ chế cơ bản của LLM làm cho các mô hình có trí nhớ tốt. Các thí nghiệm của chúng tôi phát hiện ra rằng ít hơn một tá ví dụ chất lượng thấp sẽ làm cho mô hình học và sinh ra kết quả không tối ưu. Ví dụ về dữ liệu chất lượng thấp bao gồm: ngôn ngữ thông thường hoặc nói trên web, tiếng lóng trong mạng xã hội, quảng cáo, nội dung có hại, định dạng và phong cách lộn xộn, v.v. Chúng tôi đã phát triển một bộ quy tắc và mô hình để lọc ra khoảng 10% dữ liệu chất lượng thấp như sau.

Với khối lượng lớn dữ liệu, chúng tôi thiết kế phương pháp có độ phức tạp O(n²).

1. Lọc ra dữ liệu định dạng sai sử dụng một bộ quy tắc, ví dụ: quá nhiều ký hiệu hoặc chữ số trong prompt.
2. Loại bỏ trùng lặp sử dụng so khớp chuỗi chính xác.
3. Loại bỏ trùng lặp sử dụng sequence simhash + chuỗi con chung dài nhất.

⁴sau này thêm 5% dữ liệu huấn luyện tổng thể để tạo ra ~500B token.

--- TRANG 3 ---
WebTexts, 1,199.45, 66%
Sách, 256.20, 14%
Codes, 141.93, 8%
Wiki, 119.59, 7%
Papers, 38.00, 2%
Domain, 58.85, 3%
Bộ dữ liệu trong hỗn hợp huấn luyện (GB, %)

Hình 2: Tỷ lệ dữ liệu huấn luyện

4. Lọc ra nội dung có hại sử dụng một ensemble của ba mô hình an toàn SOTA và một từ điển các từ nhạy cảm.

2.2 Phương pháp huấn luyện
Chúng tôi đổi mới trên vai những người đi trước, để kế thừa đầy đủ dữ liệu, tài nguyên tính toán và trí tuệ được đặt trước. Các mô hình với 7, 13 và 70B tham số được huấn luyện trước liên tục từ các biến thể tương ứng của Llama-2, trong khi 180B được huấn luyện trước từ BLOOM. Chúng tôi áp dụng hầu hết các cài đặt huấn luyện và kiến trúc mô hình từ những người đi trước, chủ yếu là các transformer chỉ giải mã [35], RoPE [29] và ALiBi [23] positional embedding, SwiGLU [28] và GeLU activation function, tương ứng. Chúng tôi đã có thêm những đóng góp như được trình bày chi tiết dưới đây. Các mục tiêu thiết kế của chúng tôi là: (1) đưa ra một hạ tầng huấn luyện SOTA có thể tạo ra các mô hình vượt trội theo cách kinh tế tính toán, (2) phủ sóng đa ngôn ngữ đặc biệt là tiếng Trung, và (3) hữu ích cho việc phát triển ứng dụng.

Tokenizer Các mô hình Llama thiếu khả năng đại diện ngôn ngữ khác ngoài tiếng Anh (ví dụ: tiếng Trung chỉ chiếm 0.13% trong dữ liệu huấn luyện của họ), do đó chúng tôi mở rộng từ vựng tokenizer trong Tigerbot. Đầu tiên, chúng tôi lấy mẫu một kho dữ liệu 100GB tiếng Trung và các ngôn ngữ châu Á phía đông chính (chủ yếu là tiếng Nhật và tiếng Hàn) từ dữ liệu pretraining của chúng tôi. Thứ hai, chúng tôi huấn luyện một tokenizer Byte-Pair Encoding (BPE) SentencePiece [10] sử dụng dữ liệu không phải tiếng Anh này để đảm bảo tính đại diện trong tokenizer cuối cùng. Thứ ba, chúng tôi hợp nhất tokenizer không phải tiếng Anh với tokenizer Llama-2 gốc để tạo ra tokenizer cuối cùng của chúng tôi [8]. Kích thước từ vựng gốc là 32k, chúng tôi mở rộng gần nhưng không vượt quá 65k, để tránh nhân đôi lưu trữ và IO trong dữ liệu nhị phân token hóa downstream. Chúng tôi cũng phát hiện ra rằng hơn 100GB dữ liệu huấn luyện là không cần thiết, độ phủ ký tự gần như giống hệt nhau; nhưng bộ nhớ CPU peak sẽ vượt quá 2TB, vượt quá phần cứng chính. Đối với mô hình 180B tham số, chúng tôi giữ kích thước từ vựng giống như 250k, vì BLOOM đã có phủ sóng đa ngôn ngữ tốt.

Framework huấn luyện Gia đình mô hình TigerBot đã được huấn luyện bằng cách sử dụng fork riêng của chúng tôi về Megatron-DeepSpeed [16], triển khai song song 3D bằng cách kết hợp ZeRO sharding, song song dữ liệu (DP) và song song pipeline (PP) từ DeepSpeed [25] với song song tensor (TP) từ Megatron-LM [17]. Fork của chúng tôi đã thực hiện một số nâng cấp như sau:

1. Đưa lớp Modeling Megatron-DeepSpeed cập nhật với các thành phần kiến trúc SOTA bao gồm CoreAttention, SwiGLU, grouped-query attention (GQA) [1], RoPE [29], và flash attention [9] thích ứng với kiến trúc Llama-2.

--- TRANG 4 ---
2. Thiết kế một thuật toán tốt hơn nhưng đơn giản cho phân vùng pipeline. Cho một mô hình với N attention block được chia thành M stage, đầu tiên N mod M stage chứa ⌈N/M⌉ block và các block còn lại mỗi block có ⌊N/M⌋ block. So với triển khai ban đầu nơi tổng stage bị giới hạn ở một số đặc biệt, phương pháp của chúng tôi linh hoạt hơn để giảm thiểu vấn đề phân vùng lệch.

3. Một bộ script chuyển đổi trọng số Megatron-DeepSpeed đã chia thành trọng số transformers và ngược lại.

Song song tensor đặc biệt quan trọng cho việc huấn luyện các mô hình trên 100 tỷ tham số, vì kích thước mô hình trên 70B không thể vừa với một GPU duy nhất trong khi CPU offloading chậm và chúng tôi muốn tránh. Mặt khác, nhiều TP hơn gây ra giao tiếp nặng hơn và TP qua các node không thực tế do giao tiếp lớn giữa các node. Với một giá trị cố định của TP×PP×DP, theo kinh nghiệm chúng tôi phát hiện ra rằng TP nhỏ hơn mang lại hiệu quả toàn cục tốt hơn, phần lớn do giao tiếp nặng hơn giữa các phân vùng tensor (song song hóa phép nhân ma trận) so với các phân vùng pipeline (layer). Cùng với các kỹ thuật GQA, flash attention, gradient accumulation và checkpointing, chúng tôi đã có thể tìm ra cấu hình tối ưu cho các kích thước mô hình khác nhau dưới các tài nguyên cluster khác nhau. Một tính toán sơ bộ như sau.

Llama-2-13B pretrain GPU giờ là 368,640 với dữ liệu 2TB token, theo bài báo Llama-2 [33], do đó chúng tôi có: training-tokens/gpu-sec = 1,507. Mặt khác, throughput huấn luyện TigerBot-13B đạt 25.7 ví dụ/giây hoặc tương đương 74.6 giây/iteration, như hiển thị trong Hình 3, trên cluster GPU 32×A100-40G, với độ dài sequence 2,048. Chỉ sau một tìm kiếm hình học sơ bộ của các cấu hình song song nói trên, chúng tôi tìm thấy cài đặt tối ưu là: TP=2, PP=8, DP=2, per-device-batch-size=2, và global-batch-size=1,920, để đạt khoảng 4M token global batch. Hiệu quả của chúng tôi đọc: training-tokens/gpu-sec = 1,645 (109% của huấn luyện Llama-2). Cũng xem xét rằng Llama-2 sử dụng Meta's Research Super Cluster cao cấp hơn (A100-80G, 2TB CPU memory, kết nối RDMA) [20], chúng tôi tin rằng codebase của TigerBot đã đạt tới kinh tế tính toán tiên tiến trên toàn thế giới.

(a) thời gian iteration vs. token
(b) kích thước batch vs. token
(c) seqlen vs. token

Hình 3: Hiệu quả huấn luyện cho các mô hình Tigerbot

Huấn luyện tổng thể Trong giai đoạn pretraining, chúng tôi trộn 2-5% (của dữ liệu pretraining) dữ liệu hoàn thành hướng dẫn, được tiền xử lý thành định dạng không giám sát, ví dụ: {instruction}+"\n"+{response}, và loại bỏ trùng lặp từ dữ liệu pretraining ban đầu trùng với kiến thức dữ liệu SFT. Lý do đằng sau là: hoàn thành hướng dẫn (ví dụ: hỏi đáp, sinh tạo) về bản chất vẫn là một loại tiếp tục ngôn ngữ con người. Chúng tôi muốn các mô hình học một số mẫu của việc theo hướng dẫn, một cách tổng thể cùng với kiến thức cơ sở trong quá trình pretraining. Chi phí tính toán phát sinh thêm không đáng kể, nhưng lợi ích có hai mặt:

1. Các mô hình cơ sở thể hiện khả năng mạnh mẽ để theo hướng dẫn, ngay lập tức trước khi căn chỉnh. Chúng tôi thực hiện đánh giá nhanh trên benchmark SQuAD2.0, và phát hiện ra rằng Tigerbot-13b-base đạt 86% độ chính xác dự đoán token tiếp theo như Tigerbot-13b-chat.

2. Vì khả năng cơ bản (kiến thức và theo hướng dẫn) đã được học trong quá trình pretraining, học căn chỉnh có thể nhẹ. Điều này có lợi thêm cho triển khai ứng dụng nhanh chóng và kinh tế trong các lĩnh vực khác nhau. Các thí nghiệm của chúng tôi cho thấy rằng loss đạt 95% hội tụ sau một triệu ví dụ huấn luyện SFT.

Supervised fine-tuning (SFT) Các mô hình chỉ được huấn luyện trên phần phản hồi của dữ liệu huấn luyện có giám sát, sử dụng phần lớn cùng một quy trình huấn luyện, ngoại trừ tiền xử lý dữ liệu. Chúng tôi đầu tiên giới thiệu một cặp token đặc biệt để đánh dấu hướng dẫn và phản hồi, tương ứng, cho mỗi ví dụ. Sau đó chúng tôi xử lý các ví dụ dữ liệu riêng lẻ (cắt ngắn hoặc pad đến độ dài sequence tối đa) hoặc nhóm (nối thành độ dài sequence tối đa) vào trainer. Đối với phương pháp nhóm, chúng tôi triển khai attention mask sao cho attention sẽ không được tính toán qua các ví dụ. Dữ liệu SFT thường khá thưa thớt (ít ví dụ dài và hầu hết trong max-seq-length). Phương pháp nhóm cho tốc độ tăng 5 đến 10×, mặc dù cách riêng lẻ nên mang lại độ chính xác cao hơn theo trực giác. Tuy nhiên, các thí nghiệm của chúng tôi cho thấy rằng không có sự khác biệt hiệu suất đáng chú ý giữa hai phương pháp, sau đó chúng tôi chọn phương pháp nhóm trong các lần chạy sản xuất. Chúng tôi cũng nhận thấy rằng phương pháp nhóm có thể đưa một mức độ robustness và tính tổng quát vào mô hình, tương tự như con người cũng học từ nhiễu.

Reinforcement learning with human feedback (RLHF) Trong giai đoạn RLHF, chúng tôi đầu tiên áp dụng fine-tune lấy mẫu từ chối với con người trong vòng lặp như sau:

1. Lấy mẫu 10k prompt từ dữ liệu SFT và tương tác người dùng thực.
2. Sinh 10 phản hồi ngẫu nhiên (ví dụ: với temperature=0.6) cho mỗi prompt, sử dụng mô hình chat ứng viên tốt nhất của chúng tôi.
3. Xếp hạng các sinh tạo, của tất cả prompt 90% sử dụng mô hình reward (một mô hình 13B được tinh chỉnh cho phân loại văn bản), 5% sử dụng xếp hạng của con người, và 5% sử dụng chỉnh sửa của con người (bộ vàng).
4. Trộn các sinh tạo được xếp hạng cao nhất vào dữ liệu SFT và thực hiện fine-tuning.
5. Lặp lại quy trình trên hàng tháng, khi chúng tôi thu thập prompt người dùng thực.

Chúng tôi áp dụng thêm thuật toán tối ưu hóa sở thích trực tiếp (DPO) [24] trên đầu trọng số được tinh chỉnh lấy mẫu từ chối, sử dụng 5k dữ liệu so sánh vàng. Chúng tôi chọn DPO vì nó đơn giản để triển khai, hiệu quả để huấn luyện, trong khi thực hiện tốt như hoặc tốt hơn các phương pháp dựa trên PPO. Bằng cách tự nhiên xử lý LLM được fit như một mô hình reward, DPO về bản chất công thức hóa một vấn đề phân loại cho so sánh từng cặp, với một loss cross-entropy đơn giản như sau [24]:

L_DPO(π_θ;π_ref) = -E_{(x,y_w,y_l)∼D}[log σ(β log π_θ(y_w|x)/π_ref(y_w|x) - β log π_θ(y_l|x)/π_ref(y_l|x))] (1)

trong đó σ là hàm logistic, và β là một siêu tham số kiểm soát độ lệch từ chính sách tham chiếu π_ref. Cũng như trong [24], theo kinh nghiệm chúng tôi phát hiện ra DPO ổn định hơn để huấn luyện, có thể do công thức hóa thanh lịch của nó và không cần mạng riêng biệt như trong thuật toán PPO kiểu actor-critic. Huấn luyện hiệu quả và ổn định dẫn đến lặp lại nhanh chóng trong triển khai ứng dụng.

Sequence dài Khả năng sequence dài của một mô hình có xu hướng quan trọng trong các ứng dụng, ví dụ: cửa sổ ngữ cảnh dài để đọc một cuốn tiểu thuyết, hoặc độ dài sinh tạo dài để viết một cuốn sách, tất cả cùng một lúc. Vì dữ liệu văn bản tuân theo luật power, tức là hầu hết ngắn và ít dài, suy luận với sequence dài có thể tương tự như phóng to một bức tranh. Có hai yếu tố quan trọng: (1) độ phân giải của hình ảnh gốc, và (2) thuật toán nội suy. Trước là độ dài của các mẫu huấn luyện và sau là phương pháp ngoại suy RoPE.

Trong quá trình huấn luyện TigerBot, chúng tôi tăng tần số cơ sở RoPE lên 500k [37] và nhóm các mẫu huấn luyện thành 4k. Song song attention cũng có thể được thực hiện theo chiều độ dài sequence để cho phép độ dài huấn luyện vượt quá giới hạn của tổng bộ nhớ GPU trên một node duy nhất. Điều này có thể mang lại sequence cực dài, ví dụ: vài trăm nghìn cho một số ứng dụng, nhưng với chi phí tốc độ cao hơn. Song song tensor giữa các node trở nên cực kỳ chậm do giao tiếp lớn, gián tiếp giới hạn độ dài mẫu trong mỗi node. Chúng tôi chọn không trộn dữ liệu sequence dài mục đích đặc biệt để bảo toàn tính tổng quát của các mô hình, nhưng trong giai đoạn căn chỉnh chúng tôi quan sát khoảng 2‰ ví dụ vượt quá 2k token.

Trong giai đoạn suy luận, ngoại suy độ dài được đạt được thông qua nội suy RoPE position embedding [29]. Sự khác biệt chính giữa các phương pháp phổ biến như Dynamic và YaRN [21] nằm ở kỹ thuật nội suy của chúng. Thách thức là làm thế nào để duy trì tính nhất quán đầu ra. Trong các triển khai của Dynamic và YaRN bởi Transformers [13] và TGI [12], phương pháp liên quan đến "cache position embedding của các sequence dài nhất được thấy". Dưới triển khai này, ngay cả khi mô hình quan sát cùng một đầu vào, đầu ra có thể khác nhau do biến đổi trong độ dài của position embedding được cache. Tigerbot giải quyết điều này bằng cách tính tổng của input-token-length và max-new-token-length mỗi request. Giá trị này được sử dụng như một tham chiếu khi tính toán position embedding được tỷ lệ hóa. Điều này đảm bảo hiệu suất của mô hình vẫn nhất quán khi ngoại suy độ dài. Hành vi của mô hình đối với các sequence không vượt quá độ dài huấn luyện cũng được bảo toàn. Chúng tôi ngoại suy độ dài sequence tối đa đến 32k sử dụng hệ số scaling RoPE là 8.

Lượng tử hóa Lượng tử hóa một LLM liên quan đến việc sử dụng biểu diễn số nguyên độ chính xác giảm cho trọng số và activation, có thể quan trọng cho các cân nhắc thực tế về giới hạn bộ nhớ GPU và suy luận nhanh. Chúng tôi triển khai cả lượng tử hóa tĩnh và động.

Trong lượng tử hóa tĩnh, trọng số và activation của mô hình được tính toán sử dụng một bộ dữ liệu calibration trước. Các mô hình TigerBot được lượng tử hóa sử dụng ExLlamaV2 [34], dựa trên cùng phương pháp tối ưu hóa như GPTQ. Chúng tôi chứng minh tăng tốc lên đến 3× và giảm bộ nhớ 4× cho các mô hình Tigerbot-4bit lượng tử hóa với mất mát độ chính xác không đáng kể.

Trong lượng tử hóa động, trọng số vẫn được lượng tử hóa trước sau huấn luyện, nhưng activation được lượng tử hóa trong quá trình suy luận ngay lập tức. Đặc biệt, chúng tôi sử dụng lượng tử hóa trọng số 8-bit và activation 16-bit (W8A16). Các thí nghiệm của chúng tôi cho thấy rằng activation 8-bit có thể gây ra suy giảm độ chính xác đáng kể tương tự như [38], trong khi W8A16 mang lại sự cân bằng tốt giữa độ chính xác và tăng tốc. Phương pháp động có lợi thế trong việc thích ứng với các phần cứng phục vụ khác nhau, đặc biệt là những cái bị nghẽn cổ chai bởi băng thông bộ nhớ hơn là tính toán.

An toàn Chúng tôi đã thực hiện lọc an toàn trên dữ liệu huấn luyện, chúng tôi cũng thực hiện các biện pháp để giảm thiểu rủi ro an toàn trong quá trình huấn luyện và suy luận tại runtime cho bất kỳ sản phẩm TigerBot nào giao tiếp với người dùng. Chúng tôi áp dụng một danh mục an toàn bao gồm 5 danh mục và 31 danh mục con. Các danh mục chính bao gồm:

1. Vi phạm các giá trị cốt lõi của an ninh quốc gia và xã hội
2. Nội dung phân biệt đối xử
3. Các hoạt động thương mại bất hợp pháp và không được quy định
4. Xâm phạm quyền và lợi ích hợp pháp của người khác
5. Không thể đáp ứng các yêu cầu an toàn cho một số dịch vụ mục đích đặc biệt, ví dụ: dịch vụ thông tin y tế và hạ tầng thông tin quan trọng.

Trong quá trình huấn luyện, chúng tôi sử dụng chú thích của con người để thu thập khoảng 40k dữ liệu minh họa an toàn, tham khảo ý kiến với hướng dẫn quản lý và chuyên gia lĩnh vực. Dữ liệu này sau đó được đưa vào học căn chỉnh của chúng tôi cũng như pretraining theo huấn luyện tổng thể. Dữ liệu huấn luyện an toàn được làm mới hàng tháng và phản ánh vào quy trình căn chỉnh lặp lại của chúng tôi. Cả các biện pháp an toàn cấp dữ liệu và huấn luyện đều mang tính phòng ngừa, trong khi kiểm tra an toàn cấp runtime mang tính bảo vệ.

Trong quá trình suy luận runtime, đầu vào của người dùng được kiểm tra an toàn trước khi đưa vào mô hình để sinh tạo. Nếu đầu vào người dùng hoặc sinh tạo mô hình được phát hiện có hại, các sản phẩm của chúng tôi cung cấp một phản hồi mặc định nhưng gợi ý cho người dùng. Tất cả nội dung trải qua kiểm tra hai giai đoạn, đầu tiên là một từ điển khoảng 120k từ vựng nhạy cảm, sau đó là một ensemble của ba classifier dựa trên BERT. Các classifier an toàn này được huấn luyện trên hàng triệu mẫu có chú thích tích cực (vi phạm) và tập trung vào các khía cạnh khác nhau trong các danh mục an toàn nói trên. Từ điển được thiết kế để toàn diện để đảm bảo recall cao, trong khi độ chính xác tốt được đạt được bằng cách điều chỉnh ngưỡng tích cực từ các classifier an toàn. Nhãn an toàn cuối cùng là một hàm tham số hóa của phát hiện từ điển và đầu ra classifier, và có thể được thay đổi cho các lĩnh vực và ứng dụng khác nhau. Hơn nữa, chúng tôi có đội an toàn để giữ cho từ điển và classifier của chúng tôi cập nhật với các chủ đề mới nổi và hướng dẫn quản lý.

Siêu tham số Chúng tôi pretrain các mô hình TigerBot sử dụng kích thước batch toàn cục (GBS) 4M token, trong khi fine-tune các mô hình với GBS nhỏ như 100–400k token. Các thí nghiệm của chúng tôi cho thấy rằng, với dữ liệu chất lượng cao, batch nhỏ hơn cho các cập nhật chi tiết hơn có thể mang lại loss thấp hơn, như hiển thị trong Hình 4. Chúng tôi pretrain các mô hình cho một epoch trên dữ liệu huấn luyện, fine-tune cho hai epoch, sau đó theo sau bởi học căn chỉnh cho một epoch.

Chúng tôi sử dụng optimizer adamW, với β₁ = 0.9, β₂ = 0.95, ε = 10⁻⁵. Chúng tôi áp dụng lịch learning rate (LR) cosine decay giữa [2.0⁻⁵, 2.0⁻⁴] cho 7B và 13B, và [1.0⁻⁵, 1.0⁻⁴] cho pretraining mô hình 70B và 180B. Đối với fine-tuning, LR là [2.0⁻⁶, 2.0⁻⁵]. Chúng tôi sử dụng warmup steps gần 1% của training steps, tỷ lệ weight decay 0.1, và gradient clipping đến 1.0. Tất cả huấn luyện được thực hiện dưới độ chính xác bfloat16 và flash attention, ngoại trừ 180B chúng tôi sử dụng gradient checkpointing thay thế.

--- TRANG 5 ---
(a) training loss
(b) validation loss

Hình 4: Tigerbot-70b-SFT loss với kích thước batch khác nhau

Phần cứng huấn luyện Cluster huấn luyện của chúng tôi bao gồm 512×A100-40G GPU (64 node ×8 GPU), được trang bị kết nối GPU intra-node NVLink, và giao tiếp inter-node RoCE (RDMA over Converged Ethernet). Mỗi node có CPU Intel Platinum 64-core và 1024GB RAM.

2.3 Đánh giá
Đánh giá LLM là một vấn đề thách thức nhưng quan trọng. Nó phục vụ như một cơ sở có hệ thống cho việc lựa chọn mô hình, và tiết lộ các lĩnh vực cần cải thiện. Tuy nhiên, thực hiện đánh giá chỉ ở cuối pipeline huấn luyện đặt các tính toán đắt tiền vào rủi ro chìm xuống, có thể chỉ vì một sự bất cẩn nhỏ trong dữ liệu hoặc lỗi trong mã. Hơn nữa, các nhà phát triển đã phát hiện ra rằng thường có sự khác biệt giữa các metric benchmark và nhận thức của con người. Để giải quyết những thách thức này, chúng tôi phát triển một phương pháp đánh giá ba giai đoạn như dưới đây.

Bảng 3: Kết quả đánh giá mô hình cơ sở Tigerbot
Ngôn ngữ | Nhiệm vụ | Benchmark | TigerBot | Llama-2
| | | 70B-base | 13B-base | 70B-base | 13B-base
En | Code | HumanEval | 28.66 | 18.29 | 31.10 | 15.27
| Commonsense Reasoning | PIQA | 83.30 | 79.33 | 82.21 | 79.21
| | SIQA | 48.77 | 48.52 | 46.01 | 46.32
| | HellaSwag | 78.62 | 72.64 | 79.46 | 74.96
| | WinoGrande | 69.38 | 64.17 | 69.69 | 64.09
| | OpenBookQA | 88.60 | 72.40 | 57.40 | 57.00
| Reading Comprehension | BoolQ | 67.52 | 63.18 | 69.69 | 71.50
| Math | GSM8K | 65.66 | 35.86 | 63.99 | 28.81
| Multi-choice Questions | MMLU | 68.68 | 55.35 | 69.58 | 55.81
| Trung bình (En) | | | 66.58 | 56.64 | 63.24 | 54.77
Zh | Reading Comprehension | CMRC | 85.93 | 66.57 | 68.97 | 76.73
| | C3 | 77.37 | 67.01 | 60.16 | 47.51
| Natural Language Inference | OCNLI | 30.00 | 30.60 | 30.03 | 30.00
| Multi-choice Questions | C-EVAL | 67.75 | 48.46 | 49.90 | 38.67
| Trung bình (Zh) | | | 65.26 | 53.16 | 52.27 | 48.23

--- TRANG 6 ---
Bảng 4: Kết quả đánh giá mô hình chat Tigerbot
Ngôn ngữ | Nhiệm vụ | Benchmark | TigerBot | Llama-2
| | | 70B-chat | 13B-chat | 70B-chat | 13B-chat
En | Code | HumanEval | 31.10 | 26.83 | 26.22 | 11.59
| Commonsense Reasoning | PIQA | 83.57 | 80.09 | 80.25 | 78.67
| | SIQA | 51.89 | 49.44 | 51.59 | 50.97
| | HellaSwag | 76.68 | 70.63 | 77.63 | 74.71
| | WinoGrande | 67.01 | 63.30 | 68.11 | 65.82
| | OpenBookQA | 85.00 | 67.40 | 85.00 | 80.00
| Reading Comprehension | BoolQ | 80.67 | 78.87 | 78.00 | 78.32
| Math | GSM8K | 84.91 | 51.25 | 58.91 | 54.62
| Multi-choice Questions | MMLU | 68.03 | 55.94 | 64.84 | 54.61
| Trung bình (En) | | | 69.87 | 60.42 | 65.62 | 59.43
Zh | Reading Comprehension | CMRC | 85.37 | 76.17 | 80.06 | 74.62
| | C3 | 75.34 | 69.42 | 54.85 | 51.01
| Natural Language Inference | OCNLI | 38.07 | 40.17 | 36.23 | 30.00
| Multi-choice Questions | C-EVAL | 60.40 | 48.89 | 44.17 | 39.22
| Trung bình (Zh) | | | 64.80 | 58.66 | 53.83 | 48.71

1. Trong quá trình huấn luyện cho các checkpoint chính, chúng tôi thực hiện đánh giá nhẹ để xem trước nhanh. Chúng tôi đầu tiên rút một mẫu ngẫu nhiên 100k ví dụ train và validation từ 10 benchmark chính bao gồm ARC [6], CommonsenseQA [30], SQuAD 2.0 [26], WebQuestions [3], và như vậy. Sau đó chúng tôi triển khai một độ chính xác dự đoán token tiếp theo trong trainer của Transformers, và một lần chạy đánh giá chỉ mất vài phút, trên một node, ngay cả cho các mô hình 70B và 180B.

2. Sau huấn luyện, chúng tôi tiến hành đánh giá toàn diện trên 13 benchmark chính, bao gồm 8 nhiệm vụ và ngôn ngữ chủ yếu của tiếng Anh và tiếng Trung. Các bộ dữ liệu đánh giá được thiết kế để bao gồm một loạt rộng các nhiệm vụ như toán học, lý luận, mã, và đọc hiểu, v.v. Chúng tôi báo cáo kết quả cho cả mô hình cơ sở và chat trong Bảng 3 và 4, tương ứng. Các mô hình cơ sở được kiểm tra sử dụng 0-shot, và ngược lại chúng tôi tuân theo triển khai của OpenCompass [7] để thúc đẩy khả năng tái sản xuất.

3. Hơn nữa, chúng tôi thực hiện đánh giá con người trên các ứng viên hàng đầu của chúng tôi. Đánh giá con người thường được coi là tiêu chuẩn vàng để đánh giá các hệ thống sinh ngôn ngữ tự nhiên. Tuy nhiên, vì LLM có thể thực hiện một loạt rất rộng các nhiệm vụ, không thực tế để thu thập một bộ dữ liệu toàn diện để mang lại kết quả có ý nghĩa thống kê. Các thiên vị chủ quan giữa các chú thích viên và chi phí không tầm thường nếu đánh giá lặp lại cũng là những cân nhắc quan trọng. Chúng tôi sử dụng đánh giá con người như một người gác cổng, kết hợp với benchmark tự động để lựa chọn mô hình. Chúng tôi đầu tiên thu thập 5k prompt bộ vàng, phần lớn từ câu hỏi người dùng thực và không nhìn thấy từ bất kỳ quy trình upstream nào. Bộ vàng được lấy mẫu để bao gồm một loạt rộng các nhiệm vụ như mô tả trong Phần 2.1, nhưng phản ánh hành vi người dùng thực (tông giọng, lỗi chính tả, từ vựng nói, v.v.). Sau đó chúng tôi yêu cầu các chú thích viên đánh giá về tính hữu ích và an toàn sử dụng thang điểm Likert 1-5. Chúng tôi phát triển một hướng dẫn đánh giá để đào tạo các chú thích viên, bao gồm các quy tắc chi tiết và ví dụ cho các loại sinh tạo khác nhau. ví dụ: tính factual cho các câu hỏi khách quan, đa dạng cho các sinh tạo chủ quan. Chúng tôi đôi khi phát hiện ra rằng kết quả đánh giá con người không được căn chỉnh chặt chẽ với các metric benchmark, chúng tôi chọn các mô hình sản xuất như một sự cân bằng tổng thể, cũng xem xét sự suy giảm mô hình, tính ngẫu nhiên và yêu cầu ứng dụng downstream.

--- TRANG 7 ---
3 Ứng dụng
Trong phần này, chúng tôi elaboration về các triển khai của chúng tôi về một phổ ứng dụng, một số là công cụ và sản phẩm tiêu dùng, và một số là các ứng dụng được triển khai thực tế.

Hỏi đáp ngữ cảnh dài Nhiều ứng dụng liên quan đến đọc hiểu (tóm tắt và hỏi đáp) dựa trên ngữ cảnh dài, ví dụ: đọc bài báo, nghiên cứu luật, QA dựa trên cơ sở tri thức nội bộ, v.v. Hầu hết các hệ thống QA dựa trên kiến thức được triển khai như một pipeline truy xuất-đọc hai giai đoạn. Một truy xuất dày đặc thu hẹp ngữ cảnh xuống một tập hợp nhỏ các đoạn văn, theo sau bởi một LLM reader để sinh câu trả lời dựa trên các ngữ cảnh được truy xuất [14]. Thiết lập này có hai hạn chế: (1) nó không phù hợp cho các câu hỏi tóm tắt và quy nạp, nơi ngữ cảnh nên toàn diện; và (2) một phần lớn lỗi có thể đã xảy ra ở giai đoạn truy xuất, do lý do thực tế ví dụ: dữ liệu nhiễu và định dạng sai.

Như mô tả trong Phần 2.2, chúng tôi đã ngoại suy độ dài ngữ cảnh đến 32k token, hoặc khoảng 50k ký tự như trong một tài liệu pdf hoặc word 50 trang (tokenizer của Tigerbot có tỷ lệ nén ký tự-token 1.5× cho tiếng Trung và 5× cho tiếng Anh). Cửa sổ ngữ cảnh này đủ lớn cho hầu hết các nhiệm vụ QA dựa trên kiến thức ad-hoc, do đó chúng tôi bỏ qua phần truy xuất dày đặc và áp dụng phương pháp một cửa như sau.

1. Phân đoạn: nếu văn bản đầu vào vượt quá độ dài đầu vào tối đa (ví dụ: 32k), phân đoạn nó thành các chunk có độ dài tối đa, sử dụng ngắt dòng để bảo toàn ngữ nghĩa.

2. Lọc: khi nhận được một truy vấn người dùng, prompt LLM zero-shot như một classifier nhị phân để lọc ra các segment không liên quan, tương tự như phương pháp trong [5]. Chúng tôi soạn prompt như: C:{context}+"\n"+Q:{query}+"\n"+"Can the above Q be answered by C?"

3. Sinh tạo: đầu tiên sinh một phản hồi bởi mỗi segment ứng viên như kết quả trung gian, từ đó sau đó sinh phản hồi cuối cùng sử dụng một prompt như: {intermediate results}+"\n"+{query}. Phương pháp đệ quy này đã được sử dụng trong nhiệm vụ tóm tắt và thể hiện hiệu suất vượt trội [36].

Tóm tắt đệ quy Tóm tắt đã là một trong những nhiệm vụ NLP chính, và bây giờ có thể được giải quyết liền mạch bởi LLM. Để xử lý hiệu quả các văn bản cực kỳ dài, chúng tôi sử dụng phương pháp tóm tắt đệ quy, tương tự như [36] nhưng chúng tôi chỉ dựa vào LLM. Chúng tôi đầu tiên chia văn bản đầu vào thành các segment nhỏ hơn và có thể quản lý (trong max-input-length), theo các ranh giới ngữ nghĩa tự nhiên như kết thúc phần và ngắt dòng. Sau đó chúng tôi tóm tắt mỗi segment độc lập. Trong bước cuối cùng, chúng tôi tổng hợp các tóm tắt riêng lẻ này, để sinh ra một tóm tắt cuối cùng toàn diện và gắn kết. Đặc tính lĩnh vực và độ dài mong muốn có thể được hướng dẫn tự nhiên bởi prompt, ví dụ: với prompt: "Tóm tắt bài viết trên thành 200 từ, bảo toàn các thông tin tài chính quan trọng."

Gọi hàm Giao diện ngôn ngữ tự nhiên hấp dẫn đối với nhiều ứng dụng, nơi người dùng đưa ra hướng dẫn bằng ngôn ngữ tự nhiên và hệ thống có thể hiểu và thực hiện các nhiệm vụ thường yêu cầu đầu vào có cấu trúc. Khả năng hiểu ngôn ngữ tự nhiên nội tại của LLM có thể được tận dụng để trích xuất dữ liệu có cấu trúc từ truy vấn ngôn ngữ tự nhiên, và sau đó để thực hiện các nhiệm vụ downstream, được gọi là gọi hàm. Chúng tôi thiết kế khả năng gọi hàm dựa trên các mô hình TigerBot cơ bản, và như ba bước sau.

1. Trích xuất: với một định nghĩa hàm, LLM cơ bản được prompt để trích xuất các đối số hàm từ một truy vấn người dùng. Chúng tôi soạn prompt như: F:{func}+"\n"+Q:{query}+"\n"+"Extract args from Q per F."+"\n"+JSON:.

2. Gọi: sau đó chúng tôi gọi hàm target với các đối số được trích xuất để nhận một phản hồi. Các hàm có thể là hệ thống nội bộ hoặc API bên thứ ba từ web, ví dụ: báo giá cổ phiếu và API tra cứu thời tiết.

3. Sinh tạo: với phản hồi hàm được trả về, chúng tôi prompt LLM lại như: {func response}+"\n"+{query} để nhận câu trả lời cuối cùng bằng ngôn ngữ tự nhiên.

Hiệu suất end-to-end của gọi hàm phần lớn dựa vào khả năng của LLM trong hiểu ngôn ngữ tự nhiên và trích xuất cấu trúc. Với mục đích này, chúng tôi cố tình trộn một phần nhẹ dữ liệu trích xuất đa mục đích trong pretraining và fine-tuning. Chúng tôi đã quan sát hiệu suất khá thỏa mãn cho một số nhiệm vụ gọi hàm cơ bản, ví dụ: tính toán toán học và tra cứu báo giá cổ phiếu. Do đó, chúng tôi tin rằng với việc fine-tuning thêm trên dữ liệu trích xuất cụ thể lĩnh vực, khả năng gọi hàm có thể có ích trong nhiều ứng dụng thực tế, đặc biệt là thay thế cho những hệ thống legacy tốn kém và phức tạp chỉ để trích xuất cấu trúc.

Tìm kiếm trực tuyến LLM có thể được tăng cường bởi tìm kiếm, để có được ngữ cảnh factoid và thời gian thực, cũng đến mức độ nào đó để giảm thiểu vấn đề ảo giác. Chúng tôi triển khai tăng cường tìm kiếm như sau.

1. Tiền xử lý và tìm kiếm: chúng tôi đầu tiên tiền xử lý truy vấn người dùng để phù hợp với các công cụ tìm kiếm hiện đại, ví dụ: loại bỏ các thán từ nói và giải quyết tham chiếu thời gian, sau đó đưa ra truy vấn cho các công cụ tìm kiếm để nhận kết quả.

2. Lọc chất lượng và phân tích: sau đó chúng tôi lọc kết quả tìm kiếm thành 1-3 ứng viên hàng đầu dựa trên một số điều kiện chất lượng và tính kịp thời, ví dụ: chất lượng trang web và nếu kết quả trong một tuần hiện tại, loại bỏ những cái cũ hơn một tháng. Chúng tôi cũng phân tích nội dung liên quan từ những kết quả có cấu trúc đó, ví dụ: thời tiết và giá cổ phiếu.

3. Sinh tạo: cuối cùng chúng tôi sinh phản hồi cho người dùng bằng cách prompt LLM cơ bản với: {top search results}+"\n"+{query}.

Đóng vai Với sự thể hiện phong phú về kiến thức tổng quát và khả năng đối thoại, NPC (nhân vật không do người chơi điều khiển) trong các trò chơi RPG có thể được trang bị LLM để trở nên giải trí hơn. Một yêu cầu phổ biến từ các ứng dụng game là làm cho LLM hoạt động như một số vai trò, ví dụ: đối thoại và trí nhớ cá nhân. Để phát triển một LLM đóng vai, có hai mục tiêu thiết kế: (1) huấn luyện một LLM với khả năng đóng vai, và (2) thích ứng LLM vào ngữ cảnh của trò chơi. Đồng thời, các nhà phát triển thường muốn giữ lại khả năng tổng quát của LLM để làm cho NPC giống con người hơn. Hơn nữa, trong thực tế phương pháp cần phải có mức độ chắc chắn, nhẹ và có thể mở rộng. Phương pháp của chúng tôi đối với LLM đóng vai kết hợp fine-tuning và retrieval-augmented generation (RAG) như sau, và quá trình được minh họa trong Hình 5. Phương pháp của chúng tôi được lấy cảm hứng từ [15], nhưng chúng tôi thêm bước fine-tuning để có một nền tảng được game hóa.

1. Fine-tuning: chúng tôi fine-tune liên tục một mô hình chat TigerBot trên một bộ dữ liệu đối thoại đóng vai đa vòng đa mục đích, ví dụ: hoạt động như một anh hùng trong một cuốn tiểu thuyết. Fine-tuning có thể đo lường mô hình để có khả năng đóng vai, mặc dù tốn thời gian và tính ngẫu nhiên.

2. Trích xuất: với một cuốn tiểu thuyết hoặc cốt truyện làm bối cảnh nền của một trò chơi, chúng tôi trích xuất đối thoại giữa các nhân vật và tóm tắt hồ sơ của họ, cả hai sử dụng một mô hình chat TigerBot đa mục đích. Đối thoại và hồ sơ được trích xuất được đưa vào một chỉ mục embedding như cơ sở tri thức trò chơi, dưới một hệ thống phân cấp bắt đầu với "role".

3. Suy luận: trong quá trình suy luận runtime, với vai trò và câu hỏi của người dùng, chúng tôi đầu tiên truy xuất dày đặc các đối thoại tham chiếu và hồ sơ từ cơ sở tri thức, và sau đó sử dụng LLM đóng vai được fine-tune ở trên để sinh phản hồi. Prompt được soạn như: {role profiles}+"\n"+{reference dialogues}+"\n"+{question}.

Phương pháp của chúng tôi được thiết kế sao cho nó có thể được mở rộng cho các trò chơi khác nhau nhanh chóng và kinh tế. Khả năng đóng vai là tổng quát và được học thông qua fine-tuning, trong khi trích xuất cơ sở tri thức là cụ thể cho trò chơi và nhanh. Một ví dụ được hiển thị trong Hình 6 là từ một trò chơi đóng vai được phát triển trong 2-3 ngày và mất một node của 8×A100 GPU.

Phần cứng thông minh Phần cứng thông minh có thể được trang bị LLM để đầu tiên có giao diện ngôn ngữ tự nhiên và có thể điều phối các ứng dụng khác sử dụng khả năng như gọi hàm. Trong thực tế của chúng tôi ở giai đoạn này, có ba yêu cầu điển hình như sau.

1. Phản hồi tức thì: phần cứng thông minh, như loa, thiết bị đeo, và xe điện, chủ yếu được sử dụng khi di chuyển, do đó yêu cầu thời gian phản hồi gần thời gian thực. Thời gian phản hồi end-to-end của hệ thống thường không thể vượt quá 2s, trong khi có các quá trình khác liên quan bao gồm ASR, TTS, v.v. Để đạt được phản hồi nhanh, chúng tôi sử dụng sinh tạo streaming, cùng với engine suy luận tối ưu TGI (vLLM và KV cache) như mô tả trong Phần 2.2.

2. Câu hỏi thường gặp (FAQ): phần cứng thông minh được thiết kế cho các tình huống cụ thể. Để làm cho người dùng quen với cài đặt, LLM cần có khả năng trả lời một bộ FAQ, ví dụ: Nhãn hiệu và model của xe là gì?. Chúng tôi đầu tiên fine-tune liên tục một mô hình chat TigerBot trên dữ liệu lĩnh vực, ví dụ: hướng dẫn sử dụng. Sau đó trong quá trình suy luận, chúng tôi sinh câu trả lời được tăng cường bởi truy xuất từ một cơ sở tri thức chứa đầy FAQ được chú thích. Cơ sở tri thức có thể được giữ cập nhật dễ dàng.

--- TRANG 8 ---
Hình 5: Một phương pháp có thể mở rộng cho LLM đóng vai của Tigerbot

Hình 6: Một ví dụ screenshot của Tigerbot đóng vai với cuốn tiểu thuyết "Genshin Impact"

3. Cá nhân hóa: phần cứng trở nên sống động và giải trí hơn sẽ được miêu tả như một nhân vật thực tế. Cá nhân hóa được thể hiện bởi một bộ dữ liệu có chú thích mô tả hồ sơ cô ấy (tên và giới tính) và phong cách (vui vẻ hoặc chuyên nghiệp). Sau đó chúng tôi fine-tune một mô hình chat TigerBot trên bộ dữ liệu cá nhân hóa cho một đến hai epoch. Để bảo toàn khả năng đa mục đích ban đầu, dữ liệu fine-tuning được trộn với một số dữ liệu tổng quát. Một quy tắc thông thường cho tỷ lệ hỗn hợp là dữ liệu tổng quát chiếm hơn một nửa.

--- TRANG 9 ---
4 Kết luận
Trong công trình này, chúng tôi đã giới thiệu TigerBot, một gia đình LLM pretrained và chat với kích thước tham số từ 7B đến 180B. TigerBot đã đạt được hiệu suất SOTA với dữ liệu huấn luyện chất lượng cao và một ngăn xếp phương pháp và hệ thống huấn luyện tiên tiến. Chúng tôi cũng đặt trọng tâm đặc biệt vào các ứng dụng thực tế, với các triển khai chi tiết và quan sát của chúng tôi cho một phổ công cụ và tình huống thực tế. Chúng tôi tin tưởng mạnh mẽ vào đổi mới mở, và công việc của chúng tôi đã được hưởng lợi rất lớn từ cộng đồng nguồn mở LLM. Tương tự, chúng tôi hy vọng rằng công việc của chúng tôi có thể đóng góp cho cộng đồng cho nghiên cứu và phát triển lý thuyết và thực tế trong tương lai.

Sự xuất hiện của LLM đã đánh dấu một trong những khoảnh khắc rung động trái tim nhất trong nhiều thập kỷ lịch sử phát triển AI, phần lớn xét về hiệu suất áp đảo của nó với tính tổng quát cực kỳ trong khi lại đơn giản để xây dựng. Chúng tôi cảm thấy không kém phần khiêm tôn. Từ khối lượng lớn thí nghiệm và triển khai của chúng tôi, tuy nhiên, chúng tôi tin rằng chúng ta vẫn đang trong giai đoạn sơ khai của LLM và rộng hơn là sự tiến hóa AGI. Có những thách thức trong các giải pháp đáng tin cậy cho các nhiệm vụ quan trọng nhiệm vụ, hạ tầng bền vững, giá trị người dùng đột phá không tăng dần từ các ứng dụng thực tế, để chỉ đặt tên một vài. Hành trình phía trước thú vị nhưng cũng gian khổ. Hãy bình tĩnh và lập trình vui vẻ.

Tài liệu tham khảo
[1] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, và S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv:2305.13245 [cs.CL], 05 2023.

[2] Anthropic. Claude 2. https://www.anthropic.com/index/claude-2, 06 2023.

[3] J. Berant, A. Chou, R. Frostig, và P. Liang. Semantic parsing on freebase from question-answer pairs. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013.

[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei. Language models are few-shot learners. arXiv:2005.14165v4 [cs.CL], 05 2020.

[5] H. Chen, R. Pasunuru, J. Weston, và A. Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. arXiv:2310.05029 [cs.CL], 10 2023.

[6] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, và O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457 [cs.AI], 03 2018.

[7] O. Contributors. Opencompass: A universal evaluation platform for foundation models. GitHub repository, 2023.

[8] Y. Cui, Z. Yang, và X. Yao. Efficient and effective text encoding for chinese llama and alpaca. arXiv:2304.08177 [cs.CL], 04 2023.

[9] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, và C. Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. arXiv:2205.14135 [cs.LG], 05 2022.

[10] Google. Sentencepiece. GitHub repository, 2023.

[11] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, và W. Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685 [cs.CL], 06 2021.

[12] Huggingface. Text generation inference. GitHub repository, 2023.

[13] Huggingface. Transformers. GitHub repository, 2023.

[14] V. Karpukhin, B. Oğuz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, và W. tau Yih. Dense passage retrieval for open-domain question answering. EMNLP 2020, 04 2020.

[15] C. Li, Z. Leng, C. Yan, J. Shen, H. Wang, W. MI, Y. Fei, X. Feng, S. Yan, H. Wang, L. Zhan, Y. Jia, P. Wu, và H. Sun. Chatharuhi: Reviving anime character in reality via large language model. arXiv:2308.09597 [cs.CL], 2023.

[16] Microsoft. Megatron-deepspeed. GitHub repository, 2023.

[17] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. A. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, và M. Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. arXiv:2104.04473 [cs.CL], 04 2021.

[18] NVIDIA. Tensorrt open source software. GitHub repository, 2023.

[19] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, và R. Lowe. Training language models to follow instructions with human feedback. arXiv:2203.02155v1 [cs.CL], 03 2022.

[20] O. Peckham. Meta completes research supercluster, announces next-gen datacenter. HPCwire: https://www.hpcwire.com/2023/05/18/meta-completes-research-supercluster-announces-next-gen-datacenter/, 05 2023.

[21] B. Peng, J. Quesnelle, H. Fan, và E. Shippole. Yarn: Efficient context window extension of large language models. arXiv:2309.00071 [cs.CL], 09 2023.

[22] S. Pichai. An important next step on our ai journey. https://blog.google/technology/ai/bard-google-ai-search-updates/, 02 2023.

[23] O. Press, N. A. Smith, và M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv:2108.12409 [cs.CL], 08 2021.

[24] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, và C. Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv:2305.18290 [cs.LG], 05 2023.

[25] S. Rajbhandari, J. Rasley, O. Ruwase, và Y. He. Zero: Memory optimizations toward training trillion parameter models. arXiv:1910.02054 [cs.LG] và In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20), 10 2019.

[26] P. Rajpurkar, R. Jia, và P. Liang. Know what you don't know: Unanswerable questions for squad. arXiv:1806.03822 [cs.CL], 06 2018.

[27] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Levkovizh, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elsahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmumin, I. Johnson, I. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. Almubarak, K. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan, L. B. allal, L. Tanguy, M. Dey, M. R. Muñoz, M. Masoud, M. Grandury, M. Šaško, M. Huang, M. Coavoux, M. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis, O. Nguyen, O. Espejel, O. de Gibert, P. Villegas, và et al. (293 tác giả bổ sung không được hiển thị). Bloom: A 176b-parameter open-access multilingual language model. arXiv:2211.05100 [cs.CL], 11 2022.

[28] N. Shazeer. Glu variants improve transformer. arXiv:2002.05202 [cs.LG], 02 2020.

[29] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, và Y. Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv:2104.09864 [cs.CL], 04 2021.

[30] A. Talmor, J. Herzig, N. Lourie, và J. Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv:1811.00937 [cs.CL], 11 2018.

[31] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, và T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. GitHub repository, 2023.

[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, và G. Lample. Llama: Open and efficient foundation language models. arXiv:2302.13971 [cs.CL], 02 2023.

[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, và T. Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288 [cs.CL], 07 2023.

[34] Turboderp. Exllamav2. GitHub repository, 2023.

[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, và I. Polosukhin. Attention is all you need. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA., 06 2017.

[36] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, và P. Christiano. Recursively summarizing books with human feedback. arXiv:2109.10862 [cs.CL], 09 2021.

[37] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, và H. Ma. Effective long-context scaling of foundation models. arXiv:2309.16039 [cs.CL], 09 2023.

[38] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, và Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv:2206.01861 [cs.CL], 06 2022.
