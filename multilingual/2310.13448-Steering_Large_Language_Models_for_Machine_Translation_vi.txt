# Điều hướng các Mô hình Ngôn ngữ Lớn cho Dịch máy
với Tinh chỉnh và Học trong Ngữ cảnh

Duarte M. Alves1,4 Nuno M. Guerreiro1,2,4,5 João Alves2 José Pombal2
Ricardo Rei2,3,4 José G. C. de Souza2 Pierre Colombo5,6 André F. T. Martins1,2,4
1Instituto de Telecomunicações, Lisbon, Portugal 2Unbabel, Lisbon, Portugal,
3INESC-ID, Lisbon, Portugal 4Instituto Superior Técnico, University of Lisbon, Portugal
5MICS, CentraleSupélec, Université Paris-Saclay, France 6Equall, Paris, France
duartemalves@tecnico.ulisboa.pt

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLMs) là một hướng tiếp cận đầy hứa hẹn cho dịch máy (MT). Tuy nhiên, các hệ thống MT dựa trên LLM hiện tại khá mỏng manh: hiệu quả của chúng phụ thuộc cao vào việc lựa chọn các ví dụ few-shot và chúng thường đòi hỏi thêm xử lý hậu kỳ do vấn đề tạo quá nhiều. Các phương án thay thế như tinh chỉnh trên hướng dẫn dịch thuật tốn kém về mặt tính toán và có thể làm yếu khả năng học trong ngữ cảnh, do sự chuyên môn hóa quá mức. Trong bài báo này, chúng tôi cung cấp một cái nhìn kỹ hơn về vấn đề này. Chúng tôi bắt đầu bằng cách cho thấy rằng tinh chỉnh dựa trên adapter với LoRA có hiệu suất tương đương với tinh chỉnh truyền thống trong khi giảm số lượng tham số huấn luyện đi 50 lần. Phương pháp này cũng vượt trội hơn việc prompting few-shot và loại bỏ nhu cầu xử lý hậu kỳ hoặc ví dụ trong ngữ cảnh. Tuy nhiên, chúng tôi chỉ ra rằng tinh chỉnh thường làm giảm hiệu suất few-shot, cản trở khả năng thích ứng. Cuối cùng, để có được điều tốt nhất từ cả hai thế giới, chúng tôi đề xuất một phương pháp đơn giản kết hợp các ví dụ few-shot trong quá trình tinh chỉnh. Các thí nghiệm trên 10 cặp ngôn ngữ cho thấy rằng phương pháp đề xuất của chúng tôi khôi phục lại khả năng few-shot ban đầu trong khi giữ được những lợi ích thêm vào từ tinh chỉnh.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLMs) đã cho thấy hiệu suất đáng chú ý trên một loạt các tác vụ NLP bằng cách tận dụng học trong ngữ cảnh (Brown et al., 2020). Đặc biệt, khi được cung cấp các ví dụ few-shot, những mô hình này đã thể hiện khả năng ấn tượng trong việc thực hiện dịch máy (MT) mà không cần giám sát rõ ràng trên dữ liệu song song (Garcia et al., 2023). Tuy nhiên, phương pháp này có một số nhược điểm: hiệu suất phụ thuộc cao vào chất lượng của các ví dụ (Vilar et al., 2022), đầu ra bị ảnh hưởng bởi việc tạo quá nhiều (Bawden và Yvon, 2023), và chi phí suy luận được tăng lên đáng kể do phải xử lý tất cả các cặp đầu vào.

Khi có sẵn dữ liệu song song, các LLM có thể được tinh chỉnh trên các hướng dẫn dịch thuật (Li et al., 2023). Phương pháp này thường vượt trội hơn việc prompting few-shot và loại bỏ nhu cầu về các ví dụ trong ngữ cảnh. Tuy nhiên, vẫn chưa rõ liệu các mô hình đã tinh chỉnh có thể hưởng lợi từ các đặc tính mong muốn của học trong ngữ cảnh, chẳng hạn như thích ứng miền tức thời (Agrawal et al., 2022). Thêm vào đó, tinh chỉnh truyền thống (Devlin et al., 2019; Radford et al., 2018) phát sinh chi phí tính toán cao do phải cập nhật tất cả các trọng số của mô hình.

Trong bài báo này, chúng tôi cung cấp một nghiên cứu kỹ hơn về tác động của tinh chỉnh và prompting few-shot để thích ứng các LLM thực hiện dịch thuật. Các thí nghiệm của chúng tôi bao gồm 10 cặp ngôn ngữ trên các miền tổng quát và cụ thể, bao gồm hơn 100.000 bản dịch được tạo ra (§2). Những phát hiện chính của chúng tôi là:

• Chúng tôi cho thấy tinh chỉnh với adapters (Houlsby et al., 2019; Hu et al., 2022) là một phương pháp rất hiệu quả để điều hướng các LLM cho dịch thuật (§3.1). Phương pháp này có hiệu suất tương đương với tinh chỉnh truyền thống với chi phí tính toán chỉ bằng một phần nhỏ, bằng cách huấn luyện ít hơn 50 lần số lượng tham số. Nó cũng đạt được chất lượng dịch thuật tốt hơn so với học trong ngữ cảnh và loại bỏ nhu cầu xử lý hậu kỳ các đầu ra được tạo ra và lựa chọn các ví dụ trong ngữ cảnh.

• Chúng tôi cho thấy việc tinh chỉnh các mô hình ngôn ngữ lớn làm giảm hiệu suất few-shot của chúng, hạn chế khả năng thích ứng (§3.2). Đặc biệt, chúng tôi cho thấy các LLM đã tinh chỉnh hoạt động kém trong các kịch bản thích ứng miền khi được cung cấp các ví dụ trong ngữ cảnh.

• Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp đơn giản giới thiệu các ví dụ few-shot trong quá trình tinh chỉnh (§4). Kết quả của chúng tôi cho thấy chúng ta có thể khôi phục khả năng few-shot trong khi vẫn giữ được những lợi ích của tinh chỉnh.

## 2 Thiết lập Thí nghiệm

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng LLaMA 7B và 13B (Touvron et al., 2023) làm mô hình ngôn ngữ nền tảng và tinh chỉnh chúng với hàm mất mát entropy chéo tiêu chuẩn.

Chúng tôi huấn luyện mô hình của mình trên dữ liệu miền tổng quát OPUS (Tiedemann, 2012) từ các miền Europarl, Globalvoices, Paracrawl, Tilde, Ubuntu, và Wikipedia. Chúng tôi xem xét các ngôn ngữ tiếng Hà Lan (nl), tiếng Pháp (fr), tiếng Đức (de), tiếng Bồ Đào Nha (pt) và tiếng Nga (ru), cả từ và sang tiếng Anh (en). Để đảm bảo chất lượng của các bản ghi huấn luyện, trước tiên chúng tôi áp dụng Bicleaner (Ramírez-Sánchez et al., 2020) sử dụng ngưỡng 0.85 và sau đó lọc các cặp còn lại, đảm bảo cả hai hướng ngôn ngữ đều có điểm số COMETKiwi (Rei et al., 2022b) trên 0.8. Cuối cùng, chúng tôi lấy mẫu 250K bản ghi cho mỗi cặp ngôn ngữ. Trong quá trình huấn luyện, chúng tôi lấy mẫu đều từ dữ liệu để đảm bảo mỗi cặp ngôn ngữ được thấy số lần tương tự. Chúng tôi thực hiện xác thực trên tập phát triển Flores-200 cho các cặp ngôn ngữ trong dữ liệu huấn luyện.

Để đánh giá trong miền, chúng tôi xem xét tập dữ liệu kiểm tra Flores-200 (NLLB Team et al., 2022) trên tất cả các hướng dịch thuật được bao gồm trong quá trình huấn luyện, cũng như các tập kiểm tra WMT22 cho các cặp ngôn ngữ được xem xét trong dữ liệu huấn luyện của chúng tôi. Về dữ liệu cho các miền chuyên biệt, chúng tôi xem xét các miền Y tế và Pháp lý từ Aharoni và Goldberg (2020), tập dữ liệu TICO (Anastasopoulos et al., 2020) và WMT Chat (Farinha et al., 2022). Chúng tôi đánh giá mô hình của mình trong các thiết lập zero và five shot, lấy mẫu đều cho mỗi câu kiểm tra năm mẫu few-shot độc lập từ tập phát triển tương ứng.

Chỉ số đánh giá chính của chúng tôi là COMET (Rei et al., 2020, 2022a). Chúng tôi cũng báo cáo kết quả với BLEU (Papineni et al., 2002), chrF (Popovic, 2015) và COMETKiwi (Rei et al., 2022b) trong Phụ lục G.

Chúng tôi tham khảo Phụ lục A để biết chi tiết đầy đủ về các siêu tham số và định dạng hướng dẫn được sử dụng trong các thí nghiệm sau.

## 3 Tinh chỉnh LLMs trên hướng dẫn MT

Trong phần này, chúng tôi điều tra hiệu suất của các LLM được tinh chỉnh trên hướng dẫn dịch máy liên quan đến prompting few-shot với mô hình ngôn ngữ được huấn luyện trước.

Lưu ý rằng, trong suốt phần này, chúng tôi luôn phân tích prompting few-shot cho mô hình được huấn luyện trước. Chúng tôi cho rằng điều này cung cấp sự so sánh công bằng hơn với tinh chỉnh trên hướng dẫn dịch thuật, vì cả hai phương pháp đều có quyền truy cập vào các ví dụ huấn luyện. Tuy nhiên, chúng tôi cũng cung cấp kết quả cho dịch thuật zero-shot với mô hình được huấn luyện trước trong Phụ lục G. Tương tự với các phát hiện trong Bawden và Yvon (2023), hiệu suất zero-shot kém hơn nhiều so với hiệu suất few-shot, đặc biệt là đối với các cặp ngôn ngữ ngoài tiếng Anh, có thể do sự phổ biến của dữ liệu tiếng Anh trong quá trình huấn luyện trước các mô hình LLaMA.

### 3.1 Tinh chỉnh hiệu quả với LoRA

Chúng tôi bắt đầu bằng cách nghiên cứu huấn luyện hiệu quả tham số với thích ứng hạng thấp (LoRA) (Hu et al., 2022) và so sánh nó với tinh chỉnh truyền thống.

Trong Hình 1, chúng tôi quan sát thấy rằng LoRA hoạt động tương đương với tinh chỉnh truyền thống trong khi huấn luyện ít hơn 50 lần số lượng tham số. Chúng tôi cũng thấy rằng cả LoRA và tinh chỉnh truyền thống đều vượt trội hơn mô hình được huấn luyện trước với prompts few-shot—điều sau cùng phù hợp với các phát hiện trong Li et al. (2023), cho thấy rằng tinh chỉnh dẫn đến bản dịch tốt hơn so với prompting few-shot của các mô hình ngôn ngữ được huấn luyện trước. Như một xu hướng chung, tất cả các phương pháp đều thể hiện chất lượng dịch thuật tốt hơn khi dịch sang tiếng Anh, theo xu hướng gần đây trong tài liệu (Arivazhagan et al., 2019; Vilar et al., 2022).

Chúng tôi cũng thấy rằng tinh chỉnh LoRA chỉ cần một số lượng rất nhỏ bản dịch để có được hiệu suất đã báo cáo, như được hiển thị trong Hình 2. Đặc biệt, nó vượt trội hơn mô hình được huấn luyện trước few-shot chỉ với 2.000 ví dụ huấn luyện.

Xem xét chi phí tính toán cao của tinh chỉnh đầy đủ so với tinh chỉnh hiệu quả tham số và sự suy giảm không đáng kể thu được với mô hình dựa trên LoRA, chúng tôi sử dụng LoRA trong các thí nghiệm tiếp theo.

### 3.2 Prompting few-shot của các mô hình đã tinh chỉnh

Bây giờ chúng tôi hướng sự chú ý của mình đến việc so sánh hiệu suất zero- và five-shot. Chúng tôi lập luận rằng, ngay cả khi một LLM có thể đạt được chất lượng dịch thuật zero-shot cao, khả năng few-shot có thể rất có lợi cho việc thích ứng hiệu quả. Như được chỉ ra bởi Agrawal et al. (2022), các LLM có thể tận dụng một nhóm rất nhỏ các ví dụ few-shot để thực hiện dịch thuật trên các miền mới.

Trong các biểu đồ bên trái nhất của Hình 3, chúng tôi xem xét hiệu suất zero- và few-shot của các mô hình đã tinh chỉnh trên các miền tổng quát. Hiệu suất few-shot bị suy giảm và bị vượt qua bởi hiệu suất zero-shot, cho thấy rằng quy trình tinh chỉnh đang cản trở khả năng học trong ngữ cảnh.

Để nghiên cứu sâu hơn hiện tượng này, chúng tôi đánh giá các mô hình trên trên các miền chuyên biệt. Các ví dụ miền tổng quát có thể ít hữu ích cho một mô hình đã được huấn luyện trên miền đó. Ngược lại, trong các miền chuyên biệt, các ví dụ nên mang lại thông tin cụ thể về miền về các đặc tính của bản dịch, như phong cách, thanh ghi, và do đó giúp mô hình đạt được hiệu suất tốt hơn.

Trong các biểu đồ bên phải nhất của Hình 3, chúng tôi quan sát thấy rằng vấn đề trên xảy ra một cách nhất quán trong tất cả các miền, với sự suy giảm hiệu suất lớn hơn. Phát hiện này hỗ trợ thêm cho giả thuyết của chúng tôi rằng tinh chỉnh có thể làm giảm hiệu suất của prompting few-shot.

## 4 Tinh chỉnh với các ví dụ few-shot

Để khôi phục hiệu suất few-shot, chúng tôi giới thiệu các hướng dẫn với các ví dụ few-shot trong quá trình huấn luyện: cụ thể là, chúng tôi tinh chỉnh trên dữ liệu có chứa cả hướng dẫn zero-shot và few-shot. Theo Min et al. (2022), chúng tôi lấy mẫu đều từ 0 đến 5 ví dụ few-shot cho mỗi ví dụ huấn luyện từ một nhóm ví dụ đã được tách riêng trước đó từ dữ liệu huấn luyện. Từ đây, chúng tôi xây dựng một prompt hướng dẫn với ví dụ huấn luyện và các ví dụ đã chọn và tiến hành huấn luyện.

Trong Hình 3, chúng tôi quan sát thấy rằng các mô hình được huấn luyện với các ví dụ trong ngữ cảnh khôi phục lại khả năng few-shot của chúng, cả cho các miền tổng quát và chuyên biệt. Hiệu suất few-shot ngang bằng hoặc vượt trội hơn hiệu suất zero-shot, tiếp tục gợi ý rằng các mô hình đang trích xuất thông tin hữu ích từ các ví dụ. Trong Phụ lục D, chúng tôi trình bày một tập hợp các ví dụ làm nổi bật những lợi ích này.

### 4.1 Phân tích về định dạng đầu ra

Chúng tôi cũng phân tích xem liệu các mô hình đã tinh chỉnh có tiếp tục tạo ra ngữ cảnh sau bản dịch mong muốn hay không. Vấn đề này có mặt trong đầu ra LLM được huấn luyện trước và đòi hỏi xử lý hậu kỳ nội dung được tạo ra, xóa tất cả các từ được tạo ra sau dòng mới đầu tiên.

Trong Hình 4, chúng tôi hiển thị độ dài của các đầu ra được token hóa cho các mô hình 7B. Chúng tôi quan sát thấy rằng phân phối độ dài cho các đầu ra được tạo ra bởi cả hai mô hình đã tinh chỉnh khớp với phân phối của các tham chiếu. Điều này cho thấy rằng các mô hình đã tinh chỉnh không còn tạo quá nhiều.

Chúng tôi cũng phát hiện rằng những mô hình này không còn phân định đầu ra của chúng bằng ký hiệu dòng mới và thay vào đó tạo ra token kết thúc câu, loại bỏ sự cần thiết xử lý hậu kỳ và tăng hiệu quả tính toán. Trong Phụ lục F, chúng tôi cung cấp một tập hợp các ví dụ để minh họa những phát hiện này.

### 4.2 Ảnh hưởng của các ví dụ trong ngữ cảnh

Để có được phân tích chi tiết hơn về những lợi ích thu được bằng cách thêm các ví dụ trong ngữ cảnh, chúng tôi phân tích sự khác biệt trong điểm số COMET cho mỗi câu nguồn khi prompting các mô hình đã tinh chỉnh 7B với và không có các ví dụ.

Trong Hình 5, chúng tôi quan sát thấy rằng các phân phối có nồng độ cao của các điểm hơi trên 0. Tuy nhiên, chúng tôi cũng quan sát thấy những đuôi rất lớn, đặc biệt là đối với các cặp ngôn ngữ ngoài tiếng Anh.

Chúng tôi kiểm tra thủ công các ví dụ với sự khác biệt cao nhất và thấy rằng việc giới thiệu các ví dụ có thể sửa chữa việc mô hình tạo ra bằng ngôn ngữ sai, hỗ trợ các phát hiện trong Bawden và Yvon (2023). Đáng ngạc nhiên, chúng tôi cũng phát hiện các ví dụ mà mô hình tạo ra chính xác một bản dịch trong kịch bản zero-shot và việc chèn các ví dụ trong ngữ cảnh dẫn đến nội dung ảo giác.

Để đặc trưng hóa tốt hơn hiện tượng này, chúng tôi lấy cảm hứng từ phân tích về ảo giác dưới sự nhiễu loạn (Lee et al., 2018), và đo lường có bao nhiều lần prompting mô hình mà không có ví dụ dẫn đến bản dịch trên 30 điểm BLEU, và việc giới thiệu các ví dụ giảm điểm số xuống dưới 3 (những ngưỡng này được chọn dựa trên công trình trước đó (Lee et al., 2018; Raunak et al., 2021; Guerreiro et al., 2023)).

Trong Bảng 1, chúng tôi thấy rằng các mô hình được tinh chỉnh mà không có ví dụ có tỷ lệ ảo giác cao hơn so với các đối tác tương ứng của chúng, tiếp tục cho thấy sự suy giảm trong hiệu suất few-shot. Thông qua việc kiểm tra thủ công các đầu ra thu được, chúng tôi quan sát thấy rằng các mô hình tạo ra ảo giác thuộc các loại khác nhau. Đặc biệt, chúng tạo ra cả ảo giác tách rời (hoàn toàn và mạnh mẽ) và dao động, và cũng có thể tạo ra bản dịch lệch mục tiêu. Một trường hợp phổ biến là các mô hình sao chép từ hướng dẫn (hoặc từ nguồn hoặc từ các ví dụ).

Các mô hình được tinh chỉnh với các ví dụ few-shot thể hiện tỷ lệ ảo giác thấp hơn, gợi ý rằng quy trình huấn luyện đã giảm sự phổ biến của vấn đề này. Đặc biệt, những mô hình này không còn sao chép từ hướng dẫn. Tuy nhiên, chúng vẫn tạo ra ảo giác và tác động của chúng rất nghiêm trọng. Do đó, chúng tôi tin rằng điều này thúc đẩy nghiên cứu thêm về ảnh hưởng của các ví dụ trong ngữ cảnh và đầu ra được tạo ra.

## 5 Kết luận

Trong bài báo này, chúng tôi cung cấp một nghiên cứu về tinh chỉnh và prompting few-shot để thích ứng các LLM cho dịch thuật. Chúng tôi cho thấy rằng tinh chỉnh dựa trên adapter có hiệu suất tương đương với tinh chỉnh truyền thống trong khi huấn luyện ít hơn 50 lần số lượng tham số. Thêm vào đó, tinh chỉnh với adapters vượt trội hơn prompting few-shot của các mô hình ngôn ngữ lớn và loại bỏ nhu cầu xử lý hậu kỳ đầu ra và các ví dụ trong ngữ cảnh.

Ngoài ra, chúng tôi cho thấy rằng các mô hình đã tinh chỉnh thể hiện hiệu suất kém khi được prompted với các ví dụ trong ngữ cảnh. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp đơn giản kết hợp các prompts few-shot trong quá trình tinh chỉnh. Kết quả của chúng tôi cho thấy chúng ta khôi phục được khả năng few-shot ban đầu và giữ được những lợi ích của tinh chỉnh.

## Hạn chế

Trong bài báo này, chúng tôi tập trung vào các cặp ngôn ngữ tài nguyên cao lấy tiếng Anh làm trung tâm. Vẫn còn là một câu hỏi mở về việc những phát hiện này có khái quát hóa cho các cặp ngôn ngữ không phải tiếng Anh hoặc trong các thiết lập tài nguyên thấp hay không.

Chúng tôi cũng không thực hiện đánh giá của con người về chất lượng dịch thuật do thời gian và chi phí thực hiện nghiên cứu này. Thay vào đó, chúng tôi dựa vào đánh giá của mình trên COMET, một chỉ số tiên tiến để đánh giá MT, và cung cấp kết quả cho các chỉ số khác trong Phụ lục G.

## Tuyên bố Đạo đức

Bài báo này dựa trên các mô hình ngôn ngữ lớn. Những mô hình này có thể bao gồm một số rủi ro, được thảo luận chi tiết trong Brown et al. (2020) và Chowdhery et al. (2022). Cụ thể, chúng được huấn luyện trên các kho văn bản web lớn, có thể chứa nội dung độc hại (Gehman et al., 2020), và có mức tiêu thụ năng lượng cao, đặc biệt là trong quá trình huấn luyện (Strubell et al., 2019).

Thêm vào đó, đánh giá của chúng tôi dựa trên các chỉ số tự động được tinh chỉnh dựa trên sở thích của con người. Trong những trường hợp như vậy, những người chú thích có thể không xem xét các phương án tốt hơn khi đánh giá văn bản được tạo ra và phân loại sai văn bản là chất lượng cao (Bansal et al., 2021).

## Lời cảm ơn

Công trình này được hỗ trợ bởi Horizon Europe Research and Innovation Actions của EU (UTTER, hợp đồng 101070631), bởi dự án DECOLLAGE (ERC-2022-CoG 101088763), bởi Fundação para a Ciência e Tecnologia thông qua hợp đồng UIDB/50008/2020, và bởi Kế hoạch Phục hồi và Khả năng chống chịu của Bồ Đào Nha thông qua dự án C645008882- 00000055 (Center for Responsible AI). Một phần của công trình này được thực hiện sử dụng tài nguyên HPC từ GENCI-IDRIS (Grants 2022-AD01101838, 2023-103256 và 2023-101838).
