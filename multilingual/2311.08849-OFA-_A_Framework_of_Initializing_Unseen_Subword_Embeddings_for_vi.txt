OFA: Một Framework Khởi tạo Embedding Subword Chưa Được Thấy để Tiếp tục Huấn luyện Trước Đa ngôn ngữ Quy mô lớn Hiệu quả

Yihong Liu*⋄, Peiqin Lin*⋄, Mingyang Wang*†, và Hinrich Schütze*⋄
*Trung tâm Xử lý Thông tin và Ngôn ngữ, LMU Munich
⋄Trung tâm Học máy Munich (MCML)
†Trung tâm Trí tuệ Nhân tạo Bosch
{yihong, linpq, mingyang}@cis.lmu.de

Tóm tắt
Thay vì huấn luyện trước các mô hình ngôn ngữ đa ngôn ngữ từ đầu, một phương pháp hiệu quả hơn là điều chỉnh các mô hình ngôn ngữ đã được huấn luyện trước (PLM) sang ngôn ngữ mới thông qua mở rộng từ vựng và tiếp tục huấn luyện trước. Tuy nhiên, phương pháp này thường khởi tạo ngẫu nhiên các embedding của subword mới và đưa vào mô hình nhiều tham số embedding hơn đáng kể, do đó làm giảm hiệu quả. Để giải quyết các vấn đề này, chúng tôi đề xuất một framework mới: OneFor All (OFA), khởi tạo khôn ngoan các embedding của subword chưa được thấy và do đó có thể điều chỉnh PLM sang nhiều ngôn ngữ một cách hiệu quả và hiệu suất. OFA tận dụng các vector từ tĩnh đa ngôn ngữ bên ngoài được căn chỉnh tốt và tiêm kiến thức căn chỉnh vào các embedding subword. Ngoài ra, OFA áp dụng phân tích ma trận và thay thế các embedding cồng kềnh bằng hai ma trận chiều thấp hơn, điều này làm giảm đáng kể số lượng tham số. Chúng tôi cho thấy OFA tăng tốc sự hội tụ của việc tiếp tục huấn luyện trước, điều này thân thiện với môi trường vì tạo ra ít dấu chân carbon hơn nhiều. Thông qua các thí nghiệm mở rộng, chúng tôi chứng minh OFA có thể đạt được hiệu suất cạnh tranh hoặc tốt hơn so với các baseline tiếp tục huấn luyện trước mặc định trên một loạt các nhiệm vụ đa ngôn ngữ hạ nguồn. Chúng tôi công bố mã và mô hình của mình.

1 Giới thiệu
Các mô hình PLM đa ngôn ngữ (mPLM), như mBERT (Devlin et al., 2019) và XLM-R (Conneau et al., 2020), đã thể hiện khả năng chuyển giao đa ngôn ngữ zero-shot đáng chú ý (Huang et al., 2019; Artetxe et al., 2020). Tức là, chỉ với việc tinh chỉnh trong một số ngôn ngữ (tài nguyên cao) để thực hiện một nhiệm vụ, mô hình đa ngôn ngữ có thể được áp dụng trực tiếp cho các ngôn ngữ khác (tài nguyên thấp). Tuy nhiên, việc huấn luyện các mPLM như vậy từ đầu đòi hỏi dữ liệu khổng lồ của các ngôn ngữ khác nhau, và quan trọng nhất là tài nguyên tính toán và năng lượng đáng kể (Wang et al., 2019; Bender et al., 2021; Zhou et al., 2023). Do đó, việc tiếp tục huấn luyện trước từ các mô hình hiện có đã trở thành một lựa chọn tốt (Wang et al., 2022; Alabi et al., 2022; ImaniGooghari et al., 2023). Tuy nhiên, hai vấn đề thường bị bỏ qua trong bối cảnh tiếp tục huấn luyện trước đa ngôn ngữ với mở rộng từ vựng: (a) việc khởi tạo ngẫu nhiên các embedding cho subword mới không tích cực sử dụng bất kỳ kiến thức từ vựng nào được mã hóa trong mô hình; (b) việc đưa vào nhiều tham số mới có thể gây ra vấn đề hiệu quả.

Về (a), phương pháp khởi tạo ngẫu nhiên mặc định lấy mẫu từ một phân phối cho trước, ví dụ như Gaussian (Hewitt, 2021; de Vries và Nissim, 2021; Marchisio et al., 2023), không tích cực sử dụng kiến thức từ vựng của các embedding gốc. Để tận dụng tốt hơn kiến thức hiện có, một số nghiên cứu gần đây đề xuất khởi tạo các embedding cho subword ngôn ngữ đích bằng cách khai thác cả vector từ tĩnh đa ngôn ngữ bên ngoài và các embedding PLM gốc (Tran, 2020; Minixhofer et al., 2022; Dobler và de Melo, 2023). Thật không may, các phương pháp này hoặc là bilingualizing một PLM hoặc tạo một LM đơn ngôn ngữ mới cho một ngôn ngữ đích duy nhất tại một thời điểm, điều này không lý tưởng trong bối cảnh tiếp tục huấn luyện trước đa ngôn ngữ. Do đó, mục tiêu của chúng tôi là điều chỉnh đến nhiều ngôn ngữ cùng một lúc và khởi tạo khôn ngoan các embedding subword mới cho việc tiếp tục huấn luyện trước đa ngôn ngữ quy mô lớn.

Về (b), việc điều chỉnh đến nhiều ngôn ngữ hơn chắc chắn sẽ đưa vào nhiều tham số hơn. Theo Chung et al. (2021), ma trận embedding của các mô hình đa ngôn ngữ chiếm khoảng 50% tổng số tham số của mô hình. Tỷ lệ này có thể tăng thêm khi thêm nhiều subword mới hơn như hệ quả của việc điều chỉnh đến nhiều ngôn ngữ hơn. Ví dụ, XLM-V (Liang et al., 2023) tăng từ vựng lên 901K, dẫn đến các embedding chiếm khoảng 90% tổng số tham số. Trong thiết lập đơn ngôn ngữ, tham số hóa embedding được phân tích cho thấy hiệu quả mà không hy sinh nhiều hiệu suất (Lan et al., 2020). Do đó, một phương pháp tương tự được kỳ vọng sẽ thành công trong các mô hình đa ngôn ngữ, cho rằng các embedding vốn dĩ thừa hơn: các từ từ các ngôn ngữ khác nhau chỉ cùng một khái niệm thường có các biểu diễn tương tự. Do đó, chúng tôi nhằm mục đích giảm số lượng tham số trong các embedding thông qua tham số hóa được phân tích.

Để đạt được điều này, chúng tôi giới thiệu OFA, một framework khởi tạo khôn ngoan các embedding của subword mới với tham số hóa được phân tích cho việc tiếp tục huấn luyện trước đa ngôn ngữ quy mô lớn hiệu quả. OFA đầu tiên phân tích các embedding của PLM nguồn và sử dụng hai ma trận nhỏ hơn để thay thế nó. Trong không gian chiều thấp, các embedding của subword mới không được chia sẻ được biểu diễn như sự kết hợp của các embedding của một số subword từ PLM nguồn, được trọng số bởi độ tương tự được trích xuất từ các vector đa ngôn ngữ tĩnh bên ngoài được căn chỉnh tốt (Liu et al., 2023a) bao phủ 1.335 ngôn ngữ. Các embedding của subword được chia sẻ được sao chép trực tiếp. Cuối cùng, OFA sao chép tất cả các tham số không phải embedding của mô hình PLM nguồn và thay thế tokenizer nguồn (tokenizer của PLM nguồn) bằng tokenizer đích (tokenizer sau mở rộng từ vựng).

Chúng tôi sử dụng một PLM đơn ngôn ngữ, tức là RoBERTa (Liu et al., 2019) và một PLM đa ngôn ngữ, tức là XLM-R (Conneau et al., 2020) làm mô hình nguồn. Đầu tiên chúng tôi áp dụng OFA cho các mô hình này và sau đó tiếp tục huấn luyện trước các mô hình kết quả trên corpus Glot500-c (ImaniGooghari et al., 2023). Các mô hình cuối cùng được đánh giá trên một tập hợp đa dạng các nhiệm vụ hạ nguồn, bao gồm truy xuất câu, phân loại văn bản và gán nhãn chuỗi. OFA không chỉ tăng tốc sự hội tụ của việc tiếp tục huấn luyện trước do đó tạo ra ít dấu chân carbon hơn nhiều, mà còn đạt được hiệu suất cạnh tranh hoặc tốt hơn trên tất cả các nhiệm vụ so với các baseline được khởi tạo ngẫu nhiên hoặc chiều đầy đủ, như được thể hiện trong Hình 1.

Các đóng góp của công trình này như sau: (i) Chúng tôi đề xuất OFA, một framework khởi tạo khôn ngoan các embedding của subword chưa được thấy với tham số hóa phân tích, nhắm vào việc tiếp tục huấn luyện trước đa ngôn ngữ hiệu quả. (ii) Chúng tôi tiến hành các thí nghiệm mở rộng và được kiểm soát nghiêm ngặt trên một loạt các nhiệm vụ hạ nguồn và cho thấy OFA hiệu quả và thúc đẩy chuyển giao đa ngôn ngữ. (iii) Chúng tôi cho thấy OFA hiệu quả và thân thiện với môi trường: đạt được hiệu suất tốt hơn với ít tiêu thụ GPU hơn và ít dấu chân carbon hơn.

2 Nghiên cứu Liên quan
Thường có hai cách để có được một mPLM. Cách đầu tiên là huấn luyện trước một mô hình từ đầu trực tiếp trên một số ngôn ngữ với một mục tiêu tự học cụ thể, ví dụ như mô hình hóa ngôn ngữ có mặt nạ (MLM) (Devlin et al., 2019). Các mô hình điển hình áp dụng chiến lược như vậy là các mô hình chỉ mã hóa như mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), IndicBERT (Kakwani et al., 2020), AfriBERTa (Ogueji et al., 2021) và XLM-V (Liang et al., 2023), các mô hình chỉ giải mã như XGLM (Lin et al., 2022), mGPT (Shliazhko et al., 2022) và BLOOM (Scao et al., 2022), và các mô hình mã hóa-giải mã như mBART (Liu et al., 2020) và mT5 (Xue et al., 2021). Cách thay thế là sử dụng các mPLM có sẵn công khai làm mô hình nguồn và tiếp tục huấn luyện trước chúng trên một tập hợp các ngôn ngữ đích (Wang et al., 2022; Alabi et al., 2022; ImaniGooghari et al., 2023). Phương pháp tiếp tục huấn luyện trước này được ưa chuộng vì nó tiêu thụ ít tài nguyên hơn so với việc huấn luyện từ đầu, điều này quan trọng khi ngân sách tính toán bị hạn chế do kích thước mô hình ngày càng tăng (Tay et al., 2022; Gupta et al., 2023).

Một lý do chính tại sao phương pháp tiếp tục huấn luyện trước này hoạt động là khả năng đa ngôn ngữ của các mPLM gốc (Pires et al., 2019; K et al., 2020; Chai et al., 2022). Với khả năng này, trong quá trình tiếp tục huấn luyện trước, mô hình có thể tận dụng kiến thức thu được trong giai đoạn huấn luyện trước trước đó như một tiên nghiệm, và điều chỉnh nhanh chóng đến các ngôn ngữ mới. Một số nghiên cứu trước cố gắng tích cực tận dụng kiến thức tiềm ẩn được mã hóa trong các tham số (embedding hoặc thân transformer) của PLM nguồn (Artetxe et al., 2020; Pfeiffer et al., 2021) khi chuyển sang ngôn ngữ mới. Tuy nhiên, các embedding của subword mới được khởi tạo ngẫu nhiên. Gần đây nhất, Tran (2020), Minixhofer et al. (2022) và Dobler và de Melo (2023) khám phá khả năng tận dụng cả embedding PLM nguồn và vector từ đa ngôn ngữ bên ngoài được căn chỉnh tốt để khởi tạo các embedding của subword mới cho một ngôn ngữ đích duy nhất tại một thời điểm. Tuy nhiên, cách loại phương pháp này có thể được áp dụng hiệu quả cho các tình huống đa ngôn ngữ vẫn chưa được khám phá. Nghiên cứu của chúng tôi, trái ngược với nghiên cứu trước đây, nhằm thiết lập một framework để điều chỉnh một PLM, bất kể là đơn ngôn ngữ hay đa ngôn ngữ, đến nhiều ngôn ngữ. Ngoài ra, framework của chúng tôi nhắm đến hiệu quả tham số, điều này thân thiện với ngân sách tính toán hạn chế.

Nghiên cứu của chúng tôi cũng liên quan đến một số phương pháp cố gắng mở rộng từ vựng của PLM cho các nhiệm vụ hạ nguồn cụ thể (Wang et al., 2019; Tai et al., 2020; Hong et al., 2021; Nag et al., 2023). Dòng nghiên cứu này thường học từ vựng bổ sung từ dữ liệu domain mới và do đó chuyên môn hóa PLM cho các domain nhất định. Ngược lại, nghiên cứu của chúng tôi nhằm xây dựng một framework để tăng cường tính đa ngôn ngữ của mPLM cho mục đích chung thay vì tập trung vào các nhiệm vụ hạ nguồn cụ thể. Điều này được thực hiện một phần bằng cách sử dụng các vector từ đa ngôn ngữ bên ngoài mà từ đó một số kiến thức căn chỉnh có thể được tiêm vào các embedding subword mới được khởi tạo. Theo quan điểm này, nghiên cứu của chúng tôi cũng liên quan đến một số phương pháp căn chỉnh sau huấn luyện trước (Pan et al., 2021; Feng et al., 2022; Ji et al., 2023; Liu et al., 2024) sử dụng tương ứng từ, dịch thuật hoặc chuyển âm để cải thiện khả năng chuyển giao đa ngôn ngữ của mPLM.

3 Sơ bộ: Phân tích Embedding
Trước khi bước vào framework OFA, chúng tôi đầu tiên giới thiệu một kỹ thuật chính được sử dụng bởi OFA: phân tích embedding nguồn. Mặc dù bản thân phân tích ma trận không mới và được tận dụng rộng rãi, ví dụ trong ALBERT (Lan et al., 2020) (một mô hình đơn ngôn ngữ) để giảm tiêu thụ bộ nhớ. Thay vào đó, chúng tôi nhìn vào phân tích này từ góc độ đa ngôn ngữ và cung cấp trực giác về lý do tại sao tham số hóa thấp-rank như vậy hiệu quả trong việc tiếp tục huấn luyện trước đa ngôn ngữ quy mô lớn.

Cho các embedding Es∈R|Vs|×D từ một PLM nguồn được huấn luyện trước trên một số ngôn ngữ nguồn S, trong đó Vs là từ vựng subword của nó và D là chiều embedding, chúng tôi đề xuất phân tích ma trận Es thành các embedding chiều thấp Fs∈R|Vs|×D′ và ma trận up-projection trực giao P∈RD′×D: Es≈FsP, trong đó D′< D. P có thể được hiểu như các embedding của một tập hợp D′ khái niệm ngữ nghĩa tiềm ẩn không phụ thuộc ngôn ngữ. Mỗi khái niệm được biểu diễn như một vector D-chiều và các vector này như một tổng thể phục vụ như cơ sở của một không gian ngữ nghĩa trong RD (một không gian con trong RD vì D′< D) cho tất cả các subword. Do đó, chúng tôi gọi P là primitive embeddings.

Fs có thể được coi như tọa độ của tất cả các subword trong Vs trong không gian được trải bởi P. Biểu diễn cuối cùng của một subword v sẽ là sự kết hợp tuyến tính của các primitive embedding theo tọa độ tương ứng Fs{v}: PTFs{v}.

Bằng cách phân tích các embedding thành phần không phụ thuộc ngôn ngữ P và phần phụ thuộc ngôn ngữ Fs, chúng ta có thể giảm số lượng tham số có thể huấn luyện từ |Vs| ×D đến |Vs| ×D′+D′×D. Việc giảm tham số này có thể nổi bật khi D′≪D. Ngoài ra, vì P được chia sẻ giữa các ngôn ngữ, chúng ta chỉ cần tìm tọa độ đích Ft∈R|Vt|×D′ dưới cùng một cơ sở P khi chúng ta muốn điều chỉnh mô hình sang ngôn ngữ mới có từ vựng là Vt. Điều này hiệu quả hơn nhiều so với việc tìm Et∈R|Vt|×D, xem xét |Vt| có thể lớn đáng kể trong thiết lập đa ngôn ngữ. Cuối cùng, bất kỳ tọa độ nào trong Ft có thể được up-project trở lại RD thông qua P, tương ứng với kích thước ẩn của thân transformer của PLM nguồn.

4 Framework OFA
OFA khởi tạo các embedding của subword mới trong tham số hóa phân tích. Ý tưởng cơ bản của OFA như sau. Chúng tôi tận dụng một không gian vector từ đa ngôn ngữ bên ngoài (cung cấp biểu diễn chất lượng cao của cả ngôn ngữ nguồn và đích) để tạo ra một thước đo độ tương tự ngữ nghĩa trên tập hợp chung của subword và từ của cả ngôn ngữ nguồn và đích. Thước đo độ tương tự này sau đó cho phép chúng ta khởi tạo các subword của ngôn ngữ đích với các biểu diễn có ý nghĩa ngữ nghĩa trong không gian embedding PLM nguồn. Chúng tôi thể hiện tóm tắt framework OFA trong Hình 2 và mô tả quá trình từng bước như sau.

Thiết lập Vấn đề. Cho các vector từ tĩnh đa ngôn ngữ bên ngoài được căn chỉnh tốt W (từ vựng V), một PLM nguồn (embedding subword là Es) với tokenizer TOKs (từ vựng Vs) và tokenizer đích TOKt (từ vựng Vt), chúng ta muốn tìm một khởi tạo tốt của embedding cho tất cả subword trong Vt, tức là Ft, có chiều thấp hơn.

Bước 1. Chúng ta phân tích Es từ PLM nguồn thành primitive embeddings P và tọa độ nguồn Fs. P sẽ phục vụ như cơ sở của embedding subword cho tất cả ngôn ngữ, và Fs sẽ được sử dụng để khởi tạo tọa độ đích mong muốn Ft trong Bước 4. Chúng ta đơn giản để Fs=Es cho các mô hình baseline không áp dụng phân tích ma trận cho Es.

Bước 2. Chúng ta sử dụng tokenizer nguồn TOKs để tokenize tất cả từ trong V. Sau đó chúng ta tạo một đồ thị hai phía có hướng giữa các từ trong V và các subword trong Vs có thể được tokenize từ những từ đó. Chúng ta sử dụng ColexNet+ (Liu et al., 2023a) làm vector từ, vì chúng thể hiện tính đa ngôn ngữ rất mạnh và phản ánh độ tương tự khái niệm (Liu et al., 2023b; Ye et al., 2023) trong nhiều ngôn ngữ (xem §C để biết thêm chi tiết về vector từ). Tiếp theo, chúng ta tạo vector của một subword như trung bình của vector của các từ kết nối với subword:

⃗c=1/|N(c)|∑v∈N(c)W{v}

trong đó c là một subword trong đồ thị và N(c) là tập hợp các lân cận của c trong đồ thị (những lân cận này ∈V). Trực giác đằng sau phép tính này là bất kỳ từ nào bao gồm cùng một subword đều liên quan đến khái niệm mà subword đó biểu diễn, và do đó những từ đó nên đóng góp vào biểu diễn của subword. Nếu một subword trong Vs không có trong đồ thị, chúng ta tạo vector của nó là zero. Theo cách này, chúng ta tạo vector cho tất cả subword trong Vs. Chúng ta gọi các vector subword được tạo là Us.

Bước 3. Chúng ta tạo vector subword cho tất cả subword trong Vt theo cách tương tự như mô tả trong Bước 2, sử dụng decoder đích TOKt, tất cả từ trong V, và vector từ đa ngôn ngữ W. Các vector subword được tạo được ký hiệu là Ut. Lưu ý rằng Ut và Us đều trong cùng không gian vector như W, vì cả hai đều được tạo dựa trên W.

Bước 4. Sau đó chúng ta tận dụng tọa độ nguồn Fs, vector subword ngôn ngữ nguồn Us và vector subword ngôn ngữ đích Ut để khởi tạo tọa độ đích Ft. Để bắt đầu, chúng ta xử lý các subword được chia sẻ bởi Vs và Vt. Đối với những subword này, chúng ta đơn giản sao chép tọa độ của chúng từ Fs đến Ft, điều này cũng được thực hiện bởi Dobler và de Melo (2023). Đối với các subword còn lại, có thể từ ngôn ngữ mới và không được bao phủ bởi Vs, chúng ta theo WECHSEL (Minixhofer et al., 2022) để tìm một khởi tạo tốt dựa trên độ tương tự.

Cụ thể, cho mỗi subword x∈Vs và mỗi subword y∈Vt, chúng ta tính độ tương tự cosine giữa x và y trong không gian vector subword:

s(x,y)=cos-sim (Us{x},Ut{y})

Tọa độ của mỗi subword không được chia sẻ trong Vt cuối cùng được khởi tạo như một kết hợp lồi của tọa độ ngôn ngữ nguồn trong Fs:

Ft{y}=∑x∈N(y)exp(s(x,y)/τ)·Fs{x}/∑x′∈N(y)exp(s(x′,y)/τ)

trong đó N(y) là tập hợp k subword ngôn ngữ nguồn gần nhất của subword ngôn ngữ đích y và τ là nhiệt độ (chúng ta đặt k= 10 và τ= 0.1 theo mặc định, theo Minixhofer et al. (2022) báo cáo các lựa chọn tối ưu trong thí nghiệm của họ).

Trong trường hợp vector của một subword y trong Ut là zero, chúng ta khởi tạo ngẫu nhiên tọa độ Ft{y} từ phân phối Gaussian N(E[Fs],Var[Fs]). Lưu ý rằng Ft gần như trong không gian embedding của Fs, thay vì trong không gian vector của Us và Ut.

Bước 5. Cuối cùng chúng ta lắp ráp một mô hình đích bằng cách sử dụng thân transformer của PLM nguồn (tất cả tham số trừ embedding subword của nó), primitive embeddings P, và tọa độ đích được khởi tạo Ft. Chiều của Ft giống như thân transformer nếu không áp dụng phân tích ma trận, ngược lại, chúng ta cần up-project tọa độ với P để phù hợp với chiều ẩn của thân transformer. Theo cách này, chúng ta biến đổi PLM nguồn thành mô hình đa ngôn ngữ có ít tham số hơn, phục vụ như một khởi đầu tốt cho việc tiếp tục huấn luyện trước đa ngôn ngữ hiệu quả.

5 Thí nghiệm
5.1 Thiết lập
Chúng tôi sử dụng tokenizer SentencePiece (Kudo và Richardson, 2018) có kích thước từ vựng 401K làm tokenizer đích. Từ vựng được hợp nhất từ các subword trong XLM-R (Conneau et al., 2020) và subword mới học từ corpus Glot500-c (ImaniGooghari et al., 2023) (Xem §A để biết chi tiết về corpus Glot500-c.). Tokenizer đích giống như tokenizer được sử dụng trong Glot500-m (ImaniGooghari et al., 2023). Sau đó chúng tôi tạo 8 mô hình sử dụng framework OFA như sau:

OFA-mono-xxx: chúng tôi xây dựng các mô hình đích bằng OFA sử dụng English RoBERTA (Liu et al., 2019) làm mô hình nguồn. xxx ký hiệu chiều tiềm ẩn được sử dụng trong phân tích, trong đó phân tích giá trị đơn lẻ (SVD) được sử dụng và các eigenvalue/eigenvector top-k được chọn. Chúng tôi sử dụng bốn chiều khác nhau: 100, 200, 400 và 768. Khi chiều là 768, không áp dụng phân tích ma trận. Từ vựng và tokenizer giống như Glot500-m. Sau đó chúng tôi tiếp tục huấn luyện trước các mô hình được lắp ráp này trên corpus Glot500-c.

OFA-multi-xxx: chúng tôi sử dụng thiết lập tương tự như OFA-mono-xxx để xây dựng mô hình đích (chiều tiềm ẩn: 100, 200, 400, 768), trong đó XLM-R được sử dụng làm mô hình nguồn. Sau đó chúng tôi tiếp tục huấn luyện trước các mô hình này trên corpus Glot500-c.

Kiến trúc mô hình của OFA-mono-768 và OFA-multi-768 giống như Glot500-m, trong đó các embedding được liên kết với các tham số của đầu mô hình hóa ngôn ngữ. Đối với các mô hình chiều thấp hơn, hai ma trận được sử dụng để ánh xạ biểu diễn trở lại không gian từ vựng cho mô hình hóa ngôn ngữ có mặt nạ. Các tham số của hai ma trận được liên kết với primitive embeddings và tọa độ đích. Chúng tôi tiếp tục huấn luyện trước tất cả mô hình sử dụng mục tiêu MLM và theo các siêu tham số huấn luyện được sử dụng bởi ImaniGooghari et al. (2023). Mỗi bước huấn luyện chứa một batch hiệu quả 384 mẫu được chọn ngẫu nhiên từ tất cả language-script. Chúng tôi gọi các ngôn ngữ mà XLM-R bao phủ là ngôn ngữ đầu và các ngôn ngữ còn lại là ngôn ngữ đuôi. Chúng tôi lưu checkpoint cho mỗi mô hình sau mỗi 10K bước và áp dụng early stopping với hiệu suất trung bình tốt nhất trên các nhiệm vụ hạ nguồn. Chúng tôi huấn luyện tất cả mô hình trên bốn GPU NVIDIA RTX A6000 trong tối đa bốn tuần. Xem §B để biết mô tả chi tiết về thiết lập siêu tham số của việc tiếp tục huấn luyện trước và đánh giá.

5.2 Baseline
Chúng tôi xem xét các baseline sau để so sánh với OFA (xem Bảng 1 về số lượng tham số dưới các chiều embedding tiềm ẩn khác nhau):

RoBERTa: Một PLM đơn ngôn ngữ được huấn luyện trên corpus tiếng Anh (Liu et al., 2019). Embedding và tokenizer của nó không bao phủ hầu hết các subword mới của mô hình chúng tôi. Kích thước từ vựng là 50K.

RoBERTa-rand: Chúng tôi thay thế embedding của RoBERTa bằng embedding mới (kích thước từ vựng là 401K, giống như OFA-mono-768), được xây dựng bằng cách sao chép các subword được chia sẻ và khởi tạo ngẫu nhiên embedding của các subword còn lại không được bao phủ bởi RoBERTa từ phân phối Gaussian với mean và variance của embedding RoBERTa gốc, tương tự như Minixhofer et al. (2022). Tokenizer Glot500-m được sử dụng để tokenize. Sau đó chúng tôi tiếp tục huấn luyện trước nó trên Glot500-c với cùng siêu tham số.

XLM-R: Một PLM đa ngôn ngữ mạnh được huấn luyện trên 100 ngôn ngữ (Conneau et al., 2020). Chúng tôi sử dụng phiên bản base, trong đó chiều embedding là 768. Kích thước từ vựng là 250K.

XLM-R-rand: Tương tự như RoBERTa-rand, mô hình này mở rộng từ vựng từ XLM-R, và embedding của các subword không được bao phủ bởi XLM-R được khởi tạo ngẫu nhiên từ phân phối Gaussian với mean và variance của embedding XLM-R gốc. Tokenizer Glot500-m được sử dụng để tokenize. Sau đó mô hình được tiếp tục huấn luyện trước trên Glot500-c với cùng siêu tham số.

5.3 Nhiệm vụ Hạ nguồn
Truy xuất Câu. Chúng tôi xem xét hai dataset: Tatoeba (Artetxe và Schwenk, 2019) (SR-T) và Bible (SR-B). Chúng tôi chọn tối đa 1.000 câu được căn chỉnh tiếng Anh cho SR-T, theo thiết lập tương tự được sử dụng bởi Hu et al. (2020). Đối với SR-B, chúng tôi chọn tối đa 500 câu được căn chỉnh tiếng Anh. Chúng tôi báo cáo độ chính xác top-10 bằng cách tìm các lân cận gần nhất của biểu diễn của mỗi câu tiếng Anh. Theo Jalili Sabet et al. (2020), các biểu diễn được tính bằng cách lấy trung bình của embedding từ theo ngữ cảnh ở lớp thứ 8.

Gán nhãn Chuỗi. Chúng tôi xem xét hai loại nhiệm vụ: nhận dạng thực thể có tên (NER) và gán nhãn Part-Of-Speech (POS). Chúng tôi sử dụng dataset WikiANN (Pan et al., 2017) cho NER và Universal Dependencies (de Marneffe et al., 2021) phiên bản v2.11 cho POS. Chúng tôi chỉ tinh chỉnh các mô hình trên tập huấn luyện tiếng Anh, chọn mô hình tốt nhất trên tập dev tiếng Anh, và sau đó báo cáo hiệu suất zero-shot trên tập test của các ngôn ngữ khác. Điểm F1 được báo cáo cho cả NER và POS.

Phân loại Văn bản. Chúng tôi sử dụng Taxi1500 (Ma et al., 2023), một dataset phân loại văn bản cung cấp tập huấn luyện/dev/test với 6 lớp trong hơn 1.500 ngôn ngữ. Theo ImaniGooghari et al. (2023), chúng tôi chọn một tập con ngôn ngữ (351) được hỗ trợ bởi các mô hình để đánh giá. Tương tự như trong NER và POS, chúng tôi báo cáo hiệu suất zero-shot (trong điểm F1) sử dụng tiếng Anh làm nguồn.

5.4 Kết quả và Thảo luận
Bảng 2 cho thấy hiệu suất của các mô hình được khởi tạo với OFA và baseline với khởi tạo ngẫu nhiên embedding subword mới trên năm nhiệm vụ hạ nguồn (xem kết quả đầy đủ cho mỗi language-script trong §F). Các mô hình được khởi tạo với OFA thể hiện sự cải thiện nhất quán trên cả ngôn ngữ đầu hoặc đuôi so với baseline. Kết hợp với Bảng 3, chúng ta thấy rằng nhiều ngôn ngữ hơn được hưởng lợi từ khởi tạo OFA cho cả việc sử dụng PLM đơn ngôn ngữ và đa ngôn ngữ làm mô hình nguồn, điều này cho thấy sự ưu việt tổng thể của khởi tạo OFA.

Khi mô hình nguồn là đơn ngôn ngữ, với khởi tạo ngẫu nhiên của subword chưa được thấy, RoBERTa-rand chỉ đạt được 11.9, 22.0, và 15.5 trên SR-B, SR-T, và Taxi1500 tương ứng (trung bình tổng thể), thấp hơn 6.0, 6.6, 8.3 so với đối tác OFA-mono-768. Trong nhiệm vụ gán nhãn chuỗi, chúng ta cũng thấy sự cải thiện tương tự: OFA-mono-768 đạt được tốt hơn 4.3 và 5.1 so với RoBERTa-rand trên NER và POS tương ứng. Sự tăng như vậy thậm chí còn cao hơn khi so sánh với RoBERTa, vì RoBERTa là mô hình đơn ngôn ngữ. Khi mô hình nguồn là đa ngôn ngữ, các mô hình được khởi tạo với OFA cũng đạt được hiệu suất đáng chú ý. OFA-multi-768 đạt được hiệu suất tốt hơn XLM-R trên mọi nhiệm vụ. So với XLM-R-rand, nó cũng đạt được hiệu suất tốt hơn, điều này cho thấy hiệu quả của việc khởi tạo với sự giúp đỡ của embedding đa ngôn ngữ bên ngoài.

Chiều embedding cũng đóng vai trò quan trọng trong hiệu suất. Thông thường, chúng ta thấy sự cải thiện hiệu suất khi tăng chiều tiềm ẩn, đặc biệt từ 100 đến 400 cho cả mô hình OFA-mono và OFA-multi. Điều này được mong đợi vì chiều lớn hơn thường tạo ra tính biểu đạt tốt hơn. Tuy nhiên, sự cải thiện từ chiều 400 đến 768 không nhất quán lớn, và trong một số trường hợp, nó thậm chí dẫn đến sự suy giảm hiệu suất. Ví dụ, OFA-mono-400 vượt trội OFA-mono-768 trên tất cả nhiệm vụ hạ nguồn. Chúng tôi cho rằng điều này là do mô hình đơn ngôn ngữ với nhiều tham số có thể không dễ điều chỉnh đến các ngôn ngữ đa dạng. Chiều embedding nhỏ hơn có thể giảm bớt gánh nặng và tạo điều kiện cho việc huấn luyện trước, do đó đạt được hiệu suất tốt hơn. Tương tự, OFA-multi-400 rất cạnh tranh với OFA-multi-768 (OFA-multi-400 thậm chí tốt hơn trên NER và POS). Chúng tôi cho rằng điều này do tính "dư thừa" của embedding trong mPLM (xem §D để phân tích). Bằng cách sử dụng phân tích, chúng ta giữ lại thông tin quan trọng nhất được chia sẻ giữa các ngôn ngữ. Do đó có sự đánh đổi. Khi chiều rất nhỏ, ví dụ 100, có nguy cơ mất thông tin. Tuy nhiên, với kích thước vừa phải, ví dụ 400, mô hình ít dư thừa hơn và được trang bị đủ tính biểu đạt để đạt được hiệu suất tốt.

6 Phân tích
6.1 Tiến trình Tiếp tục Huấn luyện
Để phân tích cách các chiều embedding khác nhau và phương pháp khởi tạo có thể ảnh hưởng đến việc tiếp tục huấn luyện, chúng tôi trực quan hóa loss huấn luyện của các mô hình được khởi tạo với OFA và hai mô hình baseline, tức là RoBERTa-rand và XLM-R-rand. Ngoài ra, chúng tôi đánh giá tất cả các mô hình này trên năm nhiệm vụ hạ nguồn theo khoảng thời gian 10K bước cho đến 100K bước. Kết quả được thể hiện trong Hình 3. Từ Hình 3 (a), khi chiều embedding là 768, các mô hình được khởi tạo với OFA hội tụ nhanh hơn so với các mô hình được khởi tạo ngẫu nhiên, bất kể mô hình nguồn là đơn ngôn ngữ hay đa ngôn ngữ. Sự hội tụ nhanh hơn cũng liên quan đến hiệu suất, vì OFA-mono-768 (tương ứng OFA-multi-768) liên tục hoạt động tốt hơn RoBERTa-rand (tương ứng XLM-R-rand) qua các bước cho tất cả nhiệm vụ. Điều này cho thấy rằng OFA, sử dụng rõ ràng thông tin được mã hóa trong embedding PLM nguồn và vector từ đa ngôn ngữ bên ngoài, vượt trội so với khởi tạo ngẫu nhiên.

Chúng tôi cũng quan sát thấy các mô hình với chiều nhỏ hơn có xu hướng học thông tin nhanh hơn trong các bước đầu, được chỉ ra bởi tốc độ giảm loss MLM. Như đã giải thích trước đó, chiều nhỏ hơn có nghĩa là ít tham số hơn, điều này giảm bớt gánh nặng trong việc tiếp tục huấn luyện trước, đặc biệt khi mô hình nguồn là đơn ngôn ngữ. Mặt khác, tốc độ học nhanh hơn giải thích tại sao các mô hình với chiều nhỏ hơn thường hoạt động tốt hơn so với các đối tác chiều đầy đủ (OFA-mono-768 hoặc OFA-multi-768) trong giai đoạn huấn luyện đầu. Ví dụ, chỉ với 167M tham số, OFA-multi-200 đạt được hiệu suất tốt hơn hoặc rất gần so với OFA-multi-768, lớn gấp hai lần. Chúng tôi cũng quan sát thấy tất cả mô hình, đặc biệt là mô hình OFA-multi, nhanh chóng đạt đến plateau hiệu suất trên các nhiệm vụ NER và POS. Điều này phù hợp với phát hiện rằng kiến thức cú pháp được thu nhận nhanh chóng trong tiến trình huấn luyện (Blevins et al., 2022; Müller-Eberstein et al., 2023). Điều này cũng gợi ý rằng gán nhãn chuỗi có thể là nhiệm vụ đơn giản trong đó mô hình có thể chuyển các lớp phổ biến như động từ và danh từ, có thể thông qua từ vựng được chia sẻ (ImaniGooghari et al., 2023).

Kết hợp với phân tích ở trên, khởi tạo tốt hơn và chiều embedding nhỏ hơn cho phép việc tiếp tục huấn luyện trước đa ngôn ngữ hiệu quả và hiệu suất tốt hơn trong các nhiệm vụ hạ nguồn với ít bước huấn luyện hơn. Các mô hình nhẹ cũng giảm tiêu thụ GPU và cho phép batch size lớn hơn. Do đó, framework OFA được đề xuất có thể rất hữu ích khi ngân sách tính toán hạn chế được trình bày, ví dụ trong hầu hết các phòng thí nghiệm hoặc tổ chức.

Ngoài ra, vì có những lo ngại gần đây về tác động môi trường của việc huấn luyện hoặc vận hành LM (Bender et al., 2021; Rae et al., 2021; Weidinger et al., 2022), chúng tôi cũng báo cáo một số thống kê liên quan khi tiếp tục huấn luyện trước các mô hình của chúng tôi trong Bảng 4. Có hai lợi ích của việc sử dụng OFA với tham số hóa embedding phân tích: (1) thời gian huấn luyện trung bình trên 10K bước được rút ngắn và (2) tổng thể cần ít thời gian huấn luyện hơn để đạt đến checkpoint tốt nhất so với baseline ngẫu nhiên. Xem xét rằng không có sự khác biệt lớn về hiệu suất trong các nhiệm vụ hạ nguồn, việc khởi tạo bằng OFA với chiều embedding thấp hơn có thể giảm đáng kể lượng khí thải carbon và do đó thân thiện với môi trường hơn.

6.2 Ảnh hưởng của Tiếp tục Huấn luyện Trước
Tiếp tục huấn luyện trước có tác động khác nhau đến các mô hình với chiều embedding tiềm ẩn khác nhau cho các nhiệm vụ hạ nguồn khác nhau. Do đó, chúng tôi so sánh cách hiệu suất mô hình thay đổi có hoặc không có tiếp tục huấn luyện trước, như được thể hiện trong Bảng 5.

Mặc dù hầu hết các mô hình không có tiếp tục huấn luyện trước hoạt động kém nói chung, chúng ta thấy một số ngoại lệ. Ví dụ, OFA-multi-768 đạt được hơn 52.5 độ chính xác trong SR-T, trong khi chỉ 15.9 trong SR-B. Lý do chính là SR-B chứa nhiều language-script đuôi không được bao phủ bởi XLM-R. Ngược lại, SR-T chứa nhiều ngôn ngữ đầu và nhiều ngôn ngữ khác tương tự như những ngôn ngữ đầu đó. Chúng tôi cũng nhận thấy rằng việc tiếp tục huấn luyện trước có tác động ít hơn đến các nhiệm vụ gán nhãn chuỗi, tức là NER và POS, trong đó mô hình có thể sử dụng kiến thức đã được mã hóa trong các tham số của nó để hoạt động tốt trong tiếng Anh, và sau đó chuyển sang ngôn ngữ khác thông qua từ vựng được chia sẻ, hoặc tính đa ngôn ngữ đã tồn tại khi mô hình nguồn là đa ngôn ngữ.

Khi mô hình nguồn là đơn ngôn ngữ, hiệu suất không có tiếp tục huấn luyện trước là kém bất kể chiều embedding nào được sử dụng. Tuy nhiên, mô hình chiều cao hơn đạt được hiệu suất tốt hơn nhất quán so với mô hình chiều thấp hơn khi mô hình nguồn là đa ngôn ngữ. Điều này có thể được giải thích bởi thực tế rằng mô hình đa ngôn ngữ nguồn đã có tính đa ngôn ngữ mạnh và chiều cao hơn có thể khôi phục tốt hơn thông tin gốc được mã hóa trong ma trận embedding của XLM-R.

Tuy nhiên, lợi ích của chiều cao hơn giảm sau khi tiếp tục huấn luyện trước. Kết hợp với Hình 3, chúng ta thấy rằng ngay cả mô hình nhỏ nhất, tức là OFA-multi-100, cũng nhanh chóng vượt qua OFA-multi-768 trong các nhiệm vụ SR-B và Taxi500 sau 10K bước huấn luyện. Do đó chúng tôi có thể kết luận rằng các mô hình được khởi tạo với OFA có thể nhanh chóng điều chỉnh đến ngôn ngữ mới trong việc tiếp tục huấn luyện trước, đặc biệt khi mô hình nguồn đã là đa ngôn ngữ.

6.3 Hiệu suất theo Họ Ngôn ngữ
Kết quả tổng hợp được thể hiện trong Bảng 2 phản ánh rằng OFA có thể cải thiện hiệu suất tổng thể. Tuy nhiên, kết quả có thể che giấu một số thông tin như loại họ ngôn ngữ và/hoặc chữ viết nào OFA hoạt động tốt hơn hoặc tệ hơn. Do đó chúng tôi cũng báo cáo hiệu suất tổng hợp cho các họ ngôn ngữ chính trong SR-B bao phủ nhiều ngôn ngữ nhất trong các nhiệm vụ hạ nguồn của chúng tôi. Kết quả được thể hiện trong Bảng 6 (xem kết quả tổng hợp cho các chữ viết khác nhau và nhiệm vụ khác trong §E).

Có thể thấy rằng tất cả các biến thể với khởi tạo OFA đều vượt trội nhất quán so với baseline khởi tạo ngẫu nhiên trên tất cả họ ngôn ngữ khi sử dụng RoBERTa làm mô hình nguồn. Tương tự, khi chiều tiềm ẩn lớn hơn hoặc bằng 400, các mô hình với khởi tạo OFA đánh bại các đối tác trên tất cả họ ngôn ngữ. Những phát hiện này cho thấy sự ưu việt của OFA không giới hạn ở các họ ngôn ngữ nhất định. Ngoài ra, chúng tôi thấy sự khác biệt hiệu suất giữa OFA-multi-400 và OFA-multi-768 là nhỏ trên các họ ngôn ngữ, điều này tiếp tục cho thấy rằng việc giảm chiều của embedding là hiệu quả trong việc tiếp tục huấn luyện trước.

7 Kết luận
Trong công trình này, chúng tôi trình bày OFA, một framework khởi tạo khôn ngoan embedding subword chưa được thấy với tham số hóa embedding phân tích cho việc tiếp tục huấn luyện trước đa ngôn ngữ quy mô lớn hiệu quả. Chúng tôi tiến hành các thí nghiệm mở rộng và được kiểm soát nghiêm ngặt bằng cách tiếp tục huấn luyện trước các mô hình được khởi tạo từ PLM đơn ngôn ngữ hoặc đa ngôn ngữ. Chúng tôi đánh giá các mô hình này trên một loạt các nhiệm vụ hạ nguồn. Chúng tôi cho thấy rằng các mô hình được khởi tạo với OFA có sự hội tụ nhanh hơn trong quá trình huấn luyện và đạt được hiệu suất cạnh tranh hoặc tốt hơn trên các nhiệm vụ hạ nguồn, so với các baseline trong đó embedding của subword mới được khởi tạo ngẫu nhiên. Chúng tôi cũng cho thấy rằng với chiều embedding nhỏ hơn, việc tiếp tục huấn luyện trước được tạo điều kiện thêm: thời gian huấn luyện được rút ngắn và các mô hình đạt được hiệu suất tốt hơn trong giai đoạn huấn luyện đầu. Do đó, công trình này đóng góp vào việc tiếp tục huấn luyện trước đa ngôn ngữ quy mô lớn hiệu quả.
