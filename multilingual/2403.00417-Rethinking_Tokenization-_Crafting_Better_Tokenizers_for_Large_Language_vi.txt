# 2403.00417.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2403.00417.pdf
# Kích thước tệp: 624643 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Suy nghĩ lại về Tokenization: Tạo ra những Tokenizer tốt hơn cho các Mô hình Ngôn ngữ Lớn

Jinbiao Yang
Jinbiao.Yang@mpi.nl
Nhóm Ngôn ngữ và Tính toán trong Hệ thống Thần kinh, Viện Max Planck về Tâm lý Ngôn ngữ học

Tóm tắt
Tokenization ảnh hưởng đáng kể đến hiệu suất của các mô hình ngôn ngữ (LMs). Bài báo này theo dõi sự phát triển của các tokenizer từ cấp độ từ đến cấp độ từ phụ, phân tích cách chúng cân bằng token và type để tăng cường khả năng thích ứng của mô hình trong khi kiểm soát độ phức tạp. Mặc dù các tokenizer từ phụ như Byte Pair Encoding (BPE) vượt qua nhiều hạn chế của tokenizer từ, chúng gặp khó khăn trong việc xử lý các ngôn ngữ không phải Latin và phụ thuộc nhiều vào dữ liệu huấn luyện rộng lớn và tài nguyên tính toán để nắm bắt các sắc thái của biểu thức đa từ (MWEs). Bài viết này lập luận rằng các tokenizer, hơn là chỉ là công cụ kỹ thuật đơn thuần, nên lấy cảm hứng từ khoa học nhận thức về xử lý ngôn ngữ của con người. Nghiên cứu này sau đó giới thiệu "Nguyên lý Nỗ lực Tối thiểu" từ khoa học nhận thức, rằng con người tự nhiên tìm cách giảm nỗ lực nhận thức, và thảo luận về lợi ích của nguyên lý này cho việc phát triển tokenizer. Dựa trên nguyên lý này, bài báo đề xuất rằng mô hình Less-is-Better (LiB) có thể là một phương pháp mới cho tokenizer LLM. Mô hình LiB có thể tự động học một từ vựng tích hợp bao gồm từ phụ, từ và MWEs, giúp giảm hiệu quả cả số lượng token và type. Các đánh giá so sánh cho thấy tokenizer LiB vượt trội hơn các tokenizer từ và BPE hiện có, trình bày một phương pháp sáng tạo cho việc phát triển tokenizer, và gợi ý khả năng các tokenizer dựa trên khoa học nhận thức trong tương lai sẽ hiệu quả hơn.

Từ khóa: tokenizer, tokenization, mô hình ngôn ngữ

--- TRANG 2 ---
Giới thiệu

Khi đối mặt với thông tin rộng lớn hoặc phức tạp, não bộ chúng ta thường đơn giản hóa nó thành các phân đoạn nhỏ hơn, dễ tiêu hóa hơn, qua đó giúp chúng ta hiểu và ghi nhớ tốt hơn. Ngôn ngữ, minh họa cho sự phức tạp như vậy, thường đòi hỏi việc phân đoạn bản thân thành "khối" (Isbilen & Christiansen, 2020). Trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP), các khối thường được gọi là token thông qua quá trình được biết đến là tokenization.

Việc lựa chọn tokenizer có tác động quan trọng đến hiệu suất của các mô hình ngôn ngữ. Đặc biệt trong các mô hình ngôn ngữ (LMs), cách một tokenizer phân đoạn corpus xác định cách cơ bản mà mô hình xử lý ngôn ngữ. Bài viết này điều tra vai trò của token (số lượng thực tế của các đơn vị từ vựng trong một corpus) và type (số lượng các đơn vị từ vựng khác nhau của từ vựng) trong thiết kế tokenizer, và cố gắng tìm một giải pháp lý tưởng tối ưu hóa số lượng token trong khi kiểm soát số lượng type. Trong các phần tiếp theo, bài viết sẽ khám phá các ưu điểm và hạn chế của các tokenizer từ phụ, phân tích việc xử lý Biểu thức Đa từ (MWE) trong các mô hình ngôn ngữ lớn hiện tại. Bài viết này cũng lập luận rằng các tokenizer, hơn là chỉ là công cụ kỹ thuật đơn thuần, nên bắt chước và học hỏi từ các phương pháp xử lý ngôn ngữ của con người, vì các tokenizer xử lý nội dung được tạo ra bởi con người chứ không phải hiện tượng tự nhiên như âm thanh và hình ảnh, và kêu gọi một lý thuyết tổng quát để hướng dẫn việc phát triển tokenizer. Bài viết sẽ thảo luận "Nguyên lý Nỗ lực Tối thiểu" như một lý thuyết tổng quát từ khoa học nhận thức, và giới thiệu một loại mô hình tokenizer mới - mô hình Less-is-Better, dựa trên Nguyên lý Nỗ lực Tối thiểu.

Từ Tokenizer cấp Từ đến Tokenizer cấp Từ phụ

Các ứng dụng NLP ban đầu dựa vào các tokenizer cấp từ, chia văn bản thành từ sử dụng khoảng trắng và dấu chấm câu. Ví dụ, sự phát triển lịch sử của các biểu diễn ngữ nghĩa bắt đầu với mô hình Bag-of-Words, tiến triển đến Word2Vec bởi (Mikolov et al., 2013), và đến GloVe (Global Vectors for Word Representation) bởi (Pennington et al., 2014). Tất cả đều nhằm huấn luyện các biểu diễn ngữ nghĩa ở cấp độ từ. Các tokenizer cấp từ tương đối hiệu quả trong xử lý các ngôn ngữ châu Âu, nơi khoảng trắng cung cấp ranh giới từ rõ ràng. Tuy nhiên, phương pháp này bị hạn chế trong các ngôn ngữ như tiếng Trung, không có ranh giới từ rõ ràng. Hơn nữa, các biến đổi hình thái linh hoạt trong ngôn ngữ, sự xuất hiện liên tục của từ mới, và sự phổ biến của lỗi chính tả trong corpus khiến từ vựng cấp từ khó tổng quát hóa trong các ứng dụng thực tế.

Công nghệ từ phụ có thể truy nguyên về những năm 1990 (Gage, 1994). Ban đầu, các kỹ thuật này chủ yếu được sử dụng để nén dữ liệu. Với sự xuất hiện của các mô hình ngôn ngữ lớn (LLMs), nhu cầu về tokenizer tăng lên. Các mô hình phức tạp này đòi hỏi hiểu biết và tạo ra nội dung ngôn ngữ cực kỳ phong phú và đa dạng, và các tokenizer cấp từ truyền thống gặp khó khăn với từ vựng phức tạp, biến đổi hình thái, và dòng từ vựng mới liên tục. Tại thời điểm này, các tokenizer cấp từ phụ trở thành xu hướng chính mới do tính linh hoạt và khả năng tổng quát hóa của chúng.

--- TRANG 3 ---

Tokenizer | Bài báo Đại diện | Năm | Được sử dụng trong Mô hình Nổi bật
BPE | "Neural Machine Translation of Rare Words with Subword Units" (Sennrich et al., 2016) | 2016 | GPT-2&3&4
SentencePiece | "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing" (Kudo & Richardson, 2018) | 2018 | ALBERT (Lan et al., 2020), T5 (Raffel et al., 2019), XLNet (Z. Yang et al., 2020)
Unigram | "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates" (Kudo, 2018) | 2018 | T5 (Raffel et al., 2019)
WordPiece | "Japanese and Korean Voice Search" (Schuster & Nakajima, 2012);"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (Devlin et al., 2019) | 2019 | BERT (Devlin et al., 2019), ERNIE (Sun et al., 2019)

Bảng 1: Các phương pháp tokenization phổ biến đã đóng góp vào sự phát triển của các mô hình ngôn ngữ trong những năm gần đây.

Danh mục | Ví dụ
Văn bản tiếng Anh | Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models.
Từ phụ tiếng Anh | Gener|ative| Pre| -trained| Transformer| |4| (|G|PT| -|4|)| is| a| multim|odal| large| language| model| created| by| Open|AI|,| and| the| fourth| in| its| series| of| G|PT| foundation| models|.
Văn bản tiếng Trung | 您可以使用下面的工具了解语言模型如何对一段文本进行标记化，以及这段文本中的标记总数。
Từ phụ tiếng Trung | 您|可以 |使用 |下|面|的|工|具|了|解|语|言|模|型|如|何|对|一|段|文|本|进|行|标|记|化|，|以|及|这|段|文|本|中|的|标|记|总|数|。

--- TRANG 4 ---
Bảng 2: Ví dụ về phân đoạn văn bản sử dụng BPE. Tokenizer được sử dụng được cung cấp chính thức bởi OpenAI cho GPT-3.5 và GPT-4¹.

Cân bằng Token và Type bằng Từ phụ

Trong việc chuyển đổi từ cấp từ sang cấp từ phụ, một cân nhắc cốt lõi là cách cân bằng số lượng token và type. Các tokenizer cấp từ, mặc dù tạo ra ít type hơn, không thể xử lý các đơn vị Ngoài-Từ-vựng (OOV). Ngược lại, các tokenizer cấp từ phụ giảm đáng kể sự xuất hiện của OOV, tăng cường khả năng thích ứng của mô hình với từ vựng mới và hiện tượng ngôn ngữ phức tạp.

Ví dụ, BPE và WordPiece, trong việc tạo từ vựng của chúng, xử lý hiệu quả từ vựng hiếm bằng cách dần dần hợp nhất các cặp ký tự hoặc tổ hợp xuất hiện thường xuyên, trong khi giữ số lượng token trong phạm vi hợp lý. Các mô hình SentencePiece và Unigram tiếp tục cải thiện khả năng thích ứng với các ngôn ngữ khác nhau, đặc biệt trong các ngôn ngữ không sử dụng khoảng trắng để tách từ (như tiếng Trung).

Phân đoạn | #Token | #Type
Từ | 100 triệu | 1,750,000
BPE | 111 triệu | 82,000
Ký tự | 550 triệu | 3,000

Bảng 3: Một ví dụ về số lượng token/type với các tokenizer khác nhau trên một corpus tiếng Đức (Xem Bảng 1 trong Sennrich et al., (2016)).

Như được thể hiện trong so sánh ở Bảng 3, số lượng type trong BPE là 4.7% của từ, trong khi số lượng token gần bằng nhau (111%); số lượng type trong ký tự là 0.2% của từ, nhưng số lượng token là 550%. Điều này cho thấy phương pháp từ phụ có hiệu suất cao (giảm đáng kể số lượng type) và chi phí thấp (số lượng token cao hơn một chút).

Đối với các ngôn ngữ có hình thái phong phú (như corpus tiếng Đức được thể hiện trong Bảng 3), việc giảm đáng kể số lượng type với tokenization cấp từ phụ chủ yếu vì nó có thể chia nhỏ từ thành các đơn vị phụ thường xuyên, nắm bắt các biến đổi hình thái mà không cần các mục riêng biệt cho mỗi dạng từ. Mặc dù tiếng Trung không có nhiều biến đổi hình thái, sự hiện diện của nhiều từ ghép (được tạo thành từ hai hoặc nhiều hình vị, như "关闭" [tắt], "直升机" [máy bay trực thăng]) có nghĩa là tokenization cấp từ phụ cũng có thể giảm số lượng type.

Việc giảm type làm giảm độ phức tạp tính toán và yêu cầu bộ nhớ của LMs, có tác động tích cực trực tiếp đến hiệu suất của các mô hình. Hơn nữa, tokenization cấp từ phụ có khả năng tổng quát hóa mạnh hơn cho nội dung OOV hoặc lỗi chính tả trong

¹ https://platform.openai.com/tokenizer

--- TRANG 5 ---
corpus, vì các token ngắn hơn có xác suất cao hơn để bao phủ nhiều corpus hơn. Do đó, LLMs thường áp dụng phương pháp từ phụ cho các ngôn ngữ chữ cái (Bảng 1).

Sự chuyển đổi từ cấp từ sang cấp từ phụ này không chỉ đánh dấu tiến bộ của công nghệ tokenizer trong lĩnh vực NLP mà còn phản ánh hiểu biết sâu sắc hơn về sự đa dạng và phức tạp của ngôn ngữ. Tuy nhiên, như được thấy trong các ví dụ ở Bảng 2, không giống như các ngôn ngữ sử dụng bảng chữ cái Latin, các từ phụ BPE tiếng Trung chủ yếu là các ký tự đơn. Một phân tích² cho thấy một câu tiếng Trung có thể cần nhiều hơn 1.7 lần token so với một câu tương tự bằng tiếng Anh, và tiếng Miến Điện hoặc tiếng Amharic có thể cần nhiều hơn 10 lần token. Điều này chỉ ra rằng đối với LLMs trong các ngôn ngữ khác nhau, đặc biệt là các ngôn ngữ không phải Latin (như tiếng Trung), các từ phụ BPE vẫn có thiếu sót. Hơn nữa, mặc dù các tokenizer từ phụ đã đạt tiến bộ đáng kể trong việc xử lý các vấn đề OOV, chúng vẫn có hạn chế trong việc nắm bắt ngữ nghĩa tinh tế và hàm ý thành ngữ của ngôn ngữ. Điều này dẫn đến nhu cầu xử lý trực tiếp các biểu thức đa từ như một sự bổ sung và tinh chỉnh của công nghệ tokenizer hiện tại.

Sự Lề hóa hiện tại của Biểu thức Đa từ (MWEs) trong Mô hình Ngôn ngữ

Biểu thức Đa từ, mặc dù đóng vai trò quan trọng trong ngôn ngữ hàng ngày, thường bị bỏ qua trong việc phát triển LMs. Cho đến nay, chỉ có các mô hình Jurassic-X của AI21 Studio (Lieber et al., 2021) đã giới thiệu các token đa từ, bao gồm biểu thức, cụm từ và thực thể được đặt tên, vào từ vựng của chúng. Sự lề hóa này có thể chủ yếu do một số lý do:

1. Cân nhắc về hiệu suất và độ phức tạp: Việc giới thiệu MWEs như các token độc lập rõ ràng sẽ tăng số lượng type. Từ vựng của mô hình Jurassic nói trên bao gồm khoảng 250,000 type, lớn hơn nhiều so với hầu hết các từ vựng hiện có (5 lần hoặc hơn)³. Tuy nhiên, MWEs có thể hiếm hoặc rất cụ thể cho một số bối cảnh hoặc lĩnh vực nhất định, do đó việc giới thiệu chúng như các token độc lập không giảm đáng kể tổng số token. Điều này phần nào mâu thuẫn với mục tiêu hiệu suất hiệu quả được theo đuổi bởi LMs. Hơn nữa, các MWEs tần số thấp dẫn đến biểu diễn không đủ trong dữ liệu huấn luyện, khiến các mô hình khó học và dự đoán chính xác ngữ nghĩa của chúng.

2. Vai trò thay thế của dữ liệu lớn và sức mạnh tính toán: Các LLMs hiện tại, như dòng GPT và BERT, dựa vào dữ liệu huấn luyện khổng lồ và sức mạnh tính toán cao để học cách sử dụng thực tế của MWEs thay vì biểu thức theo nghĩa đen của chúng, mặc dù các biểu thức này không được xử lý như các đơn vị toàn thể trong quá trình huấn luyện (Tian et al., 2023).

Mặc dù vậy, việc nhận dạng và xử lý trực tiếp MWEs vẫn có giá trị độc đáo trong LMs và LLMs: 1. MWEs có thể có ngữ nghĩa toàn thể độc đáo: Việc kết hợp MWEs với

² https://www.artfish.ai/p/all-languages-are-not-created-tokenized
³ https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1

--- TRANG 6 ---
ngữ nghĩa toàn thể độc đáo, như "kick the bucket" hoặc "摸鱼" [lười biếng (nghĩa thực tế); sờ cá (nghĩa đen)], có thể làm phong phú khả năng hiểu ngôn ngữ của mô hình. Mặc dù điều này có thể không giảm đáng kể số lượng token, nó cho phép mô hình nắm bắt ngữ nghĩa cụ thể của văn bản chứa các MWEs này một cách trực tiếp và chính xác hơn. 2. Một số MWEs có thể giảm số lượng type: Trong một số trường hợp, bằng cách lựa chọn thích hợp MWEs, thậm chí có thể giảm tổng số type. Ví dụ, xử lý các cụm từ cố định phổ biến như các đơn vị đơn (như "鹦鹉" [vẹt], "乒乓" [bóng bàn]) có thể giảm nhu cầu cho các phần riêng lẻ của chúng.

Do lượng lớn và sự đa dạng của MWEs, đã có sự khan hiếm từ điển MWE. Sự khan hiếm này do đó cản trở việc tích hợp chúng vào việc phát triển hiện tại của các mô hình ngôn ngữ lớn. Tuy nhiên, các nhà ngôn ngữ học và tâm lý ngôn ngữ học đã nghiên cứu lâu về MWEs, và chúng ta có thể tái khám phá giá trị của chúng dựa trên nhận thức con người:

• Kết hợp hiệu suất mô hình với nhận thức ngôn ngữ con người: Với những tiến bộ công nghệ, đặc biệt khi LMs đạt đến giới hạn kỹ thuật, LMs có thể học hỏi nhiều hơn từ các quá trình nhận thức ngôn ngữ con người.

• Vượt ra ngoài sức mạnh tính toán thuần túy: Mặc dù dữ liệu lớn và tính toán mạnh mẽ có thể giải quyết xử lý MWE ở một mức độ nhất định, phương pháp "vũ phu" này có thể không hiệu quả và chính xác như một LLM/tokenizer được thiết kế cẩn thận có thể xử lý trực tiếp MWEs. Giống như mạng nơ-ron tích chập (Lecun et al., 1998) cho học sâu và Học Tăng cường từ Phản hồi Con người (RHLF)(Ouyang et al., 2022) cho LLMs, chúng ta có thể xem xét lại việc lấy cảm hứng từ các quá trình nhận thức ngôn ngữ con người khi đạt đến giới hạn kỹ thuật.

Trong khi tokenization của từ phụ và MWEs có thể mang lại những lợi thế nhất định cho LMs, việc khám phá các nguyên lý sâu sắc hơn của xử lý ngôn ngữ vẫn cần thiết trong việc hiểu và tối ưu hóa các tokenizer. Bài viết này sẽ tập trung vào một cái nhìn sâu sắc về việc học và sử dụng ngôn ngữ của con người - "Nguyên lý Nỗ lực Tối thiểu".

Tối ưu hóa Tokenizer Tương lai

Trong lĩnh vực kỹ thuật, đặc biệt do sự phát triển nhanh chóng của NLP và các mô hình ngôn ngữ, đã có sự quan tâm ngày càng tăng trong nghiên cứu về tokenizer. Bài viết này lập luận rằng các tokenizer, hơn là chỉ là công cụ kỹ thuật đơn thuần, nên bắt chước và học hỏi từ các phương pháp xử lý ngôn ngữ của con người. Lập luận này dựa trên hai lý do chính:

• Ngôn ngữ như một sản phẩm trực tiếp nhận thức: Trong khi có thể lập luận rằng việc phát minh máy bay dựa vào khí động lực học chứ không phải bắt chước cách chim bay và gợi ý rằng nghiên cứu về mô hình ngôn ngữ và tokenizer không cần bắt chước nhận thức con người, có một sự khác biệt cơ bản. Máy bay được thiết kế để bay, không phải để bắt chước chim. Tuy nhiên, tokenizer xử lý trực tiếp nội dung có nguồn gốc từ nhận thức con người, như ngôn ngữ nói hoặc viết. Như bằng chứng, các tokenizer họ BPE, là các thuật toán tokenization chủ đạo hiện tại, cũng chứng minh ưu thế của chúng về mặt tính hợp lý nhận thức

--- TRANG 7 ---
(Beinborn & Pinter, 2023). Trong nỗ lực tối ưu hóa thiết kế tokenizer, do đó, người ta không thể bỏ qua các cơ chế xử lý ngôn ngữ của con người.

• Thiếu một lý thuyết tổng quát: Sau sự suy tàn của các phương pháp dựa trên ngôn ngữ học trong kỷ nguyên LLM, việc phát triển tokenizer thường được nhắm trực tiếp đến các mục tiêu kỹ thuật cụ thể - như chia các từ không thường xuyên thành từ phụ (ví dụ, Sennrich et al., 2016) và cải thiện hiệu suất của LMs. Sự chuyển đổi này đã xảy ra mà không có nền tảng của một khung lý thuyết tổng quát mới để hướng dẫn sự chuyển đổi. Khoảng trống này đã dẫn đến sự phụ thuộc vào thử-và-sai, khiến việc phát triển tốn thời gian và khó cung cấp hướng dẫn có hệ thống cho nghiên cứu tiếp theo. Ngược lại, khoa học nhận thức, đã tiến hành nghiên cứu rộng rãi về tokenization (ví dụ, Arnon & Priva, 2013; Goldwater et al., 2009; Perruchet & Vinter, 1998; J. Yang, Cai, et al., 2020), có vị thế tốt để cung cấp (các) lý thuyết tổng quát cho việc phát triển tokenizer NLP.

Bài viết này sẽ trình bày "Nguyên lý Nỗ lực Tối thiểu," một lý thuyết tổng quát từ khoa học nhận thức có thể được áp dụng cho tokenizer. Nó cũng giới thiệu mô hình Less-is-Better (LiB), dựa trên nguyên lý này, để chứng minh cách khoa học nhận thức có thể hướng dẫn việc phát triển tokenizer.

Nguyên lý Nỗ lực Tối thiểu

Việc học một token ngôn ngữ của con người có nghĩa là con người có thể nhận thức và tạo ra token một cách toàn thể (nhưng cũng có thể phân tách nó nếu cần thiết), qua đó giảm độ phức tạp của ngôn ngữ (Isbilen & Christiansen, 2020). Các token toàn thể nhận thức như vậy, dù chúng là từ, từ phụ hay biểu thức đa từ (Hình 1), có thể được gọi là "đơn vị nhận thức" trong từ điển tinh thần của chúng ta⁴ (J. Yang, 2022). Hơn nữa, tokenization của con người có thể được mô tả là quá trình học và nhận dạng các đơn vị nhận thức này. Trái với các định nghĩa nghiêm ngặt về từ phụ, từ, hoặc MWEs, các đơn vị nhận thức được đặc trưng bởi khả năng thích ứng về kích thước và hình dạng. Khả năng thích ứng này rõ ràng trong cách trẻ sơ sinh và cá nhân mù chữ học ngôn ngữ - họ học và sử dụng các hình thức đơn vị nhận thức khác nhau từ môi trường, ngay cả khi không hiểu chính thức "từ" là gì. Quan sát này gợi ý rằng con người có khả năng xác định và áp dụng các đơn vị nhận thức phù hợp từ đầu vào ngôn ngữ một cách tự động. Các thuật toán tokenization không giám sát hiện tại, nhằm nắm bắt các quy luật ngôn ngữ như tần số (ví dụ, Sennrich et al., 2016) và xác suất chuyển đổi (ví dụ, Brugnara et al., 1993), phản ánh khả năng này. Tuy nhiên, mặc dù biết rằng não có thể học các

⁴ Mặc dù khả năng nhận thức cá nhân và kinh nghiệm ngôn ngữ có thể dẫn đến sự khác biệt cá nhân trong từ điển tinh thần của chúng ta, cộng đồng ngôn ngữ chung vẫn có thể duy trì một mức độ nhất quán nhất định trong các đơn vị nhận thức. Ví dụ, từ "apple" là một đơn vị nhận thức cho hầu hết người nói tiếng Anh, và "苹果" (bản dịch tiếng Trung của "apple") là một đơn vị nhận thức cho hầu hết người nói tiếng Trung.

--- TRANG 8 ---
quy luật xác suất này (Isbilen et al., 2020; Meltzoff et al., 2009; Schapiro et al., 2016), khó biết "thuật toán" cụ thể cho tokenization của con người.

Hình 1: Một số ví dụ về từ, trạng từ hoặc biểu thức đa từ có thể/không có khả năng trở thành các đơn vị nhận thức trong từ điển tinh thần của chúng ta.

Như một lý thuyết tổng quát trong nhận thức con người, Nguyên lý Nỗ lực Tối thiểu (PLE) của George Kingsley Zipf, được diễn đạt trong cuốn sách "Human Behavior and the Principle of Least Effort" (1949), có thể làm cầu nối khoảng cách giữa mục tiêu cuối cùng của tokenization và vô số phương pháp. Lý thuyết này về cơ bản là một quan sát thống kê về ngôn ngữ và các hệ thống hành vi con người khác, nói rằng con người có xu hướng theo con đường tối thiểu hóa nỗ lực. Người ta có thể quan sát rằng mô tả PLE đơn giản và mục tiêu của PLE là đơn giản, điều này cũng phù hợp với nguyên lý đơn giản được theo đuổi bởi nhiều học giả đầu tiên (ví dụ, Aristotle, Aquinas, Kant William of Ockham, Newton, và Kant; xem: Baker, 2022) và các nhà khoa học nhận thức hiện tại (ví dụ, Chater, 1999; Chater & Vitányi, 2003; Feldman, 2016). Áp dụng PLE trong xử lý ngôn ngữ gợi ý tối thiểu hóa gánh nặng nhận thức trong học và sử dụng ngôn ngữ. Điều này bao gồm việc đạt được sự cân bằng nơi mỗi mô-đun nhận thức tham gia vào xử lý ngôn ngữ theo đuổi gánh nặng tối thiểu của riêng mình. Tuy nhiên, xử lý ngôn ngữ là một nhiệm vụ phức tạp liên quan đến nhiều mô-đun nhận thức. Vì mục đích thực tế, quá trình nên được đơn giản hóa.

Biểu thức gốc của Zipf về nguyên lý này được bao gồm trong cuốn sách của ông (1949) rằng "[người đó] sẽ cố gắng giải quyết các vấn đề của mình theo cách tối thiểu hóa tổng công việc mà anh ta phải bỏ ra để giải quyết cả các vấn đề ngay lập tức và các vấn đề có thể xảy ra trong tương lai." Việc đề cập đến "các vấn đề ngay lập tức" và "các vấn đề có thể xảy ra trong tương lai" là quan trọng, vì chúng bao gồm cả nhu cầu ngắn hạn và dài hạn để giảm gánh nặng, có thể mâu thuẫn. Do đó, tuân theo PLE có nghĩa là đạt được sự cân bằng giữa gánh nặng nhận thức ngắn hạn và dài hạn (nó cũng phù hợp với lý thuyết nén; xem đoạn tiếp theo). Trong các phần trước, chúng ta đã đánh giá các phương pháp khác nhau dựa trên

--- TRANG 9 ---
số lượng Type và Token, và lợi ích của ít type và ít token hơn cũng đã được chứng minh bởi các thí nghiệm LLM (Delétang et al., 2023; Ruoss et al., 2023). Từ góc độ PLE: 1. Ít token hơn có thể giảm bớt gánh nặng của lưu trữ bộ nhớ làm việc và các bước giải mã thông tin; 2. Ít type hơn có thể giảm nhẹ gánh nặng của lưu trữ và truy xuất bộ nhớ dài hạn. Nguyên lý này cũng có thể giúp chúng ta hiểu sự chuyển đổi từ tokenizer cấp từ sang tokenizer cấp từ phụ, và hỗ trợ việc giới thiệu MWEs trong thiết kế tokenizer hiệu quả hơn cho LMs.

Hơn nữa, các nghiên cứu gần đây (Delétang et al., 2023; Gruver et al., 2023) đề xuất rằng LMs có thể được xem như xấp xỉ nén dữ liệu tối ưu. Điều này phù hợp với Nguyên lý Nỗ lực Tối thiểu. Sử dụng lý thuyết Độ dài Mô tả Tối thiểu (MDL) (Rissanen, 1978), chúng ta có thể thấy rằng ít type hơn đại diện cho một mô tả nén hơn của mô hình mã hóa, và ít token hơn đại diện cho một mô tả nén hơn của dữ liệu được mã hóa. Nén tìm kiếm tối thiểu tổng của mô hình mã hóa và dữ liệu được mã hóa. Đáng chú ý rằng một sự khác biệt quan trọng từ lý thuyết nén dữ liệu này là mỗi mô-đun não tìm kiếm gánh nặng tối thiểu của riêng mình cho đến khi đạt được sự cân bằng toàn cầu, vì các khu vực khác nhau của não hoạt động trong sự phối hợp và cạnh tranh, không nhất thiết được quản lý bởi một bộ điều khiển toàn cầu duy nhất.

Mô hình LiB: Một Triển khai của 'Nguyên lý Nỗ lực Tối thiểu'

Để đáp ứng các hạn chế của công nghệ tokenizer hiện có trong LMs, phần này sẽ giới thiệu một thiết kế tokenizer mới dựa trên PLE. Nỗ lực Tối thiểu bản thân, là một nguyên lý, có thể được triển khai theo nhiều cách khác nhau. Trong các nghiên cứu trước (J. Yang, Frank, et al., 2020; J. Yang et al., 2022), tác giả đề xuất một triển khai tập trung vào việc giảm gánh nặng của bộ nhớ làm việc (số lượng token) và bộ nhớ dài hạn (số lượng type), cụ thể là mô hình Less-is-Better (LiB). Mô hình này nhằm bắt chước việc học các đơn vị ngôn ngữ linh hoạt. Nó vượt qua các rào cản trong việc định nghĩa các đơn vị ngôn ngữ khác nhau thông qua các phương pháp không giám sát, và thống nhất từ phụ, từ, và biểu thức đa từ (MWEs) vào cùng một từ vựng. Trong quá trình này, nó cân bằng hiệu quả số lượng token và type để giảm gánh nặng nhận thức của việc sử dụng ngôn ngữ (Hình 2). Quá trình này, ở một mức độ nào đó, tiếp cận Độ dài Mô tả Tối thiểu, nhưng với trọng tâm vào sự cân bằng giữa hai tối thiểu hóa cá nhân (min(#tokens) vs. min(#types)) thay vì một tối thiểu hóa toàn cầu (min(#tokens + #types)).

Cơ chế Mô hình: Mô hình bao gồm một "Memorizer" và một "Forgetter". Ban đầu, mô hình LiB chia corpus đầu vào thành các token nhỏ nhất và sau đó "Memorizer" liên tục hợp nhất các token liền kề trong corpus thành các đơn vị mới (dài hơn) và lưu trữ chúng trong từ vựng. Bằng cách sử dụng các đơn vị dài hơn, số lượng token đơn vị trong văn bản giảm, trong khi số lượng type tăng. Ngược lại, "Forgetter" loại bỏ các đơn vị "rác" ít hữu ích khỏi từ vựng để giảm số lượng type đơn vị. Các đơn vị "rác" có thể là những type làm tăng số lượng token đơn vị trong câu hoặc là các type xuất hiện ít thường xuyên. "Memorizer" và "Forgetter" cân bằng lẫn nhau, cuối cùng đạt đến trạng thái tương đối ổn định, nơi từ vựng chứa các đơn vị gần với mục tiêu tối thiểu hóa gánh nặng nhận thức. Xem chi tiết hơn trong J. Yang, Frank, et al., (2020).

Hình 2: Mô hình LiB. A. tổng quan luồng thông tin, B. các chiến lược phân đoạn văn bản, C. các chiến lược cập nhật từ điển/từ vựng.

--- TRANG 10 ---

--- TRANG 11 ---
Kết quả: Phương pháp không giám sát của mô hình LiB bỏ qua các rào cản định nghĩa giữa các đơn vị ngôn ngữ truyền thống khác nhau, do đó từ vựng của nó cũng vượt qua các hạn chế thường gặp của từ phụ, từ, và biểu thức đa từ. Hai câu được trình bày trong Bảng 4 để minh họa. Ở cấp độ từ vựng, mô hình tự động học các đơn vị tiếng Anh đa dạng như "ly," "you," và "you can", cũng như các đơn vị tiếng Trung như "的" (bản dịch tiếng Anh: "'s"), "孩子" (bản dịch tiếng Anh: "kid"), và "新华社" (bản dịch tiếng Anh: "Xinhua News Agency") (J. Yang, Frank, et al., 2020). Sự hợp nhất này phản ánh tính linh hoạt của mô hình LiB trong việc học các đơn vị nhận thức có kích thước và cấp độ ngôn ngữ khác nhau.

Corpus | Type | Phân đoạn
BRphono | Input | allrightwhydon'tweputhimawaynow
 | Words | all|right|why|don't|we|put|him|away|now
 | LiB output | allright|whydon't|we|puthimaway|now
CTB8 | Input | 这个出口信贷项目委托中国银行为代理银行
 | Words | 这|个|出口|信贷|项目|委托|中国|银行|为|代理|银行
 | LiB output | 这个|出口信贷|项目|委托|中国银行|为|代理|银行

Bảng 4: Ví dụ phân đoạn chuỗi trong hai corpus. Kết quả BRphono được chuyển đổi thành từ tiếng Anh để dễ trình bày (xem Bảng 3 trong J. Yang, Frank, et al., (2020)).

Ứng dụng Thực tế: Các đơn vị được học bởi LiB có thể được sử dụng để dự đoán các mẫu cố định mắt của người đọc, gợi ý rằng các đơn vị của mô hình phù hợp với các đơn vị nhận thức con người (J. Yang et al., 2022). Đối với corpus trong các ngôn ngữ khác nhau, mô hình LiB linh hoạt học từ điển của chúng thông qua phương pháp không giám sát dựa trên PLE (J. Yang, Frank, et al., 2020; J. Yang et al., 2022), qua đó thích ứng với độ phức tạp và sự đa dạng của các đầu vào ngôn ngữ khác nhau, trong khi cân bằng tải nhận thức. Mặc dù LiB chỉ là một mô hình nhận thức và chưa được tối ưu hóa cho các mô hình ngôn ngữ, các đánh giá trên các mô hình ngôn ngữ đơn giản cho thấy các đơn vị được tạo bởi LiB hoạt động tốt hơn trong điểm Bits-per-character (Bảng 5). Hiệu suất vượt trội này có thể được quy cho việc LiB học ít token và type hơn so với tokenizer cấp từ và tokenizer BPE (Bảng 6). Điều này gợi ý giá trị của PLE trong kỷ nguyên mô hình ngôn ngữ LỚN này. Chúng ta có thể sử dụng mô hình LiB hoặc các biến thể khác cũng tuân theo PLE như tokenizer cho các mô hình ngôn ngữ lớn để tăng cường hiệu suất của chúng.

--- TRANG 12 ---
Corpus | Metric | Tokenizations
 |  | Characters | BPE subwords | Words | LiB units
BRphono (English) | Average token length | 1 | 2.8 | 2.9 | 3.6
 | Vocabulary size | 50 | 5,574 | 1,321 | 1,869
CTB8 (Chinese) | Average token length | 1 | 1.4 | 1.7 | 1.9
 | Vocabulary size | 4,697 | 7,980 | 65,410 | 39,320

Bảng 5: Độ dài token trung bình, kích thước từ điển của các tokenization khác nhau trên hai corpus. Đơn vị của Độ dài Trung bình là âm vị tiếng Anh (BRphono) hoặc ký tự tiếng Trung (CTB8) (xem Bảng 4 trong J. Yang, Frank, et al., (2020)).

Corpus | Model | Character | BPE subword | Word | LiB chunk
CTB8 | 2-Gram | 3.558 | 2.788 | 2.333 | 2.095
 | 3-Gram | 2.025 | 1.193 | 1.163 | 0.903
BRphono | 2-Gram | 2.221 | 1.003 | 0.977 | 0.791
 | 3-Gram | 1.371 | 0.563 | 0.584 | 0.484

Bảng 6: Điểm bits per character trên các tokenization khác nhau (xem Bảng 5 trong J. Yang, Frank, et al., (2020)).

Phương pháp dựa trên khoa học nhận thức này cung cấp một góc nhìn và hướng mới cho việc phát triển các mô hình ngôn ngữ trong tương lai, đặc biệt khi xử lý corpus trong các ngôn ngữ khác nhau (như tiếng Trung, thiếu ranh giới từ rõ ràng).

--- TRANG 13 ---
Tóm tắt

Bài viết này khám phá lựa chọn hiện tại và tối ưu hóa tương lai của tokenizer cho các mô hình ngôn ngữ lớn (LLMs), đặc biệt trong việc xử lý các ngôn ngữ phức tạp như tiếng Trung. Nhìn chung, tokenization từ phụ, như một kỹ thuật cân bằng, giảm đáng kể số lượng type trong khi chỉ tăng nhẹ số lượng token so với tokenization từ, hiệu quả giải quyết các vấn đề Ngoài-Từ-vựng (OOV) và tăng cường khả năng tổng quát hóa của mô hình. Tuy nhiên, phương pháp này có hạn chế trong việc kiểm soát số lượng token trong một số ngôn ngữ không phải Latin (như tiếng Trung), và cũng trong việc nắm bắt ngữ nghĩa tinh tế và hàm ý thành ngữ của ngôn ngữ.

Sự vắng mặt của MWEs trong hầu hết LMs phản ánh một điểm mù trong lĩnh vực NLP hiện tại. Mặc dù MWEs tăng đáng kể số lượng type, và các mô hình hiện tại có thể học ý nghĩa của MWEs trên tokenization từ phụ bằng dữ liệu/sức mạnh tính toán khổng lồ, việc nhận dạng và xử lý trực tiếp MWEs vẫn có thể giúp các mô hình ngôn ngữ cải thiện độ chính xác trong hiểu ngôn ngữ. Trong việc phát triển tokenizer trong tương lai, cách lựa chọn hiệu quả MWEs và cân bằng số lượng token và type có thể là một lĩnh vực quan trọng cho sự tiến bộ tokenizer.

Để giải quyết các vấn đề của công nghệ tokenizer hiện tại, bài viết này thảo luận tầm quan trọng của việc bắt chước các phương pháp xử lý ngôn ngữ con người trong thiết kế tokenizer, và giới thiệu "Nguyên lý Nỗ lực Tối thiểu" từ khoa học nhận thức, không chỉ tiết lộ hiệu quả và đơn giản trong xử lý ngôn ngữ con người mà còn, như một lý thuyết tổng quát trong khoa học nhận thức, có thể hướng dẫn thiết kế các tokenizer hiệu quả hơn. Dựa trên nguyên lý này, bài viết này đề xuất mô hình LiB, một mô hình cố gắng tối ưu hóa từ vựng của mình thông qua các cơ chế học và quên, đạt được sự cân bằng hiệu quả hơn của token và type. Nó nhằm mô phỏng các cơ chế xử lý ngôn ngữ con người, giảm gánh nặng nhận thức, và thu được các loại đơn vị nhận thức ngôn ngữ mới tích hợp từ phụ, từ, và MWEs, qua đó tăng cường hiệu quả và độ chính xác của xử lý ngôn ngữ. Mô hình LiB không chỉ là một sự phản ánh về các cơ chế xử lý ngôn ngữ con người mà còn cung cấp ý tưởng mới cho việc thiết kế tokenizer hiệu quả hơn cho LMs. Phương pháp dựa trên khoa học nhận thức này cung cấp góc nhìn và hướng mới cho việc phát triển tokenizer và mô hình ngôn ngữ trong tương lai. Việc kết hợp hiểu biết từ khoa học nhận thức với thiết kế các mô hình ngôn ngữ lớn có thể tăng cường sự phát triển cộng sinh của chúng.

Lời cảm ơn

JY được hỗ trợ bởi Nhóm Nghiên cứu Lise Meitner "Language and Computation in Neural Systems" của Tiến sĩ Andrea E. Martin, được tài trợ bởi Hiệp hội Max-Planck và Viện Max-Planck về Tâm lý Ngôn ngữ học.

Tài liệu tham khảo

--- TRANG 14 ---
Arnon, I., & Priva, U. C. (2013). More than words: The effect of multi-word frequency and constituency on phonetic duration. Lang. Speech, 56(Pt 3), 349–371. https://doi.org/10.1177/0023830913484891

Baker, A. (2022). Simplicity. In E. N. Zalta (Ed.), The Stanford encyclopedia of philosophy (Summer 2022). https://plato.stanford.edu/archives/sum2022/entries/simplicity/ ; Metaphysics Research Lab, Stanford University.

Beinborn, L., & Pinter, Y. (2023). Analyzing cognitive plausibility of subword tokenization. In H. Bouamor, J. Pino, & K. Bali (Eds.), Proceedings of the 2023 conference on empirical methods in natural language processing (pp. 4478–4486). Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.emnlp-main.272

Brugnara, F., Falavigna, D., & Omologo, M. (1993). Automatic segmentation and labeling of speech based on hidden markov models. Speech Commun., 12(4), 357–370. https://doi.org/10.1016/0167-6393(93)90083-W

Chater, N. (1999). The search for simplicity: A fundamental cognitive principle? Q. J. Exp. Psychol. A, 52A(2), 273–302. https://doi.org/10.1080/027249899391070

Chater, N., & Vitányi, P. (2003). Simplicity: A unifying principle in cognitive science? Trends Cogn. Sci., 7(1), 19–22. https://doi.org/10.1016/s1364-6613(02)00005-0

Delétang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Genewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K., Aitchison, M., Orseau, L., Hutter, M., & Veness, J. (2023). Language modeling is compression. http://arxiv.org/abs/2309.10668

Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. https://doi.org/10.18653/v1/N19-1423

Feldman, J. (2016). The simplicity principle in perception and cognition. Wiley Interdiscip. Rev. Cogn. Sci., 7(5), 330–340. https://doi.org/10.1002/wcs.1406

Gage, P. (1994). A new algorithm for data compression. The C Users Journal Archive. https://www.semanticscholar.org/paper/A-new-algorithm-for-data-compression-Gage/1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8

Goldwater, S., Griffiths, T. L., & Johnson, M. (2009). A bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1), 21–54. https://doi.org/10.1016/j.cognition.2009.03.008

Gruver, N., Finzi, M., Qiu, S., & Wilson, A. G. (2023). Large language models are Zero-Shot time series forecasters. http://arxiv.org/abs/2310.07820

--- TRANG 15 ---
Isbilen, E. S., & Christiansen, M. H. (2020). Chunk-Based memory constraints on the cultural evolution of language. Top. Cogn. Sci., 12(2), 713–726. https://doi.org/10.1111/tops.12376

Isbilen, E. S., McCauley, S. M., Kidd, E., & Christiansen, M. H. (2020). Statistically induced chunking recall: A Memory-Based approach to statistical learning. Cogn. Sci., 44(7), e12848. https://doi.org/10.1111/cogs.12848

Kudo, T. (2018). Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates. In I. Gurevych & Y. Miyao (Eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 66–75). Association for Computational Linguistics. https://doi.org/10.18653/v1/P18-1007

Kudo, T., & Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 66–71. https://doi.org/10.18653/v1/D18-2012

Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020, February 8). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. https://doi.org/10.48550/arXiv.1909.11942

Lecun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proc. IEEE, 86(11), 2278–2324. https://doi.org/10.1109/5.726791

Lieber, O., Sharir, O., Lenz, B., & Shoham, Y. (2021). Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 1.

Meltzoff, A. N., Kuhl, P. K., Movellan, J., & Sejnowski, T. J. (2009). Foundations for a new science of learning. Science, 325(5938), 284–288. https://doi.org/10.1126/science.1175626

Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. http://arxiv.org/abs/1301.3781

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022, March 4). Training language models to follow instructions with human feedback. https://doi.org/10.48550/arXiv.2203.02155

Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global vectors for word representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. https://doi.org/10.3115/v1/D14-1162

Perruchet, P., & Vinter, A. (1998). PARSER: A model for word segmentation. J. Mem. Lang., 39(2), 246–263. https://doi.org/10.1006/jmla.1998.2576

--- TRANG 16 ---
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2019). Exploring the limits of transfer learning with a unified Text-to-Text transformer. http://arxiv.org/abs/1910.10683

Rissanen, J. (1978). Modeling by shortest data description. Automatica, 14(5), 465–471. https://doi.org/10.1016/0005-1098(78)90005-5

Ruoss, A., Delétang, G., Genewein, T., Grau-Moya, J., Csordás, R., Bennani, M., Legg, S., & Veness, J. (2023). Randomized positional encodings boost length generalization of transformers. http://arxiv.org/abs/2305.16843

Schapiro, A. C., Turk-Browne, N. B., Norman, K. A., & Botvinick, M. M. (2016). Statistical learning of temporal community structure in the hippocampus. Hippocampus, 26(1), 3–8. https://doi.org/10.1002/hipo.22523

Schuster, M., & Nakajima, K. (2012). Japanese and Korean voice search. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 5149–5152. https://doi.org/10.1109/ICASSP.2012.6289079

Sennrich, R., Haddow, B., & Birch, A. (2016). Neural machine translation of rare words with subword units. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–1725. https://doi.org/10.18653/v1/P16-1162

Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., Tian, X., Zhu, D., Tian, H., & Wu, H. (2019, April 19). ERNIE: Enhanced Representation through Knowledge Integration. https://doi.org/10.48550/arXiv.1904.09223

Tian, Y., James, I., & Son, H. (2023). How Are Idioms Processed Inside Transformer Language Models? In A. Palmer & J. Camacho-collados (Eds.), Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023) (pp. 174–179). Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.starsem-1.16

Yang, J. (2022). Discovering the units in language cognition: From empirical evidence to a computational model [PhD thesis, Radboud University & Max Planck Institute for Psycholinguistics]. https://doi.org/10.13140/RG.2.2.35086.84804

Yang, J., Cai, Q., & Tian, X. (2020). How do we segment text? Two-stage chunking operation in reading. eNeuro, 7(3). https://doi.org/10.1523/ENEURO.0425-19.2020

Yang, J., Frank, S. L., & van den Bosch, A. (2020). Less is Better: A cognitively inspired unsupervised model for language segmentation. Proceedings of the Workshop on the Cognitive Aspects of the Lexicon, 33–45. https://www.aclweb.org/anthology/2020.cogalex-1.4

Yang, J., van den Bosch, A., & Frank, S. L. (2022). Unsupervised text segmentation predicts eye fixations during reading. Frontiers in Artificial Intelligence, 5. https://doi.org/10.3389/frai.2022.731615

--- TRANG 17 ---
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2020, January 2). XLNet: Generalized Autoregressive Pretraining for Language Understanding. https://doi.org/10.48550/arXiv.1906.08237

Zipf, G. K. (1949). Human behavior and the principle of least effort (Vol. 573). Addison-Wesley Press. https://psycnet.apa.org/fulltext/1950-00412-000.pdf
