# Chuyên biệt hóa từ vựng đa ngôn ngữ quy mô lớn của các Transformer đa ngôn ngữ

Tommaso Green1, Simone Paolo Ponzetto1, Goran Glavaš2
1Nhóm Khoa học Dữ liệu và Web, Đại học Mannheim, Đức
2CAIDAS, Đại học Würzburg, Đức
{tommaso.green, ponzetto}@uni-mannheim.de
goran.glavas@uni-wuerzburg.de

## Tóm tắt

Trong khi các mô hình ngôn ngữ đã huấn luyện trước (PLM) chủ yếu phục vụ như các bộ mã hóa văn bản đa mục đích có thể được tinh chỉnh cho nhiều tác vụ hạ nguồn khác nhau, nghiên cứu gần đây đã chỉ ra rằng chúng cũng có thể được tái kết nối để tạo ra các biểu diễn từ chất lượng cao (tức là, các embedding từ tĩnh) và mang lại hiệu suất tốt trong các tác vụ từ vựng cấp loại. Trong khi công việc hiện tại chủ yếu tập trung vào việc chuyên biệt hóa từ vựng của các PLM đơn ngôn ngữ, trong công trình này chúng tôi đưa các transformer đa ngôn ngữ quy mô lớn (MMT, ví dụ như mBERT hoặc XLM-R) tiếp xúc với kiến thức từ vựng đa ngôn ngữ quy mô lớn, tận dụng BabelNet như nguồn kiến thức từ vựng cấp loại đa ngôn ngữ và liên ngôn ngữ phong phú sẵn có. Cụ thể, chúng tôi sử dụng các synset đa ngôn ngữ của BabelNet để tạo ra các cặp từ đồng nghĩa (hoặc cặp từ đồng nghĩa-định nghĩa) trên 50 ngôn ngữ và sau đó đưa các MMT (mBERT và XLM-R) vào một quy trình chuyên biệt hóa từ vựng được hướng dẫn bởi một mục tiêu đối chiếu. Chúng tôi chỉ ra rằng việc chuyên biệt hóa từ vựng đa ngôn ngữ như vậy mang lại những cải thiện đáng kể trong hai tác vụ từ vựng liên ngôn ngữ chuẩn, cảm ứng từ điển song ngữ và độ tương tự từ liên ngôn ngữ, cũng như trong tìm kiếm câu liên ngôn ngữ. Quan trọng là, chúng tôi quan sát thấy những cải thiện đối với các ngôn ngữ không thấy trong chuyên biệt hóa, cho thấy rằng chuyên biệt hóa từ vựng đa ngôn ngữ cho phép tổng quát hóa cho các ngôn ngữ không có ràng buộc từ vựng. Trong một loạt các thí nghiệm được kiểm soát, chúng tôi chỉ ra rằng số lượng ràng buộc chuyên biệt hóa đóng vai trò quan trọng hơn nhiều so với tập hợp các ngôn ngữ mà chúng bắt nguồn từ đó.

## 1 Giới thiệu

Các transformer đa ngôn ngữ quy mô lớn (MMT) như mBERT (Devlin et al., 2019) và XLM-R (Conneau et al., 2020), cùng với những mô hình khác, đã là phương tiện chính của việc chuyển giao NLP liên ngôn ngữ, cung cấp hiệu suất tốt nhất cho nhiều tác vụ và ngôn ngữ đích trong các tình huống chuyển giao zero-shot và few-shot khác nhau (Pires et al., 2019; Wu và Dredze, 2019; Cao et al., 2020; Artetxe et al., 2020; Lauscher et al., 2020a; Zhao et al., 2021; Ruder et al., 2021, trong số những nghiên cứu khác). Tuy nhiên, ít công trình hơn đã điều tra khả năng của chúng như các bộ mã hóa từ cấp loại đa ngôn ngữ (Vulić et al., 2020b). Nghiên cứu gần đây, chủ yếu tập trung vào các PLM đơn ngôn ngữ, đã chứng minh rằng chúng có thể được biến thành các bộ mã hóa từ vựng cấp loại hiệu quả bằng cách sử dụng các ràng buộc từ vựng (Vulić et al., 2021; Liu et al., 2021), tức là một quá trình thường được gọi là chuyên biệt hóa từ vựng. Tuy nhiên, công việc hiện tại chỉ điều tra chuyên biệt hóa từ vựng trong môi trường đơn ngôn ngữ hoặc song ngữ, cụ thể là chuyên biệt hóa PLM cho một ngôn ngữ duy nhất hoặc một cặp ngôn ngữ bằng cách sử dụng các ràng buộc từ vựng đơn ngôn ngữ tiếng Anh hoặc các ràng buộc từ vựng song ngữ được dịch có nhiều nhiễu (Vulić et al., 2021). Điều này không chỉ không hiệu quả về mặt tính toán, vì một quá trình chuyên biệt hóa cần được thực hiện cho mỗi ngôn ngữ hoặc cặp ngôn ngữ, mà còn không khai thác được sự phong phú của kiến thức đa ngôn ngữ từ việc huấn luyện trước đồng thời của MMT trên nhiều ngôn ngữ (100+), cũng như lượng lớn kiến thức được tuyển chọn thủ công từ các cơ sở tri thức đa ngôn ngữ quy mô lớn như BabelNet (Navigli và Ponzetto, 2010).

Trong công trình này, ngược lại, chúng tôi điều tra việc chuyên biệt hóa từ vựng đa ngôn ngữ quy mô lớn của các MMT, tức là những lợi ích và hạn chế tiềm năng của một quy trình chuyên biệt hóa từ vựng duy nhất trong nhiều ngôn ngữ. Để làm điều này, chúng tôi khai thác BabelNet như nguồn tài nguyên từ vựng-ngữ nghĩa đa ngôn ngữ quy mô lớn sẵn có. Cụ thể, chúng tôi phát hành một bộ dữ liệu các cặp từ đồng nghĩa hoặc cặp từ đồng nghĩa-định nghĩa bao phủ 50 ngôn ngữ (đại diện cho 14 họ ngôn ngữ khác nhau) thu được từ các synset đa ngôn ngữ của BabelNet và tận dụng chúng như các trường hợp tích cực trong một quy trình huấn luyện chuyên biệt hóa đối chiếu. Đánh giá của chúng tôi trên hai tác vụ từ vựng đa ngôn ngữ - cảm ứng từ điển song ngữ (BLI) và độ tương tự từ ngữ nghĩa liên ngôn ngữ (XLSIM) - cũng như trên tác vụ tìm kiếm câu liên ngôn ngữ chứng minh hiệu quả của chuyên biệt hóa từ vựng đa ngôn ngữ khi so sánh với các MMT thông thường.

Chúng tôi bổ sung đánh giá của mình bằng các thí nghiệm chẩn đoán nhằm nghiên cứu các thuộc tính của các ràng buộc từ vựng đa ngôn ngữ có thể thúc đẩy hiệu suất từ vựng hạ nguồn của các mô hình chuyên biệt hóa, cụ thể là sự lựa chọn các ngôn ngữ và số lượng ràng buộc. Để làm điều này, chúng tôi thực hiện các thí nghiệm trong đó chúng tôi kiểm soát việc chuyên biệt hóa MMT cho (i) sự đa dạng ngôn ngữ học của ngôn ngữ được đại diện trong bộ dữ liệu chuyên biệt hóa và (ii) kích thước của bộ dữ liệu chuyên biệt hóa, tức là số lượng cặp từ đồng nghĩa từ BabelNet được sử dụng trong huấn luyện đối chiếu. Kết quả của các thí nghiệm chẩn đoán cho thấy rằng, một cách phản trực giác, sự đa dạng loại hình học của các ngôn ngữ được sử dụng trong chuyên biệt hóa (tức là, các ngôn ngữ chuyên biệt hóa) hầu như không có tác động nào trong việc xác định hiệu suất hạ nguồn. Những phát hiện này về chuyên biệt hóa đa ngôn ngữ cho các tác vụ từ vựng (cấp loại) tương phản với các quan sát cho các tác vụ cấp cao hơn, yêu cầu mô hình hóa ngữ nghĩa của câu hoặc cặp câu, trong đó cả việc chuyên biệt hóa/tinh chỉnh đa nguồn trên các ngôn ngữ đa dạng (Chen et al., 2019; Ansell et al., 2021) và sự gần gũi ngôn ngữ học giữa các ngôn ngữ huấn luyện và đánh giá (Lin et al., 2019; Lauscher et al., 2020a) đã được chứng minh là ảnh hưởng mạnh mẽ đến hiệu suất chuyển giao. Đồng thời, chúng tôi thấy rằng hiệu suất căn chỉnh nhanh chóng bão hòa với ít ràng buộc: điều này chứng thực giả thuyết tái kết nối của Vulić et al. (2021), ở đây trong một môi trường đa ngôn ngữ quy mô lớn. Để khuyến khích nghiên cứu thêm về chủ đề này, chúng tôi phát hành mã nguồn và tất cả các bộ dữ liệu của chúng tôi về các ràng buộc từ vựng và cặp từ đồng nghĩa-định nghĩa tại https://github.com/umanlp/babelbert.

## 2 Công trình liên quan

Kiến thức từ vựng bên ngoài từ các nguồn tài nguyên từ vựng-ngữ nghĩa (ví dụ, WordNet, ConceptNet) đã được tận dụng rộng rãi để cải thiện các biểu diễn phân bố của từ - một quá trình thường được gọi là chuyên biệt hóa ngữ nghĩa. Công việc trước đây về chuyên biệt hóa ngữ nghĩa của các embedding từ có thể được chia thành (i) các phương pháp chuyên biệt hóa kết hợp (Yu và Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015, trong số những nghiên cứu khác), tích hợp các ràng buộc từ vựng bên ngoài vào việc (tiền)huấn luyện embedding từ và (ii) các kỹ thuật trang bị lại hoặc xử lý hậu kỳ sửa đổi sau hóc các embedding đã huấn luyện trước, điều chỉnh chúng theo các ràng buộc từ vựng bên ngoài (Faruqui et al., 2015; Wieting et al., 2015; Mrkšić et al., 2017; Vulić et al., 2018; Glavaš và Vulić, 2018; Ponti et al., 2018).

Kiến thức từ vựng bên ngoài cũng đã được sử dụng để làm phong phú các PLM đơn ngôn ngữ bằng cách kết hợp mô hình hóa ngôn ngữ có mặt nạ với một mục tiêu tiền huấn luyện phụ sử dụng các quan hệ từ vựng từ WordNet (Lauscher et al., 2020b; Levine et al., 2020). Tuy nhiên, tất cả những phương pháp này đều nhằm làm phong phú Transformer với kiến thức bổ sung để khai thác trong các tác vụ hạ nguồn khác nhau, thay vì tạo ra các biểu diễn từ cấp loại tốt hơn.

Một số mô hình để thu được các không gian biểu diễn đa ngôn ngữ được cải thiện về mặt ngữ nghĩa đã được đề xuất cho các embedding từ tĩnh: (i) căn-chỉnh-và-chuyên-biệt-hóa (Mrkšić et al., 2017) bắt đầu từ các không gian embedding đơn ngôn ngữ không được căn chỉnh lẫn nhau và cả căn chỉnh chúng và chuyên biệt hóa ngữ nghĩa cho độ tương tự ngữ nghĩa (trái ngược với các loại liên quan ngữ nghĩa khác) thông qua cả các ràng buộc từ vựng đa ngôn ngữ (tức là, các ràng buộc đơn ngôn ngữ cho nhiều ngôn ngữ) và liên ngôn ngữ; (ii) trong chuyển giao chuyên biệt hóa liên ngôn ngữ (Vulić et al., 2018; Glavaš và Vulić, 2018) không gian embedding của ngôn ngữ đích trước tiên được chiếu vào không gian (không chuyên biệt hóa) của ngôn ngữ có tài nguyên cao (thường là tiếng Anh) sử dụng một lượng hạn chế các căn chỉnh từ vựng nguồn-đích liên ngôn ngữ và sau đó được chuyên biệt hóa với mô hình chuyên biệt hóa được huấn luyện sử dụng các ràng buộc từ vựng đơn ngôn ngữ phong phú của ngôn ngữ nguồn giàu tài nguyên; (iii) chuyển giao ràng buộc (Ponti et al., 2019) dịch có nhiều nhiễu các ràng buộc ngôn ngữ nguồn (phong phú) sang các ngôn ngữ đích và sử dụng chúng để chuyên biệt hóa các không gian embedding đơn ngôn ngữ của các ngôn ngữ đích.

Gần đây hơn, Vulić et al. (2020b) đã chỉ ra rằng các transformer đã huấn luyện trước mã hóa một kho kiến thức từ vựng phong phú trong các tham số của chúng và có thể thu được từ chúng các biểu diễn từ cấp loại (tức là, tĩnh, ngoài ngữ cảnh) vượt trội hơn các biểu diễn thu được với các mô hình embedding từ như fastText (Bojanowski et al., 2017). Trong một phương pháp tiếp theo được gọi là LexFit (Vulić et al., 2021), họ chuyên biệt hóa các mô hình BERT đơn ngôn ngữ cho một loạt các ngôn ngữ sử dụng các ràng buộc đơn ngôn ngữ tiếng Anh từ WordNet và Roget's Thesaurus, và các ràng buộc được dịch tự động (Ponti et al., 2019) cho các ngôn ngữ khác tiếng Anh. Cuối cùng, sau khi họ thu được các biểu diễn từ tĩnh cho mỗi ngôn ngữ, họ cảm ứng một không gian song ngữ theo cách chuẩn, bằng một phép chiếu trực giao (Smith et al., 2017; Artetxe et al., 2019).

Hiệu quả của LexFit xuất phát từ thực tế là các PLM đặc thù ngôn ngữ có xu hướng tạo ra các biểu diễn tốt hơn cho một ngôn ngữ so với các MMT (ví dụ, mBERT) do "lời nguyền của tính đa ngôn ngữ" (Conneau et al., 2020; Pfeiffer et al., 2022). Tuy nhiên, điều này hạn chế khả năng áp dụng của nó cho một vài ngôn ngữ có các PLM đơn ngôn ngữ hiện có, quan trọng là loại trừ các ngôn ngữ có tài nguyên thấp. Phương pháp này cũng tốn kém về mặt tính toán vì nó đòi hỏi (i) một quá trình chuyên biệt hóa riêng biệt cho mỗi ngôn ngữ (cộng với việc dịch có nhiều nhiễu các ràng buộc tiếng Anh sang ngôn ngữ đích), và (ii) một sự căn chỉnh song ngữ cho mỗi cặp ngôn ngữ. Ngoài ra, nó không khai thác các ràng buộc từ vựng liên ngôn ngữ trong chuyên biệt hóa, chỉ trong căn chỉnh hậu kỳ. Trong công trình này, chúng tôi đề xuất một phương pháp chuyên biệt hóa từ vựng đa ngôn ngữ quy mô lớn duy nhất (được gọi là BabelBERT) để áp dụng cho các MMT đã huấn luyện trước. Để làm điều này, chúng tôi đề xuất thu được các ràng buộc từ vựng từ BabelNet, một nguồn tài nguyên từ vựng-ngữ nghĩa đa ngôn ngữ quy mô lớn (Navigli và Ponzetto, 2010; Navigli et al., 2021). BabelBERT có một số ưu điểm: (i) chúng tôi tận dụng các ràng buộc từ vựng đa ngôn ngữ (tức là, đơn ngôn ngữ từ nhiều ngôn ngữ) cũng như liên ngôn ngữ và (ii) thực hiện một quy trình chuyên biệt hóa đa ngôn ngữ duy nhất, thay vì chuyên biệt hóa riêng biệt cho mỗi ngôn ngữ hoặc cặp ngôn ngữ; (iii) việc chuyên biệt hóa đa ngôn ngữ trên các MMT loại bỏ nhu cầu về các transformer đặc thù ngôn ngữ và bổ sung cho phép các hiệu ứng chuyên biệt hóa lan truyền đến các ngôn ngữ không thấy, tức là các ngôn ngữ không có các ràng buộc từ vựng sẵn có.

## 3 Chuyên biệt hóa từ vựng đa ngôn ngữ

Chúng tôi đầu tiên mô tả cách chúng tôi tạo ra các ràng buộc từ vựng từ BabelNet trong §3.1 và sau đó cách chúng tôi tận dụng chúng trong một quy trình học đối chiếu trong §3.2.

### 3.1 Khai thác ràng buộc

Phù hợp với hầu hết công việc về chuyên biệt hóa từ vựng cho độ tương tự ngữ nghĩa, chúng tôi khai thác quan hệ từ vựng-ngữ nghĩa của từ đồng nghĩa để thu được các ràng buộc chuyên biệt hóa. Các synset đa ngôn ngữ của BabelNet cho phép chúng tôi đồng thời thu được cả các cặp từ đồng nghĩa đơn ngôn ngữ và liên ngôn ngữ. Cho L = {L1, ..., Ln} là tập hợp các ngôn ngữ được định trước mà chúng tôi khai thác các ràng buộc và W = {w1, ..., wN} là một danh sách các từ hạt giống. Đối với mỗi từ, chúng tôi đầu tiên thu được tất cả các synset BabelNet chứa từ đó, loại bỏ các synset đại diện cho các thực thể có tên và chỉ giữ lại các synset có ít nhất hai định nghĩa trong các ngôn ngữ từ L. Sau đó chúng tôi lặp qua tất cả các synset đã lấy, tạo ra tất cả các cặp từ đồng nghĩa có thể (đơn ngôn ngữ và liên ngôn ngữ) (w1, w2), mỗi cặp được liên kết với một cặp ngôn ngữ (L^word_1, L^word_2). Chúng tôi cũng trích xuất các định nghĩa (g1, g2), tức là các câu giải thích khái niệm của synset, trong các ngôn ngữ (L^gloss_1, L^gloss_2) khác với L^word_1 và L^word_2 tương ứng. Để đảm bảo chất lượng của các từ chúng tôi trích xuất, chúng tôi chỉ giữ lại một từ nếu nó nằm trong top-k từ trong danh sách tần suất ngôn ngữ của nó. Chúng tôi bổ sung loại bỏ các từ là bản dịch tự động hoặc chuyển hướng Wikipedia, loại bỏ các từ nhiều thành phần và xóa các bản trùng lặp.

### 3.2 Chuyên biệt hóa cho độ tương tự ngữ nghĩa

Quy trình chuyên biệt hóa từ vựng-ngữ nghĩa của chúng tôi dựa trên kiến trúc Bi-Encoder (thường cũng được gọi là kiến trúc Dual-Encoder hoặc Siamese) và một mục tiêu huấn luyện đối chiếu.

**Biểu diễn từ cấp loại.** Theo các công trình liên quan (Vulić et al., 2020b; Bommasani et al., 2020; Vulić et al., 2021), chúng tôi thu được các biểu diễn từ cấp loại từ một PLM một cách độc lập cho mỗi từ từ cặp từ đồng nghĩa: chúng tôi token hóa mỗi từ thành các từ con thành phần sw1...swm và đưa chuỗi [SPEC1][sw1]...[swm][SPEC2] vào MMT, với [SPEC1] và [SPEC2] biểu thị các token đặc biệt bắt đầu và kết thúc chuỗi của MMT (ví dụ, [CLS] và [SEP] tương ứng, đối với BERT). Chúng tôi nhận được biểu diễn cuối cùng e^type_w bằng cách gộp trung bình các biểu diễn của các từ con của nó (không bao gồm các token đặc biệt) từ lớp cuối cùng của bộ mã hóa Transformer.

**Biểu diễn từ cấp nghĩa.** Các biểu diễn cấp loại của các từ đa nghĩa, do cấu trúc, kết hợp tất cả các nghĩa của nó thành một embedding. Để giải quyết điều này và làm cho các biểu diễn nắm bắt một nghĩa cụ thể, chúng tôi tận dụng ngữ cảnh bổ sung thông qua thông tin cấp nghĩa được cung cấp bởi cơ sở tri thức. Ví dụ, trong số các nghĩa của từ "bat" được tìm thấy trong BabelNet, danh sách các nghĩa thường gặp nhất chứa synset cho động vật hoạt động vào ban đêm và câu lạc bộ thể thao (ví dụ, cái được sử dụng chẳng hạn trong cricket). Ngoài tập hợp các từ đồng nghĩa, tức là các synset khác nhau mà những nghĩa này thuộc về, nghĩa khác nhau của chúng được nắm bắt bởi các định nghĩa. Định nghĩa cho nghĩa động vật đọc như sau:

Động vật có vú giống chuột hoạt động vào ban đêm với các chi trước được biến đổi để tạo thành các cánh màng [...]

trong khi đối với gậy thể thao chúng ta thấy:

Gậy cricket là một thiết bị chuyên dụng được sử dụng bởi các người đánh bóng trong môn thể thao cricket [...]

Dưới ánh sáng của điều này, chúng tôi làm cho MMT bổ sung mã hóa các cặp từ-định nghĩa, để thu được các biểu diễn cấp nghĩa: để làm điều này, chúng tôi nối một trong những định nghĩa đã khai thác g vào từ như sau:
[SPEC1][sw1]...[swm][SPEC2]g[SPEC2]
và đưa điều này làm đầu vào cho MMT. Như với các biểu diễn cấp loại, chúng tôi nhận được biểu diễn cuối cùng e^sense_w bằng cách gộp trung bình các biểu diễn của chỉ các từ con sw1...swm (tức là, định nghĩa chỉ ở đó để ngữ cảnh hóa từ) từ lớp cuối cùng của bộ mã hóa Transformer.

**Mục tiêu đối chiếu.** Chúng tôi huấn luyện theo lô B = (w^(i)_1, w^(i)_2, syn^(i)_id)^NB_i=1 của các cặp từ đồng nghĩa, với syn^(i)_id biểu thị synset BabelNet của cặp từ đồng nghĩa (w^(i)_1, w^(i)_2). Trong huấn luyện cấp nghĩa, mỗi điểm dữ liệu bổ sung chứa các định nghĩa (g^(i)_1, g^(i)_2). Chúng tôi huấn luyện bằng cách tối thiểu hóa một biến thể của hàm mất mát đối chiếu InfoNCE phổ biến (van den Oord et al., 2018). Trong một lô duy nhất, có thể có nhiều hơn một cặp thuộc về cùng một synset, do đó chúng tôi tạo thành tất cả các cặp tích cực có thứ tự có thể trong một tập P, tức là các cặp từ với cùng synid.

L^B_InfoNCE = -1/|P| ∑_(w^(i)_1,w^(j)_2)∈P log(sim(e^(i)_w1,e^(j)_w2) / (-log(sim(e^(i)_w1,e^(j)_w2) + ∑_n∈N^(i) sim(e^(i)_w1,e^(n)))))

trong đó sim(e^(i)_w1,e^(j)_w2) = exp(cos(e^(i)_w1,e^(j)_w2))/τ, với τ là siêu tham số nhiệt độ và N^(i) là tập hợp các âm tính trong lô, tức là các từ từ các cặp khác trong lô đến từ một synset BabelNet khác với syn^(i)_id.

**Tinh chỉnh dựa trên Adapter.** Ngoài việc tinh chỉnh đầy đủ các tham số của MMT, chúng tôi thí nghiệm với chuyên biệt hóa từ vựng thông qua tinh chỉnh dựa trên adapter (Houlsby et al., 2019). Các adapter, được chứng minh hữu ích trong các tình huống học tuần tự và chuyển giao khác nhau (Pfeiffer et al., 2020b; Rücklé et al., 2020; Lauscher et al., 2021; Hung et al., 2022) là các mô-đun ít tham số được chèn vào các lớp của PLM trước khi chuyên biệt hóa (tức là, tinh chỉnh): trong quá trình chuyên biệt hóa, chỉ các tham số adapter được điều chỉnh, trong khi các tham số đã huấn luyện trước của PLM được giữ cố định. Chúng tôi áp dụng kiến trúc của Pfeiffer et al. (2020b), trong đó một adapter thắt cổ chai được chèn vào mỗi lớp Transformer.

## 4 Thiết lập thí nghiệm

**Các ràng buộc từ vựng đa ngôn ngữ.** Chúng tôi tập trung vào 50 ngôn ngữ đa dạng từ điểm chuẩn XTREME-R phổ biến (Ruder et al., 2021) mà chúng tôi báo cáo với các mã ngôn ngữ để ngắn gọn: af,ar,az,bg,bn,de,el,en,es,et,eu,fa,fi,fr,gu,he,hi,ht,hu,id,it,ja,jv,ka,kk,ko,lt,ml,mr,ms,my,nl,pa,pl,pt,qu,ro,ru,sw,ta,te,th,tl,tr,uk,ur,vi,wo,yo,zh. Mẫu bao phủ 14 họ ngôn ngữ (Afro-Asiatic, Austro-Asiatic, Austronesian, Dravidian, Indo-European, Japonic, Kartvelian, Kra-Dai, Niger-Congo, Sino-Tibetan, Turkic, Uralic, Creole, và Quechuan) và bổ sung chứa Basque và Korean như hai ngôn ngữ cô lập.

Chúng tôi thu thập các ràng buộc từ BabelNet với quy trình được mô tả trong §3.1. Làm từ hạt giống, chúng tôi chọn top-N từ tiếng Anh thường gặp nhất (N = 1,000, lọc các từ dừng sử dụng NLTK (Bird và Loper, 2004)) và chỉ giữ lại các từ thuộc về top-k (k = 15,000) từ trong danh sách tần suất của một ngôn ngữ. Tập huấn luyện tổng cộng bao gồm 761,273 ràng buộc từ vựng: chúng tôi cung cấp thống kê bổ sung của bộ dữ liệu trong phụ lục A và một vài ví dụ trong Bảng 3.

**Các tác vụ đánh giá.** Chúng tôi đánh giá trên hai tác vụ cấp từ liên ngôn ngữ chuẩn, cảm ứng từ điển song ngữ (BLI) và độ tương tự từ liên ngôn ngữ (XLSIM). Chúng tôi kết hợp điều này với một đánh giá về tìm kiếm câu liên ngôn ngữ không giám sát. Đối với các tác vụ cấp từ (XLSIM và BLI): a) chúng tôi đảm bảo không bao gồm trong tập huấn luyện các ràng buộc từ vựng từ BabelNet bất kỳ từ nào xuất hiện trong các tập kiểm tra; b) chúng tôi lấy gộp trung bình của các embedding của các từ con từ lớp có hiệu suất tốt nhất (xem Bảng 4) của MMT làm biểu diễn từ.

**Tác vụ 1: Cảm ứng từ điển song ngữ.** BLI kiểm tra chất lượng của một không gian biểu diễn đa ngôn ngữ (song ngữ) thông qua việc căn chỉnh từ cấp loại giữa các ngôn ngữ. Đối với một từ truy vấn đã cho từ một ngôn ngữ nguồn, các từ từ từ vựng của một ngôn ngữ đích được xếp hạng dựa trên độ tương tự của chúng với truy vấn. Vị trí của bản dịch chính xác của truy vấn trong xếp hạng ngôn ngữ đích phản ánh chất lượng của việc căn chỉnh từ cấp loại giữa các ngôn ngữ. Chúng tôi đánh giá trên hai điểm chuẩn được thiết lập tốt: G-BLI (Glavaš et al., 2019) bao phủ 28 cặp ngôn ngữ giữa 8 ngôn ngữ (de,en,fi,fr,hr,it,ru,tr), trong khi PanlexBLI (Vulić et al., 2019) trải rộng 15 ngôn ngữ đa dạng (bg,ca,eo,et,eu,fi,he,hu,id,ka,ko,lt,no,th,tr) cho tổng cộng 210 cặp ngôn ngữ. Đối với cả hai bộ dữ liệu, chúng tôi đánh giá trên các phần kiểm tra bao gồm 2,000 cặp từ mỗi cặp ngôn ngữ và báo cáo hiệu suất theo Mean Reciprocal Rank (MRR) như được khuyến nghị bởi Glavaš et al. (2019). Theo Vulić et al. (2020b), các từ vựng được sử dụng cho tìm kiếm bao phủ top 100K từ thường gặp nhất từ các vector Wikipedia fastText tương ứng (Bojanowski et al., 2017).

**Tác vụ 2: Độ tương tự từ liên ngôn ngữ.** XLSIM đo lường sự tương quan giữa các độ tương tự của các cặp từ liên ngôn ngữ thu được dựa trên các biểu diễn của chúng trong không gian biểu diễn đa ngôn ngữ (song ngữ) và các điểm số tương tự được gán bởi các chú thích viên con người. Chúng tôi đánh giá hiệu suất trên 66 cặp ngôn ngữ giữa 12 ngôn ngữ (zh,cy,en,et,fi,fr,he,pl,ru,es,sw,yue) từ bộ dữ liệu MultiSimLex (Vulić et al., 2020a) và sử dụng Spearman's ρ giữa các độ tương tự cosine giữa các embedding từ và các điểm số tương tự được gán bởi con người tương ứng.

**Tác vụ 3: Tìm kiếm câu liên ngôn ngữ.** Đối với tìm kiếm câu liên ngôn ngữ, chúng tôi sử dụng bộ dữ liệu Tatoeba (Artetxe và Schwenk, 2019) bao gồm 112 ngôn ngữ, trong đó mỗi ngôn ngữ có 1,000 câu được ghép nối với các bản dịch của chúng trong tiếng Anh. Chúng tôi thu được embedding câu bằng cách gộp trung bình các biểu diễn của tất cả các token từ con của nó tại đầu ra của lớp Transformer có hiệu suất tốt nhất. Chúng tôi tính toán một cách đơn giản các độ tương tự giữa các câu như cosine của góc giữa các embedding của chúng. Chúng tôi so sánh mỗi câu truy vấn với hàng xóm gần nhất của nó và tính toán độ chính xác làm thước đo đánh giá của chúng tôi.

**Chi tiết huấn luyện.** Chúng tôi thí nghiệm sử dụng hai MMT khác nhau: multilingual BERT (mBERT) (Devlin et al., 2019) và XLM-R (Conneau et al., 2020). Trong tất cả các thí nghiệm, chúng tôi đặt nhiệt độ của hàm mất mát InfoNCE thành τ = 0.07.

Để tính đến phân bố rất lệch của các ràng buộc trên các cặp ngôn ngữ (cf. Hình 3), theo Conneau và Lample (2019), chúng tôi lấy mẫu các ràng buộc lô từ phân bố đa thức {qi,j} trên các cặp ngôn ngữ (Li, Lj) như sau:

qi,j = p^α_i,j / ∑_k,l p^α_k,l, pi,j = ni,j / ∑_k,l nk,l

trong đó ni,j biểu thị số lượng cặp từ đồng nghĩa cho một cặp ngôn ngữ (Li, Lj) và α là hệ số làm mịn. Chúng tôi đặt α thành 0.5.

Đối với việc lựa chọn mô hình (cả tìm kiếm siêu tham số và lựa chọn checkpoint), chúng tôi tiến hành như sau. Chúng tôi chọn ngẫu nhiên hai cặp ngôn ngữ từ G-BLI và hai cặp ngôn ngữ từ PL-BLI và chọn các tập huấn luyện tương ứng - bao gồm 5,000 cặp - làm tập validation của chúng tôi. Trước khi huấn luyện, chúng tôi chạy một vòng validation để có điểm MRR của MMT vanilla không chuyên biệt hóa cho mỗi cặp ngôn ngữ. Trong quá trình huấn luyện, chúng tôi dừng lại mỗi một phần tư epoch để thực hiện validation: chúng tôi theo dõi cho mỗi trong số bốn cặp ngôn ngữ validation này sự cải thiện tương đối của MRR so với điểm vanilla. Chúng tôi sử dụng trung bình của bốn cải thiện tương đối này làm thước đo validation tổng thể hướng dẫn lựa chọn mô hình. Chúng tôi huấn luyện trong 15 epoch sử dụng AdamW (Loshchilov và Hutter, 2019) và sử dụng PytorchLightning (Falcon và The PyTorch Lightning team, 2019) cho triển khai của chúng tôi, kết hợp với các thư viện Huggingface Transformers (Wolf et al., 2020) và Pytorch Metric Learning (Musgrave et al., 2020). Đối với các mô hình dựa trên adapter, chúng tôi sử dụng thư viện adapter-transformers (Pfeiffer et al., 2020a).

**Siêu tham số và chi tiết huấn luyện** Đối với các mô hình được tinh chỉnh đầy đủ, chúng tôi tìm kiếm tỷ lệ học tối ưu lr ∈ {2e-5, 5e-6, 1e-6} và kích thước lô NB ∈ {32, 64}. Đối với các mô hình dựa trên adapter, chúng tôi bổ sung thử lr = 1e-4 và đặt tỷ lệ giảm adapter thành 16 (tức là, chúng tôi đặt kích thước thắt cổ chai của các adapter thành 48). Mỗi thí nghiệm được thực hiện trên một GPU NVIDIA V100 hoặc A100 duy nhất trên một cụm tính toán có sẵn của chúng tôi. Tính đến các thí nghiệm thất bại, tìm kiếm lưới và các lần chạy thành công, chúng tôi báo cáo 331 ngày tính toán (bao gồm thời gian CPU cho tiền xử lý và tìm kiếm) như được ghi lại bởi logger Weights & Biases (Biewald, 2020). Chúng tôi cung cấp danh sách đầy đủ các giá trị siêu tham số trong Phụ lục (Bảng 4).

## 5 Kết quả và thảo luận

Chúng tôi trình bày kết quả chính của mình trong Bảng 1, trong đó chúng tôi so sánh quy trình chuyên biệt hóa từ vựng đa ngôn ngữ với các biểu diễn cấp loại sử dụng tinh chỉnh đầy đủ (Babel-FT) và điều chỉnh dựa trên adapter (Babel-Ad) và tinh chỉnh đầy đủ của MMT sử dụng các biểu diễn cấp nghĩa (Babel-Gl) so với các mô hình baseline được cung cấp bởi các MMT vanilla không chuyên biệt hóa. Chúng tôi hiển thị kết quả cho các biểu diễn từ đến từ lớp mà hiệu suất trung bình tốt nhất được thu được trên bộ dữ liệu đã cho: chúng tôi cung cấp thông tin về các lớp tối ưu cho các biểu diễn từ vựng trong Bảng 4 trong Phụ lục. Đối với mỗi mô hình, chúng tôi tính toán mỗi điểm số cặp ngôn ngữ như trung bình trên 3 lần chạy với các seed ngẫu nhiên khác nhau.

Nhìn chung, kết quả chỉ ra rằng tinh chỉnh từ vựng đa ngôn ngữ cải thiện hiệu suất của cả hai MMT (mBERT và XLM-R) trên tất cả các tác vụ. Cả ba biến thể chuyên biệt hóa (Babel-Ad/FT/Gl) đều mang lại hiệu suất tương tự trên tất cả ba tác vụ cho mBERT. Tuy nhiên, điều tương tự không đúng đối với XLM-R, trong đó tinh chỉnh đầy đủ (Babel-FT và Babel-Gl) dẫn đến hiệu suất tốt hơn đáng kể trên toàn bộ so với huấn luyện dựa trên adapter (Babel-Ad). Huấn luyện với thông tin cấp nghĩa dưới dạng các định nghĩa synset (Babel-Gl) dường như đặc biệt có lợi cho tìm kiếm câu liên ngôn ngữ trên Tatoeba (Ttb) - chúng tôi giả định rằng điều này là do, giống như phần còn lại của các câu trong Tatoeba, các định nghĩa trong huấn luyện chuyên biệt hóa cung cấp ngữ cảnh câu cho các biểu diễn từ.

Thật thú vị, mặc dù thực tế là vanilla mBERT tạo ra các biểu diễn từ chất lượng cao hơn đáng kể so với vanilla XLM-R (ví dụ, 14.5 so với 8.5 trên G-BLI hoặc 10.3 so với 5.9 trên XLSIM), việc chuyên biệt hóa từ vựng đa ngôn ngữ của chúng tôi nghiêng kết quả theo hướng có lợi cho XLM-R, với kết quả BLI tương đương giữa hai mô hình và hiệu suất XLM-R tốt hơn nhiều trên XLSIM (khoảng cách 9 điểm) và Ttb (khoảng cách 15 điểm). Điều này gợi ý rằng XLM-R thực sự chứa kiến thức từ vựng đa ngôn ngữ phong phú hơn mBERT, tuy nhiên, được chôn sâu hơn trong mô hình (điều này được chứng thực bởi thực tế là đối với XLM-R chúng tôi thường nhận được các biểu diễn từ vựng tốt hơn từ các lớp Transformer thấp hơn, xem Phụ lục): một khi được khám phá thông qua việc chuyên biệt hóa từ vựng đa ngôn ngữ của chúng tôi, thông tin từ vựng phong phú hơn này của XLM-R sẽ nổi lên.

Hình 1 hiển thị năm cặp ngôn ngữ cho mỗi tác vụ/bộ dữ liệu đánh giá mà chúng tôi quan sát thấy sự cải thiện hiệu suất lớn nhất với Babel-FT mBERT. Trong PL-BLI, chúng tôi thấy hai ngôn ngữ, Na Uy và Catalan, mà không có ràng buộc nào được nhìn thấy trong quá trình huấn luyện: điều này cho thấy rằng những lợi ích của việc chuyên biệt hóa từ vựng đa ngôn ngữ quy mô lớn lan truyền đến các ngôn ngữ không thấy. Điều chúng tôi thấy đặc biệt khuyến khích là thực tế rằng các cặp Tatoeba với những cải thiện lớn nhất bao gồm một số ngôn ngữ và phương ngữ có tài nguyên thấp (ví dụ, tiếng Trung Wu và Yu, Tamil, và Interlingue), một số trong đó thậm chí không có mặt trong MMT trong quá trình huấn luyện trước.

### 5.1 Phân tích bổ sung

Chúng tôi thực hiện các ablation bổ sung để cố gắng cô lập các yếu tố dẫn đến những cải thiện hiệu suất từ việc chuyên biệt hóa từ vựng đa ngôn ngữ quy mô lớn. Để làm điều này, chúng tôi thực hiện các thí nghiệm trong đó chúng tôi thay đổi (i) sự đa dạng loại hình học của mẫu các ngôn ngữ mà chúng tôi lấy các ràng buộc BabelNet và (ii) kích thước của tập huấn luyện các ràng buộc từ vựng được sử dụng cho chuyên biệt hóa.

**Vai trò của sự đa dạng ngôn ngữ.** Chúng tôi điều tra cách thay đổi sự đa dạng loại hình học của mẫu các ngôn ngữ mà chúng tôi rút ra các ràng buộc chuyên biệt hóa ảnh hưởng đến việc tổng quát hóa cho các ngôn ngữ không thấy. Tức là, chúng tôi kiểm tra liệu một lựa chọn các ngôn ngữ với các mức độ đa dạng ngôn ngữ khác nhau và không có sự chồng chéo với các ngôn ngữ kiểm tra có tác động đến việc chuyên biệt hóa từ vựng đa ngôn ngữ của các MMT. Để định lượng sự đa dạng ngôn ngữ, chúng tôi mượn chỉ số đa dạng loại hình học dtyp từ Ponti et al. (2020). Đối với một mẫu ngôn ngữ S, chúng tôi tính toán chỉ số dựa trên các vector URIEL (Littell et al., 2017) của các ngôn ngữ trong mẫu. Chúng tôi thu được dtyp của mẫu S bằng cách tính toán một giá trị entropy cho mỗi đặc trưng trên tất cả các ngôn ngữ trong S: giá trị như vậy là 0 nếu tất cả các ngôn ngữ có giá trị giống hệt nhau cho đặc trưng đó. Sau đó chúng tôi lấy trung bình các điểm số entropy trên tất cả các đặc trưng.

Chúng tôi chọn PL-BLI làm điểm chuẩn cho phân tích này vì nó đã chứng minh là tác vụ từ vựng thách thức nhất. Chúng tôi đầu tiên tạo ra 10 mẫu, mỗi mẫu chứa 10 ngôn ngữ, như sau: chúng tôi đầu tiên lấy mẫu 1 triệu mẫu ngôn ngữ khác nhau của 10 ngôn ngữ, đảm bảo không có mẫu nào trong số chúng chứa bất kỳ ngôn ngữ kiểm tra PL-BLI nào. Sau đó chúng tôi chia chúng thành 10 bin theo dtyp và chọn ngẫu nhiên một mẫu từ mỗi bin. Đối với mỗi mẫu, chúng tôi khai thác các cặp từ đồng nghĩa từ BabelNet từ đầu, chỉ xem xét những ngôn ngữ đó, và đảm bảo rằng mỗi cặp ngôn ngữ có chính xác 100 ràng buộc - dẫn đến tổng cộng 5,500 trường hợp cho mỗi mẫu (tính cả các ràng buộc liên ngôn ngữ và đơn ngôn ngữ). Trong nỗ lực hạn chế rò rỉ ngôn ngữ kiểm tra trong quy trình huấn luyện, khác với các thí nghiệm chính, chúng tôi chỉ validate trên hai cặp ngôn ngữ từ G-BLI để lựa chọn mô hình. Chúng tôi thực hiện các thí nghiệm với Babel-FT mBERT như mô hình có hiệu suất tốt nhất của chúng tôi trên PL-BLI. Chúng tôi tinh chỉnh trong 10 epoch với các siêu tham số giống như được sử dụng trong các thí nghiệm chính nhưng không có upsampling vì tất cả các cặp ngôn ngữ đều có cùng số lượng ràng buộc. Chúng tôi báo cáo kết quả, cùng với các mẫu ngôn ngữ và điểm số dtyp của chúng trong Bảng 2. Đáng ngạc nhiên, chúng tôi quan sát thấy một sự tương quan kém giữa dtyp của mẫu ngôn ngữ và hiệu suất PL-BLI tương ứng.

Chúng tôi bổ sung định lượng mức độ tương tự giữa các ngôn ngữ của mỗi mẫu và các ngôn ngữ được bao gồm trong PL-BLI: simtrain−test (cũng được hiển thị trong Bảng 2) là trung bình của các độ tương tự từng cặp của các vector URIEL giữa các ngôn ngữ của hai tập hợp.

Chúng tôi lại quan sát thấy một sự tương quan kém giữa simtrain−test và hiệu suất PL-BLI, cho thấy rằng sự gần gũi của các ngôn ngữ huấn luyện và kiểm tra không đóng vai trò. Mặc dù phản trực giác, điều này thực sự có lợi: nó cho thấy rằng chúng tôi có thể, với thành công chuyên biệt hóa tương tự, tận dụng các ràng buộc từ một loạt rộng các ngôn ngữ, điều này cho phép chúng tôi khai thác chúng cho các ngôn ngữ có tài nguyên cao mà kiến thức từ vựng có cấu trúc phong phú hơn.

**Vai trò của kích thước ràng buộc.** Chúng tôi thực hiện các thí nghiệm bổ sung để điều tra tác động của kích thước tập huấn luyện (tức là, tổng số ràng buộc). Để làm điều này, chúng tôi tạo ra ba tập huấn luyện bổ sung với kích thước 1K, 10K, và 100K trường hợp, với cùng phân bố tương đối của các ràng buộc trên các cặp ngôn ngữ như trong huấn luyện đầy đủ. Sau đó chúng tôi đưa mBERT vào chuyên biệt hóa Babel-FT trong 10 epoch (cùng siêu tham số và quy trình huấn luyện như trong đánh giá chính). Đối với mỗi tập huấn luyện, chúng tôi thực hiện 3 lần chạy khác nhau và báo cáo trung bình trong Hình 2. Trên tất cả bốn điểm chuẩn, chúng tôi quan sát thấy cùng hành vi tổng thể: hiệu suất bão hòa đã với 10K ràng buộc. Tatoeba, tác vụ cấp câu duy nhất trong thiết lập của chúng tôi, dường như là tác vụ hưởng lợi nhiều nhất từ một tập huấn luyện lớn hơn. Ngược lại, hiệu suất XLSIM bão hòa đã với 1K ràng buộc từ vựng, điều này dường như phù hợp với các phát hiện của Vulić et al. (2021).

Vulić et al. (2021) đã đề xuất và thực nghiệm validate giả thuyết tái kết nối - rằng chuyên biệt hóa từ vựng chủ yếu phơi bày kiến thức đã có mặt trong các trọng số đã huấn luyện trước, thay vì tiêm kiến thức mới. Chúng tôi tin rằng kết quả của chúng tôi xác nhận tuyên bố này: nếu chuyên biệt hóa chủ yếu tái hiện kiến thức từ vựng ẩn trong các trọng số, điều này đặt ra một giới hạn cho hiệu suất hạ nguồn. Trong trường hợp của chúng tôi, chúng tôi thấy rằng kiến thức như vậy dường như độc lập với cả sự đa dạng loại hình học của các ngôn ngữ huấn luyện và sự tương tự loại hình học của các ngôn ngữ huấn luyện và kiểm tra: theo nghĩa này, MMT dường như đang học một hàm căn chỉnh từ vựng bất khả tri ngôn ngữ ảnh hưởng đến toàn bộ không gian biểu diễn của nó. Hơn nữa, việc học hàm này dường như chỉ yêu cầu khoảng 10K mẫu, với hiệu suất nhanh chóng bão hòa với nhiều ràng buộc hơn.

## 6 Kết luận

Trong công trình này, chúng tôi đã trình bày một phương pháp chuyên biệt hóa từ vựng đa ngôn ngữ tận dụng kiến thức từ vựng đa ngôn ngữ quy mô lớn có sẵn trong BabelNet. Khác với các phương pháp trước đây, thực hiện các quy trình chuyên biệt hóa đơn ngôn ngữ hoặc song ngữ, chúng tôi đưa các MMT vào một chế độ huấn luyện duy nhất với các ràng buộc từ vựng từ 50 ngôn ngữ và báo cáo những cải thiện đáng kể so với baseline không chuyên biệt hóa (tức là, MMT không chịu chuyên biệt hóa từ vựng). Chúng tôi thực hiện cả chuyên biệt hóa từ vựng cấp loại, tức là với các từ được đưa vào transformer một cách riêng lẻ, và chuyên biệt hóa từ vựng cấp nghĩa, bằng cách đi kèm mỗi từ với một định nghĩa. Chúng tôi thực hiện một loạt các thí nghiệm bổ sung để nghiên cứu các yếu tố thúc đẩy của chuyên biệt hóa từ vựng: trong một thí nghiệm, chúng tôi giữ kích thước tập huấn luyện cố định trong khi đa dạng hóa các ngôn ngữ trong tập huấn luyện. Chúng tôi quan sát thấy rằng điều này dường như không có tác động đáng kể đến hiệu suất tổng thể. Trong một thí nghiệm tiếp theo, chúng tôi lại sử dụng các ràng buộc bao gồm 50 ngôn ngữ, nhưng hạn chế số lượng ràng buộc mỗi cặp ngôn ngữ: chúng tôi thấy rằng nhiều ràng buộc hơn giúp mô hình hoạt động tốt hơn, tuy nhiên, ít mẫu là cần thiết để đạt được hiệu suất đỉnh. Kết quả của chúng tôi hỗ trợ giả thuyết tái kết nối của Vulić et al. (2021) rằng chuyên biệt hóa từ vựng tái hiện kiến thức từ vựng hiện có được lưu trữ trong các MMT, thay vì tiêm nó. Việc trích xuất như vậy dường như độc lập với các ngôn ngữ huấn luyện và nhanh chóng bão hòa với ít ràng buộc.

**Hạn chế.** Mặc dù chúng tôi cố gắng thực hiện chuyên biệt hóa từ vựng đa ngôn ngữ trên một tập hợp các ngôn ngữ đa dạng về loại hình học, chúng tôi vẫn hạn chế phân tích của mình cho một phần nhỏ của tất cả các ngôn ngữ trên thế giới. Ngoài ra, phân tích của chúng tôi chỉ điều tra hai MMT - mặc dù có thể nói là hai mô hình được sử dụng rộng rãi nhất. Do hạn chế về phần cứng, chúng tôi đã thí nghiệm với XLM-R Base: kết quả chúng tôi báo cáo có thể khác đáng kể đối với XLM-R Large (hoặc các MMT lớn hơn khác như mT5), có thể mã hóa nhiều kiến thức từ vựng hơn.

**Cân nhắc đạo đức.** Chúng tôi tận dụng các ràng buộc từ vựng từ BabelNet, một nguồn tài nguyên được xây dựng bán tự động. BabelNet có thể chứa các liên kết từ vựng phản ánh các thiên vị xã hội tiêu cực (ví dụ, phân biệt giới tính hoặc phân biệt chủng tộc). Các ràng buộc thiên vị, khi được sử dụng làm dữ liệu huấn luyện trong chuyên biệt hóa của chúng tôi, có thể tăng cường các thiên vị xã hội có mặt trong các MMT.

**Lời cảm ơn** Tommaso Green và Simone Ponzetto đã được hỗ trợ bởi dự án JOIN-T 2 của Deutsche Forschungsgemeinschaft (DFG). Goran Glavaš đã được hỗ trợ bởi tài trợ EUINACTION được tài trợ bởi NORFACE Governance (462-19-010) thông qua Deutsche Forschungsgemeinschaft (DFG; GL950/2-1). Chúng tôi bổ sung thừa nhận sự hỗ trợ của bang Baden-Württemberg thông qua bwHPC và German Research Foundation (DFG) thông qua tài trợ INST 35/1597-1 FUGG. Chúng tôi cảm ơn đồng nghiệp Sotaro Takeshita vì những thảo luận sâu sắc trong quá trình phát triển dự án này và Ines Rehbein vì các bình luận của cô ấy về bản thảo của bài báo này.
