# GEMBA-MQM: Phát hiện các khoảng lỗi chất lượng dịch thuật bằng GPT-4

Tom Kocmi và Christian Federmann
Microsoft, One Microsoft Way, Redmond, WA-98052, USA
{tomkocmi,chrife}@microsoft.com

Tóm tắt
Bài báo này giới thiệu GEMBA-MQM, một chỉ số đánh giá dựa trên GPT được thiết kế để phát hiện các lỗi chất lượng dịch thuật, đặc biệt cho cài đặt ước lượng chất lượng mà không cần bản dịch tham chiếu của con người. Dựa trên sức mạnh của các mô hình ngôn ngữ lớn (LLM), GEMBA-MQM sử dụng kỹ thuật nhắc nhở ba mẫu cố định, truy vấn mô hình GPT-4 để đánh dấu các khoảng lỗi chất lượng. So với các nghiên cứu trước đây, phương pháp của chúng tôi có các lời nhắc bất khả tri ngôn ngữ, do đó tránh được nhu cầu chuẩn bị lời nhắc thủ công cho các ngôn ngữ mới. Trong khi các kết quả sơ bộ cho thấy GEMBA-MQM đạt được độ chính xác tiên tiến nhất cho việc xếp hạng hệ thống, chúng tôi khuyên nên thận trọng khi sử dụng nó trong các nghiên cứu học thuật để chứng minh sự cải tiến so với các phương pháp khác do sự phụ thuộc vào mô hình GPT độc quyền, hộp đen.

1 Giới thiệu
GEMBA-MQM xây dựng trên phát hiện gần đây rằng các mô hình ngôn ngữ lớn (LLM) có thể được nhắc nhở để đánh giá chất lượng dịch máy (Kocmi and Federmann, 2023a).

Nghiên cứu trước đó Kocmi and Federmann (2023a) (GEMBA-DA) đã áp dụng phương pháp đơn giản để đánh giá các giá trị điểm số đơn lẻ cho từng phân đoạn mà không chỉ định thang đo chi tiết. Sử dụng phương pháp zero-shot, kỹ thuật của họ đã cho thấy độ chính xác vô song trong đánh giá, vượt trội hơn tất cả các chỉ số không phải LLM khác trên bộ dữ liệu thử nghiệm WMT22 metrics (Freitag et al., 2022).

Tiếp theo, Lu et al. (2023) (EAPrompt) đã nghiên cứu việc nhắc nhở LLM để đánh giá các lớp lỗi riêng lẻ từ khung chỉ số chất lượng đa chiều (MQM) (Freitag et al., 2021), trong đó mỗi lỗi có thể được phân loại thành các lớp lỗi khác nhau (như độ chính xác, trôi chảy, phong cách, thuật ngữ, v.v.), các lớp phụ (độ chính xác > dịch sai), và được đánh dấu

| Chỉ số | Độ chính xác | Meta |
|--------|-------------|------|
| GEMBA-MQM | 96.5% (1) | 0.802 (3) |
| XCOMET-Ensemble | 95.2% (1) | 0.825 (1) |
| docWMT22CometDA | 93.7% (2) | 0.768 (9) |
| docWMT22CometKiwiDA | 93.7% (2) | 0.767 (9) |
| XCOMET-QE-Ensemble | 93.5% (2) | 0.808 (2) |
| COMET | 93.5% (2) | 0.779 (6) |
| MetricX-23 | 93.4% (3) | 0.808 (2) |
| CometKiwi | 93.2% (3) | 0.782 (5) |
| Calibri-COMET22 | 93.1% (3) | 0.767 (10) |
| BLEURT-20 | 93.0% (4) | 0.776 (7) |
| MaTESe | 92.8% (4) | 0.782 (5) |
| mre-score-labse-regular | 92.7% (4) | 0.743 (13) |
| mbr-bleurtxv1p-qe | 92.5% (4) | 0.788 (4) |
| KG-BERTScore | 92.5% (5) | 0.774 (7) |
| MetricX-23-QE | 92.0% (5) | 0.800 (3) |
| BERTscore | 90.2% (7) | 0.742 (13) |
| MS-COMET-QE-22 | 90.1% (8) | 0.744 (12) |
| embed_llama | 87.3% (10) | 0.701 (16) |
| f200spBLEU | 86.8% (11) | 0.704 (15) |
| BLEU | 85.9% (12) | 0.696 (16) |
| chrF | 85.2% (12) | 0.694 (17) |

Bảng 1: Kết quả sơ bộ của tác vụ chia sẻ chỉ số WMT 2023. Cột đầu tiên cho thấy độ chính xác cấp hệ thống, và cột thứ hai là siêu đánh giá Metrics 2023. Các chỉ số có nền xám cần bản dịch tham chiếu của con người. Bảng không chứa các chỉ số không chuẩn và hiệu suất kém nhất do lý do không gian.

với mức độ nghiêm trọng (nghiêm trọng, lớn, nhỏ). Điểm số phân đoạn được tính bằng cách tổng hợp các lỗi, mỗi lỗi được trọng số theo hệ số nghiêm trọng tương ứng (25, 5, 1). Trong khi phương pháp của họ sử dụng few-shot prompting với chiến lược chain-of-thought (Wei et al., 2022), phương pháp GEMBA-MQM của chúng tôi khác biệt ở hai khía cạnh: 1) Chúng tôi hợp lý hóa quy trình chỉ sử dụng single-step prompting, và 2) các lời nhắc của chúng tôi có thể áp dụng phổ quát trên các ngôn ngữ, tránh được nhu cầu chuẩn bị lời nhắc thủ công cho từng cặp ngôn ngữ.

Một nỗ lực đáng chú ý khác của Fernandes et al. (2023) song song với phương pháp EAPrompt, cũng đánh dấu các khoảng lỗi MQM. Ngược lại, phương pháp của họ sử dụng mô hình PaLM-2, tổng hợp các chú thích MQM để lấy mẫu một vài ví dụ cho lời nhắc. Các thí nghiệm fine-tuning của họ không cải thiện hiệu suất cấp hệ thống cho các mô hình hàng đầu.

(Hệ thống) Bạn là người chú thích cho chất lượng dịch máy. Nhiệm vụ của bạn là xác định các lỗi và đánh giá chất lượng bản dịch.

(người dùng) Nguồn {source_language}:\n
```{source_segment} ```\n
Bản dịch {target_language}:\n
```{target_segment} ```\n
\n
Dựa trên phân đoạn nguồn và bản dịch máy được bao quanh bởi ba dấu backtick, xác định các loại lỗi trong bản dịch và phân loại chúng. Các danh mục lỗi là: độ chính xác (thêm vào, dịch sai, bỏ sót, văn bản không được dịch), trôi chảy (mã hóa ký tự, ngữ pháp, không nhất quán, dấu câu, phong cách, chính tả),
quy ước địa phương (định dạng tiền tệ, ngày tháng, tên, điện thoại, hoặc thời gian)
phong cách (lúng túng), thuật ngữ (không phù hợp với ngữ cảnh, sử dụng không nhất quán), không dịch, khác, hoặc không có lỗi.\n
Mỗi lỗi được phân loại thành một trong ba danh mục: nghiêm trọng, lớn, và nhỏ.
Các lỗi nghiêm trọng cản trở việc hiểu văn bản. Các lỗi lớn làm gián đoạn dòng chảy, nhưng những gì văn bản đang cố gắng nói vẫn có thể hiểu được. Các lỗi nhỏ về mặt kỹ thuật là lỗi, nhưng không làm gián đoạn dòng chảy hoặc cản trở việc hiểu.

(trợ lý) {các lớp lỗi được quan sát}

Hình 1: Lời nhắc chung cho GEMBA-MQM bỏ qua phần màu xám có hiệu suất kém trên dữ liệu nội bộ (chúng tôi bao gồm nó trong GEMBA-locale-MQM). Phần "(user)" và "(assistant)" được lặp lại cho mỗi ví dụ few-shot.

2 Mô tả
Kỹ thuật của chúng tôi áp dụng few-shot learning với mô hình GPT-4 (OpenAI, 2023), nhắc nhở mô hình đánh dấu các khoảng lỗi chất lượng bằng khung MQM. Mẫu lời nhắc cơ bản được mô hình hóa dựa trên hướng dẫn cho người chú thích con người và được thể hiện trong Hình 1.

Ngược lại với các phương pháp khác, chúng tôi sử dụng ba ví dụ được xác định trước (xem Phụ lục A), cho phép phương pháp được sử dụng với bất kỳ cặp ngôn ngữ nào, tránh được nhu cầu tạo ra các ví dụ MQM few-shot cụ thể cho từng cặp ngôn ngữ. Đây là hạn chế ban đầu đã ngăn Fernandes et al. (2023) đánh giá AutoMQM vượt qua hai cặp ngôn ngữ. Quyết định của chúng tôi không được thúc đẩy bởi mong muốn nâng cao hiệu suất — vì các lời nhắc cụ thể cho miền và ngôn ngữ thường tăng cường nó (Moslem et al., 2023) — mà là để đảm bảo phương pháp của chúng tôi có thể được đánh giá trên bất kỳ cặp ngôn ngữ nào.

3 Thí nghiệm
Để đo lường hiệu suất của chỉ số GEMBA-MQM, chúng tôi theo phương pháp luận và sử dụng dữ liệu thử nghiệm được cung cấp bởi tác vụ chia sẻ chỉ số WMT22 (Freitag et al., 2022) mà tổ chức đánh giá hàng năm các chỉ số tự động, so sánh chúng với nhãn vàng của con người.

Chúng tôi so sánh phương pháp của mình với các chỉ số dựa trên tham chiếu hiệu suất tốt nhất của WMT22: MetrixX_XXL (chỉ số không công khai), COMET-22 (Rei et al., 2022), UNITE (Wan et al., 2022b), BLEURT-20 (Pu et al., 2021), và COMET-20 (Rei et al., 2020). Ngoài ra, chúng tôi cũng so sánh với các chỉ số dựa trên chuỗi "cổ điển" BLEU (Papineni et al., 2002) và ChrF (Popović, 2015).

Cuối cùng, chúng tôi so sánh với các chỉ số không tham chiếu của WMT22: CometKIWI (Rei et al., 2022), Unite-src (Wan et al., 2022a), Comet-QE (Rei et al., 2021), MS-COMET-QE-22 (Kocmi et al., 2022b).

Chúng tôi đối chiếu công việc của mình với các phương pháp đánh giá dựa trên LLM khác như GEMBA-DA (Kocmi and Federmann, 2023b) và EAPrompt (Lu et al., 2023), tiến hành thí nghiệm sử dụng hai mô hình GPT: GPT-3.5-Turbo và GPT-4 mạnh mẽ hơn (OpenAI, 2023).

3.1 Bộ dữ liệu thử nghiệm
Đánh giá chính của nghiên cứu chúng tôi đã được thực hiện trên MQM22 (Freitag et al., 2022) và dữ liệu nội bộ Microsoft. Hơn nữa, vài ngày trước thời hạn camera-ready, các tổ chức của Metrics 2023 (Freitag et al., 2023) đã công bố kết quả trên bộ dữ liệu thử nghiệm mù, cho thấy hiệu suất trên dữ liệu chưa được thấy.

Bộ dữ liệu thử nghiệm MQM22 chứa các đánh giá của con người cho ba hướng dịch: tiếng Anh sang tiếng Đức, tiếng Anh sang tiếng Nga, và tiếng Trung sang tiếng Anh. Bộ dữ liệu thử nghiệm chứa tổng cộng 54 đầu ra hệ thống dịch máy hoặc bản dịch của con người. Nó chứa tổng cộng 106k phân đoạn. Các hệ thống dịch chủ yếu từ những người tham gia tác vụ chia sẻ MT chung WMT22 (Kocmi et al., 2022a). Các phân đoạn nguồn và bản dịch tham chiếu của con người

cho mỗi cặp ngôn ngữ chứa khoảng 2.000 câu từ bốn miền văn bản khác nhau: tin tức, xã hội, đối thoại, và thương mại điện tử. Tiêu chuẩn vàng để chấm điểm chất lượng dịch dựa trên xếp hạng MQM của con người, được chú thích bởi các chuyên gia đánh dấu các lỗi riêng lẻ trong mỗi bản dịch, như được mô tả trong Freitag et al. (2021).

Bộ dữ liệu thử nghiệm MQM23 là bộ dữ liệu mù cho tác vụ chia sẻ chỉ số WMT năm nay được chuẩn bị theo cùng cách như MQM22, nhưng với dữ liệu chưa được thấy cho tất cả những người tham gia, làm cho nó trở thành đánh giá đáng tin cậy nhất vì cả những người tham gia và LLM đều không thể overfit với những dữ liệu đó. Sự khác biệt chính so với phiên bản năm ngoái là việc thay thế tiếng Anh sang tiếng Nga bằng tiếng Hebrew sang tiếng Anh. Ngoài ra, một số miền đã được cập nhật; xem Kocmi et al. (2023).

Ngoài ra, chúng tôi đã đánh giá GEMBA-MQM trên một bộ dữ liệu thử nghiệm nội bộ lớn, một phiên bản mở rộng của bộ dữ liệu được mô tả bởi Kocmi et al. (2021). Bộ dữ liệu thử nghiệm này chứa điểm số của con người được thu thập với Direct Assessment (DA, Graham et al., 2013) dựa trên nguồn và biến thể DA+SQM (Kocmi et al., 2022a). Bộ dữ liệu thử nghiệm này chứa 15 ngôn ngữ tài nguyên cao được ghép nối với tiếng Anh. Cụ thể, đây là: tiếng Ả Rập, tiếng Séc, tiếng Hà Lan, tiếng Pháp, tiếng Đức, tiếng Hindi, tiếng Ý, tiếng Nhật, tiếng Hàn, tiếng Ba Lan, tiếng Bồ Đào Nha, tiếng Nga, tiếng Trung giản thể, tiếng Tây Ban Nha, và tiếng Thổ Nhĩ Kỳ.

3.2 Phương pháp đánh giá
Trường hợp sử dụng chính của các chỉ số tự động là xếp hạng hệ thống, dù khi so sánh một baseline với một mô hình mới, khi tuyên bố kết quả tiên tiến nhất, khi so sánh các kiến trúc mô hình khác nhau trong các nghiên cứu ablation, hoặc khi quyết định có nên triển khai một mô hình mới vào sản xuất hay không. Do đó, chúng tôi tập trung vào một phương pháp đặc biệt đo lường mục tiêu này: độ chính xác theo cặp cấp hệ thống (Kocmi et al., 2021).

Độ chính xác theo cặp được định nghĩa là số cặp hệ thống được xếp hạng chính xác bởi chỉ số so với xếp hạng của con người chia cho tổng số so sánh cặp hệ thống.

Chính thức:
Độ chính xác = |sign(metric Δ) == sign(human Δ)| / |tất cả cặp hệ thống|

Chúng tôi đã tái tạo tất cả điểm số được báo cáo trong bài báo phát hiện tác vụ chia sẻ chỉ số WMT22 bằng cách sử dụng script WMT22 chính thức. Các điểm số được báo cáo khớp với Bảng 11 của bài báo phát hiện chỉ số WMT22 (Freitag et al., 2022).

Hơn nữa, các tổ chức của tác vụ chia sẻ chỉ số 2023 đã xác định một chỉ số siêu đánh giá mới dựa trên bốn tình huống khác nhau, mỗi tình huống đóng góp vào điểm số cuối cùng với trọng số 0.25:
- độ chính xác theo cặp cấp hệ thống;
- tương quan Pearson cấp hệ thống;
- Accuracy-t cấp phân đoạn (Deutsch et al., 2023); và
- tương quan Pearson cấp phân đoạn.

Động lực là đo lường các chỉ số trong các tình huống sử dụng phổ biến nhất (ví dụ, để lọc cấp phân đoạn) và không chỉ để xếp hạng hệ thống. Tuy nhiên, chúng tôi đặt câu hỏi về quyết định sử dụng tương quan Pearson, đặc biệt là ở cấp hệ thống. Như Mathur et al. (2020) đã chỉ ra, Pearson được sử dụng để đánh giá chỉ số nhạy cảm khi áp dụng cho các kích thước mẫu nhỏ (trong MQM23, kích thước mẫu chỉ có 12 hệ thống); nó bị ảnh hưởng nặng nề bởi các điểm bất thường (Osborne and Overbay, 2004; Ma et al., 2019), cần được loại bỏ trước khi chạy đánh giá; và nó đo lường tương quan tuyến tính với dữ liệu MQM vàng, không nhất thiết phải tuyến tính từ đầu (đặc biệt là các điểm số cấp phân đoạn rời rạc, với trọng số lỗi 0.1, 1, 5, 25).

Mặc dù mong muốn có một chỉ số tự động tương quan cao với hành vi chú thích của con người và hữu ích cho đánh giá cấp phân đoạn, cần có thêm nghiên cứu về cách thức thích hợp để kiểm tra những tính chất này.

4 Kết quả
Trong phần này, chúng tôi thảo luận về các kết quả quan sát được trên ba bộ dữ liệu thử nghiệm khác nhau: 1) dữ liệu thử nghiệm MQM từ WMT, 2) dữ liệu thử nghiệm nội bộ từ Microsoft, và 3) một tập con của dữ liệu thử nghiệm nội bộ để đo lường tác động của quy ước địa phương MQM.

4.1 Kết quả trên dữ liệu thử nghiệm MQM từ WMT
Kết quả của bộ dữ liệu mù MQM23 trong Bảng 1 cho thấy GEMBA-MQM vượt trội hơn tất cả các kỹ thuật khác trên ba ngôn ngữ được đánh giá trong tình huống xếp hạng hệ thống. Hơn nữa, khi được đánh giá trong tình huống siêu đánh giá, nó đạt được thứ hạng cụm thứ ba.

Ngoài các kết quả chính thức, chúng tôi cũng thử nghiệm trên dữ liệu thử nghiệm MQM22 và cho thấy kết quả trong Bảng 2. Kết luận chính là tất cả các biến thể GEMBA-MQM đều vượt trội hơn các chỉ số truyền thống (như COMET hoặc Metric XXL). Khi tập trung vào tác vụ ước lượng chất lượng

| Chỉ số | Độ chính xác |
|--------|-------------|
| EAPrompt-Turbo | 90.9% |
| GEMBA-DA-GPT4 | 89.8% |
| GEMBA-locale-MQM-Turbo | 89.8% |
| EAPrompt-Turbo | 89.4% |
| GEMBA-MQM-GPT4 | 89.4% |
| GEMBA-DA-GPT4 | 87.6% |
| GEMBA-DA-Turbo | 86.9% |
| GEMBA-MQM-Turbo | 86.5% |
| GEMBA-DA-Turbo | 86.5% |
| MetricX_XXL | 85.0% |
| BLEURT-20 | 84.7% |
| COMET-22 | 83.9% |
| COMET-20 | 83.6% |
| UniTE | 82.8% |
| COMETKiwi | 78.8% |
| COMET-QE | 78.1% |
| BERTScore | 77.4% |
| UniTE-src | 75.9% |
| MS-COMET-QE-22 | 75.5% |
| chrF | 73.4% |
| BLEU | 70.8% |

Bảng 2: Kết quả độ chính xác theo cặp cấp hệ thống cho bộ dữ liệu thử nghiệm tác vụ chỉ số WMT 22. Các chỉ số màu xám cần bản dịch tham chiếu mà không phải là trọng tâm của đánh giá hiện tại.

, chúng ta có thể thấy rằng phương pháp GEMBA-locale-MQM-Turbo vượt trội hơn một chút so với EAPrompt, đây là kỹ thuật tương tự gần nhất. Tuy nhiên, chúng ta có thể thấy rằng kỹ thuật cuối cùng của chúng tôi GEMBA-MQM đang hoạt động kém hơn đáng kể so với chỉ số GEMBA-locale-MQM, trong khi sự khác biệt duy nhất là việc loại bỏ lớp lỗi quy ước địa phương. Chúng tôi tin rằng điều này được gây ra bởi bộ dữ liệu thử nghiệm. Chúng tôi thảo luận về quyết định loại bỏ lớp lỗi quy ước địa phương trong Phần 4.3.

4.2 Kết quả trên dữ liệu thử nghiệm nội bộ
Bảng 3 cho thấy GEMBA-MQM-Turbo vượt trội hơn hầu hết tất cả các chỉ số khác, chỉ thua COMETKIWI-22. Điều này cho thấy một số hạn chế của đánh giá dựa trên GPT trên các bộ dữ liệu thử nghiệm mù. Do hạn chế về quyền truy cập, chúng tôi không có kết quả cho GPT-4, mà chúng tôi cho rằng sẽ vượt trội hơn mô hình GPT-3.5 Turbo. Chúng tôi để lại thí nghiệm này cho nghiên cứu tương lai.

4.3 Loại bỏ quy ước địa phương
Khi điều tra hiệu suất của GEMBA-locale-MQM trên một tập con của dữ liệu nội bộ (tiếng Séc và tiếng Đức), chúng tôi quan sát thấy một lỗi nghiêm trọng trong lời nhắc này liên quan đến lớp lỗi "quy ước địa phương". GPT đã gán lớp này cho các lỗi không liên quan đến bản dịch. Nó đã gắn cờ các câu tiếng Séc là lỗi quy ước địa phương khi tiền tệ Euro

| 15 ngôn ngữ | Cs + De |
|-------------|---------|
| Số cặp hệ thống (N) | 4,468 | 734 |
| COMETKiwi | 79.9 | 81.3 |
| GEMBA-locale-MQM-Turbo | 78.6 | 81.3 |
| GEMBA-MQM-Turbo | 78.4 | 83.0 |
| COMET-QE | 77.8 | 79.8 |
| COMET-22 | 76.5 | 79.2 |
| COMET-20 | 76.3 | 79.6 |
| BLEURT-20 | 75.8 | 79.7 |
| chrF | 68.1 | 70.6 |
| BLEU | 66.8 | 68.9 |

Bảng 3: Kết quả độ chính xác theo cặp cấp hệ thống cho bộ dữ liệu thử nghiệm nội bộ của chúng tôi. Cột đầu tiên là cho tất cả 15 ngôn ngữ, và cột thứ hai chỉ cho tiếng Séc và tiếng Đức. Tất cả ngôn ngữ đều được ghép nối với tiếng Anh.

| Nguồn | Vstupné do památky činí 16,50 Eur. |
|--------|-----------------------------------|
| Giả thuyết | Admission to the monument is 16.50 Euros. |
| Chú thích GPT | locale convention/currency: "euros" |

Bảng 4: Một ví dụ về lớp lỗi "quy ước địa phương" sai được đánh dấu bởi GEMBA-locale-MQM. Bản dịch là chính xác, tuy nhiên, chúng tôi cho rằng mô hình GPT có thể không thích việc sử dụng Euro trong văn bản tiếng Séc vì Euro không được sử dụng ở Cộng hòa Séc.

được đề cập, ngay cả khi bản dịch tốt, xem ví dụ trong Bảng 4. Chúng tôi cho rằng nó đã sử dụng lớp lỗi này để đánh dấu các phần không chuẩn cho một ngôn ngữ nhất định nhưng cần có thêm điều tra để đưa ra bất kỳ kết luận sâu sắc nào.

Đánh giá trên dữ liệu thử nghiệm nội bộ trong Bảng 4 cho thấy mức tăng 1.7% độ chính xác. Tuy nhiên, khi đánh giá trên 15 ngôn ngữ, chúng tôi quan sát thấy sự suy giảm nhỏ 0.2%. Đối với MQM22 trong Bảng 2, sự suy giảm còn lớn hơn.

Khi chúng tôi nhìn vào phân phối của các lớp lỗi trên mười lăm ngôn ngữ tài nguyên cao nhất trong Bảng 5, chúng tôi quan sát thấy 32% tất cả các lỗi cho GEMBA-locale-MQM được đánh dấu là quy ước địa phương gợi ý việc sử dụng sai GPT cho lớp lỗi này. Do đó, thay vì giải thích lớp này trong lời nhắc, chúng tôi đã loại bỏ nó. Điều này dẫn đến khoảng một nửa số lỗi địa phương ban đầu được phân bổ lại cho các lớp lỗi khác, trong khi nửa còn lại không được đánh dấu.

Tóm lại, chúng tôi quyết định loại bỏ lớp này vì nó không phù hợp với những gì chúng tôi mong đợi đo lường và cách GPT dường như đang sử dụng các lớp. Do đó, chúng tôi buộc GPT phải phân loại những lỗi đó bằng cách sử dụng các danh mục lỗi khác. Với hành vi khác nhau đối với dữ liệu thử nghiệm nội bộ và bên ngoài, điều này đáng được điều tra thêm trong nghiên cứu tương lai.

| Lớp lỗi | GEMBA-locale-MQM | GEMBA-MQM |
|---------|------------------|-----------|
| accuracy | 960.838 (39%) | 1.072.515 (51%) |
| locale con. | 808.702 (32%) | (0%) |
| fluency | 674.228 (27%) | 699.037 (33%) |
| style | 23.943 (1%) | 41.188 (2%) |
| terminology | 17.379 (1%) | 290.490 (14%) |
| Other errors | 4.126 (0%) | 10615 (1%) |
| Total | 2.489.216 | 2.113.845 |

Bảng 5: Phân phối lỗi cho cả hai loại lời nhắc trên tất cả phân đoạn của bộ dữ liệu thử nghiệm nội bộ cho mô hình Turbo.

5 Thận trọng với LLM "Hộp đen"
Mặc dù GEMBA-MQM là kỹ thuật tiên tiến nhất cho xếp hạng hệ thống, chúng tôi muốn thảo luận trong phần này về các hạn chế cố hữu của việc sử dụng LLM "hộp đen" (như GPT-4) khi tiến hành nghiên cứu học thuật.

Đầu tiên, chúng tôi muốn chỉ ra rằng GPT-4 là một mô hình độc quyền, dẫn đến một số vấn đề. Một trong số đó là chúng ta không biết nó được huấn luyện trên dữ liệu huấn luyện nào, do đó bất kỳ dữ liệu thử nghiệm được công bố nào cũng nên được coi là một phần của dữ liệu huấn luyện của chúng (và do đó có thể bị ô nhiễm). Thứ hai, chúng ta không thể đảm bảo rằng mô hình sẽ có sẵn trong tương lai, hoặc nó sẽ không được cập nhật trong tương lai, có nghĩa là bất kỳ kết quả nào từ mô hình như vậy chỉ có liên quan đối với thời điểm lấy mẫu cụ thể. Như Chen et al. (2023) đã chỉ ra, hiệu suất của mô hình đã dao động và giảm trong suốt năm 2023.

Vì điều này ảnh hưởng đến tất cả LLM độc quyền, chúng tôi ủng hộ việc tăng cường nghiên cứu sử dụng các mô hình có sẵn công khai, như LLama 2 (Touvron et al., 2023). Phương pháp này đảm bảo các phát hiện trong tương lai có thể được so sánh với cả LLM "hộp đen" trong khi cũng cho phép so sánh với các mô hình "mở".

6 Kết luận
Trong bài báo này, chúng tôi đã giới thiệu và đánh giá chỉ số GEMBA-MQM, một chỉ số dựa trên GPT để đánh dấu lỗi chất lượng dịch thuật. Kỹ thuật này tận dụng mô hình GPT-4 với chiến lược nhắc nhở ba mẫu cố định. Kết quả sơ bộ cho thấy GEMBA-MQM đạt được trình độ tiên tiến mới khi được sử dụng như một chỉ số cho xếp hạng hệ thống, vượt trội hơn các chỉ số đã được thiết lập như COMET và BLEURT-20.

Mặc dù LLama 2 không hoàn toàn mở, các tệp nhị phân của nó đã được phát hành. Do đó, khi sử dụng nó như một người chấm điểm, chúng ta đang sử dụng chính xác cùng một mô hình.

Chúng tôi muốn thừa nhận các hạn chế cố hữu gắn liền với việc sử dụng một mô hình độc quyền như GPT. Khuyến nghị của chúng tôi đối với cộng đồng học thuật là hãy thận trọng khi sử dụng GEMBA-MQM trên các mô hình GPT. Đối với nghiên cứu tương lai, chúng tôi muốn khám phá cách phương pháp của chúng tôi hoạt động với các LLM mở hơn khác như LLama 2 (Touvron et al., 2023). Việc xác nhận hành vi vượt trội trên các mô hình được phân phối công khai (ít nhất là các tệp nhị phân của chúng) có thể mở đường cho việc sử dụng rộng rãi hơn kỹ thuật này trong môi trường học thuật.

Hạn chế
Trong khi các phát hiện và kỹ thuật của chúng tôi với GEMBA-MQM mang lại những tiến bộ hứa hẹn trong việc đánh dấu lỗi chất lượng dịch thuật, điều cần thiết là phải làm nổi bật các hạn chế gặp phải trong nghiên cứu này.

– Phụ thuộc vào các mô hình GPT độc quyền: GEMBA-MQM phụ thuộc vào mô hình GPT-4, vẫn mang tính độc quyền. Chúng ta không biết mô hình được huấn luyện trên dữ liệu gì hoặc liệu cùng một mô hình có còn được triển khai hay không và do đó các kết quả có thể so sánh được.

Như Chen et al. (2023) đã chỉ ra, hiệu suất của mô hình đã dao động trong suốt năm 2023;

– Chỉ ngôn ngữ tài nguyên cao: Vì các đánh giá WMT chủ yếu tập trung vào các ngôn ngữ tài nguyên cao, chúng ta không thể kết luận liệu phương pháp này có hoạt động tốt trên các ngôn ngữ tài nguyên thấp hay không.

Lời cảm ơn
Chúng tôi rất biết ơn các nhà phê bình ẩn danh của chúng tôi vì những bình luận sâu sắc và sự kiên nhẫn đã giúp cải thiện bài báo. Chúng tôi muốn cảm ơn các đồng nghiệp của chúng tôi trong nhóm nghiên cứu Microsoft Translator vì phản hồi có giá trị của họ.

Tài liệu tham khảo
Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009.

Daniel Deutsch, George Foster, and Markus Freitag. 2023. Ties matter: Modifying kendall's tau for modern metric meta-evaluation. arXiv preprint arXiv:2305.14324.

Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André FT Martins, Graham Neubig, Ankush Garg, Jonathan H Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. arXiv preprint arXiv:2308.07286.

Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460–1474.

Markus Freitag, Nitika Mathur, Chi kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frédéric Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of wmt23 metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), Singapore, Singapore (Hybrid). Association for Computational Linguistics.

Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU – neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46–68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33–41, Sofia, Bulgaria. Association for Computational Linguistics.

Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Masaaki Nagata, Toshiaki Nakazawa, Martin Popel, and Maja Popović. 2023. Findings of the 2023 conference on machine translation (WMT23). In Proceedings of the Seventh Conference on Machine Translation (WMT), Singapore, Singapore (Hybrid). Association for Computational Linguistics.

Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, and Maja Popović. 2022a. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1–45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Tom Kocmi and Christian Federmann. 2023a. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520.

Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193–203, Tampere, Finland. European Association for Machine Translation.

Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478–494, Online. Association for Computational Linguistics.

Tom Kocmi, Hitokazu Matsushita, and Christian Federmann. 2022b. MS-COMET: More and better human judgements improve metric performance. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 541–548, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt.

Qingsong Ma, Johnny Wei, Ondřej Bojar, and Yvette Graham. 2019. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62–90, Florence, Italy. Association for Computational Linguistics.

Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984–4997, Online. Association for Computational Linguistics.

Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. Adaptive machine translation with large language models. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 227–237, Tampere, Finland. European Association for Machine Translation.

OpenAI. 2023. Gpt-4 technical report.

Jason W Osborne and Amy Overbay. 2004. The power of outliers (and why researchers should always check for them). Practical Assessment, Research, and Evaluation, 9(1):6.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.

Amy Pu, Hyung Won Chung, Ankur Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021. Learning compact metrics for MT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 751–762, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021. Are references really needed? unbabel-IST 2021 submission for the metrics shared task. In Proceedings of the Sixth Conference on Machine Translation, pages 1030–1040, Online. Association for Computational Linguistics.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Association for Computational Linguistics.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models.

Yu Wan, Keqin Bao, Dayiheng Liu, Baosong Yang, Derek F. Wong, Lidia S. Chao, Wenqiang Lei, and Jun Xie. 2022a. Alibaba-translate China's submission for WMT2022 metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 586–592, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. 2022b. UniTE: Unified translation evaluation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8117–8127, Dublin, Ireland. Association for Computational Linguistics.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837.

A Ba ví dụ được sử dụng cho Few-shot Prompting

Nguồn tiếng Anh: I do apologise about this, we must gain permission from the account holder to discuss an order with another person, I apologise if this was done previously, however, I would not be able to discuss this with yourself without the account holders permission.

Bản dịch tiếng Đức: Ich entschuldige mich dafür, wir müssen die Erlaubnis einholen, um eine Bestellung mit einer anderen Person zu besprechen. Ich entschuldige mich, falls dies zuvor geschehen wäre, aber ohne die Erlaubnis des Kontoinhabers wäre ich nicht in der Lage, dies mit dir involvement.

Chú thích MQM:
Nghiêm trọng:
no-error
Lớn:
accuracy/mistranslation - "involvement"
accuracy/omission - "the account holder"
Nhỏ:
fluency/grammar - "wäre"
fluency/register - "dir"

Nguồn tiếng Anh: Talks have resumed in Vienna to try to revive the nuclear pact, with both sides trying to gauge the prospects of success after the latest exchanges in the stop-start negotiations.

Bản dịch tiếng Séc: Ve Vídni se ve Vídni obnovily rozhovory o oživení jaderného paktu, přičemž obě partaje se snaží posoudit vyhlídky na úspěch po posledních výměnách v jednáních.

Chú thích MQM:
Nghiêm trọng:
no-error
Lớn:
accuracy/addition - "ve Vídni"
accuracy/omission - "the stop-start"
Nhỏ:
terminology/inappropriate for context - "partaje"

Nguồn tiếng Trung: 大众点评乌鲁木齐家居商场频道为您提供高铁居然之家地址，电话，营业时间等最新商户信息，找装修公司，就上大众点评

Bản dịch tiếng Anh: Urumqi Home Furnishing Store Channel provides you with the latest business information such as the address, telephone number, business hours, etc., of high-speed rail, and find a decoration company, and go to the reviews.

Chú thích MQM:
Nghiêm trọng:
accuracy/addition - "of high-speed rail"
Lớn:
accuracy/mistranslation - "go to the reviews"
Nhỏ:
style/awkward - "etc.,"

Hình 2: Ba ví dụ được sử dụng cho tất cả ngôn ngữ.
