# 2309.11674.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2309.11674.pdf
# Kích thước file: 610799 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
MỘT THAY ĐỔI MÔ HÌNH TRONG DỊCH MÁY:
TĂNG CƯỜNG HIỆU SUẤT DỊCH THUẬT CỦA CÁC
MÔ HÌNH NGÔN NGỮ LỚN
Haoran Xu♠, Young Jin Kim♡, Amr Sharaf♡, Hany Hassan Awadalla♡
♠Johns Hopkins University,♡Microsoft
hxu64@jhu.edu
{youki,amrsharaf,hanyh }@microsoft.com
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn Tạo sinh (LLMs) đã đạt được những tiến bộ đáng kể trong nhiều nhiệm vụ NLP khác nhau. Tuy nhiên, những tiến bộ này chưa được phản ánh trong nhiệm vụ dịch thuật, đặc biệt là những mô hình có kích thước vừa phải (tức là 7B hoặc 13B tham số), vẫn còn tụt hậu so với các mô hình dịch thuật encoder-decoder được giám sát thông thường. Các nghiên cứu trước đây đã cố gắng cải thiện khả năng dịch thuật của những LLMs này, nhưng những cải tiến của chúng rất hạn chế. Trong nghiên cứu này, chúng tôi đề xuất một phương pháp tinh chỉnh mới cho LLMs được thiết kế đặc biệt cho nhiệm vụ dịch thuật, loại bỏ nhu cầu về dữ liệu song song phong phú mà các mô hình dịch thuật truyền thống thường phụ thuộc vào. Phương pháp của chúng tôi bao gồm hai giai đoạn tinh chỉnh: tinh chỉnh ban đầu trên dữ liệu đơn ngôn ngữ, sau đó tinh chỉnh trên một tập nhỏ dữ liệu song song chất lượng cao. Chúng tôi giới thiệu LLM được phát triển thông qua chiến lược này là Advanced Language Model-based tr Anslator (ALMA). Dựa trên LLaMA-2 (Touvron et al., 2023b) làm mô hình nền tảng, kết quả của chúng tôi cho thấy mô hình có thể đạt được cải thiện trung bình hơn 12 BLEU và 12 COMET so với hiệu suất zero-shot trên 10 hướng dịch từ các bộ dữ liệu thử nghiệm WMT'21 (2 hướng) và WMT'22 (8 hướng). Hiệu suất này tốt hơn đáng kể so với tất cả các nghiên cứu trước đây và thậm chí vượt trội hơn so với mô hình NLLB-54B (NLLB TEAM et al., 2022) và GPT-3.5-text-davinci-003, chỉ với 7B hoặc 13B tham số. Phương pháp này thiết lập nền tảng cho một mô hình huấn luyện mới trong dịch máy.¹

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn tạo sinh (decoder-only) như các mô hình GPT (Brown et al., 2020; OpenAI, 2023), PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), BLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023a;b), và những mô hình khác đã thể hiện khả năng đáng kể trong nhiều nhiệm vụ NLP khác nhau. Tuy nhiên, đối với nhiệm vụ dịch thuật, chỉ có những mô hình rất lớn như GPT-3.5 và GPT-4 mới có thể cạnh tranh với các mô hình encoder-decoder được giám sát tối tân (SoTA) như NLLB (NLLB TEAM et al., 2022), trong khi chúng vẫn còn kém trong việc dịch thuật cho các ngôn ngữ ít tài nguyên (Hendy et al., 2023; Jiao et al., 2023). Sự khác biệt trở nên rõ ràng hơn khi so sánh các LLMs khác với các mô hình dịch thuật truyền thống (Zhu et al., 2023a). Ví dụ, mô hình OPT-175B tụt hậu so với mô hình NLLB-1.3B trung bình hơn 15 điểm BLEU (Papineni et al., 2002) cho các ngôn ngữ trong họ Indo-European-Romance. Khoảng cách thậm chí còn lớn hơn trong các LLMs nhỏ hơn; ví dụ, XGLM (Lin et al., 2021), với kích thước tham số 7B, tụt hậu so với NLLB-1.3B tới 30 điểm BLEU (Zhu et al., 2023a). Do đó, có nhu cầu cấp thiết để thu hẹp khoảng cách hiệu suất này giữa LLMs và các mô hình SoTA thông thường.

Như được minh họa bởi NLLB-1.3B, các mô hình dịch máy truyền thống thể hiện khả năng thành thạo trong việc tạo ra bản dịch chất lượng cao với số lượng tham số nhỏ. Theo đó, các LLMs nhỏ hơn cũng nên có khả năng xử lý nhiệm vụ dịch thuật một cách thành thạo tương tự. Nghiên cứu gần đây đã tìm cách nâng cao hiệu suất dịch thuật bằng cách bắt đầu với các LLMs nhỏ hơn (Yang et al., 2023; Zeng et al., 2023; Chen et al., 2023; Zhu et al., 2023b; Li et al., 2023; Zhang et al., 2023b), đặc biệt là 7B hoặc 13B tham số. Tuy nhiên, những cải thiện đạt được vẫn khiêm tốn và hạn chế. Như được mô tả trong Hình 1, các nghiên cứu đương đại như Balyling (Zhang et al., 2023b) và BigTranslate (Yang et al., 2023), sử dụng LLaMA làm backbone, thể hiện mức tăng tối đa 3 đến 4 BLEU hoặc COMET so với hiệu suất zero-shot của LLaMA trên tập thử nghiệm WMT'22 (8 hướng).² Trong khi những cải tiến này đại diện cho hướng nghiên cứu đầy hứa hẹn cho các LLMs nhỏ hơn trong nhiệm vụ dịch thuật, một khoảng cách hiệu suất đáng kể vẫn tồn tại khi so sánh với các LLMs rất lớn như GPT-3.5-text-davinci-003 và các mô hình dịch thuật SoTA như NLLB-54B.

Chúng tôi cho rằng những cải tiến dịch thuật khiêm tốn được quan sát trong các nghiên cứu trước có thể được quy cho một công thức huấn luyện không phù hợp.

¹Chúng tôi phát hành mã nguồn và mô hình tại: https://github.com/fe1ixxu/ALMA.

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

nên tương tự có khả năng quản lý nhiệm vụ dịch thuật một cách thành thạo. Nghiên cứu gần đây đã tìm cách nâng cao hiệu suất dịch thuật bằng cách bắt đầu với các LLMs nhỏ hơn (Yang et al., 2023; Zeng et al., 2023; Chen et al., 2023; Zhu et al., 2023b; Li et al., 2023; Zhang et al., 2023b), đặc biệt là 7B hoặc 13B tham số. Tuy nhiên, những cải thiện đạt được vẫn khiêm tốn và hạn chế. Như được mô tả trong Hình 1, các nghiên cứu đương đại như Balyling (Zhang et al., 2023b) và BigTranslate (Yang et al., 2023), sử dụng LLaMA làm backbone, thể hiện mức tăng tối đa 3 đến 4 BLEU hoặc COMET so với hiệu suất zero-shot của LLaMA trên tập thử nghiệm WMT'22 (8 hướng).² Trong khi những cải tiến này đại diện cho hướng nghiên cứu đầy hứa hẹn cho các LLMs nhỏ hơn trong nhiệm vụ dịch thuật, một khoảng cách hiệu suất đáng kể vẫn tồn tại khi so sánh với các LLMs rất lớn như GPT-3.5-text-davinci-003 và các mô hình dịch thuật SoTA như NLLB-54B.

Chúng tôi cho rằng những cải tiến dịch thuật khiêm tốn được quan sát trong các nghiên cứu trước có thể được quy cho một công thức huấn luyện không phù hợp.

7B 13B 54B 175B
Kích thước Mô hình (B)22242628303234BLEU
LLaMA-2-7B
LLaMA-2-13BLLaMA-1-7BLLaMA-1-13BALMA-7B (Của chúng tôi)ALMA-13B (Của chúng tôi)
NLLB-54BGPT-3.5
text-davinci-003
BigTranslateBayling-13BBayling-7B
(a) BLEU
7B 13B 54B 175B
Kích thước Mô hình (B)7880828486COMET
LLaMA-2-7B
LLaMA-2-13BLLaMA-1-7BLLaMA-1-13BALMA-7B (Của chúng tôi)ALMA-13B (Của chúng tôi)
NLLB-54BGPT-3.5
text-davinci-003
BigTranslateBayling-13BBayling-7B (b) COMET

Hình 1: Hiệu suất dịch thuật của các hệ thống dịch thuật LLM decoder-only đương đại dựa trên LLaMA (Yang et al., 2023; Zhang et al., 2023b), và hiệu suất zero-shot của LLaMA, cho dữ liệu thử nghiệm WMT'22 trên 8 hướng (dịch từ hoặc sang tiếng Anh cho tiếng Đức, Czech, Trung Quốc, và Nga). So sánh chuẩn cũng bao gồm hai mô hình dịch thuật hàng đầu, NLLB-54B và GPT-3.5-text-davinci-003. Các hệ thống của chúng tôi, được phát triển trên LLaMA-2 với 7B và 13B tham số, vượt trội hơn các mô hình trước đây với khoảng cách ấn tượng gần 10 BLEU và 7 COMET. Hơn nữa, chúng thậm chí còn vượt trội một chút so với GPT-3.5 và NLLB-54B trung bình.

Chúng tôi giả thuyết rằng một công thức huấn luyện hiệu quả nên tuân theo hai giai đoạn: học kiến thức ngôn ngữ đa ngôn ngữ tổng quát và dẫn dắt (chỉ dẫn) các mô hình hướng tới việc tạo ra bản dịch. Do đó, chúng tôi đề xuất phương pháp tinh chỉnh hai giai đoạn và giới thiệu LLM được phát triển thông qua chiến lược này là Advanced Language Model-based tr Anslator (ALMA). Cụ thể, vì hầu hết các LLMs được huấn luyện trên dữ liệu chủ yếu là tiếng Anh, giai đoạn đầu tiên là tinh chỉnh dữ liệu đơn ngôn ngữ không phải tiếng Anh để nâng cao trình độ của mô hình trong các ngôn ngữ khác liên quan đến nhiệm vụ dịch thuật. Thứ hai, lấy cảm hứng từ tầm quan trọng được công nhận của chất lượng dữ liệu trong các ứng dụng khác (Zhou et al., 2023; Maillard et al., 2023; Gunasekar et al., 2023), chúng tôi tinh chỉnh mô hình với một lượng nhỏ dữ liệu song song chất lượng cao.

Những đóng góp chính của chúng tôi được tóm tắt như sau:

Giảm Nhu cầu về Dữ liệu Song song Các khung dịch thuật truyền thống phụ thuộc vào lượng lớn dữ liệu song song, điều này có thể dẫn đến ấn tượng sai lầm rằng dữ liệu như vậy là cần thiết cho nhiệm vụ dịch thuật với LLMs. Các nghiên cứu trước đây đã tinh chỉnh LLMs với các bộ dữ liệu chứa hơn 300M trường hợp song song (Yang et al., 2023). Tuy nhiên, các đánh giá thực nghiệm của chúng tôi cho thấy chiến lược này có thể không tối ưu, và thậm chí có thể gây hại cho khả năng dịch thuật của LLMs.

LLM Thông qua Công thức Huấn luyện Mới: ALMA Chúng tôi giới thiệu phương pháp tinh chỉnh hai giai đoạn mới cho việc dịch thuật với LLMs decoder-only. Tận dụng LLaMA-2 làm mô hình cơ sở, chúng tôi đạt được cải thiện trung bình hơn 12 điểm BLEU và COMET so với hiệu suất zero-shot của nó

²Tất cả điểm COMET trong bài báo là COMET-22 (Unbabel/wmt22-comet-da) (Rei et al., 2022).

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

trên 10 hướng dịch từ các bộ dữ liệu thử nghiệm WMT'21 và WMT'22. Đáng chú ý, hiệu suất vượt trội so với tất cả các nghiên cứu trước đây và thậm chí tốt hơn so với mô hình NLLB-54B và GPT-3.5-text-davinci-003.

Chi phí Tính toán Hiệu quả Nghiên cứu khử các yếu tố của chúng tôi cho thấy cả hai giai đoạn đều là yếu tố quan trọng để đạt được những cải thiện lớn. Phần tốn kém tính toán nhất là tinh chỉnh dữ liệu đơn ngôn ngữ, tuy nhiên, chúng tôi chỉ ra rằng chỉ cần tinh chỉnh 1B token đơn ngôn ngữ là đủ để có hiệu suất tương đương với NLLB-54B trong 10 hướng dịch, chỉ cần khoảng 18 giờ để hoàn thành với 16 GPU MI200.

2 KHÁI NIỆM CƠ BẢN

2.1 ĐỊNH NGHĨA NHIỆM VỤ

Chúng tôi xem xét một mô hình transformer decoder-only được tham số hóa bởi θ cho việc dịch máy. Gọi x đại diện cho câu nguồn và y là câu đích tương ứng. Chúng tôi sử dụng một mẫu prompt cố định, được ký hiệu là I, để hướng dẫn mô hình tạo ra bản dịch. Hàm mất mát log-likelihood của câu song song (x,y) đối với các tham số mô hình θ có thể được công thức hóa như sau:

LNLL(x,y, θ) = −logP(y|x,I;θ) (1)
= −∑T t=1 logP(yt|y<t,x,I;θ), (2)

trong đó T là độ dài của câu đích, và yt là token thứ t của đích. Hàm mất mát này là hàm mất mát mô hình ngôn ngữ nhân quả (CLM) tiêu chuẩn, dự đoán token tiếp theo dựa trên các token trước đó. Chúng tôi sử dụng cùng một mẫu prompt dịch thuật cấp câu được đề xuất bởi Hendy et al. (2023), và minh họa prompt cũng như đầu vào/đích của mô hình trong Hình 2. Lưu ý rằng chúng tôi không tính toán hàm mất mát cho mẫu prompt và câu nguồn trong quá trình huấn luyện (Zhang et al., 2023a). Trong Phụ lục A, chúng tôi chỉ ra rằng CLM phù hợp hơn cho nhiệm vụ dịch thuật so với các phương pháp mô hình hóa khác, như mô hình ngôn ngữ tiền tố (Wang et al., 2022) và hỗn hợp khử nhiễu (Tay et al., 2022a).

Dịch 
cái này 
từ 
[ngôn ngữ nguồn] 
sang 
[ngôn ngữ đích]: 
[ngôn ngữ nguồn]: 
<câu nguồn>
[ngôn ngữ đích]:
<câu đích>
Prompt
+
Đầu vào
 
/
 
Đích
Prompt
Không tính 
mất mát
Mất mát
CLM

Hình 2: Prompt được sử dụng cho huấn luyện và đánh giá. [ngôn ngữ nguồn] và [ngôn ngữ đích] đại diện cho tên đầy đủ của ngôn ngữ, ví dụ, Dịch cái này từ tiếng Đức sang tiếng Anh. Lưu ý rằng chúng tôi không tính mất mát cho prompt.

2.2 MỘT LLM NỀN TẢNG CHO DỊCH THUẬT

Chúng tôi tìm kiếm một LLM mạnh mẽ để phục vụ như mô hình nền tảng. Với sự xuất hiện gần đây của nhiều LLMs, chúng tôi ưu tiên đánh giá hiệu suất dịch thuật zero-shot của những mô hình này trước khi đi sâu vào các công thức huấn luyện tối ưu. Vì hầu hết các mô hình này cung cấp phiên bản 7B, phân tích so sánh của chúng tôi tập trung vào quy mô này: OPT-7B (Zhang et al., 2022), Falcon-7B (Almazrouei et al., 2023), BLOOM-7B (Scao et al., 2022), MPT-7B (MosaicML, 2023), LLaMA-1-7B (Touvron et al., 2023a), và LLaMA-2-7B (Touvron et al., 2023b). Chúng tôi cũng trình bày kết quả từ GPT-3.5-text-davinci-003 (sau đây được gọi là GPT-3.5-D) và GPT-3.5-turbo-0301 (sau đây được gọi là GPT-3.5-T) để hiển thị khoảng cách hiệu suất.³

Đánh giá Zero-Shot Chúng tôi tiến hành đánh giá zero-shot trên 5 cặp ngôn ngữ lấy tiếng Anh làm trung tâm, xem xét cả hai hướng từ tiếng Anh và đến tiếng Anh: tiếng Đức (de), Czech (cs), Iceland (is), Trung Quốc (zh) và Nga (ru), trong đó dữ liệu thử nghiệm Iceland là từ WMT'21 và các ngôn ngữ khác từ WMT'22. Chúng tôi chọn các bộ dữ liệu thử nghiệm này vì chúng gần đây và ít khả năng chồng chéo với dữ liệu huấn luyện được sử dụng bởi LLMs, và quan trọng, chúng có dữ liệu chất lượng cao để tránh các vấn đề về "translationese" (Zhang & Toral, 2019). Kích thước beam là 5. Chúng tôi báo cáo sacre-BLEU (zh tokenizer cho tiếng Trung và 13a cho những ngôn ngữ khác) (Post, 2018). Chúng tôi cũng báo cáo COMET (Unbabel/wmt22-comet-da) (Rei et al., 2022) vì BLEU chỉ phản ánh mức độ khớp từ vựng. Trong bài báo này, chúng tôi phụ thuộc vào COMET hơn BLEU do sự căn chỉnh tốt hơn với đánh giá của con người (Freitag et al., 2022).⁴

³https://beta.openai.com/docs/model-index-for-researchers

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

OPT-7B
BLOOM-7BFalcon-7BLLaMA-1-7BMPT-7B
LLaMA-2-7BGPT-3.5-D GPT-3.5-T
Mô hình05101520253035BLEU
10.0412.8018.5023.0123.9026.4334.1637.36
3.417.6411.6614.5915.90
13.8628.9632.17xxen
enxx
(a) BLEU
OPT-7B
BLOOM-7BFalcon-7BLLaMA-1-7BMPT-7B
LLaMA-2-7BGPT-3.5-D GPT-3.5-T
Mô hình5060708090COMET
61.1064.3771.1175.01 75.4377.1683.9085.46
50.6759.8060.9467.4768.54 68.8884.5986.56xxen
enxx
 (b) COMET

Hình 3: Hiệu suất dịch thuật zero-shot trung bình trên 10 hướng: cs↔en, de↔en, is↔en, zh↔en, ru↔en, trong đó is↔en từ dữ liệu thử nghiệm WMT'21 và những hướng khác từ dữ liệu thử nghiệm WMT'22.

Hiệu suất Dịch thuật LLM Kết quả tổng thể cho các LLMs được trình bày trong Hình 3, với điểm số trung bình trên năm ngôn ngữ cho việc dịch đến và từ tiếng Anh. Trong số các LLMs 7B, LLaMA-2-7B thể hiện hiệu suất vượt trội khi dịch sang tiếng Anh, trong khi MPT-7B dẫn đầu trong việc dịch từ tiếng Anh, được đo bằng BLEU. Tuy nhiên, khi được đánh giá bằng COMET, LLaMA-2-7B thắng ở cả hai hướng. Chúng tôi hiển thị kết quả số trong Phụ lục B. Do đó, chúng tôi chọn LLaMA-2-7B và MPT-7B để tiến hành nghiên cứu sâu hơn về nhu cầu dữ liệu song song cho LLMs.

3 LLMS CÓ THÈM MUỐN DỮ LIỆU SONG SONG KHÔNG?

Huấn luyện dịch máy thông thường chủ yếu phụ thuộc vào việc sử dụng khối lượng lớn các bộ dữ liệu song song trong khung encoder-decoder. Xu hướng này không chỉ giới hạn trong việc huấn luyện mô hình từ đầu mà còn liên quan đến các chiến lược tinh chỉnh LLMs đã được huấn luyện trước, thường liên quan đến hàng triệu câu song song (Rothe et al., 2020; Liu et al., 2020; Xu et al., 2021; 2023; Yang et al., 2023). Trong phần này, chúng tôi kiểm tra liệu các LLMs decoder-only được đề xuất gần đây có duy trì sự phụ thuộc vào dữ liệu song song đáng kể và tuân theo mô hình huấn luyện truyền thống hay không.

3.1 THIẾT KẾ THỰC NGHIỆM

Theo Phần 2.2, chúng tôi thu hẹp tập trung vào việc tinh chỉnh LLaMA-2-7B và MPT-7B. Để cho phép phân tích sâu, chúng tôi tập trung vào một cặp ngôn ngữ, tiếng Anh → tiếng Nga (en→ru). Chúng tôi chọn một cặp ngôn ngữ đang dịch từ tiếng Anh và sang một ngôn ngữ không thuộc hệ La-tinh, vì những danh mục này cho thấy khoảng cách lớn hơn với các mô hình SoTA trong cuộc điều tra ban đầu của chúng tôi ở Phần 2.2. Chúng tôi sử dụng dữ liệu sạch được lọc từ 75M câu song song từ Hendy et al. (2023) và chia kích thước dữ liệu thành 5 cấp độ: 10K, 100K, 1M, 5M, và 20M. Chúng tôi sử dụng cùng mẫu prompt và sơ đồ huấn luyện như được mô tả trong Phần 2.1, và huấn luyện mô hình bằng cách cập nhật tất cả các tham số. Cài đặt huấn luyện chi tiết có thể được tìm thấy trong Phụ lục C.

3.2 QUAN SÁT

Kết quả tinh chỉnh cho LLaMA-2-7B và MPT-7B tại từng bước kích thước dữ liệu được trình bày trong Hình 4. Ngoài ra, chúng tôi so sánh những kết quả này với hiệu suất của mô hình NLLB-54B để hiển thị sự chênh lệch với một trong những mô hình dịch thuật đa ngôn ngữ SoTA.

⁴Theo Freitag et al. (2022), COMET đứng vị trí thứ 2 trong việc căn chỉnh với xếp hạng của con người, trong khi BLEU nằm ở vị trí thứ 19 trong số 20 chỉ số

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

0 (zero-shot) 10K 100K 1M 5M 20M
Kích thước Dữ liệu Huấn luyện15.017.520.022.525.027.530.0BLEU
MPT-7B
LLaMA-2-7B
NLLB-54B
Khởi tạo Ngẫu nhiên
(a) BLEU
0 (zero-shot) 10K 100K 1M 5M 20M
Kích thước Dữ liệu Huấn luyện72.575.077.580.082.585.087.5COMET
MPT-7B
LLaMA-2-7B
NLLB-54B
Khởi tạo Ngẫu nhiên (b) COMET

Hình 4: Điểm BLEU và COMET thu được trong quá trình tinh chỉnh MPT-7B và LLaMA-2-7B qua từng bước dữ liệu cho en→ru. Ngoài ra, chúng tôi trình bày kết quả cho NLLB-54B và một mô hình 7B được huấn luyện từ đầu. Sự suy giảm đáng chú ý trong điểm COMET của LLaMA-2-7B cho thấy rằng dữ liệu song song đáng kể có thể làm loãng kiến thức có sẵn của nó.

Dữ liệu Huấn luyện Nhỏ Là Đủ Theo COMET, có sự khác biệt đáng chú ý trong đường cong của LLaMA-2-7B và MPT-7B: LLaMA-2-7B đạt đỉnh với dữ liệu huấn luyện 10K và 100K trước khi trải qua sự suy giảm, trong khi MPT-7B thể hiện sự cải thiện liên tục với nhiều dữ liệu huấn luyện hơn. LLaMA-2-7B chỉ cần số lượng ví dụ huấn luyện hạn chế (10K và 100K) để đạt được khả năng dịch thuật thành thạo. Tuy nhiên, một lượng dư thừa ví dụ (5M hoặc 20M) dường như làm loãng kiến thức hiện có của nó trong tiếng Nga. Ngược lại, MPT-7B, có thể do khả năng dịch thuật vốn yếu hơn, thể hiện hiệu suất được cải thiện với sự gia tăng dữ liệu huấn luyện. Điều này có thể gợi ý rằng LLaMA-2 hoặc các LLMs được huấn luyện tốt khác có thể không cần thiết phải có dữ liệu song song đáng kể.

Dữ liệu Song song Lớn Xóa Sạch Kiến thức Cả hai LLMs cuối cùng đạt được BLEU và COMET tương tự với dữ liệu huấn luyện 20M, bất kể hiệu suất của chúng trên dữ liệu nhỏ hơn. Chúng tôi giả thuyết rằng hiện tượng này được gây ra bởi việc quên thảm khốc (French, 1999; Kirkpatrick et al., 2017), gợi ý rằng quá nhiều dữ liệu song song xóa sạch kiến thức có sẵn trước đó. Để xác thực giả thuyết này, chúng tôi xem xét một trường hợp cực đoan: huấn luyện mô hình từ đầu sử dụng dữ liệu 20M, do đó xóa sạch tất cả kiến thức trước đó.⁵ Như mong đợi, nó có xu hướng có hiệu suất tương tự trong cả đánh giá BLEU và COMET (hình tam giác trong Hình 4), củng cố suy đoán của chúng tôi về việc loãng kiến thức nội tại của LLM với việc huấn luyện dữ liệu mở rộng.

Vượt qua BLEU COMET tiết lộ sự suy giảm hiệu suất dịch thuật cho LLaMA-2-7B khi lượng dữ liệu song song tăng lên, một xu hướng không được BLEU nắm bắt, điều này cho thấy sự gia tăng. Sự khác biệt này phát sinh vì BLEU chủ yếu đánh giá sự chồng chéo từ vựng, và dữ liệu huấn luyện WMT rộng rãi, tương tự về lĩnh vực với tập thử nghiệm, có thể nâng cao thước đo này. Điều này nhấn mạnh sự cần thiết của việc sử dụng các chỉ số bổ sung (như COMET) để đánh giá toàn diện về dịch thuật.

Từ các quan sát của chúng tôi, LLaMA-2 (có thể các LLMs được huấn luyện tốt khác) không nên áp dụng cùng phương pháp huấn luyện như các mô hình trước đó—dù được khởi tạo ngẫu nhiên hay được huấn luyện trước—phụ thuộc nhiều vào lượng lớn dữ liệu huấn luyện.

4 MỘT CÔNG THỨC HUẤN LUYỆN MỚI

Chúng tôi chứng minh rằng các LLMs như LLaMA-2-7B không tham lam tiêu thụ dữ liệu song song. Chúng tôi giới thiệu một chiến lược huấn luyện mới giúp nâng cao đáng kể hiệu suất dịch thuật mà không phụ thuộc nhiều vào dữ liệu song song. Công thức này bao gồm hai giai đoạn: tinh chỉnh dữ liệu đơn ngôn ngữ liên tục và tinh chỉnh dữ liệu song song chất lượng cao. Sau khi áp dụng công thức huấn luyện của chúng tôi cho LLMs, chúng tôi đặt tên mô hình kết quả là ALMA (Advanced Language Model-based tr Anslator).

Tinh chỉnh Dữ liệu Đơn ngôn ngữ Các LLMs như LLaMA được huấn luyện trước trên kho dữ liệu do tiếng Anh chiếm ưu thế. Điều này có thể giải thích hiệu suất dịch thuật không đầy đủ của chúng vì cần khả năng xuyên ngôn ngữ. Để khắc phục điều này, giai đoạn đầu tiên của chúng tôi là tinh chỉnh LLMs với dữ liệu đơn ngôn ngữ của các ngôn ngữ không phải tiếng Anh liên quan đến các nhiệm vụ dịch thuật, nâng cao trình độ của chúng trong những ngôn ngữ này. Lưu ý rằng chúng tôi cũng thêm dữ liệu đơn ngôn ngữ tiếng Anh trong quá trình tinh chỉnh để ngăn ngừa việc quên kiến thức tiếng Anh. Các nghiên cứu trước đây cũng cung cấp một số manh mối rằng dữ liệu đơn ngôn ngữ giúp ích trong dịch thuật. Ví dụ, Tan et al. (2023) sử dụng kho dữ liệu đơn ngôn ngữ đích để thu hẹp khoảng cách trong các sự không khớp dịch thuật gây ra bởi sự khác biệt về lĩnh vực. BigTranslate (Yang et al., 2023) và PolyLM (Wei et al., 2023) sử dụng một lượng lớn dữ liệu đơn ngôn ngữ tiếng Trung và cải thiện việc dịch từ hoặc sang tiếng Trung. Hơn nữa, Li et al. (2023) sử dụng các chỉ dẫn tạo sinh đơn ngôn ngữ để cải thiện dịch thuật. Trong Phần 6.1, chúng tôi chỉ ra rằng việc sử dụng dữ liệu đơn ngôn ngữ nhỏ và chi phí tính toán khiêm tốn (ví dụ, 1B token đơn ngôn ngữ được trộn bởi 6 ngôn ngữ và tinh chỉnh dưới 18 giờ), có thể tạo điều kiện cho những cải thiện đáng kể trong 10 hướng dịch. Lưu ý rằng chúng tôi sử dụng tinh chỉnh toàn bộ trọng số ở giai đoạn này.

Tinh chỉnh Dữ liệu Chất lượng Cao Dựa trên những hiểu biết từ Phần 3.2 rằng LLMs có thể chỉ cần dữ liệu song song nhỏ, kết hợp với nghiên cứu trước đây nhấn mạnh chất lượng dữ liệu huấn luyện (Zhou et al., 2023; Maillard et al., 2023; Gunasekar et al., 2023), chúng tôi tinh chỉnh mô hình sử dụng một bộ dữ liệu song song nhỏ nhưng chất lượng cao ở giai đoạn này. Để đảm bảo chất lượng dữ liệu, chúng tôi thu thập các bộ dữ liệu do con người viết từ dữ liệu thử nghiệm WMT và các bộ phát triển và thử nghiệm Flores-200 (NLLB TEAM et al., 2022). Ở đây, chúng tôi khám phá cả việc tinh chỉnh toàn bộ trọng số và Thích ứng Hạng thấp nhẹ (LoRA) (Hu et al., 2022; Mangrulkar et al., 2022), trong đó chúng tôi áp dụng LoRA cho lớp chiếu xuống trong mỗi mạng truyền thẳng.

5 THỰC NGHIỆM

5.1 DỮ LIỆU

Đối với dữ liệu huấn luyện song song của chúng tôi, chúng tôi thu thập các bộ dữ liệu thử nghiệm do con người viết từ WMT'17 đến WMT'20, cộng với các bộ phát triển và thử nghiệm từ Flores-200 (NLLB TEAM et al., 2022), tổng cộng 58K ví dụ huấn luyện trên tất cả các ngôn ngữ. Đối với dữ liệu thử nghiệm, chúng tôi vẫn sử dụng cùng 10 hướng dịch để nhất quán với nghiên cứu của chúng tôi ở Phần 2: cs↔en, de↔en, is↔en, zh↔en, ru↔en, trong đó is↔en từ WMT'21 và những hướng khác từ WMT'22. Dữ liệu thử nghiệm trong WMT'21 (ngoại trừ is) được sử dụng cho bộ dữ liệu phát triển (tổng cộng 8K câu song song).⁶ Bộ dữ liệu đơn ngôn ngữ có nguồn gốc từ OSCAR (Ortiz Su'arez et al., 2019; Kreutzer et al., 2022). Chúng tôi trộn dữ liệu đơn ngôn ngữ và tinh chỉnh mô hình với tỷ lệ lấy mẫu 20%, 14%, 8%, 19%, 22%, và 17% tương ứng cho de, cs, is, zh, ru và en. Chúng tôi giải thích lý do đằng sau các tỷ lệ lấy mẫu và hiển thị thông tin dữ liệu song song chi tiết trong Phụ lục D.

5.2 THIẾT LẬP HUẤN LUYỆN

Chúng tôi huấn luyện mô hình theo cách dịch thuật đa ngôn ngữ nhiều-nhiều, và sử dụng LLaMA-2-7B (hoặc 13B) làm mô hình backbone dựa trên hiệu suất zero-shot tốt nhất của nó. Quy trình tinh chỉnh hai giai đoạn của chúng tôi tạo ra hai loại mô hình, được phân biệt dựa trên việc sử dụng LoRA:

ALMA-7B/ALMA-13B Tinh chỉnh Toàn bộ trọng số trên dữ liệu đơn ngôn ngữ, sau đó tinh chỉnh Toàn bộ trọng số trên dữ liệu song song chất lượng cao cho các mô hình LLaMA-2-7B hoặc -13B.

ALMA-7B-LoRA/ALMA-13B-LoRA Tinh chỉnh Toàn bộ trọng số trên dữ liệu đơn ngôn ngữ, sau đó tinh chỉnh LoRA trên dữ liệu song song chất lượng cao cho các mô hình LLaMA-2-7B hoặc -13B.

Nếu sử dụng LoRA, hạng LoRA là 16 và chỉ cập nhật 0.1% tham số (7.7M cho 7B và 12M cho mô hình 13B). Cả tinh chỉnh dữ liệu đơn ngôn ngữ và tinh chỉnh dữ liệu do con người viết đều chia sẻ cùng cài đặt siêu tham số. Cụ thể, chúng tôi tinh chỉnh LLaMA-2 với kích thước batch 256, tỷ lệ warm-up 0.01, và một chuỗi chứa tối đa 512 token. Đối với tinh chỉnh dữ liệu đơn ngôn ngữ, chúng tôi huấn luyện LLaMA-2-7B lên đến 20B token và LLaMA-2-13B lên đến 12B token. Tuy nhiên, rất có thể mô hình sẽ tốt hơn trong dịch thuật với việc tinh chỉnh dữ liệu đơn ngôn ngữ nhiều hơn. Đối với tinh chỉnh dữ liệu do con người viết, chúng tôi huấn luyện mô hình trong 2 epoch (đủ để thấy sự hội tụ rõ ràng) và chọn mô hình tốt nhất với mất mát xác thực thấp nhất. Đối với cả hai giai đoạn, chúng tôi áp dụng deepspeed (Rasley et al., 2020) để tăng tốc quá trình huấn luyện.

⁶Không có bộ dữ liệu phát triển cho Iceland.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

5.3 BASELINE

Chúng tôi đánh giá phương pháp của chúng tôi so với hai danh mục baseline. Đầu tiên, chúng tôi xem xét các nghiên cứu trước đây với mục tiêu tương tự với chúng tôi: tận dụng LLMs cho dịch thuật. Thứ hai, chúng tôi đánh giá chuẩn so với các mô hình dịch thuật SoTA hiện tại. Đáng chú ý rằng so sánh này không hoàn toàn công bằng do sự khác biệt trong dữ liệu huấn luyện và kiến trúc mô hình (ví dụ, GPT-3.5 175B so với mô hình 7B của chúng tôi). Tuy nhiên, việc sử dụng cùng tập thử nghiệm cung cấp hiểu biết về vị trí hiện tại của mô hình chúng tôi.

Công việc Tương tự Trước đây Chúng tôi so sánh mô hình của chúng tôi với BigTranslate (Yang et al., 2023), mở rộng LLaMA-1-13B đến hơn 100 hướng dịch; TIM (Zeng et al., 2023), sử dụng các ví dụ đúng và sai để giúp LLM học dịch thuật; SWIE (Chen et al., 2023), cải thiện LLM trong dịch thuật qua tăng cường chỉ dẫn; và BayLing (Zhang et al., 2023b), sử dụng các chỉ dẫn dịch thuật tương tác. Vì cùng dữ liệu thử nghiệm và chỉ số đánh giá được sử dụng, chúng tôi trực tiếp báo cáo BLEU và COMET từ bài báo của họ (ngoại trừ BigTranslate, chúng tôi đánh giá mô hình được phát hành của họ bằng prompt mà họ cung cấp).

Mô hình SoTA Chúng tôi xem xét mô hình NLLB-54B, là mô hình dịch thuật lớn nhất và tốt nhất được phát hành trong họ NLLB (NLLB TEAM et al., 2022); và hiệu suất zero-shot của GPT-3.5-text-davinci-003 (GPT-3.5-D) và GPT-3.5-turbo-0301 (GPT-3.5-T). Ngoài ra, chúng tôi trình bày kết quả zero-shot cho GPT-4.⁷

[Bảng 1 với kết quả chi tiết cho en→xx được dịch với các điểm số BLEU và COMET cho các mô hình khác nhau]

5.4 KẾT QUẢ

Chúng tôi hiển thị kết quả chính của en→xx và xx→en tương ứng trong Bảng 1 và 2. Tóm lại, hệ thống tốt nhất của chúng tôi (ALMA-13B-LoRA) vượt trội so với tất cả các nghiên cứu trước đây, NLLB-54B, và GPT-3.5-D, trong khi nó kém hơn một chút so với GPT-3.5-T và GPT-4.

So sánh với LLaMA-2 Zero-Shot Đối với tất cả 10 hướng dịch và cả mô hình 7B và 13B, LLaMA-2 được huấn luyện bởi công thức của chúng tôi vượt trội đáng kể so với hiệu suất zero-shot ban đầu. Ví dụ, ALMA-7B đạt +16.12 BLEU và +17.61 COMET cho en→xx trung bình. Đáng chú ý rằng LLaMA-2-13B gặp vấn đề off-target trong dịch thuật zero-shot en→xx. Tuy nhiên, nó có thể được cải thiện đáng kể bằng học ngữ cảnh few-shot (Brown et al., 2020), nhưng vẫn tụt hậu đáng kể so với các phương pháp của chúng tôi (ví dụ, hơn 10 BLEU và COMET khi dịch từ tiếng Anh). Chúng tôi thảo luận thêm về điều này trong Phụ lục E.

⁷Kết quả GPT-4 được lấy từ Zhang et al. (2023b).

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Bảng 2 với kết quả chi tiết cho xx→en được dịch với các điểm số BLEU và COMET cho các mô hình khác nhau]

So sánh với Các nghiên cứu Tương tự Trước đây ALMA vượt trội đáng kể so với tất cả các nghiên cứu trước đây. BigTranslate, được tinh chỉnh trên kho dữ liệu tiếng Trung và 300M kho dữ liệu song song, gặp khó khăn để vượt qua hiệu suất zero-shot của LLaMA-2, ngoại trừ en→zh. Quan sát này cũng phù hợp với các phát hiện của chúng tôi rằng một lượng quá mức dữ liệu song song có thể gây hại cho mô hình, trong khi dữ liệu đơn ngôn ngữ đích có ích cho dịch thuật. Cả TIM và SWIE đều nhắm mục tiêu cụ thể vào hai ngôn ngữ có tài nguyên cao, de và zh. Tuy nhiên, hiệu suất của chúng chủ yếu được xác định bởi các mô hình backbone của chúng: dịch thuật hiệu quả được quan sát cho zh nhưng kém cho de khi sử dụng BLOOMZ, và ngược lại với LLaMA-1. Ngược lại, ALMA linh hoạt, thể hiện kết quả mạnh mẽ trên tất cả các hướng.

So sánh với Mô hình SoTA Mô hình tốt nhất của chúng tôi (ALMA-13B-LoRA) vượt trội đáng kể so với NLLB-54B và GPT-3.5-D trung bình. Trong hướng en→xx, nó thậm chí vượt trội so với GPT-3.5-T trên COMET trung bình (87.00 so với 86.56) và có hiệu suất gần như tương đương khi nói đến xx→en. Đáng chú ý, các mô hình SoTA thường xuất sắc với các ngôn ngữ có tài nguyên cao nhưng gặp khó khăn với các ngôn ngữ ít tài nguyên như is. Với công thức của chúng tôi, hiệu suất của is vẫn mạnh mẽ và hoạt động tốt nhất.

[Hình 5 với biểu đồ hiệu suất trung bình của ALMA-7B sau khi hoàn thành mỗi 1B-token tinh chỉnh]

6 PHÂN TÍCH

6.1 NÊN SỬ DỤNG BAO NHIÊU DỮ LIỆU ĐƠN NGÔN NGỮ?

Trong kết quả chính của chúng tôi, chúng tôi trình bày ALMA với các cài đặt tốt nhất, được tinh chỉnh trên 20B hoặc 12B token. Tuy nhiên, chúng tôi chụp ảnh tất cả các mô hình ALMA sau mỗi 1B token đơn ngôn ngữ (và dữ liệu song song do con người viết) mà chúng đã được tinh chỉnh, và đánh giá tất cả hiệu suất dịch thuật của chúng. Như được minh họa trong Hình 5, chúng tôi báo cáo hiệu suất trung bình của ALMA-7B trên tất cả các hướng sau khi tinh chỉnh mỗi 1B token. Bộ dữ liệu thử nghiệm vẫn như nhau, tức là 10 hướng đã đề cập trước đó. Chúng tôi cung cấp kết quả số chi tiết và phân tích tương tự cho ALMA-13B trong Phụ lục F. Quan trọng, chỉ cần tinh chỉnh trên 1B token đơn ngôn ngữ, theo sau bởi tinh chỉnh trên dữ liệu do con người viết, mang lại hiệu suất tương đương với NLLB-54B và GPT-3.5-D. Trong thực tế, chúng tôi sử dụng 16 GPU MI200 với kích thước batch 256 và độ dài chuỗi 512, chỉ cần 18 giờ để hoàn thành việc tinh chỉnh 1B token và thêm một giờ được phân bổ cho tinh chỉnh dữ liệu do con người viết. Cần khoảng 19 giờ huấn luyện để có một mô hình MMT mạnh.

6.2 TÁC ĐỘNG CỦA DỮ LIỆU ĐƠN NGÔN NGỮ VÀ CHẤT LƯỢNG DỮ LIỆU SONG SONG

Để xem xét tác động của dữ liệu đơn ngôn ngữ, chúng tôi so sánh các mô hình LLaMA-2-7B được tinh chỉnh với và không có dữ liệu đơn ngôn ngữ (20B token), trong khi giữ cùng dữ liệu song song. Hơn nữa, để đánh giá tác động của chất lượng dữ liệu song song, chúng tôi giới thiệu ba bộ dữ liệu song song khác nhau cho tinh chỉnh giai đoạn 2. Bộ dữ liệu đầu tiên là dữ liệu do con người viết (HW) được sử dụng trong các thí nghiệm trước đây. Bộ thứ hai là dữ liệu được lọc (Filtered) được tham chiếu trong Phần 3.1. Cuối cùng, chúng tôi sử dụng bộ dữ liệu được chọn ngẫu nhiên (Random) có nguồn gốc từ dữ liệu WMT toàn diện. Chúng tôi dự đoán thứ bậc chất lượng là HW, theo sau bởi Filtered, và cuối cùng là Random. Đối với cả Filtered và Random, mỗi hướng dịch có 10K dữ liệu song song, căn chỉnh tổng kích thước bộ dữ liệu huấn luyện với HW. Chúng tôi hiển thị kết quả nghiên cứu khử các yếu tố trong Bảng 3. Sử dụng LLaMA-2-7B làm mô hình nền tảng, rõ ràng rằng với cùng dữ liệu song song, việc kết hợp dữ liệu đơn ngôn ngữ nâng cao đáng kể kết quả dịch thuật, ví dụ, tăng từ 74.35 đến 83.98 trong điểm COMET en→xx khi huấn luyện trên cùng dữ liệu Filtered. Hơn nữa, bất kể sự hiện diện của dữ liệu đơn ngôn ngữ, các mô hình được tinh chỉnh với dữ liệu chất lượng cao hơn thể hiện hiệu suất tốt hơn. Cả dữ liệu đơn ngôn ngữ và dữ liệu do con người viết đều xuất hiện như các yếu tố quan trọng trong việc cải thiện dịch thuật. Kết quả chi tiết cho từng cặp ngôn ngữ được trì hoãn đến Phụ lục G.

[Bảng 3 với nghiên cứu khử các yếu tố về tác động của dữ liệu đơn ngôn ngữ và chất lượng dữ liệu song song]

6.3 CÁC PHÂN TÍCH KHÁC

Chúng tôi cũng khám phá các phân tích sâu bổ sung và trình bày chi tiết trong phụ lục: 1) Tác động của khối lượng và lĩnh vực dữ liệu do con người viết đến hiệu suất dịch thuật được khám phá trong Phụ lục H; 2) So sánh giữa tinh chỉnh giai đoạn 2 (tinh chỉnh dữ liệu song song) và học few-shot trong ngữ cảnh có thể được tìm thấy trong Phụ lục I; 3) Đánh giá khả năng xuyên ngôn ngữ zero-shot của LLaMA-2 sau tinh chỉnh giai đoạn 1 trên các nhiệm vụ khác được trình bày trong Phụ lục J.

7 KẾT LUẬN

Trong bài báo này, chúng tôi chỉ ra rằng LLMs không cần một bộ sưu tập dữ liệu song song rộng lớn như các mô hình dịch thuật truyền thống. Sau đó, chúng tôi giới thiệu công thức huấn luyện mới cho LLMs decoder-only trong dịch thuật, tạo ra các mô hình dịch thuật mạnh mẽ, ALMA. Khi sử dụng LLaMA-2 làm mô hình nền tảng, ALMA vượt qua hiệu suất dịch thuật zero-shot của LLaMA-2 hơn 12 điểm BLEU và COMET trung bình trên 10 hướng. Hơn nữa, các mô hình ALMA vượt trội so với tất cả các nghiên cứu trước đây và thậm chí vượt trội so với NLLB-54B và GPT-3.5-D.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ƠN

Chúng tôi xin gửi lời cảm ơn đến Hieu Hoang, Marcin Junczys-Dowmunt, Yunmo Chen, Steven Tan, Huda Khayrallah, Thamme Gowda, Vikas Raunak, Matt Post, Anoop Kunchukuttan, Roman Grundkiewicz, Tom Kocmi, Kenton Murray và Arul Menezes vì những gợi ý sâu sắc và có giá trị của họ.

TÀI LIỆU THAM KHẢO

[Danh sách đầy đủ các tài liệu tham khảo được dịch sang tiếng Việt]

--- TRANG 9 ---
[Tiếp tục dịch nội dung còn lại...]
