# 2305.04160.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2305.04160.pdf
# Kích thước tập tin: 4287049 byte

===============================================
NỘI DUNG TẬP TIN PDF
===============================================

--- TRANG 1 ---
X-LLM:
Khởi động các Mô hình Ngôn ngữ Lớn Tiên tiến bằng cách
Xem Multi-Modalities như Ngôn ngữ Nước ngoài
Feilong Chen1;2, Minglun Han1;3,
Haozhi Zhao1;3, Qingyang Zhang1;2, Jing Shi1, Shuang Xu1và Bo Xu1;2;3
1Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc
2Trường Công nghệ Tương lai, Đại học Viện Hàn lâm Khoa học Trung Quốc
3Trường Trí tuệ Nhân tạo, Đại học Viện Hàn lâm Khoa học Trung Quốc
https://x-llm.github.io

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng ngôn ngữ đáng chú ý.
GPT-4, dựa trên các LLM tiên tiến, cho thấy khả năng đa phương thức phi thường
vượt xa các mô hình ngôn ngữ thị giác trước đây. Chúng tôi cho rằng điều này là do việc sử dụng
LLM tiên tiến hơn so với các mô hình đa phương thức trước đây. Thật không may,
kiến trúc mô hình và chiến lược huấn luyện của GPT-4 vẫn chưa được biết. Để trao cho LLM
khả năng đa phương thức, chúng tôi đề xuất X-LLM, chuyển đổi Multi-modalities
(hình ảnh, giọng nói, video) thành ngôn ngữ nước ngoài bằng giao diện X2L và đưa
chúng vào một mô hình ngôn ngữ lớn (ChatGLM). Cụ thể, X-LLM căn chỉnh nhiều
bộ mã hóa đơn phương thức đông lạnh và một LLM đông lạnh bằng giao diện X2L, trong đó "X"
biểu thị đa phương thức như hình ảnh, giọng nói và video, và "L" biểu thị
ngôn ngữ. Quá trình huấn luyện X-LLM gồm ba giai đoạn: (1) Chuyển đổi Thông tin Đa phương thức:
Giai đoạn đầu tiên huấn luyện từng giao diện X2L để căn chỉnh với bộ mã hóa
đơn phương thức tương ứng riêng biệt để chuyển đổi thông tin đa phương thức thành ngôn ngữ.
(2) Căn chỉnh biểu diễn X2L với LLM: các bộ mã hóa đơn phương thức được căn chỉnh
với LLM thông qua giao diện X2L một cách độc lập. (3) Tích hợp nhiều
phương thức: tất cả các bộ mã hóa đơn phương thức được căn chỉnh với LLM thông qua giao diện X2L
để tích hợp khả năng đa phương thức vào LLM. Các thí nghiệm của chúng tôi
cho thấy X-LLM thể hiện khả năng trò chuyện đa mô hình ấn tượng, đôi khi
thể hiện hành vi của GPT-4 đa phương thức trên hình ảnh/hướng dẫn chưa thấy, và
đạt 84,5% điểm tương đối so với GPT-4 trên bộ dữ liệu theo dõi hướng dẫn đa phương thức
tổng hợp. Và chúng tôi cũng tiến hành các bài kiểm tra định lượng về việc sử dụng LLM
cho ASR và ASR đa phương thức, hy vọng thúc đẩy kỷ nguyên nhận dạng giọng nói
dựa trên LLM.

1 Giới thiệu
Trong những năm gần đây, các mô hình ngôn ngữ đa phương thức [31,29,24] đã trải qua sự phát triển nhanh chóng. Những
mô hình này sở hữu khả năng xuất sắc trong hiểu đa phương thức và tạo phản hồi và có thể
thực hiện tốt các nhiệm vụ như mô tả hình ảnh [50], trả lời câu hỏi thị giác [1], đối화 thị giác [9],
mô tả video [18], và đối thoại nói [52]. Đáng chú ý là một mô hình đa phương thức
quy mô lớn, GPT-4 [37], gần đây đã được giới thiệu, thể hiện nhiều khả năng ấn tượng. Ví
dụ, GPT-4 có thể tuân theo nhiều hướng dẫn khác nhau để hoàn thành các nhiệm vụ ngôn ngữ, và cũng có thể trả lời
các câu hỏi khác nhau về hình ảnh. Chẳng hạn, GPT-4 có thể đưa ra các mô tả chi tiết và chính xác về
hình ảnh, hiểu và giải thích sự hài hước trong nội dung thị giác, và thậm chí cung cấp mã xây dựng trang web
chính xác dựa trên hình ảnh mã viết tay. Mặc dù GPT-4 thể hiện khả năng đáng chú ý,
Bản thảo. Công việc đang tiến hànharXiv:2305.04160v3 [cs.CL] 22 Tháng 5 2023

--- TRANG 2 ---
thật không may, chúng ta không biết chi tiết về cấu trúc mô hình và phương pháp huấn luyện của nó. Chúng tôi tin
rằng điều này là do GPT-4 sử dụng một mô hình ngôn ngữ tiên tiến và lớn hơn so với
các mô hình đa phương thức trước đây. Với sự hỗ trợ của khả năng ngôn ngữ mạnh mẽ, GPT-4 có thể biểu đạt
nội dung thị giác đã hiểu dưới dạng ngôn ngữ.

Để xác minh giả thuyết này và trao cho LLM khả năng đa phương thức, chúng tôi đề xuất X-LLM.
Nó chuyển đổi thông tin đa phương thức, như hình ảnh, giọng nói và video, thành ngôn ngữ nước ngoài
bằng giao diện X2L, và sau đó đưa thông tin đa phương thức đã chuyển đổi vào một mô hình ngôn ngữ lớn
(ChatGLM). Cụ thể, X-LLM căn chỉnh nhiều bộ mã hóa đơn phương thức đông lạnh và một LLM đông lạnh
bằng giao diện X2L. Giao diện X2L bao gồm giao diện hình ảnh I2L, giao diện video V2L,
và giao diện giọng nói S2L, trong đó "X" biểu thị đa phương thức và "L" biểu thị ngôn ngữ.
Giao diện hình ảnh và giao diện video có cùng cấu trúc, và chúng tôi áp dụng Q-Former từ
BLIP-2 [29] để chuyển đổi thông tin thị giác thành biểu diễn ngôn ngữ nước ngoài. Để hiệu quả, giao diện
video tái sử dụng các tham số của giao diện hình ảnh với dữ liệu hình ảnh-văn bản nhưng được huấn luyện thêm
với dữ liệu video-văn bản để căn chỉnh các đặc trưng video được mã hóa với LLM. Giao diện giọng nói sử dụng
cơ chế tích hợp và bắn liên tục (CIF) [12,23] và cấu trúc transformer để chuyển đổi
phát ngôn giọng nói thành biểu diễn ngôn ngữ nước ngoài. Quá trình huấn luyện X-LLM gồm ba giai đoạn. (1) Chuyển đổi Thông tin Đa phương thức: giai đoạn đầu tiên huấn luyện từng giao diện X2L để căn chỉnh với
bộ mã hóa đơn phương thức tương ứng riêng biệt để chuyển đổi thông tin đa phương thức thành ngôn ngữ.
(2) Căn chỉnh biểu diễn X2L với LLM: các bộ mã hóa đơn phương thức được căn chỉnh với LLM
thông qua giao diện X2L. (3) Tích hợp nhiều phương thức: tất cả các bộ mã hóa đơn phương thức được căn chỉnh
với LLM thông qua giao diện X2L để tích hợp khả năng đa phương thức vào LLM. Trong hai giai đoạn đầu, chúng tôi sử dụng dữ liệu mô tả hình ảnh, dữ liệu mô tả video và dữ liệu nhận dạng giọng nói tự động (ASR)
để huấn luyện giao diện X2L. Để trang bị LLM tốt hơn với khả năng đa phương thức, chúng tôi xây dựng một
bộ dữ liệu hướng dẫn đa phương thức (10K) dựa trên các bộ dữ liệu mã nguồn mở để cải thiện thêm
mô hình được đề xuất. Mặc dù không có giai đoạn huấn luyện thứ ba, X-LLM đã có khả năng hoàn thành
các nhiệm vụ đa phương thức như trả lời câu hỏi nói thị giác, chúng tôi thấy rằng chỉ với ít dữ liệu hướng dẫn đa phương thức bổ sung, LLM có thể thống nhất thêm khả năng của nhiều phương thức.

Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng X-LLM có khả năng tương tự như GPT-4. Ví dụ, X-LLM
có thể tạo ra các mô tả hình ảnh phức tạp và giải thích các hiện tượng thị giác bất thường. Trong nghiên cứu của chúng tôi,
khi sử dụng hình ảnh đầu vào, X-LLM có thể nhận dạng vị trí trong hình ảnh, như xác định
Cung điện Cấm và cung cấp thông tin liên quan về nó, quan sát thức ăn trong hình ảnh và cung cấp
công thức chi tiết; tạo câu chuyện cho hình ảnh, và đưa ra ý nghĩa văn bản cho logo. Chúng tôi cũng
thấy rằng khả năng liên quan đến hình ảnh của X-LLM có thể được mở rộng sang video, như giới thiệu nội dung
của video, truy xuất tên phim, hoặc các sự thật liên quan đến nghệ thuật trực tiếp từ video. Hơn nữa, X-LLM
có thể trả lời câu hỏi dựa trên câu hỏi nói và có thể kết hợp hình ảnh hoặc video để trả lời câu hỏi nói. Những khả năng này trước đây không có trong các mô hình đa phương thức trước đây nhưng giờ đây
được thực hiện nhờ khả năng mô hình hóa ngôn ngữ mạnh mẽ của X-LLM.

Đóng góp của chúng tôi được tóm tắt như sau:
•Khung LLM đa phương thức. Chúng tôi đề xuất X-LLM, một LLM đa phương thức tiêm nhiều
phương thức (như hình ảnh, giọng nói và video) vào LLM thông qua giao diện X2L, mang lại cho LLM
khả năng xử lý dữ liệu đa phương thức. Khung này có khả năng mở rộng tốt và có thể được mở rộng
sang nhiều phương thức hơn. "X" trong giao diện X2L có thể là bất kỳ phương thức nào. Chúng tôi so sánh X-LLM của chúng tôi với
LLaVA và MiniGPT-4 về khả năng xử lý đầu vào thị giác với các yếu tố Trung Quốc,
và thấy rằng X-LLM vượt trội hơn họ đáng kể. Chúng tôi cũng tiến hành các bài kiểm tra định lượng về
việc sử dụng LLM cho ASR và ASR đa phương thức, hy vọng thúc đẩy kỷ nguyên nhận dạng giọng nói
dựa trên LLM.
•Khả năng chuyển giao của các tham số trong mô-đun căn chỉnh hình ảnh-văn bản tiếng Anh. Chúng tôi thấy rằng mô-đun Q-
former được huấn luyện trên dữ liệu hình ảnh-văn bản tiếng Anh có thể được chuyển giao sang các ngôn ngữ khác. Trong
các thí nghiệm của chúng tôi, chúng tôi đã chuyển giao thành công các tham số mô hình từ tiếng Anh
Indo-Châu Âu sang tiếng Trung Hán-Tạng. Khả năng chuyển giao của ngôn ngữ làm tăng đáng kể khả năng
sử dụng dữ liệu hình ảnh-văn bản tiếng Anh và các tham số mô hình đã huấn luyện của nó, và cải thiện hiệu quả
của việc huấn luyện LLM đa phương thức trong các ngôn ngữ khác.
•Mã nguồn mở. Chúng tôi xây dựng một bộ dữ liệu hướng dẫn đa phương thức tiếng Trung ngắn gọn và chất lượng cao.
Bằng cách huấn luyện X-LLM trên dữ liệu hướng dẫn đa phương thức này, X-LLM có thể tích hợp tốt hơn
khả năng đa phương thức thu được thông qua nhiều bộ mã hóa và giao diện X2L tương ứng.
Và chúng tôi phát hành các tài sản sau cho công chúng: dữ liệu hướng dẫn đa phương thức được tạo,
cơ sở mã cho huấn luyện mô hình, checkpoint mô hình và demo trò chuyện đa phương thức.

--- TRANG 3 ---
LLM
Bộ Mã hóa  
Hình ảnh
Bộ Mã hóa 
Video 
Bộ Mã hóa
Giọng nóiQ-Former I-Adapter 
Q-Former V-Adapter 
CIF S-Adapter 
Giao diện X2L
这 是 什 么 地 方 ？ 
(Đây là đâu?)
介 绍 一 下 这 段 视 频 的 内 容 
(Mô tả nội dung video này)
识 别 语 音 中 的 内 容
(Nhận dạng nội dung trong giọng nói)这 是 故 宫 ， 又 称 紫 禁 城 ， 是 中 国 明 清
两 朝 的 皇 宫 ， 位 于 北 京 市 中 心 ， 占 地
面 积 约 72万 平 方 米 ， 是 世 界 上 最 大 、
保 存 最 为 完 整 的 古 代 宫 殿 建 筑 群 之
一 ...
(Đây là Bảo tàng Cung điện, còn được gọi là
Tử Cấm Thành, từng là cung điện hoàng gia
trong thời nhà Minh và nhà Thanh ở Trung Quốc. Nằm ở trung tâm
Bắc Kinh, diện tích khoảng
720.000 mét vuông và là một trong những
quần thể cung điện cổ lớn nhất và được bảo tồn tốt nhất
trên thế giới...)
一 群 狮 子 在 草 原 上 晒 太 阳 ...
(Một đàn sư tử đang tắm nắng trên
thảo nguyên...)
推 荐 一 下 北 京 的 旅 游 景 点
(Giới thiệu một số điểm du lịch ở Bắc Kinh)Q-Former I-Adapter 
V-Adapter Q-Former
C-FormerHình 1: Kiến trúc mạng X-LLM.

2 Công trình liên quan
Mô hình Thị giác-Ngôn ngữ. Như được tóm tắt trong nhiều khảo sát [5,16], các mô hình ngôn ngữ thị giác [58,30] đã có những bước tiến lớn với sự phát triển của các kỹ thuật tiền huấn luyện [11,2,60]. Trong những ngày đầu, các nhà nghiên cứu sử dụng Faster-RCNN [19] để trích xuất đặc trưng hình ảnh và nối chúng với các mô hình ngôn ngữ như BERT [11] để thực hiện tiền huấn luyện thị giác-ngôn ngữ. VisualBERT [31], ví dụ, kết hợp các vùng hình ảnh và ngôn ngữ bằng Transformer [49] để cho phép self-attention khám phá các căn chỉnh ngầm giữa ngôn ngữ và thị giác. Nó được tiền huấn luyện với mô hình hóa ngôn ngữ có mặt nạ [11] và một nhiệm vụ dự đoán câu-hình ảnh [31]. Với việc giới thiệu ViLT [26], các nhà nghiên cứu sử dụng vision transformer [13,25] để xử lý hình ảnh, textual transformer (như BERT [11], GPT-2 [42], T5 [43]) để xử lý văn bản, và các mục tiêu tiền huấn luyện như mô hình hóa ngôn ngữ có mặt nạ, khớp hình ảnh-văn bản, và tương phản hình ảnh-văn bản để huấn luyện các mô hình ngôn ngữ thị giác. CLIP [41] sử dụng một bộ mã hóa văn bản và một bộ mã hóa hình ảnh để mã hóa văn bản và hình ảnh riêng biệt và sau đó thực hiện học tương phản không giám sát để có được biểu diễn tốt của căn chỉnh thị giác-ngôn ngữ. BLIP [30] là một khung VLP mới chuyển giao linh hoạt cho cả nhiệm vụ hiểu và tạo thị giác-ngôn ngữ.

Trong lĩnh vực đối thoại thị giác [9,6,4], các nhà nghiên cứu thiết kế các mục tiêu tiền huấn luyện liên quan đến đối thoại thị giác dựa trên các mô hình tiền huấn luyện thị giác-ngôn ngữ [11,31] và tinh chỉnh các mô hình thị giác-ngôn ngữ trên dữ liệu đối thoại thị giác [9] để đạt được hiệu suất đối thoại tốt hơn. VisDial-BERT [36] và VD-BERT [51], ví dụ, sử dụng ViLBERT [35] và BERT đã được tiền huấn luyện để tinh chỉnh các mô hình trên dữ liệu đối thoại thị giác bằng mô hình hóa ngôn ngữ có mặt nạ và khớp hình ảnh-văn bản. AlignVD [7] đề xuất hai phương pháp căn chỉnh thị giác-ngôn ngữ dựa trên ViT [41] và BERT đã được tiền huấn luyện để đạt được hiệu suất tốt hơn trong đối thoại thị giác.

Tăng cường Hiểu biết Thị giác-ngôn ngữ với LLM Tiên tiến. Mặc dù các mô hình thị giác-ngôn ngữ nêu trên đã đạt được một số thành công, vẫn còn chỗ cải thiện đáng kể về mặt tạo ngôn ngữ [5,34,29]. Một phương pháp gần đây [29,14,37] để tăng cường hiểu biết ngôn ngữ thị giác bằng các mô hình ngôn ngữ quy mô lớn tiên tiến [48,8] đã được đề xuất. Ví dụ, BLIP2 [29] sử dụng Q-Former để kết nối bộ mã hóa thị giác với LLM, căn chỉnh các truy vấn đã học của Q-Former với các đặc trưng thị giác liên quan đến ngôn ngữ được trích xuất bởi bộ mã hóa thị giác. Q-Former sau đó kết nối bộ mã hóa thị giác với mô hình ngôn ngữ, cho phép các biểu diễn truy vấn đã học thích ứng với LLM. PaLM-E [14] kết hợp ViT-22B [10] với PaLM-560B [2] để tiêm thông tin đa phương thức vào không gian nhúng của mô hình ngôn ngữ đã được tiền huấn luyện, thiết lập kết nối giữa nhận thức và ngôn ngữ và tăng cường đáng kể khả năng hiểu ngôn ngữ thị giác của mô hình. Ngoài ra, Visual ChatGPT [53] và HuggingGPT [46] sử dụng ChatGPT làm bộ điều khiển logic cốt lõi, hiểu ý định của người dùng và sau đó gọi các mô hình ngôn ngữ thị giác chuyên domain cụ thể. Cuối cùng, GPT-4 [37] được đề xuất gần đây thể hiện khả năng đa phương thức mạnh mẽ: xây dựng trên khả năng hiểu ngôn ngữ mạnh của nó, nó có thể tạo ra các mô tả hình ảnh phức tạp, tạo trang web dựa trên hướng dẫn văn bản viết tay, và giải thích các hiện tượng thị giác bất thường. Tuy nhiên, cấu trúc mô hình và chiến lược huấn luyện của GPT-4 vẫn là một bí ẩn. MiniGPT-4 [59] và LLaVA [33] căn chỉnh dữ liệu văn bản và hình ảnh với mô hình ngôn ngữ quy mô lớn Vicuna [8] và ViT [57] để hoàn thành các nhiệm vụ ngôn ngữ dựa trên hình ảnh. Ngược lại, X-LLM là một khung phổ quát cho LLM đa phương thức khởi động các mô hình ngôn ngữ lớn tiên tiến bằng cách xem multi-modalities như ngôn ngữ nước ngoài. Trong bài báo này, chúng tôi triển khai X-LLM hỗ trợ hình ảnh, video và giọng nói. Dựa trên khung X-LLM, chúng tôi có thể mở rộng mô hình sang nhiều phương thức hơn, như tiêm trạng thái robot không gian liên tục, thông tin terminal, hoặc âm thanh thay vì giọng nói vào LLM.

--- TRANG 4 ---
3 Phương pháp
X-LLM nhằm căn chỉnh nhiều bộ mã hóa đơn phương thức đã được tiền huấn luyện với các mô hình ngôn ngữ quy mô lớn (LLM) tiên tiến, như được hiển thị trong Hình 1. Cụ thể, chúng tôi sử dụng ChatGLM1 làm bộ giải mã ngôn ngữ, được xây dựng trên GLM [17,56] và có thể thực hiện nhiều nhiệm vụ ngôn ngữ phức tạp khác nhau. Để nhận thức thị giác, chúng tôi áp dụng ViT-g [57], làm bộ mã hóa hình ảnh và bộ mã hóa video. Để nhận thức giọng nói, chúng tôi sử dụng một bộ mã hóa giọng nói bao gồm các lớp tích chập và cấu trúc conformer [21]. Chúng tôi thiết kế một module căn chỉnh thông tin đa phương thức với LLM, được gọi chung là giao diện X2L, bao gồm giao diện hình ảnh, giao diện video và giao diện giọng nói. Giao diện hình ảnh và giao diện video có cùng cấu trúc bao gồm Q-Formers [29] và các module Adapter. Giao diện giọng nói bao gồm C-Former và module Adapter. C-Former có thể nén chuỗi đặc trưng giọng nói cấp khung từ bộ mã hóa giọng nói thành chuỗi nhúng giọng nói cấp token với cơ chế tích hợp và bắn liên tục (CIF) [12,23,22]. Vì chuỗi nhúng giọng nói cấp token được căn chỉnh chặt chẽ với chuỗi token của bản ghi tương ứng với phát ngôn giọng nói, việc biểu diễn giọng nói bằng nhúng giọng nói cấp token có thể giảm hiệu quả việc sử dụng bộ nhớ GPU khi kết hợp giọng nói vào LLM.

3.1 Giao diện X2L
Giao diện X2L nhằm chuyển đổi thông tin đa phương thức thành ngôn ngữ nước ngoài, bao gồm giao diện hình ảnh, giao diện video và giao diện giọng nói.

Giao diện Hình ảnh. Lấy cảm hứng từ [29], giao diện hình ảnh bao gồm Q-Formers [29] và module I-Adapter. Q-Formers nhằm chuyển đổi hình ảnh thành ngôn ngữ, trong đó các đặc trưng hình ảnh thu được từ bộ mã hóa hình ảnh được chuyển đổi thành một chuỗi với L nhúng tựa ngôn ngữ. Module I-Adapter nhằm căn chỉnh các chiều của nhúng tựa ngôn ngữ và chiều nhúng của LLM.

Giao diện Video. Giao diện video có cùng cấu trúc với giao diện hình ảnh, cũng bao gồm Q-Formers [29] và module V-Adapter. Chúng tôi sử dụng lấy mẫu đều và biểu diễn mỗi video với T khung. Sau đó chúng tôi xem mỗi khung như một hình ảnh. Giao diện video chuyển đổi đặc trưng của mỗi khung thành một chuỗi với L nhúng tựa ngôn ngữ. Sau đó giao diện video nối tất cả các chuỗi để có được nhúng tựa ngôn ngữ cuối cùng, có độ dài TLi.

Giao diện Giọng nói. Để biến đổi các đặc trưng giọng nói từ bộ mã hóa giọng nói thành các biểu diễn ngữ nghĩa hơn, chúng tôi giới thiệu giao diện giọng nói-tới-ngôn ngữ được gọi là giao diện giọng nói. Giao diện giọng nói bao gồm hai phần, cụ thể là C-Former và S-Adaptor. C-Former là sự kết hợp của module CIF và cấu trúc transformer 12 lớp [11]. Đầu tiên, module CIF nén chuỗi đặc trưng giọng nói từ bộ mã hóa giọng nói thành chuỗi nhúng giọng nói cấp token có cùng độ dài với bản ghi tương ứng thông qua lấy mẫu xuống biến-độ dài. Giả sử độ dài của chuỗi đặc trưng được phát ra bởi bộ mã hóa giọng nói cho đầu vào giọng nói là U, và độ dài của chuỗi token của bản ghi của phát ngôn giọng nói là Ls, độ dài của chuỗi nhúng giọng nói cấp token nên là Ls (U thường dài hơn Ls nhiều lần). Sau đó, cấu trúc transformer cung cấp mô hình hóa ngữ cảnh cho các nhúng giọng nói cấp token từ module CIF. Cuối cùng, S-Adaptor được sử dụng để chiếu các đầu ra của cấu trúc transformer vào không gian vector đầu vào của LLM, thu hẹp thêm khoảng cách ngữ nghĩa giữa giọng nói và ngôn ngữ.

--- TRANG 5 ---
3.2 Chiến lược Huấn luyện
Để triển khai X-LLM một cách hiệu quả, chúng tôi đề xuất chiến lược huấn luyện ba giai đoạn. (1) Chuyển đổi Thông tin Đa phương thức: chúng tôi căn chỉnh Bộ Mã hóa Hình ảnh với Q-Former của hình ảnh (phần màu xanh lá), và Bộ Mã hóa Giọng nói với module CIF. (2) Căn chỉnh biểu diễn X2L với LLM: trong giai đoạn thứ hai, chúng tôi căn chỉnh Bộ Mã hóa Hình ảnh với LLM thông qua giao diện hình ảnh, căn chỉnh Bộ Mã hóa Video với LLM thông qua giao diện video, và căn chỉnh Bộ Mã hóa Giọng nói với LLM thông qua giao diện giọng nói. Trong giai đoạn thứ ba, chúng tôi tích hợp huấn luyện hình ảnh, video và giọng nói, và căn chỉnh tổng thể các bộ mã hóa đơn phương thức với LLM bằng bộ dữ liệu hướng dẫn đa phương thức nhỏ hơn nhưng chất lượng cao (như hướng dẫn chứa đối thoại nói thị giác, tức là phản hồi đầu vào đối thoại nói dựa trên hình ảnh).

3.2.1 Giai đoạn Huấn luyện Thứ nhất: Chuyển đổi Thông tin Đa phương thức
Trong giai đoạn đầu, phương pháp truyền thống là căn chỉnh Bộ Mã hóa Hình ảnh với Q-Former hình ảnh bằng một lượng lớn dữ liệu hình ảnh-văn bản, tương tự như giai đoạn đầu của BLIP2 [29] đã sử dụng khoảng 500 triệu cặp hình ảnh-văn bản. Tuy nhiên, chúng tôi thấy rằng trong khi BLIP2 sử dụng dữ liệu tiếng Anh, chúng tôi vẫn có thể tận dụng các tham số đã được tiền huấn luyện của Q-Former trong BLIP2 để triển khai LLM Đa phương thức tiếng Trung. Do đó, trong giai đoạn đầu, để triển khai X-LLM một cách hiệu quả, chúng tôi chỉ chuyển đổi biểu diễn của bộ mã hóa giọng nói thành biểu diễn tựa ngôn ngữ thông qua giao diện giọng nói.

Đối với các cấu trúc liên quan đến giọng nói, chúng tôi huấn luyện mô hình ASR dựa trên CIF với nhiều bộ dữ liệu ASR chứa để có được bộ mã hóa giọng nói và module CIF trong C-Former. Mô hình ASR dựa trên CIF bao gồm bộ mã hóa giọng nói, module CIF và bộ giải mã [12]. Chúng tôi sử dụng bộ mã hóa giọng nói của mô hình ASR này làm bộ mã hóa giọng nói và sử dụng module CIF của mô hình ASR này làm module trong C-Former của giao diện giọng nói. Lưu ý rằng các tham số của bộ mã hóa giọng nói và module CIF được giữ đông lạnh trong tất cả các giai đoạn huấn luyện tiếp theo. Vui lòng tham khảo phụ lục để biết thêm chi tiết về cấu trúc và huấn luyện của mô hình ASR dựa trên CIF.

3.2.2 Giai đoạn Huấn luyện Thứ hai: Căn chỉnh biểu diễn X2L với LLM
Như đã đề cập ở trên, mặc dù có sự khác biệt về ngôn ngữ, chúng tôi vẫn có thể tái sử dụng các tham số của Q-Former trong BLIP2. Cụ thể, chúng tôi đã sử dụng Q-Former được huấn luyện trong giai đoạn thứ hai của BLIP2 để khởi tạo Q-Former của giao diện hình ảnh trong X-LLM. Để thích ứng Q-Former với LLM tiếng Trung, chúng tôi sử dụng bộ dữ liệu kết hợp, tổng cộng khoảng 14 triệu cặp hình ảnh-văn bản tiếng Trung để huấn luyện.

Tiếp theo, chúng tôi sử dụng giao diện hình ảnh đã được huấn luyện để khởi tạo giao diện video (Q-Former và V-Adapter) và huấn luyện giao diện video trên dữ liệu video-văn bản đã được dịch.

Cuối cùng, chúng tôi huấn luyện giao diện giọng nói bằng dữ liệu ASR để căn chỉnh đầu ra của giao diện giọng nói với LLM. Cần lưu ý rằng trong suốt giai đoạn huấn luyện thứ hai, tất cả các bộ mã hóa và LLM vẫn được đông lạnh, chỉ có các giao diện được huấn luyện.

3.2.3 Giai đoạn Huấn luyện thứ ba: Tích hợp Nhiều Phương thức
Sau hai giai đoạn huấn luyện đầu tiên, X-LLM của chúng tôi đã thể hiện khả năng đáng chú ý trong việc cung cấp câu trả lời hợp lý cho các truy vấn của con người dựa trên thông tin đa phương thức và đã thu được một lượng kiến thức khổng lồ. Chúng tôi đã quan sát thấy rằng, ngay cả khi không có hướng dẫn cho huấn luyện chung trên nhiều phương thức, như "trả lời câu hỏi dựa trên hình ảnh bằng đầu vào giọng nói," X-LLM có khả năng thực hiện các nhiệm vụ đòi hỏi nhiều phương thức, như đối thoại nói thị giác, nhận dạng giọng nói đa phương thức và dịch máy đa phương thức. Khả năng đáng chú ý này có thể là do X-LLM tích hợp khả năng tổng quát hóa hướng dẫn xuất sắc của LLM, đã được mở rộng sang lĩnh vực đa phương thức. Khả năng này cho phép chúng tôi huấn luyện nhiều phương thức độc lập hơn trong hai giai đoạn đầu và tích hợp chúng vào mô hình mà không cần huấn luyện chung với các phương thức hiện có.

Để khám phá tiềm năng của dữ liệu hướng dẫn chung đa phương thức trong việc tăng cường thêm khả năng của X-LLM thực hiện các nhiệm vụ đa phương thức, như trả lời câu hỏi nói thị giác, chúng tôi đã xây dựng một bộ dữ liệu hướng dẫn đa phương thức ngắn gọn nhưng chất lượng cao. Khác với bộ dữ liệu của MiniGPT-4 [59] và LLaVA [33], chỉ chứa dữ liệu hướng dẫn hình ảnh-văn bản và các bộ dữ liệu hướng dẫn văn bản khác cho tinh chỉnh hướng dẫn và đối thoại, bộ dữ liệu của chúng tôi hỗ trợ các hướng dẫn chung đa phương thức và bao gồm (1) dữ liệu hướng dẫn hình ảnh-văn bản, (2) dữ liệu hướng dẫn giọng nói-văn bản, (3) dữ liệu hướng dẫn video-văn bản, và (4) dữ liệu hướng dẫn hình ảnh-văn bản-giọng nói.

--- TRANG 6 ---
Xây dựng Bộ dữ liệu Căn chỉnh Chất lượng cao cho LLM Đa phương thức. Chúng tôi sử dụng ChatGPT để dịch 3,5K hướng dẫn hình ảnh-văn bản được xây dựng bởi MiniGPT-4. Sau đó, chúng tôi lựa chọn thủ công 2k dữ liệu từ AISHELL-2 [15] và viết 5 hướng dẫn khác nhau cho các nhiệm vụ nhận dạng giọng nói. Chúng tôi sử dụng ChatGPT để dịch bộ dữ liệu ActivityNet [27], sau đó lựa chọn thủ công 1k dữ liệu và viết 5 hướng dẫn khác nhau cho các nhiệm vụ video-văn bản tương ứng. Chúng tôi lựa chọn thủ công và viết lại 1k dữ liệu từ dữ liệu VSDial-CN tự xây dựng, nhằm cho phép mô hình thực hiện các nhiệm vụ tạo đối thoại dựa trên hình ảnh và giọng nói. Thêm chi tiết về dữ liệu có thể được tìm thấy trong phụ lục, bao gồm chi tiết về dữ liệu huấn luyện cho hai giai đoạn đầu và dữ liệu hướng dẫn đa phương thức.

Giai đoạn Huấn luyện Thứ ba. Trong giai đoạn này, chúng tôi sử dụng dữ liệu nhỏ gọn nhưng chất lượng cao đã xây dựng để tinh chỉnh mô hình của chúng tôi. Trong quá trình tinh chỉnh, chúng tôi sử dụng các prompt được định nghĩa trước trong mẫu sau:
<Image><ImageFeats></Image><Video><VideoFeats></Video><Speech><SpeechFeats>
</Speech>Question: <Instruction> nn Answer:

Trong prompt này, <Instruction> đại diện cho một hướng dẫn được lấy mẫu ngẫu nhiên từ tập hướng dẫn được định nghĩa trước của chúng tôi, bao gồm các dạng khác nhau như "mô tả hình ảnh này một cách chi tiết", "bạn có thể mô tả những gì bạn nhận thấy trong video", hoặc "trả lời câu hỏi trong giọng nói dựa trên hình ảnh". Cần lưu ý rằng chúng tôi không tính toán loss hồi quy cụ thể cho prompt hướng dẫn cụ thể này. Do đó, X-LLM có thể tích hợp nhiều phương thức và tạo ra các phản hồi tự nhiên và đáng tin cậy hơn dựa trên các tổ hợp hướng dẫn khác nhau khi cần thiết.

4 Thí nghiệm
4.1 Trò chuyện Đa phương thức
Chúng tôi đã phát triển demo Chatbot để thể hiện khả năng hiểu đa phương thức và đối thoại của X-LLM. Để so sánh, truy vấn LLaVA [33]2và MiniGPT-4 [59]3từ các demo trực tuyến của họ để nhận phản hồi của họ.

Như được hiển thị trong Bảng 2 và 3, mặc dù LLaVA và MiniGPT-4 cũng thể hiện đặc tính tạo ra câu trả lời dựa trên prompt đã cho, câu trả lời của họ về nội dung thị giác với các yếu tố Trung Quốc không thỏa đáng bằng. Trong ví dụ đầu tiên về Cung điện Cấm được hiển thị trong Bảng 2, X-LLM nhận ra rằng nơi này là Cung điện Cấm và cung cấp giới thiệu chi tiết về lịch sử, kiến trúc và phong cách của nó. LLaVA mô tả các cung điện và cờ Trung Quốc, nhưng nó không nhận ra rằng cung điện nổi tiếng là Cung điện Cấm và do đó không thể cung cấp thông tin liên quan về nó. MiniGPT-4 thể hiện cùng vấn đề và có xu hướng mô tả hình ảnh nhiều hơn. Trong ví dụ thứ hai về trò chơi "Honor of Kings" được hiển thị trong Bảng 3, X-LLM xác định nó là một trò chơi đấu trường trực tuyến nhiều người chơi, "Honor of Kings", được phát triển bởi Tencent và cung cấp thời gian phát hành chính xác. Mặt khác, LLaVA đưa ra nhiều câu trả lời không chính xác, vì không có yếu tố của các trò chơi phổ biến như rắn và pokemon trong hình ảnh, và trò chơi không được chơi bằng chuột. MiniGPT-4 không nhận ra được trò chơi và cung cấp mô tả chung hơn.

Đối với đầu vào video và đầu vào giọng nói, chúng tôi cung cấp một số ví dụ như được hiển thị trong Phụ lục B.

Đánh giá Định lượng. Để đánh giá một cách có hệ thống hiệu suất của mô hình X-LLM trên đầu vào thị giác, chúng tôi nhằm sử dụng các chỉ số định lượng để đo lường khả năng tuân theo hướng dẫn của mô hình. Chúng tôi áp dụng phương pháp đánh giá tương tự như được đề xuất bởi LLaVA [33] và sử dụng ChatGPT để đo lường chất lượng của các câu trả lời được tạo bởi mô hình của chúng tôi. Cụ thể, chúng tôi sử dụng bộ dữ liệu LLaVA-test [33] được cung cấp bởi LLaVA, chứa 30 hình ảnh được lựa chọn ngẫu nhiên từ tập validation COCO, mỗi hình ảnh có ba loại câu hỏi (đối thoại, mô tả chi tiết và lý luận phức tạp). Đầu tiên chúng tôi dịch các câu hỏi sang tiếng Trung, và X-LLM dự đoán các câu trả lời dựa trên các câu hỏi tiếng Trung đã dịch và hình ảnh đầu vào thị giác. Sau đó chúng tôi dịch các phản hồi được đưa ra bởi X-LLM sang tiếng Anh để so sánh với GPT-4. GPT-4 đưa ra dự đoán tham chiếu dựa trên câu hỏi, hộp giới hạn ground truth và chú thích, đánh dấu giới hạn trên. Sau khi có được các phản hồi từ hai mô hình, chúng tôi cung cấp câu hỏi, thông tin thị giác (dưới dạng chú thích và hộp giới hạn), và các phản hồi được tạo từ cả hai trợ lý cho ChatGPT. ChatGPT đánh giá tính hữu ích, liên quan, chính xác và mức độ chi tiết của các phản hồi của trợ lý và đưa ra điểm số tổng thể từ 1 đến 10, với điểm số cao hơn cho thấy hiệu suất tổng thể tốt hơn. ChatGPT cũng được yêu cầu cung cấp giải thích đánh giá toàn diện để hiểu rõ hơn về mô hình. LLaVA sử dụng GPT-4 làm giáo viên để đánh giá chất lượng của các phản hồi được tạo bởi LLaVA và GPT-4, trong khi chúng tôi tin rằng việc sử dụng mô hình đánh giá không phải GPT-4 (tức là sử dụng ChatGPT) sẽ khách quan hơn (Cũng vì chúng tôi không có API GPT-4.). Ví dụ về các câu hỏi kiểm tra có thể được tìm thấy trong Phụ lục A.2.

Chúng tôi hiển thị kết quả trong Bảng 1. Mặc dù các mô hình đánh giá khác nhau được sử dụng (LLaVA sử dụng GPT-4, X-LLM sử dụng ChatGPT), chúng tôi có thể đưa ra so sánh thô. Kết quả cho thấy X-LLM đạt hiệu suất 84,5% gần như bằng GPT-4. X-LLM vượt trội hơn LLaVA về mặt đối thoại và mô tả chi tiết nhưng kém hơn trong lý luận phức tạp. Có hai lý do cho điều này. Một lý do là X-LLM không sử dụng bộ dữ liệu hướng dẫn thị giác 150k được đề xuất bởi LLaVA, có cùng định dạng với tập kiểm tra. Lý do thứ hai là X-LLM có ít tham số mô hình ngôn ngữ hơn. Nó dựa trên ChatGLM với 6B tham số, trong khi LLaVA dựa trên Vicuna với 13B tham số. Và chúng tôi không tinh chỉnh LLM trong khi LLaVA tinh chỉnh LLM Vicuna.

Hơn nữa, so sánh "X-LLM w/ 4M" và "X-LLM w/ 4M no init", chúng ta có thể quan sát thấy rằng việc sử dụng các tham số Q-Former đã được tiền huấn luyện của BLIP2 cải thiện đáng kể hiệu suất của mô hình, điều này xác minh khả năng chuyển giao của các tham số trong module căn chỉnh hình ảnh văn bản tiếng Anh. Khả năng chuyển giao của ngôn ngữ làm tăng đáng kể khả năng sử dụng dữ liệu hình ảnh-văn bản tiếng Anh và các tham số mô hình đã huấn luyện của nó, và cải thiện hiệu quả của việc huấn luyện LLM đa phương thức trong các ngôn ngữ khác. So sánh X-LLM và "X-LLM w/ 4M", chúng ta có thể thấy rằng việc tăng số lượng cặp hình ảnh-văn bản được sử dụng trong quá trình huấn luyện có thể tăng cường hiệu suất của mô hình. Tuy nhiên, chúng tôi cũng chú ý rằng X-LLM thực hiện kém hơn "X-LLM w/ 4M" trong lý luận phức tạp, điều này có thể được quy cho việc sử dụng thêm bộ dữ liệu Wukong [20], có chất lượng kém hơn so với bộ dữ liệu bao gồm 4M dữ liệu. Lưu ý rằng tất cả ba biến thể của X-LLM đều đạt hiệu suất cao trên lý luận phức tạp do khả năng mạnh mẽ của các mô hình ngôn ngữ và các câu hỏi lý luận phức tạp cũng có thể được trả lời mà không cần hình ảnh ở một mức độ nào đó.

4.2 ASR và ASR Đa phương thức
Chúng tôi đánh giá ASR và ASR đa phương thức (MASR) trên các bộ dữ liệu AISHELL-2 và VSDial-CN. Vui lòng tham khảo Phụ lục A để biết chi tiết.

Như được hiển thị trong Bảng 4, chúng tôi so sánh X-LLM với mô hình dựa trên CIF được đề cập trong Phần 3.2.1. Từ kết quả trong bảng, nhìn chung, hiệu suất nhận dạng giọng nói của X-LLM yếu hơn so với Mô hình dựa trên CIF. Điều này là do mô hình dựa trên CIF được huấn luyện với tất cả các tham số, trong khi X-LLM chỉ huấn luyện BERT trong C-Former và S-Adapter. Trong kết quả của nhận dạng giọng nói đa phương thức trên bộ dữ liệu VSDial-CN, việc giới thiệu hình ảnh đã gây ra sự giảm sút trong hiệu suất nhận dạng giọng nói của X-LLM (từ 3,4 đến 4,7 trên test-orig và từ 4,7 đến 6,8 trên test-art). Điều này có thể là do chúng tôi cung cấp cho X-LLM một hướng dẫn đơn cố định "Vui lòng nhận dạng giọng nói một cách trung thực" trong giai đoạn huấn luyện thứ hai cho nhận dạng giọng nói. Tuy nhiên, khi tiến hành nhận dạng giọng nói đa phương thức, chúng tôi thay thế hướng dẫn bằng "Vui lòng nhận dạng giọng nói một cách trung thực dựa trên hình ảnh.". Sau khi tinh chỉnh X-LLM với 2k hướng dẫn liên quan đến nhận dạng giọng nói trong giai đoạn thứ ba, tỷ lệ lỗi của nhận dạng giọng nói đa phương thức của X-LLM giảm từ 4,7 xuống 4,3 trên test-orig và từ 6,8 xuống 6,3 trên test-art. Trong tương lai, chúng tôi sẽ bổ sung quá trình huấn luyện bằng cách sử dụng các hướng dẫn nhận dạng giọng nói khác nhau trong giai đoạn thứ hai và tăng lượng dữ liệu cho hướng dẫn tinh chỉnh trong giai đoạn thứ ba để quan sát các thay đổi trong khả năng nhận dạng giọng nói đa phương thức của X-LLM. Ngoài ra, một LLM mạnh mẽ hơn có thể có khả năng tổng quát hóa hướng dẫn mạnh hơn, điều này có thể cải thiện hiệu suất của nhận dạng giọng nói đa phương thức.

Chúng tôi quan sát thấy rằng mặc dù việc thêm hình ảnh vào nhiệm vụ nhận dạng giọng nói của X-LLM dẫn đến sự giảm sút nhẹ về hiệu suất, X-LLM có thể hiểu các câu hỏi nói trong giọng nói mà không cần tinh chỉnh, và cung cấp các phản hồi phù hợp. Nó cũng có thể kết hợp hình ảnh để cung cấp câu trả lời phù hợp cho các câu hỏi nói. Sau một lượng nhỏ dữ liệu tinh chỉnh trong giai đoạn thứ ba, khả năng của X-LLM trong khía cạnh này được cải thiện thêm.

--- TRANG 7 ---
Mô hình Đối thoại Mô tả chi tiết Lý luận phức tạp Tất cả
LLaVA 83.1 75.3 96.5 85.1
X-LLM 85.4 83.5 84.6 84.5
w/ 4M 74.8 83.7 86.5 81.9
w/ 4M no init 64.7 71.9 85.0 73.8

Bảng 1: Điểm tương đối cho các cài đặt khác nhau so với GPT-4 (chỉ văn bản) trên 30 hình ảnh được lấy mẫu ngẫu nhiên từ COCO Val 2014. Mỗi hình ảnh được liên kết với một câu hỏi ngắn, một câu hỏi chi tiết và một câu hỏi lý luận phức tạp, dẫn đến tổng cộng 90 câu hỏi. Chúng tôi nhắc ChatGPT với các câu trả lời từ đầu ra mô hình của chúng tôi và các câu trả lời từ GPT-4 (chỉ văn bản), và để nó so sánh giữa cả hai phản hồi và đưa ra đánh giá với giải thích. "w/ 4M" biểu thị rằng chúng tôi chỉ huấn luyện giao diện hình ảnh bằng 4M cặp hình ảnh-văn bản. "w/ 4M no init" biểu thị rằng chúng tôi chỉ huấn luyện giao diện hình ảnh bằng 4M cặp hình ảnh-văn bản và không sử dụng các tham số của BLIP2 đã được tiền huấn luyện.

--- TRANG 8 ---
Ví dụ đầu vào thị giác, Cung điện Cấm:
Người dùng 详细介绍一下这个地方(Giới thiệu chi tiết về nơi này)
X-LLM 北京的故宫是中国封建王朝皇家宫殿的代表，也是世界上保存最完整的古代宫殿
建筑群之一，位于北京市中心紫禁城内，是中国最重要的文化遗产之一。
故宫始建于明朝永乐年间，是中国古代宫廷建筑之精华，也是中华民族文化的瑰
宝之一。故宫占地面积约72万平方米，建筑面积约15万平方米，是世界上现存最
大、保存最完整的古代宫殿建筑群之一。
故宫建筑群布局严谨、精美，包括乾清宫、交泰殿、保和殿、午门、三大殿、太
和殿、乾清宫、养心殿、紫禁城等108个建筑和部门，有9000多间房间，拥有珍贵
的文物和艺术品，被誉为世界瑰宝。
故宫的建筑风格以黄色琉璃瓦顶、红墙黄瓦、檐角飞檐为主，色彩鲜艳，富丽堂
皇。故宫的文物丰富多彩，包括宫廷用品、艺术品、图书、典章制度、历史文献
等，被誉为中国古代文化之宝库。
故宫是中国历史和文化的重要象征，也是中国人民的珍贵遗产，吸引着众多国内
外游客前来参观和探索。
(Dịch sang tiếng Anh:) Cung điện Cấm, nằm ở trung tâm Bắc Kinh trong khu phức hợp
Hoàng cung, là đại diện cho các cung điện hoàng gia của các triều đại phong kiến ở Trung
Quốc và là một trong những quần thể kiến trúc cung điện cổ hoàn chỉnh nhất trên thế giới.
Đây là một trong những di sản văn hóa quan trọng nhất của Trung Quốc. Việc xây dựng
Cung điện Cấm bắt đầu trong triều đại Vĩnh Lạc của nhà Minh, và nó đại diện cho tinh
hoa của kiến trúc cung đình cổ Trung Quốc và là một trong những báu vật của văn hóa
Trung Quốc. Cung điện Cấm có diện tích khoảng 720.000 mét vuông, với diện tích xây
dựng khoảng 150.000 mét vuông, làm cho nó trở thành một trong những quần thể kiến
trúc cung điện cổ lớn nhất và hoàn chỉnh nhất trên thế giới. Bố cục của Cung điện Cấm
nghiêm ngặt và tinh tế, bao gồm 108 tòa nhà và các bộ phận như cung Kiền Thanh, điện
Giao Thái, điện Bảo Hòa, cửa Ngọ Môn, Ba Điện Lớn, điện Thái Hòa, cung Kiền Thanh,
điện Dưỡng Tâm, và Tử Cấm Thành. Nó có hơn 9.000 phòng và chứa các di vật văn hóa
và tác phẩm nghệ thuật quý giá, làm cho nó trở thành một báu vật thế giới. Phong cách
kiến trúc của Cung điện Cấm được đặc trưng bởi mái ngói men vàng, tường đỏ, ngói vàng,
và mái hiên bay, với màu sắc tươi sáng và vẻ tráng lệ. Các di vật văn hóa trong Cung điện
Cấm phong phú và đa dạng, bao gồm đồ dùng cung đình, tác phẩm nghệ thuật, sách,
hệ thống luật pháp và quy định, và tài liệu lịch sử, làm cho nó trở thành kho báu của văn
hóa cổ Trung Quốc. Cung điện Cấm là biểu tượng quan trọng của lịch sử và văn hóa Trung
Quốc, cũng như di sản quý giá của người dân Trung Quốc, thu hút nhiều du khách trong
và ngoài nước đến tham quan và khám phá.
Người dùng Giới thiệu chi tiết về nơi này
LLaVA [33] Hình ảnh có một cung điện lớn và đẹp màu đỏ, có vẻ như là một ngôi đền Trung Quốc
hoặc tòa nhà chính phủ, nằm trên một ngọn đồi có cỏ. Tòa nhà chính được bao quanh bởi
một số tòa nhà nhỏ hơn, tất cả đều được trang trí bằng các lá cờ màu đỏ. Những lá cờ
này hiển thị thiết kế cờ Trung Quốc, nhấn mạnh ý nghĩa văn hóa của địa điểm.
Ở phía trước, có nhiều loại xe cộ, bao gồm một vài ô tô và xe tải, đỗ gần khu vực cung
điện. Ngoài ra, có thể thấy một người đứng xa hơn từ tòa nhà chính, có thể đang ngắm
nhìn kiến trúc hoặc chờ đợi ai đó. Với kiến trúc vĩ đại và bối cảnh thanh bình, cung điện
đóng vai trò là một địa danh ấn tượng trong khu vực.
Người dùng Giới thiệu chi tiết về nơi này
MiniGPT-4 [59] Hình ảnh cho thấy một tòa nhà màu đỏ với một cổng lớn ở phía trước. Tòa nhà có mái
lớn màu đỏ và tường trắng. Có một số lá cờ trên mái của tòa nhà, bao gồm cờ Trung
Quốc. Tòa nhà được bao quanh bởi cỏ xanh và bụi cây. Bầu trời trong xanh và xanh.

Bảng 2: Ví dụ prompt thể hiện khả năng đầu vào thị giác đặc trưng Trung Quốc của X-LLM, LLaVA và MiniGPT-4. Prompt yêu cầu hiểu hình ảnh.

--- TRANG 9 ---
Ví dụ đầu vào thị giác, Honor of Kings:
Người dùng 这是什么游戏？介绍一下。(Đây là trò chơi gì? Giới thiệu cho tôi.)
X-LLM 王者荣耀是由腾讯游戏开发并发行的一款多人在线战斗竞技手游，
于2015年11月26日在中国内地正式发行，是一款竞技游戏，玩家可以选择不同
的英雄进行战斗，并与其他玩家组队竞技。王者荣耀在中国受到了广泛欢迎，并
且在全球范围内也拥有大量的玩家。
(Dịch sang tiếng Anh:) Honor of Kings là một trò chơi di động đấu trường trực tuyến
nhiều người chơi (MOBA) được phát triển và xuất bản bởi Tencent Games. Nó được
chính thức phát hành tại Trung Quốc đại lục vào ngày 26 tháng 11 năm 2015. Là một
trò chơi cạnh tranh, người chơi có thể chọn các anh hùng khác nhau để chiến đấu và
thi đấu với người chơi khác trong các đội. Honor of Kings đã được ưa chuộng rộng rãi
ở Trung Quốc và có một lượng lớn người chơi trên toàn cầu.
Người dùng Đây là trò chơi gì? Giới thiệu cho tôi.
LLaVA [33] Hình ảnh cho thấy một màn hình máy tính với một trò chơi có vẻ như kết hợp các yếu tố
từ các trò chơi phổ biến như rắn và pokemon. Trò chơi đang được chơi bằng chuột, và
màn hình đầy những hình ảnh nhỏ, có thể đại diện cho các nhân vật hoặc vật phẩm khác
nhau trong trò chơi. Trò chơi có vẻ được đặt trong một cánh đồng cỏ, nơi người chơi
gặp và tương tác với nhiều sinh vật hoặc chướng ngại vật khác nhau. Là một AI, tôi
không thể cung cấp thêm thông tin về trò chơi cụ thể hoặc quy tắc của nó, nhưng rõ
ràng là người dùng đang tích cực tham gia chơi trò chơi độc đáo này trên máy tính của họ.
Người dùng Đây là trò chơi gì? Giới thiệu cho tôi.
MiniGPT-4 [59] Đây là ảnh chụp màn hình của một trò chơi với màn hình chọn nhân vật. Trò chơi có
vẻ như là một trò chơi nhiều người chơi với nhiều nhân vật khác nhau để lựa chọn. Trò
chơi có vẻ như là một trò chơi nhập vai với các nhân vật và khả năng khác nhau.

Bảng 3: Ví dụ prompt thể hiện khả năng đầu vào thị giác đặc trưng Trung Quốc của X-LLM, LLaVA và MiniGPT-4. Prompt yêu cầu hiểu hình ảnh.

Mô hìnhAISHELL-2 VSDial-CN
test-ios test-android test-mictest-orig test-art
S S + V S S + V
Mô hình dựa trên CIF 6.0 6.4 6.4 2.8 1.7 6.0 5.3
X-LLM 8.7 9.0 9.4 3.4 4.7 / 4.3 4.7 6.8 / 6.3

Bảng 4: Hiệu suất của X-LLM trên AISHELL-2 và VSDial-CN (CER%). "S" biểu thị chỉ sử dụng giọng nói cho ASR. "S+V" biểu thị sử dụng giọng nói và hình ảnh cho MASR.

--- TRANG 10 ---
5 Thảo luận
Bài báo này thể hiện hiệu quả của X-LLM, tiêm nhiều phương thức như ngôn ngữ nước ngoài vào một mô hình ngôn ngữ lớn thông qua giao diện X2L, trao cho LLM khả năng đa phương thức mạnh mẽ. Chúng tôi thiết kế phương pháp huấn luyện ba giai đoạn để huấn luyện X-LLM, trong đó mỗi giao diện phương thức có tính độc lập cao trong hai giai đoạn đầu, tạo điều kiện cho việc huấn luyện đồng thời. Thông qua hai giai đoạn huấn luyện đầu tiên, X-LLM có thể tương tác với từng phương thức thông qua ngôn ngữ. Hơn nữa, X-LLM có thể hoàn thành các nhiệm vụ liên quan đến nhiều phương thức (như trả lời câu hỏi nói thị giác) mà không cần tinh chỉnh thêm trên các bộ dữ liệu hướng dẫn chung, nhờ vào việc tích hợp khả năng tổng quát hóa hướng dẫn của các mô hình ngôn ngữ lớn và sự thích ứng của nó với lĩnh vực đa phương thức. Việc tích hợp nhiều phương thức mà không cần huấn luyện rất tạo điều kiện cho việc mở rộng phương thức của X-LLM. Để khám phá thêm tác động của dữ liệu hướng dẫn đa phương thức chung đối với khả năng tích hợp nhiều phương thức của X-LLM, chúng tôi xây dựng một bộ dữ liệu hướng dẫn đa phương thức được tinh gọn nhưng chất lượng cao, và hiệu suất của X-LLM được cải thiện thêm sau khi tinh chỉnh trên dữ liệu này.

Dự án này vẫn đang tiến hành và hiện tại có một số hạn chế: (1) Hạn chế của mô hình ngôn ngữ. X-LLM được xây dựng trên ChatGLM chỉ với 6B tham số và kế thừa các hạn chế của nó, bao gồm nhưng không giới hạn ở khả năng lý luận không đáng tin cậy và việc bịa đặt các sự thật không tồn tại. (2) Huấn luyện không đủ cho các kết nối phương thức. Khả năng nhận thức đa phương thức của X-LLM hơi hạn chế. Chúng tôi chỉ sử dụng một lượng nhỏ bộ dữ liệu đa phương thức để kết nối bộ mã hóa đa phương thức và một mô hình ngôn ngữ lớn. Có một số hướng để khám phá thêm: (1) Quy mô dữ liệu. So với BLIP2, chúng tôi chỉ sử dụng một lượng nhỏ dữ liệu đa phương thức tiếng Trung. Chúng tôi tin rằng việc sử dụng dữ liệu tiếng Trung lớn hơn để huấn luyện có thể cải thiện đáng kể hiệu suất của mô hình bằng cách tăng phạm vi khái niệm. (2) Kết nối nhiều phương thức hơn. Chúng tôi có thể kết nối âm thanh để cho phép LLM hiểu và tương tác với âm thanh không phải lời nói. Chúng tôi cũng có thể kết nối thông tin trạng thái của các terminal khác nhau với LLM, để LLM có thể điều khiển các terminal dựa trên thông tin trạng thái của chúng. (3) Sử dụng LLM tốt hơn. Do hạn chế về tài nguyên tính toán, chúng tôi chỉ sử dụng một mô hình ngôn ngữ 6B để thí nghiệm. Có thể dự đoán rằng việc sử dụng một mô hình ngôn ngữ mạnh hơn, X-LLM sẽ có được khả năng mạnh mẽ hơn.

Tài liệu tham khảo
[1]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence
Zitnick, và Devi Parikh. Vqa: Visual question answering. Trong Proceedings of the IEEE
international conference on computer vision, trang 2425–2433, 2015. 1
[2]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, và Luo
Si. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned
generation. arXiv preprint arXiv:2004.07159, 2020. 3
[3]Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, và Hao Zheng. Aishell-1: An open-source mandarin
speech corpus and a speech recognition baseline. Trong 2017 20th conference of the oriental chapter
of the international coordinating committee on speech databases and speech I/O systems and
assessment (O-COCOSDA), trang 1–5. IEEE, 2017. 15
[4]Cheng Chen, Zhenshan Tan, Qingrong Cheng, Xin Jiang, Qun Liu, Yudong Zhu, và Xiaodong
Gu. Utc: a unified transformer with inter-task contrastive learning for visual dialog. Trong
Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, trang
18103–18112, 2022. 3
[5]Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, và Bo Xu.
Vlp: A survey on vision-language pre-training. Machine Intelligence Research, 20(1):38–56,
2023. 3
[6]Feilong Chen, Fandong Meng, Jiaming Xu, Peng Li, Bo Xu, và Jie Zhou. Dmrm: A dual-
channel multi-hop reasoning model for visual dialog. Trong Proceedings of the AAAI Conference
on Artificial Intelligence, tập 34, trang 7504–7511, 2020. 3
[7]Feilong Chen, Duzhen Zhang, Xiuyi Chen, Jing Shi, Shuang Xu, và Bo Xu. Unsupervised and
pseudo-supervised vision-language alignment in visual dialog. Trong Proceedings of the 30th ACM
International Conference on Multimedia, trang 4142–4153, 2022. 3

--- TRANG 11 ---
[8]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90 quality, 2023. 3, 4
[9]Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi
Parikh, và Dhruv Batra. Visual dialog. Trong Proceedings of the IEEE conference on computer
vision and pattern recognition, trang 326–335, 2017. 1, 3, 15
[10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin
Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.
Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. 3
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018. 3, 4
[12] Linhao Dong và Bo Xu. CIF: continuous integrate-and-fire for end-to-end speech recognition.
TrongICASSP, trang 6079–6083. IEEE, 2020. 2, 4, 5
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 3
[14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied
multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 3
[15] Jiayu Du, Xingyu Na, Xuechen Liu, và Hui Bu. Aishell-2: Transforming mandarin asr research
into industrial scale. arXiv preprint arXiv:1808.10583, 2018. 6, 15, 16
[16] Yifan Du, Zikang Liu, Junyi Li, và Wayne Xin Zhao. A survey of vision-language pre-trained
models. arXiv preprint arXiv:2202.10936, 2022. 3
[17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, và Jie Tang.
Glm: General language model pretraining with autoregressive blank infilling. Trong Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), trang 320–335, 2022. 4
[18] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, và Heng Tao Shen. Video captioning with
attention-based lstm and semantic consistency. IEEE Transactions on Multimedia, 19(9):2045–
2055, 2017. 1
[19] Ross Girshick. Fast r-cnn. Trong Proceedings of the IEEE international conference on computer
vision, trang 1440–1448, 2015. 3
[20] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao,
Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: A 100 million large-scale chinese
cross-modal pre-training benchmark. Advances in Neural Information Processing Systems,
35:26418–26431, 2022. 7, 15
[21] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,
Shibo Wang, Zhengdong Zhang, Yonghui Wu, và Ruoming Pang. Conformer: Convolution-
augmented transformer for speech recognition. Trong INTERSPEECH, trang 5036–5040. ISCA,
2020. 4, 17
[22] Minglun Han, Linhao Dong, Zhenlin Liang, Meng Cai, Shiyu Zhou, Zejun Ma, và Bo Xu.
Improving end-to-end contextual speech recognition with fine-grained contextual knowledge
selection. Trong ICASSP, trang 8532–8536. IEEE, 2022. 4
[23] Minglun Han, Linhao Dong, Shiyu Zhou, và Bo Xu. Cif-based collaborative decoding for
end-to-end contextual speech recognition. Trong ICASSP, trang 6528–6532. IEEE, 2021. 2, 4

--- TRANG 12 ---
[24] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao
Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning
perception with language models. arXiv preprint arXiv:2302.14045, 2023. 1
[25] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan,
và Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR),
54(10s):1–41, 2022. 3
[26] Wonjae Kim, Bokyung Son, và Ildoo Kim. Vilt: Vision-and-language transformer without
convolution or region supervision. Trong International Conference on Machine Learning, trang
5583–5594. PMLR, 2021. 3
[27] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, và Juan Carlos Niebles. Dense-
captioning events in videos. Trong Proceedings of the IEEE international conference on computer
vision, trang 706–715, 2017. 6, 16
[28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, và Li Fei-Fei.
Visual genome: Connecting language and vision using crowdsourced dense image annotations.
International Journal of Computer Vision, 123(1):32–73, 2017. 15
[29] Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597, 2023. 1, 2, 3, 4, 5
[30] Junnan Li, Dongxu Li, Caiming Xiong, và Steven Hoi. Blip: Bootstrapping language-
image pre-training for unified vision-language understanding and generation. Trong International
Conference on Machine Learning, trang 12888–12900. PMLR, 2022. 3
[31] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, và Kai-Wei Chang. Visualbert: A
simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557,
2019. 1, 3
[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, và C Lawrence Zitnick. Microsoft coco: Common objects in context. Trong Proceedings
of the European Conference on Computer Vision, trang 740–755. Springer, 2014. 15
[33] Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485, 2023. 3, 5, 6, 8, 9
[34] Siqu Long, Feiqi Cao, Soyeon Caren Han, và Haiqing Yang. Vision-and-language pretrained
models: A survey. arXiv preprint arXiv:2204.07356, 2022. 3
[35] Jiasen Lu, Dhruv Batra, Devi Parikh, và Stefan Lee. Vilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks. Advances in neural information
processing systems, 32, 2019. 3
[36] Vishvak Murahari, Dhruv Batra, Devi Parikh, và Abhishek Das. Large-scale pretraining
for visual dialog: A simple state-of-the-art baseline. Trong Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVIII, trang
336–352. Springer, 2020. 3
[37] OpenAI. Gpt-4 technical report, 2023. 1, 3
[38] Vicente Ordonez, Girish Kulkarni, và Tamara L Berg. Im2text: Describing images using 1
million captioned photographs. Trong Advances in Neural Information Processing Systems, trang
1143–1151, 2011. 15
[39] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin Dogus Cubuk,
và Quoc V. Le. Specaugment: A simple data augmentation method for automatic speech
recognition. Trong Interspeech, 2019. 17

--- TRANG 13 ---
[40] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, và
Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentence models. Trong Proceedings of the IEEE international conference on computer
vision, trang 2641–2649, 2015. 15
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. Trong International conference on machine learning,
trang 8748–8763. PMLR, 2021. 3, 15
[42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3
[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020. 3
[44] Yi Ren, Chenxu Hu, et al. Fastspeech 2: Fast and high-quality end-to-end text to speech. Trong
Proc. ICLR, 2021. 15
[45] Piyush Sharma, Nan Ding, Sebastian Goodman, và Radu Soricut. Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning. Trong Proceedings of
the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), trang 2556–2565, 2018. 15
[46] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, và Yueting Zhuang.
Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint
arXiv:2303.17580, 2023. 3
[47] Yao Shi, Hui Bu, et al. Aishell-3: A multi-speaker mandarin tts corpus and the baselines. arXiv
preprint arXiv:2010.11567, 2020. 15
[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3
[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017. 3, 17
[50] Oriol Vinyals, Alexander Toshev, Samy Bengio, và Dumitru Erhan. Show and tell: Lessons
learned from the 2015 mscoco image captioning challenge. IEEE transactions on pattern
analysis and machine intelligence, 39(4):652–663, 2016. 1
[51] Yue Wang, Shafiq Joty, Michael Lyu, Irwin King, Caiming Xiong, và Steven CH Hoi. Vd-bert:
A unified vision and dialog transformer with bert. Trong Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP), trang 3325–3338, 2020. 3
[52] Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, và Steve Young.
Semantically conditioned lstm-based natural language generation for spoken dialogue systems.
arXiv preprint arXiv:1508.01745, 2015. 1
[53] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, và Nan Duan.
Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint
arXiv:2303.04671, 2023. 3
[54] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei
Zhou, Guosen Lin, Yanwei Fu, et al. Ai challenger: A large-scale dataset for going deeper in
image understanding. arXiv preprint arXiv:1711.06475, 2017. 15
[55] Jun Xu, Tao Mei, Ting Yao, và Yong Rui. Msr-vtt: A large video description dataset for
bridging video and language. Trong Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016. 15

--- TRANG 14 ---
[56] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan
Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang
Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, và Jie Tang. GLM-130b: An open bilingual
pre-trained model. Trong The Eleventh International Conference on Learning Representations
(ICLR), 2023. 4
[57] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, và Lucas Beyer. Scaling vision transform-
ers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
trang 12104–12113, 2022. 4
[58] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, và Jianfeng Gao.
Unified vision-language pre-training for image captioning and vqa. Trong Proceedings of the AAAI
conference on artificial intelligence, tập 34, trang 13041–13049, 2020. 3
[59] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592, 2023. 3, 5, 6, 8, 9, 16
[60] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, và
Quoc Le. Rethinking pre-training and self-training. Advances in neural information processing
systems, 33:3833–3845, 2020. 3

--- TRANG 15 ---
A Chi tiết triển khai
A.1 Dữ liệu Huấn luyện
Loại Giai đoạn Huấn luyện Module Huấn luyện Nguồn #Mẫu
Hình ảnh-Văn bản Hai giai đoạn đầu Giao diện Hình ảnhCC3M, COCO, VG-Caps,
Flickr30k, SBU, AI-Caps, Wukong14M
Video-Văn bản Giai đoạn thứ hai Giao diện Video MSRVTT 7.7k
Giọng nói-Văn bản Giai đoạn thứ nhất Bộ Mã hóa Giọng nói, module CIF trong C-Former AISHELL-1, AISHELL-2, VSDial-CN 128k + 1M + 1.2M
Giọng nói-Văn bản Giai đoạn thứ hai Giao diện Giọng nói AISHELL-2, VSDial-CN 1M + 370K
Hướng dẫn Đa phương thức Giai đoạn thứ ba I-Adapter, V-Adapter, S-AdapterMiniGPT-4, AISHELL-2,
VSDial-CN, ActivityNet Caps10k

Bảng 5: Thống kê về các bộ dữ liệu của huấn luyện ba giai đoạn.

Chúng tôi xây dựng các bộ dữ liệu huấn luyện bằng cách kết hợp dữ liệu thị giác-ngôn ngữ, dữ liệu video-ngôn ngữ và dữ liệu ASR tự động và dữ liệu đối thoại nói thị giác. Thống kê về các bộ dữ liệu huấn luyện được liệt kê trong Bảng 5.

Dữ liệu cho hai giai đoạn huấn luyện đầu tiên. Đối với dữ liệu thị giác-ngôn ngữ, chúng tôi chủ yếu áp dụng các cặp hình ảnh-văn bản, bao gồm các cặp hình ảnh-chú thích, một vài cặp video-chú thích. Đối với hai giai đoạn huấn luyện đầu tiên của giao diện hình ảnh, chúng tôi thu thập Conceptual Caption 3M (CC3M) [45], MSCOCO image captions (COCO) [32], Visual Genome Captions (VG Captions) [28], Flickr30k [40], SBU caption (SBU) [38], AI Challenger captions (AI-Caps) [54], Wukong Captions (Wukong) [20]. Xin lưu ý rằng ngoại trừ AI Challenger Caption và Wukong là dữ liệu tiếng Trung, các bộ dữ liệu khác là tiếng Anh. Chúng tôi sử dụng dịch máy để dịch chúng sang tiếng Trung. Đối với giai đoạn huấn luyện thứ hai của giao diện video, chúng tôi dịch bộ dữ liệu MSRVTT [55] sang tiếng Trung và sử dụng phần chia huấn luyện để huấn luyện giao diện video. Lưu ý rằng bộ dữ liệu Wukong gốc bao gồm khoảng 100 triệu cặp hình ảnh. Chúng tôi sử dụng CLIP [41] đã được tiền huấn luyện để lọc ra 10M cặp với độ tương tự lớn hơn 0.45 làm dữ liệu huấn luyện. Chúng tôi sử dụng tất cả dữ liệu hình ảnh-văn bản ngoại trừ bộ dữ liệu Wukong để tạo thành tập huấn luyện 4M. Chúng tôi chỉ sử dụng 4M mà không sử dụng Q-Former đã được tiền huấn luyện từ BLIP2 để huấn luyện giao diện hình ảnh, chúng tôi thấy rằng hiệu suất của mô hình giảm do không thể liên kết một số kiến thức đằng sau hình ảnh, như không thể nhận ra bộ phim Titanic.

Đối với việc huấn luyện mô hình ASR dựa trên CIF trong giai đoạn huấn luyện thứ nhất, chúng tôi sử dụng AISHELL-1 [3], AISHELL-2 [15] và một bộ dữ liệu ASR đa phương thức tự xây dựng có tên VSDial-CN. Đối với giai đoạn huấn luyện thứ hai của giao diện giọng nói, chúng tôi sử dụng AISHELL-2 [15] và bộ dữ liệu VSDial-CN làm bộ dữ liệu huấn luyện.

AISHELL-2 [15] là một bộ dữ liệu ASR tiếng Trung Phổ thông mã nguồn mở chứa khoảng 1.000 giờ dữ liệu giọng nói. Phát ngôn giọng nói chứa 12 lĩnh vực, như từ khóa, lệnh giọng nói, nhà thông minh, lái xe tự động và sản xuất công nghiệp. 1991 người nói từ các khu vực có giọng khác nhau ở Trung Quốc đã tham gia vào việc ghi âm này. Tỷ lệ chính xác của bản ghi thủ công trên 96%, thông qua việc chú thích giọng nói chuyên nghiệp và kiểm tra chất lượng nghiêm ngặt. AISHELL-2 bao gồm ba phần kiểm tra, test-ios, test-android và test-mic có giọng nói được ghi từ iPhone, điện thoại Android và microphone, tương ứng.

VSDial-CN (Visual Spoken Dialogue for Chinese, VSDial-CN) được xây dựng từ bộ dữ liệu đối thoại thị giác (VisDial) [9], bao gồm khoảng 120.000 hình ảnh, mỗi hình ảnh có một chú thích và 10 vòng đối thoại bao gồm một câu hỏi và một câu trả lời. Đối thoại xoay quanh chặt chẽ xung quanh hình ảnh và chú thích đã cho, vì vậy có một mối tương quan mạnh giữa hình ảnh và mỗi vòng của đối thoại. Trong bối cảnh ASR đa phương thức, chúng tôi sử dụng hình ảnh làm đầu vào thị giác và chú thích làm đầu vào ngôn ngữ để giúp nhận dạng phát ngôn giọng nói của câu hỏi trong mỗi vòng đối thoại. Do đó, bộ dữ liệu VSDial-CN có khoảng 1,2 triệu mẫu huấn luyện ASR đa phương thức. Do các câu hỏi trùng lặp, số lượng phát ngôn giọng nói câu hỏi duy nhất sau khi khử trùng lặp là khoảng 370.000. Đối với VSDial-CN, đầu tiên chúng tôi dịch tất cả văn bản trong bộ dữ liệu VisDial sang tiếng Trung với một mô hình dịch công cộng4, và sau đó tổng hợp tất cả các câu hỏi thành giọng nói với mô hình FastSpeech2 [44] được huấn luyện trên AISHELL-3 [47]. Tiếp theo, chúng tôi trích xuất 2.000 cặp câu hỏi-câu trả lời từ tập kiểm tra VisDial làm tập kiểm tra VSDial có tên test-orig. Ngoài ra, theo phong cách đặt câu hỏi của bộ dữ liệu VisDial, chúng tôi tạo ra 300 câu hỏi nhân tạo dựa trên hình ảnh từ tập kiểm tra VisDial để tạo ra một tập kiểm tra VSDial có tên test-art. Cuối cùng, chúng tôi ghi lại các phát ngôn giọng nói của tất cả các câu hỏi trong các tập kiểm tra bằng điện thoại Android hoặc iPhone và lấy mẫu lại tất cả các bản ghi giọng nói về 16kHz.

Dữ liệu cho giai đoạn huấn luyện thứ ba. Như mô tả trong Phần 3.2.3, chúng tôi sử dụng ChatGPT để dịch 3,5K hướng dẫn hình ảnh-văn bản được xây dựng bởi MiniGPT-4 [59]. Sau đó, chúng tôi lựa chọn thủ công 2k dữ liệu từ AISHELL-2 [15] và viết 5 hướng dẫn khác nhau cho các nhiệm vụ ASR. Chúng tôi sử dụng ChatGPT để dịch bộ dữ liệu ActivityNet [27], sau đó lựa chọn thủ công 1k dữ liệu và viết 5 hướng dẫn khác nhau cho các nhiệm vụ video-văn bản tương ứng. Chúng tôi lựa chọn thủ công và viết lại 1k dữ liệu từ dữ liệu VSDial-CN tự xây dựng, nhằm cho phép mô hình thực hiện các nhiệm vụ tạo đối thoại dựa trên hình ảnh và giọng nói. Thêm chi tiết về dữ liệu có thể được tìm thấy trong phụ lục, bao gồm chi tiết về dữ liệu huấn luyện cho hai giai đoạn đầu và dữ liệu hướng dẫn đa phương thức.

A.2 Ví dụ về Câu hỏi Kiểm tra
Chúng tôi sử dụng tập kiểm tra được cung cấp bởi LLaVA, bao gồm 30 hình ảnh được lấy mẫu ngẫu nhiên từ COCO Val 2014. Mỗi hình ảnh được liên kết với một câu hỏi ngắn, một câu hỏi chi tiết và một câu hỏi lý luận phức tạp, dẫn đến tổng cộng 90 câu hỏi. Chúng tôi liệt kê một số câu hỏi như được hiển thị trong Bảng 6.

A.3 Chi tiết Huấn luyện
X-LLM có 7,6 tỷ tham số, bao gồm tất cả các bộ mã hóa đơn phương thức và mô hình ngôn ngữ lớn ChatGLM. Các bộ mã hóa hình ảnh và video chia sẻ một ViT-g đã được tiền huấn luyện, trong khi bộ mã hóa giọng nói bao gồm các lớp tích chập và cấu trúc conformer. Các Q-Formers trong giao diện X2L được triển khai với bert-base. Q-Former sử dụng các tham số được khởi tạo từ Q-Former giai đoạn thứ hai của BLIP2, trong khi cấu trúc transformer trong C-Former được khởi tạo với các trọng số của bert-base-chinese5 đã được tiền huấn luyện. Tất cả các adapter đều là các lớp tuyến tính. Độ dài Li của các nhúng tựa ngôn ngữ cho đặc trưng hình ảnh được đặt thành 32. Chúng tôi sử dụng optimizer AdamW với β1 = 0.9, β2 = 0.98, và weight decay là 0.05. Chúng tôi sử dụng cosine learning rate decay. Chúng tôi sử dụng hình ảnh kích thước 224 x 224, được tăng cường với random resized cropping và horizontal flipping. Số bước tích lũy gradient là 16. Tất cả các thí nghiệm sử dụng tối đa 8 GPU A100-40G. Thêm chi tiết được hiển thị trong Bảng 7.

--- TRANG 16 ---
Loại Câu hỏi
Đối thoại Màu của hai chiếc vali trong hình ảnh là gì?
Đối thoại Những vật thể chính trên bàn trong hình ảnh là gì?
Đối thoại Có bao nhiêu bánh donut trong hộp?
Đối thoại Những loại vật thể nào được bao gồm trong tác phẩm nghệ thuật?
Mô tả chi tiết Viết một mô tả chi tiết về hình ảnh đã cho.
Mô tả chi tiết Giải thích nội dung thị giác của hình ảnh một cách chi tiết.
Mô tả chi tiết Bạn có thể mô tả các đặc điểm chính của hình ảnh này cho tôi không?
Mô tả chi tiết Chuyện gì đang xảy ra trong cảnh?
Lý luận phức tạp Tác phẩm nghệ thuật có thể khuyến khích tương tác giữa mọi người như thế nào?
Lý luận phức tạp Những yếu tố nào có thể đã khiến con mèo chọn vị trí này để ngủ?
Lý luận phức tạp Lý do nào có thể khiến chiếc xe được đỗ bên lề đường?
Lý luận phức tạp Tại sao những con hươu cao cổ này có thể tập hợp gần cùng một cây?

Bảng 6: Ví dụ về các câu hỏi kiểm tra.

Giai đoạn Huấn luyện Module Huấn luyện Init. LR/Min. LR/Warmup LR Warmup Steps #Epoch Batch Size GPU Thời gian Huấn luyện
Giai đoạn thứ nhất Mô hình ASR dựa trên CIF - - 84 128 8 3 ngày
Giai đoạn thứ haiGiao diện Hình ảnh 3e-5/1e-8/1e-6 6000 5 4 8 6 ngày
Giao diện Video 1e-5/1e-8/1e-6 200 5 2 2 1 giờ
Giao diện Giọng nói 1e-4/1e-8/1e-6 6000 30 4 8 3 ngày
Giai đoạn thứ baI-Adapter
V-Adapter
S-Adapter1e-6/1e-6/1e-6 0 2 2 2 1 giờ

Bảng 7: Chi tiết huấn luyện của huấn luyện ba giai đoạn. Giao diện hình ảnh và giao diện video của mô hình cuối cùng của chúng tôi được khởi tạo từ Q-Former đã được tiền huấn luyện của BLIP2 và chúng tôi bỏ qua giai đoạn huấn luyện thứ nhất của chúng.

ghi lại các phát ngôn giọng nói của tất cả các câu hỏi trong các tập kiểm tra bằng điện thoại Android hoặc iPhone và lấy mẫu lại tất cả các bản ghi giọng nói về 16kHz.

Dữ liệu cho giai đoạn huấn luyện thứ ba. Như mô tả trong Phần 3.2.3, chúng tôi sử dụng ChatGPT để dịch 3,5K hướng dẫn hình ảnh-văn bản được xây dựng bởi MiniGPT-4 [59]. Sau đó, chúng tôi lựa chọn thủ công 2k dữ liệu từ AISHELL-2 [15] và viết 5 hướng dẫn khác nhau cho các nhiệm vụ ASR. Chúng tôi sử dụng ChatGPT để dịch bộ dữ liệu ActivityNet [27], sau đó lựa chọn thủ công 1k dữ liệu và viết 5 hướng dẫn khác nhau cho các nhiệm vụ video-văn bản tương ứng. Chúng tôi lựa chọn thủ công và viết lại 1k dữ liệu từ dữ liệu VSDial-CN tự xây dựng, nhằm cho phép mô hình thực hiện các nhiệm vụ tạo đối thoại dựa trên hình ảnh và giọng nói. Thêm chi tiết về dữ liệu có thể được tìm thấy trong phụ lục, bao gồm chi tiết về dữ liệu huấn luyện cho hai giai đoạn đầu và dữ liệu hướng dẫn đa phương thức.

--- TRANG 17 ---
Huấn luyện mô hình ASR dựa trên CIF. Đối với việc huấn luyện mô hình dựa trên CIF, chúng tôi sử dụng các cài đặt khác nhau. Chúng tôi sử dụng lịch trình learning rate trong đó chúng tôi làm ấm, giữ, sau đó giảm learning rate [39]. Thời lượng của giai đoạn làm ấm, giai đoạn giữ và giai đoạn giảm lần lượt là 24.000 bước, 36.000 bước và 120.000 bước. Learning rate của giai đoạn giữ là 3e-4. Optimizer có weight decay là 0.01. Mô hình ASR dựa trên CIF được huấn luyện trên 8 GPU A100-80G.

Cấu trúc của mô hình ASR dựa trên CIF. Bộ mã hóa giọng nói của mô hình ASR dựa trên CIF bao gồm một front-end tích chập và cấu trúc conformer [21]. Front-end tích chập là một lớp tích chập 2 chiều với 256 kênh đầu ra, kernel size 3, và stride 2. Module conformer bao gồm 18 khối với dmodel = 512, dffn = 2048, h = 8 và kernel size 31 (cho depth-wise convolution), và 2 lớp max-pooling sau khối thứ 6 và thứ 12. Module CIF chứa một lớp tích chập 1 chiều với 512 kênh đầu ra, kernel size 5 và stride 1, và một lớp fully-connected theo sau bởi một sigmoid activation. Bộ giải mã bao gồm một số lớp fully-connected và một module transformer [49] bao gồm 6 khối với dmodel = 512, dffn = 2048 và h = 8. Để hỗ trợ đầu vào đa phương thức trong bối cảnh ASR đa phương thức, khối thứ 3 và thứ 4 kết hợp các đặc trưng đầu vào thị giác thông qua cơ chế cross-attention, và khối thứ 5 và thứ 6 kết hợp các đặc trưng đầu vào ngôn ngữ thông qua cơ chế cross-attention. Các đầu vào thị giác được mã hóa bởi ViT6, và các đầu vào ngôn ngữ được mã hóa bởi bert-base-chinese đã được tiền huấn luyện. Lưu ý rằng các đặc trưng đầu vào thị giác và ngôn ngữ được đặt thành các vector zero khi không có đầu vào đa phương thức.

B Ví dụ Định tính
Chúng tôi cung cấp thêm ví dụ về đầu vào video, đầu vào giọng nói và đầu vào đa phương thức. Từ các ví dụ được đưa ra, có thể thấy rằng X-LLM có khả năng hiểu video và giọng nói ấn tượng. Như được hiển thị trong Bảng 8 và 10, X-LLM có thể cung cấp phản hồi dựa trên nội dung video và thực hiện dịch máy đa phương thức, như dịch "bank" sang tiếng Trung là "đất cao dốc" thay vì "một tổ chức tài chính" theo hình ảnh đã cho. Ngoài ra, X-LLM có thể tích hợp nhiều phương thức mà không cần tinh chỉnh hướng dẫn thêm, như có thể trả lời câu hỏi nói dựa trên hình ảnh mà không cần ASR như được hiển thị trong Bảng 9.

Ngoài ra, chúng tôi đề xuất hai ví dụ về thiết kế đạo đức, đạo lý và pháp lý, và X-LLM có thể xử lý những tình huống này một cách tốt. Trong ví dụ được hiển thị trong Bảng 11, X-LLM từ chối trả lời câu hỏi của người dùng vì hình ảnh vi phạm các luật liên quan của Trung Quốc, và nhắc nhở người dùng tuân thủ các luật liên quan. Trong ví dụ được hiển thị trong Bảng 12, X-LLM từ chối đưa ra phán đoán hoặc khuyến nghị về các vấn đề đạo đức và đạo lý và nhắc nhở người dùng tuân thủ quy định giao thông và kiến thức an toàn, lái xe cẩn thận và chú ý an toàn.

--- TRANG 18 ---
Ví dụ đầu vào video, sứa
Người dùng 给这段视频写一个标题。(Viết một tiêu đề cho video này.)
X-LLM 一只水母在游动
(Dịch sang tiếng Anh:) Một con sứa đang bơi
Người dùng 给这段视频配一段文字，100字左右。(Viết một đoạn văn bản cho video này, khoảng 100 từ.)
X-LLM 视频中展示了一只水母在蓝色的海洋中游动。水母是一种无脊椎动物，生活在冷
水中，是许多动物的食物，但同时也是一类受到过度捕捞的威胁的生物。水母可
以通过繁殖来保护种群数量，但过度繁殖也会导致它们无法生存，希望这个视频
能够引起人们对海洋生物保护的关注。
(Dịch sang tiếng Anh:) Video cho thấy một con sứa bơi trong đại dương xanh. Sứa là
động vật không xương sống sống trong nước lạnh và là nguồn thức ăn cho nhiều động
vật. Tuy nhiên, chúng cũng bị đe dọa bởi việc đánh bắt quá mức. Sứa có thể bảo vệ
quần thể của chúng bằng cách sinh sản, nhưng việc sinh sản quá mức cũng có thể dẫn
đến việc chúng không thể sinh tồn. Hy vọng video này sẽ nâng cao nhận thức về nhu
cầu bảo vệ sinh vật biển.

Bảng 8: Ví dụ prompt thể hiện khả năng đầu vào video của X-LLM. Prompt yêu cầu hiểu video.

--- TRANG 19 ---
Ví dụ trả lời câu hỏi nói thị giác
Người dùng (Nhập giọng nói) 详细描述一下这个照片(Mô tả chi tiết bức ảnh này)
X-LLM 这是一张由摄影师拍摄的照片，展示了一个被冰覆盖的沙漠和仙人掌的景象。这
个沙漠位于南美洲的安第斯山脉附近，被冰雪覆盖着。在沙漠的周围，有一些巨
大的仙人掌，它们被冰雪所覆盖，但仍然能够生存。
照片中的仙人掌看起来非常巨大，它们的叶子和茎都被冰雪所覆盖，看起来非常
壮观。在沙漠中，还有一些小型的仙人掌，它们生长在冰雪的附近，可能是因为
它们需要一些水分来维持生命。
整个景象看起来非常壮观，充满了神秘感。由于该地区的气温非常低，所以冰雪
覆盖的沙漠和仙人掌非常特别，是摄影师非常喜欢拍摄的主题之一。
(Dịch sang tiếng Anh:) Đây là một bức ảnh được chụp bởi một nhiếp ảnh gia, cho thấy
một sa mạc phủ băng và xương rồng. Sa mạc nằm gần dãy núi Andes ở Nam Mỹ và
được phủ bởi tuyết và băng. Xung quanh sa mạc là một số cây xương rồng khổng lồ,
cũng được phủ tuyết và băng nhưng vẫn có thể sống sót.
Các cây xương rồng trong ảnh trông rất khổng lồ, với lá và thân được phủ tuyết và
băng, làm cho chúng trông rất nspectacular. Trong sa mạc, cũng có một số cây xương
rồng nhỏ mọc gần tuyết và băng, có lẽ vì chúng cần một ít độ ẩm để sinh tồn.
Toàn bộ cảnh tượng trông rất spectacular và bí ẩn. Do nhiệt độ cực thấp trong khu
vực, sa mạc và xương rồng phủ tuyết và băng rất đặc biệt và là một trong những chủ
đề yêu thích của các nhiếp ảnh gia.
Người dùng (Nhập giọng nói) 这个照片在现实世界中常见吗(Bức ảnh này có phổ biến trong thế giới thực không?)
X-LLM 这个照片在现实中并不常见，因为该地区的气温非常低，冰雪覆盖的沙漠和仙人
掌非常罕见。安第斯山脉是南美洲最长的山脉，其大部分地区的气温都很低，大
部分地区都没有被冰雪覆盖。
不过，该地区的自然环境非常美丽和独特，因此摄影师们非常喜欢前往该地区拍
摄各种美丽的照片。如果你有前往安第斯山脉旅行的计划，可能会有机会看到这
种独特的景象。
(Dịch sang tiếng Anh:) Bức ảnh này không phổ biến trong thực tế vì nhiệt độ trong
khu vực rất thấp, và việc thấy sa mạc phủ băng tuyết và xương rồng rất hiếm. Dãy núi
Andes là dãy núi dài nhất ở Nam Mỹ, và phần lớn khu vực có nhiệt độ thấp với hầu
hết các khu vực không được phủ băng và tuyết.
Tuy nhiên, môi trường tự nhiên trong khu vực rất đẹp và độc đáo, đó là lý do tại sao
các nhiếp ảnh gia thích đến khu vực này để chụp các bức ảnh tuyệt đẹp khác nhau. Nếu
bạn có kế hoạch du lịch đến dãy núi Andes, bạn có thể có cơ hội thấy cảnh tượng độc
đáo này.

Bảng 9: Ví dụ về trả lời câu hỏi nói thị giác. Prompt yêu cầu cả hiểu văn bản và hình ảnh và hiểu giọng nói.

--- TRANG 20 ---
Ví dụ dịch máy đa phương thức
Người dùng 结合图片将下面文本翻译成中文：A medium sized child jumps off of a dusty bank
(Dịch văn bản sau sang tiếng Trung theo hình ảnh: A medium sized child jumps off of a dusty bank)
X-LLM 一个中等大小的孩子在灰尘较多的河岸上跳下来。

Bảng 10: Ví dụ về dịch máy đa phương thức. Prompt yêu cầu cả hiểu văn bản và hình ảnh. "bank" trong câu nguồn chỉ đất dốc cao thay vì một tổ chức tài chính.

Ví dụ liên quan đến luật pháp
(Các từ ở đầu hình ảnh chứa thông tin về cờ bạc và lừa đảo.)
Người dùng 他们在干什么？
(Dịch sang tiếng Anh:) Họ đang làm gì?
X-LLM很抱歉，我无法回答这个问题，因为图片中的内容违反了中国法律和道德规范，
涉及到政治敏感话题和不良信息。作为一个人工智能助手，我的目标是提供客
观、中立和有用的信息，不会提供任何违反这些准则的内容。如果您有任何其他
问题，我会尽力回答。
(Dịch sang tiếng Anh:) Xin lỗi, tôi không thể trả lời câu hỏi này vì nội dung trong hình
ảnh vi phạm luật pháp và chuẩn mực đạo đức của Trung Quốc, liên quan đến các chủ đề
nhạy cảm chính trị và thông tin có hại. Là một trợ lý trí tuệ nhân tạo, mục tiêu của tôi
là cung cấp thông tin khách quan, trung lập và hữu ích mà không cung cấp bất kỳ nội
dung nào vi phạm những hướng dẫn này. Nếu bạn có bất kỳ câu hỏi nào khác, tôi sẽ
cố gắng hết sức để trả lời.

Bảng 11: Ví dụ liên quan đến luật pháp.

--- TRANG 21 ---
Ví dụ liên quan đến đạo đức và ethics
Người dùng 如图所示，老人和小孩开车该撞哪一个
(Dịch sang tiếng Anh:) Như được hiển thị trong hình, người già và trẻ em lái xe nên va chạm với cái nào
X-LLM作为一名AI语言模型，我不会对道德伦理问题进行判断或推荐但是，根据交通法
规和安全常识，在道路上发生交通事故时，应该优先保护弱势群体，包括老年人
和小孩。因此，如果老人和小孩在同一车道上，驾驶员应该尽可能游免直接撞击
其中任何一人，而是选择游让其他车辆或障碍物，以确保自己和其他道路用户的
安全。在任何情况下，驾驶员都应该严格遵守交通规则，谨慎驾驶，注意安全。
(Dịch sang tiếng Anh:) Là một mô hình ngôn ngữ AI, tôi sẽ không đưa ra phán đoán hoặc
khuyến nghị về các vấn đề đạo đức và ethics. Tuy nhiên, theo quy định giao thông và
kiến thức an toàn, khi xảy ra tai nạn giao thông trên đường, ưu tiên nên được dành cho
việc bảo vệ các nhóm dễ bị tổn thương, bao gồm người già và trẻ em. Do đó, nếu người
già và trẻ em đang ở cùng làn đường, tài xế nên cố gắng tránh va chạm trực tiếp với
bất kỳ ai trong số họ càng nhiều càng tốt, và thay vào đó chọn nhường đường cho các
xe khác hoặc chướng ngại vật để đảm bảo an toàn cho bản thân và người sử dụng đường
khác. Trong mọi trường hợp, tài xế nên tuân thủ nghiêm ngặt các quy tắc giao thông,
lái xe cẩn thận và chú ý an toàn

Bảng 12: Ví dụ liên quan đến đạo đức và ethics.
