# 2209.11035.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2209.11035.pdf
# File size: 222140 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MonoByte: A Pool of Monolingual Byte-level Language Models
Hugo Abonizio
FEEC, UNICAMP, Brazil
NeuralMind, Brazil
hugo.abonizio@gmail.comLeandro Rodrigues de Souza
FEEC, UNICAMP, Brazil
l231250@g.unicamp.br
Roberto Lotufo
FEEC, UNICAMP, Brazil
NeuralMind, Brazil
lotufo@unicamp.brRodrigo Nogueira
FEEC, UNICAMP, Brazil
NeuralMind, Brazil
rfn@unicamp.br
Abstract
The zero-shot cross-lingual ability of models
pretrained on multilingual and even monolin-
gual corpora has spurred many hypotheses to
explain this intriguing empirical result. How-
ever, due to the costs of pretraining, most re-
search uses public models whose pretraining
methodology, such as the choice of tokeniza-
tion, corpus size, and computational budget,
might differ drastically. When researchers pre-
train their own models, they often do so under
a constrained budget, and the resulting models
might underperform signiﬁcantly compared to
SOTA models. These experimental differences
led to various inconsistent conclusions about
the nature of the cross-lingual ability of these
models. To help further research on the topic,
we released 10 monolingual byte-level mod-
els1rigorously pretrained under the same con-
ﬁguration with a large compute budget (equiv-
alent to 420 days on a V100) and corpora that
are 4 times larger than the original BERT’s.
Because they are tokenizer-free, the problem
of unseen token embeddings is eliminated,
thus allowing researchers to try a wider range
of cross-lingual experiments in languages with
different scripts. Additionally, we release two
models pretrained on non-natural language
texts that can be used in sanity-check exper-
iments. Experiments on QA and NLI tasks
show that our monolingual models achieve
competitive performance to the multilingual
one, and hence can be served to strengthen our
understanding of cross-lingual transferability
in language models.
1 Introduction
Shortly after the publication of BERT (Devlin et al.,
2019), researchers showed that multilingual models
1https://huggingface.co/monobytepretrained on the masked language modeling objec-
tive can achieve remarkable zero-shot cross-lingual
performance on various NLP tasks (i.e., a multilin-
gual model ﬁnetuned on a high-resource language
and directly evaluated in other languages) (Con-
neau et al., 2019; Hu et al., 2020).
These empirical results triggered a wave of re-
search that aimed to explain this behavior. Pires
et al. (2019) raised the “anchor tokens” hypothe-
sis, i.e., that tokens shared between two languages
act as a reference point so models can learn sim-
ilar concepts. Artetxe et al. (2020) and Conneau
et al. (2020) questioned these ﬁndings and pointed
out that, even without a shared vocabulary, models
achieve great cross-lingual transfer performance by
leveraging shared parameters only.
Perhaps even more surprising is the perfor-
mance of monolingual models in a cross-lingual
setting. For instance, models pretrained and ﬁne-
tuned only on English can perform well in French
tasks (Oladipo et al., 2022). de Souza et al. (2021)
also show that monolingual models ﬁnetuned on a
foreign language (i.e., a language that differs from
pretraining) achieve comparable results to models
ﬁnetuned on their language.
Recent work investigates properties of the pre-
training corpora that contribute to the model’s per-
formance on natural language tasks. Models are
pretrained on non-natural language corpora, such
as code, music, proteins, and artiﬁcial languages
and their performances are compared with natu-
ral language models (Papadimitriou and Jurafsky,
2020; Chiang and yi Lee, 2020; Lu et al., 2021;
Ri and Tsuruoka, 2022). Evidence shows that pre-
training on a corpus that contains artiﬁcial recursive
or hierarchical structure between tokens results in
a similar performance when compared to modelsarXiv:2209.11035v2  [cs.CL]  27 Sep 2022

--- PAGE 2 ---
pretrained on a natural language.
However, an outstanding problem is that these
experiments are made either 1) using models pre-
trained by different research groups, whose pre-
training conﬁguration differs widely, or 2) using
models pretrained under a compute budget or pre-
training corpus that are orders of magnitude smaller
than the ones used by SOTA models. For instance,
Chiang and yi Lee (2020) pretrain their models
on a dataset 200 times smaller than BERT’s. This
is problematic because certain skills are learned
only when corpus size and training budget are large
enough (Zhang et al., 2020).
These differences in the pretraining methodol-
ogy and undertrained models make it difﬁcult to
draw a conclusion about the nature of the cross-
lingual ability of such models. Additionally, since
most models use subword tokenization, it is difﬁ-
cult to experiment with languages that do not have
a signiﬁcant subword vocabulary overlap. For ex-
ample, if a model is pretrained and ﬁnetuned on
English, it cannot be tested in Chinese because
there is very little token overlap in their vocabu-
laries, and hence, Chinese token embeddings are
not learned. This is a problem even for languages
with the same script. For example, many Spanish
embeddings are not learned in an English-only pre-
training, and hence it is difﬁcult to tell whether a
model’s inability to learn cross-lingual representa-
tions is due only to tokenization issues (Rust et al.,
2021).
Following Bommasani et al. (2021), who advo-
cate for the release of pretraining models, corpus,
and script as a way to strengthen the ﬁeld, we re-
lease byte-level models pretrained on large corpora
from the same domain and using exactly the same
training setup and compute budget. Because they
rely only on bytes to represent strings, they can
be used to compare languages that use different
scripts.
Each model takes approximately 210 hours
of pretraining, resulting in more than three
months of TPU computing. The models are
available at https://huggingface.co/monobyte and
the code used for ﬁnetuning can be found at
https://github.com/lersouza/lang-agnostic.
2 Related work
In the last years, many monolingual versions of
BERT (Devlin et al., 2019; Souza et al., 2020; Chan
et al., 2020; Martin et al., 2020; Antoun et al., 2020;Lee et al., 2021; Nguyen and Tuan Nguyen, 2020;
Cañete et al., 2020; Bhattacharjee et al., 2021) and
T5 (Raffel et al., 2020; Carmo et al., 2020; Sarti
and Nissim, 2022) have been released. The authors
often claim that these versions outperform multilin-
gual models in their languages. Rust et al. (2021)
showed that the tokenizer plays a critical role in
achieving those results. More recently, Xue et al.
(2021a) released the ByT5 model, which would
overcome this issue by leveraging a byte-level vo-
cabulary. This model is pretrained on several lan-
guages and achieves great results when compared
to mT5 (Xue et al., 2021b). However, no mono-
lingual version of ByT5 has been released, which
makes it difﬁcult to conduct further investigation
on the model’s pretraining and cross-lingual perfor-
mance that is not impacted by the tokenization in
different languages.
Current research on the properties of pretraining
often relies on monolingual models or models pre-
trained on artiﬁcial languages. Researchers often
rely on models released by other groups (de Souza
et al., 2021) or pretrain models in a very controlled
setup (Artetxe et al., 2020; Papadimitriou and Ju-
rafsky, 2020; Fujinuma et al., 2022). In both cases,
it is not clear if the results suffer from these differ-
ent methodologies or undertrained models. Blevins
and Zettlemoyer (2022), for instance, suggests that
language contamination (i.e., a monolingual cor-
pus composed of sentences in more languages) is
the reason for monolingual zero-shot effectiveness.
The same happens with models pretrained on non-
natural language corpora (Chiang and yi Lee, 2020;
Lu et al., 2021; Ri and Tsuruoka, 2022), where the
corpus size differs from the ones trained on natu-
ral language, which might affect the conclusions
drawn from the experiments. We hope to bridge
this gap by releasing the set of monolingual models
presented in this paper.
3 Pretraining
To train our monolingual models, we used
mC4 (Xue et al., 2021b), the same corpus used
to train mT5 and ByT5, which comprises 101 natu-
ral languages extracted from the Common Crawl
web scrape. For each model, we selected only the
documents written in the speciﬁc language, which
was originally identiﬁed using cld3 by Xue et al.
(2021b). The mC4 is a large corpus with different
language distributions, with some being more rep-
resented than others. Thus, we trimmed each pre-

--- PAGE 3 ---
training corpus in approximately 65 billion UTF-8
bytes for all languages. The only exception was
Bengali, which was trimmed on 32 billion bytes,
corresponding to its total size.
The pretraining was conducted on a TPU VM
v3-8 using Flax (Heek et al., 2020) library and
the pretraining script for T5-like span-masked lan-
guage modeling available on HuggingFace Trans-
formers library.2We chose the smaller architecture
of ByT5, with 300 million parameters, and used
similar hyper-parameters as reported on ByT5 (Xue
et al., 2021a) and mT5 (Xue et al., 2021b) papers.
We did not experiment with larger models due
to their computational cost. We set the sequence
length to 1024 tokens (UTF-8 bytes) and train for
1 million steps with a batch of 216bytes, resulting
in approximately 65 billion bytes. Since we could
only ﬁt a batch of 8 examples per device, we used
gradient accumulation of 128 steps to achieve a
larger batch size. The learning rate starts at 10 3
and decays linearly throughout the pretraining.
In comparison with the original ByT5, our
models are pretrained on a 16 times smaller cor-
pus. However, our pretraining corpus is 4 times
larger than BERT’s, which was trained on 3,300M
words (Devlin et al., 2019) — i.e., approximately
16 billion characters, considering an average of 5
characters per word in English (Bochkarev et al.,
2015). In comparison with GPT-2, our corpus is
1.6 times larger (Radford et al., 2019).
Compared with other monolingual models re-
search, Rust et al. (2021) pretrained on a corpus
of approximately 77GB, according to the reported
number of steps and tokens per batch. Ri and Tsu-
ruoka (2022) pretrained on a corpus of 1.2GB of
artiﬁcially generated text. A more extensive com-
parison to other work is presented in Table 1 and
demonstrates that our pretraining is comparable to
SOTA monolingual models and much larger than
corpora used in the majority of monolingual stud-
ies.
To sanity-check and validate the performance of
our pretrained models, we compared their results
with different baselines proposed in the literature.
The ﬁrst synthetic pretraining strategy, called nest-
ing parentheses, was proposed by Papadimitriou
and Jurafsky (2020), which recursively generates
random symbols respecting a hierarchical structure.
This was proposed to evaluate how this recursion
2https://github.com/huggingface/
transformers/blob/main/examples/flax/
language-modeling/run_t5_mlm_flax.pyModel name Language Size
PTT5 (Carmo et al., 2020) pt 15
BERT (Devlin et al., 2019) en 16
BERTimbau (Souza et al., 2020) pt 17
PhoBERT (Nguyen and Tuan Nguyen, 2020) vi 20
AraBERT (Antoun et al., 2020) ar 24
BanglaBERT (Bhattacharjee et al., 2021) bn 35
KoreALBERT (Lee et al., 2021) ko 43
North ByT5 (Kummervold, 2022) no 45
CamemBERT (Martin et al., 2020) fr 138
IT5 (Sarti and Nissim, 2022) it 215
Chiang and yi Lee, 2020 en, artiﬁcial 0.08
Papadimitriou and Jurafsky, 2020 many, artif. 0.5
Deshpande et al., 2021 many 0.5
Ri and Tsuruoka, 2022 many, artif. 1.2
Rust et al., 2021 many 77
MonoByte (ours, each model) many 65
Table 1: Comparison of pretraining corpus sizes in GB
of monolingual models. Models which does not explic-
itly report the corpus size were estimated based on the
reported amount of tokens.
structure of the pretraining corpus transfers into
modeling real languages. Their work showed that
the models pretrained on the synthetic hierarchical
corpus were able to predict human language far
better than other baselines (e.g., random, music,
and code), with a perplexity comparable to real
languages.
We used the code released by the authors, with
the vocabulary being the 50,000 most frequent
words based on the ﬁrst one million documents
of the Spanish mC4 corpus, and their sampling
probability based on the Zipf distribution over the
same Spanish corpus.3We generated examples un-
til reaching the same size as our natural languages
corpora sizes to make a fair comparison.
The second pretraining baseline was proposed
by Krishna et al. (2021), dubbed nonsense, where
the authors explored the knowledge transfer hypoth-
esis of the downstream performance improvement
of transfer learning. The authors pretrained on doc-
uments consisting of random n-grams and achieve
similar performance on summarization over differ-
ent datasets when compared to pretraining on real
language. We generated examples using the orig-
inal code4to match the same corpus size of our
natural language models.
3https://github.com/toizzy/
tilt-transfer/
4https://github.com/acmi-lab/
pretraining-with-nonsense/

--- PAGE 4 ---
de en es pt ru vi zh avg.
Nonsense 33.33 33.55 33.27 50.00 33.33 33.57 34.19 35.89
Hierarchical 66.01 69.32 68.04 65.65 62.12 64.49 62.83 65.49
Monolingual 70.85 76.76 75.07 83.40 67.39 70.37 69.91 73.39
Multilingual 76.02 80.55 78.39 67.73 73.58 73.01 73.39 74.67
(multi - mono ) 5.16 3.80 3.32 -15.67 6.19 2.64 3.48 1.28
Table 2: Results (accuracy) for Natural Language Inference in each language.
ar bn en ko pt ru avg.
Nonsense 0.22 0.00 0.45 0.00 0.26 0.27 0.20
Hierarchical 67.23 0.38 28.03 9.99 3.63 32.85 23.68
Monolingual 78.17 53.89 60.15 51.17 64.88 58.26 61.09
Multilingual 81.81 70.28 68.51 54.70 42.37 72.97 65.11
(multi - mono ) 3.65 16.39 8.36 3.54 -22.51 14.71 4.02
Table 3: Results (F1 Score) for Question Answering in each language.
4 Experiments
Our models are evaluated on two downstream tasks:
Natural Language Inference (NLI) and Question
Answering (QA). We use a similar setting as the
In-language model , described by Hu et al. (2020),
where the model is ﬁnetuned and evaluated in the
subset of the task that corresponds to its pretrain-
ing language. The following subsections provide
details about the datasets used and the results. The
monolingual models are compared to the ByT5
Small checkpoint. The latter is expected to per-
form better since it is pretrained on a much larger
set of a more diverse corpus, as investigated by Fu-
jinuma et al. (2022). For additional information on
the ﬁnetuning procedure, please refer to Appendix
A.
Natural Language Inference . The XNLI5
dataset (Conneau et al., 2018) is used for ﬁnetuning
and evaluating our German, English, Spanish, Rus-
sian, Vietnamese, and Chinese models. For the Por-
tuguese model, we select the Recognizing Textual
Entailment (RTE) task of the ASSIN2 dataset (Real
et al., 2020). Performance is measured by using
the accuracy metric.
Results are reported in Table 2. Compared to the
multilingual ByT5, our models achieve competitive
performance. The difference in accuracy is about
1.28 on average. The results in Russian represent
the largest gap (6.19 points), while those in Viet-
namese represent the smallest (2.64 points). Non-
5We use the machine-translated version of XNLI
for training in different languages. Available at
https://dl.fbaipublicﬁles.com/XNLI/XNLI-MT-1.0.zipsense achieves a near random performance, while
Hierarchical approaches the models pretrained on
natural language, as also evidenced by Papadim-
itriou and Jurafsky (2020) and Chiang and yi Lee
(2020).
Question Answering . The gold passage version of
the TydiQA (Clark et al., 2020) dataset ( TydiQA-
GoldP ) is selected for ﬁnetuning and evaluating the
Arabic, Bengali, English, Korean and Russian mod-
els. For ﬁnetuning the Portuguese model, we use
the FaQuAD (Sayama et al., 2019). Performance
is measured by the F1 Score metric as described by
Rajpurkar et al. (2016).
Results are reported in Table 3. For Portuguese,
our model outperforms the Multilingual checkpoint
by 22.51 points, while, for Bengali, it stays 16.39
points behind. Our monolingual models stay, on
average, 4 points behind the multilingual model
released by Google. Looking at the difference be-
tween monolingual and multilingual models for ev-
ery language, we see a higher variation. Nonsense
and Hierarchical results are very distant. Ques-
tion answering is a more difﬁcult task (Vania et al.,
2021) and a good benchmark for model quality. We
hypothesize that this difference in task complexity
requires more from the model. Pretraining with
token structure emphasis only may not provide the
required knowledge for good performance.
5 Conclusion
In this work, we introduced a pool of 10 tokenizer-
free language models pretrained on large monolin-
gual corpora. We demonstrated that our monolin-

--- PAGE 5 ---
gual models can achieve competitive results to the
multilingual ByT5, although having a smaller and
less diverse pretraining — single language, com-
pared to all languages concatenated.
The main goal of our work was to release a set of
monolingual models that carefully follow the same
pretraining methodology to support the research of
monolingual language models, pretraining proper-
ties, and cross-linguality. We hope that by releasing
our models, we bridge this gap of having a more
controlled and reproducible setup for rigorous ex-
periments.
Acknowledgments
This research was partially funded by grants
2020/09753-5 and 2022/01640-2 from Fundação
de Amparo à Pesquisa do Estado de São Paulo
(FAPESP). We also would like to thank Google
Cloud for credits to support this work.
References
Wissam Antoun, Fady Baly, and Hazem Hajj. 2020.
AraBERT: Transformer-based model for Arabic lan-
guage understanding. In Proceedings of the 4th
Workshop on Open-Source Arabic Corpora and Pro-
cessing Tools, with a Shared Task on Offensive Lan-
guage Detection , pages 9–15, Marseille, France. Eu-
ropean Language Resource Association.
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
2020. On the cross-lingual transferability of mono-
lingual representations. In Proceedings of the 58th
Annual Meeting of the Association for Computa-
tional Linguistics , pages 4623–4637, Online. Asso-
ciation for Computational Linguistics.
Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin
Ahmad, Kazi Samin, Md Saiful Islam, Anindya
Iqbal, M. Sohel Rahman, and Rifat Shahriyar.
2021. Banglabert: Language model pretraining and
benchmarks for low-resource language understand-
ing evaluation in bangla.
Terra Blevins and Luke Zettlemoyer. 2022. Language
contamination explains the cross-lingual capabilities
of english pretrained models.
Vladimir V Bochkarev, Anna V Shevlyakova, and
Valery D Solovyev. 2015. The average word length
dynamics as an indicator of cultural changes in soci-
ety.Social Evolution and History , 14(2):153–175.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli,
Russ Altman, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine
Bosselut, Emma Brunskill, Erik Brynjolfsson, Shya-
mal Buch, Dallas Card, Rodrigo Castellon, Ni-
ladri S. Chatterji, Annie S. Chen, Kathleen Creel,Jared Quincy Davis, Dorottya Demszky, Chris Don-
ahue, Moussa Doumbouya, Esin Durmus, Stefano
Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-
Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie,
Karan Goel, Noah D. Goodman, Shelby Grossman,
Neel Guha, Tatsunori Hashimoto, Peter Henderson,
John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,
Jing Huang, Thomas Icard, Saahil Jain, Dan Juraf-
sky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff
Keeling, Fereshte Khani, Omar Khattab, Pang Wei
Koh, Mark S. Krass, Ranjay Krishna, Rohith Kudi-
tipudi, and et al. 2021. On the opportunities and
risks of foundation models. CoRR , abs/2108.07258.
Diedre Carmo, Marcos Piau, Israel Campiotti, Rodrigo
Nogueira, and Roberto Lotufo. 2020. Ptt5: Pre-
training and validating the t5 model on brazilian por-
tuguese data.
José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-
Hui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-
ish pre-trained bert model and evaluation data. In
PML4DC at ICLR 2020 .
Branden Chan, Stefan Schweter, and Timo Möller.
2020. German’s next language model. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics , pages 6788–6796, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.
Cheng-Han Chiang and Hung yi Lee. 2020. Pre-
training a language model without human language.
Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan
Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and
Jennimaria Palomaki. 2020. Tydi qa: A benchmark
for information-seeking question answering in typo-
logically diverse languages. Transactions of the As-
sociation for Computational Linguistics .
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale. CoRR ,
abs/1911.02116.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-
ina Williams, Samuel R. Bowman, Holger Schwenk,
and Veselin Stoyanov. 2018. "XNLI: Evaluating
Cross-lingual Sentence Representations". In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing . Association
for Computational Linguistics.
Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Emerging cross-
lingual structure in pretrained language models. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 6022–
6034, Online. Association for Computational Lin-
guistics.

--- PAGE 6 ---
Leandro Rodrigues de Souza, Rodrigo Nogueira, and
Roberto Lotufo. 2021. On the ability of monolin-
gual models to learn language-agnostic representa-
tions.
Ameet Deshpande, Partha Talukdar, and Karthik
Narasimhan. 2021. When is bert multilingual? iso-
lating crucial ingredients for cross-lingual transfer.
arXiv preprint arXiv:2110.14782 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Yoshinari Fujinuma, Jordan Boyd-Graber, and Katha-
rina Kann. 2022. Match the script, adapt if multilin-
gual: Analyzing the effect of multilingual pretrain-
ing on cross-lingual transferability.
Jonathan Heek, Anselm Levskaya, Avital Oliver, Mar-
vin Ritter, Bertrand Rondepierre, Andreas Steiner,
and Marc van Zee. 2020. Flax: A neural network
library and ecosystem for JAX.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. XTREME: A Massively Multilingual Multi-
task Benchmark for Evaluating Cross-lingual Gener-
alisation. In International Conference on Machine
Learning , pages 4411–4421. PMLR.
Kundan Krishna, Jeffrey Bigham, and Zachary C. Lip-
ton. 2021. Does pretraining for summarization re-
quire knowledge transfer? In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2021 ,
pages 3178–3189, Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Per E Kummervold. 2022. North T5. https://
github.com/peregilk/north-t5 .
Hyunjae Lee, Jaewoong Yoon, Bonggyu Hwang,
Seongho Joe, Seungjai Min, and Youngjune Gwon.
2021. Korealbert: Pretraining a lite bert model for
korean language understanding.
Kevin Lu, Pieter Abbeel, Aditya Grover, Igor Mor-
datch, and Google Brain. 2021. Pretrained trans-
formers as universal computation engines.
Louis Martin, Benjamin Muller, Pedro Javier Or-
tiz Suárez, Yoann Dupont, Laurent Romary, Éric
de la Clergerie, Djamé Seddah, and Benoît Sagot.
2020. CamemBERT: a tasty French language model.
InProceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages
7203–7219, Online. Association for Computational
Linguistics.Dat Quoc Nguyen and Anh Tuan Nguyen. 2020.
PhoBERT: Pre-trained language models for Viet-
namese. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pages 1037–1042,
Online. Association for Computational Linguistics.
Akintunde Oladipo, Odunayo Ogundepo, Kelechi
Ogueji, and Jimmy Lin. 2022. An exploration of
vocabulary size and transfer effects in multilingual
language models for african languages. In 3rd Work-
shop on African Natural Language Processing .
Isabel Papadimitriou and Dan Jurafsky. 2020. Learn-
ing Music Helps You Read: Using transfer to study
linguistic structure in language models. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
6829–6839, Online. Association for Computational
Linguistics.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How multilingual is Multilingual BERT? ACL 2019
- 57th Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference ,
pages 4996–5001.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog , 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1–67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ Questions
for Machine Comprehension of Text.
Livy Real, Erick Fonseca, and Hugo Gonçalo Oliveira.
2020. The assin 2 shared task: A quick overview.
InComputational Processing of the Portuguese Lan-
guage , pages 406–412, Cham. Springer Interna-
tional Publishing.
Ryokan Ri and Yoshimasa Tsuruoka. 2022. Pretrain-
ing with artiﬁcial language: Studying transferable
knowledge in language models.
Phillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian
Ruder, and Iryna Gurevych. 2021. How good is
your tokenizer? on the monolingual performance of
multilingual language models. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers) , pages 3118–3135, Online. As-
sociation for Computational Linguistics.
Gabriele Sarti and Malvina Nissim. 2022. It5: Large-
scale text-to-text pretraining for italian language un-
derstanding and generation.

--- PAGE 7 ---
H. F. Sayama, A. V . Araujo, and E. R. Fernandes.
2019. FaQuAD: Reading Comprehension Dataset
in the Domain of Brazilian Higher Education. In
2019 8th Brazilian Conference on Intelligent Sys-
tems (BRACIS) , pages 443–448.
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
Fábio Souza, Rodrigo Nogueira, and Roberto Lotufo.
2020. Bertimbau: Pretrained bert models for brazil-
ian portuguese. Lecture Notes in Computer Science
(including subseries Lecture Notes in Artiﬁcial Intel-
ligence and Lecture Notes in Bioinformatics) , 12319
LNAI:403–417.
Clara Vania, Phu Mon Htut, William Huang, Dhara
Mungra, Richard Yuanzhe Pang, Jason Phang,
Haokun Liu, Kyunghyun Cho, and Samuel R. Bow-
man. 2021. Comparing test sets with item response
theory. ACL-IJCNLP 2021 - 59th Annual Meeting of
the Association for Computational Linguistics and
the 11th International Joint Conference on Natural
Language Processing, Proceedings of the Confer-
ence, pages 1141–1158.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. 2021a. Byt5: Towards a token-
free future with pre-trained byte-to-byte models.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021b. mT5: A massively
multilingual pre-trained text-to-text transformer. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 483–498, Online. Association for Computa-
tional Linguistics.
Yian Zhang, Alex Warstadt, Haau-Sing Li, and
Samuel R. Bowman. 2020. When do you need bil-
lions of words of pretraining data?

--- PAGE 8 ---
A Finetuning Details
The experiments were carried out in a single
NVIDIA A100 80GB GPU. We use the Adafactor
optimizer (Shazeer and Stern, 2018) with a con-
stant learning rate of 10 4. The models were ﬁne-
tuned using three different seeds, and we report an
average of the results. We have chosen hyperpa-
rameters based on preliminary experiments with
the ByT5 Small checkpoint6on both XNLI and Ty-
diQA datasets. We do not perform any exhaustive
hyperparameter search, and use the same settings
(per-task) for all models.
Natural Language Inference . For all NLI exper-
iments, we use a batch size of 16, accumulating
gradients for 4 steps. The maximum input length
was 1024, trimmed by batch. We train the model to
output the class identiﬁer (a number). We train for
10 epochs and evaluate every 0.2 epoch. We also
perform early stopping with patience of 5 evalua-
tions and select the best model on the validation set
of each task. The results are reported against the
test set of each dataset.
Question Answering . For QA, the selected batch
size is 6, accumulating gradients for 4 steps. The
maximum input length was 2048 (question and
context concatenated), trimmed by batch. We train
the model to output an answer to the question with
a maximum length of 768 bytes. We train for 10
epochs and evaluate at the end of each epoch. We
also perform early stopping with patience of 3 and
select the best model on the validation set of each
task.
6Available at: https://huggingface.co/
google/byt5-small
