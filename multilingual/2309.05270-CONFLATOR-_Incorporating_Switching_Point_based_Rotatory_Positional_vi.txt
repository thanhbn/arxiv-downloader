# 2309.05270.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2309.05270.pdf
# Kích thước tệp: 730706 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CONFLATOR: Tích hợp Mã hóa Vị trí Xoay dựa trên Điểm Chuyển đổi cho Mô hình Ngôn ngữ Trộn lẫn Mã
Mohsin Ali1Sai Teja Kandukuri2Neeharika Gupta3Parth Patwa4
Anubhab Chatterjee3Vinija Jain5Aman Chadha5,6*Amitava Das7
1Đại học Bang San Diego, Mỹ2Đại học Bang San Jose, Mỹ
3Wipro AI Labs, Ấn Độ4Đại học California Los Angeles, Mỹ
5Đại học Stanford, Mỹ6Amazon AI, Mỹ7Đại học South Carolina, Mỹ
1mmohammed5956@sdsu.edu2saiteja.kandukuri@sjsu.edu
7amitava@mailbox.sc.edu
Tóm tắt
Việc trộn lẫn hai hoặc nhiều ngôn ngữ được gọi là
Trộn lẫn Mã (CM). CM là một chuẩn mực xã hội trong
các xã hội đa ngôn ngữ. Các Mô hình Ngôn ngữ Thần
kinh (NLM) như transformer đã hiệu quả trong nhiều
tác vụ NLP. Tuy nhiên, NLM cho CM là một lĩnh vực
chưa được khám phá đầy đủ. Mặc dù transformer có
khả năng và mạnh mẽ, chúng không thể luôn mã hóa
thông tin vị trí vì chúng không phải là lặp lại. Do đó,
để làm phong phú thông tin từ và tích hợp thông tin
vị trí, mã hóa vị trí được định nghĩa. Chúng tôi giả
thuyết rằng các Điểm Chuyển đổi (SP), tức là các
điểm giao nhau trong văn bản nơi ngôn ngữ chuyển
đổi (L1 →L2 hoặc L2 →L1), đặt ra một thách thức
cho các Mô hình Ngôn ngữ Trộn lẫn Mã (LM), và
do đó đặc biệt nhấn mạnh SP trong quá trình mô hình
hóa. Chúng tôi thử nghiệm với một số cơ chế mã hóa
vị trí và cho thấy rằng mã hóa vị trí xoay cùng với
thông tin điểm chuyển đổi cho ra kết quả tốt nhất.
Chúng tôi giới thiệu CONFLATOR: một phương pháp
mô hình ngôn ngữ thần kinh cho các ngôn ngữ trộn
lẫn mã. CONFLATOR cố gắng học để nhấn mạnh các
điểm chuyển đổi bằng cách sử dụng mã hóa vị trí
thông minh hơn, cả ở mức unigram và bigram.
CONFLATOR vượt trội so với hiện tại tốt nhất
trên hai tác vụ dựa trên tiếng Hindi và tiếng Anh trộn
lẫn mã (Hinglish): (i) phân tích cảm xúc và (ii) dịch
máy.
1 Trộn lẫn Mã: Sự Ghép nối của hai
Ngôn ngữ
Trộn lẫn mã được định nghĩa là sự thay thế của hai
hoặc nhiều ngôn ngữ trong quá trình phát âm. Gần đây,
trộn lẫn mã đã nhận được rất nhiều sự chú ý trong
lĩnh vực NLP do sự phổ biến của việc trộn lẫn ngôn
ngữ trong các xã hội đa ngôn ngữ như Ấn Độ, Châu
Âu, Mỹ, Nam Phi, Mexico, v.v. Trong các xã hội như
vậy, trộn lẫn mã khá phổ biến, đặc biệt là trong các
cuộc trò chuyện thân mật, nơi ngôn ngữ bản địa thường
được la-tinh hóa và trộn lẫn mã với một ngôn ngữ
phụ trợ. Hiệu ứng này thỉnh thoảng biểu hiện trong
các bài đăng có nguồn gốc từ các nguồn nói trên trên
các nền tảng mạng xã hội như Twitter, Facebook, v.v.
Một ví dụ về trộn lẫn mã tiếng Hindi và tiếng Anh
được thể hiện trong cụm từ sau đây nơi một từ tiếng
Anh, dance, được trộn lẫn với các từ tiếng Hindi được
la-tinh hóa: Gaaye, aur, kare.
Gaaye HIaurHIdance ENkare HI
Bản dịch tiếng Anh: hát và nhảy
Với sự gia tăng của trộn lẫn mã trên internet, việc
nghiên cứu xử lý ngôn ngữ và mô hình ngôn ngữ cho
các ngôn ngữ trộn lẫn mã là quan trọng. Trong khi mô
hình ngôn ngữ sử dụng mạng thần kinh đã đi một
chặng đường dài, thay thế các mô hình ngôn ngữ n-gram
bằng các biểu diễn thần kinh phân tán (Bengio và cộng
sự, 2003) đến các mô hình ngôn ngữ được tiền huấn
luyện dựa trên transformer lớn gần đây (LM) như
GPT-x (Radford và cộng sự, 2019), BERT (Devlin
và cộng sự, 2018a) v.v., mô hình ngôn ngữ trộn lẫn
mã sử dụng các mô hình dựa trên Transformer hiện
đại (SoTA) vẫn chưa được khám phá đầy đủ.
Trở ngại lớn nhất trong việc áp dụng các LM dựa
trên Transformer SoTA cho trộn lẫn mã có thể được
quy cho việc khan hiếm dữ liệu. Trong khi các kiến
trúc dựa trên Transformer (Vaswani và cộng sự, 2017b)
như BERT và GPT đã thiết lập những điểm chuẩn mới
trong lĩnh vực mô hình ngôn ngữ, chúng nổi tiếng với
hiệu quả mẫu thấp. Nói cách khác, sự khát khao dữ
liệu của Transformer và việc thiếu các bộ dữ liệu
trộn lẫn mã đáng kể trong cộng đồng là lý do chính
cho các trở ngại công nghệ trong lĩnh vực mô hình
ngôn ngữ trộn lẫn mã so với mô hình ngôn ngữ thông
thường.
Để củng cố các lập luận nói trên, chúng tôi thử nghiệm
với các mô hình dựa trên Transformer như GPT-2 và
BERT cho trộn lẫn mã. Chúng tôi quan sát thực nghiệm
rằng các mô hình này hoạt động kém trên các tác vụ
liên quan đến dữ liệu trộn lẫn mã. Giả thuyết của chúng
tôi như sau: Vì thông tin liên quan đến điểm chuyển
đổi là một thành phần chính trong bối cảnh nội dung
trộn lẫn mã, nó do đó nên được tích hợp vào xử lý
hạ nguồn. Các điểm chuyển đổi là một điểm nghẽn
cho việc xử lý dữ liệu trộn lẫn mã của mô hình và
là lý do cho hiệu suất kém khi sử dụng các mô hình
ngôn ngữ thần kinh SoTA (Chatterjere và cộng sự,
2020). Các điểm chuyển đổi đóng vai trò là một yếu
tố quan trọng khi xử lý dữ liệu CM. Trong các phần
tiếp theo, chúng tôi thảo luận về các phương pháp
mã hóa vị trí khác nhau, các điểm chuyển đổi và các
phương pháp của chúng tôi cho mô hình ngôn ngữ
trên dữ liệu trộn lẫn mã. Những đóng góp chính của
chúng tôi là:
•Chúng tôi đề xuất CONFLATOR, một hệ thống LM
tích hợp thông tin vị trí liên quan đến điểm chuyển
đổi.
•Hệ thống của chúng tôi cải thiện hiệu suất của các
mô hình hiện có và đạt được SoTA mới trên hai tác
vụ.
•Chúng tôi điều tra, thử nghiệm với và giới thiệu
các kỹ thuật mã hóa vị trí dựa trên điểm chuyển
đổi khác nhau.
•Chúng tôi giới thiệu một ma trận Xoay dựa trên
Điểm Chuyển đổi mới cho Mã hóa Vị trí Xoay
(RoPE).
•Chúng tôi tuyển chọn một bộ dữ liệu mới gồm các
tweet trộn lẫn mã.
2 Công trình Liên quan
Việc nghiên cứu trộn lẫn mã là quan trọng vì nó là
một phần của hầu hết các xã hội đa ngôn ngữ và phổ
biến trên mạng xã hội. Việc xử lý văn bản trộn lẫn
mã phức tạp hơn so với văn bản đơn ngôn ngữ đối
với các tác vụ NLP (Verma, 1976). Hướng công trình
tương tự đã được theo đuổi bởi Bokamba (1988) và
Singh (1985) về sự phức tạp của đa ngôn ngữ trên
cơ sở cú pháp và ngữ pháp. Những khó khăn trong
việc xử lý các ngôn ngữ trộn lẫn mã trên mạng xã
hội còn trở nên trầm trọng hơn bởi cách viết bất
thường, nhiều cách viết độc đáo cho cùng một từ,
việc viết hoa không cần thiết, v.v. (Das và Gambäck,
2014; Laddha và cộng sự, 2020).
Với sự phổ biến ngày càng tăng trên mạng xã hội,
nhiều tác vụ khác nhau như phân tích cảm xúc (Patwa
và cộng sự, 2020a; Chakravarthi và cộng sự, 2020),
dịch thuật (Dhar và cộng sự, 2018; Srivastava và
Singh, 2020), phát hiện ngôn từ thù ghét (Bohra và
cộng sự, 2018; Banerjee và cộng sự, 2020), gắn thẻ
POS (Vyas và cộng sự, 2014), v.v. đã được thực hiện
trên dữ liệu trộn lẫn mã. Các phương pháp xử lý
trộn lẫn mã cho phân loại văn bản bao gồm việc sử
dụng CNN (Aroyehun và Gelbukh, 2018; Patwa và
cộng sự, 2020b), các mô hình Transformer hoặc giống
BERT (Samghabadi và cộng sự, 2020; Tang và cộng
sự, 2020), các mô hình tổng hợp (Tula và cộng sự,
2021; Jhanwar và Das, 2018), focal loss (Tula và
cộng sự, 2022; Ma và cộng sự, 2020) v.v.
Vaswani và cộng sự (2017a) đã đề xuất transformer
cho mô hình ngôn ngữ thần kinh sử dụng mô hình
ngôn ngữ có mặt nạ (MLM) và dự đoán câu tiếp theo,
đạt được hiệu suất SoTA trên nhiều tác vụ NLP.
Devlin và cộng sự (2018b) đã phát hành mBERT,
một mô hình được huấn luyện trên corpus đa ngôn
ngữ bao gồm 104 ngôn ngữ. Một mô hình ngôn ngữ
liên ngôn ngữ XLM đã được đề xuất trong Lample
và Conneau (2019) tận dụng corpus đơn ngôn ngữ
và liên ngôn ngữ để tiền huấn luyện. Nayak và Joshi
(2022) trình bày một bert được tiền huấn luyện trên
dữ liệu CM. Tuy nhiên, họ không thay đổi mô hình
ngôn ngữ hoặc kỹ thuật của mình để xử lý dữ liệu
trộn lẫn mã đặc biệt. Sengupta và cộng sự (2021)
đề xuất một kiến trúc transformer phân cấp thu thập
mối quan hệ ngữ nghĩa giữa các từ và học cấp bậc
ngữ nghĩa cấp câu của dữ liệu trộn lẫn mã. Ali và
cộng sự (2022) là một trong những người đầu tiên
tích hợp thông tin điểm chuyển đổi vào mã hóa vị
trí. Họ sử dụng mã hóa vị trí động trong khi phương
pháp của chúng tôi, CONFLATOR truyền thông tin
điểm chuyển đổi vào mã hóa vị trí xoay và cũng sử
dụng cả token unigram và bigram để có được embedding
cuối cùng.
3 Trích xuất Dữ liệu và Chiến lược
Trong phần này, chúng tôi thảo luận về các chi tiết
trích xuất dữ liệu trộn lẫn mã. Mục tiêu chính của
chúng tôi là trích xuất dữ liệu trộn lẫn mã được phân
bố tự nhiên.
3.1 Các Điểm Kiểm tra Định tính và Định lượng
cho Corpus Hinglish
Hiệu suất của LM phụ thuộc vào kích thước và chất
lượng dữ liệu huấn luyện, cùng với kích thước từ
vựng. Mô hình ngôn ngữ trộn lẫn mã gặp phải các
thách thức sau: i) khan hiếm dữ liệu, ii) Từ ngữ từ
2 (hoặc nhiều) ngôn ngữ trong cùng một câu, iii)
Tiếng Hindi được viết bằng chữ cái tiếng Anh (tức
là chuyển tự), do đó, không có chuẩn hóa chính tả
- điều này có hiệu quả làm gia tăng các dạng từ
(Laddha và cộng sự, 2020, 2022), iii) Trộn lẫn mã
thường được tìm thấy trên mạng xã hội và người
dùng mạng thường tích hợp sự sáng tạo trong việc
trộn lẫn của họ cùng với trò chơi chữ. Chúng tôi
xem xét hai câu hỏi cơ bản để hướng dẫn việc thu
thập dữ liệu của chúng tôi:
1.Hiệu suất trên bất kỳ tác vụ NLP nào phụ thuộc vào
độ phức tạp của dữ liệu:

--- TRANG 2 ---
Đo lường thực nghiệm: Xem xét hai tweet 4 từ
- i) Ti:wL1wL1wL2wL2 và ii) Tj:wL1wL2wL1wL2.
Cả hai tweet đều có 2 từ mỗi từ từ các ngôn ngữ
L1 và L2. Do đó tỷ lệ trộn lẫn của cả hai tweet
Ti và Tj là (4−2)/4 = 0.50. Tuy nhiên, Ti chỉ
chứa 1 điểm thay đổi mã trong khi Tj chứa 3
chuyển đổi. Có khả năng Tj khó xử lý hơn. Do
đó, chúng ta cần một chỉ số cho mức độ trộn lẫn
giữa các ngôn ngữ. Chúng tôi sử dụng Chỉ số
Trộn lẫn Mã (Gambäck và Das, 2016) (CMI) để
đo lường sự phức tạp như vậy. Vui lòng tham
khảo phần 3.2 để biết thêm chi tiết về CMI.
2.Bao nhiêu dữ liệu là đủ?
Đo lường thực nghiệm: Khi hai ngôn ngữ hòa
trộn, khá tự nhiên là số lượng dạng từ độc đáo
sẽ cao hơn nhiều trong corpus Hinglish so với
corpus tiếng Anh hoặc tiếng Hindi đơn ngôn ngữ.
Do đó, chúng tôi đặt ra một câu hỏi thiết yếu
ngay từ đầu, bao nhiêu dữ liệu là đủ?
Chúng tôi quyết định tiếp tục thu thập dữ liệu,
cho đến khi đường cong Heaps' bắt đầu hội tụ
để chúng tôi bao phủ hầu hết các từ độc đáo.
Định luật Heaps' (Gopalan và Hopkins, 2020)
nói rằng số lượng từ độc đáo trong một văn bản
gồm n từ được xấp xỉ bởi V(n) = Knβ trong đó
K là một hằng số dương và β nằm giữa 0 và 1,
K bất biến nằm giữa 10 và 100 và β giữa 0.4
và 0.6. Định luật Heaps' thường được coi là
một công cụ ước tính tốt để tính toán kích thước
từ vựng. Để so sánh, từ hình 1, có thể thấy rằng,
đối với English Wiki, việc làm phẳng của đường
cong định luật Heaps' bắt đầu ở 40K-50K, trong
khi đối với tiếng Hindi đơn ngôn ngữ, nó hội tụ
ở 80K-90K, nhưng đối với Hinglish cùng một
hành vi bắt đầu khoảng 800K từ vựng và 50M
từ.
3.2 Chỉ số Trộn lẫn Mã (CMI)
Như đã đề cập trước đây, chúng tôi mong đợi khó
khăn của các tác vụ xử lý ngôn ngữ tăng lên khi
mức độ trộn lẫn mã tăng lên. Để đo lường mức độ
trộn lẫn mã trong corpus của chúng tôi, chúng tôi
sử dụng Chỉ số trộn lẫn mã (Gambäck và Das, 2016):
Cu(x) = wmfm(x) + wpfp(x)
= wmN(x)−max LiεL(tLi)(x)
N(x)∗100 + wpP(x)
N(x)∗100
= 100∗wm((N(x)−max LiεL(tLi)(x)) + wpP(x)
N(x)
(1)
Trong đó x biểu thị phát ngôn, N là số lượng token
trong x thuộc về bất kỳ ngôn ngữ Li nào, wm và
wn là trọng số. Vui lòng tham khảo Gambäck và
Das (2016) để có giải thích chi tiết về CMI.
3.3 Đường ống Thu thập Dữ liệu
Chúng tôi làm theo một đường ống tương tự như
(Chatterjere và cộng sự, 2020). Chúng tôi thu thập
dữ liệu CM từ Twitter thông qua API Twitter. Chúng
tôi cần sử dụng các từ khóa liên quan (từ độc đáo
với tiếng Hindi) trong tìm kiếm của chúng tôi để
có được các tweet CM. Các từ có sự trùng lặp từ
vựng giữa tiếng Hindi và tiếng Anh không nên được
sử dụng để tìm kiếm. Ví dụ, từ do gây nhầm lẫn
vì nó có nghĩa là hai trong tiếng Hindi. Chúng tôi
bắt đầu với bộ dữ liệu phân tích cảm xúc Hinglish
ICON 2017 (Patra và cộng sự, 2018), được chú thích
với ngôn ngữ cấp từ. Từ dữ liệu này, chúng tôi tạo
ra hai từ vựng VHI và VEN, và tạo ra một từ vựng
của các từ tiếng Hindi độc đáo VHI−UNIQ =VHI−I,
trong đó I=VHI∩VEN. Tập VHI−UNIQ sau đó được
sắp xếp theo thứ tự giảm dần, dựa trên tần suất
từ, và được sử dụng làm từ tìm kiếm trên API
Twitter. Một khi chúng tôi có được các tweet, chúng
tôi sử dụng một bộ nhận dạng ngôn ngữ cấp từ
(Barman và cộng sự, 2014) (có độ chính xác 90%+)
trên các tweet và tính toán CMI của tweet. Một
khi chúng tôi có được các nhãn ngôn ngữ cấp từ,
chúng tôi cũng có thể biết các điểm chuyển đổi
ở đâu. Các tweet với CMI = 0 bị loại bỏ. Cuối
cùng, chúng tôi còn lại 87k tweet. Phân bố CMI
của dữ liệu của chúng tôi được đưa ra trong bảng
1. Bộ dữ liệu này được sử dụng để tiền huấn luyện
các mô hình của chúng tôi.
Dữ liệu Huấn luyện và Kiểm tra: Chúng tôi thu
thập 87K câu được phân bố trên tất cả các phạm
vi CMI, thay vì thu thập dữ liệu bằng nhau trên
các phạm vi CMI, để các ngôn ngữ kết quả được
huấn luyện trên corpus này có thể xử lý dữ liệu
thực. Chúng tôi duy trì cùng một phân bố trên
cả corpus huấn luyện và kiểm tra của chúng tôi
(tỷ lệ 4:1), cho các mô hình ngôn ngữ của chúng
tôi.
4 Điểm nghẽn của Mô hình Ngôn ngữ
Trộn lẫn Mã: Điểm Chuyển đổi
Chính thức, Điểm Chuyển đổi (SP) là các token
trong văn bản, nơi ngôn ngữ chuyển đổi. Đối với
các ngôn ngữ trộn lẫn mã, bao gồm một cặp ngôn
ngữ, có thể có hai loại điểm chuyển đổi. Giả sử
hai ngôn ngữ là một phần của ngôn ngữ trộn lẫn
mã là L1 và L2, một điểm chuyển đổi xảy ra khi
ngôn ngữ trong văn bản thay đổi từ L1 sang L2
hoặc L2 sang L1. Để giải thích rõ hơn, hãy xem
xét mẫu sau trong Hinglish:
gaana HIenjoy ENkare HI
Bản dịch tiếng Anh: Thưởng thức bài hát.
Trong ví dụ trên, khi ngôn ngữ chuyển đổi từ Hindi
sang English (gaana HIenjoy EN) một điểm chuyển
đổi HI-EN (HIndi-ENglish) xảy ra. Tương tự, một
điểm chuyển đổi EN-HI (ENglish-HIndi) xảy ra
tại - enjoy ENkare HI.
Trong bối cảnh mô hình các ngôn ngữ trộn lẫn
mã, các điểm chuyển đổi có thể được coi là các
bigram thông thường, xảy ra với các bigram đơn
ngôn ngữ khác trong một corpus. Dễ dàng suy ra
rằng các bigram SP cụ thể sẽ tương đối hiếm trong
một corpus nhất định. Do đó, những lần xuất hiện
thưa thớt như vậy của các bigram điểm chuyển
đổi làm cho bất kỳ Mô hình Ngôn ngữ nào khó
học xác suất và bối cảnh của chúng. Vì ngôn ngữ
thay đổi tại điểm chuyển đổi, LM có khả năng
thấy khó khăn để xử lý các token này. Để đối phó
với thách thức này, chúng tôi phân vùng dữ liệu
trộn lẫn mã của chúng tôi thành (i) điểm chuyển
đổi, và (ii) điểm không chuyển đổi. Sau đó chúng
tôi xây dựng LM cụ thể cho điểm chuyển đổi và
điểm không chuyển đổi, như được thảo luận trong
các phần sau.
Giả thuyết CONFLATOR: CONFLATOR được
xây dựng dựa trên 2 giả thuyết. i) Thông tin vị
trí là quan trọng đối với các mô hình ngôn ngữ,
đặc biệt khi xử lý văn bản CM. ii) Điểm chuyển
đổi là điểm nghẽn cho các mô hình ngôn ngữ
trộn lẫn mã (CMLM). Chúng tôi tích hợp thông
tin vị trí của điểm chuyển đổi vào CMLM của
chúng tôi.
5 Kỹ thuật Mã hóa Vị trí
Như đã thảo luận, SP là điểm nghẽn chính do đó
việc xử lý chúng riêng biệt là cần thiết. Mã hóa
vị trí là cần thiết để các mô hình ngôn ngữ học
các phụ thuộc giữa các token. Embedding vị trí
lần đầu tiên được giới thiệu bởi Vaswani và cộng
sự (2017b). Mã hóa vị trí sinusoidal được đề xuất
bao gồm các giá trị sine và cosine với chỉ số vị
trí làm đầu vào. Các kỹ thuật mã hóa được cải
thiện thêm bởi Liu và cộng sự (2020) nơi một
hàm động được giới thiệu để học vị trí với dòng
gradient và Shaw và cộng sự (2018) đã học biểu
diễn vị trí của các vị trí tương đối sử dụng một
tham số có thể học. Chúng tôi nói về các kỹ thuật
mã hóa vị trí khác nhau một cách chi tiết trong
các phần phụ sau.
Chúng tôi thử nghiệm với một số kỹ thuật đương
đại và thấy rằng mã hóa vị trí xoay (Su và cộng
sự, 2021) hoạt động tốt nhất.
5.1 Mã hóa Vị trí Sinusoidal (SPE)
Vaswani và cộng sự (2017b) đã giới thiệu một
vector sinusoidal được xác định trước pi∈Rd
được gán cho mỗi vị trí i. pi này được thêm vào
word embedding xi∈Rd tại vị trí i, và xi+pi được
sử dụng làm đầu vào cho mô hình sao cho Transformer
có thể phân biệt các từ đến từ các vị trí khác nhau
và điều này cũng gán cho mỗi token một sự chú
ý phụ thuộc vào vị trí. - phương trình 2.
eijabs = 1√d (xi+pi)WQ,1(xj+pj)WK,1T (2)
Trong đó W là ma trận trọng số, Q là truy vấn,
K là khóa, l trong lớp.
5.2 Mã hóa Vị trí Động (DPE)
Thay vì sử dụng các hàm tuần hoàn được xác
định trước như sin, Liu và cộng sự (2020), đã
giới thiệu một hàm động Θ(i) tại mỗi lớp encoder.
Cải thiện mã hóa vị trí sinusoidal, PE Động học
Θ(i) thay vì một pi được xác định trước để mang
hành vi động cho mô hình. Tại mỗi phát ngôn,
hàm có thể học này Θ(i) cố gắng học biểu diễn
tốt nhất có thể cho thông tin vị trí với dòng gradient.
Θ(i) được thêm vào word embedding wi như được
đưa ra trong phương trình 3.
eij = 1√d (xi+ Θ(i))WQ,1(xj+ Θ(j))WK,1T (3)
5.3 Mã hóa Vị trí Tương đối (RPE)
Trong PE tuyệt đối, việc sử dụng pi khác nhau
cho các vị trí i khác nhau giúp transformer phân
biệt các từ tại các vị trí khác nhau. Tuy nhiên,
PE tuyệt đối không hiệu quả trong việc nắm bắt
thứ tự từ tương đối. Shaw và cộng sự (2018) đã
giới thiệu một tham số có thể học ali−j học biểu
diễn vị trí của vị trí tương đối i-j tại lớp encoder
l. Với sự giúp đỡ của điều này, chúng ta có thể
nắm bắt thứ tự từ một cách rõ ràng trong mô hình
của chúng ta như sau:
eijrel = 1√d (xi)lWQ,l(xi)lWK,l+ali−jT (4)
5.4 Mã hóa Vị trí Động và Tương đối dựa trên
Điểm Chuyển đổi (SPDRPE)
Ali và cộng sự (2022) giới thiệu một PE mới, dựa
trên điểm chuyển đổi. Để minh họa, xem xét một
văn bản Hinglish trộn lẫn mã - yeHIgaana HIenjoy
ENkare HI. Chỉ số dựa trên SP (SPI) đặt chỉ số
thành 0 bất cứ khi nào SP xảy ra. Việc lập chỉ mục
thông thường sẽ là Index = (0, 1, 2, 3), nhưng
do việc tích hợp điểm chuyển đổi, điều này bị
thay đổi thành SPI= (0, 1, 0, 0). Ngoài ra, họ
sử dụng một tham số học ali−j, mã hóa vị trí tương
đối i-j tại lớp encoder l. Phương pháp mã hóa
này học các biểu diễn một cách động dựa trên
SP cùng với embedding ali−j để nó cũng có thể
nắm bắt thứ tự từ tương đối, như sau:
eij = 1√d (xi+ Θ(S(li)))lWQ,l(xi+ Θ(S(lj)))lWK,l+ali−jT (5)
5.5 Mã hóa Vị trí Xoay (RoPE)
Tương tự với ý tưởng về sóng điện từ đi qua một
bộ phân cực để bảo toàn biên độ tương đối của
chúng, (Su và cộng sự, 2021) đã nảy ra ý tưởng
về Mã hóa Vị trí Xoay (RoPE). Ý tưởng là sử
dụng ma trận xoay trên các vector embedding
để tạo ra các giá trị vị trí. Việc xoay phủ định
bất kỳ thông tin vị trí tuyệt đối nào và chỉ giữ
lại thông tin về các góc tương đối giữa mỗi cặp
word embedding trong một chuỗi. Chúng ta biết
rằng tích vô hướng giữa hai vector là một hàm
của độ lớn của các vector riêng lẻ và góc giữa
chúng. Giữ điều này trong tâm trí, trực quan cho
RoPE là biểu diễn các embedding như các số
phức và các vị trí như các phép quay thuần túy
mà chúng ta áp dụng cho chúng.
Về mặt toán học, các công thức cho một trường
hợp 2 chiều đơn giản được định nghĩa như sau:
fQ(xi, i) = (WQxi)e√−1iθ
fQ(xj, j) = (WKxj)e√−1jθ
g(xi, xj, i−j) = Re[(WQxi)(WKxi)*e√−1(i−j)θ]
(6)
trong đó Re[] là phần thực của một số phức và
(WKxi)* biểu diễn số phức liên hợp của (WKxi).
θ∈R là một hằng số khác không được đặt trước.
Công thức f(Q,K) như một phép nhân ma trận,
chúng ta có:
fQ(xi, i) = [cosmθ1 −sinmθ1; sinmθ1 cosmθ1] [W(11)Q,K W(12)Q,K; W(21)Q,K W(22)Q,K] [x(1)i; x(2)i]
(7)
trong đó (xi(1), xi(2)) là xi được biểu diễn dưới
dạng tọa độ 2D. Theo cách tương tự, chúng ta
có thể biến hàm g thành dạng ma trận. Bằng cách
xoay vector embedding đã biến đổi theo một góc
bội số của chỉ số vị trí của nó, chúng ta có thể
tích hợp thông tin vị trí tương đối. Do đặc tính
này, nó được gọi là Rotary Position Embedding.
Để tổng quát hóa kết quả trong 2D cho bất kỳ
xi trong Rd nơi d là chẵn, họ chia không gian
d chiều thành d/2 không gian con và kết hợp
chúng nhờ tính tuyến tính của tích vô hướng,
biến công thức attention:
fQ,K=eijrotary = 1√d RMdΘ,iWQ,1(xi)T RMdΘ,jWK,1(xj) (8)
RM = [Ma trận xoay với các giá trị cos và sin]
(9)
trong đó RM là ma trận trực giao và thưa thớt
các tham số được xác định trước
Θ = θi = 10000^(-2(i-1)/d), i∈[1,2, ..., d/2]. (10)
Trái ngược với bản chất cộng của các phương
pháp embedding vị trí được sử dụng bởi các
công trình khác, phương pháp của họ là nhân.
Hơn nữa, RoPE tự nhiên tích hợp thông tin vị
trí tương đối thông qua tích ma trận xoay thay
vì thay đổi các thuật ngữ trong công thức mở
rộng của mã hóa vị trí cộng khi được áp dụng
với self-attention.
6 Tích hợp Thông tin Điểm Chuyển đổi
trong CMLM
Mã hóa vị trí giúp transformer học các phụ thuộc
giữa các token tại các vị trí khác nhau của chuỗi
đầu vào. Để nâng cao mã hóa vị trí cho văn bản
trộn lẫn mã, chúng tôi sửa đổi mã hóa vị trí xoay
để tích hợp thông tin điểm chuyển đổi.

--- TRANG 3 ---
6.1 Ma trận Xoay dựa trên Điểm Chuyển đổi
Điểm chuyển đổi là điểm nghẽn tiềm năng cho
mô hình ngôn ngữ trộn lẫn mã và để giải quyết
vấn đề này, chúng tôi tích hợp mã hóa vị trí xoay
dựa trên điểm chuyển đổi trong kiến trúc của
chúng tôi. Trực quan đằng sau RoPE là sóng điện
từ. Các embedding được biểu diễn như các số
phức và các vị trí được biểu diễn như các phép
quay thuần túy được áp dụng cho chúng. Giữ
điều này trong tâm trí, chúng tôi giải quyết vấn
đề của điểm chuyển đổi (SP) với sự giúp đỡ của
các góc tham gia vào RoPE. Bất cứ khi nào chúng
tôi gặp một điểm chuyển đổi, chúng tôi thay đổi
phép quay, tức là, chúng tôi thay đổi hướng của
các góc này. Để thực hiện thay đổi phép quay,
chúng tôi định nghĩa một ma trận điểm chuyển
đổi. Ma trận điểm chuyển đổi giúp mô hình của
chúng tôi xác định và học các mẫu trộn lẫn mã
trong corpus. Ma trận của chúng tôi được định
nghĩa với 1 và -1. Khi có sự thay đổi ngôn ngữ
(L1→L2) hoặc (L2→L1), tức là khi chúng tôi
gặp một điểm chuyển đổi, chúng tôi chú thích
giá trị cột là -1 và đối với các từ tiếp theo trong
L2, chúng tôi chú thích giá trị cột là 1 cho đến
khi một điểm chuyển đổi khác xảy ra.
SPM∈Rd n∗n
if i==SP:
SPM i=−1
else:
SPM i= 1 (11)
Trực quan thị giác của phương pháp chúng tôi
được thể hiện trong Hình 2. Ma trận điểm chuyển
đổi (SPM) với 1 và -1 được định nghĩa theo cách
mà nó chuyển vị ma trận xoay, một cách trực
quan đảo ngược phép quay tại mỗi lần gặp điểm
chuyển đổi. Do đó, ma trận cuối cùng, tức là ma
trận xoay điểm chuyển đổi (SPRM) là kết quả
của phép nhân từng phần tử của ma trận điểm
chuyển đổi được định nghĩa (SPM) với ma trận
xoay (RM):
SPRM = SPM × RM (12)
eSPRotary ij = 1√d SPRMdΘ,iWQ,1(xi)T SPRMdΘ,jWK,1(xi) (13)
6.2 Mã hóa Vị trí Xoay dựa trên Bigram và
Điểm Chuyển đổi (BSPRoPE)
Vì ngôn ngữ thay đổi tại SP, chúng tôi nhận được
hai token liên tiếp với ngôn ngữ khác nhau do
đó chúng tôi cũng tích hợp thông tin cấp bigram
vào mô hình của chúng tôi. Trong phương pháp
mã hóa vị trí này, chúng tôi nhận thông tin vị
trí giữa các bigram trong một phát ngôn. Chúng
tôi sử dụng kỹ thuật mã hóa vị trí xoay dựa trên
điểm chuyển đổi ở mức từ-đến-từ và ở mức bigram
như được mô tả trong Hình 3,4 và được biểu
diễn toán học như Phương trình 16
eUniSPRotary ij = 1√d SPRMdΘ,iWQ,1(xi)T SPRMdΘ,jWK,1(xj) (14)
eBiSPRotary ij = 1√d SPRMdΘ,iWQ,1(xi)T SPRMdΘ,jWK,1(xj) (15)
prediction = a∗eUnigramSPRotary ij + b∗eBigramSPRotary ij (16)
trong đó a và b là các hệ số có thể học. xi và xi
trong phương trình 14 tham chiếu đến đầu vào
unigram trong khi trong phương trình 15 chúng
tham chiếu đến đầu vào bigram.
6.3 Kiến trúc CONFLATOR
Các phụ thuộc cục bộ cho Unigram và Bigram
(Word2Vec được huấn luyện từ đầu) cùng với
unigram và bigram SPRM được đưa vào một
Multi-Head attention (MHA) 6 đầu trong mỗi
lớp encoder của transformer riêng biệt, dẫn đến
2 ma trận attention. Chúng tôi giới thiệu 2 tham
số có thể học α và β được sử dụng như hệ số
trọng số cho ma trận unigram và bigram tương
ứng. Ma trận cuối cùng được chuyển đến lớp
decoder. Embedding và kiến trúc được mô tả
trong hình 3 và 4.
7 Thí nghiệm và Kết quả
Đối với các mô hình cơ sở của chúng tôi, mỗi
bước huấn luyện mất khoảng 0,5 giây. Chúng
tôi huấn luyện các mô hình cơ sở tổng cộng 100.000
bước hoặc 12 giờ. Đối với các mô hình lớn như
các mô hình dựa trên bigram và SPM, thời gian
bước là 1,0 giây. Các mô hình lớn được huấn
luyện trong 250.000 bước (2 ngày). Chúng tôi
sử dụng bộ tối ưu hóa ADAM với β1= 0,9, β2=
0,98 và ε= 1e-9. Chúng tôi sử dụng phương pháp
thay đổi tốc độ học trong suốt quá trình huấn
luyện từ Vaswani và cộng sự (2017b).
Chúng tôi sử dụng hai loại điều chuẩn trong
quá trình huấn luyện: Chúng tôi áp dụng dropout
cho đầu ra của mỗi lớp encoder và decoder theo
sau bởi Normalization. Ngoài ra, chúng tôi áp
dụng dropout và normalization cho tổng của
word embedding và mã hóa vị trí trong cả lớp
encoder và decoder. Chúng tôi sử dụng tỷ lệ
P drop= 0,2.
Đánh giá Nội tại: Điểm số perplexity của các
mô hình ngôn ngữ cơ sở so với CONFLATOR
trên tác vụ mô hình ngôn ngữ trộn lẫn mã được
thể hiện trong 2. Chúng tôi thấy rằng mô hình
của chúng tôi hoạt động tốt hơn nhiều so với
các mô hình khác.
Đánh giá Ngoại tại: Chúng tôi đánh giá mô hình
của chúng tôi trên hai tác vụ downstream: (i)
phân tích cảm xúc, và (ii) dịch máy. Đối với
phân tích cảm xúc, (Bảng 3) chúng tôi sử dụng
dữ liệu được cung cấp bởi Patwa và cộng sự
(2020a). CONFLATOR đạt điểm F1 76,23% và
vượt trội so với SOTA (Ali và cộng sự, 2022).
Lý do chính cho điều này là học SP bằng cách
tổng hợp với sự giúp đỡ của mã hóa vị trí xoay
với khung MHA độ dài biến đổi. Đối với dịch
máy (Bảng 4), chúng tôi sử dụng dữ liệu được
cung cấp bởi (Dhar và cộng sự, 2018). Chúng
tôi đạt điểm bleu 29,1 và vượt trội so với SOTA
(Dhar và cộng sự, 2018) sử dụng mô hình Unigram
SPRoPE có thể học các mẫu trộn lẫn ngôn ngữ
với sự giúp đỡ của mã hóa vị trí xoay dựa trên
điểm chuyển đổi.
8 Kết luận & Điểm rút ra
Trong công trình này, chúng tôi báo cáo các thí
nghiệm về các vấn đề phân tích cảm xúc Hinglish
và dịch máy thông qua lăng kính của mô hình
ngôn ngữ. Đóng góp của chúng tôi có thể được
xem như sau:
(i) Chúng tôi giới thiệu ý tưởng về mã hóa vị
trí xoay dựa trên điểm chuyển đổi. Bất cứ khi
nào gặp một điểm chuyển đổi, chúng tôi tích
hợp thay đổi phép quay để học các mẫu trộn
lẫn ngôn ngữ.
(ii) Chúng tôi giới thiệu CONFLATOR, một phương
pháp mô hình ngôn ngữ thần kinh cho các ngôn
ngữ trộn lẫn mã. CONFLATOR cố gắng học
các biểu diễn tốt hơn bằng phương tiện mã hóa
vị trí xoay dựa trên điểm chuyển đổi, ban đầu
ở mức unigram và sau đó ở mức bigram.
(iii) Chúng tôi chứng minh thực nghiệm rằng
CONFLATOR đang học các mẫu trộn lẫn mã
mà các mô hình khác với mã hóa vị trí khác
nhau không thành công, như được thể hiện
trong Hình 5.
(iv) Cũng đáng chú ý là CONFLATOR đạt được
kết quả tương đương với SOTA ngay cả khi
không có bất kỳ mô hình ngôn ngữ tiền huấn
luyện nặng nào.
9 Hạn chế
Mặc dù mô hình bigram của chúng tôi đạt được
SOTA về phân tích cảm xúc sử dụng unigram,
nó hơi tụt hậu so với mô hình bigram khi nói
đến dịch máy, nơi sử dụng bigram ở mức decoder
dẫn đến hiệu suất kém. Mặc dù tiến hành các
thí nghiệm rộng rãi, vẫn thiếu giải thích chi tiết
về lý do tại sao phương pháp dựa trên bigram
cho MT thất bại. Các thí nghiệm tương lai sẽ
tập trung vào việc khám phá hoặc hiểu vấn đề
của bigram cho MT và đưa ra giải pháp cho vấn
đề tương tự.
