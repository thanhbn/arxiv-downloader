# 2305.11129.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multilingual/2305.11129.pdf
# Kích thước tệp: 157782 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
mLongT5: Một Transformer Văn bản-sang-Văn bản Đa ngôn ngữ và Hiệu quả cho
Chuỗi Dài hơn
David Uthus, Santiago Ontañón, Joshua Ainslie, Mandy Guo
Google Research
{duthus, santiontanon, jainslie, xyguo}@google.com
Tóm tắt
Chúng tôi trình bày công trình phát triển một transformer văn bản-sang-văn bản đa ngôn ngữ, hiệu quả phù hợp để xử lý đầu vào dài. Mô hình này, được gọi là mLongT5, được xây dựng dựa trên kiến trúc của LongT5, trong khi tận dụng các tập dữ liệu đa ngôn ngữ được sử dụng để huấn luyện trước mT5 và các tác vụ huấn luyện trước của UL2. Chúng tôi đánh giá mô hình này trên nhiều tác vụ tóm tắt và hỏi đáp đa ngôn ngữ, và kết quả cho thấy hiệu suất mạnh hơn của mLongT5 khi so sánh với các mô hình đa ngôn ngữ hiện có như mBART hoặc M-BERT.

1 Giới thiệu
Trong những năm gần đây, đã có sự phát triển trong việc làm cho các mô hình dựa trên transformer hiệu quả hơn để có thể xử lý các chuỗi đầu vào dài hơn. Tuy nhiên, nhiều mô hình chỉ hoạt động với tiếng Anh, khiến chúng không thể áp dụng cho các ngôn ngữ khác.

Trong bài báo này, chúng tôi trình bày công trình mở rộng một trong những mô hình này để có thể xử lý dữ liệu đa ngôn ngữ. Mô hình của chúng tôi, được gọi là mLongT5, tận dụng kiến trúc hiệu quả của LongT5 (Guo et al., 2022), và đã được huấn luyện trước trên tập dữ liệu đa ngôn ngữ mC4 (Xue et al., 2021) để có thể hoạt động trên các tác vụ đa ngôn ngữ. Chúng tôi đã áp dụng mLongT5 cho nhiều tác vụ tóm tắt và hỏi đáp đa ngôn ngữ, và kết quả cho thấy mLongT5 thể hiện hiệu suất mạnh trong các lĩnh vực này.

Các cấu hình¹ và checkpoint² đều đã được mở nguồn.

2 Nghiên cứu liên quan
Có hai lĩnh vực nghiên cứu liên quan - các mô hình transformer hiệu quả có thể xử lý đầu vào dài, và các mô hình đa ngôn ngữ.

¹https://github.com/google/flaxformer/tree/
main/flaxformer/t5x/configs/longt5/models
²https://github.com/google-research/longt5

Gần đây đã có nhiều quan tâm đến việc làm cho các mô hình transformer hiệu quả hơn, chẳng hạn như để xử lý đầu vào dài hơn. Ví dụ về những mô hình này bao gồm ETC (Ainslie et al., 2020), Big Bird (Zaheer et al., 2020), LongT5 (Guo et al., 2022), và Longformer (Beltagy et al., 2020). Những mô hình này đã thành công trong việc áp dụng các phương pháp khác nhau để giải quyết sự tăng trưởng bậc hai của cơ chế attention trong transformer. Tuy nhiên, những mô hình này được huấn luyện trên các tập dữ liệu tiếng Anh, hạn chế việc sử dụng chúng trong các lĩnh vực đa ngôn ngữ.

Đối với các mô hình đa ngôn ngữ, chúng bao gồm mT5 (Xue et al., 2021), mBART (Liu et al., 2020), và umT5 gần đây (Chung et al., 2023). Những mô hình này tái sử dụng các kiến trúc được sử dụng bởi các mô hình tiếng Anh nhưng được huấn luyện trước trên một corpus đa ngôn ngữ lớn hơn, với mT5 và umT5 được huấn luyện trên 101 ngôn ngữ và mBART trên 25. Mặc dù những mô hình này cho thấy hiệu suất mạnh trong việc xử lý nhiều ngôn ngữ khác nhau, chúng gặp phải những hạn chế tương tự như các mô hình tiếng Anh gốc trong việc không thể mở rộng lên các chuỗi dài hơn.

3 Mô hình
mLongT5 được xây dựng dựa trên kiến trúc của LongT5 (Guo et al., 2022). LongT5 được phát triển để xử lý hiệu quả đầu vào dài bằng cách sử dụng cơ chế attention hiệu quả hơn. Mô hình đã được chứng minh có hiệu suất mạnh trên nhiều tác vụ downstream, và do đó là nền tảng cho mLongT5.

3.1 Tập dữ liệu
Để làm cho mLongT5 đa ngôn ngữ, chúng tôi tận dụng tập dữ liệu mC4 được sử dụng để huấn luyện mô hình đa ngôn ngữ mT5 (Xue et al., 2021), bao gồm 101 ngôn ngữ. Tập dữ liệu này gần đây đã được cập nhật, như được mô tả bởi Chung et al. (2023), và được sử dụng để huấn luyện umT5 và tạo một mô hình SentencePiece mới (Kudo và Richardson, 2018). Do đó, chúng tôi sử dụng cùng mô hình SentencePiece được sử dụng cho umT5, cho phép mLongT5 xử lý đầu vào đa ngôn ngữ.

3.2 Tác vụ Huấn luyện trước
Một điểm khác biệt chính với mô hình của chúng tôi và LongT5 là việc thay đổi các tác vụ để huấn luyện trước mô hình. LongT5 sử dụng Principle Sentences Generation (PSG) của PEGASUS (Zhang et al., 2020) để huấn luyện trước các mô hình. Mặc dù điều này được chứng minh có hiệu suất mạnh cho các tác vụ downstream khác nhau, một điểm yếu của PSG là nó ít phù hợp cho huấn luyện đa ngôn ngữ. PSG dựa vào khả năng chia một đoạn văn bản thành các câu, với việc triển khai hiện tại phù hợp nhất cho các ngôn ngữ dựa trên Latin. Nhu cầu chia văn bản thành câu một cách chính xác cho 101 ngôn ngữ khác nhau khiến đây trở thành một tác vụ thách thức khi sử dụng trong môi trường đa ngôn ngữ.

Để khắc phục điều này, chúng tôi quyết định áp dụng các tác vụ huấn luyện trước của UL2 (Tay et al., 2022). Tác vụ huấn luyện trước của họ, được gọi là Mixture-of-Denoisers (MoD), có mô hình học từ một hỗn hợp các tác vụ, và đã được chứng minh hoạt động tốt hơn tác vụ huấn luyện trước ban đầu của T5 (Raffel et al., 2019). Quan trọng hơn, MoD có thể được áp dụng dễ dàng hơn cho các ngôn ngữ khác so với PSG, do đó làm cho nó lý tưởng để huấn luyện trước mLongT5.

3.3 Chi tiết Huấn luyện trước
Huấn luyện trước mLongT5 có nhiều điểm tương tự với cách LongT5 được huấn luyện trước. Nó được huấn luyện trước trong một triệu bước, và chúng tôi huấn luyện trước các kích thước mô hình Base, Large, và XL. Chúng tôi cũng sử dụng cùng độ dài huấn luyện trước, 4,096 cho đầu vào và 910 cho đích. Một điểm khác biệt nhỏ là tăng kích thước batch từ 128 lên 256, cho phép mô hình huấn luyện trên cùng số lượng token như mT5. Đối với tập dữ liệu mC4, chúng tôi sử dụng phiên bản 3.1.0, là phiên bản được cập nhật bởi Chung et al. (2023). Để lấy mẫu tập dữ liệu, chúng tôi sử dụng phương pháp lấy mẫu UniMax (Chung et al., 2023).

Thay vì PSG như tác vụ huấn luyện trước, chúng tôi áp dụng MoD, sử dụng cùng cấu hình như được định nghĩa trong định nghĩa tác vụ UL2 ban đầu. Ngoại lệ duy nhất là chúng tôi không sử dụng tỷ lệ corruption 0.5 (chỉ sử dụng tỷ lệ corruption 0.15), vì độ dài đầu vào của chúng tôi (4096) dài hơn nhiều so với độ dài đích (910), khiến tỷ lệ corruption 0.5 không khả thi.

Tất cả các mô hình được huấn luyện trước sử dụng 256 chip TPUv4. Thời gian wall để huấn luyện trước những mô hình này là 1.9 ngày cho Base, 3.7 ngày cho Large, và 12.4 ngày cho XL.

4 Kết quả
Như với bài báo LongT5 ban đầu, chúng tôi xem xét hai lĩnh vực để đánh giá mô hình: tóm tắt và hỏi đáp.

Đối với tất cả những tác vụ này, chúng tôi sử dụng các giá trị mặc định như được sử dụng cho fine-tuning T5, chỉ thiết lập rõ ràng độ dài đầu vào và đích như được mô tả trong các tác vụ bên dưới.

4.1 Tóm tắt
Ba tác vụ tóm tắt mà chúng tôi đang xem xét là:

•MLSUM (Scialom et al., 2020): một bộ sưu tập các bài báo và bản tóm tắt tương ứng bằng năm ngôn ngữ: Pháp, Đức, Tây Ban Nha, Nga, và Thổ Nhĩ Kỳ.

•XL-Sum (Hasan et al., 2021): một bộ sưu tập các bài báo và bản tóm tắt BBC bằng 44 ngôn ngữ.

•WikiLingua (Ladhak et al., 2020): một bộ sưu tập tài liệu từ WikiHow (bằng tiếng Tây Ban Nha, Thổ Nhĩ Kỳ, Nga, và Việt Nam) đã được dịch và tóm tắt thành tiếng Anh. Đối với tác vụ này, chúng tôi sử dụng phiên bản GEM (Gehrmann et al., 2021) của các tập dữ liệu, cho phép chúng tôi sử dụng các sửa chữa của họ trong việc chia tập dữ liệu để huấn luyện và kiểm tra.

Những tác vụ này cho phép chúng tôi khám phá tóm tắt nơi tác vụ liên quan đến tài liệu và bản tóm tắt của chúng bằng cùng ngôn ngữ (MLSUM, XL-Sum), hoặc nơi tác vụ liên quan đến cả dịch thuật và tóm tắt cùng lúc (WikiLingua).

Chúng tôi lưu ý rằng đối với độ dài tác vụ, những tác vụ đa ngôn ngữ này không quá dài khi so sánh với các tác vụ được đề cập trong bài báo LongT5 ban đầu. Thật không may, thiếu các tác vụ tóm tắt đa ngôn ngữ dài, do đó chúng tôi sử dụng ba tác vụ này để so sánh. Do đó, chúng tôi kiểm tra với độ dài đầu vào 4k cho đầu vào và 512 cho đầu ra, điều này bao phủ hầu hết tài liệu cho tất cả các tác vụ trên.

Đối với tất cả những tác vụ này, chúng tôi báo cáo điểm ROUGE tiêu chuẩn (ROUGE-1, ROUGE-2, và ROUGE-L).

4.1.1 MLSUM
Bảng 1 cho thấy kết quả của chúng tôi cho tác vụ MLSUM. Chúng tôi so sánh với mô hình M-BERT (Devlin, 2018) được sử dụng trong bài báo ban đầu. Các tác giả chỉ báo cáo điểm ROUGE-L, trong khi chúng tôi cũng báo cáo điểm ROUGE-1 và ROUGE-2.

Nhìn vào điểm ROUGE-L, chúng ta có thể thấy rằng mLongT5 hoạt động tương đương với M-BERT cho tiếng Pháp, trong khi làm tốt hơn M-BERT cho tất cả kích thước mô hình bằng tiếng Đức, Tây Ban Nha, và Thổ Nhĩ Kỳ. Chỉ với tiếng Nga nó hoạt động hơi kém hơn. Như được lưu ý trong bài báo ban đầu, tiếng Nga là ngôn ngữ khó nhất cho các mô hình ngôn ngữ, do có tập dữ liệu nhỏ hơn nhiều khi so sánh với các ngôn ngữ khác trong corpus và tỷ lệ mới cao hơn (từ tìm thấy trong bản tóm tắt nhưng không có trong tài liệu đầu vào). Ngoài ra, như chúng tôi đã đề cập trước đó, độ dài đầu vào của tập dữ liệu không quá dài, do đó các mô hình với attention đầy đủ có thể tận dụng tốt hơn độ dài ngắn so với mLongT5. Điều này có thể góp phần vào việc mLongT5 không hoạt động tốt cho trường hợp này.

--- TRANG 2 ---
FR
Phương pháp R-1 R-2 R-L
M-BERT - - 25.09
mLongT5 (base) 30.79 14.16 23.83
mLongT5 (large) 31.44 14.74 24.36
mLongT5 (xl) 32.18 15.68 25.18
DE
Phương pháp R-1 R-2 R-L
M-BERT - - 42.01
mLongT5 (base) 45.60 35.31 42.22
mLongT5 (large) 46.21 35.68 42.71
mLongT5 (xl) 46.95 36.36 43.45
ES
Phương pháp R-1 R-2 R-L
M-BERT - - 20.44
mLongT5 (base) 28.78 10.98 23.15
mLongT5 (large) 29.05 11.58 23.50
mLongT5 (xl) 30.36 12.77 24.73
TR
Phương pháp R-1 R-2 R-L
M-BERT - - 32.94
mLongT5 (base) 44.18 30.86 38.60
mLongT5 (large) 44.92 31.55 39.29
mLongT5 (xl) 45.73 32.80 40.26
RU
Phương pháp R-1 R-2 R-L
M-BERT - - 9.48
mLongT5 (base) 7.73 1.78 7.22
mLongT5 (large) 7.71 1.86 7.23
mLongT5 (xl) 8.85 2.67 8.42

Bảng 1: Kết quả MLSUM so sánh mLongT5 với mô hình ban đầu M-BERT. Lưu ý rằng bài báo ban đầu chỉ báo cáo điểm ROUGE-L, trong khi chúng tôi cũng báo cáo ROUGE-1 và ROUGE-2.

4.1.2 XL-Sum
Đối với XL-Sum, chúng tôi fine-tune mô hình theo cách tiếp cận tương tự như bài báo ban đầu - chúng tôi fine-tune trên hỗn hợp tất cả các ngôn ngữ trong 50,000 bước, và sau đó thực hiện kiểm tra cho từng ngôn ngữ riêng lẻ từ mô hình đơn này.

Bảng 2 cho thấy một tập con các ngôn ngữ (kết quả đầy đủ có thể xem trong Phụ lục A). Chúng tôi làm nổi bật những ngôn ngữ có độ dài đầu vào dài hơn (do cả độ dài của tài liệu gốc và cách chúng được token hóa bởi SPM).

Như chúng ta có thể thấy, mLongT5 hoạt động tốt so với mT5 cho những đầu vào dài hơn này. Khi so sánh base với base, nó hoạt động hơi kém hơn, như mong đợi với mT5 có attention đầy đủ. Mô hình LongT5 ban đầu, khi được fine-tune trên các tập dữ liệu có độ dài ngắn hơn, cũng đã cho thấy hiệu suất hơi kém hơn khi so sánh với mô hình có attention đầy đủ. Chúng ta đang thấy kết quả tương tự ở đây. Nhưng mLongT5 có thể dễ dàng mở rộng lên kích thước mô hình lớn hơn, và do đó, chúng ta có thể thấy kết quả mạnh hơn khi chúng ta tăng kích thước của mô hình.

4.1.3 WikiLingua
Tác vụ tóm tắt cuối cùng là WikiLingua, với kết quả được hiển thị trong Bảng 3. Tác vụ này yêu cầu cả dịch thuật và tóm tắt, với tác vụ dịch từ một tài liệu đầy đủ của ngôn ngữ khác thành bản tóm tắt tiếng Anh. Như đã đề cập trước đó, chúng tôi sử dụng phiên bản GEM của tác vụ này, và so sánh kết quả của chúng tôi với mô hình mT5 trên bảng xếp hạng của họ.

Như được hiển thị trong kết quả, mLongT5 có xu hướng làm tốt hơn cho nhiều kích thước mô hình trên 4 ngôn ngữ, chỉ với hiệu suất hơi kém hơn với kích thước XL cho tiếng Tây Ban Nha.

4.2 Hỏi đáp
Đối với hỏi đáp, chúng tôi áp dụng mLongT5 cho TyDi QA (Clark et al., 2020). TyDi QA là một tác vụ đa ngôn ngữ bao phủ 11 ngôn ngữ, cố gắng trả lời câu hỏi dựa trên một bài báo Wikipedia. Có hai phiên bản của tác vụ này, và chúng tôi tập trung vào Minimal Answer Span Task, trong đó người ta cố gắng tìm span tối thiểu trả lời câu hỏi, đưa ra câu trả lời có/không nếu câu hỏi là câu hỏi có/không, hoặc Null nếu câu hỏi không thể được trả lời dựa trên bài báo.

Tương tự như bài báo LongT5 ban đầu và ứng dụng của họ cho Natural Questions, chúng tôi đã định nghĩa lại tác vụ này từ trích xuất answer span thành một tác vụ seq2seq để tạo ra văn bản câu trả lời. Kết quả được hiển thị sẽ khác với bảng xếp hạng TyDi QA. Do đó, chúng tôi cũng đã chạy mô hình mT5 tương tự trên cùng tác vụ để có baseline để so sánh. Ngoài ra, vì tập kiểm tra không có sẵn cho tác vụ này, chúng tôi sử dụng 90% dữ liệu huấn luyện làm tập huấn luyện và 10% còn lại làm tập dev, và sử dụng tập dev ban đầu làm tập kiểm tra để báo cáo metrics.

Không giống như các tác vụ tóm tắt, TyDi QA có độ dài đầu vào dài hơn nhiều - trung bình 5,148 token và percentile thứ 90 là 12,967 token khi được token hóa với mô hình SentencePiece. Do đó, đối với mT5 chúng tôi kiểm tra với độ dài đầu vào từ 512 đến 4k, trong khi đối với mLongT5 chúng tôi kiểm tra với độ dài đầu vào từ 4k đến 16k.

Bảng 4 cho thấy kết quả chạy mT5 và mLongT5 trên tập dữ liệu này. Đối với tác vụ này, chúng tôi báo cáo metrics Exact Match (EM) và điểm F1. Như có thể thấy trong kết quả, mLongT5 có thể trả lời câu hỏi tốt hơn do nó có thể xử lý chuỗi đầu vào dài hơn.

--- TRANG 3 ---
mT5 (base) mLongT5 (base) mLongT5 (large) mLongT5 (xl)
Ngôn ngữ R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
Gujarati 21.96 7.74 19.86 19.59 6.08 17.61 22.38 7.94 20.15 25.52 9.92 22.78
Marathi 22.01 9.54 19.92 20.33 8.62 18.41 23.35 10.56 21.22 25.90 12.03 23.07
Punjabi 30.70 12.21 25.52 28.61 10.43 23.66 31.92 12.75 26.17 34.45 14.81 28.42
Serbian (Cyrillic) 23.78 7.98 20.14 20.30 5.86 16.74 21.92 6.98 18.35 27.51 11.46 23.49
Serbian (Latin) 21.64 6.66 18.23 18.14 4.75 14.96 21.79 6.92 18.14 25.86 10.17 21.76
Vietnamese 32.88 16.22 26.08 31.58 15.41 25.02 34.54 17.63 27.59 38.17 20.49 30.98

Bảng 2: Kết quả cho XL-Sum, tập trung vào các ngôn ngữ có đầu vào dài hơn. Phần còn lại của kết quả có thể xem trong Phụ lục A.

ES-EN TR-EN RU-EN VI-EN
Phương pháp R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
mT5 (base) 30.9 10.6 26.4 32.0 13.1 26.0 27.3 8.6 23.3 25.6 7.7 21.5
mT5 (large) 34.2 12.6 29.1 34.0 14.5 27.5 32.3 11.2 26.9 32.1 10.9 26.0
mT5 (xl) 41.2 17.2 34.6 40.0 18.3 33.3 37.2 14.6 30.9 37.6 14.9 31.2
mLongT5 (base) 36.1 14.0 30.3 34.5 14.9 28.6 32.4 11.6 26.5 32.3 11.7 26.4
mLongT5 (large) 38.2 15.5 32.0 38.1 17.5 32.0 34.4 13.1 28.5 35.1 13.8 29.1
mLongT5 (xl) 40.8 17.6 34.3 42.5 20.9 36.7 37.6 15.7 31.8 38.7 16.6 32.8

Bảng 3: Kết quả tóm tắt WikiLingua. Những kết quả này sử dụng phiên bản GEM của tác vụ.

--- TRANG 4 ---
Phương pháp EM F1
mT5 (base - 512 input) 37.16 49.99
mT5 (base - 1k input) 43.09 56.36
mT5 (base - 2k input) 44.63 58.12
mT5 (base - 4k input) 45.41 58.63
mT5 (large - 512 input) 40.96 54.08
mT5 (large - 4k input) 52.77 66.54
mT5 (xl - 512 input) 43.84 56.98
mT5 (xl - 4k input) 55.03 68.26
mLongT5 (base - 4k input) 50.76 62.74
mLongT5 (base - 8k input) 51.21 63.66
mLongT5 (base - 16k input) 52.43 64.51
mLongT5 (large - 4k input) 54.04 66.75
mLongT5 (large - 8k input) 55.56 68.26
mLongT5 (large - 16k input) 55.93 68.66
mLongT5 (xl - 4k input) 58.52 70.86
mLongT5 (xl - 8k input) 59.6 71.86
mLongT5 (xl - 16k input) 60.42 72.63

Bảng 4: Kết quả TyDi QA.

5 Kết luận
Chúng tôi đã trình bày mô hình mới mLongT5. Nó có những lợi ích của kiến trúc hiệu quả của LongT5, với khả năng xử lý đầu vào và đầu ra đa ngôn ngữ. Như báo cáo của chúng tôi cho thấy, mô hình có thể hoạt động tốt trên nhiều tác vụ tóm tắt và hỏi đáp.

Hạn chế
mLongT5 có cùng hạn chế như thấy trong mô hình LongT5 ban đầu, ở chỗ chúng phù hợp hơn cho các tác vụ có đầu vào dài hơn. Các tác vụ có đầu vào ngắn hơn sẽ được phục vụ tốt hơn bởi các mô hình như mT5 và umT5, có thể tận dụng attention đầy đủ.

Tài liệu tham khảo
Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, và Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483.

--- TRANG 5 ---
Iz Beltagy, Matthew E. Peters, và Arman Cohan. 2020. Longformer: The long-document transformer.

Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, và Orhan Firat. 2023. UniMax: Fairer and more effective language sampling for large-scale multilingual pretraining.

Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, và Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454–470.

Jacob Devlin. 2018. Multilingual BERT README. https://github.com/google-research/bert/blob/master/multilingual.md.

Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, và Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120, Online. Association for Computational Linguistics.

Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontañón, Jianmo Ni, Yun-Hsuan Sung, và Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724–736, Seattle, United States. Association for Computational Linguistics.

Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, và Rifat Shahriyar. 2021. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693–4703, Online. Association for Computational Linguistics.

Taku Kudo và John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.

Faisal Ladhak, Esin Durmus, Claire Cardie, và Kathleen McKeown. 2020. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4034–4048, Online. Association for Computational Linguistics.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, và Luke Zettlemoyer. 2020. Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8:726–742.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683.

Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, và Jacopo Staiano. 2020. MLSUM: The multilingual summarization corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8051–8067, Online. Association for Computational Linguistics.

Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, và Donald Metzler. 2022. UL2: Unifying language learning paradigms.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, và Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483–498, Online. Association for Computational Linguistics.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2020. Big Bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems, volume 33, pages 17283–17297. Curran Associates, Inc.

Jingqing Zhang, Yao Zhao, Mohammad Saleh, và Peter J. Liu. 2020. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11328–11339. PMLR.

--- TRANG 6 ---
A XL-Sum
Chúng tôi hiển thị kết quả đầy đủ của việc chạy các mô hình mLongT5 trên XL-Sum trong Bảng 5. Những kết quả này là những kết quả đã được tải lên GitHub³ bởi các tác giả cùng với các tập dữ liệu được cập nhật.

Khi tính điểm ROUGE, chúng tôi sử dụng tính toán tương tự như được thực hiện trong bài báo tương ứng, với ngoại lệ đối với tiếng Trung, tiếng Nhật và tiếng Thái. Đối với những ngôn ngữ này, chúng tôi sử dụng SPM mà chúng tôi đã sử dụng trong mô hình để token hóa kết quả nhằm tính toán ROUGE.

³https://github.com/csebuetnlp/xl-sum

--- TRANG 7 ---
mT5 (base) mLongT5 (base) mLongT5 (large) mLongT5 (xl)
Ngôn ngữ R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
Amharic 20.05 7.41 18.08 16.70 5.91 14.73 20.29 7.99 18.09 22.37 8.90 19.91
Arabic 34.91 14.79 29.16 26.39 11.01 22.45 27.65 12.25 23.57 32.09 15.04 27.74
Azerbaijani 21.42 9.52 19.33 17.52 7.10 15.77 19.92 8.80 18.08 22.68 9.89 20.36
Bengali 29.57 12.11 25.13 21.39 8.22 18.65 24.69 10.04 21.25 26.83 11.32 22.86
Burmese 15.96 5.15 14.18 45.28 26.62 34.76 49.07 29.52 38.10 51.60 31.69 40.20
Chinese (Simp.) 39.41 17.79 33.41 38.90 21.78 32.59 42.62 24.70 35.80 48.42 29.99 41.28
Chinese (Trad.) 37.19 17.14 31.62 39.45 22.40 32.51 43.32 25.56 35.95 48.82 30.80 41.18
English 37.60 15.15 29.88 32.85 11.38 25.64 35.59 13.63 28.02 39.51 17.00 31.77
French 35.34 16.17 28.20 30.06 12.93 24.21 31.88 14.32 25.61 34.82 16.17 28.11
Gujarati 21.96 7.74 19.86 19.59 6.08 17.61 22.38 7.94 20.15 25.52 9.92 22.78
Hausa 39.44 17.68 31.67 34.61 13.73 27.30 38.04 16.07 30.32 40.58 18.57 32.52
Hindi 38.59 16.88 32.01 34.81 14.29 28.71 37.42 16.71 31.22 40.92 19.73 34.41
Igbo 31.61 10.16 24.53 25.82 8.05 20.19 30.41 10.01 23.68 31.31 9.88 24.07
Indonesian 37.00 17.02 30.76 32.15 13.05 26.59 35.17 15.23 29.07 38.87 18.00 32.64
Japanese 48.15 23.85 37.36 45.56 27.12 36.51 48.60 29.95 39.00 50.77 32.06 40.79
Kirundi 31.99 14.37 25.83 25.61 10.07 20.26 29.36 12.78 23.67 31.67 14.55 25.50
Korean 23.67 11.45 22.36 20.25 9.20 19.00 23.18 10.42 21.38 25.30 11.63 23.31
Kyrgyz 18.38 7.96 16.50 14.08 5.27 12.46 16.01 6.30 14.14 18.19 7.81 16.00
Marathi 22.01 9.54 19.92 20.33 8.62 18.41 23.35 10.56 21.22 25.90 12.03 23.07
Nepali 26.65 10.25 24.28 23.96 8.94 21.80 26.24 10.33 23.91 28.87 11.59 26.17
Oromo 18.70 6.17 16.19 14.88 4.38 12.71 17.91 5.65 15.28 19.52 6.50 17.18
Pashto 38.47 15.55 31.91 35.01 13.79 28.84 38.63 16.06 32.00 41.37 17.61 33.92
Persian 36.94 16.19 30.07 35.47 14.66 28.40 37.70 16.45 30.49 40.64 18.89 33.16
Pidgin 37.96 15.12 29.87 33.86 12.01 26.68 35.86 13.72 28.24 38.01 15.08 29.78
Portuguese 37.17 15.90 28.56 31.67 12.51 24.46 34.04 14.51 26.65 37.66 17.57 29.88
Punjabi 30.70 12.21 25.52 28.61 10.43 23.66 31.92 12.75 26.17 34.45 14.81 28.42
Russian 32.22 13.64 26.17 22.11 8.29 18.62 24.39 10.00 20.54 28.20 12.72 23.91
Scottish Gaelic 29.02 10.99 22.88 26.98 8.87 21.57 29.80 10.64 23.44 31.74 12.61 25.65
Serbian (Cyrillic) 23.78 7.98 20.14 20.30 5.86 16.74 21.92 6.98 18.35 27.51 11.46 23.49
Serbian (Latin) 21.64 6.66 18.23 18.14 4.75 14.96 21.79 6.92 18.14 25.86 10.17 21.76
Sinhala 27.29 13.38 23.47 22.69 10.02 19.96 25.24 11.52 21.98 27.78 13.20 24.45
Somali 31.56 11.58 24.22 27.85 9.08 21.10 30.29 10.69 23.29 31.64 11.11 24.28
Spanish 31.51 11.88 24.07 26.82 9.05 20.47 28.71 10.56 22.04 32.20 13.10 24.88
Swahili 37.67 17.85 30.91 31.79 13.25 25.67 34.29 15.22 27.82 37.29 17.22 30.96
Tamil 24.33 11.06 22.07 20.68 8.67 18.71 24.08 10.74 21.71 26.81 12.23 24.21
Telugu 19.86 7.03 17.61 15.11 4.69 13.48 17.98 6.12 16.10 21.20 7.77 18.88
Thai 37.40 17.28 28.88 35.98 21.39 26.65 38.11 22.92 28.26 40.70 25.23 30.12
Tigrinya 25.32 8.02 21.17 22.27 7.08 18.61 26.30 8.90 22.05 28.53 10.13 24.05
Turkish 32.93 15.57 29.26 25.52 11.54 22.83 28.56 13.62 25.72 31.33 15.61 28.20
Ukrainian 23.99 10.14 20.92 20.97 8.16 18.17 23.34 9.74 20.29 27.05 12.16 23.68
Urdu 39.56 18.37 32.84 37.11 15.97 30.14 39.90 18.53 32.75 43.03 21.40 35.72
Uzbek 16.83 6.34 15.41 14.60 5.36 13.39 17.26 6.42 15.49 19.18 7.80 17.29
Vietnamese 32.88 16.22 26.08 31.58 15.41 25.02 34.54 17.63 27.59 38.17 20.49 30.98
Welsh 32.66 11.60 26.12 29.96 9.40 23.96 33.66 12.26 27.01 36.49 15.34 29.79
Yoruba 31.66 11.66 25.09 25.87 8.99 20.27 29.49 10.50 23.26 32.20 12.34 25.84

Bảng 5: Kết quả đầy đủ cho XL-Sum.
