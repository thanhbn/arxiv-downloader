# 2401.07037.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2401.07037.pdf
# File size: 1011370 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
XCOT: Cross-lingual Instruction Tuning for Cross-lingual
Chain-of-Thought Reasoning
Linzheng Chai1, Jian Yang1∗, Tao Sun1, Hongcheng Guo1, Jiaheng Liu1,
Bing Wang1,Xiannian Liang1,Jiaqi Bai1,Tongliang Li3,Qiyao Peng2,Zhoujun Li1
1State Key Lab of Software Development Environment, Beihang University
2School of New Media and Communication, Tianjin University
3Beijing Information Science and Technology University
{challenging, jiaya, buaast, hongchengguo, liujiaheng, bingwang, xnliang}@buaa.edu.cn;
{bjq, lizj}@buaa.edu.cm; qypeng@tju.edu.cn; tonyliangli@bistu.edu.cn;
Abstract
Chain-of-thought (CoT) has emerged as a pow-
erful technique to elicit reasoning in large lan-
guage models and improve a variety of down-
stream tasks. CoT mainly demonstrates excel-
lent performance in English, but its usage in
low-resource languages is constrained due to
poor language generalization. To bridge the
gap among different languages, we propose
a cross-lingual instruction fine-tuning frame-
work ( XCOT) to transfer knowledge from high-
resource languages to low-resource languages.
Specifically, the multilingual instruction train-
ing data ( XCOT-INSTRUCT ) is created to en-
courage the semantic alignment of multiple lan-
guages. We introduce cross-lingual in-context
few-shot learning (xICL) to accelerate multi-
lingual agreement in instruction tuning, where
some fragments of source languages in exam-
ples are randomly substituted by their counter-
part translations of target languages. During
multilingual instruction tuning, we adopt the
randomly online CoT strategy to enhance the
multilingual reasoning ability of the large lan-
guage model by first translating the query to
another language and then answering in En-
glish. To further facilitate the language trans-
fer, we leverage the high-resource CoT to su-
pervise the training of low-resource languages
with cross-lingual distillation. Experimental re-
sults on previous benchmarks demonstrate the
superior performance of XCOTin reducing the
gap among different languages, highlighting its
potential to reduce the cross-lingual gap1.
1 Introduction
Recent advancements in Large Language Models
(LLMs) (Touvron et al., 2023a,b; Patel et al., 2023;
OpenAI, 2023; Bai et al., 2023) in natural language
processing (NLP) have intensively engaged the in-
terests of researchers. LLMs (Wei et al., 2022c;
Zhang et al., 2023; Kojima et al., 2022a) are further
∗Corresponding Author.
1The dataset and code will be released.
MultilingualQuestionLLMThEnZhDeFrThink in EnglishAnswerCode-switched ContextMultilingual QuestionAlignFigure 1: Illustration of XCOT. The cross-lingual in-
struction tuning is used to align representations of dif-
ferent languages.
equipped with the chain-of-thought (CoT) tech-
nique to gain impressive performance in complex
reasoning tasks, where LLMs first produce inter-
mediate reasoning steps and infer the final answer.
However, existing studies related to the CoT
methods are mainly constrained in high-resource
languages (e.g. English) and deliver little consider-
ation into multilingual scenarios. Recent works
(Shi et al., 2023; Qin et al., 2023; Chen et al.,
2023a) endeavor to simply use prompt engineer-
ing to improve the language generalization abil-
ity of the model without any fine-tuning. These
prompt-based methods ignore the potential of
representation-based cross-lingual alignment de-
rived from the cross-lingual supervised fine-tuning
(cross-lingual SFT). Supervised fine-tuning has
been shown to perform at a satisfactory level across
various tasks, such as FLAN (Wei et al., 2022a) and
InstructGPT (Ouyang et al., 2022). Therefore, how
to encourage cross-lingual alignment in supervised
fine-tuning still requires further exploration.
To minimize the gap among different languages,
we propose a Cross-lingual Chain-of-Thought rea-
soning ( XCOT) framework using cross-lingual su-
pervised instruction fine-tuning. Specifically, we
first construct the multilingual instruction train-arXiv:2401.07037v1  [cs.CL]  13 Jan 2024

--- PAGE 2 ---
EnZhDeSpan-level Codeswitch…English: John writes 20 pages a day. How long will it take him to write 3 books that are 400 pages each?German: John schreibt20 Seiten am Tag. Wie langewirder brauchen, um dreiBüchermitjeweils400 Seiten zuschreiben?
Large Language ModelCode-switch: John write 20Seiten am Tag. Wie langewirder brauchen, um dreiBüchermitare 400 pages each?Code-switched ContextEnglish QuestionEnglish CoTAnswerCode-switched ContextChinese QuestionChinese CoTAnswer
Multilingual CorporaCode-switched Corpora
Random Online CoTMultilingual SFTSFTon English
Epoch1−n
Code-switched Rejection Sampling
Enhanced Code-switched Corpora
EpochnCode-switched SFT 
Cross-lingual DistillationTranslation
FrFramework of Cross-lingual CoTFigure 2: Overview of XCOT. The cross-lingual in-context few-shot learning (xICL) encourages multilingual
alignment in instruction tuning, where the query in the example is mixed with different language tokens. During
multilingual instruction tuning, the randomly online CoT strategy (Random-CoT) is used to promote the multilingual
reasoning ability of LLM and then answer in English. Finally, we leverage the high-resource CoT to supervise the
training of low-resource languages with cross-lingual distillation.
ing data ( XCOT-INSTRUCT ) by translating English
to other languages. Then, we randomly substi-
tute some fragments of source languages in ex-
amples by their counterpart translations of target
languages. To transfer high-resource languages
to low-resource languages, we mix the tokens of
the source and target language in the same query
to enable the LLMs to handle different languages.
The code-switched examples and the query can be
applied to cross-lingual in-context learning in su-
pervised instruction tuning. During multilingual
instruction tuning, we adopt the randomly online
CoT strategy to enhance the multilingual reason-
ing ability of the large language model by first
translating the query to another language and then
answering in English. To further facilitate the lan-
guage transfer, we leverage the high-resource CoT
to supervise the training of low-resource languages
with cross-lingual distillation. Experimental results
on previous benchmarks demonstrate the superior
performance of XCOTin reducing the gap among
different languages, highlighting its potential to
reduce the cross-lingual gap.
Extensive experiments of XCOTare evaluated
on multilingual benchmarks MGSM of 11 lan-
guages and MSV AMP of 10 languages. The re-
sults demonstrate that our proposed method consis-
tently achieves state-of-the-art performance across
all languages, notably surpassing strong baseline
by an average margin of 15%. The contributions in
this work are summarized as follows: (1) We con-struct the multilingual instruction data to transfer
knowledge of high-resource languages into low-
resource languages. The training data is further
augmented by cross-lingual in-context learning,
where a piece of code-switched demonstration con-
text and the current query are concatenated as the
input for LLM. (2) During training, we propose
the random online CoT (Random-CoT), which first
randomly translates the query into other languages
and then answers in English. (3) To align the repre-
sentations of different languages, we propose cross-
lingual knowledge to align the output distribution
given the queries of different languages using Kull-
back–Leibler divergence.
2 Cross-lingual CoT Reasoning
Given the query q= (q1. . . , q n)of language Li,
the large language model (LLM) Moutputs the
corresponding answer a= (a1, . . . , a n)of lan-
guage Lj, where mandnare lengths of prompt
and answer in a sample (q, a).LiandLjare source
and target language, where Lall={Lk}K
k=1andK
is number of languages. LLM further enhances the
task performance by chain-of-thought reasoning,
where the chain-of-thought examples of sequences
c= (c1, . . . , c t)are added into the exemplars of
prompting. The high-quality rationales ccom-
prised of a series of intermediate natural language
reasoning steps provide helpful suggestions for the
final output. Given multiple chain-of-thought ex-
amples as demonstrations and the original prompt

--- PAGE 3 ---
qof the target language as a whole, the problem
definition of cross-lingual CoT is described as:
P(a|q, c) =nY
j=1P(aj|a<j;q, c,M) (1)
where q(question) and c(corresponding exemplars)
are concatenated as a whole pto predict the answer
denoted as P(a|p). Driven by the CoT demonstra-
tions c, the LLM first generates the intermediate
steps and then outputs the final answer a.
3XCOT
3.1 Model Overview
Figure 2 describes the overall framework of our
method XCOT. Specifically, the cross-lingual in-
context few-shot learning (xICL) encourages mul-
tilingual alignment in instruction tuning, where
the query in the example is mixed with different
language tokens. During multilingual instruction
tuning, the randomly online CoT strategy (Random-
CoT) is used to promote the multilingual reasoning
ability of LLM and then answer in English. Finally,
we leverage the high-resource CoT to supervise
the training of low-resource languages with cross-
lingual distillation.
3.2 XCOT-INSTRUCT
Data Construction We create a new multilin-
gual instruction dataset ( XCOT-INSTRUCT ) for
cross-lingual chain-of-thought reasoning, which
can be used as the training corpora for multilingual
benchmarks, such as MGSM (Shi et al., 2023) and
MSV AMP (Chen et al., 2023a). We use the mul-
tilingual translator to expand the English instruc-
tion data2into other 10 languages, including Ger-
man, French, Spanish, Russian, Chinese, Japanese,
Thai, Telugu, Bengali, and Swahili. The instruction
dataset of each language contains 7.4K samples,
where we only translate the query into other lan-
guages and retain the response in English to facil-
itate the cross-lingual transfer. Finally, we obtain
the multilingual instruction data D={DLk}K
k=1
and(qLi, cLi, aLj)∈DLi, where DLiis the SFT
training data of language Liand the number of
the languages is K.DLicontains query qLiand re-
sponse aLjwith the corresponding context cLi.qLi
is the query of source language and aLjis the re-
sponse of the high-resource language ( Ljis English
in our work). cLi={qLi
b, aLj
b}B
b=1is the context
2https://github.com/openai/grade-school-mathAlgorithm 1: Random Online CoT
Input: Multilingual Instruction Dataset: D;
Multilingual LLM: M;
Maximum supervised fine-tuning step: T;
Batch size: B;
Target language set: Lall={Lk}K
k=1;
Output: Fine-tuned LLM: M
1t←0
2while t≤T;do
3 Random sampled batch B ∈D
4 fork←1toB;do
5 (cLi,j, qLi, aLj)← B
6 Lk∼U(Lall) (Lk̸=Lj)
7 qLk← M ([cLi,j, qLi,yk])
// Translate qLi→qLk
8 aLj← M ([cLi,j, qLi, qLk,yk])
// Answer in language
Lj
9 B ← B ∪ (x′
k,yk, tk)
10 Optimize MwithB
11 i←i+ 1
12return M
demonstration comprised of Bqueries of language
Liand the responses of Lj. For each language, we
construct about 22K data context demontstration
samples.
Cross-lingual Instruction Tuning Given the
cross-lingual instruction corpora D={DLk}K
k=1,
where Dcontains Klanguages and Lall=
{Lk}K
k=1. The LLM is jointly trained on the union
of the multilingual corpora D:
Lx=−KX
i=1E
cLi,qLi,aLj∼DLih
logP(aLj|qLi, cLi;M)i
(2)
where qLiis the query of the language LiandaLj
is the response of language Lj.
3.3 Cross-lingual In-context Learning
To encourage cross-lingual alignment across dif-
ferent languages, we construct the code-switched
query by replacing the spans of the source query
with the counterparts of the target language.
Code-Switched Sequence Given a bilingual
query (qLi, qLj)with the source language query
qLi={qLi
1, . . . , qLim}ofmtokens and the target
translation qLj={yLj
1, . . . , yLjn}ofntokens, we
create the code-switched sequence qLi,jby sub-

--- PAGE 4 ---
stituing the phrase qLi
jwith counterpart transla-
tionqLjv1:v2, where qLiv1:v2is the target translation of
source piece qLju1:u2.qLiu1:u2denotes the phrase in
qLifrom the u1-th token to the u2-th token and
qLjv1:v2denotes the phrase in qLjfrom the v1-th to-
ken to the v2-th token ( 1≤v1≤v2≤n). For
each phrase in code-switched in qLi,j, it comes
from source phrase qLiu1:u2or target phrase qLjv1:v2.
The proportion of the source words in the code-
switched sequence qLi,jis denoted as α.Li,jcon-
tainsqLi/j(source sentence with target tokesn) and
qLi/j(target sentence with target tokesn).
Specifically, the code-switched sequence can be
created in two ways: (1) qLi/j(source sentence
with target tokens): most tokens in qLi/jderive
from qLi, where some source phrases qLiu1:u2are
substituted by their target counterpart phrases qLi2v1:v2
(α≥0.5). (2) qLj/i(target sentence with source
tokens): most tokens in qLj,iderive from qLjwhere
some target phrases qLjv1,v2are substituted by their
source counterpart phrases qLiu1,u2(α <0.5).
3.4 Random Online CoT
To force the model to understand the multilin-
gual queries, we introduce the random online CoT
(Random-CoT), which first prompts the LLM to
translate the query qLi1to another language qLi2
and then answer in aLjduring the LLM tuning.
Random Online CoT To scale up to multilin-
gual CoT, we perform online CoT by randomly
sampling intermediate languages Li2. Algorithm 1
describes the detail of Random-CoT, where given
the training instance (cLi1,j, qLi1, aLj)∈D, we
uniformly sample an intermediate language Li2
(Li2̸=Li1) and prompt LLM first to translate qLi1
toqLi2. Although Li2may belong to low-resource
languages and the quality of qLi2may be poor ini-
tially, our method still benefits from the translation
signal of qLi1→qLi2by aligning the representa-
tions of different languages.
3.5 Cross-lingual Distillation
To further augment the cross-lingual instruction
tuning, we use the fine-tuned LLM Mto generate
the synthetic response of the multilingual queries
and then select correct reasoning paths as the aug-
mented dataset D′. Finally, our model is trained on
the original dataset and augmented dataset D∪D′.
Then, we use the high-resource sample to
supervise the low-resource sample to transferknowledge from high-resource to low-resource
languages. Given the parallel high-resource
sample (cLi,j, qLi, aLj)and low-resource sam-
ple(cLk,j, qLk, aLj), the model separately pre-
dict the target distribution p(aLj|cLi,j, qLi)and
p(aLj|cLk,j, qLk). Since qLiandqLkare se-
mantically equal, we can leverage distribution
Phigh =p(aLj|cLi,j, qLi)to supervise Plow=
p(aLj|cLk,j, qLk)in token level:
Ld=−1
nnX
t=1
Pt
highlogPt
low
(3)
where Pt
highandPt
lowis the t-token distribution in
answer. Through the token-level cross-lingual dis-
tribution, we transfer the high-resource knowledge
to low-resource languages.
4 Experiments
4.1 Cross-lingual Supervised Fine-tuning
For each question in the dataset, we randomly se-
lect 2 other questions and corresponding answers
as the context. we set 0< α < 1with a 0.8re-
placement threshold to perform the code-switch
operation on the question in context. Specifically,
we use English as the source language, and the
language corresponding to the question is used as
the target language. For the English question, we
randomly select an other language as the target lan-
guage. We implement our model based on Llama-
2-7B, Llama-2-13B, and Bloom-7b1. We finetune
these models with 3 epochs and use a cosine sched-
uler with a learning rate of 2e-5 and set 3% warm
up. For cross-lingual distillation, we set β= 0.3.
4.2 Evaluation
To comprehensively assess the cross-lingual pro-
ficiency of XCOT, we evaluate the method using
the MGSM (Shi et al., 2023) benchmark, which
extends the English GSM8K (Cobbe et al., 2021)
dataset into ten typologically varied languages
through the manual translation of problems. To
conduct a thorough and wide-ranging evaluation
of the multilingual mathematical problem-solving
skills, we have also created an additional out-of-
domain test dataset called MSV AMP (Chen et al.,
2023a), originating from the SV AMP (Patel et al.,
2021) dataset. This dataset incorporates mathemat-
ical problems in 10 different languages, initially
translated using machine translation and subse-
quently refined through careful human review and
correction for accuracy and nuance. Finally, our

--- PAGE 5 ---
Method Base Model En De Fr Es Ru Zh Ja Th Te Bn Sw Avg.
Closed-Source Models
Native-CoT†GPT-3.5 67.2 62.0 59.2 61.2 50.4 52.8 46.8 15.6 – 7.6 40.0 46.3
Native-CoT†GPT-4 80.0 73.6 72.0 71.2 64.0 70.0 71.6 40.4 – 17.6 64.4 62.5
Open-Source Models (7B)
Llama-2†(Touvron et al., 2023b) Llama-2 43.2 37.2 34.4 32.4 28.0 22.4 15.2 4.8 – 3.2 5.2 22.6
RFT†(Yuan et al., 2023) Llama-2 44.8 33.6 34.0 34.0 29.2 16.8 6.8 2.0 – 2.4 2.8 20.6
MAmmoTH†(Yue et al., 2023) Llama-2 49.6 33.2 32.8 32.4 26.0 17.2 10.8 4.8 – 3.6 2.4 21.3
WizardMath†(Luo et al., 2023) Llama-2 47.6 30.4 30.4 34.8 30.8 22.4 24.0 4.0 – 2.0 3.4 23.0
MathOctopus†(Chen et al., 2023b) Llama-2 54.8 43.6 38.0 45.2 48.4 45.2 35.6 36.4 – 33.2 38.4 41.9
XCOT Bloom 30.0 30.4 28.8 32.4 33.6 30.0 29.6 28.4 28.4 33.2 26.8 30.1
XCOT Llama-2 48.4 47.2 49.6 48.8 50.0 50.0 50.0 49.2 42.8 40.4 48.4 47.7
Open-Source Models (13B)
LLama-2†(Touvron et al., 2023b) Llama-2 50.4 42.8 40.8 45.2 39.2 32.8 25.2 6.8 – 6.0 7.6 29.7
RFT†(Yuan et al., 2023) Llama-2 52.0 38.4 44.8 46.8 41.6 33.6 26.4 4.4 – 3.2 3.6 29.5
MAmmoth†(Yue et al., 2023) Llama-2 56.4 45.6 39.6 50.0 36.8 31.2 19.2 5.2 – 3.6 1.6 28.9
WizardMATH†(Luo et al., 2023) Llama-2 52.8 40.4 42.0 45.6 34.4 28.0 22.0 5.6 – 6.4 5.6 28.4
MathOctopus†(Chen et al., 2023b) Llama-2 51.6 49.2 49.6 53.2 47.6 51.2 39.6 46.0 – 42.0 46.0 47.6
XCOT Llama-2 54.4 52.4 46.4 54.8 56.8 54.0 49.6 50.0 47.2 50.0 51.6 51.5
Table 1: Multilingual evaluation results on the MGSM benchmark. †: Results from (Chen et al., 2023b), for
MathOctopus, we uniformly report the performance under xRFT and parallel-training settings.
Method Base Model En De Fr Es Ru Zh Ja Th Bn Sw Avg.
Closed-Source Models
Native-CoT†GPT-3.5 81.2 73.9 78.2 74.6 70.9 78.4 74.0 46.0 14.4 68.4 66.0
Native-CoT†GPT-4 80.1 78.1 83.9 81.5 77.9 78.9 74.8 68.1 31.2 75.7 73.0
Open-Source Models (7B)
Llama-2†(Touvron et al., 2023b) Llama-2 38.8 39.0 39.1 39.2 39.1 35.2 31.6 18.2 11.5 17.2 30.9
RFT†(Yuan et al., 2023) Llama-2 42.7 40.8 41.5 42.5 39.5 34.9 33.9 16.9 7.7 14.9 31.5
MAmmoTH†(Yue et al., 2023) Llama-2 45.1 39.6 39.9 42.9 33.7 26.8 26.7 6.3 4.3 4.2 27.0
WizardMath†(Luo et al., 2023) Llama-2 48.5 39.2 37.7 44.8 37.4 36.3 37.9 17.0 16.1 10.3 32.5
MathOctopus†(Chen et al., 2023b) Llama-2 46.8 43.1 45.3 44.5 42.1 43.2 43.2 40.5 32.8 42.3 42.4
Table 2: Multilingual evaluation results on the MSV AMP benchmark. †: Results from (Chen et al., 2023b), for
MathOctopus, we uniformly report the performance under xRFT and parallel-training settings.
method is evaluated on MGSM (Shi et al., 2023)
and MSV AMP (Chen et al., 2023a) with the ac-
curacy metric. In the experiments, we report the
accuracy of all methods.
4.3 Baselines
XCOTare mainly compared with: (1) close-source
LLM GPT-3.5, GPT-4; (2) open-source models
Llama-2 and Bloom. XCOTprimarily conduct ex-
periment base on Llama-2 and compare with other
Llama-2 based methods RFT, MathOctopus, MAm-
moTH, WizardMath, etc. Futhermore, we select
Bloom as base model to explore the performance
ofXCOT when combined with multilingual LLM.
4.4 Main Results
MGSM Table 1 presents the results of our
method and previous baselines on MGSM of 11
languages, including En, De, En, Fr, Es, Ru, Zh,
Ja, Th, Te, Bn, Sw. Compared to the open-source
baseline Llama-2, MAmmoTH (Yue et al., 2023)
trained with a union of chain-of-thought (CoT) and
program-of-thought (PoT) gains strong improve-
ment. Our method significantly outperforms the
previous strong baseline MAmmoTH by an averageof points. It can prove that our method can lever-
age cross-lingual in-context learning (xICL) and
random online CoT (Random-CoT) to encourage
alignment across different languages.
MSVAMP Table 2 compares the performance
of our method with previous relevant methods on
MSV AMP of 10 languages. The recent strong
multilingual baseline MathOctopus beats the previ-
ous baselines MAmmoth and WizardMath with
the help of the multilingual instruction dataset
MGSM8KInstruct. Further, our proposed method
gains the best performance of 42.9points in 7B
level across all languages, demonstrating that our
proposed framework strengthens transferability
from the high-resource languages to all other lan-
guages.
5 Analysis
Ablation Study To verify the effectiveness of
each module in our method, we conduct an ab-
lation study by adding modules gradually. The
multilingual LLM Llama-7B is first trained on the
multilingual corpora XCOT-INSTRUCT , where the
model is denoted as ①. Compared to the initial

--- PAGE 6 ---
Llama-2-7B Llama-2-13B
k De Fr Es Ru Zh De Fr Es Ru Zh Avg.
10 1.68 1.67 1.68 1.80 1.69 1.98 1.96 1.98 1.98 1.96 7.22
20 2.58 2.56 2.59 2.66 2.59 2.95 2.95 2.97 2.97 2.94 11.02
30 3.21 3.24 3.21 3.35 3.22 3.70 3.71 3.70 3.73 3.69 13.93
50 4.32 4.29 4.34 4.47 4.35 4.93 4.93 4.91 4.72 4.91 18.43
Table 3: Distinct reasoning paths of each language with
different sampling times. Different reasoning paths per
question are generated by different SFT models with
different k.
model①, the model ②with the code-switched
context in multilingual tuning gains the improve-
ment of +4.7points on average, which shows the
usage of xICL in encouraging alignment across
different languages. Then, the model ③is fur-
ther enhanced with mSampling by a large margin
+5.8points, where the model generates the mul-
tilingual responses and chooses correct reasoning
paths as the augmented dataset. During multilin-
gual tuning, our method adopts Random-CoT to
first translate the query to another language and
then answer in English. For the output distribution,
the high-resource distribution is used to supervise
the low-resource distribution (xDistill). Putting
them all together, we obtain the final model XCOT
(⑤) with 47.7points. Table 4 summarizes the re-
sults of the ablation study of cross-lingual transfer
in different parts, which emphasizes the effective-
ness of cross-lingual transfer that can gradually
improve performance in different aspects.
Cross-lingual Prompting To trigger the cross-
lingual potential capability of LLM, we introduce
xICL to force the model to understand the multi-
lingual queries and align their representations. To
advance multilingual agreement in instruction tun-
ing, we randomly replace some fragments of source
languages in examples with their counterpart trans-
lations of target languages for cross-lingual in-
context few-shot learning (xICL). Table 6 shows
the results of XCOTwith English context, native
context, and code-switched context on different
backbones. The query mixed with different lan-
guage tokens brings significant improvement in
different languages.
Example Study Given the queries of different
languages, our method prompts LLM to first con-
sider the multilingual query in English and then
answer in English. Table 7 shows examples of
the Spanish, Chinese, and German baseline. We
observe that Llama-2 tends to generate incorrect
answers for non-English queries. For the German
bn
de
en
es
fr
ja
ru
sw
te
th
zh(a) Baseline
bn
de
en
es
fr
ja
ru
sw
te
th
zh (b) XCOT
Figure 3: (a) and (b) are representations of Llama-7B
and our method from the last decoder layer. Each color
denotes one language (11 languages in MGSM).
Question “Jimmy hat 2 $ mehr als doppelt so viel
Geld wie Ethel. Wenn Ethal 8 $ hat, wie viel Geld
hat dann Jimmy im Moment?”, our method first
thinks the non-English query in English “Ques-
tion:Jimmy has $2 more than twice the money
Ethel has. If Ethel has $8, how much money does
Jimmy have now?”. and then answer in English. It
proves that our method can align both query and
response across different languages.
Cross-lingual Reasoning Path Our multilingual
instruction data is augmented by multilingual sam-
pling, where the fine-tuned LLM generates the re-
sponse and selects the correct path. Table 3 shows
that different languages have a similar number
of reasoning paths, which proves that using the
cross-lingual CoT successfully transfers reasoning
patterns from one language to another language.
XCOTcan accumulate all reasoning paths to im-
prove the model performance.
Multilingual Representations We randomly se-
lect 250 parallel queries with their 2-shot exam-
ples of each language in XCOT-INSTRUCT and
visualize their representations (Maaten and Hinton,
2008) of the last Llama decoder layers in Figure 3
using our multilingual model fine-tuned on XCOT-
INSTRUCT and the multilingual baseline. The first
hidden state of the encoder is adopted as the sen-
tence representation. Compared to Figure 3(a) of
the baseline, different languages become closer and
more likely to overlap with each other in Figure
3(b) of our method, demonstrating that our method
effectively aligns representations of different lan-
guages to the shared space.
Understanding and Reasoning in English After
the cross-lingual SFT with Random-CoT, XCOT
chooses the high-resource language (English) as
the auxiliary language to understand and answer
the non-English question. In Figure 4, our method

--- PAGE 7 ---
ID Method En De Fr Es Ru Zh Ja Th Te Bn Sw A VG
① Llama-2-7B 38.4 37.2 37.6 39.2 37.6 30.4 29.6 26.8 13.2 23.2 18.4 30.1
② +xICL 45.6 38.4 33.6 36.4 40.4 37.2 32.0 32.0 26.0 30.0 32.0 34.8
③ +mSampling 50.8 41.2 42.0 44.4 42.0 40.4 38.0 40.8 35.6 34.8 37.6 40.6
④ +Random-CoT 48.0 50.4 46.0 48.4 46.8 51.2 46.4 47.2 41.6 41.2 46.8 46.7
⑤ +xDistill 48.4 47.2 49.6 48.8 50.0 50.0 50.0 49.2 42.8 40.4 48.4 47.7
Table 4: Ablation study on MGSM based on Llama-2-7b. XCOT is the final model of our method.
Random-CoT Direction En De Fr Es Ru Zh Ja Th Te Bn Sw Avg.
Lall→En 50.8 41.2 42.0 44.4 42.0 40.4 38.0 40.8 35.6 34.8 37.6 40.6
Llow→En∪Lhigh→Lhigh 50.8 45.6 49.6 46.4 49.6 41.2 42.4 42.8 37.6 38.8 42.8 44.3
Lall→Lall 46.0 46.4 45.2 50.0 48.0 50.4 47.6 43.2 40.8 40.4 45.2 45.7
Lall→Lhigh 48.4 47.2 49.6 48.8 50.0 50.0 50.0 49.2 42.8 40.4 48.4 47.7
Table 5: Translation direction of Random-CoT.
(a)Prompt for English QueryBelow is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction:{Question}### Response: Let's think step by step in English.(b)Prompt for Other Language QueryBelow is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction:{Question}### Response: Let's think the question in {Language} and then think step by step in English.
Figure 4: The prompt of thinking in English.
uses “Let’s think in step by step in English ” to an-
swer the English question. For the non-English
question, we adopt “Let’s think the question in
{Language} and then think step by step in English ”,
where {Language} can be high-resource languages
in SFT tuning but we set textit{Language} as En-
glish during inference stage. To effectively transfer
knowledge from high-resource to low-resource lan-
guages, we force LLM to understand the query in
English and then think in English.
Analysis in Random-CoT To facilitate the align-
ment among different languages, the question of
language Li1is first translated into another lan-
guage Li2in SFT tuning. Given the query of lan-
guage Li1, we can translate another language Li2
(Li1̸=Li2). The strategy “ Lall→Le” denotes
that the Li1∈Lall∧Li2=Le. Table 5 shows the
results of our method with different Random-CoT
strategies and the strategy “ Lall→Lhigh” gets the
best performance, which can be attributed the lan-
1K 2K 3K 4K 5K 6K ALL
Size of Tuning Data203040Accuracy (%)
En
De
Fr
Es
Ru
Zh
Ja
Th
Te
Bn
Sw
Llama-7BFigure 5: Multilingual evaluation results on MGSM
with different SFT data size.
guage transfer from high-resource to low-resource
languages.
Low-resource Setting Figure 5 plots the multi-
lingual evaluation results of XCOTwith different
SFT data sizes. We observe that our method with
nearly 20% SFT data can still beat the strong base-
line Llama-7B, which can attributed to the mutual
reinforcement of multiple languages.
6 Related Work
Large Language Models Large language mod-
els (LLMs) has shown great power in numerous
NLP tasks, and as the scale of the model gets larger,
LLMs emerge with surprising capabilities (Tou-
vron et al., 2023c; Wei et al., 2022b; Du et al.,
2022; Guo et al., 2023), such as following human
instructions, in-contextual learning, and reasoning
complex tasks. Wei et al. (2022d) found that LLM
can solve complex problems efficiently by chain-
of-thought prompting strategy (providing some ex-
emplars containing reasoning steps to guide the
model to generate intermediate reasoning steps).
Moreover, Kojima et al. (2022b) found that LLMs
can solve complex problems by CoT even without

--- PAGE 8 ---
Base Model CoT En De Fr Es Ru Zh Ja Th Te Bn Sw A VG
Bloom-7BEn-Context 32.8 31.2 30.4 30.0 34.0 29.6 28.0 27.6 27.2 30.4 28.4 29.9
Native-Context 32.8 30.0 25.6 32.0 31.2 30.0 25.2 28.8 25.6 30.0 30.4 29.2
Codeswitch-Context 30.0 30.4 28.8 32.4 33.6 30.0 29.6 28.4 28.4 33.2 26.8 30.1
Llama-2-7BEn-Context 50.0 46.4 50.0 47.2 47.6 50.4 47.6 44.8 42.0 39.2 48.4 46.7
Native-Context 50.0 48.4 44.0 48.8 47.6 46.4 45.6 45.6 42.0 41.2 44.4 45.8
Codeswitch-Context 48.4 47.2 49.6 48.8 50.0 50.0 50.0 49.2 42.8 40.4 48.4 47.7
Llama-2-13BEn-Context 50.8 50.4 48.0 50.4 55.6 48.4 48.0 54.0 44.8 45.6 49.2 49.5
Native-Context 50.8 52.0 49.2 49.2 56.0 50.0 49.6 52.8 46.0 45.2 48.8 49.9
Codeswitch-Context 50.8 51.6 49.2 50.8 56.0 50.0 48.8 54.0 44.8 46.8 50.0 50.2
Table 6: Cross Lingual Promping
Spanish Question Dan planta 3 rosales. Cada rosal tiene 25 rosas. Cada rosa tiene 8 espinas. ¿Cuántas espinas hay en total?
Llama-2There are 3 x 25 = 75 roses.There are 75 x 8 = 600 thorns.
There are 600 x 2 = 1200 forks. The answer is 1200. ✘
XCOTQuestion:Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns.
How many thorns are there in total? Let’s think step by step in English.
There are 25*3=«25*3=75»75 roses So there are 75*8=«75*8=600»600 thorns. The answer is 600. ✔
Chinese Question 格雷琴有110个硬币。金币比银币多30个。格雷琴有多少个金币？
Llama-2Let x be the number of silver coins Gretchen has. Gold coins are 30 more than silver coins, so each gold coin
is worth x + 30. The total value of the gold coins is the sum of the value of each gold coin, which is x + 30,
so the total value is x + 30 * x + 30 = 110. So the equation is 30x + 30 = 110.
So the value of x is 110 / 30 = 3.5. So Gretchen has 3.5 gold coins. The answer is 3.5. ✘
XCOTQuestion:Gretchen has 110 coins. She has 30 more gold coins than silver coins. How many gold coins
does Gretchen have? Let’s think step by step in English. Let G be the number of gold coins Gretchen has.
The number of silver coins is G - 30. The total number of coins is G + (G - 30) = 110.
Combining like terms, we get G*2 - 30 = 110. Adding 30 to both sides, we get G*2 = 140.
Dividing both sides by 2, we get G = 70. The answer is 70. ✔
German Question Jimmy hat 2 $ mehr als doppelt so viel Geld wie Ethel. Wenn Ethal 8 $ hat, wie viel Geld hat dann Jimmy im Moment?
Llama-2 Jimmy’s money is 2 * $8 = $16. The answer is 16. ✘
XCOTQuestion:Jimmy has $2 more than twice the money Ethel has. If Ethel has $8, how much money does Jimmy have now?
Let’s think step by step in English. Twice the money Ethel has is 2 * 8 = «2*8=16»16 dollars. Jimmy has 2
more than twice the money Ethel has, which means he has 16 + 2 = «16+2=18»18 dollars. The answer is 18. ✔
Table 7: Comparison between Llama-2 and our proposed method within 7B-level on MGSM.
providing exemplars. However, the CoT capabil-
ity usually requires the model to have a particu-
larly large number of parameters and require mas-
sive computational resources. There is also some
works (Ho et al., 2022; Zhu et al., 2023) that ex-
plore the smaller LLMs’ CoT capability. In this
paper, we focus on the CoT capability for smaller
LLMs and further migrate it to multilingual reason-
ing.
Cross-lingual Transfer Cross-lingual transfer
pertains to utilizing labeled data from a resource
language to address the challenge of insufficient
labeled data in the target language. Previous
works (Conneau and Lample, 2019; Conneau et al.,
2020; Yang et al., 2020; Ma et al., 2020; Yang
et al., 2023) demonstrate that pre-trained models
trained on multi-lingual data proficiently perform
cross-lingual transfer tasks. These multi-lingual
pre-trained models have found extensive applica-tion across various downstream NLP tasks, such as
multi-lingual translation (Tan et al., 2019; Yang
et al., 2022b; Gaschi et al., 2023; Yang et al.,
2022c), cross-lingual summarization (Bhattachar-
jee et al., 2023; Wang et al., 2023), cross-lingual in-
formation extraction (Zhou et al., 2022; Yang et al.,
2022a; Wu et al., 2020). Many LLMs are trained
on multilingual data, endowing them with strong
cross-linguistic abilities (Scao et al., 2022; Muen-
nighoff et al., 2022). However, the cross-language
capability in smaller LLM is not significant, so we
augmenting the multilingual reasoning potential of
LLMs by employing pseudo training data derived
from labeled source-language datasets.
7 Conclusion
In this work, we propose a cross-lingual instruc-
tion fine-tuning framework ( XCOT) to address the
disparity by encouraging alignment among differ-

--- PAGE 9 ---
ent languages. A cross-lingual instruction dataset
(XCOT-INSTRUCT ) is first created to align seman-
tically the reasoning capability across various lan-
guages. Then, our method incorporates cross-
lingual in-context Learning (xICL) to trigger the
cross-lingual alignment. During instruction tun-
ing, we adopt random online CoT (Random-CoT),
which prompts LLM to translate the query into
different languages and subsequently provide an
English response. To further promote language
transfer, we leverage a high-resource CoT to guide
low-resource CoT training with cross-lingual dis-
tillation (xDistill). Our comprehensive evaluation
of established benchmarks showcases the effec-
tiveness of XCOTin narrowing the multilingual
linguistic gap. The results highlight its potential
as a robust solution for reducing the cross-lingual
divide, setting a new precedent for multilingual
language model performance.
References
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang
Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi
Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,
Yichang Zhang, Zhenru Zhang, Chang Zhou, Jin-
gren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.
Qwen technical report. CoRR , abs/2309.16609.
Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ah-
mad, Yuan-Fang Li, Yong-Bin Kang, and Rifat
Shahriyar. 2023. Crosssum: Beyond english-centric
cross-lingual summarization for 1, 500+ language
pairs. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023 , pages 2541–2564. Association for
Computational Linguistics.
Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming
Gong, Yangqiu Song, Dongmei Zhang, and Jia Li.
2023a. Breaking language barriers in multilingual
mathematical reasoning: Insights and observations.
CoRR , abs/2310.20246.
Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming
Gong, Yangqiu Song, Dongmei Zhang, and Jia Li.
2023b. Breaking language barriers in multilingual
mathematical reasoning: Insights and observations.
arXiv preprint arXiv:2310.20246 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. CoRR , abs/2110.14168.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In ACL
2020 , pages 8440–8451.
Alexis Conneau and Guillaume Lample. 2019. Cross-
lingual language model pretraining. In NeurIPS
2019 , pages 7057–7067.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:
general language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022,
Dublin, Ireland, May 22-27, 2022 , pages 320–335.
Association for Computational Linguistics.
Félix Gaschi, Xavier Fontaine, Parisa Rastin, and Yan-
nick Toussaint. 2023. Multilingual clinical NER:
translation or cross-lingual transfer? In Proceed-
ings of the 5th Clinical Natural Language Process-
ing Workshop, ClinicalNLP@ACL 2023, Toronto,
Canada, July 14, 2023 , pages 289–311. Association
for Computational Linguistics.
Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang,
Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong
Hu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao
Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, and Zhou-
jun Li. 2023. OWL: A large language model for IT
operations. CoRR , abs/2309.09298.
Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022.
Large language models are reasoning teachers. arXiv
preprint arXiv:2212.10071 .
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022a. Large lan-
guage models are zero-shot reasoners. In NeurIPS .
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022b. Large
language models are zero-shot reasoners. Advances
in neural information processing systems , 35:22199–
22213.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-
ardmath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct.
arXiv preprint arXiv:2308.09583 .
Shuming Ma, Jian Yang, Haoyang Huang, Zewen Chi,
Li Dong, Dongdong Zhang, Hany Hassan Awadalla,
Alexandre Muzio, Akiko Eriguchi, Saksham Singhal,
Xia Song, Arul Menezes, and Furu Wei. 2020. XLM-
T: scaling up multilingual machine translation with

--- PAGE 10 ---
pretrained cross-lingual transformer encoders. CoRR ,
abs/2012.15547.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR , 9(Nov):2579–
2605.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
Schoelkopf, et al. 2022. Crosslingual generaliza-
tion through multitask finetuning. arXiv preprint
arXiv:2211.01786 .
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In NeurIPS .
Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli,
Noah Constant, Colin Raffel, and Chris Callison-
Burch. 2023. Bidirectional language models are also
few-shot learners. In The Eleventh International Con-
ference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2021, Online,
June 6-11, 2021 , pages 2080–2094. Association for
Computational Linguistics.
Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang,
and Wanxiang Che. 2023. Cross-lingual prompt-
ing: Improving zero-shot chain-of-thought reasoning
across languages. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2023, Singapore, December 6-
10, 2023 , pages 2695–2709. Association for Compu-
tational Linguistics.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,
Suraj Srivats, Soroush V osoughi, Hyung Won Chung,
Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,
and Jason Wei. 2023. Language models are multi-
lingual chain-of-thought reasoners. In The Eleventh
International Conference on Learning Representa-
tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .
OpenReview.net.Xu Tan, Jiale Chen, Di He, Yingce Xia, Tao Qin, and
Tie-Yan Liu. 2019. Multilingual neural machine
translation with language clustering. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing, EMNLP-IJCNLP 2019, Hong Kong, China,
November 3-7, 2019 , pages 963–973. Association for
Computational Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. CoRR , abs/2307.09288.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023c. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Jiaan Wang, Fandong Meng, Yunlong Liang, Tingyi
Zhang, Jiarong Xu, Zhixu Li, and Jie Zhou. 2023.
Understanding translationese in cross-lingual sum-
marization. In Findings of the Association for Com-
putational Linguistics: EMNLP 2023, Singapore, De-
cember 6-10, 2023 , pages 3837–3849. Association
for Computational Linguistics.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022a. Finetuned
language models are zero-shot learners. In The Tenth
International Conference on Learning Representa-
tions, ICLR 2022, Virtual Event, April 25-29, 2022 .
OpenReview.net.

--- PAGE 11 ---
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022b. Emer-
gent abilities of large language models. Transactions
on Machine Learning Research . Survey Certifica-
tion.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022c. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
NeurIPS .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022d. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems , 35:24824–24837.
Qianhui Wu, Zijia Lin, Börje F. Karlsson, Biqing Huang,
and Jianguang Lou. 2020. Unitrans : Unifying model
transfer and data transfer for cross-lingual named en-
tity recognition with unlabeled data. In Proceedings
of the Twenty-Ninth International Joint Conference
on Artificial Intelligence, IJCAI 2020 , pages 3926–
3932. ijcai.org.
Jian Yang, Shaohan Huang, Shuming Ma, Yuwei Yin,
Li Dong, Dongdong Zhang, Hongcheng Guo, Zhou-
jun Li, and Furu Wei. 2022a. CROP: zero-shot cross-
lingual named entity recognition with multilingual
labeled sequence translation. In Findings of EMNLP
2022 , pages 486–496.
Jian Yang, Shuming Ma, Li Dong, Shaohan Huang,
Haoyang Huang, Yuwei Yin, Dongdong Zhang,
Liqun Yang, Furu Wei, and Zhoujun Li. 2023.
GanLM: Encoder-decoder pre-training with an auxil-
iary discriminator. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 9394–9412,
Toronto, Canada. Association for Computational Lin-
guistics.
Jian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi
Wu, Zhoujun Li, and Ming Zhou. 2020. Alternating
language modeling for cross-lingual pre-training. In
AAAI 2020 , pages 9386–9393.
Jian Yang, Yuwei Yin, Shuming Ma, Dongdong Zhang,
Zhoujun Li, and Furu Wei. 2022b. High-resource
language-specific training for multilingual neural ma-
chine translation. In IJCAI 2022 , pages 4461–4467.
Jian Yang, Yuwei Yin, Shuming Ma, Dongdong Zhang,
Shuangzhi Wu, Hongcheng Guo, Zhoujun Li, and
Furu Wei. 2022c. UM4: unified multilingual multi-
ple teacher-student model for zero-resource neural
machine translation. In IJCAI 2022 , pages 4454–
4460.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, Chuanqi Tan, and Chang Zhou. 2023. Scal-ing relationship on learning mathematical reason-
ing with large language models. arXiv preprint
arXiv:2308.01825 .
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-
hao Huang, Huan Sun, Yu Su, and Wenhu Chen.
2023. Mammoth: Building math generalist models
through hybrid instruction tuning. arXiv preprint
arXiv:2309.05653 .
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,
George Karypis, and Alex Smola. 2023. Multi-
modal chain-of-thought reasoning in language mod-
els.CoRR , abs/2302.00923.
Ran Zhou, Xin Li, Lidong Bing, Erik Cambria, Luo
Si, and Chunyan Miao. 2022. Conner: Consistency
training for cross-lingual named entity recognition.
InProceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2022, Abu Dhabi, United Arab Emirates, December
7-11, 2022 , pages 8438–8449. Association for Com-
putational Linguistics.
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang,
Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yu-
jiu Yang. 2023. Solving math word problems via
cooperative reasoning induced language models. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 4471–4485, Toronto, Canada.
Association for Computational Linguistics.
