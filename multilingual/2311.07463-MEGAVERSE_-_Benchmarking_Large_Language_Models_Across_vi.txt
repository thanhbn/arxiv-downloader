# MEGAVERSE: Đánh giá các Mô hình Ngôn ngữ Lớn trên Ngôn ngữ, Phương thức, Mô hình và Tác vụ

Sanchit Ahuja Divyanshu Aggarwal Varun Gumma Ishaan Watts
Ashutosh Sathe Millicent Ochieng Rishav Hada Prachi Jain
Mohamed Ahmed Kalika Bali Sunayana Sitaram
Microsoft Corporation
{t-sahuja,sunayana.sitaram}@microsoft.com

## Tóm tắt

Gần đây đã có sự gia tăng mạnh mẽ trong nghiên cứu đánh giá LLM để hiểu về khả năng và hạn chế của LLM. Tuy nhiên, phần lớn nghiên cứu này chỉ giới hạn ở tiếng Anh, để lại việc xây dựng và đánh giá LLM cho các ngôn ngữ khác tiếng Anh tương đối ít được khám phá. Nhiều LLM mới đã được giới thiệu gần đây, đòi hỏi việc đánh giá chúng trên các ngôn ngữ khác tiếng Anh. Nghiên cứu này nhằm thực hiện đánh giá toàn diện về khả năng không phải tiếng Anh của các LLM SOTA (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, và Gemma) bằng cách so sánh chúng trên cùng một bộ dữ liệu đa ngôn ngữ. Benchmark của chúng tôi bao gồm 22 bộ dữ liệu phủ 83 ngôn ngữ, bao gồm cả các ngôn ngữ châu Phi có ít tài nguyên. Chúng tôi cũng bao gồm hai bộ dữ liệu đa phương thức trong benchmark và so sánh hiệu suất của các mô hình LLaVA, GPT-4-Vision và Gemini-Pro-Vision. Các thí nghiệm của chúng tôi cho thấy các mô hình lớn hơn như GPT-4, Gemini-Pro và PaLM2 vượt trội hơn các mô hình nhỏ hơn trong nhiều tác vụ khác nhau, đặc biệt trên các ngôn ngữ có ít tài nguyên, với GPT-4 vượt trội hơn PaLM2 và Gemini-Pro trên nhiều bộ dữ liệu hơn. Chúng tôi cũng thực hiện nghiên cứu về ô nhiễm dữ liệu và phát hiện rằng một số mô hình có khả năng bị ô nhiễm với các benchmark đánh giá đa ngôn ngữ, đòi hỏi các phương pháp để phát hiện và xử lý ô nhiễm khi đánh giá hiệu suất đa ngôn ngữ của LLM.

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) đã vượt qua hiệu suất của thế hệ mô hình ngôn ngữ trước đó trên một số tác vụ và benchmark, đôi khi thậm chí tiếp cận hoặc vượt qua hiệu suất của con người (Hubert et al., 2024). Tuy nhiên, nguyên nhân gốc rễ của các khả năng được quan sát trong những mô hình này không phải lúc nào cũng rõ ràng, dù là do khả năng mô hình được tăng cường hay các yếu tố khác như ô nhiễm trong bộ dữ liệu kiểm tra và sự vắng mặt của các bộ dữ liệu thực sự đo lường khả năng của những mô hình này (Balloccu et al., 2024). Do đó, đánh giá các Mô hình Ngôn ngữ Lớn đã trở thành một lĩnh vực nghiên cứu quan trọng.

Phần lớn công việc đánh giá LLM thông qua benchmarking (Liang et al., 2022), các bài kiểm tra định tính cho khả năng cụ thể (Bubeck et al., 2023) hoặc đánh giá của con người chỉ tập trung duy nhất vào tiếng Anh. Tuy nhiên, các nghiên cứu đã chỉ ra rằng có một khoảng cách lớn giữa khả năng của LLM trong tiếng Anh và các ngôn ngữ khác (Choudhury et al., 2023). Đánh giá LLM trong các ngôn ngữ khác tiếng Anh là thách thức do nhiều yếu tố khác nhau, bao gồm thiếu benchmark phủ một số lượng lớn ngôn ngữ từ các họ ngôn ngữ đa dạng và thiếu benchmark đa ngôn ngữ phủ các tác vụ như lý luận, trò chuyện và đối thoại. Do đó, việc ưu tiên đánh giá đa ngôn ngữ là rất quan trọng để tăng cường phát triển các mô hình đa ngôn ngữ hiệu quả hơn. Việc bỏ qua khía cạnh quan trọng này có thể dẫn đến việc một phần dân số đáng kể bị bỏ lại phía sau và có thể mở rộng khoảng cách số hóa (Joshi et al., 2021).

Công việc trước đây của chúng tôi về đánh giá khả năng đa ngôn ngữ của LLM, MEGA (Ahuja et al., 2023), đã đưa ra những quan sát sau: GPT-4 (OpenAI, 2023a) gần với hiệu suất của các mô hình ngôn ngữ fine-tuned SOTA như TULRv6 (Patra et al., 2023). Các mô hình GPT hoạt động kém hơn trên các ngôn ngữ được viết bằng chữ cái không phải Latin, và trên các ngôn ngữ có ít tài nguyên. Các LLM khác như BLOOMZ (Muennighoff et al., 2023) thường hoạt động kém hơn GPT-4. Tuy nhiên, một số mô hình mới hơn có thể so sánh với GPT-4 về hiệu suất trên tiếng Anh, và việc nghiên cứu hiệu suất đa ngôn ngữ của chúng cũng rất cần thiết. Hơn nữa, có sự quan tâm ngày càng tăng đối với các Mô hình Đa phương thức Lớn (LMM), và sự hội tụ của LLM đa phương thức và đa ngôn ngữ vẫn là một lĩnh vực ít được nghiên cứu (Hu et al., 2024). Các đóng góp của chúng tôi như sau:

• Chúng tôi xây dựng dựa trên benchmark MEGA và thêm 6 bộ dữ liệu mới, từ đó mở rộng phạm vi phủ đến 22 bộ dữ liệu và 83 ngôn ngữ bao gồm nhiều ngôn ngữ châu Phi có ít tài nguyên.

• Chúng tôi benchmark 9 LLM văn bản SOTA mới - PaLM2 (Google, 2023), Llama2 (3 biến thể) (Touvron et al., 2023), Mistral-v1.0 (2 biến thể), (Jiang et al., 2023), Gemma (2 biến thể) (Mesnard et al., 2024), Gemini 1.0 pro (Anil et al., 2023a) ngoài GPT-4 và GPT-3.5-Turbo.

• Chúng tôi benchmark các mô hình họ LLaVA đa phương thức (Liu et al., 2023), GPT-4-Vision (OpenAI, 2023b) và Gemini-Pro-Vision (Anil et al., 2023a) trên hai bộ dữ liệu đa phương thức đa ngôn ngữ.

• Chúng tôi trình bày nghiên cứu ô nhiễm toàn diện về cả bộ LLM thương mại và mã nguồn mở trên một tập con các bộ dữ liệu của chúng tôi.

• Chúng tôi nghiên cứu các xu hướng tổng thể trong thí nghiệm của mình bằng cách nghiên cứu độ lệch hiệu suất trên các họ ngôn ngữ và tác vụ, và cung cấp hướng cho nghiên cứu tương lai.

## 2 Công trình liên quan

**Đánh giá LLM** Gần đây, đã có sự quan tâm ngày càng tăng trong việc đánh giá LLM trên nhiều khả năng khác nhau, do sự gia tăng về độ phổ biến và hiệu quả của chúng. BIG-Bench (Srivastava et al., 2023) bao gồm 204 tác vụ để đánh giá LLM. Mặc dù BIG-Bench bao gồm các tác vụ trong các ngôn ngữ không phải tiếng Anh, chúng chủ yếu liên quan đến dịch thuật. Liang et al. (2022) đề xuất HELM, định nghĩa một phân loại về các kịch bản và chỉ số xác định không gian đánh giá LLM, và đánh giá 30 mô hình ngôn ngữ trên 42 kịch bản và 7 chỉ số. Tuy nhiên, tất cả các kịch bản đều tập trung vào các bộ dữ liệu bằng tiếng Anh chuẩn hoặc phương ngữ, và họ nói rõ phạm vi phủ ngôn ngữ là một lĩnh vực quan trọng cần cải thiện. Bubeck et al. (2023), đã chỉ ra những hạn chế của việc sử dụng benchmark NLP chuẩn để đánh giá các mô hình tạo sinh, do tốc độ mà những benchmark này trở nên bão hòa. Cũng có những lo ngại về ô nhiễm benchmark trong đánh giá LLM. Zhou et al. (2023) cho thấy ô nhiễm bộ dữ liệu kiểm tra trong dữ liệu huấn luyện và fine-tuning dẫn đến tác động đáng kể đến hiệu suất LLM.

**Benchmark và Đánh giá Đa ngôn ngữ** Bang et al. (2023) đánh giá khả năng đa ngôn ngữ của ChatGPT và cho thấy nó không khái quát hóa được cho các ngôn ngữ có ít tài nguyên với chữ cái không phải Latin. Tuy nhiên, đánh giá đa ngôn ngữ chỉ được thực hiện trên một vài tác vụ, và một tập con 50-100 ví dụ được sử dụng để kiểm tra mô hình. Hendy et al. (2023) đánh giá khả năng dịch thuật của các mô hình GPT-3.5 và phát hiện rằng những mô hình này hoạt động tốt trong việc dịch các ngôn ngữ có nhiều tài nguyên, nhưng khả năng của chúng đối với các ngôn ngữ có ít tài nguyên bị hạn chế. BUFFET (Asai et al., 2023) phủ 54 ngôn ngữ trên 15 bộ dữ liệu và Lai et al. (2023) phủ 37 ngôn ngữ trên 7 bộ dữ liệu cũng thực hiện benchmarking đa ngôn ngữ của LLM như ChatGPT và BLOOMZ. Yang et al. (2023) thực hiện nghiên cứu toàn diện về khả năng của GPT4-Vision bao gồm phân tích hiệu suất của nó trong mô tả hình ảnh đa ngôn ngữ, nhận dạng văn bản cảnh và dịch thuật. Công việc của chúng tôi xây dựng dựa trên nỗ lực benchmarking MEGA (Ahuja et al., 2023), đánh giá các mô hình GPT trên 16 bộ dữ liệu. Chúng tôi mở rộng benchmark MEGA đến nhiều tác vụ hơn bao gồm các tác vụ đa phương thức, đánh giá một số LLM SOTA, và thực hiện phân tích ô nhiễm toàn diện hơn.

**Ô nhiễm** Một số kỹ thuật đã được đề xuất để nghiên cứu ô nhiễm của các bộ dữ liệu đánh giá có sẵn công khai. Ahuja et al. (2023) nghiên cứu ô nhiễm bằng cách nhắc các mô hình điền thẻ dữ liệu. Các phương pháp khác bao gồm Golchin và Surdeanu (2023b), không cung cấp định lượng ô nhiễm, và Oren et al. (2023), yêu cầu truy cập vào xác suất log, do đó hạn chế nghiên cứu của họ đối với LLM mã nguồn mở.

## 3 Thiết lập thí nghiệm

### 3.1 Bộ dữ liệu

Chúng tôi thực hiện thí nghiệm trên 16 bộ dữ liệu là một phần của bộ MEGA - XNLI (Conneau et al., 2018), IndicXNLI (Aggarwal et al., 2022), GLUECoS NLI (Khanuja et al., 2020a), PAWS-X (Yang et al., 2019), XCOPA (Ponti et al., 2020), XStoryCloze (Lin et al., 2022), GLUECoS Phân tích Tình cảm (En-Es-CS) (Vilares et al., 2016), TyDiQA-GoldP (Clark et al., 2020), MLQA (Lewis et al., 2020), XQUAD (Artetxe et al., 2020), IndicQA (Doddapaneni et al., 2023), PAN-X (Pan et al., 2017), UDPOS (Nivre et al., 2018), Jigsaw (Kivlichan et al., 2020), WinoMT (Stanovsky et al., 2019) và XLSum (Hasan et al., 2021). Những bộ dữ liệu này bao gồm hỗn hợp các bộ dữ liệu phân loại, Hỏi đáp, Gán nhãn chuỗi, và Tạo sinh Ngôn ngữ Tự nhiên, cùng với hai bộ dữ liệu phủ các tác vụ AI Có trách nhiệm về phát hiện độc tính và thiên kiến giới tính. Các bộ dữ liệu chúng tôi bao gồm cũng chứa hỗn hợp các bộ dữ liệu được dịch và xác minh bởi người bản ngữ, cũng như các bộ dữ liệu được tạo độc lập cho từng ngôn ngữ. Hình 1 cho thấy hệ thống phân cấp các mô hình và tác vụ trải rộng trên MEGAVERSE. Để mô tả chi tiết hơn về các bộ dữ liệu bao gồm trong benchmark MEGA gốc, chúng tôi khuyến khích độc giả tham khảo Ahuja et al. (2023). Chúng tôi mô tả sáu bộ dữ liệu được thêm vào nghiên cứu của chúng tôi dưới đây.

#### 3.1.1 AfriQA

AfriQA (Ogundepo et al., 2023) là một bộ dữ liệu QA không có đoạn văn bối cảnh. Nó phủ 10 ngôn ngữ châu Phi - Bemba, Fon, Hausa, Igbo, Kinyarwanda, Swahili, Twi, Wolof, và Yorùbá. Chúng tôi sử dụng kích thước few-shot k= 4 và chiến lược nhắc đơn ngôn ngữ để thực hiện thí nghiệm chỉ trên các mô hình GPT và Llama, vì mô hình PaLM2 chỉ hỗ trợ Swahili.

#### 3.1.2 Belebele

Belebele (Bandarkar et al., 2023) là một bộ dữ liệu hiểu đọc trắc nghiệm (MRC) song song trên 122 ngôn ngữ. Mỗi câu hỏi được liên kết với một đoạn văn ngắn từ bộ dữ liệu FLORES-200 (Costa-jussà et al., 2022). Quy trình chú thích của con người được xây dựng cẩn thận để tạo các câu hỏi phân biệt giữa các mức độ hiểu ngôn ngữ khác nhau. Chúng tôi đánh giá tiếng Ả Rập, Czech, Đan Mạch, Đức, Anh, Tây Ban Nha, Phần Lan, Pháp, Hebrew, Hungary, Ý, Nhật, Hàn, Hà Lan, Na Uy, Ba Lan, Bồ Đào Nha, Nga, Thụy Điển, Thái, Thổ Nhĩ Kỳ, Trung Quốc Giản thể và Trung Quốc Phồn thể. Kết quả cho Llama2 và GPT-3.5-Turbo được báo cáo từ bài báo bộ dữ liệu. Chúng tôi thực hiện nhắc đơn ngôn ngữ zero-shot cho thí nghiệm của mình, vì bộ dữ liệu này không có tập dev.

#### 3.1.3 IN22

IN22 (Gala et al., 2023) là một benchmark dịch thuật cho tất cả 22 ngôn ngữ Ấn Độ theo lịch. IN22-Gen là tập con đánh giá đa lĩnh vực mục đích chung của IN22 được xây dựng từ hai nguồn: Wikipedia và Nguồn Web cung cấp nội dung đa dạng trải rộng tin tức, giải trí, văn hóa, pháp lý, và các chủ đề tập trung vào Ấn Độ. IN22-Conv là tập con lĩnh vực hội thoại của IN22. Do hạn chế tài nguyên, chúng tôi đánh giá 14 ngôn ngữ: Assam, Bengal, Anh, Gujarat, Hindi, Kannada, Kashmir, Malayalam, Marathi, Nepal, Odia, Punjab, Tamil, Telugu, và Urdu.

#### 3.1.4 MaRVL

MaRVL (Lý luận Đa văn hóa về Thị giác và Ngôn ngữ) (Liu et al., 2021) là một bộ dữ liệu hình ảnh và chú thích liên quan. Các khái niệm và hình ảnh được thu thập hoàn toàn được điều khiển bởi người bản ngữ và đại diện cho các nền văn hóa khác nhau trên toàn cầu và trải rộng 5 ngôn ngữ, tức là Indonesia, Trung Quốc, Swahili, Tamil, và Thổ Nhĩ Kỳ. Mỗi thể hiện trong bộ dữ liệu bao gồm một cặp hình ảnh (hình ảnh trái và hình ảnh phải) và một tuyên bố, và nhiệm vụ là xác định xem tuyên bố có nhất quán với cặp hình ảnh đã cho hay không.

#### 3.1.5 XM-3600

CrossModal-3600 (Thapliyal et al., 2022) là một bộ dữ liệu chú thích hình ảnh đa ngôn ngữ bao gồm 3600 hình ảnh đa dạng về mặt địa lý được chú thích trực tiếp bằng 36 ngôn ngữ khác nhau, tránh bất kỳ sự không nhất quán nào do dịch thuật. Chúng tôi thí nghiệm trên 20 trong số 36 ngôn ngữ do hạn chế tài nguyên: Ả Rập, Trung Quốc, Czech, Đan Mạch, Hà Lan, Anh, Phần Lan, Pháp, Đức, Ý, Nhật, Hàn, Na Uy, Ba Lan, Bồ Đào Nha, Nga, Tây Ban Nha, Thụy Điển, Thái, và Thổ Nhĩ Kỳ.

#### 3.1.6 XRiSAWOZ

XRiSAWOZ (Moradshahi et al., 2023) là một bộ dữ liệu mô hình đối thoại hướng tác vụ. Bộ dữ liệu là bản dịch đa ngôn ngữ (Anh, Hindi, Pháp, Hàn) của bộ dữ liệu RiSAWOZ chỉ bằng tiếng Trung (Quan et al., 2020). XRiSAWOZ cũng bao gồm một thiết lập pha trộn mã Anh-Hindi. Đối với mỗi cuộc hội thoại, tác nhân phải sử dụng kiến thức có cấu trúc từ cơ sở dữ liệu để trả lời các truy vấn của người dùng. Tác vụ bao gồm 4 tác vụ con: "Theo dõi Trạng thái Đối thoại" (DST), "Phát hiện Gọi API" (API), "Tạo sinh Hành động Đối thoại" (DA) và "Tạo sinh Phản hồi" (RG). Các chỉ số được sử dụng để đánh giá bao gồm BLEU, Tỷ lệ Lỗi Slot (SER) (tính chính xác thực tế của phản hồi được tạo) (Wen et al., 2015), tỷ lệ thành công (trung bình/tác vụ) (Lin et al., 2021), độ chính xác gọi API, độ chính xác hành động đối thoại và độ chính xác mục tiêu chung (Budzianowski et al., 2018). Chúng tôi khuyến khích độc giả tham khảo Moradshahi et al. (2023) để có mô tả chi tiết về các tác vụ con và chỉ số. Chúng tôi thực hiện thí nghiệm trên 10% dữ liệu tức là khoảng 400 lượt đối thoại trên 3 lĩnh vực do hạn chế tính toán.

### 3.2 Mô hình

Dưới đây là danh sách tất cả các mô hình chúng tôi đánh giá:
• GPT-3.5-Turbo (Ouyang et al., 2022)
• GPT-4 (OpenAI, 2023a)
• GPT-4-Vision (OpenAI, 2023b)
• Llama2 (7B, 13B, 70B) (Touvron et al., 2023)
• PaLM2 (Anil et al., 2023b)
• Gemini-Pro (Anil et al., 2023a)
• Gemini-Pro-Vision (Anil et al., 2023a)
• Gemma (2B, 7B) (Mesnard et al., 2024)
• Mistral (Jiang et al., 2023)
• BakLLaVA-v1 (Liu et al., 2023)
• ViP-LLaVA (13B) (Cai et al., 2023)
• LLaVA-1.5 (13B) (Liu et al., 2023)

### 3.3 Chiến lược nhắc

Ahuja et al. (2023) khám phá ba biến thể nhắc dựa trên ngôn ngữ của các ví dụ few-shot và kiểm tra, và phát hiện rằng nhắc đơn ngôn ngữ, có các ví dụ few-shot bằng ngôn ngữ đích, vượt trội hơn nhắc chéo ngôn ngữ zero-shot bằng tiếng Anh cho hầu hết các bộ dữ liệu. Translate-test vượt trội hơn đơn ngôn ngữ cho một số ngôn ngữ có ít tài nguyên nhưng với khoảng cách tối thiểu cho các mô hình như GPT-4. Do đó, chúng tôi mặc định sử dụng nhắc đơn ngôn ngữ trừ khi được chỉ định khác. Nhắc chéo ngôn ngữ zero-shot (zs-cl) được sử dụng khi các bộ dữ liệu dev không có sẵn trong ngôn ngữ đích. Hướng dẫn tiếng Anh được duy trì cho các lời nhắc, được chứng minh vượt trội hơn hướng dẫn bằng ngôn ngữ đích (Ahuja et al., 2023). Các mẫu lời nhắc cho bộ dữ liệu mới của chúng tôi có trong Phụ lục A.2.

#### 3.3.1 XRiSAWOZ

Moradshahi et al. (2023) trình bày kết quả trong cả thiết lập đánh giá end-to-end và từng lượt. Chúng tôi thực hiện đánh giá end-to-end với lọc dựa trên regex cẩn thận của các phản hồi được tạo cho các tác vụ DST/API/DA sau mỗi lượt. Điều này được yêu cầu để đảm bảo tính chính xác của cú pháp trong các mô tả trạng thái cho những tác vụ này. Không có xử lý hậu kỳ nào được thực hiện cho tác vụ RG. Để suy ra một tác vụ con trên một lượt đối thoại, chúng tôi cung cấp các ví dụ trong ngữ cảnh tương ứng với cùng lượt từ các lĩnh vực khác. Nếu đối với một lượt cụ thể, không có đủ ví dụ trong ngữ cảnh, chúng tôi tìm lượt trước đó gần nhất mà có đủ ví dụ trong ngữ cảnh. Ví dụ: Giả sử phân phối số lượt sau và k= 4 (số ví dụ trong ngữ cảnh). Lượt 1–4: hơn 10 ví dụ, Lượt 5: 3 ví dụ, và Lượt 6 có 1 ví dụ.

Tại lượt 5 và 6, chúng tôi không có đủ ví dụ từ lượt 5 hoặc 6. Do đó, chúng tôi lấy mẫu ví dụ trong ngữ cảnh từ lượt 4 cho cả hai. Các lời nhắc của chúng tôi cho mỗi tác vụ con có thể được xem trong Hình 9, 10, 11, 12, 13.

## 4 Kết quả

### 4.1 XNLI

Tất cả mô hình hoạt động tốt nhất trên tiếng Anh, với hiệu suất hơi thấp hơn trên tiếng Hy Lạp và tiếng Đức, và hiệu suất thấp hơn trên các ngôn ngữ như Hindi, Thái, Urdu, và Swahili. Tổng thể PaLM2 hoạt động tốt nhất, theo sát là GPT-4. GPT-3.5-Turbo kém hơn trên tất cả ngôn ngữ, tuy nhiên, chúng tôi thấy rằng cả ba mô hình Llama đều hoạt động kém đáng kể, với Mistral hoạt động kém nhất. Vì XNLI là một bộ dữ liệu phổ biến, ô nhiễm bộ dữ liệu không thể được loại trừ. (Hình 18, Bảng 2).

### 4.2 IndicXNLI

Chúng tôi thực hiện thí nghiệm trên IndicXNLI trên các mô hình GPT, Mistral cũng như các mô hình Llama, tuy nhiên, các mô hình Llama cho điểm 0 cho tất cả ngôn ngữ, đó là lý do tại sao chúng tôi không vẽ chúng. Mô hình Mistral cũng hoạt động kém. Chúng tôi thấy rằng GPT-4 vượt trội hơn GPT-3.5-Turbo trên tất cả ngôn ngữ với điểm cao nhất trên Hindi, Punjab, và Bengal. Tuy nhiên, độ chính xác tổng thể không cao lắm trên bất kỳ ngôn ngữ nào so với kết quả XNLI đã thấy trước đó, và các baseline fine-tuned như MuRIL hoạt động tốt nhất. (Hình 19, Bảng 3).

### 4.3 GLUECoS NLI

Tất cả mô hình đều hoạt động tốt trên tác vụ NLI này, với GPT-4 hoạt động tốt nhất. (Hình 26, Bảng 14).

### 4.4 PAWS-X

PaLM2 vượt trội hơn các mô hình GPT trên tất cả ngôn ngữ và tất cả mô hình đều hoạt động tốt, điều này có thể do bộ dữ liệu này chứa các ngôn ngữ có nhiều tài nguyên. Tuy nhiên, ô nhiễm bộ dữ liệu không thể được loại trừ, như được chỉ ra trong Ahuja et al. (2023). Hiệu suất trên tiếng Anh là tốt nhất, theo sát là các ngôn ngữ chữ Latin, và giảm hiệu suất đối với các ngôn ngữ trong các chữ cái khác. Các mô hình Llama và Mistral hoạt động kém hơn các mô hình GPT và PaLM2, mặc dù sự khác biệt về hiệu suất không lớn như trong một số bộ dữ liệu khác. (Hình 20, Bảng 4).

### 4.5 XCOPA

Hiệu suất của GPT-4, Gemma, Gemini và PaLM2 có thể so sánh được, với GPT-4 có hiệu suất tốt nhất. Đáng chú ý, tất cả đều tốt hơn GPT-3.5-Turbo, hoạt động tốt hơn đáng kể so với các mô hình Llama2 và Mistral trừ trong tiếng Quechua, mà không có mô hình nào hoạt động tốt. Tuy nhiên, kết quả trên tất cả ngôn ngữ khác cho GPT-4 và PaLM2 cực kỳ cao, điều này có thể do ô nhiễm bộ dữ liệu. (Hình 21, Bảng 5).

### 4.6 XStoryCloze

Vì các mô hình Llama cho điểm 0 cho tất cả ngôn ngữ, chúng tôi bỏ qua nó khỏi phân tích của chúng tôi. Chúng tôi thấy rằng khoảng cách giữa các mô hình GPT và PaLM2 rất cao, với cả hai mô hình GPT hoạt động cực kỳ tốt. Đối với tất cả ngôn ngữ trừ Telugu, Basque và Burmese, Gemini-pro hoạt động tốt. Nghiên cứu ô nhiễm từ Ahuja et al. (2023) cho thấy khả năng thấp ô nhiễm bộ dữ liệu cho GPT-4, điều này chỉ ra rằng các mô hình GPT có thể thực hiện tác vụ này tốt. (Hình 22, Bảng 13).

### 4.7 Phân tích Tình cảm (En-Es-CS)

Đáng ngạc nhiên, GPT-3.5-Turbo vượt trội hơn cả GPT-4 và PaLM2 trong tác vụ này, với baseline mBERT hoạt động tốt nhất, trong khi Gemini-pro hoạt động kém nhất với một khoảng cách lớn. (Hình 26, Bảng 14).

### 4.8 TyDiQA GoldP

Mô hình TuLR hoạt động tốt nhất, theo sau là GPT-4, PaLM2, Gemini-Pro, và BLOOMZ, trong khi các mô hình Llama hoạt động kém, với Mistral tốt hơn một chút. Các mô hình nhỏ hơn, đặc biệt, thể hiện khoảng cách hiệu suất đáng kể giữa tiếng Anh và tất cả ngôn ngữ khác. Tuy nhiên, ô nhiễm bộ dữ liệu không thể được loại trừ, như được chỉ ra trong Ahuja et al. (2023). (Hình 23, Bảng 7).

### 4.9 MLQA

TULR và GPT-4 vượt trội hơn tất cả mô hình khác cho bộ dữ liệu này trừ tiếng Đức. Tiếng Anh thể hiện hiệu suất vượt trội, với tiếng Tây Ban Nha (es), tiếng Đức (de), và tiếng Việt (vi) theo sát. Những khoảng cách đáng kể nhất được ghi nhận giữa tiếng Anh và tiếng Ả Rập (ar), Hindi (hi), và tiếng Trung (zh). Mô hình Llama2-13B hoạt động tốt cho một số ngôn ngữ, như tiếng Ả Rập, tiếng Đức, và tiếng Tây Ban Nha nhưng hoạt động kém trên tiếng Trung, Hindi, và tiếng Việt, nhưng vẫn tốt hơn Mistral và Gemma. Đây là một trong những bộ dữ liệu mà PaLM2 gặp khó khăn, đặc biệt đối với tiếng Ả Rập và tiếng Trung. Ô nhiễm bộ dữ liệu trong GPT-4 không thể được loại trừ, như được chỉ ra trong Ahuja et al. (2023). Các phiên bản nhỏ hơn của mô hình Llama vượt trội hơn mô hình Llama 70B trên tất cả ngôn ngữ. (Hình 24, Bảng 8).

### 4.10 XQUAD

TuLRv6 hoạt động tốt nhất trên hầu hết tất cả ngôn ngữ trong bộ dữ liệu XQuAD, theo sau là GPT-4, PaLM 2, Gemini-Pro, và BLOOMZ. Hiệu suất của BLOOMZ giảm đáng kể trong tiếng Hy Lạp và tiếng Thái như được hiển thị trong Hình 2. PaLM2 và Gemini-Pro thể hiện hiệu suất cạnh tranh, theo sát GPT-4-32K và TuLRv6 – XXL trên các ngôn ngữ từ tầng có nhiều đến trung bình tài nguyên. Cả ba mô hình Llama đều hoạt động kém trên bộ dữ liệu này. Gemma và Mistral hoạt động tốt hơn một chút so với Llama trên tất cả ngôn ngữ nhưng tụt hậu so với các mô hình lớn hơn và mô hình finetuned. Ô nhiễm bộ dữ liệu trong GPT-4 không thể được loại trừ, như được chỉ ra trong Ahuja et al. (2023). (Hình 2, Bảng 6).

### 4.11 IndicQA

Vì các mô hình Llama cho điểm 0 cho tất cả ngôn ngữ, chúng tôi bỏ qua nó khỏi phân tích của chúng tôi. Chúng tôi sử dụng chiến lược nhắc chéo ngôn ngữ zero-shot do sự vắng mặt của tập dev. GPT-4 hoạt động tốt hơn GPT-3.5-Turbo, với hiệu suất tốt nhất được thấy cho Hindi, Marathi, và Bengal, trong khi các mô hình nhỏ hơn như Gemma hoạt động kém. (Hình 25, Bảng 9).

### 4.12 PAN-X

GPT-4 và GPT-3.5-Turbo vượt trội hơn PaLM2 và gemini-pro cho hầu hết ngôn ngữ. Tuy nhiên, tất cả mô hình đều hoạt động kém trên tiếng Thái, tiếng Nhật, và tiếng Trung trong tác vụ gán nhãn chuỗi này. Vì đây là một bộ dữ liệu cũ hơn, ô nhiễm dữ liệu GPT-4 không thể được loại trừ như được chỉ ra trong Ahuja et al. (2023). (Hình 31, Bảng 12).

### 4.13 UDPOS

PaLM2 hoạt động tốt nhất theo sau là GPT-4, GPT-3.5-Turbo và Gemini-pro kém nhất trung bình. Tất cả mô hình cho thấy hiệu suất cao tương tự trên các ngôn ngữ, trừ tiếng Ả Rập, tiếng Hy Lạp, Hebrew, Hindi, và tiếng Việt, nơi PaLM2 hoạt động tốt nhất. Ô nhiễm dữ liệu GPT-4 không thể được loại trừ như được chỉ ra trong Ahuja et al. (2023). (Hình 33, Bảng 11).

### 4.14 Jigsaw

Chúng tôi thực hiện thí nghiệm trên bộ dữ liệu Jigsaw cho GPT-3.5-Turbo và PaLM2 sử dụng chiến lược nhắc đơn ngôn ngữ và thấy rằng cả hai mô hình đều hoạt động rất tốt trên tất cả ngôn ngữ. Vì bộ dữ liệu không thể được truy cập mà không tải xuống, các mô hình ít có khả năng bị ô nhiễm với bộ dữ liệu này. (Hình 30, Bảng 19).

### 4.15 WinoMT

Chúng tôi thực hiện thí nghiệm trên bộ dữ liệu WinoMT chỉ cho GPT-3.5-Turbo sử dụng chiến lược nhắc đơn ngôn ngữ và báo cáo kết quả để hoàn thiện. Chúng tôi thấy rằng mô hình không hoạt động tốt trên bất kỳ ngôn ngữ nào. (Hình 29, Bảng 20).

### 4.16 XLSum

GPT-4 vượt trội hơn tất cả mô hình khác, với một số ngoại lệ. GPT-3.5-Turbo hoạt động tốt nhất cho các ngôn ngữ châu Phi như Swahili, Somali, và Yoruba, trong khi các mô hình Llama hoạt động tốt nhất cho tiếng Ả Rập, Kyrgyz, tiếng Việt, và Welsh. Theo phân tích ô nhiễm trong Ahuja et al. (2023), có thể, mặc dù ít khả năng rằng GPT-4 bị ô nhiễm với bộ dữ liệu này. (Hình 34, Bảng 15).

### 4.17 Belebele

Gemini-Pro có hiệu suất tốt nhất trong số tất cả mô hình cho hầu hết ngôn ngữ, trong khi đối với các mô hình nhỏ hơn chỉ có các mô hình Llama gần được. GPT-4 và PaLM2 vượt trội hơn GPT-3.5-Turbo, Llama2, và Mistral, hoạt động kém nhất. Hầu hết mô hình đều hoạt động tốt do bản chất hỏi đáp trắc nghiệm của tác vụ, làm cho việc phân tích đầu ra và đánh giá đơn giản hơn và tăng xác suất thành công ngay cả đối với các mô hình yếu hơn. (Hình 16, Bảng 17).

### 4.18 AfriQA

GPT-4 có hiệu suất tốt nhất, trong khi các mô hình Llama2 và Mistral hoạt động rất kém trên tất cả ngôn ngữ. (Hình 15, Bảng 10).

### 4.19 IN22

Chúng tôi báo cáo kết quả của mình trên các tập con IN22-Gen và IN22-Conv (Hình 35) nơi chúng tôi chọn ngẫu nhiên k= 8 cặp dịch từ tập phát triển của FLORES-200 (Costa-jussà et al., 2022) làm ví dụ trong ngữ cảnh. Chúng tôi cũng báo cáo điểm GPT-3.5-Turbo 0-shot và IndicTrans2 từ Gala et al. (2023) để so sánh. Để nhất quán, chúng tôi sử dụng indic_nlp_library và các script đánh giá từ Gala et al. (2023) để tokenize các dự đoán và tham chiếu trước khi tính chrF++ (Popović, 2017) cho các ngôn ngữ Ấn Độ. Chúng tôi không đánh giá PaLM2 trên bộ dữ liệu này, vì hầu hết ngôn ngữ trong bộ dữ liệu này không được nó hỗ trợ.

Llama2 và Mistral hoạt động kém trên tất cả ngôn ngữ Ấn Độ trong hướng En-Indic, trong khi hiệu suất tốt hơn trong hướng Indic-En. Gemma-7B hoạt động tốt hơn đáng kể so với cả Llama2 và Mistral trong cả hai hướng và trên tất cả ngôn ngữ. GPT-4 hoạt động tốt nhất trong số tất cả mô hình LLM được xem xét. Tất cả LLM hoạt động tốt hơn trong hướng Indic-En và bộ dữ liệu Hội thoại vì chúng được finetuned với dữ liệu kiểu trò chuyện hoặc hội thoại. Chúng tôi so sánh kết quả với IndicTrans2 Gala et al. (2023) và thấy rằng nó hoạt động tốt hơn đáng kể so với LLM. (Hình 35, Bảng 21 - 24).

### 4.20 XRiSAWOZ

Chúng tôi so sánh độ chính xác DA của các mô hình khác nhau trong Hình 17. Bảng 25 cho thấy so sánh với các mô hình fine-tuned cũng như. Chúng tôi thấy rằng hiệu suất của GPT-4 về độ chính xác DA gần nhất và có thể so sánh với các baseline fine-tuned cho tác vụ. Điểm kém hơn trên các mô hình khác dường như tương quan với xu hướng ảo giác của mô hình.

Chúng tôi so sánh kết quả trên tất cả 6 chỉ số trong Bảng 26 để hiểu rõ hơn về hành vi mô hình. Chúng tôi thấy rằng PaLM2, GPT-4 và Gemini-pro tạo ra các phản hồi rất ngắn gọn dẫn đến điểm BLEU cao hơn nhất quán so với các mô hình khác. Trên tất cả chỉ số khác, họ mô hình GPT vượt trội đáng kể so với cả mô hình PaLM/Gemini và mô hình mã nguồn mở. Đáng chú ý, tất cả mô hình độc quyền đạt được ít hơn 10% SER trên tiếng Trung gợi ý ô nhiễm của RiSAWOZ (bộ dữ liệu gốc chỉ bằng tiếng Trung). Các mô hình mã nguồn mở thường ảo giác các thực thể không tồn tại trong phản hồi của chúng trong khi các mô hình độc quyền không cho thấy xu hướng này.

Trong thiết lập pha trộn mã Anh-Hindi, hiệu suất kém hơn cả tiếng Anh và Hindi trung bình trên hầu hết chỉ số cho tất cả mô hình. (Hình 17, Bảng 25, 26). Điều này có thể chỉ ra thách thức trong hiểu cũng như tạo ra văn bản pha trộn mã hiệu quả cho tất cả mô hình.

### 4.21 MaRVL

Chúng tôi đánh giá các mô hình LLaVA, GPT-4-Vision, và Gemini-Pro-Vision trên các bộ dữ liệu đa phương thức với nhắc đơn ngôn ngữ và translate-test (Hình 27). Mô-đun dịch Azure BING được sử dụng để dịch các câu sang tiếng Anh. Chúng tôi thấy rằng điểm độ chính xác gần với phân loại ngẫu nhiên các mô hình LLaVA, với điểm thấp nhất trên Tamil và tiếng Trung. Chiến lược translate-test có thể so sánh với đơn ngôn ngữ. Tuy nhiên, hiệu suất vẫn giống như phân loại ngẫu nhiên. GPT-4-Vision tốt hơn đáng kể so với LLaVA, và lợi ích do translate-test chỉ thấy rõ trên tiếng Thổ Nhĩ Kỳ. Gemini-Pro-Vision hoạt động tốt hơn một chút so với ngẫu nhiên, và translate-test tốt hơn trừ trường hợp tiếng Trung. (Hình 27, Bảng 16).

### 4.22 XM-3600

Chúng tôi kiểm tra các mô hình LLaVA, GPT-4-Vision, và Gemini-Pro-Vision trên bộ dữ liệu chú thích hình ảnh XM-3600 và sử dụng chỉ số chrF (Popović, 2015) để báo cáo hiệu suất, không giống như bài báo gốc (Thapliyal et al., 2022) sử dụng CIDEr. Chúng tôi thấy rằng các mô hình LLaVA kém cho hầu hết ngôn ngữ không được viết bằng chữ Latin, đặc biệt là tiếng Nhật, tiếng Hàn, tiếng Nga, tiếng Thái, và tiếng Trung. bakLLaVA-v1 hoạt động kém hơn nhiều so với LLaVA-v1.5-13B và ViP-LLaVA-13B (trừ tiếng Anh), và hai cái sau có thể so sánh trên tất cả ngôn ngữ. Hầu hết ngôn ngữ chữ Latin có nhiều tài nguyên như tiếng Pháp, tiếng Đức, tiếng Hà Lan, tiếng Tây Ban Nha, và tiếng Ý vượt trội hoặc gần với hiệu suất tiếng Anh, với các ngôn ngữ có ít tài nguyên hơn như tiếng Đan Mạch, Czech, Ba Lan, và Na Uy hoạt động kém hơn. GPT-4-Vision vượt trội đáng kể so với các mô hình LLaVA trên tất cả ngôn ngữ, tuy nhiên, điểm trên tiếng Trung, tiếng Nhật, và tiếng Thái vẫn rất kém. Tiếng Pháp có điểm cao nhất theo sau là tiếng Ý, tiếng Tây Ban Nha, và sau đó là tiếng Anh, điều này một lần nữa cho thấy GPT-4-Vision giỏi chữ Latin và các ngôn ngữ châu Âu. Gemini-Pro-Vision là mô hình tốt thứ hai trên tất cả ngôn ngữ, và kết quả theo cùng xu hướng như GPT-4-Vision. (Hình 28, Bảng 18).

### 4.23 Độ lệch hiệu suất trên các họ ngôn ngữ và tác vụ

Dựa trên các thí nghiệm được tiến hành, chúng tôi xem xét cách hiệu suất cho một Họ Ngôn ngữ hoặc Tác vụ nhất định thay đổi so với hiệu suất trung bình (trên các mô hình được phủ trong MEGAVERSE). Trong việc làm như vậy, chúng tôi quan tâm đến việc xếp hạng mức độ hỗ trợ tốt của các mô hình đối với các Họ Ngôn ngữ hoặc Tác vụ khác nhau.

Độ lệch cho một thí nghiệm nhất định i trong Họ Ngôn ngữ hoặc Tác vụ (j) được định nghĩa là:

∆(i,j) = p_score(i,j) − (1/N) ∑N p_score(i,j)

Trong đó p_score(i,j) là điểm bị phạt cho thí nghiệm i, và giá trị dương cao chỉ ra rằng một chủ đề nhất định (Họ Ngôn ngữ hoặc Tác vụ) hoạt động tốt hơn so với trung bình trong khi giá trị âm thấp chỉ ra rằng chủ đề hoạt động thấp hơn trung bình (trên tất cả mô hình). p_score(i,j) được tính như:

p_score(i,j) = (|Xj|/∑i|Xj|) × score_i

Trong đó score_i là điểm chuẩn hóa cho thí nghiệm, bị phạt bởi tỷ lệ các thể hiện trong một họ ngôn ngữ/tác vụ nhất định (j) so với tổng số thể hiện trong tất cả các họ ngôn ngữ/tác vụ.

Do sự thưa thớt trong các kết hợp (Ngôn ngữ, Bộ dữ liệu, Mô hình) (xem Bảng 1), chúng tôi áp dụng phạt kích thước để hạn chế thiên kiến của các ngoại lệ và kết hợp với ít hỗ trợ. Ví dụ, có tổng cộng 320 thí nghiệm họ ngôn ngữ IE: Iranian trong dữ liệu của chúng tôi, với điểm trung bình 0.31, và điểm bị phạt 0.05, so với Basque có 10 thí nghiệm với điểm trung bình 0.54, nhưng điểm bị phạt 0.003.

Hình 3 cho thấy phân phối của điểm ∆(i,j) cho Họ Ngôn ngữ và Tác vụ. Chúng tôi quan sát rằng các ngôn ngữ trong Họ IE:Germanic, xếp hạng đầu, đạt điểm cao hơn đáng kể so với trung bình, trong khi ở đầu kia, các ngôn ngữ Bantu và Afro-Asiatic hoạt động kém đáng kể so với trung bình trên các mô hình và bộ dữ liệu. Chúng tôi cũng thấy rằng các mô hình được kiểm tra tốt hơn đáng kể trong các tác vụ như MCQ Reading Comprehension và Parts of Speech Tagging (trên tất cả ngôn ngữ), hơn các tác vụ mở hơn như Q&A và Text Summarization.

## 5 Phân tích Ô nhiễm

### 5.1 Nghiên cứu Ô nhiễm Mô hình Thương mại

Trong công việc của chúng tôi, chúng tôi tuân theo phương pháp được mô tả bởi Golchin và Surdeanu (2023a) nơi chúng tôi cố gắng định lượng ô nhiễm cho các mô hình thương mại như PaLM2 và GPT-4. Đầu tiên, chúng tôi nhắc mô hình tạo ra ba biến thể của các điểm dữ liệu tập kiểm tra. Tiếp theo, chúng tôi cung cấp những biến thể này được thêm vào với văn bản gốc làm bốn tùy chọn cho mô hình, và nhắc nó chọn một tùy chọn ưa thích. Chúng tôi đo ô nhiễm như độ chính xác được điều chỉnh cơ hội sử dụng Cohen's Kappa (κ) và tính đến thiên kiến vị trí của LLM đối với một tùy chọn cụ thể bằng cách điều chỉnh việc tính κ, được gọi là κ_fixed.

Chúng tôi nghiên cứu ô nhiễm trên GPT-4 và PaLM2 cho 5 bộ dữ liệu: PAWS-X, UDPOS, TyDiQA, XNLI, và XCOPA, trên 100 điểm dữ liệu mỗi ngôn ngữ trong mỗi bộ dữ liệu. Kết quả của chúng tôi cho thấy tất cả bộ dữ liệu đều bị ô nhiễm cao trừ UDPOS, và đối với tất cả bộ dữ liệu, ô nhiễm cao hơn đối với GPT-4, hơn đối với PaLM2. Giá trị ô nhiễm cho tất cả bộ dữ liệu trên các ngôn ngữ khác nhau được báo cáo trong Phụ lục A.6. Giá trị ô nhiễm khác nhau đáng kể trên các ngôn ngữ cho cùng một bộ dữ liệu, điều này có thể do các biến thể xấu được tạo ra bởi các mô hình do hiệu suất khác nhau của chúng trong các ngôn ngữ khác nhau. Một hạn chế khác của phương pháp này là Golchin và Surdeanu (2023a) chỉ nghiên cứu thiên kiến vị trí cho các mô hình GPT và thêm văn bản gốc làm tùy chọn thứ tư dựa trên quan sát của họ. Tuy nhiên, điều này có thể khác nhau đối với các mô hình khác nhau.

### 5.2 Nghiên cứu Ô nhiễm Mô hình Mã nguồn Mở

Chúng tôi tuân theo Black Box test cho nghiên cứu ô nhiễm của mô hình mã nguồn mở được mô tả bởi Oren et al. (2023). Bài kiểm tra này là bài kiểm tra thống kê cung cấp đảm bảo có thể chứng minh rằng một tập kiểm tra nhất định bị ô nhiễm. Để đạt được những đảm bảo này, họ khai thác thực tế rằng nhiều bộ dữ liệu có thuộc tính được gọi là tính có thể hoán đổi, nơi thứ tự của các ví dụ trong bộ dữ liệu có thể được xáo trộn mà không ảnh hưởng đến phân phối kết hợp của nó. Nếu một mô hình đã thấy một bộ dữ liệu benchmark, nó sẽ có sở thích cho thứ tự chính tắc (tức là thứ tự mà các ví dụ được đưa ra trong các kho lưu trữ công cộng) so với các thứ tự ví dụ được xáo trộn ngẫu nhiên. Nếu sự khác biệt giữa thứ tự chính tắc nói trên và thứ tự xáo trộn có ý nghĩa thống kê, thì bộ dữ liệu được coi là bị ô nhiễm theo phương pháp này.

Chúng tôi tiến hành kiểm tra trên các biến thể được điều chỉnh hướng dẫn 7B của Llama2, Mistral, và Gemma trên các bộ dữ liệu đánh giá sau: PAWS-X, XCOPA, XNLI, XQUAD, XRiSAWOZ, và XstoryCloze. Mức ý nghĩa cho phân tích của chúng tôi được đặt ở 0.001. Chúng tôi quan sát (Bảng 33) rằng tất cả mô hình, trừ mô hình cơ sở Gemma, thể hiện ô nhiễm. Cụ thể, các bộ dữ liệu như PAWS-X, XCOPA, XQUAD, và XRiSAWOZ được phát hiện có giá trị p của chúng nhỏ hơn giá trị có ý nghĩa cho Gemma 7B Instruct, Llama2 7B Instruct và Mistral 7B Instruct chỉ ra ô nhiễm.

## 6 Thảo luận

Trong công việc này, chúng tôi benchmark 22 bộ dữ liệu phủ 83 ngôn ngữ trên một số mô hình – GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Gemma, Llama2, Mistral cũng như các mô hình đa phương thức. Chúng tôi thấy các xu hướng tương tự trên hầu hết bộ dữ liệu chúng tôi nghiên cứu - các mô hình thương mại lớn hơn như GPT-4 và Gemini-pro vượt trội hơn các mô hình nhỏ hơn như Gemma, Llama và mô hình Mistral, đặc biệt trên các ngôn ngữ có ít tài nguyên. Điều này gợi ý rằng hiệu suất đa ngôn ngữ là một thách thức đối với các mô hình nhỏ hơn, và các hướng như mô hình cụ thể ngôn ngữ, mô hình dựa trên họ ngôn ngữ và fine-tuning nên được khám phá để có hiệu suất đa ngôn ngữ tốt hơn.

GPT-4, PaLM2 và Gemini-Pro xuất sắc trên các bộ dữ liệu khác nhau, với GPT-4 cho thấy hiệu suất vượt trội tổng thể trên các bộ dữ liệu đa ngôn ngữ so với cả PaLM2 và Gemini-Pro. GPT-4-Vision vượt trội hơn LLaVA và Gemini-Pro-Vision trên các bộ dữ liệu đa phương thức chúng tôi nghiên cứu. Độ phì nhiêu của tokenizer có tương quan với hiệu suất Mô hình Ngôn ngữ (Rust et al., 2021; Ali et al., 2023). Chúng tôi vẽ phân tích độ phì nhiêu của tất cả các tokenizer (Hình: 14) cho các mô hình mà chúng tôi nghiên cứu trong công việc này. Chúng tôi nhận thấy rằng trung bình, các ngôn ngữ chữ Latin như tiếng Tây Ban Nha, tiếng Anh có độ phì nhiêu thấp hơn so với các ngôn ngữ phức tạp về mặt hình thái như Telugu, Malay và Malayalam có độ phì nhiêu cao trong số tất cả các tokenizer.

Ô nhiễm bộ dữ liệu là một vấn đề quan trọng ảnh hưởng đến các nghiên cứu benchmarking tiếng Anh và không phải tiếng Anh. Phân tích ô nhiễm của chúng tôi trên các mô hình mã nguồn mở và thương mại cho thấy hầu hết tất cả mô hình đều bị ô nhiễm với các bộ dữ liệu bao gồm trong MEGAVERSE. Các bộ dữ liệu đánh giá đa ngôn ngữ mới khó tạo ra do hạn chế tài nguyên và tài trợ, do đó, cần chú ý để đảm bảo rằng chúng không được bao gồm trong dữ liệu huấn luyện của LLM. Để đạt được điều này, chúng ta cần tăng cường khả năng xác định các trường hợp ô nhiễm, cũng như thực hiện các biện pháp để tránh ô nhiễm trong tương lai.

## 7 Hạn chế

Công việc của chúng tôi có những hạn chế sau:

**So sánh mô hình** Chúng tôi đã bao phủ một loạt rộng các Mô hình Ngôn ngữ Lớn. Chúng tôi nhận ra rằng truy cập vào các mô hình thương mại (GPT, PaLM2, v.v.) thông qua điểm cuối API. Những mô hình này có thể đang chạy các mô-đun xử lý hậu kỳ và bộ phân loại khác nhau dẫn đến hiệu suất tăng cao so với các mô hình Mã nguồn mở (LLaVA, Llama, Mistral).

**Ô nhiễm bộ dữ liệu** Chúng tôi thực hiện bài tập ô nhiễm bộ dữ liệu trên một vài bộ dữ liệu cho PaLM2 và GPT-4 ở mức chi tiết. Chúng tôi cũng thực hiện phân tích toàn diện về các mô hình mã nguồn mở được bao phủ trong MEGAVERSE. Tuy nhiên, có một số hạn chế mà chúng tôi thảo luận sâu trong Phần 5. Chúng tôi cũng bị hạn chế bởi tính toán và thời gian, do đó chúng tôi không thực hiện nghiên cứu ô nhiễm trên tất cả bộ dữ liệu của chúng tôi và chỉ bao phủ các biến thể 7B của các mô hình mã nguồn mở của chúng tôi.

**Điều chỉnh lời nhắc** LLM nhạy cảm với việc nhắc, và chúng tôi không thực hiện điều chỉnh lời nhắc rộng rãi cho các bộ dữ liệu mới. Chúng tôi cũng không thí nghiệm với các biến thể nhắc, như translate-test và nhắc chéo ngôn ngữ zero-shot, hoặc các chiến lược phức tạp hơn như nhắc Chain of Thought do hạn chế tài nguyên.

**Thí nghiệm trên dữ liệu và bộ dữ liệu hạn chế** Do hạn chế tài nguyên, chúng tôi thực hiện thí nghiệm trên các bộ dữ liệu một phần khi được chỉ ra, và không đánh giá tất cả mô hình trên tất cả bộ dữ liệu. Chúng tôi dự định làm như vậy trong công việc tương lai.

**Tập trung vào độ chính xác tác vụ** Chúng tôi thực hiện thí nghiệm hạn chế trên các bộ dữ liệu RAI và không thực hiện thí nghiệm trên các chiều quan trọng khác như công bằng, thiên kiến, độ bền, hiệu quả, v.v., chủ yếu do thiếu các bộ dữ liệu như vậy cho các ngôn ngữ không phải tiếng Anh. Đây là một hướng nghiên cứu tương lai quan trọng.
