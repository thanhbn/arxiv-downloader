# 2309.08590.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multilingual/2309.08590.pdf
# File size: 268255 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Neural Machine Translation Models Can Learn to be Few-shot Learners
Raphael Reinauer†andPatrick Simianer†andKaden Uhlig andJohannes E. M. Mosig
andJoern Wuebker
Lilt
{raphael.reinauer,patrick}@lilt.com
Abstract
The emergent ability of Large Language Mod-
els to use a small number of examples to learn
to perform in novel domains and tasks, also
called in-context learning (ICL). In this work,
we show that a much smaller model can be
trained to perform ICL by fine-tuning towards
a specialized training objective, exemplified on
the task of domain adaptation for neural ma-
chine translation. With this capacity for ICL,
the model can take advantage of relevant few-
shot examples to adapt its output towards the
domain. We compare the quality of this domain
adaptation to traditional supervised techniques
and ICL with a 40B-parameter Large Language
Model. Our approach allows efficient batch in-
ference on a mix of domains and outperforms
state-of-the-art baselines in terms of both trans-
lation quality and immediate adaptation rate,
i.e. the ability to reproduce a specific term after
being shown a single example.
1 Introduction
Large Language Models (LLMs) have demon-
strated few-shot learning capabilities on various
natural language processing tasks, as highlighted
by Brown et al. (2020) or Garcia et al. (2023).
When prompted with suitable example translations,
they can compete with neural machine translation
(NMT) models, built and trained specifically for
translating between languages (Vilar et al., 2023).
Interestingly, one can adapt LLMs to specific do-
mains merely by adding example translations to
their prompt at inference time (Moslem et al.,
2023). This ability to adapt to specific tasks and
domains is known as in-context learning (ICL).
In contrast to traditional fine-tuning methods, ICL
does not require a separate set of customized pa-
rameters for each domain, which implies major
efficiency gains through batched inference.
†Equal contribution.In this paper, we integrate ICL for domain adap-
tation into NMT systems in multiple steps. We
compare our method for adapting NMT systems
to traditional fine-tuning approaches, as well as to
the domain adaptation abilities of an open-source
LLM. Specifically, our main contributions are the
following:
1.We evaluate an unmodified NMT system’s
ICL capacity for domain adaptation and
demonstrate its limitations.
2.We propose a training scheme to improve an
NMT model’s ICL capability.
3.We show that ICL can be combined with more
traditional adaptation methods to further im-
prove domain adaptation performance.
4.We compare our method to the performance of
the open-source LLM FALCON -40B (Penedo
et al., 2023) on a machine translation task with
ICL for domain adaptation.
2 Related Work
Bulte and Tezcan (2019) improve the translation
performance of an NMT model by integrating trans-
lation fuzzy-matched pairs from a translation mem-
ory as input to an NMT model. This idea was fur-
ther expanded by Pham et al. (2020) and Xu et al.
(2020), who for a given source segment use sen-
tence embeddings to retrieve similar examples and
compared different schemes for integrating those
as cues into the NMT network.
Our approach differs in that we only train on the
tokens belonging to the translation and not on the
tokens provided as context, which we show to work
better. In addition, Pham et al. (2020)’s training
procedure differs, as they train their model from
scratch, using training data from multiple domains
and evaluate on those same domains, while we train
on general domain data and evaluate on a new do-
main that is not in the training data. Furthermore,
1arXiv:2309.08590v1  [cs.CL]  15 Sep 2023

--- PAGE 2 ---
we focus on the multi-domain adaptation task us-
ing light-weight adapters. This approach not only
allows us to extend to new domains without retrain-
ing the full model, but also offers a more practical
and efficient strategy for real-world applications.
The authors of (Moslem et al., 2023) investi-
gated the capabilities of a proprietary LLM, specif-
ically GPT-3.5, for adaptive machine translation
using ICL. Their extensive experiments showed
that GPT-3.5 can adapt well to in-domain sentence
pairs and/or terminology.
3 Experiments
We conduct a series of experiments to develop
NMT systems that exceed at few-shot ICL domain
adaptation. Here we present the experiments in
a logical order, where we start with the baseline
models described in Section 3.1 and subsequently
introduce several stages of development. In stages
0 and 1 we attempt ICL with the unmodified and
domain-fine-tuned baseline models, respectively.
Then, in STAGE 2, we fine-tune the baseline model
to the task of domain ICL, instead of a particu-
lar domain. Finally, we combine ICL and domain
adaptation through fine-tuning in STAGE 3. Our ex-
perimental progression was guided by the metrics
and datasets that we introduce in Sections 3.5 and
3.6, respectively.
3.1 Models
Throughout this paper, we work with an NMT sys-
tem and the FALCON -40B LLM, which we both
describe here.
3.1.1 F ALCON LLM
To provide a direct comparison with LLMs and
their capacity for ICL, we conduct experiments
with the decoder-only Transformer language model
FALCON -40B (Penedo et al., 2023), specifically
the non-instruction-tuned variant1. Inference is
done with greedy decoding. Following previous
work (Bawden and Yvon, 2023; Garcia et al.,
2023; Hendy et al., 2023) ( inter-alia ) the model
is prompted to perform translation without specific
fine-tuning towards the machine translation task.
A simple prompt template is used for all k-shot
experiments with F ALCON -40B, see Figure 1.
In preliminary experiments we found that k= 0
1The model is available from the huggingface platform:
https://huggingface.co/tiiuae/falcon-40bEnglish: <source sentence>\n
German: <target sentence>\n
English: [...]
Figure 1: Prompt template for LLM.
does not work well with this specific model2– the
outputs tend to be entirely hallucinated.
3.1.2 NMT Systems
The baseline model that we use as the starting
point for all further experiments is a Transformer
(Vaswani et al., 2017) model with 12 encoder lay-
ers and two decoder layers, implemented with the
NVIDIA NeMo toolkit (Kuchaiev et al., 2019). The
embedding size is 1,024 with a feed-forward net-
work dimension of 4,096. The model has a joint vo-
cabulary of 32,768 tokens, while embedding matri-
ces are specific to the encoder, decoder, and output
projection modules, i.e. parameters are not shared
between them. The model was trained to support a
maximum input size of 1,536 tokens by augment-
ing the training data with random concatenations
of parallel sentences. We evaluate the model using
greedy decoding.
For the experiments presented here, the baseline
model is either fine-tuned in full ( STAGE 2Aand
STAGE 2B), or light-weight adapters (Bapna and
Firat, 2019) are added to the model ( STAGE 1and
STAGE 3). We choose full-model fine-tuning on
out-of-domain data to adapt the NMT model to a
new task – translating with an increased context of
related examples – and adapter layers for learning
from in-domain data.
The adapters we use follow Bapna et al. (2019)’s
formulation, but with layer normalization applied
after the bottleneck rather than before it. We use
a bottleneck width of 256 and insert adapters in
every layer of the decoder and every other layer of
the encoder.
We always fine-tune with the ADAM optimizer
(Kingma and Ba, 2014) and early stopping based
on validation loss.
3.2 S TAGE 0 & S TAGE 1: ICL with a
Standard NMT Model
Motivated by the few-shot learning capabilities
of LLMs, we examine the ability of a standard
English-to-German NMT model to adapt to a do-
main given only similar and relevant translation
2Fork= 0 the prompt contains only the single source
sentence as input and the target language followed by a colon.
2

--- PAGE 3 ---
pairs as additional context, i.e., without changing
the model’s parameters.
To find similar source segments in the translation
memory, we search for nearest neighbours in an
embedding space. We use the multi-lingual sen-
tence embedding model3from the sentence trans-
former library (Reimers and Gurevych, 2020) to
embed the source sides of all segment pairs. Then
we employ hnswlib (Malkov and Yashunin, 2020)
to find the approximate nearest neighbours: Each
source sentence in the domain-specific datasets is
first encoded with the sentence-embedding model
and then added to an index. For the sake of simplic-
ity in this paper, we will refer to the approximate
nearest neighbors simply as nearest neighbors. To
measure the similarity between a pair of segments
sands′, we use the cosine distance of the corre-
sponding embedding vectors vsandvs′, i.e.,
d(s,s′) := 1 −vs·vs′
∥vs∥2·∥vs′∥2.
For a given source sand target segment t, we
identify its nearest neighbours s1,s2, ...,sk, using
the the cosine distance above. Each source sentence
siis paired with a reference translation tifori=
1, ..., k . We sort the pairs by their distance to sin
the embedding space, i.e.,
d(s,s1)≤d(s,s2)≤...≤d(s,sk).
Our assumption is that similar segments should
have similar translations. For STAGE 0of the exper-
iments, we treat the context sentences and actual
source text as one body of text, separated only by a
single space, ordering the segments from least simi-
lar to most similar, with the current source segment
sat the end. As a result, the input of the encoder is
<bos> s ksk−1...s1s <eos>
while for the decoder, we use the prefix:
<bos> t ktk−1...t1
where <bos> and<eos> represent the beginning-of-
sentence and end-of-sentence tokens, respectively.
The model’s task is then to continue from the target
prefix by generating a translation of the source
segment s.
In our experiments, we evaluated the translation
performance using a varying number kof nearest
neighbors, specifically k∈ {1,2,5}.
3Model name on https://www.sbert.net/ :
all-MiniLM-L6-v2InSTAGE 1we run additional experiments where
we fine-tune the model for each domain, using the
in-domain training data in the original format. This
domain-specific fine-tuning is performed by inject-
ing adapter layers (Bapna and Firat, 2019) into
the network while freezing the rest of the model,
and leveraging a standard negative log-likelihood
(NLL) loss for training. For each domain, we then
test the fine-tuned model directly ( 0-shot in Tables
3 and 4) as well as with ICL ( k-shot with k̸= 0).
Adapters are trained towards convergence, i.e.
until there is no further improvement in terms of
validation loss.
3.3 S TAGE 2A& S TAGE 2B: Fine-Tuning
towards ICL
To improve the model’s capability to use nearest
neighbor examples in the context, we further fine-
tune the full model on out-of-domain data, namely
News-Commentary4(Kocmi et al., 2022), which
contains roughly 450K parallel segments. For
validation we use a sample of 2K parallel seg-
ments from EuroParl5(Koehn, 2005). For this
full model fine-tuning we do not train until conver-
gence, but apply aggressive early stopping: Train-
ing is stopped when the validation loss does not
decrease by at least 0.1 twice in a row, validating
for every 1% of an epoch. This is to encourage the
model to only learn the new task and data format,
but not adapt to a new data distribution.
Instead of directly concatenating the nearest
neighbors to the training examples, we add a spe-
cial separation token – <sep> – to separate the
source and target segments. We then construct the
training instances for the encoder as:
<bos> s k<sep> s k−1<sep>...<sep> s 1<sep> s <eos>
and for the decoder as:
<bos> t k<sep> t k−1<sep>...<sep> t 1<sep> t <eos>
(1)
and compute the NLL loss on all tokens of (1).
This training loss is identical to the one used in
Pham et al. (2020). We denote this procedure as
STAGE 2A.
ForSTAGE 2Bthe idea is that the model should
learn to predict the target segment from the source
4From the WMT’23 evaluation campaign: https://data.
statmt.org/news-commentary/v18.1/
5Also from the WMT’23 evaluation campaign: https:
//www.statmt.org/europarl/v10/
3

--- PAGE 4 ---
segment using the nearest neighbor translations
but not learn to predict tk, ..., t 1as in (Pham et al.,
2020). Hence we mask the NLL training loss such
that it is computed only on the tokens that belong to
the target segment t, excluding all context tokens,
thus fully focusing the training signal on translating
tin the context of its knearest neighbors.
We then use the same format as in STAGE 2A
for training, while at inference time we provide the
decoder with a prefix containing the ICL examples:
<bos> t k<sep> t k−1<sep>...<sep> t 1<sep>
Finally, we measure quality of the predicted trans-
lation ˆtby computing BLEU and COMET scores
with the target segment tas reference.
For both STAGE 2AandSTAGE 2B, thek-nearest
neighbors for each segment in the training data and
validation data are extracted from the entire News-
Commentary dataset as described in Section 3.2.
3.4 S TAGE 3: Combining ICL and Domain
Adaptation
To combine STAGE 2B’s ICL capacity with adapter-
based domain adaptation, we add adapters to the
model from STAGE 2Busing the same configura-
tion as for the STAGE 1experiments. Again, we
train separate adapter layers for each domain.
Each example from the training set is annotated
with its nearest neighbors from the same training
set, excluding itself.
3.5 Metrics
For evaluating translation quality, we used the
SacreBLEU framework (Post, 2018) that imple-
ments the BLEU metric (Papineni et al., 2002). We
also evaluate with reference-based COMET (Rei
et al., 2022) to compare the model outputs to the
reference translations in the test data.
3.6 Datasets
We run our experiments with the English-German
language pair on 8 domains from the ACED- and
MDNS corpus collections, which we describe in
this section. Statistics for all datasets are provided
in Table 1.
3.6.1 ACED corpus
The ACED corpus (Lin et al., 2022) is comprised of
three distinct datasets, namely Asics, Emerson, and
Digitalocean, each consisting of English-German
sentences extracted from various domains. ACED
is a real-world benchmark containing data derived
from translations performed by humans.Training Validation Test
Asics 1.4 0.5 0.6
Digitalocean 11.8 2.0 7.6
Emerson 4.3 1.3 1.7
IT 223 2.0 2.0
Koran 17.9 2.0 2.0
Law 467 2.0 2.0
Medical 248 2.0 2.0
Subtitles 500 2.0 2.0
Table 1: Segment counts for the domain-specific dataset
splits used for experimentation, in thousands.
3.6.2 MDNS corpus
The MDNS corpus (Aharoni and Goldberg, 2020)
is a multi-domain corpus containing English-
German parallel text from five diverse domains (IT,
Koran, Law, Medical, Subtitles). It was specifically
created for evaluating domain-adaptation.
4 Results
Here we discuss the experimental results, progress-
ing from STAGE 0toSTAGE 3. All results are
depicted separately for ACED- and MDNS corpora
in Tables 3 and 4 respectively.
4.1 S TAGE 0: ICL with Baseline NMT Model
When we add nearest neighbors to the inputs and
target prefixes we first observe that the automated
metrics are mostly improved across all datasets.
Notably, the result with 1-shot nearest neighbors is
the best in this group of experiments. Additionally
we find that the 5-shot result often degrades below
the baseline.
Specifically for the Medical and Subtitles cor-
pora of MDNS, we find that the model fails to
improve over the baseline for all k.
The cosine distance of the nearest neighbors
seems to be a viable indicator of performance in
this set of experiments, e.g. when comparing the re-
sults for ACED Emerson & Digitalocean, where the
average cosine distance (see Table 2) for k= 1is
much lower for Emerson at 0.13, versus 0.3 for Dig-
italocean. We find a moderate, statistically insignif-
icant, negative Pearson correlation ( r=−0.43)
between the average cosine distances for k= 1
and the difference in BLEU scores between the
STAGE 0 1-shot experiment and the baseline.
4

--- PAGE 5 ---
ACED MDNS
Asics Digitalocean Emerson IT Koran Law Medical Subtitles
k= 1 0.19 0.30 0.13 0.15 0.18 0.13 0.12 0.24
k= 2 0.21 0.31 0.14 0.17 0.20 0.15 0.14 0.25
k= 5 0.23 0.34 0.16 0.21 0.24 0.17 0.17 0.27
Table 2: Average cosine distance in embedding space of test set sources to k-nearest neighbors from train, for
k∈ {1,2,5}.
Asics Digitalocean Emerson Average
BLEU COMET BLEU COMET BLEU COMET BLEU COMET
Baseline 34.5 0.8624 53.3 0.9043 44.9 0.9108 44.2 0.8925STAGE 01-shot 43.7 0.8578 54.4 0.8982 72.1 0.9213 56.7 0.8924
2-shot 44.5 0.8525 54.5 0.8967 67.2 0.9137 55.4 0.8876
5-shot 41.0 0.8420 53.9 0.8955 28.7 0.8705 41.2 0.8693STAGE 10-shot 41.2 0.8780 60.1 0.9152 79.2 0.944 60.2 0.9124
1-shot 46.4 0.8657 59.6 0.9099 78.1 0.9378 61.4 0.9045
2-shot 46.2 0.8628 59.0 0.9090 66.3 0.9275 57.2 0.8998
5-shot 44.2 0.8500 57.3 0.9038 32.2 0.893 44.6 0.8823STAGE 2A1-shot 43.0 0.8765 55.0 0.9073 73.1 0.9382 57.0 0.9073
2-shot 43.5 0.8785 54.4 0.9072 71.6 0.9392 56.5 0.9083
5-shot 42.3 0.8662 54.4 0.9066 73.4 0.9347 56.7 0.9025STAGE 2B1-shot 44.5 0.8766 54.9 0.9046 73.1 0.9391 57.5 0.9068
2-shot 44.5 0.8777 55.4 0.9080 74.3 0.939 58.1 0.9082
5-shot 44.7 0.8734 55.0 0.9072 70.0 0.9363 56.6 0.9056STAGE 31-shot 48.8 0.8896 60.5 0.9141 78.9 0.9480 62.7 0.9172
2-shot 48.5 0.8914 60.1 0.9132 80.7 0.9456 63.1 0.9167
5-shot 47.6 0.8837 59.0 0.9095 80.2 0.9437 62.3 0.9123Falcon1-shot 31.8 0.8588 40.0 0.8677 71.6 0.9380 47.8 0.8882
2-shot 34.5 0.8671 44.8 0.8876 76.9 0.9416 52.1 0.8988
5-shot 40.8 0.8789 X X 78.5 0.9434 X X
Table 3: Results for the ACED corpus of the multi-stage evaluation for various numbers of k-nearest-neighbors,
using BLEU and COMET metrics. The "Baseline" scores are for the English-to-German NMT system described in
Section 3.1. We omit the Digitalocean dataset for the F ALCON -40B 5-shot evaluation.
While BLEU indicates improvement (COMET
reduces only for k >1), we find that the model’s
behavior is in fact degenerate. Specifically, the
model often fails to produce any output after the
given prefix and instead predicts <eos> immedi-
ately, which leads to empty translations. We find
that the rates of empty translations are 8.5%, 8.1%,
and 9.1% for k= 1,2, and 5 respectively. In con-
trast, the baseline system has a 0% rate of empty
outputs. This is despite the model being specif-
ically trained to support inputs covering the full
context-width in pre-training.4.2 S TAGE 1: Combining ICL with Domain
Fine-Tuning
ForSTAGE 1we first observe that the model can
be effectively adapted to each domain by training
adapters (see the STAGE 1, 0-shot results in Ta-
bles 3 and 4). A notable exception is MDNS Subti-
tles, where adaptation only slightly improves over
the baseline. This result is, however, consistent
with other work (Aharoni and Goldberg, 2020).
When we combine the trained adapters with ICL,
we find no improvements over STAGE 1’s 0-shot
results, with the exception of ACED Asics.
Performance drops catastrophically for the
MDNS Medical & Subtitles corpora. The rate
5

--- PAGE 6 ---
IT Koran Law Medical Subtitles Average
BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET
Baseline 34.3 0.8153 14.7 0.7229 44.7 0.8696 43.5 0.8406 27.7 0.7891 33.0 0.8075STAGE 01-shot 35.9 0.7698 17.2 0.6580 51.6 0.853 42.3 0.7964 17.5 0.6358 32.9 0.7426
2-shot 35.9 0.7433 17.2 0.6346 49.9 0.8467 38.2 0.7810 22.4 0.7024 32.7 0.7416
5-shot 31.9 0.7196 14.5 0.6000 42.3 0.8287 30.5 0.7505 24.4 0.7400 28.7 0.7278STAGE 10-shot 39.6 0.8403 22.6 0.7274 50.7 0.8824 47.8 0.8429 28.1 0.7879 37.8 0.8162
1-shot 36.7 0.7620 21.1 0.6434 51.1 0.8228 7.1 0.5078 0.0 0.4306 23.2 0.6333
2-shot 35.6 0.7436 20.5 0.6152 48.9 0.8019 15.9 0.5441 0.0 0.4208 24.2 0.6251
5-shot 32.8 0.7296 18.4 0.5980 44.9 0.7940 23.4 0.5854 16.8 0.6388 27.3 0.6692STAGE 2A1-shot 34.3 0.8277 15.5 0.7222 49.5 0.8739 43.6 0.8380 25.7 0.7838 33.7 0.8091
2-shot 35.8 0.8244 16.4 0.7154 49.6 0.8739 44.6 0.8362 24.1 0.7810 34.1 0.8062
5-shot 34.3 0.8203 15.9 0.7083 48.1 0.8659 40.7 0.8220 24.0 0.7712 32.6 0.7975STAGE 2B1-shot 34.6 0.8269 16.0 0.7217 50.4 0.8752 44.2 0.8405 25.9 0.7830 34.2 0.8095
2-shot 35.5 0.8182 16.5 0.7150 49.9 0.8747 43.4 0.8349 24.5 0.7774 34.0 0.8040
5-shot 33.5 0.8103 16.6 0.7070 48.2 0.8696 37.5 0.8274 25.2 0.7782 32.2 0.7985STAGE 31-shot 41.4 0.8423 28.8 0.7235 58.1 0.8862 52.9 0.8488 27.0 0.7846 41.6 0.8171
2-shot 41.7 0.8401 29.6 0.7225 57.3 0.8850 51.2 0.8480 27.6 0.7850 41.5 0.8161
5-shot 40.9 0.8296 29.2 0.7249 55.8 0.8804 48.7 0.8413 27.5 0.7876 40.4 0.8128Falcon1-shot 31.5 0.7985 17.9 0.7081 45.4 0.8538 42.4 0.8035 21.7 0.7586 31.8 0.7845
2-shot 35.5 0.8202 22.4 0.7263 49.5 0.8680 47.5 0.8288 21.4 0.7605 35.3 0.8008
5-shot 40.1 0.8377 24.5 0.7358 50.5 0.8749 50.1 0.8401 22.6 0.7776 37.6 0.8132
Table 4: Results for the MDNS corpus of the multi-stage evaluation for various numbers of k-nearest-neighbors
using BLEU and COMET metrics. The "Baseline" scores are for the English-to-German NMT system described in
Section 3.1.
of empty translations also increases dramatically6,
with a rate of up to 63.1% for the 1-shot result on
MDNS Medical (up from 8.0% at S TAGE 0).
4.3 S TAGE 2A& S TAGE 2B: Fine-Tuning
towards ICL
When we compare the STAGE 2B(fine-tuning with
the masked loss as described in Section 3.3) to the
STAGE 0results, we find that adding the separa-
tor and fine-tuning the model leads to generally
improved scores on the ACED corpora for all k.
BLEU Results on MDNS corpora show slightly
worse performance compared to the STAGE 0re-
sults in 3 out of 5 corpora for k= 1, but the aver-
ages are still improved. COMET scores are how-
ever consistently improved for this comparison. We
also find that the scores for k= 2 andk= 1 are
very close, with 2-shot being ahead of 1-shot by
0.6% BLEU points on average on ACED data, and
1-shot being ahead of 2-shot by 0.2 BLEU points
on MDNS. Which is in contrast to what we have
observed in STAGE 0.k= 5still performs worse,
6Empty translation rates of STAGE 1for each kover all
corpora: 1-shot: 20.0%, 2-shot: 20.6%, 5-shot: 13.6%.but we observe high relative gains compared to the
5-shot S TAGE 0 result.
When comparing STAGE 2AandSTAGE 2B, i.e.
the masked loss and the standard NLL loss the
results are inconclusive.
We further observe that STAGE 2Bexhibits al-
most negligible rates of producing empty transla-
tions, at 0.3%, 0.8%, and 1.2% for k= 1,2,5
respectively.
4.4 S TAGE 3: Combining ICL and Domain
Adaptation
When combining ICL with adapters trained with
nearest neighbor annotated data, we observe the
globally best results for the NMT models. Com-
pared to STAGE 1, which is also fine-tuned towards
each domain, we observe greatly improved results
on all automatic metrics. STAGE 32-shot deliv-
ers the best result on ACED, with an improvement
of 2.5 BLEU points compared to the runner-up
in terms of average BLEU STAGE 11-shot. On
MDNS, STAGE 31-shot improves over the runner-
up S TAGE 1 0-shot by 3.8 points.
Especially the scores for MDNS Koran improve
6

--- PAGE 7 ---
well above all previous models, with a relative im-
provement of 101% compared to the baseline. The
models seem to be able to make better use of close
nearest neighbors in this dataset, which are often
substrings of one another. See Section 4.6 for a
detailed analysis of the copying behavior on the
ACED Asics dataset.
The rate of empty translations is reduced to 0.0%
for all k.
We further notice that the results for 1- and 2-
shot ICL are very similar, and that the scores for
5-shot are also improved.
4.5 F ALCON : Adapting Both to a Task and a
Domain at the Same Time
The FALCON -40B LLM proves to excel at ICL,
learning a task and adapting to a domain at the same
time. Notably, scores improve with higher values
ofk, which is the opposite behavior to what we
have observed with NMT models. When nearest
neighbors are close to the test data, as they are
for the ACED Emerson and MDNS IT datasets,
we find results that are close to the best STAGE 3
results.
FALCON -40B ’s generation speed is however
very slow at an average of 2.6 tokens per second in
the 1-shot setting.
Also note that we have no means at this time to
check whether parts of the test data are contained
in F ALCON ’s training data.
4.6 Qualitative Analysis
Maintaining consistency in translations is an im-
portant quality criterion in the localization industry,
and is a major motivator in the use of translation
memories, which help ensure that marketing ma-
terials, for example, are uniform in the promised
features and functions of the products being adver-
tised (Emery et al., 2011). In NMT models, this
consistency is traditionally increased by fine-tuning
a translation model for a specific domain, which
we denote by " STAGE 1with 0-shot". In this sec-
tion, we compare the fine-tuning approach with our
ICL, specifically " STAGE 3with 1-shot". We eval-
uate translation consistency on the Asics dataset.
For that purpose we select segments sin the test
data for which the source nearest neighbor s′in
the Asics train data differs by exactly one word.
These segments sare denoted as word-substitution
segments. For each pair ( s,s′), we then use two
sources and one target t′in the ICL prompt and the
other target tas reference to compare the generatedtranslation to. We define the fraction of pairs for
which the generated translation exactly matches the
reference as the word substitution accuracy (WSA).
The results are in Table 6.
The translation for STAGE 31-shot achieves a
WSA score of 74.6%, compared to 57.14% for the
fine-tuning approach ( STAGE 10-shot), whereas
the non-adapted model only produces the exact
reference translation in 1.7% of cases.
5 Conclusions
We have shown that a standard NMT system can
be trained to be an effective in-context learner in
domain adaptation tasks. We find that this is most
effective when we combine generic fine-tuning to-
wards the ICL task and training adapter layers for
a specific domain with nearest neighbor annotated
data.
When the model is not fine-tuned towards the
task, we find that ICL works to some extent, but
shows degenerate behavior.
While LLMs like FALCON -40B can adapt to
the MT task with ICL, this comes at the cost of
increased compute. Generally, the results with the
LLM still underperform our dedicated MT models.
References
Roee Aharoni and Yoav Goldberg. 2020. Unsupervised
domain clusters in pretrained language models. In
Proceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers) . Association for Computational Linguistics.
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.
2019. Simple, scalable adaptation for neural machine
translation. CoRR , abs/1909.08478.
Ankur Bapna and Orhan Firat. 2019. Simple, scal-
able adaptation for neural machine translation. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 1538–
1548, Hong Kong, China. Association for Computa-
tional Linguistics.
Rachel Bawden and François Yvon. 2023. Investigating
the translation performance of a large multilingual
language model: the case of BLOOM. In Proceed-
ings of the 24th Annual Conference of the European
Association for Machine Translation , pages 157–170,
Tampere, Finland. European Association for Machine
Translation.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
7

--- PAGE 8 ---
Source Strive for every point in the women’s GEL-
DEDICATE ™ 6 CLAY tennis shoe by ASICS.
Reference Translation Strebe nach jedem Punkt in dem GEL-DEDICATE
™ 6 CLAY Tennisschuh für Damen von ASICS.
BASELINE Mit dem GEL-DEDICATE ™ 6 CLAY
Damen-Tennisschuh von ASICS kannst du jeden
Punkt erreichen.
STAGE 1 with 0-shot Mit dem ASICS GEL-DEDICATE
™ 6 CLAY Tennisschuh für Damen
kannst du jeden Punkt erreichen.
STAGE 3 with 1-shot Strebe nach jedem Punkt in dem GEL-DEDICATE
™ 6 CLAY Tennisschuh für Damen von ASICS.
Table 5: Comparison of example translation outputs from different models and the reference translation. Words
that differ from the reference translation are highlighted in blue . The nearest source neighbor is "Strive for every
point in the men’s GEL-DEDICATE ™6 CLAY tennis shoe by ASICS." with the reference translation "Strebe nach
jedem Punkt in dem GEL-DEDICATE ™6 CLAY Tennisschuh für Herren von ASICS.". Notice that the nearest
neighbor only differs by one word in each language.
STAGE 3 with 1-shot STAGE 1 with 0-shot Non-Adapted Model
Word-substitution segments 74.60% 57.14% 1.7%
Table 6: Results for word substitution accuracy (WSA, cf. subsection 4.6) for various adapted and non-adapted
models for word-substitution segments.
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in Neural Information Processing
Systems , 33:1877–1901.
Bram Bulte and Arda Tezcan. 2019. Neural fuzzy re-
pair: Integrating fuzzy matches into neural machine
translation. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 1800–1809, Florence, Italy. Association for
Computational Linguistics.
Vince Emery, Karl Kadie, and Mary Laplante. 2011.
Multilingual Marketing Content: Growing Interna-
tional Business with Global Content Value Chains .
Outsell.
Xavier Garcia, Yamini Bansal, Colin Cherry, George
Foster, Maxim Krikun, Melvin Johnson, and Orhan
Firat. 2023. The unreasonable effectiveness of few-
shot learning for machine translation. In Proceedings
of the 40th International Conference on Machine
Learning , volume 202 of Proceedings of Machine
Learning Research , pages 10867–10878. PMLR.
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,
Young Jin Kim, Mohamed Afify, and Hany Has-
san Awadalla. 2023. How good are GPT models
at machine translation? A comprehensive evaluation.
arXiv preprint arXiv:2302.09210 .
Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. International
Conference on Learning Representations .Tom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton
Dvorkovich, Christian Federmann, Mark Fishel,
Thamme Gowda, Yvette Graham, Roman Grund-
kiewicz, Barry Haddow, Rebecca Knowles, Philipp
Koehn, Christof Monz, Makoto Morishita, Masaaki
Nagata, Toshiaki Nakazawa, Michal Novák, Martin
Popel, and Maja Popovi ´c. 2022. Findings of the 2022
conference on machine translation (WMT22). In
Proceedings of the Seventh Conference on Machine
Translation (WMT) , pages 1–45, Abu Dhabi, United
Arab Emirates (Hybrid). Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
machine translation summit x: papers , pages 79–86.
Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii
Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kri-
man, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook,
Patrice Castonguay, Mariya Popova, Jocelyn Huang,
and Jonathan M. Cohen. 2019. Nemo: a toolkit for
building ai applications using neural modules.
Jessy Lin, Geza Kovacs, Aditya Shastry, Joern Wuebker,
and John DeNero. 2022. Automatic correction of
human translations. In Proceedings of the 2022 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies , pages 494–507, Seattle, United
States. Association for Computational Linguistics.
Yu A Malkov and Dmitry A Yashunin. 2020. Efficient
and robust approximate nearest neighbor search us-
8

--- PAGE 9 ---
ing hierarchical navigable small world graphs. IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence , 42(4):824–836.
Yasmin Moslem, Rejwanul Haque, John D. Kelleher,
and Andy Way. 2023. Adaptive machine translation
with large language models. In Proceedings of the
24th Annual Conference of the European Association
for Machine Translation , pages 227–237, Tampere,
Finland. European Association for Machine Transla-
tion.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow,
Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The RefinedWeb dataset
for Falcon LLM: Outperforming curated corpora with
web data, and web data only.
M. Pham, Jitao Xu, Josep Maria Crego, François Yvon,
and Jean Senellart. 2020. Priming neural machine
translation. In Conference on Machine Translation .
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Ricardo Rei, José G. C. de Souza, Duarte Alves,
Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova,
Alon Lavie, Luisa Coheur, and André F. T. Martins.
2022. COMET-22: Unbabel-IST 2022 submission
for the metrics shared task. In Proceedings of the
Seventh Conference on Machine Translation (WMT) ,
pages 578–585, Abu Dhabi, United Arab Emirates
(Hybrid). Association for Computational Linguistics.
Nils Reimers and Iryna Gurevych. 2020. Making
monolingual sentence embeddings multilingual us-
ing knowledge distillation. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing . Association for Computational
Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , pages 5998–6008.
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
Viresh Ratnakar, and George Foster. 2023. Prompt-
ing PaLM for translation: Assessing strategies and
performance. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 15406–
15427, Toronto, Canada. Association for Computa-
tional Linguistics.Jitao Xu, Josep Crego, and Jean Senellart. 2020. Boost-
ing neural machine translation with similar transla-
tions. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
1580–1590, Online. Association for Computational
Linguistics.
9
