# 2406.00302.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/federated-learning/2406.00302.pdf
# File size: 4287803 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FedAST: Federated Asynchronous Simultaneous Training
Baris Askin1Pranay Sharma1Carlee Joe-Wong1Gauri Joshi1
1Carnegie Mellon University, Pittsburgh, Pennsylvania, USA
Abstract
Federated Learning (FL) enables edge devices or
clients to collaboratively train machine learning
(ML) models without sharing their private data.
Much of the existing work in FL focuses on effi-
ciently learning a model for a single task. In this
paper, we study simultaneous training of multiple
FL models using a common set of clients. The few
existing simultaneous training methods employ
synchronous aggregation of client updates, which
can cause significant delays because large models
and/or slow clients can bottleneck the aggregation.
On the other hand, a naïve asynchronous aggrega-
tion is adversely affected by stale client updates.
We propose FedAST , a buffered asynchronous fed-
erated simultaneous training algorithm that over-
comes bottlenecks from slow models and adap-
tively allocates client resources across heteroge-
neous tasks. We provide theoretical convergence
guarantees of FedAST for smooth non-convex ob-
jective functions. Extensive experiments over mul-
tiple real-world datasets demonstrate that our pro-
posed method outperforms existing simultaneous
FL approaches, achieving up to 46.0%reduction
in time to train multiple tasks to completion.
1 INTRODUCTION
Federated Learning (FL) is a distributed learning paradigm
where edge devices or clients collaboratively train ma-
chine learning (ML) models using privately held local data
[McMahan et al., 2017, Kairouz et al., 2021]. Clients iter-
atively update their local models, which are periodically
sent to a central server for aggregation. The aggregated
model is then sent to the clients to begin the next round
of local updates. Since its introduction in [McMahan et al.,
2017], various practical and theoretical aspects of FL, in-cluding client selection [Nishio and Yonetani, 2019, Cho
et al., 2022], communication challenges [Ang et al., 2020,
Chellapandi et al., 2023], scalability and fast training [Xie
et al., 2019, Wang et al., 2020b], have been extensively stud-
ied. However, these works almost exclusively assume that
the server aims to learn model(s) for a single task . Some FL
frameworks attempt to learn models personalized to each
client [Mansour et al., 2020, Li et al., 2021, Tan et al., 2022],
but these models are still intended for the same learning
task, e.g., next-word prediction on keyboards.
Many practical applications need devices to perform a wide
range of learning tasks, which require training of multiple
ML models. For instance, our phones need language models
for keyboard next-word prediction as well as image recom-
mendation models to highlight images more likely to be
shared [McMahan et al., 2017]. Yao et al. [2023] propose
training multiple models in federated smart car networks for
different tasks, such as pothole detection and maneuver pre-
diction. Another example can be a chat application requiring
speech recognition and response text generator models con-
currently, while Le et al. [2022] suggest federated learning
of multiple models for air quality index forecasting. Thus,
in this paper, we seek to answer the following question:
How can we efficiently train models for multiple tasks in a
federated setting using a shared pool of clients?
Simple Solutions that Extend FedAvg. A naïve approach
to training multiple models is sequential training , where the
models corresponding to different tasks are trained one at
a time, each utilizing all the clients. The total training run-
time of this approach scales linearly with the number of
tasks. An alternative is for all the clients to train all tasks
at the same time. However, with this approach each client
will have to keep all models in memory, which is infeasible
for resource-limited edge clients such as smartphones. To
preserve memory, clients will have to queue the training
requests and process them sequentially, again resulting in
the runtime linearly increasing with the number of tasks. On
the other hand, parallel orsimultaneous training (ST) of allarXiv:2406.00302v1  [cs.LG]  1 Jun 2024

--- PAGE 2 ---
the models with time-varying subsets of clients assigned to
each task can strike a better trade-off between accuracy and
runtime. Bhuyan et al. [2023]’s approach assigns a disjoint
subset of clients to each model in each round, which signif-
icantly improves the time taken to reach a target accuracy
as compared to sequential training. However, these feder-
ated simultaneous training (FST) approaches leave room
for significant improvement. There are two particular draw-
backs: 1) straggler delays due to synchronous aggregation,
and 2) the lack of adaptation to the training progress of
heterogeneous tasks, which we address in this work.
Synchronous Aggregation and Straggler Delays. Con-
ventional FL employs synchronous aggregation, where in
each round, the server waits to receive updates from all
the participating clients before each aggregation. However,
when the clients have diverse hardware and communication
capabilities, faster clients must remain idle until slow or
straggling clients finish, causing a large wallclock runtime
to complete each communication round. This problem is fur-
ther exacerbated in FL with multiple simultaneous models
[Bhuyan et al., 2023, Zhou et al., 2022], where the aggre-
gation is synchronized across tasks as well. Therefore, the
server has to wait for the slowest client across allthe parallel
tasks. Solutions proposed to alleviate the straggler problem
in the single-model context include allowing faster clients to
run more local steps [Wang et al., 2020b], aggregating only
the client updates that arrive before a timeout [Bonawitz
et al., 2019], and sub-sampling from the set of available
clients [Luo et al., 2022]. Although these approaches per-
form well when stragglers appear uniformly at random, they
do not work well in the simultaneous training setting be-
cause some models (e.g., larger ones) are naturally slower
to train. When the multiple models have inherently differ-
ent training times, synchronized global aggregation rounds
are bottlenecked by the slowest client assigned to the most
computationally intensive model, leading to large idle times.
Asynchronous Aggregation and Staleness Issues. An-
other solution to the straggler problem is asynchronous
aggregation at the server, as proposed in AsyncFL [Xie
et al., 2019], where the server updates the global model
whenever it receives any client update. While asynchronous
aggregation has been extensively studied in single-model
federated learning [Chen et al., 2020, Wang et al., 2022, Xu
et al., 2023, Yu et al., 2023a], it has not been well-explored
for simultaneous federated training. Although AsyncFL ad-
dresses the straggler issue, it suffers from undesired stale-
ness even in the standard FL setting, since the received client
updates are often based on outdated models. To alleviate the
staleness problem in single-model FL, Nguyen et al. [2022]
proposed storing the incoming client updates in a buffer at
the server and aggregating when the buffer is full.Adaptive Allocation of Clients to Heterogeneous Tasks.
In this work, we employ asynchronous buffered aggregation
to overcome the straggler issue while controlling staleness.
However, extending single-model FL algorithms [Xie et al.,
2019, Nguyen et al., 2022] to the simultaneous training of
multiple models is not straightforward — running multiple
independent instances of asynchronous FL can be subop-
timal. This is because the tasks can have heterogeneous
computation complexities and different data heterogeneity
that affect both the number of rounds required to achieve a
given target accuracy as well as the wall-clock time taken to
complete each round. Since a shared set of clients is used to
train the models, the training processes are coupled – more
resources assigned to one task implies less for the others.
Moreover, the optimal resource requirement for each task
can change over time according to its data heterogeneity
and training progress and may be difficult to predict before
training. Therefore, we propose an adaptive algorithm that
dynamically reallocates clients across tasks depending on
their training progress, and also adapts the buffer size used
for asynchronous aggregation of updates.
Our Contributions. We formalize the FST setting in Sec-
tion 2 and then make the following main contributions:
•We introduce FedAST , a Federated Asynchronous
Simultaneous Training algorithm1to simultaneously train
models for multiple tasks (Section 3). Our work is one
of the first to mitigate the straggler problem faced by
synchronous FST methods that extend vanilla FedAvg.
•The proposed algorithm addresses the problem of bal-
ancing resources across heterogeneous tasks, a unique
challenge to the FST framework, using novel dynamic
client allocation, and it also dynamically adjusts the buffer
size used in asynchronous aggregation to strike the best
trade-off between staleness and runtime.
•We provide a theoretical convergence analysis of FedAST
(Section 4), which improves previous analyses even in the
single-model FL setting. It improves upon [Koloskova
et al., 2022] by considering multiple local updates and
the buffer, and on [Nguyen et al., 2022] by relaxing the
restrictive assumptions.
•We experimentally validate FedAST ’s performance (Sec-
tion 5) in terms of its wall-clock training time and model
accuracy on multiple real-world datasets compared to
synchronous and asynchronous FL baselines.
We conclude and discuss future work in Section 6.
Related Work. Only a few recent works [Zhou et al.,
2022, Bhuyan and Moharir, 2022, Siew et al., 2023, Bhuyan
et al., 2023] consider federated simultaneous training of
multiple models. In [Zhou et al., 2022], clients are selected
1Our code is provided at https://github.com/
askinb/FedAST .

--- PAGE 3 ---
with either Bayesian optimization or reinforcement learning
to minimize training time and unfairness in participation.
Bhuyan and Moharir [2022] formulate the client assignment
FL as a bandit problem leveraging local training losses as
scores. Siew et al. [2023] introduce biased client sampling,
favoring the clients with higher local losses. These methods
lack convergence guarantees. Bhuyan et al. [2023] assign
clients uniformly at random or in a round-robin fashion
and analyze the convergence assuming convex objective
functions and bounded gradients. While these works only
consider synchronous aggregation, Chang et al. [2023] pro-
pose a fully asynchronous FST algorithm. Their approach
entails solving a non-convex optimization problem to opti-
mize client assignment, which requires information about
delays and models that may be difficult to obtain in practice.
Also, the obtained bound does not converge to a stationary
point in the presence of data heterogeneity and suffers from
increased staleness when the number of clients increases.
Lastly, Liu et al. [2023] propose an extension of their single-
model adaptive asynchronous approach to the multi-model
setting. However, they do not carefully handle heteroge-
neous data distributions across clients and the staleness of
updates in their theoretical guarantees for a single model,
and they lack these guarantees for multiple models. Also,
their method under-utilizes client resources, since after send-
ing a model update to the server, the clients are idle until
the next training round.
2 PROBLEM FORMULATION
Notations. For a positive integer c, we define [c]≜
{1, . . . , c }.e∇denotes stochastic gradients. Bold lowercase
letters (e.g., x) denote vectors. |A|denotes the cardinality
of set A.∥ · ∥ denotes the Euclidean norm.
We now formally introduce the federated simultaneous
training (FST) setting, where Nclients train Mmodels
x1, . . . ,xMcorresponding to Mindependent tasks. For
each task m∈[M], our goal is to find the model that solves
that following optimization problem:
min
x∈Rdm(
fm(x) :=1
NNX
i=1fm,i(x))
, (1)
where fmis the global loss function for task m, andfm,iis
the local loss for task mat client i.
First, we examine a simple extension of FedAvg [McMahan
et al., 2017] to simultaneous training of models for Mtasks.
At the start of each round, the server randomly partitions the
available set of clients across the tasks [Bhuyan et al., 2023].
The server sends the current models {x(tm)
m}M
m=1for all
the tasks to the corresponding subset of clients. The clients
perform local training (Algorithm 1) and return their updates
to the server, which synchronously aggregates the updates
for each task. This naïve simultaneous training extension ofAlgorithm 1 LocalTrain (m,τm,x(tm)
m,ηc
m)at client i
1:Setx(tm,0)
m,i←x(tm)
m
2:fork= 1, . . . , τ mdo
3:x(tm,k)
m,i←x(t,k−1)
m,i −ηc
me∇fm,i(x(t,k−1)
m,i )
4:end for
5:Return ∆m←(x(tm)
m−x(tm,τm)
m,i )/(τmηc
m)
FedAvg performs poorly due to stragglers. The time it takes
for a client to return its updates depends on its resources and
the size of the model assigned. Since the server waits for the
slowest update across all the tasks before commencing the
next round, the server waits much longer if a large model
is assigned to a slow client. We mitigate this problem via
asynchronous training in FedAST , discussed next.
3 ALGORITHM DESCRIPTION
Next, we describe FedAST (Algorithm 2), our proposed
Federated Asynchronous Simultaneous Training algorithm,
illustrated in Figure 1 for M= 2 tasks. For each task
m∈[M], the server maintains a round index tmthat is
initialized to tm= 0, the number of active training requests
R(tm)
m, and buffer size b(tm)
m.R(tm)
m andb(tm)
m quantify the
resources (client computation and memory) allocated to
taskmin round tm. We provide two versions of FedAST
based on the value of option ∈ {S, D}. When option isS
(static ), the resource allocation for each task remains the
same throughout the training process (i.e., R(tm)
m≡Rm
andb(tm)
m≡bm). With option =D(dynamic ),FedAST
dynamically reallocates resources across tasks using the
Realloc subroutine (Algorithm 3).
Assignment of Local Training Requests to Clients and
Their Execution. Consider task m∈[M]. The server be-
gins by sending out R(0)
mlocal training requests for task mto
clients selected uniformly at random, along with the initial
model x(0)
m(Algorithm 2, Line 4). The number of local train-
ing requests R(tm)
m is adapted over time using the Realloc
function (Algorithm 3), enabling us to dynamically reallo-
cate client resources across tasks. Each client processes the
training request by performing τmlocal mini-batch SGD
iterations (see Algorithm 1) and sends the resulting model
update ∆mback to the server. If a client receives multiple
requests, they are queued and processed in a first-come-
first-served manner.2Therefore, the number of active clients
(clients working on training requests) at any time might be
less than the number of active training requests (that clients
are working on or are stored in their queues).
2Processing the requests in parallel would require clients to
keep all the Mmodels in local memory, which can be infeasible.

--- PAGE 4 ---
Algorithm 2 FedAST
1:Input : Client and server learning rates {ηc
m, ηs
m}M
m=1,
option ∈ {S, D}, no. of local updates {τm}M
m=1
2:Initialize: ∀m∈[M]:tm←0(round index), model
x(0)
m, buffer Bm← ∅. Total no. of updates c←0
3:forModels m= 1, . . . , M (in parallel) do
4: Randomly select R(0)
m clients and send
LocalTrain (m,τm,x(0)
m,ηc
m)requests
5: while tm< Tmdo
6: Wait until server receives an update ∆m
7: Bm←Bm∪ {∆m},c←c+ 1
8: {(R(ti+1)
i , b(ti+1)
i )}M
i=1←Realloc( option , c)
9: if|Bm|=b(tm)
m then
10: x(tm+1)
m ←x(tm)
m−ηs
mηc
mτm1
b(tm)
mP
∆∈Bm∆
11: tm←tm+ 1andBm← ∅
12: end if
13: Select K(tm)
m random client(s) and send
LocalTrain (m, τm,x(tm)
m, ηc
m)request(s)
14: end while
15:end for
16:Output: Trained models {x(Tm)
m}M
m=1
Buffered Asynchronous Aggregation at the Server. The
updates ∆msent by the clients are aggregated at the server
in an asynchronous manner as follows. To keep staleness in
check, the server maintains a buffer Bmfor task m, which
stores the received client updates for model m(Algorithm 2,
Line 7). The buffer size b(tm)
m can be adapted over time
(using the Realloc function). Whenever the server re-
ceives an update for task m, it randomly selects K(tm)
m
client(s) to send a new training request along with the cur-
rent global model (Algorithm 2, Line 13). As we explain
below, K(tm)
m = 1 (respectively, K(tm)
m∈ {0,1,2}) for
option =S(option =D). When the buffer for model m
gets full (formally, |Bm|=b(tm)
m) the server aggregates
the updates stored in the buffer to update the global model
(Algorithm 2, Line 10).
Dynamic Adaptation of the Number of Active Requests
and Buffer Size using Realloc (Algorithm 3). With
thestatic option ( option =S), the Realloc subroutine
always runs its Line 6 to maintain the initial values of Rm
andbmthroughout the whole training process. The resource
allocation across tasks does not change over time. On the
other hand, with the dynamic option, the Realloc subrou-
tine adjusts the resource allocation during the training. The
server maintains a counter c, tracking the total number of
updates received across all Mtasks (Algorithm 2, Line 7).
Ifoption =D(dynamic ), this counter is used to periodi-
cally trigger the dynamic adaptation of the number of active
training requests Rmand the buffer size bmacross tasks
(Algorithm 2, Line 8). Intuitively, we should allocate more
ServerReturning updateNew training requestServerReturning updateNew training requestFigure 1: In our proposed algorithm FedAST , the server
assigns local training requests (shown in striped and orange
blocks for two simultaneous tasks), which are queued at the
clients and processed in a first-come-first-served manner.
Completed requests are aggregated asynchronously at the
server. In the figure, snapshots of the process at two different
times are seen. Adjusting the number of requests, FedAST
periodically reallocates the resources shared across models.
clients (and consequently, more training requests Rm) to
tasks with larger inter-client data heterogeneity. To empiri-
cally estimate this heterogeneity, the server stores the last
V(Vis a tunable parameter) updates ∆mfor each task m
(denoted {∆m,i}V
i=1) and computes
ˆσ2
g,m∝1
V×XV
i=1∆m,i−∆m2
∆m2, (2)
where ∆mis the empirical mean of the ∆m,i’s.3Further,
in our experiments, we empirically observe that the optimal
choice of buffer size bmis proportional to the number of
active requests Rm. See Appendix C.5 for our extensive
experiments. Using (2)and these empirical observations,
the optimal resource allocation emerges as the solution to
the following constraints.
XM
i=1R(ti+1)
i =XM
i=1R(ti)
i,
R(t1+1)
1
ˆσg,1=R(t2+1)
2
ˆσg,2=···=R(tM+1)
M
ˆσg,M,(3)
where the first set of constraints maintains the total compu-
tation budget across tasks, and the second set ensures the
allocation of a larger number of training requests to clients
with higher heterogeneity. We elaborate on the theoretical
motivation for the second set of constraints in Section 4,
once we establish our convergence results. We also refer the
reader to Appendix A for more details on Realloc .
Sending out New Requests to Reach the New Resource
Allocation. To transition from one allocation {R(ti)
m}mto
3We normalize by∆m2to account for different model sizes
since larger models often have larger unnormalized variance.

--- PAGE 5 ---
Algorithm 3 Realloc( option ,c)
1:ifoption =Dandcmod cperiod = 0 then
2:{ˆσ2
g,m}M
m=1←EstimateVariances()
3: Find{R(tm+1)
m }M
m=1that solves (3)
4: b(tm+1)
m ← 
b(tm)
mR(tm+1)
m
/R(tm)
m for all m∈[M]
5:else:for all m∈ {i:R(ti+1)
i not defined }do
6: (R(tm+1)
m , b(tm+1)
m )←(R(tm)
m, b(tm)
m)
7:end if
8:Return {(R(tm+1)
m , b(tm+1)
m )}M
m=1
another {R(tm+1)
m }min an asynchronous setting, we must
adjust the number of new requests that are sent out every
time the server receives a client update. The number of new
requests K(tm)
m sent out on receiving any update ∆mis
always 1in the static ( option =S) case since Rmremains
constant throughout training. In the dynamic case ( option =
D),K(tm)
m can be 0(when R(tm+1)
m < R(tm)
m),1(when
R(tm+1)
m =R(tm)
m), or 2(when R(tm+1)
m > R(tm)
m). We
employ this gradual transition to the desired new number of
active training requests {R(tm+1)
m }mfor each task instead
of a sudden change in allocation to avoid possible longer
queues at the clients during the transition phase.
4 CONVERGENCE ANALYSIS
In this section, we provide the convergence result for
FedAST with the static option (S). Since R(tm)
m andb(tm)
m
are constant when option =S, we drop time indices for
simplicity. The convergence with dynamic allocation ( op-
tion=D) can be shown with an additional assumption. We
relegate this to Appendix F due to space limitations.
Next, we discuss the assumptions used in our analysis.
Assumption 1 (Smoothness) .The loss functions are L-
smooth, i.e., for all i∈[N], for all m∈[M], and for all
x,y∈Rdm,∥∇fm,i(x)− ∇fm,i(y)∥ ≤L∥x−y∥.
Assumption 2 (Bounded Variance) .The stochastic gra-
dient at each client is an unbiased, bounded-variance es-
timator of the true local gradient, i.e., for all x∈Rdm,
i∈[N], and m∈[M],E[e∇fm,i(x)] =∇fm,i(x)and
E∥e∇fm,i(x)− ∇fm,i(x)∥2≤σ2
l,m.
Assumption 3 (Bounded Heterogeneity) .The local gra-
dients are within bounded distance of the global gra-
dient, such that for all m∈[M]andx∈Rdm,
max
i∈[N]∥∇fm,i(x)− ∇fm(x)∥2≤σ2
g,m.
Assumption 4 (Bounded Staleness) .The client updates
of task mare received within at most γmax
mserver model
updates after the server sends the training request.
These assumptions are standard in the literature. Assump-
tions 1-3 are commonly used in the synchronous [Wanget al., 2020b, Jhunjhunwala et al., 2022] and asynchronous
[Koloskova et al., 2022, Nguyen et al., 2022] FL analyses.
Assumption 4 is used in the convergence proof to guarantee
that none of the requested client updates takes an arbitrarily
large time to return to the server and is also common in
asynchronous FL works [Koloskova et al., 2022, Nguyen
et al., 2022]. Furthermore, the maximum staleness can be en-
forced by dropping over-delayed updates in practice during
the training.
Theorem 1 (Convergence of FedAST ).Suppose that
Assumptions 1 - 4 hold, and there are Rmactive lo-
cal training requests corresponding to task m∈[M],
and the server and client learning rates, {ηs
m, ηc
m}
respectively, satisfy ηs
m≤√τmbmand ηc
m≤
min{(6Lτm√τmbm)−1,(4Lτmp
τmRmγmaxm)−1}for all
tasks m∈[M]. Here, bmis the buffer size, and τm
is the number of local training steps. Then, the iterates,
{{x(t)
m}Tm
t=1}M
m=1, of Algorithm 2 satisfy:
1
TmPTm−1
t=0E∥∇fm(x(t))∥2≤ Oδm
Tmηcmηsmτm
| {z }
FedAvg Error - I
+OLηc
mηs
m
bm+L2[ηc
m]2τm
(σ2
l,m+τmσ2
g,m)
| {z }
FedAvg Error - II
+O
L2[ηs
m]2[ηc
m]2τmRm
b2m(σ2
l,m+τmRmσ2
g,m)
| {z }
Asynchronous Aggregation Error,(4)
where δm=fm(x(0)
m)−minxfm(x).
Proof. See Section E in the Appendix.
Comparison with Synchronous FL Analyses. The
FedAvg Error - I and- IIterms in (4)capture the error
bound for synchronous FedAvg [Jhunjhunwala et al., 2022,
Theorem 1]. Since the server updates for model minvolve
aggregating bmclient updates, the buffer size bmis analo-
gous to the number of participating clients in FedAvg. The
third error term in (4)arises due to asynchronous aggre-
gation and increases with Rm, the number of active local
training requests. Intuitively, given the same buffer size bm,
increasing Rmleads to higher worst-case staleness γmax
m.
However, as long as Lηs
mηc
mR2
mτm≤bm, asynchrony is
not the dominant source of error in (4), and we achieve
the same rate of convergence as synchronous FedAvg (see
Corollary 1.1).
Comparison with Asynchronous FL Analyses. FedBuff
[Nguyen et al., 2022] considers buffered asynchronous ag-
gregation for a single model. Still, comparing [Nguyen
et al., 2022, Corollary 1] and the bound in (4)forM= 1,
their convergence result (i) depends on stronger assump-
tions (bounded gradient norm and uniform arrivals of client

--- PAGE 6 ---
updates), and (ii) has worse asynchronous aggregation er-
ror. Moreover, our analysis is more general compared to
[Koloskova et al., 2022] as they do not consider multiple
local SGD steps and the buffer. Simultaneous asynchronous
training is considered by [Chang et al., 2023], but we ob-
serve that they do not achieve convergence unless the data
distribution across clients is identical (see [Chang et al.,
2023, Eq. (19)]). We discuss the comparison of FedAST
to single-model and simultaneous asynchronous federated
training baselines in more detail in Appendix Section B.
Corollary 1.1 (Asymptotic convergence after setting learn-
ing rates) .LetTm≥τmmax{36bm,16Rmγmax}. Setting
the learning rates ηc
m= (τmL√Tm)−1, ηs
m=√τmbm,
the bound in Theorem 1 reduces to:
1
TmPTm−1
t=0E∥∇fm(x(t))∥2≤ OδmL√bmτmTm
+O1√Tmbmτm+1
τmTm
(σ2
l,m+τmσ2
g,m)
+ORm
Tmbm(σ2
l,m+τmRmσ2
g,m)
. (5)
Although the given bound in Corollary 1.1 does not seem
to depend on the staleness bound γmax(Assumption 4), its
effect is implicit in the number of active requests Rmand
buffer size bm. The maximum staleness is positively cor-
related with Rmand negatively correlated with bm. In our
experiments (Appendix C.5), we tune the buffer size to
maintain the update staleness at a reasonable level.
Looking at the bounds in (4)or(5), increasing Rmmakes
the bound worse because to reach the same accuracy in
(5), we need to run a higher number of server updates Tm.
However, increasing Rmalso shortens the duration between
two successive server updates, making the algorithm faster
in wall-clock time. We illustrate this effect with a wall-clock
comparison to FST baselines below.
Impact of Rmon Wall-clock Time. Suppose the ar-
rival times of all the client updates (assuming there is
no queue on the clients) are distributed as Exp(λ). The
expected time to fill the buffer corresponding to task m
isbm/(Rmλ). Therefore, in FedAST , the expected time
to complete one round at the server is inversely propor-
tional to Rm. On the other hand, the expected time to finish
one round of synchronous simultaneous FedAvg training
is1
λPR1+···+RM
k=11
k≈1
λlog(PM
k=1Rk), which increases
withRm. Also, the summation over simultaneously trained
tasks shows an exacerbated straggler effect since all the
clients wait for the slowest client across all the tasks.
Design of Realloc (Algorithm 3). Next, we theoreti-
cally justify the dynamic allocation of resources across tasks
described in Section 3 (Algorithm 3, with option =D),
which adjusts the number of active requests ( {Rm}M
m=1).
Given the limited number of available clients (which lim-
its the total number of active training requests), to achievethe best possible allocation, we minimize the sum of the
most dominant terms in the bounds (FedAvg Error-II in (4))
across tasks. We also use the empirical observation that the
optimal choice of buffer-size bmscales linearly with Rm
(Appendix C.5). The resulting optimization problem is
min
{Rm,bm}M
m=1MX
m=1ηs
mηc
mτm
Rmσ2
g,ms.t.MX
m=1Rm=R, (6)
where Ris the budget for the total number of training
requests across all tasks in the system depending on the
number of available clients. The Realloc function (Al-
gorithm 3) solves the optimization problem (6). See Ap-
pendix A for more details.
5 EXPERIMENTAL RESULTS
We outline our experimental setup in Section 5.1, discuss the
existing baselines in Section 5.2, and compare the baselines
withFedAST under varied settings in Section 5.3.
5.1 DATASETS AND IMPLEMENTATION
We consider image classification tasks with the MNIST
[Deng, 2012], Fashion-MNIST [Xiao et al., 2017] and
CIFAR-10 [Krizhevsky et al., 2009] datasets, and next char-
acter prediction with the Shakespeare [Caldas et al., 2019]
dataset using the same models as in previous works [Acar
et al., 2021, Yu et al., 2023b, Lecun et al., 1998]. We com-
pare the wall-clock time required by different algorithms to
reach some predetermined target test accuracy levels (see
Table 1). In Appendix D.5, we present experiments with
other target accuracy levels to show the consistency of our
results. We also validate our results with ResNet-18, a larger
model, trained for the CIFAR-100 classification task in Ap-
pendix D.4. In all experiments, we conduct three Monte
Carlo runs with different random seeds and report the aver-
age results.
Table 1: The datasets and models used in experiments, along
with corresponding target test accuracy levels.
Dataset Model Target Accuracy
MNIST MLP 93%
Fashion-MNIST LeNet-5 82%
CIFAR-10 CNN 63%
Shakespeare LSTM 42%
For image classification tasks, we partition the training data
across clients using the Dirichlet distribution with α= 0.1
to create inter-client data heterogeneity [Yurochkin et al.,
2019]. The Shakespeare dataset is naturally heterogeneous
as the lines of each role in the plays of Shakespeare are as-
signed to a different client. There are a total of 1000 clients,

--- PAGE 7 ---
0 2000 4000 6000 8000 10000 12000 14000 16000
Time0.400.450.500.550.600.650.70Accuracy
Average Accuracy of Six Simultaneous CIFAR-10 Tasks
FedAST
Sync-ST
Sync-Bayes-STSync-UCB-ST
Sync-ST-
NoStrag.Mit.
Figure 2: Mean test accuracy for compared algorithms on six
identical CIFAR-10 tasks trained simultaneously. FedAST
trains faster than synchronous methods. The synchronous
method without straggler mitigation is by far the slowest.
30% of which are available to accept new training requests,
independent of the past.
Modeling Client Delays. As suggested in [Lee et al.,
2018, Dutta et al., 2021, Shi et al., 2021, Zhou et al., 2022],
we use shifted-exponential (exponential plus constant) ran-
dom variables to model the time taken by a client to com-
plete a local training request and return the update to the
server. We pick the run-time generation parameters of each
task according to real measurements on NVIDIA GeForce
GTX TITAN X GPUs. To simulate hardware heterogeneity
across clients, we divide them into 25% slow, 50% normal-
speed, and 25% fast clients [Leconte et al., 2023]. We rele-
gate additional implementation details to the Appendix.
5.2 BASELINE ALGORITHMS
We explain the synchronous and asynchronous baseline
methods to which we compare FedAST :
Synchronous Simultaneous Training. The following
synchronous methods differ only in client selection.
1.Sync-ST [Bhuyan et al., 2023]: randomly partition the
client set across tasks at each round;
2.Sync-Bayes-ST [Zhou et al., 2022]: Bayesian
optimization-based assignment of clients to tasks;
3.Sync-UCB-ST [Bhuyan and Moharir, 2022]: client se-
lection as a multi-armed bandit problem.
In Figure 2, we first simultaneously train six CIFAR-10
models and compare the performance of all synchronous
baselines and FedAST . As synchronous methods perform
poorly due to a severe straggler issue, we augment them
with a straggler mitigation method by aggregating only
the first kclient updates for each task and discarding
the rest [Bonawitz et al., 2019] as the default option. We
choose k= 30 by validation experiments across datasets
in Appendix D.1. This extra augmentation makes the base-
lines more competitive . In Figure 2, we also add the result
ofSync-ST -NoStrag.Mit. , which is the Sync-ST
20% 30% 40% 50% 60%
The ratio of active clients0.600.650.70
CIFAR-10 Final Test Accuracy
20% 30% 40% 50% 60%
The ratio of active clients0.840.860.88
Fashion-MNIST Final Test AccuracyFigure 3: The mean final test accuracy values of FedAST
(blue), FedAST-NoBuffer (olive green) and centralized
training (violet) with varying active client ratio, when train-
ing3identical models. The left (right) figure is for CIFAR-
10 (Fashion-MNIST) dataset. With more active clients, the
importance of buffer increases due to increasing staleness.
0 1000 2000 3000 4000
Time0.40.60.8AccuracyFashion-MNIST
FedAST
FedAST-NoBuffer
0 1000 2000 3000 4000
Time0.20.40.6CIFAR-10
FedAST
FedAST-NoBuffer
Figure 4: The mean test accuracy values of FedAST and
FedAST-NoBuffer , when simultaneously training one
model for CIFAR-10 and one for Fashion-MNIST. FedAST
achieves higher and more stable accuracy levels.
without our augmented straggler mitigation. It shows that
the synchronous baselines have a large straggler effect with-
out our extra augmentation.
Asynchronous Federated Simultaneous Training. To
our knowledge, [Chang et al., 2023] is the only other
work that mainly studies asynchronous simultaneous FL.
However, their client selection scheme requires the knowl-
edge of network-wide staleness and smoothness constants,
which are hard to estimate. If the tasks have similar model
complexity and task difficulty, their client selection is
similar to that of FedAST with a buffer size of 1. We
thus include this no-buffer version of FedAST (we call
itFedAST-NoBuffer ) as a baseline.
5.3 RESULTS AND INSIGHTS
We assess the performance of FedAST under various sce-
narios. In homogeneous-task experiments, where multiple
independent copies of the same model are trained simultane-
ously using the same dataset, we report the average accuracy
over time. In heterogeneous-task experiments involving dif-
fering tasks and models, efficiently distributing resources to
accelerate the completion of all tasks is the main challenge.
For homogeneous tasks, we use FedAST with the static
option (S) and uniform client distribution across tasks. In
heterogeneous-task experiments, we use dynamic allocation
(option =D) to enhance resource allocation efficiency. To

--- PAGE 8 ---
0 500 1000 1500
Time to Reach Target2 tasks 4 tasks 6 tasks
675.6964.01012.0
871.71273.71744.3
22.5%24.3%42.0%CIFAR-10
0 250 500 750 1000
Time to Reach Target2 tasks 4 tasks 6 tasks
383.1640.3673.7
534.1813.51149.8
28.3%21.3%41.4%Fashion-MNIST
0 200 400
Time to Reach Target2 tasks 4 tasks 6 tasks
239.6283.7299.0
242.9376.8554.0
1.3%24.7%46.0%Shakespeare
0 50 100 150
Time to Reach Target2 tasks 4 tasks 6 tasks
68.2104.2125.5
71.9113.0159.5
5.2%7.8%21.4%MNIST
FedAST Sync-STFigure 5: Mean training times of FedAST andSync-ST to attain target accuracy levels in (Table 1) on 2/4/6tasks with
CIFAR-10, Fashion-MNIST, MNIST, and Shakespeare datasets. FedAST requires consistently lower wall-clock time for
training compared to Sync-ST ; the percentages represent these time gains.
show the benefits of dynamic allocation over static allo-
cation, we also explore heterogeneous-task scenarios with
option =S. Dynamic allocation reduces overall training
time by up to 11.9%, with comprehensive results shown in
Appendix D.6.
To quantify the time saved by using FedAST over some
competing baseline, we define time gain as
Gain≜TBaseline −TFedAST
TBaseline×100% ,
where TBaseline (TFedAST )is the simulated time for
Baseline (FedAST )to reach the target accuracy.
Comparison with All Synchronous FST Methods. First,
we compare the synchronous baselines discussed in Sec-
tion 5.2 on the CIFAR-10 dataset (Figure 2), where
we simultaneously train six identical models. We ob-
serve that synchronous methods without straggler mit-
igation converge very slowly. Among the straggler-
mitigated synchronous variants that we implement, Fig-
ure 2 shows that Sync-Bayes-ST , has similar per-
formance to Sync-ST because it struggles due to the
large search space of the optimization problem, stemming
from the exponential number of possible client sched-
ules. Further, we do not observe any performance gains
from using Sync-UCB-ST overSync-ST . Given that
Sync-Bayes-ST andSync-UCB-ST have similar per-
formance as Sync-ST , in subsequent experiments, we
choose Sync-ST as the sole synchronous baseline.
Need for Buffer. As discussed earlier, incorporating the
buffer mitigates the negative impact of highly stale up-
dates. Since staleness increases with the number of ac-
tive clients, asynchronous FL methods without a buffer
exhibit limited scalability as the number of clients grows.
To demonstrate this, in Figure 3, we conduct two experi-
ments: 1) training three models for CIFAR-10 simultane-
ously, and 2) training three models for Fashion-MNIST
simultaneously. We plot the final accuracy values varying
the ratio of the active clients. We observe that for smallactive client ratios, FedAST andFedAST-NoBuffer
have comparable performance. However, with more active
clients, the staleness of the updates increases, resulting in
significantly worse performance of the fully asynchronous
FedAST-NoBuffer algorithm. Then, in Figure 4, we si-
multaneously train two models, one each for CIFAR-10 and
Fashion-MNIST. Observing the unsteady learning curves of
FedAST-NoBuffer , we conclude that the buffer makes
the system more robust to stale updates.
Comparison with Sync-ST .Next, we compare
FedAST with the chosen synchronous method, Sync-ST .
0 80 160 240 320 400 480 560
Time0.20.40.60.81.0Test AccuracyFedAST
0 150 300 450 600 750 900 1050
Time0.20.40.60.81.0Test AccuracySync-ST
MNIST Fashion-MNIST CIFAR-10 Shakespeare
Figure 6: Training curves of a single Monte Carlo run of
theheterogeneous experiment . Dashed vertical lines show
times when tasks reach their target accuracy, with FedAST
reaching it faster than Sync-ST .
1)Homogeneous Tasks: We conduct experiments training
2,4, and 6identical models for each of MNIST, Fashion-
MNIST, CIFAR-10, and Shakespeare datasets. Figure 5
shows the average finish times of the algorithms, and the
significant time gains of our algorithm FedAST over syn-
chronous Sync-ST (even after incorporating straggler mit-
igation). We observe that the gain increases with the number
of simultaneously trained tasks because Sync-ST is espe-
cially vulnerable to the straggler problem.
2)Heterogeneous Tasks: The heterogeneous experiment
trains 4models simultaneously, one each for the MNIST,
Fashion-MNIST, CIFAR-10, and Shakespeare datasets.
Once one model reaches its target accuracy, its training

--- PAGE 9 ---
0 200 400 600 800 1000
Average Finish TimeSync-ST FedAST
326.6351.6742.71033.098.5354.6402.6619.0
40.1%Finish Times in Simultaneous Training
MNIST
Fashion-MNISTCIFAR-10
ShakespeareFigure 7: Mean time required to reach target accuracy and
time gain of FedAST overSync-ST inthe heterogeneous
experiment . While FedAST does not require manual fine-
tuning, the client allocation in Sync-ST is tuned at 100,84,
48, and 68clients for MNIST, Fashion-MNIST, CIFAR-10,
and Shakespeare tasks respectively. FedAST has notable
time gain ( 40.1%) over Sync-ST to finish all tasks.
stops, and its clients are reallocated to other tasks. We use
FedAST with option =Dfor dynamic client allocation.
For the synchronous baseline Sync-ST , we ran 30dif-
ferent client allocation schemes, including our proposed
allocation scheme and uniform allocation across tasks. We
report the results achieved with the best-performing scheme.
Figure 6 shows learning curves for FedAST andSync-ST
from a single Monte Carlo run. The dashed vertical lines
denote the time instants when a model reaches its target
accuracy, following which the clients training this model get
reallocated to other tasks. Figure 7 shows the average finish
times for 4simultaneously trained models with FedAST
andSync-ST . For example, with FedAST , the model for
MNIST dataset hits its target accuracy at 99, after which the
clients training this model get reallocated to the other mod-
els. At 619, the training of the final model (for CIFAR-10)
is complete. Comparing the finish times for the last model,
FedAST provides a 40.1%time gain over Sync-ST .
We observe that thanks to dynamic client allocation,
FedAST automatically detects which tasks have higher het-
erogeneity across clients and need a larger buffer. We notice
that the Shakespeare task is allocated fewer clients because it
is estimated to be less heterogeneous, which is true based on
the label distribution of data samples across clients. We also
repeat this experiment (Appendix D.6) using FedAST with
static option (S) to validate the proposed dynamic client
allocation strategy. Dynamic client allocation consistently
has time gain up to 11.9%compared to the static version.
6 CONCLUSION
In this paper, we present FedAST , a federated learning
framework to simultaneously train multiple models using
buffered asynchronous aggregations. We theoretically provethe convergence of our algorithm for smooth non-convex
objective functions. Experiments across multiple datasets,
demonstrates the FedAST ’s superiority over existing simul-
taneous FL baselines, achieving up to 46.0%reduction in
training time. In future work, we plan to enhance FedAST
by incorporating client selection based on local data distri-
butions and computational powers of clients.
ACKNOWLEDGEMENTS
This work was partially supported by the US National
Science Foundation under grants CNS-1751075 and CNS-
2106891 to CJW and NSF CCF 2045694, CNS-2112471,
CPS-2111751, and ONR N00014-23-1-2149 to GJ and the
Ben Cook Presidential Graduate Fellowship to BA.
References
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew
Mattina, Paul Whatmough, and Venkatesh Saligrama.
Federated learning based on dynamic regularization. In
International Conference on Learning Representations ,
2021. URL https://openreview.net/forum?
id=B7v4QMR6Z9w .
Fan Ang, Li Chen, Nan Zhao, Yunfei Chen, Weidong Wang,
and F Richard Yu. Robust federated learning with noisy
communication. IEEE Transactions on Communications ,
68(6):3452–3464, 2020.
Neelkamal Bhuyan and Sharayu Moharir. Multi-model
federated learning. In 2022 14th International Conference
on COMmunication Systems & NETworkS (COMSNETS) ,
pages 779–783. IEEE, 2022.
Neelkamal Bhuyan, Sharayu Moharir, and Gauri Joshi.
Multi-model federated learning with provable guarantees.
In Esa Hyytiä and Veeraruna Kavitha, editors, Perfor-
mance Evaluation Methodologies and Tools , pages 207–
222, Cham, 2023. Springer Nature Switzerland. ISBN
978-3-031-31234-2.
K. A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp,
Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloé M
Kiddon, Jakub Kone ˇcný, Stefano Mazzocchi, Brendan
McMahan, Timon Van Overveldt, David Petrou, Daniel
Ramage, and Jason Roselander. Towards federated learn-
ing at scale: System design. In SysML 2019 , 2019. URL
https://arxiv.org/abs/1902.01046 . To ap-
pear.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu,
Tian Li, Jakub Kone ˇcný, H. Brendan McMahan, Virginia
Smith, and Ameet Talwalkar. Leaf: A benchmark for
federated settings, 2019.

--- PAGE 10 ---
Zhan-Lun Chang, Seyyedali Hosseinalipour, Mung Chiang,
and Christopher G. Brinton. Asynchronous multi-model
dynamic federated learning over wireless networks: The-
ory, modeling, and optimization, 2023.
Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl
Hashemi, and Stanislaw H ˙Zak. On the convergence
of decentralized federated learning under imperfect infor-
mation sharing. IEEE Control Systems Letters , 2023.
Yujing Chen, Yue Ning, Martin Slawski, and Huzefa Rang-
wala. Asynchronous online federated learning for edge
devices with non-iid data. In 2020 IEEE International
Conference on Big Data (Big Data) , pages 15–24. IEEE,
2020.
Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Towards un-
derstanding biased client selection in federated learning.
In Gustau Camps-Valls, Francisco J. R. Ruiz, and Is-
abel Valera, editors, Proceedings of The 25th Interna-
tional Conference on Artificial Intelligence and Statis-
tics, volume 151 of Proceedings of Machine Learn-
ing Research , pages 10351–10375. PMLR, 28–30 Mar
2022. URL https://proceedings.mlr.press/
v151/jee-cho22a.html .
Li Deng. The mnist database of handwritten digit images
for machine learning research [best of the web]. IEEE
Signal Processing Magazine , 29(6):141–142, 2012. doi:
10.1109/MSP.2012.2211477.
Sanghamitra Dutta, Jianyu Wang, and Gauri Joshi. Slow
and stale gradients can win the race. IEEE Journal on
Selected Areas in Information Theory , 2(3):1012–1024,
2021. doi: 10.1109/JSAIT.2021.3103770.
Divyansh Jhunjhunwala, Pranay Sharma, Aushim Na-
garkatti, and Gauri Joshi. Fedvarp: Tackling the variance
due to partial client participation in federated learning.
InUncertainty in Artificial Intelligence , pages 906–916.
PMLR, 2022.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Au-
rélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel
Cummings, et al. Advances and open problems in fed-
erated learning. Foundations and Trends ®in Machine
Learning , 14(1–2):1–210, 2021.
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi.
Sharper convergence guarantees for asynchronous SGD
for distributed and federated learning. In Alice H.
Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho, editors, Advances in Neural Information Processing
Systems , 2022. URL https://openreview.net/
forum?id=4_oCZgBIVI .
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009.Duy-Dong Le, Anh-Khoa Tran, Minh-Son Dao, Kieu-
Chinh Nguyen-Ly, Hoang-Son Le, Xuan-Dao Nguyen-
Thi, Thanh-Qui Pham, Van-Luong Nguyen, and Bach-
Yen Nguyen-Thi. Insights into multi-model federated
learning: An advanced approach for air quality index
forecasting. Algorithms , 15(11):434, 2022.
Louis Leconte, Van Minh Nguyen, and Eric Moulines.
Favano: Federated averaging with asynchronous nodes,
2023.
Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Pro-
ceedings of the IEEE , 86(11):2278–2324, 1998. doi:
10.1109/5.726791.
Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dim-
itris Papailiopoulos, and Kannan Ramchandran. Speeding
up distributed machine learning using codes. IEEE Trans-
actions on Information Theory , 64(3):1514–1529, 2018.
doi: 10.1109/TIT.2017.2736066.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia
Smith. Ditto: Fair and robust federated learning through
personalization. In International Conference on Machine
Learning , pages 6357–6368. PMLR, 2021.
Jianchun Liu, Hongli Xu, Lun Wang, Yang Xu, Chen Qian,
Jinyang Huang, and He Huang. Adaptive asynchronous
federated learning in resource-constrained edge comput-
ing. IEEE Transactions on Mobile Computing , 22(2):
674–690, 2023. doi: 10.1109/TMC.2021.3096846.
Bing Luo, Wenli Xiao, Shiqiang Wang, Jianwei Huang,
and Leandros Tassiulas. Tackling system and statistical
heterogeneity for federated learning with adaptive client
sampling. In IEEE INFOCOM 2022-IEEE conference
on computer communications , pages 1739–1748. IEEE,
2022.
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Ben-
jamin Recht, Kannan Ramchandran, and Michael I. Jor-
dan. Perturbed iterate analysis for asynchronous stochas-
tic optimization. SIAM Journal on Optimization , 27(4):
2202–2229, 2017. doi: 10.1137/16M1057000. URL
https://doi.org/10.1137/16M1057000 .
Yishay Mansour, Mehryar Mohri, Jae Ro, and
Ananda Theertha Suresh. Three approaches for
personalization with applications to federated learning.
arXiv preprint arXiv:2002.10619 , 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized
data. In Artificial intelligence and statistics , pages 1273–
1282. PMLR, 2017.

--- PAGE 11 ---
John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan
Yousefpour, Mike Rabbat, Mani Malek, and Dzmitry
Huba. Federated learning with buffered asynchronous
aggregation. In Gustau Camps-Valls, Francisco J. R.
Ruiz, and Isabel Valera, editors, Proceedings of The
25th International Conference on Artificial Intelligence
and Statistics , volume 151 of Proceedings of Machine
Learning Research , pages 3581–3607. PMLR, 28–30 Mar
2022. URL https://proceedings.mlr.press/
v151/nguyen22b.html .
Takayuki Nishio and Ryo Yonetani. Client selection for fed-
erated learning with heterogeneous resources in mobile
edge. In ICC 2019-2019 IEEE international conference
on communications (ICC) , pages 1–7. IEEE, 2019.
Wenqi Shi, Sheng Zhou, Zhisheng Niu, Miao Jiang, and
Lu Geng. Joint device scheduling and resource allocation
for latency constrained wireless federated learning. IEEE
Transactions on Wireless Communications , 20(1):453–
467, 2021. doi: 10.1109/TWC.2020.3025446.
Marie Siew, Shoba Arunasalam, Yichen Ruan, Ziwei Zhu,
Lili Su, Stratis Ioannidis, Edmund Yeh, and Carlee Joe-
Wong. Fair training of multiple federated learning models
on resource constrained network devices. In Proceedings
of the 22nd International Conference on Information Pro-
cessing in Sensor Networks , pages 330–331, 2023.
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.
Towards personalized federated learning. IEEE Transac-
tions on Neural Networks and Learning Systems , 2022.
Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Opti-
mizing federated learning on non-iid data with reinforce-
ment learning. In IEEE INFOCOM 2020 - IEEE Confer-
ence on Computer Communications , pages 1698–1707,
2020a. doi: 10.1109/INFOCOM41043.2020.9155494.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and
H. Vincent Poor. Tackling the objective inconsistency
problem in heterogeneous federated optimization, 2020b.
Zhongyu Wang, Zhaoyang Zhang, Yuqing Tian, Qianqian
Yang, Hangguan Shan, Wei Wang, and Tony QS Quek.
Asynchronous federated learning over wireless communi-
cation networks. IEEE Transactions on Wireless Commu-
nications , 21(9):6961–6978, 2022.
Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 ,
2017.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asyn-
chronous federated optimization. CoRR , abs/1903.03934,
2019. URL http://arxiv.org/abs/1903.
03934 .Chenhao Xu, Youyang Qu, Yong Xiang, and Longxiang
Gao. Asynchronous federated learning on heterogeneous
devices: A survey. Computer Science Review , 50:100595,
2023.
Yongtao Yao, Nejib Ammar, and Weisong Shi. Flow: A
scalable multi-model federated learning framework on
the wheels. In 2023 IEEE International Conference on
Mobility, Operations, Services and Technologies (MOST) ,
pages 11–22. IEEE, 2023.
Jieling Yu, Ruiting Zhou, Chen Chen, Bo Li, and Fang
Dong. Asfl: Adaptive semi-asynchronous federated learn-
ing for balancing model accuracy and total latency in
mobile edge networks. In Proceedings of the 52nd Inter-
national Conference on Parallel Processing , ICPP ’23,
page 443–451, New York, NY , USA, 2023a. Associa-
tion for Computing Machinery. ISBN 9798400708435.
doi: 10.1145/3605573.3605582. URL https://doi.
org/10.1145/3605573.3605582 .
Xiaofan Yu, Lucy Cherkasova, Harsh Vardhan, Quanling
Zhao, Emily Ekaireb, Xiyuan Zhang, Arya Mazumdar,
and Tajana Rosing. Async-hfl: Efficient and robust asyn-
chronous federated learning in hierarchical iot networks.
InProceedings of the 8th ACM/IEEE Conference on In-
ternet of Things Design and Implementation , IoTDI ’23,
page 236–248, New York, NY , USA, 2023b. Associa-
tion for Computing Machinery. ISBN 9798400700378.
doi: 10.1145/3576842.3582377. URL https://doi.
org/10.1145/3576842.3582377 .
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh,
Kristjan Greenewald, Nghia Hoang, and Yasaman Khaza-
eni. Bayesian nonparametric federated learning of neural
networks. In International conference on machine learn-
ing, pages 7252–7261. PMLR, 2019.
Hossein Zakerinia, Shayan Talaei, Giorgi Nadiradze, and
Dan Alistarh. Communication-efficient federated learn-
ing with data and client heterogeneity, 2023.
Chendi Zhou, Ji Liu, Juncheng Jia, Jingbo Zhou, Yang Zhou,
Huaiyu Dai, and Dejing Dou. Efficient device scheduling
with multi-job federated learning. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36,
pages 9971–9979, 2022.

--- PAGE 12 ---
FedAST: Federated Asynchronous Simultaneous Training
(Appendix)
Baris Askin1Pranay Sharma1Carlee Joe-Wong1Gauri Joshi1
1Carnegie Mellon University, Pittsburgh, Pennsylvania, USA
A ADJUSTING THE NUMBER OF ACTIVE REQUESTS AND REALLOC
Before designing Realloc algorithm for dynamic client allocation option (D), we conduct initial validation experiments
with the static option (S), wherein the allocation of active local training requests across clients and buffer sizes remain
unchanged throughout the training. Note that with the static option , Algorithm 3 only executes its Line 6 and returns the
previous round’s values always. We empirically observe that setting the ratio of the number of active local training requests
to buffer size fixed and below 37works well. Refer to Section C.5 for the validation experiments. Then, incorporating
this ratio ( Rm≈37bm) within the convergence bound in Equation 4 of Theorem 1, we find out that the dominant term
(excluding smoothness constants) becomes: O
ηs
mηc
mτm
Rm
σ2
g,m. Further, given the limited number of available clients, we
cannot increase the total number of active local training requests arbitrarily without increasing the staleness. Thus, we
employPM
m=1Rm=Rwhere Ris a constant of how many active training requests we assign in total depending on the
number of available clients in the setting. Since the goal of federated simultaneous training is to minimize the objective
functions of all tasks concurrently, we propose to adjust {Rm}M
m=1by solving,
min
{Rm}M
m=1MX
m=1ηs
mηc
mτm
Rmσ2
g,msubject toMX
m=1Rm=R. (7)
The solution of the minimization problem in (7) suggests allocating local training requests, {Rm}M
m=1, in proportion to
σg,m√ηsmηcmτmfor each model m. Using this approach, Algorithm 3 adjusts resource allocation. Further, as the update
variance across clients may vary in time during training, we employ adaptive periodical reallocation of resources across
models (Line 1 in Algorithm 3). Therefore, we use round indices to denote the changing number of active training requests
and buffer sizes.
As we do not have access to true data heterogeneity levels, we need to estimate it (Line 2 in Algorithm 3). When FedAST is
run with option =D(dynamic client allocation option), the server keeps the latest Vupdates of each model. This requires
constant and small memory space kept in the server. To present how variance estimation ( EstimateVariances() )
works, assume that {∆m,1,∆m,2, . . . ,∆m,V}M
m=1are the sets of latest received updates of tasks m∈[M]where each
∆m,kfork∈[V]is the output of kthlatest local training (Algorithm 1). As the output of any local training is the average of all
calculated stochastic gradients during that local training, we use those outputs as approximations of the gradients calculated on
the local data of clients. Algorithm 4 describes EstimateVariances() . It first calculates the mean of the latest updates
for each task, {∆m}M
m=1={1
V×PV
i=1∆m,i}M
m=1. Then, EstimateVariances() returns sample variance multiplied
with other terms ( ηs
mηc
mτm) suggested by (7) and normalized by the mean update norm (to prevent large models or models
with inherently large weights from dominating others), {ˆσ2
g,m}M
m=1={ηs
mηc
mτm
V×PV
i=1∆m,i−∆m2/∆m2}M
m=1.
Then, Realloc algorithm allocates the number of active training requests proportionally to the square root of these values
(Algorithm 3, Line 3).
This approach is sensible both theoretically and intuitively. Based on our experimental observations regarding the relationship
between the number of active training requests and buffer size, increasing the number of active local training requests
necessitates an increase in buffer size. Moreover, a larger buffer proves beneficial in reducing the variance across updates,

--- PAGE 13 ---
Algorithm 4 EstimateVariances()
Require: The set of latest Vupdates {∆m,1,∆m,2, . . . ,∆m,V}M
m=1, server-side learning rates {ηs
m}M
m=1, client-side
learning rates {ηc
m}M
m=1, and the number of local SGD steps of all models {τm}M
m=1.
1:{∆m}M
m=1← {1
V×PV
i=1∆m,i}M
m=1 ▷Calculate the means of the latest updates
2:{eσ2
g,m}M
m=1← {1
V×PV
i=1∆m,i−∆m2/∆m2}M
m=1 ▷Calculate the normalized sample variances
3:{ˆσ2
g,m}M
m=1← {ηc
mηs
mτmeσ2
g,m}M
m=1 ▷Multiply with other constants suggested by the convergence guarantee (7)
4:Return {ˆσ2
g,m}
the buffered updates are averaged during the aggregation. Realloc aims to allocate more clients and provide a larger buffer
size for tasks with higher heterogeneity. We choose the number of stored latest updates V= 8and the period of number
of total updates from all clients to trigger reallocation in Realloc subroutine cperiod = 0.75×M×PM
m=1Rmin our
experiments. The benefits of dynamic allocation (option =D)over static and uniform resource allocation (option =S)are
demonstrated when tasks/models are heterogeneous, as shown in Figures 21-26.
B THEORETICAL COMPARISON OF FedAST WITH BASELINES
We compare FedAST with single-model FL methods, too. [Nguyen et al., 2022] is the most similar algorithm to FedAST
(with single-model). However, even for the single-task case, FedAST differs by employing a uniform client assignment
to ensure unbiased participation of clients irrespective of their hardware speeds. This allows us to relax the assumptions
to prove the convergence guarantee. [Nguyen et al., 2022] relies on a strong assumption that the server receives updates
from clients uniformly at random and that the norm of gradients is bounded. Moreover, compared to [Koloskova et al.,
2022], our analysis is more general as FedAST uses multiple SGD steps in local training and a buffer. Some other recent
single-model asynchronous FL works, [Zakerinia et al., 2023] and [Leconte et al., 2023], do not have straightforward and
efficient simultaneous federated training extensions for multiple models.
[Chang et al., 2023] is another asynchronous simultaneous federated learning method. However, [Chang et al., 2023] indeed
fails to converge to a stationary point asymptotically unless data is homogeneous, and their assumptions include Bounded
Gradient Norm and Weak Convexity.
Table 2: Comparison of FedAST ’s convergence guarantees to Nguyen et al. [2022] (single-task asynchronous buffered FL
algorithm) and Chang et al. [2023] (an asynchronous FST algorithm). T: #global rounds, τ: #local steps, b: buffer size.
Algorithm Non-standard assumptions Convergence
Nguyen et al. [2022] Bounded Gradient & Receiving Updates Uniformly Op
τ/(Tb)
(a)
Chang et al. [2023] Bounded Gradient & Weak Convexity Not converge
FedAST — Op
τ/(Tb)
(a) Although the convergence guarantee in the published [Nguyen et al., 2022] paper seems to have a better rate, we pointed out a mistake
in their proof. Here, we use the corrected version we received via private communication.
C EXPERIMENTAL SETUP DETAILS
In our study, we explore a simultaneous federated learning (FL) setting for multiple models. We present the details of our
experiments in this section.
C.1 SIMULATION ENVIRONMENT
We simulate the training with PyTorch on NVIDIA GeForce GTX TITAN X graphics processing units (GPUs) of our
internal cluster. We build our code upon the public codes of [Wang et al., 2020a, Yu et al., 2023b].

--- PAGE 14 ---
C.2 SETTING OVERVIEW
We consider the federated training of Mmodels simultaneously using Nclients. Nis1000 in all experiments and M,
specified for each experiment explicitly, varies between 2−6.
C.3 TASKS AND MODELS
We use 5different tasks across the experiments: MNIST [Deng, 2012], Fashion-MNIST [Xiao et al., 2017], CIFAR-10
and CIFAR-100 [Krizhevsky et al., 2009] image classification tasks, and Shakespeare [Caldas et al., 2019] next character
prediction task. We use a multilayer perceptron for MNIST as in [Acar et al., 2021], convolutional networks for Fashion-
MNIST as in [Lecun et al., 1998] and for CIFAR-10 as in [Acar et al., 2021], ResNet-18 model for CIFAR-100 as in Acar
et al. [2021], and a long short-term memory network for Shakespeare as in [Yu et al., 2023b].
C.4 DATASETS AND DATA DISTRIBUTION
We consider the data heterogeneity across clients in FL frameworks. We download MNIST, Fashion-MNIST, and
CIFAR-10/100 datasets from PyTorch built-in library methods. The train and test splits provided by the library are used
without any modifications. To simulate heterogeneous data distribution across clients, we use Dirichlet distribution with
α= 0.1following the approach suggested in Yurochkin et al. [2019]. We ensure that each client has 300data points for
MNIST, Fashion-MNIST, and CIFAR-10/100 tasks by repeating the train set if necessary. We obtain and preprocess the
Shakespeare dataset as described in [Caldas et al., 2019]. This dataset has inherently heterogeneous distribution across
clients as each client corresponds to a unique role from Shakespeare’s plays.
C.5 DESIGN PARAMETERS
In this section, we explain how we choose the design parameters.
Client dataset sizes, batch sizes, and number of local steps. While distributing CIFAR-10/100, MNIST, and Fashion-
MNIST datasets across clients, each client is allocated 300data points from each dataset. The Shakespeare dataset, however,
maintains its original distribution of data points across roles, so clients have different numbers of data samples in the
Shakespeare task. For CIFAR-10/100, MNIST, and Fashion-MNIST tasks, we set the batch size to 32while we employ a
batch size of 64for the Shakespeare task. We fix the number of local steps in local training ( τmparameter in Algorithm 1 in
the main text) of clients at 27for all tasks. This makes 3epochs for CIFAR-10/100, MNIST, and Fashion-MNIST tasks. As
the number of data points varies across clients for the Shakespeare dataset, there is no fixed number of epochs.
Buffer size. The buffer in FedAST is crucial for mitigating the negative impacts of highly stale updates, as extensively
discussed in the main text. The staleness of updates is influenced by the number of active local training requests, denoted
asRm, and the buffer size, bm, associated with all model m∈[M]. When FedAST is run with static option (S), these
numbers are kept constant during the training, but they may change (this time we denote R(tm)
m andb(tm)
m) when we use
dynamic client allocation option (D). A higher number of simultaneous local training requests leads to a higher staleness
because it increases the global model’s update frequency at the server. On the other hand, buffer size is inversely related to
staleness, given its opposing effect on the aggregation frequency. Based on our experimental observations, selecting the
number of active training requests and the buffer size of model msuch that their ratio is fixed and below 37, (Rm/bm≲37
orR(tm)
m/btmm≲37), works well. Selecting the buffer size of FedAST based on this observation avoids the detrimental
effects of stale updates while benefiting from fast training thanks to the asynchronous algorithm. We show two experimental
results in Figures 8 and 9. In Figure 8, we train one Fashion-MNIST and one CIFAR-10 models simultaneously by assigning
175active training requests to each task and observe that buffer size of 5strikes a balance between high final test accuracy
and fast training to achieve the target accuracy for both tasks. In Figure 9, we repeat a similar experiment with MNIST and
CIFAR-10 tasks by assigning 105active training requests to each. This time, we observe that a buffer size of 3performs the
best for both tasks. These experimental results support our buffer size choice.

--- PAGE 15 ---
2 4 6 8 10 12 14
Buffer Size0.850.860.870.880.89Final Test AccuracyFashion-MNIST
Accuracy
Time
Our Buffer Size Choice
2 4 6 8 10 12 14
Buffer Size0.620.640.660.680.70Final Test AccuracyCIFAR-10
Accuracy
Time
Our Buffer Size Choice300350400450500550600
Time to Target
6007008009001000
Time to TargetPerformance Across Buffer SizesFigure 8: The final test accuracy and required time to get target accuracy (in Table 1) for simultaneous training (using
FedAST with static option ) of one Fashion-MNIST and one CIFAR-10 model with different buffer sizes. We assign the
same number of local training requests ( 175) to each task.
2 4 6 8 10
Buffer Size0.97800.97850.97900.97950.98000.98050.98100.98150.9820Final Test AccuracyMNIST
Accuracy
Time
Our Buffer Size Choice
2 4 6 8 10
Buffer Size0.6600.6650.6700.6750.6800.6850.6900.695Final Test AccuracyCIFAR-10
Accuracy
Time
Our Buffer Size Choice
708090100110120130
Time to Target
600650700750800850900
Time to TargetPerformance Across Buffer Sizes
Figure 9: The final test accuracy and required time to get target accuracy (in Table 1) for simultaneous training (using
FedAST with static option ) of one MNIST and one CIFAR-10 model with different buffer sizes. We assign the same number
of local training requests ( 105) to each task.
Learning rate and weight decay. We search for the best learning rate and weight decay hyperparameters considering
the training speed and final accuracy levels. We seek client-side learning rate within the range of [1×10−3,1×10],
server-side learning rate within [3×10−2,3], and weight decays within [1×10−7,1×10−2]. We observe that client-side
learning rates of 6×10−2and7with weight decays of 3×10−4and7×10−5work best respectively for Fashion-MNIST
and Shakespeare tasks for all methods. For CIFAR-10 task, a client-side learning rate of 1×10−1with weight decays of
7×10−4and3×10−4perform best for asynchronous and synchronous methods, respectively. For MNIST, we use client-side
learning rates of 1×10−1and2×10−1for asynchronous and synchronous methods, respectively, with a weight decay of
3×10−4. For server-side learning rates, we observe that 1for synchronous methods ( Sync-ST ,Sync-Bayes-ST , and
Sync-UCB-ST ),0.1forFedAST , and 0.038forFedAST-NoBuffer perform well for all tasks.
C.6 MODELING TRAINING TIMES, MODEL SIZES, AND CLIENT SPEED HETEROGENEITY
In our experiments, following [Lee et al., 2018, Shi et al., 2021, Zhou et al., 2022, Dutta et al., 2021], we employ the
shifted-exponential random variables to model the duration between when the server sends a local training request to a client,
and when it receives the update of the local training. The exponential component of the distribution reflects the stochastic
nature of the device speeds, while the shift component accounts for unavoidable delays such as disk I/O operations.
Whenever a client iperforms local training for task m, we draw a random number from the distribution with a cumulative
distribution function (CDF) of,
P(X≤x) =(
1−exp{−x−βi,m
2βi,m}, x≥βi,m
0, otherwise,

--- PAGE 16 ---
where βi,mdepends on the speed of client iand the size of the model associated with task m. Then, we multiply this random
number by the number of local steps to calculate the simulation time between when the server requests for the local training,
and when it receives the update back.
We quantify the effect of the model sizes based on the average time required to calculate one stochastic gradient for each
model on the GPUs of our internal cluster. By our measurements, we set,
βi,MNIST
0.148=βi,Fashion-MNIST
0.240=βi,CIFAR-10
0.228=βi,Shakespeare
0.555=βi,CIFAR-100
2.071,∀i∈[N].
In our experiments, we also take the heterogeneity in the speed of client devices into consideration. We categorize clients
into three speed groups: slow ( %25), normal-speed ( %50), and fast ( %25). The speed rates for these categories are inversely
proportional to 1.3,1, and 0.7, such that,
βslow client ,m
1.3=βnormal-speed client ,m
1=βfast client ,m
0.7,∀m∈[M].
D ADDITIONAL EXPERIMENTS
In this section, we present supplementary experiments.
D.1 TUNING PARAMETER kOF THE STRAGGLER MITIGATION TECHNIQUE USED FOR
SYNCHRONOUS METHODS (ACCEPTING ONLY THE FIRST- kUPDATES)
In our experiments, to mitigate the high straggler effect, the server in synchronous methods ( Sync-ST ,Sync-Bayes-ST ,
andSync-UCB-ST ) only aggregates the first kclient updates for each task and discards the rest, following [Bonawitz
et al., 2019]. To tune parameter k, we run validation experiments with Sync-ST on single CIFAR-10, MNIST, and
Fashion-MNIST tasks and evaluated the training performance with respect to simulated time and number of global rounds.
A larger kresults in a longer simulated time per round since we wait for more clients. On the other hand, the variance in
aggregated updates on each round becomes smaller since we average more updates. Therefore, the target accuracy is attained
faster in terms of the number of global rounds. We also observed that keeping ktoo small yields lower final accuracy.
Navigating these trade-offs, we find that k= 30 strikes an effective balance.
0 20 40 60
k6008001000Time
Time required to get target accuracy
CIFAR-10: 63.0%
0 20 40 60
k200300400Global round
Global rounds required to get target accuracy
CIFAR-10: 63.0%
0 20 40 60
k0.660.68Accuracy
Final accuracy
CIFAR-10
Figure 10: Performance of Sync-ST with varying kin CIFAR-10 task. The chosen point is shown with a red star.
0 20 40 60
k400500600Time
Time required to get target accuracy
Fashion-MNIST: 82.0%
0 20 40 60
k100150200Global round
Global rounds required to get target accuracy
Fashion-MNIST: 82.0%
0 20 40 60
k0.860.88Accuracy
Final accuracy
Fashion-MNIST
Figure 11: Performance of Sync-ST with varying kin Fashion-MNIST task. The chosen point is shown with a red star.

--- PAGE 17 ---
0 20 40 60
k6080100Time
Time required to get target accuracy
MNIST: 93.0%
0 20 40 60
k204060Global round
Global rounds required to get target accuracy
MNIST: 93.0%
0 20 40 60
k0.9750.980Accuracy
Final accuracy
MNISTFigure 12: Performance of Sync-ST with varying kin MNIST task. The chosen point is shown with a red star.
D.2 TEST LOSS PLOTS OF FIGURES 3 AND 4 IN THE MAIN TEXT
We illustrate test loss plots of the experiments in Figures 3 and 4 in the main text.
20% 30% 40% 50% 60%
The ratio of active clients300350400450500550
CIFAR-10 Final Test Loss
20% 30% 40% 50% 60%
The ratio of active clients120140160180200
Fashion-MNIST Final Test Loss
Figure 13: The mean final loss values of FedAST (blue), FedAST-NoBuffer (olive green) and centralized training
(violet) with varying active client ratio, when training 3identical models. The left figure is for CIFAR-10 dataset, while the
right figure is for Fashion-MNIST dataset. With a higher number of active clients, thanks to the buffer, FedAST remains its
performance while FedAST-NoBuffer gets worse.
0 2000 4000
Time200400600800LossFashion-MNIST
FedAST
FedAST-NoBuffer
0 2000 4000
Time4006008001000CIFAR-10
FedAST
FedAST-NoBuffer
Figure 14: The mean test loss values of FedAST andFedAST-NoBuffer , when simultaneously training one model for
CIFAR-10 and one for Fashion-MNIST. FedAST achieves lower and more stable loss levels.
D.3 TRAINING CURVES OF HOMOGENEOUS EXPERIMENTS
In Figure 15, we provide the average training curves of the homogeneous-task experiment in Figure 5.

--- PAGE 18 ---
50010001500Time0.40.6CIFAR-10(FedAST)
500100015002000Time0.40.6CIFAR-10(Sync-ST)2tasks 4tasks6tasks Target(0.63)
50010001500Time0.40.60.8Fashion-MNIST(FedAST)
500100015002000Time0.40.60.8Fashion-MNIST(Sync-ST)2tasks4tasks 6tasksTarget(0.82)
100300400200Time
0.70.80.91.0MNIST(FedAST)
100300400200Time0.70.80.91.0MNIST(Sync-ST)2tasks4tasks 6tasksTarget(0.93)
2004006008001000Time0.20
0.30.40.5Shakespeare(FedAST)
50010001500Time0.200.30.40.5Shakespeare(Sync-ST)2tasks4tasks 6tasksTarget(0.42)24.32%41.98%22.50%21.29%41.40%28.26%
7.77%21.36%5.15%24.70%46.03%1.35%Figure 15: Training curves of FedAST andSync-ST on2/4/6tasks with CIFAR-10, Fashion-MNIST, MNIST, and
Shakespeare datasets. Time gains of FedAST overSync-ST to attain target accuracy are shown on the colored horizontal
lines. Horizontal black lines indicate target accuracy levels, same as the ones stated in Table 1.
D.4 AN ADDITIONAL EXPERIMENT WITH A LARGER MODEL (RESNET-18) ON CIFAR-100
We run a homogeneous-task experiment with a larger model, ResNet-18, as implemented by Acar et al. [2021] on the
CIFAR-100 dataset, a 100-class image classification dataset [Krizhevsky et al., 2009]. We use the same experimental settings
as those in other experiments except for a few differences elaborated here. We use the Dirichlet distribution with α= 1to
simulate heterogeneity following the approach suggested in Yurochkin et al. [2019]. We use a client-side learning rate ( ηs)
of0.06and the number of local SGD steps ( τ) of5for both FedAST andSync-ST . We present the experimental results
in Figure 16. Our experiments show that FedAST outperforms the synchronous baseline with a ResNet-18 model on the
CIFAR-100 dataset by providing time gains of 22.0%,40.7%, and 56.3%for2,4, and 6simultaneously trained models
respectively.
5000100001500020000Time0.20.30.40.50.6CIFAR-100(FedAST)
100002000030000Time
0.20.30.40.50.6CIFAR-100(Sync-ST)
2tasks4tasks 6tasksTarget(0.5)40.65%56.29%22.02%
(a) Training curves of FedAST andSync-ST on2/4/6simultaneous
CIFAR-100 tasks. Time gains of FedAST overSync-ST to attain target
accuracy are shown on the colored horizontal lines. Horizontal black lines
indicate the target accuracy level, %50 .
0 5000 10000 15000 20000 25000
Time to Reach Target2 tasks 4 tasks 6 tasks
10020.911315.811896.0
12851.319067.127215.8
22.0%40.7%56.3%CIFAR-100
FedAST Sync-ST(b) Mean training times of FedAST andSync-ST to attain
target accuracy level, %50 , on2/4/6simultaneous tasks with
CIFAR-100 dataset. FedAST requires consistently lower wall-
clock time for training compared to Sync-ST ; the percent-
ages represent these time gains.
Figure 16: Experimental results on simultaneous repeated tasks with FedAST andSync-ST on CIFAR-100.
D.5 EXPERIMENTS WITH DIFFERENT TARGET ACCURACY LEVELS
To see how FedAST and the competitor Sync-ST work with different target accuracy, we conduct the experiment in
Figure 7 with +3% higher and −10% lower target accuracy levels as presented in Table 3. We observe that proposed
FedAST reduces the overall training time by 55.9%and16.3%, respectively for higher and lower target accuracy levels.

--- PAGE 19 ---
We conclude that the advantage of FedAST overSync-ST increases with the difficulty of the task (i.e., reaching higher
accuracy).
Table 3: Different target accuracy levels used in experiments to validate the proposed methods, with lower and higher
accuracy targets.
Dataset Lower Target Accuracy Target Accuracy in the Main Text Higher Target Accuracy
MNIST 83% 93% 96%
Fashion-MNIST 72% 82% 85%
CIFAR-10 53% 63% 66%
Shakespeare 32% 42% 45%
0 150 300 450 600 750 900
Time0.20.40.60.81.0Test AccuracyFedAST
0250 500 750 1000 1250 1500 1750 2000
Time0.20.40.60.81.0Test AccuracySync-ST
MNIST Fashion-MNIST CIFAR-10 Shakespeare
Figure 17: Training curves of a single Monte Carlo run of the het-
erogeneous experiment with higher target accuracy levels in Table 3.
Dashed vertical lines show times when tasks reach their target accu-
racy. The setting is the same as the experiment in Figure 7.
0 500 1000 1500 2000
Average Finish TimeSync-ST FedAST
1256.2713.51826.72212.5469.9723.5933.1975.8
55.9%Finish Times in Simultaneous Training
MNIST
Fashion-MNISTCIFAR-10
ShakespeareFigure 18: Mean time required to reach target accu-
racy and time gain of FedAST overSync-ST inthe
heterogeneous experiment with higher target accu-
racy levels in Table 3. The setting is the same as the
experiment in Figure 7.
0 40 80 120 160 200 240 280
Time0.00.20.40.60.81.0Test AccuracyFedAST
0 40 80 120 160 200 240 280 320 360
Time0.00.20.40.60.81.0Test AccuracySync-ST
MNIST Fashion-MNIST CIFAR-10 Shakespeare
Figure 19: Training curves of a single Monte Carlo run of the het-
erogeneous experiment with lower target accuracy levels in Table 3.
Dashed vertical lines show times when tasks reach their target accu-
racy. The setting is the same as the experiment in Figure 7.
0 50 100 150 200 250 300 350
Average Finish TimeSync-ST FedAST
85.370.4185.8350.219.061.4108.5293.2
16.3%Finish Times in Simultaneous Training
MNIST
Fashion-MNISTCIFAR-10
ShakespeareFigure 20: Mean time required to reach target accu-
racy and time gain of FedAST overSync-ST inthe
heterogeneous experiment with lower target accuracy
levels in Table 3. The setting is the same as the exper-
iment in Figure 7.
D.6 PERFORMANCE OF FedAST WITHOUT STATIC RESOURCE ALLOCATION
We conduct heterogeneous-task experiments to validate the performance gain of dynamic resource allocation (FedAST (D))
over static option ( FedAST(S) ) with uniform allocation across tasks in heterogeneous settings. For uniform resource

--- PAGE 20 ---
allocation, we allocate the same number of active training requests to each task in FedAST(S) . To show the consistency of
our results, we run experiments at all target accuracy levels in Table 3. We present the results in Figure 22 (higher target
accuracy), Figure 24 (the target accuracy in the main text), and Figure 26 (lower target accuracy). We conclude that our
dynamic client allocation based on the variance estimates of the updates reduces the total training time compared to the
uniform static client allocation. The advantage of dynamic resource allocation becomes more prominent with more difficult
tasks (i.e., higher target accuracy level).
0 150 300 450 600 750 900
Time0.20.40.60.81.0Test AccuracyFedAST(D)
0150 300 450 600 750 900 1050 1200
Time0.20.40.60.81.0Test AccuracyFedAST(S)
MNIST Fashion-MNIST CIFAR-10 Shakespeare
Figure 21: Training curves of a single Monte Carlo run in the exper-
iment with dynamic resource allocation option (FedAST(D) ) and
static option with uniform resource allocation ( FedAST(S) ). The
setting is the heterogeneous experiment with higher target accuracy
levels in Table 3. Dashed vertical lines show times when tasks reach
their target accuracy.
0 200 400 600 800 1000
Average Finish TimeFedAST(S) FedAST(D)
541.6536.9874.01107.0469.9723.5933.1975.8
11.9%Finish Times in Simultaneous Training
MNIST
Fashion-MNISTCIFAR-10
ShakespeareFigure 22: Mean training times required to reach tar-
get accuracy and time gain of dynamic resource allo-
cation option (FedAST(D) ) over static option with
uniform resource allocation ( FedAST(S) ). The set-
ting is the heterogeneous experiment with higher tar-
get accuracy levels in Table 3.
0 80 160 240 320 400 480 560
Time0.20.40.60.81.0Test AccuracyFedAST(D)
0 80 160 240 320 400 480 560 640
Time0.20.40.60.81.0Test AccuracyFedAST(S)
MNIST Fashion-MNIST CIFAR-10 Shakespeare
Figure 23: Training curves of a single Monte Carlo run in the exper-
iment with dynamic resource allocation option (FedAST(D) ) and
static option with uniform resource allocation ( FedAST(S) ). The
setting is the heterogeneous experiment with the target accuracy levels
used in the main text in Table 3. Dashed vertical lines show times
when tasks reach their target accuracy.
0 100 200 300 400 500 600
Average Finish TimeFedAST(S) FedAST(D)
117.9239.9407.0655.398.5354.6402.6619.0
5.5%Finish Times in Simultaneous Training
MNIST
Fashion-MNISTCIFAR-10
ShakespeareFigure 24: Mean training times required to reach tar-
get accuracy and time gain of dynamic resource allo-
cation option (FedAST(D) ) over static option with
uniform resource allocation ( FedAST(S) ). The set-
ting is the heterogeneous experiment with the target
accuracy levels used in the main text in Table 3.
EPROOFS OF THE CONVERGENCE ANALYSIS OF FedAST WITH STATIC OPTION (S)
In this section, we present the proofs of the mathematical claims made in the paper. First, we define and explain the notations
used in this section. After that, we introduce intermediate lemmas used in the main proof (Section E.2). Then, we present
the proofs of Theorem 1 and Corollary 1.1 (Section E.3). Finally, we prove intermediate lemmas (Section E.4).

--- PAGE 21 ---
0 40 80 120 160 200 240 280
Time0.00.20.40.60.81.0Test AccuracyFedAST(D)
0 40 80 120 160 200 240 280
Time0.00.20.40.60.81.0Test AccuracyFedAST(S)
MNIST Fashion-MNIST CIFAR-10 ShakespeareFigure 25: Training curves of a single Monte Carlo run in the exper-
iment with dynamic resource allocation option (FedAST(D) ) and
static option with uniform resource allocation ( FedAST(S) ). The
setting is the heterogeneous experiment with lower target accuracy
levels in Table 3. Dashed vertical lines show times when tasks reach
their target accuracy.
0 50 100 150 200 250 300
Average Finish TimeFedAST(S) FedAST(D)
13.352.1125.2300.319.061.4108.5293.2
2.4%Finish Times in Simultaneous Training
MNIST
Fashion-MNISTCIFAR-10
ShakespeareFigure 26: Mean training times required to reach tar-
get accuracy and time gain of dynamic resource allo-
cation option (FedAST(D) ) over static option with
uniform resource allocation ( FedAST(S) ). The set-
ting is the heterogeneous experiment with lower target
accuracy levels in Table 3.
E.1 NOTATIONS AND DEFINITIONS
FedAST enables us to divide the convergence analyses of simultaneous tasks into individual ones. We focus on the
convergence analysis of a single task within a simultaneous multi-model setting and the analysis holds for all tasks trained
together. For brevity, we provide the proofs for a single task of multiple models trained simultaneously. Therefore, we drop
all model indices in our analysis. We also drop time indices from the number of active requests ( R) and buffer size ( b) terms
as they remain the same during the training with the static option of FedAST . Table 4 summarizes all notation. Please note
that the analysis presented here holds for every model m∈[M]simultaneously trained within FedAST framework.
E.1.1 The Update Rules of FedAST
We first revisit the local training and global update rules of FedAST . The notation may vary slightly from those in the main
paper due to dropping model indices, but still accurately depicts the same algorithmic procedures, Algorithms 1 and 2 in the
main text.
Local update rule. During local training, clients perform τconsecutive local stochastic gradient steps and return the
output to the server. When a client receives the tthversion of the global model, x(t), it takes τmini-batch stochastic gradient
descent steps (for k= 1, . . . , τ ) with following rule:
x(t,k)
i←x(t,k−1)
i −ηce∇fi
x(t,k−1)
i
, (8)
where x(t,0)
i≜x(t)ande∇denotes stochastic gradients. We define the average of local stochastic gradients as
∆(t)
i≜1
ττ−1X
k=0e∇fi
x(t,k)
i
. Then, the client returnsx(t)−x(t,τ)
i
τηc=1
ττ−1X
k=0e∇fi
x(t,k)
i
=∆(t)
ito the server. The server
stores the updates in a buffer.
Staleness. The server receives the updates of local training requests asynchronously. It means that the received updates
may come in a different order than local training requests sent to clients. Therefore, an aggregated update may have been
calculated with an older version of the model, and this is called staleness . We quantify the staleness of an update in terms of
the number of global rounds passed between the times when the server sends the local training request and receives the
update. The staleness is random for each update, depending on client selections for all tasks and all clients’ availability,
computation, and communication speeds. We denote the staleness of client i’s update received at the server at the tthround
asγt
i. Recall that Assumption 4 (Bounded Staleness) bounds this random value above at γmax.

--- PAGE 22 ---
Global update rule. On each global round t, when the buffer at the server, B(t), is full ( |B(t)|=b, where bis the buffer
size), the server aggregates the updates to proceed to the next global round. Here, B(t)is the set of clients whose updates are
received after (t−1)thand before tthaggregation. The aggregation rule can be written as follows:
x(t+1)←x(t)−τηsηc1
bX
i∈B(t)∆(t−γt
i)
i =x(t)−ηs1
bX
i∈B(t)
x(t−γt
i)−x(t−γt
i,τ)
i
(9)
=x(t)−ηsηc1
bX
i∈B(t)τ−1X
k=0e∇fi
x(t−γt
i,k)
i
.
E.1.2 Virtual Sequence and Set Definitions
We utilize the perturbed iterate idea from Koloskova et al. [2022], Mania et al. [2017].
First, let us introduce some helpful sets and notations. Consider A(t), which represents the set of clients chosen by the
server to receive the tthversion of the model. Recall that the server in FedAST selects the clients uniformly at random with
replacement from all clients. The size of this set, |A(t)|, is always equal to the buffer size, b, (except initialization, t= 0)
because bnew local training requests are made on each round. For instance, if bis set to 3, and the server selects the 2nd,
16th, and 31stclients during the 4thaggregation round, then A(4)is{2,16,31}. The server sends x(4)to the 2nd, 16th, and
31stclients and requests local training with this model. In practical terms, A(t)is a multiset, allowing multiple occurrences
of the same client if a client is selected more than once. Throughout the proof, we consider each occurrence of the same
client in multiset as a distinct update calculated on that particular client. While we acknowledge a slight abuse of notation,
this does not lead to any mathematical flaw, and we believe that this significantly enhances the clarity and comprehensibility
of the proof.
Now, let us define C(t)as the set of clients that have incomplete local training requests at the time of the tthaggregation
because of the asynchronous nature of FedAST . The size of this set, |C(t)|, is always equal to the number of active local
training requests, R, because the server sends a new local training request for every update it receives. For instance, if R
is4, and the server has sent local training requests to the 12th, 27th, 41st, and 55thclients prior to the 5thaggregation, yet
these clients are still processing their updates, then C(5)would be {12,27,41,55}. Note that C(0)is an empty set, as there
are no active local training requests before the algorithm starts. It is worth noting that C(t)is a multiset, allowing multiple
occurrences of the same client if a client has more than one active local training request (recall that multiple requests are
queued at the client side). Each occurrence of a client within this multiset represents a different local training calculated on
that client. We again acknowledge a slight abuse of notation, but this does not lead to any mathematical flaw, and we believe
that this makes the flow of proof significantly easier.
Next, we define the virtual sequence z(t)fort= 0,1, . . . , T as the model that receives local training updates of the global
model, x(t)fort= 0,1, . . . , T , in the correct order. Namely, unlike x(t),z(t)receives the local training updates in the order
in which the server sends those requests. However, it is crucial to note that the local training updates are still calculated with
the global model, x(t). The update rule of the virtual sequence is:
z(t+1)←z(t)−τηsηc1
bX
i∈A(t)∆(t)
i=z(t)−τηsηc1
bX
i∈A(t)1
ττ−1X
k=0e∇fi
x(t,k)
i
, (10)
fort= 0,1, . . . , T −1where z(0)≜x(0).
Remark 1. Now, we state an observation using the definitions of C(t), the virtual sequence, and the global model. When the
tthaggregation happens at the server, the virtual sequence, z(t), has received all the updates from all previous local training
requests on rounds 0,1, . . . , t −1. At the same time, the global model, x(t), has received the same updates except for the
updates of clients in C(t). By the update rules in (9) and (10), note that each received update at the server contributes to the
global model and virtual sequence equally. Therefore, we can express their difference as:
z(t)−x(t)=−τηsηc1
bX
i∈C(t)∆(t−γt
i)
i . (11)

--- PAGE 23 ---
Remark 2. If we count the number of occurrences of the round index yamong the model versions of assigned updates in set
C(t)over all rounds t= 0, . . . , T −1, we can bound this value as:
T−1X
t=0X
i∈C(t)1{t−γt
i=y} ≤bγmax,∀y= 0, . . . , T −1, (12)
where 1is an indicator function that returns 1if the statement is true, and returns 0otherwise. The reasoning for this
observation is as follows. On each round, the server selects bclients (since the server selects one new client for each received
and buffered update where the buffer size is b) and sends them the up-to-date global model. We also know that all local
training requests must be returned to the server within γmaxrounds by Assumption 4 (Bounded Staleness). Therefore, over
the rounds t= 0, . . . , T −1, any round indices can appear at most bγmaxtimes in the summation in the left-hand side of the
inequality in (12). We will use this remark later in the proof.
E.1.3 Notation
We define some useful variables used in the proof and present the notation used in FedAST in Table 4. Also, we again want
to remind the reader that we dropped all model indices in the proof as the theoretical results we present here hold for any of
multiple tasks trained simultaneously, satisfying Assumptions 1 - 4.
Table 4: Summary of notations used in the mathematical analysis of FedAST .
fi(·): The loss function at client i L: Smoothness constant in Assumption 1
f(·): The global loss function τ: Number of local SGD steps
x(t): The global model at the tthround ηc: Client-side learning rate
x(t,k)
i: The local model of client iat the kthlocal step of the
tthroundηs: Server-side learning rate
z(t): The virtual sequence at the tthround (Section E.1.2) b: Buffer size
∇,˜∇: Gradient and stochastic gradient operators σ2
l: Maximum local variance in Assumption 2
g(t,k)
i=e∇fi
x(t,k)
i
: Local stochastic gradient of client i
at round t and local step kσ2
g: Maximum global variance in Assumption 3
∆(t)
i=1
ττ−1X
k=0g(t,k)
i: The update of client iat round t R: Number of total active local training requests anytime
h(t)
i=Eh
∆(t)
ii
: The expected update of client iat round t γmax: Maximum staleness in Assumption 4
eηs=ηsτ: Server learning rate multiplied by the number
of local training stepsγt
i: The staleness of client i’s update at round t
A(t): The set of clients to which the server sends
thetthversion of the model (Section E.1.2)C(t): The set of clients which are requested local training, but
have not returned their updates to the server yet (Section E.1.2)
E.2 INTERMEDIATE LEMMAS
We present intermediate lemmas used through the proof.
Lemma 1. For a set of Qvectors, u1, . . . ,uQ, where Qis a positive integer,
QX
q=1uq2
≤QQX
q=1∥uq∥2.

--- PAGE 24 ---
Proof. The lemma is a direct consequence of Jensen’s inequality with a convex function ∥·∥2and uniform random
distribution over the set of vectors u1, . . . ,uQ.
Lemma 2. Suppose that fi(·)satisfies Assumption 1 (Smoothness) and Assumption 2 (Bounded Variance) for all i∈[N],
and assume that ηc≤1
Lτ. Then the iterates of FedAST satisfy,
E∇fi
x(t)
−h(t)
i2
≤L2η2
cτ
2 (1−D)σ2
l+D
1−DE∇fi
x(t)2
,∀i∈N,
where D≜L2η2
cτ(τ−1).
Further, suppose Assumption 3 (Bounded Heterogeneity) holds. Then, the iterates of FedAST satisfy,
1
NNX
i=1Eh(t)
i− ∇fi
x(t)2
≤L2η2
cτ
2 (1−D)σ2
l+D
1−DE∇f
x(t)2
+D
1−Dσ2
g.
Remark 3. The true gradient at any client using the global model is close to the local update of that client.
Lemma 3. The iterates of FedAST and defined virtual sequence satisfy,
T1≜−*
∇f
z(t)
,1
NNX
i=1h(t)
i+
≤ −1
2∇f
x(t)2
+1
2∇f
z(t)
−f
x(t)2
+1
2NX
i=1
h(t)
i− ∇fi
x(t)2
.
Lemma 4. Suppose that fi(·)satisfies Assumption 2 (Bounded Variance and Unbiased Stochastic Gradients) for all i∈[N],
then the iterates of FedAST satisfy,
T2≜E1
bX
i∈A(t)∆(t)
i2
≤E1
bX
i∈A(t)h(t)
i2
+σ2
l
τb.
Remark 4. The noisy global update due to stochastic gradients is close to the expected update calculated with full gradients.
The buffer and multiple local steps are useful to reduce the variance due to local SGD steps.
Lemma 5. The iterates of FedAST satisfy,
E1
bX
i∈A(t)
∇fi
x(t)
− ∇f
x(t)2
=1
bNNX
i=1E∇fi
x(t)
− ∇f
x(t)2
.
Further, suppose Assumption 3 (Bounded Heterogeneity) holds. Then, the iterates of FedAST also satisfy,
T3≜E1
bX
i∈A(t)h(t)
i2
≤3
NNX
i=1Eh(t)
i− ∇fi
x(t)2
+3σ2
g
b+ 3E∇f
x(t)2
.
Remark 5. FedAST benefits the global variance reduction thanks to the buffer.
Lemma 6. The virtual sequence and the iterates of FedAST satisfy,
1
TT−1X
t=0E∇f
z(t)
− ∇f
x(t)2
≤
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
b2τσ2
l
+1 +D
1−D3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1 +D
1−D1
TT−1X
t=0E∇f
x(t)2
.
Remark 6. As discussed in Remark 1, although the virtual sequence and global model get updates in a different order, they
receive the same updates. Therefore, we can bound their difference.

--- PAGE 25 ---
E.3 PROOFS OF MAIN STATEMENTS
We present and prove Theorem 1 and Corollary 1.1 here.
E.3.1 Theorem 1 (Convergence bound)
First we restate the theorem:
Theorem 1. (Convergence bound): Suppose Assumptions 1 - 4 hold, there are Ractive local training requests, and the
server and client learning rates, ηs, ηcrespectively, satisfy ηs≤√
τbandηc≤minn
1
6Lτ√
τb,1
4Lτ√τRγmaxo
, where bis the
buffer size, and τis the number of local training steps. Then, the iterations of Algorithm 2 ( FedAST ) satisfy:
1
TT−1X
t=0E∇f
x(t)2
≤ O 
f 
x(0)
−minxf(x)
Tηsηcτ!
+OLηsηc
b+L2η2
cτ+L2η2
sη2
cτR
b2
σ2
l
+OLηsηcτ
b+L2η2
cτ(τ−1) +L2η2
sη2
cτ2R2
b2
σ2
g
.
Proof. Using the update rule of the virtual sequence (10) and Assumption ( Asm. ) 1 (Smoothness), and taking the conditional
expectation with respect to z(t), we have,
Eh
f
z(t+1)i
≤f
z(t)
+D
∇f
z(t)
,Eh
z(t+1)−z(t)iE
+L
2Ez(t+1)−z(t)2
=f
z(t)
+*
∇f
z(t)
,E
−eηsηc1
bX
i∈A(t)∆(t)
i
+
+L
2Eeηsηc1
bX
i∈A(t)∆(t)
i2
Asm.2=f
z(t)
−eηsηc1
b*
∇f
z(t)
,E
X
i∈A(t)h(t)
i
+
+L
2eη2
sη2
cE1
bX
i∈A(t)∆(t)
i2
Uniform
client
selection=f
z(t)
+eηsηcE
−*
∇f
z(t)
,1
NNX
i=1h(t)
i+
| {z }
≜T1
+L
2eη2
sη2
cE1
bX
i∈A(t)∆(t)
i2
| {z }
≜T2.
Using Lemmas 3 and 4, we can bound T1andT2. Then, dividing both sides by eηsηc:
E
f 
z(t+1)
−f 
z(t)
eηsηc≤ −1
2E∇f
x(t)2
+1
2E∇f
z(t)
− ∇f
x(t)2
+1
2NNX
i=1
Eh(t)
i− ∇fi
x(t)2
+Leηsηc
2E1
bX
i∈A(t)h(t)
i2
| {z }
≜T3+Leηsηc
2σ2
l
τb.
Using Lemma 5, we can bound T3:
E
f 
z(t+1)
−f 
z(t)
eηsηc
≤ −1
2E∇f
x(t)2
+1
2E∇f
z(t)
− ∇f
x(t)2
+1
2NNX
i=1
Eh(t)
i− ∇fi
x(t)2
+Leηsηc 
3
2NNX
i=1Eh(t)
i− ∇fi
x(t)2
+3σ2
g
2b+3
2E∇f
x(t)2
+σ2
l
2τb!

--- PAGE 26 ---
=
−1
2+3Leηsηc
2
E∇f
x(t)2
+1
2E∇f
z(t)
− ∇f
x(t)2
+Leηsηc 
3σ2
g
2b+σ2
l
2τb!
+3Leηsηc
2+1
21
NNX
i=1Eh(t)
i− ∇fi
x(t)2
Lemma 2
≤
−1
2+3Leηsηc
2
E∇f
x(t)2
+1
2E∇f
z(t)
− ∇f
x(t)2
+Leηsηc 
3σ2
g
2b+σ2
l
2τb!
+3Leηsηc
2+1
2L2η2
cτ
2 (1−D)σ2
l+D
1−DE∇f
x(t)2
+D
1−Dσ2
g
=
−1
2+3Leηsηc
2+D
2 (1−D)+3LeηsηcD
2 (1−D)
E∇f
x(t)2
+1
2E∇f
z(t)
− ∇f
x(t)2
+Leηsηc
2τb+3L3η3
ceηsτ
4 (1−D)+L2η2
cτ
4 (1−D)
σ2
l+3Leηsηc
2b+3LeηsηcD
2 (1−D)+D
2 (1−D)
σ2
g,
where D≜L2η2
cτ(τ−1). Using the tower property of conditional expectation, telescoping the inequality over the round
indices t= 0,1, . . . , T −1, and using Lemma 6, we get,
1
TT−1X
t=01
2−3Leηsηc
2−D
2 (1−D)−3LeηsηcD
2 (1−D)
E∇f
x(t)2
≤1
2TT−1X
t=0E∇f
z(t)
− ∇f
x(t)2
+f 
z(0)
−E
f 
z(T)
Teηsηc+Leηsηc
2τb+3L3η3
ceηsτ
4 (1−D)+L2η2
cτ
4 (1−D)
σ2
l+3Leηsηc
2b+3LeηsηcD
2 (1−D)+D
2 (1−D)
σ2
g
Lemma 6
≤
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
2b2τσ2
l+1 +D
1−D3L2eη2
sη2
cR2
2b2σ2
g+3L2eη2
sη2
cRγmax
2b1 +D
1−D1
TT−1X
t=0E∇f
x(t)2
+f 
z(0)
−E
f 
z(T)
Teηsηc+Leηsηc
2τb+3L3η3
ceηsτ
4 (1−D)+L2η2
cτ
4 (1−D)
σ2
l+3Leηsηc
2b+3LeηsηcD
2 (1−D)+D
2 (1−D)
σ2
g.
Suppose the learning rates satisfy ηs≤√
τb(which also makes eηs≤τ√
τb) and ηc≤minn
1
6Lτ√
τb,1
4Lτ√τRγmaxo
, the
following inequality holds:
1
2−3Leηsηc
2−D
2 (1−D)−3LeηsηcD
2 (1−D)−3L2eη2
sη2
cRγmax
2b1 +D
1−D≥1
11. (13)
Also, notice that z(0)is equal to x(0)by definitions (Section E.1.2) of these sequences and minxf(x)≤f 
z(T)
.
1
TT−1X
t=0E∇f
x(t)2
≤11f 
x(0)
−minxf(x)
Teηsηc(Using (13))
+ 11Leηsηc
2τb+3L3η3
ceηsτ
4 (1−D)+L2η2
cτ
4 (1−D)+
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
2b2τ
σ2
l
+ 113Leηsηc
2b+3LeηsηcD
2 (1−D)+D
2 (1−D)+1 +D
1−D3L2eη2
sη2
cR2
2b2
σ2
g.
Define δ≜f 
x(0)
−minxf(x). After reducing high-order terms using the assumptions, ηs≤√
τb(which also makes
eηs≤τ√
τb) and ηc≤minn
1
6Lτ√
τb,1
4Lτ√τRγmaxo
, and incorporating the constants into the O(·)notation, we have:
1
TT−1X
t=0E∇f
x(t)2
≤ Oδ
Tηsηcτ
+OLηsηc
b+L2η2
cτ+L2η2
sη2
cτR
b2
σ2
l
+OLηsηcτ
b+L2η2
cτ(τ−1) +L2η2
sη2
cτ2R2
b2
σ2
g
.
This concludes the proof.

--- PAGE 27 ---
E.3.2 Proof of Corollary 1 (Convergence Rate)
First, notice that learning rates, ηs=√
τbandηc= minn
1
τL√
T,1
6Lτ√
τb,1
4Lτ√τRγmaxo
satisfy the assumptions ( ηs≤√
τb
andηc≤minn
1
6Lτ√
τb,1
4Lτ√τRγmaxo
) used through the proof.
When T≥max{36bτ,16τRγmax}; set learning rates ηs=√
τbandηc=1
τL√
T. Then, the bound in Theorem 1 reduces
to:
1
TT−1X
t=0E∇f
x(t)2
≤ OL√
Tbτ
δ+O1√
Tbτ+1
τT+R
Tb
σ2
l+Orτ
Tb+1
T+τR2
Tb
σ2
g.
E.4 PROOFS OF INTERMEDIATE LEMMAS
Proof of Lemma 2. We borrow the proof technique from [Wang et al., 2020b, C.5].
E∇fi
x(t)
−h(t)
i2
=E∇fi
x(t)
−1
ττ−1X
k=0∇fi
x(t,k)
i2
Lemma 1
≤1
ττ−1X
k=1E∇fi
x(t)
− ∇fi
x(t,k)
i2
Asm.1
≤L2
ττ−1X
k=1Ex(t)−x(t,k)
i2
| {z }
≜Trecursive=L2η2
c
ττ−1X
k=1Ek−1X
v=0g(t,v)
i2
(14)
Lemma 2 in
[Wang et al., 2020b]=L2η2
c
ττ−1X
k=1
k−1X
v=0Eg(t,v)
i− ∇fi
x(t,v)
i2
+Ek−1X
v=0∇fi
x(t,v)
i2
 (Using Assumption 2)
Lemma 1
≤L2η2
c
ττ−1X
k=1k−1X
v=0
Eg(t,v)
i− ∇fi
x(t,v)
i2
+kE∇fi
x(t,v)
i2
Asm.2
≤L2η2
c
ττ−1X
k=1 
kσ2
l+kk−1X
v=0E∇fi
x(t,v)
i2!
≤L2η2
c
τ 
(τ−1)τ
2σ2
l+(τ−1)τ
2τ−2X
k=0E∇fi
x(t,k)
i2!
≤η2
cL2τ−1
2 
σ2
l+τ−2X
k=0E∇fi
x(t,k)
i2!
≤η2
cL2τ−1
2 
σ2
l+τ−2X
k=0
2E∇fi
x(t,k)
i
− ∇fi
x(t)2
+ 2E∇fi
x(t)2!
Asm.1
≤η2
cL2τ−1
2 
σ2
l+τ−2X
k=0
2L2Ex(t,k)
i−x(t)2
+ 2E∇fi
x(t)2!
≤η2
cL2τ−1
2 
σ2
l+τ−1X
k=1
2L2Ex(t,k)
i−x(t)2
+ 2E∇fi
x(t)2!
≤η2
cL2τ−1
2
σ2
l+ 2L2Trecursive + 2τE∇fi
x(t)2
. (15)
Using the recursive appearances of Trecursive in (14) and (15):
Trecursive
τ=1
ττ−1X
k=1Ex(t,k)
i−x(t)2
≤η2
cτ−1
2σ2
l+η2
cτ(τ−1)Efi
x(t)2
+η2
cL2(τ−1)Trecursive .

--- PAGE 28 ---
Arranging the terms, defining D≜L2η2
cτ(τ−1), and assuming ηc≤1
Lτwhich makes D≤1,
E∇fi
x(t)
−h(t)
i2
≤L2Trecursive
τ≤L2η2
c(τ−1)σ2
l/2 +L2η2
cτ(τ−1)E∇fi 
x(t)2
1−L2η2cτ(τ−1)
≤L2η2
cτ
2 (1−D)σ2
l+D
1−DEfi
x(t)2
,∀i∈N.
This proves the first part of Lemma 2. Now, averaging it across clients:
1
NNX
i=1Eh(t)
i− ∇fi
x(t)2
≤1
NNX
i=1L2η2
cτ
2 (1−D)σ2
l+D
1−DEfi
x(t)2
=L2η2
cτ
2 (1−D)σ2
l+D
1−D1
NNX
i=1E∇fi
x(t)
− ∇f
x(t)
+∇f
x(t)2
=L2η2
cτ
2 (1−D)σ2
l+D
1−D1
NNX
i=1E∇fi
x(t)
− ∇f
x(t)2
+D
1−DE∇f
x(t)2
+D
1−D2
NNX
i=1D
∇fi
x(t)
− ∇f
x(t)
,∇f
x(t)E
Asm.3
≤L2η2
cτ
2 (1−D)σ2
l+D
1−DE∇f
x(t)2
+D
1−Dσ2
g. (Since1
NPN
i=1∇fi 
x(t)
=∇f 
x(t)
)
This concludes the proof of Lemma 2.
Proof of Lemma 3.
T1≜−*
∇f
z(t)
,1
NNX
i=1h(t)
i+
=−*
∇f
z(t)
,1
NNX
i=1
h(t)
i− ∇fi
x(t)
+∇f
x(t)+
=−D
∇f
z(t)
,∇f
x(t)E
−*
∇f
z(t)
,1
NNX
i=1
h(t)
i− ∇fi
x(t)+
=−1
2∇f
z(t)2
−1
2∇f
x(t)2
+1
2∇f
z(t)
− ∇f
x(t)2
−1
2∇f
z(t)2
−1
21
NNX
i=1
h(t)
i− ∇fi
x(t)2
+1
2∇f
z(t)
−1
NNX
i=1
h(t)
i− ∇fi
x(t)2
Lemma 1
≤ −∇f
z(t)2
−1
2∇f
x(t)2
+1
2∇f
z(t)
− ∇f
x(t)2
+1
21
NNX
i=1
h(t)
i− ∇fi
x(t)2
+∇f
z(t)2
=−1
2∇f
x(t)2
+1
2∇f
z(t)
− ∇f
x(t)2
+1
21
NNX
i=1
h(t)
i− ∇fi
x(t)2
.
Proof of Lemma 4.
T2≜E1
bX
i∈A(t)∆(t)
i2
=E1
bX
i∈A(t)h(t)
i+1
bX
i∈A(t)
∆(t)
i−h(t)
i2
=E1
bX
i∈A(t)h(t)
i+1
bX
i∈A(t) 
1
ττ−1X
k=0
g(t,k)
i− ∇fi
x(t,k)
i!2

--- PAGE 29 ---
=E1
bX
i∈A(t)h(t)
i2
+E1
bX
i∈A(t) 
1
ττ−1X
k=0
g(t,k)
i− ∇fi
x(t,k)
i!2
(Using Assumption 2)
Lemma 2 in
[Wang et al., 2020b]= E1
bX
i∈A(t)h(t)
i2
+1
bNNX
i=11
τ2τ−1X
k=0E
g(t,k)
i− ∇fi
x(t,k)
i2
≤E1
bX
i∈A(t)h(t)
i2
+σ2
l
τb.
Proof of Lemma 5.
E1
bX
i∈A(t)
∇fi
x(t)
− ∇f
x(t)2
=1
b2E
X
i∈A(t)∇fi
x(t)
− ∇f
x(t)2
+X
iandrare
two different
items in A(t)D
∇fi
x(t)
− ∇f
x(t)
,∇fr
x(t)
− ∇f
x(t)E

(a)=1
bNNX
i=1E∇fi
x(t)
− ∇f
x(t)2
+E"
1
N2NX
i=1NX
r=1D
∇fi
x(t)
− ∇f
x(t)
,∇fr
x(t)
− ∇f
x(t)E#
(b)=1
bNNX
i=1E∇fi
x(t)
− ∇f
x(t)2
, (16)
where (a)follows that the clients in A(t)are selected uniformly at random with replacement among all clients (see
Section E.1.2), and (b)follows thatPN
i=1∇fi 
x(t)
=Nf 
x(t)
. This proves the first part of Lemma 5.
T3≜E1
bX
i∈A(t)h(t)
i2
=E1
bX
i∈A(t)
h(t)
i− ∇fi
x(t)
+∇fi
x(t)
− ∇f
x(t)
+∇f
x(t)2
Lemma 1
≤3E1
bX
i∈A(t)
h(t)
i− ∇fi
x(t)2
+ 3E1
bX
i∈A(t)
∇fi
x(t)
− ∇f
x(t)2
+ 3E∇f
x(t)2
Using ( 16)
and
Lemma 1≤3
NNX
i=1Eh(t)
i− ∇fi
x(t)2
+3
bNNX
i=1E∇fi
x(t)
− ∇f
x(t)2
+ 3E∇f
x(t)2
Asm.3
≤3
NNX
i=1Eh(t)
i− ∇fi
x(t)2
+3σ2
g
b+ 3E∇f
x(t)2
.
Proof of Lemma 6. We start by using Assumption 1 (Smoothness) and Remark 1.
E∇f
z(t)
− ∇f
x(t)2
≤L2Ez(t)−x(t)2
=L2Eeηsηc1
bX
i∈C(t)∆(t−γt
i)
i2
=L2Eeηsηc
bX
i∈C(t)
∆(t−γt
i)
i−h(t−γt
i)
i +h(t−γt
i)
i2
Asm.2=L2eη2
sη2
cE1
bX
i∈C(t)
∆(t−γt
i)
i−h(t−γt
i)
i2
+L2eη2
sη2
cE1
bX
i∈C(t)h(t−γt
i)
i2

--- PAGE 30 ---
=L2eη2
sη2
cE1
bX
i∈C(t)1
ττ−1X
k=0
g(t−γt
i,k)
i − ∇fi
x(t−γt
i,k)
i2
+L2eη2
sη2
cE1
bX
i∈C(t)h(t−γt
i)
i2
Asm.2
≤L2eη2
sη2
cR
b2τσ2
l+L2eη2
sη2
cR
b2E
X
i∈C(t)h(t−γt
i)
i2

≤L2eη2
sη2
cR
b2τσ2
l+L2eη2
sη2
cR
b2E
X
i∈C(t)h(t−γt
i)
i− ∇fi
x(t−γt
i)
+∇fi
x(t−γt
i)
− ∇f
x(t−γt
i)
+∇f
x(t−γt
i)2

Lemma 1
≤L2eη2
sη2
cR
b2τσ2
l
+3L2eη2
sη2
cR
b2E
X
i∈C(t)∇f
x(t−γt
i)2
+h(t−γt
i)
i− ∇fi
x(t−γt
i)2
+∇f
x(t−γt
i)
− ∇fi
x(t−γt
i)2

≤L2eη2
sη2
cR
b2τσ2
l+3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cR
b2E
X
i∈C(t)∇f
x(t−γt
i)2
+h(t−γt
i)
i− ∇fi
x(t−γt
i)2
.
Telescoping the inequality over t= 0, . . . , T −1:
1
TT−1X
t=0E∇f
z(t)
− ∇f
x(t)2
≤L2eη2
sη2
cR
b2τσ2
l+3L2eη2
sη2
cR2
b2σ2
g
+3L2eη2
sη2
cR
b21
TT−1X
t=0E
X
i∈C(t)∇f
x(t−γt
i)2
+h(t−γt
i)
i− ∇fi
x(t−γt
i)2

Remark 2
≤L2eη2
sη2
cR
b2τσ2
l+3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cR
b21
TT−1X
t=0E
X
i∈C(t)h(t−γt
i)
i− ∇fi
x(t−γt
i)2

Lemma 2
≤L2eη2
sη2
cR
b2τσ2
l+3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cR
b21
TT−1X
t=0E
X
i∈C(t)L2η2
cτ
2 (1−D)σ2
l+D
1−D∇fi
x(t−γt
i)2

≤
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
b2τσ2
l+3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cR
b21
TT−1X
t=0E
X
i∈C(t)D
1−D∇fi
x(t−γt
i)2

Lemma 1
≤
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
b2τσ2
l+3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cR
b21
TT−1X
t=0E
X
i∈C(t)2D
1−D∇fi
x(t−γt
i)
− ∇f
x(t−γt
i)2
+2D
1−D∇f
x(t−γt
i)2

Asm.3
≤
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
b2τσ2
l+1 +D
1−D3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1
TT−1X
t=0E∇f
x(t)2

--- PAGE 31 ---
+3L2eη2
sη2
cR
b21
TT−1X
t=0E
X
i∈C(t)2D
1−D∇f
x(t−γt
i)2

Remark 2
≤
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
b2τσ2
l+1 +D
1−D3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cRγmax
b2D
1−D1
TT−1X
t=0E∇f
x(t)2
=
1 +3RL2η2
cτ2
2 (1−D)L2eη2
sη2
cR
b2τσ2
l+1 +D
1−D3L2eη2
sη2
cR2
b2σ2
g+3L2eη2
sη2
cRγmax
b1 +D
1−D1
TT−1X
t=0E∇f
x(t)2
.
FCONVERGENCE OF FedAST WITH DYNAMIC CLIENT ALLOCATION ( OPTION =D)
With a similar approach to the proof of static client allocation, we can show the convergence of the FedAST with dynamic
client allocation ( option =D), too. Adopting all of the previously used notation, we also need some new definitions to
analyze this version of the algorithm, as the number of active training requests and buffer size can change dynamically
during the training.
Notation for changing buffer size and number of active training requests. Let us define b(t)andR(t)as the buffer size
and the number of active local training requests of the model. Further, define bminandbmaxthe minimum and maximum
value that the buffer size can take. Similarly, define RminandRmaxas the minimum and maximum number of active training
requests. Moreover, we define ρb≜bmax/bminas the measure of skewness in buffer size.
Global update rule and virtual sequence definition. Although the local update rule remains the same, the global update
rule slightly changes for dynamic client allocation due to changing buffer size:
x(t+1)←x(t)−τηsηc1
b(t)X
i∈B(t)∆(t−γt
i)
i =x(t)−ηs1
b(t)X
i∈B(t)
x(t−γt
i)−x(t−γt
i,τ)
i
(17)
=x(t)−ηsηc1
b(t)X
i∈B(t)τ−1X
k=0e∇fi
x(t−γt
i,k)
i
,
where |B(t)|=b(t). Note that (17) is almost identical to (9), except the varying buffer-size b(t).
Next, we define ri(t)as the index of the global round when a local training request sent to client iin round treturns to the
server. Basically, it is the current round index t, added to the future value of staleness that the requested update will have.
We need to define a new virtual sequence y(t), which is different from the z(t)defined earlier.
y(t+1)←y(t)−τηsηcX
i∈A(t)1
bri(t)∆(t)
i=y(t)−τηsηcX
i∈A(t)1
bri(t)1
ττ−1X
k=0e∇fi
x(t,k)
i
, (18)
fort= 0,1, . . . , T −1where y(0)≜x(0).Here,A(t)is defined similarly as it was in Section E.1.2. Note that the probability
of being in A(t)is equal across clients due to uniform client selection. However, this time, the size of this set does not have
to be equal to the buffer size at round t. Due to the new client selection rule (Line 13 in Algorithm 2), the server may assign
0,1, or2clients for each received update. Therefore, we know that 0<|A(t)| ≤2b(t).
Here, we need a simplifying assumption for the purpose of this proof:
Assumption 5 (bri(t)Values) .We assume that any bri(t)value is known at the time when a local training request is sent to
client iat round t, and these values are independent of any future information including the received updates. We further
assume that bri(t)values are equal (denote br(t)) for all clients in A(t).
Remark 7. When we keep the period of dynamic client allocation long enough, we observe that most of the assigned
local training requests at one round fall in the same window before the next dynamic client allocation happens (Line 8 in

--- PAGE 32 ---
Algorithm 2). Hence, based on our empirical observations, what assumption implies holds for most of the local training
requests. Further, this assumption can be avoided by taking an average of the updates during aggregation weighted
inversely with the number of local training requests sent at the same global round. In other words, one may have avoided this
assumption by weighting an update from client iwith1/|A(t−γt
i)|instead of taking average over buffer during aggregation
at round t. However, we did not see any practical benefit of this type of weighting in our experiments, and this strange
weighting would be just for theoretical purposes. Therefore, we keep the current version.
We first state the theorem showing the convergence of FedAST with dynamic client allocation option.
Theorem 2. (Convergence of FedAST with option =D):Suppose Assumptions 1 - 5 hold, and the learning rates satisfy
ηs≤ρ−3/2
b√
τbandηc≤min
ρ−3/2
b
24Lτ√
τb,ρ−3/2
b
16Lτ√τRγmax
. Then, the iterations of Algorithm 1 ( FedAST ) with option =D
satisfy:
1
TT−1X
t=0E∇f
x(t)2
≤ O 
f 
x(0)
−minxf(x)
Tηsηcτ!
+OLηsηcρ3
b
bmin+L2η2
cρ2
bτ+L2η2
sη2
cτRmaxρ2
b
b2
min
σ2
l
+OLηsηcτρ3
b
bmin+L2η2
cτ(τ−1)ρ2
b+L2η2
sη2
cτ2R2
maxρ2
b
b2
min
σ2
g
.
Proof.
We will need one extra lemma corresponding to Lemma 6.
Lemma 7. The new virtual sequence 
y(t)
and the iterates of FedAST satisfy,
1
TT−1X
t=0E∇f
y(t)
− ∇f
x(t)2
≤
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmax
b2
minτσ2
l
+1 +D
1−D3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1 +D
1−D1
TT−1X
t=0E∇f
x(t)2
.
Now, using the update rule of the virtual sequence (18) and Assumption 1 (Smoothness), and taking the conditional
expectation with respect to y(t), we have,
Eh
f
y(t+1)i
≤f
y(t)
+D
∇f
y(t)
,Eh
y(t+1)−y(t)iE
+L
2Ey(t+1)−y(t)2
=f
y(t)
+*
∇f
y(t)
,E
−eηsηcX
i∈A(t)1
bri(t)∆(t)
i
+
+L
2EeηsηcX
i∈A(t)1
bri(t)∆(t)
i2
≤f
y(t)
−eηsηc|A(t)|
br(t)E
*
∇f
y(t)
,1
|A(t)|X
i∈A(t)h(t)
i+
 (Using Assumption 5)
+L|A(t)|2
2(br(t))2Eeηsηc1
|A(t)|X
i∈A(t)∆(t)
i2
(|A(t)|is not random with conditional expectation)
=f
y(t)
+eηsηc|A(t)|
br(t)E
−*
∇f
y(t)
,1
NNX
i=1h(t)
i+
| {z }
≜T1
+ 2ρ2
bLeη2
sη2
cE1
|A(t)|X
i∈A(t)∆(t)
i2
.(|A(t)|
br(t)≤2ρb)
Next, using Lemma 3 (with y(t)sequence) and Lemma 4 (with |A(t)|), using 1/ρb≤ |A(t)|/br(t)≤2ρb, and dividing both
sides by eηsηcwe obtain,
E
f 
y(t+1)
−f 
y(t)
eηsηc≤ −1
2ρbE∇f
x(t)2
+ρbE∇f
y(t)
− ∇f
x(t)2

--- PAGE 33 ---
+ρb
NNX
i=1Eh(t)
i− ∇fi
x(t)2
+ 2ρ2
bLeηsηcE1
|A(t)|X
i∈A(t)h(t)
i2
+ 2ρ2
bLeηsηcσ2
l
τbmin.
Using Lemma 5, we get,
E
f 
y(t+1)
−f 
y(t)
eηsηc
≤ −1
2ρbE∇f
x(t)2
+ρbE∇f
y(t)
− ∇f
x(t)2
+ρb
NNX
i=1Eh(t)
i− ∇fi
x(t)2
+ρ2
bLeηsηc 
6
NNX
i=1Eh(t)
i− ∇fi
x(t)2
+6σ2
g
bmin+ 6E∇f
x(t)2
+2σ2
l
τbmin!
=
−1
2ρb+ 6ρ2
bLeηsηc
E∇f
x(t)2
+ρbE∇f
y(t)
− ∇f
x(t)2
+ρ2
bLeηsηc 
6σ2
g
bmin+2σ2
l
τbmin!
+ 
6ρ2
bLeηsηc+ρb1
NNX
i=1Eh(t)
i− ∇fi
x(t)2
≤
−1
2ρb+ 6ρ2
bLeηsηc
E∇f
x(t)2
+ρbE∇f
y(t)
− ∇f
x(t)2
+ρ2
bLeηsηc 
6σ2
g
bmin+2σ2
l
τbmin!
+ 
6ρ2
bLeηsηc+ρbL2η2
cτ
2 (1−D)σ2
l+D
1−DE∇f
x(t)2
+D
1−Dσ2
g
(Using Lemma 2)
=
−1
2ρb+ 6ρ2
bLeηsηc+ρbD
(1−D)+6ρ2
bLeηsηcD
(1−D)
E∇f
x(t)2
+ρbE∇f
y(t)
− ∇f
x(t)2
+2ρ2
bLeηsηc
τbmin+3ρ2
bL3η3
ceηsτ
(1−D)+ρbL2η2
cτ
2 (1−D)
σ2
l+6ρ2
bLeηsηc
bmin+6ρ2
bLeηsηcD
(1−D)+ρbD
(1−D)
σ2
g,
where D≜L2η2
cτ(τ−1). Using tower property of conditional expectation, telescoping the inequality over the round
indices t= 0,1, . . . , T −1, and using Lemma 7, we get,
1
TT−1X
t=01
2ρb−6ρ2
bLeηsηc−ρbD
(1−D)−6ρ2
bLeηsηcD
(1−D)
E∇f
x(t)2
≤ρb
TT−1X
t=0E∇f
y(t)
− ∇f
x(t)2
+f 
y(0)
−E
f 
y(T)
Teηsηc+2ρ2
bLeηsηc
τbmin+3ρ2
bL3η3
ceηsτ
(1−D)+ρbL2η2
cτ
2 (1−D)
σ2
l+6ρ2
bLeηsηc
bmin+6ρ2
bLeηsηcD
(1−D)+ρbD
(1−D)
σ2
g
≤
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmaxρb
b2
minτσ2
l+1 +D
1−D3L2eη2
sη2
cR2
maxρb
b2
minσ2
g+6L2eη2
sη2
cRmaxρ2
bγmax
bmin1 +D
1−D1
TT−1X
t=0E∇f
x(t)2
+f 
y(0)
−E
f 
y(T)
Teηsηc+2ρ2
bLeηsηc
τbmin+3ρ2
bL3η3
ceηsτ
(1−D)+ρbL2η2
cτ
2 (1−D)
σ2
l+6ρ2
bLeηsηc
bmin+6ρ2
bLeηsηcD
(1−D)+ρbD
(1−D)
σ2
g.
Suppose the learning rates satisfy ηs≤ρ−3/2
b√
τb(which also makes eηs≤ρ−3/2
bτ√
τb) and ηc≤
min
ρ−3/2
b
24Lτ√
τb,ρ−3/2
b
16Lτ√τRγmax
, the following inequality holds:
1
2−6ρ3
bLeηsηc−ρ2
bD
(1−D)−6ρ3
bLeηsηcD
(1−D)−6L2eη2
sη2
cRmaxρ3
bγmax
bmin1 +D
1−D≥1
11. (19)
Also, notice that y(0)is equal to x(0)by definitions (Section F) of these sequences and minxf(x)≤f 
y(T)
.
1
TT−1X
t=0E∇f
x(t)2
≤11f 
x(0)
−minxf(x)
Teηsηcρb (Using (19))

--- PAGE 34 ---
+ 112ρ3
bLeηsηc
τbmin+3ρ3
bL3η3
ceηsτ
(1−D)+ρ2
bL2η2
cτ
2 (1−D)+
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmaxρ2
b
b2
minτ
σ2
l
+ 116ρ3
bLeηsηc
bmin+6ρ3
bLeηsηcD
(1−D)+ρ2
bD
(1−D)+1 +D
1−D3L2eη2
sη2
cR2
maxρ2
b
b2
min
σ2
g.
Define δ≜f 
x(0)
−minxf(x). After reducing high-order terms using the assumptions, ηs≤ρ−3/2
b√
τb(which also
makeseηs≤ρ−3/2
bτ√
τb) and ηc≤min
ρ−3/2
b
24Lτ√
τb,ρ−3/2
b
16Lτ√τRγmax
, and incorporating the constants into the O(·)notation,
we have:
1
TT−1X
t=0E∇f
x(t)2
≤ Oδρb
Tηsηcτ
+OLηsηcρ3
b
bmin+L2η2
cρ2
bτ+L2η2
sη2
cτRmaxρ2
b
b2
min
σ2
l
+OLηsηcτρ3
b
bmin+L2η2
cτ(τ−1)ρ2
b+L2η2
sη2
cτ2R2
maxρ2
b
b2
min
σ2
g
.
This concludes the proof.
Proof of Lemma 7: We start by using Assumption 1 (Smoothness) and observing that Remark 1 still holds with y(t)for the
dynamic client allocation option.
E∇f
y(t)
− ∇f
x(t)2
≤L2Ey(t)−x(t)2
=L2EeηsηcX
i∈C(t)1
br(t−γt
i)∆(t−γt
i)
i2
=L2EeηsηcX
i∈C(t)1
br(t−γt
i)
∆(t−γt
i)
i−h(t−γt
i)
i +h(t−γt
i)
i2
=L2eη2
sη2
cEX
i∈C(t)1
br(t−γt
i)
∆(t−γt
i)
i−h(t−γt
i)
i2
+L2eη2
sη2
cEX
i∈C(t)1
br(t−γt
i)h(t−γt
i)
i2
=L2eη2
sη2
cEX
i∈C(t)1
br(t−γt
i)1
ττ−1X
k=0
g(t−γt
i,k)
i − ∇fi
x(t−γt
i,k)
i2
+L2eη2
sη2
cEX
i∈C(t)1
br(t−γt
i)h(t−γt
i)
i2
(Using Assumption 2)
≤L2eη2
sη2
cRmax
b2
minτσ2
l+L2eη2
sη2
cRmax
b2
minE
X
i∈C(t)h(t−γt
i)
i2
 (Using ∥Pn
i=0xi∥2≤nPn
i=0∥xi∥2and|C(t)| ≤Rmax)
≤L2eη2
sη2
cRmax
b2
minτσ2
l+L2eη2
sη2
cRmax
b2
minE
X
i∈C(t)h(t−γt
i)
i− ∇fi
x(t−γt
i)
+∇fi
x(t−γt
i)
− ∇f
x(t−γt
i)
+∇f
x(t−γt
i)2

≤L2eη2
sη2
cRmax
b2
minτσ2
l
+3L2eη2
sη2
cRmax
b2
minE
X
i∈C(t)∇f
x(t−γt
i)2
+h(t−γt
i)
i− ∇fi
x(t−γt
i)2
+∇f
x(t−γt
i)
− ∇fi
x(t−γt
i)2

≤L2eη2
sη2
cRmax
b2
minτσ2
l+3L2eη2
sη2
cR2
max
b2
minσ2
g+3L2eη2
sη2
cRmax
b2
minE
X
i∈C(t)∇f
x(t−γt
i)2
+h(t−γt
i)
i− ∇fi
x(t−γt
i)2
.
Telescoping the inequality over t= 0, . . . , T −1:
1
TT−1X
t=0E∇f
y(t)
− ∇f
x(t)2
≤L2eη2
sη2
cRmax
b2
minτσ2
l+3L2eη2
sη2
cR2
max
b2
minσ2
g

--- PAGE 35 ---
+3L2eη2
sη2
cRmax
b2
min1
TT−1X
t=0E
X
i∈C(t)∇f
x(t−γt
i)2
+h(t−γt
i)
i− ∇fi
x(t−γt
i)2

≤L2eη2
sη2
cRmax
b2
minτσ2
l+3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1
TT−1X
t=0E∇f
x(t)2
(Using Remark 2, however, this time,)
+3L2eη2
sη2
cRmax
b2
min1
TT−1X
t=0E
X
i∈C(t)h(t−γt
i)
i− ∇fi
x(t−γt
i)2
 (the maximum appearance can be 2γmaxbmax)
≤L2eη2
sη2
cRmax
b2
minτσ2
l+3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cRmax
b2
min1
TT−1X
t=0E
X
i∈C(t)L2η2
cτ
2 (1−D)σ2
l+D
1−D∇fi
x(t−γt
i)2
 (Using Lemma 2)
≤
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmax
b2
minτσ2
l+3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cRmax
b2
min1
TT−1X
t=0E
X
i∈C(t)D
1−D∇fi
x(t−γt
i)2

≤
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmax
b2
minτσ2
l+3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cRmax
b2
min1
TT−1X
t=0E
X
i∈C(t)2D
1−D∇fi
x(t−γt
i)
− ∇f
x(t−γt
i)2
+2D
1−D∇f
x(t−γt
i)2

≤
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmax
b2
minτσ2
l+1 +D
1−D3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1
TT−1X
t=0E∇f
x(t)2
+3L2eη2
sη2
cRmax
b2
min1
TT−1X
t=0E
X
i∈C(t)2D
1−D∇f
x(t−γt
i)2

≤
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmax
b2
minτσ2
l+1 +D
1−D3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1
TT−1X
t=0E∇f
x(t)2
+6L2eη2
sη2
cRmaxρbγmax
bmin2D
1−D1
TT−1X
t=0E∇f
x(t)2
(Using Remark 2)
=
1 +3RmaxL2η2
cτ2
2 (1−D)L2eη2
sη2
cRmax
b2
minτσ2
l+1 +D
1−D3L2eη2
sη2
cR2
max
b2
minσ2
g+6L2eη2
sη2
cRmaxρbγmax
bmin1 +D
1−D1
TT−1X
t=0E∇f
x(t)2
.
