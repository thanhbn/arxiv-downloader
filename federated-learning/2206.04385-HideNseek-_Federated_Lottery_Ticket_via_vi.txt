# HideNseek: Vé số May mắn Liên bang thông qua
Cắt tỉa Phía Máy chủ và Siêu mặt nạ Dấu hiệu

Anish K. Vallapuram1 Pengyuan Zhou2 Young D. Kwon3
Lik Hang Lee4 Hengwei Xu2 Pan Hui1

1Đại học Khoa học và Công nghệ Hong Kong
2Đại học Khoa học và Công nghệ Trung Quốc
3Đại học Cambridge
4Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc

pyzhou@ustc.edu.cn

## Tóm tắt

Học liên bang giảm thiểu rủi ro riêng tư trong học phân tán bằng cách chỉ truyền các cập nhật mô hình cục bộ đến máy chủ trung tâm. Tuy nhiên, nó gặp phải những thách thức bao gồm tính không đồng nhất thống kê của tập dữ liệu của khách hàng và hạn chế tài nguyên của các thiết bị khách hàng, điều này ảnh hưởng nghiêm trọng đến hiệu suất huấn luyện và trải nghiệm người dùng. Các công trình trước đây đã giải quyết những thách thức này bằng cách kết hợp cá nhân hóa với các phương án nén mô hình bao gồm lượng tử hóa và cắt tỉa. Tuy nhiên, việc cắt tỉa phụ thuộc vào dữ liệu và do đó phải được thực hiện ở phía khách hàng, điều này đòi hỏi chi phí tính toán đáng kể. Hơn nữa, việc cắt tỉa thường huấn luyện một siêu mặt nạ nhị phân ∈ {0,1} mà hạn chế đáng kể khả năng mô hình nhưng không mang lại lợi ích tính toán. Do đó, việc huấn luyện đòi hỏi chi phí tính toán cao và thời gian dài để hội tụ trong khi hiệu suất mô hình không đáng giá. Trong công trình này, chúng tôi đề xuất HideNseek sử dụng cắt tỉa không phụ thuộc dữ liệu một lần tại khởi tạo để có được một mạng con dựa trên tính nổi bật synap của trọng số. Sau đó, mỗi khách hàng tối ưu hóa một siêu mặt nạ dấu hiệu ∈ {-1,+1} nhân với các trọng số không bị cắt tỉa để cho phép hội tụ nhanh hơn với cùng tỷ lệ nén như công nghệ tiên tiến. Kết quả thực nghiệm từ ba tập dữ liệu chứng minh rằng so với công nghệ tiên tiến, HideNseek cải thiện độ chính xác suy luận lên đến 40,6% trong khi giảm chi phí giao tiếp và thời gian huấn luyện lên đến 39,7% và 46,8% tương ứng.

## 1 Giới thiệu

Học liên bang [McMahan et al., 2017] cải thiện bảo vệ riêng tư bằng cách tách rời nhu cầu về kho dữ liệu trung tâm khỏi việc học các tập dữ liệu phân tán. Mỗi khách hàng tải lên mô hình cục bộ được huấn luyện với tập dữ liệu cục bộ của mình lên máy chủ trung tâm duy trì một mô hình toàn cục. Máy chủ cập nhật mô hình toàn cục thông qua việc tổng hợp các cập nhật của khách hàng và gửi mô hình mới đến các khách hàng. Những quy trình này được lặp lại cho đến khi mô hình hội tụ hoặc chu kỳ định trước kết thúc.

Trong khi cải thiện bảo vệ riêng tư, mô hình này cũng gặp phải những thách thức như tính không đồng nhất thống kê, đề cập đến phân phối không IID của dữ liệu giữa các khách hàng, ảnh hưởng đến sự hội tụ của mô hình toàn cục nếu dữ liệu từ các khách hàng khác nhau quá phân tán. Một số công trình giải quyết tính không đồng nhất thống kê bằng cách giới thiệu các kỹ thuật học khác nhau cho cá nhân hóa [Fallah et al., 2020; Smith et al., 2017; Lin et al., 2020; Gong et al., 2021; Zhu et al., 2021].

Một thách thức lớn khác mà học liên bang gặp phải là hạn chế tài nguyên, đề cập đến khả năng tính toán hạn chế và băng thông truyền của các thiết bị khách hàng. Tài nguyên hạn chế hạn chế kích thước mô hình có thể được huấn luyện trong các thiết bị khách hàng và truyền kịp thời đến máy chủ để tổng hợp. Một số công trình gần đây đã xuất hiện để giải quyết thách thức này bằng cách điều chỉnh các quy trình huấn luyện [Li et al., 2020; Reisizadeh et al., 2020] và sử dụng các phương án nén mô hình [Diao et al., 2021; Horvath et al., 2021; Bouacida et al., 2021].

Gần đây, FedMask [Li et al., 2021] giải quyết cả tính không đồng nhất thống kê và hạn chế tài nguyên bằng cách sử dụng triết lý "che mặt là huấn luyện" dựa trên giả thuyết vé số may mắn (LTH) [Frankle and Carbin, 2018]. FedMask cắt tỉa mô hình cục bộ để tuân thủ các hạn chế tài nguyên. Sau đó, khách hàng học một siêu mặt nạ nhị phân cục bộ thưa thớt để cá nhân hóa nhằm giảm thiểu tính không đồng nhất thống kê giữa các khách hàng.

Tuy nhiên, các phương pháp hiện tại gặp phải một số thách thức. Thứ nhất, các phương pháp cắt tỉa hiện tại trong thiết lập liên bang chủ yếu phụ thuộc vào dữ liệu và do đó phải được thực hiện ở phía khách hàng. Do đó, các thiết bị khách hàng với tài nguyên hạn chế không thể tránh khỏi chi phí tính toán đáng kể. Thứ hai, các phương pháp cắt tỉa như vậy thường sử dụng siêu mặt nạ nhị phân về cơ bản là cắt tỉa không có cấu trúc không mang lại lợi thế tính toán nhưng lại hạn chế khả năng mô hình. Do đó, hiệu suất mô hình bị hạn chế, nhưng đòi hỏi chi phí tính toán cao và thời gian hội tụ dài.

Vì vậy, trong bài báo này, chúng tôi đề xuất HideNseek, một khung học liên bang nhận thức về tính không đồng nhất thống kê cung cấp hiệu quả tính toán và giao tiếp được hỗ trợ bởi siêu mặt nạ dấu hiệu [Zhou et al., 2019]. Cụ thể, chúng tôi đóng góp như sau:

• HideNseek đề xuất phiên bản liên bang đầu tiên của LTH với tối ưu hóa dấu hiệu. So với siêu mặt nạ nhị phân thường được sử dụng, phương pháp của chúng tôi cung cấp độ chính xác cao hơn và hội tụ nhanh hơn.

• HideNseek thực hiện cắt tỉa một lần phía máy chủ tại khởi tạo bằng cách sử dụng phương pháp không phụ thuộc dữ liệu lặp đi lặp lại dựa trên tính nổi bật synap của dấu hiệu trọng số. Như vậy, HideNseek giảm đáng kể gánh nặng tính toán và chi phí giao tiếp cho các khách hàng với khả năng hạn chế.

• Kết quả thực nghiệm trên các tập dữ liệu khác nhau chứng minh rằng HideNseek vượt trội hơn các phương pháp tiên tiến về độ chính xác suy luận lên đến 40,6% trong khi giảm chi phí giao tiếp và thời gian huấn luyện lên đến 39,7% và 46,8% tương ứng.

## 2 Bối cảnh & Công thức Mục tiêu

Trong phần này, chúng tôi bắt đầu với bối cảnh về học liên bang và LTH và xây dựng mục tiêu của cắt tỉa không phụ thuộc dữ liệu cho học liên bang.

### 2.1 Học Liên bang

Trong học liên bang, một mạng nơ-ron sâu phải được học theo cách phân tán. Điều này được thực hiện bằng cách một máy chủ trung tâm tổng hợp các bản sao của trọng số mạng được học giữa các khách hàng trên tập dữ liệu cục bộ của họ. Mục tiêu học tập là tìm các trọng số w để tối thiểu hóa tổn thất thực nghiệm giữa các khách hàng:

min_w F(w) = Σ(k=1 to K) (n_k/n) L[f(x_k; w); y_k]    (1)

trong đó mạng f(·; w) là một hàm hợp thành của các lớp được tham số hóa bởi các trọng số vector hóa w ∈ R^d và L là hàm tổn thất thực nghiệm đo lường khả năng xấp xỉ hàm tạo ra tập dữ liệu cục bộ (x_k, y_k) của khách hàng k. n_k là số lượng mẫu cục bộ và n = Σ_k n_k là tổng số mẫu trên tất cả K khách hàng.

Một công trình trước đây [McMahan et al., 2017] cung cấp FedAvg được sử dụng rộng rãi để giải quyết mục tiêu này thông qua SGD phân tán. Trong mỗi vòng giao tiếp t, máy chủ trung tâm chọn một tập con gồm cK khách hàng và gửi cho họ một bản sao của trọng số toàn cục w^t. Các khách hàng sửa đổi bản sao cục bộ của trọng số w_k^t bằng cách tối thiểu hóa tổn thất thực nghiệm trên tập dữ liệu cục bộ để có được w_k^{t+1} và truyền chúng trở lại máy chủ. Máy chủ cập nhật trọng số toàn cục bằng cách đơn giản lấy trung bình các trọng số của khách hàng:

w^{t+1} = (1/c) Σ(k=1 to c) w_k^{t+1}    (2)

Một số vấn đề phát sinh trong triển khai thực tế. Thứ nhất, các khách hàng phải chịu chi phí giao tiếp do truyền trọng số và chi phí tính toán do tối ưu hóa trọng số cục bộ. Thứ hai, các tập dữ liệu cục bộ không độc lập và phân phối đồng nhất (non-IID) nên trọng số toàn cục phải tổng quát hóa tốt giữa các khách hàng. Một số công trình đã sửa đổi (các) mục tiêu học tập của họ để giải quyết những vấn đề này (xem Phần 6).

### 2.2 Giả thuyết Vé số May mắn

LTH [Frankle and Carbin, 2018] là một hướng mới nổi trong học máy có thể giải quyết các vấn đề nêu trên trong học liên bang. Nó nêu rằng một mạng nơ-ron được khởi tạo ngẫu nhiên chứa một mạng con được gọi là vé trúng khi được huấn luyện một cách riêng biệt, hoạt động tốt như mạng gốc. Một mở rộng tham vọng hơn đến từ [Ramanujan et al., 2020] nêu rằng một mạng được tham số hóa quá mức đủ chứa một vé trúng ở trạng thái được khởi tạo ngẫu nhiên. Hơn nữa, vé trúng này có thể được xác định thông qua việc đóng băng các trọng số tại khởi tạo ngẫu nhiên w_0 và cắt tỉa một tập con các trọng số này để tìm một mạng con thưa thớt. Để cải thiện hiệu suất học tập, [Chen et al., 2022] đề xuất áp dụng một hàm biến đổi U ∈ U cho các trọng số để tối thiểu hóa thêm tổn thất thực nghiệm. Mục tiêu học tập do đó là:

min_{U∈U, ||m||_0=S} L[f(x; U(w_0 ⊙ m)); y]    (3)

trong đó m ∈ {0,1}^d là một siêu mặt nạ nhị phân với mức độ thưa thớt S và cùng chiều với trọng số. Vé trúng được biểu diễn như phép nhân theo từng phần tử của trọng số và siêu mặt nạ, tức là w_0 ⊙ m. Tuy nhiên, việc tối ưu hóa Eq. (3) về mặt tính toán là không thể giải quyết được do chiều lớn của trọng số và không gian biến đổi U. Do đó, họ đề xuất tách rời việc tối ưu hóa thành hai giai đoạn. Giai đoạn đầu tiên là giai đoạn cắt tỉa trong đó một siêu mặt nạ nhị phân phải được tìm để làm thưa thớt mô hình bằng cách tối ưu hóa như sau:

m̂ ∈ min_{||m||_0=S} R(f(x; w_0 ⊙ m))    (4)

trong đó R là một hàm chấm điểm đo lường khả năng của siêu mặt nạ nhị phân m để tách biệt vé trúng khỏi mô hình. Giai đoạn thứ hai là giai đoạn huấn luyện trong đó một biến đổi trọng số được học để tối thiểu hóa tổn thất thực nghiệm:

Û ∈ min_{U∈U} L[f(x; U(w_0 ⊙ m̂)); y]    (5)

### 2.3 Công thức Mục tiêu

Về bản chất, LTH áp dụng triết lý "che mặt là huấn luyện" trong đó một mạng con thưa thớt tối ưu phải được học mà không sửa đổi trọng số. Với hiệu quả giao tiếp và tính toán mà ý tưởng này mang lại (được thảo luận trong các phần tiếp theo), chúng tôi đề xuất áp dụng giả thuyết này cho thiết lập liên bang. Tuy nhiên, không gian biến đổi trọng số U rất rộng lớn. Theo [Chen et al., 2022], chúng tôi giới hạn không gian thành không gian biến đổi lật dấu U_s ⊂ U trong đó một biến đổi U(w; s) = w ⊙ s là phép nhân theo từng phần tử của siêu mặt nạ dấu hiệu s ∈ {-1,+1}^d với trọng số w. Mục tiêu học tập cập nhật của công trình chúng tôi là:

min_{m,s} F(m,s) = Σ(k=1 to K) (n_k/n) L[f(x_k; w_0 ⊙ m ⊙ s); y_k]    (6)

Các phần tiếp theo trình bày chi tiết về thuật toán học liên bang để giải quyết mục tiêu trên.

## 3 Phương pháp

### 3.1 HideNseek

Trong công trình này, chúng tôi đề xuất một thuật toán học liên bang hiệu quả được gọi là HideNseek bằng cách giải quyết việc điều chỉnh liên bang của LTH. Hình 1 mô tả tổng quan về khung làm việc. Như đã đề cập trước đó, quá trình học tập có thể được thực hiện trong hai giai đoạn.

Trong giai đoạn đầu tiên, máy chủ trước tiên thực hiện cắt tỉa tại khởi tạo để tách biệt vé trúng (①). Vì máy chủ không có dữ liệu huấn luyện, một phương pháp cắt tỉa không phụ thuộc dữ liệu được áp dụng. Theo [Tanaka et al., 2020], chúng tôi đo điểm số của dấu hiệu trọng số thông qua tính nổi bật synap của chúng (xem Phần 3.4), và sử dụng cắt tỉa có cấu trúc toàn cục để có hiệu quả phần cứng.

Trong giai đoạn thứ hai, một biến đổi trọng số tối ưu phải được học để tối thiểu hóa tổn thất thực nghiệm theo cách liên bang. Trong mỗi bước huấn luyện t, máy chủ gửi siêu mặt nạ dấu hiệu toàn cục s^t đến các khách hàng được chọn (②) mà khởi tạo một siêu mặt nạ dấu hiệu cục bộ s_k^{t+1}. Các khách hàng sau đó đóng băng trọng số mô hình và tối ưu hóa siêu mặt nạ dấu hiệu cục bộ bằng cách tối thiểu hóa tổn thất thực nghiệm sử dụng Eq. (6) (③). Như vậy, một biến đổi lật dấu được học và gửi trở lại máy chủ (④). Máy chủ sau đó tổng hợp các siêu mặt nạ cục bộ này sử dụng Eq. (8) (⑤).

Sau giai đoạn huấn luyện, mỗi khách hàng nhân siêu mặt nạ dấu hiệu tổng hợp với trọng số của mình để có được mô hình cục bộ cuối cùng (⑥). Thuật toán 1 (trong Phụ lục A) tóm tắt các quy trình với các chi tiết được làm nổi bật được trình bày chi tiết trong các đoạn tiếp theo.

### 3.2 Cá nhân hóa

Lớp đầu ra của mô hình không thể được tối ưu hóa đơn giản cho dấu hiệu của nó vì độ lớn trọng số phải được chia tỷ lệ để huấn luyện ổn định. Do đó, chúng tôi chia mô hình thành các bộ trích xuất đặc trưng bao gồm tất cả các lớp ẩn và bộ phân loại là lớp đầu ra. Các trọng số bị đóng băng và siêu mặt nạ dấu hiệu được học chỉ cho bộ trích xuất đặc trưng trong khi các trọng số của bộ phân loại có thể sửa đổi được, như việc tối ưu hóa dữ liệu cục bộ. Phù hợp với các công trình trước đây [Zhu et al., 2021], công trình của chúng tôi củng cố thêm việc cá nhân hóa giữa các khách hàng.

### 3.3 Biến đổi Lật Dấu

Việc tối ưu hóa siêu mặt nạ dấu hiệu là rất quan trọng đối với việc học trong thuật toán của chúng tôi. Như vậy, chúng tôi xử lý các bước sơ bộ nhất định để đạt được việc tối ưu hóa. Như đã đề cập trước đó, một mô hình có thể được biểu diễn như một hợp thành của các lớp thực hiện các phép toán của trọng số được vector hóa. Ví dụ, hãy xem xét lớp kết nối đầy đủ. Lưu ý rằng số hạng bias đã được bỏ qua để ngắn gọn. Một lớp kết nối đầy đủ l có thể được biểu diễn như y^[l] = (w^[l] ⊙ s^[l])x^[l], trong đó y^[l] ∈ R^i là đầu ra, x^[l] ∈ R^j là đầu vào, và w^[l] ∈ R^{i×j} là trọng số. Do đó, chúng tôi xử lý việc học của một siêu mặt nạ dấu hiệu s^[l] ∈ {-1,+1}^{i×j} với cùng chiều với trọng số.

Tuy nhiên, việc tối ưu hóa SGD truyền thống không thể được áp dụng cho các siêu mặt nạ dấu hiệu do tính chất rời rạc của chúng. Do đó, chúng tôi thực hiện một bộ ước lượng thẳng [Bengio et al., 2013] với một siêu mặt nạ dấu hiệu có giá trị thực ŝ^[l] ∈ R^{i×j}. Trong lượt chuyển tiếp, ŝ được lượng tử hóa sử dụng hàm dấu hiệu từng đoạn:

s_{ij} = sign(ŝ_{ij}) = {+1 nếu ŝ_{ij} ≥ 0; -1 nếu ŝ_{ij} < 0}    (7)

trong đó s_{ij} là một phần tử ở hàng thứ i và cột thứ j của siêu mặt nạ dấu hiệu s. Các gradient của s trong lượt chuyển ngược được tính như ∇_s L = (∇_y L x^T) ⊙ w. Do hàm dấu hiệu không khả vi, việc gán trực tiếp các gradient từ siêu mặt nạ dấu hiệu được lượng tử hóa cho siêu mặt nạ dấu hiệu thực (tức là ∇_ŝ L = ∇_s L) sẽ dẫn đến phương sai gradient lớn [Courbariaux et al., 2016]. Do đó, chúng tôi sử dụng hàm tiếp tuyến hyperbol, được ký hiệu là tanh(·), để xấp xỉ liên tục hàm dấu hiệu cho lượt chuyển ngược, s_{ij} = tanh(ŝ_{ij}). Điều này cho phép chúng tôi tính các gradient của siêu mặt nạ dấu hiệu thực ŝ từ siêu mặt nạ dấu hiệu nhị phân s như ∇_ŝ L = Ψ ⊙ ∇_s L, trong đó Ψ là ma trận gradient của hàm tiếp tuyến hyperbol với các giá trị được tính toán rõ ràng như Ψ_{ij} = (1 - ŝ_{ij}^2).

Vì các khách hàng truyền các siêu mặt nạ dấu hiệu được lượng tử hóa đến máy chủ để có hiệu quả giao tiếp, chúng tôi sử dụng sơ đồ tổng hợp dấu hiệu sau để có được siêu mặt nạ dấu hiệu có giá trị thực tại máy chủ:

ŝ = arctanh(Σ_k (n_k/n) s_k)    (8)

### 3.4 Cắt tỉa Phía Máy chủ

Một khía cạnh quan trọng khác trong HideNseek là giai đoạn cắt tỉa trong đó vé trúng phải được tách biệt khỏi mạng. Theo [Li et al., 2021], chúng tôi sử dụng cắt tỉa một lần tại khởi tạo nhưng thực hiện nó ở phía máy chủ để giảm tải cho các khách hàng. Vì máy chủ không chứa bất kỳ dữ liệu huấn luyện nào, chúng tôi sử dụng phương pháp cắt tỉa lặp không phụ thuộc dữ liệu [Tanaka et al., 2020] trong đó điểm cắt tỉa được xác định dựa trên tính nổi bật synap của trọng số. Do các trọng số bị đóng băng trong quá trình huấn luyện siêu mặt nạ dấu hiệu, chúng tôi đo tính nổi bật synap của dấu hiệu của một trọng số cho trước trong một mô hình với L lớp như sau:

R_{SF}(s_{ij}^{[l]}) = [∏_{h=l+1}^L s^{[h]} ⊙ w^{[h]}]_i s_{ij}^{[l]} w_{ij}^{[l]} [∏_{h=1}^{l-1} s^{[h]} ⊙ w^{[h]}]_j    (9)

Về cơ bản, tính nổi bật synap của dấu hiệu trọng số là tích của tất cả các trọng số nhân với siêu mặt nạ dấu hiệu tương ứng của chúng có dấu hiệu trọng số trong đường dẫn từ lớp đầu vào đến lớp đầu ra. Để thúc đẩy hiệu quả phần cứng hơn nữa, chúng tôi sử dụng cắt tỉa có cấu trúc toàn cục bằng cách chấm điểm các nhóm trọng số theo kênh trong các lớp tích chập và nút trong các lớp kết nối đầy đủ. Điểm cắt tỉa cho kênh thứ i hoặc nút trong một lớp là ||w_i^{[l]} ⊙ R_{SF}^{[l]}||_2. Ngoài ra, chúng tôi giữ một vài lớp đầu tiên và chỉ cắt tỉa từ các lớp sau của mô hình với tỷ lệ cắt tỉa p_r.

### 3.5 Công nghệ Tiên tiến

FedMask [Li et al., 2021] là gần nhất về tinh thần với công trình của chúng tôi. Tuy nhiên, phương pháp cắt tỉa của nó phụ thuộc vào dữ liệu, và do đó, phải được thực hiện trên các thiết bị khách hàng, dẫn đến tăng tải tính toán trên các khách hàng hạn chế tài nguyên. Ngoài ra, không gian biến đổi trọng số của nó bị giới hạn trong siêu mặt nạ nhị phân U_b ⊂ U, trong đó một siêu mặt nạ nhị phân cục bộ m_k ∈ {0,1} được học trong giai đoạn huấn luyện trái ngược với siêu mặt nạ dấu hiệu nhị phân. Thực tế, FedMask thực hiện thêm cắt tỉa không có cấu trúc trong giai đoạn huấn luyện mà không có lợi thế giao tiếp hoặc tính toán. Ngược lại, HideNseek duy trì tất cả các trọng số sau giai đoạn cắt tỉa cho phép khả năng mô hình lớn hơn.

## 4 Thiết lập Thí nghiệm

### 4.1 Tập dữ liệu & Mô hình

Chúng tôi đánh giá HideNseek trên hai ứng dụng, bao gồm phân loại hình ảnh và nhận dạng hoạt động con người, sử dụng tập dữ liệu EMNIST [Caldas et al., 2018] và HAR [Anguita et al., 2013], tương ứng. EMNIST là một nhiệm vụ nhận dạng ký tự viết tay liên quan đến hình ảnh thang độ xám 28×28 thuộc 62 lớp (chữ hoa và chữ thường và chữ số) đã được phân vùng theo người viết. Do đó, mỗi người viết được coi là một khách hàng. Tập dữ liệu HAR bao gồm dữ liệu cảm biến (được làm phẳng thành một vector 1152 giá trị) được tạo ra bởi người dùng thực hiện sáu hành động có thể (tức là các lớp). Để nghiên cứu thêm tác động của tính không đồng nhất thống kê lên hiệu suất, chúng tôi theo các công trình trước đây [Zhu et al., 2021] và mô phỏng dữ liệu Non-IID trên tập dữ liệu MNIST [LeCun et al., 1998] thông qua lấy mẫu Dirichlet Dir(α), trong đó giá trị α nhỏ hơn biểu thị tính không đồng nhất lớn hơn (xem Hình 5 trong Phụ lục B). Chúng tôi sử dụng VGG9 và perceptron đa lớp (MLP) cho các nhiệm vụ phân loại hình ảnh và nhận dạng hoạt động, tương ứng, với cấu hình mô hình (xem Bảng 5 trong Phụ lục C). Chúng tôi kích hoạt cắt tỉa cho bốn lớp tích chập cuối cùng trong VGG9 và hai lớp ẩn đầu tiên trong MLP.

### 4.2 Thực hiện Hệ thống

Chúng tôi thực hiện HideNseek và các đường cơ sở với PyTorch (v1.8.0) [Paszke et al., 2019] trên một máy chủ được trang bị GPU Nvidia RTX 3090 duy nhất. Chúng tôi thử nghiệm với tổng cộng K khách hàng được đặt là 160 và 320 cho tập dữ liệu MNIST và EMNIST và 30 cho tập dữ liệu HAR. Chúng tôi lấy mẫu ngẫu nhiên γ = 10% khách hàng tham gia thực hiện E = 5 epoch cục bộ trong mỗi vòng giao tiếp với tổng cộng 300 vòng cho MNIST và EMNIST và 200 vòng cho HAR. Trọng số và siêu mặt nạ dấu hiệu được khởi tạo sử dụng Kaiming uniform [He et al., 2015] và phân phối đồng đều U(-1,1) tương ứng. Chúng tôi thực hiện cắt tỉa một lần trong 100 lần lặp trong HideNseek (theo [Tanaka et al., 2020]) và một epoch cho FedMask và Signed trên bốn lớp cuối cùng của VGG9 và hai lớp ẩn đầu tiên của MLP với tỷ lệ cắt tỉa p_r = 0,8 (80% trọng số được giữ lại). Chúng tôi sử dụng bộ tối ưu hóa SGD với tốc độ học η = 0,001 cho FedAvg, FedMask và Signed, η = 0,01 cho BNNAvg và η = 10 cho HideNseek, và momentum μ = 0,9 cho tất cả các thuật toán được chọn thực nghiệm. Chúng tôi lặp lại mỗi thí nghiệm ba lần với các hạt giống khác nhau để có thể tái tạo.

### 4.3 Đường cơ sở

Chúng tôi đánh giá HideNseek bằng cách so sánh hiệu suất của nó với một số đường cơ sở. Chúng tôi bao gồm FedAvg [McMahan et al., 2017] để nhận ra hiệu suất của mô hình khi được huấn luyện ở khả năng đầy đủ. FedMask [Li et al., 2021] gần nhất về tinh thần với công trình của chúng tôi và là công nghệ tiên tiến khi áp dụng LTH cho thiết lập liên bang với cắt tỉa phía khách hàng và học các siêu mặt nạ nhị phân. Chúng tôi cũng mượn đường cơ sở BNNAvg của họ áp dụng FedAvg để huấn luyện mạng nơ-ron nhị phân (BNN) [Courbariaux et al., 2016] với trọng số và kích hoạt được lượng tử hóa bằng dấu hiệu của chúng. Chúng tôi cũng thực hiện một mở rộng của FedMask mà chúng tôi gọi là Signed trong đó chúng tôi thay thế siêu mặt nạ nhị phân bằng siêu mặt nạ dấu hiệu và thay đổi hàm nhị phân hóa của họ từ sigmoid thành tanh.

## 5 Kết quả

### 5.1 Hiệu suất huấn luyện

Trước tiên, chúng tôi so sánh hiệu suất huấn luyện bằng cách báo cáo độ chính xác suy luận trong Bảng 1 giữa HideNseek và các đường cơ sở. Độ chính xác suy luận được đo bằng cách lấy trung bình có trọng số của độ chính xác suy luận của khách hàng dựa trên dữ liệu kiểm tra cục bộ của họ, được tính trọng số dựa trên số lượng mẫu kiểm tra trong tập dữ liệu cục bộ của họ. Trong khi HideNseek hoạt động kém hơn so với FedAvg một cách mong đợi vì FedAvg huấn luyện mô hình đầy đủ và phục vụ như giới hạn trên trong hiệu suất huấn luyện, HideNseek nói chung vượt trội hơn FedMask, Signed và BNNAvg trên các nhiệm vụ. Đáng chú ý rằng cải thiện hiệu suất là đáng kể cho tập dữ liệu HAR và MNIST với tính không đồng nhất thấp hơn tại α = {1,10} với độ chính xác suy luận cao hơn 24,1-40,6% cho HideNseek so với FedMask. Hiệu suất HideNseek giảm dần cho MNIST (α = 0,1) với tính không đồng nhất cao hơn và EMNIST với số lượng lớp lớn. Phát hiện này có thể được quy cho thực tế rằng HideNseek sử dụng bộ trích xuất đặc trưng toàn cục được chia sẻ giữa các đường cơ sở sử dụng cắt tỉa. Trong khi việc học các đặc trưng tổng quát giữa các khách hàng là thách thức, hiệu suất vẫn gần với cả FedMask và Signed học một bộ trích xuất đặc trưng được cá nhân hóa hơn. Tuy nhiên, HideNseek vẫn ghi điểm cao hơn FedMask 2,09% và 19,62% cho EMNIST và MNIST (α = 0,1).

Chúng tôi so sánh thêm hiệu suất huấn luyện bằng cách vẽ biểu đồ độ chính xác suy luận theo vòng giao tiếp. Trong trường hợp tập dữ liệu HAR trong Hình 2(a) và tập dữ liệu MNIST trong Hình 3, HideNseek hội tụ nhanh hơn FedMask, Signed và BNNAvg. Trong khi HideNseek gặp phải biến động cao hơn trong huấn luyện so với các đường cơ sở trong tập dữ liệu EMNIST trong Hình 2(b) và đến một mức độ nào đó trong tập dữ liệu MNIST với tính không đồng nhất cao hơn tại α = 0,1 trong Hình 3(a). Cả hai đều lại chỉ ra khó khăn trong việc huấn luyện một siêu mặt nạ dấu hiệu toàn cục được chia sẻ dưới điều kiện không đồng nhất.

### 5.2 Chi phí Giao tiếp

Sau đó, chúng tôi so sánh chi phí giao tiếp cho mỗi khách hàng bằng cách đo kích thước tải lên và tải xuống tính bằng MB cho mỗi khách hàng trong mỗi vòng giao tiếp như được thể hiện trong Bảng 2. Thứ nhất, BNNAvg với các tham số nhị phân nhỏ hơn FedAvg bốn lần vì 1 byte là kích thước phần tử nhỏ nhất để biểu diễn một tham số trong PyTorch. FedMask và Signed có chi phí tải lên thấp hơn so với BNNAvg do cắt tỉa phía khách hàng. HideNseek giảm thêm chi phí tải xuống nhờ cắt tỉa phía máy chủ. Hơn nữa, điểm cắt tỉa chi tiết hơn trong HideNseek so với FedMask và Signed. Điều này dẫn đến các mạng con nhỏ hơn vì chúng tôi loại bỏ tất cả các trọng số có điểm bằng ngưỡng (xem dòng 15 trong Thuật toán 1). Nhìn chung, HideNseek chứng minh việc giảm chi phí giao tiếp so với hiệu suất tốt thứ hai (FedMask) từ 20,9-39,7% trên tất cả các nhiệm vụ.

### 5.3 Chi phí Tính toán

Chúng tôi báo cáo chi phí tính toán bằng cách đo thời gian huấn luyện tổng cộng trên một GPU Nvidia RTX 3090 duy nhất (Bảng 3). FedAvg nhanh nhất chủ yếu vì nó không sử dụng bất kỳ trọng số tiềm ẩn nào, như các mặt nạ trong trường hợp Signed, FedMask và HideNseek. Ngay cả BNNAvg cũng chậm hơn FedAvg vì việc lượng tử hóa phải được thực hiện tại thời gian chạy và không có tối ưu hóa tích hợp trong PyTorch khi xử lý các tham số 1-bit. Tuy nhiên, HideNseek có thời gian huấn luyện thấp hơn so với FedMask và Signed vì các đường cơ sở này yêu cầu cắt tỉa một lần phía khách hàng mỗi khi có khách hàng mới tham gia huấn luyện. Ngoài ra, FedMask tốn kém tính toán hơn vì nó sử dụng số hạng điều hòa thưa thớt. Nhìn chung, HideNseek chứng minh việc giảm chi phí tính toán so với FedMask từ 22,8-46,8% trên tất cả các nhiệm vụ.

### 5.4 Khách hàng Hoạt động

Bây giờ chúng tôi đánh giá tác động của số lượng khách hàng hoạt động trên mỗi vòng giao tiếp lên hiệu suất huấn luyện. Bảng 4 chứng minh độ chính xác suy luận giữa các khách hàng trên MNIST (α = 1) với số lượng khách hàng hoạt động khác nhau K = {10,20,40}. Trong khi hầu hết các đường cơ sở trải qua cải thiện độ chính xác với nhiều khách hàng hoạt động hơn, HideNseek trải qua giảm nhẹ 3,92% về hiệu suất khi K tăng gấp bốn. Tuy nhiên, HideNseek vẫn chứng minh hiệu suất tốt hơn so với BNNAvg, FedMask và Signed với biên độ đáng kể. Điều này thể hiện tính mở rộng và một phần ủng hộ tính mạnh mẽ của tính không đồng nhất được chứng minh trong Hình 3.

### 5.5 Tỷ lệ Cắt tỉa

Từ các kết quả thảo luận ở trên, rõ ràng rằng mô hình VGG9 được tham số hóa quá mức cho tập dữ liệu MNIST được chứng minh bởi độ chính xác suy luận cao của FedAvg và HideNseek. Do đó, chúng tôi thử một tỷ lệ cắt tỉa mạnh mẽ hơn đáng kể là p_r = 0,2 so với các thí nghiệm trước đây trong đó p_r = 0,8. Như được thể hiện trong Hình 4, sự giảm hiệu suất rất nhỏ trong các tập dữ liệu ít không đồng nhất α = {1,10}, trong khi có sự giảm đáng kể trong tập dữ liệu không đồng nhất hơn α = 0,1. Điều này chứng minh rằng lợi thế tính toán và giao tiếp của HnS so với các đường cơ sở lớn hơn so với các kết quả trước đây mà không có sự giảm độ chính xác đáng chú ý trong một số trường hợp khi sử dụng tỷ lệ cắt tỉa cao.

## 6 Công trình Liên quan

**Tính Không đồng nhất Thống kê.** Sau công trình tiên phong về học liên bang [McMahan et al., 2017], những tiến bộ ngay lập tức tìm cách giải quyết vấn đề tính không đồng nhất thống kê trong học liên bang bằng cách áp dụng các phương án cá nhân hóa. PerFedAvg [Fallah et al., 2020] tích hợp phương pháp học meta không phụ thuộc mô hình vào FedAvg để cá nhân hóa. MOCHA [Smith et al., 2017] giới thiệu học đa nhiệm vụ liên bang trong đó mỗi khách hàng được coi như một nhiệm vụ. Nhiều công trình [Lin et al., 2020; Gong et al., 2021; Zhu et al., 2021] cũng đã áp dụng chưng cất kiến thức để học một mô hình đại diện toàn cục dạy các mô hình cục bộ của khách hàng. [Li et al., 2021] thực hiện cá nhân hóa bằng cách cho phép mỗi khách hàng học một siêu mặt nạ nhị phân cục bộ. Ngược lại, chúng tôi sử dụng cá nhân hóa bằng cách chia sẻ toàn cục tất cả các lớp ẩn của mô hình trong khi tinh chỉnh lớp cuối cùng cho dữ liệu cục bộ của khách hàng. Điều này cho phép HideNseek huấn luyện mô hình một cách ổn định bằng cách sửa đổi độ lớn trọng số cho một tập con nhỏ của trọng số trong khi lượng tử hóa các cập nhật được truyền cho tất cả các lớp ẩn. Như vậy, HideNseek giảm chi phí giao tiếp trong khi duy trì khả năng tốt hơn về học dữ liệu với tính không đồng nhất khác nhau như được thể hiện trong Hình 3.

**Chi phí Giao tiếp và Tính toán.** Một vấn đề đáng kể khác trong học liên bang là chi phí giao tiếp và tính toán tăng lên trên các thiết bị khách hàng khi tối ưu hóa và truyền trọng số. FedProx [Li et al., 2020] giảm thiểu vấn đề này thông qua việc cho phép gián đoạn huấn luyện và cập nhật từng phần, và FedPAQ [Reisizadeh et al., 2020] cho phép lấy trung bình định kỳ và lượng tử hóa cập nhật mô hình. Một số công trình cũng đã giới thiệu các biến thể của cắt tỉa và dropout [Diao et al., 2021; Horvath et al., 2021; Bouacida et al., 2021] cho nén mô hình. Ví dụ, FedMask áp dụng LTH [Frankle and Carbin, 2018] bằng cách thực hiện cắt tỉa một lần ở phía khách hàng và học một siêu mặt nạ nhị phân cục bộ được lượng tử hóa trong giao tiếp. Tuy nhiên, siêu mặt nạ nhị phân được học về cơ bản là cắt tỉa không có cấu trúc không có lợi thế tính toán và hạn chế khả năng mô hình. Do đó, chúng tôi thay thế siêu mặt nạ nhị phân bằng siêu mặt nạ dấu hiệu để hội tụ nhanh hơn và sử dụng cắt tỉa không phụ thuộc dữ liệu tại máy chủ để giảm tải tính toán cho khách hàng.

## 7 Kết luận & Công việc Tương lai

Trong công trình này, chúng tôi đã giới thiệu HideNseek áp dụng giả thuyết vé số may mắn dưới thiết lập liên bang bằng cách tối ưu hóa dấu hiệu của một mạng con nổi bật synap của mô hình. Để giảm thêm tải tính toán cho khách hàng, chúng tôi thực hiện cắt tỉa một lần tại khởi tạo ở phía máy chủ sử dụng phương pháp không phụ thuộc dữ liệu và tối ưu hóa siêu mặt nạ dấu hiệu được lượng tử hóa khi truyền cập nhật mô hình. Kết quả thực nghiệm cho thấy HideNseek chứng minh độ chính xác suy luận tốt hơn so với công nghệ tiên tiến nói chung trong khi giảm đáng kể chi phí giao tiếp và thời gian huấn luyện. Tuy nhiên, một thách thức sắp tới là chi phí bộ nhớ phát sinh do sử dụng các bộ ước lượng thẳng là đáng kể. Do đó, trong tương lai, chúng tôi sẽ khám phá hiệu quả mang lại bởi việc sử dụng bộ tối ưu hóa nhị phân [Helwegen et al., 2019] chỉ sửa đổi dấu hiệu của trọng số mà không cần các tham số tiềm ẩn như siêu mặt nạ dấu hiệu.

## 8 Tác động Rộng lớn

Trong công trình này, chúng tôi đề xuất một thuật toán trong lĩnh vực học liên bang bắt nguồn từ nhu cầu phát triển các ứng dụng học sâu sau những tiến bộ gần đây trong các quy định bảo vệ dữ liệu như GDPR [Viorescu et al., 2017]. Hơn nữa, chúng tôi khám phá một phương pháp để giảm chi phí giao tiếp và tính toán trên các thiết bị di động chạy bằng pin để giảm tác động môi trường. Trong khi công trình của chúng tôi chứng minh các tác động tiết kiệm năng lượng từ quan điểm lý thuyết, chúng tôi hy vọng các công trình tương lai sẽ nghiên cứu sâu hơn về tối ưu hóa hệ thống hướng đến bảo tồn năng lượng.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc]

## Phụ lục

### A Thuật toán

Thuật toán 1 tóm tắt quy trình huấn luyện của HideNseek. Lưu ý rằng 1 trong dòng 15 là một hàm ngưỡng (trái ngược với ma trận đơn vị trong Phương trình 9).

### B Mô phỏng Dữ liệu Non-IID

Hình 5 mô tả tác động của tham số α lên phân phối nhãn giữa các khách hàng khi sử dụng lấy mẫu Dirichlet để phân vùng tập dữ liệu MNIST theo cách Non-IID.

### C Cấu hình Mô hình

Cấu hình mô hình được sử dụng trong công trình này được mô tả trong Bảng 5. Đối với VGG9, mỗi ConvBlock(N) bao gồm một lớp tích chập với N kênh có kích thước 3×3, một lớp BatchNorm theo sau là kích hoạt ReLU. Mỗi lớp MaxPool2d có kích thước kernel và độ dài stride là 2. Số lượng nút trong lớp kết nối đầy đủ cuối cùng là 62 hoặc 10 tùy thuộc vào tập dữ liệu EMNIST hoặc MNIST.
