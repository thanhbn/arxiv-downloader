# Giải quyết sự không đồng nhất về dữ liệu và thiết bị đan xen trong học tập liên kết với độ trễ không giới hạn
Haoming Wang và Wei Gao
Đại học Pittsburgh
hw.wang, weigao@pitt.edu

Tóm tắt
Học tập liên kết (FL) có thể bị ảnh hưởng bởi sự không đồng nhất về dữ liệu và thiết bị, gây ra bởi các phân phối dữ liệu cục bộ khác nhau của khách hàng và độ trễ trong việc tải lên các cập nhật mô hình (tức là độ trễ). Các phương án truyền thống coi những sự không đồng nhất này là hai khía cạnh riêng biệt và độc lập, nhưng giả định này không thực tế trong các tình huống FL thực tế nơi những sự không đồng nhất này đan xen. Trong những trường hợp này, các phương án FL truyền thống không hiệu quả, và một cách tiếp cận tốt hơn là chuyển đổi một cập nhật mô hình bị trễ thành một cập nhật không bị trễ. Trong bài báo này, chúng tôi trình bày một khung FL mới đảm bảo độ chính xác và hiệu quả tính toán của việc chuyển đổi này, do đó giải quyết hiệu quả các sự không đồng nhất đan xen có thể gây ra độ trễ không giới hạn trong các cập nhật mô hình. Ý tưởng cơ bản của chúng tôi là ước tính phân phối dữ liệu huấn luyện cục bộ của khách hàng từ các cập nhật mô hình bị trễ mà họ tải lên, và sử dụng những ước tính này để tính toán các cập nhật mô hình khách hàng không bị trễ. Theo cách này, phương pháp của chúng tôi không yêu cầu bất kỳ tập dữ liệu phụ trợ nào hoặc các mô hình cục bộ của khách hàng phải được huấn luyện đầy đủ, và không phát sinh bất kỳ chi phí tính toán hoặc giao tiếp bổ sung nào tại các thiết bị khách hàng. Chúng tôi đã so sánh phương pháp của mình với các chiến lược FL hiện có trên các tập dữ liệu và mô hình chính, và cho thấy rằng phương pháp của chúng tôi có thể cải thiện độ chính xác mô hình được huấn luyện lên đến 25% và giảm số epoch huấn luyện cần thiết lên đến 35%. Mã nguồn có thể được tìm thấy tại: https://github.com/pittisl/FL-with-intertwined-heterogeneity .

1 Giới thiệu
Học tập liên kết (FL) McMahan [2016] có thể bị ảnh hưởng bởi cả sự không đồng nhất về dữ liệu và thiết bị. Sự không đồng nhất về dữ liệu là sự không đồng nhất của các phân phối dữ liệu non-i.i.d. trên các khách hàng khác nhau, làm cho mô hình toàn cục được tổng hợp bị thiên lệch và giảm độ chính xác mô hình Konečný [2016], Zhao [2018]. Sự không đồng nhất về thiết bị phát sinh từ độ trễ khác nhau của khách hàng trong việc tải lên các cập nhật mô hình cục bộ của họ lên máy chủ, do các điều kiện tài nguyên cục bộ khác nhau của họ (ví dụ: sức mạnh tính toán, tốc độ liên kết mạng, v.v.). Một giải pháp trực quan cho sự không đồng nhất thiết bị là FL không đồng bộ, không chờ các khách hàng chậm mà cập nhật mô hình toàn cục bất cứ khi nào nhận được cập nhật từ khách hàng Xie và Gupta. [2019]. Trong trường hợp này, nếu độ trễ quá mức của khách hàng chậm dài hơn một epoch huấn luyện, nó sẽ sử dụng mô hình toàn cục lỗi thời để tính toán cập nhật mô hình của mình, cập nhật này sẽ bị trễ khi được tổng hợp tại máy chủ và ảnh hưởng đến độ chính xác mô hình. Để giải quyết độ trễ, có thể sử dụng tổng hợp có trọng số để áp dụng trọng số giảm trên các cập nhật mô hình bị trễ Chen và Jin. [2019], Wang [2022].

Hầu hết công việc hiện có coi sự không đồng nhất về dữ liệu và thiết bị là hai khía cạnh riêng biệt và độc lập trong FL Zhou [2021]. Tuy nhiên, giả định này không thực tế trong nhiều tình huống FL nơi hai sự không đồng nhất này đan xen: dữ liệu trong các lớp nhất định hoặc với các đặc điểm cụ thể có thể chỉ có sẵn tại một số khách hàng chậm. Ví dụ, trong FL để cứu hộ thảm họa Ahmed et al. [2020], chỉ các thiết bị tại các khu vực thảm họa mới có dữ liệu quan trọng về thảm họa, nhưng chúng thường có khả năng kết nối hoặc sức mạnh tính toán hạn chế để tải lên các cập nhật mô hình kịp thời. Các tình huống tương tự cũng có thể xảy ra trong các tình huống FL nơi dữ liệu có tầm quan trọng cao đối với độ chính xác mô hình là khan hiếm và khó thu thập, chẳng hạn như đánh giá bệnh tật trong sức khỏe thông minh, nơi chỉ có ít bệnh nhân có triệu chứng nghiêm trọng nhưng rất có khả năng báo cáo triệu chứng với độ trễ dài do tình trạng của họ xấu đi Chen et al. [2017].

Trong những trường hợp này, nếu trọng số giảm được áp dụng cho các cập nhật mô hình bị trễ từ các khách hàng chậm, kiến thức quan trọng trong những cập nhật này sẽ không được học đầy đủ và do đó ảnh hưởng đến độ chính xác mô hình. Thay vào đó, một cách tiếp cận tốt hơn là tổng hợp đều tất cả các cập nhật mô hình và chuyển đổi một cập nhật mô hình bị trễ thành một cập nhật không bị trễ, nhưng các kỹ thuật hiện có cho việc chuyển đổi như vậy chỉ giới hạn ở một lượng nhỏ độ trễ. Ví dụ, bù trừ bậc nhất có thể được áp dụng trên độ trễ gradient Zheng et al. [2017], bằng cách giả định độ trễ trong FL nhỏ hơn một epoch để bỏ qua tất cả các hạng tử bậc cao trong sự khác biệt giữa các cập nhật mô hình bị trễ và không bị trễ Zhou và Lv. [2021]. Tuy nhiên, trong các tình huống FL được đề cập ở trên, việc chứng kiến độ trễ quá mức hoặc thậm chí không giới hạn là phổ biến, và các thí nghiệm của chúng tôi cho thấy lỗi bù trừ sẽ nhanh chóng tăng theo độ trễ.

Để giải quyết hiệu quả các sự không đồng nhất về dữ liệu và thiết bị đan xen với độ trễ không giới hạn, trong bài báo này chúng tôi trình bày một khung FL mới sử dụng đảo ngược gradient tại máy chủ để chuyển đổi các cập nhật mô hình bị trễ, bằng cách mô phỏng các gradient của mô hình cục bộ được tạo ra với dữ liệu huấn luyện ban đầu của chúng Zhu và Han. [2019]. Máy chủ tính toán ngược các gradient từ các cập nhật mô hình bị trễ của khách hàng để thu được ước tính phân phối dữ liệu huấn luyện của khách hàng, sao cho một mô hình được huấn luyện với phân phối dữ liệu ước tính sẽ thể hiện bề mặt mất mát tương tự như khi sử dụng dữ liệu huấn luyện ban đầu của khách hàng. Máy chủ sử dụng các phân phối dữ liệu ước tính như vậy để huấn luyện lại mô hình toàn cục hiện tại, như các ước tính của các cập nhật mô hình khách hàng không bị trễ. So với các phương pháp chuyển đổi mô hình khác, chẳng hạn như huấn luyện một mô hình sinh bổ sung Yang [2019] hoặc tối ưu hóa dữ liệu đầu vào với các ràng buộc Yin [2020], phương pháp của chúng tôi có những ưu điểm sau:

• Phương pháp của chúng tôi giữ nguyên quy trình FL của khách hàng không thay đổi, và do đó không phát sinh bất kỳ chi phí tính toán hoặc giao tiếp bổ sung nào tại các thiết bị khách hàng, thường có khả năng yếu trong các tình huống FL.

• Phương pháp của chúng tôi không yêu cầu bất kỳ tập dữ liệu phụ trợ nào hoặc các mô hình cục bộ của khách hàng phải được huấn luyện đầy đủ, và do đó có thể được áp dụng rộng rãi cho các tình huống FL thực tế.

• Trong phương pháp của chúng tôi, máy chủ sẽ không thể khôi phục bất kỳ mẫu hoặc nhãn ban đầu nào của dữ liệu huấn luyện cục bộ của khách hàng và nó không thể tạo ra bất kỳ thông tin có thể nhận dạng nào về dữ liệu cục bộ của khách hàng. Do đó, phương pháp của chúng tôi không làm tổn hại quyền riêng tư dữ liệu của khách hàng.

Chúng tôi đã đánh giá kỹ thuật đề xuất của mình bằng cách so sánh với các phương án FL chính trên nhiều tập dữ liệu và mô hình. Kết quả thí nghiệm cho thấy rằng khi giải quyết các sự không đồng nhất về dữ liệu và thiết bị đan xen với độ trễ không giới hạn, kỹ thuật của chúng tôi có thể cải thiện đáng kể độ chính xác mô hình được huấn luyện lên đến 25% và giảm lượng epoch huấn luyện cần thiết lên đến 35%. Vì các khách hàng trong FL cần tính toán và tải lên các cập nhật mô hình lên máy chủ trong mỗi epoch huấn luyện, việc giảm các epoch huấn luyện như vậy giảm đáng kể chi phí tính toán và giao tiếp tại các khách hàng.

2 Nền tảng và động lực

Trong phần này, chúng tôi trình bày các kết quả sơ bộ chứng minh sự không hiệu quả của các phương pháp hiện có trong việc giải quyết các sự không đồng nhất về dữ liệu và thiết bị đan xen, do đó tạo động lực cho phương pháp đề xuất của chúng tôi sử dụng đảo ngược gradient.

2.1 Giải quyết các sự không đồng nhất đan xen trong FL

FL không có độ trễ
Tổng hợp trực tiếp
Tổng hợp có trọng số

Hình 1: Tác động của độ trễ trong FL

Hầu hết các giải pháp hiện có cho độ trễ trong AFL đều dựa trên tổng hợp có trọng số Chen và Jin. [2019], Wang [2022], Chen [2020]. Ví dụ, [7] gợi ý rằng trọng số của một cập nhật mô hình giảm theo cấp số nhân với lượng độ trễ của nó, và một số khác sử dụng các thước đo độ trễ khác nhau để quyết định trọng số của các cập nhật mô hình Wang [2022]. Chen [2020] quyết định những trọng số này dựa trên một thuật toán học đặc trưng. Tuy nhiên, các giải pháp hiện có này thiên vị không đúng cách đối với các khách hàng nhanh, và do đó sẽ ảnh hưởng đến độ chính xác của mô hình được huấn luyện khi các sự không đồng nhất về dữ liệu và thiết bị trong FL đan xen, vì chúng sẽ bỏ lỡ kiến thức quan trọng trong các cập nhật mô hình của khách hàng chậm.

Để chứng minh điều này, chúng tôi đã tiến hành các thí nghiệm sử dụng một tập dữ liệu hình ảnh thảm họa thực tế Mouzannar et al. [2018], chứa 6k hình ảnh của 5 lớp thảm họa (ví dụ: hỏa hoạn và lũ lụt) với các mức độ thiệt hại nghiêm trọng khác nhau. Trong FL với 100 khách hàng, chúng tôi thiết lập sự không đồng nhất dữ liệu là mỗi khách hàng chỉ chứa các mẫu trong một lớp dữ liệu, và thiết lập sự không đồng nhất thiết bị là độ trễ 100 epoch trên 15 khách hàng với hình ảnh thiệt hại nghiêm trọng. Khi sử dụng tập dữ liệu này để tinh chỉnh mô hình ResNet18 được pre-train, kết quả trong Hình 1 cho thấy rằng độ trễ dẫn đến suy giảm lớn độ chính xác mô hình, và tổng hợp có trọng số dẫn đến độ chính xác thậm chí thấp hơn so với tổng hợp trực tiếp, vì đóng góp từ hình ảnh thiệt hại nghiêm trọng trên các khách hàng bị trễ được giảm bởi các trọng số.

Mặt khác, nếu chúng ta tăng đóng góp từ các khách hàng bị trễ bằng cách sử dụng trọng số lớn hơn, mặc dù độ chính xác mô hình trên những hình ảnh thiệt hại nghiêm trọng này sẽ cải thiện, các trọng số lớn hơn sẽ khuếch đại tác động của các lỗi có trong các cập nhật mô hình bị trễ và do đó ảnh hưởng đến độ chính xác tổng thể của mô hình trong các lớp dữ liệu khác. Kết quả chi tiết có thể được tìm thấy trong Phụ lục B.

Trong các tình huống thực tế như thiên tai, độ trễ lớn hoặc không giới hạn như vậy là phổ biến do gián đoạn giao tiếp tại các khu vực thảm họa, và độ trễ quá lớn để máy chủ chờ bất kỳ khách hàng chậm nào. Sự suy giảm hiệu suất lớn của tổng hợp có trọng số, khi đó, tạo động lực cho chúng ta thay vào đó chuyển đổi các cập nhật mô hình bị trễ thành những cập nhật không bị trễ.

Công việc hiện có duy nhất về việc chuyển đổi như vậy, theo hiểu biết tốt nhất của chúng tôi, sử dụng khai triển Taylor bậc nhất để bù trừ các lỗi trong các cập nhật mô hình bị trễ Zheng et al. [2017]. Đối với một cập nhật bị trễ g(wt−τ), cập nhật không bị trễ ước tính là:

g(wt)≈g(wt−τ) +∇g(wt−τ)(wt−wt−τ). (1)

Vì ma trận Hessian ∇g(wt−τ) khó tính toán cho các mạng nơ-ron, nó được xấp xỉ là
∇g(wt−τ)≈λ·g(wt−τ)⊙g(wt−τ) (2)

trong đó λ là một siêu tham số thực nghiệm. Tuy nhiên, phương pháp này chỉ có thể áp dụng cho lượng nhỏ độ trễ Zhou et al. [2021], Li et al. [2023], Tian et al. [2021], trong đó các hạng tử bậc cao trong khai triển Taylor có thể bỏ qua được. Để xác minh điều này, chúng tôi sử dụng cùng thiết lập thí nghiệm như trên và thay đổi lượng độ trễ từ 0 đến 50 epoch. Như được hiển thị trong Bảng 1, lỗi gây ra bởi các hạng tử bậc cao trong khai triển Taylor, được đo bằng khoảng cách cosine và sự khác biệt L1-norm với các cập nhật mô hình không bị trễ, đều tăng đáng kể khi độ trễ tăng. Những kết quả này tạo động lực cho chúng ta thiết kế các kỹ thuật đảm bảo chuyển đổi chính xác với độ trễ không giới hạn.

Độ trễ (epoch) 5 10 20 50
Lỗi Cos-dist 0.08 0.22 0.33 0.53
Lỗi L1-norm 0.009 0.018 0.31 0.052

Bảng 1: Lỗi gây ra bởi các hạng tử bậc cao trong khai triển Taylor khi sử dụng Zheng et al. [2017]

2.2 Đảo ngược gradient

Phương pháp đề xuất của chúng tôi được xây dựng dựa trên các kỹ thuật hiện có của đảo ngược gradient Zhu và Han. [2019], khôi phục dữ liệu huấn luyện ban đầu từ gradient của một mô hình được huấn luyện. Ý tưởng cơ bản của nó là giảm thiểu sự khác biệt giữa gradient của mô hình được huấn luyện và gradient được tính toán từ dữ liệu được khôi phục. Ký hiệu một batch dữ liệu huấn luyện là (x, y) trong đó x biểu thị dữ liệu đầu vào và y biểu thị nhãn, đảo ngược gradient giải quyết bài toán tối ưu hóa sau:

(x′∗, y′∗) = arg min (x′,y′)∥∂L[(x′, y′);wt−1]∂wt−1−gt∥²₂, (3)

trong đó (x′, y′) là dữ liệu được khôi phục, wt−1 là mô hình được huấn luyện, L[·] là hàm mất mát của mô hình, và gt là gradient được tính toán với dữ liệu huấn luyện và wt−1. Bài toán này có thể được giải quyết bằng gradient descent để cập nhật lặp (x′, y′).

Chất lượng dữ liệu được khôi phục liên quan đến số lượng mẫu dữ liệu được khôi phục. Khôi phục một tập dữ liệu lớn hơn sẽ làm rối kiến thức đã học qua các mẫu dữ liệu khác nhau và giảm chất lượng dữ liệu được khôi phục, và các phương pháp hiện có chỉ giới hạn ở việc khôi phục một batch nhỏ (<48) mẫu dữ liệu Yin [2021], Geiping [2020], Zhao và Bilen. [2020]. Tuy nhiên, hạn chế này mâu thuẫn với kích thước điển hình của tập dữ liệu khách hàng trong FL, thường lớn hơn hàng trăm mẫu Wu et al. [2023], Reddi et al. [2020]. Hạn chế này chỉ ra rằng chúng ta có thể sử dụng đảo ngược gradient để ước tính phân phối dữ liệu huấn luyện của khách hàng mà không tiết lộ các mẫu riêng lẻ của dữ liệu cục bộ khách hàng.

[Hình 2: Phương pháp đề xuất của chúng tôi để giải quyết các sự không đồng nhất về dữ liệu và thiết bị đan xen trong FL]

3 Phương pháp

Trong bài báo này, chúng tôi xem xét một tình huống FL bán không đồng bộ nơi một số khách hàng bình thường tuân theo FL đồng bộ và một số khách hàng chậm cập nhật không đồng bộ Chai [2021]. Trong trường hợp này, chúng tôi đo độ trễ bằng số epoch mà các cập nhật của khách hàng chậm bị trễ. Tại thời điểm t, một khách hàng bình thường i cung cấp cập nhật mô hình của nó như

wᵗᵢ=LocalUpdate (wᵗglobal ;Di), (4)

trong đó LocalUpdate [·] là chương trình huấn luyện cục bộ của khách hàng i, sử dụng mô hình toàn cục hiện tại wᵗglobal và tập dữ liệu cục bộ Di của khách hàng i để tạo ra wᵗᵢ. Khi cập nhật mô hình của khách hàng i bị trễ, máy chủ sẽ nhận được một cập nhật mô hình bị trễ từ i tại thời điểm t như

wᵗ⁻τᵢ=LocalUpdate (wᵗ⁻τglobal;Di), (5)

trong đó lượng độ trễ được chỉ ra bởi τ và wᵗ⁻τᵢ được tính toán từ một mô hình toàn cục lỗi thời wᵗ⁻τglobal.

Do các sự không đồng nhất về dữ liệu và thiết bị đan xen, chúng tôi coi rằng wᵗ⁻τᵢ được nhận chứa kiến thức độc đáo về Di chỉ có sẵn từ khách hàng i, và kiến thức như vậy nên được kết hợp đầy đủ vào mô hình toàn cục. Để làm như vậy, như được hiển thị trong Hình 2, máy chủ sử dụng đảo ngược gradient được mô tả trong Eq. (3) để khôi phục một tập dữ liệu trung gian Drec từ wᵗ⁻τᵢ. Khác với công việc hiện có của đảo ngược gradient Zhu và Han. [2019] nhằm khôi phục đầy đủ dữ liệu huấn luyện Di của khách hàng i, chúng tôi chỉ mong đợi Drec đại diện cho phân phối dữ liệu tương tự với Di.

Sau đó máy chủ tính toán một ước tính của wᵗᵢ từ wᵗ⁻τᵢ, cụ thể là ŵᵗᵢ, bằng cách sử dụng Drec để huấn luyện mô hình toàn cục hiện tại wᵗglobal của nó, và tổng hợp ŵᵗᵢ với các cập nhật mô hình từ các khách hàng khác để cập nhật mô hình toàn cục của nó trong epoch hiện tại. Trong quá trình này, máy chủ chỉ nhận được cập nhật mô hình bị trễ wᵗ⁻τᵢ từ khách hàng i, và chúng tôi đã chứng minh rằng ước tính của máy chủ về phân phối dữ liệu của khách hàng sẽ không tiết lộ bất kỳ thông tin có thể nhận dạng nào về dữ liệu huấn luyện cục bộ của khách hàng, do đó tránh khả năng rò rỉ quyền riêng tư dữ liệu tại khách hàng. Đồng thời, chi phí tính toán tại khách hàng i vẫn giống như trong FL vani, và không cần bất kỳ tính toán bổ sung nào cho việc ước tính ŵᵗᵢ như vậy.

3.1 Ước tính phân phối dữ liệu cục bộ từ các cập nhật mô hình bị trễ

Để tính toán Drec, trước tiên chúng tôi cố định kích thước của Drec và khởi tạo ngẫu nhiên mỗi mẫu dữ liệu và nhãn trong Drec. Sau đó, chúng tôi cập nhật Drec lặp bằng cách giảm thiểu

Disparity [LocalUpdate (wᵗ⁻τglobal;Drec), wᵗ⁻τᵢ], (6)

sử dụng gradient descent, trong đó Disparity [·] là một thước đo để đánh giá wᵗ⁻τᵢ thay đổi như thế nào nếu được huấn luyện lại sử dụng Drec. Trong FL, cập nhật mô hình của khách hàng bao gồm nhiều bước huấn luyện cục bộ thay vì một gradient duy nhất. Do đó, để sử dụng đảo ngược gradient trong FL, chúng tôi thay thế gradient duy nhất được tính toán từ Drec trong Eq. (3) bằng kết quả huấn luyện cục bộ sử dụng Drec. Theo cách này, vì bề mặt mất mát trong không gian trọng số của mô hình được tính toán sử dụng Drec tương tự như sử dụng Di, chúng ta có thể mong đợi một gradient tương tự được tính toán.

Đầu tiên chúng tôi trực quan hóa nó bằng cách sử dụng tập dữ liệu MNIST để huấn luyện mô hình LeNet. Hình 3 cho thấy rằng, bề mặt mất mát được tính toán sử dụng Drec tương tự như sử dụng Di trong vùng lân cận của (wᵗ⁻τglobal), và gradient được tính toán cũng rất tương tự.

[Hình 4: Phương pháp ước tính dựa trên đảo ngược gradient của chúng tôi có lỗi nhỏ hơn so với ước tính bậc nhất]

Để xác minh độ chính xác của việc sử dụng ŵᵗᵢ để ước tính wᵗᵢ, chúng tôi so sánh ước tính này với ước tính bậc nhất, bằng cách tính toán sự khác biệt của chúng với cập nhật mô hình không bị trễ thực sự dưới các lượng độ trễ khác nhau. Kết quả trong Hình 4 cho thấy rằng, so với Bù trừ bậc nhất Zheng et al. [2017], ước tính dựa trên đảo ngược gradient của chúng tôi có thể giảm lỗi ước tính lên đến 50%, đặc biệt khi độ trễ tăng quá mức lên hơn 50 epoch.

Một vấn đề quan trọng khác là cách quyết định kích thước của Drec. Vì đảo ngược gradient tương đương với việc lấy mẫu lại dữ liệu trong phân phối dữ liệu huấn luyện ban đầu, một kích thước đủ lớn của Drec là cần thiết để đảm bảo lấy mẫu dữ liệu không thiên lệch và giảm thiểu đầy đủ mất mát gradient qua các lần lặp. Mặt khác, khi kích thước của Drec quá lớn, chi phí tính toán của mỗi lần lặp sẽ không cần thiết quá cao. Chi tiết thêm về cách quyết định kích thước của Drec trong Phụ lục D. Kết quả thêm về lỗi của phương pháp chúng tôi với các chương trình huấn luyện cục bộ khác nhau cũng có thể được tìm thấy trong Phụ lục E.

3.2 Chuyển về FL vani trong các giai đoạn sau của FL

Như được hiển thị trong Hình 4, ước tính được thực hiện bởi đảo ngược gradient cũng chứa lỗi, vì mất mát đảo ngược gradient không thể được giảm về không. Khi huấn luyện FL tiến triển và mô hình toàn cục hội tụ, sự khác biệt giữa mô hình toàn cục trước đó và hiện tại sẽ giảm về 0, và do đó sự khác biệt giữa các cập nhật mô hình bị trễ và không bị trễ cũng sẽ giảm, cuối cùng về 0. Trong trường hợp này, ở giai đoạn cuối của huấn luyện FL, lỗi trong cập nhật mô hình ước tính của chúng tôi (ŵᵗᵢ) sẽ vượt quá lỗi của cập nhật mô hình bị trễ ban đầu wᵗ⁻τᵢ.

[Hình 5: So sánh lỗi ước tính cập nhật mô hình khi huấn luyện FL tiến triển]

Để xác minh điều này, chúng tôi đã tiến hành các thí nghiệm bằng cách huấn luyện mô hình LeNet với tập dữ liệu MNIST, và đánh giá các giá trị trung bình của E1(t) = Disparity [ŵᵗᵢ;wᵗᵢ] và E2(t) = Disparity [wᵗ⁻τᵢ;wᵗᵢ] qua các khách hàng khác nhau, sử dụng cả khoảng cách cosine và sự khác biệt L1-norm làm thước đo. Kết quả trong Hình 5 cho thấy rằng ở giai đoạn cuối của huấn luyện FL, E2(t) luôn lớn hơn E1(t).

Quyết định điểm chuyển đổi. Do đó, ở giai đoạn cuối của huấn luyện FL, cần thiết phải chuyển về FL vani và trực tiếp sử dụng các cập nhật mô hình bị trễ trong tổng hợp. Khó khăn trong việc quyết định điểm chuyển đổi như vậy là cập nhật mô hình không bị trễ thực sự (wᵗᵢ) không được biết tại thời điểm t. Thay vào đó, máy chủ có khả năng sẽ nhận được wᵗᵢ vào một thời điểm sau, cụ thể là t+τ′. Do đó, nếu chúng ta thấy rằng E1(t)> E2(t) tại thời điểm t+τ′ khi máy chủ nhận được wᵗᵢ tại t+τ′, chúng ta có thể sử dụng t+τ′ làm điểm chuyển đổi thay vì t.

Làm như vậy sẽ dẫn đến độ trễ trong việc chuyển đổi, nhưng kết quả thí nghiệm của chúng tôi trong Bảng 2 và Hình 6 với các điểm chuyển đổi khác nhau cho thấy rằng huấn luyện FL không nhạy cảm với độ trễ như vậy.

Điểm chuyển đổi (epoch) Không 135 155 175
Độ chính xác mô hình 59.3% 68.1% 67.4% 67.5%

Bảng 2: Kết quả huấn luyện FL với các điểm chuyển đổi khác nhau. E1(t)> E2(t) khi t=155, nhưng các điểm chuyển đổi khác nhau thể hiện hiệu suất huấn luyện rất tương tự.

Trong thực tế, khi chúng ta thực hiện việc chuyển đổi như vậy, độ chính xác mô hình trong huấn luyện sẽ trải qua sự giảm đột ngột do tính không nhất quán của gradient giữa ŵᵗᵢ và wᵗ⁻τᵢ. Để tránh sự giảm đột ngột như vậy, tại thời điểm t+τ′, thay vì ngay lập tức chuyển sang sử dụng ŵᵗᵢ trong tổng hợp mô hình của máy chủ, chúng tôi sử dụng trung bình có trọng số γŵᵗᵢ+ (1−γ)wᵗ⁻τᵢ trong tổng hợp, để đảm bảo chuyển đổi mượt mà. γ giảm tuyến tính từ 1 xuống 0 trong một cửa sổ thời gian, và độ dài của cửa sổ này có thể được điều chỉnh linh hoạt để phù hợp với việc tối ưu hóa độ chính xác mô hình. Kết quả thí nghiệm trong Bảng 3 cho thấy rằng, khi độ dài này được đặt ở 10% thời gian huấn luyện trước khi đạt điểm chuyển đổi, độ chính xác mô hình được tối đa hóa.

Thời gian giảm 0% 5% 10% 20%
Độ chính xác mô hình 67.4% 69.0% 70.2% 69.8%

Bảng 3: Thời gian cần thiết để γ giảm từ 1 xuống 0

3.3 Đảo ngược gradient hiệu quả về mặt tính toán

Nguyên tắc thiết kế cơ bản của chúng tôi là giữ nguyên quy trình FL của khách hàng không thay đổi, và chuyển tất cả các tính toán bổ sung phát sinh do đảo ngược gradient sang máy chủ. Theo cách này, chúng ta có thể tập trung vào việc giảm chi phí tính toán của máy chủ trong đảo ngược gradient, được gây ra bởi số lượng lớn các lần lặp liên quan, sử dụng hai phương pháp sau.

Đầu tiên, chúng tôi giảm độ phức tạp của hàm mục tiêu trong đảo ngược gradient bằng cách thưa hóa, chỉ liên quan đến các gradient quan trọng có độ lớn lớn vào các lần lặp của đảo ngược gradient. Công việc hiện có đã xác minh rằng gradient trong các mô hình chính rất thưa và chỉ có ít gradient có độ lớn lớn Lin et al. [2017]. Do đó, chúng tôi sử dụng một mặt nạ nhị phân Mask [·] để chọn các phần tử trong wᵗ⁻τᵢ với K độ lớn hàng đầu và chỉ liên quan đến các phần tử này trong đảo ngược gradient. Như được hiển thị trong Bảng 4, bằng cách chỉ liên quan đến 5% gradient hàng đầu, chúng ta có thể giảm khoảng 80% tính toán được đo bằng số lần lặp trong đảo ngược gradient, với sự gia tăng rất nhỏ trong lỗi ước tính các cập nhật mô hình không bị trễ. Bên cạnh đó, chúng tôi cũng khám phá thêm tác động của lỗi như vậy gây ra bởi thưa hóa đối với độ chính xác mô hình, và kết quả trong Phụ lục F.

Vì trong hầu hết các trường hợp dữ liệu cục bộ của khách hàng vẫn cố định, chúng ta không cần bắt đầu các lần lặp của đảo ngược gradient mỗi lần từ một khởi tạo ngẫu nhiên, mà thay vào đó có thể tối ưu hóa Drec từ những cái được tính toán trong các epoch huấn luyện trước đó. Các thí nghiệm của chúng tôi trong Bảng 5 cho thấy rằng, khi dữ liệu cục bộ của khách hàng vẫn cố định, chúng ta có thể giảm thêm lượng lần lặp trong đảo ngược gradient thêm 43%. Ngay cả khi dữ liệu khách hàng như vậy chỉ một phần cố định (ví dụ: thay đổi 20%), chúng ta vẫn có thể đạt được sự giảm đáng kể của các lần lặp như vậy.

Tỷ lệ thưa hóa 0% 90% 95% 99%
Giảm tính toán (%) 0% 68% 80% 93%
Lỗi ước tính 0.28 0.29 0.31 0.57

Bảng 4: Giảm tính toán và lỗi ước tính các cập nhật mô hình không bị trễ, với các tỷ lệ thưa hóa khác nhau

Lượng dữ liệu thay đổi 0% 5% 20% 50%
Tính toán tiết kiệm 43% 21% 12% 6%

Bảng 5: Số lần lặp trong đảo ngược gradient với các tỷ lệ phần trăm thay đổi khác nhau trong dữ liệu cục bộ của khách hàng

Lưu ý rằng, chúng tôi chỉ áp dụng đảo ngược gradient cho các cập nhật mô hình bị trễ chứa kiến thức độc đáo không có trong các cập nhật mô hình khác. Bên cạnh đó, hầu hết các hệ thống FL Charles et al. [2021] giữ số lượng khách hàng trong một vòng toàn cục không đổi. Khi số lượng như vậy đủ (ví dụ: 10-50 ngay cả đối với FL với hàng nghìn khách hàng), việc tăng thêm số lượng như vậy mang lại ít lợi ích hiệu suất nhưng tăng chi phí và gây ra sự cố huấn luyện thảm khốc Ro et al. [2022]. Do đó, chi phí máy chủ của đảo ngược gradient, ngay cả trong các hệ thống FL quy mô lớn, sẽ không tăng lớn. Khả năng mở rộng như vậy được thảo luận thêm trong Phụ lục G.

3.4 Bảo vệ quyền riêng tư dữ liệu của khách hàng

Mặc dù chúng tôi đã sử dụng đảo ngược gradient để ước tính phân phối dữ liệu cục bộ từ các cập nhật mô hình bị trễ, trong hầu hết các thiết lập FL, sẽ khó khăn hoặc gần như không thể cho máy chủ khôi phục, hoặc dữ liệu cục bộ của khách hàng bị trễ hoặc các nhãn, từ kiến thức về các phân phối như vậy, đặc biệt khi áp dụng phương pháp thưa hóa được mô tả trước đó.

Bảo vệ mẫu dữ liệu. Khó khăn trong việc khôi phục mẫu dữ liệu cục bộ của khách hàng tỷ lệ thuận với kích thước dữ liệu cục bộ của khách hàng và độ phức tạp của huấn luyện cục bộ. Trong FL, dữ liệu huấn luyện cục bộ của khách hàng thường chứa ít nhất hàng trăm mẫu [38], và sự đa dạng cao giữa các mẫu dữ liệu làm cho việc khôi phục chính xác bất kỳ mẫu riêng lẻ nào trở nên khó khăn. Để chứng minh điều này, chúng tôi đã làm thí nghiệm với tập dữ liệu CIFAR-10 và mô hình ResNet-18, và so khớp mỗi mẫu trong Drec với mẫu tương tự nhất trong Di dựa trên điểm tương tự LPIPS của chúng [48]. Như được hiển thị trong Hình 7, những mẫu dữ liệu khớp này rất khác biệt, và các mẫu dữ liệu được khôi phục trong Drec hầu hết không có ý nghĩa trong nhận thức của con người.

[Hình 7: Năm khớp tốt nhất giữa các mẫu trong Drec và Di]

Tuy nhiên, ngay cả trong tình huống dễ nhất nơi tập dữ liệu của khách hàng chỉ chứa một mẫu và huấn luyện cục bộ chỉ là gradient descent một bước, việc khôi phục như vậy vẫn sẽ không thành công.

Cụ thể hơn, mặc dù đảo ngược gradient có thể khôi phục phần lớn pixel của mẫu dữ liệu như được hiển thị trong Hình 8(a) khi không áp dụng thưa hóa, chất lượng khôi phục như vậy nhanh chóng giảm khi áp dụng thưa hóa vừa phải, như được hiển thị trong Hình 8(c) và 8(d). Điều này là do thưa hóa hiệu quả giảm phạm vi kiến thức có sẵn cho đảo ngược gradient để khôi phục dữ liệu. Kết quả trong Bảng 6 với nhiều thước đo chất lượng hình ảnh nhận thức, bao gồm LPIPS Zhang [2018] và FID Heusel et al. [2017], xác minh thêm rằng những hình ảnh được khôi phục như vậy không thể được nhận dạng bằng mắt người. Về cơ bản, khi tỷ lệ thưa hóa 95% được áp dụng, chất lượng hình ảnh được khôi phục tương tự như nhiễu ngẫu nhiên. Chúng tôi cũng đánh giá khả năng của một bộ phân loại mạng nơ-ron (ví dụ: mô hình ResNet-18) để nhận dạng hình ảnh được khôi phục. Kết quả trong hàng cuối của Bảng 6 cho thấy rằng với tỷ lệ thưa hóa 95%, độ chính xác phân loại gần như tương đương với đoán ngẫu nhiên.

Bên cạnh đó, vì phương pháp của chúng tôi chỉ sửa đổi các hoạt động FL trên máy chủ và giữ các bước FL khác (ví dụ: cập nhật mô hình cục bộ của khách hàng và giao tiếp khách hàng-máy chủ) không thay đổi, các phương pháp bảo mật thống kê, chẳng hạn như quyền riêng tư vi phân, cũng có thể được áp dụng cho các khách hàng cục bộ trong phương pháp của chúng tôi, giống như cách nó áp dụng cho FL vani. Mỗi khách hàng có thể độc lập thêm nhiễu Gauss vào các cập nhật mô hình cục bộ của họ,

[Hình 8: Hình ảnh được khôi phục dưới các tỷ lệ thưa hóa khác nhau]

trước khi gửi các cập nhật đến máy chủ Geyer et al. [2017]. Tương tự, nó cũng có thể áp dụng cho phương pháp bảo vệ quyền riêng tư của chúng tôi, bằng cách thêm nhiễu vào gradient sau thưa hóa.

Thước đo Mô hình 0% 30% 75% 95% Nhiễu ngẫu nhiên
MSE↓LeNet 5e-4 0.014 0.65 2.75 1.12
ResNet18 0 0.011 0.87 3.16 1.12
PSNR ↑LeNet 261 155 77.9 41.8 47.8
ResNet18 323 218 74.4 43.3 47.8
LPIPS Zhang [2018] ↓LeNet 0 0.04 0.13 0.56 0.50
ResNet18 0 0.01 0.18 0.59 0.50
FID Heusel et al. [2017] ↓LeNet 0 57 102 391 489
ResNet18 0 48 114 433 489
Độ chính xác mô hình (%) ↑LeNet 83.5 81.2 28.5 10.3 8.7
ResNet18 89.2 87.8 34.7 11.2 10.4

Bảng 6: Chất lượng khôi phục dữ liệu với các tỷ lệ thưa hóa khác nhau. Các thước đo khác nhau được sử dụng để đo độ tương tự giữa các mẫu dữ liệu được khôi phục và ban đầu.

Bảo vệ nhãn dữ liệu. Đảo ngược gradient có thể được sử dụng để khôi phục nhãn dữ liệu cục bộ của khách hàng Zhu và Han. [2019], Zhao và Bilen. [2020]. Như được hiển thị trong Bảng 7, trong khi độ chính xác khôi phục nhãn như vậy có thể cao tới 85% nếu không sử dụng phương pháp bảo vệ, việc áp dụng thưa hóa 95% có thể hiệu quả giảm độ chính xác như vậy xuống 66.7%. Ngoài ra, độ chính xác như vậy có thể được giảm thêm xuống 46.4% bằng cách thêm nhiễu (var= 10⁻³) vào gradient, với sự giảm nhẹ (3%) độ chính xác của mô hình được huấn luyện.

Phương pháp bảo vệ Không 95% SP 95% SP+nhiễu
Độ chính xác khôi phục 85.5% 66.7% 46.4%

Bảng 7: Độ chính xác khôi phục nhãn dưới các phương pháp bảo vệ và tỷ lệ thưa hóa khác nhau (SR)

Đảo ngược gradient chỉ nên được áp dụng cho các khách hàng bị trễ khi các sự không đồng nhất về dữ liệu và thiết bị đan xen, tức là dữ liệu cục bộ của khách hàng là độc đáo và không có sẵn ở nơi khác. Tuy nhiên, để quyết định đúng tính độc đáo như vậy, máy chủ sẽ cần biết nhãn lớp của dữ liệu khách hàng, do đó làm tổn hại quyền riêng tư dữ liệu của khách hàng. Thay vào đó, chúng tôi quyết định tính độc đáo dữ liệu bằng cách so sánh hướng của các cập nhật mô hình của khách hàng bị trễ với hướng của các cập nhật mô hình khác từ các khách hàng không bị trễ, và chỉ coi dữ liệu của khách hàng bị trễ là độc đáo nếu sự khác biệt như vậy lớn hơn một ngưỡng cho trước.

[Hình 9: Độ chính xác quyết định tính độc đáo của dữ liệu cục bộ khách hàng]

Chúng tôi định lượng sự khác biệt như vậy giữa các cập nhật mô hình wᵗᵢ,wᵗⱼ từ khách hàng i và j sử dụng khoảng cách cosine, sao cho

Dc(wᵗᵢ, wᵗⱼ) = 1−wᵗᵢ·wᵗⱼ/∥wᵗᵢ∥∥wᵗⱼ∥, (7)

và ngưỡng được tính toán như trung bình của khoảng cách cosine giữa các cập nhật mô hình không bị trễ tại t−τ:

1/∥Sᵗ⁻τunstale∥² ∑ⱼ,ₖ∈Sᵗ⁻τunstale[Dc(wᵗ⁻τⱼ, wᵗ⁻τₖ)] (8)

, trong đó Sᵗ⁻τunstale là tập hợp các khách hàng không bị trễ. Vì quy mô khoảng cách cosine thay đổi trong quá trình huấn luyện FL Li et al. [2023], giá trị trung bình của khoảng cách cosine thêm tính thích ứng cho ngưỡng.

Chúng tôi đã tiến hành các thí nghiệm sơ bộ để đánh giá liệu máy chủ có thể phát hiện chính xác các cập nhật mô hình quan trọng từ dữ liệu khách hàng độc đáo hay không. Trong thí nghiệm, chúng tôi mô phỏng sự không đồng nhất dữ liệu bằng cách gán mỗi khách hàng với các mẫu dữ liệu từ một lớp ngẫu nhiên, và kết quả trong Bảng 8 và Hình 9 cho thấy rằng độ chính xác nhanh chóng tăng lên >90% khi huấn luyện tiến triển, và độ chính xác phát hiện trung bình là 93%.

Epoch 20 100 200 800
Độ chính xác phát hiện 74.6% 89.2% 93.7% 94.5%

Bảng 8: Độ chính xác phát hiện từ các khách hàng bị trễ

4 Thí nghiệm

Chúng tôi đánh giá kỹ thuật đề xuất của mình trong hai tình huống FL. Trong tình huống đầu tiên, tất cả tập dữ liệu cục bộ của khách hàng đều cố định. Trong tình huống thứ hai, chúng tôi xem xét một thiết lập FL thực tế hơn, nơi dữ liệu cục bộ của khách hàng được cập nhật liên tục và phân phối dữ liệu toàn cục thay đổi theo thời gian, do những thay đổi động của bối cảnh môi trường. Các baseline sau đây giải quyết các cập nhật mô hình bị trễ trong FL được sử dụng:

• Tổng hợp không trọng số (Unweighted): Tổng hợp trực tiếp các cập nhật mô hình bị trễ mà không áp dụng trọng số.

• Tổng hợp có trọng số (Weighted) Shi et al. [2020]: Áp dụng trọng số cho các cập nhật bị trễ trong tổng hợp, và trọng số tỷ lệ nghịch với độ trễ.

• Bù trừ bậc nhất (1st-Order) Zheng et al. [2017], Zhu et al. [2022]: Bù trừ lỗi trong các cập nhật mô hình bị trễ sử dụng khai triển Taylor bậc nhất và xấp xỉ Hessian.

• Dự đoán trọng số toàn cục tương lai (W-Pred) Hakimi et al. [2019]: Giả định độ trễ là đã biết trước, mô hình toàn cục tương lai được dự đoán bằng phương pháp bậc nhất ở trên và được sử dụng để bù trừ các cập nhật mô hình bị trễ.

• FL với các tầng không đồng bộ (Asyn-Tiers) Chai et al. [2021]: Nó phân cụm các khách hàng thành các tầng không đồng bộ khác nhau dựa trên độ trễ và sử dụng FL đồng bộ trong mỗi tầng.

FedAvg Zhou và Lv. [2021] được sử dụng trong tất cả các thí nghiệm để tổng hợp các cập nhật mô hình. Do đó, tổng hợp không trọng số là FedAvg với độ trễ, và tổng hợp có trọng số áp dụng trọng số bổ sung cho các cập nhật mô hình trong FedAvg. 1st-Order, W-pred, và phương pháp của chúng tôi tiếp tục sửa đổi các trọng số như vậy thông qua bù trừ, và Asyn-Tiers riêng biệt sử dụng FedAvg trong mỗi tầng đồng bộ. Việc sử dụng FedAvg độc lập với phương pháp của chúng tôi và các baseline khác, và có thể được thay thế bằng các khung FL khác như FedProx Li et al. [2020].

Đối với tổng hợp có trọng số, chúng tôi đặt trọng số theo Shi et al. [2020] là 1/(1 +eᵃ⁽τ⁻ᵇ⁾), trong đó τ là lượng độ trễ và chúng tôi đặt các siêu tham số a=0.25 và b=10 dựa trên thiết lập thí nghiệm của chúng tôi về độ trễ. Đối với Asyn-Tiers, chúng tôi đặt hai tầng không đồng bộ và khi tổng hợp các cập nhật của các tầng khác nhau, các cập nhật cũng được trọng số bởi số lượng khách hàng trong các tầng khác nhau Chai et al. [2021].

Chúng tôi cũng đánh giá hiệu suất của kỹ thuật chúng tôi mà không có độ trễ, được gọi là "Unstale", để đánh giá sự khác biệt giữa các giá trị ước tính và thực của các cập nhật mô hình không bị trễ, cũng như tác động của lỗi ước tính đến hiệu suất FL.

4.1 Thiết lập thí nghiệm

Trong tất cả các thí nghiệm, chúng tôi xem xét một tình huống FL với 100 khách hàng. Mỗi cập nhật mô hình cục bộ trên một khách hàng được huấn luyện bởi 5 epoch sử dụng bộ tối ưu hóa SGD, với tỷ lệ học 0.01 và momentum 0.5.

Sự không đồng nhất dữ liệu: Chúng tôi sử dụng phân phối Dirichlet để lấy mẫu tập dữ liệu khách hàng với các phân phối nhãn khác nhau Hsu và Brown. [2019], và sử dụng tham số có thể điều chỉnh (α) để điều chỉnh lượng sự không đồng nhất dữ liệu: như được hiển thị trong Hình 10, α càng nhỏ, các phân phối nhãn này càng thiên lệch và lượng sự không đồng nhất dữ liệu càng cao. Khi α rất nhỏ, mỗi khách hàng chỉ có mẫu dữ liệu của ít lớp.

[Hình 10: Mô phỏng sự không đồng nhất dữ liệu sử dụng Phân phối Dirichlet]

Sự không đồng nhất thiết bị: Để đan xen sự không đồng nhất thiết bị với sự không đồng nhất dữ liệu, chúng tôi chọn một lớp dữ liệu bị ảnh hưởng bởi độ trễ, và áp dụng các lượng độ trễ khác nhau, được đo bằng số epoch mà các cập nhật mô hình của khách hàng bị trễ, cho 10 khách hàng hàng đầu có tập dữ liệu cục bộ chứa nhiều mẫu dữ liệu nhất của lớp dữ liệu được chọn. Tác động của độ trễ có thể được mở rộng thêm bằng cách áp dụng độ trễ theo cách tương tự cho nhiều lớp dữ liệu hơn.

Chúng tôi đánh giá hiệu suất FL bằng cách đánh giá độ chính xác của mô hình được huấn luyện trong lớp dữ liệu được chọn bị ảnh hưởng bởi độ trễ, và đánh giá thời gian huấn luyện FL bằng số epoch. Chúng tôi mong đợi rằng phương pháp của chúng tôi có thể cải thiện độ chính xác mô hình, hoặc đạt được độ chính xác tương tự với các baseline nhưng sử dụng ít epoch huấn luyện hơn.

4.2 Hiệu suất FL trong tình huống dữ liệu cố định

Trong tình huống dữ liệu cố định, 3 tập dữ liệu tiêu chuẩn và 1 tập dữ liệu chuyên ngành được sử dụng trong đánh giá:

• Sử dụng tập dữ liệu MNIST LeCun và Burges. [2010] và FMNIST Xiao et al. [2017] để huấn luyện mô hình LeNet, và lớp dữ liệu 5 bị ảnh hưởng bởi độ trễ;

• Sử dụng tập dữ liệu CIFAR-10 Krizhevsky [2009] để huấn luyện mô hình ResNet-18, lớp dữ liệu 2 bị ảnh hưởng bởi độ trễ;

• Sử dụng tập dữ liệu hình ảnh thảm họa MDI Mouzannar et al. [2018] để tinh chỉnh mô hình ResNet-18 được pre-train với ImageNet.

Độ chính xác(%) MNIST FMNIST CIFAR10 MDI
Unweighted 57.4 49.2 22.8 72.3
Weighted 39.2 30.1 12.6 61.2
1st-Order 57.4 49.3 22.6 72.3
W-Pred 57.3 48.9 22.9 72.2
Asyn-Tiers 57.6 50.3 25.9 69.8
Ours 61.2 55.4 29.4 75.4

Bảng 9: Độ chính xác của mô hình được huấn luyện với các tập dữ liệu khác nhau trong tình huống dữ liệu cố định

Độ chính xác mô hình được huấn luyện sử dụng các phương án FL khác nhau, với lượng độ trễ là 40 epoch, được liệt kê trong Bảng 9. Tiến trình huấn luyện của 1st-Order và W-Pred rất giống với tổng hợp không trọng số, cho thấy rằng ước tính các cập nhật mô hình bị trễ với khai triển Taylor không hiệu quả dưới độ trễ không giới hạn. Tương tự, tổng hợp có trọng số sẽ dẫn đến một mô hình thiên lệch với độ chính xác thấp hơn nhiều. Ngược lại, bù trừ dựa trên đảo ngược gradient của chúng tôi có thể cải thiện độ chính xác mô hình được huấn luyện ít nhất 4%, so với baseline tốt nhất. Ưu thế như vậy trong độ chính xác mô hình có thể lớn tới 25% khi so sánh với tổng hợp có trọng số. Ngoài dữ liệu hình ảnh, phương pháp của chúng tôi cũng có thể áp dụng cho các phương thức dữ liệu khác như văn bản và dữ liệu chuỗi thời gian. Kết quả và thảo luận về những phương thức này với các tập dữ liệu lớn thực tế trong Phụ lục A.

Hình 11 tiếp tục cho thấy quy trình huấn luyện FL qua các epoch khác nhau, và chứng minh rằng phương pháp của chúng tôi cũng có thể cải thiện tiến độ và tính ổn định của huấn luyện đồng thời đạt được độ chính xác mô hình cao hơn trong các giai đoạn khác nhau của huấn luyện FL. Hơn nữa, chúng tôi đã tiến hành các thí nghiệm với các lượng sự không đồng nhất dữ liệu và thiết bị khác nhau. Kết quả trong Bảng 10 và 11 cho thấy rằng, so với các baseline, phương pháp của chúng tôi thường có thể đạt được độ chính xác mô hình cao hơn hoặc đạt cùng độ chính xác với ít epoch huấn luyện hơn, đặc biệt khi lượng độ trễ lớn hoặc lượng sự không đồng nhất dữ liệu cao. Chúng tôi cũng sử dụng tập dữ liệu thực tế quy mô lớn khác để tiến hành thí nghiệm và kết quả trong Phụ lục C.

[Hình 11: Quy trình huấn luyện FL]

α 1 0.1 0.01
Acc Time Acc Time Acc Time
Unweighted 82.3 100 57.4 128 51.1 132
Weighted 82.4 102 39.2 171 31.1 179
1st-Order 82.5 100 57.3 129 51.5 131
W-Pred 82.8 100 57.6 126 50.9 131
Asyn-tiers 82.3 97 57.6 126 52.7 135
Ours 82.3 100 61.2 100 58.3 100

Bảng 10: Độ chính xác mô hình và tỷ lệ phần trăm epoch huấn luyện được tiết kiệm, với các lượng sự không đồng nhất dữ liệu khác nhau được kiểm soát bởi α trong phân phối Dirichlet. Tập dữ liệu MNIST và mô hình LeNet được sử dụng.

Độ trễ 10 40 100
Acc Time Acc Time Acc Time
Unweighted 72.6 104 57.4 128 41.5 142
Weighted 69.4 115 39.2 171 30.5 179
1st-Order 72.6 104 57.3 129 41.8 141
W-Pred 72.6 104 57.6 126 41.7 142
Asyn-tiers 72.7 103 57.6 126 38.3 138
Ours 73.3 100 61.2 100 47.2 100

Bảng 11: Độ chính xác mô hình và tỷ lệ phần trăm epoch huấn luyện được tiết kiệm, với các lượng độ trễ khác nhau được đo bằng số epoch bị trễ.

4.3 Hiệu suất FL trong tình huống dữ liệu biến đổi

[Hình 12: Tập dữ liệu cho nhận dạng chữ số: MNIST và SVHN]

Để liên tục thay đổi phân phối dữ liệu toàn cục, chúng tôi sử dụng hai tập dữ liệu công khai, cụ thể là MNIST và SVHN Netzer [2011], được dùng cho cùng một nhiệm vụ học nhưng với các biểu diễn đặc trưng khác nhau như được hiển thị trong Hình 12. Tập dữ liệu cục bộ của mỗi khách hàng được khởi tạo là tập dữ liệu MNIST theo cách tương tự như trong tình huống dữ liệu cố định. Sau đó, trong quá trình huấn luyện, mỗi khách hàng liên tục thay thế các mẫu dữ liệu ngẫu nhiên trong tập dữ liệu cục bộ của họ bằng các mẫu dữ liệu mới trong tập dữ liệu SVHN.

[Hình 13: Độ chính xác mô hình với phân phối dữ liệu biến đổi trong tập dữ liệu cục bộ của khách hàng]

Kết quả thí nghiệm trong Hình 13 cho thấy rằng trong tình huống dữ liệu biến đổi như vậy, vì phân phối dữ liệu cục bộ của khách hàng liên tục thay đổi, huấn luyện FL sẽ không bao giờ hội tụ. Do đó, độ chính xác mô hình đạt được bởi các phương án FL hiện có thể hiện dao động đáng kể theo thời gian và ở mức thấp (<40%). Ngược lại, kỹ thuật của chúng tôi có thể mô tả tốt hơn các mẫu dữ liệu biến đổi và do đó đạt được độ chính xác mô hình cao hơn nhiều, có thể so sánh với FL không có độ trễ và cao hơn 20% so với các phương án FL hiện có.

Độ trễ 10 40 100
Acc Time Acc Time Acc Time
Unweighted 60.6 99 53.2 117 39.1 131
Weighted 59.8 109 38.9 153 21.8 166
1st-Order 60.6 100 53.6 117 40.0 133
W-Pred 60.4 100 53.3 117 39.1 131
Asyn-tiers 58.2 103 46.9 118 35.7 137
Ours 63.3 100 62.5 100 61.0 100

Bảng 12: Độ chính xác mô hình và số epoch huấn luyện giảm, với các lượng độ trễ khác nhau được đo bằng số epoch bị trễ

Chúng tôi cũng đã tiến hành các thí nghiệm với các lượng độ trễ khác nhau và các tỷ lệ biến đổi phân phối dữ liệu khác nhau. Chúng tôi áp dụng các tỷ lệ biến đổi khác nhau của phân phối dữ liệu cục bộ của khách hàng bằng cách thay thế các lượng khác nhau của các mẫu dữ liệu ngẫu nhiên như vậy trong tập dữ liệu cục bộ của khách hàng trong mỗi epoch. Để ngăn chặn việc huấn luyện dừng quá sớm khi tỷ lệ biến đổi cao, chúng tôi lặp lại việc thay đổi dữ liệu khi tỷ lệ biến đổi vượt quá 1 mẫu trên epoch. Kết quả trong Bảng 12 và 13 cho thấy rằng phương pháp của chúng tôi vượt trội hơn các baseline với các lượng độ trễ khác nhau. Tổng hợp có trọng số hoạt động tệ nhất vì nó khiến mô hình thiên vị về các khách hàng ổn định khác và các baseline khác cho thấy hiệu suất tương tự vì chúng không thể bù trừ độ trễ lớn như vậy.

Tỷ lệ 0.5 1 2
Acc Time Acc Time Acc Time
Unweighted 73.1 100 39.1 131 44.1 127
Weighted 58.2 102 21.8 166 25.2 163
1st-Order 73.2 100 40.0 133 43.9 127
W-Pred 73.1 101 39.0 131 39.5 127
Asyn-tiers 68.3 98 35.7 137 39.1 130
Ours 70.3 100 60.1 100 63.3 100

Bảng 13: Độ chính xác mô hình và số epoch huấn luyện giảm, với các tỷ lệ biến đổi phân phối dữ liệu khác nhau được đo bằng số mẫu thay đổi trên epoch

5 Công việc liên quan

Hầu hết các giải pháp hiện có cho độ trễ trong FL đều dựa trên tổng hợp có trọng số Chen và Jin. [2019], Wang [2022], Chen [2020]. Những giải pháp hiện có này thiên vị về các khách hàng nhanh, và sẽ ảnh hưởng đến độ chính xác của mô hình được huấn luyện khi các sự không đồng nhất về dữ liệu và thiết bị trong FL đan xen. Các nhà nghiên cứu khác đề xuất sử dụng FL bán không đồng bộ, nơi máy chủ tổng hợp các cập nhật mô hình khách hàng với tần suất thấp hơn Nguyen [2022] hoặc phân cụm khách hàng thành các "tầng" không đồng bộ khác nhau theo tỷ lệ cập nhật của họ Chai [2021]. Tuy nhiên, làm như vậy không thể hoàn toàn loại bỏ tác động của các sự không đồng nhất về dữ liệu và thiết bị đan xen, vì việc tổng hợp của máy chủ vẫn liên quan đến các cập nhật mô hình bị trễ.

Thay vào đó, chúng ta có thể chuyển giao kiến thức từ các cập nhật mô hình bị trễ sang mô hình toàn cục, bằng cách huấn luyện một mô hình sinh và buộc dữ liệu được tạo của nó thể hiện giá trị dự đoán cao trên các cập nhật mô hình ban đầu Ye [2020], Lopes et al. [2017], Zhu et al. [2021]. Một cách tiếp cận khác là tối ưu hóa dữ liệu đầu vào được khởi tạo ngẫu nhiên cho đến khi nó có hiệu suất tốt trên mô hình ban đầu YYin [2020]. Tuy nhiên, chất lượng và độ chính xác của việc chuyển giao kiến thức trong những phương pháp này vẫn thấp, và chúng tôi đã cung cấp kết quả thí nghiệm chi tiết hơn trong Phụ lục C để chứng minh chất lượng thấp như vậy. Các nỗ lực khác tăng cường chất lượng chuyển giao kiến thức bằng cách kết hợp các tiên nghiệm hình ảnh tự nhiên Luo [2020] hoặc sử dụng một tập dữ liệu công khai khác để giới thiệu kiến thức chung Yang [2019], nhưng yêu cầu tập dữ liệu bổ sung. Hơn nữa, tất cả những phương pháp này yêu cầu các mô hình cục bộ của khách hàng phải được huấn luyện đầy đủ, điều này thường không khả thi trong FL.

6 Kết luận

Trong bài báo này, chúng tôi trình bày một khung FL mới để giải quyết các sự không đồng nhất về dữ liệu và thiết bị đan xen trong FL, bằng cách sử dụng đảo ngược gradient để ước tính các cập nhật mô hình không bị trễ của khách hàng. Các thí nghiệm cho thấy rằng kỹ thuật của chúng tôi cải thiện đáng kể độ chính xác mô hình và giảm lượng epoch huấn luyện cần thiết.

Lời cảm ơn

Chúng tôi cảm ơn các nhà phản biện ẩn danh về nhận xét và phản hồi của họ. Công việc này được hỗ trợ một phần bởi Quỹ Khoa học Quốc gia (NSF) dưới số tài trợ IIS-2205360, CCF-2217003, CCF-2215042, và Viện Y tế Quốc gia (NIH) dưới số tài trợ R01HL170368.
