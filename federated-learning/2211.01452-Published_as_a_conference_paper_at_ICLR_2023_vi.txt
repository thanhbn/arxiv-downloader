# 2211.01452.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/federated-learning/2211.01452.pdf
# Kích thước file: 1595910 byte

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
MPCF ORMER : SẠAN XUẤT TRANSFORMER NHANH, HIỆU SUẤT CAO VÀ RIÊNG TƯ
VỚI MPC
Dacheng Lic, Rulin Shaoc, Hongyi Wangc, Han Guoc, Eric Xingmpc, Hao Zhangb
cTrường Đại học Carnegie Mellonm Đại học Mohamed bin Zayed về Trí tuệ Nhân tạo
pPetuum Inc.bTrường Đại học California, Berkeley
TÓM TẮT
Việc cho phép suy luận riêng tư là rất quan trọng đối với nhiều dịch vụ suy luận đám mây
dựa trên các mô hình Transformer. Tuy nhiên, các giải pháp suy luận riêng tư hiện tại có
thể tăng độ trễ suy luận lên hơn 60 lần hoặc làm giảm đáng kể chất lượng suy luận. Trong
bài báo này, chúng tôi thiết kế khung MPCF ORMER như một giải pháp thực tế, sử dụng
Tính toán Đa bên Bảo mật (MPC) và Chưng cất Tri thức (KD). Thông qua các đánh giá
toàn diện, chúng tôi chỉ ra rằng MPCF ORMER tăng tốc đáng kể suy luận Transformer
trong môi trường MPC trong khi đạt được hiệu suất ML tương tự với mô hình đầu vào.
Trên bộ dữ liệu IMDb, nó đạt được hiệu suất tương tự BERT BASE, trong khi nhanh hơn
5.3 lần. Trên bộ đánh giá GLUE, nó đạt 97% hiệu suất của BERT BASE với tốc độ tăng
2.2 lần. MPCF ORMER vẫn hiệu quả với các trọng số Transformer được huấn luyện khác
nhau như ROBERTA BASE và các mô hình lớn hơn bao gồm BERT Large. Mã nguồn có
sẵn tại https://github.com/MccRee177/MPCFormer .

1 GIỚI THIỆU
Các mô hình Transformer được huấn luyện trước có thể dễ dàng được tinh chỉnh trên các nhiệm vụ
hạ nguồn khác nhau với hiệu suất cao và đã được phát triển rộng rãi như các dịch vụ suy luận mô
hình (Bommasani et al., 2021; Feng et al., 2020; Yang et al., 2019b; Clark et al., 2020). Tuy nhiên,
các dịch vụ suy luận mô hình này có thể gây ra mối quan ngại về quyền riêng tư. Ví dụ, GitHub
Copilot, một công cụ tạo mã được điều chỉnh từ trọng số GPT được huấn luyện trước, yêu cầu người
dùng phải tiết lộ lời nhắc mã của họ cho nhà cung cấp dịch vụ, hoặc nhà cung cấp dịch vụ phải phát
hành trọng số đã được huấn luyện của Copilot, vốn là tài sản kinh doanh độc quyền, cho người dùng
(Chen et al., 2021; Brown et al., 2020).

Tính toán Đa bên Bảo mật (MPC) cung cấp một giải pháp đầy hứa hẹn bằng cách giữ dữ liệu và
trọng số mô hình riêng tư trong quá trình suy luận (Evans et al., 2018). Tuy nhiên, suy luận
Transformer thông thường trong MPC chậm không thể chấp nhận được. Ví dụ, suy luận BERT BASE
mất <1 giây mà không có MPC, nhưng 60 giây với MPC (Hình 2). Một cách trực quan để tăng tốc
suy luận MPC là thay thế các phép toán tính toán bằng các xấp xỉ nhanh hơn và huấn luyện lại mô
hình được xấp xỉ, điều này đã được áp dụng trên mạng nơ-ron tích chập (CNN) (Chou et al., 2018).
Thật không may, việc điều chỉnh giải pháp này cho Transformer làm giảm mạnh hiệu suất của mô
hình (§ 5).

Trong bài báo này, chúng tôi thực hiện bước đầu tiên để theo đuổi suy luận mô hình Transformer
bảo tồn quyền riêng tư trong MPC, trong khi vẫn nhanh và hiệu suất cao. Chúng tôi lấy cảm hứng
từ phương pháp xấp xỉ¹ và cho rằng sự suy giảm hiệu suất là do hai thách thức. Đầu tiên, nhiều xấp
xỉ thân thiện với MPC làm khó khăn việc huấn luyện mô hình. Ví dụ, các hàm bậc hai gây ra vấn đề
bùng nổ gradient trong mạng nơ-ron sâu (Mishra et al., 2020). Thứ hai, các bộ dữ liệu hạ nguồn được
sử dụng để tinh chỉnh Transformer thường chứa dữ liệu không đủ để huấn luyện lại một Transformer
được xấp xỉ với các mục tiêu nhiệm vụ thông thường (Zhang & Sabuncu, 2018; Hinton et al., 2012).

Để giải quyết hai thách thức này, chúng tôi sử dụng khung chưng cất tri thức (KD). KD có thể làm
dễ dàng việc huấn luyện mô hình bằng cách khớp các biểu diễn trung gian giữa mô hình giáo viên và
mô hình học sinh (Romero et al., 2014); việc giám sát trung gian này có thể giảm thiểu vấn đề bùng
nổ gradient

Các tác giả đóng góp như nhau.
¹Chúng tôi sẽ sử dụng thuật ngữ xấp xỉ thân thiện với MPC.
1arXiv:2211.01452v2 [cs.LG] 16 Mar 2023

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
Trọng số Transformer được huấn luyện trước, ví dụ: GPT-3
Giáo viên: Transformer được tinh chỉnh
Bộ dữ liệu hạ nguồn Suy luận
MPCFormer
Học sinh: Transformer được xấp xỉ Giai đoạn 1: Cắm các xấp xỉ thân thiện với MPC
Giai đoạn 2: Chưng cất tri thức
Transformer được xấp xỉ đã chưng cất
Công cụ MPC, ví dụ: Crypten
Nhà cung cấp dịch vụ suy luận mô hình
Người dùng
Dữ liệu riêng tư
Công cụ MPC
Chia sẻ bí mật
Hình 1: Minh họa về khung MPCF ORMER được đề xuất của chúng tôi. MPCF ORMER lấy một mô
hình Transformer đã được huấn luyện (hoặc tinh chỉnh) và áp dụng các xấp xỉ thân thiện với MPC
đã cho, sau đó sử dụng KD trên các bộ dữ liệu hạ nguồn để xây dựng các mô hình chất lượng cao.
Trong thời gian suy luận, MPCF ORMER tận dụng công cụ MPC để đạt được suy luận mô hình riêng
tư. Để dễ minh họa, chúng tôi chỉ hiển thị nhà cung cấp dịch vụ và người dùng. Các hệ thống MPC
như CrypTen (Knott et al., 2021) cũng có thể liên quan đến một bên thứ ba đáng tin cậy (TTP) để
giúp tính toán chung.

(Lee et al., 2015). Đồng thời, mục tiêu KD có hiệu quả dữ liệu và cho phép huấn luyện một
Transformer được xấp xỉ trên các bộ dữ liệu hạ nguồn nhỏ (Touvron et al., 2021).

Phương pháp và đóng góp của chúng tôi. Trong bài báo này, chúng tôi xây dựng MPCF ORMER,
một khung dễ áp dụng cho suy luận Transformer bảo tồn quyền riêng tư. MPCF ORMER nhận vào
một xấp xỉ thân thiện với MPC và một Transformer đã được huấn luyện. Nó trả về một Transformer
với độ trễ suy luận thấp trong MPC và hiệu suất ML cao đồng thời. Để làm điều này, MPCF ORMER
đầu tiên thay thế các hàm nút thắt cổ chai trong mô hình Transformer đầu vào bằng các xấp xỉ thân
thiện với MPC đã cho. Mô hình Transformer được xấp xỉ kết quả có tốc độ suy luận nhanh hơn trong
MPC. Tiếp theo, nó áp dụng chưng cất tri thức để huấn luyện Transformer được xấp xỉ với hiệu suất
cao, sử dụng hướng dẫn từ giáo viên từ Transformer đầu vào ban đầu. Cuối cùng, nhà cung cấp mô
hình có thể sử dụng Transformer được xấp xỉ đã chưng cất trên một công cụ MPC, ví dụ: CrypTen,
cho dịch vụ suy luận mô hình riêng tư. Quy trình làm việc tổng thể của MPCF ORMER được hiển thị
trong Hình 1.

Chúng tôi triển khai MPCF ORMER trên một hệ thống MPC (Knott et al., 2021), với các xấp xỉ thân
thiện với MPC khác nhau. Trong quá trình này, chúng tôi cũng thiết kế một xấp xỉ thân thiện với
MPC mới và nhanh hơn cho hàm Softmax. Chúng tôi đánh giá toàn diện việc triển khai của mình
với các mô hình Transformer khác nhau. Trên bộ đánh giá IMDb, MPCF ORMER đạt được hiệu suất
ML tương tự BERT BASE với tốc độ tăng 5.3 lần. Nó đạt được hiệu suất ML tương tự BERT LARGE
với tốc độ tăng 5.9 lần. Trên bộ đánh giá GLUE, nó đạt 97% hiệu suất của BERT BASE với tốc độ
tăng 2.2 lần. MPCF ORMER cũng hiệu quả khi được cung cấp các mô hình Transformer được huấn
luyện khác nhau, ví dụ: RoBERTa BASE.

2 KIẾN THỨC NỀN TẢNG

Trong phần này, chúng tôi đầu tiên mô tả mô hình Transformer. Sau đó chúng tôi mô tả cách các hàm
trong mô hình Transformer có thể được triển khai trong MPC, và phân tích các nút thắt cổ chai về
hiệu suất.

2.1 CÁC MÔ HÌNH TRANSFORMER

Một mô hình Transformer n-tầng bao gồm ba thành phần: (1) Tầng nhúng. (2) Một chồng n tầng
Transformer. (3) Tầng dự đoán. Tầng nhúng ánh xạ một token (ví dụ: một từ hoặc một patch hình
ảnh) thành một biểu diễn ẩn (Devlin et al., 2018; Dosovitskiy et al., 2020). Một tầng Transformer
bao gồm một mô-đun attention và một số mô-đun nhân ma trận (Bahdanau et al., 2014). Tầng dự
đoán ánh xạ đầu ra của tầng Transformer cuối cùng thành một đầu ra nhiệm vụ (ví dụ: một phân phối
xác suất cho một nhiệm vụ phân loại). Một minh họa một phần của mô hình Transformer có thể
được tìm thấy trong Hình 3.

2.2 CÁC MÔ HÌNH TRANSFORMER TRONG MPC

Quá trình suy luận mô hình Transformer có thể được công thức hóa như một Tính toán 2-Bên (2PC).
Trong 2PC, bên người dùng nhập dữ liệu, và bên nhà cung cấp mô hình nhập mô hình Transformer.

2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
Họ cùng nhau tính toán một kết quả suy luận. Trong suốt quá trình suy luận, 2PC đảm bảo cả hai bên
chỉ biết thông tin về đầu vào của riêng họ và kết quả (Yang et al., 2019a).

Chúng tôi mô tả lược đồ chia sẻ bí mật như một phương tiện để bảo tồn quyền riêng tư trong quá
trình suy luận (Damg˚ard et al., 2012; Goldreich et al., 2019). Giả sử rằng người dùng cung cấp một
số x làm đầu vào của nó, lược đồ chia sẻ bí mật chia x thành hai số, x1 và x2. Sau đó nó để bên
người dùng giữ x1 và phân phối x2 cho bên nhà cung cấp mô hình. Có hai thuộc tính của x1 và x2.
Đầu tiên, x1 hoặc x2 một mình không chứa thông tin về x. Thuộc tính này cho phép người dùng ẩn
giá trị thực tế x khỏi nhà cung cấp mô hình. Thứ hai, cùng nhau chúng tái tạo lại x. Ví dụ, x1 và x2
cộng lại thành x: x = x1 + x2. Thuộc tính thứ hai cho phép tính toán chung.

Softmax
67.8%
GeLU18.6% MatMul12.7%Khác 0.8%
Hình 2: Phân tích thời gian chạy mô hình
BERT BASE (12-tầng, 512 token) được
đánh giá trên hệ thống MPC. Thời gian
chạy tổng thể với MPC là 59.0 giây,
nhưng <1 giây không có MPC.

Bảng 1: Thống kê truyền thông giải thích
hình bên trái. Ví dụ, 50.3GB truyền thông
trong các hàm Softmax mất 34.1 giây. Đặc
biệt, thời gian chạy trong MPC bị chi phối
bởi truyền thông chứ không phải tính toán.
Nhìn chung, truyền thông mất 46.4 giây,
chiếm 79% toàn bộ quá trình suy luận.

Các thủ tục Truyền thông (vòng)
Phép cộng 0
Phép nhân 1
So sánh 7

các hàm Truyền thông (khối lượng) Thời gian (s)
MatMul 3.5GB 2.5
GeLU 14.8GB 9.6
Softmax 50.3GB 34.1

Chúng tôi lấy phép nhân qua bộ ba Beaver làm ví dụ tính toán chung (Beaver, 1991). Trong phép
nhân, bên người dùng cung cấp x và nhà cung cấp mô hình cung cấp y, và họ chia sẻ bí mật x và y.
Vì vậy, người dùng có x1 và y1; nhà cung cấp mô hình có x2 và y2. Bộ ba Beaver giả định rằng một
bộ ba c = ab đã được tạo ra². Bộ ba này cũng được chia sẻ bí mật để bên người dùng có c1; a1; b1,
và nhà cung cấp mô hình có c2; a2; b2. Người dùng đầu tiên tính toán δ1 = x1 − a1, ε1 = y1 − b1
cục bộ. Nhà cung cấp mô hình tương tự tính toán δ2 = x2 − a2, ε2 = y2 − b2 cục bộ. Họ truyền
thông bốn số này và tái tạo lại δ = δ1 + δ2, ε = ε1 + ε2. Người dùng sau đó sử dụng hai giá trị này
để tính toán r1 = c1 + εb1 + δa1 + δε. Nhà cung cấp mô hình tính toán r2 = c2 + εb2 + δa2. Tại
thời điểm này, kết quả phép nhân xy có thể được tái tạo bởi xy = r1 + r2.

Có hai quan sát quan trọng trong ví dụ phép nhân: (1) nó không làm lộ thông tin cho bên kia. Ví dụ,
người dùng không gửi x1 cho bên mô hình. Thay vào đó, nó gửi δ1 = x1 − a1 trong đó a1 là một
mặt nạ ngẫu nhiên; (2) nó yêu cầu một vòng truyền thông bổ sung so với phép nhân không có MPC.
Điều này một phần giải thích tại sao các mô hình Transformer thông thường chậm trong MPC. Đặc
biệt, các hàm trong Transformer (ví dụ: kích hoạt phi tuyến) có thể được triển khai chủ yếu bởi ba
thủ tục³, tức là: phép cộng, phép nhân và so sánh. Bất kỳ phép toán tính toán nào được tạo thành
bởi các thủ tục này sẽ dẫn đến độ phức tạp bổ sung trong MPC⁴.

Về mặt thực nghiệm, chúng tôi hiển thị những phức tạp này bằng cách chạy BERT BASE (Hình 2)
và báo cáo thống kê truyền thông trong Bảng 1 với hệ thống MPC dựa trên chia sẻ bí mật (Knott et
al., 2021). Chúng tôi quan sát thấy rằng các hàm GeLU và các hàm Softmax trong các tầng
Transformer là nguồn chính của các nút thắt cổ chai, điều này phù hợp với những phát hiện trong
một nghiên cứu đồng thời (Wang et al., 2022). GeLU(x) = x[1/2][1 + erf(x/√2)] chậm vì hàm lỗi
Gaussian erf() được đánh giá bởi một khai triển Taylor bậc cao, yêu cầu nhiều thủ tục nhân.
Softmax(xi) = exp(xi)/Σj exp(xj) chậm vì (1) Hàm mũ được đánh giá bởi một số lần lặp bình
phương, yêu cầu nhiều thủ tục nhân; (2) phép toán maximum trên x được yêu cầu cho tính ổn định
số (Paszke et al., 2019), yêu cầu các thủ tục so sánh.

²Ví dụ, thông qua truyền giao mờ (Keller et al., 2016) hoặc mã hóa đồng cấu (Paillier, 1999).
³Chúng tôi chỉ xem xét các thủ tục lấy hai số chia sẻ bí mật để dễ minh họa.
⁴Chúng tôi cung cấp thêm chi tiết về việc triển khai các thủ tục và hàm trong MPC tại A.1.

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

3 CÔNG TRÌNH LIÊN QUAN

MPC. Tính toán Đa bên Bảo mật (MPC) cho phép tính toán chung giữa các bên trong khi giữ đầu
vào riêng tư. Tính năng riêng tư và hỗ trợ phong phú của các hệ thống đã làm cho nó phù hợp cho
suy luận Transformer (Mohassel & Zhang, 2017; Liu et al., 2017; Mohassel & Rindal, 2018; Riazi
et al., 2018; Juvekar et al., 2018; Wagh et al., 2019; Mishra et al., 2020; Knott et al., 2021). Trong
bài báo này, chúng tôi không nhằm mục đích triển khai một hệ thống MPC mới. Thay vào đó, chúng
tôi nhằm mục đích phát triển một giải pháp thuật toán để tăng tốc suy luận Transformer có thể di
động trên nhiều hệ thống MPC.

Các mô hình Transformer. Các mô hình Transformer đã đạt được thành công lớn trong hiểu biết
ngôn ngữ Yang et al. (2019b); Lan et al. (2019); Raffel et al. (2020); Clark et al. (2020), hiểu biết
thị giác Dosovitskiy et al. (2020); Liu et al. (2021); Radford et al. (2021), và xa hơn (Sharir et al.,
2021). Đặc biệt, chiến lược huấn luyện hai giai đoạn cho các mô hình Transformer đã được chứng
minh là hiệu quả trong các cài đặt rộng rãi và đã trở thành mô hình chiếm ưu thế (Liu et al., 2019;
Radford et al., 2018; Turc et al., 2019). Trong chiến lược huấn luyện này, các mô hình Transformer
đầu tiên được huấn luyện trước trên một bộ dữ liệu lớn để hiểu biết chung, và sau đó được tinh chỉnh
trên một bộ dữ liệu hạ nguồn nhỏ để học các đặc trưng cụ thể của nhiệm vụ. Trong công trình này,
chúng tôi xem xét mô hình này như cài đặt mặc định, nơi chúng tôi giả định rằng các nhà cung cấp
mô hình sử dụng trọng số Transformer được huấn luyện trước từ nơi khác, và chỉ có dữ liệu hạ
nguồn.

Các xấp xỉ thân thiện với MPC. Nghiên cứu hiện tại đã phát triển các xấp xỉ thân thiện với MPC để
tăng tốc tính toán CNN trong MPC. Chou et al. (2018) phát triển một khung tối ưu hóa để giảm
thiểu lỗi xấp xỉ của đa thức bậc 2 đối với ReLU: ReLU(x) = 0.125x² + 0.25x + 0.5. Điều này gây
ra sự giảm độ chính xác đáng kể vì kích hoạt bậc hai làm cho thuật toán Gradient Descent (GD) phân
kỳ. Mishra et al. (2020) giảm thiểu vấn đề này bằng cách sử dụng một tập hợp các heuristic được
thiết kế cẩn thận cùng với Neural Architecture Search (NAS). Mohassel & Zhang (2017) đề xuất
một xấp xỉ cho softmax bằng cách thay thế hàm mũ bằng các hàm ReLU. Chúng tôi không tập trung
vào việc phát triển heuristic cho một cặp hàm nút thắt cổ chai và xấp xỉ duy nhất. Thay vào đó,
chúng tôi tập trung vào việc phát triển một khung chung có thể nhất quán đưa ra một mô hình
Transformer hiệu suất cao với các xấp xỉ khác nhau.

Chưng cất Tri thức (KD). KD chuyển giao tri thức từ mô hình giáo viên sang mô hình học sinh bằng
cách khớp các biểu diễn ẩn của họ (Hinton et al., 2006). Một số nghiên cứu đã thiết kế các mục tiêu
hiệu quả cho các mô hình Transformer (Sanh et al., 2019; Jiao et al., 2019; Dosovitskiy et al., 2020)
như khớp các ma trận attention. Đặc biệt, Sanh et al. (2019) và Jiao et al. (2019) có mục tiêu khác
với chúng tôi và cũng huấn luyện trên bộ dữ liệu huấn luyện trước. Tuy nhiên, chúng tôi chia sẻ
cùng giả định về phía nhà cung cấp mô hình — họ chỉ có các bộ dữ liệu hạ nguồn.

4 PHƯƠNG PHÁP

Trong phần này, chúng tôi trình bày khung MPCF ORMER. MPCF ORMER cho phép nhà cung cấp
mô hình chuyển đổi mô hình Transformer của mình thành một mô hình nhanh hơn và hiệu suất cao
hơn cho dịch vụ suy luận riêng tư. Trong 4.1, chúng tôi giới thiệu quy trình làm việc của
MPCF ORMER (Hình 1), theo sau là chi tiết của từng bước trong 4.2.

4.1 QUY TRÌNH LÀM VIỆC TỔNG QUÁT

Trong dịch vụ suy luận, một nhà cung cấp mô hình giữ một mô hình Transformer T, và người dùng
giữ dữ liệu X. Họ đạt được thỏa thuận về một hệ thống MPC để thực hiện suy luận riêng tư. Trong
§2, chúng tôi minh họa rằng việc sử dụng T để thực hiện suy luận trong hệ thống MPC là chậm.
Thay vì sử dụng T, nhà cung cấp mô hình có thể sử dụng MPCF ORMER để tạo ra một mô hình phù
hợp hơn S. S chạy nhanh hơn nhiều so với T trong cài đặt MPC trong khi có hiệu suất ML tương tự
so với T.

Để sử dụng MPCF ORMER, nhà cung cấp mô hình cần cung cấp mô hình Transformer đã huấn
luyện T, bộ dữ liệu hạ nguồn D, và các xấp xỉ thân thiện với MPC A. Các xấp xỉ thân thiện với
MPC A này cần phải nhanh trong MPC và sẽ được sử dụng để thay thế các hàm nút thắt cổ chai
trong T. Quy trình làm việc có thể được mô tả ngắn gọn như:

Chuyển đổi: S = MPCF ORMER(T; D; A)
Suy luận: y = MPCS(X) (1)

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Độ chính xác Tốc độ suy luận MHA x1x1 MHA MHA Giai đoạn 1 Xấp xỉ Giai đoạn 2 Chưng cất MHA x1 Độ chính xác Tốc độ suy luận Độ chính xác Tốc độ suy luận

Hình 3: Tổng quan về khung MPCF ORMER. Giai đoạn đầu tiên sử dụng các xấp xỉ thân thiện với
MPC và T để xây dựng một kiến trúc Transformer nhanh hơn S0. Giai đoạn thứ hai sử dụng Chưng
cất Tri thức trên S0 để học một mô hình Transformer hiệu suất cao và nhanh S.

4.2 MPCF ORMER

MPCF ORMER là một khung hai giai đoạn như thể hiện trong Hình 3. Giai đoạn đầu tiên tận dụng A
và T để xây dựng một kiến trúc Transformer thân thiện với MPC S0, đạt được suy luận nhanh trong
hệ thống MPC. Giai đoạn thứ hai áp dụng chưng cất tri thức (KD) lên S0 để học mô hình đầu ra S,
nhanh trong MPC và bảo tồn hiệu suất cao của T.

4.2.1 GIAI ĐOẠN 1: XẤP XỈ

Trong giai đoạn đầu tiên, MPCF ORMER thay thế các hàm nút thắt cổ chai trong T bằng A đã cho
để xây dựng một kiến trúc Transformer thân thiện với MPC S0 (Hình 3). Dưới đây chúng tôi hiển
thị cách chúng tôi xây dựng A cho các thí nghiệm của mình, tức là sử dụng hệ thống MPC Knott et
al. (2021). Trong §2, chúng tôi xác định nút thắt cổ chai là các hàm GeLU và Softmax. Vì vậy
chúng tôi xây dựng A cho hai hàm này.

Xấp xỉ GeLU. Phân tích trong §2 cho thấy phép nhân trong MPC yêu cầu truyền thông bổ sung. Vì
vậy, các hàm bậc hai là kích hoạt phi tuyến nhanh nhất trong MPC. Vì các hàm GeLU và ReLU chia
sẻ các giá trị hàm tương tự, chúng tôi đơn giản lấy hàm bậc hai được thiết kế cho hàm ReLU (§3)
để xấp xỉ hàm GeLU: GeLU(x) ≈ 0.125x² + 0.25x + 0.5. Chúng tôi ký hiệu xấp xỉ này là "Quad".

Xấp xỉ Softmax. Các công trình trước đây trong CNN đã phát triển một xấp xỉ thân thiện với MPC
cho các hàm Softmax (Mohassel & Zhang, 2017):

softmax(x) ≈ ReLU(x)/Σ ReLU(x) (2)

Chúng tôi xác thực rằng điều này có tốc độ suy luận nhanh hơn hàm Softmax trong cài đặt của
chúng tôi (Hình 4). Chúng tôi ký hiệu xấp xỉ này là "2ReLU". Tuy nhiên, điều này vẫn chưa thỏa
mãn. Phân tích trong §2 cho thấy rằng việc đánh giá hàm ReLU yêu cầu sử dụng nhiều thủ tục so
sánh, rất tốn kém. Vì vậy, chúng tôi đề xuất một xấp xỉ tích cực hơn cho Softmax bằng cách thay
thế ReLU trong Eq. 2 bằng một hàm bậc hai:

softmax(x) ≈ (x + c)²/Σ(x + c)² (3)

Chúng tôi ký hiệu điều này là "2Quad". Quan trọng là, "2Quad" và các hàm Softmax khác nhau rất
nhiều về giá trị số, trong khi các công trình trước đây lập luận rằng sự tương tự về giá trị số là quan
trọng đối với hiệu suất của mô hình (Chou et al., 2018). Chúng tôi có thể sử dụng xấp xỉ tích cực
này vì giai đoạn chưng cất tiếp theo của chúng tôi đủ hiệu quả để thu hẹp khoảng cách hiệu suất.
Hình 4 hiển thị sự so sánh giữa thời gian chạy của hàm GeLU và softmax gốc so với các xấp xỉ của
chúng. Đặc biệt, 2Quad có tốc độ suy luận nhanh hơn nhiều so với 2ReLU.

4.2.2 GIAI ĐOẠN 2: CHƯNG CẤT

Trong giai đoạn thứ hai, chúng tôi sử dụng KD để làm cho mô hình Transformer được xấp xỉ nhanh
S0 có hiệu suất cao. Lợi ích của KD là hai mặt. Đầu tiên, nó cho phép chúng tôi sử dụng các xấp xỉ
tích cực hơn như "2Quad", dẫn đến tốc độ tăng cao hơn. Thứ hai, hiệu quả dữ liệu của nó cho phép
chúng tôi học hiệu quả một S tốt với các bộ dữ liệu hạ nguồn nhỏ. Cụ thể, chúng tôi tiến hành
chưng cất theo tầng

5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

gelu quad
Các hàm giống GeLU 0 2 4 6 8 10 Thời gian (giây) 11
0.5 9.6
0.4

softmax 2relu 2quad
Các hàm giống Softmax 0 5 10 15 20 25 30 35 40 Thời gian (giây) 40
14.7
3.2 34.1
12.8
1.7 tổng thời gian
thời gian truyền thông

Hình 4: So sánh thời gian chạy của các xấp xỉ khác nhau trong §4.2.1. Vùng màu xanh biểu thị
tổng thời gian chạy. Vùng màu cam biểu thị thời gian truyền thông. Các xấp xỉ thân thiện với MPC
giảm đáng kể cả thời gian truyền thông và tổng thời gian. Đặc biệt, "2Quad" được đề xuất của
chúng tôi nhanh hơn nhiều so với hàm Softmax gốc, và xấp xỉ "2ReLU" trước đây.

để chuyển giao tri thức từ mô hình đầu vào T sang S0 bằng cách khớp biểu diễn tại bốn vị trí sau
đây: (1) tầng nhúng, (2) ma trận attention trong mỗi tầng Transformer, (3) các trạng thái ẩn sau mỗi
tầng Transformer, và (4) tầng dự đoán cuối cùng. Bốn vị trí này đã được chứng minh là lưu trữ
thông tin có ý nghĩa trong các công trình trước đây (Hinton et al., 2015; Jiao et al., 2019; Clark et
al., 2019). Chúng tôi sử dụng hàm mất mát Mean Square Error (MSE) để khớp các biểu diễn giữa T
và S cho tất cả các vị trí. Chúng tôi tuân theo thủ tục học tập của Jiao et al. (2019) để đầu tiên
chưng cất các tầng nhúng và Transformer (bao gồm ma trận attention và các trạng thái ẩn) và sau đó
chưng cất tầng dự đoán.

Khởi tạo học sinh. Một thành phần quan trọng của chưng cất tri thức là việc khởi tạo mô hình học
sinh (Sanh et al., 2019). Tận dụng việc S0 và T chia sẻ cùng kiến trúc, chúng tôi khởi tạo S0 bằng
cách sử dụng các trọng số trong T. Chúng tôi thấy rằng điều này vượt trội hơn việc khởi tạo trọng
số ngẫu nhiên, đặc biệt là trên các bộ dữ liệu nhỏ hơn (§5.3).

5 CÁC THÍ NGHIỆM

Chúng tôi thiết kế khung MPCF ORMER để tương thích với nhiều xấp xỉ thân thiện với MPC và các
mô hình Transformer đã được huấn luyện, để các nhà cung cấp mô hình có thể thuận tiện cắm các
xấp xỉ thân thiện với MPC dựa trên hệ thống MPC của họ. Vì vậy, chúng tôi đánh giá
MPCF ORMER với các xấp xỉ thân thiện với MPC khác nhau dưới (1) Các bộ dữ liệu khác nhau
(§5.1), và (2) Các mô hình khác nhau (đặc biệt là các mô hình lớn hơn) (§5.2). Trong nghiên cứu
ablation, chúng tôi nghiên cứu (1) tác động của việc khởi tạo học sinh và, (2) tác động của số lượng
ví dụ huấn luyện trong giai đoạn chưng cất.

Thiết lập thí nghiệm. Chúng tôi sử dụng hai instance P3.2x AWS để mô phỏng các kịch bản dịch
vụ suy luận (một P3.2x cho nhà cung cấp mô hình, và một cho người dùng). Mỗi instance được trang
bị một GPU Tesla V100, và băng thông Ethernet 10GbE. Chúng tôi đặt các instance trong cùng một
nhóm vị trí để đảm bảo băng thông 10GbE trong AWS. Phân tích thời gian được đo với CrypTen,
triển khai chia sẻ bí mật với giả định kẻ thù bán trung thực (Phần §2) (Knott et al., 2021). Chúng
tôi huấn luyện và đánh giá các mô hình dựa trên HuggingFace (Wolf et al., 2019). Đặc biệt, chúng
tôi thấy rằng việc triển khai 2Quad yêu cầu sử dụng cẩn thận mã nguồn HuggingFace (§A.2).

Baseline. Chúng tôi xác định ba thuộc tính quan trọng trong suy luận Transformer trong § 1: tốc độ,
hiệu suất và quyền riêng tư. Trong quy trình làm việc của chúng tôi, quyền riêng tư đã được đảm
bảo bằng cách sử dụng các hệ thống MPC. Vì vậy, chúng tôi đánh giá MPCF ORMER bằng hai khía
cạnh khác: tốc độ và hiệu suất. Cụ thể, S phải chạy nhanh hơn T trong khi khớp hiệu suất của T. Vì
có một lượng công trình hạn chế về suy luận Transformer trong MPC, chúng tôi tìm kiếm một
baseline từ tài liệu CNN. Đặc biệt, chúng tôi chọn chiến lược huấn luyện trong (Chou et al., 2018)
và ký hiệu nó là MPCF ORMER w/o fdg. MPCF ORMER w/o fdg cũng xây dựng mô hình được xấp
xỉ S0 nhưng huấn luyện S0 trên D với mục tiêu cụ thể của nhiệm vụ, tức là không có chưng cất.
Chúng tôi lưu ý rằng S0 được khởi tạo với các trọng số trong T, tức là với các hàm khác nhau, có
tác động chưa được nghiên cứu. Vì vậy chúng tôi đề xuất một baseline thứ hai MPC-

6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

FORMER w/o fp,dg, huấn luyện S0 trên D mà không có chưng cất, và khởi tạo trọng số ngẫu nhiên.
Dưới đây chúng tôi so sánh hiệu suất của MPCF ORMER với MPCF ORMER w/o fp,dg và
MPCF ORMER w/o fdg với cùng tốc độ tăng.

Chúng tôi ký hiệu mô hình đầu ra của khung của chúng tôi với BERT BASE, Roberta-base, và BERT
LARGE tương ứng là MPCBert-B, MPCRoberta-B, và MPCBert-L cho ngắn gọn.

5.1 SO SÁNH VỚI BASELINE TRÊN CÁC BENCHMARK KHÁC NHAU

Cài đặt. Trong phần này, chúng tôi đánh giá khung MPCFormer của chúng tôi với các xấp xỉ khác
nhau và so sánh nó với các baseline trên bộ dữ liệu IMDb và benchmark GLUE (Maas et al., 2011;
Wang et al., 2018). Đối với tất cả các thí nghiệm trong phần này, chúng tôi sử dụng BERT BASE
làm mô hình cơ sở. Theo thống kê bộ dữ liệu, chúng tôi sử dụng độ dài chuỗi 512 cho bộ dữ liệu
IMDb và độ dài chuỗi 128 cho các bộ dữ liệu GLUE. Chúng tôi lưu ý rằng độ dài chuỗi dài hơn
thường phản ánh tốc độ tăng cao hơn vì các hàm Softmax có thể được tăng tốc nhiều hơn. Các
baseline được huấn luyện với tỷ lệ học được điều chỉnh từ 1e-6, 5e-6, 1e-5, và 1e-4, số epoch từ
10, 30, và 100, kích thước batch 32 cho IMDB, kích thước batch 64 và 256 cho GLUE. MPCBert-B
được huấn luyện với tỷ lệ học 5e-5 cho chưng cất tầng nhúng và Transformer, và 1e-5 cho chưng
cất tầng dự đoán. Chi tiết thêm về điều chỉnh siêu tham số có thể được tìm thấy trong A.4.

Bảng 2: Hiệu suất và tốc độ tăng trên bộ dữ liệu IMDB và một phần của benchmark GLUE (QNLI,
CoLA và RTE) với các xấp xỉ khác nhau và BERT BASE làm backbone. Mô hình đầu vào T được ký
hiệu với "*". "p" đại diện cho việc sử dụng trọng số trong T làm khởi tạo, "d" đại diện cho việc áp
dụng chưng cất tri thức với T làm giáo viên.

Phương pháp Xấp xỉ IMDb GLUE
Tốc độ tăng Độ chính xác Tốc độ tăng Điểm trung bình

Bert-B GeLU+Softmax 1× 94.1 1 73.1
MPCBert-B w/o fp,dg
Quad+Softmax 1.24 87.5
1.13 40.8
MPCBert-B w/o fdg 87.5 43.0
MPCBert-B 94.0 72.6
MPCBert-B w/o fp,dg
GeLU+2ReLU 1.76 86.1
1.40 39.6
MPCBert-B w/o fdg 93.8 71.8
MPCBert-B 94.0 72.0
MPCBert-B w/o fp,dg
Quad+2ReLU 2.61 85.8
1.93 43.5
MPCBert-B w/o fdg 86.8 48.2
MPCBert-B 94.0 69.8
MPCBert-B w/o fp,dg
GeLU+2Quad 2.65 87.3
1.55 39.6
MPCBert-B w/o fdg 90.6 69.7
MPCBert-B 94.0 71.0
MPCBert-B w/o fp,dg
Quad+2Quad 5.26 87.8
2.20 40.7
MPCBert-B w/o fdg 87.3 40.8
MPCBert-B 93.9 68.4

Bảng 3: Hiệu suất và tốc độ tăng trên bộ dữ liệu IMDB và một phần của benchmark GLUE (QNLI,
CoLA và RTE) với các xấp xỉ khác nhau và BERT BASE làm backbone. Mô hình đầu vào T được ký
hiệu với "*". "p" đại diện cho việc sử dụng trọng số trong T làm khởi tạo, "d" đại diện cho việc áp
dụng chưng cất tri thức với T làm giáo viên.

Phương pháp IMDb GLUE
Tốc độ tăng Độ chính xác Tốc độ tăng Điểm trung bình

Giải pháp 1 1× 94.1 1 73.1
Giải pháp 2 5.26 87.8 2.20 40.7
MPCFormer 93.9 68.4

Chúng tôi hiển thị độ chính xác và tốc độ tăng trên bộ dữ liệu IMDb trong Bảng 3. MPCBert-B đạt
được tốc độ tăng 5.26× với xấp xỉ "Quad+2Quad" với gần như không có sự sụt giảm độ chính xác.
Ngoài ra, chúng tôi lưu ý rằng điều này không chỉ đúng với xấp xỉ "Quad+2Quad" nhanh nhất mà
còn với các xấp xỉ khác trong

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

khoảng từ 1.24× đến 2.65× tốc độ tăng. Các baseline có hiệu suất kém hơn cho tất cả các xấp xỉ.
Ví dụ, cả hai baseline đều có ít nhất 6.8% sụt giảm độ chính xác với tốc độ tăng 5.26×. Thú vị là,
MPCBert-B w/o fdg có sụt giảm độ chính xác vừa phải với xấp xỉ "GeLU+2ReLU" với tốc độ tăng
1.76×. Tuy nhiên, nó không bảo tồn độ chính xác với các xấp xỉ khác. Ngược lại, MPCF ORMER
nhất quán bảo tồn độ chính xác với nhiều loại xấp xỉ khác nhau.

Để xác thực quan sát với nhiều bộ dữ liệu hơn, chúng tôi đánh giá MPCBert-B trên benchmark
Glue (8 bộ dữ liệu) (Wang et al., 2018). Như hiển thị trong Bảng 4, MPCBert-B đạt được tốc độ
tăng 1.93× với 98% hiệu suất của BERT BASE trên benchmark GLUE, và tốc độ tăng 2.2× với 97%
hiệu suất của BERT BASE. Cả hai baseline đều gây ra sụt giảm hiệu suất nghiêm trọng, tức là 19.5
điểm trung bình sụt giảm cho MPCBert-B w/o fdg và 26.2 điểm trung bình sụt giảm cho MPCBert-B
w/o fp,dg với tốc độ tăng 1.93×. Thú vị là, chúng tôi quan sát thấy rằng baseline MPCBert-B w/o fdg
nhất quán vượt trội hơn MPCBert-B w/o fp,dg. Điều này cho thấy rằng việc sử dụng trọng số trong T
làm khởi tạo có lợi cho việc huấn luyện S0 với mục tiêu cụ thể của nhiệm vụ, sử dụng backbone
Transformer 12-tầng. Ngoài ra, chúng tôi đánh giá MPCF ORMER với nhiều xấp xỉ hơn sử dụng
một tập con của benchmark GLUE. Kết quả được hiển thị ở phần bên phải của Bảng 3. Chúng tôi
quan sát các mẫu tương tự như trong bộ dữ liệu IMDb. Baseline MPCBert-B w/o fdg hoạt động tốt
trong các xấp xỉ "GeLU+2Quad' và "GeLU+2ReLU", nhưng MPCF ORMER đạt được hiệu suất ML
cao nhất quán dưới tất cả các xấp xỉ.

Bảng 4: Hiệu suất trên benchmark Glue với BERT BASE làm backbone. Điểm F1 được báo cáo cho
QQP và MRPC. Tương quan Pearson và Spearman trung bình được báo cáo cho STS-B. Tương quan
Matthews được báo cáo cho CoLA. Độ chính xác được báo cáo cho các bộ dữ liệu khác.

Phương pháp Xấp xỉ MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Trung bình Tốc độ
393k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k - tăng

Bert-B (GeLU+Softmax) 84.7/85.0 88.1 91.7 93.1 57.8 89.1 90.3 69.7 82.8 1×
MPCBert-B w/o fp,dg Quad 62.1/61.3 74.6 61.8 80.7 13.4 23.1 81.2 55.2 56.6
1.93× MPCBert-B w/o fdg + 73.1/72.5 82.9 75.5 83.4 16.4 41.3 81.2 52.7 63.3
MPCBert-B 2ReLU 85.0/85.3 87.8 91.2 92.0 54.0 85.7 88.9 64.3 81.1
MPCBert-B w/o fp,dg Quad 63.5/62.4 78.6 59.8 81.1 9.9 19.5 81.4 52.7 55.7
2.2× MPCBert-B w/o fdg + 70.6/70.5 83.4 69.8 83.3 0 36.1 81.2 52.7 60.9
MPCBert-B 2Quad 84.9/85.1 88.1 90.6 92.0 52.6 80.3 88.7 64.9 80.3

5.2 SO SÁNH NHIỀU HƠN VỚI CÁC MÔ HÌNH KHÁC NHAU

Chúng tôi đánh giá MPCF ORMER với các mô hình Transformer đã được huấn luyện khác ngoài
BERT BASE, tức là mô hình ROBERTA BASE (12 tầng) (Liu et al., 2019), và đặc biệt là mô hình
BERT LARGE lớn hơn (24 tầng). Kết quả được hiển thị trong Bảng 5. MPCRoberta-B bảo tồn 98%
điểm trung bình của mô hình đầu vào ROBERTA BASE, và vượt trội hơn các baseline với biên độ
lớn. So sánh hiệu suất của các baseline, chúng tôi lại quan sát thấy rằng việc khởi tạo với trọng số
trong T giúp huấn luyện với S0, tức là MPCRoberta-B w/o fdg hoạt động tốt hơn MPCRoberta-B
w/o fp,dg.

Bảng 5: Hiệu suất trên một tập con của benchmark Glue với backbone Roberta-base (được ký hiệu
là "MPCRoberta-B"). MPCRoberta-B và các baseline sử dụng xấp xỉ "Quad+2Quad" với tốc độ
tăng 2.1×. MPCBert-L và các baseline sử dụng xấp xỉ "Quad+2ReLU" với tốc độ tăng 2.0×.

MNLI-m MNLI-mm QNLI RTE Trung bình Tốc độ tăng
Roberta-B 87.4 87.0 92.6 76.5 85.8 1×
MPCRoberta-B w/o fp,dg 58.0 58.1 69.0 52.7 59.5
2.1× MPCRoberta-B w/o fdg 73.1 72.7 81.6 52.7 70.0
MPCRoberta-B 86.5 86.9 92.2 72.2 84.5
Bert-L 86.7 86.6 92.7 75.1 85.3 1×
MPCBert-L w/o fp,dg 62.3 62.3 60.0 52.7 59.3
2.0× MPCBert-L w/o fdg 35.4 35.2 50.5 52.7 43.5
MPCBert-L 86.5 86.7 92.8 72.2 84.6

Để chỉ ra phương pháp của chúng tôi có thể mở rộng theo kích thước mô hình khác nhau, chúng tôi
đánh giá MPCF ORMER với mô hình lớn hơn BERT LARGE (24-tầng). Trên bộ dữ liệu IMDb,
BERT BASE đạt được độ chính xác 95.0%. MPCBert-L đạt được tốc độ tăng 5.9× với độ chính xác
94.5%, trong khi các baseline MPCBert-L w/o fp,dg

8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

đạt được 87.1% độ chính xác, và MPCBert-L w/o fdg đạt được 50.0% độ chính xác. Chúng tôi tiếp
tục chọn bốn bộ dữ liệu từ benchmark GLUE, nơi Bert-L* vượt trội đáng kể so với Bert-B* (MNLI,
QNLI, CoLA, và RTE). So với Bert-B* trong bảng 4, Bert-L* tăng điểm trung bình từ 82.8 lên 85.3.
MPCF ORMER tăng điểm trung bình từ 81.5 lên 84.6. Điều này cho thấy rằng MPCF ORMER có
thể mở rộng theo kích thước của T. Mặt khác, các baseline không mở rộng theo mô hình đầu vào:
MPCBert-L w/o fp,dg giảm điểm trung bình từ 60.1 xuống 59.3; MPCBert-L w/o fdg giảm điểm
trung bình từ 68.5 xuống 43.5. Đặc biệt, chúng tôi quan sát thấy rằng việc khởi tạo S0 với trọng số
trong T mà không có chưng cất gây hại cho hiệu suất khi mô hình lớn hơn.

5.3 NGHIÊN CỨU ABLATION

0.0 0.2 0.4 0.6 0.8 1.0
Tỷ lệ ví dụ huấn luyện 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 1.02 Hiệu suất tương đối
MRPC
RTE
SST-2
QNLI

Hình 5: Tỷ lệ ví dụ huấn luyện so với hiệu suất (được chuẩn hóa bởi hiệu suất với tỷ lệ = 1.0) trên
MRPC, RTE, QNLI, và SST-2.

Câu hỏi đầu tiên chúng tôi nghiên cứu là tác động của việc khởi tạo học sinh khác nhau. Điều này
rất thú vị vì chúng tôi không có kiến thức chung về việc liệu khởi tạo với trọng số trong T vẫn sẽ có
lợi cho việc huấn luyện S0 sau các xấp xỉ tích cực. Chúng tôi thiết kế một thí nghiệm với khởi tạo
ngẫu nhiên (được ký hiệu là MPCBert-B r trong bảng). Chúng tôi huấn luyện MPCBert-B r với 10
epoch nhiều hơn MPCBert-B r và xác nhận rằng mục tiêu chưng cất của nó đã hội tụ. Chúng tôi
quan sát thấy rằng trên các bộ dữ liệu lớn hơn (QNLI và SST-2), khoảng cách giữa việc sử dụng
khởi tạo khác nhau là nhỏ, nhưng trên các bộ dữ liệu nhỏ hơn (STS-B, MRPC, và RTE), việc khởi
tạo với các trọng số trong T tốt hơn.

Câu hỏi thứ hai chúng tôi nghiên cứu là tác động của số lượng ví dụ huấn luyện trong giai đoạn
chưng cất. Chúng tôi thực hiện nghiên cứu trên hai bộ dữ liệu nhỏ (RTE, MRPC) và hai bộ dữ liệu
trung bình (SST-2, QNLI) trong benchmark GLUE (Hình 5). Chúng tôi thấy rằng khoảng 5% của
các bộ dữ liệu nhỏ và 2% của các bộ dữ liệu trung bình cung cấp đủ dữ liệu để học một S tốt. Điều
này cho thấy rằng KD trong cài đặt của chúng tôi đủ hiệu quả để học một S tốt trong các nhiệm vụ
hạ nguồn (sử dụng các bộ dữ liệu GLUE làm đại diện).

Bảng 6: Mô hình học sinh được khởi tạo với trọng số trong T so với với trọng số ngẫu nhiên. Kết
quả cho MPCBert-B r được điều chỉnh với tỷ lệ học chưng cất tầng nhúng và Transformer từ 5e-5
và 3e-5. Kết quả cho cả hai đều được thu được với xấp xỉ "Quad+2Quad".

QNLI SST-2 STS-B MRPC RTE Trung bình
MPCBert-B 90.6 92.0 80.8 88.7 64.9 84.0
MPCBert-B r 90.6 90.0 60.0 81.2 58.5 76.1

5.4 HẠN CHẾ VÀ HƯỚNG TƯƠNG LAI

Chúng tôi nhận ra hai hạn chế trong bài báo của chúng tôi. Đầu tiên, tốc độ tăng và hiệu suất của
chúng tôi được kiểm tra trên một hệ thống MPC duy nhất. Chúng tôi để lại phân tích lý thuyết hoặc
nghiên cứu thực nghiệm trên nhiều hệ thống MPC hơn như công việc tương lai. Thứ hai, trong thiết
kế của chúng tôi, T và S chỉ khác nhau bởi các hàm, tức là chúng có cùng kích thước mô hình.
Chúng tôi để lại việc mở rộng sang một mô hình học sinh nhỏ hơn như công việc tương lai.

6 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất một khung để đạt được suy luận mô hình Transformer riêng tư
nhanh và hiệu suất cao với MPC. Các đánh giá cho thấy rằng nó tương thích với nhiều xấp xỉ thân
thiện với MPC và các mô hình Transformer đã được huấn luyện. Chúng tôi đề xuất hai hướng quan
tâm: (1) Phân tích lý thuyết hoặc thực nghiệm trên nhiều hệ thống MPC hơn, và (2) mở rộng về
công thức vấn đề để cho phép kích thước nhỏ hơn của S.

LỜI CẢM ƠN

Các tác giả cảm ơn Qirong Ho đã phân bổ tài nguyên tính toán. Nghiên cứu này được hỗ trợ bởi
NSF IIS1563887, NSF CCF1629559, NSF IIS1617583, NGA HM04762010002, NSF IIS1955532,
NSF CNS2008248, NSF IIS2123952, và NSF BCS2040381.

9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

TÀI LIỆU THAM KHẢO

Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

Donald Beaver. Efficient multiparty protocols using circuit randomization. In Annual International
Cryptology Conference, pp. 420–432. Springer, 1991.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Edward Chou, Josh Beal, Daniel Levy, Serena Yeung, Albert Haque, and Li Fei-Fei. Faster cryptonets: Leveraging sparsity for real-world encrypted inference. arXiv preprint arXiv:1811.09953,
2018.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look
at? an analysis of bert's attention. arXiv preprint arXiv:1906.04341, 2019.

Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training
text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.

Ronald Cramer, Ivan Damgård, and Yuval Ishai. Share conversion, pseudorandom secret-sharing
and applications to secure computation. In Theory of Cryptography Conference, pp. 342–362.
Springer, 2005.

Ivan Damgård, Valerio Pastro, Nigel Smart, and Sarah Zakarias. Multiparty computation from
somewhat homomorphic encryption. In Annual Cryptology Conference, pp. 643–662. Springer,
2012.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.

David Evans, Vladimir Kolesnikov, Mike Rosulek, et al. A pragmatic introduction to secure multiparty computation. Foundations and Trends® in Privacy and Security, 2(2-3):70–246, 2018.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural
languages. arXiv preprint arXiv:2002.08155, 2020.

Oded Goldreich, Silvio Micali, and Avi Wigderson. How to play any mental game, or a completeness
theorem for protocols with honest majority. In Providing Sound Foundations for Cryptography:
On the Work of Shafi Goldwasser and Silvio Micali, pp. 307–328. 2019.

Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2(7), 2015.

Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief
nets. Neural Computation, 18:1527–1554, 2006.

10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, và Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.

Chiraag Juvekar, Vinod Vaikuntanathan, và Anantha Chandrakasan. {GAZELLE}: A low latency framework for secure neural network inference. In 27th USENIX Security Symposium (USENIX Security 18), pp. 1651–1669, 2018.

Marcel Keller, Emmanuela Orsini, và Peter Scholl. Mascot: faster malicious arithmetic secure computation with oblivious transfer. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 830–842, 2016.

Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, và Laurens van der Maaten. Crypten: Secure multi-party computation meets machine learning. Advances in Neural Information Processing Systems, 34:4961–4973, 2021.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.

Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, và Zhuowen Tu. Deeply-supervised nets. In Artificial intelligence and statistics, pp. 562–570. PMLR, 2015.

Jian Liu, Mika Juuti, Yao Lu, và Nadarajah Asokan. Oblivious neural network predictions via minionn transformations. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security, pp. 619–631, 2017.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, và Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015.

Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, và Raluca Ada Popa. Delphi: A cryptographic inference service for neural networks. In 29th USENIX Security Symposium (USENIX Security 20), pp. 2505–2522, 2020.

Payman Mohassel và Peter Rindal. Aby3: A mixed protocol framework for machine learning. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security, pp. 35–52, 2018.

Payman Mohassel và Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine learning. In 2017 IEEE symposium on security and privacy (SP), pp. 19–38. IEEE, 2017.

Pascal Paillier. Public-key cryptosystems based on composite degree residuosity classes. In International conference on the theory and applications of cryptographic techniques, pp. 223–238. Springer, 1999.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.

M Sadegh Riazi, Christian Weinert, Oleksandr Tkachenko, Ebrahim M Songhori, Thomas Schneider, và Farinaz Koushanfar. Chameleon: A hybrid secure computation framework for machine learning applications. In Proceedings of the 2018 on Asia conference on computer and communications security, pp. 707–721, 2018.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, và Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.

Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

Gilad Sharir, Asaf Noy, và Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth? arXiv preprint arXiv:2103.13915, 2021.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.

Iulia Turc, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019.

Sameer Wagh, Divya Gupta, và Nishanth Chandran. Securenn: 3-party secure computation for neural network training. Proc. Priv. Enhancing Technol., 2019(3):26–49, 2019.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

Yongqin Wang, G Edward Suh, Wenjie Xiong, Benjamin Lefaudeux, Brian Knott, Murali Annavaram, và Hsien-Hsin S Lee. Characterization of mpc-based private inference for transformer-based models. In 2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pp. 187–197. IEEE, 2022.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.

Qiang Yang, Yang Liu, Tianjian Chen, và Yongxin Tong. Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1–19, 2019a.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019b.

Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan Liu, và Maosong Sun. Openattack: An open-source textual adversarial attack toolkit. arXiv preprint arXiv:2009.09191, 2020.

Zhilu Zhang và Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in neural information processing systems, 31, 2018.

12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

A PHỤ LỤC

A.1 TRIỂN KHAI HỆ THỐNG MPC CỤ THỂ: CRYPTEN

Trong phần này, chúng tôi cung cấp cách một hệ thống MPC cụ thể (CrypTen) triển khai các thủ tục và hàm cho các mô hình Transformer một cách chi tiết (Knott et al., 2021). Chúng tôi cung cấp một phần chi tiết ở đây để giúp mô tả độ phức tạp của suy luận Transformer trong MPC. Tổng quan hệ thống hoàn chỉnh hơn và chứng minh riêng tư có sẵn trong bài báo CrypTen.

Mô hình đe dọa. CrypTen tuân theo Evans et al. (2018) để giả định rằng các bên là bán trung thực. Dưới giả định này, các bên trung thực rằng họ sẽ tuân theo các giao thức hệ thống. Tuy nhiên, mỗi bên cũng tò mò (tức là bán trung thực), có nghĩa là nó sẽ cố gắng suy ra thông tin về dữ liệu của người khác dựa trên các giá trị mà nó nhận được.

Chia sẻ bí mật CrypTen sử dụng chia sẻ bí mật để triển khai tính toán riêng tư. Một giá trị số thực xf được đầu tiên chuyển đổi thành số nguyên x⁵, sau đó được chia sẻ bí mật với cả hai bên. Chia sẻ bí mật có loại số học hoặc nhị phân. Chia sẻ bí mật số học [x] = {[x]₁; [x]₂} là một tập hợp hai số, trong đó bên đầu tiên giữ [x]₁, và bên thứ hai giữ [x]₂. Chúng được xây dựng với một cặp mặt nạ ngẫu nhiên có tổng bằng không (Cramer et al., 2005) để [x]₁ + [x]₂ = x. Chia sẻ nhị phân ⟨x⟩ được hình thành bởi các chia sẻ bí mật số học của các bit trong x, để phép xor bitwise ⟨x⟩₁ ⟨x⟩₁ = x.

Thủ tục và hàm Trong chia sẻ bí mật số học, để đánh giá riêng tư phép cộng [x] + [y], cả hai bên đơn giản tính toán: [x]ᵢ + [y]ᵢ một cách riêng lẻ. Phép nhân [x][y] được đánh giá bằng cách sử dụng bộ ba Beaver được tạo ra ngoại tuyến ([c]; [a]; [b]) trong đó c = ab (Beaver, 1991). Cả hai bên tính toán và tiết lộ các giá trị trung gian [α] = [x] − [a] và [β] = [y] − [b]. Kết quả cuối cùng được tính bởi [x][y] = [c] + β[b] + α[a] + αβ. Các hàm tuyến tính như nhân ma trận có thể được đánh giá bằng cách sử dụng phép cộng và phép nhân. Các hàm phi tuyến được đánh giá bởi các xấp xỉ số sử dụng phép cộng và phép nhân, như khai triển Taylor.

So sánh yêu cầu chuyển đổi giữa chia sẻ bí mật số học và nhị phân. Chuyển đổi từ [x] sang ⟨x⟩ đầu tiên tạo chia sẻ bí mật nhị phân ⟨[x]ᵢ⟩ từ chia sẻ bí mật số học [x]ᵢ (i = 0; 1), sau đó tính toán ⟨x⟩ = ⟨[x]₁⟩ + ⟨[x]₂⟩ sử dụng mạch cộng. Chuyển đổi từ ⟨x⟩ sang [x] được thực hiện bởi: [x] = ΣᴮᵦN₌₁ 2ᵇ[⟨x⟩⁽ᵇ⁾], trong đó ⟨x⟩⁽ᵇ⁾ là bit thứ b của ⟨x⟩. Hàm so sánh [z < 0] sau đó được đánh giá bởi: (1) chuyển đổi [z] sang ⟨z⟩ (2) tính toán bit dấu ⟨b⟩ = ⟨z⟩ >> (L - 1). (3) Chuyển đổi ⟨b⟩ sang [b].

Chúng tôi nghiên cứu trên cài đặt tiêu chuẩn nơi mỗi tensor được biểu diễn trong 64 bit (tức là L = 64). Mỗi phép nhân yêu cầu một vòng truyền thông để tiết lộ các giá trị trung gian α; β. Mỗi chuyển đổi từ [x] sang ⟨x⟩ yêu cầu log₂ L = 6 vòng truyền thông cho mạch cộng; mỗi chuyển đổi từ ⟨x⟩ sang [x] yêu cầu một vòng để tạo [⟨x⟩⁽ᵇ⁾]. Vì vậy, mỗi so sánh yêu cầu 7 vòng truyền thông. Mỗi max() giữa N phần tử yêu cầu O(log₂(N)) vòng truyền thông, giả định thuật toán tree-reduction.

Chúng tôi cung cấp một ví dụ phép cộng đơn giản ở đây. Hệ số tỷ lệ và kích thước vòng Q được đặt nhỏ để dễ hiểu.

Bảng 7: Ví dụ về tính toán phép cộng riêng tư. Giả sử m là thông điệp thực tế, thì mỗi bên giữ một phần của m sao cho: [m]₁ + [m]₂ = m:

Hành động Bên 1 Bên 2 Ghi chú
Khai báo x x = 1 x = ngẫu nhiên x được cung cấp bởi bên 1
Tạo mặt nạ chia sẻ bí mật cho x [zₓ]₁ = 4 [zₓ]₂ = -4 tổng bằng 0
chia sẻ bí mật x [x]₁ = x + [zₓ]₁ = 3 [x]₂ = [zₓ]₂ = -4 tổng bằng x¹
Khai báo y y = ngẫu nhiên y = 2 y được cung cấp bởi bên 2
Tạo mặt nạ chia sẻ bí mật cho y [zy]₁ = -50 [zy]₂ = 50 tổng bằng 0
chia sẻ bí mật y [y]₁ = [zy]₁ = -50 [y]₂ = y + [zy]₂ = 48 tổng bằng y²
Tính x + y [x + y]ᵢ = [x]₁ + [y]₁ = -47 [x + y]₂ = [x]₂ + [y]₂ = 44
Tiết lộ x + y x + y = [x + y]₁ + [x + y]₂ = 3 x + y = [x + y]₁ + [x + y]₂ = 3 cả hai đều có kết quả đúng

⁵x ∈ Z = QZ được yêu cầu cho các giao thức riêng tư, trong đó Z = QZ là một vòng với Q phần tử.

13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

Ngoài ra, chúng tôi cung cấp một phân tích chi tiết hơn về truyền thông và tải tính toán cho Hình 2 để có cái nhìn toàn diện về mẫu thực thi trong cài đặt MPC.

Bảng 8: Phân tích tính toán hàm so với truyền thông (Đơn vị: giây).

hàm Thời gian truyền thông Thời gian tính toán Tổng thời gian
MatMul 2.5 5.0 7.5
GeLU 9.6 1.4 11.0
Softmax 34.1 5.9 40.0
Khác 0.3 0.2 0.5
Tổng 46.5 12.5 59.0

Đặc biệt, tính toán chỉ chiếm 21% thời gian chạy và truyền thông chiếm 79% thời gian chạy. Do đó, số lượng phép toán dấu phẩy động (FLOP), một công cụ ước lượng phổ biến cho thời gian chạy trong suy luận Transformer văn bản thuần túy, không còn chính xác trong cài đặt MPC. Hệ thống MPC mà chúng tôi sử dụng trong bài báo sử dụng All-reduce để triển khai truyền thông trung gian, nơi cả hai bên có cùng tải truyền thông. Và họ có tải tính toán tương tự (xem ví dụ phép nhân, nơi cả hai bên đều tính toán cùng một hàm cục bộ với chia sẻ bí mật của riêng họ). Vì vậy, phân tích thời gian tương tự cho cả hai bên. Trong bảng trên, chúng tôi báo cáo thống kê từ nhà cung cấp mô hình.

A.2 CHI TIẾT TRIỂN KHAI 2QUAD

Chúng tôi lưu ý rằng, việc triển khai "2Quad" để thay thế softmax yêu cầu chú ý đến tác động của phép toán masking. Ví dụ, việc triển khai mặc định bởi Huggingface Wolf et al. (2019) sẽ dẫn đến vấn đề bùng nổ do masking. Vì vậy, chúng tôi cần thực hiện một phiên bản khác của việc triển khai masking. Chúng tôi mô tả chi tiết dưới đây.

Việc triển khai attention mặc định bởi Huggingface là

Attention(Q; K; V) = softmax(QKᵀ/√dₖ + M{0;-∞})V
= e^(QKᵀ/√dₖ + M{0;-∞}) / Σⱼ₌₁ᴷ e^(QKᵀ/√dₖ + M{0;-∞})ⱼ V.

Nếu chúng ta trực tiếp thay thế e^x bằng (x + c)² như trong xấp xỉ 2Quad, trong đó x = QKᵀ/√dₖ + M{0;-∞} sẽ bùng nổ khi được mask, gây ra vấn đề trong lượt truyền thuận. Để giải quyết vấn đề này, chúng ta có thể đơn giản thay đổi việc triển khai masking từ "thêm một số không hoặc âm vô cùng vào số mũ" thành "nhân một hoặc không với hàm mũ". Tức là,

Attention(Q; K; V) = e^(QKᵀ/√dₖ) · M{1;0} / Σⱼ₌₁ᴷ e^(QKᵀ/√dₖ)ⱼ · M{1;0} V
→ (QKᵀ/√dₖ + c)² · M{1;0} / Σⱼ₌₁ᴷ (QKᵀ/√dₖ + c)²ⱼ · M{1;0} V.

Đó chỉ là một cách triển khai khác của cùng mục đích masking nhưng tránh bùng nổ tại các vị trí masking.

Trong các thí nghiệm của chúng tôi, chúng tôi thử nghiệm thực nghiệm c = 5 và nó hoạt động khá tốt, cho thấy việc lựa chọn hằng số c có thể linh hoạt.

14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

A.3 TÍNH BỀN VỮNG CỦA MÔ HÌNH HỌC SINH

Một số xấp xỉ có thể tăng các hằng số Lipschitz cục bộ, làm giảm tính bền vững. Chúng tôi áp dụng một số cuộc tấn công đối nghịch văn bản thực nghiệm để đánh giá tính bền vững đối nghịch của mô hình BERT-base trước và sau các xấp xỉ (Zeng et al., 2020). Như hiển thị trong Bảng 9, mô hình học sinh có sự gia tăng vừa phải về tỷ lệ thành công tấn công (ASR) trên ba cuộc tấn công dựa trên điểm số. Nhưng mô hình học sinh có ASR thấp hơn với cuộc tấn công dựa trên gradient HotFlip. Xem xét những kết quả này, tác động đến tính bền vững bởi các xấp xỉ là vừa phải về mặt thực nghiệm.

Bảng 9: Độ chính xác sức khỏe (SA) và tỷ lệ thành công tấn công (ASR) chống lại các cuộc tấn công văn bản khác nhau. TextFooler, PWWS, và BERT-ATTACK là các cuộc tấn công dựa trên điểm số, và HotFlip là cuộc tấn công dựa trên gradient. Đối với ASR, thấp hơn là tốt hơn.

SA TextFooler PWWS BERT-Attack HotFlip Tốc độ tăng
Bert-base (giáo viên) 0.93 0.76 0.78 0.88 0.55 1.0x
Bert-base (học sinh) 0.92 0.78 0.82 0.91 0.51 2.2x

A.4 LỰA CHỌN SIÊU THAM SỐ

Đối với các baseline, chúng tôi nghiên cứu tác động của các siêu tham số bằng cách chạy tìm kiếm lưới trên bộ dữ liệu STS-B⁶, với tỷ lệ học từ [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4], kích thước batch từ [256, 128, 64, 32, 16], epoch từ [3, 10, 30, 50, 80, 100, 200]. Chúng tôi hiển thị kết quả tìm kiếm lưới với BERT BASE trong hình 6, 7, và một tìm kiếm lưới nhỏ hơn cho BERT Large và ROBERTA BASE trong Hình 8, 9. Chúng tôi khám phá thực nghiệm rằng tỷ lệ học từ 1e-6, 5e-6, 1e-5, kích thước batch từ 64 và 256, epoch từ 10, 100 cho hiệu suất tốt. Để cho phép các baseline khám phá nhiều siêu tham số hơn, chúng tôi sử dụng tỷ lệ học từ [1e-6, 5e-6, 1e-5, 1e-4], kích thước batch từ [64, 256], epoch từ [10, 30, 100] cho tất cả các bộ dữ liệu Glue. Vì chúng tôi sử dụng độ dài chuỗi 512 cho bộ dữ liệu IMDB, chúng tôi sử dụng kích thước batch 32 để vừa với GPU Tesla V100 16GB của chúng tôi. Chúng tôi cũng khám phá thực nghiệm rằng (1) MPCBert-B w/o fdg (tốt nhất 0.43) không thể mở rộng khi mô hình cơ sở mở rộng sang BERT Large tức là MPCBert-L w/o fdg (tốt nhất 0.08). (2) baseline có lợi từ việc sử dụng các trọng số được huấn luyện trước, tức là MPCBert-B w/o fdg (tốt nhất 0.42) hoạt động tốt hơn MPCBert-B w/o fp, dg (tốt nhất 0.23). (3) MPCFormer w/o fdg có lợi khi mô hình cơ sở trở nên tốt hơn, tức là MPCRoberta-B w/o fdg (tốt nhất 0.62) hoạt động tốt hơn MPCBert-B w/o fdg (tốt nhất 0.42).

Đối với MPCF ORMER, chúng tôi quyết định số lượng epoch theo hàm mất mát MSE cho chưng cất tầng nhúng và Transformer, 5 epoch cho chưng cất tầng dự đoán, và kích thước batch 8 cho các bộ dữ liệu nhỏ (CoLA, MRPC, RTE) và 32 cho những bộ lớn hơn (MNLI, QQP, SST2, STS-B). Chúng tôi giảm thiểu việc điều chỉnh siêu tham số cho MPCF ORMER, vì chúng tôi muốn hiệu suất là một kỳ vọng cho các nhà nghiên cứu tương lai sử dụng MPCF ORMER, những người thích không điều chỉnh siêu tham số. Cụ thể, chúng tôi sử dụng 5 epoch cho MNLI, 5 epoch cho QQP, 10 epoch cho QNLI, 10 epoch cho SST-2, 20 epoch cho MRPC, 30 epoch cho IMDB, 50 epoch cho STS-B, 50 epoch cho CoLA, 50 epoch cho RTE, cho giai đoạn chưng cất tầng nhúng và Transformer.

[Các hình 6-9 hiển thị các biểu đồ kết quả tìm kiếm lưới với các trục x là số epoch và trục y là tương quan (chưa được chia tỷ lệ) cho các cài đặt khác nhau của kích thước batch và tỷ lệ học]

⁶Chúng tôi chọn STS-B vì đó là một nhiệm vụ hồi quy, nơi hiệu suất thay đổi trong một phạm vi lớn.

15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

[Các hình tiếp theo hiển thị kết quả tìm kiếm lưới cho MPCBert-B w/o fdg, MPCBert-L w/o fdg, và MPCRoberta-B w/o fdg trên bộ dữ liệu STS-B với các cài đặt tham số khác nhau]

16
