# 2309.06805.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/federated-learning/2309.06805.pdf
# File size: 1028588 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FedDIP: Federated Learning with Extreme
Dynamic Pruning and Incremental Regularization
Qianyu Long∗, Christos Anagnostopoulos∗, Shameem Puthiya Parambath∗, and Daning Bi†
∗School of Computing Science ,University of Glasgow, UK
2614994L@student.gla.ac.uk, {christos.anagnostopoulos, sham.puthiya }@glasgow.ac.uk
†College of Finance and Statistics ,Hunan University, China
daningbi@hnu.edu.cn
Abstract —Federated Learning (FL) has been successfully
adopted for distributed training and inference of large-scale Deep
Neural Networks (DNNs). However, DNNs are characterized
by an extremely large number of parameters, thus, yielding
significant challenges in exchanging these parameters among
distributed nodes and managing the memory. Although recent
DNN compression methods (e.g., sparsification, pruning) tackle
such challenges, they do not holistically consider an adaptively
controlled reduction of parameter exchange while maintaining
high accuracy levels. We, therefore, contribute with a novel FL
framework (coined FedDIP), which combines (i) dynamic model
pruning with error feedback to eliminate redundant information
exchange, which contributes to significant performance improve-
ment, with (ii) incremental regularization that can achieve
extreme sparsity of models. We provide convergence analysis
of FedDIP and report on a comprehensive performance and
comparative assessment against state-of-the-art methods using
benchmark data sets and DNN models. Our results showcase
that FedDIP not only controls the model sparsity but efficiently
achieves similar or better performance compared to other
model pruning methods adopting incremental regularization
during distributed model training. The code is available at :
https://github.com/EricLoong/feddip.
Index Terms —Federated Learning, dynamic pruning, extreme
sparsification, incremental regularization.
I. I NTRODUCTION
Federated Learning (FL) [1] is a prevalent distributed learn-
ingparadigm due to its ability to tackle learning at scale.
FL plays a significant role in large-scale predictive analytics
by enabling the decentralization of knowledge discovery. FL
contributes towards privacy preservation, which overcomes
fundamental issues of data governance and ownership [2]. Dis-
tributed training and deploying large-scale Machine Learning
(ML) models, i.e., Deep Neural Networks (DNNs), impose
significant challenges due to the huge volumes of training data,
large models, and diversity in data distributions.
Distributed computing nodes, mainly located at the network
edge being as close to data sources as possible, collaboratively
engineer ML models rather than depending on collecting all
the data to a centralized location (data center or Cloud) for
training [3]. This computing paradigm coined Edge Com-
puting, has been successfully applied to various predictive
modeling, mining, and analytics applications, e.g., in finance
[4], healthcare [5] and wireless sensor networks [6].
DNNs are characterized by an extremely large number of
parameters. For instance, the Convolutional Neural Networks(CNN) ResNet50 [7] and VGG16 [8] consist of 27 and 140
million parameters, respectively, while generative AI models,
like GPT-2 have more than 1.5 billion parameters [9]. Evi-
dently, this places a great burden on distributed computing
nodes when exchanging model parameters during training,
tuning, and inference.
Model size reduction (pruning) methods, e.g., [10], [11],
[12] aim to retain the prediction accuracy while reducing
the communication overhead by decreasing the number of
model parameters exchanged among nodes. However, most
pruning methods focus on the compression of model gradients.
Even though they yield high compression rates, they do not
achieve significantly compact models for exchange. But in
general, methods that can produce compact models along with
significant redundancy in the number of DNN weights by
sophisticatedly pruning the weights are deemed appropriate
[13]. In contrast to model gradient compression, model weight
compression significantly shrinks the model size by setting
most of the weights to zero. This is desirable for eliminating
redundancy in model exchange during distributed knowledge
extraction. But often such models result in performance degra-
dation. Therefore, the question we are addressing is: How to
effectively introduce model pruning mechanisms in a decen-
tralized learning setting that is capable of achieving extremely
high compression rates while preserving optimal predictive
performance? We contribute with an efficient method based
ondynamic pruning with error feedback and incremental
regularization , coined FedDIP. FedDIP’s novelty lies in the
principle of adapting dynamic pruning in a decentralized way
by pushing unimportant weights to zeros (extreme pruning)
whilst maintaining high accuracy through incremental reg-
ularization. To the best of our knowledge, FedDIP is the
first approach that combines incremental regularization and
extreme dynamic pruning in FL.
The paper is organized as follows: Section II reports on
related work and our contribution. Section III provides pre-
liminaries in FL and model pruning methods. Section IV
elaborates on the FedDIP framework, while Section V reports
on the theoretical properties of FedDIP and convergence
analysis. Our experimental results in Section VI showcase
the efficiency of FedDIP in distributed learning. Section VII
concludes the paper with future research directions.arXiv:2309.06805v1  [cs.LG]  13 Sep 2023

--- PAGE 2 ---
II. R ELATED WORK & C ONTRIBUTION
A. Model Gradient & Model Weight Sparsification
Expensive and redundant sharing of model weights is a
significant obstacle in distributed learning [14]. The size of
the exchanged models among nodes can be reduced by com-
pression and sparsification. The work in [11] adopts magnitude
selection on model gradients to yield sparsification when using
Stochastic Gradient Descent (SGD). Instead of dense updates
of weights, [10] proposed a distributed SGD that keeps 1%
of the gradients by comparing their magnitude values. The
method in [15] scales up SGD training of DNN via controlling
the rate of weight update per individual weight. [16] develops
encoding SGD-based vectors achieving reduced communica-
tion overhead. [17] proposed the periodic quantized averaging
SGD strategy that attains similar model predictive performance
while the size of shared model gradients is reduced 95%. In
[18], the authors argued that 99% of gradients are redundant
and introduced a deep gradient compression method, which
achieves compression rates in the range 270-600 with sac-
rificing accuracy. The gTop-k gradient sparsification method
in [19] reduces communication cost based on the Top-k
method in [18]. [20] develops a method based on [21] that
adaptively compresses the size of exchanged model gradients
via quantization.
In contrast to gradient sparsification, the shrinkage of the
entire model size is of paramount importance in distributed
learning. It not only eliminates communication redundancy
during training but also enables less storage and inference
time, which makes FL welcome in distributed knowledge sys-
tems. However, so far, only centralized learning adopts model
compression via, e.g., weight pruning, quantization, low-rank
factorization, transferred convolutional filters, and knowledge
distillation [22], with pruning being our focus in this work.
SNIP [23] introduces a method that prunes a DNN model once
(i.e., prior to training) based on the identification of important
connections in the model. [24] proposes a centralized two-
step method that prunes each layer of a DNN via regression-
based channel selection and least squares reconstruction. The
method in [25] prunes CNNs centrally using the Alternating
Direction Method of Multipliers (ADMM). Following [25], the
PruneTrain method [26] uses structured group-LASSO regu-
larization to accelerate CNN training in a centralized location
only. The DPF [27] method allows dynamic management of
the model sparsity with a feedback mechanism that re-activates
pruned weights.
B. Contribution
Most of the approaches in FL take into account only
the communication overhead and thus adopt gradient spar-
sification. Nonetheless, weight sparsification is also equally
important and can lead to accurate distributed sparse models.
Such sparse models are lightweight and, thus, suitable for
storage, transfer, training, and fast inference. As shown in [28],
model weights and gradients averaging policies are equivalent
only when the local number of model training epochs equalsone. FedDIP tries to bridge the gap of weights average pruning
in FL by obtaining highly accurate sparse models through
incremental regularization and reducing communication during
training through dynamic pruning.
To the best of our knowledge in distributed learning,
PruneFL [12] FedDST [29] and LotteryFL [30] methods
attempt model pruning. However, LotteryFL focuses on a
completely different problem from ours. LotteryFL tries to
discover sparse local sub-networks (a.k.a. Lottery Ticket Net-
works) of a base DNN model. In contrast, FedDIP searches
for a sparse global DNN model with mask readjustments
on a central server, as we will elaborate on later. PruneFL
starts with a pre-selected node to train a global shared mask
function, while FedDIP generates the mask function with
weights following the Erd ˝os-Ren ´eyi-Kernel (ERK) distribution
[31], as we will discuss in the later sections. FedDST, as
proposed by Bibikar et al., initially derives a pruning mask
based on the ERK distribution. Subsequent stages involve
layerwise pruning on the global model. The method ensures
efficient training through a prune-regrow procedure, which
maintains a local sparse mask, particularly under non-iid data
distributions. Our technical contributions are:
•An innovative federated learning paradigm, coined Fed-
DIP, combines extreme sparsity-driven model pruning
with incremental regularization.
•FedDIP achieves negligible overhead keeping accuracy
at the same or even higher levels over extremely pruned
models.
•Theoretical convergence and analysis of FedDIP.
•A comprehensive performance evaluation and compar-
ative assessment of FedDIP with benchmark i.i.d. and
non-i.i.d. datasets and various DNN models. Our experi-
mental results reveal that FedDIP, in the context of high
model compression rates, delivers superior prediction
performance compared to the baseline methods and other
approaches found in the literature, specifically, FedAvg
[1], PruneFL [12], PruneTrain [26], FedDST [29], DPF
[27], and SNIP [23].
Notations Definition
N, K N: total number of nodes, where K < N nodes
participated in each training round
nindexes a node; zindexes a DNN layer; n∈[N], z∈[Z]
[N]abbreviates the integer sequence 1,2, . . . , N
Dn, Dn Dataset and its size on node n.
(x, y)∈ Dn x, yare features and labels in node n’s dataset
f(·),∇f(·) Loss function and its derivative
ρn, η Weight percentage and learning rate
ωG,ωn,ω′
n Global, local and pruned local model parameters
T, El, τ, ℓ Global and local rounds, global and local epochs
λ Regularization hyperparameter
⊙ Element-wise (Hadamard) product
s0,st,sp initial sparsity, sparsity at round t, final sparsity
TABLE I: Table of Notations

--- PAGE 3 ---
…
Client 1 Client 2 Client Nf1 f2fNServer node
ω'G
ω'Gω'Gω'Gω'G = agg({ωn})
ω1 ω2ωnωNFig. 1: Illustration of the FedDIP framework:
(1) During the downlink phase, the pruned global model ω′
G
is broadcasted to participating clients.
(2) In the uplink phase, each selected client communicates its
local dense model ωnback to the server for aggregation.
(3) The global mask mGis derived from the global model,
directing the sparse training (DPF) across clients.
III. P RELIMINARIES
A. Federated Learning
For the general notations and definitions, please refer to
Table I. Consider a distributed learning system involving a set
ofNnodes (clients) N={1,2, . . . , N }. LetDn={(x, y)}
be the local dataset associated with a node n∈ N such that
x∈ X ⊂ Rd,y∈ Y ⊂ R, and Dn=|Dn|. In the standard FL
setting, given a subset of K < N nodes Nc⊂ N , the local
loss is given by:
fn(ω) =1
DnX
(x,y)∈DnL(G(ω,x), y) (1)
where ωis the model parameter, Gis the discriminant function
that maps the input space to output space and Lis a loss
function that measures the quality of the prediction, e.g., mean-
squared-error, maximum likelihood, cross-entropy loss. The
global loss function for all the selected nodes n∈ N cis:
f(ω) =X
n∈Ncρnfn(ω),where ρn=DnP
j∈NcDj.(2)
The model training process spans periodically over Tglobal
rounds with Llocal rounds. Let t∈={0,1, . . . , T −1}be
a discrete-time instance during the training process. Then,
τ=⌊t
L⌋Lis the start time of the current global epoch. At
τ, the nodes (clients) receive updated aggregated weights ¯ωτ
from the node responsible for aggregating the nodes’ model
parameters, a.k.a. the server node. The local training at client
nat local epoch l= 1, . . . , L proceeds as:
ω(τ+l)+1
n =ωτ+l
n−ητ+l∇fn(ωτ+l
n), (3)
where η∈(0,1)is the learning rate. The weight averaging
policy on the server node can be written as:
¯ωτ=X
n∈Nρnωτ
n. (4)B. Model Pruning
In centralized learning systems (e.g., in Cloud), where all
data are centrally stored and available, the model pruning [32]
aims to sparsify various connection matrices that represent the
weights of the DNN models. Notably, sparsity , hereinafter
noted by s∈[0,1], indicates the proportion of non-zero
weights among overall weights. A 100% sparse ( s= 1) model
indicates that all the weights are negligible (their values are
close to 0), while a 0% sparse ( s= 0) model stands for the full
model with original weight values. Typically, the reduction of
the number of nonzero weights (pruning) of a DNN model is
achieved using mask functions . A mask function macts like an
indicator function that decides whether the parameter/weight
at a certain position in a layer of a DNN model is zero or
not. The model pruning based on mask functions requires a
criterion to select the parameters to prune. The most common
pruning criterion considers the absolute value of the weights of
each parameter in a layer. Generally, a parameter is removed
from the training process if its absolute value of the weight is
less than a predefined threshold.
On the other hand, model pruning in FL is vital in light of
reducing communication cost in each training round. More-
over, the global number of rounds should be reduced as
this significantly contributes to the overall communication
overhead. Hence, in FL, pruning aims at extreme model
compression rates, i.e., s≥0.8with a relatively small com-
promise in prediction accuracy. It is then deemed appropriate
to introduce a distributed and adaptive pruning method with
relatively high and controlled DNN model sparsity, which
reduces communication costs per round along with ensuring
convergence under high sparsity with only a marginal decrease
in prediction accuracy.
The pruning techniques are typically categorized into three:
pruning before training (e.g., SNIP [23]), pruning during
training (e.g., PruneTrain [26], FedDST [29], DPF [27] and
PruneFL [12]), and pruning after training . In this work, we
concentrate on the two former techniques, which deal with
efficient model training. The pruning after training approach
offers limited utility in the context of distributed learning.
The two commonly employed techniques for pruning are: (i)
Regularization-based Pruning (RP) and (ii) Importance-based
Pruning (IP) [33]. The interested reader may refer to [24],
[25], [33] and the references therein for a comprehensive
survey of RP and IP techniques. RP uses intrinsic sparsity-
inducing properties of L1(Manhattan distance) and L2(Eu-
clidean distance) norms to limit the importance of different
model parameters. The sparsity-inducing norms constrain the
weights of the unimportant parameters to small absolute values
during training. Moreover, RP can effectively constrain the
weights into a sparse model space via tuning the regularization
hyperparameter λ. Whereas in IP, parameters are pruned purely
based on predefined formulae that are defined in terms of
the weights of the parameters or the sum of the weights.
IP techniques were originally proposed in the unstructured
pruning settings that can result in sparse models not capable of

--- PAGE 4 ---
speeding up the computation. Even though RP techniques are
considered superior to IP techniques, they struggle with two
fundamental challenges: ( C1) The first challenge pertains to
controlling the sparsity value sduring pruning. For example,
in PruneTrain [26], employing a pruning threshold value of
10−4to eliminate model parameters does not guarantee the
delivery of a sparse model. ( C2) The second challenge is
dynamically tuning a regularization parameter λ. A large λ
leads to model divergence during training, as the model may
excessively lean towards penalty patterns. By adding regular-
ization terms in DNN training traditionally aims for overfitting
issues. However, additional regularization for prunable layers
is required for RP, which is the core difference between
traditional training and RP-based training.
IV. T HEFEDDIP F RAMEWORK
The proposed FedDIP framework integrates extreme dy-
namic pruning with error feedback and incremental regulariza-
tion in distributed learning environments. Figure 1 illustrates
a schematic representation of the FedDIP, which will be
elaborated on in this section. FedDIP attempts to effectively
train pruned DNN models across collaborative clients ensuring
convergence by addressing the two challenges C1and C2
prevalent in RP-based methods discussed in Section III-B.
The dynamic pruning method (DPF) in [27] demonstrates
improved performance in comparison with other baselines
under high sparsity. Given the SGD update scheme, the model
gradient in DPF is computed on the pruned model as:
ωt+1=ωt−ηt∇f(ω′
t) =ωt−ηt∇f(ωt⊙mt), (5)
taking into account the error feedback (analytically):
ωt+1=ωt−ηt∇f(ωt+et), (6)
where et=ω′
t−ωt. In (5), ⊙represents the Hadamard
(element-wise) product between the two model weights, ωt
represents the entire model parameters, ω′
trepresents the
pruned model parameters, and mis the adopted mask function
used for pruning as in, e.g., in [12], [26], and [27]. The
mask is applied on the model parameters ωtto eliminate
weights according to the magnitude of each weight, thus,
producing the pruned ω′
t. Applying the gradient, in this case,
allows recovering from errors due to premature masking out
of important weights, i.e., the rule in (5) takes a step that best
suits the pruned model (our target). In contrast, all the pruning
methods adopted in FL, e.g., [12], led to sub-optimal decisions
by adopting the rule:
ωt+1=ω′
t−ηt∇f(ω′
t). (7)
One can observe that the update rule in (5) retains more
information, as it only computes gradients of the pruned
model, compared to the update rule in (7). This is expected to
yield superior performance under high sparsity.
Moreover, it is known that the multi-collinearity1challenge
is alleviated by the Least Absolute Shrinkage and Selection
1In multi-collinearity, two or more independent variables are highly corre-
lated in a regression model, which violates the independence assumption.Operator (LASSO). LASSO performs simultaneous variable
selection and regularisation [34]. LASSO adds the L1reg-
ularization term to the regression loss function, providing a
solution to cases where the number of model parameters is
significantly larger than the available observations. Apparently,
this is the case in DNNs, which typically involve millions
of parameters with only tens of thousands of observations.
The two challenges reported in Section III-B deal with se-
lecting appropriate dynamic policies for sparsity control and
regularization hyperparameter λ. To address the challenge C1,
we dynamically drop the least s·100% percentile according
to weights magnitude. The challenge C2is addressed by in-
crementally increasing the regularization parameter departing
from the principles of LASSO regression. It is also evidenced
in [33] that growing regularization benefits pruning. Based
on these observations, we establish the FedDIP algorithm
to maintain the predictive model performance under extreme
sparsity with incremental regularization and dynamic pruning.
To clarify terminology, we refer to our algorithm that directly
applies dynamic pruning as ‘FedDP’ (addressing challenge
C1), while ‘FedDIP’ represents the variant that also adds
incremental regularization (addressing both challenges C1and
C2). Collectively, we refer to these variants as ‘FedD(I)P’.
Each node n∈ N first trains a local sparse DNN model,
which contains weights with relatively small magnitudes (see
also Fig. 1). Then, the node noptimizes the proposed local
incrementally regularized loss function at round tas:
fn(ωt) =1
DnX
(x,y)∈DL(G(ωt,x), y) +λtZX
z=1∥ω(z)
t∥2,(8)
where the step tdependent regularization parameter λtcon-
trols the degree of model shrinkage, i.e., the sparsity , and
Zis the number of the DNN layers (this, of course, de-
pends on the DNN architecture; in our experiments, it is the
sum of convolutional and fully connected layers). The norm
∥ω(z)∥2= (P
k|ω(z)
k|2)1/2is the L2norm of the pruned
zthlayer of model weights ω(z). We then introduce the
incremental regularization over λtbased on the schedule:
λt=

0 if0≤t <T
Q
......
λmax·(i−1)
Qif(i−1)T
Q≤t <iT
Q
......
λmax(Q−1)
Qif(Q−1)T
Q≤t≤T(9)
with quantization step size Q > 0. The influence of Qon
regularization is controlled by adapting λmax. Such step size
divides the regularization parameter space fromλmax
Qtoλmax
to achieve a gradual increase of regularization at everyT
Q
rounds. In addition, each node nadopts dynamic pruning to
progressively update its local model weights ωτ+L
n to optimize
(8) as:
ωτ,l+1
n =ωτ,l
n−ητ∇fn(ω′(τ,l)
n), (10)

--- PAGE 5 ---
where ω′(τ+l)
n is obtained through pruning based on a global
mask function mτgenerated by the server node. Moreover,
our gradual pruning policy modifies the sparsity update policy
per round from [35] by incrementally updating the sparsity as:
st=sp+ (s0−sp)
1−t
T3
, (11)
where strepresents the sparsity applied to the model pruning
at round t,s0is the initial sparsity, and spis the desired/target
sparsity. Notably, in our approach s0is strictly non-zero; this
can be a moderate sparsity of s0= 0.5. Such adaptation
differentiates our method from [35], where s0= 0. In essence,
we permit the sparsity to increment from moderate to extreme
levels throughout the process. If considering s0>0, the layer-
wise sparsity of the initial mask follows the ERK distribution
introduced in [31]. At the end of a local epoch l, the server
node collects K < N model weights ωτ+l
nfrom the selected
nodes n∈ N c, and calculates the global weights average as:
¯ωτ+l
G=X
n∈Nρnωτ+l
n. (12)
In addition, the mτmask function is generated based on
pruning on ¯ωτ+l
Gwith current sparsity sτ. The FedDIP process
is summarized in Algorithm 1, where only pruned models
are exchanged from server to nodes, while pruning is locally
achieved in the clients. Note: FedDIP achieves data-free
initialization and generalizes the DPF [27] in dynamic pruning
process. When we set initial s0= 0 and no incremental
regularization, i.e., λt= 0,∀t, then FedDIP reduces to DPF.
Moreover, we obtain our variant FedDP if we set λt= 0,∀t
withs0>0w.r.t. ERK distribution.
Remark 1. Trade-off between Pruning and Fine-tuning:
The FedDIP approach introduces a reconfiguration horizon,
denoted as R, during the model training phase to periodically
update the mask function. Specifically, the mask function mτis
updated at every Rglobal round, i.e., when τmod R= 0,
to ensure a consistent and smooth accuracy learning curve.
The value of this horizon is determined empirically. Potential
Outcomes of Insufficient Pruning: If the mask function remains
unchanged throughout the horizon T, there’s a risk that the
model could converge to a local optimum. Consequences
of Insufficient Fine-tuning: Conversely, if the mask function
undergoes frequent updates, the changes in the model might
not align with the alterations in the sparse model structure.
Remark 2. Integration of Incremental Regularization and
DPF: Differing from the approach in [33], which centralizes
increasing penalty factors on pre-trained models, FedDIP
initiates this from the outset within a distributed learning
context. The integration of incremental regularization with
DPF offers advantages, primarily because DPF obviates the
need for post-pruning fine-tuning, making it preferable to one-
shot pruning methods like SNIP .
V. T HEORETICAL & C ONVERGENCE ANALYSIS
In this section, we provide a theoretical analysis of FedDIP
including the convergence Theorem 1 ensuring stability inAlgorithm 1 The FedDIP Algorithm
Input: Nnodes; Tglobal rounds; Ellocal rounds; initial and
target sparsity s0andsp; maximum regularization λmax;
quantization step Q; reconfiguration horizon R
Output: Global pruned DNN model weights ω′
G
1://Server initiliazation
2:ifs0>0then
3: Server initializes global mask m0(ERK distribution)
4:end if
5://Node update & pruning
6:forglobal round τ= 1, . . . , T do
7: Server randomly selects Knodes Nc⊂ N
8: forselected node n∈ N cin parallel do
9: Receive pruned weights ω′(τ−1)
G from server node
10: Obtain mask mτ−1fromω′(τ−1)
G
11: Trainωτ
noverElrounds on data Dnusing (10)
12: ifincremental regularization is chosen then
13: Optimize (8) with incremental λτin (9)
14: else
15: Optimize (1)
16: end if
17: end for
18: //Server update, aggregation & reconfiguration
19: Server receives models and aggregates ωτ
Gin (12)
20: ifτmod R== 0 then
21: Reconfigure global mask mτbased on pruning ωτ
G
22: end if
23: Server prunes global model with mτand obtains ω′(τ)
G
24: Server node returns ω′(τ)
Gto all nodes.
25:end for
training models w.r.t. incremental regularization and dynamic
extreme pruning. Note for Proofs :The proofs of our Theorem
1 and lemmas are in the Appendix A
At each global round t∈ {1, . . . , T },Kout of Nnodes
participate, each one selected with probability ρnaligned with
[36], [37] andPN
n=1ρn= 1. Letωt
nandω′(t)
nbe the weights
and pruned ones at round ton node n, respectively, with
ω′(t)
n=ωt
n⊙mt. (13)
Let also vt
nand˜vt
nbe the expected and estimated gradients
att, respectively, on node n. Based on ω′(t)
n, we obtain:
v′(t)
n=∇f(ω′(t)
n)while ˜v′(t)
nis the estimated one. The global
aggregated model for FedAvg is:
¯ωt=1
KX
n∈Ncωt
n, (14)
while before the server sends the model, it is pruned as
¯ω′(t)=1
KX
n∈Ncωt
n⊙mt. (15)

--- PAGE 6 ---
The global estimated aggregated gradient and expected global
gradient, respectively, are:
˜ vt=1
KX
n∈Nc˜vt
nand¯ vt=1
KX
n∈Ncvt
n. (16)
Similarly, for DPF, we have that:
˜ v′(t)=1
KX
n∈Nc˜v′(t)
nand¯ v′(t)=1
KX
n∈Ncv′(t)
n. (17)
In FedAvg, ¯ωtis updated as: ¯ωt+1=¯ωt−ηt˜vt, while the
update rule based on DPF at node nis:
ωt+1
n=ωt
n−ηt˜v′(t)
n, (18)
where ωt
n=¯ω′(t). Similarly, ¯ωt+1is updated as:
¯ωt+1=¯ω′(t)−ηt˜v′(t). (19)
Definition 1. According to [27], the quality of pruning is
defined by the parameter δt∈[0,1]as:
δt:=∥ωt−ω′(t)∥2
F
∥ωt∥2
F(20)
where ∥.∥2
Fis the square of Frobenius matrix norm. δt
indicates the degree of information loss by pruning in terms
of magnitude. A smaller δtstands for less information loss.
Definition 2. Following the Definition 1in [38], a mea-
surement γof non-i.i.d. (non-independent and identically
distributed) data is defined as follows:
γ=PN
n=1pn∥∇fn(ω)∥2
∥PN
n=1pn∇fn(ω)∥2, (21)
withγ≥1;γ= 1 holds in i.i.d case.
We list our assumptions for proving the convergence of
FedDIP in the learning phase.
Assumption 1. L−Smoothness .,∀ωt1,ωt2∈Rd,L∈R
f(ωt1)≤f(ωt2) + (ωt1−ωt2)⊤∇f(ωt2) +L
2∥ωt1−ωt2∥2
Assumption 2. µ−Lipschitzness .∀ωt1,ωt2∈Rdandµ∈R
∥f(ωt1)−f(ωt2)∥ ≤µ∥ωt1−ωt2∥ (22)
Assumption 3. Bounded variance for gradients . Following
Assumption 3 in [37], the local model gradients on each node
nare self-bounded in variance:
E[∥˜vt
n−vt
n∥2]≤σ2
n. (23)
Assumption 4. Bounded weighted aggregation of gradients.
Following Assumption 4 in [38], the aggregation of local
gradients at time tare bounded as:
∥NX
n=1ρnvt
n∥2≤G2, (24)
wherePN
n=1ρn= 1andPN
n=1ρnvt
nstands for the weighted
aggregation of local gradients; G∈R.Theorem 1 (FidDIP Convergence) .Consider the Assumptions
1, 2 and 3, Lemmas 1, 2, 3, and let ηt=1
tL,L > 0. Then,
the convergence rate of the FedDIP process is bounded by:
1
TTX
t=1∥∇f(¯ω′(t))∥2≤2LE(f(ω1)−f∗)+
2LTX
t=1[µE[p
δt+1∥¯ωt+1∥] +π2
3L2χ, (25)
where f(ω1)andf∗stand for the initial loss and the final
convergent stable loss, with χ=(γ−1)L2+L
2KPN
n=1ρnσ2
n+
(γ−1)γE2
lL2G2
2, and γdefined in Definition 2.
Proof. Refer to ‘Note for Proofs’ at the beginning of this
section.
In Theorem 1, the first term of the right-hand side of the
inequality (25) denotes the gap between the initial and final
loss, while χgoes to zero as K≫1and the i.i.d. case
assumption holds. This also suggests that non-i.i.d. case re-
sults in large boundaries. The quantity1
TPT
t=1∥∇f(¯ω′(t))∥2
is bounded by the loss produced by pruning. Overall, the
convergence result shows that the L2norms of the pruned
gradients parameters vanish over time, which indicates that a
stable model is obtained at the end (recall, a stable gradient
vector enables a small change on the model under SGD).
VI. E XPERIMENTAL EVALUATION
A. Experimental Setup
Datasets and Models: We experiment with the datasets
Fashion-MNIST [39], CIFAR10 , and CIFAR100 [40]. Fashion-
MNIST consists of 60,000 training and 10,000 test 28x28
grayscale images labeled from 10 classes. Both of CIFAR
datasets consist of 50,000 training and 10,000 test 32x32
color images; in CIFAR10 andCIFAR100 there are 10 classes
(6000 images per class) and 100 classes (600 images per
class), respectively. We consider the i.i.d. (independent and
identically distributed) case to compare all the algorithms
and extend FedDIP to be applied for non-i.i.d. cases. To test
and compare the efficiency of FedDIP, we use different well-
known CNN architectures: LeNet-5 [41], AlexNet [42] and
Resnet-18 [7] as backbone (dense or unpruned) models, with
the baseline FedAvg [1] and the pruning baselines PruneFL
[12], PruneTrain [26], FedDST [29], DPF [27] (equivalent
to FedDP as discussed above), and SNIP [23]. For the non-
i.i.d. case, we adopt the pathological data partition method
in [1], which assigns only two classes for each node. We
merge FedDIP with FedProx [43], a generalization and re-
parametrization of FedAvg to address the heterogeneity of data
(coined FedDIP+Prox), and compare with baseline FedAvg
and FedProx. Our target is to evaluate FedDIP’s accuracy,
storage, and communication efficiency in FL environments
under extreme sparsity.
Configurations: Table II details our configurations. For
PruneFL, FedDST, and PruneTrain, we experimentally deter-
mined the optimal reconfiguration intervals Rto be 20,20, and

--- PAGE 7 ---
1, respectively, to ensure the best possible model performance;
the same for step size Qfor all models. Especially, the
annealing factor for FedDST is set as 0.5. As SNIP prunes the
model before training, the global mask is pruned via one-shot
achieving the target sparsity sp. We used grid-search to fix the
penalty factor for PruneTrain ranging from 10−1to10−5for
different experiments. When necessary, other hyperparameters
were set to match ours. In non-i.i.d. case, the penalty for
the proximal term in FedProx is determined via grid-search
ranging from 10−1to10−5. FedDIP+Prox adopts the optimal
combination of penalty values for FedDIP and FedProx.
Hardware: Our FedDIP framework and experiments are
implemented and conducted on GeForce RTX 3090s GPUs in
the institution’s HPC environment.
B. Performance Under Extreme Sparsity
To demonstrate the performance of FedDIP and other base-
line methods under extreme sparsity, we set target sp= 0.9
for both Fashion-MNIST andCIFAR10 tasks and sp= 0.8for
theCIFAR100 task. Notably, as sp= 0.9causes divergence
during the training of AlexNet with SNIP, we adjust spto0.8
for SNIP in this particular case.
1) Accuracy: Figures 2a, 3a, and 4a demonstrate that
FedDIP surpasses other baselines in achieving the highest
top-1 accuracy (ratio of the correctly classified images) while
maintaining the same extreme target sparsity. As indicated in
Table III, to attain target sparsity of sp= 0.9andsp= 0.8
respectively, FedDIP only compromises LeNet-5 andResNet-
18model accuracy by 1.24% and1.25%, respectively. For
AlexNet , FedDIP can even improve model performance 0.7%,
compared with FedAvg with sp= 0.9.
2) Cumulative Communication & Training Cost: To make
a fair comparison of cumulative communication cost during
training (amount of information exchanged in MB) w.r.t. a
fixed budget, we showcase the relationship between commu-
nication cost and accuracy. Figures 2b, 3b, 4b, and specifically
Table IV present a comprehensive overview, emphasizing that
FedDIP, when provided with adequate communication cost
(budget), effectively prunes the model across all experiments
outperforming the other models. This indicates the trade-off
between model performance and communication/training cost.
FedDIP demonstrates comparable communication efficiency to
other baselines, principally due to the minimal decrement in
model performance. Through our experiments, it is evidenced
that FedDIP achieves optimally pruned models under condi-
tions of extreme sparsity, while incurring less or equivalent
communication costs compared to FedAvg. Even in the early
stages (i.e., in restricted budget cases), FedDIP manages to
match the communication efficiency of other pruning methods
in the CIFAR experiments. This underscores the capacity of
our approach to effectively balance model performance and
communication expenditure. All in all, FedDIP introduces
only minor computational overhead due to the incremental
regularization, while achieving high accuracy compared to
baselines. This computational requirement is on par with that
of PruneTrain, PruneFL, and SNIP, given the same sparsity
0 200 400 600 800 1000
Rounds0.10.20.30.40.50.60.70.80.9Top 1 AccuracyLeNet5 on Fashionmnist
FedAvg
FedDP
FedDIP
PruneFL
SNIP
PruneTrain
FedDST(a) Test accuracy.
0 1000 2000 3000 4000 5000
Communication Cost (in MBs)0.10.20.30.40.50.60.70.80.9Top 1 AccuracyLeNet5 on Fashionmnist
FedAvg
FedDP
FedDIP
PruneFL
SNIP
PruneTrain
FedDST
(b) Test accuracy vs. communication budget.
Fig. 2: Fashion-MNIST experiment with LeNet-5.
at each epoch. A slight increase in computational cost can
be justified by the improvements achieved in the final model
performance considering extremely high sparsity. The size of
the pruned CNN models (Table III) has been significantly
reduced ( ∼1 order of magnitude) from the un-pruned models
in FedAvg.
3) Experiments with non-i.i.d. data: As shown in Table
V, our methodology exhibits strong adaptability to FedProx
(non-pruning), yielding commendable results on non-i.i.d.
data. When juxtaposed with FedAvg, our approach manages
to maintain comparable results even after pruning 90% of
model parameters, albeit at a slight trade-off of 1-2% in
model accuracy in the experiments with LeNet-5 andAlexNet .
Across a span of T= 1000 rounds, FedDIP emerges as the
superior performer in terms of top-1 accuracy, particularly at
sparsity sp= 0.8inResNet-18 . This comprehensive suite of
results underscores the adaptability of FedDIP in effectively
managing non-i.i.d. cases, even in extreme sparsity.
C. FedDIP Sparsity Analysis
1) Layerwise sparsity: Figure 5 shows the sparsity per
layer of ResNet-18 ( sp= 0.8), LeNet-5 ( sp= 0.9), and
AlexNet ( sp= 0.9). Notably, the first layers of all models
are the least pruned ( 0.3≤s≤0.4), which is attributed to
their significant role in general feature extraction. Furthermore,
there is a correlation between the number of weights per layer
and the corresponding sparsity level. This stems from the

--- PAGE 8 ---
Datasets Fashion-MNIST CIFAR10 CIFAR100
DNN/CNN Model LeNet-5 AlexNet ResNet-18
Number of pruning layers ( Z) 5 8 18
Initial learning rate ( η0) 0.01 0.1 0.1
Number of clients per round ( K)5(out of 50) 5(out of 50)5(out of 50)
Batchsize in SGD 64 128 128
Initial sparsity ( s0) 0.5 0.5 0.05
Global rounds ( T) 1,000 1,000 1,000
Reconfiguration interval ( R) 5 5 5
Regularization step size ( Q) 10 10 10
Local round ( El) 5 5 5
Maximum penalty ( λmax) 10−310−35·10−3
TABLE II: Configuration Table
0 200 400 600 800 1000
Rounds0.10.20.30.40.50.60.70.8Top 1 AccuracyAlexNet on CIFAR10
FedAvg
FedDP
FedDIP
PruneFL
SNIP sp=0.8
PruneTrain
FedDST
(a) Test accuracy.
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Communication Cost (in MBs)1e60.10.20.30.40.50.60.70.8Top 1 AccuracyAlexNet on CIFAR10
FedAvg 
FedDP
FedDIP
PruneFL
SNIP (sp=0.8)
PruneTrain
FedDST
(b) Test accuracy vs. communication budget.
Fig. 3: CIFAR10 experiment with AlexNet.
initial ERK distribution, which allocates a higher degree of
sparsity to layers containing more weights, although we adopt
global magnitude pruning in a later process. Such correlation
is remarkable in both convolutional and fully-connected layers
of the models. In convolutional layers, the correlations are
found to be perfectly linear for LeNet-5 with a correlation
coefficient ϱ≃1, for AlexNet we obtain ϱ= 0.86, while for
ResNet-18 ϱ= 0.8. For fully-connected layers, since only one
exists in ResNet-18 , we obtain ϱ= (0.91,0.82)forLeNet-5 ,
AlexNet , respectively. These findings highlight the dependency
of layerwise sparsity and the number of weights per layer,
reflecting the influence of the ERK distribution in FedDIP’s
initialization.
0 200 400 600 800 1000
Rounds0.00.10.20.30.40.50.60.7Top 1 AccuracyResNet18 on CIFAR100
FedAvg
FedDP
FedDIP
PruneFL
SNIP
PruneTrain
FedDST(a) Test accuracy.
0.0 0.5 1.0 1.5 2.0 2.5
Communication Costs (in MBs)1e60.00.10.20.30.40.50.60.7Top 1 AccuracyResNet18 on CIFAR100
FedAvg
FedDP
FedDIP
PruneFL
SNIP
PruneTrain
FedDST
(b) Test accuracy vs. communication budget.
Fig. 4: CIFAR100 experiment with ResNet18.
2) FedDIP in extreme sparsity: We examine the efficiency
of FedDIP under varying conditions of extreme sparsity. For
Fashion-MNIST andCIFAR10 experiments, we investigate two
additional extreme sparsity levels sp= 0.95andsp= 0.99,
and for CIFAR100 experiments, we investigate sp= 0.9and
sp= 0.95. These conditions provide a robust assessment
of FedDIP’s performance across a range of extreme sparsity.
As shown in Figure 6, under extreme sparsity like 0.95and
0.99, the largest drops ∆in classification accuracy are only
∆ = 6 .97%,∆ = 5 .03%, and ∆ = 8 .08%, respectively. This
also comes with further 90%, 89%, and 74% reduction on
LeNet-5, Alex-Net, and ResNet-18 model sizes, respectively.
This indicates (i) FedDIP’s efficiency in storing and managing
trained and pruned models as well as (ii) efficiency in inference

--- PAGE 9 ---
TABLE III: Test Accuracy ( top-1 )
Model Performance ( %)1with target sparsity sp
Model LeNet ;sp=.9 AlexNet ;sp=.9 ResNet ;sp=.8
FedAvg 89.50 (.09) 85.07 (.13) 70.92 (.10)
FedDP 88.06 (.08) 84.81 (.18) 69.23 (.14)
FedDIP 88.26 (0.09) 85.14 (.22) 69.67 (.10)
PruneFL 86.00 (.10) 81.64 (.17) 68.17 (.20)
SNIP 86.08 (.15) 80.10 (.15) 51.46 (.11)
PruneTrain 84.36 (.10) 79.73 (.10) 69.39 (.08)
FedDST 80.37 (.20) – 68.06 (.20)
# param.(FedAvg) 62K 23.3M 11.2M
# param.(pruned) 6.1K 2.3M 2.2M
1Mean accuracy; standard deviation in ‘()’.
TABLE IV: Communication Efficiency
Model Performance ( %) with communication budget
Case LeNet-5(1)AlexNet(2)ResNet-18(3)
FedAvg 86.76 78.98 70.06
FedDP 86.54 82.29 69.10
FedDIP 86.62 82.58 69.57
PruneFL 85.57 81.73 68.4
SNIP 86.32 80.11 51.63
PruneTrain 82.68 78.16 69.42
FedDST 80.37 – 68.06
Communication budget(1)4·103MB,(2)1.8·106MB,(3)2·106MB
tasks (after training) due to relatively small models. All in
all, the pruned DNN models’ performance is relatively high
with small accuracy drops and high model compression (92%)
across different tasks.
VII. C ONCLUSIONS
We propose FedDIP, a novel FL framework with dynamic
pruning and incremental regularization achieving highly ac-
curate and extremely sparse DNN models. FedDIP gradually
regularizes sparse DNN models obtaining extremely com-
pressed models that maintain baseline accuracy and ensure
controllable communication overhead. FedDIP is a data-free
initialization method based on ERK distribution. We provide
a theoretical convergence analysis of FedDIP and evaluate it
across different DNN structures. FedDIP achieves comparable
and higher accuracy against FL baselines and state-of-the-
art FL-based model pruning approaches, respectively, over
extreme sparsity using benchmark data sets (i.i.d. & non-
i.i.d. cases). Our agenda includes addressing heterogeneity in
personalized FL environments.
ACKNOWLEDGEMENT
The authors would like to express their sincere gratitude to
Dr. Fani Deligianni for her invaluable insights and discussions
during peer communications.
This work is partially funded by the EU Horizon Grant ‘In-
tegration and Harmonization of Logistics Operations’ TRACE
(#101104278) and ’National Natural Science Foundation of
China’ (NSFC) under Grant #72201093.TABLE V: Extension to non-i.i.d. data
Model Performance1(%) (non-i.i.d. case)
Case LeNet-5 AlexNet ResNet-18
FedAvg 76.42(0.28) 61.59 (0.73) 16.44 (0.49)
FedProx 76.63 (0.34) 65.74 (0.26) 18.48 (0.91)
FedDIP+Prox 74.49 (0.09) 60.47 (0.52) 19.22 (0.8)
1Mean of the highest five top-1 test accuracy during Trounds.
L1L2L3L4L5L6L7L8L9L10L11L12L13L14L15L16L17L18
Layers of ResNet-18020406080Sparsity (%)Layerwise Sparsity of ResNet-18
(a) Distribution of layer sparsity; ResNet-18.
f0w f3w f6w f8w f10w c1w c4w c6w
Layers of AlexNet020406080100Sparsity (%)Layerwise Sparsity of AlexNet
(b) Distribution of layer sparsity; AlexNet.
f0w f3w c0w c2w c4w
Layers of LeNet-5020406080Sparsity (%)Layerwise Sparsity of LeNet-5
(c) Distribution of layer sparsiy; LeNet-5.
Fig. 5: Layerwise pruning sparsity; f0wstands for ( f)eatures
layer, layer index (e.g, 0), and ( w)eights, respectively. cstands
for the fully-connected classifier layer (the same notation is
used for other layers). ResNet-18 consists of 18 pruning layers.
REFERENCES
[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in Artificial intelligence and statistics . PMLR, 2017, pp. 1273–
1282.
[2] G. A. Kaissis, M. R. Makowski, D. R ¨uckert, and R. F. Braren, “Secure,

--- PAGE 10 ---
LeNet
sp=.9LeNet
sp=.95LeNet
sp=.99AlexNet
sp=.9AlexNet
sp=.95AlexNet
sp=.99ResNet
sp=.8ResNet
sp=.9ResNet
sp=.95
Model and Sparsity Level020406080Accuracy (%) -1.24
  -1.66
 -6.97
 0.07
 -3.74
 -5.03
 -1.25
 -5.37
 -8.08
Model Performance at Different Sparsity LevelsFig. 6: FedDIP performance on extreme sparsity values.
privacy-preserving and federated machine learning in medical imaging,”
Nature Machine Intelligence , vol. 2, no. 6, pp. 305–311, 2020.
[3] Q. Yang, Y . Liu, Y . Cheng, Y . Kang, T. Chen, and H. Yu, “Federated
learning,” Synthesis Lectures on Artificial Intelligence and Machine
Learning , vol. 13, no. 3, pp. 1–207, 2019.
[4] G. Long, Y . Tan, J. Jiang, and C. Zhang, “Federated learning for open
banking,” in Federated learning . Springer, 2020, pp. 240–254.
[5] J. Xu, B. S. Glicksberg, C. Su, P. Walker, J. Bian, and F. Wang,
“Federated learning for healthcare informatics,” Journal of Healthcare
Informatics Research , vol. 5, no. 1, pp. 1–19, 2021.
[6] S. Niknam, H. S. Dhillon, and J. H. Reed, “Federated learning for
wireless communications: Motivation, opportunities, and challenges,”
IEEE Comm. Magazine , vol. 58, no. 6, pp. 46–51, 2020.
[7] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in IEEE CVPR , 2016, pp. 770–778.
[8] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.
[9] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” 2019.
[10] A. F. Aji and K. Heafield, “Sparse communication for distributed
gradient descent,” in EMNLP’17 , 2017, pp. 440–445.
[11] D. Alistarh, T. Hoefler, M. Johansson, N. Konstantinov, S. Khirirat,
and C. Renggli, “The convergence of sparsified gradient methods,”
NeurIPS’18 , vol. 31, 2018.
[12] Y . Jiang, S. Wang, V . Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and
L. Tassiulas, “Model pruning enables efficient federated learning on edge
devices,” IEEE TNNLS , 2022.
[13] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge
intelligence: Paving the last mile of artificial intelligence with edge
computing,” Proc. of IEEE , vol. 107, no. 8, pp. 1738–1762, 2019.
[14] T. Li, A. K. Sahu, A. Talwalkar, and V . Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Proc. Maga-
zine, vol. 37, no. 3, pp. 50–60, 2020.
[15] N. Strom, “Scalable distributed dnn training using commodity gpu cloud
computing,” in 16th Intl Conf Speech Comm. Assoc. , 2015.
[16] J. Kone ˇcn`y and P. Richt ´arik, “Randomized distributed mean estimation:
Accuracy vs. communication,” Frontiers in Applied Mathematics and
Statistics , vol. 4, p. 62, 2018.
[17] P. Jiang and G. Agrawal, “A linear speedup analysis of distributed
deep learning with sparse and quantized communication,” NeurIPS’18 ,
vol. 31, 2018.
[18] Y . Lin, S. Han, H. Mao, Y . Wang, and W. J. Dally, “Deep Gradient
Compression: Reducing the communication bandwidth for distributed
training,” in ICLR , 2018.
[19] S. Shi, K. Zhao, Q. Wang, Z. Tang, and X. Chu, “A convergence analysis
of distributed sgd with communication-efficient gradient sparsification.”
inIJCAI , 2019, pp. 3411–3417.
[20] J. Sun, T. Chen, G. Giannakis, and Z. Yang, “Communication-efficient
distributed learning via lazily aggregated quantized gradients,” Advances
in Neural Information Processing Systems , vol. 32, 2019.[21] T. Chen, G. Giannakis, T. Sun, and W. Yin, “Lag: Lazily aggregated
gradient for communication-efficient distributed learning,” NeurIPS’18 ,
vol. 31, 2018.
[22] Y . Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model
compression and acceleration for deep neural networks,” arXiv preprint
arXiv:1710.09282 , 2017.
[23] N. Lee, T. Ajanthan, and P. H. Torr, “Snip: Single-shot network
pruning based on connection sensitivity,” ICLR 2019 arXiv preprint
arXiv:1810.02340 , 2018.
[24] Y . He, X. Zhang, and J. Sun, “Channel pruning for accelerating very
deep neural networks,” in IEEE ICCV , 2017, pp. 1389–1397.
[25] T. Zhang, S. Ye, K. Zhang, J. Tang, W. Wen, M. Fardad, and Y . Wang,
“A systematic dnn weight pruning framework using alternating direction
method of multipliers,” in ECCV , 2018, pp. 184–199.
[26] S. Lym, E. Choukse, S. Zangeneh, W. Wen, S. Sanghavi, and M. Erez,
“Prunetrain: fast neural network training by dynamic sparse model
reconfiguration,” in SC’19 , 2019, pp. 1–13.
[27] T. Lin, S. U. Stich, L. F. Barba Flores, D. Dmitriev, and M. Jaggi,
“Dynamic model pruning with feedback,” in ICLR , no. CONF, 2020.
[28] X. Yao, T. Huang, R.-X. Zhang, R. Li, and L. Sun, “Federated learning
with unbiased gradient aggregation and controllable meta updating,”
arXiv preprint arXiv:1910.08234 , 2019.
[29] S. Bibikar, H. Vikalo, Z. Wang, and X. Chen, “Federated dynamic sparse
training: Computing less, communicating less, yet learning better,” in
Proceedings of the AAAI Conference on Artificial Intelligence , vol. 36,
no. 6, 2022, pp. 6080–6088.
[30] A. Li, J. Sun, B. Wang, L. Duan, S. Li, Y . Chen, and H. Li, “Lot-
teryfl: empower edge intelligence with personalized and communication-
efficient federated learning,” in IEEE/ACM SEC . IEEE, 2021, pp. 68–
79.
[31] U. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen, “Rigging the
lottery: Making all tickets winners,” in ICML . PMLR, 2020, pp. 2943–
2952.
[32] M. Zhu and S. Gupta, “To prune, or not to prune: exploring the efficacy
of pruning for model compression,” arXiv preprint arXiv:1710.01878 ,
2017.
[33] H. Wang, C. Qin, Y . Zhang, and Y . Fu, “Neural pruning via growing
regularization,” in ICLR , 2021.
[34] W. Fu and K. Knight, “Asymptotics for lasso-type estimators,” The
Annals of Statistics , vol. 28, no. 5, pp. 1356–1378, 2000.
[35] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V . Smith,
“Federated optimization in heterogeneous networks,” PMLR , vol. 2, pp.
429–450, 2020.
[36] F. Haddadpour and M. Mahdavi, “On the convergence of local descent
methods in federated learning,” CoRR , vol. abs/1910.14425, 2019.
[Online]. Available: http://arxiv.org/abs/1910.14425
[37] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence
of fedavg on non-iid data,” in ICLR , 2019.
[38] S. Wan, J. Lu, P. Fan, Y . Shao, C. Peng, and K. B. Letaief, “Conver-
gence analysis and system design for federated learning over wireless
networks,” IEEE JSAC , vol. 39, no. 12, pp. 3622–3639, 2021.
[39] H. Xiao, K. Rasul, and R. V ollgraf, “Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,” arXiv preprint
arXiv:1708.07747 , 2017.
[40] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of features
from tiny images,” 2009.
[41] Y . LeCun et al. , “Lenet-5, convolutional neural networks,” URL:
http://yann. lecun. com/exdb/lenet , vol. 20, no. 5, p. 14, 2015.
[42] A. Krizhevsky, “One weird trick for parallelizing convolutional neural
networks,” CoRR , vol. abs/1404.5997, 2014. [Online]. Available:
http://arxiv.org/abs/1404.5997
[43] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V . Smith,
“Federated optimization in heterogeneous networks,” in PMLS , vol. 2,
2020, pp. 429–450.

--- PAGE 11 ---
APPENDIX
Before the proof of convergence Theorem 1, we stress that:
E[f(¯ω′(t+1))−f(¯ω′(t))] =E[f(¯ω′(t+1))]−E[f(¯ω(t+1))]
+E[f(¯ω(t+1))]−E[f(¯ω′(t))],(26)
where mask update only happens at server and ¯ω′(t)is the
global model received by nodes at time t, as the start of the
local model training phase.
Lemma 1. Given any mask function m:={0,1}n×pfor
pruning, the Frobenius norm of model weight/gradients matrix
ωis greater than or equal to the pruned one m⊙ω, i.e.,
∥ω∥ ≥ ∥m⊙ω∥. (27)
Proof. According to the definition of Frobenius norm, we have
∥m⊙ω∥2=Tr([m⊙ω]T·[m⊙ω])
=nX
i=1pX
j=1|mijωij|2≤nX
i=1pX
j=1|ωij|2=∥ω∥2.
Lemma 1 provides the foundation that the quality of pruning
δt, respectively, are in the range [0,1].
Lemma 2. Given the Definition 1 and Assumption 2, the
effect of pruning on pruned model weights at server ( δt+1)
is bounded as:
E[f(¯ω′(t+1))]−E[f(¯ωt+1)]≤µE[p
δt+1∥¯ωt+1∥](28)
Proof.
E[f(¯ω′(t+1))]−E[f(¯ωt+1)]≤µE[∥¯ω′(t+1)−¯ωt+1∥]
=µE[p
δt+1∥¯ωt+1∥].(29)
Lemma 3. Under the definitions provided in Section V and
Assumptions 1&4,E[f(¯ωt+1)]−E[f(¯ω′(t))]is bounded by:
E[f(¯ωt+1)]−E[f(¯ω′(t))]≤(γ−1)L2η2
t+η2
tL
2KNX
n=1ρnσ2
n+
(γ−1)γElη2
tL2
2tc+ElX
k=tc+1∥NX
n=1ρnv′(k)
n∥2−ηt
2∥∇f(¯ω′(t))∥2+
γη2
tL−ηt
2∥NX
n=1ρn˜v′(t)
n∥2.(30)
Proof. Firstly, according to the Assumption 1, we have
E[f(¯ωt+1)]−E[f(¯ω′(t))]
≤E[<¯ωt+1−¯ω′(t),∇f(¯ω′(t))>] +L
2E∥¯ωt+1−¯ω′(t)∥2
a⃝=η2
tL
2E∥˜v′(t)∥2−E[< ηt˜v′(t),∇f(¯ω′(t))>],(31)wherea⃝=holds because of (19). Following the proof structure
in [38], we give the boundary for −E[< ηt˜v′(t),∇f(¯ω′(t))>]
andE∥˜v′(t)∥2, respectively.
According to the variance formula and the definition of
¯v′(t), we can expand E∥˜v′(t)∥2as follows:
E∥˜v′(t)∥2=E∥˜v′(t)−E(˜v′(t))∥2+ [E(˜v′(t))]2
=E∥˜v′(t)−¯v′(t)∥2+∥¯v′(t)∥2.(32)
Recall that Ptis the selection probability of clients, thus, one
can obtain:
∥¯v′(t)∥2=1
KEPt[X
n∈Ptv′(t)
n]2
≤1
KEPt"X
n∈Pt∥v′(t)
n∥2#
(33)
The last inequality holds due to Jenson’s Inequality . Similarly,
based on the definitions in (17), we have:
E[∥˜v′(t)−¯v′(t)∥2] =E[∥EPt[1
KX
n∈Pt˜v′(t)
n−1
KX
n∈Ptv′(t)
n]∥2]
=1
K2{E[EPt[X
n∈Pt∥˜v′(t)
n−v′(t)
n∥2]]
+X
i̸=j<˜v′(t)
i−v′(t)
i,˜v′(t)
j−v′(t)
j>}
=1
KE[NX
n=1ρn∥˜v′(t)
n−v′(t)
n∥2].(34)
Substituting (33) and (34) into (32), and according to the
Lemma 1, Definition 2 and Assumption 4, we obtain that:
E∥˜v′(t)∥2≤1
KE[X
n∈Pt∥v′(t)
n∥2]+1
K2E[X
n∈Pt∥˜v′(t)
n−v′(t)
n∥2]
≤NX
n=1ρn∥v′(t)
n∥2+1
KE[NX
n=1∥˜v′(t)
n−v′(t)
n∥2]
≤NX
n=1ρn∥v′(t)
n∥2+1
KNX
n=1ρnσ2
n
≤γNX
n=1∥ρnv′(t)
n∥2+1
KNX
n=1ρnσ2
n.(35)
Next, we provide the boundary for −E[< ηt˜v′(t),∇f(¯ω′(t))>

--- PAGE 12 ---
].
−E[⟨˜v′(t),∇f(¯ω′(t))⟩] =−⟨E[1
KX
n∈Pt˜v′(t)
n],∇f(¯ω′(t))⟩
=−⟨E[NX
n=1ρn˜v′(t)
n],∇f(¯ω′(t))⟩
a⃝=−1
2∥∇f(¯ω′(t))∥2−1
2∥NX
n=1ρn˜v′(t)
n∥2+1
2∥∇f(¯ω′(t))−NX
n=1ρn˜v′(t)
n∥2
=−1
2∥∇f(¯ω′(t))∥2−1
2∥NX
n=1ρn˜v′(t)
n∥2+
1
2∥NX
n=1ρn(∇fn(¯ω′(t))− ∇fn(ω′(t)
n))∥2
b⃝
≤ −1
2∥∇f(¯ω′(t))∥2−1
2∥NX
n=1ρn˜v′(t)
n∥2+
L2
2NX
n=1ρn∥(¯ω′(t)−ω′(t)
n)∥2,(36)
where the condition a⃝is satisfied due to the relation
ab=a2+b2−(a−b)2
2, while condition b⃝is satisfied by virtue
of Assumption 1. Our approach to the proof utilizes a similar
structure to the one found in the reference [38], owing to the
fact that the global mask m tremains consistent throughout the
local training process. The specifics of this proof methodology
are explicated as follows. Let tc= [t
El]Elbe the start time of
local training. Then ¯ω′(t)andω′(t)
ncan be written as
¯ω′(t)= (¯ω′(tc)−1
KX
n∈Ptt−1X
k=tc+1ηk˜v′(k)
n)⊙mt,(37)
ω′(t)
n = ( ¯ω′(tc)−t−1X
k=tc+1ηk˜v′(k)
n)⊙mt.(38)
According to Lemma 1, Assumption 3, Definition 2 and Eq.
(50)−(54) in [38], while taking expectation of Pt, we can
obtain
NX
n=1ρn∥(¯ω′(t)−ω′(t)
n)∥2
≤(γ−1)[1
KNX
n=1t−1X
k=tc+1η2
kρnσ2
n+γElt−1X
k=tc+1η2
k∥NX
n=1ρnv′(k)
n∥2].
(39)
Consider decreasing learning rate η2
k≤ηtwithtc+1< k < tand substitute Eq. (39) into (36), we have
−E[⟨˜v′(t),∇f(¯ω′(t))⟩]≤(γ−1)L2ηt
2KNX
n=1ρnσ2
n+
(γ−1)γElηtL2
2tc+ElX
k=tc+1∥NX
n=1ρnv′(k)
n∥2−1
2∥∇f(¯ω′(t))∥2
−1
2∥NX
n=1ρn˜v′(t)
n∥2.(40)
Substituting (35) and (40) into (31) finishes the proof.
Proof of Theorem 1:
Proof. Recall (26), we sum up the theoretical results of Lemma
2 and 3.
E[f(¯ω′(t+1))]−E[f(¯ω′(t))]≤(γ−1)L2η2
t+η2
tL
2KNX
n=1ρnσ2
n+
(γ−1)γElη2
tL2
2tc+ElX
k=tc+1∥NX
n=1ρnv′(k)
n∥2−ηt
2∥∇f(¯ω′(t))∥2+
γη2
tL−ηt
2∥NX
n=1ρn˜v′(t)
n∥2+µE[p
δt+1∥¯ωt+1∥].(41)
While ηt≤1
tLand Assumption 4, Equation (41) can be
expressed as:
E[f(¯ω′(t+1))]−E[f(¯ω′(t))]≤(γ−1)L2η2
t+η2
tL
2KNX
n=1ρnσ2
n+
(γ−1)γE2
lη2
tL2G2
2+µE[p
δt+1∥¯ωt+1∥.(42)
Denote χ=(γ−1)L2+L
2KPN
n=1ρnσ2
n+(γ−1)γE2
lL2G2
2, (42) is
written as
ηt
2∥∇f(¯ω′(t))∥2≤E[f(¯ω′(t))]−E[f(¯ω′(t+1))]
+χη2
t+µE[p
δt+1∥¯ωt+1∥.(43)
After taking the average of (43) over time T, we have the
results as follows after rearranging. Note that we assume that
the model will converge to a stable point regarded as the
optimum f∗.
1
TTX
t=11
tL∥∇f(¯ω′(t))∥2≤1
TE(f(ω1)−f∗)+
2
TTX
t=1[µE[p
δt+1∥¯ωt+1∥+χη2
t].(44)

--- PAGE 13 ---
Since1
TPT
t=11
T∥∇f(¯ω′(t))∥2≤1
TPT
t=11
t∥∇f(¯ω′(t))∥2,
(44) can be expressed as
1
TTX
t=1∥∇f(¯ω′(t))∥2≤2LE(f(ω1)−f∗)+
2LTX
t=1[µE[p
δt+1∥¯ωt+1∥] +π2
3L2χ, (45)
where it is known thatPT
t=11
t2=π2
6.
