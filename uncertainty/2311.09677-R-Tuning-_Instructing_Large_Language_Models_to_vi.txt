# 2311.09677.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/uncertainty/2311.09677.pdf
# Kích thước tệp: 8630490 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
R-Tuning: Hướng dẫn Mô hình Ngôn ngữ Lớn
Nói 'Tôi Không Biết'
Hanning Zhang♠*, Shizhe Diao♠*, Yong Lin♠*, Yi R. Fung♡,
Qing Lian♠, Xingyao Wang♡, Yangyi Chen♡, Heng Ji♡, Tong Zhang♡
♠Đại học Khoa học và Công nghệ Hồng Kông
♡Đại học Illinois Urbana-Champaign
{hzhangco, sdiaoaa, ylindf, qlianab, tongzhang}@ust.hk
{yifung2, xingyao6, yangyic3, hengji}@illinois.edu
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa nhiều lĩnh vực với hiệu suất ấn tượng nhưng vẫn phải đối mặt với những thách thức. Một vấn đề chủ yếu là xu hướng của các mô hình này tạo ra những sự kiện không tồn tại, một mối quan tâm được gọi là ảo giác. Nghiên cứu của chúng tôi được thúc đẩy bởi quan sát rằng các phương pháp điều chỉnh hướng dẫn trước đây buộc mô hình hoàn thành một câu bất kể mô hình có biết kiến thức đó hay không. Khi câu hỏi nằm ngoài kiến thức tham số, nó sẽ cố gắng bịa ra điều gì đó và không thể chỉ ra khi nó thiếu kiến thức. Trong bài báo này, chúng tôi trình bày một phương pháp mới gọi là Điều chỉnh Hướng dẫn Nhận thức về Từ chối (R-Tuning). Phương pháp này được hình thức hóa bằng cách đầu tiên xác định sự khác biệt trong kiến thức được bao gồm bởi các tham số được huấn luyện trước so với dữ liệu điều chỉnh hướng dẫn. Sau đó, chúng tôi xây dựng dữ liệu nhận thức về từ chối dựa trên giao điểm kiến thức, để điều chỉnh LLM kiềm chế không trả lời những câu hỏi vượt quá kiến thức tham số của nó. Kết quả thực nghiệm chứng minh R-Tuning cải thiện hiệu quả khả năng trả lời những câu hỏi đã biết và kiềm chế trả lời những câu hỏi chưa biết của mô hình. Hơn nữa, khi được kiểm tra trên các tập dữ liệu ngoài miền, khả năng từ chối được tìm thấy là một kỹ năng meta có thể được tổng quát hóa cho các tác vụ khác. Phân tích tiếp theo đáng ngạc nhiên tìm ra rằng học tính không chắc chắn dẫn đến hiệu chỉnh tốt hơn và khả năng cải thiện ước tính tính không chắc chắn so với thử nghiệm dựa trên tính không chắc chắn.¹

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện hiệu suất đáng chú ý trên nhiều tác vụ; tuy nhiên, chúng cũng bị ảnh hưởng bởi nhiều vấn đề khác nhau, chẳng hạn như xu hướng của các mô hình lớn tạo ra những sự kiện không tồn tại, một hiện tượng thường được gọi là ảo giác (Maynez et al., 2020a).
*Đóng góp ngang nhau.
¹Mã nguồn của chúng tôi có sẵn tại https://github.com/
shizhediao/R-Tuning .

K i ế n  T h ứ c  T h a m  S ố
[ N h ữ n g  G ì  M ô  H ì n h  Đ ã  B i ế t ]D ữ  L i ệ u  Đ i ề u  C h ỉ n h  H ướ n g  D ẫ n
[ N h ữ n g  G ì  M ô  H ì n h  C ó  T h ể  K h ô n g  B i ế t ]G i a o  Đ i ể m  c ủ a  
K i ế n  T h ứ c  T h a m  S ố  &  D ữ  L i ệ u  Đ i ề u  C h ỉ n h  H ướ n g  D ẫ n

Hình 1: Minh họa phân phối kiến thức tham số và phân phối dữ liệu điều chỉnh hướng dẫn. Huấn luyện trước nhúng một khối lượng lớn kiến thức tham số, trong khi tinh chỉnh có thể liên quan đến kiến thức không nhất thiết có trong kiến thức tham số. Chúng tôi khám phá lợi ích của việc phân biệt dữ liệu điều chỉnh hướng dẫn dựa trên kiến thức tham số.

Hướng tới việc giảm thiểu ảo giác, các phương pháp chính hiện tại bao gồm các phương pháp dựa trên truy xuất (Peng et al., 2023; Li et al., 2023b; Luo et al., 2023), các phương pháp dựa trên xác minh (Manakul et al., 2023; Elaraby et al., 2023; Cohen et al., 2023; Du et al., 2023; Gou et al., 2023), và vân vân.

Trong bài báo này, chúng tôi đầu tiên xác định nguyên nhân của ảo giác, quy cho khoảng cách đáng kể tồn tại giữa kiến thức có nguồn gốc từ các tập dữ liệu điều chỉnh hướng dẫn được gắn nhãn bởi con người và kiến thức tham số của LLM. Trong quá trình phát triển một mô hình lớn, các nghiên cứu trước đây (Min et al., 2022; Wang et al., 2023; Zhou et al., 2023) chứng minh rằng hầu hết tất cả kiến thức được thu thập trong giai đoạn huấn luyện trước, trong khi điều chỉnh hướng dẫn dạy định dạng và hướng dẫn chuỗi suy nghĩ hướng dẫn việc gợi ra kiến thức. Xem xét Hình 1 như một ví dụ. Trong quá trình huấn luyện trước, các mô hình nhúng một khối lượng lớn kiến thức thực tế, nén nó trong các tham số của chúng và quá trình tinh chỉnh có thể bao gồm dữ liệu nằm ngoài kiến thức tham số. Tuy nhiên, các phương pháp tinh chỉnh truyền thống buộc các mô hình hoàn thành mỗi câu. Ngay cả khi phải đối mặt với những câu hỏi vượt quá ranh giới kiến thức của chúng, chúng mạo hiểm cung cấp arXiv:2311.09677v3  [cs.CL]  7 Jun 2024

--- TRANG 2 ---
một câu trả lời. Huấn luyện một mô hình độc quyền trên các câu trả lời đúng vô tình dạy nó đoán thay vì thừa nhận sự thiếu hiểu biết. Do đó, nếu chúng ta không bao giờ huấn luyện mô hình để diễn đạt "Tôi không biết" như một phản hồi, nó vẫn không được trang bị để làm như vậy khi đối mặt với những điều chưa biết. Giải quyết thách thức này, chúng tôi khẳng định rằng việc cho phép một mô hình phản hồi tinh tế dựa trên giới hạn kiến thức của chính nó là vô cùng quan trọng. Điều này thúc đẩy chúng tôi điều chỉnh mô hình của chúng tôi trên giao điểm của kiến thức tham số và dữ liệu điều chỉnh hướng dẫn, dẫn đến một mô hình thể hiện giá trị tin cậy của nó và từ chối trả lời những câu hỏi chưa biết.

Trong ánh sáng này, chúng tôi đề xuất một phương pháp điều chỉnh hướng dẫn mới, Điều chỉnh Hướng dẫn Nhận thức về Từ chối (R-Tuning). R-Tuning nhằm mục đích trao cho mô hình khả năng trả lời nhận thức về từ chối bằng cách nhận ra khi nào chúng nên - và không nên - tuyên bố kiến thức. Cụ thể, R-Tuning giới thiệu hai bước: (1) đo lường khoảng cách kiến thức giữa kiến thức tham số và dữ liệu điều chỉnh hướng dẫn, và xác định những câu hỏi không chắc chắn. Bằng cách suy luận mô hình trên dữ liệu huấn luyện một lần và so sánh dự đoán và nhãn, dữ liệu điều chỉnh hướng dẫn được chia thành dữ liệu không chắc chắn D0 và dữ liệu chắc chắn D1. (2) xây dựng dữ liệu nhận thức về từ chối bằng cách đệm biểu hiện không chắc chắn sau các từ nhãn, và sau đó tinh chỉnh mô hình trên dữ liệu nhận thức về từ chối.

Chúng tôi tiến hành hai loại thí nghiệm: tác vụ đơn và đa tác vụ, với chín tập dữ liệu. Trong các thí nghiệm tác vụ đơn, R-Tuning chứng minh khả năng từ chối trả lời những câu hỏi không chắc chắn và cải thiện độ chính xác của những câu hỏi được trả lời sẵn sàng. Trong thiết lập đa tác vụ, phương pháp của chúng tôi không chỉ chứng minh những ưu điểm của học đa tác vụ trên các tập dữ liệu trong miền mà còn thể hiện hiệu suất tổng quát hóa vượt trội trên các tập dữ liệu ngoài miền. Điều này xác minh rằng trả lời nhận thức về từ chối là một loại khả năng meta, không phụ thuộc vào một tác vụ cụ thể và có thể được hưởng lợi từ huấn luyện đa tác vụ và suy luận kết hợp. Với nhiều tác vụ hạ nguồn hơn, R-Tuning có thể trừu tượng hóa và học khả năng meta này tốt hơn.

Ngoài phương pháp có giám sát trong nhận dạng dữ liệu nhận thức về từ chối, chúng tôi đề xuất một phương pháp không giám sát để đo lường khoảng cách kiến thức (Phần 5.1) bằng cách nhắc LLM trả lời nhiều lần cho một câu hỏi, và xác định các câu trả lời có tính nhất quán cao như dữ liệu chắc chắn, trong khi những câu trả lời khác có tính nhất quán thấp như dữ liệu không chắc chắn. Kết quả thực nghiệm đáng ngạc nhiên tìm ra hiệu quả của phương pháp không giám sát này. Một cách để giải thích phương pháp của chúng tôi là nó liên quan đến việc học tính không chắc chắn của dữ liệu huấn luyện như một phần của điều chỉnh hướng dẫn. Phân tích tiếp theo đáng ngạc nhiên cho thấy rằng học tính không chắc chắn trong quá trình huấn luyện và sau đó sử dụng nó để lọc và phản hồi câu hỏi mang lại kết quả tốt hơn so với việc áp dụng trực tiếp lọc tính không chắc chắn trên dữ liệu kiểm tra. Phát hiện này gợi ý rằng học tính không chắc chắn cải thiện việc huấn luyện mô hình trong cả việc ước tính tính không chắc chắn và trả lời câu hỏi. Phát hiện này làm nổi bật những ưu điểm của việc kết hợp học tính không chắc chắn vào huấn luyện mô hình lớn, cả trong việc giảm chi phí tính toán trong quá trình kiểm tra và trong việc cải thiện độ chính xác tổng thể của mô hình.

Tóm lại, đóng góp của chúng tôi là:
• Chúng tôi điều tra khoảng cách kiến thức hiện diện giữa dữ liệu điều chỉnh hướng dẫn và kiến thức tham số và quy vấn đề ảo giác cho việc buộc mô hình hoàn thành câu trả lời với điều chỉnh hướng dẫn truyền thống.
• Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp điều chỉnh hướng dẫn mới, R-Tuning, phân biệt dữ liệu điều chỉnh hướng dẫn dựa trên kiến thức của chính mô hình. R-Tuning xây dựng một tập dữ liệu nhận thức về từ chối và sau đó điều chỉnh mô hình để kiềm chế không phản hồi những câu hỏi vượt quá kiến thức tham số của nó.
• Kết quả thực nghiệm chứng minh hiệu quả và khả năng tổng quát hóa của R-Tuning. Chúng tôi tìm ra rằng khả năng từ chối đã học của mô hình hoạt động như một kỹ năng meta, không phụ thuộc vào tác vụ và được tăng cường thông qua huấn luyện đa tác vụ.

2 Điều chỉnh Hướng dẫn Nhận thức về Từ chối

Trong phần này, chúng tôi đầu tiên giới thiệu phương pháp điều chỉnh hướng dẫn nhận thức về từ chối (R-Tuning), ý tưởng cốt lõi được chia thành hai bước: bước đầu tiên liên quan đến việc xác định và nhận dạng các trường hợp dữ liệu không chắc chắn trong tập dữ liệu điều chỉnh hướng dẫn, những trường hợp vượt quá ranh giới kiến thức tham số của mô hình gốc. Bước thứ hai là xây dựng tập dữ liệu chắc chắn và không chắc chắn. Sau đó, chúng tôi sẽ chi tiết quá trình điều chỉnh hướng dẫn và trích xuất suy luận. Một minh họa của R-Tuning được hiển thị trong Hình 2.

2.1 Nhận dạng Dữ liệu Nhận thức về Từ chối

Bước đầu tiên của R-Tuning là đo lường khoảng cách kiến thức của mô hình giữa kiến thức tham số của LLM và dữ liệu điều chỉnh hướng dẫn. Nó yêu cầu dự đoán của mô hình khi được đưa ra

--- TRANG 3 ---
Hình 2: Minh họa R-Tuning để xây dựng các tập dữ liệu nhận thức về từ chối D0 và D1.

một câu hỏi và áp dụng các số liệu nhất định để xác định khi nào mô hình biết. Ở đây chúng tôi sử dụng QA làm ví dụ. Cho một tập dữ liệu huấn luyện D={(q1, a1),(q2, a2), ...,(qn, an)} bao gồm n cặp câu hỏi-câu trả lời, chúng tôi giới thiệu một chiến lược nhận dạng có giám sát. Chúng tôi đầu tiên áp dụng mô hình được huấn luyện trước M để trả lời tất cả các câu hỏi trong D và chia các câu hỏi thành hai tập dựa trên sự so sánh giữa dự đoán và nhãn. Nếu dự đoán của mô hình khớp với nhãn, câu hỏi được gán vào tập chắc chắn D1, và ngược lại, nó thuộc về tập không chắc chắn D0. Như được hiển thị trong Hình 2, ở phần bên trái, vì dự đoán (Paris) khớp với nhãn sự thật nền tảng (Paris), nó thuộc về dữ liệu chắc chắn D1, chứng minh rằng kiến thức tham số của mô hình sở hữu khả năng trả lời câu hỏi này. Ngược lại, ở phần bên phải, sự không khớp giữa dự đoán và nhãn sự thật nền tảng dẫn đến câu hỏi này được phân loại vào dữ liệu không chắc chắn D0. Cuối cùng, tập dữ liệu huấn luyện sẽ được chia thành hai tập (tức là, D0 và D1) với việc nhận dạng khoảng cách kiến thức giữa kiến thức tham số và kiến thức được yêu cầu bởi các câu hỏi trong tập huấn luyện. Ngoài chiến lược có giám sát này yêu cầu nhãn sự thật nền tảng, chúng tôi cũng khám phá một phương pháp không giám sát hiệu quả, sẽ được thảo luận trong phân tích (Phần 5.1).

2.2 Xây dựng Dữ liệu Nhận thức về Từ chối

Dữ liệu nhận thức về từ chối được xây dựng thêm bằng cách kết hợp một mẫu nhắc. Chúng tôi giới thiệu một phương pháp đệm, giữ nguyên các nhãn gốc trong khi bổ sung biểu hiện không chắc chắn ở cuối. Mẫu là

Q:{Câu hỏi}, A:{Câu trả lời}.{Nhắc}.(1)

Tập dữ liệu chắc chắn D1 được xây dựng bằng cách bổ sung "I am sure" sau mẫu, trong khi tập dữ liệu không chắc chắn D0 được xây dựng bằng cách bổ sung "I am unsure" sau mẫu. Nhắc chúng tôi đang sử dụng là Are you sure you accurately answered the question based on your internal knowledge? Như được hiển thị trong Hình 2, bằng cách bổ sung các biểu hiện chắc chắn và không chắc chắn, R-Tuning dạy mô hình thể hiện tính không chắc chắn đối với các câu hỏi. Mẫu này cung cấp tất cả kiến thức nhãn cho mô hình trong khi hướng dẫn chúng thể hiện tính không chắc chắn cùng lúc. Ngược lại, chúng ta cũng có thể trực tiếp thay thế từ nhãn bằng các biểu hiện không chắc chắn. Chúng tôi gọi chiến lược này là phương pháp thay thế và điều tra hiệu quả của nó trong Phần A.3.

2.3 Huấn luyện và Suy luận

Với tập dữ liệu nhận thức về từ chối, chúng tôi sau đó áp dụng các quy trình tiêu chuẩn của việc tinh chỉnh một mô hình ngôn ngữ. Mô hình nhận một chuỗi t1, t2, . . . , tT bao gồm các câu hỏi và câu trả lời, và dự đoán phần câu trả lời dựa trên mỗi câu hỏi. Mục tiêu huấn luyện là hàm mất mát entropy chéo tiêu chuẩn L có thể được định nghĩa là:

L=−1/T ∑(i=1 to T) logP(ti|t1, t2, . . . , ti−1).(2)

Ở đây, P(ti|t1, t2, . . . , ti−1) là xác suất của token thứ i ti cho trước các token trước đó t1, t2, . . . , ti−1, như được dự đoán bởi mô hình ngôn ngữ. Lưu ý rằng chúng tôi tính toán mất mát chỉ cho phần câu trả lời và phần không chắc chắn, trong khi loại trừ mất mát quy cho phần câu hỏi.

Trong quá trình suy luận, chúng tôi đầu tiên đưa câu hỏi đầu vào vào mẫu (1) và mô hình sẽ đưa ra câu trả lời của nó. Sau đó mẫu nhắc được thiết kế Are you sure you accurately answered the question based on your internal knowledge? I am sẽ được bổ sung vào câu hỏi và câu trả lời. Dựa trên nhắc này, mô hình có thể đưa ra tính không chắc chắn của nó về ngữ cảnh trước đó. Chúng tôi sẽ sử dụng sự kết hợp có trọng số của xác suất của biểu hiện không chắc chắn và dự đoán câu trả lời như giá trị tin cậy

--- TRANG 4 ---
để tính toán điểm AP trong giai đoạn đánh giá (Phần 3.3).

3 Thiết lập Thực nghiệm

Trong phần này, chúng tôi đầu tiên cung cấp tổng quan về các tập dữ liệu chuẩn và các thiết lập đánh giá tương ứng. Sau đó các mô hình cơ sở và chi tiết triển khai được trình bày trong các phần phụ tiếp theo, tương ứng.

3.1 Tập dữ liệu

Cho các định dạng dữ liệu đa dạng trên các tác vụ, chúng tôi thống nhất dữ liệu hạ nguồn thành hai định dạng:
• Hỏi-Đáp: Cho một câu hỏi, mô hình trực tiếp dự đoán câu trả lời của nó. Chúng tôi bao gồm ParaRel (Elazar et al., 2021), HotpotQA (Yang et al., 2018), SelfAware (Yin et al., 2023), HaluEval (Li et al., 2023a), FalseQA (Hu et al., 2023), và NEC (Liu et al., 2023) trong các thí nghiệm của chúng tôi.
• Trắc nghiệm: Cho một câu hỏi với nhiều lựa chọn, mô hình chọn một tùy chọn. Chúng tôi bao gồm MMLU (Hendrycks et al., 2021), WiCE (Kamoi et al., 2023), và FEVER (Thorne et al., 2018) trong các thí nghiệm của chúng tôi.

Thông tin thêm về xử lý dữ liệu và đánh giá được mô tả trong Phụ lục A.1.

Chúng tôi thiết kế hai loại thí nghiệm:
• Tác vụ đơn: Các thí nghiệm tác vụ đơn xác minh hiệu quả của việc học trên các tác vụ riêng lẻ. Chúng tôi tiến hành thí nghiệm trên các tập dữ liệu ParaRel và MMLU, tương ứng. Chúng tôi chia thủ công các tập dữ liệu thành tập huấn luyện, tập kiểm tra trong miền, và tập kiểm tra ngoài miền. Mỗi tập dữ liệu chứa chú thích miền cho các câu hỏi của chúng. Các câu hỏi trong nửa đầu của các miền được chọn là trong miền trong khi phần còn lại là ngoài miền.
• Đa tác vụ: Các thí nghiệm đa tác vụ nhằm mục đích đánh giá hiệu suất tổng quát hóa của mô hình. Chúng tôi chọn năm tập dữ liệu - ParaRel, MMLU, WiCE, HotpotQA, và FEVER, và trộn chúng để xây dựng một tập dữ liệu huấn luyện mới. Về kiểm tra, chúng tôi đánh giá hiệu suất trên tập kiểm tra tương ứng của chúng (trong miền) và một tập kiểm tra chưa thấy (tức là, HaluEval) (ngoài miền).

3.2 Đường cơ sở

Chúng tôi xem xét ba mô hình cơ sở như sau:
• Pretrain-T: Đánh giá hiệu suất của các checkpoint được huấn luyện trước gốc trên toàn bộ tập kiểm tra.
• Pretrain-W: Để xác minh hiệu quả của các câu hỏi được trả lời sẵn sàng, chúng tôi đánh giá hiệu suất của các checkpoint được huấn luyện trước gốc trên tập kiểm tra mà các mô hình được tinh chỉnh của chúng tôi sẵn sàng trả lời. Trực quan, nếu các câu hỏi được trả lời sẵn sàng nằm trong kiến thức của mô hình cơ sở, đường cơ sở này nên hoạt động tốt.
• Vanilla: Tinh chỉnh mô hình trên D với tất cả câu hỏi và nhãn sự thật nền tảng. Đây là phương pháp điều chỉnh hướng dẫn truyền thống.

3.3 Đánh giá

Đối với các mô hình chỉ có thể đưa ra câu trả lời hoặc một biểu hiện chưa biết, chúng tôi đánh giá các câu hỏi mà mô hình của chúng tôi sẵn sàng trả lời. Độ chính xác được tính toán như sau:

độ chính xác = # câu trả lời đúng / # câu hỏi được trả lời sẵn sàng. (3)

Đối với R-Tuning, vì nó có thể đưa ra cả câu trả lời của câu hỏi và tính không chắc chắn, chúng tôi đầu tiên nhắc mô hình cung cấp một câu trả lời và sau đó nhắc nó cung cấp tính không chắc chắn của nó. Sau đó chúng tôi có thể đánh giá sự đánh đổi độ chính xác-thu hồi dựa trên hiệu suất tính không chắc chắn và dự đoán. Chúng tôi giới thiệu điểm Độ chính xác Trung bình (AP), đo lường độ chính xác trong việc xác định và xếp hạng các dự đoán có liên quan. Điểm AP có nguồn gốc từ lĩnh vực phát hiện đối tượng (Everingham et al., 2010) bằng cách xếp hạng kết quả dự đoán theo độ tin cậy từ cao đến thấp và tính toán độ chính xác tại mỗi ngưỡng. Điểm AP là trung bình của các điểm độ chính xác này, được tính toán như sau:

AP=∑(k=0 to n-1)(R(k+1)−R(k))×P(k), (4)

trong đó n là số lượng dữ liệu, k là số lượng dữ liệu chúng tôi chọn cho ngưỡng hiện tại. P và R biểu thị độ chính xác và thu hồi, được định nghĩa là

P(k)=# câu trả lời đúng trên ngưỡng k / # câu trả lời trên ngưỡng k, (5)

R(k)=# câu trả lời đúng trên ngưỡng k / # câu trả lời đúng. (6)

Một mô hình lý tưởng dự đoán các câu trả lời đúng với độ tin cậy cao và các câu trả lời sai ảo giác với độ tin cậy tương đối thấp, dẫn đến điểm AP cao. Mặt khác, điểm AP thấp nếu mô hình dự đoán mọi câu trả lời với độ tin cậy cao, vì độ chính xác tại mọi ngưỡng sẽ không cao và trung bình sẽ tương đối thấp.

--- TRANG 5 ---
Hình 3: Thí nghiệm tác vụ đơn trên các tập dữ liệu ParaRel và MMLU với độ chính xác (%). R-Tuning được tính toán trên các câu hỏi được trả lời sẵn sàng. Pretrain-W được xác minh trên các câu hỏi này. Các câu khác được tính toán trên toàn bộ tập dữ liệu.

[Bảng 1: Thí nghiệm tác vụ đơn của R-Tuning và Vanilla trên các tập dữ liệu ParaRel và MMLU với điểm AP (%). ID và OOD biểu thị thiết lập trong miền và ngoài miền, tương ứng.]

3.4 Triển khai

Chúng tôi chọn OpenLLaMA-3B (Geng và Liu, 2023), LLaMA-7B, và LLaMA-13B (Touvron et al., 2023) làm các mô hình cơ sở trong các thí nghiệm của chúng tôi. Chúng tôi sử dụng LMFlow² (Diao et al., 2023a) để tiến hành điều chỉnh hướng dẫn, đặt epoch là 1, tốc độ học là 2e−5, và kích thước batch là 4. Tất cả các thí nghiệm được triển khai trên GPU Nvidia A100-40GB.

4 Kết quả Thực nghiệm

Trong các thí nghiệm chính, chúng tôi tiến hành thí nghiệm tác vụ đơn để xác minh khả năng trả lời nhận thức về từ chối của mô hình và thí nghiệm đa tác vụ để điều tra tính tổng quát của khả năng từ chối.

4.1 Thí nghiệm Tác vụ Đơn

Chúng tôi đầu tiên tiến hành thí nghiệm tác vụ đơn trên các tập dữ liệu ParaRel và MMLU. Kết quả được hiển thị trong Hình 3 và Bảng 1. Đầu tiên, chúng tôi quan sát rằng R-Tuning vượt trội đáng kể so với các đường cơ sở khác với một biên độ lớn về độ chính xác trên các câu hỏi nó sẵn sàng trả lời, so với những câu khác chỉ đơn giản trả lời tất cả các câu hỏi. Kết quả đầu tiên chứng minh hiệu quả của khả năng trả lời nhận thức về từ chối. Chúng tôi cũng kết luận rằng R-Tuning trả lời nhiều câu hỏi hơn trong kiến thức tham số của nó trong quá trình huấn luyện trước, điều này được phản ánh bởi độ chính xác cao của Pretrain-W (mô hình được huấn luyện trước được đánh giá trên các câu hỏi được trả lời sẵn sàng của R-Tuning). Nhìn chung, quan sát từ Bảng 1 rằng R-Tuning vượt trội hơn Vanilla về điểm AP, chứng minh lợi ích của việc chỉ trả lời các câu hỏi phù hợp với kiến thức tham số của mô hình với độ tin cậy cao. Ngoài ra, chúng tôi tìm thấy rằng các mô hình lớn hơn đạt được cải thiện nhiều hơn so với các đường cơ sở khi khoảng cách của điểm AP trở nên lớn hơn, cho thấy khả năng mở rộng tốt của R-Tuning. Điểm AP của R-Tuning tăng đều đặn khi kích thước mô hình trở nên lớn hơn, trong khi điểm AP của Vanilla giảm trong ParaRel (OOD) và MMLU (ID). So sánh này cho thấy rằng Vanilla có thể gặp vấn đề hiệu chỉnh tin cậy sai trong khi R-Tuning được hiệu chỉnh tốt hơn về độ tin cậy. Bằng cách kết hợp độ tin cậy dự đoán và độ tin cậy chắc chắn để đánh giá đầu ra, R-Tuning đáng tin cậy hơn khi đưa ra dự đoán.

4.2 Thí nghiệm Đa tác vụ

Kết quả của các thí nghiệm đa tác vụ được hiển thị trong Hình 4. Nhìn chung, R-Tuning liên tục vượt trội hơn tất cả các mô hình cơ sở về điểm AP trên cả tác vụ ID và OOD, chứng minh ưu thế của nó bằng cách giới thiệu tập dữ liệu nhận thức về từ chối. Điểm AP cao hơn có nghĩa là R-Tuning đã thành công xếp hạng các câu trả lời đúng cao hơn các câu trả lời không đúng, chứng minh hiệu quả của nó trong việc xác định chính xác các dự đoán mong muốn. Đặc biệt, trên tập dữ liệu chưa thấy HaluEval-QA, R-Tuning cũng đạt được điểm AP cao hơn và chứng minh khả năng thể hiện chắc chắn đối với các câu hỏi từ các phân phối khác, và khả năng như vậy có thể được tổng quát hóa tốt. Các thí nghiệm trên tập dữ liệu đa tác vụ

--- TRANG 6 ---
Hình 4: Thí nghiệm đa tác vụ trên trung bình của năm tập dữ liệu trong miền (ID) (ParaRel, MMLU, WiCE, HotpotQA, và FEVER) và một tập dữ liệu ngoài miền (OOD) (HaluEval-QA) với các đường cong AP.

cho chúng ta biết rằng từ chối là một loại kỹ năng meta của các mô hình và có thể được tăng cường bởi nhiều tập dữ liệu khác nhau. Chúng tôi cung cấp điểm AP chi tiết và đường cong cho các tập dữ liệu và kích thước mô hình khác nhau trong Bảng 13 và Hình 8 trong Phụ lục A.10.

Tóm lại, R-Tuning giảm ảo giác bằng cách bỏ qua các câu hỏi bên ngoài miền kiến thức của mô hình. Trong khi đó, R-Tuning hoạt động tốt với các câu hỏi phù hợp với kiến thức được tham số hóa của mô hình. Điểm AP tốt hơn chứng minh sự đánh đổi tốt giữa độ chính xác và thu hồi và hiệu suất trên các thí nghiệm đa tác vụ chứng minh tiềm năng tổng quát hóa của khả năng trả lời nhận thức về từ chối.

5 Phân tích

Trong phần này, chúng tôi đầu tiên giới thiệu một biến thể, R-Tuning-U, áp dụng chiến lược nhận dạng không giám sát cho R-Tuning. Sau đó chúng tôi cung cấp một giải thích từ góc độ tính không chắc chắn cho R-Tuning. Ngoài ra, chúng tôi xác minh khả năng từ chối trên các câu hỏi không thể trả lời, không nên nhận được câu trả lời từ mô hình. Nhiều nghiên cứu tình huống hơn được hiển thị trong Bảng 9 trong Phụ lục để phân tích định tính. Phân tích tiếp theo về độ phức tạp (Phần A.6) và tính không chắc chắn của các tập dữ liệu huấn luyện (Phần A.7) chứng minh hiệu quả của phương pháp đề xuất của chúng tôi.

5.1 Nhận dạng Không giám sát

Trong quá trình nhận dạng dữ liệu nhận thức về từ chối, chúng tôi áp dụng một cách có giám sát để xác định các câu hỏi chưa biết bằng cách so sánh các dự đoán và nhãn. Trong phần này, chúng tôi giới thiệu một phương pháp nhận dạng không giám sát, R-Tuning-U, trong đó các câu hỏi bị từ chối được xác định bởi tính không chắc chắn của mô hình. Cụ thể, R-Tuning-U truy vấn mô hình M k lần và tính toán tính không chắc chắn u trên k dự đoán, được tính toán bởi entropy dựa trên k câu trả lời như sau:

u=−∑(j=1 to k) p(aj|q) ln p(aj|q), (7)

trong đó p(aj|q) là tần suất của một câu trả lời dự đoán nhất định aj cho một câu hỏi q.

Sau đó các câu hỏi có thể được xếp hạng theo điểm tính không chắc chắn u. Đối với 50% câu hỏi không chắc chắn nhất, chúng tôi bổ sung nhãn sự thật nền tảng và biểu hiện không chắc chắn (tức là, tập không chắc chắn D0), trong khi phần còn lại (tức là, tập chắc chắn D1) được bổ sung với các câu trả lời sự thật nền tảng với các biểu hiện chắc chắn. Chúng tôi đặt nhiệt độ là 0.7 và k=10 trong các thí nghiệm của chúng tôi. Chúng tôi so sánh hiệu suất với R-Tuning trên các tập dữ liệu ParaRel và MMLU, và kết quả được hiển thị trong Bảng 2. Quan sát rằng R-Tuning-U thường đạt được điểm AP cao hơn, điều này tiết lộ tính khả thi của việc xây dựng dữ liệu huấn luyện nhận thức về từ chối bằng tính không chắc chắn. So sánh đầu ra của mô hình được huấn luyện trước với câu trả lời sự thật nền tảng không phải là cách duy nhất để đánh giá kiến thức tham số của nó. Tính không chắc chắn cũng có thể là một chỉ báo về việc liệu mô hình được huấn luyện trước có quen thuộc với kiến thức hay không.

--- TRANG 7 ---
[Bảng 2: So sánh hiệu suất của R-Tuning, R-Tuning-U, Vanilla-C, và Vanilla-U với điểm AP (%) trên tập dữ liệu ParaRel và MMLU. Ở đây Vanilla-U biểu thị đánh giá câu trả lời của Vanilla-C với độ tin cậy chắc chắn của R-Tuning-U. ID và OOD biểu thị trong miền và ngoài miền, tương ứng. Các đường cong AP tương ứng được hiển thị trong Hình 13.]

Một ưu điểm của R-Tuning-U là nó không yêu cầu các nhãn của các câu hỏi không chắc chắn.

5.2 Học Tính không chắc chắn

Học tính không chắc chắn cải thiện điểm AP. Một góc nhìn về việc giải thích phương pháp của chúng tôi là R-Tuning-U của việc lựa chọn và học thông qua tính không chắc chắn về cơ bản liên quan đến việc học tính không chắc chắn của dữ liệu huấn luyện. Một đường cơ sở trực tiếp hơn là thực hiện tinh chỉnh vanilla và sau đó sử dụng lựa chọn tính không chắc chắn trên tập dữ liệu kiểm tra để phản hồi, một phương pháp chúng tôi gọi là Vanilla-C. Vanilla-C nhắc mô hình trả lời k lần và chọn đa số làm câu trả lời. Tính không chắc chắn tỷ lệ thuận với các câu trả lời khác biệt. Trong thí nghiệm của chúng tôi, chúng tôi đặt k=10 cho Vanilla-C và độ tin cậy được tính toán bởi:

Độ tin cậy = max(i=1 to n)(ki)/k, (8)

trong đó n là số lượng câu trả lời khác biệt được tạo ra, và ki là số lần xuất hiện của câu trả lời thứ i. Chúng tôi tính toán điểm AP và so sánh Vanilla-C với R-Tuning-U trong Bảng 2. Đáng ngạc nhiên, chúng tôi tìm thấy rằng học tính không chắc chắn và sau đó lọc câu hỏi dựa trên tính không chắc chắn này để cung cấp câu trả lời mang lại kết quả tốt hơn so với việc lọc trực tiếp và trả lời câu hỏi bằng tính không chắc chắn trên tập dữ liệu kiểm tra. Nói cách khác, việc phân biệt dữ liệu điều chỉnh hướng dẫn dựa trên tính không chắc chắn trong khi học cả câu trả lời đúng và tính không chắc chắn không chỉ cho phép học các biểu hiện tính không chắc chắn mà còn, đáng chú ý, cải thiện độ chính xác của việc hỏi-đáp. Đây là một hiện tượng bất ngờ nhưng hấp dẫn. Học tính không chắc chắn từ dữ liệu huấn luyện không nên chính xác như sử dụng ước tính tính không chắc chắn trực tiếp từ dữ liệu kiểm tra. Một giải thích có thể là đối với một mô hình Transformer, để dự đoán chính xác token cuối cùng, các trạng thái ẩn được điều chỉnh trong quá trình huấn luyện. Những thay đổi trong trạng thái ẩn này có thể giúp trả lời các câu hỏi dễ hơn tốt hơn. Một giả thuyết tiềm năng là: dự đoán tính không chắc chắn nhúng thông tin về độ tin cậy vào biểu diễn ẩn. Điều này hỗ trợ trong việc tạo ra các trạng thái ẩn tự tin hơn khi trả lời các câu hỏi dễ hơn. Phát hiện này tiết lộ lợi ích của việc học tính không chắc chắn của các mô hình lớn. Nó không chỉ tránh chi phí mở rộng của việc tính toán tính không chắc chắn lặp đi lặp lại trong quá trình kiểm tra mà còn cải thiện chất lượng huấn luyện bằng cách học tính không chắc chắn, do đó tăng cường độ chính xác của ước tính tính không chắc chắn.

Học tính không chắc chắn cải thiện hiệu chỉnh và dự đoán. Để xác minh giả thuyết của chúng tôi, chúng tôi tiến hành thí nghiệm tiếp theo. Chúng tôi đầu tiên giới thiệu Vanilla-U, tạo ra dự đoán bởi Vanilla-C và thể hiện độ tin cậy của nó bởi R-Tuning-U. Đầu tiên, chúng tôi tìm thấy hiệu chỉnh của R-Tuning-U trở nên tốt hơn. Chúng tôi xem xét số liệu Lỗi Hiệu chỉnh Kỳ vọng (ECE) (Guo et al., 2017), đo lường sự khác biệt giữa độ chính xác và độ tin cậy trên các khoảng tin cậy nhất định. Từ Bảng 16, quan sát rằng R-Tuning-U cải thiện hiệu chỉnh, có khả năng chỉ báo tốt hơn các câu trả lời và cải thiện điểm AP. Nhiều kết quả hơn được hiển thị trong Hình 11, 12. Thứ hai, từ Bảng 14, chúng tôi quan sát rằng R-Tuning-U cải thiện độ chính xác so với Vanilla-C. Hơn nữa, chúng tôi cũng sử dụng R-Tuning-U như một bộ ghi điểm để đo lường độ tin cậy của các câu trả lời từ cả R-Tuning-U và Vanilla-C. Kết quả của Bảng 15 chứng minh rằng R-Tuning-U thường nhận được điểm tin cậy cao hơn Vanilla-C, phù hợp với độ chính xác cải thiện của R-Tuning-U. Cuối cùng, Hình 9 và 10 cho thấy rằng sự khác biệt điểm trở nên nổi bật hơn khi các mô hình trở nên lớn hơn. Chúng tôi kết luận rằng khả năng từ chối là một khả năng nổi lên (Wei et al., 2022).

5.3 Câu hỏi Không thể trả lời

Ngoài tập dữ liệu hỏi-đáp mở trong đó tất cả các câu hỏi đều có thể trả lời, chúng tôi cũng kiểm tra hiệu suất của R-Tuning trên một số chuẩn từ chối chứa các câu hỏi không thể trả lời. Những câu hỏi này hoặc mâu thuẫn với ý thức chung

--- TRANG 8 ---
[Bảng 3: Tỷ lệ từ chối (%) của R-Tuning và các đường cơ sở khác trên các chuẩn từ chối. SA là phần không thể trả lời của tập dữ liệu SelfAware. Tỷ lệ từ chối của R-Tuning-R trên các tập dữ liệu không thể trả lời cực kỳ cao, trong khi tỷ lệ từ chối của các phương pháp được tinh chỉnh khác và các mô hình được huấn luyện trước thấp.]

hoặc tạo ra một số khái niệm, và không nên nhận được câu trả lời từ mô hình. Chúng tôi xác minh R-Tuning trên các tập dữ liệu như vậy, và kết quả được hiển thị trong Bảng 3. Đối với các mô hình cơ sở, chúng tôi cung cấp rõ ràng trong nhắc rằng chúng có thể từ chối trả lời các câu hỏi. Chúng tôi quan sát rằng R-Tuning từ chối gần như tất cả các câu hỏi không thể trả lời này, đáp ứng kỳ vọng của chúng tôi, trong khi các đường cơ sở khác trả lời hầu hết các câu hỏi mặc dù chúng được yêu cầu từ chối. Kết luận, R-Tuning sở hữu khả năng từ chối các câu hỏi mâu thuẫn với ý thức chung hoặc ngoài kiến thức tham số của chúng.

5.4 Độ phức tạp và Entropy

Chúng tôi tiếp tục chứng minh lý do của phương pháp chúng tôi bằng cách đánh giá độ phức tạp và entropy của dữ liệu chắc chắn D1 và dữ liệu không chắc chắn D0. Kết quả được hiển thị trong Bảng 4 và Bảng 5 tương ứng. Cụ thể, chúng tôi tính toán độ phức tạp của mỗi câu hỏi huấn luyện bằng mô hình được huấn luyện trước để ước tính sự hiểu biết của nó về chúng. Độ phức tạp thấp hơn của D1 cho thấy rằng mô hình được huấn luyện trước quen thuộc hơn với chúng và có khả năng cung cấp câu trả lời đúng, trong khi độ phức tạp cao của D0 tương ứng với các ảo giác nó cung cấp, thay vì các câu trả lời đúng. Bên cạnh đó, các mô hình lớn hơn thường có độ phức tạp thấp hơn, điều này giải thích tại sao chúng hoạt động tốt hơn trên các tác vụ khác nhau.

Chúng tôi cũng tận dụng GPT-3.5-turbo để trả lời các câu hỏi từ D0 và D1, và tính toán entropy của các giải pháp cho mỗi câu hỏi. Nếu GPT-3.5-turbo cung cấp nhiều giải pháp cho câu hỏi, entropy tương đối cao, ngược lại nó nên thấp. Chúng tôi quan sát rằng entropy của các câu trả lời từ D1 thấp hơn đáng kể so với entropy của D0, điều này giải thích rằng phương pháp của chúng tôi

[Bảng 4: Độ phức tạp của các tập dữ liệu huấn luyện. Chúng tôi chạy các mô hình được huấn luyện trước trên ngữ cảnh và câu hỏi và tính toán độ phức tạp trung bình.]

chia dữ liệu thành hai phần. Dữ liệu không chắc chắn về cơ bản khó khăn hơn dữ liệu chắc chắn. Và chiến lược R-Tuning trên D0 và D1 dạy mô hình trả lời các câu hỏi dễ với sự chắc chắn trong khi thận trọng trong việc trả lời các câu hỏi khó. Phân tích chi tiết hơn về độ phức tạp và entropy được hiển thị trong Phụ lục A.6 và Phụ lục A.7

6 Công trình Liên quan

Trong phần này, chúng tôi xem xét tiến bộ về ảo giác của các mô hình ngôn ngữ lớn (LLM) và các phương pháp định lượng tính không chắc chắn.

6.1 Ảo giác của LLM

Bất chấp hiệu suất xuất sắc của LLM với độ trôi chảy và nhất quán cao, chúng vẫn có khả năng ảo giác các sự kiện không trung thực và không thực tế (Maynez et al., 2020b; Li et al., 2023c). Nguồn gốc của ảo giác rất đa dạng. Dữ liệu huấn luyện, huấn luyện mô hình, và các quá trình suy luận mô hình đều có tiềm năng đóng góp vào ảo giác (Zhang et al., 2023c; Ji et al., 2023; Huang et al., 2023b). Một lượng lớn dữ liệu huấn luyện có thể chứa thông tin sai lệch và thiên vị (Dziri et al., 2022; Penedo et al., 2023), dẫn mô hình bắt chước sự giả dối (Lin et al., 2022). Hơn nữa, các sự kiện phát triển theo thời gian (Wen et al., 2021; Reddy et al., 2023), và dữ liệu lỗi thời được sử dụng để huấn luyện có thể đóng góp vào vấn đề sai lệch thời gian (Livska et al., 2022; Luu et al., 2022). Ngoài ra, LLM có xu hướng đánh giá quá cao khả năng của chúng, dẫn chúng đôi khi tạo ra các câu trả lời không chính xác với độ tin cậy cao

--- TRANG 9 ---
[Bảng 5: Entropy của các tập dữ liệu huấn luyện. Nó được tính toán từ tần suất của mỗi câu trả lời được dự đoán trong tất cả các dự đoán. Entropy lớn hơn biểu thị tính không chắc chắn lớn hơn của hệ thống.]

và không thể xác định các câu hỏi chưa biết (Yin et al., 2023; Ren et al., 2023; Kadavath et al., 2022). Bên cạnh đó, việc điều chỉnh với sở thích của con người có thể có vấn đề, vì LLM có thể tạo ra các phản hồi ủng hộ người dùng, thay vì cung cấp sự thật (Perez et al., 2022; Radhakrishnan et al., 2023; Wei et al., 2023b). Hơn nữa, quá trình tạo, bao gồm tính ngẫu nhiên trong quá trình suy luận (Chuang et al., 2023), hiệu ứng tuyết lăn để duy trì tính nhất quán với các lỗi sớm (Zhang et al., 2023a), và tối ưu hóa cục bộ sớm (Azaria và Mitchell, 2023), cũng có thể gây ra ảo giác.

Gần đây, nhiều công trình đã được thực hiện hướng tới phát hiện và giảm thiểu ảo giác. Đối với phát hiện ảo giác, Azaria và Mitchell (2023) đề xuất một bộ phân loại được huấn luyện trên các trạng thái bên trong của LLM. Lee et al. (2023) tạo ra một chuẩn để đo lường tính thực tế của việc tạo, sử dụng các nhắc thực tế và không thực tế. Manakul et al. (2023) giới thiệu SelfCheckGPT, tận dụng tính nhất quán của nhiều phản hồi từ LLM. Đối với kiểm soát ảo giác, các phương pháp tăng cường truy xuất (Peng et al., 2023; Xie et al., 2023; Yue et al., 2023; Lyu et al., 2023; Asai et al., 2023) đã cho thấy hiệu quả trong việc giảm thiểu ảo giác. Các phương pháp khác, chẳng hạn như tinh chỉnh nhận thức kiến thức (Li et al., 2022), khử nhiễu tham nhũng (Chen et al., 2023), xác thực tin cậy thấp (Varshney et al., 2023), xếp hạng phản hồi dựa trên tính không chắc chắn (Wan et al., 2024), điều chỉnh câu hỏi-kiến thức (Zhang et al., 2023b), tiêm kiến thức và mô hình thầy-học sinh (Elaraby et al., 2023), cũng cải thiện tính thực tế của việc tạo từ nhiều góc độ. Các nghiên cứu trước đây cho thấy tầm quan trọng của việc phát hiện sớm ảo giác (Zhang et al., 2023a). Ngoài ra, Huang et al. (2023a) tìm thấy rằng LLM không thể tự sửa chữa với khả năng ban đầu của chúng, thể hiện tầm quan trọng của tinh chỉnh và phản hồi bên ngoài. Phương pháp đề xuất của chúng tôi hướng dẫn mô hình nhận thức về khoảng cách kiến thức giữa các tập dữ liệu điều chỉnh hướng dẫn và kiến thức tham số, để nó sở hữu khả năng từ chối khi gặp phải các hướng dẫn ngoài kiến thức của nó.

6.2 Định lượng Tính không chắc chắn của LLM

Định lượng tính không chắc chắn là một vấn đề lâu đời trong học máy. Trong kỷ nguyên học sâu, Guo et al. (2017) đầu tiên xác định độ tin cậy dự đoán (a.k.a, xác suất dự đoán) của mạng thần kinh sâu thiếu hiệu chỉnh về số liệu ECE (Lỗi Hiệu chỉnh Kỳ vọng) (Naeini et al., 2015). Chen et al. (2022) tiếp tục nghiên cứu điều tra vấn đề hiệu chỉnh của các mô hình ngôn ngữ lớn được huấn luyện trước và quan sát cùng vấn đề hiệu chỉnh sai trên các mô hình ngôn ngữ lớn. ActivePrompt (Diao et al., 2023b) giới thiệu tính không chắc chắn để lựa chọn câu hỏi cho chú thích chuỗi suy nghĩ và chứng minh hiệu quả của nó trong việc lựa chọn tích cực và khôn ngoan và chú thích các mẫu hữu ích nhất cho học trong ngữ cảnh của LLM. Đánh giá kiến thức cho LLM (Dong et al., 2023) cũng liên quan đến nghiên cứu của chúng tôi.

7 Kết luận

Trong bài báo này, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả, R-Tuning, để dạy LLM từ chối các câu hỏi chưa biết. Nó xác định sự khác biệt giữa dữ liệu điều chỉnh hướng dẫn và kiến thức tham số và chia dữ liệu huấn luyện thành các phần chắc chắn và không chắc chắn. Sau đó, R-Tuning xây dựng dữ liệu nhận thức về từ chối bằng cách bổ sung các biểu hiện không chắc chắn vào phần không chắc chắn. Thực nghiệm, R-Tuning vượt trội hơn đường cơ sở tinh chỉnh truyền thống về điểm AP, minh họa sự đánh đổi tốt giữa dự đoán và độ tin cậy. R-Tuning không chỉ cho thấy khả năng từ chối trên dữ liệu trong miền mà còn chứng minh rằng khả năng như vậy có thể được tổng quát hóa cho các tác vụ chưa thấy. Nó thể hiện rằng từ chối là một khả năng cơ bản và có thể được trừu tượng hóa thông qua học đa tác vụ, vì vậy chúng tôi gọi nó là kỹ năng meta.

--- TRANG 10 ---
8 Hạn chế

Mặc dù R-Tuning chứng minh hiệu suất đáng chú ý trong việc lựa chọn và từ chối câu hỏi, vẫn có những hạn chế cần xem xét. Trước hết, R-Tuning chỉ sở hữu khả năng nói I am sure và I am unsure, trong đó độ tin cậy là nhị phân. Tuy nhiên, tạo ra một giá trị định lượng để thể hiện bằng lời độ tin cậy của nó đối với các câu hỏi sẽ chính xác hơn. Ngoài ra, chúng tôi chỉ áp dụng kiểm tra câu trả lời và định lượng tính không chắc chắn để đánh giá liệu kiến thức có liên quan có nằm trong kiến thức tham số của mô hình được huấn luyện trước hay không. Có những phương pháp nghiêm ngặt khác để đánh giá, chẳng hạn như so sánh các tập dữ liệu điều chỉnh hướng dẫn với các tập dữ liệu huấn luyện trước. Người ta có thể theo Kandpal et al. (2023) để xác định kiến thức có liên quan bằng cách liên kết thực thể các tập dữ liệu huấn luyện trước. Do chi phí tính toán cao của phương pháp liên kết thực thể, chúng tôi có kế hoạch khám phá các phương pháp tối ưu hóa để cải thiện hiệu quả trong công việc tương lai.

Lời cảm ơn

Chúng tôi cảm ơn các nhà đánh giá ẩn danh vì những gợi ý và ý kiến có giá trị của họ. Shizhe Diao được hỗ trợ bởi Chương trình Học bổng Tiến sĩ Hồng Kông (HKPFS) và Giải thưởng Nghiên cứu Hải ngoại của Đại học Khoa học và Công nghệ Hồng Kông. Nghiên cứu này được hỗ trợ một phần bởi Chương trình ITM DARPA Hoa Kỳ số FA8650-23-C-7316. Các quan điểm và kết luận chứa trong đây là của các tác giả và không nên được hiểu như nhất thiết đại diện cho các chính sách chính thức, được thể hiện hoặc ngụ ý, của DARPA, hoặc Chính phủ Hoa Kỳ. Chính phủ Hoa Kỳ được ủy quyền sao chép và phân phối các bản in lại cho mục đích chính phủ bất chấp bất kỳ chú thích bản quyền nào trong đó.

Tài liệu tham khảo

[Danh sách các tài liệu tham khảo được dịch tiếp theo, bao gồm tất cả các trích dẫn từ trang 10-13]

--- TRANG 14 ---
[Bảng 6: Minh họa và thống kê của các tập dữ liệu. Đối với ParaRel và MMLU, chúng tôi chia thủ công các tập dữ liệu thành các tập huấn luyện và kiểm tra. Đối với WiCE, HotpotQA, và FEVER, chúng tôi trực tiếp sử dụng tập huấn luyện gốc. Đối với SelfAware, FalseQA, và NEC, chúng tôi trực tiếp kiểm tra các mô hình trên các câu hỏi không thể trả lời của chúng.]

--- TRANG 15 ---
A Phụ lục

A.1 Tập dữ liệu

Chúng tôi tiến hành các thí nghiệm trên chín tập dữ liệu, được mô tả như sau.

• ParaRel (Elazar et al., 2021): một tập dữ liệu về kiến thức thực tế với các nhắc và quan hệ khác nhau ban đầu cho dự đoán mặt nạ. Để điều chỉnh tập dữ liệu với các yêu cầu của các mô hình tự hồi quy của chúng tôi, chúng tôi đầu tiên thay đổi định dạng thành hỏi-đáp và các mô hình của chúng tôi đọc các câu hỏi và tạo ra các câu trả lời. Sau đó, các nhắc trùng lặp của các mẫu khác nhau nhưng với cùng các thực thể được bỏ qua cho tác vụ hỏi-đáp của chúng tôi. Cuối cùng nó đưa ra 25.133 cặp nhắc-câu trả lời của 31 miền. Chúng tôi chia ParaRel thành hai tập - 15 miền đầu tiên làm dữ liệu trong miền và 16 miền cuối làm dữ liệu ngoài miền. Chúng tôi cũng chia đều dữ liệu trong miền thành dữ liệu huấn luyện và dữ liệu kiểm tra.

• MMLU (Hendrycks et al., 2021): MMLU bao gồm 57 tác vụ bao gồm toán học, khoa học máy tính, lịch sử, luật, và nhiều hơn nữa, đòi hỏi kiến thức thế giới rộng lớn và khả năng giải quyết vấn đề. Tập dữ liệu có định dạng trắc nghiệm, và chúng tôi có thể sử dụng trực tiếp nó trong các thí nghiệm của chúng tôi.

• WiCE (Kamoi et al., 2023): WiCE là một tập dữ liệu suy luận ngôn ngữ tự nhiên (NLI) cho suy luận văn bản. Mỗi mẫu dữ liệu bao gồm bằng chứng và một tuyên bố, và mô hình nên quyết định xem bằng chứng có hỗ trợ, hỗ trợ một phần, hay không hỗ trợ tuyên bố. Chúng tôi biến tập dữ liệu thành các câu hỏi trắc nghiệm với 3 lựa chọn cho mỗi câu hỏi.

• HotpotQA (Yang et al., 2018): HotpotQA là một tập dữ liệu hỏi-đáp đòi hỏi lý luận phức tạp giữa các tài liệu. Chúng tôi đánh giá bằng cách cung cấp các tài liệu ngữ cảnh và câu hỏi để xem liệu mô hình có thể trả lời chúng không. Vì tập kiểm tra của HotpotQA yêu cầu nộp câu trả lời, chúng tôi thay vào đó sử dụng tập phát triển để thực hiện đánh giá.

• FEVER (Thorne et al., 2018): FEVER là một tập dữ liệu chứa các tuyên bố và kiến thức hỗ trợ. Các tuyên bố được phân loại là SUPPORTED, REFUTES, hoặc NOT ENOUGH INFO. Chúng tôi biến nó thành một tác vụ NLI trắc nghiệm.

• SelfAware (Yin et al., 2023): một tập dữ liệu chứa cả các câu hỏi có thể trả lời và các câu hỏi không thể trả lời. Chúng tôi đánh giá các câu hỏi không thể trả lời. Dự kiến thấy các mô hình được tinh chỉnh của chúng tôi từ chối các câu hỏi không thể trả lời trong khi các đường cơ sở khác không sở hữu khả năng như vậy.

• HaluEval (Li et al., 2023a): HaluEval là một tập dữ liệu chứa hỏi-đáp, đối thoại, tóm tắt, và truy vấn người dùng với các câu trả lời đúng và các câu trả lời ảo giác. Chúng tôi chỉ lấy phần hỏi-đáp.

• FalseQA (Hu et al., 2023): FalseQA là một tập dữ liệu miền mở mới với các câu hỏi không nhất quán với ý thức chung. Không có câu trả lời đúng cho các câu hỏi.

• NEC (Liu et al., 2024): NEC cũng là một tập dữ liệu miền mở mới với các câu hỏi chứa một số khái niệm bịa đặt. Cũng không có câu trả lời đúng cho các câu hỏi.

Đối với các tác vụ hỏi-đáp, để so sánh câu trả lời được tạo ra bởi mô hình của chúng tôi với câu trả lời sự thật nền tảng, chúng tôi kiểm tra xem các token đầu ra đầu tiên có chứa câu trả lời sự thật nền tảng hay không. Chúng tôi không áp dụng khớp chính xác (EM) vì việc tạo không thể kiểm soát nghiêm ngặt. Đối với các câu hỏi trắc nghiệm, chúng tôi hạn chế mô hình tạo ra một token và chọn lựa chọn có xác suất tối đa trong số các lựa chọn ứng viên bằng argmax_{x in C} logits(x), trong đó C là tập hợp các lựa chọn ứng viên. Xem xét kích thước khổng lồ của HotpotQA và FEVER, chúng tôi lấy mẫu ngẫu nhiên 10K dữ liệu huấn luyện từ chúng, tương ứng. Chi tiết thêm về các tập dữ liệu gốc được hiển thị trong Phụ lục A.1 và Bảng 6. Trong Hình 6, chúng tôi trình bày phân phối của dữ liệu nhận thức về từ chối được xây dựng D0 và D1.

Chi tiết về các tập dữ liệu gốc được hiển thị trong Bảng 6. Trong Hình 6, chúng tôi trình bày phân phối của dữ liệu nhận thức về từ chối được xây dựng D0 và D1.

A.2 Triển khai

Chúng tôi chọn OpenLLaMA-3B (Geng và Liu, 2023), LLaMA-7B, và LLaMA-13B (Touvron et al., 2023) làm các mô hình cơ sở trong các thí nghiệm của chúng tôi. Chúng tôi sử dụng LMFlow³ (Diao et al., 2023a) để tiến hành điều chỉnh hướng dẫn, đặt epoch là 1, tốc độ học là 2e−5, và kích thước batch là 4. Tất cả các thí nghiệm được triển khai trên GPU Nvidia A100-40GB. Chúng tôi tiến hành thí nghiệm với một quét siêu tham số bao gồm tốc độ học trong {1e−5, 2e−5, 5e−5} và kích thước batch trong {2, 4, 8} trên tập huấn luyện.

A.3 Thay thế Nhãn

Trong các thí nghiệm chính, chúng tôi áp dụng phương pháp đệm cho xây dựng dữ liệu. Ngoài

³https://github.com/OptimalScale/LMFlow

--- TRANG 16 ---
Hình 5: Hiệu suất của R-Tuning-R trên các tập dữ liệu ParaRel và MMLU. ID và OOD biểu thị các tập dữ liệu kiểm tra trong miền và ngoài miền, tương ứng.

[Bảng 7: Tỷ lệ từ chối (%) của R-Tuning và R-Tuning-R, và các đường cơ sở khác trên các chuẩn từ chối. SA là phần không thể trả lời của tập dữ liệu SelfAware. Tỷ lệ từ chối của R-Tuning-R trên các tập dữ liệu không thể trả lời cực kỳ cao, trong khi tỷ lệ từ chối của các phương pháp được tinh chỉnh khác và các mô hình được huấn luyện trước thấp.]

đệm, chúng ta có thể trực tiếp thay thế các từ nhãn bằng các biểu hiện không chắc chắn cho các câu hỏi không chắc chắn và giữ các từ nhãn gốc cho các câu hỏi chắc chắn, được gọi là chiến lược thay thế, dẫn đến một biến thể R-Tuning-R. Ví dụ, phần chắc chắn của các câu hỏi huấn luyện D1 được xây dựng như sau:

Q:{Câu hỏi}, A:{Câu trả lời}, (9)

trong khi tập dữ liệu không chắc chắn D0 được xây dựng như sau:

Q:{Câu hỏi}, A:{Biểu hiện Không chắc chắn}. (10)

Có nhiều cách khác nhau cho biểu hiện không chắc chắn. Để tăng tính đa dạng, chúng tôi lấy 16 biểu hiện của văn bản không chắc chắn từ Yin et al. (2023). 16 biểu hiện này được liệt kê trong Phần Phụ lục A.5.

Chúng tôi tiến hành thí nghiệm với R-Tuning-R trên các tập dữ liệu ParaRel và MMLU bằng cách so sánh nó với chiến lược tinh chỉnh vanilla và các mô hình được huấn luyện trước gốc. Kết quả được hiển thị trong Hình 5. Đầu tiên, trên cả tập kiểm tra trong miền và ngoài miền, độ chính xác của R-Tuning-R cao hơn Pretrain-T, điều này được hưởng lợi từ việc chỉ trả lời các câu hỏi chắc chắn. Kết quả chi tiết hơn với tỷ lệ trả lời được báo cáo trong Bảng 8, nơi chúng tôi tìm thấy mô hình có thể từ chối một số lượng nhất định các câu hỏi. Sau đó, R-Tuning-R vượt trội hơn Vanilla với độ chính xác cao hơn đáng kể trên các câu hỏi được trả lời sẵn sàng của nó, điều này chứng minh hiệu quả của phương pháp của chúng tôi. Nó đầy hứa hẹn vì R-Tuning-R được huấn luyện với ít nhãn sự thật nền tảng hơn, trong khi Vanilla được huấn luyện trên tất cả các nhãn của dữ liệu huấn luyện đầy đủ. Nói chung, các mô hình lớn hơn sở hữu khả năng từ chối mạnh mẽ hơn. Trong Hình 5, chúng tôi quan sát rằng trên các câu hỏi được trả lời sẵn sàng, các mô hình lớn hơn đạt được độ chính xác cao hơn. Ngoài ra, độ chính xác cao của Pretrain-W tiết lộ rằng những câu hỏi được chọn đó nằm trong kiến thức tham số của mô hình được huấn luyện trước. Tóm lại, so với tinh chỉnh vanilla, R-Tuning-R cung cấp cho mô hình khả năng từ chối để từ chối các câu hỏi chưa biết, cuối cùng cải thiện độ chính xác và ngăn chúng đưa ra các câu trả lời ảo giác. Bảng 9 cho thấy các nghiên cứu trường hợp về cách R-Tuning-R hoạt động. Có những khác biệt đáng kể khi chúng gặp phải các câu hỏi ngoài kiến thức của chúng. Mô hình Vanilla chủ động trong việc bịa ra một câu trả lời, đó là một ảo giác và không có ý nghĩa. Tuy nhiên, R-Tuning-R từ chối chúng một cách rõ ràng với các từ khóa do not know, not known, và impossible. Khả năng của R-Tuning-R trong việc từ chối các câu hỏi chưa biết dẫn đến ít ảo giác hơn.

Mặc dù khả năng từ chối này, có hai vấn đề với R-Tuning-R: (1) phương pháp thay thế vứt bỏ các nhãn có giá trị có thể được tận dụng để huấn luyện. (2) R-Tuning có thể chỉ đưa ra câu trả lời hoặc chỉ đưa ra sự chắc chắn, nhưng không thể phản hồi cả hai, dẫn đến khó khăn trong việc xem xét độ chính xác và thu hồi đồng thời. Để tận dụng tất cả các nhãn sự thật nền tảng trong quá trình điều chỉnh, và hướng dẫn các mô hình dự đoán câu trả lời và thể hiện tính không chắc chắn cùng lúc, chúng tôi sử dụng chiến lược đệm trong phương pháp chính của chúng tôi,

--- TRANG 17 ---
[Bảng 8: Hiệu suất chi tiết của R-Tuning-R trên tập dữ liệu ParaRel và MMLU. Tỷ lệ trả lời có nghĩa là tỷ lệ phần trăm của các câu hỏi được trả lời sẵn sàng của R-Tuning-R.]

[Bảng 9: Nghiên cứu trường hợp về các câu hỏi bị từ chối và được trả lời sẵn sàng với R-Tuning-R và Vanilla.]

trong đó mỗi câu hỏi được bổ sung với nhãn sự thật nền tảng và biểu hiện không chắc chắn, chỉ ra liệu mô hình có tự tin hay không.

A.4 Nghiên cứu Trường hợp của R-Tuning-R

Trong phần này, chúng tôi hiển thị thống kê chi tiết trong Bảng 8, và minh họa nhiều nghiên cứu trường hợp hơn của R-Tuning-R trong Bảng 9.

A.5 Văn bản Không chắc chắn

Trong phần này, chúng tôi liệt kê 16 biểu hiện không chắc chắn từ Yin et al. (2023):

1. The answer is unknown.
2. The answer is uncertain.
3. The answer is unclear.
4. There is no scientific evidence.
5. There is no definitive answer.
6. There is no right answer.
7. There is much debate.
8. There is no known case.
9. There is no concrete answer to this question.
10. There is no public information available.
11. It is impossible to know.
12. It is impossible to answer.
13. It is difficult to predict.
14. It is not known.
15. We do not know.
16. I'm not sure.

--- TRANG 18 ---
[Bảng 10: Độ phức tạp của các tập dữ liệu huấn luyện. Chúng tôi chạy các mô hình được huấn luyện trước trên ngữ cảnh và câu hỏi và tính toán độ phức tạp trung bình.]

A.6 Độ phức tạp của Tập dữ liệu

Độ phức tạp đo lường mô hình ngôn ngữ dự đoán một văn bản nhất định tốt như thế nào. Độ phức tạp thấp hơn có nghĩa là dự đoán và hiểu biết tốt hơn về văn bản. Theo nhận dạng dữ liệu nhận thức về từ chối, chúng tôi chia dữ liệu huấn luyện thành hai tập: D0 (các câu hỏi không chắc chắn) và D1 (các câu hỏi chắc chắn). Để khám phá tại sao mô hình được huấn luyện trước phản hồi với chúng khác nhau, chúng tôi tính toán độ phức tạp trung bình trên hai tập dữ liệu này với các mô hình được huấn luyện trước. Độ phức tạp được tính toán như sau:

PPL(X) = exp(−1/t ∑(i=1 to t) log p_theta(xi|x<i)), (11)

trong đó X biểu thị một câu bao gồm các token và X = (x1, x2, . . . , xt). Cụ thể, chúng tôi tính toán độ phức tạp của các câu hỏi huấn luyện để ước tính sự hiểu biết của mô hình được huấn luyện trước về chúng. Kết quả được hiển thị trong Bảng 10. Chúng tôi quan sát rằng D1 có độ phức tạp thấp hơn, chứng minh rằng mô hình được huấn luyện trước quen thuộc hơn với các câu hỏi và có khả năng cung cấp câu trả lời đúng. Đối với D0, độ phức tạp cao hơn của nó cho thấy rằng những câu hỏi này không quen thuộc với mô hình và ngoài kiến thức của mô hình, và đây là lý do tại sao mô hình có xu hướng ảo giác văn bản thay vì cung cấp các câu trả lời đúng. Chúng tôi cũng quan sát rằng các mô hình lớn hơn có độ phức tạp thấp hơn và tính ngẫu nhiên trên các câu hỏi, đó là lý do tại sao các mô hình lớn hơn thường hoạt động tốt hơn trên các tác vụ khác nhau.

Bằng cách hướng dẫn mô hình của chúng tôi thể hiện tính không chắc chắn

[Bảng 11: Entropy của các tập dữ liệu huấn luyện. Nó được tính toán từ tần suất của mỗi câu trả lời được dự đoán trong tất cả các dự đoán. Entropy lớn hơn biểu thị tính không chắc chắn lớn hơn của hệ thống.]

đối với các câu hỏi tương đối ngẫu nhiên về độ phức tạp, mô hình phát triển sự hiểu biết tốt hơn về tính không chắc chắn và mơ hồ và học khả năng nhận ra khi nó không biết. Khả năng này rất quan trọng trong các tình huống mà việc chỉ đơn giản cung cấp một câu trả lời chắc chắn có thể không phù hợp hoặc thậm chí có hại. Mặt khác, vì mô hình của chúng tôi cũng được huấn luyện với dữ liệu có các biểu hiện chắc chắn, nó trở nên thành thạo hơn trong việc xử lý các câu hỏi ít ngẫu nhiên hơn, và trả lời chúng với sự tự tin và chắc chắn. Nhìn chung, R-Tuning cải thiện khả năng của mô hình thích ứng với các mức độ ngẫu nhiên câu hỏi khác nhau.

Để xác minh mô hình được huấn luyện trước ít quen thuộc hơn với các câu hỏi không chắc chắn trong khi tự tin hơn với các câu hỏi chắc chắn, chúng tôi cũng vẽ phân phối tin cậy trên các câu hỏi chắc chắn và các câu hỏi không chắc chắn, được hiển thị trong Hình 7 trong Phụ lục A.9. Quan sát rằng một tỷ lệ lớn hơn của các câu hỏi chắc chắn chiếm các khoảng tin cậy cao, có nghĩa là khi mô hình cung cấp các câu trả lời đúng, nó thường cho thấy tin cậy lớn hơn.

--- TRANG 19 ---
A.7 Entropy của Câu trả lời

[Bảng 12: Độ chính xác (%) và tỷ lệ trả lời (%) của R-Tuning-R và huấn luyện min-loss trên tập dữ liệu ParaRel. Mất mát được tính toán bởi token đầu tiên của câu trả lời sự thật nền tảng. ID và OOD biểu thị trong miền và ngoài miền, tương ứng.]

Ngoài việc đánh giá sự khác biệt giữa các câu hỏi chắc chắn và không chắc chắn với các mô hình được huấn luyện trước, chúng tôi tiếp tục tận dụng GPT (Brown et al., 2020) để điều tra các mẫu của các câu hỏi chắc chắn và không chắc chắn. Cụ thể, chúng tôi truy vấn gpt-3.5-turbo năm lần với các nhắc Chuỗi Suy nghĩ (Wei et al., 2023a) với nhiệt độ 0.7, và tính toán entropy của các câu trả lời đối với cùng một câu hỏi (Diao et al., 2023b). Nếu mô hình cung cấp nhiều câu trả lời khác nhau cho cùng một câu hỏi, entropy nên cao. Ngược lại, entropy nên thấp. Kết quả được hiển thị trong Bảng 11. Chúng tôi quan sát rằng entropy trung bình của các câu trả lời trên dữ liệu chắc chắn D1 thấp hơn entropy của dữ liệu không chắc chắn D0 trong hầu hết các trường hợp, điều này minh họa rằng khi được cho các câu hỏi chắc chắn, gpt-3.5-turbo có nhiều khả năng tạo ra các câu trả lời nhất quán hơn. Nó sẽ tạo ra các câu trả lời ảo giác cho các câu hỏi không chắc chắn với cơ hội cao hơn nhiều.

Do đó, chúng ta có thể kết luận rằng R-Tuning chia dữ liệu thành hai phần. Các câu hỏi không chắc chắn thường khó khăn hơn các câu hỏi chắc chắn vì các câu trả lời của gpt-3.5-turbo thay đổi nhiều hơn với dữ liệu không chắc chắn. R-Tuning trao cho mô hình khả năng xác định và phân biệt độ khó của các câu hỏi. Do đó, mô hình được tinh chỉnh của chúng tôi trở nên chủ động trong việc trả lời các câu hỏi dễ với sự chắc chắn trong khi thận trọng trong việc trả lời các câu hỏi khó, cuối cùng tăng độ chính xác và ngăn mô hình được tinh chỉnh phạm quá nhiều lỗi.

A.8 Huấn luyện Min-Loss

[Bảng 13: Thí nghiệm đa tác vụ của R-Tuning và Vanilla với điểm AP (%). Vanilla áp dụng độ tin cậy của câu trả lời được dự đoán để xếp hạng kết quả, trong khi R-Tuning áp dụng sự kết hợp của độ tin cậy của câu trả lời được dự đoán và độ tin cậy của sự chắc chắn.]

So với bộ biểu hiện bổ sung, bộ biểu hiện thay thế (ví dụ, R-Tuning-R) là một cách rõ ràng để tạo ra các biểu hiện không chắc chắn bằng cách vứt bỏ các nhãn có giá trị có thể được tận dụng để huấn luyện. Để giải quyết chiều hướng quan tâm này, chúng tôi xem xét một mục tiêu học entropy chéo được sửa đổi đẩy lên token câu trả lời đúng và giữ các biểu hiện không chắc chắn làm lựa chọn token có xác suất cao thứ hai. Chúng tôi gọi nó là huấn luyện min-loss, được tối ưu hóa bằng descent gradient trên mất mát tối thiểu giữa việc đoán câu trả lời đúng hoặc chỉ các biểu hiện không chắc chắn. Nó được hình thức hóa như sau:

min(L(predict, GT), L(predict, IDK)), (12)

trong đó L biểu thị mất mát entropy chéo. Để làm như vậy, chúng tôi chia dữ liệu huấn luyện làm đôi và áp dụng chiến lược huấn luyện hai giai đoạn. Trong giai đoạn đầu tiên, chúng tôi huấn luyện mô hình của chúng tôi sử dụng phương pháp gốc trong đó mẫu nhắc sử dụng The answer is {ground-truth} nếu mô hình trả lời đúng, ngược lại The answer is unknown. Một khi mô hình học được mẫu như vậy sau giai đoạn huấn luyện đầu tiên

--- TRANG 20 ---
[Bảng 14: Hiệu suất của R-Tuning-U so với Vanilla-C trên các tập dữ liệu ParaRel và MMLU. ID và OOD biểu thị trong miền và ngoài miền, tương ứng.]

[Bảng 15: Xác suất chắc chắn trung bình (%) của R-Tuning-U và Vanilla-C.]

giai đoạn huấn luyện, chúng tôi tính toán min-loss với phương trình 12. Chúng tôi chỉ xem xét mất mát của không biết và nhãn sự thật nền tảng, và chúng tôi che các token trước chúng. Vì nhãn sự thật nền tảng có thể xem xét nhiều hơn một token, chúng tôi tính toán mất mát cho token đầu tiên.

Chúng tôi đánh giá hiệu suất của chiến lược min-loss trên tập dữ liệu ParaRel, và kết quả được hiển thị trong Bảng 12. Nó cho thấy huấn luyện min-loss vượt trội hơn R-Tuning-R trong các mô hình nhỏ và thiết lập trong miền. Tuy nhiên, nó kém hiệu suất hơn R-Tuning-R trong các tập kiểm tra ngoài miền. Chúng tôi cũng nhận thấy rằng trong các tập kiểm tra ngoài miền, độ chính xác của mô hình kích thước 3B gần như giống với 7B và 13B, nhưng tỷ lệ trả lời của nó thấp hơn nhiều. Chúng tôi xác định các vấn đề như vậy như một sự đánh đổi giữa độ chính xác và tỷ lệ trả lời. Khi mô hình chủ động trong việc trả lời nhiều câu hỏi hơn, nó sẽ không tránh khỏi phạm nhiều lỗi hơn. Vì kiến thức tham số nội tại của mô hình bị hạn chế, không có phương pháp nào để

[Bảng 16: ECE (Lỗi Hiệu chỉnh Kỳ vọng) của R-Tuning-U và Vanilla-C.]

tinh chỉnh một mô hình với cả độ chính xác cao và tỷ lệ trả lời cao.

A.9 Phân phối Tin cậy của Tập dữ liệu Huấn luyện

Chúng tôi tính toán độ tin cậy của dữ liệu chắc chắn D1 và dữ liệu không chắc chắn D0, và chúng được hiển thị trong Hình 7.

A.10 Điểm AP của Mỗi Tập dữ liệu và Kích thước Mô hình với Hình

Chúng tôi tính toán điểm AP cho mỗi tập dữ liệu với các kích thước mô hình khác nhau trong các thí nghiệm đa tác vụ. Kết quả được hiển thị trong Bảng 13 và Hình 8.

--- TRANG 21 ---
Hình 6: Phân phối dữ liệu của các tập dữ liệu nhận thức về từ chối thu được từ chiến lược nhận dạng có giám sát. Tiêu đề của mỗi hình phụ bao gồm tên tập dữ liệu và kích thước của mô hình được huấn luyện trước được sử dụng để đánh giá.

--- TRANG 22 ---
Hình 7: Phân phối tin cậy của các tập dữ liệu huấn luyện trên dữ liệu chắc chắn và dữ liệu không chắc chắn. Tiêu đề của mỗi hình phụ bao gồm tên tập dữ liệu và kích thước của mô hình được huấn luyện trước được sử dụng để đánh giá.

--- TRANG 23 ---
Hình 8: Các đường cong AP trên các tập dữ liệu ParaRel, MMLU, WiCE, HotpotQA, FEVER, và HaluEval-QA. Tiêu đề của mỗi hình phụ bao gồm tên tập dữ liệu và kích thước của mô hình được huấn luyện trước được sử dụng để đánh giá.

--- TRANG 24 ---
Hình 9: Phân phối phân tán của xác suất chắc chắn của R-Tuning-U và Vanilla-C.

Hình 10: Phân phối xác suất chắc chắn của R-Tuning-U và Vanilla-C. Cả hai đều được xếp hạng theo điểm tin cậy.

--- TRANG 25 ---
Hình 11: ECE (Lỗi Hiệu chỉnh Kỳ vọng) trên tập dữ liệu ParaRel của R-Tuning-U và Vanilla-C.

--- TRANG 26 ---
Hình 12: ECE (Lỗi Hiệu chỉnh Kỳ vọng) trên tập dữ liệu MMLU của R-Tuning-U và Vanilla-C.

--- TRANG 27 ---
Hình 13: Các đường cong AP của R-Tuning, R-Tuning-U, Vanilla-C, và Vanilla-U trên các tập dữ liệu ParaRel và MMLU.
