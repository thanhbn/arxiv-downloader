# 2306.12190.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/uncertainty/2306.12190.pdf
# File size: 1418397 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Quantifying lottery tickets under label noise:
accuracy, calibration, and complexity
Viplove Arora1Daniele Irto2Sebastian Goldt1Guido Sanguinetti1
1Theoretical and Scientific Data Science, SISSA, Trieste, Italy
2Data Science and Scientific Computing, University of Trieste, Trieste, Italy
Abstract
Pruning deep neural networks is a widely used
strategy to alleviate the computational burden in
machine learning. Overwhelming empirical evid-
ence suggests that pruned models retain very high
accuracy even with a tiny fraction of parameters.
However, relatively little work has gone into char-
acterising the small pruned networks obtained, bey-
ond a measure of their accuracy. In this paper, we
use the sparse double descent approach to identify
univocally and characterise pruned models associ-
ated with classification tasks. We observe empir-
ically that, for a given task, iterative magnitude
pruning (IMP) tends to converge to networks of
comparable sizes even when starting from full net-
works with sizes ranging over orders of magnitude.
We analyse the best pruned models in a controlled
experimental setup and show that their number of
parameters reflects task difficulty and that they are
much better than full networks at capturing the
true conditional probability distribution of the la-
bels. On real data, we similarly observe that pruned
models are less prone to overconfident predictions.
Our results suggest that pruned models obtained
via IMP not only have advantageous computational
properties but also provide a better representation
of uncertainty in learning.
1 INTRODUCTION
Conventional statistical wisdom suggests that increasing the
size of a model leads to an initial improvement in general-
isation performance, followed by dramatic overfitting and
degradation of accuracy in highly overparameterised mod-
els. In reality, the implicit regularisation of large models
leads to a double descent which, under suitable conditions,
leads to overparameterised models with even stronger gener-alisation performance [Vallet et al., 1989, Opper et al., 1990,
Geman et al., 1992, Belkin et al., 2019, Nakkiran et al.,
2021, Loog et al., 2020]. This phenomenon is well under-
stood theoretically in simple cases and has been replicated
in practice in a variety of modern deep neural networks ar-
chitectures [Belkin et al., 2019, Nakkiran et al., 2021, Arpit
et al., 2017, Neyshabur et al., 2019, Belkin et al., 2020].
However, decades of research on pruning neural networks
[LeCun et al., 1989, Hassibi and Stork, 1992, Gale et al.,
2019, Hoefler et al., 2021] has also shown that a large num-
ber of parameters (weights) in these overparameterized mod-
els can be removed without compromising on the general-
isation error. He et al. [2022] recently reported that iterative
pruning leads to a converse phenomenon to double descent,
asparse double descent : generalisation performance ini-
tially degrades upon pruning, and then improves to reach an
optimum frequently providing even better performance than
the original full model. The sparse double descent allows us
to identify an optimal model by plotting the generalisation
error as a function of the number of parameters during the it-
erative pruning process. How the architectural bias induced
by pruning enables the networks to reverse the double des-
cent curve is not understood theoretically, and is relatively
unexplored empirically.
Here we seek to address this knowledge gap in the iterative
magnitude pruning (IMP) framework [Han et al., 2015], a
simple and successful approach for pruning deep neural
networks [Blalock et al., 2020, Renda et al., 2020] which
is pivotal to finding “lottery tickets” [Frankle and Carbin,
2019, Frankle et al., 2019, 2020], i.e. sparse subnetworks
that can be trained in isolation without compromising on
accuracy. We start with a remarkable empirical observation.
Figure 1 shows the double descent curve (thick line) and the
sparse double descent curves (thin lines) for fully connected
networks trained on MNIST (left) and a ResNet-18 trained
on CIFAR-10 (right). Naturally, the starting point of the
double descent curve is fixed at the smallest possible model.
He et al. [2022] considered sparse double descent starting
from a single, large model. Here, we consider sparse double
Accepted for the 39thConference on Uncertainty in Artificial Intelligence (UAI 2023).arXiv:2306.12190v1  [cs.LG]  21 Jun 2023

--- PAGE 2 ---
103104105106107
Parameters0.00.10.20.30.40.50.6Error
Test
Train
104105106107
Parameters0.00.10.20.30.40.50.6Error
Test
TrainFigure 1: Pruning models along the double descent curve (dark red) shows that sparse double descent curves (light red)
from different models coincide at the minima. Results are shown for three-layer FCNs on MNIST (left) and ResNet-18 on
CIFAR-10 (right) with 20% label noise averaged over three replicates.
descent curves starting from neural networks with a large
range of parameters. We show in fig. 1 that irrespective
of the initial full model size (or indeed test accuracy), all
sparse double descent curves tend to achieve their minimum
(best pruned model) within a very narrow range of model
sizes, which is systematically smaller than the trough of
the first descent (best small full model). What is special
about such sparse models? What enables the improvement
in generalisation by pruning?
To answer this question quantitatively, we investigate the
behaviour of the best pruned models in a controlled binary
classification scenario where data is generated from distri-
butions of differing complexity. We find that the size of
the best pruned model correlates well with task complexity.
Moreover, best pruned models are considerably better at cap-
turing the underlying true conditional label distribution than
either full models (which tend to be significantly overconfid-
ent) or best small full models. These empirical observations
are replicated in our analysis of real data, suggesting that
IMP indeed captures important aspects of the uncertainty in
the learning problem, as well as producing computationally
more tractable models.
We see a key contribution of our empirical study in suggest-
ing a way to estimate the number of “effective” parameters
that a trained model contains, and that are required for solv-
ing a classification task. Quantifying the effective number
of parameters in trained neural networks has been a central
question for the theory of neural networks for a long time,
see for example Breiman [1995], yet it remains an open
challenge. In this manuscript, we focus on the empirical
findings and leave the theoretical analysis for future work.
Our main contributions are:
1.We find that IMP prunes models of different sizes to
produce sparse double descent curves that coincide
at the minima of the test error. We define the effect-ive number of parameters required for a given classi-
fication task and model/architecture. We demonstrate
this phenomenon in fully-connected and convolutional
neural networks trained on MNIST and CIFAR-10.
Additionally, the effective number of parameters is
comparable across architectures, suggesting that our
procedure identifies an intrinsic property of the classi-
fication task.
2.By training neural networks on binary Gaussian mix-
ture classification tasks of increasing difficulty, we
show that the effective number of parameters in a
model correlates with task difficulty.
3.We finally study the calibration of pruned models and
show that pruned models capture the true distribution
of the synthetic data models better than their unpruned
counterparts. As a consequence, we show that the best
pruned models are better at capturing uncertainty in
their predictions.
2 RELATED WORK
Dating back to 1998, Poppi and Massart [1998] used pruning
to find the optimal neural network architecture. They found
that pruning could improve generalisation and even recover
the optimal model in a linear dataset. Kuhn et al. [2021]
introduces a new complexity measure for neural networks,
namely the fraction of weights that can be pruned from
the network without affecting its performance. They found
that the fraction of prunable weights increases with network
width for a ResNet trained on CIFAR-10. Li et al. [2018]
uses the idea of subspace training to estimate the number of
parameters (or intrinsic dimension) needed to achieve good
performance using neural networks. They found that many
problems have smaller intrinsic dimensions than one might
suspect, and the intrinsic dimension for a given dataset var-
ies little across a family of models with vastly different sizes.

--- PAGE 3 ---
While these conclusions align with our findings, their res-
ults rely on the usage of random subspace solutions with a
performance ≈90% of the baseline to define the intrinsic di-
mension. Our approach instead uses sparsification to find the
effective number of parameters using models that generalise
better than the baseline.
Venkatesh et al. [2020] used different calibration strategies
on overparameterised models to study the impact on the res-
ulting lottery tickets. Lei et al. [2023] propose a new sparse
training method that improves the reliability of pruned mod-
els. From a theoretical perspective, Sakamoto and Sato
[2022] used PAC-Bayesian theory to understand the gener-
alisation behaviour of pruned networks. Zhang et al. [2021]
characterise the performance of training a pruned neural net-
work to show that pruning enlarges the convex region near a
desirable model with guaranteed generalisation. Yang et al.
[2023] used a controlled setting under random pruning to
determine pruning fractions that can improve generalisation
performance. A series of theoretical works [Malach et al.,
2020, Orseau et al., 2020, Pensia et al., 2020, da Cunha
et al., 2022, Burkholz, 2022] have shown the existence of a
winning ticket inside larger (deeper and wider) networks of
different sizes, thus providing some intuition on the amount
of overparameterisation required.
To understand how IMP can improve the generalisation
of neural networks by acting as a regulariser, Jin et al.
[2022] studied the loss of influential samples in the op-
timally pruned models. Paul et al. [2022] use the geometry
of the error landscape at each level of pruning to understand
the principles behind the success of IMP (with rewinding)
without label noise. Our focus is primarily on the properties
(accuracy, number of parameters, and calibration) of the
best pruned models. Ankner et al. [2022] used the frame-
work of Pope et al. [2021], which uses GANs to generate
images with known intrinsic dimensions, to find that the in-
trinsic dimensionality of data correlates with the prunability
of neural networks. Certain efforts have also been made to
understand the masks learned using pruning [Paganini and
Forde, 2020, Pellegrini and Biroli, 2021].
Since the rediscovery of the double descent behaviour in
deep neural networks [Belkin et al., 2019, Nakkiran et al.,
2021], characterising double descent curves from simple
models to deep networks has become a very active area
of research, see Hastie et al. [2022], Spigler et al. [2019],
Advani et al. [2020], Belkin et al. [2019], d’Ascoli et al.
[2020a], Lin and Dobriban [2021], d’Ascoli et al. [2020b],
Mei and Montanari [2022] for a small sample. Here, our
focus is not on this double descent phenomenon – instead,
our goal is to provide a univocal procedure to associate a
pruned model with a large model using the sparse double
descent.3 EXPERIMENTAL SETUP
Datasets: We used MNIST [LeCun et al., 1998] and
CIFAR-10 [Krizhevsky et al., 2009] for our experiments
with real data. Additional experiments were also performed
using the Fashion-MNIST dataset [Xiao et al., 2017] (see
fig. S3 for results). We also perform experiments in a con-
trolled setting, where we consider binary mixture classi-
fication with inputs sampled from a Gaussian mixture in
D= 100 dimensions. This simple setting for the data
model allows us to precisely compute the true conditional
class probability of data and characterise the actual decision
boundary. This allows us to closely investigate the architec-
tural bias induced by pruning. We consider two settings for
mixture classification. The linear dataset consists of two
clusters that can be separated using a linear classifier. The
two clusters have means µ1̸=µ2, but same covariance
matrix Σ1= Σ 2. The XOR dataset was created such that
the resulting Gaussians (that have the same covariance) are
placed like the graphic representation of the XOR logical
function. The training and test sets contain 10 000 and5000
samples respectively. See appendix A for more details.
A crucial aspect of our experiments is adding symmetric
label noise by randomly permuting the labels for a frac-
tion of the training data. Adding label noise to training data
provides a straightforward way to produce double descent
[Nakkiran et al., 2021] and sparse double descent [He et al.,
2022] in deep neural networks. Note that using other kinds
of label noise could provide different conclusions, but is
not the focus of the current study and hence is out of scope
for this paper. While adding label noise seems unrealistic,
Northcutt et al. [2021] found that labelling errors are per-
vasive in several benchmark machine learning datasets and
lower capacity models may be more practical. Building
on this observation, we show that pruning can potentially
provide a way of finding this capacity.
Models: A two-layer fully-connected network (FCN) was
used for our experiments on the Gaussian mixture classifica-
tion tasks with five replicates for each model (see details in
appendix C). The width of the hidden layer was varied to ob-
tain models of varying sizes and produce the double descent
curve. For MNIST, we used two and three-layer FCNs while
varying the width of the first hidden layer, and ResNet-6
with convolutional filters of different widths. For CIFAR-10,
we used ResNet-18 [He et al., 2016] and varied the width of
the convolutional filter to obtain the double descent curve.
Each experiment was replicated three times for real-world
datasets. The hyperparameters used in these experiments are
described in appendix C. The cross-entropy loss function
was used to train all the models. For CIFAR-10, we also
successively removed one class at a time and then trained
and pruned a ResNet-18 model with a fixed width of the con-
volutional filter. All our models were trained long enough
to ensure that they were not in the epoch-wise double des-

--- PAGE 4 ---
cent regime [Nakkiran et al., 2021]. The OpenLTH library1
was used for our experimental evaluations. The code for
replicating our experiments is available on GitHub2.
Network pruning: IMP with 20% weights removal at
each iteration (lottery ticket rewinding was used when re-
quired, see appendix B) was used for our experiments as it
provides an effective procedure to find subnetworks with
nontrivial sparsities that have low test error [Frankle and
Carbin, 2019, Frankle et al., 2020, Blalock et al., 2020,
Renda et al., 2020]. It should be noted that other pruning
techniques like random pruning and gradient-based prun-
ing also produce the sparse double descent curve [He et al.,
2022] and can be used instead.
Computational costs: Our analysis requires us to work
in the double descent paradigm, which allows us to prop-
erly define a best pruned model. Unfortunately, this implies
significant computational costs as we have to perform the
full sparse double descent analysis. For example, figs. 1,
2 and S2 required us to train approximately 300 different
models and close to 1000 models when including replicates.
Nevertheless, we believe that our extensive investigations
using the mixture classification task and different architec-
tures for MNIST and CIFAR-10 lay a solid foundation for
the phenomenon described in the paper.
Terminology: The results in fig. 1 show that repeated
pruning of different-sized models produces sparse double
descent curves that achieve the lowest test error within a
small range of parameters. To facilitate further discussion on
this phenomenon, we define best pruned model andeffective
number of parameters .
Definition 1 (Best pruned model) LetE(·)denote the test
error (or 0-1 loss) of a model. For a trained model fw, test
dataset Stest, and pruning method ϕ, the best pruned model
fbp
wis defined as
fbp
w= arg min
wE(ϕ(fw,Stest)).
These best pruned models have low generalisation error but
unlike their overparameterized parent models, they have a
non-zero error on the training data.
Definition 2 (Effective number of parameters) The
number of non-zero weights in the best pruned models fbp
w
obtained by pruning an overparameterised model.
By pruning overparameterised models of different sizes we
can obtain best pruned models that have similar accuracy
but a slightly different effective number of parameters.
1https://github.com/facebookresearch/open_lth
2https://github.com/viplovearora/noisy_lottery_
tickets4 RESULTS
We first confirm that fully-connected neural networks ex-
hibit both traditional double descent and sparse double des-
cent on the mixture classification tasks: fig. 2 shows the
same trend in the effective number of parameters as we ob-
served in fig. 1. As expected, the test error decreases with
the distance between cluster means ν(see fig. S1).
Next, we considered the effective number of parameters
in the best pruned models (cf. definition 1). Focusing on
the linear dataset in fig. 2a, we see that the pruned mod-
els (thin red lines) generally achieve the lowest test error
with roughly 500parameters. This number of parameters
is optimal for starting networks of different size, which we
use to determine the effective number of parameters for
that dataset (see definitions 1 and 2). For networks trained
on the linear dataset, we find that the effective number of
parameters, i.e. the number of parameters in the best pruned
models, ranges from ∼200to∼500for different starting
models and for different values of ν(see appendix D for the
full distribution). We find the same behaviour for the XOR
dataset, fig. 2b, but with a higher number of parameters
(between 250and1000 ) than for the linear dataset with the
same ν.
The double descent and sparse double descent curves for
MNIST and CIFAR-10 datasets can be seen in figs. 1 and S2.
Tables 1 and S2 show the number of parameters and test
errors for the full/unpruned models and the corresponding
best pruned models. Notice, importantly, that the best pruned
models do not interpolate, so they exhibit a significant im-
provement in generalisation gap. Focusing on the number
of parameters, we observe that a 200×increase for the full
models results in only a ∼3.5×increase in the number of
parameters for the best pruned models. Comparing the test
errors, a surprising finding is that sparse subnetworks with
low test error exist inside the unpruned models that lie at
the interpolation regime of the double descent curve. This
means that even though the unpruned models have high test
errors, they can be pruned using IMP to achieve much lower
test errors. Pruning thus acts as a type of regularisation in
this case, which is known to mitigate the peak of the test
accuracy at the interpolation threshold [Belkin et al., 2019].
Our findings for CIFAR-10 similarly show that the best
pruned models have better generalisation than the unpruned
counterparts (see table 1). Similar to MNIST, we observe
that the number of parameters in the best pruned models
increases with the size of the original model but at a much
slower rate. Comparing the effective number of parameters
for MNIST and CIFAR-10, one can easily conclude that
more parameters are required for CIFAR-10.
The effective number of parameters correlates with task
difficulty: As illustrated in the data plots of fig. 2, optim-
ally separating the linear dataset requires a plane, whereas

--- PAGE 5 ---
102103104105106
Parameters0.00.10.20.30.40.5Errorν= 0.5
Test
Train
102103104105106
Parameters0.00.10.20.30.40.5Error
ν = 0.8
Test
Train(a) The first two principal components (PCs) of data generated using the linear datasets along with the double descent curves.
102103104105106
Parameters0.00.10.20.30.40.5Errorν= 0.8
Test
Train
102103104105106
Parameters0.00.10.20.30.40.5Errorν= 1
Test
Train
(b) The first two principal components (PCs) of data generated using the XOR datasets along with the double descent curves.
Figure 2: Average over 5 replicates of train and test error for models with different sizes, after numerous iterations of
pruning, on the (a) linear, and (b) XOR datasets as the distance between clusters νis varied. The red bold line with dots
shows the traditional double descent curve, while the thinner lines represent the test error reached after pruning iterations of
the initial models (sparse double descent). Similarly, black lines show train error both for the full and pruned models.
Table 1: Number of parameters and test error for unpruned (full) and best pruned models for MNIST and CIFAR-10. Average
values over 3 replicates are reported. We observe that a 200×increase for the full models results in only a ∼3.5×increase
in the number of parameters for the best pruned models. Notice also that the error achieved by pruned models appears
insensitive to the error rate of the original full model, i.e. even models with poor generalisation can be rescued by pruning.
MNIST (3 layer FC) CIFAR-10 (ResNet-18)
Parameters Test error Parameters Test error
Full Pruned Full Pruned Full Pruned Full Pruned
45 200 6061 0.105 0 .048 128 271 17 211 0.143 0 .141
89 400 4908 0.176 0 .059 238 155 16 359 0.159 0 .143
266 200 5987 0.163 0 .071 238 155 16 359 0.159 0 .143
443 000 6377 0.144 0 .064 1 689 080 19 647 0.187 0 .130
885 000 10 196 0.123 0 .084 3 798 420 35 028 0.165 0 .128
4 421 000 10 683 0.099 0 .089 10 546 700 49 799 0.146 0 .128
8 841 000 17 094 0.091 0 .097 23 725 050 57 356 0.137 0 .129
two planes are needed for separating the classes in the XOR
dataset. Thus, one would expect that the effective number
of parameters would double going from the linear to XOR
datasets. Interestingly, this is what we observe in fig. 3a,
even though the best pruned models are obtained from full
models of the same size. The pruning approach thus sug-gests a way to quantify the effective number of parameters
in deep, over-parameterised neural networks. Can we extend
this approach to measuring task difficulty to more realistic
datasets and convolutional networks? The results in fig. 3b
and table S3 show that for a fixed ResNet model, as we
decrease the number of classes in the CIFAR-10 dataset (or

--- PAGE 6 ---
0.6 0.7 0.8 0.9 1
ν0250500750100012501500Number of effective parameters
Linear XOR(a) Comparing the effective number of parameters in the two
mixture classification datasets for different values of ν.
104105106
Parameters0.000.050.100.150.200.250.30ErrorClasses
5
6
78
9
10(b) Sparse double descent curves on subsets of CIFAR-10
dataset show that effective number of parameters is related to
task difficulty.
Figure 3: Our analysis shows that the effective number of parameters correlates with task difficulty: (a) the effective number
of parameters required for the XOR dataset is approximately twice that of the linear dataset, which reflects the higher
complexity of the XOR task, and (b) decreasing the number of classes in the CIFAR-10 dataset allows IMP to find models
with fewer effective number of parameters.
2-layer 3-layer ResNet0500010000150002000025000300003500040000
(a) Effective number of parameters for
different models trained on the MNIST
dataset.
104105106107
Parameters0.100.150.200.250.300.350.40ErrorDenseNet
VGG
ResNet(b) Sparse double descent curves obtained on pruning different
convolutional neural networks for CIFAR-10 dataset.
Figure 4: Comparing the effective number of parameters across different neural network architecture for MNIST and
CIFAR-10 datasets.
equivalently make the task easier for an overparameterised
model), the effective number of parameters reduces for a
smaller classification task. This further affirms our obser-
vation on the mixture classification task that the effective
number of parameters is related to the difficulty of the task.
Interestingly, the effective number of parameters does not
decrease linearly with the number of classes.
The effective number of parameters is comparable
across architectures: How does the choice of neural net-
work architecture impact the effective number of parameters
for a given dataset? We compare the size of best pruned
models obtained for the MNIST dataset using three differ-
ent neural network architectures: two-layer FCN, three-layer
FCN, and ResNet-6, in fig. 4a. We find that although the testerror of the best pruned models for the different architectures
is different (see tables 1 and S2), the effective number of
parameters are fairly comparable. This provides additional
empirical evidence that our procedure might be invariant
to the neural network architecture and reflects a notion of
the ‘difficulty’ of the classification task, something that we
already noted in the mixture classification task. This obser-
vation also carries forward to different convolutional neural
network architectures on CIFAR-10. Figure 4b shows the
sparse double descent curves obtained for DenseNet, VGG,
and ResNet. The effective number of parameters is similar
across the three different architectures: 44 590 for DenseNet,
44 468 for VGG, and 39 483 for ResNet.

--- PAGE 7 ---
(a) Comparing the decision boundaries learned by the best
pruned and full models we find that best pruned models are
more aligned with the optimal classifier.
(b) Calibration curves on the linear (left) and XOR (right) datasets show that
best pruned models and small full models are better calibrated, whereas the
full models are overconfident and poorly calibrated.
20
 0 2020
10
01020Best pruned
20
 0 2020
10
01020Full
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.000.060.120.180.240.300.360.420.48
0.0000.0750.1500.2250.3000.3750.4500.5250.6000.675
20
 0 20 4020
10
01020304050Best pruned
20
 0 20 4020
10
01020304050Full
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
0.00.10.20.30.40.50.60.70.8
0.000.120.240.360.480.600.720.840.96
(c) Visualising the absolute difference between the predicted class probabilities and the optimal probabilities for the linear (left) and XOR
(right) datasets projected on a 2D plane. Lower values (darker blue colours) are preferred.
Figure 5: Analysis of prediction uncertainty in the mixture classification task. We find that the best pruned models are better
at capturing uncertainty, whereas the overparameterised models tend to be overconfident in their predictions.
4.1 PRUNED MODELS BETTER CAPTURE
UNCERTAINTY
We first use the mixture classification task to understand the
architectural bias induced by pruning. For the results presen-
ted in fig. 5, we evaluate the models with 500neurons in the
hidden layer for the linear ( ν= 0.2) and XOR ( ν= 0.6)
datasets. Similar results are obtained for different starting
models and other values of ν. To understand what differen-
tiates the pruned models we analyse the decision boundary
learned by these models. We sample 100 000 points from
the original data distribution and compute the corresponding
probabilities for the models of interest along with the condi-
tional class probabilities. In fig. 5a, we plot the probability
of data belonging to class y= 0as a function of the relative
distance between the centres of the two clusters (located at
x= 0 andx= 1) for the linear dataset. As expected, the
optimal probabilities are given by a logistic function shown
using the black dots. Comparing the best pruned and full
models, we can conclude that the function learnt by the best
pruned model is much closer to the true class conditional
probability used to generate the data, whereas the full over-
parameterised model is overconfident in predicting either
class.
In fig. 5c, we visualise the absolute difference between the
predicted probabilities and the conditional class probabilit-
ies relative to the position of each point projected on a 2Dplane. A value close to 0on the colour scale in fig. 5c im-
plies that the model has correctly estimated the conditional
probability. One would expect that a model would confid-
ently predict the correct class farther from the boundary
and make errors near the boundary. This intuition is clearly
illustrated using the results for the linear dataset. Strikingly,
the full model makes confident predictions near the bound-
ary, thus resulting in a value of ≈0.45shown using the
red colour in fig. 5c. In the XOR dataset, the best pruned
model is more uncertain in its predictions, especially near
the boundary, but is better than the full model. This is further
illustrated using the calibration curves in fig. 5b where we
see a near-perfect calibration in the best pruned models for
both datasets, whereas the full models are overconfident and
poorly calibrated. We also observe that the calibration of
the best small full models is comparable to the best pruned
models.
The calibration curves for MNIST and CIFAR-10 in fig. 6
show that the curves for the best pruned models are usually
above the black line, which means that they make the cor-
rect prediction while being uncertain. Figure 6c illustrates
this using the probabilities corresponding to the predicted
class. This implies that even though the pruned models are
accurate, they tend to be underconfident in their predictions
while the full models are overconfident. This is at odds
with our observation in the mixture classification task. On
further analysis, we found that the best pruned models are

--- PAGE 8 ---
0.0 0.2 0.4 0.6 0.8 1.0
Average probability0.00.20.40.60.81.0Fraction of positivebest pruned
full(a) MNIST with two-layer FCN. ECE for pruned and
full models are 0.114and0.052, respectively.
0.0 0.2 0.4 0.6 0.8 1.0
Average probability0.00.20.40.60.81.0Fraction of positivebest pruned
full(b) CIFAR-10 with ResNet-18. ECE for pruned and
full models are 0.122and0.101, respectively.
0.2 0.4 0.6 0.8 1.00200040006000best pruned
full
0.2 0.4 0.6 0.8 1.002000400060008000
best pruned
full(c) Probabilities of the pre-
dicted class.
Figure 6: Class-averaged calibration curves for the best pruned and full models on (a) MNIST and (b) CIFAR-10 datasets
show that the pruned models are underconfident while the full models are overconfident. The highlighted areas signify
deviation between classes. (c) Probabilities of the predicted class for the MNIST (top) and CIFAR-10 (bottom) datasets.
104105106
Parameters0.120.140.16Error
0.10.20.3
ECEtrue
noisy
104105106
Parameters0.140.160.180.20Error
0.10.2
ECEtrue
noisy
Figure 7: Sparse double descent (red) and expected calibration error (blue) on pruning two-layer FCN on MNIST (left) and
ResNet-18 for CIFAR-10 (right). Highlighted area shows the deviation across three replicates. Comparing the calibration
error for true and noisy labels we find that the best pruned models are optimally calibrated to noisy data.
calibrated to test data with label noise (see fig. S4). This
is a reasonable outcome since the model was trained with
label noise, which could introduce significant bias in the
classification learned by the model.
To understand how iterative pruning impacts calibration,
in fig. 7 we plot the test error and expected calibration er-
ror [Guo et al., 2017] (ECE) for a fixed model. For both
noisy and true labels, we observe that the ECE initially
increases slightly on iterative pruning but then drops signi-
ficantly. The major difference between the two scenarios is
the location of the minimum. For true labels, the lowest ECE
is observed for models that have high test error, whereas the
best pruned models are better calibrated to data with noisy
labels. This strange behaviour requires further investigation
to understand the impacts of pruning on model calibration.
Perhaps, a trade-off between test error and calibration error
can be used to choose models that have low error and are
well-calibrated.5 CONCLUSIONS
The existence and identifiability of small pruned net-
works, which can still generalise as well as large,
over-parameterised models, remains an empirically well-
supported fact that still lacks a satisfactory theoretical ex-
planation. In this paper, we provided a number of empirical
observations which unveiled two intriguing characteristics
of small pruned networks: (1) their number of parameters
correlate with task difficulty and provide a measure of the
effective number of parameters in large models, and (2)
pruned models are less prone to overconfident predictions.
Working in the IMP framework, we consistently reproduce
the sparse double descent phenomenon first observed by
He et al. [2022] in a number of synthetic and real datasets.
While He et al. [2022] focused on the sparse double descent
of the large model, we observe that irrespective of the size
(and test accuracy) of the model from which we start the
sparse double descent, the resulting optimal pruned models
all achieve a similar generalisation error and have compar-

--- PAGE 9 ---
able sizes. The size of the best pruned models (defined by
the number of retained parameters) appears to correlate well
with natural notions of task complexity, both in real and
simulated datasets.
We then analysed more in-depth the functions learned by
these small networks: on simulated data, where we know the
actual class conditional probability functions, we observe
that pruned models capture much better the true underlying
function, whereas full models tend to be overconfident. On
real data, we again verify that full models are more confident
than pruned models, which in this case tend to be slightly
under-confident. A possible explanation is that, as customary
in double descent studies, our models are trained on data
with label noise, which impacts their calibration on data
with true labels. Indeed, we find that pruned models are
nearly optimally calibrated to data with label noise, while
full models are once again over-confident.
Our results are somewhat at odds with a recent claim
by Yang et al. [2023], which showed evidence of over-
confidence of pruned models in one example (ResNet-50
on CIFAR-100). A possible reason for this discrepancy is
that the pruning algorithm employed in [Yang et al., 2023]
is greedy and does not require re-training from initialisa-
tion. Additionally, their setup did not involve training on
noisy labels; both of these observations might explain the
different conclusions reached by that study. Indeed, these
considerations point to a non-trivial interaction between
pruning algorithm, training procedure and statistical char-
acteristics of the resulting pruned models. Exploring such
questions, both theoretically and empirically, might finally
shed light on the unreasonable success of pruning large
neural networks.
In the future, it would be interesting to see how other prun-
ing procedures impact our findings and extend our study
to larger datasets like ImageNet. It would be interesting
to see if these findings apply to other architectures like
auto-encoders and recurrent neural networks. Finally, our
findings highlight the importance of the sparse double des-
cent thus encouraging theoretical work to understand the
phenomenon.
Author Contributions
Viplove Arora, Sebastian Goldt, and Guido Sanguinetti con-
ceived the idea and wrote the paper. Daniele Irto performed
the experiments for the mixture classification task. Viplove
Arora performed the rest of the experimental analysis.
Acknowledgements
We acknowledge co-funding from Next Generation EU, in
the context of the National Recovery and Resilience Plan,
Investment PE1 – Project FAIR “Future Artificial Intelli-gence Research”. This resource was co-financed by the
Next Generation EU [DM 1555 del 11.10.22]. The views
and opinions expressed are only those of the authors and
do not necessarily reflect those of the European Union or
the European Commission. Neither the European Union
nor the European Commission can be held responsible for
them. Viplove Arora acknowledges partial support from
Assicurazioni Generali through a liberal grant.
References
Madhu S. Advani, Andrew M. Saxe, and Haim Sompolinsky.
High-dimensional dynamics of generalization error in
neural networks. Neural Networks , 132:428 – 446, 2020.
Zachary Ankner, Alex Renda, Gintare Karolina Dziugaite,
Jonathan Frankle, and Tian Jin. The effect of data dimen-
sionality on neural network prunability. arXiv preprint
arXiv:2212.00291 , 2022.
Devansh Arpit, Stanisław Jastrzebski, Nicolas Ballas, David
Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan
Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio,
et al. A closer look at memorization in deep networks.
InInternational conference on machine learning , pages
233–242. PMLR, 2017.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Man-
dal. Reconciling modern machine-learning practice and
the classical bias–variance trade-off. Proceedings of the
National Academy of Sciences , 116(32):15849–15854,
2019.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of
double descent for weak features. SIAM Journal on Math-
ematics of Data Science , 2(4):1167–1180, 2020.
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle,
and John Guttag. What is the state of neural network
pruning? Proceedings of machine learning and systems ,
2:129–146, 2020.
Leo Breiman. Reflections after refereeing papers for nips.
The Mathematics of Generalization , pages 11–15, 1995.
Rebekka Burkholz. Most activation functions can win the
lottery without excessive depth. In Thirty-sixth Confer-
ence on Neural Information Processing Systems , 2022.
Arthur da Cunha, Emanuele Natale, and Laurent Viennot.
Proving the strong lottery ticket hypothesis for convolu-
tional neural networks. In ICLR 2022-10th International
Conference on Learning Representations , 2022.
S. d’Ascoli, M. Refinetti, G. Biroli, and F. Krzakala. Double
trouble in double descent : Bias and variance(s) in the
lazy regime. In ICML , 2020a.

--- PAGE 10 ---
Stéphane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple
descent and the two kinds of overfitting: where &amp;
why do they appear? In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances
in Neural Information Processing Systems , volume 33,
pages 3058–3069. Curran Associates, Inc., 2020b.
Jonathan Frankle and Michael Carbin. The lottery ticket
hypothesis: Finding sparse, trainable neural networks. In
International Conference on Learning Representations ,
2019.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M
Roy, and Michael Carbin. Stabilizing the lottery ticket
hypothesis. arXiv preprint arXiv:1903.01611 , 2019.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy,
and Michael Carbin. Linear mode connectivity and the
lottery ticket hypothesis. In International Conference on
Machine Learning , pages 3259–3269. PMLR, 2020.
Trevor Gale, Erich Elsen, and Sara Hooker. The state
of sparsity in deep neural networks. arXiv preprint
arXiv:1902.09574 , 2019.
Stuart Geman, Elie Bienenstock, and René Doursat. Neural
networks and the bias/variance dilemma. Neural compu-
tation , 4(1):1–58, 1992.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural networks. In Interna-
tional conference on machine learning , pages 1321–1330.
PMLR, 2017.
Song Han, Jeff Pool, John Tran, and William Dally. Learn-
ing both weights and connections for efficient neural net-
work. Advances in neural information processing systems ,
28, 2015.
Babak Hassibi and David Stork. Second order derivatives
for network pruning: Optimal brain surgeon. Advances
in neural information processing systems , 5, 1992.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and
Ryan J Tibshirani. Surprises in high-dimensional ridge-
less least squares interpolation. The Annals of Statistics ,
50(2):949–986, 2022.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition , pages 770–778, 2016.
Zheng He, Zeke Xie, Quanzhi Zhu, and Zengchang Qin.
Sparse double descent: Where network pruning aggrav-
ates overfitting. In International Conference on Machine
Learning , pages 8635–8659. PMLR, 2022.Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden,
and Alexandra Peste. Sparsity in deep learning: Pruning
and growth for efficient inference and training in neural
networks. J. Mach. Learn. Res. , 22(241):1–124, 2021.
Tian Jin, Michael Carbin, Daniel M. Roy, Jonathan Frankle,
and Gintare Karolina Dziugaite. Pruning’s effect on gen-
eralization through the lens of training and regularization.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information
Processing Systems , 2022.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009.
Lorenz Kuhn, Clare Lyle, Aidan N Gomez, Jonas Roth-
fuss, and Yarin Gal. Robustness to pruning predicts
generalization in deep neural networks. arXiv preprint
arXiv:2103.06002 , 2021.
Yann LeCun, John Denker, and Sara Solla. Optimal brain
damage. Advances in neural information processing sys-
tems, 2, 1989.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278–2324,
1998.
Bowen Lei, Ruqi Zhang, Dongkuan Xu, and Bani Mallick.
Calibrating the rigged lottery: Making all tickets reliable.
InInternational Conference on Learning Representations ,
2023.
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason
Yosinski. Measuring the intrinsic dimension of objective
landscapes. In International Conference on Learning
Representations , 2018.
Licong Lin and Edgar Dobriban. What causes the test error?
going beyond bias-variance via anova. The Journal of
Machine Learning Research , 22(1):6925–7006, 2021.
Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe,
and David MJ Tax. A brief prehistory of double descent.
Proceedings of the National Academy of Sciences , 117
(20):10625–10626, 2020.
Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and
Ohad Shamir. Proving the lottery ticket hypothesis: Prun-
ing is all you need. In International Conference on Ma-
chine Learning , pages 6682–6691. PMLR, 2020.
Song Mei and Andrea Montanari. The generalization error
of random features regression: Precise asymptotics and
the double descent curve. Communications on Pure and
Applied Mathematics , 75(4):667–766, 2022.

--- PAGE 11 ---
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan
Yang, Boaz Barak, and Ilya Sutskever. Deep double des-
cent: Where bigger models and more data hurt. Journal
of Statistical Mechanics: Theory and Experiment , 2021
(12):124003, 2021.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli,
Yann LeCun, and Nathan Srebro. Towards understand-
ing the role of over-parametrization in generalization of
neural networks. In International Conference on Learn-
ing Representations (ICLR) , 2019.
Curtis G Northcutt, Anish Athalye, and Jonas Mueller. Per-
vasive label errors in test sets destabilize machine learning
benchmarks. In Thirty-fifth Conference on Neural In-
formation Processing Systems Datasets and Benchmarks
Track (Round 1) , 2021.
M Opper, W Kinzel, J Kleinz, and R Nehl. On the ability of
the optimal perceptron to generalise. Journal of Physics
A: Mathematical and General , 23(11):L581, 1990.
Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Log-
arithmic pruning is all you need. Advances in Neural
Information Processing Systems , 33:2925–2934, 2020.
Michela Paganini and Jessica Forde. On iterative neural
network pruning, reinitialization, and the similarity of
masks. arXiv preprint arXiv:2001.05050 , 2020.
Mansheej Paul, Feng Chen, Brett W Larsen, Jonathan
Frankle, Surya Ganguli, and Gintare Karolina Dziu-
gaite. Unmasking the lottery ticket hypothesis: What’s
encoded in a winning ticket’s mask? arXiv preprint
arXiv:2210.03044 , 2022.
Franco Pellegrini and Giulio Biroli. Sifting out the fea-
tures by pruning: Are convolutional networks the win-
ning lottery ticket of fully connected ones? arXiv preprint
arXiv:2104.13343 , 2021.
Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit
Vishwakarma, and Dimitris Papailiopoulos. Op-
timal lottery tickets via subset sum: Logarithmic over-
parameterization is sufficient. Advances in neural inform-
ation processing systems , 33:2599–2610, 2020.
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum,
and Tom Goldstein. The intrinsic dimension of images
and its impact on learning. In International Conference
on Learning Representations , 2021.
RJ Poppi and DL Massart. The optimal brain surgeon for
pruning neural network architecture applied to multivari-
ate calibration. Analytica chimica acta , 375(1-2):187–
195, 1998.
Maria Refinetti, Sebastian Goldt, Florent Krzakala, and
Lenka Zdeborová. Classifying high-dimensional gaussianmixtures: Where kernel methods fail and neural networks
succeed. In International Conference on Machine Learn-
ing, pages 8936–8947. PMLR, 2021.
Alex Renda, Jonathan Frankle, and Michael Carbin. Com-
paring rewinding and fine-tuning in neural network prun-
ing. In International Conference on Learning Represent-
ations , 2020.
Keitaro Sakamoto and Issei Sato. Analyzing lottery ticket
hypothesis from PAC-bayesian theory perspective. In
Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information
Processing Systems , 2022.
Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali
Tishby. Statistical mechanics of learning from examples.
Physical review A , 45(8):6056, 1992.
S Spigler, M Geiger, S d’Ascoli, L Sagun, G Biroli, and
M Wyart. A jamming transition from under-to over-
parametrization affects generalization in deep learning.
Journal of Physics A: Mathematical and Theoretical , 52
(47):474001, 2019.
F Vallet, J-G Cailton, and Ph Refregier. Linear and nonlin-
ear extension of the pseudo-inverse solution for learning
boolean functions. EPL (Europhysics Letters) , 9(4):315,
1989.
Bindya Venkatesh, Jayaraman J Thiagarajan, Kowshik
Thopalli, and Prasanna Sattigeri. Calibrate and prune:
Improving reliability of lottery tickets through prediction
calibration. arXiv preprint arXiv:2002.03875 , 2020.
Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 ,
2017.
Hongru Yang, Yingbin Liang, Xiaojie Guo, Lingfei Wu, and
Zhangyang Wang. Theoretical characterization of how
neural network pruning affects its generalization. arXiv
preprint arXiv:2301.00335 , 2023.
Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and
Jinjun Xiong. Why lottery ticket wins? a theoretical per-
spective of sample complexity on sparse neural networks.
Advances in Neural Information Processing Systems , 34:
2707–2720, 2021.

--- PAGE 12 ---
A MIXTURE CLASSIFICATION DATASETS
A common technique in the study of neural network theory is to operate in a controlled setting, where models and data
are simplified to allow analytical computations for otherwise intractable objects. An example of such an approach is the
teacher-student framework [Seung et al., 1992], where the dataset is created by a neural network built for that purpose
(teacher) and another neural network is tasked with learning on that data (student). For our analysis, we create two types
of balanced binary classification datasets using Gaussian mixtures with different characteristics and varying levels of
difficulty. Using the mixture model allowed us to create datasets that are easy to understand and can be easily visualised in a
two-dimensional space.
We consider two settings for mixture classification. The linear dataset consists of two clusters that can be separated using a
linear function. The two clusters have different means µ1̸=µ2but same covariance Σ1= Σ 2. The XOR dataset was created
such that the resulting clusters are placed like the graphic representation of the XOR logical function. We assume that the
difficulty of the classification task would increase going from linear to XOR dataset. Data was sampled using a Gaussian
mixture model conditioned on a predefined label y. We also define a coefficient of separation νto modulate the distance
between clusters.
To properly define the formulation of our mixture models, we use an approach similar to that used by Refinetti et al. [2021].
The data distribution for a single input sample x∈RDgiven class ysampled uniformly at random is:
p(x, y) =p(y)p(x|y), p(x|y) =X
α∈ST(y)PαN(µα,Σα). (S1)
where p(x|y)is the probability of sampling xconditioned on the class y∈ {0,1}of the sample. Each xis sampled
using a multivariate normal distribution. ST(y)is the set of all possible indexes for class yand dataset type T.Pαis the
probability of the α-thD-dimensional multivariate normal distribution N(µα,Σα), which depends on the size of ST(y).
This formulation can easily be used to generate data with a generic number of clusters, classes, and dataset types. In
particular, more classes and dataset types can be included by defining proper, additional sets of indexes ST(y). For our
experiments, we only considered two classes.
A.1 DATA GENERATION PROCESS
Labels: The first step in generating the data was to create a vector yof classes 0or1of size equal to the desired number
of training samples N. This was done by sampling Nvalues from a uniform distribution U(0,1)and reassigning them to
values 0or1depending on whether they were ≤0.5or>0.5, respectively. The resulting vector yis an array of values 0or
1in balanced proportions and it corresponds to the labels of the training samples in our dataset.
Linear dataset: For each observation xi,i= 1, . . . , N , of the linear dataset, the set of indexes SL(y)has only one
element for each class. This means that, for each class, the input points can be sampled by one multivariate normal:
SL(y= 0) = {α0} → µα0= 0· 1D(S2a)
SL(y= 1) = {α1} → µα1=ν· 1D(S2b)
Σα0=Σα1=ID. (S2c)
1Dis aD-dimensional vector of all ones that can be multiplied by a scalar, meaning that all its elements get multiplied by
that scalar. IDis the D×Didentity matrix, and its elements can be multiplied by a scalar number as well3. The distance
between the two clusters, which makes the two classes more or less discernible, can be changed by simply increasing
or decreasing the value of the νcoefficient. Performing PCA on the linear dataset and plotting the first two principal
components yields the clusters shown in fig. 2a. In those plots, it is possible to observe a clear distinction between the
clusters belonging to the two classes. We can also see the change in distance between the clusters as νis changed.
3This notation is used consistently in this section, to indicate the means and covariances of the normal distributions.

--- PAGE 13 ---
XOR dataset: For the second dataset, our goal was to make the data appear as four separate clusters placed like the visual
representation of the XOR logical operator. In this case, the sets of indexes corresponding to class y= 0andy= 1that
have two elements each. Thus, the data points of each class can be sampled from two different distributions with equal
probabilities. For class y= 0, the samples are generated like the linear dataset described in eq. (S2). For the other class, its
main feature is that the mean vectors of the multivariate normal distributions consist of twoD
2-dimensional halves with
different values:
SX(y= 0) = {αA
0, αB
0} →(
µαA
0= 0· 1D
µαB
0=ν· 1D(S3a)
SX(y= 1) = {αA
1, αB
1} →(
µαA
1= [0· 1D
2, ν· 1D
2]
µαB
0= [ν· 1D
2,0· 1D
2](S3b)
ΣαA
0=ΣαB
0=ΣαA
1=ΣαB
1=ID. (S3c)
The PC plots of this dataset are shown in fig. 2b, where we can see the positioning of the four clusters in a cross-like layout.
B PRUNING AND REWINDING
In our experiments, training is always followed by a series of pruning iterations that were performed according to the IMP
[Han et al., 2015, Frankle and Carbin, 2019] technique, which is described below:
1. Initialise the model with weight W0, obtaining a model function f(x;W0).
2. Train the model for the desired number of epochs j, reaching a set of weights Wj.
3. Sort all the weights in Wjby their absolute value and select the lowest p%, where pis a number between 0 and 100.
4. Prune the selected weights by applying a mask to the original model, obtaining f(x;mask⊙ W j).
5. Reset the remaining parameters to their first initialisation values, obtaining f(x;mask⊙ W 0).
6. Iterate the process ntimes, pruning p%of the remaining connections at each iteration.
In certain cases, the larger models need to be pruned using the lottery ticket rewinding technique [Frankle et al., 2019].
Rewinding simply consists of modifying only one step of the procedure described above:
5 Reset the remaining parameters to the values at iteration k≪jof the training loop, obtaining f(x;mask⊙ W k)
where kis a hyperparameter representing the number of the rewinding iterations. This technique is also conveniently
included in the OpenLTH4library.
C MODEL HYPERPARAMETERS
C.1 MIXTURE CLASSIFICATION
We used two-layer fully connected networks without bias for our experiments on the two mixture classification datasets.
Models with P∈ {1,3,5,8,10,15,20,25,50,100,200,500,1000,10000}neurons in the hidden layer were used to
produce the double descent curve. These models were subsequently pruned using IMP to produce the sparse double descent
curves. Network weights are initialised using the Kaiming uniform distribution. The activation function chosen for this
model is the Rectified Linear Unit (ReLU). Stochastic gradient descent with a learning rate of 0.1 was used as the optimiser.
All models were trained for 1000 epochs using a batch size of 1024 . All models were sufficiently pruned to observe the
sparse double descent curves. All experiments were replicated five times.
Linear datasets: We varied the distance between cluster means ν∈ {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0}to see
how it impacts the test error and the effective number of parameters. 5% random label noise in the training set was used to
observe the two double descent curves.
4https://github.com/facebookresearch/open_lth

--- PAGE 14 ---
XOR datasets: We varied the distance between cluster means ν∈ {0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3}to see how it
impacts the test error and the effective number of parameters. Higher values of νwere needed to ensure sufficient distance
between the clusters. Higher label noise of 25% was needed to consistently observe the two double descent curves in the
XOR datasets.
C.2 MNIST
The three-layer fully connected architecture used for MNIST is an extension of the standard model used for MNIST
in the pruning literature. We varied the number of neurons P∈ {3,5,10,25,50,100,300,500,1000,5000,10000}
in the first hidden layer. The size of the second hidden layer was kept fixed at 100. For the two-layer FCN,
P∈ {5,10,50,100,300,500,1000,2000,5000,10000}neurons were used in the hidden layer. For the fully connec-
ted networks, lottery ticket rewinding was used for networks with P≥1000 . For ResNet-6, we varied the width
W∈ {1,2,5,8,11,15,20,40,80,120}of the convolutional filters to obtain networks of different sizes. Further details can
be found in table S1.
C.3 FASHION-MNIST
We also performed a small set of experiments on the Fashion-MNIST dataset. We only performed a limited number of
experiments focused on finding the effective number of parameters using overparameterised models. Since reproducing
the double descent curve was not the target, we only used three-layer FCNs with P∈ {500,1000}neurons in the first
hidden layer. The size of the second hidden layer was kept fixed at 100. Note that more epochs (320) are needed to train a
three-layer FCN on Fashion-MNIST.
C.4 CIFAR-10
We primarily considered ResNet-18 for CIFAR-10, where the width W∈ {2,5,8,11,15,20,40,60,80,100,120,150}of
the convolutional filters was varied to obtain networks of different sizes. Based on previous observations Frankle et al.
[2019], He et al. [2022], we used 10 epochs of rewinding to consistently find lottery tickets in ResNet-18. To compare
the effective number of parameters across different CNN architectures, we performed our analysis on DenseNet-121 and
VGG-16. Further details can be found in table S1.
Table S1: Neural network architectures used on real data. LR refers to the learning rate.
Network Dataset Epochs Batch size Optimiser Momentum LR Rewind Iter
Two-layer FCN MNIST 120 128 SGD - 0.1 -
Three-layer FCN MNIST 120 128 SGD - 0.1 -
ResNet-6 MNIST 120 128 SGD 0.9 0.1 -
Three-layer FCN Fashion MNIST 320 128 SGD - 0.1 -
ResNet-18 CIFAR-10 160 128 SGD 0.9 0.1 10 epochs
VGG-16 CIFAR-10 160 128 SGD 0.9 0.1 10 epochs
DenseNet-121 CIFAR-10 160 128 SGD 0.9 0.1 10 epochs
D ADDITIONAL RESULTS
Figure S1 shows how the effective number of parameters and test error of the best pruned models vary with νfor the linear
and XOR datasets. As expected, the test error decreases with the distance between cluster means ν. For networks trained
on the linear dataset, we find that the effective number of parameters, i.e. the number of parameters in the best pruned
models, ranges from ∼200to∼500for different starting models and for different values of ν(see appendix D for the full
distribution). We find the same behaviour for the XOR dataset, fig. 2b, but with a higher number of parameters (between 250
and1000 ) than for the linear dataset with the same ν.
Double descent and sparse double descent curves obtained for MNIST on two-layer FCN and ResNet-6 can be seen in
fig. S2. Table S2 shows the number of parameters and test errors for the full/unpruned models and the corresponding best

--- PAGE 15 ---
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
ν025050075010001250Number of effective parameters
0.35 0.21
0.11 0.07
0.040.02
0.02 0.010.01 0.0Linear
0.7 0.8 0.9 1 1.1 1.2 1.3
ν025050075010001250Number of effective parameters
0.25
0.210.18
0.160.16
0.14
0.12XOR(a) XOR datasets.
Figure S1: Distribution of the effective number of parameters of the best pruned models (y-axis) as the distance between the
clusters ν(x-axis) is varied for the linear and xor datasets. Only pruned models originating from overparameterised full
models are considered. The numbers above the boxes report the test error of the model with median effective number of
parameters for each ν.
pruned models. Sparse double descent curves for two different models on Fashion-MNIST in fig. S3 show that, compared to
MNIST, the test error for the best pruned models is higher while the effective number of parameters is approximately 10 000 .
103104105106107
Parameters0.00.10.20.30.40.50.6Error
Test
Train
103104105106
Parameters0.00.20.40.60.8Error
Test
Train
Figure S2: Pruning models along the double descent curve (dark red) show that sparse double descent curves (light red)
from different models coincide at the minima. Results are shown for two-layer FCNs (left) and ResNet-6 (right) on MNIST
with 20% label noise averaged over three replicates.
Finally, the calibration curves when label noise is added to the test set for MNIST and CIFAR-10 can be seen in fig. S4. The
class-averaged calibration curves show that the pruned models are well-calibrated to noisy data while the full models are
overconfident.

--- PAGE 16 ---
104105106
Parameters0.00.10.20.30.4Error
Test
TrainFigure S3: Estimating the effective number of parameters for Fashion-MNIST: Results are shown for three-layer FCNs on
Fashion-MNIST with 20% label noise averaged over three replicates. Compared to MNIST, the test error for the best pruned
models is higher while the effective number of parameters is approximately 10 000 .
Table S2: Number of parameters and test error for unpruned and best pruned models for two architectures trained on MNIST:
two-layer FCNs, and ResNet-6. Average values over 3 replicates are reported. We observe that a 200×increase for the
full models results in only a ∼3.5×increase in the number of parameters for the best pruned models. Notice also that the
error achieved by pruned models appears insensitive to the error rate of the original full model, i.e. even models with poor
generalisation can be rescued by pruning.
2 layer FC ResNet-6
Parameters Test error Parameters Test error
Full Pruned Full Pruned Full Pruned Full Pruned
39 700 8320 0.081 0 .053 19 464 5098 0.028 0 .012
79 400 6814 0.131 0 .077 36 597 7669 0.069 0 .012
238 200 5358 0.194 0 .092 67 785 7272 0.118 0 .014
794 000 11 436 0.122 0 .106 120 180 8252 0.116 0 .013
1 588 000 11 710 0.094 0 .095 478 760 13 470 0.080 0 .013
3 970 000 23 428 0.076 0 .103 1 911 120 17 620 0.061 0 .014
7 940 000 29 990 0.067 0 .097 4 297 080 20 286 0.057 0 .013
Table S3: Effective number of parameters and test error of best pruned models on subsets of CIFAR-10 dataset.
Classes Params Error
5 16 312 0 .102
6 20 392 0 .118
7 31 869 0 .122
8 31 871 0 .125
9 31 872 0 .126
10 39 843 0 .124

--- PAGE 17 ---
0.0 0.2 0.4 0.6 0.8 1.0
Average probability0.00.20.40.60.81.0Fraction of positivebest pruned
full(a) MNIST with two-layer FCN. ECE for pruned and full models
are0.075and0.207, respectively.
0.0 0.2 0.4 0.6 0.8 1.0
Average probability0.00.20.40.60.81.0Fraction of positivebest pruned
full(b) CIFAR-10 with ResNet-18. ECE for pruned and full models are
0.053and0.254, respectively.
Figure S4: Class-averaged calibration curves for the best pruned and full models on (a) MNIST and (b) CIFAR10 datasets
with noise added to the labels in the test set show that the pruned models are well-calibrated to noisy data while the full
models are overconfident. The highlighted areas signify deviation between classes.
