# 2306.12190.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/uncertainty/2306.12190.pdf
# Kích thước tệp: 1418397 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Định lượng vé số dưới nhiễu nhãn:
độ chính xác, hiệu chuẩn và độ phức tạp
Viplove Arora1Daniele Irto2Sebastian Goldt1Guido Sanguinetti1
1Khoa học Dữ liệu Lý thuyết và Khoa học, SISSA, Trieste, Italy
2Khoa học Dữ liệu và Tính toán Khoa học, Đại học Trieste, Trieste, Italy
Tóm tắt
Cắt tỉa mạng nơ-ron sâu là một chiến lược được sử dụng rộng rãi để giảm bớt gánh nặng tính toán trong học máy. Bằng chứng thực nghiệm áp đảo cho thấy rằng các mô hình được cắt tỉa vẫn giữ được độ chính xác rất cao ngay cả với một phần nhỏ các tham số. Tuy nhiên, tương đối ít công trình đã được thực hiện để đặc trưng hóa các mạng nhỏ được cắt tỉa thu được, ngoài việc đo lường độ chính xác của chúng. Trong bài báo này, chúng tôi sử dụng phương pháp giảm kép thưa thớt để xác định rõ ràng và đặc trưng hóa các mô hình được cắt tỉa liên quan đến các tác vụ phân loại. Chúng tôi quan sát thực nghiệm rằng, đối với một tác vụ nhất định, cắt tỉa cường độ lặp (IMP) có xu hướng hội tụ đến các mạng có kích thước tương đương ngay cả khi bắt đầu từ các mạng đầy đủ với kích thước thay đổi theo bậc độ lớn. Chúng tôi phân tích các mô hình được cắt tỉa tốt nhất trong một thiết lập thực nghiệm có kiểm soát và cho thấy rằng số lượng tham số của chúng phản ánh độ khó của tác vụ và chúng tốt hơn nhiều so với các mạng đầy đủ trong việc nắm bắt phân phối xác suất có điều kiện thực của các nhãn. Trên dữ liệu thực, chúng tôi tương tự quan sát rằng các mô hình được cắt tỉa ít có xu hướng đưa ra dự đoán quá tự tin hơn. Kết quả của chúng tôi cho thấy rằng các mô hình được cắt tỉa thu được thông qua IMP không chỉ có các tính chất tính toán thuận lợi mà còn cung cấp một biểu diễn tốt hơn về tính không chắc chắn trong học tập.

1 GIỚI THIỆU
Trí tuệ thống kê thông thường cho rằng việc tăng kích thước của một mô hình dẫn đến cải thiện ban đầu trong hiệu suất tổng quát hóa, tiếp theo là hiện tượng quá khớp nghiêm trọng và suy giảm độ chính xác trong các mô hình có quá nhiều tham số. Trên thực tế, sự điều chỉnh ngầm của các mô hình lớn dẫn đến một hiện tượng giảm kép mà, dưới những điều kiện thích hợp, dẫn đến các mô hình có quá nhiều tham số với hiệu suất tổng quát hóa thậm chí còn mạnh hơn [Vallet et al., 1989, Opper et al., 1990, Geman et al., 1992, Belkin et al., 2019, Nakkiran et al., 2021, Loog et al., 2020]. Hiện tượng này được hiểu rõ về mặt lý thuyết trong các trường hợp đơn giản và đã được tái tạo trong thực tế trong nhiều kiến trúc mạng nơ-ron sâu hiện đại [Belkin et al., 2019, Nakkiran et al., 2021, Arpit et al., 2017, Neyshabur et al., 2019, Belkin et al., 2020].

Tuy nhiên, hàng thập kỷ nghiên cứu về cắt tỉa mạng nơ-ron [LeCun et al., 1989, Hassibi and Stork, 1992, Gale et al., 2019, Hoefler et al., 2021] cũng đã cho thấy rằng một số lượng lớn tham số (trọng số) trong các mô hình có quá nhiều tham số này có thể được loại bỏ mà không ảnh hưởng đến lỗi tổng quát hóa. He et al. [2022] gần đây đã báo cáo rằng cắt tỉa lặp dẫn đến một hiện tượng ngược lại với giảm kép, một hiện tượng giảm kép thưa thớt: hiệu suất tổng quát hóa ban đầu suy giảm khi cắt tỉa, và sau đó cải thiện để đạt được một tối ưu thường cung cấp hiệu suất thậm chí còn tốt hơn so với mô hình đầy đủ ban đầu. Hiện tượng giảm kép thưa thớt cho phép chúng ta xác định một mô hình tối ưu bằng cách vẽ đồ thị lỗi tổng quát hóa như một hàm của số lượng tham số trong quá trình cắt tỉa lặp. Cách thức mà thiên vị kiến trúc được tạo ra bởi cắt tỉa cho phép các mạng đảo ngược đường cong giảm kép không được hiểu về mặt lý thuyết, và tương đối chưa được khám phá về mặt thực nghiệm.

Ở đây chúng tôi tìm cách giải quyết khoảng trống kiến thức này trong khung cắt tỉa cường độ lặp (IMP) [Han et al., 2015], một phương pháp đơn giản và thành công để cắt tỉa mạng nơ-ron sâu [Blalock et al., 2020, Renda et al., 2020] đóng vai trò then chốt trong việc tìm "vé số" [Frankle and Carbin, 2019, Frankle et al., 2019, 2020], tức là các mạng con thưa thớt có thể được huấn luyện riêng lẻ mà không ảnh hưởng đến độ chính xác. Chúng tôi bắt đầu với một quan sát thực nghiệm đáng chú ý. Hình 1 cho thấy đường cong giảm kép (đường dày) và các đường cong giảm kép thưa thớt (đường mỏng) cho các mạng kết nối đầy đủ được huấn luyện trên MNIST (trái) và một ResNet-18 được huấn luyện trên CIFAR-10 (phải). Tự nhiên, điểm khởi đầu của đường cong giảm kép được cố định tại mô hình nhỏ nhất có thể. He et al. [2022] đã xem xét hiện tượng giảm kép thưa thớt bắt đầu từ một mô hình lớn duy nhất. Ở đây, chúng tôi xem xét hiện tượng giảm kép thưa thớt

Được chấp nhận cho Hội nghị lần thứ 39 về Tính không chắc chắn trong Trí tuệ Nhân tạo (UAI 2023).arXiv:2306.12190v1 [cs.LG] 21 Jun 2023

--- TRANG 2 ---
[Hình ảnh đồ thị hiển thị]

Hình 1: Cắt tỉa các mô hình dọc theo đường cong giảm kép (đỏ đậm) cho thấy rằng các đường cong giảm kép thưa thớt (đỏ nhạt) từ các mô hình khác nhau trùng khớp tại các điểm cực tiểu. Kết quả được hiển thị cho FCN ba lớp trên MNIST (trái) và ResNet-18 trên CIFAR-10 (phải) với 20% nhiễu nhãn được tính trung bình trên ba lần lặp lại.

các đường cong giảm kép thưa thớt bắt đầu từ các mạng nơ-ron với một phạm vi lớn các tham số. Chúng tôi cho thấy trong hình 1 rằng bất kể kích thước mô hình đầy đủ ban đầu (hoặc thậm chí độ chính xác kiểm tra), tất cả các đường cong giảm kép thưa thớt đều có xu hướng đạt được cực tiểu của chúng (mô hình được cắt tỉa tốt nhất) trong một phạm vi rất hẹp các kích thước mô hình, điều này một cách có hệ thống nhỏ hơn so với đáy của việc giảm đầu tiên (mô hình đầy đủ nhỏ tốt nhất). Điều gì đặc biệt về các mô hình thưa thớt như vậy? Điều gì cho phép cải thiện tổng quát hóa bằng cắt tỉa?

Để trả lời câu hỏi này một cách định lượng, chúng tôi điều tra hành vi của các mô hình được cắt tỉa tốt nhất trong một kịch bản phân loại nhị phân có kiểm soát nơi dữ liệu được tạo ra từ các phân phối có độ phức tạp khác nhau. Chúng tôi thấy rằng kích thước của mô hình được cắt tỉa tốt nhất tương quan tốt với độ phức tạp của tác vụ. Hơn nữa, các mô hình được cắt tỉa tốt nhất tốt hơn đáng kể trong việc nắm bắt phân phối nhãn có điều kiện thực sự cơ bản so với các mô hình đầy đủ (có xu hướng quá tự tin đáng kể) hoặc các mô hình đầy đủ nhỏ tốt nhất. Những quan sát thực nghiệm này được tái tạo trong phân tích của chúng tôi về dữ liệu thực, cho thấy rằng IMP thực sự nắm bắt các khía cạnh quan trọng của tính không chắc chắn trong bài toán học tập, cũng như tạo ra các mô hình dễ xử lý hơn về mặt tính toán.

Chúng tôi thấy một đóng góp chính của nghiên cứu thực nghiệm của chúng tôi trong việc đề xuất một cách để ước tính số lượng tham số "hiệu quả" mà một mô hình được huấn luyện chứa, và được yêu cầu để giải quyết một tác vụ phân loại. Định lượng số lượng tham số hiệu quả trong các mạng nơ-ron được huấn luyện đã là một câu hỏi trung tâm cho lý thuyết của mạng nơ-ron trong một thời gian dài, xem ví dụ Breiman [1995], nhưng nó vẫn là một thách thức chưa được giải quyết. Trong bản thảo này, chúng tôi tập trung vào các phát hiện thực nghiệm và để dành phân tích lý thuyết cho công việc tương lai.

Những đóng góp chính của chúng tôi là:

1. Chúng tôi thấy rằng IMP cắt tỉa các mô hình có kích thước khác nhau để tạo ra các đường cong giảm kép thưa thớt trùng khớp tại các điểm cực tiểu của lỗi kiểm tra. Chúng tôi định nghĩa số lượng tham số hiệu quả cần thiết cho một tác vụ phân loại và mô hình/kiến trúc nhất định. Chúng tôi chứng minh hiện tượng này trong các mạng nơ-ron kết nối đầy đủ và tích chập được huấn luyện trên MNIST và CIFAR-10. Ngoài ra, số lượng tham số hiệu quả tương đương trên các kiến trúc, cho thấy rằng quy trình của chúng tôi xác định một thuộc tính nội tại của tác vụ phân loại.

2. Bằng cách huấn luyện mạng nơ-ron trên các tác vụ phân loại hỗn hợp Gaussian nhị phân có độ khó tăng dần, chúng tôi cho thấy rằng số lượng tham số hiệu quả trong một mô hình tương quan với độ khó của tác vụ.

3. Cuối cùng chúng tôi nghiên cứu hiệu chuẩn của các mô hình được cắt tỉa và cho thấy rằng các mô hình được cắt tỉa nắm bắt phân phối thực của các mô hình dữ liệu tổng hợp tốt hơn so với các đối tác chưa được cắt tỉa. Kết quả là, chúng tôi cho thấy rằng các mô hình được cắt tỉa tốt nhất tốt hơn trong việc nắm bắt tính không chắc chắn trong dự đoán của chúng.

2 CÔNG TRÌNH LIÊN QUAN

Từ năm 1998, Poppi and Massart [1998] đã sử dụng cắt tỉa để tìm kiến trúc mạng nơ-ron tối ưu. Họ phát hiện rằng cắt tỉa có thể cải thiện tổng quát hóa và thậm chí phục hồi mô hình tối ưu trong một tập dữ liệu tuyến tính. Kuhn et al. [2021] giới thiệu một thước đo độ phức tạp mới cho mạng nơ-ron, cụ thể là phần trăm trọng số có thể được cắt tỉa từ mạng mà không ảnh hưởng đến hiệu suất của nó. Họ thấy rằng phần trăm trọng số có thể cắt tỉa tăng với chiều rộng mạng cho một ResNet được huấn luyện trên CIFAR-10. Li et al. [2018] sử dụng ý tưởng huấn luyện không gian con để ước tính số lượng tham số (hoặc chiều nội tại) cần thiết để đạt được hiệu suất tốt sử dụng mạng nơ-ron. Họ thấy rằng nhiều vấn đề có chiều nội tại nhỏ hơn người ta có thể nghi ngờ, và chiều nội tại cho một tập dữ liệu nhất định thay đổi ít trên một họ các mô hình với kích thước rất khác nhau.

--- TRANG 3 ---
Mặc dù những kết luận này phù hợp với các phát hiện của chúng tôi, kết quả của họ dựa vào việc sử dụng các giải pháp không gian con ngẫu nhiên với hiệu suất ~=90% của đường cơ sở để định nghĩa chiều nội tại. Phương pháp của chúng tôi thay vào đó sử dụng thưa thớt hóa để tìm số lượng tham số hiệu quả sử dụng các mô hình tổng quát hóa tốt hơn đường cơ sở.

Venkatesh et al. [2020] đã sử dụng các chiến lược hiệu chuẩn khác nhau trên các mô hình có quá nhiều tham số để nghiên cứu tác động đến các vé số kết quả. Lei et al. [2023] đề xuất một phương pháp huấn luyện thưa thớt mới cải thiện độ tin cậy của các mô hình được cắt tỉa. Từ góc độ lý thuyết, Sakamoto and Sato [2022] đã sử dụng lý thuyết PAC-Bayesian để hiểu hành vi tổng quát hóa của các mạng được cắt tỉa. Zhang et al. [2021] đặc trưng hóa hiệu suất của việc huấn luyện một mạng nơ-ron được cắt tỉa để cho thấy rằng cắt tỉa mở rộng vùng lồi gần một mô hình mong muốn với tổng quát hóa được bảo đảm. Yang et al. [2023] đã sử dụng một thiết lập có kiểm soát dưới cắt tỉa ngẫu nhiên để xác định các phần trăm cắt tỉa có thể cải thiện hiệu suất tổng quát hóa. Một loạt các công trình lý thuyết [Malach et al., 2020, Orseau et al., 2020, Pensia et al., 2020, da Cunha et al., 2022, Burkholz, 2022] đã cho thấy sự tồn tại của một vé thắng cuộc bên trong các mạng lớn hơn (sâu hơn và rộng hơn) có kích thước khác nhau, do đó cung cấp một số trực giác về lượng tham số hóa quá mức cần thiết.

Để hiểu cách IMP có thể cải thiện tổng quát hóa của mạng nơ-ron bằng cách hoạt động như một bộ điều chỉnh, Jin et al. [2022] đã nghiên cứu sự mất mát của các mẫu có ảnh hưởng trong các mô hình được cắt tỉa tối ưu. Paul et al. [2022] sử dụng hình học của cảnh quan lỗi ở mỗi mức độ cắt tỉa để hiểu các nguyên tắc đằng sau thành công của IMP (với tua lại) mà không có nhiễu nhãn. Trọng tâm của chúng tôi chủ yếu là về các thuộc tính (độ chính xác, số lượng tham số và hiệu chuẩn) của các mô hình được cắt tỉa tốt nhất. Ankner et al. [2022] đã sử dụng khung của Pope et al. [2021], sử dụng GAN để tạo ra hình ảnh với các chiều nội tại đã biết, để tìm ra rằng tính chiều nội tại của dữ liệu tương quan với khả năng cắt tỉa của mạng nơ-ron. Một số nỗ lực cũng đã được thực hiện để hiểu các mặt nạ được học bằng cắt tỉa [Paganini and Forde, 2020, Pellegrini and Biroli, 2021].

Kể từ khi tái khám phá hành vi giảm kép trong mạng nơ-ron sâu [Belkin et al., 2019, Nakkiran et al., 2021], việc đặc trưng hóa các đường cong giảm kép từ các mô hình đơn giản đến mạng sâu đã trở thành một lĩnh vực nghiên cứu rất tích cực, xem Hastie et al. [2022], Spigler et al. [2019], Advani et al. [2020], Belkin et al. [2019], d'Ascoli et al. [2020a], Lin and Dobriban [2021], d'Ascoli et al. [2020b], Mei and Montanari [2022] cho một mẫu nhỏ. Ở đây, trọng tâm của chúng tôi không phải là hiện tượng giảm kép này - thay vào đó, mục tiêu của chúng tôi là cung cấp một quy trình rõ ràng để liên kết một mô hình được cắt tỉa với một mô hình lớn sử dụng hiện tượng giảm kép thưa thớt.

3 THIẾT LẬP THỰC NGHIỆM

Tập dữ liệu: Chúng tôi đã sử dụng MNIST [LeCun et al., 1998] và CIFAR-10 [Krizhevsky et al., 2009] cho các thí nghiệm của chúng tôi với dữ liệu thực. Các thí nghiệm bổ sung cũng được thực hiện sử dụng tập dữ liệu Fashion-MNIST [Xiao et al., 2017] (xem hình S3 cho kết quả). Chúng tôi cũng thực hiện các thí nghiệm trong một thiết lập có kiểm soát, nơi chúng tôi xem xét phân loại hỗn hợp nhị phân với các đầu vào được lấy mẫu từ một hỗn hợp Gaussian trong D= 100 chiều. Thiết lập đơn giản này cho mô hình dữ liệu cho phép chúng tôi tính toán chính xác xác suất lớp có điều kiện thực của dữ liệu và đặc trưng hóa ranh giới quyết định thực tế. Điều này cho phép chúng tôi điều tra chặt chẽ thiên vị kiến trúc được tạo ra bởi cắt tỉa. Chúng tôi xem xét hai thiết lập cho phân loại hỗn hợp. Tập dữ liệu tuyến tính bao gồm hai cụm có thể được tách bằng một bộ phân loại tuyến tính. Hai cụm có các trung bình µ1̸=µ2, nhưng cùng ma trận hiệp phương sai Σ1= Σ 2. Tập dữ liệu XOR được tạo ra sao cho các Gaussian kết quả (có cùng hiệp phương sai) được đặt như biểu diễn đồ họa của hàm logic XOR. Tập huấn luyện và kiểm tra chứa lần lượt 10 000 và 5000 mẫu. Xem phụ lục A để biết thêm chi tiết.

Một khía cạnh quan trọng của các thí nghiệm của chúng tôi là thêm nhiễu nhãn đối xứng bằng cách hoán vị ngẫu nhiên các nhãn cho một phần của dữ liệu huấn luyện. Việc thêm nhiễu nhãn vào dữ liệu huấn luyện cung cấp một cách đơn giản để tạo ra hiện tượng giảm kép [Nakkiran et al., 2021] và giảm kép thưa thớt [He et al., 2022] trong mạng nơ-ron sâu. Lưu ý rằng việc sử dụng các loại nhiễu nhãn khác có thể cung cấp các kết luận khác nhau, nhưng không phải là trọng tâm của nghiên cứu hiện tại và do đó nằm ngoài phạm vi của bài báo này. Mặc dù việc thêm nhiễu nhãn có vẻ không thực tế, Northcutt et al. [2021] đã phát hiện rằng các lỗi gắn nhãn rất phổ biến trong một số tập dữ liệu học máy chuẩn và các mô hình công suất thấp hơn có thể thực tế hơn. Dựa trên quan sát này, chúng tôi cho thấy rằng cắt tỉa có thể cung cấp một cách để tìm ra công suất này.

Mô hình: Một mạng kết nối đầy đủ hai lớp (FCN) được sử dụng cho các thí nghiệm của chúng tôi về các tác vụ phân loại hỗn hợp Gaussian với năm lần lặp lại cho mỗi mô hình (xem chi tiết trong phụ lục C). Chiều rộng của lớp ẩn được thay đổi để có được các mô hình có kích thước khác nhau và tạo ra đường cong giảm kép. Đối với MNIST, chúng tôi đã sử dụng FCN hai và ba lớp trong khi thay đổi chiều rộng của lớp ẩn đầu tiên, và ResNet-6 với các bộ lọc tích chập có chiều rộng khác nhau. Đối với CIFAR-10, chúng tôi đã sử dụng ResNet-18 [He et al., 2016] và thay đổi chiều rộng của bộ lọc tích chập để có được đường cong giảm kép. Mỗi thí nghiệm được lặp lại ba lần cho các tập dữ liệu thế giới thực. Các siêu tham số được sử dụng trong các thí nghiệm này được mô tả trong phụ lục C. Hàm mất mát entropy chéo được sử dụng để huấn luyện tất cả các mô hình. Đối với CIFAR-10, chúng tôi cũng liên tiếp loại bỏ từng lớp một và sau đó huấn luyện và cắt tỉa một mô hình ResNet-18 với chiều rộng cố định của bộ lọc tích chập. Tất cả các mô hình của chúng tôi được huấn luyện đủ lâu để đảm bảo rằng chúng không ở trong chế độ giảm kép theo epoch [Nakkiran et al., 2021]. Thư viện OpenLTH1 được sử dụng cho các đánh giá thực nghiệm của chúng tôi. Mã để tái tạo các thí nghiệm của chúng tôi có sẵn trên GitHub2.

Cắt tỉa mạng: IMP với việc loại bỏ 20% trọng số tại mỗi lần lặp (tua lại vé số được sử dụng khi cần thiết, xem phụ lục B) được sử dụng cho các thí nghiệm của chúng tôi vì nó cung cấp một quy trình hiệu quả để tìm các mạng con với độ thưa thớt không tầm thường có lỗi kiểm tra thấp [Frankle and Carbin, 2019, Frankle et al., 2020, Blalock et al., 2020, Renda et al., 2020]. Cần lưu ý rằng các kỹ thuật cắt tỉa khác như cắt tỉa ngẫu nhiên và cắt tỉa dựa trên gradient cũng tạo ra đường cong giảm kép thưa thớt [He et al., 2022] và có thể được sử dụng thay thế.

Chi phí tính toán: Phân tích của chúng tôi yêu cầu chúng tôi làm việc trong mô hình giảm kép, điều này cho phép chúng tôi định nghĩa đúng đắn một mô hình được cắt tỉa tốt nhất. Thật không may, điều này ngụ ý chi phí tính toán đáng kể vì chúng tôi phải thực hiện phân tích giảm kép thưa thớt đầy đủ. Ví dụ, các hình 1, 2 và S2 yêu cầu chúng tôi huấn luyện khoảng 300 mô hình khác nhau và gần 1000 mô hình khi bao gồm các lần lặp lại. Tuy nhiên, chúng tôi tin rằng các điều tra sâu rộng của chúng tôi sử dụng tác vụ phân loại hỗn hợp và các kiến trúc khác nhau cho MNIST và CIFAR-10 đặt nền tảng vững chắc cho hiện tượng được mô tả trong bài báo.

Thuật ngữ: Kết quả trong hình 1 cho thấy rằng việc cắt tỉa lặp lại các mô hình có kích thước khác nhau tạo ra các đường cong giảm kép thưa thớt đạt được lỗi kiểm tra thấp nhất trong một phạm vi nhỏ các tham số. Để tạo điều kiện thảo luận thêm về hiện tượng này, chúng tôi định nghĩa mô hình được cắt tỉa tốt nhất và số lượng tham số hiệu quả.

Định nghĩa 1 (Mô hình được cắt tỉa tốt nhất) Cho E(·) biểu thị lỗi kiểm tra (hoặc mất mát 0-1) của một mô hình. Đối với một mô hình được huấn luyện fw, tập dữ liệu kiểm tra Stest, và phương pháp cắt tỉa ϕ, mô hình được cắt tỉa tốt nhất fbp
w được định nghĩa là
fbp
w= arg min
wE(ϕ(fw,Stest)).

Các mô hình được cắt tỉa tốt nhất này có lỗi tổng quát hóa thấp nhưng không giống như các mô hình cha có quá nhiều tham số, chúng có lỗi khác không trên dữ liệu huấn luyện.

Định nghĩa 2 (Số lượng tham số hiệu quả) Số lượng trọng số khác không trong các mô hình được cắt tỉa tốt nhất fbp
w thu được bằng cách cắt tỉa một mô hình có quá nhiều tham số.

Bằng cách cắt tỉa các mô hình có quá nhiều tham số có kích thước khác nhau, chúng ta có thể thu được các mô hình được cắt tỉa tốt nhất có độ chính xác tương tự nhưng số lượng tham số hiệu quả hơi khác.

1https://github.com/facebookresearch/open_lth
2https://github.com/viplovearora/noisy_lottery_tickets

4 KẾT QUẢ

Đầu tiên chúng tôi xác nhận rằng các mạng nơ-ron kết nối đầy đủ thể hiện cả hiện tượng giảm kép truyền thống và giảm kép thưa thớt trên các tác vụ phân loại hỗn hợp: hình 2 cho thấy cùng xu hướng trong số lượng tham số hiệu quả như chúng tôi đã quan sát trong hình 1. Như mong đợi, lỗi kiểm tra giảm theo khoảng cách giữa các trung bình cụm nu (xem hình S1).

Tiếp theo, chúng tôi xem xét số lượng tham số hiệu quả trong các mô hình được cắt tỉa tốt nhất (xem định nghĩa 1). Tập trung vào tập dữ liệu tuyến tính trong hình 2a, chúng ta thấy rằng các mô hình được cắt tỉa (đường màu đỏ mỏng) thường đạt được lỗi kiểm tra thấp nhất với khoảng 500 tham số. Con số tham số này là tối ưu cho các mạng khởi đầu có kích thước khác nhau, mà chúng tôi sử dụng để xác định số lượng tham số hiệu quả cho tập dữ liệu đó (xem định nghĩa 1 và 2). Đối với các mạng được huấn luyện trên tập dữ liệu tuyến tính, chúng tôi thấy rằng số lượng tham số hiệu quả, tức là số lượng tham số trong các mô hình được cắt tỉa tốt nhất, dao động từ ∼200 đến ∼500 cho các mô hình khởi đầu khác nhau và cho các giá trị nu khác nhau (xem phụ lục D để biết phân phối đầy đủ). Chúng tôi thấy hành vi tương tự cho tập dữ liệu XOR, hình 2b, nhưng với số lượng tham số cao hơn (giữa 250 và 1000) so với tập dữ liệu tuyến tính có cùng nu.

Các đường cong giảm kép và giảm kép thưa thớt cho các tập dữ liệu MNIST và CIFAR-10 có thể được thấy trong các hình 1 và S2. Bảng 1 và S2 cho thấy số lượng tham số và lỗi kiểm tra cho các mô hình đầy đủ/chưa cắt tỉa và các mô hình được cắt tỉa tốt nhất tương ứng. Lưu ý, quan trọng là, các mô hình được cắt tỉa tốt nhất không nội suy, vì vậy chúng thể hiện cải thiện đáng kể trong khoảng cách tổng quát hóa. Tập trung vào số lượng tham số, chúng tôi quan sát rằng việc tăng 200× cho các mô hình đầy đủ chỉ dẫn đến việc tăng ∼3.5× số lượng tham số cho các mô hình được cắt tỉa tốt nhất. So sánh các lỗi kiểm tra, một phát hiện đáng ngạc nhiên là các mạng con thưa thớt với lỗi kiểm tra thấp tồn tại bên trong các mô hình chưa cắt tỉa nằm ở chế độ nội suy của đường cong giảm kép. Điều này có nghĩa là mặc dù các mô hình chưa cắt tỉa có lỗi kiểm tra cao, chúng có thể được cắt tỉa bằng IMP để đạt được lỗi kiểm tra thấp hơn nhiều. Do đó cắt tỉa hoạt động như một loại điều chỉnh trong trường hợp này, điều này được biết là giảm thiểu đỉnh của độ chính xác kiểm tra tại ngưỡng nội suy [Belkin et al., 2019].

Các phát hiện của chúng tôi cho CIFAR-10 tương tự cho thấy rằng các mô hình được cắt tỉa tốt nhất có tổng quát hóa tốt hơn so với các đối tác chưa cắt tỉa (xem bảng 1). Tương tự như MNIST, chúng tôi quan sát rằng số lượng tham số trong các mô hình được cắt tỉa tốt nhất tăng theo kích thước của mô hình gốc nhưng với tốc độ chậm hơn nhiều. So sánh số lượng tham số hiệu quả cho MNIST và CIFAR-10, người ta có thể dễ dàng kết luận rằng cần nhiều tham số hơn cho CIFAR-10.

Số lượng tham số hiệu quả tương quan với độ khó của tác vụ: Như được minh họa trong các biểu đồ dữ liệu của hình 2, việc tách tối ưu tập dữ liệu tuyến tính đòi hỏi một mặt phẳng, trong khi

--- TRANG 5 ---
[Hình ảnh đồ thị hiển thị]

Hình 2: Trung bình trên 5 lần lặp lại của lỗi huấn luyện và kiểm tra cho các mô hình có kích thước khác nhau, sau nhiều lần lặp cắt tỉa, trên (a) tuyến tính, và (b) tập dữ liệu XOR khi khoảng cách giữa các cụm nu được thay đổi. Đường đỏ đậm có chấm cho thấy đường cong giảm kép truyền thống, trong khi các đường mỏng hơn đại diện cho lỗi kiểm tra đạt được sau các lần lặp cắt tỉa của các mô hình ban đầu (giảm kép thưa thớt). Tương tự, các đường đen cho thấy lỗi huấn luyện cả cho các mô hình đầy đủ và được cắt tỉa.

Bảng 1: Số lượng tham số và lỗi kiểm tra cho các mô hình chưa cắt tỉa (đầy đủ) và được cắt tỉa tốt nhất cho MNIST và CIFAR-10. Các giá trị trung bình trên 3 lần lặp lại được báo cáo. Chúng tôi quan sát rằng việc tăng 200× cho các mô hình đầy đủ chỉ dẫn đến việc tăng ∼3.5× số lượng tham số cho các mô hình được cắt tỉa tốt nhất. Cũng lưu ý rằng lỗi đạt được bởi các mô hình được cắt tỉa có vẻ không nhạy cảm với tỷ lệ lỗi của mô hình đầy đủ gốc, tức là ngay cả các mô hình có tổng quát hóa kém có thể được cứu vãn bằng cắt tỉa.

[Bảng hiển thị dữ liệu số]

cần hai mặt phẳng để tách các lớp trong tập dữ liệu XOR. Do đó, người ta sẽ mong đợi rằng số lượng tham số hiệu quả sẽ tăng gấp đôi từ tập dữ liệu tuyến tính sang XOR. Thú vị thay, đây là điều chúng tôi quan sát trong hình 3a, mặc dù các mô hình được cắt tỉa tốt nhất được thu được từ các mô hình đầy đủ có cùng kích thước. Phương pháp cắt tỉa do đó gợi ý một cách để định lượng số lượng tham số hiệu quả trong các mạng nơ-ron sâu, có quá nhiều tham số. Chúng ta có thể mở rộng phương pháp này để đo độ khó của tác vụ cho các tập dữ liệu thực tế hơn và mạng tích chập không? Kết quả trong hình 3b và bảng S3 cho thấy rằng đối với một mô hình ResNet cố định, khi chúng ta giảm số lượng lớp trong tập dữ liệu CIFAR-10 (hoặc

--- TRANG 6 ---
[Hình ảnh đồ thị hiển thị]

Hình 3: Phân tích của chúng tôi cho thấy rằng số lượng tham số hiệu quả tương quan với độ khó của tác vụ: (a) số lượng tham số hiệu quả cần thiết cho tập dữ liệu XOR xấp xỉ gấp đôi so với tập dữ liệu tuyến tính, điều này phản ánh độ phức tạp cao hơn của tác vụ XOR, và (b) giảm số lượng lớp trong tập dữ liệu CIFAR-10 cho phép IMP tìm các mô hình với số lượng tham số hiệu quả ít hơn.

[Hình ảnh đồ thị hiển thị]

Hình 4: So sánh số lượng tham số hiệu quả trên các kiến trúc mạng nơ-ron khác nhau cho tập dữ liệu MNIST và CIFAR-10.

tương đương làm cho tác vụ dễ hơn cho một mô hình có quá nhiều tham số), số lượng tham số hiệu quả giảm cho một tác vụ phân loại nhỏ hơn. Điều này càng khẳng định quan sát của chúng tôi về tác vụ phân loại hỗn hợp rằng số lượng tham số hiệu quả liên quan đến độ khó của tác vụ. Thú vị thay, số lượng tham số hiệu quả không giảm tuyến tính với số lượng lớp.

Số lượng tham số hiệu quả tương đương trên các kiến trúc: Lựa chọn kiến trúc mạng nơ-ron ảnh hưởng như thế nào đến số lượng tham số hiệu quả cho một tập dữ liệu nhất định? Chúng tôi so sánh kích thước của các mô hình được cắt tỉa tốt nhất thu được cho tập dữ liệu MNIST sử dụng ba kiến trúc mạng nơ-ron khác nhau: FCN hai lớp, FCN ba lớp, và ResNet-6, trong hình 4a. Chúng tôi thấy rằng mặc dù lỗi kiểm tra của các mô hình được cắt tỉa tốt nhất cho các kiến trúc khác nhau là khác nhau (xem bảng 1 và S2), số lượng tham số hiệu quả khá tương đương. Điều này cung cấp bằng chứng thực nghiệm bổ sung rằng quy trình của chúng tôi có thể bất biến với kiến trúc mạng nơ-ron và phản ánh một khái niệm về 'độ khó' của tác vụ phân loại, điều mà chúng tôi đã lưu ý trong tác vụ phân loại hỗn hợp. Quan sát này cũng chuyển sang các kiến trúc mạng nơ-ron tích chập khác nhau trên CIFAR-10. Hình 4b cho thấy các đường cong giảm kép thưa thớt thu được cho DenseNet, VGG, và ResNet. Số lượng tham số hiệu quả tương tự trên ba kiến trúc khác nhau: 44 590 cho DenseNet, 44 468 cho VGG, và 39 483 cho ResNet.

--- TRANG 7 ---
[Hình ảnh hiển thị phân tích ranh giới quyết định và đường cong hiệu chuẩn]

Hình 5: Phân tích tính không chắc chắn dự đoán trong tác vụ phân loại hỗn hợp. Chúng tôi thấy rằng các mô hình được cắt tỉa tốt nhất tốt hơn trong việc nắm bắt tính không chắc chắn, trong khi các mô hình có quá nhiều tham số có xu hướng quá tự tin trong dự đoán của chúng.

4.1 CÁC MÔ HÌNH ĐƯỢC CẮT TỈA NẮHM BẮT TÍNH KHÔNG CHẮC CHẮN TỐT HỌN

Đầu tiên chúng tôi sử dụng tác vụ phân loại hỗn hợp để hiểu thiên vị kiến trúc được tạo ra bởi cắt tỉa. Đối với kết quả được trình bày trong hình 5, chúng tôi đánh giá các mô hình với 500 nơ-ron trong lớp ẩn cho tập dữ liệu tuyến tính (nu= 0.2) và XOR (nu= 0.6). Kết quả tương tự được thu được cho các mô hình khởi đầu khác nhau và các giá trị nu khác. Để hiểu điều gì phân biệt các mô hình được cắt tỉa, chúng tôi phân tích ranh giới quyết định được học bởi các mô hình này. Chúng tôi lấy mẫu 100 000 điểm từ phân phối dữ liệu gốc và tính toán các xác suất tương ứng cho các mô hình quan tâm cùng với các xác suất lớp có điều kiện. Trong hình 5a, chúng tôi vẽ đồ thị xác suất của dữ liệu thuộc lớp y= 0 như một hàm của khoảng cách tương đối giữa các tâm của hai cụm (nằm tại x= 0 và x= 1) cho tập dữ liệu tuyến tính. Như mong đợi, các xác suất tối ưu được cho bởi một hàm logistic được hiển thị bằng các chấm đen. So sánh các mô hình được cắt tỉa tốt nhất và đầy đủ, chúng ta có thể kết luận rằng hàm được học bởi mô hình được cắt tỉa tốt nhất gần hơn nhiều với xác suất lớp có điều kiện thực được sử dụng để tạo ra dữ liệu, trong khi mô hình có quá nhiều tham số đầy đủ quá tự tin trong việc dự đoán một trong hai lớp.

Trong hình 5c, chúng tôi trực quan hóa sự khác biệt tuyệt đối giữa các xác suất dự đoán và các xác suất lớp có điều kiện tương đối với vị trí của mỗi điểm được chiếu trên một mặt phẳng 2D. Một giá trị gần 0 trên thang màu trong hình 5c ngụ ý rằng mô hình đã ước tính chính xác xác suất có điều kiện. Người ta sẽ mong đợi rằng một mô hình sẽ dự đoán lớp đúng một cách tự tin xa hơn từ ranh giới và mắc lỗi gần ranh giới. Trực giác này được minh họa rõ ràng bằng cách sử dụng kết quả cho tập dữ liệu tuyến tính. Đáng chú ý, mô hình đầy đủ đưa ra dự đoán tự tin gần ranh giới, do đó dẫn đến giá trị ~=0.45 được hiển thị bằng màu đỏ trong hình 5c. Trong tập dữ liệu XOR, mô hình được cắt tỉa tốt nhất không chắc chắn hơn trong dự đoán của nó, đặc biệt là gần ranh giới, nhưng tốt hơn mô hình đầy đủ. Điều này được minh họa thêm bằng các đường cong hiệu chuẩn trong hình 5b nơi chúng ta thấy hiệu chuẩn gần như hoàn hảo trong các mô hình được cắt tỉa tốt nhất cho cả hai tập dữ liệu, trong khi các mô hình đầy đủ quá tự tin và được hiệu chuẩn kém. Chúng tôi cũng quan sát rằng hiệu chuẩn của các mô hình đầy đủ nhỏ tốt nhất tương đương với các mô hình được cắt tỉa tốt nhất.

Các đường cong hiệu chuẩn cho MNIST và CIFAR-10 trong hình 6 cho thấy rằng các đường cong cho các mô hình được cắt tỉa tốt nhất thường nằm trên đường đen, có nghĩa là chúng đưa ra dự đoán đúng trong khi không chắc chắn. Hình 6c minh họa điều này bằng cách sử dụng các xác suất tương ứng với lớp được dự đoán. Điều này ngụ ý rằng mặc dù các mô hình được cắt tỉa chính xác, chúng có xu hướng thiếu tự tin trong dự đoán của chúng trong khi các mô hình đầy đủ quá tự tin. Điều này trái ngược với quan sát của chúng tôi trong tác vụ phân loại hỗn hợp. Khi phân tích thêm, chúng tôi thấy rằng các mô hình được cắt tỉa tốt nhất được

--- TRANG 8 ---
[Hình ảnh hiển thị đường cong hiệu chuẩn]

Hình 6: Các đường cong hiệu chuẩn trung bình theo lớp cho các mô hình được cắt tỉa tốt nhất và đầy đủ trên (a) MNIST và (b) tập dữ liệu CIFAR-10 cho thấy rằng các mô hình được cắt tỉa thiếu tự tin trong khi các mô hình đầy đủ quá tự tin. Các vùng được làm nổi bật biểu thị sự khác biệt giữa các lớp. (c) Xác suất của lớp được dự đoán cho tập dữ liệu MNIST (trên) và CIFAR-10 (dưới).

[Hình ảnh hiển thị đồ thị]

Hình 7: Giảm kép thưa thớt (đỏ) và lỗi hiệu chuẩn mong đợi (xanh) khi cắt tỉa FCN hai lớp trên MNIST (trái) và ResNet-18 cho CIFAR-10 (phải). Vùng được làm nổi bật cho thấy độ lệch trên ba lần lặp lại. So sánh lỗi hiệu chuẩn cho nhãn thực và nhiễu, chúng tôi thấy rằng các mô hình được cắt tỉa tốt nhất được hiệu chuẩn tối ưu với dữ liệu nhiễu.

hiệu chuẩn với dữ liệu kiểm tra có nhiễu nhãn (xem hình S4). Đây là một kết quả hợp lý vì mô hình được huấn luyện với nhiễu nhãn, điều này có thể tạo ra thiên vị đáng kể trong phân loại được học bởi mô hình.

Để hiểu cách cắt tỉa lặp ảnh hưởng đến hiệu chuẩn, trong hình 7 chúng tôi vẽ đồ thị lỗi kiểm tra và lỗi hiệu chuẩn mong đợi [Guo et al., 2017] (ECE) cho một mô hình cố định. Đối với cả nhãn nhiễu và thực, chúng tôi quan sát rằng ECE ban đầu tăng nhẹ khi cắt tỉa lặp nhưng sau đó giảm đáng kể. Sự khác biệt chính giữa hai kịch bản là vị trí của cực tiểu. Đối với nhãn thực, ECE thấp nhất được quan sát cho các mô hình có lỗi kiểm tra cao, trong khi các mô hình được cắt tỉa tốt nhất được hiệu chuẩn tốt hơn với dữ liệu có nhiễu nhãn. Hành vi kỳ lạ này đòi hỏi điều tra thêm để hiểu tác động của cắt tỉa lên hiệu chuẩn mô hình. Có lẽ, một sự đánh đổi giữa lỗi kiểm tra và lỗi hiệu chuẩn có thể được sử dụng để chọn các mô hình có lỗi thấp và được hiệu chuẩn tốt.

5 KẾT LUẬN

Sự tồn tại và khả năng xác định của các mạng được cắt tỉa nhỏ, vẫn có thể tổng quát hóa cũng như các mô hình lớn, có quá nhiều tham số, vẫn là một sự thật được hỗ trợ bằng thực nghiệm nhưng vẫn thiếu giải thích lý thuyết thỏa đáng. Trong bài báo này, chúng tôi đã cung cấp một số quan sát thực nghiệm tiết lộ hai đặc điểm hấp dẫn của các mạng được cắt tỉa nhỏ: (1) số lượng tham số của chúng tương quan với độ khó của tác vụ và cung cấp một thước đo số lượng tham số hiệu quả trong các mô hình lớn, và (2) các mô hình được cắt tỉa ít có xu hướng đưa ra dự đoán quá tự tin hơn.

Làm việc trong khung IMP, chúng tôi tái tạo một cách nhất quán hiện tượng giảm kép thưa thớt được quan sát lần đầu tiên bởi He et al. [2022] trong một số tập dữ liệu tổng hợp và thực. Trong khi He et al. [2022] tập trung vào hiện tượng giảm kép thưa thớt của mô hình lớn, chúng tôi quan sát rằng bất kể kích thước (và độ chính xác kiểm tra) của mô hình mà chúng ta bắt đầu hiện tượng giảm kép thưa thớt, các mô hình được cắt tỉa tối ưu kết quả đều đạt được lỗi tổng quát hóa tương tự và có

--- TRANG 9 ---
kích thước tương đương. Kích thước của các mô hình được cắt tỉa tốt nhất (được định nghĩa bởi số lượng tham số được giữ lại) có vẻ tương quan tốt với các khái niệm tự nhiên về độ phức tạp của tác vụ, cả trong tập dữ liệu thực và mô phỏng.

Sau đó chúng tôi phân tích sâu hơn các hàm được học bởi các mạng nhỏ này: trên dữ liệu mô phỏng, nơi chúng tôi biết các hàm xác suất lớp có điều kiện thực tế, chúng tôi quan sát rằng các mô hình được cắt tỉa nắm bắt tốt hơn nhiều hàm cơ bản thực sự, trong khi các mô hình đầy đủ có xu hướng quá tự tin. Trên dữ liệu thực, chúng tôi lại xác minh rằng các mô hình đầy đủ tự tin hơn so với các mô hình được cắt tỉa, trong trường hợp này có xu hướng hơi thiếu tự tin. Một giải thích có thể là, như thông lệ trong các nghiên cứu giảm kép, các mô hình của chúng tôi được huấn luyện trên dữ liệu có nhiễu nhãn, điều này ảnh hưởng đến hiệu chuẩn của chúng trên dữ liệu có nhãn thực. Thực vậy, chúng tôi thấy rằng các mô hình được cắt tỉa được hiệu chuẩn gần như tối ưu với dữ liệu có nhiễu nhãn, trong khi các mô hình đầy đủ một lần nữa quá tự tin.

Kết quả của chúng tôi hơi trái ngược với một tuyên bố gần đây của Yang et al. [2023], đã cho thấy bằng chứng về sự quá tự tin của các mô hình được cắt tỉa trong một ví dụ (ResNet-50 trên CIFAR-100). Một lý do có thể cho sự khác biệt này là thuật toán cắt tỉa được sử dụng trong [Yang et al., 2023] là tham lam và không yêu cầu huấn luyện lại từ khởi tạo. Ngoài ra, thiết lập của họ không liên quan đến huấn luyện trên nhãn nhiễu; cả hai quan sát này có thể giải thích các kết luận khác nhau được đạt bởi nghiên cứu đó. Thực vậy, những cân nhắc này chỉ ra một tương tác không tầm thường giữa thuật toán cắt tỉa, quy trình huấn luyện và các đặc điểm thống kê của các mô hình được cắt tỉa kết quả. Khám phá những câu hỏi như vậy, cả về mặt lý thuyết và thực nghiệm, cuối cùng có thể làm sáng tỏ thành công bất hợp lý của việc cắt tỉa các mạng nơ-ron lớn.

Trong tương lai, sẽ thú vị khi thấy các quy trình cắt tỉa khác ảnh hưởng như thế nào đến các phát hiện của chúng tôi và mở rộng nghiên cứu của chúng tôi cho các tập dữ liệu lớn hơn như ImageNet. Sẽ thú vị khi thấy liệu những phát hiện này có áp dụng cho các kiến trúc khác như bộ mã hóa tự động và mạng nơ-ron hồi quy hay không. Cuối cùng, các phát hiện của chúng tôi làm nổi bật tầm quan trọng của hiện tượng giảm kép thưa thớt do đó khuyến khích công việc lý thuyết để hiểu hiện tượng này.

Đóng góp của Tác giả
Viplove Arora, Sebastian Goldt, và Guido Sanguinetti đã nghĩ ra ý tưởng và viết bài báo. Daniele Irto đã thực hiện các thí nghiệm cho tác vụ phân loại hỗn hợp. Viplove Arora đã thực hiện phần còn lại của phân tích thực nghiệm.

Lời cảm ơn
Chúng tôi thừa nhận đồng tài trợ từ Next Generation EU, trong bối cảnh của Kế hoạch Phục hồi và Khả năng chống chịu Quốc gia, Đầu tư PE1 - Dự án FAIR "Nghiên cứu Trí tuệ Nhân tạo Tương lai". Nguồn lực này được đồng tài trợ bởi Next Generation EU [DM 1555 del 11.10.22]. Các quan điểm và ý kiến được bày tỏ chỉ là của các tác giả và không nhất thiết phản ánh quan điểm của Liên minh Châu Âu hoặc Ủy ban Châu Âu. Cả Liên minh Châu Âu và Ủy ban Châu Âu đều không thể chịu trách nhiệm về chúng. Viplove Arora thừa nhận sự hỗ trợ một phần từ Assicurazioni Generali thông qua một khoản tài trợ tự do.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo tiếp theo]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
A TẬP DỮ LIỆU PHÂN LOẠI HỖN HỢP

Một kỹ thuật phổ biến trong nghiên cứu lý thuyết mạng nơ-ron là hoạt động trong một thiết lập có kiểm soát, nơi các mô hình và dữ liệu được đơn giản hóa để cho phép tính toán phân tích cho các đối tượng khó xử lý khác. Một ví dụ về phương pháp như vậy là khung giáo viên-học sinh [Seung et al., 1992], nơi tập dữ liệu được tạo ra bởi một mạng nơ-ron được xây dựng cho mục đích đó (giáo viên) và một mạng nơ-ron khác được giao nhiệm vụ học trên dữ liệu đó (học sinh). Đối với phân tích của chúng tôi, chúng tôi tạo ra hai loại tập dữ liệu phân loại nhị phân cân bằng sử dụng hỗn hợp Gaussian với các đặc điểm khác nhau và các mức độ khó khác nhau. Việc sử dụng mô hình hỗn hợp cho phép chúng tôi tạo ra các tập dữ liệu dễ hiểu và có thể dễ dàng trực quan hóa trong không gian hai chiều.

Chúng tôi xem xét hai thiết lập cho phân loại hỗn hợp. Tập dữ liệu tuyến tính bao gồm hai cụm có thể được tách bằng một hàm tuyến tính. Hai cụm có các trung bình khác nhau µ1̸=µ2 nhưng cùng hiệp phương sai Σ1= Σ 2. Tập dữ liệu XOR được tạo ra sao cho các cụm kết quả được đặt như biểu diễn đồ họa của hàm logic XOR. Chúng tôi giả định rằng độ khó của tác vụ phân loại sẽ tăng từ tập dữ liệu tuyến tính sang XOR. Dữ liệu được lấy mẫu bằng một mô hình hỗn hợp Gaussian có điều kiện trên một nhãn được định nghĩa trước y. Chúng tôi cũng định nghĩa một hệ số tách nu để điều chỉnh khoảng cách giữa các cụm.

Để định nghĩa đúng công thức của các mô hình hỗn hợp của chúng tôi, chúng tôi sử dụng một phương pháp tương tự như được sử dụng bởi Refinetti et al. [2021]. Phân phối dữ liệu cho một mẫu đầu vào x duy nhất trong RD cho lớp y được lấy mẫu đồng nhất ngẫu nhiên là:

p(x, y) =p(y)p(x|y), p(x|y) =X
alpha trong ST(y)PalphaN(µalpha,Σalpha). (S1)

trong đó p(x|y) là xác suất lấy mẫu x có điều kiện trên lớp y trong {0,1} của mẫu. Mỗi x được lấy mẫu bằng một phân phối chuẩn đa biến. ST(y) là tập hợp tất cả các chỉ số có thể cho lớp y và loại tập dữ liệu T. Palpha là xác suất của phân phối chuẩn đa biến D-chiều thứ alpha N(µalpha,Σalpha), phụ thuộc vào kích thước của ST(y). Công thức này có thể dễ dàng được sử dụng để tạo ra dữ liệu với số lượng cụm, lớp và loại tập dữ liệu chung. Đặc biệt, nhiều lớp và loại tập dữ liệu hơn có thể được bao gồm bằng cách định nghĩa các tập hợp chỉ số ST(y) thích hợp, bổ sung. Đối với các thí nghiệm của chúng tôi, chúng tôi chỉ xem xét hai lớp.

A.1 QUY TRÌNH TẠO DỮ LIỆU

Nhãn: Bước đầu tiên trong việc tạo ra dữ liệu là tạo một vector y của các lớp 0 hoặc 1 có kích thước bằng số lượng mẫu huấn luyện mong muốn N. Điều này được thực hiện bằng cách lấy mẫu N giá trị từ một phân phối đồng nhất U(0,1) và gán lại chúng thành các giá trị 0 hoặc 1 tùy thuộc vào việc chúng <=0.5 hoặc >0.5, tương ứng. Vector y kết quả là một mảng các giá trị 0 hoặc 1 theo tỷ lệ cân bằng và nó tương ứng với các nhãn của các mẫu huấn luyện trong tập dữ liệu của chúng tôi.

Tập dữ liệu tuyến tính: Đối với mỗi quan sát xi,i= 1, . . . , N, của tập dữ liệu tuyến tính, tập hợp các chỉ số SL(y) chỉ có một phần tử cho mỗi lớp. Điều này có nghĩa là, đối với mỗi lớp, các điểm đầu vào có thể được lấy mẫu bởi một chuẩn đa biến:

SL(y= 0) = {alpha0} → µalpha0= 0· 1D(S2a)
SL(y= 1) = {alpha1} → µalpha1=nu· 1D(S2b)
Σalpha0=Σalpha1=ID. (S2c)

1D là một vector D-chiều của tất cả các số một có thể được nhân với một vô hướng, có nghĩa là tất cả các phần tử của nó được nhân với vô hướng đó. ID là ma trận đơn vị D×D, và các phần tử của nó có thể được nhân với một số vô hướng3. Khoảng cách giữa hai cụm, làm cho hai lớp ít hoặc nhiều phân biệt hơn, có thể được thay đổi bằng cách đơn giản tăng hoặc giảm giá trị của hệ số nu. Thực hiện PCA trên tập dữ liệu tuyến tính và vẽ hai thành phần chính đầu tiên cho ra các cụm được hiển thị trong hình 2a. Trong những biểu đồ đó, có thể quan sát sự phân biệt rõ ràng giữa các cụm thuộc về hai lớp. Chúng ta cũng có thể thấy sự thay đổi trong khoảng cách giữa các cụm khi nu được thay đổi.

3Ký hiệu này được sử dụng nhất quán trong phần này, để chỉ ra các trung bình và hiệp phương sai của các phân phối chuẩn.

--- TRANG 13 ---
Tập dữ liệu XOR: Đối với tập dữ liệu thứ hai, mục tiêu của chúng tôi là làm cho dữ liệu xuất hiện như bốn cụm riêng biệt được đặt như biểu diễn trực quan của toán tử logic XOR. Trong trường hợp này, các tập hợp chỉ số tương ứng với lớp y= 0 và y= 1 có hai phần tử mỗi tập. Do đó, các điểm dữ liệu của mỗi lớp có thể được lấy mẫu từ hai phân phối khác nhau với xác suất bằng nhau. Đối với lớp y= 0, các mẫu được tạo ra như tập dữ liệu tuyến tính được mô tả trong phương trình (S2). Đối với lớp khác, đặc điểm chính của nó là các vector trung bình của các phân phối chuẩn đa biến bao gồm hai nửa D/2-chiều với các giá trị khác nhau:

SX(y= 0) = {alphaA_0, alphaB_0} →(
µalphaA_0= 0· 1D
µalphaB_0=nu· 1D(S3a)

SX(y= 1) = {alphaA_1, alphaB_1} →(
µalphaA_1= [0· 1D/2, nu· 1D/2]
µalphaB_0= [nu· 1D/2,0· 1D/2](S3b)

ΣalphaA_0=ΣalphaB_0=ΣalphaA_1=ΣalphaB_1=ID. (S3c)

Các biểu đồ PC của tập dữ liệu này được hiển thị trong hình 2b, nơi chúng ta có thể thấy vị trí của bốn cụm trong bố cục hình chữ thập.

B CẮT TỈA VÀ TUA LẠI

Trong các thí nghiệm của chúng tôi, huấn luyện luôn được theo sau bởi một loạt các lần lặp cắt tỉa được thực hiện theo kỹ thuật IMP [Han et al., 2015, Frankle and Carbin, 2019], được mô tả dưới đây:

1. Khởi tạo mô hình với trọng số W0, thu được một hàm mô hình f(x;W0).
2. Huấn luyện mô hình trong số epoch mong muốn j, đạt được một tập hợp trọng số Wj.
3. Sắp xếp tất cả các trọng số trong Wj theo giá trị tuyệt đối và chọn p% thấp nhất, trong đó p là một số giữa 0 và 100.
4. Cắt tỉa các trọng số được chọn bằng cách áp dụng một mặt nạ lên mô hình gốc, thu được f(x;mask⊙ W j).
5. Đặt lại các tham số còn lại về các giá trị khởi tạo đầu tiên của chúng, thu được f(x;mask⊙ W 0).
6. Lặp lại quy trình n lần, cắt tỉa p% các kết nối còn lại tại mỗi lần lặp.

Trong một số trường hợp, các mô hình lớn hơn cần được cắt tỉa bằng kỹ thuật tua lại vé số [Frankle et al., 2019]. Tua lại đơn giản bao gồm việc chỉnh sửa một bước của quy trình được mô tả ở trên:

5 Đặt lại các tham số còn lại về các giá trị tại lần lặp k≪j của vòng lặp huấn luyện, thu được f(x;mask⊙ W k)

trong đó k là một siêu tham số đại diện cho số lần lặp tua lại. Kỹ thuật này cũng được bao gồm thuận tiện trong thư viện OpenLTH4.

C SIÊU THAM SỐ MÔ HÌNH

C.1 PHÂN LOẠI HỖN HỢP

Chúng tôi đã sử dụng các mạng kết nối đầy đủ hai lớp không có bias cho các thí nghiệm của chúng tôi trên hai tập dữ liệu phân loại hỗn hợp. Các mô hình với P trong {1,3,5,8,10,15,20,25,50,100,200,500,1000,10000} nơ-ron trong lớp ẩn được sử dụng để tạo ra đường cong giảm kép. Các mô hình này sau đó được cắt tỉa bằng IMP để tạo ra các đường cong giảm kép thưa thớt. Trọng số mạng được khởi tạo bằng phân phối đồng nhất Kaiming. Hàm kích hoạt được chọn cho mô hình này là Đơn vị Tuyến tính Chỉnh lưu (ReLU). Gradient descent ngẫu nhiên với tốc độ học 0.1 được sử dụng làm bộ tối ưu. Tất cả các mô hình được huấn luyện trong 1000 epoch sử dụng kích thước batch 1024. Tất cả các mô hình được cắt tỉa đủ để quan sát các đường cong giảm kép thưa thớt. Tất cả các thí nghiệm được lặp lại năm lần.

Tập dữ liệu tuyến tính: Chúng tôi thay đổi khoảng cách giữa các trung bình cụm nu trong {0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0} để xem nó ảnh hưởng như thế nào đến lỗi kiểm tra và số lượng tham số hiệu quả. 5% nhiễu nhãn ngẫu nhiên trong tập huấn luyện được sử dụng để quan sát hai đường cong giảm kép.

4https://github.com/facebookresearch/open_lth

--- TRANG 14 ---
Tập dữ liệu XOR: Chúng tôi thay đổi khoảng cách giữa các trung bình cụm nu trong {0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3} để xem nó ảnh hưởng như thế nào đến lỗi kiểm tra và số lượng tham số hiệu quả. Các giá trị nu cao hơn là cần thiết để đảm bảo khoảng cách đủ giữa các cụm. Nhiễu nhãn cao hơn 25% là cần thiết để quan sát nhất quán hai đường cong giảm kép trong các tập dữ liệu XOR.

C.2 MNIST

Kiến trúc kết nối đầy đủ ba lớp được sử dụng cho MNIST là một mở rộng của mô hình tiêu chuẩn được sử dụng cho MNIST trong tài liệu cắt tỉa. Chúng tôi thay đổi số lượng nơ-ron P trong {3,5,10,25,50,100,300,500,1000,5000,10000} trong lớp ẩn đầu tiên. Kích thước của lớp ẩn thứ hai được giữ cố định ở 100. Đối với FCN hai lớp, P trong {5,10,50,100,300,500,1000,2000,5000,10000} nơ-ron được sử dụng trong lớp ẩn. Đối với các mạng kết nối đầy đủ, tua lại vé số được sử dụng cho các mạng với P>=1000. Đối với ResNet-6, chúng tôi thay đổi chiều rộng W trong {1,2,5,8,11,15,20,40,80,120} của các bộ lọc tích chập để có được các mạng có kích thước khác nhau. Chi tiết thêm có thể được tìm thấy trong bảng S1.

C.3 FASHION-MNIST

Chúng tôi cũng đã thực hiện một tập hợp nhỏ các thí nghiệm trên tập dữ liệu Fashion-MNIST. Chúng tôi chỉ thực hiện một số lượng hạn chế các thí nghiệm tập trung vào việc tìm số lượng tham số hiệu quả bằng các mô hình có quá nhiều tham số. Vì tái tạo đường cong giảm kép không phải là mục tiêu, chúng tôi chỉ sử dụng FCN ba lớp với P trong {500,1000} nơ-ron trong lớp ẩn đầu tiên. Kích thước của lớp ẩn thứ hai được giữ cố định ở 100. Lưu ý rằng cần nhiều epoch hơn (320) để huấn luyện một FCN ba lớp trên Fashion-MNIST.

C.4 CIFAR-10

Chúng tôi chủ yếu xem xét ResNet-18 cho CIFAR-10, nơi chiều rộng W trong {2,5,8,11,15,20,40,60,80,100,120,150} của các bộ lọc tích chập được thay đổi để có được các mạng có kích thước khác nhau. Dựa trên các quan sát trước đây Frankle et al. [2019], He et al. [2022], chúng tôi đã sử dụng 10 epoch tua lại để nhất quán tìm vé số trong ResNet-18. Để so sánh số lượng tham số hiệu quả trên các kiến trúc CNN khác nhau, chúng tôi đã thực hiện phân tích của chúng tôi trên DenseNet-121 và VGG-16. Chi tiết thêm có thể được tìm thấy trong bảng S1.

Bảng S1: Các kiến trúc mạng nơ-ron được sử dụng trên dữ liệu thực. LR đề cập đến tốc độ học.

[Bảng dữ liệu]

D KẾT QUẢ BỔ SUNG

Hình S1 cho thấy cách số lượng tham số hiệu quả và lỗi kiểm tra của các mô hình được cắt tỉa tốt nhất thay đổi với nu cho các tập dữ liệu tuyến tính và XOR. Như mong đợi, lỗi kiểm tra giảm theo khoảng cách giữa các trung bình cụm nu. Đối với các mạng được huấn luyện trên tập dữ liệu tuyến tính, chúng tôi thấy rằng số lượng tham số hiệu quả, tức là số lượng tham số trong các mô hình được cắt tỉa tốt nhất, dao động từ ∼200 đến ∼500 cho các mô hình khởi đầu khác nhau và cho các giá trị nu khác nhau (xem phụ lục D để biết phân phối đầy đủ). Chúng tôi thấy hành vi tương tự cho tập dữ liệu XOR, hình 2b, nhưng với số lượng tham số cao hơn (giữa 250 và 1000) so với tập dữ liệu tuyến tính có cùng nu.

Các đường cong giảm kép và giảm kép thưa thớt thu được cho MNIST trên FCN hai lớp và ResNet-6 có thể được thấy trong hình S2. Bảng S2 cho thấy số lượng tham số và lỗi kiểm tra cho các mô hình đầy đủ/chưa cắt tỉa và các mô hình được cắt tỉa tốt nhất tương ứng. Các đường cong giảm kép thưa thớt cho hai mô hình khác nhau trên Fashion-MNIST trong hình S3 cho thấy rằng, so với MNIST, lỗi kiểm tra cho các mô hình được cắt tỉa tốt nhất cao hơn trong khi số lượng tham số hiệu quả xấp xỉ 10 000.

[Hình ảnh và bảng bổ sung]

--- TRANG 15 ---
[Hình ảnh hiển thị phân phối tham số hiệu quả]

Hình S1: Phân phối số lượng tham số hiệu quả của các mô hình được cắt tỉa tốt nhất (trục y) khi khoảng cách giữa các cụm nu (trục x) được thay đổi cho các tập dữ liệu tuyến tính và xor. Chỉ các mô hình được cắt tỉa có nguồn gốc từ các mô hình đầy đủ có quá nhiều tham số được xem xét. Các số trên các hộp báo cáo lỗi kiểm tra của mô hình có số lượng tham số hiệu quả trung vị cho mỗi nu.

[Hình ảnh và bảng hiển thị kết quả bổ sung]

--- TRANG 16 ---
[Hình ảnh hiển thị kết quả Fashion-MNIST]

Hình S3: Ước tính số lượng tham số hiệu quả cho Fashion-MNIST: Kết quả được hiển thị cho FCN ba lớp trên Fashion-MNIST với 20% nhiễu nhãn được tính trung bình trên ba lần lặp lại. So với MNIST, lỗi kiểm tra cho các mô hình được cắt tỉa tốt nhất cao hơn trong khi số lượng tham số hiệu quả xấp xỉ 10 000.

[Bảng dữ liệu bổ sung]

Cuối cùng, các đường cong hiệu chuẩn khi nhiễu nhãn được thêm vào tập kiểm tra cho MNIST và CIFAR-10 có thể được thấy trong hình S4. Các đường cong hiệu chuẩn trung bình theo lớp cho thấy rằng các mô hình được cắt tỉa được hiệu chuẩn tốt với dữ liệu nhiễu trong khi các mô hình đầy đủ quá tự tin.

--- TRANG 17 ---
[Hình ảnh hiển thị đường cong hiệu chuẩn với nhiễu]

Hình S4: Các đường cong hiệu chuẩn trung bình theo lớp cho các mô hình được cắt tỉa tốt nhất và đầy đủ trên (a) MNIST và (b) tập dữ liệu CIFAR10 với nhiễu được thêm vào các nhãn trong tập kiểm tra cho thấy rằng các mô hình được cắt tỉa được hiệu chuẩn tốt với dữ liệu nhiễu trong khi các mô hình đầy đủ quá tự tin. Các vùng được làm nổi bật biểu thị sự khác biệt giữa các lớp.
