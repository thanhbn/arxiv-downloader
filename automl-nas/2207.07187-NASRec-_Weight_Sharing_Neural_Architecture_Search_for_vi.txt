# NASRec: Chia sẻ Trọng số trong Tìm kiếm Kiến trúc Mạng Neural cho Hệ thống Gợi ý
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2207.07187.pdf
# Kích thước file: 1678161 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
NASRec: Chia sẻ Trọng số trong Tìm kiếm Kiến trúc Mạng Neural cho
Hệ thống Gợi ý
Tunhou Zhang∗
Đại học Duke
Durham, USA
tunhou.zhang@duke.eduDehua Cheng
Meta AI
Menlo Park, USA
dehuacheng@fb.comYuchen He
Meta AI
Menlo Park, USA
yuchenhe@fb.comZhengxing Chen
Meta AI
Menlo Park, USA
czxttkl@fb.com
Xiaoliang Dai
Meta AI
Menlo Park, USA
xiaoliangdai@fb.comLiang Xiong
Meta AI
Menlo Park, USA
lxiong@fb.comFeng Yan
Đại học Houston
Houston, USA
fyan5@central.uh.eduHai Li
Đại học Duke
Durham, USA
hai.li@duke.edu
Yiran Chen
Đại học Duke
Durham, USA
yiran.chen@duke.eduWei Wen†
Meta AI
Menlo Park, USA
wewen@fb.com
TÓM TẮT
Sự phát triển của mạng neural sâu mang đến cơ hội mới trong việc tối ưu hóa hệ thống gợi ý. Tuy nhiên, việc tối ưu hóa hệ thống gợi ý sử dụng mạng neural sâu đòi hỏi sự chế tạo kiến trúc tinh tế. Chúng tôi đề xuất NASRec, một mô hình huấn luyện một siêu mạng đơn lẻ và hiệu quả tạo ra nhiều mô hình/kiến trúc con thông qua chia sẻ trọng số. Để vượt qua các thách thức về tính đa phương thức của dữ liệu và tính không đồng nhất của kiến trúc trong lĩnh vực gợi ý, NASRec thiết lập một siêu mạng lớn (tức là không gian tìm kiếm) để tìm kiếm các kiến trúc đầy đủ. Siêu mạng kết hợp lựa chọn toán tử đa dạng và kết nối dày đặc để giảm thiểu nỗ lực con người trong việc tìm kiếm các tiên nghiệm. Quy mô và tính không đồng nhất trong NASRec đặt ra một số thách thức, chẳng hạn như hiệu quả huấn luyện thấp, mất cân bằng toán tử và suy giảm tương quan xếp hạng. Chúng tôi giải quyết những thách thức này bằng cách đề xuất lấy mẫu một toán tử-bất kỳ kết nối, các mô-đun tương tác cân bằng toán tử và tinh chỉnh sau huấn luyện. Các mô hình được chế tạo của chúng tôi, NASRecNet, cho thấy kết quả đầy hứa hẹn trên ba điểm chuẩn dự đoán Tỷ lệ Nhấp chuột (CTR), cho thấy NASRec vượt trội hơn cả các mô hình được thiết kế thủ công và các phương pháp NAS hiện có với hiệu suất tối tân. Công trình của chúng tôi được công bố công khai tại đây.
TỪ KHÓA
hệ thống gợi ý, tìm kiếm kiến trúc mạng neural, chia sẻ trọng số,
tiến hóa có điều chỉnh, mạng neural
∗Phần lớn công việc này được thực hiện khi tác giả đầu tiên là thực tập sinh tại Meta
Platforms, Inc.
†Tác giả liên hệ. Quản lý thực tập sinh.
Được phép tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần công trình này cho sử dụng cá nhân hoặc lớp học miễn phí với điều kiện các bản sao không được tạo hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công việc này thuộc sở hữu của những người khác ngoài (các) tác giả phải được tôn trọng. Việc trích dẫn có ghi nguồn được phép. Để sao chép theo cách khác, hoặc tái xuất bản, để đăng trên máy chủ hoặc phân phối lại cho danh sách, cần có sự cho phép trước cụ thể và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.
WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA
©2023 Bản quyền thuộc về chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho ACM.
ACM ISBN 978-1-4503-9416-1/23/04. . . $15.00
https://doi.org/10.1145/3543507.35834461 GIỚI THIỆU
Học sâu đóng vai trò thiết yếu trong việc thiết kế các hệ thống gợi ý hiện đại ở quy mô web trong các ứng dụng thực tế. Ví dụ, các công cụ tìm kiếm và mạng xã hội được sử dụng rộng rãi nhất [5,17] khai thác các hệ thống gợi ý (hoặc hệ thống xếp hạng) để tối ưu hóa Tỷ lệ Nhấp chuột (CTR) của các trang được cá nhân hóa [8,13,23]. Các mô hình học sâu dựa vào kỹ thuật kiến trúc mạng neural tinh tế.
Các hệ thống gợi ý dựa trên học sâu, đặc biệt là dự đoán CTR, mang theo một thiết kế kiến trúc mạng neural trên các đặc trưng đa phương thức. Trong thực tế, nhiều thách thức phát sinh. Các đặc trưng đa phương thức, như đặc trưng số thực, số nguyên và phân loại, đặt ra thách thức cụ thể trong mô hình hóa tương tác đặc trưng và tối ưu hóa mạng neural. Việc tìm một mô hình cơ sở tốt với các kiến trúc không đồng nhất gán các tiên nghiệm phù hợp trên các đặc trưng đa phương thức là các thực hành phổ biến trong các hệ thống gợi ý dựa trên học sâu [7,14,16,19,23,25,27,28]. Tuy nhiên, các cách tiếp cận này vẫn dựa vào nỗ lực thủ công đáng kể và gặp phải các hạn chế, như không gian thiết kế hẹp và các thử nghiệm thực nghiệm không đủ bị giới hạn bởi các nguồn lực có sẵn. Kết quả là, những hạn chế này tăng thêm khó khăn trong việc thiết kế một bộ trích xuất đặc trưng tốt.
Sự phát triển của Học Máy Tự động (AutoML), đặc biệt là Tìm kiếm Kiến trúc Mạng Neural (NAS) [4,21,37,39], trong lĩnh vực thị giác, soi sáng việc tối ưu hóa các mô hình của hệ thống gợi ý.
NAS Chia sẻ Trọng số (WS-NAS) [3,4,21] được áp dụng phổ biến trong lĩnh vực thị giác để giải quyết việc thiết kế các mô hình thị giác hiệu quả.
Tuy nhiên, việc áp dụng NAS chia sẻ trọng số cho lĩnh vực gợi ý khó khăn hơn nhiều so với lĩnh vực thị giác do tính đa phương thức trong dữ liệu và tính không đồng nhất trong kiến trúc.
Ví dụ, (1) trong thị giác, đầu vào của các khối xây dựng trong [4,33] là các tensor 3D đồng nhất, nhưng hệ thống gợi ý nhận các đặc trưng đa phương thức tạo ra tensor 2D và 3D. (2) Các mô hình thị giác đơn giản chỉ xếp chồng các khối xây dựng giống nhau, và do đó NAS tối tân trong thị giác hội tụ để đơn giản tìm kiếm cấu hình kích thước thay vì các motif kiến trúc, như chiều rộng kênh, kích thước kernel và lặp lại lớp [4,38]. Tuy nhiên, các mô hình gợi ý không đồng nhất với mỗi giai đoạn của mô hình sử dụng một khối xây dựng hoàn toàn arXiv:2207.07187v2 [cs.IR] 12 Feb 2023

--- TRANG 2 ---
WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, và Wei Wen
Bảng 1: So sánh NASRec với các phương pháp NAS hiện có cho hệ thống gợi ý.
PhươngphápCác Toán tử Xây dựng?Kết nối Dày đặc?Tìm kiếm Kiến trúc Đầy đủ?Criteo Log Loss Chi phí Huấn luyện
DNAS [18] FC, Dot-Product ✓ 0.4442 Một siêu mạng
PROFIT [11] FC, FM 0.4427 Một siêu mạng
AutoCTR [30] FC, Dot-Product, FM, EFC ✓ ✓ 0.4413 Nhiều mô hình
NASRecFC, Gating, Sum, Attention,✓ ✓ 0.4399 Một siêu mạngDot-Product, FM, EFC
khác nhau [7,14,19,23]. (3) Các mô hình thị giác chủ yếu sử dụng toán tử tích chập làm khối xây dựng chính trong khi hệ thống gợi ý được xây dựng trên các toán tử không đồng nhất, chẳng hạn như lớp Fully-Connected, Gating, Sum, Dot-Product, Multi-Head Attention, Factorization Machine, v.v.
Do các thách thức nêu trên, nghiên cứu về NAS trong hệ thống gợi ý rất hạn chế. Ví dụ, không gian tìm kiếm trong AutoCTR [30] và DNAS [18] theo nguyên tắc thiết kế của DLRM được chế tạo thủ công [23] và chúng chỉ bao gồm lớp Fully-Connected và Dot-Product làm toán tử có thể tìm kiếm. Chúng cũng phụ thuộc nhiều vào các toán tử được chế tạo thủ công, như Factorization Machine [30] hoặc mô-đun tương tác đặc trưng [11] trong không gian tìm kiếm để tăng tính không đồng nhất của kiến trúc. Hơn nữa, các công trình hiện có gặp phải chi phí tính toán khổng lồ [30] hoặc tối ưu hóa hai cấp đầy thách thức [18], và do đó chúng chỉ sử dụng không gian thiết kế hẹp (đôi khi với các tiên nghiệm con người mạnh [11]) để chế tạo kiến trúc, không khuyến khích các tương tác đặc trưng đa dạng và làm hại chất lượng của các mô hình được khám phá.
Trong bài báo này, chúng tôi do đó đề xuất NASRec, một mô hình mới để hoàn toàn kích hoạt NAS choHệ thống Gợi ý thông qua Tìm kiếm Kiến trúc Mạng Neural Chia sẻ Trọng số (WS-NAS) dưới tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc. Bảng 1 tóm tắt sự tiến bộ của NASRec so với các cách tiếp cận NAS khác. Chúng tôi đạt được điều này bằng cách đầu tiên xây dựng một siêu mạng kết hợp nhiều toán tử không đồng nhất hơn so với các công trình trước đây, bao gồm lớp Fully-Connected (FC), Gating, Sum, Dot-Product, Self-Attention và lớp Embedded Fully-Connected (EFC). Trong siêu mạng, chúng tôi kết nối dày đặc một chuỗi các khối, mỗi khối bao gồm tất cả các toán tử làm tùy chọn.
Vì bất kỳ khối nào cũng có thể nhận bất kỳ nhúng đặc trưng thô nào và tensor trung gian bằng kết nối dày đặc, siêu mạng không bị giới hạn bởi bất kỳ phương thức dữ liệu cụ thể nào. Thiết kế siêu mạng như vậy giảm thiểu việc mã hóa các tiên nghiệm con người bằng cách giới thiệu "Không gian Tìm kiếm NASRec", hỗ trợ bản chất của tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc trong hệ gợi ý, và bao phủ các mô hình vượt ra ngoài các mô hình gợi ý phổ biến như Wide & Deep [7], DeepFM [14], DLRM [23], AutoCTR [30], DNAS [18] và PROFIT [11].
Siêu mạng về bản chất tạo thành một không gian tìm kiếm. Chúng tôi có được một mô hình bằng cách đặt bằng không một số toán tử và kết nối trong siêu mạng, nghĩa là, một mạng con của siêu mạng tương đương với một mô hình. Vì tất cả các mạng con chia sẻ trọng số từ cùng một siêu mạng, nó được gọi là NAS Chia sẻ Trọng số. Để hiệu quả tìm kiếm các mô hình/mạng con trong không gian tìm kiếm NASRec, chúng tôi phát triển các cách tiếp cận một lần [4,38] cho lĩnh vực gợi ý. Chúng tôi đề xuất lấy mẫu một toán tử-bất kỳ kết nối để tách riêng việc lựa chọn toán tử và tăng độ phủ kết nối, các khối tương tác cân bằng toán tử để công bằng huấn luyện các mạng con trong siêu mạng, và tinh chỉnh sau huấn luyện để giảm sự đồng thích ứng trọng số. Những cách tiếp cận này cho phép hiệu quả huấn luyện tốt hơn và xếp hạng các mô hình mạng con trong siêu mạng, dẫn đến việc giảm log loss ∼0.001 của các mô hình được tìm kiếm trên không gian tìm kiếm NASRec đầy đủ.
Chúng tôi đánh giá các mô hình được chế tạo bằng NAS của chúng tôi, NASRecNets trên ba điểm chuẩn CTR phổ biến và chứng minh cải thiện đáng kể so với cả các mô hình được chế tạo thủ công và các mô hình được chế tạo bằng NAS.
Đáng chú ý, NASRecNet đẩy mạnh tình trạng tối tân với việc giảm log loss ∼0.001, ∼0.003 trên Criteo và KDD Cup 2012, tương ứng. Trên Avazu, NASRec vượt trội so với PROFIT tối tân [11] với cải thiện AUC ∼0.002 và log loss tương đương, trong khi vượt trội hơn PROFIT [11] trên Criteo với việc giảm log loss ∼0.003.
NASRec chỉ cần huấn luyện một siêu mạng đơn lẻ nhờ cơ chế chia sẻ trọng số hiệu quả, và do đó giảm đáng kể chi phí tìm kiếm. Chúng tôi tóm tắt các đóng góp chính dưới đây.
•Chúng tôi đề xuất NASRec, một mô hình mới để mở rộng quy mô mô hình hóa tự động của hệ thống gợi ý. NASRec thiết lập một siêu mạng linh hoạt (không gian tìm kiếm) với các tiên nghiệm con người tối thiểu, vượt qua các thách thức về phương thức dữ liệu và tính không đồng nhất kiến trúc trong lĩnh vực gợi ý.
•Chúng tôi phát triển NAS chia sẻ trọng số cho lĩnh vực gợi ý bằng cách giới thiệu lấy mẫu một toán tử-bất kỳ kết nối, các mô-đun tương tác cân bằng toán tử và tinh chỉnh sau huấn luyện.
•Các mô hình được chế tạo của chúng tôi, NASRecNet, vượt trội hơn cả các mô hình được chế tạo thủ công và các mô hình được chế tạo bằng NAS với chi phí tìm kiếm nhỏ hơn.
2 CÔNG TRÌNH LIÊN QUAN
Hệ thống gợi ý dựa trên học sâu. Hệ thống gợi ý dựa trên máy như dự đoán Tỷ lệ Nhấp chuột (CTR) đã được nghiên cứu kỹ lưỡng trong nhiều cách tiếp cận khác nhau, như Hồi quy Logistic [27] và Cây Quyết định Gradient-Boosting [16]. Các cách tiếp cận gần đây hơn nghiên cứu tương tác dựa trên học sâu của các loại đặc trưng khác nhau thông qua Mạng Neural Wide & Deep [7], DeepCrossing [28], Factorization Machines [14,19], Dot-Product [23] và cơ chế gating [34,35]. Một hướng nghiên cứu khác tìm kiếm các tương tác đặc trưng hiệu quả, như nhân đặc trưng [36] và thưa thớt hóa [9] để xây dựng hệ thống gợi ý nhẹ.
Tuy nhiên, những công trình này hoạt động với chi phí của những nỗ lực thủ công to lớn và gặp phải hiệu suất dưới tối ưu và các lựa chọn thiết kế bị ràng buộc do những hạn chế trong cung cấp nguồn lực. Công trình của chúng tôi thiết lập một mô hình mới về việc học các mô hình gợi ý hiệu quả bằng cách chế tạo một "không gian tìm kiếm NASRec" có thể mở rộng kết hợp tất cả các motif thiết kế phổ biến trong các công trình hiện có. Không gian tìm kiếm NASRec mới hỗ trợ một loạt rộng các lựa chọn thiết kế và cho phép tối ưu hóa có thể mở rộng để chế tạo các mô hình gợi ý với các yêu cầu khác nhau.
Tìm kiếm Kiến trúc Mạng Neural. Tìm kiếm Kiến trúc Mạng Neural tự động hóa việc thiết kế Mạng Neural Sâu trong các ứng dụng khác nhau:

--- TRANG 3 ---
NASRec: Chia sẻ Trọng số trong Tìm kiếm Kiến trúc Mạng Neural cho Hệ thống Gợi ý WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA
sự phổ biến của Tìm kiếm Kiến trúc Mạng Neural liên tục tăng trưởng trong việc phát triển Thị giác Máy tính [4,21,37,39], Xử lý Ngôn ngữ Tự nhiên [29,33] và Hệ thống Gợi ý [11,18,30]. Gần đây, NAS Chia sẻ Trọng số (WS-NAS) [4,33] thu hút sự chú ý của các nhà nghiên cứu: nó huấn luyện một siêu mạng đại diện cho toàn bộ không gian tìm kiếm trực tiếp trên các nhiệm vụ mục tiêu, và hiệu quả đánh giá các mạng con (tức là, kiến trúc con của siêu mạng) với trọng số siêu mạng được chia sẻ. Tuy nhiên, thực hiện WS-NAS trên hệ thống gợi ý là thách thức vì hệ thống gợi ý được phát triển trên các kiến trúc không đồng nhất dành riêng cho việc tương tác dữ liệu đa phương thức, do đó đòi hỏi các không gian tìm kiếm linh hoạt hơn và các thuật toán huấn luyện siêu mạng hiệu quả. Những thách thức đó gây ra vấn đề đồng thích ứng [2] và vấn đề mất cân bằng toán tử [20] trong WS-NAS, cung cấp tương quan xếp hạng thấp hơn để phân biệt các mô hình. NASRec giải quyết chúng bằng cách đề xuất lấy mẫu một toán tử-bất kỳ kết nối, các mô-đun tương tác cân bằng toán tử và tinh chỉnh sau huấn luyện.
3 KHÔNG GIAN NASREC PHÂN CẤP CHO
HỆ THỐNG GỢI Ý
Để hỗ trợ tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc trong hệ thống gợi ý, tính linh hoạt của không gian tìm kiếm là chìa khóa. Chúng tôi thiết lập một mô hình mới không có tiên nghiệm con người bằng cách giới thiệu không gian tìm kiếm NASRec, một thiết kế không gian tìm kiếm phân cấp kết hợp các toán tử xây dựng không đồng nhất và kết nối dày đặc, xem Hình 1. Quá trình thủ công chính trong việc thiết kế không gian tìm kiếm chỉ đơn giản là thu thập các toán tử phổ biến được sử dụng trong các cách tiếp cận hiện có [14,19,23,30,34,35]. Ngoài ra, chúng tôi còn kết hợp Transformer Encoder phổ biến [32] vào không gian tìm kiếm NASRec để có tính linh hoạt tốt hơn và tiềm năng cao hơn trong các kiến trúc được tìm kiếm, nhờ sự thống trị của nó trong các ứng dụng như ViT [10] cho nhận dạng hình ảnh, Transformer [32] cho xử lý ngôn ngữ tự nhiên và việc khám phá mới nổi của nó trong hệ thống gợi ý [6,12].
Tiếp theo, chúng tôi trình bày không gian tìm kiếm NASRec.
3.1 Không gian Tìm kiếm NASRec
Trong hệ thống gợi ý, chúng tôi định nghĩa một đầu vào dày đặc là 𝑋𝑑∈R𝐵×𝑑𝑖𝑚𝑑 là một tensor 2D từ các đặc trưng dày đặc thô hoặc được tạo ra bởi các toán tử, như FC, Gating, Sum và Dot-Product. Một đầu vào thưa thớt 𝑋𝑠∈R𝐵×𝑁𝑠×𝑑𝑖𝑚𝑠 là một tensor 3D của các nhúng thưa thớt được tạo ra bởi các đặc trưng thưa thớt/phân loại thô hoặc bởi các toán tử như EFC và self-attention. Tương tự, một đầu ra dày đặc hoặc thưa thớt (tức là, 𝑌𝑑 hoặc 𝑌𝑠) được định nghĩa tương ứng là một tensor 2D hoặc 3D được tạo ra thông qua các khối xây dựng/toán tử tương ứng. Trong NASRec, tất cả các đầu vào và đầu ra thưa thớt chia sẻ cùng 𝑑𝑖𝑚𝑠, bằng với chiều của các nhúng thưa thớt thô. Tương ứng, chúng tôi định nghĩa một toán tử dày đặc (thưa thớt) là một toán tử tạo ra một đầu ra dày đặc (thưa thớt). Trong NASRec, các toán tử dày đặc bao gồm FC, Gating, Sum và Dot-Product tạo thành "nhánh dày đặc" (được đánh dấu màu xanh), và các toán tử thưa thớt bao gồm EFC và self-attention, tạo thành "nhánh thưa thớt" (được đánh dấu màu đỏ).
Một kiến trúc ứng viên trong không gian tìm kiếm NASRec là một chồng 𝑁 khối lựa chọn, theo sau là một lớp FC cuối cùng để tính logit. Mỗi khối lựa chọn thừa nhận một số lượng tùy ý các đầu vào đa phương thức, mỗi cái là 𝑋=(𝑋𝑑,𝑋𝑠) từ một khối trước đó hoặc đầu vào thô, và tạo ra một đầu ra đa phương thức 𝑌=(𝑌𝑑,𝑌𝑠) của cả tensor dày đặc 𝑌𝑑 và tensor thưa thớt 𝑌𝑠 thông qua các toán tử xây dựng bên trong.
Trong mỗi khối lựa chọn, chúng ta có thể lấy mẫu các toán tử để tìm kiếm.
Chúng tôi xây dựng một siêu mạng để đại diện cho không gian tìm kiếm NASRec, xem Hình 1. Siêu mạng bao gồm tất cả các mô hình ứng viên/mạng con có thể và thực hiện chia sẻ trọng số giữa các mạng con để đồng thời huấn luyện tất cả chúng. Chúng tôi chính thức định nghĩa siêu mạng NASRec S là một bộ ba của các kết nối C, toán tử O và chiều D như sau: S=(C,D,O) trên tất cả 𝑁 khối lựa chọn. Cụ thể, các toán tử: O=[𝑂(1),...,𝑂(𝑁)] liệt kê tập hợp các toán tử xây dựng từ khối lựa chọn 1 đến 𝑁. Các kết nối: C=[𝐶(1),...,𝐶(𝑁)] chứa kết nối <𝑖,𝑗> giữa khối lựa chọn 𝑖 và khối lựa chọn 𝑗. Chiều: D=[𝐷(1),...,𝐷(𝑁)] chứa các cài đặt chiều từ khối lựa chọn 1 đến 𝑁.
Một mạng con 𝑆𝑠𝑎𝑚𝑝𝑙𝑒 =(O𝑠𝑎𝑚𝑝𝑙𝑒,C𝑠𝑎𝑚𝑝𝑙𝑒,D𝑠𝑎𝑚𝑝𝑙𝑒) trong siêu mạng S đại diện cho một mô hình trong không gian tìm kiếm NASRec. Một khối sử dụng phép cộng để tổng hợp các đầu ra của các toán tử được lấy mẫu trong mỗi nhánh (tức là "nhánh dày đặc" hoặc "nhánh thưa thớt"). Khi các chiều đầu ra của toán tử không khớp, chúng tôi áp dụng việc che zero để che các chiều bổ sung. Một khối sử dụng phép nối 𝐶𝑜𝑛𝑐𝑎𝑡 để tổng hợp các đầu ra từ các kết nối được lấy mẫu. Cho một mạng con được lấy mẫu 𝑆𝑠𝑎𝑚𝑝𝑙𝑒, đầu vào 𝑋(𝑁) đến khối lựa chọn 𝑁 được tính như sau cho một danh sách các đầu ra khối trước đó {𝑌(1),...,𝑌(𝑁−1)} và các kết nối được lấy mẫu 𝐶(𝑁)𝑠𝑎𝑚𝑝𝑙𝑒:
𝑋(𝑁)𝑑=𝐶𝑜𝑛𝑐𝑎𝑡𝑁−1𝑖=1[𝑌(𝑖)𝑑·1<𝑖,𝑁>∈𝐶(𝑁)𝑠𝑎𝑚𝑝𝑙𝑒], (1)
𝑋(𝑁)𝑠=𝐶𝑜𝑛𝑐𝑎𝑡𝑁−1𝑖=1[𝑌(𝑖)𝑠·1<𝑖,𝑁>∈𝐶(𝑁)𝑠𝑎𝑚𝑝𝑙𝑒]. (2)
Ở đây, 1𝑏 là 1 khi 𝑏 đúng nếu không thì là 0.
Một toán tử xây dựng 𝑜∈𝑂(𝑁)𝑠𝑎𝑚𝑝𝑙𝑒 biến đổi đầu vào được nối 𝑋(𝑁) thành một đầu ra trung gian với một chiều được lấy mẫu 𝐷(𝑁)𝑠𝑎𝑚𝑝𝑙𝑒. Điều này được thực hiện bởi một hàm mask được áp dụng trên chiều cuối cùng cho đầu ra dày đặc và chiều giữa cho đầu ra thưa thớt. Ví dụ, một đầu ra dày đặc 𝑌(𝑁)𝑑 được thu được như sau:
𝑌(𝑁)𝑑=∑︁𝑜∈O1𝑜∈O(𝑁)𝑠𝑎𝑚𝑝𝑙𝑒·𝑀𝑎𝑠𝑘(𝑜(𝑋(𝑁)𝑑),𝐷(𝑁)𝑠𝑎𝑚𝑝𝑙𝑒,𝑜). (3)
trong đó
𝑀𝑎𝑠𝑘(𝑉,𝑑)=(𝑉:,𝑖,nếu 𝑖<𝑑0,Nếu không.. (4)
Tiếp theo, chúng tôi làm rõ tập hợp các toán tử xây dựng như sau:
•Lớp Fully-Connected (FC). Lớp Fully-Connected là xương sống của các mô hình DNN cho hệ thống gợi ý [7] trích xuất các biểu diễn dày đặc. FC được áp dụng trên các đầu vào dày đặc 2D, và theo sau bởi một kích hoạt ReLU.
•Lớp Sigmoid Gating (SG). Chúng tôi theo trực giác trong [6,35] và sử dụng một toán tử xây dựng dày đặc, Sigmoid Gating, để tăng cường tiềm năng của không gian tìm kiếm. Cho hai đầu vào dày đặc 𝑋𝑑1∈R𝐵×𝑑𝑖𝑚𝑑1 và 𝑋𝑑2∈R𝐵×𝑑𝑖𝑚𝑑2, Sigmoid Gating tương tác hai đầu vào này như sau: 𝑆𝐺(𝑋𝑑1,𝑋𝑑2)=𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝐹𝐶(𝑋𝑑1))∗𝑋𝑑2. Nếu chiều của hai đầu vào dày đặc không khớp, việc đệm zero được áp dụng trên đầu vào có chiều thấp hơn.
•Lớp Sum. Toán tử xây dựng dày đặc này cộng hai đầu vào dày đặc: 𝑋𝑑1∈R𝐵×𝑑𝑖𝑚𝑑1,𝑋𝑑2∈R𝐵×𝑑𝑖𝑚𝑑2 và gộp hai đặc trưng từ các cấp độ khác nhau của các mô hình hệ thống gợi ý bằng cách đơn giản thực hiện 𝑆𝑢𝑚(𝑋𝑑1,𝑋𝑑2)=𝑋𝑑1+𝑋𝑑2. Tương tự như Sigmoid Gating, việc đệm zero được áp dụng trên đầu vào có chiều thấp hơn.

--- TRANG 4 ---
WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, và Wei Wen
Khối &
Khối 1Khối 2Khối N…FC
Đầu vàoLogits
+
+Phép cộng &Nối Kết nối Cố định Kết nối Có thể Tìm kiếm+
Sparse Op Dense OpFC SG Sum DP EFC Attn
Đầu vào Dày đặc 2D Đầu vào Thưa thớt 3DĐầu ra Dày đặc 2D Đầu ra Thưa thớt 3D
FCReshape +
FM
Hình 1: Tổng quan về không gian tìm kiếm NASRec. Không gian tìm kiếm NASRec cho phép tìm kiếm kiến trúc đầy đủ trên các toán tử xây dựng và kết nối dày đặc. Ở đây, các khối "xanh" tạo ra đầu ra dày đặc, và các khối "đỏ" tạo ra đầu ra thưa thớt.
•Lớp Dot-Product (DP). Chúng tôi tận dụng Dot-Product để nắm bắt các tương tác giữa các đầu vào đa phương thức thông qua các tích vô hướng theo cặp. Dot-Product có thể nhận đầu vào dày đặc và/hoặc thưa thớt, và tạo ra đầu ra dày đặc. Những đầu vào thưa thớt này, sau khi được gửi đến "nhánh dày đặc", có thể sau đó tận dụng các toán tử dày đặc để học các biểu diễn và tương tác tốt hơn. Cho một đầu vào dày đặc 𝑋𝑑∈R𝐵×𝑑𝑖𝑚𝑑 và một đầu vào thưa thớt 𝑋𝑠∈R𝐵×𝑁𝑐×𝑑𝑖𝑚𝑠, một Dot-Product đầu tiên nối chúng như 𝑋=𝐶𝑜𝑛𝑐𝑎𝑡[𝑋𝑑,𝑋𝑠], và sau đó thực hiện các tích vô hướng theo cặp: 𝐷𝑃(𝑋𝑑,𝑋𝑠)=𝑇𝑟𝑖𝑢(𝑋𝑋𝑇). 𝑑𝑖𝑚𝑑 đầu tiên được chiếu đến 𝑑𝑖𝑚𝑠 nếu chúng không khớp.
•Lớp Embedded Fully-Connected (EFC). Một lớp EFC là một toán tử xây dựng thưa thớt áp dụng FC dọc theo chiều giữa. Cụ thể, một EFC với trọng số 𝑊∈R𝑁𝑖𝑛×𝑁𝑜𝑢𝑡 biến đổi một đầu vào 𝑋𝑠∈R𝐵×𝑁𝑖𝑛×𝑑𝑖𝑚𝑠 thành 𝑌𝑠∈R𝐵×𝑁𝑜𝑢𝑡×𝑑𝑖𝑚𝑠
•Lớp Attention (Attn). Lớp Attention là một toán tử xây dựng thưa thớt sử dụng cơ chế Multi-Head Attention (MHA) để học trọng số của các đầu vào thưa thớt và khai thác tốt hơn tương tác của chúng trong hệ thống gợi ý. Ở đây, chúng tôi áp dụng Transformer Encoder trên một đầu vào thưa thớt cho trước 𝑋𝑠∈R𝐵×𝑁𝑠×𝑑𝑖𝑚𝑠, với các truy vấn, khóa và giá trị giống hệt nhau.
Chúng tôi quan sát thấy rằng tập hợp các toán tử xây dựng nêu trên cung cấp cơ hội cho các đầu vào thưa thớt biến đổi thành "nhánh dày đặc". Tuy nhiên, những toán tử này không cho phép biến đổi các đầu vào dày đặc hướng về "nhánh thưa thớt". Để giải quyết hạn chế này, chúng tôi giới thiệu "bộ gộp dày đặc-thưa thớt" cho phép các đầu ra dày đặc/thưa thớt tùy chọn gộp vào "nhánh thưa thớt/dày đặc". Bộ gộp dày đặc-thưa thớt chứa hai thành phần chính.
•Bộ gộp "Dày đặc-thành-thưa thớt". Bộ gộp này đầu tiên chiếu các đầu ra dày đặc 𝑋𝑑 sử dụng một lớp FC, sau đó sử dụng một lớp reshape để reshape phép chiếu thành một tensor thưa thớt 3D. Tensor 3D được reshape được gộp vào đầu ra thưa thớt thông qua phép nối.
•Bộ gộp "Thưa thớt-thành-dày đặc". Bộ gộp này sử dụng một Factorization Machine (FM) [14] để chuyển đổi đầu ra thưa thớt thành một biểu diễn dày đặc, sau đó cộng biểu diễn dày đặc vào đầu ra dày đặc.
Ngoài các lựa chọn phong phú của các toán tử xây dựng và bộ gộp, mỗi khối lựa chọn cũng có thể nhận đầu vào từ bất kỳ khối lựa chọn trước đó nào, và các đặc trưng đầu vào thô. Điều này liên quan đến việc khám phá bất kỳ kết nối nào giữa các khối lựa chọn và đầu vào thô, mở rộng tính không đồng nhất dây nối để tìm kiếm.3.2 Các Thành phần Tìm kiếm
Trong không gian tìm kiếm NASRec, chúng tôi tìm kiếm kết nối, chiều toán tử và các toán tử xây dựng trong mỗi khối lựa chọn. Chúng tôi minh họa ba thành phần tìm kiếm chính như sau:
•Kết nối. Chúng tôi không đặt hạn chế về số lượng kết nối mà một khối lựa chọn có thể nhận: mỗi khối có thể chọn đầu vào từ một số lượng tùy ý các khối trước đó và đầu vào thô. Cụ thể, khối lựa chọn thứ n có thể kết nối với bất kỳ 𝑛−1 khối lựa chọn trước đó nào và các đặc trưng dày đặc (thưa thớt) thô. Các đầu ra từ tất cả các khối trước đó được nối làm đầu vào cho các khối xây dựng dày đặc (thưa thớt). Chúng tôi riêng biệt nối các đầu ra dày đặc (thưa thớt) từ các khối trước đó.
•Chiều. Trong một khối lựa chọn, các toán tử khác nhau có thể tạo ra các chiều tensor khác nhau. Trong NASRec, chúng tôi đặt kích thước đầu ra của FC và EFC lần lượt là 𝑑𝑖𝑚𝑑 và 𝑁𝑠; và các đầu ra toán tử khác trong nhánh dày đặc (thưa thớt) được chiếu tuyến tính đến 𝑑𝑖𝑚𝑑 (𝑁𝑠). Điều này đảm bảo các đầu ra toán tử trong mỗi nhánh có cùng chiều và có thể cộng lại với nhau. Điều này cũng cho các chiều tối đa 𝑑𝑖𝑚𝑑 và 𝑁𝑠 cho đầu ra dày đặc 𝑌𝑑∈R𝐵×𝑑𝑖𝑚𝑑 và đầu ra thưa thớt 𝑌𝑠∈R𝐵×𝑁𝑠×𝑑𝑖𝑚𝑠. Cho một đầu ra dày đặc hoặc thưa thớt, một mask trong Eq. 4 đặt bằng không các chiều bổ sung, cho phép lựa chọn linh hoạt các chiều của các toán tử xây dựng.
•Toán tử. Mỗi khối có thể chọn ít nhất một toán tử xây dựng dày đặc (thưa thớt) để biến đổi đầu vào thành đầu ra dày đặc (thưa thớt). Mỗi khối nên duy trì ít nhất một toán tử trong nhánh dày đặc (thưa thớt) để đảm bảo luồng thông tin từ đầu vào đến logit. Chúng tôi độc lập lấy mẫu các toán tử xây dựng trong nhánh dày đặc (thưa thớt) để tạo thành một kiến trúc ứng viên hợp lệ. Ngoài ra, chúng tôi độc lập lấy mẫu các bộ gộp dày đặc-thưa thớt để cho phép tương tác dày đặc-thành-thưa thớt tùy chọn.
Chúng tôi chế tạo hai không gian tìm kiếm NASRec làm ví dụ để chứng minh sức mạnh của không gian tìm kiếm NASRec.
•NASRec-Small. Chúng tôi giới hạn lựa chọn toán tử trong mỗi khối là FC, EFC và Dot-Product, và cho phép bất kỳ kết nối nào giữa các khối. Điều này cung cấp quy mô không gian tìm kiếm tương tự như AutoCTR [30].
•NASRec-Full. Chúng tôi kích hoạt tất cả các toán tử xây dựng, bộ gộp và kết nối để xây dựng một không gian tìm kiếm tích cực để khám phá với các tiên nghiệm con người tối thiểu. Dưới ràng buộc rằng ít nhất một toán tử phải được lấy mẫu trong cả nhánh dày đặc và thưa thớt, kích thước không gian tìm kiếm NASRec-Full là 15𝑁× của NASRec-Small, trong đó 𝑁

--- TRANG 5 ---
NASRec: Chia sẻ Trọng số trong Tìm kiếm Kiến trúc Mạng Neural cho Hệ thống Gợi ý WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA
Bất kỳ toán tử
Bất kỳ kết nối
Đầu vào Thô
Khối 1
Khối 2
Khối 3
…Toán tử đơn
Kết nối đơn
Đầu vào Thô
Khối 1Khối 2
Khối 3
…Toán tử đơn
Bất kỳ kết nối
Đầu vào Thô
Khối 1Khối 2
Khối 3
…
Toán tử Dày đặc Được lấy mẫu Toán tử Thưa thớt Được lấy mẫu Kết nối Được lấy mẫu
Hình 2: Chúng tôi đề xuất lấy mẫu đường dẫn Toán tử đơn-Bất kỳ kết nối bằng cách kết hợp các ưu điểm của hai chiến lược lấy mẫu đầu tiên. Ở đây, các kết nối và toán tử nét đứt biểu thị một đường dẫn được lấy mẫu trong siêu mạng.
là số khối lựa chọn. Không gian tìm kiếm đầy đủ này cực kỳ thử nghiệm khả năng của NASRec.
Sự kết hợp của tìm kiếm kết nối dày đặc đầy đủ và cấu hình chiều dày đặc/thưa thớt độc lập mang lại cho không gian tìm kiếm NASRec một tính chất lớn. NASRec-Full có 𝑁=7 khối, chứa tới 5×1033 kiến trúc với tính không đồng nhất mạnh. Với các tiên nghiệm con người tối thiểu và không gian tìm kiếm không bị ràng buộc như vậy, các phương pháp dựa trên mẫu brute-force có thể mất thời gian khổng lồ để tìm một mô hình tối tân.
4 TÌMKIẾM KIẾN TRÚC MẠNG NEURAL
CHIA SẺ TRỌNG SỐ CHO
HỆ THỐNG GỢI Ý
Một siêu mạng NASRec đồng thời phát triển các mô hình mạng con khác nhau trong không gian tìm kiếm NASRec, nhưng đặt ra thách thức cho hiệu quả huấn luyện và chất lượng xếp hạng do tính chất lớn của nó. Trong phần này, chúng tôi đầu tiên đề xuất một chiến lược lấy mẫu đường dẫn mới, lấy mẫu Toán tử đơn-Bất kỳ kết nối, tách biệt việc lấy mẫu toán tử với một hội tụ lấy mẫu kết nối tốt. Chúng tôi tiếp tục quan sát hiện tượng mất cân bằng toán tử được gây ra bởi một số toán tử có quá nhiều tham số, và giải quyết vấn đề này bằng tương tác cân bằng toán tử để cải thiện xếp hạng siêu mạng. Cuối cùng, chúng tôi sử dụng tinh chỉnh sau huấn luyện để giảm thiểu sự đồng thích ứng trọng số, và tiếp tục sử dụng tiến hóa có điều chỉnh để có được mạng con tốt nhất. Chúng tôi cũng cung cấp một tập hợp các hiểu biết hiệu quả khám phá các mô hình gợi ý tốt nhất.
4.1 Lấy mẫu Toán tử đơn-Bất kỳ Kết nối
Việc huấn luyện siêu mạng áp dụng một cách tiếp cận giống như drop-out. Tại mỗi mini-batch, chúng ta lấy mẫu và huấn luyện một mạng con. Trong quá trình huấn luyện, chúng ta huấn luyện rất nhiều mạng con dưới chia sẻ trọng số, với mục tiêu là các mạng con được huấn luyện tốt để dự đoán hiệu suất của các mô hình. Các chiến lược lấy mẫu quan trọng để đạt được mục tiêu. Chúng tôi khám phá ba chiến lược lấy mẫu đường dẫn được mô tả trong Hình 2 và khám phá ra lấy mẫu Toán tử đơn-Bất kỳ Kết nối là cách hiệu quả nhất:
•Chiến lược Toán tử đơn-Kết nối đơn. Chiến lược lấy mẫu đường dẫn này có gốc rễ trong Thị giác Máy tính [15]: nó đồng đều lấy mẫu một toán tử dày đặc đơn và một toán tử thưa thớt đơn trong mỗi khối lựa chọn, và đồng đều lấy mẫu một kết nối đơn làm đầu vào cho một khối. Chiến lược này hiệu quả vì, trung bình, chỉ một
Toán tử đơn-Kết nối đơn: Pearson=0.05, Kendall=0.02
Bất kỳ toán tử-Bất kỳ kết nối: Pearson=0.367, Kendall=0.280
Toán tử đơn-Bất kỳ kết nối: Pearson=0.457, Kendall=0.436Hình 3: Đánh giá xếp hạng của các chiến lược lấy mẫu đường dẫn khác nhau trên siêu mạng NASRec-Full. Chúng tôi đánh giá tất cả các hệ số xếp hạng trên 100 mạng con được lấy mẫu ngẫu nhiên trên Criteo.
mạng con nhỏ được huấn luyện tại một mini-batch, tuy nhiên, chiến lược này chỉ khuyến khích hình thành mô hình giống chuỗi mà không có các mẫu kết nối bổ sung. Việc thiếu độ phủ kết nối dẫn đến hội tụ chậm hơn, hiệu suất kém và xếp hạng mô hình không chính xác như chúng tôi sẽ chỉ ra.
•Chiến lược Bất kỳ toán tử-Bất kỳ kết nối. Chiến lược lấy mẫu này tăng độ phủ của các kiến trúc con của siêu mạng trong quá trình huấn luyện mạng con: nó đồng đều lấy mẫu một số lượng tùy ý các toán tử dày đặc và thưa thớt trong mỗi khối lựa chọn, và đồng đều lấy mẫu một số lượng tùy ý các kết nối để tổng hợp các đầu ra khối khác nhau. Tuy nhiên, hiệu quả huấn luyện kém khi huấn luyện các mạng con lớn được lấy mẫu. Quan trọng hơn, sự đồng thích ứng trọng số của nhiều toán tử trong một khối lựa chọn có thể ảnh hưởng đến việc đánh giá độc lập của các mạng con, và do đó cuối cùng dẫn đến chất lượng xếp hạng kém như chúng tôi sẽ chỉ ra.
•Toán tử đơn-Bất kỳ kết nối. Chúng tôi đề xuất chiến lược lấy mẫu đường dẫn này để kết hợp điểm mạnh từ hai chiến lược trên. Lấy mẫu Toán tử đơn-Bất kỳ kết nối lấy mẫu một toán tử dày đặc đơn và một toán tử thưa thớt đơn trong mỗi khối lựa chọn, và lấy mẫu một số lượng tùy ý các kết nối để tổng hợp các đầu ra từ các khối lựa chọn khác nhau. Hiểu biết chính của chiến lược này là tách biệt việc lấy mẫu các toán tử có tham số để tránh sự đồng thích ứng của trọng số, và cho phép lấy mẫu tùy ý các kết nối không có tham số để có được độ phủ tốt của không gian tìm kiếm NASRec.
So với lấy mẫu Bất kỳ toán tử-Bất kỳ kết nối, lấy mẫu Toán tử đơn-Bất kỳ kết nối đạt được hiệu quả huấn luyện cao hơn: số lượng toán tử được lấy mẫu giảm làm giảm chi phí huấn luyện lên đến 1.5×. Ngoài ra, lấy mẫu Toán tử đơn-Bất kỳ kết nối lấy mẫu mạng kích thước trung bình thường xuyên hơn. Những mạng kích thước trung bình này đạt được sự cân bằng tốt nhất giữa kích thước mô hình và hiệu suất như chúng tôi sẽ chỉ ra trong Bảng 5.
Chúng tôi đánh giá xếp hạng của các mạng con bằng WS-NAS trên Criteo và bằng 100 mạng được lấy mẫu ngẫu nhiên trong Hình 3. Ở đây, chúng tôi áp dụng thiết kế của các mô-đun tương tác cân bằng toán tử trong Phần 4.2 để tối đa hóa tiềm năng của mỗi chiến lược lấy mẫu đường dẫn. Trong hình, trục y là Log Loss của các mạng con, có trọng số được sao chép từ các kiến trúc tương ứng trong siêu mạng được huấn luyện.

--- TRANG 6 ---
WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, và Wei Wen
…
DP
FCEFC
Đặc trưng Thưa thớt
Đặc trưng Dày đặc~dim𝑑𝑑Tương tác[2d𝑖𝑖𝑖𝑖𝑑𝑑]Đầu vào𝑁𝑁𝑠𝑠[2d𝑖𝑖𝑖𝑖𝑑𝑑]Tham số
dimd2Tham số
dim𝑑𝑑Units Tương tác𝑁𝑁𝑠𝑠Đầu vào
Hình 4: Tương tác cân bằng toán tử chèn một lớp EFC đơn giản trước Dot-Product để đảm bảo tiêu thụ tham số tuyến tính và cân bằng các toán tử xây dựng.
Lấy mẫu Toán tử đơn-Bất kỳ kết nối đạt được ít nhất 0.09 Pearson's Rho cao hơn và 0.15 Kendall's Tau cao hơn so với các chiến lược lấy mẫu đường dẫn khác. Ngoài ra, chúng tôi quan sát thấy rằng lấy mẫu Toán tử đơn-Bất kỳ kết nối cho phép hội tụ tốt hơn của siêu mạng NASRec và các mạng con kế thừa trọng số từ siêu mạng đạt được log loss thấp hơn trong quá trình xác thực, dẫn đến khai thác tốt hơn hiệu suất thực của chúng để có chất lượng xếp hạng tốt hơn.
4.2 Các Mô-đun Tương tác Cân bằng Toán tử
Hệ thống gợi ý liên quan đến dữ liệu đa phương thức với số lượng đầu vào không xác định, ví dụ, một số lượng lớn đầu vào thưa thớt. Chúng tôi định nghĩa mất cân bằng toán tử là sự mất cân bằng về số lượng trọng số giữa các toán tử trong một khối. Trong NAS chia sẻ trọng số, mất cân bằng toán tử có thể gây ra vấn đề rằng việc huấn luyện siêu mạng có thể ưu tiên các toán tử có nhiều trọng số hơn. Điều này sẽ bù trừ các lợi ích do tương quan xếp hạng kém của các mạng con: hiệu suất mạng con trong siêu mạng có thể lệch khỏi hiệu suất thực của nó khi được huấn luyện từ đầu. Chúng tôi xác định rằng, trong NASRec của chúng tôi, vấn đề như vậy có liên quan mạnh đến toán tử Dot-Product, và cung cấp giải pháp giảm thiểu để giải quyết sự mất cân bằng toán tử như vậy.
Cho 𝑁𝑠 nhúng thưa thớt, một khối Dot-Product tạo ra 𝑁2𝑠/2 tương tác theo cặp như một hàm bậc hai trên số lượng nhúng thưa thớt. Như đã nêu chi tiết trong Phần 3.1, siêu mạng yêu cầu một lớp chiếu tuyến tính (tức là, FC) để khớp các chiều đầu ra của các toán tử trong mỗi khối lựa chọn. Thông thường đối với Dot-Product, điều này dẫn đến (𝑁2𝑠·𝑑𝑖𝑚𝑑/2) trọng số có thể huấn luyện bổ sung.
Tuy nhiên, việc tiêu thụ trọng số của lớp chiếu như vậy lớn cho một số lượng lớn nhúng thưa thớt. Ví dụ, cho 𝑁𝑠=448 và 𝑑𝑖𝑚𝑑=512 trong một siêu mạng NASRec 7 khối, lớp chiếu gây ra hơn 50M tham số trong siêu mạng NASRec, có quy mô tiêu thụ tham số tương tự với các lớp nhúng thưa thớt. Tham số hóa trọng số khổng lồ như vậy là một hàm bậc hai của số lượng đầu vào thưa thớt 𝑁𝑠, nhưng các toán tử xây dựng khác có ít trọng số hơn nhiều, chẳng hạn như số lượng trọng số có thể huấn luyện trong EFC là một hàm tuyến tính của số lượng đầu vào thưa thớt 𝑁𝑠. Kết quả là, việc tham số hóa quá mức trong Dot-Product dẫn đến tỷ lệ hội tụ tăng cho toán tử Dot-Product và do đó ưu tiên các mạng con tiêu thụ tham số với nồng độ cao các hoạt động Dot-Product như chúng tôi đã quan sát. Ngoài ra, việc bỏ qua các toán tử không đồng nhất khác ngoài Dot-Product cung cấp xếp hạng kém của các mạng con, dẫn đến hiệu suất dưới tối ưu trên hệ thống gợi ý.Bảng 2: Tương tác Cân bằng Toán tử giảm chi phí huấn luyện siêu mạng và cải thiện xếp hạng của các mạng con.
Loại Tương tác Chi phí Huấn luyện Pearson's𝜌Kendall's𝜏
DP Mất cân bằng 4 Giờ 0.31 0.32
DP Cân bằng 1.5 Giờ 0.46 0.43
Chúng tôi chèn một EFC đơn giản làm lớp chiếu trước Dot-Product để giảm thiểu việc tham số hóa quá mức như vậy, xem Hình 4. Trực giác của chúng tôi là chiếu số lượng nhúng thưa thớt trong Dot-Product đến [√︁2𝑑𝑖𝑚𝑑], sao cho toán tử Dot-Product sau đó tạo ra khoảng 𝑑𝑖𝑚𝑑 đầu ra mà sau đó yêu cầu một lớp chiếu tối thiểu để khớp chiều. Như vậy, toán tử Dot-Product tiêu thụ tối đa (𝑑𝑖𝑚2𝑑+𝑁𝑠[√︁2𝑑𝑖𝑚𝑑]) trọng số có thể huấn luyện và đảm bảo sự tăng trưởng tuyến tính của việc tiêu thụ tham số với số lượng EFC thưa thớt 𝑁𝑠. Do đó, chúng tôi cân bằng toán tử tương tác để cho phép tỷ lệ hội tụ tương tự hơn của tất cả các toán tử xây dựng. Bảng 2 phản ánh một cải thiện đáng kể về hiệu quả huấn luyện và chất lượng xếp hạng của siêu mạng NASRec-Full với chiến lược lấy mẫu đường dẫn Toán tử đơn-Bất kỳ kết nối.
4.3 Tinh chỉnh Sau huấn luyện
Mặc dù việc huấn luyện mạng con giống dropout cung cấp một cách tuyệt vời để giảm sự thích ứng của trọng số cho một mạng con cụ thể, việc dự đoán hiệu suất mạng con bằng siêu mạng có thể thất bại khi trọng số không nên chia sẻ giữa một số mạng con. Sau việc huấn luyện siêu mạng và trong quá trình đánh giá mạng con độc lập, chúng tôi thực hiện tinh chỉnh sau huấn luyện để thích ứng lại trọng số của nó trở lại mạng con cụ thể. Điều này có thể hiệu chỉnh lại các trọng số bị hỏng khi huấn luyện các mạng con khác trong quá trình huấn luyện siêu mạng. Trong thực tế, chúng tôi thấy rằng việc tinh chỉnh FC cuối cùng trên tập dữ liệu mục tiêu trong vài bước huấn luyện (ví dụ, 0.5K) là đủ tốt. Chỉ với chi phí tìm kiếm bổ sung tối thiểu, kỹ thuật tinh chỉnh sau huấn luyện mới này tăng cường xếp hạng của các mạng con bằng cách giải quyết vấn đề thích ứng trọng số cơ bản, và do đó cung cấp cơ hội tốt hơn để khám phá các mô hình tốt hơn cho hệ thống gợi ý.
Bảng 3 chứng minh sự cải thiện của tinh chỉnh sau huấn luyện trên các chiến lược lấy mẫu đường dẫn khác nhau. Đáng ngạc nhiên, tinh chỉnh sau huấn luyện đạt được cải thiện chất lượng xếp hạng tốt dưới chiến lược lấy mẫu Toán tử đơn-Kết nối đơn và Bất kỳ toán tử-Bất kỳ kết nối. Điều này là do các mạng con dưới những chiến lược này thường không hội tụ tốt trong siêu mạng: chúng hoặc gặp phải độ phủ siêu mạng kém, hoặc hội tụ kém được gây ra bởi sự đồng thích ứng. Quá trình tinh chỉnh giải phóng tiềm năng của chúng và tiếp cận hiệu suất thực của chúng trên tập dữ liệu mục tiêu.
Đáng chú ý, chiến lược lấy mẫu đường dẫn Toán tử đơn-Bất kỳ kết nối hợp tác tốt với tinh chỉnh sau huấn luyện, và đạt được tương quan xếp hạng Pearson's 𝜌 và Kendall's 𝜏 tối ưu toàn cục giữa các cách tiếp cận khác nhau, với ít nhất 0.14 Pearson's 𝜌
Bảng 3: Hiệu ứng của tinh chỉnh sau huấn luyện trên các chiến lược lấy mẫu đường dẫn khác nhau trên NASRec-Full. Chúng tôi chứng minh Pearson's𝜌 và Kendall's 𝜏 trên 100 mạng con ngẫu nhiên trên Criteo.
Chiến lược Lấy mẫu Đường dẫnKhông Tinh chỉnh Tinh chỉnh
Pearson's𝜌Kendall's𝜏Pearson's𝜌Kendall's𝜏
Bất kỳ toán tử-Bất kỳ kết nối 0.37 0.28 0.46 0.43
Toán tử đơn-Kết nối đơn 0.05 0.02 0.43 0.29
Toán tử đơn-Bất kỳ kết nối 0.46 0.43 0.57 0.43

--- TRANG 7 ---
NASRec: Chia sẻ Trọng số trong Tìm kiếm Kiến trúc Mạng Neural cho Hệ thống Gợi ý WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA
và cải thiện Kendall's 𝜏 trên không gian tìm kiếm NASRec-Full so với lấy mẫu Toán tử đơn-Kết nối đơn với tinh chỉnh.
4.4 Tìm kiếm Tiến hóa trên Các Mô hình Tốt nhất
Chúng tôi sử dụng tiến hóa có điều chỉnh [24] để có được mạng con con tốt nhất trong không gian tìm kiếm NASRec, bao gồm NASRec Small và NASRec-Full. Ở đây, chúng tôi đầu tiên giới thiệu một đột biến đơn của một kiểu gen phân cấp với chuỗi hành động sau đây trong một trong các khối lựa chọn:
•Lấy mẫu lại chiều của một toán tử xây dựng dày đặc.
•Lấy mẫu lại chiều của một toán tử xây dựng thưa thớt.
•Lấy mẫu lại một toán tử xây dựng dày đặc.
•Lấy mẫu lại một toán tử xây dựng thưa thớt.
•Lấy mẫu lại kết nối của nó với các khối lựa chọn khác.
•Lấy mẫu lại lựa chọn của bộ gộp dày đặc-thành-thưa thớt/thưa thớt-thành-dày đặc cho phép giao tiếp giữa các đầu ra dày đặc/thưa thớt.
5 THỰC NGHIỆM
Chúng tôi đầu tiên chỉ ra cấu hình chi tiết mà NASRec sử dụng trong quá trình tìm kiếm kiến trúc, lựa chọn mô hình và đánh giá cuối cùng. Sau đó, chúng tôi chứng minh các đánh giá thực nghiệm trên ba điểm chuẩn hệ thống gợi ý phổ biến cho dự đoán Tỷ lệ Nhấp chuột (CTR): Criteo1, Avazu2 và KDD Cup 20123. Cả ba tập dữ liệu đều được tiền xử lý theo cách tương tự như AutoCTR [30].
5.1 Cấu hình Tìm kiếm
Chúng tôi đầu tiên chứng minh cấu hình chi tiết của không gian tìm kiếm NASRec-Full như sau:
•Các Thành phần Tìm kiếm Kết nối. Chúng tôi sử dụng 𝑁=7 khối trong không gian tìm kiếm NASRec của chúng tôi. Điều này cho phép so sánh công bằng với các phương pháp NAS gần đây [30]. Tất cả các khối lựa chọn có thể tùy ý kết nối với các khối lựa chọn trước đó hoặc các đặc trưng thô.
•Các Thành phần Tìm kiếm Toán tử. Trong mỗi khối lựa chọn, không gian tìm kiếm của chúng tôi chứa 6 toán tử xây dựng riêng biệt, bao gồm 4 toán tử xây dựng dày đặc: FC, Gating, Sum, Dot-Product và 2 toán tử xây dựng thưa thớt riêng biệt: EFC và Attention. Tùy chọn bộ gộp dày đặc-thưa thớt được khám phá đầy đủ.
•Các Thành phần Tìm kiếm Chiều. Đối với mỗi toán tử xây dựng dày đặc, chiều đầu ra dày đặc có thể chọn từ {16, 32, 64, 128, 256, 512, 768, 1024}. Đối với mỗi toán tử xây dựng thưa thớt, chiều đầu ra thưa thớt có thể được chọn từ {16, 32, 48, 64}.
Trong NASRec-Small, chúng tôi sử dụng cùng các cài đặt ngoại trừ việc chúng tôi chỉ sử dụng 2 toán tử xây dựng dày đặc: FC, Dot-Product và 1 toán tử xây dựng thưa thớt: EFC. Sau đó, chúng tôi minh họa một số kỹ thuật về việc phát triển siêu mạng NASRec, bao gồm cấu hình nhúng, khởi động siêu mạng và cài đặt huấn luyện siêu mạng.
•Bảng Nhúng Có Giới hạn. Chúng tôi giới hạn kích thước bảng nhúng tối đa đến 0.5M trong quá trình huấn luyện siêu mạng để tăng hiệu quả tìm kiếm. Trong quá trình đánh giá cuối cùng, chúng tôi duy trì bảng nhúng đầy đủ để lấy hiệu suất tốt nhất, tức là tổng cộng 540M tham số trong DLRM [23] trên Criteo để đảm bảo so sánh công bằng.
•Khởi động Siêu mạng. Chúng tôi quan sát thấy rằng siêu mạng có thể sụp đổ ở các giai đoạn huấn luyện ban đầu do các đường dẫn được lấy mẫu khác nhau và các lớp nhúng chưa được khởi tạo. Để giảm thiểu sự sụp đổ ban đầu của siêu mạng, chúng tôi ngẫu nhiên lấy mẫu toàn bộ siêu mạng ở 1/5 đầu của các bước huấn luyện, với xác suất 𝑝 giảm tuyến tính từ 1 xuống 0. Điều này cung cấp khởi động chiều, khởi động toán tử [3] và khởi động kết nối cho siêu mạng với tác động tối thiểu đến chất lượng của các đường dẫn được lấy mẫu.
•Cài đặt Huấn luyện Siêu mạng. Chúng tôi chèn chuẩn hóa lớp [1] vào mỗi toán tử xây dựng để ổn định việc huấn luyện siêu mạng. Lựa chọn siêu tham số của chúng tôi mạnh mẽ trên các không gian tìm kiếm NASRec khác nhau và các điểm chuẩn hệ thống gợi ý. Chúng tôi huấn luyện siêu mạng chỉ trong 1 epoch với bộ tối ưu Adagrad, tỷ lệ học ban đầu là 0.12, một lịch trình tỷ lệ học cosine [22] trên các điểm chuẩn hệ thống gợi ý mục tiêu.
Cuối cùng, chúng tôi trình bày chi tiết về tiến hóa có điều chỉnh và các chiến lược lựa chọn mô hình trên các không gian tìm kiếm NASRec.
•Tiến hóa Có Điều chỉnh. Bất chấp kích thước lớn của NASRec-Full và NASRec-small, chúng tôi sử dụng một cấu hình hiệu quả của tiến hóa có điều chỉnh để tìm kiếm các mạng con tối ưu từ siêu mạng. Cụ thể, chúng tôi duy trì một quần thể gồm 128 kiến trúc và chạy tiến hóa có điều chỉnh trong 240 lần lặp. Trong mỗi lần lặp, chúng tôi đầu tiên chọn kiến trúc tốt nhất từ 64 kiến trúc được lấy mẫu từ quần thể làm kiến trúc cha mẹ, và tạo ra 8 kiến trúc con để cập nhật quần thể.
•Lựa chọn Mô hình. Chúng tôi tuân theo các giao thức đánh giá trong AutoCTR [30] và chia mỗi tập dữ liệu mục tiêu thành 3 tập: huấn luyện (80%), xác thực (10%) và kiểm tra (10%). Trong quá trình tìm kiếm kiến trúc mạng neural chia sẻ trọng số, chúng tôi huấn luyện siêu mạng trên tập huấn luyện và chọn 15 mạng con hàng đầu trên tập xác thực. Chúng tôi huấn luyện 15 mô hình hàng đầu từ đầu, và chọn mạng con tốt nhất làm kiến trúc cuối cùng, cụ thể là NASRecNet.
5.2 Kết quả Điểm chuẩn Hệ thống Gợi ý
Chúng tôi huấn luyện NASRecNet từ đầu trên ba điểm chuẩn hệ thống gợi ý cổ điển, và so sánh hiệu suất của các mô hình được chế tạo bởi NASRec trên ba điểm chuẩn hệ thống gợi ý chung. Trong Bảng 4, chúng tôi báo cáo kết quả đánh giá của các NASRecNets từ đầu đến cuối và một đường cơ sở tìm kiếm ngẫu nhiên ngẫu nhiên lấy mẫu và huấn luyện các mô hình trong không gian tìm kiếm NASRec của chúng tôi.
Hiệu suất Tối tân. Ngay cả trong một không gian tìm kiếm NASRec-Full tích cực lớn, NASRecNets đạt được hiệu suất phá kỷ lục so với các mô hình CTR được chế tạo thủ công [14,19,23] với các tiên nghiệm con người tối thiểu như được thể hiện trong Bảng 4. So với AutoInt [31], mô hình được chế tạo thủ công chế tạo các tương tác đặc trưng với những nỗ lực kỹ thuật tinh tế, NASRecNet đạt được giảm Log Loss ∼0.003 trên Criteo, giảm Log Loss ∼0.007 trên Avazu và giảm Log Loss ∼0.003 trên KDD Cup 2012, với chuyên môn và can thiệp con người tối thiểu.
Tiếp theo, chúng tôi so sánh NASRecNet với các mô hình được chế tạo bằng NAS gần đây hơn. So với AutoCTR [30], NASRecNet đạt được Log Loss và AUC tối tân (SOTA) trên cả ba điểm chuẩn hệ thống gợi ý. Với cùng quy mô không gian tìm kiếm như AutoCTR (tức là không gian tìm kiếm NASRec-Small), NASRecNet mang lại giảm Log Loss 0.001 trên Criteo, giảm Log Loss 0.005 trên Avazu và giảm Log Loss 0.003 trên KDD Cup 2012. So với DNAS [18] và PROFIT [11] chỉ tập trung vào cấu hình một phần của các kiến trúc, như kết nối dày đặc, NASRecNet

--- TRANG 8 ---
WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, và Wei Wen
Bảng 4: Hiệu suất của NASRec trên Các Nhiệm vụ Dự đoán CTR Chung.
PhươngphápCriteo Avazu KDD Cup 2012 Chi phí Tìm kiếm
Log Loss AUC Log Loss AUC Log Loss AUC (Ngày GPU)
Nghệ thuật Được chế tạo Thủ côngDLRM [23] 0.4436 0.8085 0.3814 0.7766 0.1523 0.8004 -
xDeepFM [19] 0.4418 0.8052 - - - - -
AutoInt+ [31] 0.4427 0.8090 0.3813 0.7772 0.1523 0.8002 -
DeepFM [14] 0.4432 0.8086 0.3816 0.7767 0.1529 0.7974 -
Nghệ thuật Được chế tạo bằng NASDNAS [18] 0.4442 - - - - - -
PROFIT [11] 0.4427 0.8095 0.3735 0.7883 - - ∼0.5
AutoCTR [30] 0.4413 0.8104 0.3800 0.7791 0.1520 0.8011 ∼0.75
Tìm kiếm Ngẫu nhiên @ NASRec-Small 0.4411 0.8105 0.3748 0.7885 0.1500 0.8123 1.0
Tìm kiếm Ngẫu nhiên @ NASRec-Full 0.4418 0.8098 0.3767 0.7853 0.1509 0.8071 1.0
NASRecNet @ NASRec-Small 0.4399 0.8118 0.3747 0.7887 0.1495 0.8135 ∼0.25
NASRecNet @ NASRec-Full 0.4408 0.8107 0.3737 0.7903 0.1491 0.8154 ∼0.3
đạt được ít nhất giảm Log Loss ∼0.002 trên Criteo, chứng minh tầm quan trọng của tìm kiếm kiến trúc đầy đủ trên hệ thống gợi ý.
Bằng cách mở rộng NASRec đến một không gian tìm kiếm NASRec-Full cực kỳ lớn, NASRecNet tiếp tục cải thiện kết quả trên Avazu và vượt trội hơn PROFIT với cải thiện AUC ∼0.002 với Log Loss tương đương, chứng minh thiết kế của NASRec-Full với tính chất lớn tích cực và các tiên nghiệm con người tối thiểu. Trên Criteo và KDD Cup 2012, NASRec duy trì lợi thế trong việc khám phá các mô hình CTR tối tân so với các phương pháp NAS hiện có [11, 18, 30].
Tìm kiếm Hiệu quả trong Không gian Tìm kiếm Đa dạng. Bất chấp một không gian tìm kiếm NASRec lớn hơn đặt ra nhiều thách thức hơn để khám phá đầy đủ, NASRec đạt được hiệu quả tìm kiếm ít nhất 1.7× so với các phương pháp NAS hiệu quả tối tân [11,30] với cải thiện Log Loss đáng kể trên cả ba điểm chuẩn. Điều này được quy công nhiều cho hiệu quả của NAS Chia sẻ Trọng số được áp dụng trên các toán tử không đồng nhất và dữ liệu đa phương thức.
Chúng tôi quan sát thấy rằng một không gian tìm kiếm NASRec-Small compact tạo ra các đường cơ sở tìm kiếm ngẫu nhiên mạnh, trong khi một không gian tìm kiếm NASRec-Full lớn hơn có đường cơ sở yếu hơn. Điều này là do với ngân sách tìm kiếm hạn chế, việc khám phá các mô hình đầy hứa hẹn trong một không gian tìm kiếm lớn khó khăn hơn. Tuy nhiên, WS-NAS có thể mở rộng giải quyết việc khám phá không gian tìm kiếm NASRec-Full đầy đủ nhờ độ phủ rộng của siêu mạng. Với một chiến lược lấy mẫu đường dẫn Toán tử đơn-Bất kỳ kết nối hiệu quả, WS-NAS cải thiện chất lượng của các mô hình được khám phá trên Criteo, và khám phá một mô hình tốt hơn trên Avazu và KDD Cup 2012 so với không gian tìm kiếm NASRec-Small.
5.3 Thảo luận
Trong phần này, chúng tôi phân tích độ phức tạp của NASRecNet, và chứng minh tác động của các kỹ thuật đề xuất của chúng tôi giảm thiểu các rối loạn xếp hạng và cải thiện chất lượng của các mô hình được tìm kiếm.
Phân tích Độ phức tạp Mô hình. Chúng tôi so sánh độ phức tạp mô hình của NASRecNets với các mô hình được chế tạo thủ công và NAS SOTA. Chúng tôi thu thập tất cả các đường cơ sở từ AutoCTR [30], và so sánh hiệu suất so với số lượng Phép toán Dấu phẩy động (FLOPs) trong Bảng 5.
Chúng tôi lập hồ sơ tất cả FLOPS của NASRecNets sử dụng FvCore [26]. Ngay cả khi không có ràng buộc FLOPs nào, NASRecNets vượt trội hơn các nghệ thuật hiện có về hiệu quả. Bất chấp đạt được Log Loss thấp hơn, NASRecNets đạt được giảm FLOPS 8.5×, 3.8× và 2.8× trên các điểm chuẩn Criteo, Avazu và KDD Cup 2012. Một lý do có thể nằm ở việc sử dụng các mô-đun tương tác cân bằng toán tử: nó chiếu các Bảng 5: Phân tích Độ phức tạp Mô hình.
PhươngphápLog Loss FLOPS(M)
Criteo Avazu KDD Criteo Avazu KDD
DLRM 0.4436 0.3814 0.1523 26.92 18.29 25.84
DeepFM 0.4432 0.3816 0.1529 22.74 22.50 21.66
AutoInt+ 0.4427 0.3813 0.1523 18.33 17.49 14.88
AutoCTR 0.4413 0.3800 0.1520 12.31 7.12 3.02
NASRecNet @ NASRec-Small 0.4399 0.3747 0.1495 2.20 3.08 3.48
NASRecNet @ NASRec-Full 0.4408 0.3737 0.1491 1.45 1.87 1.09
Bảng 6: Hiệu ứng của các kỹ thuật huấn luyện khác nhau trên NASRecNet, được đánh giá trên Criteo.
Phươngpháp Log Loss FLOPS(M)
Cơ sở (Toán tử đơn-Bất kỳ kết nối + Tinh chỉnh) 0.4408 1.45
Toán tử đơn-Kết nối đơn + Tinh chỉnh 0.4417 1.78
Bất kỳ toán tử-Bất kỳ kết nối + Tinh chỉnh 0.4413 2.04
Toán tử đơn-Bất kỳ kết nối, KHÔNG Tinh chỉnh 0.4410 3.62
đầu vào thưa thớt đến một chiều nhỏ hơn trước khi thực hiện tương tác đặc trưng chéo. Điều này dẫn đến chi phí tính toán thấp hơn đáng kể, góp phần vào các mô hình gợi ý compact nhưng hiệu suất cao.
Hiệu ứng của Lấy mẫu Đường dẫn & Tinh chỉnh. Chúng tôi đã thảo luận về các kỹ thuật lấy mẫu đường dẫn và tinh chỉnh trong Phần 4.2, và chứng minh đánh giá thực nghiệm của những kỹ thuật này về chất lượng của các mô hình được tìm kiếm trong Bảng 6. Kết quả cho thấy rằng, (1) tầm quan trọng của lấy mẫu đường dẫn vượt xa tầm quan trọng của tinh chỉnh trong việc quyết định chất lượng của các mô hình được tìm kiếm, và (2) một Kendall's 𝜏 cao hơn xếp hạng chính xác các mạng con trong không gian tìm kiếm NASRec (tức là Bảng 6) cho thấy một cải thiện nhất quán trên các mô hình được tìm kiếm.
6 KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất NASRec, một mô hình mới để hoàn toàn kích hoạt NAS choHệ thống Gợi ý thông qua Tìm kiếm Kiến trúc Mạng Neural Chia sẻ Trọng số (WS-NAS) dưới tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc. NASRec thiết lập một siêu mạng lớn để đại diện cho không gian kiến trúc đầy đủ, và kết hợp các toán tử xây dựng đa dạng và kết nối khối dày đặc để giảm thiểu các tiên nghiệm con người trong thiết kế kiến trúc tự động cho hệ thống gợi ý. NASRec xác định các thách thức về quy mô và tính không đồng nhất của không gian tìm kiếm NASRec quy mô lớn làm tổn hại siêu mạng và đề xuất một loạt kỹ thuật để cải thiện hiệu quả huấn luyện và giảm thiểu rối loạn xếp hạng. Các mô hình được chế tạo của chúng tôi, NASRecNet, đạt được hiệu suất tối tân trên 3 điểm chuẩn hệ thống gợi ý phổ biến, chứng minh triển vọng đầy hứa hẹn trên không gian tìm kiếm kiến trúc đầy đủ, và hướng nghiên cứu có động lực hướng tới việc chế tạo kiến trúc tự động hoàn toàn với các tiên nghiệm con người tối thiểu.

--- TRANG 9 ---
NASRec: Chia sẻ Trọng số trong Tìm kiếm Kiến trúc Mạng Neural cho Hệ thống Gợi ý WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA
Lời cảm ơn. Công việc của Yiran Chen được hỗ trợ một phần bởi các grant sau: NSF-2120333, NSF-2112562, NSF-1937435, NSF-2140247 và ARO W911NF-19-2-0107. Công việc của Feng được hỗ trợ một phần bởi các grant sau: NSF CAREER-2048044 và IIS-1838024. Chúng tôi cũng cảm ơn Maxim Naumov, Jeff Hwang và Colin Taylor tại Meta Platforms, Inc. vì sự giúp đỡ tốt bụng của họ trong dự án này.
TÀI LIỆU THAM KHẢO
[1]Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. 2016. Chuẩn hóa lớp. arXiv preprint arXiv:1607.06450 (2016).
[2]Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, và Quoc Le. 2018. Hiểu và đơn giản hóa tìm kiếm kiến trúc một lần. Trong Hội nghị Quốc tế về Học Máy. PMLR, 550–559.
[3]Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, và Quoc V Le. 2020. Chia sẻ trọng số có thể vượt trội hơn tìm kiếm kiến trúc ngẫu nhiên không? một điều tra với tunas. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 14323–14332.
[4]Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. 2019. Once-for-all: Huấn luyện một mạng và chuyên hóa nó để triển khai hiệu quả. arXiv preprint arXiv:1908.09791 (2019).
[5]Ben Carterette và Rosie Jones. 2007. Đánh giá công cụ tìm kiếm bằng cách mô hình hóa mối quan hệ giữa sự liên quan và nhấp chuột. Advances in neural information processing systems 20 (2007).
[6]Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, và Wenwu Ou. 2019. Behavior sequence transformer cho gợi ý thương mại điện tử trong alibaba. Trong Kỷ yếu Hội thảo Quốc tế lần thứ 1 về Thực hành Học Sâu cho Dữ liệu Thưa thớt Chiều cao. 1–4.
[7]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning cho hệ thống gợi ý. Trong Kỷ yếu hội thảo lần thứ 1 về học sâu cho hệ thống gợi ý. 7–10.
[8]Paul Covington, Jay Adams, và Emre Sargin. 2016. Mạng neural sâu cho gợi ý youtube. Trong Kỷ yếu hội nghị ACM lần thứ 10 về hệ thống gợi ý. 191–198.
[9]Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, và Guang Lin. 2021. DeepLight: Tương tác đặc trưng nhẹ sâu để tăng tốc dự đoán CTR trong phục vụ quảng cáo. Trong Kỷ yếu hội nghị quốc tế ACM lần thứ 14 về Tìm kiếm Web và khai thác dữ liệu. 922–930.
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.2020. Một hình ảnh đáng giá 16x16 từ: Transformers cho nhận dạng hình ảnh ở quy mô. arXiv preprint arXiv:2010.11929 (2020).
[11] Chen Gao, Yinfeng Li, Quanming Yao, Depeng Jin, và Yong Li. 2021. Tìm kiếm Tương tác Đặc trưng Tiến bộ cho Mạng Thưa thớt Sâu. Advances in Neural Information Processing Systems 34 (2021).
[12] Luyu Gao, Zhuyun Dai, và Jamie Callan. 2020. Khung xếp hạng dựa trên transfomer mô-đun hóa. arXiv preprint arXiv:2004.13313 (2020).
[13] Guibing Guo, Jie Zhang, và Neil Yorke-Smith. 2015. Trustsvd: Lọc cộng tác với cả ảnh hưởng rõ ràng và ngầm của lòng tin người dùng và đánh giá mục. Trong Kỷ yếu hội nghị AAAI về trí tuệ nhân tạo, Vol. 29.
[14] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, và Xiuqiang He. 2017. DeepFM: một mạng neural dựa trên factorization-machine cho dự đoán CTR. arXiv preprint arXiv:1703.04247 (2017).
[15] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, và Jian Sun. 2020. Tìm kiếm kiến trúc neural đường dẫn đơn một lần với lấy mẫu đồng đều. Trong Hội nghị Châu Âu về Thị giác Máy tính. Springer, 544–560.
[16] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al.2014. Bài học thực tế từ việc dự đoán nhấp chuột trên quảng cáo tại facebook. Trong Kỷ yếu hội thảo quốc tế lần thứ tám về khai thác dữ liệu cho quảng cáo trực tuyến. 1–9.
[17] Dominik Kowald, Subhash Chandra Pujari, và Elisabeth Lex. 2017. Hiệu ứng thời gian trên việc tái sử dụng hashtag trong twitter: Một cách tiếp cận gợi ý hashtag có cảm hứng nhận thức. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 26 về World Wide Web. 1401–1410.
[18] Ravi Krishna, Aravind Kalaiah, Bichen Wu, Maxim Naumov, Dheevatsa Mudigere, Misha Smelyanskiy, và Kurt Keutzer. 2021. Khung NAS Khả vi và Ứng dụng cho Dự đoán CTR Quảng cáo. arXiv preprint arXiv:2110.14812 (2021).
[19] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, và Guangzhong Sun. 2018. xdeepfm: Kết hợp tương tác đặc trưng rõ ràng và ngầm cho hệ thống gợi ý. Trong Kỷ yếu hội nghị quốc tế ACM SIGKDD lần thứ 24 về khám phá kiến thức & khai thác dữ liệu. 1754–1763.
[20] Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang, và Zhenguo Li. 2019. Darts+: Tìm kiếm kiến trúc khả vi cải thiện với dừng sớm. arXiv preprint arXiv:1909.06035 (2019).[21] Hanxiao Liu, Karen Simonyan, và Yiming Yang. 2018. Darts: Tìm kiếm kiến trúc khả vi. arXiv preprint arXiv:1806.09055 (2018).
[22] Ilya Loshchilov và Frank Hutter. 2016. Sgdr: Gradient descent ngẫu nhiên với khởi động lại ấm. arXiv preprint arXiv:1608.03983 (2016).
[23] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al.2019. Mô hình gợi ý học sâu cho hệ thống cá nhân hóa và gợi ý. arXiv preprint arXiv:1906.00091 (2019).
[24] Esteban Real, Alok Aggarwal, Yanping Huang, và Quoc V Le. 2019. Tiến hóa có điều chỉnh cho tìm kiếm kiến trúc phân loại hình ảnh. Trong Kỷ yếu hội nghị aaai về trí tuệ nhân tạo, Vol. 33. 4780–4789.
[25] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, và Lars Schmidt-Thieme. 2011. Gợi ý nhận biết ngữ cảnh nhanh với factorization machines. Trong Kỷ yếu hội nghị quốc tế ACM SIGIR lần thứ 34 về Nghiên cứu và phát triển trong Truy xuất Thông tin. 635–644.
[26] Facebook Research. 2022. fvcore. https://github.com/facebookresearch/fvcore,.
[27] Matthew Richardson, Ewa Dominowska, và Robert Ragno. 2007. Dự đoán nhấp chuột: ước tính tỷ lệ nhấp chuột cho quảng cáo mới. Trong Kỷ yếu hội nghị quốc tế lần thứ 16 về World Wide Web. 521–530.
[28] Ying Shan, T Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, và JC Mao. 2016. Deep crossing: Mô hình hóa quy mô web mà không có đặc trưng kết hợp được chế tạo thủ công. Trong Kỷ yếu hội nghị quốc tế ACM SIGKDD lần thứ 22 về khám phá kiến thức và khai thác dữ liệu. 255–262.
[29] David So, Quoc Le, và Chen Liang. 2019. The evolved transformer. Trong Hội nghị Quốc tế về Học Máy. PMLR, 5877–5886.
[30] Qingquan Song, Dehua Cheng, Hanning Zhou, Jiyan Yang, Yuandong Tian, và Xia Hu. 2020. Hướng tới khám phá tương tác neural tự động cho dự đoán tỷ lệ nhấp chuột. Trong Kỷ yếu Hội nghị Quốc tế ACM SIGKDD lần thứ 26 về Khám phá Kiến thức & Khai thác Dữ liệu. 945–955.
[31] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, và Jian Tang. 2019. Autoint: Học tương tác đặc trưng tự động thông qua mạng neural tự chú ý. Trong Kỷ yếu Hội nghị Quốc tế ACM lần thứ 28 về Quản lý Thông tin và Kiến thức. 1161–1170.
[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[33] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, và Song Han. 2020. Hat: Hardware-aware transformers cho xử lý ngôn ngữ tự nhiên hiệu quả. arXiv preprint arXiv:2005.14187 (2020).
[34] Ruoxi Wang, Bin Fu, Gang Fu, và Mingliang Wang. 2017. Deep & cross network cho dự đoán nhấp chuột quảng cáo. Trong Kỷ yếu ADKDD'17. 1–7.
[35] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, và Ed Chi. 2021. DCN V2: Deep & cross network cải thiện và bài học thực tế cho hệ thống xếp hạng học quy mô web. Trong Kỷ yếu Hội nghị Web 2021. 1785–1797.
[36] Zhiqiang Wang, Qingyun She, và Junlin Zhang. 2021. MaskNet: Giới thiệu nhân đặc trưng cho mô hình xếp hạng CTR bằng mask được hướng dẫn bởi instance. arXiv preprint arXiv:2102.07619 (2021).
[37] Wei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Bender, và Pieter-Jan Kindermans. 2020. Neural predictor cho tìm kiếm kiến trúc neural. Trong Hội nghị Châu Âu về Thị giác Máy tính. Springer, 660–676.
[38] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, và Quoc Le. 2020. Bignas: Mở rộng tìm kiếm kiến trúc neural với mô hình giai đoạn đơn lớn. Trong Hội nghị Châu Âu về Thị giác Máy tính. Springer, 702–717.
[39] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, và Quoc V Le. 2018. Học kiến trúc có thể chuyển giao cho nhận dạng hình ảnh có thể mở rộng. Trong Kỷ yếu hội nghị IEEE về thị giác máy tính và nhận dạng mẫu. 8697–8710.

--- TRANG 10 ---
WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, và Wei Wen
7 TÀI LIỆU BỔ SUNG
Trong phần này, chúng tôi cung cấp thêm chi tiết về NASRec, bao gồm: (1) trực quan hóa và hiểu biết về các kiến trúc được tìm kiếm, (2) đánh giá NASRecNet tốt nhất trên Criteo Terabyte4 để chứng minh hiệu suất của nó trên các điểm chuẩn dự đoán CTR quy mô lớn, và (3) chi tiết về lấy mẫu và xếp hạng mạng con.
7.1 Trực quan hóa Mô hình
Chúng tôi trực quan hóa các mô hình được tìm kiếm trong không gian tìm kiếm NASRec-Small/NASRec-Full trên 3 điểm chuẩn CTR khác nhau: Criteo, Avazu và KDD. Trước khi trình bày các kiến trúc được tìm kiếm, chúng tôi chỉ ra đặc điểm của mỗi điểm chuẩn CTR trong Bảng 7.
Bảng 7: Thống kê của các điểm chuẩn CTR khác nhau.
textbfĐiểm chuẩn #Dày đặc #Thưa thớt #Mẫu (M)
Criteo 13 26 45.84
Avazu 0 23 40.42
KDD 3 10 149.64
Ở đây, chúng tôi quan sát thấy rằng Criteo có số lượng đặc trưng dày đặc (thưa thớt) nhiều nhất, do đó là điểm chuẩn phức tạp và thách thức nhất. Avazu chỉ chứa các đặc trưng dày đặc, do đó đòi hỏi ít tương tác hơn giữa các đầu ra dày đặc trong mỗi khối lựa chọn. KDD có số lượng đặc trưng ít nhất và dữ liệu nhiều nhất, làm cho nó trở thành một điểm chuẩn tương đối dễ hơn để huấn luyện và đánh giá.
Avazu. Hình 5 và Hình 6 mô tả cấu trúc chi tiết của kiến trúc tốt nhất trong không gian tìm kiếm NASRec-Small/NASRec-Full. Ở đây, một khối sọc xanh (đỏ) chỉ ra một khối dày đặc (thưa thớt) không được sử dụng trong kiến trúc cuối cùng, và một kết nối đậm chỉ ra cùng đầu vào nguồn cho một toán tử dày đặc với hai đầu vào (tức là, Sigmoid Gating và Sum).
Vì điểm chuẩn Avazu chỉ chứa các đặc trưng thưa thớt, việc tương tác và trích xuất các biểu diễn dày đặc ít quan trọng hơn. 4https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/
DP
Thô SEFCFC EFCFC EFC
123DP EFC
4FC EFCFC EFCFC EFC
567Avazu : NASRec -Small
(3.08M, 0.3747)
d/s=512/64d/s=1024/48d/s=512/16d/s=768/48
d/s=128/16d/s=128/64d/s=512/32
Bộ gộp Dày đặc-Thưa thớtKhối Dày đặc/Thưa thớt
d/sChiều Dày đặc/Thưa thớt
Khối Dày đặc/Thưa thớt Không sử dụng
Hình 5: Mô hình tốt nhất được khám phá trên Avazu @ NASRec-Small.Ví dụ, mô hình tốt nhất trong không gian tìm kiếm NASRec-Full chỉ chứa 1 toán tử (tức là, Sigmoid Gating) chỉ xử lý các biểu diễn dày đặc, nhưng với nhiều khối Dot-Product (DP) và Attention (Attn) tương tác các biểu diễn thưa thớt. Trong không gian tìm kiếm NASRec-Small, các biểu diễn dày đặc được xử lý thường xuyên hơn bởi các lớp FC sau khi tương tác với các biểu diễn thưa thớt trong khối Dot-Product. Tuy nhiên, việc xử lý các đặc trưng dày đặc đòi hỏi nhiều khối Fully-Connected hơn một chút so với cơ chế self-attention được áp dụng trong không gian tìm kiếm NASRec-Full.
Criteo. Hình 7 và Hình 8 mô tả cấu trúc chi tiết của kiến trúc tốt nhất trong không gian tìm kiếm NASRec-Small/NASRec-Full. Ở đây,
Thô SDP Attn
1Avazu : NASRec -Full
(1.87M, 0.3737)
DP EFC
2FC EFC
3Sum Attn
4SG EFC
6SG Attn
7
DP EFC
5
d/s=32/48d/s=768/64d/s=512/48d/s=768/16d/s=768/64d/s=768/16
d/s=1024/64
Bộ gộp Dày đặc-Thưa thớtKhối Dày đặc/Thưa thớt
d/sChiều Dày đặc/Thưa thớt
Khối Dày đặc/Thưa thớt Không sử dụng
Hình 6: Mô hình tốt nhất được khám phá trên Avazu @ NASRec-Full.
Thô SFC EFC
1d/s=256/48
Thô DDP EFC
2d/s=1024/48FC EFC
3d/s=128/16FC EFC
4d/s=16/48DP EFC
5d/s=128/32FC EFC
6d/s=16/64FC EFC
7d/s=1024 /64 Criteo: NASRec -Small
(2.20M, 0.4399)
Bộ gộp Dày đặc-Thưa thớtKhối Dày đặc/Thưa thớt
d/sChiều Dày đặc/Thưa thớt
Khối Dày đặc/Thưa thớt Không sử dụng
Hình 7: Mô hình tốt nhất được khám phá trên Criteo @ NASRec-Small.

--- TRANG 11 ---
NASRec: Chia sẻ Trọng số trong Tìm kiếm Kiến trúc Mạng Neural cho Hệ thống Gợi ý WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA
một khối sọc xanh (đỏ) chỉ ra một khối dày đặc (thưa thớt) không được sử dụng trong kiến trúc cuối cùng, và một kết nối đậm chỉ ra cùng đầu vào nguồn cho một toán tử dày đặc với hai đầu vào (tức là, Sigmoid Gating và Sum).
Thô SSG Attn
1Criteo: NASRec -Full
(1.45M, 0.4408)
d/s=128/64
Thô DDP EFC 2
d/s=32/64DP EFC
3SG EFC d/s=768/48
4FC EFCd/s=16/16
5FC Attn d/s=128/48
7
FC Attn
d/s=768/166
d/s=768/32
Bộ gộp Dày đặc-Thưa thớtKhối Dày đặc/Thưa thớt
d/sChiều Dày đặc/Thưa thớt
Khối Dày đặc/Thưa thớt Không sử dụng
Hình 8: Mô hình tốt nhất được khám phá trên Criteo @ NASRec-Full.
Thô SFC EFC
1d/s=768/48
Thô DKDD: NASRec -Small
(3.48M, 0.1495)
DP EFC2d/s=128/64DP EFC
3d/s=32/48FC EFC4d/s=256/64FC EFC
5d/s=1024/64DP EFC6d/s=768/64DP EFC
7d/s=1024/48
Bộ gộp Dày đặc-Thưa thớtKhối Dày đặc/Thưa thớt
d/sChiều Dày đặc/Thưa thớt
Khối Dày đặc/Thưa thớt Không sử dụng
Hình 9: Mô hình tốt nhất được khám phá trên KDD @ NASRec-Small.Criteo chứa tập hợp đặc trưng dày đặc (thưa thớt) phong phú nhất, do đó phức tạp nhất trong việc chế tạo kiến trúc. chúng tôi quan sát thấy rằng kết nối dày đặc được đánh giá cao trong cả không gian tìm kiếm NASRec-Small và NASRec-Full, cho thấy rằng việc hợp nhất đặc trưng đang tác động đáng kể đến log loss trên một điểm chuẩn phức tạp. Ngoài ra, self-gating trên các đặc trưng dày đặc thô (tức là khối 1 @ NASRec-Full) được coi là một motif quan trọng trong việc tương tác các đặc trưng. Các mẫu tương tự cũng có thể được quan sát trong kiến trúc tốt nhất được tìm kiếm trên các điểm chuẩn KDD.
Do độ phức tạp của Criteo và các khối tìm kiếm NASRec-Full, chúng tôi nhận thấy rằng kiến trúc được tìm kiếm tốt nhất không sử dụng tất cả 7 khối trong không gian tìm kiếm. Một số khối không được sử dụng trong kiến trúc cuối cùng. Ví dụ, kiến trúc tốt nhất được tìm kiếm trong NASRec-Full chỉ chứa 4 khối hợp lệ. Chúng tôi để lại điều này như một công việc tương lai để cải thiện việc huấn luyện siêu mạng sao cho các kiến trúc sâu hơn có thể được khám phá theo cách có thể mở rộng hơn.
KDD. Hình 9 và Hình 10 mô tả cấu trúc chi tiết của kiến trúc tốt nhất trong không gian tìm kiếm NASRec-Small/NASRec-Full. Ở đây, một khối sọc xanh (đỏ) chỉ ra một khối dày đặc (thưa thớt) không được sử dụng trong kiến trúc cuối cùng, và một kết nối đậm chỉ ra cùng đầu vào nguồn cho một toán tử dày đặc với hai đầu vào (tức là, Sigmoid Gating và Sum). Tương tự như những gì chúng tôi tìm thấy trên Criteo, kiến trúc được tìm kiếm trong NASRec-Full có nhiều toán tử xây dựng hơn, nhưng ít kết nối dày đặc hơn.
Vì KDD là một điểm chuẩn đơn giản hơn với ít đặc trưng dày đặc (thưa thớt) hơn, kiến trúc được tìm kiếm đơn giản hơn, đặc biệt là trong không gian tìm kiếm NASRec. Self-gating tương tự trên các đầu vào dày đặc vẫn phục vụ như một motif quan trọng trong việc thiết kế một kiến trúc tốt hơn.
Cuối cùng, chúng tôi tóm tắt các quan sát của chúng tôi trên ba điểm chuẩn độc đáo như sau:
Thô SDP Attn
1d/s=128/48
Thô DKDD: NASRec -Full
(1.09M, 0.1491)
SG Attn 2
d/s=768/48SG EFC3d/s=16/48
Sum Attn
4
d/s=64/16Sum EFC 5d/s=64/48FC EFC
d/s=128/326SG EFC d/s=768/16
7
Bộ gộp Dày đặc-Thưa thớtKhối Dày đặc/Thưa thớt
d/sChiều Dày đặc/Thưa thớt
Khối Dày đặc/Thưa thớt Không sử dụng
Hình 10: Mô hình tốt nhất được khám phá trên KDD @ NASRec-Full.

--- TRANG 12 ---
WWW '23, 1–5 tháng 5, 2023, Austin, TX, USA Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, và Wei Wen
Hình 11: Đánh giá kiến trúc tốt nhất trên Criteo Terabyte.
•Độ phức tạp Điểm chuẩn Quyết định Độ phức tạp Kiến trúc. Lựa chọn điểm chuẩn quyết định độ phức tạp của kiến trúc cuối cùng. Càng phức tạp một điểm chuẩn, mô hình được tìm kiếm càng phức tạp trong kết nối dày đặc và tính không đồng nhất toán tử.
•Không gian Tìm kiếm Quyết định Kết nối. Trên cả ba điểm chuẩn CTR, kiến trúc tốt nhất được tìm kiếm trong NASRec-Full chứa nhiều tính không đồng nhất toán tử hơn và ít kết nối dày đặc hơn. Tuy nhiên, việc giảm kết nối dày đặc giữa các khối lựa chọn khác nhau giúp giảm tiêu thụ FLOPs của các mô hình được tìm kiếm, dẫn đến ít độ phức tạp mô hình hơn và hiệu quả mô hình tốt hơn. Điều này cũng cho thấy rằng việc tìm kiếm các toán tử xây dựng có thể vượt trội hơn tầm quan trọng của việc tìm kiếm kết nối dày đặc khi chế tạo một mô hình CTR hiệu quả.
•Attention Có Tác động Khổng lồ. Các khối Attention hiếm khi được nghiên cứu trong tài liệu hiện có về hệ thống gợi ý. Các kiến trúc được tìm kiếm trên không gian tìm kiếm NASRec-Full chứng minh hiệu quả của cơ chế attention trong việc tổng hợp các đặc trưng dày đặc (thưa thớt). Ví dụ, khối đầu tiên trong kiến trúc được tìm kiếm tốt nhất luôn áp dụng một lớp attention để tương tác các đầu vào thưa thớt thô. Việc xếp chồng các khối attention cũng được quan sát trong các kiến trúc được tìm kiếm để chứng minh tương tác bậc cao giữa các đặc trưng dày đặc (thưa thớt).
•Self-Gating Là một Motif Hữu ích. Self-gating chỉ ra một toán tử gating theo cặp với các đầu vào dày đặc giống hệt nhau. Trên cả điểm chuẩn Criteo/KDD, self-gating được khám phá để xử lý các đầu vào dày đặc thô và cung cấp các phép chiếu dày đặc với chất lượng cao hơn. Trên Avazu không có đặc trưng đầu vào dày đặc, self-gating được khám phá để kết hợp một biểu diễn dày đặc cấp cao hơn để có kết quả dự đoán tốt hơn.
7.2 Đánh giá trên Criteo Terabyte
Criteo Terabyte là một điểm chuẩn quy mô lớn về dự đoán CTR, chứa nhật ký nhấp chuột 1TB trong 24 ngày. So với phiên bản kaggle của Criteo Kaggle5 chỉ chứa 45.84M dữ liệu, Criteo Terabyte chứa ∼4B dữ liệu trong huấn luyện và xác thực, do đó có quy mô lớn hơn đáng kể.
Trên Criteo Terabyte, chúng tôi sử dụng 23 ngày đầu tiên của dữ liệu làm dữ liệu huấn luyện/xác thực, và sử dụng ngày cuối cùng của dữ liệu làm dữ liệu kiểm tra. Chúng tôi đánh giá các mô hình DLRM, AutoCTR (tức là, tối tân trước đây) và NASRecNet được tìm kiếm trên không gian tìm kiếm NASRec-Full. Chúng tôi vẽ biểu đồ log loss xác thực trên tập dữ liệu huấn luyện và 5https://www.kaggle.com/c/criteo-display-ad-challengelog loss kiểm tra trên tập dữ liệu kiểm tra trong Hình 11. So với DLRM, AutoCTR cho thấy hiệu suất tương đương trên tập dữ liệu kiểm tra, nhưng NASRecNet đạt được giảm log loss 0.03% so với đường cơ sở DLRM, cho thấy kết quả thực nghiệm tốt hơn. Tuy nhiên, vì cả AutoCTR và NASRecNet đều được chế tạo trên tập dữ liệu Criteo Kaggle, chúng có thể không phù hợp tốt với các thuộc tính của một điểm chuẩn quy mô lớn, như sự dịch chuyển phân phối dữ liệu. Chúng tôi để lại việc tìm kiếm và khám phá các kiến trúc tốt hơn trên các điểm chuẩn quy mô lớn như công việc tương lai.
7.3 Chi tiết Lấy mẫu Mạng con
Trong Phần 4, chúng tôi lấy mẫu 100 mạng con trong không gian tìm kiếm NASRec-Full trên điểm chuẩn Criteo, với một cài đặt cân bằng và hiệu quả hơn trên các thành phần tìm kiếm chiều: chiều đầu ra dày đặc có thể chọn từ {32, 64, 128, 256, 512}, và chiều đầu ra thưa thớt có thể chọn từ {16, 32, 64}. Tất cả các mạng con được huấn luyện trên điểm chuẩn Criteo với kích thước batch là 1024 và tỷ lệ học là 0.12.
Chúng tôi vẽ biểu đồ phân phối CDF của các mạng con được lấy mẫu trên cả ba điểm chuẩn trong Bảng 12. Đối với 50% kiến trúc hàng đầu được đánh giá trên siêu mạng NASRec-Full, chúng tôi báo cáo Kendall's 𝜏 là 0.24 cho điểm chuẩn Criteo, cho thấy một cải thiện rõ ràng về xếp hạng các kiến trúc hiệu suất cao so với tìm kiếm ngẫu nhiên (0.0). Trong công việc tương lai, chúng tôi đề xuất thiết lập một điểm chuẩn CTR cho NAS để tăng tính chất thống kê đáng kể của các hệ số xếp hạng được đánh giá và hỗ trợ tốt hơn nghiên cứu trong việc xếp hạng chính xác các kiến trúc khác nhau.
13 đặc trưng dày đặc
26 đặc trưng thưa thớt0 đặc trưng dày đặc
23 đặc trưng thưa thớt3 đặc trưng dày đặc
10 đặc trưng thưa thớtKDD Cup 2012 Avazu Criteo
NASRec -Full NASRec -Small
Hình 12: CDF của log loss trên các điểm chuẩn CTR.
