# 2111.06495.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2111.06495.pdf
# Kích thước tệp: 2178459 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
FAIRAUTOML: ÁP DỤNG GIẢM THIỂU BẤT CÔNG BẰNG TRONG AUTOML
Qingyun Wu1Chi Wang2

TÓM TẮT
Trong công trình này, chúng tôi đề xuất một hệ thống Học Máy Tự Động (AutoML) để tìm kiếm các mô hình không chỉ có độ chính xác dự đoán tốt mà còn công bằng. Chúng tôi đầu tiên điều tra tính cần thiết và tác động của việc giảm thiểu bất công bằng trong bối cảnh AutoML. Chúng tôi thiết lập khung FairAutoML. Khung này cung cấp một thiết kế mới dựa trên các trừu tượng thực dụng, giúp việc kết hợp các định nghĩa công bằng hiện có, kỹ thuật giảm thiểu bất công bằng và phương pháp tìm kiếm siêu tham số vào quá trình tìm kiếm và đánh giá mô hình trở nên thuận tiện. Theo khung này, chúng tôi phát triển một hệ thống AutoML công bằng dựa trên một hệ thống AutoML hiện có. Hệ thống được tăng cường bao gồm một chiến lược phân bổ tài nguyên để quyết định động khi nào và trên mô hình nào thực hiện giảm thiểu bất công bằng theo độ chính xác dự đoán, tính công bằng và tiêu thụ tài nguyên trong quá trình hoạt động. Đánh giá thực nghiệm rộng rãi cho thấy hệ thống của chúng tôi có thể đạt được 'độ chính xác công bằng' tốt và hiệu quả tài nguyên cao.

1 GIỚI THIỆU
Các hệ thống học máy tự động (AutoML) hiệu quả đã được phát triển để tự động điều chỉnh cấu hình siêu tham số và tìm các mô hình ML có hiệu suất dự đoán tốt từ dữ liệu huấn luyện đã cho. Những hệ thống này ngày càng được sử dụng để quản lý các pipeline ML trong các tình huống thực tế khác nhau (Aut, 2021). Tuy nhiên, trong nhiều ứng dụng thực tế nơi các quyết định được đưa ra có tác động trực tiếp đến phạm vi của con người; tuy nhiên, chỉ có hiệu suất dự đoán tốt là chưa đủ. Chúng ta cũng mong đợi các quyết định dựa trên ML phải có đạo đức và không đặt các nhóm hoặc cá nhân không được ưu tiên nào đó vào tình thế bất lợi hệ thống. Thật không may, đã có ngày càng nhiều bằng chứng về các vấn đề bất công bằng khác nhau liên quan đến các quyết định hoặc dự đoán do máy đưa ra trong nhiều ứng dụng như vậy (O'neil, 2016; Barocas & Selbst, 2016; Angwin et al., 2016). Xem xét việc áp dụng ngày càng tăng của các hệ thống AutoML, chúng tôi thấy việc nghiên cứu cách cải thiện tính công bằng trong các hệ thống AutoML là rất quan trọng.

Nỗ lực nghiên cứu đáng kể đã được dành cho tính công bằng trong học máy trong những năm gần đây, bao gồm nghiên cứu về định nghĩa công bằng (Dwork et al., 2012; Hardt et al., 2016; Chouldechova, 2017; Lahoti et al., 2019) và giảm thiểu bất công bằng (Kamiran & Calders, 2012; Kamishima et al., 2011; Friedler et al., 2014; Hardt et al., 2016; Calmon et al., 2017; Woodworth et al., 2017; Zafar et al., 2017; Agarwal et al., 2018; Zhang et al., 2021). Nhiều phương pháp giảm thiểu bất công bằng hiện có khá hiệu quả trong việc giảm thiểu các tác hại liên quan đến bất công bằng của các mô hình học máy.

1Pennsylvania State University (một phần công việc được thực hiện khi tác giả làm việc tại Microsoft Research).2Microsoft Research. Tương ứng với: Qingyun Wu <qingyun.wu@psu.edu>.

Tuy nhiên, chúng hiếm khi được khám phá trong các hệ thống AutoML. Xem xét tác động thực tế to lớn của AutoML, chúng tôi có động lực khám phá liệu việc kết hợp các kỹ thuật giảm thiểu bất công bằng vào hệ thống AutoML có hữu ích để cải thiện tính công bằng tổng thể của hệ thống hay không, và nếu có, cách tốt để kết hợp chúng là gì.

Để trả lời câu hỏi đầu tiên, chúng tôi tiến hành cả khảo sát tài liệu và nghiên cứu tình huống. Nghiên cứu của chúng tôi cho thấy rằng việc kết hợp giảm thiểu bất công bằng vào AutoML là có lợi (và đôi khi là cần thiết). Hướng tới việc trả lời câu hỏi thứ hai, chúng tôi đề xuất một giải pháp mới và thực dụng để cho phép việc kết hợp giảm thiểu bất công bằng vào AutoML. Cụ thể, chúng tôi đầu tiên điều tra tính cần thiết của việc giảm thiểu bất công bằng trong AutoML và tác động tiềm năng của nó đối với hệ thống AutoML về tính công bằng, hiệu suất dự đoán và chi phí tính toán. Sau đó, chúng tôi cung cấp một công thức AutoML công bằng có tính đến việc giảm thiểu bất công bằng. Đi kèm với công thức, chúng tôi phát triển các trừu tượng cần thiết để đặc trưng tổng quát cho các quy trình đánh giá tính công bằng và giảm thiểu bất công bằng trong bối cảnh AutoML. Chúng tôi cũng đề xuất một chiến lược ra quyết định tự thích ứng để đạt được AutoML công bằng một cách hiệu quả và hiệu quả. Chiến lược này giúp xác định khi nào thực hiện huấn luyện mô hình thông thường, và liệu có nên và khi nào thực hiện quy trình giảm thiểu bất công bằng bổ sung trong mỗi thử nghiệm của quá trình AutoML.

Chúng tôi tóm tắt các đóng góp chính của công trình này như sau:

1. Chúng tôi cung cấp một công thức nghiêm ngặt cho bài toán AutoML công bằng có tính đến việc giảm thiểu bất công bằng, và đề xuất một khung tổng quát để giải quyết bài toán AutoML công bằng. (tham khảo Phần 3). Các trừu tượng trong khung cho phép kết hợp thuận tiện và linh hoạt hầu hết các phương pháp đánh giá tính công bằng hiện có và các kỹ thuật giảm thiểu bất công bằng vào AutoML.

arXiv:2111.06495v2 [cs.LG] 24 Nov 2022

--- TRANG 2 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

tion hầu hết các phương pháp đánh giá tính công bằng hiện có và các kỹ thuật giảm thiểu bất công bằng vào AutoML.

2. Chúng tôi đề xuất một chiến lược thích ứng để quyết định khi nào và có nên thực hiện giảm thiểu bất công bằng là mới và có tầm quan trọng thực tiễn. (tham khảo Phần 4). Với chiến lược này, tài nguyên tính toán có thể được phân bổ hiệu quả giữa việc điều chỉnh siêu tham số và giảm thiểu bất công bằng.

3. Chúng tôi phát triển một hệ thống AutoML công bằng hoạt động dựa trên một hệ thống AutoML hiện có. Hệ thống AutoML công bằng được đề xuất là linh hoạt, mạnh mẽ và hiệu quả trong việc xây dựng các mô hình công bằng với hiệu suất dự đoán tốt. Đánh giá thực nghiệm rộng rãi liên quan đến bốn bộ dữ liệu benchmark công bằng học máy, hai định nghĩa công bằng khác nhau với hai ngưỡng công bằng khác nhau, ba phương pháp giảm thiểu bất công bằng khác nhau, hai nhiệm vụ điều chỉnh (nhiệm vụ điều chỉnh trên một học viên duy nhất và nhiều học viên), và hai phương pháp điều chỉnh, xác minh hiệu suất tốt mạnh mẽ của hệ thống chúng tôi trên tất cả các tình huống được đánh giá. (tham khảo Phần 5).

2 CÔNG TRÌNH LIÊN QUAN

Định nghĩa công bằng và giảm thiểu bất công bằng. Có chủ yếu hai loại định nghĩa công bằng cho học máy: công bằng nhóm (Dwork et al., 2012; Simoiu et al., 2017; Hardt et al., 2016) và công bằng cá nhân (Dwork et al., 2012; Lahoti et al., 2019). Dưới khung công bằng nhóm, tính công bằng, nói chung, đòi hỏi một số khía cạnh của hành vi hệ thống học máy phải có thể so sánh hoặc thậm chí được cân bằng giữa các nhóm khác nhau. Các chỉ số công bằng nhóm thường được sử dụng bao gồm Tính Bình đẳng Nhân khẩu học (DP) (Dwork et al., 2012) và Tỷ lệ Cơ hội Bình đẳng (EO) (Hardt et al., 2016). Công bằng cá nhân đảm bảo rằng các cá nhân 'tương tự' về nhiệm vụ học nhận được kết quả tương tự. Các phương pháp giảm thiểu bất công bằng tìm cách giảm các tác hại liên quan đến công bằng của một mô hình học máy về một chỉ số công bằng cụ thể. Từ góc độ thủ tục, các phương pháp giảm thiểu bất công bằng có thể được phân loại một cách thô vào các phương pháp tiền xử lý (Kamiran & Calders, 2012; Calmon et al., 2017), trong quá trình xử lý (Kamishima et al., 2011; Woodworth et al., 2017; Zafar et al., 2017; Agarwal et al., 2018; Zhang et al., 2021) và hậu xử lý (Friedler et al., 2014; Hardt et al., 2016), tùy thuộc vào việc các phương pháp cần được áp dụng trước, trong hoặc sau quá trình huấn luyện mô hình. Các phương pháp tiền xử lý thường biến đổi dữ liệu huấn luyện để cố gắng loại bỏ các thiên lệch không mong muốn. Các phương pháp trong quá trình xử lý cố gắng giảm các tác hại liên quan đến công bằng bằng cách áp đặt các ràng buộc công bằng vào cơ chế học. Các phương pháp hậu xử lý trực tiếp biến đổi đầu ra của mô hình để giảm thiên lệch. Các bộ công cụ phần mềm đã được phát triển cho tính công bằng trong học máy. Những bộ công cụ đại diện bao gồm các thư viện Python nguồn mở AIF360 (Bellamy et al., 2018) và FairLearn (Bird et al., 2020). Cả hai thư viện đều cung cấp các triển khai cho các chỉ số đo lường công bằng tiên tiến, các phương pháp giảm thiểu bất công bằng và các bộ sưu tập dữ liệu thường được sử dụng cho nghiên cứu công bằng học máy.

AutoML. Nhiều bộ công cụ và hệ thống AutoML đã được phát triển (Feurer et al., 2015; Olson et al., 2016; H2O.ai, 2019; Li et al., 2020; Wang et al., 2021b). Có rất ít công trình trong lĩnh vực AutoML có tính đến tính công bằng, mặc dù có một số nỗ lực sơ bộ trong chế độ rộng của AutoML gần đây. (Schmucker et al., 2020) sử dụng một phương pháp HPO đa mục tiêu để tìm một biên Pareto của các cấu hình có tính công bằng và độ chính xác tốt. FairHO (Cruz et al., 2021) cố gắng làm cho tối ưu hóa siêu tham số nhận thức về tính công bằng bằng cách bao gồm điểm số công bằng như một mục tiêu bổ sung. Nó có thể được sử dụng để khám phá biên Pareto về tính công bằng và độ chính xác dự đoán hoặc tìm sự cân bằng tối ưu giữa độ chính xác và tính công bằng dựa trên trọng số do người dùng chỉ định. Cả hai phương pháp nói trên về cơ bản đều thêm tính công bằng như một mục tiêu bổ sung vào quá trình tối ưu hóa siêu tham số của họ. FairBO (Perrone et al., 2021) coi tính công bằng như một ràng buộc và đề xuất giảm bất công bằng của kết quả học bằng cách thay đổi các cấu hình siêu tham số và giải quyết một bài toán tối ưu hóa Bayesian có ràng buộc. Trong công trình này, chúng tôi cũng coi tính công bằng như một ràng buộc trong AutoML. Giải pháp của chúng tôi tổng quát hơn FairBO.

3 CÔNG THỨC FAIRAUTOML

Trong phần này, chúng tôi đầu tiên giới thiệu các khái niệm và ký hiệu sẽ được sử dụng trong suốt bài báo này, và các khái niệm cơ bản của AutoML. Sau đó chúng tôi điều tra tính cần thiết và tác động của việc giảm thiểu bất công bằng trong AutoML. Cuối cùng, chúng tôi đề xuất một công thức AutoML công bằng trong đó việc giảm thiểu bất công bằng được kết hợp như một thành phần hữu cơ của quá trình AutoML.

• X và Y biểu thị các vector đặc trưng và giá trị mục tiêu trong không gian đặc trưng dữ liệu X và không gian nhãn Y tương ứng. D = (X; Y) biểu thị một bộ dữ liệu nói chung, và Dtrain = (Xtrain; Ytrain) và Dval = (Xval; Yval) biểu thị một bộ dữ liệu huấn luyện và validation cụ thể tương ứng.

• c biểu thị một cấu hình siêu tham số trong một không gian tìm kiếm siêu tham số cụ thể C, với c ∈ C. Khi chỉ có một học viên ML duy nhất được liên quan trong quá trình AutoML, c biểu thị cấu hình siêu tham số mô hình của học viên được quan tâm đó, và khi nhiều học viên được liên quan, c cũng bao gồm một chiều về sự lựa chọn của học viên.

• f biểu thị một mô hình học máy nói chung. fc,Dtrain là mô hình kết quả liên kết với cấu hình siêu tham số c, được huấn luyện trên bộ dữ liệu Dtrain, và fc,Dtrain(Xval) là kết quả dự đoán của fc,Dtrain trên dữ liệu đầu vào Xval. Trong thảo luận sau, chúng tôi cũng thêm một chỉ số trên fc,Dtrain để biểu thị cách mô hình được huấn luyện. Cụ thể, chúng tôi sử dụng f(0)c,Dtrain để biểu thị biến thể của mô hình được huấn luyện

--- TRANG 3 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

(dựa trên c và Dtrain) mà không xem xét các ràng buộc công bằng, và sử dụng f(1)c,Dtrain để biểu thị biến thể được huấn luyện với giảm thiểu bất công bằng.

• Loss : Y × Y → R là một hàm mất mát.

3.1 AutoML

Cho một bộ dữ liệu huấn luyện Dtrain, một bộ dữ liệu validation Dval, và một không gian tìm kiếm siêu tham số C, mục tiêu của AutoML1 là tìm mô hình tốt nhất có mất mát validation thấp nhất:

min c∈C Loss(fc,Dtrain(Dval), Yval) (1)

Các giải pháp cho bài toán AutoML thường quyết định cấu hình siêu tham số nào để thử trong các thử nghiệm lặp và điều chỉnh các quyết định tương lai của họ hướng tới việc tìm các cấu hình tốt hơn.

Để tạo điều kiện cho phân tích thêm, chúng tôi cung cấp một trừu tượng cho các giải pháp như vậy thông qua HPSearcher. Một HPSearcher về cơ bản chịu trách nhiệm đưa ra một đề xuất cấu hình siêu tham số trong không gian tìm kiếm đã cho bất cứ khi nào được yêu cầu. Chúng tôi sử dụng HPSearcher:suggest để biểu thị quy trình đưa ra một đề xuất cấu hình siêu tham số, và HPSearcher:update để biểu thị quy trình cập nhật thông tin cần thiết. Trừu tượng này tương thích với hầu hết tất cả các phương pháp tìm kiếm siêu tham số (Bergstra et al., 2011; Bergstra & Bengio, 2012; Snoek et al., 2012; Li et al., 2020; Wu et al., 2021; Wang et al., 2021a) cho AutoML.

Dưới bối cảnh AutoML này, có thể có vẻ đơn giản rằng để tìm một mô hình công bằng với mất mát validation tốt, người ta có thể đơn giản thêm một ràng buộc công bằng lên trên Eq. (1) và thực hiện tối ưu hóa siêu tham số có ràng buộc, như những gì được thực hiện trong một công trình gần đây (Perrone et al., 2021). Tuy nhiên, một lưu ý trong phương pháp này là, đối với một nhiệm vụ ad-hoc, người ta có thể không thể tìm một mô hình duy nhất vừa thỏa mãn ràng buộc công bằng và có mất mát tốt bằng cách đơn giản điều chỉnh các siêu tham số c ∈ C trong AutoML. Điều này về cơ bản là do có thể tồn tại nhiều nguồn bất công bằng (Dudik et al., 2020), nhiều trong số đó chỉ là không thể được giảm nhẹ bằng cách đơn giản thay đổi các siêu tham số mô hình.

3.2 Giảm thiểu bất công bằng trong AutoML: tính cần thiết và tác động

Những hạn chế nói trên của việc điều chỉnh siêu tham số chung trong AutoML về việc cải thiện tính công bằng thúc đẩy chúng tôi xem xét việc đưa các phương pháp giảm thiểu bất công bằng thuật toán chuyên dụng, ví dụ, các phương pháp tiền xử lý, trong quá trình xử lý và hậu xử lý được đề cập trong Phần 2 vào AutoML.

Tính cần thiết của việc giảm thiểu bất công bằng trong AutoML. Để xác minh tính cần thiết của việc giảm thiểu bất công bằng trong AutoML, chúng tôi tiến hành hai thí nghiệm sau đây trên bộ dữ liệu Bank (chi tiết về bộ dữ liệu này được hoãn lại đến Phần 5). Trong cả hai thí nghiệm, chúng tôi sử dụng giải pháp AutoML từ một thư viện nguồn mở có tên FLAML (Wang et al., 2021b).

1Trong công trình này, chúng tôi tập trung vào công thức thường được sử dụng này của AutoML. Phạm vi của AutoML nói chung có thể vượt ra ngoài điều này.

• Trong thí nghiệm đầu tiên, chúng tôi thực hiện một quá trình AutoML thông thường trong 4 giờ. Theo các ký hiệu trước đây của chúng tôi, chúng tôi nhận được một tập hợp các mô hình ML trong thí nghiệm này được ký hiệu bởi H = {f(0)ci,Dtrain}i∈[1,2,...], trong đó ci là cấu hình được thử ở lần lặp thứ i và lần lặp lớn nhất được xác định bởi ngân sách thời gian 4 giờ.

• Trong thí nghiệm thứ hai, trong cùng ngân sách thời gian, tức là 4 giờ, đối với mỗi cấu hình siêu tham số ci được đề xuất bởi quá trình AutoML gốc tại lần lặp i, ngoài việc huấn luyện một mô hình ML thông thường f(0)ci,Dtrain, chúng tôi cũng huấn luyện một mô hình thứ hai với giảm thiểu bất công bằng, tức là f(1)ci,Dtrain. Chúng tôi sử dụng một phương pháp trong quá trình xử lý tiên tiến có tên Exponentiated Gradient (EG) (Agarwal et al., 2018) làm phương pháp giảm thiểu. Trong thí nghiệm này, chúng tôi nhận được tập hợp mô hình sau đây H̃ = {f(0)ci,Dtrain, f(1)ci,Dtrain}i∈[1,2,...]. Lưu ý rằng vì có bước giảm thiểu bất công bằng bổ sung, tổng số lần thử trong thí nghiệm này có thể nhỏ hơn so với thí nghiệm đầu tiên.

Chúng tôi thực hiện hai thí nghiệm trên hai nhiệm vụ điều chỉnh: một điều chỉnh cả việc lựa chọn học viên và siêu tham số mô hình của mỗi học viên, và một điều chỉnh siêu tham số mô hình của học viên XGBoost2 duy nhất. Chúng tôi vẽ biểu đồ kết quả từ hai nhiệm vụ trong Hình 1(a) và (b), tương ứng. Trong hình phụ đầu tiên của Hình 1(a) và (b) có tiêu đề "Tác động đến mất mát và tính công bằng", chúng tôi vẽ biểu đồ 'điểm số bất công bằng', tức là Sự khác biệt của Tính bình đẳng thống kê (DSP) và mất mát của các mô hình được đánh giá trong cả hai thí nghiệm. Từ các biểu đồ phân tán trong Hình 1, chúng tôi có những quan sát sau: (1) Chúng tôi đầu tiên quan sát một sự giảm rõ ràng tổng thể của điểm số bất công bằng sau khi áp dụng giảm thiểu (bằng cách so sánh các điểm 'Trước giảm thiểu' và 'Sau giảm thiểu') trong cả hai nhiệm vụ điều chỉnh. Nó cho thấy hiệu quả của việc giảm thiểu bất công bằng trong việc giảm bất công bằng của mỗi mô hình học máy đơn lẻ cho các loại học viên khác nhau. (2) Bằng cách so sánh các điểm 'Trước giảm thiểu' và 'Sau giảm thiểu' cùng với các điểm 'Không giảm thiểu', chúng ta có thể thấy tính cần thiết của việc thực hiện giảm thiểu bất công bằng trong AutoML: Mặc dù bằng cách đơn giản thực hiện lựa chọn học viên và điều chỉnh siêu tham số mô hình, người ta có thể tìm các mô hình với các mức độ công bằng khác nhau (như được hiển thị bởi các dấu chéo màu xám trong các biểu đồ phân tán), mất mát của các mô hình không nhất thiết phải tốt. Ví dụ, khi 0.01 được sử dụng làm ngưỡng trong ràng buộc bất công bằng, người ta chỉ nhận được các mô hình với hiệu suất dự đoán rất tệ trong nhiệm vụ điều chỉnh trong Hình 1(a),

2Lưu ý rằng cài đặt học viên đơn này phổ biến trong các tình huống thực tế nơi do các ràng buộc về cơ sở hạ tầng hoặc triển khai, tổ chức hoặc công ty chỉ có thể sử dụng một loại mô hình nhất định.

--- TRANG 4 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

[Hình 1 với các biểu đồ phân tán và histograms hiển thị tác động của giảm thiểu bất công bằng]

Hình 1. Trong các biểu đồ phân tán của cả (a) và (b), các điểm phân tán được đánh dấu chéo có nhãn 'Không giảm thiểu' tương ứng với các mô hình được huấn luyện trong thí nghiệm đầu tiên, tức là H = {f(0)ci,Dtrain}i∈[1,2,...]; các điểm phân tán được đánh dấu hình tròn biểu thị các mô hình từ thí nghiệm thứ hai, tức là H̃. Các điểm phân tán có nhãn 'Trước giảm thiểu' và 'Sau giảm thiểu' tương ứng với các mô hình có cùng cấu hình siêu tham số nhưng được huấn luyện mà không có giảm thiểu bất công bằng, tức là f(0)ci,Dtrain và với giảm thiểu bất công bằng, tức là f(1)ci,Dtrain. Các đường ngang tương ứng với điểm số bất công bằng 0.01 và 0.05 tương ứng, đó là hai ngưỡng thường được sử dụng khi chỉ định các ràng buộc công bằng. Hình phụ thứ 2 của cả (a) và (b) hiển thị phân phối của 'tỷ lệ chi phí tính toán' (chi phí huấn luyện một mô hình với giảm thiểu, tức là f(1)ci,Dtrain chia cho chi phí huấn luyện một mô hình thông thường với cùng cấu hình, tức là f(0)ci,Dtrain).

và người ta thậm chí không thể tìm một mô hình công bằng đơn lẻ thỏa mãn ràng buộc khi học viên bị hạn chế với XGBoost. Với giảm thiểu bất công bằng, người ta có thể dễ dàng tạo ra các mô hình công bằng với mất mát tương đối. Những kết quả này xác nhận tính cần thiết và hiệu quả của việc bao gồm giảm thiểu bất công bằng trong AutoML.

Bây giờ chúng tôi chuyển sang phân tích cách người ta nên kết hợp nó vào AutoML một cách hiệu quả và hiệu quả. Để trả lời câu hỏi này, chúng tôi thấy cần thiết phải đầu tiên kiểm tra tác động của các phương pháp giảm thiểu bất công bằng về chi phí tính toán và hiệu suất dự đoán, đó là những yếu tố quan trọng ảnh hưởng trực tiếp đến hiệu suất cuối cùng của AutoML.

Tác động của giảm thiểu bất công bằng đối với ML và AutoML. Một nghiên cứu gần đây (Islam et al., 2022) cung cấp một điều tra toàn diện về tác động của giảm thiểu bất công bằng trong học máy thông qua đánh giá thực nghiệm trên một loạt rộng các phương pháp giảm thiểu bất công bằng trong nhiệm vụ phân loại công bằng. Tác động có thể được hiểu từ ba khía cạnh sau. (1) Tính công bằng: Hầu hết các phương pháp giảm thiểu bất công bằng tiên tiến có thể giảm thiểu bất công bằng một cách hiệu quả. Nhưng không có phương pháp giảm thiểu nào được đảm bảo làm cho mọi mô hình công bằng cho mọi bộ dữ liệu. (2) Hiệu suất dự đoán: Trong tất cả các phương pháp giảm thiểu bất công bằng, việc giảm thiểu bất công bằng đi kèm với sự suy giảm hiệu suất dự đoán, và sự đánh đổi là phức tạp. (3) Chi phí tính toán: Các phương pháp giảm thiểu bất công bằng, đặc biệt là các phương pháp tiền xử lý và trong quá trình xử lý thường gây ra chi phí tính toán cao. Chúng tôi cũng quan sát vấn đề nghiêm trọng này về chi phí tính toán trong nghiên cứu tình huống của chúng tôi theo các histogram chi phí trong Hình 1.

Tác động phức tạp này làm cho việc kết hợp giảm thiểu bất công bằng vào AutoML trở nên rất không tầm thường: Một mặt, chúng ta muốn thực hiện giảm thiểu bất công bằng để cải thiện tính công bằng của các mô hình ML kết quả. Mặt khác, việc thực hiện giảm thiểu bất công bằng mang lại chi phí tính toán và làm suy giảm hiệu suất dự đoán của các mô hình. Cần có một sự cân bằng tinh tế.

3.3 AutoML công bằng với giảm thiểu bất công bằng

Trừu tượng. Chúng tôi đầu tiên phát triển hai trừu tượng liên quan đến tính công bằng. Những trừu tượng này hữu ích trong việc tạo điều kiện cho việc công thức hóa và phân tích bài toán AutoML công bằng.

Đánh giá tính công bằng thông qua Fair: Chúng tôi trừu tượng hóa việc đánh giá tính công bằng của một mô hình học máy f, cho một bộ dữ liệu validation D, như một quy trình tính toán một chỉ báo công bằng dựa trên kết quả dự đoán của mô hình f trên bộ dữ liệu D theo một định nghĩa tính công bằng định lượng cụ thể. Chúng tôi sử dụng Fair(f, D) để biểu thị quy trình này nói chung. Cả tính công bằng nhóm và tính công bằng cá nhân, là hai loại định nghĩa tính công bằng chủ đạo, đều phù hợp với trừu tượng này. Trừu tượng này cũng có thể xử lý trường hợp mà mục tiêu công bằng là thỏa mãn nhiều ràng buộc công bằng. Lưu ý rằng trong bối cảnh của một định nghĩa tính công bằng cụ thể, thông tin bổ sung khác ngoài dự đoán mô hình và bộ dữ liệu có thể cần thiết.

Ví dụ 1 (Tính công bằng nhóm). Dưới bối cảnh tính công bằng nhóm, một mô hình được coi là 'công bằng' nếu đầu ra của phép đo chênh lệch về thuộc tính nhạy cảm không vượt quá một ngưỡng cụ thể. Chúng tôi ký hiệu bởi A biến thuộc tính nhạy cảm, G(·, A) một hàm chênh lệch (ví dụ, chênh lệch thống kê) được tham số hóa bởi thuộc tính nhạy cảm A, và ε một hằng số ngưỡng công bằng. Cho (A, G, ε), một triển khai của trừu tượng có thể là Fair(f, D) = 1{G(X, Y, f̂(D), A) ≤ ε}, trong đó 1{·} là một hàm chỉ báo.

Giảm thiểu bất công bằng thông qua Mitigate: Chúng tôi trừu tượng hóa việc giảm thiểu bất công bằng của một thuật toán học máy cho một bộ dữ liệu huấn luyện D như một quy trình can thiệp vào quá trình xây dựng mô hình (bao gồm biến đổi dữ liệu, huấn luyện mô hình và quy tắc suy luận) theo cách mà đầu ra của mô hình cuối cùng được đẩy về một mục tiêu công bằng cụ thể (được chỉ định thông qua hàm Fair theo trừu tượng của chúng tôi). Tất cả việc giảm thiểu bất công bằng tiền xử lý, trong quá trình xử lý và hậu xử lý được giới thiệu trước đây đều phù hợp với trừu tượng này. Theo các ký hiệu được giới thiệu trước đây, f(1)c,D được thu được thông qua quy trình giảm thiểu này, tức là f(1)c,D ← Mitigate(f, c, D, Fair).

Công thức bài toán AutoML công bằng. Cho các bộ dữ liệu Dtrain và Dval, một hàm mất mát Loss, một hàm công bằng Fair và một phương pháp giảm thiểu bất công bằng cụ thể theo trừu tượng Mitigate, chúng tôi công thức hóa bài toán AutoML công bằng như việc tìm một mô hình học máy công bằng tối thiểu hóa mất mát bằng cách tìm kiếm trên một tập hợp siêu tham số và quyết định có thực hiện giảm thiểu bất công bằng hay không, như được biểu thị toán học dưới đây,

min c∈C,m∈{0,1} Loss(f(m)c,Dtrain(Xval), Yval) (2)
s.t. Fair(f(m)c,Dtrain, Dval) ≤ ε

Trong phần còn lại của bài báo, chúng tôi sử dụng Loss(m)c và Fair(m)c như từ viết tắt cho Loss(f(m)c,Dtrain(Xval), Yval) và Fair(f(m)c,Dtrain, Dval) tương ứng mà không gây nhầm lẫn. Dưới công thức này, chúng ta có thể coi AutoML công bằng như một bài toán AutoML tối ưu hóa cho mất mát công bằng, tức là mất mát khi ràng buộc công bằng được thỏa mãn. Khái niệm mất mát công bằng này, được ký hiệu là Lfair, sẽ được sử dụng trong suốt bài báo này để đánh giá hiệu quả của các giải pháp AutoML công bằng.

Công thức được đề xuất có nhiều thuộc tính mong muốn: Trừu tượng giảm thiểu bất công bằng tách rời sự phức tạp của AutoML thông thường và giảm thiểu bất công bằng. Một mặt, điều này làm cho việc tận dụng các nỗ lực nghiên cứu và phát triển hiện có về giảm thiểu bất công bằng trở nên dễ dàng. Mặt khác, cách tiếp cận này làm cho việc đạt được AutoML công bằng khá dễ dàng bằng cách tăng cường một hệ thống AutoML hiện có.

Mặc dù có các thuộc tính mong muốn của công thức được đề xuất, việc giải quyết bài toán vẫn đầy thách thức do các lý do sau: (1) Việc giải quyết một bài toán AutoML bản thân nó là rất tốn kém. Nó thường liên quan đến một số lượng lớn các thử nghiệm tốn kém. (2) Chi phí tính toán lớn làm tăng thêm thách thức tính toán này bằng cách làm cho một số thử nghiệm với giảm thiểu bất công bằng tốn kém gấp 10 đến 100 lần.

4 KHUNG VÀ HỆ THỐNG FAIRAUTOML

Chúng tôi đầu tiên đề xuất một khung AutoML công bằng tổng quát để giải quyết bài toán AutoML công bằng được đặc trưng trong Eq. (1). Khung được đề xuất được trình bày trong Alg. 1, thành phần trung tâm của nó là một trừu tượng có tên FairSearcher.

Trách nhiệm chính của FairSearcher là đề xuất một cấu hình siêu tham số hứa hẹn c ∈ C và quyết định có thực hiện giảm thiểu bất công bằng hay không, được chỉ báo bởi m = 0 hoặc m = 1 tại mỗi thử nghiệm. Trách nhiệm này được thực hiện thông qua một hàm suggest (dòng 4 của Alg. 1). Sau đó hệ thống xây dựng một mô hình học máy f(m)c dựa trên c và m được đề xuất (dòng 5 của Alg. 1), và nhận mất mát validation và tính công bằng trên bộ dữ liệu validation (dòng 6 của Alg. 1). Lưu ý rằng bước huấn luyện và validation mô hình thường gây ra chi phí không tầm thường, được ký hiệu bởi κ(m)c. Khung sau đó ghi lại chi phí phát sinh κ(m)c, cập nhật ngân sách còn lại và ghi lại quan sát thử nghiệm chi tiết, bao gồm cấu hình siêu tham số được đề xuất c, quyết định giảm thiểu m, và mất mát tương ứng Loss(m)c, tính công bằng Fair(m)c và chi phí tính toán κ(m)c, trong H. Sau đó FairSearcher sẽ được cập nhật thông qua một hàm update sao cho quan sát mới có thể được sử dụng để giúp các đề xuất tương lai.

4.1 FairSearcher

Hiểu biết. FairSearcher nên được thiết kế theo cách mà nó có thể đưa ra các đề xuất về cấu hình siêu tham số và giảm thiểu bất công bằng hướng tới một mô hình công bằng với hiệu suất dự đoán tốt. Các phương pháp điều chỉnh siêu tham số chung (theo trừu tượng HPSearcher) có thể hiệu quả trong việc tìm các mô hình với hiệu suất dự đoán tốt. Và các phương pháp giảm thiểu bất công bằng (theo trừu tượng Mitigate) có thể được sử dụng để giảm thêm bất công bằng của các mô hình. Chúng tôi tin rằng việc tận dụng điểm tốt nhất của cả hai thế giới là có lợi. Dựa trên hiểu biết này, chúng tôi đề xuất bao gồm HPSearcher và Mitigate như các mô-đun con của FairSearcher, trong đó HPSearcher chủ yếu chịu trách nhiệm đề xuất cấu hình siêu tham số c ∈ C, và FairSearcher chịu trách nhiệm giảm bất công bằng của mỗi mô hình riêng lẻ. Chúng tôi phát triển thêm một logic bổ sung để kiểm soát có nên và khi nào thực hiện giảm thiểu bất công bằng thay vì tìm kiếm siêu tham số.

Chúng tôi trình bày hai chức năng chính của FairSearcher được đề xuất trong Alg. 2 và Alg. 3 tương ứng. Mỗi lần khi hàm FairSearcher:suggest được gọi, FairSearcher đầu tiên chọn cấu hình siêu tham số đã được đánh giá nhưng chưa được giảm thiểu có mất mát thấp nhất và không thỏa mãn ràng buộc công bằng, như cấu hình siêu tham số ứng viên để thực hiện giảm thiểu bất công bằng (dòng 2 của Alg. 2). FairSearcher sau đó hoàn thiện quyết định sau khi kiểm tra một số điều kiện (dòng

--- TRANG 6 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

Thuật toán 1 FairAutoML
1: Đầu vào: Dữ liệu huấn luyện và validation, không gian tìm kiếm C, và ngân sách tài nguyên B (tùy chọn).
2: Khởi tạo: H = []
3: while B > 0 do
4:    c, m ← FairSearcher:suggest(H)
5:    Xây dựng mô hình: huấn luyện mô hình f(m)c với (khi m = 1) hoặc không có (khi m = 0) giảm thiểu bất công bằng
6:    Validation mô hình: Nhận Loss(m)c và Fair(m)c
7:    Kiểm tra chi phí phát sinh κ(m)c, và cập nhật ngân sách B ← B - κ(m)c, và quan sát lịch sử H ← H ∪ (c, m, κ(m)c, Loss(m)c, Fair(m)c)
8:    FairSearcher:update(H).
9: end while

Thuật toán 2 FairSearcher.suggest(C)
1: H(0) = {c ∈ H | mc = 0}
2: c* ← arg minc∈H(0)&¬Fairc Lossc
3: if c* and ECF(1) < ECF(0) and L̂c* < Lfair then
4:    m ← 1, c ← c*
5: else
6:    m ← 0, c ← HPSearcher:suggest(C)
7: end if
8: Return c, m

3-6 của Alg. 2): nếu các điều kiện để thực hiện giảm thiểu bất công bằng được thỏa mãn, FairSearcher đề xuất thực hiện giảm thiểu bất công bằng, tức là m = 1, trên cấu hình siêu tham số ứng viên c* (dòng 4); ngược lại nó gọi HPSearcher để đề xuất siêu tham số mô hình và đặt quyết định giảm thiểu thành m = 0 (dòng 6).

Hai lựa chọn này có ý nghĩa khác nhau đối với hệ thống. Tìm kiếm siêu tham số thông thường cung cấp các mô hình ứng viên tốt để thực hiện giảm thiểu bất công bằng và đôi khi thậm chí có thể trực tiếp tìm các mô hình công bằng tốt hơn. Nhưng tính công bằng của các mô hình thu được không được đảm bảo. Ví dụ, khi điều chỉnh XGBoost, bằng cách đơn giản điều chỉnh siêu tham số, người ta không thể tìm một mô hình đơn lẻ với điểm số bất công bằng nhỏ hơn 0.01 theo biểu đồ phân tán trong Hình 1(b). Việc giảm thiểu bất công bằng thêm vẫn cần thiết. Giảm thiểu bất công bằng nói chung hữu ích trong việc tạo ra một mô hình công bằng. Tuy nhiên nó có thể làm suy giảm độ chính xác của mô hình kết quả, và mang lại chi phí tính toán có thể cao. Chi phí tính toán cao có thể làm cho hệ thống không thể đánh giá đủ mô hình để điều hướng một mô hình với độ chính xác tốt khi ngân sách tính toán bị hạn chế. Các điều kiện trong dòng 3 của Alg. 2 được thiết kế để tạo ra sự cân bằng tinh tế giữa hai lựa chọn này: đó là một chiến lược thích ứng dựa trên các ước tính trực tuyến về tiện ích của tìm kiếm siêu tham số thông thường và giảm thiểu bất công bằng.

Hiểu biết cho dòng 3 của FairSearcher:suggest trong

Thuật toán 3 FairSearcher.update(H)
1: H(0) = {c ∈ H | mc = 0}; H(1) = H \ H(0)
2: Cập nhật ECF(0) dựa trên H(0), và cập nhật ECF(1) dựa trên H(1) theo định nghĩa của chúng
3: HPSearcher:update(H(0))

Alg. 2: Xem xét sự suy giảm mất mát tiềm năng và chi phí tính toán cao của giảm thiểu bất công bằng, thuật toán không nên thực hiện giảm thiểu bất công bằng trên cấu hình c* trừ khi cả hai điều kiện sau đều được thỏa mãn: (1) Khi việc thực hiện giảm thiểu bất công bằng 'hiệu quả' hơn việc thực hiện tìm kiếm siêu tham số thông thường thông qua HPSearcher về việc cải thiện mất mát công bằng. Nói chung, có thể tồn tại các trường hợp mà tìm kiếm siêu tham số đã đủ tốt để tìm các mô hình công bằng với mất mát tốt, ví dụ, khi ràng buộc công bằng dễ thỏa mãn. Trong trường hợp này, giảm thiểu bất công bằng không cần thiết trừ khi nó hiệu quả hơn trong việc cải thiện mất mát công bằng so với tìm kiếm siêu tham số. (2) Khi FairSearcher có thể dự đoán một cải thiện mất mát công bằng bằng cách thực hiện giảm thiểu trên cấu hình ứng viên c. Lý tưởng là, chúng ta muốn chi tiêu tài nguyên trong giảm thiểu bất công bằng trên các cấu hình có thể mang lại mất mát công bằng tốt hơn so với mất mát công bằng tốt nhất hiện tại. Mặc dù thông tin như vậy, tức là mất mát công bằng của một cấu hình sau giảm thiểu, không có sẵn a priori, mất mát ban đầu trước giảm thiểu thường là một chỉ báo quan sát có ý nghĩa của nó.

Để thực hiện điều kiện đầu tiên, chúng tôi sử dụng ECF(m), tương tự như hàm CostImp được đề xuất trong (Wang et al., 2021a), để đặc trưng hiệu quả cải thiện mất mát công bằng của cả tìm kiếm siêu tham số thông thường và giảm thiểu bất công bằng. Nó về cơ bản xấp xỉ Chi phí Ước tính cần thiết để tạo ra một mô hình với mất mát Công bằng tốt hơn bằng cách thực hiện giảm thiểu bất công bằng (ECF(1)), hoặc điều chỉnh siêu tham số với HPSearcher (tương ứng với ECF(0)). Giảm thiểu bất công bằng không nên được tiến hành trừ khi ECF(1) < ECF(0). Một thuộc tính tốt của chiến lược này là nó có thể đạt được sự cân bằng tự thích ứng giữa giảm thiểu và tìm kiếm siêu tham số: Một khi một lựa chọn trở nên kém hiệu quả hơn, chúng ta chuyển sang lựa chọn khác. Chúng tôi bao gồm định nghĩa và giải thích chi tiết của ECF(m) trong Phụ lục A.

Để thực hiện điều kiện thứ hai, chúng tôi đề xuất tạo ra một phép chiếu về mất mát công bằng có thể đạt được và tài nguyên cần thiết cho việc giảm thiểu bất công bằng trước khi thực sự thực hiện nó. Đối với một cấu hình cụ thể c, chúng tôi ký hiệu bởi L̂c mất mát được chiếu sau giảm thiểu. Cụ thể, chúng tôi ước tính L̂c bởi L̂c = Loss(0)c + Ht,B trong đó Ht,B là một ước tính trực tuyến về sự suy giảm mất mát của giảm thiểu bất công bằng dựa trên quan sát lịch sử trong Ht và ngân sách B. Thuật toán thực hiện giảm thiểu bất công bằng chỉ khi L̂c < Lfair, trong đó Lfair là mất mát công bằng tốt nhất đạt được cho đến nay bởi hệ thống. Nói một cách trực quan, thuật toán cố gắng

--- TRANG 7 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

bỏ qua giảm thiểu bất công bằng trên một số mô hình nếu việc thực hiện không có khả năng mang lại mất mát công bằng tốt hơn. Chúng tôi cung cấp công thức chi tiết của L̂c trong Phụ lục A.

Xem xét cả hai điều kiện, chúng tôi sử dụng {ECF(1) < ECF(0) & L̂c < Lfair} (dòng 3 của Alg. 2) như quy tắc quyết định để xác định có thực hiện giảm thiểu bất công bằng hay không.

Một khi các quyết định về cấu hình siêu tham số nào để thử và có thực hiện giảm thiểu bất công bằng hay không được đưa ra (dòng 4 của Alg. 1 thông qua FairSearcher.suggest), thuật toán tiến hành xây dựng mô hình và thực hiện validation mô hình tương ứng (dòng 5 và dòng 6 của Alg. 1). Thuật toán cũng ghi lại chi phí tính toán phát sinh trong bước xây dựng và validation mô hình, mất mát dự đoán và tính công bằng của mô hình kết quả, và cập nhật chúng như một phần của quan sát lịch sử trong H. Ngân sách còn lại cũng sẽ được cập nhật tương ứng. Cuối cùng, thuật toán cập nhật FairSearcher với quan sát lịch sử H thông qua FairSearcher.update (dòng 8 của Alg. 1). FairSearcher.update được trình bày trong Alg. 3. Nó cập nhật các thống kê cần thiết cho FairSearcher để đưa ra các đề xuất về giảm thiểu bất công bằng, và cũng gọi HPSearcher.update để cập nhật HPSearcher.

5 THỰC NGHIỆM

Bộ dữ liệu, baseline và chỉ số đánh giá. Chúng tôi thực hiện đánh giá thực nghiệm trên bốn bộ dữ liệu benchmark công bằng học máy, bao gồm Adult (Kohavi, 1996), Bank (Moro et al., 2014), Compas (Angwin et al., 2016) và bộ dữ liệu Medical Expenditure Panel Survey (MEPS tóm gọn). Tất cả các bộ dữ liệu đều có sẵn công khai và chúng tôi cung cấp thêm chi tiết về chúng trong Phụ lục A. Chúng tôi bao gồm FLAML và FairBO, được xem xét trong Phần 2, làm baseline. Chúng tôi sử dụng 1 - validation accuracy làm chỉ số mất mát và fair loss làm chỉ số cuối cùng để đánh giá hiệu suất của các phương pháp được so sánh.

Cài đặt thí nghiệm AutoML công bằng. (1) Cài đặt công bằng. Chúng tôi bao gồm hai định nghĩa công bằng nhóm thường được sử dụng cho các nhiệm vụ phân loại, bao gồm Demographic Parity (DP) và Equalized Odds (EO). Theo cài đặt đánh giá công bằng nhóm trong AIF360, chúng tôi sử dụng 'sex' làm thuộc tính nhạy cảm trên Adult và Compas, 'age' trên Bank, và 'race' trên MEPS. Chúng tôi sử dụng hai ngưỡng chênh lệch thường được sử dụng ε = 0.05 và 0.01 để xác định liệu chênh lệch có đủ nhỏ để được coi là công bằng hay không. Hai ngưỡng này tương ứng với yêu cầu công bằng nhẹ và nghiêm ngặt tương ứng. Chúng tôi kiểm tra hai phương pháp giảm thiểu bất công bằng trong quá trình xử lý, bao gồm Exponentiated Gradient reduction, Grid Search reduction (Agarwal et al., 2018), và một phương pháp hậu xử lý (Hardt et al., 2016). Chúng tôi báo cáo kết quả cho Exponentiated Gradient trong bài báo chính và hai phương pháp còn lại trong Phụ lục B. Chúng tôi tận dụng các triển khai hiện có của các chỉ số công bằng và phương pháp giảm thiểu bất công bằng từ thư viện nguồn mở Fairlearn. (2) Cài đặt AutoML. Chúng tôi thực hiện đánh giá trên hai nhiệm vụ điều chỉnh: (a) điều chỉnh siêu tham số trên XGBoost; (b) điều chỉnh siêu tham số (bao gồm lựa chọn học viên và điều chỉnh siêu tham số mô hình) trên nhiều học viên máy học. Tất cả các thành phần liên quan đến AutoML khác, như tiền xử lý dữ liệu, lựa chọn không gian tìm kiếm và bộ tìm kiếm siêu tham số, vẫn giống như các tùy chọn mặc định của FLAML. Lưu ý rằng hai phương pháp điều chỉnh khác nhau (Wu et al., 2021; Wang et al., 2021a) được sử dụng trong các nhiệm vụ điều chỉnh XGBoost và đa học viên, tương ứng, theo cài đặt mặc định của FLAML. Trong tất cả các kết quả được trình bày, chúng tôi đặt tên phương pháp của chúng tôi là FairFLAML (thay vì tên chung FairAutoML) để phản ánh thực tế rằng được xây dựng dựa trên hệ thống AutoML FLAML. Chúng tôi chạy tất cả các thí nghiệm tối đa 1 giờ wall-clock time với 1 lõi CPU mười lần với các seed ngẫu nhiên khác nhau nếu không có ghi chú khác. (3) Song song hóa. Chúng tôi kiểm tra hiệu suất của phương pháp chúng tôi dưới cả tài nguyên tính toán tuần tự và song song và bao gồm kết quả trong phần thứ ba.

5.1 Hiệu quả

Chúng tôi đầu tiên muốn xác minh hiệu quả của phương pháp chúng tôi về việc tạo ra các mô hình với mất mát công bằng tốt. Chúng tôi tóm tắt kết quả từ FairFLAML và các baseline khi điều chỉnh XGBoost trong Hình 2, và khi điều chỉnh nhiều học viên trong Hình 3. Trong thí nghiệm sau, chúng tôi không bao gồm FairBO vì nó không hỗ trợ điều chỉnh nhiều học viên đồng thời.

Trong các hình phụ của Hình 2, chúng tôi tóm tắt mất mát công bằng của các phương pháp khác nhau dưới các cài đặt công bằng khác nhau. Trong các hình phụ của Hình 3, chúng tôi hiển thị kết quả được tổng hợp trên tất cả các kết hợp của cài đặt công bằng và seed ngẫu nhiên trên các bộ dữ liệu khác nhau.

Chúng tôi đầu tiên quan sát rằng, trên bộ dữ liệu Compas, thực sự có thể tạo ra các mô hình ML với mất mát công bằng tốt bằng cách đơn giản điều chỉnh siêu tham số thông thường trong một số trường hợp nhất định: cả FLAML và FairFLAML đều đạt được mất mát công bằng tốt nhất khi điều chỉnh nhiều học viên như được hiển thị trong hình phụ thứ 3 của Hình 3. Tuy nhiên, trong tất cả các trường hợp khác, FairFLAML đạt được mất mát công bằng tốt hơn đáng kể so với cả FLAML và FairBO (khi điều chỉnh XGBoost), đặc biệt là khi yêu cầu công bằng nghiêm ngặt. Ví dụ, khi điều chỉnh XGBoost trên bộ dữ liệu MEPS (Hình 4(d)), khi ngưỡng ε = 0.05, hiệu suất của FLAML gần với FairFLAML. Khi ngưỡng ε = 0.01, hiệu suất của FLAML có thể trở nên rất tệ và đôi khi nó thậm chí không thể tìm một mô hình đơn lẻ thỏa mãn ràng buộc công bằng. Điều này một lần nữa xác minh lợi ích to lớn của việc giới thiệu giảm thiểu bất công bằng vào AutoML. Về các baseline, chúng tôi cũng quan sát rằng hiệu suất của FairBO tệ hơn nhiều so với cả FairFLAML và FLAML. Có chủ yếu hai lý do cho hiệu suất tệ của nó:

--- TRANG 8 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

[Hình 2 với nhiều biểu đồ phân bố tích lũy cho fair loss trên các dataset khác nhau]

Hình 2. Fair loss dưới các cài đặt công bằng khác nhau khi điều chỉnh XGBoost trên các bộ dữ liệu khác nhau. Mỗi hình phụ hiển thị tỷ lệ tích lũy của fair loss trong 20 thí nghiệm dưới một chỉ số công bằng hoặc ngưỡng cụ thể được chỉ ra bởi tiêu đề hình phụ: (2 cài đặt công bằng) × (10 seed ngẫu nhiên). Chúng tôi sử dụng 1.2 × (mất mát tốt nhất) làm giới hạn trên trục x (hiển thị fair loss) để cải thiện khả năng hiển thị của các đường cong.

[Hình 3 với biểu đồ phân bố tích lũy cho fair loss khi điều chỉnh nhiều learner]

Hình 3. Fair loss của các phương pháp khác nhau khi điều chỉnh nhiều học viên trên các bộ dữ liệu khác nhau. Mỗi hình phụ tổng hợp kết quả từ 40 thí nghiệm: (4 kết hợp cài đặt công bằng) × (10 seed ngẫu nhiên).

--- TRANG 9 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

[Hình 4 hiển thị so sánh fair loss, phân tích tài nguyên và loss trên các dataset]

Hình 4. Các hình phụ trong cột thứ 1 và cột thứ 3 hiển thị fair loss và loss (không xem xét ràng buộc công bằng) trên các bộ dữ liệu khác nhau được tổng hợp trên 40 thí nghiệm tương tự như trong Hình 3. Các hình phụ trong cột thứ 2 hiển thị phân tích chi tiết tiêu thụ tài nguyên. Mỗi thanh bao gồm tiêu thụ tài nguyên (chiều cao của mỗi thanh chỉ ra giá trị tiêu thụ tài nguyên) của ba loại thử nghiệm có thể có: các thử nghiệm mà giảm thiểu bất công bằng không được áp dụng, ký hiệu 'HPO'; các thử nghiệm mà giảm thiểu bất công bằng được áp dụng và mô hình kết quả mang lại fair loss tốt hơn (tại thời điểm đó), có nhãn 'Effective Mitigation' và các thử nghiệm mà giảm thiểu bất công bằng không mang lại fair loss tốt hơn, có nhãn 'Wasted Mitigation'. Các số phía trên các thanh là fair loss đạt được bởi các phương pháp tương ứng.

formance: (1) Tương tự như trường hợp của FLAML, FairBO cũng điều chỉnh siêu tham số mô hình thông thường để tìm các mô hình công bằng với mất mát tốt. Việc sử dụng phương pháp tối ưu hóa có ràng buộc công bằng không hòa giải tính không hiệu quả của việc điều chỉnh siêu tham số trong việc xây dựng một mô hình với fair loss tốt; (2) Không giống như FLAML, phương pháp tối ưu hóa Bayesian được sử dụng trong FairBO không xem xét chi phí tính toán khác nhau của các mô hình với độ phức tạp khác nhau, và do đó có thể thử các mô hình không cần thiết tốn kém, điều này làm cho phương pháp, nói chung, tốn thời gian hơn FLAML.

Tóm lại, các đánh giá toàn diện xác minh tính cần thiết của việc giới thiệu giảm thiểu bất công bằng vào AutoML và hiệu quả của phương pháp chúng tôi.

5.2 Nghiên cứu ablation và hiệu quả

Các cách thức thay thế để bao gồm giảm thiểu bất công bằng. Chúng tôi so sánh thêm FairFLAML với hai phương pháp thay thế để kết hợp giảm thiểu bất công bằng dưới khung autoML công bằng tổng quát được đề xuất trong Alg. 1: (1) Như đã đề cập trong Phần 3, một phương pháp đơn giản để bao gồm giảm thiểu bất công bằng là luôn thực thi nó trong quá trình huấn luyện mô hình sau khi mỗi cấu hình siêu tham số được đề xuất. Chúng tôi đặt tên phương pháp này là MAlways. (2) Một phương pháp thay thế khác là coi giảm thiểu bất công bằng như một 'siêu tham số' phân loại bổ sung nhận giá trị nhị phân (1/0 để bật/tắt giảm thiểu bất công bằng) và áp dụng tối ưu hóa siêu tham số có ràng buộc (FLAML, 2021). Chúng tôi đặt tên phương pháp này là MasHP. Cả hai phương pháp này đều có thể được thực hiện thông qua trừu tượng FairSearcher và có thể được coi là một ablation của FairFLAML. Bằng cách so sánh với hai phương pháp thay thế này, chúng tôi hiển thị tính cần thiết của chiến lược giảm thiểu tự thích ứng trong FairFLAML.

Chúng tôi cung cấp so sánh kết quả với MAlways, MasHP và FLAML trong Hình 4 trên Bank và Compas. Do giới hạn trang, chúng tôi bao gồm kết quả trên hai bộ dữ liệu khác trong Hình 6 của phụ lục.

1. Trong cột thứ 1 của Hình 4, chúng tôi hiển thị fair loss của FairFLAML và hai phương pháp thay thế của nó dưới ngân sách tài nguyên nhỏ (300s). Kết quả chứng minh lợi thế đáng kể của FairFLAML so với hai phương pháp thay thế về fair loss.

2. Để hiểu rõ hơn cách tài nguyên được sử dụng trong các phương pháp khác nhau, trong cột thứ 2 của Hình 4, chúng tôi trực quan hóa phân tích tài nguyên cho các phương pháp khác nhau để đạt được fair loss tốt nhất dưới một cài đặt thí nghiệm cụ thể với một seed ngẫu nhiên. (1) Chúng tôi đầu tiên quan sát rằng trong MAlways và MasHP, 'Wasted Mitigation' chiếm ưu thế trong tiêu thụ tài nguyên, đây là một nguồn quan trọng của sự không hiệu quả của chúng. (2) FLAML có thể hiệu quả trong việc tìm các mô hình công bằng với fair loss tốt nhất khi có thể thực hiện được thông qua tìm kiếm siêu tham số thông thường như được hiển thị trên bộ dữ liệu Compas. Tuy nhiên, hiệu quả của phương pháp này phụ thuộc rất nhiều vào bộ dữ liệu. (3) FairFLAML có thể đạt được hiệu suất tốt hơn trung bình với ít tài nguyên hơn nhiều so với hai phương pháp thay thế theo giá trị fair loss và chiều cao của các thanh tiêu thụ tài nguyên. FairFLAML có thể nhận ra lợi ích của cả tìm kiếm siêu tham số và giảm thiểu bất công bằng.

3. Chúng tôi tin rằng cũng có ý nghĩa khi điều tra mất mát ban đầu mà không xem xét ràng buộc công bằng, chúng tôi hiển thị kết quả trong cột thứ 3 của Hình 4. Chúng tôi quan sát rằng FairFLAML bảo tồn hiệu suất tốt của FLAML về mất mát ban đầu trong khi có fair loss tốt nhất. Thuộc tính này đặc biệt mong muốn khi các practitioner đang ở chế độ khám phá về công bằng học máy, điều này khá phổ biến do sự phát triển chưa đầy đủ của chủ đề này trong các tình huống thực tế. Nó giảm nhẹ những do dự tiềm năng về việc áp dụng giảm thiểu bất công bằng vào AutoML do lo ngại về mất mát ban đầu bị suy giảm. Bằng cách biết mất mát ban đầu tốt nhất có thể đạt được mà không xem xét ràng buộc công bằng, các practitioner có thể có thêm tự tin trong việc ra quyết định. FairFLAML là hệ thống duy nhất có thể tìm cả mất mát ban đầu tốt nhất và fair loss tốt nhất.

5.3 Song song hóa

Phương pháp của chúng tôi dễ dàng song song hóa. Chúng tôi bao gồm kết quả của phương pháp chúng tôi chạy với một lõi đơn và nhiều lõi dưới các ngân sách thời gian wall-clock khác nhau trên bộ dữ liệu MEPS, là bộ dữ liệu lớn nhất xem xét cả số lượng instance dữ liệu và chiều trong Hình 5. Kết quả cho thấy rằng bằng cách tăng tài nguyên tính toán song song, phương pháp của chúng tôi có thể đạt được kết quả thậm chí tốt hơn dưới cùng ngân sách thời gian wall-clock, điều này hữu ích trong việc giảm thời gian quay vòng của nhiệm vụ điều chỉnh trong các nhiệm vụ nhạy cảm về thời gian. Chúng tôi cũng quan sát rằng khi ngân sách thời gian wall-clock tăng, sự khác biệt giữa biến thể song song và biến thể tuần tự giảm.

[Hình 5 hiển thị kết quả song song hóa dưới các ngân sách thời gian khác nhau]

Hình 5. Kết quả song song hóa dưới ngân sách thời gian wall-clock trên bộ dữ liệu MEPS. FairFLAML-1 đại diện cho biến thể của phương pháp chúng tôi chạy với một lõi đơn và FairFLAML-8 với 8 lõi.

5.4 Kết quả đánh giá bổ sung và mở rộng

Do giới hạn không gian, chúng tôi bao gồm kết quả đánh giá bổ sung trong Phụ lục B. (1) Một baseline bổ sung: Ngoài hai phương pháp thay thế trong nghiên cứu ablation của chúng tôi, chúng tôi so sánh phương pháp của chúng tôi với một phương pháp thay thế thứ ba. Phương pháp thay thế này là một phương pháp post hoc trong đó chúng tôi chọn các cấu hình với mất mát tốt nhất để thực hiện giảm thiểu bất công bằng sau khi một quá trình AutoML thông thường được hoàn thành. Chúng tôi bao gồm các mô tả chi tiết về ba phương pháp thay thế này, và so sánh với phương pháp thay thế post hoc dưới các cài đặt thí nghiệm khác nhau trong Hình 7 của Phụ lục B. (2) Kết quả trên các loại phương pháp giảm thiểu bất công bằng khác nhau: Phương pháp của chúng tôi tương thích với hầu hết các phương pháp giảm thiểu bất công bằng tiên tiến. Chúng tôi bao gồm các thí nghiệm bổ sung dưới các loại phương pháp giảm thiểu bất công bằng khác nhau trong Hình 8 và Hình 9 của Phụ lục B. (3) Khả năng mở rộng: Khung và hệ thống AutoML công bằng được đề xuất của chúng tôi có khả năng mở rộng cao và tương thích với một loạt rộng các bộ tìm kiếm siêu tham số, định nghĩa công bằng và phương pháp giảm thiểu bất công bằng. Chúng tôi bao gồm hướng dẫn và ví dụ mã để xác minh khả năng tương thích rộng trong Phụ lục B.3.

6 TÓM TẮT VÀ CÔNG VIỆC TƯƠNG LAI

Trong công trình này, chúng tôi đầu tiên xác định rằng việc giới thiệu giảm thiểu bất công bằng vào quy trình AutoML là có lợi và đôi khi cần thiết để giúp cải thiện tính công bằng của hệ thống tổng thể. Chúng tôi đề xuất một khung tổng quát để kết hợp giảm thiểu bất công bằng như một phần hữu cơ của quá trình AutoML và trình bày một hệ thống AutoML công bằng linh hoạt, mạnh mẽ và hiệu quả.

--- TRANG 11 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

TÀI LIỆU THAM KHẢO

Automl market. ReportLinker, Dec 2021. URL https://www.reportlinker.com/p06191010/AutoML-Market.html?utm_source=GNW.

Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., and Wallach, H. A reductions approach to fair classification. In International Conference on Machine Learning, pp. 60–69. PMLR, 2018.

Agarwal, A., Dudik, M., and Wu, Z. S. Fair regression quantitative definitions and reduction-based algorithms. In International Conference on Machine Learning, pp. 120–129. PMLR, 2019.

Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019.

Angwin, J., Larson, J., Mattu, S., and Kirchner, L. Machine bias there's software used across the country to predict future criminals. and it's biased against blacks. ProPublica, 2016.

BankofEngland. Machine learning in uk financial services, 10, 2019.

Barocas, S. and Selbst, A. D. Big data's disparate impact. Calif. L. Rev., 104:671, 2016.

Bellamy, R. K. E., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., Nagar, S., Ramamurthy, K. N., Richards, J., Saha, D., Sattigeri, P., Singh, M., Varshney, K. R., and Zhang, Y. AI Fairness 360 an extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias, October 2018.

Bergstra, J. and Bengio, Y. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.

Bergstra, J. S., Bardenet, R., Bengio, Y., and Kégl, B. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, 2011.

Bird, S., Dudik, M., Edgar, R., Horn, B., Lutz, R., Milan, V., Sameki, M., Wallach, H., and Walker, K. Fairlearn a toolkit for assessing and improving fairness in ai. Technical Report MSR-TR-2020-32, Microsoft, May 2020.

Calmon, F., Wei, D., Vinzamuri, B., Natesan Ramamurthy, K., and Varshney, K. R. Optimized pre-processing for discrimination prevention. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, 2017.

Chouldechova, A. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2):153–163, 2017.

Cruz, A., Saleiro, P., Belem, C., Soares, C., and Bizarro, P. Promoting fairness through hyperparameter optimization. arXiv preprint arXiv2103.12715, 2021.

Dudik, M., Chen, W., Barocas, S., Inchiosa, M., Lewins, N., Oprescu, M., Qiao, J., Sameki, M., Schlener, M., Tuo, J., and Wallach, H. Assessing and mitigating unfairness in credit models with the fairlearn toolkit. Technical Report MSR-TR-2020-34, Microsoft, September 2020.

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, 2012.

Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., and Venkatasubramanian, S. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015.

Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., and Hutter, F. Efficient and robust automated machine learning. In Advances in neural information processing systems, 2015.

FLAML. Constraints on the metrics of the ml model tried in flaml. https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#constraint, 2021.

Friedler, S., Scheidegger, C., and Venkatasubramanian, S. Certifying and removing disparate impact. arXiv preprint arXiv1412.3756, 2014.

H2O.ai. H2o automl. http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html, 2019.

Hardt, M., Price, E., and Srebro, N. Equality of opportunity in supervised learning. Advances in neural information processing systems, 2016.

Islam, M. T., Fariha, A., Meliou, A., and Salimi, B. Through the data management lens: Experimental analysis and evaluation of fair classification. In Proceedings of the 2022 International Conference on Management of Data, 2022.

John, D. The road ahead artificial intelligence and the future of financial services. The Economist Intelligence Unit, 2020.

--- TRANG 12 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

Kamiran, F. and Calders, T. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1–33, 2012.

Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp. 643–650. IEEE, 2011.

Kohavi, R. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996.

Lahoti, P., Gummadi, K. P., and Weikum, G. ifair: Learning individually fair data representations for algorithmic decision making. In 2019 IEEE 35th international conference on data engineering (icde), pp. 1334–1345. IEEE, 2019.

Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Ben-Tzur, J., Hardt, M., Recht, B., and Talwalkar, A. A system for massively parallel hyperparameter tuning. Proceedings of Machine Learning and Systems, 2:230–246, 2020.

Moro, S., Cortez, P., and Rita, P. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62:22–31, 2014.

Olson, R. S., Urbanowicz, R. J., Andrews, P. C., Lavender, N. A., Kidd, L. C., and Moore, J. H. Automating biomedical data science through tree-based pipeline optimization. In Squillero, G. and Burelli, P. (eds.), Applications of Evolutionary Computation, pp. 123–137. Springer International Publishing, 2016.

O'neil, C. Weapons of math destruction How big data increases inequality and threatens democracy. Crown, 2016.

Perrone, V., Donini, M., Zafar, M. B., Schmucker, R., Kenthapadi, K., and Archambeau, C. Fair bayesian optimization. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021.

Schmucker, R., Donini, M., Perrone, V., and Archambeau, C. Multi-objective multi-fidelity hyperparameter optimization with application to fairness. In NeurIPS 2020 Workshop on Meta-learning, 2020.

Simoiu, C., Corbett-Davies, S., and Goel, S. The problem of infra-marginality in outcome tests for discrimination. The Annals of Applied Statistics, 11(3):1193–1216, 2017.

Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning algorithms. Advances in neural information processing systems, 2012.

Wang, C., Wu, Q., Huang, S., and Saied, A. Economic hyperparameter optimization with blended search strategy. In International Conference on Learning Representations, 2021a.

Wang, C., Wu, Q., Weimer, M., and Zhu, E. Flaml: A fast and lightweight automl library. In MLSys, 2021b.

Woodworth, B., Gunasekar, S., Ohannessian, M. I., and Srebro, N. Learning non-discriminatory predictors. In Conference on Learning Theory. PMLR, 2017.

Wu, Q., Wang, C., and Huang, S. Frugal optimization for cost-related hyperparameters. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.

Zafar, M. B., Valera, I., Rogriguez, M. G., and Gummadi, K. P. Fairness constraints mechanisms for fair classification. In AISTATS, 2017.

Zhang, H., Chu, X., Asudeh, A., and Navathe, S. B. Omnifair: A declarative system for model-agnostic group fairness in machine learning. In Proceedings of the 2021 International Conference on Management of Data, 2021.

--- TRANG 13 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

A CHI TIẾT BỊ BỎ QUA

Định nghĩa chính thức và các thuộc tính mong muốn của ECF.

ECF(m) := max{R(m) - R1st(m), R1st(m) - R2nd(m), 2Lfair - L1st(m) / s(m)} (3)

trong đó chỉ số trên hoặc chỉ số dưới (m) biểu thị biến thể của biến liên kết với giảm thiểu bất công bằng (m = 1) hoặc điều chỉnh siêu tham số (m = 0). R(m) là tổng tài nguyên đã chi tiêu, R1st(m) là tổng tài nguyên đã chi tiêu khi mô hình với fair loss tốt nhất được thu được lần đầu tiên và L1st fair,(m) là fair loss tốt nhất tương ứng được thu được, R2nd(m) là tổng tài nguyên đã chi tiêu khi mô hình với fair loss tốt thứ hai được thu được lần đầu tiên, s(m) là tốc độ cải thiện fair loss, và Lfair là fair loss tốt nhất tổng thể được thu được trong hệ thống.

Chúng tôi đã đề cập đến thuộc tính tự thích ứng tốt của chiến lược ra quyết định dựa trên ECF. Đáng chú ý rằng thuộc tính này vẫn giữ nguyên ngay cả khi ECF không phải là một ước tính chính xác của đối tác ground-truth của nó: Trong trường hợp khi ECF(1) và/hoặc ECF(0) không phải là ước tính chính xác của các đối tác ground-truth của chúng và một lựa chọn sai được đưa ra, hậu quả của lựa chọn sai này sẽ được phản ánh trong ECF của lựa chọn được chọn (nó sẽ trở nên lớn hơn), trong khi ECF của lựa chọn khác (lựa chọn thứ hai) vẫn không thay đổi. Do đó chúng ta sẽ chuyển sang lựa chọn thứ hai.

Định nghĩa chính thức của Ht,B. Xem xét sự suy giảm mất mát chung của giảm thiểu bất công bằng, và phân tích lỗi được cung cấp trong (Agarwal et al., 2018), chúng tôi xấp xỉ sự suy giảm mất mát với công thức sau.

Ht,B = {
  μ nếu ECI < γc, ECI + γc < B
  b Trường hợp khác
} (4)

trong đó μ và b là trung bình và bán kính tin cậy 95% của sự suy giảm mất mát sau khi thực hiện giảm thiểu theo quan sát lịch sử tương ứng; γc là tài nguyên dự kiến cần thiết để thực hiện giảm thiểu được ước tính dựa trên quan sát lịch sử; và ECI ban đầu từ hệ thống AutoML được sử dụng FLAML (Wang et al., 2021b), và là chi phí ước tính để đạt được cải thiện mất mát. Chúng tôi tính toán tài nguyên dự kiến cần thiết để thực hiện giảm thiểu bất công bằng thành công trên siêu tham số c, tức là γc theo:

γc := κ(0)c * (1/qmitigation) * (1/|H(1)|) * Σc'∈H(1) κ(1)c'/κ(0)c' (5)

trong đó |H(1)| là độ dài của H(1) (tức là tổng số cấu hình mà chúng tôi đã thực hiện giảm thiểu bất công bằng), và qmitigation := |{c' ∈ H(1) | Fairc' = 1}|/|H(1)| là tỷ lệ thành công của việc thực hiện giảm thiểu bất công bằng. Nhớ lại rằng κ(1)c' và κ(0)c' là tài nguyên thực tế được sử dụng để thực hiện huấn luyện mô hình có/ không có giảm thiểu bất công bằng dựa trên siêu tham số c' tương ứng. Yếu tố κ(1)c'/κ(0)c' là 'tỷ lệ chi phí tính toán' được trực quan hóa trong cột cuối của Hình 1, và do đó κ(0)c * (1/|H(1)|) * Σc'∈H(1) κ(1)c'/κ(0)c' là tiêu thụ tài nguyên dự kiến để áp dụng giảm thiểu cho cấu hình c. Chúng tôi phạt thêm nó bằng qmitigation để có được ước tính về tài nguyên cần thiết cho một giảm thiểu thành công.

Bộ dữ liệu. Bốn bộ dữ liệu được sử dụng trong bài báo này đều có sẵn công khai và đại diện cho ba ứng dụng học máy nhạy cảm về công bằng: phân bổ tài nguyên tài chính, tiếp thị kinh doanh và tuyên án hình sự. Bộ dữ liệu Adult là một bộ dữ liệu điều tra dân số, nhiệm vụ dự đoán ban đầu của nó là xác định liệu một người có kiếm được hơn 50K một năm hay không. Bộ dữ liệu Bank là một bộ dữ liệu phân loại, mục tiêu là dự đoán liệu khách hàng có đăng ký một khoản tiền gửi có kỳ hạn hay không. Bộ dữ liệu Compas là một bộ dữ liệu phân loại được sử dụng để dự đoán liệu một bị cáo hình sự có tái phạm hay không. Chúng tôi cung cấp thống kê chi tiết về ba bộ dữ liệu này trong Bảng 1.

Bảng 1. Thống kê bộ dữ liệu.
Adult | Bank | Compas | MEPS
# instance: 48842 | 45211 | 5278 | 15830
# thuộc tính: 18 | 16 | 10 | 138
Lĩnh vực: tài chính | tiếp thị | tội phạm | y tế

Vui lòng tham khảo các liên kết sau để truy cập ba bộ dữ liệu được kiểm tra.

• Adult. Mô tả trong AIF360: https://github.com/Trusted-AI/AIF360/tree/master/aif360/data/raw/adult

• Bank. Mô tả trong AIF360: https://github.com/Trusted-AI/AIF360/tree/master/aif360/data/raw/bank

• Compas. Mô tả trong AIF360: https://github.com/Trusted-AI/AIF360/tree/master/aif360/data/raw/compas

• MEPS. Mô tả trong AIF360: https://github.com/Trusted-AI/AIF360/tree/master/aif360/data/raw/meps

--- TRANG 14 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

B KẾT QUẢ ĐÁNH GIÁ BỔ SUNG

B.1 So sánh với phương pháp post-hoc

Trong phần này, chúng tôi so sánh FairFLAML với một phương pháp đơn giản khác được lấy cảm hứng từ phân tích trong Phần 3. Phương pháp này chủ yếu coi giảm thiểu bất công bằng như một thành phần rời rạc sau AutoML, tức là người ta đầu tiên hoàn thành nhiệm vụ AutoML và sau đó thực hiện giảm thiểu bất công bằng trên các cấu hình đã thử trong AutoML theo thứ tự tăng dần của mất mát.

Tuy nhiên, có những cạm bẫy trong phương pháp ngây thơ này. (1) Khi tương quan giữa mất mát trước và sau giảm thiểu bất công bằng yếu, phương pháp này có thể lãng phí rất nhiều thời gian làm giảm thiểu bất công bằng không cần thiết. Ngay cả khi tương quan mạnh, mối quan hệ giữa các mất mát trước và sau giảm thiểu thường không đơn điệu. (2) Để duy trì một giải pháp end-to-end, hệ thống vẫn cần quyết định khi nào dừng quá trình AutoML ban đầu và áp dụng thao tác giảm thiểu, điều này không tầm thường đặc biệt nếu chúng ta muốn áp dụng giảm thiểu trên nhiều mô hình. Nếu thời gian dừng này không được đặt đúng cách, hệ thống có thể không thể tạo ra một mô hình công bằng đơn lẻ. Để xác minh lập luận của chúng tôi, chúng tôi xây dựng một phiên bản của phương pháp này theo cách sau: cho một ngân sách tài nguyên cụ thể, phương pháp chi tiêu nửa đầu ngân sách làm tìm kiếm siêu tham số, và nửa sau ngân sách làm giảm thiểu bất công bằng. Chúng tôi đặt tên phương pháp này là MPost, và so sánh nó với FairFLAML với các mức tài nguyên khác nhau trong Hình 7. Kết quả xác minh lập luận của chúng tôi rằng hiệu suất của MPost nhạy cảm với ngân sách tài nguyên. Nói chung, MPost yêu cầu một ngân sách tài nguyên đủ lớn.

B.2 Kết quả trên các phương pháp giảm thiểu bất công bằng khác

Chúng tôi bao gồm kết quả thu được với hai phương pháp giảm thiểu bất công bằng bổ sung, bao gồm Grid Search reduction và threshold-based post-processing, trong Hình 8 và Hình 9, trong đó chúng tôi sử dụng FairFLAML-g và FairFLAML-p để biểu thị Grid Search reduction và phương pháp post-processing tương ứng. Từ những kết quả này, chúng tôi quan sát hiệu suất tốt nhất quán từ FairFLAML tương tự như trường hợp với Exponentiated Gradient như được báo cáo trong Hình 2 trong bài báo chính. Bằng cách so sánh FairFLAML với các phương pháp giảm thiểu bất công bằng khác nhau, chúng tôi thấy Exponentiated Gradient hoạt động tốt nhất tổng thể và do đó sử dụng nó làm phương pháp giảm thiểu bất công bằng mặc định trong FairFLAML nếu không có chỉ định khác.

B.3 Khả năng mở rộng

Khung và hệ thống được đề xuất của chúng tôi có khả năng mở rộng cao và tương thích với một loạt rộng các bộ tìm kiếm siêu tham số, định nghĩa công bằng và phương pháp giảm thiểu bất công bằng. Chúng tôi bao gồm các ví dụ mã để xác minh khả năng tương thích rộng trong codebase của chúng tôi (hướng dẫn được cung cấp trong tệp 'README.md' của codebase được gửi).

Khả năng tương thích với các phương pháp giảm thiểu tiền xử lý. Ngoài hai phương pháp giảm thiểu trong quá trình xử lý và một phương pháp giảm thiểu hậu xử lý được đánh giá, hệ thống của chúng tôi cũng tương thích với các phương pháp giảm thiểu tiền xử lý. Chúng tôi xác nhận khả năng tương thích với một phương pháp tiền xử lý (Feldman et al., 2015).

Khả năng tương thích với các bộ tìm kiếm siêu tham số khác. Khung của chúng tôi tương thích với bất kỳ bộ tìm kiếm siêu tham số nào thỏa mãn trừu tượng được phát triển trong Phần 4. Chúng tôi xác nhận khả năng tương thích của khung và hệ thống với hai bộ tìm kiếm tiên tiến khác: Optuna (Akiba et al., 2019), và BlendSearch (Wang et al., 2021a). Bộ tìm kiếm đầu tiên là một bộ tìm kiếm siêu tham số tối ưu hóa Bayesian, và bộ sau là một bộ tìm kiếm siêu tham số kết hợp tìm kiếm toàn cục và tìm kiếm cục bộ.

Khả năng tương thích với định nghĩa công bằng và phương pháp giảm thiểu cho nhiệm vụ hồi quy. Tất cả các đánh giá thực nghiệm được báo cáo trong công trình này đều từ các nhiệm vụ phân loại nhị phân. Đối với các nhiệm vụ hồi quy, cần có định nghĩa công bằng và phương pháp giảm thiểu khác nhau. Chúng tôi xác nhận khả năng tương thích của FairFLAML với một định nghĩa công bằng định lượng, bounded group loss (Agarwal et al., 2019), và một phương pháp giảm thiểu bất công bằng dựa trên reduction (Agarwal et al., 2019) cho các nhiệm vụ hồi quy.

C TÁC ĐỘNG XÃ HỘI VÀ HẠN CHẾ

Tác động xã hội. Trong nhiều ứng dụng thực tế nơi các quyết định được đưa ra có tác động trực tiếp đến phúc lợi của con người, chỉ có độ chính xác dự đoán cao là chưa đủ. Chúng ta cũng mong đợi các quyết định dựa trên ML phải có đạo đức và không đặt các nhóm hoặc cá nhân không được ưu tiên nào đó vào tình thế bất lợi hệ thống. Một ví dụ về các ứng dụng lấy con người làm trung tâm như vậy nơi học máy được sử dụng rộng rãi là các dịch vụ tài chính hiện đại, bao gồm các hoạt động của các tổ chức tài chính như chấm điểm tín dụng, cho vay, v.v. Theo một cuộc khảo sát của các cơ quan quản lý Anh năm 2019 (BankofEngland, 10, 2019), hai phần ba người tham gia ngành tài chính Anh dựa vào AI ngày nay để đưa ra quyết định. Hơn nữa, một báo cáo nghiên cứu của Economist Intelligence Unit (John, 2020) phát hiện rằng 86% các giám đốc điều hành dịch vụ tài chính có kế hoạch tăng các khoản đầu tư liên quan đến AI đến năm 2025. Đã có ngày càng nhiều bằng chứng cho thấy các vấn đề bất công bằng khác nhau liên quan đến các quyết định hoặc dự đoán do máy đưa ra (O'neil, 2016; Barocas & Selbst, 2016; Angwin et al., 2016). Luật pháp và quy định, ví dụ như Đạo luật Báo cáo Tín dụng Công bằng Hoa Kỳ (FCRA) và Đạo luật Cơ hội Tín dụng Bình đẳng (ECOA), đã được thực thi để cấm bất công bằng và phân biệt đối xử trong các hoạt động tài chính liên quan.

Đồng thời, AutoML đang đóng một vai trò ngày càng có tác động trong các vòng đời học máy hiện đại. Theo một báo cáo gần đây (Aut, 2021) từ ReportLinker, thị trường AutoML được dự đoán đạt $14,830.8 triệu vào năm 2030, thể hiện tỷ lệ tăng trưởng hàng năm kép là 45.6% từ năm 2020 đến 2030. Mặc dù tầm quan trọng ngày càng tăng của AutoML trong các vòng đời ML hiện đại, việc giảm thiểu bất công bằng trong AutoML vẫn chưa được khám phá đầy đủ. Công trình của chúng tôi thực hiện nỗ lực đầu tiên để giới thiệu giảm thiểu bất công bằng vào pipeline AutoML. Khung linh hoạt cho phép tích hợp hầu hết các kỹ thuật giảm thiểu bất công bằng hiện có. Chúng tôi tin rằng điều này có thể tạo điều kiện lớn cho việc xem xét các vấn đề công bằng trong thực tiễn AutoML.

Hạn chế và công việc tương lai. Hiện tại có ba hạn chế chính của công trình chúng tôi và chúng tôi dự định giải quyết một số trong số chúng trong công việc tương lai.

1. Hệ thống AutoML công bằng được đề xuất FairFLAML có lợi thế so với một phương pháp thay thế (ví dụ, MAlways) khi chi phí tính toán của quy trình giảm thiểu bất công bằng cao. Khi chi phí này là cận biên so với thời gian huấn luyện mô hình, chiến lược phân bổ tài nguyên được thiết kế cẩn thận trong FairFLAML sẽ không tạo ra nhiều khác biệt (chủ yếu vì khoảng trống để tiết kiệm tài nguyên là nhỏ). Chúng tôi nhấn mạnh rằng hạn chế này không làm suy yếu cơ bản đóng góp của công trình này: (a) trong trường hợp 'dễ' như vậy, hệ thống của chúng tôi không tệ hơn MAlways; (b) các trường hợp 'khó' được điều tra trong bài báo này không thể bỏ qua vì hiệu suất thực nghiệm tốt và bảo đảm lý thuyết của các phương pháp giảm thiểu trong quá trình xử lý (Agarwal et al., 2018).

2. Trong công trình này, chúng tôi không cung cấp các đảm bảo lý thuyết về lợi ích của FairFLAML về tiết kiệm tài nguyên để đạt đến một fair loss mục tiêu cụ thể hoặc fair loss tốt nhất có thể đạt được với bất kỳ ngân sách tài nguyên nào. Các đảm bảo lý thuyết như vậy có thể được phát triển bằng cách sử dụng phân tích cạnh tranh. Chúng tôi dự định điều tra các thuộc tính lý thuyết của hệ thống FairFLAML được đề xuất trong công việc tương lai.

3. Mặc dù khung và hệ thống được đề xuất có thể tương thích với bất kỳ giảm thiểu bất công bằng nào thỏa mãn các trừu tượng được phát triển, chúng tôi chỉ đánh giá một số phương pháp tiên tiến trong công trình này. Chúng tôi dự định theo dõi sự phát triển nghiên cứu tiên tiến trong định nghĩa công bằng và giảm thiểu bất công bằng và có thể khám phá thêm các biến thể của chúng. Chúng tôi cũng dự định mở rộng thêm khung sao cho nhiều loại phương pháp giảm thiểu bất công bằng khác nhau có thể được tự động chọn và sử dụng kết hợp khi cần thiết.

--- TRANG 16 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

[Hình 7 hiển thị so sánh FairFLAML với MPost dưới các ngân sách tài nguyên khác nhau]

Hình 7. So sánh FairFLAML với MPost trên bộ dữ liệu Adult dưới các ngân sách tài nguyên khác nhau khi điều chỉnh XGBoost.

--- TRANG 17 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

[Hình 8 hiển thị fair loss với grid search reduction làm phương pháp giảm thiểu bất công bằng]

Hình 8. Fair loss với grid search reduction làm phương pháp giảm thiểu bất công bằng khi điều chỉnh XGBoost.

--- TRANG 18 ---
FairAutoML: Áp dụng Giảm thiểu Bất công bằng trong AutoML

[Hình 9 hiển thị fair loss với threshold-based post-processing làm phương pháp giảm thiểu bất công bằng]

Hình 9. Fair loss với threshold-based post-processing làm phương pháp giảm thiểu bất công bằng khi điều chỉnh XGBoost.
