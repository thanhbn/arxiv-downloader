# 2308.15003.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2308.15003.pdf
# Kích thước file: 3755495 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Mô hình Sinh cho các Mô hình: Tùy chỉnh DNN nhanh chóng
cho các Nhiệm vụ Đa dạng và Ràng buộc Tài nguyên
Wenxing Xu *
Đại học Bưu điện và Viễn thông Bắc Kinh Yuanchun Li†
Viện Nghiên cứu Công nghiệp AI
(AIR), Đại học Thanh Hoa Jiacheng Liu *
Viện Công nghệ Bắc Kinh
Yi Sun
Viện Nghiên cứu Công nghiệp AI
(AIR), Đại học Thanh Hoa Zhengyang Cao *
Đại học Khoa học và Công nghệ Điện tử
Trung Quốc Yixuan Li *
Đại học Bưu điện và Viễn thông Bắc Kinh
Hao Wen
Viện Nghiên cứu Công nghiệp AI
(AIR), Đại học Thanh Hoa Yunxin Liu
Viện Nghiên cứu Công nghiệp AI
(AIR), Đại học Thanh Hoa

TÓM TẮT
Không giống như các mô hình học sâu dựa trên đám mây thường lớn
và đồng nhất, các mô hình triển khai tại biên thường yêu cầu tùy
chỉnh cho các nhiệm vụ cụ thể theo lĩnh vực và môi trường hạn chế
tài nguyên. Các quy trình tùy chỉnh như vậy có thể tốn kém và
mất thời gian do sự đa dạng của các tình huống biên và khối lượng
huấn luyện cho mỗi tình huống. Mặc dù đã có nhiều phương pháp
được đề xuất cho việc tùy chỉnh nhanh chóng theo tài nguyên và
tùy chỉnh theo nhiệm vụ tương ứng, việc đạt được cả hai cùng một
lúc là thách thức. Lấy cảm hứng từ AI sinh và tính tổ hợp modular
của mạng nơ-ron, chúng tôi giới thiệu NN-Factory, một khung
work một-cho-tất-cả để tạo ra các mô hình nhẹ tùy chỉnh cho
các tình huống biên đa dạng. Ý tưởng chính là sử dụng mô hình
sinh để trực tiếp tạo ra các mô hình tùy chỉnh, thay vì huấn luyện
chúng. Các thành phần chính của NN-Factory bao gồm một supernet
modular với các module được huấn luyện trước có thể được kích
hoạt có điều kiện để hoàn thành các nhiệm vụ khác nhau và một
bộ lắp ráp module sinh tự động thao tác các module theo yêu cầu
nhiệm vụ và độ thưa. Với một tình huống biên cho trước, NN-Factory
có thể hiệu quả tùy chỉnh một mô hình compact chuyên biệt trong
nhiệm vụ biên đồng thời thỏa mãn các ràng buộc tài nguyên biên
bằng cách tìm kiếm chiến lược tối ưu để lắp ráp các module. Dựa
trên các thí nghiệm về các nhiệm vụ phân loại hình ảnh và phát hiện
đối tượng với các thiết bị biên khác nhau, NN-Factory có khả năng
tạo ra các mô hình chất lượng cao cụ thể theo nhiệm vụ và tài nguyên
trong vài giây, nhanh hơn các phương pháp tùy chỉnh mô hình thông
thường hàng bậc độ lớn.

*Công việc được thực hiện khi các tác giả đang thực tập tại Viện Nghiên cứu
Công nghiệp AI (AIR), Đại học Thanh Hoa.
†Tác giả liên hệ.

TỪ KHÓA
Học sâu, tình huống biên, ràng buộc tài nguyên, tùy chỉnh mô hình,
mô hình sinh

1 GIỚI THIỆU
Học sâu (DL) đã trở thành một công nghệ trí tuệ nhân tạo (AI)
thay đổi cuộc chơi trong những năm gần đây. Nó đã đạt được
hiệu suất đáng kể trong nhiều lĩnh vực khác nhau bao gồm thị
giác máy tính, hiểu biết ngôn ngữ tự nhiên, trò chơi máy tính,
và sinh học tính toán. Đồng thời, học sâu đã cho phép và tăng
cường nhiều ứng dụng thông minh tại biên, chẳng hạn như hỗ
trợ lái xe [21,60], xác thực khuôn mặt [6], giám sát video [25,59],
nhận dạng giọng nói [42,51], v.v. Do các cân nhắc về độ trễ và
bảo mật, việc triển khai các mô hình đến các thiết bị biên [15,30,36]
đang trở thành một thực hành ngày càng phổ biến, để các mô
hình có thể được gọi trực tiếp mà không cần truyền dữ liệu đến
máy chủ.

Để áp dụng các mô hình DL trong các tình huống biên khác nhau,
các nhà phát triển thường cần tùy chỉnh các mô hình, bao gồm
hai quy trình chính sau đây.

(1)Tùy chỉnh Hướng Nhiệm vụ¹: Tùy chỉnh các mô hình cho
các nhiệm vụ cụ thể theo lĩnh vực, chẳng hạn như phát hiện
các loại xe nhất định, theo dõi người với trang phục nhất
định, hoặc nhận dạng các vật phẩm với lỗi nhất định.

¹Trong bài báo này, chúng tôi tập trung vào trường hợp đơn giản khi các nhiệm vụ
qua các tình huống biên khác nhau chia sẻ một không gian tổ hợp. Ví dụ, mỗi nhiệm
vụ là phân loại/phát hiện các đối tượng với sự kết hợp của một số thuộc tính đã biết
(ví dụ: màu sắc, loại, danh mục, yếu tố, v.v.).

1arXiv:2308.15003v1  [cs.AI]  29 Aug 2023

--- TRANG 2 ---
Preprint, , 2023 Xu và Li et al.

(2)Tùy chỉnh Hướng Tài nguyên: Tùy chỉnh các mô hình cho
các thiết bị biên, nhằm đáp ứng các ràng buộc tài nguyên
nhất định trên thiết bị biên mục tiêu, bao gồm ngân sách
bộ nhớ và yêu cầu độ trễ.

Thực hành hiện tại là xử lý từng vấn đề tùy chỉnh riêng biệt.
Ví dụ, các nhà phát triển cần tạo ra các mô hình compact thông
qua thiết kế kiến trúc mô hình/nén rộng rãi để đáp ứng các ràng
buộc tài nguyên, và sau đó huấn luyện/tinh chỉnh mô hình trên
các nhiệm vụ cụ thể theo lĩnh vực để cải thiện độ chính xác.
Một quy trình như vậy có thể liên quan đến việc thu thập dữ
liệu tốn kém, tìm kiếm kiến trúc tốn thời gian, và huấn luyện
mô hình không ổn định.

Những tiến bộ gần đây của học sâu đã chứng minh khả năng
thích ứng zero-shot xuất sắc của các mô hình được huấn luyện
trước lớn, bao gồm thích ứng kiến trúc mạng nơ-ron không
cần huấn luyện và thích ứng nhiệm vụ. Cụ thể, các kỹ thuật
tìm kiếm kiến trúc mạng nơ-ron một lần (NAS) [3,5] và mở
rộng mô hình [13,17,24,53] đã chứng minh khả năng huấn luyện
một supernet và tùy chỉnh nó cho các môi trường biên đa dạng.
Các mô hình lớn được huấn luyện trước như ChatGPT [40],
OFA [4], và Segment Anything [7] cho phép người dùng tùy
chỉnh nhiệm vụ của mô hình bằng cách đơn giản cung cấp một
prompt nhiệm vụ làm đầu vào cho mô hình. Những khả năng
thích ứng không cần huấn luyện như vậy rất mong muốn trong
các tình huống AI biên do khả năng hỗ trợ một loạt rộng các
nhiệm vụ downstream và ràng buộc phần cứng đa dạng với
nỗ lực phát triển giảm đáng kể.

Tuy nhiên, rất khó để kết hợp việc tùy chỉnh kiến trúc mạng
nơ-ron không cần huấn luyện và tùy chỉnh nhiệm vụ không
cần huấn luyện cùng nhau. Một mặt, việc sử dụng NAS một
lần hoặc mở rộng mô hình để tạo ra các mô hình cho các nhiệm
vụ khác nhau là không khả thi vì supernet được thiết kế cho
một nhiệm vụ duy nhất. Mở rộng chúng để hỗ trợ nhiều nhiệm
vụ sẽ làm tăng đáng kể không gian tìm kiếm mô hình, khiến
việc huấn luyện supernet và tìm kiếm subnet trở nên cực kỳ
thách thức. Mặt khác, khả năng thích ứng nhiệm vụ đáng kể
của các mô hình được huấn luyện trước đi kèm với chi phí
tải tính toán khổng lồ. Các nghiên cứu gần đây đã cho thấy
rằng khả năng tổng quát hóa zero-shot của các mô hình được
huấn luyện trước xuất hiện và tăng cường với việc tăng kích
thước mô hình [28]. Do đó, rất khó hoặc thậm chí không thể
trực tiếp sử dụng mô hình lớn có khả năng tổng quát hóa nhiệm
vụ do các tài nguyên tính toán biên đa dạng và hạn chế.

Để đạt được việc tùy chỉnh mô hình hiệu quả hướng nhiệm
vụ và hướng tài nguyên, chúng tôi giới thiệu NN-Factory, một
paradigm mới để tạo ra các mô hình tùy chỉnh cho các tình
huống biên đa dạng. Ý tưởng chính của chúng tôi là xem vấn
đề tùy chỉnh mô hình cụ thể theo biên như một vấn đề sinh -
thay vì các mô hình sinh hiện tại được thiết kế để tạo ra nội
dung media [8,9,40], chúng tôi sử dụng AI sinh để tạo ra các
mô hình tùy chỉnh dựa trên các đặc tả tình huống biên. Cụ
thể, với nhiệm vụ mong muốn, loại thiết bị, và các yêu cầu
tài nguyên trong một tình huống biên, NN-Factory

Hình 1: Ý tưởng chính của NN-Factory: Tạo ra các mô hình
cho các tình huống biên đa dạng bằng cách lắp ráp các module.

nhanh chóng tạo ra một mô hình compact phù hợp với nhiệm vụ
và yêu cầu bằng cách đơn giản truy vấn mô hình sinh. Một
paradigm mới về tùy chỉnh mô hình như vậy có tiềm năng
tránh hoàn toàn các quy trình nén và huấn luyện cồng kềnh
trong các phương pháp tùy chỉnh mô hình biên thông thường.

Tuy nhiên, việc trực tiếp tạo ra các tham số của mô hình là
khó khăn do số lượng tham số mô hình khổng lồ. Hiểu biết
chính của NN-Factory để giải quyết vấn đề này là tính tổ hợp
của các module mạng nơ-ron [1,41], tức là các chức năng và
kích thước khác nhau của mạng nơ-ron có thể đạt được bằng
các kết hợp nhất định của các module mạng nơ-ron, như được
minh họa trong Hình 1. Tính tổ hợp modular như vậy đã được
nghiên cứu bởi các phương pháp hiện có (ví dụ: NestDNN [13],
LegoDNN [17], AdaptiveNet [53], v.v.) cho việc mở rộng mô
hình phía biên, nhưng không có phương pháp nào trong số
chúng có thể hỗ trợ tùy chỉnh hướng nhiệm vụ.

Dựa trên hiểu biết này, NN-Factory tạo ra một mô hình tùy
chỉnh cho mỗi tình huống biên bằng cách trực tiếp tạo ra các
cấu hình để lắp ráp các module nơ-ron được huấn luyện trước.
Các thành phần chính của NN-Factory bao gồm một supernet
modular mang các module nơ-ron, một bộ lắp ráp module
nhận biết nhiệm vụ và độ thưa tạo ra các cấu hình lắp ráp
module ứng viên (mỗi cấu hình là một tập hợp các vector
gate có thể ánh xạ đến một mô hình ứng viên), và một bộ tìm
kiếm kiến trúc nhẹ nhanh chóng tìm ra cấu hình tối ưu để
lắp ráp mô hình cho tình huống biên mục tiêu.

Cụ thể, supernet modular được mở rộng từ một mạng backbone
chung, chẳng hạn như Mạng Nơ-ron Tích chập (CNN) hoặc
Transformer, trong đó mỗi khối cơ bản được phân tách thành
nhiều module được kích hoạt có điều kiện. Cách thức các
module được kích hoạt được điều khiển bởi một số vector gate,
được tạo ra bởi một mô hình sinh (tức là bộ lắp ráp module).
Bằng cách kích hoạt các tập hợp module khác nhau, các khối
cơ bản trong mạng backbone có thể được cấu hình lại để có
các chức năng và kích thước khác nhau. Bộ lắp ráp module
quyết định cách kích hoạt các module trong mạng backbone
dựa trên mô tả nhiệm vụ và yêu cầu độ thưa (tức là giới hạn
tỷ lệ kích hoạt module). Các module và bộ lắp ráp được huấn
luyện cùng nhau để hoạt động nhất quán cùng nhau.

2

--- TRANG 3 ---
Mô hình Sinh cho các Mô hình: Tùy chỉnh DNN nhanh chóng cho các Nhiệm vụ Đa dạng và Ràng buộc Tài nguyên Preprint, , 2023

Dựa trên thiết kế module-assembler, chúng tôi có thể giảm
không gian tìm kiếm lớn của các kết hợp module xuống một
không gian tìm kiếm nhỏ của các prompt sinh. Để tìm mô
hình tùy chỉnh cho một tình huống biên (được định nghĩa
bởi nhiệm vụ, thiết bị, và yêu cầu độ trễ/bộ nhớ), chúng tôi
chỉ cần tìm yêu cầu sinh mô hình tối ưu, được thực hiện bởi
module tìm kiếm kiến trúc nhẹ dưới sự hướng dẫn của một
bộ đánh giá hiệu suất mô hình cụ thể theo thiết bị. Do không
gian tìm kiếm được giảm đáng kể, việc tìm mô hình tối ưu
chỉ mất vài lần lặp ngắn.

Để đánh giá NN-Factory, chúng tôi tiến hành các thí nghiệm
với ba thiết bị trên hai loại nhiệm vụ. Kết quả đã chứng minh
rằng phương pháp của chúng tôi có thể tạo ra các mô hình
phù hợp cho một tình huống biên cho trước trong tối đa 6
giây, nhanh hơn 1000× so với phương pháp tùy chỉnh dựa
trên huấn luyện thông thường. Các mô hình được tạo ra có
thể đạt được độ chính xác cao tương đương với các mô hình
được huấn luyện lại/tinh chỉnh tại biên trên cả các nhiệm vụ
đã biết và chưa thấy.

Công việc của chúng tôi đóng góp các kỹ thuật sau:

(1)Chúng tôi đề xuất một giải pháp tùy chỉnh mô hình cụ
thể theo biên dựa trên AI sinh mới lạ. Nó cho phép tùy
chỉnh mô hình nhanh chóng không cần huấn luyện cho
các tình huống biên với nhiệm vụ đa dạng và ràng buộc
tài nguyên.

(2)Chúng tôi giới thiệu một thiết kế modular cho việc sinh
mô hình, cho phép tùy chỉnh mô hình nhanh chóng bằng
cách đơn giản truy vấn một mô hình assembler cho việc
kích hoạt module.

(3)Dựa trên các thí nghiệm với hai loại nhiệm vụ và nhiều
thiết bị biên khác nhau, phương pháp của chúng tôi có
khả năng tạo ra các mô hình chất lượng cao cho các tình
huống biên đa dạng với chi phí thấp hơn đáng kể. Hệ
thống và các mô hình sẽ được mở mã nguồn.

2 BỐI CẢNH VÀ CÔNG VIỆC LIÊN QUAN

2.1 Tùy chỉnh Mô hình cho các Tình huống Biên

Triển khai mạng nơ-ron sâu (DNN) tại biên ngày càng phổ
biến do các yêu cầu về độ trễ và quan ngại về bảo mật của
các dịch vụ học sâu. Tuy nhiên, việc trực tiếp tuân theo các
quy trình huấn luyện và triển khai mô hình dựa trên đám mây
thông thường không thỏa đáng do sự đa dạng khổng lồ của
các tình huống biên [13,17,53]. Không giống như hầu hết
các mô hình AI dựa trên đám mây được thiết kế cho các nhiệm
vụ chung và được lưu trữ trong các cụm GPU mạnh mẽ, các
tình huống AI biên thường đa dạng và phân mảnh. Mỗi tình
huống biên có thể xử lý một nhiệm vụ cụ thể theo lĩnh vực
và một tập hợp duy nhất các môi trường triển khai mục tiêu.

Theo các đối tác ngành cung cấp dịch vụ AI biên cho các
nhà tùy chỉnh, nỗ lực phát triển chính của họ được dành cho
việc tùy chỉnh các mô hình để xử lý sự đa dạng của các tình
huống biên, bao gồm đa dạng phần cứng, đa dạng nhiệm vụ,

Hình 2: Độ trễ trung bình (ms) của ResNet50 trong các
môi trường triển khai khác nhau. Trong cài đặt "multi-tenant",
chúng tôi giả định có ba quy trình nền chạy cùng một mô hình.

Bảng 1: Các nhiệm vụ AI điển hình được yêu cầu bởi khách
hàng của một nhà cung cấp dịch vụ AI biên.

Khách hàng          Nhiệm vụ
Công trường xây dựng    Phát hiện người mặc đồng phục xanh.
Nhà hàng               Phân loại người đội mũ đầu bếp trắng.
Bệnh viện              Phân loại người đeo khẩu trang.
Trung tâm mua sắm      Phát hiện người hút thuốc.
Ga giao thông          Phát hiện xe có biển số xanh.

đa dạng phân phối dữ liệu, v.v. Trong số đó, một vấn đề chính
là đa dạng phần cứng. Các nhà tùy chỉnh thường yêu cầu triển
khai các mô hình đến các nền tảng phần cứng khác nhau, chẳng
hạn như máy chủ biên, máy tính để bàn, điện thoại thông minh,
và hộp AI biên. Do khả năng tính toán khác nhau, cùng một
mô hình có thể mang lại hiệu suất khác nhau đáng kể trên
các thiết bị và môi trường biên khác nhau, như được hiển thị
trong Hình 2. Do đó, các nhà phát triển cần tùy chỉnh kiến
trúc mô hình để đáp ứng các ràng buộc độ trễ. Đa dạng nhiệm
vụ là một vấn đề quan trọng khác mà các nhà cung cấp dịch
vụ AI biên gặp phải. Mỗi nhà tùy chỉnh có thể có một nhiệm
vụ AI cụ thể theo lĩnh vực dựa trên tình huống ứng dụng,
như được minh họa trong Bảng 1. Để cho phép các dịch vụ
AI trong các tình huống như vậy, việc tùy chỉnh mô hình cho
các nhiệm vụ khác nhau là cần thiết.

Có nhiều phương pháp hiện tại được đề xuất cho từng loại
mục tiêu tùy chỉnh ở trên. Tuy nhiên, khi xem xét hai mục
tiêu cùng nhau cho nhiều tình huống biên đa dạng, chi phí
trở nên khổng lồ, vì quy trình tùy chỉnh không tầm thường
(như sẽ được giải thích sau) và phải được lặp lại cho mỗi
tình huống.

3

--- TRANG 4 ---
Preprint, , 2023 Xu và Li et al.

2.2 Tùy chỉnh Mô hình dựa trên Huấn luyện

Để tùy chỉnh một mô hình cho một nhiệm vụ cụ thể theo lĩnh
vực nhất định, thực hành phổ biến là sử dụng học chuyển giao
(TL). Phương pháp chủ đạo trong học chuyển giao là tinh
chỉnh, tức là khởi tạo các tham số mô hình với một mô hình
được huấn luyện trước và tiếp tục huấn luyện các tham số
với dữ liệu cụ thể theo lĩnh vực. Các quy trình học chuyển
giao như vậy yêu cầu các nhà phát triển thu thập các mẫu
dữ liệu cho nhiệm vụ, gắn nhãn chúng, và huấn luyện mô
hình với các mẫu được gắn nhãn, điều này thường tốn nhiều
lao động, đòi hỏi tài nguyên, và mất thời gian.

Để khớp một mô hình vào một thiết bị biên cụ thể, các giải
pháp điển hình bao gồm nén một mô hình hiện có [18,50,65]
(ví dụ: pruning, quantization, v.v.) hoặc tìm một kiến trúc
mô hình mới phù hợp với khả năng của thiết bị mục tiêu [44,
46]. Vì việc thiết kế thủ công các mô hình cho các môi trường
biên đa dạng rất cồng kềnh, thực hành phổ biến là sử dụng
các kỹ thuật sinh mô hình tự động. NAS [16,34,45,54,56]
là phương pháp sinh mô hình đại diện và được sử dụng rộng
rãi nhất, tìm kiếm kiến trúc mạng tối ưu trong một không
gian tìm kiếm được thiết kế tốt. Hầu hết các phương pháp
NAS yêu cầu huấn luyện các kiến trúc trong quá trình tìm
kiếm [38,43,45,66], điều này cực kỳ tốn thời gian (10,000+
giờ GPU) khi tạo ra các mô hình cho một số lượng lớn thiết bị.

Cả hai quy trình trên đều đặt ra thách thức cho các nhà phát
triển do sự đa dạng của các tình huống biên. Huấn luyện hoặc
tinh chỉnh mô hình cho một tình huống biên thường yêu cầu
hàng nghìn mẫu huấn luyện được gắn nhãn và vài giờ trên
các máy GPU hiệu suất cao. Tìm kiếm kiến trúc mô hình tối
ưu hoặc chiến lược nén bằng thử và sai cũng yêu cầu nhiều
nỗ lực phát triển.

2.3 Tùy chỉnh Mô hình không cần Huấn luyện

Những tiến bộ gần đây của học sâu đã cho thấy tính khả thi
của việc sử dụng một mô hình được huấn luyện trước thống
nhất để hỗ trợ nhiều nhiệm vụ downstream khác nhau mà
không cần huấn luyện thêm. Cụ thể, người ta có thể huấn
luyện một mô hình nền tảng trên một tập dữ liệu lớn với nhiều
nhiệm vụ được định nghĩa trước và trực tiếp sử dụng mô hình
cho các nhiệm vụ downstream khác nhau với các prompt đơn
giản. Theo các phân tích thực nghiệm [28], cải thiện khả
năng của mô hình được huấn luyện trước có thể dẫn đến hiệu
suất tốt hơn trên các nhiệm vụ downstream.

Đồng thời, cả cộng đồng AI và cộng đồng điện toán di động
đều đã thử nhiều cách khác nhau để giảm chi phí tạo ra các
mô hình nhẹ cho các thiết bị biên. NAS một lần [3,5,22,37]
được đề xuất để giảm đáng kể chi phí huấn luyện bằng cách
cho phép các mạng ứng viên chia sẻ một supernet được tham
số hóa quá mức chung. Các mô hình tốt nhất cho thiết bị mục
tiêu có thể được tìm thấy bằng cách trực tiếp tìm kiếm một
subnet trong supernet. Các nhà nghiên cứu điện toán di động
cũng đã đề xuất mở rộng các mô hình một cách động để cung
cấp một loạt rộng các đánh đổi tài nguyên-độ chính xác. Hầu
hết chúng áp dụng pruning có cấu trúc hoặc thích ứng kiến
trúc mô hình để tạo ra các mô hình con [13,17,39,57,58] với
các mức chi phí tính toán khác nhau.

Thích ứng mô hình không cần huấn luyện cũng có thể đạt
được bằng cách lắp ráp động các module nơ-ron, đã được
thảo luận trong mạng nơ-ron động [19] và lập trình neuro-
symbolic [41]. Mạng nơ-ron động là một loại DNN hỗ trợ
suy luận linh hoạt dựa trên độ khó của đầu vào. Khi đầu vào
dễ, mạng nơ-ron động có thể giảm tính toán bằng cách bỏ
qua một tập hợp các khối [52,55] hoặc thoát từ các lớp giữa
[2,12,31,32]. Mặc dù các subnet khác nhau (một subnet là
một đường dẫn trong NN động) có tải tính toán khác nhau,
chúng không thể được tùy chỉnh cho các nhiệm vụ khác nhau,
và tính động của mô hình phụ thuộc vào đầu vào cũng không
phù hợp cho các môi trường biên. Lập trình neuro-symbolic
chứng minh khả năng giải quyết các nhiệm vụ khác nhau với
cùng một tập hợp các module nơ-ron bằng cách kết hợp chúng
một cách ngữ nghĩa. Cụ thể, mô hình để giải quyết một nhiệm
vụ phức tạp có thể được viết như một chương trình gọi các
module chức năng nhỏ hơn. Tuy nhiên, các phương pháp lập
trình neuro-symbolic hiện có thiếu tính linh hoạt và khả năng
tùy chỉnh hướng tài nguyên, vì các module thường tĩnh và
được định nghĩa rõ ràng.

Tuy nhiên, vẫn còn thách thức trong việc tối ưu hóa quy
trình tùy chỉnh mô hình cho cả nhiệm vụ và thiết bị cùng một
lúc. Tùy chỉnh hướng nhiệm vụ và tùy chỉnh hướng tài nguyên
có phần mâu thuẫn. Để đạt được tùy chỉnh nhiệm vụ nhanh,
mô hình được mong muốn có khả năng tổng quát hóa qua
nhiệm vụ, điều này thường yêu cầu một mô hình được huấn
luyện trước thống nhất với khả năng lớn, trong khi triển khai
mô hình đến các thiết bị biên hạn chế tài nguyên đa dạng
yêu cầu các mô hình nhẹ không đồng nhất.

3 THIẾT KẾ NN-FACTORY

Định nghĩa vấn đề. Chính thức, mục tiêu của việc tùy chỉnh
DNN biên là tạo ra một mô hình 𝑓ˆ𝛼,ˆ𝜃 với kiến trúc ˆ𝛼 và
tham số ˆ𝜃 có thể xử lý chính xác nhiệm vụ biên trong khi
thỏa mãn các ràng buộc hiệu suất. tức là

ˆ𝛼,ˆ𝜃=arg min
𝛼,𝜃𝐿(𝑓𝛼,𝜃(𝑋𝑒),𝑌𝑒)
𝑠.𝑡.𝑚𝑒𝑚(𝛼)<𝑀𝐸𝑀𝑒𝑎𝑛𝑑𝑙𝑎𝑡(𝛼)<𝐿𝐴𝑇𝑒(1)

trong đó 𝑋𝑒 và 𝑌𝑒 là đầu vào và đầu ra của các mẫu dữ liệu
nhiệm vụ biên, 𝐿 là loss dự đoán, và 𝑀𝐸𝑀𝑒 và 𝐿𝐴𝑇𝑒 là
các giới hạn bộ nhớ và độ trễ tại môi trường biên.

Cụ thể, chúng tôi tập trung vào việc tạo ra các mô hình cho
các nhiệm vụ trong một không gian tổ hợp, tức là mỗi nhiệm
vụ được mô tả như một sự kết hợp của một số thuộc tính
(ví dụ: màu sắc, trạng thái, thực thể, v.v.) và các thuộc tính
được chia sẻ qua các nhiệm vụ khác nhau (ví dụ: xe tải đỏ,
mèo trắng, v.v.). Các nhà phát triển có thể linh hoạt định
nghĩa không gian nhiệm vụ theo các tình huống biên mục
tiêu của họ.

4

--- TRANG 5 ---
Mô hình Sinh cho các Mô hình: Tùy chỉnh DNN nhanh chóng cho các Nhiệm vụ Đa dạng và Ràng buộc Tài nguyên Preprint, , 2023

Công việc của chúng tôi lấy cảm hứng từ tính modular và
khả năng tổ hợp động của mạng nơ-ron và AI sinh. Tầm nhìn
của chúng tôi là tạo ra một hệ thống sinh một-cho-tất-cả trong
đó các mô hình DNN cụ thể theo biên khác nhau có thể được
tạo ra trực tiếp bằng cách cấu hình và lắp ráp các module
nơ-ron được định nghĩa trước, như được minh họa trong Hình 1.

Việc thực hiện tầm nhìn này rất thách thức vì (1) khó thiết
kế và phát triển các module có thể được lắp ráp để thực hiện
các nhiệm vụ khác nhau và đáp ứng các ràng buộc tài nguyên
khác nhau (2) ngay cả khi các module như vậy được tạo ra,
việc tạo ra một mô hình phù hợp cho một tình huống biên
cụ thể vẫn có thể khó khăn do không gian tổ hợp khổng lồ
của các ứng viên mô hình.

Chúng tôi cố gắng giải quyết những thách thức này bằng
một phương pháp end-to-end có tên NN-Factory. Chúng tôi
nhấn mạnh rằng NN-Factory là một paradigm sinh đầu tiên
từng có về tùy chỉnh mô hình cho các tình huống biên đa
dạng, cho phép sinh mô hình nhanh chóng không cần huấn
luyện cho các nhiệm vụ và ràng buộc tài nguyên khác nhau.

3.1 Tổng quan

Ý tưởng chính của NN-Factory là huấn luyện cùng nhau một
tập hợp các module nơ-ron và một bộ tạo chiến lược lắp ráp
module (tức là assembler) theo cách có căn cứ nhiệm vụ và
độ thưa. Cụ thể, các module được cắt từ một mạng nơ-ron
được huấn luyện trước (supernet), điều này tránh được việc
phát triển cồng kềnh các module riêng lẻ với chi phí khả
năng diễn giải hạn chế. Đồng thời, bằng cách học assembler
trong quá trình huấn luyện với nhiều nhiệm vụ và yêu cầu
độ thưa khác nhau, NN-Factory có thể trực tiếp tạo ra kiến
trúc mô hình cho một nhiệm vụ và yêu cầu độ thưa mới với
một lần forward pass duy nhất, giảm đáng kể không gian
tìm kiếm mô hình.

Hình 3 cho thấy tổng quan về NN-Factory. Nó chứa ba thành
phần chính, bao gồm một supernet với các module có thể
cắt được, một assembler nhận biết yêu cầu, và một module
tìm kiếm kiến trúc nhẹ. và một bộ đánh giá hiệu suất biên
cụ thể theo thiết bị. Supernet chứa các module cơ bản có
thể được kết hợp linh hoạt để tạo thành các mô hình tùy chỉnh
(subnet). Assembler module nhận biết yêu cầu là một mô hình
sinh tạo ra các cấu hình lắp ráp module dựa trên nhiệm vụ
và yêu cầu độ thưa cho trước. Mỗi cấu hình được tạo ra ánh
xạ đến một ứng viên mô hình. Module tìm kiếm kiến trúc
nhẹ tìm ra mô hình tối ưu bằng cách lặp đi lặp lại tìm kiếm
và đánh giá các ứng viên mô hình theo một bộ đánh giá hiệu
suất cụ thể theo biên.

Với một tình huống biên cho trước (được mô tả bởi nhiệm
vụ, loại thiết bị, và ràng buộc tài nguyên), quy trình tùy chỉnh
mô hình của NN-Factory bao gồm các bước sau:

(1)Module tìm kiếm kiến trúc nhẹ đề xuất một yêu cầu sinh
mô hình < task, activation limit >, trong đó activation limit
là tỷ lệ tối đa của các module được kích hoạt trong mô hình.

(2)Assembler nhận biết yêu cầu dự đoán một cấu hình lắp
ráp module dựa trên yêu cầu. Cấu hình mô tả cách tạo
ra một ứng viên mô hình bằng cách kích hoạt và lắp ráp
các module trong supernet.

(3)Bộ đánh giá hiệu suất cụ thể theo thiết bị đánh giá cấu
hình được tạo ra (tức là ứng viên mô hình) so với các
ràng buộc tài nguyên biên. Nếu các ràng buộc được thỏa
mãn và ngân sách bộ nhớ/độ trễ được sử dụng đầy đủ,
thì trả về ứng viên hiện tại. Ngược lại, quay lại bước (1)
với một yêu cầu sinh mới.

Các phần tiếp theo sẽ giới thiệu các thành phần chính chi
tiết hơn.

3.2 Supernet với các Module có thể Cắt được

Supernet chịu trách nhiệm cung cấp các module nơ-ron cơ
bản có thể được lắp ráp lại để đáp ứng các nhiệm vụ và ràng
buộc tài nguyên khác nhau. Việc cắt một mạng nơ-ron hiện
có cho các khối chức năng khác nhau để có khả năng diễn
giải đã được nghiên cứu trước đây [61–63], nhưng các module
được cắt thường thô và khó lắp ráp lại. Chúng tôi quyết định
trực tiếp huấn luyện một supernet có thể cắt được để có sự
tách biệt modular và khả năng tổ hợp tốt hơn.

Chúng tôi xây dựng supernet modular bằng cách mở rộng
một mạng backbone hiện có. Mạng backbone trích xuất các
đặc trưng từ các đầu vào cho trước và đưa ra các dự đoán
cuối cùng. Chúng tôi có thể hỗ trợ các backbone phổ biến
dựa trên Mạng Nơ-ron Tích chập (CNN) và Transformer,
chẳng hạn như ResNet [20], EfficientNet [47], và Vision
Transformer [11].

Backbone CNN. Đầu tiên, chúng tôi giới thiệu cách chuyển
đổi một lớp tích chập trong kiến trúc CNN thành các module
có thể cắt được. Cho một feature map 𝑥 làm đầu vào, đầu
ra của lớp tích chập thứ 𝑙 là 𝑂𝑙(𝑥). Trong một CNN thông
thường, 𝑂𝑙(𝑥) được tính như sau:

𝑂𝑙
𝑖=𝜎(𝐹𝑙
𝑖∗𝐼𝑙(𝑥)) (2)

trong đó 𝑂𝑙
𝑖 là kênh thứ 𝑖 của 𝑂𝑙(𝑥), 𝐹𝑙
𝑖 là bộ lọc thứ 𝑖, 𝜎(·)
biểu thị hàm kích hoạt phi tuyến theo từng phần tử và ∗
biểu thị phép tích chập. feature map đầu ra 𝑂𝑙(𝑥) được thu
được bằng cách áp dụng tất cả các bộ lọc 𝐹𝑝
𝑖 trong lớp hiện
tại lên feature map đầu vào 𝐼𝑙(𝑥).

Trong NN-Factory, chúng tôi coi mỗi bộ lọc tích chập như
một module có thể được kích hoạt có điều kiện. Bằng cách
kích hoạt các kết hợp khác nhau của các bộ lọc trong một
lớp tích chập, các lớp kết quả có thể được coi là có các chức
năng khác nhau.

Dựa trên sự phân tách modular như vậy, chúng tôi giới thiệu
một vector gate 𝑔 để điều khiển việc kích hoạt các module.
Với vector gate, việc tính toán feature map 𝑂𝑙
𝑖 trong Phương
trình 2 được diễn đạt lại như sau:

𝑂𝑙
𝑖=𝜎(𝐹𝑙
𝑖∗𝐼𝑙(𝑥))·𝑔𝑙
𝑖 (3)

5

--- TRANG 6 ---
Preprint, , 2023 Xu và Li et al.

Hình 3: Tổng quan kiến trúc của NN-Factory.

Hình 4: Minh họa về lớp tích chập modular với các bộ lọc
tích chập được kích hoạt có điều kiện. 𝑂 đại diện cho feature
map đầu ra của một lớp tích chập với bốn kênh. Mỗi phần
tử trong vector gate ánh xạ đến một kênh. Bằng cách áp
dụng các vector gate khác nhau, các kết hợp khác nhau
của các bộ lọc được kích hoạt.

trong đó 𝑔𝑙
𝑖 là mục trong 𝑔 tương ứng với bộ lọc thứ 𝑖 tại
lớp 𝑙 và 0 là một feature map 2-D với tất cả các phần tử
bằng 0, chỉ khi 𝑔𝑙
𝑖 bằng 1, bộ lọc thứ 𝑖 mới được áp dụng
lên 𝐼𝑙 để trích xuất đặc trưng. Hình 4 mô tả quy trình tính
toán như được mô tả ở trên.

Supernet modular được thu được bằng cách áp dụng quy
trình modular hóa cho các lớp tích chập chính trong mô hình
CNN. Lớp batch normalization sau mỗi lớp tích chập được
cắt theo cách từng kênh và được điều khiển bởi cùng một
vector gate. Đáng chú ý rằng, trong các kiến trúc CNN sâu
hiện đại như ResNet, EfficientNet, và MobileNet, các lớp
tích chập được tổ chức thành nhiều khối cơ bản. Chúng tôi
không chuyển đổi các lớp tích chập ở cuối mỗi khối cơ bản
để tránh xung đột hình dạng với các kết nối residual. Hình
6(a) và (b) minh họa các khối cơ bản CNN phổ biến được
tích hợp với các gate module có thể học.

Backbone Transformer. Chúng tôi cũng hỗ trợ tạo ra các
supernet modular từ các backbone Transformer. Các thành
phần chính của Transformer bao gồm các lớp self-attention
và mạng feed-forward (FFN). Theo các nghiên cứu gần đây
[10], các thành phần FFN lưu trữ các kiến thức thực tế khác
nhau được học từ dữ liệu. Một FFN là một mạng kết nối
đầy đủ hai lớp, xử lý một biểu diễn đầu vào 𝑥∈ℝ^{𝑑_{𝑚𝑜𝑑𝑒𝑙}} như sau:

ℎ=𝑥𝑊₁
𝐹(𝑥)=𝜎(ℎ)𝑊₂(4)

trong đó 𝑊₁∈ℝ^{𝑑_{𝑚𝑜𝑑𝑒𝑙}×𝑑_{𝑓𝑓}} và 𝑊₂∈ℝ^{𝑑_{𝑓𝑓}×𝑑_{𝑚𝑜𝑑𝑒𝑙}} là
các ma trận trọng số.

Kiến trúc Transformer không được thiết kế modular, nhưng
nó có thể được chuyển đổi thành một mô hình Mixture-of-
Experts (MoE) tương đương [23,64], trong đó mỗi expert
có thể được coi như một module chức năng được kích hoạt
có điều kiện. Lấy cảm hứng từ ý tưởng này, chúng tôi đề
xuất modular hóa các lớp FFN trong các backbone Transformer
và diễn đạt lại việc tính toán 𝐹(𝑥) trong Phương trình 4
như sau:

ℎ=𝑥𝑊₁
ℎ′=ℎ·𝑔_{𝐹𝐹𝑁}
𝐹(𝑥)=𝜎(ℎ′)𝑊₂(5)

trong đó 𝑔_{𝐹𝐹𝑁} đại diện cho việc lựa chọn gate cho lớp
FFN hiện tại. Nếu vị trí thứ 𝑖 trong 𝑔_{𝐹𝐹𝑁} là 0, vị trí tương
ứng trong ℎ′ cũng được đặt thành 0, cho biết các tham số
trong các phần tương ứng của 𝑊₁ và 𝑊₂ không được kích
hoạt. Trong Hình 5, chúng tôi chứng minh các tham số vẫn
không hoạt động dưới sự lựa chọn của gate.

Tương tự, bằng cách áp dụng quy trình modular hóa cho
tất cả các FFN trong một backbone Transformer, chúng tôi
có thể thu được một supernet dựa trên Transformer cho
NN-Factory, một minh họa với backbone ViT được hiển thị
trong Hình 6(c).

3.3 Assembler Module Nhận biết Yêu cầu

Assembler module đóng vai trò quan trọng trong NN-Factory
- nó tạo ra các vector gate điều khiển việc kích hoạt các
module nơ-ron trong supernet, sao cho các module được
kích hoạt có thể được lắp ráp để tạo ra một ứng viên mô hình.

Đầu vào của assembler module là một yêu cầu sinh mô hình,
được biểu diễn như một bộ < 𝑡𝑎𝑠𝑘, 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡 >. 𝑡𝑎𝑠𝑘
là mô tả của nhiệm vụ mục tiêu trong tình huống biên. Như
đã đề cập trong Phần 3, một nhiệm vụ có thể được biểu diễn
như một sự kết hợp của một số thuộc tính. Ví dụ, nhiệm vụ
'phát hiện chó đen' được biểu diễn như {𝑏𝑙𝑎𝑐𝑘, 𝑑𝑜𝑔} và 'phát
hiện mèo trắng' là {𝑤ℎ𝑖𝑡𝑒, 𝑐𝑎𝑡}. 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡 là một yêu
cầu độ thưa điều chỉnh tỷ lệ các module được kích hoạt trong
mô hình được tạo ra. Một giới hạn thấp hơn sẽ khuyến khích
assembler tạo ra các ứng viên mô hình nhỏ hơn.

Đầu ra của assembler là các vector gate 𝑔 như được mô tả
trong Phần 3.2. Mỗi vector gate 𝑔𝑙 tương ứng với một lớp
modular 𝑙 trong supernet và xác định việc kích hoạt các
module trong lớp.

Kiến trúc mạng của assembler được hiển thị trong Hình 7.
Nó bao gồm ba thành phần chính, bao gồm một bộ mã hóa
yêu cầu, một bộ mã hóa lựa chọn, và một danh sách các
layer gater. Bộ mã hóa yêu cầu chuyển đổi mô tả nhiệm vụ
và yêu cầu độ thưa 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡 thành một embedding.

6

--- TRANG 7 ---
Mô hình Sinh cho các Mô hình: Tùy chỉnh DNN nhanh chóng cho các Nhiệm vụ Đa dạng và Ràng buộc Tài nguyên Preprint, , 2023

Hình 5: Minh họa về lớp FFN modular trong các backbone
Transformer. Đặc trưng trung gian ℎ của FFN được kích
hoạt có điều kiện dọc theo chiều ẩn. Mỗi phần tử trong
vector gate ánh xạ đến một phần tử trong ℎ. Theo việc
kích hoạt của ℎ, các chiều tương ứng trong 𝑊₁ và 𝑊₂
có thể được pruned một cách chọn lọc.

Hình 6: Tích hợp gate cho các backbone khác nhau. (a)
EfficientNet và MobileNet. (b) ResNet. (c) ViT.

Chúng tôi sử dụng mã hóa one-hot cho thành phần 𝑡𝑎𝑠𝑘,
được ký hiệu bởi 𝑒𝑛𝑐_{𝑡𝑎𝑠𝑘}. Mỗi bit trong 𝑒𝑛𝑐_{𝑡𝑎𝑠𝑘} đại
diện cho sự hiện diện của một thuộc tính cụ thể. Ví dụ, trong
nhiệm vụ 'phát hiện chó đen', các phần tử cho 'đen' và 'chó'
được đặt thành một và những phần tử khác được đặt thành
không trong mã hóa nhiệm vụ. Đối với thành phần 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡,
chúng tôi sử dụng Positional Encoding [49] để mã hóa tỷ
lệ giới hạn, được ký hiệu bởi 𝑒𝑛𝑐_{𝑙𝑖𝑚𝑖𝑡}. Hai mã hóa 𝑒𝑛𝑐_{𝑡𝑎𝑠𝑘}
và 𝑒𝑛𝑐_{𝑙𝑖𝑚𝑖𝑡} được nối với nhau như mã hóa yêu cầu 𝑒𝑛𝑐_{𝑟𝑒𝑞}.

Bộ mã hóa lựa chọn chuyển đổi mã hóa nhiệm vụ 𝑒𝑛𝑐_{𝑟𝑒𝑞}
thành một biểu diễn trung gian 𝑒𝑛𝑐_{𝑠𝑒𝑙} chứa kiến thức về
việc lựa chọn gate toàn mô hình. Chúng tôi sử dụng một lớp
kết nối đầy đủ tiếp theo bởi một batch normalization (BN)
và một đơn vị ReLU để thực hiện chuyển đổi này. Mã hóa
lựa chọn toàn cầu 𝑒𝑛𝑐_{𝑠𝑒𝑙} sau đó được đưa vào mỗi layer
gater để tính toán việc kích hoạt cho mỗi lớp. Layer gater
cũng là một lớp kết nối đầy đủ tiếp theo bởi BN và đơn vị
ReLU. Giá trị đầu ra của nó 𝑤𝑒𝑖𝑔ℎ𝑡𝑙∈ℝ^{𝐷_𝑙} đại diện cho
trọng số của các module trong lớp 𝑙, trong đó 𝐷_𝑙 là số
lượng module. Việc lựa chọn gate cho lớp 𝑔𝑙 được thu được
bằng cách rời rạc hóa 𝑤𝑒𝑖𝑔ℎ𝑡𝑙 với một ngưỡng (tức là
𝑔𝑙=𝑤𝑒𝑖𝑔ℎ𝑡𝑙>𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑). Chúng tôi đặt 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑=0.5
theo mặc định trong triển khai của chúng tôi.

Hình 7: Kiến trúc của assembler module nhận biết yêu cầu,
được tích hợp với backbone modular.

3.4 Huấn luyện Cùng nhau và Triển khai Riêng biệt

Huấn luyện Cùng nhau Supernet-Assembler. Supernet
modular và assembler có mối quan hệ chặt chẽ - assembler
tạo ra việc kích hoạt các module, và việc kích hoạt đại diện
cho các ứng viên mô hình với supernet. Do đó, chúng tôi
huấn luyện chúng cùng nhau để làm cho chúng hoạt động
nhất quán cùng nhau.

Việc huấn luyện được tiến hành với một tập hợp các nhiệm
vụ huấn luyện và dữ liệu được gắn nhãn thuộc về mỗi nhiệm
vụ. Mỗi mẫu có thể được biểu diễn như một bộ {𝑡𝑎𝑠𝑤,
𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡, 𝑥, ŷ}, trong đó 𝑥 và ŷ là đầu vào và
nhãn. Tất cả các mẫu được trộn lẫn với nhau và xáo trộn
trong quá trình huấn luyện.

Trong mỗi lần forward pass, chúng tôi tính toán việc lựa
chọn gate 𝑔 bằng cách đưa 𝑡𝑎𝑠𝑘 và 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡 vào
assembler, thu được subnet 𝑀_𝑔 với việc lựa chọn gate 𝑔,
và nhận được dự đoán 𝑦 bằng cách đưa 𝑥 vào subnet 𝑀_𝑔.
Loss được tính toán bằng cách kiểm tra dự đoán 𝑦 và việc
lựa chọn gate 𝑔, tức là

𝐿=𝑇𝐿(𝑦,ŷ)+𝜆𝐺𝐿(𝑔,𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡) (6)

𝑇𝐿(𝑦,ŷ) là loss cho nhiệm vụ huấn luyện, khuyến khích
subnet được tạo ra tạo ra các dự đoán chính xác dựa trên
nhiệm vụ cơ bản. 𝐺𝐿(·) là loss cho việc lựa chọn gate,
khuyến khích assembler tạo ra các subnet thỏa mãn yêu cầu
độ thưa. Nếu tỷ lệ các số một trong 𝑔 thấp hơn 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡
cho trước, 𝐺𝐿(·) bị vô hiệu hóa. Ngược lại, 𝐺𝐿(·) là lỗi
bình phương trung bình (MSE) giữa tỷ lệ các số một trong
𝑔 và 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡. 𝜆 là một siêu tham số để cân
bằng hai mục tiêu, được đặt thành 100 theo mặc định.

Lưu ý rằng loss lựa chọn gate 𝐺𝐿(·) được tính toán với
việc lựa chọn gate nhị phân rời rạc 𝑔, nhưng đầu ra thực
tế của assembler là trọng số gate liên tục 𝑤𝑒𝑖𝑔ℎ𝑡, điều này
có thể dẫn đến khó khăn trong việc truyền lỗi ngược. Để
giải quyết vấn đề này, chúng tôi sử dụng một phương pháp
gọi là Improved SemHash [26,27] để thực hiện thủ thuật.
Việc tính toán được thực hiện như sau:

𝑔_𝛼=1(𝑤𝑒𝑖𝑔ℎ𝑡>0)
𝑔_𝛽=max(0,min(1,1.2𝜎(𝑤𝑒𝑖𝑔ℎ𝑡)−0.1))

Ở đây, 𝑔_𝛼 là một vector nhị phân, trong khi 𝑔_𝛽 là một
vector gate có giá trị thực với tất cả các mục nằm trong
khoảng [0.0, 1.0]. 𝑔_𝛼 có thuộc tính nhị phân mong muốn
mà chúng tôi muốn sử dụng trong huấn luyện, nhưng gradient
của 𝑔_𝛼 bằng không cho hầu hết các giá trị của 𝑔. Ngược
lại, gradient của 𝑔_𝛽 được định nghĩa rõ ràng, nhưng 𝑔_𝛽
không phải là một vector nhị phân. Trong lần forward pass
trong quá trình huấn luyện, chúng tôi ngẫu nhiên sử dụng
𝑔=𝑔_𝛼 cho một nửa số mẫu và sử dụng 𝑔=𝑔_𝛽 cho phần
còn lại. Khi 𝑔_𝛼 được sử dụng, chúng tôi tuân theo giải
pháp trong [26,27] và định nghĩa gradient của 𝑔_𝛼 giống
như gradient của 𝑔_𝛽 trong quá trình truyền ngược. Đối
với đánh giá và suy luận, chúng tôi luôn sử dụng các gate
rời rạc.

Triển khai Riêng biệt của Subnet. Sau khi huấn luyện trước
cùng nhau, chúng tôi có thể tạo ra việc lựa chọn gate dựa
trên các yêu cầu nhiệm vụ và độ thưa khác nhau. Khi việc
lựa chọn gate được xác định, mạng assembler và các module
bị vô hiệu hóa trong supernet modular không còn hữu ích
nữa. Do đó, khi triển khai mô hình, chúng tôi chỉ cần truyền
và triển khai subnet được lắp ráp với các module hoạt động.
Ví dụ, với supernet dựa trên CNN, chúng tôi có thể pruning
các bộ lọc không hoạt động dựa trên việc lựa chọn gate.
Bằng cách làm như vậy, subnet kết quả sẽ có kích thước mô
hình, sử dụng bộ nhớ, và độ trễ giảm đáng kể. Đối với supernet
dựa trên Transformer, chúng tôi có thể đạt được kết quả
tương tự bằng cách pruning các tham số tương ứng trong
𝑊₁ và 𝑊₂ dựa trên việc lựa chọn gate. Mô hình được triển
khai không yêu cầu bất kỳ thay đổi kiến trúc hay huấn luyện
tham số nào thêm.

7

--- TRANG 8 ---
Preprint, , 2023 Xu và Li et al.

3.5 Tìm kiếm Kiến trúc Nhẹ

Các module và assembler được huấn luyện cùng nhau cho
phép chúng tôi hiệu quả tạo ra các ứng viên mô hình với
các nhiệm vụ và mức độ thưa khác nhau. Dựa trên khả năng
như vậy, chúng tôi tiếp tục giới thiệu một chiến lược tìm
kiếm kiến trúc nhẹ để tìm mô hình tối ưu cho mỗi tình huống
biên.

Quá trình tìm kiếm kiến trúc được hướng dẫn bởi một bộ
đánh giá hiệu suất cụ thể theo thiết bị, nhận một ứng viên
mô hình làm đầu vào và cho biết liệu mô hình có thỏa mãn
các ràng buộc tài nguyên hay không (tức là độ trễ suy luận
nhỏ hơn ngân sách độ trễ và chi phí bộ nhớ nhỏ hơn ngân
sách bộ nhớ). Chúng tôi có thể trực tiếp đánh giá hiệu suất
trên các thiết bị biên mục tiêu, tuân theo thực hành lựa chọn
subnet trên thiết bị của AdaptiveNet [53].

Thay vào đó, khi việc triển khai supernet và assembler đến
thiết bị biên không dễ dàng, chúng tôi có thể xây dựng một
bộ dự đoán hiệu suất để đánh giá các ứng viên mô hình với
dữ liệu profiling được thu thập từ các thiết bị biên. Một
mô hình hiệu suất thiết bị dựa trên profiling như vậy là một
thực hành phổ biến trong điện toán di động/biên [17], trong
khi ở đây chúng tôi thực hiện một số đơn giản hóa hợp lý
dựa trên thiết kế modular của chúng tôi. Cụ thể, chúng tôi
thực hiện một phương pháp profiling và mô hình hóa hiệu
suất theo lớp, trong đó tổng độ trễ của mô hình bằng tổng
của tất cả các lớp (với một bias tĩnh), và độ trễ của mỗi
lớp phụ thuộc vào vector gate được dự đoán bởi assembler
module. Việc mô hình hóa tiêu thụ bộ nhớ tương tự, được
xác định bởi tiêu thụ bộ nhớ tĩnh (các tham số mô hình)
và bộ nhớ đỉnh tại runtime (đặc trưng trung gian lớn nhất).
Để xây dựng các bộ dự đoán này, chúng tôi tạo ra một tập
hợp (2000 trong các thí nghiệm của chúng tôi) các vector
gate ngẫu nhiên, thu được các subnet tương ứng, đo các
chỉ số hiệu suất của các subnet này trên các thiết bị biên
mục tiêu, và sử dụng dữ liệu được thu thập để huấn luyện
các bộ dự đoán hiệu suất với hồi quy tuyến tính. Bộ dự đoán
có độ chính xác cao, với cả độ chính xác dự đoán độ trễ và
độ chính xác dự đoán bộ nhớ cao hơn 96% trên bốn thiết bị
biên điển hình, như được hiển thị trong Hình 8. Chúng tôi
sử dụng bộ dự đoán theo mặc định trong NN-Factory.

(a) Độ trễ
(b) Bộ nhớ

Hình 8: Độ chính xác dự đoán hiệu suất của các mô hình
được tạo ra bởi NN-Factory trên bốn thiết bị biên.

Thuật toán 1 Tìm kiếm Kiến trúc Nhẹ
Đầu vào: Tình huống biên mục tiêu 𝑒, mô tả nhiệm vụ 𝑡𝑎𝑠𝑘_𝑒,
yêu cầu độ trễ 𝐿𝐴𝑇_𝑒, yêu cầu bộ nhớ 𝑀𝐸𝑀_𝑒, bộ đánh giá hiệu
suất cụ thể theo biên 𝐸𝑣𝑎𝑙𝑢𝑎𝑡𝑜𝑟_𝑒.
Đầu ra: Mô hình biên tùy chỉnh 𝑀
1: 𝑔𝑎𝑡𝑒_{𝑜𝑝𝑡}←𝑁𝑈𝐿𝐿
2: for 𝑙𝑖𝑚𝑖𝑡_𝑖= 1%; 𝑙𝑖𝑚𝑖𝑡_𝑖< 1; 𝑙𝑖𝑚𝑖𝑡_𝑖+= step do
3: 𝑒𝑛𝑐_𝑖←𝑅𝑒𝑞𝑢𝑖𝑟𝑒𝑚𝑒𝑛𝑡_𝐸𝑛𝑐𝑜𝑑𝑖𝑛𝑔(𝑡𝑎𝑠𝑘_𝑒,𝑙𝑖𝑚𝑖𝑡_𝑖)
4: 𝑔𝑎𝑡𝑒_𝑖←𝐴𝑠𝑠𝑒𝑚𝑏𝑙𝑒𝑟(𝑒𝑛𝑐_𝑖)
5: 𝑙𝑎𝑡𝑒𝑛𝑐,𝑚𝑒𝑚𝑒𝑛𝑐←𝐸𝑣𝑎𝑙𝑢𝑎𝑡𝑜𝑟_𝑒(𝑔𝑎𝑡𝑒_𝑖)
6: if 𝑙𝑎𝑡𝑒𝑛𝑐>𝐿𝐴𝑇_𝑒 or 𝑚𝑒𝑚𝑒𝑛𝑐>𝑀𝐸𝑀_𝑒 then
7: break
8: end if
9: 𝑔𝑎𝑡𝑒_{𝑜𝑝𝑡}←𝑔𝑎𝑡𝑒_𝑖
10: end for
11: return subnet được cắt từ supernet với 𝑔𝑎𝑡𝑒_{𝑜𝑝𝑡}

Dựa trên bộ đánh giá hiệu suất, chúng tôi có thể phân tích
độ trễ và bộ nhớ của các ứng viên mô hình. Vì càng nhiều
module được kích hoạt thường dẫn đến độ chính xác cao
hơn (xem Phần 5.2), mô hình tối ưu cho một tình huống
biên là mô hình có tỷ lệ kích hoạt module cao nhất trong
khi thỏa mãn các yêu cầu về bộ nhớ và độ trễ. Thuật toán
1 cho thấy chiến lược tìm kiếm kiến trúc nhẹ của chúng tôi.
Chúng tôi bắt đầu từ giới hạn kích hoạt module thấp nhất
𝑙𝑖𝑚𝑖𝑡_𝑖=1%, và lặp đi lặp lại tăng giới hạn. Đối với mỗi
giới hạn kích hoạt, chúng tôi tạo ra việc lựa chọn gate bằng
mạng assembler, và thu được độ trễ và bộ nhớ liên quan đến
việc lựa chọn gate. Việc lựa chọn gate ứng viên cuối cùng
đáp ứng các ràng buộc môi trường biên được xác định và
trả về. Cuối cùng, chúng tôi tạo ra subnet với việc lựa chọn
gate kết quả, có thể được triển khai trực tiếp đến biên mà
không cần bất kỳ xử lý nào thêm.

4 TRIỂN KHAI

Chúng tôi triển khai phương pháp của mình bằng Python.
Phần huấn luyện dựa trên PyTorch. Các mô hình được tạo
ra với PyTorch và được triển khai đến các thiết bị biên sử
dụng framework TensorFlow Lite cho di động và PyTorch
cho Desktop và Jetson.

Chi tiết Kiến trúc và Huấn luyện. Trong các layer gater
của assembler, chúng tôi sử dụng các lớp batch normalization
riêng biệt cho mã hóa lựa chọn của các lớp khác nhau, điều
này có thể ảnh hưởng đến chất lượng của mô hình được tạo
ra. Để tăng cường tính ổn định huấn luyện và cải thiện chất
lượng của mô hình được tạo ra, chúng tôi bổ sung tập nhiệm
vụ huấn luyện với các nhiệm vụ bổ sung để kết hợp thêm
các kết hợp của các thuộc tính nhiệm vụ, có thể tăng cường
hiểu biết của mô hình về mã hóa nhiệm vụ của chúng tôi.
Để kết hợp các yêu cầu độ thưa rộng hơn trong quá trình
huấn luyện mà không làm giảm chất lượng mô hình, chúng
tôi sử dụng nhiều layer gater để tăng cường khả năng của
mô hình, dựa trên các nguyên tắc của Mixture of Experts
(MoE), mỗi gater chuyển đổi mã hóa lựa chọn thành việc
lựa chọn gate. Ngoài ra, một mạng gating được giới thiệu
để xác định trọng số của các đầu ra từ mỗi gater tại mỗi lớp.

8

--- TRANG 9 ---
Mô hình Sinh cho các Mô hình: Tùy chỉnh DNN nhanh chóng cho các Nhiệm vụ Đa dạng và Ràng buộc Tài nguyên Preprint, , 2023

5 ĐÁNH GIÁ

Chúng tôi tiến hành các thí nghiệm để trả lời các câu hỏi sau:
(1) NN-Factory có khả năng tạo ra các mô hình cụ thể theo
biên không? Chất lượng của các mô hình được tạo ra như
thế nào? (2) Chi phí của NN-Factory là bao nhiêu? (3) Khả
năng sinh mô hình của NN-Factory tổng quát hóa tốt đến
mức nào đối với các tình huống biên chưa thấy?

5.1 Thiết lập Thí nghiệm

Nhiệm vụ và Tập dữ liệu. Chúng tôi đánh giá hiệu suất
của NN-Factory trên hai cài đặt tùy chỉnh mô hình.

• Trả lời Câu hỏi Thị giác Số học (NumVQA). Đây là một
cài đặt đơn giản để phân tích hiệu suất của việc sinh mô
hình cụ thể theo nhiệm vụ và tài nguyên. Nhiệm vụ là
trả lời một câu hỏi có-hoặc-không (ví dụ: "Có hai số
chẵn không?") dựa trên một hình ảnh đầu vào chứa bốn
chữ số. Chúng tôi công thức hóa khoảng 60 câu hỏi và
tổng hợp các hình ảnh sử dụng tập dữ liệu MNIST [33].
Hiệu suất của các mô hình được tạo ra được đo bằng
độ chính xác phân loại.

• Phát hiện Đối tượng có Thuộc tính (AttrOD). Đây là
một cài đặt thực tế hơn, trong đó mỗi nhiệm vụ là phát
hiện các đối tượng có thuộc tính cụ thể trong một hình
ảnh và dự đoán các hộp giới hạn đối tượng và danh mục.
Chúng tôi sử dụng một mô hình trả lời câu hỏi thị giác
[29] để chú thích màu sắc đối tượng trong tập dữ liệu
COCO2017 [35] và hợp nhất các màu được xác định
để tạo thành các thuộc tính mục tiêu. Chúng tôi chọn
5 thuộc tính (1-trắng, 2-đồng, 3-than, 4-đỏ tươi, 5-xanh
lá) từ 4 danh mục và kết hợp chúng để xây dựng các
nhiệm vụ huấn luyện của chúng tôi. Tổng số nhiệm vụ
khoảng 140, và mỗi nhiệm vụ có số lượng mẫu khác
nhau, từ hàng trăm đến hàng chục nghìn. Hiệu suất của
các mô hình phát hiện được đo bằng độ chính xác trung
bình (mAP) trên ngưỡng Intersection over Union 0.5
(mAP@0.5).

Backbone Mô hình. Chúng tôi xem xét các backbone CNN
và Transformer phổ biến trong thí nghiệm này, bao gồm
ResNet [20], ViT [11], cho cài đặt NumVQA và EfficientDet
[48] cho cài đặt AttrOD.

Baseline. Chúng tôi so sánh NN-Factory với hai phương
pháp tùy chỉnh mô hình thông thường:

(1)Retrain - Chúng tôi cố định kiến trúc mô hình và huấn
luyện lại nó với phương pháp học có giám sát tiêu chuẩn
cho mỗi tình huống biên.

(2)Prune&Tune - Chúng tôi huấn luyện một mô hình thống
nhất. Với một tình huống biên cho trước, chúng tôi pruning
và tinh chỉnh mô hình được huấn luyện trước để phù
hợp với các yêu cầu biên với một phương pháp pruning
SOTA [14].

Cả hai đều yêu cầu huấn luyện với dữ liệu cụ thể theo biên.
Chúng tôi sử dụng cùng backbone với các baseline này và
huấn luyện chúng cho đến khi hội tụ. Chúng tôi không bao
gồm các phương pháp sinh/mở rộng mô hình không cần huấn
luyện khác [4,13,17,53] vì chúng không hỗ trợ tùy chỉnh
hướng nhiệm vụ.

Môi trường Biên. Chúng tôi xem xét ba thiết bị biên bao
gồm một Điện thoại thông minh Android (Xiaomi 12) với
bộ xử lý Snapdragon® 8 Gen 1 và bộ nhớ 12GB, một Jetson
AGX Xavier với bộ nhớ 32 GB, và một máy tính để bàn với
Bộ xử lý 12th Gen Intel® Core™ i9-12900K×24 với bộ nhớ
64GB. Kích thước batch đều được đặt thành 1 trên ba thiết
bị để mô phỏng khối lượng công việc thực tế. Chúng tôi sử
dụng các ngân sách độ trễ khác nhau để mô phỏng sự đa
dạng phần cứng trong thiết bị.

5.2 Chất lượng Sinh Mô hình

Chúng tôi tiến hành đánh giá các mô hình được tạo ra bởi
NN-Factory trên NumVQA và AttrOD, tiếp theo là phân tích
toàn diện về chất lượng của chúng.

Chúng tôi đầu tiên đánh giá hiệu quả của các module và
assembler trong cài đặt NumVQA. Chúng tôi đưa các bộ
< 𝑡𝑎𝑠𝑘, 𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛_𝑙𝑖𝑚𝑖𝑡 > khác nhau vào assembler
NN-Factory và để nó tạo ra các mô hình đáp ứng các yêu
cầu. Kết quả được trình bày trong Bảng 2. Nhìn chung,
NN-Factory thể hiện độ chính xác đặc biệt (>99% với backbone
ResNet) qua các nhiệm vụ đa dạng, đồng thời cũng đảm bảo
rằng các mô hình được tạo ra tuân thủ giới hạn kích hoạt
được chỉ định của chúng tôi. Do quy mô nhiệm vụ tương
đối nhỏ, chỉ cần một tỷ lệ kích hoạt thấp hơn.

Tiếp theo, chúng tôi đánh giá hiệu suất end-to-end của
NN-Factory trên AttrOD. Trong Bảng 3, chúng tôi sử dụng
ResNet50 làm backbone và trình bày chất lượng của các mô
hình được tạo ra dưới các yêu cầu độ trễ và bộ nhớ khác
nhau trên các thiết bị khác nhau, và so sánh chúng với các
mô hình baseline. Nhìn chung, NN-Factory liên tục cung
cấp các mô hình chất lượng cao đáp ứng các tiêu chí được
chỉ định trong tất cả các tình huống biên. Nhờ vào việc tìm
kiếm kiến trúc mô hình nhận biết hiệu suất, NN-Factory
có thể sử dụng đầy đủ các ngân sách cho trước (bộ nhớ
hoặc độ trễ). Baseline Retrain không đáp ứng được các yêu
cầu về độ trễ và bộ nhớ vì nó không điều chỉnh kiến trúc
mô hình cho mỗi tình huống.

Bảng 2: Chất lượng của các mô hình được tạo ra trên NumVQA.

Yêu cầu Sinh        ResNet          ViT
Nhiệm vụ    Act. Limit  Acc  Act. Ratio  Acc  Act. Ratio
Có số 0     3%          99.7% 3.0%       97.8% 3.0%
Có số 0     5%          99.8% 3.7%       97.6% 4.0%
Có số 0     10%         99.9% 3.8%       97.8% 5.1%
Chỉ có hai số 1  3%     99.3% 3.0%       96.7% 3.0%
Chỉ có ba số 2   3%     99.4% 3.0%       86.7% 3.0%
Chỉ có bốn số 5  3%     99.1% 3.0%       95.5% 3.0%
Chỉ có một số 0  5%     99.8% 3.7%       94.1% 5.0%
Chỉ có ba số 0   5%     99.4% 3.8%       94.8% 5.0%
Chỉ có một số 3  5%     99.8% 3.7%       87.7% 5.0%
Chỉ có hai số lẻ  3%    99.3% 3.0%       94.3% 3.0%
Chỉ có hai số lẻ  5%    99.4% 4.0%       94.3% 3.8%

Độ chính xác của các mô hình được tạo ra bởi NN-Factory
gần với baseline Retrain và vượt trội hơn baseline Prune&Tune,
mặc dù nó không yêu cầu huấn luyện cụ thể theo biên. Đồng
thời, nó đã đạt được điểm mAP cao hơn đáng kể trên một
số nhiệm vụ (ví dụ: Xe máy {1}, Người {5}) chứa ít mẫu
huấn luyện hơn so với những nhiệm vụ khác. Điều này là
do việc huấn luyện kết hợp các nhiệm vụ, mỗi nhiệm vụ
với các kết hợp thuộc tính khác nhau, cho phép mô hình
đạt được hiểu biết sâu sắc hơn về các nhiệm vụ và do đó
đưa ra các dự đoán chính xác hơn. Độ chính xác của các
mô hình Prune&Tune thấp hơn nhiều so với NN-Factory
và Retrain do kích thước mô hình giảm.

Chúng tôi cũng khám phá sự khác biệt trong chất lượng
mô hình của NN-Factory với các backbone khác nhau. Kết
quả được hiển thị trong Hình 9. NN-Factory thể hiện hành
vi nhất quán qua các backbone supernet khác nhau, biểu
thị tính tổng quát hóa của nó. Tuy nhiên, nó thể hiện sự
khác biệt hiệu suất rõ rệt dựa trên backbone được chọn.
Ví dụ, các mô hình được tạo ra bởi NN-Factory dựa trên
ViT và NN-Factory dựa trên MobileNet thể hiện độ chính
xác thấp hơn. Điều này là do các thuộc tính vốn có của mạng
backbone, ví dụ: hiệu quả mẫu kém của ViT và khả năng
mô hình hạn chế của MobileNet.

Hơn nữa, chúng tôi thực hiện một phân tích để đánh giá
cách NN-Factory quản lý sự đánh đổi giữa chất lượng mô
hình và độ trễ. Kết quả được trình bày trong Hình 10, NN-
Factory có khả năng tạo ra các mô hình chất lượng cao hơn
khi được cung cấp các ràng buộc độ trễ cao hơn. Hình 11
minh họa mối tương quan giữa giới hạn kích hoạt đầu vào
trong quá trình sinh mô hình và tỷ lệ kích hoạt của việc
lựa chọn gate được tạo ra bởi NN-Factory. Với giới hạn
kích hoạt rất thấp, tỷ lệ kích hoạt được tạo ra không đáp
ứng được các yêu cầu. Do đó,

9

--- TRANG 10 ---
Preprint, , 2023 Xu và Li et al.

Bảng 3: Chất lượng của các mô hình được tạo ra bởi NN-Factory và baseline cho các tình huống biên khác nhau trong cài đặt AttrOD. Cột 'nhiệm vụ' là đối tượng mục tiêu theo sau bởi các thuộc tính mong muốn (ví dụ: đen, vàng). Các cột 'Lat' và 'Mem' là độ trễ và bộ nhớ tương đối so với các yêu cầu (𝐿𝐴𝑇_{𝑟𝑒𝑞} và 𝑀𝐸𝑀_{𝑟𝑒𝑞}).

Tình huống Biên                  NN-Factory              Retrain             Prune&Tune
Nhiệm vụ      Thiết bị   𝐿𝐴𝑇_{𝑟𝑒𝑞} 𝑀𝐸𝑀_{𝑟𝑒𝑞} ΔLat ΔMem mAP  ΔLat   ΔMem mAP  ΔLat   ΔMem mAP
Xe đạp {3,4}  Desktop    55ms     0.30GB    -0.11 -0.03 0.31 +156.77 +0.38 0.35 -0.33  -0.23 0.34
Người {2}     Desktop    70ms     0.35GB    -4.55 -0.05 0.27 +141.77 +0.33 0.24 -1.05  -0.22 0.22
Xe {3}        Desktop    85ms     0.40GB    -2.96 -0.01 0.38 +126.77 +0.28 0.42 -1.77  -0.22 0.41
Xe máy {1}    Desktop    100ms    0.45GB    -0.05 -0.02 0.45 +111.77 +0.23 0.23 -0.70  -0.21 0.24
Xe máy {2,3,4} Desktop   115ms    0.50GB    -2.43 -0.04 0.43 +96.77  +0.18 0.44 -1.42  -0.20 0.45
Xe đạp {3,4}  Mobile     350ms    5GB       -47.36 -0.24 0.31 +625.28 +19.97 0.35 -54.4  -0.21 0.34
Xe máy {5}    Mobile     450ms    7GB       -47.54 -0.07 0.53 +525.28 +17.97 0.19 -86.43 -0.21 0.19
Xe {3}        Mobile     550ms    9GB       -25.92 -0.01 0.38 +425.28 +15.97 0.42 -119.46 -0.18 0.41
Người {5}     Jetson     15ms     —         -0.61  —     0.22 +42.98  —     0.15 -0.40  —     0.07
Người {5}     Jetson     20ms     —         -0.81  —     0.22 +37.98  —     0.15 -0.29  —     0.15
Người {5}     Jetson     30ms     —         -0.06  —     0.23 +27.98  —     0.15 -0.08  —     0.16

Hình 10: Sự đánh đổi giữa mAP (mean Average Precision)
và độ trễ cho NN-Factory qua các nhiệm vụ khác nhau của AttrOD.

Hình 11: Mối quan hệ giữa tỷ lệ kích hoạt module thực tế
và giới hạn kích hoạt được chỉ định.

nó trải qua pruning dựa trên tầm quan trọng, dẫn đến một
mẫu gần với đường chéo trên đồ thị. Với việc tăng giới hạn
kích hoạt, NN-Factory tạo ra

Bảng 4: Thời gian chuẩn bị (một lần cho tất cả tình huống)
và thời gian tùy chỉnh (một lần cho mỗi tình huống biên)
của NN-Factory và baseline.

Phương pháp   Chuẩn bị   Tùy chỉnh
NN-Factory    130 giờ    3.6s
Retrain       –          66 giờ
Prune&Tune    66 giờ     4 giờ

các tỷ lệ kích hoạt có thể thỏa mãn các tiêu chí. Các tỷ lệ
dần dần cải thiện với việc tăng giới hạn kích hoạt và cuối
cùng đạt đến trạng thái ổn định, vì các tỷ lệ ổn định đã đủ
để tạo ra các dự đoán chính xác.

5.3 Hiệu quả Sinh Mô hình

Chúng tôi tiến hành phân tích so sánh về chi phí chuẩn bị
và tùy chỉnh giữa NN-Factory và các baseline, và kết quả
được trình bày trong Bảng 4.

Chi phí Tùy chỉnh cho Mỗi Tình huống Biên. Mục tiêu
chính của NN-Factory là giảm thời gian tùy chỉnh mô hình.
NN-Factory chỉ mất 3.6 giây trung bình để tạo ra một mô
hình tùy chỉnh cho một tình huống biên, thể hiện hiệu quả
đáng kể. Quá trình này bao gồm khoảng 12 vòng tìm kiếm,
mỗi vòng mất 0.2 giây, và trích xuất mô hình, mất 0.8 giây.
Ngược lại, các phương pháp tùy chỉnh mô hình truyền thống
gây ra sự gia tăng thời gian kinh hoàng gấp 4000 lần vì
chúng cần huấn luyện mô hình cho mỗi tình huống biên.

Chi phí Chuẩn bị Một lần. Chi phí chuẩn bị của NN-Factory
chủ yếu đến từ việc huấn luyện supernet modular. Đầu tiên,
chúng tôi phân tích mối quan hệ giữa chất lượng của mô
hình được tạo ra và số epoch huấn luyện. Như được mô tả
trong Hình 12, NN-Factory yêu cầu nhiều epoch huấn luyện
hơn để

11

--- TRANG 11 ---
Mô hình Sinh cho các Mô hình: Tùy chỉnh DNN nhanh chóng cho các Nhiệm vụ Đa dạng và Ràng buộc Tài nguyên Preprint, , 2023

Hình 12: Độ chính xác trung
bình của các mô hình được
tạo ra sau khi huấn luyện với
số #epoch khác nhau.

Hình 13: Độ chính xác của
mô hình để dự đoán hiệu suất
bằng cách huấn luyện với số
#subnet khác nhau.

dần dần cải thiện chất lượng của các mô hình được tạo ra.
Điều này liên quan đến phương pháp luận huấn luyện của
chúng tôi. NN-Factory trải qua huấn luyện đồng thời trên
nhiều nhiệm vụ, với mỗi nhiệm vụ chỉ có quyền truy cập
vào một phần dữ liệu huấn luyện trong mỗi epoch. Do đó,
nó cần nhiều epoch huấn luyện hơn để tạo ra các mô hình
chất lượng cao cho mỗi nhiệm vụ riêng lẻ. Tuy nhiên, do
các phụ thuộc liên kết đáng kể giữa các nhiệm vụ, việc huấn
luyện đồng thời của chúng có hiệu ứng hiệp đồng, đảm bảo
rằng chi phí huấn luyện của NN-Factory không tăng lên
đáng kể.

Bộ dự đoán hiệu suất được sử dụng cho tìm kiếm kiến trúc
nhẹ cũng được xây dựng trong quá trình chuẩn bị. Hình 13
cho thấy độ chính xác mô hình hiệu suất đạt được với số
lượng subnet khác nhau được sử dụng để profiling và mô
hình hóa (backbone supernet là ResNet). Khi số lượng subnet
tăng, độ chính xác dự đoán cho thấy xu hướng tăng và đạt
đến một đường tiệm cận. Chúng ta có thể thấy rằng việc sử
dụng 200 ∼ 1000 subnet để profiling và huấn luyện là đủ
để đạt được độ chính xác dự đoán độ trễ và bộ nhớ tốt (cao
hơn 95%). Việc thu thập dữ liệu profiling cho một subnet
mất 70ms (Jetson GPU) đến 985ms (Mobile CPU). Đồng
thời, thời gian cần thiết để fit mô hình hiệu suất ít hơn vài
giây. Do đó, thời gian để thiết lập mô hình dự đoán hiệu
suất trong NN-Factory dao động từ khoảng 14s đến 985s,
điều này có thể bỏ qua như một quy trình offline một lần.

Chi phí Phục vụ Trên thiết bị. Các mô hình được tạo ra
bởi NN-Factory là các mô hình tĩnh bình thường, không
tạo ra bất kỳ chi phí bổ sung nào tại runtime.

5.4 Tổng quát hóa đến Nhiệm vụ Chưa thấy

Trong phần này, chúng tôi đánh giá khả năng tổng quát hóa
của NN-Factory đến các nhiệm vụ chưa thấy. Nhiệm vụ
chưa thấy đề cập đến các nhiệm vụ không phải là một phần
của tập nhiệm vụ huấn luyện, nhưng chúng chia sẻ cùng
một không gian thuộc tính tổ hợp như các nhiệm vụ huấn
luyện.

Hình 14 minh họa độ chính xác của các nhiệm vụ chưa thấy
được chọn ngẫu nhiên trong NumVQA và AttrOD. Mặc dù
độ chính xác cho các nhiệm vụ chưa thấy có thể thể hiện
sự giảm nhẹ so với

Hình 14: Độ chính xác hoặc mAP cho các nhiệm vụ đã biết
& chưa thấy trong (a) NumVQA. (b) AttrOD.

Hình 15: Sự tương đồng gate giữa các nhiệm vụ khác nhau
(a) và giữa các giới hạn kích hoạt khác nhau (b) trong
NumVQA. Nhiệm vụ được đánh dấu màu đỏ là một nhiệm
vụ chưa thấy.

các nhiệm vụ đã biết và một số nhiệm vụ chưa thấy thậm
chí có thể mang lại độ chính xác thấp hơn nhiều, phần lớn
các nhiệm vụ thể hiện độ chính xác xuất sắc, có nghĩa là
các module trong NN-Factory có thể được lắp ráp hiệu quả
để xử lý các nhiệm vụ mới mà không cần dữ liệu huấn luyện.
Kết quả này làm nổi bật khả năng tổng quát hóa mạnh mẽ
của NN-Factory, vì nó có thể nắm bắt ý nghĩa của các thuộc
tính tạo thành một nhiệm vụ và hiểu các thao tác liên quan
đến sự kết hợp của chúng.

Chúng tôi tiếp tục phân tích khả năng tổng quát hóa của
NN-Factory dựa trên sự tương đồng giữa việc lựa chọn gate
cho các nhiệm vụ khác nhau. Kết quả được trình bày trong
Hình 15. Việc lựa chọn gate thể hiện sự tương đồng cao hơn
khi các nhiệm vụ giống nhau hơn (ví dụ: 'Chỉ có một số 0'
và 'Chỉ có ba số 0'). Ngoài ra, trong một nhiệm vụ cố định,
việc tăng độ gần gũi trong các giới hạn kích hoạt cho trước
dẫn đến sự tương đồng lớn hơn trong việc lựa chọn gate.
Điều này chứng minh khả năng của NN-Factory trong việc
hiểu cả yêu cầu nhiệm vụ và giới hạn kích hoạt được chỉ
định, và ánh xạ chúng đến các module tương ứng. Điều này
phục vụ như nền tảng cho khả năng tổng quát hóa của nó.

Với khả năng này, NN-Factory có thể xác định các module
liên quan nhất cho một nhiệm vụ chưa thấy và lắp ráp một
mô hình với chúng. Ví dụ, các module được kích hoạt cho
nhiệm vụ chưa thấy 'chỉ có hai số 0' tương tự với các module
của 'chỉ có ba số 0' và 'chỉ có một số 0' trong Hình 15. Điều
này dẫn đến độ chính xác cao trên nhiệm vụ chưa thấy. Do
đó, khả năng tổng quát hóa của NN-Factory có mối tương
quan tích cực với số lượng nhiệm vụ

12

--- TRANG 12 ---
Preprint, , 2023 Xu và Li et al.

để huấn luyện, có thể dẫn đến nhiều module hữu ích hơn
và một assembler mạnh mẽ hơn.

6 THẢO LUẬN

Ở đây chúng tôi nêu bật một số vấn đề đáng thảo luận thêm.

Khả năng áp dụng cho các loại nhiệm vụ khác. Hiện tại,
NN-Factory chỉ hỗ trợ tùy chỉnh các mô hình cho các nhiệm
vụ trong một không gian tổ hợp, điều này có thể hạn chế
khả năng áp dụng của nó cho các tình huống sử dụng tổng
quát hơn. Để cho phép các không gian nhiệm vụ linh hoạt
hơn, chúng tôi cần sử dụng một mô hình sinh mạnh mẽ hơn
như assembler, nhận định nghĩa nhiệm vụ dạng tự do (ví
dụ: mô tả nhiệm vụ bằng ngôn ngữ tự nhiên) làm đầu vào,
và hoạt động trên một tập hợp lớn hơn các module nơ-ron.
Đồng thời, một tập dữ liệu lớn chứa các ánh xạ giữa các
nhiệm vụ khác nhau và các cặp đầu vào/đầu ra tương ứng
là cần thiết để huấn luyện assembler và các module. Điều
này khả thi theo những tiến bộ gần đây của các mô hình
lớn được huấn luyện trước (ví dụ: ImageBind, ChatGPT,
v.v.), nhưng việc huấn luyện một bộ tạo mô hình mục đích
chung như vậy rất tốn thời gian và tài nguyên, điều này
không thực tế đối với hầu hết các nhà nghiên cứu. Chúng
tôi để việc phát triển NN-Factory mục đích chung như vậy
cho công việc tương lai.

Chi phí chuẩn bị của NN-Factory. Khả năng tùy chỉnh mô
hình nhanh chóng đáng kể của NN-Factory đi kèm với chi
phí thời gian chuẩn bị offline dài hơn. Cụ thể, việc huấn
luyện supernet modular và assembler module mất thời gian
dài hơn nhiều so với việc huấn luyện một mô hình tĩnh bình
thường. Điều này là do NN-Factory không chỉ cần học cách
giải quyết từng nhiệm vụ riêng lẻ, mà còn cách tách rời
các module và lắp ráp lại chúng để giải quyết các nhiệm
vụ mới. Xem xét chi phí biên giảm để hỗ trợ các tình huống
biên đa dạng, chi phí chuẩn bị một lần ít quan trọng hơn.
Những lợi ích của việc giảm chi phí biên có giá trị hơn nếu
NN-Factory hỗ trợ các không gian nhiệm vụ linh hoạt hơn.

7 KẾT LUẬN

Bài báo này đề xuất một phương pháp mới lạ để tùy chỉnh
nhanh chóng các mô hình học sâu cho các tình huống biên
đa dạng. Với thiết kế tổng thể với một supernet modular,
một assembler module, và một bộ tìm kiếm kiến trúc nhẹ,
chúng tôi có thể đạt được tùy chỉnh mô hình nhanh chóng
cho các nhiệm vụ biên đa dạng và ràng buộc tài nguyên.
Các thí nghiệm đã chứng minh chất lượng sinh mô hình
và tốc độ xuất sắc của phương pháp chúng tôi. Chúng tôi
tin rằng công việc của chúng tôi đã cho phép một trải nghiệm
tùy chỉnh mô hình sinh mới và quan trọng.

TÀI LIỆU THAM KHẢO

[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, và Dan Klein. 2016.
Neural module networks. Trong Proceedings of the IEEE conference on
computer vision and pattern recognition. 39–48.

[2] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, và Venkatesh Saligrama.
2017. Adaptive Neural Networks for Efficient Inference. Trong Proceedings
of the 34th International Conference on Machine Learning - Volume 70
(ICML'17). JMLR.org, 527–536.

[3] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han.
2020. Once for All: Train One Network and Specialize it for Efficient
Deployment. Trong International Conference on Learning Representations.
https://arxiv.org/pdf/1908.09791.pdf

[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han.
2020. Once-for-All: Train One Network and Specialize it for Efficient
Deployment. Trong 8th International Conference on Learning Representations, ICLR 2020.

[5] Han Cai, Ligeng Zhu, và Song Han. 2019. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. Trong International
Conference on Learning Representations. https://arxiv.org/pdf/1812.
00332.pdf

[6] Yimin Chen, Jingchao Sun, Xiaocong Jin, Tao Li, Rui Zhang, và
Yanchao Zhang. 2017. Your face your heart: Secure mobile face authentication with photoplethysmograms. Trong IEEE INFOCOM 2017 -
IEEE Conference on Computer Communications. 1–9. https://doi.org/
10.1109/INFOCOM.2017.8057220

[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311
(2022).

[8] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran,
Biswa Sengupta, và Anil A Bharath. 2018. Generative adversarial
networks: An overview. IEEE signal processing magazine 35, 1 (2018),
53–65.

[9] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, và
Mubarak Shah. 2023. Diffusion models in vision: A survey. IEEE
Transactions on Pattern Analysis and Machine Intelligence (2023).

[10] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, và Furu
Wei. 2021. Knowledge neurons in pretrained transformers. arXiv
preprint arXiv:2104.08696 (2021).

[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
và Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929
(2020).

[12] Biyi Fang, Xiao Zeng, Faen Zhang, Hui Xu, và Mi Zhang. 2020.
FlexDNN: Input-Adaptive On-Device Deep Learning for Efficient Mobile Vision. Trong 2020 IEEE/ACM Symposium on Edge Computing (SEC).
84–95. https://doi.org/10.1109/SEC50012.2020.00014

[13] Biyi Fang, Xiao Zeng, và Mi Zhang. 2018. NestDNN: ResourceAware Multi-Tenant On-Device Deep Learning for Continuous Mobile
Vision. Proceedings of the 24th Annual International Conference on
Mobile Computing and Networking (2018).

[14] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, và Xinchao
Wang. 2023. Depgraph: Towards any structural pruning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 16091–16101.

[15] Peizhen Guo, Bo Hu, và Wenjun Hu. 2021. Mistify: Automating
DNN Model Porting for On-Device Inference at the Edge. Trong 18th
USENIX Symposium on Networked Systems Design and Implementation
(NSDI 21). USENIX Association, 705–719. https://www.usenix.org/
conference/nsdi21/presentation/guo

[16] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
Yichen Wei, và Jian Sun. 2020. Single Path One-Shot Neural Architecture Search with Uniform Sampling. Trong Computer Vision – ECCV
2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, và Jan-Michael
Frahm (Eds.). Springer International Publishing, Cham, 544–560.

13

--- TRANG 13 ---
Preprint, , 2023 Xu và Li et al.

[17] Rui Han, Qinglong Zhang, Chi Harold Liu, Guoren Wang, Jian Tang,
và Lydia Y. Chen. 2021. LegoDNN: Block-Grained Scaling of Deep
Neural Networks for Mobile Vision. Trong Proceedings of the 27th Annual
International Conference on Mobile Computing and Networking (MobiCom '21). Association for Computing Machinery, New York, NY, USA,
406–419. https://doi.org/10.1145/3447993.3483249

[18] Song Han, Huizi Mao, và William J. Dally. 2016. Deep Compression:
Compressing Deep Neural Network with Pruning, Trained Quantization
and Huffman Coding. arXiv: Computer Vision and Pattern Recognition
(2016).

[19] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, và
Yulin Wang. 2022. Dynamic Neural Networks: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 11 (2022),
7436–7456. https://doi.org/10.1109/TPAMI.2021.3117837

[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. 2016. Deep
Residual Learning for Image Recognition. Trong 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). 770–778. https:
//doi.org/10.1109/CVPR.2016.90

[21] Yuze He, Li Ma, Zhehao Jiang, Yi Tang, và Guoliang Xing. 2021. VIEye: Semantic-Based 3D Point Cloud Registration for InfrastructureAssisted Autonomous Driving. Trong Proceedings of the 27th Annual
International Conference on Mobile Computing and Networking (MobiCom '21). Association for Computing Machinery, New York, NY, USA,
573–586. https://doi.org/10.1145/3447993.3483276

[22] Sian-Yao Huang và Wei-Ta Chu. 2021. Searching by Generating:
Flexible and Efficient One-Shot NAS with Architecture Generator. Trong
Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition.

[23] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, và Geoffrey E
Hinton. 1991. Adaptive mixtures of local experts. Neural computation
3, 1 (1991), 79–87.

[24] Seunghyeok Jeon, Yonghun Choi, Yeonwoo Cho, và Hojung Cha.
2023. HarvNet: Resource-Optimized Operation of Multi-Exit Deep
Neural Networks on Energy Harvesting Devices. Trong Proceedings
of the 21st Annual International Conference on Mobile Systems, Applications and Services (MobiSys '23). Association for Computing Machinery, New York, NY, USA, 42–55. https://doi.org/10.1145/3581791.
3596845

[25] Shiqi Jiang, Zhiqi Lin, Yuanchun Li, Yuanchao Shu, và Yunxin Liu.
2021. Flexible high-resolution object detection on edge devices with
tunable latency. Trong Proceedings of the 27th Annual International Conference on Mobile Computing and Networking. 559–572.

[26] Łukasz Kaiser và Samy Bengio. 2018. Discrete autoencoders for
sequence models. arXiv preprint arXiv:1801.09797 (2018).

[27] Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar,
Jakob Uszkoreit, và Noam Shazeer. 2018. Fast decoding in sequence
models using discrete latent variables. Trong International Conference on
Machine Learning. PMLR, 2390–2399.

[28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và
Dario Amodei. 2020. Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 (2020).

[29] Wonjae Kim, Bokyung Son, và Ildoo Kim. 2021. Vilt: Vision-andlanguage transformer without convolution or region supervision. Trong
International Conference on Machine Learning. PMLR, 5583–5594.

[30] Rui Kong, Yuanchun Li, Yizhen Yuan, và Linghe Kong. 2023. ConvReLU++: Reference-Based Lossless Acceleration of Conv-ReLU Operations on Mobile CPU. Trong Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services
(MobiSys '23). Association for Computing Machinery, New York, NY,
USA, 503–515. https://doi.org/10.1145/3581791.3596831

[31] Stefanos Laskaridis, Alexandros Kouris, và Nicholas D. Lane. 2021.
Adaptive Inference through Early-Exit Networks: Design, Challenges
and Directions. Trong Proceedings of the 5th International Workshop on
Embedded and Mobile Deep Learning (EMDL'21). Association for
Computing Machinery, New York, NY, USA, 1–6. https://doi.org/10.
1145/3469116.3470012

[32] Stefanos Laskaridis, Stylianos I. Venieris, Hyeji Kim, và Nicholas D.
Lane. 2020. HAPI: Hardware-Aware Progressive Inference. Trong 2020
IEEE/ACM International Conference On Computer Aided Design (ICCAD). 1–9.

[33] Yann LeCun, Léon Bottou, Yoshua Bengio, và Patrick Haffner. 1998.
Gradient-based learning applied to document recognition. Proc. IEEE
86, 11 (1998), 2278–2324.

[34] Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan
Liang, Liang Lin, và Xiaojun Chang. 2020. Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation. Trong 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 1986–1995. https://doi.org/10.1109/CVPR42600.2020.00206

[35] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev,
Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, và C. Lawrence Zitnick. 2014. Microsoft COCO: Common
Objects in Context. CoRR abs/1405.0312 (2014). arXiv:1405.0312
http://arxiv.org/abs/1405.0312

[36] Bingyan Liu, Yuanchun Li, Yunxin Liu, Yao Guo, và Xiangqun Chen.
2020. Pmc: A privacy-preserving deep learning model customization
framework for edge computing. Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies 4, 4 (2020), 1–25.

[37] Chia-Hsiang Liu, Yu-Shin Han, Yuan-Yao Sung, Yi Lee, Hung-Yueh
Chiang, và Kai-Chiang Wu. 2021. FOX-NAS: Fast, On-device and
Explainable Neural Architecture Search. CoRR abs/2108.08189 (2021).
arXiv:2108.08189 https://arxiv.org/abs/2108.08189

[38] Hanxiao Liu, Karen Simonyan, và Yiming Yang. 2018. DARTS:
Differentiable Architecture Search. CoRR abs/1806.09055 (2018).
arXiv:1806.09055 http://arxiv.org/abs/1806.09055

[39] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan,
và Changshui Zhang. 2017. Learning Efficient Convolutional Networks through Network Slimming. Trong 2017 IEEE International Conference on Computer Vision (ICCV). 2755–2763. https://doi.org/10.1109/
ICCV.2017.298

[40] OpenAI. 2022. ChatGPT. [Trực tuyến]. Có sẵn tại: https://openai.com/
blog/chatgpt/.

[41] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li,
Dengyong Zhou, và Pushmeet Kohli. 2016. Neuro-symbolic program
synthesis. arXiv preprint arXiv:1611.01855 (2016).

[42] Jinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin, và Wonyong
Sung. 2018. Fully Neural Network Based Speech Recognition on
Mobile and Embedded Devices. Trong NeurIPS.

[43] Esteban Real, Alok Aggarwal, Yanping Huang, và Quoc V. Le. 2019.
Regularized Evolution for Image Classifier Architecture Search. Trong
Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence
Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence (AAAI'19/IAAI'19/EAAI'19). AAAI Press, Article
587, 10 pages. https://doi.org/10.1609/aaai.v33i01.33014780

[44] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
và Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals and
Linear Bottlenecks. Trong 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 4510–4520. https://doi.org/10.1109/CVPR.
2018.00474

[45] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, và Quoc V. Le. 2019. MnasNet: Platform-Aware
Neural Architecture Search for Mobile. Trong 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR). 2815–2823.

14

--- TRANG 14 ---
Mô hình Sinh cho các Mô hình: Tùy chỉnh DNN nhanh chóng cho các Nhiệm vụ Đa dạng và Ràng buộc Tài nguyên Preprint, , 2023

https://doi.org/10.1109/CVPR.2019.00293

[46] Mingxing Tan và Quoc Le. 2021. EfficientNetV2: Smaller Models and
Faster Training. Trong Proceedings of the 38th International Conference
on Machine Learning (Proceedings of Machine Learning Research),
Marina Meila và Tong Zhang (Eds.), Vol. 139. PMLR, 10096–10106.
https://proceedings.mlr.press/v139/tan21a.html

[47] Mingxing Tan và Quoc V Le. 2019. EfficientNet: Rethinking Model
Scaling for Convolutional Neural Networks. ICML (2019).

[48] Mingxing Tan, Ruoming Pang, và Quoc V. Le. 2019. EfficientDet:
Scalable and Efficient Object Detection. CoRR abs/1911.09070 (2019).
arXiv:1911.09070 http://arxiv.org/abs/1911.09070

[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017.
Attention is All You Need. Trong Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran
Associates Inc., Red Hook, NY, USA, 6000–6010.

[50] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, và Song Han. 2019.
HAQ: Hardware-Aware Automated Quantization With Mixed Precision. Trong IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

[51] Quan Wang, Ignacio Lopez Moreno, Mert Saglam, Kevin Wilson, Alan
Chiao, Renjie Liu, Yanzhang He, Wei Li, Jason Pelecanos, Marily
Nika, et al. 2020. VoiceFilter-Lite: Streaming targeted voice separation
for on-device speech recognition. arXiv preprint arXiv:2009.04323
(2020).

[52] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, và Joseph E. Gonzalez. 2018. SkipNet: Learning Dynamic Routing in Convolutional
Networks. Trong The European Conference on Computer Vision (ECCV).

[53] Hao Wen, Yuanchun Li, Zunshuai Zhang, Shiqi Jiang, Xiaozhou Ye,
Ye Ouyang, Ya-Qin Zhang, và Yunxin Liu. 2023. AdaptiveNet: Postdeployment Neural Architecture Adaptation for Diverse Edge Environments. (2023).

[54] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun,
Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, và Kurt
Keutzer. 2019. FBNet: Hardware-Aware Efficient ConvNet Design
via Differentiable Neural Architecture Search. Trong 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10726–
10734. https://doi.org/10.1109/CVPR.2019.01099

[55] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie,
Larry S Davis, Kristen Grauman, và Rogerio Feris. 2018. BlockDrop: Dynamic Inference Paths in Residual Networks. Trong CVPR.

[56] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark
Sandler, Vivienne Sze, và Hartwig Adam. 2018. NetAdapt: PlatformAware Neural Network Adaptation for Mobile Applications. Trong The
European Conference on Computer Vision (ECCV).

[57] Jiahui Yu và Thomas S. Huang. 2019. Universally Slimmable Networks and Improved Training Techniques. CoRR abs/1903.05134
(2019). arXiv:1903.05134 http://arxiv.org/abs/1903.05134

[58] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, và Thomas S. Huang.
2018. Slimmable Neural Networks. CoRR abs/1812.08928 (2018).
arXiv:1812.08928 http://arxiv.org/abs/1812.08928

[59] Huanhuan Zhang, Anfu Zhou, Yuhan Hu, Chaoyue Li, Guangping
Wang, Xinyu Zhang, Huadong Ma, Leilei Wu, Aiyun Chen, và
Changhui Wu. 2021. Loki: Improving Long Tail Performance of
Learning-Based Real-Time Video Adaptation by Fusing Rule-Based
Models. Trong Proceedings of the 27th Annual International Conference on Mobile Computing and Networking (MobiCom '21). Association for Computing Machinery, New York, NY, USA, 775–788.
https://doi.org/10.1145/3447993.3483259

[60] Xumiao Zhang, Anlan Zhang, Jiachen Sun, Xiao Zhu, Y. Ethan Guo,
Feng Qian, và Z. Morley Mao. 2021. EMP: Edge-Assisted Multi-
Vehicle Perception. Trong Proceedings of the 27th Annual International
Conference on Mobile Computing and Networking (MobiCom '21).
Association for Computing Machinery, New York, NY, USA, 545–558.
https://doi.org/10.1145/3447993.3483242

[61] Ziqi Zhang, Yuanchun Li, Yao Guo, Xiangqun Chen, và Yunxin Liu.
2020. Dynamic slicing for deep neural networks. Trong Proceedings of
the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering.
838–850.

[62] Ziqi Zhang, Yuanchun Li, Bingyan Liu, Yifeng Cai, Ding Li, Yao
Guo, và Xiangqun Chen. 2023. FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing. Trong 2023
IEEE/ACM 45th International Conference on Software Engineering
(ICSE). IEEE, 460–472.

[63] Ziqi Zhang, Yuanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Yao
Guo, Xiangqun Chen, và Yunxin Liu. 2022. ReMoS: reducing defect
inheritance in transfer learning via relevant model slicing. Trong Proceedings of the 44th International Conference on Software Engineering.
1856–1868.

[64] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun,
và Jie Zhou. 2021. Moefication: Transformer feed-forward layers are
mixtures of experts. arXiv preprint arXiv:2110.01786 (2021).

[65] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang,
Zhelong Li, Xiuqi Yang, và Junjie Yan. 2020. Towards Unified INT8
Training for Convolutional Neural Network. Trong 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1966–1976.
https://doi.org/10.1109/CVPR42600.2020.00204

[66] Barret Zoph và Quoc V. Le. 2016. Neural Architecture Search
with Reinforcement Learning. CoRR abs/1611.01578 (2016).
arXiv:1611.01578 http://arxiv.org/abs/1611.01578

15
