# 2308.15003.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/automl-nas/2308.15003.pdf
# KÃ­ch thÆ°á»›c file: 3755495 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================


--- TRANG 1 ---
MÃ´ hÃ¬nh Sinh cho cÃ¡c MÃ´ hÃ¬nh: TÃ¹y chá»‰nh DNN nhanh chÃ³ng
cho cÃ¡c Nhiá»‡m vá»¥ Äa dáº¡ng vÃ  RÃ ng buá»™c TÃ i nguyÃªn
Wenxing Xu *
Äáº¡i há»c BÆ°u Ä‘iá»‡n vÃ  Viá»…n thÃ´ng Báº¯c Kinh Yuanchun Liâ€ 
Viá»‡n NghiÃªn cá»©u CÃ´ng nghiá»‡p AI
(AIR), Äáº¡i há»c Thanh Hoa Jiacheng Liu *
Viá»‡n CÃ´ng nghá»‡ Báº¯c Kinh
Yi Sun
Viá»‡n NghiÃªn cá»©u CÃ´ng nghiá»‡p AI
(AIR), Äáº¡i há»c Thanh Hoa Zhengyang Cao *
Äáº¡i há»c Khoa há»c vÃ  CÃ´ng nghá»‡ Äiá»‡n tá»­
Trung Quá»‘c Yixuan Li *
Äáº¡i há»c BÆ°u Ä‘iá»‡n vÃ  Viá»…n thÃ´ng Báº¯c Kinh
Hao Wen
Viá»‡n NghiÃªn cá»©u CÃ´ng nghiá»‡p AI
(AIR), Äáº¡i há»c Thanh Hoa Yunxin Liu
Viá»‡n NghiÃªn cá»©u CÃ´ng nghiá»‡p AI
(AIR), Äáº¡i há»c Thanh Hoa

TÃ“M Táº®T
KhÃ´ng giá»‘ng nhÆ° cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u dá»±a trÃªn Ä‘Ã¡m mÃ¢y thÆ°á»ng lá»›n
vÃ  Ä‘á»“ng nháº¥t, cÃ¡c mÃ´ hÃ¬nh triá»ƒn khai táº¡i biÃªn thÆ°á»ng yÃªu cáº§u tÃ¹y
chá»‰nh cho cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ theo lÄ©nh vá»±c vÃ  mÃ´i trÆ°á»ng háº¡n cháº¿
tÃ i nguyÃªn. CÃ¡c quy trÃ¬nh tÃ¹y chá»‰nh nhÆ° váº­y cÃ³ thá»ƒ tá»‘n kÃ©m vÃ 
máº¥t thá»i gian do sá»± Ä‘a dáº¡ng cá»§a cÃ¡c tÃ¬nh huá»‘ng biÃªn vÃ  khá»‘i lÆ°á»£ng
huáº¥n luyá»‡n cho má»—i tÃ¬nh huá»‘ng. Máº·c dÃ¹ Ä‘Ã£ cÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p
Ä‘Æ°á»£c Ä‘á» xuáº¥t cho viá»‡c tÃ¹y chá»‰nh nhanh chÃ³ng theo tÃ i nguyÃªn vÃ 
tÃ¹y chá»‰nh theo nhiá»‡m vá»¥ tÆ°Æ¡ng á»©ng, viá»‡c Ä‘áº¡t Ä‘Æ°á»£c cáº£ hai cÃ¹ng má»™t
lÃºc lÃ  thÃ¡ch thá»©c. Láº¥y cáº£m há»©ng tá»« AI sinh vÃ  tÃ­nh tá»• há»£p modular
cá»§a máº¡ng nÆ¡-ron, chÃºng tÃ´i giá»›i thiá»‡u NN-Factory, má»™t khung
work má»™t-cho-táº¥t-cáº£ Ä‘á»ƒ táº¡o ra cÃ¡c mÃ´ hÃ¬nh nháº¹ tÃ¹y chá»‰nh cho
cÃ¡c tÃ¬nh huá»‘ng biÃªn Ä‘a dáº¡ng. Ã tÆ°á»Ÿng chÃ­nh lÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh
sinh Ä‘á»ƒ trá»±c tiáº¿p táº¡o ra cÃ¡c mÃ´ hÃ¬nh tÃ¹y chá»‰nh, thay vÃ¬ huáº¥n luyá»‡n
chÃºng. CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a NN-Factory bao gá»“m má»™t supernet
modular vá»›i cÃ¡c module Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ thá»ƒ Ä‘Æ°á»£c kÃ­ch
hoáº¡t cÃ³ Ä‘iá»u kiá»‡n Ä‘á»ƒ hoÃ n thÃ nh cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau vÃ  má»™t
bá»™ láº¯p rÃ¡p module sinh tá»± Ä‘á»™ng thao tÃ¡c cÃ¡c module theo yÃªu cáº§u
nhiá»‡m vá»¥ vÃ  Ä‘á»™ thÆ°a. Vá»›i má»™t tÃ¬nh huá»‘ng biÃªn cho trÆ°á»›c, NN-Factory
cÃ³ thá»ƒ hiá»‡u quáº£ tÃ¹y chá»‰nh má»™t mÃ´ hÃ¬nh compact chuyÃªn biá»‡t trong
nhiá»‡m vá»¥ biÃªn Ä‘á»“ng thá»i thá»a mÃ£n cÃ¡c rÃ ng buá»™c tÃ i nguyÃªn biÃªn
báº±ng cÃ¡ch tÃ¬m kiáº¿m chiáº¿n lÆ°á»£c tá»‘i Æ°u Ä‘á»ƒ láº¯p rÃ¡p cÃ¡c module. Dá»±a
trÃªn cÃ¡c thÃ­ nghiá»‡m vá» cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i hÃ¬nh áº£nh vÃ  phÃ¡t hiá»‡n
Ä‘á»‘i tÆ°á»£ng vá»›i cÃ¡c thiáº¿t bá»‹ biÃªn khÃ¡c nhau, NN-Factory cÃ³ kháº£ nÄƒng
táº¡o ra cÃ¡c mÃ´ hÃ¬nh cháº¥t lÆ°á»£ng cao cá»¥ thá»ƒ theo nhiá»‡m vá»¥ vÃ  tÃ i nguyÃªn
trong vÃ i giÃ¢y, nhanh hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ¹y chá»‰nh mÃ´ hÃ¬nh thÃ´ng
thÆ°á»ng hÃ ng báº­c Ä‘á»™ lá»›n.

*CÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n khi cÃ¡c tÃ¡c giáº£ Ä‘ang thá»±c táº­p táº¡i Viá»‡n NghiÃªn cá»©u
CÃ´ng nghiá»‡p AI (AIR), Äáº¡i há»c Thanh Hoa.
â€ TÃ¡c giáº£ liÃªn há»‡.

Tá»ª KHÃ“A
Há»c sÃ¢u, tÃ¬nh huá»‘ng biÃªn, rÃ ng buá»™c tÃ i nguyÃªn, tÃ¹y chá»‰nh mÃ´ hÃ¬nh,
mÃ´ hÃ¬nh sinh

1 GIá»šI THIá»†U
Há»c sÃ¢u (DL) Ä‘Ã£ trá»Ÿ thÃ nh má»™t cÃ´ng nghá»‡ trÃ­ tuá»‡ nhÃ¢n táº¡o (AI)
thay Ä‘á»•i cuá»™c chÆ¡i trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y. NÃ³ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c
hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ trong nhiá»u lÄ©nh vá»±c khÃ¡c nhau bao gá»“m thá»‹
giÃ¡c mÃ¡y tÃ­nh, hiá»ƒu biáº¿t ngÃ´n ngá»¯ tá»± nhiÃªn, trÃ² chÆ¡i mÃ¡y tÃ­nh,
vÃ  sinh há»c tÃ­nh toÃ¡n. Äá»“ng thá»i, há»c sÃ¢u Ä‘Ã£ cho phÃ©p vÃ  tÄƒng
cÆ°á»ng nhiá»u á»©ng dá»¥ng thÃ´ng minh táº¡i biÃªn, cháº³ng háº¡n nhÆ° há»—
trá»£ lÃ¡i xe [21,60], xÃ¡c thá»±c khuÃ´n máº·t [6], giÃ¡m sÃ¡t video [25,59],
nháº­n dáº¡ng giá»ng nÃ³i [42,51], v.v. Do cÃ¡c cÃ¢n nháº¯c vá» Ä‘á»™ trá»… vÃ 
báº£o máº­t, viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh Ä‘áº¿n cÃ¡c thiáº¿t bá»‹ biÃªn [15,30,36]
Ä‘ang trá»Ÿ thÃ nh má»™t thá»±c hÃ nh ngÃ y cÃ ng phá»• biáº¿n, Ä‘á»ƒ cÃ¡c mÃ´
hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i trá»±c tiáº¿p mÃ  khÃ´ng cáº§n truyá»n dá»¯ liá»‡u Ä‘áº¿n
mÃ¡y chá»§.

Äá»ƒ Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh DL trong cÃ¡c tÃ¬nh huá»‘ng biÃªn khÃ¡c nhau,
cÃ¡c nhÃ  phÃ¡t triá»ƒn thÆ°á»ng cáº§n tÃ¹y chá»‰nh cÃ¡c mÃ´ hÃ¬nh, bao gá»“m
hai quy trÃ¬nh chÃ­nh sau Ä‘Ã¢y.

(1)TÃ¹y chá»‰nh HÆ°á»›ng Nhiá»‡m vá»¥Â¹: TÃ¹y chá»‰nh cÃ¡c mÃ´ hÃ¬nh cho
cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ theo lÄ©nh vá»±c, cháº³ng háº¡n nhÆ° phÃ¡t hiá»‡n
cÃ¡c loáº¡i xe nháº¥t Ä‘á»‹nh, theo dÃµi ngÆ°á»i vá»›i trang phá»¥c nháº¥t
Ä‘á»‹nh, hoáº·c nháº­n dáº¡ng cÃ¡c váº­t pháº©m vá»›i lá»—i nháº¥t Ä‘á»‹nh.

Â¹Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i táº­p trung vÃ o trÆ°á»ng há»£p Ä‘Æ¡n giáº£n khi cÃ¡c nhiá»‡m vá»¥
qua cÃ¡c tÃ¬nh huá»‘ng biÃªn khÃ¡c nhau chia sáº» má»™t khÃ´ng gian tá»• há»£p. VÃ­ dá»¥, má»—i nhiá»‡m
vá»¥ lÃ  phÃ¢n loáº¡i/phÃ¡t hiá»‡n cÃ¡c Ä‘á»‘i tÆ°á»£ng vá»›i sá»± káº¿t há»£p cá»§a má»™t sá»‘ thuá»™c tÃ­nh Ä‘Ã£ biáº¿t
(vÃ­ dá»¥: mÃ u sáº¯c, loáº¡i, danh má»¥c, yáº¿u tá»‘, v.v.).

1arXiv:2308.15003v1  [cs.AI]  29 Aug 2023

--- TRANG 2 ---
Preprint, , 2023 Xu vÃ  Li et al.

(2)TÃ¹y chá»‰nh HÆ°á»›ng TÃ i nguyÃªn: TÃ¹y chá»‰nh cÃ¡c mÃ´ hÃ¬nh cho
cÃ¡c thiáº¿t bá»‹ biÃªn, nháº±m Ä‘Ã¡p á»©ng cÃ¡c rÃ ng buá»™c tÃ i nguyÃªn
nháº¥t Ä‘á»‹nh trÃªn thiáº¿t bá»‹ biÃªn má»¥c tiÃªu, bao gá»“m ngÃ¢n sÃ¡ch
bá»™ nhá»› vÃ  yÃªu cáº§u Ä‘á»™ trá»….

Thá»±c hÃ nh hiá»‡n táº¡i lÃ  xá»­ lÃ½ tá»«ng váº¥n Ä‘á» tÃ¹y chá»‰nh riÃªng biá»‡t.
VÃ­ dá»¥, cÃ¡c nhÃ  phÃ¡t triá»ƒn cáº§n táº¡o ra cÃ¡c mÃ´ hÃ¬nh compact thÃ´ng
qua thiáº¿t káº¿ kiáº¿n trÃºc mÃ´ hÃ¬nh/nÃ©n rá»™ng rÃ£i Ä‘á»ƒ Ä‘Ã¡p á»©ng cÃ¡c rÃ ng
buá»™c tÃ i nguyÃªn, vÃ  sau Ä‘Ã³ huáº¥n luyá»‡n/tinh chá»‰nh mÃ´ hÃ¬nh trÃªn
cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ theo lÄ©nh vá»±c Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c.
Má»™t quy trÃ¬nh nhÆ° váº­y cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n viá»‡c thu tháº­p dá»¯
liá»‡u tá»‘n kÃ©m, tÃ¬m kiáº¿m kiáº¿n trÃºc tá»‘n thá»i gian, vÃ  huáº¥n luyá»‡n
mÃ´ hÃ¬nh khÃ´ng á»•n Ä‘á»‹nh.

Nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y cá»§a há»c sÃ¢u Ä‘Ã£ chá»©ng minh kháº£ nÄƒng
thÃ­ch á»©ng zero-shot xuáº¥t sáº¯c cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n
trÆ°á»›c lá»›n, bao gá»“m thÃ­ch á»©ng kiáº¿n trÃºc máº¡ng nÆ¡-ron khÃ´ng
cáº§n huáº¥n luyá»‡n vÃ  thÃ­ch á»©ng nhiá»‡m vá»¥. Cá»¥ thá»ƒ, cÃ¡c ká»¹ thuáº­t
tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng nÆ¡-ron má»™t láº§n (NAS) [3,5] vÃ  má»Ÿ
rá»™ng mÃ´ hÃ¬nh [13,17,24,53] Ä‘Ã£ chá»©ng minh kháº£ nÄƒng huáº¥n luyá»‡n
má»™t supernet vÃ  tÃ¹y chá»‰nh nÃ³ cho cÃ¡c mÃ´i trÆ°á»ng biÃªn Ä‘a dáº¡ng.
CÃ¡c mÃ´ hÃ¬nh lá»›n Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c nhÆ° ChatGPT [40],
OFA [4], vÃ  Segment Anything [7] cho phÃ©p ngÆ°á»i dÃ¹ng tÃ¹y
chá»‰nh nhiá»‡m vá»¥ cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch Ä‘Æ¡n giáº£n cung cáº¥p má»™t
prompt nhiá»‡m vá»¥ lÃ m Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh. Nhá»¯ng kháº£ nÄƒng
thÃ­ch á»©ng khÃ´ng cáº§n huáº¥n luyá»‡n nhÆ° váº­y ráº¥t mong muá»‘n trong
cÃ¡c tÃ¬nh huá»‘ng AI biÃªn do kháº£ nÄƒng há»— trá»£ má»™t loáº¡t rá»™ng cÃ¡c
nhiá»‡m vá»¥ downstream vÃ  rÃ ng buá»™c pháº§n cá»©ng Ä‘a dáº¡ng vá»›i
ná»— lá»±c phÃ¡t triá»ƒn giáº£m Ä‘Ã¡ng ká»ƒ.

Tuy nhiÃªn, ráº¥t khÃ³ Ä‘á»ƒ káº¿t há»£p viá»‡c tÃ¹y chá»‰nh kiáº¿n trÃºc máº¡ng
nÆ¡-ron khÃ´ng cáº§n huáº¥n luyá»‡n vÃ  tÃ¹y chá»‰nh nhiá»‡m vá»¥ khÃ´ng
cáº§n huáº¥n luyá»‡n cÃ¹ng nhau. Má»™t máº·t, viá»‡c sá»­ dá»¥ng NAS má»™t
láº§n hoáº·c má»Ÿ rá»™ng mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o ra cÃ¡c mÃ´ hÃ¬nh cho cÃ¡c nhiá»‡m
vá»¥ khÃ¡c nhau lÃ  khÃ´ng kháº£ thi vÃ¬ supernet Ä‘Æ°á»£c thiáº¿t káº¿ cho
má»™t nhiá»‡m vá»¥ duy nháº¥t. Má»Ÿ rá»™ng chÃºng Ä‘á»ƒ há»— trá»£ nhiá»u nhiá»‡m
vá»¥ sáº½ lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ khÃ´ng gian tÃ¬m kiáº¿m mÃ´ hÃ¬nh, khiáº¿n
viá»‡c huáº¥n luyá»‡n supernet vÃ  tÃ¬m kiáº¿m subnet trá»Ÿ nÃªn cá»±c ká»³
thÃ¡ch thá»©c. Máº·t khÃ¡c, kháº£ nÄƒng thÃ­ch á»©ng nhiá»‡m vá»¥ Ä‘Ã¡ng ká»ƒ
cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘i kÃ¨m vá»›i chi phÃ­
táº£i tÃ­nh toÃ¡n khá»•ng lá»“. CÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ cho tháº¥y
ráº±ng kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a zero-shot cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c
huáº¥n luyá»‡n trÆ°á»›c xuáº¥t hiá»‡n vÃ  tÄƒng cÆ°á»ng vá»›i viá»‡c tÄƒng kÃ­ch
thÆ°á»›c mÃ´ hÃ¬nh [28]. Do Ä‘Ã³, ráº¥t khÃ³ hoáº·c tháº­m chÃ­ khÃ´ng thá»ƒ
trá»±c tiáº¿p sá»­ dá»¥ng mÃ´ hÃ¬nh lá»›n cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a nhiá»‡m
vá»¥ do cÃ¡c tÃ i nguyÃªn tÃ­nh toÃ¡n biÃªn Ä‘a dáº¡ng vÃ  háº¡n cháº¿.

Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c viá»‡c tÃ¹y chá»‰nh mÃ´ hÃ¬nh hiá»‡u quáº£ hÆ°á»›ng nhiá»‡m
vá»¥ vÃ  hÆ°á»›ng tÃ i nguyÃªn, chÃºng tÃ´i giá»›i thiá»‡u NN-Factory, má»™t
paradigm má»›i Ä‘á»ƒ táº¡o ra cÃ¡c mÃ´ hÃ¬nh tÃ¹y chá»‰nh cho cÃ¡c tÃ¬nh
huá»‘ng biÃªn Ä‘a dáº¡ng. Ã tÆ°á»Ÿng chÃ­nh cá»§a chÃºng tÃ´i lÃ  xem váº¥n
Ä‘á» tÃ¹y chá»‰nh mÃ´ hÃ¬nh cá»¥ thá»ƒ theo biÃªn nhÆ° má»™t váº¥n Ä‘á» sinh -
thay vÃ¬ cÃ¡c mÃ´ hÃ¬nh sinh hiá»‡n táº¡i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ táº¡o ra ná»™i
dung media [8,9,40], chÃºng tÃ´i sá»­ dá»¥ng AI sinh Ä‘á»ƒ táº¡o ra cÃ¡c
mÃ´ hÃ¬nh tÃ¹y chá»‰nh dá»±a trÃªn cÃ¡c Ä‘áº·c táº£ tÃ¬nh huá»‘ng biÃªn. Cá»¥
thá»ƒ, vá»›i nhiá»‡m vá»¥ mong muá»‘n, loáº¡i thiáº¿t bá»‹, vÃ  cÃ¡c yÃªu cáº§u
tÃ i nguyÃªn trong má»™t tÃ¬nh huá»‘ng biÃªn, NN-Factory

HÃ¬nh 1: Ã tÆ°á»Ÿng chÃ­nh cá»§a NN-Factory: Táº¡o ra cÃ¡c mÃ´ hÃ¬nh
cho cÃ¡c tÃ¬nh huá»‘ng biÃªn Ä‘a dáº¡ng báº±ng cÃ¡ch láº¯p rÃ¡p cÃ¡c module.

nhanh chÃ³ng táº¡o ra má»™t mÃ´ hÃ¬nh compact phÃ¹ há»£p vá»›i nhiá»‡m vá»¥
vÃ  yÃªu cáº§u báº±ng cÃ¡ch Ä‘Æ¡n giáº£n truy váº¥n mÃ´ hÃ¬nh sinh. Má»™t
paradigm má»›i vá» tÃ¹y chá»‰nh mÃ´ hÃ¬nh nhÆ° váº­y cÃ³ tiá»m nÄƒng
trÃ¡nh hoÃ n toÃ n cÃ¡c quy trÃ¬nh nÃ©n vÃ  huáº¥n luyá»‡n cá»“ng ká»nh
trong cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ¹y chá»‰nh mÃ´ hÃ¬nh biÃªn thÃ´ng thÆ°á»ng.

Tuy nhiÃªn, viá»‡c trá»±c tiáº¿p táº¡o ra cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh lÃ 
khÃ³ khÄƒn do sá»‘ lÆ°á»£ng tham sá»‘ mÃ´ hÃ¬nh khá»•ng lá»“. Hiá»ƒu biáº¿t
chÃ­nh cá»§a NN-Factory Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y lÃ  tÃ­nh tá»• há»£p
cá»§a cÃ¡c module máº¡ng nÆ¡-ron [1,41], tá»©c lÃ  cÃ¡c chá»©c nÄƒng vÃ 
kÃ­ch thÆ°á»›c khÃ¡c nhau cá»§a máº¡ng nÆ¡-ron cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c báº±ng
cÃ¡c káº¿t há»£p nháº¥t Ä‘á»‹nh cá»§a cÃ¡c module máº¡ng nÆ¡-ron, nhÆ° Ä‘Æ°á»£c
minh há»a trong HÃ¬nh 1. TÃ­nh tá»• há»£p modular nhÆ° váº­y Ä‘Ã£ Ä‘Æ°á»£c
nghiÃªn cá»©u bá»Ÿi cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³ (vÃ­ dá»¥: NestDNN [13],
LegoDNN [17], AdaptiveNet [53], v.v.) cho viá»‡c má»Ÿ rá»™ng mÃ´
hÃ¬nh phÃ­a biÃªn, nhÆ°ng khÃ´ng cÃ³ phÆ°Æ¡ng phÃ¡p nÃ o trong sá»‘
chÃºng cÃ³ thá»ƒ há»— trá»£ tÃ¹y chá»‰nh hÆ°á»›ng nhiá»‡m vá»¥.

Dá»±a trÃªn hiá»ƒu biáº¿t nÃ y, NN-Factory táº¡o ra má»™t mÃ´ hÃ¬nh tÃ¹y
chá»‰nh cho má»—i tÃ¬nh huá»‘ng biÃªn báº±ng cÃ¡ch trá»±c tiáº¿p táº¡o ra cÃ¡c
cáº¥u hÃ¬nh Ä‘á»ƒ láº¯p rÃ¡p cÃ¡c module nÆ¡-ron Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c.
CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a NN-Factory bao gá»“m má»™t supernet
modular mang cÃ¡c module nÆ¡-ron, má»™t bá»™ láº¯p rÃ¡p module
nháº­n biáº¿t nhiá»‡m vá»¥ vÃ  Ä‘á»™ thÆ°a táº¡o ra cÃ¡c cáº¥u hÃ¬nh láº¯p rÃ¡p
module á»©ng viÃªn (má»—i cáº¥u hÃ¬nh lÃ  má»™t táº­p há»£p cÃ¡c vector
gate cÃ³ thá»ƒ Ã¡nh xáº¡ Ä‘áº¿n má»™t mÃ´ hÃ¬nh á»©ng viÃªn), vÃ  má»™t bá»™ tÃ¬m
kiáº¿m kiáº¿n trÃºc nháº¹ nhanh chÃ³ng tÃ¬m ra cáº¥u hÃ¬nh tá»‘i Æ°u Ä‘á»ƒ
láº¯p rÃ¡p mÃ´ hÃ¬nh cho tÃ¬nh huá»‘ng biÃªn má»¥c tiÃªu.

Cá»¥ thá»ƒ, supernet modular Ä‘Æ°á»£c má»Ÿ rá»™ng tá»« má»™t máº¡ng backbone
chung, cháº³ng háº¡n nhÆ° Máº¡ng NÆ¡-ron TÃ­ch cháº­p (CNN) hoáº·c
Transformer, trong Ä‘Ã³ má»—i khá»‘i cÆ¡ báº£n Ä‘Æ°á»£c phÃ¢n tÃ¡ch thÃ nh
nhiá»u module Ä‘Æ°á»£c kÃ­ch hoáº¡t cÃ³ Ä‘iá»u kiá»‡n. CÃ¡ch thá»©c cÃ¡c
module Ä‘Æ°á»£c kÃ­ch hoáº¡t Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn bá»Ÿi má»™t sá»‘ vector gate,
Ä‘Æ°á»£c táº¡o ra bá»Ÿi má»™t mÃ´ hÃ¬nh sinh (tá»©c lÃ  bá»™ láº¯p rÃ¡p module).
Báº±ng cÃ¡ch kÃ­ch hoáº¡t cÃ¡c táº­p há»£p module khÃ¡c nhau, cÃ¡c khá»‘i
cÆ¡ báº£n trong máº¡ng backbone cÃ³ thá»ƒ Ä‘Æ°á»£c cáº¥u hÃ¬nh láº¡i Ä‘á»ƒ cÃ³
cÃ¡c chá»©c nÄƒng vÃ  kÃ­ch thÆ°á»›c khÃ¡c nhau. Bá»™ láº¯p rÃ¡p module
quyáº¿t Ä‘á»‹nh cÃ¡ch kÃ­ch hoáº¡t cÃ¡c module trong máº¡ng backbone
dá»±a trÃªn mÃ´ táº£ nhiá»‡m vá»¥ vÃ  yÃªu cáº§u Ä‘á»™ thÆ°a (tá»©c lÃ  giá»›i háº¡n
tá»· lá»‡ kÃ­ch hoáº¡t module). CÃ¡c module vÃ  bá»™ láº¯p rÃ¡p Ä‘Æ°á»£c huáº¥n
luyá»‡n cÃ¹ng nhau Ä‘á»ƒ hoáº¡t Ä‘á»™ng nháº¥t quÃ¡n cÃ¹ng nhau.

2

--- TRANG 3 ---
MÃ´ hÃ¬nh Sinh cho cÃ¡c MÃ´ hÃ¬nh: TÃ¹y chá»‰nh DNN nhanh chÃ³ng cho cÃ¡c Nhiá»‡m vá»¥ Äa dáº¡ng vÃ  RÃ ng buá»™c TÃ i nguyÃªn Preprint, , 2023

Dá»±a trÃªn thiáº¿t káº¿ module-assembler, chÃºng tÃ´i cÃ³ thá»ƒ giáº£m
khÃ´ng gian tÃ¬m kiáº¿m lá»›n cá»§a cÃ¡c káº¿t há»£p module xuá»‘ng má»™t
khÃ´ng gian tÃ¬m kiáº¿m nhá» cá»§a cÃ¡c prompt sinh. Äá»ƒ tÃ¬m mÃ´
hÃ¬nh tÃ¹y chá»‰nh cho má»™t tÃ¬nh huá»‘ng biÃªn (Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a
bá»Ÿi nhiá»‡m vá»¥, thiáº¿t bá»‹, vÃ  yÃªu cáº§u Ä‘á»™ trá»…/bá»™ nhá»›), chÃºng tÃ´i
chá»‰ cáº§n tÃ¬m yÃªu cáº§u sinh mÃ´ hÃ¬nh tá»‘i Æ°u, Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi
module tÃ¬m kiáº¿m kiáº¿n trÃºc nháº¹ dÆ°á»›i sá»± hÆ°á»›ng dáº«n cá»§a má»™t
bá»™ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t mÃ´ hÃ¬nh cá»¥ thá»ƒ theo thiáº¿t bá»‹. Do khÃ´ng
gian tÃ¬m kiáº¿m Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ, viá»‡c tÃ¬m mÃ´ hÃ¬nh tá»‘i Æ°u
chá»‰ máº¥t vÃ i láº§n láº·p ngáº¯n.

Äá»ƒ Ä‘Ã¡nh giÃ¡ NN-Factory, chÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m
vá»›i ba thiáº¿t bá»‹ trÃªn hai loáº¡i nhiá»‡m vá»¥. Káº¿t quáº£ Ä‘Ã£ chá»©ng minh
ráº±ng phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ táº¡o ra cÃ¡c mÃ´ hÃ¬nh
phÃ¹ há»£p cho má»™t tÃ¬nh huá»‘ng biÃªn cho trÆ°á»›c trong tá»‘i Ä‘a 6
giÃ¢y, nhanh hÆ¡n 1000Ã— so vá»›i phÆ°Æ¡ng phÃ¡p tÃ¹y chá»‰nh dá»±a
trÃªn huáº¥n luyá»‡n thÃ´ng thÆ°á»ng. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra cÃ³
thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÃ¡c mÃ´ hÃ¬nh
Ä‘Æ°á»£c huáº¥n luyá»‡n láº¡i/tinh chá»‰nh táº¡i biÃªn trÃªn cáº£ cÃ¡c nhiá»‡m vá»¥
Ä‘Ã£ biáº¿t vÃ  chÆ°a tháº¥y.

CÃ´ng viá»‡c cá»§a chÃºng tÃ´i Ä‘Ã³ng gÃ³p cÃ¡c ká»¹ thuáº­t sau:

(1)ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t giáº£i phÃ¡p tÃ¹y chá»‰nh mÃ´ hÃ¬nh cá»¥
thá»ƒ theo biÃªn dá»±a trÃªn AI sinh má»›i láº¡. NÃ³ cho phÃ©p tÃ¹y
chá»‰nh mÃ´ hÃ¬nh nhanh chÃ³ng khÃ´ng cáº§n huáº¥n luyá»‡n cho
cÃ¡c tÃ¬nh huá»‘ng biÃªn vá»›i nhiá»‡m vá»¥ Ä‘a dáº¡ng vÃ  rÃ ng buá»™c
tÃ i nguyÃªn.

(2)ChÃºng tÃ´i giá»›i thiá»‡u má»™t thiáº¿t káº¿ modular cho viá»‡c sinh
mÃ´ hÃ¬nh, cho phÃ©p tÃ¹y chá»‰nh mÃ´ hÃ¬nh nhanh chÃ³ng báº±ng
cÃ¡ch Ä‘Æ¡n giáº£n truy váº¥n má»™t mÃ´ hÃ¬nh assembler cho viá»‡c
kÃ­ch hoáº¡t module.

(3)Dá»±a trÃªn cÃ¡c thÃ­ nghiá»‡m vá»›i hai loáº¡i nhiá»‡m vá»¥ vÃ  nhiá»u
thiáº¿t bá»‹ biÃªn khÃ¡c nhau, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³
kháº£ nÄƒng táº¡o ra cÃ¡c mÃ´ hÃ¬nh cháº¥t lÆ°á»£ng cao cho cÃ¡c tÃ¬nh
huá»‘ng biÃªn Ä‘a dáº¡ng vá»›i chi phÃ­ tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ. Há»‡
thá»‘ng vÃ  cÃ¡c mÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c má»Ÿ mÃ£ nguá»“n.

2 Bá»I Cáº¢NH VÃ€ CÃ”NG VIá»†C LIÃŠN QUAN

2.1 TÃ¹y chá»‰nh MÃ´ hÃ¬nh cho cÃ¡c TÃ¬nh huá»‘ng BiÃªn

Triá»ƒn khai máº¡ng nÆ¡-ron sÃ¢u (DNN) táº¡i biÃªn ngÃ y cÃ ng phá»•
biáº¿n do cÃ¡c yÃªu cáº§u vá» Ä‘á»™ trá»… vÃ  quan ngáº¡i vá» báº£o máº­t cá»§a
cÃ¡c dá»‹ch vá»¥ há»c sÃ¢u. Tuy nhiÃªn, viá»‡c trá»±c tiáº¿p tuÃ¢n theo cÃ¡c
quy trÃ¬nh huáº¥n luyá»‡n vÃ  triá»ƒn khai mÃ´ hÃ¬nh dá»±a trÃªn Ä‘Ã¡m mÃ¢y
thÃ´ng thÆ°á»ng khÃ´ng thá»a Ä‘Ã¡ng do sá»± Ä‘a dáº¡ng khá»•ng lá»“ cá»§a
cÃ¡c tÃ¬nh huá»‘ng biÃªn [13,17,53]. KhÃ´ng giá»‘ng nhÆ° háº§u háº¿t
cÃ¡c mÃ´ hÃ¬nh AI dá»±a trÃªn Ä‘Ã¡m mÃ¢y Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c nhiá»‡m
vá»¥ chung vÃ  Ä‘Æ°á»£c lÆ°u trá»¯ trong cÃ¡c cá»¥m GPU máº¡nh máº½, cÃ¡c
tÃ¬nh huá»‘ng AI biÃªn thÆ°á»ng Ä‘a dáº¡ng vÃ  phÃ¢n máº£nh. Má»—i tÃ¬nh
huá»‘ng biÃªn cÃ³ thá»ƒ xá»­ lÃ½ má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ theo lÄ©nh vá»±c
vÃ  má»™t táº­p há»£p duy nháº¥t cÃ¡c mÃ´i trÆ°á»ng triá»ƒn khai má»¥c tiÃªu.

Theo cÃ¡c Ä‘á»‘i tÃ¡c ngÃ nh cung cáº¥p dá»‹ch vá»¥ AI biÃªn cho cÃ¡c
nhÃ  tÃ¹y chá»‰nh, ná»— lá»±c phÃ¡t triá»ƒn chÃ­nh cá»§a há» Ä‘Æ°á»£c dÃ nh cho
viá»‡c tÃ¹y chá»‰nh cÃ¡c mÃ´ hÃ¬nh Ä‘á»ƒ xá»­ lÃ½ sá»± Ä‘a dáº¡ng cá»§a cÃ¡c tÃ¬nh
huá»‘ng biÃªn, bao gá»“m Ä‘a dáº¡ng pháº§n cá»©ng, Ä‘a dáº¡ng nhiá»‡m vá»¥,

HÃ¬nh 2: Äá»™ trá»… trung bÃ¬nh (ms) cá»§a ResNet50 trong cÃ¡c
mÃ´i trÆ°á»ng triá»ƒn khai khÃ¡c nhau. Trong cÃ i Ä‘áº·t "multi-tenant",
chÃºng tÃ´i giáº£ Ä‘á»‹nh cÃ³ ba quy trÃ¬nh ná»n cháº¡y cÃ¹ng má»™t mÃ´ hÃ¬nh.

Báº£ng 1: CÃ¡c nhiá»‡m vá»¥ AI Ä‘iá»ƒn hÃ¬nh Ä‘Æ°á»£c yÃªu cáº§u bá»Ÿi khÃ¡ch
hÃ ng cá»§a má»™t nhÃ  cung cáº¥p dá»‹ch vá»¥ AI biÃªn.

KhÃ¡ch hÃ ng          Nhiá»‡m vá»¥
CÃ´ng trÆ°á»ng xÃ¢y dá»±ng    PhÃ¡t hiá»‡n ngÆ°á»i máº·c Ä‘á»“ng phá»¥c xanh.
NhÃ  hÃ ng               PhÃ¢n loáº¡i ngÆ°á»i Ä‘á»™i mÅ© Ä‘áº§u báº¿p tráº¯ng.
Bá»‡nh viá»‡n              PhÃ¢n loáº¡i ngÆ°á»i Ä‘eo kháº©u trang.
Trung tÃ¢m mua sáº¯m      PhÃ¡t hiá»‡n ngÆ°á»i hÃºt thuá»‘c.
Ga giao thÃ´ng          PhÃ¡t hiá»‡n xe cÃ³ biá»ƒn sá»‘ xanh.

Ä‘a dáº¡ng phÃ¢n phá»‘i dá»¯ liá»‡u, v.v. Trong sá»‘ Ä‘Ã³, má»™t váº¥n Ä‘á» chÃ­nh
lÃ  Ä‘a dáº¡ng pháº§n cá»©ng. CÃ¡c nhÃ  tÃ¹y chá»‰nh thÆ°á»ng yÃªu cáº§u triá»ƒn
khai cÃ¡c mÃ´ hÃ¬nh Ä‘áº¿n cÃ¡c ná»n táº£ng pháº§n cá»©ng khÃ¡c nhau, cháº³ng
háº¡n nhÆ° mÃ¡y chá»§ biÃªn, mÃ¡y tÃ­nh Ä‘á»ƒ bÃ n, Ä‘iá»‡n thoáº¡i thÃ´ng minh,
vÃ  há»™p AI biÃªn. Do kháº£ nÄƒng tÃ­nh toÃ¡n khÃ¡c nhau, cÃ¹ng má»™t
mÃ´ hÃ¬nh cÃ³ thá»ƒ mang láº¡i hiá»‡u suáº¥t khÃ¡c nhau Ä‘Ã¡ng ká»ƒ trÃªn
cÃ¡c thiáº¿t bá»‹ vÃ  mÃ´i trÆ°á»ng biÃªn khÃ¡c nhau, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹
trong HÃ¬nh 2. Do Ä‘Ã³, cÃ¡c nhÃ  phÃ¡t triá»ƒn cáº§n tÃ¹y chá»‰nh kiáº¿n
trÃºc mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘Ã¡p á»©ng cÃ¡c rÃ ng buá»™c Ä‘á»™ trá»…. Äa dáº¡ng nhiá»‡m
vá»¥ lÃ  má»™t váº¥n Ä‘á» quan trá»ng khÃ¡c mÃ  cÃ¡c nhÃ  cung cáº¥p dá»‹ch
vá»¥ AI biÃªn gáº·p pháº£i. Má»—i nhÃ  tÃ¹y chá»‰nh cÃ³ thá»ƒ cÃ³ má»™t nhiá»‡m
vá»¥ AI cá»¥ thá»ƒ theo lÄ©nh vá»±c dá»±a trÃªn tÃ¬nh huá»‘ng á»©ng dá»¥ng,
nhÆ° Ä‘Æ°á»£c minh há»a trong Báº£ng 1. Äá»ƒ cho phÃ©p cÃ¡c dá»‹ch vá»¥
AI trong cÃ¡c tÃ¬nh huá»‘ng nhÆ° váº­y, viá»‡c tÃ¹y chá»‰nh mÃ´ hÃ¬nh cho
cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau lÃ  cáº§n thiáº¿t.

CÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i Ä‘Æ°á»£c Ä‘á» xuáº¥t cho tá»«ng loáº¡i
má»¥c tiÃªu tÃ¹y chá»‰nh á»Ÿ trÃªn. Tuy nhiÃªn, khi xem xÃ©t hai má»¥c
tiÃªu cÃ¹ng nhau cho nhiá»u tÃ¬nh huá»‘ng biÃªn Ä‘a dáº¡ng, chi phÃ­
trá»Ÿ nÃªn khá»•ng lá»“, vÃ¬ quy trÃ¬nh tÃ¹y chá»‰nh khÃ´ng táº§m thÆ°á»ng
(nhÆ° sáº½ Ä‘Æ°á»£c giáº£i thÃ­ch sau) vÃ  pháº£i Ä‘Æ°á»£c láº·p láº¡i cho má»—i
tÃ¬nh huá»‘ng.

3

--- TRANG 4 ---
Preprint, , 2023 Xu vÃ  Li et al.

2.2 TÃ¹y chá»‰nh MÃ´ hÃ¬nh dá»±a trÃªn Huáº¥n luyá»‡n

Äá»ƒ tÃ¹y chá»‰nh má»™t mÃ´ hÃ¬nh cho má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ theo lÄ©nh
vá»±c nháº¥t Ä‘á»‹nh, thá»±c hÃ nh phá»• biáº¿n lÃ  sá»­ dá»¥ng há»c chuyá»ƒn giao
(TL). PhÆ°Æ¡ng phÃ¡p chá»§ Ä‘áº¡o trong há»c chuyá»ƒn giao lÃ  tinh
chá»‰nh, tá»©c lÃ  khá»Ÿi táº¡o cÃ¡c tham sá»‘ mÃ´ hÃ¬nh vá»›i má»™t mÃ´ hÃ¬nh
Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  tiáº¿p tá»¥c huáº¥n luyá»‡n cÃ¡c tham sá»‘
vá»›i dá»¯ liá»‡u cá»¥ thá»ƒ theo lÄ©nh vá»±c. CÃ¡c quy trÃ¬nh há»c chuyá»ƒn
giao nhÆ° váº­y yÃªu cáº§u cÃ¡c nhÃ  phÃ¡t triá»ƒn thu tháº­p cÃ¡c máº«u
dá»¯ liá»‡u cho nhiá»‡m vá»¥, gáº¯n nhÃ£n chÃºng, vÃ  huáº¥n luyá»‡n mÃ´
hÃ¬nh vá»›i cÃ¡c máº«u Ä‘Æ°á»£c gáº¯n nhÃ£n, Ä‘iá»u nÃ y thÆ°á»ng tá»‘n nhiá»u
lao Ä‘á»™ng, Ä‘Ã²i há»i tÃ i nguyÃªn, vÃ  máº¥t thá»i gian.

Äá»ƒ khá»›p má»™t mÃ´ hÃ¬nh vÃ o má»™t thiáº¿t bá»‹ biÃªn cá»¥ thá»ƒ, cÃ¡c giáº£i
phÃ¡p Ä‘iá»ƒn hÃ¬nh bao gá»“m nÃ©n má»™t mÃ´ hÃ¬nh hiá»‡n cÃ³ [18,50,65]
(vÃ­ dá»¥: pruning, quantization, v.v.) hoáº·c tÃ¬m má»™t kiáº¿n trÃºc
mÃ´ hÃ¬nh má»›i phÃ¹ há»£p vá»›i kháº£ nÄƒng cá»§a thiáº¿t bá»‹ má»¥c tiÃªu [44,
46]. VÃ¬ viá»‡c thiáº¿t káº¿ thá»§ cÃ´ng cÃ¡c mÃ´ hÃ¬nh cho cÃ¡c mÃ´i trÆ°á»ng
biÃªn Ä‘a dáº¡ng ráº¥t cá»“ng ká»nh, thá»±c hÃ nh phá»• biáº¿n lÃ  sá»­ dá»¥ng
cÃ¡c ká»¹ thuáº­t sinh mÃ´ hÃ¬nh tá»± Ä‘á»™ng. NAS [16,34,45,54,56]
lÃ  phÆ°Æ¡ng phÃ¡p sinh mÃ´ hÃ¬nh Ä‘áº¡i diá»‡n vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng
rÃ£i nháº¥t, tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng tá»‘i Æ°u trong má»™t khÃ´ng
gian tÃ¬m kiáº¿m Ä‘Æ°á»£c thiáº¿t káº¿ tá»‘t. Háº§u háº¿t cÃ¡c phÆ°Æ¡ng phÃ¡p
NAS yÃªu cáº§u huáº¥n luyá»‡n cÃ¡c kiáº¿n trÃºc trong quÃ¡ trÃ¬nh tÃ¬m
kiáº¿m [38,43,45,66], Ä‘iá»u nÃ y cá»±c ká»³ tá»‘n thá»i gian (10,000+
giá» GPU) khi táº¡o ra cÃ¡c mÃ´ hÃ¬nh cho má»™t sá»‘ lÆ°á»£ng lá»›n thiáº¿t bá»‹.

Cáº£ hai quy trÃ¬nh trÃªn Ä‘á»u Ä‘áº·t ra thÃ¡ch thá»©c cho cÃ¡c nhÃ  phÃ¡t
triá»ƒn do sá»± Ä‘a dáº¡ng cá»§a cÃ¡c tÃ¬nh huá»‘ng biÃªn. Huáº¥n luyá»‡n hoáº·c
tinh chá»‰nh mÃ´ hÃ¬nh cho má»™t tÃ¬nh huá»‘ng biÃªn thÆ°á»ng yÃªu cáº§u
hÃ ng nghÃ¬n máº«u huáº¥n luyá»‡n Ä‘Æ°á»£c gáº¯n nhÃ£n vÃ  vÃ i giá» trÃªn
cÃ¡c mÃ¡y GPU hiá»‡u suáº¥t cao. TÃ¬m kiáº¿m kiáº¿n trÃºc mÃ´ hÃ¬nh tá»‘i
Æ°u hoáº·c chiáº¿n lÆ°á»£c nÃ©n báº±ng thá»­ vÃ  sai cÅ©ng yÃªu cáº§u nhiá»u
ná»— lá»±c phÃ¡t triá»ƒn.

2.3 TÃ¹y chá»‰nh MÃ´ hÃ¬nh khÃ´ng cáº§n Huáº¥n luyá»‡n

Nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y cá»§a há»c sÃ¢u Ä‘Ã£ cho tháº¥y tÃ­nh kháº£ thi
cá»§a viá»‡c sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c thá»‘ng
nháº¥t Ä‘á»ƒ há»— trá»£ nhiá»u nhiá»‡m vá»¥ downstream khÃ¡c nhau mÃ 
khÃ´ng cáº§n huáº¥n luyá»‡n thÃªm. Cá»¥ thá»ƒ, ngÆ°á»i ta cÃ³ thá»ƒ huáº¥n
luyá»‡n má»™t mÃ´ hÃ¬nh ná»n táº£ng trÃªn má»™t táº­p dá»¯ liá»‡u lá»›n vá»›i nhiá»u
nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c vÃ  trá»±c tiáº¿p sá»­ dá»¥ng mÃ´ hÃ¬nh
cho cÃ¡c nhiá»‡m vá»¥ downstream khÃ¡c nhau vá»›i cÃ¡c prompt Ä‘Æ¡n
giáº£n. Theo cÃ¡c phÃ¢n tÃ­ch thá»±c nghiá»‡m [28], cáº£i thiá»‡n kháº£
nÄƒng cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ thá»ƒ dáº«n Ä‘áº¿n hiá»‡u
suáº¥t tá»‘t hÆ¡n trÃªn cÃ¡c nhiá»‡m vá»¥ downstream.

Äá»“ng thá»i, cáº£ cá»™ng Ä‘á»“ng AI vÃ  cá»™ng Ä‘á»“ng Ä‘iá»‡n toÃ¡n di Ä‘á»™ng
Ä‘á»u Ä‘Ã£ thá»­ nhiá»u cÃ¡ch khÃ¡c nhau Ä‘á»ƒ giáº£m chi phÃ­ táº¡o ra cÃ¡c
mÃ´ hÃ¬nh nháº¹ cho cÃ¡c thiáº¿t bá»‹ biÃªn. NAS má»™t láº§n [3,5,22,37]
Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ huáº¥n luyá»‡n báº±ng cÃ¡ch
cho phÃ©p cÃ¡c máº¡ng á»©ng viÃªn chia sáº» má»™t supernet Ä‘Æ°á»£c tham
sá»‘ hÃ³a quÃ¡ má»©c chung. CÃ¡c mÃ´ hÃ¬nh tá»‘t nháº¥t cho thiáº¿t bá»‹ má»¥c
tiÃªu cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y báº±ng cÃ¡ch trá»±c tiáº¿p tÃ¬m kiáº¿m má»™t
subnet trong supernet. CÃ¡c nhÃ  nghiÃªn cá»©u Ä‘iá»‡n toÃ¡n di Ä‘á»™ng
cÅ©ng Ä‘Ã£ Ä‘á» xuáº¥t má»Ÿ rá»™ng cÃ¡c mÃ´ hÃ¬nh má»™t cÃ¡ch Ä‘á»™ng Ä‘á»ƒ cung
cáº¥p má»™t loáº¡t rá»™ng cÃ¡c Ä‘Ã¡nh Ä‘á»•i tÃ i nguyÃªn-Ä‘á»™ chÃ­nh xÃ¡c. Háº§u
háº¿t chÃºng Ã¡p dá»¥ng pruning cÃ³ cáº¥u trÃºc hoáº·c thÃ­ch á»©ng kiáº¿n
trÃºc mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o ra cÃ¡c mÃ´ hÃ¬nh con [13,17,39,57,58] vá»›i
cÃ¡c má»©c chi phÃ­ tÃ­nh toÃ¡n khÃ¡c nhau.

ThÃ­ch á»©ng mÃ´ hÃ¬nh khÃ´ng cáº§n huáº¥n luyá»‡n cÅ©ng cÃ³ thá»ƒ Ä‘áº¡t
Ä‘Æ°á»£c báº±ng cÃ¡ch láº¯p rÃ¡p Ä‘á»™ng cÃ¡c module nÆ¡-ron, Ä‘Ã£ Ä‘Æ°á»£c
tháº£o luáº­n trong máº¡ng nÆ¡-ron Ä‘á»™ng [19] vÃ  láº­p trÃ¬nh neuro-
symbolic [41]. Máº¡ng nÆ¡-ron Ä‘á»™ng lÃ  má»™t loáº¡i DNN há»— trá»£
suy luáº­n linh hoáº¡t dá»±a trÃªn Ä‘á»™ khÃ³ cá»§a Ä‘áº§u vÃ o. Khi Ä‘áº§u vÃ o
dá»…, máº¡ng nÆ¡-ron Ä‘á»™ng cÃ³ thá»ƒ giáº£m tÃ­nh toÃ¡n báº±ng cÃ¡ch bá»
qua má»™t táº­p há»£p cÃ¡c khá»‘i [52,55] hoáº·c thoÃ¡t tá»« cÃ¡c lá»›p giá»¯a
[2,12,31,32]. Máº·c dÃ¹ cÃ¡c subnet khÃ¡c nhau (má»™t subnet lÃ 
má»™t Ä‘Æ°á»ng dáº«n trong NN Ä‘á»™ng) cÃ³ táº£i tÃ­nh toÃ¡n khÃ¡c nhau,
chÃºng khÃ´ng thá»ƒ Ä‘Æ°á»£c tÃ¹y chá»‰nh cho cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau,
vÃ  tÃ­nh Ä‘á»™ng cá»§a mÃ´ hÃ¬nh phá»¥ thuá»™c vÃ o Ä‘áº§u vÃ o cÅ©ng khÃ´ng
phÃ¹ há»£p cho cÃ¡c mÃ´i trÆ°á»ng biÃªn. Láº­p trÃ¬nh neuro-symbolic
chá»©ng minh kháº£ nÄƒng giáº£i quyáº¿t cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau vá»›i
cÃ¹ng má»™t táº­p há»£p cÃ¡c module nÆ¡-ron báº±ng cÃ¡ch káº¿t há»£p chÃºng
má»™t cÃ¡ch ngá»¯ nghÄ©a. Cá»¥ thá»ƒ, mÃ´ hÃ¬nh Ä‘á»ƒ giáº£i quyáº¿t má»™t nhiá»‡m
vá»¥ phá»©c táº¡p cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ° má»™t chÆ°Æ¡ng trÃ¬nh gá»i cÃ¡c
module chá»©c nÄƒng nhá» hÆ¡n. Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p láº­p
trÃ¬nh neuro-symbolic hiá»‡n cÃ³ thiáº¿u tÃ­nh linh hoáº¡t vÃ  kháº£ nÄƒng
tÃ¹y chá»‰nh hÆ°á»›ng tÃ i nguyÃªn, vÃ¬ cÃ¡c module thÆ°á»ng tÄ©nh vÃ 
Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a rÃµ rÃ ng.

Tuy nhiÃªn, váº«n cÃ²n thÃ¡ch thá»©c trong viá»‡c tá»‘i Æ°u hÃ³a quy
trÃ¬nh tÃ¹y chá»‰nh mÃ´ hÃ¬nh cho cáº£ nhiá»‡m vá»¥ vÃ  thiáº¿t bá»‹ cÃ¹ng má»™t
lÃºc. TÃ¹y chá»‰nh hÆ°á»›ng nhiá»‡m vá»¥ vÃ  tÃ¹y chá»‰nh hÆ°á»›ng tÃ i nguyÃªn
cÃ³ pháº§n mÃ¢u thuáº«n. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÃ¹y chá»‰nh nhiá»‡m vá»¥ nhanh,
mÃ´ hÃ¬nh Ä‘Æ°á»£c mong muá»‘n cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a qua
nhiá»‡m vá»¥, Ä‘iá»u nÃ y thÆ°á»ng yÃªu cáº§u má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n
luyá»‡n trÆ°á»›c thá»‘ng nháº¥t vá»›i kháº£ nÄƒng lá»›n, trong khi triá»ƒn khai
mÃ´ hÃ¬nh Ä‘áº¿n cÃ¡c thiáº¿t bá»‹ biÃªn háº¡n cháº¿ tÃ i nguyÃªn Ä‘a dáº¡ng
yÃªu cáº§u cÃ¡c mÃ´ hÃ¬nh nháº¹ khÃ´ng Ä‘á»“ng nháº¥t.

3 THIáº¾T Káº¾ NN-FACTORY

Äá»‹nh nghÄ©a váº¥n Ä‘á». ChÃ­nh thá»©c, má»¥c tiÃªu cá»§a viá»‡c tÃ¹y chá»‰nh
DNN biÃªn lÃ  táº¡o ra má»™t mÃ´ hÃ¬nh ğ‘“Ë†ğ›¼,Ë†ğœƒ vá»›i kiáº¿n trÃºc Ë†ğ›¼ vÃ 
tham sá»‘ Ë†ğœƒ cÃ³ thá»ƒ xá»­ lÃ½ chÃ­nh xÃ¡c nhiá»‡m vá»¥ biÃªn trong khi
thá»a mÃ£n cÃ¡c rÃ ng buá»™c hiá»‡u suáº¥t. tá»©c lÃ 

Ë†ğ›¼,Ë†ğœƒ=arg min
ğ›¼,ğœƒğ¿(ğ‘“ğ›¼,ğœƒ(ğ‘‹ğ‘’),ğ‘Œğ‘’)
ğ‘ .ğ‘¡.ğ‘šğ‘’ğ‘š(ğ›¼)<ğ‘€ğ¸ğ‘€ğ‘’ğ‘ğ‘›ğ‘‘ğ‘™ğ‘ğ‘¡(ğ›¼)<ğ¿ğ´ğ‘‡ğ‘’(1)

trong Ä‘Ã³ ğ‘‹ğ‘’ vÃ  ğ‘Œğ‘’ lÃ  Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a cÃ¡c máº«u dá»¯ liá»‡u
nhiá»‡m vá»¥ biÃªn, ğ¿ lÃ  loss dá»± Ä‘oÃ¡n, vÃ  ğ‘€ğ¸ğ‘€ğ‘’ vÃ  ğ¿ğ´ğ‘‡ğ‘’ lÃ 
cÃ¡c giá»›i háº¡n bá»™ nhá»› vÃ  Ä‘á»™ trá»… táº¡i mÃ´i trÆ°á»ng biÃªn.

Cá»¥ thá»ƒ, chÃºng tÃ´i táº­p trung vÃ o viá»‡c táº¡o ra cÃ¡c mÃ´ hÃ¬nh cho
cÃ¡c nhiá»‡m vá»¥ trong má»™t khÃ´ng gian tá»• há»£p, tá»©c lÃ  má»—i nhiá»‡m
vá»¥ Ä‘Æ°á»£c mÃ´ táº£ nhÆ° má»™t sá»± káº¿t há»£p cá»§a má»™t sá»‘ thuá»™c tÃ­nh
(vÃ­ dá»¥: mÃ u sáº¯c, tráº¡ng thÃ¡i, thá»±c thá»ƒ, v.v.) vÃ  cÃ¡c thuá»™c tÃ­nh
Ä‘Æ°á»£c chia sáº» qua cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau (vÃ­ dá»¥: xe táº£i Ä‘á»,
mÃ¨o tráº¯ng, v.v.). CÃ¡c nhÃ  phÃ¡t triá»ƒn cÃ³ thá»ƒ linh hoáº¡t Ä‘á»‹nh
nghÄ©a khÃ´ng gian nhiá»‡m vá»¥ theo cÃ¡c tÃ¬nh huá»‘ng biÃªn má»¥c
tiÃªu cá»§a há».

4

--- TRANG 5 ---
MÃ´ hÃ¬nh Sinh cho cÃ¡c MÃ´ hÃ¬nh: TÃ¹y chá»‰nh DNN nhanh chÃ³ng cho cÃ¡c Nhiá»‡m vá»¥ Äa dáº¡ng vÃ  RÃ ng buá»™c TÃ i nguyÃªn Preprint, , 2023

CÃ´ng viá»‡c cá»§a chÃºng tÃ´i láº¥y cáº£m há»©ng tá»« tÃ­nh modular vÃ 
kháº£ nÄƒng tá»• há»£p Ä‘á»™ng cá»§a máº¡ng nÆ¡-ron vÃ  AI sinh. Táº§m nhÃ¬n
cá»§a chÃºng tÃ´i lÃ  táº¡o ra má»™t há»‡ thá»‘ng sinh má»™t-cho-táº¥t-cáº£ trong
Ä‘Ã³ cÃ¡c mÃ´ hÃ¬nh DNN cá»¥ thá»ƒ theo biÃªn khÃ¡c nhau cÃ³ thá»ƒ Ä‘Æ°á»£c
táº¡o ra trá»±c tiáº¿p báº±ng cÃ¡ch cáº¥u hÃ¬nh vÃ  láº¯p rÃ¡p cÃ¡c module
nÆ¡-ron Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c, nhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1.

Viá»‡c thá»±c hiá»‡n táº§m nhÃ¬n nÃ y ráº¥t thÃ¡ch thá»©c vÃ¬ (1) khÃ³ thiáº¿t
káº¿ vÃ  phÃ¡t triá»ƒn cÃ¡c module cÃ³ thá»ƒ Ä‘Æ°á»£c láº¯p rÃ¡p Ä‘á»ƒ thá»±c hiá»‡n
cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau vÃ  Ä‘Ã¡p á»©ng cÃ¡c rÃ ng buá»™c tÃ i nguyÃªn
khÃ¡c nhau (2) ngay cáº£ khi cÃ¡c module nhÆ° váº­y Ä‘Æ°á»£c táº¡o ra,
viá»‡c táº¡o ra má»™t mÃ´ hÃ¬nh phÃ¹ há»£p cho má»™t tÃ¬nh huá»‘ng biÃªn
cá»¥ thá»ƒ váº«n cÃ³ thá»ƒ khÃ³ khÄƒn do khÃ´ng gian tá»• há»£p khá»•ng lá»“
cá»§a cÃ¡c á»©ng viÃªn mÃ´ hÃ¬nh.

ChÃºng tÃ´i cá»‘ gáº¯ng giáº£i quyáº¿t nhá»¯ng thÃ¡ch thá»©c nÃ y báº±ng
má»™t phÆ°Æ¡ng phÃ¡p end-to-end cÃ³ tÃªn NN-Factory. ChÃºng tÃ´i
nháº¥n máº¡nh ráº±ng NN-Factory lÃ  má»™t paradigm sinh Ä‘áº§u tiÃªn
tá»«ng cÃ³ vá» tÃ¹y chá»‰nh mÃ´ hÃ¬nh cho cÃ¡c tÃ¬nh huá»‘ng biÃªn Ä‘a
dáº¡ng, cho phÃ©p sinh mÃ´ hÃ¬nh nhanh chÃ³ng khÃ´ng cáº§n huáº¥n
luyá»‡n cho cÃ¡c nhiá»‡m vá»¥ vÃ  rÃ ng buá»™c tÃ i nguyÃªn khÃ¡c nhau.

3.1 Tá»•ng quan

Ã tÆ°á»Ÿng chÃ­nh cá»§a NN-Factory lÃ  huáº¥n luyá»‡n cÃ¹ng nhau má»™t
táº­p há»£p cÃ¡c module nÆ¡-ron vÃ  má»™t bá»™ táº¡o chiáº¿n lÆ°á»£c láº¯p rÃ¡p
module (tá»©c lÃ  assembler) theo cÃ¡ch cÃ³ cÄƒn cá»© nhiá»‡m vá»¥ vÃ 
Ä‘á»™ thÆ°a. Cá»¥ thá»ƒ, cÃ¡c module Ä‘Æ°á»£c cáº¯t tá»« má»™t máº¡ng nÆ¡-ron
Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c (supernet), Ä‘iá»u nÃ y trÃ¡nh Ä‘Æ°á»£c viá»‡c
phÃ¡t triá»ƒn cá»“ng ká»nh cÃ¡c module riÃªng láº» vá»›i chi phÃ­ kháº£
nÄƒng diá»…n giáº£i háº¡n cháº¿. Äá»“ng thá»i, báº±ng cÃ¡ch há»c assembler
trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vá»›i nhiá»u nhiá»‡m vá»¥ vÃ  yÃªu cáº§u
Ä‘á»™ thÆ°a khÃ¡c nhau, NN-Factory cÃ³ thá»ƒ trá»±c tiáº¿p táº¡o ra kiáº¿n
trÃºc mÃ´ hÃ¬nh cho má»™t nhiá»‡m vá»¥ vÃ  yÃªu cáº§u Ä‘á»™ thÆ°a má»›i vá»›i
má»™t láº§n forward pass duy nháº¥t, giáº£m Ä‘Ã¡ng ká»ƒ khÃ´ng gian
tÃ¬m kiáº¿m mÃ´ hÃ¬nh.

HÃ¬nh 3 cho tháº¥y tá»•ng quan vá» NN-Factory. NÃ³ chá»©a ba thÃ nh
pháº§n chÃ­nh, bao gá»“m má»™t supernet vá»›i cÃ¡c module cÃ³ thá»ƒ
cáº¯t Ä‘Æ°á»£c, má»™t assembler nháº­n biáº¿t yÃªu cáº§u, vÃ  má»™t module
tÃ¬m kiáº¿m kiáº¿n trÃºc nháº¹. vÃ  má»™t bá»™ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t biÃªn
cá»¥ thá»ƒ theo thiáº¿t bá»‹. Supernet chá»©a cÃ¡c module cÆ¡ báº£n cÃ³
thá»ƒ Ä‘Æ°á»£c káº¿t há»£p linh hoáº¡t Ä‘á»ƒ táº¡o thÃ nh cÃ¡c mÃ´ hÃ¬nh tÃ¹y chá»‰nh
(subnet). Assembler module nháº­n biáº¿t yÃªu cáº§u lÃ  má»™t mÃ´ hÃ¬nh
sinh táº¡o ra cÃ¡c cáº¥u hÃ¬nh láº¯p rÃ¡p module dá»±a trÃªn nhiá»‡m vá»¥
vÃ  yÃªu cáº§u Ä‘á»™ thÆ°a cho trÆ°á»›c. Má»—i cáº¥u hÃ¬nh Ä‘Æ°á»£c táº¡o ra Ã¡nh
xáº¡ Ä‘áº¿n má»™t á»©ng viÃªn mÃ´ hÃ¬nh. Module tÃ¬m kiáº¿m kiáº¿n trÃºc
nháº¹ tÃ¬m ra mÃ´ hÃ¬nh tá»‘i Æ°u báº±ng cÃ¡ch láº·p Ä‘i láº·p láº¡i tÃ¬m kiáº¿m
vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c á»©ng viÃªn mÃ´ hÃ¬nh theo má»™t bá»™ Ä‘Ã¡nh giÃ¡ hiá»‡u
suáº¥t cá»¥ thá»ƒ theo biÃªn.

Vá»›i má»™t tÃ¬nh huá»‘ng biÃªn cho trÆ°á»›c (Ä‘Æ°á»£c mÃ´ táº£ bá»Ÿi nhiá»‡m
vá»¥, loáº¡i thiáº¿t bá»‹, vÃ  rÃ ng buá»™c tÃ i nguyÃªn), quy trÃ¬nh tÃ¹y chá»‰nh
mÃ´ hÃ¬nh cá»§a NN-Factory bao gá»“m cÃ¡c bÆ°á»›c sau:

(1)Module tÃ¬m kiáº¿m kiáº¿n trÃºc nháº¹ Ä‘á» xuáº¥t má»™t yÃªu cáº§u sinh
mÃ´ hÃ¬nh < task, activation limit >, trong Ä‘Ã³ activation limit
lÃ  tá»· lá»‡ tá»‘i Ä‘a cá»§a cÃ¡c module Ä‘Æ°á»£c kÃ­ch hoáº¡t trong mÃ´ hÃ¬nh.

(2)Assembler nháº­n biáº¿t yÃªu cáº§u dá»± Ä‘oÃ¡n má»™t cáº¥u hÃ¬nh láº¯p
rÃ¡p module dá»±a trÃªn yÃªu cáº§u. Cáº¥u hÃ¬nh mÃ´ táº£ cÃ¡ch táº¡o
ra má»™t á»©ng viÃªn mÃ´ hÃ¬nh báº±ng cÃ¡ch kÃ­ch hoáº¡t vÃ  láº¯p rÃ¡p
cÃ¡c module trong supernet.

(3)Bá»™ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»¥ thá»ƒ theo thiáº¿t bá»‹ Ä‘Ã¡nh giÃ¡ cáº¥u
hÃ¬nh Ä‘Æ°á»£c táº¡o ra (tá»©c lÃ  á»©ng viÃªn mÃ´ hÃ¬nh) so vá»›i cÃ¡c
rÃ ng buá»™c tÃ i nguyÃªn biÃªn. Náº¿u cÃ¡c rÃ ng buá»™c Ä‘Æ°á»£c thá»a
mÃ£n vÃ  ngÃ¢n sÃ¡ch bá»™ nhá»›/Ä‘á»™ trá»… Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘áº§y Ä‘á»§,
thÃ¬ tráº£ vá» á»©ng viÃªn hiá»‡n táº¡i. NgÆ°á»£c láº¡i, quay láº¡i bÆ°á»›c (1)
vá»›i má»™t yÃªu cáº§u sinh má»›i.

CÃ¡c pháº§n tiáº¿p theo sáº½ giá»›i thiá»‡u cÃ¡c thÃ nh pháº§n chÃ­nh chi
tiáº¿t hÆ¡n.

3.2 Supernet vá»›i cÃ¡c Module cÃ³ thá»ƒ Cáº¯t Ä‘Æ°á»£c

Supernet chá»‹u trÃ¡ch nhiá»‡m cung cáº¥p cÃ¡c module nÆ¡-ron cÆ¡
báº£n cÃ³ thá»ƒ Ä‘Æ°á»£c láº¯p rÃ¡p láº¡i Ä‘á»ƒ Ä‘Ã¡p á»©ng cÃ¡c nhiá»‡m vá»¥ vÃ  rÃ ng
buá»™c tÃ i nguyÃªn khÃ¡c nhau. Viá»‡c cáº¯t má»™t máº¡ng nÆ¡-ron hiá»‡n
cÃ³ cho cÃ¡c khá»‘i chá»©c nÄƒng khÃ¡c nhau Ä‘á»ƒ cÃ³ kháº£ nÄƒng diá»…n
giáº£i Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y [61â€“63], nhÆ°ng cÃ¡c module
Ä‘Æ°á»£c cáº¯t thÆ°á»ng thÃ´ vÃ  khÃ³ láº¯p rÃ¡p láº¡i. ChÃºng tÃ´i quyáº¿t Ä‘á»‹nh
trá»±c tiáº¿p huáº¥n luyá»‡n má»™t supernet cÃ³ thá»ƒ cáº¯t Ä‘Æ°á»£c Ä‘á»ƒ cÃ³ sá»±
tÃ¡ch biá»‡t modular vÃ  kháº£ nÄƒng tá»• há»£p tá»‘t hÆ¡n.

ChÃºng tÃ´i xÃ¢y dá»±ng supernet modular báº±ng cÃ¡ch má»Ÿ rá»™ng
má»™t máº¡ng backbone hiá»‡n cÃ³. Máº¡ng backbone trÃ­ch xuáº¥t cÃ¡c
Ä‘áº·c trÆ°ng tá»« cÃ¡c Ä‘áº§u vÃ o cho trÆ°á»›c vÃ  Ä‘Æ°a ra cÃ¡c dá»± Ä‘oÃ¡n
cuá»‘i cÃ¹ng. ChÃºng tÃ´i cÃ³ thá»ƒ há»— trá»£ cÃ¡c backbone phá»• biáº¿n
dá»±a trÃªn Máº¡ng NÆ¡-ron TÃ­ch cháº­p (CNN) vÃ  Transformer,
cháº³ng háº¡n nhÆ° ResNet [20], EfficientNet [47], vÃ  Vision
Transformer [11].

Backbone CNN. Äáº§u tiÃªn, chÃºng tÃ´i giá»›i thiá»‡u cÃ¡ch chuyá»ƒn
Ä‘á»•i má»™t lá»›p tÃ­ch cháº­p trong kiáº¿n trÃºc CNN thÃ nh cÃ¡c module
cÃ³ thá»ƒ cáº¯t Ä‘Æ°á»£c. Cho má»™t feature map ğ‘¥ lÃ m Ä‘áº§u vÃ o, Ä‘áº§u
ra cá»§a lá»›p tÃ­ch cháº­p thá»© ğ‘™ lÃ  ğ‘‚ğ‘™(ğ‘¥). Trong má»™t CNN thÃ´ng
thÆ°á»ng, ğ‘‚ğ‘™(ğ‘¥) Ä‘Æ°á»£c tÃ­nh nhÆ° sau:

ğ‘‚ğ‘™
ğ‘–=ğœ(ğ¹ğ‘™
ğ‘–âˆ—ğ¼ğ‘™(ğ‘¥)) (2)

trong Ä‘Ã³ ğ‘‚ğ‘™
ğ‘– lÃ  kÃªnh thá»© ğ‘– cá»§a ğ‘‚ğ‘™(ğ‘¥), ğ¹ğ‘™
ğ‘– lÃ  bá»™ lá»c thá»© ğ‘–, ğœ(Â·)
biá»ƒu thá»‹ hÃ m kÃ­ch hoáº¡t phi tuyáº¿n theo tá»«ng pháº§n tá»­ vÃ  âˆ—
biá»ƒu thá»‹ phÃ©p tÃ­ch cháº­p. feature map Ä‘áº§u ra ğ‘‚ğ‘™(ğ‘¥) Ä‘Æ°á»£c thu
Ä‘Æ°á»£c báº±ng cÃ¡ch Ã¡p dá»¥ng táº¥t cáº£ cÃ¡c bá»™ lá»c ğ¹ğ‘
ğ‘– trong lá»›p hiá»‡n
táº¡i lÃªn feature map Ä‘áº§u vÃ o ğ¼ğ‘™(ğ‘¥).

Trong NN-Factory, chÃºng tÃ´i coi má»—i bá»™ lá»c tÃ­ch cháº­p nhÆ°
má»™t module cÃ³ thá»ƒ Ä‘Æ°á»£c kÃ­ch hoáº¡t cÃ³ Ä‘iá»u kiá»‡n. Báº±ng cÃ¡ch
kÃ­ch hoáº¡t cÃ¡c káº¿t há»£p khÃ¡c nhau cá»§a cÃ¡c bá»™ lá»c trong má»™t
lá»›p tÃ­ch cháº­p, cÃ¡c lá»›p káº¿t quáº£ cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  cÃ³ cÃ¡c chá»©c
nÄƒng khÃ¡c nhau.

Dá»±a trÃªn sá»± phÃ¢n tÃ¡ch modular nhÆ° váº­y, chÃºng tÃ´i giá»›i thiá»‡u
má»™t vector gate ğ‘” Ä‘á»ƒ Ä‘iá»u khiá»ƒn viá»‡c kÃ­ch hoáº¡t cÃ¡c module.
Vá»›i vector gate, viá»‡c tÃ­nh toÃ¡n feature map ğ‘‚ğ‘™
ğ‘– trong PhÆ°Æ¡ng
trÃ¬nh 2 Ä‘Æ°á»£c diá»…n Ä‘áº¡t láº¡i nhÆ° sau:

ğ‘‚ğ‘™
ğ‘–=ğœ(ğ¹ğ‘™
ğ‘–âˆ—ğ¼ğ‘™(ğ‘¥))Â·ğ‘”ğ‘™
ğ‘– (3)

5

--- TRANG 6 ---
Preprint, , 2023 Xu vÃ  Li et al.

HÃ¬nh 3: Tá»•ng quan kiáº¿n trÃºc cá»§a NN-Factory.

HÃ¬nh 4: Minh há»a vá» lá»›p tÃ­ch cháº­p modular vá»›i cÃ¡c bá»™ lá»c
tÃ­ch cháº­p Ä‘Æ°á»£c kÃ­ch hoáº¡t cÃ³ Ä‘iá»u kiá»‡n. ğ‘‚ Ä‘áº¡i diá»‡n cho feature
map Ä‘áº§u ra cá»§a má»™t lá»›p tÃ­ch cháº­p vá»›i bá»‘n kÃªnh. Má»—i pháº§n
tá»­ trong vector gate Ã¡nh xáº¡ Ä‘áº¿n má»™t kÃªnh. Báº±ng cÃ¡ch Ã¡p
dá»¥ng cÃ¡c vector gate khÃ¡c nhau, cÃ¡c káº¿t há»£p khÃ¡c nhau
cá»§a cÃ¡c bá»™ lá»c Ä‘Æ°á»£c kÃ­ch hoáº¡t.

trong Ä‘Ã³ ğ‘”ğ‘™
ğ‘– lÃ  má»¥c trong ğ‘” tÆ°Æ¡ng á»©ng vá»›i bá»™ lá»c thá»© ğ‘– táº¡i
lá»›p ğ‘™ vÃ  0 lÃ  má»™t feature map 2-D vá»›i táº¥t cáº£ cÃ¡c pháº§n tá»­
báº±ng 0, chá»‰ khi ğ‘”ğ‘™
ğ‘– báº±ng 1, bá»™ lá»c thá»© ğ‘– má»›i Ä‘Æ°á»£c Ã¡p dá»¥ng
lÃªn ğ¼ğ‘™ Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng. HÃ¬nh 4 mÃ´ táº£ quy trÃ¬nh tÃ­nh
toÃ¡n nhÆ° Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ trÃªn.

Supernet modular Ä‘Æ°á»£c thu Ä‘Æ°á»£c báº±ng cÃ¡ch Ã¡p dá»¥ng quy
trÃ¬nh modular hÃ³a cho cÃ¡c lá»›p tÃ­ch cháº­p chÃ­nh trong mÃ´ hÃ¬nh
CNN. Lá»›p batch normalization sau má»—i lá»›p tÃ­ch cháº­p Ä‘Æ°á»£c
cáº¯t theo cÃ¡ch tá»«ng kÃªnh vÃ  Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn bá»Ÿi cÃ¹ng má»™t
vector gate. ÄÃ¡ng chÃº Ã½ ráº±ng, trong cÃ¡c kiáº¿n trÃºc CNN sÃ¢u
hiá»‡n Ä‘áº¡i nhÆ° ResNet, EfficientNet, vÃ  MobileNet, cÃ¡c lá»›p
tÃ­ch cháº­p Ä‘Æ°á»£c tá»• chá»©c thÃ nh nhiá»u khá»‘i cÆ¡ báº£n. ChÃºng tÃ´i
khÃ´ng chuyá»ƒn Ä‘á»•i cÃ¡c lá»›p tÃ­ch cháº­p á»Ÿ cuá»‘i má»—i khá»‘i cÆ¡ báº£n
Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t hÃ¬nh dáº¡ng vá»›i cÃ¡c káº¿t ná»‘i residual. HÃ¬nh
6(a) vÃ  (b) minh há»a cÃ¡c khá»‘i cÆ¡ báº£n CNN phá»• biáº¿n Ä‘Æ°á»£c
tÃ­ch há»£p vá»›i cÃ¡c gate module cÃ³ thá»ƒ há»c.

Backbone Transformer. ChÃºng tÃ´i cÅ©ng há»— trá»£ táº¡o ra cÃ¡c
supernet modular tá»« cÃ¡c backbone Transformer. CÃ¡c thÃ nh
pháº§n chÃ­nh cá»§a Transformer bao gá»“m cÃ¡c lá»›p self-attention
vÃ  máº¡ng feed-forward (FFN). Theo cÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y
[10], cÃ¡c thÃ nh pháº§n FFN lÆ°u trá»¯ cÃ¡c kiáº¿n thá»©c thá»±c táº¿ khÃ¡c
nhau Ä‘Æ°á»£c há»c tá»« dá»¯ liá»‡u. Má»™t FFN lÃ  má»™t máº¡ng káº¿t ná»‘i
Ä‘áº§y Ä‘á»§ hai lá»›p, xá»­ lÃ½ má»™t biá»ƒu diá»…n Ä‘áº§u vÃ o ğ‘¥âˆˆâ„^{ğ‘‘_{ğ‘šğ‘œğ‘‘ğ‘’ğ‘™}} nhÆ° sau:

â„=ğ‘¥ğ‘Šâ‚
ğ¹(ğ‘¥)=ğœ(â„)ğ‘Šâ‚‚(4)

trong Ä‘Ã³ ğ‘Šâ‚âˆˆâ„^{ğ‘‘_{ğ‘šğ‘œğ‘‘ğ‘’ğ‘™}Ã—ğ‘‘_{ğ‘“ğ‘“}} vÃ  ğ‘Šâ‚‚âˆˆâ„^{ğ‘‘_{ğ‘“ğ‘“}Ã—ğ‘‘_{ğ‘šğ‘œğ‘‘ğ‘’ğ‘™}} lÃ 
cÃ¡c ma tráº­n trá»ng sá»‘.

Kiáº¿n trÃºc Transformer khÃ´ng Ä‘Æ°á»£c thiáº¿t káº¿ modular, nhÆ°ng
nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh má»™t mÃ´ hÃ¬nh Mixture-of-
Experts (MoE) tÆ°Æ¡ng Ä‘Æ°Æ¡ng [23,64], trong Ä‘Ã³ má»—i expert
cÃ³ thá»ƒ Ä‘Æ°á»£c coi nhÆ° má»™t module chá»©c nÄƒng Ä‘Æ°á»£c kÃ­ch hoáº¡t
cÃ³ Ä‘iá»u kiá»‡n. Láº¥y cáº£m há»©ng tá»« Ã½ tÆ°á»Ÿng nÃ y, chÃºng tÃ´i Ä‘á»
xuáº¥t modular hÃ³a cÃ¡c lá»›p FFN trong cÃ¡c backbone Transformer
vÃ  diá»…n Ä‘áº¡t láº¡i viá»‡c tÃ­nh toÃ¡n ğ¹(ğ‘¥) trong PhÆ°Æ¡ng trÃ¬nh 4
nhÆ° sau:

â„=ğ‘¥ğ‘Šâ‚
â„â€²=â„Â·ğ‘”_{ğ¹ğ¹ğ‘}
ğ¹(ğ‘¥)=ğœ(â„â€²)ğ‘Šâ‚‚(5)

trong Ä‘Ã³ ğ‘”_{ğ¹ğ¹ğ‘} Ä‘áº¡i diá»‡n cho viá»‡c lá»±a chá»n gate cho lá»›p
FFN hiá»‡n táº¡i. Náº¿u vá»‹ trÃ­ thá»© ğ‘– trong ğ‘”_{ğ¹ğ¹ğ‘} lÃ  0, vá»‹ trÃ­ tÆ°Æ¡ng
á»©ng trong â„â€² cÅ©ng Ä‘Æ°á»£c Ä‘áº·t thÃ nh 0, cho biáº¿t cÃ¡c tham sá»‘
trong cÃ¡c pháº§n tÆ°Æ¡ng á»©ng cá»§a ğ‘Šâ‚ vÃ  ğ‘Šâ‚‚ khÃ´ng Ä‘Æ°á»£c kÃ­ch
hoáº¡t. Trong HÃ¬nh 5, chÃºng tÃ´i chá»©ng minh cÃ¡c tham sá»‘ váº«n
khÃ´ng hoáº¡t Ä‘á»™ng dÆ°á»›i sá»± lá»±a chá»n cá»§a gate.

TÆ°Æ¡ng tá»±, báº±ng cÃ¡ch Ã¡p dá»¥ng quy trÃ¬nh modular hÃ³a cho
táº¥t cáº£ cÃ¡c FFN trong má»™t backbone Transformer, chÃºng tÃ´i
cÃ³ thá»ƒ thu Ä‘Æ°á»£c má»™t supernet dá»±a trÃªn Transformer cho
NN-Factory, má»™t minh há»a vá»›i backbone ViT Ä‘Æ°á»£c hiá»ƒn thá»‹
trong HÃ¬nh 6(c).

3.3 Assembler Module Nháº­n biáº¿t YÃªu cáº§u

Assembler module Ä‘Ã³ng vai trÃ² quan trá»ng trong NN-Factory
- nÃ³ táº¡o ra cÃ¡c vector gate Ä‘iá»u khiá»ƒn viá»‡c kÃ­ch hoáº¡t cÃ¡c
module nÆ¡-ron trong supernet, sao cho cÃ¡c module Ä‘Æ°á»£c
kÃ­ch hoáº¡t cÃ³ thá»ƒ Ä‘Æ°á»£c láº¯p rÃ¡p Ä‘á»ƒ táº¡o ra má»™t á»©ng viÃªn mÃ´ hÃ¬nh.

Äáº§u vÃ o cá»§a assembler module lÃ  má»™t yÃªu cáº§u sinh mÃ´ hÃ¬nh,
Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t bá»™ < ğ‘¡ğ‘ğ‘ ğ‘˜, ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡ >. ğ‘¡ğ‘ğ‘ ğ‘˜
lÃ  mÃ´ táº£ cá»§a nhiá»‡m vá»¥ má»¥c tiÃªu trong tÃ¬nh huá»‘ng biÃªn. NhÆ°
Ä‘Ã£ Ä‘á» cáº­p trong Pháº§n 3, má»™t nhiá»‡m vá»¥ cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n
nhÆ° má»™t sá»± káº¿t há»£p cá»§a má»™t sá»‘ thuá»™c tÃ­nh. VÃ­ dá»¥, nhiá»‡m vá»¥
'phÃ¡t hiá»‡n chÃ³ Ä‘en' Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° {ğ‘ğ‘™ğ‘ğ‘ğ‘˜, ğ‘‘ğ‘œğ‘”} vÃ  'phÃ¡t
hiá»‡n mÃ¨o tráº¯ng' lÃ  {ğ‘¤â„ğ‘–ğ‘¡ğ‘’, ğ‘ğ‘ğ‘¡}. ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡ lÃ  má»™t yÃªu
cáº§u Ä‘á»™ thÆ°a Ä‘iá»u chá»‰nh tá»· lá»‡ cÃ¡c module Ä‘Æ°á»£c kÃ­ch hoáº¡t trong
mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra. Má»™t giá»›i háº¡n tháº¥p hÆ¡n sáº½ khuyáº¿n khÃ­ch
assembler táº¡o ra cÃ¡c á»©ng viÃªn mÃ´ hÃ¬nh nhá» hÆ¡n.

Äáº§u ra cá»§a assembler lÃ  cÃ¡c vector gate ğ‘” nhÆ° Ä‘Æ°á»£c mÃ´ táº£
trong Pháº§n 3.2. Má»—i vector gate ğ‘”ğ‘™ tÆ°Æ¡ng á»©ng vá»›i má»™t lá»›p
modular ğ‘™ trong supernet vÃ  xÃ¡c Ä‘á»‹nh viá»‡c kÃ­ch hoáº¡t cÃ¡c
module trong lá»›p.

Kiáº¿n trÃºc máº¡ng cá»§a assembler Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 7.
NÃ³ bao gá»“m ba thÃ nh pháº§n chÃ­nh, bao gá»“m má»™t bá»™ mÃ£ hÃ³a
yÃªu cáº§u, má»™t bá»™ mÃ£ hÃ³a lá»±a chá»n, vÃ  má»™t danh sÃ¡ch cÃ¡c
layer gater. Bá»™ mÃ£ hÃ³a yÃªu cáº§u chuyá»ƒn Ä‘á»•i mÃ´ táº£ nhiá»‡m vá»¥
vÃ  yÃªu cáº§u Ä‘á»™ thÆ°a ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡ thÃ nh má»™t embedding.

6

--- TRANG 7 ---
MÃ´ hÃ¬nh Sinh cho cÃ¡c MÃ´ hÃ¬nh: TÃ¹y chá»‰nh DNN nhanh chÃ³ng cho cÃ¡c Nhiá»‡m vá»¥ Äa dáº¡ng vÃ  RÃ ng buá»™c TÃ i nguyÃªn Preprint, , 2023

HÃ¬nh 5: Minh há»a vá» lá»›p FFN modular trong cÃ¡c backbone
Transformer. Äáº·c trÆ°ng trung gian â„ cá»§a FFN Ä‘Æ°á»£c kÃ­ch
hoáº¡t cÃ³ Ä‘iá»u kiá»‡n dá»c theo chiá»u áº©n. Má»—i pháº§n tá»­ trong
vector gate Ã¡nh xáº¡ Ä‘áº¿n má»™t pháº§n tá»­ trong â„. Theo viá»‡c
kÃ­ch hoáº¡t cá»§a â„, cÃ¡c chiá»u tÆ°Æ¡ng á»©ng trong ğ‘Šâ‚ vÃ  ğ‘Šâ‚‚
cÃ³ thá»ƒ Ä‘Æ°á»£c pruned má»™t cÃ¡ch chá»n lá»c.

HÃ¬nh 6: TÃ­ch há»£p gate cho cÃ¡c backbone khÃ¡c nhau. (a)
EfficientNet vÃ  MobileNet. (b) ResNet. (c) ViT.

ChÃºng tÃ´i sá»­ dá»¥ng mÃ£ hÃ³a one-hot cho thÃ nh pháº§n ğ‘¡ğ‘ğ‘ ğ‘˜,
Ä‘Æ°á»£c kÃ½ hiá»‡u bá»Ÿi ğ‘’ğ‘›ğ‘_{ğ‘¡ğ‘ğ‘ ğ‘˜}. Má»—i bit trong ğ‘’ğ‘›ğ‘_{ğ‘¡ğ‘ğ‘ ğ‘˜} Ä‘áº¡i
diá»‡n cho sá»± hiá»‡n diá»‡n cá»§a má»™t thuá»™c tÃ­nh cá»¥ thá»ƒ. VÃ­ dá»¥, trong
nhiá»‡m vá»¥ 'phÃ¡t hiá»‡n chÃ³ Ä‘en', cÃ¡c pháº§n tá»­ cho 'Ä‘en' vÃ  'chÃ³'
Ä‘Æ°á»£c Ä‘áº·t thÃ nh má»™t vÃ  nhá»¯ng pháº§n tá»­ khÃ¡c Ä‘Æ°á»£c Ä‘áº·t thÃ nh
khÃ´ng trong mÃ£ hÃ³a nhiá»‡m vá»¥. Äá»‘i vá»›i thÃ nh pháº§n ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡,
chÃºng tÃ´i sá»­ dá»¥ng Positional Encoding [49] Ä‘á»ƒ mÃ£ hÃ³a tá»·
lá»‡ giá»›i háº¡n, Ä‘Æ°á»£c kÃ½ hiá»‡u bá»Ÿi ğ‘’ğ‘›ğ‘_{ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡}. Hai mÃ£ hÃ³a ğ‘’ğ‘›ğ‘_{ğ‘¡ğ‘ğ‘ ğ‘˜}
vÃ  ğ‘’ğ‘›ğ‘_{ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡} Ä‘Æ°á»£c ná»‘i vá»›i nhau nhÆ° mÃ£ hÃ³a yÃªu cáº§u ğ‘’ğ‘›ğ‘_{ğ‘Ÿğ‘’ğ‘}.

Bá»™ mÃ£ hÃ³a lá»±a chá»n chuyá»ƒn Ä‘á»•i mÃ£ hÃ³a nhiá»‡m vá»¥ ğ‘’ğ‘›ğ‘_{ğ‘Ÿğ‘’ğ‘}
thÃ nh má»™t biá»ƒu diá»…n trung gian ğ‘’ğ‘›ğ‘_{ğ‘ ğ‘’ğ‘™} chá»©a kiáº¿n thá»©c vá»
viá»‡c lá»±a chá»n gate toÃ n mÃ´ hÃ¬nh. ChÃºng tÃ´i sá»­ dá»¥ng má»™t lá»›p
káº¿t ná»‘i Ä‘áº§y Ä‘á»§ tiáº¿p theo bá»Ÿi má»™t batch normalization (BN)
vÃ  má»™t Ä‘Æ¡n vá»‹ ReLU Ä‘á»ƒ thá»±c hiá»‡n chuyá»ƒn Ä‘á»•i nÃ y. MÃ£ hÃ³a
lá»±a chá»n toÃ n cáº§u ğ‘’ğ‘›ğ‘_{ğ‘ ğ‘’ğ‘™} sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»—i layer
gater Ä‘á»ƒ tÃ­nh toÃ¡n viá»‡c kÃ­ch hoáº¡t cho má»—i lá»›p. Layer gater
cÅ©ng lÃ  má»™t lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ tiáº¿p theo bá»Ÿi BN vÃ  Ä‘Æ¡n vá»‹
ReLU. GiÃ¡ trá»‹ Ä‘áº§u ra cá»§a nÃ³ ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘™âˆˆâ„^{ğ·_ğ‘™} Ä‘áº¡i diá»‡n cho
trá»ng sá»‘ cá»§a cÃ¡c module trong lá»›p ğ‘™, trong Ä‘Ã³ ğ·_ğ‘™ lÃ  sá»‘
lÆ°á»£ng module. Viá»‡c lá»±a chá»n gate cho lá»›p ğ‘”ğ‘™ Ä‘Æ°á»£c thu Ä‘Æ°á»£c
báº±ng cÃ¡ch rá»i ráº¡c hÃ³a ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘™ vá»›i má»™t ngÆ°á»¡ng (tá»©c lÃ 
ğ‘”ğ‘™=ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘™>ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘). ChÃºng tÃ´i Ä‘áº·t ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘=0.5
theo máº·c Ä‘á»‹nh trong triá»ƒn khai cá»§a chÃºng tÃ´i.

HÃ¬nh 7: Kiáº¿n trÃºc cá»§a assembler module nháº­n biáº¿t yÃªu cáº§u,
Ä‘Æ°á»£c tÃ­ch há»£p vá»›i backbone modular.

3.4 Huáº¥n luyá»‡n CÃ¹ng nhau vÃ  Triá»ƒn khai RiÃªng biá»‡t

Huáº¥n luyá»‡n CÃ¹ng nhau Supernet-Assembler. Supernet
modular vÃ  assembler cÃ³ má»‘i quan há»‡ cháº·t cháº½ - assembler
táº¡o ra viá»‡c kÃ­ch hoáº¡t cÃ¡c module, vÃ  viá»‡c kÃ­ch hoáº¡t Ä‘áº¡i diá»‡n
cho cÃ¡c á»©ng viÃªn mÃ´ hÃ¬nh vá»›i supernet. Do Ä‘Ã³, chÃºng tÃ´i
huáº¥n luyá»‡n chÃºng cÃ¹ng nhau Ä‘á»ƒ lÃ m cho chÃºng hoáº¡t Ä‘á»™ng
nháº¥t quÃ¡n cÃ¹ng nhau.

Viá»‡c huáº¥n luyá»‡n Ä‘Æ°á»£c tiáº¿n hÃ nh vá»›i má»™t táº­p há»£p cÃ¡c nhiá»‡m
vá»¥ huáº¥n luyá»‡n vÃ  dá»¯ liá»‡u Ä‘Æ°á»£c gáº¯n nhÃ£n thuá»™c vá» má»—i nhiá»‡m
vá»¥. Má»—i máº«u cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t bá»™ {ğ‘¡ğ‘ğ‘ ğ‘¤,
ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡, ğ‘¥, Å·}, trong Ä‘Ã³ ğ‘¥ vÃ  Å· lÃ  Ä‘áº§u vÃ o vÃ 
nhÃ£n. Táº¥t cáº£ cÃ¡c máº«u Ä‘Æ°á»£c trá»™n láº«n vá»›i nhau vÃ  xÃ¡o trá»™n
trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

Trong má»—i láº§n forward pass, chÃºng tÃ´i tÃ­nh toÃ¡n viá»‡c lá»±a
chá»n gate ğ‘” báº±ng cÃ¡ch Ä‘Æ°a ğ‘¡ğ‘ğ‘ ğ‘˜ vÃ  ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡ vÃ o
assembler, thu Ä‘Æ°á»£c subnet ğ‘€_ğ‘” vá»›i viá»‡c lá»±a chá»n gate ğ‘”,
vÃ  nháº­n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n ğ‘¦ báº±ng cÃ¡ch Ä‘Æ°a ğ‘¥ vÃ o subnet ğ‘€_ğ‘”.
Loss Ä‘Æ°á»£c tÃ­nh toÃ¡n báº±ng cÃ¡ch kiá»ƒm tra dá»± Ä‘oÃ¡n ğ‘¦ vÃ  viá»‡c
lá»±a chá»n gate ğ‘”, tá»©c lÃ 

ğ¿=ğ‘‡ğ¿(ğ‘¦,Å·)+ğœ†ğºğ¿(ğ‘”,ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡) (6)

ğ‘‡ğ¿(ğ‘¦,Å·) lÃ  loss cho nhiá»‡m vá»¥ huáº¥n luyá»‡n, khuyáº¿n khÃ­ch
subnet Ä‘Æ°á»£c táº¡o ra táº¡o ra cÃ¡c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c dá»±a trÃªn
nhiá»‡m vá»¥ cÆ¡ báº£n. ğºğ¿(Â·) lÃ  loss cho viá»‡c lá»±a chá»n gate,
khuyáº¿n khÃ­ch assembler táº¡o ra cÃ¡c subnet thá»a mÃ£n yÃªu cáº§u
Ä‘á»™ thÆ°a. Náº¿u tá»· lá»‡ cÃ¡c sá»‘ má»™t trong ğ‘” tháº¥p hÆ¡n ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡
cho trÆ°á»›c, ğºğ¿(Â·) bá»‹ vÃ´ hiá»‡u hÃ³a. NgÆ°á»£c láº¡i, ğºğ¿(Â·) lÃ  lá»—i
bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh (MSE) giá»¯a tá»· lá»‡ cÃ¡c sá»‘ má»™t trong
ğ‘” vÃ  ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡. ğœ† lÃ  má»™t siÃªu tham sá»‘ Ä‘á»ƒ cÃ¢n
báº±ng hai má»¥c tiÃªu, Ä‘Æ°á»£c Ä‘áº·t thÃ nh 100 theo máº·c Ä‘á»‹nh.

LÆ°u Ã½ ráº±ng loss lá»±a chá»n gate ğºğ¿(Â·) Ä‘Æ°á»£c tÃ­nh toÃ¡n vá»›i
viá»‡c lá»±a chá»n gate nhá»‹ phÃ¢n rá»i ráº¡c ğ‘”, nhÆ°ng Ä‘áº§u ra thá»±c
táº¿ cá»§a assembler lÃ  trá»ng sá»‘ gate liÃªn tá»¥c ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡, Ä‘iá»u nÃ y
cÃ³ thá»ƒ dáº«n Ä‘áº¿n khÃ³ khÄƒn trong viá»‡c truyá»n lá»—i ngÆ°á»£c. Äá»ƒ
giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p
gá»i lÃ  Improved SemHash [26,27] Ä‘á»ƒ thá»±c hiá»‡n thá»§ thuáº­t.
Viá»‡c tÃ­nh toÃ¡n Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° sau:

ğ‘”_ğ›¼=1(ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡>0)
ğ‘”_ğ›½=max(0,min(1,1.2ğœ(ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡)âˆ’0.1))

á» Ä‘Ã¢y, ğ‘”_ğ›¼ lÃ  má»™t vector nhá»‹ phÃ¢n, trong khi ğ‘”_ğ›½ lÃ  má»™t
vector gate cÃ³ giÃ¡ trá»‹ thá»±c vá»›i táº¥t cáº£ cÃ¡c má»¥c náº±m trong
khoáº£ng [0.0, 1.0]. ğ‘”_ğ›¼ cÃ³ thuá»™c tÃ­nh nhá»‹ phÃ¢n mong muá»‘n
mÃ  chÃºng tÃ´i muá»‘n sá»­ dá»¥ng trong huáº¥n luyá»‡n, nhÆ°ng gradient
cá»§a ğ‘”_ğ›¼ báº±ng khÃ´ng cho háº§u háº¿t cÃ¡c giÃ¡ trá»‹ cá»§a ğ‘”. NgÆ°á»£c
láº¡i, gradient cá»§a ğ‘”_ğ›½ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a rÃµ rÃ ng, nhÆ°ng ğ‘”_ğ›½
khÃ´ng pháº£i lÃ  má»™t vector nhá»‹ phÃ¢n. Trong láº§n forward pass
trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i ngáº«u nhiÃªn sá»­ dá»¥ng
ğ‘”=ğ‘”_ğ›¼ cho má»™t ná»­a sá»‘ máº«u vÃ  sá»­ dá»¥ng ğ‘”=ğ‘”_ğ›½ cho pháº§n
cÃ²n láº¡i. Khi ğ‘”_ğ›¼ Ä‘Æ°á»£c sá»­ dá»¥ng, chÃºng tÃ´i tuÃ¢n theo giáº£i
phÃ¡p trong [26,27] vÃ  Ä‘á»‹nh nghÄ©a gradient cá»§a ğ‘”_ğ›¼ giá»‘ng
nhÆ° gradient cá»§a ğ‘”_ğ›½ trong quÃ¡ trÃ¬nh truyá»n ngÆ°á»£c. Äá»‘i
vá»›i Ä‘Ã¡nh giÃ¡ vÃ  suy luáº­n, chÃºng tÃ´i luÃ´n sá»­ dá»¥ng cÃ¡c gate
rá»i ráº¡c.

Triá»ƒn khai RiÃªng biá»‡t cá»§a Subnet. Sau khi huáº¥n luyá»‡n trÆ°á»›c
cÃ¹ng nhau, chÃºng tÃ´i cÃ³ thá»ƒ táº¡o ra viá»‡c lá»±a chá»n gate dá»±a
trÃªn cÃ¡c yÃªu cáº§u nhiá»‡m vá»¥ vÃ  Ä‘á»™ thÆ°a khÃ¡c nhau. Khi viá»‡c
lá»±a chá»n gate Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh, máº¡ng assembler vÃ  cÃ¡c module
bá»‹ vÃ´ hiá»‡u hÃ³a trong supernet modular khÃ´ng cÃ²n há»¯u Ã­ch
ná»¯a. Do Ä‘Ã³, khi triá»ƒn khai mÃ´ hÃ¬nh, chÃºng tÃ´i chá»‰ cáº§n truyá»n
vÃ  triá»ƒn khai subnet Ä‘Æ°á»£c láº¯p rÃ¡p vá»›i cÃ¡c module hoáº¡t Ä‘á»™ng.
VÃ­ dá»¥, vá»›i supernet dá»±a trÃªn CNN, chÃºng tÃ´i cÃ³ thá»ƒ pruning
cÃ¡c bá»™ lá»c khÃ´ng hoáº¡t Ä‘á»™ng dá»±a trÃªn viá»‡c lá»±a chá»n gate.
Báº±ng cÃ¡ch lÃ m nhÆ° váº­y, subnet káº¿t quáº£ sáº½ cÃ³ kÃ­ch thÆ°á»›c mÃ´
hÃ¬nh, sá»­ dá»¥ng bá»™ nhá»›, vÃ  Ä‘á»™ trá»… giáº£m Ä‘Ã¡ng ká»ƒ. Äá»‘i vá»›i supernet
dá»±a trÃªn Transformer, chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£
tÆ°Æ¡ng tá»± báº±ng cÃ¡ch pruning cÃ¡c tham sá»‘ tÆ°Æ¡ng á»©ng trong
ğ‘Šâ‚ vÃ  ğ‘Šâ‚‚ dá»±a trÃªn viá»‡c lá»±a chá»n gate. MÃ´ hÃ¬nh Ä‘Æ°á»£c triá»ƒn
khai khÃ´ng yÃªu cáº§u báº¥t ká»³ thay Ä‘á»•i kiáº¿n trÃºc hay huáº¥n luyá»‡n
tham sá»‘ nÃ o thÃªm.

7

--- TRANG 8 ---
Preprint, , 2023 Xu vÃ  Li et al.

3.5 TÃ¬m kiáº¿m Kiáº¿n trÃºc Nháº¹

CÃ¡c module vÃ  assembler Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ¹ng nhau cho
phÃ©p chÃºng tÃ´i hiá»‡u quáº£ táº¡o ra cÃ¡c á»©ng viÃªn mÃ´ hÃ¬nh vá»›i
cÃ¡c nhiá»‡m vá»¥ vÃ  má»©c Ä‘á»™ thÆ°a khÃ¡c nhau. Dá»±a trÃªn kháº£ nÄƒng
nhÆ° váº­y, chÃºng tÃ´i tiáº¿p tá»¥c giá»›i thiá»‡u má»™t chiáº¿n lÆ°á»£c tÃ¬m
kiáº¿m kiáº¿n trÃºc nháº¹ Ä‘á»ƒ tÃ¬m mÃ´ hÃ¬nh tá»‘i Æ°u cho má»—i tÃ¬nh huá»‘ng
biÃªn.

QuÃ¡ trÃ¬nh tÃ¬m kiáº¿m kiáº¿n trÃºc Ä‘Æ°á»£c hÆ°á»›ng dáº«n bá»Ÿi má»™t bá»™
Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»¥ thá»ƒ theo thiáº¿t bá»‹, nháº­n má»™t á»©ng viÃªn
mÃ´ hÃ¬nh lÃ m Ä‘áº§u vÃ o vÃ  cho biáº¿t liá»‡u mÃ´ hÃ¬nh cÃ³ thá»a mÃ£n
cÃ¡c rÃ ng buá»™c tÃ i nguyÃªn hay khÃ´ng (tá»©c lÃ  Ä‘á»™ trá»… suy luáº­n
nhá» hÆ¡n ngÃ¢n sÃ¡ch Ä‘á»™ trá»… vÃ  chi phÃ­ bá»™ nhá»› nhá» hÆ¡n ngÃ¢n
sÃ¡ch bá»™ nhá»›). ChÃºng tÃ´i cÃ³ thá»ƒ trá»±c tiáº¿p Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t
trÃªn cÃ¡c thiáº¿t bá»‹ biÃªn má»¥c tiÃªu, tuÃ¢n theo thá»±c hÃ nh lá»±a chá»n
subnet trÃªn thiáº¿t bá»‹ cá»§a AdaptiveNet [53].

Thay vÃ o Ä‘Ã³, khi viá»‡c triá»ƒn khai supernet vÃ  assembler Ä‘áº¿n
thiáº¿t bá»‹ biÃªn khÃ´ng dá»… dÃ ng, chÃºng tÃ´i cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t
bá»™ dá»± Ä‘oÃ¡n hiá»‡u suáº¥t Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c á»©ng viÃªn mÃ´ hÃ¬nh vá»›i
dá»¯ liá»‡u profiling Ä‘Æ°á»£c thu tháº­p tá»« cÃ¡c thiáº¿t bá»‹ biÃªn. Má»™t
mÃ´ hÃ¬nh hiá»‡u suáº¥t thiáº¿t bá»‹ dá»±a trÃªn profiling nhÆ° váº­y lÃ  má»™t
thá»±c hÃ nh phá»• biáº¿n trong Ä‘iá»‡n toÃ¡n di Ä‘á»™ng/biÃªn [17], trong
khi á»Ÿ Ä‘Ã¢y chÃºng tÃ´i thá»±c hiá»‡n má»™t sá»‘ Ä‘Æ¡n giáº£n hÃ³a há»£p lÃ½
dá»±a trÃªn thiáº¿t káº¿ modular cá»§a chÃºng tÃ´i. Cá»¥ thá»ƒ, chÃºng tÃ´i
thá»±c hiá»‡n má»™t phÆ°Æ¡ng phÃ¡p profiling vÃ  mÃ´ hÃ¬nh hÃ³a hiá»‡u
suáº¥t theo lá»›p, trong Ä‘Ã³ tá»•ng Ä‘á»™ trá»… cá»§a mÃ´ hÃ¬nh báº±ng tá»•ng
cá»§a táº¥t cáº£ cÃ¡c lá»›p (vá»›i má»™t bias tÄ©nh), vÃ  Ä‘á»™ trá»… cá»§a má»—i
lá»›p phá»¥ thuá»™c vÃ o vector gate Ä‘Æ°á»£c dá»± Ä‘oÃ¡n bá»Ÿi assembler
module. Viá»‡c mÃ´ hÃ¬nh hÃ³a tiÃªu thá»¥ bá»™ nhá»› tÆ°Æ¡ng tá»±, Ä‘Æ°á»£c
xÃ¡c Ä‘á»‹nh bá»Ÿi tiÃªu thá»¥ bá»™ nhá»› tÄ©nh (cÃ¡c tham sá»‘ mÃ´ hÃ¬nh)
vÃ  bá»™ nhá»› Ä‘á»‰nh táº¡i runtime (Ä‘áº·c trÆ°ng trung gian lá»›n nháº¥t).
Äá»ƒ xÃ¢y dá»±ng cÃ¡c bá»™ dá»± Ä‘oÃ¡n nÃ y, chÃºng tÃ´i táº¡o ra má»™t táº­p
há»£p (2000 trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i) cÃ¡c vector
gate ngáº«u nhiÃªn, thu Ä‘Æ°á»£c cÃ¡c subnet tÆ°Æ¡ng á»©ng, Ä‘o cÃ¡c
chá»‰ sá»‘ hiá»‡u suáº¥t cá»§a cÃ¡c subnet nÃ y trÃªn cÃ¡c thiáº¿t bá»‹ biÃªn
má»¥c tiÃªu, vÃ  sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p Ä‘á»ƒ huáº¥n luyá»‡n
cÃ¡c bá»™ dá»± Ä‘oÃ¡n hiá»‡u suáº¥t vá»›i há»“i quy tuyáº¿n tÃ­nh. Bá»™ dá»± Ä‘oÃ¡n
cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao, vá»›i cáº£ Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n Ä‘á»™ trá»… vÃ 
Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n bá»™ nhá»› cao hÆ¡n 96% trÃªn bá»‘n thiáº¿t bá»‹
biÃªn Ä‘iá»ƒn hÃ¬nh, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 8. ChÃºng tÃ´i
sá»­ dá»¥ng bá»™ dá»± Ä‘oÃ¡n theo máº·c Ä‘á»‹nh trong NN-Factory.

(a) Äá»™ trá»…
(b) Bá»™ nhá»›

HÃ¬nh 8: Äá»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh
Ä‘Æ°á»£c táº¡o ra bá»Ÿi NN-Factory trÃªn bá»‘n thiáº¿t bá»‹ biÃªn.

Thuáº­t toÃ¡n 1 TÃ¬m kiáº¿m Kiáº¿n trÃºc Nháº¹
Äáº§u vÃ o: TÃ¬nh huá»‘ng biÃªn má»¥c tiÃªu ğ‘’, mÃ´ táº£ nhiá»‡m vá»¥ ğ‘¡ğ‘ğ‘ ğ‘˜_ğ‘’,
yÃªu cáº§u Ä‘á»™ trá»… ğ¿ğ´ğ‘‡_ğ‘’, yÃªu cáº§u bá»™ nhá»› ğ‘€ğ¸ğ‘€_ğ‘’, bá»™ Ä‘Ã¡nh giÃ¡ hiá»‡u
suáº¥t cá»¥ thá»ƒ theo biÃªn ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘œğ‘Ÿ_ğ‘’.
Äáº§u ra: MÃ´ hÃ¬nh biÃªn tÃ¹y chá»‰nh ğ‘€
1: ğ‘”ğ‘ğ‘¡ğ‘’_{ğ‘œğ‘ğ‘¡}â†ğ‘ğ‘ˆğ¿ğ¿
2: for ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡_ğ‘–= 1%; ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡_ğ‘–< 1; ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡_ğ‘–+= step do
3: ğ‘’ğ‘›ğ‘_ğ‘–â†ğ‘…ğ‘’ğ‘ğ‘¢ğ‘–ğ‘Ÿğ‘’ğ‘šğ‘’ğ‘›ğ‘¡_ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”(ğ‘¡ğ‘ğ‘ ğ‘˜_ğ‘’,ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡_ğ‘–)
4: ğ‘”ğ‘ğ‘¡ğ‘’_ğ‘–â†ğ´ğ‘ ğ‘ ğ‘’ğ‘šğ‘ğ‘™ğ‘’ğ‘Ÿ(ğ‘’ğ‘›ğ‘_ğ‘–)
5: ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘,ğ‘šğ‘’ğ‘šğ‘’ğ‘›ğ‘â†ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘œğ‘Ÿ_ğ‘’(ğ‘”ğ‘ğ‘¡ğ‘’_ğ‘–)
6: if ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘>ğ¿ğ´ğ‘‡_ğ‘’ or ğ‘šğ‘’ğ‘šğ‘’ğ‘›ğ‘>ğ‘€ğ¸ğ‘€_ğ‘’ then
7: break
8: end if
9: ğ‘”ğ‘ğ‘¡ğ‘’_{ğ‘œğ‘ğ‘¡}â†ğ‘”ğ‘ğ‘¡ğ‘’_ğ‘–
10: end for
11: return subnet Ä‘Æ°á»£c cáº¯t tá»« supernet vá»›i ğ‘”ğ‘ğ‘¡ğ‘’_{ğ‘œğ‘ğ‘¡}

Dá»±a trÃªn bá»™ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t, chÃºng tÃ´i cÃ³ thá»ƒ phÃ¢n tÃ­ch
Ä‘á»™ trá»… vÃ  bá»™ nhá»› cá»§a cÃ¡c á»©ng viÃªn mÃ´ hÃ¬nh. VÃ¬ cÃ ng nhiá»u
module Ä‘Æ°á»£c kÃ­ch hoáº¡t thÆ°á»ng dáº«n Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cao
hÆ¡n (xem Pháº§n 5.2), mÃ´ hÃ¬nh tá»‘i Æ°u cho má»™t tÃ¬nh huá»‘ng
biÃªn lÃ  mÃ´ hÃ¬nh cÃ³ tá»· lá»‡ kÃ­ch hoáº¡t module cao nháº¥t trong
khi thá»a mÃ£n cÃ¡c yÃªu cáº§u vá» bá»™ nhá»› vÃ  Ä‘á»™ trá»…. Thuáº­t toÃ¡n
1 cho tháº¥y chiáº¿n lÆ°á»£c tÃ¬m kiáº¿m kiáº¿n trÃºc nháº¹ cá»§a chÃºng tÃ´i.
ChÃºng tÃ´i báº¯t Ä‘áº§u tá»« giá»›i háº¡n kÃ­ch hoáº¡t module tháº¥p nháº¥t
ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡_ğ‘–=1%, vÃ  láº·p Ä‘i láº·p láº¡i tÄƒng giá»›i háº¡n. Äá»‘i vá»›i má»—i
giá»›i háº¡n kÃ­ch hoáº¡t, chÃºng tÃ´i táº¡o ra viá»‡c lá»±a chá»n gate báº±ng
máº¡ng assembler, vÃ  thu Ä‘Æ°á»£c Ä‘á»™ trá»… vÃ  bá»™ nhá»› liÃªn quan Ä‘áº¿n
viá»‡c lá»±a chá»n gate. Viá»‡c lá»±a chá»n gate á»©ng viÃªn cuá»‘i cÃ¹ng
Ä‘Ã¡p á»©ng cÃ¡c rÃ ng buá»™c mÃ´i trÆ°á»ng biÃªn Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh vÃ 
tráº£ vá». Cuá»‘i cÃ¹ng, chÃºng tÃ´i táº¡o ra subnet vá»›i viá»‡c lá»±a chá»n
gate káº¿t quáº£, cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai trá»±c tiáº¿p Ä‘áº¿n biÃªn mÃ 
khÃ´ng cáº§n báº¥t ká»³ xá»­ lÃ½ nÃ o thÃªm.

4 TRIá»‚N KHAI

ChÃºng tÃ´i triá»ƒn khai phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh báº±ng Python.
Pháº§n huáº¥n luyá»‡n dá»±a trÃªn PyTorch. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o
ra vá»›i PyTorch vÃ  Ä‘Æ°á»£c triá»ƒn khai Ä‘áº¿n cÃ¡c thiáº¿t bá»‹ biÃªn sá»­
dá»¥ng framework TensorFlow Lite cho di Ä‘á»™ng vÃ  PyTorch
cho Desktop vÃ  Jetson.

Chi tiáº¿t Kiáº¿n trÃºc vÃ  Huáº¥n luyá»‡n. Trong cÃ¡c layer gater
cá»§a assembler, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c lá»›p batch normalization
riÃªng biá»‡t cho mÃ£ hÃ³a lá»±a chá»n cá»§a cÃ¡c lá»›p khÃ¡c nhau, Ä‘iá»u
nÃ y cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n cháº¥t lÆ°á»£ng cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o
ra. Äá»ƒ tÄƒng cÆ°á»ng tÃ­nh á»•n Ä‘á»‹nh huáº¥n luyá»‡n vÃ  cáº£i thiá»‡n cháº¥t
lÆ°á»£ng cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra, chÃºng tÃ´i bá»• sung táº­p nhiá»‡m
vá»¥ huáº¥n luyá»‡n vá»›i cÃ¡c nhiá»‡m vá»¥ bá»• sung Ä‘á»ƒ káº¿t há»£p thÃªm
cÃ¡c káº¿t há»£p cá»§a cÃ¡c thuá»™c tÃ­nh nhiá»‡m vá»¥, cÃ³ thá»ƒ tÄƒng cÆ°á»ng
hiá»ƒu biáº¿t cá»§a mÃ´ hÃ¬nh vá» mÃ£ hÃ³a nhiá»‡m vá»¥ cá»§a chÃºng tÃ´i.
Äá»ƒ káº¿t há»£p cÃ¡c yÃªu cáº§u Ä‘á»™ thÆ°a rá»™ng hÆ¡n trong quÃ¡ trÃ¬nh
huáº¥n luyá»‡n mÃ  khÃ´ng lÃ m giáº£m cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh, chÃºng
tÃ´i sá»­ dá»¥ng nhiá»u layer gater Ä‘á»ƒ tÄƒng cÆ°á»ng kháº£ nÄƒng cá»§a
mÃ´ hÃ¬nh, dá»±a trÃªn cÃ¡c nguyÃªn táº¯c cá»§a Mixture of Experts
(MoE), má»—i gater chuyá»ƒn Ä‘á»•i mÃ£ hÃ³a lá»±a chá»n thÃ nh viá»‡c
lá»±a chá»n gate. NgoÃ i ra, má»™t máº¡ng gating Ä‘Æ°á»£c giá»›i thiá»‡u
Ä‘á»ƒ xÃ¡c Ä‘á»‹nh trá»ng sá»‘ cá»§a cÃ¡c Ä‘áº§u ra tá»« má»—i gater táº¡i má»—i lá»›p.

8

--- TRANG 9 ---
MÃ´ hÃ¬nh Sinh cho cÃ¡c MÃ´ hÃ¬nh: TÃ¹y chá»‰nh DNN nhanh chÃ³ng cho cÃ¡c Nhiá»‡m vá»¥ Äa dáº¡ng vÃ  RÃ ng buá»™c TÃ i nguyÃªn Preprint, , 2023

5 ÄÃNH GIÃ

ChÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i sau:
(1) NN-Factory cÃ³ kháº£ nÄƒng táº¡o ra cÃ¡c mÃ´ hÃ¬nh cá»¥ thá»ƒ theo
biÃªn khÃ´ng? Cháº¥t lÆ°á»£ng cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra nhÆ°
tháº¿ nÃ o? (2) Chi phÃ­ cá»§a NN-Factory lÃ  bao nhiÃªu? (3) Kháº£
nÄƒng sinh mÃ´ hÃ¬nh cá»§a NN-Factory tá»•ng quÃ¡t hÃ³a tá»‘t Ä‘áº¿n
má»©c nÃ o Ä‘á»‘i vá»›i cÃ¡c tÃ¬nh huá»‘ng biÃªn chÆ°a tháº¥y?

5.1 Thiáº¿t láº­p ThÃ­ nghiá»‡m

Nhiá»‡m vá»¥ vÃ  Táº­p dá»¯ liá»‡u. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t
cá»§a NN-Factory trÃªn hai cÃ i Ä‘áº·t tÃ¹y chá»‰nh mÃ´ hÃ¬nh.

â€¢ Tráº£ lá»i CÃ¢u há»i Thá»‹ giÃ¡c Sá»‘ há»c (NumVQA). ÄÃ¢y lÃ  má»™t
cÃ i Ä‘áº·t Ä‘Æ¡n giáº£n Ä‘á»ƒ phÃ¢n tÃ­ch hiá»‡u suáº¥t cá»§a viá»‡c sinh mÃ´
hÃ¬nh cá»¥ thá»ƒ theo nhiá»‡m vá»¥ vÃ  tÃ i nguyÃªn. Nhiá»‡m vá»¥ lÃ 
tráº£ lá»i má»™t cÃ¢u há»i cÃ³-hoáº·c-khÃ´ng (vÃ­ dá»¥: "CÃ³ hai sá»‘
cháºµn khÃ´ng?") dá»±a trÃªn má»™t hÃ¬nh áº£nh Ä‘áº§u vÃ o chá»©a bá»‘n
chá»¯ sá»‘. ChÃºng tÃ´i cÃ´ng thá»©c hÃ³a khoáº£ng 60 cÃ¢u há»i vÃ 
tá»•ng há»£p cÃ¡c hÃ¬nh áº£nh sá»­ dá»¥ng táº­p dá»¯ liá»‡u MNIST [33].
Hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra Ä‘Æ°á»£c Ä‘o báº±ng
Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i.

â€¢ PhÃ¡t hiá»‡n Äá»‘i tÆ°á»£ng cÃ³ Thuá»™c tÃ­nh (AttrOD). ÄÃ¢y lÃ 
má»™t cÃ i Ä‘áº·t thá»±c táº¿ hÆ¡n, trong Ä‘Ã³ má»—i nhiá»‡m vá»¥ lÃ  phÃ¡t
hiá»‡n cÃ¡c Ä‘á»‘i tÆ°á»£ng cÃ³ thuá»™c tÃ­nh cá»¥ thá»ƒ trong má»™t hÃ¬nh
áº£nh vÃ  dá»± Ä‘oÃ¡n cÃ¡c há»™p giá»›i háº¡n Ä‘á»‘i tÆ°á»£ng vÃ  danh má»¥c.
ChÃºng tÃ´i sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh tráº£ lá»i cÃ¢u há»i thá»‹ giÃ¡c
[29] Ä‘á»ƒ chÃº thÃ­ch mÃ u sáº¯c Ä‘á»‘i tÆ°á»£ng trong táº­p dá»¯ liá»‡u
COCO2017 [35] vÃ  há»£p nháº¥t cÃ¡c mÃ u Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh
Ä‘á»ƒ táº¡o thÃ nh cÃ¡c thuá»™c tÃ­nh má»¥c tiÃªu. ChÃºng tÃ´i chá»n
5 thuá»™c tÃ­nh (1-tráº¯ng, 2-Ä‘á»“ng, 3-than, 4-Ä‘á» tÆ°Æ¡i, 5-xanh
lÃ¡) tá»« 4 danh má»¥c vÃ  káº¿t há»£p chÃºng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c
nhiá»‡m vá»¥ huáº¥n luyá»‡n cá»§a chÃºng tÃ´i. Tá»•ng sá»‘ nhiá»‡m vá»¥
khoáº£ng 140, vÃ  má»—i nhiá»‡m vá»¥ cÃ³ sá»‘ lÆ°á»£ng máº«u khÃ¡c
nhau, tá»« hÃ ng trÄƒm Ä‘áº¿n hÃ ng chá»¥c nghÃ¬n. Hiá»‡u suáº¥t cá»§a
cÃ¡c mÃ´ hÃ¬nh phÃ¡t hiá»‡n Ä‘Æ°á»£c Ä‘o báº±ng Ä‘á»™ chÃ­nh xÃ¡c trung
bÃ¬nh (mAP) trÃªn ngÆ°á»¡ng Intersection over Union 0.5
(mAP@0.5).

Backbone MÃ´ hÃ¬nh. ChÃºng tÃ´i xem xÃ©t cÃ¡c backbone CNN
vÃ  Transformer phá»• biáº¿n trong thÃ­ nghiá»‡m nÃ y, bao gá»“m
ResNet [20], ViT [11], cho cÃ i Ä‘áº·t NumVQA vÃ  EfficientDet
[48] cho cÃ i Ä‘áº·t AttrOD.

Baseline. ChÃºng tÃ´i so sÃ¡nh NN-Factory vá»›i hai phÆ°Æ¡ng
phÃ¡p tÃ¹y chá»‰nh mÃ´ hÃ¬nh thÃ´ng thÆ°á»ng:

(1)Retrain - ChÃºng tÃ´i cá»‘ Ä‘á»‹nh kiáº¿n trÃºc mÃ´ hÃ¬nh vÃ  huáº¥n
luyá»‡n láº¡i nÃ³ vá»›i phÆ°Æ¡ng phÃ¡p há»c cÃ³ giÃ¡m sÃ¡t tiÃªu chuáº©n
cho má»—i tÃ¬nh huá»‘ng biÃªn.

(2)Prune&Tune - ChÃºng tÃ´i huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh thá»‘ng
nháº¥t. Vá»›i má»™t tÃ¬nh huá»‘ng biÃªn cho trÆ°á»›c, chÃºng tÃ´i pruning
vÃ  tinh chá»‰nh mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ phÃ¹
há»£p vá»›i cÃ¡c yÃªu cáº§u biÃªn vá»›i má»™t phÆ°Æ¡ng phÃ¡p pruning
SOTA [14].

Cáº£ hai Ä‘á»u yÃªu cáº§u huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u cá»¥ thá»ƒ theo biÃªn.
ChÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng backbone vá»›i cÃ¡c baseline nÃ y vÃ 
huáº¥n luyá»‡n chÃºng cho Ä‘áº¿n khi há»™i tá»¥. ChÃºng tÃ´i khÃ´ng bao
gá»“m cÃ¡c phÆ°Æ¡ng phÃ¡p sinh/má»Ÿ rá»™ng mÃ´ hÃ¬nh khÃ´ng cáº§n huáº¥n
luyá»‡n khÃ¡c [4,13,17,53] vÃ¬ chÃºng khÃ´ng há»— trá»£ tÃ¹y chá»‰nh
hÆ°á»›ng nhiá»‡m vá»¥.

MÃ´i trÆ°á»ng BiÃªn. ChÃºng tÃ´i xem xÃ©t ba thiáº¿t bá»‹ biÃªn bao
gá»“m má»™t Äiá»‡n thoáº¡i thÃ´ng minh Android (Xiaomi 12) vá»›i
bá»™ xá»­ lÃ½ SnapdragonÂ® 8 Gen 1 vÃ  bá»™ nhá»› 12GB, má»™t Jetson
AGX Xavier vá»›i bá»™ nhá»› 32 GB, vÃ  má»™t mÃ¡y tÃ­nh Ä‘á»ƒ bÃ n vá»›i
Bá»™ xá»­ lÃ½ 12th Gen IntelÂ® Coreâ„¢ i9-12900KÃ—24 vá»›i bá»™ nhá»›
64GB. KÃ­ch thÆ°á»›c batch Ä‘á»u Ä‘Æ°á»£c Ä‘áº·t thÃ nh 1 trÃªn ba thiáº¿t
bá»‹ Ä‘á»ƒ mÃ´ phá»ng khá»‘i lÆ°á»£ng cÃ´ng viá»‡c thá»±c táº¿. ChÃºng tÃ´i sá»­
dá»¥ng cÃ¡c ngÃ¢n sÃ¡ch Ä‘á»™ trá»… khÃ¡c nhau Ä‘á»ƒ mÃ´ phá»ng sá»± Ä‘a
dáº¡ng pháº§n cá»©ng trong thiáº¿t bá»‹.

5.2 Cháº¥t lÆ°á»£ng Sinh MÃ´ hÃ¬nh

ChÃºng tÃ´i tiáº¿n hÃ nh Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra bá»Ÿi
NN-Factory trÃªn NumVQA vÃ  AttrOD, tiáº¿p theo lÃ  phÃ¢n tÃ­ch
toÃ n diá»‡n vá» cháº¥t lÆ°á»£ng cá»§a chÃºng.

ChÃºng tÃ´i Ä‘áº§u tiÃªn Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a cÃ¡c module vÃ 
assembler trong cÃ i Ä‘áº·t NumVQA. ChÃºng tÃ´i Ä‘Æ°a cÃ¡c bá»™
< ğ‘¡ğ‘ğ‘ ğ‘˜, ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘™ğ‘–ğ‘šğ‘–ğ‘¡ > khÃ¡c nhau vÃ o assembler
NN-Factory vÃ  Ä‘á»ƒ nÃ³ táº¡o ra cÃ¡c mÃ´ hÃ¬nh Ä‘Ã¡p á»©ng cÃ¡c yÃªu
cáº§u. Káº¿t quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 2. NhÃ¬n chung,
NN-Factory thá»ƒ hiá»‡n Ä‘á»™ chÃ­nh xÃ¡c Ä‘áº·c biá»‡t (>99% vá»›i backbone
ResNet) qua cÃ¡c nhiá»‡m vá»¥ Ä‘a dáº¡ng, Ä‘á»“ng thá»i cÅ©ng Ä‘áº£m báº£o
ráº±ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra tuÃ¢n thá»§ giá»›i háº¡n kÃ­ch hoáº¡t
Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh cá»§a chÃºng tÃ´i. Do quy mÃ´ nhiá»‡m vá»¥ tÆ°Æ¡ng
Ä‘á»‘i nhá», chá»‰ cáº§n má»™t tá»· lá»‡ kÃ­ch hoáº¡t tháº¥p hÆ¡n.

Tiáº¿p theo, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t end-to-end cá»§a
NN-Factory trÃªn AttrOD. Trong Báº£ng 3, chÃºng tÃ´i sá»­ dá»¥ng
ResNet50 lÃ m backbone vÃ  trÃ¬nh bÃ y cháº¥t lÆ°á»£ng cá»§a cÃ¡c mÃ´
hÃ¬nh Ä‘Æ°á»£c táº¡o ra dÆ°á»›i cÃ¡c yÃªu cáº§u Ä‘á»™ trá»… vÃ  bá»™ nhá»› khÃ¡c
nhau trÃªn cÃ¡c thiáº¿t bá»‹ khÃ¡c nhau, vÃ  so sÃ¡nh chÃºng vá»›i cÃ¡c
mÃ´ hÃ¬nh baseline. NhÃ¬n chung, NN-Factory liÃªn tá»¥c cung
cáº¥p cÃ¡c mÃ´ hÃ¬nh cháº¥t lÆ°á»£ng cao Ä‘Ã¡p á»©ng cÃ¡c tiÃªu chÃ­ Ä‘Æ°á»£c
chá»‰ Ä‘á»‹nh trong táº¥t cáº£ cÃ¡c tÃ¬nh huá»‘ng biÃªn. Nhá» vÃ o viá»‡c tÃ¬m
kiáº¿m kiáº¿n trÃºc mÃ´ hÃ¬nh nháº­n biáº¿t hiá»‡u suáº¥t, NN-Factory
cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘áº§y Ä‘á»§ cÃ¡c ngÃ¢n sÃ¡ch cho trÆ°á»›c (bá»™ nhá»›
hoáº·c Ä‘á»™ trá»…). Baseline Retrain khÃ´ng Ä‘Ã¡p á»©ng Ä‘Æ°á»£c cÃ¡c yÃªu
cáº§u vá» Ä‘á»™ trá»… vÃ  bá»™ nhá»› vÃ¬ nÃ³ khÃ´ng Ä‘iá»u chá»‰nh kiáº¿n trÃºc
mÃ´ hÃ¬nh cho má»—i tÃ¬nh huá»‘ng.

Báº£ng 2: Cháº¥t lÆ°á»£ng cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra trÃªn NumVQA.

YÃªu cáº§u Sinh        ResNet          ViT
Nhiá»‡m vá»¥    Act. Limit  Acc  Act. Ratio  Acc  Act. Ratio
CÃ³ sá»‘ 0     3%          99.7% 3.0%       97.8% 3.0%
CÃ³ sá»‘ 0     5%          99.8% 3.7%       97.6% 4.0%
CÃ³ sá»‘ 0     10%         99.9% 3.8%       97.8% 5.1%
Chá»‰ cÃ³ hai sá»‘ 1  3%     99.3% 3.0%       96.7% 3.0%
Chá»‰ cÃ³ ba sá»‘ 2   3%     99.4% 3.0%       86.7% 3.0%
Chá»‰ cÃ³ bá»‘n sá»‘ 5  3%     99.1% 3.0%       95.5% 3.0%
Chá»‰ cÃ³ má»™t sá»‘ 0  5%     99.8% 3.7%       94.1% 5.0%
Chá»‰ cÃ³ ba sá»‘ 0   5%     99.4% 3.8%       94.8% 5.0%
Chá»‰ cÃ³ má»™t sá»‘ 3  5%     99.8% 3.7%       87.7% 5.0%
Chá»‰ cÃ³ hai sá»‘ láº»  3%    99.3% 3.0%       94.3% 3.0%
Chá»‰ cÃ³ hai sá»‘ láº»  5%    99.4% 4.0%       94.3% 3.8%

Äá»™ chÃ­nh xÃ¡c cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra bá»Ÿi NN-Factory
gáº§n vá»›i baseline Retrain vÃ  vÆ°á»£t trá»™i hÆ¡n baseline Prune&Tune,
máº·c dÃ¹ nÃ³ khÃ´ng yÃªu cáº§u huáº¥n luyá»‡n cá»¥ thá»ƒ theo biÃªn. Äá»“ng
thá»i, nÃ³ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm mAP cao hÆ¡n Ä‘Ã¡ng ká»ƒ trÃªn má»™t
sá»‘ nhiá»‡m vá»¥ (vÃ­ dá»¥: Xe mÃ¡y {1}, NgÆ°á»i {5}) chá»©a Ã­t máº«u
huáº¥n luyá»‡n hÆ¡n so vá»›i nhá»¯ng nhiá»‡m vá»¥ khÃ¡c. Äiá»u nÃ y lÃ 
do viá»‡c huáº¥n luyá»‡n káº¿t há»£p cÃ¡c nhiá»‡m vá»¥, má»—i nhiá»‡m vá»¥
vá»›i cÃ¡c káº¿t há»£p thuá»™c tÃ­nh khÃ¡c nhau, cho phÃ©p mÃ´ hÃ¬nh
Ä‘áº¡t Ä‘Æ°á»£c hiá»ƒu biáº¿t sÃ¢u sáº¯c hÆ¡n vá» cÃ¡c nhiá»‡m vá»¥ vÃ  do Ä‘Ã³
Ä‘Æ°a ra cÃ¡c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c hÆ¡n. Äá»™ chÃ­nh xÃ¡c cá»§a cÃ¡c
mÃ´ hÃ¬nh Prune&Tune tháº¥p hÆ¡n nhiá»u so vá»›i NN-Factory
vÃ  Retrain do kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh giáº£m.

ChÃºng tÃ´i cÅ©ng khÃ¡m phÃ¡ sá»± khÃ¡c biá»‡t trong cháº¥t lÆ°á»£ng
mÃ´ hÃ¬nh cá»§a NN-Factory vá»›i cÃ¡c backbone khÃ¡c nhau. Káº¿t
quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 9. NN-Factory thá»ƒ hiá»‡n hÃ nh
vi nháº¥t quÃ¡n qua cÃ¡c backbone supernet khÃ¡c nhau, biá»ƒu
thá»‹ tÃ­nh tá»•ng quÃ¡t hÃ³a cá»§a nÃ³. Tuy nhiÃªn, nÃ³ thá»ƒ hiá»‡n sá»±
khÃ¡c biá»‡t hiá»‡u suáº¥t rÃµ rá»‡t dá»±a trÃªn backbone Ä‘Æ°á»£c chá»n.
VÃ­ dá»¥, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra bá»Ÿi NN-Factory dá»±a trÃªn
ViT vÃ  NN-Factory dá»±a trÃªn MobileNet thá»ƒ hiá»‡n Ä‘á»™ chÃ­nh
xÃ¡c tháº¥p hÆ¡n. Äiá»u nÃ y lÃ  do cÃ¡c thuá»™c tÃ­nh vá»‘n cÃ³ cá»§a máº¡ng
backbone, vÃ­ dá»¥: hiá»‡u quáº£ máº«u kÃ©m cá»§a ViT vÃ  kháº£ nÄƒng
mÃ´ hÃ¬nh háº¡n cháº¿ cá»§a MobileNet.

HÆ¡n ná»¯a, chÃºng tÃ´i thá»±c hiá»‡n má»™t phÃ¢n tÃ­ch Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
cÃ¡ch NN-Factory quáº£n lÃ½ sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a cháº¥t lÆ°á»£ng mÃ´
hÃ¬nh vÃ  Ä‘á»™ trá»…. Káº¿t quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong HÃ¬nh 10, NN-
Factory cÃ³ kháº£ nÄƒng táº¡o ra cÃ¡c mÃ´ hÃ¬nh cháº¥t lÆ°á»£ng cao hÆ¡n
khi Ä‘Æ°á»£c cung cáº¥p cÃ¡c rÃ ng buá»™c Ä‘á»™ trá»… cao hÆ¡n. HÃ¬nh 11
minh há»a má»‘i tÆ°Æ¡ng quan giá»¯a giá»›i háº¡n kÃ­ch hoáº¡t Ä‘áº§u vÃ o
trong quÃ¡ trÃ¬nh sinh mÃ´ hÃ¬nh vÃ  tá»· lá»‡ kÃ­ch hoáº¡t cá»§a viá»‡c
lá»±a chá»n gate Ä‘Æ°á»£c táº¡o ra bá»Ÿi NN-Factory. Vá»›i giá»›i háº¡n
kÃ­ch hoáº¡t ráº¥t tháº¥p, tá»· lá»‡ kÃ­ch hoáº¡t Ä‘Æ°á»£c táº¡o ra khÃ´ng Ä‘Ã¡p
á»©ng Ä‘Æ°á»£c cÃ¡c yÃªu cáº§u. Do Ä‘Ã³,

9

--- TRANG 10 ---
Preprint, , 2023 Xu vÃ  Li et al.

Báº£ng 3: Cháº¥t lÆ°á»£ng cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra bá»Ÿi NN-Factory vÃ  baseline cho cÃ¡c tÃ¬nh huá»‘ng biÃªn khÃ¡c nhau trong cÃ i Ä‘áº·t AttrOD. Cá»™t 'nhiá»‡m vá»¥' lÃ  Ä‘á»‘i tÆ°á»£ng má»¥c tiÃªu theo sau bá»Ÿi cÃ¡c thuá»™c tÃ­nh mong muá»‘n (vÃ­ dá»¥: Ä‘en, vÃ ng). CÃ¡c cá»™t 'Lat' vÃ  'Mem' lÃ  Ä‘á»™ trá»… vÃ  bá»™ nhá»› tÆ°Æ¡ng Ä‘á»‘i so vá»›i cÃ¡c yÃªu cáº§u (ğ¿ğ´ğ‘‡_{ğ‘Ÿğ‘’ğ‘} vÃ  ğ‘€ğ¸ğ‘€_{ğ‘Ÿğ‘’ğ‘}).

TÃ¬nh huá»‘ng BiÃªn                  NN-Factory              Retrain             Prune&Tune
Nhiá»‡m vá»¥      Thiáº¿t bá»‹   ğ¿ğ´ğ‘‡_{ğ‘Ÿğ‘’ğ‘} ğ‘€ğ¸ğ‘€_{ğ‘Ÿğ‘’ğ‘} Î”Lat Î”Mem mAP  Î”Lat   Î”Mem mAP  Î”Lat   Î”Mem mAP
Xe Ä‘áº¡p {3,4}  Desktop    55ms     0.30GB    -0.11 -0.03 0.31 +156.77 +0.38 0.35 -0.33  -0.23 0.34
NgÆ°á»i {2}     Desktop    70ms     0.35GB    -4.55 -0.05 0.27 +141.77 +0.33 0.24 -1.05  -0.22 0.22
Xe {3}        Desktop    85ms     0.40GB    -2.96 -0.01 0.38 +126.77 +0.28 0.42 -1.77  -0.22 0.41
Xe mÃ¡y {1}    Desktop    100ms    0.45GB    -0.05 -0.02 0.45 +111.77 +0.23 0.23 -0.70  -0.21 0.24
Xe mÃ¡y {2,3,4} Desktop   115ms    0.50GB    -2.43 -0.04 0.43 +96.77  +0.18 0.44 -1.42  -0.20 0.45
Xe Ä‘áº¡p {3,4}  Mobile     350ms    5GB       -47.36 -0.24 0.31 +625.28 +19.97 0.35 -54.4  -0.21 0.34
Xe mÃ¡y {5}    Mobile     450ms    7GB       -47.54 -0.07 0.53 +525.28 +17.97 0.19 -86.43 -0.21 0.19
Xe {3}        Mobile     550ms    9GB       -25.92 -0.01 0.38 +425.28 +15.97 0.42 -119.46 -0.18 0.41
NgÆ°á»i {5}     Jetson     15ms     â€”         -0.61  â€”     0.22 +42.98  â€”     0.15 -0.40  â€”     0.07
NgÆ°á»i {5}     Jetson     20ms     â€”         -0.81  â€”     0.22 +37.98  â€”     0.15 -0.29  â€”     0.15
NgÆ°á»i {5}     Jetson     30ms     â€”         -0.06  â€”     0.23 +27.98  â€”     0.15 -0.08  â€”     0.16

HÃ¬nh 10: Sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a mAP (mean Average Precision)
vÃ  Ä‘á»™ trá»… cho NN-Factory qua cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau cá»§a AttrOD.

HÃ¬nh 11: Má»‘i quan há»‡ giá»¯a tá»· lá»‡ kÃ­ch hoáº¡t module thá»±c táº¿
vÃ  giá»›i háº¡n kÃ­ch hoáº¡t Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh.

nÃ³ tráº£i qua pruning dá»±a trÃªn táº§m quan trá»ng, dáº«n Ä‘áº¿n má»™t
máº«u gáº§n vá»›i Ä‘Æ°á»ng chÃ©o trÃªn Ä‘á»“ thá»‹. Vá»›i viá»‡c tÄƒng giá»›i háº¡n
kÃ­ch hoáº¡t, NN-Factory táº¡o ra

Báº£ng 4: Thá»i gian chuáº©n bá»‹ (má»™t láº§n cho táº¥t cáº£ tÃ¬nh huá»‘ng)
vÃ  thá»i gian tÃ¹y chá»‰nh (má»™t láº§n cho má»—i tÃ¬nh huá»‘ng biÃªn)
cá»§a NN-Factory vÃ  baseline.

PhÆ°Æ¡ng phÃ¡p   Chuáº©n bá»‹   TÃ¹y chá»‰nh
NN-Factory    130 giá»    3.6s
Retrain       â€“          66 giá»
Prune&Tune    66 giá»     4 giá»

cÃ¡c tá»· lá»‡ kÃ­ch hoáº¡t cÃ³ thá»ƒ thá»a mÃ£n cÃ¡c tiÃªu chÃ­. CÃ¡c tá»· lá»‡
dáº§n dáº§n cáº£i thiá»‡n vá»›i viá»‡c tÄƒng giá»›i háº¡n kÃ­ch hoáº¡t vÃ  cuá»‘i
cÃ¹ng Ä‘áº¡t Ä‘áº¿n tráº¡ng thÃ¡i á»•n Ä‘á»‹nh, vÃ¬ cÃ¡c tá»· lá»‡ á»•n Ä‘á»‹nh Ä‘Ã£ Ä‘á»§
Ä‘á»ƒ táº¡o ra cÃ¡c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c.

5.3 Hiá»‡u quáº£ Sinh MÃ´ hÃ¬nh

ChÃºng tÃ´i tiáº¿n hÃ nh phÃ¢n tÃ­ch so sÃ¡nh vá» chi phÃ­ chuáº©n bá»‹
vÃ  tÃ¹y chá»‰nh giá»¯a NN-Factory vÃ  cÃ¡c baseline, vÃ  káº¿t quáº£
Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 4.

Chi phÃ­ TÃ¹y chá»‰nh cho Má»—i TÃ¬nh huá»‘ng BiÃªn. Má»¥c tiÃªu
chÃ­nh cá»§a NN-Factory lÃ  giáº£m thá»i gian tÃ¹y chá»‰nh mÃ´ hÃ¬nh.
NN-Factory chá»‰ máº¥t 3.6 giÃ¢y trung bÃ¬nh Ä‘á»ƒ táº¡o ra má»™t mÃ´
hÃ¬nh tÃ¹y chá»‰nh cho má»™t tÃ¬nh huá»‘ng biÃªn, thá»ƒ hiá»‡n hiá»‡u quáº£
Ä‘Ã¡ng ká»ƒ. QuÃ¡ trÃ¬nh nÃ y bao gá»“m khoáº£ng 12 vÃ²ng tÃ¬m kiáº¿m,
má»—i vÃ²ng máº¥t 0.2 giÃ¢y, vÃ  trÃ­ch xuáº¥t mÃ´ hÃ¬nh, máº¥t 0.8 giÃ¢y.
NgÆ°á»£c láº¡i, cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ¹y chá»‰nh mÃ´ hÃ¬nh truyá»n thá»‘ng
gÃ¢y ra sá»± gia tÄƒng thá»i gian kinh hoÃ ng gáº¥p 4000 láº§n vÃ¬
chÃºng cáº§n huáº¥n luyá»‡n mÃ´ hÃ¬nh cho má»—i tÃ¬nh huá»‘ng biÃªn.

Chi phÃ­ Chuáº©n bá»‹ Má»™t láº§n. Chi phÃ­ chuáº©n bá»‹ cá»§a NN-Factory
chá»§ yáº¿u Ä‘áº¿n tá»« viá»‡c huáº¥n luyá»‡n supernet modular. Äáº§u tiÃªn,
chÃºng tÃ´i phÃ¢n tÃ­ch má»‘i quan há»‡ giá»¯a cháº¥t lÆ°á»£ng cá»§a mÃ´
hÃ¬nh Ä‘Æ°á»£c táº¡o ra vÃ  sá»‘ epoch huáº¥n luyá»‡n. NhÆ° Ä‘Æ°á»£c mÃ´ táº£
trong HÃ¬nh 12, NN-Factory yÃªu cáº§u nhiá»u epoch huáº¥n luyá»‡n
hÆ¡n Ä‘á»ƒ

11

--- TRANG 11 ---
MÃ´ hÃ¬nh Sinh cho cÃ¡c MÃ´ hÃ¬nh: TÃ¹y chá»‰nh DNN nhanh chÃ³ng cho cÃ¡c Nhiá»‡m vá»¥ Äa dáº¡ng vÃ  RÃ ng buá»™c TÃ i nguyÃªn Preprint, , 2023

HÃ¬nh 12: Äá»™ chÃ­nh xÃ¡c trung
bÃ¬nh cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c
táº¡o ra sau khi huáº¥n luyá»‡n vá»›i
sá»‘ #epoch khÃ¡c nhau.

HÃ¬nh 13: Äá»™ chÃ­nh xÃ¡c cá»§a
mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n hiá»‡u suáº¥t
báº±ng cÃ¡ch huáº¥n luyá»‡n vá»›i sá»‘
#subnet khÃ¡c nhau.

dáº§n dáº§n cáº£i thiá»‡n cháº¥t lÆ°á»£ng cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra.
Äiá»u nÃ y liÃªn quan Ä‘áº¿n phÆ°Æ¡ng phÃ¡p luáº­n huáº¥n luyá»‡n cá»§a
chÃºng tÃ´i. NN-Factory tráº£i qua huáº¥n luyá»‡n Ä‘á»“ng thá»i trÃªn
nhiá»u nhiá»‡m vá»¥, vá»›i má»—i nhiá»‡m vá»¥ chá»‰ cÃ³ quyá»n truy cáº­p
vÃ o má»™t pháº§n dá»¯ liá»‡u huáº¥n luyá»‡n trong má»—i epoch. Do Ä‘Ã³,
nÃ³ cáº§n nhiá»u epoch huáº¥n luyá»‡n hÆ¡n Ä‘á»ƒ táº¡o ra cÃ¡c mÃ´ hÃ¬nh
cháº¥t lÆ°á»£ng cao cho má»—i nhiá»‡m vá»¥ riÃªng láº». Tuy nhiÃªn, do
cÃ¡c phá»¥ thuá»™c liÃªn káº¿t Ä‘Ã¡ng ká»ƒ giá»¯a cÃ¡c nhiá»‡m vá»¥, viá»‡c huáº¥n
luyá»‡n Ä‘á»“ng thá»i cá»§a chÃºng cÃ³ hiá»‡u á»©ng hiá»‡p Ä‘á»“ng, Ä‘áº£m báº£o
ráº±ng chi phÃ­ huáº¥n luyá»‡n cá»§a NN-Factory khÃ´ng tÄƒng lÃªn
Ä‘Ã¡ng ká»ƒ.

Bá»™ dá»± Ä‘oÃ¡n hiá»‡u suáº¥t Ä‘Æ°á»£c sá»­ dá»¥ng cho tÃ¬m kiáº¿m kiáº¿n trÃºc
nháº¹ cÅ©ng Ä‘Æ°á»£c xÃ¢y dá»±ng trong quÃ¡ trÃ¬nh chuáº©n bá»‹. HÃ¬nh 13
cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh hiá»‡u suáº¥t Ä‘áº¡t Ä‘Æ°á»£c vá»›i sá»‘
lÆ°á»£ng subnet khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ profiling vÃ  mÃ´
hÃ¬nh hÃ³a (backbone supernet lÃ  ResNet). Khi sá»‘ lÆ°á»£ng subnet
tÄƒng, Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n cho tháº¥y xu hÆ°á»›ng tÄƒng vÃ  Ä‘áº¡t
Ä‘áº¿n má»™t Ä‘Æ°á»ng tiá»‡m cáº­n. ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng viá»‡c sá»­
dá»¥ng 200 âˆ¼ 1000 subnet Ä‘á»ƒ profiling vÃ  huáº¥n luyá»‡n lÃ  Ä‘á»§
Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n Ä‘á»™ trá»… vÃ  bá»™ nhá»› tá»‘t (cao
hÆ¡n 95%). Viá»‡c thu tháº­p dá»¯ liá»‡u profiling cho má»™t subnet
máº¥t 70ms (Jetson GPU) Ä‘áº¿n 985ms (Mobile CPU). Äá»“ng
thá»i, thá»i gian cáº§n thiáº¿t Ä‘á»ƒ fit mÃ´ hÃ¬nh hiá»‡u suáº¥t Ã­t hÆ¡n vÃ i
giÃ¢y. Do Ä‘Ã³, thá»i gian Ä‘á»ƒ thiáº¿t láº­p mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hiá»‡u
suáº¥t trong NN-Factory dao Ä‘á»™ng tá»« khoáº£ng 14s Ä‘áº¿n 985s,
Ä‘iá»u nÃ y cÃ³ thá»ƒ bá» qua nhÆ° má»™t quy trÃ¬nh offline má»™t láº§n.

Chi phÃ­ Phá»¥c vá»¥ TrÃªn thiáº¿t bá»‹. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o ra
bá»Ÿi NN-Factory lÃ  cÃ¡c mÃ´ hÃ¬nh tÄ©nh bÃ¬nh thÆ°á»ng, khÃ´ng
táº¡o ra báº¥t ká»³ chi phÃ­ bá»• sung nÃ o táº¡i runtime.

5.4 Tá»•ng quÃ¡t hÃ³a Ä‘áº¿n Nhiá»‡m vá»¥ ChÆ°a tháº¥y

Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a
cá»§a NN-Factory Ä‘áº¿n cÃ¡c nhiá»‡m vá»¥ chÆ°a tháº¥y. Nhiá»‡m vá»¥
chÆ°a tháº¥y Ä‘á» cáº­p Ä‘áº¿n cÃ¡c nhiá»‡m vá»¥ khÃ´ng pháº£i lÃ  má»™t pháº§n
cá»§a táº­p nhiá»‡m vá»¥ huáº¥n luyá»‡n, nhÆ°ng chÃºng chia sáº» cÃ¹ng
má»™t khÃ´ng gian thuá»™c tÃ­nh tá»• há»£p nhÆ° cÃ¡c nhiá»‡m vá»¥ huáº¥n
luyá»‡n.

HÃ¬nh 14 minh há»a Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c nhiá»‡m vá»¥ chÆ°a tháº¥y
Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn trong NumVQA vÃ  AttrOD. Máº·c dÃ¹
Ä‘á»™ chÃ­nh xÃ¡c cho cÃ¡c nhiá»‡m vá»¥ chÆ°a tháº¥y cÃ³ thá»ƒ thá»ƒ hiá»‡n
sá»± giáº£m nháº¹ so vá»›i

HÃ¬nh 14: Äá»™ chÃ­nh xÃ¡c hoáº·c mAP cho cÃ¡c nhiá»‡m vá»¥ Ä‘Ã£ biáº¿t
& chÆ°a tháº¥y trong (a) NumVQA. (b) AttrOD.

HÃ¬nh 15: Sá»± tÆ°Æ¡ng Ä‘á»“ng gate giá»¯a cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau
(a) vÃ  giá»¯a cÃ¡c giá»›i háº¡n kÃ­ch hoáº¡t khÃ¡c nhau (b) trong
NumVQA. Nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u mÃ u Ä‘á» lÃ  má»™t nhiá»‡m
vá»¥ chÆ°a tháº¥y.

cÃ¡c nhiá»‡m vá»¥ Ä‘Ã£ biáº¿t vÃ  má»™t sá»‘ nhiá»‡m vá»¥ chÆ°a tháº¥y tháº­m
chÃ­ cÃ³ thá»ƒ mang láº¡i Ä‘á»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n nhiá»u, pháº§n lá»›n
cÃ¡c nhiá»‡m vá»¥ thá»ƒ hiá»‡n Ä‘á»™ chÃ­nh xÃ¡c xuáº¥t sáº¯c, cÃ³ nghÄ©a lÃ 
cÃ¡c module trong NN-Factory cÃ³ thá»ƒ Ä‘Æ°á»£c láº¯p rÃ¡p hiá»‡u quáº£
Ä‘á»ƒ xá»­ lÃ½ cÃ¡c nhiá»‡m vá»¥ má»›i mÃ  khÃ´ng cáº§n dá»¯ liá»‡u huáº¥n luyá»‡n.
Káº¿t quáº£ nÃ y lÃ m ná»•i báº­t kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a máº¡nh máº½
cá»§a NN-Factory, vÃ¬ nÃ³ cÃ³ thá»ƒ náº¯m báº¯t Ã½ nghÄ©a cá»§a cÃ¡c thuá»™c
tÃ­nh táº¡o thÃ nh má»™t nhiá»‡m vá»¥ vÃ  hiá»ƒu cÃ¡c thao tÃ¡c liÃªn quan
Ä‘áº¿n sá»± káº¿t há»£p cá»§a chÃºng.

ChÃºng tÃ´i tiáº¿p tá»¥c phÃ¢n tÃ­ch kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a
NN-Factory dá»±a trÃªn sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a viá»‡c lá»±a chá»n gate
cho cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau. Káº¿t quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong
HÃ¬nh 15. Viá»‡c lá»±a chá»n gate thá»ƒ hiá»‡n sá»± tÆ°Æ¡ng Ä‘á»“ng cao hÆ¡n
khi cÃ¡c nhiá»‡m vá»¥ giá»‘ng nhau hÆ¡n (vÃ­ dá»¥: 'Chá»‰ cÃ³ má»™t sá»‘ 0'
vÃ  'Chá»‰ cÃ³ ba sá»‘ 0'). NgoÃ i ra, trong má»™t nhiá»‡m vá»¥ cá»‘ Ä‘á»‹nh,
viá»‡c tÄƒng Ä‘á»™ gáº§n gÅ©i trong cÃ¡c giá»›i háº¡n kÃ­ch hoáº¡t cho trÆ°á»›c
dáº«n Ä‘áº¿n sá»± tÆ°Æ¡ng Ä‘á»“ng lá»›n hÆ¡n trong viá»‡c lá»±a chá»n gate.
Äiá»u nÃ y chá»©ng minh kháº£ nÄƒng cá»§a NN-Factory trong viá»‡c
hiá»ƒu cáº£ yÃªu cáº§u nhiá»‡m vá»¥ vÃ  giá»›i háº¡n kÃ­ch hoáº¡t Ä‘Æ°á»£c chá»‰
Ä‘á»‹nh, vÃ  Ã¡nh xáº¡ chÃºng Ä‘áº¿n cÃ¡c module tÆ°Æ¡ng á»©ng. Äiá»u nÃ y
phá»¥c vá»¥ nhÆ° ná»n táº£ng cho kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a nÃ³.

Vá»›i kháº£ nÄƒng nÃ y, NN-Factory cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c module
liÃªn quan nháº¥t cho má»™t nhiá»‡m vá»¥ chÆ°a tháº¥y vÃ  láº¯p rÃ¡p má»™t
mÃ´ hÃ¬nh vá»›i chÃºng. VÃ­ dá»¥, cÃ¡c module Ä‘Æ°á»£c kÃ­ch hoáº¡t cho
nhiá»‡m vá»¥ chÆ°a tháº¥y 'chá»‰ cÃ³ hai sá»‘ 0' tÆ°Æ¡ng tá»± vá»›i cÃ¡c module
cá»§a 'chá»‰ cÃ³ ba sá»‘ 0' vÃ  'chá»‰ cÃ³ má»™t sá»‘ 0' trong HÃ¬nh 15. Äiá»u
nÃ y dáº«n Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cao trÃªn nhiá»‡m vá»¥ chÆ°a tháº¥y. Do
Ä‘Ã³, kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a NN-Factory cÃ³ má»‘i tÆ°Æ¡ng
quan tÃ­ch cá»±c vá»›i sá»‘ lÆ°á»£ng nhiá»‡m vá»¥

12

--- TRANG 12 ---
Preprint, , 2023 Xu vÃ  Li et al.

Ä‘á»ƒ huáº¥n luyá»‡n, cÃ³ thá»ƒ dáº«n Ä‘áº¿n nhiá»u module há»¯u Ã­ch hÆ¡n
vÃ  má»™t assembler máº¡nh máº½ hÆ¡n.

6 THáº¢O LUáº¬N

á» Ä‘Ã¢y chÃºng tÃ´i nÃªu báº­t má»™t sá»‘ váº¥n Ä‘á» Ä‘Ã¡ng tháº£o luáº­n thÃªm.

Kháº£ nÄƒng Ã¡p dá»¥ng cho cÃ¡c loáº¡i nhiá»‡m vá»¥ khÃ¡c. Hiá»‡n táº¡i,
NN-Factory chá»‰ há»— trá»£ tÃ¹y chá»‰nh cÃ¡c mÃ´ hÃ¬nh cho cÃ¡c nhiá»‡m
vá»¥ trong má»™t khÃ´ng gian tá»• há»£p, Ä‘iá»u nÃ y cÃ³ thá»ƒ háº¡n cháº¿
kháº£ nÄƒng Ã¡p dá»¥ng cá»§a nÃ³ cho cÃ¡c tÃ¬nh huá»‘ng sá»­ dá»¥ng tá»•ng
quÃ¡t hÆ¡n. Äá»ƒ cho phÃ©p cÃ¡c khÃ´ng gian nhiá»‡m vá»¥ linh hoáº¡t
hÆ¡n, chÃºng tÃ´i cáº§n sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh sinh máº¡nh máº½ hÆ¡n
nhÆ° assembler, nháº­n Ä‘á»‹nh nghÄ©a nhiá»‡m vá»¥ dáº¡ng tá»± do (vÃ­
dá»¥: mÃ´ táº£ nhiá»‡m vá»¥ báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn) lÃ m Ä‘áº§u vÃ o,
vÃ  hoáº¡t Ä‘á»™ng trÃªn má»™t táº­p há»£p lá»›n hÆ¡n cÃ¡c module nÆ¡-ron.
Äá»“ng thá»i, má»™t táº­p dá»¯ liá»‡u lá»›n chá»©a cÃ¡c Ã¡nh xáº¡ giá»¯a cÃ¡c
nhiá»‡m vá»¥ khÃ¡c nhau vÃ  cÃ¡c cáº·p Ä‘áº§u vÃ o/Ä‘áº§u ra tÆ°Æ¡ng á»©ng
lÃ  cáº§n thiáº¿t Ä‘á»ƒ huáº¥n luyá»‡n assembler vÃ  cÃ¡c module. Äiá»u
nÃ y kháº£ thi theo nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y cá»§a cÃ¡c mÃ´ hÃ¬nh
lá»›n Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c (vÃ­ dá»¥: ImageBind, ChatGPT,
v.v.), nhÆ°ng viá»‡c huáº¥n luyá»‡n má»™t bá»™ táº¡o mÃ´ hÃ¬nh má»¥c Ä‘Ã­ch
chung nhÆ° váº­y ráº¥t tá»‘n thá»i gian vÃ  tÃ i nguyÃªn, Ä‘iá»u nÃ y
khÃ´ng thá»±c táº¿ Ä‘á»‘i vá»›i háº§u háº¿t cÃ¡c nhÃ  nghiÃªn cá»©u. ChÃºng
tÃ´i Ä‘á»ƒ viá»‡c phÃ¡t triá»ƒn NN-Factory má»¥c Ä‘Ã­ch chung nhÆ° váº­y
cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.

Chi phÃ­ chuáº©n bá»‹ cá»§a NN-Factory. Kháº£ nÄƒng tÃ¹y chá»‰nh mÃ´
hÃ¬nh nhanh chÃ³ng Ä‘Ã¡ng ká»ƒ cá»§a NN-Factory Ä‘i kÃ¨m vá»›i chi
phÃ­ thá»i gian chuáº©n bá»‹ offline dÃ i hÆ¡n. Cá»¥ thá»ƒ, viá»‡c huáº¥n
luyá»‡n supernet modular vÃ  assembler module máº¥t thá»i gian
dÃ i hÆ¡n nhiá»u so vá»›i viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tÄ©nh bÃ¬nh
thÆ°á»ng. Äiá»u nÃ y lÃ  do NN-Factory khÃ´ng chá»‰ cáº§n há»c cÃ¡ch
giáº£i quyáº¿t tá»«ng nhiá»‡m vá»¥ riÃªng láº», mÃ  cÃ²n cÃ¡ch tÃ¡ch rá»i
cÃ¡c module vÃ  láº¯p rÃ¡p láº¡i chÃºng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c nhiá»‡m
vá»¥ má»›i. Xem xÃ©t chi phÃ­ biÃªn giáº£m Ä‘á»ƒ há»— trá»£ cÃ¡c tÃ¬nh huá»‘ng
biÃªn Ä‘a dáº¡ng, chi phÃ­ chuáº©n bá»‹ má»™t láº§n Ã­t quan trá»ng hÆ¡n.
Nhá»¯ng lá»£i Ã­ch cá»§a viá»‡c giáº£m chi phÃ­ biÃªn cÃ³ giÃ¡ trá»‹ hÆ¡n náº¿u
NN-Factory há»— trá»£ cÃ¡c khÃ´ng gian nhiá»‡m vá»¥ linh hoáº¡t hÆ¡n.

7 Káº¾T LUáº¬N

BÃ i bÃ¡o nÃ y Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p má»›i láº¡ Ä‘á»ƒ tÃ¹y chá»‰nh
nhanh chÃ³ng cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u cho cÃ¡c tÃ¬nh huá»‘ng biÃªn
Ä‘a dáº¡ng. Vá»›i thiáº¿t káº¿ tá»•ng thá»ƒ vá»›i má»™t supernet modular,
má»™t assembler module, vÃ  má»™t bá»™ tÃ¬m kiáº¿m kiáº¿n trÃºc nháº¹,
chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÃ¹y chá»‰nh mÃ´ hÃ¬nh nhanh chÃ³ng
cho cÃ¡c nhiá»‡m vá»¥ biÃªn Ä‘a dáº¡ng vÃ  rÃ ng buá»™c tÃ i nguyÃªn.
CÃ¡c thÃ­ nghiá»‡m Ä‘Ã£ chá»©ng minh cháº¥t lÆ°á»£ng sinh mÃ´ hÃ¬nh
vÃ  tá»‘c Ä‘á»™ xuáº¥t sáº¯c cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i. ChÃºng tÃ´i
tin ráº±ng cÃ´ng viá»‡c cá»§a chÃºng tÃ´i Ä‘Ã£ cho phÃ©p má»™t tráº£i nghiá»‡m
tÃ¹y chá»‰nh mÃ´ hÃ¬nh sinh má»›i vÃ  quan trá»ng.

TÃ€I LIá»†U THAM KHáº¢O

[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, vÃ  Dan Klein. 2016.
Neural module networks. Trong Proceedings of the IEEE conference on
computer vision and pattern recognition. 39â€“48.

[2] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, vÃ  Venkatesh Saligrama.
2017. Adaptive Neural Networks for Efficient Inference. Trong Proceedings
of the 34th International Conference on Machine Learning - Volume 70
(ICML'17). JMLR.org, 527â€“536.

[3] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, vÃ  Song Han.
2020. Once for All: Train One Network and Specialize it for Efficient
Deployment. Trong International Conference on Learning Representations.
https://arxiv.org/pdf/1908.09791.pdf

[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, vÃ  Song Han.
2020. Once-for-All: Train One Network and Specialize it for Efficient
Deployment. Trong 8th International Conference on Learning Representations, ICLR 2020.

[5] Han Cai, Ligeng Zhu, vÃ  Song Han. 2019. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. Trong International
Conference on Learning Representations. https://arxiv.org/pdf/1812.
00332.pdf

[6] Yimin Chen, Jingchao Sun, Xiaocong Jin, Tao Li, Rui Zhang, vÃ 
Yanchao Zhang. 2017. Your face your heart: Secure mobile face authentication with photoplethysmograms. Trong IEEE INFOCOM 2017 -
IEEE Conference on Computer Communications. 1â€“9. https://doi.org/
10.1109/INFOCOM.2017.8057220

[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311
(2022).

[8] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran,
Biswa Sengupta, vÃ  Anil A Bharath. 2018. Generative adversarial
networks: An overview. IEEE signal processing magazine 35, 1 (2018),
53â€“65.

[9] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, vÃ 
Mubarak Shah. 2023. Diffusion models in vision: A survey. IEEE
Transactions on Pattern Analysis and Machine Intelligence (2023).

[10] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, vÃ  Furu
Wei. 2021. Knowledge neurons in pretrained transformers. arXiv
preprint arXiv:2104.08696 (2021).

[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
vÃ  Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929
(2020).

[12] Biyi Fang, Xiao Zeng, Faen Zhang, Hui Xu, vÃ  Mi Zhang. 2020.
FlexDNN: Input-Adaptive On-Device Deep Learning for Efficient Mobile Vision. Trong 2020 IEEE/ACM Symposium on Edge Computing (SEC).
84â€“95. https://doi.org/10.1109/SEC50012.2020.00014

[13] Biyi Fang, Xiao Zeng, vÃ  Mi Zhang. 2018. NestDNN: ResourceAware Multi-Tenant On-Device Deep Learning for Continuous Mobile
Vision. Proceedings of the 24th Annual International Conference on
Mobile Computing and Networking (2018).

[14] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, vÃ  Xinchao
Wang. 2023. Depgraph: Towards any structural pruning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 16091â€“16101.

[15] Peizhen Guo, Bo Hu, vÃ  Wenjun Hu. 2021. Mistify: Automating
DNN Model Porting for On-Device Inference at the Edge. Trong 18th
USENIX Symposium on Networked Systems Design and Implementation
(NSDI 21). USENIX Association, 705â€“719. https://www.usenix.org/
conference/nsdi21/presentation/guo

[16] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
Yichen Wei, vÃ  Jian Sun. 2020. Single Path One-Shot Neural Architecture Search with Uniform Sampling. Trong Computer Vision â€“ ECCV
2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, vÃ  Jan-Michael
Frahm (Eds.). Springer International Publishing, Cham, 544â€“560.

13

--- TRANG 13 ---
Preprint, , 2023 Xu vÃ  Li et al.

[17] Rui Han, Qinglong Zhang, Chi Harold Liu, Guoren Wang, Jian Tang,
vÃ  Lydia Y. Chen. 2021. LegoDNN: Block-Grained Scaling of Deep
Neural Networks for Mobile Vision. Trong Proceedings of the 27th Annual
International Conference on Mobile Computing and Networking (MobiCom '21). Association for Computing Machinery, New York, NY, USA,
406â€“419. https://doi.org/10.1145/3447993.3483249

[18] Song Han, Huizi Mao, vÃ  William J. Dally. 2016. Deep Compression:
Compressing Deep Neural Network with Pruning, Trained Quantization
and Huffman Coding. arXiv: Computer Vision and Pattern Recognition
(2016).

[19] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, vÃ 
Yulin Wang. 2022. Dynamic Neural Networks: A Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 11 (2022),
7436â€“7456. https://doi.org/10.1109/TPAMI.2021.3117837

[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, vÃ  Jian Sun. 2016. Deep
Residual Learning for Image Recognition. Trong 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR). 770â€“778. https:
//doi.org/10.1109/CVPR.2016.90

[21] Yuze He, Li Ma, Zhehao Jiang, Yi Tang, vÃ  Guoliang Xing. 2021. VIEye: Semantic-Based 3D Point Cloud Registration for InfrastructureAssisted Autonomous Driving. Trong Proceedings of the 27th Annual
International Conference on Mobile Computing and Networking (MobiCom '21). Association for Computing Machinery, New York, NY, USA,
573â€“586. https://doi.org/10.1145/3447993.3483276

[22] Sian-Yao Huang vÃ  Wei-Ta Chu. 2021. Searching by Generating:
Flexible and Efficient One-Shot NAS with Architecture Generator. Trong
Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition.

[23] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, vÃ  Geoffrey E
Hinton. 1991. Adaptive mixtures of local experts. Neural computation
3, 1 (1991), 79â€“87.

[24] Seunghyeok Jeon, Yonghun Choi, Yeonwoo Cho, vÃ  Hojung Cha.
2023. HarvNet: Resource-Optimized Operation of Multi-Exit Deep
Neural Networks on Energy Harvesting Devices. Trong Proceedings
of the 21st Annual International Conference on Mobile Systems, Applications and Services (MobiSys '23). Association for Computing Machinery, New York, NY, USA, 42â€“55. https://doi.org/10.1145/3581791.
3596845

[25] Shiqi Jiang, Zhiqi Lin, Yuanchun Li, Yuanchao Shu, vÃ  Yunxin Liu.
2021. Flexible high-resolution object detection on edge devices with
tunable latency. Trong Proceedings of the 27th Annual International Conference on Mobile Computing and Networking. 559â€“572.

[26] Åukasz Kaiser vÃ  Samy Bengio. 2018. Discrete autoencoders for
sequence models. arXiv preprint arXiv:1801.09797 (2018).

[27] Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar,
Jakob Uszkoreit, vÃ  Noam Shazeer. 2018. Fast decoding in sequence
models using discrete latent variables. Trong International Conference on
Machine Learning. PMLR, 2390â€“2399.

[28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, vÃ 
Dario Amodei. 2020. Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 (2020).

[29] Wonjae Kim, Bokyung Son, vÃ  Ildoo Kim. 2021. Vilt: Vision-andlanguage transformer without convolution or region supervision. Trong
International Conference on Machine Learning. PMLR, 5583â€“5594.

[30] Rui Kong, Yuanchun Li, Yizhen Yuan, vÃ  Linghe Kong. 2023. ConvReLU++: Reference-Based Lossless Acceleration of Conv-ReLU Operations on Mobile CPU. Trong Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services
(MobiSys '23). Association for Computing Machinery, New York, NY,
USA, 503â€“515. https://doi.org/10.1145/3581791.3596831

[31] Stefanos Laskaridis, Alexandros Kouris, vÃ  Nicholas D. Lane. 2021.
Adaptive Inference through Early-Exit Networks: Design, Challenges
and Directions. Trong Proceedings of the 5th International Workshop on
Embedded and Mobile Deep Learning (EMDL'21). Association for
Computing Machinery, New York, NY, USA, 1â€“6. https://doi.org/10.
1145/3469116.3470012

[32] Stefanos Laskaridis, Stylianos I. Venieris, Hyeji Kim, vÃ  Nicholas D.
Lane. 2020. HAPI: Hardware-Aware Progressive Inference. Trong 2020
IEEE/ACM International Conference On Computer Aided Design (ICCAD). 1â€“9.

[33] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, vÃ  Patrick Haffner. 1998.
Gradient-based learning applied to document recognition. Proc. IEEE
86, 11 (1998), 2278â€“2324.

[34] Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan
Liang, Liang Lin, vÃ  Xiaojun Chang. 2020. Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation. Trong 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 1986â€“1995. https://doi.org/10.1109/CVPR42600.2020.00206

[35] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev,
Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr
DollÃ¡r, vÃ  C. Lawrence Zitnick. 2014. Microsoft COCO: Common
Objects in Context. CoRR abs/1405.0312 (2014). arXiv:1405.0312
http://arxiv.org/abs/1405.0312

[36] Bingyan Liu, Yuanchun Li, Yunxin Liu, Yao Guo, vÃ  Xiangqun Chen.
2020. Pmc: A privacy-preserving deep learning model customization
framework for edge computing. Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies 4, 4 (2020), 1â€“25.

[37] Chia-Hsiang Liu, Yu-Shin Han, Yuan-Yao Sung, Yi Lee, Hung-Yueh
Chiang, vÃ  Kai-Chiang Wu. 2021. FOX-NAS: Fast, On-device and
Explainable Neural Architecture Search. CoRR abs/2108.08189 (2021).
arXiv:2108.08189 https://arxiv.org/abs/2108.08189

[38] Hanxiao Liu, Karen Simonyan, vÃ  Yiming Yang. 2018. DARTS:
Differentiable Architecture Search. CoRR abs/1806.09055 (2018).
arXiv:1806.09055 http://arxiv.org/abs/1806.09055

[39] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan,
vÃ  Changshui Zhang. 2017. Learning Efficient Convolutional Networks through Network Slimming. Trong 2017 IEEE International Conference on Computer Vision (ICCV). 2755â€“2763. https://doi.org/10.1109/
ICCV.2017.298

[40] OpenAI. 2022. ChatGPT. [Trá»±c tuyáº¿n]. CÃ³ sáºµn táº¡i: https://openai.com/
blog/chatgpt/.

[41] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li,
Dengyong Zhou, vÃ  Pushmeet Kohli. 2016. Neuro-symbolic program
synthesis. arXiv preprint arXiv:1611.01855 (2016).

[42] Jinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin, vÃ  Wonyong
Sung. 2018. Fully Neural Network Based Speech Recognition on
Mobile and Embedded Devices. Trong NeurIPS.

[43] Esteban Real, Alok Aggarwal, Yanping Huang, vÃ  Quoc V. Le. 2019.
Regularized Evolution for Image Classifier Architecture Search. Trong
Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence
Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence (AAAI'19/IAAI'19/EAAI'19). AAAI Press, Article
587, 10 pages. https://doi.org/10.1609/aaai.v33i01.33014780

[44] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
vÃ  Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals and
Linear Bottlenecks. Trong 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 4510â€“4520. https://doi.org/10.1109/CVPR.
2018.00474

[45] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, vÃ  Quoc V. Le. 2019. MnasNet: Platform-Aware
Neural Architecture Search for Mobile. Trong 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR). 2815â€“2823.

14

--- TRANG 14 ---
MÃ´ hÃ¬nh Sinh cho cÃ¡c MÃ´ hÃ¬nh: TÃ¹y chá»‰nh DNN nhanh chÃ³ng cho cÃ¡c Nhiá»‡m vá»¥ Äa dáº¡ng vÃ  RÃ ng buá»™c TÃ i nguyÃªn Preprint, , 2023

https://doi.org/10.1109/CVPR.2019.00293

[46] Mingxing Tan vÃ  Quoc Le. 2021. EfficientNetV2: Smaller Models and
Faster Training. Trong Proceedings of the 38th International Conference
on Machine Learning (Proceedings of Machine Learning Research),
Marina Meila vÃ  Tong Zhang (Eds.), Vol. 139. PMLR, 10096â€“10106.
https://proceedings.mlr.press/v139/tan21a.html

[47] Mingxing Tan vÃ  Quoc V Le. 2019. EfficientNet: Rethinking Model
Scaling for Convolutional Neural Networks. ICML (2019).

[48] Mingxing Tan, Ruoming Pang, vÃ  Quoc V. Le. 2019. EfficientDet:
Scalable and Efficient Object Detection. CoRR abs/1911.09070 (2019).
arXiv:1911.09070 http://arxiv.org/abs/1911.09070

[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Åukasz Kaiser, vÃ  Illia Polosukhin. 2017.
Attention is All You Need. Trong Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran
Associates Inc., Red Hook, NY, USA, 6000â€“6010.

[50] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, vÃ  Song Han. 2019.
HAQ: Hardware-Aware Automated Quantization With Mixed Precision. Trong IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

[51] Quan Wang, Ignacio Lopez Moreno, Mert Saglam, Kevin Wilson, Alan
Chiao, Renjie Liu, Yanzhang He, Wei Li, Jason Pelecanos, Marily
Nika, et al. 2020. VoiceFilter-Lite: Streaming targeted voice separation
for on-device speech recognition. arXiv preprint arXiv:2009.04323
(2020).

[52] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, vÃ  Joseph E. Gonzalez. 2018. SkipNet: Learning Dynamic Routing in Convolutional
Networks. Trong The European Conference on Computer Vision (ECCV).

[53] Hao Wen, Yuanchun Li, Zunshuai Zhang, Shiqi Jiang, Xiaozhou Ye,
Ye Ouyang, Ya-Qin Zhang, vÃ  Yunxin Liu. 2023. AdaptiveNet: Postdeployment Neural Architecture Adaptation for Diverse Edge Environments. (2023).

[54] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun,
Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, vÃ  Kurt
Keutzer. 2019. FBNet: Hardware-Aware Efficient ConvNet Design
via Differentiable Neural Architecture Search. Trong 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10726â€“
10734. https://doi.org/10.1109/CVPR.2019.01099

[55] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie,
Larry S Davis, Kristen Grauman, vÃ  Rogerio Feris. 2018. BlockDrop: Dynamic Inference Paths in Residual Networks. Trong CVPR.

[56] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark
Sandler, Vivienne Sze, vÃ  Hartwig Adam. 2018. NetAdapt: PlatformAware Neural Network Adaptation for Mobile Applications. Trong The
European Conference on Computer Vision (ECCV).

[57] Jiahui Yu vÃ  Thomas S. Huang. 2019. Universally Slimmable Networks and Improved Training Techniques. CoRR abs/1903.05134
(2019). arXiv:1903.05134 http://arxiv.org/abs/1903.05134

[58] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, vÃ  Thomas S. Huang.
2018. Slimmable Neural Networks. CoRR abs/1812.08928 (2018).
arXiv:1812.08928 http://arxiv.org/abs/1812.08928

[59] Huanhuan Zhang, Anfu Zhou, Yuhan Hu, Chaoyue Li, Guangping
Wang, Xinyu Zhang, Huadong Ma, Leilei Wu, Aiyun Chen, vÃ 
Changhui Wu. 2021. Loki: Improving Long Tail Performance of
Learning-Based Real-Time Video Adaptation by Fusing Rule-Based
Models. Trong Proceedings of the 27th Annual International Conference on Mobile Computing and Networking (MobiCom '21). Association for Computing Machinery, New York, NY, USA, 775â€“788.
https://doi.org/10.1145/3447993.3483259

[60] Xumiao Zhang, Anlan Zhang, Jiachen Sun, Xiao Zhu, Y. Ethan Guo,
Feng Qian, vÃ  Z. Morley Mao. 2021. EMP: Edge-Assisted Multi-
Vehicle Perception. Trong Proceedings of the 27th Annual International
Conference on Mobile Computing and Networking (MobiCom '21).
Association for Computing Machinery, New York, NY, USA, 545â€“558.
https://doi.org/10.1145/3447993.3483242

[61] Ziqi Zhang, Yuanchun Li, Yao Guo, Xiangqun Chen, vÃ  Yunxin Liu.
2020. Dynamic slicing for deep neural networks. Trong Proceedings of
the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering.
838â€“850.

[62] Ziqi Zhang, Yuanchun Li, Bingyan Liu, Yifeng Cai, Ding Li, Yao
Guo, vÃ  Xiangqun Chen. 2023. FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing. Trong 2023
IEEE/ACM 45th International Conference on Software Engineering
(ICSE). IEEE, 460â€“472.

[63] Ziqi Zhang, Yuanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Yao
Guo, Xiangqun Chen, vÃ  Yunxin Liu. 2022. ReMoS: reducing defect
inheritance in transfer learning via relevant model slicing. Trong Proceedings of the 44th International Conference on Software Engineering.
1856â€“1868.

[64] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun,
vÃ  Jie Zhou. 2021. Moefication: Transformer feed-forward layers are
mixtures of experts. arXiv preprint arXiv:2110.01786 (2021).

[65] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang,
Zhelong Li, Xiuqi Yang, vÃ  Junjie Yan. 2020. Towards Unified INT8
Training for Convolutional Neural Network. Trong 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1966â€“1976.
https://doi.org/10.1109/CVPR42600.2020.00204

[66] Barret Zoph vÃ  Quoc V. Le. 2016. Neural Architecture Search
with Reinforcement Learning. CoRR abs/1611.01578 (2016).
arXiv:1611.01578 http://arxiv.org/abs/1611.01578

15
