# 2303.07669.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/automl-nas/2303.07669.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1052276 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
AUTOTRANSFER : AUTOML Vá»šI CHUYá»‚N GIAO KIáº¾N THá»¨C - á»¨NG Dá»¤NG CHO Máº NG NEURAL Äá»’ THá»Š
Kaidi Cao Jiaxuan You Jiaju Liu Jure Leskovec
Khoa Khoa há»c MÃ¡y tÃ­nh, Äáº¡i há»c Stanford
{kaidicao, jiaxuan, jiajuliu, jure}@cs.stanford.edu
TÃ“M Táº®T
AutoML Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c thÃ nh cÃ´ng Ä‘Ã¡ng ká»ƒ trong viá»‡c tÃ¬m ra kiáº¿n trÃºc máº¡ng neural hiá»‡u quáº£ cho má»™t nhiá»‡m vá»¥ há»c mÃ¡y nháº¥t Ä‘á»‹nh Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi má»™t táº­p dá»¯ liá»‡u cá»¥ thá»ƒ vÃ  má»™t thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡. Tuy nhiÃªn, háº§u háº¿t cÃ¡c ká»¹ thuáº­t AutoML hiá»‡n táº¡i xem xÃ©t má»—i nhiá»‡m vá»¥ má»™t cÃ¡ch Ä‘á»™c láº­p tá»« Ä‘áº§u, Ä‘iá»u nÃ y Ä‘Ã²i há»i pháº£i khÃ¡m phÃ¡ nhiá»u kiáº¿n trÃºc, dáº«n Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n cao. á» Ä‘Ã¢y chÃºng tÃ´i Ä‘á» xuáº¥t AUTOTRANSFER, má»™t giáº£i phÃ¡p AutoML cáº£i thiá»‡n hiá»‡u quáº£ tÃ¬m kiáº¿m báº±ng cÃ¡ch chuyá»ƒn giao kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc trÆ°á»›c Ä‘Ã³ cho nhiá»‡m vá»¥ má»›i quan tÃ¢m. Äá»•i má»›i chÃ­nh cá»§a chÃºng tÃ´i bao gá»“m má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh thu tháº­p hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc GNN vÃ  nhiá»‡m vá»¥, vÃ  má»™t nhÃºng nhiá»‡m vá»¥ hiá»‡u quáº£ tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘o lÆ°á»ng chÃ­nh xÃ¡c sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau. Dá»±a trÃªn ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh vÃ  cÃ¡c nhÃºng nhiá»‡m vá»¥, chÃºng tÃ´i Æ°á»›c tÃ­nh cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ cá»§a cÃ¡c mÃ´ hÃ¬nh mong muá»‘n cho nhiá»‡m vá»¥ má»›i, báº±ng cÃ¡ch tá»•ng há»£p má»™t tá»•ng cÃ³ trá»ng sá»‘ tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ top-K trÃªn cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± vá»›i nhiá»‡m vá»¥ quan tÃ¢m. CÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i báº¥t ká»³ thuáº­t toÃ¡n tÃ¬m kiáº¿m AutoML nÃ o. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER trÃªn sÃ¡u táº­p dá»¯ liá»‡u trong lÄ©nh vá»±c há»c mÃ¡y Ä‘á»“ thá»‹. CÃ¡c thÃ­ nghiá»‡m chá»©ng minh ráº±ng (i) nhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£, vÃ  cÃ¡c nhiá»‡m vá»¥ cÃ³ nhÃºng tÆ°Æ¡ng tá»± cÃ³ cÃ¡c kiáº¿n trÃºc hoáº¡t Ä‘á»™ng tá»‘t nháº¥t tÆ°Æ¡ng tá»±; (ii) AUTOTRANSFER cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ tÃ¬m kiáº¿m vá»›i cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao, giáº£m sá»‘ lÆ°á»£ng kiáº¿n trÃºc Ä‘Æ°á»£c khÃ¡m phÃ¡ Ä‘i má»™t báº­c Ä‘á»™ lá»›n. Cuá»‘i cÃ¹ng, chÃºng tÃ´i phÃ¡t hÃ nh GNN-BANK-101, má»™t táº­p dá»¯ liá»‡u quy mÃ´ lá»›n vá»›i thÃ´ng tin Ä‘Ã o táº¡o GNN chi tiáº¿t cá»§a 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘á»ƒ há»— trá»£ vÃ  truyá»n cáº£m há»©ng cho nghiÃªn cá»©u tÆ°Æ¡ng lai.

1 GIá»šI THIá»†U
Máº¡ng neural sÃ¢u cÃ³ tÃ­nh module cao, Ä‘Ã²i há»i nhiá»u quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ pháº£i Ä‘Æ°á»£c Ä‘Æ°a ra liÃªn quan Ä‘áº¿n kiáº¿n trÃºc máº¡ng vÃ  siÃªu tham sá»‘. Nhá»¯ng quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ nÃ y táº¡o thÃ nh má»™t khÃ´ng gian tÃ¬m kiáº¿m khÃ´ng lá»“i vÃ  tá»‘n kÃ©m ngay cáº£ Ä‘á»‘i vá»›i cÃ¡c chuyÃªn gia Ä‘á»ƒ tá»‘i Æ°u hÃ³a, Ä‘áº·c biá»‡t khi viá»‡c tá»‘i Æ°u hÃ³a pháº£i Ä‘Æ°á»£c láº·p láº¡i tá»« Ä‘áº§u cho má»—i trÆ°á»ng há»£p sá»­ dá»¥ng má»›i. Há»c mÃ¡y tá»± Ä‘á»™ng (AutoML) lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u tÃ­ch cá»±c nháº±m giáº£m ná»— lá»±c con ngÆ°á»i cáº§n thiáº¿t cho thiáº¿t káº¿ kiáº¿n trÃºc thÆ°á»ng bao gá»“m tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ vÃ  tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng neural. AutoML Ä‘Ã£ chá»©ng minh thÃ nh cÃ´ng (Zoph and Le, 2016; Pham et al., 2018; Zoph et al., 2018; Cai et al., 2018; He et al., 2018; Guo et al., 2020; Erickson et al., 2020; LeDell and Poirier, 2020) trong nhiá»u lÄ©nh vá»±c á»©ng dá»¥ng.

TÃ¬m ra má»™t mÃ´ hÃ¬nh tá»‘t há»£p lÃ½ cho má»™t nhiá»‡m vá»¥ há»c táº­p má»›iÂ¹ má»™t cÃ¡ch hiá»‡u quáº£ tÃ­nh toÃ¡n lÃ  ráº¥t quan trá»ng Ä‘á»ƒ lÃ m cho há»c sÃ¢u cÃ³ thá»ƒ tiáº¿p cáº­n Ä‘Æ°á»£c vá»›i cÃ¡c chuyÃªn gia lÄ©nh vá»±c cÃ³ ná»n táº£ng Ä‘a dáº¡ng. AutoML hiá»‡u quáº£ Ä‘áº·c biá»‡t quan trá»ng trong cÃ¡c lÄ©nh vá»±c mÃ  cÃ¡c kiáº¿n trÃºc/siÃªu tham sá»‘ tá»‘t nháº¥t ráº¥t nháº¡y cáº£m vá»›i nhiá»‡m vá»¥. Má»™t vÃ­ dá»¥ Ä‘Ã¡ng chÃº Ã½ lÃ  lÄ©nh vá»±c há»c Ä‘á»“ thá»‹Â². Thá»© nháº¥t, cÃ¡c phÆ°Æ¡ng phÃ¡p há»c Ä‘á»“ thá»‹ nháº­n dá»¯ liá»‡u Ä‘áº§u vÃ o bao gá»“m nhiá»u loáº¡i dá»¯ liá»‡u vÃ  tá»‘i Æ°u hÃ³a trÃªn cÃ¡c nhiá»‡m vá»¥ tráº£i rá»™ng trÃªn má»™t táº­p há»£p lÄ©nh vá»±c vÃ  phÆ°Æ¡ng thá»©c Ä‘a dáº¡ng nhÆ° Ä‘á» xuáº¥t (Ying et al., 2018; He et al., 2020), mÃ´ phá»ng váº­t lÃ½ (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2020), vÃ  tin sinh há»c (Zitnik et al., 2018). Äiá»u nÃ y khÃ¡c vá»›i thá»‹ giÃ¡c mÃ¡y tÃ­nh vÃ  xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn nÆ¡i dá»¯ liá»‡u Ä‘áº§u vÃ o cÃ³ cáº¥u trÃºc Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c, cá»‘ Ä‘á»‹nh cÃ³ thá»ƒ Ä‘Æ°á»£c chia sáº» giá»¯a cÃ¡c kiáº¿n trÃºc máº¡ng neural khÃ¡c nhau.

Â¹Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i gá»i má»™t nhiá»‡m vá»¥ lÃ  má»™t táº­p dá»¯ liá»‡u nháº¥t Ä‘á»‹nh vá»›i má»™t thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡/hÃ m máº¥t mÃ¡t, vÃ­ dá»¥: hÃ m máº¥t mÃ¡t entropy chÃ©o trÃªn phÃ¢n loáº¡i nÃºt trÃªn táº­p dá»¯ liá»‡u Cora.
Â²ChÃºng tÃ´i táº­p trung vÃ o lÄ©nh vá»±c há»c Ä‘á»“ thá»‹ trong bÃ i bÃ¡o nÃ y. AUTOTRANSFER cÃ³ thá»ƒ Ä‘Æ°á»£c tá»•ng quÃ¡t hÃ³a cho cÃ¡c lÄ©nh vá»±c khÃ¡c.

1arXiv:2303.07669v1  [cs.LG]  14 Mar 2023

--- TRANG 2 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Thá»© hai, máº¡ng neural hoáº¡t Ä‘á»™ng trÃªn Ä‘á»“ thá»‹ Ä‘i kÃ¨m vá»›i má»™t táº­p há»£p phong phÃº cÃ¡c lá»±a chá»n thiáº¿t káº¿ vÃ  má»™t táº­p há»£p lá»›n cÃ¡c tham sá»‘ Ä‘á»ƒ khÃ¡m phÃ¡. Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° cÃ¡c lÄ©nh vá»±c khÃ¡c nÆ¡i má»™t vÃ i kiáº¿n trÃºc Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c nhÆ° ResNet (He et al., 2016) vÃ  GPT-3 (Brown et al., 2020) thá»‘ng trá»‹ cÃ¡c benchmark, Ä‘Ã£ Ä‘Æ°á»£c chá»‰ ra ráº±ng thiáº¿t káº¿ máº¡ng neural Ä‘á»“ thá»‹ (GNN) tá»‘t nháº¥t phá»¥ thuá»™c ráº¥t nhiá»u vÃ o nhiá»‡m vá»¥ (You et al., 2020).

Máº·c dÃ¹ AutoML nhÆ° má»™t lÄ©nh vá»±c nghiÃªn cá»©u Ä‘ang phÃ¡t triá»ƒn nhanh chÃ³ng, cÃ¡c giáº£i phÃ¡p AutoML hiá»‡n táº¡i cÃ³ chi phÃ­ tÃ­nh toÃ¡n khá»•ng lá»“ khi má»¥c tiÃªu lÃ  tÃ¬m ra má»™t mÃ´ hÃ¬nh tá»‘t cho má»™t nhiá»‡m vá»¥ há»c táº­p má»›i. Háº§u háº¿t cÃ¡c ká»¹ thuáº­t AutoML hiá»‡n táº¡i xem xÃ©t má»—i nhiá»‡m vá»¥ má»™t cÃ¡ch Ä‘á»™c láº­p vÃ  riÃªng biá»‡t, do Ä‘Ã³ chÃºng Ä‘Ã²i há»i pháº£i lÃ m láº¡i viá»‡c tÃ¬m kiáº¿m tá»« Ä‘áº§u cho má»—i nhiá»‡m vá»¥ má»›i. CÃ¡ch tiáº¿p cáº­n nÃ y bá» qua kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc cÃ³ giÃ¡ trá»‹ tiá»m nÄƒng thu Ä‘Æ°á»£c tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³, vÃ  cháº¯c cháº¯n dáº«n Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n cao. Váº¥n Ä‘á» nÃ y Ä‘áº·c biá»‡t quan trá»ng trong lÄ©nh vá»±c há»c Ä‘á»“ thá»‹ Gao et al. (2019); Zhou et al. (2019), do nhá»¯ng thÃ¡ch thá»©c cá»§a cÃ¡c loáº¡i nhiá»‡m vá»¥ Ä‘a dáº¡ng vÃ  khÃ´ng gian thiáº¿t káº¿ khá»•ng lá»“ Ä‘Æ°á»£c tháº£o luáº­n á»Ÿ trÃªn.

á» Ä‘Ã¢y chÃºng tÃ´i Ä‘á» xuáº¥t AUTOTRANSFERÂ³, má»™t giáº£i phÃ¡p AutoML cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ viá»‡c tÃ¬m kiáº¿m kiáº¿n trÃºc AutoML báº±ng cÃ¡ch chuyá»ƒn giao kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc trÆ°á»›c Ä‘Ã³ cho nhiá»‡m vá»¥ quan tÃ¢m. Äá»•i má»›i chÃ­nh cá»§a chÃºng tÃ´i lÃ  giá»›i thiá»‡u má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh lÆ°u trá»¯ hiá»‡u suáº¥t cá»§a má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc GNN vÃ  nhiá»‡m vá»¥ Ä‘á»ƒ hÆ°á»›ng dáº«n thuáº­t toÃ¡n tÃ¬m kiáº¿m. Äá»ƒ cho phÃ©p chuyá»ƒn giao kiáº¿n thá»©c, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a má»™t khÃ´ng gian nhÃºng nhiá»‡m vá»¥ sao cho cÃ¡c nhiá»‡m vá»¥ gáº§n trong khÃ´ng gian nhÃºng cÃ³ cÃ¡c kiáº¿n trÃºc hoáº¡t Ä‘á»™ng tá»‘t nháº¥t tÆ°Æ¡ng á»©ng tÆ°Æ¡ng tá»±. ThÃ¡ch thá»©c á»Ÿ Ä‘Ã¢y lÃ  nhÃºng nhiá»‡m vá»¥ cáº§n thu tháº­p cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c kiáº¿n trÃºc khÃ¡c nhau trÃªn cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c nhau, trong khi hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n.

Äá»•i má»›i cá»§a chÃºng tÃ´i á»Ÿ Ä‘Ã¢y lÃ  nhÃºng má»™t nhiá»‡m vá»¥ báº±ng cÃ¡ch sá»­ dá»¥ng sá»‘ Ä‘iá»u kiá»‡n cá»§a Ma tráº­n ThÃ´ng tin Fisher cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn khÃ¡c nhau vÃ  cÅ©ng má»™t sÆ¡ Ä‘á»“ há»c táº­p vá»›i Ä‘áº£m báº£o tá»•ng quÃ¡t hÃ³a thá»±c nghiá»‡m. Báº±ng cÃ¡ch nÃ y, chÃºng tÃ´i ngáº§m thu tháº­p cÃ¡c thuá»™c tÃ­nh cá»§a nhiá»‡m vá»¥ há»c táº­p, trong khi nhanh hÆ¡n nhiá»u báº­c Ä‘á»™ lá»›n (trong vÃ i giÃ¢y). Sau Ä‘Ã³ chÃºng tÃ´i Æ°á»›c tÃ­nh tiÃªn nghiá»‡m thiáº¿t káº¿ cá»§a cÃ¡c mÃ´ hÃ¬nh mong muá»‘n cho nhiá»‡m vá»¥ má»›i, báº±ng cÃ¡ch tá»•ng há»£p cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ trÃªn cÃ¡c nhiá»‡m vá»¥ gáº§n vá»›i nhiá»‡m vá»¥ quan tÃ¢m. Cuá»‘i cÃ¹ng, chÃºng tÃ´i khá»Ÿi táº¡o má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m siÃªu tham sá»‘ vá»›i tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng bÃ¡o nhiá»‡m vá»¥ Ä‘Æ°á»£c tÃ­nh toÃ¡n.

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER trÃªn sÃ¡u táº­p dá»¯ liá»‡u, bao gá»“m cáº£ cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i nÃºt vÃ  phÃ¢n loáº¡i Ä‘á»“ thá»‹. ChÃºng tÃ´i chá»‰ ra ráº±ng cÃ¡c nhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£ vÃ  khoáº£ng cÃ¡ch Ä‘Æ°á»£c Ä‘o giá»¯a cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng quan cao (tÆ°Æ¡ng quan Kendall 0.43) vá»›i cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t mÃ´ hÃ¬nh. HÆ¡n ná»¯a, chÃºng tÃ´i trÃ¬nh bÃ y AUTOTRANSFER cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ tÃ¬m kiáº¿m khi sá»­ dá»¥ng tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao. AUTOTRANSFER giáº£m sá»‘ lÆ°á»£ng kiáº¿n trÃºc Ä‘Æ°á»£c khÃ¡m phÃ¡ cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c má»¥c tiÃªu Ä‘i má»™t báº­c Ä‘á»™ lá»›n so vá»›i SOTA. Cuá»‘i cÃ¹ng, chÃºng tÃ´i phÃ¡t hÃ nh GNN-BANK-101â€”cÆ¡ sá»Ÿ dá»¯ liá»‡u quy mÃ´ lá»›n Ä‘áº§u tiÃªn chá»©a cÃ¡c báº£n ghi hiá»‡u suáº¥t chi tiáº¿t cho 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o vá»›i 16.128 giá» GPUâ€”Ä‘á»ƒ há»— trá»£ nghiÃªn cá»©u tÆ°Æ¡ng lai.

2 CÃ”NG TRÃŒNH LIÃŠN QUAN

Trong pháº§n nÃ y, chÃºng tÃ´i tÃ³m táº¯t cÃ´ng trÃ¬nh liÃªn quan vá» AutoML liÃªn quan Ä‘áº¿n cÃ¡c á»©ng dá»¥ng cá»§a nÃ³ trÃªn GNN, cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m phá»• biáº¿n, vÃ  cÃ´ng trÃ¬nh tiÃªn phong liÃªn quan Ä‘áº¿n há»c chuyá»ƒn giao vÃ  nhÃºng nhiá»‡m vá»¥.

AutoML cho GNN. TÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng neural (NAS), má»™t hÃ¬nh thá»©c AutoML Ä‘á»™c Ä‘Ã¡o vÃ  phá»• biáº¿n cho há»c sÃ¢u, cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh hai loáº¡i: NAS Ä‘a thá»­ nghiá»‡m vÃ  NAS má»™t láº§n. Trong NAS Ä‘a thá»­ nghiá»‡m, má»—i kiáº¿n trÃºc Ä‘Æ°á»£c láº¥y máº«u Ä‘Æ°á»£c Ä‘Ã o táº¡o riÃªng biá»‡t. GraphNAS (Gao et al., 2020) vÃ  Auto-GNN (Zhou et al., 2019) lÃ  cÃ¡c thuáº­t toÃ¡n NAS Ä‘a thá»­ nghiá»‡m Ä‘iá»ƒn hÃ¬nh trÃªn GNN Ã¡p dá»¥ng má»™t bá»™ Ä‘iá»u khiá»ƒn RNN há»c Ä‘á» xuáº¥t cÃ¡c táº­p há»£p cáº¥u hÃ¬nh tá»‘t hÆ¡n thÃ´ng qua há»c tÄƒng cÆ°á»ng.

NAS má»™t láº§n (vÃ­ dá»¥: (Liu et al., 2018; Qin et al., 2021; Li et al., 2021)) bao gá»“m viá»‡c Ä‘Ã³ng gÃ³i toÃ n bá»™ khÃ´ng gian mÃ´ hÃ¬nh trong má»™t siÃªu mÃ´ hÃ¬nh, Ä‘Ã o táº¡o siÃªu mÃ´ hÃ¬nh má»™t láº§n, vÃ  sau Ä‘Ã³ láº·p Ä‘i láº·p láº¡i láº¥y máº«u cÃ¡c mÃ´ hÃ¬nh con tá»« siÃªu mÃ´ hÃ¬nh Ä‘á»ƒ tÃ¬m ra mÃ´ hÃ¬nh tá»‘t nháº¥t. NgoÃ i ra, cÃ³ cÃ´ng trÃ¬nh nghiÃªn cá»©u cá»¥ thá»ƒ cÃ¡c lá»±a chá»n thiáº¿t káº¿ chi tiáº¿t nhÆ° tÄƒng cÆ°á»ng dá»¯ liá»‡u (You et al., 2021), loáº¡i lá»›p truyá»n tin (Cai et al., 2021; Ding et al., 2021; Zhao et al., 2021), vÃ  gá»™p Ä‘á»“ thá»‹ (Wei et al., 2021).

ÄÃ¡ng chÃº Ã½, AUTOTRANSFER lÃ  giáº£i phÃ¡p AutoML Ä‘áº§u tiÃªn cho GNN chuyá»ƒn giao hiá»‡u quáº£ kiáº¿n thá»©c thiáº¿t káº¿ giá»¯a cÃ¡c nhiá»‡m vá»¥.

Thuáº­t toÃ¡n HPO. CÃ¡c thuáº­t toÃ¡n Tá»‘i Æ°u hÃ³a SiÃªu tham sá»‘ (HPO) tÃ¬m kiáº¿m cÃ¡c siÃªu tham sá»‘ mÃ´ hÃ¬nh tá»‘i Æ°u báº±ng cÃ¡ch láº·p Ä‘i láº·p láº¡i Ä‘á» xuáº¥t má»™t táº­p há»£p siÃªu tham sá»‘ vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a chÃºng. TÃ¬m kiáº¿m ngáº«u nhiÃªn láº¥y máº«u siÃªu tham sá»‘ tá»« khÃ´ng gian tÃ¬m kiáº¿m vá»›i xÃ¡c suáº¥t báº±ng nhau. Máº·c dÃ¹ khÃ´ng

Â³MÃ£ nguá»“n cÃ³ sáºµn táº¡i https://github.com/snap-stanford/AutoTransfer.

2

--- TRANG 3 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

NgÃ¢n hÃ ng Nhiá»‡m vá»¥-MÃ´ hÃ¬nh(cÃ¡c báº£n ghi hiá»‡u suáº¥t lá»‹ch sá»­)KhÃ´ng gian NhÃºng Nhiá»‡m vá»¥PhÃ¢n phá»‘i Thiáº¿tkáº¿Nhiá»‡m vá»¥AggDimEpoch...Val_lossCorasum6480...0.22Coramean128200...0.26TU-DDsum64200...0.46TU-DDmean128200...0.86TU-DDmax256800...0.52Arxivmean128200...0.68â€¦â€¦â€¦â€¦â€¦â€¦

AggDimEpoch
Agg
Nhiá»‡m vá»¥ má»›i quan tÃ¢mDimâ€‹Epoch...â€‹...â€‹

HÃ¬nh 1: Tá»•ng quan vá» AUTOTRANSFER. TrÃ¡i: ChÃºng tÃ´i giá»›i thiá»‡u GNN-BANK-101, má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u lá»›n chá»©a má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc GNN vÃ  siÃªu tham sá»‘ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau, cÃ¹ng vá»›i cÃ¡c thá»‘ng kÃª Ä‘Ã o táº¡o/Ä‘Ã¡nh giÃ¡ cá»§a chÃºng. Giá»¯a: ChÃºng tÃ´i giá»›i thiá»‡u má»™t khÃ´ng gian nhÃºng nhiá»‡m vá»¥, nÆ¡i má»—i Ä‘iá»ƒm tÆ°Æ¡ng á»©ng vá»›i má»™t nhiá»‡m vá»¥ khÃ¡c nhau. CÃ¡c nhiá»‡m vá»¥ gáº§n trong khÃ´ng gian nhÃºng cÃ³ cÃ¡c mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t tÆ°Æ¡ng á»©ng tÆ°Æ¡ng tá»±. Pháº£i: Cho má»™t nhiá»‡m vá»¥ má»›i quan tÃ¢m, chÃºng tÃ´i hÆ°á»›ng dáº«n tÃ¬m kiáº¿m AutoML báº±ng cÃ¡ch tham kháº£o cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ cá»§a cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± nháº¥t trong khÃ´ng gian nhÃºng nhiá»‡m vá»¥.

há»c há»i tá»« cÃ¡c thá»­ nghiá»‡m trÆ°á»›c Ä‘Ã³, tÃ¬m kiáº¿m ngáº«u nhiÃªn thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vÃ¬ tÃ­nh Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ hÆ¡n nhiá»u so vá»›i tÃ¬m kiáº¿m lÆ°á»›i (Bergstra and Bengio, 2012). Thuáº­t toÃ¡n TPE (Bergstra et al., 2011) xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh xÃ¡c suáº¥t cá»§a hiá»‡u suáº¥t nhiá»‡m vá»¥ trÃªn khÃ´ng gian siÃªu tham sá»‘ vÃ  sá»­ dá»¥ng káº¿t quáº£ cá»§a cÃ¡c thá»­ nghiá»‡m trong quÃ¡ khá»© Ä‘á»ƒ chá»n cáº¥u hÃ¬nh tiáº¿p theo há»©a háº¹n nháº¥t Ä‘á»ƒ Ä‘Ã o táº¡o, mÃ  thuáº­t toÃ¡n TPE Ä‘á»‹nh nghÄ©a lÃ  tá»‘i Ä‘a hÃ³a giÃ¡ trá»‹ Cáº£i thiá»‡n Ká»³ vá»ng (Jones, 2001). CÃ¡c thuáº­t toÃ¡n tiáº¿n hÃ³a (Real et al., 2017; Jaderberg et al., 2017) Ä‘Ã o táº¡o nhiá»u mÃ´ hÃ¬nh song song vÃ  thay tháº¿ cÃ¡c mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng kÃ©m báº±ng cÃ¡c báº£n sao "Ä‘á»™t biáº¿n" cá»§a cÃ¡c mÃ´ hÃ¬nh tá»‘t nháº¥t hiá»‡n táº¡i. AUTOTRANSFER lÃ  má»™t giáº£i phÃ¡p AutoML tá»•ng quÃ¡t vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng káº¿t há»£p vá»›i báº¥t ká»³ thuáº­t toÃ¡n HPO nÃ o trong sá»‘ nÃ y.

Há»c Chuyá»ƒn giao trong AutoML. Wong et al. (2018) Ä‘á» xuáº¥t chuyá»ƒn giao kiáº¿n thá»©c giá»¯a cÃ¡c nhiá»‡m vá»¥ báº±ng cÃ¡ch táº£i láº¡i bá»™ Ä‘iá»u khiá»ƒn cá»§a cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m há»c tÄƒng cÆ°á»ng. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y giáº£ Ä‘á»‹nh ráº±ng khÃ´ng gian tÃ¬m kiáº¿m trÃªn cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau báº¯t Ä‘áº§u vá»›i cÃ¹ng má»™t tiÃªn nghiá»‡m Ä‘Ã£ há»c. KhÃ´ng giá»‘ng nhÆ° AUTOTRANSFER, nÃ³ khÃ´ng thá»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c cá»‘t lÃµi trong AutoML GNN: thiáº¿t káº¿ GNN tá»‘t nháº¥t phá»¥ thuá»™c ráº¥t nhiá»u vÃ o nhiá»‡m vá»¥ cá»¥ thá»ƒ.

GraphGym (You et al., 2020) cá»‘ gáº¯ng chuyá»ƒn giao thiáº¿t káº¿ kiáº¿n trÃºc tá»‘t nháº¥t trá»±c tiáº¿p vá»›i má»™t khÃ´ng gian metric Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥. GraphGym (You et al., 2020) tÃ­nh toÃ¡n sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ báº±ng cÃ¡ch Ä‘Ã o táº¡o má»™t táº­p há»£p 12 "mÃ´ hÃ¬nh neo" Ä‘áº¿n há»™i tá»¥, Ä‘iá»u nÃ y tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n. NgÆ°á»£c láº¡i, AUTOTRANSFER thiáº¿t káº¿ cÃ¡c nhÃºng nhiá»‡m vá»¥ nháº¹ Ä‘Ã²i há»i chi phÃ­ tÃ­nh toÃ¡n tá»‘i thiá»ƒu.

NgoÃ i ra, Zhao and Bilen (2021); Li et al. (2021) Ä‘á» xuáº¥t tiáº¿n hÃ nh tÃ¬m kiáº¿m kiáº¿n trÃºc trÃªn má»™t táº­p con proxy cá»§a toÃ n bá»™ táº­p dá»¯ liá»‡u vÃ  sau Ä‘Ã³ chuyá»ƒn giao kiáº¿n trÃºc tÃ¬m kiáº¿m tá»‘t nháº¥t trÃªn táº­p dá»¯ liá»‡u Ä‘áº§y Ä‘á»§. Jeong et al. (2021) nghiÃªn cá»©u má»™t thiáº¿t láº­p tÆ°Æ¡ng tá»± trong lÄ©nh vá»±c thá»‹ giÃ¡c.

NhÃºng Nhiá»‡m vá»¥. CÃ³ nghiÃªn cá»©u trÆ°á»›c Ä‘Ã³ cá»‘ gáº¯ng Ä‘á»‹nh lÆ°á»£ng nhÃºng nhiá»‡m vá»¥ vÃ  sá»± tÆ°Æ¡ng Ä‘á»“ng. TÆ°Æ¡ng tá»± nhÆ° GraphGym, Taskonomy (Zamir et al., 2018) Æ°á»›c tÃ­nh ma tráº­n Ã¡i lá»±c nhiá»‡m vá»¥ báº±ng cÃ¡ch tÃ³m táº¯t cÃ¡c máº¥t mÃ¡t cuá»‘i cÃ¹ng/thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡ sá»­ dá»¥ng má»™t Quy trÃ¬nh Thá»© báº­c PhÃ¢n tÃ­ch (Saaty, 1987). Tá»« má»™t gÃ³c Ä‘á»™ khÃ¡c, Task2Vec (Achille et al., 2019) táº¡o ra nhÃºng nhiá»‡m vá»¥ cho má»™t nhiá»‡m vá»¥ nháº¥t Ä‘á»‹nh sá»­ dá»¥ng Ma tráº­n ThÃ´ng tin Fisher liÃªn quan Ä‘áº¿n má»™t máº¡ng thÄƒm dÃ² Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c. Máº¡ng thÄƒm dÃ³ nÃ y Ä‘Æ°á»£c chia sáº» giá»¯a cÃ¡c nhiá»‡m vá»¥ vÃ  cho phÃ©p Task2Vec Æ°á»›c tÃ­nh Ma tráº­n ThÃ´ng tin Fisher cá»§a cÃ¡c táº­p dá»¯ liá»‡u hÃ¬nh áº£nh khÃ¡c nhau. Le et al. (2022) má»Ÿ rá»™ng má»™t Ã½ tÆ°á»Ÿng tÆ°Æ¡ng tá»± cho tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng neural. CÃ¡c nhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ trÃªn khÃ´ng thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trá»±c tiáº¿p cho GNN vÃ¬ cÃ¡c Ä‘áº§u vÃ o khÃ´ng cÄƒn chá»‰nh giá»¯a cÃ¡c táº­p dá»¯ liá»‡u.

AUTOTRANSFER trÃ¡nh Ä‘Æ°á»£c nÃºt tháº¯t cá»• chai báº±ng cÃ¡ch sá»­ dá»¥ng thá»‘ng kÃª tiá»‡m cáº­n cá»§a Ma tráº­n ThÃ´ng tin Fisher vá»›i trá»ng sá»‘ Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn.

3 CÃ”NG THá»¨C HÃ“A Váº¤N Äá»€ VÃ€ KIáº¾N THá»¨C CÆ  Báº¢N

ChÃºng tÃ´i trÆ°á»›c tiÃªn giá»›i thiá»‡u cÃ¡c Ä‘á»‹nh nghÄ©a chÃ­nh thá»©c cá»§a cÃ¡c cáº¥u trÃºc dá»¯ liá»‡u liÃªn quan Ä‘áº¿n AUTOTRANSFER.

3

--- TRANG 4 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

â€¦ğ‘"!MÃ´ hÃ¬nh ğ‘€1MÃ´ hÃ¬nh ğ‘€2MÃ´ hÃ¬nh ğ‘€ğ‘ˆğ‘… Khá»Ÿi táº¡o Ngáº«uFIMÄo lÆ°á»ng Báº¥t biáº¿n Tá»·lá»‡Äáº·c trÆ°ng Nhiá»‡m vá»¥NhÃºng Nhiá»‡m vá»¥ğ‘”ğ‘“ğ‘“ğ‘“Nhiá»‡m vá»¥ iMá»¥c tiÃªu ÄÃ otáº¡oNhiá»‡m vá»¥ jxNhiá»‡m vá»¥ iNhiá»‡m vá»¥ kxHÃ m máº¥t mÃ¡t Xáº¿p háº¡ng BiÃªnğ‘""ğ‘"#ğ‘§$ğ‘§%ğ‘§%(')ğ‘§%())ğ‘§%(')ğ‘§%(*)

HÃ¬nh 2: Quy trÃ¬nh trÃ­ch xuáº¥t nhÃºng nhiá»‡m vá»¥. TrÃ¡i: Äá»ƒ nhÃºng má»™t nhiá»‡m vá»¥ má»™t cÃ¡ch hiá»‡u quáº£, trÆ°á»›c tiÃªn chÃºng tÃ´i trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ báº±ng cÃ¡ch ná»‘i cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c Ä‘o tá»« R mÃ´ hÃ¬nh neo Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. Sau Ä‘Ã³, chÃºng tÃ´i giá»›i thiá»‡u má»™t hÃ m chiáº¿u g() vá»›i cÃ¡c trá»ng sá»‘ Ä‘Ã£ há»c Ä‘á»ƒ biáº¿n Ä‘á»•i cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ thÃ nh nhÃºng nhiá»‡m vá»¥. Pháº£i: Má»¥c tiÃªu Ä‘Ã o táº¡o Ä‘á»ƒ tá»‘i Æ°u hÃ³a g() vá»›i giÃ¡m sÃ¡t tam phÃ¢n.

Äá»‹nh nghÄ©a 1 (Nhiá»‡m vá»¥) ChÃºng tÃ´i kÃ½ hiá»‡u má»™t nhiá»‡m vá»¥ lÃ  T = (D; L()), bao gá»“m má»™t táº­p dá»¯ liá»‡u D vÃ  má»™t hÃ m máº¥t mÃ¡t L() liÃªn quan Ä‘áº¿n thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡.

Äá»‘i vá»›i má»—i láº§n thá»­ Ä‘Ã o táº¡o trÃªn má»™t nhiá»‡m vá»¥ T(i), chÃºng ta cÃ³ thá»ƒ ghi láº¡i kiáº¿n trÃºc mÃ´ hÃ¬nh Mj, siÃªu tham sá»‘ Hj, vÃ  giÃ¡ trá»‹ máº¥t mÃ¡t tÆ°Æ¡ng á»©ng lj, tá»©c lÃ , (Mj; Hj; lj). ChÃºng tÃ´i Ä‘á» xuáº¥t duy trÃ¬ má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘á»ƒ há»— trá»£ chuyá»ƒn giao kiáº¿n thá»©c cho cÃ¡c nhiá»‡m vá»¥ má»›i trong tÆ°Æ¡ng lai.

Äá»‹nh nghÄ©a 2 (NgÃ¢n hÃ ng Nhiá»‡m vá»¥-MÃ´ hÃ¬nh) Má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t táº­p há»£p cÃ¡c nhiá»‡m vá»¥, má»—i nhiá»‡m vá»¥ vá»›i nhiá»u láº§n thá»­ Ä‘Ã o táº¡o, dÆ°á»›i dáº¡ng B = {(T(i); {(M(i)j; H(i)j; l(i)j)})}.

AutoML vá»›i Chuyá»ƒn giao Kiáº¿n thá»©c. Giáº£ sá»­ chÃºng ta cÃ³ má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B. Cho má»™t nhiá»‡m vá»¥ má»›i T(n) chÆ°a Ä‘Æ°á»£c tháº¥y trÆ°á»›c Ä‘Ã³, má»¥c tiÃªu cá»§a chÃºng ta lÃ  nhanh chÃ³ng tÃ¬m ra má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t há»£p lÃ½ trÃªn nhiá»‡m vá»¥ má»›i báº±ng cÃ¡ch sá»­ dá»¥ng kiáº¿n thá»©c tá»« ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i táº­p trung vÃ o AutoML cho cÃ¡c nhiá»‡m vá»¥ há»c Ä‘á»“ thá»‹, máº·c dÃ¹ ká»¹ thuáº­t phÃ¡t triá»ƒn cá»§a chÃºng tÃ´i lÃ  tá»•ng quÃ¡t vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c lÄ©nh vá»±c khÃ¡c. ChÃºng tÃ´i Ä‘á»‹nh nghÄ©a Ä‘á»“ thá»‹ Ä‘áº§u vÃ o lÃ  G = {V; E}, trong Ä‘Ã³ V lÃ  táº­p há»£p nÃºt vÃ  E âŠ† V Ã— V lÃ  táº­p há»£p cáº¡nh. HÆ¡n ná»¯a, gá»i y lÃ  nhÃ£n Ä‘áº§u ra cá»§a nÃ³, cÃ³ thá»ƒ lÃ  cáº¥p nÃºt, cáº¥p cáº¡nh hoáº·c cáº¥p Ä‘á»“ thá»‹. Má»™t GNN Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi trá»ng sá»‘ Î¸ xuáº¥t ra má»™t phÃ¢n phá»‘i háº­u nghiá»‡m P(G; y; Î¸) cho dá»± Ä‘oÃ¡n nhÃ£n.

4 GIáº¢I PHÃP Äá»€ XUáº¤T: AUTOTRANSFER

Trong pháº§n nÃ y, chÃºng tÃ´i giá»›i thiá»‡u giáº£i phÃ¡p AUTOTRANSFER Ä‘Æ°á»£c Ä‘á» xuáº¥t. AUTOTRANSFER sá»­ dá»¥ng khÃ´ng gian nhÃºng nhiá»‡m vá»¥ nhÆ° má»™t cÃ´ng cá»¥ Ä‘á»ƒ hiá»ƒu sá»± liÃªn quan cá»§a cÃ¡c thiáº¿t káº¿ kiáº¿n trÃºc trÆ°á»›c Ä‘Ã³ Ä‘á»‘i vá»›i nhiá»‡m vá»¥ má»¥c tiÃªu. NhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c thiáº¿t káº¿ thu tháº­p cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c kiáº¿n trÃºc khÃ¡c nhau trÃªn cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau trong khi cÅ©ng hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n. ChÃºng tÃ´i trÆ°á»›c tiÃªn giá»›i thiá»‡u má»™t giáº£i phÃ¡p cÃ³ Ä‘á»™ng lá»±c lÃ½ thuyáº¿t Ä‘á»ƒ trÃ­ch xuáº¥t má»™t biá»ƒu diá»…n hiá»‡u suáº¥t báº¥t biáº¿n tá»· lá»‡ cá»§a má»—i cáº·p nhiá»‡m vá»¥-mÃ´ hÃ¬nh. ChÃºng tÃ´i sá»­ dá»¥ng nhá»¯ng biá»ƒu diá»…n nÃ y Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ vÃ  tiáº¿p tá»¥c há»c nhÃºng nhiá»‡m vá»¥. Nhá»¯ng nhÃºng nÃ y táº¡o thÃ nh khÃ´ng gian nhÃºng nhiá»‡m vá»¥ mÃ  cuá»‘i cÃ¹ng chÃºng tÃ´i sá»­ dá»¥ng trong quÃ¡ trÃ¬nh tÃ¬m kiáº¿m AutoML.

4.1 CÆ  Báº¢N Vá»€ MA TRáº¬N THÃ”NG TIN FISHER (FIM)

Cho má»™t GNN Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a á»Ÿ trÃªn, Ma tráº­n ThÃ´ng tin Fisher (FIM) F cá»§a nÃ³ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ 

F = E[G;y][âˆ‡Î¸ log P(G; y; Î¸)âˆ‡Î¸ log P(G; y; Î¸)>]:

vá» máº·t hÃ¬nh thá»©c lÃ  hiá»‡p phÆ°Æ¡ng sai ká»³ vá»ng cá»§a cÃ¡c Ä‘iá»ƒm sá»‘ Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ mÃ´ hÃ¬nh. CÃ³ hai quan Ä‘iá»ƒm hÃ¬nh há»c phá»• biáº¿n cho FIM. Thá»© nháº¥t, FIM lÃ  má»™t cáº­n trÃªn cá»§a Hessian vÃ  trÃ¹ng vá»›i Hessian náº¿u gradient báº±ng 0. Do Ä‘Ã³, FIM Ä‘áº·c trÆ°ng cho cáº£nh quan Ä‘á»‹a phÆ°Æ¡ng cá»§a hÃ m máº¥t mÃ¡t gáº§n minimum toÃ n cá»¥c. Thá»© hai, tÆ°Æ¡ng tá»± nhÆ° Hessian, FIM mÃ´ hÃ¬nh cáº£nh quan máº¥t mÃ¡t khÃ´ng pháº£i Ä‘á»‘i vá»›i khÃ´ng gian Ä‘áº§u vÃ o, mÃ  Ä‘á»‘i vá»›i khÃ´ng gian tham sá»‘. Trong quan Ä‘iá»ƒm hÃ¬nh há»c thÃ´ng tin, náº¿u chÃºng ta thÃªm má»™t nhiá»…u nhá» vÃ o khÃ´ng gian tham sá»‘, chÃºng ta cÃ³

KL(P(G; y; Î¸)||P(G; y; Î¸ + dÎ¸)) = dÎ¸>FdÎ¸:

trong Ä‘Ã³ KL(Â·; Â·) lÃ  phÃ¢n ká»³ Kullbackâ€“Leibler. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  khÃ´ng gian tham sá»‘ cá»§a má»™t mÃ´ hÃ¬nh táº¡o thÃ nh má»™t Ä‘a táº¡p Riemannian vÃ  FIM hoáº¡t Ä‘á»™ng nhÆ° metric Riemannian cá»§a nÃ³. Do Ä‘Ã³ FIM cho phÃ©p chÃºng ta Ä‘á»‹nh lÆ°á»£ng táº§m quan trá»ng cá»§a trá»ng sá»‘ mÃ´ hÃ¬nh theo cÃ¡ch cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c kiáº¿n trÃºc khÃ¡c nhau.

4.2 CÃC Äáº¶C TRÆ¯NG NHIá»†M Vá»¤ Dá»°A TRÃŠN FIM

Biá»ƒu diá»…n Báº¥t biáº¿n Tá»· lá»‡ cá»§a Cáº·p Nhiá»‡m vá»¥-MÃ´ hÃ¬nh. ChÃºng tÃ´i nháº±m tÃ¬m ra má»™t biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡ cho má»—i cáº·p nhiá»‡m vá»¥-mÃ´ hÃ¬nh sáº½ táº¡o thÃ nh cÆ¡ sá»Ÿ Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. ThÃ¡ch thá»©c chÃ­nh trong viá»‡c sá»­ dá»¥ng FIM Ä‘á»ƒ biá»ƒu diá»…n hiá»‡u suáº¥t GNN lÃ  cÃ¡c táº­p dá»¯ liá»‡u Ä‘á»“ thá»‹ khÃ´ng cÃ³ cáº¥u trÃºc Ä‘áº§u vÃ o phá»• quÃ¡t, cá»‘ Ä‘á»‹nh, vÃ¬ váº­y khÃ´ng kháº£ thi Ä‘á»ƒ tÃ¬m má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c duy nháº¥t vÃ  trÃ­ch xuáº¥t FIM cá»§a nÃ³. Tuy nhiÃªn, viá»‡c Ä‘Ã o táº¡o nhiá»u máº¡ng gÃ¢y ra váº¥n Ä‘á» vÃ¬ cÃ¡c FIM Ä‘Æ°á»£c tÃ­nh toÃ¡n cho cÃ¡c máº¡ng khÃ¡c nhau khÃ´ng thá»ƒ so sÃ¡nh trá»±c tiáº¿p. ChÃºng tÃ´i chá»n sá»­ dá»¥ng nhiá»u máº¡ng nhÆ°ng bá»• sung Ä‘á» xuáº¥t sá»­ dá»¥ng thá»‘ng kÃª tiá»‡m cáº­n cá»§a FIM liÃªn quan Ä‘áº¿n trá»ng sá»‘ Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. LÃ½ do lÃ½ thuyáº¿t cho má»‘i quan há»‡ giá»¯a thá»‘ng kÃª tiá»‡m cáº­n cá»§a FIM vÃ  kháº£ nÄƒng Ä‘Ã o táº¡o cá»§a máº¡ng neural Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u trong (Karakida et al., 2019; Pennington and Worah, 2018) mÃ  chÃºng tÃ´i giá»›i thiá»‡u Ä‘áº¿n Ä‘á»™c giáº£. ChÃºng tÃ´i giáº£ Ä‘á»‹nh ráº±ng thÆ°á»›c Ä‘o kháº£ nÄƒng Ä‘Ã o táº¡o nhÆ° váº­y mÃ£ hÃ³a cáº£nh quan máº¥t mÃ¡t vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a vÃ  do Ä‘Ã³ tÆ°Æ¡ng quan vá»›i hiá»‡u suáº¥t mÃ´ hÃ¬nh cuá»‘i cÃ¹ng trÃªn nhiá»‡m vá»¥.

Má»™t váº¥n Ä‘á» khÃ¡c liÃªn quan Ä‘áº¿n cáº¥u trÃºc Ä‘áº§u vÃ o cá»§a cÃ¡c táº­p dá»¯ liá»‡u Ä‘á»“ thá»‹ lÃ  cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau cÃ³ sá»‘ lÆ°á»£ng tham sá»‘ khÃ¡c nhau. Máº·c dÃ¹ cÃ³ má»™t sá»‘ kiáº¿n trÃºc Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t, vÃ­ dá»¥: (Lee et al., 2019; Ma et al., 2019), háº§u háº¿t thiáº¿t káº¿ kiáº¿n trÃºc GNN cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t chuá»—i cÃ¡c lá»›p tiá»n xá»­ lÃ½, lá»›p truyá»n tin vÃ  lá»›p háº­u xá»­ lÃ½. CÃ¡c lá»›p tiá»n xá»­ lÃ½ vÃ  háº­u xá»­ lÃ½ lÃ  cÃ¡c lá»›p Perceptron Äa lá»›p (MLP), cÃ³ kÃ­ch thÆ°á»›c thay Ä‘á»•i giá»¯a cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau do cáº¥u trÃºc Ä‘áº§u vÃ o/Ä‘áº§u ra khÃ¡c nhau. CÃ¡c lá»›p truyá»n tin thÆ°á»ng Ä‘Æ°á»£c coi lÃ  thiáº¿t káº¿ chÃ­nh cho GNN vÃ  sá»‘ lÆ°á»£ng tham sá»‘ trá»ng sá»‘ cÃ³ thá»ƒ giá»¯ nguyÃªn giá»¯a cÃ¡c nhiá»‡m vá»¥. Theo quan Ä‘iá»ƒm nÃ y, chÃºng tÃ´i chá»‰ xem xÃ©t FIM Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ trong cÃ¡c lá»›p truyá»n tin Ä‘á»ƒ sá»‘ lÆ°á»£ng tham sá»‘ Ä‘Æ°á»£c xem xÃ©t giá»¯ nguyÃªn cho táº¥t cáº£ cÃ¡c táº­p dá»¯ liá»‡u. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng cÃ´ng thá»©c hÃ³a nhÆ° váº­y cÃ³ cÃ¡c háº¡n cháº¿, theo nghÄ©a lÃ  nÃ³ khÃ´ng thá»ƒ bao gá»“m táº¥t cáº£ cÃ¡c thiáº¿t káº¿ GNN trong tÃ i liá»‡u. ChÃºng tÃ´i Ä‘á»ƒ láº¡i cÃ¡c má»Ÿ rá»™ng tiá»m nÄƒng vá»›i pháº¡m vi bao phá»§ tá»‘t hÆ¡n cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.

ChÃºng tÃ´i tiáº¿p tá»¥c xáº¥p xá»‰ FIM báº±ng cÃ¡ch chá»‰ xem xÃ©t cÃ¡c má»¥c Ä‘Æ°á»ng chÃ©o, Ä‘iá»u nÃ y ngáº§m bá» qua cÃ¡c tÆ°Æ¡ng quan giá»¯a cÃ¡c tham sá»‘. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng Ä‘Ã¢y lÃ  thá»±c hÃ nh phá»• biáº¿n khi phÃ¢n tÃ­ch FIM cá»§a máº¡ng neural sÃ¢u, vÃ¬ FIM Ä‘áº§y Ä‘á»§ lÃ  khá»•ng lá»“ (báº­c hai theo sá»‘ lÆ°á»£ng tham sá»‘) vÃ  khÃ´ng kháº£ thi Ä‘á»ƒ tÃ­nh toÃ¡n ngay cáº£ trÃªn pháº§n cá»©ng hiá»‡n Ä‘áº¡i. TÆ°Æ¡ng tá»± nhÆ° Pennington and Worah (2018), chÃºng tÃ´i xem xÃ©t hai moment Ä‘áº§u tiÃªn cá»§a FIM

m1 = (1/n)tr[F] vÃ  m2 = (1/n)tr[FÂ²] (1)

vÃ  sá»­ dá»¥ng Îº = m2/m1Â² lÃ m biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡. Îº Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘Æ°á»£c giá»›i háº¡n dÆ°á»›i bá»Ÿi 1 vÃ  thu tháº­p má»©c Ä‘á»™ táº­p trung cá»§a phá»•. Má»™t Îº nhá» chá»‰ ra cáº£nh quan máº¥t mÃ¡t pháº³ng, vÃ  thiáº¿t káº¿ mÃ´ hÃ¬nh tÆ°Æ¡ng á»©ng cá»§a nÃ³ táº­n hÆ°á»Ÿng tá»‘i Æ°u hÃ³a báº­c nháº¥t nhanh vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n tiá»m nÄƒng.

Äá»ƒ mÃ£ hÃ³a thÃ´ng tin khÃ´ng gian nhÃ£n vÃ o má»—i nhiá»‡m vá»¥, chÃºng tÃ´i Ä‘á» xuáº¥t chá»‰ Ä‘Ã o táº¡o lá»›p tuyáº¿n tÃ­nh cuá»‘i cÃ¹ng cá»§a má»—i mÃ´ hÃ¬nh trÃªn má»™t nhiá»‡m vá»¥ nháº¥t Ä‘á»‹nh, Ä‘iá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n hiá»‡u quáº£. CÃ¡c tham sá»‘ trong cÃ¡c lá»›p khÃ¡c Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng sau khi Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. ChÃºng tÃ´i láº¥y trung bÃ¬nh trÃªn R láº§n khá»Ÿi táº¡o Ä‘á»ƒ Æ°á»›c tÃ­nh Îº trung bÃ¬nh.

XÃ¢y dá»±ng Äáº·c trÆ°ng Nhiá»‡m vá»¥. ChÃºng tÃ´i kÃ½ hiá»‡u cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ lÃ  cÃ¡c thÆ°á»›c Ä‘o Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« má»—i nhiá»‡m vá»¥ Ä‘áº·c trÆ°ng cho cÃ¡c Ä‘áº·c Ä‘iá»ƒm quan trá»ng cá»§a nÃ³. Thiáº¿t káº¿ cá»§a cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ nÃªn pháº£n Ã¡nh má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a chÃºng ta: sá»­ dá»¥ng nhá»¯ng Ä‘áº·c trÆ°ng nÃ y Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± vÃ  chuyá»ƒn giao cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ tá»‘t nháº¥t. Do Ä‘Ã³, chÃºng tÃ´i chá»n U thiáº¿t káº¿ mÃ´ hÃ¬nh lÃ m mÃ´ hÃ¬nh neo vÃ  ná»‘i cÃ¡c biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡ au cá»§a má»—i thiáº¿t káº¿ lÃ m Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. Äá»ƒ chá»‰ giá»¯ láº¡i xáº¿p háº¡ng tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh neo, chÃºng tÃ´i chuáº©n hÃ³a vector Ä‘áº·c trÆ°ng Ä‘Æ°á»£c ná»‘i thÃ nh tá»· lá»‡ 1. ChÃºng tÃ´i gá»i zf lÃ  Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c chuáº©n hÃ³a.

4.3 Tá»ª Äáº¶C TRÆ¯NG NHIá»†M Vá»¤ Äáº¾N NHÃšNG NHIá»†M Vá»¤

Äáº·c trÆ°ng nhiá»‡m vá»¥ zf Ä‘Æ°á»£c giá»›i thiá»‡u á»Ÿ trÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t phÆ°Æ¡ng tiá»‡n ká»¹ thuáº­t Ä‘áº·c trÆ°ng. ChÃºng tÃ´i xÃ¢y dá»±ng vector Ä‘áº·c trÆ°ng vá»›i kiáº¿n thá»©c lÄ©nh vá»±c, nhÆ°ng khÃ´ng cÃ³ Ä‘áº£m báº£o nÃ o ráº±ng nÃ³ hoáº¡t Ä‘á»™ng nhÆ° dá»± kiáº¿n. Do Ä‘Ã³ chÃºng tÃ´i Ä‘á» xuáº¥t há»c má»™t hÃ m chiáº¿u g() : RU â†’ RD Ã¡nh xáº¡ Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ zf Ä‘áº¿n nhÃºng nhiá»‡m vá»¥ cuá»‘i cÃ¹ng ze = g(zf). ChÃºng tÃ´i khÃ´ng cÃ³ báº¥t ká»³ giÃ¡m sÃ¡t theo Ä‘iá»ƒm nÃ o cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m má»¥c tiÃªu Ä‘Ã o táº¡o. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i xem xÃ©t khÃ´ng gian metric Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi GraphGym. HÃ m khoáº£ng cÃ¡ch trong GraphGym - Ä‘Æ°á»£c tÃ­nh toÃ¡n báº±ng tÆ°Æ¡ng quan thá»© háº¡ng Kendall giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh neo Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn hai nhiá»‡m vá»¥ Ä‘Æ°á»£c so sÃ¡nh - tÆ°Æ¡ng quan tá»‘t vá»›i má»¥c tiÃªu chuyá»ƒn giao kiáº¿n thá»©c mong muá»‘n cá»§a chÃºng tÃ´i.

5

--- TRANG 6 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

KhÃ´ng cÃ³ Ã½ nghÄ©a gÃ¬ khi Ã©p buá»™c ráº±ng nhÃºng nhiá»‡m vá»¥ báº¯t chÆ°á»›c khÃ´ng gian metric chÃ­nh xÃ¡c cá»§a GraphGym, vÃ¬ khÃ´ng gian metric cá»§a GraphGym váº«n cÃ³ thá»ƒ chá»©a nhiá»…u, hoáº·c khÃ´ng hoÃ n toÃ n phÃ¹ há»£p vá»›i má»¥c tiÃªu chuyá»ƒn giao. ChÃºng tÃ´i xem xÃ©t má»™t hÃ m máº¥t mÃ¡t thay tháº¿ chá»‰ thá»±c thi thá»© tá»± xáº¿p háº¡ng giá»¯a cÃ¡c nhiá»‡m vá»¥. Äá»ƒ minh há»a, hÃ£y xem xÃ©t cÃ¡c nhiá»‡m vá»¥ T(i), T(j), T(k) vÃ  cÃ¡c nhÃºng nhiá»‡m vá»¥ tÆ°Æ¡ng á»©ng cá»§a chÃºng, z(i)e, z(j)e, z(k)e. LÆ°u Ã½ ráº±ng ze Ä‘Æ°á»£c chuáº©n hÃ³a thÃ nh 1 nÃªn z(i)e>z(j)e Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c nhiá»‡m vá»¥ T(i) vÃ  T(j).

Gá»i dg(Â·; Â·) lÃ  khoáº£ng cÃ¡ch Ä‘Æ°á»£c Æ°á»›c tÃ­nh bá»Ÿi GraphGym. ChÃºng tÃ´i muá»‘n thá»±c thi

z(i)e>z(j)e > z(i)e>z(k)e náº¿u dg(T(i); T(j)) < dg(T(i); T(k)):

Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng hÃ m máº¥t mÃ¡t xáº¿p háº¡ng biÃªn lÃ m hÃ m má»¥c tiÃªu giÃ¡m sÃ¡t thay tháº¿ cá»§a chÃºng tÃ´i:

Lr(z(i)e; z(j)e; z(k)e; y) = max(0; y(z(i)e>z(j)e âˆ’ z(i)e>z(k)e) + margin): (2)

á» Ä‘Ã¢y náº¿u dg(T(i); T(j)) < dg(T(i); T(k)), thÃ¬ chÃºng ta cÃ³ nhÃ£n tÆ°Æ¡ng á»©ng y = 1, vÃ  y = âˆ’1 ngÆ°á»£c láº¡i. KhÃ´ng gian nhÃºng nhiá»‡m vá»¥ cuá»‘i cÃ¹ng cá»§a chÃºng tÃ´i sau Ä‘Ã³ lÃ  má»™t khÃ´ng gian metric dá»±a trÃªn FIM vá»›i hÃ m khoáº£ng cÃ¡ch cosine, trong Ä‘Ã³ khoáº£ng cÃ¡ch Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  de(T(i); T(j)) = 1 âˆ’ z(i)e>z(j)e. Vui lÃ²ng tham kháº£o quy trÃ¬nh Ä‘Ã o táº¡o chi tiáº¿t táº¡i Thuáº­t toÃ¡n 2 trong Phá»¥ lá»¥c.

4.4 THUáº¬T TOÃN TÃŒM KIáº¾M AUTOML Vá»šI NHÃšNG NHIá»†M Vá»¤

Äá»ƒ chuyá»ƒn giao kiáº¿n thá»©c cho má»™t nhiá»‡m vá»¥ má»›i, má»™t Ã½ tÆ°á»Ÿng ngÃ¢y thÆ¡ sáº½ lÃ  trá»±c tiáº¿p mang theo cáº¥u hÃ¬nh mÃ´ hÃ¬nh tá»‘t nháº¥t tá»« nhiá»‡m vá»¥ gáº§n nháº¥t trong ngÃ¢n hÃ ng. Tuy nhiÃªn, ngay cáº£ tÆ°Æ¡ng quan thá»© háº¡ng Kendall cao giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t mÃ´ hÃ¬nh cá»§a hai nhiá»‡m vá»¥ T(i), T(j) khÃ´ng Ä‘áº£m báº£o cáº¥u hÃ¬nh mÃ´ hÃ¬nh tá»‘t nháº¥t trong nhiá»‡m vá»¥ T(i) cÅ©ng sáº½ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t trÃªn nhiá»‡m vá»¥ T(j). NgoÃ i ra, vÃ¬ sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ cÃ³ thá»ƒ cÃ³ nhiá»…u, giáº£i phÃ¡p ngÃ¢y thÆ¡ nÃ y cÃ³ thá»ƒ gáº·p khÃ³ khÄƒn khi tá»“n táº¡i nhiá»u nhiá»‡m vá»¥ tham chiáº¿u Ä‘á»u cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao.

Äá»ƒ lÃ m cho viá»‡c chuyá»ƒn giao kiáº¿n thá»©c máº¡nh máº½ hÆ¡n Ä‘á»‘i vá»›i cÃ¡c trÆ°á»ng há»£p tháº¥t báº¡i nhÆ° váº­y, chÃºng tÃ´i giá»›i thiá»‡u khÃ¡i niá»‡m phÃ¢n phá»‘i thiáº¿t káº¿ phá»¥ thuá»™c vÃ o cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t vÃ  Ä‘á» xuáº¥t chuyá»ƒn giao phÃ¢n phá»‘i thiáº¿t káº¿ thay vÃ¬ cáº¥u hÃ¬nh thiáº¿t káº¿ tá»‘t nháº¥t. ChÃ­nh thá»©c, xem xÃ©t má»™t nhiá»‡m vá»¥ T(i) trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B, liÃªn quan Ä‘áº¿n cÃ¡c thá»­ nghiá»‡m cá»§a nÃ³ {(M(i)j; H(i)j; l(i)j)}. ChÃºng ta cÃ³ thá»ƒ tÃ³m táº¯t cÃ¡c thiáº¿t káº¿ cá»§a nÃ³ dÆ°á»›i dáº¡ng danh sÃ¡ch cÃ¡c cáº¥u hÃ¬nh C = {c1; :::; cW}, sao cho táº¥t cáº£ cÃ¡c káº¿t há»£p tiá»m nÄƒng cá»§a kiáº¿n trÃºc mÃ´ hÃ¬nh M vÃ  siÃªu tham sá»‘ H náº±m dÆ°á»›i tÃ­ch Cartesian cá»§a cÃ¡c cáº¥u hÃ¬nh. VÃ­ dá»¥, c1 cÃ³ thá»ƒ lÃ  viá»‡c khá»Ÿi táº¡o cÃ¡c lá»›p tá»•ng há»£p, vÃ  c2 cÃ³ thá»ƒ lÃ  tá»‘c Ä‘á»™ há»c ban Ä‘áº§u. Sau Ä‘Ã³ chÃºng tÃ´i Ä‘á»‹nh nghÄ©a phÃ¢n phá»‘i thiáº¿t káº¿ lÃ  cÃ¡c biáº¿n ngáº«u nhiÃªn c1; c2; :::; cW, má»—i biáº¿n tÆ°Æ¡ng á»©ng vá»›i má»™t siÃªu tham sá»‘. Má»—i biáº¿n ngáº«u nhiÃªn c Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  phÃ¢n phá»‘i táº§n sá»‘ cá»§a cÃ¡c lá»±a chá»n thiáº¿t káº¿ Ä‘Æ°á»£c sá»­ dá»¥ng trong K thá»­ nghiá»‡m hÃ ng Ä‘áº§u. ChÃºng tÃ´i nhÃ¢n táº¥t cáº£ cÃ¡c phÃ¢n phá»‘i cho cÃ¡c cáº¥u hÃ¬nh cÃ¡ nhÃ¢n {c1; :::; cW} Ä‘á»ƒ xáº¥p xá»‰ phÃ¢n phá»‘i thiáº¿t káº¿ tá»•ng thá»ƒ cá»§a nhiá»‡m vá»¥ P(C|T(i)) = âˆw P(cw|T(i)):

Trong quÃ¡ trÃ¬nh suy luáº­n, cho má»™t nhiá»‡m vá»¥ má»›i T(n), chÃºng tÃ´i chá»n má»™t táº­p con nhiá»‡m vá»¥ gáº§n S báº±ng cÃ¡ch Ä‘áº·t ngÆ°á»¡ng khoáº£ng cÃ¡ch nhÃºng nhiá»‡m vá»¥, tá»©c lÃ , S = {T(i)|de(T(n); T(i)) â‰¤ dthres}. Sau Ä‘Ã³ chÃºng tÃ´i suy ra tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao Pt(C|T(n)) cá»§a nhiá»‡m vá»¥ má»›i báº±ng cÃ¡ch cÃ¢n nháº¯c cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ tá»« táº­p con nhiá»‡m vá»¥ gáº§n S.

Pt(C|T(n)) = (âˆ‘T(i)âˆˆS (1/de(T(n); T(i)))P(C|T(i)))/(âˆ‘T(i)âˆˆS (1/de(T(n); T(i)))): (3)

TiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c suy luáº­n cho nhiá»‡m vá»¥ má»›i sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hÆ°á»›ng dáº«n cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m khÃ¡c nhau. Lá»±a chá»n tá»± nhiÃªn nháº¥t cho cháº¿ Ä‘á»™ vÃ i thá»­ nghiá»‡m lÃ  tÃ¬m kiáº¿m ngáº«u nhiÃªn. Thay vÃ¬ láº¥y máº«u má»—i cáº¥u hÃ¬nh thiáº¿t káº¿ theo phÃ¢n phá»‘i Ä‘á»“ng nháº¥t, chÃºng tÃ´i Ä‘á» xuáº¥t láº¥y máº«u tá»« tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng bÃ¡o nhiá»‡m vá»¥ Pt(C|T(n)). Vui lÃ²ng tham kháº£o Phá»¥ lá»¥c A Ä‘á»ƒ kiá»ƒm tra cÃ¡ch chÃºng tÃ´i tÄƒng cÆ°á»ng cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m khÃ¡c.

Äá»‘i vá»›i AUTOTRANSFER, chÃºng ta cÃ³ thá»ƒ tiá»n xá»­ lÃ½ ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B thÃ nh Bp = {(D(i); L(i)()); z(i)e; P(C|T(i))} vÃ¬ quy trÃ¬nh cá»§a chÃºng ta chá»‰ Ä‘Ã²i há»i sá»­ dá»¥ng nhÃºng nhiá»‡m vá»¥ z(i)e vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ P(C|T(i)) thay vÃ¬ cÃ¡c thá»­ nghiá»‡m Ä‘Ã o táº¡o chi tiáº¿t. Quy trÃ¬nh tÃ¬m kiáº¿m chi tiáº¿t Ä‘Æ°á»£c tÃ³m táº¯t trong Thuáº­t toÃ¡n 1.

6

--- TRANG 7 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Thuáº­t toÃ¡n 1 TÃ³m táº¯t quy trÃ¬nh tÃ¬m kiáº¿m AUTOTRANSFER
YÃªu cáº§u: Má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘Ã£ xá»­ lÃ½ Bp = {(D(i); L(i)()); z(i)e; P(C|T(i)))}, má»™t nhiá»‡m vá»¥ má»›i T(n) = (D(n); L(n)()), U mÃ´ hÃ¬nh neo M1; :::; MU, R chá»‰ Ä‘á»‹nh sá»‘ láº§n láº·p láº¡i.
1: for u = 1 to U do
2:   for r = 1 to R do
3:     Khá»Ÿi táº¡o trá»ng sá»‘ cho mÃ´ hÃ¬nh neo Mu ngáº«u nhiÃªn
4:     Æ¯á»›c tÃ­nh FIM F â‰ˆ ED[âˆ‡Î¸ log P(Mu; y; Î¸)âˆ‡Î¸ log P(Mu; y; Î¸)>]
5:     TrÃ­ch xuáº¥t biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡ a(v)u â† m2/m21 theo PhÆ°Æ¡ng trÃ¬nh 1
6:   end for
7:   au â† mean (a(1)u; a(2)u; :::; a(V)u)
8: end for
9: z(n)f â† concat (a1; a2; :::; aU)
10: z(n)e â† g(z(n)f)
11: Chá»n táº­p con nhiá»‡m vá»¥ gáº§n S â† {T(i)|1 âˆ’ z(n)e>z(i)e â‰¤ dthres}
12: Nháº­n tiÃªn nghiá»‡m thiáº¿t káº¿ Pt(C|T(n)) báº±ng cÃ¡ch tá»•ng há»£p táº­p con S theo PhÆ°Æ¡ng trÃ¬nh 3
13: Báº¯t Ä‘áº§u thuáº­t toÃ¡n tÃ¬m kiáº¿m HPO vá»›i tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng bÃ¡o nhiá»‡m vá»¥ Pt(C|T(n))

5 CÃC THÃ NGHIá»†M

5.1 THIáº¾T Láº¬P THÃ NGHIá»†M

NgÃ¢n hÃ ng Nhiá»‡m vá»¥-MÃ´ hÃ¬nh: GNN-BANK-101.
Äá»ƒ há»— trá»£ nghiÃªn cá»©u AutoML vá»›i chuyá»ƒn giao kiáº¿n thá»©c, chÃºng tÃ´i Ä‘Ã£ thu tháº­p GNN-BANK-101 lÃ m cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»“ thá»‹ quy mÃ´ lá»›n Ä‘áº§u tiÃªn ghi láº¡i cÃ¡c cáº¥u hÃ¬nh thiáº¿t káº¿ cÃ³ thá»ƒ tÃ¡i táº¡o vÃ  hiá»‡u suáº¥t Ä‘Ã o táº¡o chi tiáº¿t trÃªn nhiá»u nhiá»‡m vá»¥ khÃ¡c nhau. Cá»¥ thá»ƒ, GNN-BANK-101 hiá»‡n táº¡i bao gá»“m sÃ¡u nhiá»‡m vá»¥ cho phÃ¢n loáº¡i nÃºt (AmazonComputers (Shchur et al., 2018), AmazonPhoto (Shchur et al., 2018), CiteSeer (Yang et al., 2016), CoauthorCS (Shchur et al., 2018), CoauthorPhysics (Shchur et al., 2018), Cora (Yang et al., 2016)) vÃ  sÃ¡u nhiá»‡m vá»¥ cho phÃ¢n loáº¡i Ä‘á»“ thá»‹ (PROTEINS (Ivanov et al., 2019), BZR (Ivanov et al., 2019), COX2 (Ivanov et al., 2019), DD (Ivanov et al., 2019), ENZYMES (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019)). KhÃ´ng gian thiáº¿t káº¿ cá»§a chÃºng tÃ´i tuÃ¢n theo (You et al., 2020), vÃ  chÃºng tÃ´i má»Ÿ rá»™ng khÃ´ng gian thiáº¿t káº¿ Ä‘á»ƒ bao gá»“m cÃ¡c lá»›p tÃ­ch cháº­p Ä‘á»“ thá»‹ vÃ  kÃ­ch hoáº¡t thÆ°á»ng Ä‘Æ°á»£c Ã¡p dá»¥ng khÃ¡c nhau. ChÃºng tÃ´i cháº¡y rá»™ng rÃ£i 10.000 mÃ´ hÃ¬nh khÃ¡c nhau cho má»—i nhiá»‡m vá»¥, dáº«n Ä‘áº¿n tá»•ng cá»™ng 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh, vÃ  ghi láº¡i táº¥t cáº£ thÃ´ng tin Ä‘Ã o táº¡o bao gá»“m máº¥t mÃ¡t train/val/test.

Táº­p dá»¯ liá»‡u Benchmark. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER trÃªn sÃ¡u táº­p dá»¯ liá»‡u khÃ¡c nhau theo cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã³ (Qin et al., 2021). CÃ¡c táº­p dá»¯ liá»‡u cá»§a chÃºng tÃ´i bao gá»“m ba táº­p dá»¯ liá»‡u phÃ¢n loáº¡i nÃºt tiÃªu chuáº©n (CoauthorPhysics (Shchur et al., 2018), CoraFull (Bojchevski and GÃ¼nnemann, 2017) vÃ  OGB-Arxiv (Hu et al., 2020)), cÅ©ng nhÆ° ba táº­p dá»¯ liá»‡u benchmark phÃ¢n loáº¡i Ä‘á»“ thá»‹ tiÃªu chuáº©n, (COX2 (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019) vÃ  PROTEINS (Ivanov et al., 2019)). CoauthorPhysics vÃ  CoraFull lÃ  cÃ¡c táº­p dá»¯ liá»‡u phÃ¢n loáº¡i nÃºt transductive, vÃ¬ váº­y chÃºng tÃ´i gÃ¡n ngáº«u nhiÃªn cÃ¡c nÃºt vÃ o cÃ¡c táº­p train/valid/test theo tá»· lá»‡ 50%:25%:25% (Qin et al., 2021). ChÃºng tÃ´i chia ngáº«u nhiÃªn cÃ¡c Ä‘á»“ thá»‹ theo tá»· lá»‡ 80%:10%:10% cho ba táº­p dá»¯ liá»‡u phÃ¢n loáº¡i Ä‘á»“ thá»‹ (Qin et al., 2021). ChÃºng tÃ´i tuÃ¢n theo viá»‡c chia train/valid/test máº·c Ä‘á»‹nh cho táº­p dá»¯ liá»‡u OGB-Arxiv (Hu et al., 2020). Äá»ƒ Ä‘áº£m báº£o khÃ´ng cÃ³ rÃ² rá»‰ thÃ´ng tin, chÃºng tÃ´i táº¡m thá»i loáº¡i bá» táº¥t cáº£ cÃ¡c báº£n ghi liÃªn quan Ä‘áº¿n nhiá»‡m vá»¥ tá»« ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i náº¿u táº­p dá»¯ liá»‡u chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c thu tháº­p trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh.

Baseline. ChÃºng tÃ´i so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tiÃªn tiáº¿n cho AutoML GNN. ChÃºng tÃ´i sá»­ dá»¥ng GCN vÃ  GAT vá»›i kiáº¿n trÃºc máº·c Ä‘á»‹nh theo triá»ƒn khai gá»‘c cá»§a chÃºng lÃ m baseline. Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p NAS Ä‘a thá»­ nghiá»‡m, chÃºng tÃ´i xem xÃ©t GraphNAS (Gao et al., 2020). Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p NAS má»™t láº§n, chÃºng tÃ´i bao gá»“m DARTS (Liu et al., 2018) vÃ  GASSO (Qin et al., 2021). GASSO Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c thiáº¿t láº­p transductive, vÃ¬ váº­y chÃºng tÃ´i bá» qua nÃ³ cho cÃ¡c benchmark phÃ¢n loáº¡i Ä‘á»“ thá»‹. ChÃºng tÃ´i tiáº¿p tá»¥c cung cáº¥p káº¿t quáº£ cá»§a cÃ¡c thuáº­t toÃ¡n HPO dá»±a trÃªn khÃ´ng gian tÃ¬m kiáº¿m Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i lÃ m baseline: Random, Evolution, TPE (Bergstra et al., 2011) vÃ  HyperBand (Li et al., 2017).

ChÃºng tÃ´i máº·c Ä‘á»‹nh cho phÃ©p tÃ¬m kiáº¿m tá»‘i Ä‘a 30 thá»­ nghiá»‡m cho táº¥t cáº£ cÃ¡c thuáº­t toÃ¡n, tá»©c lÃ , má»™t thuáº­t toÃ¡n cÃ³ thá»ƒ Ä‘Ã o táº¡o 30 mÃ´ hÃ¬nh khÃ¡c nhau vÃ  thu tháº­p mÃ´ hÃ¬nh vá»›i Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t. ChÃºng tÃ´i sá»­ dá»¥ng thiáº¿t láº­p máº·c Ä‘á»‹nh cho cÃ¡c thuáº­t toÃ¡n NAS má»™t láº§n

7

--- TRANG 8 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Báº£ng 1: So sÃ¡nh hiá»‡u suáº¥t cá»§a AUTOTRANSFER vÃ  cÃ¡c baseline khÃ¡c. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n trÃªn mÆ°á»i láº§n cháº¡y. Chá»‰ vá»›i 3 thá»­ nghiá»‡m AUTOTRANSFER Ä‘Ã£ vÆ°á»£t trá»™i háº§u háº¿t cÃ¡c baseline SOTA vá»›i 30 thá»­ nghiá»‡m.

| PhÆ°Æ¡ng phÃ¡p | NÃºt | | | Äá»“ thá»‹ | | |
|---|---|---|---|---|---|---|
| | Physics | CoraFull | OGB-Arxiv | COX2 | IMDB | PROTEINS |
| GCN (30 thá»­ nghiá»‡m) | 95.88Â±0.16 | 67.12Â±0.52 | 70.46Â±0.18 | 79.23Â±2.19 | 50.40Â±3.02 | 74.84Â±2.82 |
| GAT (30 thá»­ nghiá»‡m) | 95.71Â±0.24 | 65.92Â±0.68 | 68.82Â±0.32 | 81.56Â±4.17 | 49.67Â±4.30 | 75.30Â±3.72 |
| GraphNAS (30 thá»­ nghiá»‡m) | 92.77Â±0.84 | 63.13Â±3.28 | 65.90Â±2.64 | 77.73Â±1.40 | 46.93Â±3.94 | 72.51Â±3.36 |
| DARTS | 95.28Â±1.67 | 67.59Â±2.85 | 69.02Â±1.18 | 79.82Â±3.15 | 50.26Â±4.08 | 75.04Â±3.81 |
| GASSOâ´ | 96.38 | 68.89 | 70.52 | - | - | - |
| Random (3 thá»­ nghiá»‡m) | 95.16Â±0.55 | 61.24Â±4.04 | 67.92Â±1.92 | 76.88Â±3.17 | 45.79Â±4.39 | 72.47Â±2.57 |
| TPE (30 thá»­ nghiá»‡m) | 96.41Â±0.36 | 66.37Â±1.73 | 71.35Â±0.44 | 82.27Â±2.01 | 50.33Â±4.00 | 79.46Â±1.28 |
| HyperBand (30 thá»­ nghiá»‡m) | 96.56Â±0.30 | 67.75Â±1.24 | 71.60Â±0.36 | 82.21Â±1.79 | 50.86Â±3.45 | 79.32Â±1.16 |
| AUTOTRANSFER (3 thá»­ nghiá»‡m) | 96.64Â±0.42 | 69.27Â±0.76 | 71.42Â±0.39 | 82.13Â±1.59 | 52.33Â±2.13 | 77.81Â±2.19 |
| AUTOTRANSFER (30 thá»­ nghiá»‡m) | 96.91Â±0.27 | 70.05Â±0.42 | 72.21Â±0.27 | 86.52Â±1.58 | 54.93Â±1.23 | 81.25Â±1.17 |

thuáº­t toÃ¡n NAS (DARTS vÃ  GASSO), vÃ¬ chÃºng chá»‰ Ä‘Ã o táº¡o má»™t siÃªu mÃ´ hÃ¬nh má»™t láº§n vÃ  cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cÃ¡c kiáº¿n trÃºc khÃ¡c nhau. ChÃºng tÃ´i chá»§ yáº¿u quan tÃ¢m Ä‘áº¿n viá»‡c nghiÃªn cá»©u cháº¿ Ä‘á»™ vÃ i thá»­ nghiá»‡m nÆ¡i háº§u háº¿t cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m tiÃªn tiáº¿n trá»Ÿ thÃ nh tÃ¬m kiáº¿m ngáº«u nhiÃªn. Do Ä‘Ã³ chÃºng tÃ´i bá»• sung bao gá»“m má»™t baseline tÃ¬m kiáº¿m ngáº«u nhiÃªn (3 thá»­ nghiá»‡m) nÆ¡i chÃºng tÃ´i chá»n mÃ´ hÃ¬nh tá»‘t nháº¥t trong chá»‰ 3 thá»­ nghiá»‡m.

5.2 CÃC THÃ NGHIá»†M Vá»€ HIá»†U QUáº¢ TÃŒM KIáº¾M

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER báº±ng cÃ¡ch bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t trung bÃ¬nh trong táº¥t cáº£ cÃ¡c thá»­ nghiá»‡m Ä‘Æ°á»£c xem xÃ©t trÃªn mÆ°á»i láº§n cháº¡y cá»§a má»—i thuáº­t toÃ¡n trong Báº£ng 1. Äá»™ chÃ­nh xÃ¡c test Ä‘Æ°á»£c thu tháº­p cho má»—i thá»­ nghiá»‡m Ä‘Æ°á»£c chá»n táº¡i epoch vá»›i Ä‘á»™ chÃ­nh xÃ¡c validation tá»‘t nháº¥t. Báº±ng cÃ¡ch so sÃ¡nh káº¿t quáº£ tá»« tÃ¬m kiáº¿m ngáº«u nhiÃªn (3 thá»­ nghiá»‡m) vÃ  AUTOTRANSFER (3 thá»­ nghiá»‡m), chÃºng tÃ´i cho tháº¥y ráº±ng tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng bÃ¡o nhiá»‡m vá»¥ Ä‘Æ°á»£c chuyá»ƒn giao cá»§a chÃºng tÃ´i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c test trong cháº¿ Ä‘á»™ vÃ i thá»­ nghiá»‡m, vÃ  cÃ³ thá»ƒ ráº¥t há»¯u Ã­ch trong cÃ¡c mÃ´i trÆ°á»ng bá»‹ háº¡n cháº¿ vá» máº·t tÃ­nh toÃ¡n. Ngay cáº£ khi chÃºng tÃ´i tÄƒng sá»‘ lÆ°á»£ng thá»­ nghiá»‡m tÃ¬m kiáº¿m lÃªn 30, AUTOTRANSFER váº«n chá»©ng minh sá»± cáº£i thiá»‡n khÃ´ng táº§m thÆ°á»ng so vá»›i TPE, cho tháº¥y ráº±ng quy trÃ¬nh Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cÃ³ lá»£i tháº¿ ngay cáº£ khi tÃ i nguyÃªn tÃ­nh toÃ¡n dá»“i dÃ o. ÄÃ¡ng chÃº Ã½, chá»‰ vá»›i 3 thá»­ nghiá»‡m tÃ¬m kiáº¿m, AUTOTRANSFER vÆ°á»£t qua háº§u háº¿t cÃ¡c baseline, ngay cáº£ nhá»¯ng baseline sá»­ dá»¥ng 30 thá»­ nghiá»‡m.

Äá»ƒ hiá»ƒu rÃµ hÆ¡n vá» hiá»‡u quáº£ máº«u cá»§a AUTOTRANSFER, chÃºng tÃ´i váº½ Ä‘á»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t Ä‘Æ°á»£c tÃ¬m tháº¥y táº¡i má»—i thá»­ nghiá»‡m trong HÃ¬nh 3 cho cÃ¡c táº­p dá»¯ liá»‡u OGB-Arxiv vÃ  TU-PROTEINS. ChÃºng tÃ´i nháº­n tháº¥y ráº±ng cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m tiÃªn tiáº¿n (Evolution vÃ  TPE) khÃ´ng cÃ³ lá»£i tháº¿ so vá»›i tÃ¬m kiáº¿m ngáº«u nhiÃªn trong cháº¿ Ä‘á»™ vÃ i thá»­ nghiá»‡m vÃ¬ lÆ°á»£ng dá»¯ liá»‡u tÃ¬m kiáº¿m trÆ°á»›c Ä‘Ã³ chÆ°a Ä‘á»§ Ä‘á»ƒ suy luáº­n cÃ¡c cáº¥u hÃ¬nh thiáº¿t káº¿ tá»‘t hÆ¡n tiá»m nÄƒng. NgÆ°á»£c láº¡i, báº±ng cÃ¡ch láº¥y máº«u tá»« tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao, AUTOTRANSFER Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ trong nhá»¯ng thá»­ nghiá»‡m Ä‘áº§u tiÃªn. Äá»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t táº¡i thá»­ nghiá»‡m 3 cá»§a AUTOTRANSFER vÆ°á»£t qua Ä‘á»‘i tÃ¡c cá»§a nÃ³ táº¡i thá»­ nghiá»‡m 10 cho má»i phÆ°Æ¡ng phÃ¡p khÃ¡c.

HÃ¬nh 3: So sÃ¡nh hiá»‡u suáº¥t trong cháº¿ Ä‘á»™ vÃ i thá»­ nghiá»‡m. Táº¡i thá»­ nghiá»‡m t, chÃºng tÃ´i váº½ Ä‘á»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t trong táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tÃ¬m kiáº¿m tá»« thá»­ nghiá»‡m 1 Ä‘áº¿n thá»­ nghiá»‡m t. AUTOTRANSFER cÃ³ thá»ƒ giáº£m sá»‘ lÆ°á»£ng thá»­ nghiá»‡m cáº§n thiáº¿t Ä‘á»ƒ tÃ¬m kiáº¿m Ä‘i má»™t báº­c Ä‘á»™ lá»›n (xem thÃªm Báº£ng 4 trong Phá»¥ lá»¥c).

5.3 PHÃ‚N TÃCH CÃC NHÃšNG NHIá»†M Vá»¤

PhÃ¢n tÃ­ch Ä‘á»‹nh tÃ­nh cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. Äá»ƒ kiá»ƒm tra cháº¥t lÆ°á»£ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t, chÃºng tÃ´i trá»±c quan hÃ³a ma tráº­n tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t (HÃ¬nh 4 (b)) cÃ¹ng vá»›i ma tráº­n tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ (HÃ¬nh 4 (a)) Ä‘Æ°á»£c Ä‘á» xuáº¥t trong GraphGym. ChÃºng tÃ´i cho tháº¥y ráº±ng ma tráº­n tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i náº¯m báº¯t cÃ¡c máº«u tÆ°Æ¡ng tá»± nhÆ° ma tráº­n tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ cá»§a GraphGym trong khi Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£ hÆ¡n nhiá»u

Â²Káº¿t quáº£ Ä‘áº¿n tá»« bÃ i bÃ¡o gá»‘c (Qin et al., 2021).

8

--- TRANG 9 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

(a)(b)(c)

HÃ¬nh 4: (a) Sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ cá»§a GraphGym giá»¯a táº¥t cáº£ cÃ¡c cáº·p nhiá»‡m vá»¥ (Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« tÆ°Æ¡ng quan thá»© háº¡ng Kendall giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn hai nhiá»‡m vá»¥ Ä‘Æ°á»£c so sÃ¡nh), giÃ¡ trá»‹ cao hÆ¡n Ä‘áº¡i diá»‡n cho sá»± tÆ°Æ¡ng Ä‘á»“ng cao hÆ¡n. (b) Sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘Æ°á»£c tÃ­nh toÃ¡n báº±ng cÃ¡ch tÃ­nh tÃ­ch vÃ´ hÆ°á»›ng giá»¯a cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c trÃ­ch xuáº¥t. (c) TÆ°Æ¡ng quan thá»© háº¡ng Kendall cá»§a cÃ¡c xáº¿p háº¡ng tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c nhiá»‡m vá»¥ khÃ¡c Ä‘á»‘i vá»›i nhiá»‡m vá»¥ trung tÃ¢m giá»¯a phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t vÃ  GraphGym.

báº±ng cÃ¡ch bá» qua Ä‘Ã o táº¡o. ChÃºng tÃ´i nháº­n tháº¥y ráº±ng cÃ¹ng loáº¡i nhiá»‡m vá»¥, tá»©c lÃ  phÃ¢n loáº¡i nÃºt vÃ  phÃ¢n loáº¡i Ä‘á»“ thá»‹, chia sáº» nhiá»u sá»± tÆ°Æ¡ng Ä‘á»“ng hÆ¡n trong má»—i nhÃ³m. NhÆ° má»™t kiá»ƒm tra Ä‘á»™ tin cáº­y, chÃºng tÃ´i kiá»ƒm tra ráº±ng nhiá»‡m vá»¥ gáº§n nháº¥t trong ngÃ¢n hÃ ng Ä‘á»‘i vá»›i CoraFull lÃ  Cora. 3 nhiá»‡m vá»¥ gáº§n nháº¥t cho OGB-Arxiv lÃ  AmazonComputers, AmazonPhoto, vÃ  CoauthorPhysics, táº¥t cáº£ Ä‘á»u lÃ  cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i nÃºt.

Tá»•ng quÃ¡t hÃ³a cá»§a hÃ m chiáº¿u g(). Äá»ƒ cho tháº¥y hÃ m chiáº¿u Ä‘Æ°á»£c Ä‘á» xuáº¥t g() cÃ³ thá»ƒ táº¡o ra cÃ¡c nhÃºng nhiá»‡m vá»¥ cÃ³ thá»ƒ tá»•ng quÃ¡t hÃ³a cho cÃ¡c nhiá»‡m vá»¥ má»›i, chÃºng tÃ´i tiáº¿n hÃ nh xÃ¡c thá»±c chÃ©o leave-one-out vá»›i táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i. Cá»¥ thá»ƒ, Ä‘á»‘i vá»›i má»—i nhiá»‡m vá»¥ Ä‘Æ°á»£c coi lÃ  nhiá»‡m vá»¥ má»›i T(n), chÃºng tÃ´i sá»­ dá»¥ng pháº§n cÃ²n láº¡i cá»§a cÃ¡c nhiá»‡m vá»¥, cÃ¹ng vá»›i metric khoáº£ng cÃ¡ch dg(Â·; Â·) cá»§a chÃºng Ä‘Æ°á»£c Æ°á»›c tÃ­nh bá»Ÿi khÃ´ng gian metric chÃ­nh xÃ¡c nhÆ°ng tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n cá»§a GraphGym, Ä‘á»ƒ Ä‘Ã o táº¡o hÃ m chiáº¿u g(). ChÃºng tÃ´i tÃ­nh toÃ¡n tÆ°Æ¡ng quan thá»© háº¡ng Kendall trÃªn sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ cho Äáº·c trÆ°ng Nhiá»‡m vá»¥ (khÃ´ng cÃ³ g()) vÃ  NhÃºng Nhiá»‡m vá»¥ (cÃ³ g()) so vá»›i sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ chÃ­nh xÃ¡c. TÆ°Æ¡ng quan thá»© háº¡ng trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n trÃªn mÆ°á»i láº§n cháº¡y Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4 (c). ChÃºng tÃ´i tháº¥y ráº±ng vá»›i g() Ä‘Æ°á»£c Ä‘á» xuáº¥t, cÃ¡c nhÃºng nhiá»‡m vá»¥ cá»§a chÃºng tÃ´i thá»±c sá»± tÆ°Æ¡ng quan tá»‘t hÆ¡n vá»›i sá»± tÆ°Æ¡ng Ä‘á»“ng nhiá»‡m vá»¥ chÃ­nh xÃ¡c, vÃ  do Ä‘Ã³, tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n cho cÃ¡c nhiá»‡m vá»¥ má»›i.

NghiÃªn cá»©u loáº¡i bá» vá» thiáº¿t káº¿ khÃ´ng gian nhiá»‡m vá»¥ thay tháº¿. Äá»ƒ chá»©ng minh sá»± Æ°u viá»‡t cá»§a nhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t, chÃºng tÃ´i tiáº¿p tá»¥c so sÃ¡nh nÃ³ vá»›i cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ thay tháº¿. Theo cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã³ (Yang et al., 2019), chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c máº¥t mÃ¡t Ä‘Æ°á»£c chuáº©n hÃ³a trong 10 bÆ°á»›c Ä‘áº§u tiÃªn lÃ m Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. Káº¿t quáº£ trÃªn OGB-Arxiv Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2. So vá»›i nhÃºng nhiá»‡m vá»¥ cá»§a AUTOTRANSFER, Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c máº¥t mÃ¡t Ä‘Æ°á»£c chuáº©n hÃ³a cÃ³ tÆ°Æ¡ng quan thá»© háº¡ng tháº¥p hÆ¡n vá»›i metric chÃ­nh xÃ¡c vÃ  cho hiá»‡u suáº¥t kÃ©m hÆ¡n. Báº£ng 2 tiáº¿p tá»¥c chá»©ng minh hiá»‡u quáº£ cá»§a viá»‡c sá»­ dá»¥ng tÆ°Æ¡ng quan thá»© háº¡ng Kendall lÃ m metric cho cháº¥t lÆ°á»£ng nhÃºng nhiá»‡m vá»¥, vÃ¬ tÆ°Æ¡ng quan thá»© háº¡ng Kendall cao hÆ¡n dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t hÆ¡n.

Báº£ng 2: NghiÃªn cá»©u loáº¡i bá» vá» thiáº¿t káº¿ khÃ´ng gian nhiá»‡m vá»¥ thay tháº¿ so vá»›i nhÃºng nhiá»‡m vá»¥ cá»§a AUTOTRANSFER. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n OGB-Arxiv trÃªn mÆ°á»i láº§n cháº¡y.

| | TÆ°Æ¡ng quan thá»© háº¡ng Kendall | Äá»™ chÃ­nh xÃ¡c test |
|---|---|---|
| Thay tháº¿: Máº¥t mÃ¡t Ä‘Æ°á»£c Chuáº©n hÃ³a | -0.07Â±0.43 | 68.13Â±1.27 |
| Äáº·c trÆ°ng Nhiá»‡m vá»¥ cá»§a AUTOTRANSFER | 0.18Â±0.30 | 70.67Â±0.52 |
| NhÃºng Nhiá»‡m vá»¥ cá»§a AUTOTRANSFER | 0.43Â±0.22 | 71.42Â±0.39 |

6 Káº¾T LUáº¬N

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i nghiÃªn cá»©u cÃ¡ch cáº£i thiá»‡n hiá»‡u quáº£ tÃ¬m kiáº¿m AutoML báº±ng cÃ¡ch chuyá»ƒn giao kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc hiá»‡n cÃ³ cho cÃ¡c nhiá»‡m vá»¥ má»›i quan tÃ¢m. ChÃºng tÃ´i giá»›i thiá»‡u má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh thu tháº­p hiá»‡u suáº¥t trÃªn má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc GNN vÃ  nhiá»‡m vá»¥. ChÃºng tÃ´i cÅ©ng giá»›i thiá»‡u má»™t nhÃºng nhiá»‡m vá»¥ hiá»‡u quáº£ tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘o lÆ°á»ng chÃ­nh xÃ¡c sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau. ChÃºng tÃ´i phÃ¡t hÃ nh GNN-BANK-101, má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u quy mÃ´ lá»›n ghi láº¡i thÃ´ng tin Ä‘Ã o táº¡o GNN chi tiáº¿t cá»§a 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh. ChÃºng tÃ´i hy vá»ng cÃ´ng trÃ¬nh nÃ y cÃ³ thá»ƒ há»— trá»£ vÃ  truyá»n cáº£m há»©ng cho nghiÃªn cá»©u tÆ°Æ¡ng lai trong AutoML hiá»‡u quáº£ Ä‘á»ƒ lÃ m cho há»c sÃ¢u cÃ³ thá»ƒ tiáº¿p cáº­n Ä‘Æ°á»£c vá»›i khÃ¡n giáº£ tá»•ng quÃ¡t hÆ¡n.

9

--- TRANG 10 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Lá»œI Cáº¢M Æ N
ChÃºng tÃ´i cáº£m Æ¡n Xiang Lisa Li, Hongyu Ren, Yingxin Wu vÃ¬ cÃ¡c cuá»™c tháº£o luáº­n vÃ  cung cáº¥p pháº£n há»“i vá» báº£n tháº£o cá»§a chÃºng tÃ´i. ChÃºng tÃ´i cÅ©ng biáº¿t Æ¡n sá»± há»— trá»£ cá»§a DARPA dÆ°á»›i sá»‘ HR00112190039 (TAMI), N660011924033 (MCS); ARO dÆ°á»›i sá»‘ W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF dÆ°á»›i sá»‘ OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), NIH dÆ°á»›i sá»‘ 3U54HG010426-04S1 (HuBMAP), Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Amazon, Docomo, GSK, Hitachi, Intel, JPMorgan Chase, Juniper Networks, KDDI, NEC, vÃ  Toshiba. Ná»™i dung hoÃ n toÃ n lÃ  trÃ¡ch nhiá»‡m cá»§a cÃ¡c tÃ¡c giáº£ vÃ  khÃ´ng nháº¥t thiáº¿t Ä‘áº¡i diá»‡n cho quan Ä‘iá»ƒm chÃ­nh thá»©c cá»§a cÃ¡c thá»±c thá»ƒ tÃ i trá»£.

10

--- TRANG 11 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

TÃ€I LIá»†U THAM KHáº¢O

Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6430â€“6439, 2019.

James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.

James Bergstra, RÃ©mi Bardenet, Yoshua Bengio, and BalÃ¡zs KÃ©gl. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, 24, 2011.

Aleksandar Bojchevski and Stephan GÃ¼nnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.

Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018.

Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, and Qingming Huang. Re-thinking graph neural architecture search from message-passing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6657â€“6666, 2021.

Yuhui Ding, Quanming Yao, Huan Zhao, and Tong Zhang. Diffmg: Differentiable meta graph search for heterogeneous graph neural networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 279â€“288, 2021.

Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. ICML 2020 Workshop on Automated Machine Learning, 2020.

Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graphnas: Graph neural architecture search with reinforcement learning. arXiv preprint arXiv:1904.09981, 2019.

Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph neural architecture search. In Christian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 1403â€“1409. International Joint Conferences on Artificial Intelligence Organization, 7 2020. URL https://doi.org/10.24963/ijcai.2020/195.

Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision, pages 544â€“560. Springer, 2020.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770â€“778, 2016.

Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 639â€“648, 2020.

Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European conference on computer vision (ECCV), pages 784â€“800, 2018.

Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118â€“22133, 2020.

11

--- TRANG 12 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Sergei Ivanov, Sergei Sviridov, and Evgeny Burnaev. Understanding isomorphism bias in graph data sets, 2019.

Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.

Wonyong Jeong, Hayeon Lee, Geon Park, Eunyoung Hyung, Jinheon Baek, and Sung Ju Hwang. Task-adaptive neural network search with meta-contrastive learning. Advances in Neural Information Processing Systems, 34:21310â€“21324, 2021.

Donald R Jones. A taxonomy of global optimization methods based on response surfaces. Journal of global optimization, 21(4):345â€“383, 2001.

Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of fisher information in deep neural networks: Mean field approach. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1032â€“1041. PMLR, 2019.

Cat P Le, Mohammadreza Soltani, Juncheng Dong, and Vahid Tarokh. Fisher task distance and its application in neural architecture search. IEEE Access, 10:47235â€“47249, 2022.

Erin LeDell and Sebastien Poirier. H2o automl: Scalable automatic machine learning. ICML 2020 Workshop on Automated Machine Learning, 2020.

Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International conference on machine learning, pages 3734â€“3743. PMLR, 2019.

Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765â€“6816, 2017.

Yanxi Li, Zean Wen, Yunhe Wang, and Chang Xu. One-shot graph neural architecture search with dynamic search space. In Proc. AAAI Conf. Artif. Intell, volume 35, pages 8510â€“8517, 2021.

Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations, 2018.

Yao Ma, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. Graph convolutional networks with eigenpooling. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 723â€“731, 2019.

Jeffrey Pennington and Pratik Worah. The spectrum of the fisher information matrix of a single-hidden-layer neural network. Advances in neural information processing systems, 31, 2018.

Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-based simulation with graph networks. In International Conference on Learning Representations, 2020.

Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In International conference on machine learning, pages 4095â€“4104. PMLR, 2018.

Yijian Qin, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Graph differentiable architecture search with structure learning. Advances in Neural Information Processing Systems, 34, 2021.

Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning, pages 2902â€“2911. PMLR, 2017.

Roseanna W Saaty. The analytic hierarchy processâ€”what it is and how it is used. Mathematical modelling, 9(3-5):161â€“176, 1987.

Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pages 8459â€“8468. PMLR, 2020.

12

--- TRANG 13 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.

Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. Pooling architecture search for graph classification. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2091â€“2100, 2021.

Catherine Wong, Neil Houlsby, Yifeng Lu, and Andrea Gesmundo. Transfer learning with neural automl. Advances in neural information processing systems, 31, 2018.

Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. Oboe: Collaborative filtering for automl model selection. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019.

Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40â€“48. PMLR, 2016.

Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 974â€“983, 2018.

Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advances in Neural Information Processing Systems, 33:17009â€“17021, 2020.

Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In International Conference on Machine Learning, pages 12121â€“12132. PMLR, 2021.

Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3712â€“3722, 2018.

Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In International Conference on Machine Learning, pages 12674â€“12685. PMLR, 2021.

Huan Zhao, Quanming Yao, and Weiwei Tu. Search to aggregate neighborhood for graph neural network. arXiv preprint arXiv:2104.06608, 2021.

Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019.

Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics, 34(13):i457â€“i466, 2018.

Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697â€“8710, 2018.

13

--- TRANG 14 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

A CHI TIáº¾T TRIá»‚N KHAI Bá»” SUNG

PhÃ¢n tÃ­ch thá»i gian cháº¡y. ChÃºng tÃ´i Ä‘Ã£ chá»©ng minh thá»±c nghiá»‡m ráº±ng AUTOTRANSFER cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ tÃ¬m kiáº¿m báº±ng cÃ¡ch giáº£m sá»‘ lÆ°á»£ng thá»­ nghiá»‡m cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c tá»‘t há»£p lÃ½. Chi phÃ­ bá»• sung duy nháº¥t chÃºng tÃ´i giá»›i thiá»‡u lÃ  quy trÃ¬nh Æ°á»›c tÃ­nh nhÃºng nhiá»‡m vá»¥. VÃ¬ chÃºng tÃ´i sá»­ dá»¥ng kiáº¿n trÃºc Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn, viá»‡c trÃ­ch xuáº¥t má»—i Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ chá»‰ Ä‘Ã²i há»i tá»‘i Ä‘a má»™t láº§n truyá»n xuÃ´i vÃ  má»™t vÃ i láº§n truyá»n ngÆ°á»£c tá»« má»™t minibatch dá»¯ liá»‡u duy nháº¥t. Thá»i gian wall-clock phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c cá»§a máº¡ng vÃ  cáº¥u trÃºc dá»¯ liá»‡u. Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, thÆ°á»ng máº¥t vÃ i giÃ¢y trÃªn GPU NVIDIA T4. ChÃºng tÃ´i láº·p láº¡i quy trÃ¬nh trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ 5 láº§n cho má»—i mÃ´ hÃ¬nh neo, vÃ  cho tá»•ng cá»™ng 12 mÃ´ hÃ¬nh neo. Do Ä‘Ã³, thá»i gian wall-clock cá»§a chi phÃ­ bá»• sung Ä‘á»ƒ tÃ­nh toÃ¡n nhÃºng nhiá»‡m vá»¥ cá»§a nhiá»‡m vá»¥ má»›i trong vÃ i phÃºt. ChÃºng tÃ´i lÆ°u Ã½ Ä‘á»™ dÃ i cá»§a quy trÃ¬nh nÃ y thÆ°á»ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i má»™t thá»­ nghiá»‡m Ä‘Ã o táº¡o trÃªn táº­p dá»¯ liá»‡u kÃ­ch thÆ°á»›c nhá», vÃ  thá»i gian tiáº¿t kiá»‡m Ä‘Æ°á»£c quan trá»ng hÆ¡n nhiá»u Ä‘á»‘i vá»›i cÃ¡c táº­p dá»¯ liá»‡u quy mÃ´ lá»›n.

Chi tiáº¿t cho Ä‘Ã o táº¡o ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh. CÃ¡c Ä‘áº·c táº£ mÃ´ hÃ¬nh GNN cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tÃ³m táº¯t trong Báº£ng 3. CÆ¡ sá»Ÿ mÃ£ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn GraphGym (You et al., 2020). Äá»‘i vá»›i táº¥t cáº£ cÃ¡c thá»­ nghiá»‡m Ä‘Ã o táº¡o, chÃºng tÃ´i sá»­ dá»¥ng bá»™ tá»‘i Æ°u Adam vÃ  bá»™ láº­p lá»‹ch tá»‘c Ä‘á»™ há»c cosine (giáº£m dáº§n vá» 0, khÃ´ng khá»Ÿi Ä‘á»™ng láº¡i). ChÃºng tÃ´i sá»­ dá»¥ng regularization L2 vá»›i weight decay 5e-4. ChÃºng tÃ´i ghi láº¡i cÃ¡c máº¥t mÃ¡t vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho cÃ¡c pháº§n chia train, validation vÃ  test má»—i 20 epoch.

Chi tiáº¿t Ä‘Ã o táº¡o cho AUTOTRANSFER. ChÃºng tÃ´i tÃ³m táº¯t quy trÃ¬nh Ä‘Ã o táº¡o cho hÃ m chiáº¿u g() trong Thuáº­t toÃ¡n 2. ChÃºng tÃ´i Ä‘áº·t U = 12 vÃ  R = 5 trong toÃ n bá»™ bÃ i bÃ¡o. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng táº­p há»£p cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh neo nhÆ° trong GraphGym. ChÃºng tÃ´i sá»­ dá»¥ng MLP hai lá»›p vá»›i chiá»u áº©n 16 Ä‘á»ƒ tham sá»‘ hÃ³a hÃ m chiáº¿u g(). ChÃºng tÃ´i sá»­ dá»¥ng bá»™ tá»‘i Æ°u Adam vá»›i tá»‘c Ä‘á»™ há»c 5e-3. ChÃºng tÃ´i sá»­ dá»¥ng margin = 0.1 vÃ  Ä‘Ã o táº¡o máº¡ng trong 1000 láº§n láº·p vá»›i batch size 128. ChÃºng tÃ´i Ã¡p dá»¥ng K = 16 khi chá»n K thá»­ nghiá»‡m hÃ ng Ä‘áº§u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ³m táº¯t cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿.

Chi tiáº¿t Ä‘á»ƒ Ä‘iá»u chá»‰nh TPE, thuáº­t toÃ¡n tiáº¿n hÃ³a. ChÃºng tÃ´i minh há»a cÃ¡ch káº¿t há»£p cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m vá»›i cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao. TPE lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ Bayesian, cÃ³ nghÄ©a lÃ  nÃ³ Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i má»™t phÃ¢n phá»‘i tiÃªn nghiá»‡m Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a khÃ´ng gian tÃ¬m kiáº¿m vÃ  cáº­p nháº­t tiÃªn nghiá»‡m khi nÃ³ Ä‘Ã¡nh giÃ¡ cÃ¡c cáº¥u hÃ¬nh siÃªu tham sá»‘ vÃ  ghi láº¡i hiá»‡u suáº¥t cá»§a chÃºng. ChÃºng tÃ´i thay tháº¿ phÃ¢n phá»‘i tiÃªn nghiá»‡m nÃ y báº±ng cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng bÃ¡o nhiá»‡m vá»¥. VÃ¬ cÃ¡c thuáº­t toÃ¡n tiáº¿n hÃ³a thÆ°á»ng khá»Ÿi táº¡o má»™t quáº§n thá»ƒ lá»›n vÃ  láº·p Ä‘i láº·p láº¡i cáº¯t tá»‰a vÃ  biáº¿n Ä‘á»•i cÃ¡c máº¡ng hiá»‡n cÃ³, chÃºng tÃ´i thay tháº¿ viá»‡c khá»Ÿi táº¡o máº¡ng ngáº«u nhiÃªn báº±ng cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng bÃ¡o nhiá»‡m vá»¥. VÃ¬ chÃºng tÃ´i chá»§ yáº¿u táº­p trung vÃ o cháº¿ Ä‘á»™ tÃ¬m kiáº¿m vÃ i thá»­ nghiá»‡m, chÃºng tÃ´i Ä‘áº·t cÃ¡c thá»­ nghiá»‡m warm-up hoÃ n toÃ n ngáº«u nhiÃªn thÃ nh 5 cho cáº£ thuáº­t toÃ¡n TPE vÃ  tiáº¿n hÃ³a.

Báº£ng 3: CÃ¡c lá»±a chá»n thiáº¿t káº¿ trong khÃ´ng gian tÃ¬m kiáº¿m cá»§a chÃºng tÃ´i

| Loáº¡i | Lá»±a chá»n |
|---|---|
| Convolution | GeneralConv, GCNConv, SAGEConv, GINConv, GATConv |
| Sá»‘ lÆ°á»£ng heads | 1, 2, 4 |
| Aggregation | Sum, Mean-Pooling, Max-Pooling |
| Activation | ReLU, pReLU, leaky_ReLU, ELU |
| Chiá»u áº©n | 64, 256 |
| Káº¿t ná»‘i lá»›p | Stack, Skip-Sum, Skip-Concat |
| Lá»›p tiá»n xá»­ lÃ½ | 1, 2 |
| Lá»›p truyá»n tin | 2, 4, 6, 8 |
| Lá»›p háº­u xá»­ lÃ½ | 2, 3 |
| Tá»‘c Ä‘á»™ há»c | 0.1, 0.001 |
| Epoch Ä‘Ã o táº¡o | 200, 800, 1600 |

B THáº¢O LUáº¬N Bá»” SUNG

Háº¡n cháº¿. Vá» nguyÃªn táº¯c, AUTOTRANSFER táº­n dá»¥ng tÆ°Æ¡ng quan giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t mÃ´ hÃ¬nh giá»¯a cÃ¡c nhiá»‡m vá»¥ Ä‘á»ƒ xÃ¢y dá»±ng hiá»‡u quáº£ cÃ¡c tiÃªn nghiá»‡m mÃ´ hÃ¬nh. Do Ä‘Ã³, nÃ³ Ã­t hiá»‡u quáº£ hÆ¡n náº¿u nhiá»‡m vá»¥ má»›i cÃ³ khoáº£ng cÃ¡ch nhiá»‡m vá»¥ lá»›n Ä‘á»‘i vá»›i táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh. Trong thá»±c táº¿, ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ liÃªn tá»¥c thÃªm cÃ¡c thá»­ nghiá»‡m tÃ¬m kiáº¿m bá»• sung vÃ o ngÃ¢n hÃ ng. Khi kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng tÄƒng lÃªn, sáº½ Ã­t cÃ³ kháº£ nÄƒng má»™t nhiá»‡m vá»¥ má»›i cÃ³ tÆ°Æ¡ng quan tháº¥p vá»›i táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ trong ngÃ¢n hÃ ng.

14

--- TRANG 15 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Thuáº­t toÃ¡n 2 Quy trÃ¬nh ÄÃ o táº¡o cho hÃ m chiáº¿u g()
YÃªu cáº§u: CÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ {z(i)f|T(i)} Ä‘Æ°á»£c trÃ­ch xuáº¥t cho má»—i nhiá»‡m vá»¥ tá»« ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh. Khoáº£ng cÃ¡ch Ä‘o dg(Â·; Â·) Ä‘Æ°á»£c Æ°á»›c tÃ­nh trong GraphGym.
1: for each iteration do
2:   Láº¥y máº«u T(i), T(j), T(k)
3:   z(i)e; z(j)e; z(k)e â† g(z(i)f); g(z(j)f); g(z(k)f)
4:   y â† 1 náº¿u dg(T(i); T(j)) < dg(T(i); T(k)) ngÆ°á»£c láº¡i âˆ’1
5:   Tá»‘i Æ°u hÃ³a hÃ m má»¥c tiÃªu Lr(z(i)e; z(j)e; z(k)e; y) trong PhÆ°Æ¡ng trÃ¬nh 2
6: end for

TÃ¡c Ä‘á»™ng XÃ£ há»™i. Má»¥c tiÃªu dÃ i háº¡n cá»§a chÃºng tÃ´i lÃ  cung cáº¥p má»™t háº¡ táº§ng GNN liá»n máº¡ch Ä‘Æ¡n giáº£n hÃ³a viá»‡c Ä‘Ã o táº¡o vÃ  triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh ML trÃªn dá»¯ liá»‡u cÃ³ cáº¥u trÃºc. CÃ¡c thuáº­t toÃ¡n AutoML hiá»‡u quáº£ vÃ  máº¡nh máº½ lÃ  ráº¥t quan trá»ng Ä‘á»ƒ lÃ m cho há»c sÃ¢u cÃ³ thá»ƒ tiáº¿p cáº­n Ä‘Æ°á»£c vá»›i nhá»¯ng ngÆ°á»i quan tÃ¢m nhÆ°ng thiáº¿u chuyÃªn mÃ´n há»c sÃ¢u cÅ©ng nhÆ° nhá»¯ng ngÆ°á»i thiáº¿u ngÃ¢n sÃ¡ch tÃ­nh toÃ¡n khá»•ng lá»“ mÃ  AutoML truyá»n thá»‘ng Ä‘Ã²i há»i. ChÃºng tÃ´i tin ráº±ng bÃ i bÃ¡o nÃ y lÃ  má»™t bÆ°á»›c quan trá»ng Ä‘á»ƒ cung cáº¥p cÃ¡c cÃ´ng cá»¥ AI cho dÃ¢n sá»‘ rá»™ng hÆ¡n vÃ  do Ä‘Ã³ cho phÃ©p AI giÃºp tÄƒng cÆ°á»ng nÄƒng suáº¥t con ngÆ°á»i. CÃ¡c táº­p dá»¯ liá»‡u chÃºng tÃ´i sá»­ dá»¥ng cho cÃ¡c thÃ­ nghiá»‡m lÃ  má»™t trong nhá»¯ng benchmark Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t, khÃ´ng nÃªn chá»©a báº¥t ká»³ thiÃªn lá»‡ch khÃ´ng mong muá»‘n nÃ o. BÃªn cáº¡nh Ä‘Ã³, cÃ¡c máº¥t mÃ¡t/Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã o táº¡o/test lÃ  cÃ¡c thá»‘ng kÃª Ä‘Æ°á»£c tÃ³m táº¯t cao mÃ  chÃºng tÃ´i tin ráº±ng khÃ´ng nÃªn gÃ¢y ra cÃ¡c váº¥n Ä‘á» báº£o máº­t tiá»m nÄƒng.

C Káº¾T QUáº¢ Bá»” SUNG

Hiá»‡u quáº£ tÃ¬m kiáº¿m. ChÃºng tÃ´i tÃ³m táº¯t sá»‘ lÆ°á»£ng thá»­ nghiá»‡m trung bÃ¬nh cáº§n thiáº¿t Ä‘á»ƒ vÆ°á»£t qua Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t trung bÃ¬nh Ä‘Æ°á»£c tÃ¬m tháº¥y bá»Ÿi TPE vá»›i 30 thá»­ nghiá»‡m trong Báº£ng 4. ChÃºng tÃ´i cho tháº¥y ráº±ng AUTOTRANSFER giáº£m sá»‘ lÆ°á»£ng kiáº¿n trÃºc Ä‘Æ°á»£c khÃ¡m phÃ¡ Ä‘i má»™t báº­c Ä‘á»™ lá»›n.

Báº£ng 4: Sá»‘ lÆ°á»£ng thá»­ nghiá»‡m tÃ¬m kiáº¿m trung bÃ¬nh cáº§n thiáº¿t Ä‘á»ƒ vÆ°á»£t qua káº¿t quáº£ tá»‘t nháº¥t trung bÃ¬nh Ä‘Æ°á»£c tÃ¬m tháº¥y bá»Ÿi TPE vá»›i 30 thá»­ nghiá»‡m

| | NÃºt | | | Äá»“ thá»‹ | | |
|---|---|---|---|---|---|---|
| | Physics | CoraFull | OGB-Arxiv | COX2 | IMDB | PROTEINS |
| Sá»‘ thá»­ nghiá»‡m | 3 | 2 | 3 | 4 | 3 | 6 |
| Äá»™ chÃ­nh xÃ¡c | 96.64Â±0.42 | 67.85Â±1.31 | 71.42Â±0.39 | 82.96Â±1.75 | 52.33Â±2.13 | 80.21Â±1.21 |

NghiÃªn cá»©u loáº¡i bá» vá» sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo vÃ  thiáº¿t káº¿ nhÃºng nhiá»‡m vá»¥. ChÃºng tÃ´i chá»©ng minh thá»±c nghiá»‡m cÃ¡ch sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo áº£nh hÆ°á»Ÿng Ä‘áº¿n tÆ°Æ¡ng quan thá»© háº¡ng trong Báº£ng 5. Trong khi 3 mÃ´ hÃ¬nh neo khÃ´ng Ä‘á»§ Ä‘á»ƒ náº¯m báº¯t khoáº£ng cÃ¡ch nhiá»‡m vá»¥, chÃºng tÃ´i tháº¥y ráº±ng 9 vÃ  12 cÃ³ sá»± cÃ¢n báº±ng thá»a Ä‘Ã¡ng giá»¯a viá»‡c náº¯m báº¯t khoáº£ng cÃ¡ch nhiá»‡m vá»¥ vÃ  hiá»‡u quáº£ tÃ­nh toÃ¡n. HÆ¡n ná»¯a, chÃºng tÃ´i chá»©ng minh thá»±c nghiá»‡m trong Báº£ng 6 ráº±ng khÃ´ng gian nhÃºng nhiá»‡m vá»¥ Ä‘Ã£ há»c vÆ°á»£t trá»™i hÆ¡n khÃ´ng gian Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t vá» máº·t tÆ°Æ¡ng quan cÅ©ng nhÆ° hiá»‡u suáº¥t tÃ¬m kiáº¿m cuá»‘i cÃ¹ng.

Báº£ng 5: TÆ°Æ¡ng quan thá»© háº¡ng Kendall trung bÃ¬nh cá»§a cÃ¡c xáº¿p háº¡ng tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c nhiá»‡m vá»¥ khÃ¡c Ä‘á»‘i vá»›i nhiá»‡m vá»¥ trung tÃ¢m giá»¯a phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t vÃ  GraphGym.

| Sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo | 3 | 6 | 9 | 12 |
|---|---|---|---|---|
| Äáº·c trÆ°ng Nhiá»‡m vá»¥ | 0.03Â±0.34 | 0.11Â±0.36 | 0.16Â±0.34 | 0.18Â±0.30 |
| NhÃºng Nhiá»‡m vá»¥ | 0.12Â±0.28 | 0.26Â±0.30 | 0.36Â±0.24 | 0.43Â±0.22 |

Trá»±c quan hÃ³a cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh. ChÃºng tÃ´i trá»±c quan hÃ³a cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ sá»± tháº­t cÆ¡ báº£n trÃªn táº­p dá»¯ liá»‡u TU-PROTEINS trong HÃ¬nh 5, cÅ©ng nhÆ° táº­p dá»¯ liá»‡u Coauthor-Physics trong HÃ¬nh 6. ChÃºng tÃ´i cÃ³ thá»ƒ quan sÃ¡t ráº±ng cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao cÃ³ tÆ°Æ¡ng quan tÃ­ch cá»±c trÃªn háº§u háº¿t cÃ¡c lá»±a chá»n thiáº¿t káº¿.

15

--- TRANG 16 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023

Báº£ng 6: NghiÃªn cá»©u loáº¡i bá» vá» sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo cÅ©ng nhÆ° nhÃºng nhiá»‡m vá»¥ so vá»›i Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ trÃªn OGB-Arxiv vá»›i 3 thá»­ nghiá»‡m. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n trÃªn mÆ°á»i láº§n cháº¡y.

| Sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo | 3 | 6 | 9 | 12 |
|---|---|---|---|---|
| Äáº·c trÆ°ng Nhiá»‡m vá»¥ | 69.42Â±0.82 | 69.86Â±0.78 | 70.41Â±0.59 | 70.67Â±0.52 |
| NhÃºng Nhiá»‡m vá»¥ | 69.80Â±0.75 | 70.59Â±0.63 | 71.16Â±0.47 | 71.42Â±0.39 |

[ÄÃ¢y lÃ  cÃ¡c biá»ƒu Ä‘á»“ hiá»ƒn thá»‹ phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ sá»± tháº­t cÆ¡ báº£n cho cÃ¡c tham sá»‘ khÃ¡c nhau nhÆ° gnn.layers_pre_mp, gnn.layers_mp, gnn.layers_post_mp, gnn.stage_type, gnn.agg, gnn.dim_inner, gnn.layer_type, gnn.act, optim.base_lr, optim.max_epoch]

HÃ¬nh 5: ChÃºng tÃ´i váº½ cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ sá»± tháº­t cÆ¡ báº£n trÃªn táº­p dá»¯ liá»‡u TU-PROTEINS.

[CÃ¡c biá»ƒu Ä‘á»“ tÆ°Æ¡ng tá»± cho táº­p dá»¯ liá»‡u Coauthor-Physics]

HÃ¬nh 6: ChÃºng tÃ´i váº½ cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ sá»± tháº­t cÆ¡ báº£n trÃªn táº­p dá»¯ liá»‡u Coauthor-Physics.

16
