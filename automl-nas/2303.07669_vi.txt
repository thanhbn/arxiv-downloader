# 2303.07669.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2303.07669.pdf
# Kích thước tệp: 1052276 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
AUTOTRANSFER : A UTOML VỚI CHUYỂN GIAO
KIẾN THỨC - ỨNG DỤNG CHO MẠNG NEURAL ĐỒ THỊ
Kaidi Cao Jiaxuan You Jiaju Liu Jure Leskovec
Khoa Khoa học Máy tính, Đại học Stanford
{kaidicao, jiaxuan, jiajuliu, jure}@cs.stanford.edu
TÓM TẮT
AutoML đã chứng minh thành công đáng kể trong việc tìm kiếm một kiến trúc mạng neural hiệu quả cho một nhiệm vụ học máy cụ thể được định nghĩa bởi một tập dữ liệu và một thước đo đánh giá. Tuy nhiên, hầu hết các kỹ thuật AutoML hiện tại xem xét mỗi nhiệm vụ một cách độc lập từ đầu, điều này yêu cầu khám phá nhiều kiến trúc, dẫn đến chi phí tính toán cao. Ở đây chúng tôi đề xuất AUTOTRANSFER, một giải pháp AutoML cải thiện hiệu quả tìm kiếm bằng cách chuyển giao kiến thức thiết kế kiến trúc trước đó cho nhiệm vụ mới quan tâm. Đổi mới chính của chúng tôi bao gồm một ngân hàng nhiệm vụ-mô hình nắm bắt hiệu suất mô hình trên một tập hợp đa dạng các kiến trúc và nhiệm vụ GNN, và một nhúng nhiệm vụ hiệu quả về mặt tính toán có thể đo lường chính xác sự tương tự giữa các nhiệm vụ khác nhau. Dựa trên ngân hàng nhiệm vụ-mô hình và các nhúng nhiệm vụ, chúng tôi ước tính các tiên nghiệm thiết kế của các mô hình mong muốn cho nhiệm vụ mới, bằng cách tổng hợp một tổng có trọng số tương tự của K phân phối thiết kế hàng đầu trên các nhiệm vụ tương tự với nhiệm vụ quan tâm. Các tiên nghiệm thiết kế được tính toán có thể được sử dụng với bất kỳ thuật toán tìm kiếm AutoML nào. Chúng tôi đánh giá AUTOTRANSFER trên sáu tập dữ liệu trong lĩnh vực học máy đồ thị. Các thí nghiệm chứng minh rằng (i) nhúng nhiệm vụ được đề xuất có thể được tính toán hiệu quả, và các nhiệm vụ có nhúng tương tự có các kiến trúc hoạt động tốt nhất tương tự; (ii) AUTOTRANSFER cải thiện đáng kể hiệu quả tìm kiếm với các tiên nghiệm thiết kế được chuyển giao, giảm số lượng kiến trúc được khám phá xuống một bậc độ lớn. Cuối cùng, chúng tôi phát hành GNN-B ANK -101, một tập dữ liệu quy mô lớn về thông tin đào tạo GNN chi tiết của 120.000 kết hợp nhiệm vụ-mô hình để tạo điều kiện và truyền cảm hứng cho nghiên cứu tương lai.

1 GIỚI THIỆU
Mạng neural sâu có tính mô-đun cao, yêu cầu nhiều quyết định thiết kế phải được đưa ra liên quan đến kiến trúc mạng và các siêu tham số. Những quyết định thiết kế này tạo thành một không gian tìm kiếm không lồi và tốn kém ngay cả đối với các chuyên gia để tối ưu hóa, đặc biệt khi việc tối ưu hóa phải được lặp lại từ đầu cho mỗi trường hợp sử dụng mới. Học máy tự động (AutoML) là một lĩnh vực nghiên cứu tích cực nhằm giảm nỗ lực con người cần thiết cho thiết kế kiến trúc thường bao gồm tối ưu hóa siêu tham số và tìm kiếm kiến trúc mạng neural. AutoML đã chứng minh thành công (Zoph và Le, 2016; Pham et al., 2018; Zoph et al., 2018; Cai et al., 2018; He et al., 2018; Guo et al., 2020; Erickson et al., 2020; LeDell và Poirier, 2020) trong nhiều lĩnh vực ứng dụng.

Việc tìm một mô hình tương đối tốt cho một nhiệm vụ học tập mới¹ một cách hiệu quả về mặt tính toán là rất quan trọng để làm cho học sâu trở nên dễ tiếp cận với các chuyên gia lĩnh vực có nền tảng đa dạng. AutoML hiệu quả đặc biệt quan trọng trong các lĩnh vực nơi các kiến trúc/siêu tham số tốt nhất rất nhạy cảm với nhiệm vụ. Một ví dụ đáng chú ý là lĩnh vực học đồ thị². Thứ nhất, các phương pháp học đồ thị nhận dữ liệu đầu vào bao gồm nhiều loại dữ liệu khác nhau và tối ưu hóa trên các nhiệm vụ trải rộng một tập hợp các lĩnh vực và phương thức đa dạng như khuyến nghị (Ying et al., 2018; He et al., 2020), mô phỏng vật lý (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2020), và tin sinh học (Zitnik et al., 2018). Điều này khác với thị giác máy tính và xử lý ngôn ngữ tự nhiên nơi

¹Trong bài báo này, chúng tôi gọi một nhiệm vụ như một tập dữ liệu cho trước với một thước đo/mất mát đánh giá, ví dụ, mất mát entropy chéo trên phân loại nút trên tập dữ liệu Cora.
²Chúng tôi tập trung vào lĩnh vực học đồ thị trong bài báo này. AUTOTRANSFER có thể được tổng quát hóa sang các lĩnh vực khác.

1arXiv:2303.07669v1  [cs.LG]  14 Mar 2023

--- TRANG 2 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
dữ liệu đầu vào có cấu trúc được định nghĩa trước, cố định có thể được chia sẻ giữa các kiến trúc mạng neural khác nhau.
Thứ hai, mạng neural hoạt động trên đồ thị đi kèm với một tập hợp phong phú các lựa chọn thiết kế và một tập hợp lớn các tham số để khám phá. Tuy nhiên, không giống như các lĩnh vực khác nơi một số kiến trúc được đào tạo trước như ResNet (He et al., 2016) và GPT-3 (Brown et al., 2020) thống trị các điểm chuẩn, đã được chỉ ra rằng thiết kế mạng neural đồ thị (GNN) tốt nhất phụ thuộc rất nhiều vào nhiệm vụ (You et al., 2020).

Mặc dù AutoML như một lĩnh vực nghiên cứu đang phát triển nhanh chóng, các giải pháp AutoML hiện tại có chi phí tính toán khổng lồ khi mục tiêu là tìm một mô hình tốt cho một nhiệm vụ học tập mới. Hầu hết các kỹ thuật AutoML hiện tại xem xét mỗi nhiệm vụ một cách độc lập và biệt lập, do đó chúng yêu cầu làm lại việc tìm kiếm từ đầu cho mỗi nhiệm vụ mới. Cách tiếp cận này bỏ qua kiến thức thiết kế kiến trúc có giá trị tiềm năng thu được từ các nhiệm vụ trước đó, và không thể tránh khỏi dẫn đến chi phí tính toán cao.
Vấn đề đặc biệt nghiêm trọng trong lĩnh vực học đồ thị Gao et al. (2019); Zhou et al. (2019), do các thách thức của các loại nhiệm vụ đa dạng và không gian thiết kế lớn được thảo luận ở trên.

Ở đây chúng tôi đề xuất AUTOTRANSFER³, một giải pháp AutoML cải thiện đáng kể tìm kiếm kiến trúc AutoML bằng cách chuyển giao kiến thức thiết kế kiến trúc trước đó cho nhiệm vụ quan tâm. Đổi mới chính của chúng tôi là giới thiệu một ngân hàng nhiệm vụ-mô hình lưu trữ hiệu suất của một tập hợp đa dạng các kiến trúc và nhiệm vụ GNN để hướng dẫn thuật toán tìm kiếm. Để cho phép chuyển giao kiến thức, chúng tôi định nghĩa một không gian nhúng nhiệm vụ sao cho các nhiệm vụ gần nhau trong không gian nhúng có các kiến trúc hoạt động tốt nhất tương ứng tương tự. Thách thức ở đây là nhúng nhiệm vụ cần nắm bắt các xếp hạng hiệu suất của các kiến trúc khác nhau trên các tập dữ liệu khác nhau, trong khi hiệu quả để tính toán.

Đổi mới của chúng tôi ở đây là nhúng một nhiệm vụ bằng cách sử dụng số điều kiện của Ma trận Thông tin Fisher của các mô hình được khởi tạo ngẫu nhiên khác nhau và cũng một lược đồ học tập với bảo đảm tổng quát hóa thực nghiệm. Bằng cách này chúng tôi ngầm nắm bắt các thuộc tính của nhiệm vụ học tập, trong khi nhanh hơn nhiều bậc độ lớn (trong vài giây). Sau đó chúng tôi ước tính tiên nghiệm thiết kế của các mô hình mong muốn cho nhiệm vụ mới, bằng cách tổng hợp các phân phối thiết kế trên các nhiệm vụ gần với nhiệm vụ quan tâm. Cuối cùng, chúng tôi khởi tạo một thuật toán tìm kiếm siêu tham số với tiên nghiệm thiết kế được thông tin nhiệm vụ tính toán.

Chúng tôi đánh giá AUTOTRANSFER trên sáu tập dữ liệu, bao gồm cả các nhiệm vụ phân loại nút và phân loại đồ thị. Chúng tôi chỉ ra rằng các nhúng nhiệm vụ được đề xuất có thể được tính toán hiệu quả và khoảng cách được đo giữa các nhiệm vụ tương quan cao (0,43 tương quan Kendall) với các xếp hạng hiệu suất mô hình. Hơn nữa, chúng tôi trình bày AUTOTRANSFER cải thiện đáng kể hiệu quả tìm kiếm khi sử dụng tiên nghiệm thiết kế được chuyển giao. AUTOTRANSFER giảm số lượng kiến trúc được khám phá cần thiết để đạt độ chính xác mục tiêu xuống một bậc độ lớn so với SOTA. Cuối cùng, chúng tôi phát hành GNN-B ANK-101 —cơ sở dữ liệu quy mô lớn đầu tiên chứa các bản ghi hiệu suất chi tiết cho 120.000 kết hợp nhiệm vụ-mô hình đã được đào tạo với 16.128 giờ GPU—để tạo điều kiện cho nghiên cứu tương lai.

2 CÔNG TRÌNH LIÊN QUAN
Trong phần này, chúng tôi tóm tắt công trình liên quan về AutoML liên quan đến các ứng dụng của nó trên GNN, các thuật toán tìm kiếm phổ biến, và công trình tiên phong liên quan đến học chuyển giao và nhúng nhiệm vụ.

AutoML cho GNN. Tìm kiếm kiến trúc mạng neural (NAS), một hình thức độc đáo và phổ biến của AutoML cho học sâu, có thể được chia thành hai loại: NAS đa thử nghiệm và NAS một lần. Trong NAS đa thử nghiệm, mỗi kiến trúc được lấy mẫu được đào tạo riêng biệt. GraphNAS (Gao et al., 2020) và Auto-GNN (Zhou et al., 2019) là các thuật toán NAS đa thử nghiệm điển hình trên GNN áp dụng một bộ điều khiển RNN học để đề xuất các tập hợp cấu hình tốt hơn thông qua học tăng cường.
Một-lần NAS (ví dụ, (Liu et al., 2018; Qin et al., 2021; Li et al., 2021)) bao gồm việc đóng gói toàn bộ không gian mô hình trong một siêu mô hình, đào tạo siêu mô hình một lần, và sau đó lặp đi lặp lại lấy mẫu các mô hình con từ siêu mô hình để tìm mô hình tốt nhất. Ngoài ra, có công trình nghiên cứu rõ ràng các lựa chọn thiết kế chi tiết như tăng cường dữ liệu (You et al., 2021), loại lớp truyền tin (Cai et al., 2021; Ding et al., 2021; Zhao et al., 2021), và gộp đồ thị (Wei et al., 2021).
Đáng chú ý, AUTOTRANSFER là giải pháp AutoML đầu tiên cho GNN chuyển giao hiệu quả kiến thức thiết kế qua các nhiệm vụ.

Thuật toán HPO. Các thuật toán Tối ưu hóa Siêu tham số (HPO) tìm kiếm các siêu tham số mô hình tối ưu bằng cách lặp đi lặp lại đề xuất một tập hợp siêu tham số và đánh giá hiệu suất của chúng.
Tìm kiếm ngẫu nhiên lấy mẫu siêu tham số từ không gian tìm kiếm với xác suất bằng nhau. Mặc dù không

³Mã nguồn có sẵn tại https://github.com/snap-stanford/AutoTransfer .

2

--- TRANG 3 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
Ngân hàng Nhiệm vụ-Mô hình (bản ghi hiệu suất lịch sử) Không gian Nhúng Nhiệm vụ Phân phối Thiết kế
Nhiệm vụ Agg Dim Epoch ... Val_loss
Cora sum 64 80 ... 0.22
Cora mean 128 200 ... 0.26
TU-DD sum 64 200 ... 0.46
TU-DD mean 128 200 ... 0.86
TU-DD max 256 800 ... 0.52
Arxiv mean 128 200 ... 0.68
... ... ... ... ... ...

Agg Dim Epoch
Agg

Nhiệm vụ mới quan tâm Dim​ Epoch... ​... ​

Hình 1: Tổng quan về A UTOTRANSFER. Trái: Chúng tôi giới thiệu GNN-B ANK -101, một cơ sở dữ liệu lớn chứa một tập hợp đa dạng các kiến trúc và siêu tham số GNN được áp dụng cho các nhiệm vụ khác nhau, cùng với thống kê đào tạo/đánh giá của chúng. Giữa: Chúng tôi giới thiệu một không gian nhúng nhiệm vụ, nơi mỗi điểm tương ứng với một nhiệm vụ khác nhau. Các nhiệm vụ gần nhau trong không gian nhúng có các mô hình hoạt động tốt nhất tương ứng tương tự. Phải: Cho một nhiệm vụ mới quan tâm, chúng tôi hướng dẫn tìm kiếm AutoML bằng cách tham khảo các phân phối thiết kế của các nhiệm vụ tương tự nhất trong không gian nhúng nhiệm vụ.

học từ các thử nghiệm trước đó, tìm kiếm ngẫu nhiên thường được sử dụng vì tính đơn giản và hiệu quả hơn nhiều so với tìm kiếm lưới (Bergstra và Bengio, 2012). Thuật toán TPE (Bergstra et al., 2011) xây dựng một mô hình xác suất về hiệu suất nhiệm vụ trên không gian siêu tham số và sử dụng kết quả của các thử nghiệm trước để chọn cấu hình hứa hẹn nhất tiếp theo để đào tạo, mà thuật toán TPE định nghĩa là tối đa hóa giá trị Cải thiện Kỳ vọng (Jones, 2001). Các thuật toán tiến hóa (Real et al., 2017; Jaderberg et al., 2017) đào tạo nhiều mô hình song song và thay thế các mô hình hoạt động kém bằng các bản sao "đột biến" của các mô hình tốt nhất hiện tại. AUTOTRANSFER là một giải pháp AutoML tổng quát và có thể được áp dụng kết hợp với bất kỳ thuật toán HPO nào trong số này.

Học Chuyển giao trong AutoML. Wong et al. (2018) đề xuất chuyển giao kiến thức qua các nhiệm vụ bằng cách tải lại bộ điều khiển của các thuật toán tìm kiếm học tăng cường. Tuy nhiên, phương pháp này giả định rằng không gian tìm kiếm trên các nhiệm vụ khác nhau bắt đầu với cùng một tiên nghiệm đã học. Không giống như AUTOTRANSFER, nó không thể giải quyết thách thức cốt lõi trong AutoML GNN: thiết kế GNN tốt nhất phụ thuộc rất nhiều vào nhiệm vụ cụ thể.
GraphGym (You et al., 2020) cố gắng chuyển giao thiết kế kiến trúc tốt nhất trực tiếp với một không gian thước đo đo lường sự tương tự nhiệm vụ. GraphGym (You et al., 2020) tính toán sự tương tự nhiệm vụ bằng cách đào tạo một tập hợp 12 "mô hình neo" đến hội tụ, điều này tốn kém về mặt tính toán. Ngược lại, AUTOTRANSFER thiết kế các nhúng nhiệm vụ nhẹ yêu cầu chi phí tính toán tối thiểu.
Ngoài ra, Zhao và Bilen (2021); Li et al. (2021) đề xuất tiến hành tìm kiếm kiến trúc trên một tập con proxy của toàn bộ tập dữ liệu và sau đó chuyển giao kiến trúc tốt nhất được tìm kiếm trên tập dữ liệu đầy đủ. Jeong et al. (2021) nghiên cứu một thiết lập tương tự trong lĩnh vực thị giác.

Nhúng Nhiệm vụ. Có nghiên cứu trước đây cố gắng định lượng nhúng và sự tương tự nhiệm vụ. Tương tự như GraphGym, Taskonomy (Zamir et al., 2018) ước tính ma trận ái lực nhiệm vụ bằng cách tóm tắt các mất mát/thước đo đánh giá cuối cùng sử dụng một Quy trình Phân cấp Phân tích (Saaty, 1987). Từ một góc độ khác, Task2Vec (Achille et al., 2019) tạo nhúng nhiệm vụ cho một nhiệm vụ cho trước sử dụng Ma trận Thông tin Fisher liên kết với một mạng thăm dò được đào tạo trước. Mạng thăm dò này được chia sẻ qua các nhiệm vụ và cho phép Task2Vec ước tính Ma trận Thông tin Fisher của các tập dữ liệu hình ảnh khác nhau. Le et al. (2022) mở rộng ý tưởng tương tự cho tìm kiếm kiến trúc mạng neural. Các nhúng nhiệm vụ nói trên không thể được áp dụng trực tiếp cho GNN vì các đầu vào không thẳng hàng qua các tập dữ liệu.
AUTOTRANSFER tránh nút thắt cổ chai bằng cách sử dụng thống kê tiệm cận của Ma trận Thông tin Fisher với trọng số được khởi tạo ngẫu nhiên.

3 CÔNG THỨC VẤN ĐỀ VÀ KIẾN THỨC CƠ BẢN
Đầu tiên chúng tôi giới thiệu các định nghĩa chính thức về cấu trúc dữ liệu liên quan đến A UTOTRANSFER.

3

--- TRANG 4 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
... 𝑎"! Mô hình 𝑀1 Mô hình 𝑀2 Mô hình 𝑀𝑈 R Khởi tạo Ngẫu nhiên FIM Thước đo Bất biến Tỷ lệ Đặc trưng Nhiệm vụ Nhúng Nhiệm vụ 𝑔 𝑓 𝑓 𝑓 Nhiệm vụ i Mục tiêu Đào tạo Nhiệm vụ j x Nhiệm vụ i Nhiệm vụ k x Mất mát Xếp hạng Biên 𝑎"" 𝑎"# 𝑧$ 𝑧% 𝑧%(') 𝑧%()) 𝑧%(') 𝑧%(*)

Hình 2: Quy trình trích xuất nhúng nhiệm vụ. Trái: Để nhúng một nhiệm vụ hiệu quả, đầu tiên chúng tôi trích xuất các đặc trưng nhiệm vụ bằng cách nối các đặc trưng được đo từ R mô hình neo được khởi tạo ngẫu nhiên. Sau đó, chúng tôi giới thiệu một hàm chiếu g() với trọng số đã học để biến đổi các đặc trưng nhiệm vụ thành nhúng nhiệm vụ. Phải: Mục tiêu đào tạo để tối ưu hóa g() với giám sát bộ ba.

Định nghĩa 1 (Nhiệm vụ) Chúng tôi ký hiệu một nhiệm vụ là T= (D;L()), bao gồm một tập dữ liệu D và một hàm mất mát L() liên quan đến thước đo đánh giá.

Đối với mỗi lần thử đào tạo trên một nhiệm vụ T(i), chúng ta có thể ghi lại kiến trúc mô hình Mj, siêu tham số Hj, và giá trị mất mát tương ứng lj, tức là, (Mj;Hj;lj). Chúng tôi đề xuất duy trì một ngân hàng nhiệm vụ-mô hình để tạo điều kiện chuyển giao kiến thức cho các nhiệm vụ mới trong tương lai.

Định nghĩa 2 (Ngân hàng Nhiệm vụ-Mô hình) Một ngân hàng nhiệm vụ-mô hình B được định nghĩa là một tập hợp các nhiệm vụ, mỗi nhiệm vụ có nhiều lần thử đào tạo, ở dạng B=f(T(i);f(M(i)j;H(i)j;l(i)j)g)g.

AutoML với Chuyển giao Kiến thức. Giả sử chúng ta có một ngân hàng nhiệm vụ-mô hình B. Cho một nhiệm vụ mới T(n) chưa được thấy trước đó, mục tiêu của chúng ta là nhanh chóng tìm một mô hình hoạt động tương đối tốt trên nhiệm vụ mới bằng cách sử dụng kiến thức từ ngân hàng nhiệm vụ-mô hình.

Trong bài báo này, chúng tôi tập trung vào AutoML cho các nhiệm vụ học đồ thị, mặc dù kỹ thuật được phát triển của chúng tôi là tổng quát và có thể được áp dụng cho các lĩnh vực khác. Chúng tôi định nghĩa đồ thị đầu vào là G=fV;Eg, trong đó V là tập hợp nút và E⊆V×V là tập hợp cạnh. Hơn nữa, hãy y ký hiệu các nhãn đầu ra của nó, có thể là cấp nút, cấp cạnh hoặc cấp đồ thị. Một GNN được tham số hóa bởi trọng số θ xuất ra một phân phối hậu nghiệm P(G;y;θ) cho dự đoán nhãn.

4 GIẢI PHÁP ĐỀ XUẤT: AUTOTRANSFER
Trong phần này, chúng tôi giới thiệu giải pháp AUTOTRANSFER được đề xuất. AUTOTRANSFER sử dụng không gian nhúng nhiệm vụ như một công cụ để hiểu mức độ liên quan của các thiết kế kiến trúc trước đó với nhiệm vụ mục tiêu. Nhúng nhiệm vụ được thiết kế nắm bắt các xếp hạng hiệu suất của các kiến trúc khác nhau trên các nhiệm vụ khác nhau trong khi cũng hiệu quả để tính toán. Đầu tiên chúng tôi giới thiệu một giải pháp có động lực lý thuyết để trích xuất một biểu diễn hiệu suất bất biến tỷ lệ của mỗi cặp nhiệm vụ-mô hình. Chúng tôi sử dụng những biểu diễn này để xây dựng các đặc trưng nhiệm vụ và học thêm nhúng nhiệm vụ. Những nhúng này tạo thành không gian nhúng nhiệm vụ mà cuối cùng chúng tôi sử dụng trong quá trình tìm kiếm AutoML.

4.1 CƠ BẢN CỦA MA TRẬN THÔNG TIN FISHER (FIM)
Cho một GNN được định nghĩa ở trên, Ma trận Thông tin Fisher (FIM) F của nó được định nghĩa là
F=EG;y[∇θlogP(G;y;θ)∇θlogP(G;y;θ)>]:

về mặt hình thức là hiệp phương sai kỳ vọng của các điểm số đối với các tham số mô hình. Có hai góc nhìn hình học phổ biến cho FIM. Thứ nhất, FIM là cận trên của Hessian và trùng với Hessian nếu gradient bằng 0. Do đó, FIM đặc trưng cho cảnh quan cục bộ của hàm mất mát gần cực tiểu toàn cục. Thứ hai, tương tự như Hessian, FIM mô hình cảnh quan mất mát không phải đối với không gian đầu vào, mà đối với không gian tham số. Trong góc nhìn hình học thông tin, nếu chúng ta thêm một nhiễu loạn nhỏ vào không gian tham số, chúng ta có
KL(P(G;y;θ)‖P(G;y;θ+dθ)) = dθ>Fdθ:

trong đó KL(·;·) là divergence Kullback–Leibler. Điều này có nghĩa là không gian tham số của một mô hình tạo thành một đa tạp Riemannian và FIM hoạt động như thước đo Riemannian của nó. Do đó FIM cho phép chúng ta định lượng tầm quan trọng của trọng số của một mô hình theo cách có thể áp dụng cho các kiến trúc khác nhau.

4.2 CÁC ĐẶC TRƯNG NHIỆM VỤ DỰA TRÊN FIM
Biểu diễn Bất biến Tỷ lệ của Cặp Nhiệm vụ-Mô hình. Chúng tôi nhằm tìm một biểu diễn bất biến tỷ lệ cho mỗi cặp nhiệm vụ-mô hình sẽ tạo cơ sở để xây dựng các đặc trưng nhiệm vụ. Thách thức chính trong việc sử dụng FIM để biểu diễn hiệu suất GNN là các tập dữ liệu đồ thị không có cấu trúc đầu vào phổ quát, cố định, vì vậy không khả thi để tìm một mô hình được đào tạo trước duy nhất và trích xuất FIM của nó. Tuy nhiên, việc đào tạo nhiều mạng đặt ra vấn đề vì các FIM được tính cho các mạng khác nhau không thể so sánh trực tiếp. Chúng tôi chọn sử dụng nhiều mạng nhưng bổ sung đề xuất sử dụng thống kê tiệm cận của FIM liên kết với trọng số được khởi tạo ngẫu nhiên. Lý giải lý thuyết cho mối quan hệ giữa thống kê tiệm cận của FIM và khả năng đào tạo của mạng neural đã được nghiên cứu trong (Karakida et al., 2019; Pennington và Worah, 2018) mà chúng tôi giới thiệu độc giả tham khảo. Chúng tôi giả định rằng thước đo khả năng đào tạo như vậy mã hóa cảnh quan mất mát và khả năng tổng quát hóa và do đó tương quan với hiệu suất mô hình cuối cùng trên nhiệm vụ. Một vấn đề khác liên quan đến cấu trúc đầu vào của các tập dữ liệu đồ thị là các mô hình khác nhau có số lượng tham số khác nhau. Mặc dù có một số kiến trúc được thiết kế đặc biệt, ví dụ, (Lee et al., 2019; Ma et al., 2019), hầu hết thiết kế kiến trúc GNN có thể được biểu diễn như một chuỗi các lớp tiền xử lý, lớp truyền tin, và lớp hậu xử lý. Các lớp tiền xử lý và hậu xử lý là các lớp Mạng Perceptron Đa lớp (MLP), có kích thước thay đổi qua các nhiệm vụ khác nhau do cấu trúc đầu vào/đầu ra khác nhau. Các lớp truyền tin thường được coi là thiết kế chính cho GNN và số lượng tham số trọng số có thể giữ nguyên qua các nhiệm vụ. Theo quan điểm này, chúng tôi chỉ xem xét FIM đối với các tham số trong các lớp truyền tin để số lượng tham số được xem xét giữ nguyên cho tất cả các tập dữ liệu. Chúng tôi lưu ý rằng công thức như vậy có những hạn chế, theo nghĩa là nó không thể bao phủ tất cả các thiết kế GNN trong tài liệu. Chúng tôi để lại các phần mở rộng tiềm năng với phạm vi bao phủ tốt hơn cho công việc tương lai. Chúng tôi tiếp tục xấp xỉ FIM bằng cách chỉ xem xét các mục đường chéo, điều này ngầm bỏ qua các tương quan giữa các tham số. Chúng tôi lưu ý rằng đây là thực hành phổ biến khi phân tích FIM của mạng neural sâu, vì FIM đầy đủ là khổng lồ (bậc hai theo số lượng tham số) và không khả thi để tính toán ngay cả trên phần cứng hiện đại. Tương tự như Pennington và Worah (2018), chúng tôi xem xét hai moment đầu tiên của FIM
m1=1ntr[F] và m2=1ntr[F2] (1)

và sử dụng κ=m2/m1² như biểu diễn bất biến tỷ lệ. κ được tính toán được giới hạn dưới bởi 1 và nắm bắt mức độ tập trung của phổ. Một κ nhỏ chỉ ra cảnh quan mất mát phẳng, và thiết kế mô hình tương ứng của nó tận hưởng tối ưu hóa bậc nhất nhanh và khả năng tổng quát hóa tốt hơn tiềm năng.

Để mã hóa thông tin không gian nhãn vào mỗi nhiệm vụ, chúng tôi đề xuất chỉ đào tạo lớp tuyến tính cuối cùng của mỗi mô hình trên một nhiệm vụ cho trước, điều này có thể được thực hiện hiệu quả. Các tham số trong các lớp khác được đóng băng sau khi được khởi tạo ngẫu nhiên. Chúng tôi lấy trung bình trên R khởi tạo để ước tính κ trung bình.

Xây dựng Đặc trưng Nhiệm vụ. Chúng tôi ký hiệu các đặc trưng nhiệm vụ là các thước đo được trích xuất từ mỗi nhiệm vụ đặc trưng cho các đặc điểm quan trọng của nó. Thiết kế của các đặc trưng nhiệm vụ nên phản ánh mục tiêu cuối cùng của chúng ta: sử dụng những đặc trưng này để xác định các nhiệm vụ tương tự và chuyển giao các phân phối thiết kế tốt nhất. Do đó, chúng tôi chọn U thiết kế mô hình làm mô hình neo và nối các biểu diễn bất biến tỷ lệ au của mỗi thiết kế làm đặc trưng nhiệm vụ. Để chỉ giữ lại thứ tự xếp hạng tương đối giữa các thiết kế mô hình neo, chúng tôi chuẩn hóa vector đặc trưng được nối với tỷ lệ 1. Chúng tôi để zf ký hiệu đặc trưng nhiệm vụ được chuẩn hóa.

4.3 TỪ ĐẶC TRƯNG NHIỆM VỤ ĐẾN NHÚNG NHIỆM VỤ
Đặc trưng nhiệm vụ zf được giới thiệu ở trên có thể được coi là một phương tiện kỹ thuật đặc trưng. Chúng tôi xây dựng vector đặc trưng với kiến thức lĩnh vực, nhưng không có đảm bảo nào rằng nó hoạt động như dự đoán. Do đó chúng tôi đề xuất học một hàm chiếu g() : RU→RD ánh xạ đặc trưng nhiệm vụ zf đến nhúng nhiệm vụ cuối cùng ze=g(zf). Chúng tôi không có bất kỳ giám sát theo điểm nào có thể được sử dụng làm mục tiêu đào tạo. Thay vào đó, chúng tôi xem xét không gian thước đo được định nghĩa bởi GraphGym. Hàm khoảng cách trong GraphGym - được tính bằng tương quan xếp hạng Kendall giữa các xếp hạng hiệu suất của các mô hình neo được đào tạo trên hai nhiệm vụ được so sánh - tương quan tốt với mục tiêu chuyển giao kiến thức mong muốn của chúng ta. Không có ý nghĩa gì khi ép buộc rằng nhúng nhiệm vụ bắt chước không gian thước đo chính xác của GraphGym, vì không gian thước đo của GraphGym vẫn có thể chứa nhiễu, hoặc không hoàn toàn thẳng hàng với mục tiêu chuyển giao. Chúng tôi xem xét một mất mát thay thế chỉ ép buộc thứ tự xếp hạng giữa các nhiệm vụ. Để minh họa, hãy xem xét các nhiệm vụ T(i),T(j),T(k) và các nhúng nhiệm vụ tương ứng của chúng, z(i)e,z(j)e,z(k)e. Lưu ý rằng ze được chuẩn hóa về 1 nên z(i)e>z(j)e đo lường sự tương tự cosine giữa các nhiệm vụ T(i) và T(j).

Hãy dg(·;·) ký hiệu khoảng cách được ước tính bởi GraphGym. Chúng ta muốn ép buộc
z(i)e>z(j)e > z(i)e>z(k)e nếu dg(T(i);T(j)) < dg(T(i);T(k)):

Để đạt được điều này, chúng tôi sử dụng mất mát xếp hạng biên làm hàm mục tiêu giám sát thay thế:
Lr(z(i)e;z(j)e;z(k)e;y) = max(0;y(z(i)e>z(j)e−z(i)e>z(k)e) + margin ): (2)

Ở đây nếu dg(T(i);T(j)) < dg(T(i);T(k)), thì chúng ta có nhãn tương ứng y = 1, và y = −1 ngược lại. Không gian nhúng nhiệm vụ cuối cùng của chúng tôi sau đó là một không gian thước đo dựa trên FIM với hàm khoảng cách cosine, trong đó khoảng cách được định nghĩa là de(T(i);T(j)) = 1−z(i)e>z(j)e. Vui lòng tham khảo quy trình đào tạo chi tiết tại Thuật toán 2 trong Phụ lục.

4.4 THUẬT TOÁN TÌM KIẾM AUTOML VỚI NHÚNG NHIỆM VỤ
Để chuyển giao kiến thức cho một nhiệm vụ mới, một ý tưởng ngây thơ sẽ là trực tiếp chuyển cấu hình mô hình tốt nhất từ nhiệm vụ gần nhất trong ngân hàng. Tuy nhiên, ngay cả một tương quan xếp hạng Kendall cao giữa các xếp hạng hiệu suất mô hình của hai nhiệm vụ T(i),T(j) không đảm bảo cấu hình mô hình tốt nhất trong nhiệm vụ T(i) cũng sẽ đạt hiệu suất tốt nhất trên nhiệm vụ T(j). Ngoài ra, vì sự tương tự nhiệm vụ có thể bị nhiễu, giải pháp ngây thơ này có thể gặp khó khăn khi tồn tại nhiều nhiệm vụ tham chiếu đều rất tương tự.

Để làm cho việc chuyển giao kiến thức mạnh mẽ hơn với những trường hợp thất bại như vậy, chúng tôi giới thiệu khái niệm phân phối thiết kế phụ thuộc vào các thiết kế mô hình hoạt động tốt nhất và đề xuất chuyển giao phân phối thiết kế thay vì các cấu hình thiết kế tốt nhất. Chính thức, xem xét một nhiệm vụ T(i) trong ngân hàng nhiệm vụ-mô hình B, liên kết với các thử nghiệm f(M(i)j;H(i)j;l(i)j)g của nó. Chúng ta có thể tóm tắt các thiết kế của nó như một danh sách các cấu hình C=fc1;:::;cWg, sao cho tất cả các kết hợp tiềm năng của kiến trúc mô hình M và siêu tham số H rơi vào tích Cartesian của các cấu hình. Ví dụ, c1 có thể là việc thể hiện của các lớp tổng hợp, và c2 có thể là tốc độ học bắt đầu. Sau đó chúng tôi định nghĩa phân phối thiết kế là các biến ngẫu nhiên c1;c2;:::;cW, mỗi cái tương ứng với một siêu tham số.
Mỗi biến ngẫu nhiên c được định nghĩa là phân phối tần số của các lựa chọn thiết kế được sử dụng trong K thử nghiệm hàng đầu. Chúng tôi nhân tất cả các phân phối cho các cấu hình cá nhân fc1;:::;cWg để xấp xỉ phân phối thiết kế tổng thể của nhiệm vụ P(C|T(i)) = ∏w P(cw|T(i)):

Trong quá trình suy luận, cho một nhiệm vụ mới T(n), chúng tôi chọn một tập con nhiệm vụ gần S bằng cách ngưỡng khoảng cách nhúng nhiệm vụ, tức là, S=fT(i)|de(T(n);T(i))≤dthresg. Sau đó chúng tôi rút ra tiên nghiệm thiết kế được chuyển giao Pt(C|T(n)) của nhiệm vụ mới bằng cách cân phân phối thiết kế từ tập con nhiệm vụ gần S.
Pt(C|T(n)) = ∑T(i)∈S 1de(T(n);T(i)) P(C|T(i)) / ∑T(i)∈S 1de(T(n);T(i)): (3)

Tiên nghiệm thiết kế được suy luận cho nhiệm vụ mới sau đó có thể được sử dụng để hướng dẫn các thuật toán tìm kiếm khác nhau.
Lựa chọn tự nhiên nhất cho chế độ ít thử nghiệm là tìm kiếm ngẫu nhiên. Thay vì lấy mẫu mỗi cấu hình thiết kế theo phân phối đồng nhất, chúng tôi đề xuất lấy mẫu từ tiên nghiệm thiết kế được thông tin nhiệm vụ Pt(C|T(n)). Vui lòng tham khảo Phụ lục A để kiểm tra cách chúng tôi tăng cường các thuật toán tìm kiếm khác.

Đối với AUTOTRANSFER, chúng ta có thể tiền xử lý ngân hàng nhiệm vụ-mô hình B thành Bp = f(D(i);L(i)());z(i)e;P(C|T(i))g vì quy trình của chúng ta chỉ yêu cầu sử dụng nhúng nhiệm vụ z(i)e và phân phối thiết kế P(C|T(i)) thay vì các thử nghiệm đào tạo chi tiết. Một quy trình tìm kiếm chi tiết được tóm tắt trong Thuật toán 1.

6

--- TRANG 7 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
Thuật toán 1 Tóm tắt quy trình tìm kiếm A UTOTRANSFER
Yêu cầu: Một ngân hàng nhiệm vụ-mô hình đã xử lý Bp=f(D(i);L(i)());z(i)e;P(C|T(i))g, một nhiệm vụ mới T(n)=
D(n);L(n)()
, U mô hình neo M1;:::;MU, R chỉ định số lần lặp lại.
1: for u = 1 to U do
2: for r = 1 to R do
3: Khởi tạo trọng số cho mô hình neo Mu ngẫu nhiên
4: Ước tính FIM F ≈ ED[∇logP(Mu;y;θ)∇logP(Mu;y;θ)>]
5: Trích xuất biểu diễn bất biến tỷ lệ a(v)u ← m2/m1² theo Eq. 1
6: end for
7: au ← mean (a(1)u;a(2)u;:::;a(V)u)
8: end for
9: z(n)f ← concat (a1;a2;:::;aU)
10: z(n)e ← g(z(n)f)
11: Chọn tập con nhiệm vụ gần S ← fT(i)|1−z(n)e>z(i)e≤dthresg
12: Lấy tiên nghiệm thiết kế Pt(C|T(n)) bằng cách tổng hợp tập con S theo Eq. 3
13: Bắt đầu một thuật toán tìm kiếm HPO với tiên nghiệm thiết kế được thông tin nhiệm vụ Pt(C|T(n))

5 THÍ NGHIỆM
5.1 THIẾT LẬP THÍ NGHIỆM
Ngân hàng Nhiệm vụ-Mô hình: GNN-B ANK -101.
Để tạo điều kiện nghiên cứu AutoML với chuyển giao kiến thức, chúng tôi đã thu thập GNN-B ANK -101 như cơ sở dữ liệu đồ thị quy mô lớn đầu tiên ghi lại các cấu hình thiết kế có thể tái tạo và hiệu suất đào tạo chi tiết trên nhiều nhiệm vụ khác nhau. Cụ thể, GNN-B ANK -101 hiện tại bao gồm sáu nhiệm vụ cho phân loại nút (AmazonComputers (Shchur et al., 2018), AmazonPhoto (Shchur et al., 2018), CiteSeer (Yang et al., 2016), CoauthorCS (Shchur et al., 2018), CoauthorPhysics (Shchur et al., 2018), Cora (Yang et al., 2016)) và sáu nhiệm vụ cho phân loại đồ thị (PROTEINS (Ivanov et al., 2019), BZR (Ivanov et al., 2019), COX2 (Ivanov et al., 2019), DD (Ivanov et al., 2019), ENZYMES (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019)). Không gian thiết kế của chúng tôi theo (You et al., 2020), và chúng tôi mở rộng không gian thiết kế để bao gồm các lớp kích hoạt và tích chập đồ thị khác nhau thường được áp dụng. Chúng tôi chạy rộng rãi 10.000 mô hình khác nhau cho mỗi nhiệm vụ, dẫn đến tổng cộng 120.000 kết hợp nhiệm vụ-mô hình, và ghi lại tất cả thông tin đào tạo bao gồm mất mát train/val/test.

Tập dữ liệu Điểm chuẩn. Chúng tôi đánh giá AUTOTRANSFER trên sáu tập dữ liệu khác nhau theo công trình trước đó (Qin et al., 2021). Các tập dữ liệu của chúng tôi bao gồm ba tập dữ liệu phân loại nút tiêu chuẩn (CoauthorPhysics (Shchur et al., 2018), CoraFull (Bojchevski và Günnemann, 2017) và OGB-Arxiv (Hu et al., 2020)), cũng như ba tập dữ liệu điểm chuẩn phân loại đồ thị tiêu chuẩn, (COX2 (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019) và PROTEINS (Ivanov et al., 2019)). CoauthorPhysics và CoraFull là các tập dữ liệu phân loại nút chuyển đổi, vì vậy chúng tôi gán ngẫu nhiên các nút vào các tập train/valid/test theo phân chia 50%:25%:25% (Qin et al., 2021). Chúng tôi phân chia ngẫu nhiên các đồ thị theo phân chia 80%:10%:10% cho ba tập dữ liệu phân loại đồ thị (Qin et al., 2021). Chúng tôi tuân theo phân chia train/valid/test mặc định cho tập dữ liệu OGB-Arxiv (Hu et al., 2020). Để đảm bảo không có rò rỉ thông tin, chúng tôi tạm thời loại bỏ tất cả các bản ghi liên quan đến nhiệm vụ từ ngân hàng nhiệm vụ-mô hình của chúng tôi nếu tập dữ liệu chúng tôi đánh giá được thu thập trong ngân hàng nhiệm vụ-mô hình.

Đường cơ sở. Chúng tôi so sánh các phương pháp của chúng tôi với các cách tiếp cận hiện đại cho AutoML GNN. Chúng tôi sử dụng GCN và GAT với kiến trúc mặc định theo triển khai ban đầu của chúng làm đường cơ sở. Đối với các phương pháp NAS đa thử nghiệm, chúng tôi xem xét GraphNAS (Gao et al., 2020). Đối với các phương pháp NAS một lần, chúng tôi bao gồm DARTS (Liu et al., 2018) và GASSO (Qin et al., 2021). GASSO được thiết kế cho các thiết lập chuyển đổi, vì vậy chúng tôi bỏ qua nó cho các điểm chuẩn phân loại đồ thị. Chúng tôi tiếp tục cung cấp kết quả của các thuật toán HPO dựa trên không gian tìm kiếm được đề xuất của chúng tôi làm đường cơ sở: Random, Evolution, TPE (Bergstra et al., 2011) và HyperBand (Li et al., 2017).

Chúng tôi mặc định cho phép tìm kiếm tối đa 30 thử nghiệm cho tất cả các thuật toán, tức là, một thuật toán có thể đào tạo 30 mô hình khác nhau và thu thập mô hình với độ chính xác tốt nhất. Chúng tôi sử dụng thiết lập mặc định cho một lần

7

--- TRANG 8 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
Bảng 1: So sánh hiệu suất của AUTOTRANSFER và các đường cơ sở khác. Chúng tôi báo cáo độ chính xác test trung bình và độ lệch chuẩn qua mười lần chạy. Chỉ với 3 thử nghiệm AUTOTRANSFER đã vượt trội hầu hết các đường cơ sở SOTA với 30 thử nghiệm.

Nút Đồ thị
Phương pháp Physics CoraFull OGB-Arxiv COX2 IMDB PROTEINS
GCN (30 thử nghiệm) 95.88±0.16 67.12±0.52 70.46±0.18 79.23±2.19 50.40±3.02 74.84±2.82
GAT (30 thử nghiệm) 95.71±0.24 65.92±0.68 68.82±0.32 81.56±4.17 49.67±4.30 75.30±3.72
GraphNAS (30 thử nghiệm) 92.77±0.84 63.13±3.28 65.90±2.64 77.73±1.40 46.93±3.94 72.51±3.36
DARTS 95.28±1.67 67.59±2.85 69.02±1.18 79.82±3.15 50.26±4.08 75.04±3.81
GASSO⁴ 96.38 68.89 70.52 - - -
Random (3 thử nghiệm) 95.16±0.55 61.24±4.04 67.92±1.92 76.88±3.17 45.79±4.39 72.47±2.57
TPE (30 thử nghiệm) 96.41±0.36 66.37±1.73 71.35±0.44 82.27±2.01 50.33±4.00 79.46±1.28
HyperBand (30 thử nghiệm) 96.56±0.30 67.75±1:24 71.60±0.36 82.21±1.79 50.86±3.45 79.32±1.16
AUTOTRANSFER (3 thử nghiệm) 96.64±0.42 69.27±0.76 71.42±0.39 82.13±1.59 52.33±2.13 77.81±2.19
AUTOTRANSFER (30 thử nghiệm) 96.91±0.27 70.05±0.42 72.21±0.27 86.52±1.58 54.93±1.23 81.25±1.17

các thuật toán NAS (DARTS và GASSO), vì chúng chỉ đào tạo một siêu mô hình một lần và có thể đánh giá hiệu quả các kiến trúc khác nhau. Chúng tôi chủ yếu quan tâm đến việc nghiên cứu chế độ ít thử nghiệm nơi hầu hết các thuật toán tìm kiếm tiên tiến suy giảm thành tìm kiếm ngẫu nhiên. Do đó chúng tôi bổ sung bao gồm một đường cơ sở tìm kiếm ngẫu nhiên (3 thử nghiệm) nơi chúng tôi chọn mô hình tốt nhất trong chỉ 3 thử nghiệm.

5.2 THÍ NGHIỆM VỀ HIỆU QUẢ TÌM KIẾM
Chúng tôi đánh giá AUTOTRANSFER bằng cách báo cáo độ chính xác test tốt nhất trung bình trong tất cả các thử nghiệm được xem xét qua mười lần chạy của mỗi thuật toán trong Bảng 1. Độ chính xác test được thu thập cho mỗi thử nghiệm được chọn tại epoch với độ chính xác validation tốt nhất. Bằng cách so sánh kết quả từ tìm kiếm ngẫu nhiên (3 thử nghiệm) và AUTOTRANSFER (3 thử nghiệm), chúng tôi chỉ ra rằng tiên nghiệm thiết kế được thông tin nhiệm vụ được chuyển giao của chúng tôi cải thiện đáng kể độ chính xác test trong chế độ ít thử nghiệm, và có thể rất hữu ích trong môi trường bị hạn chế về mặt tính toán. Ngay cả khi chúng tôi tăng số lượng thử nghiệm tìm kiếm lên 30, AUTOTRANSFER vẫn chứng minh cải thiện không tầm thường so với TPE, chỉ ra rằng quy trình được đề xuất của chúng tôi có lợi thế ngay cả khi tài nguyên tính toán dồi dào. Đáng chú ý, chỉ với 3 thử nghiệm tìm kiếm, AUTOTRANSFER vượt qua hầu hết các đường cơ sở, ngay cả những cái sử dụng 30 thử nghiệm.

Để hiểu rõ hơn hiệu quả mẫu của AUTOTRANSFER, chúng tôi vẽ độ chính xác test tốt nhất được tìm thấy tại mỗi thử nghiệm trong Hình 3 cho các tập dữ liệu OGB-Arxiv và TU-PROTEINS. Chúng tôi nhận thấy rằng các thuật toán tìm kiếm tiên tiến (Evolution và TPE) không có lợi thế so với tìm kiếm ngẫu nhiên ở chế độ ít thử nghiệm vì lượng dữ liệu tìm kiếm trước chưa đủ để suy luận các cấu hình thiết kế tốt hơn tiềm năng. Ngược lại, bằng cách lấy mẫu từ tiên nghiệm thiết kế được chuyển giao, AUTOTRANSFER đạt độ chính xác test trung bình tốt hơn đáng kể trong những thử nghiệm đầu tiên. Độ chính xác test tốt nhất tại thử nghiệm 3 của A UTOTRANSFER vượt qua đối tác của nó tại thử nghiệm 10 cho mọi phương pháp khác.

Hình 3: So sánh hiệu suất trong chế độ ít thử nghiệm. Tại thử nghiệm t, chúng tôi vẽ độ chính xác test tốt nhất trong tất cả các mô hình được tìm kiếm từ thử nghiệm 1 đến thử nghiệm t. AUTOTRANSFER có thể giảm số lượng thử nghiệm cần thiết để tìm kiếm xuống một bậc độ lớn (xem thêm Bảng 4 trong Phụ lục).

5.3 PHÂN TÍCH NHÚNG NHIỆM VỤ
Phân tích định tính về đặc trưng nhiệm vụ. Để kiểm tra chất lượng của các đặc trưng nhiệm vụ được đề xuất, chúng tôi hình dung ma trận tương tự nhiệm vụ được đề xuất (Hình 4 (b)) cùng với ma trận tương tự nhiệm vụ (Hình 4 (a)) được đề xuất trong GraphGym. Chúng tôi chỉ ra rằng ma trận tương tự nhiệm vụ được đề xuất của chúng tôi nắm bắt các mẫu tương tự như ma trận tương tự nhiệm vụ của GraphGym trong khi được tính toán hiệu quả hơn nhiều

²Kết quả đến từ bài báo gốc (Qin et al., 2021).

8

--- TRANG 9 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
(a)(b)(c)

Hình 4: (a) Sự tương tự nhiệm vụ của GraphGym giữa tất cả các cặp nhiệm vụ (được tính từ tương quan xếp hạng Kendall giữa các xếp hạng hiệu suất của các mô hình được đào tạo trên hai nhiệm vụ được so sánh), một giá trị cao hơn đại diện cho sự tương tự cao hơn. (b) Sự tương tự nhiệm vụ được đề xuất được tính bằng cách tính tích vô hướng giữa các đặc trưng nhiệm vụ được trích xuất. (c) Tương quan xếp hạng Kendall của các xếp hạng tương tự của các nhiệm vụ khác đối với nhiệm vụ trung tâm giữa phương pháp được đề xuất và GraphGym.

bằng cách bỏ qua đào tạo. Chúng tôi nhận thấy rằng cùng loại nhiệm vụ, tức là, phân loại nút và phân loại đồ thị, chia sẻ nhiều sự tương tự hơn trong mỗi nhóm. Như một kiểm tra tính hợp lý, chúng tôi kiểm tra rằng nhiệm vụ gần nhất trong ngân hàng đối với CoraFull là Cora. Top 3 nhiệm vụ gần nhất cho OGB-Arxiv là AmazonComputers, AmazonPhoto, và CoauthorPhysics, tất cả đều là các nhiệm vụ phân loại nút.

Tổng quát hóa của hàm chiếu g(). Để chỉ ra hàm chiếu được đề xuất g() có thể tạo ra nhúng nhiệm vụ có thể tổng quát hóa cho các nhiệm vụ mới, chúng tôi tiến hành xác thực chéo loại-một-ra với tất cả các nhiệm vụ trong ngân hàng nhiệm vụ-mô hình của chúng tôi. Cụ thể, đối với mỗi nhiệm vụ được xem xét như một nhiệm vụ mới T(n), chúng tôi sử dụng phần còn lại của các nhiệm vụ, cùng với thước đo khoảng cách dg(·;·) của chúng được ước tính bởi không gian thước đo chính xác nhưng tốn kém về mặt tính toán của GraphGym, để đào tạo hàm chiếu g(). Chúng tôi tính tương quan xếp hạng Kendall trên sự tương tự nhiệm vụ cho Đặc trưng Nhiệm vụ (không có g()) và Nhúng Nhiệm vụ (với g()) chống lại sự tương tự nhiệm vụ chính xác. Tương quan xếp hạng trung bình và độ lệch chuẩn qua mười lần chạy được hiển thị trên Hình 4 (c). Chúng tôi thấy rằng với g() được đề xuất, nhúng nhiệm vụ của chúng tôi thực sự tương quan tốt hơn với sự tương tự nhiệm vụ chính xác, và do đó, tổng quát hóa tốt hơn cho các nhiệm vụ mới.

Nghiên cứu loại bỏ về thiết kế không gian nhiệm vụ thay thế. Để chứng minh tính ưu việt của nhúng nhiệm vụ được đề xuất, chúng tôi tiếp tục so sánh nó với các đặc trưng nhiệm vụ thay thế. Theo công trình trước đó (Yang et al., 2019), chúng tôi sử dụng các mất mát được chuẩn hóa trên 10 bước đầu tiên làm đặc trưng nhiệm vụ. Kết quả trên OGB-Arxiv được hiển thị trong Bảng 2. So với nhúng nhiệm vụ của AUTOTRANSFER, đặc trưng nhiệm vụ được tạo ra bởi mất mát được chuẩn hóa có tương quan xếp hạng thấp hơn với thước đo chính xác và mang lại hiệu suất tệ hơn. Bảng 2 tiếp tục biện minh cho hiệu quả của việc sử dụng tương quan xếp hạng Kendall làm thước đo cho chất lượng nhúng nhiệm vụ, vì tương quan xếp hạng Kendall cao hơn dẫn đến hiệu suất tốt hơn.

Bảng 2: Nghiên cứu loại bỏ về thiết kế không gian nhiệm vụ thay thế so với nhúng nhiệm vụ của AUTOTRANSFER. Chúng tôi báo cáo độ chính xác test trung bình và độ lệch chuẩn OGB-Arxiv qua mười lần chạy.

Tương quan xếp hạng Kendall Độ chính xác test
Thay thế: Mất mát Chuẩn hóa -0.07±0.43 68.13 ± 1.27
Đặc trưng Nhiệm vụ của AUTOTRANSFER 0.18±0.30 70.67 ± 0.52
Nhúng Nhiệm vụ của AUTOTRANSFER 0.43±0.22 71.42 ± 0.39

6 KẾT LUẬN
Trong bài báo này, chúng tôi nghiên cứu cách cải thiện hiệu quả tìm kiếm AutoML bằng cách chuyển giao kiến thức thiết kế kiến trúc hiện có cho các nhiệm vụ mới quan tâm. Chúng tôi giới thiệu một ngân hàng nhiệm vụ-mô hình nắm bắt hiệu suất trên một tập hợp đa dạng các kiến trúc và nhiệm vụ GNN. Chúng tôi cũng giới thiệu một nhúng nhiệm vụ hiệu quả về mặt tính toán có thể đo lường chính xác sự tương tự giữa các nhiệm vụ khác nhau. Chúng tôi phát hành GNN-B ANK -101, một cơ sở dữ liệu quy mô lớn ghi lại thông tin đào tạo GNN chi tiết của 120.000 kết hợp nhiệm vụ-mô hình. Chúng tôi hy vọng công trình này có thể tạo điều kiện và truyền cảm hứng cho nghiên cứu tương lai trong AutoML hiệu quả để làm cho học sâu dễ tiếp cận hơn với khán giả đại chúng.

9

--- TRANG 10 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
LỜI CẢM ƠN
Chúng tôi cảm ơn Xiang Lisa Li, Hongyu Ren, Yingxin Wu vì các cuộc thảo luận và vì đã cung cấp phản hồi về bản thảo của chúng tôi. Chúng tôi cũng biết ơn sự hỗ trợ của DARPA dưới Nos. HR00112190039 (TAMI), N660011924033 (MCS); ARO dưới Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF dưới Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), NIH dưới No. 3U54HG010426-04S1 (HuBMAP), Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Amazon, Docomo, GSK, Hitachi, Intel, JPMorgan Chase, Juniper Networks, KDDI, NEC, và Toshiba. Nội dung hoàn toàn là trách nhiệm của các tác giả và không nhất thiết đại diện cho quan điểm chính thức của các tổ chức tài trợ.

10

--- TRANG 11 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
TÀI LIỆU THAM KHẢO
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes, Stefano Soatto, và Pietro Perona. Task2vec: Task embedding for meta-learning. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 6430–6439, 2019.

James Bergstra và Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.

James Bergstra, Rémi Bardenet, Yoshua Bengio, và Balázs Kégl. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, 24, 2011.

Aleksandar Bojchevski và Stephan Günnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Han Cai, Ligeng Zhu, và Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018.

Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, và Qingming Huang. Rethinking graph neural architecture search from message-passing. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 6657–6666, 2021.

Yuhui Ding, Quanming Yao, Huan Zhao, và Tong Zhang. Diffmg: Differentiable meta graph search for heterogeneous graph neural networks. Trong Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, trang 279–288, 2021.

Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, và Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. ICML 2020 Workshop on Automated Machine Learning, 2020.

Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, và Yue Hu. Graphnas: Graph neural architecture search with reinforcement learning. arXiv preprint arXiv:1904.09981, 2019.

Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, và Yue Hu. Graph neural architecture search. Trong Christian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, trang 1403–1409. International Joint Conferences on Artificial Intelligence Organization, 7 2020. URL https://doi.org/10.24963/ijcai.2020/195.

Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, và Jian Sun. Single path one-shot neural architecture search with uniform sampling. Trong European Conference on Computer Vision, trang 544–560. Springer, 2020.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016.

Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, và Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. Trong Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, trang 639–648, 2020.

Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, và Song Han. Amc: Automl for model compression and acceleration on mobile devices. Trong Proceedings of the European conference on computer vision (ECCV), trang 784–800, 2018.

Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, và Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020.

11

--- TRANG 12 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
Sergei Ivanov, Sergei Sviridov, và Evgeny Burnaev. Understanding isomorphism bias in graph data sets, 2019.

Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.

Wonyong Jeong, Hayeon Lee, Geon Park, Eunyoung Hyung, Jinheon Baek, và Sung Ju Hwang. Task-adaptive neural network search with meta-contrastive learning. Advances in Neural Information Processing Systems, 34:21310–21324, 2021.

Donald R Jones. A taxonomy of global optimization methods based on response surfaces. Journal of global optimization, 21(4):345–383, 2001.

Ryo Karakida, Shotaro Akaho, và Shun-ichi Amari. Universal statistics of fisher information in deep neural networks: Mean field approach. Trong The 22nd International Conference on Artificial Intelligence and Statistics, trang 1032–1041. PMLR, 2019.

Cat P Le, Mohammadreza Soltani, Juncheng Dong, và Vahid Tarokh. Fisher task distance and its application in neural architecture search. IEEE Access, 10:47235–47249, 2022.

Erin LeDell và Sebastien Poirier. H2o automl: Scalable automatic machine learning. ICML 2020 Workshop on Automated Machine Learning, 2020.

Junhyun Lee, Inyeop Lee, và Jaewoo Kang. Self-attention graph pooling. Trong International conference on machine learning, trang 3734–3743. PMLR, 2019.

Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, và Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765–6816, 2017.

Yanxi Li, Zean Wen, Yunhe Wang, và Chang Xu. One-shot graph neural architecture search with dynamic search space. Trong Proc. AAAI Conf. Artif. Intell, volume 35, trang 8510–8517, 2021.

Hanxiao Liu, Karen Simonyan, và Yiming Yang. Darts: Differentiable architecture search. Trong International Conference on Learning Representations, 2018.

Yao Ma, Suhang Wang, Charu C Aggarwal, và Jiliang Tang. Graph convolutional networks with eigenpooling. Trong Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, trang 723–731, 2019.

Jeffrey Pennington và Pratik Worah. The spectrum of the fisher information matrix of a single-hidden-layer neural network. Advances in neural information processing systems, 31, 2018.

Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, và Peter Battaglia. Learning mesh-based simulation with graph networks. Trong International Conference on Learning Representations, 2020.

Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, và Jeff Dean. Efficient neural architecture search via parameters sharing. Trong International conference on machine learning, trang 4095–4104. PMLR, 2018.

Yijian Qin, Xin Wang, Zeyang Zhang, và Wenwu Zhu. Graph differentiable architecture search with structure learning. Advances in Neural Information Processing Systems, 34, 2021.

Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, và Alexey Kurakin. Large-scale evolution of image classifiers. Trong International Conference on Machine Learning, trang 2902–2911. PMLR, 2017.

Roseanna W Saaty. The analytic hierarchy process—what it is and how it is used. Mathematical modelling, 9(3-5):161–176, 1987.

Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, và Peter Battaglia. Learning to simulate complex physics with graph networks. Trong International Conference on Machine Learning, trang 8459–8468. PMLR, 2020.

12

--- TRANG 13 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, và Stephan Günnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.

Lanning Wei, Huan Zhao, Quanming Yao, và Zhiqiang He. Pooling architecture search for graph classification. Trong Proceedings of the 30th ACM International Conference on Information & Knowledge Management, trang 2091–2100, 2021.

Catherine Wong, Neil Houlsby, Yifeng Lu, và Andrea Gesmundo. Transfer learning with neural automl. Advances in neural information processing systems, 31, 2018.

Chengrun Yang, Yuji Akimoto, Dae Won Kim, và Madeleine Udell. Oboe: Collaborative filtering for automl model selection. Trong Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019.

Zhilin Yang, William Cohen, và Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. Trong International conference on machine learning, trang 40–48. PMLR, 2016.

Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, và Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. Trong Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, trang 974–983, 2018.

Jiaxuan You, Zhitao Ying, và Jure Leskovec. Design space for graph neural networks. Advances in Neural Information Processing Systems, 33:17009–17021, 2020.

Yuning You, Tianlong Chen, Yang Shen, và Zhangyang Wang. Graph contrastive learning automated. Trong International Conference on Machine Learning, trang 12121–12132. PMLR, 2021.

Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, và Silvio Savarese. Taskonomy: Disentangling task transfer learning. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 3712–3722, 2018.

Bo Zhao và Hakan Bilen. Dataset condensation with differentiable siamese augmentation. Trong International Conference on Machine Learning, trang 12674–12685. PMLR, 2021.

Huan Zhao, Quanming Yao, và Weiwei Tu. Search to aggregate neighborhood for graph neural network. arXiv preprint arXiv:2104.06608, 2021.

Kaixiong Zhou, Qingquan Song, Xiao Huang, và Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019.

Marinka Zitnik, Monica Agrawal, và Jure Leskovec. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics, 34(13):i457–i466, 2018.

Barret Zoph và Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, và Quoc V Le. Learning transferable architectures for scalable image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 8697–8710, 2018.

13

--- TRANG 14 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
A CHI TIẾT TRIỂN KHAI BỔ SUNG
Phân tích thời gian chạy. Chúng tôi đã chứng minh thực nghiệm rằng AUTOTRANSFER có thể cải thiện đáng kể hiệu quả tìm kiếm bằng cách giảm số lượng thử nghiệm cần thiết để đạt độ chính xác tương đối tốt. Chi phí bổ sung duy nhất chúng tôi giới thiệu là quy trình ước tính nhúng nhiệm vụ. Vì chúng tôi sử dụng một kiến trúc được khởi tạo ngẫu nhiên, việc trích xuất mỗi đặc trưng nhiệm vụ chỉ yêu cầu tối đa một lần truyền tiến và một vài lần truyền ngược từ một minibatch đơn lẻ của dữ liệu. Thời gian wall-clock phụ thuộc vào kích thước của mạng và cấu trúc dữ liệu. Trong các thí nghiệm của chúng tôi, nó thường mất vài giây trên GPU NVIDIA T4. Chúng tôi lặp lại quy trình trích xuất đặc trưng nhiệm vụ 5 lần cho mỗi mô hình neo, và tổng cộng cho 12 mô hình neo. Do đó, thời gian wall-clock của chi phí để tính toán nhúng nhiệm vụ của nhiệm vụ mới nằm trong vài phút. Chúng tôi lưu ý độ dài của quy trình này thường tương đương với một thử nghiệm đào tạo trên tập dữ liệu kích thước nhỏ, và thời gian tiết kiệm được nhiều hơn đáng kể đối với các tập dữ liệu quy mô lớn.

Chi tiết cho đào tạo ngân hàng-nhiệm vụ-mô hình. Các đặc tả mô hình GNN của chúng tôi được tóm tắt trong Bảng 3. Codebase của chúng tôi được phát triển dựa trên GraphGym (You et al., 2020). Đối với tất cả các thử nghiệm đào tạo, Chúng tôi sử dụng trình tối ưu Adam và bộ lập lịch tốc độ học cosine (được giảm dần về 0, không khởi động lại). Chúng tôi sử dụng regularization L2 với weight decay là 5e-4. Chúng tôi ghi lại mất mát và độ chính xác cho các phân chia đào tạo, validation và test mỗi 20 epoch.

Chi tiết đào tạo cho A UTOTRANSFER. Chúng tôi tóm tắt quy trình đào tạo cho hàm chiếu g() trong Thuật toán 2. Chúng tôi đặt U = 12 và R = 5 xuyên suốt bài báo. Chúng tôi sử dụng cùng một tập hợp các thiết kế mô hình neo như trong GraphGym. Chúng tôi sử dụng MLP hai lớp với kích thước ẩn 16 để tham số hóa hàm chiếu g(). Chúng tôi sử dụng trình tối ưu Adam với tốc độ học 5e-3. Chúng tôi sử dụng margin = 0.1 và đào tạo mạng trong 1000 lần lặp với kích thước batch 128. Chúng tôi áp dụng K = 16 khi chọn K thử nghiệm hàng đầu được sử dụng để tóm tắt các phân phối thiết kế.

Chi tiết cho việc điều chỉnh TPE, thuật toán tiến hóa. Chúng tôi minh họa cách kết hợp các thuật toán tìm kiếm với các tiên nghiệm thiết kế được chuyển giao. TPE là một phương pháp tối ưu hóa siêu tham số Bayesian, có nghĩa là nó được khởi tạo với một phân phối tiên nghiệm để mô hình hóa không gian tìm kiếm và cập nhật tiên nghiệm khi nó đánh giá các cấu hình siêu tham số và ghi lại hiệu suất của chúng. Chúng tôi thay thế phân phối tiên nghiệm này bằng các tiên nghiệm thiết kế được thông tin nhiệm vụ. Vì các thuật toán tiến hóa thường khởi tạo một quần thể lớn và lặp đi lặp lại loại bỏ và đột biến các mạng hiện có, chúng tôi thay thế việc khởi tạo mạng ngẫu nhiên bằng các tiên nghiệm thiết kế được thông tin nhiệm vụ. Vì chúng tôi chủ yếu tập trung vào chế độ tìm kiếm ít thử nghiệm, chúng tôi đặt các thử nghiệm khởi động hoàn toàn ngẫu nhiên thành 5 cho cả thuật toán TPE và tiến hóa.

Bảng 3: Các lựa chọn thiết kế trong không gian tìm kiếm của chúng tôi
Loại Lựa chọn
Convolution GeneralConv, GCNConv, SAGEConv, GINConv, GATConv
Số đầu 1, 2, 4
Aggregation Sum, Mean-Pooling, Max-Pooling
Activation ReLU, pReLU, leaky_ReLU, ELU
Kích thước ẩn 64, 256
Kết nối lớp Stack, Skip-Sum, Skip-Concat
Lớp tiền xử lý 1, 2
Lớp truyền tin 2, 4, 6, 8
Lớp hậu xử lý 2, 3
Tốc độ học 0.1, 0.001
Epoch đào tạo 200, 800, 1600

B THẢO LUẬN BỔ SUNG
Hạn chế. Về nguyên tắc, AUTOTRANSFER tận dụng tương quan giữa các xếp hạng hiệu suất mô hình giữa các nhiệm vụ để xây dựng hiệu quả các tiên nghiệm mô hình. Do đó, nó ít hiệu quả hơn nếu nhiệm vụ mới có khoảng cách nhiệm vụ lớn đối với tất cả các nhiệm vụ trong ngân hàng nhiệm vụ-mô hình. Trong thực tế, người dùng có thể liên tục thêm các thử nghiệm tìm kiếm bổ sung vào ngân hàng. Khi kích thước ngân hàng tăng lên, sẽ ít có khả năng một nhiệm vụ mới có tương quan thấp với tất cả các nhiệm vụ trong ngân hàng.

14

--- TRANG 15 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
Thuật toán 2 Quy trình Đào tạo cho hàm chiếu g()
Yêu cầu: Các đặc trưng nhiệm vụ {z(i)f|T(i)} được trích xuất cho mỗi nhiệm vụ từ ngân hàng nhiệm vụ-mô hình. Khoảng cách đo dg(·;·) được ước tính trong GraphGym.
1: for mỗi lần lặp do
2: Lấy mẫu T(i), T(j), T(k)
3: z(i)e; z(j)e; z(k)e ← g(z(i)f); g(z(j)f); g(z(k)f)
4: y ← 1 nếu dg(T(i); T(j)) < dg(T(i); T(k)) else −1
5: Tối ưu hóa hàm mục tiêu Lr(z(i)e; z(j)e; z(k)e; y) trong Eq. 2
6: end for

Tác động Xã hội. Mục tiêu dài hạn của chúng tôi là cung cấp một cơ sở hạ tầng GNN liền mạch đơn giản hóa việc đào tạo và triển khai các mô hình ML trên dữ liệu có cấu trúc. Các thuật toán AutoML hiệu quả và mạnh mẽ rất quan trọng để làm cho học sâu dễ tiếp cận hơn với những người quan tâm nhưng thiếu chuyên môn học sâu cũng như những người thiếu ngân sách tính toán khổng lồ mà AutoML truyền thống yêu cầu. Chúng tôi tin rằng bài báo này là một bước quan trọng để cung cấp các công cụ AI cho một dân số rộng lớn hơn và do đó cho phép AI giúp tăng cường năng suất con người. Các tập dữ liệu chúng tôi sử dụng cho thí nghiệm nằm trong số các điểm chuẩn được sử dụng rộng rãi nhất, không nên chứa bất kỳ thiên kiến không mong muốn nào. Bên cạnh đó, mất mát/độ chính xác đào tạo/test là các thống kê được tóm tắt cao mà chúng tôi tin không nên gây ra các vấn đề về quyền riêng tư tiềm năng.

C KẾT QUẢ BỔ SUNG
Hiệu quả tìm kiếm. Chúng tôi tóm tắt số lượng thử nghiệm trung bình cần thiết để vượt qua độ chính xác tốt nhất trung bình được tìm thấy bởi TPE với 30 thử nghiệm trong Bảng 4. Chúng tôi chỉ ra rằng AUTOTRANSFER giảm số lượng kiến trúc được khám phá xuống một bậc độ lớn.

Bảng 4: Số lượng thử nghiệm tìm kiếm trung bình cần thiết để vượt qua kết quả tốt nhất trung bình được tìm thấy bởi TPE với 30 thử nghiệm

Nút Đồ thị
Physics CoraFull OGB-Arxiv COX2 IMDB PROTEINS
Số Thử nghiệm 3 2 3 4 3 6
Độ chính xác 96.64±0.42 67.85 ± 1.31 71.42 ± 0.39 82.96±1.75 52.33 ± 2.13 80.21 ± 1.21

Nghiên cứu loại bỏ về số lượng mô hình neo và thiết kế nhúng nhiệm vụ. Chúng tôi chứng minh thực nghiệm cách số lượng mô hình neo ảnh hưởng đến tương quan xếp hạng trong Bảng 5. Trong khi 3 mô hình neo không đủ để nắm bắt khoảng cách nhiệm vụ, chúng tôi thấy rằng 9 và 12 có sự đánh đổi thỏa đáng giữa việc nắm bắt khoảng cách nhiệm vụ và hiệu quả tính toán. Hơn nữa, chúng tôi chứng minh thực nghiệm trong Bảng 6 rằng không gian nhúng nhiệm vụ đã học tốt hơn không gian đặc trưng nhiệm vụ được đề xuất về mặt tương quan cũng như hiệu suất tìm kiếm cuối cùng.

Bảng 5: Tương quan xếp hạng Kendall trung bình của các xếp hạng tương tự của các nhiệm vụ khác đối với nhiệm vụ trung tâm giữa phương pháp được đề xuất và GraphGym.

Số mô hình neo 3 6 9 12
Đặc trưng Nhiệm vụ 0.03±0.34 0.11 ± 0.36 0.16 ± 0.34 0.18 ± 0.30
Nhúng Nhiệm vụ 0.12±0.28 0.26 ± 0.30 0.36 ± 0.24 0.43 ± 0.22

Hình dung các thiết kế mô hình. Chúng tôi hình dung các phân phối thiết kế được chuyển giao và phân phối thiết kế sự thật gốc trên tập dữ liệu TU-PROTEINS trong Hình 5, cũng như tập dữ liệu Coauthor-Physics trong Hình 6. Chúng ta có thể quan sát rằng các phân phối thiết kế được chuyển giao có tương quan tích cực trên hầu hết các lựa chọn thiết kế.

15

--- TRANG 16 ---
Đăng tải như một bài báo hội nghị tại ICLR 2023
Bảng 6: Nghiên cứu loại bỏ về số lượng mô hình neo cũng như nhúng nhiệm vụ so với đặc trưng nhiệm vụ trên OGB-Arxiv với 3 thử nghiệm. Chúng tôi báo cáo độ chính xác test trung bình và độ lệch chuẩn qua mười lần chạy.

Số mô hình neo 3 6 9 12
Đặc trưng Nhiệm vụ 69.42±0.82 69.86 ± 0.78 70.41 ± 0.59 70.67 ± 0.52
Nhúng Nhiệm vụ 69.80±0.75 70.59 ± 0.63 71.16 ± 0.47 71.42 ± 0.39

1 2 0.0 0.1 0.2 0.3 0.4 0.5 gnn.layers_pre_mp
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

2 4 6 8 0.0 0.1 0.2 0.3 0.4 gnn.layers_mp
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

2 3 0.0 0.2 0.4 0.6 0.8 gnn.layers_post_mp
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

skipconcat skipsum stack 0.0 0.1 0.2 0.3 0.4 gnn.stage_type
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

add max mean 0.0 0.1 0.2 0.3 0.4 0.5 gnn.agg
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

256 64 0.0 0.1 0.2 0.3 0.4 0.5 0.6 gnn.dim_inner
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

gatconv gatconv2head gatconv4head generalconv ginconv sageconv gcnconv 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 gnn.layer_type
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

elu lrelu_01 prelu relu 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 gnn.act
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

0.001 0.01 0.0 0.1 0.2 0.3 0.4 0.5 0.6 optim.base_lr
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

1600 200 800 0.0 0.1 0.2 0.3 0.4 0.5 optim.max_epoch
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

Hình 5: Chúng tôi vẽ các phân phối thiết kế được chuyển giao và phân phối thiết kế sự thật gốc trên tập dữ liệu TU-PROTEINS.

1 2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 gnn.layers_pre_mp
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

2 4 6 8 0.0 0.1 0.2 0.3 0.4 gnn.layers_mp
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

2 3 0.0 0.2 0.4 0.6 0.8 gnn.layers_post_mp
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

skipconcat skipsum stack 0.0 0.2 0.4 0.6 0.8 1.0 gnn.stage_type
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

add max mean 0.0 0.1 0.2 0.3 0.4 gnn.agg
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

256 64 0.0 0.2 0.4 0.6 0.8 gnn.dim_inner
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

gatconv gatconv2head gatconv4head generalconv ginconv sageconv gcnconv 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 gnn.layer_type
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

elu lrelu_01 prelu relu 0.0 0.1 0.2 0.3 0.4 0.5 0.6 gnn.act
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

0.001 0.01 0.0 0.1 0.2 0.3 0.4 0.5 0.6 optim.base_lr
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

1600 200 800 0.0 0.1 0.2 0.3 0.4 0.5 optim.max_epoch
Tiên nghiệm Thiết kế Được Chuyển giao
Tiên nghiệm Sự thật Gốc

Hình 6: Chúng tôi vẽ các phân phối thiết kế được chuyển giao và phân phối thiết kế sự thật gốc trên tập dữ liệu Coauthor-Physics.

16
