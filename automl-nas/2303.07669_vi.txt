# 2303.07669.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/automl-nas/2303.07669.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1052276 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
AUTOTRANSFER : A UTOML Vá»šI CHUYá»‚N GIAO
KIáº¾N THá»¨C - á»¨NG Dá»¤NG CHO Máº NG NEURAL Äá»’ THá»Š
Kaidi Cao Jiaxuan You Jiaju Liu Jure Leskovec
Khoa Khoa há»c MÃ¡y tÃ­nh, Äáº¡i há»c Stanford
{kaidicao, jiaxuan, jiajuliu, jure}@cs.stanford.edu
TÃ“M Táº®T
AutoML Ä‘Ã£ chá»©ng minh thÃ nh cÃ´ng Ä‘Ã¡ng ká»ƒ trong viá»‡c tÃ¬m kiáº¿m má»™t kiáº¿n trÃºc máº¡ng neural hiá»‡u quáº£ cho má»™t nhiá»‡m vá»¥ há»c mÃ¡y cá»¥ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi má»™t táº­p dá»¯ liá»‡u vÃ  má»™t thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡. Tuy nhiÃªn, háº§u háº¿t cÃ¡c ká»¹ thuáº­t AutoML hiá»‡n táº¡i xem xÃ©t má»—i nhiá»‡m vá»¥ má»™t cÃ¡ch Ä‘á»™c láº­p tá»« Ä‘áº§u, Ä‘iá»u nÃ y yÃªu cáº§u khÃ¡m phÃ¡ nhiá»u kiáº¿n trÃºc, dáº«n Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n cao. á» Ä‘Ã¢y chÃºng tÃ´i Ä‘á» xuáº¥t AUTOTRANSFER, má»™t giáº£i phÃ¡p AutoML cáº£i thiá»‡n hiá»‡u quáº£ tÃ¬m kiáº¿m báº±ng cÃ¡ch chuyá»ƒn giao kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc trÆ°á»›c Ä‘Ã³ cho nhiá»‡m vá»¥ má»›i quan tÃ¢m. Äá»•i má»›i chÃ­nh cá»§a chÃºng tÃ´i bao gá»“m má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh náº¯m báº¯t hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc vÃ  nhiá»‡m vá»¥ GNN, vÃ  má»™t nhÃºng nhiá»‡m vá»¥ hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘o lÆ°á»ng chÃ­nh xÃ¡c sá»± tÆ°Æ¡ng tá»± giá»¯a cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau. Dá»±a trÃªn ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh vÃ  cÃ¡c nhÃºng nhiá»‡m vá»¥, chÃºng tÃ´i Æ°á»›c tÃ­nh cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ cá»§a cÃ¡c mÃ´ hÃ¬nh mong muá»‘n cho nhiá»‡m vá»¥ má»›i, báº±ng cÃ¡ch tá»•ng há»£p má»™t tá»•ng cÃ³ trá»ng sá»‘ tÆ°Æ¡ng tá»± cá»§a K phÃ¢n phá»‘i thiáº¿t káº¿ hÃ ng Ä‘áº§u trÃªn cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± vá»›i nhiá»‡m vá»¥ quan tÃ¢m. CÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i báº¥t ká»³ thuáº­t toÃ¡n tÃ¬m kiáº¿m AutoML nÃ o. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER trÃªn sÃ¡u táº­p dá»¯ liá»‡u trong lÄ©nh vá»±c há»c mÃ¡y Ä‘á»“ thá»‹. CÃ¡c thÃ­ nghiá»‡m chá»©ng minh ráº±ng (i) nhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£, vÃ  cÃ¡c nhiá»‡m vá»¥ cÃ³ nhÃºng tÆ°Æ¡ng tá»± cÃ³ cÃ¡c kiáº¿n trÃºc hoáº¡t Ä‘á»™ng tá»‘t nháº¥t tÆ°Æ¡ng tá»±; (ii) AUTOTRANSFER cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ tÃ¬m kiáº¿m vá»›i cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao, giáº£m sá»‘ lÆ°á»£ng kiáº¿n trÃºc Ä‘Æ°á»£c khÃ¡m phÃ¡ xuá»‘ng má»™t báº­c Ä‘á»™ lá»›n. Cuá»‘i cÃ¹ng, chÃºng tÃ´i phÃ¡t hÃ nh GNN-B ANK -101, má»™t táº­p dá»¯ liá»‡u quy mÃ´ lá»›n vá» thÃ´ng tin Ä‘Ã o táº¡o GNN chi tiáº¿t cá»§a 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o Ä‘iá»u kiá»‡n vÃ  truyá»n cáº£m há»©ng cho nghiÃªn cá»©u tÆ°Æ¡ng lai.

1 GIá»šI THIá»†U
Máº¡ng neural sÃ¢u cÃ³ tÃ­nh mÃ´-Ä‘un cao, yÃªu cáº§u nhiá»u quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ pháº£i Ä‘Æ°á»£c Ä‘Æ°a ra liÃªn quan Ä‘áº¿n kiáº¿n trÃºc máº¡ng vÃ  cÃ¡c siÃªu tham sá»‘. Nhá»¯ng quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ nÃ y táº¡o thÃ nh má»™t khÃ´ng gian tÃ¬m kiáº¿m khÃ´ng lá»“i vÃ  tá»‘n kÃ©m ngay cáº£ Ä‘á»‘i vá»›i cÃ¡c chuyÃªn gia Ä‘á»ƒ tá»‘i Æ°u hÃ³a, Ä‘áº·c biá»‡t khi viá»‡c tá»‘i Æ°u hÃ³a pháº£i Ä‘Æ°á»£c láº·p láº¡i tá»« Ä‘áº§u cho má»—i trÆ°á»ng há»£p sá»­ dá»¥ng má»›i. Há»c mÃ¡y tá»± Ä‘á»™ng (AutoML) lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u tÃ­ch cá»±c nháº±m giáº£m ná»— lá»±c con ngÆ°á»i cáº§n thiáº¿t cho thiáº¿t káº¿ kiáº¿n trÃºc thÆ°á»ng bao gá»“m tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ vÃ  tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng neural. AutoML Ä‘Ã£ chá»©ng minh thÃ nh cÃ´ng (Zoph vÃ  Le, 2016; Pham et al., 2018; Zoph et al., 2018; Cai et al., 2018; He et al., 2018; Guo et al., 2020; Erickson et al., 2020; LeDell vÃ  Poirier, 2020) trong nhiá»u lÄ©nh vá»±c á»©ng dá»¥ng.

Viá»‡c tÃ¬m má»™t mÃ´ hÃ¬nh tÆ°Æ¡ng Ä‘á»‘i tá»‘t cho má»™t nhiá»‡m vá»¥ há»c táº­p má»›iÂ¹ má»™t cÃ¡ch hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n lÃ  ráº¥t quan trá»ng Ä‘á»ƒ lÃ m cho há»c sÃ¢u trá»Ÿ nÃªn dá»… tiáº¿p cáº­n vá»›i cÃ¡c chuyÃªn gia lÄ©nh vá»±c cÃ³ ná»n táº£ng Ä‘a dáº¡ng. AutoML hiá»‡u quáº£ Ä‘áº·c biá»‡t quan trá»ng trong cÃ¡c lÄ©nh vá»±c nÆ¡i cÃ¡c kiáº¿n trÃºc/siÃªu tham sá»‘ tá»‘t nháº¥t ráº¥t nháº¡y cáº£m vá»›i nhiá»‡m vá»¥. Má»™t vÃ­ dá»¥ Ä‘Ã¡ng chÃº Ã½ lÃ  lÄ©nh vá»±c há»c Ä‘á»“ thá»‹Â². Thá»© nháº¥t, cÃ¡c phÆ°Æ¡ng phÃ¡p há»c Ä‘á»“ thá»‹ nháº­n dá»¯ liá»‡u Ä‘áº§u vÃ o bao gá»“m nhiá»u loáº¡i dá»¯ liá»‡u khÃ¡c nhau vÃ  tá»‘i Æ°u hÃ³a trÃªn cÃ¡c nhiá»‡m vá»¥ tráº£i rá»™ng má»™t táº­p há»£p cÃ¡c lÄ©nh vá»±c vÃ  phÆ°Æ¡ng thá»©c Ä‘a dáº¡ng nhÆ° khuyáº¿n nghá»‹ (Ying et al., 2018; He et al., 2020), mÃ´ phá»ng váº­t lÃ½ (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2020), vÃ  tin sinh há»c (Zitnik et al., 2018). Äiá»u nÃ y khÃ¡c vá»›i thá»‹ giÃ¡c mÃ¡y tÃ­nh vÃ  xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn nÆ¡i

Â¹Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i gá»i má»™t nhiá»‡m vá»¥ nhÆ° má»™t táº­p dá»¯ liá»‡u cho trÆ°á»›c vá»›i má»™t thÆ°á»›c Ä‘o/máº¥t mÃ¡t Ä‘Ã¡nh giÃ¡, vÃ­ dá»¥, máº¥t mÃ¡t entropy chÃ©o trÃªn phÃ¢n loáº¡i nÃºt trÃªn táº­p dá»¯ liá»‡u Cora.
Â²ChÃºng tÃ´i táº­p trung vÃ o lÄ©nh vá»±c há»c Ä‘á»“ thá»‹ trong bÃ i bÃ¡o nÃ y. AUTOTRANSFER cÃ³ thá»ƒ Ä‘Æ°á»£c tá»•ng quÃ¡t hÃ³a sang cÃ¡c lÄ©nh vá»±c khÃ¡c.

1arXiv:2303.07669v1  [cs.LG]  14 Mar 2023

--- TRANG 2 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
dá»¯ liá»‡u Ä‘áº§u vÃ o cÃ³ cáº¥u trÃºc Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c, cá»‘ Ä‘á»‹nh cÃ³ thá»ƒ Ä‘Æ°á»£c chia sáº» giá»¯a cÃ¡c kiáº¿n trÃºc máº¡ng neural khÃ¡c nhau.
Thá»© hai, máº¡ng neural hoáº¡t Ä‘á»™ng trÃªn Ä‘á»“ thá»‹ Ä‘i kÃ¨m vá»›i má»™t táº­p há»£p phong phÃº cÃ¡c lá»±a chá»n thiáº¿t káº¿ vÃ  má»™t táº­p há»£p lá»›n cÃ¡c tham sá»‘ Ä‘á»ƒ khÃ¡m phÃ¡. Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° cÃ¡c lÄ©nh vá»±c khÃ¡c nÆ¡i má»™t sá»‘ kiáº¿n trÃºc Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c nhÆ° ResNet (He et al., 2016) vÃ  GPT-3 (Brown et al., 2020) thá»‘ng trá»‹ cÃ¡c Ä‘iá»ƒm chuáº©n, Ä‘Ã£ Ä‘Æ°á»£c chá»‰ ra ráº±ng thiáº¿t káº¿ máº¡ng neural Ä‘á»“ thá»‹ (GNN) tá»‘t nháº¥t phá»¥ thuá»™c ráº¥t nhiá»u vÃ o nhiá»‡m vá»¥ (You et al., 2020).

Máº·c dÃ¹ AutoML nhÆ° má»™t lÄ©nh vá»±c nghiÃªn cá»©u Ä‘ang phÃ¡t triá»ƒn nhanh chÃ³ng, cÃ¡c giáº£i phÃ¡p AutoML hiá»‡n táº¡i cÃ³ chi phÃ­ tÃ­nh toÃ¡n khá»•ng lá»“ khi má»¥c tiÃªu lÃ  tÃ¬m má»™t mÃ´ hÃ¬nh tá»‘t cho má»™t nhiá»‡m vá»¥ há»c táº­p má»›i. Háº§u háº¿t cÃ¡c ká»¹ thuáº­t AutoML hiá»‡n táº¡i xem xÃ©t má»—i nhiá»‡m vá»¥ má»™t cÃ¡ch Ä‘á»™c láº­p vÃ  biá»‡t láº­p, do Ä‘Ã³ chÃºng yÃªu cáº§u lÃ m láº¡i viá»‡c tÃ¬m kiáº¿m tá»« Ä‘áº§u cho má»—i nhiá»‡m vá»¥ má»›i. CÃ¡ch tiáº¿p cáº­n nÃ y bá» qua kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc cÃ³ giÃ¡ trá»‹ tiá»m nÄƒng thu Ä‘Æ°á»£c tá»« cÃ¡c nhiá»‡m vá»¥ trÆ°á»›c Ä‘Ã³, vÃ  khÃ´ng thá»ƒ trÃ¡nh khá»i dáº«n Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n cao.
Váº¥n Ä‘á» Ä‘áº·c biá»‡t nghiÃªm trá»ng trong lÄ©nh vá»±c há»c Ä‘á»“ thá»‹ Gao et al. (2019); Zhou et al. (2019), do cÃ¡c thÃ¡ch thá»©c cá»§a cÃ¡c loáº¡i nhiá»‡m vá»¥ Ä‘a dáº¡ng vÃ  khÃ´ng gian thiáº¿t káº¿ lá»›n Ä‘Æ°á»£c tháº£o luáº­n á»Ÿ trÃªn.

á» Ä‘Ã¢y chÃºng tÃ´i Ä‘á» xuáº¥t AUTOTRANSFERÂ³, má»™t giáº£i phÃ¡p AutoML cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ tÃ¬m kiáº¿m kiáº¿n trÃºc AutoML báº±ng cÃ¡ch chuyá»ƒn giao kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc trÆ°á»›c Ä‘Ã³ cho nhiá»‡m vá»¥ quan tÃ¢m. Äá»•i má»›i chÃ­nh cá»§a chÃºng tÃ´i lÃ  giá»›i thiá»‡u má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh lÆ°u trá»¯ hiá»‡u suáº¥t cá»§a má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc vÃ  nhiá»‡m vá»¥ GNN Ä‘á»ƒ hÆ°á»›ng dáº«n thuáº­t toÃ¡n tÃ¬m kiáº¿m. Äá»ƒ cho phÃ©p chuyá»ƒn giao kiáº¿n thá»©c, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a má»™t khÃ´ng gian nhÃºng nhiá»‡m vá»¥ sao cho cÃ¡c nhiá»‡m vá»¥ gáº§n nhau trong khÃ´ng gian nhÃºng cÃ³ cÃ¡c kiáº¿n trÃºc hoáº¡t Ä‘á»™ng tá»‘t nháº¥t tÆ°Æ¡ng á»©ng tÆ°Æ¡ng tá»±. ThÃ¡ch thá»©c á»Ÿ Ä‘Ã¢y lÃ  nhÃºng nhiá»‡m vá»¥ cáº§n náº¯m báº¯t cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c kiáº¿n trÃºc khÃ¡c nhau trÃªn cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c nhau, trong khi hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n.

Äá»•i má»›i cá»§a chÃºng tÃ´i á»Ÿ Ä‘Ã¢y lÃ  nhÃºng má»™t nhiá»‡m vá»¥ báº±ng cÃ¡ch sá»­ dá»¥ng sá»‘ Ä‘iá»u kiá»‡n cá»§a Ma tráº­n ThÃ´ng tin Fisher cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn khÃ¡c nhau vÃ  cÅ©ng má»™t lÆ°á»£c Ä‘á»“ há»c táº­p vá»›i báº£o Ä‘áº£m tá»•ng quÃ¡t hÃ³a thá»±c nghiá»‡m. Báº±ng cÃ¡ch nÃ y chÃºng tÃ´i ngáº§m náº¯m báº¯t cÃ¡c thuá»™c tÃ­nh cá»§a nhiá»‡m vá»¥ há»c táº­p, trong khi nhanh hÆ¡n nhiá»u báº­c Ä‘á»™ lá»›n (trong vÃ i giÃ¢y). Sau Ä‘Ã³ chÃºng tÃ´i Æ°á»›c tÃ­nh tiÃªn nghiá»‡m thiáº¿t káº¿ cá»§a cÃ¡c mÃ´ hÃ¬nh mong muá»‘n cho nhiá»‡m vá»¥ má»›i, báº±ng cÃ¡ch tá»•ng há»£p cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ trÃªn cÃ¡c nhiá»‡m vá»¥ gáº§n vá»›i nhiá»‡m vá»¥ quan tÃ¢m. Cuá»‘i cÃ¹ng, chÃºng tÃ´i khá»Ÿi táº¡o má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m siÃªu tham sá»‘ vá»›i tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng tin nhiá»‡m vá»¥ tÃ­nh toÃ¡n.

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER trÃªn sÃ¡u táº­p dá»¯ liá»‡u, bao gá»“m cáº£ cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i nÃºt vÃ  phÃ¢n loáº¡i Ä‘á»“ thá»‹. ChÃºng tÃ´i chá»‰ ra ráº±ng cÃ¡c nhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£ vÃ  khoáº£ng cÃ¡ch Ä‘Æ°á»£c Ä‘o giá»¯a cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng quan cao (0,43 tÆ°Æ¡ng quan Kendall) vá»›i cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t mÃ´ hÃ¬nh. HÆ¡n ná»¯a, chÃºng tÃ´i trÃ¬nh bÃ y AUTOTRANSFER cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ tÃ¬m kiáº¿m khi sá»­ dá»¥ng tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao. AUTOTRANSFER giáº£m sá»‘ lÆ°á»£ng kiáº¿n trÃºc Ä‘Æ°á»£c khÃ¡m phÃ¡ cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c má»¥c tiÃªu xuá»‘ng má»™t báº­c Ä‘á»™ lá»›n so vá»›i SOTA. Cuá»‘i cÃ¹ng, chÃºng tÃ´i phÃ¡t hÃ nh GNN-B ANK-101 â€”cÆ¡ sá»Ÿ dá»¯ liá»‡u quy mÃ´ lá»›n Ä‘áº§u tiÃªn chá»©a cÃ¡c báº£n ghi hiá»‡u suáº¥t chi tiáº¿t cho 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o vá»›i 16.128 giá» GPUâ€”Ä‘á»ƒ táº¡o Ä‘iá»u kiá»‡n cho nghiÃªn cá»©u tÆ°Æ¡ng lai.

2 CÃ”NG TRÃŒNH LIÃŠN QUAN
Trong pháº§n nÃ y, chÃºng tÃ´i tÃ³m táº¯t cÃ´ng trÃ¬nh liÃªn quan vá» AutoML liÃªn quan Ä‘áº¿n cÃ¡c á»©ng dá»¥ng cá»§a nÃ³ trÃªn GNN, cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m phá»• biáº¿n, vÃ  cÃ´ng trÃ¬nh tiÃªn phong liÃªn quan Ä‘áº¿n há»c chuyá»ƒn giao vÃ  nhÃºng nhiá»‡m vá»¥.

AutoML cho GNN. TÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng neural (NAS), má»™t hÃ¬nh thá»©c Ä‘á»™c Ä‘Ã¡o vÃ  phá»• biáº¿n cá»§a AutoML cho há»c sÃ¢u, cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh hai loáº¡i: NAS Ä‘a thá»­ nghiá»‡m vÃ  NAS má»™t láº§n. Trong NAS Ä‘a thá»­ nghiá»‡m, má»—i kiáº¿n trÃºc Ä‘Æ°á»£c láº¥y máº«u Ä‘Æ°á»£c Ä‘Ã o táº¡o riÃªng biá»‡t. GraphNAS (Gao et al., 2020) vÃ  Auto-GNN (Zhou et al., 2019) lÃ  cÃ¡c thuáº­t toÃ¡n NAS Ä‘a thá»­ nghiá»‡m Ä‘iá»ƒn hÃ¬nh trÃªn GNN Ã¡p dá»¥ng má»™t bá»™ Ä‘iá»u khiá»ƒn RNN há»c Ä‘á»ƒ Ä‘á» xuáº¥t cÃ¡c táº­p há»£p cáº¥u hÃ¬nh tá»‘t hÆ¡n thÃ´ng qua há»c tÄƒng cÆ°á»ng.
Má»™t-láº§n NAS (vÃ­ dá»¥, (Liu et al., 2018; Qin et al., 2021; Li et al., 2021)) bao gá»“m viá»‡c Ä‘Ã³ng gÃ³i toÃ n bá»™ khÃ´ng gian mÃ´ hÃ¬nh trong má»™t siÃªu mÃ´ hÃ¬nh, Ä‘Ã o táº¡o siÃªu mÃ´ hÃ¬nh má»™t láº§n, vÃ  sau Ä‘Ã³ láº·p Ä‘i láº·p láº¡i láº¥y máº«u cÃ¡c mÃ´ hÃ¬nh con tá»« siÃªu mÃ´ hÃ¬nh Ä‘á»ƒ tÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t. NgoÃ i ra, cÃ³ cÃ´ng trÃ¬nh nghiÃªn cá»©u rÃµ rÃ ng cÃ¡c lá»±a chá»n thiáº¿t káº¿ chi tiáº¿t nhÆ° tÄƒng cÆ°á»ng dá»¯ liá»‡u (You et al., 2021), loáº¡i lá»›p truyá»n tin (Cai et al., 2021; Ding et al., 2021; Zhao et al., 2021), vÃ  gá»™p Ä‘á»“ thá»‹ (Wei et al., 2021).
ÄÃ¡ng chÃº Ã½, AUTOTRANSFER lÃ  giáº£i phÃ¡p AutoML Ä‘áº§u tiÃªn cho GNN chuyá»ƒn giao hiá»‡u quáº£ kiáº¿n thá»©c thiáº¿t káº¿ qua cÃ¡c nhiá»‡m vá»¥.

Thuáº­t toÃ¡n HPO. CÃ¡c thuáº­t toÃ¡n Tá»‘i Æ°u hÃ³a SiÃªu tham sá»‘ (HPO) tÃ¬m kiáº¿m cÃ¡c siÃªu tham sá»‘ mÃ´ hÃ¬nh tá»‘i Æ°u báº±ng cÃ¡ch láº·p Ä‘i láº·p láº¡i Ä‘á» xuáº¥t má»™t táº­p há»£p siÃªu tham sá»‘ vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a chÃºng.
TÃ¬m kiáº¿m ngáº«u nhiÃªn láº¥y máº«u siÃªu tham sá»‘ tá»« khÃ´ng gian tÃ¬m kiáº¿m vá»›i xÃ¡c suáº¥t báº±ng nhau. Máº·c dÃ¹ khÃ´ng

Â³MÃ£ nguá»“n cÃ³ sáºµn táº¡i https://github.com/snap-stanford/AutoTransfer .

2

--- TRANG 3 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
NgÃ¢n hÃ ng Nhiá»‡m vá»¥-MÃ´ hÃ¬nh (báº£n ghi hiá»‡u suáº¥t lá»‹ch sá»­) KhÃ´ng gian NhÃºng Nhiá»‡m vá»¥ PhÃ¢n phá»‘i Thiáº¿t káº¿
Nhiá»‡m vá»¥ Agg Dim Epoch ... Val_loss
Cora sum 64 80 ... 0.22
Cora mean 128 200 ... 0.26
TU-DD sum 64 200 ... 0.46
TU-DD mean 128 200 ... 0.86
TU-DD max 256 800 ... 0.52
Arxiv mean 128 200 ... 0.68
... ... ... ... ... ...

Agg Dim Epoch
Agg

Nhiá»‡m vá»¥ má»›i quan tÃ¢m Dimâ€‹ Epoch... â€‹... â€‹

HÃ¬nh 1: Tá»•ng quan vá» A UTOTRANSFER. TrÃ¡i: ChÃºng tÃ´i giá»›i thiá»‡u GNN-B ANK -101, má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u lá»›n chá»©a má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc vÃ  siÃªu tham sá»‘ GNN Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau, cÃ¹ng vá»›i thá»‘ng kÃª Ä‘Ã o táº¡o/Ä‘Ã¡nh giÃ¡ cá»§a chÃºng. Giá»¯a: ChÃºng tÃ´i giá»›i thiá»‡u má»™t khÃ´ng gian nhÃºng nhiá»‡m vá»¥, nÆ¡i má»—i Ä‘iá»ƒm tÆ°Æ¡ng á»©ng vá»›i má»™t nhiá»‡m vá»¥ khÃ¡c nhau. CÃ¡c nhiá»‡m vá»¥ gáº§n nhau trong khÃ´ng gian nhÃºng cÃ³ cÃ¡c mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t tÆ°Æ¡ng á»©ng tÆ°Æ¡ng tá»±. Pháº£i: Cho má»™t nhiá»‡m vá»¥ má»›i quan tÃ¢m, chÃºng tÃ´i hÆ°á»›ng dáº«n tÃ¬m kiáº¿m AutoML báº±ng cÃ¡ch tham kháº£o cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ cá»§a cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± nháº¥t trong khÃ´ng gian nhÃºng nhiá»‡m vá»¥.

há»c tá»« cÃ¡c thá»­ nghiá»‡m trÆ°á»›c Ä‘Ã³, tÃ¬m kiáº¿m ngáº«u nhiÃªn thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vÃ¬ tÃ­nh Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ hÆ¡n nhiá»u so vá»›i tÃ¬m kiáº¿m lÆ°á»›i (Bergstra vÃ  Bengio, 2012). Thuáº­t toÃ¡n TPE (Bergstra et al., 2011) xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh xÃ¡c suáº¥t vá» hiá»‡u suáº¥t nhiá»‡m vá»¥ trÃªn khÃ´ng gian siÃªu tham sá»‘ vÃ  sá»­ dá»¥ng káº¿t quáº£ cá»§a cÃ¡c thá»­ nghiá»‡m trÆ°á»›c Ä‘á»ƒ chá»n cáº¥u hÃ¬nh há»©a háº¹n nháº¥t tiáº¿p theo Ä‘á»ƒ Ä‘Ã o táº¡o, mÃ  thuáº­t toÃ¡n TPE Ä‘á»‹nh nghÄ©a lÃ  tá»‘i Ä‘a hÃ³a giÃ¡ trá»‹ Cáº£i thiá»‡n Ká»³ vá»ng (Jones, 2001). CÃ¡c thuáº­t toÃ¡n tiáº¿n hÃ³a (Real et al., 2017; Jaderberg et al., 2017) Ä‘Ã o táº¡o nhiá»u mÃ´ hÃ¬nh song song vÃ  thay tháº¿ cÃ¡c mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng kÃ©m báº±ng cÃ¡c báº£n sao "Ä‘á»™t biáº¿n" cá»§a cÃ¡c mÃ´ hÃ¬nh tá»‘t nháº¥t hiá»‡n táº¡i. AUTOTRANSFER lÃ  má»™t giáº£i phÃ¡p AutoML tá»•ng quÃ¡t vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng káº¿t há»£p vá»›i báº¥t ká»³ thuáº­t toÃ¡n HPO nÃ o trong sá»‘ nÃ y.

Há»c Chuyá»ƒn giao trong AutoML. Wong et al. (2018) Ä‘á» xuáº¥t chuyá»ƒn giao kiáº¿n thá»©c qua cÃ¡c nhiá»‡m vá»¥ báº±ng cÃ¡ch táº£i láº¡i bá»™ Ä‘iá»u khiá»ƒn cá»§a cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m há»c tÄƒng cÆ°á»ng. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y giáº£ Ä‘á»‹nh ráº±ng khÃ´ng gian tÃ¬m kiáº¿m trÃªn cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau báº¯t Ä‘áº§u vá»›i cÃ¹ng má»™t tiÃªn nghiá»‡m Ä‘Ã£ há»c. KhÃ´ng giá»‘ng nhÆ° AUTOTRANSFER, nÃ³ khÃ´ng thá»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c cá»‘t lÃµi trong AutoML GNN: thiáº¿t káº¿ GNN tá»‘t nháº¥t phá»¥ thuá»™c ráº¥t nhiá»u vÃ o nhiá»‡m vá»¥ cá»¥ thá»ƒ.
GraphGym (You et al., 2020) cá»‘ gáº¯ng chuyá»ƒn giao thiáº¿t káº¿ kiáº¿n trÃºc tá»‘t nháº¥t trá»±c tiáº¿p vá»›i má»™t khÃ´ng gian thÆ°á»›c Ä‘o Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥. GraphGym (You et al., 2020) tÃ­nh toÃ¡n sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ báº±ng cÃ¡ch Ä‘Ã o táº¡o má»™t táº­p há»£p 12 "mÃ´ hÃ¬nh neo" Ä‘áº¿n há»™i tá»¥, Ä‘iá»u nÃ y tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n. NgÆ°á»£c láº¡i, AUTOTRANSFER thiáº¿t káº¿ cÃ¡c nhÃºng nhiá»‡m vá»¥ nháº¹ yÃªu cáº§u chi phÃ­ tÃ­nh toÃ¡n tá»‘i thiá»ƒu.
NgoÃ i ra, Zhao vÃ  Bilen (2021); Li et al. (2021) Ä‘á» xuáº¥t tiáº¿n hÃ nh tÃ¬m kiáº¿m kiáº¿n trÃºc trÃªn má»™t táº­p con proxy cá»§a toÃ n bá»™ táº­p dá»¯ liá»‡u vÃ  sau Ä‘Ã³ chuyá»ƒn giao kiáº¿n trÃºc tá»‘t nháº¥t Ä‘Æ°á»£c tÃ¬m kiáº¿m trÃªn táº­p dá»¯ liá»‡u Ä‘áº§y Ä‘á»§. Jeong et al. (2021) nghiÃªn cá»©u má»™t thiáº¿t láº­p tÆ°Æ¡ng tá»± trong lÄ©nh vá»±c thá»‹ giÃ¡c.

NhÃºng Nhiá»‡m vá»¥. CÃ³ nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y cá»‘ gáº¯ng Ä‘á»‹nh lÆ°á»£ng nhÃºng vÃ  sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥. TÆ°Æ¡ng tá»± nhÆ° GraphGym, Taskonomy (Zamir et al., 2018) Æ°á»›c tÃ­nh ma tráº­n Ã¡i lá»±c nhiá»‡m vá»¥ báº±ng cÃ¡ch tÃ³m táº¯t cÃ¡c máº¥t mÃ¡t/thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng sá»­ dá»¥ng má»™t Quy trÃ¬nh PhÃ¢n cáº¥p PhÃ¢n tÃ­ch (Saaty, 1987). Tá»« má»™t gÃ³c Ä‘á»™ khÃ¡c, Task2Vec (Achille et al., 2019) táº¡o nhÃºng nhiá»‡m vá»¥ cho má»™t nhiá»‡m vá»¥ cho trÆ°á»›c sá»­ dá»¥ng Ma tráº­n ThÃ´ng tin Fisher liÃªn káº¿t vá»›i má»™t máº¡ng thÄƒm dÃ² Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c. Máº¡ng thÄƒm dÃ² nÃ y Ä‘Æ°á»£c chia sáº» qua cÃ¡c nhiá»‡m vá»¥ vÃ  cho phÃ©p Task2Vec Æ°á»›c tÃ­nh Ma tráº­n ThÃ´ng tin Fisher cá»§a cÃ¡c táº­p dá»¯ liá»‡u hÃ¬nh áº£nh khÃ¡c nhau. Le et al. (2022) má»Ÿ rá»™ng Ã½ tÆ°á»Ÿng tÆ°Æ¡ng tá»± cho tÃ¬m kiáº¿m kiáº¿n trÃºc máº¡ng neural. CÃ¡c nhÃºng nhiá»‡m vá»¥ nÃ³i trÃªn khÃ´ng thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trá»±c tiáº¿p cho GNN vÃ¬ cÃ¡c Ä‘áº§u vÃ o khÃ´ng tháº³ng hÃ ng qua cÃ¡c táº­p dá»¯ liá»‡u.
AUTOTRANSFER trÃ¡nh nÃºt tháº¯t cá»• chai báº±ng cÃ¡ch sá»­ dá»¥ng thá»‘ng kÃª tiá»‡m cáº­n cá»§a Ma tráº­n ThÃ´ng tin Fisher vá»›i trá»ng sá»‘ Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn.

3 CÃ”NG THá»¨C Váº¤N Äá»€ VÃ€ KIáº¾N THá»¨C CÆ  Báº¢N
Äáº§u tiÃªn chÃºng tÃ´i giá»›i thiá»‡u cÃ¡c Ä‘á»‹nh nghÄ©a chÃ­nh thá»©c vá» cáº¥u trÃºc dá»¯ liá»‡u liÃªn quan Ä‘áº¿n A UTOTRANSFER.

3

--- TRANG 4 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
... ğ‘"! MÃ´ hÃ¬nh ğ‘€1 MÃ´ hÃ¬nh ğ‘€2 MÃ´ hÃ¬nh ğ‘€ğ‘ˆ R Khá»Ÿi táº¡o Ngáº«u nhiÃªn FIM ThÆ°á»›c Ä‘o Báº¥t biáº¿n Tá»· lá»‡ Äáº·c trÆ°ng Nhiá»‡m vá»¥ NhÃºng Nhiá»‡m vá»¥ ğ‘” ğ‘“ ğ‘“ ğ‘“ Nhiá»‡m vá»¥ i Má»¥c tiÃªu ÄÃ o táº¡o Nhiá»‡m vá»¥ j x Nhiá»‡m vá»¥ i Nhiá»‡m vá»¥ k x Máº¥t mÃ¡t Xáº¿p háº¡ng BiÃªn ğ‘"" ğ‘"# ğ‘§$ ğ‘§% ğ‘§%(') ğ‘§%()) ğ‘§%(') ğ‘§%(*)

HÃ¬nh 2: Quy trÃ¬nh trÃ­ch xuáº¥t nhÃºng nhiá»‡m vá»¥. TrÃ¡i: Äá»ƒ nhÃºng má»™t nhiá»‡m vá»¥ hiá»‡u quáº£, Ä‘áº§u tiÃªn chÃºng tÃ´i trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ báº±ng cÃ¡ch ná»‘i cÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c Ä‘o tá»« R mÃ´ hÃ¬nh neo Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. Sau Ä‘Ã³, chÃºng tÃ´i giá»›i thiá»‡u má»™t hÃ m chiáº¿u g() vá»›i trá»ng sá»‘ Ä‘Ã£ há»c Ä‘á»ƒ biáº¿n Ä‘á»•i cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ thÃ nh nhÃºng nhiá»‡m vá»¥. Pháº£i: Má»¥c tiÃªu Ä‘Ã o táº¡o Ä‘á»ƒ tá»‘i Æ°u hÃ³a g() vá»›i giÃ¡m sÃ¡t bá»™ ba.

Äá»‹nh nghÄ©a 1 (Nhiá»‡m vá»¥) ChÃºng tÃ´i kÃ½ hiá»‡u má»™t nhiá»‡m vá»¥ lÃ  T= (D;L()), bao gá»“m má»™t táº­p dá»¯ liá»‡u D vÃ  má»™t hÃ m máº¥t mÃ¡t L() liÃªn quan Ä‘áº¿n thÆ°á»›c Ä‘o Ä‘Ã¡nh giÃ¡.

Äá»‘i vá»›i má»—i láº§n thá»­ Ä‘Ã o táº¡o trÃªn má»™t nhiá»‡m vá»¥ T(i), chÃºng ta cÃ³ thá»ƒ ghi láº¡i kiáº¿n trÃºc mÃ´ hÃ¬nh Mj, siÃªu tham sá»‘ Hj, vÃ  giÃ¡ trá»‹ máº¥t mÃ¡t tÆ°Æ¡ng á»©ng lj, tá»©c lÃ , (Mj;Hj;lj). ChÃºng tÃ´i Ä‘á» xuáº¥t duy trÃ¬ má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o Ä‘iá»u kiá»‡n chuyá»ƒn giao kiáº¿n thá»©c cho cÃ¡c nhiá»‡m vá»¥ má»›i trong tÆ°Æ¡ng lai.

Äá»‹nh nghÄ©a 2 (NgÃ¢n hÃ ng Nhiá»‡m vá»¥-MÃ´ hÃ¬nh) Má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t táº­p há»£p cÃ¡c nhiá»‡m vá»¥, má»—i nhiá»‡m vá»¥ cÃ³ nhiá»u láº§n thá»­ Ä‘Ã o táº¡o, á»Ÿ dáº¡ng B=f(T(i);f(M(i)j;H(i)j;l(i)j)g)g.

AutoML vá»›i Chuyá»ƒn giao Kiáº¿n thá»©c. Giáº£ sá»­ chÃºng ta cÃ³ má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B. Cho má»™t nhiá»‡m vá»¥ má»›i T(n) chÆ°a Ä‘Æ°á»£c tháº¥y trÆ°á»›c Ä‘Ã³, má»¥c tiÃªu cá»§a chÃºng ta lÃ  nhanh chÃ³ng tÃ¬m má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tÆ°Æ¡ng Ä‘á»‘i tá»‘t trÃªn nhiá»‡m vá»¥ má»›i báº±ng cÃ¡ch sá»­ dá»¥ng kiáº¿n thá»©c tá»« ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i táº­p trung vÃ o AutoML cho cÃ¡c nhiá»‡m vá»¥ há»c Ä‘á»“ thá»‹, máº·c dÃ¹ ká»¹ thuáº­t Ä‘Æ°á»£c phÃ¡t triá»ƒn cá»§a chÃºng tÃ´i lÃ  tá»•ng quÃ¡t vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c lÄ©nh vá»±c khÃ¡c. ChÃºng tÃ´i Ä‘á»‹nh nghÄ©a Ä‘á»“ thá»‹ Ä‘áº§u vÃ o lÃ  G=fV;Eg, trong Ä‘Ã³ V lÃ  táº­p há»£p nÃºt vÃ  EâŠ†VÃ—V lÃ  táº­p há»£p cáº¡nh. HÆ¡n ná»¯a, hÃ£y y kÃ½ hiá»‡u cÃ¡c nhÃ£n Ä‘áº§u ra cá»§a nÃ³, cÃ³ thá»ƒ lÃ  cáº¥p nÃºt, cáº¥p cáº¡nh hoáº·c cáº¥p Ä‘á»“ thá»‹. Má»™t GNN Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi trá»ng sá»‘ Î¸ xuáº¥t ra má»™t phÃ¢n phá»‘i háº­u nghiá»‡m P(G;y;Î¸) cho dá»± Ä‘oÃ¡n nhÃ£n.

4 GIáº¢I PHÃP Äá»€ XUáº¤T: AUTOTRANSFER
Trong pháº§n nÃ y, chÃºng tÃ´i giá»›i thiá»‡u giáº£i phÃ¡p AUTOTRANSFER Ä‘Æ°á»£c Ä‘á» xuáº¥t. AUTOTRANSFER sá»­ dá»¥ng khÃ´ng gian nhÃºng nhiá»‡m vá»¥ nhÆ° má»™t cÃ´ng cá»¥ Ä‘á»ƒ hiá»ƒu má»©c Ä‘á»™ liÃªn quan cá»§a cÃ¡c thiáº¿t káº¿ kiáº¿n trÃºc trÆ°á»›c Ä‘Ã³ vá»›i nhiá»‡m vá»¥ má»¥c tiÃªu. NhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c thiáº¿t káº¿ náº¯m báº¯t cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c kiáº¿n trÃºc khÃ¡c nhau trÃªn cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau trong khi cÅ©ng hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n. Äáº§u tiÃªn chÃºng tÃ´i giá»›i thiá»‡u má»™t giáº£i phÃ¡p cÃ³ Ä‘á»™ng lá»±c lÃ½ thuyáº¿t Ä‘á»ƒ trÃ­ch xuáº¥t má»™t biá»ƒu diá»…n hiá»‡u suáº¥t báº¥t biáº¿n tá»· lá»‡ cá»§a má»—i cáº·p nhiá»‡m vá»¥-mÃ´ hÃ¬nh. ChÃºng tÃ´i sá»­ dá»¥ng nhá»¯ng biá»ƒu diá»…n nÃ y Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ vÃ  há»c thÃªm nhÃºng nhiá»‡m vá»¥. Nhá»¯ng nhÃºng nÃ y táº¡o thÃ nh khÃ´ng gian nhÃºng nhiá»‡m vá»¥ mÃ  cuá»‘i cÃ¹ng chÃºng tÃ´i sá»­ dá»¥ng trong quÃ¡ trÃ¬nh tÃ¬m kiáº¿m AutoML.

4.1 CÆ  Báº¢N Cá»¦A MA TRáº¬N THÃ”NG TIN FISHER (FIM)
Cho má»™t GNN Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a á»Ÿ trÃªn, Ma tráº­n ThÃ´ng tin Fisher (FIM) F cá»§a nÃ³ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ 
F=EG;y[âˆ‡Î¸logP(G;y;Î¸)âˆ‡Î¸logP(G;y;Î¸)>]:

vá» máº·t hÃ¬nh thá»©c lÃ  hiá»‡p phÆ°Æ¡ng sai ká»³ vá»ng cá»§a cÃ¡c Ä‘iá»ƒm sá»‘ Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ mÃ´ hÃ¬nh. CÃ³ hai gÃ³c nhÃ¬n hÃ¬nh há»c phá»• biáº¿n cho FIM. Thá»© nháº¥t, FIM lÃ  cáº­n trÃªn cá»§a Hessian vÃ  trÃ¹ng vá»›i Hessian náº¿u gradient báº±ng 0. Do Ä‘Ã³, FIM Ä‘áº·c trÆ°ng cho cáº£nh quan cá»¥c bá»™ cá»§a hÃ m máº¥t mÃ¡t gáº§n cá»±c tiá»ƒu toÃ n cá»¥c. Thá»© hai, tÆ°Æ¡ng tá»± nhÆ° Hessian, FIM mÃ´ hÃ¬nh cáº£nh quan máº¥t mÃ¡t khÃ´ng pháº£i Ä‘á»‘i vá»›i khÃ´ng gian Ä‘áº§u vÃ o, mÃ  Ä‘á»‘i vá»›i khÃ´ng gian tham sá»‘. Trong gÃ³c nhÃ¬n hÃ¬nh há»c thÃ´ng tin, náº¿u chÃºng ta thÃªm má»™t nhiá»…u loáº¡n nhá» vÃ o khÃ´ng gian tham sá»‘, chÃºng ta cÃ³
KL(P(G;y;Î¸)â€–P(G;y;Î¸+dÎ¸)) = dÎ¸>FdÎ¸:

trong Ä‘Ã³ KL(Â·;Â·) lÃ  divergence Kullbackâ€“Leibler. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  khÃ´ng gian tham sá»‘ cá»§a má»™t mÃ´ hÃ¬nh táº¡o thÃ nh má»™t Ä‘a táº¡p Riemannian vÃ  FIM hoáº¡t Ä‘á»™ng nhÆ° thÆ°á»›c Ä‘o Riemannian cá»§a nÃ³. Do Ä‘Ã³ FIM cho phÃ©p chÃºng ta Ä‘á»‹nh lÆ°á»£ng táº§m quan trá»ng cá»§a trá»ng sá»‘ cá»§a má»™t mÃ´ hÃ¬nh theo cÃ¡ch cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c kiáº¿n trÃºc khÃ¡c nhau.

4.2 CÃC Äáº¶C TRÆ¯NG NHIá»†M Vá»¤ Dá»°A TRÃŠN FIM
Biá»ƒu diá»…n Báº¥t biáº¿n Tá»· lá»‡ cá»§a Cáº·p Nhiá»‡m vá»¥-MÃ´ hÃ¬nh. ChÃºng tÃ´i nháº±m tÃ¬m má»™t biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡ cho má»—i cáº·p nhiá»‡m vá»¥-mÃ´ hÃ¬nh sáº½ táº¡o cÆ¡ sá»Ÿ Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. ThÃ¡ch thá»©c chÃ­nh trong viá»‡c sá»­ dá»¥ng FIM Ä‘á»ƒ biá»ƒu diá»…n hiá»‡u suáº¥t GNN lÃ  cÃ¡c táº­p dá»¯ liá»‡u Ä‘á»“ thá»‹ khÃ´ng cÃ³ cáº¥u trÃºc Ä‘áº§u vÃ o phá»• quÃ¡t, cá»‘ Ä‘á»‹nh, vÃ¬ váº­y khÃ´ng kháº£ thi Ä‘á»ƒ tÃ¬m má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c duy nháº¥t vÃ  trÃ­ch xuáº¥t FIM cá»§a nÃ³. Tuy nhiÃªn, viá»‡c Ä‘Ã o táº¡o nhiá»u máº¡ng Ä‘áº·t ra váº¥n Ä‘á» vÃ¬ cÃ¡c FIM Ä‘Æ°á»£c tÃ­nh cho cÃ¡c máº¡ng khÃ¡c nhau khÃ´ng thá»ƒ so sÃ¡nh trá»±c tiáº¿p. ChÃºng tÃ´i chá»n sá»­ dá»¥ng nhiá»u máº¡ng nhÆ°ng bá»• sung Ä‘á» xuáº¥t sá»­ dá»¥ng thá»‘ng kÃª tiá»‡m cáº­n cá»§a FIM liÃªn káº¿t vá»›i trá»ng sá»‘ Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. LÃ½ giáº£i lÃ½ thuyáº¿t cho má»‘i quan há»‡ giá»¯a thá»‘ng kÃª tiá»‡m cáº­n cá»§a FIM vÃ  kháº£ nÄƒng Ä‘Ã o táº¡o cá»§a máº¡ng neural Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u trong (Karakida et al., 2019; Pennington vÃ  Worah, 2018) mÃ  chÃºng tÃ´i giá»›i thiá»‡u Ä‘á»™c giáº£ tham kháº£o. ChÃºng tÃ´i giáº£ Ä‘á»‹nh ráº±ng thÆ°á»›c Ä‘o kháº£ nÄƒng Ä‘Ã o táº¡o nhÆ° váº­y mÃ£ hÃ³a cáº£nh quan máº¥t mÃ¡t vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a vÃ  do Ä‘Ã³ tÆ°Æ¡ng quan vá»›i hiá»‡u suáº¥t mÃ´ hÃ¬nh cuá»‘i cÃ¹ng trÃªn nhiá»‡m vá»¥. Má»™t váº¥n Ä‘á» khÃ¡c liÃªn quan Ä‘áº¿n cáº¥u trÃºc Ä‘áº§u vÃ o cá»§a cÃ¡c táº­p dá»¯ liá»‡u Ä‘á»“ thá»‹ lÃ  cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau cÃ³ sá»‘ lÆ°á»£ng tham sá»‘ khÃ¡c nhau. Máº·c dÃ¹ cÃ³ má»™t sá»‘ kiáº¿n trÃºc Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t, vÃ­ dá»¥, (Lee et al., 2019; Ma et al., 2019), háº§u háº¿t thiáº¿t káº¿ kiáº¿n trÃºc GNN cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t chuá»—i cÃ¡c lá»›p tiá»n xá»­ lÃ½, lá»›p truyá»n tin, vÃ  lá»›p háº­u xá»­ lÃ½. CÃ¡c lá»›p tiá»n xá»­ lÃ½ vÃ  háº­u xá»­ lÃ½ lÃ  cÃ¡c lá»›p Máº¡ng Perceptron Äa lá»›p (MLP), cÃ³ kÃ­ch thÆ°á»›c thay Ä‘á»•i qua cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau do cáº¥u trÃºc Ä‘áº§u vÃ o/Ä‘áº§u ra khÃ¡c nhau. CÃ¡c lá»›p truyá»n tin thÆ°á»ng Ä‘Æ°á»£c coi lÃ  thiáº¿t káº¿ chÃ­nh cho GNN vÃ  sá»‘ lÆ°á»£ng tham sá»‘ trá»ng sá»‘ cÃ³ thá»ƒ giá»¯ nguyÃªn qua cÃ¡c nhiá»‡m vá»¥. Theo quan Ä‘iá»ƒm nÃ y, chÃºng tÃ´i chá»‰ xem xÃ©t FIM Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ trong cÃ¡c lá»›p truyá»n tin Ä‘á»ƒ sá»‘ lÆ°á»£ng tham sá»‘ Ä‘Æ°á»£c xem xÃ©t giá»¯ nguyÃªn cho táº¥t cáº£ cÃ¡c táº­p dá»¯ liá»‡u. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng cÃ´ng thá»©c nhÆ° váº­y cÃ³ nhá»¯ng háº¡n cháº¿, theo nghÄ©a lÃ  nÃ³ khÃ´ng thá»ƒ bao phá»§ táº¥t cáº£ cÃ¡c thiáº¿t káº¿ GNN trong tÃ i liá»‡u. ChÃºng tÃ´i Ä‘á»ƒ láº¡i cÃ¡c pháº§n má»Ÿ rá»™ng tiá»m nÄƒng vá»›i pháº¡m vi bao phá»§ tá»‘t hÆ¡n cho cÃ´ng viá»‡c tÆ°Æ¡ng lai. ChÃºng tÃ´i tiáº¿p tá»¥c xáº¥p xá»‰ FIM báº±ng cÃ¡ch chá»‰ xem xÃ©t cÃ¡c má»¥c Ä‘Æ°á»ng chÃ©o, Ä‘iá»u nÃ y ngáº§m bá» qua cÃ¡c tÆ°Æ¡ng quan giá»¯a cÃ¡c tham sá»‘. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng Ä‘Ã¢y lÃ  thá»±c hÃ nh phá»• biáº¿n khi phÃ¢n tÃ­ch FIM cá»§a máº¡ng neural sÃ¢u, vÃ¬ FIM Ä‘áº§y Ä‘á»§ lÃ  khá»•ng lá»“ (báº­c hai theo sá»‘ lÆ°á»£ng tham sá»‘) vÃ  khÃ´ng kháº£ thi Ä‘á»ƒ tÃ­nh toÃ¡n ngay cáº£ trÃªn pháº§n cá»©ng hiá»‡n Ä‘áº¡i. TÆ°Æ¡ng tá»± nhÆ° Pennington vÃ  Worah (2018), chÃºng tÃ´i xem xÃ©t hai moment Ä‘áº§u tiÃªn cá»§a FIM
m1=1ntr[F] vÃ  m2=1ntr[F2] (1)

vÃ  sá»­ dá»¥ng Îº=m2/m1Â² nhÆ° biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡. Îº Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘Æ°á»£c giá»›i háº¡n dÆ°á»›i bá»Ÿi 1 vÃ  náº¯m báº¯t má»©c Ä‘á»™ táº­p trung cá»§a phá»•. Má»™t Îº nhá» chá»‰ ra cáº£nh quan máº¥t mÃ¡t pháº³ng, vÃ  thiáº¿t káº¿ mÃ´ hÃ¬nh tÆ°Æ¡ng á»©ng cá»§a nÃ³ táº­n hÆ°á»Ÿng tá»‘i Æ°u hÃ³a báº­c nháº¥t nhanh vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n tiá»m nÄƒng.

Äá»ƒ mÃ£ hÃ³a thÃ´ng tin khÃ´ng gian nhÃ£n vÃ o má»—i nhiá»‡m vá»¥, chÃºng tÃ´i Ä‘á» xuáº¥t chá»‰ Ä‘Ã o táº¡o lá»›p tuyáº¿n tÃ­nh cuá»‘i cÃ¹ng cá»§a má»—i mÃ´ hÃ¬nh trÃªn má»™t nhiá»‡m vá»¥ cho trÆ°á»›c, Ä‘iá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n hiá»‡u quáº£. CÃ¡c tham sá»‘ trong cÃ¡c lá»›p khÃ¡c Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng sau khi Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. ChÃºng tÃ´i láº¥y trung bÃ¬nh trÃªn R khá»Ÿi táº¡o Ä‘á»ƒ Æ°á»›c tÃ­nh Îº trung bÃ¬nh.

XÃ¢y dá»±ng Äáº·c trÆ°ng Nhiá»‡m vá»¥. ChÃºng tÃ´i kÃ½ hiá»‡u cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ lÃ  cÃ¡c thÆ°á»›c Ä‘o Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« má»—i nhiá»‡m vá»¥ Ä‘áº·c trÆ°ng cho cÃ¡c Ä‘áº·c Ä‘iá»ƒm quan trá»ng cá»§a nÃ³. Thiáº¿t káº¿ cá»§a cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ nÃªn pháº£n Ã¡nh má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a chÃºng ta: sá»­ dá»¥ng nhá»¯ng Ä‘áº·c trÆ°ng nÃ y Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± vÃ  chuyá»ƒn giao cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ tá»‘t nháº¥t. Do Ä‘Ã³, chÃºng tÃ´i chá»n U thiáº¿t káº¿ mÃ´ hÃ¬nh lÃ m mÃ´ hÃ¬nh neo vÃ  ná»‘i cÃ¡c biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡ au cá»§a má»—i thiáº¿t káº¿ lÃ m Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. Äá»ƒ chá»‰ giá»¯ láº¡i thá»© tá»± xáº¿p háº¡ng tÆ°Æ¡ng Ä‘á»‘i giá»¯a cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh neo, chÃºng tÃ´i chuáº©n hÃ³a vector Ä‘áº·c trÆ°ng Ä‘Æ°á»£c ná»‘i vá»›i tá»· lá»‡ 1. ChÃºng tÃ´i Ä‘á»ƒ zf kÃ½ hiá»‡u Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c chuáº©n hÃ³a.

4.3 Tá»ª Äáº¶C TRÆ¯NG NHIá»†M Vá»¤ Äáº¾N NHÃšNG NHIá»†M Vá»¤
Äáº·c trÆ°ng nhiá»‡m vá»¥ zf Ä‘Æ°á»£c giá»›i thiá»‡u á»Ÿ trÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t phÆ°Æ¡ng tiá»‡n ká»¹ thuáº­t Ä‘áº·c trÆ°ng. ChÃºng tÃ´i xÃ¢y dá»±ng vector Ä‘áº·c trÆ°ng vá»›i kiáº¿n thá»©c lÄ©nh vá»±c, nhÆ°ng khÃ´ng cÃ³ Ä‘áº£m báº£o nÃ o ráº±ng nÃ³ hoáº¡t Ä‘á»™ng nhÆ° dá»± Ä‘oÃ¡n. Do Ä‘Ã³ chÃºng tÃ´i Ä‘á» xuáº¥t há»c má»™t hÃ m chiáº¿u g() : RUâ†’RD Ã¡nh xáº¡ Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ zf Ä‘áº¿n nhÃºng nhiá»‡m vá»¥ cuá»‘i cÃ¹ng ze=g(zf). ChÃºng tÃ´i khÃ´ng cÃ³ báº¥t ká»³ giÃ¡m sÃ¡t theo Ä‘iá»ƒm nÃ o cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m má»¥c tiÃªu Ä‘Ã o táº¡o. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i xem xÃ©t khÃ´ng gian thÆ°á»›c Ä‘o Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi GraphGym. HÃ m khoáº£ng cÃ¡ch trong GraphGym - Ä‘Æ°á»£c tÃ­nh báº±ng tÆ°Æ¡ng quan xáº¿p háº¡ng Kendall giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh neo Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn hai nhiá»‡m vá»¥ Ä‘Æ°á»£c so sÃ¡nh - tÆ°Æ¡ng quan tá»‘t vá»›i má»¥c tiÃªu chuyá»ƒn giao kiáº¿n thá»©c mong muá»‘n cá»§a chÃºng ta. KhÃ´ng cÃ³ Ã½ nghÄ©a gÃ¬ khi Ã©p buá»™c ráº±ng nhÃºng nhiá»‡m vá»¥ báº¯t chÆ°á»›c khÃ´ng gian thÆ°á»›c Ä‘o chÃ­nh xÃ¡c cá»§a GraphGym, vÃ¬ khÃ´ng gian thÆ°á»›c Ä‘o cá»§a GraphGym váº«n cÃ³ thá»ƒ chá»©a nhiá»…u, hoáº·c khÃ´ng hoÃ n toÃ n tháº³ng hÃ ng vá»›i má»¥c tiÃªu chuyá»ƒn giao. ChÃºng tÃ´i xem xÃ©t má»™t máº¥t mÃ¡t thay tháº¿ chá»‰ Ã©p buá»™c thá»© tá»± xáº¿p háº¡ng giá»¯a cÃ¡c nhiá»‡m vá»¥. Äá»ƒ minh há»a, hÃ£y xem xÃ©t cÃ¡c nhiá»‡m vá»¥ T(i),T(j),T(k) vÃ  cÃ¡c nhÃºng nhiá»‡m vá»¥ tÆ°Æ¡ng á»©ng cá»§a chÃºng, z(i)e,z(j)e,z(k)e. LÆ°u Ã½ ráº±ng ze Ä‘Æ°á»£c chuáº©n hÃ³a vá» 1 nÃªn z(i)e>z(j)e Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c nhiá»‡m vá»¥ T(i) vÃ  T(j).

HÃ£y dg(Â·;Â·) kÃ½ hiá»‡u khoáº£ng cÃ¡ch Ä‘Æ°á»£c Æ°á»›c tÃ­nh bá»Ÿi GraphGym. ChÃºng ta muá»‘n Ã©p buá»™c
z(i)e>z(j)e > z(i)e>z(k)e náº¿u dg(T(i);T(j)) < dg(T(i);T(k)):

Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng máº¥t mÃ¡t xáº¿p háº¡ng biÃªn lÃ m hÃ m má»¥c tiÃªu giÃ¡m sÃ¡t thay tháº¿:
Lr(z(i)e;z(j)e;z(k)e;y) = max(0;y(z(i)e>z(j)eâˆ’z(i)e>z(k)e) + margin ): (2)

á» Ä‘Ã¢y náº¿u dg(T(i);T(j)) < dg(T(i);T(k)), thÃ¬ chÃºng ta cÃ³ nhÃ£n tÆ°Æ¡ng á»©ng y = 1, vÃ  y = âˆ’1 ngÆ°á»£c láº¡i. KhÃ´ng gian nhÃºng nhiá»‡m vá»¥ cuá»‘i cÃ¹ng cá»§a chÃºng tÃ´i sau Ä‘Ã³ lÃ  má»™t khÃ´ng gian thÆ°á»›c Ä‘o dá»±a trÃªn FIM vá»›i hÃ m khoáº£ng cÃ¡ch cosine, trong Ä‘Ã³ khoáº£ng cÃ¡ch Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  de(T(i);T(j)) = 1âˆ’z(i)e>z(j)e. Vui lÃ²ng tham kháº£o quy trÃ¬nh Ä‘Ã o táº¡o chi tiáº¿t táº¡i Thuáº­t toÃ¡n 2 trong Phá»¥ lá»¥c.

4.4 THUáº¬T TOÃN TÃŒM KIáº¾M AUTOML Vá»šI NHÃšNG NHIá»†M Vá»¤
Äá»ƒ chuyá»ƒn giao kiáº¿n thá»©c cho má»™t nhiá»‡m vá»¥ má»›i, má»™t Ã½ tÆ°á»Ÿng ngÃ¢y thÆ¡ sáº½ lÃ  trá»±c tiáº¿p chuyá»ƒn cáº¥u hÃ¬nh mÃ´ hÃ¬nh tá»‘t nháº¥t tá»« nhiá»‡m vá»¥ gáº§n nháº¥t trong ngÃ¢n hÃ ng. Tuy nhiÃªn, ngay cáº£ má»™t tÆ°Æ¡ng quan xáº¿p háº¡ng Kendall cao giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t mÃ´ hÃ¬nh cá»§a hai nhiá»‡m vá»¥ T(i),T(j) khÃ´ng Ä‘áº£m báº£o cáº¥u hÃ¬nh mÃ´ hÃ¬nh tá»‘t nháº¥t trong nhiá»‡m vá»¥ T(i) cÅ©ng sáº½ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t trÃªn nhiá»‡m vá»¥ T(j). NgoÃ i ra, vÃ¬ sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ cÃ³ thá»ƒ bá»‹ nhiá»…u, giáº£i phÃ¡p ngÃ¢y thÆ¡ nÃ y cÃ³ thá»ƒ gáº·p khÃ³ khÄƒn khi tá»“n táº¡i nhiá»u nhiá»‡m vá»¥ tham chiáº¿u Ä‘á»u ráº¥t tÆ°Æ¡ng tá»±.

Äá»ƒ lÃ m cho viá»‡c chuyá»ƒn giao kiáº¿n thá»©c máº¡nh máº½ hÆ¡n vá»›i nhá»¯ng trÆ°á»ng há»£p tháº¥t báº¡i nhÆ° váº­y, chÃºng tÃ´i giá»›i thiá»‡u khÃ¡i niá»‡m phÃ¢n phá»‘i thiáº¿t káº¿ phá»¥ thuá»™c vÃ o cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t vÃ  Ä‘á» xuáº¥t chuyá»ƒn giao phÃ¢n phá»‘i thiáº¿t káº¿ thay vÃ¬ cÃ¡c cáº¥u hÃ¬nh thiáº¿t káº¿ tá»‘t nháº¥t. ChÃ­nh thá»©c, xem xÃ©t má»™t nhiá»‡m vá»¥ T(i) trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B, liÃªn káº¿t vá»›i cÃ¡c thá»­ nghiá»‡m f(M(i)j;H(i)j;l(i)j)g cá»§a nÃ³. ChÃºng ta cÃ³ thá»ƒ tÃ³m táº¯t cÃ¡c thiáº¿t káº¿ cá»§a nÃ³ nhÆ° má»™t danh sÃ¡ch cÃ¡c cáº¥u hÃ¬nh C=fc1;:::;cWg, sao cho táº¥t cáº£ cÃ¡c káº¿t há»£p tiá»m nÄƒng cá»§a kiáº¿n trÃºc mÃ´ hÃ¬nh M vÃ  siÃªu tham sá»‘ H rÆ¡i vÃ o tÃ­ch Cartesian cá»§a cÃ¡c cáº¥u hÃ¬nh. VÃ­ dá»¥, c1 cÃ³ thá»ƒ lÃ  viá»‡c thá»ƒ hiá»‡n cá»§a cÃ¡c lá»›p tá»•ng há»£p, vÃ  c2 cÃ³ thá»ƒ lÃ  tá»‘c Ä‘á»™ há»c báº¯t Ä‘áº§u. Sau Ä‘Ã³ chÃºng tÃ´i Ä‘á»‹nh nghÄ©a phÃ¢n phá»‘i thiáº¿t káº¿ lÃ  cÃ¡c biáº¿n ngáº«u nhiÃªn c1;c2;:::;cW, má»—i cÃ¡i tÆ°Æ¡ng á»©ng vá»›i má»™t siÃªu tham sá»‘.
Má»—i biáº¿n ngáº«u nhiÃªn c Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  phÃ¢n phá»‘i táº§n sá»‘ cá»§a cÃ¡c lá»±a chá»n thiáº¿t káº¿ Ä‘Æ°á»£c sá»­ dá»¥ng trong K thá»­ nghiá»‡m hÃ ng Ä‘áº§u. ChÃºng tÃ´i nhÃ¢n táº¥t cáº£ cÃ¡c phÃ¢n phá»‘i cho cÃ¡c cáº¥u hÃ¬nh cÃ¡ nhÃ¢n fc1;:::;cWg Ä‘á»ƒ xáº¥p xá»‰ phÃ¢n phá»‘i thiáº¿t káº¿ tá»•ng thá»ƒ cá»§a nhiá»‡m vá»¥ P(C|T(i)) = âˆw P(cw|T(i)):

Trong quÃ¡ trÃ¬nh suy luáº­n, cho má»™t nhiá»‡m vá»¥ má»›i T(n), chÃºng tÃ´i chá»n má»™t táº­p con nhiá»‡m vá»¥ gáº§n S báº±ng cÃ¡ch ngÆ°á»¡ng khoáº£ng cÃ¡ch nhÃºng nhiá»‡m vá»¥, tá»©c lÃ , S=fT(i)|de(T(n);T(i))â‰¤dthresg. Sau Ä‘Ã³ chÃºng tÃ´i rÃºt ra tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao Pt(C|T(n)) cá»§a nhiá»‡m vá»¥ má»›i báº±ng cÃ¡ch cÃ¢n phÃ¢n phá»‘i thiáº¿t káº¿ tá»« táº­p con nhiá»‡m vá»¥ gáº§n S.
Pt(C|T(n)) = âˆ‘T(i)âˆˆS 1de(T(n);T(i)) P(C|T(i)) / âˆ‘T(i)âˆˆS 1de(T(n);T(i)): (3)

TiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c suy luáº­n cho nhiá»‡m vá»¥ má»›i sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hÆ°á»›ng dáº«n cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m khÃ¡c nhau.
Lá»±a chá»n tá»± nhiÃªn nháº¥t cho cháº¿ Ä‘á»™ Ã­t thá»­ nghiá»‡m lÃ  tÃ¬m kiáº¿m ngáº«u nhiÃªn. Thay vÃ¬ láº¥y máº«u má»—i cáº¥u hÃ¬nh thiáº¿t káº¿ theo phÃ¢n phá»‘i Ä‘á»“ng nháº¥t, chÃºng tÃ´i Ä‘á» xuáº¥t láº¥y máº«u tá»« tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng tin nhiá»‡m vá»¥ Pt(C|T(n)). Vui lÃ²ng tham kháº£o Phá»¥ lá»¥c A Ä‘á»ƒ kiá»ƒm tra cÃ¡ch chÃºng tÃ´i tÄƒng cÆ°á»ng cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m khÃ¡c.

Äá»‘i vá»›i AUTOTRANSFER, chÃºng ta cÃ³ thá»ƒ tiá»n xá»­ lÃ½ ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh B thÃ nh Bp = f(D(i);L(i)());z(i)e;P(C|T(i))g vÃ¬ quy trÃ¬nh cá»§a chÃºng ta chá»‰ yÃªu cáº§u sá»­ dá»¥ng nhÃºng nhiá»‡m vá»¥ z(i)e vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ P(C|T(i)) thay vÃ¬ cÃ¡c thá»­ nghiá»‡m Ä‘Ã o táº¡o chi tiáº¿t. Má»™t quy trÃ¬nh tÃ¬m kiáº¿m chi tiáº¿t Ä‘Æ°á»£c tÃ³m táº¯t trong Thuáº­t toÃ¡n 1.

6

--- TRANG 7 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
Thuáº­t toÃ¡n 1 TÃ³m táº¯t quy trÃ¬nh tÃ¬m kiáº¿m A UTOTRANSFER
YÃªu cáº§u: Má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh Ä‘Ã£ xá»­ lÃ½ Bp=f(D(i);L(i)());z(i)e;P(C|T(i))g, má»™t nhiá»‡m vá»¥ má»›i T(n)=
D(n);L(n)()
, U mÃ´ hÃ¬nh neo M1;:::;MU, R chá»‰ Ä‘á»‹nh sá»‘ láº§n láº·p láº¡i.
1: for u = 1 to U do
2: for r = 1 to R do
3: Khá»Ÿi táº¡o trá»ng sá»‘ cho mÃ´ hÃ¬nh neo Mu ngáº«u nhiÃªn
4: Æ¯á»›c tÃ­nh FIM F â‰ˆ ED[âˆ‡logP(Mu;y;Î¸)âˆ‡logP(Mu;y;Î¸)>]
5: TrÃ­ch xuáº¥t biá»ƒu diá»…n báº¥t biáº¿n tá»· lá»‡ a(v)u â† m2/m1Â² theo Eq. 1
6: end for
7: au â† mean (a(1)u;a(2)u;:::;a(V)u)
8: end for
9: z(n)f â† concat (a1;a2;:::;aU)
10: z(n)e â† g(z(n)f)
11: Chá»n táº­p con nhiá»‡m vá»¥ gáº§n S â† fT(i)|1âˆ’z(n)e>z(i)eâ‰¤dthresg
12: Láº¥y tiÃªn nghiá»‡m thiáº¿t káº¿ Pt(C|T(n)) báº±ng cÃ¡ch tá»•ng há»£p táº­p con S theo Eq. 3
13: Báº¯t Ä‘áº§u má»™t thuáº­t toÃ¡n tÃ¬m kiáº¿m HPO vá»›i tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng tin nhiá»‡m vá»¥ Pt(C|T(n))

5 THÃ NGHIá»†M
5.1 THIáº¾T Láº¬P THÃ NGHIá»†M
NgÃ¢n hÃ ng Nhiá»‡m vá»¥-MÃ´ hÃ¬nh: GNN-B ANK -101.
Äá»ƒ táº¡o Ä‘iá»u kiá»‡n nghiÃªn cá»©u AutoML vá»›i chuyá»ƒn giao kiáº¿n thá»©c, chÃºng tÃ´i Ä‘Ã£ thu tháº­p GNN-B ANK -101 nhÆ° cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»“ thá»‹ quy mÃ´ lá»›n Ä‘áº§u tiÃªn ghi láº¡i cÃ¡c cáº¥u hÃ¬nh thiáº¿t káº¿ cÃ³ thá»ƒ tÃ¡i táº¡o vÃ  hiá»‡u suáº¥t Ä‘Ã o táº¡o chi tiáº¿t trÃªn nhiá»u nhiá»‡m vá»¥ khÃ¡c nhau. Cá»¥ thá»ƒ, GNN-B ANK -101 hiá»‡n táº¡i bao gá»“m sÃ¡u nhiá»‡m vá»¥ cho phÃ¢n loáº¡i nÃºt (AmazonComputers (Shchur et al., 2018), AmazonPhoto (Shchur et al., 2018), CiteSeer (Yang et al., 2016), CoauthorCS (Shchur et al., 2018), CoauthorPhysics (Shchur et al., 2018), Cora (Yang et al., 2016)) vÃ  sÃ¡u nhiá»‡m vá»¥ cho phÃ¢n loáº¡i Ä‘á»“ thá»‹ (PROTEINS (Ivanov et al., 2019), BZR (Ivanov et al., 2019), COX2 (Ivanov et al., 2019), DD (Ivanov et al., 2019), ENZYMES (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019)). KhÃ´ng gian thiáº¿t káº¿ cá»§a chÃºng tÃ´i theo (You et al., 2020), vÃ  chÃºng tÃ´i má»Ÿ rá»™ng khÃ´ng gian thiáº¿t káº¿ Ä‘á»ƒ bao gá»“m cÃ¡c lá»›p kÃ­ch hoáº¡t vÃ  tÃ­ch cháº­p Ä‘á»“ thá»‹ khÃ¡c nhau thÆ°á»ng Ä‘Æ°á»£c Ã¡p dá»¥ng. ChÃºng tÃ´i cháº¡y rá»™ng rÃ£i 10.000 mÃ´ hÃ¬nh khÃ¡c nhau cho má»—i nhiá»‡m vá»¥, dáº«n Ä‘áº¿n tá»•ng cá»™ng 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh, vÃ  ghi láº¡i táº¥t cáº£ thÃ´ng tin Ä‘Ã o táº¡o bao gá»“m máº¥t mÃ¡t train/val/test.

Táº­p dá»¯ liá»‡u Äiá»ƒm chuáº©n. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER trÃªn sÃ¡u táº­p dá»¯ liá»‡u khÃ¡c nhau theo cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã³ (Qin et al., 2021). CÃ¡c táº­p dá»¯ liá»‡u cá»§a chÃºng tÃ´i bao gá»“m ba táº­p dá»¯ liá»‡u phÃ¢n loáº¡i nÃºt tiÃªu chuáº©n (CoauthorPhysics (Shchur et al., 2018), CoraFull (Bojchevski vÃ  GÃ¼nnemann, 2017) vÃ  OGB-Arxiv (Hu et al., 2020)), cÅ©ng nhÆ° ba táº­p dá»¯ liá»‡u Ä‘iá»ƒm chuáº©n phÃ¢n loáº¡i Ä‘á»“ thá»‹ tiÃªu chuáº©n, (COX2 (Ivanov et al., 2019), IMDB-M (Ivanov et al., 2019) vÃ  PROTEINS (Ivanov et al., 2019)). CoauthorPhysics vÃ  CoraFull lÃ  cÃ¡c táº­p dá»¯ liá»‡u phÃ¢n loáº¡i nÃºt chuyá»ƒn Ä‘á»•i, vÃ¬ váº­y chÃºng tÃ´i gÃ¡n ngáº«u nhiÃªn cÃ¡c nÃºt vÃ o cÃ¡c táº­p train/valid/test theo phÃ¢n chia 50%:25%:25% (Qin et al., 2021). ChÃºng tÃ´i phÃ¢n chia ngáº«u nhiÃªn cÃ¡c Ä‘á»“ thá»‹ theo phÃ¢n chia 80%:10%:10% cho ba táº­p dá»¯ liá»‡u phÃ¢n loáº¡i Ä‘á»“ thá»‹ (Qin et al., 2021). ChÃºng tÃ´i tuÃ¢n theo phÃ¢n chia train/valid/test máº·c Ä‘á»‹nh cho táº­p dá»¯ liá»‡u OGB-Arxiv (Hu et al., 2020). Äá»ƒ Ä‘áº£m báº£o khÃ´ng cÃ³ rÃ² rá»‰ thÃ´ng tin, chÃºng tÃ´i táº¡m thá»i loáº¡i bá» táº¥t cáº£ cÃ¡c báº£n ghi liÃªn quan Ä‘áº¿n nhiá»‡m vá»¥ tá»« ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i náº¿u táº­p dá»¯ liá»‡u chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c thu tháº­p trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh.

ÄÆ°á»ng cÆ¡ sá»Ÿ. ChÃºng tÃ´i so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vá»›i cÃ¡c cÃ¡ch tiáº¿p cáº­n hiá»‡n Ä‘áº¡i cho AutoML GNN. ChÃºng tÃ´i sá»­ dá»¥ng GCN vÃ  GAT vá»›i kiáº¿n trÃºc máº·c Ä‘á»‹nh theo triá»ƒn khai ban Ä‘áº§u cá»§a chÃºng lÃ m Ä‘Æ°á»ng cÆ¡ sá»Ÿ. Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p NAS Ä‘a thá»­ nghiá»‡m, chÃºng tÃ´i xem xÃ©t GraphNAS (Gao et al., 2020). Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p NAS má»™t láº§n, chÃºng tÃ´i bao gá»“m DARTS (Liu et al., 2018) vÃ  GASSO (Qin et al., 2021). GASSO Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c thiáº¿t láº­p chuyá»ƒn Ä‘á»•i, vÃ¬ váº­y chÃºng tÃ´i bá» qua nÃ³ cho cÃ¡c Ä‘iá»ƒm chuáº©n phÃ¢n loáº¡i Ä‘á»“ thá»‹. ChÃºng tÃ´i tiáº¿p tá»¥c cung cáº¥p káº¿t quáº£ cá»§a cÃ¡c thuáº­t toÃ¡n HPO dá»±a trÃªn khÃ´ng gian tÃ¬m kiáº¿m Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i lÃ m Ä‘Æ°á»ng cÆ¡ sá»Ÿ: Random, Evolution, TPE (Bergstra et al., 2011) vÃ  HyperBand (Li et al., 2017).

ChÃºng tÃ´i máº·c Ä‘á»‹nh cho phÃ©p tÃ¬m kiáº¿m tá»‘i Ä‘a 30 thá»­ nghiá»‡m cho táº¥t cáº£ cÃ¡c thuáº­t toÃ¡n, tá»©c lÃ , má»™t thuáº­t toÃ¡n cÃ³ thá»ƒ Ä‘Ã o táº¡o 30 mÃ´ hÃ¬nh khÃ¡c nhau vÃ  thu tháº­p mÃ´ hÃ¬nh vá»›i Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t. ChÃºng tÃ´i sá»­ dá»¥ng thiáº¿t láº­p máº·c Ä‘á»‹nh cho má»™t láº§n

7

--- TRANG 8 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
Báº£ng 1: So sÃ¡nh hiá»‡u suáº¥t cá»§a AUTOTRANSFER vÃ  cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ khÃ¡c. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n qua mÆ°á»i láº§n cháº¡y. Chá»‰ vá»›i 3 thá»­ nghiá»‡m AUTOTRANSFER Ä‘Ã£ vÆ°á»£t trá»™i háº§u háº¿t cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ SOTA vá»›i 30 thá»­ nghiá»‡m.

NÃºt Äá»“ thá»‹
PhÆ°Æ¡ng phÃ¡p Physics CoraFull OGB-Arxiv COX2 IMDB PROTEINS
GCN (30 thá»­ nghiá»‡m) 95.88Â±0.16 67.12Â±0.52 70.46Â±0.18 79.23Â±2.19 50.40Â±3.02 74.84Â±2.82
GAT (30 thá»­ nghiá»‡m) 95.71Â±0.24 65.92Â±0.68 68.82Â±0.32 81.56Â±4.17 49.67Â±4.30 75.30Â±3.72
GraphNAS (30 thá»­ nghiá»‡m) 92.77Â±0.84 63.13Â±3.28 65.90Â±2.64 77.73Â±1.40 46.93Â±3.94 72.51Â±3.36
DARTS 95.28Â±1.67 67.59Â±2.85 69.02Â±1.18 79.82Â±3.15 50.26Â±4.08 75.04Â±3.81
GASSOâ´ 96.38 68.89 70.52 - - -
Random (3 thá»­ nghiá»‡m) 95.16Â±0.55 61.24Â±4.04 67.92Â±1.92 76.88Â±3.17 45.79Â±4.39 72.47Â±2.57
TPE (30 thá»­ nghiá»‡m) 96.41Â±0.36 66.37Â±1.73 71.35Â±0.44 82.27Â±2.01 50.33Â±4.00 79.46Â±1.28
HyperBand (30 thá»­ nghiá»‡m) 96.56Â±0.30 67.75Â±1:24 71.60Â±0.36 82.21Â±1.79 50.86Â±3.45 79.32Â±1.16
AUTOTRANSFER (3 thá»­ nghiá»‡m) 96.64Â±0.42 69.27Â±0.76 71.42Â±0.39 82.13Â±1.59 52.33Â±2.13 77.81Â±2.19
AUTOTRANSFER (30 thá»­ nghiá»‡m) 96.91Â±0.27 70.05Â±0.42 72.21Â±0.27 86.52Â±1.58 54.93Â±1.23 81.25Â±1.17

cÃ¡c thuáº­t toÃ¡n NAS (DARTS vÃ  GASSO), vÃ¬ chÃºng chá»‰ Ä‘Ã o táº¡o má»™t siÃªu mÃ´ hÃ¬nh má»™t láº§n vÃ  cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cÃ¡c kiáº¿n trÃºc khÃ¡c nhau. ChÃºng tÃ´i chá»§ yáº¿u quan tÃ¢m Ä‘áº¿n viá»‡c nghiÃªn cá»©u cháº¿ Ä‘á»™ Ã­t thá»­ nghiá»‡m nÆ¡i háº§u háº¿t cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m tiÃªn tiáº¿n suy giáº£m thÃ nh tÃ¬m kiáº¿m ngáº«u nhiÃªn. Do Ä‘Ã³ chÃºng tÃ´i bá»• sung bao gá»“m má»™t Ä‘Æ°á»ng cÆ¡ sá»Ÿ tÃ¬m kiáº¿m ngáº«u nhiÃªn (3 thá»­ nghiá»‡m) nÆ¡i chÃºng tÃ´i chá»n mÃ´ hÃ¬nh tá»‘t nháº¥t trong chá»‰ 3 thá»­ nghiá»‡m.

5.2 THÃ NGHIá»†M Vá»€ HIá»†U QUáº¢ TÃŒM KIáº¾M
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ AUTOTRANSFER báº±ng cÃ¡ch bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t trung bÃ¬nh trong táº¥t cáº£ cÃ¡c thá»­ nghiá»‡m Ä‘Æ°á»£c xem xÃ©t qua mÆ°á»i láº§n cháº¡y cá»§a má»—i thuáº­t toÃ¡n trong Báº£ng 1. Äá»™ chÃ­nh xÃ¡c test Ä‘Æ°á»£c thu tháº­p cho má»—i thá»­ nghiá»‡m Ä‘Æ°á»£c chá»n táº¡i epoch vá»›i Ä‘á»™ chÃ­nh xÃ¡c validation tá»‘t nháº¥t. Báº±ng cÃ¡ch so sÃ¡nh káº¿t quáº£ tá»« tÃ¬m kiáº¿m ngáº«u nhiÃªn (3 thá»­ nghiá»‡m) vÃ  AUTOTRANSFER (3 thá»­ nghiá»‡m), chÃºng tÃ´i chá»‰ ra ráº±ng tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng tin nhiá»‡m vá»¥ Ä‘Æ°á»£c chuyá»ƒn giao cá»§a chÃºng tÃ´i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c test trong cháº¿ Ä‘á»™ Ã­t thá»­ nghiá»‡m, vÃ  cÃ³ thá»ƒ ráº¥t há»¯u Ã­ch trong mÃ´i trÆ°á»ng bá»‹ háº¡n cháº¿ vá» máº·t tÃ­nh toÃ¡n. Ngay cáº£ khi chÃºng tÃ´i tÄƒng sá»‘ lÆ°á»£ng thá»­ nghiá»‡m tÃ¬m kiáº¿m lÃªn 30, AUTOTRANSFER váº«n chá»©ng minh cáº£i thiá»‡n khÃ´ng táº§m thÆ°á»ng so vá»›i TPE, chá»‰ ra ráº±ng quy trÃ¬nh Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cÃ³ lá»£i tháº¿ ngay cáº£ khi tÃ i nguyÃªn tÃ­nh toÃ¡n dá»“i dÃ o. ÄÃ¡ng chÃº Ã½, chá»‰ vá»›i 3 thá»­ nghiá»‡m tÃ¬m kiáº¿m, AUTOTRANSFER vÆ°á»£t qua háº§u háº¿t cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ, ngay cáº£ nhá»¯ng cÃ¡i sá»­ dá»¥ng 30 thá»­ nghiá»‡m.

Äá»ƒ hiá»ƒu rÃµ hÆ¡n hiá»‡u quáº£ máº«u cá»§a AUTOTRANSFER, chÃºng tÃ´i váº½ Ä‘á»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t Ä‘Æ°á»£c tÃ¬m tháº¥y táº¡i má»—i thá»­ nghiá»‡m trong HÃ¬nh 3 cho cÃ¡c táº­p dá»¯ liá»‡u OGB-Arxiv vÃ  TU-PROTEINS. ChÃºng tÃ´i nháº­n tháº¥y ráº±ng cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m tiÃªn tiáº¿n (Evolution vÃ  TPE) khÃ´ng cÃ³ lá»£i tháº¿ so vá»›i tÃ¬m kiáº¿m ngáº«u nhiÃªn á»Ÿ cháº¿ Ä‘á»™ Ã­t thá»­ nghiá»‡m vÃ¬ lÆ°á»£ng dá»¯ liá»‡u tÃ¬m kiáº¿m trÆ°á»›c chÆ°a Ä‘á»§ Ä‘á»ƒ suy luáº­n cÃ¡c cáº¥u hÃ¬nh thiáº¿t káº¿ tá»‘t hÆ¡n tiá»m nÄƒng. NgÆ°á»£c láº¡i, báº±ng cÃ¡ch láº¥y máº«u tá»« tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao, AUTOTRANSFER Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ trong nhá»¯ng thá»­ nghiá»‡m Ä‘áº§u tiÃªn. Äá»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t táº¡i thá»­ nghiá»‡m 3 cá»§a A UTOTRANSFER vÆ°á»£t qua Ä‘á»‘i tÃ¡c cá»§a nÃ³ táº¡i thá»­ nghiá»‡m 10 cho má»i phÆ°Æ¡ng phÃ¡p khÃ¡c.

HÃ¬nh 3: So sÃ¡nh hiá»‡u suáº¥t trong cháº¿ Ä‘á»™ Ã­t thá»­ nghiá»‡m. Táº¡i thá»­ nghiá»‡m t, chÃºng tÃ´i váº½ Ä‘á»™ chÃ­nh xÃ¡c test tá»‘t nháº¥t trong táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tÃ¬m kiáº¿m tá»« thá»­ nghiá»‡m 1 Ä‘áº¿n thá»­ nghiá»‡m t. AUTOTRANSFER cÃ³ thá»ƒ giáº£m sá»‘ lÆ°á»£ng thá»­ nghiá»‡m cáº§n thiáº¿t Ä‘á»ƒ tÃ¬m kiáº¿m xuá»‘ng má»™t báº­c Ä‘á»™ lá»›n (xem thÃªm Báº£ng 4 trong Phá»¥ lá»¥c).

5.3 PHÃ‚N TÃCH NHÃšNG NHIá»†M Vá»¤
PhÃ¢n tÃ­ch Ä‘á»‹nh tÃ­nh vá» Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. Äá»ƒ kiá»ƒm tra cháº¥t lÆ°á»£ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t, chÃºng tÃ´i hÃ¬nh dung ma tráº­n tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t (HÃ¬nh 4 (b)) cÃ¹ng vá»›i ma tráº­n tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ (HÃ¬nh 4 (a)) Ä‘Æ°á»£c Ä‘á» xuáº¥t trong GraphGym. ChÃºng tÃ´i chá»‰ ra ráº±ng ma tráº­n tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i náº¯m báº¯t cÃ¡c máº«u tÆ°Æ¡ng tá»± nhÆ° ma tráº­n tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ cá»§a GraphGym trong khi Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£ hÆ¡n nhiá»u

Â²Káº¿t quáº£ Ä‘áº¿n tá»« bÃ i bÃ¡o gá»‘c (Qin et al., 2021).

8

--- TRANG 9 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
(a)(b)(c)

HÃ¬nh 4: (a) Sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ cá»§a GraphGym giá»¯a táº¥t cáº£ cÃ¡c cáº·p nhiá»‡m vá»¥ (Ä‘Æ°á»£c tÃ­nh tá»« tÆ°Æ¡ng quan xáº¿p háº¡ng Kendall giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn hai nhiá»‡m vá»¥ Ä‘Æ°á»£c so sÃ¡nh), má»™t giÃ¡ trá»‹ cao hÆ¡n Ä‘áº¡i diá»‡n cho sá»± tÆ°Æ¡ng tá»± cao hÆ¡n. (b) Sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch tÃ­nh tÃ­ch vÃ´ hÆ°á»›ng giá»¯a cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c trÃ­ch xuáº¥t. (c) TÆ°Æ¡ng quan xáº¿p háº¡ng Kendall cá»§a cÃ¡c xáº¿p háº¡ng tÆ°Æ¡ng tá»± cá»§a cÃ¡c nhiá»‡m vá»¥ khÃ¡c Ä‘á»‘i vá»›i nhiá»‡m vá»¥ trung tÃ¢m giá»¯a phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t vÃ  GraphGym.

báº±ng cÃ¡ch bá» qua Ä‘Ã o táº¡o. ChÃºng tÃ´i nháº­n tháº¥y ráº±ng cÃ¹ng loáº¡i nhiá»‡m vá»¥, tá»©c lÃ , phÃ¢n loáº¡i nÃºt vÃ  phÃ¢n loáº¡i Ä‘á»“ thá»‹, chia sáº» nhiá»u sá»± tÆ°Æ¡ng tá»± hÆ¡n trong má»—i nhÃ³m. NhÆ° má»™t kiá»ƒm tra tÃ­nh há»£p lÃ½, chÃºng tÃ´i kiá»ƒm tra ráº±ng nhiá»‡m vá»¥ gáº§n nháº¥t trong ngÃ¢n hÃ ng Ä‘á»‘i vá»›i CoraFull lÃ  Cora. Top 3 nhiá»‡m vá»¥ gáº§n nháº¥t cho OGB-Arxiv lÃ  AmazonComputers, AmazonPhoto, vÃ  CoauthorPhysics, táº¥t cáº£ Ä‘á»u lÃ  cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i nÃºt.

Tá»•ng quÃ¡t hÃ³a cá»§a hÃ m chiáº¿u g(). Äá»ƒ chá»‰ ra hÃ m chiáº¿u Ä‘Æ°á»£c Ä‘á» xuáº¥t g() cÃ³ thá»ƒ táº¡o ra nhÃºng nhiá»‡m vá»¥ cÃ³ thá»ƒ tá»•ng quÃ¡t hÃ³a cho cÃ¡c nhiá»‡m vá»¥ má»›i, chÃºng tÃ´i tiáº¿n hÃ nh xÃ¡c thá»±c chÃ©o loáº¡i-má»™t-ra vá»›i táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i. Cá»¥ thá»ƒ, Ä‘á»‘i vá»›i má»—i nhiá»‡m vá»¥ Ä‘Æ°á»£c xem xÃ©t nhÆ° má»™t nhiá»‡m vá»¥ má»›i T(n), chÃºng tÃ´i sá»­ dá»¥ng pháº§n cÃ²n láº¡i cá»§a cÃ¡c nhiá»‡m vá»¥, cÃ¹ng vá»›i thÆ°á»›c Ä‘o khoáº£ng cÃ¡ch dg(Â·;Â·) cá»§a chÃºng Ä‘Æ°á»£c Æ°á»›c tÃ­nh bá»Ÿi khÃ´ng gian thÆ°á»›c Ä‘o chÃ­nh xÃ¡c nhÆ°ng tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n cá»§a GraphGym, Ä‘á»ƒ Ä‘Ã o táº¡o hÃ m chiáº¿u g(). ChÃºng tÃ´i tÃ­nh tÆ°Æ¡ng quan xáº¿p háº¡ng Kendall trÃªn sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ cho Äáº·c trÆ°ng Nhiá»‡m vá»¥ (khÃ´ng cÃ³ g()) vÃ  NhÃºng Nhiá»‡m vá»¥ (vá»›i g()) chá»‘ng láº¡i sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ chÃ­nh xÃ¡c. TÆ°Æ¡ng quan xáº¿p háº¡ng trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n qua mÆ°á»i láº§n cháº¡y Ä‘Æ°á»£c hiá»ƒn thá»‹ trÃªn HÃ¬nh 4 (c). ChÃºng tÃ´i tháº¥y ráº±ng vá»›i g() Ä‘Æ°á»£c Ä‘á» xuáº¥t, nhÃºng nhiá»‡m vá»¥ cá»§a chÃºng tÃ´i thá»±c sá»± tÆ°Æ¡ng quan tá»‘t hÆ¡n vá»›i sá»± tÆ°Æ¡ng tá»± nhiá»‡m vá»¥ chÃ­nh xÃ¡c, vÃ  do Ä‘Ã³, tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n cho cÃ¡c nhiá»‡m vá»¥ má»›i.

NghiÃªn cá»©u loáº¡i bá» vá» thiáº¿t káº¿ khÃ´ng gian nhiá»‡m vá»¥ thay tháº¿. Äá»ƒ chá»©ng minh tÃ­nh Æ°u viá»‡t cá»§a nhÃºng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t, chÃºng tÃ´i tiáº¿p tá»¥c so sÃ¡nh nÃ³ vá»›i cÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ thay tháº¿. Theo cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã³ (Yang et al., 2019), chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c máº¥t mÃ¡t Ä‘Æ°á»£c chuáº©n hÃ³a trÃªn 10 bÆ°á»›c Ä‘áº§u tiÃªn lÃ m Ä‘áº·c trÆ°ng nhiá»‡m vá»¥. Káº¿t quáº£ trÃªn OGB-Arxiv Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2. So vá»›i nhÃºng nhiá»‡m vá»¥ cá»§a AUTOTRANSFER, Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c táº¡o ra bá»Ÿi máº¥t mÃ¡t Ä‘Æ°á»£c chuáº©n hÃ³a cÃ³ tÆ°Æ¡ng quan xáº¿p háº¡ng tháº¥p hÆ¡n vá»›i thÆ°á»›c Ä‘o chÃ­nh xÃ¡c vÃ  mang láº¡i hiá»‡u suáº¥t tá»‡ hÆ¡n. Báº£ng 2 tiáº¿p tá»¥c biá»‡n minh cho hiá»‡u quáº£ cá»§a viá»‡c sá»­ dá»¥ng tÆ°Æ¡ng quan xáº¿p háº¡ng Kendall lÃ m thÆ°á»›c Ä‘o cho cháº¥t lÆ°á»£ng nhÃºng nhiá»‡m vá»¥, vÃ¬ tÆ°Æ¡ng quan xáº¿p háº¡ng Kendall cao hÆ¡n dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t hÆ¡n.

Báº£ng 2: NghiÃªn cá»©u loáº¡i bá» vá» thiáº¿t káº¿ khÃ´ng gian nhiá»‡m vá»¥ thay tháº¿ so vá»›i nhÃºng nhiá»‡m vá»¥ cá»§a AUTOTRANSFER. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n OGB-Arxiv qua mÆ°á»i láº§n cháº¡y.

TÆ°Æ¡ng quan xáº¿p háº¡ng Kendall Äá»™ chÃ­nh xÃ¡c test
Thay tháº¿: Máº¥t mÃ¡t Chuáº©n hÃ³a -0.07Â±0.43 68.13 Â± 1.27
Äáº·c trÆ°ng Nhiá»‡m vá»¥ cá»§a AUTOTRANSFER 0.18Â±0.30 70.67 Â± 0.52
NhÃºng Nhiá»‡m vá»¥ cá»§a AUTOTRANSFER 0.43Â±0.22 71.42 Â± 0.39

6 Káº¾T LUáº¬N
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i nghiÃªn cá»©u cÃ¡ch cáº£i thiá»‡n hiá»‡u quáº£ tÃ¬m kiáº¿m AutoML báº±ng cÃ¡ch chuyá»ƒn giao kiáº¿n thá»©c thiáº¿t káº¿ kiáº¿n trÃºc hiá»‡n cÃ³ cho cÃ¡c nhiá»‡m vá»¥ má»›i quan tÃ¢m. ChÃºng tÃ´i giá»›i thiá»‡u má»™t ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh náº¯m báº¯t hiá»‡u suáº¥t trÃªn má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c kiáº¿n trÃºc vÃ  nhiá»‡m vá»¥ GNN. ChÃºng tÃ´i cÅ©ng giá»›i thiá»‡u má»™t nhÃºng nhiá»‡m vá»¥ hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘o lÆ°á»ng chÃ­nh xÃ¡c sá»± tÆ°Æ¡ng tá»± giá»¯a cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau. ChÃºng tÃ´i phÃ¡t hÃ nh GNN-B ANK -101, má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u quy mÃ´ lá»›n ghi láº¡i thÃ´ng tin Ä‘Ã o táº¡o GNN chi tiáº¿t cá»§a 120.000 káº¿t há»£p nhiá»‡m vá»¥-mÃ´ hÃ¬nh. ChÃºng tÃ´i hy vá»ng cÃ´ng trÃ¬nh nÃ y cÃ³ thá»ƒ táº¡o Ä‘iá»u kiá»‡n vÃ  truyá»n cáº£m há»©ng cho nghiÃªn cá»©u tÆ°Æ¡ng lai trong AutoML hiá»‡u quáº£ Ä‘á»ƒ lÃ m cho há»c sÃ¢u dá»… tiáº¿p cáº­n hÆ¡n vá»›i khÃ¡n giáº£ Ä‘áº¡i chÃºng.

9

--- TRANG 10 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
Lá»œI Cáº¢M Æ N
ChÃºng tÃ´i cáº£m Æ¡n Xiang Lisa Li, Hongyu Ren, Yingxin Wu vÃ¬ cÃ¡c cuá»™c tháº£o luáº­n vÃ  vÃ¬ Ä‘Ã£ cung cáº¥p pháº£n há»“i vá» báº£n tháº£o cá»§a chÃºng tÃ´i. ChÃºng tÃ´i cÅ©ng biáº¿t Æ¡n sá»± há»— trá»£ cá»§a DARPA dÆ°á»›i Nos. HR00112190039 (TAMI), N660011924033 (MCS); ARO dÆ°á»›i Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF dÆ°á»›i Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), NIH dÆ°á»›i No. 3U54HG010426-04S1 (HuBMAP), Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Amazon, Docomo, GSK, Hitachi, Intel, JPMorgan Chase, Juniper Networks, KDDI, NEC, vÃ  Toshiba. Ná»™i dung hoÃ n toÃ n lÃ  trÃ¡ch nhiá»‡m cá»§a cÃ¡c tÃ¡c giáº£ vÃ  khÃ´ng nháº¥t thiáº¿t Ä‘áº¡i diá»‡n cho quan Ä‘iá»ƒm chÃ­nh thá»©c cá»§a cÃ¡c tá»• chá»©c tÃ i trá»£.

10

--- TRANG 11 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
TÃ€I LIá»†U THAM KHáº¢O
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes, Stefano Soatto, vÃ  Pietro Perona. Task2vec: Task embedding for meta-learning. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 6430â€“6439, 2019.

James Bergstra vÃ  Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.

James Bergstra, RÃ©mi Bardenet, Yoshua Bengio, vÃ  BalÃ¡zs KÃ©gl. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, 24, 2011.

Aleksandar Bojchevski vÃ  Stephan GÃ¼nnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.

Han Cai, Ligeng Zhu, vÃ  Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018.

Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, vÃ  Qingming Huang. Rethinking graph neural architecture search from message-passing. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 6657â€“6666, 2021.

Yuhui Ding, Quanming Yao, Huan Zhao, vÃ  Tong Zhang. Diffmg: Differentiable meta graph search for heterogeneous graph neural networks. Trong Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, trang 279â€“288, 2021.

Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, vÃ  Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. ICML 2020 Workshop on Automated Machine Learning, 2020.

Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, vÃ  Yue Hu. Graphnas: Graph neural architecture search with reinforcement learning. arXiv preprint arXiv:1904.09981, 2019.

Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, vÃ  Yue Hu. Graph neural architecture search. Trong Christian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, trang 1403â€“1409. International Joint Conferences on Artificial Intelligence Organization, 7 2020. URL https://doi.org/10.24963/ijcai.2020/195.

Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, vÃ  Jian Sun. Single path one-shot neural architecture search with uniform sampling. Trong European Conference on Computer Vision, trang 544â€“560. Springer, 2020.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, vÃ  Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770â€“778, 2016.

Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, vÃ  Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. Trong Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, trang 639â€“648, 2020.

Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, vÃ  Song Han. Amc: Automl for model compression and acceleration on mobile devices. Trong Proceedings of the European conference on computer vision (ECCV), trang 784â€“800, 2018.

Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, vÃ  Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118â€“22133, 2020.

11

--- TRANG 12 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
Sergei Ivanov, Sergei Sviridov, vÃ  Evgeny Burnaev. Understanding isomorphism bias in graph data sets, 2019.

Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017.

Wonyong Jeong, Hayeon Lee, Geon Park, Eunyoung Hyung, Jinheon Baek, vÃ  Sung Ju Hwang. Task-adaptive neural network search with meta-contrastive learning. Advances in Neural Information Processing Systems, 34:21310â€“21324, 2021.

Donald R Jones. A taxonomy of global optimization methods based on response surfaces. Journal of global optimization, 21(4):345â€“383, 2001.

Ryo Karakida, Shotaro Akaho, vÃ  Shun-ichi Amari. Universal statistics of fisher information in deep neural networks: Mean field approach. Trong The 22nd International Conference on Artificial Intelligence and Statistics, trang 1032â€“1041. PMLR, 2019.

Cat P Le, Mohammadreza Soltani, Juncheng Dong, vÃ  Vahid Tarokh. Fisher task distance and its application in neural architecture search. IEEE Access, 10:47235â€“47249, 2022.

Erin LeDell vÃ  Sebastien Poirier. H2o automl: Scalable automatic machine learning. ICML 2020 Workshop on Automated Machine Learning, 2020.

Junhyun Lee, Inyeop Lee, vÃ  Jaewoo Kang. Self-attention graph pooling. Trong International conference on machine learning, trang 3734â€“3743. PMLR, 2019.

Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, vÃ  Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765â€“6816, 2017.

Yanxi Li, Zean Wen, Yunhe Wang, vÃ  Chang Xu. One-shot graph neural architecture search with dynamic search space. Trong Proc. AAAI Conf. Artif. Intell, volume 35, trang 8510â€“8517, 2021.

Hanxiao Liu, Karen Simonyan, vÃ  Yiming Yang. Darts: Differentiable architecture search. Trong International Conference on Learning Representations, 2018.

Yao Ma, Suhang Wang, Charu C Aggarwal, vÃ  Jiliang Tang. Graph convolutional networks with eigenpooling. Trong Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, trang 723â€“731, 2019.

Jeffrey Pennington vÃ  Pratik Worah. The spectrum of the fisher information matrix of a single-hidden-layer neural network. Advances in neural information processing systems, 31, 2018.

Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, vÃ  Peter Battaglia. Learning mesh-based simulation with graph networks. Trong International Conference on Learning Representations, 2020.

Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, vÃ  Jeff Dean. Efficient neural architecture search via parameters sharing. Trong International conference on machine learning, trang 4095â€“4104. PMLR, 2018.

Yijian Qin, Xin Wang, Zeyang Zhang, vÃ  Wenwu Zhu. Graph differentiable architecture search with structure learning. Advances in Neural Information Processing Systems, 34, 2021.

Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, vÃ  Alexey Kurakin. Large-scale evolution of image classifiers. Trong International Conference on Machine Learning, trang 2902â€“2911. PMLR, 2017.

Roseanna W Saaty. The analytic hierarchy processâ€”what it is and how it is used. Mathematical modelling, 9(3-5):161â€“176, 1987.

Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, vÃ  Peter Battaglia. Learning to simulate complex physics with graph networks. Trong International Conference on Machine Learning, trang 8459â€“8468. PMLR, 2020.

12

--- TRANG 13 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, vÃ  Stephan GÃ¼nnemann. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.

Lanning Wei, Huan Zhao, Quanming Yao, vÃ  Zhiqiang He. Pooling architecture search for graph classification. Trong Proceedings of the 30th ACM International Conference on Information & Knowledge Management, trang 2091â€“2100, 2021.

Catherine Wong, Neil Houlsby, Yifeng Lu, vÃ  Andrea Gesmundo. Transfer learning with neural automl. Advances in neural information processing systems, 31, 2018.

Chengrun Yang, Yuji Akimoto, Dae Won Kim, vÃ  Madeleine Udell. Oboe: Collaborative filtering for automl model selection. Trong Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019.

Zhilin Yang, William Cohen, vÃ  Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. Trong International conference on machine learning, trang 40â€“48. PMLR, 2016.

Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, vÃ  Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. Trong Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, trang 974â€“983, 2018.

Jiaxuan You, Zhitao Ying, vÃ  Jure Leskovec. Design space for graph neural networks. Advances in Neural Information Processing Systems, 33:17009â€“17021, 2020.

Yuning You, Tianlong Chen, Yang Shen, vÃ  Zhangyang Wang. Graph contrastive learning automated. Trong International Conference on Machine Learning, trang 12121â€“12132. PMLR, 2021.

Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, vÃ  Silvio Savarese. Taskonomy: Disentangling task transfer learning. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 3712â€“3722, 2018.

Bo Zhao vÃ  Hakan Bilen. Dataset condensation with differentiable siamese augmentation. Trong International Conference on Machine Learning, trang 12674â€“12685. PMLR, 2021.

Huan Zhao, Quanming Yao, vÃ  Weiwei Tu. Search to aggregate neighborhood for graph neural network. arXiv preprint arXiv:2104.06608, 2021.

Kaixiong Zhou, Qingquan Song, Xiao Huang, vÃ  Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019.

Marinka Zitnik, Monica Agrawal, vÃ  Jure Leskovec. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics, 34(13):i457â€“i466, 2018.

Barret Zoph vÃ  Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, vÃ  Quoc V Le. Learning transferable architectures for scalable image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 8697â€“8710, 2018.

13

--- TRANG 14 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
A CHI TIáº¾T TRIá»‚N KHAI Bá»” SUNG
PhÃ¢n tÃ­ch thá»i gian cháº¡y. ChÃºng tÃ´i Ä‘Ã£ chá»©ng minh thá»±c nghiá»‡m ráº±ng AUTOTRANSFER cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ tÃ¬m kiáº¿m báº±ng cÃ¡ch giáº£m sá»‘ lÆ°á»£ng thá»­ nghiá»‡m cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘á»‘i tá»‘t. Chi phÃ­ bá»• sung duy nháº¥t chÃºng tÃ´i giá»›i thiá»‡u lÃ  quy trÃ¬nh Æ°á»›c tÃ­nh nhÃºng nhiá»‡m vá»¥. VÃ¬ chÃºng tÃ´i sá»­ dá»¥ng má»™t kiáº¿n trÃºc Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn, viá»‡c trÃ­ch xuáº¥t má»—i Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ chá»‰ yÃªu cáº§u tá»‘i Ä‘a má»™t láº§n truyá»n tiáº¿n vÃ  má»™t vÃ i láº§n truyá»n ngÆ°á»£c tá»« má»™t minibatch Ä‘Æ¡n láº» cá»§a dá»¯ liá»‡u. Thá»i gian wall-clock phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c cá»§a máº¡ng vÃ  cáº¥u trÃºc dá»¯ liá»‡u. Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, nÃ³ thÆ°á»ng máº¥t vÃ i giÃ¢y trÃªn GPU NVIDIA T4. ChÃºng tÃ´i láº·p láº¡i quy trÃ¬nh trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ 5 láº§n cho má»—i mÃ´ hÃ¬nh neo, vÃ  tá»•ng cá»™ng cho 12 mÃ´ hÃ¬nh neo. Do Ä‘Ã³, thá»i gian wall-clock cá»§a chi phÃ­ Ä‘á»ƒ tÃ­nh toÃ¡n nhÃºng nhiá»‡m vá»¥ cá»§a nhiá»‡m vá»¥ má»›i náº±m trong vÃ i phÃºt. ChÃºng tÃ´i lÆ°u Ã½ Ä‘á»™ dÃ i cá»§a quy trÃ¬nh nÃ y thÆ°á»ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i má»™t thá»­ nghiá»‡m Ä‘Ã o táº¡o trÃªn táº­p dá»¯ liá»‡u kÃ­ch thÆ°á»›c nhá», vÃ  thá»i gian tiáº¿t kiá»‡m Ä‘Æ°á»£c nhiá»u hÆ¡n Ä‘Ã¡ng ká»ƒ Ä‘á»‘i vá»›i cÃ¡c táº­p dá»¯ liá»‡u quy mÃ´ lá»›n.

Chi tiáº¿t cho Ä‘Ã o táº¡o ngÃ¢n hÃ ng-nhiá»‡m vá»¥-mÃ´ hÃ¬nh. CÃ¡c Ä‘áº·c táº£ mÃ´ hÃ¬nh GNN cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tÃ³m táº¯t trong Báº£ng 3. Codebase cá»§a chÃºng tÃ´i Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn GraphGym (You et al., 2020). Äá»‘i vá»›i táº¥t cáº£ cÃ¡c thá»­ nghiá»‡m Ä‘Ã o táº¡o, ChÃºng tÃ´i sá»­ dá»¥ng trÃ¬nh tá»‘i Æ°u Adam vÃ  bá»™ láº­p lá»‹ch tá»‘c Ä‘á»™ há»c cosine (Ä‘Æ°á»£c giáº£m dáº§n vá» 0, khÃ´ng khá»Ÿi Ä‘á»™ng láº¡i). ChÃºng tÃ´i sá»­ dá»¥ng regularization L2 vá»›i weight decay lÃ  5e-4. ChÃºng tÃ´i ghi láº¡i máº¥t mÃ¡t vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho cÃ¡c phÃ¢n chia Ä‘Ã o táº¡o, validation vÃ  test má»—i 20 epoch.

Chi tiáº¿t Ä‘Ã o táº¡o cho A UTOTRANSFER. ChÃºng tÃ´i tÃ³m táº¯t quy trÃ¬nh Ä‘Ã o táº¡o cho hÃ m chiáº¿u g() trong Thuáº­t toÃ¡n 2. ChÃºng tÃ´i Ä‘áº·t U = 12 vÃ  R = 5 xuyÃªn suá»‘t bÃ i bÃ¡o. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng má»™t táº­p há»£p cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh neo nhÆ° trong GraphGym. ChÃºng tÃ´i sá»­ dá»¥ng MLP hai lá»›p vá»›i kÃ­ch thÆ°á»›c áº©n 16 Ä‘á»ƒ tham sá»‘ hÃ³a hÃ m chiáº¿u g(). ChÃºng tÃ´i sá»­ dá»¥ng trÃ¬nh tá»‘i Æ°u Adam vá»›i tá»‘c Ä‘á»™ há»c 5e-3. ChÃºng tÃ´i sá»­ dá»¥ng margin = 0.1 vÃ  Ä‘Ã o táº¡o máº¡ng trong 1000 láº§n láº·p vá»›i kÃ­ch thÆ°á»›c batch 128. ChÃºng tÃ´i Ã¡p dá»¥ng K = 16 khi chá»n K thá»­ nghiá»‡m hÃ ng Ä‘áº§u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ³m táº¯t cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿.

Chi tiáº¿t cho viá»‡c Ä‘iá»u chá»‰nh TPE, thuáº­t toÃ¡n tiáº¿n hÃ³a. ChÃºng tÃ´i minh há»a cÃ¡ch káº¿t há»£p cÃ¡c thuáº­t toÃ¡n tÃ¬m kiáº¿m vá»›i cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao. TPE lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ Bayesian, cÃ³ nghÄ©a lÃ  nÃ³ Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i má»™t phÃ¢n phá»‘i tiÃªn nghiá»‡m Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a khÃ´ng gian tÃ¬m kiáº¿m vÃ  cáº­p nháº­t tiÃªn nghiá»‡m khi nÃ³ Ä‘Ã¡nh giÃ¡ cÃ¡c cáº¥u hÃ¬nh siÃªu tham sá»‘ vÃ  ghi láº¡i hiá»‡u suáº¥t cá»§a chÃºng. ChÃºng tÃ´i thay tháº¿ phÃ¢n phá»‘i tiÃªn nghiá»‡m nÃ y báº±ng cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng tin nhiá»‡m vá»¥. VÃ¬ cÃ¡c thuáº­t toÃ¡n tiáº¿n hÃ³a thÆ°á»ng khá»Ÿi táº¡o má»™t quáº§n thá»ƒ lá»›n vÃ  láº·p Ä‘i láº·p láº¡i loáº¡i bá» vÃ  Ä‘á»™t biáº¿n cÃ¡c máº¡ng hiá»‡n cÃ³, chÃºng tÃ´i thay tháº¿ viá»‡c khá»Ÿi táº¡o máº¡ng ngáº«u nhiÃªn báº±ng cÃ¡c tiÃªn nghiá»‡m thiáº¿t káº¿ Ä‘Æ°á»£c thÃ´ng tin nhiá»‡m vá»¥. VÃ¬ chÃºng tÃ´i chá»§ yáº¿u táº­p trung vÃ o cháº¿ Ä‘á»™ tÃ¬m kiáº¿m Ã­t thá»­ nghiá»‡m, chÃºng tÃ´i Ä‘áº·t cÃ¡c thá»­ nghiá»‡m khá»Ÿi Ä‘á»™ng hoÃ n toÃ n ngáº«u nhiÃªn thÃ nh 5 cho cáº£ thuáº­t toÃ¡n TPE vÃ  tiáº¿n hÃ³a.

Báº£ng 3: CÃ¡c lá»±a chá»n thiáº¿t káº¿ trong khÃ´ng gian tÃ¬m kiáº¿m cá»§a chÃºng tÃ´i
Loáº¡i Lá»±a chá»n
Convolution GeneralConv, GCNConv, SAGEConv, GINConv, GATConv
Sá»‘ Ä‘áº§u 1, 2, 4
Aggregation Sum, Mean-Pooling, Max-Pooling
Activation ReLU, pReLU, leaky_ReLU, ELU
KÃ­ch thÆ°á»›c áº©n 64, 256
Káº¿t ná»‘i lá»›p Stack, Skip-Sum, Skip-Concat
Lá»›p tiá»n xá»­ lÃ½ 1, 2
Lá»›p truyá»n tin 2, 4, 6, 8
Lá»›p háº­u xá»­ lÃ½ 2, 3
Tá»‘c Ä‘á»™ há»c 0.1, 0.001
Epoch Ä‘Ã o táº¡o 200, 800, 1600

B THáº¢O LUáº¬N Bá»” SUNG
Háº¡n cháº¿. Vá» nguyÃªn táº¯c, AUTOTRANSFER táº­n dá»¥ng tÆ°Æ¡ng quan giá»¯a cÃ¡c xáº¿p háº¡ng hiá»‡u suáº¥t mÃ´ hÃ¬nh giá»¯a cÃ¡c nhiá»‡m vá»¥ Ä‘á»ƒ xÃ¢y dá»±ng hiá»‡u quáº£ cÃ¡c tiÃªn nghiá»‡m mÃ´ hÃ¬nh. Do Ä‘Ã³, nÃ³ Ã­t hiá»‡u quáº£ hÆ¡n náº¿u nhiá»‡m vá»¥ má»›i cÃ³ khoáº£ng cÃ¡ch nhiá»‡m vá»¥ lá»›n Ä‘á»‘i vá»›i táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ trong ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh. Trong thá»±c táº¿, ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ liÃªn tá»¥c thÃªm cÃ¡c thá»­ nghiá»‡m tÃ¬m kiáº¿m bá»• sung vÃ o ngÃ¢n hÃ ng. Khi kÃ­ch thÆ°á»›c ngÃ¢n hÃ ng tÄƒng lÃªn, sáº½ Ã­t cÃ³ kháº£ nÄƒng má»™t nhiá»‡m vá»¥ má»›i cÃ³ tÆ°Æ¡ng quan tháº¥p vá»›i táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥ trong ngÃ¢n hÃ ng.

14

--- TRANG 15 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
Thuáº­t toÃ¡n 2 Quy trÃ¬nh ÄÃ o táº¡o cho hÃ m chiáº¿u g()
YÃªu cáº§u: CÃ¡c Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ {z(i)f|T(i)} Ä‘Æ°á»£c trÃ­ch xuáº¥t cho má»—i nhiá»‡m vá»¥ tá»« ngÃ¢n hÃ ng nhiá»‡m vá»¥-mÃ´ hÃ¬nh. Khoáº£ng cÃ¡ch Ä‘o dg(Â·;Â·) Ä‘Æ°á»£c Æ°á»›c tÃ­nh trong GraphGym.
1: for má»—i láº§n láº·p do
2: Láº¥y máº«u T(i), T(j), T(k)
3: z(i)e; z(j)e; z(k)e â† g(z(i)f); g(z(j)f); g(z(k)f)
4: y â† 1 náº¿u dg(T(i); T(j)) < dg(T(i); T(k)) else âˆ’1
5: Tá»‘i Æ°u hÃ³a hÃ m má»¥c tiÃªu Lr(z(i)e; z(j)e; z(k)e; y) trong Eq. 2
6: end for

TÃ¡c Ä‘á»™ng XÃ£ há»™i. Má»¥c tiÃªu dÃ i háº¡n cá»§a chÃºng tÃ´i lÃ  cung cáº¥p má»™t cÆ¡ sá»Ÿ háº¡ táº§ng GNN liá»n máº¡ch Ä‘Æ¡n giáº£n hÃ³a viá»‡c Ä‘Ã o táº¡o vÃ  triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh ML trÃªn dá»¯ liá»‡u cÃ³ cáº¥u trÃºc. CÃ¡c thuáº­t toÃ¡n AutoML hiá»‡u quáº£ vÃ  máº¡nh máº½ ráº¥t quan trá»ng Ä‘á»ƒ lÃ m cho há»c sÃ¢u dá»… tiáº¿p cáº­n hÆ¡n vá»›i nhá»¯ng ngÆ°á»i quan tÃ¢m nhÆ°ng thiáº¿u chuyÃªn mÃ´n há»c sÃ¢u cÅ©ng nhÆ° nhá»¯ng ngÆ°á»i thiáº¿u ngÃ¢n sÃ¡ch tÃ­nh toÃ¡n khá»•ng lá»“ mÃ  AutoML truyá»n thá»‘ng yÃªu cáº§u. ChÃºng tÃ´i tin ráº±ng bÃ i bÃ¡o nÃ y lÃ  má»™t bÆ°á»›c quan trá»ng Ä‘á»ƒ cung cáº¥p cÃ¡c cÃ´ng cá»¥ AI cho má»™t dÃ¢n sá»‘ rá»™ng lá»›n hÆ¡n vÃ  do Ä‘Ã³ cho phÃ©p AI giÃºp tÄƒng cÆ°á»ng nÄƒng suáº¥t con ngÆ°á»i. CÃ¡c táº­p dá»¯ liá»‡u chÃºng tÃ´i sá»­ dá»¥ng cho thÃ­ nghiá»‡m náº±m trong sá»‘ cÃ¡c Ä‘iá»ƒm chuáº©n Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t, khÃ´ng nÃªn chá»©a báº¥t ká»³ thiÃªn kiáº¿n khÃ´ng mong muá»‘n nÃ o. BÃªn cáº¡nh Ä‘Ã³, máº¥t mÃ¡t/Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã o táº¡o/test lÃ  cÃ¡c thá»‘ng kÃª Ä‘Æ°á»£c tÃ³m táº¯t cao mÃ  chÃºng tÃ´i tin khÃ´ng nÃªn gÃ¢y ra cÃ¡c váº¥n Ä‘á» vá» quyá»n riÃªng tÆ° tiá»m nÄƒng.

C Káº¾T QUáº¢ Bá»” SUNG
Hiá»‡u quáº£ tÃ¬m kiáº¿m. ChÃºng tÃ´i tÃ³m táº¯t sá»‘ lÆ°á»£ng thá»­ nghiá»‡m trung bÃ¬nh cáº§n thiáº¿t Ä‘á»ƒ vÆ°á»£t qua Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t trung bÃ¬nh Ä‘Æ°á»£c tÃ¬m tháº¥y bá»Ÿi TPE vá»›i 30 thá»­ nghiá»‡m trong Báº£ng 4. ChÃºng tÃ´i chá»‰ ra ráº±ng AUTOTRANSFER giáº£m sá»‘ lÆ°á»£ng kiáº¿n trÃºc Ä‘Æ°á»£c khÃ¡m phÃ¡ xuá»‘ng má»™t báº­c Ä‘á»™ lá»›n.

Báº£ng 4: Sá»‘ lÆ°á»£ng thá»­ nghiá»‡m tÃ¬m kiáº¿m trung bÃ¬nh cáº§n thiáº¿t Ä‘á»ƒ vÆ°á»£t qua káº¿t quáº£ tá»‘t nháº¥t trung bÃ¬nh Ä‘Æ°á»£c tÃ¬m tháº¥y bá»Ÿi TPE vá»›i 30 thá»­ nghiá»‡m

NÃºt Äá»“ thá»‹
Physics CoraFull OGB-Arxiv COX2 IMDB PROTEINS
Sá»‘ Thá»­ nghiá»‡m 3 2 3 4 3 6
Äá»™ chÃ­nh xÃ¡c 96.64Â±0.42 67.85 Â± 1.31 71.42 Â± 0.39 82.96Â±1.75 52.33 Â± 2.13 80.21 Â± 1.21

NghiÃªn cá»©u loáº¡i bá» vá» sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo vÃ  thiáº¿t káº¿ nhÃºng nhiá»‡m vá»¥. ChÃºng tÃ´i chá»©ng minh thá»±c nghiá»‡m cÃ¡ch sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo áº£nh hÆ°á»Ÿng Ä‘áº¿n tÆ°Æ¡ng quan xáº¿p háº¡ng trong Báº£ng 5. Trong khi 3 mÃ´ hÃ¬nh neo khÃ´ng Ä‘á»§ Ä‘á»ƒ náº¯m báº¯t khoáº£ng cÃ¡ch nhiá»‡m vá»¥, chÃºng tÃ´i tháº¥y ráº±ng 9 vÃ  12 cÃ³ sá»± Ä‘Ã¡nh Ä‘á»•i thá»a Ä‘Ã¡ng giá»¯a viá»‡c náº¯m báº¯t khoáº£ng cÃ¡ch nhiá»‡m vá»¥ vÃ  hiá»‡u quáº£ tÃ­nh toÃ¡n. HÆ¡n ná»¯a, chÃºng tÃ´i chá»©ng minh thá»±c nghiá»‡m trong Báº£ng 6 ráº±ng khÃ´ng gian nhÃºng nhiá»‡m vá»¥ Ä‘Ã£ há»c tá»‘t hÆ¡n khÃ´ng gian Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ Ä‘Æ°á»£c Ä‘á» xuáº¥t vá» máº·t tÆ°Æ¡ng quan cÅ©ng nhÆ° hiá»‡u suáº¥t tÃ¬m kiáº¿m cuá»‘i cÃ¹ng.

Báº£ng 5: TÆ°Æ¡ng quan xáº¿p háº¡ng Kendall trung bÃ¬nh cá»§a cÃ¡c xáº¿p háº¡ng tÆ°Æ¡ng tá»± cá»§a cÃ¡c nhiá»‡m vá»¥ khÃ¡c Ä‘á»‘i vá»›i nhiá»‡m vá»¥ trung tÃ¢m giá»¯a phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t vÃ  GraphGym.

Sá»‘ mÃ´ hÃ¬nh neo 3 6 9 12
Äáº·c trÆ°ng Nhiá»‡m vá»¥ 0.03Â±0.34 0.11 Â± 0.36 0.16 Â± 0.34 0.18 Â± 0.30
NhÃºng Nhiá»‡m vá»¥ 0.12Â±0.28 0.26 Â± 0.30 0.36 Â± 0.24 0.43 Â± 0.22

HÃ¬nh dung cÃ¡c thiáº¿t káº¿ mÃ´ hÃ¬nh. ChÃºng tÃ´i hÃ¬nh dung cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ sá»± tháº­t gá»‘c trÃªn táº­p dá»¯ liá»‡u TU-PROTEINS trong HÃ¬nh 5, cÅ©ng nhÆ° táº­p dá»¯ liá»‡u Coauthor-Physics trong HÃ¬nh 6. ChÃºng ta cÃ³ thá»ƒ quan sÃ¡t ráº±ng cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao cÃ³ tÆ°Æ¡ng quan tÃ­ch cá»±c trÃªn háº§u háº¿t cÃ¡c lá»±a chá»n thiáº¿t káº¿.

15

--- TRANG 16 ---
ÄÄƒng táº£i nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2023
Báº£ng 6: NghiÃªn cá»©u loáº¡i bá» vá» sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh neo cÅ©ng nhÆ° nhÃºng nhiá»‡m vá»¥ so vá»›i Ä‘áº·c trÆ°ng nhiá»‡m vá»¥ trÃªn OGB-Arxiv vá»›i 3 thá»­ nghiá»‡m. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c test trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n qua mÆ°á»i láº§n cháº¡y.

Sá»‘ mÃ´ hÃ¬nh neo 3 6 9 12
Äáº·c trÆ°ng Nhiá»‡m vá»¥ 69.42Â±0.82 69.86 Â± 0.78 70.41 Â± 0.59 70.67 Â± 0.52
NhÃºng Nhiá»‡m vá»¥ 69.80Â±0.75 70.59 Â± 0.63 71.16 Â± 0.47 71.42 Â± 0.39

1 2 0.0 0.1 0.2 0.3 0.4 0.5 gnn.layers_pre_mp
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

2 4 6 8 0.0 0.1 0.2 0.3 0.4 gnn.layers_mp
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

2 3 0.0 0.2 0.4 0.6 0.8 gnn.layers_post_mp
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

skipconcat skipsum stack 0.0 0.1 0.2 0.3 0.4 gnn.stage_type
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

add max mean 0.0 0.1 0.2 0.3 0.4 0.5 gnn.agg
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

256 64 0.0 0.1 0.2 0.3 0.4 0.5 0.6 gnn.dim_inner
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

gatconv gatconv2head gatconv4head generalconv ginconv sageconv gcnconv 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 gnn.layer_type
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

elu lrelu_01 prelu relu 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 gnn.act
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

0.001 0.01 0.0 0.1 0.2 0.3 0.4 0.5 0.6 optim.base_lr
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

1600 200 800 0.0 0.1 0.2 0.3 0.4 0.5 optim.max_epoch
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

HÃ¬nh 5: ChÃºng tÃ´i váº½ cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ sá»± tháº­t gá»‘c trÃªn táº­p dá»¯ liá»‡u TU-PROTEINS.

1 2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 gnn.layers_pre_mp
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

2 4 6 8 0.0 0.1 0.2 0.3 0.4 gnn.layers_mp
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

2 3 0.0 0.2 0.4 0.6 0.8 gnn.layers_post_mp
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

skipconcat skipsum stack 0.0 0.2 0.4 0.6 0.8 1.0 gnn.stage_type
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

add max mean 0.0 0.1 0.2 0.3 0.4 gnn.agg
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

256 64 0.0 0.2 0.4 0.6 0.8 gnn.dim_inner
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

gatconv gatconv2head gatconv4head generalconv ginconv sageconv gcnconv 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 gnn.layer_type
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

elu lrelu_01 prelu relu 0.0 0.1 0.2 0.3 0.4 0.5 0.6 gnn.act
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

0.001 0.01 0.0 0.1 0.2 0.3 0.4 0.5 0.6 optim.base_lr
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

1600 200 800 0.0 0.1 0.2 0.3 0.4 0.5 optim.max_epoch
TiÃªn nghiá»‡m Thiáº¿t káº¿ ÄÆ°á»£c Chuyá»ƒn giao
TiÃªn nghiá»‡m Sá»± tháº­t Gá»‘c

HÃ¬nh 6: ChÃºng tÃ´i váº½ cÃ¡c phÃ¢n phá»‘i thiáº¿t káº¿ Ä‘Æ°á»£c chuyá»ƒn giao vÃ  phÃ¢n phá»‘i thiáº¿t káº¿ sá»± tháº­t gá»‘c trÃªn táº­p dá»¯ liá»‡u Coauthor-Physics.

16
