# Tác động của Độ sâu đến Khả năng Tổng quát Hóa Thành phần trong Mô hình Ngôn ngữ Transformer

Jackson Petty†∗Sjoerd van Steenkiste§Ishita Dasgupta★Fei Sha§
Dan Garrette★Tal Linzen§
†New York University§Google Research★Google DeepMind
petty@nyu.edu {svansteenkiste,idg,fsha,dhgarrette,linzen}@google.com

## Tóm tắt

Để xử lý các câu mới, các mô hình ngôn ngữ (LM) phải tổng quát hóa thành phần—kết hợp các yếu tố quen thuộc theo những cách mới. Những khía cạnh nào của cấu trúc mô hình thúc đẩy khả năng tổng quát hóa thành phần? Tập trung vào transformer, chúng tôi kiểm tra giả thuyết, được thúc đẩy bởi công trình lý thuyết và thực nghiệm, rằng transformer sâu hơn tổng quát hóa thành phần tốt hơn. Việc đơn giản thêm các lớp sẽ tăng tổng số tham số; để giải quyết sự khó hiểu này giữa độ sâu và kích thước, chúng tôi xây dựng ba lớp mô hình trao đổi độ sâu với độ rộng sao cho tổng số tham số được giữ không đổi (41M, 134M và 374M tham số). Chúng tôi tiền huấn luyện tất cả các mô hình như LM và tinh chỉnh chúng trên các tác vụ kiểm tra khả năng tổng quát hóa thành phần. Chúng tôi báo cáo ba kết luận chính: (1) sau khi tinh chỉnh, các mô hình sâu hơn tổng quát hóa thành phần tốt hơn các mô hình nông hơn, nhưng lợi ích của việc thêm lớp giảm dần nhanh chóng; (2) trong mỗi nhóm, các mô hình sâu hơn cho thấy hiệu suất mô hình hóa ngôn ngữ tốt hơn, nhưng hiệu quả cũng giảm dần tương tự; (3) lợi ích của độ sâu đối với khả năng tổng quát hóa thành phần không thể được quy cho chỉ việc hiệu suất tốt hơn trong mô hình hóa ngôn ngữ. Bởi vì độ trễ mô hình gần như tuyến tính với số lượng lớp, những kết quả này dẫn chúng tôi đến khuyến nghị rằng, với ngân sách tham số tổng cho trước, transformer có thể được làm nông hơn so với điển hình mà không hy sinh hiệu suất.

## 1 Giới thiệu

Số lượng câu có thể có trong ngôn ngữ tự nhiên là rất lớn; bất kể kích thước của tập huấn luyện, một mô hình ngôn ngữ (LM) sẽ thường xuyên gặp phải những câu mà nó chưa từng thấy trước đây. Khả năng diễn giải những câu như vậy dựa vào khả năng tổng quát hóa thành phần: khả năng kết hợp các từ quen thuộc và cấu trúc cú pháp theo những cách mới (Montague, 1970; Fodor và Pylyshyn, 1988). Transformer LM (Vaswani et al., 2017), mặc dù rất thành công trong nhiều bối cảnh, thường gặp khó khăn khi được kiểm tra trên các benchmark đòi hỏi khả năng tổng quát hóa thành phần (Kim và Linzen, 2020). Những yếu tố kiến trúc nào ảnh hưởng đến khả năng tổng quát hóa thành phần của transformer?

Trong bài báo này, chúng tôi kiểm tra giả thuyết rằng việc tăng độ sâu của transformer—số lượng lớp mà nó có—cải thiện hiệu suất của nó trên các tác vụ đòi hỏi khả năng tổng quát hóa thành phần. Giả thuyết này được thúc đẩy cả bởi công trình lý thuyết, đã chỉ ra rằng việc thêm lớp tăng khả năng biểu đạt của mạng neural nói chung (Raghu et al., 2017) và transformer nói riêng (Merrill et al., 2021), và bởi công trình thực nghiệm cho thấy các mô hình sâu hơn tổng quát hóa thành phần tốt hơn các mô hình nông hơn (Mueller et al., 2022; Murty et al., 2023).

Mặc dù công trình thực nghiệm hiện tại mang lại một số độ tin cậy cho giả thuyết này, để xác nhận trực tiếp nó, chúng tôi phải giải quyết sự khó hiểu giữa độ sâu và kích thước (số lượng tham số). Vì mỗi lớp bổ sung đưa vào một tập tham số mới, các mô hình sâu hơn cũng lớn hơn, tất cả những điều khác đều bằng nhau, và hiệu suất của LM trên nhiều tác vụ được biết là tương quan với kích thước của chúng (Kaplan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023). Để giải quyết sự khó hiểu này, chúng tôi xây dựng các lớp mô hình có số lượng tham số tổng bằng nhau nhưng khác nhau về độ sâu; chúng tôi làm điều này bằng cách giảm chiều feed-forward của mô hình để bù đắp cho độ sâu được thêm vào. Chúng tôi tiền huấn luyện tất cả các mô hình như mô hình ngôn ngữ và tinh chỉnh chúng trên bốn tác vụ tổng quát hóa thành phần: các tác vụ phân tích ngữ nghĩa COGS (Kim và Linzen, 2020), COGS-vf (Qiu et al., 2022a) và GeoQuery (Zelle và Mooney, 1996), và phần bị động hóa tiếng Anh của Multilingual Transformations (Mueller et al., 2022). Trong tất cả những tác vụ này, mô hình được huấn luyện trên một phân phối dữ liệu cụ thể và được mong đợi tổng quát hóa đến một phân phối khác bằng cách kết hợp các yếu tố quen thuộc theo những cách mới.

Ngoài bất kỳ tác động trực tiếp có thể có nào đối với khả năng tổng quát hóa thành phần, độ sâu cũng có thể tương quan với các yếu tố khác mà bản thân chúng có thể dự đoán khả năng tổng quát hóa thành phần, chẳng hạn như loss mô hình hóa ngôn ngữ trong quá trình tiền huấn luyện hoặc hiệu suất trong phân phối trên tác vụ tinh chỉnh. Điều này làm phức tạp việc diễn giải bất kỳ mối quan hệ nào mà chúng tôi có thể tìm thấy giữa độ sâu và hiệu suất tổng quát hóa. Để giải quyết mối quan tâm này, chúng tôi cũng điều tra và hiệu chỉnh tác động của độ sâu đối với hiệu suất mô hình hóa ngôn ngữ và loss trong phân phối.

Chúng tôi báo cáo những phát hiện sau đây, trên ba lớp kích thước mô hình (41M, 134M và 374M tham số):

1. Nói chung, các mô hình sâu hơn có perplexity thấp hơn (Phần 3.1). Sự tăng hiệu suất biên từ việc thêm lớp giảm dần nhanh chóng khi các mô hình trở nên sâu hơn, và hiệu suất bắt đầu giảm khi chiều feed-forward tiếp cận với chiều của embedding ngữ cảnh hóa của mô hình.

2. Nói chung, các mô hình sâu hơn thể hiện khả năng tổng quát hóa thành phần tốt hơn (Phần 3.2). Một lần nữa, hầu hết lợi ích của độ sâu tích lũy từ một vài lớp đầu tiên; đối với một số benchmark tổng quát hóa thành phần mà chúng tôi sử dụng, hiệu suất bão hòa rất nhanh khi các mô hình trở nên sâu hơn.

3. Các mô hình sâu hơn tổng quát hóa thành phần tốt hơn ngay cả sau khi hiệu chỉnh cho việc perplexity mô hình hóa ngôn ngữ của chúng thấp hơn và hiệu suất trong phân phối của chúng trên tác vụ tinh chỉnh cao hơn (Phần 3.3).

4. Vì độ trễ của transformer gần như tuyến tính với độ sâu của chúng, trong nhiều trường hợp hiệu quả hơn khi làm cho mô hình rộng hơn thay vì sâu hơn, với ngân sách tham số cố định (Phần 4).

## 2 Phương pháp

### 2.1 Xây dựng Nhóm Mô hình có Số lượng Tham số Bằng nhau

Để làm cho transformer sâu hơn mà không tăng tổng số tham số, chúng ta cần làm cho nó hẹp hơn. Có một số cách để làm như vậy: chúng ta có thể giảm kích thước của chiều feed-forward d_ff, giảm kích thước của embedding ngữ cảnh d_model, hoặc giảm kích thước của đầu ra attention d_attn (xem Phụ lục B cho sơ đồ của một lớp transformer được chú thích với nhãn chiều). Vaswani et al. (2017) đã ghép nối những biến này ở d_model = d_attn = d_ff/4. Hầu hết transformer LM đã áp dụng tỷ lệ này (Devlin et al., 2019; Kaplan et al., 2020; Hoffmann et al., 2022, inter alia), mặc dù Raffel et al. (2019) đã tăng kích thước của d_ff so với d_model và d_attn cho hai mô hình lớn nhất của họ. Ngược lại, chúng tôi thay đổi d_ff với độ sâu (trong khi giữ d_model = d_attn không đổi). Bằng cách giữ cơ chế attention giống nhau trên các mô hình có độ sâu khác nhau, chúng tôi loại trừ khả năng độ sâu sẽ bị nhầm lẫn với khả năng của cơ chế self-attention. Chúng tôi gọi d_model/d_ff, thường được đặt thành 1/4, là tỷ lệ feed-forward.

**Suy ra các mối quan hệ siêu tham số.** Như điểm khởi đầu cho các lớp kích thước mô hình của chúng tôi, chúng tôi sử dụng các siêu tham số được lấy từ T5-base và T5-large (Raffel et al., 2019) cũng như một mô hình nhỏ hơn từ Kim và Linzen (2020) có các siêu tham số nội bộ lớp giống hệt với T5-small nhưng ít lớp hơn. Chúng tôi triển khai các mô hình bằng t5x (Roberts et al., 2022). Sau đó chúng tôi tính toán chiều feed-forward phải thay đổi bao nhiêu để phù hợp với việc thêm hoặc bớt lớp. Bắt đầu từ công thức trong Kaplan et al. (2020), số lượng tham số M trong một lớp là

M(d_ff) = 2d_model·d_ff + 4d_model·d_attn = β·d_ff + A,

trong đó hằng số β đại diện cho đóng góp của các tham số của khối feed-forward mà chiếu các vector từ R^d_model vào R^d_ff và trở lại vào R^d_model; và hằng số A đại diện cho các tham số của mọi thứ ngoài khối feed-forward, bao gồm cơ chế attention. Tổng số tham số của một mô hình đầy đủ N theo d_ff và n_layers sau đó là

N(n_layers, d_ff) = n_layers·M(d_ff) + 2d_model·n_vocab.

Cho các giá trị ban đầu (n_layers^0, d_ff^0) đặc trưng cho mô hình cơ sở trong mỗi lớp kích thước (ví dụ, T5-large), mục tiêu của chúng tôi là tìm các cặp k, w(k) sao cho

N(n_layers^0 + k, d_ff^0 - w(k)) = N(n_layers^0, d_ff^0).

Giải w như một hàm của k cho chúng tôi biết cần tăng (hoặc giảm) d_ff^0 bao nhiêu nếu chúng ta bớt (hoặc thêm) k lớp từ một mô hình hiện có:

w(k) = [(1 - n_layers^0/(n_layers^0 + k))d_ff^0 + A/β].     (1)

Vì việc thêm hoặc bớt k lớp có thể đòi hỏi thay đổi d_ff^0 bằng một lượng phân số, chúng tôi làm tròn w(k) về số nguyên gần nhất. Bảng 2 báo cáo các giá trị siêu tham số chính xác mà chúng tôi sử dụng cho mỗi lớp kích thước ba của chúng tôi, được suy ra từ Phương trình 1 ở trên, và Hình 1 cho thấy mỗi lớp kích thước được vẽ như các cặp (n_layers, d_ff).

### 2.2 Tập dữ liệu và Huấn luyện

#### 2.2.1 Mô hình hóa Ngôn ngữ

Chúng tôi sử dụng Colossal Clean Crawled Corpus (C4; Raffel et al. 2019) để tiền huấn luyện. Chúng tôi sử dụng kích thước ngữ cảnh n_ctx là 1024 token và kích thước batch là 128 chuỗi ≈ 131k token. Chúng tôi tiền huấn luyện mỗi mô hình trong 1M bước, dẫn đến tổng tập dữ liệu huấn luyện khoảng 131B token.

#### 2.2.2 Tổng quát hóa Thành phần

Trong các tập dữ liệu tổng quát hóa thành phần, các mô hình được kiểm tra trên một phân phối chứa các kết hợp mới của các mảnh, mỗi mảnh đã được thấy trước đó một cách độc lập trong quá trình huấn luyện. Chúng tôi tinh chỉnh các mô hình tiền huấn luyện của chúng tôi trên phần huấn luyện của tập dữ liệu trong 10.000 bước với kích thước batch là 128. Loss validation tiếp tục giảm trong suốt các lần chạy huấn luyện trên mỗi tập dữ liệu, vì vậy chúng tôi báo cáo các giá trị từ cuối mỗi lần chạy tinh chỉnh mà không dừng sớm. Chúng tôi sử dụng các tập dữ liệu sau (để xem ví dụ về các thể hiện của những tác vụ này, xem Bảng 1):

1. **COGS** (Kim và Linzen, 2020) là một tập dữ liệu phân tích ngữ nghĩa bao gồm các câu ngôn ngữ tự nhiên được ghép nối với các biểu diễn ngữ nghĩa hình thức. Nó được xây dựng sao cho phân phối tổng quát hóa ngoài miền chứa hai loại tổng quát hóa: các kết hợp mới của các từ quen thuộc (tổng quát hóa từ vựng, chẳng hạn như sử dụng từ 'hedgehog' làm tân ngữ của một câu khi từ này chỉ được thấy trong quá trình huấn luyện như một chủ ngữ); hoặc các kết hợp mới của các cấu trúc cú pháp quen thuộc (tổng quát hóa cấu trúc, chẳng hạn như các mệnh đề quan hệ được lồng sâu hơn so với những gì được thấy trong huấn luyện).

2. **Variable-free COGS** (COGS-vf; Qiu et al. 2022a) là một biến thể đơn giản hóa của COGS trong đó các biểu diễn ngữ nghĩa không sử dụng các biến được đánh số (xem Bảng 1 để so sánh giữa COGS và COGS-vf). Việc loại bỏ các biến khỏi biểu diễn có lợi ích là giảm chi phí tính toán liên quan của việc huấn luyện bằng cách làm cho các chuỗi ngắn hơn. Việc chuyển đổi này đã được chỉ ra trước đây là cải thiện hiệu suất của các mô hình bằng cách giảm độ phức tạp của không gian đầu ra (Qiu et al., 2022b), nhưng đi kèm với chi phí hạn chế khả năng của ngôn ngữ hình thức để biểu diễn các hiện tượng đòi hỏi sự phối hợp của danh tính biến, chẳng hạn như điều khiển và liên kết đại từ.

3. **GeoQuery** (Zelle và Mooney, 1996) chứa các câu hỏi ngôn ngữ tự nhiên về địa lý Hoa Kỳ được ghép nối với các truy vấn cơ sở dữ liệu kiểu SQL đại diện cho những câu hỏi đó. Chúng tôi báo cáo kết quả trên phân chia GeoQuery Standard.

4. **English passivization** (Mueller et al., 2022) là một tập dữ liệu các câu chủ động tiếng Anh được ghép nối với các câu bị động tương ứng của chúng (được chuyển thể từ Mulligan et al. 2021). Benchmark này được thiết kế để kiểm tra xem các mô hình sử dụng các heuristic nông, vị trí hay những heuristic có nguyên tắc cú pháp. Trong khi Mueller et al. (2022) đã triển khai một số phép biến đổi trong các ngôn ngữ khác nhau, chúng tôi tập trung vào tác vụ English Passivization.

## 3 Kết quả

### 3.1 Mô hình hóa Ngôn ngữ

**Các mô hình sâu hơn có perplexity thấp hơn.** Độ sâu có tác động đáng kể đến hiệu suất mô hình hóa ngôn ngữ. Ở đầu nông của phổ, việc tăng độ sâu dẫn đến cải thiện đáng kể về perplexity (Hình 2). Trong Hình 3a chúng tôi so sánh perplexity của mỗi mô hình trong một lớp kích thước so với mô hình có hiệu suất tốt nhất của lớp kích thước đó. Trong trường hợp cực đoan, perplexity của mô hình một lớp có thể gần gấp đôi so với mô hình tối ưu trong lớp. Hơn nữa, khi số lượng tham số tăng, sự chênh lệch giữa các mô hình tệ hơn, nông hơn và các mô hình tốt hơn, sâu hơn cũng tăng: Đối với các mô hình 41M tham số, tỷ lệ giữa perplexity của mô hình một lớp và mô hình tối ưu (5 lớp) là 1.59; đối với các mô hình 134M tham số, tỷ lệ là 1.86; và đối với các mô hình 374M tham số, tỷ lệ là 1.99.

**Hiệu suất tăng nhanh nhất trong vài lớp đầu tiên.** Mặc dù các mô hình sâu hơn nói chung hoạt động tốt hơn các mô hình nông hơn, việc tăng hiệu suất từ việc thêm lớp giảm dần nhanh chóng khi các mô hình trở nên sâu hơn (Hình 3a). Sự khác biệt hiệu suất giữa các mô hình 1 lớp và 2 lớp là đáng kể trên tất cả các lớp kích thước; việc chuyển từ 2 lên 4 lớp dẫn đến cải thiện hiệu suất khiêm tốn hơn nhiều. Chúng tôi cũng lưu ý rằng khi các mô hình trở nên lớn hơn trong thiết lập của chúng tôi, chúng có thể sử dụng hiệu quả ngày càng nhiều lớp hơn: mô hình 41M tham số tối ưu trong thiết lập của chúng tôi có 5 lớp, trong khi mô hình 134M tham số tối ưu có 12; trong số các mô hình 374M tham số, mô hình 24 lớp có hiệu suất tốt nhất. Đồng thời, mô hình của tính hữu ích giảm dần của độ sâu vẫn giữ nguyên ngay cả đối với các mô hình lớn nhất mà chúng tôi nghiên cứu.

**Hiệu suất giảm khi các mô hình trở nên quá hẹp.** Ở đầu sâu hơn của thang đo của chúng tôi, việc thêm lớp không chỉ không hữu ích cho hiệu suất mà còn bắt đầu làm hại nó (xem phía bên phải của mỗi đường cong lớp kích thước trong Hình 3a). Như đã lưu ý trước đây, điểm mà việc trao đổi độ rộng với độ sâu trở nên có hại không phải là một hàm tuyệt đối của độ sâu, vì độ sâu của mô hình tối ưu khác nhau giữa các lớp. Tuy nhiên, việc so sánh hiệu suất tương đối của các mô hình trong một lớp kích thước với tỷ lệ feed-forward d_model/d_ff cho thấy hiệu suất mô hình bắt đầu xấu đi khi d_ff trở nên nhỏ hơn d_model (ở bên phải của đường đứt nét đỏ trong Hình 3b); khi điều này xảy ra, phép chiếu affine của các vector từ R^d_model vào R^d_ff trở thành một ánh xạ không đơn ánh. Trong Phần 5 chúng tôi phân tích các ma trận trọng số của các phép biến đổi affine trong mạng feed-forward của mỗi lớp và chứng minh rằng khi d_model/d_ff tăng, các phép biến đổi trở nên ngày càng thiếu hạng.

**Các mô hình lớn hơn có thể chịu được các tỷ lệ feed-forward cực đoan hơn.** Việc thay đổi d_ff trong khi giữ d_model không đổi dẫn đến các tỷ lệ feed-forward d_model/d_ff lệch đáng kể khỏi tỷ lệ 1/4, đây là tiêu chuẩn de-facto trong tài liệu (quy tắc dọc đen trong Hình 3b). Chúng tôi thấy rằng các mô hình nhỏ hơn nhạy cảm hơn với giá trị cụ thể của tỷ lệ feed-forward, và đối với các mô hình nhỏ, tỷ lệ tiêu chuẩn có thể không tối ưu. Trong lớp kích thước 41M tham số có một phạm vi hẹp của các tỷ lệ feed-forward trong đó hiệu suất mô hình nằm trong vài phần trăm của mô hình tốt nhất trong lớp. Khi các mô hình trở nên lớn hơn, phạm vi này mở rộng về phía trái để bao gồm các mô hình có mạng feed-forward ngày càng rộng so với kích thước của embedding ngữ cảnh của chúng. Nói cách khác, các mô hình lớn hơn có nhiều dư địa hơn để trao đổi độ sâu với độ rộng, trở nên rộng hơn tỷ lệ với chiều mô hình d_model của chúng mà không chịu phạt lớn cho perplexity của chúng. Hơn nữa, khi d_model/d_ff < 1, tỷ lệ feed-forward không dự đoán perplexity tương đối của một mô hình độc lập với kích thước của nó.

### 3.2 Tổng quát hóa Thành phần

Tiếp theo chúng tôi tinh chỉnh các mô hình được tiền huấn luyện trong phần trước trên các phần huấn luyện của mỗi tập dữ liệu tổng quát hóa thành phần, và đo độ chính xác chuỗi đầy đủ (khớp chính xác) của các mô hình trên tập tổng quát hóa ngoài phân phối.

**Các mô hình sâu hơn tổng quát tốt hơn.** Như với hiệu suất mô hình hóa ngôn ngữ, các mô hình sâu hơn có xu hướng đạt được độ chính xác tổng quát hóa cao hơn các mô hình nông hơn trong cùng lớp kích thước (Hình 4). Tác động của độ sâu đối với tổng quát hóa thành phần thay đổi nhiều hơn so với đối với mô hình hóa ngôn ngữ: đối với COGS, COGS-vf và GeoQuery có một số tính không đơn điệu nhỏ trong độ chính xác tổng quát hóa như một hàm của độ sâu. Trên English Passivization, các lớp 41M và 134M tham số cho thấy các xu hướng chủ yếu nhất quán trong đó các mô hình sâu hơn hoạt động tốt hơn các mô hình nông hơn; các mô hình 374M tham số cho thấy tính không đơn điệu đáng kể hơn, mặc dù các mô hình sâu nhất vẫn vượt trội so với các mô hình nông nhất.

**Lợi ích của độ sâu bão hòa nhanh chóng đối với một số tác vụ.** Như với mô hình hóa ngôn ngữ, hầu hết lợi ích của độ sâu được đạt được bằng cách chỉ có một vài lớp. Đối với ba tác vụ—COGS, COGS-vf và GeoQuery—chúng tôi thấy các độ sâu ngưỡng sau đó độ chính xác tổng quát hóa vẫn tương đối không đổi khi độ sâu tăng. Những độ sâu ngưỡng này thấp và không đổi trên các kích thước mô hình, nhưng thay đổi theo tập dữ liệu: 4–6 lớp cho COGS, và 2–4 lớp cho COGS-vf và GeoQuery. Hiệu suất trên COGS-vf có vẻ bão hòa với ít lớp hơn so với COGS mặc dù hai tập dữ liệu thể hiện cùng các hiện tượng ngôn ngữ; điều này cho thấy sự bão hòa mà chúng tôi quan sát trên một số tập dữ liệu có liên quan chặt chẽ đến độ phức tạp của biểu diễn đầu ra độc lập với độ phức tạp của tổng quát hóa thành phần được thể hiện trong dữ liệu. Trên English Passivization, tác động của độ sâu thay đổi nhiều hơn, điều này làm cho việc xác định một ngưỡng độc lập với kích thước trở nên khó khăn.

Các hiệu ứng ngưỡng cho thấy rằng một số tập con của các tập dữ liệu có thể được giải quyết với các mô hình tương đối đơn giản. Chúng tôi điều tra giả thuyết này bằng cách phân tích riêng hiệu suất của các mô hình trên hai loại trường hợp tổng quát hóa được bao gồm trong COGS và COGS-vf: tổng quát hóa từ vựng, trong đó một từ quen thuộc cần được diễn giải trong một ngữ cảnh cú pháp quen thuộc mà nó chưa được quan sát; và tổng quát hóa cấu trúc, trong đó cấu trúc cú pháp là mới và cần được xây dựng từ các mảnh cú pháp quen thuộc. Chúng tôi thấy rằng ngay cả các mô hình sâu ở kích thước mô hình lớn nhất cũng thất bại một cách có hệ thống trong việc tổng quát hóa cấu trúc (Hình 5); lợi ích của độ sâu chủ yếu giới hạn ở các trường hợp tổng quát hóa từ vựng dễ hơn. Điều này hỗ trợ giả thuyết rằng hiệu ứng bão hòa của độ sâu là do sự tồn tại của các tập con dễ hơn của các tập dữ liệu, và cho thấy việc tăng độ sâu một mình không cải thiện đáng kể khả năng của các mô hình để học được bias quy nạp đúng cho những tác vụ cấu trúc này.

### 3.3 Hiệu ứng Độ sâu là Độc lập giữa các Tác vụ Upstream và Downstream

Chúng tôi đã chỉ ra rằng các mô hình sâu hơn tổng quát tốt hơn các mô hình nông hơn. Nhưng trong Phần 3.1 chúng tôi cũng chỉ ra rằng các mô hình sâu hơn đạt được perplexity validation thấp hơn trong tiền huấn luyện so với các mô hình nông hơn; và các mô hình sâu hơn đạt được loss trong phân phối thấp hơn trên các tác vụ tinh chỉnh so với các mô hình nông hơn (Hình 7a). Cả hai quan sát này đều tạo thành các confound tiềm năng cho việc diễn giải phần trước: có thể độ sâu không trực tiếp cải thiện độ chính xác tổng quát hóa, mà chỉ làm như vậy một cách gián tiếp bằng cách cải thiện hiệu suất mô hình hóa ngôn ngữ hoặc độ chính xác trong phân phối trên tác vụ tinh chỉnh, điều này sau đó dẫn đến tổng quát hóa tốt hơn. Để xác định xem đây có phải là trường hợp hay không, chúng tôi hiệu chỉnh cho cả hai confound tiềm năng này.

Đầu tiên, để hiệu chỉnh cho loss tiền huấn luyện thấp hơn của các mô hình sâu hơn, chúng tôi lặp lại các thí nghiệm tinh chỉnh của mình bằng các checkpoint trung gian của các mô hình tiền huấn luyện có perplexity validation bằng nhau trong một lớp kích thước. Chúng tôi chọn mô hình ít hiệu suất nhất (tức là nông nhất) trong một lớp kích thước làm mô hình tham chiếu và ghi nhận perplexity validation của nó ở cuối tiền huấn luyện. Sau đó chúng tôi chọn các checkpoint trung gian của tất cả các mô hình sâu hơn tại thời điểm trong quá trình tiền huấn luyện khi chúng đạt được perplexity tham chiếu này (Hình 6a). Cuối cùng, chúng tôi tinh chỉnh mỗi checkpoint này trên các tác vụ tổng quát hóa thành phần. Chúng tôi lặp lại quá trình này cho các mô hình tham chiếu sâu dần. Chúng tôi thấy rằng ngay cả khi tinh chỉnh từ các checkpoint có perplexity validation bằng nhau, các mô hình sâu hơn vẫn tổng quát tốt hơn các mô hình nông hơn (Hình 6b).

Tiếp theo, để hiệu chỉnh cho việc các mô hình sâu hơn hoạt động tốt hơn các mô hình nông hơn trên phần trong phân phối của các tác vụ tổng quát hóa thành phần, chúng tôi so sánh độ chính xác tổng quát hóa của các mô hình tại các điểm trong quá trình tinh chỉnh khi chúng có loss trong phân phối bằng nhau. Hình 7b cho thấy rằng ngay cả sau khi điều chỉnh cho hiệu suất trong phân phối, các mô hình sâu hơn vẫn đạt được độ chính xác cao hơn trên tập tổng quát hóa ngoài phân phối so với các mô hình nông hơn.

## 4 Độ trễ Huấn luyện và Suy luận

Những ý nghĩa thực tế của việc lợi ích của độ sâu bão hòa sau một số ít lớp là gì? Theo kinh nghiệm, chi phí tính toán của việc huấn luyện và chạy các mô hình tham số bằng nhau của chúng tôi thể hiện mối quan hệ tuyến tính mạnh với độ sâu. Hình 8 cho thấy (đối với các mô hình lớn nhất của chúng tôi) rằng độ trễ trong quá trình huấn luyện tăng tuyến tính với độ sâu của mô hình. Nguyên nhân của hình phạt này là hai mặt. Đầu tiên, lựa chọn của chúng tôi là sử dụng chiều feed-forward hẹp hơn cho các mô hình sâu hơn để duy trì số lượng tham số tổng không đổi dẫn đến số lượng phép toán dấu phẩy động (FLOP) cao hơn một chút cho các mô hình sâu hơn. Để thấy điều này, chúng tôi bắt đầu từ công thức chi phí được giới thiệu trong Kaplan et al. (2020):

C_forward = 2N + 2n_layers·n_ctx·d_attn,

trong đó N là tổng số lượng tham số. Vì cả n_ctx và d_attn đều không đổi cho tất cả các mô hình của một kích thước cụ thể (vì d_attn và d_ff được tách rời khỏi nhau), tổng số FLOP của một mô hình tuyến tính với độ sâu, mặc dù thuật ngữ này bị chi phối bởi thuật ngữ 2N trừ khi độ sâu mô hình, kích thước attention hoặc độ dài ngữ cảnh trở nên rất lớn.

Lý do thứ hai và có thể quan trọng hơn khiến các mô hình sâu hơn chậm hơn là vì các tính toán ở lớp k phụ thuộc vào kết quả của các tính toán ở lớp k-1. Do sự phụ thuộc tuần tự này, tính song song không thể được áp dụng qua các lớp (Tay et al., 2021).

Kết hợp với tính hữu ích giảm dần của độ sâu đối với hiệu suất, chi phí độ trễ tuyến tính của độ sâu khi tổng kích thước được giữ không đổi dẫn chúng tôi đến các khuyến nghị thực tế sau:

1. Khi cố gắng giảm thiểu giờ GPU cho khối lượng dữ liệu cố định, các mô hình nông hơn có thể huấn luyện trong thời gian ít hơn nhiều so với các mô hình sâu hơn trong khi vẫn đạt được mức độ hiệu suất chấp nhận được so với mô hình có hiệu suất tốt nhất trong một lớp kích thước cho trước.

2. Với ngân sách giờ GPU cố định để huấn luyện, các mô hình nông hơn có thể huấn luyện trên nhiều dữ liệu hơn các mô hình sâu hơn có thể trong bất kỳ khoảng thời gian cố định nào, vì các mô hình nông hơn có độ trễ thấp hơn, có thể dẫn đến hiệu suất tốt hơn so với các mô hình sâu hơn được huấn luyện trên ít dữ liệu hơn.

Những lợi ích này không chỉ giới hạn ở huấn luyện: các mô hình sâu hơn cũng chịu chi phí cho mỗi lớp trong quá trình suy luận. Điều này có nghĩa là hình phạt mà một mô hình sâu hơn phải trả phải được khấu hao trong suốt chi phí tuổi thọ của việc sử dụng mô hình để suy luận. Ở đây cũng vậy, việc giảm thời gian GPU được dành cho suy luận bằng cách sử dụng các mô hình nông hơn có hiệu suất tương đương có thể giảm chi phí tính toán, cả cho các mô hình được phục vụ trên cloud và cho suy luận trên thiết bị (Strubell et al., 2019; Pope et al., 2022; Gupta và Agrawal, 2022).

## 5 Phân tích các Phép biến đổi Feed-Forward

Ở các tỷ lệ feed-forward cực đoan, chúng tôi quan sát rằng hiệu suất mô hình giảm. Chúng tôi điều tra vai trò mà tỷ lệ feed-forward đóng trong hiệu suất quan sát được của các mô hình của chúng tôi. Khi d_ff nhỏ hơn d_model, phép biến đổi này có tính mất mát, nhưng các giá trị nhỏ của d_ff vẫn có thể tác động đến hiệu suất ngay cả khi d_ff vẫn lớn hơn d_model. Để xác định xem đó có phải là trường hợp hay không, chúng tôi tiến hành phân tích hạng trên các phép biến đổi affine tạo thành khối feed-forward. Đối với một phép biến đổi affine T cho trước, chúng tôi tính toán các giá trị kỳ dị được sắp xếp {σ₁, σ₂, ..., σₖ} trong đó k = min(d_model, d_ff) là hạng của T và σᵢ ≥ σᵢ₊₁. Sau đó chúng tôi chuẩn hóa mỗi giá trị bằng cách chia cho chuẩn ℓ₁ của {σ₁, ..., σₖ} để tính toán bao nhiêu ảnh của T được tính đến bởi xấp xỉ hạng i tốt nhất của T cho i ≤ k. Hình 9 cho thấy cách trong các mô hình sâu hơn (tức là những mô hình có tỷ lệ d_model/d_ff ngày càng lớn) các phép biến đổi trở nên ngày càng lệch khỏi việc sử dụng đầy đủ các hạng có sẵn của chúng.

## 6 Công trình Liên quan

**Tính thành phần.** Công trình trước đây đã khám phá mức độ mà các mô hình neural thể hiện hành vi thành phần bằng cách huấn luyện hoặc tinh chỉnh các mô hình trên các tác vụ thành phần như chuỗi lệnh đơn giản (Lake và Baroni, 2018) hoặc phân tích ngữ nghĩa (Kim và Linzen, 2020; Keysers et al., 2020). Công trình khác đã khám phá các phương pháp để cải thiện hành vi thành phần của các mô hình, bao gồm thông qua tăng cường dữ liệu (Qiu et al., 2022a), các mô hình lớn hơn (Qiu et al., 2022b), và các thay đổi kiến trúc (Gordon et al., 2019; Csordás et al., 2021; Ontanon et al., 2022). Công trình của chúng tôi bổ sung cho những cách tiếp cận này bằng cách khám phá một thay đổi kiến trúc cụ thể: tăng độ sâu mà không thay đổi tổng kích thước mô hình.

**Tác động của độ sâu.** Công trình lý thuyết đã chỉ ra rằng khả năng biểu đạt của mạng neural nói chung (Raghu et al., 2017) và các mô hình transformer nói riêng (Merrill et al., 2021) tăng theo cấp số nhân với độ sâu. Công trình thực nghiệm cũng chỉ ra vai trò của độ sâu trong hiệu suất mô hình. Trong một bối cảnh tổng quát hơn, Tay et al. (2021) thấy rằng việc mở rộng theo độ sâu nói chung hữu ích hơn việc mở rộng theo độ rộng trên các tác vụ downstream, mặc dù họ không cố gắng kiểm soát kích thước. Đối với tổng quát hóa thành phần nói riêng, Mueller et al. (2022) thấy rằng việc giảm độ sâu có hại hơn việc giảm độ rộng đối với các mô hình encoder-decoder được tiền huấn luyện. Murty et al. (2023) quan sát rằng các transformer encoder sâu hơn thường có các biểu diễn giống cây hơn và độ chính xác phân tích cao hơn trên một số tác vụ thành phần. Làm dịu những kết quả tích cực này, Veit et al. (2016) lưu ý rằng trong các mô hình có kết nối dư, ngay cả các mạng rất sâu cũng chỉ tận dụng các mạng con nông có độ sâu gần như không đổi. Brown et al. (2022) cũng kết luận rằng các mô hình transformer rộng, nông có thể đạt được hiệu suất gần bằng với các mô hình sâu hơn. Tuy nhiên, cả hai tập kết quả đều bị nhầm lẫn bởi việc thiếu kiểm soát tổng số lượng tham số. Gần đây, Gromov et al. (2024) thấy rằng gần một nửa số lớp của các mô hình ngôn ngữ sâu có thể được cắt tỉa sau huấn luyện mà không làm hại đáng kể hiệu suất trên các tác vụ downstream.

**Các sơ đồ thoát sớm.** Nghiên cứu thoát sớm (Zhou et al. 2020; Schuster et al. 2022, inter alia) cho thấy rằng các mô hình sâu có thể được làm nông hơn một cách động bằng cách bỏ qua các lớp tính toán sau khi một heuristic đánh giá một biểu diễn được tính toán là "đủ tốt". Chúng tôi xem công trình của mình là bổ sung cho cách tiếp cận này; trong khi thoát sớm giảm chi phí tính toán của suy luận của mô hình trên cơ sở phụ thuộc vào đầu vào, công trình của chúng tôi cho thấy rằng chi phí có thể được giảm cho tất cả các đầu vào trong cả huấn luyện và suy luận. Tuy nhiên, vì chúng tôi không khám phá huấn luyện hoặc suy luận thoát sớm với các mô hình tham số bằng nhau của chúng tôi ở đây, có thể ngay cả các mô hình nông hơn của chúng tôi cũng có thể hưởng lợi từ các sơ đồ thoát sớm sẽ giảm thêm chi phí tính toán.

**Kiểm soát kích thước mô hình.** Có các cách tiếp cận khác nhau có thể để nghiên cứu tác động của các lựa chọn siêu tham số mà không ảnh hưởng đến kích thước mô hình ròng. Kaplan et al. (2020) đã biến đổi cùng số lượng lớp n_layers với chiều embedding ngữ cảnh d_model, mà họ ghép nối với d_attn nội bộ attention và chiều feed-forward ở tỷ lệ tiêu chuẩn d_model = d_attn = d_ff/4. Họ kết luận rằng sự gia tăng hiệu suất chủ yếu được thúc đẩy bởi việc tăng tổng số lượng tham số của các mô hình, và rằng trong "giới hạn hợp lý" perplexity mô hình hóa ngôn ngữ chỉ phụ thuộc yếu vào hình dạng (mặc dù Tay et al. 2021 kết luận rằng điều tương tự không đúng đối với hiệu suất trên các tác vụ downstream, nhưng đã làm như vậy mà không kiểm soát tác động của kích thước). Công trình của chúng tôi điều tra vai trò mà độ sâu đóng trên cả các tác vụ tiền huấn luyện và tinh chỉnh trong khi kiểm soát tổng số lượng tham số.

## 7 Kết luận

Tổng quát hóa thành phần là cần thiết để diễn giải các câu mới. Những khía cạnh nào của kiến trúc transformer LM góp phần vào một bias quy nạp ủng hộ tổng quát hóa thành phần? Trong một thí nghiệm được kiểm soát tách biệt độ sâu khỏi tổng số lượng tham số, chúng tôi thấy rằng transformer sâu hơn cho thấy tổng quát hóa thành phần tốt hơn, và hiệu suất mô hình hóa ngôn ngữ tốt hơn, độc lập với tổng số lượng tham số của chúng. Đồng thời, trong hầu hết các trường hợp, tính hữu ích của việc thêm lớp giảm nhanh chóng khi các mô hình trở nên sâu hơn: các mô hình tương đối nông có thể đạt được độ chính xác tổng quát hóa trên các tác vụ thành phần tương đương với các mô hình sâu hơn nhiều, và perplexity mô hình hóa ngôn ngữ trong vài phần trăm của mô hình tốt nhất trong lớp. Bởi vì transformer sâu hơn có độ trễ cao hơn, điều này chỉ ra rằng đối với một ngân sách tham số cho trước, các mô hình nông hơn có thể nhanh hơn đáng kể với sự hy sinh tối thiểu về hiệu suất.

## 8 Hạn chế

**Attention heads.** Chúng tôi không điều tra vai trò mà attention heads đóng trong tổng quát hóa thành phần một cách rộng rãi, cũng không điều tra cách chức năng của heads thay đổi với độ sâu. Công trình trước đây (Michel et al. 2019, inter alia) cho thấy rằng việc giảm số lượng attention heads trong một transformer (trước hoặc sau huấn luyện) không làm hại đáng kể hiệu suất. Công trình diễn giải cơ chế đã thấy rằng các attention heads cụ thể trong transformer học tính toán các chức năng cụ thể của tác vụ (Voita et al., 2019; Htut et al., 2019; Olsson et al., 2022). Những phát hiện của chúng tôi ở đây đặt ra hai câu hỏi cần được điều tra thêm: đầu tiên, liệu tính tương đối không quan trọng của số lượng attention heads vẫn còn giữ trong các chế độ khi một mô hình nông hơn và rộng hơn đáng kể so với quy ước; và thứ hai, liệu có bất kỳ attention heads nào trong các mô hình của chúng tôi học thực hiện các tính toán cụ thể thành phần, và điều này có thay đổi khi các mô hình trở nên sâu hơn hoặc nông hơn không?

**Các cách tiếp cận thay thế để kiểm soát tổng kích thước.** Cách tiếp cận của chúng tôi để kiểm soát tổng số lượng tham số đòi hỏi phải thực hiện trao đổi độ sâu-độ rộng. Một cách tiếp cận thay thế sẽ là xây dựng Universal Transformers (Dehghani et al., 2018), trong đó mỗi mô hình trong một lớp kích thước có một lớp transformer với cùng các tham số được lặp lại n_layers lần. Cách tiếp cận chia sẻ trọng số như vậy sẽ cho phép các mô hình sâu hơn có các mạng feed-forward rộng tùy ý, giảm thiểu tác động của việc làm cho các mô hình quá hẹp. Mặc dù việc chia sẻ trọng số như vậy ngăn các mô hình thực hiện tính toán khác nhau trong các lớp khác nhau, hạn chế như vậy thực tế có thể có lợi cho tổng quát hóa thành phần trong đó các tính toán tương tự (ví dụ, kết hợp hai cụm từ cú pháp thành một cụm từ lớn hơn) có thể cần áp dụng một cách đệ quy ở các quy mô khác nhau.

**Hiệu ứng corpus tiền huấn luyện.** Chúng tôi xem xét các mô hình được tiền huấn luyện trên dữ liệu ngôn ngữ tự nhiên. Đối với lựa chọn cụ thể của chúng tôi về các thí nghiệm tổng quát hóa thành phần, sự hiện diện của các mục từ vựng trong cả corpus tiền huấn luyện và các tập dữ liệu tổng quát hóa đại diện cho một confound tiềm năng của hiệu suất tổng quát hóa có thể được giảm thiểu bằng cách sửa đổi các tập dữ liệu thành phần (Kim et al., 2022). Tổng quát hơn, chúng tôi không nghiên cứu cách phân phối dữ liệu tiền huấn luyện ảnh hưởng đến các bias quy nạp được trao cho LM (Papadimitriou và Jurafsky, 2023). Như một lĩnh vực quan tâm cụ thể cho công trình tương lai, chúng tôi chỉ ra giả thuyết rằng việc bao gồm mã nguồn trong corpus tiền huấn luyện (OpenAI, 2023; Google et al., 2023) sẽ cải thiện tổng quát hóa thành phần.

**Tinh chỉnh so với học trong ngữ cảnh.** Chúng tôi sử dụng tinh chỉnh để thích ứng các mô hình tiền huấn luyện của chúng tôi với các tác vụ thành phần. Do chi phí tính toán và tính cụ thể của tác vụ, tinh chỉnh ít hữu ích hơn trong thực tế so với học trong ngữ cảnh khi kích thước mô hình tăng (Brown et al., 2020). Bởi vì học trong ngữ cảnh chỉ trở nên đáng tin cậy ở các quy mô lớn hơn nhiều so với những gì chúng tôi có thể huấn luyện, chúng tôi đã không khám phá tác động của độ sâu đối với độ chính xác tổng quát hóa thành phần trong học trong ngữ cảnh (Si et al., 2022); chúng tôi chỉ ra điều này như một hướng cho nghiên cứu tương lai.

## 9 Tuyên bố Đạo đức

Trong suốt quá trình thực nghiệm của chúng tôi, chúng tôi đã tìm cách tuân thủ các thực hành tốt nhất để giảm thiểu bất kỳ rủi ro nào liên quan đến nghiên cứu LLM. Chúng tôi sử dụng các tập dữ liệu mã nguồn mở có thể được kiểm tra bởi bên thứ ba cho các vấn đề như bias và độc tính. Chúng tôi không phát hành bất kỳ checkpoint công khai nào cho các mô hình mà chúng tôi huấn luyện, vì vậy không có rủi ro lạm dụng bất kỳ artifact đã tạo nào, mặc dù chúng tôi lưu ý rằng chúng tôi suy ra các mô hình được triển khai của mình từ các mô hình T5 có sẵn công khai hiện có. Chúng tôi huấn luyện các mô hình trên dữ liệu ngôn ngữ tự nhiên chỉ tiếng Anh, và cần có khám phá đầy đủ hơn để khám phá cách ngôn ngữ tác động đến các kết quả được tìm thấy ở đây.

## Lời cảm ơn

Chúng tôi cảm ơn Pete Shaw và Slav Petrov vì phản hồi hữu ích về các phiên bản trước của bài báo này.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc tiếng Anh do chứa nhiều tên riêng và thuật ngữ chuyên môn]

## A Bảng Thiết kế và Kết quả

Bảng 2 báo cáo các siêu tham số chính xác cho các lớp mô hình được huấn luyện. Bảng 3 hiển thị tiền huấn luyện và độ chính xác tổng quát hóa thành phần trên tất cả các kích thước mô hình và tác vụ.

## B Lớp Transformer được Chú thích

Hình 10 cho thấy sơ đồ cho một lớp transformer duy nhất. Đầu vào của các lớp vào ở bên trái và đi qua các thành phần mô hình khác nhau (hộp xám), được kết hợp với các kết nối dư trước khi thoát ra bên phải đến các lớp tiếp theo. Các hộp xanh dương hiển thị chiều của các vector sau khi biến đổi; chúng tôi chủ yếu quan tâm đến kích thước của các vector embedding d_model và chiều nội bộ của khối feed-forward d_ff. Kích thước của các vector nội bộ đối với cơ chế attention, d_attn, không được hiển thị ở đây nhưng thường được đặt bằng với d_model; chúng tôi tuân theo quy ước này ở đây. Các phép toán không học được như phép cộng, chuẩn hóa lớp và tính phi tuyến của mạng feed-forward được hiển thị trong các vòng tròn xám.

[Các bảng và hình được giữ nguyên cấu trúc như bản gốc]
