# 2309.01125.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2309.01125.pdf
# File size: 975434 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AutoML-GPT: Large Language Model for AutoML
Yun-Da Tsai1Yu-Che Tsai1Bo-Wei Huang1Chun-Pai Yang1Shou-De Lin1
Abstract
With the emerging trend of GPT models, we have
established a framework called AutoML-GPT that
integrates a comprehensive set of tools and li-
braries. This framework grants users access to
a wide range of data preprocessing techniques,
feature engineering methods, and model selection
algorithms. Through a conversational interface,
users can specify their requirements, constraints,
and evaluation metrics. Throughout the process,
AutoML-GPT employs advanced techniques for
hyperparameter optimization and model selection,
ensuring that the resulting model achieves optimal
performance. The system effectively manages
the complexity of the machine learning pipeline,
guiding users towards the best choices without
requiring deep domain knowledge. Through our
experimental results on diverse datasets, we have
demonstrated that AutoML-GPT significantly re-
duces the time and effort required for machine
learning tasks. Its ability to leverage the vast
knowledge encoded in large language models en-
ables it to provide valuable insights, identify po-
tential pitfalls, and suggest effective solutions to
common challenges faced during model training.
1. Introduction
Automated Machine Learning (AutoML) has gained signif-
icant attention in recent years as a powerful technique for
automating various stages of the machine learning work-
flow. It aims to simplify the model development process
by automatically searching, selecting, and optimizing ma-
chine learning models without requiring extensive manual
intervention. AutoML has the potential to democratize ma-
chine learning and make it accessible to a broader audience,
including non-experts and domain specialists. AutoML
1Department of Computer Science and Information Engineer-
ing, National Taiwan University, Taipei, Taiwan. Correspondence
to: Yun-Da Tsai <f08946007@csie.ntu.edu.tw >.
Proceedings of the 40thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).encompasses tasks such as data pre-processing, feature engi-
neering, model selection, and hyperparameter tuning. These
tasks require expertise, time, and computational resources.
To address these challenges, researchers have proposed vari-
ous approaches and frameworks for AutoML, such as Auto-
sklearn (Feurer et al., 2015), Auto-Keras (Jin et al., 2023),
and AutoGluon (Erickson et al., 2020). These approaches
aim to automate the process of model selection and config-
uration, making it easier for practitioners to build accurate
and efficient machine learning models.
Large language models, such as OpenAI’s GPT-3 (Brown
et al., 2020) and Google’s PaLM (Chowdhery et al., 2022),
have emerged as powerful tools in natural language process-
ing and understanding. These models have been extensively
trained on vast amounts of textual data and have demon-
strated remarkable capabilities in language understanding,
text generation, sentiment analysis, and other language-
related tasks. Large language models excel at capturing
complex patterns, understanding context, and generating
coherent responses. The strength of large language models
lies in their ability to comprehend and process unstructured
textual data effectively. They learn rich semantic representa-
tions of language, enabling them to understand the nuances
and subtleties present in text. By leveraging pre-trained
language models, researchers have achieved significant ad-
vancements in various natural language processing tasks,
including machine translation, question answering, and lan-
guage generation.
While large language models have found success in spe-
cific applications, their comprehensive integration into the
AutoML framework remains relatively unexplored. Exist-
ing research has primarily focused on utilizing language
models for individual tasks within AutoML, such as data
pre-processing and feature engineering. However, the po-
tential of leveraging these models to automate the entire
AutoML pipeline, from data preparation to hyperparameter
optimization, has not been extensively studied. This lack of
exploration serves as the motivation for this work. Large
language models (LLMs) exhibit exceptional strengths in
AutoML from multiple perspectives. In terms of data un-
derstanding, LLMs can be used for data preprocessing, ef-
ficiently handling missing values, normalizing or scaling
features, and detecting outliers. Additionally, large language
models have the ability to conduct correlation analysis, dis-
1arXiv:2309.01125v1  [cs.LG]  3 Sep 2023

--- PAGE 2 ---
AutoML-GPT: Large Language Model for AutoML
Reasoning Agent•Understand human request•Combining tools usage sequence•SubtaskmonitoringCoding Agent•Read documents and modules•AutoML code generation•Return code execution resultGenerate ThoughtsReturn ResultsAutoMLRequestFeedback & Suggestion•Target dataset •Optimize objective•MLrequest
User
Figure 1. Pipeline of AutoML-GPT.
cover causal relationships, and perform feature selection.
This enables them to effectively identify and eliminate ir-
relevant or redundant features. Furthermore, these models
contribute to the identification of potential models that are
well-suited for a given dataset and task, providing valuable
guidance in the model selection phase.
In this paper, we designed AutoML-GPT, a dual agent sys-
tem built upon large language models. The agents in the
system are capable of communication, planning, and using
tools to complete complex machine learning tasks. In our
experiments, AutoML-GPT demonstrated compatible per-
formance compared to human experts on 11 tabular datasets
chosen from recent Kaggle competitions, reflecting real
modern-day ML applications.
2. Methodology
To integrate a Large Language Model (LLM) into the Au-
toML framework, we propose a systematic methodology
that involves two agents: the Reasoning agent and the Cod-
ing agent, as illustrated in Figure 1. Both agents are imple-
mented using the ReAct (Yao et al., 2022) framework by
langchain1.
The Reasoning agent in the AutoML pipeline handles the
task of understanding human requests and planning the se-
quence of tool usage. It utilizes the language comprehen-
sion capabilities of the LLM to accurately interpret complex
requests and effectively plan the steps required for tasks
like end-to-end training. This agent is responsible for com-
bining different tools optimally, monitoring the pipeline’s
progress, and providing timely updates to the user. On the
other hand, the Coding agent is responsible for implement-
ing the planned tasks. It acquires the necessary knowledge
by reading documentation and modules, leveraging its un-
derstanding of programming languages and AutoML tools.
The Coding agent generates ideas, formulates code, and
executes it in a structured manner to carry out the specified
actions within the AutoML pipeline. It plays a vital role
in translating the reasoning and planning of the Reasoning
agent into executable code.
1https://github.com/hwchase17/langchain
Figure 2. Summary of the performance in rank percentile on
Leaderboard across 9 Kaggle competitions compared to other well
known automl tools.
The interaction between the Reasoning and Coding agents
is iterative and collaborative. The Reasoning agent receives
the execution output from the Coding agent and utilizes it to
provide relevant and informative responses to the user. This
enables the Reasoning agent to communicate the progress
of the AutoML pipeline, respond to user queries, and deliver
meaningful insights based on the executed tasks. By em-
ploying this methodology with the Reasoning and Coding
agents, the integration of LLM into the AutoML framework
benefits from the reasoning and planning capabilities of the
Reasoning agent, as well as the code generation and ex-
ecution expertise of the Coding agent. This collaborative
approach ensures the accurate interpretation of user requests,
precise planning of tool usage, reliable code implementa-
tion, and effective communication with the user, facilitating
a seamless and efficient AutoML pipeline.
3. Experimental Results
For the experiment, we chose to utilize the widely used
Kaggle benchmark, which holds significance in automl re-
search literature. We selected nine tabular datasets from
recent Kaggle competitions to represent contemporary ap-
plications. These competitions encompass a range of tasks,
including regression and (binary/multiclass) classification.
The competition organizers have tailored different metrics
to evaluate the predictive performance based on the specific
problem at hand. Every dataset is processed by AutoML-
GPT in a four instruction sequence though the LLM (1)
Explore the dataset (2) Process the dataset (3) Select the
model (4) Fine tune the parameters. In our experiment, we
will use a single model without employing any ensemble
techniques. The results, depicted in Figure 2, represent
the rank percentile on the Kaggle competition leaderboard.
2

--- PAGE 3 ---
AutoML-GPT: Large Language Model for AutoML
It is important to note that each result is a one-shot sub-
mission to Kaggle without any further fine-tuning after lo-
cal development. In the experiment, we compared with 6
other renowned state-of-the-art automl frameworks: auto-
sklearn, TPOT, Auto-WEKA, H2O AutoML, GCP-tables,
AutoGluon. The experiment shown in Figure 2 limits the
training time of each automl framework to 8 hours max.
4. Discussion
The experiment results in Figure 2 show AutoML-GPT
with competitive performance. The key difference between
AutoML-GPT and other automl framework is that most
automl framework focus on tasks such as hyperparameter
search and model ensemble techniques. The strength of
the performance comes from extensive computation power.
However, since we limit AutoML-GPT to single model re-
sult, the competitive performance comes from the expertise
in machine learning domain knowledge. AutoML-GPT con-
ducts great data exploration and understanding and thus
create well processed datasets for model training. The per-
formance will be further boost if we incorporate other au-
toml frameworks as one of the tools into AutoML-GPT.
5. Conclusion
In this paper, we proposed the AutoML-GPT framework
that uses LLM as machine learning expert to conduct auotml.
We showed its competitive performance by comparing to
other renowned automl frameworks and human competitiors
on Kaggle benchmarks
References
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy,
P., Li, M., and Smola, A. Autogluon-tabular: Robust
and accurate automl for structured data. arXiv preprint
arXiv:2003.06505 , 2020.
Feurer, M., Klein, A., Eggensperger, K., Springenberg, J.,
Blum, M., and Hutter, F. Efficient and robust automated
machine learning. In Advances in Neural Information
Processing Systems 28 (2015) , pp. 2962–2970, 2015.
Jin, H., Chollet, F., Song, Q., and Hu, X. Autokeras: Anautoml library for deep learning. Journal of Machine
Learning Research , 24(6):1–6, 2023. URL http://
jmlr.org/papers/v24/20-1355.html .
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K., and Cao, Y . React: Synergizing reasoning and acting
in language models. arXiv preprint arXiv:2210.03629 ,
2022.
3
