# 2207.06968.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2207.06968.pdf
# File size: 1932314 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DASS: D IFFERENTIABLE ARCHITECTURE SEARCH FORSPARSE
NEURAL NETWORKS
Hamid Mousavi
Mälardalen University
seyedhamidreza.mousavi@mdu.seMohammad Loni
Mälardalen University
mohammad.loni@mdu.se
Mina Alibeigi
mina.alibeigi@zenseact.com
Masoud Daneshtalab
Mälardalen University
masoud.daneshtalab@mdu.se
ABSTRACT
The deployment of Deep Neural Networks (DNNs) on edge devices is hindered by the substantial
gap between performance requirements and available processing power. While recent research has
made significant strides in developing pruning methods to build a sparse network for reducing the
computing overhead of DNNs, there remains considerable accuracy loss, especially at high pruning
ratios. We find that the architectures designed for dense networks by differentiable architecture
search methods are ineffective when pruning mechanisms are applied to them. The main reason is
that the current method does not support sparse architectures in their search space and uses a search
objective that is made for dense networks and does not pay any attention to sparsity.
In this paper, we propose a new method to search for sparsity-friendly neural architectures. We do
this by adding two new sparse operations to the search space and modifying the search objective.
We propose two novel parametric SparseConv andSparseLinear operations in order to expand
the search space to include sparse operations. In particular, these operations make a flexible search
space due to using sparse parametric versions of linear and convolution operations. The proposed
search objective lets us train the architecture based on the sparsity of the search space operations.
Quantitative analyses demonstrate that our search architectures outperform those used in the state-
of-the-art sparse networks on the CIFAR-10 and ImageNet datasets. In terms of performance and
hardware effectiveness, DASS increases the accuracy of the sparse version of MobileNet-v2 from
73.44% to 81.35% (+7.91% improvement) with 3.87 ×faster inference time.
Keywords Neural Architecture Search, Pruning, Network Compression
1 Introduction
Deep Neural Networks (DNNs) provide an excellent avenue for obtaining the maximum feature extraction capacities
required to resolve highly complex computer vision tasks [64, 27, 73, 61]. There is an increasing demand for DNNs
to become more efficient in order to be deployed on extremely resource-constrained edge devices. However, DNNs
are not intrinsically tailored for the limited computing and memory capacities of tiny edge devices, prohibiting their
deployment in such applications [18, 57, 51, 50, 56].
To democratize DNN acceleration, a variety of optimization approaches have been proposed, including network prun-
ing [68, 49, 83, 13], efficient architecture design [57, 50], network quantization [55, 7, 38], knowledge distillation
[31, 20], and low-rank decomposition [36]. Particularly, network pruning is known to provide remarkable computa-arXiv:2207.06968v5  [cs.CV]  12 Sep 2023

--- PAGE 2 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
tional and memory savings by removing redundant weight parameters in the unstructured scenario [68, 2, 49, 83, 24],
and the entire filter in the structured scenario [29, 30, 28, 86, 84]. Recently, unstructured pruning methods reported
to provide extreme network size reductions. The state-of-the-art unstructured pruning methods[68] provide up to 99%
pruning ratio which is an excellent scenario for tiny edge devices.
Nevertheless, these methods suffer from a substantial accuracy drop, hampering them from being applied in practice
(≈19% accuracy drop for MobileNet-v2 compared to dense one [68]). Current pruning methods use handcrafted
architectures designed without concern about sparsity. [2, 49, 83, 86, 68]. We hypothesize that the backbone archi-
tecture may not be optimal for scenarios with extreme pruning ratios. Instead, we can learn more efficient backbone
architectures adaptable to pruning techniques by exploring the space of sparse networks.
Neural Architecture Search (NAS) has achieved great success in the automated designing of high-performance DNN
architectures. Differentiable architecture search (DARTS) methods [52, 75, 76] is a popular NAS method that uses a
gradient-based search algorithm to expedite the search speed. Motivated by the promising results of NAS, we came
up with the idea of designing customized backbone architectures compatible with pruning methods. Nevertheless, the
search space of current DARTS algorithms comprises dense convolution and linear operations that are incapable of
exploring the correct backbone for pruning. To demonstrate this issue, we first prune 99% of the weights from the best
architecture designed by NAS method [52] with base search space without regard for sparsity. Disappointingly, after
applying the pruning method to the final architecture, it performs poorly with up to ≈21% accuracy loss in compression
by DASS that extends the search space by sparse operations. (Section 4). This failure is due to a lack of support for
specific sparse network characteristics leading to low generalization performance. Based on the above hypothesis and
empirical observations, we formulate a search space that includes sparse and dense operations. Therefore, the original
convolution and linear operations in the search space of the NAS have been extended by parametric SparseConv and
SparseLinear operations, respectively. Moreover, to make a consistency between the proposed search space and
search objective function, we modify the bi-level optimization problem to take sparsity into account. In this way, the
search process tries to find the best sparse operation by optimizing both architecture and pruning parameters. This
modification creates a complex bi-level optimization problem. To tackle this difficulty, we split the complex bi-level
optimization into two simple bi-level optimization problems and solve them.
We show explicitly integrating pruning into the search procedure can lead to finding sparse network architectures with
significant accuracy improvement. In Fig. 1, we compare the CIFAR-10 Top-1 accuracy and the number of parameters
of the found architecture by DASS with the state-of-the-art sparse (unstructured pruning) and dense networks. Results
show the designed architecture by DASS outperforms all competing architectures that employ the pruning method.
DASS-Small demonstrates its consistent effectiveness by achieving 15%, 10%, and 8% accuracy improvement over
MobileNet-v2 sparse [67], EfficientNet-v2 sparse [71], and DARTS sparse [52], respectively. In addition, compared to
networks with similar accuracy, DASS-Large has a significant reduction in network complexity (#Params) by 3.5 ×,
30.0×, 105.2 ×over PDO-eConv [69], CCT-6/3 ×1 [65], and MomentumNet [66], respectively. Section 6 provides a
comprehensive experimental study to evaluate different aspects of DASS. Our main contributions are summarized as
follows:
1. We perform extensive experiments to identify the limitations of applying pruning with extreme pruning ratios
to the dense architecture as a post-processing step.
2. We define a new search space by extending the base search space with a new set of parametric operations
(SparseConv andSparseLinear ) to consider the sparse operations in the search space.
3. We modify the bi-level optimization problem to be consistent with the new search space and propose a
three-step gradient-based algorithm to split the complex bi-level problem and learn architecture parameters,
network weights, and pruning parameters.
2 Related Work
2.1 Neural Architecture Search and DARTS Variants
Neural Architecture Search (NAS) has recently attracted remarkable attention by relieving human experts from the
laborious effort of designing neural networks. Early NAS methods mainly utilized evolutionary-based [62, 58, 56, 53]
or reinforcement-learning-based methods [88, 87, 35]. Despite the efficiencies of handcrafted designs, they require
tremendous computing resources. For example, the proposed method in [88] evaluates 20,000 neural candidates
across 500 NVIDIA®P100 GPUs over four days. One-shot architecture search methods [6, 23, 3] have been proposed
to identify optimal neural architectures within a few GPU days ( >1 GPU day [63]). In particular, Differentiable
Architecture Search (DARTS) [52, 75, 76] is a variation of one-shot NAS methods that relaxes the search space to
2

--- PAGE 3 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
Figure 1: Top-1 accuracy (%) vs. number of network parameters (#Params) trained on CIFAR-10 for various sparse
and dense architectures.
be continuous and differentiable. The detailed description of DARTS can be found in Section 3.1. Despite the broad
successes of DARTS in advancing NAS applicability, achieving optimal results remains a challenge for real-world
problems. Many subsequent works investigate some of these challenges by focusing on (i) increasing search speed
[34, 70], (ii) improving generalization performance [12, 74], (iii) addressing the robustness issues [81, 80, 33], (iv)
reducing quantization error [38, 55], and (v) designing hardware-aware architectures [37, 45, 9]. On the other hand,
few works attempt to prune the search space by removing inferior network operations [43, 60, 32, 14]. These works
utilized the pruning mechanism to progressively remove some operations from the search space. Unlike them, our
method aims to extend the search space to improve the performance of the sparse network by searching for the best
operations with sparse weight structures. Technically, our method extends the search space by adding the parametric
sparse version of convolution and linear operations to find the best sparse architecture. Therefore, there is a lack of
research on sparse weight parameters when designing neural architectures. DASS searches for the operations that are
most effective for sparse weight parameters in order to achieve higher generalizing performance.
2.2 Network Pruning
Network pruning is an effective method for reducing the size of DNNs, enabling them to be effectively deployed
on devices with limited resource capacity. Prior works on network pruning can be classified into two categories:
structured and unstructured pruning methods. The purpose of structured pruning is to remove redundant channels
or filters to preserve the entire structure of weight tensors with dimension reduction [29, 30, 48, 28, 86, 21]. While
structured pruning is famous for hardware acceleration, it sacrifices a certain degree of flexibility as well as weight
sparsity [54].
On the other hand, unstructured pruning methods offer superior flexibility and compression rate by removing param-
eters with the least impact on the network accuracy from the weight tensors [25, 47, 29, 54, 17, 68, 2, 49, 83]. In
general, unstructured pruning entails three stages to make a sparse network, including (i) pre-training, (ii) pruning,
and (iii) fine-tuning. Prior unstructured pruning methods used various criteria to select the lowest pruning weight
parameters. [44, 26] pruned weight parameters based on the second-derivative values of the loss function. Several
studies proposed to remove the weight parameters below a fixed pruning threshold, regardless of the training objective
[25, 47, 17, 85, 77, 22]. To address the limitation of fixed thresholding methods, [2, 41] proposed layer-wise trainable
thresholds to determine the optimal value for each layer separately. The lottery-ticket hypothesis [17, 8, 10] is a dif-
3

--- PAGE 4 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
ferent line of the method that identifies the pruning mask for an initialized CNN and trains the resulting sparse model
from scratch without changing the pruning mask. HYDRA [68] formulate the pruning objective as empirical risk
minimization and integrate it with the training objective. Unlike other methods, optimization-based pruning criteria
improve the performance of sparse networks in comparison to other metrics. Despite the success of optimization-
based pruning in achieving a significant compression rate, classification accuracy is compromised, notably when the
pruning ratio is extremely high (up to 99%). We show that the main reason for this issue is due to the non-optimal
backbone architecture. We extend the search space of DASS by parametric sparse operations and formulate pruning as
an empirical risk minimization problem and integrate it into the bi-level optimization problem to find the best sparse
network.
3 Preliminaries
3.1 Differentiable Architecture Search
Differentiable Architecture Search (DARTS) [52] is a NAS method that significantly reduces the search cost by relax-
ing the search space to be continuous and differentiable. DARTS cell template is represented by a Directed Acyclic
Graph (DAG) containing Nintra-nodes. The edge (i, j)between two nodes is associated with an operation o(i,j)(e.g.,
skip connection or 3×3max-pooling) within Osearch space. Eq. 1 computes the output of intermediate nodes.
¯o(i,j)(x(i)) =X
o∈Oexp 
α(i,j)
o
P
o′∈Oexp 
α(i,j)
o′·o(x(i)) (1)
where Oandα(i,j)
o denote the set of all candidate operations and the selection probability of o, respectively. The
output node in the cell is the concatenation of all intermediate nodes. DARTS optimizes architecture parameters ( α)
and network weights ( θ) with the following bi-level objective function:
min
αLval(θ⋆, α)s.t. θ⋆= argmin
θLtrain(θ, α) (2)
where
Ltrain =P
(x,y)∈(Xtrain ,Ytrain )l(θ,x, y)
|Xtrain|
and
Lval=P
(x,y)∈(Xval,Yval)l(θ,x, y)
|Xval|
The operation with the largest αois selected for each edge. Xtrain andYtrain represent the training dataset and
corresponding labels, respectively. Similarly, the validation dataset and labels are indicated by XvalandYval, respec-
tively. After the search process has been completed, the final architecture is re-trained from scratch to obtain maximum
accuracy.
3.2 Unstructured Pruning
Pruning is considered unstructured if it removes low-importance parameters from the weight tensors and makes sparse
ones. [54]. This paper uses the unstructured network pruning method based on optimization criteria to provide higher
flexibility and an extreme compression rate compared to structured pruning methods. The pruning method includes
three main optimization stages: (i) pre-training: training the network on the target dataset, (ii) pruning: pruning
unimportant weights from the pre-trained network, and (iii) fine-tuning: the sparse network is re-trained to recover its
original accuracy. For the pruning stage, we consider an optimization-based method with the following steps: First,
we define the pruning parameters that show the importance of each weight of the network ( s0) and initialize them
according to Eq. 3.
s0
i∝1
max(|θpre,i|)×θpre,i (3)
where θpre,i denotes the weight of ithlayer in the pre-trained network. Next, to learn the pruning parameters (ˆs), we
formulate the optimization problem as Eq. 4, which is then solved by the stochastic gradient descent (SGD) [19].
4

--- PAGE 5 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
ˆs= argmin
sE(x,y)∼D
Lprune (θpre, s, x, y )
(4)
θpreandErefer to the pre-trained network parameters and mathematical expectation, respectively. By solving this
optimization problem, we are able to determine the effect of each weight parameter on the loss function and, conse-
quently, the accuracy of the network. Finally, we convert the floating values of the pruning parameters to a binary
mask based on selecting top- kweights with the highest magnitude of pruning parameters.
4 Research Motivation
The dense network architectures that were originally designed using conventional NAS methods are inaccurate when
integrated with pruning methods, particularly at high pruning ratios. To demonstrate this assertion, we first apply the
unstructured pruning method explained in section 3.2 to the best architecture designed by DARTS [52] for CIFAR-10
and generate a sparse network. We call this solution DARTS sparse . Then, we compare the performance of the sparse
architecture designed by DASS with DARTS sparse . Fig. 2 illustrates the train and test accuracy curves for DASS and
DARTS sparse architectures trained on the CIFAR-10 dataset. Disappointingly, the network designed by DARTS sparse
results in reduced test accuracy. This implies that the dense backbone architectures designed by NAS methods without
considering sparsity are ineffective (DASS delivers 8% higher test accuracy compared to DARTS sparse ). According
to our investigations, we find two issues involved in the training failure of DARTS sparse : (i) DARTS does not support
sparse operations in its search space, and (ii) DARTS optimizes the search objective without considering sparsity into
account. Section 5.2 addresses the first issue, while the second issue is addressed in Section 5.3. We investigate
DASS in two modes to demonstrate the significance of including sparse operations and reformulating the objective
function based on sparsity. The first mode extends the search space with sparse operations solely (DASS Op) and does
not optimize the pruning parameters, while the second mode adds sparsity to the optimization process and optimizes
the architecture and pruning parameters in a bi-level optimization problem. (DASS Op+Ob). Fig. 3 indicates the
test accuracy for DASS Op, DARTS sparse and (DASS Op+Ob) architectures with various pruning ratios. As results
show, DASS Ophas≈3.4% lower accuracy compared to DASS Op+Oband≈4.47% higher accuracy compared to
DARTS sparse . In conclusion, extending the search space with proposed sparse operations (our first contribution)
in DASS produces a better architecture than DARTS sparse , but combining it with the sparsity-based optimization
objective (our second contribution) enhances the performance.
5 DASS method
5.1 DASS: Overview
We propose DASS, a differentiable architecture search method for sparse neural networks. DASS at first extends the
search space of the NAS with parametric sparse operations. Then it modifies the bi-level optimization problem to learn
the architecture, weights, and pruning parameters. DASS employs a three-step approach to solve the complicated bi-
level optimization problem, which consists of (1) Pre-training : Find the best dense architecture (pruning parameters
equal to zero) from the search space and pre-train it (2) Pruning and sparse Architecture Design : Find the best pruning
0 50 100 150 200 250 300 350 400 450 500 550 60030405060708090100
EpochTrain Accuracy
DARTS pruned
PR-DARTS
0 20 40 60 80 100 120 140 160 180 20030405060708090100
EpochTest Accuracy
DARTS pruned
PR-DARTS
(a) Train (b) Test
Figure 2: Comparison of DASS-Small and DARTS sparse on CIFAR-10 for (a) train and (b) test learning curves.
5

--- PAGE 6 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
mask (optimizing pruning parameters) and update the architecture parameters based on the sparse weights and finally
(3)Fine-tuning : we re-train the sparse architecture to achieve the maximum classification performance.
5.2 DASS Search Space
To support sparse operations, DASS proposes the parametric sparse version of convolution and linear operations called
SparseConv andSparseLinear , respectively. These operations have a sparsity mask ( m) to remove redundant
weight parameters from the network. Fig. 4 illustrates the functionality of these two operations. In addition, table 1
summarizes the operations of the DASS search space. To empirically investigate the efficiency of the proposed
Table 1: Operations of the DASS search space.
Operation Separable Dilated Max Average Skip
Type Sparse Convolution sparse Convolution Pooling pooling connect
Kernel Size 3×3,5×5 3 ×3,5×5 3 ×3 3 ×3 N/A
sparse search space, we compare the similarity of the feature maps of high-performance dense architecture (with a
large number of parameters) with the sparse architecture discovered by DASS and the architecture designed from the
original search space DARTS spasre methods. We use Kendall’s τ[1] metric to measure the similarity between output
feature maps. The τcorrelation coefficient returns a value between -1 and 1. To present the outcome more clearly,
we scale up these values between -100 and 100. Closer values to 100 indicate stronger positive similarity between
the feature maps. Fig. 5 summarizes the results. Our observations reveal a similarity between DASS feature maps
and dense architecture (up to 16%). On the other hand, the correlation between DARTS sparse and dense architecture
is insignificant. Therefore, it shows that the architecture designed by DASS based on new search space can extract
features more similar to high-performance dense architecture while DARTS sparse that use dense search space lost
important features after pruning. The level of similarity is not very high because DASS is a sparse network with a
pruning ratio of 99%. However, it can demonstrate that DASS retrieves useful features.
5.3 DASS Search objective
DASS aims to search for the optimal architecture parameters ( α⋆) to minimize the validation loss of the sparse network
weight parameters. Thus, to consistent search objective with proposed sparse search space. we formulate the entire
search objective as a complex bi-level optimization problem:
α⋆= min
α(Lval(ˆθ(α), α))
s.t.

θ⋆(α) = argminθLtrain(θ, α)
ˆm= argminm∈{0,1}N
Lprune (θ⋆(α)⊙m, α)
ˆθ(α) =θ⋆(α)⊙ˆm.(5)
90 95 9980859095100
Pruning Ratio (%)Test Accuracy (%)DASS 𝑂𝑝+𝑂𝑏
DASS 𝑂𝑝
DARTS 𝑠𝑝𝑎𝑟𝑠𝑒
Figure 3: DASS Op+Obvs. DARTS sparse and DASS with only adding sparse operations to the search space
(DASS Op).
6

--- PAGE 7 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
Input ( )
OutputMask ( )
OutputInput ( )
(a)Linear operation PrunedLinear operationInput ( )
OutputMask ( )
OutputInput ( )
(b)Convolution operation PrunedConv operation
Figure 4: Illustrating the (a) SparseLinear and (b) SparseConv operations.
Where mdenotes the binary pruning mask parameters. This formulation learns the architecture parameters based
on the sparse weight parameters. However, Eq. 5 is not a straightforward bi-level optimization problem because the
lower-level problem consists of two optimization problems. To overcome this challenge, We break the search objective
down into three distinct steps. Thus, the problem is transformed into two bi-level optimization problems to determine
the optimal architecture parameters for dense and sparse weights and an optimization problem to fine-tune the weight
parameters. In addition, the lower-level optimization problem consists of a discrete optimization problem for pruning
masks.
Section 5.4 proposes a multi-step optimization algorithm to solve the optimization problem and handle the discrete
optimization problem by converting it to a continuous optimization problem.
5.4 Optimization Algorithm
Step 1: pre-train (learn θ⋆
preandα∗
pre)
In this step, we break the Eq. 5 into a bi-level optimization problem to find the best dense architecture. This pre-
training is necessary for the next step which learn pruning mask parameters and modifying the sparse architecture.
α⋆
pre= min
αpre(Lval(θ∗
pre(αpre), αpre))
s.t. θ⋆
pre(αpre) = argmin
θpreLtrain(θpre, αpre)(6)
The first-order approximation technique use to update θ⋆
preandαprealternately using gradient descent [52].
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19−1001020
−5515
Cell NumberKendall’s τSimilarityPR-DARTS DARTS pruned
Figure 5: Comparing the Kendall’s τsimilarity metric of architectures designed by both DARTS sparse and DASS
methods with high-performance dense architecture .
7

--- PAGE 8 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
Architecture   
ModelStep1: pre-train
Architecture  
ModelStep2: prune
Final ModelStep3: fine-tune
DASS Optimization
Figure 6: The overview of the proposed optimization algorithm to find architecture parameters based on the sparse
weight parameters. It consists of three main steps: 1) pre-training: search dense architecture 2) pruning: search sparse
architecture 3) fine-tuning: re-train best sparse architecture.
Step 2: prune (learn ˆmandα∗
prune )
To make the search process aware of the sparsity mechanism, we need to solve another bi-level optimization problem
that alternately updates the pruning mask and architecture parameters. Pruning mask parameters are binary values.
Therefore, learning the mask parameters ( m) is a challenging binary optimization problem. We solve this binary
optimization problem by introducing floating point pruning parameters sand initializing them. Then we use SGD to
solve the optimization problem and find the best floating-point pruning mask parameters. Finally, Based on the values
of pruning parameters, we select the top- kweight parameters with the highest values and assign one value to them.
This step aims to jointly learn architecture parameters αprune and mask parameters ˆmto consider sparsity in learning
architecture parameters. Therefore, we use another bi-level optimization problem:
α⋆
prune = min
αprune(Lval(θ∗
pre⊙ˆm(αprune ), αprune ))
s.t. ˆs(αprune ) = argmin
sLprune (θ∗
pre, αprune , s),
ˆm(αprune ) =1(|ˆs(αprune )|>|ˆs(αprune )|k)(7)
similar to step 1, the first-order approximation method is used to alternately update ˆmandαprune by gradient descent.
Step 3: fine-tune (learn ˆθ)
In the fine-tuning step, we update the non-zero weight parameters using SGD for the best sparse architecture to improve
the network accuracy (Eq. 8).
ˆθt+1=ˆθt−ηˆθ∇ˆθLfine-tune(ˆθt⊙ˆm, α⋆
prune ) (8)
where ηˆθandLfine-tune denote the learning rate and the loss function for the fine-tuning step.
We show that the proposed three-step optimization algorithm can solve the complex bilevel problem in Eq. 5 and finds
optimal architecture parameters with higher generalization performance for sparse networks. Fig. 7 compares the
learning curves of DASS with DARTS sparse on the CIFAR-10 dataset. As shown, the DASS optimization algorithm
significantly reduces the validation loss for the sparse network. Fig. 8 compares the behavior of the generalization gap
(train minus test accuracy) for DASS and DARTS sparse . DASS has a lower generalization gap (up to 22%), indicating
DASS better regularizes the validation loss across all epochs compared to DARTS sparse . Algorithm 1 outlines our
DASS for the differentiable neural architecture search for sparse neural networks.
8

--- PAGE 9 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
Algorithm 1 Search Process of the DASS
Require: Dataset D, loss objectives: Ltrain ,Lprune , and Lfine−tune, training iteration T
Ensure: fine-tuned spasre model
Step1: Pre-train
1:fori←1toTdo
2: keepαt
prefixed, and obtain θt+1
preby gradient descent with ∇θpreLtrain(θt
pre, αt
pre)
3: keepθt+1
prefixed, and obtain αt+1
preby gradient descent with ∇αpreLval(θt+1
pre, αt
pre)
Step2: Prune
4:fori←1toTdo
5: keepαt
prune fixed, and obtain st+1by gradient descent with ∇sLprune (st, αt
prune )
6: Compute mt+1= (|st+1|>|st+1|k)
7: keepmt+1fixed, and obtain αt+1
prune by gradient descent with ∇αpruneLval(θ∗
pre⊙mt+1, αt
prune )
Step3: fine-tune
8:fori←1toTdo
9: keepα∗
prune andˆmfixed and obtain ˆθt+1by gradient descent with ∇ˆθLfine−tune(ˆθt⊙ˆm, α∗
prune )
10:return Fine-tuned sparse model
6 Experiments
6.1 Experimental Setup
1) DATASET : To evaluate DASS, we use CIFAR-10 [39] and ImageNet [40] public classification datasets. For the
search process, we split the CIFAR-10 dataset into 30k data points for training and 30k for validation. We transfer the
best-learned cells on CIFAR-10 to ImageNet [52] and re-train the final sparse network from scratch.
2) Details on Searching Networks : We create a network with 16 initial channels and eight cells. Each cell consists
of seven nodes equipped with a depth-wise concatenation operation as the output node. The SparseConv operations
follow the ReLU+ SpasreConv +Batch Normalization order. We train the network using SGD for 50 epochs with a
batch size of 64 in the DASS pre-train step. Then, we update the value of pruning and architecture parameters for 20
epochs in the DASS pruning step. Finally, we fine-tune the network for 200 epochs. The initial learning rate for the
DASS in pre-train, pruning, and fine-tuning steps is 0.025, 0.1, and 0.01, respectively. In our experiments, we use
the cosine annealing learning rate [59]. We use weight decay=3 ×10-4and momentum=0.9 in all steps. The search
process takes ≈3 GPU-days on a single NVIDIA®RTX A4000 that produces 4.35 Kg CO2. We compare the sparse
architecture design by our method, DASS, with other dense and sparse networks. NAS-Bench-101 [78] and NAS-
Bench-201 [16] are examples of NAS algorithm evaluation benchmarks. They consist of numerous dense designs and
their respective performance. Due to the fact that they do not support sparse architectures, we cannot evaluate DASS
using these benchmarks. Creating sparse benchmarks for evaluating NAS algorithms is a suggestion for future work.
0 50 100 150 200012
0.30.61.5
EpochValid. LossDARTS pruned
PR-DARTS
Figure 7: Comparing learning curves (validation loss)
of DASS and DARTS sparse on the searched architec-
tures trained with the CIFAR-10 dataset.
0 50 100 150 200020406080
EpochGap (%)DARTS pruned
PR-DARTSFigure 8: Comparing the generalization gap of DASS
and DARTS sparse over the CIFAR-10 dataset. The
lower values for the generalization gap are better.
9

--- PAGE 10 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
3) DASS variants and Hardware Configuration : Table 2 provides the configuration details of the DASS variants. Each
variation is built by stacking a different number of DASS cells and the output channels of the first layer to generate
networks for various resource budgets. Table 3 presents specifications of hardware devices utilized for evaluating the
performance of DASS at inference time.
Table 2: Configuration of the DASS variants. #Cells: the number of stacked cells. #Channels: the number of output
channels for the first SparseConv operation.
DASSCIFAR-10 ImageNet
Tiny Small Medium Large Small Medium Large
#Cells 16 20 12 14 14 15 16
#Channels 30 36 86 108 48 86 128
Table 3: Hardware Specification.
Platform Specification Value
Search & TrainGPU NVIDIA®RTX A4000 (735 MHz)
GPU Memory 16 GB GDDR6
GPU Compiler cuDNN version 11.1
System Memory 64 GB
Operating System Ubuntu 18.04
CO2Emission/Day†1.45 Kg
Real HardwareEmbedded GPUNVIDIA®Jetson TX2 (735 MHz)
256 CUDA Cores
NVIDIA®Quadro M1200 (735 MHz)
640 CUDA Cores
Embedded CPUARM CortexTM-A7 (1.2 GHz)
4/4 (Cores/Total Thread)
Intel®i5-3210M Mobile CPU
5/4 (Cores/Total Thread)
Estimation‡ Xiaomi Mi9 GPUAdreno 640 GPU (750 MHz)
986 GFLOPs FP32 (Single Precision)
Myriad VPUIntel Movidius NCS2 (700 MHz)
28-nm Co-processor
†Calculated using the ML CO2impact framework: https://mlco2.github.io/impact/ [42]
‡Performance Estimation using the nn-Meter framework [82].
6.2 DASS Compared to dense Networks
Table 4 compares the performance of DASS against the state-of-the-art and the state-of-the-practice DNNs. We select
the architecture with the highest accuracy, DrNAS [12], as the baseline for comparing compression rates. In compari-
son with DrNAS [12], DASS-Large provides 37.73 ×and 29.23 ×higher network compression rates while delivering a
comparable accuracy (less than 2.5% accuracy loss) on the CIFAR-10 and ImageNet datasets, respectively. Compared
to the best handcrafted designed network [65] on the CIFAR-10 (CCT-6/3x1), DASS-Large significantly decreases the
parameters of the network by 29.9 ×with providing slightly higher accuracy.
6.3 DASS Compared to sparse Networks
As we focus on improving the accuracy of sparse networks at extremely high pruning ratios, we compare DASS
with other sparse networks with the unstructured pruning method at 99% pruning ratio (Table 5). In comparison
with DARTS sparse , DASS-Small yields 7.81% and 7.81% higher top-1 accuracies with 1.23 ×and 1.05 ×reduction
in network size on the CIFAR-10 and ImageNet datasets, respectively. It indicates that the network design based on
new search space and sparse objective function finds better sparse architecture. In comparison with ResNet-18 sparse
10

--- PAGE 11 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
Table 4: Comparing the DASS method with the state-of-the-art dense networks on the CIFAR-10 and ImageNet
datasets.
Architecture YearCIFAR-10 ImageNet
Search Top-1 #Params #Params Top-1 Top-5 #Params #Params
Method Acc.(%) ( ×106) Compression Acc.(%) Acc.(%) ( ×106) Compression
ResNet-18‡[27] 2016 - 91.0 11.1 -2.77 × 72.33 91.80 11.7 -2.05 ×
PDO-eConv [69] 2020 - 94.62 0.37 +10.81 × - - - -
FlexTCN-7 [65] 2021 - 92.2 0.67 +5.97 × - - - -
CCT-6/3x1 [65] 2021 - 95.29 3.17 +1.26 × - - - -
MomentumNet [66] 2021 - 95.18 11.1 -2.77 × - - - -
DARTS (1storder) [52] 2018 gradient 96.86 3.3 +1.21 × - - - -
DARTS (2ndorder) [52] 2018 gradient 97.24 3.3 +1.21 × 74.3 91.3 4.7 +1.21 ×
SGAS (Cri 1. avg) [46] 2020 gradient 97.34 3.7 +1.08 × 75.9 92.7 5.4 +1.05 ×
SDARTS-RS [11] 2020 gradient 97.39 3.4 +1.17 × 75.8 92.8 3.4 +1.67 ×
DrNAS [12] 2020 gradient 97.46 4.0 1.0 × 76.3 92.9 5.7 1.0 ×
DASS-Small 2022 gradient 89.06 0.017 +235.29 × 46.48 68.36 0.029 +196.55 ×
DASS-Medium 2022 gradient 92.18 0.054 +74.07 × 68.34 82.24 0.082 +69.51 ×
DASS-Large 2022 gradient 95.31 0.106 +37.73 × 73.83 85.94 0.195 +29.23 ×
†The baseline for comparing the #params compressing rate is DrNAS [12] as the most accurate architecture.
‡ResNet-18 results are trained in https://github.com/facebook/fb.resnet.torchTorch (July 10, 2018).
on the CIFAR-10 dataset, we provide 1.56% and 4.7% higher accuracy with 2.08 ×and 1.05 ×network size reduction
for DASS-Medium and DASS-Large, respectively. Compared to ResNet-18 sparse on the ImageNet dataset, DASS-
Medium provides 0.76% higher accuracy with 1.42 ×network size reduction. MCUNET [50] is a lightweight neural
network for microcontrollers. It is designed by a tiny neural architecture search mechanism. Compared to MCUNET
on the ImageNet dataset, DASS-Large provides 1% higher accuracy with 2.89 ×network size reduction. This result
shows that only optimizing the size of the filters without considering the sparsity can not generate the best architecture.
DASS directly search for the best operations in sparse version to design high-performance lightweight network. We
can conclude that DASS increases sparse networks’ accuracy at high pruning ratios compared to NAS-based and
handcrafted networks.
Table 5: Comparing the DASS method with sparse networks on the CIFAR-10 and ImageNet datasets.
ArchitectureCIFAR-10 ImageNet
Top-1 #Params CompressionNID‡ Top-1 Top-5 #Params CompressionNID‡
Acc. (%) ( ×103) Rate†Acc. (%) Acc. (%) ( ×103) Rate†
DARTS sparse [52] 81.25 21.0 100.47 × 3.86 38.67 61.33 33.0 100 × 1.11
MobileNet-v2 sparse [67] 73.44 22.2 95.04 × 3.30 17.97 36.72 34.87 94.63 × 0.515
ResNet-18 sparse [27] 90.62 111.6 18.90 × 0.81 67.58 80.86 116.84 28.24 × 0.578
EfficientNet sparse [71] 79.69 202.3 10.43 × 0.39 - - - - -
MCUNET [50] 89.7 210.1 15.70 0.42 72.34 84.86 562.64 5.86 × 0.128
DASS-Small 89.06 17.0 124.11 × 5.23 46.48 68.36 28.94 114.02 × 1.606
DASS-Medium 92.18 53.65 39.32 × 1.71 68.34 82.24 81.95 40.26 × 0.841
DASS-Large 95.31 105.5 20 × 0.90 73.83 85.94 194.6 16.95 × 0.38
†The baseline for comparing the compressing rate is full-precision and dense DARTS architecture.
‡NID = Accuracy/#Parameters [4]. NID measures how efficiently each network uses its parameters.
6.4 Evaluation of DASS with Various Pruning Ratios
Table 6 compares DASS and the DARTS sparse method with three different pruning ratios including 90%, 95%, and
99% on the CIFAR-10 dataset. DASS achieves 1.57%, 1.04%, and 7.8% higher accuracies with 7%, 6.9%, and 23%
network size reduction compared to the DARTS sparse at 90%, 95%, and 99% pruning ratios, respectively. Thus,
DASS is significantly more effective at extremely higher pruning ratios (99%) than lower pruning ratios (90%).
6.5 DASS Compared to Other Pruning Methods
Table 7 compares DASS with state-of-the-art pruning algorithms. The results indicate that DASS outperforms other
pruning algorithms with different backbone architectures on CIFAR-10 and ImageNet datasets. On CIFAR-10, DASS-
Large shows a 1.6% higher accuracy and 3.8 ×reduction in the network size compared to the most accurate results
provided by TAS Pruning [15]. DASS-Large also provides 4.68% accuracy improvement with 38.14 ×reduction in
the network size over TAS Pruning [15] on ImageNet. In light of DASS’ higher efficiency compared to other pruning
11

--- PAGE 12 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
Table 6: Evaluating the effectiveness of DASS at various pruning ratios.
Architecture90% 95% 99%
Accuracy #Params Accuracy #Params Accuracy #Params
(×103) ( ×103) ( ×103)
DARTS sparse 95.31% 421 93.75% 210.5 81.25% 21.0
DASS-Small 96.88% 391 94.79% 196.75 89.06% 17.0
methods, we can conclude that the pruning method was not the only reason for the DASS’s effectiveness and it is
independent of the pruning algorithm.
Table 7: Comparing DASS with other pruning algorithms.
Pruning MethodCIFAR-10 ImageNet
Backbone Top-1 #Params Backbone Top-1 Top-5 #Params
Arch. Acc.(%) ( ×106) Arch. Acc.(%) Acc.(%) ( ×106)
SFP [29]
ResNet-2092.08 0.269
ResNet-1867.10 87.78 6.46
FPGM [30] 92.31 0.269 68.41 88.48 6.46
TAS Pruning [15] 93.16 0.232 69.15 88.48 7.40
DASS-Small - 89.06 0.017 - 46.48 68.36 0.029
DASS-Medium - 92.18 0.054 - 68.34 82.24 0.082
DASS-Large - 95.31 0.106 - 73.83 85.94 0.194
6.6 DASS Compared to Quantized Networks
Network quantization emerged as a promising research direction to reduce the computation of neural networks. Re-
cently, [38, 7, 55] proposed to integrate the quantization mechanism into the differentiable NAS procedure to improve
the performance of quantized networks. Table 8 compares DASS with the best results of NAS-based quantized net-
works. The compression rate is calculated asPL
l=1#Wl×32PL
l=1#Wt
l×qwhere #Wland#Wt
lare the number of weights in layer
lfor full-precision (32-bit) and quantized network with q-bit resolution [55]. DASS-Medium yields 0.24% and 3.24%
higher accuracies and significantly higher compression rate by 2.7 ×and 4.24 ×compared to TAS [55] as the most
accurate quantized network on the CIFAR-10 and ImageNet datasets, respectively.
Table 8: Comparing the DASS method with quantized networks on CIFAR-10.
ArchitectureCIFAR-10 ImageNet
#bits Top-1 #Params Compression Top-1 Top-5 #Params Compression
(W/A)‡Acc.(%) ( ×106) Rate†Acc.(%) Acc.(%) ( ×106) Rate†
Binary NAS (A) [38] 1/1 90.66 2.4 44.0 × 57.69 79.89 5.57 32.74 ×
TAS [55] 2/2 91.94 2.4 22.0 × 65.1 86.3 5.57 16.37 ×
DASS-Small 32/32 89.06 0.017 194.11 × 46.48 68.36 0.029 196.55 ×
DASS-Medium 32/32 92.18 0.054 61.11 × 68.34 82.24 0.082 69.51 ×
DASS-Large 32/32 95.31 0.106 31.13 × 73.83 85.94 0.194 29.38 ×
†The baseline for comparison is full-precision DARTS with 3.3M and 5.7M parameters for CIFAR-10 and ImageNet.
‡(Weights/Activation Function).
6.7 Hardware Performance Results of DASS
We extensively study the effectiveness of DASS in the context of hardware efficiency by computing the inference
time (latency) of various state-of-the-art sparse networks for a wide range of resource-constrained edge devices on the
CIFAR-10 dataset (Fig. 9). The batch size is equal to 1 for all experiments. It is worth noting that we did not utilize
any simplification techniques, such as [5], to compact the sparse filters by fusing weight parameters. Our results reveal
that the Pareto-frontier of DASS consistently outperforms all other counterparts by a significant margin, especially on
CPUs that have very limited parallelism. DASS-Tiny as the fastest network improves the accuracy from MobileNet-
v2’s 73.44% to 81.35% (+7.91% improvement) and accelerates the inference by up to 3.87 ×. More importantly,
12

--- PAGE 13 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
PR-DARTS DARTS pruned EFFICIENT NET-V2pruned RESNET-18 pruned MOBILE NET-V2pruned
012345678910111213707580859095100
TinySmallMediumLarge
Latency (s)Top-1 Accuracy (%)ARM®CortexTM-A7
0200 600 1,000 1,400 1,800 2,200707580859095100
TinySmallMediumLarge
Latency (ms)Top-1 Accuracy (%)Intel®i5-3210M (PyTorch CPU)
68101214161820222426283032343638404244707580859095100
TinySmallMediumLarge
Latency (ms)Top-1 Accuracy (%)Intel®Movidius NCS2 (OpenVINO)
051015202530354045505560707580859095100
TinySmallMediumLarge
Latency (ms)Top-1 Accuracy (%)Xiaomi Mi9 GPU (TFLite)
0 50 150 250 350 450707580859095100
TinySmallMediumLarge
Latency (ms)Top-1 Accuracy (%)Quadro M1200 (PyTorch GPU)
50100150200250300350400450500550600650700750800707580859095100
TinySmallMediumLarge
Latency (ms)Top-1 Accuracy (%)Tegra TX2 (PyTorch GPU)
Figure 9: Trade-off: accuracy v.s. measured latency. DASS-Tiny, DASS-Small, DASS-Medium, DASS-Large are
variants of DASS designed for different computational budgets (Table 2). DASS-Tiny consistently achieves higher
accuracy with similar latency than MobileNet-v2 sparse and provides lower latency while achieving better accuracy as
DARTS sparse .
Labels: 0123456789
0.0 0.2 0.4 0.6 0.8 1.0
tSNE 10.00.20.40.60.81.0tSNE 20 1 2 3 4 5 6 7 8 9
0.0 0.2 0.4 0.6 0.8 1.0
tSNE 10.00.20.40.60.81.0tSNE 20 1 2 3 4 5 6 7 8 9
0.0 0.2 0.4 0.6 0.8 1.0
tSNE 10.00.20.40.60.81.0tSNE 20 1 2 3 4 5 6 7 8 9
(a) DARTS (b) DARTS pruned (c) DASS-Small
Figure 10: Visualize decision boundary of (a) DARTS. (b) DARTS sparse . (c) DASS-Large with t-SNE embedding
method.
DASS-Tiny runs much faster than DARTS sparse by 1.67-4.74 ×with slightly better accuracy. Compared to ResNet-
18sparse as the closest network to DASS in terms of accuracy, DASS-Medium provides 1.46% accuracy improvement
and up to 1.94 ×acceleration on hardware.
6.8 Analyzing the Discrimination Power of DASS
We use the t-distributed stochastic neighbor embedding (t-SNE) method [72] for visualizing decision boundaries
of dense high-performance architecture designed by DARTS, DARTS sparse (sparse dense DART architecture with
pruning), and DASS ( our sparse architecture) on the CIFAR-10 dataset. Fig. 10 illustrates the decision boundaries of
classification for each network. According to the results, DASS has a higher discrimination power than DARTS sparse ,
and DASS with a 99% pruning ratio behaves very similarly to the dense and high-performance DARTS architecture.
13

--- PAGE 14 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
c_{k-2} 0sep_conv_3x3
1skip_connect2skip_connect
3skip_connect
c_{k-1}sep_conv_3x3
skip_connectskip_connectskip_connect
c_{k}
c_{k-2}
0avg_pool_3x31avg_pool_3x3
2avg_pool_3x3
3avg_pool_3x3
c_{k-1}dil_conv_3x3skip_connect
skip_connect
skip_connectc_{k}
(a) DARTS Normal Cell. (b) DARTS Reduction Cell.
c_{k-2} 0avg_pool_3x3c_{k-1}
skip_connect
1dil_conv_5x5 2skip_connect
skip_connectavg_pool_3x3
c_{k}3 skip_connectskip_connect
c_{k-2}
0sep_conv_3x32skip_connect
c_{k-1}max_pool_3x3
1max_pool_3x33max_pool_3x3
dil_conv_3x3dil_conv_3x3 c_{k}skip_connect
(a) DASS Normal Cell. (b) DASS Reduction Cell.
Figure 11: The illustration of (a) normal cell and (b) reduction cell.
6.9 Qualitative Analysis of the Searched Cell.
Fig. 11 shows the best cells searched by DASS-Small. An interesting finding is that, for the normal cell, DASS-Small
tends to select SparseConv operation with larger kernel sizes ( 5×5), providing more pruning candidates to optimize
the pruning mask. DASS-Small tends to leverage max-pooling operations in the reduction cell instead of avg-pooling
operations. This is because the max-pooling operation has a higher feature extraction capability with sparse filters
[79].
6.10 Reproducibility Analysis.
To verify the reproducibility of results, the DASS-Small search
procedure was run five times with different random seeds.
Fig. 6.10 plots the average of accuracy and loss variations as
well as the shades to indicate the confidence intervals. Re-
sults show that, while the confidence interval is wide at first,
the average of multiple runs converges to neural architectures
with similar performance with an average standard deviation
(STDEV) of 2.22%.
0 10 20 30 40 500.50.60.70.80.90.88
EpochValidation AccuracyAccuracy
0.511.5
0.3
Validation LossLoss
Figure 12: Demonstrating the reproducibility of DASS re-
sults.
7 Conclusion
We propose DASS, a differentiable architecture search method, to design high-performance sparse architectures for
DNNs. DASS significantly improves the performance of sparse architectures by proposing: (i) a new search space
that contains sparse parametric operations; and (ii) a new search objective that is consistent with sparsity and pruning
mechanisms. Our experimental results reveal that the learned sparse architectures outperform the architectures used in
the state-of-the-art on both CIFAR-10 and ImageNet datasets. In the long term, we foresee that our designed networks
can effectively contribute to the goal of green artificial intelligence by efficiently utilizing resource-constrained devices
as the edge accelerating solutions. A promising avenue for future work is to design a sparse network that is also robust
against adversarial attacks.
14

--- PAGE 15 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
References
[1] Hervé Abdi. 2007. The Kendall rank correlation coefficient. Encyclopedia of Measurement and Statistics. Sage,
Thousand Oaks, CA (2007), 508–510.
[2] Kambiz Azarian, Yash Bhalgat, Jinwon Lee, and Tijmen Blankevoort. 2020. Learned threshold pruning. arXiv
preprint arXiv:2003.00075 (2020).
[3] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. 2018. Understanding and
simplifying one-shot architecture search. In International Conference on Machine Learning . PMLR, 550–559.
[4] Simone Bianco, Remi Cadene, Luigi Celona, and Paolo Napoletano. 2018. Benchmark analysis of representative
deep neural network architectures. IEEE Access 6 (2018), 64270–64277.
[5] Andrea Bragagnolo and Carlo Alberto Barbano. 2022. Simplify: A Python library for optimizing pruned neural
networks. SoftwareX 17 (2022), 100907.
[6] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. 2017. Smash: one-shot model architecture
search through hypernetworks. arXiv preprint arXiv:1708.05344 (2017).
[7] Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. 2020. Bats: Binary architecture search. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIII 16 .
Springer, 309–325.
[8] Rebekka Burkholz, Nilanjana Laha, Rajarshi Mukherjee, and Alkis Gotovos. 2021. On the existence of universal
lottery tickets. arXiv preprint arXiv:2111.11146 (2021).
[9] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. 2019. Once-for-all: Train one network and
specialize it for efficient deployment. arXiv preprint arXiv:1908.09791 (2019).
[10] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Yang Zhang, Shiyu Chang, and Zhangyang Wang. 2022. Data-
Efficient Double-Win Lottery Tickets from Robust Pre-training. In International Conference on Machine Learn-
ing. PMLR, 3747–3759.
[11] Xiangning Chen and Cho-Jui Hsieh. 2020. Stabilizing differentiable architecture search via perturbation-based
regularization. In International Conference on Machine Learning . PMLR, 1554–1565.
[12] Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. 2020. Drnas: Dirichlet
neural architecture search. arXiv preprint arXiv:2006.10355 (2020).
[13] Enmao Diao, Ganghua Wang, Jiawei Zhan, Yuhong Yang, Jie Ding, and Vahid Tarokh. 2023. Pruning Deep
Neural Networks from a Sparsity Perspective. arXiv preprint arXiv:2302.05601 (2023).
[14] Yadong Ding, Yu Wu, Chengyue Huang, Siliang Tang, Fei Wu, Yi Yang, Wenwu Zhu, and Yueting Zhuang.
2022. NAP: Neural Architecture search with Pruning. Neurocomputing (2022).
[15] Xuanyi Dong and Yi Yang. 2019. Network pruning via transformable architecture search. Advances in Neural
Information Processing Systems 32 (2019).
[16] Xuanyi Dong and Yi Yang. 2020. Nas-bench-201: Extending the scope of reproducible neural architecture
search. arXiv preprint arXiv:2001.00326 (2020).
[17] Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 (2018).
[18] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. 2021. AI and Memory Wall.
RiseLab Medium Post (2021).
[19] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT Press. http://www.
deeplearningbook.org .
[20] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey.
International Journal of Computer Vision 129, 6 (2021), 1789–1819.
[21] Yushuo Guan, Ning Liu, Pengyu Zhao, Zhengping Che, Kaigui Bian, Yanzhi Wang, and Jian Tang. 2022. Dais:
Automatic channel pruning via differentiable annealing indicator search. IEEE Transactions on Neural Networks
and Learning Systems (2022).
[22] Shupeng Gui, Haotao N Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, and Ji Liu. 2019. Model compres-
sion with adversarial robustness: A unified optimization framework. Advances in Neural Information Processing
Systems 32 (2019), 1285–1296.
[23] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. 2020. Single
path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision .
Springer, 544–560.
15

--- PAGE 16 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
[24] Marwa El Halabi, Suraj Srinivas, and Simon Lacoste-Julien. 2022. Data-efficient structured pruning via submod-
ular optimization. arXiv preprint arXiv:2203.04940 (2022).
[25] Song Han, Jeff Pool, John Tran, and William J Dally. 2015. Learning both weights and connections for efficient
neural networks. arXiv preprint arXiv:1506.02626 (2015).
[26] Babak Hassibi and David G Stork. 1993. Second order derivatives for network pruning: Optimal brain surgeon .
Morgan Kaufmann.
[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition.
InProceedings of the IEEE conference on computer vision and pattern recognition . 770–778.
[28] Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, and Yi Yang. 2020. Learning filter pruning
criteria for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition . 2009–2018.
[29] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. 2018. Soft filter pruning for accelerating deep
convolutional neural networks. arXiv preprint arXiv:1808.06866 (2018).
[30] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. 2019. Filter pruning via geometric median for deep
convolutional neural networks acceleration. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition . 4340–4349.
[31] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 2, 7 (2015).
[32] Weijun Hong, Guilin Li, Weinan Zhang, Ruiming Tang, Yunhe Wang, Zhenguo Li, and Yong Yu. 2021. Dropnas:
Grouped operation dropout for differentiable architecture search. In Proceedings of the Twenty-Ninth Interna-
tional Conference on International Joint Conferences on Artificial Intelligence . 2326–2332.
[33] Ramtin Hosseini, Xingyi Yang, and Pengtao Xie. 2021. DSRNA: Differentiable Search of Robust Neural Archi-
tectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 6196–6205.
[34] Andrew Hundt, Varun Jain, and Gregory D Hager. 2019. sharpdarts: Faster and more accurate differentiable
architecture search. arXiv preprint arXiv:1903.09900 (2019).
[35] Yesmina Jaafra, Jean Luc Laurent, Aline Deruyver, and Mohamed Saber Naceur. 2019. Reinforcement learning
for neural architecture search: A review. Image and Vision Computing 89 (2019), 57–66.
[36] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. 2014. Speeding up convolutional neural networks with
low rank expansions. arXiv preprint arXiv:1405.3866 (2014).
[37] Xiaojie Jin, Jiang Wang, Joshua Slocum, Ming-Hsuan Yang, Shengyang Dai, Shuicheng Yan, and Jiashi Feng.
2019. Rc-darts: Resource constrained differentiable architecture search. arXiv preprint arXiv:1912.12814
(2019).
[38] Dahyun Kim, Kunal Pratap Singh, and Jonghyun Choi. 2020. Learning architectures for binary networks. In
European Conference on Computer Vision . Springer, 575–591.
[39] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2009. Cifar-10 and cifar-100 datasets. URl: https://www. cs.
toronto. edu/kriz/cifar. html 6, 1 (2009), 1.
[40] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional
neural networks. Advances in neural information processing systems 25 (2012), 1097–1105.
[41] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali
Farhadi. 2020. Soft Threshold Weight Reparameterization for Learnable Sparsity. In Proceedings of the Interna-
tional Conference on Machine Learning .
[42] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon
emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).
[43] Kevin Alexander Laube and Andreas Zell. 2019. Prune and replace nas. In 2019 18th IEEE International Con-
ference On Machine Learning And Applications (ICMLA) . IEEE, 915–921.
[44] Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal brain damage. In Advances in neural information
processing systems . 598–605.
[45] Hayeon Lee, Sewoong Lee, Song Chong, and Sung Ju Hwang. 2021. HELP: Hardware-Adaptive Efficient
Latency Predictor for NAS via Meta-Learning. arXiv preprint arXiv:2106.08630 (2021).
[46] Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali Thabet, and Bernard Ghanem. 2020. Sgas:
Sequential greedy architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition . 1620–1630.
16

--- PAGE 17 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
[47] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning filters for efficient
convnets. arXiv preprint arXiv:1608.08710 (2016).
[48] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. 2019. Compressing convolutional
neural networks via factorized convolutional filters. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition . 3977–3986.
[49] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. 2021. Pruning and quantization for
deep neural network acceleration: A survey. Neurocomputing 461 (2021), 370–403.
[50] Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. Mcunet: Tiny deep learning
on iot devices. arXiv preprint arXiv:2007.10319 (2020).
[51] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. 2022. On-device training
under 256kb memory. arXiv preprint arXiv:2206.15472 (2022).
[52] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint
arXiv:1806.09055 (2018).
[53] Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Kay Chen Tan. 2021. A survey on evolu-
tionary neural architecture search. IEEE Transactions on Neural Networks and Learning Systems (2021).
[54] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2018. Rethinking the value of network
pruning. arXiv preprint arXiv:1810.05270 (2018).
[55] Mohammad Loni, Hamid Mousavi, Mohammad Riazati, Masoud Daneshtalab, and Mikael Sjödin. 2022.
TAS:Ternarized Neural Architecture Search for Resource-Constrained Edge Devices. In Design, Automation
& Test in Europe Conference & Exhibition DATE’22, 14 March 2022, Antwerp, Belgium . IEEE. http:
//www.es.mdh.se/publications/6351-
[56] Mohammad Loni, Sima Sinaei, Ali Zoljodi, Masoud Daneshtalab, and Mikael Sjödin. 2020. DeepMaker: A
multi-objective optimization framework for deep neural networks in embedded systems. Microprocessors and
Microsystems 73 (2020), 102989.
[57] Mohammad Loni, Ali Zoljodi, Amin Majd, Byung Hoon Ahn, Masoud Daneshtalab, Mikael Sjödin, and Hadi
Esmaeilzadeh. 2021. FastStereoNet: A Fast Neural Architecture Search for Improving the Inference of Disparity
Estimation on Resource-Limited Platforms. IEEE Transactions on Systems, Man, and Cybernetics: Systems
(2021).
[58] Mohammad Loni, Ali Zoljodi, Sima Sinaei, Masoud Daneshtalab, and Mikael Sjödin. 2019. Neuropower: De-
signing energy efficient convolutional neural network architecture for embedded systems. In International con-
ference on artificial neural networks . Springer, 208–222.
[59] Ilya Loshchilov and Frank Hutter. 2016. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983 (2016).
[60] Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja Giryes, and Lihi Zelnik.
2020. Asap: Architecture search, anneal and prune. In International Conference on Artificial Intelligence and
Statistics . PMLR, 493–503.
[61] Zhuwei Qin, Fuxun Yu, Chenchen Liu, and Xiang Chen. 2018. How convolutional neural network see the world-
A survey of convolutional neural network visualization methods. arXiv preprint arXiv:1804.11191 (2018).
[62] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regularized evolution for image classifier
architecture search. In Proceedings of the aaai conference on artificial intelligence , V ol. 33. 4780–4789.
[63] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. 2021. A
comprehensive survey of neural architecture search: Challenges and solutions. ACM Computing Surveys (CSUR)
54, 4 (2021), 1–34.
[64] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection
with region proposal networks. Advances in neural information processing systems 28 (2015).
[65] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and Jan C
van Gemert. 2021. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. arXiv preprint
arXiv:2110.08059 (2021).
[66] Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyré. 2021. Momentum residual neural networks.
arXiv preprint arXiv:2102.07870 (2021).
[67] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern
recognition . 4510–4520.
17

--- PAGE 18 ---
DASS: Differentiable Architecture Search For Sparse Neural Networks
[68] Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana. 2020. Hydra: Pruning adversarially robust neural
networks. Advances in Neural Information Processing Systems 33 (2020), 19655–19666.
[69] Zhengyang Shen, Lingshen He, Zhouchen Lin, and Jinwen Ma. 2020. Pdo-econvs: Partial differential operator
based equivariant convolutions. In International Conference on Machine Learning . PMLR, 8697–8706.
[70] Shahid Siddiqui, Christos Kyrkou, and Theocharis Theocharides. 2021. Operation and Topology Aware Fast
Differentiable Architecture Search. In 2020 25th International Conference on Pattern Recognition (ICPR) . IEEE,
9666–9673.
[71] Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking model scaling for convolutional neural networks. In
International Conference on Machine Learning . PMLR, 6105–6114.
[72] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning
research 9, 11 (2008).
[73] Athanasios V oulodimos, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis. 2018. Deep
learning for computer vision: A brief review. Computational intelligence and neuroscience 2018 (2018).
[74] Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh. 2021. Rethinking archi-
tecture selection in differentiable NAS. arXiv preprint arXiv:2108.04392 (2021).
[75] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, and Wanli Ouyang. 2022. b-darts: Beta-decay regular-
ization for differentiable architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition . 10874–10883.
[76] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, and Wanli Ouyang. 2022. beta-DARTS: Beta-Decay
Regularization for Differentiable Architecture Search. In 2022 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) . IEEE, 10864–10873.
[77] Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma,
Yanzhi Wang, and Xue Lin. 2019. Adversarial robustness vs. model compression, or both?. In Proceedings of
the IEEE/CVF International Conference on Computer Vision . 111–120.
[78] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. 2019. Nas-bench-
101: Towards reproducible neural architecture search. In International Conference on Machine Learning . PMLR,
7105–7114.
[79] Dingjun Yu, Hanli Wang, Peiqiu Chen, and Zhihua Wei. 2014. Mixed pooling for convolutional neural networks.
InInternational conference on rough sets and knowledge technology . Springer, 364–375.
[80] Zhixiong Yue, Baijiong Lin, Xiaonan Huang, and Yu Zhang. 2020. Effective, Efficient and Robust Neural
Architecture Search. arXiv preprint arXiv:2011.09820 (2020).
[81] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. 2019. Under-
standing and robustifying differentiable architecture search. arXiv preprint arXiv:1909.09656 (2019).
[82] Li Lyna Zhang, Shihao Han, Jianyu Wei, Ningxin Zheng, Ting Cao, Yuqing Yang, and Yunxin Liu. 2021. nn-
Meter: towards accurate latency prediction of deep-learning model inference on diverse edge devices. In Pro-
ceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services . 81–93.
[83] Xinyu Zhang, Ian Colbert, Ken Kreutz-Delgado, and Srinjoy Das. 2021. Training Deep Neural Networks with
Joint Quantization and Pruning of Weights and Activations. arXiv preprint arXiv:2110.08271 (2021).
[84] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu.
2022. Advancing Model Pruning via Bi-level Optimization. arXiv preprint arXiv:2210.04092 (2022).
[85] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. 2019. Deconstructing lottery tickets: Zeros, signs,
and the supermask. arXiv preprint arXiv:1905.01067 (2019).
[86] Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, and Xiang Li. 2020. Neuron-level
Structured Pruning using Polarization Regularizer.. In NeurIPS .
[87] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578 (2016).
[88] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. 2018. Learning transferable architectures for
scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition .
8697–8710.
18
