# AutoML trong Thời đại của các Mô hình Ngôn ngữ Lớn: Thách thức Hiện tại, Cơ hội và Rủi ro Tương lai

Alexander Tornede a.tornede@ai.uni-hannover.de
Difan Deng d.deng@ai.uni-hannover.de
Theresa Eimer t.eimer@ai.uni-hannover.de
Viện Trí tuệ Nhân tạo, Đại học Leibniz Hannover

Joseph Giovanelli j.giovanelli@unibo.it
Alma Mater Studiorum – Đại học Bologna

Aditya Mohan a.mohan@ai.uni-hannover.de
Tim Ruhkopf t.ruhkopf@ai.uni-hannover.de
Sarah Segel s.segel@ai.uni-hannover.de
Viện Trí tuệ Nhân tạo, Đại học Leibniz Hannover

Daphne Theodorakopoulos d.theodorakopoulos@ai.uni-hannover.de
Viện Trí tuệ Nhân tạo, Đại học Leibniz Hannover, Trung tâm Nghiên cứu Trí tuệ Nhân tạo Đức
(DFKI)

Tanja Tornede t.tornede@ai.uni-hannover.de
Viện Trí tuệ Nhân tạo, Đại học Leibniz Hannover

Henning Wachsmuth h.wachsmuth@ai.uni-hannover.de
Marius Lindauer m.lindauer@ai.uni-hannover.de
Viện Trí tuệ Nhân tạo, Trung tâm Nghiên cứu L3S, Đại học Leibniz Hannover

## Tóm tắt

Cả hai lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) và Học máy Tự động (AutoML) đều đã đạt được những kết quả đáng chú ý trong những năm qua. Trong NLP, đặc biệt là các Mô hình Ngôn ngữ Lớn (LLM) đã trải qua một loạt những đột phá nhanh chóng gần đây. Chúng tôi hình dung rằng hai lĩnh vực này có thể đẩy mạnh ranh giới của nhau một cách căn bản thông qua sự tích hợp chặt chẽ. Để thể hiện tầm nhìn này, chúng tôi khám phá tiềm năng của mối quan hệ cộng sinh giữa AutoML và LLM, làm sáng tỏ cách chúng có thể mang lại lợi ích cho nhau. Cụ thể, chúng tôi điều tra cả những cơ hội để nâng cao các phương pháp AutoML với LLM từ các góc độ khác nhau và những thách thức trong việc tận dụng AutoML để cải thiện thêm LLM. Với mục đích này, chúng tôi khảo sát các công trình hiện có và đánh giá một cách phê phán các rủi ro. Chúng tôi tin tưởng mạnh mẽ rằng sự tích hợp của hai lĩnh vực này có tiềm năng làm gián đoạn cả hai lĩnh vực, NLP và AutoML. Bằng cách làm nổi bật những hiệp lực có thể hình dung được, nhưng cũng cả những rủi ro, chúng tôi nhằm thúc đẩy việc khám phá thêm tại giao điểm giữa AutoML và LLM.

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) (Zhao et al., 2023a) hiện đang được mọi người nhắc đến do loạt những đột phá nhanh chóng gần đây đã đạt được, như cơ chế tự chú ý (Vaswani et al., 2017), BERT (Devlin et al., 2019), nhiều phiên bản của GPT (Radford et al., 2018; 2019; Brown et al., 2020; OpenAI, 2022; 2023), LaMDA (Thoppilan et al., 2022), LLaMA (Touvron et al., 2023), hoặc OpenAssistant (Köpf et al., 2023). Thuật ngữ LLM đề cập đến một mô hình ngôn ngữ mà mô hình thực tế được khởi tạo bởi một mạng nơ-ron sâu thường có hàng triệu đến hàng tỷ trọng số.¹ Những LLM này được tiền huấn luyện trên các tập dữ liệu văn bản cực lớn. Do khả năng xuất sắc của chúng trên các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên (NLP) khác nhau, chúng có tiềm năng dẫn đến việc dân chủ hóa NLP, nếu các phiên bản đã được tiền huấn luyện của chúng có thể tiếp cận được với công chúng rộng rãi và sức mạnh của LLM không nằm trong tay của một vài công ty có đủ nguồn lực tài chính. Ngoài ra, chúng tôi nhấn mạnh những khả năng mới nổi của LLM đa phương thức (Yin et al., 2023; Wu et al., 2023b). Bằng cách kết hợp các phương thức dữ liệu khác nhau, như âm thanh hoặc hình ảnh, những mô hình này cho phép giao tiếp linh hoạt hơn với người dùng bằng cách nắm bắt thông tin được trình bày ở các định dạng không phải văn bản cũng như tạo điều kiện cho việc xuất thông tin ở các định dạng không phải văn bản.

Tương tự, Học máy Tự động (AutoML) (Hutter et al., 2019) dân chủ hóa Học máy (ML) bằng cách hỗ trợ các nhà khoa học dữ liệu trong việc tìm ra các đường ống ML có hiệu suất tốt cho các nhiệm vụ cụ thể thông qua việc tự động hóa (một phần). AutoML đã đạt được thành công đáng kể trong thập kỷ qua với các framework mã nguồn mở được sử dụng rộng rãi như Auto-WEKA (Thornton et al., 2013; Kotthoff et al., 2017; 2019), AutoSklearn (Feurer et al., 2015a; 2019; 2022), AutoGluon (Erickson et al., 2020), Auto-PyTorch (Zimmer et al., 2021), và các framework thương mại đóng.

Với bài báo này, chúng tôi muốn làm nổi bật tầm nhìn của chúng tôi về việc AutoML và LLM được tích hợp với nhau để đẩy mạnh ranh giới của cả AutoML và NLP một cách căn bản. Một mặt, chúng tôi kỳ vọng rằng việc áp dụng AutoML cho LLM sẽ cải thiện các giai đoạn khác nhau của vòng đời LLM bằng cách tăng khả năng của chúng và làm cho chúng hiệu quả hơn. Mặt khác, khả năng NLP và mô hình hóa tri thức đột phá của LLM có thể giải phóng toàn bộ tiềm năng của AutoML cả thông qua việc tích hợp như một thành phần tương tác người-máy, và như một thành phần meta-learning kỹ thuật trong chính các framework AutoML. Tương ứng, cuộc khảo sát này nhắm đến cả 1) các nhà nghiên cứu NLP tận dụng các phương pháp AutoML để cải thiện thêm LLM và 2) các nhà nghiên cứu AutoML tìm cách tận dụng điểm mạnh của LLM để cải thiện các mô hình và công cụ AutoML theo nhiều khía cạnh khác nhau được nêu dưới đây. Do đó, chúng tôi chỉ đề cập một cách thưa thớt đến các chủ đề hoặc vấn đề chung liên quan đến LLM, mà tập trung vào các vấn đề và khía cạnh phát sinh từ giao điểm của hai lĩnh vực. Để có cuộc khảo sát tổng quát hơn về LLM, chúng tôi giới thiệu độc giả đến Zhao et al. (2023a). Để làm cho bài báo này phù hợp nhất có thể, chúng tôi đã cố ý chọn không tập trung vào các mô hình đã được tiền huấn luyện nói chung, mà tập trung vào các mô hình ngôn ngữ vì chúng cho phép trích xuất tri thức từ tập hợp lớn dữ liệu không có cấu trúc dưới dạng văn bản trên web mà chắc chắn chứa tri thức AutoML có giá trị.²

Để thể hiện tầm nhìn của chúng tôi, chúng tôi khám phá tiềm năng của mối quan hệ cộng sinh giữa AutoML và LLM, bao gồm một cuộc khảo sát về các công trình hiện có trong vấn đề này. Chúng tôi bắt đầu bằng việc điều tra những thách thức trong việc áp dụng AutoML cho LLM, đặc biệt là Tìm kiếm Kiến trúc Mạng nơ-ron (NAS) (Elsken et al., 2019; Wistuba et al., 2019; White et al., 2023) và Tối ưu hóa Siêu tham số (HPO) (Feurer & Hutter, 2019; Bischl et al., 2023), cũng như các giải pháp tiềm năng để tối ưu hóa tiền huấn luyện, tinh chỉnh và suy luận (Phần 2). Tiếp theo, chúng tôi chuyển đổi góc nhìn và trình bày chi tiết về các cơ hội do LLM mang lại để cải thiện các phương pháp AutoML về mặt tương tác người-máy, cấu hình các hệ thống AutoML, và việc thay thế các phần của hệ thống AutoML bằng LLM (Phần 3). Để đưa ra một cái nhìn cân bằng, chúng tôi cũng đánh giá một cách phê phán các rủi ro tiềm ẩn có thể phát sinh do việc tích hợp AutoML và LLM (Phần 4).

Những hiểu biết chính mà chúng tôi muốn truyền đạt qua công trình này như sau:

(i) Các phương pháp AutoML hiện tại chưa sẵn sàng cho việc tối ưu hóa toàn diện toàn bộ vòng đời của LLM do một số thách thức, như nhu cầu tính toán của tiền huấn luyện và quá trình huấn luyện đa giai đoạn của LLM có các metric và mô hình học tập khác nhau.

(ii) Việc tích hợp LLM với các công cụ AutoML có tiềm năng cải thiện đáng kể thành phần tương tác người-máy của các công cụ tương ứng, giảm bớt nhiệm vụ tẻ nhạt của việc cấu hình chính xác một công cụ AutoML và cải thiện một số thành phần nội bộ của các công cụ AutoML thông qua tri thức thu được về meta-learning từ dữ liệu không có cấu trúc.

(iii) Việc tích hợp hai lĩnh vực nghiên cứu với nhau một cách tự nhiên cũng mang theo những rủi ro như đánh giá không đầy đủ các hệ thống AutoML được hỗ trợ bởi LLM, hành vi sai lầm một cách thảm khốc của các hệ thống AutoML do ảo giác của một thành phần được hỗ trợ bởi LLM, niềm tin (quá) cao vào kết quả của một công cụ AutoML được giải thích thông qua ngôn ngữ tự nhiên và nhu cầu tài nguyên ngày càng tăng.

## 2 AutoML cho LLM

Có thể lập luận rằng AutoML cho LLM chỉ là một ứng dụng khác của AutoML. Tuy nhiên, so với các ứng dụng trước đây của AutoML cho các mô hình học tập khác nhau, LLM đi kèm với một tập hợp hoàn toàn mới các thách thức, khiến cho các phương pháp tiêu chuẩn của các công cụ AutoML hiện có trở nên vô dụng một phần. Thực tế, Godbole et al. (2023) lập luận rằng các công cụ HPO tiêu chuẩn không thể được áp dụng nguyên bản để tối ưu hóa các mạng nơ-ron rất sâu và do đó tốn nhiều tài nguyên như LLM. Hơn nữa, như được chỉ ra bởi Hutter (2022) trong tầm nhìn của ông về Deep Learning 2.0, chúng ta cần kết hợp nhiều ý tưởng lại với nhau và đối mặt với những thách thức mới nếu chúng ta muốn tự động thu được các mô hình có cùng chất lượng hoặc tốt hơn so với những mô hình hiện được thiết kế thủ công cho deep learning bao gồm cả LLM.

Tóm lại, chúng tôi thấy năm thách thức chính cần được giải quyết:

(i) Tiền huấn luyện mô hình cơ sở của LLM cực kỳ tốn kém sao cho rất ít lần chạy huấn luyện đầy đủ – có thể thậm chí chỉ một lần duy nhất – của mô hình cơ sở LLM là khả thi.

(ii) Nhiệm vụ AutoML phức tạp và lý tưởng nhất phải được thực hiện qua nhiều bước của vòng đời LLM, bao gồm tiền huấn luyện, một số bước cho tinh chỉnh, và suy luận. Tuy nhiên, những bước này hiện không thể được giải quyết trong một tối ưu hóa chung, toàn diện duy nhất và thay vào đó phải được thực hiện độc lập hầu hết thời gian;

(iii) Việc tìm kiếm các kiến trúc mạng nơ-ron tốt để giải quyết một vấn đề cụ thể là một nhiệm vụ tẻ nhạt và các phương pháp tự động hiện đại như NAS chỉ có thể làm được đến một mức độ nhất định, nhưng chưa thể tạo ra các kiến trúc đột phá mới.

(iv) Tất cả các giai đoạn của vòng đời LLM đều yêu cầu tối ưu hóa một metric khác nhau. Đặc biệt trong tiền huấn luyện, metric này có thể được xem như một proxy cho hiệu suất cuối cùng trên nhiều nhiệm vụ khác nhau mà LLM có thể được sử dụng. Điều này có thể dẫn đến các chỉ báo hiệu suất có thể gây hiểu lầm, nhiễu và thiên vị cho quá trình AutoML.

(v) AutoML thường chỉ xem xét một mô hình học tập duy nhất (ví dụ học có giám sát) tại một thời điểm. Tuy nhiên, việc huấn luyện LLM là thách thức vì các giai đoạn khác nhau của vòng đời yêu cầu các mô hình học tập khác nhau.

Sau khi cung cấp thêm nền tảng về LLM trong phần tiếp theo, chúng tôi đi sâu vào chi tiết cho tất cả những thách thức này và thảo luận về các ý tưởng AutoML hiện có mà hoặc đã được áp dụng cho LLM hoặc có thể là một hướng tương lai đầy hứa hẹn.

### 2.1 Nền tảng về LLM

Vòng đời của một LLM thường bao gồm ba giai đoạn chính: tiền huấn luyện, tinh chỉnh và suy luận (Zhao et al., 2023a). Mỗi giai đoạn của vòng đời này đi kèm với tập hợp các mục tiêu, siêu tham số và quyết định thiết kế riêng ảnh hưởng đến hiệu suất cuối cùng trên các nhiệm vụ downstream, như được mô tả trong Hình 1.

Trong **tiền huấn luyện**, mô hình cơ sở được huấn luyện trên một tập ngữ liệu lớn văn bản không có nhãn để học các mẫu ngôn ngữ và nắm bắt tri thức chung về ngôn ngữ. Mục tiêu tổng thể của tiền huấn luyện là tạo ra các biểu diễn hữu ích của các chuỗi với một nhiệm vụ pre-text theo cách quasi-unsupervised tương ứng với giai đoạn đầu tiên của mô hình Học tự giám sát (SSL) (Balestriero et al., 2023). Đây thường là phần tốn kém nhất của việc huấn luyện LLM và chỉ một vài nhóm và công ty trên toàn thế giới có thể đủ khả năng tiền huấn luyện một mô hình cơ sở.

**Tinh chỉnh** tiếp theo, trong đó mô hình đã được tiền huấn luyện được huấn luyện thêm trên dữ liệu cụ thể theo domain hoặc nhiệm vụ để chuyên môn hóa tri thức của nó và thích ứng với các nhiệm vụ, domain hoặc trường hợp sử dụng cụ thể. Nói chung, tinh chỉnh bao gồm các nhóm phương pháp khác nhau như tinh chỉnh có giám sát cụ thể theo nhiệm vụ (Zhao et al., 2023a), instruction tuning (Wei et al., 2022), và tinh chỉnh alignment (Zhao et al., 2023a). Thành công trong việc chuyên môn hóa này được định lượng bằng một metric hoặc hàm loss tương ứng. Với điều kiện tiền huấn luyện tìm được các biểu diễn hữu ích, mục tiêu ở đây là sử dụng hoặc thích ứng những biểu diễn này với các nhiệm vụ downstream cụ thể trong tâm trí. Điều này có thể rất hiệu quả ngay cả với dữ liệu hạn chế cho nhiệm vụ downstream và do đó ít đòi hỏi tính toán hơn nhiều so với việc huấn luyện một mô hình cơ sở. Lý do có thể là đối với một nhiệm vụ downstream nhất định, thường tồn tại một reparametrization có chiều thấp của embedding chung mạnh được học trong quá trình tiền huấn luyện có số lượng lớn tham số. Điều này cho phép giải phóng toàn bộ sức mạnh của transfer learning (Aghajanyan et al., 2021).

Instruction tuning có thể được xem như tinh chỉnh có giám sát cụ thể theo nhiệm vụ tổng quát hóa mà dạy LLM tuân theo hướng dẫn dưới dạng prompts. Nó tinh chỉnh một mô hình duy nhất sử dụng hướng dẫn thông qua prompts bao trùm các nhiệm vụ khác nhau, do đó cho phép việc prompting LLM. Một nhiệm vụ rất cụ thể và, từ góc độ AutoML, phức tạp thường được giải quyết bằng một dạng instruction tuning, gọi là Reinforcement Learning from Human Feedback (RLHF) (Fernandes et al., 2023), là alignment.³ Alignment (tinh chỉnh) mô tả ý tưởng về việc căn chỉnh gần đúng hành vi và đầu ra của LLM với các mục tiêu nhiệm vụ liên quan đến giá trị con người như tính hữu ích, trung thực, vô hại, v.v. Hai thành phần cần thiết để làm cho Reinforcement Learning (RL) hoạt động trên một Language Model (LM) đã được tiền huấn luyện là: (i) Một Reward Model (RM) để chuyển đổi một chuỗi văn bản thành một phần thưởng vô hướng đại diện số cho sở thích của con người. (ii) Một policy nhận các truy vấn hoặc prompts của người dùng làm đầu vào để tạo ra các token ngôn ngữ làm đầu ra.

Chúng tôi lưu ý rằng, về nguyên tắc, người ta cũng có thể sử dụng một mô hình đã được tiền huấn luyện nguyên bản. Tuy nhiên, trong thực tế, một số instruction tuning và alignment fine-tuning được thực hiện nếu mô hình được phát hành cho người dùng cuối. Hơn nữa, một số hình thức tinh chỉnh cụ thể theo nhiệm vụ bổ sung thường được thực hiện để tăng hiệu suất của LLM cho việc sử dụng được chỉ định và làm cho nó có thể sử dụng được đúng cách.

Cuối cùng, trong **suy luận**, LLM đã được tinh chỉnh tạo ra văn bản cho các nhiệm vụ liên quan đến ngôn ngữ, như trả lời câu hỏi, dựa trên tri thức đã học và hiểu biết theo ngữ cảnh.

### 2.2 Thách thức I: Chi phí Tiền huấn luyện Mô hình Cơ sở

Việc áp dụng các phương pháp AutoML black-box tiêu chuẩn, như grid search, random search, Bayesian Optimization hoặc các thuật toán tiến hóa, để tiền huấn luyện LLM đơn giản là không khả thi vì một lần tiền huấn luyện LLM đòi hỏi hàng trăm GPU trong nhiều ngày. Brown et al. (2020) ước tính rằng một lần huấn luyện GPT-3 đòi hỏi nhiều tháng trên một nghìn GPU V100. Để đưa ra con số, hãy xem xét việc điều chỉnh 10 siêu tham số và một bộ tối ưu AutoML black-box đòi hỏi ít nhất 10 lần số siêu tham số làm mẫu⁴, chúng ta sẽ cần 100 tháng (tức là hơn 8 năm) trên một nghìn GPU V100. Ngay cả khi xem xét các LLM nhỏ hơn và có thể là mã nguồn mở, nhiều LLM vẫn đòi hỏi huấn luyện trên một lượng đáng kể đơn vị tính toán (GPU hoặc TPU) trong vài ngày, tạo ra yêu cầu tài nguyên huấn luyện hàng trăm exaFLOPS (Geiping & Goldstein, 2023) chỉ cho một lần chạy huấn luyện duy nhất. Vì các khả năng nổi lên của LLM chỉ xảy ra từ một kích thước nhất định trở lên, kích thước là quan trọng và do đó, chi phí huấn luyện lớn thường không thể tránh khỏi. Xét đến các phương pháp tiên tiến gần đây của AutoML, có thể có hai cách để giải quyết điều này, như được thảo luận dưới đây.

#### 2.2.1 Tối ưu hóa Đa độ trung thực Được hướng dẫn bởi Prior với Scaling Laws

Trong các trường hợp mà nhiều đánh giá huấn luyện đầy đủ với các siêu tham số hoặc kiến trúc mạng nơ-ron khác nhau không khả thi, một phương pháp AutoML phổ biến là tối ưu hóa đa độ trung thực. Ý tưởng chung là xấp xỉ hàm mục tiêu thực bằng một độ trung thực ít tốn kém hơn (ví dụ, bằng cách huấn luyện với ít epoch hơn hoặc bằng cách huấn luyện trên ít dữ liệu hơn) làm cho điều này trở thành một chiến lược ứng viên tự nhiên cho HPO cho LLM.

Tuy nhiên, ngay cả các phương pháp đa độ trung thực cũng đòi hỏi ít nhất hàng chục lần chạy huấn luyện đầy đủ để hoạt động tốt. Đồng thời, các chuyên gia con người có phần thành công trong việc điều chỉnh LLM thủ công mà không cần huấn luyện lại quá mức. Được hướng dẫn bởi quan sát này về các vấn đề AutoML trước đó, cộng đồng đã phát triển các phương pháp tận dụng tri thức prior trực quan của các chuyên gia. Trong khi Souza et al. (2021) và Hvarfner et al. (2022) đề xuất các cách để tích hợp tri thức prior về vị trí của các cấu hình siêu tham số đầy hứa hẹn vào Bayesian Optimization, Mallik et al. (2023) mở rộng ý tưởng này sang tối ưu hóa đa độ trung thực và đạt được hiệu suất mạnh trong vòng mười lần chạy huấn luyện đầy đủ.

Hơn nữa, các phương pháp đa độ trung thực phải được cấu hình chính xác, đặc biệt là về việc chọn loại độ trung thực, để đạt được hiệu suất mạnh. Lựa chọn này nên dựa trên quan sát rằng thứ tự giữa các cấu hình tiềm năng trên các xấp xỉ độ trung thực thấp nên tương quan với độ trung thực tối đa. Hiện tại chưa rõ, liệu một giả định như vậy có thể được đưa ra cho LLM hay không khi xem xét công trình gần đây, ví dụ, bởi Kirsch et al. (2022). Thậm chí còn liên quan hơn, trong việc theo đuổi hiểu biết về mối tương tác giữa kích thước của một mô hình ngôn ngữ và hiệu suất của nó, các công trình gần đây đã đi sâu vào các quy luật scaling của các mô hình ngôn ngữ (Radford et al., 2019; Brown et al., 2020; Kaplan et al., 2020). Họ đã chỉ ra rằng cải thiện theo các chiều scale thường dẫn đến tăng hiệu suất, cả cho các chiều scaling tham số như độ rộng và độ sâu mạng, các chiều scaling tính toán như số bước huấn luyện, và lượng dữ liệu được sử dụng để huấn luyện. Khi không bị giới hạn bởi hai yếu tố khác, hiệu suất thể hiện một mối tương quan power-law với mỗi trong ba yếu tố scale, bị giới hạn bởi lợi nhuận giảm dần. Tương ứng, dưới các giả định hợp lý, các phương pháp đa độ trung thực dường như thực sự rất phù hợp, nếu được cấu hình chính xác.

Được thúc đẩy bởi cùng ý tưởng tận dụng các xấp xỉ rẻ, Yang et al. (2021) giảm thiểu chi phí lớn của tối ưu hóa siêu tham số LLM bằng cách tận dụng các parametrization mạng cụ thể cho phép huấn luyện ổn định qua các kích thước mô hình. Bằng cách này, một phiên bản nhỏ hơn của mô hình thực tế có thể được điều chỉnh, và cấu hình siêu tham số tốt nhất được tìm thấy có thể được chuyển giao cho mô hình lớn hơn. Tuy nhiên, phương pháp này bị giới hạn cho các siêu tham số có optimum ổn định qua các scale mạng khác nhau dưới parametrization mạng. Tự nhiên, điều này loại trừ các siêu tham số xác định scale huấn luyện, mà các siêu tham số khác được chuyển giao qua. Như được chỉ ra trong bài báo, các siêu tham số có hiệu ứng regularization mạnh, như xác suất dropout và weight decay, được phát hiện thực nghiệm là không chuyển giao tốt.

#### 2.2.2 AutoML dựa trên Gradient

Thay vì có một vòng lặp ngoài huấn luyện một mô hình ML với các cấu hình khác nhau lặp đi lặp lại, sẽ là mong muốn để học các siêu tham số trong quá trình huấn luyện mô hình ML. Điều này sẽ đặc biệt thú vị cho LLM khi chúng ta chỉ có thể đủ khả năng một lần chạy huấn luyện. Mặc dù tối ưu hóa dựa trên gradient có thể thực hiện được cả cho HPO (Maclaurin et al., 2015; Luketina et al., 2016; Franceschi et al., 2017; MacKay et al., 2019; Lorraine et al., 2020) và neural architecture search đối với các cell riêng lẻ của mạng (Liu et al., 2019a; Elsken et al., 2019), những phương pháp này vẫn đang gặp khó khăn với tính mạnh mẽ và scaling lên các mạng lớn như LLM.

#### 2.2.3 Meta-Learning Khi nào và Như thế nào để Điều chỉnh Huấn luyện

Nếu chúng ta nghĩ về cách các chuyên gia con người huấn luyện LLM, họ sử dụng check-pointing để có thể can thiệp nếu quá trình huấn luyện có nguy cơ thất bại, ví dụ, thay đổi learning rate tương ứng. Nhìn chung, chiến lược thủ công này giống với các phương pháp cấu hình động (Adriaensen et al., 2022) mà meta-learn cách điều chỉnh các cấu hình siêu tham số trong khi quá trình huấn luyện mô hình đang diễn ra. Đi xa hơn một bước sẽ dẫn đến việc học chính quá trình học (Andrychowicz et al., 2016; Chen et al., 2022). Vì phương pháp này đòi hỏi một meta-learning offline để thu được meta-policy tương ứng (ví dụ, được học với RL), đây là một vấn đề mở về cách scale nó lên LLM mà việc thu thập meta-learning và đánh giá của meta-policy không dễ dàng khả thi.

### 2.3 Thách thức II: Một loạt các Giai đoạn Khác nhau

Như chúng tôi minh họa trong Hình 1, vòng đời LLM bao gồm các giai đoạn khác nhau và mỗi giai đoạn đều có các mục tiêu, chủ thể và siêu tham số khác nhau. Điều đó làm cho một phương pháp AutoML toàn diện rất khó và, có lẽ, thậm chí không thể. Mặc dù về nguyên tắc có thể điều chỉnh các pipeline phức tạp của các mô hình dự đoán (Wachsmuth et al., 2013; Feurer et al., 2015a; M. Wever & Hüllermeier, 2018; Olson & Moore, 2019), sẽ quá tốn kém để điều chỉnh tất cả các giai đoạn của việc huấn luyện LLM một cách chung. Hơn nữa, chúng thậm chí không tuân theo cùng các metric cho các mục tiêu. Do đó, chúng ta thường điều chỉnh chúng độc lập và thảo luận từng bước dưới đây.

#### 2.3.1 Tiền huấn luyện

Tối ưu hóa Siêu tham số cho tiền huấn luyện không chỉ tốn kém, như đã thảo luận trước đó, mà còn bao trùm nhiều loại siêu tham số rất khác nhau. Việc chọn nguồn dữ liệu cho tiền huấn luyện là một lựa chọn tinh tế nhưng quan trọng với mục đích này. Nó ảnh hưởng đến tri thức cụ thể theo domain và chung được mã hóa vào LLM, đồng thời cũng tác động đến khả năng đối thoại và lý luận của nó (Zhao et al., 2023a). Ngoài ra, các quyết định tiền xử lý dữ liệu tác động đến chất lượng dữ liệu và, đến lượt nó, ảnh hưởng đến hiệu suất downstream. Như với tất cả các mô hình dựa trên văn bản, tokenization (Schuster & Nakajima, 2012; Sennrich et al., 2016; Kudo, 2018) đóng vai trò chính trong biểu diễn dữ liệu huấn luyện. Tiếp theo, những phức tạp của kiến trúc backbone của LLM phải được quyết định. Các mô hình dựa trên Transformer (Vaswani et al., 2017) là tiền phong trong các mô hình ngôn ngữ. Việc thiết kế một mô hình transformer như vậy bao gồm một số lựa chọn kiến trúc quan trọng như một loạt các kiến trúc encoder-decoder khác nhau (Vaswani et al., 2017; Lewis et al., 2020), kiến trúc decoder (Brown et al., 2020; Zhang et al., 2022) và kiến trúc encoder (Devlin et al., 2019; Liu et al., 2019b). Tương tự, một tùy chọn cấu hình chính là lựa chọn các chiến lược huấn luyện tự giám sát (Lewis et al., 2020). Bên cạnh đó, các lựa chọn liên quan đến normalization (Ba et al., 2016; Xiong et al., 2020), hàm kích hoạt (Hendrycks & Gimpel, 2016; Shazeer, 2020), hoặc positional encoding (Su et al., 2022), cũng như các thiết lập huấn luyện như optimizer (Kingma & Ba, 2015; Loshchilov & Hutter, 2018; Shazeer & Stern, 2018), lịch trình learning rate (Loshchilov & Hutter, 2017), và batch size phải được đưa ra.

#### 2.3.2 Tinh chỉnh

Vì các phương pháp tinh chỉnh có thể được phân loại thành các nhóm theo các mô hình học tập khác nhau, các thách thức tương ứng cũng khác nhau theo mô hình học tập như chúng tôi nêu dưới đây.

**Tinh chỉnh Có giám sát Cụ thể theo Nhiệm vụ** Tinh chỉnh cụ thể theo nhiệm vụ có thể được coi là một giai đoạn học có giám sát tiêu chuẩn dựa trên các cặp input-output (có hoặc không có hướng dẫn). Tức là, LLM vẫn có thể được tinh chỉnh cho các nhiệm vụ cụ thể mà không cần instruction tuning, và vì việc thực hiện của hai điều này khác nhau, việc áp dụng AutoML khác nhau trong thực tế. AutoML cho tinh chỉnh có giám sát về nguyên tắc có thể tuân theo cùng các phương pháp đã được nghiên cứu rộng rãi trong cộng đồng AutoML vì nó cung cấp một tín hiệu huấn luyện có giám sát rõ ràng và khả thi về mặt tính toán. Những câu hỏi thú vị phát sinh liên quan đến neural architecture search mà chúng tôi sẽ quay lại sau. Một trong những thách thức cốt lõi liên quan đến tinh chỉnh có giám sát (cụ thể theo nhiệm vụ) là nó có thể hoàn tác một số alignment đạt được thông qua tinh chỉnh alignment (Qi et al., 2023) sao cho hai giai đoạn lý tưởng nhất được xem xét chung khi thực hiện HPO.

**Instruction Tuning** Với instruction tuning là một loại tinh chỉnh có giám sát cụ thể theo nhiệm vụ tổng quát hóa (Wei et al., 2022) mà chuẩn bị LLM cho prompting, những thách thức tương tự như cho tinh chỉnh có giám sát cụ thể theo nhiệm vụ phát sinh khi thực hiện AutoML cho giai đoạn này. Đặc biệt, đây là trường hợp vì instruction tuning cũng là một dạng tinh chỉnh có giám sát được tổng quát hóa cho nhiều nhiệm vụ và do đó, các phương pháp AutoML cho các vấn đề học có giám sát có thể được sử dụng. Trong khi việc tối ưu hóa instruction tuning sử dụng AutoML quy về các kỹ thuật đã có sẵn cho các vấn đề học có giám sát, một vài thách thức bổ sung phát sinh.

Thứ nhất, việc tuyển chọn dữ liệu cụ thể theo nhiệm vụ cần thiết với chất lượng và số lượng đầy đủ là cồng kềnh và có thể tốn nhiều lao động. Để giảm bớt vấn đề số lượng nặng nề, các LLM đã được huấn luyện có thể được prompting sử dụng các template như một mô-đun trích xuất dữ liệu (Zhang et al., 2023c). Tuy nhiên, chất lượng của tập dữ liệu được trích xuất tuân theo prompt engineering và các siêu tham số suy luận LLM được đặt trong pipeline tiền xử lý. Chúng có thể được áp dụng các phương pháp AutoML.

Thứ hai, thường thì các nhiệm vụ chia sẻ một cấu trúc hoặc tri thức vốn có như trong các nhiệm vụ coding cho code summarization hoặc code completion. Việc khai thác những điểm tương đồng sử dụng multi-task learning có thể tạo điều kiện cho hiệu suất (tổng quát hóa) được cải thiện, tính mạnh mẽ, hiệu quả dữ liệu và sự mất cân bằng dữ liệu qua các nhiệm vụ (Liu et al., 2023). Với khả năng tạo ra nhiều tập dữ liệu và các nhiệm vụ liên quan với chi phí có thể đáng kể, việc chọn nhiệm vụ có thể trở thành một vấn đề. Kung et al. (2023) dùng đến active learning để giải quyết nó, nhưng AutoML cũng có thể được gọi để hỗ trợ trong việc lựa chọn này.

Một thách thức thứ ba phát sinh từ bản chất đa nhiệm vụ của quá trình tuning vì hàm chi phí được tối ưu hóa bởi một phương pháp AutoML phải có khả năng nắm bắt hiệu suất qua nhiều nhiệm vụ. Trong khi vấn đề này phổ biến trong lĩnh vực liên quan của algorithm configuration (Schede et al., 2022), nó ít phổ biến hơn trong AutoML. Tuy nhiên, các phương pháp tương ứng tồn tại (Perrone et al., 2018; Law et al., 2019; Li et al., 2022). Nói chung, AutoML có thể giúp tìm ra các giá trị siêu tham số đúng cho instruction tuning được mô tả bởi Wei et al. (2022) như số bước gradient, batch size, optimizer hoặc learning rate.

**Tinh chỉnh Alignment** Tinh chỉnh alignment thường được thực hiện thông qua reinforcement learning, chính xác hơn là RLHF (Fernandes et al., 2023). Mặc dù RLHF có thể được xem như một dạng instruction tuning, chúng tôi dành một đoạn riêng cho alignment tuning và RLHF ở đây, vì chúng là những vấn đề phức tạp từ góc độ AutoML do sự phụ thuộc của chúng vào reinforcement learning (Eimer et al., 2023).⁵ Thật không may, RL không được nghiên cứu kỹ từ góc độ AutoML. Vẫn còn nhiều câu hỏi mở, như tính chất của các cảnh quan siêu tham số trong RL (Mohan et al., 2023), các giao thức đánh giá đáng tin cậy (Eimer et al., 2023), tính ổn định của huấn luyện do nhiệm vụ học không dừng và việc thu thập dữ liệu không xác định trong quá trình, đặc biệt trong học online và on-policy. Tính không dừng trong dữ liệu huấn luyện đưa nhiễu vào các quan sát hiệu suất cho các hệ thống AutoML và tăng nguy cơ overfitting quá trình tuning với một vài random seed (Eimer et al., 2023).

Automated Reinforcement Learning (AutoRL) (Parker-Holder et al., 2022) nhằm giải quyết những vấn đề này thông qua các kỹ thuật như tối ưu hóa siêu tham số cho RL (Li et al., 2019; Parker-Holder et al., 2020; Wan et al., 2022), learned optimizers (Lan et al., 2023), neural architecture search (Wan et al., 2022) và cấu hình động (Adriaensen et al., 2022). Hầu hết những cân nhắc này của AutoRL chuyển đổi sang các thành phần RL của LLM; do đó, các phương pháp tương ứng có thể là một lựa chọn phù hợp. Tuy nhiên, tại thời điểm hiện tại, việc điều chỉnh phù hợp của RL cho LLM thậm chí còn ít được nghiên cứu hơn AutoRL cho các benchmark RL truyền thống như control hoặc games.

Một quyết định thiết kế quan trọng là chính thuật toán RL. PPO (Schulman et al., 2017) và A2C (Mnih et al., 2016) hiện là những lựa chọn phổ biến cho RLHF. Tuy nhiên, về nguyên tắc, bản chất vô hướng của phần thưởng trong giai đoạn tối ưu hóa RL của LLM alignment cho phép tích hợp liền mạch nhiều thuật toán RL hiện có. Do đó, việc chọn thuật toán RL dựa trên nhiệm vụ hiện tại có thể tận dụng thêm tiềm năng ở đây (Laroche & Feraud, 2018).

Hầu hết các quyết định thiết kế cụ thể cho RLHF liên quan đến reward model (RM) không được sử dụng trong RL tiêu chuẩn và do đó không được nghiên cứu trong tài liệu AutoRL hiện có. Một quyết định thiết kế quan trọng cho RM là xác định kích thước của nó so với LLM đã được tiền huấn luyện. Không có tiêu chuẩn được thiết lập cho điều này, với các giải pháp hiện tại được điều khiển một cách heuristic. Ví dụ, OpenAI sử dụng một RM với 6 tỷ tham số cho một LLM với 175 tỷ tham số, trong khi Deepmind sử dụng cùng kích thước cho cả RM và LLM (Lambert et al., 2022). Quyết định thiết kế này có thể phụ thuộc vào nhiệm vụ downstream mà việc tinh chỉnh đang được thực hiện và các mục tiêu đa dạng mà điểm preference nhằm nắm bắt. Việc tối ưu hóa tỷ lệ kích thước tối ưu cho RM đối với LLM là một vấn đề cấu hình có thể được tăng tốc thông qua các phương pháp đa độ trung thực dựa trên learning-curve (Klein et al., 2017; Jawed et al., 2021; Ruhkopf et al., 2023). Kết quả gần đây về tác động tích cực của việc kết hợp nhiều reward model để tạo ra tín hiệu phần thưởng biểu cảm hơn (Wu et al., 2023c) mở ra các con đường mới cho các phương pháp có thể sử dụng các phương pháp luận ensembling cho các kiến trúc tinh chỉnh cụ thể theo nhiệm vụ.

Hơn nữa, các phương pháp cập nhật RM và policy một cách lặp lại cùng nhau (Lambert et al., 2022; Bai et al., 2022) có thể mở ra cánh cửa cho động lực phức tạp mà các cảnh quan siêu tham số AutoRL (Mohan et al., 2023) có thể được sử dụng để thiết kế các optimizer và phương pháp đa độ trung thực tốt hơn.

Một sự khác biệt khác với AutoRL tiêu chuẩn là cập nhật policy trong RLHF, thường chỉ được thực hiện trên một tập con của các trọng số do kích thước của LLM (Hu et al., 2022a; Glaese et al., 2022). Tập con trọng số này cho việc cập nhật phụ thuộc vào nhiều yếu tố, bao gồm độ phức tạp của các khái niệm trong dữ liệu, điểm preference, và kích thước của LLM. Các phương pháp cho việc tạo dữ liệu thông qua exploration và curriculum learning để chọn thích ứng dữ liệu tốt nhất cho việc cập nhật policy có thể đặc biệt hữu ích trong kịch bản này (Jiang et al., 2023). Các kỹ thuật từ tối ưu hóa đa mục tiêu có thể được sử dụng thêm để cân bằng chất lượng dữ liệu, cập nhật policy và số lượng tham số để cập nhật.

#### 2.3.3 Suy luận

Các truy vấn suy luận bao hàm các lần truyền tiến qua các mô hình có hàng tỷ tham số, dẫn đến chi phí triển khai cao có thể tốn kém về mặt tính toán cũng như sinh thái. Chi phí trước đặc biệt quan trọng vì một loạt LLM được tinh chỉnh cho nhiều nhiệm vụ khác nhau phục vụ các cộng đồng người dùng lớn. Kết quả là, việc giảm thiểu những chi phí này và tối đa hóa tiện ích cho người dùng nên là mục tiêu chính của giai đoạn này và dẫn đến một vấn đề tối ưu hóa đa mục tiêu khó khăn. Các kỹ thuật pruning tự động (Chen et al., 2020; Wang et al., 2020) có thể giúp giảm chi phí này. Ngoài ra, mixed precision (Shen et al., 2020; Dettmers et al., 2022), sử dụng 16 bit (hoặc thậm chí ít hơn) và 32 bit để biểu diễn các số dấu phẩy động, giảm sử dụng bộ nhớ và tăng tốc tính toán trong huấn luyện cũng như suy luận (Yuan & Agaian, 2023). Tương tự, việc điều chỉnh số lượng token được tạo tối đa, tức là độ dài của một phản hồi hoặc số lượng phản hồi, trong các trường hợp người dùng yêu cầu nhiều phản hồi, có thể giúp giảm chi phí nhưng có thể gây hại đến lợi ích của người dùng. Hơn nữa, việc điều chỉnh các siêu tham số ảnh hưởng đến tính ngẫu nhiên của văn bản được tạo, như temperature hoặc điều chỉnh top-k, có thể tăng tiện ích nhưng tự nhiên có thể tác động đến số lượng truy vấn dự kiến cần thiết để đạt được đầu ra mong muốn. Đáng chú ý, các chiến lược decoding tiên tiến (Zhao et al., 2023a) bao gồm prompting lặp lại và template hoặc các khái niệm liên quan đến chain-of-thought (Besta et al., 2023; Ning et al., 2023) có thể cung cấp những cải thiện đáng chú ý. Tuy nhiên, việc tự động tối ưu hóa suy luận thông qua các phương tiện của AutoML vẫn là một thách thức mở.

Tuy nhiên, các công trình đầu tiên trong lĩnh vực automated prompt engineering tồn tại. Shin et al. (2020) chứng minh rằng tối ưu hóa dựa trên gradient (xấp xỉ) có thể dẫn đến các prompt mà dẫn đến kết quả tốt hơn so với các prompt được thiết kế thủ công. Tương tự, Zhou et al. (2023) chỉ ra rằng các prompt tốt có thể được tìm thấy một cách tự động thông qua một sự tương tác lặp lại giữa nhiều mô hình ngôn ngữ trong đó một LLM đề xuất các prompt hoặc thích ứng với các prompt và một LLM khác đánh giá những prompt này. Nói chung, prompt engineering được thúc đẩy bởi hiện tượng in-context learning (Brown et al., 2020) cho phép LLM điều chỉnh hành vi của chúng thông qua việc điều kiện hóa trên các ví dụ input-output như một phần của prompt mà không có bất kỳ thay đổi nào đối với các trọng số mạng cơ bản. Do đó, công việc trong lĩnh vực automated prompt engineering cũng có thể được xem như công việc trong lĩnh vực in-context learning.

Wang et al. (2023a) thực hiện bước đầu tiên hướng tới việc áp dụng AutoML để tối ưu hóa suy luận LLM bằng cách tận dụng BlendSearch (Wang et al., 2021) và đề xuất một chiến lược pruning dựa trên chi phí để tối ưu hóa các siêu tham số suy luận của LLM. Điều này chứng minh rằng AutoML thực sự có thể được sử dụng để tối ưu hóa giai đoạn suy luận của vòng đời LLM.

Ở một mức độ tổng quát hơn, knowledge distillation có thể được sử dụng để cải thiện tốc độ suy luận của LLM bằng cách tạo ra các LLM nhỏ hơn (student) với hiệu suất tương tự như LLM gốc (teacher) (Gu et al., 2023; Hsieh et al., 2023). Các phương pháp AutoML có thể được sử dụng ở đây để tối ưu hóa các siêu tham số của các thuật toán knowledge distillation tương ứng. Ví dụ, phương pháp MiniLLM được đề xuất bởi Gu et al. (2023) tận dụng một thuật toán gradient descent cùng với một chiến lược clipping tiên tiến, cả hai đều có siêu tham số, có thể được tối ưu hóa. Tương tự, step-by-step distillation bởi Hsieh et al. (2023) có một số siêu tham số có thể được tối ưu hóa với các phương pháp AutoML. Tuy nhiên, điều này, về nguyên tắc, có thể được xem như một giai đoạn khác đòi hỏi một thiết lập AutoML khác nhau so với việc tối ưu hóa chính pipeline suy luận vì người ta có thể xây dựng một pipeline suy luận ngay cả xung quanh student LLM thu được từ quá trình distillation.

### 2.4 Thách thức III: Đa dạng các Chỉ báo Hiệu suất

Cuối cùng, chúng ta nhằm thu được một hệ thống LLM hoạt động tốt. Một thực hành tốt nhất cho AutoML là tối ưu hóa hiệu suất cuối cùng của một hệ thống (hoặc ít nhất là một metric proxy có tương quan rất tốt) để tránh bất kỳ sự không phù hợp nào giữa quá trình AutoML và các metric quan trọng sau khi triển khai hệ thống. Tuy nhiên, không dễ trả lời hiệu suất chính xác đòi hỏi gì và cách nó được định lượng. Điều này có một số lý do:

(i) Học máy tiêu chuẩn đã đi kèm với nhiều metric hiệu suất có thể, và việc chọn những metric đúng bao gồm việc đánh giá tầm quan trọng của chúng, điều này phụ thuộc vào ứng dụng nhất định. Ví dụ, bên cạnh độ chính xác, cộng đồng xem xét thời gian suy luận cho throughput cao, bộ nhớ, và tiêu thụ năng lượng cho các thiết bị edge. Trong bối cảnh LLM đa phương thức, nhiều metric bổ sung khác nhau có thể đóng vai trò quan trọng, như alignment hình ảnh-văn bản (Xu et al., 2018; Grimal et al., 2023) hoặc tính mạnh mẽ đối nghịch (Zhao et al., 2023b). AutoML đa mục tiêu cho phép tối ưu hóa cho một số metric hiệu suất này (Morales-Hernández et al., 2021; Karl et al., 2023). (ii) Trong khi huấn luyện mô hình cơ sở, nhiệm vụ downstream không được biết, nhưng mô hình cơ sở cần phải tổng quát nhất có thể. Điều này có nghĩa là chúng ta không biết metric hiệu suất cuối cùng trong các giai đoạn huấn luyện trước đó nhưng phải dựa vào khả năng của mô hình đã được tiền huấn luyện về hiệu suất của nó sau tinh chỉnh (sẽ diễn ra tại một thời điểm sau đó). (iii) Xét đến sự phổ biến của LLM và sự tương tác trực tiếp với người dùng, việc xem xét vấn đề bias và những tác động của nó là cực kỳ quan trọng (Kumar et al., 2023).

Xem xét tầm quan trọng của điều sau, hãy để chúng tôi thảo luận việc giảm bias của đầu ra mô hình ngôn ngữ thông qua tinh chỉnh với AutoML chi tiết hơn. Trong khi các mô hình ngôn ngữ tự nó có thể được sử dụng như các máy sinh dữ liệu cho dữ liệu debias (Schick et al., 2021; Hernandez et al., 2023), cũng như các template câu được định nghĩa trước (Liang et al., 2020), AutoML có thể hỗ trợ trong việc xác định lượng dữ liệu bổ sung cần thiết cũng như loại dữ liệu nên được tạo (dữ liệu để loại bỏ bias, neutralizing representations, hoặc equalizing chúng). Debiasing cũng có thể được xen kẽ với tinh chỉnh mục tiêu nhiệm vụ (Saravanan et al., 2023) xem xét rằng thời lượng và lượng dữ liệu được sử dụng trong cả hai giai đoạn là các siêu tham số quan trọng cho các phương pháp AutoML để nhắm mục tiêu. Gira et al. (2022) đã chỉ ra rằng có thể tinh chỉnh chỉ một tập con trọng số mô hình để đạt được kết quả ít bias hơn – mặc dù việc chọn các tham số đúng để điều chỉnh là quan trọng cho hiệu suất. Tuy nhiên, các hệ thống AutoML chỉ có thể hỗ trợ trong việc huấn luyện các mô hình công bằng hơn; đầu vào của con người được yêu cầu để tập trung các giá trị như công bằng trong toàn bộ pipeline huấn luyện (Bender et al., 2021; Weerts et al., 2023).

Nhìn chung, như việc trình bày của chúng tôi cho thấy, chủ đề định lượng mức độ tốt của một LLM cho một trường hợp sử dụng cụ thể là một chủ đề phức tạp, cũng đã được thảo luận sâu sắc trong tài liệu. Ví dụ, Liang et al. (2023) chứng minh rằng các metric khác nhau có tầm quan trọng khác nhau trong các trường hợp sử dụng LLM khác nhau. Tương tự, Dehghani et al. (2022) trình bày về việc báo cáo một phần các metric có thể có hại như thế nào đối với việc đưa ra kết luận cuối cùng.

### 2.5 Thách thức IV: Kết hợp các Mô hình Học tập và Phương thức Khác nhau

Liên quan chặt chẽ đến Thách thức II và III, các gói AutoML hiện tại không được chuẩn bị tốt cho các yêu cầu điều chỉnh toàn bộ quá trình huấn luyện LLM vì chúng thường chỉ xem xét một mô hình học tập duy nhất tại một thời điểm. Việc huấn luyện LLM đặc biệt thách thức vì nó kết hợp các giai đoạn của self-supervised learning, supervised learning, và thậm chí reinforcement learning. Điều này có nghĩa là chúng ta cần các không gian thiết kế và cấu hình riêng biệt cho mỗi giai đoạn trong khi cuối cùng nhằm tối ưu hóa chung tất cả chúng (Hutter, 2022). Cho đến nay, các không gian cấu hình cho supervised classification learning đã xem xét hàng chục hoặc hơn một trăm quyết định thiết kế (Feurer et al., 2015b; Zimmer et al., 2021; Feurer et al., 2022). Tuy nhiên, cho đến nay, chưa biết cách chúng ta sẽ tối ưu hóa chung toàn bộ pipeline vì điều này đặt ra các thử thách phụ của (i) tối ưu hóa chung có thể hàng trăm quyết định thiết kế với (ii) một số chỉ báo hiệu suất trên (iii) một phân phối không biết của các nhiệm vụ downstream. Các cách để tiến lên với điều này sẽ bao gồm việc nghiên cứu tầm quan trọng và tương tác giữa các quyết định thiết kế để tìm các tập con khả thi và có thể độc lập của chúng, phát triển các tín hiệu hiệu suất proxy hoặc tối ưu hóa đa mục tiêu và tối ưu hóa qua nhiều nhiệm vụ downstream có thể.

Hơn nữa, LLM đa phương thức ngày càng được công nhận về tiềm năng của chúng trong việc nắm bắt các loại dữ liệu đa dạng như âm thanh hoặc hình ảnh, và vẫn chưa thấy cách tính đa phương thức này sẽ ảnh hưởng đến việc tối ưu hóa toàn bộ pipeline. Rất có thể, điều này sẽ tác động đến cách chúng ta áp dụng các phương pháp đa độ trung thực cho AutoML hiệu quả, ví dụ, cách chọn các loại độ trung thực thông tin như các tập con dữ liệu từ các phương thức khác nhau và huấn luyện một phần của các thành phần khác nhau. Hơn nữa, điều này sẽ làm bùng nổ thêm không gian cấu hình có thể vì tất cả các phương thức dữ liệu thường đi kèm với các tùy chọn thiết kế đặc biệt riêng của chúng.

### 2.6 Thách thức V: Xác định Kiến trúc Mạng nơ-ron cho LLM

Việc chọn một kiến trúc mạng nơ-ron phù hợp là một bước quan trọng, nhưng không tầm thường trong việc thiết kế các mạng nơ-ron sâu tiên tiến, bao gồm LLM (White et al., 2023). Hiện tại, điều này chủ yếu được thực hiện thủ công bởi một chuyên gia con người (Hutter, 2022). Trái ngược với các lựa chọn kiến trúc rời rạc được chế tác thủ công, Neural Architecture Search (NAS) nhằm giảm bớt nỗ lực thủ công này trong khi cung cấp tính linh hoạt lớn trong các lựa chọn được hỗ trợ bởi việc tự động hóa (một phần) (Elsken et al., 2019). Tuy nhiên, các phương pháp NAS hiện tại chưa tìm ra các kiến trúc tiên tiến hoặc các phần của chúng theo cách mà self-attention được tìm ra bởi các chuyên gia con người (Vaswani et al., 2017). Do đó, chúng có thể rất hữu ích trong việc tối ưu hóa các giai đoạn nhất định của vòng đời LLM, nhưng không thể được áp dụng nguyên bản mà không có, cùng với những thứ khác, một không gian tìm kiếm được thiết kế tốt và có thể thích ứng theo thời gian chứa các lựa chọn kiến trúc phù hợp – chưa kể đến những thách thức tính toán đã thảo luận trước đó.

Gần đây, những nỗ lực đầu tiên để điều chỉnh các phương pháp NAS hướng tới các mô hình dựa trên transformer (Chitty-Venkata et al., 2022) để đáp ứng nhu cầu cụ thể của LLM, đã được thực hiện. Chẳng hạn, (So et al., 2021) trình bày một chiến lược tìm kiếm được thiết kế cho các mô hình ngôn ngữ transformer, sử dụng một không gian tìm kiếm của các chương trình TensorFlow và evolution để tìm kiếm các mô hình. AutoBERT-Zero (Gao et al., 2021) tập trung cụ thể vào các mô hình BERT, giới thiệu một không gian tìm kiếm inter-layer chứa các phép toán self-attention và convolution để cung cấp tính linh hoạt trong việc học các dependency toàn cục và cục bộ ở các layer khác nhau, và đề xuất một chiến lược evolution operation-priority. Được thiết kế cho vision transformers, Autoformer (Chen et al., 2021) sử dụng một chiến lược huấn luyện supernet gọi là weight entanglement, entangling các trọng số của các block khác nhau trong cùng các layer trong quá trình huấn luyện supernet, và thực hiện một tìm kiếm evolutionary trên các supernet để tìm các transformer đầy hứa hẹn.

Cũng có một số đặc điểm đặc biệt mà chúng ta tính đến khi áp dụng NAS cho tinh chỉnh. Như Aghajanyan et al. (2021) chỉ ra, ít tham số hơn được yêu cầu cho tinh chỉnh so với tiền huấn luyện, và theo Lee et al. (2019), chỉ một số layer thực sự cần tinh chỉnh. Do đó, chúng ta sẽ hưởng lợi từ các phương pháp thông minh để chọn các layer yêu cầu tinh chỉnh và tập con của các tham số mô hình có thể học được chứa encoding liên quan đến nhiệm vụ tiếp theo. Ví dụ, chỉ cần điều chỉnh một mẫu ngẫu nhiên của các tham số mô hình (Aghajanyan et al., 2021) hoặc thêm các adapter layer sau các attention layer và chỉ điều chỉnh những layer đó (Houlsby et al., 2019) giảm hiệu quả số lượng tham số mô hình có thể học được cho tinh chỉnh. Một chiến lược khác trong vấn đề này là Low-Rank adaption, đóng băng các trọng số từ tiền huấn luyện và giới thiệu một thuật ngữ correction cho chúng, điều này giảm đáng kể số lượng tham số để huấn luyện (Hu et al., 2022a). Việc tận dụng các phương pháp AutoML để chọn và điều chỉnh tập con đúng của các tham số hoặc layer có thể học được và cấu hình thích hợp kiến trúc bổ sung được giới thiệu có thể chứng minh có giá trị cho hiệu suất downstream cũng như hiệu quả tham số.

Các kết quả thực nghiệm đầu tiên cho thấy rằng NAS thực sự là một hướng đầy hứa hẹn để hỗ trợ việc tinh chỉnh các mô hình ngôn ngữ (Chitty-Venkata et al., 2022). Chẳng hạn, AdaBERT (Chen et al., 2020) sử dụng NAS để tự động nén BERT cho một nhiệm vụ cụ thể và Mahabadi et al. (2021) huấn luyện một hypernetwork của các adapter layer với các tham số được chia sẻ. Do sự tương đồng với One-Shot NAS (Bender et al., 2018; Brock et al., 2018; Shi et al., 2020), sử dụng một super-network, các phương pháp để huấn luyện hyper- hoặc super-network có thể chuyển giao được sang việc tối ưu hóa tinh chỉnh LLM.

Khi nói đến việc scaling các mô hình, việc song song hóa của việc huấn luyện mô hình cơ sở là một khía cạnh thiết yếu và giới thiệu những thách thức mới trong quá trình thiết kế. Việc song song hóa gây ra những thay đổi thiết kế kiến trúc và siêu tham số trong mô hình, như xác định vị trí tối ưu của batch normalization hoặc residual connections để đảm bảo ổn định và ảnh hưởng đến tốc độ hội tụ và hiệu suất tổng thể (Shoeybi et al., 2019). Thừa nhận tác động đáng kể và tính cần thiết của việc song song hóa cho LLM, việc các phương pháp NAS nhận thức được song song trở nên quan trọng.

Các benchmark cho NAS đã tăng tốc sự phát triển của các thuật toán NAS mới và chiếm ưu thế trong tài liệu NAS vì lượng lớn kiến trúc đã được đánh giá (Ying et al., 2019; Dong & Yang, 2020; Mehta et al., 2022) và được mở rộng thêm bởi các interpolation surrogate đến nhiều kiến trúc hơn (Zela et al., 2022). Tuy nhiên, chúng tôi chỉ biết về một benchmark duy nhất của NAS cho NLP (Klyuchnikov et al., 2022) cho phép benchmarking hiệu quả của các phương pháp NAS khác nhau bằng cách đánh giá khoảng 14k kiến trúc khác nhau. Thật không may, những kiến trúc này dựa trên RNN và không dựa trên transformer hiện đại. Do đó, đây là một thách thức mở về cách cung cấp một benchmark có thể tái tạo và hiệu quả của NAS cho LLM cho cộng đồng. Để có tổng quan chung về các tính chất mong muốn của các benchmark NAS, chúng tôi tham khảo Lindauer & Hutter (2020).

## 3 LLM cho AutoML

Tại thời điểm này, có vẻ như LLM có tiềm năng gây gián đoạn xã hội của chúng ta từ nhiều góc độ khác nhau, ví dụ, trong giáo dục (Kasneci et al., 2023), y học (Alberts et al., 2023), lập trình (Dakhel et al., 2023), hoặc trong luật pháp (Noonan, 2023). Như chúng tôi minh họa trong Hình 2 và trình bày chi tiết trong các phần sau, chúng tôi kỳ vọng rằng LLM cũng sẽ gây gián đoạn AutoML từ nhiều góc độ khác nhau:

(i) Tương tác với các hệ thống phức tạp như một hệ thống AutoML thường thách thức đối với người dùng không chuyên. Khả năng NLP đáng chú ý của LLM mang lại cơ hội để thiết kế lại một cách căn bản cách con người tương tác với các hệ thống AutoML cả từ quan điểm thiết lập chúng và diễn giải đầu ra của chúng.

(ii) Để giải phóng toàn bộ tiềm năng của chúng, các hệ thống AutoML phải được cấu hình một cách thích hợp thường đòi hỏi một chuyên gia. Khả năng chưng cất tri thức của LLM mang lại cơ hội để đề xuất một cấu hình ban đầu tốt của một hệ thống AutoML cho một vấn đề cụ thể hiện tại.

(iii) Các hệ thống AutoML tận dụng một số thành phần phụ như một neural performance predictor trong nhiều công cụ NAS. Như công việc đầu tiên cho thấy, những thành phần này có thể được thay thế bằng LLM hoạt động như các phiên bản meta-learned của các thành phần tương ứng.

### 3.1 Cơ hội I: Cải thiện Tương tác Người-Máy với LLM

Vì ngôn ngữ tự nhiên luôn là nền tảng của giao tiếp con người, những tiến bộ trong NLP đã dẫn đến sự gia tăng số lượng chatbot trong các ứng dụng khác nhau trong thập kỷ qua (Adamopoulou & Moussiades, 2020). Tuy nhiên, cho đến gần đây, hầu hết những chatbot này đã khá hạn chế trong khả năng của chúng. Như ChatGPT (OpenAI, 2022) cho thấy, LLM có tiềm năng giảm bớt tình trạng này bằng cách cho phép các chatbot mạnh mẽ hơn đáng kể và tương tác văn bản tốt hơn với người dùng. Trong bối cảnh AutoML, chúng tôi dự báo hai hướng đầy hứa hẹn: Tận dụng LLM (i) như một giao diện thân thiện với người dùng cho AutoML, và (ii) để cải thiện khả năng diễn giải của quá trình AutoML.

#### 3.1.1 Cơ hội I.a: LLM như một Giao diện cho AutoML

Lời hứa ban đầu của AutoML là nó có thể tự động hóa một số nhiệm vụ của một nhà khoa học dữ liệu ở mức độ lớn và do đó dân chủ hóa machine learning bằng cách cho phép các chuyên gia domain có ít hoặc không có kiến thức ML áp dụng ML cho domain của họ. Tuy nhiên, bất chấp thành công của chúng, nhiều công cụ AutoML hiện tại không được xây dựng xung quanh người dùng, mà thay vào đó xung quanh các ý tưởng thuật toán. Đặc biệt, hầu hết những công cụ này cho phép tương tác rất hạn chế với người dùng trong thực tế. Điều này cũng được phản ánh trong sự miễn cưỡng của nhiều nhà nghiên cứu khi sử dụng các công cụ AutoML (van der Blom et al., 2021). Kết quả là, các phần của cộng đồng đã thúc đẩy hướng tới một quá trình AutoML tập trung con người hơn nhằm hỗ trợ nhà khoa học dữ liệu sao cho họ có thể làm việc hiệu quả hơn (Lindauer & Tornede, 2022; Pfisterer et al., 2019). Tương ứng, AutoML có thể được xem có hai nhóm mục tiêu chính: (i) Các chuyên gia domain có ít kiến thức ML muốn áp dụng ML có sẵn cho vấn đề của họ, và (ii) Các chuyên gia ML muốn cải thiện workflow của họ với các công cụ tự động giữ họ trong vòng lặp. Ngay bây giờ, hầu hết các công cụ AutoML đòi hỏi coding hoặc ít nhất một số hiểu biết kỹ thuật và, về mặt khả năng sử dụng, nhắm mục tiêu nhóm thứ hai nhiều hơn nhóm đầu tiên.

LLM cho phép chúng ta suy nghĩ lại một cách căn bản cách mọi người tương tác với các hệ thống AutoML và, đặc biệt, giúp chúng ta thiết kế các giao diện tương tác dựa trên văn bản mạnh mẽ như chatbot. Những giao diện này có thể trích xuất các yêu cầu của người dùng qua một cuộc trò chuyện một cách lặp lại và, trong nền, cấu hình một hệ thống AutoML tương ứng (xem thêm Phần 3.2) dựa trên các thực hành tốt nhất ML và tri thức về các lần chạy tối ưu hóa trên các tập dữ liệu tương tự được mã hóa trong LLM. Đặc biệt, những hệ thống tương tác như vậy có thể đơn giản hóa nhiều quyết định thiết kế phức tạp ảnh hưởng đến quá trình AutoML. Ví dụ, việc chọn một metric phù hợp để tối ưu hóa có thể thách thức đối với một người không chuyên, nhưng có thể thực hiện được với một chatbot tương tác thành thạo đặt câu hỏi hướng dẫn người dùng chọn đúng metric. Tự nhiên, điều này cũng mang nguy cơ cấu hình AutoML sai (xem Phần 4).

Các phần của tầm nhìn về trợ lý AutoML đối thoại này đã là hiện thực với các công cụ trợ lý coding dựa trên AI như GitHub Copilot (Friedman, 2021). Chúng có thể tạo ra và đề xuất code để chạy AutoML với thông tin ngữ cảnh mà người dùng cung cấp. Qua đó, các trợ lý coding dựa trên AI đã hỗ trợ người dùng trong việc tìm các instantiation AutoML cụ thể phù hợp nhất với yêu cầu và thiết bị hiện có. Tuy nhiên, chúng tôi hình dung các trợ lý được điều chỉnh nhiều hơn cho nhu cầu của các nhà thực hành ML.

#### 3.1.2 Cơ hội I.b: Khả năng Diễn giải của Quá trình AutoML

Đã có sự gia tăng gần đây trong các phương pháp cố gắng đóng góp vào mô hình AutoML tập trung con người đã nêu (Pfisterer et al., 2019; Lindauer & Tornede, 2022; Moosbauer et al., 2021; 2022b; Segel et al., 2023) bằng cách đề xuất các ý tưởng để cải thiện tính tương tác với người dùng và khả năng diễn giải của quá trình AutoML. Tuy nhiên, nhiều công trình này thích ứng các phương pháp machine learning có thể diễn giải khá cổ điển vào thiết lập AutoML, có kết quả có thể vẫn khá phức tạp để hiểu đối với những người không chuyên và không cung cấp bất kỳ giải thích văn bản nào dễ hiểu.

LLM có tiềm năng tăng đáng kể tính thân thiện với người dùng của những diễn giải bằng cách trình bày chi tiết về chúng dưới dạng văn bản. Đặc biệt, một LLM được khởi tạo với lịch sử các cấu hình hoặc pipeline đã được đánh giá, ví dụ như lịch sử chạy từ một optimizer như SMAC (Lindauer et al., 2022), Hyperopt (Komer et al., 2014) hoặc BoTorch (Balandat et al., 2020), như ngữ cảnh có thể giúp tạo ra một báo cáo tối ưu hóa văn bản trình bày chi tiết về kết quả AutoML cuối cùng và chi tiết của chính quá trình. Lý tưởng nhất, LLM có thể được bối cảnh hóa thêm với kết quả của một số phương pháp ixAutoML như một nền tảng mạnh cho báo cáo. Điều này thậm chí có thể bao gồm hình ảnh, như partial dependence plots (Moosbauer et al., 2021), vì LLM đa phương thức cho phép xử lý hình ảnh cũng như (Li et al., 2023; Zhang et al., 2023b). Nếu người dùng có câu hỏi về báo cáo hoặc các yếu tố không được báo cáo bao gồm, LLM cũng có thể được sử dụng như một phần của chatbot để trả lời những câu hỏi đó. Mặc dù có một cuộc thảo luận đang diễn ra trong cộng đồng về những gì tạo nên một giải thích tốt nói chung, các giải thích văn bản dường như được người dùng tin tưởng cao (Gilpin et al., 2018).

### 3.2 Cơ hội II: LLM để Cấu hình AutoML

Để tỏa sáng đầy đủ, một hệ thống AutoML phải được thiết lập chính xác, bao gồm một cấu hình hệ thống phù hợp cho nhiệm vụ hiện tại. Đi xa hơn nữa, việc chọn lựa giữa các công cụ AutoML khác nhau (Thornton et al., 2013; Feurer et al., 2015a; Akiba et al., 2019; Jin et al., 2019; Erickson et al., 2020; Zimmer et al., 2021) có thể quan trọng tùy thuộc vào vấn đề và phần cứng hiện có. Mặc dù tồn tại công việc theo hướng loại bỏ gánh nặng chọn và cấu hình một hệ thống AutoML (Feurer & Hutter, 2018; Tornede et al., 2022; Feurer et al., 2022; Moosbauer et al., 2022a), việc thiết lập một hệ thống AutoML mà không có chuyên gia vẫn có thể thách thức. LLM mang lại cơ hội tuyệt vời để cải thiện thêm tình hình này bằng cách đề xuất một cấu hình tốt cho nhiệm vụ hiện tại. Dưới đây và trong Hình 3, chúng tôi nêu một số tùy chọn cấu hình, thường rất quan trọng nhưng khó chọn chính xác, ngay cả đối với các chuyên gia.

Phần lớn các công cụ AutoML đòi hỏi cấu hình của một không gian tìm kiếm các ứng viên mà từ đó các giải pháp có thể được rút ra. Tùy thuộc vào ứng dụng cụ thể, điều này thường bao trùm một số siêu tham số số, phân loại và thứ tự – thường với các dependency giữa chúng. Việc khởi tạo cụ thể của không gian tìm kiếm này là quan trọng để tìm ra một pipeline hoạt động tốt nhanh chóng nhưng cũng khó thiết lập ngay cả đối với một chuyên gia. Đặc biệt, loại giải pháp nào (ví dụ pipeline có hoặc không có tiền xử lý) phù hợp, hoặc siêu tham số nào để điều chỉnh và domain cụ thể của chúng, quan trọng để cấu hình chính xác. Ví dụ, việc chọn domain của một siêu tham số nhỏ có thể có lợi cho tốc độ tìm kiếm, nhưng cũng mang nguy cơ bỏ lỡ giá trị thực sự tối ưu. Cho đến nay, vấn đề này hoặc được giải quyết bởi một chuyên gia cấu hình cẩn thận công cụ AutoML, bằng các phương pháp điều chỉnh thích ứng không gian tìm kiếm trong quá trình tối ưu hóa (Wistuba et al., 2015; Nguyen et al., 2019; Hu et al., 2022b; 2020; Chen et al., 2019) hoặc bằng cách tích hợp tri thức chuyên gia con người như thông tin prior hướng dẫn tìm kiếm (Souza et al., 2021; Hvarfner et al., 2022; Mallik et al., 2022).

Tương tự quan trọng – đặc biệt từ góc độ Green AutoML (Tornede et al., 2023) – là câu hỏi chạy quá trình AutoML trong bao lâu. Việc chọn thời gian chạy quá dài có thể lãng phí cả thời gian và tài nguyên, trong khi thời gian chạy quá ngắn mang nguy cơ bỏ lỡ các giải pháp tốt. LLM có thể cung cấp các thiết lập đầu tiên cho thời gian chạy tối đa của các công cụ AutoML dựa trên kinh nghiệm từ các nhà thực hành AutoML khác. Đi xa hơn một bước, chúng ta thậm chí có thể tận dụng tri thức được mã hóa trong một LLM để kiểm tra xem liệu quá trình chạy AutoML đã hội tụ hay liệu việc điều chỉnh thêm vẫn có thể đầy hứa hẹn. Công việc gần đây của Makarova et al. (2022) đã chỉ ra rằng một sự kết thúc tự động như vậy về nguyên tắc có thể, ngay cả không có LLM, và cho phép tiết kiệm tài nguyên đáng kể. Về nguyên tắc, chúng ta thậm chí có thể tưởng tượng rằng với lượng nghiên cứu AutoML đầy đủ với chi tiết về không gian cấu hình đang được điều chỉnh, một LLM cũng có thể học cách sử dụng bao nhiêu ngân sách AutoML (ví dụ, về số lượng cấu hình được đánh giá) thường được sử dụng cho các không gian cấu hình khác nhau để đạt được một kết quả mạnh.

Cuối cùng, LLM có thể được sử dụng để cấu hình việc sử dụng các phương pháp đa độ trung thực, như Hyperband (Li et al., 2018). Như công việc gần đây cho thấy, hiệu suất của các phương pháp đa độ trung thực bị ảnh hưởng bởi việc chọn các loại độ trung thực (Deng et al., 2022) và lượng ngân sách tối thiểu/tối đa (Bohdal et al., 2023). Tương ứng, việc chọn đúng những điều này là quan trọng nhưng cũng khó trong thực tế, làm cho một đề xuất tự động rất hữu ích.

### 3.3 Cơ hội III: LLM như Thành phần của Hệ thống AutoML

Hầu hết các hệ thống AutoML là những công cụ phức tạp với rất nhiều hệ thống phụ và thành phần phục vụ các mục đích khác nhau, như ước tính hiệu suất của một pipeline (White et al., 2021), ước tính thời gian chạy của một pipeline (Mohr et al., 2021), hoặc chọn pipeline tiếp theo để đánh giá. LLM mang lại cơ hội để thay thế nhiều hệ thống phụ này như một phiên bản meta-learned của chúng. Dưới đây và trong Hình 3 chúng tôi nêu và giới thiệu một số cơ hội như vậy.

Hầu như tất cả các hệ thống AutoML đều tận dụng một số hình thức chiến lược chọn ứng viên giải pháp chọn ứng viên tiếp theo để được đánh giá như một hàm acquisition trong các hệ thống dựa trên BO. Khả năng mô hình hóa tri thức của LLM và truy cập của chúng vào lượng meta-data khổng lồ về các lần chạy ML và AutoML mang lại cơ hội để thay thế những chiến lược chọn chủ yếu được thiết kế thủ công này bằng một phiên bản meta-learned dưới dạng LLM. Các công việc đầu tiên theo hướng này đầy hứa hẹn: GPT-NAS (Yu et al., 2023a) tận dụng một mô hình GPT để dự đoán (các phần) của một kiến trúc mạng nơ-ron, tức là, một ứng viên giải pháp dựa trên một encoding được sử dụng để phát triển kiến trúc. GENIUS (Zheng et al., 2023) thậm chí đi xa hơn một bước và thay thế toàn bộ bước đề xuất kiến trúc bằng GPT-4. Nó prompting GPT-4 một cách lặp lại cho một kiến trúc, mà nó đánh giá, và re-prompt GPT-4 với hiệu suất yêu cầu một kiến trúc tốt hơn. Quá trình này được tiếp tục cho đến khi đạt được một tiêu chí dừng. Tương tự, EvoPrompting (Chen et al., 2023) tận dụng một LLM để thực hiện toán tử crossover và mutation trong một phương pháp NAS evolutionary. Cũng vậy, Nasir et al. (2023) đề xuất tận dụng LLM để tạo ra các cá thể trong một phương pháp NAS evolutionary.

Hơn nữa, tri thức được mã hóa trong LLM cũng có thể được sử dụng trong feature engineering, như được chứng minh gần đây. CAAFE (Hollmann et al., 2023b) là một phương pháp feature engineering cho các tập dữ liệu bảng tận dụng một LLM để đề xuất code bổ sung một cách lặp lại cho feature engineering dựa trên mô tả của chúng và phản hồi lại việc đánh giá của đoạn code này cho LLM. Điều này đóng góp vượt ra ngoài AutoML đến mục tiêu cuối cùng của automated data science (Bie et al., 2022).

Hơn nữa, hầu hết các công cụ AutoML đánh giá lặp lại các ứng viên giải pháp mới bằng cách huấn luyện và validation. Tự nhiên, đây là một quá trình tốn thời gian, đặc biệt nếu thời gian huấn luyện của ứng viên giải pháp cực kỳ dài, như trong deep learning. Vì lý do này, một số hệ thống tận dụng các estimator hiệu suất meta-learned như các surrogate model meta-learned trong Bayesian optimization (Vanschoren, 2019) hoặc neural performance predictor trong NAS (White et al., 2021) có thể thay thế một số đánh giá. Một lần nữa, LLM mang lại cơ hội tuyệt vời để phục vụ như một dạng đặc biệt của thay thế meta-learned dựa trên tri thức được trích xuất từ lượng lớn dữ liệu không có cấu trúc, mà không thể tiếp cận được với các phương pháp meta-learned tiêu chuẩn. Chúng cũng có thể phục vụ như một cơ sở để tạo dữ liệu huấn luyện cho các estimator hiệu suất/thời gian huấn luyện đơn giản hơn hoặc surrogate model và có thể thay thế cả hai. Chen et al. (2022) đã chứng minh rằng phương pháp OptFormer của họ có thể học cả một surrogate model cạnh tranh và một hàm acquisition từ lịch sử tối ưu hóa của nền tảng Vizier (Song et al., 2022) của Google. Tương tự, Liu et al. (2024) nhấn mạnh rằng phương pháp LLMABO của họ có thể được sử dụng để cải thiện nhiều thành phần AutoML khác nhau như warmstarting, surrogate modeling và candidate sampling.

Đi xa hơn nữa, cả Zhang et al. (2023d) và Zhang et al. (2023a) đều đề xuất AutoML-GPT và MLCopilot, tương ứng, hoạt động hoàn toàn như một công cụ AutoML zero-shot độc lập. Với một mô tả vấn đề văn bản từ người dùng và một knowledge base trong nền, chúng đề xuất một pipeline và/hoặc quy trình huấn luyện để đạt được hiệu suất tốt trên vấn đề cụ thể. Vấn đề tự nó có thể khác nhau, nhưng chứa ít nhất một mô tả của tập dữ liệu mà một mô hình được tìm kiếm, một số dạng mô tả không gian tìm kiếm như một nguồn của mô hình được tìm kiếm và một mô tả về cách hiệu suất được đánh giá. Không gian tìm kiếm có thể là một lớp mô hình cụ thể hoặc thậm chí một kiến trúc cụ thể sao cho LLM cần tìm một pipeline hoàn chỉnh hoặc chỉ siêu tham số tốt (Zhang et al., 2023d). Lưu ý rằng những hệ thống này không bao giờ đánh giá một pipeline ML duy nhất, nhưng, trong trường hợp của AutoML-GPT, chỉ sử dụng LLM để mô phỏng toàn bộ quá trình AutoML. Tsai et al. (2023) thực hiện một phương pháp hơi khác bằng cách sử dụng một LLM để tạo code thực hiện các nhiệm vụ AutoML cụ thể trong một quá trình lặp lại được kiểm soát bởi một LLM khác nhận kết quả thực hiện và điều chỉnh prompt cho code LLM tương ứng. Tương tự, Aliro (Choi et al., 2023) cung cấp một công cụ AutoML được điều chỉnh hướng tới dữ liệu y tế trong đó LLM được sử dụng trong nền để nhanh chóng tạo code cho các phân tích cụ thể cho người dùng và trường hợp.

Tri thức được mã hóa trong LLM có thể, về nguyên tắc, được rút ra từ vô số nguồn dữ liệu, bao gồm nhưng không giới hạn ở các bài báo học thuật, các cuộc thi khoa học dữ liệu và ML (ví dụ, Kaggle), các nền tảng benchmark ML (ví dụ, OpenML (Casalicchio et al., 2019)), các benchmark AutoML, hoặc các lần chạy AutoML với hiệu suất tương ứng của chúng. Sự gia tăng khả năng tiếp cận của dữ liệu và code mã nguồn mở có thể trang bị cho LLM khả năng nắm bắt mối quan hệ phức tạp giữa các lựa chọn kiến trúc, siêu tham số, kết quả hiệu suất, hoặc thời gian chạy. Dữ liệu như vậy có thể được sử dụng để điều chỉnh LLM cho các nhiệm vụ cụ thể, như được thực hiện ví dụ trong GPT-NAS (Yu et al., 2023a), được tiền huấn luyện trên tập dữ liệu NAS-Bench-101 (Ying et al., 2019) và được tinh chỉnh trên một tập dữ liệu áp dụng 36 kiến trúc mạng nơ-ron phổ biến hiện tại. Tuy nhiên, thành công của LLM trong việc thực hiện NAS (Zheng et al., 2023) hoặc đề xuất một pipeline AutoML hoàn chỉnh (Zhang et al., 2023a;d), ngay cả không có tinh chỉnh, cho thấy rằng các mô hình có thể tổng quát hóa hiểu biết của chúng từ kho tàng thông tin có sẵn.

Tất cả các ví dụ trên phụ thuộc quan trọng vào cách các prompt cho LLM được thiết kế và cách tri thức thu được cho đến nay được thêm như ngữ cảnh vào các prompt. Ví dụ, EvoPrompting (Chen et al., 2023) thêm code của tất cả các kiến trúc đã được đánh giá trước đó cùng với kết quả đánh giá của chúng như một annotation vào prompt như ngữ cảnh. Tự nhiên, điều này có thể nhanh chóng dẫn đến các prompt rất lớn, có thể thách thức đối với LLM hiện tại, mặc dù công việc gần đây cố gắng giảm bớt giới hạn kích thước prompt (Yu et al., 2023b).

## 4 Rủi ro

Bên cạnh những cách có giá trị để tích hợp LLM và AutoML, sự kết hợp này đặt ra một số rủi ro. Dưới đây, chúng tôi trình bày năm rủi ro kết hợp LLM và AutoML:

(i) Việc sử dụng LLM để cấu hình AutoML đòi hỏi trích xuất meta-knowledge về các nhiệm vụ AutoML để được đưa vào LLM. Do đó, prompt engineering tốn thời gian con người và tập trung để trích xuất tri thức đó một cách đáng tin cậy.

(ii) Việc sử dụng dữ liệu có sẵn công khai để đánh giá các phương pháp AutoML được cấu hình thông qua LLM có thể là data snooping, vì LLM có thể đã thấy dữ liệu trong quá trình huấn luyện. Việc phát hiện tri thức trước của LLM về một tập dữ liệu nhất định trở nên ngày càng quan trọng.

(iii) LLM được biết là ảo giác từ thời gian này sang thời gian khác, trình bày những sự thật sai do thiếu fact-checking. Điều này có thể quan trọng khi LLM được sử dụng để cấu hình AutoML vì các cấu hình không phù hợp có thể được sử dụng.

(iv) Tương tác full-text của LLM dường như khá đáng tin cậy đối với người bình thường, mặc dù nội dung được trình bày có thể hoàn toàn không đúng. Kết hợp với AutoML, người dùng có thể nhầm lẫn có ấn tượng tích cực về kết quả.

(v) Việc kết hợp hai lĩnh vực nghiên cứu tốn nhiều tài nguyên, tức là LLM và AutoML, sẽ dẫn đến một lĩnh vực nghiên cứu tốn nhiều tài nguyên hơn nữa. Do đó, việc minh bạch về tài nguyên được sử dụng và tìm cách hiệu quả hơn trở nên quan trọng hơn.

### 4.1 Rủi ro I: Tương tác Người phức tạp

Chúng tôi đã tận dụng giả định làm việc rằng LLM được huấn luyện trên một tập ngữ liệu lớn văn bản cũng mã hóa tri thức về các nhiệm vụ AutoML nhiều lần trong bản thảo này. Tri thức như vậy có thể bao gồm hiểu biết chung về những mô hình nào hoạt động tốt với mô tả tập dữ liệu hoặc preprocessing nào được yêu cầu cho một nhiệm vụ cụ thể hiện tại. Tuy nhiên, việc trích xuất tri thức như vậy cho người dùng một cách đáng tin cậy phụ thuộc vào prompt cụ thể, dẫn đến nhiệm vụ prompt engineering cho AutoML (Sorensen et al., 2022). Lý tưởng nhất, tương tác người-máy sẽ làm cho việc thiết kế một prompt cụ thể trở nên lỗi thời bằng cách tự động xây dựng nó một cách tự động từ một tương tác giống chat với người dùng. Tuy nhiên, điều này có thể chỉ hoạt động đến một mức độ nhất định và hiện tại vẫn là một vấn đề mở. Tương ứng, chúng ta có nguy cơ rằng bằng cách tận dụng LLM cho AutoML, chúng ta chỉ chuyển rào cản gia nhập từ yêu cầu coding và kiến thức ML sang kiến thức prompt engineering.

### 4.2 Rủi ro II: Đánh giá

Bất kỳ hình thức đánh giá nào của một phương pháp AutoML tận dụng LLM trong một số thành phần cần được thực hiện cẩn thận. Vì LLM được huấn luyện trên lượng dữ liệu không có cấu trúc cực lớn, rất có thể chúng thấy nhiều tập dữ liệu ML có sẵn công khai trong quá trình huấn luyện bao gồm dữ liệu test tương ứng hoặc một phiên bản cô đọng của nó, như một mô hình được huấn luyện trên dữ liệu. Do đó, việc đánh giá một pipeline AutoML sử dụng LLM trên một tập dữ liệu là một phần của dữ liệu huấn luyện của LLM cơ bản giống với data snooping (Kok, 1984). Tương ứng, kết quả đánh giá sẽ bị thiên vị mạnh. Chúng tôi thấy hai chiến lược chính để vượt qua vấn đề này: Thứ nhất, chúng ta có thể đánh giá nó trên các tập dữ liệu được đảm bảo không được LLM thấy trong quá trình huấn luyện, như dữ liệu được tạo sau khi LLM đã được huấn luyện (Faggioli et al., 2023). Tự nhiên, điều này đặt ra câu hỏi về cách lấy những tập dữ liệu riêng tư như vậy, vẫn có thể tiếp cận tự do cho các nhà nghiên cứu khác, có bản chất thực tế để so sánh các phương pháp, nhưng vẫn không được LLM sử dụng trước. Thứ hai, chúng ta có thể cố gắng tinh chỉnh một LLM để loại bỏ bất kỳ tri thức nào về một tập dữ liệu khỏi mô hình của nó (Yang et al., 2022). Để đạt được điều này, chúng ta cần biết cách phát hiện tri thức trước của LLM về bất kỳ tập dữ liệu được sử dụng nào hoặc một phiên bản cô đọng của nó. Một khi tri thức như vậy có thể phát hiện được, chúng ta cần tìm cách loại bỏ nó một cách đáng tin cậy thông qua tinh chỉnh. Tuy nhiên, tại thời điểm này, không có phương pháp nào đảm bảo rằng tri thức trước về một tập dữ liệu có thể được loại bỏ hoàn toàn. Nhìn chung, việc đánh giá một hệ thống AutoML có LLM còn xa mới tầm thường và đòi hỏi các giao thức mới.

### 4.3 Rủi ro III: Sự thật Sai và Lạm dụng

LLM được biết là tạo ra đầu ra nghe có vẻ tự tin nhưng có tri thức ảo giác (Ji et al., 2023), có thể không thể phân biệt hoặc ít nhất khó phân biệt với sự thật trong một số trường hợp. Tương ứng, chúng ta có thể tự hỏi liệu bất kỳ việc sử dụng LLM nào trong các hệ thống AutoML có thể dẫn đến kết quả có hại, ví dụ, khi cấu hình sai AutoML thông qua LLM bởi người dùng sao cho hầu như không có thời gian chạy nào được cấp, hoặc LLM đưa ra quyết định như chọn không gian tìm kiếm có thể không phù hợp cho vấn đề nhất định. Một giải pháp tiềm năng cho điều này có thể là việc tích hợp với một knowledge graph (Hogan et al., 2022) được điều chỉnh hướng tới AutoML có thể được sử dụng như một knowledge base để bối cảnh hóa các đề xuất LLM tiềm năng. Hơn nữa, chúng ta nên xem xét các bộ lọc hoặc safeguard cho các giải pháp suy thoái được xuất ra bởi LLM và giải quyết các mặc định được định nghĩa trước trong những trường hợp đó.

Nói chung, khi tích hợp LLM với các hệ thống AutoML, chúng ta nên ghi nhớ tiềm năng cho đầu ra LLM sai và cách đầu ra như vậy có thể được phát hiện trong một phương pháp xác suất. Một lợi thế lớn so với nhiều discipline khác là chúng ta có thể validation xem liệu các phản hồi của LLM có đúng hay không bằng cách đơn giản chạy một thí nghiệm và so sánh kết quả thí nghiệm được xuất ra bởi LLM và kết quả chúng ta thu được từ lần chạy (Zheng et al., 2023; Chen et al., 2023; Hollmann et al., 2023b).

### 4.4 Rủi ro IV: Niềm tin và Giải thích

Mặc dù việc cải thiện tương tác người-máy của các hệ thống AutoML tận dụng LLM (Phần 3.1) có tiềm năng dân chủ hóa ML hơn nữa, nó cũng mang nguy hiểm khi nhiều người bình thường hơn tương tác với những hệ thống như vậy. Ví dụ, do tương tác full-text với người dùng, có thể có rất nhiều niềm tin vào kết quả, mặc dù mô hình machine learning được trả về có thể hoàn toàn không phù hợp cho vấn đề nhất định. Tương ứng, sự chú ý của người dùng nên được hướng đến các sự mơ hồ tiềm năng trong tương tác, và họ nên được nhận thức rõ về các giả định mà hệ thống tổng thể đưa ra do tương tác văn bản. Một cách để giải quyết vấn đề này có thể là thêm các phương pháp giải thích thêm nói với người dùng cách các phản hồi nhất định được tạo ra (Deb et al., 2023).

### 4.5 Rủi ro V: Tiêu thụ Tài nguyên

Cả LLM và AutoML đều là các lĩnh vực nghiên cứu tốn nhiều tài nguyên riêng lẻ. Việc kết hợp chúng có thể dẫn đến tiêu thụ tài nguyên cao hơn nữa, đặt ra câu hỏi liệu việc chi tiêu những tài nguyên này có đáng giá hay không. Xét đến số lượng ngày càng tăng của LLM được tinh chỉnh cho các ứng dụng y tế (Han et al., 2023; Wu et al., 2023a), rất có thể cũng sẽ có LLM được tinh chỉnh cho các nhiệm vụ AutoML chuyên môn. Với tiềm năng dễ dàng tạo ra ngày càng nhiều meta-knowledge cho AutoML bằng cách đơn giản đánh giá thêm các pipeline ML hoặc cấu hình siêu tham số, nó thậm chí có thể dẫn đến một cuộc đua của các nhiệm vụ được tinh chỉnh lặp lại.⁶ Tuy nhiên, meta-knowledge có thể phong phú được lưu trữ trong những LLM này (xem Phần 3.2) cũng có thể dẫn đến những lợi ích hiệu quả mới trong AutoML. Nhìn chung, theo tinh thần Green AutoML (Tornede et al., 2023), việc minh bạch về tài nguyên được sử dụng khi tiến hành nghiên cứu tại giao điểm này sẽ trở nên quan trọng hơn nữa, và tìm cách hiệu quả hơn ở cả hai phía.

## 5 Kết luận

LLM đã bắt đầu có tác động lớn đến AutoML, cả về cách AutoML được thiết kế cho LLM, và cách LLM có thể được tận dụng cho AutoML. Tuy nhiên, chúng ta chỉ mới ở đầu một hành trình dài để giải quyết nhiều thách thức và tận dụng các cơ hội trong khi xem xét những rủi ro phía trước. Chúng tôi tin tưởng mạnh mẽ rằng chúng ta, như một cộng đồng, sẽ vượt qua những thách thức đó và tận hưởng những cơ hội đáng chú ý mà việc tích hợp LLM và AutoML mang lại.

Thách thức lớn nhất của tất cả có thể được tóm gọn thành một sự thật: Việc huấn luyện và bảo trì các LLM có khả năng nhất đòi hỏi tài nguyên khổng lồ, sao cho chỉ một vài nhóm trên toàn thế giới có thể cung cấp những hệ thống đó. Điều này bao hàm một số vấn đề cho cộng đồng: (i) Chúng ta không dễ dàng có thể nghiên cứu cách AutoML có thể được điều chỉnh hướng tới việc huấn luyện các mô hình cơ sở LLM; (ii) Chúng ta không thể dễ dàng kiểm tra dữ liệu nào được sử dụng để huấn luyện các mô hình mà mang rủi ro trong việc sử dụng và đánh giá LLM cho AutoML; (iii) Chúng ta không thể dễ dàng thêm safeguard chống lại việc lạm dụng LLM và AutoML. Tương ứng, vấn đề hơn nữa là trách nhiệm và kiểm soát đối với LLM nằm trong những bàn tay ít ỏi đó của chủ yếu các công ty tư nhân. Xét đến điều này, chúng tôi tin rằng một trong những bước tiếp theo phải là phát triển một LLM mã nguồn mở với các mục tiêu cụ thể của AutoML trong tâm trí.

## Lời cảm ơn

Chúng tôi muốn cảm ơn cộng đồng AutoML rộng lớn hơn cho các quan điểm khác nhau đã ảnh hưởng đến công trình này. Chúng tôi muốn đề cập đến các cuộc thảo luận khác nhau với supergroup AutoML.org, cuộc thảo luận panel AutoML 2022 về "AutoML in the age of large pretrained model" và các phiên breakout tại một workshop AutoML 2023 nhỏ tại Paris về "AutoML & LLMs". Thực sự, tiêu đề bài báo của chúng tôi đã được lấy cảm hứng từ những sự kiện này. Đặc biệt, chúng tôi thừa nhận (theo thứ tự bảng chữ cái): Steven Adriaensen, Deebadepta Dey, Carola Doerr, Katharina Eggensperger, Matthias Feurer, Neil Houlsby, Frank Hutter, Arjun Krishnakumar, Neeratyoy Mallik, Samuel Müller, Raghu Rajan, Elena Raponi, Zi Wang và Marc Zöller.

Alexander Tornede, Sarah Segel và Marius Lindauer thừa nhận nguồn tài trợ từ Liên minh Châu Âu (ERC, "ixAutoML", số grant 101041029). Quan điểm và ý kiến được bày tỏ tuy nhiên chỉ là của (các) tác giả và không nhất thiết phản ánh quan điểm của Liên minh Châu Âu hoặc Cơ quan Điều hành Hội đồng Nghiên cứu Châu Âu. Cả Liên minh Châu Âu và cơ quan cấp grant đều không thể chịu trách nhiệm về chúng. Tanja Tornede thừa nhận hỗ trợ tài chính trong dự án GreenAutoML4FAS (số 67KI32007A) và Daphne Theodorakopoulos trong dự án ZuSiNa (số 67KI21009A), cả hai đều được tài trợ bởi Bộ Môi trường, Bảo tồn Thiên nhiên, An toàn Hạt nhân và Bảo vệ Người tiêu dùng Liên bang Đức. Tim Ruhkopf thừa nhận hỗ trợ tài chính từ Bộ Kinh tế và Năng lượng Liên bang của Đức trong dự án CoyPu (số 01MK21007L).

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên như trong bản gốc do độ dài và tính chuyên môn cao]

---

*¹ Chúng tôi lưu ý rằng không có ngưỡng rõ ràng nào sau đó một mô hình ngôn ngữ được gọi là lớn.*

*² Chúng tôi lưu ý rằng cũng tồn tại công việc trong giao điểm rộng hơn của AutoML và các mô hình đã được tiền huấn luyện (Hollmann et al., 2023a; Müller et al., 2023) nằm ngoài phạm vi của bài báo này vì chúng tôi chỉ tập trung vào LLM (đã được tiền huấn luyện) chứ không phải các mô hình đã được tiền huấn luyện nói chung.*

*³ Chúng tôi lưu ý rằng có những phương pháp gần đây nhằm thay thế reinforcement learning bằng các phương pháp có giám sát như classification losses (Rafailov et al., 2023). Hơn nữa, nói chung instruction tuning bao gồm một loạt các kỹ thuật như self-instruct (Wang et al., 2023b).*

*⁴ Chúng tôi lưu ý rằng một số optimizer khuyến nghị 10 lần số siêu tham số làm mẫu chỉ cho thiết kế ban đầu trước khi bắt đầu tối ưu hóa thực sự.*

*⁵ Chúng tôi lưu ý rằng có những phương pháp gần đây nhằm thay thế reinforcement learning bằng các phương pháp có giám sát, như classification losses (Rafailov et al., 2023).*

*⁶ Chúng tôi lưu ý rằng trái ngược với các nhiệm vụ NLP thực sự mà đầu vào của con người được yêu cầu, việc tạo ra các ví dụ mới về cách các pipeline ML hoặc hệ thống AutoML hoạt động và thực hiện dễ dàng hơn nhiều.*
