# 2306.05785.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2306.05785.pdf
# Kích thước tệp: 936584 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Nén Mạng Nơ-ron Đầu-cuối-đầu qua Surrogate Độ Trễ Chính Quy ℓ1
ℓ2
Anshul Nasery
Google Research India
anshulnasery@google.comHardik Shah
Google Research India
hardiknshah@google.com
Arun Sai Suggala
Google Research India
arunss@google.comPrateek Jain
Google Research India
prajain@google.com

Tóm tắt
Nén mạng nơ-ron (NN) qua các kỹ thuật như tỉa, lượng tử hóa đòi hỏi thiết lập các siêu tham số nén (ví dụ, số kênh cần tỉa, độ rộng bit cho lượng tử hóa) cho mỗi lớp hoặc thủ công hoặc qua tìm kiếm kiến trúc mạng nơ-ron (NAS) có thể tốn kém về mặt tính toán. Chúng tôi giải quyết vấn đề này bằng cách cung cấp một kỹ thuật đầu-cuối-đầu tối ưu hóa cho Phép Toán Dấu Phẩy Động (FLOPs) của mô hình hoặc cho độ trễ trên thiết bị qua một surrogate độ trễ ℓ1
ℓ2 mới. Thuật toán của chúng tôi linh hoạt và có thể được sử dụng với nhiều phương pháp nén phổ biến bao gồm tỉa, phân rã hạng thấp, và lượng tử hóa. Quan trọng hơn, nó nhanh và chạy trong gần như cùng thời gian với việc huấn luyện mô hình đơn; đây là tăng tốc đáng kể về thời gian huấn luyện so với các phương pháp NAS tiêu chuẩn. Cho nén BERT trên các tác vụ fine-tuning GLUE, chúng tôi đạt được giảm 50% FLOPs chỉ với 1% giảm hiệu suất. Cho nén MobileNetV3 trên ImageNet-1K, chúng tôi đạt được giảm 15% FLOPs, và 11% giảm độ trễ trên thiết bị mà không giảm độ chính xác, trong khi vẫn đòi hỏi ít hơn 3× tính toán huấn luyện so với các kỹ thuật nén SOTA. Cuối cùng, cho học chuyển giao trên các tập dữ liệu nhỏ hơn, kỹ thuật của chúng tôi xác định các kiến trúc rẻ hơn 1.2×-1.4× so với MobileNetV3, EfficientNet suite kiến trúc tiêu chuẩn với gần như cùng chi phí huấn luyện và độ chính xác.

1 Giới thiệu
Các mạng nơ-ron quy mô lớn liên tục cung cấp hiệu suất tiên tiến trên các tác vụ học phức tạp [1,2,3]. Nhưng chúng đặt gánh nặng lớn lên các tài nguyên tính toán như pin, bộ nhớ hoặc bộ xử lý khiến chúng khó triển khai trên các thiết bị edge như điện thoại, máy ảnh và thiết bị đeo. Một số công trình gần đây đã thiết kế các kỹ thuật để nén các mô hình ML và làm cho chúng hiệu quả cho suy luận. Tuy nhiên, như được chi tiết dưới đây, nhiều kỹ thuật này khó sử dụng trong thực tế, và thường đạt được sự đánh đổi độ chính xác vs thời gian suy luận dưới tối ưu.

Tìm kiếm siêu tham số cho nén. Các công trình hiện tại thường dựa vào một trong các khối xây dựng sau để thiết kế các mô hình hiệu quả: thưa thớt trọng số không có cấu trúc [4,5], tỉa toàn bộ neuron hoặc phân rã hạng thấp [6], lượng tử hóa [7], chưng cất [8]. Tìm ra cách tối ưu để kết hợp các khối xây dựng này (hoặc để tìm ra các siêu tham số như lượng thưa thớt liên quan đến mỗi khối) trong khi thỏa mãn ràng buộc toàn cục về độ trễ/FLOPs/tài nguyên là khó khăn và liên quan đến tìm kiếm tổ hợp. Vấn đề này càng trở nên tồi tệ hơn khi nhiều khối xây dựng được sử dụng cho nén mô hình (ví dụ, đồng thời phân rã hạng thấp, thưa thớt/tỉa trọng số).

Preprint. Under review.arXiv:2306.05785v2  [cs.LG]  13 Jun 2023

--- TRANG 2 ---
[Các biểu đồ và số liệu thống kê]

Hình 1: Biểu đồ bên trái so sánh các kỹ thuật khác nhau cho nén BERT trên các tác vụ GLUE (trung bình qua các tác vụ). trục x là số FLOPs tương đối so với BERT BASE. trục y là sự giảm độ chính xác tương đối từ baseline. Số liệu SOTA tỉa được lấy từ [21], trong khi baseline chưng cất từ [22,23]. Biểu đồ bên phải so sánh các kỹ thuật khác nhau cho nén MobileNetV3 trên tập dữ liệu ImageNet-1K. MobileNetV3 tương ứng với các mô hình MobileNetV3 với nhân tử độ rộng khác nhau. TuNAS, MorphNet là các kỹ thuật SOTA cho nén có thể mở rộng. TuNAS có cách tiếp cận blackbox đối với nén mô hình, trong khi MorphNet có cách tiếp cận trực tiếp hơn bằng cách tối ưu hóa mục tiêu chính quy FLOPs.

Trong vài năm qua, đã có một lượng lớn công trình giải quyết vấn đề tìm siêu tham số cho nén mô hình. Tài liệu hiện tại trong không gian này có thể được phân loại rộng rãi như: (a) các phương pháp tìm siêu tham số của một khối xây dựng cụ thể như tỉa trọng số không có cấu trúc, (b) các kỹ thuật Tìm kiếm Kiến trúc Mạng Nơ-ron (NAS) để tìm siêu tham số của bất kỳ khối hiệu quả nào. Tập hợp kỹ thuật đầu tiên tự nhiên bị hạn chế, nhưng ngay cả đối với các khối cụ thể được xem xét, hiệu suất của chúng trên các benchmark thế giới thực không ổn định và thực tế, có thể dưới tối ưu so với cách tiếp cận tổng quát hơn mà chúng tôi đề xuất trong công trình này (xem biểu đồ bên trái trong Hình 1). Các kỹ thuật dựa trên NAS có thể áp dụng tổng quát hơn nhiều, nhưng chi phí tính toán của các phương pháp như vậy là cấm đoán vì chúng thường không khai thác bất kỳ thuộc tính cụ thể nào của các khối hiệu quả phổ biến như thưa thớt, tỉa.

Tỉa Neuron: Trong các kỹ thuật danh mục (a) được đề cập ở trên, một dòng công trình nổi bật đã tập trung vào tỉa trọng số không có cấu trúc với phân bổ ngân sách không đồng nhất qua các lớp [4, 9,10,5]. Tuy nhiên, bất kỳ lợi ích nào trong FLOPs sử dụng tỉa không có cấu trúc đều khó chuyển đổi thành lợi ích độ trễ thực vì phần cứng hiện đại – như GPU, TPU – được thiết kế nhiều hơn cho các phép toán ma trận dày đặc. Vì vậy, việc tập trung vào tỉa neuron, loại bỏ toàn bộ neuron/kênh, và phân rã hạng thấp của trọng số, có liên quan chặt chẽ đến tỉa neuron, sẽ có lợi hơn. Các kỹ thuật gần đây trong dòng công trình này thêm một regularizer độ trễ/FLOPs vào hàm mất entropy chéo tiêu chuẩn [11,12] để thiên về mô hình với số neuron thấp hơn. Không may, mục tiêu kết quả là rời rạc và khó tối ưu hóa. Để giảm bớt điều này, các công trình hiện tại đã thiết kế các surrogate liên tục có thể tối ưu hóa theo kiểu SGD dễ dàng hơn. Các phương pháp này hoặc làm việc trong không gian phân phối xác suất trên các mô hình đã tỉa và tối ưu hóa "mục tiêu kỳ vọng" [12,13,6] hoặc thay thế regularizer FLOPs không liên tục với một surrogate liên tục như chuẩn ℓ1 của trọng số mạng [11]. Tuy nhiên, lớp kỹ thuật trước thường không ổn định, khó thực hiện trong thực tế, và các nghiên cứu thực nghiệm chỉ ra rằng hiệu suất của chúng tương tự như tỉa dựa trên độ lớn đơn giản [14] (cũng xem biểu đồ bên trái của Hình 1). Hơn nữa, như chúng tôi cho thấy trong công trình này, lớp kỹ thuật sau không thể thực thi thưa thớt khi có batch, layer normalization (xem Mục 3).

NAS: Một số công trình trong danh mục (b) xây dựng nén mô hình như một vấn đề Tìm kiếm Kiến trúc Mạng Nơ-ron blackbox và dựa vào các kỹ thuật NAS tiên tiến để tìm kiếm các mô hình hiệu quả [15, 16,17,18,19]. Các kỹ thuật này trực tiếp tính đến độ trễ/FLOPs và có tiềm năng xác định phân bổ ngân sách tối ưu theo lớp cho nhiều loại khối hiệu quả/cơ chế nén. Tuy nhiên, các cách tiếp cận này thường tốn kém về mặt tính toán vì chúng có quan điểm blackbox về vấn đề và thực hiện tìm kiếm tổ hợp trên không gian kiến trúc. Mặc dù có những tiến bộ gần đây như TuNAS [18] và DARTS [20], các kỹ thuật này có thể chậm hơn và kém chính xác hơn một bậc độ lớn so với phương pháp đề xuất của chúng tôi (xem Hình 1).

Cách tiếp cận của chúng tôi: Trong công trình này, chúng tôi đề xuất một cách tiếp cận nằm ngay giữa hai danh mục được đề cập ở trên. Tức là, cách tiếp cận của chúng tôi áp dụng cho một lớp lớn các khối xây dựng hiệu quả – như thưa thớt không có cấu trúc, tỉa neuron, lượng tử hóa – mà chúng tôi có thể viết tính toán FLOPs với một surrogate liên tục (xem Bảng 1). Hơn nữa, để đảm bảo rằng các regularizer FLOPs, độ trễ của chúng tôi hoạt động ngay cả khi có batchnorm, layernorm, chúng tôi đề xuất một surrogate mới dựa trên chuẩn ℓ1
ℓ2. Trong khi các surrogate của chúng tôi liên tục, chúng không khả vi. Trong các trường hợp như vậy, các optimizer tiêu chuẩn như SGD, Adam có thể khá chậm để hội tụ [24]. Để khắc phục điều này, chúng tôi đề xuất một phép chiếu trên các biến mask, sau mỗi bước SGD. Phương pháp đề xuất của chúng tôi tăng tốc sự hội tụ và cũng xuất ra các giải pháp thưa thớt chính xác do đó loại bỏ nhu cầu thresholding hậu kỳ, trong khi đủ đơn giản để không tăng thời gian huấn luyện đáng kể.

Chúng tôi thực hiện thuật toán của mình với nhiều khối xây dựng bao gồm tỉa, phân rã hạng thấp, lượng tử hóa, và áp dụng nó trên nhiều vấn đề trong lĩnh vực phân loại hình ảnh và NLP. Cụ thể, chúng tôi chứng minh hiệu quả của kỹ thuật này cho nén MobileNetV3 trên ImageNet (xem Hình 1), nơi phương pháp của chúng tôi có thể học một kiến trúc với lên đến 15% (11%) FLOPs (độ trễ) thấp hơn trên điện thoại di động Pixel 6, mà không có bất kỳ sự giảm độ chính xác nào. Ở đây cách tiếp cận của chúng tôi chính xác hơn MorphNet, một kỹ thuật SOTA tập trung độc quyền vào tỉa neuron, cũng như TuNAS, một kỹ thuật NAS SOTA. Hơn nữa, về thời gian huấn luyện, phương pháp của chúng tôi rẻ hơn 3× so với TuNAS. Chúng tôi muốn nhấn mạnh rằng MobileNetv3 là một kiến trúc được tối ưu hóa cao được tìm thấy bằng các kỹ thuật NAS hiệu quả [25], và kỹ thuật của chúng tôi có thể nén kiến trúc này thêm nữa.

Một ứng dụng thú vị của công trình của chúng tôi là chúng tôi có thể áp dụng nó để tối ưu hóa một số mô hình baseline "nền tảng" nhất định cho các tác vụ fine-tuning cá nhân. Ví dụ, cho nén BERT trên benchmark GLUE, phương pháp của chúng tôi đạt được giảm 40−50% FLOPs chỉ với 1% giảm độ chính xác (xem Hình 1). Hơn nữa, kỹ thuật của chúng tôi vượt trội so với các baseline nén mô hình tiêu chuẩn. Tương tự cho các tác vụ phân loại thị giác nhỏ hơn, kỹ thuật của chúng tôi nén MobileNetV3, EfficientNet suite kiến trúc và xác định các kiến trúc rẻ hơn 1.2×-1.4× mà không mất độ chính xác đáng kể (xem Hình 3). Chúng tôi muốn lưu ý rằng tất cả các kết quả này được thu được với gần như cùng chi phí như huấn luyện một mô hình đơn cho tác vụ. Cuối cùng, chúng tôi cũng chứng minh tính linh hoạt của phương pháp bằng cách sử dụng nó để lượng tử hóa một CNN trên CIFAR-10, và học các độ rộng bit (2,4,8,16) cho mỗi lớp của nó. Kỹ thuật của chúng tôi tìm thấy một mô hình nhỏ hơn 55% so với mô hình float-16 baseline, trong khi đạt được cùng độ chính xác (xem Hình 5). Trong khi lượng tử hóa bit thấp thường không được khai thác bởi các accelerator mục đích chung để tăng tốc tính toán, nó vẫn có thể dẫn đến giảm thời gian suy luận của các mô hình ngôn ngữ lớn như GPT vì các mô hình này bị ràng buộc băng thông bộ nhớ [26]. Đây là tóm tắt các đóng góp của chúng tôi:

(1). Chúng tôi cung cấp một kỹ thuật nén mạng nơ-ron đầu-cuối-đầu trực tiếp tối ưu hóa mục tiêu chính quy FLOPs/độ trễ trong quá trình dẫn đến nén trong huấn luyện. Thuật toán của chúng tôi có thể được sử dụng với nhiều khối xây dựng hiệu quả phổ biến bao gồm tỉa, phân rã hạng thấp, lượng tử hóa, và có thể tối ưu hóa cho độ trễ suy luận trên thiết bị.

(2). Chúng tôi thiết kế một surrogate chính quy ℓ1
ℓ2 mới cho độ trễ hoạt động ngay cả khi có batchnorm, layernorm. Thuật toán của chúng tôi nhanh và chạy trong cùng thời gian với việc huấn luyện mô hình đơn, và không đòi hỏi bất kỳ bước hậu xử lý nào.

(3). Chúng tôi chứng minh hiệu suất của kỹ thuật này trên cả tác vụ ngôn ngữ và thị giác. Hơn nữa, cho các thiết lập học chuyển giao nơi mục tiêu là lấy một kiến trúc baseline và tối ưu hóa nó cho các tác vụ cá nhân, kỹ thuật của chúng tôi vượt trội so với các kỹ thuật SOTA trong lĩnh vực rộng của nén mạng nơ-ron tự động.

2 Công trình liên quan

2.1 Tìm kiếm Kiến trúc Mạng Nơ-ron

Các công trình đầu tiên về NAS coi vấn đề là một bài toán tối ưu hóa blackbox thuần túy (BO). Các công trình này dựa vào các kỹ thuật BO như tìm kiếm ngẫu nhiên [27], tối ưu hóa quy trình Gaussian [17], và gradient descent bậc không [15,16], thuật toán tiến hóa để tối ưu hóa mục tiêu NAS và xác định một kiến trúc tốt. Một số công trình đã cải thiện các thuật toán này bằng cách sử dụng heuristic như dừng sớm [27]. Tuy nhiên, các kỹ thuật này tốn kém về mặt tính toán, vì đánh giá mục tiêu tối ưu hóa tại bất kỳ điểm nào đòi hỏi huấn luyện một mạng nơ-ron từ đầu. Hơn nữa, do độ phức tạp tính toán, các kỹ thuật này thực hiện tìm kiếm rất thô và không phù hợp cho tìm kiếm tinh vi trên cấu trúc thưa thớt hoặc hạng thấp.

Các công trình gần đây đã cố gắng mở blackbox một chút. Trong các kỹ thuật này, không gian tìm kiếm đầu tiên được chuyển đổi sang không gian phân phối xác suất trên kiến trúc. Tiếp theo, một mô hình surrogate (nhận một kiến trúc như đầu vào và cố gắng xuất ra tập trọng số tối ưu cho kiến trúc) được huấn luyện để nhanh chóng đánh giá mục tiêu tối ưu hóa tại bất kỳ đầu vào nào [18,20,28,29,12]. Trong khi các kỹ thuật này nhanh, chúng liên quan đến huấn luyện chung của mô hình surrogate trong quá trình tìm kiếm. Huấn luyện chung này thường làm cho quá trình tối ưu hóa không ổn định [30].

--- TRANG 3 ---
NAS cho ML Hiệu quả. Một số công trình gần đây tại giao điểm của ML hiệu quả và NAS đã nhận ra tầm quan trọng của việc tính toán rõ ràng phần cứng trong quá trình tìm kiếm [15,31,32,33,34,35]. Các công trình này kết hợp thời gian suy luận thực tế vào mục tiêu tìm kiếm của chúng, thay vì các surrogate như FLOPs. Thời gian suy luận có thể được ước tính sử dụng một mạng nơ-ron khác, hoặc thông qua bảng độ trễ cho các phép toán số học cơ bản trên nền tảng mục tiêu [19]. Nhiều công trình này dựa vào heuristic tìm kiếm tham lam, ngẫu nhiên để giải quyết mục tiêu kết quả [32, 33]. Tuy nhiên, các heuristic này hoặc mất nhiều thời gian để tìm kiến trúc tối ưu hoặc không được đảm bảo hội tụ đến giải pháp tối ưu. Có một số công trình dựa vào các thuật toán NAS được mô tả ở trên [15,31,18]. Tuy nhiên, các kỹ thuật này gặp phải cùng những vấn đề như đã đề cập trước đó.

Thiết kế phần cứng, kiến trúc mạng nơ-ron đồng thiết kế. Một số tham số cấp phần cứng như cấu hình tiling của tensor ảnh hưởng đáng kể đến thời gian suy luận của một mô hình. Các kỹ thuật NAS nhận biết phần cứng gần đây phơi bày các tham số cấp phần cứng này cho thuật toán NAS và đồng thời tìm kiếm trên kiến trúc mạng nơ-ron và cấu hình phần cứng [35]. Các kỹ thuật này có tiềm năng đạt được hiệu suất tốt hơn so với các kỹ thuật NAS vanilla không tìm kiếm trên cấu hình phần cứng.

2.2 Nén Mô hình

Lĩnh vực nén mô hình rất rộng lớn. Ở đây, chúng tôi tập trung vào các kỹ thuật thực hiện nén thời gian huấn luyện (trái ngược với nén hậu huấn luyện) sử dụng các khối xây dựng sau: thưa thớt không có cấu trúc, tỉa và phân rã hạng thấp. Các công trình đầu tiên trong thưa thớt không có cấu trúc và tỉa dựa vào tỉa dựa trên độ lớn, gradient [4,36,14]. Một số công trình đã khám phá các metric chấm điểm tinh vi hơn cho tỉa [37,38,39,40,41]. Các kỹ thuật khác bao gồm thêm các chuẩn cảm ứng thưa thớt như ℓ0, ℓ1 vào mục tiêu huấn luyện [13,5]. Một số công trình cũng đã khám phá phân rã hạng thấp cho nén mô hình [42,43,44]. Một số kỹ thuật này lại dựa vào các regularizer cảm ứng thưa thớt để cảm ứng cấu trúc hạng thấp [6]. Những kỹ thuật khác dựa vào tỉa dựa trên SVD. Một số công trình gần đây cố gắng tối ưu hóa mục tiêu chính quy FLOPs để thực hiện tỉa, phân rã hạng thấp [11,12]. Tuy nhiên, như chúng tôi đã thảo luận trong giới thiệu, các kỹ thuật tối ưu hóa kết quả thường không ổn định và khó sử dụng trong thực tế.

3 Phương pháp

Trong mục này, chúng tôi mô tả cách tiếp cận của chúng tôi cho nén mô hình. Để đơn giản hóa trình bày, chúng tôi minh họa kỹ thuật của mình trên mạng feed-forward và giới hạn bản thân với tỉa. Các ý tưởng ở đây có thể được mở rộng đến các kiến trúc khác (ví dụ, convolution 1x1 trong CNN), và các khối xây dựng hiệu quả khác (ví dụ, thưa thớt không có cấu trúc, phân rã hạng thấp, lượng tử hóa) một cách đơn giản (xem Bảng 1 để biết chi tiết). Xem xét vấn đề sau: chúng ta được cho một mạng nơ-ron feed forward (FFN) đã được huấn luyện trước f∗(x) =σ(W∗
Dσ(W∗
D−1σ(. . . σ(W∗
1x)))), trong đó W∗
i∈Rdi+1×di cho tất cả i∈[D], và một tập dữ liệu {(xi, yi)}n
i=1. Mục tiêu của chúng ta là nén f∗ trong khi đồng thời hoạt động tốt trên tác vụ học. Vấn đề này có thể được công thức hóa như bài toán tối ưu hóa sau

min
W1
nnX
i=1ℓ(xi, yi;W) +λ×Latency (W). (1)

Ở đây W={Wi}D
i=1, với Wi∈Rd′
i+1×d′
i là ma trận trọng số tại lớp i, λ là tham số chính quy hóa đánh đổi độ trễ với độ chính xác và ℓ là hàm mất có giám sát.1. Tối ưu hóa trực tiếp mục tiêu trên là không khả thi vì Latency (W) là một hàm rời rạc của các chiều của ma trận trọng số, và phụ thuộc vào phần cứng cụ thể.

Bây giờ chúng tôi trình bày kỹ thuật của mình để giải Phương trình (1). Để bắt đầu, chúng tôi thay thế Latency (W) bằng FLOPs (W)2. Sau này, chúng tôi mở rộng nó đến độ trễ thực tế. Mục tiêu trong trường hợp này được cho bởi

min
W1
nnX
i=1ℓ(xi, yi;W) +λDX
i=1d′
id′
i+1. (2)

1Trong mục tiêu này, chúng tôi tìm kiếm trên d′
i sao cho d′
i≤di
2FLOPs cũng là một hàm rời rạc của các chiều của Wi, và bài toán tối ưu hóa kết quả vẫn không khả thi

--- TRANG 4 ---
Để giải quyết mục tiêu này, chúng tôi liên kết các mask với mỗi neuron trong mạng. Cụ thể, chúng tôi tham số hóa ma trận trọng số trong lớp thứ i là Wi×diag(αi). Ở đây αi∈ {0,1}di là các biến mask của lớp i. Nếu αi,j được đặt thành 0, thì neuron thứ j trong lớp thứ (i−1) sẽ bị tỉa. Regularizer FLOPs bây giờ có thể được viết theo mask là PD
i=1∥αi∥0∥αi+1∥0, trong đó αD+1 là vector tĩnh của tất cả số 1. Mục tiêu kết quả tuy nhiên không liên tục. Để làm cho nó liên tục và có thể tối ưu hóa dựa trên gradient, một lớp kỹ thuật đặt phân phối Bernoulli Bern(pi,j) trên mỗi mask αi,j và giải quyết mục tiêu được làm mượt sau [12, 13, 6]

min
W,pE"
1
nnX
i=1ℓ(xi, yi;p,W) +λDX
i=1∥αi∥0∥αi+1∥0#
.

Kỳ vọng ở trên được lấy w.r.t các mask ngẫu nhiên αi. Dễ thấy rằng mục tiêu trên tương đương với Phương trình (2), và do đó khó như việc giải quyết mục tiêu sau. Thực tế, bài toán trên có thể được chứng minh là NP-hard bằng cách sử dụng quan sát rằng hồi quy tuyến tính thưa thớt là một trường hợp đặc biệt của nó [45]. Hơn nữa, tính chất rời rạc của αi làm cho quá trình tối ưu hóa không ổn định [13]. Để khắc phục điều này, [12,13,6] dựa vào một heuristic liên quan đến việc nới lỏng phân phối Bernoulli thành phân phối liên tục như LogisticSigmoid. Tuy nhiên, nhược điểm chính của thuật toán kết quả là nó khó thực hiện trong thực tế và đòi hỏi annealing rất cẩn thận các tham số của phân phối LogisticSigmoid. Một nhược điểm khác của lớp kỹ thuật này là hiệu suất của chúng không được hiểu rõ về mặt lý thuyết, ngay cả đối với các vấn đề đơn giản và cơ bản như hồi quy tuyến tính thưa thớt.

Một cách tiếp cận khác để chuyển đổi mục tiêu rời rạc trong Phương trình (2) thành một hàm liên tục là thay thế chuẩn ℓ0 trên αi bằng chuẩn ℓ1

min
W,αi∈Rdi1
nnX
i=1ℓ(xi, yi;α,W) +λDX
i=1∥αi∥1∥αi+1∥1. (3)

Cách tiếp cận này hấp dẫn hơn nhiều so với cách tiếp cận trước vì nó được biết là khôi phục các giải pháp thưa thớt tối ưu cho nhiều vấn đề thống kê bao gồm hồi quy tuyến tính thưa thớt, hoàn thiện ma trận hạng thấp [46,47]. Hơn nữa, nó đơn giản hơn nhiều để thực hiện trong thực tế, với nhiều thuật toán được đề xuất để hội tụ nhanh đến các điểm dừng của mục tiêu [24,48]. Do đó, các kỹ thuật nén SOTA gần đây dựa vào các surrogate chuẩn ℓ1 để tính toán regularizer FLOPs [11]. Một nhược điểm lớn của chuẩn ℓ1 tuy nhiên là nó không thúc đẩy thưa thớt khi có batch normalization và layer normalization [49,50]. Để thấy điều này, xem xét mạng 1 lớp ẩn sau: σ(BN(W2diag(α2)σ(BN(W1diag(α1)x)))). Người ta có thể scale down tất cả các mục của α1 và scale up các trọng số W1 mà không ảnh hưởng đến đầu ra của mạng. Làm điều này giảm giá trị mục tiêu trong Phương trình (3), nhưng không cảm ứng bất kỳ thưa thớt nào trong mạng. Trong thực tế, chúng tôi thực sự nhận thấy hành vi này trong quá trình tối ưu hóa Phương trình (3), dẫn đến các giải pháp dưới tối ưu (xem Mục 3.2). Lưu ý rằng việc thêm penalty ℓ2 trên trọng số (tức là, weight decay) không giảm thiểu vấn đề này vì bất kỳ scaling nào của α có thể bị hấp thụ bởi các tham số batch norm mà không thay đổi đầu ra của mạng.

3.1 Cảm ứng thưa thớt thông qua regularizer ℓ1
ℓ2

Bây giờ chúng tôi giới thiệu cách tiếp cận của mình để làm cho mục tiêu trong Phương trình (2) liên tục. Chúng tôi thay thế chuẩn ℓ0 trên mask (∥αi∥0) bằng penalty ℓ1
ℓ2 (√di∥αi∥1/∥αi∥2) và giải quyết bài toán tối ưu hóa sau

min
W,αi∈Rdi1
nnX
i=1ℓ(xi, yi;α,W) +λDX
i=1√di∥αi∥1
∥αi∥2p
di+1∥αi+1∥1
∥αi+1∥2. (4)

Số hạng √di trong tử số chuẩn hóa penalty để nằm giữa [0, di]. Khi αi đều là 1, regularizer đánh giá thành FLOPs. Quan sát rằng regularizer này bất biến đối với scaling của α. Do đó, giá trị của regularizer không thể đơn giản được giảm bằng cách scale down αi. Trong các thí nghiệm của chúng tôi trong mục 3.2 và Phụ lục C.2, chúng tôi cho thấy rằng điều này xử lý batch, layer normalization tốt hơn so với regularizer ℓ1. Một số công trình đã nghiên cứu regularizer này trong bối cảnh hồi quy tuyến tính thưa thớt và cho thấy rằng nó khôi phục tín hiệu thưa thớt cơ bản dưới các điều kiện nhẹ trên

--- TRANG 5 ---
Hình 2: So sánh regularizer FLOPs cảm ứng ℓ1, ℓ1
ℓ2 cho tỉa trên FashionMNIST: Hình (a) và (b) mô tả sự tiến hóa của thống kê của các biến mask (α) khi huấn luyện tiến triển. Hình (c) cho thấy mối quan hệ giữa FLOPs thực tế của mô hình và giá trị của proxy được tính bởi Phương trình 3, 4. Hình (d) cho thấy sự tiến hóa của chuẩn Frobenius của ma trận trọng số.

dữ liệu [51,52,53]. [54] đã sử dụng một regularizer ℓ1
ℓ2 tương tự cho tỉa mạng, nhưng kỹ thuật của họ không tối ưu hóa độ trễ hoặc FLOPs, và dựa vào thresholding hậu huấn luyện để có được thưa thớt.

Vì một số lý do kỹ thuật được mô tả sau, chúng tôi thêm một ràng buộc tích cực trên αi và giải quyết mục tiêu sau

min
W,αi∈Rdi
+1
nnX
i=1ℓ(xi, yi;α,W) +λDX
i=1√diPdi
j=1αi,j
∥αi∥2p
di+1Pdi+1
j=1αi+1,j
∥αi+1∥2. (5)

Lưu ý rằng chúng tôi xem xét α∈Rdi
+ thay vì các giá trị rời rạc hoặc bị chặn. Chúng tôi muốn nhấn mạnh rằng thay đổi này không giảm sức mạnh đại diện của mô hình. Nó chủ yếu được thực hiện vì lý do tính toán. Trong phần tiếp theo, chúng tôi sử dụng ký hiệu viết tắt ∥αi∥1p (p cho positive) để biểu thị Pdi
j=1αi,j.

Tầm quan trọng của các ràng buộc tích cực. Mục tiêu trong Phương trình (4) liên tục, nhưng không mượt. Đối với các hàm mất như vậy, các kỹ thuật tối ưu hóa tiêu chuẩn như SGD, Adam chậm hội tụ đến các điểm dừng [55]. Hơn nữa, các thuật toán này không xuất ra các giải pháp thưa thớt chính xác. Điều này buộc các bước hậu xử lý bổ sung phải được đưa vào pipeline nén. Ví dụ, [11,54] dựa vào optimizer Adam và thêm một bước tỉa ở cuối, nơi các mask gần 0 bị tỉa đi. Điều này khá cồng kềnh trong thực tế vì người ta cần chọn ngưỡng phù hợp cho tỉa, điều này đưa ra một siêu tham số có thể điều chỉnh bổ sung, và cần huấn luyện lại sau khi tỉa.

Để khắc phục điều này, chúng tôi thêm một ràng buộc tích cực vào các biến mask và sửa đổi mục tiêu thành Phương trình (5). Điều này làm cho regularizer mượt (trừ tại vector toàn số 0), và dễ tối ưu hóa bằng SGD, Adam. Sau mỗi bước cập nhật SGD/Adam, chúng tôi đơn giản chiếu các mask trở lại không gian số thực dương. Cập nhật tổng thể trông như sau

W ← W − η∇W(L(α,W) +λR(α)), α←max(0 , α−η∇α(L(α,W) +λR(α))).

Ở đây L(α,W) là rủi ro thực nghiệm và R(α) là regularizer. Lưu ý, bước bổ sung duy nhất so với tối ưu hóa truyền thống, là việc clipping của α. Trong các nghiên cứu ablation của chúng tôi trong Mục 3.2 và Phụ lục C.2, chúng tôi xác nhận tầm quan trọng của bước chiếu này, cùng với chuẩn ℓ1
ℓ2, trong việc khuyến khích các giải pháp thưa thớt.

3.2 Xác minh các lựa chọn thiết kế

Để chứng minh thực nghiệm các nhược điểm của việc sử dụng penalty ℓ1 cho nén mô hình, chúng tôi thực hiện các thí nghiệm trên tập dữ liệu FashionMNIST với mạng kết nối đầy đủ một lớp ẩn có lớp batch norm sau lớp tuyến tính đầu tiên. Chúng tôi tỉa đầu vào của mạng bằng cách sử dụng một mask α trên đầu vào. Chúng tôi so sánh hiệu suất của các mạng được nén bằng regularizer FLOPs cảm ứng bởi chuẩn ℓ1 và ℓ1
ℓ2. Chúng tôi sử dụng SGD để tối ưu hóa cả hai mục tiêu. Hơn nữa, chúng tôi pre-train mạng bằng hàm mất CE tiêu chuẩn, và khởi tạo α=1. Chúng tôi theo dõi phương sai của các giá trị tuyệt đối của các mục của α, tức là Pd
i=1(|αi|−µα)2
d, trong đó µα=Pd
i=1|αi|
d. Chúng tôi cũng theo dõi trung bình µα của các giá trị tuyệt đối của các mục của α. Cuối cùng, chúng tôi vẽ đường cong giữa FLOPs và chuẩn được xem xét của α (tức là, ℓ1, ℓ1
ℓ2). Hình 2 trình bày kết quả từ các thí nghiệm này. Chúng ta có thể thấy rằng mục tiêu ℓ1 không phù hợp với giá trị thực tế của FLOPs, trong khi regularizer được tính bằng ℓ1
ℓ2 là một proxy tốt hơn. Chúng tôi cũng thấy rằng trung bình và phương sai của α giảm mạnh khi regularizer FLOPs cảm ứng ℓ1 được sử dụng cho nén. Điều này chỉ ra rằng tất cả các mục của α được scale down đồng nhất

--- TRANG 6 ---
Bảng 1: Bảng mô tả các regularizer được sử dụng bởi kỹ thuật của chúng tôi cho các khối xây dựng hiệu quả khác nhau (tham khảo Phụ lục để biết chi tiết về lượng tử hóa). Người ta có thể dễ dàng thiết kế các regularizer để tìm kiếm trên một sự kết hợp của các khối xây dựng. Ví dụ, hàng cuối cùng trình bày regularizer cho hạng thấp + tỉa, mà chúng tôi sử dụng trong các thí nghiệm quy mô lớn của mình.

Khối Xây dựng
Hiệu quả | Tham số hóa của Wi | FLOPs
(lớp thứ i) | Regularizer (FLOPs surrogate)
(lớp thứ i)

Tỉa | Wi×diag(αi) | ∥αi∥0∥αi+1∥0 | √di∥αi∥1p
∥αi∥2√
di+1∥αi+1∥1p
∥αi+1∥2

Thưa thớt Không có Cấu trúc | Wi⊙αi, trong đó αi∈Rdi+1×di
+ ,
⊙ là toán tử nhân theo từng phần tử | ∥Vec(αi)∥0 | √
didi+1∥Vec(αi)∥1p
∥Vec(αi)∥2

Phân rã Hạng Thấp | Uidiag(βi)Vi,
trong đó Ui∈Rdi+1×di,∗,
di,∗= min {di, di+1} | (di+di+1)∥βi∥0 | (di+di+1)√
di,∗∥βi∥1p
∥βi∥2

Lượng tử hóa
(lượng tử hóa 1,2,4bit) | Wi,1+αi,2(∆i,2+αi,4(∆i,4)),
trong đó αi,2, αi,4∈[0,1], là
các biến mask, Wi,b là
lượng tử hóa b-bit của Wi,
∆i,2=Wi,2−Wi,1,
∆i,4=Wi,4−Wi,2 | ∥(1−αi,2)∥0didi+1+
2∥αi,2(1−αi,4)∥0didi+1+
4∥αi,2αi,4∥0didi+1 | chuẩn ℓ1
ℓ2 trên
vector [(1−αi,2),
2αi,2(1−αi,4),4αi,2αi,4]
×didi+1

Tỉa +
Phân rã Hạng Thấp | Uidiag(βi)Vidiag(αi),
trong đó Ui∈Rdi+1×di,∗,
di,∗= min {di, di+1} | (∥αi∥0+∥αi+1∥0)∥βi∥0 | √di∥αi∥1p
∥αi∥2+√
di+1∥αi+1∥1p
∥αi+1∥2
×√
di,∗∥βi∥1p
∥βi∥2

thành một giá trị nhỏ, khác không, giảm giá trị của regularizer, trong khi không cung cấp bất kỳ thưa thớt nào. Như thấy từ hình, ℓ1
ℓ2 không gặp phải nhược điểm này. Cuối cùng, chúng tôi lưu ý rằng chuẩn frobenius của ma trận trọng số W tăng khi chính quy hóa ℓ1 được sử dụng trên α, gợi ý rằng mạng đơn giản đang scale down α và scale up các trọng số để tránh regularizer.

3.3 Nén mô hình nhận biết phần cứng

Trong mục này, chúng tôi mở rộng regularizer FLOPs để tính đến độ trễ trên phần cứng mục tiêu. Regularizer kết quả đặc biệt hữu ích để thực hiện nén mạng nhận biết phần cứng. Quan sát chính của chúng tôi là suy luận trên một mạng nơ-ron có thể được chia thành một loạt các phép toán nhân ma trận. Ví dụ, suy luận trên một FFN độ sâu D liên quan đến D phép nhân ma trận-vector, chiếm phần lớn thời gian. Vì vậy, việc có một ước tính tốt về thời gian suy luận của toàn bộ mạng quy về việc có một ước tính tốt về độ trễ của phép nhân ma trận-vector. Để làm điều này, chúng tôi dựa vào bảng tra cứu. Trước khi bắt đầu giai đoạn tỉa, chúng tôi xây dựng một bảng tra cứu 2 chiều T có mục (d1, d2) thứ là độ trễ trên thiết bị của việc nhân một ma trận kích thước d1×d2 với một vector kích thước d2. Bảng như vậy dễ xây dựng, với quyền truy cập vào thiết bị mục tiêu. Tiếp theo, để kết hợp bảng tra cứu T vào thuật toán tỉa của chúng tôi, chúng tôi chuyển đổi nó thành một hàm liên tục bằng cách thực hiện nội suy tuyến tính trên các mục trong bảng [56]. Để chính xác, đối với bất kỳ (x, y)∈[d1, d1+ 1]×[d2, d2+ 1], trong đó d1, d2∈N∪{0}, chúng tôi định nghĩa T(x, y) là: T(x, y) =t1+ (t2−t1)(y−d2), trong đó t1=T(d1, d2) + (T(d1+ 1, d2)−T(d1, d2))(x−d1), và t2=T(d1, d2+ 1) + ( T(d1+ 1, d2+ 1)−T(d1, d2+ 1))( x−d1). Lưu ý rằng trái ngược với các kỹ thuật NAS black-box như [19] tìm kiếm trên không gian rời rạc số bộ lọc cho mỗi khối, cách tiếp cận của chúng tôi cần surrogate độ trễ phải khả vi, và do đó chúng tôi cần bảng độ trễ được nội suy. Xem phụ lục để biết chi tiết về cách chúng tôi xây dựng các bảng.

Chúng tôi sử dụng bảng tra cứu được nội suy này để xây dựng regularizer độ trễ của chúng tôi như sau

DX
i=1T √di∥αi∥1p
∥αi∥2,p
di+1∥αi+1∥1p
∥αi+1∥2!
. (6)

Trong biểu thức trên, surrogate khả vi của chúng tôi cho ∥αi∥0 (tức là, √di∥αi∥1p/∥αi∥2), được sử dụng để lập chỉ mục bảng tra cứu. Chúng tôi lưu ý rằng chuẩn ℓ1
ℓ2 rất quan trọng để kỹ thuật này thành công. Điều này là do √di∥αi∥1p
∥αi∥2 được chuẩn hóa và luôn nằm giữa [0, di]. Ngược lại, việc sử dụng surrogate chuẩn ℓ1 trong regularizer cho chúng ta T(∥αi∥1,∥αi+1∥1). Scaling αi bằng một hằng số có thể thay đổi drastically regularizer này, và làm cho tối ưu hóa không ổn định.

--- TRANG 7 ---
[Các biểu đồ và số liệu thống kê]

Hình 3: Đánh đổi độ chính xác-FLOPs trên các tác vụ học chuyển giao: Hình (a) và (b) mô tả hiệu suất fine-tuning của các mô hình được tìm thấy bởi phương pháp của chúng tôi trong khi nén MobileNetv3Large và baseline MobileNetV3 trên tập dữ liệu Cars-196 và Food-101. Hình (c) và (d) cho thấy hiệu suất trên họ kiến trúc EfficientNet, trong đó baseline là EfficientNetB0-B4, trong khi phương pháp của chúng tôi nén EfficientNet B4 và B2.

4 Thí nghiệm

Trong mục này, chúng tôi áp dụng framework của mình cho các tác vụ pre-training và học chuyển giao quy mô lớn trên các benchmark ngôn ngữ và thị giác tiêu chuẩn. Để chứng minh tính linh hoạt của kỹ thuật, chúng tôi thực hiện thí nghiệm trên nhiều họ mô hình (MobileNet, EfficientNet [2], BERT), và nhiều khối xây dựng (tỉa, phân rã hạng thấp, lượng tử hóa). Chúng tôi cũng trình bày một nghiên cứu trường hợp sử dụng độ trễ thực tế trên thiết bị thay vì FLOPs. Xem Phụ lục C.2 cho các nghiên cứu ablation khác.

4.1 Pre-training ImageNet

Chúng tôi bắt đầu bằng cách so sánh hiệu suất của kỹ thuật với các baseline trên nén MobileNetV3, cho phân loại ImageNet. Chúng tôi dựa vào phân rã hạng thấp + tỉa cho nén. Kết quả từ thí nghiệm này được trình bày trong Hình 1. Bằng cách thay đổi cường độ chính quy hóa của chúng tôi, chúng tôi thu được các mô hình với MAC và độ chính xác khác nhau. Chúng tôi thấy rằng các mô hình được tạo ra bởi phương pháp của chúng tôi vượt trội đáng kể so với MobileNetV3 và TuNAS trong chế độ MAC cao và trung bình. Cụ thể, với cùng độ chính xác như MobileNetV3Large, cách tiếp cận của chúng tôi tìm thấy một mô hình với ít hơn 15% MAC. So với TuNAS, chúng tôi đạt được giảm 30% MAC ở cùng mức độ chính xác. Tuy nhiên, chúng tôi thấy rằng mô hình của chúng tôi ngang bằng với MobileNetV3Small trong chế độ MAC thấp, chỉ ra rằng mô hình trước đã được điều chỉnh tốt cho tác vụ này. Về tính toán cần thiết cho huấn luyện, TuNAS là đắt nhất trong tất cả các kỹ thuật chúng tôi đã thử; nó mất 2 ngày để huấn luyện với thiết lập phần cứng của chúng tôi. Ngược lại, phương pháp của chúng tôi mất 13 giờ (nhanh hơn 3−4× so với TuNAS), và MorphNet mất 10 giờ.

4.2 Học Chuyển giao

Một paradigm phổ biến trong việc triển khai các mô hình học máy ngày nay là đầu tiên pre-train chúng trên một tập dữ liệu quy mô lớn như ImageNet, và sau đó fine-tune chúng cho tác vụ mục tiêu mong muốn. Tuy nhiên, triển khai các mô hình lớn không khả thi trên các thiết bị edge. Kỹ thuật của chúng tôi cung cấp một sửa đổi nhẹ cho quy trình fine-tuning tiêu chuẩn bằng cách tạo ra một mô hình nén với hiệu suất học chuyển giao tương đương trên tác vụ cụ thể. Chúng tôi chứng minh điều này trên các tác vụ thị giác và ngôn ngữ.

Tác vụ thị giác. Chúng tôi xem xét tác vụ fine-tuning một mô hình được pre-train trên ImageNet cho một tập dữ liệu nhỏ hơn. Chúng tôi xem xét Cars196 [57] và Food101 [58] là các tập dữ liệu mục tiêu, và so sánh với các họ mô hình MobileNetV3 và EfficientNet. Chúng tôi sử dụng các mô hình được pre-train trên ImageNet để khởi tạo. Chúng tôi vẽ các đường cong FLOP-độ chính xác trong Hình 3. Chúng tôi nén các kiến trúc MobileNetv3Large và EfficientNet-B4 và EfficientNet-B2 trong khi chuyển giao chúng đến tác vụ mục tiêu. Chúng tôi thấy rằng phương pháp của chúng tôi liên tục cải thiện so với các kiến trúc baseline qua các chế độ FLOPs khác nhau. Điều này là do kỹ thuật của chúng tôi có thể tỉa mô hình một cách thích ứng dựa trên độ khó của tác vụ phân loại. Trên cả hai tác vụ, chúng ta thấy lợi ích độ chính xác 1% so với MobileNetV3 small. Lợi ích độ chính xác duy trì ở dấu chân độ trễ của MobileNetV3Large-0.75, nơi chúng ta thấy hơn 1.5% lợi ích độ chính xác trên cả hai tập dữ liệu. Trên EfficientNet, chúng ta thấy lên đến 40% giảm FLOPs mà không có bất kỳ sự giảm độ chính xác nào trên Food101, và khoảng 20% giảm FLOPs trên tập dữ liệu Cars196 cho các mô hình lớn nhất (B4). Chúng ta cũng thấy khoảng 30% giảm FLOP trong khi duy trì hiệu suất học chuyển giao của các biến thể B1 và B0. Điều này chứng minh rằng các mô hình học được của chúng tôi có thể scale tốt hơn so với scaling heuristic được mô tả trong [2]. Xem phụ lục cho các kết quả bổ sung.

Fine-tuning BERT trên GLUE. Chúng tôi xem xét 5 tập dữ liệu của benchmark GLUE [59] thường được sử dụng trong tài liệu, và fine-tune một mô hình BERT-Base được pre-train với regularizer FLOPs của chúng tôi. Chúng tôi tái tham số hóa các ma trận trọng số của mạng feed forward của mỗi khối transformer với tham số hóa hạng thấp+thưa thớt của chúng tôi. Chúng tôi so sánh cách tiếp cận của mình với tỉa mô hình,

--- TRANG 8 ---
[Các biểu đồ và số liệu thống kê]

Hình 4: Biểu đồ bên trái cho thấy các đường cong độ chính xác-độ trễ của các mô hình thu được bằng cách sử dụng regularizer FLOPs, độ trễ. Bảng bên phải so sánh hiệu suất của các mô hình chính quy độ trễ của chúng tôi với baseline MobileNetV3.

Hình 5: Lượng tử hóa trên CIFAR-10: Hình (a) so sánh hiệu suất của kỹ thuật của chúng tôi cho lượng tử hóa động với lượng tử hóa bit cố định cho một CNN 4 lớp trên CIFAR-10. Các baseline có trọng số được lượng tử hóa thành 2,4,8, 16 bit. Hình (b) mô tả các độ rộng bit học được cho các lớp khác nhau của các mô hình được tìm thấy bởi kỹ thuật của chúng tôi, với các nhãn biểu thị số MAC (trong Bn) của các mô hình.

nơi số liệu SOTA được lấy từ Hình 6 của [21], báo cáo độ chính xác tối đa trong số [60,61, 62,63,64,65]. Chúng tôi cũng báo cáo hiệu suất của các baseline chưng cất được sử dụng rộng rãi [22,23]. Hình 1 trình bày hiệu suất trung bình trên 5 tập dữ liệu, và Hình 6 trong phụ lục trình bày hiệu suất cá nhân. Trong cả hai hình này, chúng tôi vẽ FLOPs tương đối của mô hình nén w.r.t BERT-base so với sự giảm độ chính xác w.r.t BERT-base (tương tự như [21]). Chúng tôi thấy rằng trên 4 trong 5 tập dữ liệu được xem xét, kỹ thuật của chúng tôi cung cấp độ chính xác cao hơn với cùng số FLOPs, chỉ ra hiệu quả của phương pháp. Trên MRPC, một tập dữ liệu với rất ít mẫu, phương pháp của chúng tôi tệ hơn trên FLOPs cao hơn, nhưng vượt trội so với các baseline trong chế độ FLOP thấp.

4.3 Thí nghiệm Bổ sung

Sử dụng regularizer độ trễ. Trong Eq 6, chúng tôi đề xuất một surrogate độ trễ để tối ưu hóa độ trễ suy luận thực tế trên thiết bị. Trong mục này, chúng tôi cung cấp bằng chứng thực nghiệm về hiệu quả của cách tiếp cận này cho MobileNetv3 trên Pixel 6. Chúng tôi so sánh các đường cong độ chính xác-độ trễ của các mô hình được tạo ra bằng cách sử dụng regularizer FLOPs, độ trễ (xem Hình 4). Quan sát rằng việc sử dụng regularizer độ trễ dẫn đến các mô hình với độ trễ nhỏ hơn và do đó đánh đổi độ trễ-độ chính xác tốt hơn so với việc sử dụng regularizer FLOP. Chúng tôi cũng thấy rằng các mô hình này có hiệu suất tốt hơn so với MobileNetV3 (cải thiện 0.5−2% về độ chính xác với độ trễ tương tự), mặc dù MobileNetv3 được tạo thủ công để suy luận nhanh hơn trên các thiết bị di động.

Lượng tử hóa. Trong tập thí nghiệm này, chúng tôi xem xét phân loại CIFAR-10 và nén một CNN 3 lớp bằng cách sử dụng lượng tử hóa. Chúng tôi sử dụng công thức lượng tử hóa được trình bày trong Bảng 1 và tìm kiếm trên lượng tử hóa {2,4,8,16} bit cho mỗi lớp. Chúng tôi so sánh với một baseline sử dụng cùng mức độ lượng tử hóa ở mỗi lớp. Hình 5 trình bày kết quả từ thí nghiệm này. Chi tiết về việc thực hiện có thể được tìm thấy trong phụ lục. Chúng tôi thấy rằng kỹ thuật của chúng tôi nén kích thước mô hình gần 55% mà không giảm độ chính xác (so với một mô hình với trọng số 16-bit). Kỹ thuật của chúng tôi cũng xuất ra một mô hình chính xác hơn 1.4% so với một mô hình lượng tử hóa 2-bit chỉ với 4% FLOPs nhiều hơn. Trong biểu đồ bên phải trong Hình 5, chúng tôi trực quan hóa các độ rộng bit học được của các mô hình. Chúng tôi thấy rằng các lớp sau được gán độ rộng bit nhỏ hơn, chỉ ra tầm quan trọng của việc học các bộ lọc biểu cảm sớm trong mạng. Các mô hình khác nhau trong biểu đồ của chúng tôi được tìm thấy bằng cách thay đổi giá trị của hệ số regularizer, và do đó không cần tìm kiếm tổ hợp trên độ rộng bit.

5 Kết luận và Công việc Tương lai

Trong công trình này, chúng tôi đã trình bày một kỹ thuật đầu-cuối-đầu cho nén mạng nơ-ron. Cách tiếp cận của chúng tôi áp dụng cho nhiều loại khối hiệu quả bao gồm tỉa, thưa thớt không có cấu trúc, lượng tử hóa. Tại cốt lõi của thuật toán của chúng tôi là một surrogate mới cho FLOPs, độ trễ dựa vào chuẩn ℓ1
ℓ2, và hoạt động với batchnorm, layernorm. Thuật toán của chúng tôi hiệu quả về mặt tính toán và chạy trong cùng thời gian cần thiết để huấn luyện một mô hình đơn. Chúng tôi đã chứng minh hiệu quả của cách tiếp cận trên các tác vụ pre-training và học chuyển giao khác nhau trên các benchmark ngôn ngữ và thị giác tiêu chuẩn. Như một công việc tương lai, sẽ hữu ích khi kết hợp thêm các khối xây dựng hiệu quả như ma trận đường chéo khối vào framework của chúng tôi. Một hướng thú vị khác sẽ là làm cho kỹ thuật của chúng tôi nhận biết phần cứng hơn bằng cách kết hợp các tham số cấp phần cứng như tiling vào quá trình tìm kiếm của chúng tôi.

Tài liệu tham khảo
[1]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[2]Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105–6114. PMLR, 2019.
[3]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[4]Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.
[5]Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544–5555. PMLR, 2020.
[6]Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. arXiv preprint arXiv:1910.04732, 2019.
[7]Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021.
[8]Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Knowledge Discovery and Data Mining, 2006.
[9]Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with feedback. arXiv preprint arXiv:2006.07253, 2020.
[10] Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing fine-tuning and rewinding in neural network pruning. In International Conference on Learning Representations, 2020.
[11] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. Morphnet: Fast & simple resource-constrained structure learning of deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1586–1595, 2018.
[12] Shraman Ray Chaudhuri, Elad Eban, Hanhan Li, Max Moroz, and Yair Movshovitz-Attias. Fine-grained stochastic architecture search. arXiv preprint arXiv:2006.09581, 2020.
[13] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312, 2017.
[14] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.
[15] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820–2828, 2019.
[16] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.
[17] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing. Neural architecture search with bayesian optimisation and optimal transport. arXiv preprint arXiv:1802.07191, 2018.

--- TRANG 9 ---
[18] Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, and Quoc V Le. Can weight sharing outperform random architecture search? an investigation with tunas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14323–14332, 2020.
[19] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European Conference on Computer Vision (ECCV), pages 285–300, 2018.
[20] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018.
[21] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers. arXiv preprint arXiv:2204.09656, 2022.
[22] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020.
[23] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.
[24] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends ®in Optimization, 1(3):127–239, 2014.
[25] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1314–1324, 2019.
[26] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.
[27] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In Uncertainty in artificial intelligence, pages 367–377. PMLR, 2020.
[28] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In International Conference on Machine Learning, pages 4095–4104. PMLR, 2018.
[29] Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan Yuille, and Jianchao Yang. Atomnas: Fine-grained end-to-end neural architecture search. arXiv preprint arXiv:1912.09640, 2019.
[30] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019.
[31] Grace Chu, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. Discovering multi-hardware mobile models via architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3022–3031, 2021.
[32] Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. Mcunet: Tiny deep learning on iot devices. arXiv preprint arXiv:2007.10319, 2020.
[33] Zhen Dong, Yizhao Gao, Qijing Huang, John Wawrzynek, Hayden KH So, and Kurt Keutzer. Hao: Hardware-aware neural architecture optimization for efficient inference. In 2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pages 50–59. IEEE, 2021.
[34] Li Lyna Zhang, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu. Fast hardware-aware neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 692–693, 2020.
[35] Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, Smail Niar, Martin Wistuba, and Naigang Wang. A comprehensive survey on hardware-aware neural architecture search. arXiv preprint arXiv:2101.09336, 2021.
[36] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.

--- TRANG 10 ---
[37] Ehud D Karnin. A simple procedure for pruning back-propagation trained neural networks. IEEE transactions on neural networks, 1(2):239–242, 1990.
[38] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
[39] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11264–11272, 2019.
[40] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. Advances in neural information processing systems, 29, 2016.
[41] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in Neural Information Processing Systems, 30, 2017.
[42] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
[43] Zhiyun Lu, Vikas Sindhwani, and Tara N Sainath. Learning compact recurrent neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5960–5964. IEEE, 2016.
[44] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Wenrui Dai, Yingyong Qi, Yiran Chen, Weiyao Lin, and Hongkai Xiong. Trained rank pruning for efficient deep neural networks. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 14–17. IEEE, 2019.
[45] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on computing, 24(2):227–234, 1995.
[46] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267–288, 1996.
[47] Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep Ravikumar. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Advances in neural information processing systems, 22, 2009.
[48] Jihun Yun, Aurélie C Lozano, and Eunho Yang. Adaptive proximal gradient methods for structured neural networks. Advances in Neural Information Processing Systems, 34:24365–24378, 2021.
[49] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015.
[50] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[51] Penghang Yin, Ernie Esser, and Jack Xin. Ratio and difference of l_1 and l_2 norms and sparse representation with coherent dictionaries. Communications in Information and Systems, 14(2):87–109, 2014.
[52] Yaghoub Rahimi, Chao Wang, Hongbo Dong, and Yifei Lou. A scale-invariant approach for sparse signal recovery. SIAM Journal on Scientific Computing, 41(6):A3649–A3672, 2019.
[53] Chao Wang, Ming Yan, Yaghoub Rahimi, and Yifei Lou. Accelerated schemes for the l_1/l_2 minimization. IEEE Transactions on Signal Processing, 68:2660–2669, 2020.
[54] Huanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures. arXiv preprint arXiv:1908.09979, 2019.
[55] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
[56] Helmuth Späth. One dimensional spline interpolation algorithms. AK Peters/CRC Press, 1995.
[57] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013.
[58] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014.

--- TRANG 11 ---
[59] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
[60] Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. EBERT: Efficient BERT inference with dynamic structured pruning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4814–4823, Online, August 2021. Association for Computational Linguistics.
[61] Zi Lin, Jeremiah Liu, Zi Yang, Nan Hua, and Dan Roth. Pruning redundant mappings in transformer models via spectral-normalized identity prior. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 719–730, Online, November 2020. Association for Computational Linguistics.
[62] François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush. Block pruning for faster transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10619–10629, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
[63] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6151–6162, Online, November 2020. Association for Computational Linguistics.
[64] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513–1528, Dublin, Ireland, May 2022. Association for Computational Linguistics.
[65] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, jan 2023.
[66] Weihan Chen, Peisong Wang, and Jian Cheng. Towards mixed-precision quantization of neural networks via constrained optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5350–5359, 2021.
[67] Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso Garcia, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precision dnns: All you need is a good parametrization. arXiv preprint arXiv:1905.11452, 2019.
[68] Mart Van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. Advances in neural information processing systems, 33:5741–5752, 2020.
[69] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
[70] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.
[71] Maying Shen, Hongxu Yin, Pavlo Molchanov, Lei Mao, Jianna Liu, and Jose M Alvarez. Halp: hardware-aware latency pruning. arXiv preprint arXiv:2110.10811, 2021.
[72] Yanyu Li, Pu Zhao, Geng Yuan, Xue Lin, Yanzhi Wang, and Xin Chen. Pruning-as-search: Efficient neural architecture search via channel pruning and structural reparameterization. arXiv preprint arXiv:2206.01198, 2022.

--- TRANG 12 ---
Phụ lục

A Tái tham số hóa cho Lượng tử hóa

Trong mục này, chúng tôi trình bày tham số hóa của Wi cho lượng tử hóa. Tương tự như bài báo chính, chúng tôi xem xét một FFN. Đối với mỗi lớp của mạng, chúng tôi muốn tìm kiếm trên lượng tử hóa {1,2,4. . . B} bit của trọng số của nó3. Gọi Wi là ma trận trọng số của lớp i. Gọi clip(Wi;ri,l, ri,u) là ma trận trọng số bị hạn chế trong [ri,l, ri,u]

clip(Wi;ri,l, ri,u) =ri,u−ReLU (ri,u−ri,l−ReLU (Wi−ri,l)).

Khi rõ ràng từ ngữ cảnh, chúng tôi sử dụng ký hiệu viết tắt clip(Wi) để biểu thị clip(Wi;ri,l, ri,u). Gọi Wi,b là lượng tử hóa b-bit của Wi, và gọi r(b)
i,l, r(b)
i,u là các tham số phạm vi liên quan đến Wi,b. Wi,b được thu được bằng cách chia đều phạm vi (r(b)
i,u−r(b)
i,l) thành 2b điểm và gán mỗi phần tử của Wi đến điểm lưới gần nhất

Wi,b=r(b)
i,l+r(b)
i,u−r(b)
i,l
2b−1$
clip(Wi)−r(b)
i,l
(r(b)
i,u−r(b)
i,l)/(2b−1)'
.

Ở đây ⌊·⌉ biểu thị hàm làm tròn đến số nguyên gần nhất. Để chọn giữa Wi,1, Wi,2. . . W i,B, chúng tôi giới thiệu các biến mask nhị phân αi,1, αi,2. . . α i,B. Điều này dẫn chúng tôi đến tham số hóa sau của Wi

αi,1(Wi,1+αi,2(Wi,2−Wi,1+αi,4(Wi,4−Wi,2+αi,8(. . .)))) (7)

αi,b= 0 ngụ ý rằng các trọng số có thể được tham số hóa với ít hơn b bit. Quan sát rằng biểu thức trên có thể được viết lại là

αi,1(1−αi,2)Wi,1+αi,1αi,2(1−αi,4)Wi,2+αi,1αi,2αi,4(1−αi,8)Wi,4. . .

FLOPs cần thiết để tính đầu ra của lớp này được cho bởi

[∥αi,1(1−αi,2)∥0+ 2∥αi,1αi,2(1−αi,4)∥0+ 4∥αi,1αi,2αi,4(1−αi,8)∥0+. . .]didi+1

Vì tìm kiếm các mask nhị phân không khả thi về mặt tính toán, chúng tôi làm chúng liên tục; tức là, chúng tôi để αi,b∈[0,1],∀b∈ {1,2,4, . . . B}. Chúng tôi xem xét một surrogate FLOPs liên tục được thu được bằng cách tính chuẩn ℓ1/ℓ2 của

[αi,1(1−αi,2),2αi,1αi,2(1−αi,4),4αi,1αi,2αi,4(1−αi,8). . .].

Điều này dẫn chúng tôi đến regularizer sau

αi,1(1−αi,2) + 2αi,1αi,2(1−αi,4) + 4αi,1αi,2αi,4(1−αi,8). . .p
(αi,1(1−αi,2))2+ (2αi,1αi,2(1−αi,4))2+ (4αi,1αi,2αi,4(1−αi,8))2. . .didi+1.

Nhận xét 1. Regularizer ℓ1/ℓ2 của chúng tôi thường xuất ra αi gần với 0/1. Trong nhiều trường hợp, chúng thực sự bằng chính xác 0/1. Trong trường hợp chúng không bằng 0,1, chúng tôi chiếu αi,b lên {0,1}. Điều này đảm bảo chúng tôi có trọng số lượng tử hóa.

Nhận xét 2. Có một số công trình khác đã cố gắng học lượng lượng tử hóa/độ chính xác để sử dụng tại mỗi lớp [66,67,68]. Tuy nhiên, không giống như công trình của chúng tôi, các công trình này không tối ưu hóa trực tiếp cho FLOPs, độ trễ. Chúng tôi muốn lưu ý rằng tham số hóa của chúng tôi có liên quan chặt chẽ đến tham số hóa của [68].

Straight Through Estimator (STE). Lưu ý rằng mục tiêu huấn luyện cho lượng tử hóa không khả vi. Vì vậy, trong các thí nghiệm của chúng tôi, chúng tôi sử dụng STE để tối ưu hóa mục tiêu [69]. Đây là một kỹ thuật tiêu chuẩn để thực hiện huấn luyện nhận biết lượng tử hóa.

3B thường là lũy thừa của 2

--- TRANG 13 ---
[Các biểu đồ và số liệu thống kê]

Hình 6: Đánh đổi fine-tuning của BERT trên benchmark GLUE.

B Chi tiết Thực hiện và thí nghiệm

Trong mục này, chúng tôi cung cấp chi tiết bổ sung về việc thực hiện kỹ thuật của chúng tôi. Chúng tôi khởi động ấm quy trình tỉa của mình với mô hình được pre-train (tức là, f∗ trong Mục 3) được cung cấp cho chúng tôi. Trong các thí nghiệm của chúng tôi, chúng tôi nhận thấy rằng điều này tăng tốc sự hội tụ của thuật toán. Đối với cả nén MobileNetV3 và BERT, chúng tôi dựa vào tỉa đồng thời, phân rã hạng thấp của trọng số (xem Bảng 1 để biết chi tiết). Ở đây, chúng tôi tham số hóa trọng số Wi là Uidiag(βi)Vidiag(αi); việc đặt các mục của βi thành 0 giúp giảm hạng của ma trận trọng số, và αi giúp trong tỉa. Chúng tôi khởi tạo Ui, Vi, βi bằng cách thực hiện SVD trên các ma trận trọng số của mạng được pre-train. Trong các thí nghiệm của chúng tôi, chúng tôi áp dụng kỹ thuật của mình chỉ cho các lớp convolution 1×1 trong mạng, mà công thức của regularizer của chúng tôi vẫn giống như được mô tả trong văn bản trước. Chúng tôi anneal cường độ chính quy hóa λ, tăng nó tuyến tính để ổn định huấn luyện. Cuối cùng, chúng tôi fine-tune mô hình được trả về bởi thuật toán tỉa của chúng tôi trên dữ liệu huấn luyện để cải thiện hiệu suất của nó (điều này đơn giản liên quan đến việc đặt hệ số chính quy hóa FLOPs thành 0). Trong các giai đoạn fine-tuning và tỉa, chúng tôi tận dụng mô hình được pre-train bằng cách thêm hàm mất chưng cất vào hàm mất entropy chéo tiêu chuẩn [70]. Chúng tôi thực hiện chưng cất giữa logit của mô hình được pre-train và logit của mô hình đang được fine-tune.

B.1 Pretraining ImageNet

Thuật toán của chúng tôi được thực hiện bằng TensorFlow 2.0. Chúng tôi sử dụng các mô hình MobileNetV3 (hoặc EfficientNet) được pre-train được cung cấp trong framework này để khởi động ấm các mô hình của chúng tôi. Chúng tôi khởi tạo Ui, βi, Vi với SVD của các bộ lọc convolution 1x1, và các mục của αi đồng nhất ngẫu nhiên giữa [0,0.5]. Chúng tôi sử dụng Adam để tối ưu hóa với các tham số mặc định của nó, và tìm kiếm trên tỷ lệ học trong tập {10−4,5×10−5,10−5}, với cosine decay, là thực hành tiêu chuẩn. Hệ số chưng cất được tìm kiếm trong số {0.1,0.25,0.5,0.9} và nhiệt độ chưng cất được tìm kiếm trong số {2,3,4}. Đối với các thí nghiệm ImageNet của chúng tôi, Chúng tôi huấn luyện mô hình của mình trong 70000 bước, anneal tuyến tính regularizer trong 50000 bước đầu tiên. Chúng tôi fine-tune mô hình thu được trong 50000 bước khác. Đối với các thí nghiệm học chuyển giao, chúng tôi giảm chúng xuống 25000 cho huấn luyện và 15000 cho fine-tuning. Chúng tôi sử dụng kích thước batch 2048 cho tất cả các thí nghiệm. Hệ số regularizer của chúng tôi được thay đổi từ 10−8 đến 10−6. Phạm vi này được xác định bằng cách nhìn vào độ lớn của hàm mất entropy chéo và regularizer FLOPs, và đảm bảo rằng chúng tương tự. Pre-training MobileNet mất khoảng 13 giờ.

B.2 Học Chuyển giao

BERT. Đối với các thí nghiệm fine-tuning BERT, chúng tôi bắt đầu với một mô hình BERT được pre-train và giới thiệu tham số hóa của chúng tôi theo cách tương tự như được mô tả ở trên. Chúng tôi sử dụng AdamW để tối ưu hóa, và tìm kiếm trên tỷ lệ học trong số {10−4,5×10−5,10−5}. Hệ số regularizer của chúng tôi được thay đổi từ 10−7 đến 5∗10−6. Mỗi lần chạy fine-tuning mất từ 20 phút - 1 giờ.

EfficientNet, MobileNet. Đối với các thí nghiệm EfficientNet và MobileNet, chúng tôi có thiết lập thí nghiệm và không gian tìm kiếm siêu tham số tương tự như pretraining MobileNet ImageNet được mô tả trong Phụ lục B.1, với ngoại lệ là chúng tôi không thực hiện bất kỳ chưng cất mô hình nào. Chúng tôi cũng sử dụng RMSProp cho EfficientNet với exponential decay như lịch trình LR, vì đây là optimizer được lựa chọn cho pre-training của nó. Chúng tôi huấn luyện trong 25000 bước với regularizer, và fine-tune trong 25000 bước khác.

--- TRANG 14 ---
B.3 Lượng tử hóa

Chúng tôi huấn luyện một CNN với bốn lớp convolution, với [64,128,256,128] bộ lọc và kích thước kernel 3 với stride là 1 cho mỗi lớp. Chúng tôi thêm có các lớp batch-norm sau mỗi lớp conv. Chúng tôi tìm kiếm tỷ lệ học trên {1e-4, 5e-4, 1e-3, 5e-3} cho baseline và mô hình của chúng tôi, và hệ số regularizer trên {1e-9, 3e-9, 5e-9, 7e-9, 1e-8}. Chúng tôi huấn luyện trong 100 epoch với kích thước batch 512 trên một GPU V100 đơn, và sử dụng Adam với CosineDecay cho tỷ lệ học.

B.4 Bảng Độ trễ

Như được đề cập trong bài báo chính, các độ trễ thực tế trên thiết bị được tính toán trên Pixel6 cho các thí nghiệm độ trễ của chúng tôi. Chúng tôi tạo bảng tra cứu độ trễ T được chỉ định trong Mục 3.3 bằng cách profiling độ trễ convolution/nhân ma trận-vector 1×1 tương ứng, trên thiết bị. Lưu ý rằng phép toán convolution được tối ưu hóa tốt hơn nhiều so với phép toán nhân ma trận trên kernel Pixel6. Do đó, đối với các thí nghiệm độ trễ của chúng tôi trên MobileNet, bảng độ trễ được tạo bằng cách profiling các phép toán convolution 1×1.

Một phép toán convolution 1x1 được xác định bởi chiều đầu vào, kênh đầu vào và số bộ lọc (kênh đầu ra). Stride cũng có thể khác nhau nhưng tất cả convolution 1x1 trong kiến trúc MobileNet có stride 1. Trong kiến trúc MobileNet chúng tôi gặp các feature map với các chiều đầu vào indim∈I={1,7,14,28,56,112,224}. Hơn nữa, các kênh đầu vào (inc) và đầu ra (outc) bị ràng buộc bởi inc,outc∈D={d|∀d∈N và d <1281}. Do đó chúng tôi xây dựng bảng T, mỗi thành viên có thể được truy cập qua T(indim,inc,outc). Lưu ý rằng profiling T(indim,inc,outc). cho mọi giá trị có thể của (indim,inc,outc)∈I×D×D là đắt đỏ. Do đó chúng tôi phải chọn một số tuple (inc,outc) cho mỗi indim∈I mà chúng tôi tính toán độ trễ thực tế trên thiết bị. Phần còn lại của bảng được tạo bằng nội suy tuyến tính. Chúng tôi chọn các tuple này sao cho chúng bao phủ các convolution 1×1 gặp phải trong Kiến trúc MobileNet. Đối với indim=α, gọi β biểu thị giá trị tối đa có thể của inc, và γ biểu thị giá trị tối đa có thể của outc trong MobileNet. Chúng tôi xây dựng tập Pin biểu thị các giá trị có khả năng được gặp phải bởi regularizer cho inc và tương tự Pout cho outc. Cuối cùng, các độ trễ thực tế trên thiết bị được tính toán cho T(α, Pin×Pout). Xây dựng Pin và Pout được thực hiện bằng cách chọn một θ phù hợp và thêm tất cả các giá trị trong phạm vi (β−θ, β] vào Pin, và (γ−θ, γ] vào Pout. Ngoài ra, từ các phạm vi còn lại tức là (0, β−θ] và (0, γ−θ] các điểm được lấy mẫu theo cấp số nhân bằng cách chọn điểm giữa của phạm vi mỗi lần và thay đổi giới hạn dưới của phạm vi thành điểm giữa cho một số lần lặp nhất định.

Thiết lập thí nghiệm và cấu hình siêu tham số chúng tôi sử dụng cho các thí nghiệm bảng độ trễ giống như cho các thí nghiệm FLOPs (xem Mục B.1).

C Kết quả Thí nghiệm Bổ sung

C.1 Pretraining ResNet trên ImageNet

Trong mục này, chúng tôi trình bày kết quả thí nghiệm bổ sung để chứng minh tính tổng quát của cách tiếp cận. Chúng tôi nén kiến trúc ResNet cho phân loại ImageNet, sử dụng phương pháp của chúng tôi. Cụ thể, chúng tôi nén các convolution 1×1 bằng tỉa và phân rã hạng thấp. Chúng tôi so sánh phương pháp của mình với HALP [71] và PAS [72], hai phương pháp tiên tiến cho tìm kiếm kiến trúc và nén cho ResNet. Phương pháp của chúng tôi nén ResNet-101 thành một mô hình với FLOPs tương tự như ResNet-50, trong khi đồng thời đạt được hiệu suất tốt hơn so với baseline ResNet-50. Hơn nữa, kỹ thuật của chúng tôi vượt trội so với các phương pháp SOTA với cùng số FLOPs, như thấy trong Hình 7. Chúng tôi sử dụng cùng siêu tham số như được mô tả trong Mục B.1, nhưng chúng tôi thay đổi hệ số regularizer FLOP giữa [1e−10,1e−9] vì các mô hình ResNet có số FLOPs cao hơn.

C.2 Nghiên cứu Ablation

Ảnh hưởng của chuẩn thưa thớt. Trong mục 3 chúng tôi cung cấp các thí nghiệm quy mô nhỏ để biện minh cho các lựa chọn thiết kế của chúng tôi về việc sử dụng projected-Adam và chuẩn ℓ1
ℓ2. Trong mục này chúng tôi thực hiện các nghiên cứu ablation quy mô lớn trên MobileNetV3 cho huấn luyện ImageNet. Kết quả từ thí nghiệm này được trình bày trong Hình 8. Không có projected-Adam, chúng tôi nhận thấy rằng thuật toán tối ưu hóa không hội tụ đến các giải pháp thưa thớt. Do đó, các mô hình kết quả không có giảm lớn trong MAC. Độ chính xác của

--- TRANG 15 ---
[Các biểu đồ và số liệu thống kê]

Hình 7: Tỉa ResNet trên ImageNet: Chúng tôi so sánh với HALP và PAS, hai kỹ thuật SOTA gần đây để tỉa ResNet-50, và đạt được hiệu suất tốt hơn trên các chế độ FLOP khác nhau.

[Các biểu đồ và số liệu thống kê]

Hình 8: Nghiên cứu ablation trên ImageNet: Chúng tôi so sánh việc sử dụng chuẩn ℓ1 và ℓ1
ℓ2 trong regularizer của chúng tôi, với chỉ số dưới p chỉ ra rằng projected-Adam đã được sử dụng để tối ưu hóa. Chúng tôi cũng thí nghiệm với việc kết hợp phân rã hạng thấp (LR) với tỉa kênh.

các mô hình này cũng bị ảnh hưởng lớn. Mặt khác, việc sử dụng regularizer FLOPs dựa trên chuẩn ℓ1 với projected-Adam gặp phải vấn đề scaling được mô tả trong Mục 3.2. Điều này dẫn đến một phần lớn các kênh bị tỉa cho một số khối, tạo ra một mô hình với độ chính xác giảm sút. Phương pháp của chúng tôi có độ chính xác tốt hơn 2-4% trong các chế độ FLOPs cao và trung bình so với các lựa chọn thay thế này.

So sánh các khối xây dựng khác nhau. Trong Bảng 1, chúng tôi mô tả cách tích hợp các khối xây dựng khác nhau vào framework của chúng tôi. Trong Hình 8, chúng tôi chứng minh đánh đổi độ chính xác vs thời gian suy luận của việc sử dụng hai trong số các khối xây dựng này trong framework của chúng tôi, cụ thể là Tỉa và Tỉa+Phân rã Hạng Thấp. Chúng tôi thấy rằng tính linh hoạt bổ sung được cung cấp bởi Phân rã Hạng Thấp dẫn đến các mô hình với ít MAC hơn với cùng độ chính xác, và sự khác biệt thậm chí còn rõ rệt hơn đối với các mô hình nhỏ hơn. Chúng tôi lưu ý rằng chỉ tỉa kênh có thể cho chúng tôi giảm 10% MAC so với họ MobileNetV3 ở cùng mức độ chính xác. Cụ thể, ở độ chính xác 73.4%, mô hình của chúng tôi có 136Mn MAC so với 155Mn MAC của mô hình họ MobileNetV3. Tương tự, ở độ chính xác 75.5%, mô hình của chúng tôi có 198Mn MAC so với 216Mn MAC của mô hình họ MobileNetV3. Thêm cấu trúc Hạng Thấp giới thiệu thêm 5% giảm MAC so với lợi ích từ tỉa kênh, mà không mất độ chính xác. Điều này cũng cho thấy hiệu quả của thuật toán của chúng tôi qua nhiều khối xây dựng. Kết hợp các khối hiệu quả khác như thưa thớt có cấu trúc khối, lượng tử hóa là một hướng điều tra tương lai.

D Kết hợp các khối xây dựng

Bảng 2 trình bày tham số hóa của các ma trận trọng số cho phép chúng tôi tìm kiếm trên nhiều khối xây dựng đồng thời.

--- TRANG 16 ---
Bảng 2: Bảng mô tả các regularizer được sử dụng bởi kỹ thuật của chúng tôi cho các khối xây dựng hiệu quả khác nhau.

Khối Xây dựng
Hiệu quả | Tham số hóa của Wi | FLOPs
(lớp thứ i) | Regularizer (FLOPs surrogate)
(lớp thứ i)

Tỉa +
Thưa thớt Không có Cấu trúc | (Wi⊙βi)×diag(αi),
⊙ là toán tử nhân theo từng phần tử | ∥Vec(βi×diag(αi))∥0 | √
didi+1∥Vec(βi×diag(αi))∥1p
∥Vec(βi×diag(αi))∥2

Tỉa +
Lượng tử hóa
(lượng tử hóa 1,2,4bit) | Wi,1+αi,2(∆i,2+αi,4(∆i,4))
×diag(βi), | ∥(1−αi,2)∥0+
2∥αi,2(1−αi,4)∥0+
4∥αi,2αi,4∥0
× ∥βi∥0∥βi+1∥0 | chuẩn ℓ1
ℓ2 của
vector [(1−αi,2),
2αi,2(1−αi,4),4αi,2αi,4]
×√di∥βi∥1p
∥βi∥2√
di+1∥βi+1∥1p
∥βi+1∥2

E Hạn chế và Tác động Rộng lớn

Một hạn chế của công trình của chúng tôi là chúng tôi chỉ nghiên cứu các khối xây dựng phổ biến như thưa thớt, tỉa, phân rã hạng thấp và lượng tử hóa. Mở rộng công trình của chúng tôi đến nhiều khối xây dựng hơn như thưa thớt khối và các dạng thưa thớt có cấu trúc khác là một hướng tương lai thú vị. Một hạn chế khác, liên quan đến việc thực hiện kỹ thuật của chúng tôi, là nhu cầu thực hiện thủ công regularizer FLOPs cho các kiến trúc khác nhau. Một giải pháp tự động nhận bất kỳ kiến trúc nào và tính toán regularizer FLOPs sẽ làm cho framework của chúng tôi dễ sử dụng.

Về tác động rộng lớn, chúng tôi tin rằng kỹ thuật của chúng tôi có thể được sử dụng để tìm các kiến trúc hiệu quả hơn cho các mô hình ngôn ngữ lớn như GPT. Điều này có thể giúp dân chủ hóa các mô hình này, và cũng giảm dấu chân carbon của chúng.
