# 2207.06968.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2207.06968.pdf
# Kích thước tệp: 1932314 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
DASS: TÌMKIẾM KIẾN TRÚC KHẢ VI PHÂN CHO
MẠNG NEURAL THƯA
Hamid Mousavi
Đại học Mälardalen
seyedhamidreza.mousavi@mdu.seMohammad Loni
Đại học Mälardalen
mohammad.loni@mdu.se
Mina Alibeigi
mina.alibeigi@zenseact.com
Masoud Daneshtalab
Đại học Mälardalen
masoud.daneshtalab@mdu.se
TÓM TẮT
Việc triển khai Mạng Neural Sâu (DNN) trên các thiết bị biên gặp trở ngại bởi khoảng cách đáng kể giữa yêu cầu hiệu suất và sức mạnh xử lý có sẵn. Trong khi nghiên cứu gần đây đã đạt được những bước tiến đáng kể trong việc phát triển các phương pháp cắt tỉa để xây dựng mạng thưa nhằm giảm chi phí tính toán của DNN, vẫn còn tổn thất độ chính xác đáng kể, đặc biệt ở tỷ lệ cắt tỉa cao. Chúng tôi phát hiện rằng các kiến trúc được thiết kế cho mạng dày đặc bởi các phương pháp tìm kiếm kiến trúc khả vi phân trở nên không hiệu quả khi áp dụng cơ chế cắt tỉa. Lý do chính là phương pháp hiện tại không hỗ trợ kiến trúc thưa trong không gian tìm kiếm của chúng và sử dụng mục tiêu tìm kiếm được tạo cho mạng dày đặc và không chú ý đến độ thưa.

Trong bài báo này, chúng tôi đề xuất một phương pháp mới để tìm kiếm các kiến trúc neural thân thiện với độ thưa. Chúng tôi thực hiện điều này bằng cách thêm hai phép toán thưa mới vào không gian tìm kiếm và chỉnh sửa mục tiêu tìm kiếm. Chúng tôi đề xuất hai phép toán SparseConv và SparseLinear tham số mới để mở rộng không gian tìm kiếm bao gồm các phép toán thưa. Đặc biệt, các phép toán này tạo ra không gian tìm kiếm linh hoạt do sử dụng các phiên bản tham số thưa của các phép toán tuyến tính và tích chập. Mục tiêu tìm kiếm được đề xuất cho phép chúng tôi huấn luyện kiến trúc dựa trên độ thưa của các phép toán không gian tìm kiếm. Phân tích định lượng chứng minh rằng các kiến trúc tìm kiếm của chúng tôi vượt trội hơn những kiến trúc được sử dụng trong mạng thưa tiên tiến trên bộ dữ liệu CIFAR-10 và ImageNet. Về hiệu suất và hiệu quả phần cứng, DASS tăng độ chính xác của phiên bản thưa của MobileNet-v2 từ 73.44% lên 81.35% (cải thiện +7.91%) với thời gian suy luận nhanh hơn 3.87 lần.

Từ khóa Tìm Kiếm Kiến Trúc Neural, Cắt Tỉa, Nén Mạng

1 Giới thiệu
Mạng Neural Sâu (DNN) cung cấp một con đường xuất sắc để có được khả năng trích xuất đặc trưng tối đa cần thiết để giải quyết các tác vụ thị giác máy tính phức tạp cao [64, 27, 73, 61]. Có một nhu cầu ngày càng tăng để DNN trở nên hiệu quả hơn nhằm được triển khai trên các thiết bị biên có tài nguyên cực kỳ hạn chế. Tuy nhiên, DNN không được điều chỉnh nội tại cho khả năng tính toán và bộ nhớ hạn chế của các thiết bị biên nhỏ, cấm việc triển khai chúng trong các ứng dụng như vậy [18, 57, 51, 50, 56].

Để dân chủ hóa việc tăng tốc DNN, nhiều phương pháp tối ưu hóa đã được đề xuất, bao gồm cắt tỉa mạng [68, 49, 83, 13], thiết kế kiến trúc hiệu quả [57, 50], lượng tử hóa mạng [55, 7, 38], chưng cất kiến thức [31, 20], và phân rã hạng thấp [36]. Đặc biệt, cắt tỉa mạng được biết đến là cung cấp việc tiết kiệm tính toán và bộ nhớ đáng kể bằng cách loại bỏ các tham số trọng số dư thừa trong kịch bản không có cấu trúc [68, 2, 49, 83, 24], và toàn bộ bộ lọc trong kịch bản có cấu trúc [29, 30, 28, 86, 84]. Gần đây, các phương pháp cắt tỉa không có cấu trúc báo cáo cung cấp việc giảm kích thước mạng cực đoan. Các phương pháp cắt tỉa không có cấu trúc tiên tiến [68] cung cấp tỷ lệ cắt tỉa lên đến 99% điều này là một kịch bản xuất sắc cho các thiết bị biên nhỏ.

Tuy nhiên, các phương pháp này gặp phải sự sụt giảm độ chính xác đáng kể, cản trở chúng khỏi được áp dụng trong thực tế (giảm độ chính xác ≈19% cho MobileNet-v2 so với mạng dày đặc [68]). Các phương pháp cắt tỉa hiện tại sử dụng các kiến trúc thủ công được thiết kế mà không quan tâm đến độ thưa [2, 49, 83, 86, 68]. Chúng tôi giả thuyết rằng kiến trúc xương sống có thể không tối ưu cho các kịch bản với tỷ lệ cắt tỉa cực đoan. Thay vào đó, chúng tôi có thể học các kiến trúc xương sống hiệu quả hơn có thể thích ứng với các kỹ thuật cắt tỉa bằng cách khám phá không gian của mạng thưa.

Tìm Kiếm Kiến Trúc Neural (NAS) đã đạt được thành công lớn trong việc tự động thiết kế các kiến trúc DNN hiệu suất cao. Các phương pháp tìm kiếm kiến trúc khả vi phân (DARTS) [52, 75, 76] là một phương pháp NAS phổ biến sử dụng thuật toán tìm kiếm dựa trên gradient để tăng tốc độ tìm kiếm. Được thúc đẩy bởi kết quả hứa hẹn của NAS, chúng tôi đã nghĩ ra ý tưởng thiết kế các kiến trúc xương sống tùy chỉnh tương thích với các phương pháp cắt tỉa. Tuy nhiên, không gian tìm kiếm của các thuật toán DARTS hiện tại bao gồm các phép toán tích chập và tuyến tính dày đặc không có khả năng khám phá xương sống chính xác cho việc cắt tỉa. Để chứng minh vấn đề này, chúng tôi đầu tiên cắt tỉa 99% trọng số từ kiến trúc tốt nhất được thiết kế bởi phương pháp NAS [52] với không gian tìm kiếm cơ sở mà không quan tâm đến độ thưa. Đáng thất vọng, sau khi áp dụng phương pháp cắt tỉa cho kiến trúc cuối cùng, nó hoạt động kém với tổn thất độ chính xác lên đến ≈21% trong nén bởi DASS mở rộng không gian tìm kiếm bằng các phép toán thưa. (Phần 4). Thất bại này là do thiếu hỗ trợ cho các đặc điểm mạng thưa cụ thể dẫn đến hiệu suất tổng quát hóa thấp. Dựa trên giả thuyết trên và quan sát thực nghiệm, chúng tôi xây dựng một không gian tìm kiếm bao gồm các phép toán thưa và dày đặc. Do đó, các phép toán tích chập và tuyến tính gốc trong không gian tìm kiếm của NAS đã được mở rộng bởi các phép toán SparseConv và SparseLinear tham số, tương ứng. Hơn nữa, để tạo sự nhất quán giữa không gian tìm kiếm được đề xuất và hàm mục tiêu tìm kiếm, chúng tôi chỉnh sửa bài toán tối ưu hóa hai cấp để tính đến độ thưa. Theo cách này, quá trình tìm kiếm cố gắng tìm ra phép toán thưa tốt nhất bằng cách tối ưu hóa cả tham số kiến trúc và tham số cắt tỉa. Sự chỉnh sửa này tạo ra một bài toán tối ưu hóa hai cấp phức tạp. Để giải quyết khó khăn này, chúng tôi chia bài toán tối ưu hóa hai cấp phức tạp thành hai bài toán tối ưu hóa hai cấp đơn giản và giải quyết chúng.

Chúng tôi cho thấy việc tích hợp rõ ràng cắt tỉa vào quy trình tìm kiếm có thể dẫn đến việc tìm ra các kiến trúc mạng thưa với sự cải thiện độ chính xác đáng kể. Trong Hình 1, chúng tôi so sánh độ chính xác Top-1 CIFAR-10 và số lượng tham số của kiến trúc được tìm thấy bởi DASS với mạng thưa (cắt tỉa không có cấu trúc) và dày đặc tiên tiến. Kết quả cho thấy kiến trúc được thiết kế bởi DASS vượt trội hơn tất cả các kiến trúc cạnh tranh sử dụng phương pháp cắt tỉa. DASS-Small chứng minh hiệu quả nhất quán của nó bằng cách đạt được cải thiện độ chính xác 15%, 10%, và 8% so với MobileNet-v2 thưa [67], EfficientNet-v2 thưa [71], và DARTS thưa [52], tương ứng. Ngoài ra, so với các mạng có độ chính xác tương tự, DASS-Large có sự giảm đáng kể về độ phức tạp mạng (#Params) 3.5×, 30.0×, 105.2× so với PDO-eConv [69], CCT-6/3×1 [65], và MomentumNet [66], tương ứng. Phần 6 cung cấp một nghiên cứu thực nghiệm toàn diện để đánh giá các khía cạnh khác nhau của DASS. Những đóng góp chính của chúng tôi được tóm tắt như sau:

1. Chúng tôi thực hiện các thí nghiệm rộng rãi để xác định các hạn chế của việc áp dụng cắt tỉa với tỷ lệ cắt tỉa cực đoan cho kiến trúc dày đặc như một bước hậu xử lý.
2. Chúng tôi định nghĩa một không gian tìm kiếm mới bằng cách mở rộng không gian tìm kiếm cơ sở với một tập hợp các phép toán tham số mới (SparseConv và SparseLinear) để xem xét các phép toán thưa trong không gian tìm kiếm.
3. Chúng tôi chỉnh sửa bài toán tối ưu hóa hai cấp để nhất quán với không gian tìm kiếm mới và đề xuất một thuật toán dựa trên gradient ba bước để chia bài toán hai cấp phức tạp và học các tham số kiến trúc, trọng số mạng, và tham số cắt tỉa.

2 Công việc liên quan
2.1 Tìm Kiếm Kiến Trúc Neural và các biến thể DARTS
Tìm Kiếm Kiến Trúc Neural (NAS) gần đây đã thu hút sự chú ý đáng kể bằng cách giải thoát các chuyên gia con người khỏi nỗ lực tốn kém trong việc thiết kế mạng neural. Các phương pháp NAS ban đầu chủ yếu sử dụng các phương pháp dựa trên tiến hóa [62, 58, 56, 53] hoặc dựa trên học tăng cường [88, 87, 35]. Mặc dù hiệu quả của các thiết kế thủ công, chúng đòi hỏi tài nguyên tính toán khổng lồ. Ví dụ, phương pháp được đề xuất trong [88] đánh giá 20,000 ứng viên neural trên 500 GPU NVIDIA®P100 trong bốn ngày. Các phương pháp tìm kiếm kiến trúc một lần [6, 23, 3] đã được đề xuất để xác định các kiến trúc neural tối ưu trong vài ngày GPU (>1 ngày GPU [63]). Đặc biệt, Tìm Kiếm Kiến Trúc Khả Vi Phân (DARTS) [52, 75, 76] là một biến thể của các phương pháp NAS một lần làm mềm không gian tìm kiếm để

--- TRANG 2 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

trở nên liên tục và khả vi phân. Mô tả chi tiết của DARTS có thể được tìm thấy trong Phần 3.1. Mặc dù có những thành công rộng rãi của DARTS trong việc thúc đẩy tính ứng dụng của NAS, việc đạt được kết quả tối ưu vẫn là một thách thức đối với các vấn đề thực tế. Nhiều công việc tiếp theo nghiên cứu một số thách thức này bằng cách tập trung vào (i) tăng tốc độ tìm kiếm [34, 70], (ii) cải thiện hiệu suất tổng quát hóa [12, 74], (iii) giải quyết các vấn đề về độ bền vững [81, 80, 33], (iv) giảm lỗi lượng tử hóa [38, 55], và (v) thiết kế các kiến trúc nhận biết phần cứng [37, 45, 9]. Mặt khác, ít công việc cố gắng cắt tỉa không gian tìm kiếm bằng cách loại bỏ các phép toán mạng kém [43, 60, 32, 14]. Những công việc này sử dụng cơ chế cắt tỉa để loại bỏ dần một số phép toán khỏi không gian tìm kiếm. Không giống như chúng, phương pháp của chúng tôi nhằm mở rộng không gian tìm kiếm để cải thiện hiệu suất của mạng thưa bằng cách tìm kiếm các phép toán tốt nhất với cấu trúc trọng số thưa. Về mặt kỹ thuật, phương pháp của chúng tôi mở rộng không gian tìm kiếm bằng cách thêm phiên bản thưa tham số của các phép toán tích chập và tuyến tính để tìm kiến trúc thưa tốt nhất. Do đó, thiếu nghiên cứu về các tham số trọng số thưa khi thiết kế kiến trúc neural. DASS tìm kiếm các phép toán hiệu quả nhất cho các tham số trọng số thưa để đạt được hiệu suất tổng quát hóa cao hơn.

2.2 Cắt Tỉa Mạng
Cắt tỉa mạng là một phương pháp hiệu quả để giảm kích thước của DNN, cho phép chúng được triển khai hiệu quả trên các thiết bị có khả năng tài nguyên hạn chế. Các công việc trước đây về cắt tỉa mạng có thể được phân loại thành hai loại: các phương pháp cắt tỉa có cấu trúc và không có cấu trúc. Mục đích của cắt tỉa có cấu trúc là loại bỏ các kênh hoặc bộ lọc dư thừa để bảo toàn toàn bộ cấu trúc của tensor trọng số với việc giảm chiều [29, 30, 48, 28, 86, 21]. Trong khi cắt tỉa có cấu trúc nổi tiếng về tăng tốc phần cứng, nó hy sinh một mức độ linh hoạt nhất định cũng như độ thưa trọng số [54].

Mặt khác, các phương pháp cắt tỉa không có cấu trúc cung cấp tính linh hoạt và tỷ lệ nén vượt trội bằng cách loại bỏ các tham số có tác động ít nhất đến độ chính xác mạng từ tensor trọng số [25, 47, 29, 54, 17, 68, 2, 49, 83]. Nói chung, cắt tỉa không có cấu trúc bao gồm ba giai đoạn để tạo ra mạng thưa, bao gồm (i) tiền huấn luyện, (ii) cắt tỉa, và (iii) tinh chỉnh. Các phương pháp cắt tỉa không có cấu trúc trước đây sử dụng các tiêu chí khác nhau để chọn các tham số trọng số cắt tỉa thấp nhất. [44, 26] cắt tỉa các tham số trọng số dựa trên các giá trị đạo hàm bậc hai của hàm mất mát. Một số nghiên cứu đề xuất loại bỏ các tham số trọng số dưới một ngưỡng cắt tỉa cố định, bất kể mục tiêu huấn luyện [25, 47, 17, 85, 77, 22]. Để giải quyết hạn chế của các phương pháp ngưỡng cố định, [2, 41] đề xuất các ngưỡng có thể huấn luyện theo lớp để xác định giá trị tối ưu cho từng lớp riêng biệt. Giả thuyết vé số độc đắc [17, 8, 10] là một dòng phương pháp khác xác định mặt nạ cắt tỉa cho CNN được khởi tạo và huấn luyện mô hình thưa kết quả từ đầu mà không thay đổi mặt nạ cắt tỉa. HYDRA [68] xây dựng mục tiêu cắt tỉa như tối thiểu hóa rủi ro thực nghiệm và tích hợp nó với mục tiêu huấn luyện. Không giống như các phương pháp khác, các tiêu chí cắt tỉa dựa trên tối ưu hóa cải thiện hiệu suất của mạng thưa so với các số liệu khác. Mặc dù thành công của cắt tỉa dựa trên tối ưu hóa trong việc đạt được tỷ lệ nén đáng kể, độ chính xác phân loại bị ảnh hưởng, đặc biệt khi tỷ lệ cắt tỉa cực kỳ cao (lên đến 99%). Chúng tôi cho thấy lý do chính cho vấn đề này là do kiến trúc xương sống không tối ưu. Chúng tôi mở rộng không gian tìm kiếm của DASS bằng các phép toán thưa tham số và xây dựng cắt tỉa như một bài toán tối thiểu hóa rủi ro thực nghiệm và tích hợp nó vào bài toán tối ưu hóa hai cấp để tìm mạng thưa tốt nhất.

3 Kiến thức cơ bản
3.1 Tìm Kiếm Kiến Trúc Khả Vi Phân
Tìm Kiếm Kiến Trúc Khả Vi Phân (DARTS) [52] là một phương pháp NAS giảm đáng kể chi phí tìm kiếm bằng cách làm mềm không gian tìm kiếm để trở nên liên tục và khả vi phân. Mẫu tế bào DARTS được biểu diễn bởi một Đồ Thị Có Hướng Không Chu Trình (DAG) chứa N nút nội bộ. Cạnh (i, j) giữa hai nút được liên kết với một phép toán o(i,j) (ví dụ, kết nối bỏ qua hoặc max-pooling 3×3) trong không gian tìm kiếm O. Phương trình 1 tính toán đầu ra của các nút trung gian.

¯o(i,j)(x(i)) =X
o∈Oexp
α(i,j)
o
P
o′∈Oexp
α(i,j)
o′·o(x(i)) (1)

trong đó O và α(i,j)
o biểu thị tập hợp tất cả các phép toán ứng viên và xác suất lựa chọn của o, tương ứng. Nút đầu ra trong tế bào là sự ghép nối của tất cả các nút trung gian. DARTS tối ưu hóa các tham số kiến trúc (α) và trọng số mạng (θ) với hàm mục tiêu hai cấp sau:

min
αLval(θ⋆, α)s.t. θ⋆= argmin
θLtrain(θ, α) (2)

trong đó
Ltrain =P
(x,y)∈(Xtrain ,Ytrain )l(θ,x, y)
|Xtrain|
và
Lval=P
(x,y)∈(Xval,Yval)l(θ,x, y)
|Xval|

Phép toán với α lớn nhất được chọn cho mỗi cạnh. Xtrain và Ytrain biểu thị bộ dữ liệu huấn luyện và nhãn tương ứng, tương ứng. Tương tự, bộ dữ liệu kiểm định và nhãn được chỉ ra bởi Xval và Yval, tương ứng. Sau khi quá trình tìm kiếm hoàn thành, kiến trúc cuối cùng được huấn luyện lại từ đầu để có được độ chính xác tối đa.

3.2 Cắt Tỉa Không Có Cấu Trúc
Cắt tỉa được coi là không có cấu trúc nếu nó loại bỏ các tham số có tầm quan trọng thấp từ tensor trọng số và làm cho chúng thưa [54]. Bài báo này sử dụng phương pháp cắt tỉa mạng không có cấu trúc dựa trên tiêu chí tối ưu hóa để cung cấp tính linh hoạt cao hơn và tỷ lệ nén cực đoan so với các phương pháp cắt tỉa có cấu trúc. Phương pháp cắt tỉa bao gồm ba giai đoạn tối ưu hóa chính: (i) tiền huấn luyện: huấn luyện mạng trên bộ dữ liệu mục tiêu, (ii) cắt tỉa: cắt tỉa trọng số không quan trọng từ mạng được tiền huấn luyện, và (iii) tinh chỉnh: mạng thưa được huấn luyện lại để khôi phục độ chính xác ban đầu. Đối với giai đoạn cắt tỉa, chúng tôi xem xét một phương pháp dựa trên tối ưu hóa với các bước sau: Đầu tiên, chúng tôi định nghĩa các tham số cắt tỉa cho thấy tầm quan trọng của mỗi trọng số của mạng (s0) và khởi tạo chúng theo Phương trình 3.

s0
i∝1
max(|θpre,i|)×θpre,i (3)

trong đó θpre,i biểu thị trọng số của lớp thứ i trong mạng được tiền huấn luyện. Tiếp theo, để học các tham số cắt tỉa (ŝ), chúng tôi xây dựng bài toán tối ưu hóa như Phương trình 4, sau đó được giải quyết bằng gradient descent ngẫu nhiên (SGD) [19].

--- TRANG 3 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

ŝ= argmin
sE(x,y)∼D
Lprune (θpre, s, x, y )
(4)

θpre và E đề cập đến các tham số mạng được tiền huấn luyện và kỳ vọng toán học, tương ứng. Bằng cách giải quyết bài toán tối ưu hóa này, chúng ta có thể xác định tác động của mỗi tham số trọng số đối với hàm mất mát và do đó, độ chính xác của mạng. Cuối cùng, chúng tôi chuyển đổi các giá trị thực của các tham số cắt tỉa thành mặt nạ nhị phân dựa trên việc chọn k trọng số hàng đầu có độ lớn cao nhất của các tham số cắt tỉa.

4 Động lực nghiên cứu
Các kiến trúc mạng dày đặc ban đầu được thiết kế bằng các phương pháp NAS thông thường không chính xác khi tích hợp với các phương pháp cắt tỉa, đặc biệt ở tỷ lệ cắt tỉa cao. Để chứng minh khẳng định này, chúng tôi đầu tiên áp dụng phương pháp cắt tỉa không có cấu trúc được giải thích trong phần 3.2 cho kiến trúc tốt nhất được thiết kế bởi DARTS [52] cho CIFAR-10 và tạo ra một mạng thưa. Chúng tôi gọi giải pháp này là DARTS thưa. Sau đó, chúng tôi so sánh hiệu suất của kiến trúc thưa được thiết kế bởi DASS với DARTS thưa. Hình 2 minh họa các đường cong độ chính xác huấn luyện và kiểm tra cho các kiến trúc DASS và DARTS thưa được huấn luyện trên bộ dữ liệu CIFAR-10. Đáng thất vọng, mạng được thiết kế bởi DARTS thưa dẫn đến giảm độ chính xác kiểm tra. Điều này có nghĩa là các kiến trúc xương sống dày đặc được thiết kế bởi các phương pháp NAS mà không xem xét độ thưa là không hiệu quả (DASS cung cấp độ chính xác kiểm tra cao hơn 8% so với DARTS thưa). Theo các điều tra của chúng tôi, chúng tôi tìm thấy hai vấn đề liên quan đến thất bại huấn luyện của DARTS thưa: (i) DARTS không hỗ trợ các phép toán thưa trong không gian tìm kiếm của nó, và (ii) DARTS tối ưu hóa mục tiêu tìm kiếm mà không xem xét độ thưa. Phần 5.2 giải quyết vấn đề đầu tiên, trong khi vấn đề thứ hai được giải quyết trong Phần 5.3. Chúng tôi điều tra DASS ở hai chế độ để chứng minh tầm quan trọng của việc bao gồm các phép toán thưa và tái xây dựng hàm mục tiêu dựa trên độ thưa. Chế độ đầu tiên mở rộng không gian tìm kiếm chỉ với các phép toán thưa (DASS Op) và không tối ưu hóa các tham số cắt tỉa, trong khi chế độ thứ hai thêm độ thưa vào quá trình tối ưu hóa và tối ưu hóa các tham số kiến trúc và cắt tỉa trong một bài toán tối ưu hóa hai cấp. (DASS Op+Ob). Hình 3 chỉ ra độ chính xác kiểm tra cho các kiến trúc DASS Op, DARTS thưa và (DASS Op+Ob) với các tỷ lệ cắt tỉa khác nhau. Như kết quả cho thấy, DASS Op có độ chính xác thấp hơn ≈3.4% so với DASS Op+Ob và độ chính xác cao hơn ≈4.47% so với DARTS thưa. Kết luận, việc mở rộng không gian tìm kiếm với các phép toán thưa được đề xuất (đóng góp đầu tiên của chúng tôi) trong DASS tạo ra kiến trúc tốt hơn DARTS thưa, nhưng kết hợp nó với mục tiêu tối ưu hóa dựa trên độ thưa (đóng góp thứ hai của chúng tôi) tăng cường hiệu suất.

5 Phương pháp DASS
5.1 DASS: Tổng quan
Chúng tôi đề xuất DASS, một phương pháp tìm kiếm kiến trúc khả vi phân cho mạng neural thưa. DASS đầu tiên mở rộng không gian tìm kiếm của NAS với các phép toán thưa tham số. Sau đó nó chỉnh sửa bài toán tối ưu hóa hai cấp để học kiến trúc, trọng số, và các tham số cắt tỉa. DASS sử dụng phương pháp ba bước để giải quyết bài toán tối ưu hóa hai cấp phức tạp, bao gồm (1) Tiền huấn luyện: Tìm kiến trúc dày đặc tốt nhất (tham số cắt tỉa bằng không) từ không gian tìm kiếm và tiền huấn luyện nó (2) Cắt tỉa và Thiết kế Kiến trúc Thưa: Tìm mặt nạ cắt tỉa tốt nhất (tối ưu hóa tham số cắt tỉa) và cập nhật các tham số kiến trúc dựa trên trọng số thưa và cuối cùng (3) Tinh chỉnh: chúng tôi huấn luyện lại kiến trúc thưa để đạt được hiệu suất phân loại tối đa.

5.2 Không Gian Tìm Kiếm DASS
Để hỗ trợ các phép toán thưa, DASS đề xuất phiên bản thưa tham số của các phép toán tích chập và tuyến tính được gọi là SparseConv và SparseLinear, tương ứng. Các phép toán này có mặt nạ thưa (m) để loại bỏ các tham số trọng số dư thừa từ mạng. Hình 4 minh họa chức năng của hai phép toán này. Ngoài ra, bảng 1 tóm tắt các phép toán của không gian tìm kiếm DASS. Để điều tra thực nghiệm hiệu quả của không gian tìm kiếm thưa được đề xuất

Bảng 1: Các phép toán của không gian tìm kiếm DASS.
Loại Phép toán | Tích chập Thưa Phân tách | Tích chập Thưa Giãn | Max Pooling | Average pooling | Kết nối Bỏ qua
Kích thước Kernel | 3×3,5×5 | 3×3,5×5 | 3×3 | 3×3 | N/A

không gian tìm kiếm thưa, chúng tôi so sánh sự tương tự của bản đồ đặc trưng của kiến trúc dày đặc hiệu suất cao (với số lượng tham số lớn) với kiến trúc thưa được khám phá bởi DASS và kiến trúc được thiết kế từ không gian tìm kiếm gốc DARTS các phương pháp thưa. Chúng tôi sử dụng số liệu Kendall's τ[1] để đo sự tương tự giữa các bản đồ đặc trưng đầu ra. Hệ số tương quan τ trả về một giá trị giữa -1 và 1. Để trình bày kết quả rõ ràng hơn, chúng tôi mở rộng các giá trị này giữa -100 và 100. Các giá trị gần 100 chỉ ra sự tương tự tích cực mạnh hơn giữa các bản đồ đặc trưng. Hình 5 tóm tắt kết quả. Quan sát của chúng tôi tiết lộ sự tương tự giữa bản đồ đặc trưng DASS và kiến trúc dày đặc (lên đến 16%). Mặt khác, sự tương quan giữa DARTS thưa và kiến trúc dày đặc là không đáng kể. Do đó, nó cho thấy rằng kiến trúc được thiết kế bởi DASS dựa trên không gian tìm kiếm mới có thể trích xuất các đặc trưng tương tự hơn với kiến trúc dày đặc hiệu suất cao trong khi DARTS thưa sử dụng không gian tìm kiếm dày đặc mất các đặc trưng quan trọng sau khi cắt tỉa. Mức độ tương tự không quá cao vì DASS là một mạng thưa với tỷ lệ cắt tỉa 99%. Tuy nhiên, nó có thể chứng minh rằng DASS lấy lại các đặc trưng hữu ích.

5.3 Mục tiêu Tìm kiếm DASS
DASS nhằm tìm kiếm các tham số kiến trúc tối ưu (α⋆) để tối thiểu hóa mất mát kiểm định của các tham số trọng số mạng thưa. Do đó, để mục tiêu tìm kiếm nhất quán với không gian tìm kiếm thưa được đề xuất, chúng tôi xây dựng toàn bộ mục tiêu tìm kiếm như một bài toán tối ưu hóa hai cấp phức tạp:

α⋆= min
α(Lval(ˆθ(α), α))
s.t.

θ⋆(α) = argminθLtrain(θ, α)
ˆm= argminm∈{0,1}N
Lprune (θ⋆(α)⊙m, α)
ˆθ(α) =θ⋆(α)⊙ˆm.(5)

Trong đó m biểu thị các tham số mặt nạ cắt tỉa nhị phân. Công thức này học các tham số kiến trúc dựa trên các tham số trọng số thưa. Tuy nhiên, Phương trình 5 không phải là một bài toán tối ưu hóa hai cấp đơn giản vì bài toán cấp thấp bao gồm hai bài toán tối ưu hóa. Để vượt qua thách thức này, chúng tôi chia mục tiêu tìm kiếm thành ba bước riêng biệt. Do đó, bài toán được biến đổi thành hai bài toán tối ưu hóa hai cấp để xác định các tham số kiến trúc tối ưu cho trọng số dày đặc và thưa và một bài toán tối ưu hóa để tinh chỉnh các tham số trọng số. Ngoài ra, bài toán tối ưu hóa cấp thấp bao gồm một bài toán tối ưu hóa rời rạc cho mặt nạ cắt tỉa.

Phần 5.4 đề xuất một thuật toán tối ưu hóa nhiều bước để giải quyết bài toán tối ưu hóa và xử lý bài toán tối ưu hóa rời rạc bằng cách chuyển đổi nó thành bài toán tối ưu hóa liên tục.

5.4 Thuật toán Tối ưu hóa
Bước 1: tiền huấn luyện (học θ⋆
pre và α∗
pre)
Trong bước này, chúng tôi chia Phương trình 5 thành một bài toán tối ưu hóa hai cấp để tìm kiến trúc dày đặc tốt nhất. Việc tiền huấn luyện này là cần thiết cho bước tiếp theo học các tham số mặt nạ cắt tỉa và chỉnh sửa kiến trúc thưa.

α⋆
pre= min
αpre(Lval(θ∗
pre(αpre), αpre))
s.t. θ⋆
pre(αpre) = argmin
θpreLtrain(θpre, αpre)(6)

Kỹ thuật xấp xỉ bậc nhất được sử dụng để cập nhật θ⋆
pre và αpre xen kẽ bằng gradient descent [52].

0 50 100 150 200 250 300 350 400 450 500 550 60030405060708090100
EpochĐộ Chính xác Huấn luyện
DARTS được cắt tỉa
PR-DARTS
0 20 40 60 80 100 120 140 160 180 20030405060708090100
EpochĐộ Chính xác Kiểm tra
DARTS được cắt tỉa
PR-DARTS

(a) Huấn luyện (b) Kiểm tra
Hình 2: So sánh DASS-Small và DARTS thưa trên CIFAR-10 cho các đường cong học (a) huấn luyện và (b) kiểm tra.

--- TRANG 4 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

Kiến trúc
ModelBước1: tiền huấn luyện
Kiến trúc
ModelBước2: cắt tỉa
Mô hình Cuối cùngBước3: tinh chỉnh
Tối ưu hóa DASS

Hình 6: Tổng quan về thuật toán tối ưu hóa được đề xuất để tìm các tham số kiến trúc dựa trên các tham số trọng số thưa. Nó bao gồm ba bước chính: 1) tiền huấn luyện: tìm kiếm kiến trúc dày đặc 2) cắt tỉa: tìm kiếm kiến trúc thưa 3) tinh chỉnh: huấn luyện lại kiến trúc thưa tốt nhất.

Bước 2: cắt tỉa (học ˆm và α∗
prune)
Để làm cho quá trình tìm kiếm nhận biết về cơ chế thưa, chúng ta cần giải quyết một bài toán tối ưu hóa hai cấp khác cập nhật xen kẽ mặt nạ cắt tỉa và các tham số kiến trúc. Các tham số mặt nạ cắt tỉa là các giá trị nhị phân. Do đó, học các tham số mặt nạ (m) là một bài toán tối ưu hóa nhị phân đầy thách thức. Chúng tôi giải quyết bài toán tối ưu hóa nhị phân này bằng cách giới thiệu các tham số cắt tỉa điểm thực s và khởi tạo chúng. Sau đó chúng tôi sử dụng SGD để giải quyết bài toán tối ưu hóa và tìm các tham số mặt nạ cắt tỉa điểm thực tốt nhất. Cuối cùng, dựa trên các giá trị của tham số cắt tỉa, chúng tôi chọn k tham số trọng số hàng đầu với các giá trị cao nhất và gán giá trị một cho chúng. Bước này nhằm học đồng thời các tham số kiến trúc αprune và tham số mặt nạ ˆm để xem xét độ thưa trong việc học các tham số kiến trúc. Do đó, chúng tôi sử dụng một bài toán tối ưu hóa hai cấp khác:

α⋆
prune = min
αprune(Lval(θ∗
pre⊙ˆm(αprune), αprune))
s.t. ˆs(αprune) = argmin
sLprune(θ∗
pre, αprune, s),
ˆm(αprune) =1(|ˆs(αprune)|>|ˆs(αprune)|k)(7)

tương tự như bước 1, phương pháp xấp xỉ bậc nhất được sử dụng để cập nhật xen kẽ ˆm và αprune bằng gradient descent.

Bước 3: tinh chỉnh (học ˆθ)
Trong bước tinh chỉnh, chúng tôi cập nhật các tham số trọng số khác không bằng SGD cho kiến trúc thưa tốt nhất để cải thiện độ chính xác mạng (Phương trình 8).

ˆθt+1=ˆθt−ηˆθ∇ˆθLfine-tune(ˆθt⊙ˆm, α⋆
prune) (8)

trong đó ηˆθ và Lfine-tune biểu thị tốc độ học và hàm mất mát cho bước tinh chỉnh.

Chúng tôi cho thấy rằng thuật toán tối ưu hóa ba bước được đề xuất có thể giải quyết bài toán hai cấp phức tạp trong Phương trình 5 và tìm các tham số kiến trúc tối ưu với hiệu suất tổng quát hóa cao hơn cho mạng thưa. Hình 7 so sánh các đường cong học của DASS với DARTS thưa trên bộ dữ liệu CIFAR-10. Như được hiển thị, thuật toán tối ưu hóa DASS giảm đáng kể mất mát kiểm định cho mạng thưa. Hình 8 so sánh hành vi của khoảng cách tổng quát hóa (độ chính xác huấn luyện trừ kiểm tra) cho DASS và DARTS thưa. DASS có khoảng cách tổng quát hóa thấp hơn (lên đến 22%), cho thấy DASS điều chỉnh tốt hơn mất mát kiểm định qua tất cả các epoch so với DARTS thưa. Thuật toán 1 phác thảo DASS của chúng tôi cho tìm kiếm kiến trúc neural khả vi phân cho mạng neural thưa.

90 95 9980859095100
Tỷ lệ Cắt tỉa (%)Độ Chính xác Kiểm tra (%)DASS 𝑂𝑝+𝑂𝑏
DASS 𝑂𝑝
DARTS 𝑠𝑝𝑎𝑟𝑠𝑒

Hình 3: DASS Op+Ob so với DARTS thưa và DASS chỉ thêm các phép toán thưa vào không gian tìm kiếm (DASS Op).

--- TRANG 5 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

Đầu vào ( )
Đầu raXạ nạ ( )
Đầu raĐầu vào ( )
(a)Phép toán Tuyến tính Phép toán Tuyến tính được Cắt tỉa Đầu vào ( )
Đầu raMặt nạ ( )
Đầu raĐầu vào ( )
(b)Phép toán Tích chập Phép toán Tích chập được Cắt tỉa

Hình 4: Minh họa các phép toán (a) SparseLinear và (b) SparseConv.

Thuật toán 1 Quá trình Tìm kiếm của DASS
Yêu cầu: Bộ dữ liệu D, các mục tiêu mất mát: Ltrain, Lprune, và Lfine−tune, lần lặp huấn luyện T
Đảm bảo: mô hình thưa được tinh chỉnh
Bước1: Tiền huấn luyện
1:for i←1 to T do
2: giữ αt
pre cố định, và có được θt+1
pre bằng gradient descent với ∇θpreLtrain(θt
pre, αt
pre)
3: giữ θt+1
pre cố định, và có được αt+1
pre bằng gradient descent với ∇αpreLval(θt+1
pre, αt
pre)
Bước2: Cắt tỉa
4:for i←1 to T do
5: giữ αt
prune cố định, và có được st+1 bằng gradient descent với ∇sLprune(st, αt
prune)
6: Tính mt+1= (|st+1|>|st+1|k)
7: giữ mt+1 cố định, và có được αt+1
prune bằng gradient descent với ∇αpruneLval(θ∗
pre⊙mt+1, αt
prune)
Bước3: tinh chỉnh
8:for i←1 to T do
9: giữ α∗
prune và ˆm cố định và có được ˆθt+1 bằng gradient descent với ∇ˆθLfine−tune(ˆθt⊙ˆm, α∗
prune)
10:return Mô hình thưa được tinh chỉnh

6 Thí nghiệm
6.1 Thiết lập Thí nghiệm
1) BỘ DỮ LIỆU: Để đánh giá DASS, chúng tôi sử dụng các bộ dữ liệu phân loại công khai CIFAR-10 [39] và ImageNet [40]. Đối với quá trình tìm kiếm, chúng tôi chia bộ dữ liệu CIFAR-10 thành 30k điểm dữ liệu để huấn luyện và 30k để kiểm định. Chúng tôi chuyển các tế bào đã học tốt nhất trên CIFAR-10 sang ImageNet [52] và huấn luyện lại mạng thưa cuối cùng từ đầu.

2) Chi tiết về Tìm kiếm Mạng: Chúng tôi tạo một mạng với 16 kênh ban đầu và tám tế bào. Mỗi tế bào bao gồm bảy nút được trang bị phép toán ghép nối theo chiều sâu như nút đầu ra. Các phép toán SparseConv tuân theo thứ tự ReLU+ SpasreConv +Batch Normalization. Chúng tôi huấn luyện mạng bằng SGD trong 50 epoch với kích thước batch 64 trong bước tiền huấn luyện DASS. Sau đó, chúng tôi cập nhật giá trị của tham số cắt tỉa và kiến trúc trong 20 epoch trong bước cắt tỉa DASS. Cuối cùng, chúng tôi tinh chỉnh mạng trong 200 epoch. Tốc độ học ban đầu cho DASS trong các bước tiền huấn luyện, cắt tỉa, và tinh chỉnh lần lượt là 0.025, 0.1, và 0.01. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng tốc độ học cosine annealing [59]. Chúng tôi sử dụng weight decay=3×10-4 và momentum=0.9 trong tất cả các bước. Quá trình tìm kiếm mất ≈3 GPU-ngày trên một NVIDIA®RTX A4000 duy nhất tạo ra 4.35 Kg CO2. Chúng tôi so sánh thiết kế kiến trúc thưa bằng phương pháp của chúng tôi, DASS, với các mạng dày đặc và thưa khác. NAS-Bench-101 [78] và NAS-Bench-201 [16] là ví dụ về các chuẩn đánh giá thuật toán NAS. Chúng bao gồm nhiều thiết kế dày đặc và hiệu suất tương ứng của chúng. Do chúng không hỗ trợ kiến trúc thưa, chúng tôi không thể đánh giá DASS bằng các chuẩn này. Tạo các chuẩn thưa để đánh giá các thuật toán NAS là một gợi ý cho công việc tương lai.

0 50 100 150 200012
0.30.61.5
EpochMất mát Kiểm định DARTS được cắt tỉa
PR-DARTS

Hình 7: So sánh các đường cong học (mất mát kiểm định) của DASS và DARTS thưa trên các kiến trúc đã tìm kiếm được huấn luyện với bộ dữ liệu CIFAR-10.

0 50 100 150 200020406080
EpochKhoảng cách (%)DARTS được cắt tỉa
PR-DARTS

Hình 8: So sánh khoảng cách tổng quát hóa của DASS và DARTS thưa trên bộ dữ liệu CIFAR-10. Các giá trị thấp hơn cho khoảng cách tổng quát hóa là tốt hơn.

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19−1001020
−5515
Số Tế bàoKendall's τ Tương tựPR-DARTS DARTS được cắt tỉa

Hình 5: So sánh số liệu tương tự Kendall's τ của các kiến trúc được thiết kế bởi cả phương pháp DARTS thưa và DASS với kiến trúc dày đặc hiệu suất cao.

--- TRANG 6 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

3) Các biến thể DASS và Cấu hình Phần cứng: Bảng 2 cung cấp chi tiết cấu hình của các biến thể DASS. Mỗi biến thể được xây dựng bằng cách xếp chồng một số khác nhau các tế bào DASS và các kênh đầu ra của lớp đầu tiên để tạo ra mạng cho các ngân sách tài nguyên khác nhau. Bảng 3 trình bày thông số kỹ thuật của các thiết bị phần cứng được sử dụng để đánh giá hiệu suất của DASS tại thời gian suy luận.

Bảng 2: Cấu hình của các biến thể DASS. #Cells: số lượng tế bào được xếp chồng. #Channels: số lượng kênh đầu ra cho phép toán SparseConv đầu tiên.

DASS | CIFAR-10 | ImageNet
Tiny | Small | Medium | Large | Small | Medium | Large
#Cells | 16 | 20 | 12 | 14 | 14 | 15 | 16
#Channels | 30 | 36 | 86 | 108 | 48 | 86 | 128

Bảng 3: Thông số Kỹ thuật Phần cứng.
Nền tảng | Thông số kỹ thuật | Giá trị
Tìm kiếm & Huấn luyện | GPU | NVIDIA®RTX A4000 (735 MHz)
Bộ nhớ GPU | 16 GB GDDR6
Trình biên dịch GPU | cuDNN phiên bản 11.1
Bộ nhớ Hệ thống | 64 GB
Hệ điều hành | Ubuntu 18.04
Phát thải CO2/Ngày† | 1.45 Kg
Phần cứng Thực | GPU Nhúng | NVIDIA®Jetson TX2 (735 MHz)
256 lõi CUDA
NVIDIA®Quadro M1200 (735 MHz)
640 lõi CUDA
CPU Nhúng | ARM CortexTM-A7 (1.2 GHz)
4/4 (Lõi/Tổng Luồng)
Intel®i5-3210M Mobile CPU
5/4 (Lõi/Tổng Luồng)
Ước tính‡ | GPU Xiaomi Mi9 | Adreno 640 GPU (750 MHz)
986 GFLOPs FP32 (Độ chính xác đơn)
VPU Myriad | Intel Movidius NCS2 (700 MHz)
Bộ xử lý phụ 28-nm

†Tính toán bằng khung tác động ML CO2: https://mlco2.github.io/impact/ [42]
‡Ước tính Hiệu suất bằng khung nn-Meter [82].

6.2 DASS So sánh với Mạng dày đặc
Bảng 4 so sánh hiệu suất của DASS với các DNN tiên tiến và thực tế tiên tiến. Chúng tôi chọn kiến trúc có độ chính xác cao nhất, DrNAS [12], làm cơ sở để so sánh tỷ lệ nén. So với DrNAS [12], DASS-Large cung cấp tỷ lệ nén mạng cao hơn 37.73× và 29.23× trong khi cung cấp độ chính xác tương đương (mất độ chính xác dưới 2.5%) trên bộ dữ liệu CIFAR-10 và ImageNet, tương ứng. So với mạng được thiết kế thủ công tốt nhất [65] trên CIFAR-10 (CCT-6/3x1), DASS-Large giảm đáng kể các tham số của mạng 29.9× với việc cung cấp độ chính xác cao hơn một chút.

6.3 DASS So sánh với Mạng thưa
Vì chúng tôi tập trung vào việc cải thiện độ chính xác của mạng thưa ở tỷ lệ cắt tỉa cực kỳ cao, chúng tôi so sánh DASS với các mạng thưa khác với phương pháp cắt tỉa không có cấu trúc ở tỷ lệ cắt tỉa 99% (Bảng 5). So với DARTS thưa, DASS-Small mang lại độ chính xác top-1 cao hơn 7.81% và 7.81% với giảm kích thước mạng 1.23× và 1.05× trên bộ dữ liệu CIFAR-10 và ImageNet, tương ứng. Nó chỉ ra rằng thiết kế mạng dựa trên không gian tìm kiếm mới và hàm mục tiêu thưa tìm ra kiến trúc thưa tốt hơn. So với ResNet-18 thưa

--- TRANG 7 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

Bảng 4: So sánh phương pháp DASS với các mạng dày đặc tiên tiến trên bộ dữ liệu CIFAR-10 và ImageNet.

Kiến trúc | Năm | CIFAR-10 | ImageNet
Phương pháp Tìm kiếm | Độ chính xác Top-1 (%) | #Params (×106) | Nén #Params | Độ chính xác Top-1 (%) | Độ chính xác Top-5 (%) | #Params (×106) | Nén #Params

ResNet-18‡[27] | 2016 | - | 91.0 | 11.1 | -2.77× | 72.33 | 91.80 | 11.7 | -2.05×
PDO-eConv [69] | 2020 | - | 94.62 | 0.37 | +10.81× | - | - | - | -
FlexTCN-7 [65] | 2021 | - | 92.2 | 0.67 | +5.97× | - | - | - | -
CCT-6/3x1 [65] | 2021 | - | 95.29 | 3.17 | +1.26× | - | - | - | -
MomentumNet [66] | 2021 | - | 95.18 | 11.1 | -2.77× | - | - | - | -
DARTS (bậc 1) [52] | 2018 | gradient | 96.86 | 3.3 | +1.21× | - | - | - | -
DARTS (bậc 2) [52] | 2018 | gradient | 97.24 | 3.3 | +1.21× | 74.3 | 91.3 | 4.7 | +1.21×
SGAS (Cri 1. avg) [46] | 2020 | gradient | 97.34 | 3.7 | +1.08× | 75.9 | 92.7 | 5.4 | +1.05×
SDARTS-RS [11] | 2020 | gradient | 97.39 | 3.4 | +1.17× | 75.8 | 92.8 | 3.4 | +1.67×
DrNAS [12] | 2020 | gradient | 97.46 | 4.0 | 1.0× | 76.3 | 92.9 | 5.7 | 1.0×
DASS-Small | 2022 | gradient | 89.06 | 0.017 | +235.29× | 46.48 | 68.36 | 0.029 | +196.55×
DASS-Medium | 2022 | gradient | 92.18 | 0.054 | +74.07× | 68.34 | 82.24 | 0.082 | +69.51×
DASS-Large | 2022 | gradient | 95.31 | 0.106 | +37.73× | 73.83 | 85.94 | 0.195 | +29.23×

†Cơ sở để so sánh tỷ lệ nén #params là DrNAS [12] như kiến trúc chính xác nhất.
‡Kết quả ResNet-18 được huấn luyện trong https://github.com/facebook/fb.resnet.torchTorch (10 tháng 7, 2018).

trên bộ dữ liệu CIFAR-10, chúng tôi cung cấp độ chính xác cao hơn 1.56% và 4.7% với giảm kích thước mạng 2.08× và 1.05× cho DASS-Medium và DASS-Large, tương ứng. So với ResNet-18 thưa trên bộ dữ liệu ImageNet, DASS-Medium cung cấp độ chính xác cao hơn 0.76% với giảm kích thước mạng 1.42×. MCUNET [50] là một mạng neural nhẹ cho vi điều khiển. Nó được thiết kế bởi một cơ chế tìm kiếm kiến trúc neural nhỏ. So với MCUNET trên bộ dữ liệu ImageNet, DASS-Large cung cấp độ chính xác cao hơn 1% với giảm kích thước mạng 2.89×. Kết quả này cho thấy chỉ tối ưu hóa kích thước của các bộ lọc mà không xem xét độ thưa không thể tạo ra kiến trúc tốt nhất. DASS trực tiếp tìm kiếm các phép toán tốt nhất trong phiên bản thưa để thiết kế mạng nhẹ hiệu suất cao. Chúng ta có thể kết luận rằng DASS tăng độ chính xác của mạng thưa ở tỷ lệ cắt tỉa cao so với mạng dựa trên NAS và thủ công.

Bảng 5: So sánh phương pháp DASS với mạng thưa trên bộ dữ liệu CIFAR-10 và ImageNet.

Kiến trúc | CIFAR-10 | ImageNet
Độ chính xác Top-1 (%) | #Params (×103) | Tỷ lệ Nén† | NID‡ | Độ chính xác Top-1 (%) | Độ chính xác Top-5 (%) | #Params (×103) | Tỷ lệ Nén† | NID‡

DARTS thưa [52] | 81.25 | 21.0 | 100.47× | 3.86 | 38.67 | 61.33 | 33.0 | 100× | 1.11
MobileNet-v2 thưa [67] | 73.44 | 22.2 | 95.04× | 3.30 | 17.97 | 36.72 | 34.87 | 94.63× | 0.515
ResNet-18 thưa [27] | 90.62 | 111.6 | 18.90× | 0.81 | 67.58 | 80.86 | 116.84 | 28.24× | 0.578
EfficientNet thưa [71] | 79.69 | 202.3 | 10.43× | 0.39 | - | - | - | - | -
MCUNET [50] | 89.7 | 210.1 | 15.70 | 0.42 | 72.34 | 84.86 | 562.64 | 5.86× | 0.128
DASS-Small | 89.06 | 17.0 | 124.11× | 5.23 | 46.48 | 68.36 | 28.94 | 114.02× | 1.606
DASS-Medium | 92.18 | 53.65 | 39.32× | 1.71 | 68.34 | 82.24 | 81.95 | 40.26× | 0.841
DASS-Large | 95.31 | 105.5 | 20× | 0.90 | 73.83 | 85.94 | 194.6 | 16.95× | 0.38

†Cơ sở để so sánh tỷ lệ nén là kiến trúc DARTS dày đặc và độ chính xác đầy đủ.
‡NID = Độ chính xác/#Tham số [4]. NID đo lường mức độ hiệu quả mỗi mạng sử dụng các tham số của nó.

6.4 Đánh giá DASS với Các Tỷ lệ Cắt tỉa Khác nhau
Bảng 6 so sánh DASS và phương pháp DARTS thưa với ba tỷ lệ cắt tỉa khác nhau bao gồm 90%, 95%, và 99% trên bộ dữ liệu CIFAR-10. DASS đạt được độ chính xác cao hơn 1.57%, 1.04%, và 7.8% với giảm kích thước mạng 7%, 6.9%, và 23% so với DARTS thưa ở tỷ lệ cắt tỉa 90%, 95%, và 99%, tương ứng. Do đó, DASS hiệu quả đáng kể hơn ở tỷ lệ cắt tỉa cực kỳ cao (99%) so với tỷ lệ cắt tỉa thấp hơn (90%).

6.5 DASS So sánh với Các Phương pháp Cắt tỉa Khác
Bảng 7 so sánh DASS với các thuật toán cắt tỉa tiên tiến. Kết quả chỉ ra rằng DASS vượt trội hơn các thuật toán cắt tỉa khác với các kiến trúc xương sống khác nhau trên bộ dữ liệu CIFAR-10 và ImageNet. Trên CIFAR-10, DASS-Large cho thấy độ chính xác cao hơn 1.6% và giảm kích thước mạng 3.8× so với kết quả chính xác nhất được cung cấp bởi TAS Pruning [15]. DASS-Large cũng cung cấp cải thiện độ chính xác 4.68% với giảm kích thước mạng 38.14× so với TAS Pruning [15] trên ImageNet. Trong ánh sáng của hiệu quả cao hơn của DASS so với các phương pháp cắt tỉa khác

--- TRANG 8 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

Bảng 6: Đánh giá hiệu quả của DASS ở các tỷ lệ cắt tỉa khác nhau.

Kiến trúc | 90% | 95% | 99%
Độ chính xác | #Params | Độ chính xác | #Params | Độ chính xác | #Params
 | (×103) |  | (×103) |  | (×103)
DARTS thưa | 95.31% | 421 | 93.75% | 210.5 | 81.25% | 21.0
DASS-Small | 96.88% | 391 | 94.79% | 196.75 | 89.06% | 17.0

phương pháp, chúng ta có thể kết luận rằng phương pháp cắt tỉa không phải là lý do duy nhất cho hiệu quả của DASS và nó độc lập với thuật toán cắt tỉa.

Bảng 7: So sánh DASS với các thuật toán cắt tỉa khác.

Phương pháp Cắt tỉa | CIFAR-10 | ImageNet
Kiến trúc Xương sống | Độ chính xác Top-1(%) | #Params (×106) | Kiến trúc Xương sống | Độ chính xác Top-1(%) | Độ chính xác Top-5(%) | #Params (×106)

SFP [29] | ResNet-20 | 92.08 | 0.269 | ResNet-18 | 67.10 | 87.78 | 6.46
FPGM [30] |  | 92.31 | 0.269 |  | 68.41 | 88.48 | 6.46
TAS Pruning [15] |  | 93.16 | 0.232 |  | 69.15 | 88.48 | 7.40
DASS-Small | - | 89.06 | 0.017 | - | 46.48 | 68.36 | 0.029
DASS-Medium | - | 92.18 | 0.054 | - | 68.34 | 82.24 | 0.082
DASS-Large | - | 95.31 | 0.106 | - | 73.83 | 85.94 | 0.194

6.6 DASS So sánh với Mạng Lượng tử hóa
Lượng tử hóa mạng nổi lên như một hướng nghiên cứu hứa hẹn để giảm tính toán của mạng neural. Gần đây, [38, 7, 55] đề xuất tích hợp cơ chế lượng tử hóa vào quy trình NAS khả vi phân để cải thiện hiệu suất của mạng lượng tử hóa. Bảng 8 so sánh DASS với kết quả tốt nhất của mạng lượng tử hóa dựa trên NAS. Tỷ lệ nén được tính là PL
l=1#Wl×32PL
l=1#Wt
l×q trong đó #Wl và #Wt
l là số lượng trọng số trong lớp l cho mạng độ chính xác đầy đủ (32-bit) và lượng tử hóa với độ phân giải q-bit [55]. DASS-Medium mang lại độ chính xác cao hơn 0.24% và 3.24% và tỷ lệ nén cao hơn đáng kể 2.7× và 4.24× so với TAS [55] như mạng lượng tử hóa chính xác nhất trên bộ dữ liệu CIFAR-10 và ImageNet, tương ứng.

Bảng 8: So sánh phương pháp DASS với mạng lượng tử hóa trên CIFAR-10.

Kiến trúc | CIFAR-10 | ImageNet
#bits | Độ chính xác Top-1 | #Params | Tỷ lệ Nén | Độ chính xác Top-1 | Độ chính xác Top-5 | #Params | Tỷ lệ Nén
(W/A)‡ | (%) | (×106) | † | (%) | (%) | (×106) | †

Binary NAS (A) [38] | 1/1 | 90.66 | 2.4 | 44.0× | 57.69 | 79.89 | 5.57 | 32.74×
TAS [55] | 2/2 | 91.94 | 2.4 | 22.0× | 65.1 | 86.3 | 5.57 | 16.37×
DASS-Small | 32/32 | 89.06 | 0.017 | 194.11× | 46.48 | 68.36 | 0.029 | 196.55×
DASS-Medium | 32/32 | 92.18 | 0.054 | 61.11× | 68.34 | 82.24 | 0.082 | 69.51×
DASS-Large | 32/32 | 95.31 | 0.106 | 31.13× | 73.83 | 85.94 | 0.194 | 29.38×

†Cơ sở để so sánh là DARTS độ chính xác đầy đủ với 3.3M và 5.7M tham số cho CIFAR-10 và ImageNet.
‡(Trọng số/Hàm Kích hoạt).

6.7 Kết quả Hiệu suất Phần cứng của DASS
Chúng tôi nghiên cứu rộng rãi hiệu quả của DASS trong bối cảnh hiệu quả phần cứng bằng cách tính thời gian suy luận (độ trễ) của các mạng thưa tiên tiến khác nhau cho một loạt các thiết bị biên hạn chế tài nguyên trên bộ dữ liệu CIFAR-10 (Hình 9). Kích thước batch bằng 1 cho tất cả các thí nghiệm. Đáng chú ý là chúng tôi không sử dụng bất kỳ kỹ thuật đơn giản hóa nào, chẳng hạn như [5], để nén các bộ lọc thưa bằng cách kết hợp các tham số trọng số. Kết quả của chúng tôi tiết lộ rằng Pareto-frontier của DASS luôn vượt trội hơn tất cả các đối thủ khác với biên độ đáng kể, đặc biệt trên CPU có tính song song rất hạn chế. DASS-Tiny như mạng nhanh nhất cải thiện độ chính xác từ 73.44% của MobileNet-v2 lên 81.35% (cải thiện +7.91%) và tăng tốc suy luận lên đến 3.87×. Quan trọng hơn,

--- TRANG 9 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

PR-DARTS DARTS được cắt tỉa EFFICIENT NET-V2được cắt tỉa RESNET-18 được cắt tỉa MOBILE NET-V2được cắt tỉa

012345678910111213707580859095100
TinySmallMediumLarge
Độ trễ (s)Độ chính xác Top-1 (%)ARM®CortexTM-A7

0200 600 1,000 1,400 1,800 2,200707580859095100
TinySmallMediumLarge
Độ trễ (ms)Độ chính xác Top-1 (%)Intel®i5-3210M (PyTorch CPU)

68101214161820222426283032343638404244707580859095100
TinySmallMediumLarge
Độ trễ (ms)Độ chính xác Top-1 (%)Intel®Movidius NCS2 (OpenVINO)

051015202530354045505560707580859095100
TinySmallMediumLarge
Độ trễ (ms)Độ chính xác Top-1 (%)Xiaomi Mi9 GPU (TFLite)

0 50 150 250 350 450707580859095100
TinySmallMediumLarge
Độ trễ (ms)Độ chính xác Top-1 (%)Quadro M1200 (PyTorch GPU)

50100150200250300350400450500550600650700750800707580859095100
TinySmallMediumLarge
Độ trễ (ms)Độ chính xác Top-1 (%)Tegra TX2 (PyTorch GPU)

Hình 9: Đánh đổi: độ chính xác so với độ trễ đo được. DASS-Tiny, DASS-Small, DASS-Medium, DASS-Large là các biến thể của DASS được thiết kế cho các ngân sách tính toán khác nhau (Bảng 2). DASS-Tiny luôn đạt được độ chính xác cao hơn với độ trễ tương tự so với MobileNet-v2 thưa và cung cấp độ trễ thấp hơn trong khi đạt được độ chính xác tốt hơn như DARTS thưa.

Nhãn: 0123456789
0.0 0.2 0.4 0.6 0.8 1.0
tSNE 10.00.20.40.60.81.0tSNE 20 1 2 3 4 5 6 7 8 9
0.0 0.2 0.4 0.6 0.8 1.0
tSNE 10.00.20.40.60.81.0tSNE 20 1 2 3 4 5 6 7 8 9
0.0 0.2 0.4 0.6 0.8 1.0
tSNE 10.00.20.40.60.81.0tSNE 20 1 2 3 4 5 6 7 8 9

(a) DARTS (b) DARTS thưa (c) DASS-Small

Hình 10: Hiển thị ranh giới quyết định của (a) DARTS. (b) DARTS thưa. (c) DASS-Large với phương pháp nhúng t-SNE.

DASS-Tiny chạy nhanh hơn nhiều so với DARTS thưa 1.67-4.74× với độ chính xác hơi tốt hơn. So với ResNet-18thưa như mạng gần nhất với DASS về mặt độ chính xác, DASS-Medium cung cấp cải thiện độ chính xác 1.46% và tăng tốc lên đến 1.94× trên phần cứng.

6.8 Phân tích Sức mạnh Phân biệt của DASS
Chúng tôi sử dụng phương pháp nhúng láng giềng ngẫu nhiên phân phối t (t-SNE) [72] để hiển thị ranh giới quyết định của kiến trúc dày đặc hiệu suất cao được thiết kế bởi DARTS, DARTS thưa (kiến trúc DART dày đặc thưa với cắt tỉa), và DASS (kiến trúc thưa của chúng tôi) trên bộ dữ liệu CIFAR-10. Hình 10 minh họa ranh giới quyết định của phân loại cho mỗi mạng. Theo kết quả, DASS có sức mạnh phân biệt cao hơn DARTS thưa, và DASS với tỷ lệ cắt tỉa 99% hoạt động rất tương tự như kiến trúc DARTS dày đặc và hiệu suất cao.

--- TRANG 10 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

c_{k-2} 0sep_conv_3x3
1skip_connect2skip_connect
3skip_connect
c_{k-1}sep_conv_3x3
skip_connectskip_connectskip_connect
c_{k}

c_{k-2}
0avg_pool_3x31avg_pool_3x3
2avg_pool_3x3
3avg_pool_3x3
c_{k-1}dil_conv_3x3skip_connect
skip_connect
skip_connectc_{k}

(a) Tế bào Bình thường DARTS. (b) Tế bào Giảm DARTS.

c_{k-2} 0avg_pool_3x3c_{k-1}
skip_connect
1dil_conv_5x5 2skip_connect
skip_connectavg_pool_3x3
c_{k}3 skip_connectskip_connect

c_{k-2}
0sep_conv_3x32skip_connect
c_{k-1}max_pool_3x3
1max_pool_3x33max_pool_3x3
dil_conv_3x3dil_conv_3x3 c_{k}skip_connect

(a) Tế bào Bình thường DASS. (b) Tế bào Giảm DASS.

Hình 11: Minh họa (a) tế bào bình thường và (b) tế bào giảm.

6.9 Phân tích Định tính của Tế bào được Tìm kiếm.
Hình 11 cho thấy các tế bào tốt nhất được tìm kiếm bởi DASS-Small. Một phát hiện thú vị là, đối với tế bào bình thường, DASS-Small có xu hướng chọn phép toán SparseConv với kích thước kernel lớn hơn (5×5), cung cấp nhiều ứng viên cắt tỉa hơn để tối ưu hóa mặt nạ cắt tỉa. DASS-Small có xu hướng tận dụng các phép toán max-pooling trong tế bào giảm thay vì các phép toán avg-pooling. Điều này là do phép toán max-pooling có khả năng trích xuất đặc trưng cao hơn với các bộ lọc thưa [79].

6.10 Phân tích Khả năng Tái tạo.
Để xác minh khả năng tái tạo kết quả, quy trình tìm kiếm DASS-Small đã được chạy năm lần với các hạt giống ngẫu nhiên khác nhau. Hình 6.10 vẽ trung bình của các biến thiên độ chính xác và mất mát cũng như các bóng để chỉ ra khoảng tin cậy. Kết quả cho thấy rằng, trong khi khoảng tin cậy rộng lúc đầu, trung bình của nhiều lần chạy hội tụ về các kiến trúc neural với hiệu suất tương tự với độ lệch chuẩn trung bình (STDEV) là 2.22%.

0 10 20 30 40 500.50.60.70.80.90.88
EpochĐộ chính xác Kiểm địnhĐộ chính xác
0.511.5
0.3
Mất mát Kiểm địnhMất mát

Hình 12: Chứng minh khả năng tái tạo kết quả DASS.

7 Kết luận
Chúng tôi đề xuất DASS, một phương pháp tìm kiếm kiến trúc khả vi phân, để thiết kế các kiến trúc thưa hiệu suất cao cho DNN. DASS cải thiện đáng kể hiệu suất của các kiến trúc thưa bằng cách đề xuất: (i) một không gian tìm kiếm mới chứa các phép toán thưa tham số; và (ii) một mục tiêu tìm kiếm mới nhất quán với cơ chế thưa và cắt tỉa. Kết quả thí nghiệm của chúng tôi tiết lộ rằng các kiến trúc thưa đã học vượt trội hơn các kiến trúc được sử dụng trong tiên tiến trên cả bộ dữ liệu CIFAR-10 và ImageNet. Về lâu dài, chúng tôi dự đoán rằng các mạng được thiết kế của chúng tôi có thể đóng góp hiệu quả vào mục tiêu trí tuệ nhân tạo xanh bằng cách sử dụng hiệu quả các thiết bị hạn chế tài nguyên như các giải pháp tăng tốc biên. Một hướng đầy hứa hẹn cho công việc tương lai là thiết kế một mạng thưa cũng bền vững chống lại các cuộc tấn công đối kháng.

--- TRANG 11 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

Tài liệu tham khảo
[1] Hervé Abdi. 2007. Hệ số tương quan hạng Kendall. Bách khoa toàn thư về Đo lường và Thống kê. Sage, Thousand Oaks, CA (2007), 508–510.

[2] Kambiz Azarian, Yash Bhalgat, Jinwon Lee, và Tijmen Blankevoort. 2020. Cắt tỉa ngưỡng đã học. arXiv preprint arXiv:2003.00075 (2020).

[3] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, và Quoc Le. 2018. Hiểu và đơn giản hóa tìm kiếm kiến trúc một lần. Trong Hội nghị Quốc tế về Học máy. PMLR, 550–559.

[4] Simone Bianco, Remi Cadene, Luigi Celona, và Paolo Napoletano. 2018. Phân tích chuẩn của các kiến trúc mạng neural sâu đại diện. IEEE Access 6 (2018), 64270–64277.

[5] Andrea Bragagnolo và Carlo Alberto Barbano. 2022. Simplify: Một thư viện Python để tối ưu hóa mạng neural được cắt tỉa. SoftwareX 17 (2022), 100907.

[6] Andrew Brock, Theodore Lim, James M Ritchie, và Nick Weston. 2017. Smash: tìm kiếm kiến trúc mô hình một lần thông qua siêu mạng. arXiv preprint arXiv:1708.05344 (2017).

[7] Adrian Bulat, Brais Martinez, và Georgios Tzimiropoulos. 2020. Bats: Tìm kiếm kiến trúc nhị phân. Trong Thị giác Máy tính–ECCV 2020: Hội nghị Châu Âu lần thứ 16, Glasgow, UK, 23–28 tháng 8, 2020, Thủ tục, Phần XXIII 16. Springer, 309–325.

[8] Rebekka Burkholz, Nilanjana Laha, Rajarshi Mukherjee, và Alkis Gotovos. 2021. Về sự tồn tại của vé số phổ quát. arXiv preprint arXiv:2111.11146 (2021).

[9] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. 2019. Once-for-all: Huấn luyện một mạng và chuyên môn hóa nó để triển khai hiệu quả. arXiv preprint arXiv:1908.09791 (2019).

[10] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Yang Zhang, Shiyu Chang, và Zhangyang Wang. 2022. Vé số thắng kép hiệu quả dữ liệu từ tiền huấn luyện bền vững. Trong Hội nghị Quốc tế về Học máy. PMLR, 3747–3759.

[11] Xiangning Chen và Cho-Jui Hsieh. 2020. Ổn định tìm kiếm kiến trúc khả vi phân thông qua điều chỉnh dựa trên nhiễu loạn. Trong Hội nghị Quốc tế về Học máy. PMLR, 1554–1565.

[12] Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, và Cho-Jui Hsieh. 2020. Drnas: Tìm kiếm kiến trúc neural Dirichlet. arXiv preprint arXiv:2006.10355 (2020).

[13] Enmao Diao, Ganghua Wang, Jiawei Zhan, Yuhong Yang, Jie Ding, và Vahid Tarokh. 2023. Cắt tỉa Mạng Neural Sâu từ Góc độ Thưa. arXiv preprint arXiv:2302.05601 (2023).

[14] Yadong Ding, Yu Wu, Chengyue Huang, Siliang Tang, Fei Wu, Yi Yang, Wenwu Zhu, và Yueting Zhuang. 2022. NAP: Tìm kiếm Kiến trúc Neural với Cắt tỉa. Neurocomputing (2022).

[15] Xuanyi Dong và Yi Yang. 2019. Cắt tỉa mạng thông qua tìm kiếm kiến trúc có thể biến đổi. Advances in Neural Information Processing Systems 32 (2019).

[16] Xuanyi Dong và Yi Yang. 2020. Nas-bench-201: Mở rộng phạm vi của tìm kiếm kiến trúc neural có thể tái tạo. arXiv preprint arXiv:2001.00326 (2020).

[17] Jonathan Frankle và Michael Carbin. 2018. Giả thuyết vé số: Tìm mạng neural thưa, có thể huấn luyện. arXiv preprint arXiv:1803.03635 (2018).

[18] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, và Kurt Keutzer. 2021. AI và Bức tường Bộ nhớ. RiseLab Medium Post (2021).

[19] Ian Goodfellow, Yoshua Bengio, và Aaron Courville. 2016. Học Sâu. MIT Press. http://www.deeplearningbook.org.

[20] Jianping Gou, Baosheng Yu, Stephen J Maybank, và Dacheng Tao. 2021. Chưng cất kiến thức: Một khảo sát. International Journal of Computer Vision 129, 6 (2021), 1789–1819.

[21] Yushuo Guan, Ning Liu, Pengyu Zhao, Zhengping Che, Kaigui Bian, Yanzhi Wang, và Jian Tang. 2022. Dais: Cắt tỉa kênh tự động thông qua tìm kiếm chỉ báo ủ nhiệt khả vi phân. IEEE Transactions on Neural Networks and Learning Systems (2022).

[22] Shupeng Gui, Haotao N Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, và Ji Liu. 2019. Nén mô hình với độ bền đối kháng: Một khung tối ưu hóa thống nhất. Advances in Neural Information Processing Systems 32 (2019), 1285–1296.

[23] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, và Jian Sun. 2020. Tìm kiếm kiến trúc neural một đường một lần với lấy mẫu đồng nhất. Trong Hội nghị Châu Âu về Thị giác Máy tính. Springer, 544–560.

--- TRANG 12 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

[24] Marwa El Halabi, Suraj Srinivas, và Simon Lacoste-Julien. 2022. Cắt tỉa có cấu trúc hiệu quả dữ liệu thông qua tối ưu hóa submodular. arXiv preprint arXiv:2203.04940 (2022).

[25] Song Han, Jeff Pool, John Tran, và William J Dally. 2015. Học cả trọng số và kết nối cho mạng neural hiệu quả. arXiv preprint arXiv:1506.02626 (2015).

[26] Babak Hassibi và David G Stork. 1993. Đạo hàm bậc hai cho cắt tỉa mạng: Bác sĩ phẫu thuật não tối ưu. Morgan Kaufmann.

[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. 2016. Học tàn dư sâu cho nhận dạng hình ảnh. Trong Thủ tục của hội nghị IEEE về thị giác máy tính và nhận dạng mẫu. 770–778.

[28] Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, và Yi Yang. 2020. Học tiêu chí cắt tỉa bộ lọc cho tăng tốc mạng neural tích chập sâu. Trong Thủ tục của hội nghị IEEE/CVF về thị giác máy tính và nhận dạng mẫu. 2009–2018.

[29] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, và Yi Yang. 2018. Cắt tỉa bộ lọc mềm để tăng tốc mạng neural tích chập sâu. arXiv preprint arXiv:1808.06866 (2018).

[30] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. 2019. Cắt tỉa bộ lọc thông qua trung vị hình học để tăng tốc mạng neural tích chập sâu. Trong Thủ tục của Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 4340–4349.

[31] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Chưng cất kiến thức trong mạng neural. arXiv preprint arXiv:1503.02531 2, 7 (2015).

[32] Weijun Hong, Guilin Li, Weinan Zhang, Ruiming Tang, Yunhe Wang, Zhenguo Li, và Yong Yu. 2021. Dropnas: Loại bỏ phép toán nhóm cho tìm kiếm kiến trúc khả vi phân. Trong Thủ tục của Hội nghị Quốc tế lần thứ 29 về Hội nghị Quốc tế về Trí tuệ Nhân tạo. 2326–2332.

[33] Ramtin Hosseini, Xingyi Yang, và Pengtao Xie. 2021. DSRNA: Tìm kiếm Khả vi phân của Kiến trúc Neural Bền vững. Trong Thủ tục của Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 6196–6205.

[34] Andrew Hundt, Varun Jain, và Gregory D Hager. 2019. sharpdarts: Tìm kiếm kiến trúc khả vi phân nhanh hơn và chính xác hơn. arXiv preprint arXiv:1903.09900 (2019).

[35] Yesmina Jaafra, Jean Luc Laurent, Aline Deruyver, và Mohamed Saber Naceur. 2019. Học tăng cường cho tìm kiếm kiến trúc neural: Một đánh giá. Image and Vision Computing 89 (2019), 57–66.

[36] Max Jaderberg, Andrea Vedaldi, và Andrew Zisserman. 2014. Tăng tốc mạng neural tích chập với mở rộng hạng thấp. arXiv preprint arXiv:1405.3866 (2014).

[37] Xiaojie Jin, Jiang Wang, Joshua Slocum, Ming-Hsuan Yang, Shengyang Dai, Shuicheng Yan, và Jiashi Feng. 2019. Rc-darts: Tìm kiếm kiến trúc khả vi phân hạn chế tài nguyên. arXiv preprint arXiv:1912.12814 (2019).

[38] Dahyun Kim, Kunal Pratap Singh, và Jonghyun Choi. 2020. Học kiến trúc cho mạng nhị phân. Trong Hội nghị Châu Âu về Thị giác Máy tính. Springer, 575–591.

[39] Alex Krizhevsky, Vinod Nair, và Geoffrey Hinton. 2009. Bộ dữ liệu Cifar-10 và cifar-100. URl: https://www. cs. toronto. edu/kriz/cifar. html 6, 1 (2009), 1.

[40] Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. 2012. Phân loại imagenet với mạng neural tích chập sâu. Advances in neural information processing systems 25 (2012), 1097–1105.

[41] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, và Ali Farhadi. 2020. Tái tham số hóa Trọng số Ngưỡng Mềm cho Độ thưa Có thể Học. Trong Thủ tục của Hội nghị Quốc tế về Học máy.

[42] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, và Thomas Dandres. 2019. Định lượng lượng khí thải carbon của học máy. arXiv preprint arXiv:1910.09700 (2019).

[43] Kevin Alexander Laube và Andreas Zell. 2019. Prune and replace nas. Trong Hội nghị IEEE Quốc tế lần thứ 18 năm 2019 về Ứng dụng Học máy và Máy tính (ICMLA). IEEE, 915–921.

[44] Yann LeCun, John S Denker, và Sara A Solla. 1990. Tổn thương não tối ưu. Trong Advances in neural information processing systems. 598–605.

[45] Hayeon Lee, Sewoong Lee, Song Chong, và Sung Ju Hwang. 2021. HELP: Dự đoán Độ trễ Hiệu quả Thích ứng Phần cứng cho NAS thông qua Meta-Learning. arXiv preprint arXiv:2106.08630 (2021).

[46] Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali Thabet, và Bernard Ghanem. 2020. Sgas: Tìm kiếm kiến trúc tham lam tuần tự. Trong Thủ tục của Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 1620–1630.

--- TRANG 13 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

[47] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. 2016. Cắt tỉa bộ lọc cho convnets hiệu quả. arXiv preprint arXiv:1608.08710 (2016).

[48] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, và Wei Liu. 2019. Nén mạng neural tích chập thông qua bộ lọc tích chập được phân tích nhân tử. Trong Thủ tục của Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 3977–3986.

[49] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, và Xiaotong Zhang. 2021. Cắt tỉa và lượng tử hóa để tăng tốc mạng neural sâu: Một khảo sát. Neurocomputing 461 (2021), 370–403.

[50] Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, và Song Han. 2020. Mcunet: Học sâu nhỏ trên thiết bị iot. arXiv preprint arXiv:2007.10319 (2020).

[51] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, và Song Han. 2022. Huấn luyện trên thiết bị dưới bộ nhớ 256kb. arXiv preprint arXiv:2206.15472 (2022).

[52] Hanxiao Liu, Karen Simonyan, và Yiming Yang. 2018. Darts: Tìm kiếm kiến trúc khả vi phân. arXiv preprint arXiv:1806.09055 (2018).

[53] Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, và Kay Chen Tan. 2021. Một khảo sát về tìm kiếm kiến trúc neural tiến hóa. IEEE Transactions on Neural Networks and Learning Systems (2021).

[54] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. 2018. Suy nghĩ lại về giá trị của cắt tỉa mạng. arXiv preprint arXiv:1810.05270 (2018).

[55] Mohammad Loni, Hamid Mousavi, Mohammad Riazati, Masoud Daneshtalab, và Mikael Sjödin. 2022. TAS:Tìm kiếm Kiến trúc Neural Ba giá trị cho Thiết bị Biên Hạn chế Tài nguyên. Trong Hội nghị Thiết kế, Tự động hóa & Thử nghiệm tại Châu Âu DATE'22, 14 tháng 3, 2022, Antwerp, Bỉ. IEEE. http://www.es.mdh.se/publications/6351-

[56] Mohammad Loni, Sima Sinaei, Ali Zoljodi, Masoud Daneshtalab, và Mikael Sjödin. 2020. DeepMaker: Một khung tối ưu hóa đa mục tiêu cho mạng neural sâu trong hệ thống nhúng. Microprocessors and Microsystems 73 (2020), 102989.

[57] Mohammad Loni, Ali Zoljodi, Amin Majd, Byung Hoon Ahn, Masoud Daneshtalab, Mikael Sjödin, và Hadi Esmaeilzadeh. 2021. FastStereoNet: Tìm kiếm Kiến trúc Neural Nhanh để Cải thiện Suy luận Ước tính Chênh lệch trên Nền tảng Hạn chế Tài nguyên. IEEE Transactions on Systems, Man, and Cybernetics: Systems (2021).

[58] Mohammad Loni, Ali Zoljodi, Sima Sinaei, Masoud Daneshtalab, và Mikael Sjödin. 2019. Neuropower: Thiết kế kiến trúc mạng neural tích chập tiết kiệm năng lượng cho hệ thống nhúng. Trong Hội nghị quốc tế về mạng neural nhân tạo. Springer, 208–222.

[59] Ilya Loshchilov và Frank Hutter. 2016. Sgdr: Gradient descent ngẫu nhiên với khởi động lại ấm. arXiv preprint arXiv:1608.03983 (2016).

[60] Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja Giryes, và Lihi Zelnik. 2020. Asap: Tìm kiếm kiến trúc, ủ nhiệt và cắt tỉa. Trong Hội nghị Quốc tế về Thống kê Trí tuệ Nhân tạo. PMLR, 493–503.

[61] Zhuwei Qin, Fuxun Yu, Chenchen Liu, và Xiang Chen. 2018. Cách mạng neural tích chập nhìn thế giới- Một khảo sát về các phương pháp trực quan hóa mạng neural tích chập. arXiv preprint arXiv:1804.11191 (2018).

[62] Esteban Real, Alok Aggarwal, Yanping Huang, và Quoc V Le. 2019. Tiến hóa được điều chỉnh cho tìm kiếm kiến trúc phân loại hình ảnh. Trong Thủ tục của hội nghị aaai về trí tuệ nhân tạo, Tập. 33. 4780–4789.

[63] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, và Xin Wang. 2021. Một khảo sát toàn diện về tìm kiếm kiến trúc neural: Thách thức và giải pháp. ACM Computing Surveys (CSUR) 54, 4 (2021), 1–34.

[64] Shaoqing Ren, Kaiming He, Ross Girshick, và Jian Sun. 2015. Faster r-cnn: Hướng tới phát hiện đối tượng thời gian thực với mạng đề xuất vùng. Advances in neural information processing systems 28 (2015).

[65] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, và Jan C van Gemert. 2021. Flexconv: Tích chập kernel liên tục với kích thước kernel khả vi phân. arXiv preprint arXiv:2110.08059 (2021).

[66] Michael E Sander, Pierre Ablin, Mathieu Blondel, và Gabriel Peyré. 2021. Mạng neural tàn dư động lượng. arXiv preprint arXiv:2102.07870 (2021).

[67] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. 2018. Mobilenetv2: Tàn dư đảo ngược và nút cổ chai tuyến tính. Trong Thủ tục của hội nghị IEEE về thị giác máy tính và nhận dạng mẫu. 4510–4520.

--- TRANG 14 ---
DASS: Tìm Kiếm Kiến Trúc Khả Vi Phân Cho Mạng Neural Thưa

[68] Vikash Sehwag, Shiqi Wang, Prateek Mittal, và Suman Jana. 2020. Hydra: Cắt tỉa mạng neural bền vững đối kháng. Advances in Neural Information Processing Systems 33 (2020), 19655–19666.

[69] Zhengyang Shen, Lingshen He, Zhouchen Lin, và Jinwen Ma. 2020. Pdo-econvs: Tích chập đồng biến dựa trên toán tử đạo hàm riêng. Trong Hội nghị Quốc tế về Học máy. PMLR, 8697–8706.

[70] Shahid Siddiqui, Christos Kyrkou, và Theocharis Theocharides. 2021. Tìm kiếm Kiến trúc Khả vi phân Nhanh Nhận biết Phép toán và Tôpô. Trong Hội nghị Quốc tế lần thứ 25 năm 2020 về Nhận dạng Mẫu (ICPR). IEEE, 9666–9673.

[71] Mingxing Tan và Quoc Le. 2019. Efficientnet: Suy nghĩ lại về mở rộng mô hình cho mạng neural tích chập. Trong Hội nghị Quốc tế về Học máy. PMLR, 6105–6114.

[72] Laurens Van der Maaten và Geoffrey Hinton. 2008. Trực quan hóa dữ liệu bằng t-SNE. Journal of machine learning research 9, 11 (2008).

[73] Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, và Eftychios Protopapadakis. 2018. Học sâu cho thị giác máy tính: Một đánh giá ngắn gọn. Computational intelligence and neuroscience 2018 (2018).

[74] Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, và Cho-Jui Hsieh. 2021. Suy nghĩ lại về lựa chọn kiến trúc trong NAS khả vi phân. arXiv preprint arXiv:2108.04392 (2021).

[75] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, và Wanli Ouyang. 2022. b-darts: Điều chỉnh phân rã beta cho tìm kiếm kiến trúc khả vi phân. Trong Thủ tục của Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 10874–10883.

[76] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, và Wanli Ouyang. 2022. beta-DARTS: Điều chỉnh Phân rã Beta cho Tìm kiếm Kiến trúc Khả vi phân. Trong Hội nghị IEEE/CVF năm 2022 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR). IEEE, 10864–10873.

[77] Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, và Xue Lin. 2019. Độ bền đối kháng so với nén mô hình, hay cả hai?. Trong Thủ tục của Hội nghị IEEE/CVF Quốc tế về Thị giác Máy tính. 111–120.

[78] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, và Frank Hutter. 2019. Nas-bench-101: Hướng tới tìm kiếm kiến trúc neural có thể tái tạo. Trong Hội nghị Quốc tế về Học máy. PMLR, 7105–7114.

[79] Dingjun Yu, Hanli Wang, Peiqiu Chen, và Zhihua Wei. 2014. Pooling hỗn hợp cho mạng neural tích chập. Trong Hội nghị quốc tế về tập thô và công nghệ kiến thức. Springer, 364–375.

[80] Zhixiong Yue, Baijiong Lin, Xiaonan Huang, và Yu Zhang. 2020. Tìm kiếm Kiến trúc Neural Hiệu quả, Bền vững và Chắc chắn. arXiv preprint arXiv:2011.09820 (2020).

[81] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, và Frank Hutter. 2019. Hiểu và làm chắc chắn tìm kiếm kiến trúc khả vi phân. arXiv preprint arXiv:1909.09656 (2019).

[82] Li Lyna Zhang, Shihao Han, Jianyu Wei, Ningxin Zheng, Ting Cao, Yuqing Yang, và Yunxin Liu. 2021. nn-Meter: hướng tới dự đoán độ trễ chính xác của suy luận mô hình học sâu trên các thiết bị biên đa dạng. Trong Thủ tục của Hội nghị Quốc tế Thường niên lần thứ 19 về Hệ thống Di động, Ứng dụng, và Dịch vụ. 81–93.

[83] Xinyu Zhang, Ian Colbert, Ken Kreutz-Delgado, và Srinjoy Das. 2021. Huấn luyện Mạng Neural Sâu với Lượng tử hóa và Cắt tỉa Kết hợp của Trọng số và Kích hoạt. arXiv preprint arXiv:2110.08271 (2021).

[84] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, và Sijia Liu. 2022. Thúc đẩy Cắt tỉa Mô hình thông qua Tối ưu hóa Hai cấp. arXiv preprint arXiv:2210.04092 (2022).

[85] Hattie Zhou, Janice Lan, Rosanne Liu, và Jason Yosinski. 2019. Phân tích vé số: Số không, dấu hiệu, và siêu mặt nạ. arXiv preprint arXiv:1905.01067 (2019).

[86] Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, và Xiang Li. 2020. Cắt tỉa Có cấu trúc Cấp độ Neuron sử dụng Điều chỉnh Phân cực.. Trong NeurIPS.

[87] Barret Zoph và Quoc V Le. 2016. Tìm kiếm kiến trúc neural với học tăng cường. arXiv preprint arXiv:1611.01578 (2016).

[88] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, và Quoc V Le. 2018. Học kiến trúc có thể chuyển giao cho nhận dạng hình ảnh có thể mở rộng. Trong Thủ tục của hội nghị IEEE về thị giác máy tính và nhận dạng mẫu. 8697–8710.
