# 2103.12424.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2103.12424.pdf
# File size: 5533096 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
BossNAS: Exploring Hybrid CNN-transformers with Block-wisely
Self-supervised Neural Architecture Search
Changlin Li1, Tao Tang2, Guangrun Wang3;4, Jiefeng Peng3, Bing Wang5,
Xiaodan Liang2*, Xiaojun Chang6
1GORSE Lab, Dept. of DSAI, Monash University2Sun Yat-sen University3DarkMatter AI Research
4University of Oxford5Alibaba Group6RMIT University
changlin.li@monash.edu,
ftrent.tangtao,wanggrun,jiefengpeng,xdliang328 g@gmail.com,
fengquan.wb@alibaba-inc.com, xiaojun.chang@rmit.edu.au
Abstract
A myriad of recent breakthroughs in hand-crafted neu-
ral architectures for visual recognition have highlighted the
urgent need to explore hybrid architectures consisting of
diversiﬁed building blocks. Meanwhile, neural architecture
search methods are surging with an expectation to reduce
human efforts. However, whether NAS methods can efﬁ-
ciently and effectively handle diversiﬁed search spaces with
disparate candidates (e.g. CNNs and transformers) is still
an open question. In this work, we present Block-wisely
Self-supervised Neural Architecture Search (BossNAS), an
unsupervised NAS method that addresses the problem of in-
accurate architecture rating caused by large weight-sharing
space and biased supervision in previous methods. More
speciﬁcally, we factorize the search space into blocks and
utilize a novel self-supervised training scheme, named en-
semble bootstrapping, to train each block separately before
searching them as a whole towards the population center.
Additionally, we present HyTra search space, a fabric-like
hybrid CNN-transformer search space with searchable down-
sampling positions. On this challenging search space, our
searched model, BossNet-T, achieves up to 82.5% accuracy
on ImageNet, surpassing EfﬁcientNet by 2.4% with compara-
ble compute time. Moreover, our method achieves superior
architecture rating accuracy with 0.78 and 0.76 Spearman
correlation on the canonical MBConv search space with Im-
ageNet and on NATS-Bench size search space with CIFAR-
100, respectively, surpassing state-of-the-art NAS methods.1
1. Introduction
The development of neural network architectures has
brought about signiﬁcant progress in a wide range of visual
recognition tasks over the past several years. Representative
*Corresponding Author.
1Code: https://github.com/changlin31/BossNAS .
Label
(a) NAS with large weight-sharing space ( 39).
Label
(b) Block-wise NAS with biased supervision.
Label
(c) Block-wisely self-supervised NAS.
Figure 1: Comparision of three NAS schemes. Red arrows
represent the supervision during training and searching.
examples of such models include ResNet [ 25], SENet [ 31],
MobileNet [ 30] and EfﬁcientNet [ 64]. Recently, the newly
emerging attention-based architectures are coming to the
forefront in the vision ﬁeld, challenging the dominance of
convolutional neural networks (CNNs). This exciting break-
through in vision transformers led by ViT [ 20] and DETR
[8], are achieving competitive performance on various vi-
sion tasks, such as image classiﬁcation [ 20,66,79,14,9],
object detection [ 8,90,62], semantic segmentation [ 88], and
others [ 26,50,33]. As suggested by prior works [ 20,62,3],
hybrids of CNNs and transformers can outperform both pure
transformers and pure CNNs.
Despite the large advances brought about by network
design, manually ﬁnding well-optimized hybrid architec-
tures can be challenging, especially as the number of design
choices increases. Neural Architecture Search (NAS) is a
popular approach to reducing the human effort in network
architecture design by automatically searching for optimal
1arXiv:2103.12424v3  [cs.CV]  19 Aug 2021

--- PAGE 2 ---
architectures in a predeﬁned search space. Representative
success in performing NAS on manually designed build-
ing blocks include MobileNetV3 [ 29], EfﬁcientNet [ 64],
etc.These works are searched by multi-trial NAS methods
[63,92,2,89,12,47], which are computationally prohibitive
(costing thousands of GPU days). Recent weight-sharing
NAS methods [ 6,53,4,43] encode the entire search space
as a weight-sharing supernet to avoid repetitive training of
candidate networks, thus largely reducing the search cost.
However, as shown in Fig. 1a, architecture search
spaces with layer-level granularity grow exponentially with
increased network depth, which has been identiﬁed (in
[37,39]) as the main culprit of inaccurate architecture rat-
ing2in weight-sharing NAS methods. To reduce the size
of the large weight-sharing space, previous works [ 37,46]
factorize the search space into blocks and use a pretrained
teacher model to provide block-wise supervision (Fig. 1b).
Despite their high ranking correlation and high efﬁciency, we
ﬁnd (in Sec. 5) their results to be highly correlated with the
teacher architecture. As illustrated in Fig. 1b, when training
by a teacher with blue nodes, candidate architectures with
more blue nodes tend to get higher ranks in these methods.
This limits its application on diversiﬁed search spaces with
disparate candidates, such as CNNs and transformers.
On the other hand, unsupervised NAS [ 41] has recently
emerged as an interesting research topic. Without access
to any human-annotated labels, unsupervised NAS methods
(optimized with pretext tasks [ 41] or random labels [ 87])
have been proven capable of achieving comparable perfor-
mance to supervised NAS methods. Accordingly, we pro-
pose to use an unsupervised learning method as an alternative
to supervised distillation in the aforementioned block-wise
NAS scheme (Fig. 1c), aiming to address the problem of
architectural bias caused by the use of the teacher model.
In this work, we propose a novel unsupervised NAS
method, Block-wisely Self-supervised Neural Architecture
Search ( BossNAS ), which aims to address the problem of
inaccurate predictive architecture ranking caused by a large
weight-sharing space while avoiding possible architectural
bias caused by the use of the teacher model. As opposed
to the block-wise solutions discussed above, which utilize
distillation as intermediate supervision, we propose a self-
supervised representation learning scheme named ensemble
bootstrapping to optimize each block of our supernet. To
be more speciﬁc, each sampled sub-networks are trained to
predict the probability ensemble of all the sampled ones in
the target network, between different augmented views of the
same image. In the searching stage, an unsupervised eval-
uation metric , is proposed to ensure fairness by searching
towards the architecture population center. More speciﬁ-
cally, the probability ensemble of all the architectures in the
population is used as the evaluation target to measure the
2In this work, architecture rating accuracy refers to the correlation of
the predicted architecture ranking and the ground truth architecture ranking.performance of the sampled models.
Additionally, we design a fabric-like hybrid CNN-
transformer search space ( HyTra ) with searchable down-
sampling positions and use it as a case study for hybrid
architectures to evaluate our method. In each layer of HyTra
search space, CNN building blocks and transformer build-
ing blocks of different resolutions are in parallel and can be
chosen ﬂexibly. This diversiﬁed search space covers pure
transformers with ﬁxed content length and normal CNNs
with progressively reduced spatial scales.
We prove that our NAS method can generalize well on
three different search spaces and three datasets. On HyTra
search space, our searched models outperforms the ones
searched by our supervised NAS counterpart [ 37], prov-
ing that our method successfully avoids possible architec-
ture bias brought by supervised distillation. Our method
achieves superior architecture rating accuracy with 0.78 and
0.76 Spearman correlation on the canonical MBConv search
space with ImageNet and on NATS-Bench sizesearch space
SS[17] with CIFAR-100, respectively, surpassing state-of-
the-art NAS methods, proving that our method successfully
suppressed the problem of inaccurate architecture rating
caused by large weight-sharing space.
Our searched models on HyTra search space achieves
82.5% accuracy on ImageNet, surpassing EfﬁcientNet [ 64]
by 2.4%, with comparable compute time3. By providing
strong results through BossNet-T, we hope that this diver-
siﬁed HyTra search space with disparate candidates and
high-performance architectures can serve as a new arena for
future NAS works. We also hope that our BossNAS can
serve as a widely used tool for hybrid architecture design.
2. Related Works
Block-wise weight-sharing NAS [37,46,84,85] ap-
proaches factorize the supernet into independently optimized
blocks and thus reduce the weight-sharing space, resolving
the issue of inaccurate architecture ratings caused by weight-
sharing. DNA [ 37] ﬁrst introduced the block-wisely super-
vised architecture rating scheme with knowledge distillation.
Based on this scheme, DONNA [ 46] further propose to pre-
dict an architecture rating using a linear combination of its
blockwise ratings rather than a simplistic sum. SP [ 84] were
the ﬁrst to apply this scheme to network pruning. However,
all of the aforementioned methods rely on a supervised distil-
lation scheme, which inevitably introduces architectural bias
from the teacher. We accordingly propose a block-wisely
self-supervised scheme, which completely casts off the yoke
of the teacher architecture.
Unsupervised NAS [41,87] methods perform architecture
search without access to any human-annotated labels. Un-
NAS [ 41] introduced unsupervised pretext tasks [35,48,86]
to weight-sharing NAS for supernet training and architecture
3Following [ 62],compute time refers to the time spent for forward and
backward passes.
2

--- PAGE 3 ---
rating. RLNAS [ 87] optimized the supernet using random
labels [81,45] and further rated architectures by means of a
convergence-based angle metric [ 32]. Another line of NAS
methods [ 72,69,27,51] belonging to the category of su-
pervised NAS perform unsupervised pretraining of network
accuracy predictor or supernet before supervised ﬁnetun-
ing or evaluation. Differing from aforementioned works in
motivation and methodology, we explore self-supervised con-
trastive learning methods in our unsupervised NAS scheme
to avoid the supervision bias in block-wise NAS.
Self-supervised contrastive learning methods [ 49,71,28,
65,91,24,10] have signiﬁcantly advanced the unsupervised
learning of visual representations. These approaches learn
visual representations in a discriminative fashion by gath-
ering the representations of different views from the same
image and spreading those from different images. Recently,
the innovative BYOL [ 21] and SimSiam [ 11] learned visual
representations without the use of negative examples. These
works directly predict the representation of one view from
another using a pair of Siamese networks with the same archi-
tectures and shared weights [ 11], or with one of the Siamese
network branches being a momentum encoder, thereby form-
ing a bootstrapping scheme [ 21]. Our work introduces a
novel bootstrapping scheme with probability ensemble to
Siamese supernets .
Architecture Search Spaces. Cell-based search spaces, ﬁrst
proposed in [93], are generally used in previous NAS meth-
ods [ 42,54,43,52] and benchmarks [ 74,19,17]. They
search for a repeatable cell-level architecture, while keep-
ing a manually designed network-level architecture. By
contrast, network-level search spaces with layer-level gran-
ularity [ 7,70,15,37,46,87] and block-level granularity
[63,29,64] search for the macro network-level structure us-
ing manually designed building blocks ( e.g. MBConv [56]).
Auto-DeepLab [ 40] presents a hierarchical search space for
semantic segmentation, with repeatable cells and a fabric-
like [ 57] network-level structure. Our HyTra search space
also has a fabric-like network-level structure, albeit with
layer-level granularity rather than repeated cells.
3. Block-wisely Self-supervised NAS
In this section, we ﬁrst brieﬂy introduce the dilemma
of NAS and its block-wise solutions [ 37,46,84,85], then
present our proposed BossNAS in detail, along with its two
key elements: i)unsupervised supernet training phase with
ensemble bootstrapping ;ii)unsupervised architecture rating
and searching phase towards architecture population center.
Notations. We denote scalars, tensors and sets of tensors
using lower case, bold lower case and upper case calligraphic
letters respectively ( e.g.,n,xandX). For simplicity, we use
fxngto denote the setfxngjnj
n=1with cardinalityjnj.
3.1. Dilemma of NAS and the Block-wise Solutions
Dilemma of NAS: efﬁciency or accuracy. While classical
sample-based NAS methods produce accurate architectureratings, they are also computationally prohibitive. Weight-
sharing rating scheme in one-shot NAS methods has brought
about a tremendous reduction of search cost by encoding the
entire search space Ainto a weight-sharing supernet, with
the weightsWshared by all the candidate architectures and
optimized concurrently as: W= arg min
WLtrain(W;A;x;y).
HereLtrain()denotes the training loss function, while x
andydenote the input data and the labels, respectively.
Subsequently, architectures are searched based on the
ranking of their ratings with these shared network weights.
Without loss of generality, we choose the evaluation loss
function Lvalas the rating metric; the searching phase can
be formulated as: = arg min
82ALval(W;;x;y). However,
the architecture ranking based on the shared weights W
does not necessarily represents the correct ranking of the
architectures, as the weights inherited from the supernet
are highly entangled and are not fully and fairly optimized.
As pointed out in the literature [ 58,73,80], weight-sharing
methods suffer from low architecture rating accuracy.
Block-wisely supervised NAS. As proven theoretically and
experimentally by [ 39,37,46], reducing the weight-sharing
space ( i.e.total number of weight-sharing architectures)
can effectively improve the accuracy of architecture rat-
ing. In practice, block-wise solutions [ 37,46,84,85] ﬁnd a
way out of this dilemma of NAS by block-wisely factorizing
the search space in the depth dimension, thus reducing the
weight-sharing space while maintaining the original size of
thesearch space . Given a supernet consisting of jkjblocks
S(W;A) =fSk(Wk;Ak)g, withW=fWkgandA=fAkg
denoting its weights and architecture that are block-wisely
separable in the depth dimension, each block of the supernet
is trained separately before searching among all blocks in
combination by the sum [ 37], or a linear combination [ 46]
(with weightsfkg), of each block’s evaluation loss Lval:
=fkg= arg min
8fkgAjkjX
k=1kLval
W
k;k;xk;yk
s:t:W
k= arg min
WkLtrain(Wk;Ak;xk;yk):(1)
To isolate the training of each supernet block, given an
inputx, the intermediate input and target fxk;ykgof the
k-th block is generated by a ﬁxed teacher network T(with
architectureTand ground-truth weights WT):fx1;y1g=
fx;T1(x)g, andfxk;ykg=fTk 1(x);Tk(x)g;k > 1, where
Tkrepresents the teacher network truncated after the k-th
block. As the data used for both training and searching
phase are generated by the teacher model T(WT;T), the
architecture ratings are likely to be highly correlated with the
teacher architecture. For instance, a convolutional teacher
have a limited receptive ﬁeld and distinctive architectural
inductive biases like translation equivariance . With such a
biased supervision, candidate architectures are likely to be
trained and rated unfairly. We observes two phenomenons
that can be attrbute to the biased supervision, i.e. candidate
3

--- PAGE 4 ---
Online 
CandidatesEMA 
CandidatesStop 
GradientOnline Supernet
l2
 l2
EMA Supernet
E
Block k
Supernet 
BlocksDownsample 
& MLPsFeature 
FlowBest 
Path
EEnsemble 
Module
l2L2 
Distance
Figure 2: Illustration of the Siamese supernets training with ensemble bootstrapping.
preference andteacher preference . Detailed experimental
analysis of these two phenomenons is provided in Sec. 5. To
break these restrictions of current block-wise NAS solutions,
we explore a scheme without using a teacher model.
3.2. Training with Ensemble Bootstrapping
Starting from the dual network scheme with student-
teacher pairfS(W;A);T(WT;T)g, the ﬁrst step to cast
off the yoke of the teacher architecture is to assign T=A,
thus forming a pair of Siamese supernets .
Bootstrapping with Siamese Supernets. To optimize such
Siamese networks block-wisely, we adopt a self-supervised
contrastive learning scheme. More speciﬁcally, these two
supernets receive a pair of augmented views fx1;x2g
of the same training sample xand generate the outputs
fS(W;A;x1);T(WT;A;x2)g, respectively. Analogous to
previous teacher-student settings, the Siamese supernets are
optimized by minimizing the distance between their out-
puts. In previous Siamese networks and self-supervised
contrastive learning methods, the two networks either share
their weights [ 10,11] (i.e.WT=W) or form a mean teacher
scheme with Exponential Moving Average (EMA) [ 24,21]
(i.e.WT=W, whereW
t=W
t 1+(1 )Wtrepresents
the temporal average of W, withtbeing a training timestamp,
anddenoting the momentum factor that controls the updat-
ing speed ofW). By learning representation from the mean
teacher, analogous to the simple yet powerful BYOL [ 21],
our supernet can be optimized in an unsupervised manner
without relying on a fully supervised teacher network:
W
k= arg min
WkLtrain
fWk;W
kg;Ak;xk
: (2)
To eliminate the inﬂuence of pixel-wise differences between
two intermediate representations caused by augmentations
(e.g.random crop), as well as to ensure better generalization
on candidate architectures with different reception ﬁelds or
even different resolutions, we project the representations to
the latent space before calculating the element-wise distance.
Ensemble Bootstrapping. However, unlike single net-
works, supernets are typically optimized by path sampling
strategies, e.g. single path [ 22] or fair path [ 15]. When
naively adopting bootstrapping, each sub-network learns
from the moving average of itself. In the absence of a com-
mon objective, the weights shared by different sub-networks
suffer from convergence hardship, leading to training in-stability and inaccurate architecture ratings. To address
this problem, we propose an unsupervised supernet train-
ing scheme, named ensemble bootstrapping .
Consideringjpjsub-networksfpgA ksampled from
thek-th block of the search space Ain thet-th training itera-
tion, and given a training sample x,jpjpairs of augmented
viewsfxpgpaug(jx),fx0
pgp0
aug(jx)are generated for
each sampled sub-network of the Siamese supernets. To
form a common objective for all paths, we can use a scheme
analogous to ensemble distillation [ 59,60] in supervised
learning. As illustrated in Fig. 2, each sampled sub-network
of the online supernet learns to predict the probability en-
semble of all sampled sub-networks in the EMA supernet:
cTk
fpg;fx0
pg
=1
jpjjpjX
p=1Tk(W;p;x0
p): (3)
In summary, the block-wisely self-supervised training pro-
cess of the Siamese supernets is formulated as follows:
W
k= arg min
WkjpjX
p=1Ltrain
fWk;W
kg;fpg;x
;
where Ltrain
fWk;W
kg;fpg;x
=Sk(Wk;p;xp) cTk
W
k;fpg;fx0
pg2
2:(4)
3.3. Searching Towards the Population Center
After the convergence of the Siamese supernets is com-
plete, the architectures can be ranked and searched by the
rating determined based on the weight of the supernets, as
in Eqn. 1. In this section, we design a fair and effective
unsupervised rating metric Lvalfor searching phase.
To evaluate the performance of a network trained with
contrastive self-supervision, previous works [ 24,10,21,11]
have utilized supervised metrics, such as accuracies of linear
evaluation or few-shot classiﬁcation. To develop an unsuper-
vised NAS method, we aim to avoid schemes that depend
on human-annotated labels and instead pursue a completely
unsupervised evaluation metric. Previous unsupervised NAS
methods [ 41,87] utilize either the accuracy of pretext tasks
or convergence measurement with angle-based metrics to
rate candidate architectures. Unfortunately, the losses of
self-supervised contrastive learning do not necessarily repre-
sent either the architecture performance or the architecture
4

--- PAGE 5 ---
Hybrid CNN-Transformer Search Space
1
1/4
1/8
1/16
1/32Conv
orP
P
MHSAInput
Output1 2 3 4 L L-1......5 L-2Scale
28x28x512
14x14x1024 14x14x1024
Pposition 
encodingor
1x1 
convMHSA self-attention Conv3x3 
convFigure 3: Illustration of the fabric-like Hybrid CNN-transformer Search Space with ﬂexible down-sampling positions.
convergence, as the input views and target networks are
both randomly sampled . Moreover, the target networks are
somewhat biased and cannot serve as ground truth targets.
To avoid these concerns, we propose a fair and effective
unsupervised evaluation metric for architecture search.
Without loss of generality, we consider searching with an
evolutionary algorithm [12,54], where architectures are op-
timized by evolving an architecture population fpg. Analo-
gous to the optimization of the weights, we propose to use
probability ensemble among the population fpgas the com-
mon target to provide a fair rating for each architecture p.
Additionally, one pair of views fx1;x2gfor each validation
samplesxare generated and ﬁxed to avoid the bias intro-
duced by variable augmentation. In parallel to Eqn. 3, we
have the probability ensemble of the architecture population:
cSk
fpg;x2
=1
jpjjpjX
p=1Sk(p;x2): (5)
In practice, by dividing the supernet into medium-sized
blocks ( e.g.4 layers of 4 candidates, 44= 256 architectures),
traversal evaluation of all the candidate architectures are
affordable. In this case, the architecture population fpgis
expanded to the whole block-wise search space Ak, and the
whole searching process is ﬁnished in a single step:
= arg min
82AjkjX
k=1kLval(;xk)
where Lval(;x) =Sk(;x1) bSk(Ak;x2)2
2:(6)
4. Hybrid CNN-transformer Search Space
In this section, we present a fabric-like hybrid CNN-
transformer search space, named HyTra, with disparate can-
didate building blocks and ﬂexible down-sampling positions.
4.1. CNN and Transformer Candidate Blocks
The ﬁrst step in designing a hybrid CNN-transformer
search space is to include the proper CNN and transformer
building blocks. These two types of building blocks should
be able to perform well either when simply aggregated in
sequence or when combined freely. We choose the classical
and robust residual bottleneck (ResConv ) in ResNet [ 25] as
the CNN candidate building block. In parallel, we design a
lightweight and robust transformer building block ResAtt
based on the pluggable BoTBlock [62] and NLBlock [68].
Computation Balancing with Implicit Position Encod-
ings. To facilitate fair and meaningful competition, can-
P MHSAP
MHSACPVT BoTFigure 4: Transformer blocks in CPVT [14] and BoT [62].
didate building blocks should have similar computation com-
plexities. The original BoTBlock is slower than ResConv ,
as its relative position encodings are computed separately
through multiplication with the query . Simply removing
the content-position branch from BoTBlocks, resembling to
NLBlocks, could reduce their compute time to make them
comparable to ResConv . However, position encodings are
crucial for vision transformers to achieve good performance.
In CPVT [ 14], the authors uses single convolutions in be-
tween transformer encoder blocks as the position encoding
generator . Similarly, we replace the relative position encod-
ing branch in BoTBlock with a light depthwise separable
convolution as an implicit position encoding module, form-
ing our ResAtt . By this simple modiﬁcation, we reduce the
computation complexity of position encoding module from
the originalO(CW3)toO(CW2), withCdenoting number
of channels and Wdenoting the width or height. In contrast
to CPVT and BoT (Fig. 4), our position encoding mod-
ules (Fig. 3 right) are placed between the input projection
layer and the self-attention module. In addition, our implicit
position encoding modules are also responsible for down-
sampling. This modiﬁcation is also applied to ResConv ,
which enables weight sharing between candidate blocks with
different down-sampling rates ( i.e.1 or 2).
4.2. Fabric of Hybrid CNN-transformers
Beyond the building blocks, CNNs and transformers dif-
fer considerably in terms of their macro architectures. Un-
like CNNs, which process images in stages with various
spatial sizes, transformers typically do not change sequence
length (image patches) and retains the same scale at each
layer. As shown in Fig. 3 left, to cover both the CNNs
and transformers, our search space is designed with ﬂexible
down-sampling positions, forming a fabric [57] of Hybrid
CNN-transformers. At each choice block layer of the fabric,
the spatial resolution can either stay unchanged or be reduced
to half of its scale, until reaching the smallest scale. This
fabric-like search space contains architectures resembling
the popular vision transformers [ 20,66,14], CNNs [ 25,31]
and hybrid CNN-transformers [62] at different scales.
5

--- PAGE 6 ---
Method MAdds Steptime Top-1 (%) Top-5 (%)
ResNet50 [25] 4.1B 100ms 77.7 93.9
ViT-B/32 [20] - 68ms 73.4 -
ViT-B/16 [20] 17.6B 158ms 77.9 -
BoT50 [62] 4.0B 120ms 78.3 94.2
R50-T Conv -Only 4.1B 104ms 78.2 94.2
ViT-T/32 Att-Only 2.9B 92ms 74.5 91.7
ViT-T/16 Att-Only 3.2B 96ms 76.5 93.0
BoT50-T Hybrid 3.9B 103ms 79.5 94.8
Random-T Hybrid 3.7B 84ms 76.7 93.1
BossNet-T0 w=o SE 3.4B 101ms 80.5 95.0
SENet50 [25, 31] 4.1B 129ms 79.4 94.6
EffNetB1 [64] 0.7B 131ms 79.1 94.4
DeiT-S [66] 10.1B 84ms 79.8 -
BoT50 + SE [62] 4.0B 149ms 79.6 94.6
DNA-T [37] 3.9B 121ms 80.3 95.0
UnNAS-T [41] 3.7B 104ms 79.8 94.6
BossNet-T0 3.4B 115ms 80.8 95.2
BossNet-T0 " 5.7B 147ms 81.6 95.6
SENet101 [25, 31] 7.8B 218ms 81.4 95.7
EffNetB2 [64] 1.0B 143ms 80.1 94.9
ViT-L/16 [20] 63.6B 168ms 81.1 -
DeiT-B [66] 17.6B 152ms 81.8 -
BoTNet-S1-59 [62] 7.3B 184ms 81.7 95.8
T2T-ViT-19 [79] 8.9B 158ms 81.9 -
TNT-S [23] 5.2B 468ms 81.3 95.6
BossNet-T1 7.9B 156ms 82.2 95.8
BossNet-T1 " 10.5B 165ms 82.5 96.0
Table 1: ImageNet results of state-of-the-art models and our
searched hybrid CNN-transformers . Compute steptime is
measured on a single GeForce RTX 3090 GPU with batch
size 32. Purple is used to denote manually selected architec-
tures from search space HyTra. ": Directly tested on larger
input size without ﬁnetuning ( i.e.288 for BossNet-T0 "and
256 for BossNet-T1 ").
5. Experiments
Setups. We evaluate our method on three search spaces,
including our proposed HyTra search space and other two
existing search spaces, i.e.MBConv search space [ 7,37]
and NATS-Bench sizesearch spaceSS[17]. The datasets
we use to evaluate and analyze our method are ImageNet
[16], CIFAR-10 and CIFAR-100 [ 36]. We train each block
of the supernet for 20 epochs, including one linear warm-up
epoch. We randomly sample four paths in each training step.
See Appendix A.2 for more implementation details.
5.1. Searching for Hybrid CNN-transformer
Analysis of HyTra search space. We manually stitched
four architectures on our fabic-like HyTra search space, fol-
lowing as closely as possible to previous human-designed
networks [ 25,20,62], except using our fResConv;ResAttg
building blocks. As shown in Tab. 1, these models (in
purple) consistently outperform their prototypes. Remark-
ably, BoT50-T surpasses the original BoT50 by 1.2% top-1
accuracy with 1.17compute time reduction, proving the
superiority of our designed building blocks.
Performance of searched models. With our proposed
search space and NAS method, we explore hybrid CNN-
transformer architectures on ImageNet. The results of our
searched models (BossNet-T) and models with comparable
compute time are summarized in Tab. 1.
1/42 3 4 L L-1......5 L-2 1
1/8
1/16
1/321
1/42 3 4 L L-1......5 L-2 1
1/8
1/16
1/321
1
1/4
1/8
1/161 2 3 4 L L-1......5 L-21
1/4
1/8
1/16
1/321 2 3 4 L L-1......5 L-2
1/32(a) Architecture of BossNet-T0.
1/42 3 4 L L-1......5 L-2 1
1/8
1/16
1/321
1/42 3 4 L L-1......5 L-2 1
1/8
1/16
1/321
1
1/4
1/8
1/161 2 3 4 L L-1......5 L-21
1/4
1/8
1/16
1/321 2 3 4 L L-1......5 L-2
1/32
(b) Architecture of DNA-T.
Figure 5: Visualization of architectures searched by Boss-
NAS and DNA [ 37] in HyTra search space. Blue nodes
denotes ResConv and red nodes denotes ResAtt .
Firstly , BossNet-T0 outperforms a wide range of state-of-
the-art models. For instance, BossNet-T0 without SE mod-
ule achieves 80.5% top-1 accuracy, surpassing the human-
designed hybrid CNN-transformer, BoTNet50, by 2.2%
while being 1.19faster in terms of compute time; when
equipped with SE and SiLU activation, BossNet-T0 fur-
ther achieves 80.8% top-1 accuracy, surpassing the NAS
searched EfﬁcientNet-B1 by 1.7% while being 1.14faster.
Secondly , our searched model demonstrates absolute su-
periority over manually and randomly selected models from
search space HyTra. In particular, BossNet-T0 achieves
up to 6.0% improvement over manually selected models,
proving the effectiveness of our architecture search.
Thirdly , BossNet-T0 outperforms other recent NAS
methods on search space HyTra. BossNet-T0 achieves 0.5%
accuracy gains over DNA-T, which is searched by our super-
vised NAS counterpart [37].
Finally , when extended to larger model size and input
size, the family of BossNet-T models maintain their supe-
riority. By removing the downsampling in the last stage of
BossNet-T0 (same scheme as BoTNet-S1 [ 62]), we have
BossNet-T1, which achieves 82.2% accuracy, surpassing
EfﬁcientNet-B2 by 2.1% . By directly testing on larger input
resolutions without ﬁnetuning, BossNet-T0 "(on 288288
input size) achieves 81.6% top-1 accuracy, and outperforms
BoTNet50 + SE by 2.0% with similar runtime; BossNet-
T1"(on 256256 input size) achieves 82.5% top-1 accuracy,
surpassing T2T-ViT-19 and EfﬁcientNet-B2 by 0.6% and
2.4% with comparable steptime, respectively.
Architecture visualization and analysis. We visualize the
architecture of DNA-T and BossNet-T0 in Fig. 5. DNA-
T clearly prefers convolutions, as it contains 13 ResConv
blocks and only three ResAtt blocks. By contrast, BossNet-
T0 has similar numbers of convolutions and attentions and
eventually achieves a higher accuracy. We refer this to Phe-
nomenon I: candidate preference , and attribute it to archi-
tectural bias from the teacher supervision. Without using the
teacher model, our method successfully avoids this bias.
6

--- PAGE 7 ---
74 753.535
3.530
3.525
3.520
BossNAS, =0.65
74 7570.670.770.870.971.0
SPOS, =0.18
74 752468
DARTS, =0.08
74 7543.544.044.545.045.5
MnasNet, =0.61
74 750.5
0.4
0.3
DNA (EffNet), =0.62
74 750.400
0.375
0.350
0.325
DNA (MBNet), =0.23
80828486889092941.13
1.12
1.11
1.10
1.09
1.08
1.07
1.06
1.05
Figure 6: Left: Ranking correlations of 6 different NAS methods on MBConv Search Space .Right: Architecture ranking of
BossNAS on NATS-Bench SS. In all the diagrams, x-axis denotes ground truth accuracy; y-axis denotes evaluation metrics.
Method MAdds (M) Top-1 (%) Top-5 (%)
FairNAS-A [15] 388M 75.3 92.4
ProxylessNAS [7] 465M 75.1 92.5
FBNet-C [70] 375M 74.9 -
SPOS [22] 472M 74.8 -
RLNAS [87] 473M 75.6 92.6
BossNet-M1 w=o SE 475M 76.2 93.0
MobileNetV3 [29] 219M 75.2 -
MnasNet-A3 [63] 403M 76.7 93.3
EfﬁcientNet-B0 [64] 399M 76.3 93.2
DNA-b [37] 406M 77.5 93.3
BossNet-M2 403M 77.4 93.6
Table 2: ImageNet results of state-of-the-art NAS models
onMBConv search space .
Method Search Cost R
SPOS [22] 8.5 Gds -0.18 -0.27 -0.29
DARTS [43] 50 Gds 0.08 0.14 0.06
MnasNet [63] 288 Tds 0.61 0.77 0.78
DNA [37] (EffNetB0) 8.5 Gds 0.62 0.77 0.83
DNA [37] (MBNetV1) 8.5 Gds 0.23 0.27 0.37
BossNAS 10 Gds 0.65 0.78 0.85
Table 3: Comparison of the effectiveness and efﬁciency
of different NAS methods on MBConv search space and
ImageNet dataset . (Gds: GPU days; Tds: TPU days)
5.2. Results on MBConv Search Space
To further prove the effectiveness and generalization abil-
ity of BossNAS, we compare it with a wide range of NAS
methods on MBConv search space.
Performance of searched models. As shown in Tab. 2, our
searched models, BossNet-M, achieve competitive results
in search spaces with and without SE module. In the search
space without SE, BossNet-M1, searched under constraint
of 475M MAdds, outperforms SPOS [ 22] and another recent
unsupervised NAS method, RLNAS [ 87] by1.4% and0.6% ,
respectively. In the search space with SE, BossNet-M2,
under constraint of 405M MAdds, outperforms the popular
EfﬁcientNet [ 64] by 1.1% , and is also competitive with
our supervised counterpart, DNA [ 37]. Note that candidate
building blocks in MBConv search space are quite similar,
concealing the candidate preference phenomenon in [37].
Architecture rating accuracy. As BossNAS performs
traversal search (i.e.accuracy of searching phase is 100% ),
thearchitecture rating accuracy directly represents its ef-
fectiveness. We use the 23 open-sourced architectures in
MBConv search space and their corresponding ground truth
accuracies provided by [ 37] to calculate the architecture
rating accuracy, i.e.the ranking correlation between theMethod C-10 C-100R
FBNet v2 [67] 93.14 70.72 - - -
TuNAS [5] 92.78 70.11 - - -
CE [27] 90.55 70.78 0.43 0.60 0.60
BossNAS 93.29 70.86 0.59 0.76 0.79
Table 4: Comparison of searched model accuracy and archi-
tecture rating accuracy of different NAS methods on NATS-
BenchSS(C-10: CIFAR-10 , C-100: CIFAR-100 ).
predicted architecture ranking and the ground truth model
ranking. We use three different ranking correlation metrics:
Kendall Tau ( ) [34], Spearman Rho ( ) and Pearson R ( R).
All three metrics range from -1 to 1, with “-1” representing
a completely reversed ranking, “1” meaning an entirely cor-
rect ranking, and “0” representing no correlation between
rankings. As shown in Tab. 3 and Fig. 6 left, our BossNAS
obtains the highest rating accuracies with 0.65among sota
NAS methods, while addressing two problems.
First , classic weight sharing methods, SPOS [ 22] and
DARTS [ 43], fails to achieve reasonable ranking correlation
despite their lower search costs, while the multi-trial method,
MnasNet [ 63], achieves high rating accuracies with massive
search cost. BossNAS successfully addressed such dilemma
of NAS by achieving even higher rating accuracies than
MnasNet ( e.g.0.07R) with 28.8acceleration.
Second , supervised block-wise NAS method, DNA [ 37],
fails to achieve high rating accuracies when using a teacher
largely different from the candidates (MobileNetV1 [ 30]
vs.EfﬁcientNet-based candidates [ 64]), which we refer to
asPhenomenon II: teacher preference . Our unsupervised
BossNAS achieves higher rating accuracies than DNA ( 0.03
), successfully casting off the yoke of the teacher network.
5.3. Results on NATS-Bench SS
For NATS-Bench sizesearch spaceSS, experiments are
conducted on two datasets: CIFAR-10 and CIFAR-100. Can-
didates of different channel numbers in our supernet share
the weights in a slimmable manner [78, 77, 76, 38, 9].
Performance of searched models. After searching on our
supernet, we look up the performance of searched models in
NATS-BenchSSfor fair comparision. The results are shown
in Tab. 4. Our BossNAS outperforms recent NAS methods
[67,5] designed particularly for network size search spaces,
proving the generalization ability of our method on speciﬁed
search spaces and relatively small datasets.
Architecture rating accuracy. We rate all the 32768 archi-
tectures in the search space to compare with their ground
7

--- PAGE 8 ---
Training Evaluation R
Supv:distill. Supv:distill. 0.62 0.77 0.83
Supv:class. Supv:class. 0.46 0.65 0.71
Unsupv:bootstrap. Unsupv. eval 0.12 0.15 0.28
Unsupv. EB Supv:linear eval 0.55 0.73 0.79
Unsupv. EB Unsupv. eval 0.65 0.78 0.85
Table 5: Ablation analysis of training methods and evalua-
tion methods on MBConv Search Space .
truth accuracies in the benchmark on CIFAR-10 dataset. As
shown in Fig. 6 right, all the architectures in the search
space forms a dense, spindle-shaped pattern, proving the
effectiveness of our BossNAS.
In addition, the architecture rating accuracies on CIFAR-
100 dataset are shown in Tab. 4. Our method, without
access to the ground truth architecture accuracies and even
without access to anyhuman-annotated labels, outperforms
a predictor-based NAS method [ 27], which is trained with
ground truth architecture accuracies, by a large gap ( i.e.0.16
and0.19R). More analysis on NATS-Bench SScould be
found in Appendix A.3.
5.4. Ablation Study
In this section, we perform extensive ablation studies on
MBConv search space and ImageNet to analyze our pro-
posed training and evaluation methods separately.
training methods. We compared several training meth-
ods for the block-wise supernet: (1)Supervised distillation
method ( Supv:distill.), using a pre-trained teacher model
to provide block-wise supervision, i.e.the training scheme
used in DNA [ 37](2)Supervised classiﬁcation (Supv:class.),
using real labels directly as the block-wise supervision. (3)
Unsupervised bootstrapping (Unsupv:bootstrap.), where the
Siamese supernets are optimized by bootstrapping the corre-
sponding paths in the two networks. (4)Ourunsupervised
ensemble bootstrapping method ( Unsupv:EB), where each
sampled paths are optimized by learning to predict the prob-
ability ensemble of sampled paths from the mean teacher.
As shown in Tab. 5, our training method surpasses all oth-
ers, achieving the best results in architecture rating accuracy.
In particular, by comparing the 3- rdand 5- thline, we can
see that replacing our proposed Unsupv:EB with the naive
Unsupv:bootstrap. scheme, the architecture rating accuracy
drops sharply by 0.53. Without the probability ensemble,
bootstrapping fails to reach a reasonable rating accuracy,
proving that the proposed ensemble bootstrapping is indis-
pensable for our BossNAS.
Evaluation methods. Slimilar to the ablation analysis of
training methods, we also compare our evaluation methods
with (1)Supervised distillation method ( Supv:distill.) and
(2)Supervised classiﬁcation (Supv:class.). Additionally, to
perform ablation analysis of evaluation without changing
the training method, we also compare with (3)supervised
linear evaluation ( Supv:linear eval), where architectures are
rated by ﬁxing the weights of the supernet and ﬁnetuning a
0 10 200.00.20.40.6
0 10 200.000.250.500.75
0 10 200.000.250.500.75
Kendall 
Spearman 
Pearson RFigure 7: Ranking correlations during supernet training.
weight sharing linear classiﬁer to evaluate each architecture.
(4)Our unsupervised evaluation metric ( Unsupv:eval) rate
architectures by its distance to the ensemble probability cen-
ter of the whole search space. From the last two rows of Tab.
5, we suprisingly found that our Unsupv:eval outperforms
supervised linear evaluation scheme in architecture rating by
a remarkable gap ( 0.1).
5.5. Convergence Behavior
To further demonstrate the effectiveness of BossNAS, we
investigate the architecture rating accuracy during the super-
net training process on MBConv search space with ImageNet.
The three ranking correlation metrics of our BossNAS during
its 20 training epochs are shown in Fig. 7. The architecture
rating accuracy increases rapidly in the early stage and con-
tinues to grow with minor ﬂuctuation. The rating accuracy
converges at the 12-th epoch and continues to be stable till
the end of the training phase. The stably increasing archi-
tecture rating ability proves the stability of our BossNAS.
In addition, the fast converging ranking correlation demon-
strates that our method is easy to optimize and do not require
longer training. Please refer to Appendix A.3 for analysis of
convergence behavior on NATS-Bench SS.
6. Conclusion
In this work, we present BossNAS, a general, unsuper-
vised NAS method with the ensemble bootstrapping training
technique and an unsupervised evaluation metric . Exper-
iments on three search spaces prove that our method suc-
cessfully addressed the problem of inaccurate architecture
rating caused by large weight-sharing space while avoiding
the architectural bias brought by supervised distillation. Ab-
lation analysis proved that the two components, ensemble
bootstrapping scheme and unsupervised evaluation metric ,
are both crucial for our method. Additionally, we present a
fabric-like search space named HyTra. On this challenging
search space, our searched hybrid CNN-transformer model,
achieves 82.5% accuracy on ImageNet, surpassing Efﬁcient-
Net by 2.4% with comparable compute time.
Acknowledgement
This work was supported in part by National Key R&D
Program of China under Grant No. 2020AAA0109700 and
the funding of “Leading Innovation Team of the Zhejiang
Province” (2018R01017). Dr Xiaojun Chang is partially
supported by Australian Research Council (ARC) Discov-
ery Early Career Research Award (DECRA) under grant no.
DE190100626 and Intelligence Advanced Research Projects
Activity (IARPA) via Department of Interior/Interior Busi-
ness Center (DOI/IBC).
8

--- PAGE 9 ---
References
[1]Youhei Akimoto, Shinichi Shirakawa, Nozomu Yoshinari,
Kento Uchida, Shota Saito, and Kouhei Nishida. Adaptive
stochastic natural gradient method for one-shot neural archi-
tecture search. In ICML , 2019. 12
[2]Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh
Raskar. Designing neural network architectures using re-
inforcement learning. In ICLR , 2017. 2, 12
[3]Irwan Bello. Lambdanetworks: Modeling long-range interac-
tions without attention. In ICLR , 2021. 1
[4]Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay
Vasudevan, and Quoc V . Le. Understanding and simplifying
one-shot architecture search. In ICML , 2018. 2, 12
[5]Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang
Cheng, Pieter-Jan Kindermans, and Quoc V Le. Can weight
sharing outperform random architecture search? an investiga-
tion with tunas. In CVPR , 2020. 7
[6]Andrew Brock, Theodore Lim, James M. Ritchie, and Nick
Weston. SMASH: one-shot model architecture search through
hypernetworks. In ICLR , 2018. 2, 12
[7]Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct
neural architecture search on target task and hardware. In
ICLR , 2019. 3, 6, 7, 12
[8]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
to-end object detection with transformers. In ECCV , 2020.
1
[9]Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.
AutoFormer: Searching transformers for visual recognition.
InICCV , 2021. 1, 7, 13
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-
frey Hinton. A simple framework for contrastive learning of
visual representations. In ICML , 2020. 3, 4
[11] Xinlei Chen and Kaiming He. Exploring simple siamese
representation learning. In CVPR , 2021. 3, 4
[12] Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang,
Chang Huang, Lisen Mu, and Xinggang Wang. RENAS:
reinforced evolutionary neural architecture search. In CVPR ,
2019. 2, 5
[13] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao
Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and
Zongyuan Ge. Hierarchical neural architecture search for
deep stereo matching. In NeurIPS , 2020. 12
[14] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin
Wei, Huaxia Xia, and Chunhua Shen. Conditional positional
encodings for vision transformers. arXiv:2102.10882 , 2021.
1, 5
[15] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. FairNAS: Re-
thinking evaluation fairness of weight sharing neural architec-
ture search. In ICCV , 2021. 3, 4, 7, 12
[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
InCVPR , 2009. 6, 13
[17] Xuanyi Dong, Lu Liu, Katarzyna Musial, and Bogdan Gabrys.
NATS-Bench: Benchmarking nas algorithms for architecture
topology and size. IEEE Transactions on Pattern Analysisand Machine Intelligence (TPAMI) , 2021. doi:10.1109/
TPAMI.2021.3054824 . 2, 3, 6, 12, 13
[18] Xuanyi Dong and Yi Yang. Searching for a robust neural
architecture in four GPU hours. In CVPR , 2019. 12
[19] Xuanyi Dong and Yi Yang. NAS-Bench-201: Extending the
scope of reproducible neural architecture search. In ICLR ,
2020. 3
[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021. 1, 5, 6
[21] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-
ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-
mad Gheshlaghi Azar, et al. Bootstrap your own latent: A
new approach to self-supervised learning. In NeurIPS , 2020.
3, 4, 13
[22] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,
Zechun Liu, Yichen Wei, and Jian Sun. Single path one-
shot neural architecture search with uniform sampling. In
ECCV , 2020. 4, 7, 12
[23] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chun-
jing Xu, and Yunhe Wang. Transformer in transformer.
arXiv:2103.00112 , 2021. 6
[24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual repre-
sentation learning. In CVPR , 2020. 3, 4
[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR , 2016.
1, 5, 6, 13
[26] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,
and Wei Jiang. TransReID: Transformer-based object re-
identiﬁcation. In ICCV , 2021. 1
[27] Daniel Hesslow and Iacopo Poli. Contrastive embeddings for
neural architectures. arXiv:2102.04208 , 2021. 3, 7, 8, 13
[28] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,
Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua
Bengio. Learning deep representations by mutual information
estimation and maximization. In ICLR , 2019. 3
[29] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, Quoc V . Le, and Hartwig
Adam. Searching for MobileNetV3. In ICCV , 2019. 2, 3, 7,
12
[30] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient con-
volutional neural networks for mobile vision applications.
arXiv:1704.04861 , 2017. 1, 7
[31] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation
networks. In CVPR , 2018. 1, 5, 6
[32] Yiming Hu, Yuding Liang, Zichao Guo, Ruosi Wan, Xiangyu
Zhang, Yichen Wei, Qingyi Gu, and Jian Sun. Angle-based
search space shrinking for neural architecture search. In
ECCV , 2020. 3
9

--- PAGE 10 ---
[33] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Trans-
gan: Two transformers can make one strong gan.
arXiv:2102.07074 , 2021. 1
[34] Maurice G Kendall. A new measure of rank correlation.
Biometrika , 30(1/2):81–93, 1938. 7
[35] Nikos Komodakis and Spyros Gidaris. Unsupervised repre-
sentation learning by predicting image rotations. In ICLR ,
2018. 2, 13
[36] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Master’s thesis, Department of
Computer Science, University of Toronto , 2009. 6, 13
[37] Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang,
Xiaodan Liang, Liang Lin, and Xiaojun Chang. Blockwisely
supervised neural architecture search with knowledge distilla-
tion. In CVPR , 2020. 2, 3, 6, 7, 8, 12, 13
[38] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang,
Zhihui Li, and Xiaojun Chang. Dynamic Slimmable Network.
InCVPR , 2021. 7, 13
[39] Xiang Li, Chen Lin, Chuming Li, Ming Sun, Wei Wu, Junjie
Yan, and Wanli Ouyang. Improving one-shot nas by suppress-
ing the posterior fading. In CVPR , 2020. 2, 3
[40] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig
Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei. Auto-deeplab:
Hierarchical neural architecture search for semantic image
segmentation. In CVPR , 2019. 3
[41] Chenxi Liu, Piotr Doll ´ar, Kaiming He, Ross Girshick, Alan
Yuille, and Saining Xie. Are labels necessary for neural
architecture search? In ECCV , 2020. 2, 4, 6, 13
[42] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang,
and Kevin Murphy. Progressive neural architecture search. In
ECCV , 2018. 3, 12
[43] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS:
differentiable architecture search. In ICLR , 2019. 2, 3, 7, 12
[44] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. In ICLR , 2017. 13
[45] Hartmut Maennel, Ibrahim M Alabdulmohsin, Ilya O Tol-
stikhin, Robert Baldock, Olivier Bousquet, Sylvain Gelly, and
Daniel Keysers. What do neural networks learn when trained
with random labels? In NeurIPS , 2020. 3
[46] Bert Moons, Parham Noorzad, Andrii Skliar, Giovanni Mar-
iani, Dushyant Mehta, Chris Lott, and Tijmen Blankevoort.
Distilling optimal neural networks: Rapid search in diverse
spaces. arXiv:2012.08859 , 2020. 2, 3, 12
[47] Renato Negrinho and Geoffrey J. Gordon. Deeparchitect:
Automatically designing and training deep architectures.
arXiv:1704.08792 , 2017. 2
[48] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In ECCV ,
2016. 2
[49] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Rep-
resentation learning with contrastive predictive coding.
arXiv:1807.03748 , 2018. 3
[50] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age transformer. In ICML , 2018. 1[51] Jiefeng Peng, Jiqi Zhang, Changlin Li, Guangrun Wang, Xi-
aodan Liang, and Liang Lin. Pi-NAS: Improving neural
architecture search by reducing supernet training consistency
shift. In ICCV , 2021. 3, 12
[52] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff
Dean. Efﬁcient neural architecture search via parameters
sharing. In ICML , 2018. 3, 12
[53] Hieu Pham, Melody Y . Guan, Barret Zoph, Quoc V . Le, and
Jeff Dean. Efﬁcient neural architecture search via parameter
sharing. In ICML , 2018. 2
[54] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V .
Le. Regularized evolution for image classiﬁer architecture
search. In AAAI , 2019. 3, 5, 12
[55] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang,
Zhihui Li, Xiaojiang Chen, and Xin Wang. A comprehensive
survey of neural architecture search: Challenges and solutions.
ACM Computing Surveys , 2021. 12
[56] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey
Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR , 2018. 3
[57] Shreyas Saxena and Jakob Verbeek. Convolutional neural
fabrics. In NeurIPS , 2016. 3, 5
[58] Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Musat,
and Mathieu Salzmann. Evaluating the search phase of neural
architecture search. In ICLR , 2020. 3
[59] Zhiqiang Shen, Zhankui He, and Xiangyang Xue. Meal:
Multi-model ensemble via adversarial learning. In AAAI ,
2019. 4
[60] Zhiqiang Shen and Marios Savvides. Meal v2: Boosting
vanilla resnet-50 to 80%+ top-1 accuracy on imagenet without
tricks. arXiv:2009.08453 , 2020. 4
[61] Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James Kwok, and
Tong Zhang. Bridging the gap between sample-based and
one-shot neural architecture search with bonas. In NeurIPS ,
2020. 12
[62] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon
Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck trans-
formers for visual recognition. In CVPR , 2021. 1, 2, 5, 6
[63] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,
Mark Sandler, Andrew Howard, and Quoc V . Le. Mnasnet:
Platform-aware neural architecture search for mobile. In
CVPR , 2019. 2, 3, 7, 12
[64] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking
model scaling for convolutional neural networks. In ICML ,
2019. 1, 2, 3, 6, 7, 12, 13
[65] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive
multiview coding. arXiv:1906.05849 , 2019. 3
[66] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efﬁcient image transformers & distillation through atten-
tion. In ICML , 2021. 1, 5, 6, 13
[67] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuan-
dong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu,
Kan Chen, et al. Fbnetv2: Differentiable neural architecture
search for spatial and channel dimensions. In CVPR , 2020. 7
[68] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming
He. Non-local neural networks. In CVPR , 2018. 5
10

--- PAGE 11 ---
[69] Chen Wei, Yiping Tang, Chuang Niu, Haihong Hu, Yue Wang,
and Jimin Liang. Self-supervised representation learning for
evolutionary neural architecture search. arXiv:2011.00186 ,
2020. 3
[70] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,
Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing
Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient con-
vnet design via differentiable neural architecture search. In
CVPR , 2019. 3, 7, 12
[71] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In CVPR , 2018. 3
[72] Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang.
Does unsupervised architecture representation learning help
neural architecture search? In NeurIPS , 2020. 3
[73] Antoine Yang, Pedro M. Esperan c ¸a, and Fabio Maria Carlucci.
NAS evaluation is frustratingly hard. In ICLR , 2020. 3
[74] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real,
Kevin Murphy, and Frank Hutter. Nas-bench-101: Towards
reproducible neural architecture search. In ICML , 2019. 3
[75] Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd
batch size to 32k for imagenet training. arXiv:1708.03888 ,
2017. 13
[76] Jiahui Yu and Thomas Huang. Autoslim: Towards one-shot
architecture search for channel numbers. arXiv:1903.11728 ,
2019. 7, 13
[77] Jiahui Yu and Thomas S. Huang. Universally slimmable
networks and improved training techniques. In ICCV , 2019.
7, 13
[78] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and
Thomas S. Huang. Slimmable neural networks. In ICLR ,
2019. 7, 13
[79] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-
to-token vit: Training vision transformers from scratch on
imagenet. In ICCV , 2021. 1, 6, 13
[80] Arber Zela, Julien Siems, and Frank Hutter. Nas-bench-
1shot1: Benchmarking and dissecting one-shot neural archi-
tecture search. In ICLR , 2019. 3, 12
[81] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
and Oriol Vinyals. Understanding deep learning requires
rethinking generalization. arXiv:1611.03530 , 2016. 3
[82] Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, Zongyuan
Ge, and Steven W. Su. Differentiable neural architecture
search in equivalent space with exploration enhancement. In
NeurIPS , 2020. 12
[83] Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and
Steven W. Su. Overcoming multi-model forgetting in one-shot
NAS with diversity maximization. In CVPR , 2020. 12
[84] Mingyang Zhang and Linlin Ou. Stage-wise channel pruning
for model compression. arXiv:2011.04908 , 2020. 2, 3
[85] Man Zhang, Yong Zhou, Jiaqi Zhao, Shixiong Xia, Jiaqi
Wang, and Zizheng Huang. Semi-supervised blockwisely
architecture search for efﬁcient lightweight generative adver-
sarial network. Pattern Recognition , 112:107794, 2021. 2,
3[86] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In ECCV , 2016. 2
[87] Xuanyang Zhang, Pengfei Hou, Xiangyu Zhang, and Jian
Sun. Neural architecture search with random labels. In CVPR ,
2021. 2, 3, 4, 7, 12
[88] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xi-
ang, Philip HS Torr, et al. Rethinking semantic segmentation
from a sequence-to-sequence perspective with transformers.
InCVPR , 2021. 1
[89] Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-
Lin Liu. Practical block-wise neural network architecture
generation. In CVPR , 2018. 2
[90] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable DETR: Deformable transformers
for end-to-end object detection. In ICLR , 2021. 1
[91] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local
aggregation for unsupervised learning of visual embeddings.
InICCV , 2019. 3
[92] Barret Zoph and Quoc V . Le. Neural architecture search with
reinforcement learning. In ICLR , 2017. 2, 12
[93] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
Le. Learning transferable architectures for scalable image
recognition. In CVPR , 2018. 3
11

--- PAGE 12 ---
BossNAS: Exploring Hybrid CNN-transformers with Block-wisely
Self-supervised Neural Architecture Search
Supplementary Material
Changlin Li1, Tao Tang2, Guangrun Wang3;4, Jiefeng Peng3, Bing Wang5,
Xiaodan Liang2*, Xiaojun Chang6
1GORSE Lab, Dept. of DSAI, Monash University2Sun Yat-sen University3DarkMatter AI Research
4University of Oxford5Alibaba Group6RMIT University
changlin.li@monash.edu,
ftrent.tangtao,wanggrun,jiefengpeng,xdliang328 g@gmail.com,
fengquan.wb@alibaba-inc.com, xiaojun.chang@rmit.edu.au
A. Appendix
A.1. A brief review of NAS
NAS methods aim to automatically optimize neural net-
work architectures by exploring search spaces with search
algorithms and evaluating architectures by means of rating
schemes . NAS methods can be divided into two categories
depending on the rating scheme utilized, i.e. multi-trial
NAS and weight-sharing NAS. Multi-trial NAS methods
[92,2,54,63,42,83] rate all sampled architectures by train-
ing them from scratch, making this process computationally
prohibitive and difﬁcult to deploy on large datasets. They
either perform architecture rating by training on relatively
small datasets ( e.g. CIFAR-10) [ 92,2,54] or by training
for the ﬁrst few epochs ( e.g.5 epochs) [ 63] on ImageNet.
To avoid repeated training of candidate networks, weight-
sharing NAS methods [ 7,43,18,1,6,13,82] optimize
asupernet that encodes the whole search space, then rate
each candidate architecture according to its weights inherited
from the supernet. Among them, gradient-based approaches
[43,7,70] and sampler-based approaches [ 52,61] jointly
optimize the weight of the supernet and the factors (or agent)
used to choose the architecture; for their part, one-shot ap-
proaches1[22,15,6,4,51] optimize the supernet before
performing a search with the frozen supernet weights. We
refer to [55] for a more comprehensive NAS review.
*Corresponding Author.
1In this paper, following the pioneering works SMASH [ 6] and One-
shot [ 4], when we refer to one-shot NAS methods, we are discussing those
incorporating two-stage (i.e., a supernet training stage and a searching
stage) weight-sharing methods rather than the general weight-sharing NAS
discussed in [80].A.2. Implementation Details
Search spaces. We evaluate our method on three search
spaces:
•HyTra search space. The beginning of the networks
in this search space is the classic ResNet stem that
reduces the spatial resolution by a factor of 4 with
a strided 77convolution layer and a max-pooling
layer. It contains L= 16 choice block layers in to-
tal, as the same to ResNet50. Before the ﬁrst choice
block layer, the input can be further down-sampled to
different scales. The downsampling module consists
of multiple 33convolutions with stride of 2. At each
choice block layer, the spatial resolution can either stay
unchanged or be reduced to half of its scale, unless
reaching the smallest scale 1=32. As introduced in Sec.
4, this search space contains two disparate candidate
choices:fResConv;ResAttg. As transformer blocks are
expensive in the ﬁrst scales, we only enable the choice
ofResAtt in the last two scales ( i.e.1=16and1=32).
The total size of this challenging hybrid search space is
roughly 2:8106.
•MBConv search space. MobileNet-like search space
and its variations are generally used as benchmarks for
recent NAS methods [ 63,29,64,7,70,15,37,46,87].
Following Li et. al. [37], we use a search space with 18
layers and each layer contains 4 candidate MobileNet
blocks (combination of kernel size f3;5gand reduc-
tion ratef3;6g). This results in a large search space
containing about 4186:91010architectures.
•NATS-Bench SS.The NATS-Bench sizesearch space
SS[17] is a channel conﬁguration search space built
12

--- PAGE 13 ---
upon a ﬁxed cell-based architecture with 5 layers, where
the 2-nd and 4-th layers have a down-sample rate of
2. Number of channels in each layer is chosen from
f8, 16, 24, 32, 40, 48, 56, 64 g.SShas85= 32768
architecture candidates in total. Candidates of different
channel numbers in our supernet share the weights in
a slimmable manner [ 78,77,76,38,9]. We divide the
supernet into 3 blocks, according to spatial size.
Datasets. The datasets we use to evaluate and analyze our
method include ImageNet [ 16], CIFAR-10 and CIFAR-100
[36].ImageNet is a large-scale dataset containing 1.2 M
train set images and 50 K valset images in 1000 classes.
We randomly samples 50 K images from the original train
set to form a NAS-valset for architecture rating and use the
remainder as the NAS-train set for supernet training. No
labels are used during training and searching of our NAS
method. Finally, our searched architectures are retrained
from scratch on train set and evaluated on valset. For
CIFAR-10 andCIFAR-100 [36], we use the splits proposed
in NATS-Bench [ 17]. CIFAR-10 is divided into 25 K train
set, 25 K valset, and 10 K test set. CIFAR-100 is devided
into 50 K train set, 5 K valset, and 5 K test set. The
ﬁnal accuracies of searched architectures are queried from
NATS-BenchSS[17].
Training details.
We train each block of the BossNAS supernet for 20
epochs including 1 linear warm-up epoch on ImageNet. For
the relatively smaller CIFAR datasets, we extend it to 30
epochs. In each training step, we randomly sample 4 paths
for the ensemble bootstrapping. Other hyperparameters for
self-supervised training of the supernet follow closely to
BYOL [ 21], we use the LARS optimizer [ 75] with a cosine
decay learning rate schedule [ 44]. The base learning rate is
set to 4.8 for a total batchsize of 4096.
For ImageNet retraining of BossNet-T models , we fol-
low similar with DeiT [ 66], as we found it robust for both
CNNs and transformers. More speciﬁcally, we use AdamW
optimizer with 1e-3 initial learning rate and cosine learning
rate scheduler, for a total batch size of 1024. Weight decay
is set to 0.05. We use model EMA with decay rate 0.99996
following [ 79]. Please refer to DeiT [ 66] for more details on
data-augmentation and regularization.
For ImageNet retraining of BossNet-M models , we fol-
low closely to EfﬁcientNet [ 64]. We use batchsize 4096,
RMSprop optimizer with momentum 0.9 and initial learning
rate of 0.256 which decays by 0.97 every 2.4 epochs. Please
refer to EfﬁcientNet [64] for more details of other settings.
Re-implementation of other NAS methods on HyTra.
For DNA [ 37], we use ResNet-50 [ 25] as the teacher
model. We divide the supernet into four blocks, with four
layers in each block, and train each block for 20 epochs. The
intermediate features of every block of the student supernet
828486889092942.92
2.90
2.88
2.86
2.84
(a) BossNAS on CIFAR-10 .
80 82 84 86 88 9087.087.287.487.687.888.088.288.4
 (b) CE [27] on CIFAR-10 .
40 50 60 703.050
3.025
3.000
2.975
2.950
2.925
2.900
2.875
(c) BossNAS on CIFAR-100 .
3540455055606570575859606162636465
 (d) CE [27] on CIFAR-100 .
Figure 8: Comparison of architecture rating and its true
accuracy of our BossNAS and CE [ 27] onNATS-Bench SS
with CIFAR datasets .
Dataset Method R
CIFAR-10CE [27] 0.42 0.60 0.59
BossNAS 0.53 0.73 0.72
CIFAR-100CE [27] 0.43 0.60 0.60
BossNAS 0.59 0.76 0.79
Table 6: Architecture rating accuracy on NATS-Bench SS
with CIFAR datasets .
and the teacher are all downsampled with global pooling and
projected with one fully-connected layer before calculating
distillation loss, as the scale of different candidate block is
not the same in HyTra search space. Other settings follow
closely to DNA [37].
For UnNAS [ 41], we adopt rotation prediction [35] (Rot)
pretext task, for its simplicity. Following [41], we use three
extra stride-2 convolution layers at the beginning of the
supernet to reduce spatial resolution. The supernet is trained
for 2 epochs as in [41].
A.3. Additional Analysis on NATS-Bench SS
Architecture rating comparison. We compare with the
predictor-based NAS method CE [ 27] by architecture rat-
ing accuracy on CIFAR-10 and CIFAR-100. As shown in
Fig. 8, we compare the two NAS methods by plotting the
correlation of the architecture rating and the true accuracy
of 3000 randomly sampled architectures from NATS-Bench
sizesearch spaceSS[17]. Architectures with BossNAS form
denser and more spindly scatter pattern than CE on both of
the two datasets. Moreover, as measured quantitatively in
Tab. 6, BossNAS outperforms CE by a large margin ( 0.11
and0.16) in both datasets.
13

--- PAGE 14 ---
0 200.00.20.4
0 200.000.250.500.75
0 200.00.20.40.6
Kendall 
Spearman 
Pearson R(a) Ranking correlations during supernet training on CIFAR-10 .
0 200.00.20.40.6
0 200.000.250.500.75
0 200.000.250.500.75
Kendall 
Spearman 
Pearson R
(b) Ranking correlations during supernet training with CIFAR-100 .
Figure 9: Convergence behavior of BossNAS on NATS-
BenchSSandCIFAR datasets .
1
1/4
1/8
1/161 2 3 4 L L-1......5 L-2
1/32
(a) Architecture of ResNet50-T.
1
1/4
1/8
1/161 2 3 4 L L-1......5 L-2
1/32
(b) Architecture of ViT-T/16.
1
1/4
1/8
1/161 2 3 4 L L-1......5 L-2
1/32
(c) Architecture of BoTNet-T.
Figure 10: Visualization of Human-designed Architectures
in HyTra. Blue nodes denotes ResConv and red nodes de-
notes ResAtt .
Convergence Behavior. We illustrate the architecture rating
accuracy of BossNAS during its 30 epoch supernet training
phase on CIFAR datasets in Fig. 9. The architecture rating
accuracy increases quickly and steadily with minor ﬂuctu-
ations, in a similar manner with that on MBConv search
space (Fig. 7). In particular, architecture rating accuracy
of our BossNAS converges to a satisfactory result, 0.76,
smoothly and quickly within only 20 epochs on CIFAR-100,
and continues to be stable for the subsequent 10 epochs.
A.4. Visualization of Human-designed Architec-
tures in HyTra
The architectures of ResNet50-T, ViT-T/16 and
BoTNet50-T from our HyTra search space are illustrated
in Fig. 10. Their architectures follow as closely as possible
to the architectures of their prototypes.
14
