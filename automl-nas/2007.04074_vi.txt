# 2007.04074.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2007.04074.pdf
# Kích thước tệp: 1241518 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Journal of Machine Learning Research 23 (2022) 1-61 Nộp bài 8/21; Sửa đổi 3/22; Xuất bản 8/22
Auto-Sklearn 2.0: AutoML Tự động thông qua Meta-Learning
Matthias Feurer1feurerm@cs.uni-freiburg.de
Katharina Eggensperger1eggenspk@cs.uni-freiburg.de
Stefan Falkner2Stefan.Falkner@de.bosch.com
Marius Lindauer3lindauer@tnt.uni-hannover.de
Frank Hutter1;2fh@cs.uni-freiburg.de
1Khoa Khoa học Máy tính, Đại học Albert-Ludwigs Freiburg
2Trung tâm Trí tuệ Nhân tạo Bosch, Renningen, Đức
3Viện Xử lý Thông tin, Đại học Leibniz Hannover
Biên tập viên: Marc Schoenauer
Tóm tắt
Automated Machine Learning (AutoML) hỗ trợ các nhà thực hành và nghiên cứu với nhiệm vụ tẻ nhạt thiết kế các pipeline machine learning và gần đây đã đạt được thành công đáng kể. Trong bài báo này, chúng tôi giới thiệu các phương pháp AutoML mới được thúc đẩy bởi bài nộp thắng cuộc của chúng tôi cho thử thách ChaLearn AutoML thứ hai. Chúng tôi phát triển PoSH Auto-sklearn, cho phép các hệ thống AutoML hoạt động tốt trên các tập dữ liệu lớn dưới giới hạn thời gian nghiêm ngặt bằng cách sử dụng một kỹ thuật meta-learning mới, đơn giản và không cần meta-feature cùng với việc sử dụng chiến lược bandit thành công cho phân bổ ngân sách. Tuy nhiên, PoSH Auto-sklearn giới thiệu thêm nhiều cách chạy AutoML và có thể khiến người dùng khó thiết lập đúng cách hơn. Do đó, chúng tôi cũng tiến thêm một bước và nghiên cứu không gian thiết kế của chính AutoML, đề xuất một giải pháp hướng tới AutoML thực sự tự động. Cùng nhau, những thay đổi này tạo ra thế hệ tiếp theo của hệ thống AutoML của chúng tôi, Auto-sklearn 2.0. Chúng tôi xác minh các cải tiến bằng những bổ sung này trong một nghiên cứu thực nghiệm rộng rãi trên 39 tập dữ liệu benchmark AutoML. Chúng tôi kết thúc bài báo bằng cách so sánh với các framework AutoML phổ biến khác và Auto-sklearn 1.0, giảm lỗi tương đối lên đến hệ số 4:5, và đạt được hiệu suất trong 10 phút tốt hơn đáng kể so với những gì Auto-sklearn 1.0 đạt được trong một giờ.
Từ khóa: Automated machine learning, tối ưu hóa siêu tham số, meta-learning, AutoML tự động, benchmark

1. Giới thiệu
Tiến bộ đáng kể gần đây trong machine learning (ML) đã dẫn đến nhu cầu ngày càng tăng về các hệ thống ML tự động có thể hỗ trợ các nhà phát triển và người mới học ML trong việc tạo ra các ứng dụng ML mới một cách hiệu quả. Vì các tập dữ liệu khác nhau yêu cầu các pipeline ML khác nhau, nhu cầu này đã tạo ra lĩnh vực automated machine learning (AutoML; Hutter et al., 2019). Các hệ thống AutoML phổ biến, như Auto-WEKA (Thornton et al., 2013), hyperopt-sklearn (Komer et al., 2014), Auto-sklearn (Feurer et al., 2015a), TPOT (Olson et al., 2016a) và Auto-Keras (Jin et al., 2019) thực hiện tối ưu hóa kết hợp trên các bộ tiền xử lý, bộ phân loại hoặc bộ hồi quy khác nhau và các thiết lập siêu tham số của chúng, từ đó giảm đáng kể công sức cho người dùng.

©2022 Matthias Feurer và Katharina Eggensperger và Stefan Falkner và Marius Lindauer và Frank Hutter.
Giấy phép: CC-BY 4.0, xem https://creativecommons.org/licenses/by/4.0/. Yêu cầu ghi nhận nguồn được cung cấp tại http://jmlr.org/papers/v23/21-0992.html. arXiv:2007.04074v3 [cs.LG] 4 Oct 2022

--- TRANG 2 ---
Feurer, Eggensperger, Falkner, Lindauer và Hutter

Để đánh giá tình trạng hiện tại của AutoML và quan trọng hơn, để thúc đẩy tiến bộ trong AutoML, ChaLearn đã tiến hành một loạt các thử thách AutoML (Guyon et al., 2019), đánh giá các hệ thống AutoML một cách có hệ thống dưới các ràng buộc nghiêm ngặt về thời gian và bộ nhớ. Cụ thể, trong các thử thách này, các hệ thống AutoML được yêu cầu đưa ra dự đoán trong vòng chưa đầy 20 phút. Một mặt, điều này sẽ cho phép tích hợp AutoML một cách hiệu quả vào quy trình làm việc dựa trên nguyên mẫu nhanh của nhiều nhà khoa học dữ liệu và, mặt khác, giúp dân chủ hóa ML bằng cách yêu cầu ít tài nguyên tính toán hơn.

Chúng tôi đã thắng cả thử thách AutoML thứ nhất và thứ hai với các phiên bản được chỉnh sửa của Auto-sklearn. Trong công trình này, chúng tôi mô tả chi tiết cách chúng tôi cải thiện Auto-sklearn từ phiên bản đầu tiên (Feurer et al., 2015a) để xây dựng PoSH Auto-sklearn, đã thắng cuộc thi thứ hai và sau đó mô tả cách chúng tôi cải thiện PoSH Auto-sklearn hơn nữa để tạo ra phương pháp hiện tại của chúng tôi cho Auto-sklearn 2.0.

Đặc biệt, trong khi AutoML giải thoát người dùng khỏi việc đưa ra các quyết định thiết kế cấp thấp (ví dụ: sử dụng mô hình nào), chính AutoML mở ra vô số quyết định thiết kế cấp cao, ví dụ: sử dụng chiến lược lựa chọn mô hình nào (Guyon et al., 2010, 2015; Raschka, 2018) hoặc cách phân bổ ngân sách thời gian đã cho (Jamieson và Talwalkar, 2016). Trong khi các bài nộp của chúng tôi cho các thử thách AutoML chủ yếu được thiết kế thủ công, trong công trình này, chúng tôi tiến thêm một bước bằng cách tự động hóa chính AutoML để khai thác đầy đủ tiềm năng của AutoML trong thực tế.1

Sau khi mô tả chi tiết vấn đề AutoML mà chúng tôi xem xét trong Phần 2, chúng tôi trình bày hai phần chính với các đóng góp sau:

Phần I: Portfolio Successive Halving trong PoSH Auto-sklearn. Trong phần này (xem Phần 3), chúng tôi giới thiệu các chiến lược phân bổ ngân sách như một lựa chọn thiết kế bổ sung cho các chiến lược lựa chọn mô hình (holdout (HO) và cross-validation (CV)) cho các hệ thống AutoML. Chúng tôi đề xuất sử dụng chiến lược phân bổ ngân sách successive halving (SH) như một thay thế cho việc luôn sử dụng ngân sách đầy đủ (FB) để đánh giá một cấu hình nhằm phân bổ nhiều tài nguyên hơn cho các pipeline ML hứa hẹn. Hơn nữa, chúng tôi giới thiệu cả phương pháp thực tế cũng như lý thuyết đằng sau việc xây dựng các portfolio tốt hơn cho thành phần meta-learning của Auto-sklearn. Chúng tôi cho thấy rằng sự kết hợp này cải thiện hiệu suất đáng kể, mang lại kết quả mạnh mẽ hơn trong 10 phút so với Auto-sklearn 1.0 đạt được trong 60 phút.

Phần II: Tự động hóa AutoML trong Auto-sklearn 2.0. Trong phần này (xem Phần 4), chúng tôi đề xuất một kỹ thuật meta-learning dựa trên lựa chọn thuật toán để tự động chọn thiết lập tốt nhất của chính hệ thống AutoML cho một tập dữ liệu đã cho. Chúng tôi gọi hệ thống kết quả là Auto-sklearn 2.0 và mô tả sự tiến hóa từ Auto-sklearn 1.0 qua PoSH Auto-sklearn đến Auto-sklearn 2.0 trong Hình 1.

Trong Phần 5, chúng tôi sử dụng thêm AutoML benchmark (Gijsbers et al., 2019) để đánh giá Auto-sklearn 2.0 so với các hệ thống AutoML phổ biến khác và cho thấy hiệu suất được cải thiện dưới các ràng buộc thời gian nghiêm ngặt. Phần 6 sau đó đặt công trình của chúng tôi vào bối cảnh của các công trình liên quan, và Phần 7 kết thúc bài báo với các câu hỏi mở, hạn chế và công việc tương lai.

1. Công trình được trình bày trong bài báo này một phần dựa trên hai bài báo workshop trước đó giới thiệu một số ý tưởng được trình bày dưới dạng sơ bộ (Feurer et al., 2018; Feurer và Hutter, 2018).

--- TRANG 3 ---
Auto-sklearn 2.0

Auto-sklearn 2.0{Xtrain,Ytrain,
Xtest,b,L}Bộ chọn chọn
π∈{HO,CV}×
{SH,FB}Portfolio cho πHệ thống AutoML sử dụng π
thiết kế
ban đầuchạy
BOˆYtestPoSH Auto-sklearn{Xtrain,Ytrain,
Xtest,b,L}π= (HO,SH ) Portfolio cho πHệ thống AutoML sử dụng π
thiết kế
ban đầuchạy
BOˆYtestAuto-sklearn 1.0{Xtrain,Ytrain,
Xtest,b,L}Người dùng chọn∗
π∈{HO,CV}KNDHệ thống AutoML sử dụng π
thiết kế
ban đầuchạy
BOˆYtest
∗mặc định chọn HO

Hình 1: Tổng quan sơ đồ của Auto-sklearn 1.0, PoSH Auto-sklearn, và Auto-sklearn 2.0. Các hộp chữ nhật màu cam đề cập đến dữ liệu đầu vào và đầu ra, trong khi các hộp tròn màu tím biểu thị các phần của hệ thống AutoML (được bao quanh bởi đường nét đứt màu xanh lá cây). Hộp tròn màu hồng đề cập đến con người trong vòng lặp cần thiết cho các quyết định thiết kế thủ công. Các hệ thống AutoML mới hơn đơn giản hóa việc sử dụng Auto-sklearn và giảm đầu vào người dùng yêu cầu. Chúng tôi mô tả PoSH Auto-sklearn trong Phần 3 và đưa ra tổng quan sơ đồ trong Hình 2. Tương tự, chúng tôi mô tả Auto-sklearn 2.0 trong Phần 4 và cung cấp tổng quan sơ đồ trong Hình 5.

2. Phát biểu Vấn đề

AutoML là một thuật ngữ được sử dụng rộng rãi, vì vậy, ở đây chúng tôi đầu tiên định nghĩa vấn đề mà chúng tôi xem xét trong công trình này. Gọi P(D) là một phân phối của các tập dữ liệu mà từ đó chúng ta có thể lấy mẫu phân phối của một tập dữ liệu cá nhân Pd=Pd(x;y). Vấn đề AutoML mà chúng tôi xem xét là tạo ra một pipeline đã được huấn luyện M:x7!y, được tham số hóa bởi 2 tự động tạo ra dự đoán cho các mẫu từ phân phối Pd tối thiểu hóa lỗi tổng quát kỳ vọng:2

GE(M) =E(x;y)Pd[L(M(x);y)]: (1)

Vì một tập dữ liệu chỉ có thể được quan sát thông qua một tập hợp n quan sát độc lập Dd= f(x1;y1);:::; (xn;yn)gPd, chúng ta chỉ có thể ước lượng thực nghiệm lỗi tổng quát trên dữ liệu mẫu:

GEV(M;Dd) =1/n∑ni=1L(M(xi);yi): (2)

Trong thực tế, chúng ta có quyền truy cập vào hai mẫu hữu hạn rời rạc mà chúng tôi từ bây giờ ký hiệu là Dtrain và Dtest (Dd;train và Dd;test trong trường hợp chúng tôi tham chiếu đến một tập dữ liệu cụ thể Pd). Để tìm kiếm pipeline ML tốt nhất, chúng ta chỉ có quyền truy cập vào Dtrain, tuy nhiên, cuối cùng hiệu suất được ước lượng một lần trên Dtest. Các hệ thống AutoML sử dụng điều này để tự động tìm kiếm M tốt nhất:

M∗∈argmin λ∈Λ GEV(Mλ;Dtrain); (3)

2. Ký hiệu của chúng tôi tuân theo Vapnik (1991).

--- TRANG 4 ---
Feurer, Eggensperger, Falkner, Lindauer và Hutter

và ước lượng GE, ví dụ, bằng K-fold cross-validation:

GEVCV(Mλ;Dtrain) =1/K∑Kk=1GEV(MD(train,k)trainλ ;D(val,k)train); (4)

trong đó MD(train,k)trainλ biểu thị rằng Mλ được huấn luyện trên phần huấn luyện của fold thứ k D(train,k)train⊆Dtrain, và sau đó được đánh giá trên phần validation của fold thứ k D(val,k)train=Dtrain\D(train,k)train.3 Giả định rằng, thông qua λ, một hệ thống AutoML có thể chọn cả thuật toán và các thiết lập siêu tham số của nó, định nghĩa này sử dụng GEVCV tương đương với định nghĩa của vấn đề CASH (Combined Algorithm Selection and Hyperparameter optimization) (Thornton et al., 2013; Feurer et al., 2015a). Tuy nhiên, không có khả năng rằng, bất kể thuật toán tối ưu hóa nào chúng ta sử dụng, hệ thống AutoML sẽ tìm thấy vị trí tối ưu chính xác λ∗. Thay vào đó, hệ thống AutoML sẽ trả về pipeline ML tốt nhất mà nó đã huấn luyện trong quá trình tìm kiếm, mà chúng tôi ký hiệu là M̂λ̂, và các thiết lập siêu tham số mà nó được huấn luyện với λ̂.

2.1 AutoML bị ràng buộc thời gian

Trong thực tế, người dùng không chỉ quan tâm đến việc có được một pipeline tối ưu Mλ∗ cuối cùng, mà còn có các ràng buộc về lượng thời gian và tài nguyên tính toán họ sẵn sàng đầu tư. Chúng tôi ký hiệu thời gian cần thiết để đánh giá GEV(Mλ;Dtrain) là tλ và ngân sách tối ưu hóa tổng thể là T. Mục tiêu của chúng tôi là tìm

Mλ∗∈argmin λ∈Λ GEV(Mλ;Dtrain) s.t. ∑i tiλ<T (5)

trong đó tổng được tính trên tất cả các pipeline đã đánh giá λi, rõ ràng tôn trọng ngân sách tối ưu hóa T. Như trước, hệ thống AutoML sẽ trả về mô hình tốt nhất mà nó đã tìm thấy trong ngân sách tối ưu hóa, M̂λ̂.

2.2 Tổng quát hóa của AutoML

Cuối cùng, một hệ thống AutoML A:D7!MD̂λ̂ không chỉ nên hoạt động tốt trên một tập dữ liệu duy nhất mà trên toàn bộ phân phối trên các tập dữ liệu P(D). Do đó, meta-problem của AutoML có thể được chính thức hóa như tối thiểu hóa lỗi tổng quát trên phân phối các tập dữ liệu này:

GE(A) =EDd∼P(D)[GEV(A(Dd);Dd)]; (6)

mà lần lượt chỉ có thể được ước lượng bằng một tập hợp hữu hạn các tập dữ liệu meta-train Dmeta (mỗi tập có một tập hợp hữu hạn các quan sát):

GEV(A;Dmeta) =1/|Dmeta|∑|Dmeta|d=1GEV(A(Dd);Dd): (7)

3. Hoặc, người ta có thể sử dụng holdout để ước lượng GE với GEVHO(Mλ;Dtrain) =GEV(MDtraintrainλ ;Dvaltrain).

--- TRANG 5 ---
Auto-sklearn 2.0

Sau khi thiết lập phát biểu vấn đề, chúng ta có thể sử dụng điều này để chính thức hóa thêm các mục tiêu của chúng tôi. Thay vì sử dụng một hệ thống AutoML cố định duy nhất A, chúng tôi sẽ giới thiệu các chính sách tối ưu hóa π, một sự kết hợp của các siêu tham số của hệ thống AutoML và các thành phần cụ thể được sử dụng trong một lần chạy, có thể được sử dụng để cấu hình một hệ thống AutoML cho các trường hợp sử dụng cụ thể. Sau đó chúng tôi ký hiệu một hệ thống AutoML được cấu hình như vậy là Aπ.

Đầu tiên chúng tôi sẽ xây dựng π thủ công trong Phần 3, giới thiệu một hệ thống mới để thiết kế π từ dữ liệu trong Phần 4 và sau đó mở rộng điều này thành một ánh xạ (đã học) Ξ : D→π tự động đề xuất một chính sách tối ưu hóa cho một tập dữ liệu mới bằng cách sử dụng lựa chọn thuật toán. Thiết lập vấn đề này cũng có thể được sử dụng để giới thiệu các tổng quát hóa của vấn đề lựa chọn thuật toán như cấu hình thuật toán (Birattari et al., 2002; Hutter et al., 2009; Kleinberg et al., 2017), cấu hình thuật toán theo từng instance (Xu et al., 2010; Malitsky et al., 2012) và cấu hình thuật toán động (Biedenkapp et al., 2020) trên meta-level; nhưng chúng tôi để dành những điều này cho công việc tương lai. Ngoài ra, thay vì lựa chọn giữa nhiều chính sách của một hệ thống AutoML duy nhất, phương pháp được trình bày có thể được áp dụng để chọn giữa các hệ thống AutoML khác nhau mà không cần điều chỉnh. Tuy nhiên, thay vì tối đa hóa hiệu suất bằng cách gọi nhiều hệ thống AutoML, từ đó tăng độ phức tạp, mục tiêu của chúng tôi là cải thiện các hệ thống AutoML đơn lẻ để làm cho chúng dễ sử dụng hơn bằng cách giảm độ phức tạp cho người dùng.

3. Phần I: Portfolio Successive Halving trong PoSH Auto-sklearn

Trong phần này chúng tôi giới thiệu giải pháp thắng cuộc của chúng tôi cho cuộc thi AutoML thứ hai (Guyon et al., 2019), PoSH Auto-sklearn, viết tắt của Portfolio Successive Halving. Đầu tiên chúng tôi mô tả việc sử dụng portfolio để khởi động nóng một hệ thống AutoML và sau đó thúc đẩy việc sử dụng chiến lược bandit successive halving. Tiếp theo, chúng tôi mô tả các cân nhắc thực tế để xây dựng PoSH Auto-sklearn, đưa ra tổng quan sơ đồ và tóm tắt các kỹ thuật được chế tạo thủ công bổ sung mà chúng tôi đã sử dụng trong cuộc thi. Chúng tôi kết thúc phần đầu tiên của các đóng góp chính với một đánh giá thực nghiệm chứng minh hiệu suất của PoSH Auto-sklearn.

3.1 Xây dựng Portfolio

Tìm giải pháp tối ưu cho vấn đề tối ưu hóa bị ràng buộc thời gian từ Phương trình (5) yêu cầu tìm kiếm một không gian lớn các pipeline ML có thể một cách hiệu quả nhất có thể. BO là một phương pháp mạnh mẽ cho việc này, nhưng phiên bản vanilla của nó bắt đầu từ đầu cho mọi vấn đề mới. Một giải pháp tốt hơn là khởi động nóng BO với các pipeline ML được kỳ vọng sẽ hoạt động tốt, như được thực hiện trong phương pháp k-nearest dataset (KND) của Auto-sklearn 1.0 (Reif et al., 2012; Feurer et al., 2015b,a; xem thêm công trình liên quan trong Phần 6.4.1). Tuy nhiên, chúng tôi thấy rằng giải pháp này tạo ra các vấn đề mới:

1. Nó tốn thời gian vì nó yêu cầu tính toán các meta-feature mô tả đặc điểm của các tập dữ liệu.

2. Nó thêm độ phức tạp vào hệ thống vì việc tính toán các meta-feature cũng phải được thực hiện với giới hạn thời gian và bộ nhớ.

3. Nhiều meta-feature không được định nghĩa đối với các feature phân loại và giá trị thiếu, khiến chúng khó áp dụng cho hầu hết các tập dữ liệu.

4. Không rõ ràng ngay lập tức meta-feature nào hoạt động tốt nhất cho vấn đề nào.

--- TRANG 6 ---
Feurer, Eggensperger, Falkner, Lindauer và Hutter

5. Trong phương pháp KND, không có cơ chế để đảm bảo rằng chúng ta không thực thi các pipeline ML dư thừa.

Chúng tôi thực sự gặp phải những vấn đề này trong thử thách AutoML đầu tiên, thất bại ở một track do chạy quá thời gian cho việc tạo meta-feature, mặc dù chúng tôi đã loại bỏ các meta-feature landmarking do thời gian chạy có thể cao của chúng. Do đó, ở đây chúng tôi đề xuất một phương pháp không cần meta-feature không khởi động nóng với một tập hợp các cấu hình cụ thể cho một tập dữ liệu mới, mà sử dụng một portfolio tĩnh - một tập hợp các cấu hình bổ sung che phủ càng nhiều tập dữ liệu đa dạng càng tốt và tối thiểu hóa rủi ro thất bại khi đối mặt với một nhiệm vụ mới.

Vì vậy, thay vì đánh giá các cấu hình được chọn trực tuyến bởi phương pháp KND, chúng tôi xây dựng một portfolio, bao gồm các pipeline ML hiệu suất cao và bổ sung để hoạt động tốt trên càng nhiều tập dữ liệu càng tốt, ngoại tuyến. Sau đó, đối với một tập dữ liệu có sẵn, tất cả các pipeline trong portfolio này được đánh giá đơn giản lần lượt. Nếu còn thời gian sau đó, chúng tôi tiếp tục với các pipeline được đề xuất bởi BO được khởi động nóng với các pipeline portfolio đã đánh giá. Chúng tôi giới thiệu khởi động dựa trên portfolio để tránh tính toán meta-feature cho một tập dữ liệu mới. Tuy nhiên, các portfolio cũng hoạt động khác nhau về bản chất. Trong khi phương pháp KND nhằm mục đích chỉ sử dụng các cấu hình hoạt động tốt, một portfolio được xây dựng sao cho có một tập hợp đa dạng các cấu hình, bắt đầu với những cấu hình hoạt động tốt trung bình và sau đó chuyển sang những cấu hình chuyên biệt hơn. Do đó, nó có thể được coi như một thiết kế ban đầu được tối ưu hóa cho phương pháp BO.

Trong phần sau, chúng tôi mô tả quy trình ngoại tuyến của chúng tôi để xây dựng một portfolio như vậy và đưa ra nền tảng lý thuyết bằng một bound hiệu suất.

3.1.1 Phương pháp

Đầu tiên chúng tôi mô tả cách chúng tôi xây dựng một portfolio với một tập hợp hữu hạn các pipeline ứng viên C=fλ1;:::;λlg. Ngoài ra, chúng tôi giả định rằng tồn tại một tập hợp các tập dữ liệu Dmeta = fD1;:::;D|Dmeta|g và chúng tôi muốn xây dựng một portfolio P bao gồm một tập con của các pipeline trong C hoạt động tốt trên Dmeta.

Chúng tôi phác thảo quy trình xây dựng một portfolio như vậy trong Thuật toán 1. Đầu tiên, chúng tôi khởi tạo portfolio P của chúng tôi thành tập hợp rỗng (Dòng 2). Sau đó, chúng tôi lặp lại quy trình sau cho đến khi |P| đạt đến một giới hạn được định trước: Từ một tập hợp các pipeline ML ứng viên C, chúng tôi tham lam thêm một ứng viên λ+∈C vào P làm giảm lỗi tổng quát ước lượng trên tất cả các meta-dataset nhiều nhất (Dòng 4), và sau đó loại bỏ λ+ khỏi C (Dòng 5).

Lỗi tổng quát ước lượng của một portfolio P trên một tập dữ liệu đơn lẻ D là hiệu suất của pipeline tốt nhất λ∈P trên D theo chiến lược lựa chọn mô hình và phân bổ ngân sách. Điều này có thể được mô tả thông qua một hàm S(ρ;Λ;D), nhận đầu vào là một hàm để tính lỗi tổng quát ước lượng (ví dụ, như định nghĩa trong Phương trình 4), một tập hợp các pipeline machine learning để huấn luyện, và một tập dữ liệu. Sau đó nó trả về pipeline với lỗi tổng quát ước lượng thấp nhất như

MDλ∗=S(GEV;P;D)∈argmin MDλ∈P GEV(MDλ;D): (8)

--- TRANG 7 ---
Auto-sklearn 2.0

Thuật toán 1: Xây dựng Portfolio Tham lam
1:Đầu vào: Tập hợp các pipeline ML ứng viên C, Dmeta=fD1;:::;D|Dmeta|g, kích thước portfolio tối đa p, chiến lược lựa chọn mô hình S
2:P=;
3:while |P|<p do
4:λ+= argmin λ∈C GEVS(P[fλg;Dmeta)
// Hòa được giải quyết bằng cách ưu tiên mô hình được huấn luyện đầu tiên.
5:P=P[λ+;C=C\fλ+g
6:end while
7:return Portfolio P

Trong trường hợp kết quả của argmin không duy nhất, chúng tôi trả về mô hình đã được đánh giá đầu tiên. Lỗi tổng quát ước lượng của P trên tất cả meta-dataset Dmeta=fD1;:::;D|Dmeta|g sau đó là

GEVS(P;Dmeta) =∑|Dmeta|d=1 GEV(S(GEV;P;Dd);Dval d); (9)

Ở đây, chúng tôi đưa ra phương trình cho việc sử dụng holdout, và trong Phụ lục A chúng tôi cung cấp ký hiệu chính xác cho cross-validation và successive halving.

Bây giờ chúng tôi mô tả chi tiết cách xây dựng tập hợp các pipeline ứng viên C và mô tả cách tìm các pipeline ứng viên và xây dựng portfolio phù hợp trong bức tranh lớn hơn. Chúng tôi đưa ra tổng quan sơ đồ về quy trình này trong Hình 2. Nó bao gồm một giai đoạn huấn luyện (TR1{TR3) và một giai đoạn kiểm tra (TE1{TE2).

Sau khi thu thập các tập dữ liệu Dmeta (chúng tôi mô tả trong Phần 3.4.1 cách chúng tôi thực hiện điều này cho các thí nghiệm của mình), chúng tôi có được các pipeline ML ứng viên (TR1) bằng cách chạy Auto-sklearn không có meta-learning và không có ensembling trên mỗi tập dữ liệu. Chúng tôi giới hạn bản thân với một tập hữu hạn các ứng viên portfolio C, và chọn một ứng viên cho mỗi tập dữ liệu. Sau đó, chúng tôi xây dựng một ma trận hiệu suất có kích thước |C|×|Dmeta| bằng cách đánh giá mỗi pipeline ứng viên này trên mỗi tập dữ liệu (TR2, chúng tôi tham khảo Phần 3.4.2 để mô tả chi tiết về việc tạo meta-data). Cuối cùng, chúng tôi sử dụng ma trận này để xây dựng một portfolio sử dụng Thuật toán 1 cho sự kết hợp của chiến lược lựa chọn mô hình holdout và chiến lược phân bổ ngân sách SH trong bước huấn luyện TR3.

Đối với một tập dữ liệu mới Dnew∈Dtest, chúng tôi áp dụng hệ thống AutoML sử dụng SH, holdout và portfolio cho Dnew (TE1). Cuối cùng, chúng tôi trả về pipeline tốt nhất đã tìm thấy M̂λ̂, hoặc một ensemble của các pipeline đã đánh giá, dựa trên tập huấn luyện của Dnew (TE2.1). Tùy chọn, chúng tôi có thể tính toán loss của M̂λ̂ trên tập test của Dnew (TE2.2); chúng tôi nhấn mạnh rằng đây sẽ là lần duy nhất chúng tôi từng truy cập tập test của Dnew.

Để xây dựng một portfolio trên các tập dữ liệu, chúng ta cần tính đến rằng các lỗi tổng quát cho các tập dữ liệu khác nhau tồn tại ở các thang đo khác nhau (Bardenet et al., 2013). Do đó, trước khi lấy trung bình, đối với mỗi tập dữ liệu, chúng tôi biến đổi các lỗi tổng quát thành khoảng cách đến hiệu suất tốt nhất quan sát được được chia tỷ lệ giữa không và một, một thước đo được gọi là khoảng cách đến minimum; khi được tính trung bình trên tất cả các tập dữ liệu được gọi là khoảng cách trung bình đến minimum (ADTM) (Wistuba et al., 2015a, 2018). Chúng tôi tính toán thống kê cho việc chia tỷ lệ zero-one riêng lẻ cho mỗi sự kết hợp của lựa chọn mô hình và phân bổ ngân sách (tức là, chúng tôi sử dụng test loss thấp nhất quan sát được và test loss lớn nhất quan sát được cho mỗi meta-dataset).

--- TRANG 8 ---
Feurer, Eggensperger, Falkner, Lindauer và Hutter

Tùy chọnTập hợp đại diện
của các tập dữ liệu
{D1, . . . ,D|Dmeta |}TR1: Lấy
tập hợp các pipeline
ML ứng viên CTR2: Đánh giá ma trận
hiệu suất đầy đủ
có hình dạng |C|×|Dmeta|TR3: Xây dựng
portfolio PTEst1: Chạy
hệ thống AutoML
đánh giá
Pchạy
BOTE2.1: Trả về pipeline
tốt nhất đã tìm thấy M̂λ̂∗ hoặc
ensemble của pipelineTE2.2: Báo cáo
loss trên Dnew,testDnew,trainDnewDnew,testTRain (ngoại tuyến)
TEst (trực tuyến)

Hình 2: Tổng quan Sơ đồ của PoSH Auto-sklearn với giai đoạn xây dựng portfolio ngoại tuyến (TR1-TR3) ở trên và giai đoạn test (TE1-TE2) ở dưới đường nét đứt. Các hộp tròn, màu tím đề cập đến các bước tính toán trong khi các hộp chữ nhật, màu cam mô tả dữ liệu đầu vào cho hệ thống AutoML.

Đối với mỗi meta-dataset Dd∈Dmeta chúng ta có quyền truy cập vào cả Dd,train và Dd,test. Trong trường hợp holdout, chúng tôi chia tập huấn luyện Dd,train thành hai tập nhỏ hơn rời rạc Dtraind,train và Dvald,train. Chúng tôi thường huấn luyện các mô hình sử dụng Dtraind,train và sử dụng Dvald,train để chọn một pipeline ML Mλ từ portfolio bằng phương tiện của chiến lược lựa chọn mô hình S (thay vì holdout chúng ta tất nhiên cũng có thể sử dụng cross-validation để tính validation loss). Tuy nhiên, nếu thay vào đó chúng ta chọn pipeline ML trên tập test Dd,test, Phương trình 9 trở thành một hàm tập hợp đơn điệu và submodular, dẫn đến các đảm bảo thuận lợi cho thuật toán tham lam mà chúng tôi mô tả chi tiết trong Phần 3.1.2. Chúng tôi theo phương pháp này để xây dựng portfolio trong giai đoạn ngoại tuyến; chúng tôi nhấn mạnh rằng đối với một tập dữ liệu mới Dnew, chúng tôi tất nhiên không yêu cầu quyền truy cập vào tập test Dnew,test.

3.1.2 Tính chất Lý thuyết của Thuật toán Tham lam

Bên cạnh những lợi thế thực tế đã đề cập của thuật toán tham lam được đề xuất, thuật toán này cũng có một lỗi worst-case bị chặn.

Mệnh đề 1 Tối thiểu hóa test loss của một portfolio P trên một tập hợp các tập dữ liệu D1;:::;D|Dmeta|, khi chọn một pipeline ML từ P cho Dd sử dụng holdout hoặc cross-validation dựa trên hiệu suất của nó trên Dd,test, tương đương với vấn đề đặt sensor để tối thiểu hóa thời gian phát hiện (Krause et al., 2008).

Chúng tôi mô tả chi tiết sự tương đương này trong Phụ lục C.2. Như vậy, chúng ta có thể áp dụng các kết quả hiện có cho vấn đề đặt sensor vào vấn đề của chúng ta. Sử dụng tập test của các meta-dataset Dmeta để xây dựng một portfolio hoàn toàn ổn miễn là chúng ta không sử dụng các tập dữ liệu mới Dnew∈Dtest mà chúng ta sử dụng để kiểm tra phương pháp.

Hệ quả 1 Hàm penalty cho tất cả meta-dataset là submodular.

Chúng ta có thể áp dụng trực tiếp chứng minh từ Krause et al. (2008) rằng hàm penalty (tức là, lỗi tổng quát ước lượng tối đa trừ lỗi tổng quát ước lượng quan sát được) là submodular và đơn điệu cho thiết lập vấn đề của chúng ta. Vì các kết hợp tuyến tính của các hàm submodular cũng là submodular (Krause và Golovin, 2014), hàm penalty cũng là submodular.

Hệ quả 2 Vấn đề tìm một portfolio tối ưu P∗ là NP-hard (Nemhauser et al., 1978; Krause et al., 2008).

Hệ quả 3 Gọi R là giảm penalty kỳ vọng của một portfolio trên tất cả các tập dữ liệu, so với portfolio rỗng (cho kết quả điểm số tệ nhất có thể cho mỗi tập dữ liệu). Thuật toán tham lam trả về một portfolio P sao cho R(P)≥R(P∗)(1−1/e)R(P∗).

Điều này có nghĩa là thuật toán tham lam đóng ít nhất 63% khoảng cách giữa điểm số ADTM tệ nhất (1.0) và điểm số mà portfolio tốt nhất có thể P∗ có kích thước |P| sẽ đạt được (Nemhauser et al., 1978; Krause và Golovin, 2014). Một tổng quát hóa của kết quả này được đưa ra bởi Krause và Golovin (2014, Định lý 1.5) cũng thắt chặt bound này để đóng 99% khoảng cách giữa điểm số ADTM tệ nhất và điểm số mà portfolio tối ưu P∗ có kích thước |P| sẽ đạt được, bằng cách mở rộng portfolio được xây dựng bởi thuật toán tham lam đến kích thước 5|P|. Xin lưu ý rằng một portfolio có kích thước 5|P| có thể tốt hơn portfolio tối ưu có kích thước |P|.

Điều này có nghĩa là chúng ta có thể tìm một portfolio gần tối ưu trên các meta-train dataset Dmeta ít nhất. Dưới giả định rằng chúng ta áp dụng portfolio cho các tập dữ liệu từ cùng phân phối của các tập dữ liệu, chúng ta có một tập hợp mạnh mẽ các pipeline ML mặc định.

Chúng ta cũng có thể áp dụng các chiến lược khác cho việc đặt sensor set trong thiết lập của chúng ta, chẳng hạn như các chiến lược mixed integer programming, có thể giải quyết nó một cách tối ưu; tuy nhiên, những chiến lược này không mở rộng được đến kích thước portfolio của hàng chục pipeline ML (Krause et al., 2008; Pösterer et al., 2018).

Cùng mệnh đề (với cùng chứng minh) và các hệ quả áp dụng nếu chúng ta chọn một pipeline ML dựa trên một bước trung gian trong đường cong học tập hoặc sử dụng cross-validation thay vì holdout. Chúng tôi thảo luận về việc sử dụng tập validation và các chiến lược lựa chọn mô hình và phân bổ ngân sách khác trong Phụ lục C.3 và Phụ lục C.4.

3.2 Phân bổ Ngân sách sử dụng Successive Halving

Một vấn đề chính mà chúng tôi xác định trong thử thách AutoML cuối cùng là việc huấn luyện các cấu hình đắt tiền trên toàn bộ tập huấn luyện, kết hợp với ngân sách thời gian thấp, không mở rộng tốt cho các tập dữ liệu lớn. Đồng thời, chúng tôi nhận thấy rằng chiến lược (khi đó thủ công) của chúng tôi để chạy các pipeline được định trước trên các tập con của dữ liệu đã tạo ra các dự đoán đủ tốt để xây dựng ensemble. Điều này đặt câu hỏi về lựa chọn phổ biến là gán cùng một lượng tài nguyên cho tất cả các đánh giá pipeline, tức là thời gian, tính toán và dữ liệu.

Vì lý do này, chúng tôi giới thiệu nguyên tắc của các chiến lược phân bổ ngân sách cho AutoML, mô tả cách phân bổ tài nguyên cho các đánh giá pipeline. Đây là một quyết định thiết kế trực giao với chiến lược lựa chọn mô hình, xấp xỉ lỗi tổng quát của một pipeline ML đơn lẻ, và thường được giải quyết bằng holdout hoặc K-fold cross-validation (xem Phần 6.4.1).

Như một thay thế có nguyên tắc cho việc luôn sử dụng ngân sách đầy đủ, chúng tôi sử dụng chiến lược bandit successive halving (SH; Karnin et al., 2013; Jamieson và Talwalkar, 2016), gán nhiều ngân sách hơn cho các pipeline machine learning hứa hẹn và có thể dễ dàng kết hợp với các thuật toán lặp.

--- TRANG 9 ---
Auto-sklearn 2.0

3.2.1 Phương pháp

Các hệ thống AutoML đánh giá mỗi pipeline dưới cùng các hạn chế tài nguyên và trên cùng ngân sách (ví dụ, số lần lặp sử dụng các thuật toán lặp). Để tăng hiệu quả cho các trường hợp có hạn chế tài nguyên chặt chẽ, chúng tôi đề xuất phân bổ nhiều tài nguyên hơn cho các pipeline hứa hẹn bằng cách sử dụng SH (Karnin et al., 2013; Jamieson và Talwalkar, 2016) để loại bỏ các pipeline hoạt động kém một cách tích cực.

Cho một ngân sách tối thiểu và tối đa cho mỗi pipeline ML, SH bắt đầu bằng cách huấn luyện một số cố định các pipeline ML cho ngân sách nhỏ nhất. Sau đó, nó lặp lại chọn 1/η của các pipeline với lỗi tổng quát thấp nhất, nhân ngân sách của chúng với η, và đánh giá lại. Quá trình này tiếp tục cho đến khi chỉ còn lại một pipeline ML duy nhất hoặc ngân sách tối đa được chi tiêu, và thay thế quy trình holdout tiêu chuẩn trong đó mỗi pipeline ML được huấn luyện cho ngân sách đầy đủ.

Trong khi chính SH chọn các pipeline Mλ mới để đánh giá ngẫu nhiên, chúng tôi nhằm mục đích mở rộng công trình của chúng tôi về Auto-sklearn 1.0 và tiếp tục sử dụng BO. Để làm như vậy, chúng tôi theo công trình kết hợp SH với BO (Falkner et al., 2018).4 Cụ thể, chúng tôi sử dụng BO để lặp lại đề xuất các pipeline ML Mλ mới, mà chúng tôi đánh giá trên ngân sách thấp nhất cho đến khi một số cố định các pipeline đã được đánh giá. Sau đó, chúng tôi chạy SH như đã mô tả ở trên. Chúng tôi đang sử dụng phương pháp BO dựa trên random forest tiêu chuẩn của Auto-sklearn là SMAC và, theo phương pháp của Falkner et al. (2018) xây dựng mô hình cho BO trên ngân sách cao nhất có sẵn mà chúng tôi có đủ datapoint. Trong khi mô hình gốc có yêu cầu toán học cho n+1 pipeline hoàn thành, trong đó n là số siêu tham số cần tối ưu hóa, mô hình random forest có thể hướng dẫn tối ưu hóa với ít datapoint hơn, và chúng tôi định nghĩa đủ là n/2. Các portfolio mà chúng tôi đã giới thiệu trong Phần 3.1 tích hợp một cách liền mạch vào lược đồ này: miễn là không phải tất cả các thành viên của portfolio đã được đánh giá, chúng tôi đề xuất chúng thay vì yêu cầu BO đưa ra đề xuất mới.

SH có thể cung cấp tăng tốc lớn, nhưng nó cũng có thể quá tích cực cắt bỏ các cấu hình tốt cần ngân sách cao hơn để hoạt động tốt nhất. Do đó, chúng tôi mong đợi SH hoạt động tốt nhất cho các tập dữ liệu lớn, mà không có đủ thời gian để huấn luyện nhiều pipeline ML cho ngân sách đầy đủ (FB), nhưng việc huấn luyện một pipeline ML trên ngân sách nhỏ đã mang lại một chỉ báo tốt về lỗi tổng quát.

Chúng tôi lưu ý rằng SH có thể được sử dụng kết hợp với cả holdout hoặc cross-validation, và do đó thực sự thêm một siêu-siêu tham số khác vào hệ thống AutoML, đó là có sử dụng SH hay FB. Tuy nhiên, nó cũng thêm tính linh hoạt hơn để giải quyết một phạm vi vấn đề rộng hơn.

3.3 Cân nhắc Thực tế và Kết quả Thử thách

Để tận dụng tốt nhất thuật toán successive halving, chúng tôi đã phải thực hiện một số điều chỉnh để đạt được hiệu suất cao.

Đầu tiên, chúng tôi hạn chế không gian tìm kiếm chỉ chứa các thuật toán lặp và không còn tiền xử lý feature. Điều này đơn giản hóa việc sử dụng SH vì chúng tôi chỉ phải đối phó với một loại fidelity duy nhất, số lần lặp, trong khi nếu không chúng tôi cũng sẽ phải xem xét các tập con dataset như một thay thế. Điều này để lại cho chúng tôi extremely randomized trees (Geurts

4. Falkner et al. (2018) đề xuất sử dụng Hyperband (Li et al., 2018) cùng với BO; tuy nhiên, chúng tôi chỉ sử dụng SH vì chúng tôi mong đợi nó hoạt động tốt hơn trong trường hợp cực đoan có rất ít thời gian, vì nó giảm ngân sách cho mỗi pipeline ML một cách tích cực hơn.

--- TRANG 10 ---
Auto-sklearn 2.0

et al., 2006), random forests (Breimann, 2001), histogram-based gradient boosting (Friedman, 2001; Ke et al., 2017), một mô hình tuyến tính được khớp với thuật toán passive aggressive (Crammer et al., 2006) hoặc stochastic gradient descent và một multi-layer perceptron. Không gian cấu hình chính xác có thể được tìm thấy trong Bảng 18 của phụ lục.

Thứ hai, vì chỉ sử dụng các thuật toán lặp, chúng tôi có thể lưu trữ các mô hình được khớp một phần vào đĩa để ngăn không có dự đoán trong trường hợp time- và memouts. Tức là, sau 2;4;8;::: lần lặp, chúng tôi đưa ra dự đoán cho tập validation và dump mô hình để sử dụng sau này. Chúng tôi cung cấp thêm chi tiết, chẳng hạn như không gian tìm kiếm bị hạn chế, trong Phụ lục B.

Đối với bài nộp của chúng tôi cho thử thách AutoML thứ hai, chúng tôi đã triển khai các biện pháp bảo vệ và thủ thuật sau (Feurer et al., 2018), mà chúng tôi không sử dụng trong bài báo này vì thay vào đó chúng tôi tập trung vào việc tự động thiết kế một hệ thống AutoML mạnh mẽ:

• Đối với bài nộp, chúng tôi cũng sử dụng support vector machines sử dụng các tập con của dataset như các fidelity thấp hơn. Vì không có máy nào trong số năm ensemble cuối cùng trong cuộc thi chứa support vector machines, chúng tôi không xem xét chúng nữa cho bài báo này, đơn giản hóa phương pháp của chúng tôi.

• Chúng tôi đã phát triển một phương pháp pruning thư viện bổ sung cho lựa chọn ensemble. Tuy nhiên, trong các thí nghiệm sơ bộ, chúng tôi thấy rằng điều này, trong trường hợp tốt nhất, cung cấp một sự tăng cường không đáng kể cho area under curve và không phải balanced accuracy, mà chúng tôi sử dụng trong công trình này và do đó không theo đuổi điều đó thêm nữa.

• Để tăng tính mạnh mẽ chống lại các tập dữ liệu lớn tùy ý, chúng tôi giảm tất cả các tập dữ liệu để có tối đa 500 feature sử dụng lựa chọn feature đơn biến. Tương tự, chúng tôi cũng giảm tất cả các tập dữ liệu để có tối đa 45 000 datapoint sử dụng subsampling phân tầng. Chúng tôi không nghĩ đây là những chiến lược tốt nói chung và chỉ triển khai chúng vì chúng tôi không có thông tin về chiều của các tập dữ liệu được sử dụng trong thử thách, và để ngăn chạy hết thời gian và bộ nhớ. Nhìn lại, chỉ có một trong năm tập dữ liệu kích hoạt bước lựa chọn feature này. Bây giờ, thay vào đó chúng tôi có một chiến lược dự phòng được định nghĩa bởi dữ liệu, xem Phần 4.1.1.

• Trong trường hợp các tập dữ liệu có ít hơn 1000 datapoint, chúng tôi sẽ hoàn nguyên từ holdout sang cross-validation. Tuy nhiên, biện pháp dự phòng này không được kích hoạt do các tập dữ liệu lớn hơn trong cuộc thi.

• Chúng tôi đã thêm thủ công một linear regression được khớp với stochastic gradient descent với các siêu tham số của nó được tối ưu hóa cho thời gian chạy nhanh như mục đầu tiên trong portfolio để tối đa hóa cơ hội khớp một mô hình trong thời gian đã cho. Chúng tôi đã triển khai chiến lược này vì chúng tôi không biết giới hạn thời gian của cuộc thi. Tuy nhiên, đối với bài báo này và các ứng dụng tương lai của Auto-sklearn, chúng tôi mong đợi biết ngân sách tối ưu hóa mà chúng tôi đang tối ưu hóa portfolio cho, chúng tôi không còn yêu cầu biện pháp bảo vệ như vậy.

Bài nộp của chúng tôi, PoSH Auto-sklearn, là người thắng cuộc chung của thử thách AutoML thứ hai. Chúng tôi đưa ra kết quả của cuộc thi trong Bảng 1 và tham khảo Feurer et al. (2018) và Guyon et al. (2019) để biết thêm chi tiết, đặc biệt là thông tin về các đối thủ của chúng tôi.

--- TRANG 11 ---
Auto-sklearn 2.0

Tên Hạng Dataset #1 Dataset #2 Dataset #3 Dataset #4 Dataset #5
PoSH Auto-sklearn 2:8 0:5533(3) 0:2839(4) 0:3932(1) 0:2635(1) 0:6766(5)
narnars0 3:8 0:5418(5) 0:2894(2) 0:3665(2) 0:2005(9) 0:6922(1)
Malik 5:4 0:5085(7) 0:2297(7) 0:2670(6) 0:2413(5) 0:6853(2)
wlWangl 5:4 0:5655(2) 0:4851(1) 0:2829(5) 0:0886(16) 0:6840(3)
thanhdng 5:4 0:5131(6) 0:2256(8) 0:2605(7) 0:2603(2) 0:6777(4)

Bảng 1: Kết quả cho thử thách AutoML thứ hai (Guyon et al., 2019). Tên là tên đội, Hạng là thứ hạng cuối cùng của bài nộp, tiếp theo là kết quả cá nhân trên năm tập dữ liệu được sử dụng trong cuộc thi. Tất cả hiệu suất là normalized area under the ROC curve (Guyon et al., 2015) với thứ hạng theo từng tập dữ liệu trong ngoặc. Trong trường hợp thiếu hạng, ví dụ, hạng 1 cho dataset 1, hạng này được đạt bởi một thí sinh không đứng trong top 5.

3.4 Thiết lập Thí nghiệm

Cho đến nay, các hệ thống AutoML được thiết kế mà không có ngân sách tối ưu hóa nào hoặc với một ngân sách tối ưu hóa cố định duy nhất T trong đầu (xem Phương trình 5).5 Hệ thống của chúng tôi tính đến ngân sách tối ưu hóa khi xây dựng portfolio. Chúng tôi sẽ nghiên cứu hai ngân sách tối ưu hóa: một ngân sách tối ưu hóa ngắn, 10 phút và một ngân sách tối ưu hóa dài, 60 phút như trong bài báo Auto-sklearn gốc. Để có một thước đo duy nhất cho phân loại nhị phân, phân loại đa lớp và các tập dữ liệu không cân bằng, chúng tôi báo cáo balanced error rate (1−balanced accuracy), theo thử thách AutoML thứ 1 (Guyon et al., 2019). Vì các tập dữ liệu khác nhau có thể tồn tại ở các thang đo khác nhau, chúng tôi áp dụng biến đổi tuyến tính để có được các giá trị có thể so sánh. Cụ thể, chúng tôi có được lỗi tối thiểu và tối đa bằng cách thực thi Auto-sklearn với portfolio và sử dụng ensemble cho mỗi sự kết hợp của các chiến lược lựa chọn mô hình và phân bổ ngân sách trên mỗi tập dữ liệu. Sau đó, chúng tôi chia tỷ lệ lại bằng cách trừ lỗi tối thiểu và chia cho sự khác biệt giữa lỗi tối đa và tối thiểu (ADTM, như được giới thiệu trong Phần 3.1.1).6 Với biến đổi này, chúng tôi có được một lỗi chuẩn hóa có thể được hiểu như regret của phương pháp của chúng tôi.

Chúng tôi cũng giới hạn thời gian và bộ nhớ cho mỗi đánh giá pipeline ML. Đối với giới hạn thời gian, chúng tôi cho phép tối đa 1/10 của ngân sách tối ưu hóa, trong khi đối với bộ nhớ, chúng tôi cho phép pipeline 4GB trước khi cưỡng chế chấm dứt thực thi.

3.4.1 Tập dữ liệu

Chúng tôi yêu cầu hai tập hợp tập dữ liệu rời rạc cho thiết lập của mình: (i) Dmeta, trên đó chúng tôi xây dựng portfolio và bộ chọn chính sách dựa trên mô hình mà chúng tôi sẽ giới thiệu trong Phần 4, và (ii) Dtest, trên đó chúng tôi đánh giá phương pháp của mình. Phân phối của cả hai tập hợp lý tưởng nên bao quát một loạt các lĩnh vực vấn đề và đặc điểm tập dữ liệu. Đối với Dtest, chúng tôi dựa vào 39 tập dữ liệu được chọn cho

5. Hệ thống AutoML OBOE (Yang et al., 2019) là một ngoại lệ tiềm năng tính đến ngân sách tối ưu hóa, nhưng các thí nghiệm của Yang et al. (2019) chỉ được tiến hành cho một ngân sách tối ưu hóa duy nhất, không chứng minh rằng hệ thống thích ứng với nhiều ngân sách tối ưu hóa.
6. Chúng tôi muốn nhấn mạnh rằng điều này hơi khác so với Phần 3.1.1 nơi chúng tôi không có quyền truy cập vào hiệu suất ensemble và cũng chỉ chuẩn hóa theo chiến lược lựa chọn mô hình.

--- TRANG 12 ---
Auto-sklearn 2.0

Hình 3: Phân phối của meta và test dataset. Chúng tôi trực quan hóa mỗi tập dữ liệu đối với các meta-feature của nó và làm nổi bật các tập dữ liệu ngoài phân phối meta của chúng tôi bằng các dấu x đen.

AutoML benchmark được đề xuất bởi Gijsbers et al. (2019), bao gồm các tập dữ liệu để so sánh các bộ phân loại (Bischl et al., 2021) và các tập dữ liệu từ các thử thách AutoML (Guyon et al., 2019).

Chúng tôi thu thập các meta dataset Dmeta dựa trên OpenML (Vanschoren et al., 2014) sử dụng OpenML-Python API (Feurer et al., 2021). Để có được một tập hợp đại diện, chúng tôi xem xét tất cả các tập dữ liệu trên OpenML với hơn 500 và ít hơn 1 000 000 mẫu với ít nhất hai thuộc tính. Tiếp theo, chúng tôi loại bỏ tất cả các tập dữ liệu thưa thớt, chứa thuộc tính thời gian hoặc thuộc tính kiểu chuỗi vì Dtest không chứa bất kỳ tập dữ liệu nào như vậy. Sau đó, chúng tôi loại bỏ các tập dữ liệu tổng hợp và lấy mẫu phụ các cụm của các tập dữ liệu rất giống nhau. Cuối cùng, chúng tôi kiểm tra thủ công sự chồng chéo với Dtest và kết thúc với tổng cộng 208 tập dữ liệu huấn luyện và sử dụng chúng để huấn luyện phương pháp của chúng tôi.

Chúng tôi hiển thị phân phối của các tập dữ liệu trong Hình 3. Các điểm xanh lá cây đề cập đến Dmeta và các dấu x cam đề cập đến Dtest. Chúng ta có thể thấy rằng Dmeta bao phủ phân phối cơ bản của Dtest khá tốt, nhưng một số tập dữ liệu nằm ngoài phân phối Dmeta được chỉ ra bởi các đường xanh lá cây, được đánh dấu bằng dấu x đen. Chúng tôi đưa ra danh sách đầy đủ các tập dữ liệu cho Dmeta và Dtest trong Phụ lục E.

Đối với tất cả các tập dữ liệu, chúng tôi sử dụng một tập test holdout duy nhất là 33:33%, được định nghĩa bởi nhiệm vụ OpenML tương ứng. 66:66% còn lại là dữ liệu huấn luyện của các hệ thống AutoML của chúng tôi, tự xử lý các phân chia thêm cho lựa chọn mô hình dựa trên chiến lược lựa chọn mô hình được chọn.

--- TRANG 13 ---
Auto-sklearn 2.0

3.4.2 Tạo Meta-data

Đối với mỗi ngân sách tối ưu hóa, chúng tôi tạo bốn ma trận hiệu suất có kích thước |Dmeta|×|C|, xem Phần 3.1.1 để biết chi tiết về ma trận hiệu suất. Mỗi ma trận đề cập đến một cách đánh giá lỗi tổng quát của một mô hình: holdout, 3-fold CV, 5-fold CV hoặc 10-fold CV. Để có được mỗi ma trận, chúng tôi thực hiện như sau. Đối với mỗi tập dữ liệu D trong Dmeta, chúng tôi sử dụng lựa chọn thuật toán kết hợp và tối ưu hóa siêu tham số để tìm một pipeline ML tùy chỉnh. Trong thực tế, chúng tôi chạy Auto-sklearn không có meta-learning và không có xây dựng ensemble ba lần và chọn pipeline ML tốt nhất kết quả trên test split của D. Để đảm bảo rằng Auto-sklearn tìm thấy một cấu hình tốt, chúng tôi chạy nó trong mười lần ngân sách tối ưu hóa được đưa ra bởi người dùng (xem Phương trình 5). Sau đó, chúng tôi chạy tích chéo của tất cả các pipeline ML ứng viên và tập dữ liệu để có được ma trận hiệu suất. Chúng tôi cũng lưu trữ kết quả trung gian cho các thuật toán lặp để chúng tôi có thể xây dựng portfolio tùy chỉnh cho SH cũng vậy.

3.4.3 Chi tiết Thí nghiệm Khác

Chúng tôi luôn báo cáo kết quả trung bình trên 10 lần lặp lại để tính đến tính ngẫu nhiên và báo cáo trung bình và độ lệch chuẩn trên các lần lặp lại này. Để kiểm tra xem sự khác biệt hiệu suất có đáng kể hay không, khi có thể, chúng tôi chạy kiểm tra Wilcoxon signed-rank như một kiểm tra giả thuyết thống kê với α=0:05 (Demšar, 2006). Ngoài ra, chúng tôi vẽ thứ hạng trung bình như sau. Đối với mỗi tập dữ liệu, chúng tôi vẽ một lần chạy cho mỗi phương pháp (trong số 10 lần lặp lại) và xếp hạng các lần vẽ này theo hiệu suất, sử dụng thứ hạng trung bình trong trường hợp hòa. Sau đó chúng tôi tính trung bình trên tất cả 39 tập dữ liệu và lặp lại việc lấy mẫu này 500 lần và sau đó vẽ median và percentile thứ 10 và 90 của các mẫu này. Trong trường hợp chỉ có ba phương pháp để so sánh, chúng tôi có thể liệt kê tất cả 1000 kết hợp của các seed và làm như vậy. Chúng tôi sử dụng phương pháp chính xác cho Hình 6 và phương pháp lấy mẫu cho Hình 7 trong phụ lục.

Chúng tôi tiến hành tất cả các thí nghiệm sử dụng lựa chọn ensemble, và chúng tôi xây dựng ensemble có kích thước 50 với replacement. Chúng tôi đưa ra kết quả mà không có lựa chọn ensemble trong Phụ lục B.2.

Tất cả các thí nghiệm được tiến hành trên một cụm tính toán với các máy được trang bị 2 CPU Intel Xeon Gold 6242 với 2.8GHz (32 core) và 192 GB RAM, chạy Ubuntu 20.04.01. Chúng tôi cung cấp các script để tái tạo tất cả kết quả thí nghiệm của chúng tôi tại https://github.com/automl/ASKL2.0_experiments và cung cấp tích hợp sạch các phương pháp của chúng tôi vào repository Auto-sklearn chính thức.

3.5 Kết quả Thí nghiệm

Trong phần phụ này, bây giờ chúng tôi xác thực các cải tiến cho PoSH Auto-sklearn. Đầu tiên, chúng tôi sẽ so sánh việc sử dụng portfolio với phương pháp KND trước đó và không có khởi động nóng và thứ hai, chúng tôi sẽ so sánh PoSH Auto-sklearn với Auto-sklearn 1.0 trước đó.

3.5.1 Portfolio vs. KND

Ở đây, chúng tôi nghiên cứu hiệu suất của portfolio đã học và so sánh nó với chiến lược meta-learning mặc định của Auto-sklearn 1.0 sử dụng 25 cấu hình. Ngoài ra, chúng tôi cũng nghiên cứu cách BO thuần túy sẽ hoạt động. Chúng tôi đưa ra kết quả trong Bảng 2.

Đối với siêu tham số AutoML mới |P|, chúng tôi chọn 32 để cho phép hai lần lặp đầy đủ của SH với thiết lập siêu tham số của chúng tôi cho SH. Không có gì đáng ngạc nhiên, khởi động nóng, nói chung, cải thiện

--- TRANG 14 ---
Auto-sklearn 2.0

10 phút 60 phút
BO KND Port BO KND Port
FB; holdout 5:98 5:29 3:70 3:84 3:98 3:08
SH; holdout 5:15 4:82 4:11 3:77 3:55 3:19
FB; 3CV 8:52 7:76 6:90 6:42 6:31 4:96
SH; 3CV 7:82 7:67 6:16 6:08 5:91 5:17
FB; 5CV 9:48 9:45 7:93 6:64 6:47 5:05
SH; 5CV 9:48 8:85 7:05 6:19 5:83 5:40
FB; 10CV 16:10 15:11 12:42 10:82 10:44 9:68
SH; 10CV 16:14 15:10 12:61 10:54 10:33 9:23

Bảng 2: Tỷ lệ lỗi cân bằng chuẩn hóa trung bình. Chúng tôi báo cáo hiệu suất tổng hợp trên 10 lần lặp lại và 39 tập dữ liệu của hệ thống AutoML của chúng tôi chỉ sử dụng tối ưu hóa Bayesian (BO), hoặc BO được khởi động nóng với k-nearest-datasets (KND) hoặc một portfolio tham lam (Port). Trên mỗi dòng, chúng tôi in đậm giá trị trung bình tốt nhất (trên mỗi chiến lược lựa chọn mô hình và phân bổ ngân sách và ngân sách tối ưu hóa, và gạch chân kết quả không khác biệt về mặt thống kê theo Wilcoxon-signed-rank Test (α=0:05)).

10MIN 60MIN
μ std μ std
(1) PoSH-Auto-sklearn 4:11 0:09 3:19 0:12
(2) Auto-sklearn (1.0) 16:21 0:27 7:17 0:30

Bảng 3: Hiệu suất cuối cùng của PoSH Auto-sklearn và Auto-sklearn 1.0. Chúng tôi báo cáo tỷ lệ lỗi cân bằng chuẩn hóa trung bình trên 10 lần lặp lại trên 39 tập dữ liệu. Chúng tôi in đậm giá trị trung bình tốt nhất (trên mỗi ngân sách tối ưu hóa) và gạch chân kết quả không khác biệt về mặt thống kê theo kiểm tra wilcoxon signed-rank (α=0:05).

hiệu suất trên tất cả ngân sách tối ưu hóa và hầu hết các chiến lược lựa chọn mô hình, thường bằng một biên độ lớn. Các portfolio luôn cải thiện so với BO, trong khi KND làm như vậy trong tất cả trừ một trường hợp. Khi so sánh các portfolio với KND, chúng tôi thấy rằng kết quả thô luôn thuận lợi và rằng đối với một nửa số thiết lập, sự khác biệt cũng có ý nghĩa.

3.5.2 PoSH Auto-sklearn vs Auto-sklearn 1.0

Chúng tôi cũng có thể nhìn vào hiệu suất của PoSH Auto-sklearn so với Auto-sklearn 1.0. Đầu tiên, chúng tôi so sánh hiệu suất của PoSH Auto-sklearn với Auto-sklearn 1.0 sử dụng không gian tìm kiếm đầy đủ, và chúng tôi cung cấp những con số đó trong Bảng 3. Đối với cả hai phạm vi thời gian, có một sự giảm mạnh trong loss (10min: 16:21→4:11 và 60min: 7:17→3:19), cho thấy rằng PoSH Auto-sklearn được đề xuất thực sự là một cải tiến so với giải pháp hiện có và có thể khớp các mô hình machine learning tốt hơn trong giới hạn thời gian đã cho.

--- TRANG 15 ---
Auto-sklearn 2.0

Thứ hai, chúng tôi so sánh hiệu suất của PoSH Auto-sklearn (SH; holdout và Port) với Auto-sklearn 1.0 (FB; holdout và KND) chỉ sử dụng không gian tìm kiếm giảm dựa trên kết quả trong Bảng 2. Một lần nữa, có một sự giảm mạnh trong loss đối với cả hai phạm vi thời gian (10min: 5:29→4:11 và 60min: 3:98→3:19), xác nhận những phát hiện đã nêu ở trên. Kết hợp với portfolio, kết quả trung bình không có kết luận về việc liệu việc sử dụng successive halving của chúng tôi có phải là lựa chọn đúng hay holdout đơn giản sẽ tốt hơn. Chúng tôi cũng cung cấp các con số thô trong Phụ lục B.3, nhưng chúng cũng không có kết luận.

4. Phần II: Tự động hóa Quyết định Thiết kế trong AutoML

Mục tiêu của AutoML là mang lại hiệu suất hiện đại mà không yêu cầu người dùng đưa ra các quyết định cấp thấp, ví dụ, mô hình và cấu hình siêu tham số nào để áp dụng. Sử dụng portfolio và SH, PoSH Auto-sklearn đã là một cải tiến so với Auto-sklearn 1.0 về hiệu quả và khả năng mở rộng. Tuy nhiên, các quyết định thiết kế cấp cao, chẳng hạn như lựa chọn giữa cross-validation và holdout hoặc có sử dụng SH hay không, vẫn còn. Do đó, PoSH Auto-sklearn, và các hệ thống AutoML nói chung, gặp phải một vấn đề tương tự như chúng đang cố gắng giải quyết, vì người dùng phải đặt các đối số của họ trên cơ sở từng tập dữ liệu một cách thủ công.

Để làm nổi bật tình huống khó xử này, trong Hình 4 chúng tôi hiển thị kết quả ví dụ so sánh tỷ lệ lỗi cân bằng của pipeline ML tốt nhất được tìm thấy bằng cách tìm kiếm không gian cấu hình của chúng tôi với BO sử dụng holdout, 3CV, 5CV và 10CV với SH và FB trên các ngân sách tối ưu hóa và tập dữ liệu khác nhau. Hàng trên cùng hiển thị kết quả thu được sử dụng cùng ngân sách tối ưu hóa 10 phút trên hai tập dữ liệu khác nhau. Trong khi FB; 10CV là tốt nhất trên tập dữ liệu sylvine (trên cùng bên trái) cùng chiến lược trên median hoạt động trong số các chiến lược tệ nhất trên tập dữ liệu adult (trên cùng bên phải). Ngoài ra, trên sylvine, SH hoạt động tổng thể hơi tệ hơn ngược lại với adult, nơi SH hoạt động tốt hơn trung bình. Các hàng dưới cùng hiển thị cách giới hạn thời gian đã cho tác động đến hiệu suất trên tập dữ liệu jungle chess 2pcs rawendgame complete. Sử dụng ngân sách tối ưu hóa khá hạn chế là 10 phút (dưới cùng bên trái), SH; 3CV, cắt các pipeline ML một cách tích cực ở ngân sách thấp hơn, hoạt động tốt nhất trung bình. Với ngân sách tối ưu hóa cao hơn (dưới cùng bên phải), kết quả tổng thể cải thiện và nhiều chiến lược trở nên cạnh tranh.

Do đó, chúng tôi đề xuất mở rộng các hệ thống AutoML với một bộ chọn chính sách để tự động chọn một chính sách tối ưu hóa cho một tập dữ liệu (xem Hình 1 trong Phần 1 để có tổng quan sơ đồ). Trong phần thứ hai này, chúng tôi thảo luận về phương pháp kết quả, dẫn đến Auto-sklearn 2.0 như triển khai đầu tiên của nó.

4.1 Lựa chọn Chính sách Tự động

Cụ thể, chúng tôi xem xét trường hợp, nơi một hệ thống AutoML có thể được chạy với các chính sách tối ưu hóa khác nhau π∈Π và nghiên cứu cách tự động hóa AutoML thêm bằng cách sử dụng lựa chọn thuật toán trên meta-meta level này. Trong thực tế, chúng tôi mở rộng công thức được giới thiệu trong Phương trình 7 để không sử dụng một hệ thống AutoML A với một chính sách cố định π, mà chứa một bộ chọn chính sách Ξ:D→π:

GE^V(A,Ξ;Dmeta) =1/|Dmeta| ∑|Dmeta|d=1 GE^V(AΞ(Dd)(Dd);Dd): (10)

Trong phần còn lại của phần này, chúng tôi mô tả cách xây dựng một bộ chọn chính sách như vậy.

--- TRANG 16 ---
Auto-sklearn 2.0

Hình 4: Tỷ lệ lỗi cân bằng cuối cùng của BO sử dụng các chiến lược lựa chọn mô hình khác nhau được tính trung bình trên 10 lần lặp lại. Hàng trên cùng: Kết quả cho ngân sách tối ưu hóa 10 phút trên hai tập dữ liệu khác nhau. Hàng dưới cùng: Kết quả cho ngân sách tối ưu hóa 10 và 60 phút trên cùng tập dữ liệu.

4.1.1 Phương pháp

Các hệ thống AutoML bản thân chúng thường được tham số hóa mạnh. Trong trường hợp của chúng tôi, chúng tôi coi chiến lược lựa chọn mô hình và chiến lược phân bổ ngân sách (xem Phần 3.2 và 6.4.1) là những lựa chọn quan trọng mà người dùng phải đưa ra khi sử dụng một hệ thống AutoML để có được hiệu suất cao. Những quyết định này phụ thuộc vào cả tập dữ liệu đã cho và tài nguyên có sẵn. Vì cũng có một tương tác giữa hai chiến lược và portfolio tối ưu P, chúng tôi xem xét ở đây rằng chính sách tối ưu hóa được tham số hóa bởi một sự kết hợp của (i) chiến lược lựa chọn mô hình, (ii) chiến lược phân bổ ngân sách và (iii) một portfolio được xây dựng cho lựa chọn của hai chiến lược. Trong trường hợp của chúng tôi, đây là tám chính sách khác nhau (f3-fold CV, 5-fold CV, 10-fold CV, holdout g×f SH, FBg).

Chúng tôi giới thiệu một lớp mới trên đầu các hệ thống AutoML tự động chọn một chính sách π cho một tập dữ liệu mới. Chúng tôi hiển thị tổng quan về hệ thống này trong Hình 5 bao gồm

--- TRANG 17 ---
Auto-sklearn 2.0

Tùy chọnTập hợp đại diện
của các tập dữ liệu
{D1,...,D|Dmeta |}Cho ∀π∈π:
thực thi TR1-TR3:
xây dựng portfolio PTR4: Thực thi các hệ thống
AutoML Aπ∀π∈π
trên {D1,...,D|Dmeta |}TR5: Tính toán
meta-features của
{D1,...,D|Dmeta |}TR6: Xây
dựng bộ chọn Ξ
MtL1: Tính toán meta-
features của Dnew,trainMtL2: Áp dụng bộ chọn
Ξ để chọn chính sách πTE1: Chạy
hệ thống AutoML Aπ
đánh giá
Pchạy
BOTE2.1: Trả về pipeline
tốt nhất đã tìm thấy M̂λ̂∗ hoặc
ensemble của pipelineTE2.2: Báo cáo
loss trên Dnew,testDnew,train Dnew Dnew,testTRain (ngoại tuyến)
Meta-Learning/TEst (trực tuyến)

Hình 5: Tổng quan sơ đồ của hệ thống Auto-sklearn 2.0 được đề xuất với giai đoạn huấn luyện (TR1{TR6) ở trên và giai đoạn test (MtL1{MtL2&TE1{TE2) ở dưới đường nét đứt. Các hộp tròn, màu tím đề cập đến các bước tính toán, trong khi các hộp chữ nhật, màu cam mô tả dữ liệu đầu vào cho hệ thống AutoML.

một giai đoạn huấn luyện (TR1{TR6) và một giai đoạn test (MtL1{2 và TE1{TE2). Tóm lại, trong các bước huấn luyện TR1{TR3, chúng tôi thực hiện các bước tương tự mà chúng tôi đã phác thảo trong Hình 2. Tuy nhiên, bây giờ chúng tôi làm như vậy cho mỗi sự kết hợp của chiến lược lựa chọn mô hình và phân bổ ngân sách. Các chính sách của chúng tôi là sự kết hợp của một portfolio, một chiến lược lựa chọn mô hình và một chiến lược phân bổ ngân sách. Sau đó chúng tôi thực thi hệ thống AutoML đầy đủ cho mỗi chính sách như vậy trong bước TR4 để có được ước lượng hiệu suất thực tế. Trong bước TR5, chúng tôi tính toán meta-features và sử dụng chúng cùng với ước lượng hiệu suất từ TR4 trong bước TR6 để huấn luyện một bộ chọn chính sách dựa trên mô hình Ξ, mà chúng tôi sẽ sử dụng trong giai đoạn test trực tuyến.

Để không đánh giá quá hiệu suất của Ξ trên một tập dữ liệu Dd, tập dữ liệu Dd không được là một phần của meta-data để xây dựng portfolio. Để khắc phục vấn đề này, chúng tôi thực hiện một 5-fold cross-validation bên trong và xây dựng mỗi Ξ trên bốn phần năm của meta-datasets Dmeta và đánh giá nó trên một phần năm còn lại của meta-datasets Dmeta. Đối với hệ thống AutoML cuối cùng, chúng tôi sau đó sử dụng một portfolio được xây dựng trên tất cả meta-datasets Dmeta.

Đối với một tập dữ liệu mới Dnew∈Dtest, đầu tiên chúng tôi tính toán meta-features mô tả Dnew (MtL1) và sử dụng bộ chọn chính sách dựa trên mô hình từ bước TR6 để tự động chọn một chính sách thích hợp π cho Dnew dựa trên meta-features (MtL2). Điều này sẽ giải thoát người dùng khỏi việc đưa ra quyết định này một mình. Cho một chính sách tối ưu hóa π, sau đó chúng tôi áp dụng hệ thống AutoML Aπ cho Dnew (TE1). Cuối cùng, chúng tôi trả về pipeline tốt nhất đã tìm thấy M̂λ̂ dựa trên tập huấn luyện của Dnew (TE2.1). Tùy chọn, chúng tôi có thể tính toán loss của M̂λ̂ trên tập test của Dnew (TE2.2); chúng tôi nhấn mạnh rằng đây sẽ là lần duy nhất chúng tôi từng truy cập tập test của Dnew. Các bước TE1{TE2 giống như trong Hình 2, và sự khác biệt duy nhất tại thời điểm đánh giá là chúng tôi sử dụng lựa chọn thuật toán để quyết định chính sách nào sử dụng tại thời điểm test thay vì dựa vào một chính sách được chọn thủ công.

Trong phần sau, chúng tôi mô tả hai cách để xây dựng một bộ chọn chính sách và giới thiệu một chiến lược dự phòng bổ sung để làm cho nó mạnh mẽ đối với các thất bại.

Xây dựng chính sách tốt nhất duy nhất Một cách đơn giản để xây dựng một bộ chọn dựa trên giả định rằng các meta-datasets Dmeta đồng nhất và một tập dữ liệu mới tương tự như những tập dữ liệu này. Trong trường hợp như vậy, chúng ta có thể sử dụng lựa chọn thuật toán theo tập hợp (Kerschke et al., 2019), nhằm mục đích tìm thuật toán duy nhất hoạt động tốt nhất trung bình trên một tập hợp

--- TRANG 18 ---
Auto-sklearn 2.0

siêu tham số loại giá trị
Số lượng mẫu tối thiểu để tạo phân chia thêm int [3;20]
Số lượng mẫu tối thiểu để tạo lá mới int [2;20]
Độ sâu tối đa của cây int [0;20]
Số lượng feature tối đa được sử dụng cho phân chia int [1;2]
Bootstrapping trong random forest cat fyes;nog
Voting mềm hoặc cứng khi kết hợp các mô hình cat fsoft;hardg
Chia tỷ lệ giá trị lỗi để tính trọng số dataset cat xem text

Bảng 4: không gian cấu hình của bộ chọn chính sách dựa trên mô hình.

các instance vấn đề. Trong bối cảnh của chúng tôi, nó nhằm mục đích tìm sự kết hợp của lựa chọn mô hình và phân bổ ngân sách tốt nhất trung bình cho tập hợp các meta-datasets Dmeta đã cho. Chính sách tốt nhất duy nhất này sau đó là sự thay thế tự động cho lựa chọn thủ công của chúng tôi về SH và holdout trong PoSH Auto-sklearn. Trong khi điều này có vẻ là một baseline tầm thường, nó thực sự yêu cầu cùng lượng sức mạnh tính toán như chiến lược phức tạp hơn mà chúng tôi giới thiệu tiếp theo.

Xây dựng Bộ chọn Chính sách theo Dataset Thay vì sử dụng một chính sách cố định, đã học, bây giờ chúng tôi đề xuất thích ứng chính sách với tập dữ liệu có sẵn bằng cách sử dụng lựa chọn thuật toán theo instance, có nghĩa là chúng tôi chọn thuật toán thích hợp cho mỗi tập dữ liệu bằng cách tính đến các tính chất của nó. Để xây dựng mô hình lựa chọn meta (TR6), chúng tôi theo thiết kế bộ chọn chính sách của HydraMIP (Xu et al., 2011): cho mỗi cặp chính sách AutoML, chúng tôi khớp một random forest để dự đoán liệu chính sách Aπ có vượt trội hơn chính sách Aπ' cho các meta-features của tập dữ liệu hiện tại hay không. Vì loss phân loại sai phụ thuộc vào sự khác biệt giữa các loss của hai chính sách (tức là ADTM khi chọn chính sách sai), chúng tôi cân nhọng mỗi quan sát meta bằng sự khác biệt loss của chúng. Để làm cho các lỗi có thể so sánh trên các tập dữ liệu khác nhau (Bardenet et al., 2013), chúng tôi chia tỷ lệ các giá trị lỗi cá nhân cho mỗi tập dữ liệu. Tại thời điểm test (TE2), chúng tôi truy vấn tất cả các mô hình cặp đôi cho các meta-features đã cho và sử dụng voting cho Ξ để chọn một chính sách π. Chúng tôi sẽ gọi chiến lược này là Bộ chọn Chính sách.

Để cải thiện hiệu suất của bộ chọn chính sách dựa trên mô hình, chúng tôi áp dụng BO để tối ưu hóa các siêu tham số của bộ chọn chính sách dựa trên mô hình để tối thiểu hóa lỗi cross-validation (Lindauer et al., 2015). Chúng tôi tối ưu hóa tổng cộng bảy siêu tham số, năm trong số đó liên quan đến random forest, một là cách kết hợp các mô hình cặp đôi để có được dự đoán, và cái cuối cùng là chiến lược chia tỷ lệ giá trị lỗi để tính trọng số cho việc so sánh các tập dữ liệu, tức là sử dụng các quan sát thô, chia tỷ lệ với [min;max]/[min;1] trên một cặp hoặc tất cả chính sách hoặc sử dụng sự khác biệt trong thứ hạng làm trọng số (xem Bảng 4). Các siêu tham số được chia sẻ giữa tất cả các mô hình cặp đôi để tránh tăng trưởng giai thừa của số lượng siêu tham số với số lượng chiến lược lựa chọn mô hình mới. Chúng tôi cho phép độ sâu cây là 0, tức là, một cây với tất cả dữ liệu trong một lá duy nhất, tương đương với chiến lược tốt nhất duy nhất được mô tả ở trên.

Meta-Features. Để huấn luyện bộ chọn chính sách dựa trên mô hình của chúng tôi và chọn một chính sách, cũng như sử dụng chiến lược dự phòng, chúng tôi sử dụng meta-features (Brazdil et al., 2008; Vanschoren, 2019) mô tả tất cả các tập dữ liệu meta-train (TR5) và các tập dữ liệu mới (TE1). Để tránh các vấn đề đã thảo luận trong Phần 3.1, chúng tôi chỉ sử dụng các meta-features rất đơn giản và mạnh mẽ, có thể được

--- TRANG 19 ---
Auto-sklearn 2.0

tính toán một cách đáng tin cậy trong thời gian tuyến tính cho mọi tập dữ liệu: 1) số lượng datapoint và 2) số lượng feature. Thực tế, những điều này đã được lưu trữ như meta-data cho cấu trúc dữ liệu giữ tập dữ liệu. Việc chỉ sử dụng hai meta-features tầm thường và rẻ tiền này cho bộ chọn có thể được coi như học các fallback được thiết kế thủ công mà chúng tôi đã thảo luận trong Phần 3.3. Trong các thí nghiệm của chúng tôi, chúng tôi sẽ cho thấy rằng ngay cả với chỉ những meta-features tầm thường và rẻ tiền này, chúng tôi có thể cải thiện đáng kể so với một chính sách tĩnh.

Chiến lược dự phòng. Vì không có đảm bảo rằng bộ chọn chính sách dựa trên mô hình của chúng tôi sẽ ngoại suy tốt cho các tập dữ liệu ngoài meta-datasets, chúng tôi triển khai một biện pháp dự phòng để tránh thất bại. Những thất bại như vậy có thể có hại nếu một tập dữ liệu mới, ví dụ, lớn hơn nhiều so với bất kỳ tập dữ liệu nào trong meta-dataset, và bộ chọn chính sách dựa trên mô hình đề xuất sử dụng một chính sách sẽ timeout mà không có giải pháp nào. Cụ thể hơn, nếu không có tập dữ liệu nào trong meta-datasets có giá trị cao hơn hoặc bằng cho mỗi meta-feature (tức là thống trị các meta-features của tập dữ liệu), hệ thống của chúng tôi quay lại sử dụng holdout với SH, đây là chính sách tích cực và rẻ nhất mà chúng tôi xem xét. Chúng tôi trực quan hóa điều này trong Hình 3 nơi chúng tôi đánh dấu các tập dữ liệu ngoài phân phối meta của chúng tôi bằng các dấu x đen.

4.2 Kết quả Thí nghiệm

Để nghiên cứu hiệu suất của bộ chọn chính sách, chúng tôi so sánh nó với PoSH Auto-sklearn như đã mô tả trong Phần 3 và Auto-sklearn 1.0. Từ bây giờ chúng tôi gọi PoSH Auto-sklearn + bộ chọn chính sách là Auto-sklearn 2.0. Như trước, chúng tôi nghiên cứu hai phạm vi, 10 phút và 60 phút, và sử dụng các phiên bản của PoSH Auto-sklearn và Auto-sklearn 2.0 được xây dựng với những phạm vi thời gian này trong đầu. Tương tự, chúng tôi sử dụng 208 tập dữ liệu tương tự để xây dựng các hệ thống AutoML của chúng tôi và 39 tập dữ liệu tương tự để đánh giá chúng.

Nhìn vào Bảng 5, chúng ta thấy rằng Auto-sklearn 2.0 đạt được lỗi thấp nhất, tốt hơn đáng kể cho cả hai ngân sách tối ưu hóa. Đáng chú ý nhất, Auto-sklearn 2.0 giảm lỗi tương đối so với Auto-sklearn 1.0 78% (10MIN) và 65%, tương ứng, có nghĩa là giảm hệ số 4:5 và ba.

Hóa ra những kết quả này bị lệch bởi một số tập dữ liệu lớn (task ID 189873 và 75193 cho cả hai phạm vi; 189866, 189874, 168796 và 168797 chỉ cho phạm vi mười phút) trên đó việc khởi tạo KND của Auto-sklearn 1.0 chỉ đề xuất các pipeline ML

10MIN 60MIN
μ std μ std
(1) Auto-sklearn (2.0) 3:58 0:23 2:47 0:18
(2) PoSH-Auto-sklearn 4:11 0:09 3:19 0:12
(3) Auto-sklearn (1.0) 16:21 0:27 7:17 0:30

Bảng 5: Lỗi cân bằng chuẩn hóa trung bình (ADTM, thấp hơn là tốt hơn) của Auto-sklearn 2.0, PoSH Auto-sklearn và Auto-sklearn 1.0 trung bình trên 10 lần lặp lại trên 39 tập dữ liệu. Chúng tôi in đậm giá trị trung bình tốt nhất (trên mỗi ngân sách tối ưu hóa) và gạch chân kết quả không khác biệt về mặt thống kê theo Wilcoxon-signed-rank Test (α=0:05).

--- TRANG 20 ---
Auto-sklearn 2.0

0 10 20 30 40 50 60
thời gian [phút]1.41.61.82.02.22.42.62.8thứ hạng trung bìnhAuto-sklearn (2.0)
PoSH-Auto-sklearn
Auto-sklearn (1.0)

Hình 6: Hiệu suất theo thời gian. Chúng tôi báo cáo thứ hạng median (thấp hơn là tốt hơn) và percentile thứ 10 và 90 theo thời gian cho Auto-sklearn 2.0 và các hệ thống AutoML trước đó. Cụ thể, chúng tôi tính toán thứ hạng trung bình cho tất cả 39 cho tất cả 1000 tổ hợp của 10 seed của 3 hệ thống AutoML, và tính toán median và percentile của 1000 thứ hạng trung bình này.

timeout hoặc đạt giới hạn bộ nhớ và do đó cạn kiệt ngân sách tối ưu hóa cho không gian cấu hình đầy đủ. Hệ thống AutoML mới của chúng tôi không gặp phải vấn đề này vì nó a) chọn SH để tránh chi quá nhiều thời gian cho các pipeline ML không hứa hẹn và b) có thể trả về dự đoán và kết quả ngay cả khi một pipeline ML không được đánh giá cho ngân sách đầy đủ hoặc hội tụ sớm; và ngay cả sau khi loại bỏ các tập dữ liệu được đề cập khỏi trung bình, hiệu suất của Auto-sklearn 1.0 vẫn tệ hơn đáng kể so với Auto-sklearn 2.0.

Khi nhìn vào hệ thống trung gian, tức là PoSH Auto-sklearn, chúng tôi thấy rằng nó vượt trội hơn Auto-sklearn 1.0 về tỷ lệ lỗi cân bằng chuẩn hóa, nhưng bước bổ sung để chọn chiến lược lựa chọn mô hình và phân bổ ngân sách mang lại cho Auto-sklearn 2.0 một lợi thế. Khi không xem xét các tập dữ liệu lớn mà Auto-sklearn 1.0 thất bại, hiệu suất của chúng trở nên rất giống nhau.

Hình 6 cung cấp một góc nhìn khác về kết quả, trình bày thứ hạng trung bình (nơi các thất bại có trọng số ít hơn so với hiệu suất trung bình). Auto-sklearn 2.0 vẫn có thể đưa ra kết quả tốt nhất, PoSH Auto-sklearn nên được ưu tiên hơn Auto-sklearn 1.0 trong 30 phút đầu tiên và sau đó hội tụ đến xấp xỉ cùng thứ hạng.

4.3 Ablation

Bây giờ, chúng tôi nghiên cứu đóng góp của mỗi cải tiến của chúng tôi trong một nghiên cứu ablation. Chúng tôi lần lượt vô hiệu hóa một thành phần và so sánh hiệu suất với toàn bộ hệ thống sử dụng 39 tập dữ liệu từ AutoML benchmark như đã thực hiện trong các phần thí nghiệm trước đó.

--- TRANG 21 ---
Auto-sklearn 2.0

Các thành phần này là (1) sử dụng bộ chọn chính sách dựa trên mô hình theo tập dữ liệu để chọn một chính sách, (2) chỉ sử dụng một tập con của các chính sách có sẵn, và (3) khởi động nóng BO với một portfolio.

4.3.1 Chúng ta có cần lựa chọn theo tập dữ liệu không?

Đầu tiên chúng tôi kiểm tra xem chúng tôi đạt được bao nhiều hiệu suất bằng cách có một bộ chọn chính sách dựa trên mô hình để quyết định giữa các chiến lược AutoML khác nhau dựa trên meta-features và cách xây dựng bộ chọn chính sách dựa trên mô hình này, hoặc liệu có đủ để chọn một chiến lược duy nhất dựa trên các tập dữ liệu meta-training hay không. Chúng tôi so sánh hiệu suất của toàn bộ hệ thống sử dụng bộ chọn chính sách dựa trên mô hình với việc sử dụng một chiến lược tĩnh duy nhất (single best) và cả bộ chọn chính sách dựa trên mô hình và single best, mà không có cơ chế dự phòng cho các tập dữ liệu out-of-distribution và đưa ra tất cả kết quả trong Bảng 6. Chúng tôi cũng cung cấp hai baseline bổ sung: một baseline ngẫu nhiên, gán ngẫu nhiên một chính sách cho một lần chạy và một baseline oracle, đánh dấu lỗi thấp nhất có thể đạt được bởi bất kỳ chính sách nào.7

Đầu tiên, chúng tôi so sánh hiệu suất của bộ chọn chính sách dựa trên mô hình với single best. Chúng ta có thể quan sát rằng đối với 10 phút, có một cải thiện nhẹ về hiệu suất, trong khi hiệu suất cho 60 phút gần như bằng nhau. Trong khi không có sự khác biệt đáng kể so với single best cho 10 phút, có cho 60 phút. Những con số này có thể được so sánh với Bảng 2 để xem chúng tôi làm thế nào so với việc chọn một chính sách duy nhất bằng tay. Chúng tôi thấy rằng lựa chọn thuật toán được đề xuất của chúng tôi so sánh thuận lợi, đặc biệt là cho phạm vi thời gian dài hơn.

Thứ hai, để nghiên cứu cần bao nhiều tài nguyên để chi tiêu cho việc tạo dữ liệu huấn luyện cho bộ chọn chính sách dựa trên mô hình của chúng tôi, chúng tôi xem xét ba phương pháp: (P) chỉ sử dụng hiệu suất portfolio mà chúng tôi đã tính toán trước và lưu trữ trong các ma trận hiệu suất như đã mô tả trong Phần 3.1.1, (P+BO) thực sự chạy Auto-sklearn sử dụng portfolio và BO trong 10 và 60 phút, tương ứng, và (P+BO+E) ngoài ra cũng xây dựng ensemble, mang lại meta-data thực tế nhất. Chạy BO trên tất cả 208 tập dữ liệu (P+BO) đắt hơn nhiều so với tra cứu bảng (P); xây dựng ensemble (P+BO+E) chỉ thêm vài giây đến phút so với (P+BO).

Đối với cả hai ngân sách tối ưu hóa sử dụng P+BO mang lại kết quả tốt nhất sử dụng bộ chọn chính sách dựa trên mô hình theo sát bởi P+BO+ENS, xem Bảng 6. Phương pháp rẻ nhất, P, mang lại kết quả tệ nhất cho thấy rằng đáng để đầu tư tài nguyên vào việc tính toán meta-data tốt. Đáng ngạc nhiên, khi nhìn vào single best, hiệu suất trở nên tệ hơn khi sử dụng meta-data có vẻ tốt hơn. Chúng tôi điều tra lý do tại sao P+BO hoạt động hơi tốt hơn P+BO+ENS. Khi sử dụng bộ chọn chính sách dựa trên mô hình, điều này có thể được giải thích bởi một tập dữ liệu duy nhất cho cả hai phạm vi thời gian mà chính sách được chọn bởi bộ chọn chính sách dựa trên mô hình tệ hơn so với chính sách single best. Khi nhìn vào single best, không có tập dữ liệu đơn lẻ nào nổi bật. Để tóm tắt, đầu tư thêm tài nguyên để tính toán meta-data thực tế dẫn đến hiệu suất được cải thiện, nhưng cho đến nay, có vẻ như có hiệu ứng của BO trong meta-data là đủ, trong khi ensemble dường như dẫn đến chất lượng meta-data thấp hơn.

7. Chúng tôi muốn lưu ý rằng hiệu suất oracle có thể không bằng không vì chúng tôi chuẩn hóa kết quả bằng test loss tốt nhất duy nhất được tìm thấy cho một mô hình duy nhất để chuẩn hóa kết quả. Khi đánh giá chính sách tốt nhất trên một tập dữ liệu, điều này rất có thể dẫn đến việc chọn một mô hình trên tập validation không phải là mô hình tốt nhất duy nhất trên tập test mà chúng tôi sử dụng để chuẩn hóa dữ liệu.

--- TRANG 22 ---
Auto-sklearn 2.0

10 Phút 60 Phút
được huấn luyện trên P P+BO P+BO+E P P+BO P+BO+E
bộ chọn chính sách dựa trên mô hình 3:58 3.56 3:58 2:53 2.32 2:47
bộ chọn chính sách dựa trên mô hình w/o fallback 5:43 5:68 4:79 4:98 5:36 5:43
single best 3:88 3:67 3:69 2:49 2:38 2:44
single best w/o fallback 5:18 6:38 6:40 5:10 5:01 5:07
oracle 2:33 1:22
random 8:32 6:18

Bảng 6: Lỗi cân bằng chuẩn hóa trung bình (ADTM, thấp hơn là tốt hơn) cho 10 và 60 phút. Chúng tôi báo cáo hiệu suất cho bộ chọn chính sách dựa trên mô hình và single best khi được huấn luyện trên dữ liệu khác nhau thu được trên Dmeta (P = Portfolio, BO = Bayesian Optimization, E = Ensemble) cũng như bộ chọn chính sách dựa trên mô hình mà không có fallback. Phần thứ hai của bảng hiển thị kết quả luôn chọn chính sách tốt nhất trên tập test (oracle) và kết quả cho việc chọn một chính sách ngẫu nhiên (random) như baseline. Chúng tôi in đậm giá trị trung bình tốt nhất (trên mỗi ngân sách tối ưu hóa) và gạch chân kết quả không khác biệt về mặt thống kê theo Wilcoxon-signed-rank Test (α=0:05).

Cuối cùng, chúng tôi cũng xem xét kỹ hơn tác động của cơ chế fallback để xác minh rằng các cải tiến của chúng tôi không chỉ đơn thuần do thành phần này. Chúng tôi quan sát rằng hiệu suất giảm đối với tất cả các chiến lược lựa chọn chính sách không sử dụng cơ chế fallback. Đối với thiết lập 10 phút ngắn hơn, chúng tôi thấy rằng bộ chọn chính sách dựa trên mô hình vẫn vượt trội hơn single best, trong khi đối với thiết lập 60 phút dài hơn, single best dẫn đến hiệu suất tốt hơn. Sự suy giảm hiệu suất khá mạnh so với bộ chọn chính sách dựa trên mô hình thông thường chủ yếu có thể được giải thích bởi một số tập dữ liệu khổng lồ, mà bộ chọn chính sách dựa trên mô hình không thể ngoại suy (và mà single best không tính đến). Dựa trên những quan sát này, chúng tôi đề xuất nghiên cứu về một chiến lược fallback thích ứng có thể thay đổi chiến lược lựa chọn mô hình trong quá trình thực thi của hệ thống AutoML để một bộ chọn chính sách có thể được sử dụng trên các tập dữ liệu out-of-distribution. Chúng tôi kết luận rằng việc sử dụng bộ chọn chính sách dựa trên mô hình là có lợi, và việc sử dụng chiến lược fallback để đối phó với các tập dữ liệu out-of-distribution có thể cải thiện hiệu suất đáng kể.

4.3.2 Chúng ta có cần các chiến lược lựa chọn mô hình khác nhau không?

Tiếp theo, chúng tôi nghiên cứu liệu chúng tôi có cần các chiến lược lựa chọn mô hình khác nhau hay không. Để làm điều này, chúng tôi xây dựng các bộ chọn chính sách dựa trên mô hình trên các tập con khác nhau của tám sự kết hợp có sẵn của các chiến lược lựa chọn mô hình và phân bổ ngân sách: f3-fold CV, 5-fold CV, 10-fold CV, holdout g×f SH, FBg. Only Holdout bao gồm holdout với SH hoặc FB (2 sự kết hợp), Only CV bao gồm 3-fold CV, 5-fold CV và 10-fold CV, tất cả chúng với SH hoặc FB (6 sự kết hợp), FB chứa cả holdout và cross-validation và gán cho mỗi đánh giá pipeline cùng ngân sách (4 sự kết hợp) và Only SH sử dụng SH để gán ngân sách (4 sự kết hợp).

--- TRANG 23 ---
Auto-sklearn 2.0

Bộ chọn Ngẫu nhiên Oracle
μ std μ std μ std
10 PhútTất cả 3.58 0.23 7.46 2.02 2.33 0.06
Chỉ Holdout 4.03 0.14 3.78 0.23 3.23 0.10
Chỉ CV 6.11 0.11 8.66 0.70 5.28 0.06
Chỉ FB 3.50 0.20 7.64 2.00 2.59 0.09
Chỉ SH 3.63 0.19 6.95 1.98 2.75 0.07
60 PhútTất cả 2.47 0.18 5.64 1.95 1.22 0.08
Chỉ Holdout 3.18 0.15 3.13 0.12 2.62 0.07
Chỉ CV 5.09 0.19 6.85 0.86 3.94 0.10
Chỉ FB 2.39 0.18 5.46 1.52 1.51 0.06
Chỉ SH 2.44 0.24 5.13 1.72 1.68 0.12

Bảng 7: Lỗi cân bằng chuẩn hóa trung bình (ADTM, thấp hơn là tốt hơn) cho toàn bộ hệ thống và khi không xem xét tất cả các chiến lược lựa chọn mô hình.

Trong Bảng 7, chúng tôi so sánh hiệu suất của việc chọn một chính sách ngẫu nhiên (random), hiệu suất của việc chọn chính sách tốt nhất trên tập test và do đó đưa ra bound dưới trên ADTM (oracle) và bộ chọn chính sách dựa trên mô hình của chúng tôi. Oracle chỉ ra hiệu suất tốt nhất có thể với mỗi tập con các chiến lược lựa chọn mô hình này. Hóa ra cả Only Holdout và Only CV đều có hiệu suất oracle tệ hơn nhiều so với All, với hiệu suất oracle của Only CV thậm chí còn tệ hơn so với hiệu suất của bộ chọn chính sách dựa trên mô hình cho All. Khi nhìn vào Full budget (FB), hóa ra tập con này sẽ tốt hơn một chút về hiệu suất với một bộ chọn chính sách. Tuy nhiên, hiệu suất oracle tệ hơn so với All cho thấy rằng có một số tính bổ sung giữa các chính sách khác nhau mà bộ chọn chính sách chưa thể khai thác. Đối với Only Holdout, đáng ngạc nhiên là bộ chọn chính sách ngẫu nhiên hoạt động hơi tốt hơn bộ chọn chính sách dựa trên mô hình. Chúng tôi cho rằng điều này do thực tế là holdout với cả SH và FB hoạt động tương tự và lựa chọn giữa hai cái này chưa thể được học, có thể cũng được chỉ ra bởi hiệu suất gần của bộ chọn ngẫu nhiên.

Những kết quả này cho thấy rằng một loạt các chiến lược lựa chọn mô hình có sẵn để lựa chọn tăng hiệu suất tốt nhất có thể. Tuy nhiên, chúng cũng cho thấy rằng một bộ chọn chính sách dựa trên mô hình chưa nhất thiết có thể tận dụng tiềm năng này. Điều này đặt câu hỏi về tính hữu ích của việc lựa chọn từ tất cả các chiến lược lựa chọn mô hình, tương tự như một phát hiện gần đây chứng minh rằng việc tăng số lượng các chính sách khác nhau mà một bộ chọn chính sách có thể chọn dẫn đến giảm tổng quát hóa (Balcan et al., 2021). Tuy nhiên, chúng tôi tin rằng điều này chỉ ra câu hỏi nghiên cứu về liệu chúng ta có thể học trên meta-datasets những chiến lược lựa chọn mô hình và phân bổ ngân sách nào để bao gồm trong tập hợp các chiến lược để lựa chọn hay không. Ngoài ra, với sự sẵn có ngày càng tăng của meta-datasets và nghiên cứu tiếp tục về các bộ chọn chính sách mạnh mẽ, chúng tôi mong đợi tính linh hoạt này cuối cùng sẽ mang lại hiệu suất được cải thiện.

--- TRANG 24 ---
Auto-sklearn 2.0

10phút 60phút
μ std μ std
Với PortfolioBộ chọn chính sách 3.58 0.23 2.47 0.18
Single best 3.69 0.14 2.44 0.12
Không có PortfolioBộ chọn chính sách 5.63 0.89 3.42 0.32
Single best 5.37 0.58 3.61 0.61

Bảng 8: Lỗi cân bằng chuẩn hóa trung bình (ADTM, thấp hơn là tốt hơn) sau 10 và sau 60 phút với portfolio (trên) và không có (dưới). Hàng "với portfolio" và "bộ chọn chính sách" tạo thành hệ thống AutoML đầy đủ bao gồm portfolio, BO và ensemble) và hàng "không có portfolio" và "bộ chọn chính sách" chỉ loại bỏ portfolio (cả từ meta-data cho việc xây dựng bộ chọn chính sách dựa trên mô hình và tại runtime). Chúng tôi in đậm giá trị trung bình tốt nhất (trên mỗi ngân sách tối ưu hóa) và gạch chân kết quả không khác biệt về mặt thống kê theo Wilcoxon-signed-rank Test (α=0:05).

4.3.3 Chúng ta có vẫn cần khởi động nóng tối ưu hóa Bayesian không?

Cuối cùng, chúng tôi phân tích tác động của portfolio. Với những cải tiến khác, bây giờ chúng tôi thảo luận liệu chúng tôi vẫn cần thêm độ phức tạp và đầu tư tài nguyên để khởi động nóng BO (và do đó có thể tiết kiệm thời gian để xây dựng ma trận hiệu suất để xây dựng portfolio) hay không. Đối với nghiên cứu này, chúng tôi hoàn toàn loại bỏ portfolio khỏi hệ thống AutoML của chúng tôi, có nghĩa là chúng tôi trực tiếp bắt đầu với BO và xây dựng ensemble { cả để tạo dữ liệu mà chúng tôi huấn luyện bộ chọn chính sách của chúng tôi và để báo cáo hiệu suất. Chúng tôi báo cáo kết quả trong Bảng 8.

So sánh hiệu suất của hệ thống AutoML với bộ chọn chính sách dựa trên mô hình có và không có portfolio (Hàng 1 và 3), có một sự giảm rõ ràng về hiệu suất khi vô hiệu hóa portfolio. So sánh Hàng 2 và 4 cũng chứng minh rằng một portfolio là cần thiết khi sử dụng chính sách single best. Ablation này làm nổi bật tầm quan trọng của việc khởi tạo quy trình tìm kiếm của các hệ thống AutoML với các pipeline hoạt động tốt.

5. So sánh với các hệ thống AutoML khác

Sau khi thiết lập rằng Auto-sklearn 2.0 thực sự cải thiện so với Auto-sklearn 1.0, bây giờ chúng tôi so sánh hệ thống của chúng tôi với các hệ thống AutoML được thiết lập khác. Để làm điều này, chúng tôi sử dụng bộ AutoML benchmark có sẵn công khai định nghĩa một môi trường benchmarking cố định cho các so sánh hệ thống AutoML (Gijsbers et al., 2019). Chúng tôi sử dụng triển khai gốc của benchmark và so sánh Auto-sklearn 1.0 và Auto-sklearn 2.0 với các triển khai được cung cấp của Auto-WEKA (Thornton et al., 2013), TPOT (Olson et al., 2016a,b), H2O AutoML (LeDell và Poirier, 2020) và một baseline random forest với điều chỉnh siêu tham số trên 39 tập dữ liệu như được triển khai bởi bộ benchmark.

--- TRANG 25 ---
Auto-sklearn 2.0

5.1 Tích hợp và thiết lập

Để tránh sự khác biệt hiệu suất phụ thuộc phần cứng, chúng tôi (tái) chạy tất cả các hệ thống AutoML trên phần cứng local của chúng tôi (xem Phần 3.4.3). Chúng tôi sử dụng thiết lập 1h8c được định trước, chia mỗi tập dữ liệu thành mười fold và cho mỗi framework một giờ trên tám CPU core để tạo ra một mô hình cuối cùng. Chúng tôi cũng gán cho mỗi lần chạy 32GB RAM, mà một cluster manager SLURM kiểm soát. Ngoài ra, chúng tôi tiến hành năm lần lặp lại để tính đến tính ngẫu nhiên. Benchmark đi kèm với các container Docker (Merkel, 2014). Tuy nhiên, Docker yêu cầu quyền truy cập superuser trên các node thực thi, không có sẵn trên cluster tính toán của chúng tôi. Do đó, chúng tôi mở rộng AutoML benchmark với hỗ trợ cho Singularity images (Kurtzer et al., 2017), và sử dụng chúng để cô lập các cài đặt framework khỏi nhau. Để tái tạo, chúng tôi đưa ra các phiên bản chính xác mà chúng tôi sử dụng trong Bảng 17 trong Phụ lục.

Phân bổ tài nguyên mặc định của AutoML benchmark là một thiết lập song song cao với tám core. Chúng tôi chọn cách đơn giản nhất để tận dụng những tài nguyên này cho Auto-sklearn và đánh giá tám pipeline ML song song, gán cho mỗi totalmemory/num_cores RAM, là 4GB. Điều này cho phép chúng tôi đánh giá các cấu hình thu được từ portfolio hoặc KND song song nhưng cũng yêu cầu một chiến lược song song để chạy BO sau đó. Chúng tôi mở rộng gói tối ưu hóa Bayesian SMAC3 (Lindauer et al., 2022) để cho phép tối ưu hóa song song không đồng bộ. Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng tính ngẫu nhiên vốn có của random forest được sử dụng bởi SMAC kết hợp với tìm kiếm ngẫu nhiên xen kẽ của SMAC là đủ để có được kết quả hoạt động tốt hơn nhiều so với tính song song trước đó được triển khai trong Auto-sklearn qua SMAC (Ramage, 2015). Bất cứ khi nào một pipeline hoàn thành huấn luyện, Auto-sklearn kiểm tra xem có một instance của việc xây dựng ensemble đang chạy hay không, và nếu không, nó sử dụng một trong tám slot để tiến hành xây dựng ensemble và nếu không thì tiếp tục khớp một pipeline mới. Chúng tôi triển khai phiên bản song song của Auto-sklearn này sử dụng Dask (Dask Development Team, 2016).

5.2 Kết quả

Chúng tôi đưa ra kết quả cho AutoML benchmark trong Bảng 9. Đối với mỗi tập dữ liệu, chúng tôi đưa ra hiệu suất trung bình của các hệ thống AutoML trên tất cả mười fold và năm lần lặp lại và in đậm cái có lỗi thấp nhất (chúng tôi không thể đưa ra bất kỳ thông tin nào về việc liệu sự khác biệt có đáng kể hay không vì chúng tôi không thể tính toán ý nghĩa trên các fold cross-validation như đã mô tả bởi Bengio và Grandvalet, 2004).

Chúng tôi báo cáo log loss cho các tập dữ liệu đa lớp và 1−AUC cho các tập dữ liệu nhị phân (thấp hơn là tốt hơn). Ngoài ra, chúng tôi cung cấp thứ hạng trung bình như một thước đo tổng hợp (được tính bằng cách tính trung bình tất cả các fold và lần lặp lại trên mỗi tập dữ liệu và sau đó tính toán thứ hạng). Hơn nữa, chúng tôi đếm số lần mỗi framework là người thắng cuộc trên một tập dữ liệu (champion), và đưa ra các loss, thắng và hòa so với Auto-sklearn 2.0. Sau đó chúng tôi sử dụng những điều này để thực hiện kiểm tra dấu hiệu nhị thức (Demšar, 2006) để so sánh các thuật toán cá nhân với Auto-sklearn 2.0.

Kết quả trong Bảng 9 cho thấy rằng không có hệ thống AutoML nào tốt nhất trên tất cả các tập dữ liệu, và ngay cả TunedRF cũng hoạt động tốt nhất trên một số tập dữ liệu. Tuy nhiên, chúng ta cũng có thể quan sát rằng Auto-sklearn 2.0 được đề xuất có thứ hạng trung bình thấp nhất. Nó được theo sau bởi H2O AutoML và Auto-sklearn 1.0 hoạt động xấp xỉ ngang bằng đối với điểm số thứ hạng và số lần chúng là người thắng cuộc trên một tập dữ liệu. Theo cả hai thước đo tổng hợp, TunedRF, Auto-WEKA và TPOT không thể theo kịp và dẫn đến kết quả tệ hơn đáng kể. Cuối cùng,

--- TRANG 26 ---
Auto-sklearn 2.0

AS 2.0 AS 1.0 AW TPOT H2O TunedRF
adult 0:0692 0:0701 0:0920 0:0750 0:0690 0:0902
airlines 0:2724 0:2726 0:3241 0:2758 0:2682 -
albert 0:2413 0:2381 - 0:2681 0:2530 0:2616
amazon 0:1233 0:1412 0:1836 0:1345 0:1218 0:1377
apsfailure 0:0085 0:0081 0:0365 0:0099 0:0081 0:0087
australian 0:0594 0:0702 0:0709 0:0670 0:0607 0:0610
bank-marketing 0:0607 0:0616 0:1441 0:0664 0:0610 0:0692
blood-transfusion 0:2428 0:2474 0:2619 0:2761 0:2430 0:3122
car 0:0012 0:0046 0:1910 2:7843 0:0032 0:0421
christine 0:1821 0:1703 0:2026 0:1821 0:1763 0:1908
cnae-9 0:1424 0:1779 0:7045 0:1483 0:1807 0:3119
connect-4 0:3387 0:3535 1:7083 0:3856 0:3127 0:4777
covertype 0:1103 0:1435 3:3515 0:5332 0:1281 -
credit-g 0:2031 0:2159 0:2505 0:2144 0:2078 0:1985
dilbert 0:0399 0:0332 2:0791 0:1153 0:0359 0:3283
dionis 0:5620 0:7171 - - 4:7758 -
fabert 0:7386 0:7466 5:4784 0:8431 0:7274 0:8060
fashion-mnist 0:2511 0:2524 0:9505 0:4314 0:2762 0:3613
guillermo 0:0945 0:0871 0:1251 0:1680 0:0911 0:0973
helena 2:4974 2:5432 14:3523 2:8738 2:7578 -
higgs 0:1824 0:1846 0:3379 0:1969 0:1846 0:1966
jannis 0:6709 0:6637 2:9576 0:7244 0:6695 0:7288
jasmine 0:1141 0:1196 0:1356 0:1123 0:1141 0:1118
jungle chess 0:2104 0:1956 1:6969 0:9557 0:1479 0:4020
kc1 0:1611 0:1594 0:1780 0:1530 0:1745 0:1590
kddcup09 0:1580 0:1632 - 0:1696 0:1636 0:2058
kr-vs-kp 0:0001 0:0003 0:0217 0:0003 0:0002 0:0004
mfeat-factors 0:0726 0:0901 0:5678 0:1049 0:1009 0:2091
miniboone 0:0121 0:0128 0:0352 0:0177 0:0129 0:0183
nomao 0:0035 0:0039 0:0157 0:0047 0:0036 0:0049
numerai28.6 0:4696 0:4705 0:4729 0:4741 0:4695 0:4792
phoneme 0:0299 0:0366 0:0416 0:0307 0:0325 0:0347
riccardo 0:0002 0:0002 0:0020 0:0021 0:0003 0:0002
robert 1:4302 1:3800 - 1:8600 1:4927 1:6877
segment 0:1482 0:1749 1:2497 0:1660 0:1580 0:1718
shuttle 0:0002 0:0004 0:0100 0:0008 0:0004 0:0006
sylvine 0:0105 0:0091 0:0290 0:0075 0:0106 0:0159
vehicle 0:3341 0:3754 2:0662 0:4402 0:3067 0:4839
volkert 0:7477 0:7862 3:4235 0:9852 0:8121 0:9792
Thứ hạng 1:79 2:64 5:72 4:08 2:38 4:38
Hiệu suất tốt nhất 19 8 0 2 8 2
Thắng/Thua/Hòa của AS 2.0 - 28=11=0 39=0=0 35=4=0 26=13=0 36=3=0
P-values (AS 2.0 vs. phương pháp khác), - :009 <:000 <:000 :053 <:000
dựa trên kiểm tra dấu hiệu Nhị thức

Bảng 9: Kết quả của AutoML benchmark trung bình trên năm lần lặp lại. Chúng tôi báo cáo log loss cho các tập dữ liệu đa lớp và 1−AUC cho các tập dữ liệu phân loại nhị phân (thấp hơn là tốt hơn). AS là viết tắt của Auto-sklearn và AW của Auto-WEKA. Auto-sklearn có thứ hạng tổng thể tốt nhất, hiệu suất tốt nhất trong hầu hết các tập dữ liệu và, dựa trên P-values của kiểm tra dấu hiệu Nhị thức, chúng tôi có thêm sự tin tưởng vào hiệu suất mạnh mẽ của nó.

--- TRANG 27 ---
Auto-sklearn 2.0

cả hai phiên bản của Auto-sklearn dường như khá mạnh mẽ vì chúng đáng tin cậy cung cấp kết quả trên tất cả các tập dữ liệu, bao gồm cả những tập lớn nhất nơi một số phương pháp khác thất bại.

6. Công trình Liên quan

Bây giờ chúng tôi trình bày công trình liên quan về các đóng góp cá nhân của chúng tôi (portfolio, chiến lược lựa chọn mô hình, và lựa chọn thuật toán) cũng như về các hệ thống AutoML liên quan.

6.1 Công trình Liên quan về Portfolio

Portfolio được giới thiệu cho các vấn đề tối ưu hóa tổ hợp khó, nơi thời gian chạy giữa các thuật toán khác nhau biến đổi mạnh và việc phân bổ thời gian chia sẻ cho nhiều thuật toán thay vì phân bổ tất cả thời gian có sẵn cho một thuật toán duy nhất giảm chi phí trung bình để giải quyết một vấn đề (Huberman et al., 1997; Gomes và Selman, 2001), và có ứng dụng trong các lĩnh vực phụ khác nhau của AI (Smith-Miles, 2008; Kotthoff, 2014; Kerschke et al., 2019).

Portfolio thuật toán được giới thiệu cho ML với tên gọi algorithm ranking để giảm thời gian cần thiết để thực hiện lựa chọn mô hình so với việc chạy tất cả các thuật toán đang được xem xét (Brazdil và Soares, 2000; Soares và Brazdil, 2000), bỏ qua những cái dư thừa (Brazdil et al., 2001). Portfolio ML có thể vượt trội hơn tối ưu hóa siêu tham số với tối ưu hóa Bayesian (Wistuba et al., 2015b), tối ưu hóa Bayesian với một mô hình có tính đến dữ liệu hiệu suất quá khứ (Wistuba et al., 2015a) hoặc có thể được áp dụng khi đơn giản là không có thời gian để thực hiện tối ưu hóa siêu tham số đầy đủ (Feurer et al., 2018). Hơn nữa, tối ưu hóa không cần mô hình dựa trên portfolio như vậy vừa dễ triển khai hơn tối ưu hóa Bayesian thông thường và các giải pháp dựa trên meta-feature, và portfolio có thể được chia sẻ dễ dàng giữa các nhà nghiên cứu và thực hành mà không cần chia sẻ meta-data (Wistuba et al., 2015a,b; Pösterer et al., 2018) hoặc phần mềm tối ưu hóa siêu tham số bổ sung. Ở đây, mục tiêu của chúng tôi là có các thiết lập siêu tham số mạnh mẽ khi không có thời gian để tối ưu hóa với một thuật toán black-box điển hình.

Việc tạo ra portfolio thuật toán hiệu quả là một lĩnh vực nghiên cứu tích cực với Thuật toán Tham lam là một lựa chọn phổ biến (Xu et al., 2010, 2011; Seipp et al., 2015; Wistuba et al., 2015b; Lindauer et al., 2017; Feurer et al., 2018; Feurer và Hutter, 2018) do tính đơn giản của nó. Wistuba et al. (2015b) đầu tiên đề xuất việc sử dụng Thuật toán Tham lam cho các pipeline của portfolio ML, tối thiểu hóa thứ hạng trung bình trên meta-datasets cho một thuật toán ML duy nhất. Sau đó, họ mở rộng công trình của mình để cập nhật các thành viên của một portfolio theo kiểu round-robin, lần này sử dụng lỗi phân loại sai chuẩn hóa trung bình như một hàm loss và dựa vào một mô hình Gaussian process (Wistuba et al., 2015a). Hàm loss của phương pháp đầu tiên không tối ưu hóa thước đo quan tâm, trong khi phương pháp thứ hai yêu cầu một mô hình và không đảm bảo rằng các thuật toán hoạt động tốt được thực thi sớm, điều này có thể có hại dưới các ràng buộc thời gian.

Nghiên cứu về Thuật toán Tham lam tiếp tục sau bài nộp của chúng tôi cho thử thách AutoML thứ hai và việc xuất bản các phương pháp được sử dụng (Feurer et al., 2018). Pösterer et al. (2018) đề xuất sử dụng một tập hợp các giá trị mặc định để đơn giản hóa tối ưu hóa siêu tham số. Họ lập luận rằng việc xây dựng một portfolio tối ưu các thiết lập siêu tham số là một tổng quát hóa của vấn đề Maximum coverage và đề xuất hai giải pháp dựa trên Mixed Integer Programming và Thuật toán Tham lam mà chúng tôi cũng sử dụng làm cơ sở cho thuật toán của chúng tôi. Thuật toán tham lam gần đây cũng thu hút sự quan tâm trong nghiên cứu deep learning, nơi nó được áp dụng ở dạng cơ bản cho việc điều chỉnh các siêu tham số của thuật toán ADAM phổ biến (Metz et al., 2020).

Mở rộng các chiến lược portfolio này, được học ngoại tuyến, có các portfolio trực tuyến có thể chọn từ một tập hợp cố định các pipeline machine learning, có tính đến các đánh giá trước đó (Leite et al., 2012; Wistuba et al., 2015a,b; Fusi et al., 2018; Yang et al., 2019, 2020). Tuy nhiên, những phương pháp như vậy không thể được kết hợp trực tiếp với tất cả các chiến lược phân bổ ngân sách vì chúng yêu cầu định nghĩa một mô hình đặc biệt để ngoại suy các đường cong học tập (Klein et al., 2017b; Falkner et al., 2018) và cũng giới thiệu độ phức tạp bổ sung vào các hệ thống AutoML.

Tồn tại công trình khác về xây dựng portfolio mà không có discretization trước (mà chúng tôi thực hiện cho công trình của chúng tôi và được thực hiện cho hầu hết công trình đã đề cập ở trên), trực tiếp tối ưu hóa các siêu tham số của các pipeline ML để thêm tiếp theo vào portfolio theo kiểu tham lam (Xu et al., 2010, 2011; Seipp et al., 2015), để tối ưu hóa đồng thời tất cả các cấu hình của portfolio với tối ưu hóa toàn cục (Winkelmolen et al., 2020), và cũng để xây dựng portfolio song song (Lindauer et al., 2017). Chúng tôi coi những điều này trực giao với việc sử dụng portfolio ngay từ đầu và dự định nghiên cứu các chiến lược tối ưu hóa được cải thiện trong công việc tương lai.

--- TRANG 28 ---
Auto-sklearn 2.0

6.2 Công trình Liên quan về Successive Halving

Các tập dữ liệu lớn, các pipeline ML đắt tiền và các hạn chế tài nguyên chặt chẽ đòi hỏi các phương pháp tinh vi để tăng tốc lựa chọn pipeline. Một hướng nghiên cứu, các phương pháp tối ưu hóa đa fidelity, giải quyết vấn đề này bằng cách sử dụng các xấp xỉ rẻ hơn của mục tiêu quan tâm. Các ví dụ thực tế là đánh giá một pipeline chỉ trên một tập con của tập dữ liệu hoặc đối với các thuật toán lặp giới hạn số lần lặp. Tồn tại một lượng lớn nghiên cứu về các phương pháp tối ưu hóa tận dụng các fidelity thấp hơn, ví dụ làm việc với một tập hợp cố định các nhiệm vụ phụ trợ (Forrester et al., 2007; Swersky et al., 2013; Poloczek et al., 2017; Moss et al., 2020), các giải pháp cho các lớp mô hình cụ thể (Swersky et al., 2014; Domhan et al., 2015; Chandrashekaran và Lane, 2017) và chọn một giá trị fidelity từ một phạm vi liên tục (Klein et al., 2017a; Kandasamy et al., 2017; Wu et al., 2020; Takeno et al., 2020).

Ở đây, chúng tôi tập trung vào một chiến lược bandit đơn giản về mặt phương pháp, SH (Karnin et al., 2013; Jamieson và Talwalkar, 2016), liên tiếp giảm số lượng ứng viên và đồng thời tăng tài nguyên được phân bổ cho mỗi lần chạy cho đến khi chỉ còn lại một ứng viên. Việc sử dụng SH của chúng tôi trong thử thách AutoML thứ 2 cũng truyền cảm hứng cho công trình kết hợp thuật toán di truyền với SH (Parmentier et al., 2019). Một cách khác để nhanh chóng loại bỏ các pipeline không hứa hẹn là quy trình intensify được sử dụng bởi Auto-WEKA (Thornton et al., 2013) để tăng tốc cross-validation. Thay vì đánh giá tất cả các fold cùng một lúc, nó đánh giá các fold theo kiểu lặp. Sau mỗi đánh giá, hiệu suất trung bình trên các fold đã đánh giá được so sánh với hiệu suất của pipeline tốt nhất cho đến nay trên những fold này. Việc đánh giá chỉ được tiếp tục nếu hiệu suất bằng hoặc tốt hơn. Trong khi điều này cho phép đánh giá nhiều cấu hình trong thời gian ngắn, nó không thể được kết hợp với ensembling post-hoc và giảm chi phí của một pipeline đến, nhiều nhất, chi phí của holdout, có thể vẫn quá cao.

6.3 Công trình Liên quan về Lựa chọn Thuật toán

Tự động chọn một chiến lược lựa chọn mô hình để đánh giá hiệu suất của một pipeline ML cho tối ưu hóa siêu tham số chưa được giải quyết trước đây, và chỉ Guyon et al. (2015) thừa nhận sự thiếu của một phương pháp như vậy. Tuy nhiên, coi việc lựa chọn chiến lược lựa chọn mô hình như một vấn đề lựa chọn thuật toán cho phép chúng tôi áp dụng các phương pháp từ lĩnh vực lựa chọn thuật toán (Smith-Miles, 2008; Kotthoff, 2014; Kerschke et al., 2019) và chúng tôi có thể trong công việc tương lai tái sử dụng các kỹ thuật hiện có bên cạnh phân loại cặp đôi mà chúng tôi sử dụng trong bài báo này (Xu et al., 2011), chẳng hạn như hệ thống AutoAI AutoFolio (Lindauer et al., 2015).

6.4 Nền tảng về Hệ thống AutoML và Các Thành phần của chúng

Các hệ thống AutoML gần đây đã thu hút sự chú ý trong cộng đồng nghiên cứu, và tồn tại nhiều phương pháp, thường được đi kèm với phần mềm mã nguồn mở. Trong phần sau, chúng tôi cung cấp nền tảng về các thành phần chính của các framework AutoML trước khi mô tả một số instantiation nổi bật chi tiết hơn.

6.4.1 Thành phần của các hệ thống AutoML

Các hệ thống AutoML yêu cầu một không gian cấu hình pipeline linh hoạt và được điều khiển bởi một phương pháp hiệu quả để tìm kiếm không gian này. Hơn nữa, chúng dựa vào các chiến lược lựa chọn mô hình và phân bổ ngân sách khi đánh giá các pipeline khác nhau. Ngoài ra, để tăng tốc quy trình tìm kiếm, thông tin thu được trên các tập dữ liệu khác có thể được sử dụng để khởi động hoặc hướng dẫn quy trình tìm kiếm (tức là meta-learning). Cuối cùng, người ta cũng có thể kết hợp các mô hình được huấn luyện trong giai đoạn tìm kiếm trong một bước ensembling post-hoc.

Không gian Cấu hình và Cơ chế Tìm kiếm Trong khi có các công thức không gian cấu hình cho phép áp dụng nhiều cơ chế tìm kiếm, không phải tất cả các công thức của một không gian cấu hình và một cơ chế tìm kiếm có thể được trộn và phối hợp, và do đó chúng tôi mô tả các công thức khác nhau và các cơ chế tìm kiếm có thể áp dụng lần lượt.

Mô tả phổ biến nhất của không gian tìm kiếm là công thức CASH. Có một lượng cố định các siêu tham số, mỗi cái với một phạm vi các giá trị hợp pháp hoặc lựa chọn phân loại, và một số trong số chúng có thể có điều kiện, có nghĩa là chúng chỉ hoạt động nếu các siêu tham số khác thỏa mãn các điều kiện nhất định. Một ví dụ như vậy là việc lựa chọn một thuật toán phân loại và các siêu tham số của nó. Các siêu tham số của một SVM chỉ hoạt động nếu siêu tham số phân loại của thuật toán phân loại được đặt thành SVM.

Các thuật toán tối ưu hóa black-box tiêu chuẩn có thể giải quyết vấn đề CASH, và SMAC (Hutter et al., 2011) và TPE (Bergstra et al., 2011) được đề xuất đầu tiên cho nhiệm vụ này. Những người khác đề xuất việc sử dụng thuật toán tiến hóa (Bürger và Pauli, 2015) và tìm kiếm ngẫu nhiên (LeDell và Poirier, 2020). Nó cũng được biết đến như vấn đề lựa chọn mô hình đầy đủ (Escalante et al., 2009), và các giải pháp trong hướng nghiên cứu đó đề xuất việc sử dụng tối ưu hóa bầy đàn hạt (Escalante et al., 2009) và một sự kết hợp của thuật toán di truyền với tối ưu hóa bầy đàn hạt (Sun et al., 2013). Để cải thiện hiệu suất, người ta có thể pruning không gian cấu hình để giảm kích thước của không gian mà thuật toán tối ưu hóa phải tìm kiếm (Zhang et al., 2016), chia không gian cấu hình thành các không gian con nhỏ hơn, dễ quản lý hơn (Alaa và van der Schaar, 2018; Liu et al., 2020), hoặc sử dụng kiến thức chuyên gia mạnh mẽ (LeDell và Poirier, 2020).

Thay vì một không gian cấu hình cố định, lập trình di truyền có thể sử dụng một không gian linh hoạt và có thể vô hạn của các thành phần được kết nối (Olson et al., 2016b,a). Phương pháp này có thể được chính thức hóa thêm bằng cách sử dụng lập trình di truyền dựa trên ngữ pháp (de Sa et al., 2017). Ngữ pháp không ngữ cảnh cũng có thể được tìm kiếm bởi các thuật toán học tăng cường dựa trên mô hình (Drori et al., 2019).

Việc chính thức hóa vấn đề tìm kiếm như một cây tìm kiếm cho phép áp dụng tìm kiếm cây Monte-Carlo tùy chỉnh (Rakotoarison et al., 2019) và mạng nhiệm vụ phân cấp với tìm kiếm tốt nhất đầu tiên (Mohr et al., 2018). Với các không gian rời rạc, cũng có thể sử dụng các kết hợp của meta-learning và phân tích ma trận (Yang et al., 2019, 2020; Fusi et al., 2018). Trong trường hợp đặc biệt chỉ sử dụng mạng neural trong một hệ thống AutoML, có thể gắn bó với tối ưu hóa black-box tiêu chuẩn (Mendoza et al., 2016, 2019; Zimmer et al., 2021), nhưng người ta cũng có thể sử dụng các tiến bộ gần đây trong tìm kiếm kiến trúc neural (Elsken et al., 2019).

Meta-Learning. Khi có kiến thức về các lần chạy trước đó của hệ thống AutoML trên các tập dữ liệu khác, có thể sử dụng meta-learning. Một tùy chọn là định nghĩa một thước đo tương tự tập dữ liệu, thường bằng cách sử dụng các meta-feature được chế tạo thủ công mô tả các tập dữ liệu (Brazdil et al., 1994), để sử dụng các giải pháp tốt nhất trên các tập dữ liệu gần nhất đã thấy để khởi động nóng thuật toán tìm kiếm (Feurer et al., 2015a). Trong khi cách meta-learning này có thể được coi như một add-on cho các phương pháp hiện có, các công trình khác sử dụng các chiến lược tìm kiếm được thiết kế để tính đến meta-learning, ví dụ phân tích ma trận (Yang et al., 2019, 2020; Fusi et al., 2018) hoặc học tăng cường (Drori et al., 2019; Heetz et al., 2020).

Lựa chọn Mô hình. Với dữ liệu huấn luyện, mục tiêu của một hệ thống AutoML là tìm pipeline ML hoạt động tốt nhất. Việc làm như vậy yêu cầu xấp xỉ tốt nhất lỗi tổng quát để 1) cung cấp một tín hiệu đáng tin cậy và chính xác cho quy trình tối ưu hóa8 và 2) chọn mô hình để được trả về cuối cùng. Thông thường, lỗi tổng quát được đánh giá thông qua giao thức train-validation-test (Bishop, 1995; Raschka, 2018). Điều này có nghĩa là một số mô hình được huấn luyện trên một tập huấn luyện, tốt nhất được chọn thông qua holdout (sử dụng một phân chia duy nhất) hoặc K-fold cross-validation, và lỗi tổng quát sau đó được báo cáo trên tập test. Hệ thống AutoML sau đó trả về một mô hình duy nhất trong trường hợp holdout và một sự kết hợp của K mô hình trong trường hợp K-fold cross-validation (Caruana et al., 2006). Người ta cũng có thể sử dụng các chiến lược lựa chọn mô hình nhằm giảm hiệu ứng của overfitting đối với tập validation (Dwork et al., 2015; Tsamardinos et al., 2018), nhưng trong khi các chiến lược lựa chọn mô hình như vậy là một lĩnh vực nghiên cứu quan trọng, holdout hoặc K-fold cross-validation vẫn là những lựa chọn nổi bật nhất (Henery, 1994; Kohavi và John, 1995; Hastie et al., 2001; Guyon et al., 2010; Bischl et al., 2012; Raschka, 2018).

Ảnh hưởng của chiến lược lựa chọn mô hình đối với hiệu suất được biết đến rõ (Kalousis và Hilario, 2003), và các nhà nghiên cứu đã nghiên cứu tác động của chúng (Kohavi, 1995). Tuy nhiên, không có chiến lược tốt nhất duy nhất, vì có một sự đánh đổi giữa chất lượng xấp xỉ và thời gian cần thiết để tính toán validation loss.

Ensembling Post-hoc. Các hệ thống AutoML đánh giá hàng chục hoặc hàng trăm mô hình trong quy trình tối ưu hóa của chúng. Do đó, đó là một bước tiếp theo tự nhiên để không chỉ sử dụng một mô hình duy nhất ở cuối mà ensemble nhiều mô hình để cải thiện hiệu suất và giảm overfitting.

8. Các chiến lược lựa chọn mô hình khác nhau có thể bị bỏ qua từ góc độ tối ưu hóa, nơi mục tiêu là tối ưu hóa hiệu suất với một hàm loss, như thường được thực hiện trong các lĩnh vực nghiên cứu meta-learning và tối ưu hóa siêu tham số. Tuy nhiên, đối với các hệ thống AutoML, điều này rất liên quan vì chúng tôi không quan tâm đến hiệu suất tối ưu hóa (của một phần phụ nào đó) của các hệ thống này, mà hiệu suất tổng quát ước lượng cuối cùng khi áp dụng cho dữ liệu mới.

--- TRANG 29 ---
Auto-sklearn 2.0

Điều này lần đầu tiên được đề xuất để kết hợp các giải pháp được tìm thấy bởi tối ưu hóa bầy đàn hạt (Escalante et al., 2010) và sau đó bởi một thuật toán tiến hóa (Bürger và Pauli, 2015). Trong khi những công trình này sử dụng các phương pháp heuristic để kết hợp nhiều mô hình thành một ensemble cuối cùng, cũng có thể coi điều này như một vấn đề tối ưu hóa khác (Feurer et al., 2015a) và giải quyết nó với lựa chọn ensemble (Caruana et al., 2004) hoặc stacking (LeDell và Poirier, 2020).

Thay vì sử dụng một lớp duy nhất của các mô hình machine learning, Automatic Frankensteining (Wistuba et al., 2017) đề xuất stacking hai lớp, áp dụng AutoML cho các đầu ra của một hệ thống AutoML thay vì một lớp duy nhất của các thuật toán ML theo sau bởi một cơ chế ensembling. Auto-Stacker đi thêm một bước, trực tiếp tối ưu hóa cho một hệ thống AutoML hai lớp (Chen et al., 2018).

6.4.2 Hệ thống AutoML

Theo hiểu biết tốt nhất của chúng tôi, hệ thống AutoML đầu tiên điều chỉnh cả siêu tham số và chọn thuật toán là một phương pháp ensemble (Caruana et al., 2004). Hệ thống này ngẫu nhiên tạo ra 2 000 bộ phân loại từ một loạt rộng các thuật toán ML và xây dựng một ensemble post-hoc. Nó sau đó được robustified (Caruana et al., 2006) và được sử dụng trong một bài nộp thắng cuộc cho thử thách KDD (Niculescu-Mizil et al., 2009).

Hệ thống AutoML đầu tiên tối ưu hóa đồng thời toàn bộ pipeline là Particle Swarm Model Selection (Escalante et al., 2007, 2009). Nó sử dụng một biểu diễn chiều dài cố định của pipeline và chứa lựa chọn feature, xử lý feature, phân loại và xử lý hậu được triển khai trong gói CLOP9 và được phát triển cho thử thách agnostic learning vs. prior knowledge IJCNN 2007 (Guyon et al., 2007). Nó đứng thứ 2 trong số các giải pháp sử dụng gói CLOP do ban tổ chức cung cấp, chỉ thua một bài nộp dựa trên tối ưu hóa siêu tham số mạnh mẽ và ensembling (Reunanen, 2007).

Các hệ thống sau đó bắt đầu sử dụng các thuật toán tối ưu hóa toàn cục dựa trên mô hình, chẳng hạn như Auto-WEKA (Thornton et al., 2013; Kotthoff et al., 2019), được xây dựng xung quanh phần mềm WEKA (Hall et al., 2009) và SMAC (Hutter et al., 2011) và sử dụng cross-validation với racing để đánh giá mô hình, và Hyperopt-sklearn (Komer et al., 2014), là công cụ đầu tiên sử dụng scikit-learn hiện nay phổ biến (Pedregosa et al., 2011) và kết hợp nó với thuật toán TPE từ gói hyperopt (Bergstra et al., 2011, 2013) và holdout.

Chúng tôi mở rộng phương pháp tham số hóa một thư viện machine learning phổ biến và tối ưu hóa các siêu tham số của nó với một thuật toán tối ưu hóa black-box sử dụng meta-learning và ensemble post-hoc trong Auto-sklearn (Feurer et al., 2015a, 2019). Đối với phân loại, không gian của các pipeline ML có thể hiện tại bao trùm 16 bộ phân loại, 14 phương pháp tiền xử lý feature và nhiều phương pháp tiền xử lý dữ liệu, cộng lại đến 122 siêu tham số cho bản phát hành mới nhất. Auto-sklearn sử dụng holdout như một chiến lược lựa chọn mô hình mặc định nhưng cho phép các chiến lược khác như cross-validation. Auto-sklearn là giải pháp thống trị của thử thách AutoML đầu tiên (Guyon et al., 2019).

Công cụ tối ưu hóa pipeline dựa trên cây (TPOT; Olson et al., 2016b; Olson và Moore, 2019) sử dụng tiến hóa ngữ pháp để xây dựng các pipeline ML có chiều dài tùy ý. Hiện tại, nó sử dụng scikit-learn (Pedregosa et al., 2011) và XGBoost (Chen và Guestrin, 2016) cho các khối xây dựng ML của mình và 5-fold cross-validation để đánh giá các giải pháp cá nhân. TPOT-SH (Parmentier et al., 2019), được truyền cảm hứng bởi bài nộp của chúng tôi cho thử thách AutoML thứ hai, sử dụng successive halving để tăng tốc TPOT trên các tập dữ liệu lớn.

Cũng có nhiều hệ thống AutoML khai thác stacking (Wolpert, 1992). Đầu tiên, Automatic Frankensteining (Wistuba et al., 2017) giới thiệu một quy trình tối ưu hóa hai giai đoạn để xây dựng một mô hình stacking hai lớp. Thứ hai, AutoStacker trực tiếp tối ưu hóa một mô hình stacking hai lớp với một thuật toán di truyền (Chen et al., 2018). Thứ ba, gói H2O AutoML xây dựng trên một tập hợp các mặc định được thiết kế thủ công và tìm kiếm ngẫu nhiên và kết hợp chúng trong một bước stacking post-hoc, sử dụng các khối xây dựng từ thư viện H2O (H2O.ai, 2020) và XGBoost (Chen và Guestrin, 2016), và sử dụng cross-validation. Cuối cùng, AutoGluon có một phương pháp hoàn toàn khác và hoàn toàn bỏ tối ưu hóa siêu tham số và đầu tư tất cả thời gian có sẵn vào việc xây dựng một mô hình stacking mạnh mẽ (Erickson et al., 2020).

Gần đây, cũng có các công trình nhằm tạo ra các hệ thống AutoML có thể tận dụng các tiến bộ gần đây trong deep learning, sử dụng tối ưu hóa black-box (Mendoza et al., 2016; Zimmer et al., 2021) hoặc tìm kiếm kiến trúc neural (Jin et al., 2019).

Tất nhiên, cũng có nhiều kỹ thuật liên quan đến AutoML không được sử dụng trong một trong các hệ thống AutoML được thảo luận trong phần này, và chúng tôi tham khảo Hutter et al. (2019) để có tổng quan về lĩnh vực Automated Machine Learning, Brazdil et al. (2008) để có tổng quan về nghiên cứu meta-learning có trước công trình về AutoML và Escalante (2021) để thảo luận về lịch sử của AutoML.

7. Thảo luận và Kết luận

Trong bài báo này, chúng tôi giới thiệu bài nộp thắng cuộc của chúng tôi cho thử thách ChaLearn AutoML thứ 2, PoSH Auto-sklearn, và tự động hóa thêm các thiết lập nội bộ của nó, dẫn đến thế hệ tiếp theo của hệ thống AutoML của chúng tôi: Auto-sklearn 2.0. Nó cung cấp một giải pháp thực sự tự động, mà, với một nhiệm vụ mới và các hạn chế tài nguyên, tự động chọn thiết lập tốt nhất. Cụ thể, chúng tôi giới thiệu ba cải tiến cho AutoML nhanh hơn và hiệu quả hơn: (i) để có được kết quả mạnh mẽ nhanh chóng, chúng tôi đề xuất sử dụng portfolio, có thể được xây dựng ngoại tuyến và do đó giảm chi phí khởi động, (ii) để giảm thời gian dành cho các pipeline hoạt động kém, chúng tôi đề xuất thêm successive halving như một chiến lược phân bổ ngân sách vào không gian cấu hình của hệ thống AutoML của chúng tôi và (iii) để đóng không gian thiết kế mà chúng tôi mở ra cho AutoML, chúng tôi đề xuất tự động chọn cấu hình tốt nhất của hệ thống của chúng tôi.

Chúng tôi tiến hành một nghiên cứu quy mô lớn dựa trên 208 meta-datasets để xây dựng các hệ thống AutoML của chúng tôi và 39 datasets để đánh giá chúng và có được hiệu suất được cải thiện đáng kể so với Auto-sklearn 1.0, giảm ADTM lên đến hệ số 4:5 và đạt được loss thấp hơn sau 10 phút so với Auto-sklearn 1.0 sau 60 phút. Nghiên cứu ablation của chúng tôi cho thấy rằng việc sử dụng bộ chọn chính sách dựa trên mô hình để chọn chiến lược lựa chọn mô hình có tác động lớn nhất đến hiệu suất và cho phép Auto-sklearn 2.0 chạy một cách mạnh mẽ trên các tập dữ liệu mới, chưa thấy. Hơn nữa, chúng tôi cho thấy rằng phương pháp của chúng tôi có tính cạnh tranh cao và vượt trội hơn các hệ thống AutoML hiện đại khác trong AutoML benchmark OpenML.

Tuy nhiên, hệ thống của chúng tôi cũng giới thiệu một số hạn chế vì nó tối ưu hóa hiệu suất hướng tới một ngân sách tối ưu hóa, thước đo hiệu suất và không gian cấu hình đã cho. Mặc dù tất cả những điều này, cùng với các meta datasets, có thể được cung cấp bởi người dùng để tự động xây dựng một phiên bản tùy chỉnh của Auto-sklearn 2.0, sẽ thú vị nếu chúng ta có thể học cách chuyển một hệ thống AutoML cụ thể sang các ngân sách tối ưu hóa và thước đo khác nhau.

Mặc dù chúng tôi đã quan sát hiệu suất thực nghiệm mạnh mẽ sử dụng SH, chúng tôi không có bất kỳ đảm bảo hiệu suất nào khi chúng tôi kết hợp SH với BO. Do đó, chúng tôi coi việc phát triển các phương pháp tăng ngân sách thấp hơn của successive halving theo thời gian là các bước tiếp theo hứa hẹn.

Ngoài ra, vẫn còn một số siêu tham số được chọn thủ công ở cấp độ của hệ thống AutoML, mà chúng tôi dự định tự động hóa trong công việc tương lai. Ví dụ, những điều này là tự động học kích thước portfolio, học thêm các siêu-siêu tham số của các chiến lược phân bổ ngân sách khác nhau (ví dụ, của SH) và đề xuất các không gian cấu hình phù hợp với một tập dữ liệu và tài nguyên. Bên cạnh những điều này, việc sử dụng hai meta-feature của chúng tôi cho bộ chọn mở ra câu hỏi nghiên cứu về liệu các meta-feature khác có thể dẫn đến hiệu suất tốt hơn hay không. Chúng tôi mong đợi rằng chúng tôi có thể giải quyết nhiều vấn đề này bằng cách thực hiện một vòng lặp tối ưu hóa bổ sung trên dữ liệu huấn luyện. Cuối cùng, việc xây dựng dữ liệu huấn luyện hiện tại khá đắt đỏ. Mặc dù điều này chỉ phải được thực hiện một lần, sẽ thú vị khi xem liệu chúng ta có thể có các phím tắt ở đây, ví dụ, bằng cách sử dụng một mô hình xếp hạng chung (Tornede et al., 2020) hoặc lọc cộng tác phi tuyến (Fusi et al., 2018).

Lời cảm ơn

Các tác giả ghi nhận sự hỗ trợ của bang Baden-Württemberg thông qua bwHPC và Deutsche Forschungsgemeinschaft (DFG) thông qua grant số INST 39/963-1 FUGG. Công trình này một phần được hỗ trợ bởi European Research Council (ERC) dưới chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh châu Âu dưới grant số 716721. Robert Bosch GmbH được ghi nhận cho sự hỗ trợ tài chính. Chúng tôi hơn nữa cảm ơn tất cả những người đóng góp cho Auto-sklearn vì sự giúp đỡ của họ trong việc làm cho nó trở thành một công cụ AutoML hữu ích và cũng cảm ơn Francisco Rivera vì đã cung cấp tích hợp Singularity cho AutoML benchmark.

--- TRANG 34 ---
Auto-sklearn 2.0

[Phần còn lại của tài liệu chứa các phụ lục kỹ thuật, mã giả, kết quả thực nghiệm bổ sung, chi tiết triển khai, và danh sách tham khảo. Do độ dài của tài liệu, tôi sẽ tiếp tục dịch nếu bạn yêu cầu cụ thể phần nào.]
