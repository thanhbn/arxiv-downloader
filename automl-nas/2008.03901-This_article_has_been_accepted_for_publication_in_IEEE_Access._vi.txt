# 2008.03901.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2008.03901.pdf
# Kích thước tệp: 2987168 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bài báo này đã được chấp nhận để xuất bản trên IEEE Access.
Mã định danh đối tượng số 10.1109/ACCESS.2022.3185095
RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả
FANGHUI XUE1, YINGYONG QI1, VÀ JACK XIN.1
1Khoa Toán học, Đại học California tại Irvine, Irvine, CA 92697, USA
Tác giả liên hệ: Fanghui Xue (e-mail: fanghuix@uci.edu).
Công trình được hỗ trợ một phần bởi các tài trợ NSF DMS-1854434, DMS-1952644, và Giải thưởng Khoa của Qualcomm.
TÓM TẮT Tìm kiếm kiến trúc có thể vi phân (DARTS) là một phương pháp hiệu quả để thiết kế mạng nơ-ron dựa trên dữ liệu dựa trên việc giải quyết một bài toán tối ưu hóa hai cấp. Mặc dù thành công trong nhiều nhiệm vụ tìm kiếm kiến trúc, vẫn còn một số mối quan ngại về độ chính xác của DARTS bậc nhất và hiệu quả của DARTS bậc hai. Trong bài báo này, chúng tôi xây dựng một giải pháp thay thế một cấp và một phương pháp tìm kiếm kiến trúc thư giãn (RARTS) sử dụng toàn bộ tập dữ liệu trong việc học kiến trúc thông qua cả việc phân chia dữ liệu và mạng, mà không liên quan đến các đạo hàm bậc hai hỗn hợp của các hàm mất mát tương ứng như DARTS. Trong công thức phân chia mạng của chúng tôi, hai mạng với các trọng số khác nhau nhưng có liên quan hợp tác tìm kiếm một kiến trúc được chia sẻ. Ưu thế của RARTS so với DARTS được chứng minh bởi một định lý hội tụ và một mô hình có thể giải quyết được bằng phân tích. Hơn nữa, RARTS vượt trội hơn DARTS và các biến thể của nó về độ chính xác và hiệu quả tìm kiếm, như được thể hiện trong các kết quả thực nghiệm đầy đủ. Đối với nhiệm vụ tìm kiếm kiến trúc tôpô, tức là các cạnh và các phép toán, RARTS đạt được độ chính xác cao hơn và giảm 60% chi phí tính toán so với DARTS bậc hai trên CIFAR-10. RARTS tiếp tục vượt trội hơn DARTS khi chuyển sang ImageNet và ngang hàng với các biến thể gần đây của DARTS mặc dù đổi mới của chúng tôi hoàn toàn về thuật toán huấn luyện mà không sửa đổi không gian tìm kiếm. Đối với nhiệm vụ tìm kiếm chiều rộng, tức là số lượng kênh trong các lớp tích chập, RARTS cũng vượt trội hơn các điểm chuẩn cắt tỉa mạng truyền thống. Các thí nghiệm tiếp theo trên điểm chuẩn tìm kiếm kiến trúc công khai như NATS-Bench cũng hỗ trợ sự vượt trội của RARTS.
THUẬT NGỮ CHỈ MỤC Mạng nơ-ron tích chập, tìm kiếm kiến trúc nơ-ron, tìm kiếm kiến trúc có thể vi phân, nén mạng.

I. GIỚI THIỆU
Tìm Kiếm Kiến Trúc Nơ-ron (NAS) là một kỹ thuật học máy tự động để thiết kế một kiến trúc mạng nơ-ron tối ưu bằng cách tìm kiếm các khối xây dựng của mạng nơ-ron sâu từ một tập hợp các cấu trúc và phép toán ứng viên. Mặc dù NAS đã đạt được nhiều thành công trong một số nhiệm vụ thị giác máy tính [1]–[6], quá trình tìm kiếm đòi hỏi tài nguyên tính toán khổng lồ. Thời gian tìm kiếm hiện tại đã giảm đáng kể từ lên đến 2000 ngày GPU trong NAS sớm [2], nhờ vào các nghiên cứu tiếp theo [7]–[13] cùng với những nghiên cứu khác. Tìm Kiếm Kiến Trúc Có thể Vi phân (DARTS) [14] là một phương pháp hấp dẫn tránh tìm kiếm trên tất cả các kết hợp có thể bằng cách thư giãn các chỉ số kiến trúc phân loại thành các tham số liên tục. Kiến trúc cấp cao hơn có thể được học cùng với các trọng số cấp thấp hơn thông qua giảm độ dốc ngẫu nhiên bằng cách giải quyết gần đúng một bài toán tối ưu hóa hai cấp. DARTS có thể được phân loại thêm thành DARTS bậc nhất và DARTS bậc hai, phù hợp với việc có sử dụng ước tính đạo hàm bậc hai hỗn hợp của hàm mất mát hay không.

Mặc dù hiệu quả tìm kiếm đạt được từ việc thư giãn liên tục, DARTS vẫn có thể có một số vấn đề về mặt thực nghiệm và lý thuyết. Đó là vấn đề hiệu quả với DARTS bậc hai, vấn đề hội tụ với DARTS bậc nhất, và vấn đề sụp đổ kiến trúc (tức là kiến trúc được chọn chứa quá nhiều kết nối bỏ qua) với cả hai DARTS. DARTS bậc hai mất nhiều thời gian tìm kiếm hơn DARTS bậc nhất vì nó liên quan đến các đạo hàm bậc hai hỗn hợp. Cũng đã được chỉ ra rằng DARTS bậc hai có thể có hiệu ứng chồng chất [15], có nghĩa là việc xấp xỉ gradient của dựa trên việc xấp xỉ trọng số w một bước phía trước. Điều này được cho là gây ra lỗi gradient và thất bại trong việc tìm kiếm các kiến trúc tối ưu. Do đó, nó được sử dụng ít thường xuyên hơn trong thực tế so với
VOLUME 4, 2016 1arXiv:2008.03901v2  [cs.LG]  24 Jun 2022

--- TRANG 2 ---
F . Xue et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả
DARTS bậc nhất [15], [16]. Tuy nhiên, DARTS bậc nhất chỉ học kiến trúc bằng một nửa dữ liệu. Các bằng chứng được cung cấp để chỉ ra rằng nó có thể dẫn đến các giới hạn không chính xác và hiệu suất kém hơn [14]. Các kết quả thực nghiệm cũng cho thấy rằng DARTS bậc nhất (3,00% lỗi) kém chính xác hơn DARTS bậc hai (2,76% lỗi) trên tập dữ liệu CIFAR-10 [14], [17]. Đối với vấn đề sụp đổ kiến trúc, thường thì sự thiên vị trong việc lựa chọn phép toán này làm giảm hiệu suất mô hình. Vấn đề này đã được quan sát bởi một vài nhà nghiên cứu [18], [19], những người đã cố gắng giải quyết nó bằng cách thay thế một số phép toán của kiến trúc.

Ngoài việc tìm kiếm các kiến trúc tôpô, tức là các cạnh và phép toán của các tế bào (khối xây dựng) trong một số công trình NAS sớm và DARTS [2], [14], nhiều phương pháp kiểu NAS đã được phát triển để tìm kiếm chiều rộng của một mô hình, tức là số lượng kênh trong các lớp tích chập [16], [20]. Tìm kiếm chiều rộng được cho là một cách cắt tỉa kênh, đây là một công cụ phổ biến cho nén mạng, tức là xây dựng các mạng mỏng từ những mạng dư thừa [21]. Cụ thể, cắt tỉa kênh có thể được công thức hóa như một bài toán tìm kiếm kiến trúc, thông qua việc thiết lập các tham số tính điểm kênh có thể học [21]–[23] như các tham số kiến trúc. Đây là một cách tiếp cận thanh lịch cho việc nén mà không dựa vào độ lớn kênh (chuẩn `1 nhóm), được sử dụng trong các phương pháp chính quy hóa trước đây [24]. Cách thiết lập các tham số tính điểm kênh trước đây [21] sử dụng các tham số tỷ lệ của các lớp chuẩn hóa batch, tuy nhiên chúng không được chứa trong nhiều mạng hiện đại [25], [26]. Một thách thức khác vẫn cần được giải quyết là thay thế việc giảm gradient thông thường bằng các thuật toán kiểu DARTS chính xác hơn.

Ngoài công thức hai cấp của DARTS, một cách tiếp cận một cấp (SNAS) dựa trên một hàm mất mát có thể vi phân và lấy mẫu đã được đề xuất [27]. Trên CIFAR-10, SNAS chính xác hơn DARTS bậc nhất nhưng với thời gian tìm kiếm nhiều hơn 50% so với DARTS bậc hai. Điều này thúc đẩy chúng tôi xây dựng một phương pháp một cấp mới hiệu quả và chính xác hơn. Đóng góp chính của chúng tôi là giới thiệu một phương pháp Tìm Kiếm Kiến Trúc Thư Giãn (RARTS) mới dựa trên tối ưu hóa một cấp, và việc tính toán chỉ các đạo hàm riêng bậc nhất của các hàm mất mát, cho cả tìm kiếm tôpô và chiều rộng của kiến trúc. Thông qua cả việc phân chia dữ liệu và mạng, mục tiêu huấn luyện (một hàm Lagrangian thư giãn) của RARTS cho phép hai mạng với các trọng số khác nhau nhưng có liên quan hợp tác trong việc tìm kiếm một kiến trúc được chia sẻ.

Chúng tôi đã thực hiện cả các nghiên cứu phân tích và thực nghiệm dưới đây để chỉ ra rằng RARTS đạt được hiệu suất tốt hơn so với DARTS bậc nhất và bậc hai, với hiệu quả tìm kiếm cao hơn DARTS bậc hai một cách nhất quán:
So sánh RARTS với DARTS trực tiếp trên mô hình phân tích với các hàm mất mát bậc hai, nơi các lần lặp RARTS tiến gần đến điểm tối thiểu toàn cục thực bị bỏ lỡ bởi DARTS bậc nhất, theo cách mạnh mẽ. Một định lý hội tụ được chứng minh cho RARTS dựa trên sự giảm của hàm Lagrangian của nó, và các phương trình cân bằng được khám phá cho các giới hạn.

Trên việc tìm kiếm kiến trúc tôpô dựa trên CIFAR-10, mô hình được tìm thấy bởi RARTS đạt được kích thước nhỏ hơn và độ chính xác kiểm tra cao hơn so với DARTS bậc hai với việc tiết kiệm 65% thời gian tìm kiếm. Một tùy chọn tìm kiếm nhận thức phần cứng thông qua một phạt độ trễ trong hàm Lagrangian giúp kiểm soát kích thước mô hình. Khi chuyển sang ImageNet [28], [29], mô hình được tìm thấy bởi RARTS cũng đạt được hiệu suất tốt hơn, so với DARTS và các biến thể của nó. Ngoài không gian tìm kiếm tiêu chuẩn được sử dụng trong bài báo DARTS, RARTS cũng đánh bại DARTS trên điểm chuẩn NAS công khai của các không gian tìm kiếm như NATS-Bench [30].

Đối với việc cắt tỉa kênh của ResNet-164 [31] trên CIFAR-10 và CIFAR-100 [17] với tỷ lệ cắt tỉa cố định (phần trăm kênh bị cắt tỉa), RARTS vượt trội hơn các điểm chuẩn cắt tỉa có thể vi phân: Network Slimming [21] và TAS [20]. Các so sánh giữa DARTS và RARTS cũng đã được thực hiện trong một nhiệm vụ cắt tỉa được chính quy hóa `1 (tỷ lệ không cố định), nơi RARTS đạt được độ thưa cao 70% và vượt trội DARTS về độ chính xác.

II. CÔNG TRÌNH LIÊN QUAN
A. TÌM KIẾM KIẾN TRÚC CÓ THỂ VI PHÂN
Huấn luyện DARTS dựa vào một thuật toán lặp để giải quyết một bài toán tối ưu hóa hai cấp [14], [32] liên quan đến hai hàm mất mát được tính toán thông qua việc phân chia dữ liệu (chia tập dữ liệu thành hai nửa, tức là dữ liệu huấn luyện và dữ liệu xác thực):
min
Lval(w(α);α);
trong đó w(α) = arg min
wLtrain(w;α):(1)
Ở đây w biểu thị các trọng số mạng, α là tham số kiến trúc, Ltrain và Lval là các hàm mất mát được tính toán trên dữ liệu huấn luyện Dtrain và dữ liệu xác thực Dval.
Vì nhiều tập dữ liệu phổ biến như CIFAR không bao gồm dữ liệu xác thực, Dtrain và Dval thường là hai nửa không chồng lấp của dữ liệu huấn luyện gốc. Chúng tôi ký hiệu Ltrain và Lval bằng Lt và Lv để tránh nhầm lẫn với ý nghĩa của các chỉ số dưới. Dt và Dv được định nghĩa tương tự. DARTS đã áp dụng việc phân chia dữ liệu vì người ta tin rằng việc huấn luyện kết hợp cả α và w thông qua giảm gradient trên toàn bộ tập dữ liệu bằng cách tối thiểu hóa hàm mất mát tổng thể:
L(w;α) =Lt(w;α) +Lv(w;α) (2)
có thể dẫn đến việc quá khớp [14], [15]. Do đó, DARTS tìm kiếm các kiến trúc thông qua một thuật toán có thể vi phân hai bước cập nhật các trọng số mạng và các tham số kiến trúc theo cách luân phiên:
cập nhật trọng số w bằng cách giảm theo ∇wLt(w;α)
cập nhật tham số kiến trúc α bằng cách giảm theo:
∇αLv(w∗−ξ∇wLt(w∗;α);α)
trong đó ξ= 0 (ξ > 0) cho xấp xỉ bậc nhất hoặc bậc hai. Bài toán tối ưu hóa hai cấp cũng xuất hiện trong tối ưu hóa siêu tham số và meta-learning, nơi một thuật toán bậc hai và một định lý hội tụ về các điểm tối thiểu đã được đề xuất trong công trình trước đây [33] (Định lý 3.2), dưới giả định rằng việc tối thiểu hóa α được giải quyết chính xác, và wt(α) hội tụ đều đến w∗(α). Tuy nhiên, việc tối thiểu hóa α của DARTS chỉ được xấp xỉ bằng các phương pháp gradient, và do đó sự hội tụ của thuật toán DARTS vẫn chưa được biết về mặt lý thuyết.

2 VOLUME 4, 2016

--- TRANG 3 ---
Author et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

Chúng tôi nhận thức được sự thật rằng DARTS bậc nhất cập nhật các tham số kiến trúc trên Dv bằng cách giảm theo ∇αLv(w;α), có nghĩa là nó chỉ sử dụng một nửa dữ liệu để huấn luyện α và có thể gây ra một số vấn đề hội tụ (xem Hình 1). MiLeNAS đã phát triển một giải pháp hỗn hợp cấp, nơi các tham số kiến trúc có thể được học trên D=Dt∪Dv thông qua một thuật toán giảm bậc nhất [15]:
wt+1=wt−ηt
w∇w∇wLt(wt;αt)
αt+1=αt−ηt
α

∇αLt(wt+1;αt) +∇αLv(wt+1;αt)
;
(3)
Chúng ta sẽ thấy rằng MiLeNAS thực sự là một trường hợp ràng buộc của RARTS khi hai phân chia mạng của chúng ta trở nên giống hệt nhau. Tuy nhiên, chúng tôi chỉ ra rằng việc tính toán Lv bằng một mạng giống hệt nhau khiến MiLeNAS vẫn gặp phải cùng vấn đề hội tụ trong một ví dụ sau (Phần III-D). DARTS bậc hai được quan sát là xấp xỉ tối ưu tốt hơn DARTS bậc nhất trong một mô hình có thể giải quyết được và thông qua các thí nghiệm, tuy nhiên nó đòi hỏi việc tính toán đạo hàm hỗn hợp ∇2
α,wLt, với chi phí đáng kể. Tìm kiếm bằng DARTS cũng có thể dẫn đến vấn đề sụp đổ kiến trúc, có nghĩa là kiến trúc được chọn chứa quá nhiều kết nối bỏ qua. Thông thường sự thiên vị trong việc lựa chọn phép toán này làm giảm hiệu suất mô hình. SNAS [27], FBNet [12], và GDAS [18] sử dụng Gumbel-Softmax có thể vi phân để mô phỏng mã hóa một-hot ngụ ý cạnh tranh độc quyền và rủi ro của lợi thế không công bằng [19]. Sự thống trị không công bằng này của các kết nối bỏ qua trong DARTS cũng đã được FairDARTS chú ý [19], đã đề xuất một cách tiếp cận cạnh tranh hợp tác bằng cách làm cho các tham số kiến trúc độc lập, thông qua việc thay thế softmax bằng sigmoid. Họ đã tiếp tục phạt các phép toán trong không gian tìm kiếm với xác suất gần bằng 1
2, tức là một lựa chọn trung lập và mơ hồ. Vì các phương pháp này tập trung vào việc thay thế một số phép toán hoặc hàm mất mát, sẽ đáng giá để khám phá các giải pháp khác như thay thế thuật toán tìm kiếm DARTS dựa trên gradient.

Ngoài DARTS, nhiều phương pháp có thể vi phân khác cho tìm kiếm kiến trúc đã được đề xuất, xem xét các khía cạnh khác nhau như không gian tìm kiếm, tiêu chí lựa chọn, và các thủ thuật huấn luyện. SNAS [27] đã thảo luận nó từ một góc độ thống kê với tuy nhiên một cải thiện hiệu suất nhỏ đến vừa phải. Hiệu quả tìm kiếm cũng đã được cải thiện bằng cách lấy mẫu một phần của không gian tìm kiếm trong mỗi lần cập nhật trong huấn luyện. Một sơ đồ lựa chọn dựa trên nhiễu loạn đã được đề xuất trong [34], vì độ lớn của các tham số kiến trúc được cho là không đầy đủ như một tiêu chí lựa chọn. P-DARTS [35] đã áp dụng việc loại bỏ phép toán và chính quy hóa trên các kết nối bỏ qua. Từ phía thủ tục để trì hoãn một tập hợp đường tắt nhanh chóng, nó cũng đã chia giai đoạn tìm kiếm thành nhiều giai đoạn và dần dần thêm độ sâu hơn DARTS. PC-DARTS [36] lấy mẫu một tỷ lệ kênh để giảm thiên vị của việc lựa chọn phép toán và tăng kích thước batch. GDAS [18] tìm kiếm kiến trúc với một phép toán được lấy mẫu tại một thời điểm. Các cách tiếp cận khác áp dụng các phương pháp có thể vi phân trên các không gian tìm kiếm lớn hơn nhiều với các kỹ thuật lấy mẫu để tiết kiệm bộ nhớ và tránh chuyển mô hình [7], [12]. Chúng ta sẽ thấy rằng các biến thể này của phương pháp tìm kiếm kiến trúc có thể vi phân thực sự bổ sung cho cách tiếp cận của chúng tôi tiến bộ DARTS về mặt thuật toán thuần túy bằng cách huy động các trọng số. Hơn nữa, nhiều công trình [7], [12], [16], [37]–[39] quản lý để cân bằng độ trễ với hiệu suất của mô hình để tăng cường hiệu quả của mô hình. Mặc dù việc sử dụng rộng rãi các phương pháp có thể vi phân trong các công trình chúng tôi đã đề cập, người ta có thể thắc mắc làm thế nào DARTS và các biến thể của nó đánh bại tìm kiếm ngẫu nhiên. Một so sánh chi tiết trong [40] đã nêu rõ ưu thế của DARTS về độ chính xác và hiệu quả so với tìm kiếm ngẫu nhiên.

B. TÌM KIẾM CHIỀU RỘNG VÀ CẮT TỈA KÊNH
Phương pháp tìm kiếm có thể vi phân đã đóng góp cho một loạt rộng các nhiệm vụ khác ngoài tìm kiếm kiến trúc tôpô. TAS [20] tìm kiếm chiều rộng của mỗi lớp, tức là số lượng kênh, bằng cách học tối ưu từ việc tập hợp của một số bản đồ đặc trưng ứng viên thông qua một phương pháp có thể vi phân và lấy mẫu. FasterSeg [16] tìm kiếm các phép toán tế bào và chiều rộng lớp, cũng như đường dẫn mạng đa độ phân giải trên nhiệm vụ phân đoạn ngữ nghĩa. Các công trình tìm kiếm chiều rộng này có liên quan chặt chẽ đến việc cắt tỉa kênh, có nghĩa là cắt tỉa các kênh dư thừa từ các lớp tích chập. Trong số nhiều phương pháp để cắt tỉa các kênh dư thừa [24], [41]–[45], một cách tiếp cận cổ điển là áp dụng LASSO nhóm [46] trên các trọng số để xác định các kênh không quan trọng. Các trọng số trong mỗi kênh tạo thành một nhóm, và độ lớn của mỗi nhóm được đo bằng chuẩn `2 của các trọng số của nó. Mạng được huấn luyện bằng cách tối thiểu hóa một hàm mất mát bị phạt bởi chuẩn `1 của các độ lớn này từ tất cả các nhóm. Các kênh được cắt tỉa dựa trên việc ngưỡng hóa các chuẩn của chúng. Việc chọn các ngưỡng tốt như các siêu tham số cho các kênh khác nhau có thể tốn công cho các mạng sâu. Mặt khác, việc lựa chọn kênh về bản chất là một vấn đề kiến trúc mạng. Có thể tranh luận liệu việc ngưỡng hóa bằng độ lớn trọng số có luôn có ý nghĩa hay không [47].

Một cách tiếp cận khác của việc cắt tỉa kênh [21], [22] liên quan đến việc gán một hệ số tỷ lệ kênh (tính điểm) cho mỗi kênh, đây là một tham số có thể học độc lập với các trọng số. Trong quá trình huấn luyện, các hệ số và các trọng số được học kết hợp, và các kênh với hệ số tỷ lệ thấp được cắt tỉa. Sau đó, các trọng số tối ưu của mạng đã cắt tỉa được điều chỉnh bằng một giai đoạn tinh chỉnh nữa. Về mặt các hệ số tỷ lệ kênh, bài toán cắt tỉa kênh trở thành một trường hợp đặc biệt của tìm kiếm kiến trúc nơ-ron. Bên cạnh công thức này, có một số phương pháp cắt tỉa dựa trên NAS. AMC [37] đã định nghĩa một hàm phần thưởng và cắt tỉa các kênh thông qua học tăng cường. MetaPruning [48]

VOLUME 4, 2016 3

--- TRANG 4 ---
F . Xue et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

tạo ra mô hình và trọng số đã cắt tỉa tốt nhất từ một meta mạng.

III. PHƯƠNG PHÁP LUẬN
Trong phần này, chúng tôi giới thiệu công thức RARTS, thuật toán lặp và các tính chất hội tụ của nó. RARTS khác với tất cả các thuật toán có thể vi phân mà chúng tôi đã đề cập, ở chỗ nó đưa ra một công thức thư giãn của một bài toán một cấp có lợi từ cả việc phân chia dữ liệu và phân chia mạng.

A. PHÂN CHIA DỮ LIỆU VÀ PHÂN CHIA MẠNG
Như đã chỉ ra trong DARTS [14] và MiLeNAS [15], khi học tham số kiến trúc α, việc phân chia dữ liệu huấn luyện và xác thực nên được tính đến để tránh quá khớp. Tuy nhiên, chúng tôi đã thảo luận rằng công thức hai cấp (1) và thuật toán huấn luyện của DARTS có thể dẫn đến một số vấn đề: hội tụ không biết, hiệu quả thấp và việc lựa chọn phép toán không công bằng. Do đó, chúng tôi theo thói quen phân chia dữ liệu huấn luyện-xác thực, nhưng muốn công thức hóa một bài toán một cấp, trái ngược với DARTS và MiLeNAS. Đầu tiên, nếu chúng ta sử dụng (w;α), cặp tham số trọng số và kiến trúc trong Pt. (2) để đại diện cho một mạng, những gì chúng tôi đề xuất làm là tiếp tục thư giãn các trọng số mạng w thông qua việc phân chia một bản sao mạng được ký hiệu bởi (y;α). Chúng tôi gọi (y;α) và (w;α) là các mạng chính và phụ trợ, chia sẻ cùng kiến trúc và cùng kích thước như các tensor trọng số, nhưng có thể có khởi tạo trọng số khác nhau.

Tiếp theo, một mất mát chính Lv(y;α) được tính toán với các tham số (y;α) được đưa vào dữ liệu Dv, trong khi một mất mát phụ trợ Lt(w;α) được tính toán với các tham số (w;α) được đưa vào dữ liệu Dt. Lưu ý rằng việc tính toán mất mát phụ trợ Lt(w;α) giống như của DARTS. Sự khác biệt là mất mát chính được tính toán trên mạng chính (y;α), thay vì (w;α).

Bây giờ chúng tôi trình bày mục tiêu một cấp của khung tìm kiếm kiến trúc thư giãn (RARTS) của chúng tôi. Với một phạt `2 trên khoảng cách giữa w và y, hai hàm mất mát được kết hợp thông qua Lagrangian thư giãn L= L(y;w; α) sau đây của Pt. (2):
L:=Lv(y;α) +Lt(w;α) +1
2‖y−w‖2
2; (4)
trong đó λ và β là các siêu tham số kiểm soát tỷ lệ phạt và quá trình học. Chúng ta sẽ thấy trong thuật toán tìm kiếm rằng số hạng phạt cho phép hai mạng trao đổi thông tin và hợp tác để tìm kiếm kiến trúc mà chúng chia sẻ cùng nhau. Kỹ thuật phân chia w và y này được gọi là phân chia mạng, cũng được truyền cảm hứng từ một số công trình trước đây [49]. Trong công trình của họ, việc phân chia các biến có thể xấp xỉ một bài toán tối thiểu hóa không trơn thông qua một thuật toán kết hợp các giải pháp dạng đóng và giảm gradient.

Vì các cách tiếp cận NAS khác nhau khám phá các kiến trúc có kích thước hoặc FLOPS không nhất quán, nó đã làm cho việc so sánh thông qua các phương pháp khác nhau trở nên không công bằng, bởi vì các mô hình lớn hơn có thể có hiệu suất tốt hơn nhưng hiệu quả thấp. Nhiều phương pháp NAS đã áp dụng độ trễ như một ràng buộc mô hình [7], [16]. Để kiểm soát kích thước mô hình, chúng tôi theo kỹ thuật xấp xỉ độ trễ mô hình với tổng độ trễ từ tất cả các phép toán [16], và thêm độ trễ xấp xỉ vào hàm mất mát như một phạt. Vì mỗi thành phần của tensor độ trễ (ký hiệu bởi Lat) là lượng độ trễ liên quan đến một phép toán ứng viên, kích thước của Lat giống như của α. Do đó, chúng tôi cung cấp một mục tiêu thay thế bị phạt bởi độ trễ của mô hình:
L:=Lv(y;α) +Lt(w;α) +1
2‖y−w‖2
2
+⟨Softmax(α);Lat⟩; (5)
trong đó dấu ngoặc là tích vô hướng.

B. THUẬT TOÁN RARTS
Chúng tôi tối thiểu hóa Lagrangian thư giãn L(y;w; α) trong (4) bằng lặp trên ba biến theo cách luân phiên để cho phép các lịch trình học cá nhân và linh hoạt cho ba biến. Tương tự như phương pháp Gauss-Seidel trong đại số tuyến tính số [50], chúng tôi sử dụng các biến được cập nhật ngay lập tức trong mỗi bước và có được lặp ba bước sau:
wt+1=wt−ηt
w∇w L(yt;wt;αt)
yt+1=yt−ηt
y∇y L(yt;wt+1;αt)
αt+1=αt−ηt
α∇α L(yt+1;wt+1;αt):(6)
Với gradient tường minh ∇w,y‖y−w‖2
2, chúng ta có:
wt+1=wt−ηt
w∇wLt(wt;αt)−ηt
wβ(wt−yt)
yt+1=yt−ηt
y∇yLv(yt;αt)−ηt
yβ(yt−wt+1)
αt+1=αt−ηt
α∇αLt(wt+1;αt)
−ηt
α∇αLv(yt+1;αt):(7)
Để tối thiểu hóa Lagrangian (5), hai bước đầu giống như Pt. (7) vì độ trễ chỉ phụ thuộc vào α. Bước thứ ba trở thành:
αt+1=αt−ηt
α∇αLt(wt+1;αt)
−ηt
α∇αLv(yt+1;αt)
−ηt
α∇α⟨Softmax(αt);Lat⟩:(8)
Lưu ý rằng việc cập nhật α trong Pt. (7) liên quan đến cả Lt và Lv, tương tự như DARTS bậc hai nhưng không có các đạo hàm bậc hai hỗn hợp. DARTS bậc nhất chỉ sử dụng ∇αLv trong bước này. Trong phần trước, chúng tôi đã thảo luận về vấn đề sụp đổ kiến trúc của DARTS, tức là chọn quá nhiều kết nối bỏ qua. Một lý do có thể tại sao DARTS có thể dẫn đến sụp đổ kiến trúc là các tham số kiến trúc của nó hội tụ nhanh hơn các trọng số trong các lớp tích chập. Có nghĩa là, khi DARTS chọn các tham số kiến trúc, nó có xu hướng chọn các phép toán kết nối bỏ qua, vì các lớp tích chập không được huấn luyện tốt. Sự thật rằng DARTS bậc nhất chỉ sử dụng một trong hai phân chia dữ liệu để huấn luyện các trọng số, làm cho việc huấn luyện các lớp tích chập tệ hơn. Đối với RARTS, chúng tôi sử dụng cả Lt và Lv để cập nhật các tham số trọng số w và y trong hai bước đầu của Pt. (7). Trong bước thứ ba của Pt. (7), cả Lt và Lv cũng được sử dụng để cập nhật kiến trúc được chia sẻ α. Theo cách này, kiến trúc được học tốt hơn, vì nhiều dữ liệu được tham gia trong quá trình huấn luyện. Nếu y=w được thực thi trong Pt. (7) ví dụ thông qua một nhân tử, RARTS về cơ bản giảm xuống thành MiLeNAS bậc nhất [15]. Tuy nhiên, việc thư giãn thành y≠w có các ưu thế của việc có tính tổng quát và mạnh mẽ hơn vì nó được tối ưu hóa trên hai mạng với các trọng số khác nhau nhưng có liên quan. Ngược lại, MiLeNAS chỉ huấn luyện các trọng số mạng trên dữ liệu huấn luyện Dt, và gặp phải cùng vấn đề hội tụ như DARTS bậc nhất (Phần III-D). Chúng tôi tóm tắt thuật toán RARTS trong Thuật toán 1.

Thuật toán 1 Tìm Kiếm Kiến Trúc Thư Giãn (RARTS)
Đầu vào: số lần lặp N, các siêu tham số λ và β, một lịch trình tốc độ học (ηt
w;ηt
u;ηt
α), khởi tạo của các tham số trọng số w0,u0 và các tham số kiến trúc α0.
Đầu ra: α, kiến trúc chúng ta muốn.
Chia tập dữ liệu D thành hai tập con Dp và Da.
for t= 0;1;:::;N do
Tính toán Lp và La trên Dp và Da, tương ứng, và sau đó tính toán L sử dụng Pt. (4)
Cập nhật các tham số thông qua giảm gradient:
wt+1=wt−ηt
w∇w L(yt;wt;αt)
yt+1=yt−ηt
y∇y L(yt;wt+1;αt)
αt+1=αt−ηt
α∇α L(ut+1;wt+1;αt)
end for

C. PHÂN TÍCH HỘI TỤ
Giả sử rằng Lt và Lv đều thỏa mãn tính chất gradient Lipschitz, hoặc tồn tại các hằng số dương L1 và L2 sao cho (z= (y;α),z′= (y′;α′)):
‖∇zLv(z)−∇zLv(z′)‖≤L1‖z−z′‖;∀(z;z′);
điều này ngụ ý:
Lv(z)≤Lv(z′)+⟨∇zLv(z′);(z−z′)⟩+L1
2‖z−z′‖2;
cho bất kỳ (z;z′); tương tự (ξ= (w;α),ξ′= (w′;α′)):
‖∇ξLt(ξ)−∇ξLt(ξ′)‖≤L2‖ξ−ξ′‖;∀(ξ;ξ′);
điều này ngụ ý:
Lt(ξ)≤Lt(ξ′)+⟨∇ξLt(ξ′);(ξ−ξ′)⟩+L2
2‖ξ−ξ′‖2;
cho bất kỳ (ξ;ξ′).

Định lý 1. Giả sử rằng các hàm mất mát Lt và Lv thỏa mãn tính chất gradient Lipschitz. Nếu các tốc độ học ηt
w, ηt
y và ηt
α đủ nhỏ chỉ phụ thuộc vào các hằng số Lipschitz cũng như (λ;β), và tiếp cận giới hạn khác không tại t lớn, hàm Lagrangian L(y;w; α) giảm trên các lần lặp của (7). Nếu bổ sung Lagrangian L bị chặn dưới và coercive (tính bị chặn của nó ngụ ý tính bị chặn của các biến của nó), dãy (yt;wt;αt) hội tụ theo từng dãy con đến một điểm tới hạn (y∗;w∗;α∗) của L(y;w; α) tuân theo các phương trình cân bằng:
∇wLt( w∗;α∗) +β( w∗−y∗) = 0;
∇yLv(y∗;α∗) +β(y∗−w∗) = 0;
∇αLt( w∗;α∗) +∇αLv(y∗;α∗) = 0: (9)
Nếu mất mát bị phạt bởi độ trễ như trong (5), phương trình cân bằng cuối cùng trở thành:
∇αLt( w∗;α∗) +∇αLv(y∗;α∗)
+∇α⟨Softmax(α∗);Lat⟩= 0:(10)

Chứng minh. Chúng tôi chỉ cần chứng minh cho mất mát (5) và các lần lặp (8), vì mất mát (4) là trường hợp đặc biệt của nó khi Lat = 0. Chúng tôi chú ý rằng hàm phạt độ trễ ⟨Softmax(αt);Lat⟩ cũng thỏa mãn tính chất gradient Lipschitz. Điều này bởi vì
∇Softmax(αt) = diag(Softmax( αt))
−Softmax(αt)
⊗(Softmax(αt))′;
và do đó tất cả các đạo hàm bậc nhất và bậc hai của ⟨Softmax(αt);Lat⟩ bị chặn đều bất kể αt.
Áp dụng các bất đẳng thức gradient Lipschitz trên Lv và Lt, chúng ta có:
L(yt+1;wt+1;αt+1)−L(yt;wt;αt)
=Lv(yt+1;αt+1) +Lt(wt+1;αt+1)
+β
2‖yt+1−wt+1‖2+⟨Softmax(αt+1);Lat⟩
−Lv(yt;αt)−Lt(wt;αt)
−β
2‖yt−wt‖2
−⟨Softmax(αt);Lat⟩
≤ ⟨∇y,αLv(yt;αt);(yt+1−yt;αt+1−αt)⟩
+L1
2‖(yt+1−yt;αt+1−αt)‖2
+⟨∇w,αLt(wt;αt);(wt+1−wt;αt+1−αt)⟩
+L2
2‖(wt+1−wt;αt+1−αt)‖2
+β
2(‖yt+1−wt+1‖2−‖yt−wt‖2)
+⟨∇α⟨Softmax(αt);Lat⟩;αt+1−αt)⟩
+L3
2‖αt+1−αt‖2:
Thay thế cho các gradient (w;y) từ các lần lặp (8), chúng ta tiếp tục:
L(yt+1;wt+1;αt+1)−L(yt;wt;αt)
≤  −(ηt
y)−1
⟨yt+1−yt+ηt
yβ(yt−wt+1);yt+1−yt⟩
+⟨∇αLv(yt;αt) +∇αLt(wt;αt);αt+1−αt⟩
−(ηt
w)−1
⟨wt+1−wt+ηt
wβ(wt−yt);wt+1−wt⟩
+L1
2‖yt+1−yt‖2+L1+L2+L3
2‖αt+1−αt‖2

VOLUME 4, 2016 5

--- TRANG 6 ---
F . Xue et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

HÌNH 1. Quỹ đạo học của cách tiếp cận RARTS tiếp cận điểm tối thiểu toàn cục (1;1) của mô hình có thể giải quyết được tại các giá trị phù hợp của λ, β và y0 (= 10 trong các biểu đồ con giữa/phải, β= 10 trong các biểu đồ con trái/phải, y0= 0 trong các biểu đồ con trái/giữa), so với của đường cơ sở (DARTS bậc nhất).

+L2
2‖wt+1−wt‖2
+β
2(‖yt+1−wt+1‖2−‖yt−wt‖2)
+⟨∇α⟨Softmax(αt);Lat⟩;αt+1−αt⟩
= −((ηt
y)−1+L1/2)‖yt+1−yt‖2
+((ηt
w)−1+L2/2)‖wt+1−wt‖2
−⟨yt−wt+1;yt+1−yt⟩
−⟨wt−yt;wt+1−wt⟩
+β
2(‖yt+1−wt+1‖2−‖yt−wt‖2)
+⟨∇αLv(yt;αt) +∇αLt(wt;αt);αt+1−αt⟩
+L1+L2+L3
2‖αt+1−αt‖2
+⟨∇α⟨Softmax(αt);Lat⟩;αt+1−αt⟩: (11)

Chúng ta chú ý đẳng thức sau
‖yt+1−wt+1‖2
=‖yt+1−wt+wt−wt+1‖2
=‖yt+1−wt‖2+ 2⟨yt+1−wt;wt−wt+1⟩
+‖wt−wt+1‖2;
trong đó
‖yt+1−wt‖2
=‖wt+yt−yt+yt+1‖2
=‖yt−wt‖2+ 2⟨yt−wt;yt+1−yt⟩+‖yt+1−yt‖2:
Khi thay thế các kết quả trên vào vế phải của (11), chúng ta thấy rằng:
L(yt+1;wt+1;αt+1)−L(yt;wt;αt)
≤−((ηt
y)−1+L1/2 +β/2)‖yt+1−yt‖2
+((ηt
w)−1+L2/2 +β/2)‖wt+1−wt‖2
+⟨wt+1−wt;yt+1−yt⟩
+⟨yt+1−yt;wt−wt+1⟩
+⟨∇αLv(yt;αt) +∇αLt(wt;αt);αt+1−αt⟩+L1+L2+L3
2‖αt+1−αt‖2
+⟨∇α⟨Softmax(αt);Lat⟩;αt+1−αt⟩:
Các số hạng β triệt tiêu. Thay thế cho α-gradient từ các lần lặp (8), chúng ta có:
L(yt+1;wt+1;αt+1)−L(yt;wt;αt)
≤−((ηt
y)−1+L1/2 +β/2)‖yt+1−yt‖2
+((ηt
w)−1+L2/2 +β/2)‖wt+1−wt‖2
+((ηt
α)−1+L1+L2+L3
2)‖αt+1−αt‖2
+⟨∇αLv(yt;αt)−∇αLv(yt+1;αt);αt+1−αt⟩
+⟨∇αLt(wt;αt)−∇αLt(wt+1;αt);αt+1−αt⟩
trong đó hai số hạng tích vô hướng cuối cùng bị chặn trên bởi:
(1 +ε)L4(‖yt−yt+1‖+‖wt−wt+1‖)‖αt+1−αt‖;
cho hằng số dương L4:= max(L1;L2). Từ đó suy ra:
L(yt+1;wt+1;αt+1)−L(yt;wt;αt)
≤−
(ηt
y)−1+L1
2+β
2−(1+ε)L4
2
‖yt+1−yt‖2
+
(ηt
w)−1+L2
2+β
2−(1+ε)L4
2
‖wt+1−wt‖2
+
(ηt
α)−1+L1+L2+L3
2−(1+ε)L4
2
‖αt+1−αt‖2: (12)
Nếu
ηt
y<1
2L1
2+β
2+ (1 +ε)L4
2−1
:=c1;
ηt
w<1
2L2
2+β
2+ (1 +ε)L4
2−1
:=c2;
ηt
α<1
2L1+L2+L3
2+(1+ε)L4
2−1
:=c3;

6 VOLUME 4, 2016

--- TRANG 7 ---
Author et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

L giảm dọc theo dãy (yt;wt;αt). Với c4= 1
2min{c−1
1;c−1
2;c−1
3}, từ (12) suy ra:
c4‖(yt+1−yt;wt+1−wt;αt+1−αt)‖2
≤L(yt;wt;αt)−L(yt+1;wt+1;αt+1)→0
khi t→+∞, ngụ ý rằng
lim
t→∞‖(yt+1−yt;wt+1−wt;αt+1−αt)‖= 0:
Vì L bị chặn dưới và coercive, ‖(yt;wt;αt)‖ bị chặn đều theo t. Cho (ηt
w;ηt
y;ηt
α) hướng tới giới hạn khác không tại t lớn. Khi đó (yt;wt;αt) hội tụ theo từng dãy con đến một điểm giới hạn (y∗;w∗;α∗) thỏa mãn hệ cân bằng (9) hoặc (10).

D. MÔ HÌNH HAI CẤP CÓ THỂ GIẢI QUYẾT ĐƯỢC
Chúng tôi so sánh một vài phương pháp có thể vi phân thông qua một ví dụ [14] có nghiệm giải tích. Bất kể phạt độ trễ, chúng tôi xem xét các hàm bậc hai Lv= w2+α−1,Lt=w2−2αw+α2 cho bài toán hai cấp (1). Do đó, nghiệm của bài toán cấp trong là:
w∗(α) = arg min
wLt(w;α) =α:
Khi đó Lv(w∗(α);α) =α2−2α+1, và điểm tối thiểu toàn cục của bài toán hai cấp này là (w∗;α∗) = (1;1). Tuy nhiên, các phương trình cân bằng của DARTS bậc nhất là:
∇wLt( w∗;α∗) = 0
∇αLv( w∗;α∗) = 0;
cho một cân bằng giả (w∗;α∗) = (2;2). Các phương trình cân bằng của MiLeNAS bậc nhất là:
∇wLt( w∗;α∗) = 0
∇αLt( w∗;α∗) +∇αLv( w∗;α∗) = 0;
cũng dẫn đến cân bằng giả (w∗;α∗) = (2;2).

Mặt khác, RARTS có thể xấp xỉ điểm tối thiểu đúng (w∗;α∗) = (1;1) tốt hơn. Lưu ý rằng cả Lv và Lt đều thỏa mãn tính chất gradient Lipschitz, điều này ngụ ý sự giảm của Lagrangian L bởi chứng minh của Định lý 1. Nếu λ > 1/2, β > 3/2, L bị chặn và coercive, điều này suy ra từ phân tích giá trị riêng của hệ tuyến tính (7) và được quan sát trong tính toán. Do đó, Định lý 1 có thể được áp dụng cho ví dụ này, và hệ cân bằng (9) viết:
(2w∗−2α∗) +β(w∗−y∗) = 0; (13)
−α∗+(y∗−w∗) = 0; (14)
(2w∗+ 2α∗) + y∗−2α∗ = 0: (15)
Cộng (13) và (14) cho: w∗=2α∗−1
β+2, kết hợp với (15) xác định (α∗;w∗;y∗) duy nhất: (α∗;w∗;y∗) = (4β+4
4β+2;4β+2
4β+2;4β+2−4
4β+2), nếu 4β+2≠0. Tại λ=β= 15 ,(α∗;w∗;y∗)≈(1:053;1:018;0:947) nơi hội tụ toàn cục giữ cho toàn bộ dãy RARTS. Động lực học học bắt đầu từ (α0;w0;y0) = (2;2;y0), được tái tạo trong Hình 1, cùng với ba đường cong học từ RARTS khi các tham số (λ;β) và giá trị khởi tạo y0 thay đổi. Trong Hình 1a, λ= 10 ,y0= 0. Trong Hình 1b, β= 10 ,y0= 0. Trong Hình 1c, λ=β= 10 . Trong tất cả các thí nghiệm, các tốc độ học được cố định ở 0:01. Với một phạm vi của (λ;β) và y0, chúng ta thấy rằng các đường cong học của chúng tôi đi vào một vòng tròn nhỏ xung quanh (1;1), trong khi DARTS bậc nhất hội tụ đến điểm giả.

IV. CÁC THÍ NGHIỆM
Chúng tôi chỉ ra bằng một loạt thí nghiệm cách RARTS hoạt động hiệu quả cho các nhiệm vụ khác nhau: tìm kiếm tôpô và tìm kiếm chiều rộng, trên các tập dữ liệu và không gian tìm kiếm khác nhau.

A. TÌM KIẾM TÔPÔ
Đối với các siêu tham số và thiết lập như lịch trình tốc độ học, số epoch cho CIFAR-10 và kỹ thuật học chuyển tiếp cho ImageNet, chúng tôi theo những gì của DARTS [14]. Chúng tôi cũng xem xét các kết quả trên CIFAR-10 và CIFAR-100 cho NATS-Bench [30], đây là một không gian tìm kiếm điểm chuẩn khác.

So sánh trên CIFAR-10. Tập dữ liệu CIFAR-10 bao gồm 50,000 hình ảnh huấn luyện và 10,000 hình ảnh kiểm tra [17]. Những hình ảnh 3 kênh với độ phân giải 32×32 này được phân bổ đều cho 10 lớp đối tượng. Đối với nhiệm vụ tìm kiếm kiến trúc trên CIFAR-10, dữ liệu Dt và Dv mà chúng tôi đã sử dụng là các nửa không chồng lấp ngẫu nhiên của dữ liệu huấn luyện gốc, giống như DARTS. Các thiết lập để tìm kiếm tôpô với RARTS theo những gì của DARTS. Tức là, kích thước batch = 64, tốc độ học trọng số ban đầu = 0.025, momentum = 0.9, phân rã trọng số = 0.0003, tốc độ học alpha ban đầu = 0.0003, phân rã trọng số alpha = 0.001, epoch = 50. Đối với giai đoạn huấn luyện, kích thước batch = 96, tốc độ học = 0.025, momentum = 0.9, phân rã trọng số = 0.0003 [14]. Đối với mỗi tế bào (bình thường hoặc giảm), 8 cạnh được chọn, với 1 trong 8 phép toán ứng viên được chọn cho mỗi cạnh (xem Hình 2). Bên cạnh chính quy hóa `2 tiêu chuẩn của các trọng số, chúng tôi cũng áp dụng phạt độ trễ. Mất mát chính quy hóa độ trễ được cân nhắc để nó cân bằng với các số hạng mất mát khác. Thông thường, nếu chúng ta tăng trọng số độ trễ, mô hình chúng ta tìm thấy sẽ nhỏ hơn về kích thước. Số hạng độ trễ Lat cho mỗi phép toán được đo thông qua PyTorch/TensorRT [16], và do đó nó phụ thuộc vào các thiết bị chúng ta sử dụng. Đối với tìm kiếm hiện tại, trọng số độ trễ là 0.002 để kích thước mô hình có thể so sánh với những gì trong các công trình trước. Mất mát độ trễ cuối cùng là tổng có trọng số của độ trễ từ mỗi phép toán, nơi các trọng số là các tham số kiến trúc.

Như được thể hiện trong Bảng 1, chi phí tìm kiếm của RARTS là 1:1 ngày GPU, ít hơn nhiều so với DARTS bậc hai. Lỗi kiểm tra của RARTS là 2:65%, vượt trội 3:00% của DARTS bậc nhất và 2:76% của DARTS bậc hai. Cũng nên chỉ ra rằng mô hình được tìm thấy bởi RARTS có 3.2M tham số, nhỏ hơn mô hình 3.3M được tìm thấy bởi DARTS. Hơn nữa, RARTS vượt trội các phương pháp có thể vi phân gần đây khác về độ chính xác và chi phí tìm kiếm ở kích thước mô hình tương đương. Chúng tôi cũng chú ý rằng phương sai hiệu suất RARTS thấp hơn so với

VOLUME 4, 2016 7

--- TRANG 8 ---
F . Xue et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

BẢNG 1. So sánh DARTS, RARTS và các phương pháp khác trên tìm kiếm mạng dựa trên CIFAR-10. DARTS-1/2 đại diện cho DARTS bậc 1/bậc 2, SNAS-Mi/Mo đại diện cho SNAS cộng với các ràng buộc nhẹ/vừa phải. Lưu ý rằng thời gian tìm kiếm nhanh hơn cũng phụ thuộc vào tốc độ và dung lượng bộ nhớ của các máy cục bộ được sử dụng. Cột V100 chỉ ra liệu mô hình có được huấn luyện trên GPU Tesla V100 cao cấp hay không. Mỗi lần chạy thí nghiệm của chúng tôi được thực hiện trên một GPU GTX 1080 Ti duy nhất. Các số trong ngoặc chỉ ra ngày GPU tìm kiếm của DARTS trên máy của chúng tôi. Trung bình của 5 lần chạy. Những lần chạy này được thực hiện trên máy của chúng tôi.

Phương pháp|Lỗi Kiểm tra (%)|Para. (M)|V100|Ngày GPU Tìm kiếm
Random Baseline [14]|3.29 ± 0.15|3.2|✗|4
AmoebaNet-B [10]|2.55 ± 0.05|2.8|✗|3150
SNAS-Mi [27]|2.98|2.9|✗|1.5
SNAS-Mo [27]|2.85 ± 0.02|2.8|✗|1.5
DARTS-1 [14]|3.00 ± 0.14|3.3|✗|1.5 (0.7)
DARTS-2 [14]|2.76 ± 0.09|3.3|✗|4 (3.1)
GDAS [18]|2.82|2.5|✓|0.2
ProxylessNAS [7]|2.08|5.7|✓|4.0
FairDARTS [19]|2.54 ± 0.05|3.3|✓|0.4
FairDARTS [19]|2.94 ± 0.05|3.2|✗|0.3
P-DARTS [35]|2.50|3.4|✓|0.3
PC-DARTS [36]|2.57 ± 0.07|3.6|✓|0.1
PC-DARTS [36]|2.71 ±|2.9|✗|0.1
MiLeNAS [15]|2.80 ± 0.04|2.9|✓|0.3
MiLeNAS [15]|2.51 ± 0.11|3.9|✓|0.3
RARTS|2.65 ± 0.07|3.2|✗|1.1

BẢNG 2. So sánh độ trễ cho các mô hình được tìm thấy dưới các siêu tham số khác nhau. Thiết lập kích thước batch = 64, tốc độ học = 3×10⁻⁴, phân rã trọng số =1×10⁻³ nhất quán với các thiết lập của DARTS và các biến thể DARTS khác, và được chọn làm thiết lập cơ sở của chúng tôi.

Trọng số Độ trễ|Kích thước Batch|Tốc độ Học|Phân rã Trọng số|Độ trễ (ms)
2×10⁻³|64|3×10⁻⁴|1×10⁻³|21.7
2×10⁻²|64|3×10⁻⁴|1×10⁻³|12.4
2×10⁻⁴|64|3×10⁻⁴|1×10⁻³|23.4
2×10⁻³|64|3×10⁻⁴|1×10⁻³|21.7
2×10⁻³|64|3×10⁻³|1×10⁻³|21.0
2×10⁻³|64|3×10⁻⁵|1×10⁻³|23.1
2×10⁻³|64|3×10⁻⁴|1×10⁻³|21.7
2×10⁻³|64|3×10⁻⁴|1×10⁻⁴|21.3
2×10⁻³|64|3×10⁻⁴|1×10⁻²|22.9
2×10⁻³|64|3×10⁻⁴|1×10⁻³|21.7
2×10⁻³|32|3×10⁻⁴|1×10⁻³|20.1
2×10⁻³|16|3×10⁻⁴|1×10⁻³|16.5

DARTS. RARTS cũng đã ngăn chặn sụp đổ kiến trúc và chỉ chọn một kết nối bỏ qua, như được thể hiện trong Hình 2. Chúng tôi nhận thức rằng các giá trị khác nhau của siêu tham số trong giai đoạn tìm kiếm RARTS có thể ảnh hưởng đến độ trễ của các mô hình được tìm thấy bởi RARTS. Bảng 2 đã liệt kê độ trễ của một số mô hình với các siêu tham số khác nhau. Ở đây chúng tôi sử dụng thiết lập cơ sở của trọng số độ trễ = 2×10⁻³, kích thước batch = 64, tốc độ học = 3×10⁻⁴, phân rã trọng số = 1×10⁻³. Chúng tôi thay đổi giá trị của một siêu tham số và giữ các siêu tham số khác giống nhau trong mỗi thí nghiệm, để chúng ta có thể thấy độ trễ kết quả nhạy cảm như thế nào với một siêu tham số cụ thể. Đầu tiên, kết quả cho thấy rằng kích thước batch nhỏ là 16 có thể ảnh hưởng đến độ trễ của mô hình, trong khi kích thước batch là 32 hoặc 64 có thể dẫn đến độ trễ tương tự. Đây là một hiện tượng tích cực, vì chúng ta thích kích thước batch lớn hơn vì nó đòi hỏi ít thời gian huấn luyện hơn. Trong số các siêu tham số khác, rõ ràng rằng yếu tố duy nhất có thể gây ra sự khác biệt đáng kể là trọng số độ trễ. Trọng số độ trễ 2×10⁻² lớn đến nỗi mô hình của nó chỉ có 60% độ trễ so với cơ sở. Độ trễ của mô hình không nhạy cảm với các siêu tham số khác, vì độ trễ khoảng 22.0, và chỉ thay đổi trong phạm vi 10%. Phát hiện này có lợi, vì chúng ta có thể cố định mức độ trễ thông qua việc cố định trọng số độ trễ và tìm mô hình với độ chính xác tốt nhất trong số các mô hình có mức độ trễ tương tự thông qua việc điều chỉnh các siêu tham số khác.

So sánh trên ImageNet. ImageNet [28], [29] bao gồm hơn 1.2 triệu hình ảnh huấn luyện và 5,000 hình ảnh kiểm tra từ 1,000 lớp đối tượng. Kiến trúc được xây dựng từ các tế bào được học trên CIFAR-10 được chuyển để học trên ImageNet-1000, tạo ra các kết quả trong Bảng 3. Ngay cả khi các thí nghiệm của chúng tôi được thực hiện trên GTX 1080 Ti có bộ nhớ tối đa chỉ cho phép kích thước batch là 128, tỷ lệ lỗi 25.9% của chúng tôi vượt trội so với DARTS và SNAS (kích thước batch 128), và cũng có thể so sánh với GDAS (kích thước batch 128) và MiLeNAS. MiLeNAS trong số một số thuật toán khác trong Bảng 2 đã được triển khai trên Tesla V100 với kích thước batch 1024, một phần cứng cao cấp hơn nhiều so với trong các thí nghiệm của chúng tôi. Điều này một phần giải thích độ chính xác thấp hơn của nó

8 VOLUME 4, 2016

--- TRANG 9 ---
Author et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

BẢNG 3. Chuyển sang ImageNet: so sánh lỗi kiểm tra của DARTS, RARTS và các phương pháp khác trên các máy cục bộ tương ứng. Cột V100 chỉ ra liệu mô hình có được huấn luyện trên GPU Tesla V100 cao cấp hay không. Bộ nhớ GPU lớn hơn có thể hỗ trợ kích thước batch lớn hơn, dẫn đến độ chính xác và hiệu quả huấn luyện tốt hơn trên ImageNet. Cột Direct chỉ ra liệu mô hình có được tìm kiếm trực tiếp trên ImageNet mà không có học chuyển tiếp hay không. Tìm kiếm trực tiếp có xu hướng chính xác hơn nhưng tốn nhiều tài nguyên tính toán hơn.

Phương pháp|Top-1 (%)|Top-5 (%)|Tham số (M)|V100|Direct
SNAS [27]|27.3|9.2|4.3|✗|✗
DARTS [14]|26.7|8.7|4.7|✗|✗
GDAS [18]|26.0|8.5|5.3|✓|✗
ProxylessNAS [7]|24.9|7.5|7.1|✓|✓
FairDARTS [19]|24.9|7.5|4.8|✓|✗
FairDARTS [19]|24.4|7.4|4.3|✓|✓
P-DARTS [35]|24.4|7.4|4.9|✓|✗
PC-DARTS [36]|25.1|7.8|5.3|✓|✗
PC-DARTS [36]|24.2|7.3|5.3|✓|✓
MiLeNAS [15]|25.4|7.9|4.9|✓|✗
RARTS|25.9|8.3|4.7|✗|✗

[THIS IS FIGURE: Hinh 2 showing architecture diagrams of normal (top) and reduction (bottom) cells found by RARTS, containing only one skip connection. The diagram shows nodes connected with different operations like sep_conv_5x5, sep_conv_3x3, avg_pool_3x3, dil_conv_3x3, dil_conv_5x5]

HÌNH 2. Kiến trúc của các tế bào bình thường (trên) và giảm (dưới) được tìm thấy bởi RARTS. Kiến trúc này chỉ chứa một kết nối bỏ qua. Bốn cạnh cuối cùng chỉ đơn giản được nối với nhau để xây dựng tế bào tiếp theo. Vì vậy không có tìm kiếm dọc theo các cạnh này, theo quy ước của DARTS [14].

hiệu quả xuất hiện (2.80) trên CIFAR-10 nhưng độ chính xác cao hơn sau khi chuyển sang ImageNet. Thông thường ImageNet được huấn luyện tốt hơn trên GPU lớn hơn vì kích thước batch lớn hơn. ProxylessNAS đã đạt được độ chính xác cao trên cả CIFAR-10 và ImageNet, nhưng các mô hình của họ lớn hơn nhiều so với các phương pháp khác. Nó đã tránh học chuyển tiếp vì chi phí huấn luyện được giảm thông qua lấy mẫu đường dẫn. Kế thừa các khối xây dựng từ DARTS và ProxylessNAS, FairDARTS đã phạt các tham số kiến trúc trung lập (gần 0.5), nhưng độ chính xác cao của nó cũng được hưởng lợi từ việc thư giãn trên không gian tìm kiếm. Các tế bào bình thường của họ chứa ít hơn 8 phép toán vì các phép toán với tham số kiến trúc thấp hơn một ngưỡng được đặt trước sẽ bị loại bỏ. Điều này giải thích kích thước mô hình nhỏ hơn và độ chính xác có thể so sánh của họ. P-DARTS đã thiết kế một phương pháp tiến bộ để tăng độ sâu của tìm kiếm. Công trình của họ cho thấy rằng các tế bào sâu hơn có khả năng biểu diễn tốt hơn, đây cũng là một cải tiến trên không gian tìm kiếm. PC-DARTS như một phương pháp lấy mẫu đã đạt được chi phí tìm kiếm ít nhất và có thể được huấn luyện trực tiếp trên ImageNet. Những phương pháp này bổ sung cho công trình của chúng tôi hoàn toàn về thuật toán tìm kiếm có thể vi phân mà không sửa đổi không gian tìm kiếm của DARTS.

So sánh trên NATS-Bench. Đối với NATS-Bench, người ta

BẢNG 4. Lỗi kiểm tra của DARTS so với RARTS trên không gian tìm kiếm NATS-Bench. Các kết quả của DARTS trên NATS-Bench là từ [30]. Tỷ lệ = số kết nối bỏ qua trên số phép toán tổng cộng trong kiến trúc được khám phá.

Tập dữ liệu|Phương pháp|Lỗi (%)|Tỷ lệ (%)
CIFAR-10|DARTS-1|40.16|100
|DARTS-2|34.62|100
|RARTS|11.48|0
CIFAR-100|DARTS-1|38.74|38.9
|DARTS-2|39.51|38.9
|RARTS|32.37|0

phải tìm kiếm một khối 6 nút từ không gian tìm kiếm của 5 phép toán khác nhau, bao gồm zero, kết nối bỏ qua, average pooling 3×3, convolution 1×1 hoặc convolution 3×3 [30]. Do đó, nó bao gồm 15,625 kiến trúc ứng viên khác nhau và bất kỳ phương pháp kiểu DARTS nào cũng có thể được điều chỉnh dễ dàng cho không gian tìm kiếm của nó. NATS-Bench đã đo hiệu suất của mỗi kiến trúc dưới cùng thiết lập huấn luyện, và do đó có thể thực hiện so sánh công bằng giữa các kiến trúc được khám phá vì không cần đánh giá thêm trên các máy cục bộ. Trong các thí nghiệm của chúng tôi, chúng tôi đặt kích thước batch = 64, tốc độ học trọng số ban đầu = 0.025, momentum = 0.9, phân rã trọng số = 0.0005, tốc độ học alpha ban đầu = 0.0003, phân rã trọng số alpha = 0.001, số epoch = 100. Bảng 4 trình bày các kết quả tìm kiếm của DARTS so với RARTS trên NATS-Bench. RARTS đã vượt qua cả DARTS-1 và DARTS-2 về độ chính xác hơn 20% trên CIFAR-10 và 6% trên CIFAR-100. Bên cạnh thành công về độ chính xác, RARTS đã hoàn toàn thoát khỏi vấn đề sụp đổ kiến trúc, tức là các kiến trúc được tìm thấy bởi RARTS từ NATS-Bench không chứa kết nối bỏ qua nào. Ngược lại, cả hai kiến trúc được tìm thấy bởi DARTS-1 và DARTS-2 đều chứa 100% và 38.9% (trung bình của 3 lần chạy) kết nối bỏ qua trên CIFAR-10 và CIFAR-100, tương ứng. Rõ ràng rằng quá nhiều kết nối bỏ qua dẫn đến sụp đổ kiến trúc sẽ ảnh hưởng đến hiệu suất của các mô hình rất nhiều.

VOLUME 4, 2016 9

--- TRANG 10 ---
F . Xue et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

B. TÌM KIẾM CHIỀU RỘNG
Để tìm kiếm chiều rộng của kiến trúc (số lượng kênh trong các lớp tích chập), chúng tôi theo các thiết lập của Network Slimming [21], bằng cách giới thiệu các tham số tính điểm để đo tầm quan trọng của kênh. Ký hiệu bản đồ đặc trưng gốc bởi Fi,j và định nghĩa bản đồ đặc trưng mới F̃i,j=γi,jFi,j, trong đó (i,j) là các chỉ số lớp và kênh. Nhân một kênh của bản đồ đặc trưng đầu ra với γ tương đương với việc nhân các kernel tích chập kết nối đến bản đồ đặc trưng đầu ra này với cùng γ. Chúng ta cắt tỉa một kênh nếu γ tương ứng bằng 0 hoặc rất nhỏ. Các γij là các tham số kiến trúc có thể học độc lập với các trọng số kênh, và do đó được coi là có vai trò tương tự như các tham số kiến trúc trong trường hợp tìm kiếm kiến trúc tôpô.

Mặc dù cách xử lý các tham số tính điểm như vậy giống như trong Network Slimming [21], chúng tôi chỉ ra rằng công thức một cấp của RARTS và thuật toán huấn luyện để học các tham số tính điểm đó là mới. Sự khác biệt đầu tiên là Network Slimming huấn luyện cả tham số trọng số và kiến trúc trên toàn bộ dữ liệu (huấn luyện và xác thực), không giống DARTS hoặc RARTS, mà không sử dụng việc phân chia tập dữ liệu hoặc phân chia mạng. Một sự khác biệt chính khác giữa cắt tỉa RARTS và Network Slimming là trong thuật toán tìm kiếm, tức là Network Slimming huấn luyện các trọng số và kiến trúc kết hợp trong một bước, trong khi RARTS huấn luyện chúng trong một lặp ba bước. Hơn nữa, Network Slimming đã sử dụng các trọng số chuẩn hóa batch như các tham số tính điểm. Chúng tôi chỉ ra rằng chúng ta vẫn có thể định nghĩa một tập hợp các tham số kiến trúc có thể học α như vậy, ngay cả khi phép toán chuẩn hóa batch không được chứa trong kiến trúc.

Chúng tôi cũng so sánh RARTS với TAS [20], đây là một phương pháp tìm kiếm chiều rộng khác dựa trên NAS có thể vi phân, dựa vào cả việc thư giãn liên tục thông qua các bản đồ đặc trưng có kích thước khác nhau và chưng cất mô hình. Sự khác biệt đầu tiên là về cách các tham số tính điểm kênh được áp dụng cho các bản đồ đặc trưng. Đối với TAS, các tham số kênh được xử lý như xác suất của các bản đồ đặc trưng ứng viên, được làm mượt bởi Gumbel-Softmax. Sau đó một tập con của các bản đồ đặc trưng được lấy mẫu để giảm thiểu chi phí bộ nhớ cao. RARTS đơn giản hơn nhiều trong công thức của nó, vì nó là một tích vô hướng của các tham số kênh với bộ lọc cần được cắt tỉa. Sự khác biệt chính thứ hai là việc sử dụng một kỹ thuật huấn luyện gọi là Knowledge Distillation (KD) [51] bởi TAS để cải thiện độ chính xác. Có một số phương pháp dựa trên NAS khác cho tìm kiếm chiều rộng, hoặc cắt tỉa kênh [37], [48] được đề cập trong Phần II-B. Lưu ý rằng công thức của chúng tôi về vấn đề và tiêu chí để đánh giá kết quả là khác nhau, chúng tôi nhấn mạnh rằng tiến bộ của chúng tôi là trong việc kết hợp một thuật toán tìm kiếm mới và nhiệm vụ tìm kiếm chiều rộng.

Khi sử dụng RARTS để tìm kiếm chiều rộng, chúng tôi cũng theo các siêu tham số và thiết lập của Network Slimming. Tức là, tốc độ học = 0.1, phân rã trọng số = 0.0001, epoch = 160 [21]. Trong Bảng 5, RARTS vượt trội hơn cơ sở chưa cắt tỉa, Network Slimming (NS) và TAS [20] hơn 10% giảm lỗi trên CIFAR-10. Trong khi TAS không cung cấp tùy chọn để chỉ định tỷ lệ cắt tỉa của các kênh (PRC), tỷ lệ cắt tỉa của FLOPS khoảng 30% cho NS (40% PRC),

BẢNG 5. Ứng dụng RARTS cho việc cắt tỉa kênh ResNet-164 (cơ sở, 1.7 M tham số) trên CIFAR-10 và CIFAR-100, so sánh với cơ sở, TAS và Network Slimming. Các số trong ngoặc chỉ ra tỷ lệ cắt tỉa của kênh (PRC). Đối với NS và RARTS, PRC được cố định ở 40% hoặc 60%. NS = Network Slimming.

Dữ liệu|Phương pháp|Lỗi Kiểm tra (%)
CIFAR-10|Baseline [21]|5.42
|TAS [20]|6.00
|NS (40% PRC) [21]|5.08
|RARTS (40% PRC)|4.58
|NS (60% PRC) [21]|5.27
|RARTS (60% PRC)|4.90
CIFAR-100|Baseline [21]|23.37
|TAS [20]|22.24
|NS (40% PRC) [21]|22.87
|RARTS (40% PRC)|22.64
|NS (60% PRC) [21]|23.91
|RARTS (60% PRC)|23.26

BẢNG 6. Ứng dụng RARTS cho việc cắt tỉa MobileNetV2 trên tập dữ liệu ImageNet-R (một tập con được lấy mẫu ngẫu nhiên của ImageNet-1000, với 20 lớp đối tượng), so sánh với cơ sở, cắt tỉa ngẫu nhiên, DARTS bậc 1 và bậc 2. Ở đây cắt tỉa ngẫu nhiên có nghĩa là chúng tôi làm zero các kênh ngẫu nhiên theo tỷ lệ cắt tỉa của RARTS. Trung bình của 5 lần chạy. PRC = tỷ lệ cắt tỉa kênh trung bình trên các lớp được cắt tỉa. Chúng tôi lưu ý rằng PRC có thể cao vì tập dữ liệu nhỏ hơn nhiều.

Phương pháp|Lỗi Kiểm tra. (%)|PRC (%)
Baseline|12.3 ± 1.4|-
Random Pruning|12.0 ± 1.1|71.2 ± 1.9
DARTS-1|10.1 ± 2.0|69.0 ± 0.9
DARTS-2|9.8 ± 1.7|72.6 ± 2.0
RARTS|8.2±1.9|71.2±1.9

RARTS (40% PRC) và TAS. Vì vậy việc so sánh là công bằng. Trên CIFAR-100, RARTS vẫn dẫn đầu NS ở cùng PRC. Khoảng cách nhỏ hơn vì mạng cơ sở ít dư thừa hơn. Các kết quả thực nghiệm của chúng tôi tiết lộ rằng độ chính xác của TAS với KD thấp hơn (trên CIFAR-10) hoặc tương tự (trên CIFAR-100) so với RARTS, trong khi TAS không có kỹ thuật huấn luyện như KD tệ hơn 2% [20]. Điều này hỗ trợ sự thật rằng RARTS hoạt động tốt hơn như một phương pháp có thể vi phân cho tìm kiếm chiều rộng, không quan tâm đến bất kỳ thủ thuật huấn luyện nào khác. Ngoài các so sánh với các phương pháp trên, chúng tôi cũng xem xét một nhiệm vụ cắt tỉa để so sánh DARTS và RARTS, có thể được xem như một nghiên cứu ablation của RARTS trên nhiệm vụ tìm kiếm chiều rộng. Đối với nhiệm vụ này, chúng tôi cắt tỉa MobileNetV2 [52] trên một tập con 20 lớp được lấy mẫu ngẫu nhiên của ImageNet-1000, với chính quy hóa `1 nhưng tỷ lệ cắt tỉa không cố định. Tỷ lệ cắt tỉa có thể được học tự động bởi số hạng chính quy hóa mạnh, vì nhiều tham số kiến trúc đơn giản bằng zero. Bảng 6 cho thấy rằng RARTS cũng đánh bại cả cắt tỉa ngẫu nhiên và DARTS về độ chính xác. Mặc dù DARTS bậc 2 đạt được độ thưa cao hơn, nó hy sinh độ chính xác.

V. KẾT LUẬN
Chúng tôi đã phát triển RARTS, một phương pháp có thể vi phân thư giãn mới cho tìm kiếm kiến trúc nơ-ron. Chúng tôi đã chứng minh định lý hội tụ của nó và so sánh nó với DARTS trên một mô hình có thể giải quyết được bằng phân tích. Nhờ thiết kế phân chia dữ liệu và mạng, RARTS đã đạt được độ chính xác cao và hiệu quả tìm kiếm so với các phương pháp có thể vi phân tiên tiến, đặc biệt là DARTS, với một loạt rộng các thí nghiệm, bao gồm cả tìm kiếm tôpô và tìm kiếm chiều rộng. Những kết quả này hỗ trợ RARTS trở thành một công cụ tìm kiếm kiến trúc nơ-ron có thể vi phân đáng tin cậy và mạnh mẽ hơn cho các tập dữ liệu và không gian tìm kiếm khác nhau. Trong công trình tương lai, chúng tôi dự định kết hợp các kỹ thuật lấy mẫu không gian tìm kiếm và chính quy hóa để tăng tốc RARTS (như đã thấy trong một số biến thể gần đây của DARTS) cho các ứng dụng rộng hơn trong học sâu.

LỜI CẢM ơN
Các tác giả muốn cảm ơn biên tập viên liên kết và các phản biện ẩn danh vì việc đọc cẩn thận và phản hồi hữu ích của họ, điều này đã cải thiện việc trình bày của bài báo. Các tác giả cũng muốn bày tỏ sự đánh giá cao của họ đối với Tiến sĩ Shuai Zhang và Tiến sĩ Jiancheng Lyu vì những thảo luận tuyệt vời liên quan đến dự án.

TÀI LIỆU THAM KHẢO
[1] B. Zoph and Q. V . Le, "Neural architecture search with reinforcement learning," ICLR, 2017; arXiv preprint arXiv:1611.01578 , 2016.
[2] B. Zoph, V . Vasudevan, J. Shlens, and Q. V . Le, "Learning transferable architectures for scalable image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition , 2018, pp. 8697– 8710.
[3] G. Ghiasi, T.-Y . Lin, and Q. V . Le, "Nas-fpn: Learning scalable feature pyramid architecture for object detection," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 7036– 7045.
[4] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and L. Fei-Fei, "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 82–92.
[5] T. Elsken, J. H. Metzen, and F. Hutter, "Neural architecture search: A survey," The Journal of Machine Learning Research , vol. 20, no. 1, pp. 1997–2017, 2019.
[6] P. Ren, Y . Xiao, X. Chang, P.-Y . Huang, Z. Li, X. Chen, and X. Wang, "A comprehensive survey of neural architecture search: Challenges and solutions," ACM Computing Surveys (CSUR) , vol. 54, no. 4, pp. 1–34, 2021.
[7] H. Cai, L. Zhu, and S. Han, "Proxylessnas: Direct neural architecture search on target task and hardware," ICLR, 2019; arXiv preprint arXiv:1812.00332 , 2018.
[8] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, F.-F. Li, A. Yuille, J. Huang, and K. Murphy, "Progressive neural architecture search," in Proceedings of the European Conference on Computer Vision (ECCV) , 2018, pp. 19–34.
[9] H. Pham, M. Y . Guan, B. Zoph, Q. V . Le, and J. Dean, "Efficient neural architecture search via parameter sharing," ICML, 2018; arXiv preprint arXiv:1802.03268 , 2018.
[10] E. Real, A. Aggarwal, Y . Huang, and Q. V . Le, "Regularized evolution for image classifier architecture search," in Proceedings of the aaai conference on artificial intelligence , vol. 33, 2019, pp. 4780–4789.
[11] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu, and D. Marculescu, "Single-path nas: Designing hardware-efficient convnets in less than 4 hours," in Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 2019, pp. 481–497.
[12] B. Wu, X. Dai, P. Zhang, Y . Wang, F. Sun, Y . Wu, Y . Tian, P. Vajda, Y . Jia, and K. Keutzer, "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 10 734–10 742.
[13] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, "Single path one-shot neural architecture search with uniform sampling," in European Conference on Computer Vision . Springer, 2020, pp. 544–560.

10 VOLUME 4, 2016

--- TRANG 11 ---
Author et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

[14] H. Liu, K. Simonyan, and Y . Yang, "Darts: Differentiable architecture search," in ICLR 2019 , 2019. [Online]. Available: https://www.microsoft. com/en-us/research/publication/darts-differentiable-architecture-search/
[15] C. He, H. Ye, L. Shen, and T. Zhang, "Milenas: Efficient neural architecture search via mixed-level reformulation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 11 993–12 002.
[16] W. Chen, X. Gong, X. Liu, Q. Zhang, Y . Li, and Z. Wang, "Fasterseg: Searching for faster real-time semantic segmentation," ICLR, 2020; arXiv preprint: 1912.10917 , 2019.
[17] A. Krizhevsky, G. Hinton et al. , "Learning multiple layers of features from tiny images," Citeseer, Tech. Rep., 2009.
[18] X. Dong and Y . Yang, "Searching for a robust neural architecture in four gpu hours," in Proceedings of the IEEE Conference on computer vision and pattern recognition , 2019, pp. 1761–1770.
[19] X. Chu, T. Zhou, B. Zhang, and J. Li, "Fair darts: Eliminating unfair advantages in differentiable architecture search," in European conference on computer vision . Springer, 2020, pp. 465–480.
[20] X. Dong and Y . Yang, "Network pruning via transformable architecture search," in Advances in Neural Information Processing Systems , 2019, pp. 760–771.
[21] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, "Learning efficient convolutional networks through network slimming," in Proceedings of the IEEE International Conference on Computer Vision , 2017, pp. 2736–2744.
[22] Z. Huang and N. Wang, "Data-driven sparse structure selection for deep neural networks," in Proceedings of ECCV , 2018.
[23] K. Bui, F. Park, S. Zhang, Y . Qi, and J. Xin, "Improving network slimming with nonconvex regularization," IEEE Access , vol. 9, pp. 115 292–115 314, 2021.
[24] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li, "Learning structured sparsity in deep neural networks," in Advances in neural information processing systems , 2016, pp. 2074–2082.
[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems , vol. 30, 2017.
[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. , "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929 , 2020.
[27] S. Xie, H. Zheng, C. Liu, and L. Lin, "SNAS: stochastic neural architecture search," ICLR, 2019; arXiv preprint arXiv:1812.09926 , 2018.
[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li, "Imagenet: A large-scale hierarchical image database," in 2009 IEEE conference on computer vision and pattern recognition . IEEE, 2009, pp. 248–255.
[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al. , "Imagenet large scale visual recognition challenge," International journal of computer vision , vol. 115, no. 3, pp. 211–252, 2015.
[30] X. Dong, L. Liu, K. Musial, and B. Gabrys, "Nats-bench: Benchmarking nas algorithms for architecture topology and size," IEEE transactions on pattern analysis and machine intelligence , 2021.
[31] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 770–778.
[32] B. Colson, P. Marcotte, and G. Savard, "An overview of bilevel optimization," Annals of operations research , vol. 153, no. 1, pp. 235–256, 2007.
[33] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, "Bilevel programming for hyperparameter optimization and meta-learning," Proc. ICML , 2018.
[34] R. Wang, M. Cheng, X. Chen, X. Tang, and C.-J. Hsieh, "Rethinking architecture selection in differentiable nas," in International Conference on Learning Representations , 2021.
[35] X. Chen, L. Xie, J. Wu, and Q. Tian, "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation," in Proceedings of the IEEE International Conference on Computer Vision , 2019, pp. 1294–1303.
[36] Y . Xu, L. Xie, X. Zhang, X. Chen, G.-J. Qi, Q. Tian, and H. Xiong, "Pc-darts: Partial channel connections for memory-efficient architecture search," in International Conference on Learning Representations , 2020.
[37] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, "Amc: Automl for model compression and acceleration on mobile devices," in Proceedings of the European Conference on Computer Vision (ECCV) , 2018, pp. 784– 800.

VOLUME 4, 2016 11

--- TRANG 12 ---
F . Xue et al. : RARTS: Một Phương Pháp Tìm Kiếm Kiến Trúc Thư Giãn Bậc Nhất Hiệu Quả

[38] M. Tan, B. Chen, R. Pang, V . Vasudevan, M. Sandler, A. Howard, and Q. V . Le, "Mnasnet: Platform-aware neural architecture search for mobile," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 2820–2828.
[39] A. Wan, X. Dai, P. Zhang, Z. He, Y . Tian, S. Xie, B. Wu, M. Yu, T. Xu, K. Chen et al. , "Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 12 965–12 974.
[40] L. Li and A. Talwalkar, "Random search and reproducibility for neural architecture search," in Uncertainty in artificial intelligence . PMLR, 2020, pp. 367–377.
[41] H. Hu, R. Peng, Y . Tai, and C. Tang, "Network trimming: A data-driven neuron pruning approach towards efficient deep architectures," CoRR , vol. abs/1607.03250, 2016.
[42] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, "Pruning filters for efficient convnets," ICLR, 2017; arXiv preprint arXiv:1608.08710 , 2016.
[43] Y . He, X. Zhang, and J. Sun, "Channel pruning for accelerating very deep neural networks," in Proceedings of the IEEE International Conference on Computer Vision , 2017, pp. 1389–1397.
[44] J.-H. Luo, J. Wu, and W. Lin, "Thinet: A filter level pruning method for deep neural network compression," in Proceedings of the IEEE international conference on computer vision , 2017, pp. 5058–5066.
[45] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, "Rethinking the value of network pruning," ICLR, 2019; arXiv preprint arXiv:1810.05270 , 2018.
[46] M. Yuan and Y . Lin, "Model selection and estimation in regression with grouped variables," Journal of the Royal Statistical Society: Series B (Statistical Methodology) , vol. 68, no. 1, pp. 49–67, 2006.
[47] J. Ye, L. Xin, Z. Lin, and J. Z. Wang, "Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers," in Proceedings of ICLR , 2018.
[48] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and J. Sun, "Metapruning: Meta learning for automatic neural network channel pruning," in Proceedings of the IEEE International Conference on Computer Vision , 2019, pp. 3296–3305.
[49] T. Dinh and J. Xin, "Convergence of a relaxed variable splitting method for learning sparse neural networks via `1,`0, and transformed- `1penalties," in Proceedings of SAI Intelligent Systems Conference . Springer, 2020, pp. 360–374.
[50] G. H. Golub and C. F. Van Loan, Matrix Computations (3rd ed.) . Johns Hopkins Univ. Press, 1996.
[51] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531 , 2015.
[52] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, "Mobilenetv2: Inverted residuals and linear bottlenecks," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2018, pp. 4510–4520.

FANGHUI XUE nhận bằng B.S. về toán học từ Đại học Fudan, Thượng Hải, Trung Quốc, năm 2016 và bằng M.S. về phân tích và quản lý rủi ro toán học từ Đại học Georgia State, Atlanta, GA, năm 2018. Vào tháng 5 năm 2022, ông nhận bằng Ph.D. về toán học tại Đại học California, Irvine, CA. Các lĩnh vực nghiên cứu của ông bao gồm học sâu và thị giác máy tính, tập trung vào AutoML và xây dựng các mạng nơ-ron hiệu quả.

YINGYONG QI nhận bằng Ph.D. về khoa học lời nói và thính giác từ Đại học Ohio State, năm 1989, và bằng Ph.D. về kỹ thuật điện và máy tính từ Đại học Arizona, năm 1993. Ông giữ vị trí giảng viên tại Đại học Arizona, từ 1989 đến 1999. Ông là Nhà khoa học Thỉnh giảng tại Phòng thí nghiệm Điện tử Nghiên cứu, Viện Công nghệ Massachusetts, từ 1995 đến 1996, và Nhà khoa học Thỉnh giảng tại Phòng thí nghiệm Tính toán Thị giác của Hewlett Packard, Palo Alto, năm 1998. Ông hiện là Giám đốc Cấp cao về công nghệ tại Qualcomm và là Nhà nghiên cứu của Khoa Toán học, Đại học California tại Irvine. Ông đã xuất bản hơn 100 bài báo khoa học và bằng sáng chế Hoa Kỳ trong thời gian làm việc tại trường đại học và ngành công nghiệp. Các lĩnh vực nghiên cứu của ông bao gồm xử lý lời nói, thị giác máy tính, và học máy. Ông nhận Giải thưởng Klatt Memorial về Khoa học Lời nói từ Hiệp hội Âm học Hoa Kỳ, năm 1991, Giải thưởng Đầu tiên từ Viện Y tế Quốc gia, năm 1992, và Giải thưởng AASFAA Outstanding Faculty từ Đại học Arizona, năm 1998. Gần đây hơn, ông dẫn dắt một đội giành được Giải ba trong Cuộc thi Nhận dạng Hình ảnh Công suất Thấp IEEE, được tài trợ bởi Google tại CVPR 2019.

JACK XIN nhận bằng Ph.D. về toán học từ Viện Khoa học Toán học Courant của Đại học New York, năm 1990. Ông là giảng viên tại Đại học Arizona, từ 1991 đến 1999, và Đại học Texas tại Austin, từ 1999 đến 2005. Ông hiện là Giáo sư Chancellor về toán học tại UC Irvine. Các lĩnh vực nghiên cứu của ông bao gồm phân tích ứng dụng và các phương pháp tính toán, và ứng dụng của chúng trong các vấn đề đa tỷ lệ và khoa học dữ liệu. Ông là thành viên của Quỹ Guggenheim, Hiệp hội Toán học Hoa Kỳ, Hiệp hội Thúc đẩy Khoa học Hoa Kỳ, và Hiệp hội Toán học Công nghiệp và Ứng dụng. Ông là người nhận Giải thưởng Khoa Qualcomm (2019–2022).

12 VOLUME 4, 2016
