# 2008.03901.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2008.03901.pdf
# File size: 2987168 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
This article has been accepted for publication in IEEE Access.
Digital Object IdentiÔ¨Åer 10.1109/ACCESS.2022.3185095
RARTS: An EfÔ¨Åcient First-Order Relaxed
Architecture Search Method
FANGHUI XUE1, YINGYONG QI1, AND JACK XIN.1
1Department of Mathematics, University of California at Irvine, Irvine, CA 92697, USA
Corresponding author: Fanghui Xue (e-mail: fanghuix@uci.edu).
The work was partially supported by NSF grants DMS-1854434, DMS-1952644, and a Qualcomm Faculty Award.
ABSTRACT Differentiable architecture search (DARTS) is an effective method for data-driven neural
network design based on solving a bilevel optimization problem. Despite its success in many architecture
search tasks, there are still some concerns about the accuracy of Ô¨Årst-order DARTS and the efÔ¨Åciency of
the second-order DARTS. In this paper, we formulate a single level alternative and a relaxed architecture
search (RARTS) method that utilizes the whole dataset in architecture learning via both data and network
splitting, without involving mixed second derivatives of the corresponding loss functions like DARTS. In
our formulation of network splitting, two networks with different but related weights cooperate in search of
a shared architecture. The advantage of RARTS over DARTS is justiÔ¨Åed by a convergence theorem and an
analytically solvable model. Moreover, RARTS outperforms DARTS and its variants in accuracy and search
efÔ¨Åciency, as shown in adequate experimental results. For the task of searching topological architecture,
i.e., the edges and the operations, RARTS obtains a higher accuracy and 60% reduction of computational
cost than second-order DARTS on CIFAR-10. RARTS continues to out-perform DARTS upon transfer to
ImageNet and is on par with recent variants of DARTS even though our innovation is purely on the training
algorithm without modifying search space. For the task of searching width, i.e., the number of channels
in convolutional layers, RARTS also outperforms the traditional network pruning benchmarks. Further
experiments on the public architecture search benchmark like NATS-Bench also support the preeminence
of RARTS.
INDEX TERMS Convolutional neural networks, neural architecture search, differentiable architecture
search, network compression.
I. INTRODUCTION
Neural Architecture Search (NAS) is an automated machine
learning technique to design an optimal neural network ar-
chitecture by searching its building blocks of deep neural
networks from a collection of candidate structures and opera-
tions. Although NAS has achieved many successes in several
computer vision tasks [1]‚Äì[6], the search process demands
huge computational resources. The current search times have
come down considerably from as many as 2000 GPU days
in early NAS [2], thanks to subsequent studies [7]‚Äì[13]
among others. Differentiable Architecture Search (DARTS)
[14] is an appealing method that avoids searching over all
possible combinations by relaxing the categorical architec-
ture indicators to continuous parameters. The higher level
architecture can be learned along with lower level weights
via stochastic gradient descent by approximately solving a
bilevel optimization problem. DARTS can be further sortedinto Ô¨Årst-order DARTS and second-order DARTS, in line
with whether a mixed second derivative estimation of loss
function is used or not.
Despite its search efÔ¨Åciency obtained from continuous
relaxation, DARTS can still have some problems experimen-
tally and theoretically. They are the efÔ¨Åciency problem with
second-order DARTS, the convergence problem with Ô¨Årst-
order DARTS, and the architecture collapse problem (i.e.,
the selected architecture contains too many skip-connections)
with both DARTS. Second-order DARTS takes much longer
search time than Ô¨Årst-order DARTS as it involves the mixed
second derivatives. It has also been pointed out that second-
order DARTS can have superposition effect [15], which
means the approximation of the gradient of is based on the
approximation of the weight wone step ahead. This is be-
lieved to cause gradient errors and failures in Ô¨Ånding optimal
architectures. Therefore, it is used less often in practice than
VOLUME 4, 2016 1arXiv:2008.03901v2  [cs.LG]  24 Jun 2022

--- PAGE 2 ---
F . Xue et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
Ô¨Årst-order DARTS [15], [16]. However, Ô¨Årst-order DARTS
learns the architecture using half of the data only. Evidences
are provided to show that it can result in incorrect limits
and worse performance [14]. The experimental results also
show that Ô¨Årst-order DARTS (3.00% error) is less accurate
than second-order DARTS (2.76% error) on the CIFAR-10
dataset [14], [17]. For the architecture collapse problem,
typically such a bias in operation selection degrades the
model performance. This problem has been observed by a
few researchers [18], [19], who have tried to solve it by
replacing some operations of the architecture.
In addition to the search for topological architectures,
i.e., edges and operations of cells (building blocks) in some
early NAS works and DARTS [2], [14], many NAS style
methods have been developed to search for the width of a
model, i.e., the number of channels in convolutional layers
[16], [20]. Searching for width is supposed to be a way
of channel pruning, which is a common tool for network
compression , i.e., constructing slim networks from redundant
ones [21]. SpeciÔ¨Åcally, channel pruning can be formulated
as an architecture search problem, via the setup of learnable
channel scoring parameters [21]‚Äì[23] as architecture param-
eters. This is an elegant approach for compression without
relying on channel magnitude (group `1norm), which is used
in previous regularization methods [24]. The previous way of
setting up channel scoring parameters [21] utilizes the scale
parameters of the batch normalization layers, yet they are
not contained in many modern networks [25], [26]. Another
challenge remains to be solved is to replace its plain gradient
descent by the more accurate DARTS style algorithms.
Apart from the bilevel formulation of DARTS, a single
level approach (SNAS) based on a differentiable loss and
sampling has been proposed [27]. On CIFAR-10, SNAS is
more accurate than the Ô¨Årst-order DARTS yet with 50% more
search time than the second-order DARTS. This inspires
us to formulate a new single level method which is more
efÔ¨Åcient and accurate. Our main contribution is to introduce
a novel Relaxed Architecture Search (RARTS) method based
on single level optimization, and the computation of only
the Ô¨Årst-order partial derivatives of loss functions, for both
topology and width search of architectures. Through both
data and network splitting, the training objective (a relaxed
Lagrangian function) of RARTS allows two networks with
different but related weights to cooperate in the search of a
shared architecture.
We have carried out both analytical and experimental stud-
ies below to show that RARTS achieves better performance
than Ô¨Årst and second-order DARTS, with higher search efÔ¨Å-
ciency than second-order DARTS consistently:
Compare RARTS with DARTS directly on the ana-
lytical model with quadratic loss functions, where the
RARTS iterations approach the true global minimal
point missed by the Ô¨Årst-order DARTS, in a robust fash-
ion. A convergence theorem is proved for RARTS based
on descent of its Lagrangian function, and equilibrium
equations are discovered for the limits.On the CIFAR-10 based search of topological architec-
ture, the model found by RARTS obtains smaller size
and higher test accuracy than that by the second-order
DARTS with 65% search time saving. A hardware-
aware search option via a latency penalty in the La-
grangian function helps control the model size. Upon
transfer to ImageNet [28], [29], the model found by
RARTS achieves better performance as well, compared
with DARTS and its variants. Apart from the standard
search space used in the DARTS paper, RARTS also
beats DARTS on the public NAS benchmark of search
spaces like NATS-Bench [30].
For channel pruning of ResNet-164 [31] on CIFAR-
10 and CIFAR-100 [17] with Ô¨Åxed pruning ratio (per-
centage of pruned channels), RARTS outperforms the
differentiable pruning benchmarks: Network Slimming
[21] and TAS [20]. Comparisons between DARTS and
RARTS have also been made in a `1regularized (un-
Ô¨Åxed ratio) pruning task, where RARTS achieves a high
sparsity of 70% and exceeds DARTS in accuracy.
II. RELATED WORK
A. DIFFERENTIABLE ARCHITECTURE SEARCH
DARTS training relies on an iterative algorithm to solve
a bilevel optimization problem [14], [32] which involves
two loss functions computed via data splitting (splitting the
dataset into two halves, i.e. training data and validation data):
min
Lval(w(););
wherew() = arg min
wLtrain(w;):(1)
Herewdenotes the network weights, is the architecture
parameter,Ltrain andLvalare the loss functions computed
on the training data Dtrain and the validation data Dval.
Since many common datasets like CIFAR do not include
the validation data, Dtrain andDvalare usually two non-
overlapping halves of the original training data. We denote
Ltrain andLvalbyLtandLvto avoid any confusions
with the meaning of the subscripts. DtandDvare deÔ¨Åned
similarly. DARTS has adopted data splitting because it is
believed that joint training of both andwvia gradient
descent on the whole dataset by minimizing the overall loss
function:
L(w;) =Lt(w;) +Lv(w;) (2)
can lead to overÔ¨Åtting [14], [15]. Therefore, DARTS searches
for the architectures through a two-step differentiable algo-
rithm which updates the network weights and the architecture
parameters in an alternating way:
update weight wby descending along rwLt(w;)
update architecture parameter by descending along:
rLv(w rwLt(w;);)
where= 0 ( > 0) gives the Ô¨Årst or second-order
approximation. The bilevel optimization problem also arises
in hyperparameter optimization and meta-learning, where a
2 VOLUME 4, 2016

--- PAGE 3 ---
Author et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
second-order algorithm and a convergence theorem on mini-
mizers have been proposed in previous work [33] (Theorem
3.2), under the assumption that the -minimization is solved
exactly, and wt()converges uniformly to w(). However,
the-minimization of DARTS is approximated by gradient
methods only, and hence the convergence of DARTS algo-
rithm remains unknown theoretically.
We are aware of the fact that the Ô¨Årst-order DARTS up-
dates the architecture parameters on Dvby descending along
rLv(w;), which means it merely uses half of the data to
trainand might cause some convergence issues (see Fig.
1). MiLeNAS has developed a mixed-level solution, where
the architecture parameters can be learned on D=Dt[Dv
via a Ô¨Årst-order descending algorithm [15]:
wt+1=wt t
wrwLt(wt;t)
t+1=t t
 
rLt(wt+1;t) +rLv(wt+1;t)
;
(3)
We shall see that MiLeNAS is actually a constrained case
of RARTS when our two network splits become identical.
However, we point out that computing Lvusing an identical
network makes MiLeNAS still suffer from the same conver-
gence issue in a later example (Section III-D). The second-
order DARTS is observed to approximate the optimum better
than Ô¨Årst-order DARTS in a solvable model and through
experiments, yet it requires computing the mixed derivative
r2
;wLt, at a considerable overhead. Searching by DARTS
can also lead to the architecture collapse issue, meaning
the selected architecture contains too many skip-connections.
Typically such a bias in operation selection degrades the
model performance. SNAS [27], FBNet [12], and GDAS [18]
use differentiable Gumbel-Softmax to mimic one-hot encod-
ing which implies exclusive competition and risks of unfair
advantages [19]. This unfair dominance of skip-connections
in DARTS has also been noted by FairDARTS [19], which
has proposed a collaborative competition approach by mak-
ing the architecture parameters independent, through replac-
ing the softmax with sigmoid. They have further penalized
the operations in the search space with probability close to
1
2, i.e. a neutral and ambiguous selection. As these methods
focus on replacing some operations or the loss function,
it would be worthwhile to explore other solutions such as
replacing the gradient-based DARTS search algorithm.
In addition to DARTS, many other differentiable meth-
ods for architecture search have been proposed, considering
various aspects such as the search space, selection crite-
rion, and training tricks. SNAS [27] has discussed it from
a statistical perspective with however a minor to moderate
performance improvement. The search efÔ¨Åciency has also
been improved by sampling a portion of the search space
during each update in training. A perturbation-based selec-
tion scheme has been proposed in [34], as the magnitude
of architecture parameters is believed to be inadequate as
a selection criterion. P-DARTS [35] has adopted operation
dropout and regularization on skip-connections. From the
procedure side to delay a quick short cut aggregation, ithas also divided the search stage into multiple stages and
progressively adds more depth than DARTS. PC-DARTS
[36] samples a proportion of channels to reduce the bias of
operation selection and enlarge the batch size as well. GDAS
[18] searches the architecture with one operation sampled
at a time. Other approaches apply differentiable methods
on much larger search spaces with sampling techniques to
save memory and avoid model transfer [7], [12]. We will see
that these variants of the differentiable architecture search
method are actually complementary to our approach that ad-
vances DARTS on the purely algorithmic side by mobilizing
weights. Moreover, many works [7], [12], [16], [37]‚Äì[39]
manage to balance latency with the performance of the model
to enhance the efÔ¨Åciency of the model. Despite the broad use
of differentiable methods in the works we have mentioned,
one may wonder how DARTS and its variants beat random
search. A detailed comparison in [40] has elaborated the
advantage of DARTS in accuracy and efÔ¨Åciency compared
with random search.
B. SEARCH FOR WIDTH AND CHANNEL PRUNING
Differentiable search method has contributed to a wide range
of tasks other than topological architecture search. TAS [20]
searches for the width of each layer, i. e. number of channels,
by learning the optimal one from the aggregation of sev-
eral candidate feature maps via a differentiable method and
sampling. FasterSeg [16] searches for the cell operations and
layer width, as well as the multi-resolution network path over
the semantic segmentation task. These works of searching
width are closely related to channel pruning, which means
pruning redundant channels from the convolutional layers.
Among numerous methods to prune redundant channels [24],
[41]‚Äì[45], a classical approach is to apply group LASSO [46]
on the weights to identify unimportant channels. The weights
in each channel form one group, and the magnitude of each
group is measured by `2norm of its weights. The network
is trained by minimizing a loss function penalized by the
`1norm of these magnitudes from all groups. The channels
are pruned based on thresholding their norms. Selecting good
thresholds as hyperparameters for different channels can be
laborious for deep networks. On the other hand, channel
selection is intrinsically a network architecture issue. It is
debatable if thresholding by weight magnitudes is always
meaningful [47].
Another approach of channel pruning [21], [22] involves
assigning a channel scaling (scoring) factor to each channel,
which is a learnable parameter independent of the weights.
In the training process, the factors and the weights are
learned jointly, and the channels with low scaling factors
are pruned. After that, the optimal weights of the pruned
network are adjusted by one more stage of Ô¨Åne-tuning. In
terms of channel scaling factors, the channel pruning problem
becomes a special case of neural architecture search. Besides
this formulation, there are several pruning methods based on
NAS. AMC [37] has deÔ¨Åned a reward function and pruned
the channels via reinforcement learning. MetaPruning [48]
VOLUME 4, 2016 3

--- PAGE 4 ---
F . Xue et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
generates the best pruned model and weights from a meta
network.
III. METHODOLOGY
In this section, we introduce the RARTS formulation, its
iterative algorithm and convergence properties. RARTS is
different from all the differentiable algorithms we have men-
tioned, in that it puts forward a relaxed formulation of a single
level problem which beneÔ¨Åts from both data splitting and
network splitting.
A. DATA SPLITTING AND NETWORK SPLITTING
As pointed out in DARTS [14] and MiLeNAS [15], when
learning the architecture parameter , splitting training and
validation data should be taken into account to avoid overÔ¨Åt-
ting. However, we have discussed that the bilevel formulation
(1) and training algorithm of DARTS may lead to several
issues: unknown convergence, low efÔ¨Åciency and the unfair
selection of operations. Therefore, we follow the routine of
train-validation data splitting, but want to formulate a single
level problem, in contrast to DARTS and MiLeNAS. First, if
we use (w;), the pair of weight and architecture parameters
in Eq. (2) to represent a network, what we propose to do is
to further relax the network weights wvia splitting a network
copy denoted by (y;). We call (y;)and(w;)the primary
and auxiliary networks, which share the same architecture
and the same dimensions as weight tensors, but can have
different weight initialization.
Next, a primary loss Lv(y;)is computed with parameters
(y;)fed on data Dv, while an auxiliary loss Lt(w;)is
computed with parameters (w;)fed on dataDt. Note that
the computation of the auxiliary loss Lt(w;)is the same
as that of DARTS. The difference is that the primary loss is
computed on the primary network (y;), instead of (w;).
Now we present the single level objective of our relaxed
architecture search (RARTS) framework. With a `2penalty
on the distance between wandy, the two loss functions are
combined through the following relaxed Lagrangian L=
L(y;w; )of Eq. (2):
L:=Lv(y;) +Lt(w;) +1
2ky wk2
2; (4)
whereandare hyperparameters controlling the penalty
scale and the learning process. We will see in the search
algorithm that the penalty term enables the two networks to
exchange information and cooperate to search the architec-
ture which they share together. This technique of splitting w
andyis called network splitting , which is also inspired by
some previous work [49]. In their work, splitting of variables
is able to approximate a non-smooth minimization problem
via an algorithm of combined closed-form solutions and
gradient descent.
Since various NAS approaches discover architectures of
inconsistent sizes or FLOPS, it has made the comparison
through different methods unfair, because larger models are
likely to have better performance but low efÔ¨Åciency. ManyNAS methods have adopted latency as a model constraint
[7], [16]. To control model size, we follow the technique
of approximating the model latency with the sum of latency
from all the operations [16], and add the approximated la-
tency to the loss function as a penalty. Since each component
of the latency tensor (denoted by Lat) is the latency amount
associated with a candidate operation, the dimension of Lat
is the same as that of . Therefore, we provide an alternative
objective which is penalized by the latency of the model:
L:=Lv(y;) +Lt(w;) +1
2ky wk2
2
+hSoftmax();Lati; (5)
where the bracket is inner product.
B. RARTS ALGORITHM
We minimize the relaxed Lagrangian L(y;w; )in (4) by
iteration on the three variables in an alternating way to
allow individual and Ô¨Çexible learning schedules for the three
variables. Similar to Gauss-Seidel method in numerical linear
algebra [50], we use updated variables immediately in each
step and obtain the following three-step iteration:
wt+1=wt t
wrwL(yt;wt;t)
yt+1=yt t
yryL(yt;wt+1;t)
t+1=t t
rL(yt+1;wt+1;t):(6)
With explicit gradient rw;yky wk2
2, we have:
wt+1=wt t
wrwLt(wt;t) t
w(wt yt)
yt+1=yt t
yryLv(yt;t) t
y(yt wt+1)
t+1=t t
rLt(wt+1;t)
 t
rLv(yt+1;t):(7)
To minimize the Lagrangian (5), the Ô¨Årst two steps are the
same as Eq. (7) since the latency only depends on . The
third step becomes:
t+1=t t
rLt(wt+1;t)
 t
rLv(yt+1;t)
 t
rhSoftmax(t);Lati:(8)
Note that the update of in Eq. (7) involves both Lt
andLv, which is similar to the second-order DARTS but
without the mixed second derivatives. The Ô¨Årst-order DARTS
usesrLvonly in this step. In the previous section, we
have discussed the architecture collapse issue of DARTS, i.e.,
selecting to many skip-connections. A possible reason why
DARTS may lead to architecture collapse is that its architec-
ture parameters converge more quickly than the weights in
the convolutional layers. That means, when DARTS selects
architecture parameters, it tends to select skip-connection
operations, since the convolutional layers are not trained
well. The fact that Ô¨Årst-order DARTS has only used one of
the two data splits to train the weights, makes the training
of convolutional layers worse. For RARTS, we make use of
bothLtandLvto update the weight parameters wandyin
4 VOLUME 4, 2016

--- PAGE 5 ---
Author et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
the Ô¨Årst two steps of Eq. (7). In the third step of Eq. (7), both
LtandLvare also used to update the shared architecture .
In this way, the architecture is learned better, as more data
are involved during training. If y=wis enforced in Eq.
(7) e.g. through a multiplier, RARTS essentially reduces to
Ô¨Årst-order MiLeNAS [15]. However, relaxing to y6=whas
its advantages of having more generality and robustness as
it is optimized on two networks with different but related
weights. In contrast, MiLeNAS trains the network weigts
on the training data Dtonly, and suffers from the same
convergence issue as Ô¨Årst-order DARTS (Section III-D). We
summarize the RARTS algorithm in Algorithm 1.
Algorithm 1 Relaxed Architecture Search (RARTS)
Input : the number of iterations N, the hyperparameters
and, a learning rate schedule (t
w;t
u;t
), initialization
of the weight parameters w0,u0and the architecture param-
eters0.
Output :, the architecture we want.
Split the dataset Dinto two subsets DpandDa.
fort= 0;1;:::;N do
ComputeLpandLaonDpandDa, respectively, and
then compute Lusing Eq. (4)
Update the parameters via gradient descent:
wt+1=wt t
wrwL(yt;wt;t)
yt+1=yt t
yryL(yt;wt+1;t)
t+1=t t
rL(ut+1;wt+1;t)
end for
C. CONVERGENCE ANALYSIS
Suppose that LtandLvboth satisfy Lipschitz gradient prop-
erty, or there exist positive constants L1andL2such that
(z= (y;),z0= (y0;0)):
krzLv(z) rzLv(z0)kL1kz z0k;8(z;z0);
which implies:
Lv(z) Lv(z0)hrzLv(z0);(z z0)i+L1
2kz z0k2;
for any (z;z0); similarly (= (w;),0= (w0;0)):
krLt() rLt(0)kL2k 0k;8(;0);
which implies:
Lt() Lt(0)hrLt(0);( 0)i+L2
2k 0k2;
for any (;0).
Theorem 1. Suppose that the loss functions LtandLv
satisfy Lipschitz gradient property. If the learning rates t
w,
t
yandt
are small enough depending only on the Lipschitz
constants as well as (;), and approach nonzero limit at
larget, the Lagrangian function L(y;w; )is descending
on the iterations of (7). If additionally the Lagrangian L
is lower bounded and coercive (its boundedness implies
that of its variables), the sequence (yt;wt;t)convergessub-sequentially to a critical point (y;w;)ofL(y;w; )
obeying the equilibrium equations:
rwLt( w;) +( w y) = 0;
ryLv(y;) +(y w) = 0;
rLt( w;) +rLv(y;) = 0: (9)
If the loss is penalized by latency as in (5), the last equilib-
rium equation becomes:
rLt( w;) +rLv(y;)
+rhSoftmax();Lati= 0:(10)
Proof. We only need to prove for the loss (5) and the itera-
tions (8), as the loss (4) is its special case when Lat = 0 . We
notice the latency penalty function hSoftmax(t);Latialso
satisÔ¨Åes the Lipschitz gradient property. This is because
rSoftmax(t) = diag(Softmax( t))
 Softmax(t)
(Softmax(t))0;
and hence all the Ô¨Årst and second derivatives of
hSoftmax(t);Latiare bounded uniformly regardless of t.
Applying Lipschitz gradient inequalities on LvandLt, we
have:
L(yt+1;wt+1;t+1) L(yt;wt;t)
=Lv(yt+1;t+1) +Lt(wt+1;t+1)
+
2kyt+1 wt+1k2+hSoftmax(t+1);Lati
 Lv(yt;t) Lt(wt;t) 
2kyt wtk2
 hSoftmax(t);Lati
 hry;Lv(yt;t);(yt+1 yt;t+1 t)i
+L1
2k(yt+1 yt;t+1 t)k2
+hrw;Lt(wt;t);(wt+1 wt;t+1 t)i
+L2
2k(wt+1 wt;t+1 t)k2
+
2(kyt+1 wt+1k2 kyt wtk2)
+hrhSoftmax(t);Lati;t+1 t)i
+L3
2kt+1 tk2:
Substituting for the (w;y)-gradients from the iterations (8),
we continue:
L(yt+1;wt+1;t+1) L(yt;wt;t)
   (t
y) 1
hyt+1 yt+t
y(yt wt+1);yt+1 yti
+hrLv(yt;t) +rLt(wt;t);t+1 ti
+( t
w) 1
hwt+1 wt+t
w(wt yt);wt+1 wti
+L1
2kyt+1 ytk2+L1+L2+L3
2kt+1 tk2
VOLUME 4, 2016 5

--- PAGE 6 ---
F . Xue et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
FIGURE 1. Learning trajectories of RARTS approach the global minimal point (1;1)of the solvable model at suitable values of ,andy0(= 10 in middle/right
subplots,= 10 in left/right subplots, y0= 0in left/middle subplots), compared with that of the baseline (Ô¨Årst-order DARTS).
+L2
2kwt+1 wtk2
+
2(kyt+1 wt+1k2 kyt wtk2)
+hrhSoftmax(t);Lati;t+1 t)i
= ( (t
y) 1+L1=2)kyt+1 ytk2
+( (t
w) 1+L2=2)kwt+1 wtk2
 hyt wt+1;yt+1 yti
 hwt yt;wt+1 wti
+
2(kyt+1 wt+1k2 kyt wtk2)
+hrLv(yt;t) +rLt(wt;t);t+1 ti
+L1+L2+L3
2kt+1 tk2
+hrhSoftmax(t);Lati;t+1 t)i: (11)
We note the following identity
kyt+1 wt+1k2
=kyt+1 wt+wt wt+1k2
=kyt+1 wtk2+ 2hyt+1 wt;wt wt+1i
+kwt wt+1k2;
where
kyt+1 wtk2
=k wt+yt yt+yt+1k2
=kyt wtk2+ 2hyt wt;yt+1 yti+kyt+1 ytk2:
Upon substitution of the above in the right hand side of (11),
we Ô¨Ånd that:
L(yt+1;wt+1;t+1) L(yt;wt;t)
( (t
y) 1+L1=2 +=2)kyt+1 ytk2
+( (t
w) 1+L2=2 +=2)kwt+1 wtk2
+hwt+1 wt;yt+1 yti
+hyt+1 yt;wt wt+1i
+hrLv(yt;t) +rLt(wt;t);t+1 ti+L1+L2+L3
2kt+1 tk2
+hrhSoftmax(t);Lati;t+1 t)i:
The-terms cancel out. Substituting for the -gradient from
the iterations (8), we get:
L(yt+1;wt+1;t+1) L(yt;wt;t)
( (t
y) 1+L1=2 +=2)kyt+1 ytk2
+( (t
w) 1+L2=2 +=2)kwt+1 wtk2
+( (t
) 1+L1+L2+L3
2)kt+1 tk2
+hrLv(yt;t) rLv(yt+1;t);t+1 ti
+hrLt(wt;t) rLt(wt+1;t);t+1 ti
where the last two inner product terms are upper bounded by:
(1 +)L4(kyt yt+1k+kwt wt+1k)kt+1 tk;
for positive constant L4:= max(L1;L2). It follows that:
L(yt+1;wt+1;t+1) L(yt;wt;t)

 (t
y) 1+L1
2+
2+(1+)L4
2
kyt+1 ytk2
+
 (t
w) 1+L2
2+
2+(1+)L4
2
kwt+1 wtk2
+
 (t
) 1+L1+L2+L3
2+(1+)L4
2
kt+1 tk2: (12)
If
t
y<1
2L1
2+
2+ (1 +)L4
2 1
:=c1;
t
w<1
2L2
2+
2+ (1 +)L4
2 1
:=c2;
t
<1
2L1+L2+L3
2+(1+)L4
2 1
:=c3;
6 VOLUME 4, 2016

--- PAGE 7 ---
Author et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
Lis descending along the sequence (yt;wt;t). Forc4=
1
2minfc 1
1;c 1
2;c 1
3g, it follows from (12) that:
c4k(yt+1 yt;wt+1 wt;t+1 t)k2
L(yt;wt;t) L(yt+1;wt+1;t+1)!0
ast!+1, implying that
lim
t!1k(yt+1 yt;wt+1 wt;t+1 t)k= 0:
SinceLis lower bounded and coercive, k(yt;wt;t)kare
uniformly bounded in t. Let (t
w;t
y;t
)tend to non-zero
limit at large t. Then (yt;wt;t)sub-sequentially converges
to a limit point (y;w;)satisfying the equilibrium system
(9) or (10).
D. A SOLVABLE BILEVEL MODEL
We compare a few differentibale methods through an ex-
ample [14] which has an analytical solution. Regardless of
the latency penalty, we consider quadratic functions Lv=
w 2+ 1,Lt=w2 2w+2for the bilevel problem
(1). Therefore, the solution to the inner level problem is:
w() = arg min
wLt(w;) =:
ThenLv(w();) =2 2+1, and the global minimizer
of this bilevel problem is (w;) = (1;1). However, the
equilibrium equations of Ô¨Årst-order DARTS is:
rwLt( w;) = 0
rLv( w;) = 0;
which gives a spurious equilibrium ( w;) = (2;2). The
equilibrium equations of Ô¨Årst-order MiLeNAS is:
rwLt( w;) = 0
rLt( w;) +rLv( w;) = 0;
which also results in the spurious equilibrium ( w;) =
(2;2).
On the other hand, RARTS can approximate the correct
minimizer (w;) = (1;1)better. Note that both Lvand
Ltsatisfy the Lipschitz gradient property, which implies
the descent of Lagrangian Lby the proof of Theorem 1.
If > 1=2, > 3=2,Lis bounded and coercive, which
follows from an eigenvalue analysis of linear system (7) and
is observed in computation. Hence, Theorem 1 can be applied
to this example, and the equilibrium system (9) reads:
(2 w 2) +( w y) = 0; (13)
+(y w) = 0; (14)
( 2 w+ 2) + y 2 = 0: (15)
Adding (13) and (14) gives: w=2 1
2, which to-
gether with (15) determines (;w;y)uniquely: (;w;y) =
(4
4  2;4 2
4  2;4 2 4
4  2), if4  26= 0. At
== 15 ,(;w;y)(1:053;1:018;0:947) where global
convergence holds for the whole RARTS sequence. The learn-
ing dynamics starting from (0;w0;y0) = (2; 2;y0), isreproduced in Fig. 1, along with three learning curves from
RARTS as the parameters (;)and the initial value y0vary.
In Fig. 1a,= 10 ,y0= 0. In Fig. 1b, = 10 ,y0= 0. In
Fig. 1c,== 10 . In all experiments, the learning rates
are Ô¨Åxed at 0:01. For a range of (;)andy0, we see that
our learning curves enter a small circle around (1;1), while
Ô¨Årst-order DARTS converges to the spurious point.
IV. EXPERIMENTS
We show by a series of experiments how RARTS works
efÔ¨Åciently for different tasks: the search for topology and the
search for width, on various datasets and search spaces.
A. SEARCH FOR TOPOLOGY
For the hyperparameters and settings like learning rate sched-
ules, number of epochs for CIFAR-10 and the transfer learn-
ing technique for ImageNet, we follow those of DARTS [14].
We also consider the results on CIFAR-10 and CIFAR-100
for NATS-Bench [30], which is another benchmark search
space.
Comparisons on CIFAR-10. The CIFAR-10 dataset con-
sists of 50,000 training images and 10,000 test images [17].
These 3-channel images of 3232resolutions are allocated
to 10 object classes evenly. For the architecture search task
on CIFAR-10, the DtandDvdata we have used are random
non-overlapping halves of the original training data, the
same as DARTS. The settings for searching topology with
RARTS follows those of DARTS. That is, batch size = 64,
initial weight learning rate = 0.025, momentum = 0.9, weight
decay = 0.0003, initial alpha learning rate = 0.0003, alpha
weight decay = 0.001, epochs = 50. For the stage of training,
batch size = 96, learning rate = 0.025, momentum = 0.9,
weight decay = 0.0003 [14]. For each cell (either normal or
reduction), 8 edges are selected, with 1 out of 8 candidate
operations selected for each edge (see Fig. 2). Besides the
standard`2regularization of the weights, we also adopt the
latency penalty. The latency regularization loss is weighted
so that it is balanced with other loss terms. Typically, if
we increase the latency weight, the model we Ô¨Ånd will be
smaller in size. The latency term Lat for each operation is
measured via PyTorch/TensorRT [16], and thus it depends
on the devices we use. For the current search, the latency
weight is 0.002 so that the model size is comparable to those
in prior works. The Ô¨Ånal latency loss is the weighted sum of
the latency from each operation, where the weights are the
architecture parameters.
As shown in Table 1, the search cost of RARTS is 1:1
GPU days, far less than that of the second-order DARTS.
The test error of RARTS is 2:65%, outperforming the 3:00%
of the Ô¨Årst-order DARTS and the 2:76% of the second-
order DARTS. It should also be pointed out that the model
found by RARTS has 3.2M parameters, which is smaller
than the 3.3M model found by DARTS. Moreover, RARTS
outperforms other recent differentiable methods in accuracy
and search cost at comparable model size. We also notice
that the variance of RARTS performance is lower than that of
VOLUME 4, 2016 7

--- PAGE 8 ---
F . Xue et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
TABLE 1. Comparison of DARTS, RARTS and other methods on CIFAR-10 based network search. DARTS-1/2 stands for DARTS 1st/2nd-order, SNAS-Mi/Mo
stands for SNAS plus mild/moderate constraints. Note that faster search times also depend on speed and memory capacity of local machines used. The V100
column indicates whether the model is trained on high-end Tesla V100 GPUs or not. Each run of our experiment is conducted on a single GTX 1080 Ti GPU. The
numbers in the parentheses indicate the search GPU days of DARTS on our machine. Average of 5 runs. These runs are conducted on our machine.
Method Test Error (%) Para. (M) V100Search
GPU Days
Random Baseline [14] 3.29 0.15 3.2 7 4
AmoebaNet-B [10] 2.55 0.05 2.8 7 3150
SNAS-Mi [27] 2.98 2.9 7 1.5
SNAS-Mo [27] 2.85 0.02 2.8 7 1.5
DARTS-1 [14] 3.00 0.14 3.3 7 1.5 (0.7)
DARTS-2 [14] 2.76 0.09 3.3 7 4 (3.1)
GDAS [18] 2.82 2.5 3 0.2
ProxylessNAS [7] 2.08 5.7 3 4.0
FairDARTS [19] 2.54 0.05 3.3 3 0.4
FairDARTS [19] 2.94 0.05 3.2 7 0.3
P-DARTS [35] 2.50 3.4 3 0.3
PC-DARTS [36] 2.57 0.07 3.6 3 0.1
PC-DARTS [36] 2.71  2.9 7 0.1
MiLeNAS [15] 2.80 0.04 2.9 3 0.3
MiLeNAS [15] 2.51 0.11 3.9 3 0.3
RARTS 2.65 0.07 3.2 7 1.1
TABLE 2. Comparison of the latency for the models found under different hyperparameters. The setting of batch size = 64, learning rate = 310 4, weight decay
=110 3is consistent with the settings of DARTS and other DARTS variants, and is selected to be our baseline setting.
Latency Weight Batch Size Learning Rate Weight Decay Latency (ms)
210 364 310 4110 321.7
210 264 310 4110 312.4
210 464 310 4110 323.4
210 364 310 4110 321.7
210 364 310 3110 321.0
210 364 310 5110 323.1
210 364 310 4110 321.7
210 364 310 4110 421.3
210 364 310 4110 222.9
210 364 310 4110 321.7
210 332 310 4110 320.1
210 316 310 4110 316.5
DARTS. RARTS has also arrested architecture collapse and
only selected one skip-connection, as shown in Fig. 2. We are
aware that different values of hyperparameters in the RARTS
search stage may impact the latency of the models found by
RARTS. Table 2 has listed the latency of several models with
different hyperparameters. Here we use the baseline setting
of latency weight = 210 3, batch size = 64, learning
rate = 310 4, weight decay = 110 3. We change the
value of one hyperparameter and keep the others the same
during each experiment, so that we can see how sensitive
the resulting latency is to a speciÔ¨Åc hyperparameter. First,
the result shows that a small batch size of 16 can impact
the model‚Äôs latency, whereas a batch size of 32 or 64 can
lead to similar latency. This is a positive phenomenon, since
we prefer larger batch size as it requires less training time.
Among the other hyperparameters, it is clear that the only
factor that could cause a signiÔ¨Åcant difference is the latency
weight. A latency weight of 210 2is so large that its
model has only 60% latency compared with the baseline. Themodel‚Äôs latency is not sensitive to the other hyperparameters,
as the latency is around 22.0, and varies within 10% only.
This Ô¨Ånding is beneÔ¨Åcial, since we can Ô¨Åx the latency level
via Ô¨Åxing the latency weight and Ô¨Ånd the model with the best
accuracy among the models of similar latency level via tuning
the other hyperparameters.
Comparisons on ImageNet. ImageNet [28], [29] is com-
posed of over 1.2 million training images and 5,000 test
images from 1,000 object classes. The architecture which is
built of the cells learned on CIFAR-10 is transferred to be
learned on ImageNet-1000, producing the results in Table 3.
Even if our experiments are performed on a GTX 1080 Ti
whose maximum memory allows only a batch size of 128,
our 25.9% error rate outperforms those of DARTS and SNAS
(batch size 128), and is also comparable to those of GDAS
(batch size 128) and MiLeNAS. MiLeNAS among some
other algorithms in Table 2 have been implemented on Tesla
V100 with batch size 1024, a much higher end hardware
than that in our experiments. This partly explains its lower
8 VOLUME 4, 2016

--- PAGE 9 ---
Author et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
TABLE 3. Transfer to ImageNet: test error comparison of DARTS, RARTS and other methods on local machines resp. The V100 column indicates whether the
model is trained on high-end Tesla V100 GPUs or not. The larger GPU memory can support larger batch size, which leads to better accuracy and training efÔ¨Åciency
on ImageNet. The Direct column indicates if the model is searched directly on ImageNet without transfer-learning. The direct search tends to be more accurate but
costs more computational resources.
Method Top-1 (%) Top-5 (%) Parameters (M) V100 Direct
SNAS [27] 27.3 9.2 4.3 7 7
DARTS [14] 26.7 8.7 4.7 7 7
GDAS [18] 26.0 8.5 5.3 3 7
ProxylessNAS [7] 24.9 7.5 7.1 3 3
FairDARTS [19] 24.9 7.5 4.8 3 7
FairDARTS [19] 24.4 7.4 4.3 3 3
P-DARTS [35] 24.4 7.4 4.9 3 7
PC-DARTS [36] 25.1 7.8 5.3 3 7
PC-DARTS [36] 24.2 7.3 5.3 3 3
MiLeNAS [15] 25.4 7.9 4.9 3 7
RARTS 25.9 8.3 4.7 7 7
ùëêùëò‚àí1
ùëêùëò‚àí2013ùëêùëò2sep_conv_5x5
sep_conv_3x3
sep_conv_3x3skip_connectsep_conv_5x5
sep_conv_3x3avg_pool_3x3
avg_pool_3x3
ùëêùëò‚àí1
ùëêùëò‚àí2ùëêùëò01
23sep_conv_3x3
sep_conv_5x5
dil_conv_3x3
dil_conv_3x3dil_conv_5x5
sep_conv_5x5
sep_conv_3x3sep_conv_3x3
FIGURE 2. The architecture of the normal (top) and reduction (bottom) cells
found by RARTS. This architecture contains only one skip connection. The last
four edges are simply concatenated together to construct the next cell. So
there is no search along these edges, following the convention of DARTS [14].
accuracy occurrence (2.80) on CIFAR-10 but higher accuracy
after transfer to ImageNet. Typically ImageNet is trained
better on larger GPU‚Äôs because of the larger batch size.
ProxylessNAS has obtained high accuracy on both CIFAR-
10 and ImageNet, but their models are much larger than the
other methods. It has avoided transfer learning as the training
cost is reduced via path sampling. Inheriting the building
blocks from DARTS and ProxylessNAS, FairDARTS has
penalized the neutral (close to 0.5) architecture parameters,
but its high accuracy also beneÔ¨Åts from the relaxation on the
search space. Their normal cells contain less than 8 opera-
tions since the operations with architecture parameters lower
than a preset threshold are eliminated. This explains their
smaller model size and comparable accuracy. P-DARTS has
devised a progressive method to increase the depth of search.
Their work shows that deeper cells have better capability of
representation, which is also an improvement on the search
space. PC-DARTS as a sampling method has achieved the
least searching cost and can be trained directly on ImageNet.
These methods are complementary to our work which is
purely on the differentiable search algorithm without mod-
ifying the search space of DARTS.
Comparisons on NATS-Bench. For NATS-Bench, oneTABLE 4. Test errors of DARTS vs. RARTS on NATS-Bench search space.
The results of DARTS on NATS-Bench are from [30]. Ratio = the number of
skip-connections over the number of total operations in the discovered
architecture.
Dataset Method Error (%) Ratio (%)
DARTS-1 40.16 100
CIFAR-10 DARTS-2 34.62 100
RARTS 11.48 0
DARTS-1 38.74 38.9
CIFAR-100 DARTS-2 39.51 38.9
RARTS 32.37 0
has to search a block of 6 nodes from the search space of 5
different operations, including zero, skip-connection, 33
average pooling, 11convolution or 33convolution
[30]. Therefore, it includes 15,625 different candidate ar-
chitectures and any DARTS style methods can be adapted
easily to its search space. NATS-Bench has measured each
architecture‚Äôs performance under the same training settings,
and hence fair comparisons can be made between the dis-
covered architectures since no further evaluation is needed
on the local machines. In our experiments, we set batch size
= 64, initial weight learning rate = 0.025, momentum = 0.9,
weight decay = 0.0005, initial alpha learning rate = 0.0003,
alpha weight decay = 0.001, number of epochs = 100. Table 4
presents the search results of DARTS vs. RARTS on NATS-
Bench. RARTS has surpassed both DARTS-1 and DARTS-
2 in accuracy by more than 20% on CIFAR-10 and 6% on
CIFAR-100. Besides its success in accuracy, RARTS has
totally escaped from the architecture collapse issue, i. e., the
architectures found by RARTS from NATS-Bench contain no
skip-connections. On the contrary, both architectures found
by DARTS-1 and DARTS-2 contain 100% and 38.9% (aver-
age of 3 runs) skip-connections on CIFAR-10 and CIFAR-
100, respectively. It is clear that too many skip-connections
resulting in architecture collapse will impact the performance
of the models greatly.
VOLUME 4, 2016 9

--- PAGE 10 ---
F . Xue et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
B. SEARCH FOR WIDTH
To search the width of the architecture (number of channels
in convolutional layers), we follow the settings of Network
Slimming [21], by introducing scoring parameters to mea-
sure channel importance. Denote the original feature map by
Fi;jand deÔ¨Åne the new feature map ~Fi;j=i;jFi;j, where
(i;j)are the layer and channel indices. Multiplying a channel
of output feature map by is equivalent to multiplying the
convolutional kernels connecting to this output feature map
by the same. We prune a channel if the corresponding is 0
or very small. The ij‚Äôs are learnable architecture parameters
independent of channel weights, and hence is considered to
have similar roles to the architecture parameters in the case
of searching topological architecture.
Although such treatment of scoring parameters is much
like that in Network Slimming [21], we point out that the sin-
gle level formulation of RARTS and the training algorithm to
learn those scoring parameters are novel. The Ô¨Årst difference
is that Network Slimming trains both weight and architecture
parameters on the whole (training and validation) data, unlike
DARTS or RARTS, without using either dataset splitting or
network splitting. Another key difference between RARTS
pruning and Network Slimming is in the search algorithm,
i.e., Network Slimming trains the weights and the archi-
tecture jointly in one step, while RARTS trains them in a
three-step iteration. Moreover, Network Slimming has used
batch normalization weights as the scoring parameters. We
point out that we could still deÔ¨Åne such a set of learnable
architecture parameters , even if the batch normalization
operation is not contained in the architecture.
We also compare RARTS with TAS [20], which is another
width search method based on differentiable NAS, relying
on both continuous relaxation via feature maps of various
sizes and model distillation . The Ô¨Årst difference is on how the
channel scoring parameters are applied to the feature maps.
For TAS, the channel parameters are treated as probabilities
of candidate feature maps, smoothed by Gumbel-Softmax.
Then a subset of feature maps is sampled to alleviate the high
memory costs. RARTS is much simpler in its formulation, as
it is a dot product of the channel parameters with the Ô¨Ålter to
be pruned. The second key difference is the use of a training
technique called Knowledge Distillation (KD) [51] by TAS to
improve accuracy. There are some other NAS based methods
for width search, or channel pruning [37], [48] mentioned in
Section II-B. Noting that our formulation of the problem and
the criterion to evaluate results are different, we emphasize
that out progress is in fusion of a new search algorithm and
the width search task.
When using RARTS to search for width, we follow the
hyperparameters and settings of Network Slimming as well.
That is, learning rate = 0.1, weight decay = 0.0001, epochs
= 160 [21]. In Table 5, RARTS outperforms the un-pruned
baseline, Network Slimming (NS) and TAS [20] by over 10%
error reduction on CIFAR-10. While TAS does not offer an
option to specify the pruning ratio of channels (PRC), the
pruning ratio of FLOPs is around 30% for NS (40% PRC),TABLE 5. Application of RARTS to ResNet-164 (baseline, 1.7 M parameters)
channel pruning on CIFAR-10 and CIFAR-100, in comparison with the
baseline, TAS and Network Slimming. The numbers in the parentheses
indicate the pruning ratio of channels (PRC). For NS and RARTS, PRC is Ô¨Åxed
at 40% or 60%. NS = Network Slimming.
Data Method Test Error (%)
CIFAR-10Baseline [21] 5.42
TAS [20] 6.00
NS (40% PRC) [21] 5.08
RARTS (40% PRC) 4.58
NS (60% PRC) [21] 5.27
RARTS (60% PRC) 4.90
CIFAR-100Baseline [21] 23.37
TAS [20] 22.24
NS (40% PRC) [21] 22.87
RARTS (40% PRC) 22.64
NS (60% PRC) [21] 23.91
RARTS (60% PRC) 23.26
TABLE 6. Application of RARTS to MobileNetV2 pruning on the ImageNet-R
dataset (a randomly sampled subset of ImageNet-1000, with 20 object
classes), compared with the baseline, random pruning, 1st and 2nd-order
DARTS. Here random pruning means that we zero out channels randomly in
accordance with the pruning ratio of RARTS. Average of 5 runs. PRC = the
average pruning ratio of channels over the pruned layers. We note that the
PRC can be high because the dataset is much smaller.
Method Test Error. (%) PRC (%)
Baseline 12.3 1.4 -
Random Pruning 12.0 1.1 71.2 1.9
DARTS-1 10.1 2.0 69.0 0.9
DARTS-2 9.8 1.7 72.6 2.0
RARTS 8.21.9 71.21.9
RARTS (40% PRC) and TAS. So the comparison is fair. On
CIFAR-100, RARTS still leads NS at the same PRC. The
gap is smaller as the baseline network is less redundant.
Our experimental results reveal that the accuracy of TAS
with KD is lower than (on CIFAR-10) or similar to (on
CIFAR-100) that of RARTS, while TAS without the training
technique like KD is 2% worse [20]. This supports the fact
that RARTS works better as a differentiable method for width
search, without regard to any other training tricks. Apart from
the comparisons with the above methods, we also consider
a pruning task for comparing DARTS and RARTS, which
can be viewed as an ablation study of RARTS on the width
search task. For this task, we prune MobileNetV2 [52] on a
randomly sampled 20-class subset of ImageNet-1000, with
`1regularization but unÔ¨Åxed pruning ratio. The pruning ratio
can be learned automatically by the strong regularization
term, as many of the architecture parameters are simply zero.
Table 6 shows that RARTS also beats both random pruning
and DARTS in accuracy. Even though the 2nd DARTS ob-
tains a higher sparsity, it sacriÔ¨Åces the accuracy.
V. CONCLUSION
We have developed RARTS, a novel relaxed differentiable
method for neural architecture search. We have proved its
convergence theorem and compared it with DARTS on an
analytically solvable model. Thanks to the design of data and
10 VOLUME 4, 2016

--- PAGE 11 ---
Author et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
network splitting, RARTS has achieved high accuracy and
search efÔ¨Åciency over the state-of-the-art differentiable meth-
ods, especially DARTS, with a wide range of experiments, in-
cluding both topology search and width search. These results
support RARTS to be a more reliable and robust differen-
tiable neural architecture search tool for various datasets and
search spaces. In future work, we plan to incorporate search
space sampling and regularization techniques to accelerate
RARTS (as seen in several recent variants of DARTS) for
broader applications in deep learning.
ACKNOWLEDGMENT
The authors would like to thank the associate editor and
the anonymous referees for their careful reading and helpful
feedback, which improved the presentation of the paper.
The authors would also like to express their appreciation
to Dr. Shuai Zhang and Dr. Jiancheng Lyu for wonderful
discussions related to the project.
REFERENCES
[1] B. Zoph and Q. V . Le, ‚ÄúNeural architecture search with reinforcement
learning,‚Äù ICLR, 2017; arXiv preprint arXiv:1611.01578 , 2016.
[2] B. Zoph, V . Vasudevan, J. Shlens, and Q. V . Le, ‚ÄúLearning transferable
architectures for scalable image recognition,‚Äù in Proceedings of the IEEE
conference on computer vision and pattern recognition , 2018, pp. 8697‚Äì
8710.
[3] G. Ghiasi, T.-Y . Lin, and Q. V . Le, ‚ÄúNas-fpn: Learning scalable feature
pyramid architecture for object detection,‚Äù in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2019, pp. 7036‚Äì
7045.
[4] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and
L. Fei-Fei, ‚ÄúAuto-deeplab: Hierarchical neural architecture search for
semantic image segmentation,‚Äù in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , 2019, pp. 82‚Äì92.
[5] T. Elsken, J. H. Metzen, and F. Hutter, ‚ÄúNeural architecture search: A
survey,‚Äù The Journal of Machine Learning Research , vol. 20, no. 1, pp.
1997‚Äì2017, 2019.
[6] P. Ren, Y . Xiao, X. Chang, P.-Y . Huang, Z. Li, X. Chen, and X. Wang,
‚ÄúA comprehensive survey of neural architecture search: Challenges and
solutions,‚Äù ACM Computing Surveys (CSUR) , vol. 54, no. 4, pp. 1‚Äì34,
2021.
[7] H. Cai, L. Zhu, and S. Han, ‚ÄúProxylessnas: Direct neural architec-
ture search on target task and hardware,‚Äù ICLR, 2019; arXiv preprint
arXiv:1812.00332 , 2018.
[8] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, F.-F. Li,
A. Yuille, J. Huang, and K. Murphy, ‚ÄúProgressive neural architecture
search,‚Äù in Proceedings of the European Conference on Computer Vision
(ECCV) , 2018, pp. 19‚Äì34.
[9] H. Pham, M. Y . Guan, B. Zoph, Q. V . Le, and J. Dean, ‚ÄúEfÔ¨Åcient neural
architecture search via parameter sharing,‚Äù ICML, 2018; arXiv preprint
arXiv:1802.03268 , 2018.
[10] E. Real, A. Aggarwal, Y . Huang, and Q. V . Le, ‚ÄúRegularized evolution for
image classiÔ¨Åer architecture search,‚Äù in Proceedings of the aaai conference
on artiÔ¨Åcial intelligence , vol. 33, 2019, pp. 4780‚Äì4789.
[11] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu,
and D. Marculescu, ‚ÄúSingle-path nas: Designing hardware-efÔ¨Åcient con-
vnets in less than 4 hours,‚Äù in Joint European Conference on Machine
Learning and Knowledge Discovery in Databases . Springer, 2019, pp.
481‚Äì497.
[12] B. Wu, X. Dai, P. Zhang, Y . Wang, F. Sun, Y . Wu, Y . Tian, P. Vajda,
Y . Jia, and K. Keutzer, ‚ÄúFbnet: Hardware-aware efÔ¨Åcient convnet de-
sign via differentiable neural architecture search,‚Äù in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.
10 734‚Äì10 742.
[13] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, ‚ÄúSingle
path one-shot neural architecture search with uniform sampling,‚Äù in Euro-
pean Conference on Computer Vision . Springer, 2020, pp. 544‚Äì560.[14] H. Liu, K. Simonyan, and Y . Yang, ‚ÄúDarts: Differentiable architecture
search,‚Äù in ICLR 2019 , 2019. [Online]. Available: https://www.microsoft.
com/en-us/research/publication/darts-differentiable-architecture-search/
[15] C. He, H. Ye, L. Shen, and T. Zhang, ‚ÄúMilenas: EfÔ¨Åcient neural ar-
chitecture search via mixed-level reformulation,‚Äù in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 11 993‚Äì12 002.
[16] W. Chen, X. Gong, X. Liu, Q. Zhang, Y . Li, and Z. Wang, ‚ÄúFasterseg:
Searching for faster real-time semantic segmentation,‚Äù ICLR, 2020; arXiv
preprint: 1912.10917 , 2019.
[17] A. Krizhevsky, G. Hinton et al. , ‚ÄúLearning multiple layers of features from
tiny images,‚Äù Citeseer, Tech. Rep., 2009.
[18] X. Dong and Y . Yang, ‚ÄúSearching for a robust neural architecture in four
gpu hours,‚Äù in Proceedings of the IEEE Conference on computer vision
and pattern recognition , 2019, pp. 1761‚Äì1770.
[19] X. Chu, T. Zhou, B. Zhang, and J. Li, ‚ÄúFair darts: Eliminating unfair
advantages in differentiable architecture search,‚Äù in European conference
on computer vision . Springer, 2020, pp. 465‚Äì480.
[20] X. Dong and Y . Yang, ‚ÄúNetwork pruning via transformable architecture
search,‚Äù in Advances in Neural Information Processing Systems , 2019, pp.
760‚Äì771.
[21] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, ‚ÄúLearning efÔ¨Åcient
convolutional networks through network slimming,‚Äù in Proceedings of the
IEEE International Conference on Computer Vision , 2017, pp. 2736‚Äì2744.
[22] Z. Huang and N. Wang, ‚ÄúData-driven sparse structure selection for deep
neural networks,‚Äù in Proceedings of ECCV , 2018.
[23] K. Bui, F. Park, S. Zhang, Y . Qi, and J. Xin, ‚ÄúImproving network slimming
with nonconvex regularization,‚Äù IEEE Access , vol. 9, pp. 115 292‚Äì115 314,
2021.
[24] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li, ‚ÄúLearning structured sparsity
in deep neural networks,‚Äù in Advances in neural information processing
systems , 2016, pp. 2074‚Äì2082.
[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in
neural information processing systems , vol. 30, 2017.
[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-
terthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. , ‚ÄúAn
image is worth 16x16 words: Transformers for image recognition at scale,‚Äù
arXiv preprint arXiv:2010.11929 , 2020.
[27] S. Xie, H. Zheng, C. Liu, and L. Lin, ‚ÄúSNAS: stochastic neural architecture
search,‚Äù ICLR, 2019; arXiv preprint arXiv:1812.09926 , 2018.
[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li, ‚ÄúImagenet:
A large-scale hierarchical image database,‚Äù in 2009 IEEE conference on
computer vision and pattern recognition . IEEE, 2009, pp. 248‚Äì255.
[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein et al. , ‚ÄúImagenet large scale visual
recognition challenge,‚Äù International journal of computer vision , vol. 115,
no. 3, pp. 211‚Äì252, 2015.
[30] X. Dong, L. Liu, K. Musial, and B. Gabrys, ‚ÄúNats-bench: Benchmarking
nas algorithms for architecture topology and size,‚Äù IEEE transactions on
pattern analysis and machine intelligence , 2021.
[31] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770‚Äì778.
[32] B. Colson, P. Marcotte, and G. Savard, ‚ÄúAn overview of bilevel optimiza-
tion,‚Äù Annals of operations research , vol. 153, no. 1, pp. 235‚Äì256, 2007.
[33] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, ‚ÄúBilevel
programming for hyperparameter optimization and meta-learning,‚Äù Proc.
ICML , 2018.
[34] R. Wang, M. Cheng, X. Chen, X. Tang, and C.-J. Hsieh, ‚ÄúRethinking
architecture selection in differentiable nas,‚Äù in International Conference
on Learning Representations , 2021.
[35] X. Chen, L. Xie, J. Wu, and Q. Tian, ‚ÄúProgressive differentiable archi-
tecture search: Bridging the depth gap between search and evaluation,‚Äù in
Proceedings of the IEEE International Conference on Computer Vision ,
2019, pp. 1294‚Äì1303.
[36] Y . Xu, L. Xie, X. Zhang, X. Chen, G.-J. Qi, Q. Tian, and H. Xiong,
‚ÄúPc-darts: Partial channel connections for memory-efÔ¨Åcient architecture
search,‚Äù in International Conference on Learning Representations , 2020.
[37] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, ‚ÄúAmc: Automl for
model compression and acceleration on mobile devices,‚Äù in Proceedings
of the European Conference on Computer Vision (ECCV) , 2018, pp. 784‚Äì
800.
VOLUME 4, 2016 11

--- PAGE 12 ---
F . Xue et al. : RARTS: An EfÔ¨Åcient First-Order Relaxed Architecture Search Method
[38] M. Tan, B. Chen, R. Pang, V . Vasudevan, M. Sandler, A. Howard, and
Q. V . Le, ‚ÄúMnasnet: Platform-aware neural architecture search for mobile,‚Äù
inProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2019, pp. 2820‚Äì2828.
[39] A. Wan, X. Dai, P. Zhang, Z. He, Y . Tian, S. Xie, B. Wu, M. Yu, T. Xu,
K. Chen et al. , ‚ÄúFbnetv2: Differentiable neural architecture search for spa-
tial and channel dimensions,‚Äù in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2020, pp. 12 965‚Äì12 974.
[40] L. Li and A. Talwalkar, ‚ÄúRandom search and reproducibility for neural
architecture search,‚Äù in Uncertainty in artiÔ¨Åcial intelligence . PMLR,
2020, pp. 367‚Äì377.
[41] H. Hu, R. Peng, Y . Tai, and C. Tang, ‚ÄúNetwork trimming: A data-driven
neuron pruning approach towards efÔ¨Åcient deep architectures,‚Äù CoRR , vol.
abs/1607.03250, 2016.
[42] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, ‚ÄúPruning Ô¨Ålters
for efÔ¨Åcient convnets,‚Äù ICLR, 2017; arXiv preprint arXiv:1608.08710 ,
2016.
[43] Y . He, X. Zhang, and J. Sun, ‚ÄúChannel pruning for accelerating very deep
neural networks,‚Äù in Proceedings of the IEEE International Conference on
Computer Vision , 2017, pp. 1389‚Äì1397.
[44] J.-H. Luo, J. Wu, and W. Lin, ‚ÄúThinet: A Ô¨Ålter level pruning method for
deep neural network compression,‚Äù in Proceedings of the IEEE interna-
tional conference on computer vision , 2017, pp. 5058‚Äì5066.
[45] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, ‚ÄúRethinking the value
of network pruning,‚Äù ICLR, 2019; arXiv preprint arXiv:1810.05270 , 2018.
[46] M. Yuan and Y . Lin, ‚ÄúModel selection and estimation in regression with
grouped variables,‚Äù Journal of the Royal Statistical Society: Series B
(Statistical Methodology) , vol. 68, no. 1, pp. 49‚Äì67, 2006.
[47] J. Ye, L. Xin, Z. Lin, and J. Z. Wang, ‚ÄúRethinking the smaller-norm-
less-informative assumption in channel pruning of convolution layers,‚Äù in
Proceedings of ICLR , 2018.
[48] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and J. Sun,
‚ÄúMetapruning: Meta learning for automatic neural network channel prun-
ing,‚Äù in Proceedings of the IEEE International Conference on Computer
Vision , 2019, pp. 3296‚Äì3305.
[49] T. Dinh and J. Xin, ‚ÄúConvergence of a relaxed variable splitting method for
learning sparse neural networks via `1,`0, and transformed- `1penalties,‚Äù
inProceedings of SAI Intelligent Systems Conference . Springer, 2020,
pp. 360‚Äì374.
[50] G. H. Golub and C. F. Van Loan, Matrix Computations (3rd ed.) . Johns
Hopkins Univ. Press, 1996.
[51] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a neural
network,‚Äù arXiv preprint arXiv:1503.02531 , 2015.
[52] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, ‚ÄúMo-
bilenetv2: Inverted residuals and linear bottlenecks,‚Äù in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , 2018, pp.
4510‚Äì4520.
FANGHUI XUE received the B.S. degree in
mathematics from Fudan University, Shanghai,
China, in 2016 and the M.S. degrees in analytics
and mathematical risk management from Georgia
State University, Atlanta, GA, in 2018. In May,
2022, he received his Ph.D. degree in mathe-
matics at University of California, Irvine, CA.
His research interests include deep learning and
computer vision, with a focus on AutoML and
constructing efÔ¨Åcient neural networks.
YINGYONG QI received the Ph.D. degree in
speech and hearing sciences from Ohio State Uni-
versity, in 1989, and the Ph.D. degree in electrical
and computer engineering from the University of
Arizona, in 1993. He held a faculty position at
the University of Arizona, from 1989 to 1999. He
was a Visiting Scientist at the Research Laboratory
of Electronics, Massachusetts Institute of Technol-
ogy, from 1995 to 1996, and a Visiting Scientist
at the Visual Computing Laboratory of Hewlett
Packard, Palo Alto, in 1998. He is currently a Senior Director of technology
at Qualcomm and a Researcher of the Department of Mathematics, Univer-
sity of California at Irvine. He has published over 100 scientiÔ¨Åc articles
and U.S. patents during his tenure at university and industry. His research
interests include speech processing, computer vision, and machine learning.
He received Klatt Memorial Award in Speech Science from the Acoustical
Society of America, in 1991, the First Award from the National Institute
of Health, in 1992, and the AASFAA Outstanding Faculty Award from the
University of Arizona, in 1998. More recently, he led a team winning the
3rd Place of IEEE Low Power Image Recognition Competition, sponsored
by Google at CVPR 2019.
JACK XIN received the Ph.D. degree in mathe-
matics from New York University‚Äôs Courant Insti-
tute of Mathematical Sciences, in 1990. He was a
faculty at the University of Arizona, from 1991 to
1999, and the University of Texas at Austin, from
1999 to 2005. He is currently a Chancellor‚Äôs Pro-
fessor of mathematics at UC Irvine. His research
interests include applied analysis and computa-
tional methods, and their applications in multi-
scale problems and data science. He is a fellow
of Guggenheim Foundation, American Mathematical Society, American
Association for the Advancement of Science, and the Society for Industrial
and Applied Mathematics. He was a recipient of Qualcomm Faculty Award
(2019‚Äì2022).
12 VOLUME 4, 2016
