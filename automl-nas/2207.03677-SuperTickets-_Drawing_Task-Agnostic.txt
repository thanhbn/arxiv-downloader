# 2207.03677.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2207.03677.pdf
# File size: 19579077 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SuperTickets: Drawing Task-Agnostic
Lottery Tickets from Supernets via Jointly
Architecture Searching and Parameter Pruning
Haoran You1⋆, Baopu Li3, Zhanyi Sun2, Xu Ouyang2, and Yingyan Lin1
1Georgia Institute of Technology2Rice University3Oracle Health and AI
{hyou37, celine.lin }@gatech.edu {zs19,xo2 }@rice.edu ,baopu.li@oracle.com
Abstract. Neural architecture search (NAS) has demonstrated amaz-
ing success in searching for efficient deep neural networks (DNNs) from
a given supernet. In parallel, lottery ticket hypothesis has shown that
DNNs contain small subnetworks that can be trained from scratch to
achieve a comparable or even higher accuracy than the original DNNs.
As such, it is currently a common practice to develop efficient DNNs via
a pipeline of first search and then prune. Nevertheless, doing so often
requires a tedious and costly process of search-train-prune-retrain and
thus prohibitive computational cost. In this paper, we discover for the
first time that both efficient DNNs and their lottery subnetworks (i.e.,
lottery tickets) can be directly identified from a supernet, which we term
asSuperTickets , via a two-in-one training scheme with jointly archi-
tecture searching and parameter pruning. Moreover, we develop a pro-
gressive and unified SuperTickets identification strategy that allows the
connectivity of subnetworks to change during supernet training, achiev-
ing better accuracy and efficiency trade-offs than conventional sparse
training. Finally, we evaluate whether such identified SuperTickets drawn
from one task can transfer well to other tasks, validating their potential
of simultaneously handling multiple tasks. Extensive experiments and
ablation studies on three tasks and four benchmark datasets validate
that our proposed SuperTickets achieve boosted accuracy and efficiency
trade-offs than both typical NAS and pruning pipelines, regardless of
having retraining or not. Codes and pretrained models are available at
\textcolor{blue}{https://github.com/RICE-EIC/SuperTickets} .
Keywords: Lottery Ticket Hypothesis, Efficient Training/Inference, Neu-
ral Architecture Search, Task-agnostic DNNs
1 Introduction
While deep neural networks (DNNs) have achieved unprecedented performance
in various tasks and applications like classification, segmentation, and detec-
tion [8], their prohibitive training and inference costs limit their deployment on
⋆Work done while interning at Baidu USA; Correspondence to Baopu Li and Yingyan
LinarXiv:2207.03677v5  [cs.CV]  3 Mar 2025

--- PAGE 2 ---
2 H. You et al.
resource-constrained devices for more pervasive intelligence. For example, one
forward pass of the ResNet50 [16] requires 4 GFLOPs (FLOPs: floating point
operations) and its training requires 1018FLOPs [49]. To close the aforemen-
tioned gap, extensive attempts have been made to compress DNNs from either
macro-architecture (e.g., NAS [37,44,8]) or fine-grained parameter (e.g., network
pruning [15,11]) levels. A commonly adopted DNN compression pipeline follow-
ing a coarse-to-fine principle is to first automatically search efficient and powerful
DNN architectures from a larger supernet and then prune the searched DNNs
via costly train-prune-retrain process [10,9,21] to derive smaller and sparser sub-
networks with a comparable or degraded accuracy but largely reduced inference
costs. However, such pipeline requires a tedious search-train-prune-retrain pro-
cess and thus still prohibitive training costs.
To address the above limitation for simplifying the pipeline and further im-
prove the accuracy-efficiency trade-offs of the identified networks, we advocate
atwo-in-one training framework for simultaneously identifying both efficient
DNNs and their lottery subnetworks via jointly architecture searching and pa-
rameter pruning. We term the identified small subnetworks as SuperTickets
if they achieve comparable or even superior accuracy-efficiency trade-offs than
previously adopted search-then-prune baselines, because they are drawn from su-
pernets and represent both coarse-grained DNN architectures and fine-grained
DNN subnetworks. We make non-trivial efforts to explore and validate the po-
tential of SuperTickets by answering three key questions: (1) whether such Su-
perTickets can be directly found from a supernet via two-in-one training? If yes,
then (2) how to effectively identify such SuperTickets? and (3) can SuperTick-
ets found from one task/dataset transfer to another, i.e., have the potential to
handle different tasks/datasets? To the best of our knowledge, this is the first
attempt taken towards identifying both DNN architectures and their correspond-
ing lottery ticket subnetworks through a unified two-in-one training scheme. Our
contributions can be summarized as follows:
•Wefor the first time discover that efficient DNN architectures and their
lottery subnetworks, i.e., SuperTickets, can be simultaneously identified from
a supernet leading to superior accuracy-efficiency trade-offs.
•We develop an unified progressive identification strategy to effectively find
the SuperTickets via a two-in-one training scheme which allows the subnet-
works to iteratively reactivate the pruned connections during training, of-
fering better performance than conventional sparse training. Notably, our
identified SuperTickets without retraining already outperform previously
adopted first-search-then-prune baselines, and thus can be directly deployed.
•We validate the transferability of identified SuperTickets across different
tasks/datasets, and conduct extensive experiments to compare the proposed
SuperTickets with those from existing search-then-prune baselines, typical
NAS techniques, and pruning works. Results on three tasks and four datasets
demonstrate the consistently superior accuracy-efficiency trade-offs and the
promising transferability for handling different tasks offered by SuperTickets.

--- PAGE 3 ---
SuperTickets 3
2 Related Works
Neural Architecture Search (NAS). NAS has achieved an amazing success
in automating the design of efficient DNN architectures and boosting accuracy-
efficiency trade-offs [56,38,17]. To search for task-specific DNNs, early works
[38,37,17] adopt reinforcement learning based methods that require a prohibitive
search time and computing resources, while recent works [24,44,40,47] update
both the weights and architectures during supernet training via differentiable
search that can greatly improve the search efficiency as compared to prior NAS
works. More recently, some works adopt one-shot NAS [14,3,52,41] to decouple
the architecture search from supernet training. Such methods are generally ap-
plicable to search for efficient CNNs [14,2] or Transformers [42,4,36] for solving
both vision and language tasks. To search for multi-task DNNs, recently emerg-
ing works like HR-NAS [8] and FBNetv5 [45] advocate supernet designs with
multi-resolution branches so as to accommodate both image classification and
other dense prediction tasks that require high-resolution representations. In this
work, we propose to directly search for not only efficient DNNs but also their lot-
tery subnetworks from supernets to achieve better accuracy-efficiency trade-offs
while being able to handle different tasks.
Lottery Ticket Hypothesis (LTH). Frankle et al. [11,12] showed that
winning tickets (i.e., small subnetworks) exist in randomly initialized dense net-
works, which can be retrained to restore a comparable or even better accuracy
than their dense network counterparts. This finding has inspired lots of research
directions as it implies the potential of sparse subnetworks. For efficient train-
ing, You et al. [49] consistently find winning tickets at early training stages,
largely reducing DNNs’ training costs. Such finding has been extended to lan-
guage models (e.g., BERT) [5], generative models (e.g., GAN) [31], and graph
neural networks [50]; Zhang et al. [54] recognize winning tickets more efficiently
by training with only a specially selected subset of data; and Ramanujan et al.
[33] further identify winning tickets directly from random initialization that per-
form well even without retraining. In contrast, our goal is to simultaneously find
both efficient DNNs and their lottery subnetworks from supernets, beyond the
scope of sparse training or drawing winning tickets from dense DNN models.
Task-Agnostic DNNs Design. To facilitate designing DNNs for differ-
ent tasks, recent works [25,17,43] propose to design general architecture back-
bones for various computer vision tasks. For example, HR-Net [43] maintains
high-resolution representations through the whole network for supporting dense
prediction tasks, instead of connecting high-to-low resolution convolutions in se-
ries like ResNet or VGGNet; Swin-Transformer [25] adopts hierarchical vision
transformers to serve as a general-purpose backbone that is compatible with
a broad range of vision tasks; ViLBERT [27,28] proposes a multi-modal two-
stream model to learn task-agnostic joint representations of both image and
language; Data2vec [1] designs a general framework for self-supervised learning
in speech, vision and language. Moreover, recent works [8,45,46] also leverage
NAS to automatically search for task-agnostic and efficient DNNs from hand-

--- PAGE 4 ---
4 H. You et al.
crafted supernets. In this work, we aim to identify task-agnostic SuperTickets
that achieve better accuracy-efficiency trade-offs.
3 The Proposed SuperTickets Method
In this section, we address the three key questions of SuperTickets. First, we
develop a two-in-one training scheme to validate our hypothesis that SuperTick-
ets exist and can be found directly from a supernet. Second, we further explore
more effective SuperTickets identification strategies via iterative neuron reacti-
vation and progressive pruning, largely boosting the accuracy-efficiency trade-
offs. Third, we evaluate the transferability of the identified SuperTickets across
different datasets or tasks, validating their potential of being task-agnostic.
3.1 Do SuperTickets Exist in Supernets?
SuperTickets Hypothesis. We hypothesize that both efficient DNN archi-
tectures and their lottery subnetworks can be directly identified from a su-
pernet, and term these subnetworks as SuperTickets if they achieve on par
or even better accuracy-efficiency trade-offs than those from first-search-then-
prune counterparts. Considering a supernet f(x;θS), various DNN architectures
aare sampled from it whose weights are represented by θS(a), then we can de-
fine SuperTickets as f(x;m⊙θS(a)), where m∈ {0,1}is a mask to indicate
the pruned and unpruned connections in searched DNNs. The SuperTickets Hy-
pothesis implies that jointly optimizing DNN architectures aand corresponding
sparse masks mworks better, i.e., resulting in superior accuracy-efficiency trade-
offs, than sequentially optimizing them.
Experiment Settings. To perform experiments for exploring whether Su-
perTickets generally exist, we need (1) a suitable supernet taking both classical
efficient building blocks and task-agnostic DNN design principles into consid-
eration and (2) corresponding tasks, datasets, and metrics. We elaborate our
settings below. NAS and Supernets: We consider a multi-branch search space
containing both efficient convolution and attention building blocks following one
state-of-the-art (SOTA) work of HR-NAS [8], whose unique hierarchical multi-
resolution search space for handling multiple vision tasks stands out compared
to others. In general, it contains two paths: MixConv [39] and lightweight Trans-
former for extracting both local and global context information. Both the number
of convolutional channels with various kernel sizes and the number of tokens in
the Transformer are searchable parameters. Tasks, Datasets, and Metrics: We
consider semantic segmentation on Cityscapes [6] and human pose estimation
on COCO keypoint [22] as two representative tasks for illustrative purposes. For
Cityscapes, the mean Intersection over Union (mIoU), mean Accuracy (mAcc),
and overall Accuracy (aAcc) are evaluation metrics. For COCO keypoint, we
train the model using input size 256 ×192, an initial learning rate of 1e-3, a batch
size of 384 for 210 epochs. The average precision (AP), recall scores (AR), APM
and APLfor medium or large objects are evaluation metrics. All experiments
are run on Tesla V100*8 GPUs.

--- PAGE 5 ---
SuperTickets 5
Algorithm 1: Two-in-One Framework for Identifying SuperTickets.
Input: The supernet weights θS, drop threshold ϵ, and pruning ratio p;
Output: Efficient DNNs and their lottery subnetworks f(x;m⊙θS(a)).
1while t(epoch) < tmaxdo
2 t=t+ 1;
3 Update weights θSand importance factor rusing SGD training;
4 iftmod ts= 0then ▷Search for DNNs
5 Remove search units whose importance factors r < ϵ ;
6 Recalibrate the running statistics of BN layers to obtain subnet a;
// If enabling the iterative reactivation technique
7 Reactivate the gradients of pruned weights;
8 else if tmod tp= 0then ▷Prune for subnetworks
// If enabling the progressive pruning technique
9 Redefine the pruning ratio as min {p,10%× ⌊t/tp⌋};
10 Perform magnitude-based pruning towards the target ratio;
11 Keep the sparse mask mtand disable pruned weights’ gradients;
12 end
13end
14return f(x;mt⊙θS(a)); ▷SuperTickets
Efficient DNNs
…
Search PruneSubnetworks w/ or w/o Retraining Supenet Training
Supenet Training
 SuperTickets
Two-in-One(a) First-search-then-prune (S+P) pipeline
(b) Two-in-One Training (Proposed)
Fig. 1. Illustrating first-search-then-prune (S+P)
vs. our two-in-One training.Two-in-One Training.
To validate the SuperTickets
hypothesis, we propose a two-
in-one training algorithm that
simultaneously searches and
prunes during supernet train-
ing of NAS. As shown in Alg.
1 and Fig. 1, for searching
for efficient DNNs, we adopt a
progressive shrinking NAS by
gradually removing unimpor-
tant search units that can be
either convolutional channels
or Transformer tokens. After
every pstraining epochs, we
will detect and remove the unimportant search units once their corresponding
importance factors r(i.e., the scales in Batch Normalization (BN) layers) are
less than a predefined drop threshold ϵ. Note that rcan be jointly learned with
supernet weights, such removing will not affect the remaining search units since
channels in depth-wise convolutions are independent among each other, as also
validated by [8,30]. In addition, we follow network slimming [26] to add a l1
penalty as a regularization term for polarizing the importance factors to ease
the detection of unimportant units. After removing them, the running statistics
in BN layers are recalibrated in order to match the searched DNN architecture
afor avoiding covariate shift [19,48]. For pruning of searched DNNs, we per-

--- PAGE 6 ---
6 H. You et al.
107108109
FLOPs020406080mIoU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
107108109
FLOPs020406080mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
107108109
FLOPs020406080100aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Fig. 2. Comparing the mIoU, mAcc, aAcc and inference FLOPs of the resulting net-
works from the proposed two-in-one training and first-search-then-prune (S+P) base-
lines on semantic segmentation task and Cityscapes dataset, where Rand., Mag., and
Grad. represent random, magnitude, and graident-based pruning, respectively. Note
that each method has a series of points for representing different pruning ratios rang-
ing from 10% to 98%. All accuracies are averaged over three runs.
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060AP (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APM (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APL (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Fig. 3. Comparing the AP, APM, APLand inference FLOPs of the resulting networks
from the proposed two-in-one training and baselines on human pose estimation task and
COCO keypoint dataset. Each method has a series of points for representing different
pruning ratios ranging from 10% to 98%. All accuracies are averaged over three runs.
form magnitude-based pruning towards the given pruning ratio per tpepochs,
the generated spare mask mtwill be kept so as to disable the gradients flow of
the pruned weights during the following training. Note that we do not incorpo-
rate the iterative reactivation and progressive pruning techniques (highlighted
with colors/shadows in Alg. 1, which will be elaborated later) as for now. Such
vanilla two-in-one training algorithm can be regarded as the first step towards
answering the puzzle whether SuperTickets generally exist.
Existence of SuperTickets. We compare the proposed two-in-one training
with first-search-then-prune (S+P) baselines and report the results on Cityscapes
and COCO keypoint at Fig. 2 and Fig. 3, respectively. We see that the proposed
two-in-one training consistently generates comparable or even better accuracy-
efficiency trade-offs as compared to S+P with various pruning criteria (random,
magnitude, and gradient) since our methods demonstrate much better perfor-
mance of segmentation or human pose estimation under different FLOPs reduc-
tions as shown in the above two figures, indicating that SuperTickets generally
exist in a supernet and have great potential to outperform the commonly adopted
approaches, i.e., sequentially optimizing DNN architectures and sparse masks.

--- PAGE 7 ---
SuperTickets 7
Table 1. Breakdown analysis of the proposed SuperTickets identification strategy. We
report the performance of found subnetworks under 90%/80% sparsity on two datasets.
Methods 2-in-1 PP IR-P IR-S RetrainCityscapes COCO Keypoint
mIoU mAcc aAcc AP APMAPLAR
S+P (Mag.) 42.12 50.49 87.45 5.04 4.69 5.89 10.67
S+P (Mag.) ! 51.03 59.61 90.88 48.63 46.82 51.74 53.38
Ours ! 55.84 67.38 92.97 58.38 56.68 61.26 62.23
Ours ! ! 63.89 73.56 94.17 60.14 57.93 63.70 63.79
Ours ! ! ! 45.73 55.52 89.36 5.48 7.43 4.36 10.85
Ours ! ! ! 66.61 76.30 94.63 61.02 58.80 64.64 64.78
Ours ! ! ! ! 67.17 77.03 94.73 61.48 59.30 65.19 65.20
3.2 How to More Effectively Identify SuperTickets?
We have validated the existence of SuperTickets, the natural next question is how
to more effectively identify them. To this end, we propose two techniques that
can be seamlessly incorporated into the two-in-one training framework to more
effectively identify SuperTickets and further boost their achievable performance.
Progressive Pruning (PP). Although simultaneously searching and prun-
ing during supernet training enables the opportunity of cooperation between
coarse-grained search units removal and fine-grained weights pruning, i.e., NAS
helps to refine the pruned networks as a compensation by removing over-pruned
units for avoiding bottlenecked layers, we find that over-pruning at the early
training stages inevitably hurts the networks’ generalizability, and further pro-
pose a progressive pruning (PP) techniques to overcome this shortcoming. As
highlighted in the cyan part of Alg. 1, the pruning ratio is defined as min {p,10%×
⌊t/tp⌋}, which means that the network sparsity will gradually increase from 10%
to the target ratio p, by 10% per tpepochs. The PP technique helps to effec-
tively avoid over-pruning at early training stages and thus largely boosts the
final performance. As demonstrated in Table 1, two-in-one training with PP
achieves 8.05%/6.18%/1.2% mIoU/mAcc/aAcc and 1.76%/1.25%/2.44%/1.56%
AP/APM/APL/AR improvements on Cityscapes and COCO keypoint datasets,
respectively, as compared to the vanilla two-in-one training under 90% sparsity.
Iterative Reactivation (IR). Another problem in the two-in-one frame-
work is that the pruned weights will never get gradients updates throughout
the remaining training. To further boost the performance, we design an iterative
reactivation (IR) strategy to facilitate the effective SuperTickets identification
by allowing the connectivity of subnetworks to change during supernet train-
ing. Specifically, we reactivate the gradients of pruned weights as highlighted
in the orange part of Alg. 1. Note that we reactivate during searching instead
of right after pruning, based on a hypothesis that sparse training is also essen-
tial to the two-in-one training framework. In practice, the pruning interval pt
is different from the searching interval psin order to allow a period of sparse
training. To validate the hypothesis, we design two variants: IR-S and IR-P that
reactivate pruned weights’ gradients during searching and pruning, respectively,
and show the comparisons in Table 1. We observe that: (1) IR-P leads to even

--- PAGE 8 ---
8 H. You et al.
worse accuracy than vanilla two-in-one training, validating that sparse training is
essential; (2) IR-S further leads to 2.72%/2.74%/0.46% mIoU/mAcc/aAcc and
0.88%/0.87%/0.94%/0.99% AP/APM/ APL/AR improvements on Cityscapes
and COCO keypoint, respectively, on top of two-in-one training with PP.
SuperTickets w/ or w/o Retraining. Since the supernet training, ar-
chitecture search, and weight pruning are conducted in an unified end-to-end
manner, the resulting SuperTickets can be deployed directly without retrain-
ing, achieving better accuracy-efficiency trade-offs than S+P baselines (even
with retraining) as indicated by Table 1. To investigate whether retraining can
further boost the performance, we retrain the found SuperTickets for another
50 epochs and report the results at Table 1. We see that retraining further
leads to 0.56%/0.73%/0.10% mIoU/mAcc/aAcc and 0.46%/0.50%/0.55%/0.42%
AP/APM/ APL/AR improvements on Cityscapes and COCO keypoint datasets,
respectively.
3.3 Can the Identified SuperTickets Transfer?
To validate the potential of identified SuperTickets for handling different tasks
and datasets, we provide empirical experiments and analysis as follows. Note that
we adjust the final classifier to match target datasets during transfer learning.
Table 2. Supertickets transfer validation tests
under 90% sparsity.
Methods Params FLOPsCityscapes
mIoU mAcc aAcc
S+P (Grad.) 0.13M 203M 8.41 12.39 56.77
S+P (Mag.) 0.13M 203M 42.12 50.49 87.45
S+P (Mag.) w/ RT 0.13M 203M 60.76 70.40 93.38
ADE20K Tickets 0.20M 247M 62.91 73.32 93.82
ImageNet Tickets 0.18M 294M 61.64 71.78 93.75
Methods Params FLOPsADE20K
mIoU mAcc aAcc
S+P (Grad.) 0.11M 154M 0.79 1.50 25.58
S+P (Mag.) 0.11M 154M 3.37 4.70 39.47
Cityscapes Tickets 0.13M 119M 20.83 29.95 69.00
ImageNet Tickets 0.21M 189M 22.42 31.87 70.21SuperTickets Transferring
Among Datasets. We first test
the transferability of the identi-
fied SuperTickets among different
datasets within the same task, i.e.,
Cityscapes and ADE20K as two
representatives in the semantic seg-
mentation task. Table 2 shows that
SuperTickets identified from one
dataset can transfer to another
dataset while leading to compa-
rable or even better performance
than S+P baselines with (denoted
as “w/ RT”) or without retrain-
ing (by default). For example, when
tested on Cityscapes, SuperTickets
identified from ADE20K after fine-tuning lead to 2.2% and 20.8% higher mIoU
than S+P (Mag.) w/ and w/o RT baselines which are directly trained on target
Cityscapes dataset. Likewise, the SuperTickets transferred from Cityscapes to
ADE20K also outperform baselines on target dataset.
SuperTickets Transferring Among Tasks. To further investigate whether
the identified SuperTickets can transfer among different tasks. We consider to
transfer SuperTickets’s feature extraction modules identified from ImageNet on
classification task to Cityscapes and ADE20K on segmentation tasks, where
the dense prediction heads and final classifier are still inherited from the target
datasets. The results are presented in the last row of the two sub-tables in Table

--- PAGE 9 ---
SuperTickets 9
2. We observe that such transferred networks still perform well on downstream
tasks. Sometimes, it even achieves better performance than transferring within
one task, e.g., ImageNet →ADE20K works better (1.6% higher mIoU) than
Cityscapes →ADE20K. We supply more experiments on various pruning ratios
in Sec. 4.3.2.
4 Experiment Results
4.1 Experiment Setting
Tasks, Datasets, and Supernets. Tasks and Datasets. We consider four bench-
mark datasets and three representative vision tasks to demonstrate the effective-
ness of SuperTickets, including image classification on ImageNet [7] dataset with
1.2 million training images and 50K validation images; semantic segmentation on
Cityscapes [6] and ADE20K [55] datasets with 2975/500/1525 and 20K/2K/3K
images for training, validation, and testing, respectively; human pose estimation
on COCO keypoint [22] dataset with 57K images and 150K person instances for
training, and 5K images for validation. These selected datasets require differ-
ent receptive fields and global/local contexts, manifesting themselves as proper
test-beds for SuperTickets on multiple tasks. Supernets. For all experiments,
we adopt the same supernet as HR-NAS [8] thanks to the task-agnostic multi-
resolution supernet design. It begins with two 3 ×3 convolutions with stride 2,
which is followed by five parallel modules to gradually divide it into four branches
of decreasing resolutions, the learned features from all branches are then merged
together for classification or dense prediction.
Search and Training Settings. For training supernets on ImageNet, we
adopt a RMSProp optimizer with 0.9 momentum and 1e-5 weight decay, expo-
nential moving average (EMA) with 0.9999 decay, and exponential learning rate
decay with an initial learning rate of 0.016 and 256 batch size for 350 epochs.
For Cityscapes and ADE20K, we use an AdamW optimizer, an initial learning
rate of 0.04 with batch size 32 due to larger input image sizes, and train for 430
and 200 epochs, respectively, following [8]. For COCO keypoint, we follow [43]
to use an Adam optimizer for 210 epochs, the initial learning rate is set to 1e-3,
and is divided by 10 at the 170th and 200th epochs, respectively. In addition, we
perform architecture search during supernet training. For all search units, we use
the scales from their attached BN layers as importance factors r; search units
with r <0.001 are regarded as unimportant and removed every 10 epochs (i.e.,
ts= 10); Correspondingly, magnitude-based pruning will be performed per 25
epochs for ImageNet and Cityscapes, or per 15 epochs for ADE20K and COCO
keypoint (i.e., tp= 25/15), resulting intervals for sparse training as in Sec. 3.2.
Baselines and Evaluation Metrics. Baselines. For all experiments, we
consider the S+P pipeline as one of our baselines, where the search method
follows [8]; the pruning methods can be chosen from random pruning, magnitude
pruning [15,11], and gradient pruning [20]. In addition, we also benchmark with
hand-crafted DNNs, e.g., ShuffleNet [53,29] and MobiletNetV2 [34], and prior

--- PAGE 10 ---
10 H. You et al.
0.5 1.0 1.5 2.0
FLOPs 1e8020406080Top-1 Acc. (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
0.5 1.0 1.5 2.0
FLOPs 1e8020406080100Top-5 Acc. (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
Fig. 4. Comparing the top-1/5 accuracy and FLOPs of the proposed SuperTickets and
S+P baselines on ImageNet. Each method has a series of points to represent different
pruning ratios ranging from 10% to 98%. All accuracies are averaged over three runs.
We also benchmark all methods with retraining (denoted as w/ RT).
typical NAS resulting task-specific DNNs, e.g., MobileNetV3 [17] and Auto-
DeepLab [23]. We do not compare with NAS/tickets works with SOTA accuracy
due to different goals and experimental settings. All baselines are benchmarked
under similar FLOPs or accuracy for fair comparisons. Evaluation Metrics. We
evaluate the SuperTickets and all baselines in terms of accuracy-efficiency trade-
offs. Specifically, the accuracy metrics refer to top-1/5 accuracy for classification
tasks; mIoU, mAcc, and aAcc for segmentation tasks; AP, AR, APM, and APL
for human pose estimation tasks. For efficiency metrics, we evaluate and compare
both the number of parameters and inference FLOPs.
4.2 Evaluating SuperTickets over Typical Baselines
4.2.1 SuperTickets on the Classification Task
Table 3. SuperTickets vs. some typical meth-
ods on ImageNet. FLOPs is measured with the
input size of 224 ×224.
Model Params FLOPs Top-1 Acc.
CondenseNet [18] 2.9M 274M 71.0%
ShuffleNetV1 [53] 3.4M 292M 71.5%
ShuffleNetV2 [29] 3.5M 299M 72.6%
MobileNetV2 [34] 3.4M 300M 72.0%
FBNet [44] 4.5M 295M 74.1%
S+P (Grad.) 2.7M 114M 64.3%
S+P (Mag.) 2.7M 114M 72.8%
SuperTickets 2.7M 125M 74.2%We show the overall compar-
isons between SuperTickets and
some typical baselines in terms
of accuracy-efficiency trade-offs in
Fig. 4 and Table. 3, from which we
have two observations . First ,
SuperTickets consistently outper-
form all baselines by reducing
the inference FLOPs while achiev-
ing a comparable or even bet-
ter accuracy. Specifically, Su-
perTickets reduce 61.4% ∼81.5%
FLOPs while offering a compara-
ble or better accuracy (+0.1% ∼
+4.6%) as compared to both S+P
and some task-specific DNNs; Likewise, when comparing under comparable num-
ber of parameters or FLOPs, SuperTickets lead to on average 26.5% (up to

--- PAGE 11 ---
SuperTickets 11
108109
FLOPs020406080mIoU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs020406080mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs020406080100aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
(a) Comparing SuperTickets with S+P baselines on Cityscapes.
108109
FLOPs0102030mIOU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs01020304050mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs020406080aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
(b) Comparing SuperTickets with S+P baselines on ADE20K.
Fig. 5. Comparing the mIoU, mAcc, aAcc and inference FLOPs of the proposed Su-
perTickets and S+P baselines on Cityscapes and ADE20K datasets. Each method has
a series of points to represent different pruning ratios ranging from 10% to 98%.
64.5%) and on average 41.3% (up to 71.9%) top-1 accuracy improvements as
compared to S+P (Mag.) and S+P (Grad.) across various pruning ratios, e.g.,
under 50% pruning ratios, SuperTickets achieve 74.2% top-1 accuracy, +1.4%
and +9.9% over S+P (Mag.) and S+P (Grad.), respectively. Second , SuperTick-
ets w/o retraining even surpass S+P baselines with retraining as demonstrated
in Fig. 4, leading to on average 6.7% (up to 29.2%) higher top-1 accuracy under
comparable FLOPs across various pruning ratios (10% ∼98%). Furthermore,
SuperTickets w/ retraining achieve 0.1% ∼31.9% (on average 5.3%) higher ac-
curacy than the counterparts w/o retraining, pushing forward the frontier of
accuracy-efficiency trade-offs.
Table 4. SuperTickets vs. some typical
methods on Cityscapes. FLOPs is mea-
sured with the input size of 512 ×1024.
Model Params FLOPs mIoU
BiSeNet [51] 5.8M 6.6G 69.00%
MobileNetV3 [17] 1.5M 2.5G 72.36%
ShuffleNetV2 [29] 3.0M 6.9G 71.30%
Auto-DeepLab [23] 3.2M 27.3G 71.21%
SqueezeNAS [35] 0.73M 8.4G 72.40%
S+P (Grad.) w/ RT 0.63M 1.0G 60.66%
S+P (Mag.) w/ RT 0.63M 1.0G 72.31%
SuperTickets 0.63M 1.0G 72.68%4.2.2 SuperTickets on the Seg-
mentation Task
Experiments on Cityscapes. We
compare SuperTickets with typical base-
lines on Cityscapes as shown in Fig. 5 (a)
and Table 4. We see that SuperTickets
consistently outperform all baselines in
terms of mIoU/mAcc/aAcc and FLOPs.
Specifically, SuperTickets reduce 60% ∼
80.86% FLOPs while offering a compa-
rable or better mIoU (0.28 % ∼43.26%)
as compared to both S+P and task-
specific DNNs; Likewise, when compar-

--- PAGE 12 ---
12 H. You et al.
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060AP (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
SuperTickets w/ RT
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APM (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
SuperTickets w/ RT
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APL (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
SuperTickets w/ RT
Fig. 6. Comparing the AP, APM, APLand inference FLOPs of the proposed Su-
perTickets and baselines on human pose estimation task and COCO keypoint dataset.
Each method has a series of points for representing different pruning ratios ranging
from 10% to 98%. All accuracies are averaged over three runs.
ing under comparable number of parameters or FLOPs, SuperTickets lead to on
average 17.70% (up to 42.86%) and 33.36% (up to 58.05%) mIoU improvements
as compared to S+P (Mag.) and S+P (Grad.) across various pruning ratios,
e.g., under 50% pruning ratios, SuperTickets achieve 72.68% mIoU, +0.37% and
+12% over S+P (Mag.) and S+P (Grad.), respectively. We also report the com-
parison among methods after retraining at Fig. 5, as denoted by “w/ RT”. We
find that S+P (Grad.) w/ RT suffers from overfitting and even leads to worse
performance; In contrast, SuperTickets w/ retraining further achieve 0.51% ∼
1.64% higher accuracy than the counterparts w/o retraining, pushing forward
the frontier of accuracy-efficiency trade-offs.
Table 5. SuperTickets vs. typical methods on
ADE20K. FLOPs is measured with the input
size of 512 ×512.
Model Params FLOPs mIoU
MobileNetV2 [34] 2.2M 2.8G 32.04%
MobileNetV3 [17] 1.6M 1.3G 32.31%
S+P (Grad.) 1.0M 0.8G 24.14%
S+P (Mag.) 1.0M 0.8G 31.59%
SuperTickets 1.0M 0.8G 32.54%Experiments on ADE20K.
Similarly, we test the superiority
of SuperTickets on ADE20K as
shown in Fig. 5 (b) and Table 5.
The proposed SuperTickets con-
sistently outperform all baselines
in terms of accuracy-efficiency
trade-offs, reducing 38.46% ∼
48.53% FLOPs when comparing
under similar mIoU. When com-
pared under comparable num-
ber of parameters or FLOPs, Su-
perTickets lead to an average of
9.43% (up to 22.6%) and 14.17% (up to 27.61%) mIoU improvements as com-
pared to S+P (Mag.) and S+P (Grad.), respectively, across various pruning
ratios. In addition, SuperTickets w/ retraining further achieve 0.01% ∼5.3%
higher accuracy than the counterparts w/o retraining on ADE20K.
4.2.3 SuperTickets on the Human Pose Estimation Task
We compare SuperTickets with a few typical baselines on COCO keypoint as
shown in Fig. 6 and Table 6. We see that SuperTickets consistently outperform

--- PAGE 13 ---
SuperTickets 13
107108109
FLOPs020406080mIoU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Two-in-One w/ IR
Two-in-One w/ IR & PP
107108109
FLOPs020406080mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Two-in-One w/ IR
Two-in-One w/ IR & PP
107108109
FLOPs020406080100aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Two-in-One w/ IR
Two-in-One w/ IR & PP
Fig. 7. Ablation studies of the SuperTickets identified from two-in-one framework w/
or w/o the proposed iterative activation (IR) and progressive pruning (PP) techniques.
Table 6. SuperTickets vs. typical algorithms on
COCO. FLOPs is measured with the input size
of 256×192.
Model Params FLOPs AP APMAPLAR
ShuffleNetV1 [53] 1.0M 0.16G 58.5 55.2 64.6 65.1
ShuffleNetV2 [29] 1.3M 0.17G 59.8 56.5 66.2 66.4
MobileNetV2 [34] 2.3M 0.33G 64.6 61.0 71.1 70.7
S+P (Mag.) 0.6M 0.23G 63.4 61.2 66.8 67.3
SuperTickets 0.6M 0.23G 65.4 63.4 69.0 68.9all the related baselines in terms
of AP/APM/APL/AR and FLOPs.
Specifically, SuperTickets reduce
30.3% ∼78.1% FLOPs while of-
fering a comparable or better AP
(+0.8% ∼11.79%) as compared
to both S+P and task-specific
DNNs; Likewise, when comparing
under comparable number of pa-
rameters or FLOPs, SuperTickets
lead to on average 17.4% (up to 55.9%) AP improvements. In addition, Su-
perTickets w/ retraining further achieve on average 1.1% higher accuracy than
the counterparts w/o retraining on COCO keypoint.
4.3 Ablation Studies of the Proposed SuperTickets
4.3.1 Ablation Studies of SuperTickets’ Identification
We provide comprehensive ablation studies to show the benefit breakdown of
the proposed two-in-one training framework and more effective identification
techniques, i.e., progressive pruning (PP) and iterative reactivation (IR). As
shown in Fig. 7, we report the complete mIoU-FLOPs trade-offs with various
pruning ratios ranging from 10% to 99% when testing on Cityscapes dataset,
where x axis is represented by log-scale for emphasizing the improvements when
pruning ratio reaches high. As compared to S+P (Mag.), SuperTickets identified
from vanilla two-in-one framework achieve up to 40.17% FLOPs reductions when
comparing under similar mIoU, or up to 13.72% accuracy improvements when
comparing under similar FLOPs; Adopting IR during two-in-one training further
leads to up to 68.32% FLOPs reductions or up to 39.12% mIoU improvements;
On top of the above, adopting both IR and PP during two-in-one training offers
up to 80.86% FLOPs reductions or up to 43.26% mIoU improvements. This set
of experiments validate the effectiveness of the general two-in-one framework
and each of the proposed techniques.

--- PAGE 14 ---
14 H. You et al.
80 90 95 98
Pruning Ratio (%)0510152025mIOU (%)CityscapesADE20K
S+P (Mag.)
S+P (Grad.)
Transferred Tickets
80 90 95 98
Pruning Ratio (%)0204060mIOU (%)ADE20KCityscapes
80 90 95 98
Pruning Ratio (%)0510152025mIOU (%)ImageNetADE20K
80 90 95 98
Pruning Ratio (%)0204060mIOU (%)ImageNetCityscapes
80 90 95 98
Pruning Ratio (%)010203040AP (%)ImageNetCOCO
Fig. 8. Ablation studies of transferring identified SuperTickets from one dataset/task
to another dataset/task under various pruning ratios ranging from 80% to 98%.
4.3.2 Ablation Studies of SuperTickets’ Transferability
We previously use one set of experiments under 90% sparsity in Sec. 3.3 to
validate that the identified SuperTickets can transfer well. In this section, we
supply more comprehensive ablation experiments under various pruning ratios
and among several datasets/tasks. As shown in Fig. 8, the left two subplots indi-
cate the transfer between different datasets (Cityscapes ↔ADE20K) generally
works across four pruning ratios. In particular, transferred SuperTickets lead to
76.14% ∼81.35% FLOPs reductions as compared to the most competitive S+P
baseline, while offering comparable mIoU (0.27% ∼1.85%). Furthermore, the
right three subplots validate that the identified SuperTickets from classification
task can transfer well to other tasks (i.e., segmentation and human pose estima-
tion). Specifically, it leads to 68.67% ∼69.43% FLOPs reductions as compared
to the S+P (Mag.) baseline, when achieving comparable mIoU or AP.
5 Conclusion
In this paper, we advocate a two-in-one framework where both efficient DNN
architectures and their lottery subnetworks (i.e., SuperTickets) can be identified
from a supernet simultaneously, resulting in better performance than first-search-
then-prune baselines. Also, we develop two techniques during supernet training
to more effectively identify such SuperTickets, pushing forward the frontier of
accuracy-efficiency trade-offs. Moreover, we test the transferability of SuperTick-
ets to reveal their potential for being task-agnostic. Results on three tasks and
four datasets consistently demonstrate the superiority of proposed two-in-one
framework and the resulting SuperTickets, opening up a new perspective in
searching and pruning for more accurate and efficient networks.
Acknowledgement
We would like to acknowledge the funding support from the NSF NeTS funding
(Award number: 1801865) and NSF SCH funding (Award number: 1838873) for
this project.

--- PAGE 15 ---
SuperTickets 15
References
1. Baevski, A., Hsu, W.N., Xu, Q., Babu, A., Gu, J., Auli, M.: data2vec: A gen-
eral framework for self-supervised learning in speech, vision and language. arXiv
preprint arXiv:2202.03555 (2022)
2. Bender, G., Kindermans, P.J., Zoph, B., Vasudevan, V., Le, Q.: Understanding and
simplifying one-shot architecture search. In: International Conference on Machine
Learning. pp. 550–559. PMLR (2018)
3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-all: Train one network
and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791 (2019)
4. Chen, M., Peng, H., Fu, J., Ling, H.: Autoformer: Searching transformers for vi-
sual recognition. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) (2021)
5. Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., Liu, J.: Earlybert: Efficient
bert training via early-bird lottery tickets. arXiv preprint arXiv:2101.00063 (2020)
6. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 3213–3223 (2016)
7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248–255. Ieee (2009)
8. Ding, M., Lian, X., Yang, L., Wang, P., Jin, X., Lu, Z., Luo, P.: Hr-nas: Search-
ing efficient high-resolution neural architectures with lightweight transformers. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. pp. 2982–2992 (2021)
9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:
Nap: Neural architecture search with pruning. Neurocomputing (2022)
10. Feng, Q., Xu, K., Li, Y., Sun, Y., Wang, D.: Edge-wise one-level global pruning
on nas generated networks. In: Chinese Conference on Pattern Recognition and
Computer Vision (PRCV). pp. 3–15. Springer (2021)
11. Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In: International Conference on Learning Representations (2019),
https://openreview.net/forum?id=rJl-b3RcF7
12. Frankle, J., Dziugaite, G.K., Roy, D., Carbin, M.: Linear mode connectivity and
the lottery ticket hypothesis. In: International Conference on Machine Learning.
pp. 3259–3269. PMLR (2020)
13. Gordon, M.A., Duh, K., Andrews, N.: Compressing bert: Studying the effects of
weight pruning on transfer learning. arXiv preprint arXiv:2002.08307 (2020)
14. Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., Sun, J.: Single path one-
shot neural architecture search with uniform sampling. In: European Conference
on Computer Vision. pp. 544–560. Springer (2020)
15. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 (2015)
16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770–778 (2016), https://github.com/facebookarchive/fb.resnet.torch
17. Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu,
Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: Proceedings

--- PAGE 16 ---
16 H. You et al.
of the IEEE/CVF International Conference on Computer Vision. pp. 1314–1324
(2019)
18. Huang, G., Liu, S., Van der Maaten, L., Weinberger, K.Q.: Condensenet: An ef-
ficient densenet using learned group convolutions. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 2752–2761 (2018)
19. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In: International conference on machine learning.
pp. 448–456. PMLR (2015)
20. Lee, N., Ajanthan, T., Torr, P.H.: Snip: Single-shot network pruning based on
connection sensitivity. arXiv preprint arXiv:1810.02340 (2018)
21. Li, Z., Yuan, G., Niu, W., Zhao, P., Li, Y., Cai, Y., Shen, X., Zhan, Z., Kong,
Z., Jin, Q., et al.: Npas: A compiler-aware framework of unified network pruning
and architecture search for beyond real-time mobile acceleration. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
14255–14266 (2021)
22. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference
on computer vision. pp. 740–755. Springer (2014)
23. Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille, A.L., Fei-Fei, L.:
Auto-deeplab: Hierarchical neural architecture search for semantic image segmen-
tation. In: Proceedings of the IEEE/CVF conference on computer vision and pat-
tern recognition. pp. 82–92 (2019)
24. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055 (2018)
25. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer: Hierarchical vision transformer using shifted windows. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 10012–10022
(2021)
26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-
tional networks through network slimming. In: Proceedings of the IEEE interna-
tional conference on computer vision. pp. 2736–2744 (2017)
27. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguis-
tic representations for vision-and-language tasks. Advances in neural information
processing systems 32(2019)
28. Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Multi-task vision
and language representation learning. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 10437–10446 (2020)
29. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In: Proceedings of the European conference on
computer vision (ECCV). pp. 116–131 (2018)
30. Mei, J., Li, Y., Lian, X., Jin, X., Yang, L., Yuille, A., Yang, J.: Atomnas: Fine-
grained end-to-end neural architecture search. arXiv preprint arXiv:1912.09640
(2019)
31. Mukund Kalibhat, N., Balaji, Y., Feizi, S.: Winning lottery tickets in deep gener-
ative models. arXiv e-prints pp. arXiv–2010 (2020)
32. Qin, E., Samajdar, A., Kwon, H., Nadella, V., Srinivasan, S., Das, D., Kaul, B.,
Krishna, T.: Sigma: A sparse and irregular gemm accelerator with flexible in-
terconnects for dnn training. In: 2020 IEEE International Symposium on High
Performance Computer Architecture (HPCA). pp. 58–70. IEEE (2020)

--- PAGE 17 ---
SuperTickets 17
33. Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., Rastegari, M.: What’s
hidden in a randomly weighted neural network? In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 11893–11902 (2020)
34. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-
verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 4510–4520 (2018)
35. Shaw, A., Hunter, D., Landola, F., Sidhu, S.: Squeezenas: Fast neural architec-
ture search for faster semantic segmentation. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision Workshops. pp. 0–0 (2019)
36. Su, X., You, S., Xie, J., Zheng, M., Wang, F., Qian, C., Zhang, C., Wang, X., Xu,
C.: Vision transformer architecture search. arXiv preprint arXiv:2106.13700 (2021)
37. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V.:
Mnasnet: Platform-aware neural architecture search for mobile. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
2820–2828 (2019)
38. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural net-
works. In: International Conference on Machine Learning. pp. 6105–6114. PMLR
(2019)
39. Tan, M., Le, Q.V.: Mixconv: Mixed depthwise convolutional kernels. arXiv preprint
arXiv:1907.09595 (2019)
40. Wan, A., Dai, X., Zhang, P., He, Z., Tian, Y., Xie, S., Wu, B., Yu, M., Xu, T.,
Chen, K., et al.: Fbnetv2: Differentiable neural architecture search for spatial and
channel dimensions. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 12965–12974 (2020)
41. Wang, D., Gong, C., Li, M., Liu, Q., Chandra, V.: Alphanet: Improved training of
supernet with alpha-divergence. arXiv preprint arXiv:2102.07954 (2021)
42. Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., Han, S.: Hat: Hardware-
aware transformers for efficient natural language processing. arXiv preprint
arXiv:2005.14187 (2020)
43. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan,
M., Wang, X., et al.: Deep high-resolution representation learning for visual recog-
nition. IEEE transactions on pattern analysis and machine intelligence 43(10),
3349–3364 (2020)
44. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,
Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-
ral architecture search. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 10734–10742 (2019)
45. Wu, B., Li, C., Zhang, H., Dai, X., Zhang, P., Yu, M., Wang, J., Lin, Y., Vajda, P.:
Fbnetv5: Neural architecture search for multiple tasks in one run. arXiv preprint
arXiv:2111.10007 (2021)
46. Xu, J., Tan, X., Luo, R., Song, K., Li, J., Qin, T., Liu, T.Y.: Nas-bert: task-
agnostic and adaptive-size bert compression with neural architecture search. In:
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining. pp. 1933–1943 (2021)
47. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving the
consistency, efficiency, and flexibility of differentiable neural architecture search.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 6667–6676 (June 2021)
48. You, F., Li, J., Zhao, Z.: Test-time batch statistics calibration for covariate shift.
arXiv preprint arXiv:2110.04065 (2021)

--- PAGE 18 ---
18 H. You et al.
49. You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.G., Wang, Z.,
Lin, Y.: Drawing early-bird tickets: Toward more efficient training of deep net-
works. In: International Conference on Learning Representations (2020), https:
//openreview.net/forum?id=BJxsrgStvr
50. You, H., Lu, Z., Zhou, Z., Fu, Y., Lin, Y.: Early-bird gcns: Graph-network co-
optimization towards more efficient gcn training and inference via drawing early-
bird lottery tickets. In: Association for the Advancement of Artificial Intelligence
(2022)
51. Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Bilateral segmenta-
tion network for real-time semantic segmentation. In: Proceedings of the European
conference on computer vision (ECCV). pp. 325–341 (2018)
52. Yu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.J., Tan, M., Huang, T., Song,
X., Pang, R., Le, Q.: Bignas: Scaling up neural architecture search with big single-
stage models. In: European Conference on Computer Vision. pp. 702–717. Springer
(2020)
53. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-
tional neural network for mobile devices. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 6848–6856 (2018)
54. Zhang, Z., Chen, X., Chen, T., Wang, Z.: Efficient lottery ticket finding: Less
data is more. In: International Conference on Machine Learning. pp. 12380–12390.
PMLR (2021)
55. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing
through ade20k dataset. In: Proceedings of the IEEE conference on computer vision
and pattern recognition. pp. 633–641 (2017)
56. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures
for scalable image recognition. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 8697–8710 (2018)

--- PAGE 19 ---
SuperTickets 19
A Visualization of The Adopted Supernet Architecture
We visualize the adopted supernet following [8] in Fig. 9. It begins with two
3×3 convolutions with stride 2, which are followed by five fusion modules and
five parallel modules to gradually divide it into four branches of decreasing res-
olutions, the learned features from all branches are then merged together for
classification or dense prediction.
T ext
Fusion
ModuleFusion
ModuleFusion
ModuleFusion
ModuleFusion
ModuleConcatDense
Prediction
ClassificationPredict
270
144723618
2418
324
Input
LayersParallel
ModuleParallel
ModuleParallel
ModuleParallel
ModuleParallel
ModuleOutput
Layers
min= 1 
mout=1 
nsb=[1] 
nc=[18]min= 1 
mout=2 
nsb=[2,2]
nc=[18,36]min= 2 
mout=3 
nsb=[2,2,3]  
nc=[18,36,72]min= 3 
mout=4 
nsb=[2,2,3,4]  
nc=[18,36,72,144]min= 4 
mout=4 
nsb=[2,2,3,4]  
nc=[18,36,72,144]
Fig. 9. Visualization of the adopted supernet architecture, where minandmoutdenote
the number of input and output branches in the fusion module; nsbandncrepresent
the number of searching blocks and channels in the parallel module, respectively.
B SuperTickets (ST) vs. Random Pruning (RP) and
Random Re-Initialization (RR-Init).
We compare the proposed SuperTickets (ST) with both the “ST w/ RP” and
“ST w/ RR-Init” in Table 7. We consider two datasets under 80% and 90%
sparsity: ST consistently outperforms the two baselines, achieving on-average
36.28%/42.03%/21.95% and 11.20%/12.27%/4.34% mIoU/mAcc/aAcc improve-
ments over “ST w/ RP” and “ST w/ RR-Init”, respectively, under a comparable
number of parameters and FLOPs. These experiments show that SuperTickets
performs better than both RP and RR-Init, which is consistent with the LTH
finding.
Transferability of ST vs. RP and RR-Init. Similarly, we compare the
transferability of the three ST variants when transferring them across different
datasets, including (1) ADE20K →Cityscapes or (2) Cityscapes →ADE20K. As
shown in Table 8, ST achieves on-average 33.14%/39.38%/21.92% and 11.37%/13.
67%/3.20% mIoU/mAcc/aAcc improvements over the “ST w/ RP” and “ST w/

--- PAGE 20 ---
20 H. You et al.
Table 7. Comparing ST with “ST w/ RP” and “ST w/ RR-Init” on both Cityscapes
and ADE20K under 80% and 90% sparsity.
Methods FLOPsCityscapes ( p= 80%)
FLOPsCityscapes ( p= 90%)
mIoU mAcc aAcc mIoU mAcc aAcc
S+P w/ RP 405M 1.30% 5.17% 21.96% 203M 1.15% 5.26% 21.9%
ST w/ RP 397M 20.17% 27.57% 68.73% 200M 16.55% 23.83% 65.90%
ST w/ RR-Init 397M 56.88% 67.33% 92.62% 200M 52.96% 62.66% 91.91%
ST 397M 69.77% 79.76% 95.12% 200M 66.61% 76.30% 94.63%
Methods FLOPsADE20K ( p= 80%)
FLOPsADE20K ( p= 90%)
mIoU mAcc aAcc mIoU mAcc aAcc
S+P w/ RP 308M 0.06% 0.66% 6.54% 154M 0.01% 0.66% 1.72%
ST w/ RP 317M 8.58% 11.93% 60.21% 159M 4.98% 7.19% 55.30%
ST w/ RR-Init 317M 21.24% 30.62% 68.57% 159M 19.49% 28.46% 67.26%
ST 317M 31.19% 43.10% 74.82% 159M 27.82% 39.49% 73.37%
Table 8. ST variants transfer validation tests under 90% sparsity.
MethodsADE20K →Cityscapes
MethodsCityscapes →ADE20K
mIoU mAcc aAcc mIoU mAcc aAcc
ST w/ RP 10.51% 14.42% 61.28% ST w/ RP 6.95% 10.1% 57.7%
ST w/ RR-Init 46.19% 54.92% 90.88% ST w/ RR-Init 14.82% 21.02% 65.54%
ST 62.91% 73.32% 93.82% ST 20.83% 29.95% 69.00%
RR-Init” baselines, respectively, indicating that RP and RR-Init are inferior in
transferability as compared to the proposed ST.
C Clarification of the LTH Settings.
There are two confusing settings when talking about LTH: (1) directly test the
accuracy of the found structure and the trained weights; and (2) the weights
are restored to their initial value and trained with the obtained mask to obtain
test accuracy. We tried both of the aforementioned settings and find the former,
i.e., directly testing the accuracy of the found structure and trained weights, has
already achieved good results. This is another highlight of our work, as it can
help to largely save the retraining time. Furthermore, to address your concern,
we re-initialize the SuperTickets to (1) their initial values, following the origin
LTH (“ST w/ LT-Init”) and (2) early or (3) late stages following [12] (“ST
w/ ELT-Init or LLT-Init”), and compare them with the RR-Init counterparts.
From Table 9, we can see that (1)ST under all LTH settings achieves better
accuracy than RR-Init, indicating the effectiveness of ST; (2)vanilla LT-Init
underperforms both ELT-Init and LLT-Init under ST settings, consistent with
[12]; and (3)ST w/ ELT-Init or LLT-Init achieves comparable or slightly better
accuracy than ST w/o Retrain at a cost of retraining.

--- PAGE 21 ---
SuperTickets 21
Table 9. Comparing ST w/ various LTH settings (90% sparsity).
MethodsCityscapes ( p= 90%)
MethodsADE20K ( p= 90%)
mIoU mAcc aAcc mIoU mAcc aAcc
ST w/ RR-Init 52.96% 62.66% 91.91% ST w/ RR-Init 19.49% 28.46% 67.26%
ST w/ LT-Init 59.63% 70.24% 93.33% ST w/ LT-Init 25.32% 36.76% 71.33%
ST w/ ELT-Init 65.82% 76.74% 94.54% ST w/ ELT-Init 25.79% 37.33% 72.10%
ST w/ LLT-Init 67.17% 77.03% 94.73% ST w/ LLT-Init 28.51% 40.63% 73.49%
ST w/o Retrain 66.61% 77.03% 94.73% ST w/o Retrain 27.82% 39.49% 73.37%
D Speedups in terms of Inference Time
In addition to the number of parameters and FLOPs, we measure the inference
FPS and speedups on both 1080Ti GPUs and a SOTA sparse DNN inference
accelerator [32]. As shown in Table 10, ST achieves on par or even higher (i.e.,
1.8×∼2.9×speedups) FPS on GPUs and much reduced accelerator time (i.e.,
2.9×∼4.1×speedups) on [32] than the baselines, thanks to simultaneous archi-
tecture searching and parameter pruning (i.e., 2-in-1) and ST.
Table 10. ST vs. typical baselines on Cityscapes, in terms of inference time measured
on both GPUs and sparse accelerators.
Model Params FLOPs mIoU GPU FPS Sparse Acc. Time
BiSeNet 5.8M 6.6G 69.00% 105.8 180.8ms
DF1-Seg-d8 - - 71.40% 136.9 181.7ms
FasterSeg 4.4M - 71.50% 163.9 142.4ms
SqueezeNAS 0.73M 8.4G 72.40% 117.2 198.5ms
ST ( p= 50%) 0.63M 1.0G 72.68% 310.7 48.3ms
E Discussions
Limitations of Transferred SuperTickets. Although identified SuperTickets
can transfer with only classifiers as task-specific, there is still a limitation in the
transferred SuperTickets. That is, transferred SuperTickets cannot surpass those
SuperTickets directly found on the target datasets/tasks. Moreover, when the
sparsity is low (e.g., 30%), the transferred SuperTickets will underperform both
SuperTickets and S+P. This is counterintuitive and opposite to the observation
in compressing pretrained models [13], where low pruning ratios do not hurt the
accuracy after transferring while overpruning leads to under-fitting. It implies
that the dedicated search is necessary when pruning ratio is relatively low; while
for high sparsity, the impacts of neural architectures will be less.
Visualization and Discussion. We visualize the results of SuperTickets
and S+P baselines on COCO keypoint and Cityscapes datasets under different

--- PAGE 22 ---
22 H. You et al.
S+P (Mag.) SuperT ickets
90% SparsityS+P (Mag.) SuperT ickets
70% Sparsity
70% SparsityS+P (Mag.) SuperT ickets
95% SparsityS+P (Mag.) SuperT ickets(a) Human Pose Estimation
(b) Semantic Segmentation
Fig. 10. Visualization of the human pose estimation on COCO keypoint dataset and
the streetview/semantic labels on Cityscapes dataset under different pruning ratios.
pruning ratios, as shown in Fig. 10. We observe that S+P baselines work but
miss some keypoints or semantic understandings under medium sparsity (e.g.,
70%) while collapse under high pruning ratios (e.g, 90/95%); In contrast, our
identified SuperTickets consistently work well among a wide range of pruning
ratios, validating the effectiveness of our proposed SuperTickets.
F More Visualization of Visual Recognition Results
We further visualize the results of SuperTickets and S+P baselines on COCO
keypoint and Cityscapes datasets under different pruning ratios, as shown in Fig.
11 and Fig. 12, respectively. We observe that S+P baselines work but miss some
keypoints or semantic understandings under medium sparsity (e.g., 70/80%)
while collapse under high pruning ratios (e.g, 90/95%); In contrast, our identi-
fied SuperTickets consistently work well among a wide range of pruning ratios,
validating the effectiveness of our proposed SuperTickets.

--- PAGE 23 ---
SuperTickets 23
Fig. 11. Visualization of the human pose estimation on COCO keypoint dataset under
various pruning ratios.

--- PAGE 24 ---
24 H. You et al.
70% SparsityS+P (Mag.) SuperTickets
95% SparsityS+P (Mag.) SuperTickets
Semantic Segmentation
80% Sparsity
90% Sparsity
S+P (Mag.) SuperTickets S+P (Mag.) SuperTickets
70% SparsityS+P (Mag.) SuperTickets
95% SparsityS+P (Mag.) SuperTickets
Semantic Segmentation
80% Sparsity
90% Sparsity
S+P (Mag.) SuperTickets S+P (Mag.) SuperTickets
Fig. 12. Visualization of the streetview/semantic labels on Cityscapes dataset under
various pruning ratios.
