# NASRec: Chia sẻ Trọng số Tìm kiếm Kiến trúc Mạng Neural cho Hệ thống Gợi ý

Tunhou Zhang∗
Duke University
Durham, USA
tunhou.zhang@duke.edu

Dehua Cheng
Meta AI
Menlo Park, USA
dehuacheng@fb.com

Yuchen He
Meta AI
Menlo Park, USA
yuchenhe@fb.com

Zhengxing Chen
Meta AI
Menlo Park, USA
czxttkl@fb.com

Xiaoliang Dai
Meta AI
Menlo Park, USA
xiaoliangdai@fb.com

Liang Xiong
Meta AI
Menlo Park, USA
lxiong@fb.com

Feng Yan
University of Houston
Houston, USA
fyan5@central.uh.edu

Hai Li
Duke University
Durham, USA
hai.li@duke.edu

Yiran Chen
Duke University
Durham, USA
yiran.chen@duke.edu

Wei Wen†
Meta AI
Menlo Park, USA
wewen@fb.com

## TÓM TẮT

Sự phát triển của mạng neural sâu mang đến những cơ hội mới trong việc tối ưu hóa hệ thống gợi ý. Tuy nhiên, việc tối ưu hóa hệ thống gợi ý sử dụng mạng neural sâu đòi hỏi sự chế tác kiến trúc tinh xảo. Chúng tôi đề xuất NASRec, một mô hình huấn luyện một supernet duy nhất và hiệu quả tạo ra nhiều mô hình/kiến trúc con phong phú bằng cách chia sẻ trọng số. Để vượt qua những thách thức về tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc trong lĩnh vực gợi ý, NASRec thiết lập một supernet lớn (tức là không gian tìm kiếm) để tìm kiếm toàn bộ kiến trúc. Supernet kết hợp sự lựa chọn đa dạng của toán tử và kết nối dày đặc để giảm thiểu nỗ lực con người trong việc tìm kiếm tiên nghiệm. Quy mô và tính không đồng nhất trong NASRec đặt ra một số thách thức, như hiệu quả huấn luyện, mất cân bằng toán tử, và suy giảm tương quan thứ hạng. Chúng tôi giải quyết những thách thức này bằng cách đề xuất lấy mẫu toán tử đơn kết nối bất kỳ, mô-đun tương tác cân bằng toán tử, và tinh chỉnh sau huấn luyện. Các mô hình được chế tác của chúng tôi, NASRecNet, cho thấy kết quả hứa hẹn trên ba bộ chuẩn dự đoán Tỷ lệ Nhấp chuột (CTR), cho thấy rằng NASRec vượt trội cả mô hình thiết kế thủ công và phương pháp NAS hiện có với hiệu suất tiên tiến nhất. Công trình của chúng tôi được công bố công khai tại đây.

**TỪ KHÓA**
hệ thống gợi ý, tìm kiếm kiến trúc mạng neural, chia sẻ trọng số, tiến hóa có quy tắc, mạng neural

∗Phần lớn công việc này được thực hiện khi tác giả đầu tiên là thực tập sinh tại Meta Platforms, Inc.
†Tác giả liên hệ. Quản lý thực tập sinh.

## 1 GIỚI THIỆU

Học sâu đóng vai trò thiết yếu trong việc thiết kế hệ thống gợi ý hiện đại ở quy mô web trong các ứng dụng thực tế. Ví dụ, các công cụ tìm kiếm và mạng xã hội được sử dụng rộng rãi nhất [5,17] khai thác hệ thống gợi ý (hoặc hệ thống xếp hạng) để tối ưu hóa Tỷ lệ Nhấp chuột (CTR) của các trang cá nhân hóa [8,13,23]. Các mô hình học sâu dựa vào kỹ thuật thiết kế kiến trúc mạng neural tinh xảo.

Hệ thống gợi ý dựa trên học sâu, đặc biệt là dự đoán CTR, mang theo một thiết kế kiến trúc mạng neural dựa trên các đặc trưng đa phương thức. Trong thực tế, nhiều thách thức phát sinh. Các đặc trưng đa phương thức, như đặc trưng điểm nổi, số nguyên và phân loại, đưa ra một thách thức cụ thể trong mô hình hóa tương tác đặc trưng và tối ưu hóa mạng neural. Việc tìm một mô hình xương sống tốt với các kiến trúc không đồng nhất gán các tiên nghiệm phù hợp cho các đặc trưng đa phương thức là những thực hành phổ biến trong hệ thống gợi ý dựa trên học sâu [7,14,16,19,23,25,27,28]. Tuy nhiên, các phương pháp này vẫn dựa vào nỗ lực thủ công đáng kể và gặp phải những hạn chế, như không gian thiết kế hẹp và thử nghiệm thực nghiệm không đủ bị giới hạn bởi tài nguyên có sẵn. Kết quả là, những hạn chế này làm tăng khó khăn trong việc thiết kế một bộ trích xuất đặc trưng tốt.

Sự phát triển của Học máy Tự động (AutoML), đặc biệt là Tìm kiếm Kiến trúc Mạng Neural (NAS) [4,21,37,39], trong lĩnh vực thị giác, soi sáng việc tối ưu hóa mô hình của hệ thống gợi ý. NAS Chia sẻ Trọng số (WS-NAS) [3,4,21] được áp dụng phổ biến trong lĩnh vực thị giác để giải quyết việc thiết kế các mô hình thị giác hiệu quả. Tuy nhiên, việc áp dụng NAS chia sẻ trọng số cho lĩnh vực gợi ý thách thức hơn nhiều so với lĩnh vực thị giác vì tính đa phương thức trong dữ liệu và tính không đồng nhất trong kiến trúc. Ví dụ, (1) trong thị giác, các đầu vào của các khối xây dựng trong [4,33] là các tensor 3D đồng nhất, nhưng hệ thống gợi ý nhận các đặc trưng đa phương thức tạo ra tensor 2D và 3D. (2) Các mô hình thị giác đơn giản xếp chồng các khối xây dựng giống nhau, và do đó NAS tiên tiến nhất trong thị giác hội tụ để đơn giản tìm kiếm cấu hình kích thước thay vì motif kiến trúc, như chiều rộng kênh, kích thước kernel, và lặp lại lớp [4,38]. Tuy nhiên, các mô hình gợi ý không đồng nhất với mỗi giai đoạn của mô hình sử dụng một khối xây dựng hoàn toàn khác [7,14,19,23]. (3) Các mô hình thị giác chủ yếu sử dụng toán tử tích chập làm khối xây dựng chính trong khi hệ thống gợi ý được xây dựng trên các toán tử không đồng nhất, như lớp Fully-Connected, Gating, Sum, Dot-Product, Multi-Head Attention, Factorization Machine, v.v.

Do những thách thức nêu trên, nghiên cứu về NAS trong hệ thống gợi ý rất hạn chế. Ví dụ, các không gian tìm kiếm trong AutoCTR [30] và DNAS [18] theo nguyên tắc thiết kế của DLRM [23] được chế tác thủ công và chúng chỉ bao gồm lớp Fully-Connected và Dot-Product như các toán tử có thể tìm kiếm. Chúng cũng dựa nhiều vào các toán tử được chế tác thủ công, như Factorization Machine [30] hoặc mô-đun tương tác đặc trưng [11] trong không gian tìm kiếm để tăng tính không đồng nhất kiến trúc. Hơn nữa, các công trình hiện có gặp phải chi phí tính toán khổng lồ [30] hoặc tối ưu hóa hai cấp thách thức [18], và do đó chúng chỉ sử dụng các không gian thiết kế hẹp (đôi khi với tiên nghiệm con người mạnh [11]) để chế tác kiến trúc, không khuyến khích tương tác đặc trưng đa dạng và làm hại chất lượng của các mô hình được khám phá.

Trong bài báo này, chúng tôi đề xuất NASRec, một mô hình mới để hoàn toàn kích hoạt NAS cho Hệ thống Gợi ý thông qua Tìm kiếm Kiến trúc Mạng Neural Chia sẻ Trọng số (WS-NAS) dưới tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc. Bảng 1 tóm tắt sự tiến bộ của NASRec so với các phương pháp NAS khác. Chúng tôi đạt được điều này bằng cách đầu tiên xây dựng một supernet kết hợp nhiều toán tử không đồng nhất hơn các công trình trước, bao gồm lớp Fully-Connected (FC), Gating, Sum, Dot-Product, Self-Attention, và lớp Embedded Fully-Connected (EFC). Trong supernet, chúng tôi kết nối dày đặc một chuỗi các khối, mỗi khối bao gồm tất cả toán tử như các tùy chọn. Vì bất kỳ khối nào cũng có thể nhận bất kỳ nhúng đặc trưng thô và tensor trung gian nào bằng kết nối dày đặc, supernet không bị giới hạn bởi bất kỳ phương thức dữ liệu cụ thể nào. Thiết kế supernet như vậy giảm thiểu việc mã hóa tiên nghiệm con người bằng cách giới thiệu "Không gian Tìm kiếm NASRec", hỗ trợ bản chất của tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc trong hệ thống gợi ý, và bao phủ các mô hình vượt ra ngoài các mô hình gợi ý phổ biến như Wide & Deep [7], DeepFM [14], DLRM [23], AutoCTR [30], DNAS [18], và PROFIT [11].

Supernet về cơ bản tạo thành một không gian tìm kiếm. Chúng tôi thu được một mô hình bằng cách loại bỏ một số toán tử và kết nối trong supernet, nghĩa là một subnet của supernet tương đương với một mô hình. Vì tất cả subnet chia sẻ trọng số từ cùng một supernet, nó được gọi là NAS Chia sẻ Trọng số. Để hiệu quả tìm kiếm mô hình/subnet trong không gian tìm kiếm NASRec, chúng tôi cải tiến các phương pháp một lần [4,38] cho lĩnh vực gợi ý. Chúng tôi đề xuất lấy mẫu toán tử đơn kết nối bất kỳ để tách rời việc lựa chọn toán tử và tăng độ bao phủ kết nối, các khối tương tác cân bằng toán tử để công bằng huấn luyện subnet trong supernet, và tinh chỉnh sau huấn luyện để giảm đồng thích ứng trọng số. Những phương pháp này cho phép hiệu quả huấn luyện tốt hơn và xếp hạng của các mô hình subnet trong supernet, dẫn đến giảm ~0.001 log loss của các mô hình được tìm kiếm trên không gian tìm kiếm NASRec đầy đủ.

Chúng tôi đánh giá các mô hình được chế tác bởi NAS, NASRecNet trên ba bộ chuẩn CTR phổ biến và chứng minh những cải thiện đáng kể so với cả mô hình chế tác thủ công và mô hình chế tác NAS. Đáng chú ý, NASRecNet cải tiến hiện trạng với giảm log loss ~0.001, ~0.003 trên Criteo và KDD Cup 2012, tương ứng. Trên Avazu, NASRec cải tiến PROFIT [11] hiện trạng với cải thiện AUC ~0.002 và log loss ngang bằng, trong khi vượt trội PROFIT [11] trên Criteo với giảm log loss ~0.003.

NASRec chỉ cần huấn luyện một supernet duy nhất nhờ vào cơ chế chia sẻ trọng số hiệu quả, và do đó giảm đáng kể chi phí tìm kiếm. Chúng tôi tóm tắt những đóng góp chính dưới đây.

• Chúng tôi đề xuất NASRec, một mô hình mới để mở rộng mô hình tự động của hệ thống gợi ý. NASRec thiết lập một supernet linh hoạt (không gian tìm kiếm) với tiên nghiệm con người tối thiểu, vượt qua những thách thức về tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc trong lĩnh vực gợi ý.

• Chúng tôi cải tiến NAS chia sẻ trọng số cho lĩnh vực gợi ý bằng cách giới thiệu lấy mẫu toán tử đơn kết nối bất kỳ, mô-đun tương tác cân bằng toán tử, và tinh chỉnh sau huấn luyện.

• Các mô hình được chế tác của chúng tôi, NASRecNet, vượt trội cả mô hình chế tác thủ công và mô hình chế tác NAS với chi phí tìm kiếm nhỏ hơn.

## 2 CÔNG TRÌNH LIÊN QUAN

**Hệ thống gợi ý dựa trên học sâu.** Hệ thống gợi ý dựa trên máy như dự đoán Tỷ lệ Nhấp chuột (CTR) đã được nghiên cứu kỹ lưỡng trong nhiều phương pháp khác nhau, như Hồi quy Logistic [27], và Cây Quyết định Gradient-Boosting [16]. Các phương pháp gần đây hơn nghiên cứu tương tác dựa trên học sâu của các loại đặc trưng khác nhau thông qua Mạng Neural Wide & Deep [7], DeepCrossing [28], Factorization Machines [14,19], Dot-Product [23] và cơ chế gating [34,35]. Một hướng nghiên cứu khác tìm kiếm tương tác đặc trưng hiệu quả, như nhân đặc trưng [36] và sparsification [9] để xây dựng hệ thống gợi ý nhẹ. Tuy nhiên, những công trình này hoạt động với chi phí nỗ lực thủ công to lớn và gặp phải hiệu suất dưới tối ưu và lựa chọn thiết kế bị hạn chế do giới hạn trong cung cấp tài nguyên. Công trình của chúng tôi thiết lập một mô hình mới về học các mô hình gợi ý hiệu quả bằng cách chế tác một "không gian tìm kiếm NASRec" có thể mở rộng kết hợp tất cả motif thiết kế phổ biến trong các công trình hiện có. Không gian tìm kiếm NASRec mới hỗ trợ một loạt rộng lựa chọn thiết kế và cho phép tối ưu hóa có thể mở rộng để chế tác các mô hình gợi ý với các yêu cầu khác nhau.

**Tìm kiếm Kiến trúc Mạng Neural.** Tìm kiếm Kiến trúc Mạng Neural tự động hóa việc thiết kế Mạng Neural Sâu trong nhiều ứng dụng khác nhau: sự phổ biến của Tìm kiếm Kiến trúc Mạng Neural liên tục tăng trong Thị giác Máy tính [4,21,37,39], Xử lý Ngôn ngữ Tự nhiên [29,33], và Hệ thống Gợi ý [11,18,30]. Gần đây, NAS Chia sẻ Trọng số (WS-NAS) [4,33] thu hút sự chú ý của các nhà nghiên cứu: nó huấn luyện một supernet đại diện cho toàn bộ không gian tìm kiếm trực tiếp trên các nhiệm vụ mục tiêu, và hiệu quả đánh giá subnet (tức là kiến trúc con của supernet) với trọng số supernet được chia sẻ. Tuy nhiên, thực hiện WS-NAS trên hệ thống gợi ý thách thức vì hệ thống gợi ý được xây dựng trên các kiến trúc không đồng nhất dành riêng cho tương tác dữ liệu đa phương thức, do đó đòi hỏi không gian tìm kiếm linh hoạt hơn và thuật toán huấn luyện supernet hiệu quả. Những thách thức đó gây ra vấn đề đồng thích ứng [2] và vấn đề mất cân bằng toán tử [20] trong WS-NAS, cung cấp tương quan thứ hạng thấp hơn để phân biệt mô hình. NASRec giải quyết chúng bằng cách đề xuất lấy mẫu toán tử đơn kết nối bất kỳ, mô-đun tương tác cân bằng toán tử, và tinh chỉnh sau huấn luyện.

## 3 KHÔNG GIAN NASREC PHÂN CẤP CHO HỆ THỐNG GỢI Ý

Để hỗ trợ tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc trong hệ thống gợi ý, tính linh hoạt của không gian tìm kiếm là chìa khóa. Chúng tôi thiết lập một mô hình mới không có tiên nghiệm con người bằng cách giới thiệu không gian tìm kiếm NASRec, một thiết kế không gian tìm kiếm phân cấp kết hợp các toán tử xây dựng không đồng nhất và kết nối dày đặc, xem Hình 1. Quá trình thủ công chính trong việc thiết kế không gian tìm kiếm là đơn giản thu thập các toán tử phổ biến được sử dụng trong các phương pháp hiện có [14,19,23,30,34,35]. Ngoài ra, chúng tôi tiếp tục kết hợp Transformer Encoder [32] phổ biến vào không gian tìm kiếm NASRec để có tính linh hoạt tốt hơn và tiềm năng cao hơn trong các kiến trúc được tìm kiếm, nhờ vào sự thống trị của nó trong các ứng dụng như ViT [10] cho nhận dạng hình ảnh, Transformer [32] cho xử lý ngôn ngữ tự nhiên, và khám phá mới nổi của nó trong hệ thống gợi ý [6,12].

Tiếp theo, chúng tôi trình bày không gian tìm kiếm NASRec.

### 3.1 Không gian Tìm kiếm NASRec

Trong hệ thống gợi ý, chúng tôi định nghĩa một đầu vào dày đặc là X_d ∈ R^(B×dim_d) là một tensor 2D từ các đặc trưng dày đặc thô hoặc được tạo bởi các toán tử, như FC, Gating, Sum, và Dot-Product. Một đầu vào thưa X_s ∈ R^(B×N_s×dim_s) là một tensor 3D của nhúng thưa được tạo bởi các đặc trưng thưa/phân loại thô hoặc bởi các toán tử như EFC và self-attention. Tương tự, một đầu ra dày đặc hoặc thưa (tức là Y_d hoặc Y_s) được định nghĩa tương ứng là một tensor 2D hoặc 3D được tạo thông qua các khối xây dựng/toán tử tương ứng. Trong NASRec, tất cả đầu vào và đầu ra thưa chia sẻ cùng dim_s, bằng với chiều của nhúng thưa thô. Tương ứng, chúng tôi định nghĩa một toán tử dày đặc (thưa) là một toán tử tạo ra một đầu ra dày đặc (thưa). Trong NASRec, các toán tử dày đặc bao gồm FC, Gating, Sum, và Dot-Product tạo thành "nhánh dày đặc" (được đánh dấu màu xanh), và các toán tử thưa bao gồm EFC và self-attention, tạo thành "nhánh thưa" (được đánh dấu màu đỏ).

Một kiến trúc ứng viên trong không gian tìm kiếm NASRec là một chồng của N khối lựa chọn, tiếp theo là một lớp FC cuối cùng để tính logit. Mỗi khối lựa chọn chấp nhận một số lượng tùy ý các đầu vào đa phương thức, mỗi đầu vào là X = (X_d, X_s) từ một khối trước hoặc đầu vào thô, và tạo ra một đầu ra đa phương thức Y = (Y_d, Y_s) của cả tensor dày đặc Y_d và tensor thưa Y_s thông qua các toán tử xây dựng bên trong.

Trong mỗi khối lựa chọn, chúng tôi có thể lấy mẫu toán tử để tìm kiếm.

Chúng tôi xây dựng một supernet để đại diện cho không gian tìm kiếm NASRec, xem Hình 1. Supernet bao trùm tất cả các mô hình/subnet ứng viên có thể và thực hiện chia sẻ trọng số giữa các subnet để đồng thời huấn luyện tất cả chúng. Chúng tôi chính thức định nghĩa supernet NASRec S là một bộ ba kết nối C, toán tử O, và chiều D như sau: S = (C, D, O) trên tất cả N khối lựa chọn. Cụ thể, các toán tử: O = [O^(1), ..., O^(N)] liệt kê tập hợp các toán tử xây dựng từ khối lựa chọn 1 đến N. Các kết nối: C = [C^(1), ..., C^(N)] chứa kết nối <i, j> giữa khối lựa chọn i và khối lựa chọn j. Chiều: D = [D^(1), ..., D^(N)] chứa các cài đặt chiều từ khối lựa chọn 1 đến N.

Một subnet S_sample = (O_sample, C_sample, D_sample) trong supernet S đại diện cho một mô hình trong không gian tìm kiếm NASRec. Một khối sử dụng phép cộng để tổng hợp các đầu ra của các toán tử được lấy mẫu trong mỗi nhánh (tức là "nhánh dày đặc" hoặc "nhánh thưa"). Khi các chiều đầu ra toán tử không khớp, chúng tôi áp dụng một mặt nạ không để che các chiều thừa. Một khối sử dụng nối Concat để tổng hợp các đầu ra từ các kết nối được lấy mẫu. Cho một subnet được lấy mẫu S_sample, đầu vào X^(N) cho khối lựa chọn N được tính như sau cho một danh sách các đầu ra khối trước {Y^(1), ..., Y^(N-1)} và các kết nối được lấy mẫu C^(N)_sample:

X^(N)_d = Concat^(N-1)_(i=1) [Y^(i)_d · 1_{<i,N>∈C^(N)_sample}], (1)

X^(N)_s = Concat^(N-1)_(i=1) [Y^(i)_s · 1_{<i,N>∈C^(N)_sample}]. (2)

Ở đây, 1_b là 1 khi b đúng nếu không là 0.

Một toán tử xây dựng o ∈ O^(N)_sample biến đổi đầu vào được nối X^(N) thành một đầu ra trung gian với một chiều được lấy mẫu D^(N)_sample. Điều này đạt được bằng một hàm mặt nạ được áp dụng trên chiều cuối cùng cho đầu ra dày đặc và chiều giữa cho đầu ra thưa. Ví dụ, một đầu ra dày đặc Y^(N)_d được thu được như sau:

Y^(N)_d = Σ_(o∈O1) 1_{o∈O^(N)_sample} · Mask(o(X^(N)_d), D^(N)_sample, o). (3)

trong đó

Mask(V, d) = (V_{:,i}, nếu i < d; 0, Nếu không.). (4)

Tiếp theo, chúng tôi làm rõ tập hợp các toán tử xây dựng như sau:

• **Lớp Fully-Connected (FC).** Lớp Fully-Connected là xương sống của các mô hình DNN cho hệ thống gợi ý [7] trích xuất biểu diễn dày đặc. FC được áp dụng trên đầu vào dày đặc 2D, và theo sau bởi kích hoạt ReLU.

• **Lớp Sigmoid Gating (SG).** Chúng tôi theo trực giác trong [6,35] và sử dụng một toán tử xây dựng dày đặc, Sigmoid Gating, để tăng cường tiềm năng của không gian tìm kiếm. Cho hai đầu vào dày đặc X_d1 ∈ R^(B×dim_d1) và X_d2 ∈ R^(B×dim_d2), Sigmoid Gating tương tác hai đầu vào này như sau: SG(X_d1, X_d2) = sigmoid(FC(X_d1)) * X_d2. Nếu chiều của hai đầu vào dày đặc không khớp, một zero padding được áp dụng trên đầu vào có chiều thấp hơn.

• **Lớp Sum.** Toán tử xây dựng dày đặc này cộng hai đầu vào dày đặc: X_d1 ∈ R^(B×dim_d1), X_d2 ∈ R^(B×dim_d2) và hợp nhất hai đặc trưng từ các cấp độ khác nhau của mô hình hệ thống gợi ý bằng cách đơn giản thực hiện Sum(X_d1, X_d2) = X_d1 + X_d2. Tương tự như Sigmoid Gating, một zero padding được áp dụng trên đầu vào có chiều thấp hơn.

• **Lớp Dot-Product (DP).** Chúng tôi tận dụng Dot-Product để nắm bắt các tương tác giữa các đầu vào đa phương thức thông qua các tích vô hướng từng cặp. Dot-Product có thể nhận đầu vào dày đặc và/hoặc thưa, và tạo ra một đầu ra dày đặc. Những đầu vào thưa này, sau khi được gửi đến "nhánh dày đặc", có thể sau này tận dụng các toán tử dày đặc để học biểu diễn và tương tác tốt hơn. Cho một đầu vào dày đặc X_d ∈ R^(B×dim_d) và một đầu vào thưa X_s ∈ R^(B×N_c×dim_s), một Dot-Product đầu tiên nối chúng như X = Concat[X_d, X_s], và sau đó thực hiện tích vô hướng từng cặp: DP(X_d, X_s) = Triu(XX^T). dim_d được chiếu đầu tiên về dim_s nếu chúng không khớp.

• **Lớp Embedded Fully-Connected (EFC).** Một lớp EFC là một toán tử xây dựng thưa áp dụng FC dọc theo chiều giữa. Cụ thể, một EFC với trọng số W ∈ R^(N_in×N_out) biến đổi một đầu vào X_s ∈ R^(B×N_in×dim_s) thành Y_s ∈ R^(B×N_out×dim_s)

• **Lớp Attention (Attn).** Lớp Attention là một toán tử xây dựng thưa sử dụng cơ chế Multi-Head Attention (MHA) để học trọng số của đầu vào thưa và khai thác tốt hơn tương tác của chúng trong hệ thống gợi ý. Ở đây, chúng tôi áp dụng Transformer Encoder trên một đầu vào thưa cho trước X_s ∈ R^(B×N_s×dim_s), với các query, key, và value giống hệt nhau.

Chúng tôi quan sát rằng tập hợp các toán tử xây dựng nêu trên cung cấp cơ hội cho các đầu vào thưa biến đổi thành "nhánh dày đặc". Tuy nhiên, những toán tử này không cho phép một biến đổi của đầu vào dày đặc hướng về "nhánh thưa". Để giải quyết hạn chế này, chúng tôi giới thiệu "bộ hợp nhất dày đặc-thưa" cho phép đầu ra dày đặc/thưa tùy chọn hợp nhất vào "nhánh thưa/dày đặc".

• **Bộ hợp nhất "Dày đặc-thành-thưa".** Bộ hợp nhất này đầu tiên chiếu đầu ra dày đặc X_d sử dụng một lớp FC, sau đó sử dụng một lớp reshape để định hình lại phép chiếu thành một tensor thưa 3D. Tensor 3D được định hình lại được hợp nhất vào đầu ra thưa thông qua nối.

• **Bộ hợp nhất "Thưa-thành-dày đặc".** Bộ hợp nhất này sử dụng một Factorization Machine (FM) [14] để chuyển đổi đầu ra thưa thành một biểu diễn dày đặc, sau đó cộng biểu diễn dày đặc vào đầu ra dày đặc.

Ngoài sự lựa chọn phong phú của các toán tử xây dựng và bộ hợp nhất, mỗi khối lựa chọn cũng có thể nhận đầu vào từ bất kỳ khối lựa chọn trước đó nào, và các đặc trưng đầu vào thô. Điều này liên quan đến một khám phá của bất kỳ kết nối nào giữa các khối lựa chọn và đầu vào thô, mở rộng tính không đồng nhất dây nối cho tìm kiếm.

### 3.2 Các Thành phần Tìm kiếm

Trong không gian tìm kiếm NASRec, chúng tôi tìm kiếm kết nối, chiều toán tử, và toán tử xây dựng trong mỗi khối lựa chọn. Chúng tôi minh họa ba thành phần tìm kiếm chính như sau:

• **Kết nối.** Chúng tôi không đặt hạn chế nào về số lượng kết nối mà một khối lựa chọn có thể nhận: mỗi khối có thể chọn đầu vào từ một số lượng tùy ý các khối trước đó và đầu vào thô. Cụ thể, khối lựa chọn thứ n có thể kết nối với bất kỳ n-1 khối lựa chọn trước đó nào và các đặc trưng dày đặc (thưa) thô. Các đầu ra từ tất cả các khối trước được nối như đầu vào cho các khối xây dựng dày đặc (thưa). Chúng tôi riêng biệt nối các đầu ra dày đặc (thưa) từ các khối trước.

• **Chiều.** Trong một khối lựa chọn, các toán tử khác nhau có thể tạo ra các chiều tensor khác nhau. Trong NASRec, chúng tôi đặt kích thước đầu ra của FC và EFC thành dim_d và N_s, tương ứng; và các đầu ra toán tử khác trong nhánh dày đặc (thưa) được chiếu tuyến tính thành dim_d (N_s). Điều này đảm bảo các đầu ra toán tử trong mỗi nhánh có cùng chiều và có thể cộng lại với nhau. Điều này cũng cho các chiều tối đa dim_d và N_s cho đầu ra dày đặc Y_d ∈ R^(B×dim_d) và đầu ra thưa Y_s ∈ R^(B×N_s×dim_s). Cho một đầu ra dày đặc hoặc thưa, một mặt nạ trong Eq. 4 loại bỏ các chiều thừa, cho phép lựa chọn linh hoạt các chiều của toán tử xây dựng.

• **Toán tử.** Mỗi khối có thể chọn ít nhất một toán tử xây dựng dày đặc (thưa) để biến đổi đầu vào thành một đầu ra dày đặc (thưa). Mỗi khối nên duy trì ít nhất một toán tử trong nhánh dày đặc (thưa) để đảm bảo luồng thông tin từ đầu vào đến logit. Chúng tôi độc lập lấy mẫu các toán tử xây dựng trong nhánh dày đặc (thưa) để tạo thành một kiến trúc ứng viên hợp lệ. Ngoài ra, chúng tôi độc lập lấy mẫu các bộ hợp nhất dày đặc-thưa để cho phép tương tác dày đặc-thành-thưa tùy chọn.

Chúng tôi chế tác hai không gian tìm kiếm NASRec như ví dụ để chứng minh sức mạnh của không gian tìm kiếm NASRec.

• **NASRec-Small.** Chúng tôi giới hạn sự lựa chọn của toán tử trong mỗi khối thành FC, EFC, và Dot-Product, và cho phép bất kỳ kết nối nào giữa các khối. Điều này cung cấp một quy mô tương tự của không gian tìm kiếm như AutoCTR [30].

• **NASRec-Full.** Chúng tôi kích hoạt tất cả toán tử xây dựng, bộ hợp nhất và kết nối để xây dựng một không gian tìm kiếm tích cực để khám phá với tiên nghiệm con người tối thiểu. Dưới ràng buộc rằng ít nhất một toán tử phải được lấy mẫu trong cả nhánh dày đặc và thưa, kích thước không gian tìm kiếm NASRec-Full là 15^N lần của NASRec-Small, trong đó N là số khối lựa chọn. Không gian tìm kiếm đầy đủ này cực kỳ thử nghiệm khả năng của NASRec.

Sự kết hợp của tìm kiếm kết nối dày đặc đầy đủ và cấu hình chiều dày đặc/thưa độc lập cho không gian tìm kiếm NASRec một cardinality lớn. NASRec-Full có N = 7 khối, chứa lên đến 5×10^33 kiến trúc với tính không đồng nhất mạnh. Với tiên nghiệm con người tối thiểu và không gian tìm kiếm không bị ràng buộc như vậy, các phương pháp dựa trên mẫu brute-force có thể mất thời gian khổng lồ để tìm một mô hình tiên tiến nhất.

## 4 TÌMKIẾM KIẾN TRÚC MẠNG NEURAL CHIA SẺ TRỌNG SỐ CHO HỆ THỐNG GỢI Ý

Một supernet NASRec đồng thời ủ các mô hình subnet khác nhau trong không gian tìm kiếm NASRec, tuy nhiên đặt ra những thách thức cho hiệu quả huấn luyện và chất lượng xếp hạng do cardinality lớn của nó. Trong phần này, chúng tôi đầu tiên đề xuất một chiến lược lấy mẫu đường dẫn mới, lấy mẫu toán tử đơn kết nối bất kỳ, tách rời việc lấy mẫu toán tử với một hội tụ lấy mẫu kết nối tốt. Chúng tôi tiếp tục quan sát hiện tượng mất cân bằng toán tử được gây ra bởi một số toán tử quá tham số hóa, và giải quyết vấn đề này bằng tương tác cân bằng toán tử để cải thiện xếp hạng supernet. Cuối cùng, chúng tôi sử dụng tinh chỉnh sau huấn luyện để giảm nhẹ đồng thích ứng trọng số, và tiếp tục sử dụng tiến hóa có quy tắc để thu được subnet tốt nhất. Chúng tôi cũng cung cấp một tập hợp insights hiệu quả khám phá các mô hình gợi ý tốt nhất.

### 4.1 Lấy mẫu Toán tử Đơn Kết nối Bất kỳ

Huấn luyện supernet áp dụng một phương pháp giống như drop-out. Tại mỗi mini-batch, chúng tôi lấy mẫu và huấn luyện một subnet. Trong quá trình huấn luyện, chúng tôi huấn luyện rất nhiều subnet dưới chia sẻ trọng số, với mục tiêu rằng các subnet được huấn luyện tốt để dự đoán hiệu suất của mô hình. Các chiến lược lấy mẫu quan trọng để đạt được mục tiêu. Chúng tôi khám phá ba chiến lược lấy mẫu đường dẫn được mô tả trong Hình 2 và khám phá lấy mẫu Toán tử Đơn Kết nối Bất kỳ là cách hiệu quả nhất:

• **Chiến lược toán tử đơn kết nối đơn.** Chiến lược lấy mẫu đường dẫn này có gốc rễ trong Thị giác Máy tính [15]: nó đồng đều lấy mẫu một toán tử dày đặc đơn và một toán tử thưa đơn trong mỗi khối lựa chọn, và đồng đều lấy mẫu một kết nối đơn như một đầu vào cho một khối. Chiến lược này hiệu quả vì, trung bình, chỉ một subnet nhỏ được huấn luyện tại một mini-batch, tuy nhiên, chiến lược này chỉ khuyến khích công thức giống như chuỗi của mô hình mà không có các mẫu kết nối thêm. Việc thiếu độ bao phủ kết nối mang lại hội tụ chậm hơn, hiệu suất kém, và xếp hạng không chính xác của mô hình như chúng tôi sẽ chỉ ra.

• **Chiến lược Toán tử Bất kỳ Kết nối Bất kỳ.** Chiến lược lấy mẫu này tăng độ bao phủ của các kiến trúc con của supernet trong quá trình huấn luyện subnet: nó đồng đều lấy mẫu một số lượng tùy ý các toán tử dày đặc và thưa trong mỗi khối lựa chọn, và đồng đều lấy mẫu một số lượng tùy ý kết nối để tổng hợp các đầu ra khối khác nhau. Tuy nhiên, hiệu quả huấn luyện kém khi huấn luyện các subnet lớn được lấy mẫu. Quan trọng hơn, đồng thích ứng trọng số của nhiều toán tử trong một khối lựa chọn có thể ảnh hưởng đến đánh giá độc lập của các subnet, và do đó cuối cùng dẫn đến chất lượng xếp hạng kém như chúng tôi sẽ chỉ ra.

• **Toán tử đơn Kết nối bất kỳ.** Chúng tôi đề xuất chiến lược lấy mẫu đường dẫn này để kết hợp các điểm mạnh từ hai chiến lược trên. Lấy mẫu toán tử đơn Kết nối bất kỳ lấy mẫu một toán tử dày đặc đơn và một toán tử thưa đơn trong mỗi khối lựa chọn, và lấy mẫu một số lượng tùy ý kết nối để tổng hợp các đầu ra từ các khối lựa chọn khác nhau. Insight chính của chiến lược này là tách việc lấy mẫu của các toán tử tham số để tránh đồng thích ứng của trọng số, và cho phép lấy mẫu tùy ý của các kết nối không tham số để đạt được độ bao phủ tốt của không gian tìm kiếm NASRec.

So với lấy mẫu Toán tử bất kỳ Kết nối bất kỳ, lấy mẫu toán tử đơn Kết nối bất kỳ đạt được hiệu quả huấn luyện cao hơn: số lượng giảm của các toán tử được lấy mẫu giảm chi phí huấn luyện lên đến 1.5×. Ngoài ra, lấy mẫu toán tử đơn Kết nối bất kỳ lấy mẫu các mạng cỡ trung bình thường xuyên hơn. Những mạng cỡ trung bình này đạt được sự cân bằng tốt nhất giữa kích thước mô hình và hiệu suất như chúng tôi sẽ chỉ ra trong Bảng 5.

Chúng tôi đánh giá xếp hạng của subnet bởi WS-NAS trên Criteo và bởi 100 mạng được lấy mẫu ngẫu nhiên trong Hình 3. Ở đây, chúng tôi áp dụng thiết kế của các mô-đun tương tác cân bằng toán tử trong Phần 4.2 để tối đa hóa tiềm năng của mỗi chiến lược lấy mẫu đường dẫn. Trong hình, trục y là Log Loss của subnet, có trọng số được sao chép từ các kiến trúc tương ứng trong supernet được huấn luyện.

Lấy mẫu toán tử đơn Kết nối bất kỳ đạt được ít nhất 0.09 Pearson's Rho cao hơn và 0.15 Kendall's Tau cao hơn so với các chiến lược lấy mẫu đường dẫn khác. Ngoài ra, chúng tôi quan sát rằng lấy mẫu toán tử đơn Kết nối bất kỳ cho phép hội tụ tốt hơn của supernet NASRec và các subnet kế thừa trọng số từ supernet đạt được log loss thấp hơn trong quá trình validation, dẫn đến một khai thác tốt hơn hiệu suất thực tế của chúng cho một chất lượng xếp hạng tốt hơn.

### 4.2 Mô-đun Tương tác Cân bằng Toán tử

Hệ thống gợi ý liên quan đến dữ liệu đa phương thức với một số lượng không xác định các đầu vào, ví dụ, một số lượng lớn đầu vào thưa. Chúng tôi định nghĩa mất cân bằng toán tử là sự mất cân bằng của số lượng trọng số giữa các toán tử trong một khối. Trong NAS chia sẻ trọng số, mất cân bằng toán tử có thể gây ra vấn đề rằng huấn luyện supernet có thể thiên về các toán tử có nhiều trọng số hơn. Điều này sẽ bù trừ các lợi ích do tương quan xếp hạng kém của subnet: hiệu suất subnet trong supernet có thể lệch khỏi hiệu suất thực tế của nó khi được huấn luyện từ đầu. Chúng tôi xác định rằng, trong NASRec của chúng tôi, vấn đề như vậy có liên quan mạnh đến toán tử Dot-Product, và cung cấp giảm nhẹ để giải quyết mất cân bằng toán tử như vậy.

Cho N_s nhúng thưa, một khối Dot-Product tạo ra N_s^2/2 tương tác từng cặp như một hàm bậc hai trên số lượng nhúng thưa. Như chi tiết trong Phần 3.1, supernet yêu cầu một lớp chiếu tuyến tính (tức là FC) để khớp các chiều đầu ra của toán tử trong mỗi khối lựa chọn. Thông thường đối với Dot-Product, điều này dẫn đến một (N_s^2 · dim_d/2) trọng số có thể huấn luyện thêm.

Tuy nhiên, việc tiêu thụ trọng số của lớp chiếu như vậy lớn cho một số lượng lớn nhúng thưa. Ví dụ, cho N_s = 448 và dim_d = 512 trong một supernet NASRec 7-khối, lớp chiếu gây ra hơn 50M tham số trong supernet NASRec, có quy mô tiêu thụ tham số tương tự với các lớp nhúng thưa. Tham số hóa khổng lồ như vậy là một hàm bậc hai của số lượng đầu vào thưa N_s, tuy nhiên các toán tử xây dựng khác có ít trọng số hơn nhiều, như số lượng trọng số có thể huấn luyện trong EFC là một hàm tuyến tính của số lượng đầu vào thưa N_s. Kết quả là, việc quá tham số hóa trong Dot-Product dẫn đến tốc độ hội tụ tăng cho toán tử Dot-Product và do đó thiên về các subnet tiêu thụ tham số với nồng độ cao các hoạt động Dot-Product như chúng tôi quan sát. Ngoài ra, việc bỏ qua các toán tử không đồng nhất khác ngoài Dot-Product cung cấp xếp hạng kém của subnet, dẫn đến hiệu suất dưới tối ưu trên hệ thống gợi ý.

Chúng tôi chèn một EFC đơn giản như một lớp chiếu trước Dot-Product để giảm nhẹ việc quá tham số hóa như vậy, xem Hình 4. Trực giác của chúng tôi là chiếu số lượng nhúng thưa trong Dot-Product thành [√(2dim_d)], sao cho toán tử Dot-Product tiếp theo tạo ra khoảng dim_d đầu ra mà sau này yêu cầu một lớp chiếu tối thiểu để khớp chiều. Như vậy, toán tử Dot-Product tiêu thụ nhiều nhất (dim_d^2 + N_s[√(2dim_d)]) trọng số có thể huấn luyện và đảm bảo tăng trưởng tuyến tính của việc tiêu thụ tham số với số lượng EFC thưa N_s. Do đó, chúng tôi cân bằng toán tử tương tác để cho phép tốc độ hội tụ tương tự hơn của tất cả toán tử xây dựng. Bảng 2 phản ánh một cải thiện đáng kể về hiệu quả huấn luyện và chất lượng xếp hạng của supernet NASRec-Full với chiến lược lấy mẫu đường dẫn toán tử đơn Kết nối bất kỳ.

### 4.3 Tinh chỉnh Sau huấn luyện

Mặc dù huấn luyện subnet giống như dropout cung cấp một cách tuyệt vời để giảm sự thích ứng của trọng số cho một subnet cụ thể, dự đoán hiệu suất subnet bởi supernet có thể thất bại khi trọng số không nên chia sẻ qua một số subnet. Sau huấn luyện supernet và trong quá trình đánh giá subnet độc lập, chúng tôi thực hiện một tinh chỉnh sau huấn luyện tái thích ứng trọng số của nó trở lại subnet cụ thể. Điều này có thể tái hiệu chỉnh các trọng số bị hỏng khi huấn luyện các subnet khác trong quá trình huấn luyện supernet. Trong thực tế, chúng tôi thấy rằng tinh chỉnh FC cuối cùng trên tập dữ liệu mục tiêu trong một vài bước huấn luyện (ví dụ, 0.5K) là đủ tốt. Với chỉ chi phí tìm kiếm bổ sung nhỏ, kỹ thuật tinh chỉnh sau huấn luyện mới này tăng cường xếp hạng của subnet bằng cách giải quyết vấn đề thích ứng trọng số cơ bản, và do đó cung cấp cơ hội tốt hơn để khám phá các mô hình tốt hơn cho hệ thống gợi ý.

Bảng 3 chứng minh sự cải thiện của tinh chỉnh sau huấn luyện trên các chiến lược lấy mẫu đường dẫn khác nhau. Đáng ngạc nhiên, tinh chỉnh sau huấn luyện đạt được cải thiện chất lượng xếp hạng đáng kể dưới chiến lược lấy mẫu toán tử đơn Kết nối đơn và Toán tử bất kỳ Kết nối bất kỳ. Điều này là do các subnet dưới những chiến lược này thường không hội tụ tốt trong supernet: chúng hoặc gặp phải độ bao phủ supernet kém, hoặc hội tụ kém được gây ra bởi đồng thích ứng. Quá trình tinh chỉnh giải phóng tiềm năng của chúng và tiếp cận hiệu suất thực của chúng trên tập dữ liệu mục tiêu. Đáng chú ý, chiến lược lấy mẫu đường dẫn toán tử đơn Kết nối bất kỳ hợp tác tốt với tinh chỉnh sau huấn luyện, và đạt được tương quan xếp hạng Pearson's ρ và Kendall's τ tối ưu toàn cục giữa các phương pháp khác nhau, với ít nhất 0.14 Pearson's ρ và cải thiện Kendall's τ trên không gian tìm kiếm NASRec-Full so với lấy mẫu toán tử đơn Kết nối đơn với tinh chỉnh.

### 4.4 Tìm kiếm Tiến hóa trên Mô hình Tốt nhất

Chúng tôi sử dụng tiến hóa có quy tắc [24] để thu được subnet con tốt nhất trong không gian tìm kiếm NASRec, bao gồm NASRec Small và NASRec-Full. Ở đây, chúng tôi đầu tiên giới thiệu một đột biến đơn của một kiểu gen phân cấp với chuỗi hành động sau trong một trong các khối lựa chọn:

• Lấy mẫu lại chiều của một toán tử xây dựng dày đặc.
• Lấy mẫu lại chiều của một toán tử xây dựng thưa.
• Lấy mẫu lại một toán tử xây dựng dày đặc.
• Lấy mẫu lại một toán tử xây dựng thưa.
• Lấy mẫu lại kết nối của nó với các khối lựa chọn khác.
• Lấy mẫu lại sự lựa chọn của bộ hợp nhất dày đặc-thành-thưa/thưa-thành-dày đặc cho phép giao tiếp giữa các đầu ra dày đặc/thưa.

## 5 THỰC NGHIỆM

Chúng tôi đầu tiên chỉ ra cấu hình chi tiết mà NASRec sử dụng trong quá trình tìm kiếm kiến trúc, lựa chọn mô hình và đánh giá cuối cùng. Sau đó, chúng tôi chứng minh các đánh giá thực nghiệm trên ba bộ chuẩn hệ thống gợi ý phổ biến cho dự đoán Tỷ lệ Nhấp chuột (CTR): Criteo¹, Avazu² và KDD Cup 2012³. Tất cả ba tập dữ liệu được tiền xử lý theo cùng một cách như AutoCTR [30].

### 5.1 Cấu hình Tìm kiếm

Chúng tôi đầu tiên chứng minh cấu hình chi tiết của không gian tìm kiếm NASRec-Full như sau:

• **Thành phần Tìm kiếm Kết nối.** Chúng tôi sử dụng N = 7 khối trong không gian tìm kiếm NASRec của chúng tôi. Điều này cho phép so sánh công bằng với các phương pháp NAS gần đây [30]. Tất cả các khối lựa chọn có thể kết nối tùy ý với các khối lựa chọn trước hoặc đặc trưng thô.

• **Thành phần Tìm kiếm Toán tử.** Trong mỗi khối lựa chọn, không gian tìm kiếm của chúng tôi chứa 6 toán tử xây dựng riêng biệt, bao gồm 4 toán tử xây dựng dày đặc: FC, Gating, Sum, Dot-Product và 2 toán tử xây dựng thưa riêng biệt: EFC và Attention. Tùy chọn bộ hợp nhất dày đặc-thưa được khám phá đầy đủ.

• **Thành phần Tìm kiếm Chiều.** Đối với mỗi toán tử xây dựng dày đặc, chiều đầu ra dày đặc có thể chọn từ {16, 32, 64, 128, 256, 512, 768, 1024}. Đối với mỗi toán tử xây dựng thưa, chiều đầu ra thưa có thể được chọn từ {16, 32, 48, 64}.

Trong NASRec-Small, chúng tôi sử dụng cùng cài đặt ngoại trừ chúng tôi chỉ sử dụng 2 toán tử xây dựng dày đặc: FC, Dot-Product và 1 toán tử xây dựng thưa: EFC. Sau đó, chúng tôi minh họa một số kỹ thuật về ủ supernet NASRec, bao gồm cấu hình nhúng, khởi động supernet, và cài đặt huấn luyện supernet.

• **Bảng Nhúng Có giới hạn.** Chúng tôi giới hạn kích thước bảng nhúng tối đa thành 0.5M trong quá trình huấn luyện supernet để có hiệu quả tìm kiếm. Trong quá trình đánh giá cuối cùng, chúng tôi duy trì bảng nhúng đầy đủ để truy xuất hiệu suất tốt nhất, tức là tổng cộng 540M tham số trong DLRM [23] trên Criteo để đảm bảo so sánh công bằng.

• **Khởi động Supernet.** Chúng tôi quan sát rằng supernet có thể sụp đổ tại các giai đoạn huấn luyện ban đầu do các đường dẫn được lấy mẫu khác nhau và các lớp nhúng chưa được khởi tạo. Để giảm nhẹ sự sụp đổ ban đầu của supernet, chúng tôi ngẫu nhiên lấy mẫu supernet đầy đủ tại 1/5 đầu của các bước huấn luyện, với xác suất p giảm tuyến tính từ 1 về 0. Điều này cung cấp khởi động chiều, khởi động toán tử [3] và khởi động kết nối cho supernet với tác động tối thiểu đến chất lượng của các đường dẫn được lấy mẫu.

• **Cài đặt Huấn luyện Supernet.** Chúng tôi chèn layer normalization [1] vào mỗi toán tử xây dựng để ổn định huấn luyện supernet. Lựa chọn siêu tham số của chúng tôi mạnh mẽ trên các không gian tìm kiếm NASRec khác nhau và các bộ chuẩn hệ thống gợi ý. Chúng tôi huấn luyện supernet chỉ trong 1 epoch với bộ tối ưu Adagrad, tốc độ học ban đầu 0.12, một lịch trình tốc độ học cosine [22] trên các bộ chuẩn hệ thống gợi ý mục tiêu.

Cuối cùng, chúng tôi trình bày chi tiết của tiến hóa có quy tắc và các chiến lược lựa chọn mô hình trên các không gian tìm kiếm NASRec.

• **Tiến hóa Có quy tắc.** Mặc dù kích thước lớn của NASRec-Full và NASRec-small, chúng tôi sử dụng một cấu hình hiệu quả của tiến hóa có quy tắc để tìm kiếm các subnet tối ưu từ supernet. Cụ thể, chúng tôi duy trì một quần thể 128 kiến trúc và chạy tiến hóa có quy tắc trong 240 vòng lặp. Trong mỗi vòng lặp, chúng tôi đầu tiên chọn kiến trúc tốt nhất từ 64 kiến trúc được lấy mẫu từ quần thể làm kiến trúc cha, và tạo ra 8 kiến trúc con để cập nhật quần thể.

• **Lựa chọn Mô hình.** Chúng tôi theo các giao thức đánh giá trong AutoCTR [30] và chia mỗi tập dữ liệu mục tiêu thành 3 tập: huấn luyện (80%), validation (10%) và kiểm tra (10%). Trong quá trình tìm kiếm kiến trúc mạng neural chia sẻ trọng số, chúng tôi huấn luyện supernet trên tập huấn luyện và chọn top-15 subnet trên tập validation. Chúng tôi huấn luyện 15 mô hình hàng đầu từ đầu, và chọn subnet tốt nhất làm kiến trúc cuối cùng, cụ thể là NASRecNet.

### 5.2 Kết quả Bộ chuẩn Hệ thống Gợi ý

Chúng tôi huấn luyện NASRecNet từ đầu trên ba bộ chuẩn hệ thống gợi ý cổ điển, và so sánh hiệu suất của các mô hình được chế tác bởi NASRec trên ba bộ chuẩn hệ thống gợi ý tổng quát. Trong Bảng 4, chúng tôi báo cáo kết quả đánh giá của NASRecNet end-to-end và một baseline tìm kiếm ngẫu nhiên lấy mẫu ngẫu nhiên và huấn luyện các mô hình trong không gian tìm kiếm NASRec của chúng tôi.

**Hiệu suất Tiên tiến nhất.** Ngay cả trong một không gian tìm kiếm NASRec-Full cực kỳ lớn, NASRecNet đạt được hiệu suất phá kỷ lục so với các mô hình CTR chế tác thủ công [14,19,23] với tiên nghiệm con người tối thiểu như thể hiện trong Bảng 4. So với AutoInt [31], mô hình chế tác thủ công chế tác tương tác đặc trưng với nỗ lực kỹ thuật tinh xảo, NASRecNet đạt được giảm Log Loss ~0.003 trên Criteo, giảm Log Loss ~0.007 trên Avazu, và giảm Log Loss ~0.003 trên KDD Cup 2012, với chuyên môn và can thiệp con người tối thiểu.

Tiếp theo, chúng tôi so sánh NASRecNet với các mô hình chế tác NAS gần đây hơn. So với AutoCTR [30], NASRecNet đạt được Log Loss và AUC tiên tiến nhất (SOTA) trên tất cả ba bộ chuẩn hệ thống gợi ý. Với cùng quy mô không gian tìm kiếm như AutoCTR (tức là không gian tìm kiếm NASRec-Small), NASRecNet mang lại giảm Log Loss 0.001 trên Criteo, giảm Log Loss 0.005 trên Avazu, và giảm Log Loss 0.003 trên KDD Cup 2012. So với DNAS [18] và PROFIT [11] chỉ tập trung vào cấu hình một phần của kiến trúc, như kết nối dày đặc, NASRecNet đạt được ít nhất giảm Log Loss ~0.002 trên Criteo, chứng minh tầm quan trọng của tìm kiếm kiến trúc đầy đủ trên hệ thống gợi ý.

Bằng cách mở rộng NASRec thành một không gian tìm kiếm NASRec-Full cực kỳ lớn, NASRecNet tiếp tục cải thiện kết quả của nó trên Avazu và vượt trội PROFIT với cải thiện AUC ~0.002 với Log Loss ngang bằng, chứng minh thiết kế của NASRec-Full với cardinality cực kỳ lớn và tiên nghiệm con người tối thiểu. Trên Criteo và KDD Cup 2012, NASRec duy trì lợi thế trong việc khám phá các mô hình CTR tiên tiến nhất so với các phương pháp NAS hiện có [11, 18, 30].

**Tìm kiếm Hiệu quả trong Không gian Tìm kiếm Đa dạng.** Mặc dù một không gian tìm kiếm NASRec lớn hơn đặt ra nhiều thách thức hơn để khám phá đầy đủ, NASRec đạt được ít nhất hiệu quả tìm kiếm 1.7× so với các phương pháp NAS hiệu quả tiên tiến nhất [11,30] với cải thiện Log Loss đáng kể trên tất cả ba bộ chuẩn. Điều này phần lớn được quy cho hiệu quả của NAS Chia sẻ Trọng số được áp dụng trên các toán tử không đồng nhất và dữ liệu đa phương thức.

Chúng tôi quan sát rằng một không gian tìm kiếm NASRec-Small nhỏ gọn tạo ra các baseline tìm kiếm ngẫu nhiên mạnh, trong khi một không gian tìm kiếm NASRec-Full lớn hơn có baseline yếu hơn. Điều này là do với ngân sách tìm kiếm hạn chế, việc khám phá các mô hình hứa hẹn trong một không gian tìm kiếm lớn thách thức hơn. Tuy nhiên, WS-NAS có thể mở rộng giải quyết việc khám phá không gian tìm kiếm NASRec-Full đầy đủ nhờ vào độ bao phủ rộng của supernet. Với một chiến lược lấy mẫu đường dẫn Toán tử Đơn Kết nối Bất kỳ hiệu quả, WS-NAS cải thiện chất lượng của các mô hình được khám phá trên Criteo, và khám phá một mô hình tốt hơn trên Avazu và KDD Cup 2012 so với không gian tìm kiếm NASRec-Small.

### 5.3 Thảo luận

Trong phần này, chúng tôi phân tích độ phức tạp của NASRecNet, và chứng minh tác động của các kỹ thuật được đề xuất của chúng tôi giảm nhẹ các rối loạn xếp hạng và cải thiện chất lượng của các mô hình được tìm kiếm.

**Phân tích Độ phức tạp Mô hình.** Chúng tôi so sánh độ phức tạp mô hình của NASRecNet với các mô hình SOTA chế tác thủ công và NAS. Chúng tôi thu thập tất cả baseline từ AutoCTR [30], và so sánh hiệu suất so với số lượng Phép toán Điểm nổi (FLOPs) trong Bảng 5. Chúng tôi profile tất cả FLOPS của NASRecNet sử dụng FvCore [26]. Ngay cả không có bất kỳ ràng buộc FLOPs nào, NASRecNet vượt trội các nghệ thuật hiện có về hiệu quả. Mặc dù đạt được Log Loss thấp hơn, NASRecNet đạt được giảm FLOPS 8.5×, 3.8×, và 2.8× trên các bộ chuẩn Criteo, Avazu, và KDD Cup 2012. Một lý do có thể nằm trong việc sử dụng các mô-đun tương tác cân bằng toán tử: nó chiếu các đầu vào thưa về một chiều nhỏ hơn trước khi thực hiện tương tác đặc trưng cross-term. Điều này dẫn đến chi phí tính toán thấp hơn đáng kể, đóng góp vào các mô hình gợi ý nhỏ gọn nhưng hiệu suất cao.

**Hiệu ứng của Lấy mẫu Đường dẫn & Tinh chỉnh.** Chúng tôi thảo luận các kỹ thuật lấy mẫu đường dẫn và tinh chỉnh trong Phần 4.2, và chứng minh đánh giá thực nghiệm của những kỹ thuật này về chất lượng của các mô hình được tìm kiếm trong Bảng 6. Kết quả cho thấy rằng, (1) tầm quan trọng của lấy mẫu đường dẫn vượt xa tầm quan trọng của tinh chỉnh trong việc quyết định chất lượng của các mô hình được tìm kiếm, và (2) một Kendall's τ cao hơn xếp hạng chính xác các subnet trong không gian tìm kiếm NASRec (tức là Bảng 6) cho thấy một cải thiện nhất quán trên các mô hình được tìm kiếm.

## 6 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất NASRec, một mô hình mới để hoàn toàn kích hoạt NAS cho Hệ thống Gợi ý thông qua Tìm kiếm Kiến trúc Mạng Neural Chia sẻ Trọng số (WS-NAS) dưới tính đa phương thức dữ liệu và tính không đồng nhất kiến trúc. NASRec thiết lập một supernet lớn để đại diện cho không gian kiến trúc đầy đủ, và kết hợp các toán tử xây dựng đa dạng và kết nối khối dày đặc để giảm thiểu tiên nghiệm con người trong thiết kế kiến trúc tự động cho hệ thống gợi ý. NASRec xác định những thách thức về quy mô và tính không đồng nhất của không gian tìm kiếm NASRec quy mô lớn làm tổn hại supernet và đề xuất một loạt kỹ thuật để cải thiện hiệu quả huấn luyện và giảm nhẹ rối loạn xếp hạng. Các mô hình được chế tác của chúng tôi, NASRecNet, đạt được hiệu suất tiên tiến nhất trên 3 bộ chuẩn hệ thống gợi ý phổ biến, chứng minh triển vọng hứa hẹn trên không gian tìm kiếm kiến trúc đầy đủ, và định hướng nghiên cứu động lực hướng tới chế tác kiến trúc hoàn toàn tự động với tiên nghiệm con người tối thiểu.

**Lời cảm ơn.** Công việc của Yiran Chen được hỗ trợ một phần bởi các grant sau: NSF-2120333, NSF-2112562, NSF-1937435, NSF-2140247 và ARO W911NF-19-2-0107. Công việc của Feng được hỗ trợ một phần bởi các grant sau: NSF CAREER-2048044 và IIS-1838024. Chúng tôi cũng cảm ơn Maxim Naumov, Jeff Hwang và Colin Taylor tại Meta Platforms, Inc. vì sự giúp đỡ tốt bụng của họ trong dự án này.

## TÀI LIỆU THAM KHẢO

[1] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016).

[2] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, và Quoc Le. 2018. Understanding and simplifying one-shot architecture search. In International Conference on Machine Learning. PMLR, 550–559.

[3] Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, và Quoc V Le. 2020. Can weight sharing outperform random architecture search? an investigation with tunas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14323–14332.

[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. 2019. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791 (2019).

[5] Ben Carterette và Rosie Jones. 2007. Evaluating search engines by modeling the relationship between relevance and clicks. Advances in neural information processing systems 20 (2007).

[6] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, và Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data. 1–4.

[7] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems. 7–10.

[8] Paul Covington, Jay Adams, và Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems. 191–198.

[9] Wei Deng, Junwei Pan, Tian Zhou, Deguang Kong, Aaron Flores, và Guang Lin. 2021. DeepLight: Deep lightweight feature interactions for accelerating CTR predictions in ad serving. In Proceedings of the 14th ACM international conference on Web search and data mining. 922–930.

[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).

[11] Chen Gao, Yinfeng Li, Quanming Yao, Depeng Jin, và Yong Li. 2021. Progressive Feature Interaction Search for Deep Sparse Network. Advances in Neural Information Processing Systems 34 (2021).

[12] Luyu Gao, Zhuyun Dai, và Jamie Callan. 2020. Modularized transfomer-based ranking framework. arXiv preprint arXiv:2004.13313 (2020).

[13] Guibing Guo, Jie Zhang, và Neil Yorke-Smith. 2015. Trustsvd: Collaborative filtering with both the explicit and implicit influence of user trust and of item ratings. In Proceedings of the AAAI conference on artificial intelligence, Vol. 29.

[14] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, và Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017).

[15] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, và Jian Sun. 2020. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision. Springer, 544–560.

[16] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising. 1–9.

[17] Dominik Kowald, Subhash Chandra Pujari, và Elisabeth Lex. 2017. Temporal effects on hashtag reuse in twitter: A cognitive-inspired hashtag recommendation approach. In Proceedings of the 26th International Conference on World Wide Web. 1401–1410.

[18] Ravi Krishna, Aravind Kalaiah, Bichen Wu, Maxim Naumov, Dheevatsa Mudigere, Misha Smelyanskiy, và Kurt Keutzer. 2021. Differentiable NAS Framework and Application to Ads CTR Prediction. arXiv preprint arXiv:2110.14812 (2021).

[19] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, và Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1754–1763.

[20] Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang, và Zhenguo Li. 2019. Darts+: Improved differentiable architecture search with early stopping. arXiv preprint arXiv:1909.06035 (2019).

[21] Hanxiao Liu, Karen Simonyan, và Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018).

[22] Ilya Loshchilov và Frank Hutter. 2016. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016).

[23] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019).

[24] Esteban Real, Alok Aggarwal, Yanping Huang, và Quoc V Le. 2019. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, Vol. 33. 4780–4789.

[25] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, và Lars Schmidt-Thieme. 2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 635–644.

[26] Facebook Research. 2022. fvcore. https://github.com/facebookresearch/fvcore,.

[27] Matthew Richardson, Ewa Dominowska, và Robert Ragno. 2007. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web. 521–530.

[28] Ying Shan, T Ryan Hoens, Jian Jiao, Haijing Wang, Dong Yu, và JC Mao. 2016. Deep crossing: Web-scale modeling without manually crafted combinatorial features. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 255–262.

[29] David So, Quoc Le, và Chen Liang. 2019. The evolved transformer. In International Conference on Machine Learning. PMLR, 5877–5886.

[30] Qingquan Song, Dehua Cheng, Hanning Zhou, Jiyan Yang, Yuandong Tian, và Xia Hu. 2020. Towards automated neural interaction discovery for click-through rate prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 945–955.

[31] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, và Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 1161–1170.

[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).

[33] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, và Song Han. 2020. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187 (2020).

[34] Ruoxi Wang, Bin Fu, Gang Fu, và Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17. 1–7.

[35] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, và Ed Chi. 2021. DCN V2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021. 1785–1797.

[36] Zhiqiang Wang, Qingyun She, và Junlin Zhang. 2021. MaskNet: introducing feature-wise multiplication to CTR ranking models by instance-guided mask. arXiv preprint arXiv:2102.07619 (2021).

[37] Wei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Bender, và Pieter-Jan Kindermans. 2020. Neural predictor for neural architecture search. In European Conference on Computer Vision. Springer, 660–676.

[38] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, và Quoc Le. 2020. Bignas: Scaling up neural architecture search with big single-stage models. In European Conference on Computer Vision. Springer, 702–717.

[39] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, và Quoc V Le. 2018. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 8697–8710.

## 7 TÀI LIỆU BỔ SUNG

Trong phần này, chúng tôi cung cấp thêm chi tiết về NASRec, bao gồm: (1) trực quan hóa và insight của các kiến trúc được tìm kiếm, (2) một đánh giá của NASRecNet tốt nhất trên Criteo Terabyte⁴ để chứng minh hiệu suất của nó trên các bộ chuẩn dự đoán CTR quy mô lớn, và (3) chi tiết về lấy mẫu và xếp hạng subnet.

### 7.1 Trực quan hóa Mô hình

Chúng tôi trực quan hóa các mô hình được tìm kiếm trong không gian tìm kiếm NASRec-Small/NASRec-Full trên 3 bộ chuẩn CTR khác nhau: Criteo, Avazu, và KDD. Trước khi trình bày các kiến trúc được tìm kiếm, chúng tôi chỉ ra các đặc điểm của mỗi bộ chuẩn CTR trong Bảng 7.

Ở đây, chúng tôi quan sát rằng Criteo có số lượng đặc trưng dày đặc (thưa) nhiều nhất, do đó là bộ chuẩn phức tạp và thách thức nhất. Avazu chỉ chứa các đặc trưng dày đặc, do đó yêu cầu ít tương tác giữa các đầu ra dày đặc trong mỗi khối lựa chọn hơn. KDD có ít đặc trưng nhất và nhiều dữ liệu nhất, làm cho nó trở thành một bộ chuẩn tương đối dễ để huấn luyện và đánh giá.

**Avazu.** Hình 5 và Hình 6 mô tả các cấu trúc chi tiết của kiến trúc tốt nhất trong không gian tìm kiếm NASRec-Small/NASRec-Full. Ở đây, một khối xanh (đỏ) có sọc biểu thị một khối dày đặc (thưa) không được sử dụng trong kiến trúc cuối cùng, và một kết nối đậm biểu thị cùng đầu vào nguồn cho một toán tử dày đặc với hai đầu vào (tức là Sigmoid Gating và Sum).

Vì bộ chuẩn Avazu chỉ chứa các đặc trưng thưa, tương tác và trích xuất biểu diễn dày đặc ít quan trọng hơn. Ví dụ, mô hình tốt nhất trong không gian tìm kiếm NASRec-Full chỉ chứa 1 toán tử (tức là Sigmoid Gating) chỉ xử lý các biểu diễn dày đặc, tuy nhiên với nhiều khối Dot-Product (DP) và Attention (Attn) hơn tương tác các biểu diễn thưa. Trong không gian tìm kiếm NASRec-Small, các biểu diễn dày đặc được xử lý thường xuyên hơn bởi các lớp FC sau khi tương tác với các biểu diễn thưa trong khối Dot-Product. Tuy nhiên, xử lý các đặc trưng dày đặc yêu cầu nhiều khối Fully-Connected hơn một chút so với cơ chế self-attention được áp dụng trong không gian tìm kiếm NASRec-Full.

**Criteo.** Hình 7 và Hình 8 mô tả các cấu trúc chi tiết của kiến trúc tốt nhất trong không gian tìm kiếm NASRec-Small/NASRec-Full. Ở đây, một khối xanh (đỏ) có sọc biểu thị một khối dày đặc (thưa) không được sử dụng trong kiến trúc cuối cùng, và một kết nối đậm biểu thị cùng đầu vào nguồn cho một toán tử dày đặc với hai đầu vào (tức là Sigmoid Gating và Sum).

Criteo chứa tập hợp đặc trưng dày đặc (thưa) phong phú nhất, do đó phức tạp nhất trong chế tác kiến trúc. chúng tôi quan sát rằng kết nối dày đặc được đánh giá cao trong cả không gian tìm kiếm NASRec-Small và NASRec-Full, cho thấy rằng hợp nhất đặc trưng có tác động đáng kể đến log loss trên một bộ chuẩn phức tạp. Ngoài ra, self-gating trên các đặc trưng dày đặc thô (tức là khối 1 @ NASRec-Full) được coi là một motif quan trọng trong tương tác đặc trưng. Các mẫu tương tự cũng có thể được quan sát trong kiến trúc tốt nhất được tìm kiếm trên bộ chuẩn KDD.

Do độ phức tạp của Criteo và các khối tìm kiếm NASRec-Full, chúng tôi chú ý rằng kiến trúc tốt nhất được tìm kiếm không sử dụng tất cả 7 khối trong không gian tìm kiếm. Một số khối không được sử dụng trong kiến trúc cuối cùng. Ví dụ, kiến trúc tốt nhất được tìm kiếm trong NASRec-Full chỉ chứa 4 khối hợp lệ. Chúng tôi để lại điều này như một công việc tương lai để cải thiện huấn luyện supernet sao cho các kiến trúc sâu hơn có thể được khám phá theo cách có thể mở rộng hơn.

**KDD.** Hình 9 và Hình 10 mô tả các cấu trúc chi tiết của kiến trúc tốt nhất trong không gian tìm kiếm NASRec-Small/NASRec-Full. Ở đây, một khối xanh (đỏ) có sọc biểu thị một khối dày đặc (thưa) không được sử dụng trong kiến trúc cuối cùng, và một kết nối đậm biểu thị cùng đầu vào nguồn cho một toán tử dày đặc với hai đầu vào (tức là Sigmoid Gating và Sum). Tương tự như những gì chúng tôi thấy trên Criteo, kiến trúc được tìm kiếm trong NASRec-Full có nhiều toán tử xây dựng hơn, tuy nhiên ít kết nối dày đặc hơn.

Vì KDD là một bộ chuẩn đơn giản hơn với ít đặc trưng dày đặc (thưa) hơn, kiến trúc được tìm kiếm đơn giản hơn, đặc biệt là trong không gian tìm kiếm NASRec. Self-gating tương tự trên các đầu vào dày đặc vẫn phục vụ như một motif quan trọng trong việc thiết kế một kiến trúc tốt hơn.

Cuối cùng, chúng tôi tóm tắt các quan sát của chúng tôi trên ba bộ chuẩn độc đáo như sau:

• **Độ phức tạp Bộ chuẩn Quyết định Độ phức tạp Kiến trúc.** Sự lựa chọn của một bộ chuẩn quyết định độ phức tạp của kiến trúc cuối cùng. Một bộ chuẩn càng phức tạp, một mô hình được tìm kiếm càng phức tạp trong kết nối dày đặc và tính không đồng nhất toán tử.

• **Không gian Tìm kiếm Quyết định Kết nối.** Trên tất cả ba bộ chuẩn CTR, kiến trúc tốt nhất được tìm kiếm trong NASRec-Full chứa nhiều tính không đồng nhất toán tử hơn và ít kết nối dày đặc hơn. Tuy nhiên, kết nối dày đặc giảm giữa các khối lựa chọn khác nhau giúp giảm tiêu thụ FLOPs của các mô hình được tìm kiếm, dẫn đến ít độ phức tạp mô hình hơn và hiệu quả mô hình tốt hơn. Điều này cũng cho thấy rằng tìm kiếm các toán tử xây dựng có thể vượt trội tầm quan trọng của tìm kiếm kết nối dày đặc khi chế tác một mô hình CTR hiệu quả.

• **Attention Có Tác động Lớn.** Các khối Attention hiếm khi được nghiên cứu trong tài liệu hiện có về hệ thống gợi ý. Các kiến trúc được tìm kiếm trên không gian tìm kiếm NASRec-Full chứng minh hiệu quả của cơ chế attention trong việc tổng hợp các đặc trưng dày đặc (thưa). Ví dụ, khối đầu tiên trong kiến trúc tốt nhất được tìm kiếm luôn áp dụng một lớp attention để tương tác các đầu vào thưa thô. Việc xếp chồng các khối attention cũng được quan sát trong các kiến trúc được tìm kiếm để chứng minh tương tác bậc cao giữa các đặc trưng dày đặc (thưa).

• **Self-Gating Là một Motif Hữu ích.** Self-gating biểu thị một toán tử gating từng cặp với các đầu vào dày đặc giống hệt nhau. Trên cả bộ chuẩn Criteo/KDD, self-gating được khám phá để xử lý các đầu vào dày đặc thô và cung cấp các phép chiếu dày đặc với chất lượng cao hơn. Trên Avazu không có đặc trưng đầu vào dày đặc, self-gating được khám phá để kết hợp một biểu diễn dày đặc cấp cao hơn cho kết quả dự đoán tốt hơn.

### 7.2 Đánh giá trên Criteo Terabyte

Criteo Terabyte là một bộ chuẩn quy mô lớn về dự đoán CTR, chứa 1TB click logs trong 24 ngày. So với phiên bản kaggle của Criteo Kaggle⁵ chỉ chứa 45.84M dữ liệu, Criteo Terabyte chứa ~4B dữ liệu trong huấn luyện và validation, do đó có quy mô lớn hơn đáng kể.

Trên Criteo Terabyte, chúng tôi sử dụng 23 ngày đầu của dữ liệu làm dữ liệu huấn luyện/validation, và sử dụng ngày cuối cùng của dữ liệu làm dữ liệu kiểm tra. Chúng tôi đánh giá các mô hình DLRM, AutoCTR (tức là tiên tiến nhất trước đó) và NASRecNet được tìm kiếm trên không gian tìm kiếm NASRec-Full. Chúng tôi vẽ biểu đồ validation log loss trên tập dữ liệu huấn luyện và testing log loss trên tập dữ liệu kiểm tra trên Hình 11. So với DLRM, AutoCTR cho thấy hiệu suất ngang bằng trên tập dữ liệu kiểm tra, tuy nhiên NASRecNet đạt được giảm log loss 0.03% so với baseline DLRM, cho thấy kết quả thực nghiệm tốt hơn. Tuy nhiên, vì cả AutoCTR và NASRecNet đều được chế tác trên tập dữ liệu Criteo Kaggle, chúng có thể không phù hợp tốt với các thuộc tính của một bộ chuẩn quy mô lớn, như sự thay đổi phân phối dữ liệu. Chúng tôi để lại việc tìm kiếm và khám phá các kiến trúc tốt hơn trên các bộ chuẩn quy mô lớn như công việc tương lai.

### 7.3 Chi tiết Lấy mẫu Subnet

Trong Phần 4, chúng tôi lấy mẫu 100 subnet trong không gian tìm kiếm NASRec-Full trên bộ chuẩn Criteo, với một cài đặt cân bằng và hiệu quả hơn trên các thành phần tìm kiếm chiều: chiều đầu ra dày đặc có thể chọn từ {32, 64, 128, 256, 512}, và chiều đầu ra thưa có thể chọn từ {16, 32, 64}. Tất cả subnet được huấn luyện trên bộ chuẩn Criteo với batch size 1024 và tốc độ học 0.12.

Chúng tôi vẽ biểu đồ phân phối CDF của các subnet được lấy mẫu trên tất cả ba bộ chuẩn trong Bảng 12. Đối với 50% kiến trúc hàng đầu được đánh giá trên supernet NASRec-Full, chúng tôi báo cáo Kendall's τ là 0.24 cho bộ chuẩn Criteo, cho thấy một cải thiện rõ ràng trong việc xếp hạng các kiến trúc hiệu suất cao so với tìm kiếm ngẫu nhiên (0.0). Trong công việc tương lai, chúng tôi đề xuất thiết lập một bộ chuẩn CTR cho NAS để tăng ý nghĩa thống kê của các hệ số xếp hạng được đánh giá và hỗ trợ tốt hơn nghiên cứu trong việc xếp hạng chính xác các kiến trúc khác nhau.
