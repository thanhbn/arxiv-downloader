# 2111.06495.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2111.06495.pdf
# File size: 2178459 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FAIRAUTOML: E MBRACING UNFAIRNESS MITIGATION IN AUTOML
Qingyun Wu1Chi Wang2
ABSTRACT
In this work, we propose an Automated Machine Learning (AutoML) system to search for models not only with
good prediction accuracy but also fair. We Ô¨Årst investigate the necessity and impact of unfairness mitigation in the
AutoML context. We establish the FairAutoML framework. The framework provides a novel design based on
pragmatic abstractions, which makes it convenient to incorporate existing fairness deÔ¨Ånitions, unfairness mitigation
techniques, and hyperparameter search methods into the model search and evaluation process. Following this
framework, we develop a fair AutoML system based on an existing AutoML system. The augmented system
includes a resource allocation strategy to dynamically decide when and on which models to conduct unfairness
mitigation according to the prediction accuracy, fairness, and resource consumption on the Ô¨Çy. Extensive empirical
evaluation shows that our system can achieve a good ‚Äòfair accuracy‚Äô and high resource efÔ¨Åciency.
1 I NTRODUCTION
Effective automated machine learning (AutoML) systems
have been developed to automatically tune hyperparameter
conÔ¨Ågurations and Ô¨Ånd ML models with a good predictive
performance from given training data. These systems are
increasingly used to manage ML pipelines in various practi-
cal scenarios (Aut, 2021). In many real-world applications
where the decisions to be made have a direct impact on the
well-being of human beings; however, it is not sufÔ¨Åcient
to only have good predictive performance. We also expect
ML-based decisions to be ethical that do not put certain
unprivileged groups or individuals at a systemic disadvan-
tage. Unfortunately, there has been increasing evidence of
various unfairness issues associated with machine-made de-
cisions or predictions in many of such applications (O‚Äôneil,
2016; Barocas & Selbst, 2016; Angwin et al., 2016). Con-
sidering the increasing adoption of AutoML systems, we
Ô¨Ånd it crucial to study how to improve fairness in AutoML
systems.
Considerable research effort has been devoted to machine
learning fairness in recent years, including research on fair-
ness deÔ¨Ånitions (Dwork et al., 2012; Hardt et al., 2016;
Chouldechova, 2017; Lahoti et al., 2019) and unfairness
mitigation (Kamiran & Calders, 2012; Kamishima et al.,
2011; Friedler et al., 2014; Hardt et al., 2016; Calmon et al.,
2017; Woodworth et al., 2017; Zafar et al., 2017; Agar-
wal et al., 2018; Zhang et al., 2021). Many of existing
1Pennsylvania State University (part of the work is done when
the author is at Microsoft Research).2Microsoft Research. Corre-
spondence to: Qingyun Wu <qingyun.wu@psu.edu>.unfairness mitigation methods are quite effective in mitigat-
ing unfairness-related harms of machine learning models.
However, they are rarely explored in AutoML systems. Con-
sidering the enormous practical impact of AutoML, we are
motivated to explore whether it is helpful to incorporate
unfairness mitigation techniques to AutoML systems to im-
prove the systems‚Äô overall fairness, and if so, what is a good
way to incorporate them .
To answer the Ô¨Årst question, we conduct both literature sur-
vey and case studies. Our study suggests that it is beneÔ¨Åcial
(and sometimes essential) to incorporate unfairness mitiga-
tion into AutoML. Toward answering the second question,
we propose a novel and pragmatic solution to enable the
incorporation of unfairness mitigation into AuotML. Specif-
ically, we Ô¨Årst investigate the necessity of unfairness miti-
gation in AutoML and its potential impact on the AutoML
system regarding fairness, predictive performance, and com-
putation cost. We then provide a fair AutoML formulation
taking unfairness mitigation into consideration. Accompa-
nying the formulation, we develop necessary abstractions
to generally characterize the fairness assessment and unfair-
ness mitigation procedures in the AutoML context. We also
propose a self-adaptive decision-making strategy for achiev-
ing fair AutoML effectively and efÔ¨Åciently. The strategy
helps determine when to conduct regular model training,
and whether and when to conduct an additional unfairness
mitigation procedure in each trial of the AutoML process.
We summarize key contributions of this work as follows:
1.We provide a rigorous formulation for the fair AutoML
problem taking unfairness mitigation into consideration,
and propose a general framework for solving the fair
AutoML problem. (ref. Section 3). The abstractions in
the framework allow convenient and Ô¨Çexible incorpora-arXiv:2111.06495v2  [cs.LG]  24 Nov 2022

--- PAGE 2 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
tion of most existing fairness assessment methods and
unfairness mitigation techniques into AutoML.
2.We propose an adaptive strategy for deciding when and
whether to perform unÔ¨Årenss mitigation are novel and of
practical importance. (ref. Section 4). With this strategy,
computation resources can be allocated efÔ¨Åciently be-
tween hyperparameter tuning and unfairness mitigation.
3.We develop a working fair AutoML system based on an
existing AutoML system. The proposed fair AutoML
system is Ô¨Çexible ,robust andefÔ¨Åcient in terms of building
fair models with good predictive performance. Exten-
sive empirical evaluation involving four machine learn-
ing fairness benchmark datasets, two different fairness
deÔ¨Ånitions with two different fairness thresholds, three
different unfairness mitigation methods, two tuning tasks
(tuning tasks on a single learner and multiple learners),
and two tuning methods, verify the robust good perfor-
mance of our system over all the evaluated scenarios.
(ref. Section 5).
2 R ELATED WORK
Fairness deÔ¨Ånitions and unfairness mitigation. There are
mainly two types of fairness deÔ¨Ånitions for machine learn-
ing: group fairness (Dwork et al., 2012; Simoiu et al., 2017;
Hardt et al., 2016) and individual fairness (Dwork et al.,
2012; Lahoti et al., 2019). Under the group fairness fram-
ing, fairness, generally speaking, requires some aspects
of the machine learning system‚Äôs behavior to be compara-
ble or even equalized across different groups. Commonly
used group fairness metrics include Demographic Parity
(DP) (Dwork et al., 2012) and Equalized Odds (EO) (Hardt
et al., 2016). Individual fairness ensures that individuals
who are ‚Äòsimilar‚Äô with respect to the learning task receive
similar outcomes. Unfairness mitigation methods seek to
reduce fairness-related harms of a machine learning model
regarding a particular fairness metric. From a procedural
viewpoint, unfairness mitigation methods can be roughly
categorized into pre-processing (Kamiran & Calders, 2012;
Calmon et al., 2017), in-processing (Kamishima et al.,
2011; Woodworth et al., 2017; Zafar et al., 2017; Agar-
wal et al., 2018; Zhang et al., 2021), and post-processing
approaches (Friedler et al., 2014; Hardt et al., 2016), de-
pending on whether the methods should be applied before,
during or after the model training process. Pre-processing
methods typically transform the training data to try to re-
move undesired biases. In-processing methods try to reduce
fairness-related harms by imposing fairness constraints into
the learning mechanism. Post-processing methods directly
transform the model outputs to reduce biases. Software
toolkits have been developed for machine learning fair-
ness. Representative ones include the open-source Python
libraries AIF360 (Bellamy et al., 2018) and FairLearn (Bird
et al., 2020). Both libraries provide implementations forstate-of-the-art fairness measurement metrics, unfairness
mitigation methods, and collections of datasets commonly
used for machine learning fairness research.
AutoML. Many AutoML toolkits and systems have been
developed (Feurer et al., 2015; Olson et al., 2016; H2O.ai,
2019; Li et al., 2020; Wang et al., 2021b). There is lit-
tle work in the AutoML Ô¨Åeld that takes fairness into con-
sideration, despite some preliminary attempt in the broad
regime of AutoML recently. (Schmucker et al., 2020) uti-
lize a multi-objective HPO method to Ô¨Ånd a Pareto frontier
of conÔ¨Ågurations which have good fairness and accuracy.
FairHO (Cruz et al., 2021) tries to make hyperparameter op-
timization fairness-aware by including the fairness score as
an additional objective. It can be used to either explore the
Pareto frontier regarding fairness and prediction accuracy or
Ô¨Ånd an optimal balance between accuracy and fairness based
on a user-speciÔ¨Åed weight. Both the aforementioned two
methods essentially add fairness as an additional objective
to their hyperparameter optimization process. FairBO (Per-
rone et al., 2021) takes fairness as a constraint and proposes
to reduce the unfairness of learning outcomes by varying the
hyperparameter conÔ¨Ågurations and solving a constrained
Bayesian optimization problem. In this work, we also take
fairness as a constraint in AutoML. Our solution is more
general than FairBO.
3 F AIRAUTOML FORMULATION
In this section, we Ô¨Årst introduce notions and notations to be
used throughout this paper, and basic concepts of AutoML.
We then investigate the necessity and impact of unfairness
mitigation in AutoML. At last, we propose a fair AutoML
formulation in which unfairness mitigation is incorporated
as an organic component of the AutoML process.
‚Ä¢XandYdenote feature vectors and target values in
the data feature space Xand label spaceYrespec-
tively.D= (X;Y)denotes a dataset in general, and
Dtrain= (Xtrain;Ytrain)andDval= (Xval;Yval)denote a
particular training and validation dataset respectively.
‚Ä¢cdenotes a hyperparameter conÔ¨Åguration in a particular
hyperparameter search space C, withc2C. When only a
single ML learner is involved in the AutoML process, c
denotes model hyperparameter conÔ¨Åguration of that con-
cerned learner, and when multiple learners are involved, c
also includes a dimension about the choice of the learner.
‚Ä¢fdenotes a machine learning model in general. fc;D trainis
the resulting model associated with hyperparameter con-
Ô¨Ågurationc, trained on dataset Dtrain, andfc;D train(Xval)is
the prediction outcome of fc;D trainon input data Xval. In
the later discussion, we also add a superscript on fc;D train
to denote how the model is trained. SpeciÔ¨Åcally, we use
f(0)
c;D trainto denote the variant of the model which is trained

--- PAGE 3 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
(based oncandDtrain) without considering fairness con-
straints, and use f(1)
c;D trainto denote the variant which is
trained with unfairness mitigation.
‚Ä¢Loss :YY! Ris a loss function.
3.1 AutoML
Given a training dataset Dtrain, a validation dataset Dval, and
a hyperparameter search space C, the goal of AutoML1is to
Ô¨Ånd the best model which has the lowest validation loss:
min
c2CLoss (fc;D train(Dval);Yval) (1)
Solutions to the AutoML problem typically decide which hy-
perparameter conÔ¨Åguration to try in iterative trials and adjust
their future decisions toward Ô¨Ånding better conÔ¨Ågurations.
To facilitate further analysis, we provide an abstraction for
such solutions via HPSearcher . AHPSearcher is es-
sentially responsible for making a hyperparameter conÔ¨Åg-
uration suggestion in the given search space whenever re-
quested. We use HPSearcher :suggest to denote the
procedure for making a hyperparameter conÔ¨Åguration sug-
gestion, and HPSearcher :update to denote the pro-
cedure for updating necessary information needed. This
abstraction is compatible with almost all hyperparameter
search methods (Bergstra et al., 2011; Bergstra & Bengio,
2012; Snoek et al., 2012; Li et al., 2020; Wu et al., 2021;
Wang et al., 2021a) for AutoML.
Under this AuotML context, it may seem straightforward
that to Ô¨Ånd a fair model with good validation loss, one
can simply add a fairness constraint on top of Eq. (1)and
perform constrained hyperparameter optimization, as what
is done in a recent work (Perrone et al., 2021). However,
one caveat in this approach is that, for an ad-hoc task, one
may not be able to Ô¨Ånd a single model that both satisÔ¨Åes
the fairness constraint and has a good loss by simply tuning
the hyperparameters c2C in AutoML. This essentially
is because there may exist multiple sources of unfairness
(Dudik et al., 2020), many of which are just impossible to
be alleviated by simply varying model hyperparameters.
3.2 Unfairness mitigation in AutoML: necessity and
impact
The aforementioned limitations of generic hyperparameter
tuning in AutoML in terms of improving fairness motivate
us to consider taking dedicated algorithmic unfairness mit-
igation methods, e.g., the pre-, in-, and post-processing
methods mentioned in Sec. 2 into AutoML.
Necessity of unfairness mitigation in AutoML. To verify
the necessity of unfairness mitigation in AutoML, we con-
1In this work, we focus on this commonly used formulation of
AutoML. The scope of AutoML in general can be beyond this.duct the following two experiments on the Bank dataset
(details about this dataset are deferred to Section 5). In
both experiments, we use the AutoML solution from an
open-source library named FLAML (Wang et al., 2021b).
‚Ä¢In the Ô¨Årst experiment, we perform a regular AutoML
process for 4 hours. Following our previous notations,
we get a set of ML models in this experiment denoted
byH=ff(0)
ci;Dtraingi2[1;2;:::], in whichciis the conÔ¨Ågu-
ration tried at the i-th iteration and the largest iteration
is determined by the 4-hours time budget.
‚Ä¢In the second experiment, within the same time budget,
i.e., 4 hours, for each hyperparameter conÔ¨Åguration
ciproposed by the original AutoML process at iter-
ationi, in addition to training a regular ML model
f(0)
ci;Dtrain, we also train a second model with unfair-
ness mitigation, i.e., f(1)
ci;Dtrain. We use a state-of-the-art
in-processing method named Exponentiated Gradient
(EG) (Agarwal et al., 2018) as the mitigation method.
In this experiment, we get the following set of models
~H=ff(0)
ci;Dtrain;f(1)
ci;Dtraingi2[1;2;:::]. Note that because
of the additional unfairness mitigation step, the to-
tal number of trials in this experiment is presumably
smaller than that in the Ô¨Årst one.
We perform the two experiments on two tuning tasks: one
tune both learner selection and each learner‚Äôs model hyper-
parameters, and one tune a single learner XGBoost‚Äôs2model
hyperparameters. We plot the results from the two tasks in
Figure 1(a) and (b), respectively. In the Ô¨Årst sub-Ô¨Ågure of
Figure 1(a) and (b) entitled ‚ÄúImpact on loss and fairness",
we plot the ‚Äòunfairness score‚Äô, i.e., Difference of Statsistical
Parity (DSP) and loss of the models evaluated in both ex-
periments. From the scatter plots in Figure 1, we have the
following observations: (1) We Ô¨Årst observe an obvious over-
all reduction of unfairness scores after mitigation is applied
(by comparing the ‚ÄòBefore mitigation‚Äô and ‚ÄòAfter mitigation‚Äô
points) in both tuning tasks. It shows the effectiveness of
unfairness mitigation in reducing the unfairness of each sin-
gle machine learning model for different types of learners.
(2) By comparing the ‚ÄòBefore mitigation‚Äô and ‚ÄòAfter miti-
gation‚Äô points together with the ‚ÄòNo mitigation‚Äô points, we
can see the necessity of performing unfairness mitigation in
AutoML: Although by simply performing learner selection
and model hyperparameter tuning, one can Ô¨Ånd models with
different degrees of fairness (as shown by the grey crosses
in the scatter plots), the loss of the models is not necessarily
good. For example, when 0.01 is used as the threshold in
the unfairness constraint, one only gets models with very
bad predictive performance in the tuning task in Figure 1(a),
2Note that this single learner setting is common in practical
scenarios where due to infrastructure or deployment constraints,
the institution or company can only use a certain type of model.

--- PAGE 4 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.09 0.10 0.11 0.12 0.13
Loss0.000.020.040.060.080.100.12Unfairness scoreImpact on loss and fairness
No mitigation
Before mitigation
After mitigation
60 80 100 120 140 160
Computation cost ratio0.00.51.01.52.02.53.03.54.0Number of trialsImpact on computation cost
(a) Unfairness mitigation‚Äôs impact in tuning multiple (7) learners
0.090 0.095 0.100 0.105 0.110 0.115
Loss0.000.020.040.060.080.100.12Unfairness scoreImpact on loss and fairness
No mitigation
Before mitigation
After mitigation
20 40 60 80 100 120 140 160
Computation cost ratio051015202530Number of trialsImpact on computation cost (b) Unfairness mitigation‚Äôs impact in tuning XGBoost
Figure 1. In scatter plots of both (a) and (b), the cross-marked scatters labeled ‚ÄòNo mitigation‚Äô correspond to models trained in the Ô¨Årst
experiment, i.e.,H=ff(0)
ci;Dtraingi2[1;2;:::]; the circle-marked scatters denote models from the second experiments, i.e., ~H. The scatters
labeled ‚ÄòBefore mitigation‚Äô and ‚ÄòAfter mitigation‚Äô correspond to models with the same hyperparameter conÔ¨Åguration but trained without
unfairness mitigation, i.e., f(0)
ci;Dtrainand with unfairness mitigation, i.e., f(1)
ci;Dtrain. The horizontal lines correspond to unfairness scores of
0.01 and 0.05 respectively, which are two commonly used thresholds when specifying fairness constraints. The 2nd sub-Ô¨Ågure of both (a)
and (b) show the distribution of the ‚Äòcomputation cost ratio‚Äô (the cost of training a model with mitigation, i.e., f(1)
ci;Dtraindivided by that of
training a regular model with the same conÔ¨Åguration, i.e., f(0)
ci;Dtrain).
and one cannot even Ô¨Ånd a single fair model satisfying the
constraint when the learner is restricted to XGBoost. With
unfairness mitigation, one can easily produce fair models
with a decent loss. These results conÔ¨Årm the necessity and
effectiveness of including unfairness mitigation in AutoML.
We now move on to analyze how one should incorporate
it into AutoML effectively and efÔ¨Åciently. To answer this
question, we Ô¨Ånd it necessary to Ô¨Årst examine the impact of
unfairness mitigation methods in terms of computation over-
head and predictive performance, which are crucial factors
that directly affect the Ô¨Ånal performance of AutoML.
Unfairness mitigation‚Äôs impact on ML and AutoML. A
recent study (Islam et al., 2022) provides a comprehensive
investigation on the impact of unfairness mitigation in ma-
chine learning via experimental evaluation on a wide range
of unfairness mitigation methods in the fair classiÔ¨Åcation
task. The impact can be understood from the following
three aspects. (1) Fairness: Most of the state-of-the-art
unfairness mitigation methods can effectively mitigate the
unfairness. But none of the mitigation methods are guaran-
teed to make every model fair for every dataset. (2) Pre-
dictive performance: In all unfairness mitigation methods,
the mitigation of unfairness comes with the degradation of
predictive performance, and the trade-off is complex. (3)
Computation overhead: Unfairness mitigation methods,
especially pre- and in-processing approaches generally incur
high computation overhead. We also observe this severe
issue on computation overhead in our case study according
to the cost histograms in Figure 1.
This complex impact makes the incorporation of unfairnessmitigation to AutoML highly non-trivial: On the one hand,
we want to perform unfairness mitigation to improve the fair-
ness of resulting ML models. On the other hand, performing
unfairness mitigation brings in computation overhead and
degrades the predictive performance of the models. A subtle
balance needs to be made.
3.3 Fair AutoML with unfairness mitigation
Abstractions. We Ô¨Årst develop two fairness-related abstrac-
tions. These abstractions are helpful in facilitating the for-
mulation and analysis of the fair AutoML problem.
Fairness assessment via Fair : We abstract the fairness as-
sessment of a machine learning model f, given a validation
datasetD, as a procedure that calculates a fairness indicator
based on the model f‚Äôs prediction outcome on the dataset D
according to a particular quantitative fairness deÔ¨Ånition. We
useFair (f;D)to denote this procedure in general. Both
group fairness and individual fairness, which are the two
dominating types of fairness deÔ¨Ånitions, Ô¨Åt into this abstrac-
tion. This abstraction can also handle the case where the
fairness goal is to satisfy multiple fairness constraints. Note
that in the context of a particular concrete fairness deÔ¨Ånition,
additional information other than the model prediction and
the dataset might be needed.
Example 1 (Group fairness) .Under the group fairness
context, a model is considered ‚Äòfair‚Äô if the output of the
disparity measurement regarding the sensitive attribute(s)
does not exceed a particular threshold. We denote by A
the sensitive attribute variable, G(;A)a disparity func-
tion (e.g., statistical disparity) parameterized by sensitive

--- PAGE 5 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
attributeA, anda fairness threshold constant. Given
(A;G;), an implementation of the abstraction can be
Fair (f;D) = 1fG(X;Y;^f(D);A)g, in which 1fg
is an indicator function.
Unfairness mitigation via Mitigate : We abstract the un-
fairness mitigation of a machine learning algorithm given
a training dataset Das a procedure that intervenes in
the model building process (including data transformation,
model training, and inference rules) in a way such that the
Ô¨Ånal model‚Äôs outputs are pushed toward a particular fairness
target (speciÔ¨Åed via Fair function following our abstrac-
tion). All the pre-, in-, and post-processing unfairness miti-
gation introduced earlier Ô¨Åt into this abstraction. Following
notations introduced earlier, f(1)
c;Dis obtained via this mitiga-
tion procedure, i.e., f(1)
c;D Mitigate (f;c;D; Fair ).
Fair AutoML problem formulation. Given datasets Dtrain
andDval, a loss function Loss , a fairness function Fair
and a particular unfairness mitigation method following
theMitigate abstraction, we formulate the fair AutoML
problem as Ô¨Ånding a fair machine learning model minimiz-
ing the loss by searching over a set of hyperparameters and
deciding whether to do unfairness mitigation, as mathemati-
cally expressed below,
min
c2C;m2f0;1gLoss (f(m)
c;D train(Xval);Yval) (2)
s:t:Fair (f(m)
c;D train;Dval)
In the rest of the paper we use Loss(m)
c and
Fair(m)
cas shorthand for Loss (f(m)
c;D train(Xval);Yval)and
Fair (fm
c;D train;Dval)respectively without ambiguity. Un-
der this formulation, we can consider fair AutoML as an
AutoML problem optimizing for the fair loss , i.e., the loss
when fairness constraint is satisÔ¨Åed. This fair loss notion,
denoted asLfair, will be used though this paper to evaluate
the effectiveness of fair AutoML solutions.
The proposed formulation has many desirable properties:
The unfairness mitigation abstraction decouples the com-
plexity of regular AutoML and unfairness mitigation. On
the one hand, this makes it easy to leverage existing research
and development efforts on unfairness mitigation. On the
other hand, this framing makes it fairly easy to achieve fair
AutoML by augmenting an existing AutoML system.
Despite the desirable properties of the proposed formulation,
solving the problem is still challenging due to the following
reasons: (1) Solving an AutoML problem itself is notori-
ously expensive. It typically involves a large number of
expensive trials. (2) The large computation overhead com-
pounds this computation challenge by making some of the
trials with unfairness mitigation as large as 10x to 100x
more expensive.4 F AIRAUTOML FRAMEWORK AND
SYSTEM
We Ô¨Årst propose a general fair AutoML framework to solve
the fair AutoML problem characterized in Eq. (1). The
proposed framework is presented in Alg. 1, the central com-
ponent of which is an abstraction named FairSearcher .
The main responsibility of FairSearcher is to suggest
a promising hyperparameter conÔ¨Åguration c2C and de-
cide whether to perform unfairness mitigation, indicated
bym= 0 orm= 1 at each trial. This responsibility is
realized via a suggest function (line 4 of Alg. 1). The
system then builds a machine learning model f(m)
cbased on
the suggested candm(line 5 of Alg. 1), and gets validation
loss and fairness on the validation dataset (line 6 of Alg. 1).
Notice that the model training and validation step typically
incurs a non-trivial cost, denoted by (m)
c. The framework
then records the incurred cost (m)
c, updates the budget left
and records the detailed trial observation, including the sug-
gested hyperparameter conÔ¨Åguration c, mitigation decision
m, and the corresponding loss Loss(m)
c, fairness Fair(m)
c
and computation cost (m)
c, inH. Then FairSearcher
will be updated via an update function such that the new
observation can be used to help with future suggestions.
4.1 FairSearcher
Insights. FairSearcher shall be designed in a way such
that it is able to make suggestions on hyperparameter conÔ¨Åg-
uration and unfairness mitigation toward a fair model with
good predictive performance. Generic hyperparameter tun-
ing methods (following the HPSearcher abstraction) can
be effective in Ô¨Ånding models with good predictive perfor-
mance. And unfairness mitigation methods (following the
Mitigate abstractions) can be used to further reduce the
unfairness of the models. We believe it is beneÔ¨Åcial to take
the best of both worlds. Based on this insight, we propose
to include HPSearcher andMitigate as sub-modules
of the FairSearcher , where HPSearcher is primar-
ily responsible for proposing hyperparameter conÔ¨Åguration
c2C, and FairSearcher is responsible for reducing
the unfairness of each individual model. We further develop
an additional logic to control whether and when to perform
unfairness mitigation instead of hyperparameter search.
We present the two major functionalities of the proposed
FairSearcher in Alg. 2 and Alg. 3 respectively. Each
time when the FairSearcher :suggest function is in-
volved, the FairSearcher Ô¨Årst selects the evaluated but
unmitigated hyperparmeter conÔ¨Åguration which has the low-
est loss and does not satisfy the fairness constraint, as the
candidate hyperparmeter conÔ¨Åguration to perform unfair-
ness mitigation (line 2 of Alg. 2). FairSearcher then
Ô¨Ånalizes the decision after checking several conditions (line

--- PAGE 6 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
Algorithm 1 FairAutoML
1:Inputs: Training and validation data, search space C,
and resource budget B(optional).
2:Initialization:H= []
3:whileB > 0do
4:c;m FairSearcher :suggest (H)
5: Model building: train model f(m)
cwith (whenm=
1) or without (when m= 0) unfairness mitigation
6: Model validation: Get Loss(m)
candFair(m)
c
7: Audit the incurred cost (m)
c, and update budget B 
B (m)
c, and historical observations H H[
(c;m;(m)
c;Loss(m)
c;Fair(m)
c)
8:FairSearcher :update (H).
9:end while
Algorithm 2 FairSearcher .suggest (C)
1:H(0)=fc2Hjmc= 0g
2:c0 arg minc2H(0)&:Fair cLossc
3:ifc0and ECF(1)<ECF(0)andLc<L
fair then
4:m 1,c c0
5:else
6:m 0,c HPSearcher :suggestC)
7:end if
8:Returnc;m
3-6 of Alg. 2): if the conditions for doing unfairness mit-
igation are satisÔ¨Åed, the FairSearcher suggests to do
unfairness mitigation, i.e., m= 1, on the candidate hyper-
parameter conÔ¨Åguration c0(line 4); otherwise it invokes the
HPSearcher to suggest model hyperparmeter and set the
mitigation decision to m= 0(line 6).
These two choices have different implications to the system.
Regular hyperparameter search provides good candidate
models to perform unfairness mitigation and sometimes can
even directly Ô¨Ånd better fair models. But the fairness of the
models obtained is not guaranteed. For example, when tun-
ing XGBoost, by simply tuning hyperparameters one cannot
Ô¨Ånd a single model with unfairness score smaller than 0.01
according to the scatter plot in Figure 1(b). Further unfair-
ness mitigation is still needed. Unfairness mitigation is in
general useful in producing a model that is fair. However
it may degrade the accuracy of the resulting model, and
brings in potentially high computation overhead. The high
computation overhead may make the system unable to eval-
uate enough models to navigate a model with good accuracy
when the computation budget is limited. The conditions in
line 3 of Alg. 2 are designed to make a subtle balance be-
tween these two choices: it is an adaptive strategy based on
online estimations of the utility of regular hyperparameter
search and unfairness mitigation.
Insights for line 3 of FairSearcher :suggest inAlgorithm 3 FairSearcher .update (H)
1:H(0)=fc2Hjmc= 0g;H(1)=H H(0)
2:Update ECF(0)based onH(0), and update ECF(1)
based onH(1)according to their deÔ¨Ånitions
3:HPSearcher :update (H0)
Alg. 2: Considering the potential loss degradation and high
computation overhead of unfairness mitigation, the algo-
rithm should not do unfairness mitigation on the conÔ¨Ågura-
tionc0unless both the following two conditions hold: (1)
When doing unfairness mitigation is more ‚ÄòefÔ¨Åcient‚Äô than
doing regular hyperparameter search via HPSearcher in
terms of improving fair loss. In general, there may exist
cases where hyperparameter search is already good enough
to Ô¨Ånd fair models with good loss, for example, when the
fairness constraint is easy to satisfy. In this case, unfairness
mitigation is not needed unless it is more efÔ¨Åcient in improv-
ing the fair loss than hyperparameter search. (2) When the
FairSearcher can anticipate a fair loss improvement by
doing mitigation on the candidate conÔ¨Åguration c. Ideally,
we want to spend resources in unfairness mitigation on the
conÔ¨Ågurations that can yield better fair loss than the current
best fair loss. Although such information, i.e., the fair loss
of a conÔ¨Åguration after mitigation, is not available a priori ,
the original loss before mitigation is usually a meaningful
observable indicator of it.
To realize the Ô¨Årst condition, we use ECF(m), which is anal-
ogous to the CostImp function proposed in (Wang et al.,
2021a), to characterize the fair loss improvement efÔ¨Åciency
of both regular hyperparameter search and unfairness mitiga-
tion. It essentially approximates the Estimated Cost needed
for producing a model with a better Fair loss by doing unfair-
ness mitigation ( ECF(1)), or hyperparameter tuning with the
HPSearcher (corresponds to ECF(0)). Unfairness miti-
gation should not be conducted unless ECF(1)<ECF(0).
One nice property of this strategy is that it can achieve a
self-adaptive balance between mitigation and hyperparame-
ter search: Once one choice becomes less efÔ¨Åcient we turn
to the other choice. We include the deÔ¨Ånition and detailed
explanation of ECF(m)in Appendix A.
To realize the second condition, we propose to make a pro-
jection on the fair loss that can be achieved and the resource
needed for the unfairness mitigation before actually per-
forming it. For a particular conÔ¨Åguration c, we denote by
Lcthe projected loss after mitigation. SpeciÔ¨Åcally, we es-
timate LcbyLc=Loss(0)
c+Ht;BwhereHt;Bis an
online estimation of the loss degradation of unfairness mit-
igation based on historical observations in Htand budget
B. The algorithm performs unfairness mitigation only when
Lc<L
fair, in whichL
fairis the best fair loss achieved so far
by the system. Intuitively speaking, the algorithm tries to

--- PAGE 7 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
skip unfairness mitigation on some of the models if doing
is unlikely to yield better fair loss. We provide the detailed
formula of Lcin Appendix A.
Considering both conditions, we use fECF(1)<
ECF(0)&Lc< L
fairg(line 3 of Alg. 2) as the decision
rule to determine whether to perform unfairness mitigation.
Once the decisions on which hyperparameter conÔ¨Åguration
to try and whether to perform unfairness mitigation is made
(line 4 of Alg. 1 via FairSearcher .suggest ), the al-
gorithm proceeds to build a model and performs model
validation accordingly (line 5 and line 6 of Alg. 1). The
algorithm also records the computation cost incurred dur-
ing the model building and validation step, the resulting
model‚Äôs predictive loss and fairness, and updates them as
part of the historical observations in H. The remaining
budgets will also be updated accordingly. At last, the al-
gorithm updates the FairSearcher with the historical
observationsHviaFairSearcher .update (line 8 of
Alg. 1). FairSearcher .update is presented in Alg. 3.
It updates the statistics needed for the FairSearcher to
make suggestions on unfairness mitigation, and also invokes
HPSearcher .update to update the HPSearcher .
5 E XPERIMENTS
Datasets, baselines, and evaluation metrics. We per-
form empirical evaluation on four machine learning fair-
ness benchmark datasets, including Adult (Kohavi, 1996),
Bank (Moro et al., 2014), Compas (Angwin et al., 2016)
and a Medical Expenditure Panel Survey dataset ( MEPS
in short). All the datasets are publicly available and we
provide more details about them in Appendix A. We include
FLAML and FairBO, which are reviewed in Section 2, as
the baselines. We use 1 validation accuracy as the loss
metric and fair loss as the Ô¨Ånal metric for evaluating the
performance of the compared methods.
Fair AutoML experiment settings. (1) Fairness settings.
We include two commonly used group fairness deÔ¨Ånitions
for classiÔ¨Åcation tasks, including Demographic Parity (DP)
and Equalized Odds (EO). Following the group fairness
evaluation settings in AIF360, we use ‚Äòsex‚Äô as the sensitive
attribute on Adult andCompas , ‚Äòage‚Äô on Bank , and ‚Äòrace‚Äô
onMEPS . We use two commonly used disparity thresholds
= 0:05and0:01to determine whether the disparity is
small enough to be fair. These two thresholds correspond to
a mild and a harsh fairness requirement respectively. We test
two in-processing unfairness methods, including Exponen-
tiated Gradient reduction, Grid Search reduction (Agarwal
et al., 2018), and one post-processing method (Hardt et al.,
2016). We report the results for Exponentiated Gradient
in the main paper and the other two in Appendix B. We
leverage existing implementations of the fairness metricsand unfairness mitigation methods from the open-source
library Fairlearn. (2) AutoML settings. We do evaluations
on two tuning tasks: (a) hyperparameter tuning on XGBoost;
(b) hyperparameter tuning (including learner selection and
model hyperparameter tuning) on multiple machine learn-
ers. All the other AutoML-related components, such as
data pre-processing, the choice of search space, and the
hyperparameter searcher, remain the same as FLAML‚Äôs de-
fault options. Note that two different tuning methods (Wu
et al., 2021; Wang et al., 2021a) are used in XGBoost and
multi-learner tuning tasks, respectively, according to the
default settings of FLAML. In all the presented results, we
name our method FairFLAML (instead of the general name
FairAutoML) to reÔ¨Çect the fact that built based on the Au-
toML system FLAML. We run all the experiments up to
1 hour wall-clock time with 1 CPU core ten times with
different random seeds if not otherwise noted. (3) Paral-
lelization . We test the performance of our method under
both sequential and parallel computation resources and in-
clude the results in the third subsection.
5.1 Effectiveness
We Ô¨Årst want to verify the effectiveness of our method in
terms of producing models with good fair loss. We summa-
rize the results from FairFLAML and the baselines when
tuning XGBoost in Figure 2, and when tuning multiple
learners in in Figure 3. In the latter experiment, we do
not include FairBO as it does not support tuning multiple
learners simultaneously.
In the sub-Ô¨Ågures of Figure 2, we summarize the fair loss
of different methods under different fairness settings. In the
sub-Ô¨Ågures of Figure 3, we show results aggregated over
all combinations of fairness settings and random seeds on
different datasets.
We Ô¨Årst observe that, on the Compas dataset, it is indeed
possible to produce ML models with good fair loss by sim-
ply tuning regular hyperparameters in certain cases: both
FLAML and FairFLAML achieve the best fair loss when
tuning multiple learners as shown in the 3rd sub-Ô¨Ågure of
Figure 3. However, in all the other cases, FairFLAML
achieves signiÔ¨Åcantly better fair loss than both FLAML
and FairBO (when tuning XGBoost), especially when the
fairness requirement is harsh. For example, when tuning
XGBoost on the MEPS dataset (Figure 4(d)), when thresh-
old = 0.05, FLAML‚Äôs performance is close to FairFLAML .
When threshold = 0.01, FLAML‚Äôs performance can become
very bad and sometimes it cannot even Ô¨Ånd a single model
satisfying the fairness constraint. This again veriÔ¨Åes the
huge beneÔ¨Åt of introducing unfairness mitigation to Au-
toML. Regarding the baselines, we also observe that the per-
formance of FairBO is much worse than both FairFLAML
and FLAML. There are mainly two reasons for its bad per-

--- PAGE 8 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
FairFLAML
FLAML
FairBO
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.01
(a) Fair loss on Adult
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.8Cumulative Proportion
Fairness threshold=0.01
(b) Fair loss on Bank
0.32 0.33 0.34 0.35 0.36 0.37
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
0.32 0.33 0.34 0.35 0.36 0.37
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.32 0.33 0.34 0.35 0.36 0.37
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.32 0.33 0.34 0.35 0.36 0.37
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.01
(c) Fair loss on Compas
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.8Cumulative Proportion
Fairness metric=EO
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.01
(d) Fair loss on MEPS
Figure 2. Fair loss under different fairness settings when tuning XGBoost on different datasets. Each sub-Ô¨Ågure shows the fair losses‚Äô
cumulative proportion in 20 experiments under a particular fairness metric or threshold indicated by subÔ¨Ågure titles: (2 fairness settings)
(10 random seeds). We use 1:2(best loss) as the x-axis (showing the fair loss) upper bound to improve visibility of the curves.
0.210 0.215 0.220 0.225 0.230 0.235
Fair Loss0.00.20.40.60.81.0Cumulative Proportion
Dataset=Adult
0.090 0.095 0.100 0.105 0.110 0.115 0.120 0.125 0.130
Fair Loss0.00.20.40.60.81.0Cumulative Proportion
Dataset=Bank
0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46
Fair Loss0.00.20.40.60.81.0Cumulative Proportion
Dataset=Compas
0.130 0.135 0.140 0.145 0.150 0.155 0.160 0.165
Fair Loss0.00.20.40.60.81.0Cumulative Proportion
Dataset=MEPS
Figure 3. Fair loss of different methods when tuning multiple learners on different datasts. Each sub-Ô¨Ågure aggregates results from 40
experiments: (4 combinations of fairness settings) (10 random seeds).

--- PAGE 9 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050 0.1075
Fair Loss0.00.20.40.60.8Cumulative Proportion
FairFLAML
MAlways
MasHP
FLAML
FairFLAML MAlways MasHP FLAML020406080100120140160Percentage of resouce used0.0910.092 0.092 0.092Wasted Mitigation
Effective Mitigation
HPO
0.089 0.090 0.091 0.092 0.093 0.094 0.095 0.096 0.097
Loss0.20.40.60.81.0Cumulative proportion
(a) Fair loss, resource consumption breakdown, and loss on the Bank dataset
0.32 0.33 0.34 0.35 0.36 0.37
Fair Loss0.00.10.20.30.40.50.60.70.8Cumulative Proportion
FairFLAML MAlways MasHP FLAML020406080100120140160
0.3280.337 0.336 0.344
0.312 0.314 0.316 0.318 0.320
Loss0.20.40.60.81.0Cumulative proportion
(b) Fair loss, resource consumption breakdown, and loss on the Compas dataset
Figure 4. Sub-Ô¨Ågures in the 1st column and the 3rd column show fair loss and loss (without considering fairness constraints) on different
datasets aggregated over 40 experiments similar to that in Figure 3. Sub-Ô¨Ågures in the 2nd column show the detailed resource consumption
breakdown. Each bar consists of resource consumption (the height of each bar indicates the resource consumption value) of three possible
types of trials: trials where unfairness mitigation is not applied, denoted ‚ÄòHPO‚Äô; trials where unfairness mitigation is applied and the
resulting model yields better fair loss (at that time point), labeled ‚ÄòEffective Mitigation‚Äô and trials where unfairness mitigation does not
yield better fair loss, labeled ‚ÄòWasted Mitigation‚Äô. The numbers above the bars are the fair losses achieved by the corresponding methods.
formance: (1) Similar to the case of FLAML, FairBO is also
tuning regular model hyperparameters to Ô¨Ånd fair models
with good loss. Using a fairness-constrained optimization
method does not reconcile the ineffectiveness of hyperpa-
rameter tuning in building a model with good fair loss; (2)
Unlike FLAML, the Bayesian optimization method used in
FairBO does not consider the different computation cost of
models with different complexities, and thus may try unnec-
essarily costly models, which makes the method, in general,
more time-consuming than FLAML.
In summary, the comprehensive evaluations verify the ne-
cessity of introducing unfairness mitigation to AutoML and
the effectiveness of our method.
5.2 Ablation study and efÔ¨Åciency
Alternative ways to include unfairness mitigation. We
further compare FairFLAML with two alternative ap-
proaches to incorporate unfairness mitigation under the
general fair autoML framework proposed in Alg. 1: (1)
As mentioned in Section 3, one straightforward approach to
include unfairness mitigation is to always enforce it during
model training after each hyperparameter conÔ¨Åguration is
proposed. We name this approach MAlways . (2) Anotheralternative is to treat unfairness mitigation as an additional
categorical ‚Äòhyperparameter‚Äô taking binary values (1/0 for
enabling/disabling unfairness mitigation) and apply con-
strained hyperparameter optimization (FLAML, 2021). We
name this method MasHP . Both these approaches can be
realized via the FairSearcher abstraction and can be
considered as an ablation of FairFLAML . By comparing
with these two alternative approaches, we show the necessity
of the self-adaptive mitigation strategy in FairFLAML.
We provide result comparisons with MAlways ,MasHP ,
and FLAML in on the Bank andCompas Figure 4. Due to
page limit, we include the results on the other two datasets
in Figure 6 of the appendix.
1.In the 1st column of Figure 4, we show the fair
loss of FairFLAML and its two alternatives under a
small resource budget (300s). The results demonstrate
FairFLAML ‚Äôs signiÔ¨Åcant advantage over the two alter-
natives regarding the fair loss.
2.To better understand how resource is used in different
methods, in the 2nd column of Figure 4, we visual-
ize the resource breakdown for different methods to
achieve the best fair loss under a particular experiment
setting with one random seed. (1) We Ô¨Årst observe that

--- PAGE 10 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.1370 0.1375 0.1380 0.1385 0.1390 0.1395 0.1400 0.1405
Fair Loss0.20.40.60.81.0Cumulative Proportion
Wall-clock time=300s
FairFLAML-1
FairFLAML-8
0.1370 0.1375 0.1380 0.1385 0.1390 0.1395 0.1400
Fair Loss0.20.40.60.81.0Cumulative Proportion
Wall-clock time=600s
0.1370 0.1375 0.1380 0.1385 0.1390 0.1395 0.1400
Fair Loss0.20.40.60.81.0Cumulative Proportion
Wall-clock time=1200s
0.1365 0.1370 0.1375 0.1380 0.1385 0.1390 0.1395 0.1400
Fair Loss0.20.40.60.81.0Cumulative Proportion
Wall-clock time=2400s
Figure 5. Parallelization results under wall-clock time budgets on the MEPS dataset. FairFLAML -1 represents the variant of our method
ran with a single core and FairFLAML-8 with 8 cores.
inMAlways andMasHP , ‚ÄòWasted Mitigation‚Äô domi-
nates the resource consumption, which is an important
source of their inefÔ¨Åciency. (2) FLAML can be efÔ¨Å-
cient in Ô¨Ånding fair models with the best fair loss when
it is possible to do so via regular hyperparameter search
as shown on the Compas dataset. However, the effec-
tiveness of this method is highly dataset-dependent.
(3)FairFLAML can achieve on average better perfor-
mance with much less resource than the two alterna-
tives according to the fair loss values and the heights of
the resource consumption bars. FairFLAML is able to
realize the beneÔ¨Åt of both hyperparameter search and
unfairness mitigation.
3.We believe it is also meaningful to investigate the orig-
inal loss without considering fairness constrains, we
show the results in the 3rd column of Figure 4. We
observe that FairFLAML preserves FLAML‚Äôs good
performance regarding the original loss while having
the best fair loss. This property is especially desir-
able when the practitioners are in an explorative mode
regarding machine learning fairness, which is quite
common due to the under-development of this topic in
practical scenarios. It alleviates potential hesitations
of adopting unfairness mitigation to AutoML due to
concerns about a degraded original loss. By knowing
the best original loss achievable without considering
fairness constraints, practitioners can gain more con-
Ô¨Ådence in their decision-making. FairFLAML is the
only system that can Ô¨Ånd both the best original loss
and the best fair loss.
5.3 Parallelization
Our method is easy to parallelize. We include the results
of our method run with a single core and multiple cores un-
der different wall-clock time budgets on the MEPS dataset,
which is the largest dataset considering both the number of
data instances and dimensionality in Figure 5. The results
show that by increasing the parallel computation resources,
our method can achieve even better results under the same
wall-clock time budget, which is helpful in reducing the
turn-around time of the tuning task in time-sensitive tasks.We also observe that when wall-clock time budgets increase,
the difference between the parallel variant and sequential
variant decreases.
5.4 Additional evaluation results and extensions
Due to space limit, we include additional evaluation results
in Appendix B. (1) An additional baseline: In addition to
the two alternative approaches in our ablation study, we com-
pare our method with a third alternative. This alternative is
a post hoc approach in which we select the conÔ¨Ågurations
with the best loss to perform unfairness mitigation after a
regular AutoML process is Ô¨Ånished. We include the detailed
descriptions about these three alternatives, and comparisons
with the post hoc alternative under different experiment set-
tings in Figure 7 of Appendix B. (2) Results on different
types of unfairness mitigation methods: Our method is
compatible with most of the state-of-the-art unfairness miti-
gation methods. We include additional experiments under
different types of unfairness mitigation methods in Figure 8
and Figure 9 of Appendix B. (3) Extensibility: Our pro-
posed framework and system are highly extensible and are
compatible with a wide range of hyperparameter searchers,
fairness deÔ¨Ånitions, and unfairness mitigation methods. We
include instructions and code examples to verify the wide
compatibility in Appendix B.3.
6 S UMMARY AND FUTURE WORK
In this work, we Ô¨Årst identify that it is beneÔ¨Åcial and some-
times necessary to introduce unfairness mitigation to the
AutoML workÔ¨Çow to help improve the fairness of the overall
system. We propose a general framework for incorporat-
ing unfairness mitigation as an organic part of the AutoML
process and present a fair AutoML system that is Ô¨Çexible,
robust, and efÔ¨Åcient.

--- PAGE 11 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
REFERENCES
Automl market. ReportLinker , Dec 2021. URL https:
//www.reportlinker.com/p06191010/
AutoML-Market.html?utm_source=GNW .
Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., and
Wallach, H. A reductions approach to fair classiÔ¨Åcation.
InInternational Conference on Machine Learning , pp.
60‚Äì69. PMLR, 2018.
Agarwal, A., Dudik, M., and Wu, Z. S. Fair regression
quantitative deÔ¨Ånitions and reduction-based algorithms.
InInternational Conference on Machine Learning , pp.
120‚Äì129. PMLR, 2019.
Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M.
Optuna: A next-generation hyperparameter optimization
framework. In Proceedings of the 25th ACM SIGKDD
international conference on knowledge discovery & data
mining , 2019.
Angwin, J., Larson, J., Mattu, S., and Kirchner, L. Machine
bias there‚Äôs software used across the country to predict fu-
ture criminals. and it‚Äôs biased against blacks. ProPublica ,
2016.
BankofEngland. Machine learning in uk Ô¨Ånancial services,
10, 2019.
Barocas, S. and Selbst, A. D. Big data‚Äôs disparate impact.
Calif. L. Rev. , 104:671, 2016.
Bellamy, R. K. E., Dey, K., Hind, M., Hoffman, S. C.,
Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S.,
Mojsilovic, A., Nagar, S., Ramamurthy, K. N., Richards,
J., Saha, D., Sattigeri, P., Singh, M., Varshney, K. R.,
and Zhang, Y . AI Fairness 360 an extensible toolkit
for detecting, understanding, and mitigating unwanted
algorithmic bias, October 2018.
Bergstra, J. and Bengio, Y . Random search for hyper-
parameter optimization. Journal of machine learning
research , 13(2), 2012.
Bergstra, J. S., Bardenet, R., Bengio, Y ., and K√©gl, B. Algo-
rithms for hyper-parameter optimization. In Advances in
neural information processing systems , 2011.
Bird, S., Dudik, M., Edgar, R., Horn, B., Lutz, R., Milan,
V ., Sameki, M., Wallach, H., and Walker, K. Fairlearn a
toolkit for assessing and improving fairness in ai. Techni-
cal Report MSR-TR-2020-32, Microsoft, May 2020.
Calmon, F., Wei, D., Vinzamuri, B., Natesan Ramamurthy,
K., and Varshney, K. R. Optimized pre-processing for
discrimination prevention. In Guyon, I., Luxburg, U. V .,
Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,
and Garnett, R. (eds.), Advances in Neural Information
Processing Systems , 2017.Chouldechova, A. Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments. Big
data, 5(2):153‚Äì163, 2017.
Cruz, A., Saleiro, P., Belem, C., Soares, C., and Bizarro, P.
Promoting fairness through hyperparameter optimization.
arXiv preprint arXiv2103.12715 , 2021.
Dudik, M., Chen, W., Barocas, S., Inchiosa, M., Lewins, N.,
Oprescu, M., Qiao, J., Sameki, M., Schlener, M., Tuo, J.,
and Wallach, H. Assessing and mitigating unfairness in
credit models with the fairlearn toolkit. Technical Report
MSR-TR-2020-34, Microsoft, September 2020.
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel,
R. Fairness through awareness. In Proceedings of the 3rd
innovations in theoretical computer science conference ,
2012.
Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C.,
and Venkatasubramanian, S. Certifying and removing dis-
parate impact. In proceedings of the 21th ACM SIGKDD
international conference on knowledge discovery and
data mining , 2015.
Feurer, M., Klein, A., Eggensperger, K., Springenberg, J.,
Blum, M., and Hutter, F. EfÔ¨Åcient and robust automated
machine learning. In Advances in neural information
processing systems , 2015.
FLAML. Constraints on the metrics of the ml
model tried in Ô¨Çaml. https://microsoft.
github.io/FLAML/docs/Use-Cases/
Task-Oriented-AutoML#constraint , 2021.
Friedler, S., Scheidegger, C., and Venkatasubramanian, S.
Certifying and removing disparate impact. arXiv preprint
arXiv1412.3756 , 2014.
H2O.ai. H2o automl. http://docs.h2o.ai/h2o/
latest-stable/h2o-docs/automl.html ,
2019.
Hardt, M., Price, E., and Srebro, N. Equality of opportunity
in supervised learning. Advances in neural information
processing systems , 2016.
Islam, M. T., Fariha, A., Meliou, A., and Salimi, B. Through
the data management lens: Experimental analysis and
evaluation of fair classiÔ¨Åcation. In Proceedings of the
2022 International Conference on Management of Data ,
2022.
John, D. The road ahead artiÔ¨Åcial intelligence and the future
of Ô¨Ånancial services. The Economist Intelligence Unit ,
2020.

--- PAGE 12 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
Kamiran, F. and Calders, T. Data preprocessing techniques
for classiÔ¨Åcation without discrimination. Knowledge and
Information Systems , 33(1):1‚Äì33, 2012.
Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware
learning through regularization approach. In 2011 IEEE
11th International Conference on Data Mining Work-
shops , pp. 643‚Äì650. IEEE, 2011.
Kohavi, R. Scaling up the accuracy of naive-bayes clas-
siÔ¨Åers: A decision-tree hybrid. In Proceedings of the
Second International Conference on Knowledge Discov-
ery and Data Mining , 1996.
Lahoti, P., Gummadi, K. P., and Weikum, G. ifair: Learning
individually fair data representations for algorithmic deci-
sion making. In 2019 IEEE 35th international conference
on data engineering (icde) , pp. 1334‚Äì1345. IEEE, 2019.
Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Ben-
Tzur, J., Hardt, M., Recht, B., and Talwalkar, A. A system
for massively parallel hyperparameter tuning. Proceed-
ings of Machine Learning and Systems , 2:230‚Äì246, 2020.
Moro, S., Cortez, P., and Rita, P. A data-driven approach
to predict the success of bank telemarketing. Decision
Support Systems , 62:22‚Äì31, 2014.
Olson, R. S., Urbanowicz, R. J., Andrews, P. C., Lavender,
N. A., Kidd, L. C., and Moore, J. H. Automating biomedi-
cal data science through tree-based pipeline optimization.
In Squillero, G. and Burelli, P. (eds.), Applications of
Evolutionary Computation , pp. 123‚Äì137. Springer Inter-
national Publishing, 2016.
O‚Äôneil, C. Weapons of math destruction How big data
increases inequality and threatens democracy . Crown,
2016.
Perrone, V ., Donini, M., Zafar, M. B., Schmucker, R., Ken-
thapadi, K., and Archambeau, C. Fair bayesian optimiza-
tion. In Proceedings of the 2021 AAAI/ACM Conference
on AI, Ethics, and Society , 2021.
Schmucker, R., Donini, M., Perrone, V ., and Archambeau,
C. Multi-objective multi-Ô¨Ådelity hyperparameter opti-
mization with application to fairness. In NeurIPS 2020
Workshop on Meta-learning , 2020.
Simoiu, C., Corbett-Davies, S., and Goel, S. The problem
of infra-marginality in outcome tests for discrimination.
The Annals of Applied Statistics , 11(3):1193‚Äì1216, 2017.
Snoek, J., Larochelle, H., and Adams, R. P. Practical
bayesian optimization of machine learning algorithms.
Advances in neural information processing systems , 2012.Wang, C., Wu, Q., Huang, S., and Saied, A. Economic hy-
perparameter optimization with blended search strategy.
InInternational Conference on Learning Representations ,
2021a.
Wang, C., Wu, Q., Weimer, M., and Zhu, E. Flaml: A fast
and lightweight automl library. In MLSys , 2021b.
Woodworth, B., Gunasekar, S., Ohannessian, M. I., and
Srebro, N. Learning non-discriminatory predictors. In
Conference on Learning Theory . PMLR, 2017.
Wu, Q., Wang, C., and Huang, S. Frugal optimization for
cost-related hyperparameters. In Proceedings of the AAAI
Conference on ArtiÔ¨Åcial Intelligence , 2021.
Zafar, M. B., Valera, I., Rogriguez, M. G., and Gummadi,
K. P. Fairness constraints mechanisms for fair classiÔ¨Åca-
tion. In AISTATS , 2017.
Zhang, H., Chu, X., Asudeh, A., and Navathe, S. B. Om-
nifair: A declarative system for model-agnostic group
fairness in machine learning. In Proceedings of the 2021
International Conference on Management of Data , 2021.

--- PAGE 13 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
A O MITTED DETAILS
Formal deÔ¨Ånition and desirable properties of ECF.
ECF(m):= maxfR(m) R1st
(m);R1st
(m) R2nd
(m); (3)
2L1st
fair;(m) L
fair
s(m)g
in which the superscript or subscript (m)denotes the variant
of the variable associated with unfairness mitigation ( m=
1) or hyperparameter tuning ( m= 0).R(m)is the total
resource spent, R1st
(m)is the total resource spent when the
model with the best fair loss is Ô¨Årst obtained and L1st
fair;(m)is
the corresponding best fair loss obtained, R2nd
Fis the total
resource spent when the model with the second best fair loss
is Ô¨Årst obtained, s(m)is the speed of fair loss improvement,
andL
fairis the overall best fair loss obtained in the system.
We mentioned the nice self-adaptive property of the
decision-making strategy based on ECF. It is worth mention-
ing that this property holds even if ECF is not an accurate
estimation of its ground-truth counterpart: In the case when
ECF(1)and/or ECF(0)are not accurate estimation of their
ground-truth counterparts and a wrong choice is made, the
consequence of this wrong choice will be reÔ¨Çected in the
ECF of the selected choice (it will become larger), while
theECF of the other choice (the second choice) remains
unchanged. Thus we will turn to the second choice.
Formal deÔ¨Ånition of Ht;B.Considering the general loss
degradation of unfairness mitigation, and the the error anal-
ysis provided in (Agarwal et al., 2018), we approximate the
loss degradation with the following formula.
Ht;B=(
 ECI<c;ECI+c<B
 bOtherwise(4)
in whichandbare the average and 95% conÔ¨Ådence ra-
dius of loss degradation after doing mitigation according to
the historical observations respectively; cis the projected
resource needed to do the mitigation estimated based on
historical observations; and ECIis originally from the em-
ployed AutoML system FLAML (Wang et al., 2021b), and
is the estimated cost for achieving loss improvement. We
calculated the projected resource needed for performing a
successful unfairness mitigation on hyperparameter c, i.e.,
caccording to:
c:=(0)
c
qmitigation1
jH(1)jX
c02H(1)(1)
c0
(0)
c0(5)
in whichjH(1)
Fjis the length ofH(1)(i.e., the total number
of conÔ¨Ågurations on which we performed unfairness miti-
gation), and qmitigation :=jfc02H(1)jFairc0=1gj
jH(1)jis the successrate of performing unfairness mitigation. Recall that (1)
c0
and(0)
c0are the actual resource used for performing model
training w/ and w/o unfairness mitigation based on hyperpa-
rameterc0respectively. The factor(1)
c0
(0)
c0is the ‚Äòcomputation
cost ratio‚Äô visualized in the last column of Figure 1, and thus
(0)
c1
jH(1)jP
c02H(1)(1)
c0
(0)
c0is the expected resource consump-
tion for applying mitigation to conÔ¨Åguration c. We further
penalize it by qmitigation to get an estimation of the resource
needed for a successful mitigation.
Datasets. The four datasets used in this paper are all pub-
licly available and representative for three fairness-sensitive
machine learning applications: Ô¨Ånancial resource alloca-
tion, business marketing and criminal sentencing. The Adult
dataset is a census dataset, the original prediction task of
which is to determine whether a person makes over 50K a
year. The Bank dataset is a classiÔ¨Åcation dataset, the goal of
which is to predict if the client will subscribe a term deposit.
TheCompas dataset is a classiÔ¨Åcation dataset used to predict
whether a criminal defendant will re-offend. We provide
detailed statistics about these three datasets in Table 1.
Table 1. Dataset statistics.
Adult Bank Compas MEPS
# of instance 48842 45211 5278 15830
# of attributes 18 16 10 138
Area Ô¨Ånance marketing crime medical
Please refer to the following links to access the three datasets
tested.
‚Ä¢Adult . Description in AIF360: https:
//github.com/Trusted-AI/AIF360/tree/
master/aif360/data/raw/adult
‚Ä¢Bank . Description in AIF360: https:
//github.com/Trusted-AI/AIF360/tree/
master/aif360/data/raw/bank
‚Ä¢Compas . Description in AIF360: https:
//github.com/Trusted-AI/AIF360/tree/
master/aif360/data/raw/compas
‚Ä¢MEPS . Description in AIF360: https:
//github.com/Trusted-AI/AIF360/tree/
master/aif360/data/raw/meps

--- PAGE 14 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
B A DDITIONAL EVALUATION RESULTS
B.1 Comparison with a post-hoc approach
In this section, we compare FairFLAML with another
straightforward approach which is inspired by the analy-
sis in Section 3. This approach primarily treats unfairness
mitigation as a disjoint component post to AutoML, i.e., one
Ô¨Årst Ô¨Ånishes the AutoML task and then performs unfairness
mitigation on the conÔ¨Ågurations tried during AutoML in
ascending order of the loss.
However, there are pitfalls in this naive approach. (1) When
the correlation between the loss before and after unfairness
mitigation is weak, this approach may waste a lot of time
doing unnecessary unfairness mitigation. Even if the corre-
lation is strong, the relationship between the losses before
and after mitigation is typically non-monotonic. (2) To re-
main an end-to-end solution, the system still needs to decide
when to stop the original AutoML process and apply the
mitigation operation, which is non-trivial especially if we
want to apply the mitigation on multiple models. If this
stopping time is not set properly, the system may not be
able to produce a single fair model. To verify our argument,
we constructed a version of this approach in the follow-
ing way: given a particular resource budget, the method
spends the Ô¨Årst half of the budget doing hyperparameter
search, and the second half of the budget doing unfairness
mitigation. We name this method MPost , and compare it
with FairFLAML with different resource levels in Figure 7.
The results veriÔ¨Åed our arguments that the performance
ofMPost is sensitive to the resource budget. In general,
MPost requires a sufÔ¨Åciently large resource budget.
B.2 Results on more unfairness mitigation methods
We include the results obtained with two additional un-
fairness mitigation methods, including Grid Search reduc-
tion and threshold-based post-processing, in Figure 8 and
Figure 9, where we use FairFLAML-g andFairFLAML-p
to denote Grid Search reduction and the post-processing
method respectively. From these results, we observe con-
sistently good performance from FairFLAML similar to the
case with Exponentiated Gradient as reported in Figure 2
in the main paper. By comparing FairFLAML with differ-
ent unfairness mitigation methods, we Ô¨Ånd Exponentiated
Gradient works the best overall and thus used it as the de-
fault unfairness mitigation method in FairFLAML if not
otherwise speciÔ¨Åed.
B.3 Extensibility
Our proposed framework and system are highly extensible
and are compatible with a wide range of hyperparameter
searchers, fairness deÔ¨Ånitions, and unfairness mitigation
methods. We include code examples to verify the widecompatibility in our codebase (instructions are provided in
the ‚ÄòREADME.md‚Äô Ô¨Åle of the submitted code base).
Compatibility with pre-processing mitigation methods.
In addition to the two in-processing mitigation methods,
and one post-processing mitigation method evaluated, our
system is also compatible with pre-processing mitigation
methods. We conÔ¨Årmed the compatibility with one pre-
processing method (Feldman et al., 2015).
Compatibility with other hyperparameter searchers.
Our framework is compatible with any hyperparameter
searcher that satisÔ¨Åes the abstraction developed in Section 4.
We conÔ¨Årmed our framework‚Äôs and system‚Äôs compatibility
with another two state-of-the-art searchers: Optuna (Akiba
et al., 2019), and BlendSearch (Wang et al., 2021a). The
former is a Bayesian optimization hyperparameter searcher,
and the latter is a hyperparameter searcher which combines
global search and local search.
Compatibility with fairness deÔ¨Ånitions and mitigation
methods for regression tasks. All the empirical evalua-
tions reported in this work are from binary classiÔ¨Åcation
tasks. For regression tasks, different fairness deÔ¨Ånitions
and mitigation methods are needed. We conÔ¨Årmed Fair-
FLAML‚Äôs compatibility with one quantitative fairness def-
inition, bounded group loss (Agarwal et al., 2019), and
a reduction-based unfairness mitigation method (Agarwal
et al., 2019) for regression tasks.
C S OCIAL IMPACT AND LIMITATIONS
Social Impact. In many real-world applications where
the decisions to be made have a direct impact on the well-
being of human beings, it is not sufÔ¨Åcient to only have
high prediction accuracy. We also expect the ML-based
decisions to be ethical that do not put certain unprivileged
groups or individuals at systematic disadvantages. One
example of such human-centered applications where ma-
chine learning is heavily used is modern Ô¨Ånancial services,
including Ô¨Ånancial organizations‚Äô activities such as credit
scoring, lending and etc. According to a survey of UK regu-
lators in 2019 (BankofEngland, 10, 2019), two-thirds of UK
Ô¨Ånancial industry participants rely on AI today to make de-
cisions. Moreover, an Economist Intelligence Unit research
report (John, 2020) found that 86% of Ô¨Ånancial services
executives plan on increasing their AI-related investments
through 2025. There has been increasing evidence showing
various unfairness issues associated with machine-made de-
cisions or predictions (O‚Äôneil, 2016; Barocas & Selbst, 2016;
Angwin et al., 2016). Laws and regulations, for example the
U.S. Fair Credit Reporting Act (FCRA) and Equal Credit
Opportunity Act (ECOA), have been enforced to prohibit
unfairness and discrimination in related Ô¨Ånancial activities.

--- PAGE 15 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.210 0.215 0.220 0.225 0.230 0.235
Fair Loss0.00.20.40.60.8Cumulative Proportion
FairFLAML MAlways MasHP FLAML020406080100120140160Percentage of resouce used 0.2090.2090.2090.213Wasted Mitigation
Effective Mitigation
HPO
0.19400 0.19425 0.19450 0.19475 0.19500 0.19525 0.19550
Loss0.20.40.60.81.0Cumulative proportion
(a) Fair loss under low resource budget, resource consumption breakdown, and loss on the Adult dataset
0.1300 0.1325 0.1350 0.1375 0.1400 0.1425 0.1450
Fair Loss0.10.20.30.40.50.60.7Cumulative Proportion
FairFLAML MAlways MasHP FLAML020406080100120
0.1310.133 0.134
0.131
0.128 0.129 0.130 0.131 0.132 0.133 0.134
Loss0.20.40.60.81.0Cumulative proportion
(b) Fair loss, resource consumption breakdown, and loss on the MEPS dataset
Figure 6. Fair loss, resource consumption breakdown, and loss without considering fairness constraints on different datasets.
At the same time, AutoML is playing an increasingly impact-
ful role in modern machine learning lifecycles. According
to a recent report (Aut, 2021) from ReportLinker, the Au-
toML market is predicted to reach $14;830:8million by
2030, demonstrating a compound annual growth rate of
45.6%from 2020 to 2030. Despite the growing importance
of AutoML in modern ML lifecycles, the mitigation of un-
fairness in AutoML is under-explored. Our work makes
the Ô¨Årst attempt to introduce unfairness mitigation into the
AutoML pipeline. The Ô¨Çexible framework allows integra-
tion of most existing unfairness mitigation techniques. We
believe this can greatly facilitate consideration of fairness
issues in AutoML practices.
Limitations and future work. There are currently three
major limitations of our work and we plan to address some
of them in our future work.
1.The proposed fair AutoML system FairFLAML is ad-
vantageous to an alternative (e.g., MAlways) when the
computation overhead of the unfairness mitigation proce-
dure is high. When this overhead is marginal compared
to the model training time, the carefully designed re-
source allocation strategy in FairFLAML won‚Äôt make
much difference (mainly because the room for resource-
saving is small). We emphasize that this limitation does
not fundamentally undermine the contribution of thiswork: (a) in such an ‚Äòeasy‚Äô case, our system is no worse
than MAlways; (b) the ‚Äòhard‚Äô cases investigated in this
paper are non-negligible because of the in-processing
mitigation methods‚Äô good empirical performance and
theoretical guarantee (Agarwal et al., 2018).
2.In this work, we are not providing theoretical guarantees
on the beneÔ¨Åt of FairFLAML in terms of the resource-
saving for reaching a particular target fair loss or the best
fair loss can be achieved given any resource budget. Such
theoretical guarantees can potentially be developed using
competitive analysis. We plan to investigate the theoret-
ical properties of our proposed FairFLAML system in
future work.
3.Although the proposed framework and system are pre-
sumably compatible with any unfairness mitigation sat-
isfying the developed abstractions, we only evaluated a
few state-of-the-art ones in this work. We plan to keep
track of the cutting-edge research development in fair-
ness deÔ¨Ånition and unfairness mitigation and potentially
explore more variants of them. We also plan to further
expand the framework such that multiple different types
of unfairness mitigation methods can be automatically
selected and used in combination when necessary.

--- PAGE 16 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.215 0.220 0.225 0.230 0.235
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
FairFLAML
MPost
0.210 0.215 0.220 0.225 0.230 0.235
Fair Loss0.10.20.30.40.50.60.7Cumulative Proportion
Fairness metric=EO
0.210 0.215 0.220 0.225 0.230 0.235
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.215 0.220 0.225 0.230 0.235
Fair Loss0.30.40.50.60.7Cumulative Proportion
Fairness threshold=0.01
(a) Wall-clock-time = 300s
0.215 0.220 0.225 0.230 0.235
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
FairFLAML
MPost
0.210 0.215 0.220 0.225 0.230 0.235
Fair Loss0.10.20.30.40.50.60.7Cumulative Proportion
Fairness metric=EO
0.208 0.209 0.210 0.211 0.212 0.213 0.214 0.215 0.216
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.215 0.220 0.225 0.230 0.235
Fair Loss0.10.20.30.40.50.60.7Cumulative Proportion
Fairness threshold=0.01
(b) Wall-clock-time = 600s
Figure 7. Comparisons of FairFLAML with MPost on the Adult dataset under different resource budgets when tuning XGBoost.

--- PAGE 17 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
FairFLAML-g
FLAML
FairBO
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.01
(a) Fair loss on Adult
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.10.20.30.40.50.60.70.80.9Cumulative Proportion
Fairness threshold=0.01
(b) Fair loss on Bank
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness metric=DP
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness metric=EO
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness threshold=0.05
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness threshold=0.01
(c) Fair loss on Compas
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.01
(d) Fair loss on MEPS
Figure 8. Fair loss with grid search reduction as the unfairness mitigation method when tuning XGBoost.

--- PAGE 18 ---
FairAutoML: Embracing Unfairness Mitigation in AutoML
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
FairFLAML-p
FLAML
FairBO
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.210 0.215 0.220 0.225 0.230
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.01
(a) Fair loss on Adult
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.0900 0.0925 0.0950 0.0975 0.1000 0.1025 0.1050
Fair Loss0.10.20.30.40.50.60.70.80.9Cumulative Proportion
Fairness threshold=0.01
(b) Fair loss on Bank
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness metric=DP
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness metric=EO
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness threshold=0.05
0.32 0.33 0.34 0.35 0.36 0.37
Fair loss0.20.40.60.81.0Cumulative Proportion
fairness threshold=0.01
(c) Fair loss on Compas
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=DP
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness metric=EO
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.05
0.125 0.130 0.135 0.140 0.145 0.150
Fair Loss0.20.40.60.81.0Cumulative Proportion
Fairness threshold=0.01
(d) Fair loss on MEPS
Figure 9. Fair loss with threshold-based post-processing as the unfairness mitigation method when tuning XGBoost.
