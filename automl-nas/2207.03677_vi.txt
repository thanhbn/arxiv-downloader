# 2207.03677.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2207.03677.pdf
# Kích thước tập tin: 19579077 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================

--- TRANG 1 ---
SuperTickets: Rút Ra Những Vé Số May Mắn Bất Khả Tri Tác Vụ
Từ Supernets Thông Qua Việc Tìm Kiếm Kiến Trúc Kết Hợp
Và Cắt Tỉa Tham Số
Haoran You1⋆, Baopu Li3, Zhanyi Sun2, Xu Ouyang2, và Yingyan Lin1
1Georgia Institute of Technology2Rice University3Oracle Health and AI
{hyou37, celine.lin }@gatech.edu {zs19,xo2 }@rice.edu ,baopu.li@oracle.com
Tóm tắt. Tìm kiếm kiến trúc mạng nơ-ron (NAS) đã thể hiện thành công
đáng kinh ngạc trong việc tìm kiếm các mạng nơ-ron sâu (DNNs) hiệu quả
từ một supernet cho trước. Đồng thời, giả thuyết vé số may mắn đã chỉ ra
rằng DNNs chứa các mạng con nhỏ có thể được huấn luyện từ đầu để đạt
được độ chính xác tương đương hoặc thậm chí cao hơn so với DNNs gốc.
Như vậy, hiện tại một thực hành phổ biến là phát triển DNNs hiệu quả thông
qua một quy trình đầu tiên tìm kiếm sau đó cắt tỉa. Tuy nhiên, làm như vậy
thường yêu cầu một quá trình tẻ nhạt và tốn kém của tìm kiếm-huấn luyện-
cắt tỉa-huấn luyện lại và do đó chi phí tính toán cấm đoán. Trong bài báo
này, chúng tôi khám phá lần đầu tiên rằng cả DNNs hiệu quả và các mạng
con vé số của chúng (tức là, vé số may mắn) có thể được xác định trực tiếp
từ một supernet, mà chúng tôi gọi là SuperTickets, thông qua một lược đồ
huấn luyện hai-trong-một với việc tìm kiếm kiến trúc kết hợp và cắt tỉa tham
số. Hơn nữa, chúng tôi phát triển một chiến lược xác định SuperTickets tiến
bộ và thống nhất cho phép khả năng kết nối của các mạng con thay đổi trong
quá trình huấn luyện supernet, đạt được sự cân bằng độ chính xác và hiệu
quả tốt hơn so với huấn luyện thưa thông thường. Cuối cùng, chúng tôi đánh
giá liệu các SuperTickets được xác định như vậy được rút ra từ một tác vụ có
thể chuyển giao tốt sang các tác vụ khác hay không, xác thực tiềm năng của
chúng trong việc xử lý đồng thời nhiều tác vụ. Các thí nghiệm rộng rãi và
nghiên cứu loại bỏ trên ba tác vụ và bốn bộ dữ liệu chuẩn xác nhận rằng
SuperTickets được đề xuất của chúng tôi đạt được sự cân bằng độ chính xác
và hiệu quả được tăng cường hơn so với cả quy trình NAS và cắt tỉa điển
hình, bất kể có huấn luyện lại hay không. Mã nguồn và các mô hình được
huấn luyện trước có sẵn tại \textcolor{blue}{https://github.com/RICE-EIC/SuperTickets}.
Từ khóa: Giả Thuyết Vé Số May Mắn, Huấn Luyện/Suy Luận Hiệu Quả,
Tìm Kiếm Kiến Trúc Mạng Nơ-ron, DNNs Bất Khả Tri Tác Vụ
1 Giới thiệu
Trong khi các mạng nơ-ron sâu (DNNs) đã đạt được hiệu suất chưa từng có
trong các tác vụ và ứng dụng khác nhau như phân loại, phân đoạn, và phát
hiện [8], chi phí huấn luyện và suy luận cấm đoán của chúng hạn chế việc triển
khai trên các thiết bị có nguồn lực hạn chế để có trí tuệ phổ biến hơn. Ví dụ,
một lượt truyền xuôi của ResNet50 [16] yêu cầu 4 GFLOPs (FLOPs: phép toán
điểm nổi) và quá trình huấn luyện của nó yêu cầu 1018FLOPs [49]. Để thu hẹp
khoảng cách nói trên, các nỗ lực rộng rãi đã được thực hiện để nén DNNs từ
cấp độ kiến trúc vĩ mô (ví dụ, NAS [37,44,8]) hoặc tham số chi tiết (ví dụ, cắt
tỉa mạng [15,11]). Một quy trình nén DNN được áp dụng phổ biến theo nguyên
tắc thô đến mịn là đầu tiên tự động tìm kiếm các kiến trúc DNN hiệu quả và
mạnh mẽ từ một supernet lớn hơn và sau đó cắt tỉa các DNNs được tìm kiếm
thông qua quá trình huấn luyện-cắt tỉa-huấn luyện lại tốn kém [10,9,21] để rút
ra các mạng con nhỏ hơn và thưa hơn với độ chính xác tương đương hoặc giảm
nhưng chi phí suy luận giảm đáng kể. Tuy nhiên, quy trình như vậy yêu cầu một
quá trình tìm kiếm-huấn luyện-cắt tỉa-huấn luyện lại tẻ nhạt và do đó vẫn có
chi phí huấn luyện cấm đoán.

Để giải quyết hạn chế trên nhằm đơn giản hóa quy trình và cải thiện hơn nữa
sự cân bằng độ chính xác-hiệu quả của các mạng được xác định, chúng tôi ủng
hộ một khung huấn luyện hai-trong-một để đồng thời xác định cả DNNs hiệu
quả và các mạng con vé số của chúng thông qua việc tìm kiếm kiến trúc kết hợp
và cắt tỉa tham số. Chúng tôi gọi các mạng con nhỏ được xác định là SuperTickets
nếu chúng đạt được sự cân bằng độ chính xác-hiệu quả tương đương hoặc thậm
chí vượt trội hơn so với các đường cơ sở tìm kiếm-sau-đó-cắt tỉa đã được áp
dụng trước đây, bởi vì chúng được rút ra từ supernets và đại diện cho cả kiến
trúc DNN thô và mạng con DNN mịn. Chúng tôi thực hiện những nỗ lực không
tầm thường để khám phá và xác thực tiềm năng của SuperTickets bằng cách trả
lời ba câu hỏi chính: (1) liệu các SuperTickets như vậy có thể được tìm thấy trực
tiếp từ một supernet thông qua huấn luyện hai-trong-một hay không? Nếu có,
thì (2) làm thế nào để xác định hiệu quả các SuperTickets như vậy? và (3) liệu
SuperTickets được tìm thấy từ một tác vụ/bộ dữ liệu có thể chuyển giao sang
tác vụ khác, tức là, có tiềm năng xử lý các tác vụ/bộ dữ liệu khác nhau? Theo
hiểu biết tốt nhất của chúng tôi, đây là nỗ lực đầu tiên được thực hiện hướng
tới việc xác định cả kiến trúc DNN và các mạng con vé số tương ứng của chúng
thông qua một lược đồ huấn luyện hai-trong-một thống nhất. Các đóng góp của
chúng tôi có thể được tóm tắt như sau:

• Chúng tôi lần đầu tiên khám phá rằng các kiến trúc DNN hiệu quả và các
mạng con vé số của chúng, tức là, SuperTickets, có thể được xác định đồng
thời từ một supernet dẫn đến sự cân bằng độ chính xác-hiệu quả vượt trội.

• Chúng tôi phát triển một chiến lược xác định tiến bộ thống nhất để tìm hiệu
quả các SuperTickets thông qua một lược đồ huấn luyện hai-trong-một cho
phép các mạng con tái kích hoạt lặp đi lặp lại các kết nối đã cắt tỉa trong
quá trình huấn luyện, mang lại hiệu suất tốt hơn so với huấn luyện thưa
thông thường. Đáng chú ý, các SuperTickets được xác định của chúng tôi
mà không cần huấn luyện lại đã vượt trội hơn các đường cơ sở tìm kiếm-
trước-sau-đó-cắt tỉa đã được áp dụng trước đây, và do đó có thể được triển
khai trực tiếp.

• Chúng tôi xác thực khả năng chuyển giao của các SuperTickets được xác
định qua các tác vụ/bộ dữ liệu khác nhau, và thực hiện các thí nghiệm rộng
rãi để so sánh các SuperTickets được đề xuất với những cái từ các đường
cơ sở tìm kiếm-sau-đó-cắt tỉa hiện có, các kỹ thuật NAS điển hình, và các
công trình cắt tỉa. Kết quả trên ba tác vụ và bốn bộ dữ liệu chứng minh sự
cân bằng độ chính xác-hiệu quả vượt trội một cách nhất quán và khả năng
chuyển giao đầy hứa hẹn để xử lý các tác vụ khác nhau được cung cấp bởi
SuperTickets.

--- TRANG 2 ---
2 H. You et al.
các thiết bị có nguồn lực hạn chế để có trí tuệ phổ biến hơn. Ví dụ, một
lượt truyền xuôi của ResNet50 [16] yêu cầu 4 GFLOPs (FLOPs: phép toán
điểm nổi) và quá trình huấn luyện của nó yêu cầu 1018FLOPs [49]. Để thu hẹp
khoảng cách nói trên, các nỗ lực rộng rãi đã được thực hiện để nén DNNs từ
cấp độ kiến trúc vĩ mô (ví dụ, NAS [37,44,8]) hoặc tham số chi tiết (ví dụ, cắt
tỉa mạng [15,11]). Một quy trình nén DNN được áp dụng phổ biến theo nguyên
tắc thô đến mịn là đầu tiên tự động tìm kiếm các kiến trúc DNN hiệu quả và
mạnh mẽ từ một supernet lớn hơn và sau đó cắt tỉa các DNNs được tìm kiếm
thông qua quá trình huấn luyện-cắt tỉa-huấn luyện lại tốn kém [10,9,21] để rút
ra các mạng con nhỏ hơn và thưa hơn với độ chính xác tương đương hoặc giảm
nhưng chi phí suy luận giảm đáng kể. Tuy nhiên, quy trình như vậy yêu cầu một
quá trình tìm kiếm-huấn luyện-cắt tỉa-huấn luyện lại tẻ nhạt và do đó vẫn có
chi phí huấn luyện cấm đoán.

Để giải quyết hạn chế trên nhằm đơn giản hóa quy trình và cải thiện hơn nữa
sự cân bằng độ chính xác-hiệu quả của các mạng được xác định, chúng tôi ủng
hộ một khung huấn luyện hai-trong-một để đồng thời xác định cả DNNs hiệu
quả và các mạng con vé số của chúng thông qua việc tìm kiếm kiến trúc kết hợp
và cắt tỉa tham số. Chúng tôi gọi các mạng con nhỏ được xác định là SuperTickets
nếu chúng đạt được sự cân bằng độ chính xác-hiệu quả tương đương hoặc thậm
chí vượt trội hơn so với các đường cơ sở tìm kiếm-sau-đó-cắt tỉa đã được áp
dụng trước đây, bởi vì chúng được rút ra từ supernets và đại diện cho cả kiến
trúc DNN thô và mạng con DNN mịn. Chúng tôi thực hiện những nỗ lực không
tầm thường để khám phá và xác thực tiềm năng của SuperTickets bằng cách trả
lời ba câu hỏi chính: (1) liệu các SuperTickets như vậy có thể được tìm thấy trực
tiếp từ một supernet thông qua huấn luyện hai-trong-một hay không? Nếu có,
thì (2) làm thế nào để xác định hiệu quả các SuperTickets như vậy? và (3) liệu
SuperTickets được tìm thấy từ một tác vụ/bộ dữ liệu có thể chuyển giao sang
tác vụ khác, tức là, có tiềm năng xử lý các tác vụ/bộ dữ liệu khác nhau? Theo
hiểu biết tốt nhất của chúng tôi, đây là nỗ lực đầu tiên được thực hiện hướng
tới việc xác định cả kiến trúc DNN và các mạng con vé số tương ứng của chúng
thông qua một lược đồ huấn luyện hai-trong-một thống nhất. Các đóng góp của
chúng tôi có thể được tóm tắt như sau:

•Chúng tôi lần đầu tiên khám phá rằng các kiến trúc DNN hiệu quả và các
mạng con vé số của chúng, tức là, SuperTickets, có thể được xác định đồng
thời từ một supernet dẫn đến sự cân bằng độ chính xác-hiệu quả vượt trội.

•Chúng tôi phát triển một chiến lược xác định tiến bộ thống nhất để tìm hiệu
quả các SuperTickets thông qua một lược đồ huấn luyện hai-trong-một cho
phép các mạng con tái kích hoạt lặp đi lặp lại các kết nối đã cắt tỉa trong
quá trình huấn luyện, mang lại hiệu suất tốt hơn so với huấn luyện thưa
thông thường. Đáng chú ý, các SuperTickets được xác định của chúng tôi
mà không cần huấn luyện lại đã vượt trội hơn các đường cơ sở tìm kiếm-
trước-sau-đó-cắt tỉa đã được áp dụng trước đây, và do đó có thể được triển
khai trực tiếp.

•Chúng tôi xác thực khả năng chuyển giao của các SuperTickets được xác
định qua các tác vụ/bộ dữ liệu khác nhau, và thực hiện các thí nghiệm rộng
rãi để so sánh các SuperTickets được đề xuất với những cái từ các đường
cơ sở tìm kiếm-sau-đó-cắt tỉa hiện có, các kỹ thuật NAS điển hình, và các
công trình cắt tỉa. Kết quả trên ba tác vụ và bốn bộ dữ liệu chứng minh sự
cân bằng độ chính xác-hiệu quả vượt trội một cách nhất quán và khả năng
chuyển giao đầy hứa hẹn để xử lý các tác vụ khác nhau được cung cấp bởi
SuperTickets.

--- TRANG 3 ---
SuperTickets 3
2 Các Công Trình Liên Quan
Tìm Kiếm Kiến Trúc Mạng Nơ-ron (NAS). NAS đã đạt được thành công
đáng kinh ngạc trong việc tự động hóa thiết kế các kiến trúc DNN hiệu quả và
tăng cường sự cân bằng độ chính xác-hiệu quả [56,38,17]. Để tìm kiếm DNNs
đặc thù cho tác vụ, các công trình ban đầu [38,37,17] áp dụng các phương pháp
dựa trên học tăng cường đòi hỏi thời gian tìm kiếm cấm đoán và tài nguyên tính
toán, trong khi các công trình gần đây [24,44,40,47] cập nhật cả trọng số và kiến
trúc trong quá trình huấn luyện supernet thông qua tìm kiếm có thể vi phân có
thể cải thiện đáng kể hiệu quả tìm kiếm so với các công trình NAS trước đó.
Gần đây hơn, một số công trình áp dụng NAS một lần [14,3,52,41] để tách rời
việc tìm kiếm kiến trúc khỏi huấn luyện supernet. Các phương pháp như vậy
thường áp dụng được để tìm kiếm CNNs hiệu quả [14,2] hoặc Transformers
[42,4,36] để giải quyết cả các tác vụ thị giác và ngôn ngữ. Để tìm kiếm DNNs
đa tác vụ, các công trình mới nổi gần đây như HR-NAS [8] và FBNetv5 [45] ủng
hộ các thiết kế supernet với các nhánh đa độ phân giải để đáp ứng cả phân loại
hình ảnh và các tác vụ dự đoán dày đặc khác đòi hỏi biểu diễn độ phân giải cao.
Trong công trình này, chúng tôi đề xuất trực tiếp tìm kiếm không chỉ DNNs hiệu
quả mà còn các mạng con vé số của chúng từ supernets để đạt được sự cân bằng
độ chính xác-hiệu quả tốt hơn trong khi có thể xử lý các tác vụ khác nhau.

Giả Thuyết Vé Số May Mắn (LTH). Frankle et al. [11,12] đã chỉ ra rằng
các vé chiến thắng (tức là, các mạng con nhỏ) tồn tại trong các mạng dày đặc
được khởi tạo ngẫu nhiên, có thể được huấn luyện lại để khôi phục độ chính xác
tương đương hoặc thậm chí tốt hơn so với các đối tác mạng dày đặc của chúng.
Phát hiện này đã truyền cảm hứng cho nhiều hướng nghiên cứu vì nó ngụ ý
tiềm năng của các mạng con thưa. Để huấn luyện hiệu quả, You et al. [49] nhất
quán tìm thấy các vé chiến thắng ở giai đoạn huấn luyện sớm, giảm đáng kể chi
phí huấn luyện của DNNs. Phát hiện như vậy đã được mở rộng đến các mô
hình ngôn ngữ (ví dụ, BERT) [5], các mô hình sinh (ví dụ, GAN) [31], và các
mạng nơ-ron đồ thị [50]; Zhang et al. [54] nhận biết các vé chiến thắng hiệu
quả hơn bằng cách huấn luyện chỉ với một tập con được chọn đặc biệt của dữ
liệu; và Ramanujan et al. [33] tiếp tục xác định các vé chiến thắng trực tiếp từ
khởi tạo ngẫu nhiên hoạt động tốt ngay cả khi không huấn luyện lại. Ngược
lại, mục tiêu của chúng tôi là đồng thời tìm cả DNNs hiệu quả và các mạng con
vé số của chúng từ supernets, vượt ra ngoài phạm vi của huấn luyện thưa hoặc
rút các vé chiến thắng từ các mô hình DNN dày đặc.

Thiết Kế DNNs Bất Khả Tri Tác Vụ. Để tạo điều kiện thiết kế DNNs cho
các tác vụ khác nhau, các công trình gần đây [25,17,43] đề xuất thiết kế các
xương sống kiến trúc chung cho các tác vụ thị giác máy tính khác nhau. Ví dụ,
HR-Net [43] duy trì các biểu diễn độ phân giải cao xuyên suốt toàn bộ mạng để
hỗ trợ các tác vụ dự đoán dày đặc, thay vì kết nối các phép tích chập độ phân
giải cao đến thấp theo chuỗi như ResNet hoặc VGGNet; Swin-Transformer [25]
áp dụng các transformer thị giác phân cấp để phục vụ như một xương sống đa
mục đích tương thích với một phạm vi rộng các tác vụ thị giác; ViLBERT [27,28]
đề xuất một mô hình hai luồng đa phương thức để học các biểu diễn chung bất
khả tri tác vụ của cả hình ảnh và ngôn ngữ; Data2vec [1] thiết kế một khung
chung cho học tự giám sát trong lời nói, thị giác và ngôn ngữ. Hơn nữa, các
công trình gần đây [8,45,46] cũng tận dụng NAS để tự động tìm kiếm DNNs
bất khả tri tác vụ và hiệu quả từ các supernets được thiết kế thủ công. Trong
công trình này, chúng tôi mục tiêu xác định các SuperTickets bất khả tri tác vụ
đạt được sự cân bằng độ chính xác-hiệu quả tốt hơn.

--- TRANG 4 ---
4 H. You et al.
crafted supernets. Trong công trình này, chúng tôi mục tiêu xác định các SuperTickets
bất khả tri tác vụ đạt được sự cân bằng độ chính xác-hiệu quả tốt hơn.

3 Phương Pháp SuperTickets Được Đề Xuất
Trong phần này, chúng tôi giải quyết ba câu hỏi chính của SuperTickets. Đầu tiên,
chúng tôi phát triển một lược đồ huấn luyện hai-trong-một để xác thực giả thuyết
của chúng tôi rằng SuperTickets tồn tại và có thể được tìm thấy trực tiếp từ một
supernet. Thứ hai, chúng tôi tiếp tục khám phá các chiến lược xác định SuperTickets
hiệu quả hơn thông qua tái kích hoạt nơ-ron lặp đi lặp lại và cắt tỉa tiến bộ, tăng
cường đáng kể sự cân bằng độ chính xác-hiệu quả. Thứ ba, chúng tôi đánh giá khả
năng chuyển giao của các SuperTickets được xác định qua các bộ dữ liệu hoặc tác
vụ khác nhau, xác thực tiềm năng của chúng trong việc trở thành bất khả tri tác vụ.

3.1 SuperTickets Có Tồn Tại Trong Supernets Không?
Giả Thuyết SuperTickets. Chúng tôi giả thuyết rằng cả kiến trúc DNN hiệu
quả và các mạng con vé số của chúng có thể được xác định trực tiếp từ một
supernet, và gọi các mạng con này là SuperTickets nếu chúng đạt được sự cân
bằng độ chính xác-hiệu quả ngang bằng hoặc thậm chí tốt hơn so với những
đối tác tìm kiếm-trước-sau-đó-cắt tỉa. Xem xét một supernet f(x;θS), các kiến
trúc DNN khác nhau a được lấy mẫu từ nó có trọng số được biểu diễn bởi θS(a),
sau đó chúng ta có thể định nghĩa SuperTickets như f(x;m⊙θS(a)), trong đó
m∈ {0,1} là một mặt nạ để chỉ ra các kết nối đã cắt tỉa và chưa cắt tỉa trong
các DNNs được tìm kiếm. Giả Thuyết SuperTickets ngụ ý rằng tối ưu hóa đồng
thời các kiến trúc DNN a và các mặt nạ thưa tương ứng m hoạt động tốt hơn,
tức là, dẫn đến sự cân bằng độ chính xác-hiệu quả vượt trội, so với tối ưu hóa
chúng một cách tuần tự.

Cài Đặt Thí Nghiệm. Để thực hiện các thí nghiệm khám phá liệu SuperTickets
có tồn tại chung hay không, chúng tôi cần (1) một supernet phù hợp có tính đến
cả các khối xây dựng hiệu quả cổ điển và các nguyên tắc thiết kế DNN bất khả
tri tác vụ và (2) các tác vụ, bộ dữ liệu, và số liệu tương ứng. Chúng tôi trình
bày chi tiết các cài đặt của chúng tôi dưới đây. NAS và Supernets: Chúng tôi
xem xét một không gian tìm kiếm đa nhánh chứa cả các khối xây dựng tích chập
và chú ý hiệu quả theo một công trình tiên tiến (SOTA) của HR-NAS [8], có
không gian tìm kiếm đa độ phân giải phân cấp độc đáo để xử lý nhiều tác vụ
thị giác nổi bật so với những cái khác. Nói chung, nó chứa hai đường dẫn:
MixConv [39] và Transformer nhẹ để trích xuất cả thông tin ngữ cảnh cục bộ
và toàn cục. Cả số kênh tích chập với các kích thước kernel khác nhau và số
token trong Transformer đều là các tham số có thể tìm kiếm. Tác Vụ, Bộ Dữ
Liệu, và Số Liệu: Chúng tôi xem xét phân đoạn ngữ nghĩa trên Cityscapes [6]
và ước lượng tư thế con người trên COCO keypoint [22] như hai tác vụ đại diện
cho mục đích minh họa. Đối với Cityscapes, mean Intersection over Union
(mIoU), mean Accuracy (mAcc), và overall Accuracy (aAcc) là các số liệu đánh
giá. Đối với COCO keypoint, chúng tôi huấn luyện mô hình sử dụng kích thước
đầu vào 256 ×192, tốc độ học ban đầu là 1e-3, kích thước batch là 384 trong
210 epochs. Các số liệu đánh giá là average precision (AP), recall scores (AR),
APM và APL cho các đối tượng trung bình hoặc lớn. Tất cả thí nghiệm được
chạy trên Tesla V100*8 GPUs.

--- TRANG 5 ---
SuperTickets 5
Thuật toán 1: Khung Hai-Trong-Một để Xác Định SuperTickets.
Đầu vào: Trọng số supernet θS, ngưỡng bỏ ϵ, và tỷ lệ cắt tỉa p;
Đầu ra: DNNs hiệu quả và các mạng con vé số của chúng f(x;m⊙θS(a)).
1while t(epoch) < tmaxdo
2 t=t+ 1;
3 Cập nhật trọng số θS và hệ số quan trọng r sử dụng huấn luyện SGD;
4 iftmod ts= 0then ▷Tìm kiếm DNNs
5 Loại bỏ các đơn vị tìm kiếm có hệ số quan trọng r < ϵ ;
6 Tái hiệu chỉnh thống kê chạy của các lớp BN để thu được subnet a;
// Nếu bật kỹ thuật tái kích hoạt lặp đi lặp lại
7 Tái kích hoạt gradient của các trọng số đã cắt tỉa;
8 else if tmod tp= 0then ▷Cắt tỉa cho mạng con
// Nếu bật kỹ thuật cắt tỉa tiến bộ
9 Định nghĩa lại tỷ lệ cắt tỉa là min {p,10%× ⌊t/tp⌋};
10 Thực hiện cắt tỉa dựa trên độ lớn hướng tới tỷ lệ mục tiêu;
11 Giữ mặt nạ thưa mt và tắt gradient của các trọng số đã cắt tỉa;
12 end
13end
14return f(x;mt⊙θS(a)); ▷SuperTickets

DNNs Hiệu Quả
…
Tìm Kiếm Cắt Tỉa Mạng Con Có/Không Huấn Luyện Lại Huấn Luyện Supenet
Huấn Luyện Supenet
 SuperTickets
Hai-Trong-Một(a) Quy trình tìm kiếm-trước-sau-đó-cắt tỉa (S+P)
(b) Huấn Luyện Hai-Trong-Một (Được Đề Xuất)

Hình 1. Minh họa tìm kiếm-trước-sau-đó-cắt
tỉa (S+P) so với huấn luyện hai-trong-một của
chúng tôi. Huấn Luyện Hai-Trong-Một.
Để xác thực giả thuyết SuperTickets,
chúng tôi đề xuất một thuật toán huấn
luyện hai-trong-một đồng thời tìm kiếm
và cắt tỉa trong quá trình huấn luyện
supernet của NAS. Như được hiển thị
trong Thuật toán 1 và Hình 1, để tìm
kiếm DNNs hiệu quả, chúng tôi áp dụng
một NAS thu nhỏ tiến bộ bằng cách
dần dần loại bỏ các đơn vị tìm kiếm
không quan trọng có thể là các kênh
tích chập hoặc token Transformer.
Sau mỗi ps epoch huấn luyện, chúng
tôi sẽ phát hiện và loại bỏ các đơn vị
tìm kiếm không quan trọng khi các hệ
số quan trọng tương ứng r (tức là, các
tỷ lệ trong các lớp Batch Normalization (BN)) nhỏ hơn ngưỡng bỏ được định
trước ϵ. Lưu ý rằng r có thể được học kết hợp với trọng số supernet, việc loại
bỏ như vậy sẽ không ảnh hưởng đến các đơn vị tìm kiếm còn lại vì các kênh
trong tích chập theo chiều sâu độc lập với nhau, như cũng được xác thực bởi
[8,30]. Ngoài ra, chúng tôi theo network slimming [26] để thêm một phạt l1 như
một thuật ngữ điều chuẩn để phân cực các hệ số quan trọng nhằm tạo điều kiện
phát hiện các đơn vị không quan trọng. Sau khi loại bỏ chúng, các thống kê
chạy trong các lớp BN được tái hiệu chỉnh để phù hợp với kiến trúc DNN được
tìm kiếm a để tránh covariate shift [19,48]. Để cắt tỉa các DNNs được tìm kiếm,
chúng tôi thực

--- TRANG 6 ---
6 H. You et al.
107108109
FLOPs020406080mIoU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
107108109
FLOPs020406080mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
107108109
FLOPs020406080100aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One

Hình 2. So sánh mIoU, mAcc, aAcc và FLOPs suy luận của các mạng kết quả từ
huấn luyện hai-trong-một được đề xuất và các đường cơ sở tìm kiếm-trước-sau-đó-cắt
tỉa (S+P) trên tác vụ phân đoạn ngữ nghĩa và bộ dữ liệu Cityscapes, trong đó Rand.,
Mag., và Grad. đại diện cho cắt tỉa ngẫu nhiên, độ lớn, và dựa trên gradient, tương
ứng. Lưu ý rằng mỗi phương pháp có một chuỗi điểm để đại diện cho các tỷ lệ cắt
tỉa khác nhau từ 10% đến 98%. Tất cả độ chính xác được lấy trung bình qua ba lần chạy.

0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060AP (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APM (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APL (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One

Hình 3. So sánh AP, APM, APL và FLOPs suy luận của các mạng kết quả từ huấn
luyện hai-trong-một được đề xuất và các đường cơ sở trên tác vụ ước lượng tư thế con
người và bộ dữ liệu COCO keypoint. Mỗi phương pháp có một chuỗi điểm để đại
diện cho các tỷ lệ cắt tỉa khác nhau từ 10% đến 98%. Tất cả độ chính xác được lấy
trung bình qua ba lần chạy.

hiện cắt tỉa dựa trên độ lớn hướng tới tỷ lệ cắt tỉa cho trước mỗi tp epochs,
mặt nạ thưa được tạo ra mt sẽ được giữ để tắt luồng gradient của các trọng số
đã cắt tỉa trong quá trình huấn luyện tiếp theo. Lưu ý rằng chúng tôi không
kết hợp các kỹ thuật tái kích hoạt lặp đi lặp lại và cắt tỉa tiến bộ (được tô sáng
bằng màu/bóng trong Thuật toán 1, sẽ được trình bày chi tiết sau) như hiện tại.
Thuật toán huấn luyện hai-trong-một đơn giản như vậy có thể được coi là bước
đầu tiên hướng tới việc trả lời câu đố liệu SuperTickets có tồn tại chung hay không.

Sự Tồn Tại của SuperTickets. Chúng tôi so sánh huấn luyện hai-trong-một
được đề xuất với các đường cơ sở tìm kiếm-trước-sau-đó-cắt tỉa (S+P) và báo
cáo kết quả trên Cityscapes và COCO keypoint ở Hình 2 và Hình 3, tương ứng.
Chúng ta thấy rằng huấn luyện hai-trong-một được đề xuất nhất quán tạo ra
sự cân bằng độ chính xác-hiệu quả tương đương hoặc thậm chí tốt hơn so với
S+P với các tiêu chí cắt tỉa khác nhau (ngẫu nhiên, độ lớn, và gradient) vì các
phương pháp của chúng tôi thể hiện hiệu suất tốt hơn nhiều về phân đoạn hoặc
ước lượng tư thế con người dưới các mức giảm FLOPs khác nhau như được hiển
thị trong hai hình trên, chỉ ra rằng SuperTickets thường tồn tại trong một supernet
và có tiềm năng lớn để vượt trội hơn các phương pháp được áp dụng thông
thường, tức là, tối ưu hóa tuần tự các kiến trúc DNN và mặt nạ thưa.

--- TRANG 7 ---
SuperTickets 7
Bảng 1. Phân tích chi tiết của chiến lược xác định SuperTickets được đề xuất. Chúng
tôi báo cáo hiệu suất của các mạng con được tìm thấy dưới mức thưa 90%/80% trên
hai bộ dữ liệu.

Phương Pháp 2-in-1 PP IR-P IR-S Retrain Cityscapes COCO Keypoint
mIoU mAcc aAcc AP APM APL AR
S+P (Mag.) 42.12 50.49 87.45 5.04 4.69 5.89 10.67
S+P (Mag.) ! 51.03 59.61 90.88 48.63 46.82 51.74 53.38
Ours ! 55.84 67.38 92.97 58.38 56.68 61.26 62.23
Ours ! ! 63.89 73.56 94.17 60.14 57.93 63.70 63.79
Ours ! ! ! 45.73 55.52 89.36 5.48 7.43 4.36 10.85
Ours ! ! ! 66.61 76.30 94.63 61.02 58.80 64.64 64.78
Ours ! ! ! ! 67.17 77.03 94.73 61.48 59.30 65.19 65.20

3.2 Làm Thế Nào Để Xác Định SuperTickets Hiệu Quả Hơn?
Chúng tôi đã xác thực sự tồn tại của SuperTickets, câu hỏi tự nhiên tiếp theo
là làm thế nào để xác định chúng hiệu quả hơn. Để làm điều này, chúng tôi đề
xuất hai kỹ thuật có thể được kết hợp một cách liền mạch vào khung huấn luyện
hai-trong-một để xác định SuperTickets hiệu quả hơn và tăng cường hơn nữa
hiệu suất có thể đạt được của chúng.

Cắt Tỉa Tiến Bộ (PP). Mặc dù việc đồng thời tìm kiếm và cắt tỉa trong quá
trình huấn luyện supernet cho phép cơ hội hợp tác giữa việc loại bỏ đơn vị tìm
kiếm thô và cắt tỉa trọng số mịn, tức là, NAS giúp tinh chỉnh các mạng đã cắt
tỉa như một sự bù đắp bằng cách loại bỏ các đơn vị bị cắt tỉa quá mức để tránh
các lớp thắt cổ chai, chúng tôi phát hiện rằng việc cắt tỉa quá mức ở giai đoạn
huấn luyện sớm không thể tránh khỏi làm tổn hại khả năng tổng quát hóa của
mạng, và tiếp tục đề xuất một kỹ thuật cắt tỉa tiến bộ (PP) để khắc phục nhược
điểm này. Như được tô sáng trong phần màu xanh của Thuật toán 1, tỷ lệ cắt
tỉa được định nghĩa là min {p,10%× ⌊t/tp⌋}, có nghĩa là độ thưa của mạng sẽ
dần dần tăng từ 10% đến tỷ lệ mục tiêu p, theo 10% mỗi tp epochs. Kỹ thuật
PP giúp tránh hiệu quả việc cắt tỉa quá mức ở giai đoạn huấn luyện sớm và do
đó tăng cường đáng kể hiệu suất cuối cùng. Như được chứng minh trong Bảng
1, huấn luyện hai-trong-một với PP đạt được cải thiện 8.05%/6.18%/1.2%
mIoU/mAcc/aAcc và 1.76%/1.25%/2.44%/1.56% AP/APM/APL/AR trên các
bộ dữ liệu Cityscapes và COCO keypoint, tương ứng, so với huấn luyện hai-
trong-một đơn giản dưới mức thưa 90%.

Tái Kích Hoạt Lặp Đi Lặp Lại (IR). Một vấn đề khác trong khung hai-
trong-một là các trọng số đã cắt tỉa sẽ không bao giờ nhận được cập nhật gradient
trong suốt quá trình huấn luyện còn lại. Để tăng cường hơn nữa hiệu suất,
chúng tôi thiết kế một chiến lược tái kích hoạt lặp đi lặp lại (IR) để tạo điều
kiện cho việc xác định SuperTickets hiệu quả bằng cách cho phép khả năng kết
nối của các mạng con thay đổi trong quá trình huấn luyện supernet. Cụ thể,
chúng tôi tái kích hoạt gradient của các trọng số đã cắt tỉa như được tô sáng
trong phần màu cam của Thuật toán 1. Lưu ý rằng chúng tôi tái kích hoạt trong
quá trình tìm kiếm thay vì ngay sau khi cắt tỉa, dựa trên giả thuyết rằng huấn
luyện thưa cũng quan trọng đối với khung huấn luyện hai-trong-một. Trong
thực tế, khoảng thời gian cắt tỉa pt khác với khoảng thời gian tìm kiếm ps để
cho phép một khoảng thời gian huấn luyện thưa. Để xác thực giả thuyết, chúng
tôi thiết kế hai biến thể: IR-S và IR-P tái kích hoạt gradient của các trọng số
đã cắt tỉa trong quá trình tìm kiếm và cắt tỉa, tương ứng, và hiển thị các so
sánh trong Bảng 1. Chúng tôi quan sát rằng: (1) IR-P dẫn đến độ chính xác
thậm chí tệ hơn so với huấn luyện hai-trong-một đơn giản, xác thực rằng huấn
luyện thưa là cần thiết; (2) IR-S tiếp tục dẫn đến cải thiện 2.72%/2.74%/0.46%
mIoU/mAcc/aAcc và 0.88%/0.87%/0.94%/0.99% AP/APM/ APL/AR trên
Cityscapes và COCO keypoint, tương ứng, trên cơ sở huấn luyện hai-trong-một
với PP.

SuperTickets Có/Không Huấn Luyện Lại. Vì quá trình huấn luyện supernet,
tìm kiếm kiến trúc, và cắt tỉa trọng số được thực hiện theo cách thống nhất đầu
cuối đến cuối, các SuperTickets kết quả có thể được triển khai trực tiếp mà
không cần huấn luyện lại, đạt được sự cân bằng độ chính xác-hiệu quả tốt hơn
so với các đường cơ sở S+P (thậm chí với huấn luyện lại) như được chỉ ra
trong Bảng 1. Để điều tra liệu huấn luyện lại có thể tăng cường hơn nữa hiệu
suất hay không, chúng tôi huấn luyện lại các SuperTickets được tìm thấy trong
50 epochs khác và báo cáo kết quả ở Bảng 1. Chúng ta thấy rằng huấn luyện
lại tiếp tục dẫn đến cải thiện 0.56%/0.73%/0.10% mIoU/mAcc/aAcc và
0.46%/0.50%/0.55%/0.42% AP/APM/ APL/AR trên các bộ dữ liệu Cityscapes
và COCO keypoint, tương ứng.

3.3 Các SuperTickets Được Xác Định Có Thể Chuyển Giao Không?
Để xác thực tiềm năng của các SuperTickets được xác định để xử lý các tác vụ
và bộ dữ liệu khác nhau, chúng tôi cung cấp các thí nghiệm và phân tích thực
nghiệm như sau. Lưu ý rằng chúng tôi điều chỉnh bộ phân loại cuối để phù hợp
với các bộ dữ liệu mục tiêu trong quá trình học chuyển giao.

Bảng 2. Các bài kiểm tra xác thực chuyển giao
Supertickets dưới mức thưa 90%.

Phương Pháp Params FLOPs Cityscapes
mIoU mAcc aAcc
S+P (Grad.) 0.13M 203M 8.41 12.39 56.77
S+P (Mag.) 0.13M 203M 42.12 50.49 87.45
S+P (Mag.) w/ RT 0.13M 203M 60.76 70.40 93.38
ADE20K Tickets 0.20M 247M 62.91 73.32 93.82
ImageNet Tickets 0.18M 294M 61.64 71.78 93.75

Phương Pháp Params FLOPs ADE20K
mIoU mAcc aAcc
S+P (Grad.) 0.11M 154M 0.79 1.50 25.58
S+P (Mag.) 0.11M 154M 3.37 4.70 39.47
Cityscapes Tickets 0.13M 119M 20.83 29.95 69.00
ImageNet Tickets 0.21M 189M 22.42 31.87 70.21

SuperTickets Chuyển Giao Giữa
Các Bộ Dữ Liệu. Chúng tôi đầu tiên
kiểm tra khả năng chuyển giao của
các SuperTickets được xác định giữa
các bộ dữ liệu khác nhau trong cùng
một tác vụ, tức là, Cityscapes và
ADE20K như hai đại diện trong tác
vụ phân đoạn ngữ nghĩa. Bảng 2 cho
thấy rằng SuperTickets được xác định
từ một bộ dữ liệu có thể chuyển giao
sang bộ dữ liệu khác trong khi dẫn
đến hiệu suất tương đương hoặc thậm
chí tốt hơn so với các đường cơ sở
S+P có (được ký hiệu là "w/ RT")
hoặc không có huấn luyện lại (mặc
định). Ví dụ, khi được kiểm tra trên
Cityscapes, SuperTickets được xác
định từ ADE20K sau khi tinh chỉnh
dẫn đến mIoU cao hơn 2.2% và 20.8%
so với các đường cơ sở S+P (Mag.)
w/ và w/o RT được huấn luyện trực tiếp trên bộ dữ liệu Cityscapes mục tiêu.
Tương tự, các SuperTickets được chuyển giao từ Cityscapes sang ADE20K cũng
vượt trội hơn các đường cơ sở trên bộ dữ liệu mục tiêu.

SuperTickets Chuyển Giao Giữa Các Tác Vụ. Để tiếp tục điều tra liệu các
SuperTickets được xác định có thể chuyển giao giữa các tác vụ khác nhau hay
không. Chúng tôi xem xét việc chuyển giao các mô-đun trích xuất đặc trưng của
SuperTickets được xác định từ ImageNet trên tác vụ phân loại sang Cityscapes
và ADE20K trên các tác vụ phân đoạn, trong đó các đầu dự đoán dày đặc và
bộ phân loại cuối vẫn được thừa kế từ các bộ dữ liệu mục tiêu. Kết quả được
trình bày trong hàng cuối của hai bảng con trong Bảng

--- TRANG 8 ---
8 H. You et al.
2. Chúng tôi quan sát rằng các mạng được chuyển giao như vậy vẫn hoạt động
tốt trên các tác vụ hạ nguồn. Đôi khi, nó thậm chí đạt được hiệu suất tốt hơn
so với việc chuyển giao trong một tác vụ, ví dụ, ImageNet →ADE20K hoạt động
tốt hơn (mIoU cao hơn 1.6%) so với Cityscapes →ADE20K. Chúng tôi cung cấp
thêm thí nghiệm trên các tỷ lệ cắt tỉa khác nhau trong Phần 4.3.2.

4 Kết Quả Thí Nghiệm
4.1 Cài Đặt Thí Nghiệm
Tác Vụ, Bộ Dữ Liệu, và Supernets. Tác Vụ và Bộ Dữ Liệu. Chúng tôi xem
xét bốn bộ dữ liệu chuẩn và ba tác vụ thị giác đại diện để chứng minh hiệu quả
của SuperTickets, bao gồm phân loại hình ảnh trên bộ dữ liệu ImageNet [7] với
1.2 triệu hình ảnh huấn luyện và 50K hình ảnh xác thực; phân đoạn ngữ nghĩa
trên các bộ dữ liệu Cityscapes [6] và ADE20K [55] với 2975/500/1525 và
20K/2K/3K hình ảnh cho huấn luyện, xác thực, và kiểm tra, tương ứng; ước
lượng tư thế con người trên bộ dữ liệu COCO keypoint [22] với 57K hình ảnh
và 150K thể hiện người cho huấn luyện, và 5K hình ảnh cho xác thực. Các bộ
dữ liệu được chọn này yêu cầu các trường tiếp nhận khác nhau và ngữ cảnh
toàn cục/cục bộ, thể hiện bản thân như các bàn kiểm tra phù hợp cho SuperTickets
trên nhiều tác vụ. Supernets. Đối với tất cả thí nghiệm, chúng tôi áp dụng cùng
một supernet như HR-NAS [8] nhờ thiết kế supernet đa độ phân giải bất khả
tri tác vụ. Nó bắt đầu với hai tích chập 3 ×3 với stride 2, được theo sau bởi
năm mô-đun song song để dần dần chia nó thành bốn nhánh có độ phân giải
giảm dần, các đặc trưng đã học từ tất cả các nhánh sau đó được hợp nhất với
nhau để phân loại hoặc dự đoán dày đặc.

Cài Đặt Tìm Kiếm và Huấn Luyện. Để huấn luyện supernets trên ImageNet,
chúng tôi áp dụng một trình tối ưu RMSProp với momentum 0.9 và weight
decay 1e-5, exponential moving average (EMA) với decay 0.9999, và exponential
learning rate decay với tốc độ học ban đầu 0.016 và kích thước batch 256 trong
350 epochs. Đối với Cityscapes và ADE20K, chúng tôi sử dụng một trình tối
ưu AdamW, tốc độ học ban đầu 0.04 với kích thước batch 32 do kích thước
hình ảnh đầu vào lớn hơn, và huấn luyện trong 430 và 200 epochs, tương ứng,
theo [8]. Đối với COCO keypoint, chúng tôi theo [43] để sử dụng một trình tối
ưu Adam trong 210 epochs, tốc độ học ban đầu được đặt thành 1e-3, và được
chia cho 10 ở epoch thứ 170 và thứ 200, tương ứng. Ngoài ra, chúng tôi thực
hiện tìm kiếm kiến trúc trong quá trình huấn luyện supernet. Đối với tất cả
các đơn vị tìm kiếm, chúng tôi sử dụng các tỷ lệ từ các lớp BN đính kèm của
chúng như các hệ số quan trọng r; các đơn vị tìm kiếm với r <0.001 được coi
là không quan trọng và được loại bỏ mỗi 10 epochs (tức là, ts= 10); Tương
ứng, cắt tỉa dựa trên độ lớn sẽ được thực hiện mỗi 25 epochs cho ImageNet
và Cityscapes, hoặc mỗi 15 epochs cho ADE20K và COCO keypoint (tức là,
tp= 25/15), dẫn đến các khoảng thời gian cho huấn luyện thưa như trong Phần 3.2.

Đường Cơ Sở và Số Liệu Đánh Giá. Đường Cơ Sở. Đối với tất cả thí nghiệm,
chúng tôi xem xét quy trình S+P như một trong những đường cơ sở của chúng
tôi, trong đó phương pháp tìm kiếm theo [8]; các phương pháp cắt tỉa có thể
được chọn từ cắt tỉa ngẫu nhiên, cắt tỉa độ lớn [15,11], và cắt tỉa gradient [20].
Ngoài ra, chúng tôi cũng đánh giá với các DNNs được thiết kế thủ công, ví dụ,
ShuffleNet [53,29] và MobiletNetV2 [34], và các

--- TRANG 9 ---
9 H. You et al.
0.5 1.0 1.5 2.0
FLOPs 1e8020406080Top-1 Acc. (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
0.5 1.0 1.5 2.0
FLOPs 1e8020406080100Top-5 Acc. (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT

Hình 4. So sánh độ chính xác top-1/5 và FLOPs của SuperTickets được đề xuất và
các đường cơ sở S+P trên ImageNet. Mỗi phương pháp có một chuỗi điểm để đại
diện cho các tỷ lệ cắt tỉa khác nhau từ 10% đến 98%. Tất cả độ chính xác được lấy
trung bình qua ba lần chạy. Chúng tôi cũng đánh giá tất cả các phương pháp với
huấn luyện lại (được ký hiệu là w/ RT).

DNNs đặc thù cho tác vụ kết quả từ NAS điển hình trước đó, ví dụ, MobileNetV3
[17] và Auto-DeepLab [23]. Chúng tôi không so sánh với các công trình NAS/tickets
có độ chính xác SOTA do các mục tiêu và cài đặt thí nghiệm khác nhau. Tất cả
đường cơ sở được đánh giá dưới FLOPs hoặc độ chính xác tương tự để so sánh
công bằng. Số Liệu Đánh Giá. Chúng tôi đánh giá SuperTickets và tất cả đường
cơ sở về mặt sự cân bằng độ chính xác-hiệu quả. Cụ thể, các số liệu độ chính
xác đề cập đến độ chính xác top-1/5 cho các tác vụ phân loại; mIoU, mAcc, và
aAcc cho các tác vụ phân đoạn; AP, AR, APM, và APL cho các tác vụ ước lượng
tư thế con người. Đối với các số liệu hiệu quả, chúng tôi đánh giá và so sánh
cả số lượng tham số và FLOPs suy luận.

4.2 Đánh Giá SuperTickets So Với Các Đường Cơ Sở Điển Hình
4.2.1 SuperTickets Trên Tác Vụ Phân Loại
Bảng 3. SuperTickets so với một số phương
pháp điển hình trên ImageNet. FLOPs được
đo với kích thước đầu vào 224 ×224.

Mô Hình Params FLOPs Top-1 Acc.
CondenseNet [18] 2.9M 274M 71.0%
ShuffleNetV1 [53] 3.4M 292M 71.5%
ShuffleNetV2 [29] 3.5M 299M 72.6%
MobileNetV2 [34] 3.4M 300M 72.0%
FBNet [44] 4.5M 295M 74.1%
S+P (Grad.) 2.7M 114M 64.3%
S+P (Mag.) 2.7M 114M 72.8%
SuperTickets 2.7M 125M 74.2%

Chúng tôi hiển thị các so sánh tổng
thể giữa SuperTickets và một số
đường cơ sở điển hình về mặt sự
cân bằng độ chính xác-hiệu quả trong
Hình 4 và Bảng 3, từ đó chúng tôi
có hai quan sát. Thứ nhất, SuperTickets
nhất quán vượt trội hơn tất cả đường
cơ sở bằng cách giảm FLOPs suy
luận trong khi đạt được độ chính
xác tương đương hoặc thậm chí tốt
hơn (+0.1% ∼ +4.6%) so với cả
S+P và một số DNNs đặc thù cho
tác vụ; Tương tự, khi so sánh dưới
số lượng tham số hoặc FLOPs tương
đương, SuperTickets dẫn đến trung
bình 26.5% (lên đến

--- TRANG 10 ---
10 H. You et al.
108109
FLOPs020406080mIoU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs020406080mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs020406080100aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT

(a) So sánh SuperTickets với các đường cơ sở S+P trên Cityscapes.

108109
FLOPs0102030mIOU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs01020304050mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT
108109
FLOPs020406080aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
S+P (Grad.) w/ RT
SuperTickets w/ RT

(b) So sánh SuperTickets với các đường cơ sở S+P trên ADE20K.

Hình 5. So sánh mIoU, mAcc, aAcc và FLOPs suy luận của SuperTickets được đề
xuất và các đường cơ sở S+P trên các bộ dữ liệu Cityscapes và ADE20K. Mỗi phương
pháp có một chuỗi điểm để đại diện cho các tỷ lệ cắt tỉa khác nhau từ 10% đến 98%.

64.5%) và trung bình 41.3% (lên đến 71.9%) cải thiện độ chính xác top-1 so
với S+P (Mag.) và S+P (Grad.) qua các tỷ lệ cắt tỉa khác nhau, ví dụ, dưới
tỷ lệ cắt tỉa 50%, SuperTickets đạt được 74.2% độ chính xác top-1, +1.4% và
+9.9% so với S+P (Mag.) và S+P (Grad.), tương ứng. Thứ hai, SuperTickets
w/o huấn luyện lại thậm chí vượt trội hơn các đường cơ sở S+P có huấn luyện
lại như được chứng minh trong Hình 4, dẫn đến trung bình 6.7% (lên đến
29.2%) độ chính xác top-1 cao hơn dưới FLOPs tương đương qua các tỷ lệ cắt
tỉa khác nhau (10% ∼98%). Hơn nữa, SuperTickets w/ huấn luyện lại đạt được
0.1% ∼31.9% (trung bình 5.3%) độ chính xác cao hơn so với các đối tác w/o
huấn luyện lại, đẩy xa hơn ranh giới của sự cân bằng độ chính xác-hiệu quả.

Bảng 4. SuperTickets so với một số phương
pháp điển hình trên Cityscapes. FLOPs được
đo với kích thước đầu vào 512 ×1024.

Mô Hình Params FLOPs mIoU
BiSeNet [51] 5.8M 6.6G 69.00%
MobileNetV3 [17] 1.5M 2.5G 72.36%
ShuffleNetV2 [29] 3.0M 6.9G 71.30%
Auto-DeepLab [23] 3.2M 27.3G 71.21%
SqueezeNAS [35] 0.73M 8.4G 72.40%
S+P (Grad.) w/ RT 0.63M 1.0G 60.66%
S+P (Mag.) w/ RT 0.63M 1.0G 72.31%
SuperTickets 0.63M 1.0G 72.68%

4.2.2 SuperTickets Trên Tác Vụ Phân
Đoạn
Thí Nghiệm Trên Cityscapes. Chúng
tôi so sánh SuperTickets với các đường
cơ sở điển hình trên Cityscapes như
được hiển thị trong Hình 5 (a) và
Bảng 4. Chúng ta thấy rằng SuperTickets
nhất quán vượt trội hơn tất cả đường
cơ sở về mặt mIoU/mAcc/aAcc và
FLOPs. Cụ thể, SuperTickets giảm
60% ∼ 80.86% FLOPs trong khi cung
cấp mIoU tương đương hoặc tốt hơn
(0.28 % ∼43.26%) so với cả S+P và
DNNs đặc thù cho tác vụ; Tương tự,
khi so sánh

--- TRANG 11 ---
11 H. You et al.
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060AP (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
SuperTickets w/ RT
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APM (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
SuperTickets w/ RT
0.0 0.5 1.0 1.5 2.0 2.5
FLOPs 1e80204060APL (%)
S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
SuperTickets
S+P (Mag.) w/ RT
SuperTickets w/ RT

Hình 6. So sánh AP, APM, APL và FLOPs suy luận của SuperTickets được đề xuất
và các đường cơ sở trên tác vụ ước lượng tư thế con người và bộ dữ liệu COCO
keypoint. Mỗi phương pháp có một chuỗi điểm để đại diện cho các tỷ lệ cắt tỉa
khác nhau từ 10% đến 98%. Tất cả độ chính xác được lấy trung bình qua ba lần chạy.

dưới số lượng tham số hoặc FLOPs tương đương, SuperTickets dẫn đến trung
bình 17.70% (lên đến 42.86%) và 33.36% (lên đến 58.05%) cải thiện mIoU so
với S+P (Mag.) và S+P (Grad.) qua các tỷ lệ cắt tỉa khác nhau, ví dụ, dưới
tỷ lệ cắt tỉa 50%, SuperTickets đạt được 72.68% mIoU, +0.37% và +12% so
với S+P (Mag.) và S+P (Grad.), tương ứng. Chúng tôi cũng báo cáo so sánh
giữa các phương pháp sau huấn luyện lại ở Hình 5, được ký hiệu bằng "w/
RT". Chúng tôi thấy rằng S+P (Grad.) w/ RT bị overfitting và thậm chí dẫn
đến hiệu suất tệ hơn; Ngược lại, SuperTickets w/ huấn luyện lại tiếp tục đạt
được 0.51% ∼ 1.64% độ chính xác cao hơn so với các đối tác w/o huấn luyện
lại, đẩy xa hơn ranh giới của sự cân bằng độ chính xác-hiệu quả.

Bảng 5. SuperTickets so với các phương pháp
điển hình trên ADE20K. FLOPs được đo với
kích thước đầu vào 512 ×512.

Mô Hình Params FLOPs mIoU
MobileNetV2 [34] 2.2M 2.8G 32.04%
MobileNetV3 [17] 1.6M 1.3G 32.31%
S+P (Grad.) 1.0M 0.8G 24.14%
S+P (Mag.) 1.0M 0.8G 31.59%
SuperTickets 1.0M 0.8G 32.54%

Thí Nghiệm Trên ADE20K.
Tương tự, chúng tôi kiểm tra tính
ưu việt của SuperTickets trên ADE20K
như được hiển thị trong Hình 5 (b)
và Bảng 5. SuperTickets được đề
xuất nhất quán vượt trội hơn tất cả
đường cơ sở về mặt sự cân bằng độ
chính xác-hiệu quả, giảm 38.46% ∼
48.53% FLOPs khi so sánh dưới mIoU
tương tự. Khi so sánh dưới số lượng
tham số hoặc FLOPs tương đương,
SuperTickets dẫn đến trung bình
9.43% (lên đến 22.6%) và 14.17%
(lên đến 27.61%) cải thiện mIoU so với S+P (Mag.) và S+P (Grad.), tương ứng,
qua các tỷ lệ cắt tỉa khác nhau. Ngoài ra, SuperTickets w/ huấn luyện lại tiếp
tục đạt được 0.01% ∼5.3% độ chính xác cao hơn so với các đối tác w/o huấn
luyện lại trên ADE20K.

4.2.3 SuperTickets Trên Tác Vụ Ước Lượng Tư Thế Con Người
Chúng tôi so sánh SuperTickets với một vài đường cơ sở điển hình trên COCO
keypoint như được hiển thị trong Hình 6 và Bảng 6. Chúng ta thấy rằng SuperTickets
nhất quán vượt trội hơn

--- TRANG 12 ---
12 H. You et al.
107108109
FLOPs020406080mIoU (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Two-in-One w/ IR
Two-in-One w/ IR & PP
107108109
FLOPs020406080mAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Two-in-One w/ IR
Two-in-One w/ IR & PP
107108109
FLOPs020406080100aAcc (%)S+P (Rand.)
S+P (Mag.)
S+P (Grad.)
Two-in-One
Two-in-One w/ IR
Two-in-One w/ IR & PP

Hình 7. Nghiên cứu loại bỏ của SuperTickets được xác định từ khung hai-trong-một
có/không có các kỹ thuật kích hoạt lặp đi lặp lại (IR) và cắt tỉa tiến bộ (PP) được đề xuất.

Bảng 6. SuperTickets so với các thuật toán
điển hình trên COCO. FLOPs được đo với
kích thước đầu vào 256×192.

Mô Hình Params FLOPs AP APM APL AR
ShuffleNetV1 [53] 1.0M 0.16G 58.5 55.2 64.6 65.1
ShuffleNetV2 [29] 1.3M 0.17G 59.8 56.5 66.2 66.4
MobileNetV2 [34] 2.3M 0.33G 64.6 61.0 71.1 70.7
S+P (Mag.) 0.6M 0.23G 63.4 61.2 66.8 67.3
SuperTickets 0.6M 0.23G 65.4 63.4 69.0 68.9

tất cả các đường cơ sở liên quan về
mặt AP/APM/APL/AR và FLOPs.
Cụ thể, SuperTickets giảm 30.3% ∼
78.1% FLOPs trong khi cung cấp AP
tương đương hoặc tốt hơn (+0.8% ∼
11.79%) so với cả S+P và DNNs đặc
thù cho tác vụ; Tương tự, khi so sánh
dưới số lượng tham số hoặc FLOPs
tương đương, SuperTickets dẫn đến
trung bình 17.4% (lên đến 55.9%)
cải thiện AP. Ngoài ra, SuperTickets
w/ huấn luyện lại tiếp tục đạt được
trung bình 1.1% độ chính xác cao hơn so với các đối tác w/o huấn luyện lại
trên COCO keypoint.

4.3 Nghiên Cứu Loại Bỏ Của SuperTickets Được Đề Xuất
4.3.1 Nghiên Cứu Loại Bỏ Về Xác Định SuperTickets
Chúng tôi cung cấp các nghiên cứu loại bỏ toàn diện để hiển thị sự phân chia
lợi ích của khung huấn luyện hai-trong-một được đề xuất và các kỹ thuật xác
định hiệu quả hơn, tức là, cắt tỉa tiến bộ (PP) và tái kích hoạt lặp đi lặp lại
(IR). Như được hiển thị trong Hình 7, chúng tôi báo cáo sự cân bằng mIoU-
FLOPs hoàn chỉnh với các tỷ lệ cắt tỉa khác nhau từ 10% đến 99% khi kiểm
tra trên bộ dữ liệu Cityscapes, trong đó trục x được biểu diễn bằng tỷ lệ log
để nhấn mạnh các cải thiện khi tỷ lệ cắt tỉa đạt mức cao. So với S+P (Mag.),
SuperTickets được xác định từ khung hai-trong-một đơn giản đạt được lên đến
40.17% giảm FLOPs khi so sánh dưới mIoU tương tự, hoặc lên đến 13.72%
cải thiện độ chính xác khi so sánh dưới FLOPs tương tự; Áp dụng IR trong
quá trình huấn luyện hai-trong-một tiếp tục dẫn đến lên đến 68.32% giảm FLOPs
hoặc lên đến 39.12% cải thiện mIoU; Trên cơ sở những điều trên, áp dụng cả
IR và PP trong quá trình huấn luyện hai-trong-một cung cấp lên đến 80.86%
giảm FLOPs hoặc lên đến 43.26% cải thiện mIoU. Tập hợp thí nghiệm này xác
thực hiệu quả của khung hai-trong-một chung và từng kỹ thuật được đề xuất.

--- TRANG 13 ---
13 H. You et al.
80 90 95 98
Pruning Ratio (%)0510152025mIOU (%)CityscapesADE20K
S+P (Mag.)
S+P (Grad.)
Transferred Tickets
80 90 95 98
Pruning Ratio (%)0204060mIOU (%)ADE20KCityscapes
80 90 95 98
Pruning Ratio (%)0510152025mIOU (%)ImageNetADE20K
80 90 95 98
Pruning Ratio (%)0204060mIOU (%)ImageNetCityscapes
80 90 95 98
Pruning Ratio (%)010203040AP (%)ImageNetCOCO

Hình 8. Nghiên cứu loại bỏ về việc chuyển giao SuperTickets được xác định từ một
bộ dữ liệu/tác vụ sang bộ dữ liệu/tác vụ khác dưới các tỷ lệ cắt tỉa khác nhau từ 80% đến 98%.

4.3.2 Nghiên Cứu Loại Bỏ Về Khả Năng Chuyển Giao Của SuperTickets
Chúng tôi trước đây sử dụng một tập thí nghiệm dưới mức thưa 90% trong
Phần 3.3 để xác thực rằng các SuperTickets được xác định có thể chuyển giao tốt.
Trong phần này, chúng tôi cung cấp thêm các thí nghiệm loại bỏ toàn diện dưới
các tỷ lệ cắt tỉa khác nhau và giữa một số bộ dữ liệu/tác vụ. Như được hiển thị
trong Hình 8, hai biểu đồ con bên trái chỉ ra việc chuyển giao giữa các bộ dữ
liệu khác nhau (Cityscapes ↔ADE20K) thường hoạt động qua bốn tỷ lệ cắt tỉa.
Đặc biệt, các SuperTickets được chuyển giao dẫn đến 76.14% ∼81.35% giảm
FLOPs so với đường cơ sở S+P cạnh tranh nhất, trong khi cung cấp mIoU tương
đương (0.27% ∼1.85%). Hơn nữa, ba biểu đồ con bên phải xác thực rằng các
SuperTickets được xác định từ tác vụ phân loại có thể chuyển giao tốt sang các
tác vụ khác (tức là, phân đoạn và ước lượng tư thế con người). Cụ thể, nó dẫn
đến 68.67% ∼69.43% giảm FLOPs so với đường cơ sở S+P (Mag.), khi đạt được
mIoU hoặc AP tương đương.

5 Kết Luận
Trong bài báo này, chúng tôi ủng hộ một khung hai-trong-một trong đó cả
kiến trúc DNN hiệu quả và các mạng con vé số của chúng (tức là, SuperTickets)
có thể được xác định từ một supernet đồng thời, dẫn đến hiệu suất tốt hơn so
với các đường cơ sở tìm kiếm-trước-sau-đó-cắt tỉa. Ngoài ra, chúng tôi phát
triển hai kỹ thuật trong quá trình huấn luyện supernet để xác định hiệu quả
hơn các SuperTickets như vậy, đẩy xa hơn ranh giới của sự cân bằng độ chính
xác-hiệu quả. Hơn nữa, chúng tôi kiểm tra khả năng chuyển giao của SuperTickets
để tiết lộ tiềm năng của chúng trong việc trở thành bất khả tri tác vụ. Kết quả
trên ba tác vụ và bốn bộ dữ liệu nhất quán chứng minh tính ưu việt của khung
hai-trong-một được đề xuất và các SuperTickets kết quả, mở ra một góc nhìn
mới trong việc tìm kiếm và cắt tỉa cho các mạng chính xác và hiệu quả hơn.

Lời Cảm Ơn
Chúng tôi muốn ghi nhận sự hỗ trợ tài trợ từ tài trợ NSF NeTS (Số giải thưởng:
1801865) và tài trợ NSF SCH (Số giải thưởng: 1838873) cho dự án này.

--- TRANG 14 ---
14 H. You et al.
Tài Liệu Tham Khảo
1. Baevski, A., Hsu, W.N., Xu, Q., Babu, A., Gu, J., Auli, M.: data2vec: Một khung
chung cho học tự giám sát trong lời nói, thị giác và ngôn ngữ. arXiv
preprint arXiv:2202.03555 (2022)
2. Bender, G., Kindermans, P.J., Zoph, B., Vasudevan, V., Le, Q.: Hiểu và
đơn giản hóa tìm kiếm kiến trúc một lần. Trong: International Conference on Machine
Learning. trang 550–559. PMLR (2018)
3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-all: Huấn luyện một mạng
và chuyên biệt hóa nó để triển khai hiệu quả. arXiv preprint arXiv:1908.09791 (2019)
4. Chen, M., Peng, H., Fu, J., Ling, H.: Autoformer: Tìm kiếm transformers cho
nhận dạng thị giác. Trong: Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) (2021)
5. Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., Liu, J.: Earlybert: Huấn luyện
bert hiệu quả thông qua vé số early-bird. arXiv preprint arXiv:2101.00063 (2020)
6. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: Bộ dữ liệu cityscapes cho hiểu cảnh đô thị
ngữ nghĩa. Trong: Proceedings of the IEEE conference on computer vision and
pattern recognition. trang 3213–3223 (2016)
7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: Một cơ sở
dữ liệu hình ảnh phân cấp quy mô lớn. Trong: 2009 IEEE conference on computer vision
and pattern recognition. trang 248–255. Ieee (2009)
8. Ding, M., Lian, X., Yang, L., Wang, P., Jin, X., Lu, Z., Luo, P.: Hr-nas: Tìm
kiếm kiến trúc mạng nơ-ron độ phân giải cao hiệu quả với transformers nhẹ. Trong:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. trang 2982–2992 (2021)
9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:
Nap: Tìm kiếm kiến trúc mạng nơ-ron với cắt tỉa. Neurocomputing (2022)
10. Feng, Q., Xu, K., Li, Y., Sun, Y., Wang, D.: Cắt tỉa toàn cục một cấp theo cạnh
trên các mạng được tạo bởi nas. Trong: Chinese Conference on Pattern Recognition and
Computer Vision (PRCV). trang 3–15. Springer (2021)
11. Frankle, J., Carbin, M.: Giả thuyết vé số may mắn: Tìm các mạng nơ-ron thưa,
có thể huấn luyện. Trong: International Conference on Learning Representations (2019),
https://openreview.net/forum?id=rJl-b3RcF7
12. Frankle, J., Dziugaite, G.K., Roy, D., Carbin, M.: Kết nối chế độ tuyến tính và
giả thuyết vé số may mắn. Trong: International Conference on Machine Learning.
trang 3259–3269. PMLR (2020)
13. Gordon, M.A., Duh, K., Andrews, N.: Nén bert: Nghiên cứu các hiệu ứng của
cắt tỉa trọng số trong học chuyển giao. arXiv preprint arXiv:2002.08307 (2020)
14. Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., Sun, J.: Tìm kiếm kiến
trúc mạng nơ-ron một đường với lấy mẫu đồng nhất. Trong: European Conference
on Computer Vision. trang 544–560. Springer (2020)
15. Han, S., Mao, H., Dally, W.J.: Nén sâu: Nén mạng nơ-ron sâu với cắt tỉa,
lượng tử hóa được huấn luyện và mã hóa huffman. arXiv preprint
arXiv:1510.00149 (2015)
16. He, K., Zhang, X., Ren, S., Sun, J.: Học dư thừa sâu cho nhận dạng hình ảnh. Trong:
Proceedings of the IEEE conference on computer vision and pattern recognition.
trang 770–778 (2016), https://github.com/facebookarchive/fb.resnet.torch
17. Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu,
Y., Pang, R., Vasudevan, V., et al.: Tìm kiếm mobilenetv3. Trong: Proceedings

--- TRANG 15 ---
SuperTickets 15
của IEEE/CVF International Conference on Computer Vision. trang 1314–1324
(2019)
18. Huang, G., Liu, S., Van der Maaten, L., Weinberger, K.Q.: Condensenet: Một
densenet hiệu quả sử dụng tích chập nhóm đã học. Trong: Proceedings of the IEEE
conference on computer vision and pattern recognition. trang 2752–2761 (2018)
19. Ioffe, S., Szegedy, C.: Chuẩn hóa batch: Tăng tốc huấn luyện mạng sâu bằng
cách giảm covariate shift nội bộ. Trong: International conference on machine learning.
trang 448–456. PMLR (2015)
20. Lee, N., Ajanthan, T., Torr, P.H.: Snip: Cắt tỉa mạng một lần dựa trên
độ nhạy kết nối. arXiv preprint arXiv:1810.02340 (2018)
21. Li, Z., Yuan, G., Niu, W., Zhao, P., Li, Y., Cai, Y., Shen, X., Zhan, Z., Kong,
Z., Jin, Q., et al.: Npas: Một khung nhận biết trình biên dịch của cắt tỉa mạng thống
nhất và tìm kiếm kiến trúc cho gia tốc di động vượt thời gian thực. Trong: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. trang
14255–14266 (2021)
22. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Các đối tượng chung trong ngữ cảnh. Trong: European conference
on computer vision. trang 740–755. Springer (2014)
23. Liu, C., Chen, L.C., Schroff, F., Adam, H., Hua, W., Yuille, A.L., Fei-Fei, L.:
Auto-deeplab: Tìm kiếm kiến trúc mạng nơ-ron phân cấp cho phân đoạn hình ảnh
ngữ nghĩa. Trong: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. trang 82–92 (2019)
24. Liu, H., Simonyan, K., Yang, Y.: Darts: Tìm kiếm kiến trúc có thể vi phân. arXiv
preprint arXiv:1806.09055 (2018)
25. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer: Transformer thị giác phân cấp sử dụng cửa sổ dịch chuyển. Trong: Proceedings
of the IEEE/CVF International Conference on Computer Vision. trang 10012–10022
(2021)
26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Học các mạng tích chập
hiệu quả thông qua network slimming. Trong: Proceedings of the IEEE international conference on computer vision. trang 2736–2744 (2017)
27. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Huấn luyện trước các biểu diễn
visiolinguistic bất khả tri tác vụ cho các tác vụ thị giác và ngôn ngữ. Advances in neural information
processing systems 32(2019)
28. Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Học biểu diễn
thị giác và ngôn ngữ đa tác vụ. Trong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. trang 10437–10446 (2020)
29. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Hướng dẫn thực tế cho
thiết kế kiến trúc cnn hiệu quả. Trong: Proceedings of the European conference on
computer vision (ECCV). trang 116–131 (2018)
30. Mei, J., Li, Y., Lian, X., Jin, X., Yang, L., Yuille, A., Yang, J.: Atomnas: Tìm
kiếm kiến trúc mạng nơ-ron đầu cuối đến cuối chi tiết. arXiv preprint arXiv:1912.09640
(2019)
31. Mukund Kalibhat, N., Balaji, Y., Feizi, S.: Thắng vé số may mắn trong các mô
hình sinh sâu. arXiv e-prints trang arXiv–2010 (2020)
32. Qin, E., Samajdar, A., Kwon, H., Nadella, V., Srinivasan, S., Das, D., Kaul, B.,
Krishna, T.: Sigma: Một bộ gia tốc gemm thưa và không đều với các kết nối linh
hoạt cho huấn luyện dnn. Trong: 2020 IEEE International Symposium on High
Performance Computer Architecture (HPCA). trang 58–70. IEEE (2020)

--- TRANG 16 ---
16 H. You et al.
33. Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., Rastegari, M.: Điều gì
ẩn trong một mạng nơ-ron có trọng số ngẫu nhiên? Trong: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. trang 11893–11902 (2020)
34. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Dư
thừa nghịch đảo và thắt cổ chai tuyến tính. Trong: Proceedings of the IEEE conference on
computer vision and pattern recognition. trang 4510–4520 (2018)
35. Shaw, A., Hunter, D., Landola, F., Sidhu, S.: Squeezenas: Tìm kiếm kiến trúc
mạng nơ-ron nhanh cho phân đoạn ngữ nghĩa nhanh hơn. Trong: Proceedings of the IEEE/CVF
International Conference on Computer Vision Workshops. trang 0–0 (2019)
36. Su, X., You, S., Xie, J., Zheng, M., Wang, F., Qian, C., Zhang, C., Wang, X., Xu,
C.: Tìm kiếm kiến trúc vision transformer. arXiv preprint arXiv:2106.13700 (2021)
37. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V.:
Mnasnet: Tìm kiếm kiến trúc mạng nơ-ron nhận biết nền tảng cho di động. Trong: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. trang
2820–2828 (2019)
38. Tan, M., Le, Q.: Efficientnet: Suy nghĩ lại về việc mở rộng mô hình cho mạng nơ-ron
tích chập. Trong: International Conference on Machine Learning. trang 6105–6114. PMLR
(2019)
39. Tan, M., Le, Q.V.: Mixconv: Các kernel tích chập chiều sâu hỗn hợp. arXiv preprint
arXiv:1907.09595 (2019)
40. Wan, A., Dai, X., Zhang, P., He, Z., Tian, Y., Xie, S., Wu, B., Yu, M., Xu, T.,
Chen, K., et al.: Fbnetv2: Tìm kiếm kiến trúc mạng nơ-ron có thể vi phân cho các
chiều không gian và kênh. Trong: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. trang 12965–12974 (2020)
41. Wang, D., Gong, C., Li, M., Liu, Q., Chandra, V.: Alphanet: Cải thiện huấn luyện
supernet với alpha-divergence. arXiv preprint arXiv:2102.07954 (2021)
42. Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., Han, S.: Hat: Transformers
nhận biết phần cứng cho xử lý ngôn ngữ tự nhiên hiệu quả. arXiv preprint
arXiv:2005.14187 (2020)
43. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan,
M., Wang, X., et al.: Học biểu diễn độ phân giải cao sâu cho nhận dạng thị giác. IEEE transactions on pattern analysis and machine intelligence 43(10),
3349–3364 (2020)
44. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,
Keutzer, K.: Fbnet: Thiết kế convnet hiệu quả nhận biết phần cứng thông qua tìm kiếm
kiến trúc mạng nơ-ron có thể vi phân. Trong: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. trang 10734–10742 (2019)
45. Wu, B., Li, C., Zhang, H., Dai, X., Zhang, P., Yu, M., Wang, J., Lin, Y., Vajda, P.:
Fbnetv5: Tìm kiếm kiến trúc mạng nơ-ron cho nhiều tác vụ trong một lần chạy. arXiv preprint
arXiv:2111.10007 (2021)
46. Xu, J., Tan, X., Luo, R., Song, K., Li, J., Qin, T., Liu, T.Y.: nas-bert: nén bert
bất khả tri tác vụ và kích thước thích ứng với tìm kiếm kiến trúc mạng nơ-ron. Trong:
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining. trang 1933–1943 (2021)
47. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Hướng tới cải thiện
tính nhất quán, hiệu quả, và linh hoạt của tìm kiếm kiến trúc mạng nơ-ron có thể vi phân.
Trong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). trang 6667–6676 (June 2021)
48. You, F., Li, J., Zhao, Z.: Hiệu chỉnh thống kê batch thời gian kiểm tra cho covariate shift.
arXiv preprint arXiv:2110.04065 (2021)

--- TRANG 17 ---
SuperTickets 17
49. You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.G., Wang, Z.,
Lin, Y.: Vẽ vé early-bird: Hướng tới huấn luyện hiệu quả hơn của các mạng sâu. Trong: International Conference on Learning Representations (2020), https:
//openreview.net/forum?id=BJxsrgStvr
50. You, H., Lu, Z., Zhou, Z., Fu, Y., Lin, Y.: Early-bird gcns: Tối ưu hóa đồng thời
mạng-đồ thị hướng tới huấn luyện và suy luận gcn hiệu quả hơn thông qua vẽ vé số
early-bird. Trong: Association for the Advancement of Artificial Intelligence
(2022)
51. Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Bisenet: Mạng phân đoạn
song phương cho phân đoạn ngữ nghĩa thời gian thực. Trong: Proceedings of the European
conference on computer vision (ECCV). trang 325–341 (2018)
52. Yu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.J., Tan, M., Huang, T., Song,
X., Pang, R., Le, Q.: Bignas: Mở rộng tìm kiếm kiến trúc mạng nơ-ron với các mô
hình một giai đoạn lớn. Trong: European Conference on Computer Vision. trang 702–717. Springer
(2020)
53. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: Một mạng nơ-ron tích chập cực
kỳ hiệu quả cho các thiết bị di động. Trong: Proceedings of the IEEE conference
on computer vision and pattern recognition. trang 6848–6856 (2018)
54. Zhang, Z., Chen, X., Chen, T., Wang, Z.: Tìm vé số may mắn hiệu quả: Ít dữ liệu
là nhiều hơn. Trong: International Conference on Machine Learning. trang 12380–12390.
PMLR (2021)
55. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Phân tích cảnh
thông qua bộ dữ liệu ade20k. Trong: Proceedings of the IEEE conference on computer vision
and pattern recognition. trang 633–641 (2017)
56. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Học các kiến trúc có thể chuyển giao
cho nhận dạng hình ảnh có thể mở rộng. Trong: Proceedings of the IEEE conference on computer
vision and pattern recognition. trang 8697–8710 (2018)

--- TRANG 18 ---
SuperTickets 18
A Trực Quan Hóa Kiến Trúc Supernet Được Áp Dụng
Chúng tôi trực quan hóa supernet được áp dụng theo [8] trong Hình 9. Nó bắt
đầu với hai tích chập 3×3 với stride 2, được theo sau bởi năm mô-đun fusion
và năm mô-đun song song để dần dần chia nó thành bốn nhánh có độ phân giải
giảm dần, các đặc trưng đã học từ tất cả các nhánh sau đó được hợp nhất với
nhau để phân loại hoặc dự đoán dày đặc.

Văn Bản
Mô-đun
FusionMô-đun
FusionMô-đun
FusionMô-đun
FusionMô-đun
FusionConcatDự Đoán
Dày Đặc
Phân LoạiDự Đoán
270
144723618
2418
324
Đầu Vào
Các LớpMô-đun
Song SongMô-đun
Song SongMô-đun
Song SongMô-đun
Song SongMô-đun
Song SongĐầu Ra
Các Lớp
min= 1 
mout=1 
nsb=[1] 
nc=[18]min= 1 
mout=2 
nsb=[2,2]
nc=[18,36]min= 2 
mout=3 
nsb=[2,2,3]  
nc=[18,36,72]min= 3 
mout=4 
nsb=[2,2,3,4]  
nc=[18,36,72,144]min= 4 
mout=4 
nsb=[2,2,3,4]  
nc=[18,36,72,144]

Hình 9. Trực quan hóa kiến trúc supernet được áp dụng, trong đó min và mout biểu
thị số lượng nhánh đầu vào và đầu ra trong mô-đun fusion; nsb và nc đại diện cho
số lượng khối tìm kiếm và kênh trong mô-đun song song, tương ứng.

B SuperTickets (ST) so với Cắt Tỉa Ngẫu Nhiên (RP) và
Khởi Tạo Lại Ngẫu Nhiên (RR-Init).
Chúng tôi so sánh SuperTickets (ST) được đề xuất với cả "ST w/ RP" và "ST
w/ RR-Init" trong Bảng 7. Chúng tôi xem xét hai bộ dữ liệu dưới mức thưa
80% và 90%: ST nhất quán vượt trội hơn hai đường cơ sở, đạt được trung bình
36.28%/42.03%/21.95% và 11.20%/12.27%/4.34% cải thiện mIoU/mAcc/aAcc
so với "ST w/ RP" và "ST w/ RR-Init", tương ứng, dưới số lượng tham số và
FLOPs tương đương. Các thí nghiệm này cho thấy rằng SuperTickets hoạt động
tốt hơn so với cả RP và RR-Init, điều này phù hợp với phát hiện LTH.

Khả Năng Chuyển Giao của ST so với RP và RR-Init. Tương tự, chúng
tôi so sánh khả năng chuyển giao của ba biến thể ST khi chuyển giao chúng qua
các bộ dữ liệu khác nhau, bao gồm (1) ADE20K →Cityscapes hoặc (2) Cityscapes
→ADE20K. Như được hiển thị trong Bảng 8, ST đạt được trung bình 33.14%/39.38%/21.92%
và 11.37%/13.67%/3.20% cải thiện mIoU/mAcc/aAcc so với "ST w/ RP" và "ST w/

--- TRANG 19 ---
19 H. You et al.
Bảng 7. So sánh ST với "ST w/ RP" và "ST w/ RR-Init" trên cả Cityscapes
và ADE20K dưới mức thưa 80% và 90%.

Phương Pháp FLOPs Cityscapes ( p= 80%)
FLOPs Cityscapes ( p= 90%)
mIoU mAcc aAcc mIoU mAcc aAcc
S+P w/ RP 405M 1.30% 5.17% 21.96% 203M 1.15% 5.26% 21.9%
ST w/ RP 397M 20.17% 27.57% 68.73% 200M 16.55% 23.83% 65.90%
ST w/ RR-Init 397M 56.88% 67.33% 92.62% 200M 52.96% 62.66% 91.91%
ST 397M 69.77% 79.76% 95.12% 200M 66.61% 76.30% 94.63%

Phương Pháp FLOPs ADE20K ( p= 80%)
FLOPs ADE20K ( p= 90%)
mIoU mAcc aAcc mIoU mAcc aAcc
S+P w/ RP 308M 0.06% 0.66% 6.54% 154M 0.01% 0.66% 1.72%
ST w/ RP 317M 8.58% 11.93% 60.21% 159M 4.98% 7.19% 55.30%
ST w/ RR-Init 317M 21.24% 30.62% 68.57% 159M 19.49% 28.46% 67.26%
ST 317M 31.19% 43.10% 74.82% 159M 27.82% 39.49% 73.37%

Bảng 8. Các bài kiểm tra xác thực chuyển giao các biến thể ST dưới mức thưa 90%.

Phương Pháp ADE20K →Cityscapes
Phương Pháp Cityscapes →ADE20K
mIoU mAcc aAcc mIoU mAcc aAcc
ST w/ RP 10.51% 14.42% 61.28% ST w/ RP 6.95% 10.1% 57.7%
ST w/ RR-Init 46.19% 54.92% 90.88% ST w/ RR-Init 14.82% 21.02% 65.54%
ST 62.91% 73.32% 93.82% ST 20.83% 29.95% 69.00%

RR-Init", tương ứng, chỉ ra rằng RP và RR-Init kém hơn về khả năng chuyển
giao so với ST được đề xuất.

C Làm Rõ Các Cài Đặt LTH.
Có hai cài đặt gây nhầm lẫn khi nói về LTH: (1) trực tiếp kiểm tra độ chính
xác của cấu trúc được tìm thấy và trọng số đã huấn luyện; và (2) trọng số được
khôi phục về giá trị ban đầu của chúng và được huấn luyện với mặt nạ thu được
để có độ chính xác kiểm tra. Chúng tôi đã thử cả hai cài đặt nói trên và thấy
rằng cài đặt đầu tiên, tức là trực tiếp kiểm tra độ chính xác của cấu trúc được
tìm thấy và trọng số đã huấn luyện, đã đạt được kết quả tốt. Đây là một điểm
nổi bật khác của công trình của chúng tôi, vì nó có thể giúp tiết kiệm đáng kể
thời gian huấn luyện lại. Hơn nữa, để giải quyết mối quan tâm của bạn, chúng
tôi tái khởi tạo SuperTickets về (1) giá trị ban đầu của chúng, theo LTH gốc
("ST w/ LT-Init") và (2) giai đoạn sớm hoặc (3) muộn theo [12] ("ST w/ ELT-Init
hoặc LLT-Init"), và so sánh chúng với các đối tác RR-Init. Từ Bảng 9, chúng
ta có thể thấy rằng (1) ST dưới tất cả các cài đặt LTH đạt được độ chính xác
tốt hơn so với RR-Init, chỉ ra hiệu quả của ST; (2) LT-Init đơn giản kém hiệu
suất hơn so với cả ELT-Init và LLT-Init dưới các cài đặt ST, phù hợp với [12];
và (3) ST w/ ELT-Init hoặc LLT-Init đạt được độ chính xác tương đương hoặc
hơi tốt hơn so với ST w/o Retrain với chi phí của việc huấn luyện lại.

--- TRANG 20 ---
20 H. You et al.
Bảng 9. So sánh ST w/ các cài đặt LTH khác nhau (mức thưa 90%).

Phương Pháp Cityscapes ( p= 90%)
Phương Pháp ADE20K ( p= 90%)
mIoU mAcc aAcc mIoU mAcc aAcc
ST w/ RR-Init 52.96% 62.66% 91.91% ST w/ RR-Init 19.49% 28.46% 67.26%
ST w/ LT-Init 59.63% 70.24% 93.33% ST w/ LT-Init 25.32% 36.76% 71.33%
ST w/ ELT-Init 65.82% 76.74% 94.54% ST w/ ELT-Init 25.79% 37.33% 72.10%
ST w/ LLT-Init 67.17% 77.03% 94.73% ST w/ LLT-Init 28.51% 40.63% 73.49%
ST w/o Retrain 66.61% 77.03% 94.73% ST w/o Retrain 27.82% 39.49% 73.37%

D Tăng Tốc Về Thời Gian Suy Luận
Ngoài số lượng tham số và FLOPs, chúng tôi đo FPS suy luận và tăng tốc trên
cả GPUs 1080Ti và một bộ gia tốc suy luận DNN thưa SOTA [32]. Như được
hiển thị trong Bảng 10, ST đạt được FPS ngang bằng hoặc thậm chí cao hơn
(tức là, tăng tốc 1.8×∼2.9×) trên GPUs và thời gian bộ gia tốc giảm nhiều (tức
là, tăng tốc 2.9×∼4.1×) trên [32] so với các đường cơ sở, nhờ vào việc tìm kiếm
kiến trúc đồng thời và cắt tỉa tham số (tức là, 2-in-1) và ST.

Bảng 10. ST so với các đường cơ sở điển hình trên Cityscapes, về mặt thời gian suy
luận được đo trên cả GPUs và bộ gia tốc thưa.

Mô Hình Params FLOPs mIoU GPU FPS Sparse Acc. Time
BiSeNet 5.8M 6.6G 69.00% 105.8 180.8ms
DF1-Seg-d8 - - 71.40% 136.9 181.7ms
FasterSeg 4.4M - 71.50% 163.9 142.4ms
SqueezeNAS 0.73M 8.4G 72.40% 117.2 198.5ms
ST ( p= 50%) 0.63M 1.0G 72.68% 310.7 48.3ms

E Thảo Luận
Hạn Chế của SuperTickets Được Chuyển Giao. Mặc dù các SuperTickets
được xác định có thể chuyển giao chỉ với các bộ phân loại là đặc thù tác vụ, vẫn
có một hạn chế trong các SuperTickets được chuyển giao. Đó là, các SuperTickets
được chuyển giao không thể vượt trội hơn những SuperTickets được tìm thấy
trực tiếp trên các bộ dữ liệu/tác vụ mục tiêu. Hơn nữa, khi độ thưa thấp (ví dụ,
30%), các SuperTickets được chuyển giao sẽ kém hiệu suất hơn cả SuperTickets
và S+P. Điều này trái với trực giác và ngược lại với quan sát trong việc nén các
mô hình được huấn luyện trước [13], trong đó các tỷ lệ cắt tỉa thấp không làm
tổn hại độ chính xác sau khi chuyển giao trong khi cắt tỉa quá mức dẫn đến
under-fitting. Nó ngụ ý rằng việc tìm kiếm chuyên biệt là cần thiết khi tỷ lệ cắt
tỉa tương đối thấp; trong khi đối với độ thưa cao, tác động của kiến trúc mạng
nơ-ron sẽ ít hơn.

Trực Quan Hóa và Thảo Luận. Chúng tôi trực quan hóa kết quả của SuperTickets
và các đường cơ sở S+P trên các bộ dữ liệu COCO keypoint và Cityscapes dưới
các

--- TRANG 21 ---
21 H. You et al.
S+P (Mag.) SuperTickets
90% SparsityS+P (Mag.) SuperTickets
70% Sparsity
70% SparsityS+P (Mag.) SuperTickets
95% SparsityS+P (Mag.) SuperTickets(a) Ước Lượng Tư Thế Con Người
(b) Phân Đoạn Ngữ Nghĩa

Hình 10. Trực quan hóa ước lượng tư thế con người trên bộ dữ liệu COCO keypoint
và nhãn streetview/ngữ nghĩa trên bộ dữ liệu Cityscapes dưới các tỷ lệ cắt tỉa khác nhau.

tỷ lệ cắt tỉa khác nhau, như được hiển thị trong Hình 10. Chúng tôi quan sát
rằng các đường cơ sở S+P hoạt động nhưng bỏ lỡ một số điểm chính hoặc hiểu
biết ngữ nghĩa dưới độ thưa trung bình (ví dụ, 70%) trong khi sụp đổ dưới các
tỷ lệ cắt tỉa cao (ví dụ, 90/95%); Ngược lại, các SuperTickets được xác định
của chúng tôi nhất quán hoạt động tốt trong một phạm vi rộng các tỷ lệ cắt tỉa,
xác thực hiệu quả của SuperTickets được đề xuất của chúng tôi.

F Trực Quan Hóa Thêm Về Kết Quả Nhận Dạng Thị Giác
Chúng tôi tiếp tục trực quan hóa kết quả của SuperTickets và các đường cơ sở
S+P trên các bộ dữ liệu COCO keypoint và Cityscapes dưới các tỷ lệ cắt tỉa
khác nhau, như được hiển thị trong Hình 11 và Hình 12, tương ứng. Chúng tôi
quan sát rằng các đường cơ sở S+P hoạt động nhưng bỏ lỡ một số điểm chính
hoặc hiểu biết ngữ nghĩa dưới độ thưa trung bình (ví dụ, 70/80%) trong khi sụp
đổ dưới các tỷ lệ cắt tỉa cao (ví dụ, 90/95%); Ngược lại, các SuperTickets được
xác định của chúng tôi nhất quán hoạt động tốt trong một phạm vi rộng các tỷ
lệ cắt tỉa, xác thực hiệu quả của SuperTickets được đề xuất của chúng tôi.

--- TRANG 22 ---
22 H. You et al.

Hình 11. Trực quan hóa ước lượng tư thế con người trên bộ dữ liệu COCO keypoint
dưới các tỷ lệ cắt tỉa khác nhau.

--- TRANG 23 ---
23 H. You et al.
70% SparsityS+P (Mag.) SuperTickets
95% SparsityS+P (Mag.) SuperTickets
Phân Đoạn Ngữ Nghĩa
80% Sparsity
90% Sparsity
S+P (Mag.) SuperTickets S+P (Mag.) SuperTickets
70% SparsityS+P (Mag.) SuperTickets
95% SparsityS+P (Mag.) SuperTickets
Phân Đoạn Ngữ Nghĩa
80% Sparsity
90% Sparsity
S+P (Mag.) SuperTickets S+P (Mag.) SuperTickets

Hình 12. Trực quan hóa nhãn streetview/ngữ nghĩa trên bộ dữ liệu Cityscapes dưới
các tỷ lệ cắt tỉa khác nhau.
