# 2207.13955.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2207.13955.pdf
# File size: 679542 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Neural Architecture Search on Efﬁcient Transformers and Beyond
Zexiang Liu1, Dong Li1, Kaiyue Lu1, Zhen Qin1, Weixuan Sun3;1, Jiacheng Xu1, Yiran Zhong1;2*
1SenseTime Research2Shanghai AI Lab3Australian National University
Abstract
Recently, numerous efﬁcient Transformers have been
proposed to reduce the quadratic computational complex-
ity of standard Transformers caused by the Softmax atten-
tion. However, most of them simply swap Softmax with an
efﬁcient attention mechanism without considering the cus-
tomized architectures specially for the efﬁcient attention.
In this paper, we argue that the handcrafted vanilla Trans-
former architectures for Softmax attention may not be suit-
able for efﬁcient Transformers. To address this issue, we
propose a new framework to ﬁnd optimal architectures for
efﬁcient Transformers with the neural architecture search
(NAS) technique. The proposed method is validated on pop-
ular machine translation and image classiﬁcation tasks. We
observe that the optimal architecture of the efﬁcient Trans-
former has the reduced computation compared with that of
the standard Transformer, but the general accuracy is less
comparable. It indicates that the Softmax attention and
efﬁcient attention have their own distinctions but neither
of them can simultaneously balance the accuracy and ef-
ﬁciency well. This motivates us to mix the two types of at-
tention to reduce the performance imbalance. Besides the
search spaces that commonly used in existing NAS Trans-
former approaches, we propose a new search space that
allows the NAS algorithm to automatically search the at-
tention variants along with architectures. Extensive exper-
iments on WMT’14 En-De and CIFAR-10 demonstrate that
our searched architecture maintains comparable accuracy
to the standard Transformer with notably improved compu-
tational efﬁciency.
1. Introduction
Efﬁcient Transformers [17, 20, 25, 31] have achieved
remarkable advances in recent years. They reduce the
quadratic computational complexity of the standard Trans-
former [35] by spasifying or approximating Softmax atten-
tion in a more efﬁcient fashion. Currently, the conﬁgura-
tion of the efﬁcient network, e.g., the number of heads and
*Zexiang Liu, Dong Li, and Kaiyue Lu contributed equally to this re-
search. Corresponding author: Yiran Zhong ( zhongyiran@gmail.com )
(a)WMT’14 En-De translation-505101520253035
7 8 9 10 11 12 13 14BLEU
FLOPS (G)Transformer *mFormer (ours)
cosFormer *Vanilla Transformer
Vanilla cosFormer
8486889092949698
7 7.5 8 8.5 9 9.5 10 10.5 11 11.5Accuracy (%)
FLOPS (G)mFormer (ours)
Transformer *
cosFormer *Vanilla Transformer
Vanilla cosFormer
(b)CIFAR -10 classificationFigure 1. Performance comparison in accuracy and efﬁciency.
Models marked with “*” are the optimal architectures that found
by NAS. The size of the circle represents the relative parame-
ter scale on each task. The proposed mixed use of Softmax and
efﬁcient attention, i.e., mFormer, is beneﬁcial for balancing ac-
curacy and efﬁciency. Notice: (a) The cosFormer presents very
low BLEU on the translation task due to the inappropriate conver-
gence. (b) The vanilla Transformer and cosFormer in CIFAR-10
have a smaller FLOPS because they only have 6 layers, while the
searched optimal models (including ours) have more than 10 lay-
ers. With similar FLOPS, ours has a much higher accuracy.
the input embedding size, is directly copied from the Trans-
former, which may not be suitable for efﬁcient Transform-
ers [25]. The customized network structure speciﬁcally for
efﬁcient Transformers has not been well studied. However,
the manual design always involves expensive engineering
work and could be sub-optimal due to the human bias [14].
Hence, in this paper, we aim to explore how to automat-
ically ﬁnd an appropriate architecture for efﬁcient Trans-
formers.arXiv:2207.13955v1  [cs.CL]  28 Jul 2022

--- PAGE 2 ---
Softmax attention
or
Linear attentionQuery
Key
ValueOutputFigure 2. The proposed search space for attention types. It con-
tains the Softmax attention and linear attention ( i.e., cosFormer
attention [25] in this paper). We let the model automatically deter-
mine which attention to use with NAS.
To achieve the goal, we need to (1) specify a suitable ef-
ﬁcient Transformer as the target, and (2) ﬁnd an approach to
automating the network design. For the ﬁrst point, we give
a brief overview of existing efﬁcient Transformers. In terms
of how to treat the Softmax attention, they can be roughly
classiﬁed into pattern-based and kernel-based [25]. Pattern-
based methods sparsify the attention matrix with predeﬁned
or learnable patterns, e.g., chunking input sequences into
ﬁxed blocks [26], calculating attention at ﬁxed intervals [7],
or using axial attention [15]. Although the sparsity gen-
erated by speciﬁc patterns is beneﬁcial for simplifying at-
tention calculation, it still suffers from the quadratic com-
plexity with respect to the input length N[33] and is com-
plicated to implement. Differently, kernel-based methods
aim at reducing the quadratic complexity into linear ( i.e.,
O(N)in both time and space complexity) [17, 25]. They
reformulate the self-attention mechanism to avoid explicitly
computing the NNmatrix. Moreover, they are easier to
reproduce in practice. Considering the intriguing proper-
ties of kernel methods, we choose the cosFormer [25], a
new kernel-based model with state-of-the-art performance
among efﬁcient Transformers, as our target. We attempt to
seek for its customized and optimal architecture.
To automate the network design, we take advantage of
neural architecture search (NAS) [11, 23]. It has been
widely employed in searching standard Transformer archi-
tectures in Natural Language Processing [14, 16, 29, 36, 38]
and Computer Vision [5, 6, 10, 13]. These studies mainly
focus on reﬁning search space and/or improving search al-
gorithms. For example, AutoAttend [14] explores the ef-
fecitve connections between the Query ,Key, and Value by
creating primitive operations to the input data, e.g.,11
convolution and 13max pooling. The evolved Trans-
former [29] applies tournament selection [27] to NAS to
generate candidate models more robustly. However, these
methods suffer from long training and large search costs
because all the candidates need to be optimized, evaluated,
and ranked. For the purpose of lowering these costs, we
utilize RankNAS [16], a new efﬁcient NAS framework for
searching the standard Transformer [35]. It can signiﬁcantly
speed up the search procedure through pairwise ranking,
search space pruning, and the hardware-aware constraint.
Given both the efﬁcient Transformer ( i.e., cosFormer[25]) and the search algorithm ( i.e., RankNAS [16]), we
conduct a preliminary study on pure efﬁcient Transformer-
based NAS. Speciﬁcally, we replace Softmax with the lin-
ear attention introduced in the cosFormer, and then search
it with RankNAS. To comprehensively investigate the op-
timal architecture, we perform the search on two represen-
tative tasks in NLP and CV ﬁelds, i.e., machine translation
(WMT’14 En-De [4]) and image classiﬁcation (CIFAR-10
[19]). Generally, we observe that the optimal structure of
the cosFormer has fewer computational costs, e.g., smaller
FLOPS as illustrated in Fig. 1. However, the general accu-
racy is less comparable to that of the standard Transformer
(see the vertical axis in Fig. 1). This performance imbalance
between accuracy and efﬁciency has also been revealed in
other efﬁcient Transformers [8, 17, 18, 32, 37].
Considering that the vanilla Softmax attention and linear
attention have their own distinctions regarding the perfor-
mance, we propose to use Softmax and linear attention in a
mixed way for the beneﬁt of a better balance between accu-
racy and efﬁciency (named as “mFormer”). Moreover, we
expect the model to determine which type of attention to
use automatically. To this end, we introduce a new search
space specially for attention and incorporate it into the NAS
framework. After re-searching the optimal architecture, we
ﬁnd that the combination of the two attention types achieves
comparable performance to the Transformer with notably
improved efﬁciency on both tasks (see Fig. 1).
In summary, we make three primary contributions:
• To the best of our knowledge, it is the ﬁrst work to
search the optimal architecture of efﬁcient Transform-
ers. We employ NAS for searching the cosFormer, a
representative efﬁcient model. The searched results
give new insights to the community, i.e., how to design
customized, effective, and efﬁcient network structures
for efﬁcient Transformers.
• We propose a novel usage of attention, i.e., mixing
Softmax attention and linear attention in the Trans-
former, and deﬁne a new search space for attention
search in the NAS framework. This pushes existing
Transformer search further by enabling automatic se-
lection of appropriate attention types.
• The proposed mixed attention achieves better balance
between accuracy and efﬁciency, i.e., having compa-
rable performance to the standard Transformer while
maintaining good efﬁciency. This is validated on both
machine translation and image classiﬁcation tasks.
2. Related Work
2.1. Efﬁcient transformers
The standard Transformer [35] suffers from the quadratic
time and space complexity caused by the Softmax atten-
tion. To reduce computational costs, efﬁcient Transformers

--- PAGE 3 ---
𝜑(𝐐) 𝐐
𝐊T
𝐕𝐎𝑁×𝑑
𝑑×𝑁
𝑁×𝑑𝑁×𝑁𝑁×𝑑
𝜑(𝐊)T
𝐕𝐎
𝑁×𝑑𝑑×𝑁
𝑑×𝑑𝑁×𝑑
𝑁×𝑑S ×
××
(a) Softmax attention (b) Linear attention×SSoftmax
Matrix multiplicationFigure 3. Illustration of attention. (a) Softmax calculates QKTﬁrst, leading to the quadratic complexity with respect to the sequence
lengthN. (b) Linear attention decomposes the similarity function with a kernel function, and multiplies KandVﬁrst. This reduces the
quadratic complexity to linear.
either sparsify or approximate Softmax in a more efﬁcient
fashion. Sparsifying the attention is adopted by the pattern-
based methods, where the attention matrix is sparsiﬁed with
predeﬁned or learnable patterns. [26] chunks the input se-
quence into ﬁxed blocks, so that the complexity is reduced
fromN2toB2(Bis the block size smaller than N). Al-
ternatively, [22] downsamples the input into ﬁxed length.
Instead of adjusting the sequence length, [3, 7] make use of
strided/dilated patterns to compute attention at ﬁxed inter-
vals. Compared with these methods with one ﬁxed pattern,
the combination of multiple patterns can diversify the cov-
erage of attention access [33]. For example, [15] calculates
the attention along every axis of the input, and [7] aggre-
gates the attention from both strided and local patterns. To
further improve the quality of patterns, learning patterns in
a data-driven manner is becoming a new trend [18, 28, 32].
The beneﬁt over ﬁxed patterns is that learnable ones are able
to more precisely cluster input tokens based on token rele-
vance while still maintaining the efﬁciency [33]. In sum-
mary, pattern-based methods can enhance the efﬁciency by
sparsifying the Softmax attention. However, they still have
the quadratic complexity, and it will increase with the input
length becoming larger. In addition to that, they are rela-
tively complicated to reproduce in practice.
Another general category of efﬁcient Transformers is
based on kernels (orkernel functions ), which aims to reduce
the complexity from quadratic to linear. By this means,
Softmax can be re-written with other forms to avoid the
NNattention matrix [33]. To achieve this, [37] assumes
a low-rank prior within the NNstructure, and transforms
theKeyandValue into a lower dimension with extra projec-
tion layers. [24] approximates Softmax with the production
of a series of Gaussian kernels, and alternatively, [9] uses
Haar kernels instead. The PerFormer [8] employs orthogo-
nal random features to generate random kernels. The Lin-
ear Transformer [17] makes use of the associative property
of matrix products, and reformulates the similarity function
with kernel decomposition. The cosFormer [25] follows
this kernelization strategy, and utilizes ReLU [1] as the ker-
nel function with an additional cosine reweighting mech-
anism. Generally, kernel-based methods can linearize the
complexity and effectively enhance the efﬁciency. More-over, they are easier to implement than pattern-based ap-
proaches.
2.2. Neural architecture search (NAS)
NAS [11, 23] aims to automatically ﬁnd the most appro-
priate network architecture, and has been extensively ap-
plied to Computer Vision [5,6,10,13] and Natural Language
Processing [14,16,29,36,38]. The core of NAS is to design
a suitable search space, rank all candidate architectures gen-
erated from the space, and ﬁnd the optimal structure. For
Transformers in NLP, the Evolved Transformer (ET) [29]
is the pioneering work that applies NAS into Transformer
architecture search. It deﬁnes a large-scale search space,
which covers the components in the input, normalization,
layers, the output dimension, activations, etc. An evolu-
tion algorithm [27] is adopted to make architecture selec-
tion more stable and robust, i.e., consistently selecting the
most promising architecture based on the ﬁtness. Although
ET can ﬁnd a better and more efﬁcient structure, its compu-
tational costs are still very large. This is because the algo-
rithm has to cover all the search features. Besides, training
the evolution process is also time-consuming.
Subsequent works focus on improving the search algo-
rithm and/or pruning the search space. [41] employs the dif-
ferentiable architecture search (DARTS) [21] that includes
all the operations into a node ( i.e., constituting a super-
net) and leverages Softmax for the speciﬁc choice. This
method relaxes the need of training each candidate net-
work separately. [34] decomposes the Transformer struc-
ture into smaller components, and introduces an one-shot
search algorithm based on sampling. HAT [36] introduces a
hardware-aware constraint for accelerating the search pro-
cess. RankNAS [16] treats NAS as a pairwise ranking prob-
lem, which signiﬁcantly speeds up the search process.
Pruning the search space, i.e., only retaining the most im-
portant search features, is another approach to reducing the
search costs. TextNAS [38] speciﬁes a tailored search space
for text representation, which consists of convolutional, re-
current, pooling, and self-attention layers. The Primer [30]
modiﬁes the search space with squaring ReLU activations
and a depth-wise convolutional layers after the Query ,Key,
andValue . AutoAttend [14] specially searches the connec-

--- PAGE 4 ---
Embedding Size
Layer Num
Self-Attn Head Num
Self-Attention Type
FFN DimensionEmbedding Size
Layer Num
Self-Attn Head Num
Self-Attention Type
FFN DimensionCross -Attn Head Num
Cross -Attention TypeEncoderDecoderFigure 4. Search space in this paper. The blue blocks are primi-
tive features in Transformer search, and the gray ones denote the
proposed attention type search.
tions between QKV . RankNAS [16] proposes a feature se-
lection algorithm that measures the importance of each fea-
ture and sorts out the most useful ones for further search. In
summary, search space pruning can effectively improve the
search efﬁciency and maintain good performance.
3. NAS on Efﬁcient Transformers
In this section, we ﬁrst give a brief review of the prelim-
inary knowledge of the cosFormer [25] and RankNAS [16].
After that, we specify the main steps of applying NAS to the
efﬁcient Transformer search and discuss the general results.
Finally, we introduce the proposed idea of using mixed at-
tention.
3.1. Preliminary knowledge
3.1.1 cosFormer
Given the Query Q2RNd, KeyK2RNd, and Value
V2RNd(dis the feature dimension of each head), the
standard Transformer [35] produces the output O2RNd
by multiplying the normalized QK attention to V,i.e.,
O=(QKT)V; (1)
whereis the similarity function, e.g., Softmax in the
Transformer. The calculation of QKT2RNNleads
to the quadratic complexity with respect to the sequence
lengthN(see Fig. 3(a)).
Linearizing attention can reduce the complexity from
quadratic to linear [17, 25]. This is achieved by decompos-
ing the similarity function and multiplying KandVﬁrst,
i.e.,
(QKT)V= ('(Q)'(K)T)V='(Q)('(K)TV);(2)
where'is a kernel function that transforms QandK
into hidden representations. In this case, the complexityO(N2d)lowers toO(Nd2). Generally we always have
Nd, soO(N2d)O(N2)andO(Nd2)O(N),
yielding the linear complexity.
In essence, it is recognized that the similarity function
should be non-negative andnon-linear [17, 25]. The non-
negativity strengthens the positively correlated features, and
[25] achieves this by directly utilizing the ReLU function
[1],i.e.,ReLU(x) = max(0;x). The non-linearity concen-
trates the most relevant features to be aggregated and sta-
bilizes training [2, 12]. To this end, [25] further introduces
acosine -based reweighting mechanism such that nearby to-
kens are encouraged to have higher attention connection.
The entire formulation is
Oi=NX
j=1(ReLU( Qi)ReLU( Kj)Tcos(
2i j
N))Vj:
(3)
Here for simplicity, we ignore the normalization term. Eq. 3
can be re-written as the linear form in Eq. 2, and we refer
readers to the original paper [25] for more details.
3.1.2 RankNAS
Conventional NAS methods [14, 29, 38] have to evaluate
the performance of every candidate network for ranking,
which is time-consuming especially when the search space
is large. The recently-proposed RankNAS [16] can effec-
tively reduce training costs with the following three distinc-
tions:
1.Pairwise ranking. Instead of sorting all candidate ar-
chitectures, RankNAS treats NAS as a pairwise rank-
ing problem. That is, a binary classiﬁcation is con-
ducted to each candidate pair, and the label is simpli-
ﬁed into correctly ordered orincorrectly ordered as per
their estimated performance.
2.Search space pruning. RankNAS prunes the search
space by only containing the most important features
that have large impact to the performance.
3.Hardware-aware search. In addition to the stan-
dard search based on losses, RankNAS also leverages
a hardware constraint, i.e., estimating the latency and
discarding the fastest and slowest 10% models.
Considering the good efﬁciency of RankNAS, we take it as
our NAS framework. More technical details can be found
in [16].
3.2. RankNAS on cosFormer
3.2.1 Search space
Our primitive search space is adopted from RankNAS,
which contains fundamental features including the embed-

--- PAGE 5 ---
Primitive features
(from RankNAS [16])Enc Layer Num 6
Enc Emb Dim 768, 1024
Enc FFN Dim 2048, 3072, 4096, 5120
Enc Head Num 4, 8, 16
Dec Layer Num 1, 2, 3, 4, 5, 6
Dec Emb Dim 768, 1024
Dec FFN Dim 2048, 3072, 4096, 5120
Dec Head Num 4, 8, 16
En-De Head Num 4, 8, 16
En-De Connect 1, 2, 3
Proposed featureEnc Self-Attn Softmax, Linear
Dec Self-Attn Softmax, Linear
En-De Cross-Attn Softmax, Linear
Table 1. Search space for WMT’14 En-De translation.
ding size, the number of encoder/decoder layers, the num-
ber of heads, and the dimension of the feed-forward net-
work (see Fig. 4). We also make corresponding modiﬁca-
tion to the space as per each task. Detailed deﬁnition is
introduced in Section 4.1.
3.2.2 Main steps
We replace all Softmax with the cosFormer attention, and
follow RankNAS [16] for the search process. A supernet
containing all possible architectures is ﬁrstly trained. After-
wards, the loss and latency data are separately collected and
ranked. Based on the loss and latency constraints, the most
important features are selected, i.e., search space pruning.
The evolution algorithm is performed on the reﬁned search
space, and the optimal architecture is sorted out. Finally, we
re-train the optimal network from scratch.
3.2.3 Discussion
For comparison, we also repeat the above steps to the stan-
dard Transformer [35] only with the Softmax attention.
For simplicity, we denote the optimal architecture of the
cosFormer as “cosFormer ”, and that of the Transformer
as “Transformer”. From Fig. 1, we ﬁnd that on both
tasks, the cosFormer presents better efﬁciency than the
Transformer(smaller FlOPS) with a comparable param-
eter scale, but the general accuracy is less competitive. We
name this phenomenon as the imbalance between accuracy
and efﬁciency , which has also been observed in other efﬁ-
cient Transformers [8, 17, 18, 32, 37].
In terms of the attention itself, the advantage of Soft-
max in accuracy is associated with its ability in “imposing
a categorical distribution constraint on the query-context
relevance scores” [40]. This constraint has two essential
properties: (1) dense coverage ,i.e., attending every fea-
ture to the query [35]; and (2) non-linear concentration ,
i.e., larger weights are only assigned to more relevant fea-
tures [2, 12, 25]. In spite of the useful properties, we shouldPrimitive features
(from [6])Enc Layer Num 12, 13, 14
Enc Emb Dim 320. 384, 448
Enc FFN Dim 672, 896, 1344, 1568, 1792
Enc Head Num 5, 6, 7
Proposed feature Enc Self-Attn Softmax, Linear
Table 2. Search space for CIFAR-10 classiﬁcation.
also notice that the primary limitation of Softmax is its high
computational complexity, as analyzed in Section 3.1.
To reduce the computational costs, efﬁcient Transform-
ers either sparsify the Softmax matrix (pattern-based) or
replace it with other kernel functions (kernel-based). Al-
though pattern-based methods retain the Softmax formu-
lation, they normally utilize ﬁxed patterns, which are less
comprehensive, ﬂexible and robust [40]. Kernel approaches
take all features into account, but the non-linearity is less
powerful in concentrating relevant features, e.g., the cos-
Former [25] focuses more on locality so that the relevant
information in long-range distance may not be well aggre-
gated. These efﬁcient approximations cannot fully cover
the aforementioned two properties of Softmax, so their ac-
curacy may be negatively affected. Only a few models
[25, 39, 40] have occasionally achieved improved accuracy
over the Transformer [35] on some speciﬁc tasks or set-
tings, but their performance is not consistently better in all
cases. Hence, it is still challenging for efﬁcient Transform-
ers to approximate Softmax effectively and reduce the per-
formance imbalance.
3.3. Mixed attention as a new search feature
Considering that Softmax attention and linear attention
have their own distinctions but cannot simultaneously bal-
ance accuracy and efﬁciency well, we propose to employ
the two types of attention in a mixed way. That is, Softmax
attention or linear attention only occurs in certain layers,
and we let the model automatically determine their layer po-
sition with NAS. To this end, we deﬁne a new search space,
i.e., the attention type, which consists of Softmax attention
and the linear attention ( i.e., cosFormer attention [25]). We
repeat the main steps after incorporating the new search
space, and name the optimal architecture as “mFormer”.
Fig. 1 demonstrates that mFormer has better balance be-
tween accuracy and efﬁciency on both image classiﬁcation
and machine translation tasks. Detailed evaluation can be
found in the next section.
It should be noted that in Transformer NAS, deﬁning a
new search feature normally requires imposing extra con-
straints to prevent it from introducing redundant or irrele-
vant search content [14, 29]. By contrast, our attention op-
tion (only containing two types) can be directly embedded
into the search space without the need of considering any
constraint. This is beneﬁcial for the convenient implemen-
tation and deployment.

--- PAGE 6 ---
4. Experiments
4.1. Experimental settings
4.1.1 Datasets
Machine translation: We use the WMT’14 En-De dataset
[4] containing 4.5M pairs of sentences. The training, val-
idation, and test split is identical to that of [16, 36], i.e.,
WMT’16 for training, Newstest2013 for validation, and
Newstest2014 for test. Image classiﬁcation: We leverage
the CIFAR-10 dataset [19], which consists of 60K 3232
images in 10 classes. We use the ofﬁcial split for training
(50K) and test (10K).
4.1.2 Search space deﬁnition
Machine translation: The search space for WMT’14 En-
De is adopted from RankNAS [16], as listed in Table 1. The
number of encoder layers is set as 6, which is consistent
with [16, 36]. “En-De Connect” represents the number of
encoder layers attended to the decoder, e.g., 2 means the last
two encoder layers are involved in calculating the encoder-
decoder cross-attention. The proposed attention search is
composed of the Softmax attention and linear attention ( i.e.,
the cosFormer attention [25]). Note that in practice, we
do not include the attention type search in decoder self-
attention and encoder-decoder cross-attention. This is be-
cause we empirically ﬁnd that employing the linear atten-
tion into decoder layers would lead to inappropriate conver-
gence and poor performance *.Image classiﬁcation: For
CIFAR-10, we follow the setting in [6], i.e., only using the
encoder part and constructing the primitive search space in
Table 2.
4.1.3 Training conﬁguration
We perform all experiments on A100 GPU cards with the
same training steps as RankNAS [16]. On the image clas-
siﬁcation task, the CIFAR-10 images are ﬁrst downsampled
with convolutional layers to 1414, and then ﬂattened as
an 1D vector with 256 elements [6]. The vector is the input
to the Transformer.
4.1.4 Evaluation metrics
For efﬁciency in both two tasks, we calculate the FLOPS
(ﬂoating point operations per second). For accuracy, we
measure BLEU (bilingual evaluation understudy) for trans-
lation with beam 4 and length penalty 0.6. The model is
tested with the averaged last ten checkpoints. The accuracy
of CIFAR-10 is evaluated by computing the percentage of
correctly classiﬁed images. Both metrics are implemented
*We refer the reader to the supplementary material for more details.with the source code in [16] and measured on a Nvidia Tesla
A100 card.
4.2. Optimal architecture comparison on machine
translation
From Table 5, the searched architectures of the cos-
Former and Transformer are more efﬁcient while retain-
ing comparable accuracy to the vanilla models. It validates
the effectiveness of NAS. We compare optimal architectures
below in detail.
4.2.1 Network structure
The searched optimal architecture of the cosFormer [25] is
illustrated in Table 3. For comparison, we also report the
searched results of the Transformer [35]. In spite of the
identical embedding size in both encoders and decoders,
the cosFormerhas a lighter structure, e.g., fewer decoder
layers, smaller encoder FFN dimensions and decoder head
numbers. It implies that the optimal structure of the cos-
Former tends to be “shallow and thin” .
FFN dimension. Fig. 5(a) and (c) display the FFN di-
mensions in the encoder and decoder respectively. We draw
the tendency curves with the polynomial ﬁtting function for
better visualization of changes. The overall FFN dimension
of the cosFormeris smaller than the Transformer . More
speciﬁcally, larger dimensions are required in top layers of
the encoder in the cosFormer . Both the cosFormer and
Transformertend to have a smaller dimension in last lay-
ers of the decoder.
Head number. We plot head numbers in Fig. 5(b)(d)(e).
In general, the cosFormer requires a smaller number of
heads.
En-De Connect. From Fig. 5(f), we observe that de-
coders in both models only attend the last layer of the en-
coder, i.e., obtaining the cross information only from the
last encoder block.
Parameters. The cosFormerhas 12.6M fewer parame-
ters than the Transformer .
4.2.2 Accuracy
On the WMT’14 En-De test set, the cosFormer presents
extremely poor BLEU†. It reﬂects the limitation of the
linear attention in achieving comparable accuracy to the
Transformer.
We also investigate the importance of each feature to the
accuracy, which is measured by RankNAS [16]. The top-5
ranked features are:
•cosFormer:En-De Connect, Dec Layer Num, En-De
Head Num, Dec FFN Dim, Dec Head Num.
†It has the inappropriate convergence, and we are communicating with
the authors of [25] for dealing with this issue.

--- PAGE 7 ---
Transformer  cosFormer  mFormer (ours)
Searched
architecturesEnc Layer Num 3 6 6
Enc Emb Dim 1024 1024 1024
Enc FFN Dim A VG 3925 3413 3072
Enc Head Num A VG 8 8 10.67
Dec Layer Num 3 2 2
Dec Emb Dim 1024 1024 1024
Dec FFN Dim A VG 3755 4096 3584
Dec Head Num A VG 13 10 16
En-De Head Num A VG 13 6 4
En-De Connect A VG 1 1 1
Searched results#Parameters (M) 92.38 79.98 75.59
FLOPS (G) 9.86 8.32 8.08
BLEU 28.34 3.05 25.79
Table 3. Searched architectures and results on machine translation. “A VG” means the average value.
(a) Encoder FFN dimensions (b) Encoder head numbers
(c) Decoder FFN dimension (d) Decoder head numbers
(e) En-De head numbers (f) En-De Connect15002500350045005500
1 2_m 3 4_m 5 6FFN Dim
Encoder LayerTransformer* cosFormer* mFormer
271217
1 2_m 3 4_m 5 6Head Num
Encoder LayerTransformer* cosFormer* mFormer
15002500350045005500
1 2 3FFN Dim
Decoder LayerTransformer* cosFormer* mFormer
271217
1 2 3Head Num
Decoder LayerTransformer* cosFormer* mFormer
271217
1 2 3En-De Head Num
Decoder LayerTransformer* cosFormer* mFormer
0.511.5
1 2 3En-De Connect
Decoder LayerTransformer* cosFormer* mFormer
Figure 5. Detailed searched features on WMT’14 En-De. Layers with linear attention are sufﬁxed with “ m”. Dashed lines reﬂect the
tendency of changes.
•Transformer:Dec Layer Num, En-De Head Num,
Dec FFN Dim, Dec Head Num, Enc FFN Dim.
Hence, when designing the architecture for the efﬁcient
Transformer, we need to focus more on the encoder-decoder
interaction, e.g., the number of encoder layers to attend
and the cross-attention heads. Both the cosFormer and
Transformerare also sensitive to the decoder layer num-
ber, but the best performance does not necessarily need the
largest number of layers.4.2.3 Efﬁciency
Clearly, the cosFormer is more efﬁcient, i.e., its FLOPS is
8.32G while that of the Transformer is 9.86G. This differ-
ence is mainly due to the more complex optimal structure of
the Transformer. The Softmax attention also increases the
computational costs, which will be detailed in Section 4.5.
The top-5 ranked features regarding the efﬁciency are:
•cosFormer:Dec Layer Num, En-De Head Num, En-
De Connect, Enc FFN Dim, Dec Head Num.

--- PAGE 8 ---
Transformer  cosFormer  mFormer (ours)
Searched
architecturesEnc Layer Num 14 12 12
Enc Emb Dim 384 448 384
Enc FFN Dim A VG 1328 1419 1232
Enc Head Num A VG 6.86 6.25 6.33
Searched results#Parameters (M) 24.26 24.31 19.31
FLOPS (G) 10.90 9.55 8.39
Accuracy (%) 95.10 88.40 93.59
Table 4. Optimal architecture and performance comparison on CIFAR-10 image classiﬁcation. “A VG” means the average value.
60011001600
1 2 3_m 4 5 6 7_m 8 9 10 11_m 12 13 14FFN Dim
Encoder LayerTransformer* cosFormer* mFormer*
45678
1 2 3_m 4 5 6 7_m 8 9 10 11_m 12 13 14Head Num
Encoder LayerTransformer* cosFormer* mFormer*
(a) FFN dimensions (b) Head numbers
Figure 6. Detailed searched features on image classiﬁcation. Layers with linear attention are sufﬁxed with “ m”. Dashed lines reﬂect the
tendency of changes.
•Transformer:Dec Layer Num, Dec Head Num, Enc
Head Num, Enc FFN Dim, Dec FFN Dim.
It is obvious that the number of decoder layers has the
largest impact to the efﬁciency. For the efﬁcient Trans-
former, the encoder-decoder interaction also plays an essen-
tial role in determining the computational costs. By con-
trast, the Transformer ’s efﬁciency is less sensitive to the
cross interaction.
4.2.4 Summary
We summarize empirical hints to design an appropriate ar-
chitecture for the efﬁcient Transformer on machine transla-
tion: (1) Fewer decoder layers; (2) Focusing more on the
encoder-decoder interaction. The encoder layers to attend
and the number of encoder-decoder heads are not necessar-
ily too large; (3) The encoder FFN dimension is more im-
portant to the efﬁciency while the decoder FFN dimension
is more important to the accuracy.
4.3. Optimal architecture comparison on image
classiﬁcation
The optimal structures of the cosFormer and Trans-
former signiﬁcantly outperform their vanilla versions in ac-
curacy (see Table 5, which further veriﬁes the necessity and
usefulness of NAS. The FLOPS is larger because vanilla
models only have 6 encoder layers. We compare optimal
structures in the following.
4.3.1 Network structure
Table 4 shows the optimal architectures of the cosFormer
[25] and Transformer [35]. Clearly, the Transformer hasmore layers, a smaller embedding size, and a smaller aver-
aged FFN dimension than the cosFormer . From a general
view, the cosFormer prefers a “shallow and wide” struc-
ture while the optimal architecture of the Transformer tends
to be “deep and thin” .
FFN dimension. Fig. 6(a) displays the FFN dimensions
in different layers. From the tendency curve, we observe
that the cosFormerhas a relatively smaller FFN dimension
in intermediate layers. Differently, the dimension is smaller
in early layers in the Transformer .
Head number. The head numbers in encoder layers are
plotted in Fig. 6(b). The cosFormer generally has more
heads in early and top layers. By contrast, the head numbers
in the Transformer do not vary too much and their values
are large.
Parameters. The cosFormerand the Transformer 
have a similar parameter scale, i.e.,24M.
4.3.2 Accuracy
On the CIFAR-10 test set, the cosFormer achieves the
88.4% accuracy, which is surpassed by the Transformer 
(95.10%) by around 7.6%.
According to the importance to the accuracy, the features
ranked by RankNAS [16] are:
•cosFormer:Layer Num, FFN Dim, Emb Dim, Head
Num.
•Transformer:FFN Dim, Head Num, Emb Dim,
Layer Num.
Clearly, the number of layers has the largest impact to the
accuracy of the cosFormer. Similar to the case in machine
translation, the cosFormer does not select the greatest layer
number.

--- PAGE 9 ---
#Parameters (M) FLOPS (G) BLEU/Accuracy (%)
Machine translationmFormer 75.59 8.085 25.79
mFormer (all linear) 75.59 8.081 3.53
mFormer (all Softmax) 75.59 8.09 27.95
Vanilla cosFormer [25] 213.00 13.39 Null
cosFormer  79.78 8.32 3.05
cosFormer (all Softmax) 79.78 8.34 27.83
Vanilla Transformer [35] 213.00 12.7 28.4
Transformer  92.38 9.86 28.34
Transformer (all linear) 92.38 9.84 3.47
Image classiﬁcationmFormer 19.31 8.39 93.59
mFormer (all linear) 19.23 7.55 83.49
mFormer (all Softmax) 19.33 8.67 94.35
Vanilla cosFormer [25] 19.34 7.60 87.13
cosFormer  24.31 9.55 88.40
cosFormer (all Softmax) 24.42 10.66 94.80
Vanilla Transformer [35] 19.31 8.07 88.70
Transformer  24.26 10.90 95.10
Transformer (all linear) 24.14 9.48 81.82
Table 5. Ablation study on attention types.
4.3.3 Efﬁciency
The FLOPS of the cosFormer is 7.01G, which is signif-
icantly better than the Transformer (10.9G) by 36%. It
reveals the advantage of the cosFormer’s linear attention in
efﬁciency. The important features regarding the efﬁciency
are:
•cosFormer:Layer Num, Emb Dim, Head Num, FFN
Dim.
•Transformer:Layer Num, FFN Dim, Head Num,
Emb Dim.
Similar to the conclusion in machine translation, the layer
number in image classiﬁcation is also the most related to
the efﬁciency. The efﬁcient model has the preference of
using fewer layers, which further reduces the computational
burden.
4.3.4 Summary
We give a brief summary of how to appropriately design
the efﬁcient Transformer in image classiﬁcation: (1) Fewer
layers; (2) A smaller FFN dimension in intermediate layers
with a slightly larger embedding size; (3) A smaller head
number in intermediate layers.
4.4. Our results using mixed attention
4.4.1 Machine translation
Table 3 shows the optimal structure and the performance
of our mFormer. In general, the mFormer is “shallow
and thin” , which has the fewest parameters and smallest
FLOPS among the three optimal models. The linear at-
tention exists in Layer 2 and 4 in the encoder, and all the
other encoder/decoder layers retain the Softmax attention.In mFormer, the FFN dimensions in the encoder undergo
an “up-and-down” change, i.e., early layers rely on smaller
FFN sizes. The number of heads in the encoder tends to
be smaller in intermediate layers. The primary features in
the decoder of the mFormer consistently have the no larger
quantity than the other two, except that the head number in
the ﬁrst decoder layer is larger than that of the cosFormer .
In terms of the performance, the mFormer achieves com-
parable BLEU to the Transformer with 18% fewer param-
eters and 18% smaller FLOPS. It also signiﬁcantly outper-
forms the cosFormer in accuracy with slightly better efﬁ-
ciency. The results indicate that the mixed use of attention
can effectively facilitate the reduction of the imbalance be-
tween accuracy and efﬁciency.
4.4.2 Image classiﬁcation
We display the optimal architecture of the proposed
mFormer in Table 4. Intuitively, our structure is ”shallow
and thin” , which is consistent with the observation in ma-
chine translation. This is also reﬂected by the parameter
scale, i.e., we have around 20% fewer parameters than the
other two. The linear attention exists in Layer 3, 7 and 11
(accounting for 25%), and the remaining attention types in
other layers are all Softmax. The FFN dimensions are large
in early layers, and has a “down-up-down” tendency in sub-
sequent layers. The changes of head numbers seem to be in-
versely proportional to the FFN dimension, i.e., larger FFN
dimensions normally correspond to smaller head numbers.
Remarkably, the mFormer outperforms the cosFormer 
to a large margin in accuracy (by 5.19 in percentage) while
maintaining good efﬁciency, i.e., it only has 1.38G more
FLOPS than the cosFormer . Moreover, it achieves com-

--- PAGE 10 ---
parable performance to the Transformer in accuracy (the
latter surpasses ours only by 1.51 in percentage) but is sig-
niﬁcantly more efﬁcient (our FLOPS is smaller than the
Transformerby 2.51G, which is 23%). It demonstrates
that the use of mixed attention can achieve a better balance
between accuracy and efﬁciency on the image classiﬁcation
task.
4.5. Ablation study on attention types
Since the attention search is newly introduced in the
NAS framework, we specially study the inﬂuence of each
attention type in optimal architectures. This is to validate
(1) the effectiveness of the proposed mixed attention in bal-
ancing accuracy and efﬁciency, and (2) the inherent differ-
ence between Softmax and linear attention in performance.
The results on machine translation and image classiﬁcation
are reported in Table 5.
4.5.1 All Softmax attention
We ﬁrst uniform the mixed attention and linear attention in
the optimal architectures of the mFormer and cosFormer 
with Softmax. For our mFormer, the accuracy is slightly
improved but the FLOPS becomes larger, indicating that
the better accuracy is always along with the sacriﬁce in
efﬁciency. This performance imbalance is more obvious
when we replace all the linear attention in the cosFormer 
with Softmax, where the accuracy is signiﬁcantly enhanced
with much degraded computational efﬁciency. Notably, our
model is able to achieve comparable accuracy to the per-
formance with all Softmax and keep satisfactory efﬁciency
without dropping too much.
4.5.2 All linear attention
In this step, we replace the mixed attention in the mFormer
and Softmax attention in the Transformer uniformly with
the linear attention. We notice a signiﬁcant performance
drop in accuracy on both tasks. It demonstrates the less
competitive ability of the linear attention in yielding accu-
rate results. However, after the replacement, the efﬁciency
gain is remarkable, which further indicates the advantage of
the efﬁcient Transformer in reducing computational costs.
The mixed use of attention is beneﬁcial for achieving both
comparable accuracy and efﬁciency.
5. Conclusion
In this paper, we utilize NAS to ﬁnd the optimal archi-
tecture of efﬁcient Transformers ( i.e., the cosFormer [25]).
The searched architectures reveal that the optimal structures
of the efﬁcient Transformer are relatively lighter than those
of the standard Transformer, e.g., the reduced parameter
scale and improved FLOPS. This provides useful insights tothe community for the appropriate design of efﬁcient Trans-
formers. However, the general accuracy of efﬁcient models
is less competitive. Based on this observation, we propose
a novel usage of attention, i.e., employing the linear atten-
tion and Softmax in a mixed manner in each layer. The
searched optimal architecture presents comparable accuracy
to the Transformer and maintains as good efﬁciency as the
efﬁcient Transformer. The proposed method supplies a new
direction in the Transformer study, i.e., taking advantage of
both Softmax and linear attention. In our future work, we
will study the mixed attention on large-scale pretrain mod-
els as well as other downstream tasks.
References
[1] Abien Fred Agarap. Deep learning using rectiﬁed linear units
(relu). arXiv preprint arXiv:1803.08375 , 2018. 3, 4
[2] Titsias RC AUEB et al. One-vs-each approximation to soft-
max for scalable estimation of probabilities. Advances in
Neural Information Processing Systems , 29, 2016. 4, 5
[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-
former: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020. 3
[4] Ond ˇrej Bojar, Christian Buck, Christian Federmann, Barry
Haddow, Philipp Koehn, Johannes Leveling, Christof Monz,
Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Find-
ings of the 2014 workshop on statistical machine translation.
InProceedings of the ninth workshop on statistical machine
translation , pages 12–58, 2014. 2, 6
[5] Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen
Lin, Ming Sun, Junjie Yan, and Wanli Ouyang. Glit: Neural
architecture search for global and local image transformer.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 12–21, 2021. 2, 3
[6] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin
Ling. Autoformer: Searching transformers for visual recog-
nition. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 12270–12280, 2021. 2,
3, 5, 6
[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
Generating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019. 2, 3
[8] Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe-
ter Hawkins, Jared Davis, David Belanger, Lucy Colwell,
et al. Masked language modeling for proteins via lin-
early scalable long-context transformers. arXiv preprint
arXiv:2006.03555 , 2020. 2, 3, 5
[9] Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,
et al. Rethinking attention with performers. arXiv preprint
arXiv:2009.14794 , 2020. 3
[10] Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xi-
aojie Jin, Zhiwu Lu, and Ping Luo. Hr-nas: searching ef-
ﬁcient high-resolution neural architectures with lightweight
transformers. In Proceedings of the IEEE/CVF Conference

--- PAGE 11 ---
on Computer Vision and Pattern Recognition , pages 2982–
2992, 2021. 2, 3
[11] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.
Neural architecture search: A survey. The Journal of Ma-
chine Learning Research , 20(1):1997–2017, 2019. 2, 3
[12] Bolin Gao and Lacra Pavel. On the properties of the softmax
function with application in game theory and reinforcement
learning. arXiv preprint arXiv:1704.00805 , 2017. 4, 5
[13] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen,
Zhicheng Yan, Yuandong Tian, Vikas Chandra, et al. Nasvit:
Neural architecture search for efﬁcient vision transformers
with gradient conﬂict aware supernet training. In Interna-
tional Conference on Learning Representations , 2021. 2, 3
[14] Chaoyu Guan, Xin Wang, and Wenwu Zhu. Autoattend:
Automated attention representation search. In International
Conference on Machine Learning , pages 3864–3874. PMLR,
2021. 1, 2, 3, 4, 5
[15] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim
Salimans. Axial attention in multidimensional transformers.
arXiv preprint arXiv:1912.12180 , 2019. 2, 3
[16] Chi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao
Li, Tong Xiao, Jingbo Zhu, and Changliang Li. RankNAS:
Efﬁcient neural architecture search by pairwise ranking. In
Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 2469–2480, Online
and Punta Cana, Dominican Republic, Nov. 2021. Associa-
tion for Computational Linguistics. 2, 3, 4, 5, 6, 8, 13
[17] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and
Franc ¸ois Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In International Confer-
ence on Machine Learning , pages 5156–5165. PMLR, 2020.
1, 2, 3, 4, 5
[18] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-
former: The efﬁcient transformer. In International Confer-
ence on Learning Representations , 2020. 2, 3, 5
[19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 2, 6
[20] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-
ungjin Choi, and Yee Whye Teh. Set transformer: A frame-
work for attention-based permutation-invariant neural net-
works. In International Conference on Machine Learning ,
pages 3744–3753. PMLR, 2019. 1
[21] Hanxiao Liu, Karen Simonyan, and Yiming Yang.
Darts: Differentiable architecture search. arXiv preprint
arXiv:1806.09055 , 2018. 3
[22] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich,
Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Gen-
erating wikipedia by summarizing long sequences. arXiv
preprint arXiv:1801.10198 , 2018. 3
[23] Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G
Yen, and Kay Chen Tan. A survey on evolutionary neural
architecture search. IEEE transactions on neural networks
and learning systems , 2021. 2, 3
[24] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz,
Noah Smith, and Lingpeng Kong. Random feature atten-
tion. In International Conference on Learning Representa-
tions , 2021. 3[25] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen
Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran
Zhong. cosformer: Rethinking softmax in attention. In In-
ternational Conference on Learning Representations , 2022.
1, 2, 3, 4, 5, 6, 8, 9, 10, 13
[26] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau
Yih, Sinong Wang, and Jie Tang. Blockwise self-
attention for long document understanding. arXiv preprint
arXiv:1911.02972 , 2019. 2, 3
[27] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V
Le. Regularized evolution for image classiﬁer architecture
search. In Proceedings of the aaai conference on artiﬁcial
intelligence , volume 33, pages 4780–4789, 2019. 2, 3
[28] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David
Grangier. Efﬁcient content-based sparse attention with rout-
ing transformers. Transactions of the Association for Com-
putational Linguistics , 9:53–68, 2021. 3
[29] David So, Quoc Le, and Chen Liang. The evolved trans-
former. In International Conference on Machine Learning ,
pages 5877–5886. PMLR, 2019. 2, 3, 4, 5
[30] David R So, Wojciech Ma ´nke, Hanxiao Liu, Zihang Dai,
Noam Shazeer, and Quoc V Le. Primer: Searching for ef-
ﬁcient transformers for language modeling. arXiv preprint
arXiv:2109.08668 , 2021. 3
[31] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi
Zhang, Kaihao Zhang, Nick Barnes, Stan Birchﬁeld, Ling-
peng Kong, and Yiran Zhong. Vicinity vision transformer.
arXiv preprint arXiv:2206.10552 , 2022. 1
[32] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-
Cheng Juan. Sparse sinkhorn attention. In International
Conference on Machine Learning , pages 9438–9447. PMLR,
2020. 2, 3, 5
[33] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-
zler. Efﬁcient transformers: A survey. arXiv preprint
arXiv:2009.06732 , 2020. 2, 3
[34] Henry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung Won
Chung, and Jason Riesa. Finding fast transformers: One-
shot neural architecture search by component composition.
arXiv preprint arXiv:2008.06808 , 2020. 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 1, 2, 4, 5, 6, 8, 9
[36] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng
Zhu, Chuang Gan, and Song Han. Hat: Hardware-aware
transformers for efﬁcient natural language processing. In An-
nual Conference of the Association for Computational Lin-
guistics , 2020. 2, 3, 6
[37] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020. 2, 3, 5
[38] Yujing Wang, Yaming Yang, Yiren Chen, Jing Bai, Ce
Zhang, Guinan Su, Xiaoyu Kou, Yunhai Tong, Mao Yang,
and Lidong Zhou. Textnas: A neural architecture search
space tailored for text representation. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 34, pages
9242–9249, 2020. 2, 3, 4

--- PAGE 12 ---
[39] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip
Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big
bird: Transformers for longer sequences. Advances in Neu-
ral Information Processing Systems , 33:17283–17297, 2020.
5
[40] Biao Zhang, Ivan Titov, and Rico Sennrich. Sparse attention
with linear units. arXiv preprint arXiv:2104.07012 , 2021. 5
[41] Yuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang,
Furu Wei, and Weizhu Chen. Memory-efﬁcient differ-
entiable transformer architecture search. arXiv preprint
arXiv:2105.14669 , 2021. 3

--- PAGE 13 ---
Supplementary Material
6. Training Details
We specify training details of machine translation and
image classiﬁcation tasks below. For those not listed, we
use the default setting in RankNAS [16].
6.1. Machine translation
• max relative length: 8
• optimizer: adam
• adam betas: (0.9, 0.998)
• weight decay: 0.1
• max tokens: 4400
• criterion: label smoothed cross entropy
• label smoothing: 0.1
• min learning rate: 10 9
• max update: 200000
• warmup updates: 4000
• lr scheduler: inverse sqrt
• lr: 0.0007
• warmup init lr: 10 7
• max lr: 1
6.2. Image classiﬁcation
• max relative length: 14
• optimizer: adam
• adam betas: (0.9, 0.998)
• weight decay: 0.05
• max tokens: 100000
• criterion: label smoothed cross entropy
• label smoothing: 0.1
• min learning rate: 10 7
• max update: 140000
• max sentences: 64
• warmup updates: 976
• lr scheduler: cosine
• lr:10 6
• warmup init lr: 10 6
• max lr: 10 3
7. Feature Importance in mFormer
With RankNAS [16], we can know the importance of
each feature when searching the proposed mFormer.
7.1. Machine translation
The top 5 features regarding the accuracy are: Dec Layer
Num, En-De Head Num, En-De Head Num, Enc Attn Type,
and Dec FFN Dim. Similar to the Transformer , the most
important feature is the number of decoder layers. In our
optimal structure, this number is 2, which indicates that the
best performance does not necessarily need many layers.Besides, the encoder-decoder interaction also has large im-
pact to the accuracy, which requires careful design. The
proposed attention type is important as well, further vali-
dating the effectiveness of attention search.
The top 5 features regarding the efﬁciency are: Dec
Layer Num, Dec Head Num, En-De Connect, En-De Head
Num, and Enc FFN Dim. Again, the number of decoder
layers has the largest impact to the efﬁciency. Note that
the proposed attention type is not contained here, which is
because it only exists in encoders with 6 layers and the efﬁ-
ciency gain is thus not very large.
7.2. Image classiﬁcation
The top 5 features regarding the accuracy are: Enc Attn
Type, Enc Layer Num, Enc FFN Dim, Enc Head Num, and
Enc Embed Dim. The top 5 features regarding the efﬁciency
are: Enc Layer Num, Enc Attn Type, Enc Embed Dim, Enc
Head Num, and Enc FFN Dim.
Clearly, on this task, our attention type plays a crucial
role in both accuracy and efﬁciency, which well veriﬁes the
effectiveness of the mixed use of attention.
8. Linear Attention in Decoders
On the machine translation task, when we put the linear
attention in decoder layers, the BLEU values are all 0 (cor-
responding to three cases: 1) only decoder self attention, 2)
only encoder-decoder attention, and 3) both). We also ﬁnd
that the output sentences have the “repeater” problem, i.e.,
repeating a single word for many times. We have not found
the speciﬁc reason for this issue, and we are communicating
it with the authors in [25]. We infer that the potential reason
is the locality in the cosFormer attention [25] may be more
suitable for feature fusion in encoders.
