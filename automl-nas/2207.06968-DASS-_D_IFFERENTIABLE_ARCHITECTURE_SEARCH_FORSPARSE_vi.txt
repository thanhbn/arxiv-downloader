# 2207.06968.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2207.06968.pdf
# Kích thước file: 1932314 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
DASS: TÌMKIẾM KIẾN TRÚC KHẢVI PHÂN CHO MẠNG NEURAL THƯA THỚT

Hamid Mousavi
Đại học Mälardalen
seyedhamidreza.mousavi@mdu.se

Mohammad Loni
Đại học Mälardalen
mohammad.loni@mdu.se

Mina Alibeigi
mina.alibeigi@zenseact.com

Masoud Daneshtalab
Đại học Mälardalen
masoud.daneshtalab@mdu.se

TÓM TẮT
Việc triển khai Mạng Neural Sâu (DNN) trên các thiết bị biên bị cản trở bởi khoảng cách đáng kể giữa yêu cầu hiệu suất và khả năng xử lý có sẵn. Trong khi nghiên cứu gần đây đã đạt được những bước tiến đáng kể trong việc phát triển các phương pháp cắt tỉa để xây dựng mạng thưa thớt nhằm giảm chi phí tính toán của DNN, vẫn còn mất mát độ chính xác đáng kể, đặc biệt ở tỷ lệ cắt tỉa cao. Chúng tôi phát hiện ra rằng các kiến trúc được thiết kế cho mạng dày đặc bằng phương pháp tìm kiếm kiến trúc khả vi phân không hiệu quả khi áp dụng cơ chế cắt tỉa vào chúng. Lý do chính là phương pháp hiện tại không hỗ trợ các kiến trúc thưa thớt trong không gian tìm kiếm của chúng và sử dụng mục tiêu tìm kiếm được tạo ra cho mạng dày đặc và không chú ý đến tính thưa thớt.

Trong bài báo này, chúng tôi đề xuất một phương pháp mới để tìm kiếm các kiến trúc neural thân thiện với tính thưa thớt. Chúng tôi thực hiện điều này bằng cách thêm hai phép toán thưa thớt mới vào không gian tìm kiếm và sửa đổi mục tiêu tìm kiếm. Chúng tôi đề xuất hai phép toán tham số mới SparseConv và SparseLinear để mở rộng không gian tìm kiếm bao gồm các phép toán thưa thớt. Đặc biệt, các phép toán này tạo ra một không gian tìm kiếm linh hoạt do sử dụng các phiên bản tham số thưa thớt của các phép toán tuyến tính và tích chập. Mục tiêu tìm kiếm được đề xuất cho phép chúng tôi huấn luyện kiến trúc dựa trên tính thưa thớt của các phép toán không gian tìm kiếm.

Các phân tích định lượng chứng minh rằng các kiến trúc tìm kiếm của chúng tôi vượt trội hơn những kiến trúc được sử dụng trong các mạng thưa thớt tiên tiến trên bộ dữ liệu CIFAR-10 và ImageNet. Về hiệu suất và hiệu quả phần cứng, DASS tăng độ chính xác của phiên bản thưa thớt của MobileNet-v2 từ 73.44% lên 81.35% (cải thiện +7.91%) với thời gian suy luận nhanh hơn 3.87 lần.

Từ khóa: Tìm kiếm Kiến trúc Neural, Cắt tỉa, Nén Mạng

1 Giới thiệu

Mạng Neural Sâu (DNN) cung cấp một con đường tuyệt vời để có được khả năng trích xuất đặc trưng tối đa cần thiết để giải quyết các tác vụ thị giác máy tính rất phức tạp [64, 27, 73, 61]. Có nhu cầu ngày càng tăng về việc DNN trở nên hiệu quả hơn để có thể được triển khai trên các thiết bị biên có tài nguyên cực kỳ hạn chế. Tuy nhiên, DNN không được tạo ra một cách nội tại để phù hợp với khả năng tính toán và bộ nhớ hạn chế của các thiết bị biên nhỏ, cấm việc triển khai chúng trong các ứng dụng như vậy [18, 57, 51, 50, 56].

Để dân chủ hóa việc tăng tốc DNN, nhiều phương pháp tối ưu hóa đã được đề xuất, bao gồm cắt tỉa mạng [68, 49, 83, 13], thiết kế kiến trúc hiệu quả [57, 50], lượng tử hóa mạng [55, 7, 38], chưng cất kiến thức [31, 20], và phân tách hạng thấp [36]. Đặc biệt, cắt tỉa mạng được biết đến với việc cung cấp tiết kiệm tính toán và bộ nhớ đáng kể bằng cách loại bỏ các tham số trọng số dư thừa trong kịch bản không có cấu trúc [68, 2, 49, 83, 24], và toàn bộ bộ lọc trong kịch bản có cấu trúc [29, 30, 28, 86, 84]. Gần đây, các phương pháp cắt tỉa không có cấu trúc đã báo cáo cung cấp sự giảm kích thước mạng cực đoan. Các phương pháp cắt tỉa không có cấu trúc tiên tiến [68] cung cấp tỷ lệ cắt tỉa lên đến 99% là một kịch bản tuyệt vời cho các thiết bị biên nhỏ.

Tuy nhiên, các phương pháp này bị giảm độ chính xác đáng kể, cản trở chúng không thể được áp dụng trong thực tế (≈19% giảm độ chính xác cho MobileNet-v2 so với phiên bản dày đặc [68]). Các phương pháp cắt tỉa hiện tại sử dụng các kiến trúc được thiết kế thủ công mà không quan tâm đến tính thưa thớt [2, 49, 83, 86, 68]. Chúng tôi giả định rằng kiến trúc xương sống có thể không tối ưu cho các kịch bản với tỷ lệ cắt tỉa cực đoan. Thay vào đó, chúng tôi có thể học các kiến trúc xương sống hiệu quả hơn có thể thích ứng với các kỹ thuật cắt tỉa bằng cách khám phá không gian của các mạng thưa thớt.

Tìm kiếm Kiến trúc Neural (NAS) đã đạt được thành công lớn trong việc thiết kế tự động các kiến trúc DNN hiệu suất cao. Các phương pháp tìm kiếm kiến trúc khả vi phân (DARTS) [52, 75, 76] là một phương pháp NAS phổ biến sử dụng thuật toán tìm kiếm dựa trên gradient để tăng tốc độ tìm kiếm. Được thúc đẩy bởi các kết quả hứa hẹn của NAS, chúng tôi nảy ra ý tưởng thiết kế các kiến trúc xương sống tùy chỉnh tương thích với các phương pháp cắt tỉa. Tuy nhiên, không gian tìm kiếm của các thuật toán DARTS hiện tại bao gồm các phép toán tích chập và tuyến tính dày đặc không có khả năng khám phá xương sống chính xác cho việc cắt tỉa. Để chứng minh vấn đề này, trước tiên chúng tôi cắt tỉa 99% trọng số từ kiến trúc tốt nhất được thiết kế bởi phương pháp NAS [52] với không gian tìm kiếm cơ sở mà không quan tâm đến tính thưa thớt. Đáng thất vọng, sau khi áp dụng phương pháp cắt tỉa vào kiến trúc cuối cùng, nó hoạt động kém với mất mát độ chính xác lên đến ≈21% trong nén bởi DASS mở rộng không gian tìm kiếm bằng các phép toán thưa thớt. (Phần 4). Thất bại này là do thiếu hỗ trợ cho các đặc tính mạng thưa thớt cụ thể dẫn đến hiệu suất tổng quát hóa thấp. Dựa trên giả thuyết và quan sát thực nghiệm trên, chúng tôi công thức hóa một không gian tìm kiếm bao gồm các phép toán thưa thớt và dày đặc. Do đó, các phép toán tích chập và tuyến tính gốc trong không gian tìm kiếm của NAS đã được mở rộng bằng các phép toán SparseConv và SparseLinear tham số tương ứng. Hơn nữa, để tạo sự nhất quán giữa không gian tìm kiếm được đề xuất và hàm mục tiêu tìm kiếm, chúng tôi sửa đổi bài toán tối ưu hóa hai cấp để tính đến tính thưa thớt. Bằng cách này, quá trình tìm kiếm cố gắng tìm phép toán thưa thớt tốt nhất bằng cách tối ưu hóa cả tham số kiến trúc và tham số cắt tỉa. Sự sửa đổi này tạo ra một bài toán tối ưu hóa hai cấp phức tạp. Để giải quyết khó khăn này, chúng tôi chia bài toán tối ưu hóa hai cấp phức tạp thành hai bài toán tối ưu hóa hai cấp đơn giản và giải quyết chúng.

Chúng tôi cho thấy việc tích hợp cắt tỉa một cách rõ ràng vào quy trình tìm kiếm có thể dẫn đến việc tìm ra các kiến trúc mạng thưa thớt với cải thiện độ chính xác đáng kể. Trong Hình 1, chúng tôi so sánh độ chính xác Top-1 CIFAR-10 và số lượng tham số của kiến trúc được tìm thấy bởi DASS với các mạng thưa thớt (cắt tỉa không có cấu trúc) và dày đặc tiên tiến. Kết quả cho thấy kiến trúc được thiết kế bởi DASS vượt trội hơn tất cả các kiến trúc cạnh tranh sử dụng phương pháp cắt tỉa. DASS-Small chứng minh hiệu quả nhất quán của nó bằng cách đạt được cải thiện độ chính xác 15%, 10%, và 8% so với MobileNet-v2 thưa thớt [67], EfficientNet-v2 thưa thớt [71], và DARTS thưa thớt [52] tương ứng. Ngoài ra, so với các mạng có độ chính xác tương tự, DASS-Large có sự giảm đáng kể về độ phức tạp mạng (#Tham số) bằng 3.5×, 30.0×, 105.2× so với PDO-eConv [69], CCT-6/3×1 [65], và MomentumNet [66] tương ứng. Phần 6 cung cấp một nghiên cứu thực nghiệm toàn diện để đánh giá các khía cạnh khác nhau của DASS. Những đóng góp chính của chúng tôi được tóm tắt như sau:

1. Chúng tôi thực hiện các thí nghiệm mở rộng để xác định các hạn chế của việc áp dụng cắt tỉa với tỷ lệ cắt tỉa cực đoan cho kiến trúc dày đặc như một bước hậu xử lý.

2. Chúng tôi định nghĩa một không gian tìm kiếm mới bằng cách mở rộng không gian tìm kiếm cơ sở với một tập hợp các phép toán tham số mới (SparseConv và SparseLinear) để xem xét các phép toán thưa thớt trong không gian tìm kiếm.

3. Chúng tôi sửa đổi bài toán tối ưu hóa hai cấp để nhất quán với không gian tìm kiếm mới và đề xuất một thuật toán ba bước dựa trên gradient để chia bài toán hai cấp phức tạp và học tham số kiến trúc, trọng số mạng, và tham số cắt tỉa.

2 Nghiên cứu Liên quan

2.1 Tìm kiếm Kiến trúc Neural và Các Biến thể DARTS

Tìm kiếm Kiến trúc Neural (NAS) gần đây đã thu hút sự chú ý đáng kể bằng cách giải thoát các chuyên gia con người khỏi nỗ lực tốn công trong việc thiết kế mạng neural. Các phương pháp NAS ban đầu chủ yếu sử dụng các phương pháp dựa trên tiến hóa [62, 58, 56, 53] hoặc dựa trên học tăng cường [88, 87, 35]. Mặc dù có hiệu quả so với các thiết kế thủ công, chúng đòi hỏi tài nguyên tính toán khổng lồ. Ví dụ, phương pháp được đề xuất trong [88] đánh giá 20.000 ứng viên neural trên 500 GPU NVIDIA® P100 trong bốn ngày. Các phương pháp tìm kiếm kiến trúc một lần [6, 23, 3] đã được đề xuất để xác định các kiến trúc neural tối ưu trong vòng vài ngày GPU (>1 ngày GPU [63]). Đặc biệt, Tìm kiếm Kiến trúc Khả vi phân (DARTS) [52, 75, 76] là một biến thể của các phương pháp NAS một lần làm cho không gian tìm kiếm trở nên liên tục và khả vi phân. Mô tả chi tiết về DARTS có thể được tìm thấy trong Phần 3.1. Mặc dù có những thành công rộng rãi của DARTS trong việc thúc đẩy khả năng ứng dụng NAS, việc đạt được kết quả tối ưu vẫn là một thách thức đối với các vấn đề thực tế. Nhiều nghiên cứu tiếp theo điều tra một số thách thức này bằng cách tập trung vào (i) tăng tốc độ tìm kiếm [34, 70], (ii) cải thiện hiệu suất tổng quát hóa [12, 74], (iii) giải quyết các vấn đề về tính mạnh mẽ [81, 80, 33], (iv) giảm lỗi lượng tử hóa [38, 55], và (v) thiết kế các kiến trúc nhận biết phần cứng [37, 45, 9]. Mặt khác, ít nghiên cứu cố gắng cắt tỉa không gian tìm kiếm bằng cách loại bỏ các phép toán mạng kém [43, 60, 32, 14]. Các nghiên cứu này sử dụng cơ chế cắt tỉa để dần dần loại bỏ một số phép toán khỏi không gian tìm kiếm. Không giống như chúng, phương pháp của chúng tôi nhằm mở rộng không gian tìm kiếm để cải thiện hiệu suất của mạng thưa thớt bằng cách tìm kiếm các phép toán tốt nhất với cấu trúc trọng số thưa thớt. Về mặt kỹ thuật, phương pháp của chúng tôi mở rộng không gian tìm kiếm bằng cách thêm phiên bản thưa thớt tham số của các phép toán tích chập và tuyến tính để tìm kiến trúc thưa thớt tốt nhất. Do đó, có thiếu nghiên cứu về tham số trọng số thưa thớt khi thiết kế kiến trúc neural. DASS tìm kiếm các phép toán hiệu quả nhất cho tham số trọng số thưa thớt để đạt được hiệu suất tổng quát hóa cao hơn.

2.2 Cắt tỉa Mạng

Cắt tỉa mạng là một phương pháp hiệu quả để giảm kích thước của DNN, cho phép chúng được triển khai hiệu quả trên các thiết bị có khả năng tài nguyên hạn chế. Các nghiên cứu trước đây về cắt tỉa mạng có thể được phân loại thành hai loại: phương pháp cắt tỉa có cấu trúc và không có cấu trúc. Mục đích của cắt tỉa có cấu trúc là loại bỏ các kênh hoặc bộ lọc dư thừa để bảo toàn toàn bộ cấu trúc của tensor trọng số với việc giảm chiều [29, 30, 48, 28, 86, 21]. Trong khi cắt tỉa có cấu trúc nổi tiếng về tăng tốc phần cứng, nó hy sinh một mức độ linh hoạt nhất định cũng như tính thưa thớt trọng số [54].

Mặt khác, các phương pháp cắt tỉa không có cấu trúc cung cấp tính linh hoạt và tỷ lệ nén vượt trội bằng cách loại bỏ các tham số có tác động ít nhất đến độ chính xác mạng khỏi tensor trọng số [25, 47, 29, 54, 17, 68, 2, 49, 83]. Nói chung, cắt tỉa không có cấu trúc bao gồm ba giai đoạn để tạo ra một mạng thưa thớt, bao gồm (i) tiền huấn luyện, (ii) cắt tỉa, và (iii) tinh chỉnh. Các phương pháp cắt tỉa không có cấu trúc trước đây sử dụng các tiêu chí khác nhau để chọn các tham số trọng số cắt tỉa thấp nhất. [44, 26] cắt tỉa các tham số trọng số dựa trên các giá trị đạo hàm bậc hai của hàm mất mát. Một số nghiên cứu đề xuất loại bỏ các tham số trọng số dưới ngưỡng cắt tỉa cố định, bất kể mục tiêu huấn luyện [25, 47, 17, 85, 77, 22]. Để giải quyết hạn chế của các phương pháp ngưỡng cố định, [2, 41] đề xuất các ngưỡng có thể huấn luyện theo từng lớp để xác định giá trị tối ưu cho mỗi lớp riêng biệt. Giả thuyết vé số may mắn [17, 8, 10] là một hướng khác của phương pháp xác định mặt nạ cắt tỉa cho CNN được khởi tạo và huấn luyện mô hình thưa thớt kết quả từ đầu mà không thay đổi mặt nạ cắt tỉa. HYDRA [68] công thức hóa mục tiêu cắt tỉa như tối thiểu hóa rủi ro thực nghiệm và tích hợp nó với mục tiêu huấn luyện. Không giống như các phương pháp khác, tiêu chí cắt tỉa dựa trên tối ưu hóa cải thiện hiệu suất của mạng thưa thớt so với các số liệu khác. Mặc dù thành công của cắt tỉa dựa trên tối ưu hóa trong việc đạt được tỷ lệ nén đáng kể, độ chính xác phân loại bị ảnh hưởng, đặc biệt khi tỷ lệ cắt tỉa cực kỳ cao (lên đến 99%). Chúng tôi cho thấy lý do chính cho vấn đề này là do kiến trúc xương sống không tối ưu. Chúng tôi mở rộng không gian tìm kiếm của DASS bằng các phép toán thưa thớt tham số và công thức hóa cắt tỉa như một bài toán tối thiểu hóa rủi ro thực nghiệm và tích hợp nó vào bài toán tối ưu hóa hai cấp để tìm mạng thưa thớt tốt nhất.

3 Kiến thức Cơ bản

3.1 Tìm kiếm Kiến trúc Khả vi phân

Tìm kiếm Kiến trúc Khả vi phân (DARTS) [52] là một phương pháp NAS giảm đáng kể chi phí tìm kiếm bằng cách làm cho không gian tìm kiếm trở nên liên tục và khả vi phân. Mẫu ô DARTS được biểu diễn bằng một Đồ thị Có hướng Không có Chu kỳ (DAG) chứa N nút nội bộ. Cạnh (i, j) giữa hai nút được liên kết với một phép toán o(i,j) (ví dụ: kết nối bỏ qua hoặc max-pooling 3×3) trong không gian tìm kiếm O. Công thức 1 tính toán đầu ra của các nút trung gian.

¯o(i,j)(x(i)) = ∑o∈O exp(α(i,j)o)/∑o′∈O exp(α(i,j)o′) · o(x(i)) (1)

trong đó O và α(i,j)o biểu thị tập hợp tất cả các phép toán ứng viên và xác suất lựa chọn của o tương ứng. Nút đầu ra trong ô là sự nối của tất cả các nút trung gian. DARTS tối ưu hóa các tham số kiến trúc (α) và trọng số mạng (θ) với hàm mục tiêu hai cấp sau:

min_α L_val(θ⋆, α) s.t. θ⋆ = argmin_θ L_train(θ, α) (2)

trong đó L_train = ∑_(x,y)∈(X_train,Y_train) l(θ,x,y)/|X_train| và L_val = ∑_(x,y)∈(X_val,Y_val) l(θ,x,y)/|X_val|

Phép toán với α_o lớn nhất được chọn cho mỗi cạnh. X_train và Y_train biểu thị bộ dữ liệu huấn luyện và nhãn tương ứng. Tương tự, bộ dữ liệu xác thực và nhãn được biểu thị bằng X_val và Y_val tương ứng. Sau khi quá trình tìm kiếm hoàn thành, kiến trúc cuối cùng được huấn luyện lại từ đầu để có được độ chính xác tối đa.

3.2 Cắt tỉa Không có Cấu trúc

Cắt tỉa được coi là không có cấu trúc nếu nó loại bỏ các tham số có tầm quan trọng thấp khỏi tensor trọng số và làm cho chúng thưa thớt [54]. Bài báo này sử dụng phương pháp cắt tỉa mạng không có cấu trúc dựa trên tiêu chí tối ưu hóa để cung cấp tính linh hoạt cao hơn và tỷ lệ nén cực đoan so với các phương pháp cắt tỉa có cấu trúc. Phương pháp cắt tỉa bao gồm ba giai đoạn tối ưu hóa chính: (i) tiền huấn luyện: huấn luyện mạng trên bộ dữ liệu mục tiêu, (ii) cắt tỉa: cắt tỉa các trọng số không quan trọng khỏi mạng được tiền huấn luyện, và (iii) tinh chỉnh: mạng thưa thớt được huấn luyện lại để khôi phục độ chính xác ban đầu. Đối với giai đoạn cắt tỉa, chúng tôi xem xét một phương pháp dựa trên tối ưu hóa với các bước sau: Đầu tiên, chúng tôi định nghĩa các tham số cắt tỉa hiển thị tầm quan trọng của mỗi trọng số của mạng (s0) và khởi tạo chúng theo Công thức 3.

s0_i ∝ 1/max(|θ_pre,i|) × θ_pre,i (3)

trong đó θ_pre,i biểu thị trọng số của lớp thứ i trong mạng được tiền huấn luyện. Tiếp theo, để học các tham số cắt tỉa (ŝ), chúng tôi công thức hóa bài toán tối ưu hóa như Công thức 4, sau đó được giải quyết bằng gradient descent ngẫu nhiên (SGD) [19].

ŝ = argmin_s E_(x,y)∼D L_prune(θ_pre, s, x, y) (4)

θ_pre và E đề cập đến các tham số mạng được tiền huấn luyện và kỳ vọng toán học tương ứng. Bằng cách giải quyết bài toán tối ưu hóa này, chúng tôi có thể xác định tác động của mỗi tham số trọng số đối với hàm mất mát và do đó là độ chính xác của mạng. Cuối cùng, chúng tôi chuyển đổi các giá trị thực của tham số cắt tỉa thành mặt nạ nhị phân dựa trên việc chọn k trọng số hàng đầu có độ lớn cao nhất của tham số cắt tỉa.

4 Động lực Nghiên cứu

Các kiến trúc mạng dày đặc ban đầu được thiết kế bằng các phương pháp NAS thông thường không chính xác khi được tích hợp với các phương pháp cắt tỉa, đặc biệt ở tỷ lệ cắt tỉa cao. Để chứng minh khẳng định này, trước tiên chúng tôi áp dụng phương pháp cắt tỉa không có cấu trúc được giải thích trong phần 3.2 cho kiến trúc tốt nhất được thiết kế bởi DARTS [52] cho CIFAR-10 và tạo ra một mạng thưa thớt. Chúng tôi gọi giải pháp này là DARTS thưa thớt. Sau đó, chúng tôi so sánh hiệu suất của kiến trúc thưa thớt được thiết kế bởi DASS với DARTS thưa thớt. Hình 2 minh họa các đường cong độ chính xác huấn luyện và kiểm tra cho các kiến trúc DASS và DARTS thưa thớt được huấn luyện trên bộ dữ liệu CIFAR-10. Đáng thất vọng, mạng được thiết kế bởi DARTS thưa thớt dẫn đến giảm độ chính xác kiểm tra. Điều này ngụ ý rằng các kiến trúc xương sống dày đặc được thiết kế bởi các phương pháp NAS mà không xem xét tính thưa thớt là không hiệu quả (DASS cung cấp độ chính xác kiểm tra cao hơn 8% so với DARTS thưa thớt). Theo điều tra của chúng tôi, chúng tôi tìm thấy hai vấn đề liên quan đến thất bại huấn luyện của DARTS thưa thớt: (i) DARTS không hỗ trợ các phép toán thưa thớt trong không gian tìm kiếm của nó, và (ii) DARTS tối ưu hóa mục tiêu tìm kiếm mà không xem xét tính thưa thớt. Phần 5.2 giải quyết vấn đề đầu tiên, trong khi vấn đề thứ hai được giải quyết trong Phần 5.3. Chúng tôi điều tra DASS trong hai chế độ để chứng minh tầm quan trọng của việc bao gồm các phép toán thưa thớt và công thức lại hàm mục tiêu dựa trên tính thưa thớt. Chế độ đầu tiên chỉ mở rộng không gian tìm kiếm với các phép toán thưa thớt (DASS Op) và không tối ưu hóa các tham số cắt tỉa, trong khi chế độ thứ hai thêm tính thưa thớt vào quá trình tối ưu hóa và tối ưu hóa các tham số kiến trúc và cắt tỉa trong một bài toán tối ưu hóa hai cấp (DASS Op+Ob). Hình 3 chỉ ra độ chính xác kiểm tra cho các kiến trúc DASS Op, DARTS thưa thớt và (DASS Op+Ob) với các tỷ lệ cắt tỉa khác nhau. Như kết quả cho thấy, DASS Op có độ chính xác thấp hơn ≈3.4% so với DASS Op+Ob và độ chính xác cao hơn ≈4.47% so với DARTS thưa thớt. Kết luận, mở rộng không gian tìm kiếm với các phép toán thưa thớt được đề xuất (đóng góp đầu tiên của chúng tôi) trong DASS tạo ra kiến trúc tốt hơn DARTS thưa thớt, nhưng kết hợp nó với mục tiêu tối ưu hóa dựa trên tính thưa thớt (đóng góp thứ hai của chúng tôi) tăng cường hiệu suất.

5 Phương pháp DASS

5.1 DASS: Tổng quan

Chúng tôi đề xuất DASS, một phương pháp tìm kiếm kiến trúc khả vi phân cho mạng neural thưa thớt. DASS trước tiên mở rộng không gian tìm kiếm của NAS với các phép toán thưa thớt tham số. Sau đó nó sửa đổi bài toán tối ưu hóa hai cấp để học kiến trúc, trọng số, và tham số cắt tỉa. DASS sử dụng phương pháp ba bước để giải quyết bài toán tối ưu hóa hai cấp phức tạp, bao gồm (1) Tiền huấn luyện: Tìm kiến trúc dày đặc tốt nhất (tham số cắt tỉa bằng không) từ không gian tìm kiếm và tiền huấn luyện nó (2) Cắt tỉa và Thiết kế Kiến trúc thưa thớt: Tìm mặt nạ cắt tỉa tốt nhất (tối ưu hóa tham số cắt tỉa) và cập nhật tham số kiến trúc dựa trên trọng số thưa thớt và cuối cùng (3) Tinh chỉnh: chúng tôi huấn luyện lại kiến trúc thưa thớt để đạt được hiệu suất phân loại tối đa.

5.2 Không gian Tìm kiếm DASS

Để hỗ trợ các phép toán thưa thớt, DASS đề xuất phiên bản thưa thớt tham số của các phép toán tích chập và tuyến tính được gọi là SparseConv và SparseLinear tương ứng. Các phép toán này có một mặt nạ thưa thớt (m) để loại bỏ các tham số trọng số dư thừa khỏi mạng. Hình 4 minh họa chức năng của hai phép toán này. Ngoài ra, bảng 1 tóm tắt các phép toán của không gian tìm kiếm DASS. Để điều tra thực nghiệm hiệu quả của không gian tìm kiếm thưa thớt được đề xuất, chúng tôi so sánh sự tương đồng của các bản đồ đặc trưng của kiến trúc dày đặc hiệu suất cao (với số lượng lớn tham số) với kiến trúc thưa thớt được khám phá bởi DASS và kiến trúc được thiết kế từ không gian tìm kiếm gốc các phương pháp DARTS thưa thớt. Chúng tôi sử dụng số liệu Kendall's τ [1] để đo sự tương đồng giữa các bản đồ đặc trưng đầu ra. Hệ số tương quan τ trả về một giá trị giữa -1 và 1. Để trình bày kết quả rõ ràng hơn, chúng tôi mở rộng các giá trị này giữa -100 và 100. Các giá trị gần với 100 cho thấy sự tương đồng tích cực mạnh hơn giữa các bản đồ đặc trưng. Hình 5 tóm tắt kết quả. Quan sát của chúng tôi tiết lộ sự tương đồng giữa các bản đồ đặc trưng DASS và kiến trúc dày đặc (lên đến 16%). Mặt khác, sự tương quan giữa DARTS thưa thớt và kiến trúc dày đặc là không đáng kể. Do đó, nó cho thấy rằng kiến trúc được thiết kế bởi DASS dựa trên không gian tìm kiếm mới có thể trích xuất các đặc trưng tương tự hơn với kiến trúc dày đặc hiệu suất cao trong khi DARTS thưa thớt sử dụng không gian tìm kiếm dày đặc đã mất các đặc trưng quan trọng sau khi cắt tỉa. Mức độ tương đồng không quá cao vì DASS là một mạng thưa thớt với tỷ lệ cắt tỉa 99%. Tuy nhiên, nó có thể chứng minh rằng DASS khôi phục các đặc trưng hữu ích.

Bảng 1: Các phép toán của không gian tìm kiếm DASS.

Loại Phép toán | Tách biệt | Giãn nở | Max | Trung bình | Bỏ qua
Thưa thớt | Tích chập thưa thớt | Tích chập | Pooling | pooling | kết nối
Kích thước Kernel | 3×3,5×5 | 3×3,5×5 | 3×3 | 3×3 | N/A

5.3 Mục tiêu Tìm kiếm DASS

DASS nhằm tìm kiếm các tham số kiến trúc tối ưu (α⋆) để tối thiểu hóa mất mát xác thực của tham số trọng số mạng thưa thớt. Do đó, để mục tiêu tìm kiếm nhất quán với không gian tìm kiếm thưa thớt được đề xuất, chúng tôi công thức hóa toàn bộ mục tiêu tìm kiếm như một bài toán tối ưu hóa hai cấp phức tạp:

α⋆ = min_α(L_val(θ̂(α), α))
s.t. {
θ⋆(α) = argmin_θ L_train(θ, α)
m̂ = argmin_m∈{0,1}^N L_prune(θ⋆(α)⊙m, α)
θ̂(α) = θ⋆(α)⊙m̂
} (5)

Trong đó m biểu thị các tham số mặt nạ cắt tỉa nhị phân. Công thức này học các tham số kiến trúc dựa trên các tham số trọng số thưa thớt. Tuy nhiên, Công thức 5 không phải là một bài toán tối ưu hóa hai cấp đơn giản vì bài toán cấp thấp bao gồm hai bài toán tối ưu hóa. Để vượt qua thách thức này, chúng tôi chia mục tiêu tìm kiếm thành ba bước riêng biệt. Do đó, bài toán được chuyển thành hai bài toán tối ưu hóa hai cấp để xác định các tham số kiến trúc tối ưu cho trọng số dày đặc và thưa thớt và một bài toán tối ưu hóa để tinh chỉnh các tham số trọng số. Ngoài ra, bài toán tối ưu hóa cấp thấp bao gồm một bài toán tối ưu hóa rời rạc cho mặt nạ cắt tỉa.

Phần 5.4 đề xuất một thuật toán tối ưu hóa nhiều bước để giải quyết bài toán tối ưu hóa và xử lý bài toán tối ưu hóa rời rạc bằng cách chuyển đổi nó thành một bài toán tối ưu hóa liên tục.

5.4 Thuật toán Tối ưu hóa

Bước 1: tiền huấn luyện (học θ⋆_pre và α⋆_pre)

Trong bước này, chúng tôi chia Công thức 5 thành một bài toán tối ưu hóa hai cấp để tìm kiến trúc dày đặc tốt nhất. Tiền huấn luyện này cần thiết cho bước tiếp theo học các tham số mặt nạ cắt tỉa và sửa đổi kiến trúc thưa thớt.

α⋆_pre = min_α_pre(L_val(θ⋆_pre(α_pre), α_pre))
s.t. θ⋆_pre(α_pre) = argmin_θ_pre L_train(θ_pre, α_pre) (6)

Kỹ thuật xấp xỉ bậc nhất được sử dụng để cập nhật θ⋆_pre và α_pre luân phiên bằng gradient descent [52].

Bước 2: cắt tỉa (học m̂ và α⋆_prune)

Để làm cho quá trình tìm kiếm nhận thức về cơ chế thưa thớt, chúng tôi cần giải quyết một bài toán tối ưu hóa hai cấp khác cập nhật luân phiên mặt nạ cắt tỉa và tham số kiến trúc. Tham số mặt nạ cắt tỉa là các giá trị nhị phân. Do đó, học các tham số mặt nạ (m) là một bài toán tối ưu hóa nhị phân thách thức. Chúng tôi giải quyết bài toán tối ưu hóa nhị phân này bằng cách giới thiệu các tham số cắt tỉa điểm thực s và khởi tạo chúng. Sau đó chúng tôi sử dụng SGD để giải quyết bài toán tối ưu hóa và tìm các tham số mặt nạ cắt tỉa điểm thực tốt nhất. Cuối cùng, dựa trên các giá trị của tham số cắt tỉa, chúng tôi chọn k tham số trọng số hàng đầu với các giá trị cao nhất và gán giá trị một cho chúng.

Bước này nhằm học đồng thời tham số kiến trúc α_prune và tham số mặt nạ m̂ để xem xét tính thưa thớt trong việc học tham số kiến trúc. Do đó, chúng tôi sử dụng một bài toán tối ưu hóa hai cấp khác:

α⋆_prune = min_α_prune(L_val(θ⋆_pre⊙m̂(α_prune), α_prune))
s.t. ŝ(α_prune) = argmin_s L_prune(θ⋆_pre, α_prune, s),
m̂(α_prune) = 1(|ŝ(α_prune)|>|ŝ(α_prune)|_k) (7)

tương tự như bước 1, phương pháp xấp xỉ bậc nhất được sử dụng để cập nhật luân phiên m̂ và α_prune bằng gradient descent.

Bước 3: tinh chỉnh (học θ̂)

Trong bước tinh chỉnh, chúng tôi cập nhật các tham số trọng số khác không bằng SGD cho kiến trúc thưa thớt tốt nhất để cải thiện độ chính xác mạng (Công thức 8).

θ̂^(t+1) = θ̂^t - η_θ̂∇_θ̂L_fine-tune(θ̂^t⊙m̂, α⋆_prune) (8)

trong đó η_θ̂ và L_fine-tune biểu thị tốc độ học và hàm mất mát cho bước tinh chỉnh.

Chúng tôi cho thấy rằng thuật toán tối ưu hóa ba bước được đề xuất có thể giải quyết bài toán hai cấp phức tạp trong Công thức 5 và tìm các tham số kiến trúc tối ưu với hiệu suất tổng quát hóa cao hơn cho mạng thưa thớt. Hình 7 so sánh các đường cong học của DASS với DARTS thưa thớt trên bộ dữ liệu CIFAR-10. Như đã thấy, thuật toán tối ưu hóa DASS giảm đáng kể mất mát xác thực cho mạng thưa thớt. Hình 8 so sánh hành vi của khoảng cách tổng quát hóa (độ chính xác huấn luyện trừ kiểm tra) cho DASS và DARTS thưa thớt. DASS có khoảng cách tổng quát hóa thấp hơn (lên đến 22%), cho thấy DASS điều chỉnh tốt hơn mất mát xác thực qua tất cả các epoch so với DARTS thưa thớt. Thuật toán 1 phác thảo DASS của chúng tôi cho tìm kiếm kiến trúc neural khả vi phân cho mạng neural thưa thớt.

6 Thí nghiệm

6.1 Thiết lập Thí nghiệm

1) BỘ DỮ LIỆU: Để đánh giá DASS, chúng tôi sử dụng các bộ dữ liệu phân loại công khai CIFAR-10 [39] và ImageNet [40]. Cho quá trình tìm kiếm, chúng tôi chia bộ dữ liệu CIFAR-10 thành 30k điểm dữ liệu cho huấn luyện và 30k cho xác thực. Chúng tôi chuyển các ô được học tốt nhất trên CIFAR-10 sang ImageNet [52] và huấn luyện lại mạng thưa thớt cuối cùng từ đầu.

2) Chi tiết về Tìm kiếm Mạng: Chúng tôi tạo một mạng với 16 kênh ban đầu và tám ô. Mỗi ô bao gồm bảy nút được trang bị một phép toán nối theo chiều sâu như nút đầu ra. Các phép toán SparseConv tuân theo thứ tự ReLU + SparseConv + Batch Normalization. Chúng tôi huấn luyện mạng bằng SGD trong 50 epoch với kích thước batch 64 trong bước tiền huấn luyện DASS. Sau đó, chúng tôi cập nhật giá trị của tham số cắt tỉa và kiến trúc trong 20 epoch trong bước cắt tỉa DASS. Cuối cùng, chúng tôi tinh chỉnh mạng trong 200 epoch. Tốc độ học ban đầu cho DASS trong các bước tiền huấn luyện, cắt tỉa, và tinh chỉnh lần lượt là 0.025, 0.1, và 0.01. Trong thí nghiệm của chúng tôi, chúng tôi sử dụng lịch trình học cosine annealing [59]. Chúng tôi sử dụng weight decay=3×10^-4 và momentum=0.9 trong tất cả các bước. Quá trình tìm kiếm mất ≈3 GPU-ngày trên một NVIDIA® RTX A4000 tạo ra 4.35 Kg CO2. Chúng tôi so sánh thiết kế kiến trúc thưa thớt bằng phương pháp của chúng tôi, DASS, với các mạng dày đặc và thưa thớt khác. NAS-Bench-101 [78] và NAS-Bench-201 [16] là ví dụ về các benchmark đánh giá thuật toán NAS. Chúng bao gồm nhiều thiết kế dày đặc và hiệu suất tương ứng của chúng. Do chúng không hỗ trợ kiến trúc thưa thớt, chúng tôi không thể đánh giá DASS bằng các benchmark này. Tạo benchmark thưa thớt để đánh giá thuật toán NAS là một đề xuất cho công việc tương lai.

3) Các biến thể DASS và Cấu hình Phần cứng: Bảng 2 cung cấp chi tiết cấu hình của các biến thể DASS. Mỗi biến thể được xây dựng bằng cách xếp chồng số lượng ô DASS khác nhau và các kênh đầu ra của lớp đầu tiên để tạo ra mạng cho các ngân sách tài nguyên khác nhau. Bảng 3 trình bày thông số kỹ thuật của các thiết bị phần cứng được sử dụng để đánh giá hiệu suất của DASS tại thời gian suy luận.

Bảng 2: Cấu hình của các biến thể DASS. #Ô: số lượng ô được xếp chồng. #Kênh: số lượng kênh đầu ra cho phép toán SparseConv đầu tiên.

DASS | CIFAR-10 | ImageNet
 | Tiny | Small | Medium | Large | Small | Medium | Large
#Ô | 16 | 20 | 12 | 14 | 14 | 15 | 16
#Kênh | 30 | 36 | 86 | 108 | 48 | 86 | 128

Bảng 3: Thông số Kỹ thuật Phần cứng.

Nền tảng | Thông số kỹ thuật | Giá trị
Tìm kiếm & Huấn luyện | GPU | NVIDIA® RTX A4000 (735 MHz)
 | GPU Memory | 16 GB GDDR6
 | GPU Compiler | cuDNN version 11.1
 | System Memory | 64 GB
 | Operating System | Ubuntu 18.04
 | CO2 Emission/Day† | 1.45 Kg
Phần cứng Thực | Embedded GPU | NVIDIA® Jetson TX2 (735 MHz)
 |  | 256 CUDA Cores
 |  | NVIDIA® Quadro M1200 (735 MHz)
 |  | 640 CUDA Cores
 | Embedded CPU | ARM Cortex™-A7 (1.2 GHz)
 |  | 4/4 (Cores/Total Thread)
 |  | Intel® i5-3210M Mobile CPU
 |  | 5/4 (Cores/Total Thread)
Ước tính‡ | Xiaomi Mi9 GPU | Adreno 640 GPU (750 MHz)
 |  | 986 GFLOPs FP32 (Single Precision)
 | Myriad VPU | Intel Movidius NCS2 (700 MHz)
 |  | 28-nm Co-processor

†Tính toán sử dụng framework ML CO2 impact: https://mlco2.github.io/impact/ [42]
‡Ước tính Hiệu suất sử dụng framework nn-Meter [82].

6.2 DASS So sánh với Mạng dày đặc

Bảng 4 so sánh hiệu suất của DASS với các DNN tiên tiến và thực tiễn. Chúng tôi chọn kiến trúc có độ chính xác cao nhất, DrNAS [12], làm baseline để so sánh tỷ lệ nén. So với DrNAS [12], DASS-Large cung cấp tỷ lệ nén mạng cao hơn 37.73× và 29.23× trong khi đạt độ chính xác tương đương (mất mát độ chính xác dưới 2.5%) trên bộ dữ liệu CIFAR-10 và ImageNet. So với mạng được thiết kế thủ công tốt nhất [65] trên CIFAR-10 (CCT-6/3x1), DASS-Large giảm đáng kể tham số của mạng bằng 29.9× với việc cung cấp độ chính xác hơi cao hơn.

6.3 DASS So sánh với Mạng thưa thớt

Vì chúng tôi tập trung vào cải thiện độ chính xác của mạng thưa thớt ở tỷ lệ cắt tỉa cực kỳ cao, chúng tôi so sánh DASS với các mạng thưa thớt khác với phương pháp cắt tỉa không có cấu trúc ở tỷ lệ cắt tỉa 99% (Bảng 5). So với DARTS thưa thớt, DASS-Small đạt độ chính xác top-1 cao hơn 7.81% và 7.81% với giảm kích thước mạng 1.23× và 1.05× trên bộ dữ liệu CIFAR-10 và ImageNet tương ứng. Điều này cho thấy rằng thiết kế mạng dựa trên không gian tìm kiếm mới và hàm mục tiêu thưa thớt tìm kiến trúc thưa thớt tốt hơn. So với ResNet-18 thưa thớt trên bộ dữ liệu CIFAR-10, chúng tôi cung cấp độ chính xác cao hơn 1.56% và 4.7% với giảm kích thước mạng 2.08× và 1.05× cho DASS-Medium và DASS-Large tương ứng. So với ResNet-18 thưa thớt trên bộ dữ liệu ImageNet, DASS-Medium cung cấp độ chính xác cao hơn 0.76% với giảm kích thước mạng 1.42×. MCUNET [50] là một mạng neural nhẹ cho vi điều khiển. Nó được thiết kế bằng cơ chế tìm kiếm kiến trúc neural nhỏ. So với MCUNET trên bộ dữ liệu ImageNet, DASS-Large cung cấp độ chính xác cao hơn 1% với giảm kích thước mạng 2.89×. Kết quả này cho thấy rằng chỉ tối ưu hóa kích thước của các bộ lọc mà không xem xét tính thưa thớt không thể tạo ra kiến trúc tốt nhất. DASS trực tiếp tìm kiếm các phép toán tốt nhất trong phiên bản thưa thớt để thiết kế mạng nhẹ hiệu suất cao. Chúng tôi có thể kết luận rằng DASS tăng độ chính xác của mạng thưa thớt ở tỷ lệ cắt tỉa cao so với các mạng dựa trên NAS và được thiết kế thủ công.

Bảng 4: So sánh phương pháp DASS với các mạng dày đặc tiên tiến trên bộ dữ liệu CIFAR-10 và ImageNet.

Kiến trúc | Năm | CIFAR-10 |  |  | ImageNet |  |  | 
 |  | Phương pháp Tìm kiếm | Top-1 Acc.(%) | #Params (×10^6) | #Params Compression | Top-1 Acc.(%) | Top-5 Acc.(%) | #Params (×10^6) | #Params Compression
ResNet-18‡[27] | 2016 | - | 91.0 | 11.1 | -2.77× | 72.33 | 91.80 | 11.7 | -2.05×
PDO-eConv [69] | 2020 | - | 94.62 | 0.37 | +10.81× | - | - | - | -
FlexTCN-7 [65] | 2021 | - | 92.2 | 0.67 | +5.97× | - | - | - | -
CCT-6/3x1 [65] | 2021 | - | 95.29 | 3.17 | +1.26× | - | - | - | -
MomentumNet [66] | 2021 | - | 95.18 | 11.1 | -2.77× | - | - | - | -
DARTS (1st order) [52] | 2018 | gradient | 96.86 | 3.3 | +1.21× | - | - | - | -
DARTS (2nd order) [52] | 2018 | gradient | 97.24 | 3.3 | +1.21× | 74.3 | 91.3 | 4.7 | +1.21×
SGAS (Cri 1. avg) [46] | 2020 | gradient | 97.34 | 3.7 | +1.08× | 75.9 | 92.7 | 5.4 | +1.05×
SDARTS-RS [11] | 2020 | gradient | 97.39 | 3.4 | +1.17× | 75.8 | 92.8 | 3.4 | +1.67×
DrNAS [12] | 2020 | gradient | 97.46 | 4.0 | 1.0× | 76.3 | 92.9 | 5.7 | 1.0×
DASS-Small | 2022 | gradient | 89.06 | 0.017 | +235.29× | 46.48 | 68.36 | 0.029 | +196.55×
DASS-Medium | 2022 | gradient | 92.18 | 0.054 | +74.07× | 68.34 | 82.24 | 0.082 | +69.51×
DASS-Large | 2022 | gradient | 95.31 | 0.106 | +37.73× | 73.83 | 85.94 | 0.195 | +29.23×

†Baseline để so sánh tỷ lệ nén #params là DrNAS [12] là kiến trúc chính xác nhất.
‡Kết quả ResNet-18 được huấn luyện tại https://github.com/facebook/fb.resnet.torch (10 tháng 7, 2018).

Bảng 5: So sánh phương pháp DASS với mạng thưa thớt trên bộ dữ liệu CIFAR-10 và ImageNet.

Kiến trúc | CIFAR-10 |  |  |  | ImageNet |  |  |  | 
 | Top-1 Acc. (%) | #Params (×10^3) | Compression Rate† | NID‡ | Top-1 Acc. (%) | Top-5 Acc. (%) | #Params (×10^3) | Compression Rate† | NID‡
DARTS sparse [52] | 81.25 | 21.0 | 100.47× | 3.86 | 38.67 | 61.33 | 33.0 | 100× | 1.11
MobileNet-v2 sparse [67] | 73.44 | 22.2 | 95.04× | 3.30 | 17.97 | 36.72 | 34.87 | 94.63× | 0.515
ResNet-18 sparse [27] | 90.62 | 111.6 | 18.90× | 0.81 | 67.58 | 80.86 | 116.84 | 28.24× | 0.578
EfficientNet sparse [71] | 79.69 | 202.3 | 10.43× | 0.39 | - | - | - | - | -
MCUNET [50] | 89.7 | 210.1 | 15.70 | 0.42 | 72.34 | 84.86 | 562.64 | 5.86× | 0.128
DASS-Small | 89.06 | 17.0 | 124.11× | 5.23 | 46.48 | 68.36 | 28.94 | 114.02× | 1.606
DASS-Medium | 92.18 | 53.65 | 39.32× | 1.71 | 68.34 | 82.24 | 81.95 | 40.26× | 0.841
DASS-Large | 95.31 | 105.5 | 20× | 0.90 | 73.83 | 85.94 | 194.6 | 16.95× | 0.38

†Baseline để so sánh tỷ lệ nén là kiến trúc DARTS dày đặc và full-precision.
‡NID = Accuracy/#Parameters [4]. NID đo lường mức độ hiệu quả mỗi mạng sử dụng tham số của nó.

6.4 Đánh giá DASS với Các Tỷ lệ Cắt tỉa Khác nhau

Bảng 6 so sánh DASS và phương pháp DARTS thưa thớt với ba tỷ lệ cắt tỉa khác nhau bao gồm 90%, 95%, và 99% trên bộ dữ liệu CIFAR-10. DASS đạt độ chính xác cao hơn 1.57%, 1.04%, và 7.8% với giảm kích thước mạng 7%, 6.9%, và 23% so với DARTS thưa thớt ở tỷ lệ cắt tỉa 90%, 95%, và 99% tương ứng. Do đó, DASS hiệu quả hơn đáng kể ở tỷ lệ cắt tỉa cực kỳ cao (99%) so với tỷ lệ cắt tỉa thấp hơn (90%).

6.5 DASS So sánh với Các Phương pháp Cắt tỉa Khác

Bảng 7 so sánh DASS với các thuật toán cắt tỉa tiên tiến. Kết quả cho thấy rằng DASS vượt trội hơn các thuật toán cắt tỉa khác với các kiến trúc xương sống khác nhau trên bộ dữ liệu CIFAR-10 và ImageNet. Trên CIFAR-10, DASS-Large cho thấy độ chính xác cao hơn 1.6% và giảm kích thước mạng 3.8× so với kết quả chính xác nhất được cung cấp bởi TAS Pruning [15]. DASS-Large cũng cung cấp cải thiện độ chính xác 4.68% với giảm kích thước mạng 38.14× so với TAS Pruning [15] trên ImageNet. Xét đến hiệu quả cao hơn của DASS so với các phương pháp cắt tỉa khác, chúng tôi có thể kết luận rằng phương pháp cắt tỉa không phải là lý do duy nhất cho hiệu quả của DASS và nó độc lập với thuật toán cắt tỉa.

Bảng 6: Đánh giá hiệu quả của DASS ở các tỷ lệ cắt tỉa khác nhau.

Kiến trúc | 90% |  | 95% |  | 99% | 
 | Accuracy | #Params (×10^3) | Accuracy | #Params (×10^3) | Accuracy | #Params (×10^3)
DARTS sparse | 95.31% | 421 | 93.75% | 210.5 | 81.25% | 21.0
DASS-Small | 96.88% | 391 | 94.79% | 196.75 | 89.06% | 17.0

Bảng 7: So sánh DASS với các thuật toán cắt tỉa khác.

Phương pháp Cắt tỉa | CIFAR-10 |  |  | ImageNet |  |  | 
 | Backbone Arch. | Top-1 Acc.(%) | #Params (×10^6) | Backbone Arch. | Top-1 Acc.(%) | Top-5 Acc.(%) | #Params (×10^6)
SFP [29] | ResNet-20 | 92.08 | 0.269 | ResNet-18 | 67.10 | 87.78 | 6.46
FPGM [30] |  | 92.31 | 0.269 |  | 68.41 | 88.48 | 6.46
TAS Pruning [15] |  | 93.16 | 0.232 |  | 69.15 | 88.48 | 7.40
DASS-Small | - | 89.06 | 0.017 | - | 46.48 | 68.36 | 0.029
DASS-Medium | - | 92.18 | 0.054 | - | 68.34 | 82.24 | 0.082
DASS-Large | - | 95.31 | 0.106 | - | 73.83 | 85.94 | 0.194

6.6 DASS So sánh với Mạng Lượng tử hóa

Lượng tử hóa mạng nổi lên như một hướng nghiên cứu hứa hẹn để giảm tính toán của mạng neural. Gần đây, [38, 7, 55] đề xuất tích hợp cơ chế lượng tử hóa vào quy trình NAS khả vi phân để cải thiện hiệu suất của mạng lượng tử hóa. Bảng 8 so sánh DASS với kết quả tốt nhất của mạng lượng tử hóa dựa trên NAS. Tỷ lệ nén được tính như ∑^L_{l=1}#W_l×32/∑^L_{l=1}#W^t_l×q trong đó #W_l và #W^t_l là số lượng trọng số trong lớp l cho mạng full-precision (32-bit) và mạng lượng tử hóa với độ phân giải q-bit [55]. DASS-Medium đạt độ chính xác cao hơn 0.24% và 3.24% và tỷ lệ nén cao hơn đáng kể bằng 2.7× và 4.24× so với TAS [55] là mạng lượng tử hóa chính xác nhất trên bộ dữ liệu CIFAR-10 và ImageNet.

Bảng 8: So sánh phương pháp DASS với mạng lượng tử hóa trên CIFAR-10.

Kiến trúc | CIFAR-10 |  |  |  | ImageNet |  |  |  | 
 | #bits (W/A)‡ | Top-1 Acc.(%) | #Params (×10^6) | Compression Rate† | Top-1 Acc.(%) | Top-5 Acc.(%) | #Params (×10^6) | Compression Rate†
Binary NAS (A) [38] | 1/1 | 90.66 | 2.4 | 44.0× | 57.69 | 79.89 | 5.57 | 32.74×
TAS [55] | 2/2 | 91.94 | 2.4 | 22.0× | 65.1 | 86.3 | 5.57 | 16.37×
DASS-Small | 32/32 | 89.06 | 0.017 | 194.11× | 46.48 | 68.36 | 0.029 | 196.55×
DASS-Medium | 32/32 | 92.18 | 0.054 | 61.11× | 68.34 | 82.24 | 0.082 | 69.51×
DASS-Large | 32/32 | 95.31 | 0.106 | 31.13× | 73.83 | 85.94 | 0.194 | 29.38×

†Baseline để so sánh là DARTS full-precision với 3.3M và 5.7M tham số cho CIFAR-10 và ImageNet.
‡(Weights/Activation Function).

6.7 Kết quả Hiệu suất Phần cứng của DASS

Chúng tôi nghiên cứu rộng rãi hiệu quả của DASS trong bối cảnh hiệu quả phần cứng bằng cách tính thời gian suy luận (độ trễ) của các mạng thưa thớt tiên tiến khác nhau cho một loạt các thiết bị biên hạn chế tài nguyên trên bộ dữ liệu CIFAR-10 (Hình 9). Kích thước batch bằng 1 cho tất cả thí nghiệm. Đáng chú ý là chúng tôi không sử dụng bất kỳ kỹ thuật đơn giản hóa nào, chẳng hạn như [5], để nén các bộ lọc thưa thớt bằng cách hợp nhất các tham số trọng số. Kết quả của chúng tôi tiết lộ rằng Pareto-frontier của DASS luôn vượt trội hơn tất cả các đối thủ khác với biên độ đáng kể, đặc biệt trên CPU có tính song song rất hạn chế. DASS-Tiny là mạng nhanh nhất cải thiện độ chính xác từ 73.44% của MobileNet-v2 lên 81.35% (cải thiện +7.91%) và tăng tốc suy luận lên đến 3.87×. Quan trọng hơn, DASS-Tiny chạy nhanh hơn nhiều so với DARTS thưa thớt bằng 1.67-4.74× với độ chính xác hơi tốt hơn. So với ResNet-18 thưa thớt là mạng gần nhất với DASS về độ chính xác, DASS-Medium cung cấp cải thiện độ chính xác 1.46% và tăng tốc lên đến 1.94× trên phần cứng.

6.8 Phân tích Khả năng Phân biệt của DASS

Chúng tôi sử dụng phương pháp t-distributed stochastic neighbor embedding (t-SNE) [72] để trực quan hóa các biên quyết định của kiến trúc dày đặc hiệu suất cao được thiết kế bởi DARTS, DARTS thưa thớt (kiến trúc DART dày đặc thưa thớt với cắt tỉa), và DASS (kiến trúc thưa thớt của chúng tôi) trên bộ dữ liệu CIFAR-10. Hình 10 minh họa các biên quyết định phân loại cho mỗi mạng. Theo kết quả, DASS có khả năng phân biệt cao hơn DARTS thưa thớt, và DASS với tỷ lệ cắt tỉa 99% hoạt động rất tương tự với kiến trúc DARTS dày đặc và hiệu suất cao.

6.9 Phân tích Định tính của Ô được Tìm kiếm

Hình 11 cho thấy các ô tốt nhất được tìm kiếm bởi DASS-Small. Một phát hiện thú vị là, đối với ô bình thường, DASS-Small có xu hướng chọn phép toán SparseConv với kích thước kernel lớn hơn (5×5), cung cấp nhiều ứng viên cắt tỉa hơn để tối ưu hóa mặt nạ cắt tỉa. DASS-Small có xu hướng tận dụng các phép toán max-pooling trong ô giảm thay vì các phép toán avg-pooling. Điều này là do phép toán max-pooling có khả năng trích xuất đặc trưng cao hơn với các bộ lọc thưa thớt [79].

6.10 Phân tích Tính Tái tạo

Để xác minh tính tái tạo của kết quả, quy trình tìm kiếm DASS-Small được chạy năm lần với các hạt giống ngẫu nhiên khác nhau. Hình 6.10 vẽ trung bình của các biến thiên độ chính xác và mất mát cũng như các vùng bóng để chỉ ra khoảng tin cậy. Kết quả cho thấy rằng, trong khi khoảng tin cậy rộng lúc đầu, trung bình của nhiều lần chạy hội tụ đến các kiến trúc neural với hiệu suất tương tự với độ lệch chuẩn trung bình (STDEV) là 2.22%.

7 Kết luận

Chúng tôi đề xuất DASS, một phương pháp tìm kiếm kiến trúc khả vi phân, để thiết kế các kiến trúc thưa thớt hiệu suất cao cho DNN. DASS cải thiện đáng kể hiệu suất của các kiến trúc thưa thớt bằng cách đề xuất: (i) một không gian tìm kiếm mới chứa các phép toán thưa thớt tham số; và (ii) một mục tiêu tìm kiếm mới nhất quán với tính thưa thớt và cơ chế cắt tỉa. Kết quả thực nghiệm của chúng tôi tiết lộ rằng các kiến trúc thưa thớt được học vượt trội hơn các kiến trúc được sử dụng trong các phương pháp tiên tiến trên cả bộ dữ liệu CIFAR-10 và ImageNet. Về lâu dài, chúng tôi dự báo rằng các mạng được thiết kế của chúng tôi có thể đóng góp hiệu quả vào mục tiêu của trí tuệ nhân tạo xanh bằng cách sử dụng hiệu quả các thiết bị hạn chế tài nguyên như các giải pháp tăng tốc biên. Một hướng hứa hẹn cho công việc tương lai là thiết kế một mạng thưa thớt cũng mạnh mẽ chống lại các cuộc tấn công đối kháng.

Tài liệu tham khảo

[1] Hervé Abdi. 2007. Hệ số tương quan hạng Kendall. Bách khoa toàn thư về Đo lường và Thống kê. Sage, Thousand Oaks, CA (2007), 508–510.

[2] Kambiz Azarian, Yash Bhalgat, Jinwon Lee, và Tijmen Blankevoort. 2020. Cắt tỉa ngưỡng đã học. arXiv preprint arXiv:2003.00075 (2020).

[3] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, và Quoc Le. 2018. Hiểu và đơn giản hóa tìm kiếm kiến trúc một lần. Trong Hội nghị Quốc tế về Học máy. PMLR, 550–559.

[4] Simone Bianco, Remi Cadene, Luigi Celona, và Paolo Napoletano. 2018. Phân tích benchmark của các kiến trúc mạng neural sâu đại diện. IEEE Access 6 (2018), 64270–64277.

[5] Andrea Bragagnolo và Carlo Alberto Barbano. 2022. Simplify: Một thư viện Python để tối ưu hóa mạng neural được cắt tỉa. SoftwareX 17 (2022), 100907.

[6] Andrew Brock, Theodore Lim, James M Ritchie, và Nick Weston. 2017. Smash: tìm kiếm kiến trúc mô hình một lần thông qua hypernetworks. arXiv preprint arXiv:1708.05344 (2017).

[7] Adrian Bulat, Brais Martinez, và Georgios Tzimiropoulos. 2020. Bats: Tìm kiếm kiến trúc nhị phân. Trong Computer Vision–ECCV 2020: Hội nghị Châu Âu lần thứ 16, Glasgow, UK, 23–28 tháng 8, 2020, Kỷ yếu, Phần XXIII 16. Springer, 309–325.

[8] Rebekka Burkholz, Nilanjana Laha, Rajarshi Mukherjee, và Alkis Gotovos. 2021. Về sự tồn tại của vé số may mắn toàn cầu. arXiv preprint arXiv:2111.11146 (2021).

[9] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. 2019. Một lần cho tất cả: Huấn luyện một mạng và chuyên biệt hóa nó để triển khai hiệu quả. arXiv preprint arXiv:1908.09791 (2019).

[10] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Yang Zhang, Shiyu Chang, và Zhangyang Wang. 2022. Vé số thắng kép hiệu quả dữ liệu từ tiền huấn luyện mạnh mẽ. Trong Hội nghị Quốc tế về Học máy. PMLR, 3747–3759.

[11] Xiangning Chen và Cho-Jui Hsieh. 2020. Ổn định tìm kiếm kiến trúc khả vi phân thông qua điều chỉnh dựa trên nhiễu loạn. Trong Hội nghị Quốc tế về Học máy. PMLR, 1554–1565.

[12] Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, và Cho-Jui Hsieh. 2020. Drnas: Tìm kiếm kiến trúc neural Dirichlet. arXiv preprint arXiv:2006.10355 (2020).

[13] Enmao Diao, Ganghua Wang, Jiawei Zhan, Yuhong Yang, Jie Ding, và Vahid Tarokh. 2023. Cắt tỉa Mạng Neural Sâu từ Góc độ Thưa thớt. arXiv preprint arXiv:2302.05601 (2023).

[14] Yadong Ding, Yu Wu, Chengyue Huang, Siliang Tang, Fei Wu, Yi Yang, Wenwu Zhu, và Yueting Zhuang. 2022. NAP: Tìm kiếm Kiến trúc Neural với Cắt tỉa. Neurocomputing (2022).

[15] Xuanyi Dong và Yi Yang. 2019. Cắt tỉa mạng thông qua tìm kiếm kiến trúc có thể biến đổi. Advances in Neural Information Processing Systems 32 (2019).

[16] Xuanyi Dong và Yi Yang. 2020. Nas-bench-201: Mở rộng phạm vi của tìm kiếm kiến trúc neural có thể tái tạo. arXiv preprint arXiv:2001.00326 (2020).

[17] Jonathan Frankle và Michael Carbin. 2018. Giả thuyết vé số may mắn: Tìm kiếm mạng neural thưa thớt, có thể huấn luyện. arXiv preprint arXiv:1803.03635 (2018).

[18] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, và Kurt Keutzer. 2021. AI và Rào cản Bộ nhớ. RiseLab Medium Post (2021).

[19] Ian Goodfellow, Yoshua Bengio, và Aaron Courville. 2016. Học Sâu. MIT Press. http://www.deeplearningbook.org.

[20] Jianping Gou, Baosheng Yu, Stephen J Maybank, và Dacheng Tao. 2021. Chưng cất kiến thức: Một khảo sát. International Journal of Computer Vision 129, 6 (2021), 1789–1819.

[21] Yushuo Guan, Ning Liu, Pengyu Zhao, Zhengping Che, Kaigui Bian, Yanzhi Wang, và Jian Tang. 2022. Dais: Cắt tỉa kênh tự động thông qua tìm kiếm chỉ báo annealing khả vi phân. IEEE Transactions on Neural Networks and Learning Systems (2022).

[22] Shupeng Gui, Haotao N Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, và Ji Liu. 2019. Nén mô hình với tính mạnh mẽ đối kháng: Một khung tối ưu hóa thống nhất. Advances in Neural Information Processing Systems 32 (2019), 1285–1296.

[23] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, và Jian Sun. 2020. Tìm kiếm kiến trúc neural một đường với lấy mẫu đồng nhất. Trong European Conference on Computer Vision. Springer, 544–560.

[24] Marwa El Halabi, Suraj Srinivas, và Simon Lacoste-Julien. 2022. Cắt tỉa có cấu trúc hiệu quả dữ liệu thông qua tối ưu hóa submodular. arXiv preprint arXiv:2203.04940 (2022).

[25] Song Han, Jeff Pool, John Tran, và William J Dally. 2015. Học cả trọng số và kết nối cho mạng neural hiệu quả. arXiv preprint arXiv:1506.02626 (2015).

[26] Babak Hassibi và David G Stork. 1993. Đạo hàm bậc hai cho cắt tỉa mạng: Phẫu thuật viên não tối ưu. Morgan Kaufmann.

[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. 2016. Học tàn dư sâu cho nhận dạng hình ảnh. Trong Kỷ yếu hội nghị IEEE về thị giác máy tính và nhận dạng mẫu. 770–778.

[28] Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, và Yi Yang. 2020. Học tiêu chí cắt tỉa bộ lọc cho tăng tốc mạng neural tích chập sâu. Trong Kỷ yếu hội nghị IEEE/CVF về thị giác máy tính và nhận dạng mẫu. 2009–2018.

[29] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, và Yi Yang. 2018. Cắt tỉa bộ lọc mềm để tăng tốc mạng neural tích chập sâu. arXiv preprint arXiv:1808.06866 (2018).

[30] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. 2019. Cắt tỉa bộ lọc thông qua trung vị hình học để tăng tốc mạng neural tích chập sâu. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 4340–4349.

[31] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Chưng cất kiến thức trong mạng neural. arXiv preprint arXiv:1503.02531 2, 7 (2015).

[32] Weijun Hong, Guilin Li, Weinan Zhang, Ruiming Tang, Yunhe Wang, Zhenguo Li, và Yong Yu. 2021. Dropnas: Dropout phép toán nhóm cho tìm kiếm kiến trúc khả vi phân. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 29 về Hội nghị Quốc tế về Trí tuệ Nhân tạo. 2326–2332.

[33] Ramtin Hosseini, Xingyi Yang, và Pengtao Xie. 2021. DSRNA: Tìm kiếm Khả vi phân các Kiến trúc Neural Mạnh mẽ. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 6196–6205.

[34] Andrew Hundt, Varun Jain, và Gregory D Hager. 2019. sharpdarts: Tìm kiếm kiến trúc khả vi phân nhanh hơn và chính xác hơn. arXiv preprint arXiv:1903.09900 (2019).

[35] Yesmina Jaafra, Jean Luc Laurent, Aline Deruyver, và Mohamed Saber Naceur. 2019. Học tăng cường cho tìm kiếm kiến trúc neural: Một đánh giá. Image and Vision Computing 89 (2019), 57–66.

[36] Max Jaderberg, Andrea Vedaldi, và Andrew Zisserman. 2014. Tăng tốc mạng neural tích chập với mở rộng hạng thấp. arXiv preprint arXiv:1405.3866 (2014).

[37] Xiaojie Jin, Jiang Wang, Joshua Slocum, Ming-Hsuan Yang, Shengyang Dai, Shuicheng Yan, và Jiashi Feng. 2019. Rc-darts: Tìm kiếm kiến trúc khả vi phân hạn chế tài nguyên. arXiv preprint arXiv:1912.12814 (2019).

[38] Dahyun Kim, Kunal Pratap Singh, và Jonghyun Choi. 2020. Học kiến trúc cho mạng nhị phân. Trong European Conference on Computer Vision. Springer, 575–591.

[39] Alex Krizhevsky, Vinod Nair, và Geoffrey Hinton. 2009. Bộ dữ liệu cifar-10 và cifar-100. URl: https://www.cs.toronto.edu/kriz/cifar.html 6, 1 (2009), 1.

[40] Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. 2012. Phân loại imagenet với mạng neural tích chập sâu. Advances in neural information processing systems 25 (2012), 1097–1105.

[41] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, và Ali Farhadi. 2020. Tái tham số hóa Trọng số Ngưỡng Mềm cho Thưa thớt Có thể Học. Trong Kỷ yếu Hội nghị Quốc tế về Học máy.

[42] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, và Thomas Dandres. 2019. Định lượng phát thải carbon của học máy. arXiv preprint arXiv:1910.09700 (2019).

[43] Kevin Alexander Laube và Andreas Zell. 2019. Cắt tỉa và thay thế nas. Trong Hội nghị IEEE Quốc tế lần thứ 18 năm 2019 về Ứng dụng Học máy (ICMLA). IEEE, 915–921.

[44] Yann LeCun, John S Denker, và Sara A Solla. 1990. Tổn thương não tối ưu. Trong Advances in neural information processing systems. 598–605.

[45] Hayeon Lee, Sewoong Lee, Song Chong, và Sung Ju Hwang. 2021. HELP: Dự đoán Độ trễ Hiệu quả Thích ứng Phần cứng cho NAS thông qua Meta-Learning. arXiv preprint arXiv:2106.08630 (2021).

[46] Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali Thabet, và Bernard Ghanem. 2020. Sgas: Tìm kiếm kiến trúc tham lam tuần tự. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 1620–1630.

[47] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. 2016. Cắt tỉa bộ lọc cho convnets hiệu quả. arXiv preprint arXiv:1608.08710 (2016).

[48] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, và Wei Liu. 2019. Nén mạng neural tích chập thông qua bộ lọc tích chập được phân tích. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 3977–3986.

[49] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, và Xiaotong Zhang. 2021. Cắt tỉa và lượng tử hóa để tăng tốc mạng neural sâu: Một khảo sát. Neurocomputing 461 (2021), 370–403.

[50] Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, và Song Han. 2020. Mcunet: Học sâu nhỏ trên thiết bị iot. arXiv preprint arXiv:2007.10319 (2020).

[51] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, và Song Han. 2022. Huấn luyện trên thiết bị dưới 256kb bộ nhớ. arXiv preprint arXiv:2206.15472 (2022).

[52] Hanxiao Liu, Karen Simonyan, và Yiming Yang. 2018. Darts: Tìm kiếm kiến trúc khả vi phân. arXiv preprint arXiv:1806.09055 (2018).

[53] Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, và Kay Chen Tan. 2021. Một khảo sát về tìm kiếm kiến trúc neural tiến hóa. IEEE Transactions on Neural Networks and Learning Systems (2021).

[54] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. 2018. Suy nghĩ lại về giá trị của cắt tỉa mạng. arXiv preprint arXiv:1810.05270 (2018).

[55] Mohammad Loni, Hamid Mousavi, Mohammad Riazati, Masoud Daneshtalab, và Mikael Sjödin. 2022. TAS: Tìm kiếm Kiến trúc Neural Ba phân cho Thiết bị Biên Hạn chế Tài nguyên. Trong Design, Automation & Test in Europe Conference & Exhibition DATE'22, 14 tháng 3, 2022, Antwerp, Belgium. IEEE. http://www.es.mdh.se/publications/6351-

[56] Mohammad Loni, Sima Sinaei, Ali Zoljodi, Masoud Daneshtalab, và Mikael Sjödin. 2020. DeepMaker: Một khung tối ưu hóa đa mục tiêu cho mạng neural sâu trong hệ thống nhúng. Microprocessors and Microsystems 73 (2020), 102989.

[57] Mohammad Loni, Ali Zoljodi, Amin Majd, Byung Hoon Ahn, Masoud Daneshtalab, Mikael Sjödin, và Hadi Esmaeilzadeh. 2021. FastStereoNet: Một Tìm kiếm Kiến trúc Neural Nhanh để Cải thiện Suy luận Ước tính Chênh lệch trên Nền tảng Hạn chế Tài nguyên. IEEE Transactions on Systems, Man, and Cybernetics: Systems (2021).

[58] Mohammad Loni, Ali Zoljodi, Sima Sinaei, Masoud Daneshtalab, và Mikael Sjödin. 2019. Neuropower: Thiết kế kiến trúc mạng neural tích chập tiết kiệm năng lượng cho hệ thống nhúng. Trong Hội nghị quốc tế về mạng neural nhân tạo. Springer, 208–222.

[59] Ilya Loshchilov và Frank Hutter. 2016. Sgdr: Gradient descent ngẫu nhiên với khởi động lại ấm. arXiv preprint arXiv:1608.03983 (2016).

[60] Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja Giryes, và Lihi Zelnik. 2020. Asap: Tìm kiếm kiến trúc, annealing và cắt tỉa. Trong Hội nghị Quốc tế về Thống kê Trí tuệ Nhân tạo. PMLR, 493–503.

[61] Zhuwei Qin, Fuxun Yu, Chenchen Liu, và Xiang Chen. 2018. Cách mạng neural tích chập nhìn thế giới - Một khảo sát về các phương pháp trực quan hóa mạng neural tích chập. arXiv preprint arXiv:1804.11191 (2018).

[62] Esteban Real, Alok Aggarwal, Yanping Huang, và Quoc V Le. 2019. Tiến hóa có điều chỉnh cho tìm kiếm kiến trúc phân loại hình ảnh. Trong Kỷ yếu hội nghị aaai về trí tuệ nhân tạo, Tập 33. 4780–4789.

[63] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, và Xin Wang. 2021. Một khảo sát toàn diện về tìm kiếm kiến trúc neural: Thách thức và giải pháp. ACM Computing Surveys (CSUR) 54, 4 (2021), 1–34.

[64] Shaoqing Ren, Kaiming He, Ross Girshick, và Jian Sun. 2015. Faster r-cnn: Hướng tới phát hiện đối tượng thời gian thực với mạng đề xuất vùng. Advances in neural information processing systems 28 (2015).

[65] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, và Jan C van Gemert. 2021. Flexconv: Tích chập kernel liên tục với kích thước kernel khả vi phân. arXiv preprint arXiv:2110.08059 (2021).

[66] Michael E Sander, Pierre Ablin, Mathieu Blondel, và Gabriel Peyré. 2021. Mạng neural tàn dư động lượng. arXiv preprint arXiv:2102.07870 (2021).

[67] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. 2018. Mobilenetv2: Tàn dư đảo ngược và nút cổ chai tuyến tính. Trong Kỷ yếu hội nghị IEEE về thị giác máy tính và nhận dạng mẫu. 4510–4520.

[68] Vikash Sehwag, Shiqi Wang, Prateek Mittal, và Suman Jana. 2020. Hydra: Cắt tỉa mạng neural mạnh mẽ đối kháng. Advances in Neural Information Processing Systems 33 (2020), 19655–19666.

[69] Zhengyang Shen, Lingshen He, Zhouchen Lin, và Jinwen Ma. 2020. Pdo-econvs: Tích chập đẳng biến dựa trên toán tử vi phân từng phần. Trong Hội nghị Quốc tế về Học máy. PMLR, 8697–8706.

[70] Shahid Siddiqui, Christos Kyrkou, và Theocharis Theocharides. 2021. Tìm kiếm Kiến trúc Khả vi phân Nhanh Nhận biết Phép toán và Tôpô. Trong Hội nghị Quốc tế lần thứ 25 năm 2020 về Nhận dạng Mẫu (ICPR). IEEE, 9666–9673.

[71] Mingxing Tan và Quoc Le. 2019. Efficientnet: Suy nghĩ lại về mở rộng mô hình cho mạng neural tích chập. Trong Hội nghị Quốc tế về Học máy. PMLR, 6105–6114.

[72] Laurens Van der Maaten và Geoffrey Hinton. 2008. Trực quan hóa dữ liệu bằng t-SNE. Journal of machine learning research 9, 11 (2008).

[73] Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, và Eftychios Protopapadakis. 2018. Học sâu cho thị giác máy tính: Một đánh giá ngắn gọn. Computational intelligence and neuroscience 2018 (2018).

[74] Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, và Cho-Jui Hsieh. 2021. Suy nghĩ lại về lựa chọn kiến trúc trong NAS khả vi phân. arXiv preprint arXiv:2108.04392 (2021).

[75] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, và Wanli Ouyang. 2022. b-darts: Điều chỉnh phân rã beta cho tìm kiếm kiến trúc khả vi phân. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 10874–10883.

[76] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, và Wanli Ouyang. 2022. beta-DARTS: Điều chỉnh Phân rã Beta cho Tìm kiếm Kiến trúc Khả vi phân. Trong Hội nghị IEEE/CVF 2022 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR). IEEE, 10864–10873.

[77] Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, và Xue Lin. 2019. Tính mạnh mẽ đối kháng so với nén mô hình, hay cả hai?. Trong Kỷ yếu Hội nghị IEEE/CVF Quốc tế về Thị giác Máy tính. 111–120.

[78] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, và Frank Hutter. 2019. Nas-bench-101: Hướng tới tìm kiếm kiến trúc neural có thể tái tạo. Trong Hội nghị Quốc tế về Học máy. PMLR, 7105–7114.

[79] Dingjun Yu, Hanli Wang, Peiqiu Chen, và Zhihua Wei. 2014. Pooling hỗn hợp cho mạng neural tích chập. Trong Hội nghị quốc tế về tập thô và công nghệ kiến thức. Springer, 364–375.

[80] Zhixiong Yue, Baijiong Lin, Xiaonan Huang, và Yu Zhang. 2020. Tìm kiếm Kiến trúc Neural Hiệu quả, Hiệu suất và Mạnh mẽ. arXiv preprint arXiv:2011.09820 (2020).

[81] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, và Frank Hutter. 2019. Hiểu và củng cố tìm kiếm kiến trúc khả vi phân. arXiv preprint arXiv:1909.09656 (2019).

[82] Li Lyna Zhang, Shihao Han, Jianyu Wei, Ningxin Zheng, Ting Cao, Yuqing Yang, và Yunxin Liu. 2021. nn-Meter: hướng tới dự đoán độ trễ chính xác của suy luận mô hình học sâu trên các thiết bị biên đa dạng. Trong Kỷ yếu Hội nghị Quốc tế thường niên lần thứ 19 về Hệ thống Di động, Ứng dụng và Dịch vụ. 81–93.

[83] Xinyu Zhang, Ian Colbert, Ken Kreutz-Delgado, và Srinjoy Das. 2021. Huấn luyện Mạng Neural Sâu với Lượng tử hóa và Cắt tỉa Kết hợp của Trọng số và Kích hoạt. arXiv preprint arXiv:2110.08271 (2021).

[84] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, và Sijia Liu. 2022. Thúc đẩy Cắt tỉa Mô hình thông qua Tối ưu hóa Hai cấp. arXiv preprint arXiv:2210.04092 (2022).

[85] Hattie Zhou, Janice Lan, Rosanne Liu, và Jason Yosinski. 2019. Phân tích vé số: Số không, dấu hiệu và supermask. arXiv preprint arXiv:1905.01067 (2019).

[86] Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, và Xiang Li. 2020. Cắt tỉa Có cấu trúc Cấp Neuron sử dụng Điều chỉnh Phân cực.. Trong NeurIPS.

[87] Barret Zoph và Quoc V Le. 2016. Tìm kiếm kiến trúc neural với học tăng cường. arXiv preprint arXiv:1611.01578 (2016).

[88] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, và Quoc V Le. 2018. Học kiến trúc có thể chuyển giao cho nhận dạng hình ảnh có thể mở rộng. Trong Kỷ yếu hội nghị IEEE về thị giác máy tính và nhận dạng mẫu. 8697–8710.
