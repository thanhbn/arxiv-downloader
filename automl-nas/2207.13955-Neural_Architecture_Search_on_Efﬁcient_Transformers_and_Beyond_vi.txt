# Tìm kiếm Kiến trúc Mạng Neural trên các Transformer Hiệu quả và Hơn thế nữa
Zexiang Liu1, Dong Li1, Kaiyue Lu1, Zhen Qin1, Weixuan Sun3;1, Jiacheng Xu1, Yiran Zhong1;2*
1SenseTime Research2Shanghai AI Lab3Australian National University

## Tóm tắt
Gần đây, nhiều Transformer hiệu quả đã được đề xuất để giảm độ phức tạp tính toán bậc hai của các Transformer tiêu chuẩn gây ra bởi cơ chế chú ý Softmax. Tuy nhiên, hầu hết chúng chỉ đơn giản thay thế Softmax bằng một cơ chế chú ý hiệu quả mà không xem xét các kiến trúc tùy chỉnh đặc biệt cho cơ chế chú ý hiệu quả. Trong bài báo này, chúng tôi lập luận rằng các kiến trúc Transformer vanilla thủ công cho cơ chế chú ý Softmax có thể không phù hợp cho các Transformer hiệu quả. Để giải quyết vấn đề này, chúng tôi đề xuất một khung công tác mới để tìm kiếm kiến trúc tối ưu cho các Transformer hiệu quả với kỹ thuật tìm kiếm kiến trúc mạng neural (NAS). Phương pháp đề xuất được xác thực trên các tác vụ dịch máy và phân loại hình ảnh phổ biến. Chúng tôi quan sát thấy rằng kiến trúc tối ưu của Transformer hiệu quả có tính toán giảm so với Transformer tiêu chuẩn, nhưng độ chính xác tổng quát kém tương đương. Điều này cho thấy rằng cơ chế chú ý Softmax và cơ chế chú ý hiệu quả có những đặc điểm riêng nhưng không cơ chế nào có thể đồng thời cân bằng tốt độ chính xác và hiệu quả. Điều này thúc đẩy chúng tôi kết hợp hai loại cơ chế chú ý để giảm sự mất cân bằng hiệu suất. Ngoài các không gian tìm kiếm thường được sử dụng trong các phương pháp NAS Transformer hiện có, chúng tôi đề xuất một không gian tìm kiếm mới cho phép thuật toán NAS tự động tìm kiếm các biến thể chú ý cùng với kiến trúc. Các thí nghiệm mở rộng trên WMT'14 En-De và CIFAR-10 chứng minh rằng kiến trúc được tìm kiếm của chúng tôi duy trì độ chính xác tương đương với Transformer tiêu chuẩn với hiệu quả tính toán được cải thiện đáng kể.

## 1. Giới thiệu
Các Transformer hiệu quả [17, 20, 25, 31] đã đạt được những tiến bộ đáng kể trong những năm gần đây. Chúng giảm độ phức tạp tính toán bậc hai của Transformer tiêu chuẩn [35] bằng cách làm thưa hoặc xấp xỉ cơ chế chú ý Softmax theo cách hiệu quả hơn. Hiện tại, cấu hình của mạng hiệu quả, ví dụ như số lượng đầu và kích thước nhúng đầu vào, được sao chép trực tiếp từ Transformer, điều này có thể không phù hợp cho các Transformer hiệu quả [25]. Cấu trúc mạng tùy chỉnh đặc biệt cho các Transformer hiệu quả chưa được nghiên cứu kỹ lưỡng. Tuy nhiên, thiết kế thủ công luôn liên quan đến công việc kỹ thuật tốn kém và có thể không tối ưu do thiên kiến của con người [14]. Do đó, trong bài báo này, chúng tôi nhằm khám phá cách tự động tìm kiếm một kiến trúc phù hợp cho các Transformer hiệu quả.

[Tiếp tục với phần dịch của các phần còn lại...]

--- TRANG 2 ---
[Hình 2. Không gian tìm kiếm được đề xuất cho các loại chú ý. Nó chứa cơ chế chú ý Softmax và cơ chế chú ý tuyến tính (tức là cơ chế chú ý cosFormer [25] trong bài báo này). Chúng tôi để mô hình tự động xác định loại chú ý nào sử dụng với NAS.]

Để đạt được mục tiêu này, chúng tôi cần (1) chỉ định một Transformer hiệu quả phù hợp làm mục tiêu, và (2) tìm một phương pháp để tự động hóa việc thiết kế mạng. Đối với điểm đầu tiên, chúng tôi đưa ra một tổng quan ngắn gọn về các Transformer hiệu quả hiện có. Về cách xử lý cơ chế chú ý Softmax, chúng có thể được phân loại thô thành dựa trên mẫu và dựa trên kernel [25]. Các phương pháp dựa trên mẫu làm thưa ma trận chú ý với các mẫu được định nghĩa trước hoặc có thể học, ví dụ như chia chuỗi đầu vào thành các khối cố định [26], tính toán chú ý tại các khoảng cách cố định [7], hoặc sử dụng chú ý trục [15]. Mặc dù độ thưa được tạo ra bởi các mẫu cụ thể có lợi cho việc đơn giản hóa tính toán chú ý, nó vẫn gặp phải độ phức tạp bậc hai đối với độ dài đầu vào N [33] và phức tạp để triển khai. Khác biệt là, các phương pháp dựa trên kernel nhằm giảm độ phức tạp bậc hai xuống tuyến tính (tức là O(N) về cả độ phức tạp thời gian và không gian) [17, 25]. Chúng tái công thức hóa cơ chế tự chú ý để tránh tính toán rõ ràng ma trận N×N. Hơn nữa, chúng dễ tái tạo hơn trong thực tế. Xem xét các tính chất hấp dẫn của các phương pháp kernel, chúng tôi chọn cosFormer [25], một mô hình dựa trên kernel mới với hiệu suất tiên tiến nhất trong số các Transformer hiệu quả, làm mục tiêu của chúng tôi. Chúng tôi cố gắng tìm kiếm kiến trúc tùy chỉnh và tối ưu cho nó.

Để tự động hóa việc thiết kế mạng, chúng tôi tận dụng tìm kiếm kiến trúc mạng neural (NAS) [11, 23]. Nó đã được sử dụng rộng rãi trong việc tìm kiếm các kiến trúc Transformer tiêu chuẩn trong Xử lý Ngôn ngữ Tự nhiên [14, 16, 29, 36, 38] và Thị giác Máy tính [5, 6, 10, 13]. Các nghiên cứu này chủ yếu tập trung vào việc tinh chỉnh không gian tìm kiếm và/hoặc cải thiện thuật toán tìm kiếm. Ví dụ, AutoAttend [14] khám phá các kết nối hiệu quả giữa Query, Key và Value bằng cách tạo ra các phép toán nguyên thủy cho dữ liệu đầu vào, ví dụ như tích chập 1×1 và max pooling 1×3. Evolved Transformer [29] áp dụng lựa chọn giải đấu [27] cho NAS để tạo ra các mô hình ứng viên một cách mạnh mẽ hơn. Tuy nhiên, các phương pháp này gặp phải thời gian huấn luyện dài và chi phí tìm kiếm lớn vì tất cả các ứng viên cần được tối ưu hóa, đánh giá và xếp hạng. Với mục đích giảm các chi phí này, chúng tôi sử dụng RankNAS [16], một khung NAS hiệu quả mới để tìm kiếm Transformer tiêu chuẩn [35]. Nó có thể tăng tốc đáng kể quy trình tìm kiếm thông qua xếp hạng theo cặp, cắt tỉa không gian tìm kiếm và ràng buộc nhận biết phần cứng.

Với cả Transformer hiệu quả (tức là cosFormer [25]) và thuật toán tìm kiếm (tức là RankNAS [16]), chúng tôi tiến hành một nghiên cứu sơ bộ về NAS dựa trên Transformer hiệu quả thuần túy. Cụ thể, chúng tôi thay thế Softmax bằng cơ chế chú ý tuyến tính được giới thiệu trong cosFormer, và sau đó tìm kiếm nó với RankNAS. Để điều tra toàn diện kiến trúc tối ưu, chúng tôi thực hiện tìm kiếm trên hai tác vụ đại diện trong lĩnh vực NLP và CV, tức là dịch máy (WMT'14 En-De [4]) và phân loại hình ảnh (CIFAR-10 [19]). Nói chung, chúng tôi quan sát thấy rằng cấu trúc tối ưu của cosFormer có chi phí tính toán ít hơn, ví dụ như FLOPS nhỏ hơn như được minh họa trong Hình 1. Tuy nhiên, độ chính xác tổng quát kém tương đương với Transformer tiêu chuẩn (xem trục dọc trong Hình 1). Sự mất cân bằng hiệu suất giữa độ chính xác và hiệu quả này cũng đã được tiết lộ trong các Transformer hiệu quả khác [8, 17, 18, 32, 37].

Xem xét rằng cơ chế chú ý Softmax vanilla và cơ chế chú ý tuyến tính có những đặc điểm riêng về hiệu suất, chúng tôi đề xuất sử dụng cơ chế chú ý Softmax và tuyến tính theo cách kết hợp để có lợi cho việc cân bằng tốt hơn giữa độ chính xác và hiệu quả (được gọi là "mFormer"). Hơn nữa, chúng tôi mong đợi mô hình tự động xác định loại chú ý nào sử dụng. Để đạt được điều này, chúng tôi giới thiệu một không gian tìm kiếm mới đặc biệt cho chú ý và tích hợp nó vào khung NAS. Sau khi tìm kiếm lại kiến trúc tối ưu, chúng tôi thấy rằng sự kết hợp của hai loại chú ý đạt được hiệu suất tương đương với Transformer với hiệu quả được cải thiện đáng kể trên cả hai tác vụ (xem Hình 1).

Tóm lại, chúng tôi đóng góp ba điểm chính:
• Theo hiểu biết tốt nhất của chúng tôi, đây là công trình đầu tiên tìm kiếm kiến trúc tối ưu của các Transformer hiệu quả. Chúng tôi sử dụng NAS để tìm kiếm cosFormer, một mô hình hiệu quả đại diện. Các kết quả tìm kiếm đưa ra những hiểu biết mới cho cộng đồng, tức là cách thiết kế các cấu trúc mạng tùy chỉnh, hiệu quả và có hiệu suất cao cho các Transformer hiệu quả.

• Chúng tôi đề xuất một cách sử dụng mới của cơ chế chú ý, tức là kết hợp cơ chế chú ý Softmax và cơ chế chú ý tuyến tính trong Transformer, và định nghĩa một không gian tìm kiếm mới cho việc tìm kiếm chú ý trong khung NAS. Điều này đẩy việc tìm kiếm Transformer hiện có xa hơn bằng cách cho phép lựa chọn tự động các loại chú ý phù hợp.

• Cơ chế chú ý kết hợp được đề xuất đạt được sự cân bằng tốt hơn giữa độ chính xác và hiệu quả, tức là có hiệu suất tương đương với Transformer tiêu chuẩn trong khi duy trì hiệu quả tốt. Điều này được xác thực trên cả tác vụ dịch máy và phân loại hình ảnh.

## 2. Công trình liên quan

### 2.1. Các transformer hiệu quả
Transformer tiêu chuẩn [35] gặp phải độ phức tạp thời gian và không gian bậc hai gây ra bởi cơ chế chú ý Softmax. Để giảm chi phí tính toán, các Transformer hiệu quả

--- TRANG 3 ---
[Hình 3. Minh họa về cơ chế chú ý. (a) Softmax tính QKT trước, dẫn đến độ phức tạp bậc hai đối với độ dài chuỗi N. (b) Cơ chế chú ý tuyến tính phân tách hàm tương tự với một hàm kernel, và nhân K và V trước. Điều này giảm độ phức tạp bậc hai xuống tuyến tính.]

hoặc làm thưa hoặc xấp xỉ Softmax theo cách hiệu quả hơn. Làm thưa cơ chế chú ý được áp dụng bởi các phương pháp dựa trên mẫu, trong đó ma trận chú ý được làm thưa với các mẫu được định nghĩa trước hoặc có thể học. [26] chia chuỗi đầu vào thành các khối cố định, do đó độ phức tạp được giảm từ N² xuống B² (B là kích thước khối nhỏ hơn N). Thay vào đó, [22] giảm mẫu đầu vào xuống độ dài cố định. Thay vì điều chỉnh độ dài chuỗi, [3, 7] sử dụng các mẫu bước/giãn để tính toán chú ý tại các khoảng cách cố định. So với các phương pháp này với một mẫu cố định, việc kết hợp nhiều mẫu có thể đa dạng hóa phạm vi truy cập chú ý [33]. Ví dụ, [15] tính toán chú ý dọc theo mọi trục của đầu vào, và [7] tổng hợp chú ý từ cả mẫu bước và cục bộ. Để cải thiện hơn nữa chất lượng của các mẫu, việc học các mẫu theo cách dựa trên dữ liệu đang trở thành một xu hướng mới [18, 28, 32]. Lợi ích so với các mẫu cố định là các mẫu có thể học có thể phân cụm các token đầu vào dựa trên mức độ liên quan của token một cách chính xác hơn trong khi vẫn duy trì hiệu quả [33]. Tóm lại, các phương pháp dựa trên mẫu có thể tăng hiệu quả bằng cách làm thưa cơ chế chú ý Softmax. Tuy nhiên, chúng vẫn có độ phức tạp bậc hai, và nó sẽ tăng khi độ dài đầu vào trở nên lớn hơn. Ngoài ra, chúng tương đối phức tạp để tái tạo trong thực tế.

Một danh mục chung khác của các Transformer hiệu quả dựa trên kernel (hoặc hàm kernel), nhằm giảm độ phức tạp từ bậc hai xuống tuyến tính. Bằng cách này, Softmax có thể được viết lại với các dạng khác để tránh ma trận chú ý N×N [33]. Để đạt được điều này, [37] giả định một tiên nghiệm rank thấp trong cấu trúc N×N, và biến đổi Key và Value thành một chiều thấp hơn với các lớp chiếu bổ sung. [24] xấp xỉ Softmax với tích của một chuỗi các kernel Gaussian, và thay vào đó, [9] sử dụng các kernel Haar. PerFormer [8] sử dụng các đặc trương ngẫu nhiên trực giao để tạo ra các kernel ngẫu nhiên. Linear Transformer [17] tận dụng tính chất kết hợp của tích ma trận, và tái công thức hóa hàm tương tự với phân tách kernel. cosFormer [25] tuân theo chiến lược kernelization này, và sử dụng ReLU [1] làm hàm kernel với một cơ chế tái trọng số cosine bổ sung. Nói chung, các phương pháp dựa trên kernel có thể tuyến tính hóa độ phức tạp và hiệu quả tăng hiệu quả. Hơn nữa, chúng dễ triển khai hơn các phương pháp dựa trên mẫu.

### 2.2. Tìm kiếm kiến trúc mạng neural (NAS)
NAS [11, 23] nhằm tự động tìm kiếm kiến trúc mạng phù hợp nhất, và đã được áp dụng rộng rãi trong Thị giác Máy tính [5,6,10,13] và Xử lý Ngôn ngữ Tự nhiên [14,16,29,36,38]. Cốt lõi của NAS là thiết kế một không gian tìm kiếm phù hợp, xếp hạng tất cả các kiến trúc ứng viên được tạo ra từ không gian, và tìm ra cấu trúc tối ưu. Đối với các Transformer trong NLP, Evolved Transformer (ET) [29] là công trình tiên phong áp dụng NAS vào tìm kiếm kiến trúc Transformer. Nó định nghĩa một không gian tìm kiếm quy mô lớn, bao gồm các thành phần trong đầu vào, chuẩn hóa, các lớp, chiều đầu ra, kích hoạt, v.v. Một thuật toán tiến hóa [27] được áp dụng để làm cho việc lựa chọn kiến trúc ổn định và mạnh mẽ hơn, tức là liên tục lựa chọn kiến trúc triển vọng nhất dựa trên độ phù hợp. Mặc dù ET có thể tìm thấy một cấu trúc tốt hơn và hiệu quả hơn, chi phí tính toán của nó vẫn rất lớn. Điều này là do thuật toán phải bao gồm tất cả các đặc trưng tìm kiếm. Bên cạnh đó, việc huấn luyện quá trình tiến hóa cũng tốn thời gian.

Các công trình tiếp theo tập trung vào việc cải thiện thuật toán tìm kiếm và/hoặc cắt tỉa không gian tìm kiếm. [41] sử dụng tìm kiếm kiến trúc có thể vi phân (DARTS) [21] bao gồm tất cả các phép toán vào một nút (tức là tạo thành một siêu mạng) và tận dụng Softmax cho lựa chọn cụ thể. Phương pháp này làm giảm nhu cầu huấn luyện từng mạng ứng viên riêng biệt. [34] phân tách cấu trúc Transformer thành các thành phần nhỏ hơn, và giới thiệu một thuật toán tìm kiếm một lần dựa trên lấy mẫu. HAT [36] giới thiệu một ràng buộc nhận biết phần cứng để tăng tốc quá trình tìm kiếm. RankNAS [16] coi NAS như một vấn đề xếp hạng theo cặp, điều này tăng tốc đáng kể quá trình tìm kiếm.

Cắt tỉa không gian tìm kiếm, tức là chỉ giữ lại các đặc trưng tìm kiếm quan trọng nhất, là một phương pháp khác để giảm chi phí tìm kiếm. TextNAS [38] chỉ định một không gian tìm kiếm phù hợp cho biểu diễn văn bản, bao gồm các lớp tích chập, tuần hoàn, pooling và tự chú ý. Primer [30] sửa đổi không gian tìm kiếm với các kích hoạt ReLU bình phương và các lớp tích chập theo chiều sâu sau Query, Key và Value. AutoAttend [14] đặc biệt tìm kiếm các kết nối

--- TRANG 4 ---
[Hình 4. Không gian tìm kiếm trong bài báo này. Các khối màu xanh là các đặc trưng nguyên thủy trong tìm kiếm Transformer, và các khối màu xám biểu thị việc tìm kiếm loại chú ý được đề xuất.]

giữa QKV. RankNAS [16] đề xuất một thuật toán lựa chọn đặc trưng đo lường tầm quan trọng của mỗi đặc trưng và sắp xếp ra những đặc trưng hữu ích nhất để tìm kiếm thêm. Tóm lại, việc cắt tỉa không gian tìm kiếm có thể hiệu quả cải thiện hiệu quả tìm kiếm và duy trì hiệu suất tốt.

## 3. NAS trên các Transformer Hiệu quả

Trong phần này, trước tiên chúng tôi đưa ra một đánh giá ngắn gọn về kiến thức sơ bộ của cosFormer [25] và RankNAS [16]. Sau đó, chúng tôi chỉ định các bước chính của việc áp dụng NAS vào tìm kiếm Transformer hiệu quả và thảo luận về các kết quả chung. Cuối cùng, chúng tôi giới thiệu ý tưởng được đề xuất về việc sử dụng cơ chế chú ý kết hợp.

### 3.1. Kiến thức sơ bộ

#### 3.1.1 cosFormer
Cho Query Q∈ℝN×d, Key K∈ℝN×d, và Value V∈ℝN×d (d là chiều đặc trưng của mỗi đầu), Transformer tiêu chuẩn [35] tạo ra đầu ra O∈ℝN×d bằng cách nhân chú ý QK được chuẩn hóa với V, tức là
O = σ(QKT)V;                                                    (1)
trong đó σ là hàm tương tự, ví dụ như Softmax trong Transformer. Việc tính toán QKT∈ℝN×N dẫn đến độ phức tạp bậc hai đối với độ dài chuỗi N (xem Hình 3(a)).

Tuyến tính hóa chú ý có thể giảm độ phức tạp từ bậc hai xuống tuyến tính [17, 25]. Điều này được đạt được bằng cách phân tách hàm tương tự và nhân K và V trước, tức là
σ(QKT)V = (φ(Q)φ(K)T)V = φ(Q)(φ(K)TV);                        (2)
trong đó φ là một hàm kernel biến đổi Q và K thành các biểu diễn ẩn. Trong trường hợp này, độ phức tạp O(N²d) giảm xuống O(Nd²). Nói chung chúng ta luôn có N≫d, vì vậy O(N²d)≈O(N²) và O(Nd²)≈O(N), tạo ra độ phức tạp tuyến tính.

Về bản chất, người ta nhận ra rằng hàm tương tự nên là không âm và phi tuyến [17, 25]. Tính không âm tăng cường các đặc trưng tương quan tích cực, và [25] đạt được điều này bằng cách trực tiếp sử dụng hàm ReLU [1], tức là ReLU(x) = max(0;x). Tính phi tuyến tập trung các đặc trưng liên quan nhất để được tổng hợp và ổn định việc huấn luyện [2, 12]. Để đạt được điều này, [25] tiếp tục giới thiệu một cơ chế tái trọng số dựa trên cosine sao cho các token gần nhau được khuyến khích có kết nối chú ý cao hơn. Công thức toàn bộ là

Oi = ∑(j=1 to N) (ReLU(Qi)·ReLU(Kj)T cos(2πij/N))Vj.         (3)

Ở đây để đơn giản, chúng tôi bỏ qua số hạng chuẩn hóa. Phương trình 3 có thể được viết lại dưới dạng tuyến tính trong Phương trình 2, và chúng tôi giới thiệu độc giả đến bài báo gốc [25] để biết thêm chi tiết.

#### 3.1.2 RankNAS
Các phương pháp NAS thông thường [14, 29, 38] phải đánh giá hiệu suất của mọi mạng ứng viên để xếp hạng, điều này tốn thời gian đặc biệt khi không gian tìm kiếm lớn. RankNAS được đề xuất gần đây [16] có thể hiệu quả giảm chi phí huấn luyện với ba điểm khác biệt sau:

1. Xếp hạng theo cặp. Thay vì sắp xếp tất cả các kiến trúc ứng viên, RankNAS coi NAS như một vấn đề xếp hạng theo cặp. Đó là, một phân loại nhị phân được tiến hành cho mỗi cặp ứng viên, và nhãn được đơn giản hóa thành "được sắp xếp đúng" hoặc "được sắp xếp sai" theo hiệu suất ước lượng của chúng.

2. Cắt tỉa không gian tìm kiếm. RankNAS cắt tỉa không gian tìm kiếm bằng cách chỉ chứa các đặc trưng quan trọng nhất có tác động lớn đến hiệu suất.

3. Tìm kiếm nhận biết phần cứng. Ngoài tìm kiếm tiêu chuẩn dựa trên mất mát, RankNAS cũng tận dụng một ràng buộc phần cứng, tức là ước lượng độ trễ và loại bỏ 10% mô hình nhanh nhất và chậm nhất.

Xem xét hiệu quả tốt của RankNAS, chúng tôi lấy nó làm khung NAS của chúng tôi. Các chi tiết kỹ thuật khác có thể được tìm thấy trong [16].

### 3.2. RankNAS trên cosFormer

#### 3.2.1 Không gian tìm kiếm
Không gian tìm kiếm nguyên thủy của chúng tôi được áp dụng từ RankNAS, bao gồm các đặc trưng cơ bản bao gồm kích thước nhúng

--- TRANG 5 ---
[Bảng 1. Không gian tìm kiếm cho dịch WMT'14 En-De.]

[Bảng 2. Không gian tìm kiếm cho phân loại CIFAR-10.]

dung, số lượng lớp encoder/decoder, số lượng đầu và chiều của mạng feed-forward (xem Hình 4). Chúng tôi cũng thực hiện sửa đổi tương ứng với không gian theo từng tác vụ. Định nghĩa chi tiết được giới thiệu trong Phần 4.1.

#### 3.2.2 Các bước chính
Chúng tôi thay thế tất cả Softmax bằng cơ chế chú ý cosFormer, và tuân theo RankNAS [16] cho quá trình tìm kiếm. Một siêu mạng chứa tất cả các kiến trúc có thể được huấn luyện trước. Sau đó, dữ liệu mất mát và độ trễ được thu thập và xếp hạng riêng biệt. Dựa trên các ràng buộc mất mát và độ trễ, các đặc trưng quan trọng nhất được lựa chọn, tức là cắt tỉa không gian tìm kiếm. Thuật toán tiến hóa được thực hiện trên không gian tìm kiếm được tinh chỉnh, và kiến trúc tối ưu được sắp xếp ra. Cuối cùng, chúng tôi huấn luyện lại mạng tối ưu từ đầu.

#### 3.2.3 Thảo luận
Để so sánh, chúng tôi cũng lặp lại các bước trên với Transformer tiêu chuẩn [35] chỉ với cơ chế chú ý Softmax. Để đơn giản, chúng tôi ký hiệu kiến trúc tối ưu của cosFormer là "cosFormer*", và của Transformer là "Transformer*". Từ Hình 1, chúng tôi thấy rằng trên cả hai tác vụ, cosFormer* thể hiện hiệu quả tốt hơn Transformer* (FLOPS nhỏ hơn) với quy mô tham số tương đương, nhưng độ chính xác tổng quát kém cạnh tranh hơn. Chúng tôi gọi hiện tượng này là sự mất cân bằng giữa độ chính xác và hiệu quả, điều này cũng đã được quan sát thấy trong các Transformer hiệu quả khác [8, 17, 18, 32, 37].

Về bản thân cơ chế chú ý, ưu điểm của Softmax trong độ chính xác liên quan đến khả năng "áp đặt một ràng buộc phân phối phân loại trên các điểm số liên quan query-context" [40]. Ràng buộc này có hai tính chất thiết yếu: (1) phủ sóng dày đặc, tức là chú ý đến mọi đặc trưng cho query [35]; và (2) tập trung phi tuyến, tức là trọng số lớn hơn chỉ được gán cho các đặc trưng liên quan hơn [2, 12, 25]. Mặc dù có các tính chất hữu ích, chúng ta cũng cần chú ý rằng hạn chế chính của Softmax là độ phức tạp tính toán cao của nó, như được phân tích trong Phần 3.1.

Để giảm chi phí tính toán, các Transformer hiệu quả hoặc làm thưa ma trận Softmax (dựa trên mẫu) hoặc thay thế nó bằng các hàm kernel khác (dựa trên kernel). Mặc dù các phương pháp dựa trên mẫu giữ lại công thức Softmax, chúng thường sử dụng các mẫu cố định, ít toàn diện, linh hoạt và mạnh mẽ hơn [40]. Các phương pháp kernel xem xét tất cả các đặc trưng, nhưng tính phi tuyến kém mạnh mẽ hơn trong việc tập trung các đặc trưng liên quan, ví dụ như cosFormer [25] tập trung nhiều hơn vào tính cục bộ nên thông tin liên quan ở khoảng cách xa có thể không được tổng hợp tốt. Các xấp xỉ hiệu quả này không thể bao gồm đầy đủ hai tính chất được đề cập ở trên của Softmax, vì vậy độ chính xác của chúng có thể bị ảnh hưởng tiêu cực. Chỉ có một số ít mô hình [25, 39, 40] đôi khi đạt được độ chính xác được cải thiện so với Transformer [35] trên một số tác vụ hoặc cài đặt cụ thể, nhưng hiệu suất của chúng không tốt hơn một cách nhất quán trong tất cả các trường hợp. Do đó, vẫn còn thách thức đối với các Transformer hiệu quả để xấp xỉ Softmax một cách hiệu quả và giảm sự mất cân bằng hiệu suất.

### 3.3. Cơ chế chú ý kết hợp như một đặc trưng tìm kiếm mới
Xem xét rằng cơ chế chú ý Softmax và cơ chế chú ý tuyến tính có những đặc điểm riêng nhưng không thể đồng thời cân bằng tốt độ chính xác và hiệu quả, chúng tôi đề xuất sử dụng hai loại chú ý theo cách kết hợp. Đó là, cơ chế chú ý Softmax hoặc cơ chế chú ý tuyến tính chỉ xuất hiện trong các lớp nhất định, và chúng tôi để mô hình tự động xác định vị trí lớp của chúng với NAS. Để đạt được điều này, chúng tôi định nghĩa một không gian tìm kiếm mới, tức là loại chú ý, bao gồm cơ chế chú ý Softmax và cơ chế chú ý tuyến tính (tức là cơ chế chú ý cosFormer [25]). Chúng tôi lặp lại các bước chính sau khi tích hợp không gian tìm kiếm mới, và đặt tên kiến trúc tối ưu là "mFormer".

Hình 1 chứng minh rằng mFormer có sự cân bằng tốt hơn giữa độ chính xác và hiệu quả trên cả tác vụ phân loại hình ảnh và dịch máy. Đánh giá chi tiết có thể được tìm thấy trong phần tiếp theo.

Cần lưu ý rằng trong NAS Transformer, việc định nghĩa một đặc trưng tìm kiếm mới thường yêu cầu áp đặt các ràng buộc bổ sung để ngăn chặn nó khỏi việc giới thiệu nội dung tìm kiếm dư thừa hoặc không liên quan [14, 29]. Ngược lại, tùy chọn chú ý của chúng tôi (chỉ chứa hai loại) có thể được nhúng trực tiếp vào không gian tìm kiếm mà không cần xem xét bất kỳ ràng buộc nào. Điều này có lợi cho việc triển khai và triển khai thuận tiện.

## 4. Thí nghiệm

### 4.1. Cài đặt thí nghiệm

#### 4.1.1 Bộ dữ liệu
Dịch máy: Chúng tôi sử dụng bộ dữ liệu WMT'14 En-De [4] chứa 4.5M cặp câu. Phân chia huấn luyện, xác thực và kiểm tra giống với [16, 36], tức là WMT'16 cho huấn luyện, Newstest2013 cho xác thực và Newstest2014 cho kiểm tra. Phân loại hình ảnh: Chúng tôi tận dụng bộ dữ liệu CIFAR-10 [19], bao gồm 60K hình ảnh 32×32 trong 10 lớp. Chúng tôi sử dụng phân chia chính thức cho huấn luyện (50K) và kiểm tra (10K).

#### 4.1.2 Định nghĩa không gian tìm kiếm
Dịch máy: Không gian tìm kiếm cho WMT'14 En-De được áp dụng từ RankNAS [16], như được liệt kê trong Bảng 1. Số lượng lớp encoder được đặt là 6, điều này phù hợp với [16, 36]. "En-De Connect" đại diện cho số lượng lớp encoder được chú ý đến decoder, ví dụ như 2 có nghĩa là hai lớp encoder cuối cùng được tham gia vào việc tính toán chú ý chéo encoder-decoder. Tìm kiếm chú ý được đề xuất bao gồm cơ chế chú ý Softmax và cơ chế chú ý tuyến tính (tức là cơ chế chú ý cosFormer [25]). Lưu ý rằng trong thực tế, chúng tôi không bao gồm tìm kiếm loại chú ý trong tự chú ý decoder và chú ý chéo encoder-decoder. Điều này là do chúng tôi thấy thực nghiệm rằng việc sử dụng cơ chế chú ý tuyến tính vào các lớp decoder sẽ dẫn đến hội tụ không phù hợp và hiệu suất kém*. Phân loại hình ảnh: Đối với CIFAR-10, chúng tôi tuân theo cài đặt trong [6], tức là chỉ sử dụng phần encoder và xây dựng không gian tìm kiếm nguyên thủy trong Bảng 2.

#### 4.1.3 Cấu hình huấn luyện
Chúng tôi thực hiện tất cả các thí nghiệm trên card GPU A100 với cùng số bước huấn luyện như RankNAS [16]. Trên tác vụ phân loại hình ảnh, các hình ảnh CIFAR-10 trước tiên được giảm mẫu với các lớp tích chập xuống 14×14, và sau đó được làm phẳng thành một vector 1D với 256 phần tử [6]. Vector này là đầu vào cho Transformer.

#### 4.1.4 Chỉ số đánh giá
Đối với hiệu quả trong cả hai tác vụ, chúng tôi tính toán FLOPS (số phép toán dấu phẩy động trên giây). Đối với độ chính xác, chúng tôi đo BLEU (đánh giá song ngữ) cho dịch thuật với beam 4 và length penalty 0.6. Mô hình được kiểm tra với trung bình mười checkpoint cuối cùng. Độ chính xác của CIFAR-10 được đánh giá bằng cách tính toán tỷ lệ phần trăm hình ảnh được phân loại đúng. Cả hai chỉ số đều được triển khai

*Chúng tôi giới thiệu độc giả đến tài liệu bổ sung để biết thêm chi tiết.

với mã nguồn trong [16] và được đo trên card Nvidia Tesla A100.

### 4.2. So sánh kiến trúc tối ưu trên dịch máy
Từ Bảng 5, các kiến trúc được tìm kiếm của cosFormer* và Transformer* hiệu quả hơn trong khi giữ lại độ chính xác tương đương với các mô hình vanilla. Nó xác thực hiệu quả của NAS. Chúng tôi so sánh các kiến trúc tối ưu dưới đây một cách chi tiết.

#### 4.2.1 Cấu trúc mạng
Kiến trúc tối ưu được tìm kiếm của cosFormer [25] được minh họa trong Bảng 3. Để so sánh, chúng tôi cũng báo cáo các kết quả tìm kiếm của Transformer [35]. Mặc dù kích thước nhúng giống nhau trong cả encoder và decoder, cosFormer* có cấu trúc nhẹ hơn, ví dụ như ít lớp decoder hơn, chiều FFN encoder nhỏ hơn và số đầu decoder ít hơn. Nó ngụ ý rằng cấu trúc tối ưu của cosFormer* có xu hướng "nông và mỏng".

Chiều FFN. Hình 5(a) và (c) hiển thị các chiều FFN trong encoder và decoder tương ứng. Chúng tôi vẽ các đường cong xu hướng với hàm khớp đa thức để trực quan hóa tốt hơn các thay đổi. Chiều FFN tổng thể của cosFormer* nhỏ hơn Transformer*. Cụ thể hơn, các chiều lớn hơn được yêu cầu trong các lớp trên cùng của encoder trong cosFormer*. Cả cosFormer* và Transformer* đều có xu hướng có chiều nhỏ hơn trong các lớp cuối của decoder.

Số đầu. Chúng tôi vẽ biểu đồ số đầu trong Hình 5(b)(d)(e). Nói chung, cosFormer* yêu cầu số lượng đầu nhỏ hơn.

En-De Connect. Từ Hình 5(f), chúng tôi quan sát thấy rằng các decoder trong cả hai mô hình chỉ chú ý đến lớp cuối cùng của encoder, tức là chỉ nhận thông tin chéo từ khối encoder cuối cùng.

Tham số. cosFormer* có ít hơn 12.6M tham số so với Transformer*.

#### 4.2.2 Độ chính xác
Trên bộ kiểm tra WMT'14 En-De, cosFormer* thể hiện BLEU cực kỳ kém†. Nó phản ánh hạn chế của cơ chế chú ý tuyến tính trong việc đạt được độ chính xác tương đương với Transformer.

Chúng tôi cũng điều tra tầm quan trọng của mỗi đặc trưng đối với độ chính xác, được đo bởi RankNAS [16]. 5 đặc trưng được xếp hạng hàng đầu là:
• cosFormer*: En-De Connect, Dec Layer Num, En-De Head Num, Dec FFN Dim, Dec Head Num.

†Nó có hội tụ không phù hợp, và chúng tôi đang liên lạc với các tác giả của [25] để xử lý vấn đề này.

--- TRANG 7 ---
[Bảng 3. Các kiến trúc được tìm kiếm và kết quả trên dịch máy. "AVG" có nghĩa là giá trị trung bình.]

[Hình 5. Các đặc trưng được tìm kiếm chi tiết trên WMT'14 En-De. Các lớp với cơ chế chú ý tuyến tính được gắn hậu tố "_m". Các đường nét đứt phản ánh xu hướng thay đổi.]

• Transformer*: Dec Layer Num, En-De Head Num, Dec FFN Dim, Dec Head Num, Enc FFN Dim.

Do đó, khi thiết kế kiến trúc cho Transformer hiệu quả, chúng ta cần tập trung nhiều hơn vào tương tác encoder-decoder, ví dụ như số lượng lớp encoder để chú ý và các đầu chú ý chéo. Cả cosFormer* và Transformer* cũng nhạy cảm với số lượng lớp decoder, nhưng hiệu suất tốt nhất không nhất thiết cần số lượng lớp lớn nhất.

#### 4.2.3 Hiệu quả
Rõ ràng, cosFormer* hiệu quả hơn, tức là FLOPS của nó là 8.32G trong khi của Transformer* là 9.86G. Sự khác biệt này chủ yếu do cấu trúc tối ưu phức tạp hơn của Transformer*. Cơ chế chú ý Softmax cũng tăng chi phí tính toán, điều này sẽ được chi tiết trong Phần 4.5.

5 đặc trưng được xếp hạng hàng đầu về hiệu quả là:
• cosFormer*: Dec Layer Num, En-De Head Num, En-De Connect, Enc FFN Dim, Dec Head Num.

--- TRANG 8 ---
[Bảng 4. So sánh kiến trúc tối ưu và hiệu suất trên phân loại hình ảnh CIFAR-10. "AVG" có nghĩa là giá trị trung bình.]

[Hình 6. Các đặc trưng được tìm kiếm chi tiết trên phân loại hình ảnh. Các lớp với cơ chế chú ý tuyến tính được gắn hậu tố "_m". Các đường nét đứt phản ánh xu hướng thay đổi.]

• Transformer*: Dec Layer Num, Dec Head Num, Enc Head Num, Enc FFN Dim, Dec FFN Dim.

Rõ ràng rằng số lượng lớp decoder có tác động lớn nhất đến hiệu quả. Đối với Transformer hiệu quả, tương tác encoder-decoder cũng đóng vai trò quan trọng trong việc xác định chi phí tính toán. Ngược lại, hiệu quả của Transformer* ít nhạy cảm hơn với tương tác chéo.

#### 4.2.4 Tóm tắt
Chúng tôi tóm tắt các gợi ý thực nghiệm để thiết kế một kiến trúc phù hợp cho Transformer hiệu quả trên dịch máy: (1) Ít lớp decoder hơn; (2) Tập trung nhiều hơn vào tương tác encoder-decoder. Các lớp encoder để chú ý và số lượng đầu encoder-decoder không nhất thiết phải quá lớn; (3) Chiều FFN encoder quan trọng hơn đối với hiệu quả trong khi chiều FFN decoder quan trọng hơn đối với độ chính xác.

### 4.3. So sánh kiến trúc tối ưu trên phân loại hình ảnh
Các cấu trúc tối ưu của cosFormer* và Transformer* vượt trội đáng kể so với các phiên bản vanilla của chúng về độ chính xác (xem Bảng 5), điều này tiếp tục xác minh sự cần thiết và hữu ích của NAS. FLOPS lớn hơn vì các mô hình vanilla chỉ có 6 lớp encoder. Chúng tôi so sánh các cấu trúc tối ưu như sau.

#### 4.3.1 Cấu trúc mạng
Bảng 4 cho thấy các kiến trúc tối ưu của cosFormer [25] và Transformer [35]. Rõ ràng, Transformer* có nhiều lớp hơn, kích thước nhúng nhỏ hơn và chiều FFN trung bình nhỏ hơn so với cosFormer*. Từ góc nhìn tổng quan, cosFormer* thích cấu trúc "nông và rộng" trong khi kiến trúc tối ưu của Transformer* có xu hướng "sâu và mỏng".

Chiều FFN. Hình 6(a) hiển thị các chiều FFN trong các lớp khác nhau. Từ đường cong xu hướng, chúng tôi quan sát thấy rằng cosFormer* có chiều FFN tương đối nhỏ hơn trong các lớp trung gian. Khác biệt là, chiều này nhỏ hơn trong các lớp đầu trong Transformer*.

Số đầu. Số lượng đầu trong các lớp encoder được vẽ trong Hình 6(b). cosFormer* thường có nhiều đầu hơn trong các lớp đầu và trên cùng. Ngược lại, số lượng đầu trong Transformer* không thay đổi quá nhiều và các giá trị của chúng lớn.

Tham số. cosFormer* và Transformer* có quy mô tham số tương tự, tức là ~24M.

#### 4.3.2 Độ chính xác
Trên bộ kiểm tra CIFAR-10, cosFormer* đạt độ chính xác 88.4%, bị vượt qua bởi Transformer* (95.10%) khoảng 7.6%.

Theo tầm quan trọng đối với độ chính xác, các đặc trưng được xếp hạng bởi RankNAS [16] là:
• cosFormer*: Layer Num, FFN Dim, Emb Dim, Head Num.
• Transformer*: FFN Dim, Head Num, Emb Dim, Layer Num.

Rõ ràng, số lượng lớp có tác động lớn nhất đến độ chính xác của cosFormer*. Tương tự như trường hợp trong dịch máy, cosFormer* không chọn số lượng lớp lớn nhất.

--- TRANG 9 ---
[Bảng 5. Nghiên cứu ablation về các loại chú ý.]

#### 4.3.3 Hiệu quả
FLOPS của cosFormer* là 7.01G, tốt hơn đáng kể so với Transformer* (10.9G) 36%. Nó tiết lộ ưu điểm của cơ chế chú ý tuyến tính của cosFormer* về hiệu quả. Các đặc trưng quan trọng về hiệu quả là:
• cosFormer*: Layer Num, Emb Dim, Head Num, FFN Dim.
• Transformer*: Layer Num, FFN Dim, Head Num, Emb Dim.

Tương tự như kết luận trong dịch máy, số lượng lớp trong phân loại hình ảnh cũng liên quan nhất đến hiệu quả. Mô hình hiệu quả có sự ưu tiên sử dụng ít lớp hơn, điều này tiếp tục giảm gánh nặng tính toán.

#### 4.3.4 Tóm tắt
Chúng tôi đưa ra một tóm tắt ngắn gọn về cách thiết kế phù hợp Transformer hiệu quả trong phân loại hình ảnh: (1) Ít lớp hơn; (2) Chiều FFN nhỏ hơn trong các lớp trung gian với kích thước nhúng lớn hơn một chút; (3) Số lượng đầu nhỏ hơn trong các lớp trung gian.

### 4.4. Kết quả của chúng tôi sử dụng cơ chế chú ý kết hợp

#### 4.4.1 Dịch máy
Bảng 3 cho thấy cấu trúc tối ưu và hiệu suất của mFormer của chúng tôi. Nói chung, mFormer là "nông và mỏng", có ít tham số nhất và FLOPS nhỏ nhất trong ba mô hình tối ưu. Cơ chế chú ý tuyến tính tồn tại trong Lớp 2 và 4 trong encoder, và tất cả các lớp encoder/decoder khác giữ lại cơ chế chú ý Softmax.

Trong mFormer, các chiều FFN trong encoder trải qua một thay đổi "lên-và-xuống", tức là các lớp đầu dựa vào kích thước FFN nhỏ hơn. Số lượng đầu trong encoder có xu hướng nhỏ hơn trong các lớp trung gian. Các đặc trưng chính trong decoder của mFormer liên tục có số lượng không lớn hơn hai cái kia, ngoại trừ số lượng đầu trong lớp decoder đầu tiên lớn hơn so với cosFormer*.

Về hiệu suất, mFormer đạt BLEU tương đương với Transformer* với ít hơn 18% tham số và FLOPS nhỏ hơn 18%. Nó cũng vượt trội đáng kể so với cosFormer* về độ chính xác với hiệu quả tốt hơn một chút. Các kết quả cho thấy rằng việc sử dụng kết hợp cơ chế chú ý có thể hiệu quả tạo điều kiện giảm sự mất cân bằng giữa độ chính xác và hiệu quả.

#### 4.4.2 Phân loại hình ảnh
Chúng tôi hiển thị kiến trúc tối ưu của mFormer được đề xuất trong Bảng 4. Trực quan, cấu trúc của chúng tôi là "nông và mỏng", điều này phù hợp với quan sát trong dịch máy. Điều này cũng được phản ánh bởi quy mô tham số, tức là chúng tôi có khoảng 20% ít tham số hơn hai cái kia. Cơ chế chú ý tuyến tính tồn tại trong Lớp 3, 7 và 11 (chiếm 25%), và các loại chú ý còn lại trong các lớp khác đều là Softmax. Các chiều FFN lớn trong các lớp đầu, và có xu hướng "xuống-lên-xuống" trong các lớp tiếp theo. Các thay đổi của số lượng đầu dường như tỷ lệ nghịch với chiều FFN, tức là các chiều FFN lớn hơn thường tương ứng với số lượng đầu nhỏ hơn.

Đáng chú ý, mFormer vượt trội so với cosFormer* với một khoảng cách lớn về độ chính xác (5.19 điểm phần trăm) trong khi duy trì hiệu quả tốt, tức là nó chỉ có 1.38G FLOPS nhiều hơn cosFormer*. Hơn nữa, nó đạt được

--- TRANG 10 ---
hiệu suất tương đương với Transformer* về độ chính xác (cái sau chỉ vượt trội chúng tôi 1.51 điểm phần trăm) nhưng hiệu quả hơn đáng kể (FLOPS của chúng tôi nhỏ hơn Transformer* 2.51G, tức là 23%). Nó chứng minh rằng việc sử dụng cơ chế chú ý kết hợp có thể đạt được sự cân bằng tốt hơn giữa độ chính xác và hiệu quả trên tác vụ phân loại hình ảnh.

### 4.5. Nghiên cứu ablation về các loại chú ý
Vì tìm kiếm chú ý được giới thiệu mới trong khung NAS, chúng tôi đặc biệt nghiên cứu ảnh hưởng của mỗi loại chú ý trong các kiến trúc tối ưu. Điều này nhằm xác thực (1) hiệu quả của cơ chế chú ý kết hợp được đề xuất trong việc cân bằng độ chính xác và hiệu quả, và (2) sự khác biệt cố hữu giữa cơ chế chú ý Softmax và tuyến tính về hiệu suất. Các kết quả trên dịch máy và phân loại hình ảnh được báo cáo trong Bảng 5.

#### 4.5.1 Tất cả cơ chế chú ý Softmax
Trước tiên chúng tôi thống nhất cơ chế chú ý kết hợp và cơ chế chú ý tuyến tính trong các kiến trúc tối ưu của mFormer và cosFormer* với Softmax. Đối với mFormer của chúng tôi, độ chính xác được cải thiện một chút nhưng FLOPS trở nên lớn hơn, cho thấy rằng độ chính xác tốt hơn luôn đi kèm với sự hy sinh về hiệu quả. Sự mất cân bằng hiệu suất này rõ ràng hơn khi chúng tôi thay thế tất cả cơ chế chú ý tuyến tính trong cosFormer* bằng Softmax, trong đó độ chính xác được tăng cường đáng kể với hiệu quả tính toán giảm sút nhiều. Đáng chú ý, mô hình của chúng tôi có thể đạt được độ chính xác tương đương với hiệu suất với tất cả Softmax và giữ hiệu quả thỏa đáng mà không giảm quá nhiều.

#### 4.5.2 Tất cả cơ chế chú ý tuyến tính
Trong bước này, chúng tôi thay thế cơ chế chú ý kết hợp trong mFormer và cơ chế chú ý Softmax trong Transformer* một cách thống nhất bằng cơ chế chú ý tuyến tính. Chúng tôi nhận thấy sự sụt giảm hiệu suất đáng kể về độ chính xác trên cả hai tác vụ. Nó chứng minh khả năng kém cạnh tranh của cơ chế chú ý tuyến tính trong việc tạo ra kết quả chính xác. Tuy nhiên, sau khi thay thế, lợi ích về hiệu quả là đáng kể, điều này tiếp tục cho thấy ưu điểm của Transformer hiệu quả trong việc giảm chi phí tính toán. Việc sử dụng kết hợp cơ chế chú ý có lợi cho việc đạt được cả độ chính xác và hiệu quả tương đương.

## 5. Kết luận
Trong bài báo này, chúng tôi sử dụng NAS để tìm kiếm kiến trúc tối ưu của các Transformer hiệu quả (tức là cosFormer [25]). Các kiến trúc được tìm kiếm tiết lộ rằng các cấu trúc tối ưu của Transformer hiệu quả tương đối nhẹ hơn so với Transformer tiêu chuẩn, ví dụ như quy mô tham số giảm và FLOPS được cải thiện. Điều này cung cấp những hiểu biết hữu ích cho cộng đồng về thiết kế phù hợp của các Transformer hiệu quả. Tuy nhiên, độ chính xác tổng quát của các mô hình hiệu quả kém cạnh tranh hơn. Dựa trên quan sát này, chúng tôi đề xuất một cách sử dụng mới của cơ chế chú ý, tức là sử dụng cơ chế chú ý tuyến tính và Softmax theo cách kết hợp trong mỗi lớp. Kiến trúc tối ưu được tìm kiếm thể hiện độ chính xác tương đương với Transformer và duy trì hiệu quả tốt như Transformer hiệu quả. Phương pháp được đề xuất cung cấp một hướng mới trong nghiên cứu Transformer, tức là tận dụng cả cơ chế chú ý Softmax và tuyến tính. Trong công việc tương lai của chúng tôi, chúng tôi sẽ nghiên cứu cơ chế chú ý kết hợp trên các mô hình pretrain quy mô lớn cũng như các tác vụ downstream khác.

## Tài liệu tham khảo
[1] Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375, 2018. 3, 4
[2] Titsias RC AUEB et al. One-vs-each approximation to softmax for scalable estimation of probabilities. Advances in Neural Information Processing Systems, 29, 2016. 4, 5
[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. 3
[4] Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pages 12–58, 2014. 2, 6
[5] Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming Sun, Junjie Yan, and Wanli Ouyang. Glit: Neural architecture search for global and local image transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12–21, 2021. 2, 3
[6] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12270–12280, 2021. 2, 3, 5, 6
[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. 2, 3
[8] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, et al. Masked language modeling for proteins via linearly scalable long-context transformers. arXiv preprint arXiv:2006.03555, 2020. 2, 3, 5
[9] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. 3
[10] Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xiaojie Jin, Zhiwu Lu, and Ping Luo. Hr-nas: searching efficient high-resolution neural architectures with lightweight transformers. In Proceedings of the IEEE/CVF Conference

--- TRANG 11 ---
on Computer Vision and Pattern Recognition, pages 2982–2992, 2021. 2, 3
[11] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. 2, 3
[12] Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017. 4, 5
[13] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, Vikas Chandra, et al. Nasvit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training. In International Conference on Learning Representations, 2021. 2, 3
[14] Chaoyu Guan, Xin Wang, and Wenwu Zhu. Autoattend: Automated attention representation search. In International Conference on Machine Learning, pages 3864–3874. PMLR, 2021. 1, 2, 3, 4, 5
[15] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. 2, 3
[16] Chi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo Zhu, and Changliang Li. RankNAS: Efficient neural architecture search by pairwise ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2469–2480, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. 2, 3, 4, 5, 6, 8, 13
[17] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156–5165. PMLR, 2020. 1, 2, 3, 4, 5
[18] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. 2, 3, 5
[19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 2, 6
[20] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pages 3744–3753. PMLR, 2019. 1
[21] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. 3
[22] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018. 3
[23] Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Kay Chen Tan. A survey on evolutionary neural architecture search. IEEE transactions on neural networks and learning systems, 2021. 2, 3
[24] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations, 2021. 3
[25] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022. 1, 2, 3, 4, 5, 6, 8, 9, 10, 13
[26] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019. 2, 3
[27] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 4780–4789, 2019. 2, 3
[28] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. 3
[29] David So, Quoc Le, and Chen Liang. The evolved transformer. In International Conference on Machine Learning, pages 5877–5886. PMLR, 2019. 2, 3, 4, 5
[30] David R So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Primer: Searching for efficient transformers for language modeling. arXiv preprint arXiv:2109.08668, 2021. 3
[31] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. arXiv preprint arXiv:2206.10552, 2022. 1
[32] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438–9447. PMLR, 2020. 2, 3, 5
[33] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv preprint arXiv:2009.06732, 2020. 2, 3
[34] Henry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung Won Chung, and Jason Riesa. Finding fast transformers: One-shot neural architecture search by component composition. arXiv preprint arXiv:2008.06808, 2020. 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1, 2, 4, 5, 6, 8, 9
[36] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat: Hardware-aware transformers for efficient natural language processing. In Annual Conference of the Association for Computational Linguistics, 2020. 2, 3, 6
[37] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 2, 3, 5
[38] Yujing Wang, Yaming Yang, Yiren Chen, Jing Bai, Ce Zhang, Guinan Su, Xiaoyu Kou, Yunhai Tong, Mao Yang, and Lidong Zhou. Textnas: A neural architecture search space tailored for text representation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9242–9249, 2020. 2, 3, 4

--- TRANG 12 ---
[39] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297, 2020. 5
[40] Biao Zhang, Ivan Titov, and Rico Sennrich. Sparse attention with linear units. arXiv preprint arXiv:2104.07012, 2021. 5
[41] Yuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang, Furu Wei, and Weizhu Chen. Memory-efficient differentiable transformer architecture search. arXiv preprint arXiv:2105.14669, 2021. 3

--- TRANG 13 ---
## Tài liệu Bổ sung

### 6. Chi tiết Huấn luyện
Chúng tôi chỉ định chi tiết huấn luyện của các tác vụ dịch máy và phân loại hình ảnh dưới đây. Đối với những cài đặt không được liệt kê, chúng tôi sử dụng cài đặt mặc định trong RankNAS [16].

#### 6.1. Dịch máy
• max relative length: 8
• optimizer: adam
• adam betas: (0.9, 0.998)
• weight decay: 0.1
• max tokens: 4400
• criterion: label smoothed cross entropy
• label smoothing: 0.1
• min learning rate: 10⁻⁹
• max update: 200000
• warmup updates: 4000
• lr scheduler: inverse sqrt
• lr: 0.0007
• warmup init lr: 10⁻⁷
• max lr: 1

#### 6.2. Phân loại hình ảnh
• max relative length: 14
• optimizer: adam
• adam betas: (0.9, 0.998)
• weight decay: 0.05
• max tokens: 100000
• criterion: label smoothed cross entropy
• label smoothing: 0.1
• min learning rate: 10⁻⁷
• max update: 140000
• max sentences: 64
• warmup updates: 976
• lr scheduler: cosine
• lr: 10⁻⁶
• warmup init lr: 10⁻⁶
• max lr: 10⁻³

### 7. Tầm quan trọng Đặc trưng trong mFormer
Với RankNAS [16], chúng ta có thể biết tầm quan trọng của mỗi đặc trưng khi tìm kiếm mFormer được đề xuất.

#### 7.1. Dịch máy
5 đặc trưng hàng đầu về độ chính xác là: Dec Layer Num, En-De Head Num, En-De Head Num, Enc Attn Type, và Dec FFN Dim. Tương tự như Transformer*, đặc trưng quan trọng nhất là số lượng lớp decoder. Trong cấu trúc tối ưu của chúng tôi, số này là 2, điều này cho thấy rằng hiệu suất tốt nhất không nhất thiết cần nhiều lớp. Bên cạnh đó, tương tác encoder-decoder cũng có tác động lớn đến độ chính xác, đòi hỏi thiết kế cẩn thận. Loại chú ý được đề xuất cũng quan trọng, tiếp tục xác thực hiệu quả của tìm kiếm chú ý.

5 đặc trưng hàng đầu về hiệu quả là: Dec Layer Num, Dec Head Num, En-De Connect, En-De Head Num, và Enc FFN Dim. Một lần nữa, số lượng lớp decoder có tác động lớn nhất đến hiệu quả. Lưu ý rằng loại chú ý được đề xuất không có ở đây, điều này là do nó chỉ tồn tại trong encoder với 6 lớp và do đó lợi ích hiệu quả không quá lớn.

#### 7.2. Phân loại hình ảnh
5 đặc trưng hàng đầu về độ chính xác là: Enc Attn Type, Enc Layer Num, Enc FFN Dim, Enc Head Num, và Enc Embed Dim. 5 đặc trưng hàng đầu về hiệu quả là: Enc Layer Num, Enc Attn Type, Enc Embed Dim, Enc Head Num, và Enc FFN Dim.

Rõ ràng, trên tác vụ này, loại chú ý của chúng tôi đóng vai trò quan trọng trong cả độ chính xác và hiệu quả, điều này xác thực tốt hiệu quả của việc sử dụng kết hợp cơ chế chú ý.

### 8. Cơ chế Chú ý Tuyến tính trong Decoder
Trên tác vụ dịch máy, khi chúng tôi đưa cơ chế chú ý tuyến tính vào các lớp decoder, các giá trị BLEU đều là 0 (tương ứng với ba trường hợp: 1) chỉ tự chú ý decoder, 2) chỉ chú ý encoder-decoder, và 3) cả hai). Chúng tôi cũng thấy rằng các câu đầu ra có vấn đề "lặp lại", tức là lặp lại một từ duy nhất nhiều lần. Chúng tôi chưa tìm ra lý do cụ thể cho vấn đề này, và chúng tôi đang liên lạc với các tác giả trong [25]. Chúng tôi suy đoán rằng lý do tiềm ẩn là tính cục bộ trong cơ chế chú ý cosFormer [25] có thể phù hợp hơn cho việc fusion đặc trưng trong encoder.
