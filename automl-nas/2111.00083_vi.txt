# 2111.00083.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2111.00083.pdf
# Kích thước tệp: 2754448 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Một Phương Pháp AutoML Có Thể Mở Rộng Dựa trên Mạng Nơ-ron Đồ Thị
Mossad Helali∗, Essam Mansour∗, Ibrahim Abdelaziz§, Julian Dolby§, Kavitha Srinivas§
∗Đại học Concordia§IBM Research AI
Montreal, Canada New York, US
{fname}.{lname}@concordia.ca,ibrahim.abdelaziz1,dolby,Kavitha.Srinivas@ibm.com
TÓM TẮT
Các hệ thống AutoML xây dựng các mô hình học máy một cách tự động bằng cách thực hiện tìm kiếm trên các phép biến đổi dữ liệu và bộ học hợp lệ, cùng với tối ưu hóa siêu tham số cho mỗi bộ học. Nhiều hệ thống AutoML sử dụng meta-learning để hướng dẫn tìm kiếm các pipeline tối ưu. Trong công trình này, chúng tôi trình bày một hệ thống meta-learning mới có tên KGpip, nó (1) xây dựng cơ sở dữ liệu các tập dữ liệu và các pipeline tương ứng bằng cách khai thác hàng nghìn script với phân tích chương trình, (2) sử dụng embedding tập dữ liệu để tìm các tập dữ liệu tương tự trong cơ sở dữ liệu dựa trên nội dung thay vì các đặc trưng dựa trên metadata, (3) mô hình hóa việc tạo pipeline AutoML như một bài toán sinh đồ thị, để đặc trưng hóa một cách ngắn gọn các pipeline đa dạng được thấy cho một tập dữ liệu duy nhất.
Meta-learning của KGpip là một thành phần con cho các hệ thống AutoML. Chúng tôi chứng minh điều này bằng cách tích hợp KGpip với hai hệ thống AutoML.
Đánh giá toàn diện của chúng tôi sử dụng 121 tập dữ liệu, bao gồm các tập dữ liệu được sử dụng bởi các hệ thống tiên tiến, cho thấy KGpip vượt trội đáng kể so với các hệ thống này.

Định dạng Tham khảo PVLDB:
Mossad Helali∗, Essam Mansour∗, Ibrahim Abdelaziz§, Julian Dolby§,
Kavitha Srinivas§. Một Phương Pháp AutoML Có Thể Mở Rộng Dựa trên Mạng Nơ-ron Đồ Thị. PVLDB, 14(1): XXX-XXX, 2020.
doi:XX.XX/XXX.XX

Tính sẵn có Artifact PVLDB:
Mã nguồn, dữ liệu, và/hoặc các artifact khác đã được cung cấp tại
https://github.com/CoDS-GCS/kgpip-public.

1 GIỚI THIỆU
AutoML là quá trình mà các mô hình học máy được xây dựng tự động cho một tập dữ liệu mới. Với một tập dữ liệu, các hệ thống AutoML thực hiện tìm kiếm trên các phép biến đổi dữ liệu và bộ học hợp lệ, cùng với tối ưu hóa siêu tham số cho mỗi bộ học [ 17].
Việc chọn các phép biến đổi và bộ học để tìm kiếm là trọng tâm của chúng tôi. Một số lượng đáng kể các hệ thống khai thác từ các lần chạy trước của pipeline trên một tập các tập dữ liệu để chọn các transformer và bộ học hiệu quả với các loại tập dữ liệu khác nhau (ví dụ [ 10], [32], [9]). Do đó, chúng xây dựng một cơ sở dữ liệu bằng cách thực sự chạy các pipeline khác nhau với một tập các tập dữ liệu đa dạng để ước lượng độ chính xác của các pipeline tiềm năng. Vì vậy, chúng có thể được sử dụng để giảm hiệu quả không gian tìm kiếm. Một tập dữ liệu mới, dựa trên một tập các đặc trưng (meta-features) sau đó được khớp với cơ sở dữ liệu này để tìm các ứng viên khả thi nhất

Công trình này được cấp phép theo Giấy phép Quốc tế Creative Commons BY-NC-ND 4.0. Truy cập https://creativecommons.org/licenses/by-nc-nd/4.0/ để xem bản sao của giấy phép này. Đối với bất kỳ sử dụng nào vượt quá những gì được bao gồm bởi giấy phép này, hãy xin phép bằng cách gửi email tới info@vldb.org. Bản quyền thuộc sở hữu của chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho VLDB Endowment.
Kỷ yếu của VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX cho cả việc lựa chọn bộ học và điều chỉnh siêu tham số.
Quá trình chọn điểm bắt đầu trong không gian tìm kiếm này được gọi là meta-learning cho vấn đề khởi động lạnh.

Các phương pháp meta-learning khác bao gồm khai thác mã data science hiện có và các tập dữ liệu liên quan để học từ chuyên môn con người. Hệ thống AL [ 2] đã khai thác các notebook Kaggle hiện có bằng phân tích động, tức là thực sự chạy các script, và cho thấy rằng một hệ thống như vậy có tiềm năng. Tuy nhiên, phương pháp meta-learning này không thể mở rộng vì việc thực thi một số lượng lớn script pipeline trên các tập dữ liệu là khó khăn, tiền xử lý các tập dữ liệu không bao giờ đơn giản, và các script cũ hơn ngừng chạy hoàn toàn khi phần mềm phát triển. Không ngạc nhiên khi AL do đó chỉ thực hiện phân tích động trên chín tập dữ liệu.

Hệ thống của chúng tôi, KGpip, cung cấp một phương pháp meta-learning có thể mở rộng để tận dụng chuyên môn con người, sử dụng phân tích tĩnh để khai thác pipeline từ các kho lưu trữ lớn các script. Phân tích tĩnh có ưu điểm mở rộng đến hàng nghìn hoặc hàng triệu script [ 1] một cách dễ dàng, nhưng thiếu dữ liệu hiệu suất được thu thập bởi phân tích động. Phương pháp meta-learning KGpip hướng dẫn quá trình học bằng tìm kiếm độ tương tự tập dữ liệu có thể mở rộng, dựa trên embedding tập dữ liệu, để tìm các tập dữ liệu tương tự nhất và ngữ nghĩa của các pipeline ML được áp dụng trên chúng.

Nhiều hệ thống hiện có, như Auto-Sklearn [ 9] và AL [ 2], tính toán một tập các meta-features cho mỗi tập dữ liệu. Chúng tôi đã phát triển một mô hình mạng nơ-ron sâu để tạo embedding ở mức độ chi tiết của một tập dữ liệu, ví dụ, một bảng hoặc tệp CSV, để nắm bắt độ tương tự ở cấp độ của toàn bộ tập dữ liệu thay vì dựa vào một tập các meta-features.

Bởi vì chúng tôi sử dụng phân tích tĩnh để nắm bắt ngữ nghĩa của quá trình meta-learning, chúng tôi không có cơ chế để chọn pipeline tốt nhất từ nhiều pipeline đã thấy, không giống như trường hợp thực thi động nơi người ta có thể dựa vào runtime để chọn pipeline hiệu suất tốt nhất. Quan sát rằng các pipeline về cơ bản là các đồ thị quy trình công việc, chúng tôi sử dụng các mô hình nơ-ron sinh đồ thị để nắm bắt một cách ngắn gọn các pipeline được quan sát tĩnh cho một tập dữ liệu duy nhất. Trong KGpip, chúng tôi xây dựng việc lựa chọn bộ học như một bài toán sinh đồ thị để dự đoán các pipeline tối ưu dựa trên các pipeline được thấy trong các notebook thực tế.

KGpip thực hiện lựa chọn bộ học và phép biến đổi, và do đó là một thành phần của các hệ thống AutoML. Để đánh giá thành phần này, chúng tôi đã tích hợp nó vào hai hệ thống AutoML hiện có, FLAML [ 30] và Auto-Sklearn [ 9]. Chúng tôi chọn FLAML vì nó chưa có bất kỳ thành phần meta-learning nào cho vấn đề khởi động lạnh và thay vào đó cho phép người dùng lựa chọn bộ học và transformer. Các tác giả của FLAML đã chỉ ra rõ ràng rằng FLAML có thể hưởng lợi từ một thành phần meta-learning và chỉ ra nó như một khả năng cho công việc tương lai. Đối với FLAML, nếu việc khai thác các pipeline lịch sử cung cấp một lợi thế, chúng tôi nên cải thiện hiệu suất của nó. Chúng tôi cũng chọn Auto-Sklearn vì nó có một thành phần lựa chọn bộ học dựa trên meta-features, như được mô tả trước đó [ 8]. Đối với Auto-Sklearn, chúng tôi ít nhất nên khớp hiệu suất nếu việc khai thác tĩnh pipeline của chúng tôi arXiv:2111.00083v4  [cs.LG]  14 Jul 2022

--- TRANG 2 ---
có thể khớp với cơ sở dữ liệu rộng lớn của họ. Để có bối cảnh, chúng tôi cũng so sánh KGpip với VolcanoML gần đây [ 17], cung cấp một chiến lược phân rã và thực thi hiệu quả cho không gian tìm kiếm AutoML.
Ngược lại, KGpip cắt tỉa không gian tìm kiếm bằng mô hình meta-learning của chúng tôi để thực hiện tối ưu hóa siêu tham số chỉ cho các ứng viên hứa hẹn nhất.

Những đóng góp của bài báo này như sau:
•Mục 3 định nghĩa một phương pháp meta-learning có thể mở rộng dựa trên việc học biểu diễn của ngữ nghĩa pipeline ML đã khai thác và các tập dữ liệu cho hơn 100 tập dữ liệu và 11K script Python.
•Mục 4 xây dựng việc sinh pipeline AutoML như một bài toán sinh đồ thị. KGpip dự đoán hiệu quả một pipeline ML tối ưu cho một tập dữ liệu chưa thấy dựa trên mô hình meta-learning của chúng tôi. Theo hiểu biết tốt nhất của chúng tôi, KGpip là phương pháp đầu tiên xây dựng việc sinh pipeline AutoML theo cách như vậy.
•Mục 5 trình bày một đánh giá toàn diện sử dụng một bộ sưu tập lớn 121 tập dữ liệu từ các benchmark AutoML chính và Kaggle. Kết quả thực nghiệm của chúng tôi cho thấy KGpip vượt trội tất cả các hệ thống AutoML hiện có và đạt được kết quả tiên tiến trên phần lớn các tập dữ liệu này. KGpip cải thiện đáng kể hiệu suất của cả FLAML và Auto-Sklearn trong các tác vụ phân loại và hồi quy. Chúng tôi cũng vượt trội AL trong 75 trong số 77 tập dữ liệu và VolcanoML trong 75 trong số 121 tập dữ liệu, bao gồm 44 tập dữ liệu chỉ được sử dụng bởi VolcanoML [ 17]. Trung bình, KGpip đạt được điểm số tốt hơn về mặt thống kê so với trung bình của tất cả các hệ thống khác.

2 CÔNG TRÌNH LIÊN QUAN
Trong mục này, chúng tôi tóm tắt công trình liên quan và giới hạn đánh giá của chúng tôi đến các phương pháp meta-learning cho AutoML, embedding tập dữ liệu, và xử lý dữ liệu có cấu trúc dạng bảng.

Lựa chọn bộ học và tiền xử lý. Trong hầu hết các hệ thống AutoML, việc lựa chọn bộ học và tiền xử lý cho vấn đề khởi động lạnh được điều khiển bởi một cơ sở dữ liệu các lần thực thi thực tế của pipeline và dữ liệu; ví dụ, [ 2], [9], [10]. Cơ sở dữ liệu này thường điều khiển cả việc lựa chọn bộ học và tối ưu hóa siêu tham số (HPO), vì vậy chúng tôi tập trung ở đây nhiều hơn vào cách cơ sở dữ liệu được thu thập hoặc áp dụng cho một trong hai vấn đề, vì việc áp dụng thực tế cho việc lựa chọn bộ học hoặc HPO ít liên quan hơn. Đối với HPO, một số đã mô hình hóa việc áp dụng cơ sở dữ liệu như một vấn đề đa tác vụ (xem [ 26]), nơi các siêu tham số cho khởi động lạnh được chọn dựa trên nhiều tập dữ liệu liên quan. Những người khác, ví dụ, [ 9,25], tính toán một cơ sở dữ liệu các meta-features tập dữ liệu trên nhiều tập dữ liệu OpenML [28], bao gồm các thuộc tính tập dữ liệu như số lượng thuộc tính số, số lượng mẫu hoặc độ lệch của các đặc trưng trong mỗi tập dữ liệu.

Những hệ thống này đo độ tương tự giữa các tập dữ liệu và sử dụng pipeline từ các tập dữ liệu gần nhất dựa trên khoảng cách giữa các vector đặc trưng của tập dữ liệu như chúng tôi làm, nhưng việc tính toán các vector này khác nhau, như chúng tôi mô tả chi tiết dưới đây. Auto-Sklearn 2.0 [8] thay vào đó định nghĩa một danh mục tĩnh các pipeline hoạt động trên nhiều tập dữ liệu đa dạng, và sử dụng chúng để khởi động lạnh thành phần lựa chọn bộ học - nghĩa là, mọi tập dữ liệu mới sử dụng cùng một tập các pipeline. Những người khác đã tạo ra các ma trận lớn ghi lại hiệu suất của các pipeline ứng viên cho các tập dữ liệu khác nhau và xem việc lựa chọn các pipeline liên quan như một vấn đề lọc cộng tác [10].

Embedding tập dữ liệu. Cơ chế được sử dụng nhiều nhất để nắm bắt các đặc trưng tập dữ liệu dựa vào việc sử dụng meta-features cho một tập dữ liệu như [9,25]. Các thuộc tính tập dữ liệu này khác nhau từ đơn giản, như số lượng lớp (xem, ví dụ [ 6]), đến phức tạp và tốn kém, như các đặc trưng thống kê (xem, ví dụ [ 29]) hoặc các đặc trưng mốc (xem, ví dụ [ 23]). Như được chỉ ra trong Auto-Sklearn 2.0 [ 8], các meta-features này không được định nghĩa đối với các loại cột nhất định như cột phân loại, và chúng cũng tốn kém để tính toán, trong ngân sách hạn chế. Embedding tập dữ liệu mà chúng tôi áp dụng xây dựng các embedding cột riêng lẻ, và sau đó gộp chúng cho một embedding cấp bảng.

Tương tự như phương pháp của chúng tôi, Drori et al . [5] sử dụng các mô hình ngôn ngữ được đào tạo trước để có embedding tập dữ liệu dựa trên thông tin văn bản tập dữ liệu có sẵn, ví dụ tiêu đề, mô tả và từ khóa. Với những embedding này, phương pháp của họ cố gắng tìm các tập dữ liệu tương tự nhất và các baseline liên quan của chúng. Không giống như [ 5], phương pháp của chúng tôi dựa vào việc embedding dữ liệu thực tế bên trong tập dữ liệu chứ không chỉ mô tả văn bản tổng thể của chúng, mà trong nhiều trường hợp không có sẵn.
OBOE [ 33] sử dụng hiệu suất của một vài mô hình không tốn kém, có thông tin để tính toán các đặc trưng của một mô hình.

Sinh pipeline. Có một lượng công việc đáng kể xem việc lựa chọn bộ học cũng như siêu tham số như một vấn đề tối ưu hóa bayesian như [ 26,27]. Các hệ thống khác đã sử dụng các thuật toán tiến hóa cùng với các template hoặc ngữ pháp do người dùng định nghĩa cho mục đích này như TPOT [ 15] hoặc Recipe [ 4]. Tuy nhiên, những người khác đã xem vấn đề sinh pipeline như một phân rã ma trận xác suất [ 10], một vấn đề lập kế hoạch AI khi kết hợp với một ngữ pháp do người dùng chỉ định [ 14,31], một vấn đề tối ưu hóa bayesian kết hợp với Tìm kiếm Cây Monte Carlo [ 24], hoặc một vấn đề tối ưu hóa phương pháp nhân tử thay thế lặp lại (ADMM) [ 19]. Các hệ thống như VolcanoML tập trung vào một phân rã hiệu quả của không gian tìm kiếm [ 17]. Theo hiểu biết tốt nhất của chúng tôi, KGpip là hệ thống đầu tiên mô hình hóa việc sinh thực tế của pipeline như một vấn đề sinh đồ thị nơ-ron.

Một số hệ thống AutoML gần đây đã chuyển từ các pipeline khá tuyến tính được tạo ra bởi hầu hết các hệ thống trước đó để sử dụng ensemble hoặc stacking một cách rộng rãi. H2O chẳng hạn sử dụng tìm kiếm ngẫu nhiên nhanh kết hợp với ensembling cho vấn đề sinh pipeline [ 16]. Những người khác dựa vào "stacking một tập các mô hình tùy chỉnh theo thứ tự được định nghĩa trước", nơi stacking và training được xử lý theo cách đặc biệt để đạt được hiệu suất mạnh [ 7]. Tương tự, PIPER [20] sử dụng một thuật toán tìm kiếm tham lam best-first để duyệt không gian các pipeline một phần được hướng dẫn trên một ngữ pháp định nghĩa các pipeline phức tạp như Đồ thị Có hướng Không chu trình (DAGs). Các pipeline được tạo ra bởi PIPER phức tạp hơn các cấu trúc tuyến tính được sử dụng trong các hệ thống AutoML hiện tại mà chúng tôi sử dụng để kiểm tra ý tưởng của chúng tôi cho việc mô hình hóa pipeline lịch sử, và chúng tôi chưa sử dụng các kỹ thuật ensembling trong phương pháp của chúng tôi. Tuy nhiên, không có điều gì bị loại trừ, vì mô hình meta-learning KGpip có thể sinh ra bất kỳ loại cấu trúc nào, bao gồm các cấu trúc phức tạp mà các pipeline đã khai thác có thể có.
2

--- TRANG 3 ---
Hình 1: Tổng quan về phương pháp meta-learning của KGpip để khai thác cơ sở dữ liệu các pipeline ML để huấn luyện một mô hình sinh đồ thị dự đoán các khung pipeline ML dưới dạng đồ thị.

3 META-LEARNING CÓ THỂ MỞ RỘNG CỦA KGPIP
Phương pháp meta-learning của chúng tôi dựa trên việc khai thác các cơ sở dữ liệu lớn các pipeline ML liên quan đến các tập dữ liệu được sử dụng, như được minh họa trong Hình 1. Quá trình khai thác sử dụng phân tích chương trình tĩnh thay vì thực thi các script pipeline thực tế hoặc chuẩn bị dữ liệu thô thực tế. Thành phần meta-learning KGpip tăng cường chiến lược tìm kiếm của các hệ thống AutoML hiện có, như AutoSklearn và FLAML, và cho phép các hệ thống này xử lý các tập dữ liệu ad-hoc, tức là những tập chưa thấy. Để duy trì mức độ linh hoạt tối đa, KGpip nắm bắt metadata và ngữ nghĩa trong một định dạng đồ thị linh hoạt, và dựa vào các mô hình sinh đồ thị như cơ sở dữ liệu của các pipeline.

Không giống như các phương pháp meta-learning hiện có, phương pháp của chúng tôi được thiết kế để học từ một cơ sở dữ liệu quy mô lớn và đạt được mức độ bao phủ và đa dạng cao. Một số cổng thông tin ML, như Kaggle hoặc OpenML [ 28], cung cấp quyền truy cập vào hàng nghìn tập dữ liệu liên quan đến hàng trăm nghìn notebook công khai, tức là các pipeline/mã ML. KGpip khai thác những cơ sở dữ liệu lớn này của các tập dữ liệu và pipeline bằng phân tích tĩnh và lọc chúng thành các pipeline ML được tùy chỉnh cho vấn đề lựa chọn bộ học. Phương pháp meta-learning KGpip tận dụng [ 1] để hiểu mã thông qua phân tích tĩnh các script/mã của các pipeline ML. Nó trích xuất ngữ nghĩa của những script này như mã và tạo thành một đồ thị ban đầu cho mỗi script.
KGpip làm sạch các đồ thị được tạo ra bởi [ 1] để giữ lại ngữ nghĩa cần thiết cho quá trình meta-learning ML. Hơn nữa, phương pháp của chúng tôi giới thiệu các nút tập dữ liệu và liên kết ngữ nghĩa pipeline liên quan với chúng. Vì vậy, phương pháp meta-learning của chúng tôi tạo ra MetaPip, một đồ thị có liên kết cao của các tập dữ liệu đã thấy và các pipeline được áp dụng cho chúng. Chúng tôi cũng phát triển một mô hình embedding sâu để tìm các tập dữ liệu gần nhất với một tập chưa thấy, tức là để cắt tỉa MetaPip một cách hiệu quả.
Sau đó chúng tôi huấn luyện một mô hình sinh đồ thị sâu [ 18] sử dụng MetaPip.
Mô hình này là cốt lõi của thành phần meta-learning của chúng tôi như được minh họa trong Hình 1 và thảo luận trong mục tiếp theo.

3.1 Biểu diễn Đồ thị của Ngữ nghĩa Mã
Các kỹ thuật phân tích chương trình tĩnh và động có thể được sử dụng để trừu tượng hóa ngữ nghĩa của các chương trình và trích xuất các biểu diễn độc lập với ngôn ngữ của mã. Mã nguồn chương trình được kiểm tra trong phân tích tĩnh mà không chạy chương trình. Ngược lại, phân tích động kiểm tra mã nguồn trong runtime để thu thập traces bộ nhớ và thống kê chi tiết hơn đặc trưng cho kỹ thuật phân tích. Không giống như phân tích tĩnh, phân tích động giúp nắm bắt ngữ nghĩa phong phú hơn từ các chương trình với chi phí cao của việc thực thi và lưu trữ traces bộ nhớ khổng lồ. Các cổng thông tin ML, như

 df = pd. read_csv ('example.csv ')
 df_train, df_test = train_test_split (df)
 X = df_train[' X'] 
 model = svm. SVC()
 model.fit(X, df_train[' Y'])

Hình 2: Một ví dụ từ một notebook data science.

read_csvtrain_test_splitdf_train['X']df_train['Y']SVC/f_it

Hình 3: Đồ thị mã tương ứng với Hình 2 được thu với GraphGen4Code. Đồ thị hiển thị luồng điều khiển với các cạnh màu xám và luồng dữ liệu với các cạnh màu đen. Nhiều nút và cạnh khác không được hiển thị để đơn giản.

read_csvtrain_test_splitSVC/f_itexample.csv

Hình 4: Đồ thị MetaPip của chúng tôi từ đồ thị ở Hình 3, nơi pipeline ML được trừu tượng hóa được liên kết với một nút tập dữ liệu (được tô màu cam). MetaPip chứa ít nhất 96% ít nút và cạnh hơn đồ thị gốc trong khi tăng cường chất lượng tổng thể của quá trình sinh đồ thị, như được thử nghiệm trong Mục 5.5.

Kaggle, có hàng trăm nghìn pipeline ML mà không có hướng dẫn để chạy hoặc quản lý môi trường của những pipeline này.
KGpip kết hợp embedding tập dữ liệu với các công cụ phân tích mã tĩnh, như GraphGen4Code [ 1], để làm phong phú ngữ nghĩa thu thập được của các pipeline ML trong khi tránh nhu cầu chạy chúng.

GraphGen4Code được tối ưu hóa để xử lý hiệu quả hàng triệu chương trình Python, thực hiện phân tích luồng dữ liệu và luồng điều khiển liên thủ tục để kiểm tra chẳng hạn, điều gì xảy ra với dữ liệu được đọc từ một dataframe Pandas, nó được thao tác và biến đổi như thế nào, và những transformer hoặc estimator nào được gọi trên dataframe. Các đồ thị của GraphGen4Code làm cho việc gọi các API và hàm nào trên các đối tượng trở nên rõ ràng mà không cần mô hình hóa các thư viện được sử dụng; do đó GraphGen4Code có thể mở rộng phân tích tĩnh đến hàng triệu chương trình. Hình 2 và 3 hiển thị một đoạn mã nhỏ và đồ thị phân tích tĩnh tương ứng từ GraphGen4Code, tương ứng. Như được hiển thị trong Hình 3, đồ thị nắm bắt luồng điều khiển (cạnh màu xám), luồng dữ liệu (cạnh màu đen), cũng như nhiều nút và cạnh khác không được hiển thị trong hình. Ví dụ về những nút và cạnh này bao gồm những nút nắm bắt vị trí của các lời gọi bên trong một tệp script và các tham số lời gọi hàm.
Ví dụ, GraphGen4Code tạo ra một đồ thị khoảng 1600 nút và 3700 cạnh cho một script pipeline ML Kaggle có 72 dòng mã. Số lượng nút và cạnh chi phối độ phức tạp của việc huấn luyện một mô hình sinh đồ thị.

3.2 MetaPip: từ Mã đến Ngữ nghĩa Pipeline
Đối với các hệ thống AutoML, một pipeline là một tập các phép biến đổi dữ liệu, lựa chọn bộ học, và tối ưu hóa siêu tham số cho mỗi mô hình được chọn. Các notebook data science đã khai thác thường chứa phân tích dữ liệu, trực quan hóa dữ liệu, và đánh giá mô hình. Hơn nữa, mỗi
3

--- TRANG 4 ---
notebook được liên kết với một hoặc nhiều tập dữ liệu. Do đó, điều cần thiết cho mô hình meta-learning của chúng tôi là phân biệt giữa các loại pipeline khác nhau và nhận ra mối liên kết này với các tập dữ liệu. Các hệ thống hiện có cho phân tích mã tĩnh trích xuất ngữ nghĩa tổng quát của mã và không thể liên kết các script pipeline với các tập dữ liệu được sử dụng. Do đó, các đồ thị được tạo ra bởi các hệ thống, như GraphGen4Code, bị phân tán và không liên kết, tức là một đồ thị cho mỗi script pipeline ML. Hơn nữa, mỗi đồ thị sẽ có các nút và cạnh không liên quan đến quá trình meta-learning. Những nút và cạnh không liên quan này, tức là bộ ba, sẽ thêm nhiễu vào dữ liệu huấn luyện. Do đó, một mô hình meta-learning sẽ không thể học từ các pipeline đồ thị trừu tượng được tạo ra bởi các công cụ như vậy, như được hiển thị trong Bảng 4. Chúng tôi đã phát triển một phương pháp để lọc ra loại bộ ba này từ đồ thị của GraphGen4Code và phân tích các pipeline ML để chuẩn bị một tập huấn luyện kết nối các kho lưu trữ script pipeline ML với các tập dữ liệu liên quan của chúng. Hơn nữa, phương pháp của chúng tôi làm sạch các nút và cạnh nhiễu và các lời gọi đến các module bên ngoài thư viện ML mục tiêu. Ví dụ, phương pháp của chúng tôi sẽ trích xuất các bộ ba liên quan đến các thư viện, như Scikit-learn, XGBoost, và LGBM. Những thư viện này phổ biến nhất trong số các pipeline ML đạt điểm cao nhất trong các cổng thông tin ML. Mã cho phương pháp làm sạch có sẵn tại kho lưu trữ của KGpip.

Thành phần meta-learning của chúng tôi nhằm chọn bộ học và transformer cho các tập dữ liệu chưa thấy. Do đó, KGpip liên kết các pipeline ML đã lọc với các tập dữ liệu được sử dụng. Kết quả của việc thêm những nút tập dữ liệu này là một đồ thị có liên kết cao cho các pipeline ML, chúng tôi gọi nó là MetaPip . Đồ thị MetaPip của chúng tôi nắm bắt cả khía cạnh mã và dữ liệu của các pipeline ML. Do đó, chúng tôi có thể điền vào đồ thị MetaPip với các tập dữ liệu từ các nguồn khác nhau, như OpenML và Kaggle, và các pipeline được áp dụng trên những tập dữ liệu này. Hình 4 hiển thị đồ thị MetaPip tương ứng với đoạn mã trong Hình 2. KGpip sử dụng MetaPip để huấn luyện một mô hình dựa trên một tập lớn các pipeline liên quan đến các tập dữ liệu tương tự. Ví dụ, một nút pandas.read_csv sẽ được liên kết với nút bảng được sử dụng, tức là tệp csv. Trong một số trường hợp, mã đọc tệp csv không nêu rõ tên tập dữ liệu.
Các pipeline thường được liên kết với các tập dữ liệu, như các pipeline và tập dữ liệu Kaggle, như được hiển thị trong Hình 1.

3.3 Học Biểu diễn Tập dữ liệu
Phương pháp của chúng tôi hướng dẫn hiệu quả quá trình meta-learning bằng cách liên kết ngữ nghĩa được trích xuất của các pipeline với các nút tập dữ liệu đại diện cho các tập dữ liệu được sử dụng. Có một lượng lớn các tập dữ liệu có kích thước khác nhau và chúng tôi cần phát triển một phương pháp có thể mở rộng để tìm các tập dữ liệu tương tự nhất cho một tập chưa thấy. Việc so sánh theo cặp dựa trên nội dung thực tế của các tập dữ liệu, tức là các bộ trong tệp CSV, không thể mở rộng. Do đó, chúng tôi đã phát triển một phương pháp học biểu diễn tập dữ liệu để tạo ra một embedding có kích thước cố định và dày đặc ở mức độ chi tiết của một tập dữ liệu, ví dụ, một bảng hoặc tệp CSV. Embedding của một tập dữ liệu D là trung bình của các embedding cột của nó, tức là:

ℎ_θ(D) = 1/|D| ∑_{c∈D} ℎ_θ(c) (1)

trong đó |D| là số lượng cột trong D. Công trình của chúng tôi tổng quát hóa phương pháp được phác thảo trong [ 21] cho các embedding cột riêng lẻ, nơi các embedding cột được thu được bằng cách huấn luyện một mạng nơ-ron trên một tác vụ phân loại nhị phân. Mô hình học khi nào hai cột đại diện cho cùng một khái niệm, nhưng với các giá trị khác nhau, trái ngược với

Hình 5: Tổng quan về quy trình công việc của KGpip sinh pipeline ML cho một tập dữ liệu chưa thấy nhất định và ngân sách thời gian nhất định. KGpip sử dụng các hệ thống để tối ưu hóa siêu tham số, như FLAML hoặc Auto-Sklearn, để tối ưu hóa các pipeline top-K được dự đoán của KGpip ( 𝑉𝐺), tức là cắt tỉa không gian tìm kiếm.

các cột đại diện cho các khái niệm khác nhau. Các embedding cho một tập dữ liệu chưa thấy được tạo ra bởi lớp cuối cùng của mạng nơ-ron.

KGpip đọc các tập dữ liệu chỉ một lần và tận dụng PySpark DataFrame để đạt được tính song song tác vụ và dữ liệu cao. Chúng tôi sử dụng các embedding của các tập dữ liệu để đo độ tương tự của chúng. Với những embedding này, chúng tôi xây dựng một chỉ mục của các embedding vector cho tất cả các tập dữ liệu trong tập huấn luyện của chúng tôi. Chúng tôi sử dụng các thư viện hiệu quả [ 13] để tìm kiếm độ tương tự của các vector dày đặc để truy xuất tập dữ liệu tương tự nhất với một tập dữ liệu đầu vào mới dựa trên embedding của nó. Do đó, phương pháp của chúng tôi mở rộng tốt và dẫn đến kết quả chính xác trong việc nắm bắt độ tương tự giữa các tập dữ liệu.

4 TỰ ĐỘNG HÓA PIPELINE KGPIP
Quy trình công việc KGpip cho tự động hóa pipeline dựa trên mô hình meta-learning của chúng tôi, như được minh họa trong Hình 5. KGpip dự đoán các khung pipeline top-K, tức là một tập cụ thể { 𝑃,𝐸} của Preprocessor ( 𝑃) và Estimators ( 𝐸), cho một tập dữ liệu chưa thấy ( 𝐷) dựa trên tập dữ liệu đã thấy tương tự nhất ( 𝑆𝐷), tức là tập dữ liệu láng giềng gần nhất. KGpip bắt đầu bằng việc tìm 𝑆𝐷 dựa trên embedding của tập dữ liệu chưa thấy. Sau đó, KGpip tạo ra các đồ thị pipeline ML top-K đã được xác thực 𝑉𝐺 và chuyển đổi chúng thành các khung pipeline ML {𝑃,𝐸}. Sau đó, nó thực hiện tối ưu hóa siêu tham số sử dụng các hệ thống, như FLAML [ 30] và Auto-Sklearn [ 9], để tìm các siêu tham số tối ưu cho mỗi khung pipeline trong một ngân sách thời gian cụ thể.

4.1 Sinh Đồ thị cho Pipeline ML
KGpip xây dựng việc sinh các pipeline ML như một bài toán sinh đồ thị. Trực giác đằng sau ý tưởng này là một bộ sinh đồ thị nơ-ron có thể nắm bắt một cách ngắn gọn hơn nhiều pipeline được thấy trong thực tế cho một tập dữ liệu nhất định, và cũng có thể nắm bắt độ tương tự thống kê giữa các pipeline khác nhau một cách hiệu quả hơn. Để sử dụng hiệu quả một mạng như vậy, chúng tôi thêm một nút tập dữ liệu duy nhất làm điểm bắt đầu cho các pipeline đã lọc mà chúng tôi tạo ra từ các notebook Python. Nút được giả định chảy vào một lời gọi read_csv mà thường là điểm bắt đầu cho các pipeline. Để sinh một pipeline ML, chúng tôi đơn giản truyền vào một nút tập dữ liệu cho láng giềng gần nhất của tập dữ liệu chưa thấy, tức là tập dữ liệu tương tự nhất dựa trên độ tương tự nội dung, như được hiển thị trong Hình 5.

Mô hình meta-learning của chúng tôi sinh các đồ thị pipeline ML theo cách tuần tự từng nút một. Thuật toán 1 minh họa việc triển khai mô hình sinh đồ thị. Với một đồ thị trống 𝐺
4

--- TRANG 5 ---
Thuật toán 1: Quá trình Sinh Đồ thị
Đầu vào: Đồ thị 𝐺: (𝐸=𝜙,𝑉=𝜙), Nút Tập dữ liệu Tương tự: 𝑆𝐷,
Mạng Nơ-ron: 𝑓_{AddNode},𝑓_{AddEdge},𝑓_{ChooseNode}
1𝑉←𝑉∪{𝑆𝐷,𝑝𝑎𝑛𝑑𝑎𝑠.𝑟𝑒𝑎𝑑_𝑐𝑠𝑣}
2𝐸←𝐸∪{(𝑆𝐷,𝑝𝑎𝑛𝑑𝑎𝑠.𝑟𝑒𝑎𝑑_𝑐𝑠𝑣)}
3𝑛𝑜𝑑𝑒𝑇𝑜𝐴𝑑𝑑 =𝑓_{AddNode}(𝑉,𝐸)
4while𝑛𝑜𝑑𝑒𝑇𝑜𝐴𝑑𝑑 ≠𝑁𝑢𝑙𝑙 do
5𝑉←𝑉∪{𝑛𝑜𝑑𝑒𝑇𝑜𝐴𝑑𝑑}
6𝑎𝑑𝑑𝐸𝑑𝑔𝑒 =𝑓_{AddEdge}(𝑉,𝐸)
7 while𝑎𝑑𝑑𝐸𝑑𝑔𝑒 do
8𝑛𝑜𝑑𝑒𝑇𝑜𝐿𝑖𝑛𝑘 =𝑓_{ChooseNode}(𝑉,𝐸)
9𝐸←𝐸∪{(𝑛𝑜𝑑𝑒𝑇𝑜𝐴𝑑𝑑,𝑛𝑜𝑑𝑒𝑇𝑜𝐿𝑖𝑛𝑘 )}
10𝑎𝑑𝑑𝐸𝑑𝑔𝑒←𝑓_{AddEdge}(𝑉,𝐸)
11 end
12𝑛𝑜𝑑𝑒𝑇𝑜𝐴𝑑𝑑←𝑓_{AddNode}(𝑉,𝐸)
13end
14𝑉𝐺=𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑒_𝑝𝑖𝑝𝑒𝑙𝑖𝑛𝑒_𝑔𝑟𝑎𝑝ℎ(𝐺)
15return VG

và tập dữ liệu tương tự nhất 𝑆𝐷, thuật toán bắt đầu bằng việc thêm một cạnh giữa 𝑆𝐷 và pandas.read_csv . Sau đó, mạng nơ-ron đồ thị 𝑓_{AddNode} quyết định có thêm một nút mới của một loại nhất định hay không. Mạng 𝑓_{AddEdge} quyết định có thêm một cạnh vào nút mới được thêm hay không. Sau đó, mạng 𝑓_{ChooseNode} quyết định nút hiện có mà cạnh sẽ được thêm vào. Vòng lặp While ở dòng 7 được lặp lại cho đến khi không có cạnh nào được thêm nữa.
Vòng lặp While ở dòng 4 được lặp lại cho đến khi không có nút nào được thêm nữa. Ba mạng nơ-ron, cụ thể là 𝑓_{AddNode} ,𝑓_{AddEdge} , và 𝑓_{ChooseNode} , sử dụng các embedding nút được học trong quá trình huấn luyện thông qua các vòng lan truyền đồ thị. Những embedding này nắm bắt cấu trúc của các đồ thị pipeline ML.

Đồ thị được sinh ra 𝐺 không được đảm bảo là một pipeline ML hợp lệ. Do đó, Thuật toán 1 ở dòng 14 kiểm tra rằng 𝐺 là một đồ thị pipeline ML hợp lệ. Trong KGpip, một đồ thị 𝐺 hợp lệ nếu 1) nó chứa ít nhất một estimator khớp với tác vụ, tức là hồi quy hoặc phân loại, và 2) estimator được hỗ trợ bởi bộ tối ưu hóa siêu tham số (AutoSklearn hoặc FLAML trong trường hợp của chúng tôi). Với những sửa đổi này, có thể sinh các pipeline ML cho các tập dữ liệu chưa thấy bằng cách sử dụng nút tập dữ liệu đã thấy gần nhất – cụ thể hơn, embedding nội dung của nó được thu từ module embedding tập dữ liệu. Chúng tôi đã xây dựng Thuật toán 1 dựa trên hệ thống được đề xuất trong [ 18]. Hệ thống này không hỗ trợ sinh đồ thị có điều kiện tại thời điểm kiểm tra theo mặc định, tức là xây dựng một đồ thị trên một nút tập dữ liệu được cung cấp. Chúng tôi đã mở rộng hệ thống này để sinh các đồ thị pipeline ML hợp lệ, như được minh họa trong Thuật toán 1.

4.2 Tối ưu hóa Siêu tham số
KGpip ánh xạ các đồ thị hợp lệ thành các khung pipeline ML, nơi mỗi khung là một tập các bộ tiền xử lý và một estimator với các vị trí giữ chỗ cho các tham số tối ưu. Trong KGpip, bộ tối ưu hóa siêu tham số chịu trách nhiệm tìm các tham số tối ưu cho các bộ tiền xử lý và bộ học trên tập dữ liệu mục tiêu. Sau đó, KGpip thay thế các vị trí giữ chỗ bằng những tham số này. Cuối cùng, KGpip tạo một script python sử dụng các bộ tiền xử lý và estimator đạt được điểm số cao nhất. KGpip được thiết kế tốt để hỗ trợ cả các tập dữ liệu số và không số. Do đó, KGpip áp dụng các kỹ thuật tiền xử lý khác nhau trên tập dữ liệu nhất định ( 𝐷) và tạo ra một tập dữ liệu đã được tiền xử lý ( 𝐷′). Việc tiền xử lý của chúng tôi bao gồm 1) phát hiện loại tác vụ (tức là hồi quy hoặc phân loại) tự động dựa trên phân phối của cột mục tiêu 2) tự động suy luận các loại dữ liệu chính xác của các cột, 3) vector hóa các cột văn bản bằng word embeddings [ 3], và 4) điền các giá trị thiếu trong tập dữ liệu. Trong KGpip, bộ tối ưu hóa siêu tham số sử dụng 𝐷′.

Tương tự như các bộ tối ưu hóa siêu tham số được triển khai trong các hệ thống AutoML, như FLAML hoặc Auto-Sklearn, KGpip hoạt động trong một ngân sách thời gian được cung cấp cho mỗi tập dữ liệu. Chúng tôi lưu ý ở đây rằng phần lớn ngân sách thời gian được phân bổ cho việc sinh pipeline ML được dành cho tối ưu hóa siêu tham số; nghĩa là, nếu người dùng chỉ muốn biết những bộ học nào sẽ hoạt động tốt nhất cho tập dữ liệu của họ, KGpip có thể làm điều đó gần như ngay lập tức. Với một ngân sách thời gian ( 𝑇), KGpip tính toán 𝑡, thời gian tiêu thụ trong việc sinh và xác thực các đồ thị. KGpip sau đó chia phần còn lại của ngân sách thời gian giữa 𝐾 đồ thị. Do đó, bộ tối ưu hóa siêu tham số có giới hạn thời gian ((𝑇−𝑡)/𝐾) để tối ưu hóa mỗi đồ thị một cách độc lập.

Bộ tối ưu hóa siêu tham số lặp đi lặp lại áp dụng các bộ học và bộ tiền xử lý với các cấu hình khác nhau trong khi theo dõi số liệu điểm mục tiêu trong suốt quá trình. KGpip tiếp tục cập nhật đầu ra của nó với khung pipeline tốt nhất, tức là bộ học và bộ tiền xử lý, và điểm số của nó. Ví dụ, nếu bộ học được dự đoán là LogisticRegression, nó tìm kiếm sự kết hợp tốt nhất của loại regularization (L1 hoặc L2) và tham số regularization. Sự khác biệt giữa các bộ tối ưu hóa siêu tham số là chiến lược tìm kiếm được theo dõi để đạt đến các siêu tham số tốt nhất trong ngân sách thời gian được phân bổ. Một phương pháp ngây thơ sẽ là thực hiện tìm kiếm lưới đầy đủ trên tất cả các kết hợp, trong khi một phương pháp tiên tiến hơn sẽ là bắt đầu với các cấu hình hứa hẹn trước. Chúng tôi tích hợp KGpip với các bộ tối ưu hóa siêu tham số của cả FLAML [ 30] và Auto-Sklearn [9] để chứng minh tính tổng quát của KGpip. Việc tích hợp một bộ tối ưu hóa siêu tham số vào KGpip cần một tài liệu JSON của các bộ tiền xử lý và estimator cụ thể được hỗ trợ bởi bộ tối ưu hóa siêu tham số. Do đó, việc tích hợp tương đối dễ dàng. Cuối cùng, việc sinh đồ thị nơ-ron của chúng tôi tạo ra một tập các pipeline đa dạng qua các lần chạy, cho phép khám phá và khai thác.

5 CÁC THỰC NGHIỆM
5.1 Benchmark
Chúng tôi đánh giá KGpip cũng như các baseline khác trên bốn tập dữ liệu benchmark: 1) Open AutoML Benchmark [11], một bộ sưu tập 39 tập dữ liệu phân loại nhị phân và đa lớp (được sử dụng bởi FLAML [ 30]). Các tập dữ liệu được chọn sao cho chúng đại diện cho thế giới thực từ đa dạng các lĩnh vực vấn đề và có đủ độ khó cho các thuật toán học. 2) Penn Machine Learning Benchmark (PMLB) [ 22]: Vì Open AutoML Benchmark giới hạn ở các tập dữ liệu phân loại, các tác giả của FLAML [ 30] đã đánh giá hệ thống của họ trên 14 tập dữ liệu hồi quy được chọn thêm từ PMLB, sao cho số lượng mẫu lớn hơn 10,000. Để chứng minh tính tổng quát của phương pháp, chúng tôi cũng bao gồm những tập dữ liệu đó trong đánh giá của chúng tôi. 3) Tập dữ liệu của AL : Chúng tôi cũng đánh giá trên các tập dữ liệu được sử dụng cho đánh giá của AL [ 2] bao gồm 6 tập dữ liệu Kaggle (2 hồi quy và 4 phân loại) và 18 tập dữ liệu phân loại khác (9 từ PMLB và 9 từ OpenML). Không giống như các benchmark khác, các tập dữ liệu Kaggle bao gồm các tập dữ liệu với các đặc trưng văn bản. 4) Tập dữ liệu của VolcanoML: cuối cùng, chúng tôi đánh giá KGpip trên 44 tập dữ liệu được sử dụng thêm bởi VolcanoML [ 17]. Các tác giả của VolcanoML đánh giá hệ thống của họ trên tổng cộng 66 tập dữ liệu từ OpenML và Kaggle, từ đó
5

--- TRANG 6 ---
Bảng 1: Phân tích tất cả 121 tập dữ liệu được sử dụng trong đánh giá của chúng tôi, chỉ ra những tập được sử dụng bởi FLAML∗, AL†, và VolcanoML§.

Nguồn
Tác vụ | AutoML | PMLB | OpenML | Kaggle
Nhị phân | 22(18∗+1∗†+3∗§) | 5(4†+1†§) | 27(3†§+3†+21§) | 2†
Đa lớp | 17(15∗+1∗†+1∗§) | 4† | 7(2†§+1†+ 4§) | 2†
Hồi quy | 0 | 14∗ | 19§ | 2†
Tổng | 39 | 23 | 53 | 6

11 tập dữ liệu không được chỉ định, 10 tập dữ liệu trùng lặp với của chúng tôi, và 1 tập dữ liệu gồm các mẫu hình ảnh. Bảng 1 bao gồm tóm tắt tất cả 121 tập dữ liệu benchmark. Thống kê chi tiết của tất cả các tập dữ liệu được hiển thị trong phụ lục. Những thống kê này bao gồm tên, số hàng và cột, số đặc trưng số, phân loại, và văn bản, số lớp, kích thước, nguồn, và các bài báo đã đánh giá trên chúng.

5.2 Baseline
Chúng tôi xác thực thực nghiệm KGpip so với ba hệ thống AutoML: (1) Auto-Sklearn (v0.14.0) [ 9] là người chiến thắng tổng thể của nhiều thử thách trong cuộc thi ChaLearn AutoML [ 12], và một trong 4 đối thủ hàng đầu được báo cáo trong Open AutoML Benchmark [11]. (2) FLAML (v0.6.6) [ 30]: một thư viện AutoML được thiết kế với cả độ chính xác và chi phí tính toán trong tâm trí. FLAML vượt trội Auto-Sklearn trong số các hệ thống khác trên hai benchmark AutoML sử dụng ngân sách tính toán thấp, (3) AL [ 2]: một phương pháp AutoML dựa trên meta-learning sử dụng phân tích động của các notebook Kaggle, một phương pháp có điểm tương đồng với của chúng tôi, và (4) VolcanoML (v0.5.0) [ 17], một phương pháp AutoML gần đây đề xuất các chiến lược phân rã hiệu quả cho không gian tìm kiếm AutoML lớn. Trong tất cả các thực nghiệm của chúng tôi, chúng tôi đã sử dụng mã mới nhất được cung cấp bởi các tác giả cho các hệ thống hiện có, cùng phần cứng, ngân sách thời gian, và các tham số được khuyến nghị bởi các tác giả của những hệ thống này.

5.3 Thiết lập Huấn luyện
Bởi vì phương pháp của chúng tôi để khai thác các pipeline lịch sử từ các script tương đối rẻ, chúng tôi có thể áp dụng nó dễ dàng hơn trên một loạt các tập dữ liệu rộng hơn để tạo thành một cơ sở tốt hơn khi ngày càng nhiều script được tạo ra bởi các chuyên gia lĩnh vực trong các cuộc thi Kaggle. Trong công trình này, chúng tôi đã thực hiện phân tích chương trình trên 11.7K script liên quan đến 142 tập dữ liệu, và sau đó chọn những script có estimator từ sklearn , XGBoost và LightGBM vì đó là những estimator được hỗ trợ bởi hầu hết các hệ thống AutoML cho phân loại và hồi quy. Điều này dẫn đến việc lựa chọn 2,046 notebook cho 104 tập dữ liệu; một phần lớn của 11.7K chương trình là về phân tích dữ liệu khám phá, hoặc liên quan đến các thư viện không được hỗ trợ bởi Auto-Sklearn [ 9] hoặc FLAML (ví dụ, PyTorch và Keras) [ 30]. Chúng tôi đã sử dụng Macro F1 cho các tác vụ phân loại để tính đến sự mất cân bằng dữ liệu, nếu có, và sử dụng 𝑅² cho các tác vụ hồi quy, như trong FLAML [ 30]. Chúng tôi cũng thay đổi ngân sách thời gian được cung cấp cho mỗi hệ thống giữa 1 giờ và 30 phút, để đo KGpip có thể tìm một pipeline hiệu quả nhanh như thế nào so với các phương pháp khác. Ngân sách thời gian là từ đầu đến cuối, từ việc tải tập dữ liệu đến tạo ra pipeline AutoML tốt nhất. Trong tất cả các thực nghiệm, chúng tôi báo cáo trung bình trên 3 lần chạy.

Bảng 2: Điểm số trung bình (trung bình và độ lệch chuẩn) của KGpip so với FLAML, Auto-Sklearn, và VolcanoML cho phân loại nhị phân (F1), phân loại đa lớp (F1) và các tác vụ hồi quy ( 𝑅²) trên 77 tập dữ liệu benchmark. Giá trị T-test cho KGpip vs. FLAML và KGpip vs. Auto-Sklearn.

| | Nhị phân | Đa lớp | Hồi quy | T-Test |
|---|---|---|---|---|
| FLAML | 0.74 (0.23) | 0.70 (0.29) | 0.65 (0.29) | 0.0129 |
| KGpip_FLAML | 0.81 (0.14) | 0.76 (0.24) | 0.72 (0.24) | - |
| Auto-Sklearn | 0.76 (0.20) | 0.65 (0.29) | 0.71 (0.24) | 0.0002 |
| KGpip_AutoSklearn | 0.83 (0.14) | 0.73 (0.28) | 0.72 (0.24) | - |
| VolcanoML | 0.55 (0.43) | 0.51 (0.38) | 0.56 (0.32) | - |

5.4 So sánh với Các Hệ thống Hiện có
Trong mục này, chúng tôi đánh giá KGpip so với các phương pháp tiên tiến; FLAML [ 30] và Auto-Sklearn [ 9]. Hình 6 hiển thị biểu đồ radar của tất cả các hệ thống khi được cung cấp ngân sách thời gian 1 giờ. Nó hiển thị hiệu suất của tất cả các hệ thống trên ba tác vụ trong tất cả các benchmark, cụ thể là phân loại nhị phân, phân loại đa lớp, và hồi quy. Đối với mỗi tập dữ liệu, hình hiển thị số liệu hiệu suất thực tế (F1 cho phân loại và 𝑅² cho hồi quy) được thu từ mỗi hệ thống 1. Do đó, đường cong ngoài cùng từ tâm của biểu đồ radar có hiệu suất tốt nhất. Trong Hình 6, cả hai biến thể của KGpip đạt được hiệu suất tốt nhất trên tất cả các tác vụ, vượt trội cả FLAML và Auto-Sklearn. Chúng tôi cũng thực hiện kiểm định t hai phía giữa hiệu suất thu được bởi KGpip so với các hệ thống khác. Kết quả cho thấy KGpip đạt được hiệu suất tốt hơn đáng kể so với cả FLAML và Auto-Sklearn với giá trị t-Test lần lượt là 0.01 và 0.0002 (cả hai đều có 𝑝<0.05).

Bảng 2 cũng hiển thị các giá trị F1 và 𝑅² trung bình cho các tác vụ phân loại và hồi quy, tương ứng. Kết quả cho thấy cả hai biến thể của KGpip đạt được hiệu suất tốt hơn so với cả FLAML và Auto-Sklearn trên tất cả các tác vụ và tập dữ liệu.

Khả năng mở rộng của meta-learning KGpip so với các hệ thống hiện có : Phương pháp meta-learning AL [ 2] khai thác pipeline bằng phân tích mã động, có chi phí cao như đã thảo luận trong Mục 3.1. Do đó, các tác giả của AL đã cung cấp một mô hình meta-learning được đào tạo trước trên 500 pipeline và 9 tập dữ liệu, điều này không mở rộng để bao phủ các trường hợp khác nhau. Ngược lại, chúng tôi đã huấn luyện mô hình meta-learning của chúng tôi sử dụng 2000 pipeline và 142 tập dữ liệu. Không có tập dữ liệu nào trong số này được bao gồm trong 77 tập dữ liệu được sử dụng trong thử nghiệm. AL thất bại trong 22 và hết thời gian trong 38 tập dữ liệu. Điều này cho thấy rằng phương pháp meta-learning KGpip, dựa trên ngữ nghĩa pipeline và học biểu diễn tập dữ liệu, hiệu quả hơn. AL thất bại trên nhiều tập dữ liệu trong quá trình fitting. Như hình hiển thị, KGpip vẫn vượt trội tất cả các phương pháp khác, bao gồm AL, một cách đáng kể. Trên những tập dữ liệu này, AL đạt được điểm F1 thấp nhất trên các tác vụ phân loại nhị phân và đa lớp với các giá trị lần lượt là 0.36 và 0.36. Điều này so với 0.74 và 0.75 của FLAML, 0.73 và 0.68 của Auto-Sklearn, 0.79 và 0.79 của KGpip_FLAML, và 0.79 và 0.74 của KGpip_Auto-Sklearn.

Tập dữ liệu VolcanoML : VolcanoML đã sử dụng nhiều tập dữ liệu không được bao gồm trong 77 tập dữ liệu của chúng tôi ở Hình 6. Một số tập dữ liệu này khá lớn nhằm kiểm tra khả năng mở rộng của hệ thống.
Do đó, chúng tôi cũng đã thu thập tất cả 49 tập dữ liệu mà chúng tôi có thể tìm thấy trong bài báo của họ và kiểm tra phiên bản tốt nhất của KGpip (KGpip_FLAML)

1Điểm số chi tiết cho mỗi hệ thống và tập dữ liệu cũng như tên tương ứng của các tập dữ liệu được hiển thị trong bảng 6-9 trong phụ lục.
6

--- TRANG 7 ---
Hình 6: Biểu đồ radar về hiệu suất của KGpip so với các hệ thống hiện có trên nhiều tác vụ (77 tập dữ liệu) với ngân sách thời gian 1 giờ cho tất cả các hệ thống. Các số bên ngoài chỉ ra ID tập dữ liệu khác nhau và các vạch bên trong hình biểu thị phạm vi hiệu suất của các số liệu tương ứng; ví dụ, 0.2, 0.4, ..., v.v. cho F1 trong phân loại nhị phân. Đối với bất kỳ tập dữ liệu nào, hệ thống có đường cong ngoài cùng có hiệu suất tốt nhất. Ví dụ, KGpip_AutoSklearn và KGpip_FLAML đạt được 100% và 97% F1 trên tập dữ liệu #23 (phân loại đa lớp) so với 65% và 26% cho AutoSklearn và FLAML, tương ứng.

[Biểu đồ radar phức tạp với nhiều hệ thống so sánh]

Hình 7: Sự khác biệt điểm số giữa KGpip_FLAML và VolcanoML trên 44 tập dữ liệu phân loại và hồi quy từ VolcanoML với ngân sách thời gian 1 giờ. Để ngắn gọn, chúng tôi đã loại bỏ khỏi Hình này 22 tập dữ liệu mà cả hai hệ thống hoạt động tương đương (trong phạm vi khác biệt ≤0.01).

Bảng 3: Điểm số trung bình (trung bình và độ lệch chuẩn) của KGpip_FLAML so với VolcanoML trên 44 tập dữ liệu từ VolcanoML. Nhìn chung, KGpip_FLAML đạt được tốt hơn đáng kể so với VolcanoML, theo một kiểm định ý nghĩa thống kê của 𝑝<0.05.

| | Nhị phân | Đa lớp | Hồi quy | T-Test |
|---|---|---|---|---|
| KGpip_FLAML | 0.82 (0.14) | 0.86 (0.16) | 0.83 (0.13) | - |
| VolcanoML | 0.69 (0.23) | 0.70 (0.31) | 0.68 (0.25) | 0.0001 |

so với VolcanoML trên những tập dữ liệu này với ngân sách thời gian 1 giờ.
Hiệu suất của KGpip_FLAML và VolcanoML được hiển thị trong Hình 7. Để ngắn gọn, chúng tôi đã bỏ qua khỏi hình tất cả các tập dữ liệu mà sự khác biệt hiệu suất giữa cả hai hệ thống ≤0.01 và các tập dữ liệu trùng lặp với những tập được hiển thị trong Hình 6. Trên những tập dữ liệu đó, KGpip_FLAML tìm thấy một pipeline hợp lệ cho tất cả chúng, đôi khi với sự khác biệt tuyệt đối đáng kể trong điểm F1 hoặc 𝑅² ≥0.90. Trên tất cả 44 tập dữ liệu, KGpip_FLAML đạt được trung bình điểm số tốt hơn đáng kể so với VolcanoML (kiểm định ý nghĩa thống kê của 𝑝< 0.05), xem Bảng 3 để biết chi tiết.

Bảng 4: Các khía cạnh khác nhau so sánh một mô hình được huấn luyện trên một tập các đồ thị mã so với một mô hình được huấn luyện trên một tập các đồ thị MetaPip. Mô hình dựa trên đồ thị mã gốc thất bại trong các tập dữ liệu tầm thường để sinh ra các pipeline hợp lệ và giới hạn khả năng mở rộng của KGpip đến một tập lớn hơn các script pipeline ML và việc học của KGpip bằng cách sử dụng ít epoch hơn.

| Tập dữ liệu/Khía cạnh | Đồ thị Mã | Đồ thị MetaPip |
|---|---|---|
| kr-vs-kp | 0 (0) | 1.00 (0) |
| nomao | 0 (0) | 0.96 (0) |
| cnae-9 | 0 (0) | 0.95 (0.01) |
| mfeat-factors | 0 (0) | 0.98 (0) |
| segment | 0 (0) | 0.98 (0) |
| Trung bình F1 | 0 (0) | 0.97 (0.02) |
| Số Nút | 29,139 | 974 |
| Số Cạnh | 252,486 | 1,052 |
| Thời gian Huấn luyện | 175 (phút) | 2 (phút) |

5.5 Nghiên cứu Ablation
5.5.1 Hiệu quả của MetaPip. Phương pháp MetaPip của chúng tôi quản lý để giảm đáng kể số lượng nút và cạnh trong đồ thị mã. Sử dụng đồ thị gốc thu được từ phân tích tĩnh, nó tạo ra đồ thị MetaPip tập trung vào các khía cạnh cốt lõi cần thiết để huấn luyện một mô hình sinh đồ thị cho pipeline ML, như phép biến đổi dữ liệu, lựa chọn bộ học, và lựa chọn siêu tham số.
Thực nghiệm này điều tra khả năng mở rộng của mô hình sinh đồ thị của chúng tôi dựa trên hai tập huấn luyện khác nhau, tức là tập các đồ thị MetaPip được mô tả trong mục 3.2 so với tập gốc các đồ thị mã từ phân tích tĩnh cho cùng các script pipeline ML.

Đối với thực nghiệm này, chúng tôi sử dụng một tập huấn luyện quy mô nhỏ gồm 82 đồ thị pipeline liên quan đến một tập dữ liệu phân loại. Các đồ thị mã gốc cho 82 pipeline này bao gồm 29,139 nút và 252,486 cạnh. Tuy nhiên, đồ thị MetaPip của chúng tôi bao gồm 974 nút và 1052 cạnh. Đây là tỷ lệ giảm đồ thị ít nhất 96.6%,
7

--- TRANG 8 ---
[BIỂU ĐỒ: Hiển thị số lượng pipeline và các loại learner/transformer được KGpip chọn]

Hình 8: Các learner và transformer hàng đầu được KGpip chọn (A) có phạm vi bao phủ và đa dạng rộng (B).

Bảng 5: Hiệu suất của KGpip_FLAML (trung bình và độ lệch chuẩn) khi chúng tôi thay đổi số lượng đồ thị pipeline được dự đoán trong giới hạn thời gian 30 phút. Chúng tôi thu được kết quả tương tự cho KGpip_AutoSklearn, và do đó bỏ qua kết quả của nó.

| | Nhị phân | Đa lớp | Hồi quy |
|---|---|---|---|
| Top-3 graphs | 0.80 (0.14) | 0.70 (0.31) | 0.71 (0.23) |
| Top-5 graphs | 0.81 (0.14) | 0.73 (0.26) | 0.70 (0.23) |
| Top-7 graphs | 0.81 (0.14) | 0.75 (0.24) | 0.71 (0.24) |

Hình 4 hiển thị những thống kê chi tiết này. Cuộc điều tra chính ở đây là liệu tỷ lệ giảm khổng lồ này có giúp cải thiện độ chính xác và khả năng mở rộng của mô hình sinh đồ thị của chúng tôi hay không. Chúng tôi huấn luyện một mô hình trên đồ thị mã gốc và một mô hình khác trên các đồ thị MetaPip. Cả hai mô hình đều được huấn luyện trong 15 epoch với cùng tập siêu tham số. Điều đáng chú ý là do thời gian rất lớn cần thiết để xử lý các nút và cạnh trong đồ thị mã, chúng tôi phải giảm số epoch từ 400 xuống 15.

Chúng tôi kiểm tra hiệu suất của KGpip khi được huấn luyện trên cả hai đồ thị trên các tập dữ liệu phân loại nhị phân và đa lớp tầm thường nhất trong benchmark AutoML. Đây là những tập dữ liệu mà điểm F1 của tất cả các hệ thống được báo cáo trong mục 5.4 trên 0.9. Kết quả là tổng cộng 5 tập dữ liệu (1 nhị phân và 4 đa lớp). Cả hai mô hình đều sử dụng Auto-Sklearn làm bộ tối ưu hóa siêu tham số với ngân sách thời gian 15 phút và 3 đồ thị. Chúng tôi lấy trung bình của ba lần chạy.
Kết quả được tóm tắt trong Bảng 4. Đối với những tập dữ liệu tầm thường này, mô hình được huấn luyện bằng đồ thị mã không thể sinh ra bất kỳ pipeline ML hợp lệ nào. Điều này có nghĩa là mô hình thất bại trong việc nắm bắt các khía cạnh cốt lõi của pipeline ML, tức là phép biến đổi hoặc bộ học hợp lệ.
Hơn nữa, phương pháp MetaPip của chúng tôi giúp KGpip giảm thời gian huấn luyện 99%, như được hiển thị trong Bảng 4.

5.5.2 Chất lượng meta-learning của KGpip. Thực nghiệm này kiểm tra chất lượng của thành phần meta-learning của chúng tôi. Chúng tôi kiểm tra hiệu suất khi thay đổi số lượng đồ thị được chọn từ giai đoạn sinh đồ thị trước khi đưa vào module tối ưu hóa siêu tham số. Bảng 5 hiển thị hiệu suất KGpip_FLAML khi chúng tôi thay đổi số lượng đồ thị được dự đoán giữa 3, 5 và 7.

Chỉ với 3 đồ thị, KGpip vẫn vượt trội FLAML (hệ thống tốt thứ hai sau KGpip_FLAML) , mặc dù hiệu ứng yếu hơn (giá trị t-Test = 0.06). So với Auto-sklearn (hệ thống tốt thứ ba sau KGpip_FLAML), tất cả các biến thể đều có hiệu suất tương tự hoặc tốt hơn, nhưng sự khác biệt không đáng kể. Thực nghiệm này cho thấy rằng ngay cả với ba đồ thị, KGpip vượt trội FLAML và KGpip_Auto-Sklearn, tức là các pipeline đúng thường xuất hiện trong top 3. Như một đánh giá khác về chất lượng dự đoán của chúng tôi, chúng tôi đo pipeline tốt nhất xuất hiện ở đâu trong danh sách xếp hạng các pipeline được dự đoán của chúng tôi. Lý tưởng nhất, pipeline hàng đầu sẽ luôn là đầu tiên, và chúng tôi sử dụng Mean Reciprocal Rank (MRR) để đo mức độ gần của dự đoán của chúng tôi với điều đó. Trên tất cả các lần chạy, MRR là 0.71, cho thấy pipeline hàng đầu thường rất gần đỉnh.

5.5.3 Tính đa dạng meta-learning của KGpip. Một câu hỏi chúng tôi đã giải quyết là liệu KGpip có tạo ra các pipeline khác nhau cho cùng một tập dữ liệu qua các lần chạy khác nhau hay không. Điều này cho chúng tôi cảm nhận về việc KGpip có tính tất định hay không, hoặc liệu nó có tạo ra các pipeline khác nhau để giúp cắt tỉa không gian tìm kiếm AutoML. Chúng tôi lấy các lần chạy khác nhau cho cùng một tập dữ liệu, và tạo ra một danh sách các bộ học và transformer được tạo ra cho mỗi tập dữ liệu qua các lần chạy. Danh sách được giới hạn bởi số lượng bộ học và transformer ngắn nhất được tạo ra qua các lần chạy. Sau đó chúng tôi tính toán tương quan cho các tập dữ liệu qua các lần chạy 1, 2, và 3. Các tương quan dao động từ 0.60 - 0.64, cho thấy rằng các lần chạy không tạo ra cùng các transformer và bộ học qua các lần chạy. Chúng tôi cũng kiểm tra các loại bộ học được KGpip chọn để xem xét. Hình 8a hiển thị các bộ học và transformer được tìm thấy ít nhất 20 lần trong các pipeline huấn luyện. Người ta có thể thấy từ hình rằng KGpip không mù quáng xuất ra các bộ học và transformer theo số lượng. Hình 8b hiển thị tính đa dạng hơn trong những gì đã được chọn tổng thể. Vì vậy, nhiều phương pháp được bao phủ bởi KGpip.

6 KẾT LUẬN
Bài báo này đề xuất một công thức mới cho vấn đề AutoML như một vấn đề sinh đồ thị, nơi chúng ta có thể đặt việc lựa chọn bộ học và tiền xử lý như việc sinh các đồ thị khác nhau đại diện cho các pipeline ML. Do đó, chúng tôi đã phát triển hệ thống KGpip dựa trên việc khai thác các kho lưu trữ lớn các script, và tận dụng các kỹ thuật gần đây cho phân tích mã tĩnh. KGpip sử dụng các embedding được tạo ra dựa trên nội dung tập dữ liệu để dự đoán và tối ưu hóa một tập các pipeline ML dựa trên các tập dữ liệu đã thấy tương tự nhất. KGpip được thiết kế để hoạt động với các hệ thống AutoML, như Auto-Sklearn và FLAML, để sử dụng các bộ tối ưu hóa siêu tham số của chúng. Chúng tôi đã tiến hành đánh giá toàn diện nhất của 121 tập dữ liệu, bao gồm các tập dữ liệu được sử dụng bởi FLAML, VolcanoML, và AL. Đánh giá toàn diện của chúng tôi cho thấy KGpip cải thiện đáng kể hiệu suất của FLAML và Auto-Sklearn trong các tác vụ phân loại và hồi quy.
Hơn nữa, KGpip vượt trội AL, dựa trên một quá trình meta-learning tốn kém hơn, trong 97% các tập dữ liệu. Hiệu suất xuất sắc này cho thấy rằng phương pháp meta-learning KGpip hiệu quả và hiệu suất hơn. Cuối cùng, KGpip vượt trội VolcanoML trong 62% các tập dữ liệu và hòa với nó trong 22%.

TÀI LIỆU THAM KHẢO
[1]Ibrahim Abdelaziz, Julian Dolby, James P. McCusker, và Kavitha Srinivas. 2020.
Một Toolkit để Tạo ra Đồ thị Tri thức Mã. ArXiv (2020). https://arxiv.
org/abs/2002.09440
[2]José P. Cambronero và Martin C. Rinard. 2019. AL: Tự động tạo ra Chương trình Học có Giám sát. In Proceedings of the ACM on Programming Languages , Vol. 3.
https://doi.org/10.1145/3360601
[3]Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St.
John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-
Hsuan Sung, Brian Strope, và Ray Kurzweil. 2018. Universal Sentence Encoder.
CoRR abs/1803.11175 (2018). http://arxiv.org/abs/1803.11175
[4]Alex G. C. de Sá, Walter José G. S. Pinto, Luiz Otavio V. B. Oliveira, và Gisele L.
Pappa. 2017. RECIPE: Một Framework Dựa trên Ngữ pháp để Tự động Phát triển Các Pipeline Phân loại. In Genetic Programming , James McDermott, Mauro
Castelli, Lukas Sekanina, Evert Haasdijk, và Pablo García-Sánchez (Eds.). 246–
261. https://doi.org/10.1007/978-3-319-55696-3_16
[5]Iddo Drori, Lu Liu, Yi Nian, Sharath C Koorathota, Jung-Shian Li, Antonio Khalil
Moretti, Juliana Freire, và Madeleine Udell. 2019. AutoML sử dụng Metadata
Language Embeddings. ArXiv (2019). https://arxiv.org/abs/1910.03698
[6]Robert Engels và Christiane Theusinger. 1998. Sử dụng một Data Metric để Tư vấn Tiền xử lý cho Ứng dụng Khai thác Dữ liệu. In In Proceedings of the European
Conference on Artificial Intelligence (ECAI) . 430–434. http://citeseerx.ist.psu.edu/
viewdoc/download?doi=10.1.1.56.7414&rep=rep1&type=pdf
[7]Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy,
Mu Li, và Alexander Smola. 2020. AutoGluon-Tabular: AutoML Mạnh mẽ và Chính xác cho Dữ liệu Có cấu trúc. ArXiv (2020). https://arxiv.org/abs/2003.06505
[8]Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, và
Frank Hutter. 2020. Auto-Sklearn 2.0: The Next Generation. arXiv (2020). https:
//arxiv.org/abs/2007.04074
[9]Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel
Blum, và Frank Hutter. 2015. Học Máy Tự động Hiệu quả và Mạnh mẽ.
InProceedings of the International Conference on Neural Information Processing
Systems (NeurIPS) . 2962–2970. https://dl.acm.org/doi/10.5555/2969442.2969547
[10] Nicolo Fusi, Rishit Sheth, và Melih Elibol. 2018. Phân rã Ma trận Xác suất cho Học Máy Tự động. In Proceedings of the Interna-
tional Conference on Neural Information Processing Systems (NeurIPS) . 3352–3361.
https://dl.acm.org/doi/10.5555/3327144.3327254
[11] P. Gijsbers, E. LeDell, S. Poirier, J. Thomas, B. Bischl, và J. Vanschoren. 2019.
Một Benchmark AutoML Mã nguồn Mở. In AutoML Workshop at the International
Conference on Machine Learning (ICML) . https://arxiv.org/abs/1907.00909
[12] Isabelle Guyon, Imad Chaabane, Hugo Jair Escalante, Sergio Escalera, Damir
Jajetic, James Robert Lloyd, Núria Macià, Bisakha Ray, Lukasz Romaszko, Michèle
Sebag, Alexander Statnikov, Sébastien Treguer, và Evelyne Viegas. 2016. Một Đánh giá Ngắn gọn về Thử thách ChaLearn AutoML: Học Bất cứ lúc nào Bất cứ tập dữ liệu nào mà không có Sự can thiệp của Con người. In Proceedings of Machine Learning Research , Vol. 64.
21–30. https://proceedings.mlr.press/v64/guyon_review_2016.html
[13] Jeff Johnson, Matthijs Douze, và Hervé Jégou. 2021. Tìm kiếm Độ tương tự Quy mô Tỷ với GPUs. IEEE Transactions on Big Data 7, 3 (2021), 535–547. https:
//doi.org/10.1109/TBDATA.2019.2921572
[14] Michael Katz, Parikshit Ram, Shirin Sohrabi, và Octavian Udrea. 2020. Khám phá Ngôn ngữ Không có Ngữ cảnh thông qua Lập kế hoạch: Trường hợp Tự động hóa Học Máy. In Proceedings of the International Conference on Automated Planning
and Scheduling (ICAPS) . 403–411. https://ojs.aaai.org//index.php/ICAPS/article/
view/6686
[15] Trang T Le, Weixuan Fu, và Jason H Moore. 2020. Mở rộng học máy tự động dựa trên cây đến dữ liệu lớn sinh y học với bộ chọn tập đặc trưng. Bioinformatics
36, 1 (2020), 250–256. https://doi.org/10.1093/bioinformatics/btz470
[16] Erin LeDell và Sebastien Poirier. 2020. H2O AutoML: Học Máy Tự động Có thể Mở rộng. In AutoML Workshop at the International Conference on Machine
Learning (ICML) . www.automl.org/wp-content/uploads/2020/07/AutoML_2020_
paper_61.pdf
[17] Yang Li, Yu Shen, Wentao Zhang, Jiawei Jiang, Yaliang Li, Bolin Ding, Jingren
Zhou, Zhi Yang, Wentao Wu, Ce Zhang, và Bin Cui. 2021. VolcanoML: Tăng tốc AutoML End-to-End thông qua Phân rã Không gian Tìm kiếm Có thể Mở rộng. Proceedings
of the VLDB Endowment 14, 11 (2021), 2167–2176. http://www.vldb.org/pvldb/
vol14/p2167-li.pdf
[18] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, và Peter Battaglia. 2018.
Học Các Mô hình Sinh Sâu của Đồ thị. ArXiv (2018). https://arxiv.org/
abs/1803.03324
[19] Sijia Liu, Parikshit Ram, Deepak Vijaykeerthy, Djallel Bouneffouf, Gregory
Bramble, Horst Samulowitz, Dakuo Wang, Andrew Conn, và Alexander Gray.
2020. Một Framework Dựa trên ADMM cho Cấu hình Pipeline AutoML. In
Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 4892–4899.
https://doi.org/10.1609/aaai.v34i04.5926
[20] Radu Marinescu, Akihiro Kishimoto, Parikshit Ram, Ambrish Rawat, Martin
Wistuba, Paulito P. Palmes, và Adi Botea. 2021. Tìm kiếm Pipeline Học Máy Sử dụng Ngữ pháp Không có Ngữ cảnh. In Proceedings of the AAAI Conference
on Artificial Intelligence . 8902–8911. https://ojs.aaai.org/index.php/AAAI/article/
view/17077
[21] Jonas Mueller và Alex Smola. 2019. Nhận diện Biến từ Dữ liệu của chúng thông qua Deep Embeddings của Phân phối. International Conference on Data Mining
(ICDM) (2019), 1264–1269. https://doi.org/10.1109/ICDM.2019.00158
[22] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz,
và Jason H. Moore. 2017. PMLB: một bộ benchmark lớn cho đánh giá và so sánh học máy. BioData Mining 10, 1 (2017), 36. https://doi.org/10.
1186/s13040-017-0154-
[23] Bernhard Pfahringer, Hilan Bensusan, và Christophe Giraud-Carrier. 2000. Meta-
Learning bằng Landmarking Các Thuật toán Học Khác nhau. In Proceedings of the
International Conference on Machine Learning (ICML) . 743–750. https://dl.acm.
org/doi/10.5555/645529.658105
[24] Herilalaina Rakotoarison, Marc Schoenauer, và Michèle Sebag. 2019. Học Máy Tự động với Tìm kiếm Cây Monte-Carlo. In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI) . 3296–3303.
https://doi.org/10.24963/ijcai.2019/457
[25] Matthias Reif, Faisal Shafait, và Andreas Dengel. 2012. Meta-learning cho tối ưu hóa tham số tiến hóa của bộ phân loại. Machine Learning 87, 3 (2012),
357–380. https://doi.org/10.1007/s10994-012-5286-7
[26] Kevin Swersky, Jasper Snoek, và Ryan P. Adams. 2013. Tối ưu hóa Bayesian Đa-Tác vụ. In Proceedings of the Interna-
tional Conference on Neural Information Processing Systems (NeurIPS) . 2004–2012.
https://dl.acm.org/doi/10.5555/2999792.2999836
[27] Chris Thornton, Frank Hutter, Holger H Hoos, và Kevin Leyton-Brown. 2013.
Auto-WEKA: Lựa chọn kết hợp và tối ưu hóa siêu tham số của các thuật toán phân loại. In Proceedings of the International Conference on Knowledge
Discovery and Data Mining (SIGKDD) . 847–855. https://doi.org/10.1145/2487575.
2487629
[28] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, và Luis Torgo. 2014. OpenML:
Khoa học Mạng trong Học Máy. SIGKDD Explorations 15 (2014), 49–60.
https://doi.org/10.1145/2641190.2641198
[29] Ricardo Vilalta, Christophe Giraud-carrier, Pavel Brazdil, và Carlos Soares. 2004.
Sử dụng Meta-Learning để Hỗ trợ Khai thác Dữ liệu. International Journal of Computer
Science & Applications 1 (2004). https://citeseerx.ist.psu.edu/viewdoc/download?
doi=10.1.1.105.1351&rep=rep1&type=pdf
[30] Chi Wang, Qingyun Wu, Markus Weimer, và Erkang Zhu. 2021. FLAML: Một Thư viện AutoML Nhanh và Nhẹ. In Proceedings of Machine Learning and
Systems (MLSys) , Vol. 3. 434–447. https://proceedings.mlsys.org/paper/2021/file/
92cc227532d17e56e07902b254dfad10-Paper.pdf
[31] Marcel Wever, Felix Mohr, và Eyke Hüllermeier. 2018. ML-Plan cho Pipeline Học Máy Độ dài Không giới hạn. In AutoML Workshop at the International
Conference on Machine Learning (ICML) . https://ris.uni-paderborn.de/download/
3852/3853/38.pdf
[32] Anatoly Yakovlev, Hesam Fathi Moghadam, Ali Moharrer, Jingxiao Cai, Nikan
Chavoshi, Venkatanathan Varadarajan, Sandeep R. Agrawal, Sam Idicula, Tomas
Karnagel, Sanjay Jinturkar, và Nipun Agarwal. 2020. Oracle AutoML: Một Pipeline AutoML Nhanh và Dự đoán được. Proceedings of the VLDB Endowment 13, 12 (2020),
3166–3180. https://doi.org/10.14778/3415478.3415542
[33] Chengrun Yang, Yuji Akimoto, Dae Won Kim, và Madeleine Udell. 2019. OBOE:
Lọc Cộng tác cho Lựa chọn Mô hình AutoML. In Proceedings of the Interna-
tional Conference on Knowledge Discovery and Data Mining (SIGKDD) .
http://doi.org/10.1145/3292500.3330909
9

--- TRANG 10 ---
A THỐNG KÊ CỦA TẤT CẢ CÁC TẬP DỮ LIỆU
Bảng 6 và 7 hiển thị thống kê của tất cả các tập dữ liệu benchmark bao gồm số hàng và cột, số đặc trưng số, phân loại, và văn bản, số lớp, kích thước, nguồn, và các bài báo đã đánh giá trên những tập dữ liệu này.

B ĐIỂM SỐ CHI TIẾT CHO TẤT CẢ CÁC HỆ THỐNG
Bảng 8 và 9 hiển thị điểm số macro F1 và 𝑅² chi tiết cho tất cả các hệ thống trên tất cả các tập dữ liệu benchmark.
10

--- TRANG 11 ---
Bảng 6: Thống kê của 77 tập dữ liệu benchmark được sử dụng trong FLAML và AL. Từ trái sang phải: tên tập dữ liệu, số hàng, số cột, số cột số, số cột phân loại, số cột văn bản, số lớp (cho các tập dữ liệu phân loại), kích thước tính bằng MB, nguồn của tập dữ liệu, và các bài báo đã đánh giá trên tập dữ liệu.

[Bảng chi tiết với 77 hàng dữ liệu về các tập dữ liệu, bao gồm tên, số hàng, cột, loại đặc trưng, kích thước, nguồn và bài báo tham khảo]

--- TRANG 12 ---
Bảng 7: Thống kê của 44 tập dữ liệu được sử dụng bởi VolcanoML. Từ trái sang phải: tên tập dữ liệu, số hàng, số cột, số cột số, số cột phân loại, số cột văn bản, số lớp (cho các tập dữ liệu phân loại), kích thước tính bằng MB, nguồn của tập dữ liệu, và các bài báo đã đánh giá trên tập dữ liệu.

[Bảng chi tiết với 44 hàng dữ liệu về các tập dữ liệu VolcanoML]

--- TRANG 13 ---
Bảng 8: Điểm số Macro F1 và 𝑅² cho tất cả các hệ thống trên tất cả 77 tập dữ liệu benchmark. Điểm số được báo cáo là trung bình của 3 lần chạy với ngân sách thời gian 1 giờ.

[Bảng chi tiết hiển thị điểm số hiệu suất của các hệ thống khác nhau trên từng tập dữ liệu]

--- TRANG 14 ---
Bảng 9: Điểm số Macro F1 và 𝑅² cho KGpip_FLAML và VolcanoML trên 44 tập dữ liệu từ VolcanoML. Điểm số được báo cáo là trung bình của 3 lần chạy với ngân sách thời gian 1 giờ.

[Bảng so sánh điểm số giữa KGpip_FLAML và VolcanoML trên 44 tập dữ liệu]

14