# 2307.08364.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2307.08364.pdf
# Kích thước tệp: 846738 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Q(D)O-ES: Tối ưu hóa Chất lượng (Đa dạng) dựa trên Quần thể
cho Lựa chọn Ensemble Hậu hoc trong AutoML

Lennart Purucker¹Lennart Schneider²,³Marie Anastacio⁵
Joeran Beel¹Bernd Bischl²,³Holger Hoos⁴,⁵,⁶
¹Đại học Siegen, ²LMU Munich, ³Trung tâm Học máy Munich (MCML),
⁴Đại học Leiden, ⁵RWTH Aachen, ⁶Đại học British Columbia

Tóm tắt: Các hệ thống học máy tự động (AutoML) thường sử dụng ensemble các mô hình hậu hoc để
cải thiện hiệu suất dự đoán, điển hình thông qua lựa chọn ensemble tham lam (GES). Tuy nhiên,
chúng tôi tin rằng GES có thể không phải lúc nào cũng tối ưu, vì nó thực hiện một tìm kiếm tham lam
xác định đơn giản. Trong nghiên cứu này, chúng tôi giới thiệu hai phương pháp lựa chọn ensemble
dựa trên quần thể mới, QO-ES và QDO-ES, và so sánh chúng với GES. Trong khi QO-ES chỉ tối ưu hóa
hiệu suất dự đoán, QDO-ES còn xem xét tính đa dạng của các ensemble trong quần thể,
duy trì một tập hợp đa dạng các ensemble có hiệu suất tốt trong quá trình tối ưu hóa dựa trên ý tưởng
của tối ưu hóa chất lượng đa dạng. Các phương pháp được đánh giá sử dụng 71 bộ dữ liệu phân loại
từ điểm chuẩn AutoML, chứng minh rằng QO-ES và QDO-ES thường vượt trội hơn GES,
mặc dù chỉ có ý nghĩa thống kê trên dữ liệu xác thực. Kết quả của chúng tôi cũng gợi ý rằng
tính đa dạng có thể có lợi cho ensemble hậu hoc nhưng cũng làm tăng nguy cơ overfitting.

1 Giới thiệu

Nhiều hệ thống học máy tự động (AutoML) không trả về một mô hình đơn lẻ mà là một
ensemble các mô hình. Theo phân loại ensemble từ Cruz et al. (2018), các hệ thống AutoML
thực hiện ba bước ensemble: Tạo ra, hệ thống tạo ra các mô hình cơ sở trong khi tìm kiếm
một cấu hình tối ưu (ví dụ: Auto-Sklearn (Feurer et al., 2015, 2022)) hoặc xác thực chéo
các cấu hình được xác định trước (ví dụ: AutoGluon (Erickson et al., 2020)); Lựa chọn, chúng chọn một tập con
của các mô hình cơ sở được tạo ra ở bước đầu tiên, ví dụ: 50 mô hình hàng đầu; Tổng hợp, chúng sử dụng
ensemble hậu hoc để tổng hợp dự đoán của tất cả các mô hình cơ sở đã chọn.

Xem xét mười hệ thống AutoML mã nguồn mở nổi bật, 60% dựa vào ensemble hậu hoc:
Auto-Sklearn 1 (Feurer et al., 2015), Auto-Sklearn 2 (Feurer et al., 2022), AutoGluon (Erickson
et al., 2020), Auto-PyTorch (Mendoza et al., 2018; Zimmer et al., 2021), H2O AutoML (LeDell and
Poirier, 2020), và MLJAR (Płońska and Płoński, 2021). Trong số đó, chỉ H2O AutoML sử dụng
stacking (Wolpert, 1992) với một mô hình tuyến tính, trong khi tất cả các hệ thống khác dựa vào lựa chọn ensemble tham lam
với thay thế (GES) (Caruana et al., 2004, 2006). Bốn hệ thống không sử dụng ensemble hậu hoc
theo mặc định – TPOT (Olson et al., 2016), GAMA (Gijsbers and Vanschoren, 2019), FLAML
(Wang et al., 2021), và LightAutoML (Vakhrushev et al., 2021) – trả về mô hình tốt nhất đơn lẻ;
tuy nhiên, tất cả trừ TPOT đều cung cấp ensemble hậu hoc như một tùy chọn.

Mười hệ thống này chưa bao giờ nghiên cứu tiềm năng của ensemble hậu hoc hay đánh giá các
phương pháp khác nhau trong các ấn phẩm của họ. Phương pháp được sử dụng thường xuyên nhất, GES, tìm kiếm theo cách tham lam
một vector trọng số tối ưu để tổng hợp tuyến tính các dự đoán của các mô hình cơ sở. Chúng tôi tin rằng, vì
GES sử dụng một tìm kiếm tham lam xác định đơn giản, có tiềm năng cải thiện GES.

Để làm điều đó, chúng tôi tập trung vào các phương pháp tối ưu hóa dựa trên quần thể vì chúng đã cho thấy
thành công trong tối ưu hóa (Kennedy and Eberhart, 1995; Das and Suganthan, 2010; Hansen and Auger,
2014; Kochenderfer and Wheeler, 2019). Đặc biệt, các phương pháp như vậy cho phép chúng tôi thêm tính ngẫu nhiên
trong khi xây dựng ensemble, ví dụ: thông qua lai ghép hoặc lấy mẫu ngẫu nhiên. Ngược lại, GES thực hiện

AutoML 2023 ©2023 các tác giả, phát hành dưới CC BY 4.0arXiv:2307.08364v2 [cs.LG] 2 Aug 2023

--- TRANG 2 ---
cải tiến tham lam ở mỗi bước tìm kiếm và do đó không thể tận dụng các tương tác giữa
các mô hình cơ sở mà không cải thiện hiệu suất ngay lập tức; điều này có thể dẫn đến tối ưu cục bộ.
Điều này đã thúc đẩy chúng tôi khám phá lựa chọn ensemble dựa trên quần thể với thay thế cho AutoML.

Nó được chấp nhận rộng rãi trong tài liệu rằng tính đa dạng của một ensemble, từ nay gọi là
tính đa dạng ensemble, có thể cải thiện hiệu suất của nó (Dietterich, 2000b; Kuncheva and Whitaker, 2003;
Sagi and Rokach, 2018). Công trình trước đây về lựa chọn ensemble không thay thế, nơi người ta tìm kiếm
một tập con tối ưu của các mô hình cơ sở thay vì một vector trọng số, đã kết hợp tính đa dạng ensemble trong
tìm kiếm (Partridge and Yates, 1996; Banfield et al., 2005; Partalas et al., 2010; Li et al., 2012). Điều này
đã thúc đẩy chúng tôi cũng bao gồm tính đa dạng ensemble trong phương pháp tối ưu hóa của chúng tôi.

Trong bối cảnh học máy, tính đa dạng ensemble có thể được hiểu như một thước đo về
mức độ mà các mô hình cơ sở tạo ra các lỗi khác nhau (Hansen and Salamon, 1990; Dietterich, 2000a;
Banfield et al., 2005; Kumar and Kumar, 2012; Wood et al., 2023). Tuy nhiên, đối với một hàm mục tiêu hoặc
nhiệm vụ nhất định, thường có vẻ như có một sự đánh đổi giữa tính đa dạng ensemble và hiệu suất
(Tang et al., 2006; Ahmed et al., 2017; Wood et al., 2023). Điều này gợi ý rằng các cách tiếp cận như
của Li et al. (2012), tối ưu hóa đồng thời tính đa dạng ensemble và hiệu suất, có thể không hiệu quả.
Vì lý do này, trong nghiên cứu của chúng tôi, chúng tôi tập trung vào tối ưu hóa chất lượng đa dạng (QDO).

QDO (Cully et al., 2015; Mouret and Clune, 2015; Chatzilygeroudis et al., 2021) là một xu hướng gần đây
trong tối ưu hóa dựa trên quần thể. Để tránh nhầm lẫn, chúng tôi lưu ý rằng thuật ngữ đa dạng trong
QDO không đề cập đến tính đa dạng ensemble, mà là đa dạng hành vi, tức là sự biến thiên trong
hành vi giữa các thành viên của một quần thể nhất định. QDO tối đa hóa một hàm mục tiêu đơn
trong khi duy trì một quần thể đa dạng về mặt hành vi. Trong robot học, ví dụ, chúng ta có thể
quan tâm đến việc xây dựng robot hai chân với hành vi khác nhau, ví dụ: cao và nặng hoặc nhỏ và
nhẹ, tất cả đều có thể đi nhanh theo đường thẳng (Mouret and Clune, 2015).

Với QDO cho lựa chọn ensemble, chúng tôi đề xuất tối đa hóa hiệu suất ensemble trong khi
duy trì một quần thể ensemble đa dạng về mặt hành vi w.r.t. tính đa dạng ensemble.
Nói cách khác, chúng tôi duy trì một quần thể các ensemble có hiệu suất hàng đầu với lượng
tính đa dạng ensemble khác nhau. Ngược lại, một tìm kiếm dựa trên quần thể truyền thống chỉ giữ lại các ensemble có hiệu suất tốt nhất trong quần thể – bất kể tính đa dạng ensemble của chúng. Vì một quần thể
đa dạng về mặt hành vi có thể có lợi trong quá trình tối ưu hóa (Mouret and Clune, 2015; Nguyen et al.,
2015; Lehman and Stanley, 2011; Chatzilygeroudis et al., 2021), và tính đa dạng ensemble có thể
có lợi cho hiệu suất (Sagi and Rokach, 2018), chúng tôi thấy tiềm năng đáng kể trong việc cũng xem xét
QDO trong khám phá lựa chọn ensemble dựa trên quần thể của chúng tôi.

Đóng góp của chúng tôi là so sánh hiệu suất thực nghiệm của GES với tìm kiếm dựa trên quần thể
có và không có đa dạng (QDO-ES và QO-ES, trong đó QO là viết tắt của tối ưu hóa chất lượng)
cho lựa chọn ensemble hậu hoc trong AutoML. Chúng tôi cho thấy rằng các phương pháp dựa trên quần thể thường xếp hạng
tốt hơn GES trên 71 bộ dữ liệu phân loại từ điểm chuẩn AutoML (Gijsbers et al., 2022).
Hơn nữa, chúng tôi phát hiện rằng QDO-ES và QO-ES vượt trội có ý nghĩa thống kê so với GES trên dữ liệu xác thực,
mặc dù ý nghĩa này không tổng quát hóa đến dữ liệu kiểm tra.

Mã và dữ liệu của chúng tôi có sẵn công khai: xem Phụ lục H.

2 Nền tảng

Mục tiêu của ensemble hậu hoc cho AutoML là tổng hợp một pool 𝑃={𝑝₁,...,𝑝ₘ} của 𝑚 mô hình cơ sở
bao gồm tất cả các mô hình được huấn luyện và xác thực trong quá trình lựa chọn mô hình hoặc một tập con của chúng.
Ví dụ, Auto-Sklearn đặt 𝑃 thành 50 mô hình tốt nhất theo điểm xác thực. Ensemble được huấn luyện trên các dự đoán của mô hình được tạo ra trong quá trình xác thực, tức là các dự đoán trên
dữ liệu xác thực tách biệt hoặc các dự đoán ngoài fold của xác thực chéo 𝑘-fold. Do đó, các phương pháp ensemble hậu hoc tối ưu hóa một hàm mục tiêu do người dùng định nghĩa. Trong phần còn lại của bài báo này, chúng tôi tập trung
vào phân loại, nhưng khái niệm được giải thích và phương pháp của chúng tôi có thể được mở rộng cho hồi quy.

Chính thức, lựa chọn ensemble với thay thế tối thiểu hóa một hàm mất mát ensemble 𝐿(𝐸) trên
dữ liệu xác thực, trong đó 𝐸 là một multiset của các mô hình cơ sở, tức là một tập hợp cho phép lặp lại, có thể

2

--- TRANG 3 ---
được viết là 𝐸=(𝑃,𝑟), với 𝑟:𝑃→Z₀⁺ và 𝑟(𝑖) biểu thị số lần mô hình cơ sở 𝑝ᵢ được
lặp lại trong 𝐸. Cho một ensemble 𝐸, chúng ta có thể tính vector trọng số của nó

𝑤_𝐸 = [
𝑟(𝑖)/∑ⱼ₌₁ᵐ𝑟(𝑗) | 𝑖∈[1...𝑚]
] (1)

Dự đoán của 𝐸 là trung bình số học có trọng số 𝑤_𝐸 của xác suất dự đoán của tất cả các mô hình cơ sở. Lớp có xác suất dự đoán cao nhất được dự đoán.

Lựa chọn ensemble tham lam với thay thế (GES) (Caruana et al., 2004, 2006) lặp đi lặp lại xây dựng
𝐸 bằng một quá trình tìm kiếm tham lam xác định thêm một mô hình cơ sở ở mỗi lần lặp.
GES là một phương pháp lựa chọn ensemble, còn được gọi là cắt tỉa ensemble (Tsoumakas et al., 2009). Do đó,
nó tạo ra một 𝑤_𝐸 thưa thớt theo thiết kế. Các mô hình cơ sở có trọng số bằng không có thể được loại bỏ khỏi ensemble
và không làm tăng thời gian suy luận cũng như kích thước ensemble; làm cho cả hai nhỏ hơn so với các phương pháp
cân bằng ensemble không thưa thớt, như stacking. Thuật toán 1 tương ứng với định nghĩa gốc của
GES và đã được triển khai như vậy trong AutoGluon. Auto-Sklearn đã bỏ qua dòng 8, có nghĩa là
ensemble kết quả có thể không phải là ensemble có hiệu suất tốt nhất trên dữ liệu xác thực.
Cách tiếp cận để phá vỡ sự bằng nhau tại dòng 5 và 8 cũng cụ thể cho việc triển khai.

Thuật toán 1 Lựa chọn Ensemble Tham lam với Thay thế
Đầu vào: Pool các mô hình cơ sở 𝑃, hàm mất mát ensemble 𝐿, số lần lặp 𝐼
Đầu ra: Vector trọng số 𝑤 có độ dài |𝑃|
1: 𝑟←[0···0] ⊲ Khởi tạo ensemble trống 𝐸=(𝑃,𝑟).
2: 𝐻←{𝑟} ⊲ Khởi tạo lịch sử ensemble.
3: for 1...𝐼 do
4:   𝑅←{𝑟′|𝑟′=𝑟 với một phần tử tăng thêm 1} ⊲ Tất cả các lần lặp có thể của mô hình cơ sở cho lần lặp này.
5:   𝑟←𝑝𝑖𝑐𝑘𝑂𝑛𝑒(arg min_{𝑟′∈𝑅} 𝐿((𝑃,𝑟′))) ⊲ Chọn (các) lần lặp tối thiểu hóa mất mát và chọn một (để phá vỡ sự bằng nhau).
6:   H←H∪{𝑟}
7: end for
8: 𝑟*←𝑝𝑖𝑐𝑘𝑂𝑛𝑒(arg min_{𝑟′∈H} 𝐿((𝑃,𝑟′))) ⊲ Lấy ensemble tốt nhất đã thấy làm ensemble cuối cùng.
9: return 𝑤 được tính bằng Phương trình 1 sử dụng 𝐸=(𝑃,𝑟*).

3 Công trình liên quan

GES lần đầu tiên được giới thiệu vào AutoML bởi Auto-Sklearn 1 (Feurer et al., 2015). Các tác giả nêu rằng
GES vượt trội hơn các lựa chọn thay thế như stacking hoặc tối ưu hóa số học không có gradient. Gần đây,
Purucker and Beel (2022) đã cho thấy rằng GES có thể hoạt động tốt hơn các phương pháp ensemble hậu hoc khác
cho dữ liệu từ OpenML (Vanschoren et al., 2013). Ngoài ra, theo hiểu biết tốt nhất của chúng tôi,
GES chưa bao giờ được so sánh với bất kỳ phương pháp ensemble hậu hoc nào khác.

Theo Auto-Sklearn, hầu hết các hệ thống AutoML đã chọn sử dụng GES cũng như vậy; Auto-PyTorch
và AutoGluon thậm chí còn dựa việc triển khai ban đầu của họ trên Auto-Sklearn. Ngoài
các biến thể nhỏ (ví dụ: trong việc phá vỡ sự bằng nhau hoặc tính toán trọng số cuối cùng), trong tất cả các trường hợp này,
thuật toán cơ bản tuân theo GES như được định nghĩa bởi Caruana et al. (2004).

Các lựa chọn thay thế dựa trên quần thể cho GES đã được khám phá trong các lĩnh vực khác, ví dụ: để tìm một tập con của
các mô hình cơ sở cho một ensemble (Partridge and Yates, 1996; Zhou et al., 2002; Zhou and Tang, 2003;
Cavalcanti et al., 2016; Onan et al., 2017). QDO đã được sử dụng trước đây để xây dựng ensemble (Boisvert
and Sheppard, 2021; Nickerson and Hu, 2021; Cardoso et al., 2021b,a, 2022; Ferigo et al., 2023), nhưng
luôn tập trung vào duy trì một quần thể đa dạng về mặt hành vi của các mô hình cơ sở, từ đó
xây dựng một ensemble sau khi tối ưu hóa. Ngược lại, chúng tôi tập trung vào tính đa dạng hành vi của một
quần thể các ensemble. Cách tiếp cận QDO của chúng tôi liên quan chặt chẽ hơn đến tối ưu hóa đa mục tiêu
cho lựa chọn ensemble với chất lượng và đa dạng như các mục tiêu (Partridge and Yates, 1996; Martínez-
Munoz and Suárez, 2004; Banfield et al., 2005; Partalas et al., 2010; Li et al., 2012; Cavalcanti et al.,

3

--- TRANG 4 ---
2016). Tuy nhiên, các cách tiếp cận đa mục tiêu tối ưu hóa cả hai mục tiêu, trong khi QDO chỉ tối ưu hóa
hiệu suất, hưởng lợi từ một quần thể đa dạng về mặt hành vi trong quá trình tối ưu hóa.

4 Phương pháp: Tối ưu hóa Chất lượng (Đa dạng) dựa trên Quần thể cho Lựa chọn Ensemble

Các phương pháp của chúng tôi duy trì một quần thể ensemble để thực hiện tìm kiếm ngẫu nhiên cho một
vector trọng số tối ưu. Theo các khái niệm của lựa chọn ensemble với thay thế, như được giới thiệu bởi
GES, vector trọng số cuối cùng là thưa thớt theo thiết kế, và chúng tôi biểu diễn một ensemble như một multiset của các mô hình cơ sở 𝐸. Chúng tôi phân biệt giữa tối ưu hóa chất lượng cho lựa chọn ensemble (QO-ES) và tối ưu hóa đa dạng chất lượng cho lựa chọn ensemble (QDO-ES) dựa trên cách quần thể được duy trì.

Theo thuật ngữ QDO, chúng tôi lưu trữ quần thể trong một archive 𝐴, một tập hợp có kích thước 𝑎. Chúng tôi xây dựng
trên pyribs (Tjanaka et al., 2021), một thư viện Python cho QDO, để triển khai các archive. Trong QO-ES, chúng tôi
duy trì quần thể bằng cách đơn giản lưu trữ 𝑎 giải pháp quan sát được với mất mát thấp nhất trong 𝐴. Ngược lại, để duy trì một quần thể cho QDO-ES, chúng tôi cần thêm một khái niệm về
đa dạng hành vi và một cơ chế lưu trữ cho 𝐴 xem xét đa dạng hành vi.

Trong phần sau, chúng tôi trước tiên chi tiết cách chúng tôi duy trì đa dạng hành vi trong QDO-ES, và sau đó
các cách mà các quyết định ngẫu nhiên được sử dụng trong các cách tiếp cận của chúng tôi.

4.1 Duy trì Quần thể với Đa dạng Hành vi

QDO-ES yêu cầu một không gian hành vi B cho đa dạng ensemble, sao cho chúng ta có thể xác định
đa dạng hành vi w.r.t. đa dạng ensemble, và một archive xem xét đa dạng hành vi.

4.1.1 Một Không gian Hành vi cho Đa dạng Ensemble. Chúng tôi ánh xạ một ensemble 𝐸 đến một không gian hành vi B
theo hai chỉ số đa dạng ensemble:

Tương quan mất mát trung bình (ALC) đo đa dạng ensemble rõ ràng theo công trình trước đây
về các chỉ số đa dạng ensemble dựa trên tương quan (Tumer and Ghosh, 1999; Brown et al., 2005). Việc triển khai của chúng tôi đo tương quan Pearson trung bình giữa các vector mất mát trên tất cả các cặp
mô hình cơ sở có trọng số khác không trong 𝐸. Một vector mất mát chứa sự khác biệt giữa 1 và
xác suất dự đoán của lớp đúng cho mỗi thể hiện.

Tương tự không gian cấu hình (CSS) đo đa dạng ensemble ngầm bằng cách đo
tương tự cặp đôi trung bình của các cấu hình của các mô hình cơ sở được bao gồm trong 𝐸 sử dụng
tương tự Gower (Gower, 1971); nó là ngầm, vì các cấu hình khác nhau không đảm bảo các dự đoán khác nhau. Theo hiểu biết tốt nhất của chúng tôi, tương tự của các cấu hình chưa bao giờ được sử dụng trong bối cảnh ensemble, nhưng đo đa dạng hành vi ngầm đã được sử dụng thành công
trong QDO cho học tăng cường (Ferigo et al., 2023). Hơn nữa, CSS không yêu cầu một định nghĩa có thể
thiên lệch về đa dạng ensemble, vì nó đo một biến thiên hiện có trong không gian đầu vào
của các thuật toán tạo ra các mô hình cơ sở; so với, ALC đo đa dạng ensemble
trong không gian đầu ra của các mô hình cơ sở được tạo ra bởi các thuật toán này.

ALC và CSS được hình thức hóa trong Phụ lục D.1. Chúng tôi chọn một không gian hành vi hai chiều,
vì những không gian này được biết là hoạt động tốt khi đa dạng hành vi không phù hợp với hiệu suất
(Pugh et al., 2016). Như đã chỉ ra trong phần giới thiệu, có vẻ như có một sự đánh đổi giữa
đa dạng ensemble và hiệu suất. Do đó, sự phù hợp của đa dạng hành vi và hiệu suất
của một ensemble thường không được biết.

4.1.2 Một Archive cho QDO-ES. Trong một ứng dụng QDO điển hình, chúng ta chia không gian hành vi một lần thành
𝑎 niches (đôi khi được gọi là phân vùng), được biểu diễn bởi các bin. Các niches được tính toán dựa trên
phạm vi lý thuyết của các chiều của B sao cho các ranh giới của niches được phân bố đều
trên B. Trong quá trình tối ưu hóa, các giải pháp tốt nhất quan sát được cho mỗi niche sau đó được lưu trữ trong
archive 𝐴. Chi tiết, các giải pháp được lưu trữ trong niche tương ứng với các giá trị hành vi của chúng nếu
mất mát của chúng nhỏ hơn giải pháp hiện tại trong bin (hoặc nếu bin trống). Do đó, 𝐴 trong QDO thực thi
cạnh tranh cục bộ giữa các giải pháp tương tự về mặt hành vi (Chatzilygeroudis et al., 2021). Chúng tôi đã sử dụng

4

--- TRANG 5 ---
một archive ranh giới trượt (Fontaine et al., 2019) cho QDO-ES để cho phép cạnh tranh cục bộ tốt hơn
và lấy mẫu ngẫu nhiên đại diện hơn; được thúc đẩy chi tiết hơn trong Phụ lục D.2.

Lưu ý rằng các nhà thực hành QDO quan tâm đến các giải pháp tốt nhất cho tất cả các niches, trong khi chúng tôi, trong
AutoML, chỉ quan tâm đến ensemble tốt nhất đơn lẻ. Ứng dụng QDO của chúng tôi là không điển hình;
tuy nhiên, có quyền truy cập vào một quần thể đa dạng trong quá trình tối ưu hóa có thể cải thiện hiệu suất,
ngay cả khi người ta chỉ quan tâm đến một giải pháp tốt nhất đơn lẻ (Nguyen et al., 2015; Mouret and Clune, 2015).

4.2 Tính ngẫu nhiên trong quá trình Tối ưu hóa

Chúng tôi đã triển khai ba cách để bao gồm tính ngẫu nhiên trong quá trình tối ưu hóa: lấy mẫu của phụ huynh,
lai ghép, và đột biến. Chúng tôi lấy mẫu hai giải pháp để áp dụng lai ghép (có thể theo sau bởi
đột biến) lên chúng; thay vào đó, nếu không có lai ghép nào được áp dụng, chúng tôi chỉ lấy mẫu một và đột biến nó.

4.2.1 Lấy mẫu. Chúng tôi đã triển khai ba cách tiếp cận để lấy mẫu phụ huynh từ một archive: lấy mẫu xác định,
trả về (các) giải pháp tốt nhất từ archive; một biến thể không xác định của
lựa chọn giải đấu (Miller and Goldberg, 1995), lấy mẫu một tập hợp các giải pháp và trả về
(các) người chiến thắng của một giải đấu ngẫu nhiên hóa – được mô tả chi tiết trong Phụ lục D.3; và lấy mẫu động,
một cách tiếp cận thích ứng sử dụng lấy mẫu xác định hoặc ngẫu nhiên.

Trong lấy mẫu động, xác suất ban đầu của lấy mẫu ngẫu nhiên là 50% sao cho cả hai lựa chọn
đều có khả năng như nhau trong lần lặp đầu tiên. Xác suất của lấy mẫu xác định vs. ngẫu nhiên được
cập nhật sau mỗi lần lặp, dựa trên tỷ lệ giữa hiệu suất trung bình của cả hai cách tiếp cận lấy mẫu
trên một cửa sổ các lần lặp gần đây, sao cho chiến lược lấy mẫu với hiệu suất trung bình cao hơn
có nhiều khả năng được sử dụng trong lần lặp tiếp theo; xem Phụ lục D.4.

Chúng tôi đã chọn một cách tiếp cận lấy mẫu thích ứng vì chúng tôi kỳ vọng xác suất tối ưu của
lấy mẫu xác định vs. ngẫu nhiên thay đổi theo thời gian – như một sự đánh đổi khám phá-khai thác
(Qin and Suganthan, 2005; Audibert et al., 2009; Li et al., 2013). Hơn nữa, chúng tôi không muốn
giới thiệu một siêu tham số bổ sung.

4.2.2 Đột biến. Để giữ cho vector trọng số kết quả thưa thớt và để tuân thủ các khái niệm của lựa chọn ensemble
với thay thế, chúng tôi theo ý tưởng của GES, theo nghĩa là chúng tôi điều chỉnh 𝑟 trong quá trình đột biến
bằng cách tăng một trong các phần tử của nó. Chúng tôi giới thiệu tính ngẫu nhiên bổ sung bằng cách chọn ngẫu nhiên
phần tử này. Chúng tôi cung cấp thêm chi tiết và mã giả về toán tử đột biến trong Phụ lục D.5.

4.2.3 Lai ghép. Chúng tôi sử dụng lai ghép hai điểm (Jong and Spears, 1992) hoặc lai ghép trung bình (Li et al., 2013).
Chúng tôi áp dụng lai ghép lên 𝑟′ và 𝑟′′ cho hai ensemble 𝐸′ và 𝐸′′ thay vì 𝑤_𝐸′ và 𝑤_𝐸′′; nếu không,
lai ghép có thể tạo ra một vector mà chúng ta không thể tạo ra một 𝑟 cho multiset 𝐸, tức là tính toán
nghịch đảo của Phương trình 1. Để tạo ra một multiset hợp lệ, chúng tôi làm tròn kết quả của lai ghép trung bình đến
số nguyên cao hơn tiếp theo. Đối với lai ghép hai điểm, chúng tôi quan sát thấy rằng con cái thường gần như
chỉ toàn số không, vì 𝑟′ và 𝑟′′ thưa thớt. Để chống lại điều này, chúng tôi áp dụng lai ghép hai điểm chỉ trên
các phần tử khác không trong 𝑟′ hoặc 𝑟′′, xem Phụ lục D.6 để biết thêm chi tiết.

Xác suất sử dụng lai ghép thích ứng qua quá trình chạy, tương tự như cách tiếp cận lấy mẫu thích ứng.
Theo cùng một cơ chế, con cái được tạo ra bởi lai ghép có thể đột biến.

4.3 Kết hợp Mọi thứ lại với nhau

Việc thực hiện cuối cùng của Q(D)O-ES được mô tả trong Thuật toán 2. Đầu tiên, 𝑖𝑛𝑖𝑡𝐴𝑟𝑐ℎ𝑖𝑣𝑒(𝐴,𝑃) cố gắng chèn
một tập hợp ban đầu các ensemble vào archive (Thuật toán 2, dòng 1). Chúng tôi đã triển khai ba cách tiếp cận
khởi tạo tập hợp: các ensemble chỉ bao gồm một mô hình cơ sở, tất cả các ensemble có kích thước 2 bao gồm
mô hình cơ sở tốt nhất, hoặc 𝑚-nhiều ensemble ngẫu nhiên có kích thước 2, được hình thức hóa trong Phụ lục D.7.

Thứ hai, một lô giải pháp được xây dựng trong mỗi lần lặp (Dòng 4-11) sử dụng 𝑠𝑎𝑚𝑝𝑙𝑒(), 𝑐𝑟𝑜𝑠𝑠𝑜𝑣𝑒𝑟(),
và 𝑚𝑢𝑡𝑎𝑡𝑒(). Hoạt động bên trong của mỗi hàm đó phụ thuộc vào siêu tham số của chúng và
xác suất thích ứng hiện tại. Cuối cùng, một giải pháp mới được tạo ra chỉ bằng lai ghép, chỉ đột biến,
hoặc lai ghép và đột biến. Cuối cùng, chúng tôi đánh giá các giải pháp đề xuất có trong lô và

5

--- TRANG 6 ---
cố gắng chèn chúng vào archive (Dòng 12). Do tính ngẫu nhiên, các giải pháp đề xuất có thể bằng
những giải pháp đã được đánh giá trước đó. Do đó, chúng tôi đã giới thiệu lấy mẫu từ chối (Thuật toán 2, dòng 8) sao cho
các ensemble đã được đánh giá trước đó không được thêm vào lô. Để tránh lấy mẫu từ chối vô tận trong
các trường hợp cạnh quan sát được, chúng tôi đã thêm một phanh khẩn cấp để dừng vòng lặp, xem Phụ lục D.8.

Cuối cùng, chúng tôi tính toán 𝑟* sử dụng 𝑏𝑒𝑠𝑡𝑆𝑜𝑙𝑢𝑡𝑖𝑜𝑛(𝐴), trả về 𝑟 tốt nhất theo mất mát xác thực
trong số: mô hình tốt nhất đơn lẻ, lai ghép trung bình của tất cả các giải pháp trong 𝐴, và giải pháp tốt nhất
trong 𝐴.

Thuật toán 2 Tối ưu hóa Chất lượng (Đa dạng) dựa trên Quần thể cho Lựa chọn Ensemble
Đầu vào: Pool các mô hình cơ sở 𝑃, hàm mất mát ensemble 𝐿, archive chất lượng (đa dạng) 𝐴, số lần lặp 𝐼, kích thước lô 𝐵
Đầu ra: Vector trọng số 𝑤 có độ dài |𝑃|
1: 𝐴←𝑖𝑛𝑖𝑡𝐴𝑟𝑐ℎ𝑖𝑣𝑒(𝐴,𝑃) ⊲ Điền archive với một tập hợp các ensemble ban đầu.
2: for 1...𝐼 do
3:   𝑆←∅
4:   while |𝑆|<𝐵 do ⊲ Xây dựng lô 𝑆.
5:     𝑟,𝑟′←𝑠𝑎𝑚𝑝𝑙𝑒(𝐴) ⊲ Nếu lai ghép bị hủy kích hoạt, 𝑟=𝑟′.
6:     𝑟_{sol}←𝑐𝑟𝑜𝑠𝑠𝑜𝑣𝑒𝑟(𝑟,𝑟′)
7:     𝑟_{sol}←𝑚𝑢𝑡𝑎𝑡𝑒((𝑃,𝑟_{sol})) ⊲ Nếu đột biến bị hủy kích hoạt, 𝑟_{sol}=𝑚𝑢𝑡𝑎𝑡𝑒((𝑃,𝑟_{sol})).
8:     if 𝑢𝑛𝑘𝑛𝑜𝑤𝑛(𝑟_{sol}) then ⊲ Lấy mẫu từ chối.
9:       𝑆←𝑆∪𝑟_{sol}
10:    end if
11:  end while
12:  𝐴←𝑖𝑛𝑠𝑒𝑟𝑡𝐼𝑛𝑡𝑜𝐴𝑟𝑐ℎ𝑖𝑣𝑒(𝐴,𝑆) ⊲ Thêm giải pháp vào archive và cập nhật ranh giới cho QDO-ES.
13: end for
14: 𝑟*←𝑔𝑒𝑡𝐵𝑒𝑠𝑡𝑆𝑜𝑙𝑢𝑡𝑖𝑜𝑛(𝐴)
15: return 𝑤 được tính bằng Phương trình 1 sử dụng 𝐸=(𝑃,𝑟*).

5 Thực nghiệm

Chúng tôi so sánh GES, QO-ES, và QDO-ES. Hơn nữa, chúng tôi đã sử dụng mô hình tốt nhất đơn lẻ làm baseline.
Các bộ dữ liệu được sử dụng trong thực nghiệm của chúng tôi và đánh giá cuối cùng tuân theo quy trình đánh giá
được mô tả và thảo luận trong điểm chuẩn AutoML (AMLB) (Gijsbers et al., 2022). Chúng tôi tuân theo
AMLB vì, trong thực nghiệm của chúng tôi, đầu ra của một phương pháp ensemble mô phỏng đầu ra của một
hệ thống AutoML như thể nó đã sử dụng phương pháp được đánh giá cho ensemble hậu hoc.

Chính xác hơn, chúng tôi đã đánh giá các phương pháp w.r.t. ROC AUC sử dụng xác thực chéo 10-fold trên
71 bộ dữ liệu phân loại OpenML được sử dụng trong AMLB. Đối với đa lớp, chúng tôi sử dụng trung bình macro
ROC AUC một-so-với-phần-còn-lại. Vì ROC AUC yêu cầu xác suất dự đoán và độc lập với
ngưỡng quyết định, chúng tôi bổ sung nó bằng cách cũng đánh giá w.r.t. độ chính xác cân bằng, yêu cầu
nhãn được dự đoán và phụ thuộc vào ngưỡng (thay vì một chỉ số độc lập ngưỡng khác
như mất mát log). Theo quy trình AMLB, chúng tôi phân biệt giữa nhị phân và
đa lớp. Chúng tôi gọi một sự kết hợp của một cài đặt phân loại với một chỉ số đánh giá là một
kịch bản (ví dụ: ROC AUC cho đa lớp).

Tạo Mô hình Cơ sở. Để mô phỏng các phương pháp ensemble hậu hoc cho AutoML, chúng tôi yêu cầu một pool
các mô hình cơ sở 𝑃 được tạo ra bởi một hệ thống AutoML. Do đó, chúng tôi đã chạy Auto-Sklearn 1 trên mỗi
fold của mỗi bộ dữ liệu, hai lần – một lần cho mỗi chỉ số. Đối với mỗi fold, chúng tôi đã lưu trữ tập hợp tất cả các mô hình
được xác thực trong quá trình lựa chọn mô hình. Trong thực nghiệm của chúng tôi và theo mặc định, Auto-Sklearn 1 sử dụng một phần chia 33%
hold-out từ dữ liệu huấn luyện làm dữ liệu xác thực. Sau đó, chúng tôi đã cắt tỉa tập hợp này trên mỗi fold
thành 50. Vì cắt tỉa là một bước tiền xử lý trước khi ensemble hậu hoc, chúng tôi quyết định bao gồm hai
chiến lược cắt tỉa trong thực nghiệm của chúng tôi, được ký hiệu là TopN và SiloTopN, sao cho chúng tôi có được hai 𝑃s
trên mỗi fold. Đối với TopN, chúng tôi cắt tỉa thành 50 mô hình có hiệu suất tốt nhất theo điểm xác thực
theo hành vi mặc định của Auto-Sklearn. Đối với SiloTopN, chúng tôi cắt tỉa thành 50, sao cho nhiều
mô hình có hiệu suất cao nhất của mỗi họ thuật toán được giữ lại theo cách tiếp cận của AutoGluon.

Theo AMLB, chúng tôi đã cung cấp cho Auto-Sklearn ngân sách 4 giờ, 32 GB bộ nhớ, và 8 lõi
trên mỗi fold. Trong tất cả các thực nghiệm của chúng tôi, chúng tôi đã sử dụng CPU AMD EPYC 7452. Chúng tôi đã tăng bộ nhớ lên

6

--- TRANG 7 ---
64 hoặc 128 GB, để ngăn Auto-Sklearn chạy hết bộ nhớ cho 11 bộ dữ liệu. Kết quả là,
Auto-Sklearn đã tạo ra ít nhất 50 mô hình cho tất cả trừ 5 bộ dữ liệu, xem Phụ lục E.1 để có tổng quan.

Cài đặt Siêu tham số. Vì hành vi của QO-ES và QDO-ES phụ thuộc nhiều vào
siêu tham số của chúng, và để đảm bảo rằng tất cả các phương pháp được so sánh công bằng, chúng tôi đã định nghĩa một lưới
cài đặt siêu tham số và đánh giá một cách toàn diện. Điều này dẫn đến việc đánh giá 219 cấu hình
riêng biệt: 1 cho mô hình tốt nhất đơn lẻ; 2 cho GES, một cho biến thể của AutoGluon và một cho Auto-Sklearn
(xem Phần 2); 108 cho QD-ES; và 108 cho QDO-ES. Chúng tôi cũng coi chiến lược cắt tỉa như một
siêu tham số, điều này làm tăng gấp đôi không gian cấu hình của chúng tôi thành 437 (mô hình tốt nhất đơn lẻ giống hệt nhau cho cả hai
phương pháp cắt tỉa). Danh sách các cài đặt siêu tham số cho Q(D)O-ES có sẵn trong Phụ lục E.2.

Để tính toán điểm đánh giá cuối cùng, trung bình trên các folds, chúng tôi đã chạy mỗi cấu hình cho
mỗi fold của mỗi bộ dữ liệu cho cả hai chỉ số – tức là, tổng cộng, chúng tôi đã đánh giá 437∗71∗10∗2=620540
lần chạy ensemble. Chúng tôi đặt số lần lặp thành 50 cho GES theo mặc định của Auto-Sklearn
và đảm bảo rằng Q(D)O-ES sử dụng cùng số lượng đánh giá hàm tổng, tức là 50∗𝑚, bằng cách
điều chỉnh số lần lặp trong Thuật toán 2 tùy thuộc vào kích thước lô và có một
lô dư nếu cần thiết. Để đo hiệu quả, chúng tôi ghi lại thời gian chạy của các phương pháp ensemble
để hoàn thành 50 lần lặp hoặc 50∗𝑚 đánh giá hàm. Hơn nữa, chúng tôi ghi lại
kích thước của ensemble cuối cùng bằng cách đếm có bao nhiêu mô hình cơ sở có trọng số khác không và do đó
ảnh hưởng đến dự đoán cuối cùng. Chúng tôi đã triển khai đa xử lý cho GES và Q(D)O-ES và chạy tất cả
các cấu hình với cùng phần cứng và tài nguyên được sử dụng cho Auto-Sklearn.

Để chọn một cấu hình cho mỗi phương pháp trên mỗi bộ dữ liệu, chúng tôi đã sử dụng xác thực chéo
leave-one-out (LOO CV). Đối với 𝑑 bộ dữ liệu, cấu hình với cải tiến chuẩn hóa trung vị cao nhất trên
𝑑−1 bộ dữ liệu được chọn để đại diện cho phương pháp trên bộ dữ liệu out-of-fold. Một thảo luận
về lý do chúng tôi sử dụng cải tiến chuẩn hóa trung vị và thấy đó là phương pháp thích hợp duy nhất
cho thực nghiệm của chúng tôi có thể được tìm thấy trong Phụ lục E.3.

Cải tiến Chuẩn hóa. Chúng tôi sử dụng cải tiến chuẩn hóa, theo AMLB. Do đó, chúng tôi
chuẩn hóa các điểm số trên mỗi bộ dữ liệu, sao cho 0 bằng với hiệu suất của cấu hình tốt nhất trên
bộ dữ liệu này và −1 bằng với hiệu suất của mô hình tốt nhất đơn lẻ. Nếu hiệu suất của chúng bằng nhau,
thì chúng tôi đặt mọi thứ tốt như mô hình tốt nhất đơn lẻ thành −1 và phạt các cấu hình tệ hơn thành
−10, xem Phụ lục E.4. Đối với việc chọn LOO CV, chúng tôi chỉ chuẩn hóa trên các cấu hình từ
cùng một phương pháp, trong khi chúng tôi chuẩn hóa trên tất cả các cấu hình đã chọn cho đánh giá cuối cùng.

Kiểm định Thống kê. Một lần nữa theo AMLB, chúng tôi thực hiện kiểm định Friedman với kiểm định hậu hoc Nemenyi
(𝛼=0.05) trên các điểm số không chuẩn hóa của mỗi phương pháp để có được một xếp hạng tuyệt đối của tất cả
các phương pháp và để kiểm định sự khác biệt có ý nghĩa thống kê giữa chúng.

6 Kết quả

Đối với mỗi kịch bản, chúng tôi trực quan hóa xếp hạng tuyệt đối trung bình và kết quả của các kiểm định thống kê thông qua sự khác biệt
quan trọng trong Hình 1. Chúng tôi quan sát thấy rằng đối với tất cả các kịch bản, lựa chọn ensemble hậu hoc có ý nghĩa thống kê
tốt hơn mô hình tốt nhất đơn lẻ – chúng ta luôn có thể tăng hiệu suất trung bình.
Q(D)O-ES xếp hạng cao hơn GES trong tất cả các kịch bản, ngoại trừ độ chính xác cân bằng đa lớp.

Để điều tra thêm kết quả của chúng tôi, chúng tôi sử dụng boxplots của cải tiến chuẩn hóa để trực quan hóa
phân phối hiệu suất tương đối cho tất cả các phương pháp trên các bộ dữ liệu – xem Hình 2. Mặc dù
các phương pháp ensemble vượt trội hơn mô hình tốt nhất đơn lẻ (được chỉ ra bởi một đường đỏ) trung bình, có
các bộ dữ liệu mà điều này không đúng (như được chỉ ra bởi các số trong dấu ngoặc vuông trong
Hình 2). Vì tất cả các phương pháp đều có quyền truy cập vào điểm xác thực của mô hình tốt nhất đơn lẻ và chỉ đề xuất
một ensemble nếu nó vượt trội hơn mô hình tốt nhất đơn lẻ, hiện tượng này liên quan trực tiếp đến overfitting.

Chúng tôi muốn lưu ý rằng việc tính toán tất cả kết quả mà không có song song hóa trên các bộ dữ liệu, folds,
cấu hình, và chỉ số nhưng có song song hóa của các hệ thống AutoML và phương pháp ensemble
trên 8 lõi mất khoảng 3.9 năm thời gian thực, xem Phụ lục F.1.

7

--- TRANG 8 ---
1 2 3 4
SingleBest
GES QDO-ESQO-ESCD(a) ROC AUC - Nhị phân (41 Bộ dữ liệu)

1 2 3 4
SingleBest
GES QDO-ESQO-ESCD (b) ROC AUC - Đa lớp (30 Bộ dữ liệu)

1 2 3 4
SingleBest
GES QDO-ESQO-ESCD
(c) Độ chính xác Cân bằng - Nhị phân (41 Bộ dữ liệu)

1 2 3 4
SingleBest
QO-ES QDO-ESGESCD (d) Độ chính xác Cân bằng - Đa lớp (30 Bộ dữ liệu)

Hình 1: Biểu đồ Sự khác biệt Quan trọng cho Điểm Kiểm tra: Xếp hạng trung bình của các phương pháp (thấp hơn là tốt hơn). Các phương pháp
được kết nối bởi một thanh không khác biệt đáng kể.

2.0
 1.5
 1.0
 0.5
 0.0
Cải tiến Chuẩn hóaQO-ES [0]
QDO-ES [2]
GES [2]Phương pháp
SingleBest
(a) ROC AUC - Nhị phân (41 Bộ dữ liệu)

2.0
 1.5
 1.0
 0.5
 0.0
Cải tiến Chuẩn hóaQO-ES [2]
QDO-ES [2]
GES [4]Phương pháp
SingleBest (b) ROC AUC - Đa lớp (30 Bộ dữ liệu)

2.0
 1.5
 1.0
 0.5
 0.0
Cải tiến Chuẩn hóaQO-ES [5]
QDO-ES [5]
GES [8]Phương pháp
SingleBest
(c) Độ chính xác Cân bằng - Nhị phân (41 Bộ dữ liệu)

2.0
 1.5
 1.0
 0.5
 0.0
Cải tiến Chuẩn hóaQO-ES [1]
QDO-ES [2]
GES [1]Phương pháp
SingleBest (d) Độ chính xác Cân bằng - Đa lớp (30 Bộ dữ liệu)

Hình 2: Boxplots Cải tiến Chuẩn hóa: Cao hơn là tốt hơn. Mỗi chấm đại diện cho một bộ dữ liệu.
Số trong dấu ngoặc vuông bên cạnh tên phương pháp đếm các giá trị ngoại lai nhỏ hơn −2.

8

--- TRANG 9 ---
Overfitting. Chúng tôi quan sát thấy rằng overfitting có ảnh hưởng lớn đến hiệu suất kiểm tra cuối cùng của
các phương pháp của chúng tôi khi lặp lại đánh giá của chúng tôi cho điểm xác thực thay vì điểm kiểm tra; xem Hình 3.
Chúng tôi lưu ý rằng sự khác biệt giữa Q(D)O-ES và GES có ý nghĩa thống kê trong tất cả các kịch bản.
Hơn nữa, các lợi ích cải tiến chuẩn hóa cho Q(D)O-ES cũng lớn hơn đáng kể so với
GES, xem Phụ lục F.2. Tương tự, như mong đợi, không có phương pháp ensemble hậu hoc nào hoạt động tệ hơn
mô hình tốt nhất đơn lẻ trên bất kỳ bộ dữ liệu nào. Do đó, thách thức chính mà ensemble hậu hoc gặp phải là
thiếu khả năng tổng quát hóa từ tập xác thực đến tập kiểm tra.

1 2 3 4
SingleBest
GES QO-ESQDO-ESCD
(a) ROC AUC - Nhị phân (41 Bộ dữ liệu)

1 2 3 4
SingleBest
GES QO-ESQDO-ESCD (b) ROC AUC - Đa lớp (30 Bộ dữ liệu)

1 2 3 4
SingleBest
GES QO-ESQDO-ESCD
(c) Độ chính xác Cân bằng - Nhị phân (41 Bộ dữ liệu)

1 2 3 4
SingleBest
GES QO-ESQDO-ESCD (d) Độ chính xác Cân bằng - Đa lớp (30 Bộ dữ liệu)

Hình 3: Biểu đồ Sự khác biệt Quan trọng cho Điểm Xác thực: Xếp hạng trung bình của các phương pháp (thấp hơn là tốt hơn).
Các phương pháp được kết nối bởi một thanh không khác biệt đáng kể.

Chúng tôi tin rằng dữ liệu xác thực được tạo ra bởi phân chia hold-out 33% của Auto-Sklearn có thể
chịu trách nhiệm cho sự khác biệt lớn giữa hiệu suất xác thực và kiểm tra. Trong nghiên cứu tương lai,
sẽ thú vị khi điều tra hiệu suất và khả năng tổng quát hóa của các phương pháp của chúng tôi cho các
hệ thống AutoML khác sử dụng các quy trình xác thực tinh vi hơn, ví dụ: AutoGluon với xác thực chéo
k-fold lặp lại n lần.

Về GES, chúng tôi quan sát thấy rằng dựa trên quy trình LOO CV, biến thể của AutoGluon luôn được
chọn cho độ chính xác cân bằng, trong khi đối với ROC AUC, biến thể của Auto-Sklearn được chọn cho tất cả các bộ dữ liệu phân loại đa lớp và ∼12% (5/41) bộ dữ liệu phân loại nhị phân. Đối với điểm xác thực,
biến thể của AutoGluon luôn được chọn. Chúng tôi lưu ý rằng sự khác biệt trong việc triển khai của
AutoGluon và Auto-Sklearn (xem Phần 2) có thể được hiểu như sự khác biệt trong việc xử lý overfitting:
biến thể đầu tiên dễ bị overfitting hơn biến thể sau, do dựa nhiều hơn vào điểm xác thực
cho việc lựa chọn ensemble cuối cùng.

Đa dạng Ensemble. Chúng tôi lưu ý rằng đối với tất cả các phương pháp, cách tiếp cận tiền xử lý được chọn dựa trên
quy trình LOO CV là SiloTopN, thay vì TopN. Sự đa dạng của các mô hình cơ sở được thúc đẩy bởi
việc bao gồm các thuật toán từ mỗi họ dường như luôn có lợi. Sự khác biệt này trong đa dạng thuật toán
cũng có thể thấy trong pool các mô hình cơ sở 𝑃 trên mỗi bộ dữ liệu. Số lượng trung bình các thuật toán riêng biệt
trong 𝑃 trên tất cả các bộ dữ liệu và chỉ số là ∼2.77 cho TopN trong khi SiloTopN đạt ∼15.33;
xem Bảng 2 trong Phụ lục E.1 cho các số liệu trên mỗi bộ dữ liệu. Chúng tôi cũng quan sát điều này khi phân tích
tầm quan trọng của siêu tham số, xem Phụ lục F.3; chúng tôi thấy phương pháp tiền xử lý là siêu tham số
quan trọng nhất, tiếp theo là phương pháp lấy mẫu và khởi tạo.

9

--- TRANG 10 ---
Bảng 1: Hiệu quả Ensemble: Kích thước ensemble trung bình và thời gian chạy cho 50 lần lặp tính bằng giây.

Kích thước Ensemble Trung bình | Thời gian Chạy Trung bình @50
Độ chính xác Cân bằng | ROC AUC | Độ chính xác Cân bằng | ROC AUC
Phương pháp | Nhị phân | Đa lớp | Nhị phân | Đa lớp | Nhị phân | Đa lớp | Nhị phân | Đa lớp
GES | 7.95 | 7.91 | 8.02 | 10.09 | 49.13 | 120.36 | 43.66 | 197.57
QDO-ES | 9.23 | 11.47 | 14.06 | 13.27 | 81.92 | 158.24 | 91.04 | 451.48
QO-ES | 8.02 | 10.28 | 15.76 | 13.88 | 65.41 | 132.76 | 75.5 | 388.62

Về việc sử dụng đa dạng ensemble cho tối ưu hóa, chúng tôi thấy rằng QO-ES đánh bại QDO-ES
ngoại trừ độ chính xác cân bằng đa lớp trên dữ liệu kiểm tra. Trên dữ liệu xác thực, QDO-ES luôn đánh bại
QO-ES, mặc dù sự khác biệt trong xếp hạng không có ý nghĩa. Chúng tôi giả thuyết rằng việc thêm đa dạng ensemble
vào quá trình tối ưu hóa có thể, về nguyên tắc, có lợi, nhưng cũng làm tăng nguy cơ
overfitting, vì nó sử dụng phân phối của đa dạng ensemble, có thể không tổng quát hóa.

Hiệu quả. Xem Bảng 1 để có tổng quan về hiệu quả cho mỗi phương pháp ensemble trên mỗi kịch bản.
Kích thước của một ensemble tương ứng với có bao nhiêu mô hình cơ sở phải tính toán dự đoán và
do đó trực tiếp đại diện cho hiệu quả suy luận của một ensemble. Để điều tra hiệu quả tối ưu hóa
của các phương pháp chúng tôi nghiên cứu, chúng tôi đã xem xét thời gian chạy trung bình cần thiết để hoàn thành
50 lần lặp. Lưu ý rằng đối với đa lớp, các phương pháp hiệu quả hơn cho độ chính xác cân bằng so với
ROC AUC, vì tính toán độ chính xác cân bằng tốn kém hơn ROC AUC cho đa lớp.

Kích thước trung bình trên tất cả các kịch bản của các ensemble được tạo ra là ∼8.5 cho GES, ∼12 cho QO-ES,
và ∼12 cho QDO-ES. Mặc dù Q(D)O-ES tạo ra các ensemble hơi lớn hơn, các vector trọng số của chúng
vẫn thưa thớt, xem xét rằng 50 mô hình cơ sở có sẵn trong hầu hết các trường hợp. GES cần trung bình
trên tất cả các kịch bản ∼103 giây. Việc triển khai hiện tại của chúng tôi cho QO-ES mất ∼165 giây, và
QDO-ES ∼196 giây, nhưng tối ưu hóa mã của chúng tôi có thể sẽ tăng hiệu quả. Chúng tôi lưu ý rằng
những thời gian này không đáng kể so với ngân sách thời gian 4 giờ được Auto-Sklearn chi tiêu cho
việc tìm các mô hình cơ sở.

Nghiên cứu Ablation. Khi thực hiện các nghiên cứu ablation cho Q(D)O-ES, chúng tôi thấy rằng cách tiếp cận tiền xử lý
dường như là thành phần quan trọng nhất cho cách tiếp cận của chúng tôi; tiếp theo là phương pháp lấy mẫu
và cách tiếp cận khởi tạo archive (xem Phụ lục F.3). Cụ thể, SiloTopN dường như
luôn hoạt động tốt hơn TopN, như được chỉ ra bởi cách tiếp cận tiền xử lý được chọn ở trên (xem
Phụ lục F.4). Phương pháp lấy mẫu và khởi tạo tốt nhất dường như khác nhau một chút trên mỗi kịch bản.

7 Kết luận

Ensemble hậu hoc được sử dụng rộng rãi trong các hệ thống AutoML và có thể rất quan trọng để có được
hiệu suất dự đoán tối đa. Một trong những phương pháp phổ biến nhất là lựa chọn ensemble tham lam với
thay thế (GES) (Caruana et al., 2004, 2006). Trong nghiên cứu này, chúng tôi đã trình bày QO-ES và QDO-ES,
hai thuật toán tối ưu hóa dựa trên quần thể mới cho lựa chọn ensemble hậu hoc trong AutoML.
QO-ES duy trì một archive của các ensemble có hiệu suất tốt nhất và cải thiện tuần tự
chúng, sử dụng các toán tử đột biến và lai ghép. QDO-ES xây dựng trên QO-ES, nhưng tận dụng
các khái niệm từ tối ưu hóa chất lượng đa dạng (Chatzilygeroudis et al., 2021), duy trì một archive
của các ensemble đa dạng hành vi khác nhau trong quá trình tối ưu hóa. Trong các thực nghiệm mở rộng,
chúng tôi đã chứng minh rằng 1) lựa chọn ensemble hậu hoc cải thiện so với mô hình tốt nhất đơn lẻ, 2)
các phương pháp dựa trên quần thể QO-ES và QDO-ES của chúng tôi thường vượt trội hơn GES (mặc dù
sự khác biệt hiệu suất không có ý nghĩa thống kê trên dữ liệu kiểm tra), và 3) tôn trọng
đa dạng của các ensemble trong quá trình tối ưu hóa có thể có lợi nhưng cũng làm tăng nguy cơ overfitting.
Cuối cùng, chúng tôi muốn nhấn mạnh rằng overfitting là một thách thức nghiêm trọng cho ensemble hậu hoc trong AutoML.

10

--- TRANG 11 ---
Lời cảm ơn. Các nút CPU của cụm OMNI của Đại học Siegen (Bắc
Rhine-Westphalia, Đức) đã được sử dụng cho tất cả các thực nghiệm được trình bày trong nghiên cứu này. Nghiên cứu này
được hỗ trợ một phần bởi Bộ Kinh tế Bavaria, Phát triển Khu vực và
Năng lượng thông qua Trung tâm Phân tích - Dữ liệu - Ứng dụng (ADACenter) trong khuôn khổ
của BAYERN DIGITAL II (20-3410-2-9-8) và bởi TAILOR, là một phần của chương trình nghiên cứu và đổi mới EU Horizon 2020
dưới GA số 952215. Cuối cùng, chúng tôi cảm ơn các nhà bình duyệt vì phản hồi
mang tính xây dựng và đóng góp vào việc cải thiện bài báo.

Tài liệu tham khảo

Ahmed, M. A. O., Didaci, L., Lavi, B., and Fumera, G. (2017). Using diversity for classifier ensemble
pruning: an empirical investigation. Theoretical and Applied Informatics, 1(29).

Audibert, J., Munos, R., and Szepesvári, C. (2009). Exploration-exploitation tradeoff using variance
estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902.

Banfield, R. E., Hall, L. O., Bowyer, K. W., and Kegelmeyer, W. P. (2005). Ensemble diversity measures
and their application to thinning. Information Fusion, 6(1):49–62.

Boisvert, S. and Sheppard, J. W. (2021). Quality diversity genetic programming for learning decision
tree ensembles. In Genetic Programming - 24th European Conference, Proceedings, volume 12691
of Lecture Notes in Computer Science, pages 3–18. Springer.

Brown, G., Wyatt, J. L., Harris, R., and Yao, X. (2005). Diversity creation methods: a survey and
categorisation. Information Fusion, 6(1):5–20.

Cardoso, R. P., Hart, E., Kurka, D. B., and Pitt, J. (2021a). WILDA: wide learning of diverse
architectures for classification of large datasets. In Castillo, P. A. and Laredo, J. L. J., editors,
Applications of Evolutionary Computation - 24th International Conference, Proceedings, volume
12694 of Lecture Notes in Computer Science, pages 649–664. Springer.

Cardoso, R. P., Hart, E., Kurka, D. B., and Pitt, J. V. (2021b). Using novelty search to explicitly create
diversity in ensembles of classifiers. In Chicano, F. and Krawiec, K., editors, GECCO '21: Genetic
and Evolutionary Computation Conference, pages 849–857. ACM.

Cardoso, R. P., Hart, E., Kurka, D. B., and Pitt, J. V. (2022). The diversity-accuracy duality in
ensembles of classifiers. In Fieldsend, J. E. and Wagner, M., editors, GECCO '22: Genetic and
Evolutionary Computation Conference, Companion Volume, pages 627–630. ACM.

Caruana, R., Munson, A., and Niculescu-Mizil, A. (2006). Getting the most out of ensemble selection.
In Proceedings of the 6th IEEE International Conference on Data Mining, pages 828–833. IEEE
Computer Society.

Caruana, R., Niculescu-Mizil, A., Crew, G., and Ksikes, A. (2004). Ensemble selection from libraries of
models. In Machine Learning, Proceedings of the Twenty-first International Conference, volume 69
of ACM International Conference Proceeding Series. ACM.

Cavalcanti, G. D. C., Oliveira, L. S., Moura, T. J. M., and Carvalho, G. V. (2016). Combining diversity
measures for ensemble pruning. Pattern Recognition Letters, 74:38–45.

Chatzilygeroudis, K., Cully, A., Vassiliades, V., and Mouret, J.-B. (2021). Quality-diversity optimiza-
tion: a novel branch of stochastic optimization. In Black Box Optimization, Machine Learning,
and No-Free Lunch Theorems, pages 109–135. Springer.

11

--- TRANG 12 ---
Cruz, R. M. O., Sabourin, R., and Cavalcanti, G. D. C. (2018). Dynamic classifier selection: Recent
advances and perspectives. Information Fusion, 41:195–216.

Cully, A., Clune, J., Tarapore, D., and Mouret, J. (2015). Robots that can adapt like animals. Nature,
521(7553):503–507.

Das, S. and Suganthan, P. N. (2010). Differential evolution: A survey of the state-of-the-art. IEEE
Transactions on Evolutionary Computation, 15(1):4–31.

Dietterich, T. G. (2000a). Ensemble methods in machine learning. In Multiple Classifier Systems,
First International Workshop, MCS 2000, Proceedings, volume 1857 of Lecture Notes in Computer
Science, pages 1–15. Springer.

Dietterich, T. G. (2000b). An experimental comparison of three methods for constructing ensembles
of decision trees: Bagging, boosting, and randomization. Machine learning, 40(2):139–157.

Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., and Smola, A. J. (2020). Autogluon-
tabular: Robust and accurate automl for structured data. CoRR, abs/2003.06505.

Feldman, V., Frostig, R., and Hardt, M. (2019). The advantages of multiple classes for reducing
overfitting from test set reuse. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, volume 97 of Proceedings of Machine Learning Research, pages 1892–1900.
PMLR.

Ferigo, A., Custode, L. L., and Iacca, G. (2023). Quality diversity evolutionary learning of decision
trees. In Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing, pages 425–432.

Feurer, M., Eggensperger, K., Falkner, S., Lindauer, M., and Hutter, F. (2022). Auto-sklearn 2.0:
Hands-free automl via meta-learning. The Journal of Machine Learning Research, 23(1):11936–
11996.

Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., and Hutter, F. (2015). Efficient
and robust automated machine learning. Advances in neural information processing systems, 28.

Fontaine, M. C., Lee, S., Soros, L. B., de Mesentier Silva, F., Togelius, J., and Hoover, A. K. (2019).
Mapping hearthstone deck spaces through map-elites with sliding boundaries. In Proceedings of
The Genetic and Evolutionary Computation Conference, pages 161–169.

Gijsbers, P., Bueno, M. L. P., Coors, S., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren,
J. (2022). AMLB: an automl benchmark. CoRR, abs/2207.12560.

Gijsbers, P. and Vanschoren, J. (2019). GAMA: Genetic automated machine learning assistant.
Journal of Open Source Software, 4(33):1132.

Gower, J. C. (1971). A general coefficient of similarity and some of its properties. Biometrics,
27(4):857–871.

Hansen, L. K. and Salamon, P. (1990). Neural network ensembles. IEEE transactions on pattern
analysis and machine intelligence, 12(10):993–1001.

Hansen, N. and Auger, A. (2014). Principled design of continuous stochastic search: From theory to
practice. In Theory and Principled Methods for the Design of Metaheuristics, Natural Computing
Series, pages 145–180. Springer.

12

--- TRANG 13 ---
Hutter, F., Hoos, H. H., and Leyton-Brown, K. (2014). An efficient approach for assessing hyperpa-
rameter importance. In Proceedings of the 31th International Conference on Machine Learning,
ICML 2014, volume 32 of JMLR Workshop and Conference Proceedings, pages 754–762. JMLR.org.

Jong, K. A. D. and Spears, W. M. (1992). A formal analysis of the role of multi-point crossover in
genetic algorithms. Annals of mathematics and Artificial intelligence, 5(1):1–26.

Kennedy, J. and Eberhart, R. (1995). Particle swarm optimization. In Proceedings of ICNN'95 -
International Conference on Neural Networks, volume 4, pages 1942–1948. IEEE.

Kochenderfer, M. J. and Wheeler, T. A. (2019). Algorithms for optimization, Chapter 8, Stochastic
Methods. Mit Press.

Kumar, G. and Kumar, K. (2012). The use of artificial-intelligence-based ensembles for intrusion
detection: A review. Applied Computational Intelligence and Soft Computing, 2012:850160:1–
850160:20.

Kuncheva, L. I. and Whitaker, C. J. (2003). Measures of diversity in classifier ensembles and their
relationship with the ensemble accuracy. Mach. Learn., 51(2):181–207.

LeDell, E. and Poirier, S. (2020). H2O AutoML: Scalable automatic machine learning. 7th ICML
Workshop on Automated Machine Learning (AutoML).

Lehman, J. and Stanley, K. O. (2011). Abandoning objectives: Evolution through the search for
novelty alone. Evolutionary Computation, 19(2):189–223.

Li, N., Yu, Y., and Zhou, Z. (2012). Diversity regularized ensemble pruning. In Machine Learning
and Knowledge Discovery in Databases - European Conference, ECML PKDD 2012, Proceedings, Part
I, volume 7523 of Lecture Notes in Computer Science, pages 330–345. Springer.

Li, R., Emmerich, M. T. M., Eggermont, J., Bäck, T., Schütz, M., Dijkstra, J., and Reiber, J. H. C.
(2013). Mixed integer evolution strategies for parameter optimization. Evolutionary Computation,
21(1):29–64.

Lindauer, M., van Rijn, J. N., and Kotthoff, L. (2019). The algorithm selection competitions 2015 and
2017. Artificial Intelligence, 272:86–100.

Martínez-Munoz, G. and Suárez, A. (2004). Aggregation ordering in bagging. In Proc. of the IASTED
International Conference on Artificial Intelligence and Applications, pages 258–263. Citeseer.

Mendoza, H., Klein, A., Feurer, M., Springenberg, J. T., Urban, M., Burkart, M., Dippel, M., Lindauer,
M., and Hutter, F. (2018). Towards automatically-tuned deep neural networks. In AutoML:
Methods, Sytems, Challenges, chapter 7, pages 141–156. Springer.

Miller, B. L. and Goldberg, D. E. (1995). Genetic algorithms, tournament selection, and the effects of
noise. Complex systems, 9(3).

Mouret, J. and Clune, J. (2015). Illuminating search spaces by mapping elites. CoRR, abs/1504.04909.

Nguyen, A. M., Yosinski, J., and Clune, J. (2015). Innovation engines: Automated creativity and
improved stochastic optimization via deep learning. In Proceedings of the Genetic and Evolutionary
Computation Conference, GECCO 2015, pages 959–966. ACM.

Nickerson, K. L. and Hu, T. (2021). Principled quality diversity for ensemble classifiers using
map-elites. In GECCO '21: Genetic and Evolutionary Computation Conference, Companion Volume,
pages 259–260. ACM.

13

--- TRANG 14 ---
Olson, R. S., Bartley, N., Urbanowicz, R. J., and Moore, J. H. (2016). Evaluation of a tree-based
pipeline optimization tool for automating data science. In Proceedings of the 2016 on Genetic and
Evolutionary Computation Conference, pages 485–492. ACM.

Onan, A., Korukoglu, S., and Bulut, H. (2017). A hybrid ensemble pruning approach based on
consensus clustering and multi-objective evolutionary algorithm for sentiment classification.
Information Processing & Management, 53(4):814–833.

Partalas, I., Tsoumakas, G., and Vlahavas, I. P. (2010). An ensemble uncertainty aware measure for
directed hill climbing ensemble pruning. Machine Learning, 81(3):257–282.

Partridge, D. and Yates, W. B. (1996). Engineering multiversion neural-net systems. Neural
Computation, 8(4):869–893.

Płońska, A. and Płoński, P. (2021). Mljar: State-of-the-art automated machine learning framework
for tabular data. version 0.10.3.

Pugh, J. K., Soros, L. B., and Stanley, K. O. (2016). Searching for quality diversity when diversity is
unaligned with quality. In Parallel Problem Solving from Nature - PPSN XIV - 14th International
Conference, volume 9921 of Lecture Notes in Computer Science, pages 880–889. Springer.

Purucker, L. and Beel, J. (2022). Assembled-OpenML: Creating efficient benchmarks for ensembles
in AutoML with OpenML. In First International Conference on Automated Machine Learning
(Late-Breaking Workshop).

Qin, A. K. and Suganthan, P. N. (2005). Self-adaptive differential evolution algorithm for numerical
optimization. In Proceedings of the IEEE Congress on Evolutionary Computation, CEC, pages
1785–1791. IEEE.

Sagi, O. and Rokach, L. (2018). Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data
Mining and Knowledge Discovery, 8(4).

Tang, E. K., Suganthan, P. N., and Yao, X. (2006). An analysis of diversity measures. Machine
Learning, 65(1):247–271.

Tjanaka, B., Fontaine, M. C., Lee, D. H., Zhang, Y., Vu, T. T. M., Sommerer, S., Dennler, N., and
Nikolaidis, S. (2021). pyribs: A bare-bones python library for quality diversity optimization.
https://github.com/icaros-usc/pyribs.

Tsoumakas, G., Partalas, I., and Vlahavas, I. P. (2009). An ensemble pruning primer. In Applications
of Supervised and Unsupervised Ensemble Methods, volume 245 of Studies in Computational
Intelligence, pages 1–13. Springer.

Tumer, K. and Ghosh, J. (1999). Linear and order statistics combiners for pattern classification.
CoRR, cs.NE/9905012.

Vakhrushev, A., Ryzhkov, A., Savchenko, M., Simakov, D., Damdinov, R., and Tuzhilin, A. (2021).
Lightautoml: Automl solution for a large financial services ecosystem. CoRR, abs/2109.01528.

Vanschoren, J., van Rijn, J. N., Bischl, B., and Torgo, L. (2013). Openml: networked science in
machine learning. ACM SIGKDD Explorations Newsletter, 15(2):49–60.

Wang, C., Wu, Q., Weimer, M., and Zhu, E. (2021). FLAML: A fast and lightweight automl library.
In Proceedings of Machine Learning and Systems 2021. mlsys.org.

14

--- TRANG 15 ---
Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2):241–259.

Wood, D., Mu, T., Webb, A. M., Reeve, H. W. J., Luján, M., and Brown, G. (2023). A unified theory of
diversity in ensemble learning. CoRR, abs/2301.03962.

Zhou, Z. and Tang, W. (2003). Selective ensemble of decision trees. In Rough Sets, Fuzzy Sets,
Data Mining, and Granular Computing, 9th International Conference, Proceedings, volume 2639 of
Lecture Notes in Computer Science, pages 476–483. Springer.

Zhou, Z., Wu, J., and Tang, W. (2002). Ensembling neural networks: Many could be better than all.
Artificial intelligence, 137(1-2):239–263.

Zimmer, L., Lindauer, M., and Hutter, F. (2021). Auto-pytorch tabular: Multi-fidelity metalearning
for efficient and robust autodl. IEEE Transactions on Pattern Analysis and Machine Intelligence,
pages 3079 – 3090. also available under https://arxiv.org/abs/2006.13799.

A Danh sách Kiểm tra Nộp bài

1. Đối với tất cả các tác giả...
(a) Các tuyên bố chính được đưa ra trong tóm tắt và giới thiệu có phản ánh chính xác đóng góp và phạm vi của bài báo không? [Có] Chúng tôi nêu trong tóm tắt và giới thiệu rằng chúng tôi so sánh
GES với QO-ES và QDO-ES, đó là những gì chúng tôi đã làm trong bài báo.
(b) Bạn có mô tả các hạn chế của công trình mình không? [Có] Trong Phụ lục, xem B.
(c) Bạn có thảo luận về bất kỳ tác động tiêu cực tiềm tàng nào của công trình mình đến xã hội không? [Có] Trong Phụ lục,
xem C.
(d) Bạn có đọc các hướng dẫn đạo đức cho tác giả và bình duyệt và đảm bảo rằng bài báo của bạn
tuân thủ chúng không? https://2023.automl.cc/ethics/ [Có] Chúng tôi tin rằng bài báo của chúng tôi tuân thủ
chúng.

2. Nếu bạn bao gồm kết quả lý thuyết...
(a) Bạn có nêu tập hợp đầy đủ các giả định của tất cả kết quả lý thuyết không? [N/A] Chúng tôi không bao gồm
kết quả lý thuyết nào.
(b) Bạn có bao gồm bằng chứng hoàn chính của tất cả kết quả lý thuyết không? [N/A] Chúng tôi không bao gồm
kết quả lý thuyết nào.

3. Nếu bạn chạy thực nghiệm...
(a) Bạn có bao gồm mã, dữ liệu, và hướng dẫn cần thiết để tái tạo các kết quả thực nghiệm chính,
bao gồm tất cả yêu cầu (ví dụ: requirements.txt với phiên bản rõ ràng), một
README hướng dẫn với cài đặt, và lệnh thực thi (trong tài liệu bổ sung
hoặc dưới dạng url) không? [Có] Xem kho lưu trữ của chúng tôi được đề cập trong phần giới thiệu cho tất cả chi tiết.
(b) Bạn có bao gồm kết quả thô của việc chạy các hướng dẫn đã cho trên mã và
dữ liệu đã cho không? [Có] Xem kho lưu trữ của chúng tôi được đề cập trong phần giới thiệu cho kết quả.
(c) Bạn có bao gồm các script và lệnh có thể được sử dụng để tạo ra các hình và bảng
trong bài báo của bạn dựa trên kết quả thô của mã, dữ liệu, và hướng dẫn đã cho không? [Có] Xem
kho lưu trữ của chúng tôi được đề cập trong phần giới thiệu.

15

--- TRANG 16 ---
(d) Bạn có đảm bảo chất lượng mã đủ để mã của bạn có thể được thực thi an toàn và
mã được ghi chép đúng cách không? [Có] Chúng tôi tin rằng chất lượng mã và tài liệu
của chúng tôi là đủ.
(e) Bạn có chỉ định tất cả chi tiết huấn luyện (ví dụ: phân chia dữ liệu, tiền xử lý, không gian tìm kiếm, cài đặt
siêu tham số cố định, và cách chúng được chọn) không? [Có] Xem Phần 5 và
Phụ lục để biết chi tiết. Ngoài ra, xem kho lưu trữ của chúng tôi được đề cập trong phần giới thiệu.
(f) Bạn có đảm bảo rằng bạn so sánh các phương pháp khác nhau (bao gồm cả của bạn) chính xác trên
cùng điểm chuẩn, bao gồm cùng bộ dữ liệu, không gian tìm kiếm, mã cho huấn luyện và
siêu tham số cho mã đó không? [Có] Chúng tôi chạy tất cả phương pháp trên cùng dữ liệu.
(g) Bạn có chạy nghiên cứu ablation để đánh giá tác động của các thành phần khác nhau trong cách tiếp cận của bạn không?
[Có] Chúng tôi chạy cách tiếp cận của chúng tôi cho một lưới siêu tham số và đánh giá tầm quan trọng của chúng,
xem Phụ lục E.2 và F.3.
(h) Bạn có sử dụng cùng giao thức đánh giá cho các phương pháp được so sánh không? [Có] Chúng tôi chạy
tất cả phương pháp trên cùng dữ liệu với cùng giao thức đánh giá và mã.
(i) Bạn có so sánh hiệu suất theo thời gian không? [Không] Chúng tôi so sánh hiệu suất cho một thời điểm
cụ thể (sau 50 lần lặp của GES, tức là sau 𝑚∗50 đánh giá hàm). Hiệu suất
theo thời gian nằm ngoài phạm vi cho thực nghiệm của chúng tôi. Theo ý kiến của chúng tôi, hiệu suất bất cứ lúc nào ít
liên quan hơn nhiều cho ensemble hậu hoc so với các phương pháp AutoML khác như
HPO hoặc NAS.
(j) Bạn có thực hiện nhiều lần chạy thực nghiệm và báo cáo seed ngẫu nhiên không? [Có] Có,
chúng tôi đã sử dụng xác thực chéo 10-fold cho tất cả các lần chạy của chúng tôi. Các seed ngẫu nhiên được sử dụng có thể được tìm thấy trong
mã của chúng tôi.
(k) Bạn có báo cáo thanh lỗi (ví dụ: đối với seed ngẫu nhiên sau khi chạy thực nghiệm
nhiều lần) không? [Không] Chúng tôi lấy trung bình trên 10 folds làm điểm số theo công trình trước đây
và không báo cáo phương sai trên các folds. Tuy nhiên, boxplots cho phép đánh giá
phương sai hiệu suất đối với các bộ dữ liệu khác nhau.
(l) Bạn có sử dụng điểm chuẩn dạng bảng hoặc surrogate cho đánh giá chuyên sâu không? [N/A] Các điểm chuẩn như vậy
không có sẵn cho trường hợp sử dụng của chúng tôi.
(m) Bạn có bao gồm tổng lượng tính toán và loại tài nguyên được sử dụng (ví dụ: loại
GPUs, cụm nội bộ, hoặc nhà cung cấp đám mây) không? [Có] Xem Phần 5.
(n) Bạn có báo cáo cách bạn điều chỉnh siêu tham số, và thời gian và tài nguyên này yêu cầu
(nếu chúng không được điều chỉnh tự động bởi phương pháp AutoML của bạn, ví dụ: trong cách tiếp cận NAS; và
cũng siêu tham số của phương pháp riêng của bạn) không? [Có] Xem Phần 5.

4. Nếu bạn đang sử dụng tài sản hiện có (ví dụ: mã, dữ liệu, mô hình) hoặc quản lý/phát hành tài sản mới...
(a) Nếu công trình của bạn sử dụng tài sản hiện có, bạn có trích dẫn người tạo ra chúng không? [Có] Xem Phần 5 và Phụ lục
G.
(b) Bạn có đề cập đến giấy phép của các tài sản không? [Có] Xem Phụ lục G.
(c) Bạn có bao gồm bất kỳ tài sản mới nào trong tài liệu bổ sung hoặc dưới dạng url không? [Có] Xem
kho lưu trữ của chúng tôi được đề cập trong phần giới thiệu.
(d) Bạn có thảo luận về việc liệu và làm thế nào để có được sự đồng ý từ những người mà dữ liệu của họ bạn đang
sử dụng/quản lý không? [N/A] Chúng tôi chỉ sử dụng dữ liệu có sẵn công khai đã được sử dụng trước đây trong
điểm chuẩn.

16

--- TRANG 17 ---
(e) Bạn có thảo luận về việc liệu dữ liệu bạn đang sử dụng/quản lý có chứa thông tin có thể nhận dạng cá nhân
hoặc nội dung xúc phạm không? [N/A] Chúng tôi tin rằng dữ liệu chúng tôi đang sử dụng không
chứa thông tin có thể nhận dạng cá nhân hoặc nội dung xúc phạm.

5. Nếu bạn sử dụng crowdsourcing hoặc tiến hành nghiên cứu với đối tượng con người...
(a) Bạn có bao gồm văn bản đầy đủ của các hướng dẫn được đưa cho người tham gia và ảnh chụp màn hình, nếu có
không? [N/A] Chúng tôi không sử dụng crowdsourcing hoặc tiến hành nghiên cứu với đối tượng con người.
(b) Bạn có mô tả bất kỳ rủi ro tiềm tàng nào của người tham gia, với liên kết đến phê duyệt của Hội đồng Đánh giá Thể chế
(IRB), nếu có không? [N/A] Chúng tôi không sử dụng crowdsourcing hoặc tiến hành nghiên cứu
với đối tượng con người.
(c) Bạn có bao gồm mức lương theo giờ ước tính được trả cho người tham gia và tổng số tiền chi tiêu
cho bồi thường người tham gia không? [N/A] Chúng tôi không sử dụng crowdsourcing hoặc tiến hành nghiên cứu
với đối tượng con người.

B Hạn chế

Chúng tôi lưu ý rằng công trình của chúng tôi bị hạn chế đối với các điểm sau: 1) chúng tôi thấy rằng overfitting là
một vấn đề lớn nhưng chỉ có thể so sánh các phương pháp của chúng tôi cho Auto-Sklearn 1 và không có thêm
các hệ thống AutoML khác có dữ liệu xác thực tốt hơn, như Auto-Sklearn 2 hoặc AutoGluon; 2)
chúng tôi chỉ có thể khám phá một tập con của tất cả các siêu tham số có thể và cài đặt siêu tham số
cho các phương pháp mới được đề xuất của chúng tôi; 3) chúng tôi chỉ có thể đánh giá w.r.t. hai chỉ số; và 4) chúng tôi
không thể khám phá tất cả các biến thể tiềm năng của QDO có thể đã được sử dụng cho QDO-ES.

C Tuyên bố Tác động Rộng hơn

Chúng tôi tin rằng công trình của chúng tôi chủ yếu là trừu tượng và có tính phương pháp. Do đó, sau khi suy nghĩ cẩn thận, chúng tôi
xác định rằng công trình này không có tác động tiêu cực đáng chú ý hoặc mới nào đến xã hội hoặc môi trường
mà chưa có sẵn cho các hệ thống AutoML hiện đại hiện tại. Chúng tôi đề xuất thay thế
một thành phần của hệ thống AutoML sao cho hiệu suất dự đoán cải thiện trong khi hiệu quả
vẫn gần như giống nhau. Do đó, chúng tôi chỉ thấy tác động tích cực tiềm tàng mà hiệu suất dự đoán cao hơn
có thể giúp đưa ra quyết định tốt hơn bằng cách sử dụng AutoML.

D Phụ lục cho Mô tả Thuật toán

D.1 Công thức cho Các chỉ số Đa dạng

Để đo đa dạng của một ensemble 𝐸=(𝑃,𝑟), chúng tôi xem xét tất cả các mô hình cơ sở có trọng số khác không
trong ensemble, tức là 𝑃′={𝑝ᵢ∈𝑃|𝑟(𝑖)>0} với |𝑃′|=𝑚′.

Tương quan mất mát trung bình (ALC) được định nghĩa cho 𝑃′ và một tập hợp các vector mất mát 𝐿={𝑙₁,...,𝑙ₘ′}. Một
vector mất mát 𝑙ᵢ tương ứng với sự khác biệt giữa 1 và xác suất dự đoán của 𝑝ᵢ cho
lớp đúng cho mỗi thể hiện. Sau đó, ALC được định nghĩa là tương quan Pearson trung bình 𝜌 của hai
vector mất mát trên tất cả các cặp mô hình trong 𝑃′:

𝐴𝐿𝐶=2/(𝑚′(𝑚′−1)) ∑ᵢ₌₁^(𝑚′−1) ∑ⱼ₌ᵢ₊₁^𝑚′ 𝜌(𝑙ᵢ,𝑙ⱼ). (2)

Tương tự không gian cấu hình (CSS) được định nghĩa cho 𝑃′ và một tập hợp các cấu hình 𝐶=
{𝑐₁,...,𝑐ₘ′} trong đó 𝑐ᵢ là cấu hình của 𝑝ᵢ. Chúng tôi giả định rằng các cấu hình đến từ
cùng một không gian cấu hình sao cho ít nhất siêu tham số mô tả thuật toán được sử dụng
tồn tại cho bất kỳ hai 𝑐 nào. Hơn nữa, chúng tôi giả định rằng chúng ta được cho các phạm vi quan sát tối đa trong 𝐶

17

--- TRANG 18 ---
của tất cả các siêu tham số số học. Đối với một siêu tham số số học ℎ, chúng tôi ký hiệu phạm vi của nó bằng
𝑅ₕ. Nói chung, chúng tôi ký hiệu giá trị của một cấu hình 𝑐 cho một siêu tham số ℎ bằng ℎ_𝑐. CSS là
khoảng cách Gower trung bình (Gower, 1971) trên tất cả các cặp mô hình trong 𝑃′:

𝐶𝑆𝑆=2/(𝑚′(𝑚′−1)) ∑ᵢ₌₁^(𝑚′−1) ∑ⱼ₌ᵢ₊₁^𝑚′ (1−𝐺ᵢ,ⱼ). (3)

Tương tự Gower 𝐺ᵢ,ⱼ chỉ được định nghĩa cho tập hợp các siêu tham số 𝐾={ℎ₁,...,ℎ|𝐾|}
xuất hiện trong 𝑐ᵢ và 𝑐ⱼ. Hơn nữa, tương tự Gower phân biệt giữa siêu tham số phân loại và
số học và là trung bình trên tương tự của mỗi siêu tham số trong 𝐾:

𝐺ᵢ,ⱼ=1/|𝐾| ∑ₖ₌₁^|𝐾| (𝐺cat_{ᵢ,ⱼ,ₖ}, nếu ℎₖ là phân loại; 𝐺num_{ᵢ,ⱼ,ₖ}, ngược lại), (4)

với
𝐺cat_{ᵢ,ⱼ,ₖ} = (1, nếu ℎ_𝑐ᵢ_ₖ = ℎ_𝑐ⱼ_ₖ; 0, ngược lại), (5)

và,
𝐺num_{ᵢ,ⱼ,ₖ} = |ℎ_𝑐ᵢ_ₖ − ℎ_𝑐ⱼ_ₖ| / 𝑅ₕₖ. (6)

Vì ít nhất siêu tham số mô tả thuật toán được sử dụng tồn tại cho cả hai cấu hình,
𝐺ᵢ,ₖ≥0. Hơn nữa, nó nhiều nhất là 1 nếu tất cả siêu tham số giống hệt nhau.

D.2 Động lực để sử dụng Archive Ranh giới Trượt cho QDO-ES

Đối với QDO-ES, chúng tôi đã triển khai một archive ranh giới trượt (Fontaine et al., 2019). Một archive ranh giới trượt
thường xuyên tính toán lại các ranh giới niche dựa trên phạm vi quan sát của các giá trị hành vi
sau khi khởi tạo các niches dựa trên phạm vi lý thuyết.

Chúng tôi sử dụng archive ranh giới trượt vì chúng tôi không thể đảm bảo rằng các giá trị có thể quan sát của
một không gian hành vi cho đa dạng ensemble được phân phối đều, như thường được giả định cho QDO
(Chatzilygeroudis et al., 2021; Nickerson and Hu, 2021). Hơn nữa, phạm vi quan sát có thể của các chỉ số đa dạng ensemble
khác nhau giữa các bộ dữ liệu vì chúng phụ thuộc, ví dụ: vào dự đoán của các mô hình cơ sở.
Archive ranh giới trượt cho phép chúng tôi duy trì một độ chi tiết tương tự của các phân vùng giữa
các bộ dữ liệu. Độ chi tiết cũng sẽ mịn hơn các phân vùng được tính toán ban đầu và do đó cho phép
nhiều cạnh tranh cục bộ hơn.

Ngoài ra và quan trọng hơn, vì chúng tôi giữ một ensemble trên mỗi niche, archive ranh giới trượt
căn chỉnh quần thể với phân phối cơ bản của không gian hành vi quan sát được,
làm cho lấy mẫu ngẫu nhiên đại diện hơn.

D.3 Biến thể được sử dụng của Lựa chọn Giải đấu

Trong biến thể không xác định của lựa chọn giải đấu, chúng tôi định nghĩa kích thước giải đấu 𝑇 dựa trên
số lượng giải pháp chúng tôi muốn lấy mẫu. Nếu chúng tôi muốn lấy mẫu một giải pháp, chúng tôi lấy mẫu ngẫu nhiên
10 giải pháp từ archive. Chúng tôi lấy mẫu 20 giải pháp nếu chúng tôi muốn lấy mẫu hai giải pháp.
Trong vài lần lặp đầu tiên, chúng tôi có thể có ít hơn 𝑇 giải pháp trong archive. Trong trường hợp như vậy, chúng tôi
lấy tất cả giải pháp trong archive và thêm đột biến các giải pháp, được lấy từ tập hợp ban đầu của
các giải pháp được đề xuất cho archive, với Thuật toán 3.

Cho 𝑇-nhiều giải pháp, chúng tôi gán cho mỗi giải pháp một xác suất được lấy mẫu. Giải pháp có hiệu suất tốt nhất
được gán xác suất 0.8, tốt thứ hai 0.8∗0.2¹, tốt thứ ba 0.8∗0.2², 
và tiếp tục cho đến 0.8∗0.2^(𝑇−1). Cuối cùng, chúng tôi lấy mẫu một hoặc hai giải pháp với các xác suất này.
Nếu chúng tôi lấy mẫu hai giải pháp, chúng tôi lấy mẫu không thay thế.

18

--- TRANG 19 ---
D.4 Triển khai của Xác suất Thích ứng

Chúng tôi có một số thành phần trong thuật toán của mình trong đó một xác suất định nghĩa liệu chúng có được
kích hoạt hay hủy kích hoạt hoặc cách chúng được cấu hình. Trong tất cả các trường hợp, chúng tôi đã sử dụng xác suất tự thích ứng
thay vì một giá trị hằng số hoặc hàm được xác định trước theo thời gian. Do đó, tránh các siêu tham số bổ sung
và thiên kiến của việc chọn một giá trị hằng số hoặc hàm theo thời gian cho tất cả nhiệm vụ. Điều này bao gồm:
lấy mẫu động; liệu lai ghép có được sử dụng hay không; và liệu đột biến có được sử dụng sau lai ghép hay không.

Chúng tôi có thể biểu diễn lựa chọn như vậy như một quyết định nhị phân giữa việc sử dụng 𝑎 hoặc 𝑏. Ban đầu, xác suất
của việc sử dụng 𝑎, 𝑃𝑟(𝑎), được đặt thành 50% (do đó 𝑃𝑟(𝑏)=1−𝑃𝑟(𝑎)). Tức là, chúng tôi chọn ngẫu nhiên
liệu sử dụng 𝑎 hay 𝑏. Sau đó, sau mỗi lần lặp, xác suất được thích ứng dựa trên hiệu suất quan sát được
của 𝑎 và 𝑏 trong 𝑓 lần lặp cuối cùng. 𝑓 là một siêu tham số. Chúng tôi đặt 𝑓=10 vì chúng tôi
nhắm đến một cửa sổ trên hiệu suất sao cho hiệu suất kém trong các lần lặp trước đó có thể được
quên đi trước những thay đổi động trong hiệu suất của 𝑎 và 𝑏. Chúng tôi theo dõi hiệu suất của
các giải pháp liên quan đến nguồn gốc của chúng, tức là liệu các giải pháp có được tạo ra với 𝑎 hay 𝑏. Hơn nữa,
chúng tôi không thay đổi xác suất nếu ít hơn 2 giải pháp được tạo ra với 𝑎 hoặc 𝑏.

Để cập nhật, trước tiên chúng tôi tính toán hiệu suất trung bình trên tất cả các giải pháp được thấy trong 10
lần lặp cuối cùng cho 𝑎 và 𝑏, được ký hiệu là 𝑎_{avg} và 𝑏_{avg}. Sử dụng các giá trị này, chúng tôi tính toán: 𝑃𝑟(𝑎)=1−𝑎_{avg}/(𝑎_{avg}+𝑏_{avg}).

Cuối cùng, chúng tôi giới hạn 𝑃𝑟(𝑎) trong [0.05,0.95] cho lấy mẫu động và trong [0.1,0.9] ngoài ra. Chúng tôi áp dụng
các giới hạn này để tránh đạt xác suất 0 cho 𝑎 hoặc 𝑏 sao cho nó sẽ không được chọn
nữa và do đó không có giải pháp cho một bản cập nhật.

Chúng tôi có giới hạn chặt hơn cho đột biến sau lai ghép và lai ghép, vì chúng tôi quan sát thấy rằng một
xác suất 0.05 hoạt động tương tự như xác suất 0 – không đủ giải pháp được tạo ra sao cho
các bản cập nhật trong tương lai có thể thay đổi xác suất. Điều này chỉ xảy ra cho đột biến sau lai ghép
và lai ghép, vì cả hai chỉ được sử dụng cho một tập con của tất cả giải pháp. Ngược lại, lấy mẫu động luôn
được sử dụng như bước ban đầu để tạo ra một giải pháp mới (xem Thuật toán 2); giả định lấy mẫu động
được sử dụng và không phải phương pháp lấy mẫu khác.

D.5 Toán tử Đột biến

Toán tử đột biến của chúng tôi tuân theo ý tưởng của GES (Thuật toán 1), theo nghĩa là chúng tôi điều chỉnh 𝑟 trong
đột biến bằng cách tăng một trong các phần tử của nó. Chúng tôi giới thiệu tính ngẫu nhiên bổ sung bằng cách chọn ngẫu nhiên
phần tử này. Do đó, chúng tôi đã triển khai một phiên bản ngẫu nhiên hóa của bản cập nhật GES như
một toán tử đột biến, xem Thuật toán 3. Tuy nhiên, vì 𝐸 không được xây dựng lặp đi lặp lại trong Q(D)O-ES, chúng tôi
cần theo dõi mô hình cơ sở nào chúng tôi đã thêm trước đó để tránh tạo ra cùng một ensemble hai lần
(được xử lý bởi dòng 2, 5, và 6).

Thuật toán 3 Đột biến Giống như Thay thế Ngẫu nhiên hóa Độc lập Lần lặp
Đầu vào: Ensemble 𝐸=(𝑃,𝑟)
Đầu ra: Ensemble 𝐸_{mutated}=(𝑃,𝑟_{mutated})
1: 𝑅←{𝑟′|𝑟′=𝑟 với một phần tử tăng thêm 1} ⊲ Có được tất cả các mở rộng có thể của 𝑟
2: 𝑅_{seen}←𝑙𝑜𝑜𝑘𝑈𝑝𝑆𝑒𝑒𝑛𝐹𝑜𝑟(𝑟) ⊲ Có được các mở rộng đã tạo ra trước đó
3: 𝑅_{potential}←𝑅\𝑅_{seen}
4: 𝑟_{mutated}←𝑟𝑎𝑛𝑑𝑜𝑚𝑆𝑎𝑚𝑝𝑙𝑒(𝑅_{potential})
5: 𝑅_{seen}←𝑅_{seen}∪𝑟_{mutated} ⊲ Cập nhật các mở rộng đã tạo ra trước đó
6: 𝑠𝑡𝑜𝑟𝑒𝐹𝑜𝑟𝐿𝑜𝑜𝑘𝑈𝑝(𝑟,𝑅_{seen});𝑠𝑡𝑜𝑟𝑒𝐹𝑜𝑟𝐿𝑜𝑜𝑘𝑈𝑝(𝑟_{mutated},∅) ⊲ Lưu trữ các mở rộng đã tạo ra trước đó
7: return (𝑃,𝑟_{mutated})

D.6 Lai ghép Hai điểm của Các Vector Lặp lại

Đối với lai ghép hai điểm của hai vector lặp lại 𝑟 và 𝑟′, trước tiên chúng tôi chọn các tập con 𝑟_{sub} và 𝑟′_{sub}
của các phần tử khác không trong 𝑟 hoặc 𝑟′. Nếu tập con này nhỏ hơn ba, chúng tôi quay lại
lai ghép trung bình. Chúng tôi yêu cầu một vector có ít nhất độ dài ba để thực hiện lai ghép hai điểm.
Ngược lại, chỉ có một khả năng để thực hiện lai ghép hai điểm sẽ tồn tại.

19

--- TRANG 20 ---
Tiếp theo, chúng tôi chọn hai điểm riêng biệt ngẫu nhiên và lai ghép 𝑟_{sub} và 𝑣′_{sub} với các điểm này.
Sau đó, chúng tôi điền 𝑟/𝑟′ với các phần tử của 𝑟_{sub}/𝑟′_{sub} tạo ra 𝑟_{co}/𝑟′_{co}. Cuối cùng, chúng tôi xác minh rằng
các phần tử của 𝑟_{co}/𝑟′_{co} không phải tất cả đều bằng không và trả về 𝑟_{co}/𝑟′_{co}. Nếu cả 𝑟_{co} và 𝑟′_{co} đều không chứa
các phần tử khác không, thì chúng tôi lại quay lại lai ghép trung bình để tạo ra con cái.

Lai ghép hai điểm có thể tạo ra hai con cái duy nhất. Nếu điều này xảy ra, chúng tôi thêm cả hai con cái (đã đột biến)
vào lô trong QO-ES và QDO-ES (Thuật toán 2).

D.7 Các cách tiếp cận Khởi tạo

Tìm kiếm dựa trên quần thể yêu cầu một quần thể ban đầu 𝑆. Để tạo ra quần thể ban đầu cho
Q(D)O-ES, chúng tôi đã triển khai ba cách tiếp cận: tất cả ensemble chỉ bao gồm một mô hình cơ sở, tất cả
ensemble có kích thước 2 bao gồm mô hình cơ sở tốt nhất, hoặc 𝑚-nhiều ensemble ngẫu nhiên có kích thước 2. Chính thức,
tập hợp tất cả ensemble chỉ bao gồm một mô hình cơ sở được cho bởi

𝑆₁={(𝑃,𝑟)|∑ᵢ₌₁ᵐ 𝑟(𝑖)=1}. (7)

Tập hợp tất cả ensemble có kích thước 2 bao gồm mô hình cơ sở tốt nhất với 𝑗 là chỉ số của mô hình tốt nhất đơn lẻ,
được cho bởi

𝑆₂={(𝑃,𝑟)|𝑟(𝑗)=1∧∑ᵢ₌₁ᵐ 𝑟(𝑖)=2}. (8)

Mô hình tốt nhất đơn lẻ là mô hình cơ sở trong 𝑃 với điểm xác thực cao nhất. Để xây dựng tập hợp
𝑆₃ của 𝑚-nhiều ensemble ngẫu nhiên có kích thước 2, chúng tôi chọn ngẫu nhiên hai thành viên của ensemble 𝑚
lần. Chính thức, 𝑆₃⊂𝑆̂₃ với |𝑆₃|=𝑚 và 𝑆̂₃ là tập hợp tất cả ensemble có thể có kích thước 2:

𝑆̂₃={(𝑃,𝑟)|∃!𝑖,𝑗∈{1,...,𝑚},𝑖≠𝑗:𝑟(𝑖)=1=𝑟(𝑗)}. (9)

Lưu ý, kích thước của quần thể ban đầu này khác nhau giữa các phương pháp theo định nghĩa; 𝑆₁=𝑆₃=𝑚
trong khi 𝑆₂=𝑚−1.

D.8 Phanh Khẩn cấp cho Lấy mẫu Từ chối

Chúng tôi đã thêm một phanh khẩn cấp tạm thời thay đổi việc tăng trong đột biến (Thuật toán
3, Dòng 1) và/hoặc xác suất lai ghép khi hơn 50 giải pháp bị từ chối trong một lần lặp.
Chúng tôi được thúc đẩy làm điều này vì chúng tôi quan sát một trường hợp cạnh nơi Thuật toán 2 không thể
đề xuất bất kỳ giải pháp chưa thấy nào. Do đó, thuật toán rơi vào vòng lặp vô tận của lấy mẫu từ chối.
Chúng tôi quan sát điều này trong vài lần lặp đầu tiên của thuật toán sau khi archive được khởi tạo với
ensemble chỉ bao gồm một mô hình cơ sở, hoặc khi xác suất lai ghép rất cao trong
vài lần lặp đầu tiên.

Phanh khẩn cấp tổng thể của chúng tôi bao gồm hai phần, một phanh cho việc từ chối do
đột biến và một cho việc từ chối do lai ghép. Trong một lần lặp, cả hai phanh có thể được
kích hoạt (nhiều) lần cùng một lúc nếu thuật toán vẫn không tạo ra giải pháp chưa thấy.

Nếu đột biến không tạo ra bất kỳ giải pháp mới nào và lai ghép bị hủy kích hoạt (hoặc cũng không
tạo ra giải pháp mới), thì phanh khẩn cấp đột biến được kích hoạt sau 50 lần từ chối
do đột biến. Khi phanh được kích hoạt, việc tăng của các vector lặp lại trong
đột biến (Thuật toán 3, Dòng 1) được tăng thêm 1 cho lần lặp hiện tại.

Việc từ chối cũng có thể do các giải pháp được tạo ra bởi lai ghép và không phải bởi đột biến
vì lai ghép không thể tạo ra con cái chưa thấy, và xác suất thích ứng của đột biến sau
lai ghép quá nhỏ để được gọi trong lần lặp hiện tại. Trong trường hợp này, chúng tôi tăng xác suất
của đột biến sau lai ghép lên 100% cho lần lặp này sau 50 lần từ chối do lai ghép.

20

--- TRANG 21 ---
E Phụ lục cho Thực nghiệm

E.1 Tổng quan Bộ dữ liệu

Bảng 2 đưa ra tổng quan về các bộ dữ liệu và mô hình cơ sở được sử dụng trong thực nghiệm của chúng tôi. Ngoài ra, chúng tôi
hiển thị trung bình (trên các folds) cho số lượng mô hình cơ sở và số lượng thuật toán riêng biệt
trên các pools mô hình cơ sở cho độ chính xác cân bằng BA và ROC AUC R, tương ứng cho các
phương pháp tiền xử lý SiloTopN và TopN.

E.2 Không gian Cấu hình

Bảng 3 hiển thị tất cả siêu tham số và các giá trị có thể của chúng cho không gian cấu hình của QO-ES
và QDO-ES. Chúng tôi gọi tập hợp tất cả ensemble chỉ bao gồm một mô hình cơ sở đơn lẻ là L1
Ensembles, trong khi chúng tôi gọi tất cả ensemble có kích thước 2 bao gồm mô hình tốt nhất đơn lẻ, hoặc 𝑚-nhiều
ensemble ngẫu nhiên có kích thước 2 là L2 Ensembles.

E.3 Thảo luận về Cách tiếp cận Lựa chọn Cấu hình

Chúng tôi sử dụng cải tiến chuẩn hóa trung vị để chọn cấu hình đại diện nhất trong
xác thực chéo leave-one-out vì nó phản ánh những gì chúng tôi tin là yếu tố phân biệt
giữa các phương pháp khác nhau dựa trên đánh giá theo điểm chuẩn AutoML (AMLB)
(Gijsbers et al., 2022) – trung vị cao nhất trong boxplots của cải tiến chuẩn hóa.

Chúng tôi nhận thức rằng các cách tiếp cận lựa chọn thay thế có thể thay đổi kết quả của chúng tôi hoặc người ta có thể,
về nguyên tắc, học để chọn cấu hình tốt nhất. Tuy nhiên, lựa chọn thay thế duy nhất hợp lệ trong một đánh giá
theo AMLB sẽ là xếp hạng trung bình. Vì đây là yếu tố phân biệt duy nhất khác
được sử dụng trong các biểu đồ, cụ thể là các biểu đồ sự khác biệt quan trọng. Tuy nhiên, như đã lưu ý trong AMLB và
có thể thấy trong kết quả của nó, xếp hạng tuyệt đối không đại diện cho phân phối hiệu suất và che giấu
sự khác biệt hiệu suất tương đối. Ngược lại, cải tiến chuẩn hóa trung vị dựa trên một
thước đo tương đối và liên quan đến phân phối hiệu suất thông qua trung vị.

E.4 Cải tiến Chuẩn hóa

Việc triển khai cải tiến chuẩn hóa của chúng tôi tuân theo điểm chuẩn AutoML (Gijsbers et al.,
2022). Tức là, chúng tôi chuẩn hóa các điểm số cho một bộ dữ liệu sao cho −1 bằng với điểm số của mô hình tốt nhất đơn lẻ,
và 0 bằng với điểm số của phương pháp tốt nhất trên bộ dữ liệu.

Chính thức, chúng tôi chuẩn hóa điểm số 𝑠_𝐷 của một phương pháp cho bộ dữ liệu 𝐷 sử dụng:

(𝑠_𝐷−𝑠_𝐷^𝑏)/(𝑠_𝐷^*−𝑠_𝐷^𝑏)−1, (10)

với điểm số của baseline 𝑠_𝐷^𝑏 và điểm số tốt nhất quan sát được cho bộ dữ liệu 𝑠_𝐷^*. Chúng tôi giả định rằng
điểm số cao hơn luôn tốt hơn.

Chúng tôi mở rộng định nghĩa này cho các trường hợp cạnh nơi không có phương pháp nào tốt hơn baseline, tức là
𝑠_𝐷^*−𝑠_𝐷^𝑏=0. Chúng tôi giả định rằng trường hợp cạnh này chưa bao giờ xảy ra trong điểm chuẩn AutoML. Nếu không,
định nghĩa và triển khai của họ sẽ không được xác định / bị crash. Trong cài đặt của chúng tôi, một
trường hợp cạnh như vậy có thể xảy ra do overfitting sao cho các phương pháp ensemble trở nên tệ hơn
mô hình tốt nhất đơn lẻ.

Chúng tôi tổng quát hóa định nghĩa của trường hợp cạnh này thành trường hợp |𝑠_𝐷^*−𝑠_𝐷^𝑏|<=Δ. Chúng tôi đặt Δ thành 0.0001,
vì cải tiến chuẩn hóa cũng trở nên không đại diện cho một bộ dữ liệu nếu 𝑠_𝐷^*−𝑠_𝐷^𝑏 rất
nhỏ (vì chúng ta chia cho 𝑠_𝐷^*−𝑠_𝐷^𝑏).

Nếu trường hợp cạnh xảy ra, chúng tôi đặt điểm số của tất cả phương pháp tệ hơn baseline thành −10,
theo cách tiếp cận phạt (ví dụ: PAR10 từ Lựa chọn Thuật toán (Lindauer et al., 2019)).
Các phương pháp mà |𝑠_𝐷−𝑠_𝐷^𝑏|<=Δ được gán điểm số −1.

21

--- TRANG 22 ---
Bảng 2: Thông tin bổ sung cho tất cả bộ dữ liệu và mô hình cơ sở được tạo ra.

Tên Bộ dữ liệu | Số trung bình #Mô hình Cơ sở | Số trung bình #Thuật toán Riêng biệt
OpenML Task ID | #Thể hiện | #Đặc trưng | #Lớp | Bộ nhớ (GB) | BA_{TopN} | BA_{SiloTopN} | R_{TopN} | R_{SiloTopN} | BA_{TopN} | BA_{SiloTopN} | R_{TopN} | R_{SiloTopN}

[Bảng chi tiết với 71 dòng dữ liệu từ yeast đến KDDCup09-Upselling với các giá trị số tương ứng]

22

--- TRANG 23 ---
Bảng 3: Không gian Cấu hình Q(D)O-ES.

Siêu tham số | Giá trị
Tiền xử lý | SiloTopN, TopN
Kích thước Lô | 20, 40
Kích thước Archive | 16, 49
Khởi tạo Archive | L1 Ensembles, L2 của Single Best, Random L2
Phương pháp Lấy mẫu | Deterministic, Tournament, Dynamic
Lai ghép | two-point crossover, average crossover, no crossover

F Phụ lục cho Kết quả

F.1 Ước tính Giới hạn Dưới của Thời gian Wall-Clock cho Thực nghiệm

Chúng tôi ước tính giới hạn dưới cho thời gian wall-clock cho thực nghiệm của chúng tôi là khoảng 3.91
năm. Chúng tôi không ghi lại thời gian CPU trong khi chạy thực nghiệm và không thể tính toán điều này
hồi tưởng do song song hóa. Chúng tôi lưu ý rằng thời gian wall-clock là một giới hạn dưới tuyệt đối
cho thời gian CPU. Hơn nữa, cho ước tính thời gian wall-clock của chúng tôi, chúng tôi loại trừ overhead mà
song song hóa và containerization gây ra, vì chúng tôi không đo thời gian overhead này. Tuy nhiên,
chúng tôi tin rằng đây là một overhead không nhỏ, xem xét số lượng đáng kể các lần chạy ensemble
và kinh nghiệm của chúng tôi trong việc chạy thực nghiệm.

Để tính toán ước tính, trước tiên chúng tôi xem xét thời gian wall-clock cần thiết để tạo ra các mô hình cơ sở
với Auto-Sklearn. Do đó, chúng tôi nhân ngân sách 4 giờ mà chúng tôi đã cấp cho Auto-Sklearn
với 8 lõi: 4 giờ×10 folds×71 bộ dữ liệu×2 chỉ số; tương đương ∼0.65 năm.

Tiếp theo, chúng tôi xem xét thời gian wall-clock cần thiết để chạy tất cả cấu hình của tất cả phương pháp ensemble.
Do đó, chúng tôi sử dụng thời gian cần thiết để fit và dự đoán với các phương pháp ensemble. Chúng tôi đã ghi lại điều này
trong khi đánh giá các phương pháp ensemble (dữ liệu liên quan có thể được tìm thấy trong kho mã của chúng tôi). Lưu ý
rằng đo thời gian fit và predict loại trừ bất kỳ thời gian nào liên quan đến các mô hình cơ sở, vì
đánh giá chỉ hoạt động trên dự đoán của các mô hình cơ sở được lưu trữ sau khi tạo ra chúng – điều này
loại trừ việc tải dự đoán hoặc bộ dữ liệu. Tương tự, thời gian đo được loại trừ thời gian
cần thiết để chấm điểm các phương pháp ensemble và lưu trữ kết quả liên quan đến đánh giá. Cuối cùng, phương pháp ensemble
có 8 lõi có sẵn cho fitting và predicting. Với những ràng buộc này, chúng tôi ước tính một
giới hạn dưới bằng cách tổng thời gian fit và predict của tất cả cấu hình trên tất cả bộ dữ liệu, folds,
cấu hình, và cả hai chỉ số; tương đương ∼3.26 năm.

Kết luận, chúng tôi ước tính giới hạn dưới cho thời gian wall-clock cho thực nghiệm của chúng tôi là
khoảng 3.91 năm mà không song song hóa trên các bộ dữ liệu, folds, cấu hình, hoặc chỉ số.

F.2 Kết quả Bổ sung trên Dữ liệu Xác thực

Để trực quan hóa phân phối hiệu suất tương đối của các phương pháp ensemble trên
dữ liệu xác thực, xem Hình 4 cho boxplots của cải tiến chuẩn hóa. Chúng tôi quan sát thấy rằng
cải tiến chuẩn hóa cho QDO-ES lớn hơn đáng kể so với GES và QO-ES.

F.3 Nghiên cứu Ablation: Tầm quan trọng Siêu tham số Q(D)O-ES

Chúng tôi thực hiện nghiên cứu ablation các thành phần của Q(D)O-ES bằng cách xác định tầm quan trọng của
siêu tham số đối với hiệu suất cuối cùng.

Chúng tôi phân tích tầm quan trọng của siêu tham số sử dụng fANOVA (Hutter et al., 2014). Do đó,
chúng tôi tính toán tầm quan trọng của các siêu tham số được định nghĩa trong Bảng 3 cho mỗi bộ dữ liệu cho QO-ES
và QDO-ES. Bảng 4 hiển thị tầm quan trọng trung bình (cao hơn là quan trọng hơn) trên tất cả bộ dữ liệu chia
theo chỉ số và nhiệm vụ phân loại cho QDO-ES và QO-ES.

23

--- TRANG 24 ---
2.0
 1.5
 1.0
 0.5
 0.0
Cải tiến Chuẩn hóaQO-ES [0]
QDO-ES [0]
GES [0]Phương pháp
SingleBest(a) ROC AUC - Nhị phân (41 Bộ dữ liệu)

2.0
 1.5
 1.0
 0.5
 0.0
Cải tiến Chuẩn hóaQO-ES [0]
QDO-ES [0]
GES [0]Phương pháp
SingleBest (b) ROC AUC - Đa lớp (30 Bộ dữ liệu)

2.0
 1.5
 1.0
 0.5
 0.0
Cải tiến Chuẩn hóaQO-ES [0]
QDO-ES [0]
GES [0]Phương pháp
SingleBest
(c) Độ chính xác Cân bằng - Nhị phân (41 Bộ dữ liệu)

2.0