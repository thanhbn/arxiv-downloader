# Network Pruning via Transformable Architecture Search
## Cắt tỉa mạng thông qua Tìm kiếm Kiến trúc Có thể Biến đổi

**Xuanyi Dongyz, Yi Yangy**
yThe ReLER Lab, University of Technology Sydney, zBaidu Research
xuanyi.dong@student.uts.edu.au; yi.yang@uts.edu.au

## Tóm tắt

Cắt tỉa mạng làm giảm chi phí tính toán của một mạng có quá nhiều tham số mà không làm tổn hại hiệu suất. Các thuật toán cắt tỉa phổ biến trước đó định nghĩa trước chiều rộng và độ sâu của các mạng đã cắt tỉa, sau đó chuyển giao các tham số từ mạng chưa cắt tỉa sang các mạng đã cắt tỉa. Để phá vỡ giới hạn cấu trúc của các mạng đã cắt tỉa, chúng tôi đề xuất áp dụng tìm kiếm kiến trúc mạng neural để tìm kiếm trực tiếp một mạng với kích thước kênh và lớp linh hoạt. Số lượng kênh/lớp được học bằng cách tối thiểu hóa mất mát của các mạng đã cắt tỉa. Bản đồ đặc trưng của mạng đã cắt tỉa là một tổng hợp của K phần bản đồ đặc trưng (được tạo bởi K mạng có kích thước khác nhau), được lấy mẫu dựa trên phân phối xác suất. Mất mát có thể được lan truyền ngược không chỉ đến các trọng số mạng, mà còn đến phân phối được tham số hóa để điều chỉnh rõ ràng kích thước của các kênh/lớp. Cụ thể, chúng tôi áp dụng nội suy theo kênh để giữ cho bản đồ đặc trưng với các kích thước kênh khác nhau được căn chỉnh trong quy trình tổng hợp. Xác suất tối đa cho kích thước trong mỗi phân phối đóng vai trò là chiều rộng và độ sâu của mạng đã cắt tỉa, có các tham số được học thông qua chuyển giao kiến thức, ví dụ, chưng cất kiến thức, từ các mạng gốc. Các thí nghiệm trên CIFAR-10, CIFAR-100 và ImageNet chứng minh hiệu quả của góc nhìn mới về cắt tỉa mạng so với các thuật toán cắt tỉa mạng truyền thống. Các phương pháp tìm kiếm và chuyển giao kiến thức khác nhau được thực hiện để cho thấy hiệu quả của hai thành phần. Mã nguồn tại: https://github.com/D-X-Y/NAS-Projects.

## 1 Giới thiệu

Các mạng neural tích chập sâu (CNN) đã trở nên rộng hơn và sâu hơn để đạt được hiệu suất cao trên các ứng dụng khác nhau [17,22,48]. Mặc dù có thành công lớn, việc triển khai chúng trên các thiết bị có tài nguyên hạn chế, chẳng hạn như thiết bị di động và máy bay không người lái, là không thực tế.

**Huấn luyện một CNN lớn T → Cắt tỉa bộ lọc, nhận được một CNN nhỏ S → Tinh chỉnh CNN S → Một CNN hiệu quả S**
(a) Mô hình Cắt tỉa Truyền thống

**Huấn luyện một CNN lớn T → Tìm kiếm chiều rộng và độ sâu của CNN S → Chuyển giao kiến thức từ T sang S → Một CNN hiệu quả S**
(b) Mô hình Cắt tỉa Được đề xuất

**Hình 1:** So sánh giữa mô hình cắt tỉa điển hình và mô hình được đề xuất.

Một giải pháp đơn giản để giải quyết vấn đề này là sử dụng cắt tỉa mạng [29,12,13,20,18] để giảm chi phí tính toán của các CNN có quá nhiều tham số. Một quy trình điển hình cho cắt tỉa mạng, như được chỉ ra trong Hình 1(a), được thực hiện bằng cách loại bỏ các bộ lọc dư thừa và sau đó tinh chỉnh các mạng đã cắt, dựa trên các mạng gốc. Các tiêu chí khác nhau cho tầm quan trọng của các bộ lọc được áp dụng, chẳng hạn như chuẩn L2 của bộ lọc [30], lỗi tái tạo [20], và hệ số tỷ lệ có thể học [32]. Cuối cùng, các nhà nghiên cứu áp dụng các chiến lược tinh chỉnh khác nhau [30,18] cho mạng đã cắt tỉa để chuyển giao hiệu quả các tham số của các mạng chưa cắt tỉa và tối đa hóa hiệu suất của các mạng đã cắt tỉa.

*Công việc này được thực hiện khi Xuanyi Dong là thực tập sinh nghiên cứu tại Baidu Research.*

Hội nghị lần thứ 33 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2019), Vancouver, Canada.
arXiv:1905.09717v5 [cs.CV] 16 Oct 2019

Các phương pháp cắt tỉa mạng truyền thống đạt được tác động hiệu quả về nén mạng trong khi duy trì độ chính xác. Cấu trúc mạng của chúng được thiết kế một cách trực quan, ví dụ, cắt tỉa 30% bộ lọc trong mỗi lớp [30,18], dự đoán tỷ lệ thưa [15] hoặc tận dụng điều chuẩn hóa [2]. Độ chính xác của mạng đã cắt tỉa bị giới hạn trên bởi các cấu trúc được thiết kế thủ công hoặc các quy tắc cho cấu trúc. Để phá vỡ giới hạn này, chúng tôi áp dụng Tìm kiếm Kiến trúc Neural (NAS) để biến việc thiết kế cấu trúc kiến trúc thành một quy trình học và đề xuất một mô hình mới cho cắt tỉa mạng như được giải thích trong Hình 1(b).

Các phương pháp NAS phổ biến [31,48,8,4,40] tối ưu hóa cấu trúc liên kết mạng, trong khi trọng tâm của bài báo này là kích thước mạng tự động. Để đáp ứng các yêu cầu và thực hiện so sánh công bằng với các chiến lược cắt tỉa trước đó, chúng tôi đề xuất một sơ đồ NAS mới được gọi là Tìm kiếm Kiến trúc Có thể Biến đổi (TAS). TAS nhằm tìm kiếm kích thước tốt nhất của một mạng thay vì cấu trúc liên kết, được điều chuẩn bằng cách tối thiểu hóa chi phí tính toán, ví dụ, các phép toán dấu phẩy động (FLOP). Các tham số của các mạng đã tìm kiếm/cắt tỉa sau đó được học thông qua chuyển giao kiến thức [21, 44, 46].

TAS là một thuật toán tìm kiếm có thể vi phân, có thể tìm kiếm chiều rộng và độ sâu của các mạng một cách hiệu quả và hiệu quả. Cụ thể, các ứng viên khác nhau của kênh/lớp được gắn với một xác suất có thể học. Phân phối xác suất được học bằng cách lan truyền ngược mất mát được tạo bởi các mạng đã cắt tỉa, có bản đồ đặc trưng là một tổng hợp của K phần bản đồ đặc trưng (đầu ra của các mạng có kích thước khác nhau) được lấy mẫu dựa trên phân phối xác suất. Các bản đồ đặc trưng này với các kích thước kênh khác nhau được tổng hợp với sự trợ giúp của nội suy theo kênh. Xác suất tối đa cho kích thước trong mỗi phân phối đóng vai trò là chiều rộng và độ sâu của mạng đã cắt tỉa.

Trong các thí nghiệm, chúng tôi cho thấy rằng kiến trúc đã tìm kiếm với các tham số được chuyển giao bằng chưng cất kiến thức (KD) vượt trội hơn các phương pháp cắt tỉa tiên tiến trước đó trên CIFAR-10, CIFAR-100 và ImageNet. Chúng tôi cũng thử nghiệm các phương pháp chuyển giao kiến thức khác nhau trên các kiến trúc được tạo bởi các phương pháp cắt tỉa được thiết kế thủ công truyền thống [30,18] và phương pháp tìm kiếm kiến trúc ngẫu nhiên [31]. Các cải thiện nhất quán trên các kiến trúc khác nhau chứng minh tính tổng quát của chuyển giao kiến thức.

## 2 Các Nghiên cứu Liên quan

Cắt tỉa mạng [29,33] là một kỹ thuật hiệu quả để nén và tăng tốc CNN, và do đó cho phép chúng ta triển khai các mạng hiệu quả trên các thiết bị phần cứng có lưu trữ và tài nguyên tính toán hạn chế. Nhiều kỹ thuật khác nhau đã được đề xuất, chẳng hạn như phân tích hạng thấp [47], cắt tỉa trọng số [14, 29,13,12], cắt tỉa kênh [18,33], tính toán động [9,7] và lượng tử hóa [23,1]. Chúng nằm trong hai phương thức: cắt tỉa không có cấu trúc [29, 9, 7, 12] và cắt tỉa có cấu trúc [30, 20, 18, 33].

Các phương pháp cắt tỉa không có cấu trúc [29,9,7,12] thường buộc các trọng số tích chập [29,14] hoặc bản đồ đặc trưng [7,9] phải thưa. Những người tiên phong trong cắt tỉa không có cấu trúc, LeCun et al. [29] và Hassibi et al. [14], đã điều tra việc sử dụng thông tin đạo hàm bậc hai để cắt tỉa trọng số của các CNN nông. Sau khi mạng sâu ra đời vào năm 2012 [28], Han et al. [12,13,11] đã đề xuất một loạt các công trình để có được các CNN sâu được nén cao dựa trên điều chuẩn hóa L2. Sau sự phát triển này, nhiều nhà nghiên cứu đã khám phá các kỹ thuật điều chuẩn hóa khác nhau để cải thiện độ thưa trong khi bảo tồn độ chính xác, chẳng hạn như điều chuẩn hóa L0 [35] và độ nhạy đầu ra [41]. Vì các phương pháp không có cấu trúc này làm cho một mạng lớn thưa thay vì thay đổi toàn bộ cấu trúc của mạng, chúng cần thiết kế chuyên dụng cho các phụ thuộc [11] và phần cứng cụ thể để tăng tốc quy trình suy luận.

Các phương pháp cắt tỉa có cấu trúc [30,20,18,33] nhắm vào việc cắt tỉa các bộ lọc tích chập hoặc toàn bộ các lớp, và do đó các mạng đã cắt tỉa có thể được phát triển và áp dụng dễ dàng. Các công trình sớm trong lĩnh vực này [2,42] đã tận dụng Lasso nhóm để cho phép độ thưa có cấu trúc của các mạng sâu. Sau đó, Li et al. [30] đã đề xuất mô hình cắt tỉa ba giai đoạn điển hình (huấn luyện một mạng lớn, cắt tỉa, huấn luyện lại). Các thuật toán cắt tỉa này coi các bộ lọc có chuẩn nhỏ là không quan trọng và có xu hướng cắt tỉa chúng, nhưng giả định này không đúng trong các mạng phi tuyến sâu [43]. Do đó, nhiều nhà nghiên cứu tập trung vào tiêu chí tốt hơn cho các bộ lọc có thông tin. Ví dụ, Liu et al. [32] đã tận dụng điều chuẩn hóa L1; Ye et al. [43] đã áp dụng phạt ISTA; và He et al. [19] đã sử dụng tiêu chí dựa trên trung vị hình học.

Trái ngược với các quy trình cắt tỉa trước đó, phương pháp của chúng tôi cho phép số lượng kênh/lớp được tối ưu hóa một cách rõ ràng để cấu trúc đã học có hiệu suất cao và chi phí thấp.

Bên cạnh các tiêu chí cho các bộ lọc có thông tin, tầm quan trọng của cấu trúc mạng đã được đề xuất trong [33]. Một số phương pháp ngầm tìm một kiến trúc cụ thể cho dữ liệu [42,2,15], bằng cách tự động xác định tỷ lệ cắt tỉa và nén của mỗi lớp. Ngược lại, chúng tôi khám phá kiến trúc một cách rõ ràng bằng cách sử dụng NAS. Hầu hết các thuật toán NAS trước đó [48,8,31,40] tự động khám phá cấu trúc cấu trúc liên kết của một mạng neural, trong khi chúng tôi tập trung vào tìm kiếm độ sâu và chiều rộng của một mạng neural. Các phương pháp dựa trên học tăng cường (RL) [48,3] hoặc các phương pháp dựa trên thuật toán tiến hóa [40] có thể tìm kiếm các mạng với chiều rộng và độ sâu linh hoạt, tuy nhiên, chúng đòi hỏi tài nguyên tính toán khổng lồ và không thể được sử dụng trực tiếp trên các tập dữ liệu mục tiêu quy mô lớn. Các phương pháp có thể vi phân [8,31,4] giảm đáng kể chi phí tính toán nhưng chúng thường giả định rằng số lượng kênh trong các ứng viên tìm kiếm khác nhau là giống nhau. TAS là một phương pháp NAS có thể vi phân, có thể tìm kiếm hiệu quả các mạng có thể biến đổi với chiều rộng và độ sâu linh hoạt.

Biến đổi mạng [5,10,3] cũng đã nghiên cứu độ sâu và chiều rộng của các mạng. Chen et al. [5] đã mở rộng và làm sâu thêm một mạng một cách thủ công, và đề xuất Net2Net để khởi tạo mạng lớn hơn. Ariel et al. [10] đã đề xuất một chiến lược heuristic để tìm chiều rộng phù hợp của các mạng bằng cách xen kẽ giữa thu nhỏ và mở rộng. Cai et al. [3] đã sử dụng một tác nhân RL để phát triển độ sâu và chiều rộng của CNN, trong khi TAS của chúng tôi là một phương pháp có thể vi phân và không chỉ có thể mở rộng mà còn có thể thu nhỏ CNN.

Chuyển giao kiến thức đã được chứng minh là hiệu quả trong tài liệu về cắt tỉa. Các tham số của các mạng có thể được chuyển giao từ khởi tạo được huấn luyện trước [30,18]. Minnehan et al. [37] đã chuyển giao kiến thức của mạng không nén thông qua mất mát tái tạo theo khối. Trong bài báo này, chúng tôi áp dụng một phương pháp KD đơn giản [21] để thực hiện chuyển giao kiến thức, điều này đạt được hiệu suất mạnh mẽ cho các kiến trúc đã tìm kiếm.

## 3 Phương pháp luận

Phương pháp cắt tỉa của chúng tôi bao gồm ba bước: (1) huấn luyện mạng lớn chưa cắt tỉa bằng quy trình huấn luyện phân loại tiêu chuẩn. (2) tìm kiếm độ sâu và chiều rộng của một mạng nhỏ thông qua TAS được đề xuất. (3) chuyển giao kiến thức từ mạng lớn chưa cắt tỉa sang mạng nhỏ đã tìm kiếm bằng phương pháp KD đơn giản [21]. Chúng tôi sẽ giới thiệu bối cảnh, hiển thị chi tiết của TAS, và giải thích quy trình chuyển giao kiến thức.

### 3.1 Tìm kiếm Kiến trúc Có thể Biến đổi

Cắt tỉa kênh mạng nhằm giảm số lượng kênh trong mỗi lớp của một mạng. Cho một hình ảnh đầu vào, một mạng lấy nó làm đầu vào và tạo ra xác suất trên mỗi lớp mục tiêu. Giả sử X và O là các tensor đặc trưng đầu vào và đầu ra của lớp tích chập thứ l (chúng tôi lấy tích chập 3×3 làm ví dụ), lớp này tính toán quy trình sau:

O_j = Σ_{k=1}^{c_in} X_{k,:,:} * W_{j,k,:,:} với 1 ≤ j ≤ c_out;     (1)

trong đó W ∈ R^{c_out×c_in×3×3} chỉ trọng số kernel tích chập, c_in là kênh đầu vào, và c_out là kênh đầu ra. W_{j,k,:,:} tương ứng với kênh đầu vào thứ k và kênh đầu ra thứ j. * biểu thị phép toán tích chập. Các phương pháp cắt tỉa kênh có thể giảm số lượng c_out, và do đó, c_in trong lớp tiếp theo cũng được giảm.

**Tìm kiếm chiều rộng.** Chúng tôi sử dụng các tham số α ∈ R^{|C|} để chỉ phân phối của số lượng kênh có thể có trong một lớp, được chỉ bởi C và max(C) ≤ c_out. Xác suất chọn ứng viên thứ j cho số lượng kênh có thể được công thức hóa như:

p_j = exp(α_j) / Σ_{k=1}^{|C|} exp(α_k) với 1 ≤ j ≤ |C|;     (2)

Tuy nhiên, hoạt động lấy mẫu trong quy trình trên là không thể vi phân, điều này ngăn chúng ta lan truyền ngược gradient qua p_j đến α_j. Được thúc đẩy bởi [8], chúng tôi áp dụng Gumbel-Softmax [26,36] để làm mềm quy trình lấy mẫu để tối ưu hóa α:

p̂_j = exp((log(p_j) + o_j)/τ) / Σ_{k=1}^{|C|} exp((log(p_k) + o_k)/τ) s.t. o_j = -log(-log(u)) & u ~ U(0,1);     (3)

trong đó U(0,1) có nghĩa là phân phối đồng nhất giữa 0 và 1. τ là nhiệt độ softmax. Khi τ → 0, p̂ = [p̂_1, ..., p̂_j, ...] trở thành one-shot, và phân phối Gumbel-softmax rút ra từ p̂ trở nên giống hệt với phân phối categorical. Khi τ → ∞, phân phối Gumbel-softmax trở thành một phân phối đồng nhất trên C. Bản đồ đặc trưng trong phương pháp của chúng tôi được định nghĩa là tổng có trọng số của các phần bản đồ đặc trưng gốc với các kích thước khác nhau, trong đó trọng số là p̂. Các bản đồ đặc trưng với các kích thước khác nhau được căn chỉnh bằng nội suy theo kênh (CWI) để thực hiện phép toán tổng có trọng số. Để giảm chi phí bộ nhớ, chúng tôi chọn một tập con nhỏ với các chỉ số I ⊆ [|C|] để tổng hợp thay vì sử dụng tất cả các ứng viên. Ngoài ra, các trọng số được chuẩn hóa lại dựa trên xác suất của các kích thước đã chọn, được công thức hóa như:

Ô = Σ_{j∈I} (exp((log(p_j) + o_j)/τ) / Σ_{k∈I} exp((log(p_k) + o_k)/τ)) CWI(O_{1:C_j,:,:}, max(C_I))  s.t. I ~ T_{p̂};     (4)

trong đó T_{p̂} chỉ phân phối xác suất đa thức được tham số hóa bởi p̂. CWI được đề xuất là một phép toán tổng quát để căn chỉnh các bản đồ đặc trưng với các kích thước khác nhau. Nó có thể được thực hiện theo nhiều cách, chẳng hạn như biến thể 3D của mạng biến đổi không gian [25] hoặc phép toán pooling thích ứng [16]. Trong bài báo này, chúng tôi chọn phép toán pooling trung bình thích ứng 3D [16] làm CWI², vì nó không mang lại thêm tham số nào và chi phí bổ sung không đáng kể. Chúng tôi sử dụng Batch Normalization [24] trước CWI để chuẩn hóa các phần khác nhau. Hình 2 minh họa quy trình trên bằng cách lấy |I| = 2 làm ví dụ.

**Thảo luận về chiến lược lấy mẫu trong Phương trình (4).** Chiến lược này nhằm giảm đáng kể chi phí bộ nhớ và thời gian huấn luyện xuống một mức độ có thể chấp nhận bằng cách chỉ lan truyền ngược gradient của các kiến trúc đã lấy mẫu thay vì tất cả các kiến trúc. So với lấy mẫu qua phân phối đồng nhất, phương pháp lấy mẫu được áp dụng (lấy mẫu dựa trên xác suất) có thể làm yếu đi sự khác biệt gradient gây ra bởi việc lấy mẫu mỗi lần lặp sau nhiều lần lặp.

**Tìm kiếm độ sâu.** Chúng tôi sử dụng các tham số β ∈ R^L để chỉ phân phối của số lượng lớp có thể có trong một mạng với L lớp tích chập. Chúng tôi sử dụng một chiến lược tương tự để lấy mẫu số lượng lớp theo Phương trình (3) và cho phép β có thể vi phân như α, sử dụng phân phối lấy mẫu q̂_l cho độ sâu l. Sau đó chúng tôi tính toán đặc trưng đầu ra cuối cùng của các mạng đã cắt tỉa là một tổng hợp từ tất cả các độ sâu có thể, có thể được công thức hóa như:

O_out = Σ_{l=1}^L q̂_l CWI(Ô_l, C_out);     (5)

trong đó Ô_l chỉ bản đồ đặc trưng đầu ra thông qua Phương trình (4) tại lớp thứ l. C_out chỉ kênh tối đa được lấy mẫu trong tất cả Ô_l. Bản đồ đặc trưng đầu ra cuối cùng O_out được đưa vào lớp phân loại cuối cùng để đưa ra dự đoán. Theo cách này, chúng ta có thể lan truyền ngược gradient đến cả tham số chiều rộng α và tham số độ sâu β.

²Công thức của CWI đã chọn: giả sử B = CWI(A, C_out), trong đó B ∈ R^{C_out×H×W} và A ∈ R^{C×H×W}; thì B_{i,h,w} = mean(A_{s:e-1,h,w}), trong đó s = ⌊iC/C_out⌋ và e = ⌊(i+1)C/C_out⌋. Chúng tôi đã thử các dạng khác của CWI, ví dụ, nội suy song tuyến và tam tuyến. Chúng đạt được độ chính xác tương tự nhưng chậm hơn nhiều so với lựa chọn của chúng tôi.

**logits ← hình ảnh → lấy mẫu hai lựa chọn kênh: 3 và 4 qua p1 → CWI += → đầu ra thực tế của conv-1 ← CNN chưa cắt tỉa**

**CWI += ← CWI += ← ×p13 ← ×p14 ← ×p21 ← ×p23 ← ×p31 ← ×p32**

**p1 = [p11, p12, p13, p14] ← lấy mẫu hai lựa chọn kênh: 1 và 3 qua p2 ← lấy mẫu hai lựa chọn kênh: 1 và 2 qua p3**

**kênh=4 bản đồ đặc trưng ← kênh=4 bản đồ đặc trưng ← kênh=4 bản đồ đặc trưng**

**xác suất C=1 2 3 4 p11 p12 p13 p14 ← C=1 2 3 4 p21 p22 p23 p24 xác suất ← C=1 2 3 4 p31 p32 p33 p34 xác suất**

**phân phối xác suất của #kênh lớp thứ 1 ← lớp thứ 2 ← lớp thứ 3**

**CNN đã cắt tỉa - lớp thứ 1 ← CNN đã cắt tỉa - lớp thứ 2 ← CNN đã cắt tỉa - lớp thứ 3 ← CNN đã cắt tỉa - logits ← hình ảnh ← hình ảnh ← hình ảnh ← hình ảnh**

**Hình 2:** Tìm kiếm chiều rộng của một CNN đã cắt tỉa từ một CNN ba lớp chưa cắt tỉa. Mỗi lớp tích chập được trang bị một phân phối có thể học cho kích thước của các kênh trong lớp này, được chỉ bởi p_i ở bên trái. Bản đồ đặc trưng cho mỗi lớp được xây dựng tuần tự bởi các lớp, như được hiển thị ở bên phải. Đối với một lớp cụ thể, K (2 trong ví dụ này) bản đồ đặc trưng có kích thước khác nhau được lấy mẫu theo phân phối tương ứng và kết hợp bằng nội suy theo kênh (CWI) và tổng có trọng số. Bản đồ đặc trưng tổng hợp này được đưa làm đầu vào cho lớp tiếp theo.

**Mục tiêu tìm kiếm.** Kiến trúc cuối cùng A được dẫn xuất bằng cách chọn ứng viên có xác suất tối đa, được học bởi các tham số kiến trúc Θ_A, bao gồm α cho mỗi lớp và β. Mục tiêu của TAS của chúng tôi là tìm một kiến trúc A với mất mát xác thực tối thiểu L_val sau khi được huấn luyện bằng cách tối thiểu hóa mất mát huấn luyện L_train như:

min_A L_val(ω*_A, A)  s.t. ω*_A = arg min_ω L_train(ω, A);     (6)

trong đó ω*_A chỉ các trọng số tối ưu hóa của A. Mất mát huấn luyện là mất mát phân loại cross-entropy của các mạng. Các phương pháp NAS phổ biến [31,48,8,4,40] tối ưu hóa A trên các ứng viên mạng với các cấu trúc liên kết khác nhau, trong khi TAS của chúng tôi tìm kiếm trên các ứng viên có cùng cấu trúc liên kết cũng như chiều rộng và độ sâu nhỏ hơn. Kết quả là, mất mát xác thực trong quy trình tìm kiếm của chúng tôi bao gồm không chỉ mất mát xác thực phân loại mà còn phạt cho chi phí tính toán:

L_val = -log(exp(z_y) / Σ_{j=1}^{|z|} exp(z_j)) + λ_cost L_cost;     (7)

trong đó z là một vector biểu thị các logit đầu ra từ các mạng đã cắt tỉa, y chỉ lớp thực tế của một đầu vào tương ứng, và λ_cost là trọng số của L_cost. Mất mát chi phí khuyến khích chi phí tính toán của mạng (ví dụ, FLOP) hội tụ đến một mục tiêu R để chi phí có thể được điều chỉnh động bằng cách đặt R khác nhau. Chúng tôi đã sử dụng mất mát chi phí tính toán từng phần như:

L_cost = {
  -log(E_cost(A))     nếu F_cost(A) > (1 + t)R
  0                   nếu (1-t)R < F_cost(A) < (1 + t)R
  log(E_cost(A))      nếu F_cost(A) < (1-t)R
};     (8)

trong đó E_cost(A) tính kỳ vọng của chi phí tính toán, dựa trên các tham số kiến trúc Θ_A. Cụ thể, nó là tổng có trọng số của chi phí tính toán cho tất cả các mạng ứng viên, trong đó trọng số là xác suất lấy mẫu. F_cost(A) chỉ chi phí thực tế của kiến trúc đã tìm kiếm, có chiều rộng và độ sâu được dẫn xuất từ Θ_A. t ∈ [0,1] biểu thị tỷ lệ dung sai, làm chậm tốc độ thay đổi kiến trúc đã tìm kiếm. Lưu ý rằng chúng tôi sử dụng FLOP để đánh giá chi phí tính toán của một mạng, và có thể dễ dàng thay thế FLOP bằng metric khác, chẳng hạn như latency [4].

**Thuật toán 1 Quy trình TAS**
**Đầu vào:** chia tập huấn luyện thành hai tập không giao nhau: D_train và D_val
1: **while** not converge **do**
2:    Lấy mẫu dữ liệu batch D_t từ D_train
3:    Tính L_train trên D_t để cập nhật trọng số mạng
4:    Lấy mẫu dữ liệu batch D_v từ D_val
5:    Tính L_val trên D_v qua Phương trình (7) để cập nhật Θ_A
6: **end while**
7: Dẫn xuất mạng đã tìm kiếm từ Θ_A
8: Khởi tạo ngẫu nhiên mạng đã tìm kiếm và tối ưu hóa nó bằng KD qua Phương trình (10) trên tập huấn luyện

Chúng tôi hiển thị thuật toán tổng thể trong Thuật toán 1. Trong quá trình tìm kiếm, chúng tôi chuyển tiếp mạng sử dụng Phương trình (5) để làm cho cả trọng số và tham số kiến trúc có thể vi phân. Chúng tôi luân phiên tối thiểu hóa L_train trên tập huấn luyện để tối ưu hóa trọng số của các mạng đã cắt tỉa và L_val trên tập xác thực để tối ưu hóa các tham số kiến trúc Θ_A. Sau khi tìm kiếm, chúng tôi chọn số lượng kênh có xác suất tối đa làm chiều rộng và số lượng lớp có xác suất tối đa làm độ sâu. Mạng đã tìm kiếm cuối cùng được xây dựng bởi chiều rộng và độ sâu đã chọn. Mạng này sẽ được tối ưu hóa thông qua KD, và chúng tôi sẽ giới thiệu chi tiết trong Phần 3.2.

### 3.2 Chuyển giao Kiến thức

Chuyển giao kiến thức rất quan trọng để học một mạng đã cắt tỉa mạnh mẽ, và chúng tôi sử dụng một thuật toán KD đơn giản [21] trên một kiến trúc mạng đã tìm kiếm. Thuật toán này khuyến khích các dự đoán z của mạng nhỏ khớp với các mục tiêu mềm từ mạng chưa cắt tỉa thông qua mục tiêu sau:

L_match = Σ_{i=1}^{|z|} (exp(ẑ_i/T) / Σ_{j=1}^{|z|} exp(ẑ_j/T)) log(exp(z_i/T) / Σ_{j=1}^{|z|} exp(z_j/T));     (9)

trong đó T là nhiệt độ, và ẑ_i chỉ vector đầu ra logit từ mạng chưa cắt tỉa được huấn luyện trước. Ngoài ra, nó sử dụng softmax với mất mát cross-entropy để khuyến khích mạng nhỏ dự đoán các mục tiêu thực. Mục tiêu cuối cùng của KD như sau:

L_KD = -log(exp(z_y) / Σ_{j=1}^{|z|} exp(z_j)) + α(1-α)L_match  s.t. 0 ≤ α ≤ 1;     (10)

trong đó y chỉ lớp mục tiêu thực của một đầu vào tương ứng. α là trọng số của mất mát để cân bằng mất mát phân loại tiêu chuẩn và mất mát khớp mềm. Sau khi chúng tôi có được mạng đã tìm kiếm (Phần 3.1), chúng tôi đầu tiên huấn luyện trước mạng chưa cắt tỉa và sau đó tối ưu hóa mạng đã tìm kiếm bằng cách chuyển giao từ mạng chưa cắt tỉa thông qua Phương trình (10).

## 4 Phân tích Thí nghiệm

Chúng tôi giới thiệu thiết lập thí nghiệm trong Phần 4.1. Chúng tôi đánh giá các khía cạnh khác nhau của TAS trong Phần 4.2, chẳng hạn như siêu tham số, chiến lược lấy mẫu, các phương pháp chuyển giao khác nhau, v.v. Cuối cùng, chúng tôi so sánh TAS với các phương pháp cắt tỉa tiên tiến khác trong Phần 4.3.

### 4.1 Tập dữ liệu và Cài đặt

**Tập dữ liệu.** Chúng tôi đánh giá phương pháp của chúng tôi trên CIFAR-10, CIFAR-100 [27] và ImageNet [6]. CIFAR-10 chứa 50K hình ảnh huấn luyện và 10K hình ảnh kiểm tra với 10 lớp. CIFAR-100 tương tự như CIFAR-10 nhưng có 100 lớp. ImageNet chứa 1.28 triệu hình ảnh huấn luyện và 50K hình ảnh kiểm tra với 1000 lớp. Chúng tôi sử dụng tăng cường dữ liệu điển hình của ba tập dữ liệu này. Trên CIFAR-10 và CIFAR-100, chúng tôi cắt ngẫu nhiên patch 32×32 với 4 pixel padding ở mỗi viền, và chúng tôi cũng áp dụng lật ngang ngẫu nhiên. Trên ImageNet, chúng tôi sử dụng cắt thay đổi kích thước ngẫu nhiên điển hình, thay đổi ngẫu nhiên độ sáng/độ tương phản/độ bão hòa, và lật ngang ngẫu nhiên để tăng cường dữ liệu. Trong quá trình đánh giá, chúng tôi thay đổi kích thước hình ảnh thành 256×256 và cắt trung tâm một patch 224×224.

**Cài đặt tìm kiếm.** Chúng tôi tìm kiếm số lượng kênh trên {0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} của số lượng gốc trong mạng chưa cắt tỉa. Chúng tôi tìm kiếm độ sâu trong mỗi giai đoạn tích chập. Chúng tôi lấy mẫu |I| = 2 ứng viên trong Phương trình (4) để giảm chi phí bộ nhớ GPU trong quá trình tìm kiếm. Chúng tôi đặt R theo FLOP của các thuật toán cắt tỉa so sánh và đặt λ_cost là 2. Chúng tôi tối ưu hóa các trọng số thông qua SGD và các tham số kiến trúc thông qua Adam. Đối với các trọng số, chúng tôi bắt đầu tốc độ học từ 0.1 và giảm nó bằng bộ lập lịch cosine [34]. Đối với các tham số kiến trúc, chúng tôi sử dụng tốc độ học không đổi là 0.001 và weight decay là 0.001. Trên cả CIFAR-10 và CIFAR-100, chúng tôi huấn luyện mô hình trong 600 epoch với batch size 256. Trên ImageNet, chúng tôi huấn luyện ResNet [17] trong 120 epoch với batch size 256. Tỷ lệ dung sai t luôn được đặt là 5%. τ trong Phương trình (3) được giảm tuyến tính từ 10 xuống 0.1.

**Huấn luyện.** Đối với các thí nghiệm CIFAR, chúng tôi sử dụng SGD với momentum 0.9 và weight decay 0.0005. Chúng tôi huấn luyện mỗi mô hình trong 300 epoch, bắt đầu tốc độ học ở 0.1, và giảm nó bằng bộ lập lịch cosine [34]. Chúng tôi sử dụng batch size 256 và 2 GPU. Khi sử dụng KD trên CIFAR, chúng tôi sử dụng α là 0.9 và nhiệt độ T là 4 theo [46]. Đối với các mô hình ResNet trên ImageNet, chúng tôi tuân theo hầu hết các siêu tham số như CIFAR, nhưng sử dụng weight decay 0.0001. Chúng tôi sử dụng 4 GPU để huấn luyện mô hình trong 120 epoch với batch size 256. Khi sử dụng KD trên ImageNet, chúng tôi đặt α là 0.5 và T là 4 trên ImageNet.

### 4.2 Nghiên cứu Trường hợp

Trong phần này, chúng tôi đánh giá các khía cạnh khác nhau của TAS được đề xuất. Chúng tôi cũng so sánh nó với các thuật toán tìm kiếm và phương pháp chuyển giao kiến thức khác nhau để chứng minh hiệu quả của TAS.

**Tác động của các chiến lược khác nhau để vi phân α.** Chúng tôi áp dụng TAS trên CIFAR-100 để cắt tỉa ResNet-56. Chúng tôi thử hai phương pháp tổng hợp khác nhau, tức là sử dụng CWI được đề xuất để căn chỉnh các bản đồ đặc trưng hoặc không. Chúng tôi cũng thử hai loại trọng số tổng hợp khác nhau, tức là lấy mẫu Gumbel-softmax như Phương trình (3) (được ký hiệu là "sample" trong Hình 3) và vanilla-softmax như Phương trình (2) (được ký hiệu là "mixture" trong Hình 3). Do đó, có bốn chiến lược khác nhau, tức là với/không có CWI kết hợp với Gumbel-softmax/vanilla-softmax. Giả sử chúng ta không hạn chế chi phí tính toán, thì các tham số kiến trúc nên được tối ưu hóa để tìm chiều rộng và độ sâu tối đa. Điều này là do mạng như vậy sẽ có khả năng tối đa và dẫn đến hiệu suất tốt nhất trên CIFAR-100. Chúng tôi thử tất cả bốn chiến lược với và không sử dụng ràng buộc về chi phí tính toán. Chúng tôi hiển thị kết quả trong Hình 3c và Hình 3a. Khi chúng ta không hạn chế FLOP, TAS của chúng tôi có thể tìm thành công kiến trúc tốt nhất nên có chiều rộng và độ sâu tối đa. Tuy nhiên, ba chiến lược khác đã thất bại. Khi chúng ta sử dụng ràng buộc FLOP, chúng ta có thể hạn chế thành công chi phí tính toán trong phạm vi mục tiêu. Chúng tôi cũng điều tra sự khác biệt giữa xác suất cao nhất và xác suất cao thứ hai trong Hình 3d và Hình 3b. Về mặt lý thuyết, sự khác biệt cao hơn chỉ ra rằng mô hình tự tin hơn để chọn một chiều rộng nhất định, trong khi sự khác biệt thấp hơn có nghĩa là mô hình bối rối và không biết ứng viên nào để chọn. Như được hiển thị trong Hình 3d, với quy trình huấn luyện diễn ra, TAS của chúng tôi trở nên tự tin hơn để chọn chiều rộng phù hợp. Ngược lại, các chiến lược không có CWI không thể tối ưu hóa các tham số kiến trúc; và "mixture with CWI" cho thấy sự khác biệt tệ hơn so với chúng tôi.

[CHÈN HÌNH 3: Tác động của các lựa chọn khác nhau để làm cho các tham số kiến trúc có thể vi phân.]

**Bảng 1:** Độ chính xác trên CIFAR-100 khi cắt tỉa khoảng 40% FLOP của ResNet-32.

| | FLOP | độ chính xác |
|---|---|---|
| Pre-defined | 41.1 MB | 68.18% |
| Pre-defined w/ Init | 41.1 MB | 69.34% |
| Pre-defined w/ KD | 41.1 MB | 71.40% |
| Random Search | 42.9 MB | 68.57% |
| Random Search w/ Init | 42.9 MB | 69.14% |
| Random Search w/ KD | 42.9 MB | 71.71% |
| TAS† | 42.5 MB | 68.95% |
| TAS† w/ Init | 42.5 MB | 69.70% |
| TAS† w/ KD (TAS) | 42.5 MB | 72.41% |

**So sánh về cấu trúc được tạo bởi các phương pháp khác nhau trong Bảng 1.** "Pre-defined" có nghĩa là cắt tỉa một tỷ lệ cố định ở mỗi lớp [30]. "Random Search" chỉ một baseline NAS được sử dụng trong [31]. "TAS†" là thuật toán tìm kiếm có thể vi phân được đề xuất của chúng tôi. Chúng tôi có hai quan sát: (1) tìm kiếm có thể tìm cấu trúc tốt hơn sử dụng các phương pháp chuyển giao kiến thức khác nhau; (2) TAS của chúng tôi vượt trội hơn baseline NAS ngẫu nhiên.

**Bảng 2:** Kết quả của các cấu hình khác nhau khi cắt tỉa ResNet-32 trên CIFAR-10 với một GPU V100. "#SC" chỉ số lượng kênh được chọn. "H" chỉ giờ.

| #SC | Thời gian Tìm kiếm | Bộ nhớ | Thời gian Huấn luyện | FLOP | Độ chính xác |
|---|---|---|---|---|---|
| \|I\|=1 | 2.83 H | 1.5GB | 0.71 H | 23.59 MB | 89.85% |
| \|I\|=2 | 3.83 H | 2.4GB | 0.84 H | 38.95 MB | 92.98% |
| \|I\|=3 | 4.94 H | 3.4GB | 0.67 H | 39.04 MB | 92.63% |
| \|I\|=5 | 7.18 H | 5.1GB | 0.60 H | 37.08 MB | 93.18% |
| \|I\|=8 | 10.64 H | 7.3GB | 0.81 H | 38.28 MB | 92.65% |

**So sánh về các phương pháp chuyển giao kiến thức khác nhau trong Bảng 1.** Dòng đầu tiên trong mỗi khối không sử dụng bất kỳ phương pháp chuyển giao kiến thức nào. "w/ Init" chỉ việc sử dụng mạng chưa cắt tỉa được huấn luyện trước làm khởi tạo. "w/ KD" chỉ việc sử dụng KD. Từ Bảng 1, các phương pháp chuyển giao kiến thức có thể cải thiện nhất quán độ chính xác của mạng đã cắt tỉa, ngay cả khi một phương pháp đơn giản được áp dụng (Init). Bên cạnh đó, KD mạnh mẽ và cải thiện mạng đã cắt tỉa hơn 2% độ chính xác trên CIFAR-100.

**Tìm kiếm chiều rộng so với tìm kiếm độ sâu.** Chúng tôi thử (1) chỉ tìm kiếm độ sâu ("TAS (D)"), (2) chỉ tìm kiếm chiều rộng ("TAS (W)"), và (3) tìm kiếm cả độ sâu và chiều rộng ("TAS") trong Bảng 3. Kết quả của chỉ tìm kiếm độ sâu tệ hơn kết quả của chỉ tìm kiếm chiều rộng. Nếu chúng ta tìm kiếm cùng lúc cả độ sâu và chiều rộng, chúng ta có thể đạt được độ chính xác tốt hơn với FLOP tương tự so với chỉ tìm kiếm độ sâu và chỉ tìm kiếm chiều rộng.

**Tác động của việc chọn số lượng mẫu kiến trúc khác nhau I trong Phương trình (4).** Chúng tôi so sánh số lượng kênh được chọn khác nhau trong Bảng 2 và thực hiện thí nghiệm trên một NVIDIA Tesla V100. Thời gian tìm kiếm và sử dụng bộ nhớ GPU sẽ tăng tuyến tính theo |I|. Khi |I|=1, vì xác suất được chuẩn hóa lại trong Phương trình (4) trở thành một vô hướng hằng số là 1, gradient của tham số α sẽ trở thành 0 và việc tìm kiếm thất bại. Khi |I|>1, hiệu suất cho |I| khác nhau là tương tự.

**Tăng tốc độ.** Như được hiển thị trong Bảng 2, TAS có thể hoàn thành quy trình tìm kiếm của ResNet-32 trong khoảng 3.8 giờ trên một GPU V100. Nếu chúng ta sử dụng chiến lược tiến hóa (ES) hoặc các phương pháp tìm kiếm ngẫu nhiên, chúng ta cần huấn luyện mạng với nhiều cấu hình ứng viên khác nhau một cách tuần tự và sau đó đánh giá chúng để tìm ra tốt nhất. Theo cách này, chi phí tính toán nhiều hơn nhiều so với TAS của chúng tôi được yêu cầu. Một giải pháp có thể để tăng tốc các phương pháp ES hoặc tìm kiếm ngẫu nhiên là chia sẻ tham số của các mạng với các cấu hình khác nhau [39, 45], điều này nằm ngoài phạm vi của bài báo này.

### 4.3 So sánh với tiên tiến nhất

**Kết quả trên CIFAR trong Bảng 3.** Chúng tôi cắt tỉa các ResNet khác nhau trên cả CIFAR-10 và CIFAR-100. Hầu hết các thuật toán trước đó hoạt động kém trên CIFAR-100, trong khi TAS của chúng tôi vượt trội nhất quán hơn 2% độ chính xác trong hầu hết các trường hợp. Trên CIFAR-10, TAS của chúng tôi vượt trội hơn các thuật toán tiên tiến trên ResNet-20,32,56,110. Ví dụ, TAS đạt được 72.25% độ chính xác bằng cách cắt tỉa ResNet-56 trên CIFAR-100, cao hơn 69.66% của FPGM [19]. Để cắt tỉa ResNet-32 trên CIFAR-100, chúng tôi đạt được độ chính xác lớn hơn và FLOP ít hơn so với mạng chưa cắt tỉa. Chúng tôi đạt được hiệu suất hơi tệ hơn LCCL [7] trên ResNet-164. Điều này là do có 8×16×3×18×3 cấu trúc mạng ứng viên để tìm kiếm cho việc cắt tỉa ResNet-164. Việc tìm kiếm trên không gian tìm kiếm khổng lồ như vậy là thách thức, và mạng rất sâu có vấn đề overfitting trên CIFAR-10 [17].

**Bảng 3:** So sánh các thuật toán cắt tỉa khác nhau cho ResNet trên CIFAR. "Acc" = độ chính xác, "FLOP" = FLOP (tỷ lệ cắt tỉa), "TAS (D)" = tìm kiếm độ sâu, "TAS (W)" = tìm kiếm chiều rộng, "TAS" = tìm kiếm cả chiều rộng và độ sâu.

[BẢNG 3 - Chi tiết như trong nguyên văn]

**Kết quả trên ImageNet trong Bảng 4.** Chúng tôi cắt tỉa ResNet-18 và ResNet-50 trên ImageNet. Đối với ResNet-18, mất khoảng 59 giờ để tìm kiếm mạng đã cắt tỉa trên 4 GPU NVIDIA Tesla V100. Thời gian huấn luyện của ResNet-18 chưa cắt tỉa mất khoảng 24 giờ, và do đó thời gian tìm kiếm là có thể chấp nhận. Với nhiều máy hơn và triển khai được tối ưu hóa, chúng ta có thể hoàn thành TAS với chi phí thời gian ít hơn. Chúng tôi hiển thị kết quả cạnh tranh so với các thuật toán cắt tỉa tiên tiến khác. Ví dụ, TAS cắt tỉa ResNet-50 bằng 43.5% FLOP, và mạng đã cắt tỉa đạt được 76.20% độ chính xác, cao hơn FPGM 0.7. Các cải thiện tương tự có thể được tìm thấy khi cắt tỉa ResNet-18. Lưu ý rằng chúng tôi áp dụng trực tiếp các siêu tham số trên CIFAR-10 để cắt tỉa các mô hình trên ImageNet, và do đó TAS có thể đạt được kết quả tốt hơn bằng cách điều chỉnh cẩn thận các tham số trên ImageNet.

**Bảng 4:** So sánh các thuật toán cắt tỉa khác nhau cho các ResNet khác nhau trên ImageNet.

[BẢNG 4 - Chi tiết như trong nguyên văn]

TAS được đề xuất của chúng tôi là một công trình sơ bộ cho quy trình cắt tỉa mạng mới. Quy trình này có thể được cải thiện bằng cách thiết kế thuật toán tìm kiếm và phương pháp chuyển giao kiến thức hiệu quả hơn. Chúng tôi hy vọng rằng công việc tương lai để khám phá hai thành phần này sẽ mang lại các mạng nhỏ gọn mạnh mẽ.

## 5 Kết luận

Trong bài báo này, chúng tôi đề xuất một mô hình mới cho cắt tỉa mạng, bao gồm hai thành phần. Đối với thành phần đầu tiên, chúng tôi đề xuất áp dụng NAS để tìm kiếm độ sâu và chiều rộng tốt nhất của một mạng. Vì hầu hết các phương pháp NAS trước đó tập trung vào cấu trúc liên kết mạng thay vì kích thước mạng, chúng tôi gọi sơ đồ NAS mới này là Tìm kiếm Kiến trúc Có thể Biến đổi (TAS). Hơn nữa, chúng tôi đề xuất một phương pháp TAS có thể vi phân để tìm độ sâu và chiều rộng phù hợp nhất của một mạng một cách hiệu quả và hiệu quả. Đối với thành phần thứ hai, chúng tôi đề xuất tối ưu hóa mạng đã tìm kiếm bằng cách chuyển giao kiến thức từ mạng chưa cắt tỉa. Trong bài báo này, chúng tôi áp dụng một thuật toán KD đơn giản để thực hiện chuyển giao kiến thức, và tiến hành các phương pháp chuyển giao khác để chứng minh hiệu quả của thành phần này. Kết quả của chúng tôi cho thấy rằng những nỗ lực mới tập trung vào tìm kiếm và chuyển giao có thể dẫn đến những đột phá mới trong cắt tỉa mạng.

## Tài liệu tham khảo

[1] M. Alizadeh, J. Fernández-Marqués, N. D. Lane, and Y. Gal. An empirical study of binary neural networks' optimisation. In International Conference on Learning Representations (ICLR), 2019.

[2] J. M. Alvarez and M. Salzmann. Learning the number of neurons in deep networks. In The Conference on Neural Information Processing Systems (NeurIPS), pages 2270–2278, 2016.

[3] H. Cai, T. Chen, W. Zhang, Y. Yu, and J. Wang. Efficient architecture search by network transformation. In AAAI Conference on Artificial Intelligence (AAAI), pages 2787–2794, 2018.

[4] H. Cai, L. Zhu, and S. Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations (ICLR), 2019.

[5] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating learning via knowledge transfer. In International Conference on Learning Representations (ICLR), 2016.

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248–255, 2009.

[7] X. Dong, J. Huang, Y. Yang, and S. Yan. More is less: A more complicated network with less inference complexity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5840–5848, 2017.

[8] X. Dong and Y. Yang. Searching for a robust neural architecture in four gpu hours. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1761–1770, 2019.

[9] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang, D. Vetrov, and R. Salakhutdinov. Spatially adaptive computation time for residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1039–1048, 2017.

[10] A. Gordon, E. Eban, O. Nachum, B. Chen, H. Wu, T.-J. Yang, and E. Choi. MorphNet: Fast & simple resource-constrained structure learning of deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1586–1595, 2018.

[11] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally. EIE: efficient inference engine on compressed deep neural network. In The ACM/IEEE International Symposium on Computer Architecture (ISCA), pages 243–254, 2016.

[12] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations (ICLR), 2015.

[13] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural network. In The Conference on Neural Information Processing Systems (NeurIPS), pages 1135–1143, 2015.

[14] B. Hassibi and D. G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. In The Conference on Neural Information Processing Systems (NeurIPS), pages 164–171, 1993.

[15] J. L. Z. L. H. W. L.-J. L. He, Yihui and S. Han. AMC: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), pages 183–202, 2018.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 37(9):1904–1916, 2015.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.

[18] Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang. Soft filter pruning for accelerating deep convolutional neural networks. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2234–2240, 2018.

[19] Y. He, P. Liu, Z. Wang, and Y. Yang. Pruning filter via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4340–4349, 2019.

[20] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1389–1397, 2017.

[21] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In The Conference on Neural Information Processing Systems Workshop (NeurIPS-W), 2014.

[22] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4700–4708, 2017.

[23] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research (JMLR), 18(1):6869–6898, 2017.

[24] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In The International Conference on Machine Learning (ICML), pages 448–456, 2015.

[25] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In The Conference on Neural Information Processing Systems (NeurIPS), pages 2017–2025, 2015.

[26] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations (ICLR), 2017.

[27] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.

[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In The Conference on Neural Information Processing Systems (NeurIPS), pages 1097–1105, 2012.

[29] Y. LeCun, J. S. Denker, and S. A. Solla. Optimal brain damage. In The Conference on Neural Information Processing Systems (NeurIPS), pages 598–605, 1990.

[30] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. In International Conference on Learning Representations (ICLR), 2017.

[31] H. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations (ICLR), 2019.

[32] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2736–2744, 2017.

[33] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell. Rethinking the value of network pruning. In International Conference on Learning Representations (ICLR), 2018.

[34] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2017.

[35] C. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through l_0 regularization. In International Conference on Learning Representations (ICLR), 2018.

[36] C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations (ICLR), 2017.

[37] B. Minnehan and A. Savakis. Cascaded projection: End-to-end network compression and acceleration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10715–10724, 2019.

[38] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11264–11272, 2019.

[39] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Efficient neural architecture search via parameter sharing. In The International Conference on Machine Learning (ICML), pages 4092–4101, 2018.

[40] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le. Regularized evolution for image classifier architecture search. In AAAI Conference on Artificial Intelligence (AAAI), 2019.

[41] E. Tartaglione, S. Lepsøy, A. Fiandrotti, and G. Francini. Learning sparse neural networks via sensitivity-driven regularization. In The Conference on Neural Information Processing Systems (NeurIPS), pages 3878–3888, 2018.

[42] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks. In The Conference on Neural Information Processing Systems (NeurIPS), pages 2074–2082, 2016.

[43] J. Ye, X. Lu, Z. Lin, and J. Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. In International Conference on Learning Representations (ICLR), 2018.

[44] J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4133–4141, 2017.

[45] J. Yu and T. Huang. Network slimming by slimmable networks: Towards one-shot architecture search for channel numbers. arXiv preprint arXiv:1903.11728, 2019.

[46] S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In International Conference on Learning Representations (ICLR), 2017.

[47] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep convolutional networks for classification and detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 38(10):1943–1955, 2016.

[48] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations (ICLR), 2017.
