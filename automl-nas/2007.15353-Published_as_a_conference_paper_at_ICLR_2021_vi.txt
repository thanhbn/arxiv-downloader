# 2007.15353.pdf
# ƒê√£ chuy·ªÉn ƒë·ªïi t·ª´ PDF sang TXT
# ƒê∆∞·ªùng d·∫´n ngu·ªìn: /home/admin88/arxiv-downloader/automl-nas/2007.15353.pdf
# K√≠ch th∆∞·ªõc t·ªáp: 1205658 bytes

===============================================
N·ªòI DUNG T·ªÜP PDF
===============================================


--- TRANG 1 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
PH√ÅT TRI·ªÇN M·∫†NG S√ÇU√ô HI·ªÜU QU·∫¢
B·∫∞NG TH∆ØA TH·ªöT LI√äN T·ª§C C√ì C·∫§U TR√öC
Xin Yuan
University of Chicago
yuanx@uchicago.eduPedro Savarese
TTI-Chicago
savarese@ttic.eduMichael Maire
University of Chicago
mmaire@uchicago.edu
T√ìM T·∫ÆT
Ch√∫ng t√¥i ph√°t tri·ªÉn m·ªôt ph∆∞∆°ng ph√°p ph√°t tri·ªÉn ki·∫øn tr√∫c m·∫°ng s√¢u trong qu√° tr√¨nh hu·∫•n luy·ªán, ƒë∆∞·ª£c ƒëi·ªÅu khi·ªÉn b·ªüi s·ª± k·∫øt h·ª£p c√≥ nguy√™n t·∫Øc c·ªßa c√°c m·ª•c ti√™u ƒë·ªô ch√≠nh x√°c v√† ƒë·ªô th∆∞a th·ªõt. Kh√¥ng gi·ªëng nh∆∞ c√°c k·ªπ thu·∫≠t c·∫Øt t·ªâa ho·∫∑c t√¨m ki·∫øm ki·∫øn tr√∫c hi·ªán c√≥ ho·∫°t ƒë·ªông tr√™n c√°c m√¥ h√¨nh c√≥ k√≠ch th∆∞·ªõc ƒë·∫ßy ƒë·ªß ho·∫∑c ki·∫øn tr√∫c supernet, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i c√≥ th·ªÉ b·∫Øt ƒë·∫ßu t·ª´ m·ªôt ki·∫øn tr√∫c h·∫°t gi·ªëng nh·ªè, ƒë∆°n gi·∫£n v√† ph√°t tri·ªÉn c≈©ng nh∆∞ c·∫Øt t·ªâa ƒë·ªông c·∫£ c√°c l·ªõp v√† b·ªô l·ªçc. B·∫±ng c√°ch k·∫øt h·ª£p vi·ªác n·ªõi l·ªèng li√™n t·ª•c c·ªßa t·ªëi ∆∞u h√≥a c·∫•u tr√∫c m·∫°ng r·ªùi r·∫°c v·ªõi m·ªôt s∆° ƒë·ªì l·∫•y m·∫´u c√°c m·∫°ng con th∆∞a th·ªõt, ch√∫ng t√¥i t·∫°o ra c√°c m·∫°ng nh·ªè g·ªçn, ƒë∆∞·ª£c c·∫Øt t·ªâa, ƒë·ªìng th·ªùi c≈©ng gi·∫£m ƒë√°ng k·ªÉ chi ph√≠ t√≠nh to√°n c·ªßa vi·ªác hu·∫•n luy·ªán. V√≠ d·ª•, ch√∫ng t√¥i ƒë·∫°t ƒë∆∞·ª£c 49.7% ti·∫øt ki·ªám FLOPs suy lu·∫≠n v√† 47.4% ti·∫øt ki·ªám FLOPs hu·∫•n luy·ªán so v·ªõi ResNet-50 baseline tr√™n ImageNet, trong khi duy tr√¨ ƒë·ªô ch√≠nh x√°c top-1 75.2% ‚Äî t·∫•t c·∫£ m√† kh√¥ng c√≥ b·∫•t k·ª≥ giai ƒëo·∫°n fine-tuning chuy√™n d·ª•ng n√†o. C√°c th√≠ nghi·ªám tr√™n CIFAR, ImageNet, PASCAL VOC, v√† Penn Treebank, v·ªõi m·∫°ng t√≠ch ch·∫≠p cho ph√¢n lo·∫°i h√¨nh ·∫£nh v√† ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a, v√† m·∫°ng h·ªìi quy cho m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, ch·ª©ng minh r·∫±ng ch√∫ng t√¥i v·ª´a hu·∫•n luy·ªán nhanh h∆°n v·ª´a t·∫°o ra c√°c m·∫°ng hi·ªáu qu·∫£ h∆°n so v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa ho·∫∑c t√¨m ki·∫øm ki·∫øn tr√∫c c·∫°nh tranh.

1 GI·ªöI THI·ªÜU
M·∫°ng neural s√¢u l√† ph∆∞∆°ng ph√°p chi·∫øm ∆∞u th·∫ø cho nhi·ªÅu nhi·ªám v·ª• h·ªçc m√°y, bao g·ªìm ph√¢n lo·∫°i h√¨nh ·∫£nh (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng (Girshick, 2015; Liu et al., 2016), ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a (Long et al., 2015; Chen et al., 2017) v√† m√¥ h√¨nh h√≥a ng√¥n ng·ªØ (Zaremba et al., 2014; Vaswani et al., 2017; Devlin et al., 2019). C√°c m·∫°ng neural hi·ªán ƒë·∫°i b·ªã tham s·ªë h√≥a qu√° m·ª©c v√† vi·ªác hu·∫•n luy·ªán c√°c m·∫°ng l·ªõn h∆°n th∆∞·ªùng mang l·∫°i ƒë·ªô ch√≠nh x√°c t·ªïng qu√°t h√≥a c·∫£i thi·ªán. Nghi√™n c·ª©u g·∫ßn ƒë√¢y (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) minh h·ªça xu h∆∞·ªõng n√†y th√¥ng qua vi·ªác tƒÉng ƒë·ªô s√¢u v√† ƒë·ªô r·ªông c·ªßa m·∫°ng neural t√≠ch ch·∫≠p (CNN). Tuy nhi√™n, vi·ªác hu·∫•n luy·ªán ti√™u t·ªën nhi·ªÅu t√≠nh to√°n, v√† c√°c tri·ªÉn khai trong th·∫ø gi·ªõi th·ª±c th∆∞·ªùng b·ªã gi·ªõi h·∫°n b·ªüi ng√¢n s√°ch tham s·ªë v√† t√≠nh to√°n.

T√¨m ki·∫øm ki·∫øn tr√∫c neural (NAS) (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Savarese & Maire, 2019) v√† c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa m√¥ h√¨nh (Han et al., 2016; 2015; Guo et al., 2016) nh·∫±m gi·∫£m nh·ªØng g√°nh n·∫∑ng n√†y. NAS gi·∫£i quy·∫øt m·ªôt v·∫•n ƒë·ªÅ l√†m tƒÉng th√™m chi ph√≠ hu·∫•n luy·ªán: kh√¥ng gian kh·ªïng l·ªì c·ªßa c√°c ki·∫øn tr√∫c m·∫°ng c√≥ th·ªÉ c√≥. Trong khi vi·ªác ƒëi·ªÅu ch·ªânh th·ªß c√¥ng c√°c chi ti·∫øt ki·∫øn tr√∫c, ch·∫≥ng h·∫°n nh∆∞ c·∫•u tr√∫c k·∫øt n·ªëi c·ªßa c√°c l·ªõp t√≠ch ch·∫≠p, c√≥ th·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t (Iandola et al., 2016; Sifre & Mallat, 2014; Chollet, 2017; Howard et al., 2017; Zhang et al., 2018; Huang et al., 2018), m·ªôt c√°ch c√≥ nguy√™n t·∫Øc ƒë·ªÉ r√∫t ra c√°c thi·∫øt k·∫ø nh∆∞ v·∫≠y v·∫´n c√≤n kh√≥ n·∫Øm b·∫Øt. C√°c ph∆∞∆°ng ph√°p NAS nh·∫±m t·ª± ƒë·ªông h√≥a vi·ªác kh√°m ph√° c√°c ki·∫øn tr√∫c c√≥ th·ªÉ c√≥, t·∫°o ra m·ªôt thi·∫øt k·∫ø hi·ªáu qu·∫£ cho m·ªôt nhi·ªám v·ª• m·ª•c ti√™u d∆∞·ªõi c√°c r√†ng bu·ªôc t√†i nguy√™n th·ª±c t·∫ø. Tuy nhi√™n, trong qu√° tr√¨nh hu·∫•n luy·ªán, h·∫ßu h·∫øt c√°c ph∆∞∆°ng ph√°p NAS ho·∫°t ƒë·ªông tr√™n m·ªôt ki·∫øn tr√∫c supernet l·ªõn, bao g·ªìm c√°c th√†nh ph·∫ßn ·ª©ng vi√™n v∆∞·ª£t qu√° nh·ªØng th√†nh ph·∫ßn cu·ªëi c√πng ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ ƒë∆∞a v√†o m·∫°ng k·∫øt qu·∫£ (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Savarese & Maire, 2019). Do ƒë√≥, vi·ªác hu·∫•n luy·ªán d·ª±a tr√™n NAS c√≥ th·ªÉ th∆∞·ªùng k·ªπ l∆∞·ª°ng h∆°n, nh∆∞ng t·ªën k√©m t√≠nh to√°n h∆°n so v·ªõi vi·ªác hu·∫•n luy·ªán m·ªôt ki·∫øn tr√∫c ƒë∆∞·ª£c thi·∫øt k·∫ø th·ªß c√¥ng ƒë∆°n l·∫ª.

C√°c k·ªπ thu·∫≠t c·∫Øt t·ªâa m√¥ h√¨nh t∆∞∆°ng t·ª± t·∫≠p trung v√†o vi·ªác c·∫£i thi·ªán hi·ªáu qu·∫£ t√†i nguy√™n c·ªßa m·∫°ng neural trong qu√° tr√¨nh suy lu·∫≠n, c√≥ th·ªÉ v·ªõi chi ph√≠ tƒÉng chi ph√≠ hu·∫•n luy·ªán. C√°c chi·∫øn l∆∞·ª£c ph·ªï bi·∫øn nh·∫±m t·∫°o ra m·ªôt phi√™n b·∫£n nh·∫π h∆°n c·ªßa m·ªôt ki·∫øn tr√∫c m·∫°ng ƒë√£ cho b·∫±ng c√°ch lo·∫°i b·ªè c√°c tr·ªçng s·ªë ri√™ng l·∫ª (Han et al., 2015; 2016; Molchanov et al., 2017) ho·∫∑c c√°c t·∫≠p tham s·ªë c√≥ c·∫•u tr√∫c (Li et al., 2017; He et al., 2018; Luo et al., 2017). Tuy nhi√™n, ph·∫ßn l·ªõn c√°c ph∆∞∆°ng ph√°p n√†y hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh c√≥ k√≠ch th∆∞·ªõc ƒë·∫ßy ƒë·ªß tr∆∞·ªõc khi c·∫Øt t·ªâa v√†,

1arXiv:2007.15353v2  [cs.LG]  6 Jun 2023

--- TRANG 2 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
C·∫•p 1: Theo k√™nh
Epoch Hu·∫•n luy·ªán T=0,1,2‚Ä¶
C·∫•p 2: Theo l·ªõp‚Ä¶
(a) Ph√°t tri·ªÉn L·ªõp v√† B·ªô l·ªçc CNN
Epoch Hu·∫•n luy·ªán
FLOPs
Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i
C·∫Øt t·ªâa
T√¨m ki·∫øm Ki·∫øn tr√∫c (b) Chi ph√≠ Hu·∫•n luy·ªán theo Epoch
NAS C·∫Øt t·ªâa C·ªßa ch√∫ng t√¥i
FLOPs (c) T·ªïng Chi ph√≠ Hu·∫•n luy·ªán
H√¨nh 1: Ph√°t tri·ªÉn M·∫°ng trong qu√° tr√¨nh Hu·∫•n luy·ªán. Ch√∫ng t√¥i ƒë·ªãnh nghƒ©a m·ªôt kh√¥ng gian c·∫•u h√¨nh ki·∫øn tr√∫c v√† ƒë·ªìng th·ªùi th√≠ch ·ª©ng c·∫•u tr√∫c m·∫°ng v√† tr·ªçng s·ªë. (a)√Åp d·ª•ng ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i cho CNN, ch√∫ng t√¥i duy tr√¨ c√°c bi·∫øn ph·ª• tr·ª£ x√°c ƒë·ªãnh c√°ch ph√°t tri·ªÉn v√† c·∫Øt t·ªâa c·∫£ b·ªô l·ªçc (t·ª©c l√† theo k√™nh) v√† c√°c l·ªõp, tu√¢n theo c√°c r√†ng bu·ªôc t√†i nguy√™n th·ª±c t·∫ø. (b)B·∫±ng c√°ch b·∫Øt ƒë·∫ßu v·ªõi m·ªôt m·∫°ng nh·ªè v√† ph√°t tri·ªÉn k√≠ch th∆∞·ªõc c·ªßa n√≥, ch√∫ng t√¥i s·ª≠ d·ª•ng √≠t t√†i nguy√™n h∆°n trong c√°c epoch hu·∫•n luy·ªán ƒë·∫ßu, so v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa ho·∫∑c NAS. (c)Do ƒë√≥, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i gi·∫£m ƒë√°ng k·ªÉ t·ªïng chi ph√≠ t√≠nh to√°n c·ªßa vi·ªác hu·∫•n luy·ªán, trong khi cung c·∫•p c√°c m·∫°ng ƒë∆∞·ª£c hu·∫•n luy·ªán c√≥ k√≠ch th∆∞·ªõc v√† ƒë·ªô ch√≠nh x√°c t∆∞∆°ng ƒë∆∞∆°ng ho·∫∑c t·ªët h∆°n.

sau khi c·∫Øt t·ªâa, s·ª≠ d·ª•ng c√°c giai ƒëo·∫°n fine-tuning b·ªï sung ƒë·ªÉ duy tr√¨ ƒë·ªô ch√≠nh x√°c. Hubara et al. (2016) v√† Rastegari et al. (2016) ƒë·ªÅ xu·∫•t s·ª≠ d·ª•ng tr·ªçng s·ªë v√† k√≠ch ho·∫°t nh·ªã ph√¢n, cho ph√©p suy lu·∫≠n ƒë∆∞·ª£c h∆∞·ªüng l·ª£i t·ª´ chi ph√≠ l∆∞u tr·ªØ gi·∫£m v√† t√≠nh to√°n hi·ªáu qu·∫£ th√¥ng qua c√°c ph√©p to√°n ƒë·∫øm bit. Tuy nhi√™n, vi·ªác hu·∫•n luy·ªán v·∫´n li√™n quan ƒë·∫øn vi·ªác theo d√µi tr·ªçng s·ªë ƒë·ªô ch√≠nh x√°c cao c√πng v·ªõi c√°c x·∫•p x·ªâ ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n.

Ch√∫ng t√¥i c√≥ m·ªôt c√°i nh√¨n th·ªëng nh·∫•t v·ªÅ c·∫Øt t·ªâa v√† t√¨m ki·∫øm ki·∫øn tr√∫c, coi c·∫£ hai ƒë·ªÅu ho·∫°t ƒë·ªông tr√™n m·ªôt kh√¥ng gian c·∫•u h√¨nh, v√† ƒë·ªÅ xu·∫•t m·ªôt ph∆∞∆°ng ph√°p ph√°t tri·ªÉn ƒë·ªông c√°c m·∫°ng s√¢u b·∫±ng c√°ch c·∫•u h√¨nh l·∫°i ki·∫øn tr√∫c c·ªßa ch√∫ng m·ªôt c√°ch li√™n t·ª•c trong qu√° tr√¨nh hu·∫•n luy·ªán. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i kh√¥ng ch·ªâ t·∫°o ra c√°c m√¥ h√¨nh v·ªõi ƒë·∫∑c t√≠nh suy lu·∫≠n hi·ªáu qu·∫£, m√† c√≤n gi·∫£m chi ph√≠ t√≠nh to√°n c·ªßa vi·ªác hu·∫•n luy·ªán; xem H√¨nh 1. Thay v√¨ b·∫Øt ƒë·∫ßu v·ªõi m·ªôt m·∫°ng c√≥ k√≠ch th∆∞·ªõc ƒë·∫ßy ƒë·ªß ho·∫∑c m·ªôt supernet, ch√∫ng t√¥i b·∫Øt ƒë·∫ßu t·ª´ c√°c m·∫°ng h·∫°t gi·ªëng ƒë∆°n gi·∫£n v√† ƒëi·ªÅu ch·ªânh d·∫ßn (ph√°t tri·ªÉn v√† c·∫Øt t·ªâa) ch√∫ng. C·ª• th·ªÉ, ch√∫ng t√¥i tham s·ªë h√≥a m·ªôt kh√¥ng gian c·∫•u h√¨nh ki·∫øn tr√∫c v·ªõi c√°c bi·∫øn ch·ªâ th·ªã ƒëi·ªÅu ch·ªânh vi·ªác th√™m ho·∫∑c lo·∫°i b·ªè c√°c th√†nh ph·∫ßn c·∫•u tr√∫c. H√¨nh 2(a) cho th·∫•y m·ªôt v√≠ d·ª•, d∆∞·ªõi d·∫°ng kh√¥ng gian c·∫•u h√¨nh hai c·∫•p cho c√°c l·ªõp v√† b·ªô l·ªçc CNN. Ch√∫ng t√¥i cho ph√©p h·ªçc c√°c gi√° tr·ªã ch·ªâ th·ªã (v√† do ƒë√≥, c·∫•u tr√∫c ki·∫øn tr√∫c) th√¥ng qua vi·ªác k·∫øt h·ª£p vi·ªác n·ªõi l·ªèng li√™n t·ª•c v·ªõi l·∫•y m·∫´u nh·ªã ph√¢n, nh∆∞ ƒë∆∞·ª£c minh h·ªça trong H√¨nh 2(b). M·ªôt tham s·ªë nhi·ªát ƒë·ªô tr√™n m·ªói th√†nh ph·∫ßn ƒë·∫£m b·∫£o r·∫±ng c√°c c·∫•u tr√∫c t·ªìn t·∫°i l√¢u cu·ªëi c√πng ƒë∆∞·ª£c n∆∞·ªõng v√†o c·∫•u h√¨nh ki·∫øn tr√∫c r·ªùi r·∫°c c·ªßa m·∫°ng.

Trong khi AutoGrow ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t g·∫ßn ƒë√¢y (Wen et al., 2020) c≈©ng t√¨m c√°ch ph√°t tri·ªÉn m·∫°ng trong qu√° tr√¨nh hu·∫•n luy·ªán, ph∆∞∆°ng ph√°p k·ªπ thu·∫≠t c·ªßa ch√∫ng t√¥i kh√°c ƒë√°ng k·ªÉ v√† d·∫´n ƒë·∫øn c√°c l·ª£i th·∫ø th·ª±c t·∫ø ƒë√°ng k·ªÉ. ·ªû c·∫•p ƒë·ªô k·ªπ thu·∫≠t, AutoGrow th·ª±c hi·ªán m·ªôt quy tr√¨nh t√¨m ki·∫øm ki·∫øn tr√∫c tr√™n m·ªôt c·∫•u tr√∫c m√¥-ƒëun ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc, tu√¢n theo c√°c ch√≠nh s√°ch ph√°t tri·ªÉn v√† d·ª´ng ƒë∆∞·ª£c thi·∫øt k·∫ø th·ªß c√¥ng, d·ª±a tr√™n ƒë·ªô ch√≠nh x√°c. Ng∆∞·ª£c l·∫°i, ch√∫ng t√¥i tham s·ªë h√≥a c√°c c·∫•u h√¨nh ki·∫øn tr√∫c v√† s·ª≠ d·ª•ng gradient descent stochastic ƒë·ªÉ h·ªçc c√°c bi·∫øn ph·ª• tr·ª£ ch·ªâ ƒë·ªãnh c√°c th√†nh ph·∫ßn c·∫•u tr√∫c, trong khi ƒë·ªìng th·ªùi hu·∫•n luy·ªán tr·ªçng s·ªë trong c√°c th√†nh ph·∫ßn ƒë√≥. Ph∆∞∆°ng ph√°p k·ªπ thu·∫≠t ƒë·ªôc ƒë√°o c·ªßa ch√∫ng t√¥i mang l·∫°i c√°c l·ª£i th·∫ø sau:

‚Ä¢Hu·∫•n luy·ªán Nhanh b·∫±ng Ph√°t tri·ªÉn: Hu·∫•n luy·ªán l√† m·ªôt quy tr√¨nh th·ªëng nh·∫•t, t·ª´ ƒë√≥ ng∆∞·ªùi ta c√≥ th·ªÉ y√™u c·∫ßu c·∫•u tr√∫c m·∫°ng v√† tr·ªçng s·ªë li√™n quan b·∫•t c·ª© l√∫c n√†o. Kh√¥ng gi·ªëng nh∆∞ AutoGrow v√† ph·∫ßn l·ªõn c√°c k·ªπ thu·∫≠t c·∫Øt t·ªâa, fine-tuning ƒë·ªÉ t·ªëi ∆∞u h√≥a tr·ªçng s·ªë trong m·ªôt ki·∫øn tr√∫c ƒë∆∞·ª£c kh√°m ph√° l√† t√πy ch·ªçn. Ch√∫ng t√¥i ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ xu·∫•t s·∫Øc ngay c·∫£ khi kh√¥ng c√≥ b·∫•t k·ª≥ giai ƒëo·∫°n fine-tuning n√†o.

‚Ä¢Ph∆∞∆°ng ph√°p C√≥ nguy√™n t·∫Øc th√¥ng qua H·ªçc b·∫±ng Continuation + Sampling: Ch√∫ng t√¥i x√¢y d·ª±ng ph∆∞∆°ng ph√°p c·ªßa m√¨nh theo tinh th·∫ßn c·ªßa c√°c ph∆∞∆°ng ph√°p h·ªçc b·∫±ng continuation, n·ªõi l·ªèng m·ªôt b√†i to√°n t·ªëi ∆∞u h√≥a r·ªùi r·∫°c th√†nh m·ªôt x·∫•p x·ªâ li√™n t·ª•c ng√†y c√†ng c·ª©ng nh·∫Øc. Quan tr·ªçng, ch√∫ng t√¥i gi·ªõi thi·ªáu m·ªôt b∆∞·ªõc l·∫•y m·∫´u b·ªï sung cho chi·∫øn l∆∞·ª£c n√†y. T·ª´ s·ª± k·∫øt h·ª£p n√†y, ch√∫ng t√¥i c√≥ ƒë∆∞·ª£c t√≠nh linh ho·∫°t c·ªßa vi·ªác kh√°m ph√° m·ªôt ki·∫øn tr√∫c supernet, nh∆∞ng hi·ªáu qu·∫£ t√≠nh to√°n c·ªßa vi·ªác ch·ªâ th·ª±c s·ª± hu·∫•n luy·ªán m·ªôt m·∫°ng con ho·∫°t ƒë·ªông nh·ªè h∆°n nhi·ªÅu.

‚Ä¢M·ª•c ti√™u T·ªëi ∆∞u h√≥a Nh·∫≠n th·ª©c Ng√¢n s√°ch: C√°c tham s·ªë ƒëi·ªÅu ch·ªânh c·∫•u h√¨nh ki·∫øn tr√∫c c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√¥ng qua gradient descent. Ch√∫ng t√¥i c√≥ t√≠nh linh ho·∫°t ƒë·ªÉ x√¢y d·ª±ng nhi·ªÅu h√†m m·∫•t m√°t nh·∫°y c·∫£m v·ªõi t√†i nguy√™n, ch·∫≥ng h·∫°n nh∆∞ ƒë·∫øm t·ªïng FLOPs, theo c√°c tham s·ªë n√†y.

‚Ä¢Kh·∫£ nƒÉng √Åp d·ª•ng R·ªông: M·∫∑c d√π ch√∫ng t√¥i s·ª≠ d·ª•ng ph√°t tri·ªÉn ti·∫øn b·ªô c·ªßa CNN theo chi·ªÅu r·ªông v√† chi·ªÅu s√¢u nh∆∞ m·ªôt v√≠ d·ª• th√∫c ƒë·∫©y, k·ªπ thu·∫≠t c·ªßa ch√∫ng t√¥i √°p d·ª•ng cho h·∫ßu nh∆∞ b·∫•t k·ª≥ ki·∫øn tr√∫c neural n√†o. Ng∆∞·ªùi ta c√≥ t√≠nh linh ho·∫°t trong c√°ch tham s·ªë h√≥a kh√¥ng gian c·∫•u h√¨nh ki·∫øn tr√∫c. Ch√∫ng t√¥i c≈©ng cho th·∫•y k·∫øt qu·∫£ v·ªõi LSTM.

Ch√∫ng t√¥i ch·ª©ng minh nh·ªØng l·ª£i th·∫ø n√†y trong khi so s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p NAS v√† c·∫Øt t·ªâa g·∫ßn ƒë√¢y th√¥ng qua c√°c th√≠ nghi·ªám m·ªü r·ªông v·ªÅ ph√¢n lo·∫°i, ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a v√† m√¥ h√¨nh h√≥a ng√¥n ng·ªØ c·∫•p t·ª´.

2

--- TRANG 3 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
ùíôùíäùíèùíôùíêùíñùíïB·ªô l·ªçc Ho·∫°t ƒë·ªông = 11000‚Ä¶ùíôùíäùíèùíôùíêùíñùíï1100‚Ä¶
ùíôùíäùíèùíôùíêùíñùíïùíôùíäùíèùíôùíêùíñùíï1010‚Ä¶Ph√°t tri·ªÉnT√°ch r·ªùiPh√°t tri·ªÉnCh·ªâ th·ªã K√™nhCh·ªâ th·ªã K√™nhCh·ªâ th·ªã K√™nhT=0T=1Epoch Ph√°t tri·ªÉn T=2‚Ä¶C·∫•p 1: Kh√¥ng gian C·∫•u h√¨nh Theo k√™nh
C·∫•p 2: Kh√¥ng gian C·∫•u h√¨nh Theo l·ªõpB·ªô l·ªçc Ho·∫°t ƒë·ªông = 2B·ªô l·ªçc Ho·∫°t ƒë·ªông = 2
1000‚Ä¶Ch·ªâ th·ªã L·ªõpL·ªõp Ho·∫°t ƒë·ªông = 1ùíôùíäùíèùíôùíêùíñùíï1100‚Ä¶Ch·ªâ th·ªã L·ªõpL·ªõp Ho·∫°t ƒë·ªông = 2ùíôùíêùíñùíï0111‚Ä¶L·ªõp Ho·∫°t ƒë·ªông = 3Ch·ªâ th·ªã L·ªõpùíôùíäùíèPh√°t tri·ªÉnPh√°t tri·ªÉnT√°ch r·ªùiPh√°t tri·ªÉn

(a) Kh√¥ng gian C·∫•u h√¨nh Ki·∫øn tr√∫c cho CNN
Bi·∫øn M·∫∑t n·∫° C√≥ th·ªÉ hu·∫•n luy·ªánùíîKh√¥ng gian C·∫•u h√¨nh R·ªùi r·∫°cN·ªõi l·ªèng Li√™n t·ª•c Ti·∫øn b·ªôCh·ªâ th·ªã Nh·ªã ph√¢n
11000ùíí‚àºùë©ùíÜùíìùíè(ùùà(ùú∑ùíî))M·∫∑t n·∫° li√™n t·ª•cùúé(ùõΩùë†)L·∫•y m·∫´u000ùùàùú∑ùíî‚äôùíí (b) T·ªëi ∆∞u h√≥a v·ªõi Continuation C√≥ c·∫•u tr√∫c

H√¨nh 2: Khung K·ªπ thu·∫≠t. (a) Ch√∫ng t√¥i ƒë·ªãnh k·ª≥ t√°i c·∫•u tr√∫c m·ªôt CNN b·∫±ng c√°ch truy v·∫•n c√°c ch·ªâ th·ªã nh·ªã ph√¢n x√°c ƒë·ªãnh kh√¥ng gian c·∫•u h√¨nh hai c·∫•p cho b·ªô l·ªçc v√† l·ªõp. (b)ƒê·ªÉ l√†m cho t·ªëi ∆∞u h√≥a kh·∫£ thi trong khi ph√°t tri·ªÉn m·∫°ng, ch√∫ng t√¥i r√∫t ra c√°c ch·ªâ th·ªã nh·ªã ph√¢n n√†y t·ª´ c√°c bi·∫øn m·∫∑t n·∫° li√™n t·ª•c c√≥ th·ªÉ hu·∫•n luy·ªán. Ch√∫ng t√¥i s·ª≠ d·ª•ng m·ªôt ph·∫ßn m·ªü r·ªông c√≥ c·∫•u tr√∫c c·ªßa th∆∞a th·ªõt h√≥a li√™n t·ª•c (Savarese et al., 2020), k·∫øt h·ª£p v·ªõi l·∫•y m·∫´u. C√°c bi·∫øn ph·ª• tr·ª£ stochastic nh·ªã ph√¢n q, ƒë∆∞·ª£c l·∫•y m·∫´u theo œÉ(Œ≤s), t·∫°o ra c√°c th√†nh ph·∫ßn r·ªùi r·∫°c ho·∫°t ƒë·ªông t·∫°i m·ªôt th·ªùi ƒëi·ªÉm c·ª• th·ªÉ.

2 C√îNG TR√åNH LI√äN QUAN
C·∫Øt t·ªâa M·∫°ng. C√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa c√≥ th·ªÉ ƒë∆∞·ª£c chia th√†nh hai nh√≥m: nh·ªØng ph∆∞∆°ng ph√°p c·∫Øt t·ªâa tr·ªçng s·ªë ri√™ng l·∫ª v√† nh·ªØng ph∆∞∆°ng ph√°p c·∫Øt t·ªâa c√°c th√†nh ph·∫ßn c√≥ c·∫•u tr√∫c. C√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa d·ª±a tr√™n tr·ªçng s·ªë ri√™ng l·∫ª kh√°c nhau v·ªÅ ti√™u ch√≠ lo·∫°i b·ªè. V√≠ d·ª•, Han et al. (2015) ƒë·ªÅ xu·∫•t c·∫Øt t·ªâa tr·ªçng s·ªë m·∫°ng c√≥ ƒë·ªô l·ªõn nh·ªè v√† sau ƒë√≥ l∆∞·ª£ng t·ª≠ h√≥a nh·ªØng tr·ªçng s·ªë c√≤n l·∫°i (Han et al., 2016). Louizos et al. (2018) h·ªçc c√°c m·∫°ng th∆∞a th·ªõt b·∫±ng c√°ch x·∫•p x·ªâ ch√≠nh quy h√≥a ‚Ñì0 v·ªõi m·ªôt tham s·ªë h√≥a l·∫°i stochastic. Tuy nhi√™n, ch·ªâ tr·ªçng s·ªë th∆∞a th·ªõt th∆∞·ªùng ch·ªâ d·∫´n ƒë·∫øn tƒÉng t·ªëc tr√™n ph·∫ßn c·ª©ng chuy√™n d·ª•ng v·ªõi c√°c th∆∞ vi·ªán h·ªó tr·ª£.

Trong c√°c ph∆∞∆°ng ph√°p c√≥ c·∫•u tr√∫c, c·∫Øt t·ªâa ƒë∆∞·ª£c √°p d·ª•ng ·ªü c·∫•p ƒë·ªô neuron, k√™nh, ho·∫∑c th·∫≠m ch√≠ l·ªõp. V√≠ d·ª•, c·∫Øt t·ªâa L1 (Li et al., 2017) lo·∫°i b·ªè k√™nh d·ª±a tr√™n norm c·ªßa b·ªô l·ªçc c·ªßa ch√∫ng. He et al. (2018) s·ª≠ d·ª•ng th∆∞a th·ªõt nh√≥m ƒë·ªÉ l√†m m∆∞·ª£t qu√° tr√¨nh c·∫Øt t·ªâa sau hu·∫•n luy·ªán. MorphNet (Gordon et al., 2018) ch√≠nh quy h√≥a tr·ªçng s·ªë v·ªÅ zero cho ƒë·∫øn khi ch√∫ng ƒë·ªß nh·ªè sao cho c√°c k√™nh ƒë·∫ßu ra t∆∞∆°ng ·ª©ng ƒë∆∞·ª£c ƒë√°nh d·∫•u ƒë·ªÉ lo·∫°i b·ªè kh·ªèi m·∫°ng. Intrinsic Structured Sparsity (ISS) (Wen et al., 2018) ho·∫°t ƒë·ªông tr√™n LSTM (Hochreiter & Schmidhuber, 1997) b·∫±ng c√°ch lo·∫°i b·ªè t·∫≠p th·ªÉ c√°c c·ªôt v√† h√†ng c·ªßa ma tr·∫≠n tr·ªçng s·ªë th√¥ng qua LASSO nh√≥m. M·∫∑c d√π c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa c√≥ c·∫•u tr√∫c v√† thu·∫≠t to√°n c·ªßa ch√∫ng t√¥i c√≥ c√πng tinh th·∫ßn t·∫°o ra c√°c m√¥ h√¨nh hi·ªáu qu·∫£, ch√∫ng t√¥i c√≥ ƒë∆∞·ª£c ti·∫øt ki·ªám chi ph√≠ hu·∫•n luy·ªán b·∫±ng c√°ch ph√°t tri·ªÉn m·∫°ng t·ª´ c√°c ki·∫øn tr√∫c ban ƒë·∫ßu nh·ªè thay v√¨ c·∫Øt t·ªâa nh·ªØng m·∫°ng c√≥ k√≠ch th∆∞·ªõc ƒë·∫ßy ƒë·ªß.

T√¨m ki·∫øm Ki·∫øn tr√∫c Neural. C√°c ph∆∞∆°ng ph√°p NAS ƒë√£ c·∫£i thi·ªán ƒë√°ng k·ªÉ hi·ªáu su·∫•t ƒë·∫°t ƒë∆∞·ª£c b·ªüi c√°c m√¥ h√¨nh m·∫°ng nh·ªè. C√°c ph∆∞∆°ng ph√°p NAS ti√™n phong s·ª≠ d·ª•ng h·ªçc tƒÉng c∆∞·ªùng (Zoph et al., 2018; Zoph & Le, 2017) v√† thu·∫≠t to√°n di truy·ªÅn (Real et al., 2019; Xie & Yuille, 2017) ƒë·ªÉ t√¨m ki·∫øm c√°c kh·ªëi m·∫°ng c√≥ th·ªÉ chuy·ªÉn ƒë·ªïi c√≥ hi·ªáu su·∫•t v∆∞·ª£t qua nhi·ªÅu kh·ªëi ƒë∆∞·ª£c thi·∫øt k·∫ø th·ªß c√¥ng. Tuy nhi√™n, c√°c ph∆∞∆°ng ph√°p nh∆∞ v·∫≠y ƒë√≤i h·ªèi t√≠nh to√°n kh·ªïng l·ªì trong qu√° tr√¨nh t√¨m ki·∫øm ‚Äî th∆∞·ªùng l√† h√†ng ngh√¨n ng√†y GPU.

ƒê·ªÉ gi·∫£m chi ph√≠ t√≠nh to√°n, c√°c n·ªó l·ª±c g·∫ßn ƒë√¢y s·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t t√¨m ki·∫øm hi·ªáu qu·∫£ h∆°n, ch·∫≥ng h·∫°n nh∆∞ t·ªëi ∆∞u h√≥a d·ª±a tr√™n gradient tr·ª±c ti·∫øp (Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Tan et al., 2019; Cai et al., 2019; Wortsman et al., 2019). Tuy nhi√™n, h·∫ßu h·∫øt c√°c ph∆∞∆°ng ph√°p NAS th·ª±c hi·ªán t√¨m ki·∫øm trong kh√¥ng gian supernet ƒë√≤i h·ªèi nhi·ªÅu t√≠nh to√°n h∆°n so v·ªõi vi·ªác hu·∫•n luy·ªán c√°c ki·∫øn tr√∫c c√≥ k√≠ch th∆∞·ªõc th√¥ng th∆∞·ªùng.

Ph√°t tri·ªÉn M·∫°ng. Network Morphism (Wei et al., 2016) t√¨m ki·∫øm c√°c m·∫°ng s√¢u hi·ªáu qu·∫£ b·∫±ng c√°ch m·ªü r·ªông c√°c l·ªõp trong khi b·∫£o to√†n c√°c tham s·ªë. Autogrow ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t g·∫ßn ƒë√¢y (Wen et al., 2020) c√≥ ph∆∞∆°ng ph√°p AutoML ƒë·ªÉ ph√°t tri·ªÉn c√°c l·ªõp. C√°c ph∆∞∆°ng ph√°p n√†y ho·∫∑c y√™u c·∫ßu m·ªôt ch√≠nh s√°ch ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát ƒë·ªÉ d·ª´ng ph√°t tri·ªÉn (v√≠ d·ª•, sau m·ªôt s·ªë l·ªõp c·ªë ƒë·ªãnh) ho·∫∑c d·ª±a v√†o vi·ªác ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c trong qu√° tr√¨nh hu·∫•n luy·ªán, ph√°t sinh chi ph√≠ t√≠nh to√°n b·ªï sung ƒë√°ng k·ªÉ.

H·ªçc b·∫±ng Continuation. C√°c ph∆∞∆°ng ph√°p continuation th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ x·∫•p x·ªâ c√°c b√†i to√°n t·ªëi ∆∞u h√≥a kh√≥ gi·∫£i b·∫±ng c√°ch d·∫ßn d·∫ßn tƒÉng ƒë·ªô kh√≥ c·ªßa m·ª•c ti√™u c∆° b·∫£n, v√≠ d·ª• b·∫±ng c√°ch √°p d·ª•ng c√°c n·ªõi l·ªèng d·∫ßn cho c√°c b√†i to√°n nh·ªã ph√¢n. Wu et al. (2019); Xie et al. (2019b; 2020) s·ª≠ d·ª•ng gumbel-softmax (Jang et al., 2017) ƒë·ªÉ lan truy·ªÅn ng∆∞·ª£c l·ªói trong qu√° tr√¨nh t√¨m ki·∫øm ki·∫øn tr√∫c v√† th∆∞a th·ªõt h√≥a ƒë·∫∑c tr∆∞ng kh√¥ng gian. Savarese et al. (2020) ƒë·ªÅ xu·∫•t th∆∞a th·ªõt h√≥a li√™n t·ª•c ƒë·ªÉ tƒÉng t·ªëc c·∫Øt t·ªâa v√† t√¨m ki·∫øm v√© (Frankle & Carbin, 2019). M·∫∑c d√π th√†nh c√¥ng c·ªßa c√°c ph∆∞∆°ng ph√°p continuation trong vi·ªác t·∫°o ra c√°c m·∫°ng th∆∞a th·ªõt khi ho√†n th√†nh hu·∫•n luy·ªán, ch√∫ng kh√¥ng ho·∫°t ƒë·ªông tr√™n c√°c m·∫°ng th∆∞a th·ªõt

3

--- TRANG 4 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
trong qu√° tr√¨nh hu·∫•n luy·ªán v√† thay v√†o ƒë√≥ l√†m vi·ªác v·ªõi m·ªôt n·ªõi l·ªèng c√≥ gi√° tr·ªã th·ª±c. Vi·ªác ho√£n lo·∫°i b·ªè th·ª±c t·∫ø c√°c th√†nh ph·∫ßn g·∫ßn zero ngƒÉn ch·∫∑n vi·ªác √°p d·ª•ng ng√¢y th∆° c·ªßa c√°c ph∆∞∆°ng ph√°p n√†y kh·ªèi vi·ªác gi·∫£m chi ph√≠ hu·∫•n luy·ªán.

3 PH∆Ø∆†NG PH√ÅP
3.1 KH√îNG GIAN C·∫§U H√åNH KI·∫æN TR√öC
M·ªôt topology m·∫°ng c√≥ th·ªÉ ƒë∆∞·ª£c xem nh∆∞ m·ªôt ƒë·ªì th·ªã acyclic c√≥ h∆∞·ªõng bao g·ªìm m·ªôt chu·ªói c√°c n√∫t ƒë∆∞·ª£c s·∫Øp x·∫øp. M·ªói n√∫t x(i)
in l√† m·ªôt ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o v√† m·ªói c·∫°nh l√† m·ªôt √¥ t√≠nh to√°n v·ªõi c√°c si√™u tham s·ªë c√≥ c·∫•u tr√∫c (v√≠ d·ª•, s·ªë b·ªô l·ªçc v√† l·ªõp trong m·∫°ng t√≠ch ch·∫≠p). M·ªôt kh√¥ng gian c·∫•u h√¨nh ki·∫øn tr√∫c c√≥ th·ªÉ ƒë∆∞·ª£c tham s·ªë h√≥a b·∫±ng c√°ch li√™n k·∫øt m·ªôt bi·∫øn m·∫∑t n·∫° m‚àà {0,1} v·ªõi m·ªói √¥ t√≠nh to√°n (c·∫°nh), cho ph√©p ƒë·ªông l·ª±c c·∫Øt t·ªâa (m= 1‚Üí0) v√† ph√°t tri·ªÉn (m= 0‚Üí1) trong th·ªùi gian hu·∫•n luy·ªán.

Nh∆∞ m·ªôt v√≠ d·ª• minh h·ªça, ch√∫ng t√¥i xem x√©t m·ªôt kh√¥ng gian c·∫•u h√¨nh hai c·∫•p cho ki·∫øn tr√∫c CNN, ƒë∆∞·ª£c m√¥ t·∫£ trong H√¨nh 2(a), cho ph√©p ph√°t tri·ªÉn ƒë·ªông m·∫°ng c·∫£ theo chi·ªÅu r·ªông (theo k√™nh) v√† chi·ªÅu s√¢u (theo l·ªõp). C√°c kh√¥ng gian c·∫•u h√¨nh thay th·∫ø l√† c√≥ th·ªÉ; ch√∫ng t√¥i ƒë·ªÉ l·∫°i trong Ph·ª• l·ª•c chi ti·∫øt v·ªÅ c√°ch ch√∫ng t√¥i tham s·ªë h√≥a thi·∫øt k·∫ø c·ªßa ki·∫øn tr√∫c LSTM.

Kh√¥ng gian C·∫•u h√¨nh K√™nh CNN: ƒê·ªëi v·ªõi m·ªôt l·ªõp t√≠ch ch·∫≠p v·ªõi lin k√™nh ƒë·∫ßu v√†o, lout k√™nh ƒë·∫ßu ra (b·ªô l·ªçc) v√† kernel c√≥ k√≠ch th∆∞·ªõc k√ók, ƒë·∫∑c tr∆∞ng ƒë·∫ßu ra th·ª© i ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n b·ªô l·ªçc th·ª© i, t·ª©c l√† cho i‚àà {1, . . . , l out}:
x(i)
out=f(xin,F(i)¬∑m(i)
c), (1)
trong ƒë√≥ m(i)
c‚àà {0,1} l√† m·ªôt tham s·ªë nh·ªã ph√¢n lo·∫°i b·ªè k√™nh ƒë·∫ßu ra th·ª© i khi ƒë∆∞·ª£c ƒë·∫∑t v·ªÅ zero v√† f bi·ªÉu th·ªã ph√©p to√°n t√≠ch ch·∫≠p. m(i)
c ƒë∆∞·ª£c chia s·∫ª qua m·ªôt b·ªô l·ªçc v√† broadcast ƒë·∫øn c√πng h√¨nh d·∫°ng v·ªõi tensor b·ªô l·ªçc F(i), cho ph√©p ph√°t tri·ªÉn/c·∫Øt t·ªâa to√†n b·ªô b·ªô l·ªçc. Nh∆∞ H√¨nh 2(a) (tr√™n) cho th·∫•y, ch√∫ng t√¥i b·∫Øt ƒë·∫ßu t·ª´ m·ªôt c·∫•u h√¨nh k√™nh m·ªèng. Sau ƒë√≥ ch√∫ng t√¥i truy v·∫•n c√°c bi·∫øn ch·ªâ th·ªã v√† th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi tr·∫°ng th√°i: (1) Khi l·∫≠t m·ªôt bi·∫øn ch·ªâ th·ªã t·ª´ 0 th√†nh 1 l·∫ßn ƒë·∫ßu ti√™n, ch√∫ng t√¥i ph√°t tri·ªÉn m·ªôt b·ªô l·ªçc ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n v√† n·ªëi n√≥ v√†o m·∫°ng. (2) N·∫øu m·ªôt ch·ªâ th·ªã l·∫≠t t·ª´ 1 th√†nh 0, ch√∫ng t√¥i t·∫°m th·ªùi t√°ch b·ªô l·ªçc t∆∞∆°ng ·ª©ng kh·ªèi ƒë·ªì th·ªã t√≠nh to√°n; n√≥ s·∫Ω ƒë∆∞·ª£c ph√°t tri·ªÉn l·∫°i v·ªÅ v·ªã tr√≠ ban ƒë·∫ßu n·∫øu ch·ªâ th·ªã c·ªßa n√≥ l·∫≠t tr·ªü l·∫°i 1, ho·∫∑c ng∆∞·ª£c l·∫°i s·∫Ω b·ªã c·∫Øt t·ªâa vƒ©nh vi·ªÖn v√†o cu·ªëi hu·∫•n luy·ªán. (3) ƒê·ªëi v·ªõi c√°c tr∆∞·ªùng h·ª£p kh√°c, c√°c b·ªô l·ªçc t∆∞∆°ng ·ª©ng ho·∫∑c t·ªìn t·∫°i v√† ti·∫øp t·ª•c hu·∫•n luy·ªán ho·∫∑c v·∫´n b·ªã t√°ch ch·ªù truy v·∫•n ti·∫øp theo ƒë·∫øn ch·ªâ th·ªã c·ªßa ch√∫ng. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i t·ª± ƒë·ªông h√≥a s·ª± ti·∫øn h√≥a ki·∫øn tr√∫c, v·ªõi ƒëi·ªÅu ki·ªán ch√∫ng t√¥i c√≥ th·ªÉ hu·∫•n luy·ªán c√°c ch·ªâ th·ªã.

Kh√¥ng gian C·∫•u h√¨nh L·ªõp CNN: ƒê·ªÉ ph√°t tri·ªÉn ƒë·ªô s√¢u m·∫°ng, ch√∫ng t√¥i thi·∫øt k·∫ø m·ªôt kh√¥ng gian c·∫•u h√¨nh l·ªõp trong ƒë√≥ m·ªôt m·∫°ng n√¥ng ban ƒë·∫ßu s·∫Ω d·∫ßn m·ªü r·ªông th√†nh m·ªôt m√¥ h√¨nh s√¢u ƒë∆∞·ª£c hu·∫•n luy·ªán, nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong H√¨nh 2(a) (d∆∞·ªõi). T∆∞∆°ng t·ª± nh∆∞ kh√¥ng gian c·∫•u h√¨nh k√™nh, n∆°i b·ªô l·ªçc ph·ª•c v·ª• nh∆∞ c√°c ƒë∆°n v·ªã c·∫•u tr√∫c c∆° b·∫£n, ch√∫ng t√¥i y√™u c·∫ßu m·ªôt c√¥ng th·ª©c th·ªëng nh·∫•t ƒë·ªÉ h·ªó tr·ª£ vi·ªác ph√°t tri·ªÉn c√°c m·∫°ng ph·ªï bi·∫øn v·ªõi k·∫øt n·ªëi shortcut (v√≠ d·ª•, ResNet) v√† kh√¥ng c√≥ (v√≠ d·ª•, plain net gi·ªëng VGG). Ch√∫ng t√¥i ƒë·∫ßu ti√™n gi·ªõi thi·ªáu m·ªôt l·ªõp abstract flayer nh∆∞ m·ªôt ƒë∆°n v·ªã c·∫•u tr√∫c c∆° b·∫£n, ho·∫°t ƒë·ªông tr√™n c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o xin v√† t·∫°o ra c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu ra xout. flayer c√≥ th·ªÉ ƒë∆∞·ª£c kh·ªüi t·∫°o nh∆∞ c√°c l·ªõp t√≠ch ch·∫≠p cho plain net ho·∫∑c c√°c kh·ªëi residual cho ResNet, t∆∞∆°ng ·ª©ng. Ch√∫ng t√¥i ƒë·ªãnh nghƒ©a kh√¥ng gian c·∫•u h√¨nh l·ªõp nh∆∞:
xout=g(xin;flayer¬∑m(j)
l) =(
flayer(xin),n·∫øu m(j)
l= 1
xin, n·∫øu m(j)
l= 0, (2)
trong ƒë√≥ m(j)
l‚àà {0,1} l√† ch·ªâ th·ªã nh·ªã ph√¢n cho l·ªõp th·ª© j flayer, v·ªõi ƒë√≥ ch√∫ng t√¥i th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi tr·∫°ng th√°i t∆∞∆°ng t·ª± nh∆∞ kh√¥ng gian c·∫•u h√¨nh k√™nh. C√°c ch·ªâ th·ªã l·ªõp c√≥ ∆∞u ti√™n h∆°n c√°c ch·ªâ th·ªã k√™nh: n·∫øu m(j)
l ƒë∆∞·ª£c ƒë·∫∑t l√† 0, t·∫•t c·∫£ b·ªô l·ªçc ch·ª©a trong l·ªõp t∆∞∆°ng ·ª©ng s·∫Ω b·ªã t√°ch, b·∫•t k·ªÉ tr·∫°ng th√°i ch·ªâ th·ªã c·ªßa ch√∫ng. Ch√∫ng t√¥i kh√¥ng t√°ch c√°c l·ªõp th·ª±c hi·ªán thay ƒë·ªïi ƒë·ªô ph√¢n gi·∫£i (v√≠ d·ª•, t√≠ch ch·∫≠p c√≥ stride).

3.2 PH√ÅT TRI·ªÇN V·ªöI TH∆ØA TH·ªöT H√ìA LI√äN T·ª§C C√ì C·∫§U TR√öC
Ch√∫ng t√¥i c√≥ th·ªÉ t·ªëi ∆∞u h√≥a s·ª± ƒë√°nh ƒë·ªïi gi·ªØa ƒë·ªô ch√≠nh x√°c v√† th∆∞a th·ªõt c√≥ c·∫•u tr√∫c b·∫±ng c√°ch xem x√©t m·ª•c ti√™u:
min
w,mc,l,flayerLE(g(f(x;w‚äômc);flayer¬∑ml)) +Œª1‚à•mc‚à•0+Œª2‚à•ml‚à•0, (3)

4

--- TRANG 5 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
trong ƒë√≥ f l√† ph√©p to√°n trong Eq. (1) ho·∫∑c Eq. (9) (trong Ph·ª• l·ª•c A.6), trong khi g ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong Eq. (2). w‚äômc v√† flayer¬∑ml l√† c√°c bi·ªÉu th·ª©c t·ªïng qu√°t c·ªßa b·ªô l·ªçc v√† l·ªõp th∆∞a th·ªõt c√≥ c·∫•u tr√∫c v√† LE bi·ªÉu th·ªã m·ªôt h√†m m·∫•t m√°t (v√≠ d·ª•, cross-entropy loss cho ph√¢n lo·∫°i). C√°c s·ªë h·∫°ng ‚Ñì0 khuy·∫øn kh√≠ch th∆∞a th·ªõt, trong khi Œª1,2 l√† c√°c tham s·ªë ƒë√°nh ƒë·ªïi gi·ªØa LE v√† c√°c ph·∫°t ‚Ñì0.

Thu·∫≠t to√°n 1 : T·ªëi ∆∞u h√≥a
ƒê·∫ßu v√†o: D·ªØ li·ªáu X=(xi)n
i=1, nh√£n Y=(yi)n
i=1
ƒê·∫ßu ra: M√¥ h√¨nh hi·ªáu qu·∫£ ƒë√£ ph√°t tri·ªÉn G
Kh·ªüi t·∫°o: G,w,u,Œªbase
1 v√† Œªbase
2.
ƒê·∫∑t ts l√† t·∫•t c·∫£ vector 0 li√™n k·∫øt v·ªõi c√°c h√†m œÉ.
for epoch = 1 to T do
ƒê√°nh gi√° ƒë·ªô th∆∞a th·ªõt uG c·ªßa G v√† t√≠nh
‚àÜu=u‚àíuG
C·∫≠p nh·∫≠t Œª1‚ÜêŒªbase
1¬∑‚àÜu;Œª2‚ÜêŒªbase
2¬∑‚àÜu
trong Eq. (6) s·ª≠ d·ª•ng Eq. (4)
for r= 1 to R do
L·∫•y m·∫´u mini-batch xi, yi t·ª´ X,Y
Hu·∫•n luy·ªán G s·ª≠ d·ª•ng Eq. (6) v·ªõi SGD
end for
L·∫•y m·∫´u ch·ªâ th·ªã qc,l‚àºBern(œÉ(Œ≤sc,l))
v√† ghi l·∫°i ch·ªâ s·ªë idx n∆°i gi√° tr·ªã q l√† 1.
C·∫≠p nh·∫≠t ts[idx] =ts[idx] + 1
C·∫≠p nh·∫≠t Œ≤ s·ª≠ d·ª•ng Eq. (7)
end for
return G

Ph√°t tri·ªÉn Nh·∫≠n th·ª©c Ng√¢n s√°ch. Trong th·ª±c t·∫ø, vi·ªác s·ª≠ d·ª•ng Eq. (3) c√≥ th·ªÉ y√™u c·∫ßu t√¨m ki·∫øm l∆∞·ªõi tr√™n Œª1 v√† Œª2 cho ƒë·∫øn khi m·ªôt m·∫°ng v·ªõi ƒë·ªô th∆∞a th·ªõt mong mu·ªën ƒë∆∞·ª£c t·∫°o ra. ƒê·ªÉ tr√°nh m·ªôt quy tr√¨nh t·ªën k√©m nh∆∞ v·∫≠y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt qu√° tr√¨nh ph√°t tri·ªÉn nh·∫≠n th·ª©c ng√¢n s√°ch, ƒë∆∞·ª£c h∆∞·ªõng d·∫´n b·ªüi m·ªôt ng√¢n s√°ch m·ª•c ti√™u v·ªÅ tham s·ªë m√¥ h√¨nh ho·∫∑c FLOPs. Thay v√¨ coi Œª1 v√† Œª2 nh∆∞ c√°c h·∫±ng s·ªë, ch√∫ng t√¥i ƒë·ªãnh k·ª≥ c·∫≠p nh·∫≠t ch√∫ng nh∆∞:
Œª1‚ÜêŒªbase
1¬∑‚àÜu, Œª 2‚ÜêŒªbase
2¬∑‚àÜu , (4)
trong ƒë√≥ ‚àÜu ƒë∆∞·ª£c t√≠nh nh∆∞ ƒë·ªô th∆∞a th·ªõt m·ª•c ti√™u u tr·ª´ ƒë·ªô th∆∞a th·ªõt m·∫°ng hi·ªán t·∫°i uG, v√† Œªbase
1, Œªbase
2 l√† c√°c h·∫±ng s·ªë c∆° s·ªü ban ƒë·∫ßu. Trong c√°c giai ƒëo·∫°n ph√°t tri·ªÉn ƒë·∫ßu, v√¨ m·∫°ng qu√° th∆∞a th·ªõt v√† ‚àÜu √¢m, b·ªô t·ªëi ∆∞u s·∫Ω ƒë·∫©y m·∫°ng v·ªÅ m·ªôt tr·∫°ng th√°i v·ªõi nhi·ªÅu dung l∆∞·ª£ng h∆°n (r·ªông h∆°n/s√¢u h∆°n). Hi·ªáu ·ª©ng ch√≠nh quy h√≥a d·∫ßn y·∫øu ƒëi khi ƒë·ªô th∆∞a th·ªõt c·ªßa m·∫°ng ti·∫øp c·∫≠n ng√¢n s√°ch (v√† ‚àÜu ti·∫øp c·∫≠n zero). ƒêi·ªÅu n√†y cho ph√©p ch√∫ng t√¥i th√≠ch ·ª©ng ph√°t tri·ªÉn m·∫°ng v√† t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh m·ª©c ƒë·ªô th∆∞a th·ªõt c·ªßa n√≥ trong khi ƒë·ªìng th·ªùi hu·∫•n luy·ªán tr·ªçng s·ªë m√¥ h√¨nh. Ph·ª• l·ª•c A.1 cung c·∫•p ph√¢n t√≠ch chi ti·∫øt h∆°n. C√°c th√≠ nghi·ªám c·ªßa ch√∫ng t√¥i m·∫∑c ƒë·ªãnh ƒë·ªãnh nghƒ©a ng√¢n s√°ch b·∫±ng s·ªë l∆∞·ª£ng tham s·ªë, nh∆∞ng c≈©ng ƒëi·ªÅu tra c√°c kh√°i ni·ªám ng√¢n s√°ch thay th·∫ø.

H·ªçc b·∫±ng Continuation. M·ªôt v·∫•n ƒë·ªÅ kh√°c trong vi·ªác t·ªëi ∆∞u h√≥a Eq. (3) l√† ‚à•mc‚à•0 v√† ‚à•ml‚à•0 l√†m cho b√†i to√°n tr·ªü n√™n kh√≥ gi·∫£i v·ªÅ m·∫∑t t√≠nh to√°n do b·∫£n ch·∫•t t·ªï h·ª£p c·ªßa c√°c tr·∫°ng th√°i nh·ªã ph√¢n. ƒê·ªÉ l√†m cho kh√¥ng gian c·∫•u h√¨nh li√™n t·ª•c v√† t·ªëi ∆∞u h√≥a kh·∫£ thi, ch√∫ng t√¥i m∆∞·ª£n kh√°i ni·ªám h·ªçc b·∫±ng continuation (Cao et al., 2017; Wu et al., 2019; Savarese et al., 2020; Xie et al., 2020). Ch√∫ng t√¥i tham s·ªë h√≥a l·∫°i m nh∆∞ d·∫•u nh·ªã ph√¢n c·ªßa m·ªôt bi·∫øn li√™n t·ª•c s: sign(s) l√† 1 n·∫øu s >0 v√† 0 n·∫øu s <0. Ch√∫ng t√¥i vi·∫øt l·∫°i m·ª•c ti√™u trong Eq. (3) nh∆∞:
min
w,sc,lÃ∏=0,flayerLE
g
f(x;w‚äôsign(sc));flayer¬∑sign(sl)
+Œª1‚à•sign(sc)‚à•1+Œª2‚à•sign(sl)‚à•1.(5)
Ch√∫ng t√¥i t·∫•n c√¥ng b√†i to√°n t·ªëi ∆∞u h√≥a c·ª©ng v√† kh√¥ng li√™n t·ª•c trong Eq. (5) b·∫±ng c√°ch b·∫Øt ƒë·∫ßu v·ªõi m·ªôt m·ª•c ti√™u d·ªÖ h∆°n tr·ªü n√™n kh√≥ h∆°n khi hu·∫•n luy·ªán ti·∫øn tri·ªÉn. Ch√∫ng t√¥i s·ª≠ d·ª•ng m·ªôt chu·ªói c√°c h√†m c√≥ gi·ªõi h·∫°n l√† ph√©p to√°n d·∫•u: ƒë·ªëi v·ªõi b·∫•t k·ª≥ sÃ∏= 0, limŒ≤‚Üí‚àûœÉ(Œ≤s) = sign( s) n·∫øu œÉ l√† h√†m sigmoid ho·∫∑c limŒ≤‚Üí0œÉ(Œ≤s) = sign( s) n·∫øu œÉ l√† gumbel-softmax exp((‚àílog(s0)+g1(s))/Œ≤) P
j‚àà{0,1} exp((‚àílog(sj)+gj(s))/Œ≤) (Jang et al., 2017), trong ƒë√≥ Œ≤ > 0 l√† m·ªôt tham s·ªë nhi·ªát ƒë·ªô v√† g0,1 l√† gumbel. B·∫±ng c√°ch ƒë·ªãnh k·ª≥ thay ƒë·ªïi Œ≤, œÉ(Œ≤s) tr·ªü n√™n kh√≥ t·ªëi ∆∞u h√≥a h∆°n, trong khi c√°c m·ª•c ti√™u h·ªôi t·ª• v·ªÅ m·ª•c ti√™u r·ªùi r·∫°c ban ƒë·∫ßu.

Duy tr√¨ Th∆∞a th·ªõt h√≥a B·∫•t c·ª© l√∫c n√†o. M·∫∑c d√π c√°c ph∆∞∆°ng ph√°p continuation c√≥ th·ªÉ l√†m cho t·ªëi ∆∞u h√≥a kh·∫£ thi, ch√∫ng ch·ªâ ti·∫øn h√†nh th∆∞a th·ªõt h√≥a th√¥ng qua m·ªôt ti√™u ch√≠ ng∆∞·ª°ng trong giai ƒëo·∫°n suy lu·∫≠n. Trong tr∆∞·ªùng h·ª£p n√†y, ki·∫øn tr√∫c th·ªùi gian hu·∫•n luy·ªán l√† d√†y ƒë·∫∑c v√† kh√¥ng ph√π h·ª£p trong b·ªëi c·∫£nh ph√°t tri·ªÉn m·ªôt m·∫°ng. ƒê·ªÉ gi·∫£m hi·ªáu qu·∫£ chi ph√≠ t√≠nh to√°n c·ªßa vi·ªác hu·∫•n luy·ªán, ch√∫ng t√¥i duy tr√¨ m·ªôt ki·∫øn tr√∫c th∆∞a th·ªõt b·∫±ng c√°ch gi·ªõi thi·ªáu m·ªôt bi·∫øn ph·ª• tr·ª£ ƒë∆∞·ª£c l·∫•y m·∫´u 0-1 q d·ª±a tr√™n gi√° tr·ªã x√°c su·∫•t œÉ(Œ≤s). M·ª•c ti√™u cu·ªëi c√πng c·ªßa ch√∫ng t√¥i tr·ªü th√†nh:
min
w,sc,lÃ∏=0,flayerLE
g
f(x;w‚äôœÉ(Œ≤sc)‚äôqc);flayer¬∑œÉ(Œ≤sl)¬∑ql
+Œª1‚à•œÉ(Œ≤sc)‚à•1+Œª2‚à•œÉ(Œ≤sl)‚à•1,(6)
trong ƒë√≥ qc v√† ql l√† c√°c bi·∫øn ng·∫´u nhi√™n ƒë∆∞·ª£c l·∫•y m·∫´u t·ª´ Bern(œÉ(Œ≤sc)) v√† Bern(œÉ(Œ≤sl)), hi·ªáu qu·∫£ duy tr√¨ th∆∞a th·ªõt h√≥a b·∫•t c·ª© l√∫c n√†o v√† tr√°nh ng∆∞·ª°ng kh√¥ng t·ªëi ∆∞u, nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong H√¨nh 2(b).

B·ªô l·∫≠p l·ªãch Nhi·ªát ƒë·ªô C·∫£i ti·∫øn. Trong c√°c ph∆∞∆°ng ph√°p continuation hi·ªán c√≥, gi√° tr·ªã Œ≤ ban ƒë·∫ßu th∆∞·ªùng ƒë∆∞·ª£c ƒë·∫∑t l√† Œ≤0= 1 v√† m·ªôt b·ªô l·∫≠p l·ªãch ƒë∆∞·ª£c s·ª≠ d·ª•ng v√†o cu·ªëi m·ªói epoch hu·∫•n luy·ªán ƒë·ªÉ c·∫≠p nh·∫≠t Œ≤ trong t·∫•t c·∫£ c√°c h√†m k√≠ch ho·∫°t œÉ, th∆∞·ªùng theo Œ≤=Œ≤0¬∑Œ≥t, trong ƒë√≥ t l√† epoch hi·ªán t·∫°i v√† Œ≥ l√† m·ªôt si√™u tham s·ªë (>1 khi œÉ l√† h√†m sigmoid, <1 khi œÉ l√† gumbel softmax). C·∫£ Œ≥ v√† t ƒë·ªÅu ki·ªÉm so√°t t·ªëc ƒë·ªô

5

--- TRANG 6 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
B·∫£ng 1: So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa k√™nh L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018), ThiNet (Luo et al., 2017), Provable (Liebenwein et al., 2020) v√† BAR (Lemaire et al., 2019) tr√™n CIFAR-10.

| M√¥ h√¨nh | Ph∆∞∆°ng ph√°p | Val Acc(%) | Params(M) | FLOPs(%) | Ti·∫øt ki·ªám Chi ph√≠ Hu·∫•n luy·ªán(√ó) |
|---------|-------------|------------|-----------|----------|----------------------------------|
|         | Original    | 92.9 ¬±0.16 (-0.0) | 14.99 (100%) | 100 | 1.0 √ó |
|         | L1-Pruning  | 91.8 ¬±0.12 (-1.1) | 2.98 (19.9%) | 19.9 | 2.5 √ó |
| VGG     | SoftNet     | 92.1 ¬±0.09 (-0.8) | 5.40 (36.0%) | 36.1 | 1.6 √ó |
| -16     | ThiNet      | 90.8 ¬±0.11 (-2.1) | 5.40 (36.0%) | 36.1 | 1.6 √ó |
|         | Provable    | 92.4 ¬±0.12 (-0.5) | 0.85 (5.7%) | 15.0 | 3.5√ó |
|         | Ours        | 92.50¬±0.10 (-0.4) | 0.754 ¬±0.005 (5.0%) | 13.55 ¬±0.03 | 4.95 ¬±0.17√ó |
|         | Original    | 91.3 ¬±0.12 (-0.0) | 0.27 (100%) | 100 | 1.0 √ó |
|         | L1-Pruning  | 90.9 ¬±0.10 (-0.4) | 0.15 (55.6%) | 55.4 | 1.1 √ó |
| ResNet  | SoftNet     | 90.8 ¬±0.13 (-0.5) | 0.14 (53.6%) | 50.6 | 1.2√ó |
| -20     | ThiNet      | 89.2 ¬±0.18 (-2.1) | 0.18 (67.1%) | 67.3 | 1.1 √ó |
|         | Provable    | 90.8 ¬±0.08 (-0.5) | 0.10 (37.3%) | 54.5 | 1.7 √ó |
|         | Ours        | 90.91¬±0.07 (-0.4) | 0.096 ¬±0.002 (35.8%) | 50.20 ¬±0.01 | 2.40 ¬±0.09√ó |
| WRN     | Original    | 96.2 ¬±0.10 (-0.0) | 36.5 (100%) | 100 | 1.0 √ó |
| -28     | L1-Pruning  | 95.2 ¬±0.10 (-1.0) | 7.6 (20.8%) | 49.5 | 1.5 √ó |
| -10     | BAR(16x V)  | 92.0 ¬±0.08 (-4.2) | 2.3 (6.3%) | 1.5 | 2.6√ó |
|         | Ours        | 95.32¬±0.11 (-0.9) | 3.443¬±0.010 (9.3%) | 28.25¬±0.04 | 3.12¬±0.11√ó |

m√† nhi·ªát ƒë·ªô tƒÉng trong qu√° tr√¨nh hu·∫•n luy·ªán. C√°c ph∆∞∆°ng ph√°p continuation v·ªõi b·ªô l·∫≠p l·ªãch nhi·ªát ƒë·ªô to√†n c·ª•c ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng th√†nh c√¥ng trong c·∫Øt t·ªâa v√† NAS. Tuy nhi√™n, trong tr∆∞·ªùng h·ª£p c·ªßa ch√∫ng t√¥i, m·ªôt l·ªãch tr√¨nh to√†n c·ª•c d·∫´n ƒë·∫øn ƒë·ªông l·ª±c kh√¥ng c√¢n b·∫±ng gi·ªØa c√°c bi·∫øn c√≥ x√°c su·∫•t l·∫•y m·∫´u th·∫•p v√† cao: vi·ªác tƒÉng nhi·ªát ƒë·ªô c·ªßa nh·ªØng bi·∫øn √≠t ƒë∆∞·ª£c l·∫•y m·∫´u ·ªü giai ƒëo·∫°n ƒë·∫ßu c√≥ th·ªÉ c·∫£n tr·ªü vi·ªác hu·∫•n luy·ªán c·ªßa ch√∫ng ho√†n to√†n, v√¨ v·ªÅ cu·ªëi hu·∫•n luy·ªán ƒë·ªô kh√≥ t·ªëi ∆∞u h√≥a cao h∆°n. ƒê·ªÉ kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ n√†y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt b·ªô l·∫≠p l·ªãch nhi·ªát ƒë·ªô ri√™ng bi·ªát theo c·∫•u tr√∫c b·∫±ng c√°ch th·ª±c hi·ªán m·ªôt s·ª≠a ƒë·ªïi ƒë∆°n gi·∫£n: ƒë·ªëi v·ªõi m·ªói bi·∫øn m·∫∑t n·∫°, thay v√¨ s·ª≠ d·ª•ng s·ªë epoch hi·ªán t·∫°i t ƒë·ªÉ t√≠nh nhi·ªát ƒë·ªô c·ªßa n√≥, ch√∫ng t√¥i ƒë·∫∑t m·ªôt b·ªô ƒë·∫øm ri√™ng bi·ªát ts ch·ªâ ƒë∆∞·ª£c tƒÉng khi bi·∫øn ch·ªâ th·ªã li√™n quan c·ªßa n√≥ ƒë∆∞·ª£c l·∫•y m·∫´u l√† 1 trong Eq. (6). Ch√∫ng t√¥i ƒë·ªãnh nghƒ©a b·ªô l·∫≠p l·ªãch nhi·ªát ƒë·ªô theo c·∫•u tr√∫c c·ªßa ch√∫ng t√¥i nh∆∞
Œ≤=Œ≤0¬∑Œ≥ts, (7)
trong ƒë√≥ ts l√† c√°c vector li√™n k·∫øt v·ªõi c√°c h√†m œÉ. C√°c th√≠ nghi·ªám s·ª≠ d·ª•ng b·ªô l·∫≠p l·ªãch ri√™ng bi·ªát n√†y theo m·∫∑c ƒë·ªãnh, nh∆∞ng c≈©ng so s√°nh hai l·ª±a ch·ªçn thay th·∫ø. Thu·∫≠t to√°n 1 t√≥m t·∫Øt quy tr√¨nh t·ªëi ∆∞u h√≥a c·ªßa ch√∫ng t√¥i.

4 TH√ç NGHI·ªÜM
Ch√∫ng t√¥i ƒë√°nh gi√° ph∆∞∆°ng ph√°p c·ªßa m√¨nh so v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa k√™nh, ph√°t tri·ªÉn m·∫°ng v√† t√¨m ki·∫øm ki·∫øn tr√∫c neural (NAS) hi·ªán c√≥ tr√™n: CIFAR-10 (Krizhevsky et al., 2014) v√† ImageNet (Deng et al., 2009) cho ph√¢n lo·∫°i h√¨nh ·∫£nh, PASCAL (Everingham et al., 2015) cho ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a v√† Penn Treebank (PTB) (Marcus et al., 1993) cho m√¥ h√¨nh h√≥a ng√¥n ng·ªØ. Xem chi ti·∫øt dataset trong Ph·ª• l·ª•c A.2. Trong c√°c b·∫£ng, k·∫øt qu·∫£ t·ªët nh·∫•t ƒë∆∞·ª£c l√†m n·ªïi b·∫≠t b·∫±ng ch·ªØ ƒë·∫≠m v√† t·ªët th·ª© hai ƒë∆∞·ª£c g·∫°ch ch√¢n.

4.1 SO S√ÅNH V·ªöI C√ÅC PH∆Ø∆†NG PH√ÅP C·∫ÆT T·ªàA K√äNH
Chi ti·∫øt Tri·ªÉn khai. ƒê·ªÉ so s√°nh c√¥ng b·∫±ng, ch√∫ng t√¥i ch·ªâ ph√°t tri·ªÉn b·ªô l·ªçc trong khi gi·ªØ c√°c tham s·ªë c√≥ c·∫•u tr√∫c kh√°c c·ªßa m·∫°ng (s·ªë l∆∞·ª£ng l·ªõp/kh·ªëi) gi·ªëng nh∆∞ c√°c m√¥ h√¨nh baseline ch∆∞a ƒë∆∞·ª£c c·∫Øt t·ªâa. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i li√™n quan ƒë·∫øn hai lo·∫°i bi·∫øn c√≥ th·ªÉ hu·∫•n luy·ªán: tr·ªçng s·ªë m√¥ h√¨nh v√† tr·ªçng s·ªë m·∫∑t n·∫°. ƒê·ªëi v·ªõi tr·ªçng s·ªë m√¥ h√¨nh, ch√∫ng t√¥i √°p d·ª•ng c√πng c√°c si√™u tham s·ªë ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán c√°c m√¥ h√¨nh baseline ch∆∞a ƒë∆∞·ª£c c·∫Øt t·ªâa t∆∞∆°ng ·ª©ng, ngo·∫°i tr·ª´ vi·ªác ƒë·∫∑t x√°c su·∫•t gi·ªØ dropout cho m√¥ h√¨nh h√≥a ng√¥n ng·ªØ l√† 0.65. Ch√∫ng t√¥i kh·ªüi t·∫°o tr·ªçng s·ªë m·∫∑t n·∫° sao cho m·ªôt b·ªô l·ªçc duy nh·∫•t ƒë∆∞·ª£c k√≠ch ho·∫°t trong m·ªói l·ªõp. Ch√∫ng t√¥i hu·∫•n luy·ªán v·ªõi SGD, t·ªëc ƒë·ªô h·ªçc ban ƒë·∫ßu 0.1, weight decay 10‚àí6 v√† momentum 0.9. Tham s·ªë ƒë√°nh ƒë·ªïi Œªbase
1 ƒë∆∞·ª£c ƒë·∫∑t l√† 0.5 tr√™n t·∫•t c·∫£ nhi·ªám v·ª•; Œª2 kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng v√¨ ch√∫ng t√¥i kh√¥ng th·ª±c hi·ªán ph√°t tri·ªÉn l·ªõp ·ªü ƒë√¢y. Ch√∫ng t√¥i ƒë·∫∑t œÉ l√† h√†m sigmoid v√† Œ≥ l√† 1001
T trong ƒë√≥ T l√† t·ªïng s·ªë epoch.

VGG-16, ResNet-20, v√† WideResNet-28-10 tr√™n CIFAR-10. B·∫£ng 1 t√≥m t·∫Øt c√°c m√¥ h√¨nh ƒë∆∞·ª£c t·∫°o ra b·ªüi ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i v√† c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa k√™nh c·∫°nh tranh. L∆∞u √Ω r·∫±ng chi ph√≠ hu·∫•n luy·ªán ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n t·ªïng FLOPs trong c√°c giai ƒëo·∫°n c·∫Øt t·ªâa v√† ph√°t tri·ªÉn. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i t·∫°o ra c√°c m·∫°ng th∆∞a th·ªõt h∆°n v·ªõi √≠t suy gi·∫£m ƒë·ªô ch√≠nh x√°c h∆°n, v√† li√™n t·ª•c ti·∫øt ki·ªám nhi·ªÅu t√≠nh to√°n h∆°n trong qu√° tr√¨nh hu·∫•n luy·ªán ‚Äî m·ªôt h·ªá qu·∫£ c·ªßa vi·ªác ph√°t tri·ªÉn t·ª´ m·ªôt m·∫°ng ƒë∆°n gi·∫£n. ƒê·ªëi v·ªõi WideResNet-28-10 ƒë∆∞·ª£c c·∫Øt t·ªâa t√≠ch c·ª±c, ch√∫ng t√¥i quan s√°t r·∫±ng BAR (Lemaire et al., 2019) c√≥ th·ªÉ kh√¥ng c√≥ ƒë·ªß dung l∆∞·ª£ng ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c s·ª± s·ª•t gi·∫£m ƒë·ªô ch√≠nh x√°c kh√¥ng ƒë√°ng k·ªÉ, ngay c·∫£ v·ªõi knowledge distillation (Hinton et al., 2015) trong qu√° tr√¨nh hu·∫•n luy·ªán. L∆∞u √Ω r·∫±ng ch√∫ng t√¥i

6

--- TRANG 7 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
B·∫£ng 2: So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫Øt t·ªâa k√™nh: L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018) v√† Provable (Liebenwein et al., 2020) tr√™n ImageNet.

| M√¥ h√¨nh | Ph∆∞∆°ng ph√°p | Top-1 Acc(%) | Params(M) | FLOPs(%) | Ti·∫øt ki·ªám Chi ph√≠ Hu·∫•n luy·ªán(√ó) |
|---------|-------------|-------------|-----------|----------|----------------------------------|
|         | Original    | 76.1 (-0.0) | 23.0 (100%) | 100 | 1.0(√ó) |
| ResNet  | L1-Pruning  | 74.7 (-1.4) | 19.6 (85.2%) | 77.5 | 1.1(√ó) |
| -50     | SoftNet     | 74.6 (-1.5) | N/A | 58.2 | 1.2(√ó) |
|         | Provable    | 75.2 (-0.9) | 15.2 (65.9%) | 70.0 | 1.2(√ó) |
|         | Ours        | 75.2 (-0.9) | 14.1 (61.2%) | 50.3 | 1.9(√ó) |

B·∫£ng 3: K·∫øt qu·∫£ so s√°nh v·ªõi AutoGrow (Wen et al., 2020) tr√™n CIFAR-10 v√† ImageNet.

| Dataset | Ph∆∞∆°ng ph√°p | Bi·∫øn th·ªÉ | M·∫°ng T√¨m ƒë∆∞·ª£c | Val Acc(%) | ƒê·ªô s√¢u | K√™nh Th∆∞a |
|---------|-------------|----------|---------------|-----------|--------|-----------|
|         | Ours        | Basic3ResNet | 23-29-31 | 94.50 | 83 | ‚úì |
| CIFAR-10| Plain3Net   | 11-14-19 | 90.99 | 44 | ‚úì |
|         | AutoGrow    | Basic3ResNet | 42-42-42 | 94.27 | 126 | ‚úó |
|         | Plain3Net   | 23-22-22 | 90.82 | 67 | ‚úó |
|         | Ours        | Bottleneck4ResNet | 5-6-5-7 | 77.41 | 23 | ‚úì |
| ImageNet| Plain4Net   | 3-4-4-5 | 70.79 | 16 | ‚úì |
|         | AutoGrow    | Bottleneck4ResNet | 6-7-3-9 | 77.33 | 25 | ‚úó |
|         | Plain4Net   | 5-5-5-4 | 70.54 | 19 | ‚úó |

b√°o c√°o hi·ªáu su·∫•t c·ªßa ph∆∞∆°ng ph√°p ch√∫ng t√¥i nh∆∞ trung b√¨nh ¬± ƒë·ªô l·ªách chu·∫©n, ƒë∆∞·ª£c t√≠nh tr√™n 5 l·∫ßn ch·∫°y v·ªõi c√°c seed ng·∫´u nhi√™n kh√°c nhau. Ph∆∞∆°ng sai nh·ªè quan s√°t ƒë∆∞·ª£c cho th·∫•y r·∫±ng ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ho·∫°t ƒë·ªông nh·∫•t qu√°n qua c√°c l·∫ßn ch·∫°y.

ResNet-50 v√† MobileNetV1 tr√™n ImageNet. ƒê·ªÉ x√°c th·ª±c hi·ªáu qu·∫£ tr√™n c√°c dataset quy m√¥ l·ªõn, ch√∫ng t√¥i ph√°t tri·ªÉn, t·ª´ ƒë·∫ßu, c√°c b·ªô l·ªçc c·ªßa ResNet-50 ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i tr√™n ImageNet; ch√∫ng t√¥i kh√¥ng fine-tune. B·∫£ng 2 cho th·∫•y k·∫øt qu·∫£ c·ªßa ch√∫ng t√¥i t·ªët h·∫•t nh·ªØng k·∫øt qu·∫£ ƒë∆∞·ª£c b√°o c√°o tr·ª±c ti·∫øp trong c√°c b√†i b√°o c·ªßa c√°c ph∆∞∆°ng ph√°p c·∫°nh tranh t∆∞∆°ng ·ª©ng. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ƒë·∫°t ƒë∆∞·ª£c 49.7% ti·∫øt ki·ªám suy lu·∫≠n v√† 47.4% ti·∫øt ki·ªám chi ph√≠ hu·∫•n luy·ªán v·ªÅ FLOPs trong khi duy tr√¨ ƒë·ªô ch√≠nh x√°c top-1 75.2%, m√† kh√¥ng c√≥ b·∫•t k·ª≥ giai ƒëo·∫°n fine-tuning n√†o. Ph·ª• l·ª•c A.4 cho th·∫•y nh·ªØng c·∫£i thi·ªán c·ªßa ch√∫ng t√¥i v·ªÅ nhi·ªám v·ª• ƒë·∫ßy th·ª≠ th√°ch c·ªßa vi·ªác ph√°t tri·ªÉn k√™nh c·ªßa MobileNetV1 ƒë√£ compact. Ngo√†i ra, H√¨nh 3 cho th·∫•y s·ª± ƒë√°nh ƒë·ªïi ƒë·ªô ch√≠nh x√°c top-1/FLOPs cho MobileNetV1 tr√™n ImageNet, ch·ª©ng minh r·∫±ng ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i v∆∞·ª£t tr·ªôi so v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫°nh tranh.

Deeplab-v3-ResNet-101 tr√™n PASCAL VOC. Ph·ª• l·ª•c A.5 cung c·∫•p k·∫øt qu·∫£ ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a.

2-Stacked-LSTM tr√™n PTB: Ch√∫ng t√¥i chi ti·∫øt c√°c ph·∫ßn m·ªü r·ªông cho c√°c t·∫ø b√†o h·ªìi quy v√† so s√°nh ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t c·ªßa ch√∫ng t√¥i v·ªõi ISS d·ª±a tr√™n vanilla two-layer stacked LSTM trong Ph·ª• l·ª•c A.6. Nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong B·∫£ng 8, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i t√¨m th·∫•y c·∫•u tr√∫c m√¥ h√¨nh compact h∆°n v·ªõi chi ph√≠ hu·∫•n luy·ªán th·∫•p h∆°n, trong khi ƒë·∫°t ƒë∆∞·ª£c perplexity t∆∞∆°ng t·ª± tr√™n c·∫£ t·∫≠p validation v√† test.

4.2 SO S√ÅNH V·ªöI AUTOGROW
Chi ti·∫øt Tri·ªÉn khai. Ch√∫ng t√¥i ph√°t tri·ªÉn c·∫£ b·ªô l·ªçc v√† l·ªõp. Ch√∫ng t√¥i tu√¢n theo c√†i ƒë·∫∑t c·ªßa AutoGrow trong vi·ªác kh√°m ph√° c√°c bi·∫øn th·ªÉ ki·∫øn tr√∫c x√°c ƒë·ªãnh m·∫°ng h·∫°t gi·ªëng ban ƒë·∫ßu, kh√¥ng gian c·∫•u h√¨nh theo l·ªõp v√† c√°c ƒë∆°n v·ªã c·∫•u tr√∫c c∆° b·∫£n flayer c·ªßa ch√∫ng t√¥i: Basic3ResNet, Bottleneck4ResNet, Plain3Net, Plain4Net. Kh√°c v·ªõi vi·ªác kh·ªüi t·∫°o c·ªßa AutoGrow s·ª≠ d·ª•ng b·ªô l·ªçc c√≥ k√≠ch th∆∞·ªõc ƒë·∫ßy ƒë·ªß trong m·ªói l·ªõp, kh√¥ng gian c·∫•u h√¨nh theo k√™nh c·ªßa ch√∫ng t√¥i b·∫Øt ƒë·∫ßu t·ª´ b·ªô l·ªçc ƒë∆°n v√† m·ªü r·ªông ƒë·ªìng th·ªùi v·ªõi c√°c l·ªõp. Ph·ª• l·ª•c A.7 ch·ª©a so s√°nh chi ti·∫øt c·ªßa c√°c ki·∫øn tr√∫c h·∫°t gi·ªëng. ƒê·ªëi v·ªõi vi·ªác hu·∫•n luy·ªán tr·ªçng s·ªë m√¥ h√¨nh, ch√∫ng t√¥i √°p d·ª•ng c√°c si√™u tham s·ªë c·ªßa c√°c m√¥ h√¨nh ResNet ho·∫∑c VGG t∆∞∆°ng ·ª©ng v·ªõi c√°c bi·∫øn th·ªÉ h·∫°t gi·ªëng ban ƒë·∫ßu. ƒê·ªëi v·ªõi c√°c bi·∫øn m·∫∑t n·∫° theo l·ªõp v√† theo k√™nh, ch√∫ng t√¥i kh·ªüi t·∫°o tr·ªçng s·ªë sao cho ch·ªâ m·ªôt b·ªô l·ªçc ƒë∆°n trong m·ªói l·ªõp v√† m·ªôt ƒë∆°n v·ªã c∆° b·∫£n trong m·ªói giai ƒëo·∫°n (v√≠ d·ª•, BasicBlock trong Basic3ResNet) l√† ho·∫°t ƒë·ªông. Ch√∫ng t√¥i s·ª≠ d·ª•ng hu·∫•n luy·ªán SGD v·ªõi t·ªëc ƒë·ªô h·ªçc ban ƒë·∫ßu 0.1, weight decay 10‚àí6 v√† momentum 0.9 tr√™n t·∫•t c·∫£ dataset. B·ªô l·∫≠p l·ªãch t·ªëc ƒë·ªô h·ªçc gi·ªëng nh∆∞ cho tr·ªçng s·ªë m√¥ h√¨nh t∆∞∆°ng ·ª©ng. C√°c tham s·ªë ƒë√°nh ƒë·ªïi Œªbase
1 v√† Œªbase
2 ƒë∆∞·ª£c ƒë·∫∑t l√† 1.0 v√† 0.1 tr√™n t·∫•t c·∫£ dataset. ƒê·ªÉ so s√°nh c√¥ng b·∫±ng, ch√∫ng t√¥i fine-tune c√°c m√¥ h√¨nh cu·ªëi c√πng v·ªõi 40 epoch v√† 20 epoch tr√™n CIFAR-10 v√† ImageNet, t∆∞∆°ng ·ª©ng.

K·∫øt qu·∫£ tr√™n CIFAR-10 v√† ImageNet. B·∫£ng 3 so s√°nh k·∫øt qu·∫£ c·ªßa ch√∫ng t√¥i v·ªõi AutoGrow. ƒê·ªëi v·ªõi t·∫•t c·∫£ c√°c bi·∫øn th·ªÉ ph√°t tri·ªÉn theo l·ªõp tr√™n c·∫£ hai dataset, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i nh·∫•t qu√°n t·∫°o ra c·∫•u h√¨nh ƒë·ªô s√¢u v√† chi·ªÅu r·ªông t·ªët h∆°n AutoGrow, v·ªÅ s·ª± ƒë√°nh ƒë·ªïi gi·ªØa ƒë·ªô ch√≠nh x√°c v√† chi ph√≠ hu·∫•n luy·ªán/suy lu·∫≠n. V·ªÅ th·ªùi gian hu·∫•n luy·ªán c·ªßa Bottleneck4ResNet tr√™n ImageNet, AutoGrow y√™u c·∫ßu 61.6 gi·ªù cho giai ƒëo·∫°n ph√°t tri·ªÉn v√† 78.6 gi·ªù cho fine-tuning tr√™n 4 GPU TITAN V, trong khi ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i m·∫•t 48.2 v√† 31.3 gi·ªù, t∆∞∆°ng ·ª©ng. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i mang l·∫°i 43% ti·∫øt ki·ªám th·ªùi gian hu·∫•n luy·ªán h∆°n so v·ªõi AutoGrow. Ch√∫ng t√¥i kh√¥ng ch·ªâ y√™u c·∫ßu √≠t epoch hu·∫•n luy·ªán h∆°n, m√† c√≤n ph√°t tri·ªÉn t·ª´ m·ªôt b·ªô l·ªçc ƒë∆°n ƒë·∫øn m·ªôt m·∫°ng t∆∞∆°ng ƒë·ªëi th∆∞a th·ªõt, trong khi AutoGrow lu√¥n gi·ªØ c√°c t·∫≠p b·ªô l·ªçc c√≥ k√≠ch th∆∞·ªõc ƒë·∫ßy ƒë·ªß m√† kh√¥ng c√≥ b·∫•t k·ª≥ t√°i ph√¢n b·ªï n√†o.

7

--- TRANG 8 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
4.3 SO S√ÅNH V·ªöI C√ÅC PH∆Ø∆†NG PH√ÅP NAS
Nh∆∞ m·ªôt so s√°nh c√¥ng b·∫±ng v·ªõi c√°c ph∆∞∆°ng ph√°p NAS li√™n quan ƒë·∫øn c√°c giai ƒëo·∫°n t√¨m ki·∫øm v√† hu·∫•n luy·ªán l·∫°i, ch√∫ng t√¥i c≈©ng chia ph∆∞∆°ng ph√°p c·ªßa m√¨nh th√†nh c√°c giai ƒëo·∫°n ph√°t tri·ªÉn v√† hu·∫•n luy·ªán. C·ª• th·ªÉ, ch√∫ng t√¥i ph√°t tri·ªÉn c√°c l·ªõp v√† k√™nh t·ª´ ki·∫øn tr√∫c h·∫°t gi·ªëng Bottleneck4ResNet tr·ª±c ti·∫øp tr√™n ImageNet b·∫±ng c√°ch ƒë·∫∑t Œªbase
1= 2.0, Œªbase
2= 0.1 v√† ng√¢n s√°ch tham s·ªë d∆∞·ªõi 7M. Sau ƒë√≥ ch√∫ng t√¥i ti·∫øp t·ª•c hu·∫•n luy·ªán ki·∫øn tr√∫c ƒë√£ ph√°t tri·ªÉn v√† so s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p NAS hi·ªán c√≥ v·ªÅ tham s·ªë, ƒë·ªô ch√≠nh x√°c validation top-1 v√† gi·ªù GPU V100 y√™u c·∫ßu b·ªüi c√°c giai ƒëo·∫°n t√¨m ki·∫øm ho·∫∑c ph√°t tri·ªÉn, nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong B·∫£ng 4. L∆∞u √Ω r·∫±ng DARTS (Liu et al., 2019) ti·∫øn h√†nh t√¨m ki·∫øm tr√™n CIFAR-10, sau ƒë√≥ chuy·ªÉn sang ImageNet thay v√¨ t√¨m ki·∫øm tr·ª±c ti·∫øp. ƒêi·ªÅu n√†y l√† do DARTS ho·∫°t ƒë·ªông tr√™n m·ªôt supernet b·∫±ng c√°ch bao g·ªìm t·∫•t c·∫£ c√°c ƒë∆∞·ªùng d·∫´n ·ª©ng vi√™n v√† g·∫∑p ph·∫£i s·ª± b√πng n·ªï b·ªô nh·ªõ GPU. V·ªÅ FLOPs theo epoch, k·∫øt qu·∫£ ƒë∆∞·ª£c hi·ªÉn th·ªã trong H√¨nh 1(c) d√†nh cho vi·ªác hu·∫•n luy·ªán t∆∞∆°ng ƒë∆∞∆°ng ResNet-20 tr√™n CIFAR-10 so v·ªõi DARTS v√† ph∆∞∆°ng ph√°p c·∫Øt t·ªâa k√™nh Provable (Liebenwein et al., 2020). C≈©ng l∆∞u √Ω r·∫±ng ki·∫øn tr√∫c EfficientNet-B0, ƒë∆∞·ª£c bao g·ªìm trong B·∫£ng 4, ƒë∆∞·ª£c t·∫°o ra b·∫±ng t√¨m ki·∫øm l∆∞·ªõi trong kh√¥ng gian t√¨m ki·∫øm MnasNet, do ƒë√≥ c√≥ c√πng chi ph√≠ t√¨m ki·∫øm n·∫∑ng. ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c hi·ªáu su·∫•t ƒë∆∞·ª£c b√°o c√°o, EfficientNet-B0 s·ª≠ d·ª•ng c√°c m√¥-ƒëun squeeze-and-excitation (SE) b·ªï sung (Hu et al., 2018), AutoAugment (Cubuk et al., 2019), c≈©ng nh∆∞ c√°c epoch hu·∫•n luy·ªán l·∫°i d√†i h∆°n nhi·ªÅu tr√™n ImageNet.

B·∫£ng 4: Hi·ªáu su·∫•t so s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p NAS AmoebaNet-A (Real et al., 2019), MnasNet (Tan et al., 2019), EfficientNet-B0 (Tan & Le, 2019), DARTS (Liu et al., 2019) v√† ProxylessNet (Cai et al., 2019) tr√™n ImageNet.

| Ph∆∞∆°ng ph√°p | Params | Top-1 | Chi ph√≠ T√¨m ki·∫øm/Ph√°t tri·ªÉn |
|-------------|--------|-------|----------------------------|
| AmoebaNet-A | 5.1M | 74.5% | 76K gi·ªù GPU |
| MnasNet | 4.4M | 74.0% | 40K gi·ªù GPU |
| EfficientNet-B0 | 5.3M | 77.1% (+SE) | 40K gi·ªù GPU |
| DARTS | 4.7M | 73.1% | N/A |
| ProxylessNet(GPU) | 7.1M | 75.1% | 200 gi·ªù GPU |
| Ours | 6.8M | 74.3% | 80 gi·ªù GPU |
| Ours | 6.7M | 74.8% | 110 gi·ªù GPU |
| Ours | 6.9M | 75.1% | 140 gi·ªù GPU |

ProxylessNet v·∫´n b·∫Øt ƒë·∫ßu v·ªõi m·ªôt supernet ƒë∆∞·ª£c tham s·ªë h√≥a qu√° m·ª©c, nh∆∞ng √°p d·ª•ng m·ªôt ph∆∞∆°ng ph√°p t√¨m ki·∫øm gi·ªëng c·∫Øt t·ªâa b·∫±ng c√°ch nh·ªã ph√¢n h√≥a c√°c tham s·ªë ki·∫øn tr√∫c v√† bu·ªôc ch·ªâ m·ªôt ƒë∆∞·ªùng d·∫´n ƒë∆∞·ª£c k√≠ch ho·∫°t t·∫°i th·ªùi ƒëi·ªÉm t√¨m ki·∫øm. ƒêi·ªÅu n√†y cho ph√©p t√¨m ki·∫øm tr·ª±c ti·∫øp tr√™n ImageNet, ƒë·∫°t ƒë∆∞·ª£c ti·∫øt ki·ªám chi ph√≠ t√¨m ki·∫øm 200√ó so v·ªõi MnasNet. T∆∞∆°ng ph·∫£n v·ªõi ProxylessNet, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i d·∫ßn th√™m b·ªô l·ªçc v√† l·ªõp v√†o c√°c ki·∫øn tr√∫c h·∫°t gi·ªëng ƒë∆°n gi·∫£n trong khi duy tr√¨ th∆∞a th·ªõt h√≥a, d·∫´n ƒë·∫øn ti·∫øt ki·ªám kh√¥ng ch·ªâ t√≠nh to√°n theo epoch m√† c√≤n ti√™u th·ª• b·ªô nh·ªõ, cho ph√©p hu·∫•n luy·ªán batch l·ªõn h∆°n, nhanh h∆°n. Nh∆∞ v·∫≠y, ch√∫ng t√¥i ti·∫øt ki·ªám th√™m 45% gi·ªù GPU t√¨m ki·∫øm, trong khi ƒë·∫°t ƒë∆∞·ª£c s·ª± ƒë√°nh ƒë·ªïi ƒë·ªô ch√≠nh x√°c-tham s·ªë c√≥ th·ªÉ so s√°nh.

4.4 PH√ÇN T√çCH
Ti·∫øt ki·ªám Chi ph√≠ Hu·∫•n luy·ªán. H√¨nh 4 minh h·ªça ƒë·ªông l·ª±c th∆∞a th·ªõt h√≥a c·ªßa ch√∫ng t√¥i, hi·ªÉn th·ªã FLOPs theo epoch trong khi ph√°t tri·ªÉn ResNet-20. Ph·ª• l·ª•c A.8 tr√¨nh b√†y c√°c visualization b·ªï sung. Ngay c·∫£ v·ªõi ph·∫ßn c·ª©ng GPU song song ho√†n to√†n, vi·ªác b·∫Øt ƒë·∫ßu v·ªõi √≠t b·ªô l·ªçc v√† l·ªõp trong m·∫°ng cu·ªëi c√πng s·∫Ω ti·∫øt ki·ªám th·ªùi gian wall-clock, v√¨ hu·∫•n luy·ªán batch l·ªõn h∆°n (Goyal et al., 2017) lu√¥n c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ l·∫•p ƒë·∫ßy ph·∫ßn c·ª©ng.

H√¨nh 5 cho th·∫•y ƒë·ªô ch√≠nh x√°c validation, ƒë·ªô ph·ª©c t·∫°p m√¥ h√¨nh v√† s·ªë l∆∞·ª£ng l·ªõp trong khi ph√°t tri·ªÉn Basic3ResNet. ƒê·ªô ph·ª©c t·∫°p ƒë∆∞·ª£c ƒëo nh∆∞ t·ª∑ l·ªá tham s·ªë m√¥ h√¨nh c·ªßa m√¥ h√¨nh m·ª•c ti√™u c·ªßa AutoGrow. V√†o cu·ªëi 160 epoch, ƒë·ªô ch√≠nh x√°c validation c·ªßa ph∆∞∆°ng ph√°p ch√∫ng t√¥i l√† 92.36%, cao h∆°n 84.65% c·ªßa AutoGrow t·∫°i 360 epoch. Do ƒë√≥ ch√∫ng t√¥i y√™u c·∫ßu √≠t epoch fine-tuning h∆°n ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c ƒë·ªô ch√≠nh x√°c cu·ªëi c√πng 94.50% tr√™n CIFAR.

B·∫£ng 5: So s√°nh v·ªõi baseline c·∫Øt t·ªâa ng·∫´u nhi√™n tr√™n CIFAR-10.

| M√¥ h√¨nh | Ph∆∞∆°ng ph√°p | Val Acc(%) | Params(M) |
|---------|-------------|------------|-----------|
| VGG-16 | Random | 90.01 ¬±0.69 | 0.770 ¬±0.050 |
|        | Ours | 92.50¬±0.10 | 0.754¬±0.005 |
| ResNet-20 | Random | 89.18 ¬±0.55 | 0.100 ¬±0.010 |
|           | Ours | 90.91¬±0.07 | 0.096¬±0.002 |
| WRN-28-10 | Random | 92.26 ¬±0.87 | 3.440 ¬±0.110 |
|           | Ours | 95.32¬±0.11 | 3.443¬±0.010 |

Ph√°t tri·ªÉn Nh·∫≠n th·ª©c Ng√¢n s√°ch. Trong H√¨nh 6, ƒë·ªëi v·ªõi ResNet-20 tr√™n CIFAR-10, ch√∫ng t√¥i so s√°nh c√°c ki·∫øn tr√∫c thu ƒë∆∞·ª£c b·ªüi (1) c·∫Øt t·ªâa ƒë·ªìng ƒë·ªÅu: m·ªôt ph∆∞∆°ng ph√°p c·∫Øt t·ªâa ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc ng√¢y th∆° c·∫Øt t·ªâa c√πng t·ª∑ l·ªá ph·∫ßn trƒÉm k√™nh trong m·ªói l·ªõp, (2) c·ªßa ch√∫ng t√¥i: c√°c bi·∫øn th·ªÉ c·ªßa ph∆∞∆°ng ph√°p ch√∫ng t√¥i b·∫±ng c√°ch ƒë·∫∑t c√°c ƒë·ªô th∆∞a th·ªõt tham s·ªë m√¥ h√¨nh kh√°c nhau nh∆∞ ng√¢n s√°ch m·ª•c ti√™u trong qu√° tr√¨nh ph√°t tri·ªÉn, v√† (3) thi·∫øt k·∫ø tr·ª±c ti·∫øp: c√°c ki·∫øn tr√∫c ƒë∆∞·ª£c ph√°t tri·ªÉn c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c kh·ªüi t·∫°o l·∫°i v·ªõi tr·ªçng s·ªë ng·∫´u nhi√™n v√† hu·∫•n luy·ªán l·∫°i. Trong h·∫ßu h·∫øt c√°c c√†i ƒë·∫∑t ng√¢n s√°ch, ph∆∞∆°ng ph√°p ph√°t tri·ªÉn c·ªßa ch√∫ng t√¥i v∆∞·ª£t tr·ªôi so v·ªõi thi·∫øt k·∫ø tr·ª±c ti·∫øp v√† c·∫Øt t·ªâa ƒë·ªìng ƒë·ªÅu, ch·ª©ng minh

8

--- TRANG 9 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021

[C√°c h√¨nh ·∫£nh v√† bi·ªÉu ƒë·ªì ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ trong nguy√™n vƒÉn]

hi·ªáu qu·∫£ tham s·ªë cao h∆°n. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i c≈©ng c√≥ v·∫ª c√≥ t√°c d·ª•ng t√≠ch c·ª±c v·ªÅ m·∫∑t ch√≠nh quy h√≥a ho·∫∑c ƒë·ªông l·ª±c t·ªëi ∆∞u h√≥a, nh·ªØng ƒëi·ªÅu n√†y b·ªã m·∫•t n·∫øu ng∆∞·ªùi ta c·ªë g·∫Øng hu·∫•n luy·ªán tr·ª±c ti·∫øp c·∫•u tr√∫c compact cu·ªëi c√πng. Ph·ª• l·ª•c A.9 ƒëi·ªÅu tra c√°c m·ª•c ti√™u ng√¢n s√°ch d·ª±a tr√™n FLOPs.

So s√°nh v·ªõi Baseline Ng·∫´u nhi√™n. Ngo√†i baseline c·∫Øt t·ªâa ƒë·ªìng ƒë·ªÅu trong H√¨nh 6, ch√∫ng t√¥i c≈©ng so s√°nh v·ªõi m·ªôt baseline l·∫•y m·∫´u ng·∫´u nhi√™n ƒë·ªÉ t√°ch bi·ªát th√™m ƒë√≥ng g√≥p c·ªßa kh√¥ng gian c·∫•u h√¨nh v√† ph∆∞∆°ng ph√°p ph√°t tri·ªÉn c·ªßa ch√∫ng t√¥i, theo ti√™u ch√≠ trong (Xie et al., 2019a; Li & Talwalkar, 2019; Yu et al., 2020; Radosavovic et al., 2019). C·ª• th·ªÉ, baseline ng·∫´u nhi√™n n√†y thay th·∫ø quy tr√¨nh l·∫•y m·∫´u c√°c m·ª•c c·ªßa q trong Eq. 6. Thay v√¨ s·ª≠ d·ª•ng x√°c su·∫•t l·∫•y m·∫´u ƒë∆∞·ª£c r√∫t ra t·ª´ c√°c tham s·ªë m·∫∑t n·∫° ƒë√£ h·ªçc s, n√≥ l·∫•y m·∫´u v·ªõi x√°c su·∫•t c·ªë ƒë·ªãnh. Nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong B·∫£ng 5, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i nh·∫•t qu√°n ho·∫°t ƒë·ªông t·ªët h∆°n nhi·ªÅu so v·ªõi baseline ng·∫´u nhi√™n n√†y. Nh·ªØng k·∫øt qu·∫£ n√†y, c≈©ng nh∆∞ c√°c baseline tinh vi h∆°n trong H√¨nh 6, ch·ª©ng minh hi·ªáu qu·∫£ c·ªßa ph∆∞∆°ng ph√°p ph√°t tri·ªÉn v√† c·∫Øt t·ªâa c·ªßa ch√∫ng t√¥i.

B·ªô l·∫≠p l·ªãch Nhi·ªát ƒë·ªô. Ch√∫ng t√¥i so s√°nh ki·ªÉm so√°t nhi·ªát ƒë·ªô theo c·∫•u tr√∫c c·ªßa ch√∫ng t√¥i v·ªõi m·ªôt b·ªô l·∫≠p l·ªãch to√†n c·ª•c trong c√°c th√≠ nghi·ªám ph√°t tri·ªÉn k√™nh tr√™n CIFAR-10 s·ª≠ d·ª•ng VGG-16, ResNet-20, v√† WideResNet-28-10. K·∫øt qu·∫£ B·∫£ng 1 s·ª≠ d·ª•ng b·ªô l·∫≠p l·ªãch theo c·∫•u tr√∫c c·ªßa ch√∫ng t√¥i. ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c ƒë·ªô th∆∞a th·ªõt t∆∞∆°ng t·ª± v·ªõi b·ªô l·∫≠p l·ªãch to√†n c·ª•c, c√°c m√¥ h√¨nh t∆∞∆°ng ·ª©ng b·ªã s·ª•t gi·∫£m ƒë·ªô ch√≠nh x√°c 1.4%, 0.6%, v√† 2.5%. V·ªõi b·ªô l·∫≠p l·ªãch to√†n c·ª•c, t·ªëi ∆∞u h√≥a c√°c bi·∫øn m·∫∑t n·∫° d·ª´ng s·ªõm trong hu·∫•n luy·ªán v√† c√°c epoch sau t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác hu·∫•n luy·ªán tr·ª±c ti·∫øp m·ªôt m·∫°ng compact c·ªë ƒë·ªãnh. ƒêi·ªÅu n√†y c√≥ th·ªÉ bu·ªôc m·∫°ng b·ªã k·∫πt v·ªõi m·ªôt ki·∫øn tr√∫c kh√¥ng t·ªëi ∆∞u. Ph·ª• l·ª•c A.10 ƒëi·ªÅu tra c√°c t∆∞∆°ng t√°c gi·ªØa t·ªëc ƒë·ªô h·ªçc v√† l·ªãch tr√¨nh nhi·ªát ƒë·ªô.

5 K·∫æT LU·∫¨N
Ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£ ƒë·ªÉ ph√°t tri·ªÉn c√°c m·∫°ng s√¢u hi·ªáu qu·∫£ th√¥ng qua th∆∞a th·ªõt h√≥a li√™n t·ª•c c√≥ c·∫•u tr√∫c, gi·∫£m chi ph√≠ t√≠nh to√°n kh√¥ng ch·ªâ c·ªßa suy lu·∫≠n m√† c√≤n c·ªßa hu·∫•n luy·ªán. Ph∆∞∆°ng ph√°p n√†y ƒë∆°n gi·∫£n ƒë·ªÉ tri·ªÉn khai v√† nhanh ch√≥ng th·ª±c thi; n√≥ t·ª± ƒë·ªông h√≥a qu√° tr√¨nh t√°i ph√¢n b·ªï c·∫•u tr√∫c m·∫°ng d∆∞·ªõi ng√¢n s√°ch t√†i nguy√™n th·ª±c t·∫ø. ·ª®ng d·ª•ng cho c√°c m·∫°ng s√¢u ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i tr√™n nhi·ªÅu nhi·ªám v·ª• kh√°c nhau cho th·∫•y r·∫±ng ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i nh·∫•t qu√°n t·∫°o ra c√°c m√¥ h√¨nh v·ªõi s·ª± ƒë√°nh ƒë·ªïi ƒë·ªô ch√≠nh x√°c-hi·ªáu qu·∫£ t·ªët h∆°n so v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫°nh tranh, trong khi ƒë·∫°t ƒë∆∞·ª£c ti·∫øt ki·ªám chi ph√≠ hu·∫•n luy·ªán ƒë√°ng k·ªÉ.

L·ªùi c·∫£m ∆°n. C√¥ng tr√¨nh n√†y ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi Trung t√¢m CERES c·ªßa University of Chicago cho ƒêi·ªán to√°n Kh√¥ng th·ªÉ D·ª´ng v√† Qu·ªπ Khoa h·ªçc Qu·ªëc gia d∆∞·ªõi grant CNS-1956180.

9

--- TRANG 10 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
T√ÄI LI·ªÜU THAM KH·∫¢O
Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. In ICLR, 2019.
Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. Hashnet: Deep learning to hash by continuation. In ICCV, 2017.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587, 2017.
Kyunghyun Cho, Bart van Merrienboer, √áaglar G√ºl√ßehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.
Fran√ßois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017.
Ekin Dogus Cubuk, Barret Zoph, Dandelion Man√©, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data. In CVPR, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV, 2015.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In ICLR, 2019.
Ross B. Girshick. Fast R-CNN. In ICCV, 2015.
Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. MorphNet: Fast & simple resource-constrained structure learning of deep networks. In CVPR, 2018.
Priya Goyal, Piotr Doll√°r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv:1706.02677, 2017.
Sam Gross and Michael Wilber. Training and investigating residual nets. http://torch.ch/blog/2016/02/04/resnets.html, 2016.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In NeurIPS, 2016.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks. In NeurIPS, 2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In ICLR, 2016.
Bharath Hariharan, Pablo Arbelaez, Lubomir D. Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. IJCAI, 2018.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NeurIPS Deep Learning and Representation Learning Workshop, 2015.

10

--- TRANG 11 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, 2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.
Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. CondenseNet: An efficient DenseNet using learned group convolutions. In CVPR, 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In NeurIPS, 2016.
Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size. arXiv:1602.07360, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. In NeurIPS, 2012.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. http://www.cs.toronto.edu/~kriz/cifar.html, 2014.
Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning of neural networks with budget-aware regularization. In CVPR, 2019.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient ConvNets. In ICLR, 2017.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In UAI, 2019.
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning for efficient neural networks. In ICLR, 2020.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv:1312.4400, 2013.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR, 2019.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. SSD: single shot multibox detector. In ECCV, 2016.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization. In ICLR, 2018.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A filter level pruning method for deep neural network compression. In ICCV, 2017.

11

--- TRANG 12 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. In NeurIPS, 2018.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 1993.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry P. Vetrov. Variational dropout sparsifies deep neural networks. In ICML, 2017.
Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In ICML, 2018.
Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll√°r. On network design spaces for visual recognition. In ICCV, 2019.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: Imagenet classification using binary convolutional neural networks. In ECCV, 2016.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image classifier architecture search. In AAAI, 2019.
Pedro Savarese and Michael Maire. Learning implicitly recurrent CNNs through parameter sharing. In ICLR, 2019.
Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsification. In NeurIPS, 2020.
Laurent Sifre and PS Mallat. Rigid-motion scattering for image classification. PhD thesis, Ecole Polytechnique, CMAP, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. Network morphism. In ICML, 2016.
Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, and Hai Li. Learning intrinsic sparse structures within long short-term memory. In ICLR, 2018.
Wei Wen, Feng Yan, Yiran Chen, and Hai Li. Autogrow: Automatic layer growing in deep convolutional networks. In KDD, 2020.
Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari. Discovering neural wirings. In NeurIPS, 2019.
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR, 2019.
Lingxi Xie and Alan L Yuille. Genetic cnn. In ICCV, 2017.
Saining Xie, Alexander Kirillov, Ross B. Girshick, and Kaiming He. Exploring randomly wired neural networks for image recognition. In ICCV, 2019a.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. In ICLR, 2019b.

12

--- TRANG 13 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptive inference with stochastic feature sampling and interpolation. In ECCV, 2020.
Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. NetAdapt: Platform-aware neural network adaptation for mobile applications. In ECCV, 2018.
Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating the search phase of neural architecture search. In ICLR, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv:1409.2329, 2014.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In CVPR, 2018.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In ICLR, 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018.

13

--- TRANG 14 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
A PH·ª§ L·ª§C
A.1 PH√ÇN T√çCH CHI TI·∫æT H·ª¨N V·ªÄ PH√ÅT TRI·ªÇN NH·∫¨N TH·ª®C NG√ÇN S√ÅCH
Vi·ªác ti·∫øn h√†nh t√¨m ki·∫øm l∆∞·ªõi tr√™n c√°c tham s·ªë ƒë√°nh ƒë·ªïi Œª1 v√† Œª2 c·ª±c k·ª≥ t·∫ª nh·∫°t v√† t·ªën th·ªùi gian. V√≠ d·ª•, ƒë·ªÉ ph√°t tri·ªÉn m·ªôt m·∫°ng hi·ªáu qu·∫£ tr√™n CIFAR-10, ng∆∞·ªùi ta c·∫ßn l·∫∑p l·∫°i nhi·ªÅu l·∫ßn m·ªôt l·∫ßn ch·∫°y hu·∫•n luy·ªán 160-epoch, v√† sau ƒë√≥ ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t t·ª´ t·∫•t c·∫£ c√°c ·ª©ng vi√™n ƒë√£ ph√°t tri·ªÉn. ƒê·ªÉ tr√°nh quy tr√¨nh l·∫∑p ƒëi l·∫∑p l·∫°i t·∫ª nh·∫°t n√†y, thay v√¨ s·ª≠ d·ª•ng c√°c h·∫±ng s·ªë Œª1 v√† Œª2, ch√∫ng t√¥i c·∫≠p nh·∫≠t ƒë·ªông Œª1 v√† Œª2 trong t·ªëi ∆∞u h√≥a ph√°t tri·ªÉn nh·∫≠n th·ª©c ng√¢n s√°ch m·ªôt l·∫ßn c·ªßa ch√∫ng t√¥i.

·ªû ƒë√¢y ch√∫ng t√¥i th·∫£o lu·∫≠n v·ªÅ c√°ch ph√°t tri·ªÉn ƒë·ªông nh·∫≠n th·ª©c ng√¢n s√°ch ho·∫°t ƒë·ªông trong ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i. Kh√¥ng m·∫•t t√≠nh t·ªïng qu√°t, ch√∫ng t√¥i r√∫t ra quy t·∫Øc c·∫≠p nh·∫≠t SGD c·ªßa mc cho s·ªë h·∫°ng ch√≠nh quy h√≥a ‚Ñì0 trong Eq. 3 nh∆∞:
mc‚Üêmc‚àíŒ∑Œªbase
1‚àÜuŒ¥‚Ñì
Œ¥mc‚àíŒ∑¬µŒªbase
1‚àÜumc (8)
trong ƒë√≥ Œ∑ l√† t·ªëc ƒë·ªô h·ªçc v√† ¬µ l√† h·ªá s·ªë weight decay. ·ªû ƒë·∫ßu c√°c epoch ph√°t tri·ªÉn, khi ki·∫øn tr√∫c b·ªã th∆∞a th·ªõt qu√° m·ª©c, ‚àÜu v√† Œªbase
1‚àÜu l√† c√°c gi√° tr·ªã √¢m. Khi ƒë√≥ c·∫≠p nh·∫≠t c·ªßa mc l√† theo h∆∞·ªõng ng∆∞·ª£c l·∫°i c·ªßa gradient c·ªßa s·ªë h·∫°ng ch√≠nh quy h√≥a ‚Ñì0, khuy·∫øn kh√≠ch th∆∞a th·ªõt h√≥a c·ªßa mc. K·∫øt qu·∫£ l√†, m·ªôt s·ªë mc c√≥ gi√° tr·ªã zero s·∫Ω ƒë∆∞·ª£c k√≠ch ho·∫°t v√† ƒë·ªô ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh ƒë∆∞·ª£c tƒÉng m·∫°nh ƒë·ªÉ c√≥ ƒë·ªß dung l∆∞·ª£ng cho vi·ªác hu·∫•n luy·ªán th√†nh c√¥ng. Sau ƒë√≥, ph√°t tri·ªÉn d·∫ßn y·∫øu ƒëi khi ƒë·ªô th∆∞a th·ªõt c·ªßa m·∫°ng ti·∫øp c·∫≠n ng√¢n s√°ch (‚àÜu v·ªÅ zero). L∆∞u √Ω r·∫±ng n·∫øu ki·∫øn tr√∫c b·ªã tham s·ªë h√≥a qu√° m·ª©c, ‚àÜu v√† Œªbase
1‚àÜu tr·ªü th√†nh d∆∞∆°ng v√† quy t·∫Øc c·∫≠p nh·∫≠t c·ªßa SGD gi·ªëng nh∆∞ c·ªßa ch√≠nh quy h√≥a ‚Ñì0. Nh∆∞ v·∫≠y, ph√°t tri·ªÉn nh·∫≠n th·ª©c ng√¢n s√°ch c·ªßa ch√∫ng t√¥i c√≥ th·ªÉ t·ª± ƒë·ªông v√† ƒë·ªông th√≠ch ·ª©ng ƒë·ªô ph·ª©c t·∫°p ki·∫øn tr√∫c kh√¥ng ch·ªâ d·ª±a tr√™n m·∫•t m√°t nhi·ªám v·ª• LE m√† c√≤n tr√™n y√™u c·∫ßu ng√¢n s√°ch th·ª±c t·∫ø trong qu√° tr√¨nh hu·∫•n luy·ªán m·ªôt l·∫ßn.

Ch√∫ng t√¥i c≈©ng l∆∞u √Ω r·∫±ng c√°c ph∆∞∆°ng ph√°p NAS th∆∞·ªùng s·ª≠ d·ª•ng ƒë·ªô ch√≠nh x√°c validation nh∆∞ m·ªôt m·ª•c ti√™u trong giai ƒëo·∫°n t·ªëi ∆∞u h√≥a ki·∫øn tr√∫c c·ªßa h·ªç, c√≥ th·ªÉ y√™u c·∫ßu m·ªôt s·ªë ki·∫øn th·ª©c tr∆∞·ªõc v·ªÅ ƒë·ªô ch√≠nh x√°c validation tr√™n m·ªôt dataset ƒë√£ cho. Quy tr√¨nh ph√°t tri·ªÉn c·ªßa ch√∫ng t√¥i ch·ªçn ng√¢n s√°ch th∆∞a th·ªõt thay v√¨ ƒë·ªô ch√≠nh x√°c nh∆∞ m·ª•c ti√™u v√¨: (1) Trong qu√° tr√¨nh ph√°t tri·ªÉn, ƒë·ªô ch√≠nh x√°c validation b·ªã ·∫£nh h∆∞·ªüng kh√¥ng ch·ªâ b·ªüi ki·∫øn tr√∫c m√† c√≤n b·ªüi tr·ªçng s·ªë m√¥ h√¨nh. Vi·ªác s·ª≠ d·ª•ng tr·ª±c ti·∫øp ‚àÜacc c√≥ th·ªÉ d·∫´n ƒë·∫øn t·ªëi ∆∞u h√≥a ki·∫øn tr√∫c kh√¥ng t·ªëi ∆∞u. (2) M·ªôt m·ª•c ti√™u ng√¢n s√°ch th∆∞a th·ªõt th·ª±c t·∫ø h∆°n v√† d·ªÖ ƒë·∫∑t h∆°n theo thi·∫øt b·ªã m·ª•c ti√™u ƒë·ªÉ tri·ªÉn khai.

A.2 CHI TI·∫æT C·ª¶A C√ÅC DATASET ƒê√ÅNH GI√Å
ƒê√°nh gi√° ƒë∆∞·ª£c ti·∫øn h√†nh tr√™n c√°c nhi·ªám v·ª• kh√°c nhau ƒë·ªÉ ch·ª©ng minh hi·ªáu qu·∫£ c·ªßa ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t c·ªßa ch√∫ng t√¥i. ƒê·ªëi v·ªõi ph√¢n lo·∫°i h√¨nh ·∫£nh, ch√∫ng t√¥i s·ª≠ d·ª•ng CIFAR-10 (Krizhevsky et al., 2014) v√† ImageNet (Deng et al., 2009): CIFAR-10 bao g·ªìm 60,000 h√¨nh ·∫£nh c·ªßa 10 l·ªõp, v·ªõi 6,000 h√¨nh ·∫£nh m·ªói l·ªõp. C√°c t·∫≠p train v√† test ch·ª©a 50,000 v√† 10,000 h√¨nh ·∫£nh t∆∞∆°ng ·ª©ng. ImageNet l√† m·ªôt dataset l·ªõn cho nh·∫≠n d·∫°ng th·ªã gi√°c ch·ª©a h∆°n 1.2M h√¨nh ·∫£nh trong t·∫≠p hu·∫•n luy·ªán v√† 50K h√¨nh ·∫£nh trong t·∫≠p validation bao ph·ªß 1,000 danh m·ª•c. ƒê·ªëi v·ªõi ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a, ch√∫ng t√¥i s·ª≠ d·ª•ng benchmark PASCAL VOC 2012 (Everingham et al., 2015) ch·ª©a 20 l·ªõp ƒë·ªëi t∆∞·ª£ng foreground v√† m·ªôt l·ªõp background. Dataset g·ªëc ch·ª©a 1,464 (train), 1,449 (val), v√† 1,456 (test) h√¨nh ·∫£nh ƒë∆∞·ª£c g·∫Øn nh√£n ·ªü m·ª©c pixel cho hu·∫•n luy·ªán, validation, v√† testing, t∆∞∆°ng ·ª©ng. Dataset ƒë∆∞·ª£c tƒÉng c∆∞·ªùng b·ªüi c√°c ch√∫ th√≠ch b·ªï sung ƒë∆∞·ª£c cung c·∫•p b·ªüi (Hariharan et al., 2011), k·∫øt qu·∫£ l√† 10,582 h√¨nh ·∫£nh hu·∫•n luy·ªán. ƒê·ªëi v·ªõi m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, ch√∫ng t√¥i s·ª≠ d·ª•ng dataset Penn Treebank (PTB) c·∫•p t·ª´ (Marcus et al., 1993) bao g·ªìm 929k t·ª´ hu·∫•n luy·ªán, 73k t·ª´ validation, v√† 82k t·ª´ test, v·ªõi 10,000 t·ª´ duy nh·∫•t trong t·ª´ v·ª±ng c·ªßa n√≥.

A.3 C√ÅC M√î H√åNH BASELINE CH∆ØA ƒê∆Ø·ª¢C C·∫ÆT T·ªàA
ƒê·ªëi v·ªõi CIFAR-10, ch√∫ng t√¥i s·ª≠ d·ª•ng VGG-16 (Simonyan & Zisserman, 2015) v·ªõi BatchNorm (Ioffe & Szegedy, 2015), ResNet-20 (He et al., 2016) v√† WideResNet-28-10 (Zagoruyko & Komodakis, 2016) l√†m baseline. Ch√∫ng t√¥i √°p d·ª•ng m·ªôt s∆° ƒë·ªì tƒÉng c∆∞·ªùng d·ªØ li·ªáu chu·∫©n (shifting/mirroring) theo (Lin et al., 2013; Huang et al., 2016), v√† chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫ßu v√†o v·ªõi gi√° tr·ªã trung b√¨nh k√™nh v√† ƒë·ªô l·ªách chu·∫©n. L∆∞u √Ω r·∫±ng ch√∫ng t√¥i s·ª≠ d·ª•ng phi√™n b·∫£n CIFAR c·ªßa ResNet-201, VGG-162, v√† WideResNet-28-103. VGG-16, ResNet-20, v√† WideResNet-28-10 ƒë∆∞·ª£c hu·∫•n luy·ªán trong 160, 160, v√† 200 epoch, t∆∞∆°ng ·ª©ng, v·ªõi

1https://github.com/akamaster/pytorch resnet cifar10/blob/master/resnet.py
2https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py
3https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide resnet.py

14

--- TRANG 15 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
batch size 128 v√† t·ªëc ƒë·ªô h·ªçc ban ƒë·∫ßu 0.1. ƒê·ªëi v·ªõi VGG-16 v√† ResNet-20, ch√∫ng t√¥i chia t·ªëc ƒë·ªô h·ªçc cho 10 t·∫°i epoch 80 v√† 120, v√† ƒë·∫∑t weight decay v√† momentum l√† 10‚àí4 v√† 0.9. ƒê·ªëi v·ªõi WideResNet-28-10, t·ªëc ƒë·ªô h·ªçc ƒë∆∞·ª£c chia cho 5 t·∫°i epoch 60, 120, v√† 160; weight decay v√† momentum ƒë∆∞·ª£c ƒë·∫∑t l√† 5√ó10‚àí4 v√† 0.9. ƒê·ªëi v·ªõi ImageNet, ch√∫ng t√¥i hu·∫•n luy·ªán c√°c m√¥ h√¨nh baseline ResNet-50 v√† MobileNetV1 theo c√°c b√†i b√°o t∆∞∆°ng ·ª©ng. Ch√∫ng t√¥i √°p d·ª•ng c√πng s∆° ƒë·ªì tƒÉng c∆∞·ªùng d·ªØ li·ªáu nh∆∞ trong (Gross & Wilber, 2016) v√† b√°o c√°o ƒë·ªô ch√≠nh x√°c validation top-1. ƒê·ªëi v·ªõi ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a, hi·ªáu su·∫•t ƒë∆∞·ª£c ƒëo b·∫±ng intersection-over-union (IOU) pixel ƒë∆∞·ª£c t√≠nh trung b√¨nh tr√™n 21 l·ªõp (mIOU). Ch√∫ng t√¥i s·ª≠ d·ª•ng Deeplab-v3-ResNet-1014(Chen et al., 2017) l√†m m√¥ h√¨nh baseline theo chi ti·∫øt hu·∫•n luy·ªán trong (Chen et al., 2017). ƒê·ªëi v·ªõi m√¥ h√¨nh h√≥a ng√¥n ng·ªØ, ch√∫ng t√¥i s·ª≠ d·ª•ng vanilla two-layer stacked LSTM (Zaremba et al., 2014) l√†m baseline. T·ª∑ l·ªá gi·ªØ dropout l√† 0.35 cho m√¥ h√¨nh baseline. K√≠ch th∆∞·ªõc t·ª´ v·ª±ng, k√≠ch th∆∞·ªõc embedding, v√† k√≠ch th∆∞·ªõc hidden c·ªßa c√°c stacked LSTM ƒë∆∞·ª£c ƒë·∫∑t l√† 10,000, 1,500, v√† 1,500, t∆∞∆°ng ·ª©ng, nh·∫•t qu√°n v·ªõi c√°c c√†i ƒë·∫∑t trong (Zaremba et al., 2014).

A.4 PH√ÅT TRI·ªÇN K√äNH MOBILENETV1 TR√äN IMAGENET
ƒê·ªÉ x√°c th·ª±c th√™m hi·ªáu qu·∫£ c·ªßa ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t tr√™n c√°c m·∫°ng compact, ch√∫ng t√¥i ph√°t tri·ªÉn c√°c b·ªô l·ªçc c·ªßa MobileNetV1 tr√™n ImageNet v√† so s√°nh hi·ªáu su·∫•t c·ªßa ph∆∞∆°ng ph√°p ch√∫ng t√¥i v·ªõi k·∫øt qu·∫£ ƒë∆∞·ª£c b√°o c√°o tr·ª±c ti·∫øp trong c√°c b√†i b√°o t∆∞∆°ng ·ª©ng, nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong B·∫£ng 6. Trong c√°c th√≠ nghi·ªám MobileNetV1, theo c√πng c√†i ƒë·∫∑t v·ªõi Netadapt (Yang et al., 2018), ch√∫ng t√¥i √°p d·ª•ng ph∆∞∆°ng ph√°p c·ªßa m√¨nh tr√™n c·∫£ (1) c√†i ƒë·∫∑t nh·ªè: ph√°t tri·ªÉn MobileNetV1(128) v·ªõi multiplier 0.5 trong khi ƒë·∫∑t multiplier c·ªßa m√¥ h√¨nh g·ªëc l√† 0.25 ƒë·ªÉ so s√°nh v√† (2) c√†i ƒë·∫∑t l·ªõn: ph√°t tri·ªÉn MobileNetV1(224) chu·∫©n trong khi ƒë·∫∑t multiplier c·ªßa m√¥ h√¨nh g·ªëc l√† 0.75 ƒë·ªÉ so s√°nh. L∆∞u √Ω r·∫±ng MobileNetV1 l√† m·ªôt trong nh·ªØng m·∫°ng compact nh·∫•t, v√† do ƒë√≥ th·ª≠ th√°ch h∆°n ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a so v·ªõi c√°c m·∫°ng l·ªõn kh√°c. Ph∆∞∆°ng ph√°p ph√°t tri·ªÉn chi ph√≠ th·∫•p h∆°n c·ªßa ch√∫ng t√¥i v·∫´n c√≥ th·ªÉ t·∫°o ra m·ªôt m√¥ h√¨nh MobileNetV1 th∆∞a th·ªõt h∆°n so v·ªõi c√°c ph∆∞∆°ng ph√°p c·∫°nh tranh.

B·∫£ng 6: T·ªïng quan v·ªÅ hi·ªáu su·∫•t c·∫Øt t·ªâa c·ªßa m·ªói thu·∫≠t to√°n tr√™n MobileNetV1 ImageNet.

| M√¥ h√¨nh | Ph∆∞∆°ng ph√°p | Top-1 Val Acc(%) | FLOPs(%) | Ti·∫øt ki·ªám Chi ph√≠ Hu·∫•n luy·ªán(√ó) |
|---------|-------------|------------------|----------|----------------------------------|
| | Original(25%) | 45.1 (+0.0) | 100 | 1.0(√ó) |
| MobileNet | MorphNet | 46.0 (+0.9) | 110 | 0.9(√ó) |
| V1(128) | Netadapt | 46.3 (+1.2) | 81 | 1.1(√ó) |
| | Ours | 46.0 (+0.9) | 73 | 1.7(√ó) |
| MobileNet | Original(75%) | 68.8 (+0.0) | 100 | 1.0(√ó) |
| V1(224) | Netadapt | 69.1 (+0.3) | 87 | 1.2(√ó) |
| | Ours | 69.3 (+0.5) | 83 | 1.5(√ó) |

A.5 DEEPLAB-V3-RESNET-101 TR√äN PASCAL VOC 2012
Ch√∫ng t√¥i c≈©ng ki·ªÉm tra hi·ªáu qu·∫£ c·ªßa ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t tr√™n m·ªôt nhi·ªám v·ª• ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a b·∫±ng c√°ch ph√°t tri·ªÉn s·ªë l∆∞·ª£ng b·ªô l·ªçc c·ªßa m√¥ h√¨nh Deeplab-v3-ResNet-101 t·ª´ ƒë·∫ßu tr·ª±c ti·∫øp tr√™n dataset PASCAL VOC 2012. Ch√∫ng t√¥i √°p d·ª•ng ph∆∞∆°ng ph√°p c·ªßa m√¨nh cho c·∫£ backbone ResNet-101 v√† m√¥-ƒëun ASPP. So v·ªõi baseline, m·∫°ng ƒë∆∞·ª£c t·∫°o ra cu·ªëi c√πng gi·∫£m FLOPs 58.5% v√† s·ªë l∆∞·ª£ng tham s·ªë 49.8%, trong khi x·∫•p x·ªâ duy tr√¨ mIoU (76.5% xu·ªëng 76.4%). Xem B·∫£ng 7.

B·∫£ng 7: K·∫øt qu·∫£ tr√™n dataset PASCAL VOC.

| M√¥ h√¨nh | Ph∆∞∆°ng ph√°p | mIOU | Params(M) | FLOPs(%) | Ti·∫øt ki·ªám Chi ph√≠ Hu·∫•n luy·ªán(√ó) |
|---------|-------------|------|-----------|----------|----------------------------------|
| Deeplab | Original | 76.5 (-0.0) | 58.0 (100%) | 100 | 1.0(√ó) |
| -v3- | L1-Pruning | 75.1 (-1.4) | 45.7 (78.8%) | 62.5 | 1.3(√ó) |
| ResNet101 | Ours | 76.4 (-0.1) | 29.1 (50.2%) | 41.5 | 2.3(√ó) |

4https://github.com/chenxi116/DeepLabv3.pytorch

15

--- TRANG 16 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
A.6 M·ªû R·ªòNG CHO C√ÅC T·∫æ B√ÄO H·ªíI QUY TR√äN DATASET PTB
Ch√∫ng t√¥i t·∫≠p trung v√†o LSTM (Hochreiter & Schmidhuber, 1997) v·ªõi lh neuron ·∫©n, m·ªôt bi·∫øn th·ªÉ ph·ªï bi·∫øn5 c·ªßa RNN h·ªçc c√°c ph·ª• thu·ªôc d√†i h·∫°n:
ft=œÉg((Wf‚äô(emT
c))xt+ (Uf‚äô(mcmT
c))ht‚àí1+bf)
it=œÉg((Wi‚äô(emT
c))xt+ (Ui‚äô(mcmT
c))ht‚àí1+bi)
ot=œÉg((Wo‚äô(emT
c))xt+ (Uo‚äô(mcmT
c))ht‚àí1+bo)
Àúct=œÉh((Wc‚äô(emT
c))xt+ (Uc‚äô(mcmT
c))ht‚àí1+bc)
ct=ft‚äôct‚àí1+it‚äôÀúct, h t=ot‚äôœÉh(ct)s.t. m c‚àà {0,1}lh,e= 1lh, (9)
trong ƒë√≥ œÉg l√† h√†m sigmoid, ‚äô bi·ªÉu th·ªã ph√©p nh√¢n element-wise v√† œÉh l√† h√†m hyperbolic tangent. xt bi·ªÉu th·ªã vector ƒë·∫ßu v√†o t·∫°i time-step t, ht bi·ªÉu th·ªã tr·∫°ng th√°i ·∫©n hi·ªán t·∫°i, v√† ct bi·ªÉu th·ªã tr·∫°ng th√°i t·∫ø b√†o b·ªô nh·ªõ d√†i h·∫°n. Wf, Wi, Wo, Wc bi·ªÉu th·ªã c√°c ma tr·∫≠n tr·ªçng s·ªë input-to-hidden v√† Uf, Ui, Uo, Uc bi·ªÉu th·ªã c√°c ma tr·∫≠n tr·ªçng s·ªë hidden-to-hidden. mc l√† ch·ªâ th·ªã nh·ªã ph√¢n v√† ƒë∆∞·ª£c chia s·∫ª qua t·∫•t c·∫£ c√°c c·ªïng ƒë·ªÉ ki·ªÉm so√°t ƒë·ªô th∆∞a th·ªõt c·ªßa c√°c neuron ·∫©n.

Ch√∫ng t√¥i so s√°nh ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t c·ªßa ch√∫ng t√¥i v·ªõi ISS d·ª±a tr√™n vanilla two-layer stacked LSTM. Nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong B·∫£ng 8, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i t√¨m th·∫•y c·∫•u tr√∫c m√¥ h√¨nh compact h∆°n v·ªõi chi ph√≠ hu·∫•n luy·ªán th·∫•p h∆°n, trong khi ƒë·∫°t ƒë∆∞·ª£c perplexity t∆∞∆°ng t·ª± tr√™n c·∫£ t·∫≠p validation v√† test. Nh·ªØng c·∫£i thi·ªán n√†y c√≥ th·ªÉ do ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ph√°t tri·ªÉn v√† c·∫Øt t·ªâa ƒë·ªông c√°c neuron ·∫©n t·ª´ tr·∫°ng th√°i r·∫•t ƒë∆°n gi·∫£n h∆∞·ªõng t·ªõi m·ªôt s·ª± ƒë√°nh ƒë·ªïi t·ªët h∆°n gi·ªØa ƒë·ªô ph·ª©c t·∫°p m√¥ h√¨nh v√† hi·ªáu su·∫•t so v·ªõi ISS, ch·ªâ ƒë∆°n gi·∫£n s·ª≠ d·ª•ng group lasso ƒë·ªÉ ph·∫°t norm c·ªßa t·∫•t c·∫£ c√°c nh√≥m m·ªôt c√°ch t·∫≠p th·ªÉ cho s·ª± compact.

B·∫£ng 8: K·∫øt qu·∫£ tr√™n dataset PTB.

| Ph∆∞∆°ng ph√°p | Perplexity (val,test) | C·∫•u tr√∫c Cu·ªëi c√πng | Weight(M) | FLOPs(%) | Ti·∫øt ki·ªám Chi ph√≠ Hu·∫•n luy·ªán(√ó) |
|-------------|----------------------|-------------------|-----------|----------|----------------------------------|
| Original | (82.57, 78.57) | (1500, 1500) | 66.0M (100%) | 100 | 1.0(√ó) |
| ISS | (82.59, 78.65 ) | (373, 315) | 21.8M (33.1%) | 13.4 | 3.8(√ó) |
| Ours | ( 82.46 , 78.68) | (310, 275) | 20.6M (31.2%) | 11.9 | 5.1 (√ó) |

A.7 C√ÅC BI·∫æN TH·ªÇ C·ª¶A KI·∫æN TR√öC H·∫†T GI·ªêNG BAN ƒê·∫¶U
Trong B·∫£ng 9, ch√∫ng t√¥i th·ª±c hi·ªán so s√°nh chi ti·∫øt gi·ªØa c√°c bi·∫øn th·ªÉ ki·∫øn tr√∫c h·∫°t gi·ªëng ban ƒë·∫ßu c·ªßa ch√∫ng t√¥i v√† AutoGrow (Wen et al., 2020). ƒê·ªëi v·ªõi c·∫£ ch√∫ng t√¥i v√† AutoGrow, "Basic" v√† "Bottleneck" ƒë·ªÅ c·∫≠p ƒë·∫øn ResNet v·ªõi c√°c kh·ªëi residual basic v√† bottleneck chu·∫©n, trong khi "PlainLayers" ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c k·∫øt h·ª£p l·ªõp convolutional, batch normalization, v√† ReLU ƒë∆∞·ª£c x·∫øp ch·ªìng. T∆∞∆°ng t·ª± nh∆∞ ResNet chu·∫©n, ƒë·ªëi v·ªõi c√°c bi·∫øn th·ªÉ c·ªßa ki·∫øn tr√∫c h·∫°t gi·ªëng, ch√∫ng t√¥i √°p d·ª•ng ba giai ƒëo·∫°n cho CIFAR-10 v√† b·ªën giai ƒëo·∫°n cho ImageNet. PlainNet c√≥ th·ªÉ thu ƒë∆∞·ª£c b·∫±ng c√°ch ƒë∆°n gi·∫£n lo·∫°i b·ªè shortcut t·ª´ c√°c bi·∫øn th·ªÉ h·∫°t gi·ªëng ResNet n√†y v·ªõi s·ªë giai ƒëo·∫°n b·∫±ng nhau. ƒê·ªëi v·ªõi m·ªói giai ƒëo·∫°n, ch√∫ng t√¥i b·∫Øt ƒë·∫ßu t·ª´ ch·ªâ m·ªôt ƒë∆°n v·ªã ph√°t tri·ªÉn, trong ƒë√≥ s·ªë l∆∞·ª£ng b·ªô l·ªçc ban ƒë·∫ßu c≈©ng ƒë∆∞·ª£c kh·ªüi t·∫°o t·∫°i m·ªôt cho ph√°t tri·ªÉn k√™nh.

A.8 THEO D√ïI TH∆ØA TH·ªöT H√ìA B·∫§T C·ª® L√öC N√ÄO TRONG QU√Å TR√åNH PH√ÅT TRI·ªÇN K√äNH
H√¨nh 7 v√† H√¨nh 8 cho th·∫•y ƒë·ªông l·ª±c c·ªßa t·ª∑ l·ªá k√™nh ph√°t tri·ªÉn th·ªùi gian hu·∫•n luy·ªán c·ªßa ResNet-20 v√† VGG-16 tr√™n CIFAR-10, t∆∞∆°ng ·ª©ng. ƒê·ªÉ ph√¢n t√≠ch t·ªët h∆°n c√°c m·∫´u ph√°t tri·ªÉn, ch√∫ng t√¥i visualize ƒë·ªông l·ª±c k√™nh ƒë∆∞·ª£c nh√≥m theo giai ƒëo·∫°n trong H√¨nh 9 cho ResNet-20 v√† H√¨nh 10 cho VGG-16, t∆∞∆°ng ·ª©ng. L∆∞u √Ω r·∫±ng, ƒë·ªëi v·ªõi VGG-16, ch√∫ng t√¥i chia n√≥ th√†nh 5 giai ƒëo·∫°n d·ª±a tr√™n v·ªã tr√≠ l·ªõp pooling v√† chu·∫©n h√≥a t·ª∑ l·ªá k√™nh b·∫±ng 0.5 ƒë·ªÉ visualization t·ªët h∆°n. Ch√∫ng t√¥i th·∫•y r·∫±ng ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ph√°t tri·ªÉn nhi·ªÅu k√™nh h∆°n c·ªßa c√°c l·ªõp s·ªõm h∆°n trong m·ªói giai ƒëo·∫°n c·ªßa ResNet-20. Ngo√†i ra, ƒë·ªô th∆∞a th·ªõt k√™nh cu·ªëi c√πng c·ªßa ResNet-20 ƒë·ªìng ƒë·ªÅu h∆°n do c√°c k·∫øt n·ªëi residual.

16

--- TRANG 17 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021
B·∫£ng 9: So s√°nh chi ti·∫øt gi·ªØa c√°c bi·∫øn th·ªÉ ki·∫øn tr√∫c h·∫°t gi·ªëng c·ªßa ph∆∞∆°ng ph√°p ch√∫ng t√¥i v√† AutoGrow (Wen et al., 2020). Trong thu·∫≠t ng·ªØ ƒë∆°n v·ªã ph√°t tri·ªÉn, "Basic" v√† "Bottleneck" ƒë·ªÅ c·∫≠p ƒë·∫øn ResNet v·ªõi c√°c kh·ªëi residual basic v√† bottleneck chu·∫©n trong khi "PlainLayers" ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c k·∫øt h·ª£p l·ªõp convolutional chu·∫©n, BN, v√† ReLu trong c√°c m·∫°ng gi·ªëng VGG kh√¥ng c√≥ shortcut.

| H·ªç | Bi·∫øn th·ªÉ | Ph∆∞∆°ng ph√°p | Ph√°t tri·ªÉn K√™nh | ƒê∆°n v·ªã Ph√°t tri·ªÉn | Giai ƒëo·∫°n | Shortcut |
|-----|----------|-------------|-----------------|-------------------|-----------|----------|
| | Basic3ResNet | Ours | ‚úì | Basic | 3 | ‚úì |
| ResNet | | AutoGrow | ‚úó | ‚úì | | |
| | Bottleneck4ResNet | Ours | ‚úì | Bottleneck | 4 | ‚úì |
| | | AutoGrow | ‚úó | ‚úì | | |
| | Plain3Net | Ours | ‚úì | PlainLayers | 3 | ‚úó |
| VGG-like | | AutoGrow | ‚úó | ‚úó | | |
| | Plain4Net | Ours | ‚úì | PlainLayers | 4 | ‚úó |
| | | AutoGrow | ‚úó | ‚úó | | |

[C√ÅC H√åNH ·∫¢NH V√Ä BI·ªÇU ƒê·ªí TI·∫æP THEO...]

A.9 PH√ÅT TRI·ªÇN NH·∫¨N TH·ª®C NG√ÇN S√ÅCH D·ª∞A TR√äN FLOPS
Ch√∫ng t√¥i c≈©ng ƒëi·ªÅu tra hi·ªáu qu·∫£ c·ªßa vi·ªác ƒë·∫∑t m·ª•c ti√™u FLOPs cho ph√°t tri·ªÉn nh·∫≠n th·ª©c ng√¢n s√°ch trong H√¨nh 11. Ch√∫ng t√¥i quan s√°t xu h∆∞·ªõng t∆∞∆°ng t·ª± gi·ªØa c·∫Øt t·ªâa ƒë·ªìng ƒë·ªÅu, ph√°t tri·ªÉn c·ªßa ch√∫ng t√¥i, v√† thi·∫øt k·∫ø tr·ª±c ti·∫øp c·ªßa ch√∫ng t√¥i: trong h·∫ßu h·∫øt c√°c c√†i ƒë·∫∑t ng√¢n s√°ch FLOPs, ph∆∞∆°ng ph√°p ph√°t tri·ªÉn c·ªßa ch√∫ng t√¥i v∆∞·ª£t tr·ªôi so v·ªõi thi·∫øt k·∫ø tr·ª±c ti·∫øp v√† c·∫Øt t·ªâa ƒë·ªìng ƒë·ªÅu. Ch√∫ng t√¥i c≈©ng quan s√°t r·∫±ng khi ƒë·∫∑t m·ª•c ti√™u FLOPs th∆∞a th·ªõt c·ª±c k·ª≥ (v√≠ d·ª•, 85%), ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ƒë·∫°t ƒë∆∞·ª£c ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n so v·ªõi hai bi·∫øn th·ªÉ kh√°c. L√Ω do l√† ph√°t tri·ªÉn k√™nh c·ªßa ch√∫ng t√¥i b·ªã bu·ªôc ch·ªâ ph√°t tri·ªÉn ki·∫øn tr√∫c t·ª´ ~99% th∆∞a th·ªõt l√™n ~85% FLOPs v√† ~90% th∆∞a th·ªõt tham s·ªë, trong ƒë√≥ c√°c m√¥ h√¨nh kh√¥ng th·ªÉ c√≥ ƒë·ªß dung l∆∞·ª£ng ƒë·ªÉ ƒë∆∞·ª£c hu·∫•n luy·ªán t·ªët.

A.10 T∆Ø∆†NG T√ÅC GI·ªÆA T·ªêC ƒê·ªò H·ªåC V√Ä B·ªò L·∫¨P L·ªäCH NHI·ªÜT ƒê·ªò
Hai y·∫øu t·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn t·ªëc ƒë·ªô t·ªëi ∆∞u h√≥a ph√°t tri·ªÉn trong ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i: nhi·ªát ƒë·ªô v√† t·ªëc ƒë·ªô h·ªçc, l√† c√°c si√™u tham s·ªë ƒë∆∞·ª£c ki·ªÉm so√°t b·ªüi c√°c b·ªô l·∫≠p l·ªãch t∆∞∆°ng ·ª©ng c·ªßa ch√∫ng. Ch√∫ng t√¥i ƒë·∫ßu ti√™n visualize ƒë·ªông l·ª±c nhi·ªát ƒë·ªô ri√™ng bi·ªát theo c·∫•u tr√∫c trong H√¨nh 12 b·∫±ng c√°ch t√≠nh trung b√¨nh nhi·ªát ƒë·ªô m·ªói l·ªõp trong qu√° tr√¨nh ph√°t tri·ªÉn k√™nh ResNet-20 tr√™n CIFAR-10. Ch√∫ng t√¥i th·∫•y r·∫±ng nhi·ªát ƒë·ªô ƒëang tƒÉng v·ªõi t·ªëc ƒë·ªô kh√°c nhau cho c√°c k√™nh. Th∆∞·ªùng th√¨, t·ªëc ƒë·ªô h·ªçc th·∫•p v√† nhi·ªát ƒë·ªô cao trong c√°c epoch hu·∫•n luy·ªán mu·ªôn l√†m cho t·ªëi ∆∞u h√≥a ph√°t tri·ªÉn m·∫°ng tr·ªü n√™n r·∫•t ·ªïn ƒë·ªãnh. Trong H√¨nh 13, ch√∫ng t√¥i c·ªë √Ω decay Œ≥ trong b·ªô l·∫≠p l·ªãch nhi·ªát ƒë·ªô, ph·∫£n √°nh l·ªãch tr√¨nh decay t·ªëc ƒë·ªô h·ªçc, ƒë·ªÉ bu·ªôc ph√°t tri·ªÉn cho ƒë·∫øn cu·ªëi. Nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong H√¨nh 14, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i v·∫´n th√≠ch ·ª©ng m·ªôt s·ªë l·ªõp ngay c·∫£ ·ªü epoch cu·ªëi c√πng. Ch√∫ng t√¥i th·∫•y r·∫±ng s·ª± b·∫•t ·ªïn nh∆∞ v·∫≠y l√†m suy gi·∫£m hi·ªáu su·∫•t, v√¨ m·ªôt s·ªë b·ªô l·ªçc m·ªõi ƒë∆∞·ª£c ph√°t tri·ªÉn c√≥ th·ªÉ kh√¥ng c√≥ ƒë·ªß th·ªùi gian ƒë·ªÉ ƒë∆∞·ª£c hu·∫•n luy·ªán t·ªët.

5Kh√¥ng gian c·∫•u h√¨nh ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t c√≥ th·ªÉ d·ªÖ d√†ng √°p d·ª•ng cho vi·ªác n√©n GRU (Cho et al., 2014) v√† RNN vanilla.

17

--- TRANG 18 ---
ƒê∆∞·ª£c xu·∫•t b·∫£n nh∆∞ m·ªôt b√†i b√°o h·ªôi ngh·ªã t·∫°i ICLR 2021

[C√ÅC H√åNH ·∫¢NH V√Ä BI·ªÇU ƒê·ªí TI·∫æP THEO ƒê∆Ø·ª¢C M√î T·∫¢ NH∆Ø TRONG NGUY√äN VƒÇN]

18
