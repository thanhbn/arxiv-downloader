# 2007.15353.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2007.15353.pdf
# Kích thước tệp: 1205658 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
PHÁT TRIỂN MẠNG SÂUÙ HIỆU QUẢ
BẰNG THƯA THỚT LIÊN TỤC CÓ CẤU TRÚC
Xin Yuan
University of Chicago
yuanx@uchicago.eduPedro Savarese
TTI-Chicago
savarese@ttic.eduMichael Maire
University of Chicago
mmaire@uchicago.edu
TÓM TẮT
Chúng tôi phát triển một phương pháp phát triển kiến trúc mạng sâu trong quá trình huấn luyện, được điều khiển bởi sự kết hợp có nguyên tắc của các mục tiêu độ chính xác và độ thưa thớt. Không giống như các kỹ thuật cắt tỉa hoặc tìm kiếm kiến trúc hiện có hoạt động trên các mô hình có kích thước đầy đủ hoặc kiến trúc supernet, phương pháp của chúng tôi có thể bắt đầu từ một kiến trúc hạt giống nhỏ, đơn giản và phát triển cũng như cắt tỉa động cả các lớp và bộ lọc. Bằng cách kết hợp việc nới lỏng liên tục của tối ưu hóa cấu trúc mạng rời rạc với một sơ đồ lấy mẫu các mạng con thưa thớt, chúng tôi tạo ra các mạng nhỏ gọn, được cắt tỉa, đồng thời cũng giảm đáng kể chi phí tính toán của việc huấn luyện. Ví dụ, chúng tôi đạt được 49.7% tiết kiệm FLOPs suy luận và 47.4% tiết kiệm FLOPs huấn luyện so với ResNet-50 baseline trên ImageNet, trong khi duy trì độ chính xác top-1 75.2% — tất cả mà không có bất kỳ giai đoạn fine-tuning chuyên dụng nào. Các thí nghiệm trên CIFAR, ImageNet, PASCAL VOC, và Penn Treebank, với mạng tích chập cho phân loại hình ảnh và phân đoạn ngữ nghĩa, và mạng hồi quy cho mô hình hóa ngôn ngữ, chứng minh rằng chúng tôi vừa huấn luyện nhanh hơn vừa tạo ra các mạng hiệu quả hơn so với các phương pháp cắt tỉa hoặc tìm kiếm kiến trúc cạnh tranh.

1 GIỚI THIỆU
Mạng neural sâu là phương pháp chiếm ưu thế cho nhiều nhiệm vụ học máy, bao gồm phân loại hình ảnh (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), phát hiện đối tượng (Girshick, 2015; Liu et al., 2016), phân đoạn ngữ nghĩa (Long et al., 2015; Chen et al., 2017) và mô hình hóa ngôn ngữ (Zaremba et al., 2014; Vaswani et al., 2017; Devlin et al., 2019). Các mạng neural hiện đại bị tham số hóa quá mức và việc huấn luyện các mạng lớn hơn thường mang lại độ chính xác tổng quát hóa cải thiện. Nghiên cứu gần đây (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) minh họa xu hướng này thông qua việc tăng độ sâu và độ rộng của mạng neural tích chập (CNN). Tuy nhiên, việc huấn luyện tiêu tốn nhiều tính toán, và các triển khai trong thế giới thực thường bị giới hạn bởi ngân sách tham số và tính toán.

Tìm kiếm kiến trúc neural (NAS) (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Savarese & Maire, 2019) và các phương pháp cắt tỉa mô hình (Han et al., 2016; 2015; Guo et al., 2016) nhằm giảm những gánh nặng này. NAS giải quyết một vấn đề làm tăng thêm chi phí huấn luyện: không gian khổng lồ của các kiến trúc mạng có thể có. Trong khi việc điều chỉnh thủ công các chi tiết kiến trúc, chẳng hạn như cấu trúc kết nối của các lớp tích chập, có thể cải thiện hiệu suất (Iandola et al., 2016; Sifre & Mallat, 2014; Chollet, 2017; Howard et al., 2017; Zhang et al., 2018; Huang et al., 2018), một cách có nguyên tắc để rút ra các thiết kế như vậy vẫn còn khó nắm bắt. Các phương pháp NAS nhằm tự động hóa việc khám phá các kiến trúc có thể có, tạo ra một thiết kế hiệu quả cho một nhiệm vụ mục tiêu dưới các ràng buộc tài nguyên thực tế. Tuy nhiên, trong quá trình huấn luyện, hầu hết các phương pháp NAS hoạt động trên một kiến trúc supernet lớn, bao gồm các thành phần ứng viên vượt quá những thành phần cuối cùng được chọn để đưa vào mạng kết quả (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Savarese & Maire, 2019). Do đó, việc huấn luyện dựa trên NAS có thể thường kỹ lưỡng hơn, nhưng tốn kém tính toán hơn so với việc huấn luyện một kiến trúc được thiết kế thủ công đơn lẻ.

Các kỹ thuật cắt tỉa mô hình tương tự tập trung vào việc cải thiện hiệu quả tài nguyên của mạng neural trong quá trình suy luận, có thể với chi phí tăng chi phí huấn luyện. Các chiến lược phổ biến nhằm tạo ra một phiên bản nhẹ hơn của một kiến trúc mạng đã cho bằng cách loại bỏ các trọng số riêng lẻ (Han et al., 2015; 2016; Molchanov et al., 2017) hoặc các tập tham số có cấu trúc (Li et al., 2017; He et al., 2018; Luo et al., 2017). Tuy nhiên, phần lớn các phương pháp này huấn luyện một mô hình có kích thước đầy đủ trước khi cắt tỉa và,

1arXiv:2007.15353v2  [cs.LG]  6 Jun 2023

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Cấp 1: Theo kênh
Epoch Huấn luyện T=0,1,2…
Cấp 2: Theo lớp…
(a) Phát triển Lớp và Bộ lọc CNN
Epoch Huấn luyện
FLOPs
Phương pháp của chúng tôi
Cắt tỉa
Tìm kiếm Kiến trúc (b) Chi phí Huấn luyện theo Epoch
NAS Cắt tỉa Của chúng tôi
FLOPs (c) Tổng Chi phí Huấn luyện
Hình 1: Phát triển Mạng trong quá trình Huấn luyện. Chúng tôi định nghĩa một không gian cấu hình kiến trúc và đồng thời thích ứng cấu trúc mạng và trọng số. (a)Áp dụng phương pháp của chúng tôi cho CNN, chúng tôi duy trì các biến phụ trợ xác định cách phát triển và cắt tỉa cả bộ lọc (tức là theo kênh) và các lớp, tuân theo các ràng buộc tài nguyên thực tế. (b)Bằng cách bắt đầu với một mạng nhỏ và phát triển kích thước của nó, chúng tôi sử dụng ít tài nguyên hơn trong các epoch huấn luyện đầu, so với các phương pháp cắt tỉa hoặc NAS. (c)Do đó, phương pháp của chúng tôi giảm đáng kể tổng chi phí tính toán của việc huấn luyện, trong khi cung cấp các mạng được huấn luyện có kích thước và độ chính xác tương đương hoặc tốt hơn.

sau khi cắt tỉa, sử dụng các giai đoạn fine-tuning bổ sung để duy trì độ chính xác. Hubara et al. (2016) và Rastegari et al. (2016) đề xuất sử dụng trọng số và kích hoạt nhị phân, cho phép suy luận được hưởng lợi từ chi phí lưu trữ giảm và tính toán hiệu quả thông qua các phép toán đếm bit. Tuy nhiên, việc huấn luyện vẫn liên quan đến việc theo dõi trọng số độ chính xác cao cùng với các xấp xỉ độ chính xác thấp hơn.

Chúng tôi có một cái nhìn thống nhất về cắt tỉa và tìm kiếm kiến trúc, coi cả hai đều hoạt động trên một không gian cấu hình, và đề xuất một phương pháp phát triển động các mạng sâu bằng cách cấu hình lại kiến trúc của chúng một cách liên tục trong quá trình huấn luyện. Phương pháp của chúng tôi không chỉ tạo ra các mô hình với đặc tính suy luận hiệu quả, mà còn giảm chi phí tính toán của việc huấn luyện; xem Hình 1. Thay vì bắt đầu với một mạng có kích thước đầy đủ hoặc một supernet, chúng tôi bắt đầu từ các mạng hạt giống đơn giản và điều chỉnh dần (phát triển và cắt tỉa) chúng. Cụ thể, chúng tôi tham số hóa một không gian cấu hình kiến trúc với các biến chỉ thị điều chỉnh việc thêm hoặc loại bỏ các thành phần cấu trúc. Hình 2(a) cho thấy một ví dụ, dưới dạng không gian cấu hình hai cấp cho các lớp và bộ lọc CNN. Chúng tôi cho phép học các giá trị chỉ thị (và do đó, cấu trúc kiến trúc) thông qua việc kết hợp việc nới lỏng liên tục với lấy mẫu nhị phân, như được minh họa trong Hình 2(b). Một tham số nhiệt độ trên mỗi thành phần đảm bảo rằng các cấu trúc tồn tại lâu cuối cùng được nướng vào cấu hình kiến trúc rời rạc của mạng.

Trong khi AutoGrow được đề xuất gần đây (Wen et al., 2020) cũng tìm cách phát triển mạng trong quá trình huấn luyện, phương pháp kỹ thuật của chúng tôi khác đáng kể và dẫn đến các lợi thế thực tế đáng kể. Ở cấp độ kỹ thuật, AutoGrow thực hiện một quy trình tìm kiếm kiến trúc trên một cấu trúc mô-đun được xác định trước, tuân theo các chính sách phát triển và dừng được thiết kế thủ công, dựa trên độ chính xác. Ngược lại, chúng tôi tham số hóa các cấu hình kiến trúc và sử dụng gradient descent stochastic để học các biến phụ trợ chỉ định các thành phần cấu trúc, trong khi đồng thời huấn luyện trọng số trong các thành phần đó. Phương pháp kỹ thuật độc đáo của chúng tôi mang lại các lợi thế sau:

•Huấn luyện Nhanh bằng Phát triển: Huấn luyện là một quy trình thống nhất, từ đó người ta có thể yêu cầu cấu trúc mạng và trọng số liên quan bất cứ lúc nào. Không giống như AutoGrow và phần lớn các kỹ thuật cắt tỉa, fine-tuning để tối ưu hóa trọng số trong một kiến trúc được khám phá là tùy chọn. Chúng tôi đạt được kết quả xuất sắc ngay cả khi không có bất kỳ giai đoạn fine-tuning nào.

•Phương pháp Có nguyên tắc thông qua Học bằng Continuation + Sampling: Chúng tôi xây dựng phương pháp của mình theo tinh thần của các phương pháp học bằng continuation, nới lỏng một bài toán tối ưu hóa rời rạc thành một xấp xỉ liên tục ngày càng cứng nhắc. Quan trọng, chúng tôi giới thiệu một bước lấy mẫu bổ sung cho chiến lược này. Từ sự kết hợp này, chúng tôi có được tính linh hoạt của việc khám phá một kiến trúc supernet, nhưng hiệu quả tính toán của việc chỉ thực sự huấn luyện một mạng con hoạt động nhỏ hơn nhiều.

•Mục tiêu Tối ưu hóa Nhận thức Ngân sách: Các tham số điều chỉnh cấu hình kiến trúc của chúng tôi được cập nhật thông qua gradient descent. Chúng tôi có tính linh hoạt để xây dựng nhiều hàm mất mát nhạy cảm với tài nguyên, chẳng hạn như đếm tổng FLOPs, theo các tham số này.

•Khả năng Áp dụng Rộng: Mặc dù chúng tôi sử dụng phát triển tiến bộ của CNN theo chiều rộng và chiều sâu như một ví dụ thúc đẩy, kỹ thuật của chúng tôi áp dụng cho hầu như bất kỳ kiến trúc neural nào. Người ta có tính linh hoạt trong cách tham số hóa không gian cấu hình kiến trúc. Chúng tôi cũng cho thấy kết quả với LSTM.

Chúng tôi chứng minh những lợi thế này trong khi so sánh với các phương pháp NAS và cắt tỉa gần đây thông qua các thí nghiệm mở rộng về phân loại, phân đoạn ngữ nghĩa và mô hình hóa ngôn ngữ cấp từ.

2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
𝒙𝒊𝒏𝒙𝒐𝒖𝒕Bộ lọc Hoạt động = 11000…𝒙𝒊𝒏𝒙𝒐𝒖𝒕1100…
𝒙𝒊𝒏𝒙𝒐𝒖𝒕𝒙𝒊𝒏𝒙𝒐𝒖𝒕1010…Phát triểnTách rờiPhát triểnChỉ thị KênhChỉ thị KênhChỉ thị KênhT=0T=1Epoch Phát triển T=2…Cấp 1: Không gian Cấu hình Theo kênh
Cấp 2: Không gian Cấu hình Theo lớpBộ lọc Hoạt động = 2Bộ lọc Hoạt động = 2
1000…Chỉ thị LớpLớp Hoạt động = 1𝒙𝒊𝒏𝒙𝒐𝒖𝒕1100…Chỉ thị LớpLớp Hoạt động = 2𝒙𝒐𝒖𝒕0111…Lớp Hoạt động = 3Chỉ thị Lớp𝒙𝒊𝒏Phát triểnPhát triểnTách rờiPhát triển

(a) Không gian Cấu hình Kiến trúc cho CNN
Biến Mặt nạ Có thể huấn luyện𝒔Không gian Cấu hình Rời rạcNới lỏng Liên tục Tiến bộChỉ thị Nhị phân
11000𝒒∼𝑩𝒆𝒓𝒏(𝝈(𝜷𝒔))Mặt nạ liên tục𝜎(𝛽𝑠)Lấy mẫu000𝝈𝜷𝒔⊙𝒒 (b) Tối ưu hóa với Continuation Có cấu trúc

Hình 2: Khung Kỹ thuật. (a) Chúng tôi định kỳ tái cấu trúc một CNN bằng cách truy vấn các chỉ thị nhị phân xác định không gian cấu hình hai cấp cho bộ lọc và lớp. (b)Để làm cho tối ưu hóa khả thi trong khi phát triển mạng, chúng tôi rút ra các chỉ thị nhị phân này từ các biến mặt nạ liên tục có thể huấn luyện. Chúng tôi sử dụng một phần mở rộng có cấu trúc của thưa thớt hóa liên tục (Savarese et al., 2020), kết hợp với lấy mẫu. Các biến phụ trợ stochastic nhị phân q, được lấy mẫu theo σ(βs), tạo ra các thành phần rời rạc hoạt động tại một thời điểm cụ thể.

2 CÔNG TRÌNH LIÊN QUAN
Cắt tỉa Mạng. Các phương pháp cắt tỉa có thể được chia thành hai nhóm: những phương pháp cắt tỉa trọng số riêng lẻ và những phương pháp cắt tỉa các thành phần có cấu trúc. Các phương pháp cắt tỉa dựa trên trọng số riêng lẻ khác nhau về tiêu chí loại bỏ. Ví dụ, Han et al. (2015) đề xuất cắt tỉa trọng số mạng có độ lớn nhỏ và sau đó lượng tử hóa những trọng số còn lại (Han et al., 2016). Louizos et al. (2018) học các mạng thưa thớt bằng cách xấp xỉ chính quy hóa ℓ0 với một tham số hóa lại stochastic. Tuy nhiên, chỉ trọng số thưa thớt thường chỉ dẫn đến tăng tốc trên phần cứng chuyên dụng với các thư viện hỗ trợ.

Trong các phương pháp có cấu trúc, cắt tỉa được áp dụng ở cấp độ neuron, kênh, hoặc thậm chí lớp. Ví dụ, cắt tỉa L1 (Li et al., 2017) loại bỏ kênh dựa trên norm của bộ lọc của chúng. He et al. (2018) sử dụng thưa thớt nhóm để làm mượt quá trình cắt tỉa sau huấn luyện. MorphNet (Gordon et al., 2018) chính quy hóa trọng số về zero cho đến khi chúng đủ nhỏ sao cho các kênh đầu ra tương ứng được đánh dấu để loại bỏ khỏi mạng. Intrinsic Structured Sparsity (ISS) (Wen et al., 2018) hoạt động trên LSTM (Hochreiter & Schmidhuber, 1997) bằng cách loại bỏ tập thể các cột và hàng của ma trận trọng số thông qua LASSO nhóm. Mặc dù các phương pháp cắt tỉa có cấu trúc và thuật toán của chúng tôi có cùng tinh thần tạo ra các mô hình hiệu quả, chúng tôi có được tiết kiệm chi phí huấn luyện bằng cách phát triển mạng từ các kiến trúc ban đầu nhỏ thay vì cắt tỉa những mạng có kích thước đầy đủ.

Tìm kiếm Kiến trúc Neural. Các phương pháp NAS đã cải thiện đáng kể hiệu suất đạt được bởi các mô hình mạng nhỏ. Các phương pháp NAS tiên phong sử dụng học tăng cường (Zoph et al., 2018; Zoph & Le, 2017) và thuật toán di truyền (Real et al., 2019; Xie & Yuille, 2017) để tìm kiếm các khối mạng có thể chuyển đổi có hiệu suất vượt qua nhiều khối được thiết kế thủ công. Tuy nhiên, các phương pháp như vậy đòi hỏi tính toán khổng lồ trong quá trình tìm kiếm — thường là hàng nghìn ngày GPU.

Để giảm chi phí tính toán, các nỗ lực gần đây sử dụng các kỹ thuật tìm kiếm hiệu quả hơn, chẳng hạn như tối ưu hóa dựa trên gradient trực tiếp (Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Tan et al., 2019; Cai et al., 2019; Wortsman et al., 2019). Tuy nhiên, hầu hết các phương pháp NAS thực hiện tìm kiếm trong không gian supernet đòi hỏi nhiều tính toán hơn so với việc huấn luyện các kiến trúc có kích thước thông thường.

Phát triển Mạng. Network Morphism (Wei et al., 2016) tìm kiếm các mạng sâu hiệu quả bằng cách mở rộng các lớp trong khi bảo toàn các tham số. Autogrow được đề xuất gần đây (Wen et al., 2020) có phương pháp AutoML để phát triển các lớp. Các phương pháp này hoặc yêu cầu một chính sách được thiết kế đặc biệt để dừng phát triển (ví dụ, sau một số lớp cố định) hoặc dựa vào việc đánh giá độ chính xác trong quá trình huấn luyện, phát sinh chi phí tính toán bổ sung đáng kể.

Học bằng Continuation. Các phương pháp continuation thường được sử dụng để xấp xỉ các bài toán tối ưu hóa khó giải bằng cách dần dần tăng độ khó của mục tiêu cơ bản, ví dụ bằng cách áp dụng các nới lỏng dần cho các bài toán nhị phân. Wu et al. (2019); Xie et al. (2019b; 2020) sử dụng gumbel-softmax (Jang et al., 2017) để lan truyền ngược lỗi trong quá trình tìm kiếm kiến trúc và thưa thớt hóa đặc trưng không gian. Savarese et al. (2020) đề xuất thưa thớt hóa liên tục để tăng tốc cắt tỉa và tìm kiếm vé (Frankle & Carbin, 2019). Mặc dù thành công của các phương pháp continuation trong việc tạo ra các mạng thưa thớt khi hoàn thành huấn luyện, chúng không hoạt động trên các mạng thưa thớt

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
trong quá trình huấn luyện và thay vào đó làm việc với một nới lỏng có giá trị thực. Việc hoãn loại bỏ thực tế các thành phần gần zero ngăn chặn việc áp dụng ngây thơ của các phương pháp này khỏi việc giảm chi phí huấn luyện.

3 PHƯƠNG PHÁP
3.1 KHÔNG GIAN CẤU HÌNH KIẾN TRÚC
Một topology mạng có thể được xem như một đồ thị acyclic có hướng bao gồm một chuỗi các nút được sắp xếp. Mỗi nút x(i)
in là một đặc trưng đầu vào và mỗi cạnh là một ô tính toán với các siêu tham số có cấu trúc (ví dụ, số bộ lọc và lớp trong mạng tích chập). Một không gian cấu hình kiến trúc có thể được tham số hóa bằng cách liên kết một biến mặt nạ m∈ {0,1} với mỗi ô tính toán (cạnh), cho phép động lực cắt tỉa (m= 1→0) và phát triển (m= 0→1) trong thời gian huấn luyện.

Như một ví dụ minh họa, chúng tôi xem xét một không gian cấu hình hai cấp cho kiến trúc CNN, được mô tả trong Hình 2(a), cho phép phát triển động mạng cả theo chiều rộng (theo kênh) và chiều sâu (theo lớp). Các không gian cấu hình thay thế là có thể; chúng tôi để lại trong Phụ lục chi tiết về cách chúng tôi tham số hóa thiết kế của kiến trúc LSTM.

Không gian Cấu hình Kênh CNN: Đối với một lớp tích chập với lin kênh đầu vào, lout kênh đầu ra (bộ lọc) và kernel có kích thước k×k, đặc trưng đầu ra thứ i được tính dựa trên bộ lọc thứ i, tức là cho i∈ {1, . . . , l out}:
x(i)
out=f(xin,F(i)·m(i)
c), (1)
trong đó m(i)
c∈ {0,1} là một tham số nhị phân loại bỏ kênh đầu ra thứ i khi được đặt về zero và f biểu thị phép toán tích chập. m(i)
c được chia sẻ qua một bộ lọc và broadcast đến cùng hình dạng với tensor bộ lọc F(i), cho phép phát triển/cắt tỉa toàn bộ bộ lọc. Như Hình 2(a) (trên) cho thấy, chúng tôi bắt đầu từ một cấu hình kênh mỏng. Sau đó chúng tôi truy vấn các biến chỉ thị và thực hiện chuyển đổi trạng thái: (1) Khi lật một biến chỉ thị từ 0 thành 1 lần đầu tiên, chúng tôi phát triển một bộ lọc được khởi tạo ngẫu nhiên và nối nó vào mạng. (2) Nếu một chỉ thị lật từ 1 thành 0, chúng tôi tạm thời tách bộ lọc tương ứng khỏi đồ thị tính toán; nó sẽ được phát triển lại về vị trí ban đầu nếu chỉ thị của nó lật trở lại 1, hoặc ngược lại sẽ bị cắt tỉa vĩnh viễn vào cuối huấn luyện. (3) Đối với các trường hợp khác, các bộ lọc tương ứng hoặc tồn tại và tiếp tục huấn luyện hoặc vẫn bị tách chờ truy vấn tiếp theo đến chỉ thị của chúng. Phương pháp của chúng tôi tự động hóa sự tiến hóa kiến trúc, với điều kiện chúng tôi có thể huấn luyện các chỉ thị.

Không gian Cấu hình Lớp CNN: Để phát triển độ sâu mạng, chúng tôi thiết kế một không gian cấu hình lớp trong đó một mạng nông ban đầu sẽ dần mở rộng thành một mô hình sâu được huấn luyện, như được hiển thị trong Hình 2(a) (dưới). Tương tự như không gian cấu hình kênh, nơi bộ lọc phục vụ như các đơn vị cấu trúc cơ bản, chúng tôi yêu cầu một công thức thống nhất để hỗ trợ việc phát triển các mạng phổ biến với kết nối shortcut (ví dụ, ResNet) và không có (ví dụ, plain net giống VGG). Chúng tôi đầu tiên giới thiệu một lớp abstract flayer như một đơn vị cấu trúc cơ bản, hoạt động trên các đặc trưng đầu vào xin và tạo ra các đặc trưng đầu ra xout. flayer có thể được khởi tạo như các lớp tích chập cho plain net hoặc các khối residual cho ResNet, tương ứng. Chúng tôi định nghĩa không gian cấu hình lớp như:
xout=g(xin;flayer·m(j)
l) =(
flayer(xin),nếu m(j)
l= 1
xin, nếu m(j)
l= 0, (2)
trong đó m(j)
l∈ {0,1} là chỉ thị nhị phân cho lớp thứ j flayer, với đó chúng tôi thực hiện chuyển đổi trạng thái tương tự như không gian cấu hình kênh. Các chỉ thị lớp có ưu tiên hơn các chỉ thị kênh: nếu m(j)
l được đặt là 0, tất cả bộ lọc chứa trong lớp tương ứng sẽ bị tách, bất kể trạng thái chỉ thị của chúng. Chúng tôi không tách các lớp thực hiện thay đổi độ phân giải (ví dụ, tích chập có stride).

3.2 PHÁT TRIỂN VỚI THƯA THỚT HÓA LIÊN TỤC CÓ CẤU TRÚC
Chúng tôi có thể tối ưu hóa sự đánh đổi giữa độ chính xác và thưa thớt có cấu trúc bằng cách xem xét mục tiêu:
min
w,mc,l,flayerLE(g(f(x;w⊙mc);flayer·ml)) +λ1∥mc∥0+λ2∥ml∥0, (3)

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
trong đó f là phép toán trong Eq. (1) hoặc Eq. (9) (trong Phụ lục A.6), trong khi g được định nghĩa trong Eq. (2). w⊙mc và flayer·ml là các biểu thức tổng quát của bộ lọc và lớp thưa thớt có cấu trúc và LE biểu thị một hàm mất mát (ví dụ, cross-entropy loss cho phân loại). Các số hạng ℓ0 khuyến khích thưa thớt, trong khi λ1,2 là các tham số đánh đổi giữa LE và các phạt ℓ0.

Thuật toán 1 : Tối ưu hóa
Đầu vào: Dữ liệu X=(xi)n
i=1, nhãn Y=(yi)n
i=1
Đầu ra: Mô hình hiệu quả đã phát triển G
Khởi tạo: G,w,u,λbase
1 và λbase
2.
Đặt ts là tất cả vector 0 liên kết với các hàm σ.
for epoch = 1 to T do
Đánh giá độ thưa thớt uG của G và tính
∆u=u−uG
Cập nhật λ1←λbase
1·∆u;λ2←λbase
2·∆u
trong Eq. (6) sử dụng Eq. (4)
for r= 1 to R do
Lấy mẫu mini-batch xi, yi từ X,Y
Huấn luyện G sử dụng Eq. (6) với SGD
end for
Lấy mẫu chỉ thị qc,l∼Bern(σ(βsc,l))
và ghi lại chỉ số idx nơi giá trị q là 1.
Cập nhật ts[idx] =ts[idx] + 1
Cập nhật β sử dụng Eq. (7)
end for
return G

Phát triển Nhận thức Ngân sách. Trong thực tế, việc sử dụng Eq. (3) có thể yêu cầu tìm kiếm lưới trên λ1 và λ2 cho đến khi một mạng với độ thưa thớt mong muốn được tạo ra. Để tránh một quy trình tốn kém như vậy, chúng tôi đề xuất một quá trình phát triển nhận thức ngân sách, được hướng dẫn bởi một ngân sách mục tiêu về tham số mô hình hoặc FLOPs. Thay vì coi λ1 và λ2 như các hằng số, chúng tôi định kỳ cập nhật chúng như:
λ1←λbase
1·∆u, λ 2←λbase
2·∆u , (4)
trong đó ∆u được tính như độ thưa thớt mục tiêu u trừ độ thưa thớt mạng hiện tại uG, và λbase
1, λbase
2 là các hằng số cơ sở ban đầu. Trong các giai đoạn phát triển đầu, vì mạng quá thưa thớt và ∆u âm, bộ tối ưu sẽ đẩy mạng về một trạng thái với nhiều dung lượng hơn (rộng hơn/sâu hơn). Hiệu ứng chính quy hóa dần yếu đi khi độ thưa thớt của mạng tiếp cận ngân sách (và ∆u tiếp cận zero). Điều này cho phép chúng tôi thích ứng phát triển mạng và tự động điều chỉnh mức độ thưa thớt của nó trong khi đồng thời huấn luyện trọng số mô hình. Phụ lục A.1 cung cấp phân tích chi tiết hơn. Các thí nghiệm của chúng tôi mặc định định nghĩa ngân sách bằng số lượng tham số, nhưng cũng điều tra các khái niệm ngân sách thay thế.

Học bằng Continuation. Một vấn đề khác trong việc tối ưu hóa Eq. (3) là ∥mc∥0 và ∥ml∥0 làm cho bài toán trở nên khó giải về mặt tính toán do bản chất tổ hợp của các trạng thái nhị phân. Để làm cho không gian cấu hình liên tục và tối ưu hóa khả thi, chúng tôi mượn khái niệm học bằng continuation (Cao et al., 2017; Wu et al., 2019; Savarese et al., 2020; Xie et al., 2020). Chúng tôi tham số hóa lại m như dấu nhị phân của một biến liên tục s: sign(s) là 1 nếu s >0 và 0 nếu s <0. Chúng tôi viết lại mục tiêu trong Eq. (3) như:
min
w,sc,l̸=0,flayerLE
g
f(x;w⊙sign(sc));flayer·sign(sl)
+λ1∥sign(sc)∥1+λ2∥sign(sl)∥1.(5)
Chúng tôi tấn công bài toán tối ưu hóa cứng và không liên tục trong Eq. (5) bằng cách bắt đầu với một mục tiêu dễ hơn trở nên khó hơn khi huấn luyện tiến triển. Chúng tôi sử dụng một chuỗi các hàm có giới hạn là phép toán dấu: đối với bất kỳ s̸= 0, limβ→∞σ(βs) = sign( s) nếu σ là hàm sigmoid hoặc limβ→0σ(βs) = sign( s) nếu σ là gumbel-softmax exp((−log(s0)+g1(s))/β) P
j∈{0,1} exp((−log(sj)+gj(s))/β) (Jang et al., 2017), trong đó β > 0 là một tham số nhiệt độ và g0,1 là gumbel. Bằng cách định kỳ thay đổi β, σ(βs) trở nên khó tối ưu hóa hơn, trong khi các mục tiêu hội tụ về mục tiêu rời rạc ban đầu.

Duy trì Thưa thớt hóa Bất cứ lúc nào. Mặc dù các phương pháp continuation có thể làm cho tối ưu hóa khả thi, chúng chỉ tiến hành thưa thớt hóa thông qua một tiêu chí ngưỡng trong giai đoạn suy luận. Trong trường hợp này, kiến trúc thời gian huấn luyện là dày đặc và không phù hợp trong bối cảnh phát triển một mạng. Để giảm hiệu quả chi phí tính toán của việc huấn luyện, chúng tôi duy trì một kiến trúc thưa thớt bằng cách giới thiệu một biến phụ trợ được lấy mẫu 0-1 q dựa trên giá trị xác suất σ(βs). Mục tiêu cuối cùng của chúng tôi trở thành:
min
w,sc,l̸=0,flayerLE
g
f(x;w⊙σ(βsc)⊙qc);flayer·σ(βsl)·ql
+λ1∥σ(βsc)∥1+λ2∥σ(βsl)∥1,(6)
trong đó qc và ql là các biến ngẫu nhiên được lấy mẫu từ Bern(σ(βsc)) và Bern(σ(βsl)), hiệu quả duy trì thưa thớt hóa bất cứ lúc nào và tránh ngưỡng không tối ưu, như được hiển thị trong Hình 2(b).

Bộ lập lịch Nhiệt độ Cải tiến. Trong các phương pháp continuation hiện có, giá trị β ban đầu thường được đặt là β0= 1 và một bộ lập lịch được sử dụng vào cuối mỗi epoch huấn luyện để cập nhật β trong tất cả các hàm kích hoạt σ, thường theo β=β0·γt, trong đó t là epoch hiện tại và γ là một siêu tham số (>1 khi σ là hàm sigmoid, <1 khi σ là gumbel softmax). Cả γ và t đều kiểm soát tốc độ

5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Bảng 1: So sánh với các phương pháp cắt tỉa kênh L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018), ThiNet (Luo et al., 2017), Provable (Liebenwein et al., 2020) và BAR (Lemaire et al., 2019) trên CIFAR-10.

| Mô hình | Phương pháp | Val Acc(%) | Params(M) | FLOPs(%) | Tiết kiệm Chi phí Huấn luyện(×) |
|---------|-------------|------------|-----------|----------|----------------------------------|
|         | Original    | 92.9 ±0.16 (-0.0) | 14.99 (100%) | 100 | 1.0 × |
|         | L1-Pruning  | 91.8 ±0.12 (-1.1) | 2.98 (19.9%) | 19.9 | 2.5 × |
| VGG     | SoftNet     | 92.1 ±0.09 (-0.8) | 5.40 (36.0%) | 36.1 | 1.6 × |
| -16     | ThiNet      | 90.8 ±0.11 (-2.1) | 5.40 (36.0%) | 36.1 | 1.6 × |
|         | Provable    | 92.4 ±0.12 (-0.5) | 0.85 (5.7%) | 15.0 | 3.5× |
|         | Ours        | 92.50±0.10 (-0.4) | 0.754 ±0.005 (5.0%) | 13.55 ±0.03 | 4.95 ±0.17× |
|         | Original    | 91.3 ±0.12 (-0.0) | 0.27 (100%) | 100 | 1.0 × |
|         | L1-Pruning  | 90.9 ±0.10 (-0.4) | 0.15 (55.6%) | 55.4 | 1.1 × |
| ResNet  | SoftNet     | 90.8 ±0.13 (-0.5) | 0.14 (53.6%) | 50.6 | 1.2× |
| -20     | ThiNet      | 89.2 ±0.18 (-2.1) | 0.18 (67.1%) | 67.3 | 1.1 × |
|         | Provable    | 90.8 ±0.08 (-0.5) | 0.10 (37.3%) | 54.5 | 1.7 × |
|         | Ours        | 90.91±0.07 (-0.4) | 0.096 ±0.002 (35.8%) | 50.20 ±0.01 | 2.40 ±0.09× |
| WRN     | Original    | 96.2 ±0.10 (-0.0) | 36.5 (100%) | 100 | 1.0 × |
| -28     | L1-Pruning  | 95.2 ±0.10 (-1.0) | 7.6 (20.8%) | 49.5 | 1.5 × |
| -10     | BAR(16x V)  | 92.0 ±0.08 (-4.2) | 2.3 (6.3%) | 1.5 | 2.6× |
|         | Ours        | 95.32±0.11 (-0.9) | 3.443±0.010 (9.3%) | 28.25±0.04 | 3.12±0.11× |

mà nhiệt độ tăng trong quá trình huấn luyện. Các phương pháp continuation với bộ lập lịch nhiệt độ toàn cục đã được áp dụng thành công trong cắt tỉa và NAS. Tuy nhiên, trong trường hợp của chúng tôi, một lịch trình toàn cục dẫn đến động lực không cân bằng giữa các biến có xác suất lấy mẫu thấp và cao: việc tăng nhiệt độ của những biến ít được lấy mẫu ở giai đoạn đầu có thể cản trở việc huấn luyện của chúng hoàn toàn, vì về cuối huấn luyện độ khó tối ưu hóa cao hơn. Để khắc phục vấn đề này, chúng tôi đề xuất một bộ lập lịch nhiệt độ riêng biệt theo cấu trúc bằng cách thực hiện một sửa đổi đơn giản: đối với mỗi biến mặt nạ, thay vì sử dụng số epoch hiện tại t để tính nhiệt độ của nó, chúng tôi đặt một bộ đếm riêng biệt ts chỉ được tăng khi biến chỉ thị liên quan của nó được lấy mẫu là 1 trong Eq. (6). Chúng tôi định nghĩa bộ lập lịch nhiệt độ theo cấu trúc của chúng tôi như
β=β0·γts, (7)
trong đó ts là các vector liên kết với các hàm σ. Các thí nghiệm sử dụng bộ lập lịch riêng biệt này theo mặc định, nhưng cũng so sánh hai lựa chọn thay thế. Thuật toán 1 tóm tắt quy trình tối ưu hóa của chúng tôi.

4 THÍ NGHIỆM
Chúng tôi đánh giá phương pháp của mình so với các phương pháp cắt tỉa kênh, phát triển mạng và tìm kiếm kiến trúc neural (NAS) hiện có trên: CIFAR-10 (Krizhevsky et al., 2014) và ImageNet (Deng et al., 2009) cho phân loại hình ảnh, PASCAL (Everingham et al., 2015) cho phân đoạn ngữ nghĩa và Penn Treebank (PTB) (Marcus et al., 1993) cho mô hình hóa ngôn ngữ. Xem chi tiết dataset trong Phụ lục A.2. Trong các bảng, kết quả tốt nhất được làm nổi bật bằng chữ đậm và tốt thứ hai được gạch chân.

4.1 SO SÁNH VỚI CÁC PHƯƠNG PHÁP CẮT TỈA KÊNH
Chi tiết Triển khai. Để so sánh công bằng, chúng tôi chỉ phát triển bộ lọc trong khi giữ các tham số có cấu trúc khác của mạng (số lượng lớp/khối) giống như các mô hình baseline chưa được cắt tỉa. Phương pháp của chúng tôi liên quan đến hai loại biến có thể huấn luyện: trọng số mô hình và trọng số mặt nạ. Đối với trọng số mô hình, chúng tôi áp dụng cùng các siêu tham số được sử dụng để huấn luyện các mô hình baseline chưa được cắt tỉa tương ứng, ngoại trừ việc đặt xác suất giữ dropout cho mô hình hóa ngôn ngữ là 0.65. Chúng tôi khởi tạo trọng số mặt nạ sao cho một bộ lọc duy nhất được kích hoạt trong mỗi lớp. Chúng tôi huấn luyện với SGD, tốc độ học ban đầu 0.1, weight decay 10−6 và momentum 0.9. Tham số đánh đổi λbase
1 được đặt là 0.5 trên tất cả nhiệm vụ; λ2 không được sử dụng vì chúng tôi không thực hiện phát triển lớp ở đây. Chúng tôi đặt σ là hàm sigmoid và γ là 1001
T trong đó T là tổng số epoch.

VGG-16, ResNet-20, và WideResNet-28-10 trên CIFAR-10. Bảng 1 tóm tắt các mô hình được tạo ra bởi phương pháp của chúng tôi và các phương pháp cắt tỉa kênh cạnh tranh. Lưu ý rằng chi phí huấn luyện được tính dựa trên tổng FLOPs trong các giai đoạn cắt tỉa và phát triển. Phương pháp của chúng tôi tạo ra các mạng thưa thớt hơn với ít suy giảm độ chính xác hơn, và liên tục tiết kiệm nhiều tính toán hơn trong quá trình huấn luyện — một hệ quả của việc phát triển từ một mạng đơn giản. Đối với WideResNet-28-10 được cắt tỉa tích cực, chúng tôi quan sát rằng BAR (Lemaire et al., 2019) có thể không có đủ dung lượng để đạt được sự sụt giảm độ chính xác không đáng kể, ngay cả với knowledge distillation (Hinton et al., 2015) trong quá trình huấn luyện. Lưu ý rằng chúng tôi

6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Bảng 2: So sánh với các phương pháp cắt tỉa kênh: L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018) và Provable (Liebenwein et al., 2020) trên ImageNet.

| Mô hình | Phương pháp | Top-1 Acc(%) | Params(M) | FLOPs(%) | Tiết kiệm Chi phí Huấn luyện(×) |
|---------|-------------|-------------|-----------|----------|----------------------------------|
|         | Original    | 76.1 (-0.0) | 23.0 (100%) | 100 | 1.0(×) |
| ResNet  | L1-Pruning  | 74.7 (-1.4) | 19.6 (85.2%) | 77.5 | 1.1(×) |
| -50     | SoftNet     | 74.6 (-1.5) | N/A | 58.2 | 1.2(×) |
|         | Provable    | 75.2 (-0.9) | 15.2 (65.9%) | 70.0 | 1.2(×) |
|         | Ours        | 75.2 (-0.9) | 14.1 (61.2%) | 50.3 | 1.9(×) |

Bảng 3: Kết quả so sánh với AutoGrow (Wen et al., 2020) trên CIFAR-10 và ImageNet.

| Dataset | Phương pháp | Biến thể | Mạng Tìm được | Val Acc(%) | Độ sâu | Kênh Thưa |
|---------|-------------|----------|---------------|-----------|--------|-----------|
|         | Ours        | Basic3ResNet | 23-29-31 | 94.50 | 83 | ✓ |
| CIFAR-10| Plain3Net   | 11-14-19 | 90.99 | 44 | ✓ |
|         | AutoGrow    | Basic3ResNet | 42-42-42 | 94.27 | 126 | ✗ |
|         | Plain3Net   | 23-22-22 | 90.82 | 67 | ✗ |
|         | Ours        | Bottleneck4ResNet | 5-6-5-7 | 77.41 | 23 | ✓ |
| ImageNet| Plain4Net   | 3-4-4-5 | 70.79 | 16 | ✓ |
|         | AutoGrow    | Bottleneck4ResNet | 6-7-3-9 | 77.33 | 25 | ✗ |
|         | Plain4Net   | 5-5-5-4 | 70.54 | 19 | ✗ |

báo cáo hiệu suất của phương pháp chúng tôi như trung bình ± độ lệch chuẩn, được tính trên 5 lần chạy với các seed ngẫu nhiên khác nhau. Phương sai nhỏ quan sát được cho thấy rằng phương pháp của chúng tôi hoạt động nhất quán qua các lần chạy.

ResNet-50 và MobileNetV1 trên ImageNet. Để xác thực hiệu quả trên các dataset quy mô lớn, chúng tôi phát triển, từ đầu, các bộ lọc của ResNet-50 được sử dụng rộng rãi trên ImageNet; chúng tôi không fine-tune. Bảng 2 cho thấy kết quả của chúng tôi tốt hất những kết quả được báo cáo trực tiếp trong các bài báo của các phương pháp cạnh tranh tương ứng. Phương pháp của chúng tôi đạt được 49.7% tiết kiệm suy luận và 47.4% tiết kiệm chi phí huấn luyện về FLOPs trong khi duy trì độ chính xác top-1 75.2%, mà không có bất kỳ giai đoạn fine-tuning nào. Phụ lục A.4 cho thấy những cải thiện của chúng tôi về nhiệm vụ đầy thử thách của việc phát triển kênh của MobileNetV1 đã compact. Ngoài ra, Hình 3 cho thấy sự đánh đổi độ chính xác top-1/FLOPs cho MobileNetV1 trên ImageNet, chứng minh rằng phương pháp của chúng tôi vượt trội so với các phương pháp cạnh tranh.

Deeplab-v3-ResNet-101 trên PASCAL VOC. Phụ lục A.5 cung cấp kết quả phân đoạn ngữ nghĩa.

2-Stacked-LSTM trên PTB: Chúng tôi chi tiết các phần mở rộng cho các tế bào hồi quy và so sánh phương pháp đề xuất của chúng tôi với ISS dựa trên vanilla two-layer stacked LSTM trong Phụ lục A.6. Như được hiển thị trong Bảng 8, phương pháp của chúng tôi tìm thấy cấu trúc mô hình compact hơn với chi phí huấn luyện thấp hơn, trong khi đạt được perplexity tương tự trên cả tập validation và test.

4.2 SO SÁNH VỚI AUTOGROW
Chi tiết Triển khai. Chúng tôi phát triển cả bộ lọc và lớp. Chúng tôi tuân theo cài đặt của AutoGrow trong việc khám phá các biến thể kiến trúc xác định mạng hạt giống ban đầu, không gian cấu hình theo lớp và các đơn vị cấu trúc cơ bản flayer của chúng tôi: Basic3ResNet, Bottleneck4ResNet, Plain3Net, Plain4Net. Khác với việc khởi tạo của AutoGrow sử dụng bộ lọc có kích thước đầy đủ trong mỗi lớp, không gian cấu hình theo kênh của chúng tôi bắt đầu từ bộ lọc đơn và mở rộng đồng thời với các lớp. Phụ lục A.7 chứa so sánh chi tiết của các kiến trúc hạt giống. Đối với việc huấn luyện trọng số mô hình, chúng tôi áp dụng các siêu tham số của các mô hình ResNet hoặc VGG tương ứng với các biến thể hạt giống ban đầu. Đối với các biến mặt nạ theo lớp và theo kênh, chúng tôi khởi tạo trọng số sao cho chỉ một bộ lọc đơn trong mỗi lớp và một đơn vị cơ bản trong mỗi giai đoạn (ví dụ, BasicBlock trong Basic3ResNet) là hoạt động. Chúng tôi sử dụng huấn luyện SGD với tốc độ học ban đầu 0.1, weight decay 10−6 và momentum 0.9 trên tất cả dataset. Bộ lập lịch tốc độ học giống như cho trọng số mô hình tương ứng. Các tham số đánh đổi λbase
1 và λbase
2 được đặt là 1.0 và 0.1 trên tất cả dataset. Để so sánh công bằng, chúng tôi fine-tune các mô hình cuối cùng với 40 epoch và 20 epoch trên CIFAR-10 và ImageNet, tương ứng.

Kết quả trên CIFAR-10 và ImageNet. Bảng 3 so sánh kết quả của chúng tôi với AutoGrow. Đối với tất cả các biến thể phát triển theo lớp trên cả hai dataset, phương pháp của chúng tôi nhất quán tạo ra cấu hình độ sâu và chiều rộng tốt hơn AutoGrow, về sự đánh đổi giữa độ chính xác và chi phí huấn luyện/suy luận. Về thời gian huấn luyện của Bottleneck4ResNet trên ImageNet, AutoGrow yêu cầu 61.6 giờ cho giai đoạn phát triển và 78.6 giờ cho fine-tuning trên 4 GPU TITAN V, trong khi phương pháp của chúng tôi mất 48.2 và 31.3 giờ, tương ứng. Phương pháp của chúng tôi mang lại 43% tiết kiệm thời gian huấn luyện hơn so với AutoGrow. Chúng tôi không chỉ yêu cầu ít epoch huấn luyện hơn, mà còn phát triển từ một bộ lọc đơn đến một mạng tương đối thưa thớt, trong khi AutoGrow luôn giữ các tập bộ lọc có kích thước đầy đủ mà không có bất kỳ tái phân bổ nào.

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
4.3 SO SÁNH VỚI CÁC PHƯƠNG PHÁP NAS
Như một so sánh công bằng với các phương pháp NAS liên quan đến các giai đoạn tìm kiếm và huấn luyện lại, chúng tôi cũng chia phương pháp của mình thành các giai đoạn phát triển và huấn luyện. Cụ thể, chúng tôi phát triển các lớp và kênh từ kiến trúc hạt giống Bottleneck4ResNet trực tiếp trên ImageNet bằng cách đặt λbase
1= 2.0, λbase
2= 0.1 và ngân sách tham số dưới 7M. Sau đó chúng tôi tiếp tục huấn luyện kiến trúc đã phát triển và so sánh với các phương pháp NAS hiện có về tham số, độ chính xác validation top-1 và giờ GPU V100 yêu cầu bởi các giai đoạn tìm kiếm hoặc phát triển, như được hiển thị trong Bảng 4. Lưu ý rằng DARTS (Liu et al., 2019) tiến hành tìm kiếm trên CIFAR-10, sau đó chuyển sang ImageNet thay vì tìm kiếm trực tiếp. Điều này là do DARTS hoạt động trên một supernet bằng cách bao gồm tất cả các đường dẫn ứng viên và gặp phải sự bùng nổ bộ nhớ GPU. Về FLOPs theo epoch, kết quả được hiển thị trong Hình 1(c) dành cho việc huấn luyện tương đương ResNet-20 trên CIFAR-10 so với DARTS và phương pháp cắt tỉa kênh Provable (Liebenwein et al., 2020). Cũng lưu ý rằng kiến trúc EfficientNet-B0, được bao gồm trong Bảng 4, được tạo ra bằng tìm kiếm lưới trong không gian tìm kiếm MnasNet, do đó có cùng chi phí tìm kiếm nặng. Để đạt được hiệu suất được báo cáo, EfficientNet-B0 sử dụng các mô-đun squeeze-and-excitation (SE) bổ sung (Hu et al., 2018), AutoAugment (Cubuk et al., 2019), cũng như các epoch huấn luyện lại dài hơn nhiều trên ImageNet.

Bảng 4: Hiệu suất so sánh với các phương pháp NAS AmoebaNet-A (Real et al., 2019), MnasNet (Tan et al., 2019), EfficientNet-B0 (Tan & Le, 2019), DARTS (Liu et al., 2019) và ProxylessNet (Cai et al., 2019) trên ImageNet.

| Phương pháp | Params | Top-1 | Chi phí Tìm kiếm/Phát triển |
|-------------|--------|-------|----------------------------|
| AmoebaNet-A | 5.1M | 74.5% | 76K giờ GPU |
| MnasNet | 4.4M | 74.0% | 40K giờ GPU |
| EfficientNet-B0 | 5.3M | 77.1% (+SE) | 40K giờ GPU |
| DARTS | 4.7M | 73.1% | N/A |
| ProxylessNet(GPU) | 7.1M | 75.1% | 200 giờ GPU |
| Ours | 6.8M | 74.3% | 80 giờ GPU |
| Ours | 6.7M | 74.8% | 110 giờ GPU |
| Ours | 6.9M | 75.1% | 140 giờ GPU |

ProxylessNet vẫn bắt đầu với một supernet được tham số hóa quá mức, nhưng áp dụng một phương pháp tìm kiếm giống cắt tỉa bằng cách nhị phân hóa các tham số kiến trúc và buộc chỉ một đường dẫn được kích hoạt tại thời điểm tìm kiếm. Điều này cho phép tìm kiếm trực tiếp trên ImageNet, đạt được tiết kiệm chi phí tìm kiếm 200× so với MnasNet. Tương phản với ProxylessNet, phương pháp của chúng tôi dần thêm bộ lọc và lớp vào các kiến trúc hạt giống đơn giản trong khi duy trì thưa thớt hóa, dẫn đến tiết kiệm không chỉ tính toán theo epoch mà còn tiêu thụ bộ nhớ, cho phép huấn luyện batch lớn hơn, nhanh hơn. Như vậy, chúng tôi tiết kiệm thêm 45% giờ GPU tìm kiếm, trong khi đạt được sự đánh đổi độ chính xác-tham số có thể so sánh.

4.4 PHÂN TÍCH
Tiết kiệm Chi phí Huấn luyện. Hình 4 minh họa động lực thưa thớt hóa của chúng tôi, hiển thị FLOPs theo epoch trong khi phát triển ResNet-20. Phụ lục A.8 trình bày các visualization bổ sung. Ngay cả với phần cứng GPU song song hoàn toàn, việc bắt đầu với ít bộ lọc và lớp trong mạng cuối cùng sẽ tiết kiệm thời gian wall-clock, vì huấn luyện batch lớn hơn (Goyal et al., 2017) luôn có thể được sử dụng để lấp đầy phần cứng.

Hình 5 cho thấy độ chính xác validation, độ phức tạp mô hình và số lượng lớp trong khi phát triển Basic3ResNet. Độ phức tạp được đo như tỷ lệ tham số mô hình của mô hình mục tiêu của AutoGrow. Vào cuối 160 epoch, độ chính xác validation của phương pháp chúng tôi là 92.36%, cao hơn 84.65% của AutoGrow tại 360 epoch. Do đó chúng tôi yêu cầu ít epoch fine-tuning hơn để đạt được độ chính xác cuối cùng 94.50% trên CIFAR.

Bảng 5: So sánh với baseline cắt tỉa ngẫu nhiên trên CIFAR-10.

| Mô hình | Phương pháp | Val Acc(%) | Params(M) |
|---------|-------------|------------|-----------|
| VGG-16 | Random | 90.01 ±0.69 | 0.770 ±0.050 |
|        | Ours | 92.50±0.10 | 0.754±0.005 |
| ResNet-20 | Random | 89.18 ±0.55 | 0.100 ±0.010 |
|           | Ours | 90.91±0.07 | 0.096±0.002 |
| WRN-28-10 | Random | 92.26 ±0.87 | 3.440 ±0.110 |
|           | Ours | 95.32±0.11 | 3.443±0.010 |

Phát triển Nhận thức Ngân sách. Trong Hình 6, đối với ResNet-20 trên CIFAR-10, chúng tôi so sánh các kiến trúc thu được bởi (1) cắt tỉa đồng đều: một phương pháp cắt tỉa được xác định trước ngây thơ cắt tỉa cùng tỷ lệ phần trăm kênh trong mỗi lớp, (2) của chúng tôi: các biến thể của phương pháp chúng tôi bằng cách đặt các độ thưa thớt tham số mô hình khác nhau như ngân sách mục tiêu trong quá trình phát triển, và (3) thiết kế trực tiếp: các kiến trúc được phát triển của chúng tôi được khởi tạo lại với trọng số ngẫu nhiên và huấn luyện lại. Trong hầu hết các cài đặt ngân sách, phương pháp phát triển của chúng tôi vượt trội so với thiết kế trực tiếp và cắt tỉa đồng đều, chứng minh

8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

[Các hình ảnh và biểu đồ được mô tả như trong nguyên văn]

hiệu quả tham số cao hơn. Phương pháp của chúng tôi cũng có vẻ có tác dụng tích cực về mặt chính quy hóa hoặc động lực tối ưu hóa, những điều này bị mất nếu người ta cố gắng huấn luyện trực tiếp cấu trúc compact cuối cùng. Phụ lục A.9 điều tra các mục tiêu ngân sách dựa trên FLOPs.

So sánh với Baseline Ngẫu nhiên. Ngoài baseline cắt tỉa đồng đều trong Hình 6, chúng tôi cũng so sánh với một baseline lấy mẫu ngẫu nhiên để tách biệt thêm đóng góp của không gian cấu hình và phương pháp phát triển của chúng tôi, theo tiêu chí trong (Xie et al., 2019a; Li & Talwalkar, 2019; Yu et al., 2020; Radosavovic et al., 2019). Cụ thể, baseline ngẫu nhiên này thay thế quy trình lấy mẫu các mục của q trong Eq. 6. Thay vì sử dụng xác suất lấy mẫu được rút ra từ các tham số mặt nạ đã học s, nó lấy mẫu với xác suất cố định. Như được hiển thị trong Bảng 5, phương pháp của chúng tôi nhất quán hoạt động tốt hơn nhiều so với baseline ngẫu nhiên này. Những kết quả này, cũng như các baseline tinh vi hơn trong Hình 6, chứng minh hiệu quả của phương pháp phát triển và cắt tỉa của chúng tôi.

Bộ lập lịch Nhiệt độ. Chúng tôi so sánh kiểm soát nhiệt độ theo cấu trúc của chúng tôi với một bộ lập lịch toàn cục trong các thí nghiệm phát triển kênh trên CIFAR-10 sử dụng VGG-16, ResNet-20, và WideResNet-28-10. Kết quả Bảng 1 sử dụng bộ lập lịch theo cấu trúc của chúng tôi. Để đạt được độ thưa thớt tương tự với bộ lập lịch toàn cục, các mô hình tương ứng bị sụt giảm độ chính xác 1.4%, 0.6%, và 2.5%. Với bộ lập lịch toàn cục, tối ưu hóa các biến mặt nạ dừng sớm trong huấn luyện và các epoch sau tương đương với việc huấn luyện trực tiếp một mạng compact cố định. Điều này có thể buộc mạng bị kẹt với một kiến trúc không tối ưu. Phụ lục A.10 điều tra các tương tác giữa tốc độ học và lịch trình nhiệt độ.

5 KẾT LUẬN
Chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả để phát triển các mạng sâu hiệu quả thông qua thưa thớt hóa liên tục có cấu trúc, giảm chi phí tính toán không chỉ của suy luận mà còn của huấn luyện. Phương pháp này đơn giản để triển khai và nhanh chóng thực thi; nó tự động hóa quá trình tái phân bổ cấu trúc mạng dưới ngân sách tài nguyên thực tế. Ứng dụng cho các mạng sâu được sử dụng rộng rãi trên nhiều nhiệm vụ khác nhau cho thấy rằng phương pháp của chúng tôi nhất quán tạo ra các mô hình với sự đánh đổi độ chính xác-hiệu quả tốt hơn so với các phương pháp cạnh tranh, trong khi đạt được tiết kiệm chi phí huấn luyện đáng kể.

Lời cảm ơn. Công trình này được hỗ trợ bởi Trung tâm CERES của University of Chicago cho Điện toán Không thể Dừng và Quỹ Khoa học Quốc gia dưới grant CNS-1956180.

9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
TÀI LIỆU THAM KHẢO
Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. In ICLR, 2019.
Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. Hashnet: Deep learning to hash by continuation. In ICCV, 2017.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587, 2017.
Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, 2014.
François Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017.
Ekin Dogus Cubuk, Barret Zoph, Dandelion Mané, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data. In CVPR, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV, 2015.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In ICLR, 2019.
Ross B. Girshick. Fast R-CNN. In ICCV, 2015.
Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. MorphNet: Fast & simple resource-constrained structure learning of deep networks. In CVPR, 2018.
Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv:1706.02677, 2017.
Sam Gross and Michael Wilber. Training and investigating residual nets. http://torch.ch/blog/2016/02/04/resnets.html, 2016.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In NeurIPS, 2016.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks. In NeurIPS, 2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In ICLR, 2016.
Bharath Hariharan, Pablo Arbelaez, Lubomir D. Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. IJCAI, 2018.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NeurIPS Deep Learning and Representation Learning Workshop, 2015.

10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, 2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.
Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. CondenseNet: An efficient DenseNet using learned group convolutions. In CVPR, 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In NeurIPS, 2016.
Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size. arXiv:1602.07360, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks. In NeurIPS, 2012.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. http://www.cs.toronto.edu/~kriz/cifar.html, 2014.
Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning of neural networks with budget-aware regularization. In CVPR, 2019.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient ConvNets. In ICLR, 2017.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In UAI, 2019.
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning for efficient neural networks. In ICLR, 2020.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv:1312.4400, 2013.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR, 2019.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. SSD: single shot multibox detector. In ECCV, 2016.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization. In ICLR, 2018.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A filter level pruning method for deep neural network compression. In ICCV, 2017.

11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. In NeurIPS, 2018.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 1993.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry P. Vetrov. Variational dropout sparsifies deep neural networks. In ICML, 2017.
Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In ICML, 2018.
Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: Imagenet classification using binary convolutional neural networks. In ECCV, 2016.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image classifier architecture search. In AAAI, 2019.
Pedro Savarese and Michael Maire. Learning implicitly recurrent CNNs through parameter sharing. In ICLR, 2019.
Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsification. In NeurIPS, 2020.
Laurent Sifre and PS Mallat. Rigid-motion scattering for image classification. PhD thesis, Ecole Polytechnique, CMAP, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. Network morphism. In ICML, 2016.
Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, and Hai Li. Learning intrinsic sparse structures within long short-term memory. In ICLR, 2018.
Wei Wen, Feng Yan, Yiran Chen, and Hai Li. Autogrow: Automatic layer growing in deep convolutional networks. In KDD, 2020.
Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari. Discovering neural wirings. In NeurIPS, 2019.
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR, 2019.
Lingxi Xie and Alan L Yuille. Genetic cnn. In ICCV, 2017.
Saining Xie, Alexander Kirillov, Ross B. Girshick, and Kaiming He. Exploring randomly wired neural networks for image recognition. In ICCV, 2019a.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. In ICLR, 2019b.

12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptive inference with stochastic feature sampling and interpolation. In ECCV, 2020.
Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. NetAdapt: Platform-aware neural network adaptation for mobile applications. In ECCV, 2018.
Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating the search phase of neural architecture search. In ICLR, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv:1409.2329, 2014.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In CVPR, 2018.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In ICLR, 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018.

13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
A PHỤ LỤC
A.1 PHÂN TÍCH CHI TIẾT HỬN VỀ PHÁT TRIỂN NHẬN THỨC NGÂN SÁCH
Việc tiến hành tìm kiếm lưới trên các tham số đánh đổi λ1 và λ2 cực kỳ tẻ nhạt và tốn thời gian. Ví dụ, để phát triển một mạng hiệu quả trên CIFAR-10, người ta cần lặp lại nhiều lần một lần chạy huấn luyện 160-epoch, và sau đó chọn mô hình tốt nhất từ tất cả các ứng viên đã phát triển. Để tránh quy trình lặp đi lặp lại tẻ nhạt này, thay vì sử dụng các hằng số λ1 và λ2, chúng tôi cập nhật động λ1 và λ2 trong tối ưu hóa phát triển nhận thức ngân sách một lần của chúng tôi.

Ở đây chúng tôi thảo luận về cách phát triển động nhận thức ngân sách hoạt động trong phương pháp của chúng tôi. Không mất tính tổng quát, chúng tôi rút ra quy tắc cập nhật SGD của mc cho số hạng chính quy hóa ℓ0 trong Eq. 3 như:
mc←mc−ηλbase
1∆uδℓ
δmc−ηµλbase
1∆umc (8)
trong đó η là tốc độ học và µ là hệ số weight decay. Ở đầu các epoch phát triển, khi kiến trúc bị thưa thớt quá mức, ∆u và λbase
1∆u là các giá trị âm. Khi đó cập nhật của mc là theo hướng ngược lại của gradient của số hạng chính quy hóa ℓ0, khuyến khích thưa thớt hóa của mc. Kết quả là, một số mc có giá trị zero sẽ được kích hoạt và độ phức tạp của mô hình được tăng mạnh để có đủ dung lượng cho việc huấn luyện thành công. Sau đó, phát triển dần yếu đi khi độ thưa thớt của mạng tiếp cận ngân sách (∆u về zero). Lưu ý rằng nếu kiến trúc bị tham số hóa quá mức, ∆u và λbase
1∆u trở thành dương và quy tắc cập nhật của SGD giống như của chính quy hóa ℓ0. Như vậy, phát triển nhận thức ngân sách của chúng tôi có thể tự động và động thích ứng độ phức tạp kiến trúc không chỉ dựa trên mất mát nhiệm vụ LE mà còn trên yêu cầu ngân sách thực tế trong quá trình huấn luyện một lần.

Chúng tôi cũng lưu ý rằng các phương pháp NAS thường sử dụng độ chính xác validation như một mục tiêu trong giai đoạn tối ưu hóa kiến trúc của họ, có thể yêu cầu một số kiến thức trước về độ chính xác validation trên một dataset đã cho. Quy trình phát triển của chúng tôi chọn ngân sách thưa thớt thay vì độ chính xác như mục tiêu vì: (1) Trong quá trình phát triển, độ chính xác validation bị ảnh hưởng không chỉ bởi kiến trúc mà còn bởi trọng số mô hình. Việc sử dụng trực tiếp ∆acc có thể dẫn đến tối ưu hóa kiến trúc không tối ưu. (2) Một mục tiêu ngân sách thưa thớt thực tế hơn và dễ đặt hơn theo thiết bị mục tiêu để triển khai.

A.2 CHI TIẾT CỦA CÁC DATASET ĐÁNH GIÁ
Đánh giá được tiến hành trên các nhiệm vụ khác nhau để chứng minh hiệu quả của phương pháp đề xuất của chúng tôi. Đối với phân loại hình ảnh, chúng tôi sử dụng CIFAR-10 (Krizhevsky et al., 2014) và ImageNet (Deng et al., 2009): CIFAR-10 bao gồm 60,000 hình ảnh của 10 lớp, với 6,000 hình ảnh mỗi lớp. Các tập train và test chứa 50,000 và 10,000 hình ảnh tương ứng. ImageNet là một dataset lớn cho nhận dạng thị giác chứa hơn 1.2M hình ảnh trong tập huấn luyện và 50K hình ảnh trong tập validation bao phủ 1,000 danh mục. Đối với phân đoạn ngữ nghĩa, chúng tôi sử dụng benchmark PASCAL VOC 2012 (Everingham et al., 2015) chứa 20 lớp đối tượng foreground và một lớp background. Dataset gốc chứa 1,464 (train), 1,449 (val), và 1,456 (test) hình ảnh được gắn nhãn ở mức pixel cho huấn luyện, validation, và testing, tương ứng. Dataset được tăng cường bởi các chú thích bổ sung được cung cấp bởi (Hariharan et al., 2011), kết quả là 10,582 hình ảnh huấn luyện. Đối với mô hình hóa ngôn ngữ, chúng tôi sử dụng dataset Penn Treebank (PTB) cấp từ (Marcus et al., 1993) bao gồm 929k từ huấn luyện, 73k từ validation, và 82k từ test, với 10,000 từ duy nhất trong từ vựng của nó.

A.3 CÁC MÔ HÌNH BASELINE CHƯA ĐƯỢC CẮT TỈA
Đối với CIFAR-10, chúng tôi sử dụng VGG-16 (Simonyan & Zisserman, 2015) với BatchNorm (Ioffe & Szegedy, 2015), ResNet-20 (He et al., 2016) và WideResNet-28-10 (Zagoruyko & Komodakis, 2016) làm baseline. Chúng tôi áp dụng một sơ đồ tăng cường dữ liệu chuẩn (shifting/mirroring) theo (Lin et al., 2013; Huang et al., 2016), và chuẩn hóa dữ liệu đầu vào với giá trị trung bình kênh và độ lệch chuẩn. Lưu ý rằng chúng tôi sử dụng phiên bản CIFAR của ResNet-201, VGG-162, và WideResNet-28-103. VGG-16, ResNet-20, và WideResNet-28-10 được huấn luyện trong 160, 160, và 200 epoch, tương ứng, với

1https://github.com/akamaster/pytorch resnet cifar10/blob/master/resnet.py
2https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py
3https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide resnet.py

14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
batch size 128 và tốc độ học ban đầu 0.1. Đối với VGG-16 và ResNet-20, chúng tôi chia tốc độ học cho 10 tại epoch 80 và 120, và đặt weight decay và momentum là 10−4 và 0.9. Đối với WideResNet-28-10, tốc độ học được chia cho 5 tại epoch 60, 120, và 160; weight decay và momentum được đặt là 5×10−4 và 0.9. Đối với ImageNet, chúng tôi huấn luyện các mô hình baseline ResNet-50 và MobileNetV1 theo các bài báo tương ứng. Chúng tôi áp dụng cùng sơ đồ tăng cường dữ liệu như trong (Gross & Wilber, 2016) và báo cáo độ chính xác validation top-1. Đối với phân đoạn ngữ nghĩa, hiệu suất được đo bằng intersection-over-union (IOU) pixel được tính trung bình trên 21 lớp (mIOU). Chúng tôi sử dụng Deeplab-v3-ResNet-1014(Chen et al., 2017) làm mô hình baseline theo chi tiết huấn luyện trong (Chen et al., 2017). Đối với mô hình hóa ngôn ngữ, chúng tôi sử dụng vanilla two-layer stacked LSTM (Zaremba et al., 2014) làm baseline. Tỷ lệ giữ dropout là 0.35 cho mô hình baseline. Kích thước từ vựng, kích thước embedding, và kích thước hidden của các stacked LSTM được đặt là 10,000, 1,500, và 1,500, tương ứng, nhất quán với các cài đặt trong (Zaremba et al., 2014).

A.4 PHÁT TRIỂN KÊNH MOBILENETV1 TRÊN IMAGENET
Để xác thực thêm hiệu quả của phương pháp đề xuất trên các mạng compact, chúng tôi phát triển các bộ lọc của MobileNetV1 trên ImageNet và so sánh hiệu suất của phương pháp chúng tôi với kết quả được báo cáo trực tiếp trong các bài báo tương ứng, như được hiển thị trong Bảng 6. Trong các thí nghiệm MobileNetV1, theo cùng cài đặt với Netadapt (Yang et al., 2018), chúng tôi áp dụng phương pháp của mình trên cả (1) cài đặt nhỏ: phát triển MobileNetV1(128) với multiplier 0.5 trong khi đặt multiplier của mô hình gốc là 0.25 để so sánh và (2) cài đặt lớn: phát triển MobileNetV1(224) chuẩn trong khi đặt multiplier của mô hình gốc là 0.75 để so sánh. Lưu ý rằng MobileNetV1 là một trong những mạng compact nhất, và do đó thử thách hơn để đơn giản hóa so với các mạng lớn khác. Phương pháp phát triển chi phí thấp hơn của chúng tôi vẫn có thể tạo ra một mô hình MobileNetV1 thưa thớt hơn so với các phương pháp cạnh tranh.

Bảng 6: Tổng quan về hiệu suất cắt tỉa của mỗi thuật toán trên MobileNetV1 ImageNet.

| Mô hình | Phương pháp | Top-1 Val Acc(%) | FLOPs(%) | Tiết kiệm Chi phí Huấn luyện(×) |
|---------|-------------|------------------|----------|----------------------------------|
| | Original(25%) | 45.1 (+0.0) | 100 | 1.0(×) |
| MobileNet | MorphNet | 46.0 (+0.9) | 110 | 0.9(×) |
| V1(128) | Netadapt | 46.3 (+1.2) | 81 | 1.1(×) |
| | Ours | 46.0 (+0.9) | 73 | 1.7(×) |
| MobileNet | Original(75%) | 68.8 (+0.0) | 100 | 1.0(×) |
| V1(224) | Netadapt | 69.1 (+0.3) | 87 | 1.2(×) |
| | Ours | 69.3 (+0.5) | 83 | 1.5(×) |

A.5 DEEPLAB-V3-RESNET-101 TRÊN PASCAL VOC 2012
Chúng tôi cũng kiểm tra hiệu quả của phương pháp đề xuất trên một nhiệm vụ phân đoạn ngữ nghĩa bằng cách phát triển số lượng bộ lọc của mô hình Deeplab-v3-ResNet-101 từ đầu trực tiếp trên dataset PASCAL VOC 2012. Chúng tôi áp dụng phương pháp của mình cho cả backbone ResNet-101 và mô-đun ASPP. So với baseline, mạng được tạo ra cuối cùng giảm FLOPs 58.5% và số lượng tham số 49.8%, trong khi xấp xỉ duy trì mIoU (76.5% xuống 76.4%). Xem Bảng 7.

Bảng 7: Kết quả trên dataset PASCAL VOC.

| Mô hình | Phương pháp | mIOU | Params(M) | FLOPs(%) | Tiết kiệm Chi phí Huấn luyện(×) |
|---------|-------------|------|-----------|----------|----------------------------------|
| Deeplab | Original | 76.5 (-0.0) | 58.0 (100%) | 100 | 1.0(×) |
| -v3- | L1-Pruning | 75.1 (-1.4) | 45.7 (78.8%) | 62.5 | 1.3(×) |
| ResNet101 | Ours | 76.4 (-0.1) | 29.1 (50.2%) | 41.5 | 2.3(×) |

4https://github.com/chenxi116/DeepLabv3.pytorch

15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
A.6 MỞ RỘNG CHO CÁC TẾ BÀO HỒI QUY TRÊN DATASET PTB
Chúng tôi tập trung vào LSTM (Hochreiter & Schmidhuber, 1997) với lh neuron ẩn, một biến thể phổ biến5 của RNN học các phụ thuộc dài hạn:
ft=σg((Wf⊙(emT
c))xt+ (Uf⊙(mcmT
c))ht−1+bf)
it=σg((Wi⊙(emT
c))xt+ (Ui⊙(mcmT
c))ht−1+bi)
ot=σg((Wo⊙(emT
c))xt+ (Uo⊙(mcmT
c))ht−1+bo)
˜ct=σh((Wc⊙(emT
c))xt+ (Uc⊙(mcmT
c))ht−1+bc)
ct=ft⊙ct−1+it⊙˜ct, h t=ot⊙σh(ct)s.t. m c∈ {0,1}lh,e= 1lh, (9)
trong đó σg là hàm sigmoid, ⊙ biểu thị phép nhân element-wise và σh là hàm hyperbolic tangent. xt biểu thị vector đầu vào tại time-step t, ht biểu thị trạng thái ẩn hiện tại, và ct biểu thị trạng thái tế bào bộ nhớ dài hạn. Wf, Wi, Wo, Wc biểu thị các ma trận trọng số input-to-hidden và Uf, Ui, Uo, Uc biểu thị các ma trận trọng số hidden-to-hidden. mc là chỉ thị nhị phân và được chia sẻ qua tất cả các cổng để kiểm soát độ thưa thớt của các neuron ẩn.

Chúng tôi so sánh phương pháp đề xuất của chúng tôi với ISS dựa trên vanilla two-layer stacked LSTM. Như được hiển thị trong Bảng 8, phương pháp của chúng tôi tìm thấy cấu trúc mô hình compact hơn với chi phí huấn luyện thấp hơn, trong khi đạt được perplexity tương tự trên cả tập validation và test. Những cải thiện này có thể do phương pháp của chúng tôi phát triển và cắt tỉa động các neuron ẩn từ trạng thái rất đơn giản hướng tới một sự đánh đổi tốt hơn giữa độ phức tạp mô hình và hiệu suất so với ISS, chỉ đơn giản sử dụng group lasso để phạt norm của tất cả các nhóm một cách tập thể cho sự compact.

Bảng 8: Kết quả trên dataset PTB.

| Phương pháp | Perplexity (val,test) | Cấu trúc Cuối cùng | Weight(M) | FLOPs(%) | Tiết kiệm Chi phí Huấn luyện(×) |
|-------------|----------------------|-------------------|-----------|----------|----------------------------------|
| Original | (82.57, 78.57) | (1500, 1500) | 66.0M (100%) | 100 | 1.0(×) |
| ISS | (82.59, 78.65 ) | (373, 315) | 21.8M (33.1%) | 13.4 | 3.8(×) |
| Ours | ( 82.46 , 78.68) | (310, 275) | 20.6M (31.2%) | 11.9 | 5.1 (×) |

A.7 CÁC BIẾN THỂ CỦA KIẾN TRÚC HẠT GIỐNG BAN ĐẦU
Trong Bảng 9, chúng tôi thực hiện so sánh chi tiết giữa các biến thể kiến trúc hạt giống ban đầu của chúng tôi và AutoGrow (Wen et al., 2020). Đối với cả chúng tôi và AutoGrow, "Basic" và "Bottleneck" đề cập đến ResNet với các khối residual basic và bottleneck chuẩn, trong khi "PlainLayers" đề cập đến các kết hợp lớp convolutional, batch normalization, và ReLU được xếp chồng. Tương tự như ResNet chuẩn, đối với các biến thể của kiến trúc hạt giống, chúng tôi áp dụng ba giai đoạn cho CIFAR-10 và bốn giai đoạn cho ImageNet. PlainNet có thể thu được bằng cách đơn giản loại bỏ shortcut từ các biến thể hạt giống ResNet này với số giai đoạn bằng nhau. Đối với mỗi giai đoạn, chúng tôi bắt đầu từ chỉ một đơn vị phát triển, trong đó số lượng bộ lọc ban đầu cũng được khởi tạo tại một cho phát triển kênh.

A.8 THEO DÕI THƯA THỚT HÓA BẤT CỨ LÚC NÀO TRONG QUÁ TRÌNH PHÁT TRIỂN KÊNH
Hình 7 và Hình 8 cho thấy động lực của tỷ lệ kênh phát triển thời gian huấn luyện của ResNet-20 và VGG-16 trên CIFAR-10, tương ứng. Để phân tích tốt hơn các mẫu phát triển, chúng tôi visualize động lực kênh được nhóm theo giai đoạn trong Hình 9 cho ResNet-20 và Hình 10 cho VGG-16, tương ứng. Lưu ý rằng, đối với VGG-16, chúng tôi chia nó thành 5 giai đoạn dựa trên vị trí lớp pooling và chuẩn hóa tỷ lệ kênh bằng 0.5 để visualization tốt hơn. Chúng tôi thấy rằng phương pháp của chúng tôi phát triển nhiều kênh hơn của các lớp sớm hơn trong mỗi giai đoạn của ResNet-20. Ngoài ra, độ thưa thớt kênh cuối cùng của ResNet-20 đồng đều hơn do các kết nối residual.

16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Bảng 9: So sánh chi tiết giữa các biến thể kiến trúc hạt giống của phương pháp chúng tôi và AutoGrow (Wen et al., 2020). Trong thuật ngữ đơn vị phát triển, "Basic" và "Bottleneck" đề cập đến ResNet với các khối residual basic và bottleneck chuẩn trong khi "PlainLayers" đề cập đến các kết hợp lớp convolutional chuẩn, BN, và ReLu trong các mạng giống VGG không có shortcut.

| Họ | Biến thể | Phương pháp | Phát triển Kênh | Đơn vị Phát triển | Giai đoạn | Shortcut |
|-----|----------|-------------|-----------------|-------------------|-----------|----------|
| | Basic3ResNet | Ours | ✓ | Basic | 3 | ✓ |
| ResNet | | AutoGrow | ✗ | ✓ | | |
| | Bottleneck4ResNet | Ours | ✓ | Bottleneck | 4 | ✓ |
| | | AutoGrow | ✗ | ✓ | | |
| | Plain3Net | Ours | ✓ | PlainLayers | 3 | ✗ |
| VGG-like | | AutoGrow | ✗ | ✗ | | |
| | Plain4Net | Ours | ✓ | PlainLayers | 4 | ✗ |
| | | AutoGrow | ✗ | ✗ | | |

[CÁC HÌNH ẢNH VÀ BIỂU ĐỒ TIẾP THEO...]

A.9 PHÁT TRIỂN NHẬN THỨC NGÂN SÁCH DỰA TRÊN FLOPS
Chúng tôi cũng điều tra hiệu quả của việc đặt mục tiêu FLOPs cho phát triển nhận thức ngân sách trong Hình 11. Chúng tôi quan sát xu hướng tương tự giữa cắt tỉa đồng đều, phát triển của chúng tôi, và thiết kế trực tiếp của chúng tôi: trong hầu hết các cài đặt ngân sách FLOPs, phương pháp phát triển của chúng tôi vượt trội so với thiết kế trực tiếp và cắt tỉa đồng đều. Chúng tôi cũng quan sát rằng khi đặt mục tiêu FLOPs thưa thớt cực kỳ (ví dụ, 85%), phương pháp của chúng tôi đạt được độ chính xác thấp hơn so với hai biến thể khác. Lý do là phát triển kênh của chúng tôi bị buộc chỉ phát triển kiến trúc từ ~99% thưa thớt lên ~85% FLOPs và ~90% thưa thớt tham số, trong đó các mô hình không thể có đủ dung lượng để được huấn luyện tốt.

A.10 TƯƠNG TÁC GIỮA TỐC ĐỘ HỌC VÀ BỘ LẬP LỊCH NHIỆT ĐỘ
Hai yếu tố ảnh hưởng đến tốc độ tối ưu hóa phát triển trong phương pháp của chúng tôi: nhiệt độ và tốc độ học, là các siêu tham số được kiểm soát bởi các bộ lập lịch tương ứng của chúng. Chúng tôi đầu tiên visualize động lực nhiệt độ riêng biệt theo cấu trúc trong Hình 12 bằng cách tính trung bình nhiệt độ mỗi lớp trong quá trình phát triển kênh ResNet-20 trên CIFAR-10. Chúng tôi thấy rằng nhiệt độ đang tăng với tốc độ khác nhau cho các kênh. Thường thì, tốc độ học thấp và nhiệt độ cao trong các epoch huấn luyện muộn làm cho tối ưu hóa phát triển mạng trở nên rất ổn định. Trong Hình 13, chúng tôi cố ý decay γ trong bộ lập lịch nhiệt độ, phản ánh lịch trình decay tốc độ học, để buộc phát triển cho đến cuối. Như được hiển thị trong Hình 14, phương pháp của chúng tôi vẫn thích ứng một số lớp ngay cả ở epoch cuối cùng. Chúng tôi thấy rằng sự bất ổn như vậy làm suy giảm hiệu suất, vì một số bộ lọc mới được phát triển có thể không có đủ thời gian để được huấn luyện tốt.

5Không gian cấu hình được đề xuất có thể dễ dàng áp dụng cho việc nén GRU (Cho et al., 2014) và RNN vanilla.

17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

[CÁC HÌNH ẢNH VÀ BIỂU ĐỒ TIẾP THEO ĐƯỢC MÔ TẢ NHƯ TRONG NGUYÊN VĂN]

18
