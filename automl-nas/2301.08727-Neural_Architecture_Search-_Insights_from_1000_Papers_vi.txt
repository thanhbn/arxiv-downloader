# Neural Architecture Search: Insights from 1000 Papers
Colin White colin@abacus.ai
Abacus.AI
San Francisco, CA 94105, USA
Mahmoud Safari safarim@cs.uni-freiburg.de
University of Freiburg
Freiburg im Breisgau, 79110, Germany
Rhea Sukthanker sukthank@cs.uni-freiburg.de
University of Freiburg
Freiburg im Breisgau, 79110, Germany
Binxin Ru robinru@sailyond.com
Sailyond Technology & Research Institute of Tsinghua University
Shenzhen, 518071, China
Thomas Elsken thomas.elsken@de.bosch.com
Bosch Center for Articial Intelligence
Renningen, 71272, Germany
Arber Zela zelaa@cs.uni-freiburg.de
University of Freiburg
Freiburg im Breisgau, 79110, Germany
Debadeepta Dey dedey@microsoft.com
Microsoft Research
Redmond, WA 98052, USA
Frank Hutter fh@cs.uni-freiburg.de
University of Freiburg & Bosch Center for Articial Intelligence
Freiburg im Breisgau, 79110, Germany

# Tìm Kiếm Kiến Trúc Mạng Nơ-ron: Hiểu Biết từ 1000 Bài Báo

## Tóm tắt
Trong thập kỷ qua, những tiến bộ trong học sâu đã dẫn đến những đột phá trong nhiều lĩnh vực, bao gồm thị giác máy tính, hiểu ngôn ngữ tự nhiên, nhận dạng giọng nói và học tăng cường. Các kiến trúc mạng nơ-ron chuyên biệt, hiệu suất cao là yếu tố quan trọng cho thành công của học sâu trong những lĩnh vực này. Tìm kiếm kiến trúc mạng nơ-ron (NAS), quá trình tự động hóa thiết kế kiến trúc mạng nơ-ron cho một tác vụ nhất định, là bước tiếp theo tất yếu trong việc tự động hóa học máy và đã vượt qua những kiến trúc được thiết kế bởi con người tốt nhất trong nhiều tác vụ. Trong vài năm qua, nghiên cứu về NAS đã phát triển nhanh chóng, với hơn 1000 bài báo được phát hành kể từ năm 2020 (Deng và Lindauer, 2021). Trong khảo sát này, chúng tôi cung cấp một hướng dẫn có tổ chức và toàn diện về tìm kiếm kiến trúc mạng nơ-ron. Chúng tôi đưa ra một phân loại không gian tìm kiếm, thuật toán và kỹ thuật tăng tốc, và thảo luận về các tài nguyên như điểm chuẩn, thực hành tốt nhất, các khảo sát khác và thư viện mã nguồn mở.

Từ khóa: tìm kiếm kiến trúc mạng nơ-ron, học máy tự động, học sâu

©2022 Colin White, Mahmoud Safari, Rhea Sukthanker, Binxin Ru, Thomas Elsken, Arber Zela, Debadeepta Dey và Frank Hutter.
Giấy phép: CC-BY 4.0, xem https://creativecommons.org/licenses/by/4.0/ .arXiv:2301.08727v2 [cs.LG] 25 Jan 2023

## 1. Giới thiệu
Trong thập kỷ qua, học sâu đã trở thành mô hình chủ đạo trong học máy cho nhiều ứng dụng và đã được sử dụng trong một số đột phá trong thị giác máy tính (He et al., 2016a; Huang et al., 2017; Krizhevsky et al., 2012; Szegedy et al., 2017), hiểu ngôn ngữ tự nhiên (Bahdanau et al., 2015; Hochreiter và Schmidhuber, 1997; Vaswani et al., 2017), nhận dạng giọng nói (Chan et al., 2016; Chorowski et al., 2015; Hannun et al., 2014), và học tăng cường (Mnih et al., 2015; Silver et al., 2016); nó cũng đang trở thành một phương pháp rất mạnh mẽ để phân tích dữ liệu dạng bảng (Hollmann et al., 2022; Kadra et al., 2021; Somepalli et al., 2021). Trong khi nhiều yếu tố đã góp phần vào sự phát triển của các phương pháp học sâu, bao gồm khả năng tự động hóa trích xuất đặc trưng của học sâu, cũng như sự gia tăng dữ liệu và khả năng tiếp cận lớn hơn đối với tài nguyên tính toán, việc thiết kế các kiến trúc mạng nơ-ron hiệu suất cao đã là yếu tố quan trọng cho thành công của học sâu.

Gần đây, giống như kỹ thuật đặc trưng thủ công đã được thay thế bằng học đặc trưng tự động thông qua học sâu, việc tự động hóa bước thiết kế kiến trúc tốn thời gian thông qua tìm kiếm kiến trúc mạng nơ-ron đang ngày càng trở nên phổ biến. Tìm kiếm kiến trúc mạng nơ-ron (NAS), quá trình tự động hóa thiết kế kiến trúc mạng nơ-ron cho một tác vụ nhất định, đã vượt qua những kiến trúc được thiết kế bởi con người tốt nhất trong nhiều tác vụ (Chen et al., 2018; Du et al., 2020; Ghiasi et al., 2019; So et al., 2019; Zoph et al., 2018), đặc biệt là ImageNet (Hu et al., 2019; Liu et al., 2018a; Real et al., 2019; Zoph et al., 2018), cũng như các bộ dữ liệu đa dạng và ít được nghiên cứu (Shen et al., 2022), và trong các thiết lập bị ràng buộc về bộ nhớ hoặc độ trễ (Benmeziane et al., 2021). Thật vậy, trong vài năm qua, nghiên cứu về NAS đã phát triển nhanh chóng. Mặc dù một số khảo sát đã được viết về NAS và các lĩnh vực liên quan trong quá khứ (Elsken et al., 2019b; Wistuba et al., 2019, cũng xem Mục 10.2), hơn 1000 bài báo NAS mới đã được phát hành trong hai năm qua (Deng và Lindauer, 2021), điều này đòi hỏi một khảo sát mới về những tiến bộ tổng thể, mà chúng tôi nhằm mục đích cung cấp với công trình này.

### 1.1 Lịch sử ngắn gọn về NAS và mối quan hệ với các lĩnh vực khác

NAS nổi lên như một lĩnh vực con của học máy tự động (AutoML) (Hutter et al., 2019), quá trình tự động hóa tất cả các bước trong đường ống học máy, từ làm sạch dữ liệu, đến kỹ thuật và lựa chọn đặc trưng, đến tìm kiếm siêu tham số và kiến trúc. NAS có sự chồng chéo lớn với tối ưu hóa siêu tham số (HPO) (Feurer và Hutter, 2019), điều này đề cập đến việc tối ưu hóa tự động các siêu tham số của mô hình học máy. NAS đôi khi được gọi là một tập con của HPO (Li và Talwalkar, 2019), vì NAS có thể được biểu diễn như việc tối ưu hóa chỉ các siêu tham số tương ứng với kiến trúc, một tập con của toàn bộ tập hợp siêu tham số mô hình. Tuy nhiên, các kỹ thuật cho HPO so với NAS thường khác nhau đáng kể.

Một bài toán HPO điển hình tối ưu hóa sự kết hợp của các siêu tham số liên tục và phân loại, như tốc độ học, tỷ lệ dropout, kích thước batch, momentum, hàm kích hoạt, chiến lược chuẩn hóa, v.v. Thông thường, các miền của hầu hết các siêu tham số là độc lập (tức là, tập hợp các giá trị có thể cho mỗi siêu tham số không bị ảnh hưởng bởi các giá trị có thể của các siêu tham số khác). Do đó, không gian tìm kiếm điển hình của một bài toán HPO là không gian tích của sự kết hợp các chiều liên tục và phân loại. Ngược lại, NAS tập trung cụ thể vào việc tối ưu hóa topology của kiến trúc, có thể phức tạp hơn nhiều. Topology thường được biểu diễn bằng một đồ thị có hướng không có chu trình (DAG), trong đó các nút hoặc cạnh được gắn nhãn bằng các phép toán mạng nơ-ron. Do đó, không gian tìm kiếm của một bài toán NAS thường là rời rạc¹ và có thể được biểu diễn trực tiếp như một đồ thị, hoặc như một cấu trúc phân cấp của các siêu tham số có điều kiện.

Mặc dù các thuật toán HPO tiêu chuẩn đôi khi có thể được điều chỉnh cho NAS (Izquierdo et al., 2021; Klein et al., 2020; Li et al., 2020c; Mendoza et al., 2016; Zela et al., 2018; Zimmer et al., 2021), thường hiệu quả và hiệu suất hơn nhiều khi sử dụng các kỹ thuật NAS được thiết kế riêng để tối ưu hóa không gian phức tạp của các kiến trúc mạng nơ-ron. Hơn nữa, hầu hết các kỹ thuật NAS hiện đại vượt ra ngoài các thuật toán tối ưu hóa hộp đen bằng cách khai thác các chi tiết cụ thể cho NAS, như chia sẻ trọng số giữa các kiến trúc mạng nơ-ron tương tự để tránh huấn luyện từng cái từ đầu.

[Hình 1: Số lượng bài báo NAS theo năm (Deng và Lindauer, 2021).]

Về mặt lịch sử, NAS đã tồn tại ít nhất từ cuối những năm 1980 (Angeline et al., 1994; Kitano, 1990; Miller et al., 1989; Tenorio và Lee, 1988) nhưng nó không thu hút sự chú ý rộng rãi cho đến bài báo nổi tiếng, NAS with Reinforcement Learning, của Zoph và Le (2017). Kể từ đó đã có sự quan tâm rất lớn đến NAS, với hơn 1000 bài báo được phát hành trong hai năm qua (xem Hình 1).

Đến nay, nhiều phương pháp khác nhau, như học tăng cường, thuật toán tiến hóa, tối ưu hóa Bayes, và các kỹ thuật dành riêng cho NAS dựa trên chia sẻ trọng số đã được khám phá. Có lẽ các phương pháp phổ biến gần đây nhất là các kỹ thuật one-shot (Bender et al., 2018; Liu et al., 2019c), thường tăng tốc đáng kể quá trình tìm kiếm so với các kỹ thuật tối ưu hóa hộp đen. Trong những năm gần đây, một khối lượng lớn công việc tiếp theo đã tập trung vào việc làm cho các phương pháp one-shot mạnh mẽ và đáng tin cậy hơn (Wang et al., 2021; Zela et al., 2020a). Song song, đã có một động lực lớn để làm cho nghiên cứu NAS có thể tái tạo và khoa học hơn, bắt đầu với việc phát hành NAS-Bench-101 (Ying et al., 2019), điểm chuẩn dạng bảng đầu tiên cho NAS. Hơn nữa, trong khi những ngày đầu của NAS chủ yếu tập trung vào các bài toán phân loại hình ảnh như CIFAR-10 và ImageNet, lĩnh vực này hiện đã mở rộng sang nhiều miền khác, như phát hiện đối tượng (Ghiasi et al., 2019; Xu et al., 2019a), phân đoạn ngữ nghĩa (Chen et al., 2018; Liu et al., 2019a), nhận dạng giọng nói (Mehrotra et al., 2021), giải phương trình vi phân từng phần (Roberts et al., 2021; Shen et al., 2022; Tu et al., 2022a), gấp protein (Roberts et al., 2021; Shen et al., 2022), và dự báo thời tiết (Tu et al., 2022b), và lĩnh vực này đã thấy một sự quan tâm mới trong xử lý ngôn ngữ tự nhiên (Chitty-Venkata et al., 2022; Javaheripi et al., 2022).

### 1.2 Bối cảnh và Định nghĩa

Các khảo sát NAS trước đây (ví dụ: Elsken et al., 2019b; Wistuba et al., 2019) đã đề cập đến ba chiều của NAS: không gian tìm kiếm, chiến lược tìm kiếm, và chiến lược đánh giá hiệu suất (xem

1. Đáng chú ý, một số kỹ thuật NAS như DARTS (Liu et al., 2019c) nới lỏng miền thành liên tục trong quá trình tìm kiếm, nhưng sau đó các siêu tham số được rời rạc hóa để trả về kiến trúc cuối cùng.

[Hình 2]. Chúng tôi định nghĩa từng thuật ngữ dưới đây, vì đây là một sự phân biệt hữu ích để hiểu nhiều phương pháp NAS. Tuy nhiên, điều đáng chú ý là phép chia ba không thể được áp dụng cho lĩnh vực con lớn của các phương pháp one-shot, bởi vì đối với những phương pháp này, chiến lược tìm kiếm được kết hợp với chiến lược đánh giá hiệu suất (Xie et al., 2021).

[Hình 2: Tổng quan về tìm kiếm kiến trúc mạng nơ-ron (Elsken et al., 2019b; Weng, 2020). Một chiến lược tìm kiếm lặp lại lựa chọn các kiến trúc (thường bằng cách sử dụng một phương pháp mã hóa kiến trúc) từ một không gian tìm kiếm A được định nghĩa trước. Các kiến trúc được chuyển đến một chiến lược ước lượng hiệu suất, trả về ước lượng hiệu suất cho chiến lược tìm kiếm. Đối với các phương pháp one-shot, chiến lược tìm kiếm và chiến lược ước lượng hiệu suất được kết hợp vốn dĩ.]

Một không gian tìm kiếm là tập hợp tất cả các kiến trúc mà thuật toán NAS được phép lựa chọn. Các không gian tìm kiếm NAS phổ biến có kích thước từ vài nghìn đến hơn 10²⁰. Trong khi không gian tìm kiếm về nguyên tắc có thể cực kỳ tổng quát, việc kết hợp kiến thức miền khi thiết kế không gian tìm kiếm có thể đơn giản hóa việc tìm kiếm. Tuy nhiên, việc thêm quá nhiều kiến thức miền sẽ tạo ra thiên vị của con người, làm giảm cơ hội một phương pháp NAS tìm ra các kiến trúc thực sự mới lạ. Các không gian tìm kiếm được thảo luận chi tiết hơn trong Mục 2.

Một chiến lược tìm kiếm là một kỹ thuật tối ưu hóa được sử dụng để tìm một kiến trúc hiệu suất cao trong không gian tìm kiếm. Thường có hai danh mục chính của các chiến lược tìm kiếm: các kỹ thuật dựa trên tối ưu hóa hộp đen (bao gồm các kỹ thuật đa độ tin cậy) và các kỹ thuật one-shot. Tuy nhiên, có một số phương pháp NAS mà cả hai hoặc không danh mục nào áp dụng. Các kỹ thuật dựa trên tối ưu hóa hộp đen, như học tăng cường, tối ưu hóa Bayes, và tìm kiếm tiến hóa, được khảo sát trong Mục 3. Các phương pháp one-shot, bao gồm các phương pháp dựa trên supernet và hypernet, được khảo sát trong Mục 4.

Một chiến lược ước lượng hiệu suất là bất kỳ phương pháp nào được sử dụng để nhanh chóng dự đoán hiệu suất của các kiến trúc mạng nơ-ron để tránh huấn luyện đầy đủ kiến trúc. Ví dụ, trong khi chúng ta có thể chạy một chiến lược tìm kiếm rời rạc bằng cách huấn luyện và đánh giá đầy đủ các kiến trúc được chọn trong suốt quá trình tìm kiếm, việc sử dụng một chiến lược ước lượng hiệu suất như ngoại suy đường cong học có thể tăng tốc độ tìm kiếm một cách đáng kể. Các chiến lược ước lượng hiệu suất, và tổng quát hơn là các kỹ thuật tăng tốc, được khảo sát trong Mục 5.

Định nghĩa cơ bản nhất của NAS như sau. Cho một không gian tìm kiếm A, một bộ dữ liệu D, một đường ống huấn luyện P, và một ngân sách thời gian hoặc tính toán t, mục tiêu là tìm một kiến trúc a ∈ A trong ngân sách t có độ chính xác validation cao nhất khi được huấn luyện bằng bộ dữ liệu D và đường ống huấn luyện P. Một phương pháp phổ biến để tiếp cận NAS là xấp xỉ giải biểu thức sau trong thời gian t:

min_{a∈A} L_val(w*(a); a) s.t. w*(a) = argmin_w L_train(w; a).

Ở đây, L_val và L_train biểu thị lỗi validation và lỗi huấn luyện, tương ứng. Trong khi đây là định nghĩa cốt lõi của NAS, các biến thể khác sẽ được thảo luận trong suốt khảo sát này. Ví dụ, chúng ta có thể muốn trả về một kiến trúc với các ràng buộc về số lượng tham số (Mục 6.2), hoặc chúng ta có thể sử dụng meta-learning (Mục 5.3) để cải thiện hiệu suất.

Trong phần còn lại của bài viết này, chúng tôi cung cấp một hướng dẫn toàn diện về các kỹ thuật và tài nguyên NAS mới nhất. Các mục 2 đến 5 được dành cho các kỹ thuật NAS, khảo sát các không gian tìm kiếm, kỹ thuật tối ưu hóa hộp đen, kỹ thuật one-shot, và kỹ thuật tăng tốc, tương ứng. Các mục 6 đến 10 bao gồm các mở rộng, ứng dụng và tài nguyên, và Mục 11 kết thúc bằng việc thảo luận các hướng tương lai đầy hứa hẹn.

## 2. Không gian Tìm kiếm

Không gian tìm kiếm có lẽ là thành phần quan trọng nhất của NAS. Trong khi các lĩnh vực khác của AutoML chồng chéo với NAS về mặt các phương pháp tối ưu hóa được sử dụng, không gian tìm kiếm kiến trúc là duy nhất đối với NAS. Hơn nữa, không gian tìm kiếm thường là bước đầu tiên khi thiết lập NAS. Phần lớn các không gian tìm kiếm phổ biến là dành riêng cho tác vụ và được lấy cảm hứng mạnh mẽ từ các kiến trúc thủ công tiên tiến trong các miền ứng dụng tương ứng của chúng. Ví dụ, NAS-Bench-101, một không gian tìm kiếm phân loại hình ảnh phổ biến (Ying et al., 2019) được lấy cảm hứng từ ResNet (He et al., 2016a) và Inception (Szegedy et al., 2017).

Thực tế, thiết kế không gian tìm kiếm đại diện cho một sự đánh đổi quan trọng giữa thiên vị của con người và hiệu quả tìm kiếm: nếu kích thước của không gian tìm kiếm nhỏ và bao gồm nhiều quyết định được chọn bằng tay, thì các thuật toán NAS sẽ dễ dàng hơn trong việc tìm một kiến trúc hiệu suất cao. Mặt khác, nếu không gian tìm kiếm lớn với nhiều khối xây dựng nguyên thủy hơn, một thuật toán NAS sẽ cần chạy lâu hơn, nhưng có khả năng khám phá ra các kiến trúc thực sự mới lạ (Real et al., 2020).

Trong mục này, chúng tôi khảo sát các danh mục chính của không gian tìm kiếm cho NAS như được tóm tắt trong Bảng 1. Chúng tôi bắt đầu trong Mục 2.1 bằng việc định nghĩa thuật ngữ chung. Trong Mục 2.2 và 2.3, chúng tôi thảo luận về các không gian tìm kiếm macro và cấu trúc chuỗi tương đối đơn giản, tương ứng. Trong Mục 2.4, chúng tôi mô tả loại không gian tìm kiếm phổ biến nhất: không gian tìm kiếm dựa trên cell. Trong Mục 2.5, chúng tôi mô tả các không gian tìm kiếm phân cấp. Cuối cùng, trong Mục 2.6, chúng tôi thảo luận về mã hóa kiến trúc, một quyết định thiết kế quan trọng cho các thuật toán NAS vốn dĩ gắn liền với việc lựa chọn không gian tìm kiếm.

### 2.1 Thuật ngữ

Các thuật ngữ không gian tìm kiếm khác nhau trong tài liệu, tùy thuộc vào loại không gian tìm kiếm. Để rõ ràng, chúng tôi định nghĩa các thuật ngữ chính ở đây và trong Hình 9 của Phụ lục.

• Operation/primitive biểu thị đơn vị nguyên tử của không gian tìm kiếm. Đối với gần như tất cả các không gian tìm kiếm phổ biến, đây là một bộ ba của một kích hoạt cố định, phép toán, và chuẩn hóa cố định, như ReLU-conv 1x1-batchnorm, trong đó ReLU và BatchNorm là cố định, và phép toán ở giữa là sự lựa chọn giữa một số phép toán khác nhau.

[Bảng 1: Tóm tắt các loại không gian tìm kiếm NAS.]

| Không gian Tìm kiếm | Cấu trúc | Siêu tham số có thể tìm kiếm | Mức độ Topology |
|---------------------|----------|------------------------------|-----------------|
| Không gian tìm kiếm Macro | DAG | Loại phép toán, topology DAG, siêu tham số macro | 1 |
| Không gian tìm kiếm cấu trúc chuỗi | Chuỗi | Loại phép toán, siêu tham số macro | 1 |
| Không gian tìm kiếm dựa trên cell | Cell trùng lặp | Loại phép toán, topology cell | 1 |
| Không gian tìm kiếm phân cấp | Khác nhau | Loại phép toán, topology cell/DAG, siêu tham số macro | >1 |

• Layer thường được sử dụng trong các không gian tìm kiếm cấu trúc chuỗi hoặc macro để biểu thị cùng một thứ như operation hoặc primitive. Tuy nhiên, đôi khi nó đề cập đến các kết hợp phép toán được biết đến, như inverted bottleneck residual (Cai et al., 2019; Sandler et al., 2018; Tan và Le, 2019; Tan et al., 2019).

• Block/Module đôi khi được sử dụng để biểu thị một chồng tuần tự của các layer theo ký hiệu được sử dụng trong hầu hết các không gian tìm kiếm cấu trúc chuỗi và macro (Cai et al., 2020; Tan và Le, 2019; Tan et al., 2019).

• Cell được sử dụng để biểu thị một đồ thị có hướng không có chu trình của các phép toán trong các không gian tìm kiếm dựa trên cell. Số lượng phép toán tối đa trong một cell thường được cố định.

• Motif được sử dụng để biểu thị một mẫu con được hình thành từ nhiều phép toán trong một kiến trúc. Một số tài liệu đề cập đến một cell như một motif cấp cao và một tập nhỏ các phép toán như một motif cấp cơ bản.

### 2.2 Không gian Tìm kiếm Macro

Trong tài liệu NAS, các không gian tìm kiếm macro có thể đề cập đến một trong hai loại. Thứ nhất, chúng có thể đề cập đến các không gian tìm kiếm mã hóa toàn bộ kiến trúc trong một mức (trái ngược với các không gian tìm kiếm dựa trên cell hoặc phân cấp), phổ biến vào năm 2017 và 2018. Thứ hai, chúng có thể đề cập đến các không gian tìm kiếm chỉ tập trung vào các siêu tham số cấp macro.

Đối với loại trước, toàn bộ kiến trúc được biểu diễn như một đồ thị có hướng không có chu trình duy nhất (Baker et al., 2017; Kandasamy et al., 2018; Real et al., 2017; Zoph và Le, 2017). Những không gian tìm kiếm này thường có sự lựa chọn phép toán tại mỗi nút trong đồ thị, cũng như sự lựa chọn topology DAG. Ví dụ, không gian tìm kiếm CNN NASBOT (Kandasamy et al., 2018) bao gồm các lựa chọn của các layer convolution, pooling và fully connected khác nhau, với bất kỳ topology DAG nào, với độ sâu tối đa 25.

Loại thứ hai của các không gian tìm kiếm macro (Dong et al., 2021b; Duan et al., 2021; Tan và Le, 2019), tập trung vào sự thay đổi của các siêu tham số cấp macro, như vị trí và mức độ giảm mẫu độ phân giải không gian trong suốt kiến trúc, trong khi giữ topology kiến trúc và các phép toán cố định.² Ví dụ, Tan và Le (2019) đề xuất một không gian tìm kiếm CNN bằng cách thay đổi độ sâu mạng, độ rộng và độ phân giải đặc trưng đầu vào.

So với các không gian tìm kiếm khác, các không gian tìm kiếm macro có sức mạnh biểu diễn cao: cấu trúc linh hoạt của chúng cho phép khả năng khám phá ra các kiến trúc mới lạ. Tuy nhiên, nhược điểm chính của chúng là chúng rất chậm để tìm kiếm. Trong hai mục tiếp theo, chúng tôi thảo luận về các loại không gian tìm kiếm có tính cứng nhắc hơn, làm cho chúng nhanh hơn để tìm kiếm.

### 2.3 Không gian Tìm kiếm Cấu trúc Chuỗi

Các không gian tìm kiếm cấu trúc chuỗi, như tên gọi, có topology kiến trúc đơn giản: một chuỗi tuần tự của các layer phép toán. Chúng thường lấy các thiết kế thủ công tiên tiến, như ResNet (He et al., 2016b) hoặc MobileNets (Howard et al., 2017), làm xương sống.

Có một số không gian tìm kiếm cấu trúc chuỗi dựa trên mạng tích chập. ProxylessNAS (Cai et al., 2019) bắt đầu với kiến trúc MobileNetV2 (Sandler et al., 2018) và tìm kiếm kích thước kernel và tỷ lệ mở rộng trong các layer inverted bottleneck residual. XD (Roberts et al., 2021) và DASH (Shen et al., 2022) bắt đầu với LeNet (LeCun et al., 1999), ResNet (He et al., 2016a), hoặc WideResNet (Zagoruyko và Komodakis, 2016), và tìm kiếm một tổng quát hóa biểu cảm của convolution dựa trên ma trận Kaleidoscope (Dao et al., 2020), hoặc kích thước kernel và dilations, tương ứng.

Các không gian tìm kiếm cấu trúc chuỗi cũng phổ biến trong các không gian tìm kiếm dựa trên transformer. Ví dụ, không gian tìm kiếm từ Lightweight Transformer Search (LTS) (Javaheripi et al., 2022) bao gồm một cấu hình cấu trúc chuỗi của họ kiến trúc GPT phổ biến (Brown et al., 2020; Radford et al., 2019) cho mô hình hóa ngôn ngữ tự hồi quy, với các lựa chọn có thể tìm kiếm cho số lượng layer, chiều mô hình, chiều embedding thích ứng, chiều của mạng nơ-ron feedforward trong một layer transformer, và số lượng head trong mỗi layer transformer. Các không gian tìm kiếm từ NAS-BERT (Xu et al., 2021a) và MAGIC (Xu et al., 2022) đều bao gồm một không gian tìm kiếm cấu trúc chuỗi trên kiến trúc BERT (Devlin et al., 2019) với tối đa 26 lựa chọn phép toán bao gồm các biến thể của multi-head attention, layer feedforward, và convolution với các kích thước kernel khác nhau.

Các không gian tìm kiếm cấu trúc chuỗi về mặt khái niệm đơn giản, làm cho chúng dễ thiết kế và triển khai. Chúng cũng thường chứa các kiến trúc mạnh có thể được tìm thấy tương đối nhanh chóng. Nhược điểm chính của chúng là, do topology kiến trúc đơn giản, có cơ hội thấp hơn so sánh để khám phá ra một kiến trúc thực sự mới lạ.

### 2.4 Không gian Tìm kiếm dựa trên Cell

Không gian tìm kiếm dựa trên cell có lẽ là loại không gian tìm kiếm phổ biến nhất trong NAS. Nó được lấy cảm hứng từ thực tế rằng các CNN được thiết kế bởi con người tiên tiến thường bao gồm các mẫu lặp lại, ví dụ, các block residual trong ResNet (Zoph et al., 2018). Do đó, thay vì tìm kiếm toàn bộ kiến trúc mạng từ đầu, Zoph et al. (2018) đề xuất chỉ tìm kiếm các cell tương đối nhỏ, và xếp chồng các cell nhiều lần theo trình tự để tạo thành kiến trúc tổng thể. Chính thức, các cell có thể tìm kiếm tạo nên cấu trúc vi mô của không gian tìm kiếm, trong khi khung xương bên ngoài (cấu trúc macro) được cố định.

[Hình 3: Minh họa các không gian tìm kiếm dựa trên cell. Khung xương bên ngoài qua các cell (trái) được cố định, trong khi các cell có thể tìm kiếm. NASNet gán phép toán cho các nút (giữa) trong khi DARTS gán phép toán cho các cạnh (phải).]

Không gian tìm kiếm dựa trên cell hiện đại đầu tiên, NASNet, được đề xuất bởi Zoph et al. (2018). Nó bao gồm hai loại cell: normal cell và reduction cell. Cả hai loại đều có cùng cấu trúc, nhưng các phép toán ban đầu trong reduction cell có stride hai để giảm một nửa độ phân giải không gian đầu vào. Mỗi NASNet cell có thể được biểu diễn như một DAG với mười bảy nút không phải đầu vào (xem Hình 3 (giữa)). Các nút được sắp xếp thành bộ ba gồm hai nút phép toán (như các phép toán convolution và pooling) và một nút kết hợp (như addition hoặc concatenation). Kiến trúc NASNet cuối cùng được hình thành bằng cách xếp chồng nhiều normal và reduction cell theo trình tự (xem Hình 3 (trái)). Tổng thể, có 10³⁵ kiến trúc duy nhất trong không gian tìm kiếm NASNet.

Kể từ không gian tìm kiếm NASNet, nhiều không gian tìm kiếm cell khác đã được đề xuất, tất cả đều có sự tương đồng cấp cao với NASNet, với những khác biệt chính là cấu trúc macro cố định, bố cục và ràng buộc trong các cell, và lựa chọn các phép toán trong các cell. Hai trong số các không gian tìm kiếm dựa trên cell phổ biến nhất là NAS-Bench-101 (Ying et al., 2019) và không gian tìm kiếm DARTS (Liu et al., 2019c). NAS-Bench-101 là điểm chuẩn dạng bảng đầu tiên cho NAS (thảo luận trong Mục 8), và các cell của nó bao gồm bảy nút, mỗi nút có ba lựa chọn phép toán; nó chứa 423,624 kiến trúc duy nhất. Không gian tìm kiếm DARTS khác biệt cơ bản hơn: trong khi nó cũng có hai cell có thể tìm kiếm, các DARTS cell có lựa chọn phép toán trên các cạnh của đồ thị thay vì trên các nút. Trong DARTS cell, các nút biểu diễn các biểu diễn tiềm ẩn và các cạnh là phép toán, trong khi trong NASNet cell, các biểu diễn tiềm ẩn nằm trên các cạnh và các nút là phép toán. Các DARTS cell (xem Hình 3 (phải)) chứa tám cạnh, mỗi cạnh có tám lựa chọn phép toán. Tổng thể, không gian DARTS chứa tổng cộng 10¹⁸ kiến trúc duy nhất.

Ngoài phân loại hình ảnh, các thiết kế cell tương tự cũng đã được áp dụng cho các mô hình ngôn ngữ. Ví dụ, NAS-Bench-ASR (Mehrotra et al., 2021) cung cấp một không gian tìm kiếm của các cell mô hình giọng nói tích chập cho nhận dạng giọng nói tự động, và có một số không gian tìm kiếm dựa trên LSTM (Klyuchnikov et al., 2022; Liu et al., 2019c; Pham et al., 2018).

Thiết kế dựa trên cell giảm đáng kể độ phức tạp của các không gian tìm kiếm, trong khi thường dẫn đến một kiến trúc cuối cùng hiệu suất cao. Điều này đã dẫn đến các không gian tìm kiếm dựa trên cell trở thành loại không gian tìm kiếm phổ biến nhất trong những năm gần đây. Hơn nữa, bằng cách tách độ sâu của một kiến trúc khỏi việc tìm kiếm, cấu trúc dựa trên cell có thể chuyển giao: các cell tối ưu được học trên một bộ dữ liệu nhỏ (ví dụ: CIFAR-10) thường chuyển giao tốt sang một bộ dữ liệu lớn (ví dụ: ImageNet) bằng cách tăng số lượng cell và filter trong kiến trúc tổng thể (Liu et al., 2019c; Zoph et al., 2018).

Mặc dù phổ biến, các không gian tìm kiếm dựa trên cell phải đối mặt với một số chỉ trích. Thứ nhất, trong khi không gian tìm kiếm DARTS chứa một số lượng 10¹⁸ kiến trúc có vẻ lớn, phương sai trong hiệu suất của các kiến trúc DARTS khá nhỏ (Wan et al., 2022b; Yang et al., 2020). Phương sai nhỏ này có thể góp phần vào thực tế rằng các chiến lược tìm kiếm tinh vi chỉ có thể đem lại lợi ích biên so với hiệu suất trung bình của các kiến trúc được lấy mẫu ngẫu nhiên (Yang et al., 2020). Hơn nữa, có nhiều lựa chọn thiết kế ad-hoc và siêu tham số cố định đi kèm với các không gian tìm kiếm dựa trên cell mà tác động của chúng không rõ ràng (Wan et al., 2022b), như việc tách normal và reduction cell, số lượng nút, và tập hợp các phép toán. Cuối cùng, mặc dù việc giới hạn tìm kiếm vào một cell giảm đáng kể độ phức tạp tìm kiếm, thực tế này làm giảm tính biểu cảm của không gian tìm kiếm NAS, làm cho việc tìm ra các kiến trúc có tính mới lạ cao với các không gian tìm kiếm cell trở nên khó khăn. Dưới ánh sáng này, một số công trình gần đây ủng hộ việc tìm kiếm các kết nối macro giữa các cell ngoài cấu trúc cell vi mô. Chúng tôi thảo luận điều này chi tiết hơn trong mục tiếp theo.

### 2.5 Không gian Tìm kiếm Phân cấp

Cho đến thời điểm này, tất cả các không gian tìm kiếm được mô tả đều có biểu diễn phẳng, trong đó một kiến trúc được xây dựng bằng cách định nghĩa các siêu tham số, topology và các primitive phép toán của nó trong một cấp độ thiết kế duy nhất. Cụ thể, chỉ một cấp độ topology được tìm kiếm, dù ở cấp độ cell hay kiến trúc. Mặt khác, các không gian tìm kiếm phân cấp liên quan đến việc thiết kế các motif ở các cấp độ khác nhau, trong đó mỗi motif cấp cao hơn thường được biểu diễn như một DAG của các motif cấp thấp hơn (Chrostoforidis et al., 2021; Liu et al., 2018b; Ru et al., 2020b).

Một lớp đơn giản của các không gian tìm kiếm phân cấp có hai cấp độ có thể tìm kiếm bằng cách thêm các siêu tham số kiến trúc cấp macro vào các không gian tìm kiếm cell hoặc cấu trúc chuỗi. Ví dụ, không gian tìm kiếm MnasNet (Tan et al., 2019) sử dụng MobileNetV2 làm xương sống. Liu et al. (2019b) thiết kế một không gian tìm kiếm hai cấp độ cho phân đoạn hình ảnh ngữ nghĩa, và công trình tiếp theo mở rộng nó cho khử nhiễu hình ảnh (Zhang et al., 2020a) và ghép nối stereo (Kumari và Kaur, 2016). Cuối cùng, Chen et al. (2021a) đề xuất một không gian tìm kiếm dựa trên transformer hai cấp độ cho các tác vụ thị giác được lấy cảm hứng từ ViT (Dosovitskiy et al., 2021) và DeiT (Touvron et al., 2021). Không gian tìm kiếm bao gồm một số block tuần tự có thể là sự kết hợp của các layer cục bộ (convolution) hoặc toàn cục (self-attention).

Ngoài hai cấp độ, Liu et al. (2018b) và Wu et al. (2021) đề xuất các phân cấp ba cấp độ. Liu et al. (2018b) đề xuất một phân cấp ba cấp độ, trong đó mỗi cấp độ là một đồ thị được tạo từ các thành phần từ cấp độ trước (xem Hình 4). Wu et al. (2021) đề xuất một phân cấp ba cấp độ khác, bao gồm các siêu tham số kernel, siêu tham số dựa trên cell, và siêu tham số macro. Thiết kế trước được mở rộng ngoài ba cấp độ trong hai công trình tiếp theo: Ru et al. (2020b) đề xuất một thiết kế phân cấp bốn cấp độ, được kiểm soát bởi một tập hợp các siêu tham số tương ứng với một bộ sinh đồ thị ngẫu nhiên, và Chrostoforidis et al. (2021) giới thiệu một quá trình xây dựng đệ quy để cho phép một số cấp độ phân cấp thay đổi cũng như một topology linh hoạt giữa các motif cấp cao nhất.

[Hình 4: Minh họa biểu diễn phân cấp được đề xuất trong Liu et al. (2018b). Cấp độ 1 của phân cấp bao gồm các lựa chọn của primitive phép toán. Cấp độ 2 bao gồm việc chọn topology qua các tập nhỏ primitive phép toán. Cấp độ 3 bao gồm việc chọn topology qua các cấu trúc từ cấp độ 2.]

Có nhiều lợi ích khi sử dụng các không gian tìm kiếm phân cấp. Thứ nhất, các không gian tìm kiếm phân cấp có xu hướng biểu cảm hơn. Hầu hết các không gian tìm kiếm cấu trúc chuỗi, dựa trên cell và macro có thể được xem như một không gian tìm kiếm phân cấp với một cấp độ có thể tìm kiếm duy nhất, nhưng việc có hai hoặc nhiều cấp độ cho phép chúng ta tìm kiếm các thiết kế kiến trúc đa dạng và phức tạp hơn. Hơn nữa, một biểu diễn phân cấp của một kiến trúc lớn là một cách hiệu quả để giảm độ phức tạp tìm kiếm, có thể dẫn đến hiệu quả tìm kiếm tốt hơn (Chrostoforidis et al., 2021; Liu et al., 2018b; Ru et al., 2020b). Mặt khác, các không gian tìm kiếm phân cấp có thể khó triển khai và tìm kiếm hơn.

### 2.6 Mã hóa Kiến trúc

Trong suốt mục này, chúng tôi đã thảo luận về nhiều loại không gian tìm kiếm NAS đa dạng. Như một cầu nối vào hai mục tiếp theo tập trung vào các chiến lược tìm kiếm, chúng tôi lưu ý rằng nhiều thuật toán NAS và quy trình phụ cần có một biểu diễn súc tích của mỗi kiến trúc, hay mã hóa, để thực hiện các phép toán như biến đổi một kiến trúc, định lượng sự tương tự giữa hai kiến trúc, hoặc dự đoán hiệu suất test của một kiến trúc. Điều này làm cho mã hóa kiến trúc trở nên quan trọng cho một số lĩnh vực của NAS, bao gồm các thuật toán NAS rời rạc (Mục 3) và dự đoán hiệu suất (Mục 5.1).

Trong hầu hết các không gian tìm kiếm, kiến trúc có thể được biểu diễn nhỏ gọn như một đồ thị có hướng không có chu trình (DAG), trong đó mỗi nút hoặc cạnh biểu diễn một phép toán. Ví dụ, các kiến trúc trong các không gian tìm kiếm dựa trên cell và cấu trúc chuỗi có thể được biểu diễn theo cách này. Tuy nhiên, các không gian tìm kiếm phân cấp không thể được biểu diễn đầy đủ bằng một DAG, và thường cần một mã hóa có cấu trúc có điều kiện, trong đó số lượng cấp độ của các siêu tham số có điều kiện tương ứng với số lượng cấp độ của phân cấp.

Đối với các không gian tìm kiếm dựa trên cell, một trong những mã hóa được sử dụng phổ biến nhất là ma trận kề cùng với một danh sách các phép toán, của (các) cell có thể tìm kiếm (Ying et al., 2019; Zoph và Le, 2017). Để có khả năng tổng quát tốt hơn, Ning et al. (2020) đề xuất một sơ đồ mã hóa dựa trên đồ thị và White et al. (2021a) đề xuất một sơ đồ mã hóa dựa trên đường dẫn, cả hai đều mô hình hóa luồng lan truyền thông tin trong mạng. Cuối cùng, một loại mã hóa khác cho tất cả các không gian tìm kiếm là mã hóa được học sử dụng pre-training không giám sát. Trong kỹ thuật này, trước khi chúng ta chạy NAS, chúng ta sử dụng một tập hợp các kiến trúc chưa được huấn luyện để học một mã hóa kiến trúc, ví dụ, bằng cách sử dụng một autoencoder (Li et al., 2020b; Lukasik et al., 2021, 2022; Yan et al., 2020; Zhang et al., 2019) hoặc một transformer (Yan et al., 2021a).

Khi chọn một mã hóa kiến trúc, khả năng mở rộng và tổng quát là những đặc tính quan trọng. Công trình gần đây đã chỉ ra rằng các quy trình phụ NAS khác nhau, như lấy mẫu một kiến trúc ngẫu nhiên, biến đổi một kiến trúc, hoặc huấn luyện một mô hình thay thế, mỗi cái có thể hoạt động tốt nhất với các mã hóa khác nhau (White et al., 2020). Hơn nữa, ngay cả những thay đổi nhỏ đối với sơ đồ mã hóa kiến trúc có thể có tác động đáng kể đến hiệu suất của NAS (White et al., 2020; Ying et al., 2019).

## 3. Kỹ thuật Tối ưu hóa Hộp đen

Bây giờ chúng tôi đã bao gồm các không gian tìm kiếm, chúng tôi chuyển sang có lẽ thành phần được nghiên cứu rộng rãi nhất của NAS: chiến lược tìm kiếm. Đây là thứ chúng ta chạy để tìm một kiến trúc tối ưu từ không gian tìm kiếm. Các chiến lược tìm kiếm thường rơi vào hai danh mục: kỹ thuật tối ưu hóa hộp đen và kỹ thuật one-shot. Tuy nhiên, một số phương pháp mà chúng tôi thảo luận bao gồm đặc tính của cả hai, hoặc không có danh mục nào trong số này. Chúng tôi thảo luận trước tiên về các kỹ thuật tối ưu hóa hộp đen trong mục này, tiếp theo là các kỹ thuật one-shot trong Mục 4.

Đối với tối ưu hóa hộp đen, chúng tôi thảo luận về các baseline (Mục 3.1), học tăng cường (Mục 3.2), tiến hóa (Mục 3.3), tối ưu hóa Bayes (Mục 3.4), và tìm kiếm cây Monte-Carlo (Mục 3.5). Các kỹ thuật tối ưu hóa hộp đen được sử dụng và nghiên cứu rộng rãi ngày nay, do hiệu suất mạnh mẽ và dễ sử dụng. Nói chung, các kỹ thuật tối ưu hóa hộp đen có xu hướng sử dụng nhiều tài nguyên tính toán hơn các kỹ thuật one-shot, do huấn luyện nhiều kiến trúc độc lập (không chia sẻ trọng số qua các kiến trúc như các kỹ thuật one-shot). Tuy nhiên, chúng cũng có nhiều ưu điểm so với các kỹ thuật one-shot, như tính mạnh mẽ (và thiếu các chế độ thất bại thảm khốc), tối ưu hóa đơn giản hơn các mục tiêu không khả vi, song song hóa đơn giản hơn, tối ưu hóa kết hợp với các siêu tham số khác, và dễ thích ứng hơn, ví dụ, với các bài toán, bộ dữ liệu hoặc không gian tìm kiếm mới. Chúng cũng thường đơn giản hơn về mặt khái niệm, làm cho chúng dễ triển khai và sử dụng hơn.

### 3.1 Baseline

Một trong những baseline đơn giản nhất có thể cho NAS là tìm kiếm ngẫu nhiên: các kiến trúc được chọn ngẫu nhiên từ không gian tìm kiếm và sau đó được huấn luyện đầy đủ. Cuối cùng, kiến trúc có độ chính xác validation tốt nhất được xuất ra. Mặc dù naïve, nhiều bài báo đã chỉ ra rằng tìm kiếm ngẫu nhiên hoạt động tốt một cách đáng ngạc nhiên (Chen et al., 2018; Li và Talwalkar, 2019; Sciuto et al., 2020; Yang et al., 2020). Điều này đặc biệt đúng đối với các không gian tìm kiếm được kỹ thuật hóa cao với tỷ lệ cao các kiến trúc mạnh, vì tìm kiếm ngẫu nhiên với ngân sách k đánh giá sẽ, kỳ vọng, tìm ra các kiến trúc trong top 100/k% của không gian tìm kiếm. Tuy nhiên, các công trình khác cho thấy rằng tìm kiếm ngẫu nhiên không hoạt động tốt trên các không gian tìm kiếm lớn, đa dạng (Bender et al., 2020; Real et al., 2020). Tuy vậy, tìm kiếm ngẫu nhiên được khuyến khích mạnh mẽ như một so sánh baseline cho các thuật toán NAS mới (Lindauer và Hutter, 2020; Yang et al., 2020), và có thể được làm cho có tính cạnh tranh cao bằng cách kết hợp chia sẻ trọng số (Li và Talwalkar, 2019), zero-cost proxy (Abdelfattah et al., 2021), hoặc ngoại suy đường cong học (Yan et al., 2021b). Nhiều bài báo (Sciuto et al., 2020; Yang et al., 2020) cũng đã đề xuất một baseline liên quan, đơn giản hơn: lấy mẫu ngẫu nhiên, hiệu suất trung bình của các kiến trúc trên toàn bộ không gian tìm kiếm.

Ngoài tìm kiếm ngẫu nhiên, các bài báo gần đây cho thấy rằng tìm kiếm cục bộ là một baseline mạnh cho NAS trên cả không gian tìm kiếm nhỏ (Ottelander et al., 2021; White et al., 2021b) và lớn (Siems et al., 2020). Điều này đúng ngay cả đối với dạng đơn giản nhất của tìm kiếm cục bộ: lặp lại huấn luyện và đánh giá tất cả các hàng xóm của kiến trúc tốt nhất tìm được cho đến nay, trong đó vùng lân cận thường được định nghĩa là tất cả các kiến trúc khác nhau bởi một phép toán hoặc cạnh. Tìm kiếm cục bộ có thể được tăng tốc đáng kể bằng cách sử dụng network morphism để khởi động ấm việc tối ưu hóa các kiến trúc lân cận (Elsken et al., 2017).

### 3.2 Học Tăng cường

Học tăng cường (RL) rất nổi bật trong những ngày đầu của NAS hiện đại. Đáng chú ý, công trình tiên phong của Zoph và Le (2017) đã sử dụng RL trên 800 GPU trong hai tuần để đạt được hiệu suất cạnh tranh trên CIFAR-10 và Penn Treebank; phát hiện này nhận được sự chú ý đáng kể từ truyền thông và bắt đầu sự hồi sinh hiện đại của NAS. Điều này được theo sau bởi một số phương pháp học tăng cường khác (Pham et al., 2018; Zoph et al., 2018).

[Thuật toán 1: Thuật toán NAS Học Tăng cường Tổng quát]
```
Đầu vào: Không gian tìm kiếm A, số lần lặp T.
Khởi tạo ngẫu nhiên trọng số của kiến trúc controller.
for t = 1, ..., T do
    Huấn luyện kiến trúc a ~ π(a; θ), được lấy mẫu ngẫu nhiên từ policy controller π(a; θ).
    Cập nhật tham số controller bằng cách thực hiện một cập nhật gradient ∇θ E_{a~π(a;θ)}[L_val(a)].
end for
Đầu ra: Kiến trúc được chọn từ policy được huấn luyện π(a; θ)
```

Hầu hết các phương pháp học tăng cường mô hình hóa các kiến trúc như một chuỗi các hành động được tạo ra bởi một controller (Baker et al., 2017; Zoph và Le, 2017). Độ chính xác validation của các kiến trúc được lấy mẫu sau khi huấn luyện được sử dụng như một tín hiệu reward để cập nhật controller nhằm tối đa hóa giá trị kỳ vọng của nó. Xem Thuật toán 1. Controller thường là một mạng nơ-ron hồi quy (RNN) (Zoph và Le, 2017; Zoph et al., 2018) xuất ra một chuỗi các thành phần tương ứng với một kiến trúc. Sau khi mỗi kiến trúc được xuất ra được huấn luyện và đánh giá, các tham số RNN được cập nhật để tối đa hóa độ chính xác validation kỳ vọng của các kiến trúc được xuất ra, sử dụng REINFORCE (Williams, 1992; Zoph và Le, 2017) hoặc proximal policy optimization (Schulman et al., 2017; Zoph et al., 2018).

ENAS (Pham et al., 2018) theo một chiến lược tương tự nhưng tăng tốc ước lượng reward bằng cách sử dụng chia sẻ trọng số; chúng tôi sẽ thảo luận điều này chi tiết trong Mục 4.

Gần đây hơn, RL đã không được sử dụng nổi bật cho NAS, vì nó đã được chỉ ra là bị vượt qua trong các so sánh trực tiếp bởi các phương pháp tiến hóa (Real et al., 2019) và tối ưu hóa Bayes (Ying et al., 2019), mà chúng tôi sẽ thảo luận tiếp theo.

### 3.3 Thuật toán Tiến hóa và Di truyền

Nhiều thập kỷ trước sự hồi sinh NAS gần đây, một trong những công trình đầu tiên trong NAS đã sử dụng một thuật toán tiến hóa (Miller et al., 1989). Trong các công trình đầu khác, việc sử dụng thuật toán tiến hóa để đồng thời tối ưu hóa kiến trúc mạng nơ-ron và trọng số của nó là phổ biến (Angeline et al., 1994; Floreano et al., 2008; Stanley và Miikkulainen, 2002; Stanley et al., 2009). Ngày nay, thuật toán tiến hóa vẫn phổ biến cho việc tối ưu hóa kiến trúc do tính linh hoạt, sự đơn giản về mặt khái niệm và kết quả cạnh tranh (Real et al., 2019), nhưng việc tối ưu hóa trọng số thường được để lại cho các phương pháp dựa trên SGD tiêu chuẩn.

[Thuật toán 2: Thuật toán NAS Tiến hóa Tổng quát]
```
Đầu vào: Không gian tìm kiếm A, số lần lặp T.
Lấy mẫu ngẫu nhiên và huấn luyện một quần thể kiến trúc từ không gian tìm kiếm A.
for t = 1, ..., T do
    Lấy mẫu (dựa trên độ chính xác) một tập hợp kiến trúc cha mẹ từ quần thể.
    Biến đổi các kiến trúc cha mẹ để tạo ra kiến trúc con, và huấn luyện chúng.
    Thêm con vào quần thể, và loại bỏ các kiến trúc lâu đời nhất (hoặc có độ chính xác thấp nhất) trong quần thể hiện tại.
end for
Đầu ra: Kiến trúc từ quần thể có độ chính xác validation cao nhất.
```

Các thuật toán NAS tiến hóa hoạt động bằng cách cập nhật lặp lại một quần thể kiến trúc. Trong mỗi bước, một hoặc nhiều kiến trúc "cha mẹ" trong quần thể được lấy mẫu (thường dựa trên độ chính xác validation của các kiến trúc), kết hợp và biến đổi để tạo ra các kiến trúc "con" mới. Những kiến trúc này sau đó được huấn luyện và thêm vào quần thể, thay thế các cá thể trong quần thể có hiệu suất tệ hơn. Xem Thuật toán 2.

Có nhiều cách khác mà các thuật toán tiến hóa khác nhau, bao gồm lấy mẫu quần thể ban đầu, chọn lựa cha mẹ, và tạo ra con. Để chọn quần thể ban đầu, các phương pháp bao gồm sử dụng các kiến trúc tầm thường (Real et al., 2017), lấy mẫu ngẫu nhiên kiến trúc từ không gian tìm kiếm (Real et al., 2019; Sun et al., 2019), hoặc sử dụng các kiến trúc hiệu suất cao được chọn bằng tay (Fujino et al., 2017).

Chọn lựa cha mẹ từ quần thể tạo nên một trong những thành phần cốt lõi của thuật toán tiến hóa. Có lẽ phương pháp phổ biến nhất để lấy mẫu cha mẹ là tournament selection (Almalaq và Zhang, 2018; Goldberg và Deb, 1991; Real et al., 2017, 2019; Sun et al., 2019, 2020), chọn (các) kiến trúc tốt nhất từ một quần thể được lấy mẫu ngẫu nhiên. Các phương pháp phổ biến khác bao gồm lấy mẫu ngẫu nhiên có trọng số theo fitness (Gibb et al., 2018; Loni et al., 2020; Song et al., 2020; Xie và Yuille, 2017), hoặc chọn (các) kiến trúc tốt nhất hiện tại làm cha mẹ (Elsken et al., 2017; Suganuma et al., 2017, 2018). Những phương pháp này đánh đổi giữa khám phá so với khai thác vùng tốt nhất tìm được cho đến nay. Một thuật toán tiến hóa đặc biệt thành công là regularized evolution của Real et al. (2019). Đây là một phương pháp tiến hóa khá tiêu chuẩn, với sự mới lạ là loại bỏ kiến trúc trong mỗi bước đã ở trong quần thể lâu nhất, ngay cả khi nó có hiệu suất cao nhất. Phương pháp này vượt qua tìm kiếm ngẫu nhiên và RL trong một so sánh trực tiếp và đạt được hiệu suất tiên tiến trên ImageNet tại thời điểm phát hành (Real et al., 2019).

### 3.4 Tối ưu hóa Bayes

Tối ưu hóa Bayes (BO, xem, ví dụ Frazier (2018) hoặc Garnett (2023)) là một phương pháp mạnh mẽ để tối ưu hóa các hàm đắt đỏ, và nó đã thấy thành công đáng kể trong NAS. Có hai thành phần chính của BO: (1) xây dựng một surrogate xác suất để mô hình hóa mục tiêu chưa biết dựa trên các quan sát trong quá khứ, và (2) định nghĩa một hàm acquisition để cân bằng việc khám phá và khai thác trong quá trình tìm kiếm. BO là một thuật toán lặp hoạt động bằng cách chọn kiến trúc tối đa hóa hàm acquisition (được tính bằng surrogate), huấn luyện kiến trúc này, và huấn luyện lại surrogate sử dụng kiến trúc mới này để bắt đầu lần lặp tiếp theo. Xem Thuật toán 3.

[Thuật toán 3: Thuật toán NAS Tối ưu hóa Bayes Tổng quát]
```
Đầu vào: Không gian tìm kiếm A, số lần lặp T, hàm acquisition α.
Lấy mẫu ngẫu nhiên và huấn luyện một quần thể kiến trúc từ không gian tìm kiếm A.
for t = 1, ..., T do
    Huấn luyện một mô hình surrogate dựa trên quần thể hiện tại.
    Chọn kiến trúc a_t bằng cách tối đa hóa α(a); dựa trên mô hình surrogate.
    Huấn luyện kiến trúc a_t và thêm nó vào quần thể hiện tại.
end for
Đầu ra: Kiến trúc từ quần thể có độ chính xác validation cao nhất.
```

Các kỹ thuật NAS dựa trên BO ban đầu phát triển các metric khoảng cách tùy chỉnh giữa các kiến trúc, ví dụ, với một kernel kiến trúc chuyên biệt (Swersky et al., 2014), một hàm khoảng cách lấy cảm hứng từ optimal transport (Kandasamy et al., 2018), hoặc một hàm khoảng cách tree-Wasserstein (Nguyen et al., 2021), cho phép một surrogate dựa trên Gaussian process (GP) điển hình với BO. Tuy nhiên, việc sử dụng một surrogate GP tiêu chuẩn thường không hoạt động tốt cho NAS, vì các không gian tìm kiếm thường có chiều cao, không liên tục, và giống đồ thị.

Để vượt qua điều này, một hướng công việc trước tiên mã hóa các kiến trúc, sử dụng các mã hóa được thảo luận trong Mục 2.6, và sau đó huấn luyện một mô hình, như tree-Parzen estimator (Bergstra et al., 2011; Falkner et al., 2018), random forest (Hutter et al., 2011; Ying et al., 2019), hoặc mạng nơ-ron (Springenberg et al., 2016; White et al., 2021a). Một hướng công việc khác chiếu thông tin kiến trúc vào một không gian latent liên tục chiều thấp trên đó BO thông thường có thể được áp dụng hiệu quả (Ru et al., 2020b; Wan et al., 2022a). Một lớp mô hình surrogate khác sử dụng graph neural network (Ma et al., 2019; Ru et al., 2021; Shi et al., 2020) hoặc một kernel dựa trên đồ thị (Ru et al., 2021) để xử lý tự nhiên biểu diễn đồ thị của các kiến trúc mà không cần một mã hóa rõ ràng.

Hàm acquisition, điều này đánh đổi giữa khám phá và khai thác trong quá trình tìm kiếm, là một thành phần thiết kế quan trọng khác cho BO. Có nhiều loại hàm acquisition được sử dụng trong NAS, như expected improvement (Jones et al., 1998; Močkus, 1975), upper confidence bound (Cox và John, 1992; Srinivas et al., 2010) và những cái dựa trên lý thuyết thông tin (Hennig và Schuler, 2012; Hernández-Lobato et al., 2014; Hvarfner et al., 2022; Wang và Jegelka, 2017). Trong NAS, việc tối ưu hóa hàm acquisition trong mỗi vòng của BO là thách thức do các không gian tìm kiếm không liên tục, và hơn nữa, việc đánh giá toàn diện giá trị hàm acquisition trên tất cả các kiến trúc có thể là không khả thi về mặt tính toán. Phương pháp phổ biến nhất để tối ưu hóa hàm acquisition trong NAS là bằng cách biến đổi ngẫu nhiên một nhóm nhỏ các kiến trúc tốt nhất được truy vấn cho đến nay, và trong số các kiến trúc được biến đổi, chọn (các) cái có giá trị hàm acquisition cao nhất (Kandasamy et al., 2018; Ma et al., 2019; Ru et al., 2021; Schneider et al., 2021; Shi et al., 2020; White et al., 2021a). Các phương pháp khác để tối ưu hóa hàm acquisition bao gồm tìm kiếm cục bộ, tìm kiếm tiến hóa, và tìm kiếm ngẫu nhiên (Ru et al., 2021; Shi et al., 2020; Ying et al., 2019).

### 3.5 Tìm kiếm Cây Monte Carlo

Một lớp phương pháp NAS khác dựa trên Tìm kiếm Cây Monte Carlo (MCTS). MCTS là thuật toán tìm kiếm nền tảng chính được sử dụng trong AlphaGO (Silver et al., 2016) và AlphaZero (Silver et al., 2017), đạt được hiệu suất siêu con người trong Go và cờ vua, tương ứng. MCTS tìm các quyết định tối ưu bằng cách lặp lại lấy mẫu các quyết định mới (ví dụ: thực hiện một nước đi trong cờ vua, hoặc chọn một phép toán cho một kiến trúc trong NAS), chạy các rollout ngẫu nhiên để có được reward (như thắng một ván cờ, hoặc khám phá một kiến trúc hiệu suất cao) và sau đó lan truyền ngược để cập nhật trọng số của quyết định ban đầu. Qua các lần lặp, thuật toán xây dựng một cây quyết định để thiên vị việc tìm kiếm về phía các vùng hứa hẹn hơn bằng cách cân bằng khám phá và khai thác trong việc ra quyết định (Browne et al., 2012).

MCTS được áp dụng đầu tiên cho NAS bởi Negrinho và Gordon (2017) những người biểu diễn không gian tìm kiếm và các siêu tham số của nó bằng một ngôn ngữ mô-đun. Điều này dẫn đến một không gian tìm kiếm có cấu trúc cây, có thể mở rộng, trái ngược với các không gian tìm kiếm cố định của công việc trước đó. Wistuba (2018) giới thiệu một phương pháp tương tự nhưng với hai thuật toán UCT (Upper Confidence bounds applied to Trees) khác nhau. MCTS được điều chỉnh đầu tiên cho các không gian tìm kiếm dựa trên cell bằng cách sử dụng một biểu diễn state-action (Wang et al., 2018). Các tác giả cũng cải thiện hiệu quả mẫu bằng cách sử dụng một mạng nơ-ron để ước lượng độ chính xác của các kiến trúc được lấy mẫu, do đó cho phép số lượng rollout cao hơn. Điều này được theo sau bằng việc thêm hiệu quả hơn nữa trong việc cắt tỉa cây bằng cách học các phân vùng (Wang et al., 2020b), và bằng ứng dụng cho NAS đa mục tiêu (Zhao et al., 2021a).

## 4. Kỹ thuật One-Shot

Trong suốt Mục 3, chúng ta đã thấy rằng phương pháp chủ đạo trong các giai đoạn đầu của nghiên cứu NAS là lặp lại lấy mẫu các kiến trúc từ không gian tìm kiếm, huấn luyện chúng, và sử dụng hiệu suất của chúng để hướng dẫn việc tìm kiếm. Nhược điểm chính của những phương pháp này, khi được áp dụng mà không có kỹ thuật tăng tốc, là chi phí tính toán khổng lồ của chúng, đôi khi theo thứ tự hàng nghìn ngày GPU (Real et al., 2019; Zoph và Le, 2017) do cần huấn luyện hàng nghìn kiến trúc độc lập và từ đầu³.

Như một lựa chọn thay thế, các kỹ thuật one-shot được giới thiệu để tránh huấn luyện mỗi kiến trúc từ đầu, do đó tránh được gánh nặng tính toán liên quan. Tính đến năm 2022, chúng hiện là một trong những kỹ thuật phổ biến nhất trong nghiên cứu NAS. Thay vì huấn luyện mỗi kiến trúc từ đầu, các phương pháp one-shot ngầm huấn luyện tất cả các kiến trúc trong không gian tìm kiếm thông qua một huấn luyện duy nhất ("one-shot") của một hypernetwork hoặc supernetwork.

Một hypernetwork là một mạng nơ-ron tạo ra trọng số của các mạng nơ-ron khác (Schmidhuber, 1992), trong khi một supernetwork (thường được sử dụng đồng nghĩa với "one-shot model" trong tài liệu) là một kiến trúc quá-tham số hóa chứa tất cả các kiến trúc có thể trong không gian tìm kiếm như các subnetwork (xem Hình 5). Ý tưởng của supernetwork được giới thiệu bởi Saxena và Verbeek (2016) và được phổ biến vào năm 2018 bởi các công trình như Bender et al. (2018), Pham et al. (2018), và Liu et al. (2019c).

[Hình 5: Một supernet bao gồm tất cả các kiến trúc có thể trong không gian tìm kiếm. Mỗi kiến trúc là một subnetwork (subgraph) trong supernet.]

Một khi một supernet được huấn luyện, mỗi kiến trúc từ không gian tìm kiếm có thể được đánh giá bằng cách thừa hưởng trọng số của nó từ subnet tương ứng trong supernet. Lý do cho khả năng mở rộng và hiệu quả của supernet là việc tăng tuyến tính số lượng ứng viên phép toán chỉ gây ra sự tăng tuyến tính chi phí tính toán cho huấn luyện, nhưng số lượng subnet trong supernet tăng theo cấp số nhân. Do đó, supernet cho phép chúng ta huấn luyện một số lượng kiến trúc theo cấp số nhân với chi phí tính toán tuyến tính.

Một giả định chính được đưa ra trong các phương pháp one-shot là khi sử dụng mô hình one-shot để đánh giá các kiến trúc, thứ hạng của các kiến trúc tương đối nhất quán với thứ hạng mà người ta sẽ có được từ việc huấn luyện chúng độc lập. Mức độ mà giả định này đúng đã được tranh luận đáng kể, với công việc cho thấy bằng chứng ủng hộ (Li et al., 2021c; Pham et al., 2018; Yu et al., 2020) và chống lại (Pourchot et al., 2020; Sciuto et al., 2020; Zela et al., 2020b; Zhang et al., 2020b) tuyên bố này trong các thiết lập khác nhau. Tính hợp lệ của giả định phụ thuộc vào thiết kế không gian tìm kiếm, các kỹ thuật được sử dụng để huấn luyện mô hình one-shot, và bản thân bộ dữ liệu, và khó dự đoán mức độ mà giả định sẽ đúng trong một trường hợp cụ thể (Sciuto et al., 2020; Zhang et al., 2020b).

Trong khi supernet cho phép đánh giá nhanh tất cả các kiến trúc, chúng ta vẫn phải quyết định về một chiến lược tìm kiếm, có thể đơn giản như chạy một thuật toán tối ưu hóa hộp đen trong khi supernet đang huấn luyện (như trong Pham et al. (2018)) hoặc sau khi supernet được huấn luyện (như trong Bender et al. (2018)). Chúng tôi thảo luận về những họ kỹ thuật này trong Mục 4.1. Một dòng công việc phổ biến sử dụng gradient descent để tối ưu hóa các siêu tham số kiến trúc song song với việc huấn luyện supernet (như DARTS (Liu et al., 2019c) và nhiều phương pháp tiếp theo). Chúng tôi thảo luận về họ kỹ thuật này trong Mục 4.2. Cuối cùng, trong Mục 4.3, chúng tôi thảo luận về hypernetwork. Hình 6 cung cấp một phân loại của các họ one-shot.

[Hình 6: Một phân loại của các họ one-shot chủ đạo. Một hypernetwork là một mạng nơ-ron tạo ra trọng số của các mạng nơ-ron khác. Một supernetwork là một mạng nơ-ron quá-tham số hóa chứa tập hợp các mạng nơ-ron từ không gian tìm kiếm như các subnetwork, và nó có thể được sử dụng với tối ưu hóa khả vi (bao gồm DARTS và các tiếp theo), hoặc tối ưu hóa không khả vi.]

### 4.1 Phương pháp dựa trên Supernet Không khả vi

Chúng tôi bắt đầu bằng việc mô tả các phương pháp dựa trên supernet không sử dụng tối ưu hóa khả vi. Một số phương pháp trong họ này tách biệt việc huấn luyện supernet và tìm kiếm kiến trúc: trước tiên huấn luyện một supernet, và sau đó chạy một thuật toán tối ưu hóa hộp đen để tìm kiếm kiến trúc tốt nhất. Các phương pháp khác huấn luyện một supernet trong khi đồng thời chạy một thuật toán tìm kiếm không khả vi, như học tăng cường, để chọn subnetwork.

Bender et al. (2018), Li và Talwalkar (2019), và Guo et al. (2020b) đề xuất các phương pháp đơn giản để huấn luyện supernet và sau đó sử dụng một thuật toán tối ưu hóa hộp đen để trích xuất kiến trúc tốt nhất từ nó. Bender et al. (2018) xây dựng supernet bằng cách tạo một nút riêng biệt tương ứng với một phép toán, trong mọi nơi có sự lựa chọn phép toán; sau đó họ huấn luyện supernet như thể nó là một mạng nơ-ron tiêu chuẩn, với một ngoại lệ: các nút được loại bỏ ngẫu nhiên trong quá trình huấn luyện, với mức độ dropout tăng tuyến tính trong suốt quá trình huấn luyện. Trong công việc tiếp theo, Li và Talwalkar (2019) và Guo et al. (2020b) đưa ý tưởng này xa hơn: trong mỗi bước huấn luyện, họ lấy mẫu ngẫu nhiên một kiến trúc và chỉ cập nhật trọng số của supernet tương ứng với kiến trúc đó. Những kỹ thuật này mô phỏng tốt hơn những gì đang xảy ra tại thời điểm đánh giá: chỉ một subnetwork được đánh giá thay vì toàn bộ supernet. Hơn nữa, những quy trình này sử dụng ít bộ nhớ hơn đáng kể so với việc huấn luyện tất cả trọng số của một supernet. Mỗi phương pháp kết thúc bằng việc sử dụng supernet được huấn luyện để nhanh chóng đánh giá các kiến trúc khi thực hiện tìm kiếm ngẫu nhiên (Bender et al., 2018; Li và Talwalkar, 2019) hoặc tìm kiếm tiến hóa (Guo et al., 2020b). Kiến trúc được xác định cuối cùng sau đó được huấn luyện từ đầu.

Như sẽ được thảo luận trong Mục 6.2, việc triển khai mạng nơ-ron trong thực tế thường đi kèm với các ràng buộc về độ trễ hoặc bộ nhớ. Trong khi các supernet được xem xét cho đến nay có xu hướng chỉ chứa các kiến trúc có kích thước gần như tương tự, Cai et al. (2020) đề xuất một supernet chứa các subnetwork có kích thước khác nhau. Phương pháp Once-for-all (OFA) này sử dụng một chiến lược co dần tiến bộ bắt đầu bằng việc lấy mẫu các subnetwork lớn nhất, và sau đó chuyển sang các subnetwork nhỏ hơn, để giảm thiểu sự đồng thích ứng giữa các subnetwork và hiệu quả huấn luyện các mạng có kích thước khác nhau "once for all". Trong một giai đoạn tìm kiếm tiếp theo, các kiến trúc được chọn dựa trên các ràng buộc khác nhau về độ trễ và bộ nhớ. Trong khi Cai et al. (2020) sử dụng tìm kiếm ngẫu nhiên cho giai đoạn tìm kiếm này, Guo et al. (2020b) đề xuất cải thiện phương pháp này hơn nữa bằng cách sử dụng tìm kiếm tiến hóa trong giai đoạn tìm kiếm.

Một trong những phương pháp dựa trên supernet sớm nhất là ENAS (Efficient Neural Architecture Search) (Pham et al., 2018), huấn luyện supernet trong khi chạy một thuật toán tìm kiếm song song. Cụ thể, chiến lược tìm kiếm tương tự như phương pháp dựa trên controller RL từ Zoph và Le (2017) (được mô tả trong Mục 3.2) nhưng ước lượng hiệu suất của mỗi kiến trúc bằng một supernet. Quy trình huấn luyện xen kẽ giữa việc chọn một kiến trúc, đánh giá nó, và cập nhật trọng số của supernet, và cập nhật trọng số của controller bằng cách lấy mẫu một số kiến trúc để ước lượng reward của REINFORCE.

Trong khi phương pháp này tìm kiếm một kiến trúc song song với việc huấn luyện supernet, nó sử dụng một mạng controller riêng biệt để hướng dẫn việc tìm kiếm. Trong mục tiếp theo, chúng tôi thảo luận về các phương pháp tiến hành tìm kiếm thông qua gradient descent chỉ sử dụng supernet.

### 4.2 Phương pháp dựa trên Supernet Khả vi

Trong mục này, chúng tôi xem xét các phương pháp NAS dựa trên supernet sử dụng các kỹ thuật tối ưu hóa khả vi. Chúng tôi mô tả trước tiên phương pháp DARTS (Differentiable Architecture Search) tinh túy của Liu et al. (2019c), và sau đó chúng tôi chuyển sang các công việc tiếp theo và các phương pháp khả vi khác.

[Thuật toán 4: DARTS - Tìm kiếm Kiến trúc Khả vi]
```
Đầu vào: Không gian tìm kiếm A, số lần lặp T, siêu tham số λ.
Khởi tạo ngẫu nhiên một mô hình one-shot dựa trên A với trọng số w và siêu tham số kiến trúc α.
for t = 1, ..., T do
    Thực hiện một cập nhật gradient trên trọng số kiến trúc theo Phương trình 1.
    Thực hiện một cập nhật gradient trên w theo ∇_w L_train(w; α).
end for
Đầu ra: Dẫn xuất kiến trúc cuối cùng bằng cách lấy argmax của α, qua tất cả lựa chọn phép toán, và sau đó huấn luyện lại kiến trúc này từ đầu.
```

Phương pháp DARTS sử dụng một relaxation liên tục của không gian tìm kiếm kiến trúc rời rạc, cho phép sử dụng gradient descent để tìm một local optimum hiệu suất cao nhanh hơn đáng kể so với các phương pháp tối ưu hóa hộp đen. Nó có thể được áp dụng cho bất kỳ không gian tìm kiếm dựa trên DAG nào có các lựa chọn phép toán khác nhau trên mỗi cạnh bằng cách sử dụng một phép toán "zero" để mô phỏng sự vắng mặt của một cạnh.

Tại đầu, mỗi cạnh (i, j) trong không gian tìm kiếm DARTS bao gồm nhiều phép toán ứng viên có thể o, mỗi cái được liên kết với một siêu tham số liên tục α_o^(i,j) ∈ [0,1]. Trong khi supernet đang huấn luyện, cạnh (i, j) bao gồm một hỗn hợp của tất cả phép toán ứng viên, được trọng số bởi mỗi α_o^(i,j). Các siêu tham số kiến trúc được tối ưu hóa cùng với trọng số mô hình supernet w thông qua gradient descent xen kẽ. Cụ thể, để cập nhật trọng số kiến trúc thông qua gradient descent, DARTS sử dụng xấp xỉ sau:

∇_α L_val(w*(α), α) ≈ ∇_α L_val(w - λ∇_w L_train(w, α), α), (1)

trong đó L_train biểu thị training loss, L_val biểu thị validation loss, λ là learning rate, và w*(α) biểu thị trọng số tối thiểu hóa training loss của kiến trúc tương ứng với α. Nói cách khác, để tránh tối ưu hóa bên trong đắt đỏ, w*(α) được xấp xỉ bằng một bước gradient descent đơn (w - λ∇_w L_train(w, α)). Điều này tương tự như MAML (Finn et al., 2017) và các công việc khác (Luketina et al., 2016; Metz et al., 2017). Mặc dù chiến lược này không được đảm bảo hội tụ, Liu et al. (2019c) cho thấy rằng nó hoạt động tốt trong thực tế với một lựa chọn λ phù hợp. Sau giai đoạn huấn luyện, DARTS có được một kiến trúc rời rạc bằng cách chọn phép toán có giá trị α tối đa trên mỗi cạnh (bước rời rạc hóa) và sau đó huấn luyện lại nó từ đầu. Hình 7 cung cấp một minh họa về DARTS.

[Hình 7: Các thuật toán NAS one-shot khả vi có bốn bước chính: khởi tạo ngẫu nhiên các siêu tham số kiến trúc, tối ưu hóa các siêu tham số kiến trúc và trọng số thông qua gradient descent xen kẽ, rời rạc hóa các siêu tham số kiến trúc được tối ưu hóa, và huấn luyện lại subnetwork kết quả từ đầu.]

DARTS thu hút sự chú ý đáng kể trong cộng đồng AutoML do tính đơn giản, tính mới lạ và việc phát hành mã dễ sử dụng. Hơn nữa, kỹ thuật gốc để lại chỗ cho cải tiến qua nhiều trục khác nhau. Do đó, đã có một khối lượng lớn công việc tiếp theo tìm cách cải thiện các phần khác nhau của phương pháp DARTS. Trong phần còn lại của mục, chúng tôi bao gồm các danh mục cải tiến chính (xem Hình 6).

#### 4.2.1 Rank Disorder

Như đã đề cập ở đầu Mục 4, gần như tất cả các phương pháp one-shot đều đưa ra một giả định chính: thứ hạng của các kiến trúc được đánh giá với supernet tương đối nhất quán với thứ hạng mà người ta sẽ có được từ việc huấn luyện chúng độc lập; khi giả định này không được đáp ứng, nó được biết đến như rank disorder (Li et al., 2021c; Sciuto et al., 2020). Trong khi có tranh luận đáng kể cả ủng hộ (Li et al., 2021c; Pham et al., 2018; Yu et al., 2020) và chống lại (Pourchot et al., 2020; Sciuto et al., 2020; Zela et al., 2020b; Zhang et al., 2020b) giả định này, nhiều công việc đã cố gắng giảm vấn đề rank disorder.

Một số phương pháp đề xuất tăng dần độ sâu mạng, hoặc tỉa dần tập hợp ứng viên phép toán trong quá trình huấn luyện, cho thấy rằng điều này khiến trọng số thích ứng tốt hơn với các lựa chọn phép toán hứa hẹn nhất. Progressive-DARTS (Chen et al., 2019a) tăng dần độ sâu mạng trong khi đồng thời tỉa các phép toán có trọng số nhỏ nhất. SGAS (Li et al., 2020a) chọn các phép toán trong suốt quy trình huấn luyện, dựa trên hai tiêu chí: certainty lựa chọn (được tính thông qua entropy của phân phối phép toán) và stability lựa chọn (được tính thông qua sự chuyển động của phân phối phép toán). Cuối cùng, XNAS (Nayman et al., 2019) sử dụng thuật toán exponentiated gradient (Kivinen và Warmuth, 1997), động tỉa các lựa chọn phép toán kém trong quá trình tìm kiếm trong khi cũng cho phép phục hồi của "late bloomers", tức là, các lựa chọn phép toán chỉ trở nên chính xác sau này trong quy trình huấn luyện.

#### 4.2.2 Operation Biases

Một số công việc cho thấy rằng các kỹ thuật NAS khả vi có xu hướng ưa chuộng skip connection hơn các lựa chọn phép toán khác (Liang et al., 2019; Wang et al., 2021; Zela et al., 2020a), điều này có thể được gây ra bởi supernet sử dụng skip connection để bù đắp quá mức cho vanishing gradient (Chu et al., 2021). Nhiều phương pháp đã được đề xuất để sửa thiên vị này.

DARTS+ (Liang et al., 2019) đề xuất một phương pháp early stopping dựa trên sự ổn định của thứ hạng của trọng số kiến trúc, trong khi DARTS− (Chu et al., 2021) tách trọng số skip connection khỏi trọng số phép toán khác thông qua các cạnh phụ trợ. FairDARTS (Chu et al., 2020) đặt tất cả trọng số phép toán độc lập với tất cả những cái khác, và sau đó đẩy những trọng số kiến trúc này về phía không hoặc một trong hàm loss.

Lấy một phương pháp khác, Wang et al. (2021) cho thấy rằng việc skip connection có trọng số cao hơn là ổn, miễn là chúng ta không chọn kiến trúc cuối cùng dựa trên những trọng số này. Thay vào đó, sau khi huấn luyện supernet, thuật toán của họ, DARTS-PT, chọn mỗi phép toán mà việc loại bỏ nó có sự giảm lớn nhất của độ chính xác trong supernet.

Thay vì sửa chữa thiên vị giữa một tập nhỏ phép toán được chọn bằng tay, Shen et al. (2022) thay vào đó sử dụng một không gian tìm kiếm giảm đáng kể thiên vị của con người: họ cố định một mạng tích chập tiêu chuẩn và tìm kiếm kích thước kernel và dilation của các phép toán của nó. Phương pháp đơn giản này có thể áp dụng rộng rãi qua thị giác máy tính, giải PDE, gấp protein, và các tác vụ khác. Để làm cho huấn luyện one-shot hiệu quả hơn, thuật toán của họ, DASH, tính hỗn hợp của các phép toán bằng cách sử dụng chéo hóa Fourier của convolution.

#### 4.2.3 Tổng quát Test Kém

Một số công việc tìm cách cải thiện hiệu suất tổng quát của DARTS thông qua nhiều phương tiện khác nhau. Zela et al. (2020a) và Chen và Hsieh (2020) cho thấy rằng DARTS thường hội tụ về local minima sắc trong loss landscape (độ cong validation loss cao trong không gian siêu tham số kiến trúc), điều này, sau khi chạy bước rời rạc hóa, có thể khiến thuật toán trả về một kiến trúc với khả năng tổng quát test kém. Robust-DARTS (Zela et al., 2020a) sửa vấn đề này bằng cách làm cho việc huấn luyện mạnh mẽ hơn thông qua data augmentation, điều chỉnh L2 của mục tiêu bên trong L_train, và early stopping. Tương tự, thay vì tối ưu hóa training loss, Smooth-DARTS (Chen và Hsieh, 2020) tối ưu hóa expected hoặc worst-case training loss trên một vùng lân cận địa phương của các siêu tham số kiến trúc.

Lấy một phương pháp khác, GAEA (Li et al., 2021c), XD (Roberts et al., 2021), và StacNAS (Guilin et al., 2019) đều sử dụng tối ưu hóa đơn cấp thay vì tối ưu hóa hai cấp thông thường, bằng cách coi các siêu tham số kiến trúc như trọng số kiến trúc bình thường, cho thấy điều này dẫn đến tổng quát tốt hơn. Hơn nữa, GAEA tái tham số hóa các tham số kiến trúc trên simplex và cập nhật chúng bằng thuật toán exponentiated gradient (tương tự như XNAS từ Mục 4.2.1), cho thấy điều này phù hợp hơn với hình học cơ bản của không gian tìm kiếm kiến trúc.

Cuối cùng, Amended-DARTS (Bi et al., 2019) và iDARTS (Zhang et al., 2021a) đều lấy phương pháp dẫn xuất các xấp xỉ chính xác hơn của gradient của α (Phương trình 1), cho thấy rằng điều này dẫn đến một tối ưu hóa ổn định hơn và tổng quát tốt hơn.

#### 4.2.4 Tiêu thụ Bộ nhớ Cao

Bộ nhớ cần thiết để huấn luyện một supernet cao hơn nhiều so với một mạng nơ-ron bình thường—nó tỷ lệ tuyến tính với kích thước của tập hợp phép toán ứng viên. Nhớ lại từ Mục 4.1 rằng nhiều công việc đã giảm bộ nhớ này bằng cách, trong mỗi bước huấn luyện, che tất cả phép toán trừ những cái tương ứng với một hoặc một vài subnetwork. Nhiều công việc đã đề xuất kỹ thuật để che phép toán cho NAS khả vi cũng vậy, tức là, trong khi đồng thời tối ưu hóa các siêu tham số kiến trúc.

Cai et al. (2019) đề xuất ProxylessNAS, giải quyết vấn đề này bằng cách sửa đổi phương pháp rời rạc hóa BinaryConnect (Courbariaux et al., 2015): trong mỗi bước huấn luyện, cho mỗi lựa chọn phép toán, tất cả đều bị che trừ một phép toán được chọn ngẫu nhiên với xác suất tỷ lệ với giá trị α hiện tại của nó. Cai et al. (2019) cho thấy rằng quy trình này hội tụ về một subnetwork hiệu suất cao duy nhất. GDAS (Dong và Yang, 2019) và DSNAS (Hu et al., 2020; Xie et al., 2018) sử dụng phân phối Gumbel-softmax trên một one-hot encoding của các lựa chọn phép toán, đây là một cách khác để cho phép lấy mẫu phép toán đơn trong mỗi bước huấn luyện trong khi duy trì khả vi.

PC-DARTS (Xu et al., 2019b) đề xuất một phương pháp tương đối đơn giản hơn: tại mỗi bước huấn luyện, và cho mỗi cạnh trong DAG, một tập con các channel được lấy mẫu và gửi qua các phép toán có thể, trong khi các channel còn lại được truyền trực tiếp đến đầu ra. Trong khi giảm bộ nhớ do huấn luyện ít channel hơn, điều này cũng hoạt động như một bộ điều chỉnh.

DrNAS (Chen et al., 2021f) cũng giảm tiêu thụ bộ nhớ bằng cách tăng dần số lượng channel được chuyển tiếp đến các phép toán hỗn hợp, và tỉa dần các lựa chọn phép toán, được mô hình hóa bởi một phân phối Dirichlet.

### 4.3 Hypernetwork

Một hypernetwork là một mạng nơ-ron tạo ra trọng số của các mạng nơ-ron khác. Hypernetwork được xem xét đầu tiên bởi Schmidhuber (1992, 1993), và ứng dụng hiện đại đầu tiên là của Ha et al. (2017), những người sử dụng chúng để có được trọng số tốt hơn cho một kiến trúc LSTM cố định. Hypernetwork kể từ đó đã được sử dụng cho nhiều tác vụ, bao gồm HPO (Mackay et al., 2019; Navon et al., 2021), hiệu chuẩn độ không chắc chắn của mô hình (Krueger et al., 2017), và NAS (Brock et al., 2018; Zhang et al., 2018).

Công việc đầu tiên sử dụng hypernetwork cho NAS (và trong số những công việc đầu tiên sử dụng một mô hình one-shot cho NAS) là SMASH (one-Shot Model Architecture Search through Hypernetworks) (Brock et al., 2018). SMASH bao gồm hai giai đoạn: thứ nhất, huấn luyện một hypernetwork để xuất ra trọng số cho bất kỳ kiến trúc nào trong không gian tìm kiếm. Tiếp theo, lấy mẫu ngẫu nhiên một tập lớn các kiến trúc, tạo trọng số của chúng bằng hypernetwork, và xuất ra cái có độ chính xác validation tốt nhất. Hypernetwork, một mạng nơ-ron tích chập, nhận đầu vào là một mã hóa kiến trúc và xuất ra một tập trọng số cho kiến trúc đó, và được huấn luyện bằng cách lấy mẫu ngẫu nhiên một kiến trúc, tạo trọng số của nó, tính training error của nó, và sau đó lan truyền ngược qua toàn bộ hệ thống (bao gồm trọng số hypernetwork).

Một thuật toán NAS dựa trên hypernet khác là GHN (Graph Hypernetworks) (Zhang et al., 2018). Sự khác biệt chính giữa SMASH và GHN là mã hóa kiến trúc và kiến trúc của hypernetwork. Cụ thể, hypernetwork GHN là một hỗn hợp giữa một graph neural network và một hypernetwork tiêu chuẩn. Nó nhận đầu vào là đồ thị tính toán của một kiến trúc a và sử dụng các phép toán message-passing điển hình trong GNN, để xuất ra trọng số của a. Việc huấn luyện hypernetwork, và thuật toán NAS cuối cùng, đều giống như trong SMASH.

## 5. Kỹ thuật Tăng tốc

Trong mục này, chúng tôi bao gồm các kỹ thuật tăng tốc chung cho các thuật toán NAS, bao gồm dự đoán hiệu suất (Mục 5.1), các phương pháp đa độ tin cậy (Mục 5.2), các phương pháp meta-learning (Mục 5.3), và thừa hưởng trọng số (Mục 5.4).

### 5.1 Dự đoán Hiệu suất

Một khối lượng lớn công việc đã được dành cho việc dự đoán hiệu suất của các mạng nơ-ron trước khi chúng được huấn luyện đầy đủ. Những kỹ thuật như vậy có tiềm năng tăng tốc đáng kể thời gian chạy của các thuật toán NAS, vì chúng loại bỏ nhu cầu huấn luyện đầy đủ mỗi kiến trúc đang được xem xét. Những kỹ thuật tăng tốc này có thể cải thiện gần như tất cả các loại thuật toán NAS, từ tối ưu hóa hộp đen (Ru et al., 2020a; White et al., 2021c) đến NAS one-shot (Xiang et al., 2021). Trong mục này, chúng tôi thảo luận về bản thân các kỹ thuật dự đoán hiệu suất, trong khi trong Mục 5.2, chúng tôi thảo luận về các phương pháp kết hợp chúng vào các thuật toán NAS.

Chính thức, cho một không gian tìm kiếm A và kiến trúc a ∈ A, biểu thị độ chính xác validation cuối cùng thu được với một đường ống huấn luyện cố định là f(a). Một predictor hiệu suất f' được định nghĩa là bất kỳ hàm nào dự đoán độ chính xác hoặc độ chính xác tương đối của các kiến trúc, mà không cần huấn luyện đầy đủ chúng. Nói cách khác, việc đánh giá f'(a) mất ít thời gian hơn việc đánh giá f(a), và {f'(a) | a ∈ A} lý tưởng có tương quan cao hoặc tương quan thứ hạng cao với {f(a) | a ∈ A}. Trong phần còn lại của mục này, chúng tôi đưa ra một tổng quan về các loại predictor hiệu suất khác nhau, bao gồm ngoại suy đường cong học (Mục 5.1.1), zero-cost proxy (Mục 5.1.2), và các phương pháp khác (Mục 5.1.3). Lưu ý rằng các mô hình surrogate (Mục 3.4) và các mô hình one-shot (Mục 4) cũng có thể được xem như các loại predictor hiệu suất.

#### 5.1.1 Ngoại suy Đường cong Học

Các phương pháp ngoại suy đường cong học tìm cách dự đoán hiệu suất cuối cùng của một kiến trúc nhất định sau khi huấn luyện một phần nó, bằng cách ngoại suy từ cái gọi là đường cong học một phần (chuỗi độ chính xác validation tại tất cả các epoch cho đến nay). Điều này có thể, ví dụ, được thực hiện bằng cách fit đường cong học một phần vào một mô hình tham số (Domhan et al., 2015) (xem Hình 8 (trái)). Các phương pháp ngoại suy đường cong học cũng có thể được sử dụng cùng với một mô hình surrogate: trong trường hợp đó, mô hình nhận đầu vào cả mã hóa của a và một đường cong học một phần của a, và xuất ra một dự đoán f'(a) (Baker et al., 2018; Klein et al., 2017).

[Hình 8: Minh họa các loại chính của predictor hiệu suất: ngoại suy đường cong độ chính xác validation thông qua một mô hình tham số (trái), đánh giá khả năng tổng quát của một kiến trúc với một forward pass duy nhất của một minibatch dữ liệu duy nhất (giữa), và huấn luyện kiến trúc trên một tập con của dữ liệu (phải).]

Các phương pháp ngoại suy đường cong học có thể được sử dụng để tăng tốc các thuật toán NAS hộp đen (Domhan et al., 2015; Ru et al., 2020a; Yan et al., 2021b) hoặc kết hợp với các thuật toán đa độ tin cậy như Hyperband hoặc BOHB (được mô tả trong Mục 5.2).

#### 5.1.2 Zero-Cost Proxy

Zero-cost proxy là một họ kỹ thuật dự đoán hiệu suất được phát triển gần đây. Ý tưởng là chạy một tính toán rất nhanh (như một forward và backward pass duy nhất của một minibatch dữ liệu duy nhất) trên một tập các kiến trúc gán một điểm số cho mỗi kiến trúc, với hy vọng rằng các điểm số tương quan với độ chính xác cuối cùng (Mellor et al., 2021). Những kỹ thuật này có tên "zero-cost" vì tổng thời gian để chấm điểm mỗi kiến trúc là không đáng kể (thường ít hơn 5 giây) so với hầu hết các kỹ thuật dự đoán hiệu suất khác (Abdelfattah et al., 2021). Trong khi hầu hết zero-cost proxy tính điểm kiến trúc từ một (duy nhất) minibatch dữ liệu, một số là data-independent, tính điểm chỉ từ trọng số được khởi tạo hoặc số lượng tham số của mạng nơ-ron.

Zero-cost proxy được giới thiệu đầu tiên bởi Mellor et al. (2021), những người ước lượng hiệu suất tương đối của các mạng nơ-ron dựa trên mức độ tốt các vùng tuyến tính khác nhau của bản đồ mạng được tách biệt (xem Hình 8 (giữa)). Kể từ kỹ thuật ban đầu, một số zero-cost proxy mới đã được giới thiệu. Abdelfattah et al. (2021) tạo ra một kết nối với tài liệu pruning-at-initialization (Lee et al., 2019b; Tanaka et al., 2020; Theis et al., 2018; Wang et al., 2020a) và sử dụng kết nối này để giới thiệu năm zero-cost proxy. Phương pháp hoạt động tốt nhất của họ, synflow (Tanaka et al., 2020), là một phương pháp data-independent tính L1 path-norm của mạng: nó tính tổng của tích của tất cả trọng số được khởi tạo trong mỗi đường dẫn kết nối đầu vào với đầu ra.

Kể từ đó, hai phương pháp data-independent khác đã được giới thiệu, dựa trên một chuỗi các tác vụ proxy tổng hợp để kiểm tra tính bất biến tỷ lệ và thông tin không gian (Li et al., 2021d), và dựa trên việc xấp xỉ mạng nơ-ron như một hàm tuyến tính từng khúc (Lin et al., 2021). Các phương pháp data-dependent khác sử dụng neural tangent kernel (NTK) (Jacot et al., 2018), dựa trên việc xấp xỉ trace norm của nó (Shu et al., 2021) hoặc xấp xỉ phổ của nó (Chen et al., 2021e).

Mặc dù zero-cost proxy đã nhận được sự chú ý đáng kể kể từ khi chúng được giới thiệu lần đầu, công việc gần đây đã chỉ ra rằng các baseline đơn giản như "số lượng tham số" và "FLOPs" có tính cạnh tranh đáng ngạc nhiên với tất cả các kỹ thuật hàng đầu. Những nhược điểm chính của việc sử dụng zero-cost proxy là chúng có thể không đáng tin cậy, đặc biệt trên các không gian tìm kiếm lớn hơn (Chen et al., 2022; Ning et al., 2021; White et al., 2022). Chúng cũng có thể có thiên vị, như ưa chuộng các mô hình lớn hơn (Ning et al., 2021) hoặc channel rộng (Chen et al., 2022), mặc dù thiên vị có thể được loại bỏ (Krishnakumar et al., 2022).

Mặt khác, công việc gần đây khuyến khích quan điểm rằng zero-cost proxy là "weak learner" có thể được kết hợp với các kỹ thuật khác, bao gồm zero-cost proxy khác, để cải thiện hiệu suất (Krishnakumar et al., 2022; White et al., 2022). Công việc ban đầu cho thấy rằng zero-cost proxy có thể được thêm thành công vào cả NAS dựa trên tối ưu hóa Bayes (Shen et al., 2021; White et al., 2021c) và NAS one-shot (Xiang et al., 2021).

#### 5.1.3 Các Dự đoán Độ tin cậy Thấp Khác

Bên cạnh huấn luyện ít epoch hơn, các công việc khác đưa ra một ước lượng độ tin cậy thấp của độ chính xác cuối cùng bằng cách huấn luyện trên một tập con của dữ liệu huấn luyện (hoặc một bộ dữ liệu nhỏ hơn, được tạo tổng hợp). Điều này được trực quan hóa trong Hình 8 (phải).

Nhiều công việc đã nghiên cứu các thuật toán lựa chọn tập con khác nhau, như lấy mẫu ngẫu nhiên, lấy mẫu dựa trên entropy (Na et al., 2021), clustering thông qua core-set (Shim et al., 2021), facility location (Prasad et al., 2022), và k-center (Na et al., 2021). Prasad et al. (2022) giới thiệu lựa chọn tập con thích ứng cho NAS, trong đó tập con được cập nhật trong suốt quá trình huấn luyện để tối đa hóa độ chính xác validation.

Such et al. (2020) giới thiệu generative teaching network sử dụng một tập nhỏ dữ liệu tổng hợp để huấn luyện các mạng nơ-ron nhanh hơn nhiều so với việc sử dụng dữ liệu huấn luyện thực ban đầu. Dữ liệu tổng hợp được tạo bằng một mạng tạo dữ liệu để khớp độ chính xác của một mạng được huấn luyện trên dữ liệu thực. Một phương pháp liên quan là synthetic petri dish (Rawal et al., 2020), đánh giá các motif kiến trúc bằng cách đặt chúng vào một mạng nơ-ron nhỏ và sau đó huấn luyện chúng bằng một bộ dữ liệu tổng hợp nhỏ. Phương pháp sau này cũng tối ưu hóa rõ ràng tương quan giữa thứ hạng kiến trúc với xấp xỉ và huấn luyện đầy đủ.

### 5.2 Thuật toán Đa độ tin cậy

Trong khi mục trước được dành cho các phương pháp dự đoán hiệu suất của các mạng nơ-ron, bây giờ chúng tôi bao gồm các thuật toán sử dụng những phương pháp này để chạy NAS hiệu quả.

Chính thức, hàm mục tiêu f: X → R, thường đắt đỏ để đánh giá đầy đủ, có thể được xấp xỉ rẻ bởi một phiên bản độ tin cậy thấp hơn f̂(·, b) của f(·), được tham số hóa bởi tham số độ tin cậy b. Khi b = b_max, chúng ta lấy lại hàm thực f(·) = f̂(·, b_max). Đây là một tổng quát hóa của định nghĩa từ Mục 5.1. Tham số độ tin cậy có thể biểu thị số epoch huấn luyện, kích thước tập con dữ liệu huấn luyện, và nó có thể sử dụng các kỹ thuật dự đoán hiệu suất từ mục trước. Người ta thậm chí có thể sử dụng nhiều tham số độ tin cậy cùng một lúc (Kandasamy et al., 2017; Zhou et al., 2020). Tiếp theo, chúng tôi mô tả các thuật toán tối ưu hóa khai thác quyền truy cập vào các ước lượng hàm đa độ tin cậy f̂(·, b):

Successive Halving (SH) (Jamieson và Talwalkar, 2016) là một trong những thuật toán đa độ tin cậy đơn giản nhất. Nó bắt đầu huấn luyện một số lượng lớn kiến trúc, từ từ loại bỏ ngày càng nhiều kiến trúc không hứa hẹn dựa trên các đánh giá độ tin cậy thấp hơn, cho đến khi chỉ những kiến trúc hứa hẹn nhất được đánh giá ở độ tin cậy cao nhất. Các ngưỡng độ tin cậy và số lượng kiến trúc để thăng tiến lên độ tin cậy cao hơn được kiểm soát bởi một siêu tham số. Một cải tiến phổ biến cho SH là Hyperband (HB) (Li et al., 2018), một chiến lược multi-armed bandit gọi SH lặp lại như một quy trình con, sử dụng các giá trị khác nhau của ngân sách tối thiểu cho mỗi lần gọi. Do đó, HB hedge bet của nó chống lại bất kỳ lựa chọn đơn lẻ nào của ngân sách tối thiểu.

Trong khi SH và HB hoàn toàn dựa trên tìm kiếm ngẫu nhiên (thông minh), các công việc gần đây đã kết hợp HB với cả tối ưu hóa Bayes và tiến hóa. Bayesian optimization hyperband (BOHB) (Falkner et al., 2018; Lindauer et al., 2022) hoạt động tương tự như HB trong lần lặp đầu tiên của nó, và trong các lần lặp sau nó fit một mô hình surrogate xác suất cho mỗi độ tin cậy để đưa ra quyết định lấy mẫu thông tin. Tương tự, DEHB (Mallik và Awad, 2021) kết hợp differential evolution (Storn và Price, 1997) với HB, cải thiện đáng kể các lần lặp sau của HB. ASHA (Li et al., 2020c) và ABOHB (Klein et al., 2020) cải thiện SH và BOHB hơn nữa, tương ứng, bằng cách sử dụng tính toán không đồng bộ song song khối lượng lớn và các chiến lược early stopping. Cuối cùng, EcoNAS (Zhou et al., 2020) đề xuất một phương pháp tìm kiếm tiến hóa phân cấp phân vùng không gian tìm kiếm thành các tập con và phân bổ độ tin cậy tăng dần cho các kiến trúc hứa hẹn nhất trong mỗi tập con.

### 5.3 Meta-Learning

Phần lớn các phương pháp NAS xem xét việc giải quyết một tác vụ đơn lẻ từ đầu, bỏ qua các giải pháp đã được khám phá trước đó. Tuy nhiên, điều này trái ngược với những gì cả nhà nghiên cứu và người thực hành thường làm. Thường, các kiến trúc được chuyển giao qua các bộ dữ liệu và thậm chí qua các tác vụ, và trên một tác vụ mới, các nhà nghiên cứu thường bắt đầu với một giải pháp tiên tiến. Vậy, người ta có thể hỏi: tại sao chạy NAS từ đầu thay vì tái sử dụng thông tin từ, ví dụ, các thí nghiệm trước đó? Câu hỏi này dẫn đến ý tưởng meta-learning hoặc learning to learn (Hochreiter et al., 2001; Schmidhuber, 1987; Thrun và Pratt, 1998), nhằm cải thiện một thuật toán học bằng cách tận dụng thông tin từ các thí nghiệm liên quan trong quá khứ (Hospedales et al., 2021; Vanschoren, 2019).

Wong et al. (2018) và Zimmer et al. (2021) sử dụng các chiến lược meta-learning trong một thiết lập học máy tự động tổng quát hơn. Vì trọng tâm không phải là NAS, cả hai chỉ xem xét một tập nhỏ các kiến trúc ứng viên. Trong Wong et al. (2018), các tác vụ được mã hóa theo cách tương tự như word embedding trong NLP (Mikolov et al., 2013). Ngược lại, Zimmer et al. (2021) chỉ đơn giản warm-start việc tìm kiếm của họ dựa trên các cấu hình hoạt động tốt trước đó.

Lian et al. (2020) và Elsken et al. (2020) tập trung vào few-shot learning: vấn đề học một tác vụ mới chỉ với một vài điểm dữ liệu để huấn luyện. Các tác giả mở rộng các phương pháp meta-learning dựa trên gradient, model-agnostic như MAML (Finn et al., 2017) và REPTILE (Nichol et al., 2018) không chỉ để meta-learning một tập trọng số ban đầu cho một kiến trúc mạng nơ-ron cố định, mà còn cho chính kiến trúc bằng cách kết hợp một phương pháp khả vi như DARTS (Liu et al., 2019c) vào thuật toán meta-learning.

Công việc của Lee et al. (2021) không bị hạn chế ở few-shot learning cũng không bị hạn chế ở việc chọn kiến trúc từ một tập nhỏ ứng viên. Thay vào đó, họ sử dụng các không gian tìm kiếm NAS điển hình như những cái được thảo luận trong Mục 2. Các tác giả đề xuất một set encoder mới để cải thiện deep set (Zaheer et al., 2017) và set transformer (Lee et al., 2019a). Một decoder dựa trên graph neural network được sử dụng để tạo ra các kiến trúc mạng nơ-ron cho một mã hóa tập hợp. Ngoài ra, một graph neural network được sử dụng để mã hóa các kiến trúc được tạo ra. Mã hóa kiến trúc kết hợp với mã hóa tập hợp sau đó được sử dụng để meta-learning một mô hình surrogate để dự đoán hiệu suất của cặp kiến trúc, bộ dữ liệu. Shala et al. (2022) mở rộng công việc của Lee et al. (2021) bằng cách sử dụng mã hóa bộ dữ liệu và kiến trúc trong một framework tối ưu hóa Bayes, dẫn đến một predictor surrogate xác suất. Điều này cho phép thích ứng surrogate với các điểm dữ liệu nhìn thấy tại thời điểm test.

### 5.4 Thừa hưởng Trọng số và Network Morphism

Trong khi các thuật toán NAS dựa trên tối ưu hóa hộp đen huấn luyện mỗi kiến trúc từ đầu, và các phương pháp one-shot huấn luyện tất cả kiến trúc với cùng một tập trọng số, một dòng công việc đề xuất một giải pháp trung gian: tái sử dụng trọng số của các kiến trúc đã được huấn luyện trên các kiến trúc chưa được huấn luyện tương tự. Ý tưởng này đặc biệt hữu ích cho các phương pháp tối ưu hóa hộp đen chỉ áp dụng những thay đổi nhỏ, tuần tự cho các kiến trúc khi tạo ra một kiến trúc ứng viên mới. Ví dụ, Real et al. (2017) đề xuất sao chép trọng số của tất cả các layer không bị ảnh hưởng bởi các mutation được áp dụng từ kiến trúc cha mẹ cho con cái của nó.

Ý tưởng này cũng đã được mở rộng bởi khái niệm network morphism (Chen et al., 2016; Wei et al., 2016). Network morphism là các toán tử hoạt động trên không gian của các kiến trúc mạng nơ-ron. Chúng thay đổi kiến trúc của một mạng nơ-ron mà không thay đổi hàm mà chúng biểu diễn, tức là, cho một đầu vào tùy ý, đầu ra giữ nguyên cho kiến trúc gốc và kiến trúc đã được sửa đổi bởi một network morphism. Điều này thường được đạt được bằng cách khởi tạo đúng cách kiến trúc được sửa đổi. Network morphism đã được sử dụng trong các thuật toán tiến hóa (Elsken et al., 2017, 2019a; Schorn et al., 2020; Wistuba, 2019), học tăng cường (Cai et al., 2018a,b), tối ưu hóa Bayes (Jin et al., 2019b), và thậm chí các phương pháp one-shot (Fang et al., 2020).

## 6. Mở rộng

Các mục trước đã nghiên cứu các kỹ thuật chính từ instantiation cổ điển của NAS. Trong mục này, chúng tôi khảo sát một vài mở rộng phổ biến: NAS + HPO kết hợp, NAS hạn chế/đa mục tiêu, và tìm kiếm ensemble mạng nơ-ron.

### 6.1 NAS + HPO Kết hợp

Trong khi một khối lượng lớn tài liệu NAS giả định các siêu tham số cố định trong thiết lập thí nghiệm của họ, nó đã được chỉ ra - có lẽ không đáng ngạc nhiên - rằng các siêu tham số cũng đóng một vai trò đáng kể. Ví dụ, trên không gian tìm kiếm DARTS, việc điều chỉnh siêu tham số có thể dẫn đến một cải thiện khổng lồ, vượt qua những lợi ích hiệu suất thu được bởi NAS (Yang et al., 2020). Tuy nhiên, các siêu tham số tốt nhất có thể thay đổi đáng kể qua các kiến trúc ngay cả trong cùng một không gian tìm kiếm (Yang et al., 2020). Do đó, một khối lượng công việc gần đây tìm cách vượt qua những thách thức này và đưa ra các thuật toán hiệu quả cho NAS + HPO (Dai et al., 2021; Dong et al., 2020; Izquierdo et al., 2021; Zela et al., 2018; Zhou et al., 2021).

Chạy NAS + HPO kết hợp thách thức hơn đáng kể so với chạy NAS hoặc HPO riêng lẻ. Thứ nhất, độ phức tạp của không gian tìm kiếm được tăng lên đáng kể, do số lượng siêu tham số tăng và tính không đồng nhất của các siêu tham số. Thứ hai, tương tác giữa kiến trúc và siêu tham số huấn luyện về mặt hiệu suất mạng khó mô hình hóa. Hơn nữa, một số siêu tham số có thể có tác động khác nhau đến hiệu suất dưới các ngân sách đánh giá khác nhau, làm giảm hiệu quả của nhiều kỹ thuật đa độ tin cậy và dự đoán hiệu suất.

Dưới ánh sáng những thách thức này, một số giải pháp đã được đề xuất. Nhiều phương pháp đã được giới thiệu để đồng nhất không gian tìm kiếm, như tái cấu trúc NAS như một vấn đề HPO với các siêu tham số phân loại (Zela et al., 2018), hoặc chuẩn hóa biểu diễn của các siêu tham số NAS và HPO bằng cách gán các hệ số có giá trị liên tục trong [0, 1] (Dong et al., 2020). Các chiến lược tìm kiếm giống các thuật toán NAS tiêu chuẩn như BO (Dai et al., 2021; Izquierdo et al., 2021; Zela et al., 2018), tiến hóa (Dai et al., 2021; Izquierdo et al., 2021), hoặc REINFORCE với chia sẻ trọng số (Dong et al., 2020).

### 6.2 NAS Hạn chế và Đa mục tiêu

Mặc dù NAS đã rất phổ biến trong những năm gần đây, hầu hết công việc tập trung vào việc tối ưu hóa chỉ cho một mục tiêu duy nhất, thường là độ chính xác hoặc tỷ lệ lỗi. Tuy nhiên, có nhiều thiết lập mà điều này không đủ, như khi mạng nơ-ron phải được triển khai trên một thiết bị edge hoặc phải thỏa mãn một định nghĩa pháp lý về công bằng. Trong những ứng dụng như vậy, chúng ta có thể cần hạn chế độ trễ, sử dụng bộ nhớ, hoặc tỷ lệ lỗi qua các lớp (Sukthanker et al., 2022). Đã có sự quan tâm đặc biệt đến các ràng buộc liên quan đến thiết bị edge và phần cứng khác, được gọi là hardware-aware NAS (Benmeziane et al., 2021). Để đạt được một hoặc nhiều mục tiêu ngoài độ chính xác, mục tiêu NAS tiêu chuẩn thường được sửa đổi thành một vấn đề tối ưu hóa có ràng buộc (ví dụ: Bender et al. (2020); Cai et al. (2019); Tan et al. (2019)) hoặc một vấn đề tối ưu hóa đa mục tiêu (ví dụ: Elsken et al. (2019a); Hu et al. (2019); Izquierdo et al. (2021); Lu et al. (2019, 2020)).

Trong tối ưu hóa có ràng buộc, người ta cố gắng giải phương trình sau:

min_{a∈A} f(a) subject to h_i(a) ≤ c_i for i ∈ {1, ..., k} (2)

trong đó f(a) biểu thị, như trước, hàm mục tiêu gốc (ví dụ: lỗi validation), và h_i biểu diễn các ràng buộc phần cứng như một hàm của kiến trúc. Vấn đề này thường được giải quyết bằng một chuyển đổi thành một vấn đề không ràng buộc cộng tính hoặc nhân như min_{a∈A} f(a) + Σ_i λ_i g_i(a) với các hàm phạt g_i phạt các kiến trúc không thỏa mãn ràng buộc, ví dụ: g_i(a) = max{0, h_i(a) - c_i} và các siêu tham số λ_i đánh đổi các mục tiêu và ràng buộc. Vấn đề tối ưu hóa mục tiêu đơn này sau đó được giải quyết bằng các phương pháp tối ưu hóa hộp đen hoặc các phương pháp one-shot. Trong trường hợp sau, các hàm phạt g_i cần phải khả vi, điều này thường không đúng. Do đó, các metric rời rạc như độ trễ được relaxed thành các biến liên tục thông qua nhiều kỹ thuật khác nhau, như với một hàm Gumbel softmax (Wu et al., 2019b).

Trong tối ưu hóa đa mục tiêu, các yêu cầu trong Phương trình 2 được coi như các mục tiêu riêng biệt được tối ưu hóa cùng với mục tiêu gốc:

min_{a∈A} {f(a), h_1(a), ..., h_k(a)}.

Trong khi điều này có thể lại được giảm xuống một vấn đề mục tiêu đơn thông qua các phương pháp scalarization, một phương pháp phổ biến khác là tìm kiếm một tập hợp các giải pháp không bị chi phối là tối ưu theo nghĩa rằng người ta không thể giảm bất kỳ mục tiêu nào mà không tăng ít nhất một mục tiêu khác. Tập hợp các giải pháp không bị chi phối được gọi là Pareto front. Phương pháp phổ biến nhất trong trường hợp này là sử dụng các thuật toán tiến hóa đa mục tiêu duy trì một quần thể kiến trúc và nhằm cải thiện Pareto front thu được từ quần thể hiện tại bằng cách phát triển quần thể hiện tại (Elsken et al., 2019a; Hu et al., 2019; Izquierdo et al., 2021; Lu et al., 2019). Các thuật toán tiến hóa đa mục tiêu cũng đã được sử dụng kết hợp với chia sẻ trọng số trong các mô hình one-shot (Lu et al., 2020; Muñoz et al., 2022).

Một trong những vấn đề NAS có ràng buộc được nghiên cứu rộng rãi nhất liên quan đến hiệu quả phần cứng như bộ nhớ hoặc độ trễ, và nhiều công việc đã được dành cho việc xấp xỉ hiệu quả các metric phần cứng quan tâm. Trong khi các metric đơn giản như số lượng tham số dễ tính toán, chúng thường không tương quan đủ với các metric quan tâm khác như bộ nhớ hoặc độ trễ. Các giải pháp khác bao gồm tính chi phí phần cứng theo mô-đun như tổng chi phí phần cứng của mỗi phép toán (Cai et al., 2019) hoặc bằng cách sử dụng một mô hình surrogate dự đoán chi phí phần cứng (Dudziak et al., 2020; Laube et al., 2022).

### 6.3 Tìm kiếm Ensemble Mạng nơ-ron

Trong khi mục tiêu của tìm kiếm kiến trúc mạng nơ-ron là trả về kiến trúc độc lập tốt nhất, các phương pháp ensemble rất phổ biến trong cộng đồng học sâu cho dự đoán mạnh mẽ và định lượng độ không chắc chắn dễ dàng. Một mở rộng mới nổi của NAS liên quan đến việc tìm ensemble tốt nhất của các mạng nơ-ron với các kiến trúc đa dạng, có thể vượt qua NAS tiêu chuẩn về độ chính xác, hiệu chuẩn độ không chắc chắn, và tính mạnh mẽ đối với dataset shift (Zaidi et al., 2021). Tìm kiếm ensemble mạng nơ-ron được định nghĩa như sau:

min_{a_1,...,a_M∈A} L_val(Ensemble(π(w*(a_1), a_1), ..., π(w*(a_M), a_M))) (3)
s.t. w*(a) = argmin_w L_train(w, a) ∀a ∈ A,

trong đó Ensemble là hàm tổng hợp đầu ra của π_1, ..., π_M. Lưu ý rằng cardinality không gian tìm kiếm là |A|^M thay vì |A| như trong NAS tiêu chuẩn.

Zaidi et al. (2021) đề xuất hai quy trình đơn giản nhưng hiệu quả dựa trên tìm kiếm ngẫu nhiên và regularized evolution (Real et al., 2019) tìm kiếm các kiến trúc tối ưu hóa Phương trình 3. Mặc dù hiệu quả, những thuật toán này tốn đáng kể tính toán do bản chất hộp đen của các thuật toán tối ưu hóa. Multi-headed NES (Narayanan et al., 2021) vượt qua vấn đề này bằng cách áp dụng các phương pháp NAS khả vi trên các head của một mạng multi-headed. Các head được điều chỉnh rõ ràng để tối ưu hóa ensemble loss cùng với một thành phần đa dạng khuyến khích dự đoán không tương quan đến từ các head riêng lẻ. Các công việc khác đã thiết lập tìm kiếm ensemble mạng nơ-ron với một mô hình one-shot cho toàn bộ kiến trúc. NESBS (Neural Ensemble Search via Bayesian Sampling) (Shu et al., 2022) đề xuất sử dụng một supernet để ước lượng hiệu suất ensemble của các base learner được huấn luyện độc lập và sau đó sử dụng Bayesian sampling để tìm một ensemble hiệu suất cao. NADS (Neural Architecture Distribution Search) (Ardywibowo et al., 2020) theo một dòng tương tự bằng cách huấn luyện một supernet để tối ưu hóa một mục tiêu được thiết kế riêng để cung cấp ước lượng độ không chắc chắn tốt hơn và phát hiện out-of-distribution. Chen et al. (2021b) chạy tìm kiếm tiến hóa trên supernet để tìm một ensemble hiệu suất cao.

## 7. Ứng dụng

Cùng với việc khám phá các kiến trúc cải thiện cho các bộ dữ liệu nổi tiếng, một trong những mục tiêu chính của lĩnh vực NAS là nhanh chóng và tự động tìm ra các kiến trúc hiệu suất cao cho các bộ dữ liệu và tác vụ hoàn toàn mới. Mặc dù phần lớn tài liệu NAS tập trung vào phân loại hình ảnh, có nhiều câu chuyện thành công của NAS được áp dụng cho các thiết lập ít nổi tiếng hơn. Trong mục này, chúng tôi thảo luận về một vài thành công này, bao gồm graph neural network, generative adversarial network, dự đoán dày đặc, và transformer.

### 7.1 Graph Neural Network

Graph neural network (GNN) được thiết kế để xử lý dữ liệu được biểu diễn bởi đồ thị. Việc sử dụng NAS để thiết kế GNN đặt ra những vấn đề độc đáo: không gian tìm kiếm cho GNN phức tạp hơn các không gian tìm kiếm tích chập điển hình, và cả NAS và GNN đều được biết đến độc lập với overhead tính toán lớn của chúng.

Zhou et al. (2019) khởi xướng một dòng công việc áp dụng NAS cho GNN bằng cách định nghĩa một không gian tìm kiếm mới với các phép toán dành riêng cho GNN và sau đó sử dụng một chiến lược học tăng cường. Công việc tiếp theo thiết kế các không gian tìm kiếm tương tự (Gao et al., 2020b; Zhang et al., 2021b) với các tính năng chuyên biệt như meta-path (Ding et al., 2021b), edge feature (Jiang và Balaprakash, 2020), hoặc các phép toán lấy mẫu nhanh (Gao et al., 2020b).

Tổng thể, sự khác biệt chính giữa NAS cho GNN và các thiết lập NAS tiêu chuẩn hơn nằm trong việc xây dựng không gian tìm kiếm. Các chiến lược tìm kiếm chính được sử dụng bởi các thuật toán GNN NAS là các phương pháp NAS điển hình: học tăng cường (Gao et al., 2020b; Zhao et al., 2020a; Zhou et al., 2019), các phương pháp one-shot (Ding et al., 2021b; Zhao et al., 2020b), và thuật toán tiến hóa (Jiang và Balaprakash, 2020; Nunes và Pappa, 2020). Để có một khảo sát chi tiết về NAS cho GNN, xem Zhang et al. (2021b).

### 7.2 Generative Adversarial Network

Generative adversarial network (GAN) (Goodfellow et al., 2014) là một lựa chọn phổ biến cho mô hình hóa sinh trong các tác vụ như thị giác máy tính. GAN sử dụng hai mạng riêng biệt huấn luyện song song: một generator và một discriminator. Do có hai mạng riêng biệt, và động lực huấn luyện khó khăn nổi tiếng của chúng (Gulrajani et al., 2017), GAN yêu cầu các kỹ thuật đặc biệt cho NAS hiệu quả.

Các công việc khác nhau đã đạt được hiệu suất cải thiện thông qua NAS bằng cách tìm kiếm chỉ kiến trúc generator với một discriminator cố định (Doveh và Giryes, 2021), với một discriminator progressively growing được định nghĩa trước (Fu et al., 2020), hoặc bằng cách tìm kiếm đồng thời cả kiến trúc generator và discriminator (Gong et al., 2019). Lựa chọn phổ biến nhất của không gian tìm kiếm là không gian tìm kiếm dựa trên cell. Cell cho generator bao gồm một cell tích chập tiêu chuẩn, với việc bổ sung các phép toán upsampling khác nhau (Ganepola và Wirasingha, 2021; Gong et al., 2019; Tian et al., 2020).

Các kỹ thuật tìm kiếm giống các kỹ thuật được sử dụng cho NAS tiêu chuẩn: học tăng cường (Fu et al., 2020; Tian et al., 2020; Wang và Huan, 2019), NAS one-shot (Doveh và Giryes, 2021; Gao et al., 2020a; Lutz et al., 2018), và thuật toán tiến hóa (Kobayashi và Nagao, 2020), với chấm điểm dựa trên Inception Score (IS) (Salimans et al., 2016) hoặc Fréchet Inception Distance (FID) (Heusel et al., 2017). Để có một khảo sát toàn diện về NAS cho GAN, xem Ganepola và Wirasingha (2021).

### 7.3 Tác vụ Dự đoán Dày đặc

Dự đoán dày đặc cho thị giác máy tính bao gồm nhiều tác vụ phổ biến như phân đoạn ngữ nghĩa, phát hiện đối tượng, optical flow, và ước lượng disparity, và nó yêu cầu các kiến trúc phức tạp hơn so với các vấn đề phân loại hình ảnh tiêu chuẩn. Ví dụ, các kiến trúc thường bao gồm một decoder (Ronneberger et al., 2015), các mô-đun để tạo ra các đặc trưng multi-scale (He et al., 2015) hoặc các head dành riêng cho tác vụ (Girshick et al., 2014) ngoài mạng chính. Do đó, các thuật toán NAS đã được áp dụng để tìm kiếm những thành phần này, hoặc riêng lẻ (Chen et al., 2018; Ghiasi et al., 2019; Xu et al., 2019a) hoặc cùng nhau (Guo et al., 2020a; Yao et al., 2020), hoặc bằng cách khám phá ra các mẫu thiết kế mới (Du et al., 2020). Để có một khảo sát về NAS cho dự đoán dày đặc, xem Elsken et al. (2022).

Một lần nữa, các kỹ thuật NAS tiêu chuẩn được sử dụng: Guo et al. (2020a); Liu et al. (2019a); Saikia et al. (2019); Xu et al. (2019a) sử dụng tìm kiếm dựa trên gradient thông qua DARTS (Liu et al., 2019c); Du et al. (2020); Ghiasi et al. (2019) sử dụng RL; Bender et al. (2020) được lấy cảm hứng từ ProxylessNAS (Cai et al., 2019) và ENAS (Pham et al., 2018).

Các phương pháp cho tác vụ dự đoán dày đặc (ví dụ: Bender et al. (2020); Chen et al. (2019b); Guo et al. (2020a); Shaw et al. (2019); Wu et al. (2019a)) thường xây dựng không gian tìm kiếm dựa trên các mạng phân loại hình ảnh tiên tiến, với các thành phần dành riêng cho tác vụ từ các thành phần kiến trúc dự đoán dày đặc hoạt động tốt. Vì nhiều phương pháp cố định backbone và chỉ tìm kiếm các thành phần dành riêng cho tác vụ khác của kiến trúc, chúng thường sử dụng các kiến trúc backbone được pre-train (Chen et al., 2020; Guo et al., 2020a) hoặc thậm chí cache các đặc trưng được tạo ra bởi một backbone (Chen et al., 2018; Nekrasov et al., 2019; Wang et al., 2020c) để tăng tốc tìm kiếm kiến trúc. Chen et al. (2018); Ghiasi et al. (2019) cũng sử dụng một kiến trúc backbone thu nhỏ hoặc khác trong quá trình tìm kiếm.

Các phương pháp cũng đôi khi sử dụng nhiều giai đoạn tìm kiếm, với mục tiêu đầu tiên loại bỏ các kiến trúc hoạt động kém (hoặc các phần của không gian tìm kiếm) và cải thiện tuần tự các kiến trúc còn lại (Du et al., 2020; Guo et al., 2020a).

Tổng thể, trong khi việc chạy NAS trên các tác vụ dự đoán dày đặc khó hơn nhiều so với các tác vụ phân loại hình ảnh vì yêu cầu tính toán của dự đoán dày đặc, đã có sự gia tăng nhanh chóng trong các phát triển với sự phát triển của các phương pháp NAS one-shot hiệu quả về tính toán. Trong khi những nỗ lực cho đến nay đã tập trung vào phân đoạn ngữ nghĩa và phát hiện đối tượng, các hướng cho công việc tương lai bao gồm ước lượng disparity, phân đoạn panoptic, phát hiện và phân đoạn 3D, và ước lượng optical flow.

### 7.4 Transformer

Transformer được đề xuất bởi Vaswani et al. (2017) để giúp giải quyết vấn đề của các chuỗi dài hơn mà RNN gặp khó khăn trong việc mô hình hóa, bằng cách sử dụng cơ chế self-attention và cross-attention sao cho biểu diễn của mỗi token trong một chuỗi đầu vào được tính từ một trung bình có trọng số của biểu diễn của tất cả các token khác. Thiết kế transformer cốt lõi được giới thiệu cho dịch máy, nhưng nó đã tìm thấy việc sử dụng rộng rãi trong mô hình hóa ngôn ngữ causal (Brown et al., 2020; Radford et al., 2019), mô hình hóa ngôn ngữ masked (Clark et al., 2020; Devlin et al., 2019; Liu et al., 2019d), và gần đây hơn, thị giác máy tính (Dosovitskiy et al., 2021; Liu et al., 2021b). Kể từ khi phát hành, đã có nhiều nỗ lực cải thiện transformer thông qua NAS. Các chiến lược tìm kiếm phổ biến nhất cho transformer là tiến hóa (Chen et al., 2021c; So et al., 2019, 2021) hoặc one-shot (Ding et al., 2021a; Gong et al., 2021; Li et al., 2021a; Su et al., 2021). Mặt khác, có một sự đa dạng lớn của các không gian tìm kiếm khác nhau đã được thử gần đây, so với các lĩnh vực khác (ví dụ: trong NAS cho kiến trúc tích chập, phần lớn các công việc sử dụng không gian tìm kiếm dựa trên cell). Tổng thể, lĩnh vực NAS cho transformer chưa hội tụ về một loại không gian tìm kiếm "tốt nhất". Dưới đây, chúng tôi khảo sát các phương pháp NAS cho bốn loại transformer: chỉ decoder, chỉ encoder, encoder-decoder, và vision transformer. Xem Chitty-Venkata et al. (2022) để có một khảo sát sâu.

Các kiến trúc chỉ decoder, như dòng kiến trúc GPT (Brown et al., 2020; Radford et al., 2019) trực tiếp tiêu thụ prompt văn bản đầu vào và xuất ra chuỗi các token văn bản có khả năng cao nhất để theo sau. Primer (So et al., 2021) là một thuật toán NAS sử dụng tìm kiếm tiến hóa trên một không gian tìm kiếm macro decoder-only lớn. Phương pháp tìm thấy hai cải tiến nhất quán cho transformer block: bình phương ReLU trong feedforward block trong transformer layer, và thêm depthwise convolution sau self-attention head.

Các kiến trúc chỉ encoder, như BERT (Devlin et al., 2019) mã hóa văn bản đầu vào thành một biểu diễn có thể được sử dụng cho nhiều loại tác vụ downstream. Nhiều công việc (Xu et al., 2021a, 2022; Yin et al., 2021) tìm cách khám phá các phiên bản nén của BERT, trong đó độ trễ mong muốn và tác vụ được chỉ định bởi người dùng. Phương pháp điển hình là huấn luyện một supernet trên một tác vụ self-supervised tiêu chuẩn (masked language modeling), sau đó có thể được sử dụng để khám phá các mô hình nén cho một tác vụ ngôn ngữ nhất định.

Các kiến trúc encoder-decoder như T5 (Raffel et al., 2020) được sử dụng trong các tác vụ sequence-to-sequence như dịch máy, trong đó ngôn ngữ nguồn được mã hóa thành một biểu diễn, sau đó được giải mã thành ngôn ngữ đích. So et al. (2019) sử dụng tìm kiếm tiến hóa cùng với một kỹ thuật mới để phân bổ động nhiều tài nguyên hơn cho các mô hình ứng viên hứa hẹn hơn, trong khi Zhao et al. (2021b) đề xuất một thuật toán dựa trên DARTS với một kỹ thuật mới cho hiệu quả bộ nhớ trong backpropagation. Cuối cùng, KNAS (Xu et al., 2021b) và SemiNAS (Luo et al., 2020) tăng tốc việc tìm kiếm bằng cách sử dụng zero-cost proxy và một mô hình transformer surrogate, tương ứng.

Một loạt lớn các thuật toán NAS đã được nghiên cứu cho các không gian tìm kiếm vision transformer, với phần lớn sử dụng các phương pháp one-shot. AutoFormer (Chen et al., 2021c) tìm kiếm các kiến trúc và siêu tham số vision transformer bằng cách sử dụng chiến lược single-path-one-shot (Guo et al., 2020b) và sau đó chạy tìm kiếm tiến hóa trên supernet được huấn luyện. Một công việc tiếp theo, AutoFormerv2 (Chen et al., 2021d), tự động hóa thiết kế của bản thân không gian tìm kiếm bằng cách phát triển dần các chiều tìm kiếm khác nhau. Các công việc khác đã cải thiện huấn luyện supernet thông qua huấn luyện gradient conflict aware (Gong et al., 2021) hoặc huấn luyện channel-aware (Su et al., 2021). Cuối cùng, Li et al. (2021a) và Ding et al. (2021a) chạy các phương pháp one-shot trên các không gian tìm kiếm CNN và transformer hybrid cho thị giác máy tính.

## 8. Điểm chuẩn

Trong những ngày đầu của nghiên cứu NAS, các metric phổ biến nhất là độ chính xác test cuối cùng trên CIFAR-10 và ImageNet. Điều này gây ra các không gian tìm kiếm và đường ống huấn luyện không nhất quán qua các bài báo, và cũng đẩy chi phí tính toán lên cao. Ví dụ, nó trở thành tiêu chuẩn để huấn luyện kiến trúc cuối cùng trong 600 epoch, mặc dù độ chính xác test chỉ tăng một phần nhỏ của một phần trăm sau 200 epoch. Gần đây, các điểm chuẩn NAS có thể truy vấn đã giúp lĩnh vực giảm tính toán khi phát triển các kỹ thuật NAS và đạt được các so sánh công bằng, có ý nghĩa thống kê giữa các phương pháp.

Một điểm chuẩn NAS (Lindauer và Hutter, 2020) được định nghĩa là một bộ dữ liệu với một chia train-test cố định, một không gian tìm kiếm, và một đường ống đánh giá cố định để huấn luyện các kiến trúc. Một điểm chuẩn NAS dạng bảng là một điểm chuẩn bổ sung đưa ra các đánh giá được tính toán trước cho tất cả các kiến trúc có thể trong không gian tìm kiếm. Một điểm chuẩn NAS surrogate là một điểm chuẩn NAS cùng với một mô hình surrogate có thể được sử dụng để dự đoán hiệu suất của bất kỳ kiến trúc nào trong không gian tìm kiếm. Một điểm chuẩn NAS có thể truy vấn nếu nó là một điểm chuẩn dạng bảng hoặc surrogate. Các điểm chuẩn NAS có thể truy vấn có thể được sử dụng để mô phỏng hiệu quả nhiều thí nghiệm NAS chỉ sử dụng CPU, bằng cách truy vấn hiệu suất của các mạng nơ-ron từ điểm chuẩn, thay vì huấn luyện chúng từ đầu. Trong phần còn lại của mục, chúng tôi đưa ra một tổng quan về các điểm chuẩn NAS phổ biến. Xem Bảng 2 của Phụ lục để có một tóm tắt.

Điểm chuẩn dạng bảng đầu tiên là NAS-Bench-101 (Ying et al., 2019). Nó bao gồm một không gian tìm kiếm dựa trên cell của 423,624 kiến trúc, mỗi cái có độ chính xác validation và test được tính toán trước trên CIFAR-10 cho ba seed khác nhau. Một công việc tiếp theo, NAS-Bench-1Shot1 (Zela et al., 2020b), có thể mô phỏng các thuật toán one-shot bằng cách định nghĩa các tập con của không gian tìm kiếm NAS-Bench-101 có số lượng nút cố định. NAS-Bench-201 (Dong và Yang, 2020) là một điểm chuẩn NAS dạng bảng phổ biến khác, bao gồm 6,466 kiến trúc duy nhất, mỗi cái có độ chính xác validation và test được tính toán trước trên CIFAR-10, CIFAR-100, và ImageNet-16-120 cho ba seed mỗi cái. NATS-Bench (Dong et al., 2021b) là một mở rộng của NAS-Bench-201 cũng bao gồm một không gian tìm kiếm macro. Một mở rộng khác, HW-NAS-Bench-201 (Li et al., 2021b), đưa ra chi phí phần cứng được đo hoặc ước lượng cho tất cả các kiến trúc qua sáu thiết bị phần cứng.

Surr-NAS-Bench-DARTS (trước đây được gọi là NAS-Bench-301) (Siems et al., 2020) là điểm chuẩn NAS surrogate đầu tiên, được tạo bằng cách huấn luyện 60,000 kiến trúc từ không gian tìm kiếm DARTS (Liu et al., 2019c) trên CIFAR-10 và sau đó huấn luyện một mô hình surrogate. Các tác giả cũng phát hành Surr-NAS-Bench-FBNet cho không gian tìm kiếm FBNet (Wu et al., 2019b). Một công việc tiếp theo, NAS-Bench-x11 (Yan et al., 2021b), đã phát triển một kỹ thuật để dự đoán đường cong học đầy đủ, cho phép độ chính xác validation được truy vấn tại các epoch tùy ý, điều này cần thiết để mô phỏng các thuật toán NAS đa độ tin cậy.

TransNAS-Bench-101 (Duan et al., 2021) là một điểm chuẩn dạng bảng bao gồm bảy tác vụ thị giác máy tính khác nhau từ bộ dữ liệu Taskonomy (Zamir et al., 2018). Ngoài thị giác máy tính, NAS-Bench-NLP (Klyuchnikov et al., 2022) bao gồm một không gian tìm kiếm lấy cảm hứng từ LSTM cho NLP, và NAS-Bench-ASR (Mehrotra et al., 2021) là một điểm chuẩn NAS dạng bảng cho nhận dạng giọng nói tự động (Garofolo, 1993). NAS-Bench-360 (Tu et al., 2022a) là một bộ điểm chuẩn đưa ra các điểm chuẩn NAS trên mười vấn đề đa dạng như điều khiển prothetric, giải PDE, gấp protein, và hình ảnh thiên văn học, và là search space agnostic, mặc dù ba trong số các tác vụ có các kiến trúc được pre-train trên không gian tìm kiếm NAS-Bench-201. Cuối cùng, NAS-Bench-Suite (Mehta et al., 2022) là một bộ điểm chuẩn kết hợp phần lớn các điểm chuẩn NAS có thể truy vấn hiện có, tổng cộng 28 tác vụ, thành một giao diện thống nhất duy nhất. Một mở rộng, NAS-Bench-Suite-Zero, cung cấp các giá trị zero-cost proxy được tính toán trước qua tất cả các tác vụ (Krishnakumar et al., 2022).

Việc sử dụng các điểm chuẩn có thể truy vấn cho phép các nhà nghiên cứu dễ dàng mô phỏng hàng trăm thử nghiệm của các thuật toán với các seed ngẫu nhiên ban đầu khác nhau, làm cho việc báo cáo các so sánh có ý nghĩa thống kê trở nên dễ dàng. Tuy nhiên, sự phụ thuộc quá mức vào một vài điểm chuẩn có thể dẫn đến việc lĩnh vực over-fitting (Koch et al., 2021; Raji et al., 2021) và không có lợi cho việc khám phá ra các phương pháp thực sự mới lạ. Do đó, các nhà nghiên cứu nên sử dụng một tập lớn các điểm chuẩn NAS đa dạng bất cứ khi nào có thể.

## 9. Thực hành Tốt nhất

Lĩnh vực NAS đôi khi gặp phải các vấn đề về khả năng tái tạo và so sánh công bằng, có ý nghĩa thống kê giữa các phương pháp. Những vấn đề này cản trở tiến bộ nghiên cứu tổng thể trong lĩnh vực NAS. Gần đây, một vài bài báo đã đặt ra các thực hành tốt nhất và hướng dẫn để tiến hành nghiên cứu NAS lành mạnh có thể tái tạo và thực hiện các so sánh công bằng (Li và Talwalkar, 2019; Lindauer và Hutter, 2020; Yang et al., 2020). Những thực hành tốt nhất này cũng có sẵn như một danh sách kiểm tra (Lindauer và Hutter, 2020). Chúng tôi khuyến khích các nhà nghiên cứu NAS tuân theo danh sách kiểm tra và đính kèm nó vào phụ lục của bài báo của họ. Bây giờ, chúng tôi tóm tắt những thực hành tốt nhất này cho nghiên cứu NAS.

### 9.1 Phát hành Mã và Chi tiết Quan trọng

Gần như không thể tái tạo các phương pháp NAS mà không có mã đầy đủ. Ngay cả khi đó, các seed ngẫu nhiên nên được chỉ định và báo cáo. Hơn nữa, việc phát hành mã dễ sử dụng có thể dẫn đến nhiều phương pháp tiếp theo và tác động hơn. Ví dụ, Liu et al. (2019c) đã phát hành mã dễ sử dụng cho DARTS, điều này đã tạo điều kiện cho nhiều công việc tiếp theo.

Khi phát hành mã, điều quan trọng là phát hành tất cả các thành phần, bao gồm (các) đường ống huấn luyện, không gian tìm kiếm, siêu tham số, seed ngẫu nhiên, và phương pháp NAS. Nhiều bài báo sử dụng các đường ống huấn luyện kiến trúc khác nhau trong quá trình tìm kiếm và trong quá trình đánh giá cuối cùng, vì vậy điều quan trọng là bao gồm cả hai. Lưu ý rằng việc sử dụng các điểm chuẩn NAS phổ biến như NAS-Bench-101 hoặc NAS-Bench-201 (xem Mục 8) làm cho điều này dễ dàng hơn đáng kể: đường ống huấn luyện đã được cố định sẵn.

Các phương pháp NAS thường có một số phần chuyển động. Do đó, chúng thường có nhiều siêu tham số riêng có thể được điều chỉnh. Thực tế, nhiều phương pháp NAS tự chúng sử dụng các mạng nơ-ron - người ta thậm chí có thể chạy một thuật toán NAS trên thuật toán NAS! Do độ phức tạp này, điều quan trọng là báo cáo nếu, hoặc cách, những siêu tham số này được điều chỉnh. Khi báo cáo kết quả trên một tập lớn các không gian tìm kiếm và bộ dữ liệu, thực hành tốt nhất là điều chỉnh các siêu tham số của phương pháp NAS trên một bộ dữ liệu, và sau đó cố định những siêu tham số này cho các đánh giá còn lại trên các bộ dữ liệu khác. Chúng tôi cũng lưu ý rằng, nói chung, việc phát triển các phương pháp NAS với ít siêu tham số hơn là mong muốn hơn, đặc biệt là vì gần đây đã được chỉ ra rằng các siêu tham số thường không chuyển giao tốt qua các bộ dữ liệu và không gian tìm kiếm (Mehta et al., 2022).

### 9.2 So sánh Phương pháp NAS

Khi so sánh các phương pháp NAS, không đủ để sử dụng cùng các bộ dữ liệu. Chính xác cùng các điểm chuẩn NAS phải được sử dụng: một bộ dữ liệu với một chia train-test cố định, không gian tìm kiếm, và đường ống đánh giá. Nếu không, không rõ liệu sự khác biệt trong hiệu suất là do thuật toán NAS hay đường ống huấn luyện.

Một số bài báo đã chỉ ra rằng các baseline đơn giản có tính cạnh tranh với các thuật toán NAS tiên tiến (Li và Talwalkar, 2019; Ottelander et al., 2021; Sciuto et al., 2020; White et al., 2021b). Khi thiết kế một phương pháp mới cho NAS, điều quan trọng là so sánh phương pháp với các baseline như lấy mẫu ngẫu nhiên và tìm kiếm ngẫu nhiên. Hơn nữa, nhiều phương pháp NAS là các thuật toán anytime: một ngân sách thời gian không nhất thiết cần được chỉ định trước, và phương pháp có thể được dừng bất cứ lúc nào, trả về kiến trúc tốt nhất tìm được cho đến nay. Phương pháp NAS chạy càng lâu, kết quả cuối cùng càng tốt. Những phương pháp NAS này nên được so sánh trên một biểu đồ hiệu suất theo thời gian. Ngay cả các thuật toán one-shot cũng có thể được so sánh theo cách này, vì supernet có thể được rời rạc hóa và huấn luyện tại bất kỳ điểm nào.

Chúng tôi khuyến nghị rằng các nhà nghiên cứu NAS chạy các nghiên cứu ablation kỹ lưỡng để chỉ ra (các) phần nào của phương pháp NAS dẫn đến hiệu suất cải thiện nhất. Như đã đề cập trong mục trước, các phương pháp NAS thường có một số phần chuyển động, vì vậy một hiểu biết rõ ràng về tầm quan trọng của mỗi phần và cách chúng hoạt động cùng nhau, là quan trọng để báo cáo. Cuối cùng, chúng tôi khuyến nghị rằng các nhà nghiên cứu chạy nhiều thử nghiệm của các thí nghiệm của họ và báo cáo các seed ngẫu nhiên cho mỗi thí nghiệm. Các phương pháp NAS có thể có phương sai cao trong tính ngẫu nhiên của thuật toán, vì vậy việc chạy nhiều thử nghiệm quan trọng để xác minh các so sánh có ý nghĩa thống kê.

## 10. Tài nguyên

Trong mục này, chúng tôi thảo luận về các tài nguyên NAS bao gồm thư viện (Mục 10.1), các bài báo khảo sát khác (Mục 10.2), và tài nguyên bổ sung (Mục 10.3).

### 10.1 Thư viện

Một dòng kỹ thuật dài đã tập trung vào việc tự động hóa các đường ống học máy: Auto-WEKA (Thornton et al., 2013), Auto-Sklearn (Feurer et al., 2015), TPOT (Olson et al., 2016), và AutoGluon-Tabular (Erickson et al., 2020). Gần đây hơn, một sự tập trung đặc biệt đã được dành cho việc phát triển các công cụ có thể tạo điều kiện cho việc triển khai các thuật toán NAS khác nhau cho người thực hành, như Auto-Keras (Jin et al., 2019a), Auto-PyTorch Tabular (Zimmer et al., 2021), AutoGluon (Erickson et al., 2020), và NNI (Microsoft, 2021).

Để cung cấp một bộ công cụ để tạo điều kiện cho nghiên cứu NAS, cả trong việc phát triển các phương pháp NAS mới và áp dụng NAS cho các miền vấn đề mới, nhiều thư viện khác nhau đã được đề xuất. Thư viện DeepArchitect (Negrinho và Gordon, 2017), tách không gian tìm kiếm khỏi bộ tối ưu hóa, là một bước quan trọng đầu tiên hướng tới hướng này trong cộng đồng NAS. NASLib (Ruchte et al., 2020) thống nhất và đơn giản hóa nghiên cứu NAS bằng cách có một trừu tượng duy nhất cho các thuật toán one-shot và BBO, và một trừu tượng duy nhất cho các không gian tìm kiếm của gần như tất cả các điểm chuẩn NAS có thể truy vấn. Archai (Hu et al., 2019) cũng cung cấp các trừu tượng thống nhất cho các thuật toán NAS one-shot và rời rạc. Mục tiêu cho Archai là cả để hỗ trợ nguyên mẫu nhanh có thể tái tạo cho nghiên cứu NAS cũng như là một giải pháp turnkey cho các nhà khoa học dữ liệu tìm cách thử NAS trên các tác vụ của họ. PyGlove (Peng et al., 2020) giới thiệu một phương pháp mới để xây dựng các phương pháp NAS thông qua lập trình symbolic, trong đó các chương trình ML có thể biến đổi và có thể được thao tác và xử lý bởi các chương trình khác.

### 10.2 Các Bài báo Khảo sát NAS Khác

Có một số bài báo khảo sát NAS cũ hơn. Elsken et al. (2019b) cung cấp một giới thiệu nhỏ gọn về NAS và giới thiệu "ba trụ cột" của NAS: không gian tìm kiếm, chiến lược tìm kiếm, và chiến lược đánh giá hiệu suất. Khảo sát của Wistuba et al. (2019) cung cấp một cái nhìn toàn diện hơn về bối cảnh nghiên cứu NAS, thống nhất và phân loại các phương pháp hiện có. Ren et al. (2020) đưa ra một bố cục tập trung vào các thách thức lịch sử trong lĩnh vực NAS, cũng như các giải pháp được tìm thấy để khắc phục những thách thức này.

Các khảo sát khác đã được phát hành tập trung vào một lĩnh vực con cụ thể của NAS. Liu et al. (2021a) tập trung vào NAS tiến hóa, Benmeziane et al. (2021) tập trung vào NAS nhận biết phần cứng (HW-NAS), Zhang et al. (2021b) khảo sát AutoML (với trọng tâm NAS) trên đồ thị, Elsken et al. (2022) khảo sát NAS cho dự đoán dày đặc trong thị giác máy tính, và Xie et al. (2021), Santra et al. (2021), và Cha et al. (2022) đều khảo sát các phương pháp NAS one-shot.

Cuối cùng, có nhiều bài báo khảo sát hơn với trọng tâm rộng hơn như học máy tự động (AutoML) hoặc học sâu tự động (AutoDL), dành một mục cho NAS (Dong et al., 2021a; He et al., 2021; Kedziora et al., 2020; Yao et al., 2018; Yu và Zhu, 2020). Đáng chú ý, cuốn sách đầu tiên về học máy tự động (mã nguồn mở) đã được phát hành vào tháng 5 năm 2019 bởi Hutter et al. (2019).

### 10.3 Tài nguyên Bổ sung

Có nhiều workshop dài hạn tập trung vào NAS và các chủ đề liên quan. Workshop AutoML tại ICML (2014-2021) và workshop Meta-Learning tại NeurIPS (2017-2022) đã có sự chồng chéo tốt trong việc tham dự với cộng đồng NAS, đặc biệt trong vài năm qua, trong khi ICLR (2020, 2021) và CVPR (2021) đã có các workshop dành riêng cho NAS. Cuối cùng, sau nhiều năm workshop AutoML và NAS, cộng đồng đã đủ lớn để bắt đầu hội nghị AutoML đầu tiên: https://automl.cc/.

Để có một danh sách có thể tìm kiếm, được cập nhật liên tục các bài báo NAS, xem https://www.automl.org/automl/literature-on-neural-architecture-search/. Để có một danh sách được cập nhật liên tục các bài báo NAS được xuất bản tại các venue ML, cũng như các tài nguyên khác, xem https://github.com/D-X-Y/Awesome-AutoDL.

## 11. Hướng Tương lai

Tìm kiếm kiến trúc mạng nơ-ron đã đi một chặng đường dài trong vài năm qua. Hiệu quả của các thuật toán NAS đã cải thiện theo thứ tự độ lớn, các công cụ tồn tại để so sánh các thuật toán NAS mà không cần GPU, và các nhà nghiên cứu đã tạo ra nhiều kỹ thuật mới lạ và không gian tìm kiếm đa dạng. Các kiến trúc được khám phá bởi NAS tạo thành tiên tiến trong nhiều tác vụ. Tuy nhiên, vẫn còn nhiều vấn đề chưa được giải quyết và hướng tương lai đầy hứa hẹn. Trong mục này, chúng tôi thảo luận về một vài hướng quan trọng nhất cho công việc tương lai trong NAS.

### 11.1 Tính Mạnh mẽ của Phương pháp Hiệu quả

Các phương pháp one-shot là một trong những kỹ thuật phổ biến nhất cho NAS do tăng tốc theo thứ tự độ lớn so với các kỹ thuật tối ưu hóa hộp đen. Trong khi các kỹ thuật one-shot đã thấy tiến bộ lớn, chúng vẫn đối mặt với các vấn đề hiệu suất.

Mặc dù nhiều cải tiến của các thuật toán one-shot như DARTS đã được đề xuất (xem Mục 4.2), những công việc này thường tập trung vào một cải tiến duy nhất; lĩnh vực thiếu một so sánh quy mô lớn, công bằng giữa các phương pháp one-shot. Hơn nữa, như hiện tại, việc áp dụng các phương pháp one-shot cho một tác vụ mới đòi hỏi một lượng đáng kể chuyên môn. Việc phát triển các phương pháp one-shot hoạt động mạnh mẽ và đáng tin cậy qua các bộ dữ liệu và tác vụ mới là một lĩnh vực quan trọng cho nghiên cứu tương lai.

Một tập hợp kỹ thuật gần đây hơn hứa hẹn tăng tốc theo thứ tự độ lớn là zero-cost proxy (xem Mục 5.1.2). Mặc dù công việc gần đây đã chỉ ra rằng nhiều zero-cost proxy không vượt qua các baseline đơn giản một cách nhất quán (Ning et al., 2021), công việc khác lập luận rằng có tiềm năng chưa được khai thác cho zero-cost proxy (White et al., 2022), đặc biệt khi kết hợp với các kỹ thuật NAS hiện có (White et al., 2021c; Xiang et al., 2021). Việc phát triển một hiểu biết tốt hơn về khi nào và tại sao zero-cost proxy hoạt động trong các thiết lập nhất định là một lĩnh vực quan trọng cho nghiên cứu tương lai.

### 11.2 Vượt ra ngoài Không gian Tìm kiếm Thủ công, Cứng nhắc

Các không gian tìm kiếm cho các phương pháp NAS thường được thiết kế cẩn thận bằng tay bởi các chuyên gia con người. Trong khi việc thiết kế không gian tìm kiếm cẩn thận giảm thời gian tìm kiếm, nó cũng mâu thuẫn với ý tưởng có một hệ thống tự động có thể được sử dụng bởi những người không chuyên, và nó giới hạn phạm vi của NAS cho các miền mà các không gian tìm kiếm mạnh có sẵn. Hơn nữa, trong vài năm qua, loại không gian tìm kiếm được nghiên cứu nhiều nhất cho đến nay là không gian tìm kiếm dựa trên cell, cứng nhắc hơn đáng kể so với các loại không gian tìm kiếm khác.

Các không gian tìm kiếm phân cấp cung cấp một sự đánh đổi tốt hơn giữa tính linh hoạt và dễ tìm kiếm, tuy nhiên chúng tương đối ít được khám phá khi so sánh với các không gian tìm kiếm dựa trên cell (xem Mục 2.5). Hơn nữa, các không gian tìm kiếm phân cấp về bản chất có tính đa dạng cao hơn khi so sánh với các không gian tìm kiếm dựa trên cell, giảm thiên vị tổng thể của con người đối với không gian tìm kiếm.

Tối ưu hóa các không gian tìm kiếm một cách tự động (Ru et al., 2020b) như bắt đầu với các không gian tìm kiếm lớn, đa dạng và sau đó lặp lại tỉa các phần có hiệu suất thấp của không gian (Guo et al., 2020a; Radosavovic et al., 2020) có thể cho phép các nhà nghiên cứu xem xét một loạt các kiến trúc đa dạng hơn đáng kể.

### 11.3 Học Sâu Hoàn toàn Tự động

Mặc dù NAS đã thấy một lượng quan tâm khổng lồ, công việc gần đây đã chỉ ra rằng trên các không gian tìm kiếm phổ biến như không gian tìm kiếm DARTS, tối ưu hóa các siêu tham số huấn luyện dẫn đến sự gia tăng hiệu suất lớn hơn so với tối ưu hóa kiến trúc (Yang et al., 2020; Zela et al., 2020b). Trong khi những kết quả này cho thấy rằng đối với một số không gian tìm kiếm, tối ưu hóa siêu tham số có thể quan trọng hơn so với tối ưu hóa kiến trúc, trường hợp tốt nhất là tối ưu hóa đồng thời cả siêu tham số và kiến trúc.

Một luồng nghiên cứu mới tìm cách đồng thời tối ưu hóa siêu tham số và kiến trúc: NAS + HPO (xem Mục 6.1). Việc thay đổi siêu tham số cùng với kiến trúc cũng giảm đáng kể thiên vị của con người, làm cho việc khám phá ra các kết hợp kiến trúc và siêu tham số chưa từng biết trước đó có thể vượt qua các phương pháp hiện có một cách đáng kể trở nên khả thi. Do đó, trong khi vấn đề này thách thức hơn đáng kể so với chỉ NAS hoặc HPO, những cải tiến tiềm năng cao hơn nhiều.

Hơn nữa, chúng ta không cần dừng lại chỉ ở NAS + HPO: chúng ta có thể tối ưu hóa toàn bộ đường ống học sâu, bao gồm công thức vấn đề, xử lý dữ liệu, data augmentation, triển khai mô hình, và giám sát liên tục. Nói cách khác, mục tiêu là chạy học sâu hoàn toàn tự động (AutoDL) (Dong et al., 2021a). Khi lĩnh vực NAS trưởng thành, AutoDL có tiềm năng đóng vai trò lớn trong việc hiện thực hóa những cải tiến đáng kể về hiệu suất cho các vấn đề thế giới thực.

## Lời Cảm ơn và Tiết lộ Tài trợ

Chúng tôi cảm ơn Difan Deng và Marius Lindauer đã tạo ra và duy trì danh sách tài liệu của họ về tìm kiếm kiến trúc mạng nơ-ron, điều này đã làm cho khảo sát này dễ viết hơn đáng kể (Deng và Lindauer, 2021). Nghiên cứu này được hỗ trợ một phần bởi TAILOR, một dự án được tài trợ bởi chương trình nghiên cứu và đổi mới EU Horizon 2020 dưới GA No 952215. Chúng tôi thừa nhận tài trợ của European Research Council (ERC) Consolidator Grant "Deep Learning 2.0" (grant no. 101045765). Được tài trợ bởi Liên minh Châu Âu. Quan điểm và ý kiến được bày tỏ tuy nhiên chỉ là của (các) tác giả và không nhất thiết phản ánh quan điểm của Liên minh Châu Âu hoặc ERC. Cả Liên minh Châu Âu hay ERC đều không thể chịu trách nhiệm về chúng.
