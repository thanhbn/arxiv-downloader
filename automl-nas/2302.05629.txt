# 2302.05629.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2302.05629.pdf
# File size: 1100350 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Improving Differentiable Architecture Search via
Self-Distillation
Xunyu Zhua,b, Jian Lia,b,∗, Yong Liuc, Weiping Wanga,b
aInstitute of Information Engineering Chinese Academy of Sciences.
bSchool of Cyber Security University of Chinese Academy of Sciences.
cGaoling School of Artificial Intelligence Renmin University of China.
Abstract
Differentiable Architecture Search (DARTS) is a simple yet efficient Neu-
ral Architecture Search (NAS) method. During the search stage, DARTS
trains a supernet by jointly optimizing architecture parameters and network
parameters. During the evaluation stage, DARTS discretizes the supernet to
derive the optimal architecture based on architecture parameters. However,
recent research has shown that during the training process, the supernet
tends to converge towards sharp minima rather than flat minima. This is
evidenced by the higher sharpness of the loss landscape of the supernet, which
ultimately leads to a performance gap between the supernet and the optimal
architecture. In this paper, we propose Self-Distillation Differentiable Neural
Architecture Search (SD-DARTS) to alleviate the discretization gap. We
utilize self-distillation to distill knowledge from previous steps of the supernet
to guide its training in the current step, effectively reducing the sharpness of
the supernet’s loss and bridging the performance gap between the supernet
and the optimal architecture. Furthermore, we introduce the concept of
voting teachers, where multiple previous supernets are selected as teachers,
and their output probabilities are aggregated through voting to obtain the
final teacher prediction. Experimental results on real datasets demonstrate
the advantages of our novel self-distillation-based NAS method compared to
state-of-the-art alternatives.
Keywords: neural architecture search, neural networks, flatness, knowledge
∗Corresponding author
Email addresses: zhuxunyu@iie.ac.cn (Xunyu Zhu), lijian9026@iie.ac.cn (Jian
Li),liuyonggsai@ruc.edu.cn (Yong Liu), wangweiping@iie.ac.cn (Weiping Wang)
Preprint submitted to Neural Networks September 4, 2023arXiv:2302.05629v2  [cs.CV]  1 Sep 2023

--- PAGE 2 ---
distillation, sharpness-aware minimization
1. Introduction
Deep learning has gained significant popularity across various domains due
to its ability to automate feature learning and achieve impressive performance.
It has been successfully applied in areas such as image classification [ 1],
object tracking [ 2], image processing [ 3], and text detection [ 4]. To enhance
the efficiency of deep learning, several outstanding architectures have been
developed by researchers and professors, including VGG [ 5], ResNet [ 6],
Transformer [ 7], Bert [ 8], and GPT [ 9]. These architectures have significantly
contributed to improving the performance of deep learning models.
However, designing high-quality architectures manually can be a laborious
and time-consuming process. The cost of manual architecture design is often
high, requiring extensive expertise and trial-and-error experimentation. As a
result, Neural Architecture Search (NAS) has gained popularity in academia
and industry as it enables machines to automatically discover architectures
with good performance. Initially, NAS approaches such as Reinforcement
Learning [ 10] and Evolutionary Algorithms [ 11,12] are introduced, which
involve sampling and evaluating individual architectures by training them
from scratch. However, these methods face challenges in dealing with the
exponentially increasing search space, resulting in extensive computational
resources being required to find the optimal architecture. NASNet [ 13,
14] introduces a cell-based approach, where individual cells are searched
and then stacked together to form the final network. Another approach,
ENAS [ 15], introduces the concept of a supernet, where a single network
is trained with shared weights that serve as a proxy for evaluating the
performance of individual subnetworks. These methods significantly reduce
the computational cost associated with NAS. However, NAS is still considered
a discrete optimization problem. In response, gradient-based methods such
as SPOS [ 16] and NAO [ 17] are proposed. These methods relaxes the NAS
problem as a continuous optimization problem and utilized gradient-based
optimizers to explore the continuous search space. Among these methods,
DARTS stands out as one of the most successful gradient-based approaches,
offering high search efficiency.
Differentiable Architecture Search (DARTS) revolutionizes NAS by formu-
lating it as a continuous optimization problem and leveraging gradient-based
2

--- PAGE 3 ---
optimization techniques. DARTS achieves low search cost, requiring only
0.3 GPU days to discover high-quality architectures. To further improve
efficiency, PC-DARTS [ 18] introduces partial channel connections and edge
normalization to reduce memory consumption and computational overhead.
SGAS [ 19] introduces sequential greedy architecture search to address the
degenerate search-evaluation correlation problem. During the evaluation stage
of DARTS, the supernet is discretized to determine the optimal architecture.
However, despite consistently reducing the validation error of the supernet
and the optimal architecture, a performance gap remains during evaluation.
Several papers [ 20,21] have discussed this issue and attributed the perfor-
mance gap to the geometry of the supernet’s loss landscape. SDARTS [ 21]
and R-DARTS [ 20] identifiy the sharpness of the validation loss landscape
in DARTS as the cause of the performance gap. SDARTS introduces the
generation of weight perturbations to improve the supernet’s training process,
but this approach incured a significant computational cost. R-DARTS uses
the dominant eigenvalue of the Hessian norm as an indicator, employing early
stopping when the indicator reaches a threshold. However, R-DARTS prevents
DARTS from exploring architectures with potentially better performance.
Alleviating the discretization gap by flattening the supernet’s loss landscape
remains an open question that warrants further exploration.
Our paper introduces a novel method called Self-Distillation DARTS (SD-
DARTS), which is illustrated in Fig. 1. SD-DARTS leverages knowledge
distillation to guide the training of the supernet. Specifically, it transfers the
knowledge learned by the supernet in the previous time step to guide the
training of the current supernet. By enforcing DARTS to consider information
from previous epochs during optimization, SD-DARTS reduces the sharpness
of the supernet’s loss landscape. In contrast to existing approaches, our
method utilizes the information from previous epochs instead of generating
weight perturbations. Additionally, we allow the supernet to be trained until
convergence. These distinguishing factors highlight the novelty and value of
our method in improving the performance of DARTS.
Additionally, we recognize that the information provided by a single
teacher in SD-DARTS may not be sufficient. Ensemble learning suggests that
the predictions of multiple models combined together are often more accurate
than those of a single model, as the collective knowledge from multiple models
is typically more comprehensive. Inspired by ensemble learning, we introduce
the concept of “voting teachers” to guide the training of the supernet. In
practice, we aim to leverage the knowledge from multiple supernets in the
3

--- PAGE 4 ---
previous Ktime steps and aggregate their predictions through voting. By
combining the predictions of these supernets, we generate the final teacher
output probability. This integration of SD-DARTS with voting teachers
enables us to explore and discover high-quality architectures more effectively.
We conduct a series of experiments to evaluate the effectiveness of our
proposed method in improving the performance of DARTS. We utilize our
method to search for high-quality architectures and assess their performance
on various datasets. On the CIFAR-10 dataset, our architecture achieved a
remarkable test error rate of 2.58%, surpassing the performance of DARTS
and lots of its variants. The result highlights the superiority of our approach in
producing architectures with improved performance on this dataset. Further-
more, we evaluated the performance of our architecture on the challenging
ImageNet dataset and achieved a test error rate of 25.0%. These results
reinforce the effectiveness of our novel self-distillation-based NAS method and
demonstrate its advantages over state-of-the-art alternatives. Through these
experiments on real datasets, we provide empirical evidence to support the
efficacy and superiority of our proposed method in generating high-quality
architectures with improved performance.
Our contributions can be summarized as follows:
•We propose SD-DARTS, a new DARTS method with better performance.
It distills the knowledge from the previous steps of the supernet to guide
the training of the supernet in the current step. It can flatten the
supernet’s loss landscape to bridge the performance gap between the
supernet and the optimal subnet.
•We propose voting teachers, a new method by using multiple teachers
to guide the training of the supernet. The predictions of voting teachers
are more accurate than a single teacher. We vote the predications of
supernets in the previous Ktime steps to generate the final teacher
output probability and use the teacher output probability to guide the
training of the supernet itself.
•Extensive experiments demonstrate that our method can effectively
improve the performance of DARTS and achieves good results on main-
stream datasets.
4

--- PAGE 5 ---
. . . . . .Supernet Supernet
Epoch 𝒕−𝟏TeacherTeacher output 
probability 
Student output 
probability Ground-truth 
labels
Epoch  𝒕Warm up supernet=
UpdatingLossFigure 1: The training stage of SD-DARTS involves utilizing the supernet from epoch t−1
as the teacher for epoch t. The correlation between the teacher’s output probability and
the supernet’s output probability is calculated, serving as a regularization term to guide
the supernet’s training.
2. Related Work
2.1. Differentiable Architecture Search
Differentiable Architecture Search (DARTS) has gained popularity as a low-
cost NAS method by formulating NAS as a continuous optimization problem.
Several methods have been proposed to enhance the performance of DARTS.
P-DARTS [ 22] gradually increases the depth of the supernet to address the
discrepancy between architecture depths in search and evaluation scenarios. R-
DARTS [ 20] employes early stopping based on hessian eigenvalues to monitor
changes in the optimization process. SDARTS [ 21] introduces perturbations
on architecture parameters to control hessian eigenvalues and flatten the
supernet’s loss landscape. DARTS+ [ 23] incorporates early stopping when
the number of skip connections in the normal cell exceeds a threshold. DARTS-
PT [24] introduces an alternative perturbation-based architecture selection
method. DAAS [ 25] introduces an entropy-based loss term to guide the super-
network towards the desired topology. Gold-nas [ 26] employes a variable
resource constraint in one-level optimization to gradually prune out weaker
operators. DARTS- [ 27] addes an auxiliary skip connection to ensure fair
competition among all operations. Different from these works, we aim to
alleviate the discretization gap of DARTS by focusing on flattening the
supernet’s loss landscape with less computational cost.
2.2. Self Distillation
Knowledge Distillation (KD) [ 28] is a technique that transfers the knowl-
edge of a well-trained teacher model to a student model during training.
5

--- PAGE 6 ---
Traditionally, a well-trained teacher model with higher capacity than the
student model is used to guide the training process. However, obtaining a
well-trained teacher model can be challenging. To address this issue, recent
works [ 29,30] have explored the use of self-distillation, where the student
model itself is used as a teacher to distill its own knowledge.
Self-distillation has gained popularity in various domains. For instance,
Mean Teacher [ 31] combines self-distillation with semi-supervised learn-
ing to improve performance. It minimizes the L2 distance between the
teacher model’s predictions and the student model’s predictions during semi-
supervised learning, where the teacher model is the student model itself.
Dual-Teacher++ [ 32] introduces a state-of-the-art semi-supervised domain
adaptation framework using dual teacher models. It includes an inter-domain
teacher model that explores cross-modality priors from the source domain
and an intra-domain teacher model that investigates the knowledge within
the unlabeled target domain. LE-UDA [ 33] introduces a self-ensembling con-
sistency approach for knowledge transfer in unsupervised domain adaptation,
combining it with a self-ensembling adversarial learning module to achieve
better feature alignment. Li et al. [ 34] proposes a hierarchical consistency
regularized mean teacher framework for 3D left atrium segmentation, where
the student model is optimized using multi-scale deep supervision and hierar-
chical consistency regularization. ACT-NET [ 35] advances teacher-student
learning with a co-teacher network, facilitating knowledge distillation from
large models to small ones by alternating student and teacher roles. ODC
[36] applies self-distillation to neural machine translation tasks, updating the
teacher model when its performance surpasses the current teacher model on
the validation data. In our paper, we aim to combine self-distillation with
DARTS to enhance the performance of architecture search. By leveraging
the knowledge distilled from the student model itself, we expect to improve
the quality of the generated architectures in DARTS.
2.3. Sharpness Aware Minimisation
Sharpness-aware minimization (SAM) [ 37] is a regularization technique
that aims to flatten a network’s loss landscape. SAM achieves this by intro-
ducing adversarial noise to the network parameters during training, which
encourages the loss to be more stable and less sensitive to small parameter
perturbations. By promoting a flatter loss landscape, SAM can enhance the
generalization ability of the model. ASAM [ 38] builds upon SAM by utilizing
a generalization bound to establish a connection between the sharpness of the
6

--- PAGE 7 ---
loss landscape and the generalization gap. By minimizing the sharpness of
the loss landscape, ASAM aims to reduce overfitting and improve the model’s
ability to generalize to unseen data. Zhao et al. [ 39] proposes to penalize
the gradient norm of the loss function during optimization to flatten the
network’s loss landscape. By regularizing the gradient norm, the optimization
process becomes more stable and less prone to sharp fluctuations, leading to
a flatter loss landscape. SAF [ 40] addresses the issue of sudden drops in loss
that can occur in sharp local minima during the trajectory of weight updates.
SAF aims to avoid these sudden drops by dynamically adjusting the learning
rate based on the sharpness of the loss landscape. By preventing sharp drops
in loss, SAF promotes a flatter optimization process and helps the model
converge to better solutions. These approaches highlight the importance of
considering the flatness of the loss landscape in training deep neural networks.
By promoting a flatter and more stable loss landscape, these techniques aim to
improve the generalization ability and convergence properties of the models.
3. Methodology
3.1. Preliminary
Differentiable Architecture Search (DARTS) [ 41] is a popular Neural
Architecture Search (NAS) method known for its high search efficiency and
low computational cost. Unlike searching for the entire network, DARTS
focuses on searching for cells within the network. Each cell is represented as a
directed acyclic graph (DAG) with multiple nodes, where each node represents
a feature map. The cell has two input nodes, one output node, and several
intermediate nodes. The information from the input nodes is propagated
through the intermediate nodes to the output node using operations associated
with directed edges. To determine the importance of different operations in the
candidate operation space O, DARTS assigns weights to each operation using
architecture parameters α(i,j). The weighted sum of operations is computed
as¯o(i,j)(x) =P
o∈Oexp
α(i,j)
o
P
o′∈Oexp
α(i,j)
o′o(x). The architecture parameters control
the importance of each operation on the corresponding edge.
The search process of DARTS can be formulated as a bilevel optimization
problem, where the goal is to minimize the validation loss Lvalwith respect to
the architecture parameters α, while finding the optimal network parameters
w∗(α) that minimize the training loss Ltrain with respect to w. This is
expressed as:
7

--- PAGE 8 ---
min
αLval(w∗(α), α)
s.t.w∗(α) = arg min
wLtrain(w, α).(1)
After optimizing this bilevel optimization problem, the supernet is dis-
cretized to derive the optimal architecture based on the architecture param-
eters. This is done by selecting the operation with the highest weight for
each edge, i.e., o(i,j)=arg max o∈Oα(i,j)
o. The discretization step allows us to
obtain the final architecture based on the learned architecture parameters,
enabling the deployment of the optimized network architecture.
3.2. Motivation
Recent studies [ 20,21] shows that there is a performance gap between
the supernet and the optimal subnet, and an important factor that results
in discretization gap of Differentiable Architecture Search (DARTS) is the
sharpness of the supernet’s loss landscape. It has been observed that when
the supernet’s loss landscape is flatter, DARTS is more likely to discover
architectures with better performance through the discretization process. Con-
versely, when the supernet’s loss landscape is sharp, the found architectures
have worse performance. However, the existing methods proposed in these
papers have limitations in effectively and efficiently addressing the problem
of sharpness in the supernet’s loss landscape. More research is needed to
develop more effective and efficient techniques to alleviate this issue and
further improve the performance of DARTS.
Several recent papers [ 37,38] have explored the idea of flattening the
loss landscape by attaching adversarial noise to network parameters during
the training stage. Building on this concept, SDARTS [ 21] introduces the
attachment of adversarial noise to architecture parameters during the search
stage to enhance the flatness of the supernet’s loss landscape. However, this
method incurs significant computational cost in generating the adversarial
noise. Another approach introduced by SAF [ 40] suggests that minimizing
the difference in loss between two consecutive iterations is equivalent to
minimizing the sharpness of the loss landscape. Motivated by the concept
of self-distillation, which utilizes the network’s own information to guide its
training, we employ self-distillation as a framework to guide the training of
the supernet. This framework aims to enhance the flatness of the supernet’s
loss landscape.
8

--- PAGE 9 ---
3.3. Self-Distillation DARTS
In our paper, we propose the self-distillation DARTS (SD-DARTS) method,
which utilizes the self-distillation framework to guide the training of the
supernet in DARTS. SD-DARTS aims to improve the performance of the
supernet by attaching self-distillation on the inner-level problem of DARTS’s
bi-level optimization. The formulation of SD-DARTS is as follows:
min
αLval(w∗(α), α) +λH 
fS(x), fT(x)
s.t.w∗(α) = arg min
wLtrain(w, α) +λH 
fS(x), fT(x)
,(2)
In Eq. (2),frepresents the supernet of DARTS, fSandfTare the student
and teacher models, respectively. The student model corresponds to the
supernet at time step t, i.e., fS=ft(x), and the teacher model corresponds to
the student model at the previous time step. The output probabilities of the
student and teacher models are denoted as fS(x) and fT(x), respectively. The
metric His used to calculate the correlation between the output probabilities,
and in our paper, we use the Kullback-Leibler (KL) divergence as the metric.
The regularization coefficient λis used to balance the importance of the two
loss terms. In SD-DARTS, the teacher at the present time step is the supernet
at the previous time step, i.e.,
fT(x) =ft−1(x).
Thus, the formulation in Equation (2) can be simplified as:
min
αLval(w∗(α), α) +λH(ft(x), ft−1(x))
s.t.w∗(α) = arg min
wLtrain(w, α) +λH(ft(x), ft−1(x)).
The detailed process of SD-DARTS is illustrated in Fig. 1. Here we will
introduce SD-DARTS in detail.
Firstly, we start by warming up the supernet f. Both the architecture
parameters αand the network parameters ware randomly initialized. The
goal of this warm-up phase is to allow the supernet to learn valuable knowledge
from the training data. It is important to note that only high-quality supernets
can serve as qualified teachers to guide the training of the supernet itself.
This is because low-quality teachers may provide misleading guidance and
hinder the training process. Therefore, the warm-up phase plays a crucial
role in preparing the supernet for effective self-distillation.
9

--- PAGE 10 ---
After the warm-up phase, we introduce a self-distillation framework to
guide the training of DARTS. At epoch t, we utilize the supernet at epoch
t−1 as the teacher model, denoted as fT, where the previous supernet itself
becomes the teacher. To obtain the teacher output probability, there are two
approaches. The first approach involves storing the parameters of the teacher
model in the GPU memory and generating the teacher output probability by
running the teacher model when needed. However, this approach requires
significant computational resources. To mitigate the computational cost, we
propose an alternative approach. We store the output probability of the
supernet at the previous time step, i.e., fT(x), directly in the memory and
utilize it when needed. In practice, computer memory capacity is typically
large, allowing for the allocation of a small portion of memory to store the stu-
dent output probability. This approach effectively reduces the computational
consumption associated with accessing the teacher output probability.
Next, we calculate the correlation between the teacher output probability,
fT(x), and the student output probability, ft(x), at epoch t, using the chosen
correlation metric H. There are various correlation metrics available, such as
Euclidean Distance (ED), Manhattan Distance (MD), Cosine Distance (CD),
and Kullback-Leibler divergence (KL). Since our task is image classification,
we utilize KL divergence as the correlation metric Hin our paper. KL
divergence is a simple metric used to quantify the differences between two
probability distributions. In our case, we calculate the KL divergence as
follows:
H(fS(x), fT(x)) =X
ifS(xi) logfS(xi)
fT(xi),
where f(xi) denotes the output probability for the i-th example. We use
this correlation metric as a regularization term and incorporate it into the
training of the supernet.
The overall process of SD-DARTS is summarized in Algorithm 1. Similar
to DARTS, our method also uses alternative optimization strategy to optimize
architecture parameters and network parameters. SGD and Adam are used
to train network parameters and architecture parameters, respectively. The
experiment settings in detail are shown in Sec. 4.1. After the training of
the supernet, the optimal architecture is derived from the supernet based on
architecture parameters.
Here, we demonstrate how self-distillation can effectively flatten the loss
10

--- PAGE 11 ---
Algorithm 1 SD-DARTS
Input: supernet f, correlation metric H, total epochs E, warm-up epochs
ξ.
Initialize architecture parameters αand network parameters w.
Warm up the supernet fforξepochs.
fort←(ξ+ 1) to Edo
Select the supernet ft−1at the previous time step (epoch t−1) as the
teacher.
Update αby optimizing min
αLval(w∗(α), α) +λH(ft(x), ft−1(x));
Update wby optimizing min
wLtrain(w, α) +λH(ft(x), ft−1(x));
end for
landscape of an universal neural network fwith its parameters denoted as
αby employing theoretical tools from SAM [ 37] and SAF [ 40]. SAM [ 37]
is introduced to flatten the loss landscape by solving the following minmax
problem:
min
αmax
∥ϵ∥2≤ρL(fα+ϵ).
where L,ρandϵdenote the loss function, allowed perturbation constraint
and perturbation on parameters, respectively. Further, the sharpness loss
R(fα) can be represented as L(fα+ˆϵ)− L(fα), where ˆϵdenotes the optimal
perturbation on parameters, and ˆϵis calculated by ρ∇αL(fα)⊤
∥∇αL(fα)∥2[37]. We use a
first-order Taylor expansion to decompose the sharpness loss as follows:
R(fα) =L(fα+ˆϵ)− L(fα)≈ L(fα) + ˆϵ∇αL(fα)− L(fα)
=ρ∇αL(fα)⊤
∥∇αL(fα)∥2∇αL(fα) =ρ∥∇αL(fα)∥2, (3)
Eq.(3)shows that minimizing the sharpness loss is equivalent to minimizing
theℓ2-norm of the gradient ∇αL(fα).
On the other hand, motivated by SAF [ 40], the change of the network’s
validation loss in the two consecutive iterations can be represented as
L(fα)− L(fα−λ∇αL(fα))≈λ∥∇αL(fα)∥2
2≈λ
ρ2R(fα)2. (4)
where λdenotes as the learning rate in the training of parameters. SAM
[37] shows that the learning rate λin the training of parameters is typically
11

--- PAGE 12 ---
0 10 20 30 40 50
Epoch0.00.10.20.30.4Dominant Eigenvalue of  Hessian Normkdarts
dartsFigure 2: Trajectory of the supernet’s Hessian norm on standard DARTS and SD-DARTS.
Dominant Eigenvalue of the supernet’s Hessian norm on SD-DARTS is smaller than
standard DARTS, and the result shows that the loss landscape of SD-DARTS is flatter
than DARTS.
smaller than ρ. Based on Eq. (4), we find that the change of the loss is
proportional to R(fα)2. Hence, minimizing the loss difference is equal to
minimizing the sharpness of the network in this case. Our analysis can be
easily generalized to DARTS. Fig. 2 also shows that dominant eigenvalue of
the supernet’s Hessian norm on SD-DARTS is smaller than standard DARTS,
i.e., the loss landscape of SD-DARTS is flatter than DARTS.
Remark 1. In our method, we utilize self-distillation (SD) as a framework
to train the supernet. To assess the sharpness of the validation loss landscape,
we compute the dominant eigenvalue of the supernet’s Hessian norm. Fig. 2
demonstrates that the dominant eigenvalue of the supernet’s Hessian norm in
SD-DARTS is smaller than that in standard DARTS. This indicates that the
loss landscape of SD-DARTS is flatter compared to DARTS. The flatter loss
landscape in SD-DARTS suggests that our method is effective in reducing the
sharpness and improving the stability of the supernet’s training process.
12

--- PAGE 13 ---
… Supernet Supernet Supernet Supernet … …Voting
Teacher output 
probability
Teacher Teacher Teacher …Teacher output 
probabilityTeacher output 
probabilityTeacher output 
probability
Student output 
probability
Epoch 𝒕−𝟏 Epoch 𝒕−𝑲 Epoch 𝒕−𝑲+𝟏 Epoch 𝒕Figure 3: Voting teachers. We utilize supernets at previous Ktime steps as teachers and
then vote teacher output probabilities outputted by supernet to generate the final teacher
output probability.
3.4. Voting Teachers
In SD-DARTS, we utilize the supernet from the previous time step as
the teacher to guide the training process. However, we recognize that the
information provided by a single teacher may be limited. To overcome this
limitation, we propose to utilize multiple teachers in order to obtain a more
diverse and rich source of information. Each teacher contributes its own
expertise and experience to guide the training of the supernet. To make
predictions, we employ a voting scheme where the output probabilities of
each teacher are combined to generate the final teacher prediction. This
approach of utilizing multiple teachers allows us to leverage a broader range of
knowledge and increase the diversity of guidance during the training process.
By aggregating the insights from multiple teachers, we can potentially achieve
better performance and more robust architectures in SD-DARTS.
In our proposed method, known as SD-DARTS, we introduce a concept
called “voting teachers” to guide the training of the supernet. Inspired by
ensemble learning, which demonstrates that the performance of multiple
models is often better than that of a single model, we leverage the idea of
combining multiple teachers to enhance the training process. In the context
of SD-DARTS, we consider a time window of size K, where we select multiple
supernets from the previous Ktime steps as teachers. The output probabilities
of these teacher supernets are then averaged to generate the final teacher
13

--- PAGE 14 ---
Algorithm 2 Voting Teachers
Input: supernet f, correlation metric H, total epochs E, warm-up epochs
ξ, time window K.
Initialize architecture parameters αand network parameters w.
Warm up the supernet fforξepochs.
fort←(ξ+ 1) to Edo
Select the supernets at the previous Ktime steps (from epoch t−Kto
epoch t−1) as the teachers.
Update αby optimizing min
αLval(w∗(α), α) +λH
f(x),1
KKP
i=1ft−i(x)
;
Update wby optimizing min
wLtrain(w, α) +λH
f(x),1
KKP
i=1ft−i(x)
;
end for
output probability. Mathematically, the final teacher output probability can
be represented as follows:
fT(x) =1
KKX
i=1ft−i(x),
where ft−i(x) denotes the output probability of the supernet at time step
t−i. By combining the predictions from multiple teachers, we aim to benefit
from their collective knowledge and insights, leading to improved guidance
in the training process of the supernet. This approach allows us to leverage
the strengths of different teacher supernets and potentially achieve better
performance and more robust architectures in SD-DARTS.
In our proposed method, which combines voting teachers with SD-DARTS,
we aim to leverage the knowledge and insights from multiple teacher supernets
to guide the search for architectures. The objective function in this case is
formulated as follows:
min
αLval(w∗(α), α) +λH 
f(x),1
KKX
i=1ft−i(x)!
s.t.w∗(α) = arg min
wLtrain(w, α) +λH
f(x),1
KKP
i=1ft−i(x)
,
where trepresents the present time step, λrepresents the regularization
coefficient, Kis the time window that controls the number of teachers,
14

--- PAGE 15 ---
and1
KKP
i=1ft−i(x) denotes the final teacher output probability obtained by
averaging the output probabilities of the multiple teacher supernets. The
regularization term H
f(x),1
KKP
i=1ft−i(x)
encourages consistency between
the supernet’s output probability and the averaged output probabilities of
the voting teachers. To implement the voting teachers approach, we provide
an algorithmic description in Algorithm 2. This algorithm outlines the steps
for selecting the teacher supernets from the previous time steps, averaging
their output probabilities, and using the averaged output probability as the
final teacher output probability to guide the training of the supernet. By
incorporating voting teachers into SD-DARTS, we aim to take advantage
of the collective knowledge and insights from multiple teacher supernets,
potentially leading to improved performance and more robust architectures
during the search process.
By utilizing voting teachers in combination with SD-DARTS, we can
leverage the collective knowledge of multiple teacher supernets to guide the
search for architectures. This approach allows for more diverse and informed
guidance during the training of the supernet, potentially leading to better
performance and improved architectures. The additional memory required
to store the output probabilities of the teacher supernets is relatively small
compared to the overall memory requirements of the training process. This
allows us to leverage the rich information from multiple teachers without
significantly increasing the computational or memory overhead.
Remark 2. 1.Ensemble learning has shown that combining the knowledge
of multiple models can lead to improved performance compared to a single
model. By selecting multiple supernets from the previous time steps as
teachers, we can leverage the diverse knowledge and expertise of these
models to guide the training of the supernet. The use of multiple teachers
helps to capture a broader range of information and insights, leading to
a more comprehensive and robust training process. It allows the supernet
to benefit from the collective wisdom of multiple architectures, increasing
the chances of finding high-quality architectures.
2.Our method differs from Mean Teacher [ 31] in the approach of averaging.
While Mean Teacher focuses on averaging the weights of the network, our
method aims to average the output of the network. We have two reasons
for choosing the voting teachers approach over Mean Teacher. Firstly,
15

--- PAGE 16 ---
the parameter scale of the network’s output is significantly smaller
than the weights, and computational and memory overhead will become
bigger when we average weights instead of outputs. Because weights and
outputs are very closely linked, by using the average of the output to
guide our self-distillation training, our method still has an impact on
the weights. It allows us to leverage the benefits of the output averaging
while considering the relative importance of the weights. Secondly, the
coupling between the architecture parameters and network parameters
in our model, due to alternative optimization strategies, can lead to
instability during the training process when using teachers generated by
averaging the weights. The instability can hinder convergence and affect
the overall performance of the model. By considering these factors, we
have determined that the voting teachers approach is better suited to
our specific objectives and model characteristics. The decision allows
us to effectively leverage the advantages of averaging while mitigating
potential instabilities in the training process.
4. Experiment
In the sections, we carry out extensive experiments to verify the effec-
tiveness of our method. Our experiments are conducted on 3 ×NVIDIA
GeForce GTX 3090 GPUs. CIFAR-10 [ 42] and ImageNet [ 1] are two image
classification datasets which our experiments are conducted on. We search
for the optimal architecture by SD-DARTS on CIFAR-10 and then evaluate
the architecture on CIFAR-10 and ImageNet.
4.1. Architecture Search
CIFAR-10 is the dataset that we search for the optimal architecture by
SD-DARTS. There are 50K training images and 10K test images in CIFAR-10,
c_{k-2}
0sep_conv_3x3
1skip_connect2dil_conv_3x3
3skip_connect
c_{k-1}sep_conv_3x3
sep_conv_3x3sep_conv_3x3
dil_conv_5x5
c_{k}
c_{k-2}0max_pool_3x3
1 max_pool_3x3
2max_pool_3x3
3avg_pool_3x3c_{k-1}sep_conv_5x5 skip_connect
skip_connect
c_{k}sep_conv_3x3
(a) Normal cell (b) Reduction cell
Figure 4: Cells are searched by SD-DARTS when epochs for warm-up ξare 25 and time
window Kis 2.
16

--- PAGE 17 ---
and each image’s spatial resolution is 32 ×32. These images are divided
into 10 classes. During the search stage of SD-DARTS, training images
are split into two subsets, i.e., the valid subset for optimizing architecture
parameters and the training subset for optimizing network parameters. After
SD-DARTS searches for the optimal architecture, training images in CIFAR-10
are used to train the optimal architecture and then evaluate the architecture’s
performance on test images.
The search space of our method is the same as DARTS, and the search
space includes 8 candidate operations, skip connect ,max pool 3×3,avgpool 3×
3,sepconv 3×3,sepconv 5×5,dilconv 3×3,dilconv 5×5,zero. The
supernet is stacked by 6 normal cells and 2 reduction cells, which the reduction
cells are inserted in1
3and2
3of the total depth of the supernet, respectively.
The stride of convolution in the reduction cell is 2; thus, the reduction cell
can reduce the spatial resolution of feature maps. However, the stride of
convolution in the normal cell is 1, i.e., the spatial size of the feature map is
not reduced. Our method aims to search for one normal cell and one reduction
cell to build the optimal architecture.
Table 1: Hyperparameter Settings of SD-DARTS.
Hyperparameter Value Meaning
ξ 25 epochs for warm-up
λ 1.0 regularization coefficient
K 2 time window
E 50 total epochs for search
m 64 batch size for search
The train setting of our method is also the same as DARTS. The total
epochs Eof training the supernet are 50, and the batch size of the train is
64. The initial channel size of the supernet is 16. SGD is used to optimize
network parameters wwith an initial learning rate 0.025, momentum 0.9 and
weight decay 3 ×10−4. Adam is used to optimize architecture parameters with
an initial learning rate 3 ×10−4, momentum (0.5, 0.999) and weight decay
10−3. We warm up the supernet for 25 epochs, i.e., the epoch for warm-up
ξis 25. Then, time window Kis set as 2, and it means that we will select
supernets at the two previous time steps as teachers to guide the training of
the supernet. Furthermore, we empirically set the regularization coefficient λ
as 1.0 to balance classification loss and correlation loss. Table 1 shows these
hyperparameter settings of our method.
17

--- PAGE 18 ---
Table 2: Comparison with state-of-the-art image classifiers on CIFAR-10.
ArchitectureTest Err.
(%)Params
(M)Search Cost
(GPU-days)Search
Method
DenseNet-BC [43] 3.46 25.6 - manual
NASNet-A [13] 2.65 3.3 1800 RL
AmoebaNet-A [11] 3.34 ±0.06 3.2 3150 evolution
AmoebaNet-B [11] 2.55 ±0.05 2.8 3150 evolution
PNAS [44] 3.41 ±0.09 3.2 225 SMBO
ENAS [15] 2.89 4.6 0.5 RL
DARTS (1storder) [41] 3.00 ±0.14 3.3 0.4 gradient
DARTS (2ndorder) [41] 2.76 ±0.09 3.3 1 gradient
SNAS (mild) [45] 2.98 2.9 1.5 gradient
ProxylessNAS [46] 2.08 - 4 gradient
P-DARTS [22] 2.5 3.4 0.3 gradient
PC-DARTS [18] 2.57 ±0.07 3.6 0.1 gradient
β-DARTS [47] 2.53 ±0.08 3.8 0.4 gradient
SDARTS-RS [21] 2.67 ±0.03 3.4 0.4 gradient
SDARTS-ADV [21] 2.61 ±0.02 3.3 1.3 gradient
GDAS [48] 2.93 3.4 0.3 gradient
R-DARTS (L2) [20] 2.95 ±0.21 - 1.6 gradient
SGAS (Cri 1. avg) [19] 2.66 ±0.24 3.7 0.25 gradient
DARTS-PT [24] 2.61 ±0.08 3.0 0.8 gradient
DARTS-[27] 2.59 ±0.08 3.5 0.4 gradient
U-DARTS [49] 2.59 ±0.06 3.3 4 gradient
DARTS-PAP [50] 2.51 3.9 0.4 gradient
SD-DARTS 2.58 ±0.11 3.3 0.37 gradient
SD-DARTS (best) 2.44 3.3 0.37 gradient
4.2. Architecture Evaluation on CIFAR-10
Fig. 4 shows cells searched by SD-DARTS, and we stack these cells to
build the optimal architecture. In the subsection, we evaluate the performance
of the optimal architecture on CIFAR-10. The training setting of our method
is also the same as DARTS. We stack 20 cells to build a network with an
initial channel size of 36, where two reduction cells are put into1
3and2
3of
the total depth of the network. We train the network for 600 epochs with
batch size 96. SGD is utilized to optimize the network with an initial learning
rate of 0.025. Furthermore, we also use data augmentation to regularize the
network’s training, including cutout and auxiliary towers; the cutout length is
16, the weight of auxiliary towers is 0.4, and the probability of path dropout
is 0.3.
Table 2 shows the performance of the architecture searched by our method
on CIFAR-10. The architecture searched by our method achieves a 2.58%
mean test error on CIFAR-10, and it can achieve a 2.44% best test error
when setting the appropriate random seed to train the architecture. The
18

--- PAGE 19 ---
performance of our method is beyond DARTS and its variants (e.g., DARTS-
PT, SGAS, GDAS) a lot. In other words, our method achieves state-of-the-art
(SOTA) performance. The magnitude of our architecture’s parameters is
3.3M, and it is equal to the architecture searched by DARTS and less than
other variants.
Furthermore, the search cost of our method is 0.37 GPU days, and it costs
a few less GPU days than DARTS. In a word, with limited resources, our
method can find the architecture with SOTA performance.
Table 3: Comparison with state-of-the-art classifiers on ImageNet.
ArchitectureTest Err.(%) Params
(M)×+
(M)Search Cost
(GPU-days)Search
Method top-1 top-5
Inception-v1 [51] 30.2 10.1 6.6 1448 - manual
MobileNet [52] 29.4 10.5 4.2 569 - manual
ShuffleNet 2x (v1) [53] 26.4 10.2 ∼5 524 - manual
ShuffleNet 2x (v2) [54] 25.1 - ∼5 591 - manual
NASNet-A [13] 26 8.4 5.3 564 1800 RL
NASNet-B [13] 27.2 8.7 5.3 488 1800 RL
NASNet-C [13] 27.5 9 4.9 558 1800 RL
AKD [55] 27.9 8.3 - - - RL
AmoebaNet-A [11] 25.5 8 5.1 555 3150 evolution
AmoebaNet-B [11] 26 8.5 5.3 555 3150 evolution
AmoebaNet-C [11] 24.3 7.6 6.4 570 3150 evolution
FairNAS-A [56] 24.7 7.6 4.6 388 12 evolution
PNAS [44] 25.8 8.1 5.1 588 225 SMBO
MnasNet-92 [57] 25.2 8 4.4 388 - RL
DARTS(2ndorder) [41] 26.7 8.7 4.7 574 4.0 gradient
SNAS (mild) [45] 27.3 9.2 4.3 522 1.5 gradient
ProxylessNAS [46] 24.9 7.5 7.1 465 8.3 gradient
P-DARTS [22] 24.4 7.4 4.9 557 0.3 gradient
PC-DARTS [18] 25.1 7.8 5.3 586 0.1 gradient
β-DARTS [47] 24.2 7.1 5.5 609 0.4 gradient
SGAS (Cri.1 avg.) [19] 24.41 7.29 5.3 579 0.25 gradient
GDAS [48] 26.0 8.5 5.3 581 0.21 gradient
SDARTS-RS [21] 25.6 8.2 - - - gradient
SDARTS-ADV [21] 25.2 7.8 - - - gradient
DARTS-PT [24] 25.5 8 4.6 - 0.8 gradient
U-DARTS [49] 26.1 8.1 4.9 582 3 gradient
DARTS-PAP [50] 25.41 8.04 5.4 629 0.4 gradient
SD-DARTS 25.0 7.62 4.7 534 0.37 gradient
4.3. Architecture Evaluation on ImageNet
In the subsection, we utilize ILSVRC2012 [ 58] to evaluate the transferabil-
ity of our method. ILSVRC2012 is a lightweight version of ImageNet with
19

--- PAGE 20 ---
1,000 classes. It contains 1.28M train images and 50K valid images, and each
image’s spatial resolution is 224 ×224.
Fig. 4 shows the cells searched by our method on CIFAR-10. We stack the
cells to build a network and evaluate the network’s performance on ImageNet.
The train setting of our method on ImageNet is the same as DARTS. We
utilize 12 normal cells and 2 reduction cells to build the network, and the
reduction cells are inserted into1
3and2
3of the total depth of the network.
The initial channel size of the network is 48. We train the network for 250
epochs with batch size 1024 on 3 ×NVIDIA GeForce GTX 3090 GPUs. We
utilize SGD to optimize the network with an initial learning rate of 0.5, a
momentum of 0.9, and a weight decay of 3 ×10−5. Furthermore, we utilize
an auxiliary loss tower to help train our network with an auxiliary weight of
0.4. We spend three GPU days training the network on 3 ×NVIDIA GeForce
GTX 3090 GPUs.
Table 3 demonstrates the evaluation results of our network on the Ima-
geNet dataset, where we achieve a test error rate of 25.0%. This performance
indicates that our method’s transferability is superior to DARTS and some of
its variants. It is worth noting that certain DARTS variants exhibit lower
test errors than our method. However, our architecture has fewer parameters
compared to most of its variants. While there may be baseline methods with
better results, it is important to consider that their goals might differ from
ours. For instance, methods like SGAS [ 19] and P-DARTS [ 22] address weight
sharing and the depth gap between search and evaluation scenarios, respec-
tively. In contrast, our focus is on flattening the supernet’s loss landscape.
Furthermore, our method outperforms our concurrent works in terms of both
performance and search cost efficiency. These results demonstrate the value
and efficiency of our approach.
While our experimental results on the ImageNet dataset may not be as
strong as desired, we believe that they still provide valuable insights into the
performance of our method. Prior works such as SDARTS [ 21] and SAM [ 37]
have also demonstrated that the dominant eigenvalue of the Hessian norm
is a reliable indicator for characterizing the sharpness of the loss landscape.
The observation that the dominant eigenvalue of the supernet’s Hessian norm
in our method is smaller than in standard DARTS, as well as the analysis
showing that self-distillation can flatten the loss landscape, provide strong
evidence that our proposed algorithm effectively reduces the discretization gap.
Combining these pieces of evidence, we can confidently assert that our method
successfully mitigates the discretization gap and flattens the supernet’s loss
20

--- PAGE 21 ---
landscape. Prior works such as R-DARTS [ 20] and SDARTS [ 21] have shown
that a high level of sharpness in the supernet’s loss landscape can have a
detrimental effect on the performance of DARTS. These works have proposed
techniques like early stopping and parameter perturbation to address this
issue. However, early stopping may limit the exploration of architectures
with better performance, and generating parameter perturbations can be
time-consuming. In comparison, our method is more efficient as we only need
to preserve the logits from previous training steps. This approach allows us to
take advantage of the knowledge accumulated in earlier steps without the need
for expensive parameter perturbations. By leveraging self-distillation and
voting teachers, our method effectively flatten the supernet’s loss landscape
and achieves better performance in a more efficient manner compared to prior
works such as SDARTS.
Table 4: Performance comparison on NAS-Bench-201 benchmark [ 59]. Note that SD-
DARTS only searches on CIFAR-10 dataset, but can achieve new SOTA on CIFAR-10,
CIFAR-100 and ImageNet16-120.
MethodsCIFAR-10 CIFAR-100 ImageNet16-120
valid test valid test valid test
DARTS(1st) [41] 39.77 ±0.00 54.30 ±0.00 15.03 ±0.00 15.61 ±0.00 16.43 ±0.00 16.32 ±0.00
DARTS(2nd) [41] 39.77 ±0.00 54.30 ±0.00 15.03 ±0.00 15.61 ±0.00 16.43 ±0.00 16.32 ±0.00
GDAS [48] 89.89 ±0.08 93.61 ±0.09 71.34 ±0.04 70.70 ±0.30 41.59 ±1.33 41.71 ±0.98
SNAS [45] 90.10 ±1.04 92.77 ±0.83 69.69 ±2.39 69.34 ±1.98 42.84 ±1.79 43.16 ±2.64
DSNAS [60] 89.66 ±0.29 93.08 ±0.13 30.87 ±16.40 31.01 ±16.38 40.61 ±0.09 41.07 ±0.09
PC-DARTS [18] 89.96 ±0.15 93.41 ±0.30 67.12 ±0.39 67.48 ±0.89 40.83 ±0.08 41.31 ±0.22
iDARTS [61] 89.86 ±0.60 93.58 ±0.32 70.57 ±0.24 70.83 ±0.48 40.38 ±0.59 40.89 ±0.68
DARTS- [62] 91.03 ±0.44 93.80 ±0.40 71.36±1.51 71.53±1.51 44.87 ±1.46 45.12 ±0.82
SD-DARTS 91.21 ±0.11 93.91 ±0.08 70.87±0.33 71.88±0.76 45.75 ±0.26 46.03 ±0.59
4.4. Results on NAS-Bench-201 Search Space
NAS-Bench-201 [ 59] is the most widely used NAS benchmark. NAS-
Bench-201 contains 4 internal nodes with 5 candidate operations. The search
space includes 15,625 architectures, and the ground truth performance of
each architecture on CIFAR-10, CIFAR-100 and ImageNet16-120 is provided.
The searching settings of our method are the same as DARTS on on NAS-
Bench-201.
Table 4 shows comparison results on NAS-Bench-201. We search on
CIFAR-10 and use the found genotype to query the performance of various
datasets. It is observed that our method achieves competitive results on
CIFAR-10, CIFAR-100, and ImageNet16-120 when compared to other NAS
21

--- PAGE 22 ---
methods. Although the CIFAR-100-valid performance of our method is
slightly lower than DARTS- [ 27], it outperforms other methods such as PC-
DARTS [ 18] and iDARTS [ 61] in terms of overall performance. These results
indicate that SD-DARTS has the capability to discover architectures with
competitive performance across different datasets. While it may not achieve
the absolute state-of-the-art on every individual dataset, it offers a strong
trade-off between search efficiency and performance. These results highlight
the value of our method in finding high-quality architectures for a wide range
of applications.
0 5 10 15 20 25 30 35 40
Epochs of Warm-up96.296.496.696.897.097.297.4Accuracy
Figure 5: The effect of warm-up : accuracy of the architectures discovered by SD-DARTS in
different warm-up epochs on CIFAR-10. The accuracy grows gradually when the warm-up
epochs increase until 25.
5. Ablation Study
5.1. The Effect of Warm-up
Because architecture and network parameters are randomly initialized,
we need to warm up the supernet. After the warm-up, the supernet learns
enough knowledge from train data to become a qualified teacher. In the
subsection, we make an ablation study about the importance of warm-up.
22

--- PAGE 23 ---
We set time window Kas 1 in the experiment, i.e., we utilize the supernet
at the previous time step as the teacher to guide the training of the supernet.
Our aim to choose a single teacher is to neglect the effect of multiple teachers.
We run our method several times with different epochs for warm-up. Then,
we evaluate the architecture’s performance discovered by our experiments.
Fig. 5 shows the tendency of the accuracy of the architectures discovered
by SD-DARTS in the different warm-ups. We find that the accuracy grows
gradually when the warm-up epochs increase until 25. When the warm-up
epochs are insufficient, the supernet cannot learn enough knowledge to become
a qualified teacher. It will misguide the training of supernet and generate
architecture with poor performance. When the warm-up epochs increase from
25 epochs, the accuracy decreases gradually. This is because the supernet
is over-optimized as the training epochs increase, and the supernet plays a
negative role when it is taken for a teacher. Thus, warm-up plays an essential
role in our method, and we are supposed to warm up the supernet for proper
epochs.
5.2. The Effect of Voting Teachers
In this subsection, the effect of voting teachers is analyzed by considering
different time windows K, where Krepresents the number of voting teachers.
The warm-up epochs are set to 25, and the optimal architecture is searched
using several supernets from the previous Ktime steps as teachers to guide the
self-distillation training of the supernet. The performance of these architecture
is then evaluated. The purpose of this analysis is to understand the impact
of the number of voting teachers on the performance of the search process.
By varying the value of K, different amounts of knowledge and information
from previous supernets are integrated to guide the training. This allows for
a comprehensive exploration of the effect of using multiple teachers in the
self-distillation process.
Fig. 6 illustrates the tendency of accuracy in different time windows when
using the voting teachers method. It shows that the performance of the
searched architectures varies with the size of the time window. Interestingly,
the best performance is achieved when the time window is set to 2, indicating
that utilizing the supernets from the previous two time steps as the teachers
provides the most effective guidance for the self-distillation training of the
supernet. As the size of the time window increases beyond 2, the performance
of the architectures starts to deteriorate. This can be attributed to the fact
that the information from supernets in earlier time steps becomes less relevant
23

--- PAGE 24 ---
1 2 3 4 5 6
The Size of Time Window97.197.297.397.497.5Accuracy
Figure 6: The effect of voting teachers : accuracy of the architectures discovered by
SD-DARTS in different time windows on CIFAR-10 when the epochs of warm-up are 25.
When the size of time window is 2, it can achieve the highest accuracy. However, as the
size of time window gets larger, the accuracy declines slowly.
and correlated with the supernet at the present time step. Consequently,
using supernets from these earlier time steps as voting teachers may introduce
misleading information and hinder the effectiveness of the self-distillation
training. This observation highlights the trade-off between the number of
voting teachers and the correlation between the teachers and the student.
Future research can focus on finding a balance between these factors to
fully leverage the information from supernets at different time steps and
optimize the guidance provided by the voting teachers in the self-distillation
process. This would allow for a more effective and accurate search for optimal
architectures.
6. Conclusion
In summary, the proposed SD-DARTS method, which combines self-
distillation and voting teachers, aims to improve the performance of DARTS
by addressing the sharpness of the loss landscape. It achieves state-of-
the-art performance on CIFAR-10 and ImageNet datasets by reducing the
24

--- PAGE 25 ---
discretization gap between the supernet and the optimal subnet. While SD-
DARTS demonstrates promising results, it also has limitations regarding the
utilization of information from supernets at different time steps to guide the
training of the supernet. In future research, it would be valuable to explore
alternative approaches, such as using a decayed exponential average, to strike
a better balance between the number of voting teachers and the correlation
between the teachers and the student. This could potentially enhance the
performance and effectiveness of SD-DARTS even further.
Acknowledgments
The work of Jian Li is supported partially by Natural Science Founda-
tion of China (No. 62106257), China Postdoctoral Science Foundation (No.
2023T160680), and Excellent Talents Program of Institute of Information
Engineering, CAS. The work of Yong Liu is supported partially by Natural
Science Foundation of China (No. 62076234), Beijing Outstanding Young Sci-
entist Program (No. BJJWZYJH012019100020098), the Unicom Innovation
Ecological Cooperation Plan, and the CCF-Huawei Populus Grove Fund.
References
[1]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
large-scale hierarchical image database, in: IEEE conference on computer
vision and pattern recognition (CVPR), 2009, pp. 248–255.
[2]A. Yilmaz, O. Javed, M. Shah, Object tracking: A survey, Acm comput-
ing surveys (CSUR) 38 (4) (2006) 13–es.
[3]M. van Heel, G. Harauz, E. V. Orlova, R. Schmidt, M. Schatz, A new
generation of the imagic image processing system, Journal of structural
biology 116 (1) (1996) 17–24.
[4]H. Li, D. Doermann, O. Kia, Automatic text detection and tracking
in digital video, IEEE transactions on image processing 9 (1) (2000)
147–156.
[5]K. Simonyan, A. Zisserman, Very deep convolutional networks for large-
scale image recognition, in: Y. Bengio, Y. LeCun (Eds.), International
Conference on Learning Representations (ICLR), 2015.
25

--- PAGE 26 ---
[6]K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image
recognition, in: Proceedings of the IEEE conference on computer vision
and pattern recognition (CVPR), 2016, pp. 770–778.
[7]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
 L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural
information processing systems (NeurIPS) 30 (2017).
[8]J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep
bidirectional transformers for language understanding, in: J. Burstein,
C. Doran, T. Solorio (Eds.), Proceedings of the Conference of the North
American Chapter of the Association for Computational Linguistics
(NAACL), 2019, pp. 4171–4186.
[9]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models
are few-shot learners, Advances in neural information processing systems
(NeurIPS) 33 (2020) 1877–1901.
[10]B. Zoph, Q. V. Le, Neural architecture search with reinforcement learning,
in: International Conference on Learning Representations (ICLR), 2017.
[11]E. Real, A. Aggarwal, Y. Huang, Q. V. Le, Regularized evolution for
image classifier architecture search, in: Proceedings of the aaai conference
on artificial intelligence (AAAI), Vol. 33, 2019, pp. 4780–4789.
[12]K. Ostad-Ali-Askari, M. Shayan, Subsurface drain spacing in the unsteady
conditions by hydrus-3d and artificial neural networks, Arabian Journal
of Geosciences 14 (2021) 1–14.
[13]B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le, Learning transferable
architectures for scalable image recognition, in: Proceedings of the IEEE
conference on computer vision and pattern recognition (CVPR), 2018,
pp. 8697–8710.
[14]K. Ostad-Ali-Askari, M. Shayannejad, H. Ghorbanizadeh-Kharazi, Arti-
ficial neural network for modeling nitrate pollution of groundwater in
marginal area of zayandeh-rood river, isfahan, iran, KSCE Journal of
Civil Engineering 21 (2017) 134–140.
26

--- PAGE 27 ---
[15]H. Pham, M. Guan, B. Zoph, Q. Le, J. Dean, Efficient neural architecture
search via parameters sharing, in: International conference on machine
learning (ICML), 2018, pp. 4095–4104.
[16]Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, J. Sun, Single path
one-shot neural architecture search with uniform sampling, in: A. Vedaldi,
H. Bischof, T. Brox, J. Frahm (Eds.), European Conference on Computer
Vision (ECCV), Vol. 12361, 2020, pp. 544–560.
[17]R. Luo, F. Tian, T. Qin, E. Chen, T.-Y. Liu, Neural architecture opti-
mization, Advances in neural information processing systems (NeurIPS)
31 (2018).
[18]Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, H. Xiong, PC-DARTS:
partial channel connections for memory-efficient architecture search, in:
International Conference on Learning Representations (ICLR), 2020.
[19]G. Li, G. Qian, I. C. Delgadillo, M. Muller, A. Thabet, B. Ghanem, Sgas:
Sequential greedy architecture search, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2020,
pp. 1620–1630.
[20]A. Zela, T. Elsken, T. Saikia, Y. Marrakchi, T. Brox, F. Hutter, Under-
standing and robustifying differentiable architecture search, in: Interna-
tional Conference on Learning Representations (ICLR), 2020.
[21]X. Chen, C.-J. Hsieh, Stabilizing differentiable architecture search via
perturbation-based regularization, in: International conference on ma-
chine learning (ICML), 2020, pp. 1554–1565.
[22]X. Chen, L. Xie, J. Wu, Q. Tian, Progressive DARTS: bridging the
optimization gap for NAS in the wild, Int. J. Comput. Vis. 129 (3) (2021)
638–655.
[23]H. Liang, S. Zhang, J. Sun, X. He, W. Huang, K. Zhuang, Z. Li, DARTS+:
improved differentiable architecture search with early stopping, CoRR
abs/1909.06035 (2019).
[24]R. Wang, M. Cheng, X. Chen, X. Tang, C. Hsieh, Rethinking architecture
selection in differentiable NAS, in: International Conference on Learning
Representations (ICLR), 2021.
27

--- PAGE 28 ---
[25]Y. Tian, C. Liu, L. Xie, Q. Ye, et al., Discretization-aware architecture
search, Pattern Recognition 120 (2021) 108186.
[26]K. Bi, L. Xie, X. Chen, L. Wei, Q. Tian, GOLD-NAS: gradual, one-level,
differentiable, CoRR abs/2007.03331 (2020).
[27]X. Chu, X. Wang, B. Zhang, S. Lu, X. Wei, J. Yan, DARTS-: robustly
stepping out of performance collapse without indicators, in: International
Conference on Learning Representations (ICLR), 2021.
[28]Y. Lin, C. Wang, C. Chang, H. Sun, An efficient framework for counting
pedestrians crossing a line using low-cost devices: the benefits of distilling
the knowledge in a neural network, Multim. Tools Appl. 80 (3) (2021)
4037–4051.
[29]L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, K. Ma, Be your own
teacher: Improve the performance of convolutional neural networks
via self distillation, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), 2019, pp. 3713–3722.
[30]S. Yun, J. Park, K. Lee, J. Shin, Regularizing class-wise predictions via
self-knowledge distillation, in: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition (CVPR), 2020, pp. 13876–
13885.
[31]A. Tarvainen, H. Valpola, Mean teachers are better role models: Weight-
averaged consistency targets improve semi-supervised deep learning re-
sults, Advances in neural information processing systems (NeurIPS) 30
(2017).
[32]K. Li, S. Wang, L. Yu, P. A. Heng, Dual-teacher++: Exploiting intra-
domain and inter-domain knowledge with reliable transfer for cardiac
segmentation, IEEE Transactions on Medical Imaging 40 (10) (2020)
2771–2782.
[33]Z. Zhao, F. Zhou, K. Xu, Z. Zeng, C. Guan, S. K. Zhou, Le-uda: Label-
efficient unsupervised domain adaptation for medical image segmentation,
IEEE Transactions on Medical Imaging 42 (3) (2022) 633–646.
[34]S. Li, Z. Zhao, K. Xu, Z. Zeng, C. Guan, Hierarchical consistency
regularized mean teacher for semi-supervised 3d left atrium segmentation,
28

--- PAGE 29 ---
in: International Conference of the IEEE Engineering in Medicine &
Biology Society (EMBC), IEEE, 2021, pp. 3395–3398.
[35]Z. Zhao, A. Zhu, Z. Zeng, B. Veeravalli, C. Guan, Act-net: Asymmetric
co-teacher network for semi-supervised memory-efficient medical image
segmentation, in: IEEE International Conference on Image Processing
(ICIP), 2022, pp. 1426–1430.
[36]H.-R. Wei, S. Huang, R. Wang, X. Dai, J. Chen, Online distilling
from checkpoints for neural machine translation, in: Proceedings of
the Conference of the North American Chapter of the Association for
Computational Linguistics (NAACL), 2019, pp. 1932–1941.
[37]P. Foret, A. Kleiner, H. Mobahi, B. Neyshabur, Sharpness-aware mini-
mization for efficiently improving generalization, in: International Con-
ference on Learning Representations (ICLR), 2021.
[38]J. Kwon, J. Kim, H. Park, I. K. Choi, Asam: Adaptive sharpness-aware
minimization for scale-invariant learning of deep neural networks, in:
Proceedings of the 38th International Conference on Machine Learning
(ICML), Vol. 139, 2021, pp. 5905–5914.
[39]Y. Zhao, H. Zhang, X. Hu, Penalizing gradient norm for efficiently
improving generalization in deep learning, in: International Conference
on Machine Learning (ICML), Vol. 162, 2022, pp. 26982–26992.
[40]J. Du, D. Zhou, J. Feng, V. Tan, J. T. Zhou, Sharpness-aware training
for free, in: NeurIPS, 2022.
[41]H. Liu, K. Simonyan, Y. Yang, DARTS: Differentiable architecture search,
in: International Conference on Learning Representations (ICLR), 2019.
[42]A. Krizhevsky, G. Hinton, et al., Learning multiple layers of features
from tiny images (2009).
[43]G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely
connected convolutional networks, in: Proceedings of the IEEE conference
on computer vision and pattern recognition (CVPR), 2017, pp. 4700–
4708.
29

--- PAGE 30 ---
[44]C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
A. Yuille, J. Huang, K. Murphy, Progressive neural architecture search,
in: Proceedings of the European conference on computer vision (ECCV),
2018, pp. 19–34.
[45]S. Xie, H. Zheng, C. Liu, L. Lin, SNAS: stochastic neural architecture
search, in: International Conference on Learning Representations (ICLR),
2019.
[46]H. Cai, L. Zhu, S. Han, Proxylessnas: Direct neural architecture search
on target task and hardware, in: International Conference on Learning
Representations (ICLR), 2019.
[47]P. Ye, B. Li, Y. Li, T. Chen, J. Fan, W. Ouyang, β-darts: Beta-decay
regularization for differentiable architecture search, in: IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR), 2022, pp.
10864–10873.
[48]X. Dong, Y. Yang, Searching for a robust neural architecture in four
gpu hours, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), 2019, pp. 1761–1770.
[49]L. Huang, S. Sun, J. Zeng, W. Wang, W. Pang, K. Wang, U-DARTS:
uniform-space differentiable architecture search, Inf. Sci. 628 (2023) 339–
349.
[50]Y. Li, S. Li, Z. Yu, DARTS-PAP: differentiable neural architecture search
by polarization of instance complexity weighted architecture parameters,
in: MultiMedia Modeling (MMM), Vol. 13834, 2023, pp. 277–288.
[51]C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-
han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in:
Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR), 2015, pp. 1–9.
[52]A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,
M. Andreetto, H. Adam, Mobilenets: Efficient convolutional neural
networks for mobile vision applications, CoRR abs/1704.04861 (2017).
[53]X. Zhang, X. Zhou, M. Lin, J. Sun, Shufflenet: An extremely efficient
convolutional neural network for mobile devices, in: Proceedings of the
30

--- PAGE 31 ---
IEEE conference on computer vision and pattern recognition (CVPR),
2018, pp. 6848–6856.
[54]N. Ma, X. Zhang, H.-T. Zheng, J. Sun, Shufflenet v2: Practical guidelines
for efficient cnn architecture design, in: Proceedings of the European
conference on computer vision (ECCV), 2018, pp. 116–131.
[55]Y. Liu, X. Jia, M. Tan, R. Vemulapalli, Y. Zhu, B. Green, X. Wang,
Search to distill: Pearls are everywhere but not the eyes, in: 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR, 2020, pp. 7536–7545.
[56]X. Chu, B. Zhang, R. Xu, Fairnas: Rethinking evaluation fairness
of weight sharing neural architecture search, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), 2021,
pp. 12239–12248.
[57]M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, Q. V.
Le, Mnasnet: Platform-aware neural architecture search for mobile, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2019, pp. 2820–2828.
[58]O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual
recognition challenge, International journal of computer vision 115 (3)
(2015) 211–252.
[59]X. Dong, Y. Yang, Nas-bench-201: Extending the scope of reproducible
neural architecture search, in: International Conference on Learning
Representations (ICLR), 2020.
[60]S. Hu, S. Xie, H. Zheng, C. Liu, J. Shi, X. Liu, D. Lin, Dsnas: Direct
neural architecture search without parameter retraining, in: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2020.
[61]M. Zhang, S. W. Su, S. Pan, X. Chang, E. M. Abbasnejad, R. Haffari,
idarts: Differentiable architecture search with stochastic implicit gradi-
ents, in: Proceedings of the 38th International Conference on Machine
Learning (ICML), Vol. 139, 2021, pp. 12557–12566.
31

--- PAGE 32 ---
[62]X. Chu, X. Wang, B. Zhang, S. Lu, X. Wei, J. Yan, {DARTS }-: Robustly
stepping out of performance collapse without indicators, in: International
Conference on Learning Representations (ICLR), 2021.
32
