# OMNIFORCE: HỆ THỐNG AUTOML HƯỚNG CON NGƯỜI, TĂNG CƯỜNG BỞI MÔ HÌNH LỚN VÀ CỘNG TÁC ĐÁM MÂY-BIÊN

Chao Xue Wei Liu Shuai Xie Zhenfang Wang Jiaxing Li Xuyang Peng Liang Ding
Shanshan Zhao Qiong Cao Yibo Yang Fengxiang He Bohua Cai Rongcheng Bian Yiyan Zhao
Heliang Zheng Xiangyang Liu Dongkai Liu Daqing Liu Li Shen Chang Li Shijin Zhang
Yukang Zhang Guanpu Chen Shixiang Chen Yibing Zhan Jing Zhang Chaoyue Wang∗
Dacheng Tao
JD Explore Academy
{xuechao19, wangchaoyue9}@jd.com

TÓM TẮT
Học máy tự động (AutoML) tìm cách xây dựng các mô hình ML với nỗ lực tối thiểu từ con người. Trong khi đã có nhiều nghiên cứu đáng kể trong lĩnh vực AutoML nói chung, nhằm loại bỏ con người khỏi vòng lặp khi xây dựng các ứng dụng trí tuệ nhân tạo (AI), ít tài liệu tập trung vào cách AutoML hoạt động tốt trong các tình huống môi trường mở như quá trình huấn luyện và cập nhật các mô hình lớn, chuỗi cung ứng công nghiệp hoặc metaverse công nghiệp, nơi mọi người thường gặp phải các vấn đề vòng lặp mở trong quá trình tìm kiếm: họ phải liên tục thu thập dữ liệu, cập nhật dữ liệu và mô hình, đáp ứng các yêu cầu của môi trường phát triển và triển khai, hỗ trợ các thiết bị lớn, sửa đổi các chỉ số đánh giá, v.v. Giải quyết vấn đề môi trường mở với các phương pháp thuần túy dựa trên dữ liệu đòi hỏi dữ liệu đáng kể, tài nguyên tính toán và nỗ lực từ các kỹ sư dữ liệu chuyên dụng, làm cho các hệ thống và nền tảng AutoML hiện tại không hiệu quả và không thể tính toán được. Tương tác người-máy tính là một cách thực tế và khả thi để giải quyết vấn đề AI môi trường mở. Trong bài báo này, chúng tôi giới thiệu OmniForce, một hệ thống AutoML hướng con người (HAML) mang lại cả kỹ thuật ML hỗ trợ con người và con người hỗ trợ ML, để đưa hệ thống AutoML vào thực tế và xây dựng AI thích ứng trong các tình huống môi trường mở. Cụ thể, chúng tôi trình bày OmniForce về mặt quản lý phiên bản ML cho dữ liệu, nhãn, mô hình, thuật toán và không gian tìm kiếm; cộng tác phát triển và triển khai theo hướng pipeline; khung chiến lược tìm kiếm linh hoạt; và các thuật toán ứng dụng được cung cấp rộng rãi và crowdsourced, bao gồm cả các mô hình lớn. Phương pháp OmniForce cloud-native được đề xuất có thể chạy trên đám mây công cộng/riêng tư hoặc trong môi trường on-premise. Hơn nữa, các mô hình (lớn) được xây dựng bởi OmniForce có thể được tự động chuyển thành dịch vụ từ xa trong vài phút; quá trình này được gọi là model as a service (MaaS). Kết quả thí nghiệm thu được trong nhiều không gian tìm kiếm và các trường hợp sử dụng thực tế chứng minh hiệu quả và hiệu suất của OmniForce.

Từ khóa: Học Máy Tự Động Hướng Con Người (HAML) · Cộng Tác Đám Mây-Biên · Mô Hình Lớn · Mô Hình-như-Dịch Vụ (MaaS)

∗Tác giả liên hệ

## 1 Giới thiệu

Trong những thập kỷ gần đây, học máy (ML) đã đạt được thành công lớn trong các lĩnh vực thị giác máy tính [1,2], xử lý ngôn ngữ tự nhiên (NLP) [3,4], nhận dạng giọng nói [5,6], tạo nội dung [7,8] và xử lý dữ liệu dạng bảng [9,10]. Sự phát triển nhanh chóng của công nghệ ML đã sinh ra các sản phẩm trí tuệ nhân tạo (AI) cực kỳ phổ biến, như Tesla Autopilot [11], Google Translate [12], Siri [13], và ChatGPT [14]. Với các yêu cầu ML ngày càng tăng theo cấp số nhân về cả lượng dữ liệu huấn luyện và số lượng mô hình/mạng nơ-ron được điều chỉnh cho các nhiệm vụ khác nhau, việc thiết kế các siêu tham số phù hợp và xác định các mạng nơ-ron để huấn luyện một cách hoàn toàn tự động mà không cần sự can thiệp của con người, được gọi là ML tự động (AutoML), đã mang lại những thành tựu lớn.

Tiến bộ gần đây của AutoML đã được đặc trưng bởi các thuật toán và hệ thống. Về mặt trước, các nghiên cứu đáng kể đã sử dụng các phương pháp dựa trên thuật toán di truyền [15], tìm kiếm ngẫu nhiên [16], tối ưu hóa Bayesian [17], học tăng cường [18] và các kỹ thuật có thể vi phân [19]. Để đạt được điều sau, nhiều framework như Optuna [20], Ray-Tune [21], HyperOpt [22], NNI [23] và Orion [24] cho tối ưu hóa siêu tham số đã được phát triển để hỗ trợ các thử nghiệm có thể mở rộng và các thuật toán tìm kiếm có thể tùy chỉnh. Auto-sklearn [25] cũng hỗ trợ việc sử dụng meta-learning để tận dụng các bản ghi lịch sử để khởi động nóng quy trình tìm kiếm. Không giống như các framework khác đòi hỏi nỗ lực bổ sung để hỗ trợ Kubernetes [26], Katib [27] là một framework cloud-native và có thể thực sự chạy trong môi trường sản xuất. So với việc khám phá hệ thống AutoML mã nguồn mở, nhiều công ty cung cấp các sản phẩm AutoML của họ ra thị trường, như Google Cloud AutoML [28], IBM Watson AutoAI [29], Amazon SageMaker [30], và H2O Driverless AI [31]. Các nền tảng như vậy nhắm đến việc xây dựng các mô hình AI trong thời gian ngắn cho các nhà phát triển có kiến thức ML hạn chế.

Mặc dù đã có nhiều framework được đề xuất cho AutoML, như đã mô tả ở trên, chúng tôi chưa thấy việc áp dụng rộng rãi các hệ thống AutoML trong ngành công nghiệp như mong đợi. Chúng tôi giả định các lý do sau cho tỷ lệ áp dụng thấp của các framework này.

• Chỉ nhắm vào các vấn đề vòng lặp đóng – Hầu hết các framework AutoML chỉ tập trung vào các vấn đề vòng lặp đóng, nơi dữ liệu, thuật toán và chỉ số là xác định; do đó, khái niệm thiết kế của chúng là loại bỏ con người khỏi vòng lặp khi xây dựng các ứng dụng AI. Tuy nhiên, các vấn đề liên quan đến AI thường là các nhiệm vụ vòng lặp mở trong thực tế, đặc biệt là trong quá trình huấn luyện và cập nhật các mô hình lớn hoặc trong các chuỗi cung ứng công nghiệp, nơi mọi người cần thu thập dữ liệu liên tục, cập nhật các phiên bản của dữ liệu và mô hình, và sửa đổi các đánh giá và phần thưởng được tạo ra trong quá trình sản xuất. Sẽ không hiệu quả và thậm chí không thể tính toán được khi sử dụng các hệ thống AutoML dựa trên dữ liệu hiện tại để giải quyết các vấn đề vòng lặp mở vì chúng đòi hỏi dữ liệu đáng kể để học kiến thức miền và logic kinh doanh.

• Thiếu các cân nhắc triển khai – Hầu hết các framework AutoML chỉ tập trung vào các giai đoạn tìm kiếm và huấn luyện, bỏ qua các giai đoạn suy luận và triển khai. Tuy nhiên, các thiết bị lớn với các yêu cầu triển khai (suy luận) khác nhau được gặp phải trong các tình huống công nghiệp thực tế hoặc metaverse công nghiệp, nơi công nghệ mô phỏng hoặc XR2 được sử dụng để giảm rủi ro thất bại trong quá trình sản xuất vật lý và xây dựng chuỗi cung ứng hiệu quả cao, bao gồm các giai đoạn thiết kế, phát triển, sản xuất, định giá, bán hàng, lưu trữ, vận chuyển và dịch vụ hậu mãi.

• Thuật toán ứng dụng hạn chế – Hầu hết các framework AutoML chỉ có một số thuật toán ứng dụng được xác định trước hoặc tích hợp sẵn và các không gian tìm kiếm tương ứng của chúng, thường trong suốt với người dùng để dễ sử dụng. Tuy nhiên, một số lượng lớn các ứng dụng AI khác nhau có sẵn trong thực tế, và một hệ thống AutoML bị giới hạn bởi một số lượng cố định các thuật toán ứng dụng tích hợp sẵn. Xem xét số lượng lớn các ứng dụng khác nhau này, đặc biệt là những ứng dụng có các yêu cầu phát triển và triển khai khác nhau, ví dụ như cộng tác Đám mây-Biên, việc sử dụng rộng rãi các hệ thống AutoML sẽ bị hạn chế nếu chỉ cung cấp các thuật toán ứng dụng được xác định trước cho quá trình tìm kiếm.

Trong nỗ lực giải quyết các vấn đề trên, chúng tôi phát triển OmniForce để hỗ trợ AutoML trong các môi trường mở; OmniForce tập trung vào các ý tưởng sau.

• AutoML hướng con người và thích ứng (HAML) – Chúng tôi thiết kế OmniForce cho cả ML hỗ trợ con người và con người hỗ trợ ML. Do đó, người dùng có thể xử lý hiệu quả logic kinh doanh và quy trình thu thập dữ liệu của họ bằng cách tương tác với OmniForce.

• Cộng tác đám mây-biên trong thực tế – Chúng tôi đề xuất một framework AutoML theo hướng pipeline với cộng tác phát triển và triển khai để tìm kiếm các ứng dụng AI với các yêu cầu huấn luyện và triển khai khác nhau.

• Thuật toán ứng dụng crowdsourced – Chúng tôi giới thiệu khái niệm crowdsourcing để tích hợp các thuật toán ứng dụng quan sát được khác nhau vào nền tảng OmniForce. Bằng cách chuẩn hóa mô hình trừu tượng dữ liệu, thuật toán ứng dụng và không gian tìm kiếm, chúng tôi có thể dễ dàng tích hợp và tái sử dụng các thuật toán ứng dụng và không gian tìm kiếm.

Chúng tôi minh họa khái niệm HAML trong Hình 1. Người dùng tương tác với hệ thống AutoML về mặt ML hỗ trợ con người và con người hỗ trợ ML. Cụ thể, các nhiệm vụ HAML có các yếu tố thu thập và chú thích dữ liệu, đặc trưng, thuật toán ứng dụng, không gian tìm kiếm, tìm kiếm, huấn luyện và triển khai, và trực quan hóa. Đối với thu thập và chú thích dữ liệu, một mặt, người dùng thu thập dữ liệu để làm cho các thuật toán ML chính xác bằng cách sử dụng học tích cực; mặt khác, các thuật toán ML giúp người dùng gắn nhãn dữ liệu một cách hiệu quả. Hơn nữa, bảo vệ quyền riêng tư dữ liệu và bảo mật đóng vai trò quan trọng trong các tương tác giữa con người và hệ thống AutoML. OmniForce hỗ trợ kỹ thuật quyền riêng tư vi phân để bảo vệ dữ liệu của người dùng. Về đặc trưng, OmniForce hỗ trợ các pipeline đặc trưng tùy chỉnh thông qua tương tác người dùng và SQL. Ngoài ra, người dùng có thể xem và phân tích thông tin meta-data thống kê của dữ liệu và đặc trưng. Về thuật toán ứng dụng, với tính tổng quát của crowdsourcing và các mô hình siêu sâu, OmniForce có khả năng áp dụng rộng rãi hơn nhiều hệ thống AutoML khác chỉ với các thuật toán ứng dụng tích hợp sẵn quy mô nhỏ. OmniForce ẩn các chi tiết được sử dụng để thiết lập không gian tìm kiếm theo mặc định, nhưng người dùng có thể cấu hình không gian tìm kiếm bằng cách điều chỉnh các ưu tiên hoặc sở thích khi họ thu được kiến thức từ các trực quan hóa. Đối với tìm kiếm, huấn luyện và triển khai, người dùng xác định môi trường phát triển và triển khai, đặt các yêu cầu và ràng buộc của họ, và thực hiện tối ưu hóa đơn/đa mục tiêu. OmniForce hỗ trợ cộng tác Đám mây-Biên để giải quyết các yêu cầu khác nhau của môi trường huấn luyện và triển khai thông qua khả năng tìm kiếm mạnh mẽ của nó. Đối với phần trực quan hóa, là cốt lõi của HAML, người dùng học kiến thức về pipeline AutoML của họ từ OmniForce, thu được các giải thích về kiến trúc và siêu tham số được tìm kiếm, và thu được lời khuyên có thể được sử dụng để hướng dẫn các bước tiếp theo trong công việc của họ. Ví dụ, cố vấn có thể đề xuất rằng người dùng cập nhật không gian tìm kiếm dựa trên phân phối thống kê của các ứng viên lấy mẫu. Ngoài ra, nó có thể khuyến khích người dùng thu thập thêm dữ liệu từ một lớp cụ thể hoặc nới lỏng một số ràng buộc độ trễ và hạn chế công suất. Chu kỳ HAML cho phép người dùng tham gia đầy đủ vào hợp tác người-máy tính và đạt được mục đích của cả việc sử dụng máy móc để tăng cường khả năng con người và tận dụng kinh nghiệm và hoạt động của con người để cải thiện trí thông minh máy móc.

Đóng góp của chúng tôi như sau.

1) OmniForce là một hệ thống AutoML hướng con người và thích ứng tiên tiến hỗ trợ các tình huống môi trường mở như quá trình huấn luyện và cập nhật các mô hình lớn, chuỗi cung ứng công nghiệp và metaverse công nghiệp. Nó bao gồm một tập hợp các chiến lược tìm kiếm mới, chính sách cập nhật không gian tìm kiếm và các thuật toán mô hình lớn cho thị giác máy tính (CV), NLP và nội dung được tạo bởi AI (AIGC). Như vậy, OmniForce phục vụ cả các nhà phát triển có kiến thức ML hạn chế và các nhà khoa học dữ liệu.

2) OmniForce là một hệ thống AutoML cloud-native có thể mở rộng, chịu lỗi và cộng tác đám mây-biên; do đó, nó có thể chạy trong môi trường sản xuất.

3) OmniForce tuân theo mô hình model-as-a-service (MaaS) và kết nối đầy đủ các quy trình tìm kiếm, huấn luyện, suy luận và triển khai. Vào thời điểm OmniForce hoàn thành thành công quy trình xây dựng mô hình, người dùng sẽ không chỉ thu được mô hình mà còn có dịch vụ suy luận và triển khai của mô hình. Điều này cho phép người dùng chuyển đổi mô hình thành dịch vụ từ xa có thể được triển khai trên đám mây hoặc ở biên trong vài phút, giúp người dùng nhanh chóng xây dựng các ứng dụng tiên tiến với khả năng AI.

Phần còn lại của bài báo này được sắp xếp như sau. Phần 2 mô tả kiến trúc hệ thống và quy trình làm việc của OmniForce. Phần 3 mô tả các khái niệm thiết kế chi tiết, và Phần 4 hiển thị các tính năng được hỗ trợ, tiếp theo là các đánh giá trong Phần 5. Chúng tôi so sánh các công trình liên quan trong Phần 6 và cuối cùng kết luận trong Phần 7.

## 2 Kiến trúc Hệ thống

Như chúng ta sẽ thấy, OmniForce được triển khai với một số thành phần như một bộ ước lượng công việc, các worker, một bộ ước lượng nhiệm vụ huấn luyện (TTE), một bộ ước lượng triển khai nhiệm vụ (DTE), một sidecar nhiệm vụ, một bộ lập lịch và một trình quản lý. Hình 2 hiển thị sơ đồ khối đơn giản của tổng quan hệ thống của OmniForce.

Lớp ứng dụng quan tâm đến việc xử lý logic kinh doanh, như tải lên dữ liệu, chọn thuật toán ứng dụng và các mô hình được huấn luyện trước, và thiết lập không gian tìm kiếm để bắt đầu quá trình tìm kiếm. OmniForce cung cấp dữ liệu và thuật toán ứng dụng được cung cấp rộng rãi chứa các phương pháp dựa trên mô hình lớn cho người dùng tập trung vào logic kinh doanh của họ bất kể chi tiết của các thuật toán ML. Người dùng cũng có thể sử dụng và đóng góp vào các tài nguyên crowdsourced được tích hợp vào OmniForce để đáp ứng nhu cầu ngày càng tăng về các ứng dụng ML.

Tất cả dữ liệu, thuật toán và không gian tìm kiếm của lớp ứng dụng tương tác với công cụ tìm kiếm thông qua các trừu tượng thống nhất, có nghĩa là các tài nguyên này được đại diện và tổ chức như các dạng xem thống nhất. Các thay đổi tiếp theo được phản ánh trong các phiên bản khác nhau của các tài nguyên. Quyền riêng tư và bảo mật cũng được liên quan đến lớp giao diện. Thêm chi tiết về lớp giao diện có thể được tìm thấy trong Phần 3.1.

Công cụ tìm kiếm được triển khai thông qua một framework chiến lược tìm kiếm với cộng tác đa mục tiêu, gắn liền với một bộ điều khiển bán đồng bộ cho việc gửi đa chương trình/đa dữ liệu (MPMD), một bộ lập lịch gán công việc và nhiệm vụ cho các tài nguyên đám mây, và một bộ định dạng để học kiến thức lịch sử nhằm tạo ra pipeline AutoML (phục vụ như một meta-learner). Cụ thể, chúng tôi triển khai một framework tối ưu hóa Bayesian (BO) song song linh hoạt chạy hoàn toàn trên PyTorch, hỗ trợ BO với nhiều mô hình surrogate và hàm thu thập khác nhau, bao gồm mô hình mới của chúng tôi, để xử lý các không gian rời rạc lớn (tạo thành tình huống tìm kiếm kiến trúc mạng nơ-ron (NAS)). Chúng tôi cũng hỗ trợ một hyperband được sửa đổi [32], MF-NAS [33], và một phương pháp tiến hóa mới trong framework chiến lược tìm kiếm.

Quá trình tìm kiếm trở nên khó khăn hơn khi không gian tìm kiếm lớn. Chúng tôi liên quan đến nhiều worker để tìm các ứng viên tốt song song. Xem xét sự đánh đổi giữa hiệu quả song song và đồng bộ hóa không thể tránh khỏi mà một số chiến lược cần để đảm bảo hiệu suất, chúng tôi triển khai một bộ điều phối MPMD bán đồng bộ. Tương tự như Pathways [34], đã chứng minh các giới hạn của mô hình đơn chương trình/đa dữ liệu (SPMD) cho các tính toán ML, chúng tôi thể hiện quá trình NAS như MPMD. Thêm chi tiết về công cụ tìm kiếm có thể được tìm thấy trong Phần 3.4.

OmniForce chạy trên Kubernetes và Kubeflow [35] để hỗ trợ khả năng mở rộng, chịu lỗi và đa người thuê. OmniForce là một hệ thống cloud-native sẵn sàng sản phẩm có thể được triển khai như một dịch vụ trong đám mây công cộng/riêng tư hoặc trong môi trường on-premise.

### 2.1 Thành phần

Trong phần này, chúng tôi giải thích một số thành phần cơ bản của OmniForce chi tiết, bao gồm bộ ước lượng công việc, worker, TTE, DTE, sidecar nhiệm vụ, bộ lập lịch (bộ định dạng và bộ lập lịch tài nguyên), trình quản lý và cố vấn.

#### 2.1.1 Bộ Ước lượng Công việc

Bộ ước lượng công việc là một bộ điều khiển bán đồng bộ của quá trình AutoML xác định những nhiệm vụ nào nên được đánh giá tiếp theo, cách xử lý phần thưởng của chúng, và khi nào bắt đầu lần lặp tiếp theo. Hai thành phần chính được chứa trong bộ ước lượng công việc: một không gian tìm kiếm do người dùng xác định từ đó các ứng viên được tạo ra và một chiến lược tìm kiếm thực hiện các quá trình tìm kiếm tuần tự và song song thông qua các thuật toán tìm kiếm tiên tiến. Chi tiết về không gian tìm kiếm và chiến lược tìm kiếm có thể được tìm thấy trong Phần 3.2.2 và 3.4, tương ứng.

Một số framework AutoML có xu hướng hoàn toàn đồng bộ, nơi lần lặp tiếp theo sẽ không bắt đầu cho đến khi tất cả các nhiệm vụ trong vòng hiện tại được hoàn thành. Tuy nhiên, các framework đồng bộ không thể đảm bảo hiệu quả cao trong các thiết lập MPMD. Ngược lại, bộ ước lượng công việc của chúng tôi áp dụng một cơ chế bán đồng bộ kiểm soát việc bắt đầu lần lặp tiếp theo thông qua thời gian chờ tối đa thích ứng. Trạng thái của các nhiệm vụ vượt quá thời gian này được đặt thành timeout, và bộ ước lượng công việc bỏ qua các nhiệm vụ này trong lần lặp hiện tại và tạo ra các nhiệm vụ mới thông qua các quan sát đã hoàn thành. Tính toán thời gian chờ là linh hoạt. Ví dụ, chúng ta có thể sử dụng trung bình và phương sai của thời gian thực hiện của các nhiệm vụ đã hoàn thành trước đó để ước lượng thời gian chờ. Xem xét sự khác biệt lớn về thời gian thực hiện giữa các nhiệm vụ trong thiết lập MPMD, chúng ta cũng có thể xây dựng một mô hình surrogate nhận biết chi phí thời gian được cung cấp bởi framework BO của chúng tôi để đưa ra ước lượng cho mỗi nhiệm vụ.

#### 2.1.2 Worker

Một worker là một nhóm các quá trình dự trữ và đánh giá các nhiệm vụ ứng viên. Trong thiết kế của chúng tôi, bộ ước lượng công việc và các worker không giao tiếp trực tiếp mà thông qua một task broker, thường là middleware. Các hệ thống AutoML phân tán quy mô lớn hiện có luôn có nhu cầu tính toán lớn và đặt giá trị cao vào khả năng mở rộng và chịu lỗi. Thiết kế của chúng tôi cẩn thận xem xét các yêu cầu này và tách rời bộ ước lượng công việc và các worker, đạt được khả năng mở rộng thông qua đó người dùng có thể tự do tăng hoặc giảm số lượng worker được sử dụng dựa trên tài nguyên thực tế của họ. Hơn nữa, nếu một số nút worker bị hỏng bất ngờ, quá trình tìm kiếm của chúng tôi thích ứng với hoàn cảnh mới này mà không cần nỗ lực gì và tiếp tục tìm kiếm miễn là còn một worker còn sống. Ngoài ra, các worker có thể xử lý nhiều ngoại lệ phổ biến, như cạn kiệt bộ nhớ GPU và mất kết nối middleware.

#### 2.1.3 TTE

TTE là thực thể thực tế được sử dụng để đánh giá các nhiệm vụ ứng viên. Sau khi một worker dự trữ một nhiệm vụ, một TTE cụ thể sẽ được khởi chạy để đánh giá nhiệm vụ và báo cáo kết quả cho worker khi hoàn thành. OmniForce phát triển một dịch vụ sidecar nhiệm vụ đi kèm với TTE để proxy quá trình giao tiếp. OmniForce hỗ trợ crowdsourcing, và người dùng có thể dễ dàng nâng cấp mã huấn luyện của họ thành mã có thể tìm kiếm trong TTE bằng cách triển khai giao diện thuật toán được định nghĩa trong Phần 3.2.1. Quan trọng hơn, vì nó chạy trên Kubernetes và Kubeflow, OmniForce tạo điều kiện cho việc huấn luyện phân tán linh hoạt với các workload cloud-native như PytorchJob [36] và MPIJob [37]. Tính năng này giúp sử dụng hiệu quả các tài nguyên cluster có sẵn và xử lý các thiết lập huấn luyện biến đổi như một mô hình lớn.

#### 2.1.4 DTE

DTE là một thực thể triển khai hợp tác với TTE. Khi TTE hoàn thành quá trình huấn luyện mô hình, mô hình được huấn luyện được gửi đến DTE để đánh giá hiệu suất của mô hình trong môi trường triển khai. Giao tiếp giữa DTE và TTE được thực hiện bởi các sidecar nhiệm vụ. Ví dụ, trong tình huống cộng tác đám mây-biên, phía đám mây chịu trách nhiệm tìm kiếm và huấn luyện mô hình với khả năng tính toán đám mây mạnh mẽ, trong khi phía biên là môi trường thực tế cho sản xuất. OmniForce quan tâm đến yêu cầu này và cung cấp dịch vụ tối ưu hóa đa mục tiêu để đánh giá chung hiệu suất mô hình trong môi trường huấn luyện và triển khai. Thiết kế tận dụng ưu thế của tính toán đám mây và triển khai biên, thu hẹp khoảng cách giữa môi trường phát triển và sản xuất.

#### 2.1.5 Task Sidecar

Một sidecar [38] là một mẫu thiết kế trong hệ thống cloud-native rất hữu ích khi chạy hai quá trình kết hợp chặt chẽ cùng nhau. OmniForce xây dựng các thành phần sidecar nhiệm vụ để tách rời logic tìm kiếm và logic kết nối để điều chỉnh với các môi trường huấn luyện và triển khai phức tạp. Trong quy trình làm việc AutoML của chúng tôi, sidecar nhiệm vụ hoạt động như một người trung gian để kết nối các worker, TTE và DTE. Thành phần này xử lý các hoạt động tầm thường giữa các thành phần và mở rộng ranh giới của dịch vụ, làm cho nó có thể áp dụng cho nhiều tình huống downstream hơn.

#### 2.1.6 Bộ Lập lịch

Bộ lập lịch ước lượng và lập lịch tài nguyên cho các công việc và bao gồm hai phần. Phần đầu tiên định dạng một công việc mới như một cấu hình thích hợp bao gồm một không gian tìm kiếm và tài nguyên ước lượng, và thành phần này được gọi là "formatter" trong hệ thống của chúng tôi. Phần thứ hai, được gọi là bộ lập lịch tài nguyên, gán các tài nguyên tính toán thực tế cho công việc đã cho dựa trên tài nguyên ước lượng và tải trọng hiện tại của hệ thống AutoML.

Để ước lượng khe thời gian, bộ nhớ và tài nguyên tính toán, formatter xây dựng một cơ sở kiến thức chứa các thí nghiệm lịch sử đã được tìm kiếm trước đó. Chúng tôi thu được kiến thức từ các công việc trước đó, như siêu tham số, cấu hình chỉ số, đa độ trung thực của quá trình tìm kiếm, bộ nhớ và tính toán được sử dụng. Do việc sử dụng cơ sở kiến thức có sẵn, formatter tìm một thuật toán tìm kiếm và không gian tìm kiếm thích hợp cho một công việc đang vào và cung cấp cho bộ lập lịch tài nguyên các tài nguyên ước lượng và tính song song của các worker. Ngoài ra, formatter định nghĩa cấu trúc của pipeline tìm kiếm.

Bộ lập lịch tài nguyên quản lý tất cả các tài nguyên cluster và ngăn chặn deadlock hoặc các ngoại lệ pending do việc chiếm quyền tài nguyên. Cụ thể, bộ lập lịch tài nguyên phân bổ tài nguyên và điều chỉnh tính song song của giai đoạn tìm kiếm và huấn luyện theo chi tiết công việc được cung cấp bởi formatter và tải trọng hiện tại của hệ thống. Nó chia các công việc thành các giai đoạn tìm kiếm, huấn luyện, suy luận và triển khai và lập lịch tài nguyên của chúng riêng biệt. Bộ lập lịch tài nguyên xử lý một công việc của một loại nhất định tại một thời điểm.

#### 2.1.7 Trình quản lý

Trình quản lý là một dịch vụ housekeeper cho phép OmniForce tương tác với các tài nguyên Kubernetes và Kubeflow cloud-native. Ngoài các hoạt động tạo, đọc, cập nhật và xóa (CRUD) cơ bản, theo triết lý hoạt động ML, trình quản lý OmniForce tự động hóa quy trình làm việc AutoML với pipeline Kubeflow [39], là một dịch vụ cloud-native để xây dựng các quy trình làm việc ML di động và có thể mở rộng. Ngoài ra, chúng tôi xây dựng một số pipeline AutoML cụ thể và tích hợp các workload cloud-native khác nhau, như Kubernetes job, Kubeflow PytorchJob và KServe InferenceService, vào một pipeline. Do đó, OmniForce có thể quản lý các tài nguyên cloud-native này dưới dạng thống nhất, điều này đơn giản hóa quá trình tự động hóa và tái tạo quy trình làm việc AutoML.

Hơn nữa, xem xét các yêu cầu linh hoạt của quy trình làm việc AutoML và tính không thể dự đoán của tài nguyên tính toán cluster, trình quản lý OmniForce trừu tượng hóa các workload cloud-native thành các phần tử pipeline modular để bộ lập lịch và formatter OmniForce có thể tổ chức các phần tử này thành các pipeline với kiến trúc cụ thể và song song tài nguyên. Sau khi một pipeline được tạo ra, trình quản lý OmniForce khởi động các thành phần liên quan theo thứ tự topo.

Chúng tôi trình bày hai ví dụ pipeline AutoML trong Hình 3; một được gọi là pipeline NoCode, và cái khác được gọi là pipeline HPO. Trong pipeline NoCode, bộ ước lượng công việc và các worker được khởi chạy đồng thời để tìm kiếm kiến trúc và siêu tham số tốt nhất. Sau đó, bộ ước lượng nhiệm vụ xây dựng mô hình với sự cộng tác giữa huấn luyện và triển khai. Cuối cùng, cố vấn thu thập meta-data được tạo ra trong quy trình làm việc và cung cấp những hiểu biết sâu sắc thông tin cho người dùng. Các thành phần bắt đầu và kết thúc ở đây chịu trách nhiệm cho một số bước chuẩn bị và dọn dẹp. Trong pipeline HPO, bộ ước lượng công việc smoke và các worker được thêm vào để xác minh tính chính xác của các thuật toán crowdsourcing trước khi vào giai đoạn tìm kiếm thực tế. Sau đó, bộ ước lượng công việc mặc định và các worker được thêm vào để tái tạo hiệu suất thuật toán mặc định để so sánh với kết quả tìm kiếm AutoML.

#### 2.1.8 Cố vấn

Cố vấn là một thành phần cung cấp cho người dùng những hiểu biết toàn diện về tập dữ liệu và thuật toán crowdsourcing của họ. Cố vấn OmniForce trực quan hóa quá trình tìm kiếm trong quá trình thực hiện nhiệm vụ. Ngoài các chỉ số cơ bản như loss và accuracy, cố vấn cung cấp bản đồ trực quan của các ứng viên trong không gian tìm kiếm và bản đồ thanh của các mức độ quan trọng siêu tham số để chứng minh tương quan của chúng với các chỉ số đã chọn, nhằm gỡ rối các tương tác phức tạp giữa các siêu tham số. Thông tin này giúp người dùng hiểu những siêu tham số nào quan trọng nhất đối với hiệu suất của mô hình của họ. Hơn nữa, cố vấn cung cấp các đề xuất có giá trị về không gian tìm kiếm và tập dữ liệu. Ví dụ, cố vấn có thể đề xuất rằng người dùng cập nhật không gian tìm kiếm hoặc khuyến khích người dùng thu thập thêm dữ liệu về một lớp cụ thể.

### 2.2 Quy trình Làm việc Hệ thống

Trong phần này, chúng tôi minh họa quy trình làm việc AutoML tổng thể mà người dùng sẽ tương tác trong hệ thống OmniForce, được xem tốt nhất trong Hình 4. Tuân thủ khái niệm hướng con người, OmniForce thiết kế các giao diện người dùng thông tin và thân thiện để dễ sử dụng. Ngoài ra, OmniForce vận hành các dịch vụ theo cách cloud-native, ổn định và đa nền tảng bằng cách tận dụng Kubernetes và Kubeflow. Sau khi người dùng đặt mục tiêu của họ, OmniForce chuyển tiếp quá trình xây dựng và phân tích mô hình tiếp theo. Các bước chính như sau.

1. Đầu vào của người dùng được dịch thành yêu cầu mô hình tiêu chuẩn và gửi đến máy chủ ứng dụng.

2. Máy chủ ứng dụng xác minh danh tính và đặc quyền của người dùng, chuyển đổi yêu cầu mô hình thành công việc AutoML cụ thể, và lưu trữ thông tin công việc trong cơ sở dữ liệu. Cuối cùng, công việc này được gửi đến bộ lập lịch.

3. Bộ lập lịch tổ chức pipeline dựa trên thông tin công việc và tài nguyên cluster hiện tại. Quá trình này có thể được chia thành hai bước. Đầu tiên, formatter tạo ra một không gian tìm kiếm, chọn chiến lược tìm kiếm thích hợp dựa trên thông tin công việc và bản ghi lịch sử, và sau đó tổ chức các phần tử pipeline modular thành pipeline logic. Thứ hai, bộ lập lịch tài nguyên tính toán một giải pháp phân bổ tài nguyên hợp lý dựa trên tài nguyên cluster hiện tại. Đáng chú ý, bước này chuyển đổi pipeline logic thành pipeline tài nguyên cụ thể sẵn sàng được khởi chạy bởi trình quản lý.

4. Trình quản lý chuyển đổi pipeline tài nguyên thành định dạng tiêu chuẩn được hỗ trợ bởi bộ điều khiển pipeline Kubeflow và khởi động các thành phần pipeline theo thứ tự logic, như được minh họa trong Hình 3.

5. Bộ ước lượng công việc phân tích không gian tìm kiếm và tạo ra các ứng viên tìm kiếm dựa trên các chiến lược tìm kiếm cụ thể như tìm kiếm hyperband và BO.

6. Các ứng viên được tạo ra trong bước cuối được lưu trữ trong middleware và sẵn sàng được dự trữ bởi các worker.

7. Các worker dự trữ các ứng viên từ middleware theo chế độ loại trừ lẫn nhau và khởi chạy các bộ ước lượng nhiệm vụ cụ thể để đánh giá các ứng viên với sự giúp đỡ của sidecar nhiệm vụ.

8. Sidecar nhiệm vụ bắc cầu các ứng viên được dự trữ bởi các worker và khởi tạo chúng thành các workload cloud-native như job và PytorchJob để thực hiện đánh giá. Sau khi các workload này hoàn thành, kết quả đánh giá được lấy và báo cáo cho các worker và bộ ước lượng công việc cho vòng tạo ứng viên tiếp theo.

9. Sidecar nhiệm vụ cũng đóng vai trò quan trọng trong cộng tác huấn luyện và triển khai. Khi quá trình huấn luyện hoàn thành, sidecar nhiệm vụ ở phía huấn luyện chuyển tiếp mô hình được huấn luyện đến phía triển khai để đánh giá hiệu suất mô hình (như độ trễ và công suất) trong môi trường triển khai. Bước này là một phần thiết yếu của thiết kế tối ưu hóa đa mục tiêu của OmniForce.

10. Trong quá trình tìm kiếm mô hình, cố vấn phân tích toàn diện meta-data và tạo ra các đề xuất hữu ích cho người dùng như phân tích tập dữ liệu, thuật toán hoặc cấu hình.

## 3 Khái niệm Thiết kế

### 3.1 Trừu tượng và Quản lý cho AI Hướng Con người

Là một nền tảng AI hướng con người, OmniForce giảm độ phức tạp của các quy trình hoạt động ML và để người dùng tập trung vào công việc liên quan nhất của họ, như logic kinh doanh. Cụ thể, OmniForce áp dụng khái niệm dự án để quản lý thống nhất dữ liệu, đặc trưng, mô hình và meta-data khác. Một dự án chứa tất cả các tài liệu cần thiết để xây dựng một giải pháp kinh doanh cụ thể. Bên dưới dự án, các khả năng cơ bản cần thiết cho AI trong sản xuất, như quyền riêng tư dữ liệu, kiểm soát phiên bản, kỹ thuật đặc trưng, huấn luyện mô hình, phục vụ và giám sát, và tự động hóa pipeline ML, được trừu tượng hóa tốt và thuận tiện để sử dụng. Trong phần này, chúng tôi chi tiết quản lý dữ liệu, pipeline đặc trưng và mô hình của OmniForce để minh họa khả năng quản lý vòng đời AI của nó.

#### 3.1.1 Quản lý Dữ liệu

Dữ liệu tạo thành nền tảng để xây dựng các giải pháp AI thành công. Khi mọi người chọn các nền tảng ML, họ thường có những mối quan tâm trung tâm sau. Dữ liệu của tôi có an toàn ở đây không? Quyền riêng tư của tôi có được bảo vệ cẩn thận không? Nền tảng có tương thích với dữ liệu đa dạng của tôi và có thể phát huy hết giá trị dữ liệu không? Nền tảng có thể xử lý lặp lại dữ liệu nhanh chóng và cập nhật giải pháp kịp thời không? OmniForce cung cấp dịch vụ quản lý dữ liệu toàn diện, bao gồm quyền riêng tư, khả năng truy cập và phiên bản dữ liệu, chú thích dữ liệu và học suốt đời.

**Khả năng Truy cập và Phiên bản Dữ liệu** Hầu hết các nhà phát triển nền tảng AI đồng ý rằng truy cập dữ liệu là vấn đề chính khi kết nối người dùng với nền tảng. Dữ liệu đa dạng xuất phát từ mọi tầng lớp xã hội làm tăng gánh nặng chuẩn hóa và quản trị dữ liệu. Để giải quyết vấn đề này, OmniForce phát triển một nhóm các giao diện lập trình ứng dụng (API) truy cập và lấy dữ liệu thống nhất. Với nỗ lực nhỏ, người dùng có thể tải lên dữ liệu đa dạng từ nhiều nguồn và chuyển đổi dữ liệu thô thành định dạng tiêu chuẩn với API truy cập dữ liệu. API lấy dữ liệu trừu tượng hóa độ phức tạp của các nguồn dữ liệu và giúp các nhà phát triển và nhà khoa học dữ liệu truy xuất dữ liệu tự do.

Sau khi chuẩn hóa dữ liệu, OmniForce thực hiện kiểm soát phiên bản dữ liệu với DVC [40], được sử dụng để xử lý các tệp lớn, tập dữ liệu, mô hình, cấu hình và mã. Khi người dùng tải lên dữ liệu, OmniForce chuyển đổi tất cả các tệp phương tiện (trừ tệp dữ liệu dạng bảng) thành tệp metadata duy trì các tham chiếu đến dữ liệu gốc. Người dùng có thể tự do thực hiện các hoạt động trên meta-data và lưu những thay đổi này vào phiên bản mới. Các hoạt động này không thay đổi meta-data trước đó, và tất cả meta-data có thể truy vết và tái sử dụng. Các hoạt động này chủ yếu bao gồm:

1. Thêm hoặc sửa đổi các chú thích.
2. Lọc dữ liệu, ví dụ: lọc 5 lớp từ toàn bộ tập dữ liệu.
3. Hợp nhất hai phiên bản của tập dữ liệu.

Đáng chú ý, tất cả dữ liệu được tải lên OmniForce được lưu trữ an toàn với thuật toán bảo vệ quyền riêng tư của chúng tôi.

**Học Tích cực và Chú thích Thông minh** Học tích cực (AL) [41] là một công nghệ thực tế để giải quyết tình trạng thiếu dữ liệu chú thích. Vấn đề cơ bản trong AL là phát triển một chiến lược xếp hạng dữ liệu hiệu quả về chi phí để tìm các mẫu thông tin nhất trong nhóm dữ liệu chưa được gắn nhãn rộng lớn. Trong một vòng lặp AL điển hình, một mô hình được sử dụng để đề xuất các mẫu với các chiến lược xếp hạng thích hợp. Sau đó, các chú thích viên chuyên nghiệp đánh giá các đề xuất này và tích cực chú thích các mẫu mà mô hình cần gấp. Khi ngân sách chú thích được thỏa mãn, mô hình cập nhật sử dụng tập dữ liệu được gắn nhãn mới, thường với nhiều mẫu hơn. Đổi lại, mô hình được huấn luyện tốt hơn sẽ được sử dụng để đề xuất dữ liệu tốt hơn cho vòng đề xuất tiếp theo. Dữ liệu là nhiên liệu thúc đẩy làn sóng AI. AL giúp thu thập nhiên liệu chất lượng cao và nhanh chóng xây dựng các mô hình AI hiệu suất cao, điều này có ý nghĩa trong các lĩnh vực công nghiệp.

AL liên quan đến con người trong vòng đời ML và nhanh chóng xây dựng các tập dữ liệu chất lượng cao với sự hỗ trợ lẫn nhau giữa con người và máy móc. OmniForce kết hợp khả năng này với các dịch vụ khác của chúng tôi và thiết kế dịch vụ chú thích dữ liệu thông minh. Dịch vụ này có các tính năng chính sau.

1. Chú thích dữ liệu phong phú hỗ trợ các nhiệm vụ AI phổ biến nhất.

2. Chú thích cộng tác con người-máy. OmniForce hỗ trợ hai chế độ chú thích, trực tuyến và ngoại tuyến. Trong chế độ trực tuyến, người dùng có thể chỉnh sửa kết quả chú thích máy móc trong thời gian thực, hưởng lợi từ dịch vụ triển khai mô hình thuận tiện của OmniForce. Trong chế độ ngoại tuyến, người dùng có thể gửi một nhiệm vụ chú thích với một lượng lớn dữ liệu chưa được gắn nhãn cùng lúc và xem lại tất cả kết quả khi quá trình chú thích tự động hoàn thành, điều này hưởng lợi từ dịch vụ suy luận hàng loạt của OmniForce.

3. Khả năng chú thích máy móc có thể tùy chỉnh. Không giống như các đối tác khác trên thị trường chỉ hỗ trợ các mô hình được cung cấp với khả năng chú thích hạn chế, OmniForce hưởng lợi từ thiết kế crowdsourcing của nó và hỗ trợ người dùng triển khai các mô hình chú thích của họ. Các mô hình này có thể được tải lên trực tiếp bởi người dùng hoặc được tạo ra bởi OmniForce.

4. Dịch vụ đề xuất dữ liệu được điều khiển bởi AL. OmniForce tích hợp nhiều thuật toán AL thực tế và tiên tiến để giúp người dùng tìm dữ liệu thông tin nhất và nhanh chóng xây dựng các tập dữ liệu chất lượng cao.

**Quyền riêng tư Vi phân** Kỹ thuật quyền riêng tư vi phân (DP) đầu tiên được đề xuất để đảm bảo quyền riêng tư của các hoạt động truy vấn cơ sở dữ liệu [42]. Gần đây, nó cũng được mở rộng để đo mức độ bảo tồn quyền riêng tư của một thuật toán [43,44]. Giả sử hai tập dữ liệu liền kề (S, S′), trong đó S và S′ chỉ khác nhau nhiều nhất một mẫu, một tập con tùy ý H của không gian giả thuyết mô hình, và một thuật toán A được cho. DP của A được định nghĩa là sự thay đổi trong giả thuyết đầu ra của A khi A được áp dụng cho S và S′. Cụ thể, (ε, δ)-DP được định nghĩa toán học là log⟨PA(S)(A(S)∈H)−δ / PA(S′)(A(S′)∈H)⟩ ≤ ε. Điều này có nghĩa là một thuật toán với quyền riêng tư vi phân (ε, δ) nhỏ mạnh mẽ đối với các thay đổi trong các mẫu huấn luyện riêng lẻ. Do đó, giá trị của (ε, δ) lập chỉ mục mức độ DP, tức là khả năng chống lại các cuộc tấn công vi phân sử dụng các mẫu riêng lẻ làm đầu dò để tấn công các thuật toán ML; sau đó, quyền riêng tư cá nhân được suy ra thông qua các thay đổi trong các giả thuyết đầu ra. Nói chung, mức độ bảo tồn quyền riêng tư của một thuật toán lặp bị giảm sút cùng với số lần lặp vì lượng thông tin bị rò rỉ tích lũy khi thuật toán tiến triển [43,45,46].

Các mạng nơ-ron sâu đã được chứng minh thực tế có khả năng tổng quát hóa tốt. Tuy nhiên, một mạng nơ-ron sâu là một mô hình được tham số hóa quá mức và khó giải thích bằng lý thuyết học thống kê hiện có [47]. Điều này đã thu hút sự quan tâm của cộng đồng trong việc nghiên cứu nó, và nhiều công trình đã tìm thấy rằng một mô hình DP thường cũng có khả năng tổng quát hóa được đảm bảo [48, 49, 50, 51, 46].

#### 3.1.2 Quản lý Pipeline Đặc trưng

Các mô hình ML phụ thuộc cao vào chất lượng của dữ liệu đầu vào, và tiền xử lý dữ liệu thô thường là một phần quan trọng của pipeline ML. Để hỗ trợ các nhà khoa học dữ liệu và kỹ sư truyền đạt hiệu quả và chính xác kinh nghiệm của họ vào các sản phẩm AI, OmniForce cung cấp các khả năng xử lý dữ liệu như phân tích dữ liệu khám phá, kỹ thuật đặc trưng tương tác và xử lý SQL tiên tiến. Các pipeline đặc trưng tùy chỉnh có thể giảm bớt gánh nặng chuẩn bị dữ liệu theo cách tự động và ít mã, cho phép tập trung nhiều hơn vào thu thập dữ liệu, thiết kế đặc trưng và đổi mới lựa chọn mô hình.

**Kiểm soát Phiên bản Pipeline Đặc trưng** Khác với kiểm soát phiên bản dữ liệu, kiểm soát phiên bản pipeline đặc trưng nhấn mạnh việc truy vết các hoạt động kỹ thuật đặc trưng của người dùng. Một pipeline đặc trưng chứa một loạt các bước hoạt động. Khi người dùng thực hiện các hoạt động trên dữ liệu của họ trong giao diện, chúng được áp dụng vào một phần nhỏ dữ liệu và lưu trữ để xử lý tương lai theo cách lazy loading. Do đó, chúng tôi cung cấp một thỏa hiệp thanh lịch giữa phản hồi hoạt động người dùng thời gian thực và kiểm soát phiên bản hoạt động. Dựa trên các phiên bản hoạt động lịch sử, người dùng có thể xây dựng pipeline đặc trưng mới với nỗ lực nhỏ bằng cách sửa đổi hoặc hợp nhất các bước hoạt động.

**Pipeline Đặc trưng Tùy chỉnh** Người dùng có thể sử dụng kỹ thuật đặc trưng tương tác và viết các câu lệnh SQL để tạo ra các đặc trưng dựa trên kinh nghiệm con người, cho phép ML hỗ trợ con người đạt được kết quả thỏa mãn nhanh hơn. Cụ thể, kỹ thuật đặc trưng tương tác được chia thành các danh mục sau: trích xuất đặc trưng thời gian, tính toán cột đơn, tính toán liên cột và xử lý điều kiện cụ thể, bao gồm các hoạt động nguyên tử thường được sử dụng trong các quy trình xử lý dữ liệu. Chúng tôi cũng hỗ trợ người dùng thực hiện các hoạt động cấp cao trên dữ liệu thông qua các câu lệnh SQL tùy chỉnh, cải thiện đáng kể hiệu quả kỹ thuật đặc trưng của họ. Ngoài ra, chúng tôi sử dụng chiến lược xử lý lazy để thực hiện các hoạt động nhằm đảm bảo tương tác thời gian thực, làm cho các cú nhấp chuột của người dùng mượt mà hơn khi xử lý dữ liệu lớn.

#### 3.1.3 Quản lý Mô hình

Trong phần này, chúng tôi giới thiệu kỹ thuật quản lý mô hình trong OmniForce, bao gồm ba phần chính: xây dựng và phiên bản mô hình, triển khai mô hình và giám sát trong sản xuất. Ý tưởng cốt lõi của quản lý mô hình là thực hiện lặp lại mô hình tự động với vòng lặp đóng của huấn luyện, triển khai, giám sát và tối ưu hóa để phát hành mô hình vào sản xuất theo cách agile.

**Xây dựng và Phiên bản Mô hình** OmniForce xây dựng các mô hình với các pipeline AutoML theo cách tự động, và tất cả các mô hình được tạo ra đều được kiểm soát phiên bản. Chúng tôi thiết kế kiến trúc quản lý mô hình ba lớp để chứng minh mối quan hệ giữa các mô hình. Trong thiết kế của chúng tôi, lớp trên cùng được gọi là họ mô hình, một tập hợp các mô hình có tương quan mạnh. Ví dụ, khi sử dụng các mô hình lớn cho các nhiệm vụ downstream, OmniForce quản lý các mô hình dẫn xuất như một họ. Lớp giữa được gọi là đăng ký mô hình, chỉ ra mô hình hiện đang được sử dụng. Lớp dưới cùng được gọi là tài sản mô hình, đại diện cho mô hình thô được tạo ra bởi pipeline AutoML.

**Triển khai Mô hình** Triển khai một mô hình được huấn luyện vào môi trường sản xuất là một phần quan trọng của quản lý mô hình và đóng vai trò lớn trong mô hình MaaS vì nhiều nhà phát triển tận dụng khả năng AI để xây dựng ứng dụng thông qua các API được cung cấp sau khi mô hình của họ được triển khai.

OmniForce áp dụng KServe [52] để triển khai mô hình vào sản xuất và nhanh chóng xây dựng kiến trúc MaaS. Mô hình chạy trên cluster Kubernetes dưới dạng serverless, và người dùng có thể truy cập mô hình thông qua API được cung cấp. OmniForce được thiết kế với đặc tả mã crowdsourced, và các mô hình được tạo ra bởi mã được viết bởi các nhà phát triển thuật toán theo các hướng dẫn này có thể được triển khai nhanh chóng vào sản xuất. Các mô hình được triển khai sẽ có các thuộc tính sau:

1. Hỗ trợ cập nhật rolling và rollback.
2. Hỗ trợ đồng thời cao và độ trễ thấp.
3. Hỗ trợ autoscaling, bao gồm scaling về 0, để giải quyết xung đột giữa độ nhạy độ trễ và khả năng dự đoán nhu cầu.
4. Hỗ trợ các phương pháp triển khai tiên tiến như canary release, blue-green release và A/B testing.
5. Hỗ trợ triển khai đồng thời nhiều mô hình và mô hình cascade.
6. Hỗ trợ suy luận mô hình điều kiện đa giai đoạn.
7. Hỗ trợ giải thích mô hình.
8. Hỗ trợ giám sát mô hình.

**Giám sát Mô hình** Giám sát mô hình là giai đoạn hoạt động trong vòng đời ML diễn ra sau triển khai mô hình. Vì môi trường sản xuất thay đổi liên tục, một mô hình sản xuất sẽ dẫn đến mất hiệu suất, được gọi là model drift. Model drift có hai dạng:

1. Data drift. Data drift được gây ra bởi các thay đổi phân phối dữ liệu. Vì các mô hình AI nhạy cảm với phân phối dữ liệu đã cho, khi phân phối dữ liệu thay đổi mạnh hơn, hiệu suất của mô hình giảm nhanh chóng.

2. Concept drift. Concept drift đề cập đến tình huống khi một mô hình không còn áp dụng được cho môi trường của nó do các thay đổi trong thuộc tính của biến phụ thuộc.

Giám sát mô hình được sử dụng để giám sát liệu mô hình có bị drift trong môi trường hiện tại hay không. Hệ thống giám sát mô hình của chúng tôi có thể giám sát các mô hình trong sản xuất theo thời gian thực, gửi cảnh báo khi model drift xảy ra, và bắt đầu huấn luyện lại mô hình sử dụng dữ liệu được thu thập gần đây để cập nhật mô hình một cách lặp lại và giữ hiệu suất của chúng ở mức có thể chấp nhận được. Các mô hình mới được tạo ra bởi việc huấn luyện lại mô hình được tự động lưu trữ trong họ mô hình gốc và cập nhật với phiên bản mới.

#### 3.1.4 Trực quan hóa và Hiểu biết Thông tin

Là cốt lõi của AI hướng con người, trực quan hóa giúp người dùng hiểu dữ liệu và thuật toán của họ. OmniForce cung cấp thông tin trực quan hóa phong phú, như phân phối dữ liệu, không gian tìm kiếm, mức độ quan trọng siêu tham số và thống kê suy luận. Người dùng có thể tham gia vào vòng lặp xây dựng giải pháp bằng cách phân tích thông tin này và giúp OmniForce xây dựng các mô hình đa dạng hơn. Ví dụ, người dùng có thể thiết kế lại không gian tìm kiếm hoặc điều chỉnh phân phối dữ liệu để xây dựng các mô hình tốt hơn.

**Phân tích Tầm quan trọng Siêu tham số** Hiệu suất của một mô hình deep learning phụ thuộc cao vào các thiết lập siêu tham số của nó. Một số phương pháp tối ưu hóa hiện đại đã được đề xuất và đã tối ưu hóa siêu tham số tự động thành công. Tuy nhiên, các phương pháp này không giải thích cách các siêu tham số cụ thể ảnh hưởng đến hiệu suất mô hình kết quả. Để cung cấp cho người dùng báo cáo siêu tham số dễ hiểu hơn, OmniForce trích xuất mối quan hệ giữa siêu tham số và chỉ số với phương pháp đánh giá tầm quan trọng siêu tham số, được đặt tên là phân tích phương sai chức năng (fANOVA) [53].

Cho các chỉ số mô hình và các thiết lập siêu tham số tương ứng, fANOVA khớp một random forest để xấp xỉ ánh xạ giữa không gian siêu tham số và không gian hiệu suất. Sau đó, fANOVA được áp dụng để đánh giá tầm quan trọng của mỗi siêu tham số. Hơn nữa, chúng tôi thực hiện một số cải tiến để thích ứng phương pháp này với các siêu tham số có phụ thuộc phân cấp.

**Giải thích Suy luận Mô hình Thời gian Thực** Hiện tại, hầu hết các mô hình sâu hoạt động theo cách hộp đen, thiếu khả năng giải thích và cản trở việc áp dụng các mô hình sâu trong nhiều lĩnh vực. Theo ý tưởng của HAML, OmniForce cung cấp dịch vụ giải thích mô hình thời gian thực để giải thích đầu ra của mô hình. Cụ thể, OmniForce xây dựng một mô hình có thể giải thích và triển khai mô hình này cùng với mô hình mục tiêu sử dụng KServe [52]. Khi người dùng thực hiện suy luận trực tuyến, cả hai mô hình hoạt động, và mô hình có thể giải thích phân tích kết quả của mô hình mục tiêu. Dựa trên thông tin giải thích, người dùng có thể hiểu tại sao mô hình của họ cho ra kết quả như vậy. Ngoài ra, xem xét rằng các phương pháp có thể giải thích nên tương thích với các mô hình không đồng nhất khác nhau được hỗ trợ bởi nền tảng của chúng tôi, chúng tôi chủ yếu chọn các thuật toán giải thích mô hình hộp đen, như anchor [54]. Phương pháp này có thể xấp xỉ quá trình ra quyết định của mô hình mạng nơ-ron thành quá trình phân biệt dựa trên quy tắc. Các đặc điểm giải thích cục bộ của nó tạo điều kiện cho việc giải thích một mẫu đơn được dự đoán bởi mô hình và giúp người dùng hiểu trực quan quá trình giải thích.

**Đánh giá Hiệu suất Mô hình Đa mục tiêu**

**Chỉ số Toàn diện Huấn luyện và Triển khai** Hầu hết các nền tảng AutoML hiện có chỉ tập trung vào hiệu suất mô hình đạt được trong giai đoạn huấn luyện, bỏ qua giai đoạn triển khai. OmniForce bắc cầu khoảng cách này với sidecar nhiệm vụ. Mô hình được phát triển trong giai đoạn huấn luyện được gửi đến các môi trường triển khai như Qualcomm A650 [55] và NVIDIA Jetson Develop Kit [56] để đánh giá hiệu suất mô hình trong sản xuất. Các chỉ số ở phía triển khai, như độ trễ và công suất, được thu thập để tính toán phần thưởng toàn diện cho vòng tìm kiếm mô hình tiếp theo.

**Độ bền Mô hình** Độ bền mô hình là khả năng chống lại các nhiễu động bên ngoài, là điều kiện tiên quyết để sử dụng AI rộng rãi. Một mô hình bền có thể tạo ra đầu ra ổn định và thích ứng với các môi trường khác nhau trong sản xuất. Tính năng này khá quan trọng trong một số ứng dụng, như chăm sóc sức khỏe, tài chính và bảo mật. Độ bền mô hình có liên quan chặt chẽ đến phân phối dữ liệu cơ bản. Nghiên cứu gần đây đã tìm thấy rằng một sự lệch nhỏ của dữ liệu có thể khiến mô hình liên quan đưa ra kết quả hoàn toàn khác nhau, làm nổi bật thực tế rằng các mô hình AI hiện tại quá nhạy cảm với dữ liệu. Trong sản xuất, những vấn đề này có thể gặp phải một cách tình cờ, như bởi sự dịch chuyển phân phối dữ liệu tự nhiên. Tuy nhiên, chúng cũng có thể là cố ý, như các cuộc tấn công hack. OmniForce cung cấp dịch vụ đánh giá độ bền mô hình để đánh giá khả năng chống lại các cuộc tấn công nhiễu của mô hình. Người dùng có thể chọn mô hình được triển khai trong sản xuất theo hiệu suất chính xác và độ bền toàn diện của nó. Ngoài ra, OmniForce cung cấp công cụ đánh giá độ bền, bao gồm hai phương pháp đánh giá phổ biến: đánh giá tấn công đối nghịch mô hình [57,58] và đánh giá quyền riêng tư mô hình dựa trên suy luận thành viên [59,46,58]. Phương pháp trước đánh giá độ bền của mô hình dưới các ví dụ đối nghịch, và phương pháp sau đánh giá quyền riêng tư dữ liệu của mô hình dưới suy luận thành viên.

### 3.2 Cộng tác Huấn luyện và Triển khai Theo hướng Pipeline cho Crowd-sourcing

Sau khi tải lên và xử lý tập dữ liệu đã cho trên OmniForce, để tạo ra một ứng dụng công nghiệp, người dùng cần tải lên mô hình của riêng họ hoặc chọn mô hình crowdsourcing được đề xuất bởi formatter OmniForce. Sau đó, OmniForce tổ chức toàn bộ quá trình từ huấn luyện đến triển khai thông qua pipeline tự động. Cụ thể, dựa trên phương pháp điều khiển pipeline, việc thích ứng và thu nhỏ của các mô hình lớn có thể được đạt được một cách tự phát. Khi tìm kiếm một mô hình mới, một kỹ sư hoặc nhà khoa học dữ liệu hoàn thành giao diện thuật toán theo tài liệu và cấu hình thiết kế không gian tìm kiếm, thiết bị chạy (CPU, GPU) và chỉ số tối ưu hóa tương ứng.

#### 3.2.1 Giao diện Thuật toán Ứng dụng

Một tracer chương trình mới được triển khai trong thiết kế của chúng tôi, như hiển thị trong Listing 1. Để crowdsource một mô hình mới, người dùng cần hoàn thành giao diện và bọc một số khối chức năng của mã Python. Xem xét mong muốn có giao diện thân thiện với người dùng, các hoạt động gửi và nhận tin nhắn phức tạp được triển khai nội bộ.

```python
from omnitools import estimator

class YourEstimator(metaclass=ABCMeta):
    """ This is the abstract base class for OmniForce task estimators."""
    
    @estimator.wrap
    def __init__(
        self,
        run_epochs: int,
        data_path: str,
        is_trainer: bool,
        **model_args,  # Parameters customized for your algorithm.
    ) -> None:
        self.model = YourModel(**model_args)
    
    @abstractmethod
    @estimator.wrap
    def calculate_score(self) -> float:
        """ Compute scores to evaluate search models. """
        return val_score
    
    def run(self) -> float:
        """ Omniforce triggers the training process through this function. """
        # train
        self.train(self.train_loader, self.run_epochs)
        # test
        val_score = self.calculate_score(self.valid_loader)
        # save models
        if self.is_trainer:
            self.save(self.save_dir)
        return val_score
```

#### 3.2.2 Cấu hình Thuật toán Ứng dụng

**Cấu hình Không gian Tìm kiếm** Điều chỉnh siêu tham số là một bước quan trọng khi tạo ra và triển khai các thuật toán AI cho các ứng dụng công nghiệp. Đối với nhiệm vụ tìm kiếm mô hình mới, OmniForce cho phép người dùng điều chỉnh siêu tham số của họ và tùy chỉnh không gian tìm kiếm. Trong quy trình cấu hình của chúng tôi, không gian tìm kiếm có thể được cấu hình thông qua các tương tác thân thiện với người dùng trên frontend hoặc bằng cách tải lên các tệp YAML cho các không gian phức tạp.

Các nhiệm vụ khác nhau có thể có các định nghĩa rất khác nhau về không gian tìm kiếm. Nói chung, trong các mô hình deep learning, luôn có các phụ thuộc giữa các tham số. Ví dụ, khi tìm kiếm cấu trúc mạng của một mạng nơ-ron, chúng ta thường muốn khám phá độ sâu và chiều rộng của mạng. Độ sâu chỉ ra có bao nhiêu khối tích chập hoặc lớp trong mạng backbone, trong khi chiều rộng luôn đề cập đến số kênh mỗi khối hoặc lớp. Do đó, độ dài của mảng kênh được lấy mẫu phụ thuộc vào giá trị độ sâu được lấy mẫu. Nếu độ sâu là ba, ba giá trị kênh được lấy mẫu từ không gian tìm kiếm, như hiển thị trong Listing 2. Một phức tạp khác liên quan đến không gian điều kiện. Ví dụ, giả sử các loại khối khác nhau với các tham số khác nhau được tìm kiếm trong một mô hình sâu. Trong trường hợp đó, các tham số được lấy mẫu được xác định sau khi lấy mẫu các loại của các khối tương ứng. Những tình huống này thường xảy ra trong không gian tìm kiếm của các thuật toán khác nhau. Do đó, OmniForce hỗ trợ các quy tắc lấy mẫu không gian tìm kiếm dựa trên cây và DAG, mô hình hóa các phụ thuộc giữa các tham số như một đồ thị.

```yaml
backbone_nums_block:
  type: int
  range: [2...5]
  submodule:
    block_type:
      type: choice
      range: {resnet, transformer}
      submodule:
        resnet:
          nums_layer:
            type: int
            range: [3...7]
            submodule:
              nums_channel:
                type: choice
                range: {64, 256}
        transformer:
          mlp_expend_ratio:
            type: choice
            range: {1, 2, 4, 8}
```

**Cấu hình Suy luận** OmniForce yêu cầu người dùng cung cấp các cấu hình tương ứng trong giai đoạn suy luận hàng loạt của crowdsourcing, bao gồm số lượng thiết bị suy luận và việc sử dụng tài nguyên suy luận. Hai chỉ số này đảm bảo rằng OmniForce có thể xây dựng môi trường suy luận cho lượng lớn dữ liệu.

**Cấu hình Triển khai** Trong giai đoạn triển khai, OmniForce cũng yêu cầu người dùng cung cấp các cấu hình thích hợp để nhanh chóng triển khai mô hình của họ vào sản xuất. Các cấu hình này bao gồm các thiết bị triển khai, việc sử dụng tài nguyên triển khai và độ trễ suy luận mẫu đơn. Độ trễ có thể giúp chúng ta suy ra QPS sau khi mô hình được triển khai.

**Môi trường và Yêu cầu Huấn luyện Cộng tác Đám mây-Biên** OmniForce đề xuất một thực hành AutoML mới dựa trên framework cộng tác đám mây-biên. Theo cách này, người dùng có thể phát triển các mô hình phù hợp nhất cho các thiết bị cụ thể của họ dưới cả chỉ số hiệu suất và độ trễ bằng cách cài đặt gói python cộng tác đám mây-biên OmniForce và đăng ký thiết bị của họ. Ngoài ra, khi huấn luyện các mô hình lớn, OmniForce có thể hỗ trợ tương tác giữa môi trường sản xuất và môi trường siêu tính toán.

**Tối ưu hóa Suy luận** Do thiếu các cân nhắc triển khai, nhiều framework AutoML chỉ xem xét hiệu suất của mô hình kết quả ở phía tìm kiếm và bỏ qua hiệu suất của mô hình trong môi trường triển khai thực tế. Trên các thiết bị biên như ARM [60] và ROCm [61], OmniForce giải quyết vấn đề này bằng cách sử dụng TVM [62], một trình biên dịch deep learning cho phép ML hiệu suất cao ở mọi nơi. Chúng tôi kết hợp TVM để thiết lập kết nối giữa môi trường huấn luyện và triển khai thông qua sidecar nhiệm vụ và hoàn thành quá trình tìm kiếm cộng tác của nhiệm vụ AutoML trong các bài kiểm tra huấn luyện và triển khai thông qua phương pháp relay. Chiến lược tìm kiếm toàn diện này giúp OmniForce tìm các mô hình xuất sắc trong sản xuất. OmniForce cũng sử dụng các công cụ khác nhau cho các thiết bị cụ thể, như TensorRT [63] cho GPU NVIDIA và OpenVINO [64] cho CPU Intel. Hơn nữa, để chuyển đổi mô hình của chúng tôi giữa các framework học máy khác nhau, OmniForce sử dụng ONNX [65] như một trung gian. Trong những tình huống này, chúng tôi chuyển đổi mô hình của framework cấp cao như PyTorch thành định dạng ONNX, một định dạng tệp chung cho các mô hình học máy, và sau đó chuyển đổi thêm thành TensorRT hoặc OpenVINO để tối ưu hóa mục tiêu suy luận deep learning trên các thiết bị khác nhau.

#### 3.2.3 Đăng ký Thuật toán Ứng dụng

Sau khi người dùng triển khai thuật toán ứng dụng của họ với giao diện tiêu chuẩn của chúng tôi và chuẩn bị cấu hình tốt, thuật toán này có thể được đóng gói trong một docker image tự chứa và đăng ký trong hệ thống crowdsourcing OmniForce. Hệ thống của chúng tôi thực hiện một bài kiểm tra smoke để xác thực tính đầy đủ của thuật toán, có ba giai đoạn chính: tìm kiếm, suy luận và triển khai.

1. Bài kiểm tra tìm kiếm xác minh liệu thuật toán đã cho có thể được tìm kiếm bằng chiến lược tìm kiếm AutoML của OmniForce hay không. Cấu hình không gian tìm kiếm được kiểm tra trong giai đoạn này.

2. Bài kiểm tra suy luận xác minh liệu thuật toán có thể thực hiện suy luận hàng loạt ngoại tuyến hay không. Cấu hình suy luận được kiểm tra trong giai đoạn này.

3. Bài kiểm tra triển khai xác minh liệu thuật toán có thể được triển khai và tạo ra API tương ứng hay không. Cấu hình triển khai được kiểm tra trong giai đoạn này.

Nói chung, OmniForce đánh giá bốn khả năng của thuật toán được kiểm tra: khả năng tìm kiếm, suy luận hàng loạt, triển khai đám mây và tối ưu hóa phía biên. Đáng chú ý, tất cả các bài kiểm tra trên không cần thiết phải vượt qua, nhưng việc vượt qua càng nhiều càng tốt được khuyến nghị để thuật toán có thể được áp dụng trong nhiều tình huống hơn. Ví dụ, nếu một thuật toán chỉ vượt qua bài kiểm tra tìm kiếm, nó chỉ có thể được sử dụng cho tối ưu hóa siêu tham số nhưng không thể được triển khai trong môi trường sản xuất. Sau khi vượt qua bài kiểm tra smoke, các thuật toán của người dùng được crowdsourced trên OmniForce với kiểm soát phiên bản. Cả docker image và cấu hình đều được kiểm soát phiên bản để cập nhật agile trong tương lai.

#### 3.2.4 Tạo Pipeline

Tương tự như pipeline AutoML được mô tả trong Phần 2.1.7, hệ thống crowdsourcing OmniForce áp dụng phương pháp điều khiển pipeline để tự động áp dụng các thuật toán được cung cấp và crowdsourced. Pipeline được tạo ra có thể hỗ trợ linh hoạt các nhiệm vụ khác nhau. Ví dụ, một người dùng có thể cần một mô hình được tăng cường bởi một mô hình lớn để chạy trên các thiết bị có tài nguyên hạn chế, như các thiết bị biên. OmniForce có thể sử dụng một pipeline bao gồm hai bước, bao gồm thích ứng mô hình lớn và tối ưu hóa đa mục tiêu, để tạo ra các mô hình hiệu quả.

### 3.3 (Large) MaaS

Tận dụng sức mạnh của các mô hình lớn, xu hướng gần đây trong cộng đồng AI đã làm sáng tỏ việc cải thiện hiệu suất do việc mở rộng quy mô mang lại. Các mô hình lớn đã đạt được thành công trong các sản phẩm AI như ChatGPT [14] và Pathways [34]. Tuy nhiên, các mô hình lớn chịu gánh nặng tiêu thụ bộ nhớ và tính toán lớn. Tính thực tế có thể bị cản trở khi các mô hình lớn được triển khai trên các thiết bị biên hoặc áp dụng cho các tình huống có tài nguyên tính toán và bộ nhớ hạn chế. Đặc biệt khi tình huống ứng dụng yêu cầu độ trễ thấp, như lái xe tự động, một mô hình lớn với tốc độ suy luận thấp không thể đảm bảo dự đoán trực tuyến chính xác, điều này không thể tránh khỏi gây ra các vấn đề an toàn. Do khả năng lớn và hiệu suất tiên tiến của chúng, các mô hình lớn thể hiện khả năng tổng quát hóa cao trên một số lượng lớn các nhiệm vụ. Trong thực tế, điều này thường đi kèm với chi phí pretraining cao, làm cho các mô hình này không phù hợp cho các ứng dụng liên quan đến các bước thích ứng và cập nhật mô hình thường xuyên trong các trường hợp công nghiệp. Do đó, khi sử dụng các mô hình lớn trong những miền này, cần có các quy trình thích ứng và thu nhỏ để tăng tốc độ lặp lại và suy luận của các mô hình trong khi duy trì hiệu suất của chúng ở mức tối đa có thể. OmniForce hỗ trợ các công nghệ thích ứng và thu nhỏ mô hình lớn, chủ yếu bao gồm thích ứng tự động, lọc và chưng cất kiến thức với tối ưu hóa suy luận mô hình, như hiển thị trong Hình 5.

#### 3.3.1 Hỗ trợ Mô hình Lớn

OmniForce hỗ trợ công nghệ mô hình lớn. Hiện tại, AI đối mặt với nhiều ngành công nghiệp và tình huống kinh doanh cũng như nhu cầu của họ; ví dụ, mọi người cần thiết kế kiến trúc nơ-ron, điều chỉnh siêu tham số và triển khai mô hình dựa trên các yêu cầu phần cứng cho mỗi tình huống cụ thể. Khái niệm mô hình lớn là một công nghệ đột phá cho AI đa năng nhằm giải quyết vấn đề phân mảnh của các ứng dụng AI. OmniForce hỗ trợ công nghệ mô hình lớn với thích ứng thống nhất và hiệu quả cao trong các nhiệm vụ thị giác máy tính 3.5.2 và NLP 3.5.3, cũng như thích ứng tự động 3.3.2 và thu nhỏ 3.3.3. Người dùng có thể tìm kiếm và huấn luyện các mô hình lớn của riêng họ thông qua giao diện crowdsourcing hoặc mô hình lớn được cung cấp bởi OmniForce. Phương pháp gọi mô hình thông qua API đơn giản và dịch vụ của mô hình lớn làm giảm rào cản cho người dùng truy cập, do đó rút ngắn chu kỳ phát triển và lặp lại sản phẩm AI. Quy trình làm việc mô hình lớn được chứng minh trong Hình 6. Theo meta kiến thức hệ thống và các tương tác của người dùng, OmniForce quyết định có sử dụng mô hình lớn hay nhỏ và khi nào tái sử dụng hoặc cập nhật mô hình trong thực tế.

#### 3.3.2 Thích ứng

Với sự phát triển của các bộ tăng tốc hiệu suất cao, kích thước của các mô hình đã tăng theo cấp số nhân [4]. Do thiếu tài nguyên được gắn nhãn, để huấn luyện các mô hình lớn như vậy, một số phương pháp tự giám sát đã đạt được thành công lớn, như MAE [66] và masked language modeling [4]. Mô hình pretrained tự giám sát kết quả sau đó được fine-tuned để thích ứng với nhiệm vụ và tập dữ liệu cụ thể. Một số checkpoint pretrained nổi tiếng, như bidirectional encoder representations from transformers (BERT) [67] và Clip [68], đã trở nên phổ biến, tạo điều kiện cho một loạt các ứng dụng downstream. Tuy nhiên, dưới xu hướng này, với một tập dữ liệu downstream thực tế, việc huấn luyện một mô hình từ đầu hoặc thường xuyên fine-tune toàn bộ mô hình lớn cho mỗi nhiệm vụ là khó khăn do chi phí tính toán và lưu trữ lớn cần thiết. Để giải quyết những vấn đề này, một số phương pháp điều chỉnh tham số hiệu quả đã thu hút sự quan tâm của các nhà nghiên cứu. Ba nhánh ấn tượng nhất của lĩnh vực này là Prompt [4], Adapter [69], và phân tích bậc thấp [70], được hỗ trợ trong OmniForce. Ngoài ra, với sự giúp đỡ của cơ sở kiến thức được duy trì bởi hệ thống, formatter 2.1.6 có thể tự động tạo ra một pipeline phù hợp để chọn và tìm kiếm các module thích ứng thích hợp cho các nhiệm vụ downstream và mô hình lớn. Sau quá trình thích ứng, mô hình lớn phục vụ người dùng thông qua API đơn giản.

#### 3.3.3 Thu nhỏ

Trong khi các mô hình lớn đã cho thấy sức mạnh lớn trong nhiều nhiệm vụ, một số tình huống nhất định, như môi trường công nghiệp thực tế, yêu cầu chạy mô hình với tài nguyên hạn chế, như các thiết bị biên Internet of Things (IoT). Trong trường hợp này, thời gian suy luận của một mô hình lớn có thể chiếm một phần lớn của toàn bộ hệ thống, dẫn đến thời gian phản hồi lớn. Do đó, các nhà phát triển thích một mô hình tạo thành sự đánh đổi giữa hiệu suất và tốc độ. OmniForce hỗ trợ hai cách để tạo ra các mô hình hiệu quả được hỗ trợ bởi các mô hình lớn. Một cách là thu nhỏ một mô hình dung lượng cao dưới các nhiệm vụ downstream, và cách khác là chuyển giao kiến thức trong giai đoạn pretraining (upstream) từ các mô hình lớn.

**Thu nhỏ trong Giai đoạn Downstream** Lọc đã là một kỹ thuật phổ biến để giảm tham số và chi phí tính toán của các mô hình lớn. Các phương pháp hiện tại chủ yếu học cách loại bỏ một số kết nối hoặc kênh không quan trọng, còn được gọi là pruning hoặc nén mạng nơ-ron [71]. Biến lọc có thể được đại diện bởi một mask nhị phân, trong đó 1 chỉ ra rằng kết nối hoặc kênh tương ứng được giữ lại; nếu không, nó nên được pruned. Một mask được học để tạo ra tính thưa thớt, dẫn đến số lượng tham số và tính toán nhỏ hơn. Sau khi lọc các kết nối và kênh không quan trọng, mô hình nhỏ hơn thường được fine-tuned trên nhiệm vụ huấn luyện gốc để khôi phục sự suy giảm hiệu suất. Gần đây, các nghiên cứu lọc trên transformers đã học một mask thưa thớt để chỉ giữ lại một phần nhỏ tổng số token để tạo ra gia tốc [72]. Chưng cất kiến thức được phát triển để chưng cất kiến thức phong phú của một mô hình pretrained, được gọi là teacher, vào một mô hình mới, được gọi là student [73]. Nói chung, mô hình teacher là một mô hình lớn với số lượng tham số lớn. Nó có tốc độ suy luận chậm nhưng có hiệu suất cao. Mô hình student là một mô hình dung lượng nhỏ được tạo thành bởi thiết kế thủ công hoặc NAS [74,75,76,77]. Nó chạy nhanh, nhưng huấn luyện trực tiếp nó sẽ không mang lại hiệu suất thỏa mãn. Áp dụng kỹ thuật chưng cất kiến thức [78], chúng tôi sử dụng mô hình lớn làm teacher và huấn luyện mô hình nhỏ với giám sát bổ sung từ đầu ra biểu diễn thu được từ lớp cuối cùng của mô hình lớn. Mô hình nhỏ được huấn luyện với chưng cất kiến thức hoạt động tốt hơn phiên bản gốc của nó. Chúng ta có thể có độ trễ thấp và hiệu suất cao khi triển khai mô hình nhỏ trên các thiết bị biên.

**Thu nhỏ trong Giai đoạn Upstream** Các mô hình nhỏ luôn bị giới hạn bởi khả năng thấp của chúng trong việc hấp thụ kiến thức từ các tập dữ liệu lớn, trong khi các mô hình lớn được huấn luyện với dữ liệu lớn có khả năng chuyển giao tốt hơn cho các nhiệm vụ downstream. Tuy nhiên, pretraining một mô hình lớn với các tập dữ liệu lớn tốn kém. Thực hiện chưng cất trong giai đoạn pretraining là một cách mới để giải quyết vấn đề này [79,80]. OmniForce hỗ trợ phương pháp thu nhỏ bằng cách sử dụng kỹ thuật chưng cất kiến thức trong giai đoạn pretraining mà không huấn luyện các mô hình lớn từ đầu. Một pipeline như vậy được thiết lập trong OmniForce để tạo điều kiện cho việc chia sẻ các mô hình lớn và tạo ra các mô hình lặp lại nhanh và thường xuyên.

### 3.4 Framework Chiến lược Tìm kiếm Linh hoạt

Chiến lược tìm kiếm là một trong những phần quan trọng nhất của kỹ thuật AutoML, vì nó thực hiện toàn bộ quá trình tìm kiếm với một không gian tìm kiếm đã cho. Một chiến lược tìm kiếm được thiết kế tốt có xu hướng là chìa khóa của hiệu quả và hiệu suất. Trong phần này, chúng tôi giới thiệu giao diện của chiến lược tìm kiếm để hiển thị vai trò của nó trong framework của chúng tôi cũng như một framework tìm kiếm linh hoạt trong OmniForce.

#### 3.4.1 Giao diện Chiến lược Tìm kiếm

Trong framework của chúng tôi, không có bất kỳ hạn chế bổ sung nào khác, giao diện của chiến lược tìm kiếm bao gồm hai hàm, generate_tasks và handle_rewards, và một đối tượng không gian tìm kiếm, như Listing 3 hiển thị. Hàm trước được sử dụng để tạo ra các siêu tham số phù hợp nhất cho không gian đã cho theo các quan sát. Khi tất cả các nhiệm vụ được hoàn thành hoặc timeout (xem Phần 2.1.1) trong lần lặp hiện tại, hàm sau được gọi để xử lý phần thưởng của các nhiệm vụ này và cập nhật các quan sát cho lần lặp tiếp theo.

```python
class SearchStrategy(metaclass=ABCMeta):
    """ This is the abstract base class for OmniForce search strategies. """
    
    def bind_space(self, search_space):
        self.search_space = search_space
    
    @abstractmethod
    def generate_tasks(self):
        pass
    
    @abstractmethod
    def handle_rewards(self, rewards):
        pass
```

#### 3.4.2 Framework Chiến lược Tìm kiếm

OmniForce hỗ trợ nhiều chiến lược tìm kiếm như hyperband được sửa đổi [32], MF-NAS [33], BO mới và các phương pháp tiến hóa. BO là một phương pháp hiệu quả mẫu nhằm tìm x∗= arg minx∈χf(x), trong đó f là một hàm hộp đen tốn kém để đánh giá và χ là không gian tìm kiếm hoặc miền [81]. BO bao gồm hai thành phần chính: một mô hình surrogate để mô hình hóa bề mặt phản hồi của f và một hàm thu thập tạo thành sự đánh đổi khai thác-khám phá. Trong khi nhiều thư viện đã được phát triển cho BO, như Spearmint [82], GPyOpt [83], scikit-optimize [84], RoBO [85], ProBO [86], GPyTorch [87] và BoTorch [88], tất cả đều tập trung vào khai thác một khía cạnh nhất định, và không có framework toàn diện nào có thể chứa tất cả chúng. Ví dụ, một số phương pháp BO hàng loạt tiên tiến, như local penalization (LP) [89], không thể được triển khai dễ dàng trong các phương pháp được đề cập ở trên ngoại trừ GPyOpt.

Để chứa các thuật toán BO ngày càng tiên tiến, OmniForce sử dụng một framework BO có thể kết hợp duy trì năm tập hợp thành phần chính, bao gồm một mô hình surrogate, một hàm thu thập, một bộ tối ưu hóa thu thập, một bộ tạo ứng viên và một suggester. Tương tự như BoTorch, framework của chúng tôi được xây dựng trên Pytorch [90] và hưởng lợi từ auto-differentiation và tăng tốc GPU, và tổng quan về framework của chúng tôi được hiển thị trong Hình 7. Đối với mô hình surrogate, chúng tôi triển khai các phương pháp phổ biến nhất, như GP [82], SMAC [91], BNN [92,93], và một phương pháp mới (OF) để giải quyết các vấn đề tối ưu hóa rời rạc.

Mô hình surrogate OF được đề xuất của chúng tôi với hàm thu thập OF đạt được độ chính xác tiên tiến trên NAS-Bench-201 [94]. Hơn nữa, xem xét các vấn đề tối ưu hóa đa nhiệm vụ và chiều cao, chúng tôi thiết kế OF-Trans và OF-HD, tương ứng, và triển khai một số phương pháp hiện có, điều này không có sẵn trong các thư viện BO khác. Về các hàm thu thập, ngoài các hàm cơ bản phổ biến như expected improvement (EI) [95], lower confidence bound (LCB) [96] và entropy search (ES) [97], chúng tôi cũng hỗ trợ một số kỹ thuật thu thập hàng loạt mạnh mẽ như LP và các hàm thu thập Monte Carlo. Để cung cấp một lô truy vấn mới, một số logic bổ sung thường cần thiết, điều này truyền cảm hứng cho chúng tôi định nghĩa một thành phần mới trong framework BO được gọi là suggester. Trong giai đoạn suy luận, suggester tiếp quản tất cả các thành phần khác và thực hiện quy trình tạo ra cho các truy vấn mới. Ví dụ, trong hầu hết các thiết lập BO hàng loạt, chúng ta cần nhìn trước những nhiệm vụ đang chờ và fine-tune mô hình surrogate sử dụng các mẫu MC [82] hoặc constant liar [98] để thu được các truy vấn mới. Để tối ưu hóa hàm thu thập, chúng tôi giới thiệu một bộ tạo ứng viên để lấy mẫu các ứng viên đủ như các điểm bắt đầu cho bộ tối ưu hóa lbfgs, là lựa chọn phổ biến nhất trong bối cảnh BO. Các phương pháp lấy mẫu khác nhau có thể được cắm vào dễ dàng, và không gian lấy mẫu thay đổi với không gian tìm kiếm để thích ứng với một số phương pháp thu hẹp không gian tìm kiếm.

Trong giai đoạn huấn luyện, chúng tôi sử dụng các quan sát để khớp mô hình surrogate thông qua các phương pháp MCMC hoặc dựa trên gradient. Khi một mô hình surrogate được huấn luyện được cung cấp, bộ đề xuất kiểm soát bộ tạo để tạo ra các ứng viên và tối ưu hóa hàm thu thập theo posterior và bộ tối ưu hóa thu thập. Sau đó, với các truy vấn mới được đề xuất bởi requester gửi đến cơ sở dữ liệu, các worker song song dự trữ các nhiệm vụ mới này để đánh giá và gửi các quan sát trở lại cho lần lặp tiếp theo cho đến khi ngân sách cạn kiệt.

Tóm lại, so với các thư viện BO hiện có khác, các đóng góp của chúng tôi cho framework BO được nổi bật như sau.

• Linh hoạt và có thể kết hợp.
• Các phương pháp mới cho một số vấn đề.
• Auto-differentiation và tăng tốc GPU.

### 3.5 Thuật toán Ứng dụng Được Cung cấp Rộng rãi

Các ứng dụng AI có thể được tìm thấy trong sản xuất công nghiệp và cuộc sống hàng ngày của chúng ta. Sự phổ biến của AI này được phản ánh không chỉ trong sự đa dạng của các tình huống ứng dụng có sẵn, bao gồm cộng tác đám mây-biên, tích hợp VR và AI môi trường mở, mà còn trong sự đa dạng của các thuật toán ứng dụng được sử dụng, như phân tích và xử lý dữ liệu bảng, NLP, thị giác máy tính, AIGC và học biểu diễn đồ thị. Vì sự đa dạng tình huống đã được giới thiệu ở trên, chương này chủ yếu tập trung vào thuật toán ứng dụng được cung cấp rộng rãi trong OmniForce.

#### 3.5.1 Dữ liệu Dạng bảng

Phân tích dữ liệu dạng bảng là một chủ đề lâu đời thực hiện phân tích liên kết trên dữ liệu có cấu trúc và khai thác các mối quan hệ kinh doanh phức tạp thông qua kết hợp đặc trưng và trích xuất đặc trưng. Các nhiệm vụ phổ biến trong lĩnh vực này bao gồm phân tích chuỗi thời gian[99][100], phát hiện bất thường[101][102], dự báo tỷ lệ nhấp qua[103][104], v.v.

Là một ứng dụng AI dưới framework HAML, OmniForce khuyến khích người dùng tập trung nhiều hơn vào thu thập dữ liệu, pipeline đặc trưng tùy chỉnh và thiết kế mô hình, có thể mang lại hiệu suất cải thiện thông qua kinh nghiệm con người. Đối với phân tích dữ liệu khám phá cơ bản, làm sạch dữ liệu, điền null, mã hóa danh mục và các quy trình tẻ nhạt nhưng cần thiết khác, kỹ thuật đặc trưng tự động dựa trên meta-learning tự động hoàn thành các nhiệm vụ trên để giảm gánh nặng đặt lên con người. Nói chung, kỹ thuật đặc trưng tự động chủ yếu được chia thành ba phần: làm sạch và điền, biến đổi đặc trưng và kết hợp đặc trưng. OmniForce cung cấp các phương pháp xử lý tiêu chuẩn khác nhau cho các loại đặc trưng khác nhau như không gian tìm kiếm và sau đó thu được các phương pháp xử lý phù hợp nhất cho dữ liệu đã cho. Ví dụ, đối với các đặc trưng số, các phương pháp được sử dụng để điền các giá trị trống bao gồm lấy trung bình, trung vị, tứ phân vị trên và tứ phân vị dưới; các phương pháp biến đổi đặc trưng bao gồm max-min, z score và chuẩn hóa log scale; các phương pháp kết hợp đặc trưng bao gồm nhân và chia và các biến đổi toán học thông thường khác.

Các mô hình ensemble dựa trên cây quyết định, như random forest [105] và LightGBM [106], luôn được ngành công nghiệp ưa chuộng do chi phí tính toán thấp và khả năng giải thích cao. Trong những năm gần đây, các đột phá trong mạng nơ-ron đã thu hút sự chú ý đến các mô hình deep learning đã đạt được kết quả ấn tượng trong các nhiệm vụ như hệ thống đề xuất và dự đoán tỷ lệ nhấp qua[103][107]. Chúng tôi sử dụng cả mô hình ensemble dựa trên cây và mô hình deep learning để xây dựng một không gian tìm kiếm nhằm đảm bảo hiệu suất xuất sắc trên các tập dữ liệu thách thức khác nhau.

**Dữ liệu Dạng bảng Lớn** Trong thời đại dữ liệu lớn hiện tại, các thuật toán ML được sử dụng để phân tích lượng lớn dữ liệu, phục vụ cuộc sống con người và sản xuất công nghiệp và mang lại giá trị to lớn[108]. Do đó, khả năng huấn luyện hiệu quả các mô hình trên các tập dữ liệu dạng bảng lớn là một vấn đề cạnh tranh cao cho các công ty IT.

So với huấn luyện một mô hình sâu trên GPU, việc đọc hàng trăm GB dữ liệu từ đĩa vào bộ nhớ là dư thừa và rất tốn thời gian cho mỗi worker/task estimator. Do đó, chúng tôi đề xuất một thiết kế hệ thống có tên double shared memory để tăng tốc quá trình huấn luyện trên dữ liệu dạng bảng lớn trong một nút. Một mặt, chúng tôi lưu một phần dữ liệu huấn luyện vào /dev/shm của hệ thống Linux, mount nó vào nhiều container worker, và sử dụng bộ nhớ truy cập tốc độ cao để tải dữ liệu nhanh chóng. Theo cách này, các task estimator của cùng một nút có thể chia sẻ cùng dữ liệu trong bộ nhớ, mà chúng tôi gọi là outer shared memory. Mặt khác, do kỹ thuật đặc trưng tự động, dữ liệu được sử dụng bởi mỗi task estimator là khác nhau. Để tránh chi phí thời gian của kỹ thuật đặc trưng, chúng tôi cho phép một cơ chế chia sẻ được gọi là inner shared memory. Epoch đầu tiên thực hiện kỹ thuật đặc trưng và huấn luyện mô hình theo cách batch-by-batch và ghi dữ liệu đã xử lý vào container, cho phép các epoch tiếp theo tiết kiệm thời gian trong quá trình tải tệp và kỹ thuật đặc trưng.

Phương pháp chia sẻ bộ nhớ hiệu quả này được sử dụng cho việc tải dữ liệu có thể được áp dụng rộng rãi cho các mô hình deep learning như multilayer perceptron (MLP). Ngoài ra, đối với các nút có lượng bộ nhớ nhỏ, bộ lập lịch có thể chỉ sử dụng inner hoặc outer shared memory để tăng tốc linh hoạt quá trình huấn luyện.

**Dữ liệu Chuỗi Thời gian** Là một trong những vấn đề dữ liệu dạng bảng thách thức nhất với nhiều ứng dụng, dự báo chuỗi thời gian đã là một trong những lĩnh vực nghiên cứu chính mà cộng đồng AI đã cố gắng giải quyết với ML và deep learning[109][110].

Tuân thủ triết lý hướng con người, chúng tôi mở ra các tương tác như khoảng thời gian chuỗi thời gian và độ dài dự báo. Người dùng có thể tạo ra các mô hình tùy chỉnh dựa trên đặc điểm dữ liệu và tình huống kinh doanh theo kinh nghiệm của họ. Tương tự, kỹ thuật đặc trưng tự động làm sạch và điền dữ liệu, xác định các trường hợp đa chuỗi, nhóm chúng và tạo ra các đặc trưng thời gian như cửa sổ thời gian trượt và lag.

Không gian tìm kiếm bao gồm các mô hình autoregressive truyền thống[111], mô hình dựa trên cây[9][106], và các mô hình dựa trên transformer được đề xuất gần đây[112]. Formatter trong phần 2.1.6 gán trọng số cho các mô hình cho các tập dữ liệu với thống kê khác nhau theo thông tin lịch sử có sẵn để đảm bảo hiệu suất tối ưu.

#### 3.5.2 Thị giác Máy tính

Thị giác máy tính, là lĩnh vực nghiên cứu chính của AI, nhằm trích xuất, xử lý và hiểu thông tin chứa trong hình ảnh và video kỹ thuật số. Các thuật toán thị giác máy tính truyền thống, như support vector machine (SVM) [113], thường giải quyết các nhiệm vụ cụ thể thông qua kỹ thuật đặc trưng thủ công. Hưởng lợi từ sơ đồ học dựa trên dữ liệu, các thuật toán deep learning đã đạt được thành công lớn trong các nhiệm vụ thị giác máy tính khác nhau. Một mặt, các chiến lược huấn luyện và suy luận end-to-end cho phép các mô hình deep learning dễ dàng thích ứng với các nhiệm vụ thị giác máy tính khác nhau. Mặt khác, với sự phát triển của các kiến trúc mạng nơ-ron sâu (ví dụ: AlexNet [114] và ResNet [1]), khả năng của các mô hình deep learning đang tăng nhanh chóng, và ngày càng nhiều mô hình deep learning được sử dụng trong các ứng dụng thực tế.

Mặc dù các mô hình deep learning, đặc biệt là các mạng nơ-ron tích chập sâu (CNN), đã chứng minh hiệu suất hứa hẹn trong các nhiệm vụ thị giác máy tính, vẫn tồn tại một khoảng cách tương đối lớn để các mô hình này trở thành các mô hình thị giác máy tính mạnh mẽ và chung. Gần đây, được hỗ trợ bởi sự tăng trưởng nhanh chóng của tài nguyên tính toán và lượng lớn dữ liệu hình ảnh, các mô hình siêu sâu đã thu hút sự chú ý ngày càng tăng từ cộng đồng thị giác máy tính [115,116,117,118,119,120]. Dựa vào khả năng học mạnh mẽ của chúng, các mô hình siêu sâu có thể học các biểu diễn chung và phân biệt. Ngoài ra, khả năng mô hình hóa mạnh mẽ của chúng cũng cho phép các mô hình siêu sâu thích ứng với tình huống mới với một lượng nhỏ dữ liệu được gắn nhãn [117,118,120]. Khả năng như vậy là thiết yếu trong các ứng dụng thực tế. Ví dụ, chúng ta có thể xác thực tính khả thi của các giải pháp kỹ thuật với chi phí ít hơn và do đó tăng tốc chu kỳ phát triển. Để giúp người dùng nhanh chóng phát triển và triển khai mô hình cho các ứng dụng cụ thể, chúng tôi cung cấp các mô hình thị giác siêu sâu khác nhau cho các nhiệm vụ thị giác khác nhau, ví dụ: phát hiện đối tượng 2D/3D [121,122], phân đoạn ngữ nghĩa [123,124], phát hiện đường và làn [125,126], matting hình ảnh [127,128], phát hiện điểm mấu chốt [129,118], phát hiện và nhận dạng văn bản cảnh [130,131,132]. Bằng cách cung cấp mô tả nhiệm vụ và một số mẫu được gắn nhãn, người dùng có thể dễ dàng thu được các mô hình cụ thể nhiệm vụ hiệu suất cao từ nền tảng. Hơn nữa, các mô hình siêu sâu thường yêu cầu tài nguyên lớn để triển khai, trong khi người dùng thích các mô hình nhẹ với ít tham số hơn và hiệu quả suy luận cao hơn. Để đáp ứng các yêu cầu như vậy, chúng tôi cung cấp một số giải pháp để nén và tăng tốc các mô hình này. Cụ thể hơn, chúng tôi phát triển các kỹ thuật nén mô hình hiệu quả, như các kỹ thuật dựa trên quantization và pruning, để compact và tăng tốc các mô hình. Hơn nữa, dựa vào một không gian tìm kiếm được thiết kế chứa các hoạt động, khối, hàm loss khác nhau, v.v., nền tảng của chúng tôi có thể tự động tìm kiếm một mô hình nhẹ và cải thiện hiệu suất của mô hình được tìm kiếm với sự hướng dẫn của các mô hình siêu sâu thông qua chưng cất kiến thức.

#### 3.5.3 Xử lý Ngôn ngữ Tự nhiên

Xử lý ngôn ngữ tự nhiên (NLP) là một trong những nhánh chính của AI nhằm tự động xử lý ngôn ngữ con người (cả nói và viết) bằng máy tính, và các nhiệm vụ liên quan thường được phân loại là trí thông minh nhận thức. NLP đã phát triển từ một số ngành học, ví dụ: khoa học máy tính, AI và ngôn ngữ học. NLP có thể được chia cơ bản thành hai danh mục: 1) Hiểu ngôn ngữ tự nhiên (NLU). NLU khám phá các chiến lược cho phép máy tính nắm bắt các hướng dẫn văn bản được cung cấp bởi người dùng. Các nhiệm vụ NLU phổ biến nhất bao gồm phân loại văn bản [133], phân tích tình cảm [134,135,136], trả lời câu hỏi [137,138], nhận dạng thực thể có tên [139,140], v.v. 2) Tạo ngôn ngữ tự nhiên (NLG). NLG cho phép máy tính tạo ra đầu ra văn bản sau khi hiểu đầu vào của người dùng bằng ngôn ngữ tự nhiên như tiếng Anh và tiếng Trung. Các nhiệm vụ NLG phổ biến bao gồm dịch máy [141,142,143,144,145], tóm tắt [146, 147], đối thoại [148, 149], v.v.

Mặc dù các mô hình NLP dựa trên deep learning đã chứng minh hiệu suất hứa hẹn trên một loạt các nhiệm vụ, mô hình học hiện tại thiếu khả năng tận dụng nhiều nhiệm vụ và dữ liệu, dẫn đến một loạt vấn đề, ví dụ: dư thừa huấn luyện mô hình cho các nhiệm vụ khác nhau, vấn đề đảo dữ liệu, vấn đề triển khai phức tạp và khả năng học kém trong các tình huống ít tài nguyên. Để giúp người dùng phát triển và triển khai hiệu quả các mô hình cho các ứng dụng khác nhau, hệ thống của chúng tôi bao gồm một loạt các mô hình nền tảng, bao gồm một mô hình siêu cho hiểu và tạo ngôn ngữ chung [150,151,152], một mô hình siêu cho tạo đa ngôn ngữ [153,154,155,156,157], và các phiên bản điều chỉnh và chưng cất hiệu quả của chúng [158], học prompt dựa trên chưng cất [159] và PESF-KD [160], tương ứng. Bằng cách cung cấp mô tả nhiệm vụ và một vài mẫu được gắn nhãn, người dùng có thể thu được mô hình/adapter nhỏ hiệu suất cao được điều chỉnh bởi các mô hình nền tảng tích hợp sẵn của chúng tôi từ nền tảng. Đáng khích lệ, với chiến lược điều chỉnh hiệu quả của chúng tôi về học prompt dựa trên chưng cất, mô hình nền tảng phía máy chủ của chúng tôi chỉ yêu cầu fine-tuning 0,5% tham số, tương đương với tham số của mô hình nền tảng gốc, trong khi đạt được hiệu suất tương đương hoặc thậm chí tốt hơn. Theo cách này, người dùng chỉ cần triển khai prompt/adapter nhỏ của họ, đây là một cách hiệu quả và riêng tư để cập nhật tăng dần prompt/adapter nhỏ phía người dùng mà không tải lên dữ liệu quý giá của họ lên nền tảng của chúng tôi.

#### 3.5.4 Học Mô hình Tạo sinh Sâu Đa phương thức

Một trong những mục tiêu chính của trí tuệ nhân tạo và học máy là học và thao tác các phân phối xác suất chiều cao của dữ liệu thế giới thực [161]. Bằng cách làm như vậy, các công nghệ này có thể trích xuất những hiểu biết có giá trị từ dữ liệu có thể được sử dụng để cải thiện nhiều nhiệm vụ liên quan [162]. Trong những năm gần đây, các mô hình tạo sinh sâu đã nổi lên như một phương tiện mạnh mẽ để học phân phối dữ liệu. Các mô hình này, bao gồm generative adversarial network (GAN) [163,164,165,166], Vector-Quantized Variational Autoencoder (VQ-VAE) [167,168], mô hình autoregressive [169,170], và mô hình khuếch tán [171,172], đã chứng minh khả năng ấn tượng trong một loạt ứng dụng rộng. Bằng cách học phân phối xác suất cơ bản đã tạo ra dữ liệu, các nghiên cứu có thể hiểu biết về cơ chế cơ bản của quá trình tạo dữ liệu. Hơn nữa, các mô hình tạo sinh được huấn luyện tốt có thể được sử dụng rộng rãi trong các nhiệm vụ liên quan đến tạo nội dung.

Hệ thống của chúng tôi bao gồm một loạt các mô hình tạo sinh sâu tích hợp sẵn, được thiết kế để cải thiện tính thực tế của nội dung được tạo ra và cung cấp một mô hình tạo sinh có thể xử lý các nhiệm vụ tạo nội dung chung. Bằng cách học sự liên kết đặc trưng giữa các phương thức khác nhau, các mô hình này có thể tạo ra nội dung đa dạng và chất lượng cao hơn. Để đạt được những mục tiêu này, chúng tôi đã phát triển một tập hợp các thuật toán tiên tiến có thể huấn luyện và áp dụng các mô hình tạo sinh này cho nhiều ứng dụng thực tế. Các thuật toán này đã được thiết kế để tăng cường hiệu suất của các mô hình tạo sinh trong các nhiệm vụ như khám phá khái niệm thị giác, khả năng kiểm soát tạo sinh và đa dạng nội dung. Tổng thể, các mô hình và thuật toán tích hợp sẵn của chúng tôi đã được sử dụng trong một số nhiệm vụ nội dung được tạo bởi trí tuệ nhân tạo (AIGC), bao gồm tạo thị giác-ngôn ngữ [173], tạo cảnh phức tạp [174], hoạt hình chân dung [175,176], kết xuất đối tượng 3D [177], v.v. Chúng tôi tin rằng hệ thống của chúng tôi có tiềm năng cách mạng hóa lĩnh vực tạo nội dung và mở đường cho các ứng dụng mới, sáng tạo của công nghệ AIGC trong tương lai.

#### 3.5.5 Học Biểu diễn Đồ thị

Dữ liệu đồ thị có mặt khắp nơi quanh chúng ta; các ví dụ bao gồm đồ thị xã hội, đồ thị kiến thức và cấu trúc protein. Thông thường, một đồ thị bao gồm các nút và cạnh kết nối các nút. Thậm chí các câu và hình ảnh có thể được biểu diễn bằng đồ thị: các từ trong câu và các patch của hình ảnh có thể được coi như các nút, và các kết nối giữa các nút đại diện cho các cạnh của chúng. Xem xét rằng đồ thị có mặt khắp nơi, việc phân tích dữ liệu đồ thị và học biểu diễn đồ thị để giải quyết các ứng dụng cấp nút, cấp cạnh và cấp đồ thị là quan trọng.

Mạng nơ-ron đồ thị (GNN) là các mạng nơ-ron hoạt động trong miền đồ thị. Hệ thống của chúng tôi cung cấp nhiều GNN tích hợp sẵn, như graph convolutional network (GCN) [178], graph attention network (GAT) [179], và graph transformer [180]. Ví dụ, hệ thống của chúng tôi cung cấp một loại graph transformer hiệu quả, tức là Gapformer, tích hợp sâu graph pooling vào graph transformer. Bằng cách sử dụng Gapformer, tác động tiêu cực của việc có một số nút không liên quan được giảm thiểu trong khi thông tin tầm xa được bảo tồn, và độ phức tạp bậc hai của message passing được giảm xuống độ phức tạp tuyến tính. Ngoài ra, hệ thống của chúng tôi phát triển nhiều module plugin đa dạng để cải thiện khả năng của GNN. Ví dụ, hệ thống của chúng tôi áp dụng SkipNode [181], lấy mẫu các nút đồ thị trong mỗi lớp tích chập để bỏ qua hoạt động tích chập, do đó giảm thiểu các vấn đề oversmoothing và gradient vanishing của các mạng dựa trên GCN. Hệ thống của chúng tôi cũng sử dụng sơ đồ plug-and-play cho graph pooling, được gọi là MID, với không gian điểm đa chiều và hai hoạt động điểm, để khám phá sự đa dạng của các đặc trưng nút và cấu trúc đồ thị trong đồ thị để đạt được biểu diễn cấp đồ thị được cải thiện. Hơn nữa, hệ thống của chúng tôi cũng cung cấp các ứng dụng đồ thị điển hình, bao gồm các cấu trúc mạng được thiết kế để học trên signed network embedding [182], GCN với học đa cấp để phân loại ảnh hyperspectral [183], và mạng heterophily để tạo đồ thị cảnh [184].

## 4 Tính năng

**Dễ sử dụng cho Cộng tác Phát triển-Triển khai** Cam kết xây dựng các mô hình phù hợp cho sản xuất, OmniForce thiết kế một framework xây dựng mô hình cộng tác phát triển-triển khai thực sự. Không giống như nhiều nền tảng AI chỉ có khả năng phát hành mô hình trong sản xuất theo cách agile với pipeline CI/CD, OmniForce bắc cầu môi trường phát triển và triển khai và áp dụng phương pháp tối ưu hóa đa mục tiêu để xây dựng các mô hình thực tế và đa năng hơn trong các giai đoạn tìm kiếm và huấn luyện. OmniForce nhằm cho phép cả các nhà phát triển có kiến thức ML hạn chế và các nhà khoa học dữ liệu triển khai dịch vụ mô hình của riêng họ chỉ với vài cú nhấp chuột.

**Khả năng Công nghiệp của Thích ứng Môi trường Mở** Không giống như các nền tảng AutoML thông thường, chúng tôi đề xuất OmniForce để nghiên cứu các vấn đề môi trường mở và vòng lặp mở vì dữ liệu, nhãn, đặc trưng, mô hình, đánh giá và chỉ số thường thay đổi trong quá trình học trong thực tế [185]. Trực giác của chúng tôi là chúng ta cần liên quan đến con người trong vòng lặp để tận dụng kiến thức con người và tăng cường khả năng con người dựa trên các tương tác mượt mà được hiển thị trong Hình 1 để đạt được mục tiêu của HAML.

**Sản xuất Cloud-Native và (Large) MaaS** Ngày càng nhiều hệ thống quy mô lớn được xây dựng thông qua container và được trang bị hệ thống điều phối container để quản lý tất cả các thành phần, vì các hệ thống này thường có các ưu điểm về sử dụng tài nguyên cao, cách ly mạnh và giao hàng liên tục. OmniForce được thiết kế dựa trên Kubernetes, có nghĩa là OmniForce là một hệ thống AutoML cloud-native hoàn toàn và có thể tận dụng nhiều công cụ cloud-native xuất sắc. Dựa trên Kubernetes và Kubeflow, OmniForce hỗ trợ đa người thuê, khả năng mở rộng cao, khả năng phục hồi thảm họa mạnh và chuyển đổi tự động từ mô hình được huấn luyện thành dịch vụ triển khai.

**Crowdsourcing** OmniForce hỗ trợ các thuật toán quy mô lớn và mở rộng tập hợp các thuật toán được áp dụng. Với một hệ thống được sử dụng rộng rãi trong một nhóm kỹ sư, trước đây có thể trực tiếp tìm kiếm và triển khai một nhiệm vụ mới trên tập dữ liệu mới thông qua các thuật toán ứng dụng crowdsourced. Để truyền cảm hứng cho khái niệm crowdsourcing, chúng tôi bắt đầu với quản lý phiên bản ML cho dữ liệu, nhãn, mô hình, thuật toán và không gian tìm kiếm; cộng tác phát triển và triển khai theo hướng pipeline; framework chiến lược tìm kiếm linh hoạt; và cung cấp rộng rãi các thuật toán ứng dụng bao gồm các phương pháp dựa trên mô hình siêu sâu (lớn).

## 5 Đánh giá

Phần này chứa ba phần. Phần đầu tiên đưa ra giới thiệu ngắn gọn về các đổi mới được cung cấp bởi metaverse công nghiệp. Phần thứ hai trình bày một tập hợp các trường hợp sử dụng trong tình huống metaverse công nghiệp thực tế hướng con người, hiển thị hoạt động thực tế của mô phỏng XR, thu thập dữ liệu liên tục, crowdsourcing và cộng tác đám mây-biên để giải quyết các vấn đề AI vòng lặp mở. Cuối cùng, chúng tôi chứng minh khả năng của OmniForce thông qua một số thí nghiệm về khả năng mở rộng, chịu lỗi, hiệu suất tìm kiếm, hiệu suất thuật toán và thực hành AutoML hướng con người.

### 5.1 Đổi mới của Metaverse Công nghiệp

Các dây chuyền lắp ráp tiêu chuẩn đã sử dụng các công nghệ AI để cải thiện hiệu quả sản xuất của một nhà máy đơn lẻ. Những tiến bộ gần đây trong công nghệ metaverse công nghiệp sẽ đưa việc áp dụng AI lên cấp độ tiếp theo và thay đổi cấu trúc của toàn bộ chuỗi cung ứng. Metaverse công nghiệp đã tạo thành một mô hình sản xuất mới, khuyến khích sự hợp tác giữa các nhà máy và đẩy nhanh kết nối giữa các phần upstream và downstream của chuỗi công nghiệp. Ví dụ, trong mô hình kinh doanh khách hàng-đến-nhà sản xuất (C2M) thông thường, các nhà sản xuất sản xuất các lô thử nghiệm thị trường nhỏ trước khi phát hành cuối cùng của sản phẩm và sau đó cải thiện thiết kế sản phẩm của họ dựa trên phản hồi thị trường hoặc tăng sản xuất khi sản phẩm bán tốt. Tuy nhiên, sự xuất hiện của metaverse sẽ thay đổi cấu trúc của mô hình kinh doanh C2M thông thường.

Với sự phát triển của trải nghiệm nhập vai trong thế giới ảo, nội dung kỹ thuật số từ nền kinh tế thực sẽ được sử dụng làm dữ liệu chính để giúp xây dựng thế giới kỹ thuật số. Công nghệ đã tái cấu trúc hình thức của chuỗi công nghiệp hiện có. Metaverse cho phép người tiêu dùng trải nghiệm sản phẩm và đưa ra quyết định mua hàng trong giai đoạn thiết kế sản phẩm, cho phép các nhà sản xuất thu được phản hồi chi tiết hơn trong giai đoạn này. Họ cải thiện thiết kế sản phẩm dựa trên phản hồi của khách hàng và thậm chí bán các dịch vụ nội dung kỹ thuật số (các nguồn doanh thu mới). Ngoài ra, sự xuất hiện của metaverse sẽ gây ra những thay đổi trong cấu trúc cung và cầu. Nó sẽ rút ngắn khoảng cách giữa các nhà sản xuất và khách hàng bằng cách loại bỏ các bước trung gian.

Trong phần tiếp theo, chúng tôi cung cấp một trường hợp sử dụng OmniForce để giúp sản xuất các phương tiện tự lái trong bối cảnh metaverse công nghiệp. Trong giai đoạn thiết kế, trải nghiệm và phản hồi của người dùng được tham gia. Trong giai đoạn phát triển và sản xuất của metaverse, OmniForce sử dụng công nghệ cộng tác đám mây-biên để thực hiện lặp lại sản phẩm nhanh chóng cũng như phản hồi để cải thiện hiệu suất và trải nghiệm người dùng của sản phẩm. Ngoài ra, OmniForce hỗ trợ trong việc tháo gỡ đơn hàng tự động, đặt hàng tự động, tìm nguồn cung ứng chuỗi cung ứng và kho thông minh trong toàn bộ dịch vụ chuỗi cung ứng.

### 5.2 Trường hợp Sử dụng của Chuỗi Cung ứng Công nghiệp và Metaverse Công nghiệp

**Thiết kế** Trong trường hợp này, các nhà sản xuất nhằm thiết kế các xe tự lái phù hợp với các tình huống khác nhau. Ví dụ, công dân từ các thành phố khác nhau có thể thích màu sắc và mẫu mã đa dạng trong xe của họ. Một số hình dạng nhỏ có thể phù hợp hơn để giao hàng và lái xe quanh cộng đồng hơn những chiếc xe tải lớn. Nhiệm vụ này có thể dựa vào khả năng AIGC của OmniForce. Trong nhiệm vụ tạo sinh này, khách hàng muốn công cụ tự động tạo ra các mô hình xe. Sau khi một tập dữ liệu mô hình xe được đưa vào hệ thống, OmniForce đưa ra một mô hình để tạo nội dung. Mô hình xe kết quả có thể được mô phỏng và kiểm tra bởi hệ thống XR với tương tác khách hàng. Bước này rút ngắn quá trình phản hồi người tiêu dùng và là một cách triển khai các chương trình thiết kế metaverse công nghiệp, tiết kiệm thời gian và tài nguyên ngân sách. Nếu kết quả thu được không đáp ứng yêu cầu của người tiêu dùng, người dùng có thể điều chỉnh phần thưởng của mô hình trên OmniForce, kích hoạt vòng lặp sản xuất mô hình một lần nữa cho đến khi kết quả có thể được sử dụng cho bước tiếp theo.

**Phát triển và Sản xuất** Trong trường hợp này, khách hàng đến từ một công ty giao hàng tiên tiến với thiết bị công nghệ mới được tăng cường bởi AI. Họ muốn chạy một tập hợp các mô hình trên các tình huống khác nhau trên phương tiện thông minh của họ, liên quan đến các thiết bị lớn với các yêu cầu triển khai (suy luận) khác nhau trong tình huống công nghiệp thực tế này. Ví dụ, họ cần ba mô hình với các yêu cầu khác nhau:

• một xe tải di chuyển giữa các thành phố;
• một xe bán tải lái trên các đường phố chính của thành phố;
• và một xe giao hàng nhỏ di chuyển qua các cộng đồng và tòa nhà;

Cụ thể, khi lái trên đường cao tốc giữa các thành phố, xe tải có thể đạt tốc độ 80 km/h hoặc cao hơn trong điều kiện đường tương đối sạch. Do đó, mô hình được xây dựng cần phản hồi nhanh chóng và nhạy bén với các chướng ngại vật phía trước xe tải. Trong khi chạy trên các đường phố chính, mô hình được triển khai trên xe bán tải nên xử lý các điều kiện đường phức tạp, như người đi bộ, xe đạp, xe máy và thú cưng. Ngược lại, các phương tiện chuyển phát nhanh nhỏ thường di chuyển qua các cộng đồng và tòa nhà với tốc độ thấp nhưng có thể lái trên đường băng trong thời tiết cực đoan.

Để giải quyết những thách thức này, OmniForce liên tục áp dụng một pipeline tự động bao gồm thu thập dữ liệu và tìm kiếm mô hình để trích xuất giá trị từ các giai đoạn phát triển, triển khai và bảo trì. Để thu thập dữ liệu, chúng ta có thể thu thập một tập hợp các hình ảnh ví dụ từ các tập dữ liệu công cộng hoặc thế giới thực. Thông thường, chúng ta cần một số yêu cầu dữ liệu để làm cho mô hình AI học tốt. Một yêu cầu là các cảnh và các loại xe, người đi bộ và cây cối trên đường nên đa dạng. Sau khi tải lên dữ liệu lên OmniForce và thu được tập dữ liệu kết quả, chúng ta có thể thấy rằng "sedan" hoặc "đường thành phố trong ngày nắng" xuất hiện thường xuyên hơn nhiều trong hình ảnh so với các loại xe hoặc cảnh khác, dẫn đến vấn đề phân phối đuôi dài. Do đó, vòng lặp mở hướng con người được kích hoạt, và tín hiệu từ OmniForce là người dùng cần liên tục thu thập các loại dữ liệu khác, như xe thể thao, xe van và xe limousine. OmniForce cũng có thể tạo ra hình ảnh trong các loại cảnh khác nhau bao gồm đường cao tốc, xe đạp đông đúc, xe máy, đường ướt trong ngày mưa, bóng cây và trẻ em trong khu phố. Khi dữ liệu mới được thu thập, OmniForce cập nhật phiên bản dữ liệu để đáp ứng các yêu cầu cân bằng được áp đặt. Để tìm kiếm các mô hình dưới các ràng buộc khác nhau, OmniForce hỗ trợ cộng tác đám mây-biên cho các nhà phát triển có kiến thức ML hạn chế và các nhà khoa học dữ liệu để điều chỉnh nhiều mục tiêu như độ chính xác, recall, công suất và độ trễ, sửa đổi các đánh giá và phần thưởng để tự động tạo ra các mô hình khác nhau. Sau khi tạo ra mô hình, chúng ta có thể xác thực mô hình trong hệ thống mô phỏng nơi dữ liệu được thu thập bởi các tài xế trong ứng dụng XR và sau đó kiểm tra mô hình trong một số tình huống quy mô nhỏ trước khi triển khai. Khi model drift xảy ra, OmniForce liên quan đến con người trong vòng lặp để kiểm tra dữ liệu, thu thập dữ liệu nếu cần, tìm kiếm lại hoặc huấn luyện lại mô hình, và cập nhật phiên bản của mô hình.

**Dịch vụ Chuỗi Cung ứng** Trong quá trình metaverse công nghiệp, kho thông minh và tích trữ thực hiện chuỗi cung ứng nhanh và giảm ngân sách nguyên liệu thô và thời gian sản xuất. Đó là, mọi người có thể sử dụng công nghệ ML tự động để có được dịch vụ chuỗi cung ứng hiệu quả, bao gồm đặt hàng, lưu trữ, vận chuyển và tiếp thị. OmniForce hỗ trợ phân tích và dự đoán dữ liệu dạng bảng và dữ liệu chuỗi thời gian. Sau khi tự động tạo ra các mô hình với OmniForce, một số hệ thống mô phỏng, như cellular automata và các bài kiểm tra quy mô nhỏ, có thể được sử dụng để xác thực mô hình với chi phí tương đối thấp. Dựa trên phản hồi, trực quan hóa và giải thích của các kiến trúc và siêu tham số được tìm kiếm, OmniForce cung cấp giao diện thuận tiện để đưa con người vào vòng lặp, hướng dẫn họ điều chỉnh không gian tìm kiếm, tinh chỉnh chỉ số, thu thập dữ liệu kết hợp, điều chỉnh hệ thống mô phỏng, kích hoạt chu kỳ sản xuất mô hình tiếp theo và cập nhật phiên bản dữ liệu và mô hình. Trong quá trình này, các thuật toán có thể được triển khai bởi cơ sở kiến thức crowdsourced. Bằng cách chuẩn hóa các quy trình trừu tượng dữ liệu, thuật toán ứng dụng và không gian tìm kiếm, OmniForce làm cho việc tích hợp và tái sử dụng các thuật toán ứng dụng và không gian tìm kiếm trở nên dễ dàng. Hơn nữa, người dùng có thể học kiến thức của các pipeline ML từ OmniForce.

### 5.3 Kết quả Thực nghiệm

**Khả năng Mở rộng và Chịu lỗi** OmniForce hỗ trợ các công việc tìm kiếm có thể mở rộng được gán bởi bộ lập lịch. Như hiển thị trong Hình 8, các thí nghiệm chạy với các tài nguyên và tham số có thể mở rộng khác nhau trên cùng một tập dữ liệu và sử dụng hai thuật toán tìm kiếm: một thuật toán tiến hóa [186] và Hyperband [187]. Trong các trường hợp thử nghiệm quy mô lớn, các nhóm lớn hơn với nhiều tài nguyên tính toán hơn có thể đạt được cùng độ chính xác trong thời gian ít hơn so với những nhóm có ít tài nguyên hơn. Ví dụ, các thí nghiệm được thực hiện trên 64 GPU mất khoảng một giờ để đạt được độ chính xác thử nghiệm trên 90%. Ngược lại, các thí nghiệm được thực hiện trên 16 GPU mất hơn bốn giờ để có được cùng độ chính xác. Do đó, các mô hình được huấn luyện trên 16 GPU không thể đạt được hiệu suất tốt dưới ràng buộc thời gian được hiển thị trong Hình 8 (b). Ngoài ra, các thí nghiệm được thực hiện trên 16 GPU và 64 GPU mất thời gian tương tự để đạt được độ chính xác thấp vì việc đạt được mức hiệu suất kém như vậy không phải là thách thức đối với một công việc tìm kiếm.

Có thể thấy rằng trong Hình 9, các tỷ lệ lỗi khác nhau có ít ảnh hưởng đến hiệu suất. Bốn thí nghiệm đạt được hiệu suất tương đương nhưng với các tỷ lệ lỗi ứng viên khác nhau. Trong những thí nghiệm này, chúng tôi thêm nhiễu để giết một số ứng viên sống sót. Với sơ đồ bán đồng bộ được thiết kế cẩn thận của chúng tôi, các công việc có thể tiếp tục chạy dưới một số lượng hạn chế các ứng viên chết hoặc được tạm dừng và khởi động lại trong quá trình tìm kiếm.

**Hiệu suất Tìm kiếm** Dựa trên framework BO được đề xuất của chúng tôi, chúng tôi thiết kế một phương pháp BO mới phù hợp với các vấn đề tối ưu hóa rời rạc. Chúng tôi so sánh phương pháp của chúng tôi với các thuật toán NAS và phương pháp BO khác nhau trên một benchmark phổ biến (NAS-Bench-201 [94]), chứa 15625 kiến trúc mạng và các đánh giá của chúng trên ba tập dữ liệu phân loại thị giác. Theo thiết lập của [94], chúng tôi tìm kiếm trên tập xác thực CIFAR-10 sau 12 epoch huấn luyện và sau đó trực tiếp tra cứu các đánh giá trong các tập dữ liệu khác. Chúng tôi chạy các phương pháp BO này trong 80 lần lặp với 12 điểm ban đầu và báo cáo trung bình và độ lệch chuẩn của quan sát tốt nhất gặp phải trong quá trình tìm kiếm trên 10 lần chạy trùng lặp. Bảng 1 minh họa rằng phương pháp của chúng tôi đạt được hiệu suất tốt nhất trên cả ba tập dữ liệu.

**Hiệu suất Thuật toán** Ở đây, chúng tôi cung cấp hiệu suất của các mô hình được cung cấp bởi OmniForce. Đầu tiên, chúng tôi hiển thị các mô hình thị giác được mở rộng tiên tiến của chúng tôi (ViTAE [197,117]) và so sánh với các mô hình sâu dựa trên transformer khác trong Bảng 2. Chúng tôi thấy rằng với hơn một trăm triệu tham số, các mô hình có thể đạt được hiệu suất ấn tượng. Dựa vào khả năng mạnh mẽ như vậy, các mô hình có thể hoạt động tốt trên các tình huống mới sau khi được fine-tuned với ít dữ liệu được gắn nhãn. Kết quả là, người dùng có thể xác thực các phương pháp mới và nhanh chóng triển khai mô hình.

Ngoài ra, chúng tôi cũng báo cáo hiệu suất NLP của các mô hình nền tảng tích hợp sẵn của chúng tôi trên NLU (tức là hiệu suất trên benchmark GLUE) và NLG (tức là các nhiệm vụ dịch máy). Bảng 3 hiển thị kết quả đối chiếu thu được trên 9 nhiệm vụ NLU với một mô hình, cho thấy rằng phương pháp của chúng tôi có thể tận dụng bất kỳ prompt được fine-tuned hiện có nào để đạt được hiệu suất học chuyển giao tốt hơn. Quan trọng là, với phương pháp hiệu quả tích hợp sẵn của chúng tôi, hiệu suất thậm chí tốt hơn so với việc điều chỉnh mô hình đầy đủ có thể đạt được bằng cách chỉ điều chỉnh 0,5% tham số gốc, điều này cực kỳ quan trọng cho người dùng thực hiện huấn luyện và triển khai tài nguyên thấp/chi phí thấp chỉ với một vài (hoặc thậm chí không có) dữ liệu được gắn nhãn. Hình 10 hiển thị hiệu suất của các mô hình dịch Vega-MT của chúng tôi, nơi chúng tôi tham gia vào 10 nhiệm vụ chia sẻ, bao gồm Trung Quốc ↔ Anh (Zh ↔ En), Đức ↔ Anh (De ↔ En), Séc ↔ Anh (Cs ↔ En), Nga ↔ Anh (Ru ↔ En), và Nhật ↔ Anh (Ja ↔ En). Với mô hình nền tảng đa ngôn ngữ của chúng tôi, chúng tôi đạt được 7 chức vô địch, 2 vị trí á quân và 1 vị trí thứ ba về điểm BLEU. Một nền tảng với Vega-MT có thể tăng cường cho người dùng khả năng dễ dàng hiểu và tạo ra bất kỳ nội dung đa ngôn ngữ nào. Đáng chú ý, nền tảng của chúng tôi cũng có thể tăng tốc quá trình tạo ngôn ngữ bằng cách chuyển sang các thuật toán tạo sinh phi tự hồi quy được phát triển của chúng tôi [204, 205, 206].

Cuối cùng, chúng tôi báo cáo kết quả so sánh của một thí nghiệm tạo cảnh phức tạp được thực hiện với OmniForce. Kết quả định lượng được tạo ra bởi các đối thủ tham gia trên cả tập dữ liệu COCO-stuff và Visual Genome được báo cáo trong Bảng 4. Để so sánh công bằng, chúng tôi áp dụng các mô hình pretrained được phát hành chính thức của họ hoặc các điểm được báo cáo chính thức trong các bài báo của họ. So với cả các phương pháp tạo cảnh phức tạp dựa trên CNN và transformer, TwFA [207] đạt được cải thiện đáng kể về tất cả các chỉ số. Hơn nữa, vì chúng tôi sử dụng cùng chiến lược tokenization texture được sử dụng trong phương pháp dựa trên transformer, HCSS [208], hiệu suất tạo sinh chứng minh khả năng mô hình hóa các thành phần của các cảnh phức tạp với sự chú ý tiêu điểm của transformer.

**Thực hành HAML** Như đã đề cập ở trên, cả ML hỗ trợ con người và con người hỗ trợ ML đều đóng vai trò quan trọng trong hệ thống AutoML. Phần này lấy một cuộc thi Kaggle—Test Time Cost Forecasting for Mercedes-Benz—làm ví dụ để giới thiệu cách nền tảng OmniForce triển khai khái niệm hướng con người và đạt được tương tác người-máy hiệu quả.

Mục đích của cuộc thi là dự báo thời gian cần thiết cho xe tự định nghĩa của người dùng để vượt qua bài kiểm tra an toàn theo tập dữ liệu ẩn danh được cung cấp, từ đó giúp đội Mercedes-Benz tối ưu hóa hệ thống kiểm tra. Tập dữ liệu được tự động nhận dạng là tập dữ liệu dạng bảng bởi OmniForce sau khi người dùng tải nó lên nền tảng. Hơn nữa, các báo cáo phân tích dữ liệu khám phá được tạo ra, giúp con người khám phá thêm thống kê của dữ liệu từ mọi khía cạnh. Các giao diện liên quan được hiển thị trong Hình 11 và Hình 12.

Dưới các tập dữ liệu đa dạng và ứng viên hạn chế trên không gian tìm kiếm phức tạp, AutoML dựa trên máy móc thuần túy gặp khó khăn trong việc tìm ra giải pháp tối ưu trong một lần lặp. Do đó, OmniForce giới thiệu quá trình tối ưu hóa đa vòng với tương tác người-máy.

Trong vòng đầu tiên, chúng tôi trực tiếp đưa dữ liệu thô vào nền tảng OmniForce và chạy vòng đời AutoML, bao gồm tạo bảng hợp nhất tự động, xử lý dữ liệu tự động, kỹ thuật đặc trưng tự động, tìm kiếm mô hình và điều chỉnh siêu tham số, để có được kết quả baseline. Về việc thiếu sự can thiệp của con người, formatter được mô tả trong phần 2.1.6 cấu hình không gian tìm kiếm cho các nhiệm vụ hồi quy theo cơ sở kiến thức và sau đó tạo ra pipeline AutoML để chạy. Trang chi tiết tìm kiếm tương tác được hiển thị trong Hình 13, bao gồm hiệu suất mô hình, hiệu suất ứng viên, tọa độ siêu tham số song song, v.v. Sau khi có được kết quả bằng suy luận hàng loạt, chúng tôi đánh giá nó trên trang web Kaggle để mô phỏng tình huống dịch vụ trực tuyến của mô hình. Kết quả nguyên thủy của chúng tôi có được điểm r2 là 0,54956 trên bảng xếp hạng riêng tư, và thứ hạng tương ứng là 1540 trong tổng số 3823 đội (Late Submission).

Trong vòng thứ hai, chúng tôi sửa đổi không gian tìm kiếm bằng cách tuân theo cố vấn OmniForce được trình bày trong Phần 2.1.8, cung cấp các đề xuất để thay đổi không gian tìm kiếm và thực hiện kỹ thuật đặc trưng. Con người có thể cải thiện đáng kể hiệu quả của máy móc dựa trên các đề xuất trên và kinh nghiệm của riêng họ.

Trực quan hóa quá trình tìm kiếm là một chủ đề lâu đời trong các hệ thống AutoML và gần đây đã thu hút sự chú ý rộng rãi [215][216][217][218]. Hơn nữa, chúng tôi cho rằng các cách hiệu quả để hiển thị và sửa đổi không gian tìm kiếm là điều kiện tiên quyết cần thiết để con người hỗ trợ máy móc trong các hệ thống HAML. Trên trang chi tiết tìm kiếm, OmniForce hiển thị các ứng viên hiện tại và không gian tìm kiếm trong ba sơ đồ khác nhau, được hiển thị trong Hình 14 (xem toàn bộ), Hình 15 (xem giảm chiều) và Hình 16 (xem cặp).

Trạng thái ban đầu của giao diện chỉnh sửa là cấu hình được đề xuất bởi cố vấn. Cố vấn OmniForce tóm tắt một số sơ đồ tối ưu hóa dựa trên thống kê dữ liệu, kết quả tìm kiếm hiện tại và cơ sở kiến thức. Hầu hết các đề xuất đều tổng quát và di động; người dùng có thể chọn có áp dụng chúng hay cải thiện thêm chúng theo kinh nghiệm của họ. Sau khi tương tác với hình, một so sánh giữa không gian tìm kiếm được sửa đổi và gốc được hiển thị trong Hình 17, có thể được xem trước và chỉnh sửa thêm.

Chúng tôi điều chỉnh cấu hình tìm kiếm ở trên và có được điểm r2 là 0,5511 trên bảng xếp hạng riêng tư, đạt được thứ hạng 820/3823, điều này minh họa hiệu quả của cố vấn.

Ngoài ra, mọi người có thể chú ý đến mọi khía cạnh của quy trình tìm kiếm để tăng cường khả năng của máy móc. Ba sơ đồ trên liên quan đến không gian tìm kiếm có thể được chỉnh sửa trực tiếp bằng cách nhấp chuột hoặc sử dụng công cụ lasso. Cụ thể, chúng ta có thể giảm thêm max_depth của cây và tỷ lệ lấy mẫu hàng với chi phí vài cú nhấp chuột, và quá trình thực hiện điều này được hiển thị trong Hình 18. Sau khi hấp thụ kinh nghiệm có giá trị của con người, hiệu suất tìm kiếm của máy móc được cải thiện thêm, với điểm r2 là 0,55184 trên bảng xếp hạng riêng tư, và thứ hạng tương ứng là 338.

Tiếp theo, cố vấn cung cấp các đề xuất cho kỹ thuật đặc trưng dựa trên thông tin quan sát được bằng cách phân tích dữ liệu huấn luyện và mức độ quan trọng của các đặc trưng trong quy trình tìm kiếm. Cụ thể, các đặc trưng được đề xuất được hiển thị trong Hình 19, và người dùng có thể chọn có áp dụng các đề xuất dựa trên kinh nghiệm của họ hay không. Sau khi hoàn thành cấu hình đặc trưng, nền tảng tạo ra phiên bản mới của pipeline đặc trưng và tự động áp dụng các thay đổi đã chọn vào dữ liệu gốc, như hiển thị trong Hình 20.

Sau khi thực hiện sửa đổi trên dựa trên đề xuất của máy móc, chúng tôi có được điểm 0,55284, và thứ hạng tương ứng là 76.

Trong vòng cuối cùng, người dùng có thể phát huy khả năng kỹ thuật đặc trưng của họ để đạt được hiệu suất cải thiện hơn nữa. Cụ thể, con người có thể vào giao diện kỹ thuật đặc trưng thông qua nút 'Go to feature pipeline' và sửa đổi dữ liệu dựa trên kinh nghiệm bằng cách thực hiện các câu lệnh SQL hoặc phương pháp đặt trước, như hiển thị trong Hình 21. Cuối cùng, chúng tôi có được điểm 0,55394, và thứ hạng tương ứng là 11 trong tổng số 3823.

Như hiển thị trong Hình 22, tất cả các điểm cải thiện dẫn xuất đều đến từ kinh nghiệm con người và các đề xuất của máy móc, điều này minh họa sự tiến bộ của tính năng tương tác người-máy được đề xuất bởi hệ thống OmniForce. Các nỗ lực trên được lưu như các phiên bản pipeline đặc trưng khác nhau, phiên bản cấu hình tìm kiếm và phiên bản mô hình để người dùng chuyển đổi khi cần.

## 6 Công trình Liên quan

Trong những năm gần đây, một số công ty thương mại đã phát hành các nền tảng AutoML của riêng họ cho các ứng dụng công nghiệp. Một số xác định các phân khúc thị trường và phát triển các sản phẩm khác nhau cho các nhiệm vụ khác nhau. Ví dụ, Amazon SageMaker Canvas [219] cho phép các nhà phân tích kinh doanh xây dựng các mô hình AI và đưa ra dự đoán chính xác cho dữ liệu dạng bảng theo cách không cần mã, trong khi Amazon Forecast [220] sử dụng dịch vụ dự đoán chuỗi thời gian. IBM Watson AutoAI [29] hỗ trợ xây dựng các nhiệm vụ phân loại và dự đoán cho dữ liệu dạng bảng, trong khi Watson Natural Language Understanding [221] tập trung vào phân tích văn bản tiên tiến. Những công ty khác như Abacus [222], Microsoft Azure AutoML [223] và Google Cloud Vertex AI [224] hỗ trợ các loại dữ liệu và nhiệm vụ khác nhau trong một nền tảng. OmniForce chọn tùy chọn sau và nhắm đến một pipeline sản xuất ứng dụng AI chung và có thể tái sử dụng. Để phá vỡ đảo cô lập của tự động hóa nhiệm vụ và tối ưu hóa điểm đơn, OmniForce nhấn mạnh các hoạt động hướng con người; liên quan đến con người kinh doanh trong quá trình xây dựng các ứng dụng AI; và tích hợp kinh doanh, dữ liệu, thuật toán và bảo trì theo cách cộng tác và liên tục.

Một tổng quan về hệ sinh thái của các hệ thống AutoML được trình bày trong Bảng 5, nơi một số nền tảng mã nguồn mở như HyperOpt [22], Katib [27], NNI [23], Ray-Tune [21] và các nền tảng thương mại như Abacus [222], Google Vertex AI [224], IBM Watson AutoAI [29], Amazon SageMaker Canvas [219], Microsoft Azure AutoML [223] được so sánh với OmniForce. OmniForce là một trong những hệ thống hiếm hoi hỗ trợ crowdsourcing, cộng tác đám mây-biên, mô hình siêu sâu (mô hình lớn) và hệ thống hướng con người. OmniForce là một hệ thống AutoML cloud-native có thể chạy trong môi trường sản xuất để sử dụng thương mại.

## 7 Kết luận

Do phương pháp AutoML hướng con người và cộng tác đám mây-biên được đề xuất và các ứng dụng được cung cấp rộng rãi cho phép các mô hình lớn và các chiến lược chuyển giao hiệu quả cao của chúng, OmniForce xây dựng một hệ thống thực tế và mạnh mẽ cho những người và công ty muốn hưởng lợi từ công nghệ AI trong các tình huống môi trường mở, như chuỗi cung ứng công nghiệp và metaverse công nghiệp, nơi mọi người thường gặp phải các vấn đề vòng lặp mở và cần huấn luyện hoặc cập nhật các mô hình lớn và triển khai mô hình trên các thiết bị lớn với các ràng buộc và yêu cầu khác nhau. OmniForce có giao diện người dùng thống nhất, framework chiến lược tìm kiếm linh hoạt và liền mạch, và các tính năng cloud-native, làm cho nó trở thành hệ thống có thể truy cập, đa năng và sẵn sàng sản phẩm để giúp mọi người thiết lập và cải thiện kỹ thuật AI trong thực tế và liên tục trích xuất giá trị kinh doanh từ công nghệ AI.

## Lời cảm ơn

Chúng tôi cảm ơn Tiến sĩ Xiaodong He đã cung cấp hỗ trợ có giá trị và các bình luận xây dựng. Công trình này được thực hiện (một phần) trong chuyến thăm của Tiến sĩ Jing Zhang và thực tập của Jiaxing Li tại JD Explore Academy.
