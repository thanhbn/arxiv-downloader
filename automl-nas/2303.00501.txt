# 2303.00501.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2303.00501.pdf
# File size: 4737608 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
OMNIFORCE : ONHUMAN -CENTERED , LARGE MODEL
EMPOWERED AND CLOUD -EDGE COLLABORATIVE AUTOML
SYSTEM
Chao Xue Wei Liu Shuai Xie Zhenfang Wang Jiaxing Li Xuyang Peng Liang Ding
Shanshan Zhao Qiong Cao Yibo Yang Fengxiang He Bohua Cai Rongcheng Bian Yiyan Zhao
Heliang Zheng Xiangyang Liu Dongkai Liu Daqing Liu Li Shen Chang Li Shijin Zhang
Yukang Zhang Guanpu Chen Shixiang Chen Yibing Zhan Jing Zhang Chaoyue Wang∗
Dacheng Tao
JD Explore Academy
{xuechao19, wangchaoyue9}@jd.com
ABSTRACT
Automated machine learning (AutoML) seeks to build ML models with minimal human effort. While
considerable research has been conducted in the area of AutoML in general, aiming to take humans
out of the loop when building artificial intelligence (AI) applications, scant literature has focused
on how AutoML works well in open-environment scenarios such as the process of training and
updating large models, industrial supply chains or the industrial metaverse, where people often face
open-loop problems during the search process: they must continuously collect data, update data and
models, satisfy the requirements of the development and deployment environment, support massive
devices, modify evaluation metrics, etc. Addressing the open-environment issue with pure data-driven
approaches requires considerable data, computing resources, and effort from dedicated data engineers,
making current AutoML systems and platforms inefficient and computationally intractable. Human-
computer interaction is a practical and feasible way to tackle the problem of open-environment AI. In
this paper, we introduce OmniForce, a human-centered AutoML (HAML) system that yields both
human-assisted ML and ML-assisted human techniques, to put an AutoML system into practice and
build adaptive AI in open-environment scenarios. Specifically, we present OmniForce in terms of
ML version management for data, labels, models, algorithms and search spaces; pipeline-driven
development and deployment collaborations; a flexible search strategy framework; and widely
provisioned and crowdsourced application algorithms, including large models. Our proposed cloud-
native OmniForce method can be run either on a public/private cloud or in an on-premise environment.
Furthermore, the (large) models constructed by OmniForce can be automatically turned into remote
services in a few minutes; this process is dubbed model as a service (MaaS). Experimental results
obtained in multiple search spaces and real-world use cases demonstrate the efficacy and efficiency
of OmniForce.
Keywords Human-Centered Automated Machine Learning (HAML) ·Cloud-Edge Collaborations ·Large Model ·
Model-as-a-Service (MaaS)
∗Correspondence authorarXiv:2303.00501v2  [cs.LG]  8 Jul 2023

--- PAGE 2 ---
1 Introduction
In recent decades, machine learning (ML) has achieved great success in the fields of computer vision [ 1,2], natural
language processing (NLP) [ 3,4], speech recognition [ 5,6], content generation [ 7,8] and tabular data processing [ 9,10].
The rapid development of ML technology has given birth to highly popular artificial intelligence (AI) products, such as
Tesla Autopilot [ 11], Google Translate [ 12], Siri [ 13], and ChatGPT [ 14]. With ML requirements growing exponentially
in terms of both the amount of training data and the number of models/neural networks tailored to different tasks, the
design of tailored hyperparameters and identifying neural networks for training in a fully automatic fashion without
human intervention, which is referred to as automated ML (AutoML), has yielded great achievements.
The recent progress of AutoML has been characterized by algorithms and systems. Regarding the former, considerable
studies have used methods based on genetic algorithms [ 15], random search [ 16], Bayesian optimization [ 17], rein-
forcement learning [ 18] and differentiable techniques [ 19]. To achieve the latter, numerous frameworks such as Optuna
[20], Ray-Tune [ 21], HyperOpt [ 22], NNI [ 23] and Orion [ 24] for hyperparameter optimization have been developed to
support scalable trials and customizable search algorithms. Auto-sklearn [ 25] also supports the use of meta-learning
to leverage historical records to warm-start the search procedure. Unlike other frameworks that require additional
effort to support Kubernetes [ 26], Katib [ 27] is a cloud-native framework and can realistically be run in a production
environment. Compared to the exploration of an open-source AutoML system, many companies offer their AutoML
products to the market, such as Google Cloud AutoML [ 28], IBM Watson AutoAI [ 29], Amazon SageMaker [ 30], and
H2O Driverless AI [ 31]. Such platforms are targeted at building AI models in a short period of time for developers with
limited ML expertise.
While numerous frameworks have been proposed for AutoML, as described above, we have not seen the expected
widespread adoption of AutoML systems in industry. We presume the following reasons for the low adoption rate of
these frameworks.
•Only targeting closed-loop problems – Most AutoML frameworks only focus on closed-loop problems, where
the data, algorithms, and metrics are deterministic; thus, their design concept is to take humans out of the loop
when building AI applications. However, AI-related problems are often open-loop tasks in practice, especially
in the process of training and updating large models or in industrial supply chains, where people need to
collect data continuously, update the versions of data and models, and modify the evaluations and rewards
produced during the production process. It would be inefficient and even computationally intractable to use
current data-driven AutoML systems to address open-loop problems since they require considerable data to
learn domain knowledge and business logic.
•Lack of deployment considerations – Most AutoML frameworks only focus on the search and training phases,
ignoring the inference and deployment phases. However, massive devices with different deployment (inference)
requirements are encountered in real industrial scenarios or industrial metaverses, where simulation or XR2
technology is used to reduce the risk of failure in the physical production process and to build a highly efficient
supply chain, including the design, development, manufacturing, pricing, sales, storage, transportation, and
after-sale service phases.
•Limited application algorithms – Most AutoML frameworks only have some predefined or built-in application
algorithms and their corresponding search spaces, which are often transparent to users for ease of use. However,
a large number of various AI applications are available in practice, and an AutoML system is limited to a fixed
number of built-in application algorithms. Considering this large number of various applications, especially
those with different development and deployment requirements, e.g., Cloud-Edge collaborations, the wide use
of AutoML systems will be restricted if only predefined application algorithms are provided for the search
process.
In an attempt to address the above issues, we develop OmniForce for supporting AutoML in open environments;
OmniForce is centered on the following ideas.
•Human-centered and adaptive AutoML (HAML) – We design OmniForce for both human-assisted ML and
ML-assisted humans. Thus, users can efficiently deal with their business logic and data collection processes
by interacting with OmniForce.
•Cloud-edge collaborations in practice – We propose a pipeline-driven AutoML framework with collaborative
development and deployment to search AI applications with different training and deployment requirements.
•Crowdsourced application algorithms – We introduce the crowdsourcing concept to integrate the various
observed application algorithms into the OmniForce platform. By standardizing the data abstraction paradigm,
2XR is an umbrella term covering virtual reality (VR), augmented reality (AR), and mixed reality (MR).
2

--- PAGE 3 ---
application algorithm, and search space, we can easily integrate and reuse the application algorithms and
search space.
We illustrate the concept of HAML in Figure 1. The user interacts with the AutoML system in terms of human-assisted
ML and ML-assisted humans. In particular, HAML tasks have the elements of data collection and annotation, features,
application algorithms, search spaces, searching, training and deployment, and visualization. For data collection and
annotation, on the one hand, users collect data to make ML algorithms accurate by using active learning; on the other
hand, ML algorithms help users label data efficiently. Moreover, data privacy protection and security play important
roles in interactions between human and AutoML systems. OmniForce supports the differential privacy technique for
protecting users’ data. Regarding features, OmniForce supports customized feature pipelines through user interaction
and SQL. Additionally, users can view and analyze the statistical meta-data information of data and features. In
terms of application algorithms, given the generality of crowdsourcing and super-deep models, OmniForce is more
widely applicable than many other AutoML systems with only built-in small-scale application algorithms. OmniForce
hides the details used to set the search space by default, but users can configure the search space by tuning priors or
preferences when they obtain knowledge from visualizations. For searching, training, and deployment, users define the
development and deployment environment, set their requirements and constraints, and perform single/multiple-objective
optimizations. OmniForce supports Cloud-Edge collaboration to address the different requirements of the training
and deployment environments by means of its powerful search ability. For the visualization part, which is the core of
HAML, users learn the knowledge of their AutoML pipeline from OmniForce, obtain explanations of the searched
architecture and hyperparameters, and acquire advice that can be used to guide the next steps of their work. For example,
the advisor may suggest that the user update the search space based on the statistical distribution of the sampling
candidates. Additionally, it may encourage the user to collect more data from a specific class or relax some latency
constraints and power restrictions. The HAML cycle enables users to fully participate in human-computer cooperation
and achieve the purposes of both using machines to enhance human abilities and leveraging human experiences and
operations to improve machine intelligence.
Our contributions are as follows.
1) OmniForce is a cutting-edge human-centered and adaptive AutoML system that supports for open-environment
scenarios such as the process of training and updating large models, industrial supply chains and industrial metaverses.
It includes a set of novel search strategies, a search space update policy, and large model algorithms for computer vision
(CV), NLP, and AI-generated content (AIGC). As such, OmniForce caters to both developers with limited ML expertise
and data scientists.
2) OmniForce is a cloud-native AutoML system that is scalable, fault tolerant and cloud-edge collaborative; thus, it can
be run in a production environment.
3) OmniForce follows the model-as-a-service (MaaS) pattern and fully connects the search, training, inference, and
deployment processes. At the moment when OmniForce successfully completes the model construction process, users
will not only obtain the model but also have the inference and deployment service of the model. This enables users to
transform the model into a remote service that can be deployed in the cloud or on the edge in a few minutes, helping
users quickly build cutting-edge applications with AI capabilities.
The rest of this paper is arranged as follows. Section 2 describes the system architecture and workflow of OmniForce.
Section 3 describes the detailed design concepts, and Section 4 shows the supported features, followed by evaluations
in Section 5. We compare the related work in Section 6 and finally conclude in Section 7.
2 System Architecture
As we will see, OmniForce is implemented with several components such as a job estimator, workers, a training task
estimator (TTE), a task deployment estimator (DTE), a task sidecar, a scheduler, and a manager. Figure 2 shows a
simplified block diagram of the system overview of OmniForce.
The application layer is concerned with processing business logic, such as uploading the data, choosing the application
algorithms and pretrained models, and setting the search space to start the search process. OmniForce provides widely
provisioned data and application algorithms that contain large model-based methods for users who focus on their
business logic regardless of the details of the ML algorithms. Users can also utilize and contribute to the crowdsourced
resources that are integrated into OmniForce to satisfy the growing demand for ML applications.
All data, algorithms, and search spaces of the application layer interact with the search engine via uniform abstractions,
meaning that these resources are represented and organized as uniform views. Further changes are reflected in the
3

--- PAGE 4 ---
Figure 1: HAML. The ring represents the pipeline of HAML. Users interact with the key steps/nodes in the loop. Unlike
most AutoML frameworks that only focus on the searching and training parts, HAML pays attention to the whole ML
pipeline, where one needs to consider data collection; updating the data version, feature pipelines, algorithms, and
search spaces; modifying the evaluations and rewards; and obtaining knowledge from the visualization that can be used
to guide his or her work in the next steps.
different versions of the resources. Privacy and security are also involved in the interface layer. More details regarding
the interface layer can be found in Section 3.1.
The search engine is implemented via a search strategy framework with multiobjective collaboration, which attaches to
a semisynchronized controller for multiple programs/multiple data (MPMD) dispatch, a scheduler that assigns jobs and
tasks to the cloud resources, and a formatter for learning the historical knowledge to generate the AutoML pipeline
(serving as a meta-learner). In particular, we implement a flexible parallel Bayesian optimization (BO) framework that
fully runs on PyTorch, which supports BO with a variety of surrogate models and acquisition functions, including our
novel model, to deal with large discrete spaces (which form the scenario of neural architecture search (NAS)). We also
support a revised hyperband [ 32], MF-NAS [ 33], and a novel evolution approach in the search strategy framework.
The search process becomes more difficult when the search space is large. We involve multiple workers to find good
candidates in parallel. Considering the tradeoff between parallel efficiency and inevitable synchronization that some
strategies need to guarantee performance, we implement a semisynchronized MPMD dispatcher. Similar to Pathways
[34], which has demonstrated the limits of the single program/multiple data (SPMD) paradigm for ML computations,
we express the NAS process as MPMD. More details regarding the search engine can be found in Section 3.4.
OmniForce runs on Kubernetes and Kubeflow [ 35] to support scalability, fault tolerance, and multitenancy. OmniForce
is a product-ready, cloud-native system that can be deployed as a service either in a public/private cloud or in an
on-premise environment.
2.1 Components
In this section, we explain some fundamental components of OmniForce in detail, including the job estimator, workers,
TTE, DTE, task sidecar, scheduler (formatter and resource scheduler), manager and advisor.
4

--- PAGE 5 ---
Figure 2: OmniForce system overview. Based on Kubernetes and Kubeflow, OmniForce builds an AutoML system with
widely provisioned ML algorithms and uniform interfaces for crowdsourcing algorithms, models, and search spaces.
2.1.1 Job Estimator
The job estimator is a semisynchronized controller of the AutoML process that determines which tasks should be
evaluated next, how to handle their rewards, and when to start the next iteration. Two key components are contained
in the job estimator: a user-defined search space from which the candidates are generated and a search strategy that
conducts the sequential and parallel search processes via advanced search algorithms. The details of the search space
and search strategy can be found in Sections 3.2.2 and 3.4, respectively.
Some AutoML frameworks tend to be fully synchronized, where the next iteration will not start until all tasks in the
current round are completed. However, synchronized frameworks cannot guarantee high efficiency in MPMD settings.
In contrast, our job estimator adopts a semisynchronized mechanism that controls the starting of the next iteration
through an adaptive maximum waiting time. The statuses of tasks that exceed this time are set to timeout, and the
job estimator ignores these tasks in the current iteration and generates new tasks via the completed observations. The
waiting time calculation is flexible. For example, we can use the mean and variance of the execution times of previously
completed tasks to estimate the waiting time. Considering the massive execution time differences between tasks in
the MPMD setting, we can also build a time cost-aware surrogate model provided by our BO framework to give an
estimation for each task.
2.1.2 Worker
A worker is a group of processes that reserve and evaluate candidate tasks. In our design, the job estimator and
workers do not communicate directly but rather through a task broker, which is usually middleware. The existing
large-scale distributed AutoML systems always have large computational demands and place a high value on scalability
and fault tolerance. Our design carefully considers these requirements and decouples the job estimator and workers,
achieving scalability through which users can freely increase or decrease the number of utilized workers based on their
practical resources. Furthermore, if some worker nodes break down unexpectedly, our search process adapts to this new
circumstance without any effort and continues to search as long as one worker is alive. In addition, workers can handle
many common exceptions, such as GPU memory exhaustion and middleware connection loss.
5

--- PAGE 6 ---
2.1.3 TTE
The TTE is the actual entity used to evaluate candidate tasks. After a worker reserves a task, a specific TTE will be
launched to evaluate the task and report the result to the worker when finished. OmniForce develops a task sidecar
service accompanied by the TTE to proxy the communication process. OmniForce supports crowdsourcing, and users
can easily upgrade their training code to a searchable code in the TTE by implementing the algorithm interface defined
in Section 3.2.1. More importantly, as it runs on Kubernetes and Kubeflow, OmniForce facilitates elastic distributed
training with cloud-native workloads such as PytorchJob [ 36] and MPIJob [ 37]. This feature helps to efficiently use the
available cluster resources and to handle variable training settings such as a large model.
2.1.4 DTE
The DTE is a deployment entity that cooperates with the TTE. When the TTE finishes the model training process, the
trained model is sent to the DTE to assess the model’s performance in the deployment environment. The communication
between the DTE and TTE is carried out by task sidecars. For example, in the cloud-edge collaboration scenario, the
cloud side is responsible for model searching and training with its powerful cloud computing capability, while the edge
side is the practical environment for production. OmniForce cares about this requirement and provides a multiobjective
optimization service to jointly evaluate model performance in the training and deployment environments. The design
takes advantage of cloud computing and edge deployment, bridging the gap between the development and production
environments.
2.1.5 Task Sidecar
A sidecar [ 38] is a design pattern in a cloud-native system that is quite useful when running two tightly coupled
processes together. OmniForce builds task sidecar components to decouple the search logic and the connection logic
to adjust to complex training and deployment environments. In our AutoML workflow, the task sidecar behaves as a
middleman to connect the workers, TTE, and DTE. This component handles the trivial operations between components
and expands the boundary of the service, making it applicable to more downstream scenarios.
2.1.6 Scheduler
The scheduler estimates and schedules resources for jobs and consists of two parts. The first part formats a new job
as an appropriate configuration including a search space and estimated resources, and this component is dubbed the
“formatter” in our system. The second part, called the resource scheduler, assigns the actual computing resources to the
given job based on the estimated resources and the current payload of the AutoML system.
To estimate the time slot, memory, and computing resources, the formatter builds a knowledge base containing the his-
torical experiments that have been searched before. We obtain knowledge from previous jobs, such as hyperparameters,
metrics configurations, the multifidelity of the searching process, memory, and utilized computations. Due to its use of
an off-the-shelf knowledge base, the formatter finds a proper search algorithm and search space for an entering job and
provides the resource scheduler with the estimated resources and the parallelism of workers. Additionally, the formatter
defines the structure of the search pipeline.
The resource scheduler manages all the cluster resources and prevents deadlocks or pending exceptions due to the
preemption of resources. Specifically, the resource scheduler allocates the resources and adjusts the parallelism of
the search and training stage according to the job details given by the formatter and the current payload of the system.
It divides jobs into search, training, inference, and deployment phases and schedules their resources separately. The
resource scheduler processes one job of a certain type at a time.
2.1.7 Manager
The manager is a housekeeper service that enables OmniForce to interact with cloud-native Kubernetes and Kubeflow
resources. In addition to the basic create, read, update and delete (CRUD) operations, deferring to the ML operation
philosophy, the OmniForce manager automates the AutoML workflow with a Kubeflow pipeline [ 39], which is a
cloud-native service for building portable and scalable ML workflows. In addition, we build some specific AutoML
pipelines and integrate various cloud-native workloads, such as Kubernetes jobs, Kubeflow PytorchJobs, and KServe
InferenceServices, into one pipeline. Thus, OmniForce can manage these cloud-native resources in a unified form,
which simplifies the process of automating and reproducing an AutoML workflow.
Moreover, considering the flexible requirements of AutoML workflows and the unpredictability of cluster computing
resources, the OmniForce manager abstracts the cloud-native workloads into modular pipeline elements so that the
OmniForce scheduler and formatter can orchestrate these elements into pipelines with specific architectures and resource
6

--- PAGE 7 ---
(a)
 (b)
Figure 3: AutoML pipeline examples of OmniForce. (a) An example of a NoCode pipeline. (b) An example of a HPO
pipeline. Each pipeline depicts a specific AutoML workflow in a directed acyclic graph (DAG) format, where every
component executes in topological order. Each component launches appropriate cloud-native workloads to complete its
work.
parallelism. After a pipeline is generated, the OmniForce manager starts the related components in topological order.
We present two AutoML pipeline examples in Figure 3; one is called the NoCode pipeline, and the other is called the
HPO pipeline. In the NoCode pipeline, the job estimator and workers are launched simultaneously to search for the best
architecture and hyperparameters. Then, the task estimator builds the model with collaboration between training and
deployment. Finally, the advisor gathers the meta-data generated in the workflow and provides informative insights
for users. The start and end components here are responsible for some preparation and cleaning steps. In the HPO
pipeline, the smoke job estimator and workers are added to verify the correctness of the crowdsourcing algorithms
before entering the actual search phase. Then, the default job estimator and workers are added to reproduce the default
algorithm performance for a comparison with the AutoML search results.
2.1.8 Advisor
An advisor is a component that gives users comprehensive insights into their datasets and crowdsourcing algorithms.
The OmniForce advisor visualizes the search process during task execution. In addition to basic metrics such as the
loss and accuracy, the advisor provides a visualized map of the candidates in the search space and a bar map of the
hyperparameter importance levels to demonstrate their correlations with the selected metrics, aiming to untangle the
complicated interactions between the hyperparameters. This information helps users understand which hyperparameters
matter the most to the performance of their models. Moreover, the advisor supplies valuable suggestions about the
search space and datasets. For example, the advisor may suggest that the user update the search space or encourage the
user to collect more data on a specific class.
2.2 System Workflow
In this section, we illustrate the overall AutoML workflow that a user would interact with in the OmniForce system,
best viewed in Figure 4. Adhering to the human-centric concept, OmniForce designs informative and friendly user
interfaces for ease of use. In addition, OmniForce operates the services in a cloud-native, stable and cross-platform
manner by leveraging Kubernetes and Kubeflow. After users set their objectives, OmniForce relays the subsequent
model construction and analysis process. The major steps are as follows.
1. The user’s input is translated to a standard model requirement and sent to the application server.
7

--- PAGE 8 ---
Figure 4: The AutoML workflow of OmniForce. The components of the OmniForce system interact with each other by
following this workflow.
2.The application server verifies the user’s identity and privileges, converts the model requirement to a specific
AutoML job, and stores the job information in a database. Finally, this job is submitted to the scheduler.
3.The scheduler orchestrates the pipeline based on the job information and current cluster resources. This process
can be divided into two steps. First, the formatter generates a search space, selects the proper search strategy
based on the job information and historical records, and then organizes the modular pipeline elements into a
logical pipeline. Second, the resource scheduler computes a rational resource allocation solution based on the
current cluster resources. Notably, this step converts the logical pipeline to a specific resource pipeline that is
ready to be launched by the manager.
4.The manager converts the resource pipeline into a standard format supported by the Kubeflow pipeline
controller and starts the pipeline components in logical order, as illustrated in Figure 3.
5.The job estimator parses the search space and generates search candidates based on specific search strategies
such as hyperband search and BO.
6.The candidates generated in the last step are stored in the middleware and ready to be reserved by the workers.
7.The workers reserve the candidates from the middleware in a mutually exclusive mode and launch specific
task estimators to evaluate the candidates with the help of the task sidecar.
8.The task sidecar bridges the candidates reserved by the workers and instantiates them to cloud-native workloads
such as jobs and PytorchJobs to conduct an evaluation. After these workloads are complete, the evaluation
results are fetched and reported to the workers and job estimator for the next round of candidate generation.
9.The task sidecar also plays a vital role in training and deployment collaboration. When the training process is
finished, the task sidecar on the training side relays the trained model to the deployment side to evaluate the
model performance (such as latency and power) in the deployment environment. This step is an essential part
of OmniForce’s multiobjective optimization design.
10.During the model search process, the advisor comprehensively analyzes the meta-data and generates useful
suggestions for users such as datasets, algorithms or configuration analyses.
8

--- PAGE 9 ---
3 Design Concept
3.1 Abstraction and Management for Human-Centered AI
As a human-centered AI platform, OmniForce reduces the complexity of ML operation processes and lets users focus on
their most relevant work, such as business logic. Specifically, OmniForce adopts a project concept to uniformly manage
data, features, models, and other meta-data . A project contains all the materials needed to build a specific business
solution. Below the project, the fundamental abilities required for AI in production, such as data privacy, version
control, feature engineering, model training, serving and monitoring, and ML pipeline automation, are well abstracted
and convenient to use. In this section, we detail the data, feature pipeline, and model management of OmniForce to
illustrate its AI life cycle management capability.
3.1.1 Data Management
Data form the cornerstone for building successful AI solutions. When people choose ML platforms, they usually have
the following central concerns. Is my data safe here? Will my privacy be carefully protected? Is the platform compatible
with my diverse data and able to give full play to the data value? Can the platform handle rapid data iteration and update
solutions in time? OmniForce provides a comprehensive data management service, covering privacy, data accessibility
and versioning, data annotation, and lifelong learning.
Data Accessibility and Versioning Most AI platform developers agree that data access is the main problem when
connecting users to a platform. Diverse data derived from all walks of life increase the burden of data standardization
and governance. To tackle this problem, OmniForce develops a group of uniform data accessing and fetching application
programming interfaces (APIs). With little effort, users can upload diverse data from multiple sources and transform
raw data into a standard format with the data accessing API. The data fetching API abstracts the complexity of data
sources and helps developers and data scientists retrieve data freely.
After standardizing the data, OmniForce performs data version control with DVC [ 40], which is used to handle large
files, datasets, models, configurations, and codes. When users upload data, OmniForce converts all the media files
(except tabular data files) to a metadata file that maintains the references to the original data. Users can freely perform
operations on meta-data and save these changes to a new version. These operations do not change the previous meta-data,
and all the meta-data are traceable and reusable. These operations mainly include the following:
1. Adding or modifying the annotations.
2. Data filtering, e.g., filtering 5 classes from the whole dataset.
3. Merging two versions of a dataset.
Notably, all the data uploaded to OmniForce are safely hosted with our privacy protection algorithm.
Active Learning and Intelligent Annotation Active learning (AL) [ 41] is a practical technology for addressing a
lack of annotation data. The fundamental problem in AL is to develop a cost-effective data ranking strategy to find
the most informative samples among a vast unlabeled data pool. In a typical AL loop, a model is used to recommend
samples with appropriate ranking strategies. Then, professional annotators evaluate these recommendations and actively
annotate the samples that the model urgently needs. When the annotation budget is satisfied, the model updates using
the newly labeled dataset, typically with more samples. In return, the better-trained model will be used to recommend
better data for the next round of recommendations. Data are the fuel moving the AI wave. AL helps collect high-quality
fuel and quickly build high-performance AI models, which is significant in industrial fields.
AL involves humans in the ML life cycle and quickly builds high-quality datasets with mutual human–machine
assistance. OmniForce combines this ability with our other services and devises an intelligent data annotation service.
This service has the following main features.
1. Abundant data annotations support the most common AI tasks.
2.Human–machine collaborative annotation. OmniForce supports two annotation modes, online and offline. In
the online mode, users can correct the machine annotation results in real time, benefitting from the convenient
model deployment service of OmniForce. In the offline mode, users can submit an annotation task with a
large amount of unlabeled data at one time and review all the results when the automated annotation process is
finished, which benefits from the batch inference service of OmniForce.
3.Customizable machine annotation capability. Unlike the other counterparts in the market that only support
provisioned models with limited annotation ability, OmniForce benefits from its crowdsourcing design and
9

--- PAGE 10 ---
supports users in deploying their annotation models. These models can be directly uploaded by users or
generated by OmniForce.
4.AL-driven data recommendation service. OmniForce integrates many practical and advanced AL algorithms
to help users find the most informative data and quickly build high-quality datasets.
Differential Privacy The differential privacy (DP) technique was first proposed to guarantee the privacy of database
querying operations [ 42]. Recently, it has also been extended to measure the privacy preservation level of an algorithm
[43,44]. Suppose two adjacent datasets (S, S′), where SandS′only differ by at most one sample, an arbitrary
subset Hof the model hypothesis space, and an algorithm Aare given. The DP of Ais defined as the change
inA’s output hypothesis when Ais applied to SandS′. In particular, (ε, δ)-DP is mathematically defined as
loghPA(S)(A(S)∈H)−δ
PA(S′)(A(S′)∈H)i
≤ε. This means that an algorithm with small differential privacy (ε, δ)is robust to changes
in the individual training samples. Thus, the value of (ε, δ)indexes the DP level, i.e., the ability to resist differential
attacks that use individual samples as probes to attack ML algorithms; then, the individual privacy is inferred via the
changes in the output hypotheses. Generally, the privacy preservation level of an iterative algorithm degrades along with
the number of iterations since the amount of leaked information accumulates as the algorithm progresses [ 43,45,46].
Deep neural networks have been practically demonstrated to have good generalization abilities. However, a deep neural
network is an overparameterized model and is difficult for the existing statistical learning theory to explain [ 47]. This
has attracted the community’s interest in studying it, and many works have found that a DP model usually also has a
guaranteed generalization ability [48, 49, 50, 51, 46].
3.1.2 Feature Pipeline Management
ML models are highly dependent on the quality of the input data, and raw data preprocessing is often a crucial part
of the ML pipeline. To assist data scientists and engineers in efficiently and accurately infusing their experience into
AI products, OmniForce provides data processing capabilities such as exploratory data analysis, interactive feature
engineering, and advanced SQL processing. Custom feature pipelines can ease the burden of data preparation in
an automated and low-code manner, enabling more focus on data collection, feature design, and model selection
innovations.
Feature Pipeline Version Control Distinguished from data version control, feature pipeline version control empha-
sizes tracing users’ feature engineering operations. A feature pipeline contains a series of operation steps. When users
perform operations on their data in the interface, they are cast to a small part of the data and stored for future processing
in a lazy loading manner. Thus, we provide an elegant compromise between real-time user operation feedback and
operation version control. Based on the historical operation versions, users can build a new feature pipeline with little
effort by modifying or merging the operation steps.
Customized Feature Pipeline Users can employ interactive feature engineering and write SQL statements to
generate features based on human experience, enabling human-assisted ML to achieve satisfactory results more quickly.
Specifically, interactive feature engineering is divided into the following categories: temporal feature extraction,
single-column calculation, intercolumn calculation, and specific condition processing, which covers atomic operations
that are commonly used in data processing procedures. We also support users in performing high-level operations on
data through custom SQL statements, dramatically improving their feature engineering efficiency. In addition, we use
a lazy processing strategy to perform operations to ensure real-time interaction, making user clicks smoother when
dealing with big data.
3.1.3 Model Management
In this section, we introduce the model management technique in OmniForce, which includes three main parts: model
construction and versioning, model deployment, and monitoring in production. The core idea of model management
is to realize automatic model iteration with a closed loop of training, deployment, monitoring, and optimization for
releasing the model to production in an agile manner.
Model Construction and Versioning OmniForce constructs models with AutoML pipelines in an automated manner,
and all the produced models are version-controlled. We design a three-layer model management architecture to
demonstrate the relationships among models. In our design, the top layer is called the model family , a collection of
models with strong correlations. For example, when using large models for downstream tasks, OmniForce manages
derived models as a family. The middle layer is called the model register , pointing out the model that is currently in use.
The bottom layer is called the model asset , representing the raw model generated by the AutoML pipeline.
10

--- PAGE 11 ---
Model Deployment Deploying a trained model into a production environment is a crucial part of model management
and plays a large role in the MaaS paradigm because many developers leverage AI capabilities to build applications
through provided APIs after their models are deployed.
OmniForce applies KServe [ 52] for deploying a model into production and quickly building an MaaS architecture.
The model runs on a Kubernetes cluster in a serverless form, and users can access the model through the provided
API. OmniForce is designed with a crowdsourced code specification, and the models produced by the code written by
algorithm developers according to these guidelines can be quickly deployed into production. The deployed models will
have the following properties:
1. Support rolling updates and rollbacks.
2. Support high concurrency and low latency.
3.Support autoscaling, including scaling to 0, to resolve the conflict between latency sensitivity and demand
predictability.
4. Support advanced deployment methods such as canary release, blue–green release, and A/B testing.
5. Support the simultaneous deployment of multiple models and cascaded models.
6. Support multistage conditional model inference.
7. Support model explanation.
8. Support model monitoring.
Model Monitoring Model monitoring is an operational stage in the ML lifecycle that comes after model deployment.
Since the production environment changes all the time, a production model will result in performance loss, which is
called model drift. Model drift comes in two forms:
1.Data drift. Data drift is caused by data distribution changes. Since AI models are sensitive to the given data
distribution, as the data distribution changes more drastically, the performance of the model drops rapidly.
2.Concept drift. Concept drift refers to the situation when a model is no longer applicable to its environment due
to changes in the properties of the dependent variable.
Model monitoring is used to monitor whether the model has drifted in the current environment. Our model monitoring
system can monitor models in production in real time, send out warnings when model drift occurs, and initiate model
retraining using recently collected data to iteratively update models and keep their performance at an acceptable level.
New models generated by model retraining are automatically archived in the original model family and updated with a
new version.
3.1.4 Informative Visualization and Insights
As the core of human-centered AI, visualization helps users understand their data and algorithms. OmniForce provides
rich visualization information, such as data distributions, search spaces, hyperparameter importance levels, and inference
statistics. Users can join the solution construction loop by analyzing this information and helping OmniForce build
more versatile models. For example, users can redesign the search space or adjust the data distribution to construct
better models.
Hyperparameter Importance Analysis The performance of a deep learning model highly depends on its hyperparam-
eter settings. Some modern optimization methods have been proposed and have successfully optimized hyperparameters
automatically. However, these methods do not interpret how the specific hyperparameters affect the resulting model
performance. To provide users with a more apprehensible hyperparameter report, OmniForce extracts the relationships
between hyperparameters and metrics with a hyperparameter importance assessment method, named functional analysis
of variance (fANOV A) [53].
Given the model metrics and corresponding hyperparameter settings, fANOV A fits a random forest to approximate
the mapping between the hyperparameter space and the performance space. Then, fANOV A is applied to assess the
importance of each hyperparameter. Furthermore, we make some improvements to adapt this method to hyperparameters
with hierarchical dependencies.
Real-Time Model Inference Explanation Currently, most deep models work in a black-box way, which lacks
explainability and hinders the application of deep models in many fields. Following the idea of HAML, OmniForce
provides a real-time model explanation service to explain the model’s output. Specifically, OmniForce builds an
11

--- PAGE 12 ---
explainable model and deploys this model along with the target model using KServe [ 52]. When users make an
inference online, both models operate, and the explainable model analyzes the results of the target model. Based on
the explanatory information, users can understand why their model outputs such a result. Additionally, considering
that interpretable methods should be compatible with the various heterogeneous models supported by our platform,
we mainly choose black-box model interpretation algorithms, such as anchors [54]. This method can approximate the
decision-making process of a neural network model to a rule-based discrimination process. Its local interpretation
characteristics facilitate the interpretation of a single sample predicted by the model and help users intuitively understand
the interpretation process.
Multiobjective Model Performance Evaluation
Training and Deploying Comprehensive Metrics Most existing AutoML platforms only focus on the model
performance achieved during the training phase, ignoring the deployment phase. OmniForce bridges this gap with the
task sidecar. The model developed in the training phase is sent to deployment environments such as Qualcomm A650
[55] and the NVIDIA Jetson Develop Kit [ 56] to benchmark the model performance in production. The metrics on the
deployment side, such as latency and power, are collected to compute a comprehensive reward for the next round of
model searching.
Model Robustness Model robustness is the ability to resist external disturbances, which is a prerequisite for widely
using AI. A robust model can produce stable outputs and adapt to various environments in production. This feature is
quite critical in some applications, such as healthcare, finance, and security. Model robustness is closely related to the
underlying data distribution. Recent research has found that a slight data deviation may causing the associated model to
give completely different results, highlighting the fact that current AI models are too sensitive to data. In production,
these problems may be encountered accidentally, such as by a natural data distribution shift. However, they may also be
intentional, such as hacking attacks. OmniForce provides a model robustness evaluation service to evaluate a model’s
ability to resist noise attacks. Users can choose the model to be deployed in production according to its comprehensive
accuracy and robustness performance. Additionally, OmniForce provides a robustness evaluation tool, which includes
two popular evaluation methods: model adversarial attack evaluation [ 57,58] and model privacy evaluation based on
membership inference [ 59,46,58]. The former evaluates the model’s robustness under adversarial examples, and the
latter evaluates the model’s data privacy under membership inference.
3.2 Pipeline-Driven Training and Deployment Collaboration for Crowd-sourcing
After uploading and processing the given dataset on OmniForce, to generate an industrial application, users need
to upload their own models or choose a crowdsourcing model recommended by the OmniForce formatter. Then,
OmniForce organizes the entire process from training to deployment through an automated pipeline. In particular, based
on the pipeline-driven approach, the adaptation and miniaturization of large models can be achieved spontaneously.
When searching for a new model, an engineer or data scientist completes the algorithm interface according to the
document and configures the corresponding search space design, running device (CPU, GPU), and optimization metrics.
3.2.1 Application Algorithm Interface
A new program tracer is implemented in our design, as shown in Listing 1. To crowdsource a new model, users need to
complete the interface and wrap several functional blocks of Python code. Considering the desire for a user-friendly
interface, complex message sending and receiving operations are implemented internally.
Listing 1: Python user application algorithm interface for crowdsourcing with the decorator.
from omnitools import estimator
c l a s s YourEstimator ( metaclass= ABCMeta) :
""" This is the abstract base class for OmniForce task estimators."""
@estimator.wrap
def _ _ i n i t _ _ (
s e l f ,
run_epochs : i n t ,
data_path : str ,
i s _ t r a i n e r : bool ,
* *model_args , # Parameters customized for your algorithm.
) −> None :
12

--- PAGE 13 ---
self.model = YourModel (* *model_args )
@abstractmethod
@estimator.wrap
def calculate_score ( s e l f ) −> f l o a t :
""" Compute scores to evaluate search models. """
return val_score
def run ( s e l f ) −> f l o a t :
""" Omniforce triggers the training process through this function. """
# train
s e l f . t r a i n ( s e l f . t r a i n _ l o a d e r , self.run_epochs )
# test
val_score = s e l f . c a l c u l a t e _ s c o r e ( s e l f . v a l i d _ l o a d e r )
# save models
i fs e l f . i s _ t r a i n e r :
s e l f . s a v e ( s e l f . s a v e _ d i r )
return val_score
3.2.2 Application Algorithm Configuration
Search Space Configuration Hyperparameter tuning is a crucial step when generating and deploying AI algorithms
for industrial applications. For the new model search task, OmniForce allows users to tune their hyperparameters
and customize the search space. In our configuration workflow, search spaces can be configured via user-friendly
interactions on the front end or by uploading YAML files for complex spaces.
Various tasks may have very different definitions of search spaces. Generally, in deep learning models, there are always
dependencies among the parameters. For example, when searching the network structure of a neural network, we
usually want to explore the network’s depth and width. Depth indicates how many convolutional blocks or layers are in
the backbone network, while width always refers to the number of channels per block or layer. Therefore, the length of
the channel array to be sampled depends on the sampled depth value. If the depth is three, three channel values are
sampled from the search space, as shown in Listing 2. Another complication concerns conditional space. For example,
suppose that different kinds of blocks with various parameters are searched in a deep model. In that case, the parameters
to be sampled are determined after sampling the types of the corresponding blocks. These situations frequently occur in
the search spaces of various algorithms. Therefore, OmniForce supports tree- and DAG-based search space sampling
rules, modeling the dependencies among the parameters as a graph.
Listing 2: A multilayer search space example.
backbone_nums_block :
type : i n t
range : [ 2 . . . 5 ]
submodule :
block_type :
type : c h o i c e
range : { r e s n e t , t r a n s f o r m e r }
submodule :
r e s n e t :
nums_layer :
type : i n t
range : [ 3 . . . 7 ]
submodule :
nums_channel :
type : c h o i c e
range : {64 , 256}
transformer :
mlp_expend_ratio :
type : c h o i c e
range : {1 , 2 , 4 , 8}
13

--- PAGE 14 ---
Inference Configuration OmniForce requires users to provide corresponding configurations during the batch in-
ference phase of crowdsourcing, including the number of inference devices and inference resource usage. These two
metrics ensure that OmniForce can build an inference environment for large amounts of data.
Deployment Configuration During the deployment phase, OmniForce also requires users to provide appropriate
configurations to quickly deploy their models into production. These configurations include the deployment devices,
deployment resource usage, and single-sample inference latency. The latency can help us deduce the QPS after the
model is deployed.
Cloud-Edge Collaborative Training Environments and Requirements OmniForce proposes a novel AutoML
practice on the basis of a cloud–edge collaborative framework. In this way, users are able to develop the most suitable
models for their specific devices under both performance and latency metrics by installing the OmniForce cloud–edge
collaborative python package and registering their devices. In addition, when training large models, OmniForce can
support the interaction between the production environment and the supercomputing environment.
Inference Optimization Due to the lack of deployment considerations, many AutoML frameworks consider only the
performance of the resulting model on the search side and ignore the performance of the model in the actual deployment
environment. On edge devices such as ARM [ 60] and ROCm [ 61], OmniForce addresses this issue by using TVM [ 62],
a deep learning compiler that enables high-performance ML anywhere. We incorporate TVM to establish a connection
between the training and deployment environments through the task sidecar and complete the collaborative search
process of the AutoML task during the training and deployment tests through the relay method. This comprehensive
search strategy helps OmniForce find models that excel in production. OmniForce also uses different tools for specific
devices, such as TensorRT [ 63] for NVIDIA GPUs and OpenVINO [ 64] for Intel CPUs. Moreover, to convert our
model between different machine learning frameworks, OmniForce uses ONNX [ 65] as an intermediary. In these
scenarios, we convert the model of the high-level framework such as PyTorch into ONNX format, a common file format
for machine learning models, and then further convert it into TensorRT or OpenVINO for targeted optimization of deep
learning inference on different devices.
3.2.3 Application Algorithm Register
After users implement their application algorithm with our standard interface and prepare the configurations well, this
algorithm can be sealed in a self-contained docker image and registered in the OmniForce crowdsourcing system. Our
system conducts a smoke test to validate the completeness of the algorithm, which has three main stages: searching,
inference, and deployment.
1.The search test verifies whether the given algorithm can be searched using OmniForce’s AutoML search
strategy. The search space configuration is checked during this phase.
2.The inference test verifies whether the algorithm can perform offline batch inference. The inference configura-
tion is checked in this phase.
3.The deployment test verifies whether the algorithm can be deployed and generates the corresponding API. The
deployment configuration is checked in this phase.
In general, OmniForce evaluates four capabilities of the tested algorithm: searchability, batch inference, cloud
deployment and edge-side optimization. Notably, all the above tests are not required to pass, but passing as much as
possible is recommended so that the algorithm can be applied in more scenarios. For example, if an algorithm only
passes the searching test, it can only be used for hyperparametric optimization but is not able to be deployed in a
production environment. After passing the smoke test, users’ algorithms are crowdsourced on OmniForce with version
control. Both docker images and configurations are versions controlled for agile updating in the future.
3.2.4 Pipeline Generation
Analogous to the AutoML pipeline described in Section 2.1.7, the OmniForce crowdsourcing system adopts a pipeline-
driven method to automatically apply the provisioned and crowdsourced algorithms. The generated pipeline can flexibly
support different tasks. For example, a user might need a model that is empowered by a large model to run on devices
with limited resources, such as edge devices. OmniForce may use a pipeline that consists of two steps, including large
model adaptation and multiobjective optimization, to generate efficient models.
14

--- PAGE 15 ---
3.3 (Large) MaaS
Leveraging the power of large models, recent trends in the AI community have shed light on the performance
improvement yielded by scaling. Large models have achieved success in AI products such as ChatGPT [ 14] and
Pathways [ 34]. However, large models bear massive memory and computation consumption burdens. Practicability
may be hindered when large models are deployed on edge devices or applied to situations with limited computation
and memory resources. Especially when the application scenario requires low latency, such as autonomous driving,
a large model with low inference speed cannot guarantee accurate online prediction, which inevitably induces safety
problems. Due to their large capacities and state-of-the-art performance, large models exhibit high generalization across
a large number of tasks. In practice, this usually comes with a high pretraining cost, making these models unsuitable
for applications involving frequent model adaptation and update steps in industrial cases. Therefore, when using large
models in these domains, there is a need for adaptation and miniaturization procedures that increase the iteration
and inference speeds of the models while retaining their performance to the maximum extent possible. OmniForce
supports large model adaptation and miniaturization technologies, mainly including automatic adaptation, filtering, and
knowledge distillation with model inference optimization, as shown in Figure 5.
3.3.1 Large Model Support
OmniForce supports large model technology. At present, AI faces a variety of industries and business scenarios and
their needs; for example, people need to design neural architectures, adjust hyperparameters, and deploy models based
on the hardware requirements for each specific scenario. The large model concept is a breakthrough technology for
general-purpose AI that aims to solve the fragmentation problem of AI applications. OmniForce supports large model
technology with highly efficient and uniform adaptation in computer vision 3.5.2 and NLP 3.5.3 tasks, as well as
automated adaptation 3.3.2 and miniaturization 3.3.3. Users can search and train either their own large models through
the crowdsourcing interface or the large model provided by OmniForce. The method of invoking a model through
the simple API and service of the large model lowers the barrier for users to access, thus shortening the cycle of
developing and iterating AI products.The large model workflow is demonstrated in Figure 6. According to the system
meta knowledge and the user’s interactions, OmniForce decides whether to use large or small models and when to reuse
or update models in practice.
3.3.2 Adaptation
With the development of high-performance accelerators, the sizes of models have grown exponentially [ 4]. Due to the
lack of labeled resources, to train such large models, some self-supervised methods have achieved great success, such
as MAE [ 66] and masked language modeling [ 4]. The resulting self-supervised pretrained model is then fine-tuned
to adapt to the specific task and dataset. Some well-known pretrained checkpoints, such as bidirectional encoder
representations from transformers (BERT) [ 67] and Clip [ 68], have become popular, facilitating a range of downstream
applications. However, under this trend, given a realistic downstream dataset, it is difficult to train a model from scratch
or frequently fine-tune an entire large model for each task due to the massive computational and storage costs required.
To solve these problems, some effective parameter tuning methods have aroused the interest of researchers. Three of
the most impressive branches of this field are Prompt [ 4], Adapter [ 69], and low-order decomposition [ 70], which are
supported in OmniForce. In addition, with the help of the knowledge base maintained by the system, the formatter 2.1.6
can automatically create a suitable pipeline to select and search for adaptation modules that are appropriate for the
downstream tasks and the large model. After the adaptation process, the large model serves users through a simple API.
3.3.3 Miniaturization
While large models have shown great power in many tasks, certain situations, such as real industrial environments,
require running models with limited resources, such as Internet of Things (IoT) edge devices. In this case, the inference
time of a large model may take up a large part of the overall system, leading to a large response time. Therefore,
developers prefer a model that forms a tradeoff between performance and speed. OmniForce supports two ways to
generate efficient models powered by large models. One way is to minimize a high-capacity model under downstream
tasks, and the other way is to transfer knowledge during the pretraining (upstream) stage from the large models.
Miniaturization in the Downstream Stage Filtering has been a popular technique for reducing the parameters and
computation costs of large models. Current methods mainly learn to drop some unimportant connections or channels,
which is also known as neural network pruning or compression [ 71]. The filtering variable can be represented by a
binary mask, where 1 indicates that the corresponding connection or channel is kept; otherwise, it should be pruned. A
mask is learned to induce sparsity, which leads to smaller numbers of parameters and computations. After filtering the
unimportant connections and channels, the smaller model is usually fine-tuned on the original training task to restore
15

--- PAGE 16 ---
Small ModelSmall ModelLarge Model
Edge DevicesAutomatedMiniaturization
Small ModelAutomatedAdaptationAutoML
Open VINO, TensorRT,ONNXRuntimeNCNN, TVMTensorRT, 
LargeModelAdapterLoRAPromptMixture-of-ExpertsLarge Model
APIProjectDataMeta-Data
Users(Developers)Cloud
Figure 5: Illustration of the large MaaS paradigm in OmniForce. Given input meta-data and the corresponding data,
OmniForce automatically creates an AutoML pipeline from selecting an algorithm to deploying the final output model,
which may be a large model or an efficient model. Large models and their derived adaptive models can be served
through the API. To meet the needs of different objectives, the derived adaptation model can be automatically generated
by a combination of one or more adaptation methods. The miniaturization approaches enable efficient model generation
for edge devices. OmniForce provides the optimized and accelerated model as an API to developers, lowering the
barrier to using AI technology.
the performance drop. Recently, filtering studies on transformers have learned a sparse mask to keep only a small
portion of the total tokens to induce acceleration [ 72]. Knowledge distillation is developed to distill the rich knowledge
of a pretrained model, named the teacher, into a new model, named the student [ 73]. Generally, the teacher model
is a large model with a large number of parameters. It has a slow inference speed but enjoys high performance. The
student model is a small-capacity model formed by handcrafted design or NAS [ 74,75,76,77]. It runs fast, but directly
training it will not yield a satisfactory performance. Adopting the knowledge distillation technique [ 78], we use the
large model as the teacher and train the small model with extra supervision from the representation output obtained
from the last layer of the large model. The small model trained with knowledge distillation performs better than its
original version. We can enjoy low latency and high performance when deploying the small model on edge devices.
Miniaturization in the Upstream Stage Small models are always limited by their low capacity to absorb knowledge
from large datasets, while large models trained with massive data have better transfer capabilities for downstream tasks.
However, pretraining a large model with large datasets is costly. Performing distillation during the pretraining stage is a
novel way to tackle this problem [ 79,80]. OmniForce supports the miniaturization method by using the knowledge
distillation technique in the pretraining phase without training large models from scratch. Such a pipeline is set up in
OmniForce to facilitate the sharing of large models and generate fast and frequent iterative models.
16

--- PAGE 17 ---
EvaluationReuseTraining
Adaptationinthe downstreamMiniaturization in the downstreamActive learningMiniaturization in the upstream𝐵"##$𝐵"##%𝐴"##%$𝐵"'%$offline unlabeled and labeled data for training or updating large modelonline unlabeled and labeled data
offline labeled dataRequest for labelingoffline unlabeled data
Model DriftLarge model training & continuallearningSurrogate modelFigure 6: Illustration of the large model workflow in OmniForce. Five blocks and different types of data interact in the
workflow, including the model drift detector, large model training and updating, active learning, miniaturization and
adaptation. A surrogate model is designed for efficiently training and updating large models. OmniForce provides an
elegant and consistent AutoML technology to solve the complicated problems in each block.
3.4 Flexible Search Strategy Framework
The search strategy is one of the most important parts of an AutoML technique, as it conducts the entire search process
given a search space. A well-designed search strategy tends to be the key to efficiency and efficacy. In this subsection,
we introduce the interface of our search strategy to show what role it plays in our framework as well as a flexible search
framework in OmniForce.
3.4.1 Search Strategy Interface
In our framework, without any other additional restrictions, the interface of the search strategy consists of two functions,
generate_tasks and handle_rewards, and a search space object, as Listing 3 shows. The former function is used to
generate the hyperparameters that are most appropriate for the given space according to observations. When all tasks
are completed or timed out (see Section 2.1.1) during the current iteration, the latter function is called to handle the
rewards of these tasks and update the observations for the next iteration.
Listing 3: Search strategy interface.
c l a s s SearchStrategy ( metaclass= ABCMeta) :
""" This is the abstract base class for OmniForce search strategies. """
def bind_space ( s e l f , search_space ) :
self.search_space = search_space
@abstractmethod
def generate_tasks ( s e l f ) :
pass
@abstractmethod
def handle_rewards ( s e l f , rewards ) :
pass
3.4.2 Search Strategy Framework
OmniForce supports a variety of search strategies such as revised hyperband [ 32], MF-NAS [ 33], novel BO and
evolution approaches. BO is a sample-efficient method that aims to find x∗= arg minx∈χf(x), where fis a black-box
17

--- PAGE 18 ---
function that is expensive to evaluate and χis the search space or domain [ 81]. BO consists of two main components: a
surrogate model for modeling the response surface of fand an acquisition function forming an exploitation-exploration
tradeoff. While many libraries have been developed for BO, such as Spearmint [ 82], GPyOpt [ 83], scikit-optimize [ 84],
RoBO [ 85], ProBO [ 86], GPyTorch [ 87] and BoTorch [ 88], they all focus on exploiting a certain aspect, and there is
no one inclusive framework that can hold them all. For example, some advanced batch BO methods, such as local
penalization (LP) [89], cannot be implemented easily in the methods mentioned above except GPyOpt.
To accommodate increasingly advanced BO algorithms, OmniForce utilizes a composable BO framework that maintains
five main component sets, including a surrogate model, an acquisition function, an acquisition optimizer, a candidate
generator, and a suggester. Similar to BoTorch, our framework is built on Pytorch [ 90] and benefits from autodiffer-
entiation and GPU acceleration, and the overview of our framework is shown in Figure 7. For the surrogate model,
we implement the most popular methods, such as GPs [ 82], SMAC [ 91], BNNs [ 92,93], and a novel method (OF) to
address discrete optimization problems.
Our proposed OF surrogate model with the OF acquisition function achieves state-of-the-art accuracy on NAS-Bench-
201 [ 94]. Moreover, considering multitasks and high-dimensional optimization problems, we design OF-Trans and
OF-HD, respectively, and implement some existing methods, which is unavailable in other BO libraries. Regarding
the acquisition functions, in addition to popular basic functions such as the expected improvement (EI) [ 95], lower
confidence bound (LCB) [ 96] and entropy search (ES) [ 97] functions, we also support some powerful batch acquisition
techniques such as LP and Monte Carlo acquisition functions. To provide a batch of new quires, some additional logic is
usually needed, which inspires us to define a new component in the BO framework called the suggester. In the inference
stage, the suggester takes over all other components and conducts the generation process for new quires. For example,
in most batch BO settings, we need to look ahead to those pending tasks and fine-tune the surrogate model using MC
samples [ 82] or constant liars [ 98] to obtain new quires. To optimize the acquisition function, we introduce a candidate
generator to sample sufficient candidates as the starting points for the lbfgs optimizer, which is the most common choice
in the BO context. Different sampling methods can be easily plugged in, and the sampling space changes with the
search space to adapt to some search space shrinking methods.
During the training stage, we use the observations to fit the surrogate model via MCMC or gradient-based methods.
When a trained surrogate model is given, the recommender controls the generator to generate candidates and optimize
the acquisition function according to the posterior and the acquisition optimizer. Then, with the new queries suggested
by the requester sent to the database, the parallel workers reserve these new tasks for evaluation purposes and send the
observations back for the next iteration until the budget is exhausted.
In conclusion, compared to other existing BO libraries, our contributions to the BO framework are highlighted as
follows.
• Flexible and composable.
• Novel methods for several problems.
• Autodifferentiation and GPU acceleration.
3.5 Widely Provisioned Application Algorithm
AI applications can be found in industrial production and our daily lives. This ubiquity of AI is reflected not only in the
variety of available application scenarios, including cloud-edge collaborations, VR integration, and open-environment
AI, but also in the diversity of the utilized application algorithms, such as table data analysis and processing, NLP,
computer vision, AIGC, and graph representation learning. As scenario diversity has been introduced above, this
chapter mainly focuses on the widely provisioned application algorithm in OmniForce.
3.5.1 Tabular Data
Tabular data analysis is a long-standing topic that performs association analysis on structured data and mines complex
business relationships through feature combination and feature extraction. Common tasks in this field include time
series analysis[99][100], abnormality detection[101][102], click-through rate forecasting[103][104], etc.
As an AI application under the HAML framework, OmniForce encourages users to focus more on data collection,
custom feature pipelines, and model design, which can yield improved performance through the human experience.
For fundamental exploratory data analysis, data cleaning, null filling, category coding, and other tedious but necessary
procedures, meta-learning-based feature engineering automatically completes the above tasks to reduce the burden
imposed on humans. In general, automatic feature engineering is mainly divided into three parts: cleaning and filling,
feature transformation, and feature combination. OmniForce provides various standard processing methods for different
18

--- PAGE 19 ---
Training
 surrogate
modelInference
parameters
fittingcandidates
generatoracquisition
functionacquisition  
optimizer
trained surrogate
modelcandidatesposteriorgradientsEvaluation
new observationsDatabase
Parallel
Workernew quires
resultsnew tasks
...
......suggester
GPMCMC
Gradient
descent
OFEI
LCB
q-EI
LP-LCB
MCEI
OFUniform
samplingSobolThompson
samplingLP Sequential
SMACLiar MC Acq
observationOF-T rans OF-HDlbfgs samplingFigure 7: The BO framework of OmniForce. Five key component sets are contained in our framework: a surrogate
model, an acquisition function, an acquisition optimizer, a candidate generator, and a suggester. In addition, we divide
the BO procedure into three steps. During the training stage, we train the surrogate model to fit the observations via
gradient descent or MCMC methods. Then, during the inference stage, we suggest finding the optimal candidates as
new queries with the trained surrogate model and the acquisition function. During the evaluation stage, as the new
queries are sent to the database, we use the parallel workers in OmniForce to evaluate these candidates and update the
observations for the next iteration.
types of features as search spaces and then obtains the most suitable processing methods for the given data. For example,
for numerical features, the methods used to filling empty values include taking the mean, median, upper quartile,
and lower quartile; feature transformation methods include max-min, z score, and log scale normalization; feature
combination methods include multiplication and division and other conventional mathematical transformations.
Ensemble models based on decision trees, such as random forests [ 105] and the LightGBM [ 106], have always been
favored by the industry due to their low computational costs and high interpretability. In recent years, breakthroughs in
neural networks have drawn attention to deep learning models that have achieved impressive results in tasks such as
recommender systems and click-through rate prediction[ 103][107]. We use both tree-based ensemble models and deep
learning models to construct a search space to ensure excellent performance on various challenging datasets.
Large Tabular Data In the current era of big data, ML algorithms are used to analyze massive amounts of data,
serving human life and industrial production and bringing tremendous value[ 108]. Therefore, the ability to efficiently
train models on large tabular datasets is a highly competitive issue for IT companies.
Compared to training a deep model on a GPU, reading hundreds of GB of data from a disk into memory is redundant
and highly time-consuming for each worker/task estimator. Therefore, we propose a systematic design named double
shared memory to speed up the process of training on large tabular data in a node. On the one hand, we save one piece
of training data to /dev/shm of the Linux system, mount it into multiple worker containers, and use the high-speed
19

--- PAGE 20 ---
access memory to quickly load the data. In this way, the task estimators of the same node can share the same data in
memory, which we call the outer shared memory. On the other hand, due to automatic feature engineering, the data
used by each task estimator are different. To avoid the time cost of feature engineering, we enable a sharing mechanism
called inner shared memory. The first epoch performs feature engineering and model training in a batch-by-batch
manner and writes the processed data into the container, which allows subsequent epochs to save time during their file
loading and feature engineering processes.
This efficient memory sharing approached used for data loading can be widely applied to deep learning models such as
multilayer perceptrons (MLPs). In addition, for nodes with small amounts of memory, the scheduler can use only inner
or outer shared memory to flexibly accelerate the training process.
Time Series Data As one of the most challenging tabular data problems with numerous applications, time series
forecasting has been one of the primary research areas that the AI community has attempted to solve with ML and deep
learning[109][110].
Adhering to the human-centric philosophy, we open up interactions such as time series intervals and forecasting time
lengths. Users can create customized models based on data characteristics and business scenarios according to their
experience. Likewise, automated feature engineering cleans and fills the data, identifies multiseries cases, groups them,
and generates temporal features such as sliding time windows and lags.
The search space includes traditional autoregressive models[ 111] tree-based models[ 9][106], and the recently proposed
transformer-based models [ 112]. The formatter in subsection 2.1.6 assigns weights to the models for datasets with
different statistics according to the available historical information to ensure the optimal performance.
3.5.2 Computer Vision
Computer vision, as the main research field of AI, aims to extract, process, and understand the information contained in
digital images and videos. Traditional computer vision algorithms, such as support vector machines (SVMs) [ 113],
usually solve specific tasks via handcrafted feature engineering. Benefiting from their data-driven learning scheme,
deep learning algorithms have achieved great success in various computer vision tasks. On the one hand, end-to-end
training and inference strategies enable deep learning models to be easily adapted to different computer vision tasks.
On the other hand, with the development of deep neural network architectures (e.g., AlexNet [ 114] and ResNet [ 1]), the
capability of deep learning models is growing quickly, and an increasing number of deep learning models are being
employed in real-world applications.
Although deep learning models, especially deep convolutional neural networks (CNNs), have demonstrated promising
performance in computer vision tasks, there still exists a relatively large gap for these models to become robust
and generic computer vision models. Recently, supported by the rapid growth of computational resources and
massive amounts of visual data, super-deep models have attracted increasing attention from the computer vision
community [ 115,116,117,118,119,120]. Relying on their powerful learning capacity, super-deep models can learn
general and discriminative representations. Additionally, their strong modeling capacity also enables super-deep
models to adapt to a new scenario with a small amount of labeled data [ 117,118,120]. Such an ability is essential
in real-world applications. For example, we can validate the feasibility of technical solutions with fewer costs and
thus accelerate the development cycle. To help users quickly develop and deploy models for specific applications, we
provide various super-deep vision models for different vision tasks, e.g., 2D/3D object detection [ 121,122], semantic
segmentation [ 123,124], road and lane detection [ 125,126], image matting [ 127,128], keypoint detection [ 129,118],
scene text detection and spotting [ 130,131,132]. By providing task descriptions and some labeled samples, users
can easily obtain high-performance task-specific models from the platform. Moreover, super-deep models usually
require large resources for deployment, while users prefer lightweight models with fewer parameters and higher
inference efficiency. To satisfy such requirements, we provide several solutions to compress and speed up these models.
More specifically, we develop efficient model compression techniques, such as quantization-based and pruning-based
techniques, to compact and accelerate the models. Moreover, relying on a designed search space containing various
operations, blocks, loss functions, etc., our platform can automatically search a lightweight model and improve the
performance of the searched model with the guidance of the super-deep models via knowledge distillation.
3.5.3 Natural Language Processing
Natural language processing (NLP) is one of the major branches of AI that aims to automatically process human
languages (both spoken and written) with computers, and the tasks involved are often classified as cognitive intelligence.
NLP has evolved from several disciplines, e.g., computer science, AI, and linguistics. NLP can be basically divided
into two categories: 1) Natural language understanding (NLU). NLU explores the strategies that enable computers to
20

--- PAGE 21 ---
grasp textual instructions provided by human users. The most common NLU tasks include text classification [ 133],
sentiment analysis [ 134,135,136], question answering [ 137,138], named entity recognition [ 139,140], etc. 2) Natural
language generation (NLG). NLG allows computers to generate textual outputs after understanding user inputs in natural
languages such as English and Chinese. The common NLG tasks include machine translation [ 141,142,143,144,145],
summarization [146, 147], dialogue [148, 149], etc.
Although deep learning-based NLP models have demonstrated promising performance on a series of tasks, the existing
learning paradigm lacks the capacity to leverage many tasks and data, leading to a series of issues, e.g., model training
redundancy for different tasks, data island problems, complex deployment problems, and poor learning ability in
low-resource scenarios. To help users efficiently and effectively develop and deploy models for different applications,
our system consists of a series of foundation models, including a super model for general language understanding and
generation [ 150,151,152], a super model for cross-lingual generation [ 153,154,155,156,157], and their efficient
tuning and distillation versions [ 158], distillation-based prompt learning [ 159] and PESF-KD [ 160], respectively.
By providing the task description and a few labeled samples, the users can obtain a high-performing model/small
adapter tuned by our built-in foundation models from the platform. Encouragingly, with our efficient tuning strategy of
distillation-based prompt learning, our server-end foundation model requires finetuning only 0.5% of the parameters,
which is on par with the parameters of the original foundation model, while achieving comparable or even better
performance. In this way, the users only need to deploy their small prompt/adapter, which is an efficient and private
way to incrementally update the user-end small prompt/adapter without uploading their precious data to our platform.
3.5.4 Learning Multimodal Deep Generative Models
One of the main objectives of artificial intelligence and machine learning is to learn and manipulate high-dimensional
probability distributions of real-world data [ 161]. By doing so, these technologies can extract valuable insights from
data that can be used to improve many related tasks [ 162]. In recent years, deep generative models have emerged
as a powerful means of learning data distributions. These models, which include generative adversarial networks
(GANs) [ 163,164,165,166], Vector-Quantized Variational Autoencoders (VQ-V AEs) [ 167,168], autoregressive
models [ 169,170], and diffusion models [ 171,172], have demonstrated impressive capabilities in a wide range of
applications. By learning the underlying probability distribution that generated the data, researches can gain insights
into the underlying mechanisms of the data-generating process. Furthermore, well-trained generative models can be
widely used in content generation-related tasks.
Our system consists of a series of built-in deep generative models, which have been designed to improve the realism
of generated content and deliver a generative model that can handle general content generation tasks. By learning
the feature alignment between different modalities, these models can generate more diverse, high-quality content.To
achieve these goals, we have developed a set of advanced algorithms that can train and apply these generative models
to a variety of real-world applications. These algorithms have been designed to enhance the performance of the
generative models in tasks such as visual concept exploration, generation controllability, and content diversity. Overall,
our built-in models and algorithms have been used in several artificial intelligence-generated content (AIGC) tasks,
including vision-language generation [ 173], complex scene generation [ 174], portrait animation [ 175,176], 3D object
rendering [ 177], etc. We believe that our system has the potential to revolutionize the field of content generation and
pave the way for new, innovative applications of AIGC technologies in the future.
3.5.5 Graph Representation Learning
Graph data are all around us; examples include social graphs, knowledge graphs, and protein structures. Typically, a
graph consists of nodes and edges that connect the nodes. Even sentences and images can be represented by graphs: the
words in a sentence and the patches of images can be treated as nodes, and the connections between nodes represent
their edges. Considering that graphs are ubiquitous, it is important to analyze graph data and learn graph representations
for solving node-level, edge-level, and graph-level applications.
Graph neural networks (GNNs) are neural networks that operate in the graph domain. Our systems provide a variety
of built-in GNNs, such as graph convolutional networks (GCNs) [ 178], graph attention networks (GATs) [ 179], and
graph transformers [ 180]. For example, our systems provide a type of efficient graph transformer, i.e., Gapformer,
that deeply incorporates graph pooling into a graph transformer. By using Gapformer, the negative impact of having
several unrelated nodes is minimized while long-range information is preserved, and the quadratic complexity of
message passing is reduced to linear complexity. In addition, our systems develop many diverse plugin modules to
improve the capabilities of GNNs. For instance, our systems adopt SkipNode [ 181], which samples graph nodes in each
convolutional layer to skip the convolution operation, thereby alleviating the oversmoothing and gradient vanishing
problems of GCN-based networks. Our systems also use a plug-and-play scheme for graph pooling, referred to as MID,
with a multidimensional score space and two score operations, to explore the diversity of the node features and graph
21

--- PAGE 22 ---
structures in graphs to achieve improved graph-level representations. Furthermore, our systems also provide typical
graph applications, including network structures that are designed for learning on signed network embeddings [ 182],
GCNs with multilevel learning for hyperspectral image classification [ 183], and heterophily networks for scene graph
generation [184].
4 Features
Ease of Use for Development-Deployment Collaboration Committed to building models that are suitable for
production, OmniForce devises a veritable development-deployment collaborative model construction framework.
Unlike many AI platforms that only have the ability to release the model in production in an agile manner with a CI/CD
pipeline, OmniForce bridges the development and deployment environments and adopts a multiobjective optimization
method to construct more practical and versatile models in the searching and training phases. OmniForce aims to enable
both developers with limited ML expertise and data scientists to deploy their own model services with only a few clicks.
Industrial Availability of Open-Environment Adaptation Unlike conventional AutoML platforms, we propose
OmniForce to study open-environmental and open-loop problems because data, labels, features, models, evaluations,
and metrics usually change during the learning process in practice [ 185]. Our intuition is that we need to involve people
in the loop for leveraging human knowledge and enhancing human capabilities based on the smooth interactions shown
in Figure 1 to achieve the goal of HAML.
Cloud-Native Production and (Large) MaaS An increasing number of large-scale systems are being built through
containers and equipped with a container orchestration system to manage all components, as these system generally
have the advantages of high resource utilization, strong isolation, and continuous delivery. OmniForce is designed
based on Kubernetes, which means that OmniForce is a fully cloud-native AutoML system and can leverage many
excellent cloud-native tools. Based on Kubernetes and Kubeflow, OmniForce supports multitenancy, high scalability,
strong disaster recovery capabilities, and automated transformation from a trained model into a deployment service.
Crowdsourcing OmniForce supports large-scale algorithms and extends the set of applied algorithms. With a system
that was widely used among a group of engineers, it was previously possible to directly search and deploy a new task
on the new dataset through crowdsourced application algorithms. To inspire the concept of crowdsourcing, we start
with ML version management for data, labels, models, algorithms, and search spaces; pipeline-driven development and
deployment collaboration; a flexible search strategy framework; and a broad offering of applied algorithms that include
super-deep (large) model-based methods.
5 Evaluation
This section contains three parts. The first subsection gives a brief introduction to the innovations provided by the
industrial metaverse. The second part presents a set of use cases in a human-centered real-world industrial metaverse
scenario, showing the practical operation of XR simulation, continuous data acquisition, crowdsourcing, and cloud-edge
collaboration to solve open-loop AI problems. Finally, we demonstrate the capabilities of OmniForce through some
experiments on scalability, fault tolerance, search performance, algorithm performance, and a human-centered AutoML
practice.
5.1 Innovations of the Industrial Metaverse
Standard assembly lines have employed AI technologies to improve the production efficiency of a single factory. Recent
advancements in industrial metaverse technologies will further the application of AI to the next level and change the
structure of the entire supply chain. The industrial metaverse has formed a new manufacturing paradigm, encouraged
collaboration between factories, and accelerated the connection between the upstream and downstream parts of the
industrial chain. For example, in the conventional customer-to-manufacturer (C2M) business model, manufacturers
produce small market testing batches before products’ final releases and then improve their product designs based on
market feedback or increase production when products sell well. However, the emergence of the metaverse will change
the structure of the conventional C2M business model.
With the development of immersive experiences in the virtual world, digital content from the real economy will be used
as primary data to help construct the digital world. Technology has reconstructed the form of the existing industrial
chain. The metaverse allows consumers to experience products and make purchasing decisions during the product
design stage, allowing manufacturers to obtain more detailed feedback in this stage. They improve their product
22

--- PAGE 23 ---
designs based on customers’ feedback and even sell digital content services (new revenue streams). In addition, the
emergence of the metaverse will cause changes in the supply and demand structure. It will shorten the distance between
manufacturers and customers by eliminating the intermediate steps.
In the next part, we provide a case of using OmniForce to help manufacture self-driving vehicles in the context of
the industrial metaverse. In the design stage, user experience and feedback are involved. In the development and
manufacturing stage of the metaverse, OmniForce uses cloud-edge collaboration technology to conduct rapid product
iteration as well as feedback to improve the performance and user experience of the product. Additionally, OmniForce
assists in automatic order disassembly, automatic order placement, supply chain sourcing, and intelligent stocking in
the whole supply chain service.
5.2 User Case of the Industrial Supply Chain and Industrial Metaverse
Design In this case, manufacturers aim to design self-driving cars that are suitable for different scenarios. For example,
citizens from different cities may prefer diverse colors and patterns in their cars. Some small shapes may be better suited
for delivering packages and driving around communities than massive trucks. This mission can rely on OmniForce’s
AIGC capabilities. In this generation task, the client wants the tool to automatically generate car models. After a car
model dataset is fed into the system, OmniForce outputs a model to generate content. The resulting car model can
be simulated and checked by an XR system with customer interaction. This step shortens the process of consumer
feedback and is a way of implementing industrial metaverse design programs, saving time and budget resources. If the
obtained results do not meet the requirements of consumers, the user can adjust the reward of the model on OmniForce,
triggering the model production loop again until the results can be used for the next step.
Development and Manufacturing In this case, the customer is from an advanced delivery company with new
technology equipment empowered by AI. They want to run a set of models across different scenarios on their smart
vehicle, which involves massive devices with different deployment (inference) requirements in this real industrial
scenario. For example, they needed three models with different requirements:
• a truck traveling between cities;
• a pickup truck driving on the city’s main streets;
• and a microdelivery van traveling across communities and buildings;
Specifically, when driving on highways between cities, trucks can reach speeds of 80 km/h or higher in relatively clean
road conditions. Therefore, the constructed model needs to respond quickly and responsively to obstacles in front of the
truck. While running on main streets, the model deployed on the pickup truck should handle complex road conditions,
such as pedestrians, bicycles, motorcycles, and pets. In contrast, tiny courier vehicles usually travel across communities
and buildings at low speeds but may drive on icy roads under extreme weather.
To address these challenges, OmniForce constantly applies an automated pipeline including data collection and model
searching to extract values from the development, deployment, and maintenance phases. To collect the data, we can
collect a set of example images from public datasets or the real world. Usually, we need some data requirements to
make the AI model learn well. One requirement is that the scenes and the types of cars, pedestrians, and trees on the
road should be divergent. After uploading the data to OmniForce and obtaining the resulting dataset, we may find
that "sedans" or "city roads on sunny days" appear much more frequently in the images than other types of cars or
scenes, leading to a long-tailed distribution problem. Therefore, the human-centered open loop is triggered, and the cue
from OmniForce is that users need to constantly collect other types of data, such as sports cars, vans, and limousines.
OmniForce can also generate images in different types of scenes that include highways, crowded bicycles, motorcycles,
wet roads on rainy days, shadows of trees, and children in neighborhoods. As new data are acquired, OmniForce updates
the version of the data to satisfy the imposed balance requirements. To search for models under different constraints,
OmniForce supports cloud-edge collaboration for developers with limited ML expertise and data scientists to adjust
multiple objectives such as accuracy, recall, power, and latency, modifying the evaluations and rewards to automatically
produce various models. After generating the model, we can validate the model in a simulated system where data are
collected by drivers in XR applications and then test the model in some small-scale scenarios before deployment. When
model drift occurs, OmniForce involves people in the loop for checking the data, collecting data if needed, re-searching
or retraining the model, and updating the model’s version.
Supply Chain Service In the process of the industrial metaverse, intelligent warehousing and stocking realize a fast
supply chain and reduce the raw material budget and production time. That is, people can use automated ML technology
to obtain an efficient supply chain service, including ordering, storage, transportation, and marketing. OmniForce
supports the analysis and prediction of tabular data and time series data. After automatically generating models with
23

--- PAGE 24 ---
16 64
Number of GPUs01234Time (hour)Acc=0.87 / 64 GPUs
Acc=0.89 / 64 GPUs
Acc=0.91 / 64 GPUs
Acc=0.94 / 64 GPUs
Acc=0.87 / 16 GPUs
Acc=0.89 / 16 GPUs
Acc=0.91 / 16 GPUs
Acc=0.94 / 16 GPUs(a) Evolutionary algorithm results obtained on CIFAR-10
16 64
Number of GPUs024Time (hour)Acc=0.87 / 64 GPUs
Acc=0.89 / 64 GPUs
Acc=0.91 / 64 GPUs
Acc=0.94 / 64 GPUs
Acc=0.87 / 16 GPUs
Acc=0.89 / 16 GPUs
Acc=0.91 / 16 GPUs (b) Hyperband results obtained on CIFAR-10
Figure 8: Scalability of OmniForce with the evolutionary algorithm and Hyperband. The red lines represent the
experiments conducted using 64 GPUs, while the blue lines represent the experiments conducted using 16 GPUs. Lines
with various patterns show experiments with different precision results. The experiments conducted using 64 GPUs
take less time to achieve the same accuracy than the experiments conducted using 16 GPUs.
0 100 200 300 400 500
Number of CandidatesTrial 0Trial 1Trial 2Trial 3
384 / Acc:95.84% 1286 / Acc:95.97% 294 / Acc:95.60% 2320 / Acc:95.60% 4
Figure 9: Four trials concerning fault tolerance. These trials achieve comparable performance with various fault
tolerances. Yellow bars show defective candidates, while blue bars represent surviving candidates. We mark the number
of surviving candidates and the resulting accuracy next to the bars.
OmniForce, some simulation systems, such as cellular automata and small-scale tests, can be used to validate the
model at a relatively low cost. Based on the feedback, visualizations and explanations of the searched architectures
and hyperparameters, OmniForce provides a convenient interface to bring people in the loop, guiding them to tune
the search space, refine the metrics, collect the combined data, adjust the simulation system, trigger the next model
production cycle, and update the version of the data and model. During this process, algorithms can be implemented by
a crowdsourced knowledge base. By standardizing the data abstraction processes, application algorithms, and search
spaces, OmniForce makes it easy to integrate and reuse application algorithms and search spaces. Furthermore, users
can learn the knowledge of ML pipelines from OmniForce.
5.3 Empirical Results
Scalability and Fault Tolerance OmniForce supports scalable search jobs assigned by the scheduler. As shown in
Figure 8, the experiments run with different scalable resources and parameters on the same dataset and use two search
algorithms: an evolutionary algorithm [ 186] and Hyperband [ 187]. In large-scale testing cases, larger groups with more
computational resources can achieve the same accuracy in less time than those with fewer resources. For example,
experiments conducted on 64 GPUs take approximately one hour to achieve above 90% test accuracy. In contrast,
experiments conducted on 16 GPUs take more than four hours to obtain the same accuracy. Hence, the models trained
on 16 GPUs cannot reach good performance under the time constraint shown in Figure 8 (b). Additionally, experiments
conducted on 16 GPUs and 64 GPUs take similar amounts of time to achieve low accuracy since reaching such an
inferior level of performance is not challenging for a search job.
It can be seen that in Figure 9, different failure rates have little effect on performance. The four experiments achieve
comparable performance but with different candidate fault rates. In these experiments, we add perturbations to kill
some surviving candidates. With our carefully designed semisynchronization scheme, jobs can continue to run under a
limited number of dead candidates or be paused and restarted during a search.
Search Performance Based on our proposed BO framework, we design a novel BO method that is well-suited for
discrete optimization problems. We compare our method with various NAS algorithms and BO methods on a popular
benchmark (NAS-Bench-201 [ 94]), which contains 15625 network architectures and their evaluations on three visual
24

--- PAGE 25 ---
Table 1: Top-1 test accuracy (%) for classification on NAS-Bench-201. The first block shows the results of parameter
sharing-based NAS methods. The second block shows the results of nonparameter sharing algorithms and various BO
methods. The third block shows the results of our proposed BO method. The †symbol means that the performance of
the corresponding method is directly obtained from NAS-Bench-201 [94].
MethodsCIFAR-10 CIFAR-100 ImageNet-16-120
valid test valid test valid test
RSPS†[188] 80.42 ±3.58 84.07 ±3.61 52.12 ±5.55 52.31 ±5.77 27.22 ±3.24 26.28 ±3.09
DARTS-V1†[189] 39.77 ±0.00 54.30 ±0.00 15.03 ±0.00 15.61 ±0.00 16.43 ±0.00 16.32 ±0.00
DARTS-V2†[189] 39.77 ±0.00 54.30 ±0.00 15.03 ±0.00 15.61 ±0.00 16.43 ±0.00 16.32 ±0.00
GDAS†[190] 89.89 ±0.08 93.61 ±0.09 71.34 ±0.04 70.70 ±0.30 41.59 ±1.33 41.71 ±0.98
SETN†[191] 84.04 ±0.28 87.64 ±0.00 58.86 ±0.06 59.05 ±0.24 33.06 ±0.02 32.52 ±0.21
ENAS†[74] 37.51 ±3.19 53.89 ±0.58 13.37 ±2.35 13.96 ±2.33 15.06 ±1.95 14.84 ±2.10
GibbsNAS†[192] 90.02 ±0.60 92.72 ±0.60 68.88 ±1.43 69.20 ±1.40 42.31 ±1.69 42.08 ±1.95
REA†[193] 91.19 ±0.31 93.92 ±0.30 71.81 ±1.12 71.84 ±0.99 45.15 ±0.89 45.54 ±1.03
RS†[16] 90.93 ±0.36 93.70 ±0.36 70.93 ±1.09 71.04 ±1.07 44.45 ±1.10 44.57 ±1.25
REINFORCE†[194] 91.09 ±0.37 93.85 ±0.37 71.61 ±1.12 71.71 ±1.09 45.05 ±1.02 45.24 ±1.18
BOHB†[195] 90.82 ±0.53 93.61 ±0.52 70.74 ±1.29 70.85 ±1.28 44.26 ±1.36 44.42 ±1.49
TPE [196] 91.30 ±0.18 94.07 ±0.17 71.93 ±0.89 72.08 ±0.83 45.71 ±0.68 45.94 ±0.83
SMAC [91] 91.23 ±0.21 94.05 ±0.23 72.17 ±0.61 72.21 ±0.76 45.51 ±0.33 46.08 ±0.74
BOHAMIANN [93] 91.36 ±0.16 94.13 ±0.23 72.36 ±0.82 72.38 ±0.81 45.93 ±0.66 46.18 ±0.60
OF-BO 91.52 ±0.05 94.35 ±0.03 73.21 ±0.29 73.25 ±0.18 46.27 ±0.36 46.54 ±0.19
classification datasets. Following the setting of [ 94], we search on the CIFAR-10 validation set after 12 epochs of
training and then directly look up the evaluations in other datasets. We run these BO methods for 80 iterations with 12
initial points and report the mean and standard deviation of the best observation encountered during the search process
across 10 duplicated runs. Table 1 illustrates that our method achieves the best performance on all three datasets.
Algorithmic Performance Here, we provide the performance of the models provisioned by OmniForce. First, we
show our state-of-the-art scaled-up vision models (ViTAE [ 197,117]) and a comparison with other transformer-based
deep models in Table 2. We find that with over one hundred million parameters, the models are able to achieve
impressive performance. Relying on such powerful capacity, the models can perform well on new scenarios after being
fine-tuned with few labeled data. As a result, users can validate new methods and quickly deploy models.
Table 2: The performance of scaled-up ViTAE models on the ImageNet1K dataset.†indicates that ImageNet22K is
used to further fine-tune the models with 224 ×224 resolution for 90 epochs.
#ParamsTest
sizeImageNet
Top-1Real
Top-1
Swin-L†[198] 197 M 384 87.3 90.0
SwinV2-L†[199] 197 M 384 87.7 -
CoAtNet-4†[200] 275 M 384 87.9 -
CvT-W24†[201] 277 M 384 87.7 -
ViT-L∗[66] 304 M 224 85.5 90.1
ViT-L [202] 304 M 224 85.7 -
ViTAE-L [117] 311 M 224 86.0 90.3
ViTAE-L†[117] 311 M 224 87.5 90.8
ViTAE-L†[117] 311 M 384 88.3 91.1
SwinV2-H [203] 658 M 224 85.7 -
SwinV2-H [203] 658 M 512 87.1 -
ViTAE-H [117] 644 M 224 86.9 90.6
ViTAE-H [117] 644 M 512 87.8 91.2
ViTAE-H†[117] 644 M 224 88.0 90.7
ViTAE-H†[117] 644 M 448 88.5 90.8
25

--- PAGE 26 ---
In addition, we also report the NLP performance of our built-in platform models on NLU (i.e., performance on the
GLUE benchmark) and NLG (i.e., machine translation tasks). Table 3 shows the contrastive results obtained on 9
NLU tasks with one model, showing that our method can leverage any existing fine-tuned prompt to achieve better
transfer learning performance. Importantly, with our built-in efficient approach, even better performance than that
achieved with full model tuning can be attained by tuning only 0.5% of the original parameters, which is extremely
critical for users to perform low-resource/low-cost training and deployment with only a few (or even zero) labeled data.
Figure 10 shows the performance of our Vega-MT translation models, where we participate in 10 shared tasks, including
Chinese ↔English (Zh ↔En), German ↔English (De ↔En), Czech ↔English (Cs ↔En), Russian ↔English (Ru ↔En),
and Japanese ↔English (Ja ↔En). With our multilingual foundation model, we achieve 7 championships, 2 runners-up
finishes, and 1 third-place result with respect to the BLEU points. A platform with Vega-MT can empower users with
the ability to easily understand and generate any cross-lingual content. Notably, our platform also could speed up the
language generation process by switching to our developed non-autoregressive generation algorithms [204, 205, 206].
Table 3: Results (%) of part of the cross-task efficient prompt transfer experiment based on foundation language
models. Note that our method is model-agnostic and therefore can be used to enhance any foundation model. Here, we
use BERT-Large as an example. In groups (a) and (b), each cell denotes the target task performance achieved when
transferring the prompt from the source task (row) to the associated target task (column). “A VG.” denotes the average
performance across all target tasks. Notably, positive prompt transfers are in bold, and numbers in the subscripts
indicate relative improvements.
Method CB COPA WSC RTE WIC CoLA MRPC STSB Conll 04 A VG.
model tuning 94.6 69.0 68.3 75.8 74.9 60.6 88.0 90.0 85.6 78.5
prompt tuning 87.5 76.0 64.4 76.2 66.9 63.8 86.8 90.5 85.5 77.5
(a) Transfer with the vanilla prompt transfer approach
MNLI 96.4 71.0 67.3 80.9 66.5 58.9 88.2 91.0 83.0 78.1
QNLI 89.3 76.0 65.4 76.2 70.4 63.7 88.5 90.7 83.5 78.2
Record 78.6 63.0 65.4 53.8 51.7 0.0 77.7 85.0 82.7 62.0
SQuAD 87.5 74.0 66.3 71.8 51.7 6.0 87.3 89.3 82.5 68.5
CoNLL03 73.2 64.0 63.5 60.3 51.9 0.0 71.3 16.4 84.8 53.9
Ontonotes 78.6 65.0 66.3 56.7 54.1 59.3 82.4 84.5 86.1 70.3
CoNLL05 87.5 65.0 64.4 69.3 68.3 61.3 88.7 88.4 83.8 75.2
CoNLL12 89.3 62.0 67.3 63.2 67.4 58.7 90.4 88.5 83.6 74.5
SST2 92.9 74.0 64.4 71.8 66.8 60.1 87.0 89.6 84.3 76.8
(b) Transfer with Our built-in efficient approach
MNLI 92.9 77.0 67.3 78.0 68.8 66.3 88.5 90.6 85.4 79.4 1.3
QNLI 92.9 77.0 66.3 77.3 70.8 63.9 87.5 90.8 86.6 79.2 1.0
Record 87.5 76.0 66.3 77.3 68.5 62.4 87.5 90.7 84.9 77.9 15.9
SQuAD 89.3 75.0 66.3 75.5 69.3 63.1 87.3 88.9 85.7 77.8 9.3
CoNLL03 91.1 72.0 68.3 76.9 67.4 63.6 86.5 90.6 85.6 78.0 24.1
Ontonotes 89.3 74.0 66.3 76.2 69.1 64.2 88.0 90.8 85.7 78.2 7.8
CoNLL05 87.5 79.0 65.4 77.6 69.6 63.7 87.5 90.8 84.8 78.4 3.2
CoNLL12 87.5 76.0 66.3 74.4 68.5 63.7 87.5 90.8 85.0 77.7 3.3
SST2 92.9 77.0 68.3 76.5 70.1 64.8 88.5 90.7 86.3 79.5 2.7
Finally, we report the comparison results of a complex scene generation experiment conducted with OmniForce. The
quantitative results produced by the involved competitors on both the COCO-stuff and Visual Genome datasets are
reported in Table 4. For a fair comparison, we adopt their officially released pretrained models or the officially reported
scores in their papers. Compared with both CNN-based and transformer-based complex scene generation methods,
TwFA [ 207] achieves significant improvements in terms of all metrics. Furthermore, since we employ the same texture
tokenization strategy utilized in the transformer-based approach, HCSS [ 208], the generation performance demonstrates
how well a transformer can model the compositions of complex scenes with focal attention.
HAML Practice As mentioned above, both human-assisted ML and ML-assisted humans play important roles in
an AutoML system. This section takes a Kaggle competition—Test Time Cost Forecasting for Mercedes-Benz—as
an example to introduce how the OmniForce platform implements the human-centric concept and achieves efficient
human–machine interaction.
The purpose of the competition is to forecast the time required for the user-defined auto to pass a safety test according
to the provided anonymous dataset, thereby helping the Mercedes-Benz team optimize the test system. The dataset
is automatically recognized as a tabular dataset by OmniForce after the user uploads it to the platform. Furthermore,
26

--- PAGE 27 ---
1st
!
1st
!1st
!
1st
!1st
!1st
!1st
!2nd
"2nd
"3rd
#Figure 10: Vega-MT achieves state-of-the-art BLEU points on 7 out of 10 high-resource translation tasks among all
constrained systems in WMT-2022 and significantly outperforms the competitive transformer- BIG baselines in terms
of BLEU scores.
Table 4: Comparisons among the results obtained on COCO-stuff[ 209] and Visual Genome (VG) [ 210]. All the results
are taken from the original papers and are based on a 256×256resolution. ‘-’ means that the related value is unavailable
in the corresponding papers.
FID↓ SceneFID ↓ Inception Score ↑ Diversity Score ↑
COCO VG COCO VG COCO VG COCO VG
LostGAN-V2 [211] 42.55 47.62 22.00 18.27 18.01±0.50 14.10±0.38 0.55±0.09 0.53±0.09
OCGAN [212] 41.65 40.85 - - - - - -
HCSS [208] 33.68 19.14 13.36 8.61 - - - -
LAMA [213] 31.12 31.63 18.64 13.66 - - 0.48±0.11 0.54±0.09
Frido [214] 37.14 - 14.91 - 18.62±0.54 - - -
TwFA [207] 22.15 17.74 11.99 7.54 24.25±1.04 25.13±0.66 0.67±0.00 0.64±0.00
exploratory data analysis reports are generated, which helps humans further explore the statistics of the data from all
aspects. The relevant interfaces are shown in Figure 11 and Figure 12.
Under variegated datasets and limited candidates over a complex search space, pure machine-based AutoML has
difficulty finding the optimal solution in one iteration. Therefore, OmniForce introduces a multiround optimization
process with human–machine interaction.
In the first round, we directly input the raw data into the OmniForce platform and run the AutoML lifecycle, including
automatic merge table generation, automatic data processing, automatic feature engineering, model search, and
hyperparameter tuning, to obtain the baseline result. Regarding the lack of human intervention, the formatter described
in section 2.1.6 configures the search space for regression tasks according to the knowledge base and then generates the
AutoML pipeline to be run. An interactive search detail page is shown in Figure 13, including model performance,
candidates’ performance, parallel hyperparameter coordinates, etc. After obtaining the result by batch inference, we
evaluate it on the Kaggle website to simulate the model’s online service scenario. Our primitive results obtain an r2
score of 0.54956 on the private leaderboard, and the corresponding ranking is 1540 out of a total of 3823 teams (Late
Submission).
In the second round, we modify the search space by following the OmniForce advisor presented in Section 2.1.8, which
provides suggestions for changing the search space and performing feature engineering. Humans can significantly
improve the machine’s efficiency based on the above suggestions and their own experience.
Search process visualization is a long-standing topic in AutoML systems and has recently attracted widespread attention
recently[ 215][216][217][218]. Furthermore, we argue that efficient ways to display and modify the search space are
necessary prerequisites for humans to assist machines in HAML systems. On the search detail page, OmniForce shows
the current candidates and search space in three different diagrams, which are shown in Figure 14 (full view), Figure 15
(dimensionality reduction view), and Figure 16 (pairwise view).
27

--- PAGE 28 ---
Figure 11: Data detail page. The meta-data, data preview and version management interface are shown here. The
meta-data contain data IDs, related feature pipeline IDs and file sizes. The uploaded data can be previewed on this page
for further checking. Version management is used to handle rapid data iteration and update solutions in time.
Figure 12: Data analysis page. OmniForce can help users gain more insights into data in an efficient manner by
analyzing data based on automatically displayed statistical features such as histograms and means.
28

--- PAGE 29 ---
Figure 13: Search detail page. The details of the job, such as the data version and feature pipeline version, are illustrated
on this page. The performance preview and detailed candidate information are also displayed in the charts.
The initial state of the editing interface is the configuration suggested by the advisor. The OmniForce advisor summarizes
some optimization schemes based on data statistics, the current search results, and the knowledge base. Most suggestions
are general and portable; users can choose whether to adopt them or further improve them according to their experience.
After interacting with the figure, a comparison between the modified and original search spaces is shown in Figure 17,
which can be previewed and further edited.
We adjust the search configuration above and obtain an r2 score of 0.5511 on the private leaderboard, obtaining a
ranking of 820/3823, which illustrates the effectiveness of the advisor.
Additionally, people can pay attention to every aspect of the search procedure to enhance the capability of the machine.
The above three diagrams related to the search space can be directly edited by clicking or using the lasso tool. Namely,
we can further reduce the max_depth of the tree and the ratio of row sampling at the cost of a few clicks, and the process
for doing so is shown in Figure 18. After absorbing the valuable experience of humans, the search performance of the
machine is further improved, with an r2 score of 0.55184 on the private leaderboard, and the corresponding ranking is
338.
Next, the advisor provides suggestions for feature engineering based on the observed information by analyzing the
training data and the importance levels of the features in the search procedure. Specifically, the suggested features are
shown in Figure 19, and users can choose whether to apply the suggestions based on their experience. After completing
feature configuration, the platform creates a new version of the feature pipeline and automatically applies the selected
changes to the original data, as shown in Figure 20.
After making the above modification based on the machine’s suggestion, we obtain a score of 0.55284, and the
corresponding ranking is 76.
29

--- PAGE 30 ---
Figure 14: Parallel hyperparameter coordinate diagrams and the machine’s suggestions. Users can obtain insights
from the relationships between the hyperparameters and model performance. For the convenience of adjusting the
hyperparameters, OmniForce provides a multilevel search space configuration interface next to these parameters.
In the last round, users can unleash their feature engineering abilities to further achieve improved performance.
Specifically, humans can enter the feature engineering interface through the ‘Go to feature pipeline’ button and modify
the data based on experience by executing SQL statements or preset methods, as shown in Figure 21. Finally, we obtain
a score of 0.55394, and the corresponding ranking is 11 out of 3823.
As shown in Figure 22, all of the derived improvement points come from human experience and the machine suggestions,
which illustrates the advancement of the human–machine interaction feature proposed by the OmniForce system. The
above attempts are saved as different feature pipeline versions, search configuration versions, and model versions for
users to switch between as needed.
6 Related Work
In recent years, some commercial companies have released their own AutoML platforms for industrial applications.
Some identify market segments and develop different products for different tasks. For example, Amazon SageMaker
Canvas [ 219] allows business analysts to build AI models and gives accurate predictions for tabular data in a no-code
manner, while Amazon Forecast [ 220] employs a time series prediction service. IBM Watson AutoAI [ 29] supports the
construction of classification and prediction tasks for tabular data, while Watson Natural Language Understanding [ 221]
focuses on advanced text analytics. Others such as Abacus [ 222], Microsoft Azure AutoML [ 223] and Google Cloud
Vertex AI [ 224] support different types of data and tasks in one platform. OmniForce chooses the latter option and
targets a general and reusable AI application production pipeline. To break the isolated island of task automation and
single-point optimization, OmniForce emphasizes human-centered operations; involves business people in the process
30

--- PAGE 31 ---
Figure 15: Search space heatmap and the machine’s suggestions. OmniForce maps the high-dimensional and multilevel
search space to a two-dimensional space, which can clearly show the relationships between the candidates and
performance. Incumbents represent candidates for achieving large performance gains, and regions selected by lasso
regression represent suggested search spaces.
Figure 16: Hyperparameter binary heatmap and the machine’s suggestions. The binary hyperparameter heatmap reflects
the interaction relationship between the two selected hyperparameters, which can help humans make finer adjustments.
31

--- PAGE 32 ---
Figure 17: Comparison between the modified and original search spaces. Row-by-row comparisons can clearly draw
attention to search space modifications. Furthermore, users can directly modify the search space by using the syntax of
YAML files.
Figure 18: The process of modifying the search space. We use balls to represent categorical hyperparameters and click
them to change their value ranges; sliders are used to represent numerical hyperparameters, and the search space can be
modified by dragging the sliders.
32

--- PAGE 33 ---
Figure 19: Feature importance and the machine’s suggestions. The importance of all features is shown on the search
detail page, and the advisor provides suggestions for further adjustments. Humans can choose whether to apply the
suggestions based on their own experience.
Figure 20: Suggested feature construction method in the feature pipeline. All applied feature construction methods are
automatically executed and updated to the new feature pipeline version, which is described in 3.1.2.
33

--- PAGE 34 ---
Figure 21: Customized feature pipeline. After inheriting the feature pipeline of the previous version, users can further
create features based on their insight into feature importance. The version of the feature pipeline can be flexibly
switched to compare the final performance or adapt to different scenarios.
Figure 22: Illustration of human–machine interaction in OmniForce. Based on the human–machine interactions in both
the search space and feature pipelines, the HAML system can achieve better performance.
34

--- PAGE 35 ---
Table 5: Comparison of the features in open-source and commercial AutoML platforms. We compare them based on
the provisioned application and the system point of view. * means that we did not find enough published information to
verify the corresponding item. V . G. represents "very good".
HyperOpt Katib NNI Ray-Tune Abacus Vertex AI AutoAI Canvas Azure AutoML OmniForce
Application
Black-box tuning Yes Yes Yes Yes No Yes No No No Yes
Computer vision No No No No Yes Yes Yes No Yes Yes
NLP No No No No Yes Yes Yes No Yes Yes
Tabular and time series No No No No Yes Yes Yes Yes Yes Yes
AI-generated content No No No No No No No* No No* Yes
System
Automatic scalability No Yes Part Yes Yes Yes Yes Yes* Yes Yes
Cloud-native No Yes Part Part Yes Yes Yes Yes Yes Yes
Visualizations Min Min Min Min Good Good V . G. Good Good Good
Cloud-edge collaboration No No No No No Part No No Part Yes
Large model supported No No No No No* Yes* No No Yes Yes
Crowdsourcing No Part No Yes No Yes No No Yes* Yes
Advanced search strategy Yes No Part Yes Yes Yes Yes Yes Yes Yes
MLOps No Part No No Yes Part Yes Part Yes Yes
of constructing of AI applications; and integrates business, data, algorithms, and maintenance in a collaborative and
continuous way.
An overview of the ecosystem of AutoML systems is presented in Table 5, where some open-source platforms such as
HyperOpt [22], Katib [27], NNI [23], Ray-Tune [21] and commercial platforms such as Abacus [222], Google Vertex
AI [224], IBM Watson AutoAI [ 29], Amazon SageMaker Canvas [ 219], Microsoft Azure AutoML [ 223] are compared
with OmniForce. OmniForce is one of the rare systems to support crowdsourcing, cloud-edge collaboration, super-deep
models (large models), and human-centric systems. OmniForce is a cloud-native AutoML system that can be run in a
production environment for commercial use.
7 Conclusion
Due to the proposed human-centered and cloud-edge collaborative AutoML method and the widely provisioned
applications that enable large models and their highly efficient transfer strategies, OmniForce builds a practicable and
powerful system for people and companies that want to benefit from AI technology in open-environment scenarios,
such as the industrial supply chain and industrial metaverse, where people often face open-loop problems and need
to train or update large models and deploy models on massive devices with different constraints and requirements.
OmniForce has a uniform user interface, a seamless and flexible search strategy framework, and cloud-native features,
making it an accessible, versatile and product-ready system for helping people establish and improve AI engineering in
practice and continuously extract business value from AI technology.
Acknowledgments
We thank Dr. Xiaodong He for providing valuable support and constructive comments. This work was (partially) done
during Dr. Jing Zhang’s visit and Jiaxing Li’s internship at JD Explore Academy.
References
[1]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR , 2016.
35

--- PAGE 36 ---
[2]Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object
detection. In CVPR , 2016.
[3]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NeurIPS , 2017.
[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In
NeurIPS , 2020.
[5]Qiantong Xu, Alexei Baevski, Tatiana Likhomanenko, Paden Tomasello, Alexis Conneau, Ronan Collobert,
Gabriel Synnaeve, and Michael Auli. Self-training and pre-training are complementary for speech recognition.
InICASSP , 2021.
[6]Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,
Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition.
arXiv preprint arXiv:2005.08100 , 2020.
[7]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language
supervision. In ICML , 2021.
[8]Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. In ICML , 2021.
[9] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In SIGKDD , 2016.
[10] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson,
Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings
of the 1st workshop on DLRS , 2016.
[11] Tesla artificial intelligence & autopilot. https://www.tesla.com/AI , 2022.
[12] Google translation. https://translate.google.com/ , 2022.
[13] Apple siri. https://www.apple.com/siri/ , 2022.
[14] Openai chatgpt. https://openai.com/blog/chatgpt/ , 2022.
[15] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V . Le, and
Alexey Kurakin. Large-scale evolution of image classifiers. In ICML , 2017.
[16] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine
Learning Research , 2012.
[17] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning
algorithms. In NeurIPS , 2012.
[18] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using
reinforcement learning. In ICLR , 2017.
[19] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In ICLR , 2019.
[20] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation
hyperparameter optimization framework. In SIGKDD , 2019.
[21] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Stoica. Tune: A
research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118 , 2018.
[22] James Bergstra, Brent Komer, Chris Eliasmith, Dan Yamins, and David D Cox. Hyperopt: a python library for
model selection and hyperparameter optimization. Computational Science & Discovery , 2015.
[23] Nni: An open source automl toolkit for neural architecture search, model compression and hyper-parameter
tuning. https://github.com/microsoft/nni , 2022.
[24] Orion: An asynchronous framework for black-box function optimization. https://github.com/Epistimio/
orion , 2022.
[25] Auto-sklearn: An automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.
https://github.com/automl/auto-sklearn , 2022.
[26] Kubernetes: Production-grade container orchestration. https://kubernetes.io/ , 2020.
[27] Johnu George, Ce Gao, Richard Liu, Hou Gang Liu, Yuan Tang, Ramdoot Pydipaty, and Amit Kumar Saha. A
scalable and cloud-native hyperparameter tuning system. arXiv preprint arXiv:2006.02085 , 2020.
36

--- PAGE 37 ---
[28] Google cloud automl: Train high-quality custom machine learning models with minimal effort and machine
learning expertise. https://cloud.google.com/automl , 2022.
[29] Ibm watson autoai: Build and train high-quality predictive models quickly. simplify ai lifecycle management.
https://www.ibm.com/cloud/watson-studio/autoai , 2022.
[30] Amazon sagemaker: Build, train, and deploy machine learning (ml) models for any use case with fully managed
infrastructure, tools, and workflows. https://aws.amazon.com/sagemaker , 2022.
[31] H2o driverless ai: Award-winning automatic machine learning (automl) platform. https://h2o.ai/platform/
ai-cloud/make/h2o-driverless-ai/ , 2022.
[32] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Bandit-
based configuration evaluation for hyperparameter optimization. In ICLR , 2017.
[33] Chao Xue, Xiaoxing Wang, Junchi Yan, and Chun-Guang Li. A max-flow based approach for neural architecture
search. In ECCV , 2022.
[34] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard,
Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi, Laurent Shafey, Chandu
Thekkath, and Yonghui Wu. Pathways: Asynchronous distributed dataflow for ml. In MLSys , 2022.
[35] Kubeflow: The machine learning toolkit for kubernetes. https://github.com/kubeflow/kubeflow/ , 2022.
[36] Training operator in kubeflow. https://github.com/kubeflow/training-operator , 2022.
[37] Mpi operator in kubeflow. https://github.com/kubeflow/mpi-operator , 2022.
[38] The distributed system toolkit: Patterns for composite containers. https://kubernetes.io/blog/2015/06/
the-distributed-system-toolkit-patterns/ , 2022.
[39] Kubeflow pipelines: A platform for building and deploying portable, scalable machine learning (ml) workflows
based on docker containers. https://github.com/kubeflow/pipelines , 2022.
[40] Amine Barrak, Ellis E Eghan, and Bram Adams. On the co-evolution of ml pipelines and source code-empirical
study of dvc projects. In SANER , 2021.
[41] Burr Settles. Active learning literature survey. 2009.
[42] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEE Symposium
on Foundations of Computer Science (FOCS’07) , pages 94–103. IEEE, 2007.
[43] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends ®
in Theoretical Computer Science , 2014.
[44] Cynthia Dwork. Differential privacy. In Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo Wegener,
editors, Automata, Languages and Programming . Springer Berlin Heidelberg, 2006.
[45] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. IEEE
Transactions on Information Theory , 2017.
[46] Fengxiang He, Bohan Wang, and Dacheng Tao. Tighter generalization bounds for iterative differentially private
learning algorithms. In UAI, 2021.
[47] Fengxiang He and Dacheng Tao. Foundations of deep learning . Springer, 2018.
[48] Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserv-
ing statistical validity in adaptive data analysis. In STOC , 2015.
[49] Kobbi Nissim and Uri Stemmer. On the generalization properties of differential privacy. Computer Research
Repository , 2015.
[50] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In ACM CCS , 2016.
[51] Luca Oneto, Sandro Ridella, and Davide Anguita. Differential privacy and generalization: Sharper bounds with
applications. Pattern Recognition Letters , 2017.
[52] The KServe authors. Kserve: Highly scalable and standards based model inference platform on kubernetes for
trusted ai. https://github.com/kserve/kserve , 2022.
[53] Frank Hutter, Holger Hoos, and Kevin Leyton-Brown. An efficient approach for assessing hyperparameter
importance. In ICML , 2014.
[54] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations.
InAAAI , 2018.
37

--- PAGE 38 ---
[55] Qualcomm adreno 650. https://developer.qualcomm.com/software/adreno-gpu-sdk/gpu .
[56] Nvidia jetson developer kits. https://developer.nvidia.com/embedded/jetson-developer-kits ,
2022.
[57] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In ICLR , 2018.
[58] Fengxiang He, Shaopeng Fu, Bohan Wang, and Dacheng Tao. Robustness, privacy, and generalization of
adversarial training. arXiv preprint arXiv:2012.13573 , 2020.
[59] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing
the connection to overfitting. In CSF, 2018.
[60] Arm. https://www.arm.com , 2022.
[61] Rocm. https://docs.amd.com/ , 2022.
[62] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q. Yan, Haichen Shen, Meghan Cowan,
Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: an automated
end-to-end optimizing compiler for deep learning. In OSDI , 2018.
[63] Tensorrt. https://developer.nvidia.com/tensorrt , 2022.
[64] Openvino. https://docs.openvino.ai/ , 2022.
[65] Onnx. https://onnx.ai/ , 2022.
[66] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are
scalable vision learners. In CVPR , 2022.
[67] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional
transformers for language understanding. In NAACL-HLT , 2019.
[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , 2021.
[69] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo,
Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML , 2019.
[70] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. In ICLR , 2022.
[71] P Molchanov, S Tyree, T Karras, T Aila, and J Kautz. Pruning convolutional neural networks for resource
efficient inference. In ICLR , 2019.
[72] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient
vision transformers with dynamic token sparsification. In NeurIPS , 2021.
[73] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
[74] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via
parameters sharing. In ICML , 2018.
[75] Yibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, and Zhouchen Lin. Ista-nas: Efficient and consistent
neural architecture search by sparse coding. In NeurIPS , 2020.
[76] Benteng Ma, Jing Zhang, Yong Xia, and Dacheng Tao. Auto learning attention. Advances in neural information
processing systems , 33:1488–1500, 2020.
[77] Yibo Yang, Shan You, Hongyang Li, Fei Wang, Chen Qian, and Zhouchen Lin. Towards improving the
consistency, efficiency, and flexibility of differentiable neural architecture search. In CVPR , 2021.
[78] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. Interna-
tional Journal of Computer Vision , 129:1789–1819, 2021.
[79] Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast
pretraining distillation for small vision transformers. In ECCV , 2022.
[80] Yutong Bai, Zeyu Wang, Junfei Xiao, Chen Wei, Huiyu Wang, Alan Yuille, Yuyin Zhou, and Cihang Xie. Masked
autoencoders enable efficient knowledge distillers. arXiv preprint arXiv:2208.12256 , 2022.
[81] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the human out of
the loop: A review of bayesian optimization. Proceedings of the IEEE , 2016.
38

--- PAGE 39 ---
[82] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning
algorithms. In NeurIPS , 2012.
[83] The GPyOpt authors. GPyOpt: A bayesian optimization framework in python. http://github.com/
SheffieldML/GPyOpt , 2016.
[84] Tim Head, Manoj Kumar, Holger Nahrstaedt, Gilles Louppe, and Iaroslav Shcherbatyi. scikit-optimize/scikit-
optimize. https://doi.org/10.5281/zenodo.5565057 , 2021.
[85] A. Klein, S. Falkner, N. Mansur, and F. Hutter. Robo: A flexible and robust bayesian optimization framework in
python. In NeurIPS Bayesian Optimization Workshop , 2017.
[86] Willie Neiswanger, Kirthevasan Kandasamy, Barnabás Póczos, Jeff Schneider, and Eric P. Xing. Probo: a
framework for using probabilistic programming in bayesian optimization. Computer Research Repository , 2019.
[87] Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson. Gpytorch:
Blackbox matrix-matrix gaussian process inference with gpu acceleration. In NeurIPS , 2018.
[88] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon
Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. In NeurIPS ,
2020.
[89] Javier González, Zhenwen Dai, Philipp Hennig, and Neil D. Lawrence. Batch bayesian optimization via local
penalization. In AISTATS , 2016.
[90] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS , 2019.
[91] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general
algorithm configuration. In LION , 2011.
[92] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali
Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using deep neural networks. In ICML ,
2015.
[93] Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust
bayesian neural networks. In NeurIPS , 2016.
[94] Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In
ICLR , 2020.
[95] J. Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of bayesian methods for seeking the
extremum. Towards Global Optimization , 2014.
[96] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Information-theoretic regret
bounds for gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory , 2012.
[97] Philipp Hennig and Christian J. Schuler. Entropy search for information-efficient global optimization. Journal of
Machine Learning Research , 2012.
[98] Clément Chevalier and David Ginsbourger. Fast computation of the multi-points expected improvement with
applications in batch selection. In LION , 2013.
[99] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer:
Beyond efficient transformer for long sequence time-series forecasting. In AAAI , 2021.
[100] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting?
arXiv preprint arXiv:2205.13504 , 2022.
[101] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder,
Emmanuel Müller, and Marius Kloft. Deep one-class classification. In ICML , 2018.
[102] Chen Qiu, Aodong Li, Marius Kloft, Maja Rudolph, and Stephan Mandt. Latent outlier exposure for anomaly
detection with contaminated data. In ICML , 2022.
[103] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine
based neural network for ctr prediction. In IJCAI , 2017.
[104] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and
Kun Gai. Deep interest network for click-through rate prediction. In SIGKDD , pages 1059–1068, 2018.
[105] Leo Breiman. Random forests. Machine learning , 2001.
39

--- PAGE 40 ---
[106] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
Lightgbm: A highly efficient gradient boosting decision tree. In NeurIPS , 2017.
[107] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson,
Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings
of the 1st workshop on deep learning for recommender systems , 2016.
[108] Seref Sagiroglu and Duygu Sinanc. Big data: A review. In CTS, 2013.
[109] Jan G De Gooijer and Rob J Hyndman. 25 years of time series forecasting. International journal of forecasting ,
2006.
[110] Bryan Lim and Stefan Zohren. Time-series forecasting with deep learning: a survey. Philosophical Transactions
of the Royal Society A , 2021.
[111] George EP Box and David A Pierce. Distribution of residual autocorrelations in autoregressive-integrated moving
average time series models. Journal of the American statistical Association , 1970.
[112] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with
auto-correlation for long-term series forecasting. In NeurIPS , 2021.
[113] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning , 1995.
[114] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural
networks. In NeurIPS , 2012.
[115] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12104–12113,
2022.
[116] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,
et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 12009–12019, 2022.
[117] Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao. Vitaev2: Vision transformer advanced by exploring
inductive bias for image recognition and beyond. International Journal of Computer Vision , pages 1–22, 2023.
[118] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose+: Vision transformer foundation model for
generic body pose estimation. arXiv preprint arXiv:2212.04246 , 2022.
[119] Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Advancing plain
vision transformer towards remote sensing foundation model. IEEE Transactions on Geoscience and Remote
Sensing , 2022.
[120] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion
parameters. arXiv preprint arXiv:2302.05442 , 2023.
[121] Wen Wang, Jing Zhang, Yang Cao, Yongliang Shen, and Dacheng Tao. Towards data-efficient detection
transformers. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part IX , pages 88–105. Springer, 2022.
[122] Chen Chen, Zhe Chen, Jing Zhang, and Dacheng Tao. Sasa: Semantics-augmented set abstraction for point-based
3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages 221–229,
2022.
[123] Haobo Yuan, Xiangtai Li, Yibo Yang, Guangliang Cheng, Jing Zhang, Yunhai Tong, Lefei Zhang, and Dacheng
Tao. Polyphonicformer: unified query learning for depth-aware video panoptic segmentation. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVII ,
pages 582–599. Springer, 2022.
[124] Yangyang Xu, Xiangtai Li, Haobo Yuan, Yibo Yang, Jing Zhang, Yunhai Tong, Lefei Zhang, and Dacheng Tao.
Multi-task learning with multi-query transformer for dense prediction. arXiv preprint arXiv:2205.14354 , 2022.
[125] Zhe Chen, Jing Zhang, and Dacheng Tao. Progressive lidar adaptation for road detection. IEEE/CAA Journal of
Automatica Sinica , 6(3):693–702, 2019.
[126] Lefei Zhang, Meng Lan, Jing Zhang, and Dacheng Tao. Stagewise unsupervised domain adaptation with
adversarial self-training for road segmentation of remote-sensing images. IEEE Transactions on Geoscience and
Remote Sensing , 60:1–13, 2021.
[127] Jizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. Privacy-preserving portrait matting. In Proceedings of the
29th ACM International Conference on Multimedia , pages 3501–3509, 2021.
40

--- PAGE 41 ---
[128] Sihan Ma, Jizhizi Li, Jing Zhang, He Zhang, and Dacheng Tao. Rethinking portrait matting with privacy
preserving. arXiv preprint arXiv:2203.16828 , 2022.
[129] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human
pose estimation. In Advances in Neural Information Processing Systems .
[130] Bo Du, Jian Ye, Jing Zhang, Juhua Liu, and Dacheng Tao. I3cl: intra-and inter-instance collaborative learning
for arbitrary-shaped scene text detection. International Journal of Computer Vision , 130(8):1961–1977, 2022.
[131] Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Bo Du, and Dacheng Tao. Dptext-detr: Towards better
scene text detection with dynamic points in transformer. arXiv preprint arXiv:2207.04491 , 2022.
[132] Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, and Dacheng Tao. Deepsolo: Let
transformer decoder with explicit points solo for text spotting. arXiv preprint arXiv:2211.10772 , 2022.
[133] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classifica-
tion. arXiv preprint , 2016.
[134] Walaa Medhat, Ahmed Hassan, and Hoda Korashy. Sentiment analysis algorithms and applications: A survey.
Ain Shams engineering journal , 2014.
[135] Bing Wang, Liang Ding, Qihuang Zhong, Ximing Li, and Dacheng Tao. A contrastive cross-channel data
augmentation framework for aspect-based sentiment analysis. In COLING , 2022.
[136] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Hua Jin, and Dacheng Tao. Knowledge graph augmented
network towards multiview representation learning for aspect-based sentiment analysis. IEEE Transactions on
Knowledge and Data Engineering , 2023.
[137] Lynette Hirschman and Robert Gaizauskas. Natural language question answering: the view from here. Natural
Language Engineering , 2001.
[138] Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, and Ruifeng Xu. Interpretable proof generation via iterative backward
reasoning. In NAACL , 2022.
[139] Behrang Mohit. Named entity recognition. Natural Language Processing of Semitic Languages , 2014.
[140] Di Wu, Liang Ding, Fan Lu, and Jian Xie. Slotrefine: A fast non-autoregressive model for joint intent detection
and slot filling. In EMNLP , 2020.
[141] Philipp Koehn. Statistical machine translation . Cambridge University Press, 2009.
[142] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NeurIPS ,
2014.
[143] Liang Ding, Longyue Wang, and Dacheng Tao. Self-attention with cross-lingual position representation. In ACL,
2020.
[144] Liang Ding, Longyue Wang, Di Wu, Dacheng Tao, and Zhaopeng Tu. Context-aware cross-attention for
non-autoregressive translation. In COLING , 2020.
[145] Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and Zhaopeng Tu. Understanding and
improving lexical choice in non-autoregressive translation. In ICLR , 2021.
[146] Inderjeet Mani. Automatic summarization . John Benjamins Publishing, 2001.
[147] Zheng Zhang, Liang Ding, Dazhao Cheng, Xuebo Liu, Min Zhang, and Dacheng Tao. Bliss: Robust sequence-
to-sequence learning via self-supervised input representation. arXiv preprint , 2022.
[148] David Bohm, Peter M Senge, and Lee Nichol. On dialogue . Routledge, 2004.
[149] Yu Cao, Liang Ding, Zhiliang Tian, and Meng Fang. Towards efficiently diversifying dialogue generation via
embedding augmentation. In ICASSP , 2021.
[150] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. E2s2: Encoding-enhanced sequence-to-
sequence pretraining for language understanding and generation. arXiv preprint , 2022.
[151] Qihuang Zhong, Liang Ding, Keqin Peng, Juhua Liu, Bo Du, Li Shen, Yibing Zhan, and Dacheng Tao. Bag of
tricks for effective language model pretraining and downstream adaptation: A case study on glue. arXiv preprint ,
2023.
[152] Qihuang Zhong, Liang Ding, Yibing Zhan, Y . Qiao, Yonggang Wen, Li Shen, Juhua Liu, Baosheng Yu, Bo Du,
Yixin Chen, Xinbo Gao, Chun Miao, Xiaoou Tang, and Dacheng Tao. Toward efficient language model
pretraining and downstream adaptation via self-evolution: A case study on superglue. arXiv preprint , 2022.
[153] Liang Ding, Di Wu, and Dacheng Tao. Improving neural machine translation by bidirectional training. In
EMNLP , 2021.
41

--- PAGE 42 ---
[154] Changtong Zan, Keqin Peng, Liang Ding Baopu Qiu, Boan Liu, Shwai He, Qingyu Lu, Zheng Zhang, Chuang
Liu, Weifeng Liu, Yibing Zhan, and Dacheng Tao. Vega-mt: The jd explore academy translation system for
wmt22. In WMT , 2022.
[155] Changtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng Liu, and Dacheng Tao. On the complementarity between
pre-training and random-initialization for resource-rich machine translation. In COLING , 2022.
[156] Liang Ding and Dacheng Tao. The university of sydney’s machine translation system for wmt19. In WMT , 2019.
[157] Liang Ding, Di Wu, and Dacheng Tao. The usyd-jd speech translation system for iwslt2021. In IWSLT , 2021.
[158] Shwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao. Sparseadapter: An easy approach for
improving the parameter-efficiency of adapters. In EMNLP , 2022.
[159] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. Panda: Prompt transfer meets knowledge
distillation for efficient model adaptation. arXiv preprint , 2022.
[160] Jun Rao, Xv Meng, Liang Ding, Shuhan Qi, and Dacheng Tao. Parameter-efficient and student-friendly
knowledge distillation. arXiv preprint , 2022.
[161] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . MIT Press, Cambridge, MA, USA,
2016. http://www.deeplearningbook.org .
[162] Chaoyue Wang, Chang Xu, Chaohui Wang, and Dacheng Tao. Perceptual adversarial networks for image-to-
image transformation. IEEE Transactions on Image Processing , 27(8):4066–4079, 2018.
[163] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):139–144, 2020.
[164] Chaoyue Wang, Chang Xu, Xin Yao, and Dacheng Tao. Evolutionary generative adversarial networks. IEEE
Transactions on Evolutionary Computation , 23(6):921–934, 2019.
[165] Ziqiang Li, Muhammad Usman, Rentuo Tao, Pengfei Xia, Chaoyue Wang, Huanhuan Chen, and Bin Li. A
systematic survey of regularization and normalization in gans. ACM Computing Surveys , 2022.
[166] Yonghao Xu, Fengxiang He, Bo Du, Dacheng Tao, and Liangpei Zhang. Self-ensembling gan for cross-domain
semantic segmentation. IEEE Transactions on Multimedia , 2022.
[167] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information
processing systems , 30, 2017.
[168] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.
Advances in neural information processing systems , 32, 2019.
[169] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.
InProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12873–12883,
2021.
[170] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander
Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image
generation. arXiv preprint arXiv:2206.10789 , 2022.
[171] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
[172] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10684–10695, 2022.
[173] Minghui Hu, Chuanxia Zheng, Zuopeng Yang, Tat-Jen Cham, Heliang Zheng, Chaoyue Wang, Dacheng Tao,
and Ponnuthurai N. Suganthan. Unified discrete diffusion for simultaneous vision-language generation. In The
Eleventh International Conference on Learning Representations , 2023.
[174] Zuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng Tao. Modeling image composition for
complex scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7764–7773, 2022.
[175] Zhuo Chen, Chaoyue Wang, Haimei Zhao, Bo Yuan, and Xiu Li. D2animator: Dual distillation of stylegan for
high-resolution face animation. In Proceedings of the 30th ACM International Conference on Multimedia , pages
1769–1778, 2022.
[176] Chaoyue Wang, Chang Xu, and Dacheng Tao. Self-supervised pose adaptation for cross-domain image animation.
IEEE Transactions on Artificial Intelligence , 1(1):34–46, 2020.
42

--- PAGE 43 ---
[177] Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, and Dacheng Tao. 3ddesigner:
Towards photorealistic 3d object generation and editing with text-guided diffusion models. arXiv preprint
arXiv:2211.14108 , 2022.
[178] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
International Conference on Learning Representations (ICLR) , 2017.
[179] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph
Attention Networks. International Conference on Learning Representations , 2018. accepted as poster.
[180] Qitian Wu, Wentao Zhao, Zenan Li, David Wipf, and Junchi Yan. Nodeformer: A scalable graph structure
learning transformer for node classification. In Advances in Neural Information Processing Systems (NeurIPS) ,
2022.
[181] W. Lu, Y . Zhan, Z. Guan, L. Liu, B. Yu, W. Zhao, Y . Yang, and D. Tao. Skipnode: On alleviating over-smoothing
for deep graph convolutional networks. 2021.
[182] Pinghua Xu, Yibing Zhan, Liu Liu, Baosheng Yu, Bo Du, Jia Wu, and Wenbin Hu. Dual-branch density ratio
estimation for signed network embedding. In Proceedings of the ACM Web Conference 2022 , pages 1651–1662,
2022.
[183] S. Wan, S. Pan, S. Zhong, J. Yang, Y . Zhan, and C. Gong. Multi-level graph learning network for hyperspectral
image classification. Pattern Recognition: The Journal of the Pattern Recognition Society , page 129, 2022.
[184] Xin Lin, Changxing Ding, Yibing Zhan, Zijian Li, and Dacheng Tao. Hl-net: Heterophily learning network
for scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19476–19485, 2022.
[185] Zhi-Hua Zhou. Open-environment machine learning. National Science Review , 2022.
[186] David R. So, Quoc V . Le, and Chen Liang. The evolved transformer. In ICML , 2019.
[187] Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel
bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research , 2017.
[188] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In UAI, 2019.
[189] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In ICLR , 2019.
[190] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four GPU hours. In CVPR , 2019.
[191] Xuanyi Dong and Yi Yang. One-shot neural architecture search via self-evaluated template network. In ICCV ,
2019.
[192] Chao Xue, Xiaoxing Wang, Junchi Yan, Yonggang Hu, Xiaokang Yang, and Kewei Sun. Rethinking bi-level
optimization in neural architecture search: A gibbs sampling perspective. In AAAI , 2021.
[193] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V . Le. Regularized evolution for image classifier
architecture search. In AAAI , 2019.
[194] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine Learning , 1992.
[195] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimization at scale.
InICML , 2018.
[196] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization.
InNeurIPS , 2011.
[197] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by exploring
intrinsic inductive bias. Advances in Neural Information Processing Systems , 34:28522–28535, 2021.
[198] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In ICCV , 2021.
[199] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,
et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR , 2022.
[200] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all
data sizes. In NeurIPS , 2021.
[201] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing
convolutions to vision transformers. In ICCV , 2021.
[202] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature
prediction for self-supervised visual pre-training. In CVPR , 2022.
43

--- PAGE 44 ---
[203] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A
simple framework for masked image modeling. In CVPR , 2022.
[204] Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and Zhaopeng Tu. Rejuvenating
low-frequency words: Making the most of parallel data in non-autoregressive translation. In ACL, 2021.
[205] Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, and Zhaopeng Tu. Progressive multi-
granularity training for non-autoregressive translation. In Findings of ACL , 2021.
[206] Liang Ding, Longyue Wang, Shuming Shi, Dacheng Tao, and Zhaopeng Tu. Redistributing low-frequency words:
Making the most of monolingual data in non-autoregressive translation. In ACL, 2022.
[207] Zuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng Tao. Modeling image composition for
complex scene generation. In CVPR , 2022.
[208] Manuel Jahn, Robin Rombach, and Björn Ommer. High-resolution complex scene synthesis with transformers.
InCVPRW , 2021.
[209] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In CVPR ,
2018.
[210] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yan-
nis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using
crowdsourced dense image annotations. IJCV , 2017.
[211] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. TPAMI ,
2021.
[212] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, and Shikhar Sharma. Object-centric image
generation from layouts. In AAAI , 2021.
[213] Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. Image synthesis from layout with
locality-aware mask adaption. In ICCV , 2021.
[214] Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido:
Feature pyramid diffusion for complex scene image synthesis. arXiv preprint arXiv:2208.13753 , 2022.
[215] Dakuo Wang, Josh Andres, Justin D Weisz, Erick Oduor, and Casey Dugan. Autods: Towards human-centered
automation of data science. In Proceedings of the 2021 CHI Conference on Human Factors in Computing
Systems , pages 1–12, 2021.
[216] Wenzhuo Yang, Hung Le, Silvio Savarese, and Steven CH Hoi. Omnixai: A library for explainable ai. arXiv
preprint arXiv:2206.01612 , 2022.
[217] Marc-André Zöller, Waldemar Titov, Thomas Schlegel, and Marco F Huber. Xautoml: A visual analytics tool for
establishing trust in automated machine learning. arXiv preprint arXiv:2202.11954 , 2022.
[218] Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, and Alexandru Coca. Alibi explain: Algorithms for
explaining machine learning models. Journal of Machine Learning Research , 22(181):1–7, 2021.
[219] Amazon sagemaker canvas: Generate accurate ml predictions - no code required. https://aws.amazon.com/
cn/sagemaker/canvas/ , 2022.
[220] Amazon forecast: Forecast business outcomes easily and accurately using machine learning. https://aws.
amazon.com/cn/forecast/ , 2022.
[221] Watson natural language understanding: The natural language processing (nlp) service for advanced text analytics.
https://www.ibm.com/cloud/watson-natural-language-understanding , 2022.
[222] Abacus ai: Effortlessly create cutting-edge ai systems at scale. https://abacus.ai/ , 2022.
[223] Automated machine learning: Automatically build machine learning models with speed and scale. https:
//azure.microsoft.com/en-us/services/machine-learning/automatedml/ , 2022.
[224] Vertex ai: Build, deploy, and scale ml models faster, with pre-trained and custom tooling within a unified artificial
intelligence platform. https://cloud.google.com/vertex-ai , 2022.
44
