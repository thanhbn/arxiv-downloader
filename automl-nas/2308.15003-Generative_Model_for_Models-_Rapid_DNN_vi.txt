# Mô hình Sinh cho Mô hình: Tùy chỉnh DNN Nhanh chóng cho Các Tác vụ Đa dạng và Ràng buộc Tài nguyên

Wenxing Xu *
Đại học Bưu chính và Viễn thông Bắc Kinh

Yuanchun Li†
Viện Nghiên cứu Công nghiệp AI (AIR), Đại học Thanh Hoa

Jiacheng Liu *
Viện Công nghệ Bắc Kinh

Yi Sun
Viện Nghiên cứu Công nghiệp AI (AIR), Đại học Thanh Hoa

Zhengyang Cao *
Đại học Khoa học và Công nghệ Điện tử Trung Quốc

Yixuan Li *
Đại học Bưu chính và Viễn thông Bắc Kinh

Hao Wen
Viện Nghiên cứu Công nghiệp AI (AIR), Đại học Thanh Hoa

Yunxin Liu
Viện Nghiên cứu Công nghiệp AI (AIR), Đại học Thanh Hoa

## TÓM TẮT

Không như các mô hình học sâu dựa trên đám mây thường lớn và đồng nhất, các mô hình triển khai trên biên thường đòi hỏi tùy chỉnh cho các tác vụ cụ thể theo lĩnh vực và môi trường hạn chế tài nguyên. Các quy trình tùy chỉnh như vậy có thể tốn kém và mất thời gian do sự đa dạng của các kịch bản biên và tải huấn luyện cho mỗi kịch bản. Mặc dù đã có nhiều phương pháp được đề xuất cho tùy chỉnh nhanh hướng tài nguyên và tùy chỉnh hướng tác vụ tương ứng, việc đạt được cả hai cùng lúc là thách thức. Lấy cảm hứng từ AI sinh tạo và khả năng kết hợp mô-đun của mạng neural, chúng tôi giới thiệu NN-Factory, một khung một-cho-tất-cả để tạo ra các mô hình nhẹ tùy chỉnh cho các kịch bản biên đa dạng. Ý tưởng chính là sử dụng một mô hình sinh tạo để trực tiếp tạo ra các mô hình tùy chỉnh, thay vì huấn luyện chúng. Các thành phần chính của NN-Factory bao gồm một supernet mô-đun với các mô-đun được huấn luyện trước có thể được kích hoạt có điều kiện để hoàn thành các tác vụ khác nhau và một trình lắp ráp mô-đun sinh tạo để thao tác các mô-đun theo yêu cầu tác vụ và độ thưa. Cho một kịch bản biên, NN-Factory có thể hiệu quả tùy chỉnh một mô hình nhỏ gọn chuyên biệt trong tác vụ biên trong khi thỏa mãn các ràng buộc tài nguyên biên bằng cách tìm kiếm chiến lược tối ưu để lắp ráp các mô-đun. Dựa trên thí nghiệm về các tác vụ phân loại hình ảnh và phát hiện đối tượng với các thiết bị biên khác nhau, NN-Factory có thể tạo ra các mô hình chất lượng cao cụ thể cho tác vụ và tài nguyên trong vài giây, nhanh hơn các phương pháp tùy chỉnh mô hình thông thường bằng các bậc độ lớn.

**TỪ KHÓA**
Học sâu, kịch bản biên, ràng buộc tài nguyên, tùy chỉnh mô hình, mô hình sinh tạo

## 1 GIỚI THIỆU

Học sâu (DL) đã trở thành một công nghệ trí tuệ nhân tạo (AI) thay đổi cuộc chơi trong những năm gần đây. Nó đã đạt được hiệu suất đáng chú ý trong nhiều lĩnh vực khác nhau bao gồm thị giác máy tính, hiểu ngôn ngữ tự nhiên, trò chơi máy tính và sinh học tính toán. Đồng thời, học sâu đã cho phép và nâng cao nhiều ứng dụng thông minh tại biên, như hỗ trợ lái xe [21,60], xác thực khuôn mặt [6], giám sát video [25,59], nhận dạng giọng nói [42,51], v.v. Do các cân nhắc về độ trễ và quyền riêng tư, việc triển khai các mô hình lên thiết bị biên [15,30,36] đang trở thành một thực tiễn ngày càng phổ biến, để các mô hình có thể được gọi trực tiếp mà không cần truyền dữ liệu đến máy chủ.

Để áp dụng các mô hình DL trong các kịch bản biên khác nhau, các nhà phát triển thường cần tùy chỉnh các mô hình, bao gồm hai quy trình chính sau đây.

(1) **Tùy chỉnh Hướng Tác vụ**: Tùy chỉnh các mô hình cho các tác vụ cụ thể theo lĩnh vực, như phát hiện các loại xe nhất định, theo dõi người với trang phục nhất định, hoặc nhận dạng các mặt hàng với những lỗi nhất định.

(2) **Tùy chỉnh Hướng Tài nguyên**: Tùy chỉnh các mô hình cho thiết bị biên, để đáp ứng các ràng buộc tài nguyên nhất định trên thiết bị biên đích, bao gồm ngân sách bộ nhớ và yêu cầu độ trễ.

Thực tiễn hiện tại là xử lý từng vấn đề tùy chỉnh một cách riêng biệt. Ví dụ, các nhà phát triển cần tạo ra các mô hình nhỏ gọn thông qua thiết kế/nén kiến trúc mô hình rộng rãi để đáp ứng các ràng buộc tài nguyên, sau đó huấn luyện/tinh chỉnh mô hình trên các tác vụ cụ thể theo lĩnh vực để cải thiện độ chính xác. Quy trình như vậy có thể liên quan đến việc thu thập dữ liệu tốn kém, tìm kiếm kiến trúc mất thời gian và huấn luyện mô hình không ổn định.

Những tiến bộ gần đây của học sâu đã chứng minh khả năng thích ứng zero-shot xuất sắc của các mô hình lớn được huấn luyện trước, bao gồm thích ứng kiến trúc neural không cần huấn luyện và thích ứng tác vụ. Cụ thể, các kỹ thuật tìm kiếm kiến trúc neural một lần (NAS) [3,5] và mở rộng mô hình [13,17,24,53] đã chứng minh khả năng huấn luyện một supernet và tùy chỉnh nó cho các môi trường biên đa dạng. Các mô hình lớn được huấn luyện trước như ChatGPT [40], OFA [4], và Segment Anything [7] cho phép người dùng tùy chỉnh tác vụ của mô hình bằng cách đơn giản cung cấp một prompt tác vụ làm đầu vào của mô hình. Những khả năng thích ứng không cần huấn luyện như vậy là mong muốn trong các kịch bản AI biên do khả năng hỗ trợ một loạt rộng các tác vụ downstream và các ràng buộc phần cứng đa dạng với nỗ lực phát triển giảm đáng kể.

Tuy nhiên, việc kết hợp tùy chỉnh kiến trúc neural không cần huấn luyện và tùy chỉnh tác vụ không cần huấn luyện cùng nhau là khó khăn. Một mặt, việc sử dụng NAS một lần hoặc mở rộng mô hình để tạo ra các mô hình cho các tác vụ khác nhau là không khả thi vì supernet được thiết kế cho một tác vụ duy nhất. Mở rộng chúng để hỗ trợ nhiều tác vụ sẽ mở rộng đáng kể không gian tìm kiếm mô hình, làm cho việc huấn luyện supernet và tìm kiếm subnet trở nên cực kỳ thách thức. Mặt khác, khả năng thích ứng tác vụ đáng chú ý của các mô hình được huấn luyện trước có cái giá là tải tính toán khổng lồ. Các nghiên cứu gần đây đã chỉ ra rằng khả năng tổng quát hóa zero-shot của các mô hình được huấn luyện trước xuất hiện và tăng cường với việc tăng kích thước mô hình [28]. Do đó, việc sử dụng trực tiếp mô hình lớn có khả năng tổng quát hóa tác vụ là khó khăn hoặc thậm chí không thể do các tài nguyên tính toán biên đa dạng và hạn chế.

Để đạt được cả tùy chỉnh mô hình hiệu quả hướng tác vụ và hướng tài nguyên, chúng tôi giới thiệu NN-Factory, một mô hình mới để tạo ra các mô hình tùy chỉnh cho các kịch bản biên đa dạng. Ý tưởng chính của chúng tôi là xem vấn đề tùy chỉnh mô hình cụ thể cho biên như một vấn đề sinh tạo - thay vì các mô hình sinh tạo hiện có được thiết kế để tạo ra nội dung truyền thông [8,9,40], chúng tôi sử dụng AI sinh tạo để tạo ra các mô hình tùy chỉnh dựa trên các đặc tả kịch bản biên. Cụ thể, cho tác vụ mong muốn, loại thiết bị và yêu cầu tài nguyên trong một kịch bản biên, NN-Factory nhanh chóng tạo ra một mô hình nhỏ gọn khớp với tác vụ và yêu cầu bằng cách đơn giản truy vấn mô hình sinh tạo. Mô hình tùy chỉnh mô hình mới như vậy có tiềm năng tránh hoàn toàn các quy trình nén và huấn luyện phức tạp trong các phương pháp tùy chỉnh mô hình biên thông thường.

Tuy nhiên, việc tạo ra trực tiếp các tham số của một mô hình là khó khăn do số lượng tham số mô hình khổng lồ. Hiểu biết chính của NN-Factory để giải quyết vấn đề này là khả năng kết hợp của các mô-đun mạng neural [1,41], tức là các chức năng và kích thước khác nhau của mạng neural có thể đạt được bằng các kết hợp nhất định của các mô-đun mạng neural, như được minh họa trong Hình 1. Khả năng kết hợp mô-đun như vậy đã được nghiên cứu bởi các phương pháp hiện có (ví dụ NestDNN [13], LegoDNN [17], AdaptiveNet [53], v.v.) cho việc mở rộng mô hình phía biên, nhưng không có phương pháp nào trong số chúng có thể hỗ trợ tùy chỉnh hướng tác vụ.

Dựa trên hiểu biết này, NN-Factory tạo ra một mô hình tùy chỉnh cho mỗi kịch bản biên bằng cách trực tiếp tạo ra các cấu hình để lắp ráp các mô-đun neural được huấn luyện trước. Các thành phần chính của NN-Factory bao gồm một supernet mô-đun chứa các mô-đun neural, một trình lắp ráp mô-đun nhận biết tác vụ và độ thưa tạo ra các cấu hình lắp ráp mô-đun ứng viên (mỗi cấu hình là một tập hợp các vector cổng có thể ánh xạ đến một mô hình ứng viên), và một bộ tìm kiếm kiến trúc nhẹ nhanh chóng tìm ra cấu hình tối ưu để lắp ráp mô hình cho kịch bản biên đích.

Cụ thể, supernet mô-đun được mở rộng từ một mạng backbone phổ biến, như Mạng Neural Tích chập (CNN) hoặc Transformer, trong đó mỗi khối cơ bản được phân tách thành nhiều mô-đun được kích hoạt có điều kiện. Cách các mô-đun được kích hoạt được điều khiển bởi một số vector cổng, được tạo ra bởi một mô hình sinh tạo (tức là trình lắp ráp mô-đun). Bằng cách kích hoạt các tập hợp mô-đun khác nhau, các khối cơ bản trong mạng backbone có thể được cấu hình lại để có các chức năng và kích thước khác nhau. Trình lắp ráp mô-đun quyết định cách kích hoạt các mô-đun trong mạng backbone dựa trên mô tả tác vụ và yêu cầu độ thưa (tức là giới hạn tỷ lệ kích hoạt mô-đun). Các mô-đun và trình lắp ráp được huấn luyện chung để làm việc cùng nhau một cách mạch lạc.

Dựa trên thiết kế mô-đun-trình lắp ráp, chúng tôi có thể giảm không gian tìm kiếm lớn của các kết hợp mô-đun xuống không gian tìm kiếm nhỏ của các prompt sinh tạo. Để tìm mô hình tùy chỉnh cho một kịch bản biên (được định nghĩa bởi tác vụ, thiết bị và yêu cầu độ trễ/bộ nhớ), chúng tôi chỉ cần tìm yêu cầu sinh tạo mô hình tối ưu, điều này được thực hiện bởi mô-đun tìm kiếm kiến trúc nhẹ dưới sự hướng dẫn của một bộ đánh giá hiệu suất mô hình cụ thể cho thiết bị. Do không gian tìm kiếm được giảm đáng kể, việc tìm mô hình tối ưu chỉ mất vài vòng lặp ngắn.

Để đánh giá NN-Factory, chúng tôi tiến hành thí nghiệm với ba thiết bị trên hai loại tác vụ. Kết quả đã chứng minh rằng phương pháp của chúng tôi có thể tạo ra các mô hình được điều chỉnh cho một kịch bản biên cho trước trong tối đa 6 giây, nhanh hơn 1000× so với phương pháp tùy chỉnh dựa trên huấn luyện thông thường. Các mô hình được tạo ra có thể đạt được độ chính xác cao có thể so sánh với các mô hình được huấn luyện lại/tinh chỉnh trên biên trên cả các tác vụ đã biết và chưa biết.

Công trình của chúng tôi đóng góp các kỹ thuật sau:

(1) Chúng tôi đề xuất một giải pháp tùy chỉnh mô hình cụ thể cho biên dựa trên AI sinh tạo mới. Nó cho phép tùy chỉnh mô hình nhanh chóng không cần huấn luyện cho các kịch bản biên với các tác vụ và ràng buộc tài nguyên đa dạng.

(2) Chúng tôi giới thiệu một thiết kế mô-đun cho sinh tạo mô hình, cho phép tùy chỉnh mô hình nhanh chóng bằng cách đơn giản truy vấn một mô hình trình lắp ráp cho các kích hoạt mô-đun.

(3) Dựa trên thí nghiệm với hai loại tác vụ và các thiết bị biên khác nhau, phương pháp của chúng tôi có thể tạo ra các mô hình chất lượng cao cho các kịch bản biên đa dạng với chi phí thấp hơn đáng kể. Hệ thống và các mô hình sẽ được mã nguồn mở.

## 2 KIẾN THỨC NỀN TẢNG VÀ CÔNG TRÌNH LIÊN QUAN

### 2.1 Tùy chỉnh Mô hình cho Kịch bản Biên

Triển khai mạng neural sâu (DNN) tại biên ngày càng phổ biến do yêu cầu độ trễ và mối quan tâm về quyền riêng tư của các dịch vụ học sâu. Tuy nhiên, việc trực tiếp tuân theo các thủ tục huấn luyện và triển khai mô hình dựa trên đám mây phổ biến không thỏa đáng do sự đa dạng khổng lồ của các kịch bản biên [13,17,53]. Không như hầu hết các mô hình AI dựa trên đám mây được thiết kế cho các tác vụ chung và được lưu trữ trong các cụm GPU mạnh mẽ, các kịch bản AI biên thường đa dạng và phân mảnh. Mỗi kịch bản biên có thể xử lý một tác vụ cụ thể theo lĩnh vực và một tập hợp duy nhất các môi trường triển khai đích.

Theo các đối tác trong ngành cung cấp dịch vụ AI biên cho các nhà tùy chỉnh, nỗ lực phát triển chính của họ được dành cho việc tùy chỉnh mô hình để xử lý sự đa dạng của các kịch bản biên, bao gồm đa dạng phần cứng, đa dạng tác vụ, đa dạng phân phối dữ liệu, v.v. Trong số đó, một vấn đề chính là đa dạng phần cứng. Các nhà tùy chỉnh thường yêu cầu triển khai mô hình lên các nền tảng phần cứng khác nhau, như máy chủ biên, máy tính để bàn, điện thoại thông minh và hộp AI biên. Do khả năng tính toán khác nhau, cùng một mô hình có thể cho hiệu suất khác nhau đáng kể trên các thiết bị và môi trường biên khác nhau, như được thể hiện trong Hình 2. Do đó, các nhà phát triển cần tùy chỉnh kiến trúc mô hình để đáp ứng các ràng buộc độ trễ. Đa dạng tác vụ là một vấn đề quan trọng khác mà các nhà cung cấp dịch vụ AI biên phải đối mặt. Mỗi nhà tùy chỉnh có thể có một tác vụ AI cụ thể theo lĩnh vực dựa trên kịch bản ứng dụng, như được minh họa trong Bảng 1. Để cho phép các dịch vụ AI trong những kịch bản như vậy, việc tùy chỉnh mô hình cho các tác vụ khác nhau là cần thiết.

Có nhiều phương pháp hiện có được đề xuất cho mỗi loại mục tiêu tùy chỉnh trên. Tuy nhiên, khi xem xét hai mục tiêu một cách kết hợp cho nhiều kịch bản biên đa dạng, chi phí trở nên lớn, vì quy trình tùy chỉnh là không đơn giản (như sẽ được giải thích sau) và phải được lặp lại cho mỗi kịch bản.

### 2.2 Tùy chỉnh Mô hình Dựa trên Huấn luyện

Để tùy chỉnh một mô hình cho một tác vụ cụ thể theo lĩnh vực nhất định, thực tiễn phổ biến là sử dụng học chuyển giao (TL). Phương pháp chủ đạo trong học chuyển giao là tinh chỉnh, tức là khởi tạo các tham số mô hình với một mô hình được huấn luyện trước và tiếp tục huấn luyện các tham số với dữ liệu cụ thể theo lĩnh vực. Các quy trình học chuyển giao như vậy yêu cầu các nhà phát triển thu thập các mẫu dữ liệu cho tác vụ, gắn nhãn chúng và huấn luyện mô hình với các mẫu được gắn nhãn, thường tốn nhiều lao động, đòi hỏi tài nguyên và mất thời gian.

Để khớp một mô hình vào một thiết bị biên cụ thể, các giải pháp điển hình bao gồm nén một mô hình hiện có [18,50,65] (ví dụ: pruning, quantization, v.v.) hoặc tìm một kiến trúc mô hình mới phù hợp với khả năng của thiết bị đích [44,46]. Vì việc thiết kế thủ công các mô hình cho các môi trường biên đa dạng là cồng kềnh, thực tiễn phổ biến là sử dụng các kỹ thuật sinh tạo mô hình tự động. NAS [16,34,45,54,56] là phương pháp sinh tạo mô hình đại diện và được sử dụng rộng rãi nhất, tìm kiếm kiến trúc mạng tối ưu trong một không gian tìm kiếm được thiết kế tốt. Hầu hết các phương pháp NAS yêu cầu huấn luyện các kiến trúc trong quá trình tìm kiếm [38,43,45,66], điều này cực kỳ mất thời gian (10.000+ giờ GPU) khi tạo ra mô hình cho một số lượng lớn thiết bị.

Cả hai quy trình trên đều đặt ra thách thức cho các nhà phát triển do sự đa dạng của các kịch bản biên. Huấn luyện hoặc tinh chỉnh mô hình cho một kịch bản biên thường yêu cầu hàng nghìn mẫu huấn luyện được gắn nhãn và vài giờ trên các máy GPU hiệu suất cao. Tìm kiến trúc mô hình tối ưu hoặc chiến lược nén bằng thử và sai cũng yêu cầu nhiều nỗ lực phát triển.

### 2.3 Tùy chỉnh Mô hình Không cần Huấn luyện

Những tiến bộ gần đây của học sâu đã cho thấy khả năng sử dụng một mô hình được huấn luyện trước thống nhất để hỗ trợ nhiều tác vụ downstream khác nhau mà không cần huấn luyện thêm. Cụ thể, người ta có thể huấn luyện một mô hình nền tảng trên một tập dữ liệu lớn với nhiều tác vụ được định nghĩa trước khác nhau và trực tiếp sử dụng mô hình cho các tác vụ downstream khác nhau với các prompt đơn giản. Theo các phân tích thực nghiệm [28], việc cải thiện khả năng của mô hình được huấn luyện trước có thể dẫn đến hiệu suất tốt hơn trên các tác vụ downstream.

Đồng thời, cả cộng đồng AI và cộng đồng tính toán di động đã thử nghiệm nhiều cách khác nhau để giảm chi phí tạo ra các mô hình nhẹ cho thiết bị biên. NAS một lần [3,5,22,37] được đề xuất để giảm đáng kể chi phí huấn luyện bằng cách cho phép các mạng ứng viên chia sẻ một supernet quá tham số hóa chung. Các mô hình tốt nhất cho thiết bị đích có thể được tìm thấy bằng cách trực tiếp tìm kiếm một subnet trong supernet. Các nhà nghiên cứu tính toán di động cũng đã đề xuất mở rộng mô hình động để cung cấp một loạt rộng các đánh đổi tài nguyên-độ chính xác. Hầu hết trong số họ áp dụng pruning có cấu trúc hoặc thích ứng kiến trúc mô hình để tạo ra các mô hình con cháu [13,17,39,57,58] với các mức chi phí tính toán khác nhau.

Thích ứng mô hình không cần huấn luyện cũng có thể đạt được bằng cách lắp ráp động các mô-đun neural, đã được thảo luận trong mạng neural động [19] và lập trình neurosymbolic [41]. Mạng neural động là một loại DNN hỗ trợ suy luận linh hoạt dựa trên độ khó của đầu vào. Khi đầu vào dễ, mạng neural động có thể giảm tính toán bằng cách bỏ qua một tập hợp các khối [52,55] hoặc thoát từ các lớp giữa [2,12,31,32]. Mặc dù các subnet khác nhau (một subnet là một đường dẫn trong NN động) có tải tính toán khác nhau, chúng không thể được tùy chỉnh cho các tác vụ khác nhau, và tính động của mô hình phụ thuộc đầu vào cũng không phù hợp cho môi trường biên. Lập trình neurosymbolic chứng minh khả năng giải quyết các tác vụ khác nhau với cùng một tập hợp các mô-đun neural bằng cách kết hợp chúng về mặt ngữ nghĩa. Cụ thể, mô hình để giải quyết một tác vụ phức tạp có thể được viết như một chương trình gọi các mô-đun chức năng nhỏ hơn. Tuy nhiên, các phương pháp lập trình neurosymbolic hiện có thiếu tính linh hoạt và khả năng tùy chỉnh hướng tài nguyên, vì các mô-đun thường tĩnh và được định nghĩa rõ ràng.

Tuy nhiên, vẫn còn thách thức để tối ưu hóa quy trình tùy chỉnh mô hình cho các tác vụ và thiết bị cùng lúc. Tùy chỉnh hướng tác vụ và tùy chỉnh hướng tài nguyên có phần mâu thuẫn nhau. Để đạt được tùy chỉnh tác vụ nhanh, mô hình được mong muốn có khả năng tổng quát hóa cross-task, thường yêu cầu một mô hình được huấn luyện trước thống nhất với khả năng lớn, trong khi triển khai mô hình lên các thiết bị biên hạn chế tài nguyên đa dạng yêu cầu các mô hình nhẹ không đồng nhất.

## 3 THIẾT KẾ NN-FACTORY

**Định nghĩa vấn đề.** Chính thức, mục tiêu của tùy chỉnh DNN biên là tạo ra một mô hình $f_{\hat{\alpha},\hat{\theta}}$ với kiến trúc $\hat{\alpha}$ và tham số $\hat{\theta}$ có thể xử lý chính xác tác vụ biên trong khi thỏa mãn các ràng buộc hiệu suất. tức là

$$\hat{\alpha},\hat{\theta} = \arg \min_{\alpha,\theta} L(f_{\alpha,\theta}(X_e), Y_e)$$
$$s.t. \quad mem(\alpha) < MEM_e \quad and \quad lat(\alpha) < LAT_e \quad (1)$$

trong đó $X_e$ và $Y_e$ là đầu vào và đầu ra của các mẫu dữ liệu tác vụ biên, $L$ là mất mát dự đoán, và $MEM_e$ và $LAT_e$ là giới hạn bộ nhớ và độ trễ tại môi trường biên.

Cụ thể, chúng tôi tập trung vào việc tạo ra các mô hình cho các tác vụ trong một không gian kết hợp, tức là mỗi tác vụ được mô tả như một kết hợp của một số thuộc tính (ví dụ: màu sắc, trạng thái, thực thể, v.v.) và các thuộc tính được chia sẻ giữa các tác vụ khác nhau (ví dụ: xe tải đỏ, mèo trắng, v.v.). Các nhà phát triển có thể linh hoạt định nghĩa không gian tác vụ theo các kịch bản biên đích của họ.

Công trình của chúng tôi lấy cảm hứng từ tính mô-đun và khả năng kết hợp động của mạng neural và AI sinh tạo. Tầm nhìn của chúng tôi là tạo ra một hệ thống sinh tạo một-cho-tất-cả trong đó các mô hình DNN cụ thể cho biên khác nhau có thể được tạo ra trực tiếp bằng cách cấu hình và lắp ráp các mô-đun neural được định nghĩa trước, như được minh họa trong Hình 1.

Việc thực hiện tầm nhìn này là thách thức vì (1) khó thiết kế và phát triển các mô-đun có thể được lắp ráp để thực hiện các tác vụ khác nhau và đáp ứng các ràng buộc tài nguyên khác nhau (2) ngay cả khi các mô-đun như vậy được tạo ra, việc tạo ra một mô hình phù hợp cho một kịch bản biên cụ thể vẫn có thể khó khăn do không gian kết hợp khổng lồ của các ứng viên mô hình.

Chúng tôi cố gắng giải quyết những thách thức này với một phương pháp đầu-cuối có tên NN-Factory. Chúng tôi nhấn mạnh rằng NN-Factory là một mô hình sinh tạo tùy chỉnh mô hình đầu tiên cho các kịch bản biên đa dạng, cho phép sinh tạo mô hình nhanh chóng không cần huấn luyện cho các tác vụ và ràng buộc tài nguyên khác nhau.

### 3.1 Tổng quan

Ý tưởng chính của NN-Factory là huấn luyện chung một tập hợp các mô-đun neural và một bộ sinh tạo chiến lược lắp ráp mô-đun (tức là trình lắp ráp) theo cách dựa trên tác vụ và độ thưa. Cụ thể, các mô-đun được cắt từ một mạng neural được huấn luyện trước (supernet), điều này tránh việc phát triển cồng kềnh các mô-đun riêng lẻ với chi phí giảm khả năng diễn giải. Đồng thời, bằng cách học trình lắp ráp trong quá trình huấn luyện với nhiều tác vụ và yêu cầu độ thưa khác nhau, NN-Factory có thể trực tiếp tạo ra kiến trúc mô hình cho một tác vụ và yêu cầu độ thưa mới với một lần chuyển tiếp duy nhất, giảm đáng kể không gian tìm kiếm mô hình.

Hình 3 thể hiện tổng quan về NN-Factory. Nó chứa ba thành phần chính, bao gồm một supernet với các mô-đun có thể cắt được, một trình lắp ráp nhận biết yêu cầu, và một mô-đun tìm kiếm kiến trúc nhẹ và một bộ đánh giá hiệu suất biên cụ thể cho thiết bị. Supernet chứa các mô-đun cơ bản có thể được kết hợp linh hoạt để tạo thành các mô hình tùy chỉnh (subnet). Trình lắp ráp mô-đun nhận biết yêu cầu là một mô hình sinh tạo tạo ra các cấu hình lắp ráp mô-đun dựa trên tác vụ và yêu cầu độ thưa cho trước. Mỗi cấu hình được tạo ra ánh xạ đến một ứng viên mô hình. Mô-đun tìm kiếm kiến trúc nhẹ tìm mô hình tối ưu bằng cách lặp đi lặp lại tìm kiếm và đánh giá các ứng viên mô hình theo một bộ đánh giá hiệu suất cụ thể cho biên.

Cho một kịch bản biên (được mô tả bởi tác vụ, loại thiết bị và ràng buộc tài nguyên), quy trình tùy chỉnh mô hình của NN-Factory bao gồm các bước sau:

(1) Mô-đun tìm kiếm kiến trúc nhẹ đề xuất một yêu cầu sinh tạo mô hình <task, activation limit>, trong đó activation limit là tỷ lệ tối đa của các mô-đun được kích hoạt trong mô hình.

(2) Trình lắp ráp nhận biết yêu cầu dự đoán một cấu hình lắp ráp mô-đun dựa trên yêu cầu. Cấu hình mô tả cách tạo ra một mô hình ứng viên bằng cách kích hoạt và lắp ráp các mô-đun trong supernet.

(3) Bộ đánh giá hiệu suất cụ thể cho thiết bị đánh giá cấu hình được tạo ra (tức là mô hình ứng viên) so với các ràng buộc tài nguyên biên. Nếu các ràng buộc được thỏa mãn và ngân sách bộ nhớ/độ trễ được sử dụng đầy đủ, thì trả về ứng viên hiện tại. Ngược lại, quay lại bước (1) với một yêu cầu sinh tạo mới.

Các phần phụ sau sẽ giới thiệu các thành phần chính chi tiết hơn.

### 3.2 Supernet với Các Mô-đun Có thể Cắt

Supernet chịu trách nhiệm cung cấp các mô-đun neural cơ bản có thể được lắp ráp lại để đáp ứng các tác vụ và ràng buộc tài nguyên khác nhau. Việc cắt một mạng neural hiện có cho các khối chức năng khác nhau để có khả năng diễn giải đã được nghiên cứu trước đây [61-63], nhưng các mô-đun được cắt thường thô và khó lắp ráp lại. Chúng tôi quyết định trực tiếp huấn luyện một supernet có thể cắt để phân tách và khả năng kết hợp mô-đun tốt hơn.

Chúng tôi xây dựng supernet mô-đun bằng cách mở rộng một mạng backbone hiện có. Mạng backbone trích xuất đặc trưng từ các đầu vào cho trước và đưa ra dự đoán cuối cùng. Chúng tôi có thể hỗ trợ các backbone phổ biến dựa trên Mạng Neural Tích chập (CNN) và Transformer, như ResNet [20], EfficientNet [47], và Vision Transformer [11].

**CNN Backbone.** Đầu tiên, chúng tôi giới thiệu cách chuyển đổi một lớp tích chập trong kiến trúc CNN thành các mô-đun có thể cắt. Cho một feature map $x$ làm đầu vào, đầu ra của lớp tích chập thứ $l$ là $O_l(x)$. Trong một CNN thông thường, $O_l(x)$ được tính như:

$$O_l^i = \sigma(F_l^i * I_l(x)) \quad (2)$$

trong đó $O_l^i$ là kênh thứ $i$ của $O_l(x)$, $F_l^i$ là bộ lọc thứ $i$, $\sigma(\cdot)$ biểu thị hàm kích hoạt phi tuyến từng phần tử và $*$ biểu thị tích chập. feature map đầu ra $O_l(x)$ được thu được bằng cách áp dụng tất cả các bộ lọc $F_l^i$ trong lớp hiện tại lên feature map đầu vào $I_l(x)$.

Trong NN-Factory, chúng tôi coi mỗi bộ lọc tích chập như một mô-đun có thể được kích hoạt có điều kiện. Bằng cách kích hoạt các kết hợp bộ lọc khác nhau trong một lớp tích chập, các lớp kết quả có thể được coi là có các chức năng khác nhau.

Dựa trên việc phân tách mô-đun như vậy, chúng tôi giới thiệu một vector cổng $g$ để điều khiển việc kích hoạt các mô-đun. Với vector cổng, việc tính toán feature map $O_l^i$ trong Phương trình 2 được công thức hóa lại như dưới đây:

$$O_l^i = \sigma(F_l^i * I_l(x)) \cdot g_l^i \quad (3)$$

trong đó $g_l^i$ là mục trong $g$ tương ứng với bộ lọc thứ $i$ tại lớp $l$ và $0$ là một feature map 2-D với tất cả các phần tử bằng 0, chỉ khi $g_l^i$ bằng 1, bộ lọc thứ $i$ mới được áp dụng lên $I_l$ để trích xuất đặc trưng. Hình 4 mô tả quy trình tính toán như được mô tả ở trên.

Supernet mô-đun được thu được bằng cách áp dụng quy trình mô-đun hóa cho các lớp tích chập chính trong mô hình CNN. Lớp batch normalization sau mỗi lớp tích chập được cắt theo cách kênh và được điều khiển bởi cùng một vector cổng. Đáng chú ý là, trong các kiến trúc CNN sâu hiện đại như ResNet, EfficientNet và MobileNet, các lớp tích chập được tổ chức thành nhiều khối cơ bản. Chúng tôi không chuyển đổi các lớp tích chập ở cuối mỗi khối cơ bản để tránh xung đột hình dạng với các kết nối dư. Hình 6(a) và (b) minh họa các khối cơ bản CNN phổ biến được tích hợp với các cổng mô-đun có thể học.

**Transformer backbone.** Chúng tôi cũng hỗ trợ tạo ra supernet mô-đun từ các backbone Transformer. Các thành phần chính của Transformer bao gồm các lớp self-attention và mạng feed-forward (FFN). Theo các nghiên cứu gần đây [10], các thành phần FFN lưu trữ nhiều kiến thức thực tế khác nhau được học từ dữ liệu. Một FFN là một mạng kết nối đầy đủ hai lớp, xử lý một biểu diễn đầu vào $x \in \mathbb{R}^{d_{model}}$ như:

$$h = xW_1$$
$$F(x) = \sigma(h)W_2 \quad (4)$$

trong đó $W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$ và $W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$ là các ma trận trọng số.

Kiến trúc Transformer không mô-đun theo thiết kế, nhưng nó có thể được chuyển đổi thành một mô hình Mixture-of-Experts (MoE) tương đương [23,64], trong đó mỗi expert có thể được coi như một mô-đun chức năng được kích hoạt có điều kiện. Lấy cảm hứng từ ý tưởng này, chúng tôi đề xuất mô-đun hóa các lớp FFN trong các backbone Transformer và công thức hóa lại việc tính toán $F(x)$ trong Phương trình 4 như dưới đây:

$$h = xW_1$$
$$h' = h \cdot g_{FFN}$$
$$F(x) = \sigma(h')W_2 \quad (5)$$

trong đó $g_{FFN}$ đại diện cho việc lựa chọn cổng cho lớp FFN hiện tại. Nếu vị trí thứ $i$ trong $g_{FFN}$ là 0, vị trí tương ứng trong $h'$ cũng được đặt thành 0, chỉ ra rằng các tham số trong các phần tương ứng của $W_1$ và $W_2$ không được kích hoạt. Trong Hình 5, chúng tôi chứng minh các tham số vẫn không hoạt động dưới sự lựa chọn của cổng.

Tương tự, bằng cách áp dụng quy trình mô-đun hóa cho tất cả các FFN trong một backbone Transformer, chúng tôi có thể thu được một supernet dựa trên Transformer cho NN-Factory, một minh họa với backbone ViT được thể hiện trong Hình 6(c).

### 3.3 Trình Lắp ráp Mô-đun Nhận biết Yêu cầu

Trình lắp ráp mô-đun đóng vai trò quan trọng trong NN-Factory - nó tạo ra các vector cổng điều khiển việc kích hoạt các mô-đun neural trong supernet, sao cho các mô-đun được kích hoạt có thể được lắp ráp để tạo ra một mô hình ứng viên.

Đầu vào của trình lắp ráp mô-đun là một yêu cầu sinh tạo mô hình, được biểu diễn như một tuple <$task$, $activation\_limit$>. $task$ là một mô tả của tác vụ đích trong kịch bản biên. Như đã đề cập trong Phần 3, một tác vụ có thể được biểu diễn như một kết hợp của một số thuộc tính. Ví dụ, tác vụ 'detect black dogs' được biểu diễn là {$black$, $dog$} và 'detect white cat' là {$white$, $cat$}. $activation\_limit$ là một yêu cầu độ thưa điều chỉnh tỷ lệ các mô-đun được kích hoạt trong mô hình được tạo ra. Một giới hạn thấp hơn sẽ khuyến khích trình lắp ráp tạo ra các mô hình ứng viên nhỏ hơn.

Đầu ra của trình lắp ráp là các vector cổng $g$ như được mô tả trong Phần 3.2. Mỗi vector cổng $g_l$ tương ứng với một lớp mô-đun $l$ trong supernet và xác định việc kích hoạt các mô-đun trong lớp.

Kiến trúc mạng của trình lắp ráp được thể hiện trong Hình 7. Nó bao gồm ba thành phần chính, bao gồm một bộ mã hóa yêu cầu, một bộ mã hóa lựa chọn và một danh sách các gater lớp. Bộ mã hóa yêu cầu chuyển đổi mô tả tác vụ và yêu cầu độ thưa $activation\_limit$ thành một embedding. Chúng tôi sử dụng mã hóa one-hot cho thành phần $task$, được ký hiệu bởi $enc_{task}$. Mỗi bit trong $enc_{task}$ đại diện cho sự hiện diện của một thuộc tính cụ thể. Ví dụ, trong tác vụ 'detect black dog', các phần tử cho 'black' và 'dog' được đặt thành một và những phần tử khác được đặt thành không trong mã hóa tác vụ. Đối với thành phần $activation\_limit$, chúng tôi sử dụng Positional Encoding [49] để mã hóa tỷ lệ giới hạn, được ký hiệu bởi $enc_{limit}$. Hai mã hóa $enc_{task}$ và $enc_{limit}$ được nối với nhau thành mã hóa yêu cầu $enc_{req}$.

Bộ mã hóa lựa chọn chuyển đổi mã hóa tác vụ $enc_{req}$ thành một biểu diễn trung gian $enc_{sel}$ chứa kiến thức về việc lựa chọn cổng toàn mô hình. Chúng tôi sử dụng một lớp kết nối đầy đủ theo sau bởi một batch normalization (BN) và một đơn vị ReLU để thực hiện việc chuyển đổi này. Mã hóa lựa chọn toàn cục $enc_{sel}$ sau đó được đưa vào mỗi gater lớp để tính toán việc kích hoạt cho mỗi lớp. Gater lớp cũng là một lớp kết nối đầy đủ theo sau bởi BN và đơn vị ReLU. Giá trị đầu ra của nó $weight_l \in \mathbb{R}^{D_l}$ đại diện cho trọng số của các mô-đun trong lớp $l$, trong đó $D_l$ là số lượng mô-đun. Việc lựa chọn cổng cho lớp $g_l$ được thu được bằng cách rời rạc hóa $weight_l$ với một ngưỡng (tức là $g_l = weight_l > threshold$). Chúng tôi đặt $threshold = 0.5$ theo mặc định trong triển khai của chúng tôi.

### 3.4 Huấn luyện Chung và Triển khai Riêng biệt

**Huấn luyện Chung Supernet-Assembler.** Supernet mô-đun và trình lắp ráp có liên quan chặt chẽ - trình lắp ráp tạo ra việc kích hoạt các mô-đun, và việc kích hoạt đại diện cho các mô hình ứng viên với supernet. Do đó, chúng tôi huấn luyện chúng cùng nhau để làm cho chúng làm việc cùng nhau một cách mạch lạc.

Việc huấn luyện được tiến hành với một tập hợp các tác vụ huấn luyện và dữ liệu được gắn nhãn thuộc về mỗi tác vụ. Mỗi mẫu có thể được biểu diễn như một tuple {$task$, $activation\_limit$, $x$, $\hat{y}$}, trong đó $x$ và $\hat{y}$ là đầu vào và nhãn. Tất cả các mẫu được trộn với nhau và xáo trộn trong quá trình huấn luyện.

Trong mỗi lần chuyển tiếp, chúng tôi tính toán việc lựa chọn cổng $g$ bằng cách đưa $task$ và $activation\_limit$ vào trình lắp ráp, thu được subnet $M_g$ với việc lựa chọn cổng $g$, và nhận được dự đoán $y$ bằng cách đưa $x$ vào subnet $M_g$. Mất mát được tính toán bằng cách kiểm tra dự đoán $y$ và việc lựa chọn cổng $g$, tức là

$$L = TL(y, \hat{y}) + \lambda GL(g, activation\_limit) \quad (6)$$

$TL(y, \hat{y})$ là mất mát cho tác vụ huấn luyện, khuyến khích subnet được tạo ra tạo ra dự đoán chính xác dựa trên tác vụ cơ bản. $GL(\cdot)$ là mất mát cho việc lựa chọn cổng, khuyến khích trình lắp ráp tạo ra các subnet thỏa mãn yêu cầu độ thưa. Nếu tỷ lệ số một trong $g$ thấp hơn $activation\_limit$ cho trước, $GL(\cdot)$ bị vô hiệu hóa. Ngược lại, $GL(\cdot)$ là lỗi bình phương trung bình (MSE) giữa tỷ lệ số một trong $g$ và $activation\_limit$. $\lambda$ là một siêu tham số để cân bằng hai mục tiêu, được đặt thành 100 theo mặc định.

Lưu ý rằng mất mát lựa chọn cổng $GL(\cdot)$ được tính toán với việc lựa chọn cổng nhị phân rời rạc $g$, nhưng đầu ra thực tế của trình lắp ráp là trọng số cổng liên tục $weight$, có thể dẫn đến khó khăn trong việc lan truyền lỗi ngược. Để giải quyết vấn đề này, chúng tôi sử dụng một phương pháp gọi là Improved SemHash [26,27] để thực hiện thủ thuật. Việc tính toán được thực hiện như sau:

$$g_\alpha = \mathbf{1}(weight > 0)$$
$$g_\beta = \max(0, \min(1, 1.2\sigma(weight) - 0.1))$$

Ở đây, $g_\alpha$ là một vector nhị phân, trong khi $g_\beta$ là một vector cổng có giá trị thực với tất cả các mục nằm trong khoảng [0.0, 1.0]. $g_\alpha$ có thuộc tính nhị phân mong muốn mà chúng tôi muốn sử dụng trong huấn luyện, nhưng gradient của $g_\alpha$ bằng không cho hầu hết các giá trị của $g$. Ngược lại, gradient của $g_\beta$ được định nghĩa rõ ràng, nhưng $g_\beta$ không phải là một vector nhị phân. Trong lần chuyển tiếp trong quá trình huấn luyện, chúng tôi ngẫu nhiên sử dụng $g = g_\alpha$ cho một nửa các mẫu và sử dụng $g = g_\beta$ cho phần còn lại. Khi $g_\alpha$ được sử dụng, chúng tôi tuân theo giải pháp trong [26,27] và định nghĩa gradient của $g_\alpha$ giống như gradient của $g_\beta$ trong lan truyền ngược. Đối với đánh giá và suy luận, chúng tôi luôn sử dụng các cổng rời rạc.

**Triển khai Riêng biệt của Subnet.** Sau khi huấn luyện trước chung, chúng tôi có thể tạo ra việc lựa chọn cổng dựa trên các yêu cầu tác vụ và độ thưa khác nhau. Một khi việc lựa chọn cổng được xác định, mạng trình lắp ráp và các mô-đun bị vô hiệu hóa trong supernet mô-đun không còn hữu ích. Do đó, khi triển khai mô hình, chúng tôi chỉ phải truyền và triển khai subnet được lắp ráp với các mô-đun hoạt động. Ví dụ, với supernet dựa trên CNN, chúng tôi có thể cắt tỉa các bộ lọc không hoạt động dựa trên lựa chọn cổng. Bằng cách làm như vậy, subnet kết quả sẽ có kích thước mô hình, sử dụng bộ nhớ và độ trễ giảm đáng kể. Đối với supernet dựa trên Transformer, chúng tôi có thể đạt được kết quả tương tự bằng cách cắt tỉa các tham số tương ứng trong $W_1$ và $W_2$ dựa trên lựa chọn cổng. Mô hình được triển khai không yêu cầu thay đổi kiến trúc hoặc huấn luyện tham số thêm.

### 3.5 Tìm kiếm Kiến trúc Nhẹ

Các mô-đun và trình lắp ráp được huấn luyện chung cho phép chúng tôi hiệu quả tạo ra các mô hình ứng viên với các tác vụ và mức độ thưa khác nhau. Dựa trên khả năng như vậy, chúng tôi tiếp tục giới thiệu một chiến lược tìm kiếm kiến trúc nhẹ để tìm mô hình tối ưu cho mỗi kịch bản biên.

Quy trình tìm kiếm kiến trúc được hướng dẫn bởi một bộ đánh giá hiệu suất cụ thể cho thiết bị, nhận một mô hình ứng viên làm đầu vào và cho biết liệu mô hình có thỏa mãn các ràng buộc tài nguyên không (tức là độ trễ suy luận nhỏ hơn ngân sách độ trễ và chi phí bộ nhớ nhỏ hơn ngân sách bộ nhớ). Chúng tôi có thể trực tiếp đánh giá hiệu suất trên các thiết bị biên đích, tuân theo thực tiễn lựa chọn subnet trên thiết bị của AdaptiveNet [53].

Ngoài ra, khi việc triển khai supernet và trình lắp ráp lên thiết bị biên không dễ dàng, chúng tôi có thể xây dựng một bộ dự đoán hiệu suất để đánh giá các mô hình ứng viên với dữ liệu profiling được thu thập từ các thiết bị biên. Mô hình hóa hiệu suất thiết bị dựa trên profiling như vậy là một thực tiễn phổ biến trong tính toán di động/biên [17], trong khi ở đây chúng tôi thực hiện một số đơn giản hóa hợp lý dựa trên thiết kế mô-đun của chúng tôi. Cụ thể, chúng tôi thực hiện một phương pháp profiling và mô hình hóa hiệu suất theo lớp, trong đó tổng độ trễ của mô hình bằng tổng của tất cả các lớp (với một bias tĩnh), và độ trễ của mỗi lớp phụ thuộc vào vector cổng được dự đoán bởi trình lắp ráp mô-đun. Việc mô hình hóa tiêu thụ bộ nhớ tương tự, được xác định bởi tiêu thụ bộ nhớ tĩnh (các tham số mô hình) và bộ nhớ đỉnh tại runtime (đặc trưng trung gian lớn nhất). Để xây dựng các bộ dự đoán này, chúng tôi tạo ra một tập hợp (2000 trong thí nghiệm của chúng tôi) các vector cổng ngẫu nhiên, thu được các subnet tương ứng, đo các chỉ số hiệu suất của các subnet này trên thiết bị biên đích, và sử dụng dữ liệu được thu thập để huấn luyện các bộ dự đoán hiệu suất với hồi quy tuyến tính. Bộ dự đoán có độ chính xác cao, với cả độ chính xác dự đoán độ trễ và độ chính xác dự đoán bộ nhớ cao hơn 96% trên bốn thiết bị biên điển hình, như được thể hiện trong Hình 8. Chúng tôi sử dụng bộ dự đoán theo mặc định trong NN-Factory.

Dựa trên bộ đánh giá hiệu suất, chúng tôi có thể phân tích độ trễ và bộ nhớ của các mô hình ứng viên. Vì việc kích hoạt nhiều mô-đun hơn thường dẫn đến độ chính xác cao hơn (xem Phần 5.2), mô hình tối ưu cho một kịch bản biên là mô hình có tỷ lệ kích hoạt mô-đun cao nhất trong khi thỏa mãn các yêu cầu bộ nhớ và độ trễ. Thuật toán 1 thể hiện chiến lược tìm kiếm kiến trúc nhẹ của chúng tôi. Chúng tôi bắt đầu từ giới hạn kích hoạt mô-đun thấp nhất $limit_i = 1\%$, và lặp đi lặp lại tăng giới hạn. Đối với mỗi giới hạn kích hoạt, chúng tôi tạo ra việc lựa chọn cổng sử dụng mạng trình lắp ráp, và thu được độ trễ và bộ nhớ liên quan đến việc lựa chọn cổng. Việc lựa chọn cổng ứng viên cuối cùng đáp ứng các ràng buộc môi trường biên được xác định và trả về. Cuối cùng, chúng tôi tạo ra subnet với việc lựa chọn cổng kết quả, có thể được triển khai trực tiếp lên biên mà không cần xử lý thêm.

## 4 TRIỂN KHAI

Chúng tôi triển khai phương pháp của chúng tôi sử dụng Python. Phần huấn luyện dựa trên PyTorch. Các mô hình được tạo ra với PyTorch và triển khai lên thiết bị biên sử dụng framework TensorFlow Lite cho di động và PyTorch cho Desktop và Jetson.

**Chi tiết Kiến trúc và Huấn luyện.** Trong các gater lớp của trình lắp ráp, chúng tôi sử dụng các lớp batch normalization riêng biệt cho mã hóa lựa chọn của các lớp khác nhau, điều này có thể ảnh hưởng đến chất lượng của mô hình được tạo ra. Để tăng cường tính ổn định huấn luyện và cải thiện chất lượng của mô hình được tạo ra, chúng tôi bổ sung tập tác vụ huấn luyện với các tác vụ bổ sung để kết hợp thêm các kết hợp thuộc tính tác vụ, có thể tăng cường hiểu biết của mô hình về mã hóa tác vụ của chúng tôi. Để kết hợp các yêu cầu độ thưa rộng hơn trong quá trình huấn luyện mà không làm tổn hại chất lượng mô hình, chúng tôi sử dụng nhiều gater lớp để tăng cường khả năng của mô hình, dựa trên các nguyên tắc của Mixture of Experts (MoE), mỗi gater chuyển đổi mã hóa lựa chọn thành việc lựa chọn cổng. Ngoài ra, một mạng gating được giới thiệu để xác định trọng số của các đầu ra từ mỗi gater tại mỗi lớp.

## 5 ĐÁNH GIÁ

Chúng tôi tiến hành thí nghiệm để trả lời các câu hỏi sau: (1) NN-Factory có thể tạo ra các mô hình cụ thể cho biên không? Chất lượng của các mô hình được tạo ra như thế nào? (2) Chi phí của NN-Factory là bao nhiêu? (3) Khả năng sinh tạo mô hình của NN-Factory tổng quát hóa tốt như thế nào đến các kịch bản biên chưa thấy?

### 5.1 Thiết lập Thí nghiệm

**Tác vụ và Tập dữ liệu.** Chúng tôi đánh giá hiệu suất của NN-Factory trên hai cài đặt tùy chỉnh mô hình.

• **Numeric Visual Question Answering (NumVQA).** Đây là một cài đặt đơn giản để phân tích hiệu suất của sinh tạo mô hình cụ thể cho tác vụ và tài nguyên. Tác vụ là trả lời một câu hỏi có-hoặc-không (ví dụ: "Có hai số chẵn không?") dựa trên một hình ảnh đầu vào chứa bốn chữ số. Chúng tôi công thức hóa khoảng 60 câu hỏi và tổng hợp hình ảnh sử dụng tập dữ liệu MNIST [33]. Hiệu suất của các mô hình được tạo ra được đo bằng độ chính xác phân loại.

• **Attributed Object Detection (AttrOD).** Đây là một cài đặt thực tế hơn, trong đó mỗi tác vụ là phát hiện các đối tượng với các thuộc tính cụ thể trong một hình ảnh và dự đoán các hộp giới hạn và danh mục đối tượng. Chúng tôi sử dụng một mô hình trả lời câu hỏi thị giác [29] để chú thích màu sắc đối tượng trong tập dữ liệu COCO2017 [35] và hợp nhất các màu được xác định để tạo thành các thuộc tính đích. Chúng tôi chọn 5 thuộc tính (1-trắng, 2-đồng, 3-than, 4-đỏ thẫm, 5-vàng chanh) từ 4 danh mục và kết hợp chúng để xây dựng các tác vụ huấn luyện của chúng tôi. Tổng số tác vụ là khoảng 140, và mỗi tác vụ có số lượng mẫu khác nhau, từ hàng trăm đến hàng chục nghìn. Hiệu suất của các mô hình phát hiện được đo bằng độ chính xác trung bình trên ngưỡng Intersection over Union 0.5 (mAP@0.5).

**Backbone Mô hình.** Chúng tôi xem xét các backbone CNN và Transformer phổ biến trong thí nghiệm này, bao gồm ResNet [20], ViT [11], cho cài đặt NumVQA và EfficientDet [48] cho cài đặt AttrOD.

**Baseline.** Chúng tôi so sánh NN-Factory với hai phương pháp tùy chỉnh mô hình thông thường:

(1) **Retrain** - Chúng tôi cố định kiến trúc mô hình và huấn luyện lại nó với phương pháp học có giám sát tiêu chuẩn cho mỗi kịch bản biên.

(2) **Prune&Tune** - Chúng tôi huấn luyện một mô hình thống nhất. Cho một kịch bản biên, chúng tôi cắt tỉa và tinh chỉnh mô hình được huấn luyện trước để khớp với yêu cầu biên với phương pháp cắt tỉa SOTA [14].

Cả hai đều yêu cầu huấn luyện với dữ liệu cụ thể cho biên. Chúng tôi sử dụng cùng backbone với các baseline này và huấn luyện chúng cho đến khi hội tụ. Chúng tôi không bao gồm các phương pháp sinh tạo/mở rộng mô hình không cần huấn luyện khác [4,13,17,53] vì chúng không hỗ trợ tùy chỉnh hướng tác vụ.

**Môi trường Biên.** Chúng tôi xem xét ba thiết bị biên bao gồm một Smartphone Android (Xiaomi 12) với bộ xử lý Snapdragon® 8 Gen 1 và bộ nhớ 12GB, một Jetson AGX Xavier với bộ nhớ 32 GB, và một máy tính để bàn với bộ xử lý 12th Gen Intel® Core™ i9-12900K×24 với bộ nhớ 64GB. Kích thước batch đều được đặt thành 1 trên ba thiết bị để mô phỏng tải công việc thực. Chúng tôi sử dụng các ngân sách độ trễ khác nhau để mô phỏng đa dạng phần cứng nội thiết bị.

### 5.2 Chất lượng Sinh tạo Mô hình

Chúng tôi tiến hành đánh giá các mô hình được tạo ra bởi NN-Factory trên NumVQA và AttrOD, theo sau bởi phân tích toàn diện về chất lượng của chúng.

Đầu tiên, chúng tôi đánh giá hiệu quả của các mô-đun và trình lắp ráp trong cài đặt NumVQA. Chúng tôi đưa các tuple <$task$, $activation\_limit$> khác nhau vào trình lắp ráp NN-Factory và để nó tạo ra các mô hình đáp ứng yêu cầu. Kết quả được trình bày trong Bảng 2. Nhìn chung, NN-Factory chứng minh độ chính xác đặc biệt (>99% với backbone ResNet) trên các tác vụ đa dạng, đồng thời đảm bảo rằng các mô hình được tạo ra tuân thủ giới hạn kích hoạt đã chỉ định của chúng tôi. Do quy mô tác vụ tương đối nhỏ, chỉ yêu cầu tỷ lệ kích hoạt thấp hơn.

Tiếp theo, chúng tôi đánh giá hiệu suất đầu-cuối của NN-Factory trên AttrOD. Trong Bảng 3, chúng tôi sử dụng ResNet50 làm backbone và trình bày chất lượng của các mô hình được tạo ra dưới các yêu cầu độ trễ và bộ nhớ khác nhau trên các thiết bị khác nhau, và so sánh chúng với các mô hình baseline. Nhìn chung, NN-Factory liên tục cung cấp các mô hình chất lượng cao đáp ứng các tiêu chí đã chỉ định trong tất cả các kịch bản biên. Nhờ tìm kiếm kiến trúc mô hình nhận biết hiệu suất, NN-Factory có thể sử dụng đầy đủ các ngân sách cho trước (bộ nhớ hoặc độ trễ). Baseline Retrain không đáp ứng được các yêu cầu độ trễ và bộ nhớ vì nó không điều chỉnh kiến trúc mô hình cho mỗi kịch bản.

Độ chính xác của các mô hình được tạo ra bởi NN-Factory gần với baseline Retrain và vượt trội hơn baseline Prune&Tune, mặc dù nó không yêu cầu huấn luyện cụ thể cho biên. Đồng thời, nó đã đạt được điểm mAP cao hơn đáng kể trên một số tác vụ (ví dụ: Motorcycle {1}, Person {5}) chứa ít mẫu huấn luyện hơn những tác vụ khác. Điều này là do việc huấn luyện hỗn hợp các tác vụ, mỗi tác vụ với các kết hợp thuộc tính khác nhau, cho phép mô hình đạt được hiểu biết sâu sắc hơn về các tác vụ và do đó, đưa ra dự đoán chính xác hơn. Độ chính xác của các mô hình Prune&Tune thấp hơn nhiều so với NN-Factory và Retrain do kích thước mô hình giảm.

Chúng tôi cũng khám phá sự khác biệt trong chất lượng mô hình của NN-Factory với các backbone khác nhau. Kết quả được hiển thị trong Hình 9. NN-Factory chứng minh hành vi nhất quán trên các backbone supernet khác nhau, biểu thị khả năng tổng quát hóa của nó. Tuy nhiên, nó thể hiện sự khác biệt hiệu suất riêng biệt dựa trên backbone được chọn. Ví dụ, các mô hình được tạo ra bởi NN-Factory dựa trên ViT và NN-Factory dựa trên MobileNet thể hiện độ chính xác thấp hơn. Điều này là do các thuộc tính vốn có của mạng backbone, ví dụ: hiệu quả mẫu kém của ViT và khả năng mô hình hạn chế của MobileNet.

Hơn nữa, chúng tôi thực hiện phân tích để đánh giá cách NN-Factory quản lý sự đánh đổi giữa chất lượng mô hình và độ trễ. Kết quả được trình bày trong Hình 10, NN-Factory có khả năng tạo ra các mô hình chất lượng cao hơn khi được cho các ràng buộc độ trễ cao hơn. Hình 11 minh họa mối tương quan giữa giới hạn kích hoạt đầu vào trong quá trình sinh tạo mô hình và tỷ lệ kích hoạt của việc lựa chọn cổng được tạo ra bởi NN-Factory. Với giới hạn kích hoạt rất thấp, tỷ lệ kích hoạt được tạo ra không đáp ứng được yêu cầu. Do đó, nó trải qua cắt tỉa dựa trên tầm quan trọng, dẫn đến một mẫu gần khớp với đường chéo trên đồ thị. Với việc tăng giới hạn kích hoạt, NN-Factory tạo ra các tỷ lệ kích hoạt có thể thỏa mãn tiêu chí. Các tỷ lệ cải thiện dần dần với việc tăng giới hạn kích hoạt và cuối cùng đạt trạng thái ổn định, vì các tỷ lệ ổn định đã đủ để tạo ra dự đoán chính xác.

### 5.3 Hiệu quả Sinh tạo Mô hình

Chúng tôi tiến hành phân tích so sánh về chi phí chuẩn bị và tùy chỉnh giữa NN-Factory và các baseline, và kết quả được trình bày trong Bảng 4.

**Chi phí Tùy chỉnh cho Mỗi Kịch bản Biên.** Mục tiêu chính của NN-Factory là giảm thời gian tùy chỉnh mô hình. Chỉ mất trung bình 3.6 giây cho NN-Factory để tạo ra một mô hình tùy chỉnh cho một kịch bản biên, chứng minh hiệu quả đáng chú ý. Quy trình này bao gồm khoảng 12 vòng tìm kiếm, mỗi vòng mất 0.2 giây, và trích xuất mô hình, mất 0.8 giây. Ngược lại, các phương pháp tùy chỉnh mô hình truyền thống phải chịu chi phí tăng lên 4000 lần về thời gian vì chúng cần huấn luyện mô hình cho mỗi kịch bản biên.

**Chi phí Chuẩn bị Một lần.** Chi phí chuẩn bị của NN-Factory chủ yếu đến từ việc huấn luyện supernet mô-đun. Đầu tiên, chúng tôi phân tích mối quan hệ giữa chất lượng mô hình được tạo ra và số epoch huấn luyện. Như được mô tả trong Hình 12, NN-Factory yêu cầu nhiều epoch huấn luyện hơn để cải thiện dần chất lượng của các mô hình được tạo ra. Điều này liên quan đến phương pháp huấn luyện của chúng tôi. NN-Factory trải qua huấn luyện đồng thời trên nhiều tác vụ, với mỗi tác vụ chỉ có quyền truy cập vào một phần dữ liệu huấn luyện trong mỗi epoch. Do đó, nó cần nhiều epoch huấn luyện hơn để tạo ra các mô hình chất lượng cao cho mỗi tác vụ riêng lẻ. Tuy nhiên, do sự phụ thuộc lẫn nhau đáng kể giữa các tác vụ, việc huấn luyện đồng thời của chúng có tác dụng hiệp đồng, đảm bảo rằng chi phí huấn luyện của NN-Factory không tăng đáng kể.

Bộ dự đoán hiệu suất được sử dụng cho tìm kiếm kiến trúc nhẹ cũng được xây dựng trong quá trình chuẩn bị. Hình 13 thể hiện độ chính xác mô hình hóa hiệu suất đạt được với số lượng subnet khác nhau được sử dụng cho profiling và mô hình hóa (backbone supernet là ResNet). Khi số lượng subnet tăng, độ chính xác dự đoán cho thấy xu hướng tăng và đạt đường tiệm cận. Chúng ta có thể thấy rằng việc sử dụng 200∼1000 subnet cho profiling và huấn luyện là đủ để đạt được độ chính xác dự đoán độ trễ và bộ nhớ tốt (cao hơn 95%). Việc thu thập dữ liệu profiling cho một subnet mất 70ms (Jetson GPU) đến 985ms (Mobile CPU). Đồng thời, thời gian cần thiết để khớp mô hình hiệu suất ít hơn vài giây. Do đó, thời gian để thiết lập mô hình dự đoán hiệu suất trong NN-Factory dao động từ khoảng 14s đến 985s, có thể bỏ qua như một quy trình offline một lần.

**Chi phí Phục vụ Trên thiết bị.** Các mô hình được tạo ra bởi NN-Factory là các mô hình tĩnh bình thường, không tạo ra chi phí bổ sung nào tại runtime.

### 5.4 Tổng quát hóa đến Các Tác vụ Chưa thấy

Trong phần này, chúng tôi đánh giá khả năng tổng quát hóa của NN-Factory đến các tác vụ chưa thấy. Các tác vụ chưa thấy đề cập đến các tác vụ không phải là một phần của tập tác vụ huấn luyện, nhưng chúng chia sẻ cùng không gian thuộc tính kết hợp như các tác vụ huấn luyện.

Hình 14 minh họa độ chính xác của các tác vụ chưa thấy được chọn ngẫu nhiên trong NumVQA và AttrOD. Mặc dù độ chính xác cho các tác vụ chưa thấy có thể thể hiện sự giảm nhẹ so với các tác vụ đã biết và một số tác vụ chưa thấy thậm chí có thể cho độ chính xác thấp hơn nhiều, phần lớn các tác vụ chứng minh độ chính xác xuất sắc, có nghĩa là các mô-đun trong NN-Factory có thể được lắp ráp hiệu quả để xử lý các tác vụ mới mà không cần dữ liệu huấn luyện. Kết quả này làm nổi bật khả năng tổng quát hóa mạnh mẽ của NN-Factory, vì nó có thể nắm bắt ý nghĩa của các thuộc tính tạo thành một tác vụ và hiểu các thao tác liên quan đến việc kết hợp chúng.

Chúng tôi tiếp tục phân tích khả năng tổng quát hóa của NN-Factory dựa trên sự tương đồng giữa việc lựa chọn cổng cho các tác vụ khác nhau. Kết quả được trình bày trong Hình 15. Việc lựa chọn cổng thể hiện sự tương đồng cao hơn khi các tác vụ giống nhau hơn (ví dụ: 'Only one 0' và 'Only three 0'). Ngoài ra, trong một tác vụ cố định, việc tăng gần gũi trong các giới hạn kích hoạt cho trước dẫn đến sự tương đồng lớn hơn trong việc lựa chọn cổng. Điều này chứng minh khả năng của NN-Factory hiểu cả yêu cầu tác vụ và giới hạn kích hoạt đã chỉ định, và ánh xạ chúng đến các mô-đun tương ứng. Điều này phục vụ như nền tảng cho khả năng tổng quát hóa của nó.

Với khả năng này, NN-Factory có thể xác định các mô-đun liên quan nhất cho một tác vụ chưa thấy và lắp ráp một mô hình với chúng. Ví dụ, các mô-đun được kích hoạt cho tác vụ chưa thấy 'only two 0' tương tự như các mô-đun của 'only three 0' và 'only one 0' trong Hình 15. Điều này dẫn đến độ chính xác cao trên tác vụ chưa thấy. Do đó, khả năng tổng quát hóa của NN-Factory có tương quan tích cực với số lượng tác vụ để huấn luyện, có thể dẫn đến nhiều mô-đun hữu ích hơn và một trình lắp ráp mạnh mẽ hơn.

## 6 THẢO LUẬN

Ở đây chúng tôi làm nổi bật một số vấn đề đáng thảo luận thêm.

**Khả năng áp dụng cho các loại tác vụ khác.** Hiện tại, NN-Factory chỉ hỗ trợ tùy chỉnh mô hình cho các tác vụ trong không gian kết hợp, có thể hạn chế khả năng áp dụng của nó cho các kịch bản sử dụng tổng quát hơn. Để cho phép không gian tác vụ linh hoạt hơn, chúng tôi cần sử dụng một mô hình sinh tạo mạnh mẽ hơn làm trình lắp ráp, nhận định nghĩa tác vụ dạng tự do (ví dụ: mô tả tác vụ ngôn ngữ tự nhiên) làm đầu vào, và hoạt động trên một tập hợp các mô-đun neural lớn hơn. Đồng thời, một tập dữ liệu lớn chứa các ánh xạ giữa các tác vụ khác nhau và các cặp đầu vào/đầu ra tương ứng được yêu cầu để huấn luyện trình lắp ráp và các mô-đun. Điều này khả thi theo những tiến bộ gần đây của các mô hình lớn được huấn luyện trước (ví dụ: ImageBind, ChatGPT, v.v.), nhưng việc huấn luyện một bộ sinh tạo mô hình đa năng như vậy rất tốn thời gian và tài nguyên, không thực tế cho hầu hết các nhà nghiên cứu. Chúng tôi để việc phát triển NN-Factory đa năng như vậy cho công việc tương lai.

**Chi phí chuẩn bị của NN-Factory.** Khả năng tùy chỉnh mô hình nhanh chóng đáng chú ý của NN-Factory có cái giá là thời gian chuẩn bị offline dài hơn. Cụ thể, việc huấn luyện supernet mô-đun và trình lắp ráp mô-đun mất thời gian lâu hơn nhiều so với việc huấn luyện một mô hình tĩnh bình thường. Điều này là do NN-Factory cần không chỉ học cách giải quyết từng tác vụ riêng lẻ, mà còn cách tách rời các mô-đun và lắp ráp lại chúng để giải quyết các tác vụ mới. Xem xét chi phí biên giảm để hỗ trợ các kịch bản biên đa dạng, chi phí chuẩn bị một lần ít quan trọng hơn. Lợi ích của việc giảm chi phí biên như vậy có giá trị hơn nếu NN-Factory hỗ trợ không gian tác vụ linh hoạt hơn.

## 7 KẾT LUẬN

Bài báo này đề xuất một phương pháp mới cho việc tùy chỉnh nhanh chóng các mô hình học sâu cho các kịch bản biên đa dạng. Với thiết kế toàn diện gồm supernet mô-đun, trình lắp ráp mô-đun và bộ tìm kiếm kiến trúc nhẹ, chúng tôi có thể đạt được tùy chỉnh mô hình nhanh chóng cho các tác vụ biên đa dạng và ràng buộc tài nguyên. Thí nghiệm đã chứng minh chất lượng và tốc độ sinh tạo mô hình xuất sắc của phương pháp chúng tôi. Chúng tôi tin rằng công trình của chúng tôi đã cho phép một trải nghiệm tùy chỉnh mô hình sinh tạo mới và quan trọng.

## TÀI LIỆU THAM KHẢO

[1] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 39–48.

[2] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. 2017. Adaptive Neural Networks for Efficient Inference. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 (ICML'17). JMLR.org, 527–536.

[3] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. 2020. Once for All: Train One Network and Specialize it for Efficient Deployment. In International Conference on Learning Representations. https://arxiv.org/pdf/1908.09791.pdf

[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. 2020. Once-for-All: Train One Network and Specialize it for Efficient Deployment. In 8th International Conference on Learning Representations, ICLR 2020.

[5] Han Cai, Ligeng Zhu, and Song Han. 2019. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. In International Conference on Learning Representations. https://arxiv.org/pdf/1812.00332.pdf

[6] Yimin Chen, Jingchao Sun, Xiaocong Jin, Tao Li, Rui Zhang, and Yanchao Zhang. 2017. Your face your heart: Secure mobile face authentication with photoplethysmograms. In IEEE INFOCOM 2017 - IEEE Conference on Computer Communications. 1–9. https://doi.org/10.1109/INFOCOM.2017.8057220

[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).

[8] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. 2018. Generative adversarial networks: An overview. IEEE signal processing magazine 35, 1 (2018), 53–65.

[9] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. 2023. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).

[10] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2021. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696 (2021).

[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929 (2020).

[12] Biyi Fang, Xiao Zeng, Faen Zhang, Hui Xu, and Mi Zhang. 2020. FlexDNN: Input-Adaptive On-Device Deep Learning for Efficient Mobile Vision. In 2020 IEEE/ACM Symposium on Edge Computing (SEC). 84–95. https://doi.org/10.1109/SEC50012.2020.00014

[13] Biyi Fang, Xiao Zeng, and Mi Zhang. 2018. NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for Continuous Mobile Vision. Proceedings of the 24th Annual International Conference on Mobile Computing and Networking (2018).

[14] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. 2023. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16091–16101.

[15] Peizhen Guo, Bo Hu, and Wenjun Hu. 2021. Mistify: Automating DNN Model Porting for On-Device Inference at the Edge. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21). USENIX Association, 705–719. https://www.usenix.org/conference/nsdi21/presentation/guo

[Tiếp tục với các tài liệu tham khảo còn lại...]
