# 2302.05629.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/2302.05629.pdf
# Kích thước file: 1100350 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Cải thiện Tìm kiếm Kiến trúc Khả vi thông qua
Tự Chưng cất
Xunyu Zhua,b, Jian Lia,b,∗, Yong Liuc, Weiping Wanga,b
aViện Kỹ thuật Thông tin Viện Hàn lâm Khoa học Trung Quốc.
bTrường An ninh Mạng Đại học Viện Hàn lâm Khoa học Trung Quốc.
cTrường Trí tuệ Nhân tạo Gaoling Đại học Nhân dân Trung Quốc.
Tóm tắt
Tìm kiếm Kiến trúc Khả vi (DARTS) là một phương pháp Tìm kiếm Kiến trúc
Mạng nơ-ron (NAS) đơn giản nhưng hiệu quả. Trong giai đoạn tìm kiếm,
DARTS huấn luyện một siêu mạng bằng cách tối ưu hóa đồng thời các tham số
kiến trúc và tham số mạng. Trong giai đoạn đánh giá, DARTS rời rạc hóa siêu
mạng để tạo ra kiến trúc tối ưu dựa trên các tham số kiến trúc. Tuy nhiên,
nghiên cứu gần đây đã chỉ ra rằng trong quá trình huấn luyện, siêu mạng có
xu hướng hội tụ về các cực tiểu sắc nhọn thay vì các cực tiểu phẳng. Điều này
được chứng minh bởi độ sắc nhọn cao hơn của địa hình mất mát của siêu mạng,
cuối cùng dẫn đến khoảng cách hiệu suất giữa siêu mạng và kiến trúc tối ưu.
Trong bài báo này, chúng tôi đề xuất Tìm kiếm Kiến trúc Mạng nơ-ron Khả vi
Tự Chưng cất (SD-DARTS) để giảm thiểu khoảng cách rời rạc hóa. Chúng tôi
sử dụng tự chưng cất để chưng cất kiến thức từ các bước trước đó của siêu mạng
để hướng dẫn việc huấn luyện của nó trong bước hiện tại, hiệu quả giảm độ sắc
nhọn của mất mát siêu mạng và thu hẹp khoảng cách hiệu suất giữa siêu mạng
và kiến trúc tối ưu. Hơn nữa, chúng tôi giới thiệu khái niệm các giáo viên bỏ
phiếu, nơi nhiều siêu mạng trước đó được chọn làm giáo viên, và các xác suất
đầu ra của chúng được tổng hợp thông qua bỏ phiếu để có được dự đoán cuối
cùng của giáo viên. Kết quả thực nghiệm trên các tập dữ liệu thực cho thấy ưu
điểm của phương pháp NAS dựa trên tự chưng cất mới của chúng tôi so với các
phương án thay thế tiên tiến.
Từ khóa: tìm kiếm kiến trúc mạng nơ-ron, mạng nơ-ron, độ phẳng, kiến thức
∗Tác giả liên hệ
Địa chỉ email: zhuxunyu@iie.ac.cn (Xunyu Zhu), lijian9026@iie.ac.cn (Jian
Li),liuyonggsai@ruc.edu.cn (Yong Liu), wangweiping@iie.ac.cn (Weiping Wang)
Bản thảo gửi tới Neural Networks Ngày 4 tháng 9 năm 2023arXiv:2302.05629v2  [cs.CV]  1 Sep 2023

--- TRANG 2 ---
chưng cất, tối thiểu hóa nhận thức độ sắc nhọn
1. Giới thiệu
Học sâu đã trở nên rất phổ biến trong nhiều lĩnh vực khác nhau do khả năng
tự động hóa việc học đặc trưng và đạt được hiệu suất ấn tượng. Nó đã được
áp dụng thành công trong các lĩnh vực như phân loại hình ảnh [1], theo dõi
đối tượng [2], xử lý hình ảnh [3], và phát hiện văn bản [4]. Để nâng cao
hiệu quả của học sâu, một số kiến trúc xuất sắc đã được phát triển bởi các
nhà nghiên cứu và giáo sư, bao gồm VGG [5], ResNet [6], Transformer [7],
Bert [8], và GPT [9]. Những kiến trúc này đã đóng góp đáng kể vào việc cải
thiện hiệu suất của các mô hình học sâu.

Tuy nhiên, việc thiết kế các kiến trúc chất lượng cao một cách thủ công có thể
là một quá trình tốn công sức và thời gian. Chi phí của việc thiết kế kiến trúc
thủ công thường cao, đòi hỏi chuyên môn rộng rãi và thử nghiệm trial-and-error.
Do đó, Tìm kiếm Kiến trúc Mạng nơ-ron (NAS) đã trở nên phổ biến trong
học thuật và công nghiệp vì nó cho phép máy móc tự động khám phá các kiến
trúc với hiệu suất tốt. Ban đầu, các phương pháp NAS như Học Tăng cường
[10] và Thuật toán Tiến hóa [11,12] được giới thiệu, bao gồm việc lấy mẫu và
đánh giá các kiến trúc riêng lẻ bằng cách huấn luyện chúng từ đầu. Tuy nhiên,
những phương pháp này gặp khó khăn trong việc xử lý không gian tìm kiếm
tăng theo cấp số nhân, dẫn đến việc cần nhiều tài nguyên tính toán để tìm ra
kiến trúc tối ưu. NASNet [13, 14] giới thiệu phương pháp dựa trên tế bào,
nơi các tế bào riêng lẻ được tìm kiếm và sau đó xếp chồng lại với nhau để tạo
thành mạng cuối cùng. Một phương pháp khác, ENAS [15], giới thiệu khái
niệm siêu mạng, nơi một mạng duy nhất được huấn luyện với các trọng số
được chia sẻ làm proxy để đánh giá hiệu suất của các mạng con riêng lẻ.
Những phương pháp này giảm đáng kể chi phí tính toán liên quan đến NAS.
Tuy nhiên, NAS vẫn được coi là một bài toán tối ưu hóa rời rạc. Để đáp ứng,
các phương pháp dựa trên gradient như SPOS [16] và NAO [17] được đề xuất.
Những phương pháp này nới lỏng bài toán NAS thành một bài toán tối ưu hóa
liên tục và sử dụng các bộ tối ưu hóa dựa trên gradient để khám phá không
gian tìm kiếm liên tục. Trong số những phương pháp này, DARTS nổi bật
như một trong những phương pháp dựa trên gradient thành công nhất, cung
cấp hiệu quả tìm kiếm cao.

--- TRANG 3 ---
Tìm kiếm Kiến trúc Khả vi (DARTS) cách mạng hóa NAS bằng cách công
thức hóa nó như một bài toán tối ưu hóa liên tục và tận dụng các kỹ thuật
tối ưu hóa dựa trên gradient. DARTS đạt được chi phí tìm kiếm thấp, chỉ
cần 0.3 GPU ngày để khám phá các kiến trúc chất lượng cao. Để cải thiện
hiệu quả hơn nữa, PC-DARTS [18] giới thiệu kết nối kênh một phần và chuẩn
hóa cạnh để giảm tiêu thụ bộ nhớ và chi phí tính toán. SGAS [19] giới thiệu
tìm kiếm kiến trúc tham lam tuần tự để giải quyết vấn đề tương quan tìm
kiếm-đánh giá thoái hóa. Trong giai đoạn đánh giá của DARTS, siêu mạng
được rời rạc hóa để xác định kiến trúc tối ưu. Tuy nhiên, mặc dù liên tục
giảm lỗi xác thực của siêu mạng và kiến trúc tối ưu, vẫn còn tồn tại khoảng
cách hiệu suất trong quá trình đánh giá.

Một số bài báo [20,21] đã thảo luận về vấn đề này và quy kết khoảng cách
hiệu suất cho hình học của địa hình mất mát của siêu mạng. SDARTS [21]
và R-DARTS [20] xác định độ sắc nhọn của địa hình mất mát xác thực trong
DARTS là nguyên nhân của khoảng cách hiệu suất. SDARTS giới thiệu việc
tạo ra các nhiễu loạn trọng số để cải thiện quá trình huấn luyện của siêu mạng,
nhưng phương pháp này phát sinh chi phí tính toán đáng kể. R-DARTS sử
dụng giá trị riêng chiếm ưu thế của chuẩn Hessian làm chỉ số, sử dụng dừng
sớm khi chỉ số đạt đến ngưỡng. Tuy nhiên, R-DARTS ngăn cản DARTS khám
phá các kiến trúc với hiệu suất có thể tốt hơn. Việc giảm thiểu khoảng cách
rời rạc hóa bằng cách làm phẳng địa hình mất mát của siêu mạng vẫn là một
câu hỏi mở đáng để khám phá thêm.

Bài báo của chúng tôi giới thiệu một phương pháp mới gọi là Tự Chưng cất
DARTS (SD-DARTS), được minh họa trong Hình 1. SD-DARTS tận dụng
chưng cất kiến thức để hướng dẫn việc huấn luyện của siêu mạng. Cụ thể, nó
chuyển giao kiến thức được học bởi siêu mạng trong bước thời gian trước đó
để hướng dẫn việc huấn luyện của siêu mạng hiện tại. Bằng cách ép buộc
DARTS xem xét thông tin từ các epoch trước đó trong quá trình tối ưu hóa,
SD-DARTS giảm độ sắc nhọn của địa hình mất mát của siêu mạng. Trái ngược
với các phương pháp hiện có, phương pháp của chúng tôi sử dụng thông tin
từ các epoch trước đó thay vì tạo ra các nhiễu loạn trọng số. Ngoài ra, chúng
tôi cho phép siêu mạng được huấn luyện đến khi hội tụ. Những yếu tố phân
biệt này làm nổi bật tính mới lạ và giá trị của phương pháp chúng tôi trong
việc cải thiện hiệu suất của DARTS.

Ngoài ra, chúng tôi nhận ra rằng thông tin được cung cấp bởi một giáo viên
duy nhất trong SD-DARTS có thể không đủ. Học tập tổ hợp cho thấy rằng
các dự đoán của nhiều mô hình kết hợp lại với nhau thường chính xác hơn
so với một mô hình duy nhất, vì kiến thức tập thể từ nhiều mô hình thường
toàn diện hơn. Lấy cảm hứng từ học tập tổ hợp, chúng tôi giới thiệu khái
niệm "giáo viên bỏ phiếu" để hướng dẫn việc huấn luyện của siêu mạng. Trong
thực tế, chúng tôi nhắm đến việc tận dụng kiến thức từ nhiều siêu mạng trong

--- TRANG 4 ---
K bước thời gian trước đó và tổng hợp các dự đoán của chúng thông qua bỏ
phiếu. Bằng cách kết hợp các dự đoán của những siêu mạng này, chúng tôi
tạo ra xác suất đầu ra cuối cùng của giáo viên. Việc tích hợp SD-DARTS với
các giáo viên bỏ phiếu này cho phép chúng tôi khám phá và phát hiện các
kiến trúc chất lượng cao một cách hiệu quả hơn.

Chúng tôi tiến hành một loạt thí nghiệm để đánh giá hiệu quả của phương
pháp đề xuất trong việc cải thiện hiệu suất của DARTS. Chúng tôi sử dụng
phương pháp của mình để tìm kiếm các kiến trúc chất lượng cao và đánh giá
hiệu suất của chúng trên các tập dữ liệu khác nhau. Trên tập dữ liệu CIFAR-10,
kiến trúc của chúng tôi đạt được tỷ lệ lỗi kiểm tra đáng chú ý là 2.58%, vượt
qua hiệu suất của DARTS và nhiều biến thể của nó. Kết quả này làm nổi bật
sự vượt trội của phương pháp chúng tôi trong việc tạo ra các kiến trúc với
hiệu suất được cải thiện trên tập dữ liệu này. Hơn nữa, chúng tôi đánh giá
hiệu suất của kiến trúc trên tập dữ liệu ImageNet đầy thử thách và đạt được
tỷ lệ lỗi kiểm tra 25.0%. Những kết quả này củng cố hiệu quả của phương
pháp NAS dựa trên tự chưng cất mới của chúng tôi và chứng minh ưu điểm
của nó so với các phương án thay thế tiên tiến. Thông qua những thí nghiệm
này trên các tập dữ liệu thực, chúng tôi cung cấp bằng chứng thực nghiệm
để hỗ trợ hiệu quả và sự vượt trội của phương pháp đề xuất trong việc tạo
ra các kiến trúc chất lượng cao với hiệu suất được cải thiện.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi đề xuất SD-DARTS, một phương pháp DARTS mới với hiệu suất
tốt hơn. Nó chưng cất kiến thức từ các bước trước đó của siêu mạng để
hướng dẫn việc huấn luyện của siêu mạng trong bước hiện tại. Nó có thể
làm phẳng địa hình mất mát của siêu mạng để thu hẹp khoảng cách hiệu
suất giữa siêu mạng và mạng con tối ưu.
• Chúng tôi đề xuất các giáo viên bỏ phiếu, một phương pháp mới bằng cách
sử dụng nhiều giáo viên để hướng dẫn việc huấn luyện của siêu mạng. Các
dự đoán của các giáo viên bỏ phiếu chính xác hơn so với một giáo viên
duy nhất. Chúng tôi bỏ phiếu các dự đoán của các siêu mạng trong K bước
thời gian trước đó để tạo ra xác suất đầu ra cuối cùng của giáo viên và sử
dụng xác suất đầu ra của giáo viên để hướng dẫn việc huấn luyện của chính
siêu mạng đó.
• Các thí nghiệm rộng rãi chứng minh rằng phương pháp của chúng tôi có thể
cải thiện hiệu quả hiệu suất của DARTS và đạt được kết quả tốt trên các
tập dữ liệu chính.

--- TRANG 5 ---
. . . . . .Siêu mạng Siêu mạng
Epoch 𝒕−𝟏Giáo viênXác suất đầu ra 
của giáo viên
Xác suất đầu ra 
của học sinh Nhãn thực tế
Epoch  𝒕Khởi động siêu mạng=
Cập nhậtMất mátHình 1: Giai đoạn huấn luyện của SD-DARTS bao gồm việc sử dụng siêu mạng từ epoch t−1
làm giáo viên cho epoch t. Tương quan giữa xác suất đầu ra của giáo viên và
xác suất đầu ra của siêu mạng được tính toán, phục vụ như một số hạng điều chuẩn để hướng dẫn
việc huấn luyện của siêu mạng.

2. Công trình liên quan
2.1. Tìm kiếm Kiến trúc Khả vi
Tìm kiếm Kiến trúc Khả vi (DARTS) đã trở nên phổ biến như một phương
pháp NAS chi phí thấp bằng cách công thức hóa NAS như một bài toán tối
ưu hóa liên tục. Một số phương pháp đã được đề xuất để nâng cao hiệu suất
của DARTS. P-DARTS [22] dần dần tăng độ sâu của siêu mạng để giải quyết
sự khác biệt giữa độ sâu kiến trúc trong các tình huống tìm kiếm và đánh giá.
R-DARTS [20] sử dụng dừng sớm dựa trên các giá trị riêng hessian để giám sát
các thay đổi trong quá trình tối ưu hóa. SDARTS [21] giới thiệu các nhiễu loạn
trên các tham số kiến trúc để kiểm soát các giá trị riêng hessian và làm phẳng
địa hình mất mát của siêu mạng. DARTS+ [23] kết hợp dừng sớm khi số lượng
kết nối bỏ qua trong tế bào bình thường vượt quá ngưỡng. DARTS-PT [24]
giới thiệu một phương pháp lựa chọn kiến trúc dựa trên nhiễu loạn thay thế.
DAAS [25] giới thiệu một số hạng mất mát dựa trên entropy để hướng dẫn
siêu mạng về phía cấu trúc liên kết mong muốn. Gold-nas [26] sử dụng ràng
buộc tài nguyên biến đổi trong tối ưu hóa một cấp để dần loại bỏ các toán tử
yếu hơn. DARTS- [27] thêm một kết nối bỏ qua phụ trợ để đảm bảo cạnh tranh
công bằng giữa tất cả các phép toán. Khác với những công trình này, chúng
tôi nhắm đến việc giảm thiểu khoảng cách rời rạc hóa của DARTS bằng cách
tập trung vào việc làm phẳng địa hình mất mát của siêu mạng với chi phí
tính toán ít hơn.

2.2. Tự Chưng cất
Chưng cất Kiến thức (KD) [28] là một kỹ thuật chuyển giao kiến thức của
một mô hình giáo viên được huấn luyện tốt cho một mô hình học sinh trong

--- TRANG 6 ---
quá trình huấn luyện. Theo truyền thống, một mô hình giáo viên được huấn
luyện tốt với khả năng cao hơn mô hình học sinh được sử dụng để hướng dẫn
quá trình huấn luyện. Tuy nhiên, việc có được một mô hình giáo viên được
huấn luyện tốt có thể là thách thức. Để giải quyết vấn đề này, các công trình
gần đây [29,30] đã khám phá việc sử dụng tự chưng cất, nơi chính mô hình
học sinh được sử dụng làm giáo viên để chưng cất kiến thức của chính nó.

Tự chưng cất đã trở nên phổ biến trong nhiều lĩnh vực khác nhau. Ví dụ,
Mean Teacher [31] kết hợp tự chưng cất với học bán giám sát để cải thiện hiệu
suất. Nó tối thiểu hóa khoảng cách L2 giữa các dự đoán của mô hình giáo viên
và các dự đoán của mô hình học sinh trong học bán giám sát, nơi mô hình
giáo viên chính là mô hình học sinh. Dual-Teacher++ [32] giới thiệu một
khung thích ứng miền bán giám sát tiên tiến sử dụng mô hình giáo viên kép.
Nó bao gồm một mô hình giáo viên liên miền khám phá các prior đa phương
thức từ miền nguồn và một mô hình giáo viên nội miền điều tra kiến thức
trong miền đích không có nhãn. LE-UDA [33] giới thiệu một phương pháp
nhất quán tự tổ hợp cho chuyển giao kiến thức trong thích ứng miền không
giám sát, kết hợp nó với một mô-đun học đối kháng tự tổ hợp để đạt được
căn chỉnh đặc trưng tốt hơn. Li et al. [34] đề xuất một khung giáo viên trung
bình điều chuẩn nhất quán phân cấp cho phân đoạn tâm nhĩ trái 3D, nơi mô
hình học sinh được tối ưu hóa bằng cách sử dụng giám sát sâu đa tỷ lệ và
điều chuẩn nhất quán phân cấp. ACT-NET [35] thúc đẩy học tập giáo viên-
học sinh với mạng đồng giáo viên, tạo điều kiện cho chưng cất kiến thức từ
các mô hình lớn sang các mô hình nhỏ bằng cách luân phiên vai trò học sinh
và giáo viên. ODC [36] áp dụng tự chưng cất cho các nhiệm vụ dịch máy nơ-ron,
cập nhật mô hình giáo viên khi hiệu suất của nó vượt qua mô hình giáo viên
hiện tại trên dữ liệu xác thực. Trong bài báo của chúng tôi, chúng tôi nhắm
đến việc kết hợp tự chưng cất với DARTS để nâng cao hiệu suất của tìm
kiếm kiến trúc. Bằng cách tận dụng kiến thức được chưng cất từ chính mô
hình học sinh, chúng tôi hy vọng cải thiện chất lượng của các kiến trúc được
tạo ra trong DARTS.

2.3. Tối thiểu hóa Nhận thức Độ sắc nhọn
Tối thiểu hóa nhận thức độ sắc nhọn (SAM) [37] là một kỹ thuật điều chuẩn
nhắm đến việc làm phẳng địa hình mất mát của mạng. SAM đạt được điều
này bằng cách giới thiệu nhiễu đối kháng vào các tham số mạng trong quá
trình huấn luyện, điều này khuyến khích mất mát ổn định hơn và ít nhạy
cảm hơn với các nhiễu loạn tham số nhỏ. Bằng cách thúc đẩy một địa hình
mất mát phẳng hơn, SAM có thể nâng cao khả năng tổng quát hóa của mô
hình. ASAM [38] xây dựng dựa trên SAM bằng cách sử dụng một ranh giới
tổng quát hóa để thiết lập kết nối giữa độ sắc nhọn của địa hình mất mát và
khoảng cách tổng quát hóa. Bằng cách tối thiểu hóa độ sắc nhọn của địa hình
mất mát, ASAM nhắm đến việc giảm overfitting và cải thiện khả năng tổng

--- TRANG 7 ---
quát hóa của mô hình với dữ liệu chưa thấy. Zhao et al. [39] đề xuất phạt
chuẩn gradient của hàm mất mát trong quá trình tối ưu hóa để làm phẳng
địa hình mất mát của mạng. Bằng cách điều chuẩn chuẩn gradient, quá trình
tối ưu hóa trở nên ổn định hơn và ít dễ bị biến động sắc nhọn, dẫn đến một
địa hình mất mát phẳng hơn. SAF [40] giải quyết vấn đề giảm đột ngột trong
mất mát có thể xảy ra trong các cực tiểu địa phương sắc nhọn trong quỹ đạo
của các cập nhật trọng số. SAF nhắm đến việc tránh những giảm đột ngột này
bằng cách điều chỉnh động tỷ lệ học dựa trên độ sắc nhọn của địa hình mất
mát. Bằng cách ngăn chặn giảm sắc nhọn trong mất mát, SAF thúc đẩy một
quá trình tối ưu hóa phẳng hơn và giúp mô hình hội tụ đến các giải pháp tốt
hơn. Những phương pháp này làm nổi bật tầm quan trọng của việc xem xét
độ phẳng của địa hình mất mát trong việc huấn luyện các mạng nơ-ron sâu.
Bằng cách thúc đẩy một địa hình mất mát phẳng và ổn định hơn, những kỹ
thuật này nhắm đến việc cải thiện khả năng tổng quát hóa và tính chất hội
tụ của các mô hình.

3. Phương pháp luận
3.1. Sơ bộ
Tìm kiếm Kiến trúc Khả vi (DARTS) [41] là một phương pháp Tìm kiếm
Kiến trúc Mạng nơ-ron (NAS) phổ biến được biết đến với hiệu quả tìm kiếm
cao và chi phí tính toán thấp. Thay vì tìm kiếm toàn bộ mạng, DARTS tập
trung vào việc tìm kiếm các tế bào trong mạng. Mỗi tế bào được biểu diễn
như một đồ thị có hướng không chu trình (DAG) với nhiều nút, nơi mỗi nút
đại diện cho một bản đồ đặc trưng. Tế bào có hai nút đầu vào, một nút đầu
ra, và một số nút trung gian. Thông tin từ các nút đầu vào được truyền qua
các nút trung gian đến nút đầu ra bằng cách sử dụng các phép toán liên kết
với các cạnh có hướng. Để xác định tầm quan trọng của các phép toán khác
nhau trong không gian phép toán ứng viên O, DARTS gán trọng số cho mỗi
phép toán bằng cách sử dụng các tham số kiến trúc α(i,j). Tổng có trọng số
của các phép toán được tính như ¯o(i,j)(x) =P
o∈Oexp
α(i,j)
o
P
o′∈Oexp
α(i,j)
o′o(x). Các tham số kiến trúc kiểm soát tầm quan trọng của mỗi phép toán
trên cạnh tương ứng.

Quá trình tìm kiếm của DARTS có thể được công thức hóa như một bài toán
tối ưu hóa hai cấp, nơi mục tiêu là tối thiểu hóa mất mát xác thực Lval đối
với các tham số kiến trúc α, trong khi tìm các tham số mạng tối ưu w∗(α)
tối thiểu hóa mất mát huấn luyện Ltrain đối với w. Điều này được biểu diễn
như:

--- TRANG 8 ---
min
αLval(w∗(α), α)
s.t.w∗(α) = arg min
wLtrain(w, α).(1)

Sau khi tối ưu hóa bài toán tối ưu hóa hai cấp này, siêu mạng được rời rạc
hóa để tạo ra kiến trúc tối ưu dựa trên các tham số kiến trúc. Điều này được
thực hiện bằng cách chọn phép toán có trọng số cao nhất cho mỗi cạnh, tức
là o(i,j)=arg max o∈Oα(i,j)
o. Bước rời rạc hóa cho phép chúng ta có được
kiến trúc cuối cùng dựa trên các tham số kiến trúc đã học, cho phép triển
khai kiến trúc mạng được tối ưu hóa.

3.2. Động lực
Các nghiên cứu gần đây [20,21] cho thấy rằng có một khoảng cách hiệu
suất giữa siêu mạng và mạng con tối ưu, và một yếu tố quan trọng dẫn đến
khoảng cách rời rạc hóa của Tìm kiếm Kiến trúc Khả vi (DARTS) là độ sắc
nhọn của địa hình mất mát của siêu mạng. Người ta đã quan sát thấy rằng
khi địa hình mất mát của siêu mạng phẳng hơn, DARTS có nhiều khả năng
khám phá các kiến trúc với hiệu suất tốt hơn thông qua quá trình rời rạc hóa.
Ngược lại, khi địa hình mất mát của siêu mạng sắc nhọn, các kiến trúc tìm
được có hiệu suất tệ hơn. Tuy nhiên, các phương pháp hiện có được đề xuất
trong những bài báo này có hạn chế trong việc giải quyết hiệu quả và hiệu
quả vấn đề độ sắc nhọn trong địa hình mất mát của siêu mạng. Cần nhiều
nghiên cứu hơn để phát triển các kỹ thuật hiệu quả và hiệu quả hơn để giảm
thiểu vấn đề này và cải thiện thêm hiệu suất của DARTS.

Một số bài báo gần đây [37,38] đã khám phá ý tưởng làm phẳng địa hình
mất mát bằng cách gắn nhiễu đối kháng vào các tham số mạng trong giai
đoạn huấn luyện. Dựa trên khái niệm này, SDARTS [21] giới thiệu việc gắn
nhiễu đối kháng vào các tham số kiến trúc trong giai đoạn tìm kiếm để nâng
cao độ phẳng của địa hình mất mát của siêu mạng. Tuy nhiên, phương pháp
này phát sinh chi phí tính toán đáng kể trong việc tạo ra nhiễu đối kháng.
Một phương pháp khác được giới thiệu bởi SAF [40] gợi ý rằng việc tối thiểu
hóa sự khác biệt trong mất mát giữa hai lần lặp liên tiếp tương đương với
việc tối thiểu hóa độ sắc nhọn của địa hình mất mát. Được thúc đẩy bởi khái
niệm tự chưng cất, sử dụng thông tin của chính mạng để hướng dẫn việc
huấn luyện của nó, chúng tôi sử dụng tự chưng cất như một khung để hướng
dẫn việc huấn luyện của siêu mạng. Khung này nhắm đến việc nâng cao độ
phẳng của địa hình mất mát của siêu mạng.

--- TRANG 9 ---
3.3. Tự chưng cất DARTS
Trong bài báo của chúng tôi, chúng tôi đề xuất phương pháp tự chưng cất
DARTS (SD-DARTS), sử dụng khung tự chưng cất để hướng dẫn việc huấn
luyện của siêu mạng trong DARTS. SD-DARTS nhắm đến việc cải thiện hiệu
suất của siêu mạng bằng cách gắn tự chưng cất vào bài toán cấp trong của
tối ưu hóa hai cấp của DARTS. Công thức của SD-DARTS như sau:

min
αLval(w∗(α), α) +λH
fS(x), fT(x)
s.t.w∗(α) = arg min
wLtrain(w, α) +λH
fS(x), fT(x)
,(2)

Trong Phương trình (2), f đại diện cho siêu mạng của DARTS, fS và fT
lần lượt là các mô hình học sinh và giáo viên. Mô hình học sinh tương ứng
với siêu mạng tại bước thời gian t, tức là fS=ft(x), và mô hình giáo viên
tương ứng với mô hình học sinh tại bước thời gian trước đó. Các xác suất
đầu ra của các mô hình học sinh và giáo viên được ký hiệu là fS(x) và fT(x),
tương ứng. Số liệu H được sử dụng để tính toán tương quan giữa các xác
suất đầu ra, và trong bài báo của chúng tôi, chúng tôi sử dụng phân kỳ
Kullback-Leibler (KL) làm số liệu. Hệ số điều chuẩn λ được sử dụng để
cân bằng tầm quan trọng của hai số hạng mất mát. Trong SD-DARTS, giáo
viên tại bước thời gian hiện tại là siêu mạng tại bước thời gian trước đó, tức là

fT(x) =ft−1(x).

Do đó, công thức trong Phương trình (2) có thể được đơn giản hóa thành:

min
αLval(w∗(α), α) +λH(ft(x), ft−1(x))
s.t.w∗(α) = arg min
wLtrain(w, α) +λH(ft(x), ft−1(x)).

Quá trình chi tiết của SD-DARTS được minh họa trong Hình 1. Ở đây chúng
tôi sẽ giới thiệu SD-DARTS một cách chi tiết.

Đầu tiên, chúng tôi bắt đầu bằng việc khởi động siêu mạng f. Cả các tham
số kiến trúc α và các tham số mạng w đều được khởi tạo ngẫu nhiên. Mục
tiêu của giai đoạn khởi động này là cho phép siêu mạng học kiến thức có giá
trị từ dữ liệu huấn luyện. Điều quan trọng cần lưu ý là chỉ những siêu mạng
chất lượng cao mới có thể phục vụ như những giáo viên đủ tiêu chuẩn để
hướng dẫn việc huấn luyện của chính siêu mạng. Điều này là do những giáo
viên chất lượng thấp có thể cung cấp hướng dẫn sai lầm và cản trở quá trình
huấn luyện. Do đó, giai đoạn khởi động đóng vai trò quan trọng trong việc
chuẩn bị siêu mạng cho tự chưng cất hiệu quả.

--- TRANG 10 ---
Sau giai đoạn khởi động, chúng tôi giới thiệu một khung tự chưng cất để
hướng dẫn việc huấn luyện của DARTS. Tại epoch t, chúng tôi sử dụng siêu
mạng tại epoch t−1 làm mô hình giáo viên, ký hiệu là fT, nơi siêu mạng
trước đó chính nó trở thành giáo viên. Để có được xác suất đầu ra của giáo
viên, có hai phương pháp. Phương pháp đầu tiên bao gồm việc lưu trữ các
tham số của mô hình giáo viên trong bộ nhớ GPU và tạo ra xác suất đầu ra
của giáo viên bằng cách chạy mô hình giáo viên khi cần thiết. Tuy nhiên,
phương pháp này đòi hỏi tài nguyên tính toán đáng kể. Để giảm thiểu chi
phí tính toán, chúng tôi đề xuất một phương pháp thay thế. Chúng tôi lưu
trữ xác suất đầu ra của siêu mạng tại bước thời gian trước đó, tức là fT(x),
trực tiếp trong bộ nhớ và sử dụng nó khi cần thiết. Trong thực tế, dung
lượng bộ nhớ máy tính thường lớn, cho phép phân bổ một phần nhỏ bộ nhớ
để lưu trữ xác suất đầu ra của học sinh. Phương pháp này giảm hiệu quả
việc tiêu thụ tính toán liên quan đến việc truy cập xác suất đầu ra của giáo
viên.

Tiếp theo, chúng tôi tính toán tương quan giữa xác suất đầu ra của giáo viên,
fT(x), và xác suất đầu ra của học sinh, ft(x), tại epoch t, sử dụng số liệu
tương quan đã chọn H. Có nhiều số liệu tương quan khả dụng, như Khoảng
cách Euclide (ED), Khoảng cách Manhattan (MD), Khoảng cách Cosine (CD),
và phân kỳ Kullback-Leibler (KL). Vì nhiệm vụ của chúng tôi là phân loại
hình ảnh, chúng tôi sử dụng phân kỳ KL làm số liệu tương quan H trong
bài báo của chúng tôi. Phân kỳ KL là một số liệu đơn giản được sử dụng để
định lượng sự khác biệt giữa hai phân phối xác suất. Trong trường hợp của
chúng tôi, chúng tôi tính toán phân kỳ KL như sau:

H(fS(x), fT(x)) =X
ifS(xi) logfS(xi)
fT(xi),

nơi f(xi) ký hiệu xác suất đầu ra cho ví dụ thứ i. Chúng tôi sử dụng số liệu
tương quan này như một số hạng điều chuẩn và kết hợp nó vào việc huấn
luyện của siêu mạng.

Quá trình tổng thể của SD-DARTS được tóm tắt trong Thuật toán 1. Tương
tự như DARTS, phương pháp của chúng tôi cũng sử dụng chiến lược tối ưu
hóa xen kẽ để tối ưu hóa các tham số kiến trúc và tham số mạng. SGD và
Adam được sử dụng để huấn luyện các tham số mạng và tham số kiến trúc,
tương ứng. Các cài đặt thí nghiệm chi tiết được hiển thị trong Mục 4.1. Sau
việc huấn luyện siêu mạng, kiến trúc tối ưu được tạo ra từ siêu mạng dựa
trên các tham số kiến trúc.

Ở đây, chúng tôi chứng minh cách tự chưng cất có thể làm phẳng hiệu quả

--- TRANG 11 ---
Thuật toán 1 SD-DARTS
Đầu vào: siêu mạng f, số liệu tương quan H, tổng epochs E, epochs khởi
động ξ.
Khởi tạo các tham số kiến trúc α và tham số mạng w.
Khởi động siêu mạng f trong ξ epochs.
for t←(ξ+ 1) to E do
Chọn siêu mạng ft−1 tại bước thời gian trước đó (epoch t−1) làm
giáo viên.
Cập nhật α bằng cách tối ưu hóa min
αLval(w∗(α), α) +λH(ft(x), ft−1(x));
Cập nhật w bằng cách tối ưu hóa min
wLtrain(w, α) +λH(ft(x), ft−1(x));
end for

địa hình mất mát của một mạng nơ-ron phổ quát f với các tham số của nó
được ký hiệu là α bằng cách sử dụng các công cụ lý thuyết từ SAM [37] và
SAF [40]. SAM [37] được giới thiệu để làm phẳng địa hình mất mát bằng
cách giải quyết bài toán minmax sau:

min
αmax
∥ϵ∥2≤ρL(fα+ϵ).

nơi L, ρ và ϵ ký hiệu hàm mất mát, ràng buộc nhiễu loạn cho phép và nhiễu
loạn trên các tham số, tương ứng. Hơn nữa, mất mát độ sắc nhọn R(fα) có
thể được biểu diễn như L(fα+ˆϵ)− L(fα), nơi ˆϵ ký hiệu nhiễu loạn tối ưu
trên các tham số, và ˆϵ được tính bằng ρ∇αL(fα)⊤
∥∇αL(fα)∥2[37]. Chúng tôi sử dụng
khai triển Taylor bậc nhất để phân tích mất mát độ sắc nhọn như sau:

R(fα) =L(fα+ˆϵ)− L(fα)≈ L(fα) + ˆϵ∇αL(fα)− L(fα)
=ρ∇αL(fα)⊤
∥∇αL(fα)∥2∇αL(fα) =ρ∥∇αL(fα)∥2, (3)

Phương trình (3) cho thấy rằng việc tối thiểu hóa mất mát độ sắc nhọn tương
đương với việc tối thiểu hóa chuẩn ℓ2 của gradient ∇αL(fα).

Mặt khác, được thúc đẩy bởi SAF [40], sự thay đổi của mất mát xác thực
của mạng trong hai lần lặp liên tiếp có thể được biểu diễn như

L(fα)− L(fα−λ∇αL(fα))≈λ∥∇αL(fα)∥2
2≈λ
ρ2R(fα)2. (4)

nơi λ ký hiệu như tỷ lệ học trong việc huấn luyện các tham số. SAM [37]
cho thấy rằng tỷ lệ học λ trong việc huấn luyện các tham số thường

--- TRANG 12 ---
0 10 20 30 40 50
Epoch0.00.10.20.30.4Giá trị riêng chiếm ưu thế của Chuẩn Hessiankdarts
dartsHình 2: Quỹ đạo của chuẩn Hessian của siêu mạng trên DARTS tiêu chuẩn và SD-DARTS.
Giá trị riêng chiếm ưu thế của chuẩn Hessian của siêu mạng trên SD-DARTS nhỏ hơn
DARTS tiêu chuẩn, và kết quả cho thấy rằng địa hình mất mát của SD-DARTS phẳng hơn
DARTS.

nhỏ hơn ρ. Dựa trên Phương trình (4), chúng tôi thấy rằng sự thay đổi của
mất mát tỷ lệ thuận với R(fα)2. Do đó, việc tối thiểu hóa sự khác biệt mất
mát bằng với việc tối thiểu hóa độ sắc nhọn của mạng trong trường hợp này.
Phân tích của chúng tôi có thể dễ dàng được tổng quát hóa cho DARTS.
Hình 2 cũng cho thấy rằng giá trị riêng chiếm ưu thế của chuẩn Hessian
của siêu mạng trên SD-DARTS nhỏ hơn DARTS tiêu chuẩn, tức là địa hình
mất mát của SD-DARTS phẳng hơn DARTS.

Nhận xét 1. Trong phương pháp của chúng tôi, chúng tôi sử dụng tự chưng
cất (SD) như một khung để huấn luyện siêu mạng. Để đánh giá độ sắc nhọn
của địa hình mất mát xác thực, chúng tôi tính toán giá trị riêng chiếm ưu
thế của chuẩn Hessian của siêu mạng. Hình 2 chứng minh rằng giá trị riêng
chiếm ưu thế của chuẩn Hessian của siêu mạng trong SD-DARTS nhỏ hơn
so với DARTS tiêu chuẩn. Điều này chỉ ra rằng địa hình mất mát của SD-
DARTS phẳng hơn so với DARTS. Địa hình mất mát phẳng hơn trong SD-
DARTS gợi ý rằng phương pháp của chúng tôi hiệu quả trong việc giảm độ
sắc nhọn và cải thiện tính ổn định của quá trình huấn luyện siêu mạng.

--- TRANG 13 ---
… Siêu mạng Siêu mạng Siêu mạng Siêu mạng … …Bỏ phiếu
Xác suất đầu ra 
của giáo viên
Giáo viên Giáo viên Giáo viên …Xác suất đầu ra 
của giáo viênXác suất đầu ra 
của giáo viênXác suất đầu ra 
của giáo viên
Xác suất đầu ra 
của học sinh
Epoch 𝒕−𝟏 Epoch 𝒕−𝑲 Epoch 𝒕−𝑲+𝟏 Epoch 𝒕Hình 3: Giáo viên bỏ phiếu. Chúng tôi sử dụng các siêu mạng tại K bước thời gian trước đó làm giáo viên và
sau đó bỏ phiếu các xác suất đầu ra của giáo viên được xuất ra bởi siêu mạng để tạo ra xác suất đầu ra cuối cùng của giáo viên.

3.4. Giáo viên Bỏ phiếu
Trong SD-DARTS, chúng tôi sử dụng siêu mạng từ bước thời gian trước
đó làm giáo viên để hướng dẫn quá trình huấn luyện. Tuy nhiên, chúng tôi
nhận ra rằng thông tin được cung cấp bởi một giáo viên duy nhất có thể bị
hạn chế. Để vượt qua hạn chế này, chúng tôi đề xuất sử dụng nhiều giáo
viên để có được một nguồn thông tin đa dạng và phong phú hơn. Mỗi giáo
viên đóng góp chuyên môn và kinh nghiệm riêng của mình để hướng dẫn việc
huấn luyện của siêu mạng. Để đưa ra dự đoán, chúng tôi sử dụng một sơ
đồ bỏ phiếu nơi các xác suất đầu ra của mỗi giáo viên được kết hợp để tạo
ra dự đoán cuối cùng của giáo viên. Phương pháp sử dụng nhiều giáo viên
này cho phép chúng tôi tận dụng một phạm vi kiến thức rộng hơn và tăng
tính đa dạng của hướng dẫn trong quá trình huấn luyện. Bằng cách tổng hợp
những hiểu biết từ nhiều giáo viên, chúng tôi có thể đạt được hiệu suất tốt
hơn và các kiến trúc mạnh mẽ hơn trong SD-DARTS.

Trong phương pháp đề xuất của chúng tôi, được biết đến như SD-DARTS,
chúng tôi giới thiệu một khái niệm gọi là "giáo viên bỏ phiếu" để hướng dẫn
việc huấn luyện của siêu mạng. Lấy cảm hứng từ học tập tổ hợp, chứng minh
rằng hiệu suất của nhiều mô hình thường tốt hơn so với một mô hình duy
nhất, chúng tôi tận dụng ý tưởng kết hợp nhiều giáo viên để nâng cao quá
trình huấn luyện. Trong bối cảnh của SD-DARTS, chúng tôi xem xét một
cửa sổ thời gian có kích thước K, nơi chúng tôi chọn nhiều siêu mạng từ K
bước thời gian trước đó làm giáo viên. Các xác suất đầu ra của những siêu
mạng giáo viên này sau đó được tính trung bình để tạo ra xác suất đầu ra

--- TRANG 14 ---
Thuật toán 2 Giáo viên Bỏ phiếu
Đầu vào: siêu mạng f, số liệu tương quan H, tổng epochs E, epochs khởi
động ξ, cửa sổ thời gian K.
Khởi tạo các tham số kiến trúc α và tham số mạng w.
Khởi động siêu mạng f trong ξ epochs.
for t←(ξ+ 1) to E do
Chọn các siêu mạng tại K bước thời gian trước đó (từ epoch t−K đến
epoch t−1) làm giáo viên.
Cập nhật α bằng cách tối ưu hóa min
αLval(w∗(α), α) +λH
f(x),1
KKP
i=1ft−i(x)
;
Cập nhật w bằng cách tối ưu hóa min
wLtrain(w, α) +λH
f(x),1
KKP
i=1ft−i(x)
;
end for

cuối cùng của giáo viên. Về mặt toán học, xác suất đầu ra cuối cùng của giáo
viên có thể được biểu diễn như sau:

fT(x) =1
KKX
i=1ft−i(x),

nơi ft−i(x) ký hiệu xác suất đầu ra của siêu mạng tại bước thời gian t−i.
Bằng cách kết hợp các dự đoán từ nhiều giáo viên, chúng tôi nhắm đến việc
hưởng lợi từ kiến thức và hiểu biết tập thể của họ, dẫn đến hướng dẫn được
cải thiện trong quá trình huấn luyện của siêu mạng. Phương pháp này cho
phép chúng tôi tận dụng thế mạnh của các siêu mạng giáo viên khác nhau
và có thể đạt được hiệu suất tốt hơn và các kiến trúc mạnh mẽ hơn trong
SD-DARTS.

Trong phương pháp đề xuất của chúng tôi, kết hợp giáo viên bỏ phiếu với
SD-DARTS, chúng tôi nhắm đến việc tận dụng kiến thức và hiểu biết từ
nhiều siêu mạng giáo viên để hướng dẫn việc tìm kiếm kiến trúc. Hàm mục
tiêu trong trường hợp này được công thức hóa như sau:

min
αLval(w∗(α), α) +λH 
f(x),1
KKX
i=1ft−i(x)!
s.t.w∗(α) = arg min
wLtrain(w, α) +λH
f(x),1
KKP
i=1ft−i(x)
,

nơi t đại diện cho bước thời gian hiện tại, λ đại diện cho hệ số điều chuẩn,
K là cửa sổ thời gian kiểm soát số lượng giáo viên, và 1
KKP
i=1ft−i(x) ký hiệu
xác suất đầu ra cuối cùng của giáo viên được thu thập bằng cách tính trung
bình các xác suất đầu ra của nhiều siêu mạng giáo viên. Số hạng điều chuẩn

--- TRANG 15 ---
H
f(x),1
KKP
i=1ft−i(x)
khuyến khích tính nhất quán giữa xác suất đầu ra
của siêu mạng và các xác suất đầu ra trung bình của các giáo viên bỏ phiếu.
Để thực hiện phương pháp giáo viên bỏ phiếu, chúng tôi cung cấp mô tả
thuật toán trong Thuật toán 2. Thuật toán này phác thảo các bước để chọn
các siêu mạng giáo viên từ các bước thời gian trước đó, tính trung bình các
xác suất đầu ra của chúng, và sử dụng xác suất đầu ra trung bình làm xác
suất đầu ra cuối cùng của giáo viên để hướng dẫn việc huấn luyện của siêu
mạng. Bằng cách kết hợp giáo viên bỏ phiếu vào SD-DARTS, chúng tôi nhắm
đến việc tận dụng kiến thức tập thể và hiểu biết từ nhiều siêu mạng giáo
viên, có thể dẫn đến hiệu suất được cải thiện và các kiến trúc mạnh mẽ hơn
trong quá trình tìm kiếm.

Bằng cách sử dụng giáo viên bỏ phiếu kết hợp với SD-DARTS, chúng tôi
có thể tận dụng kiến thức tập thể của nhiều siêu mạng giáo viên để hướng
dẫn việc tìm kiếm kiến trúc. Phương pháp này cho phép hướng dẫn đa dạng
và có thông tin hơn trong quá trình huấn luyện của siêu mạng, có thể dẫn
đến hiệu suất tốt hơn và các kiến trúc được cải thiện. Bộ nhớ bổ sung cần
thiết để lưu trữ các xác suất đầu ra của các siêu mạng giáo viên tương đối
nhỏ so với yêu cầu bộ nhớ tổng thể của quá trình huấn luyện. Điều này cho
phép chúng tôi tận dụng thông tin phong phú từ nhiều giáo viên mà không
tăng đáng kể chi phí tính toán hoặc bộ nhớ.

Nhận xét 2. 1.Học tập tổ hợp đã chỉ ra rằng việc kết hợp kiến thức của
nhiều mô hình có thể dẫn đến hiệu suất được cải thiện so với một mô hình
duy nhất. Bằng cách chọn nhiều siêu mạng từ các bước thời gian trước đó
làm giáo viên, chúng tôi có thể tận dụng kiến thức và chuyên môn đa dạng
của những mô hình này để hướng dẫn việc huấn luyện của siêu mạng.
Việc sử dụng nhiều giáo viên giúp nắm bắt một phạm vi thông tin và
hiểu biết rộng hơn, dẫn đến một quá trình huấn luyện toàn diện và mạnh
mẽ hơn. Nó cho phép siêu mạng hưởng lợi từ trí tuệ tập thể của nhiều
kiến trúc, tăng cơ hội tìm ra các kiến trúc chất lượng cao.
2.Phương pháp của chúng tôi khác với Mean Teacher [31] trong cách tiếp
cận tính trung bình. Trong khi Mean Teacher tập trung vào việc tính trung
bình các trọng số của mạng, phương pháp của chúng tôi nhắm đến việc
tính trung bình đầu ra của mạng. Chúng tôi có hai lý do để chọn phương
pháp giáo viên bỏ phiếu thay vì Mean Teacher. Thứ nhất, quy mô tham
số của đầu ra mạng nhỏ hơn đáng kể so với các trọng số, và chi phí tính
toán và bộ nhớ sẽ trở nên lớn hơn khi chúng ta tính trung bình trọng số
thay vì đầu ra. Bởi vì trọng số và đầu ra có liên kết rất chặt chẽ, bằng
cách sử dụng trung bình của đầu ra để hướng dẫn việc huấn luyện tự
chưng cất của chúng tôi, phương pháp của chúng tôi vẫn có tác động lên

--- TRANG 16 ---
các trọng số. Nó cho phép chúng tôi tận dụng lợi ích của việc tính trung
bình đầu ra trong khi xem xét tầm quan trọng tương đối của các trọng số.
Thứ hai, sự kết hợp giữa các tham số kiến trúc và tham số mạng trong
mô hình của chúng tôi, do các chiến lược tối ưu hóa xen kẽ, có thể dẫn
đến bất ổn trong quá trình huấn luyện khi sử dụng các giáo viên được
tạo ra bằng cách tính trung bình các trọng số. Sự bất ổn có thể cản trở
sự hội tụ và ảnh hưởng đến hiệu suất tổng thể của mô hình. Bằng cách
xem xét những yếu tố này, chúng tôi đã xác định rằng phương pháp giáo
viên bỏ phiếu phù hợp hơn với các mục tiêu cụ thể và đặc điểm mô hình
của chúng tôi. Quyết định này cho phép chúng tôi tận dụng hiệu quả các
ưu điểm của việc tính trung bình trong khi giảm thiểu các bất ổn tiềm
ẩn trong quá trình huấn luyện.

4. Thí nghiệm
Trong các phần này, chúng tôi tiến hành các thí nghiệm rộng rãi để xác
minh hiệu quả của phương pháp chúng tôi. Các thí nghiệm của chúng tôi
được tiến hành trên 3 ×NVIDIA GeForce GTX 3090 GPUs. CIFAR-10 [42]
và ImageNet [1] là hai tập dữ liệu phân loại hình ảnh mà các thí nghiệm của
chúng tôi được tiến hành trên đó. Chúng tôi tìm kiếm kiến trúc tối ưu bằng
SD-DARTS trên CIFAR-10 và sau đó đánh giá kiến trúc trên CIFAR-10 và
ImageNet.

4.1. Tìm kiếm Kiến trúc
CIFAR-10 là tập dữ liệu mà chúng tôi tìm kiếm kiến trúc tối ưu bằng SD-
DARTS. Có 50K hình ảnh huấn luyện và 10K hình ảnh kiểm tra trong CIFAR-
10,

c_{k-2}
0sep_conv_3x3
1skip_connect2dil_conv_3x3
3skip_connect
c_{k-1}sep_conv_3x3
sep_conv_3x3sep_conv_3x3
dil_conv_5x5
c_{k}
c_{k-2}0max_pool_3x3
1 max_pool_3x3
2max_pool_3x3
3avg_pool_3x3c_{k-1}sep_conv_5x5 skip_connect
skip_connect
c_{k}sep_conv_3x3
(a) Tế bào bình thường (b) Tế bào giảm kích thước
Hình 4: Các tế bào được tìm kiếm bởi SD-DARTS khi epochs cho khởi động ξ là 25 và cửa sổ thời gian K
là 2.

--- TRANG 17 ---
và mỗi hình ảnh có độ phân giải không gian là 32 ×32. Những hình ảnh này
được chia thành 10 lớp. Trong giai đoạn tìm kiếm của SD-DARTS, các hình
ảnh huấn luyện được chia thành hai tập con, tức là tập con hợp lệ để tối ưu
hóa các tham số kiến trúc và tập con huấn luyện để tối ưu hóa các tham số
mạng. Sau khi SD-DARTS tìm kiếm kiến trúc tối ưu, các hình ảnh huấn luyện
trong CIFAR-10 được sử dụng để huấn luyện kiến trúc tối ưu và sau đó đánh
giá hiệu suất của kiến trúc trên các hình ảnh kiểm tra.

Không gian tìm kiếm của phương pháp chúng tôi giống với DARTS, và không
gian tìm kiếm bao gồm 8 phép toán ứng viên, skip connect, max pool 3×3,
avgpool 3× 3, sepconv 3×3, sepconv 5×5, dilconv 3×3, dilconv 5×5, zero.
Siêu mạng được xếp chồng bởi 6 tế bào bình thường và 2 tế bào giảm kích
thước, các tế bào giảm kích thước được chèn vào 1/3 và 2/3 của tổng độ sâu
của siêu mạng, tương ứng. Stride của convolution trong tế bào giảm kích
thước là 2; do đó, tế bào giảm kích thước có thể giảm độ phân giải không
gian của các bản đồ đặc trưng. Tuy nhiên, stride của convolution trong tế
bào bình thường là 1, tức là kích thước không gian của bản đồ đặc trưng
không bị giảm. Phương pháp của chúng tôi nhắm đến việc tìm kiếm một tế
bào bình thường và một tế bào giảm kích thước để xây dựng kiến trúc tối ưu.

Bảng 1: Cài đặt Siêu tham số của SD-DARTS.
Siêu tham số Giá trị Ý nghĩa
ξ 25 epochs cho khởi động
λ 1.0 hệ số điều chuẩn
K 2 cửa sổ thời gian
E 50 tổng epochs cho tìm kiếm
m 64 kích thước batch cho tìm kiếm

Cài đặt huấn luyện của phương pháp chúng tôi cũng giống như DARTS.
Tổng epochs E của việc huấn luyện siêu mạng là 50, và kích thước batch của
việc huấn luyện là 64. Kích thước kênh ban đầu của siêu mạng là 16. SGD
được sử dụng để tối ưu hóa các tham số mạng w với tỷ lệ học ban đầu 0.025,
momentum 0.9 và weight decay 3 ×10−4. Adam được sử dụng để tối ưu hóa
các tham số kiến trúc với tỷ lệ học ban đầu 3 ×10−4, momentum (0.5, 0.999)
và weight decay 10−3. Chúng tôi khởi động siêu mạng trong 25 epochs, tức
là epochs cho khởi động ξ là 25. Sau đó, cửa sổ thời gian K được đặt là 2,
và điều này có nghĩa là chúng tôi sẽ chọn các siêu mạng tại hai bước thời
gian trước đó làm giáo viên để hướng dẫn việc huấn luyện của siêu mạng.
Hơn nữa, chúng tôi đặt hệ số điều chuẩn λ theo kinh nghiệm là 1.0 để cân
bằng mất mát phân loại và mất mát tương quan. Bảng 1 hiển thị những cài
đặt siêu tham số này của phương pháp chúng tôi.

--- TRANG 18 ---
Bảng 2: So sánh với các bộ phân loại hình ảnh tiên tiến trên CIFAR-10.
Kiến trúcLỗi Kiểm tra
(%)Params
(M)Chi phí Tìm kiếm
(GPU-ngày)Phương pháp
Tìm kiếm
DenseNet-BC [43] 3.46 25.6 - thủ công
NASNet-A [13] 2.65 3.3 1800 RL
AmoebaNet-A [11] 3.34 ±0.06 3.2 3150 evolution
AmoebaNet-B [11] 2.55 ±0.05 2.8 3150 evolution
PNAS [44] 3.41 ±0.09 3.2 225 SMBO
ENAS [15] 2.89 4.6 0.5 RL
DARTS (1storder) [41] 3.00 ±0.14 3.3 0.4 gradient
DARTS (2ndorder) [41] 2.76 ±0.09 3.3 1 gradient
SNAS (mild) [45] 2.98 2.9 1.5 gradient
ProxylessNAS [46] 2.08 - 4 gradient
P-DARTS [22] 2.5 3.4 0.3 gradient
PC-DARTS [18] 2.57 ±0.07 3.6 0.1 gradient
β-DARTS [47] 2.53 ±0.08 3.8 0.4 gradient
SDARTS-RS [21] 2.67 ±0.03 3.4 0.4 gradient
SDARTS-ADV [21] 2.61 ±0.02 3.3 1.3 gradient
GDAS [48] 2.93 3.4 0.3 gradient
R-DARTS (L2) [20] 2.95 ±0.21 - 1.6 gradient
SGAS (Cri 1. avg) [19] 2.66 ±0.24 3.7 0.25 gradient
DARTS-PT [24] 2.61 ±0.08 3.0 0.8 gradient
DARTS-[27] 2.59 ±0.08 3.5 0.4 gradient
U-DARTS [49] 2.59 ±0.06 3.3 4 gradient
DARTS-PAP [50] 2.51 3.9 0.4 gradient
SD-DARTS 2.58 ±0.11 3.3 0.37 gradient
SD-DARTS (best) 2.44 3.3 0.37 gradient

4.2. Đánh giá Kiến trúc trên CIFAR-10
Hình 4 hiển thị các tế bào được tìm kiếm bởi SD-DARTS, và chúng tôi xếp
chồng những tế bào này để xây dựng kiến trúc tối ưu. Trong tiểu mục này,
chúng tôi đánh giá hiệu suất của kiến trúc tối ưu trên CIFAR-10. Cài đặt
huấn luyện của phương pháp chúng tôi cũng giống như DARTS. Chúng tôi
xếp chồng 20 tế bào để xây dựng một mạng với kích thước kênh ban đầu là
36, nơi hai tế bào giảm kích thước được đặt vào 1/3 và 2/3 của tổng độ sâu
của mạng. Chúng tôi huấn luyện mạng trong 600 epochs với kích thước batch
96. SGD được sử dụng để tối ưu hóa mạng với tỷ lệ học ban đầu 0.025. Hơn
nữa, chúng tôi cũng sử dụng tăng cường dữ liệu để điều chuẩn việc huấn
luyện mạng, bao gồm cutout và tháp phụ trợ; độ dài cutout là 16, trọng số
của tháp phụ trợ là 0.4, và xác suất của path dropout là 0.3.

Bảng 2 hiển thị hiệu suất của kiến trúc được tìm kiếm bởi phương pháp
chúng tôi trên CIFAR-10. Kiến trúc được tìm kiếm bởi phương pháp chúng
tôi đạt được lỗi kiểm tra trung bình 2.58% trên CIFAR-10, và nó có thể đạt
được lỗi kiểm tra tốt nhất 2.44% khi đặt hạt giống ngẫu nhiên phù hợp để
huấn luyện kiến trúc. Hiệu suất của phương pháp chúng tôi vượt qua DARTS
và các biến thể của nó (ví dụ: DARTS-PT, SGAS, GDAS) rất nhiều. Nói
cách khác, phương pháp của chúng tôi đạt được hiệu suất tiên tiến (SOTA).
Độ lớn của các tham số kiến trúc của chúng tôi là 3.3M, và nó bằng với kiến
trúc được tìm kiếm bởi DARTS và ít hơn các biến thể khác.

Hơn nữa, chi phí tìm kiếm của phương pháp chúng tôi là 0.37 GPU ngày,
và nó tiêu tốn ít GPU ngày hơn so với DARTS. Tóm lại, với tài nguyên hạn
chế, phương pháp của chúng tôi có thể tìm ra kiến trúc với hiệu suất SOTA.

--- TRANG 19 ---
Bảng 3: So sánh với các bộ phân loại tiên tiến trên ImageNet.
Kiến trúcLỗi Kiểm tra(%) Params
(M)×+
(M)Chi phí Tìm kiếm
(GPU-ngày)Phương pháp
Tìm kiếm top-1 top-5
Inception-v1 [51] 30.2 10.1 6.6 1448 - thủ công
MobileNet [52] 29.4 10.5 4.2 569 - thủ công
ShuffleNet 2x (v1) [53] 26.4 10.2 ∼5 524 - thủ công
ShuffleNet 2x (v2) [54] 25.1 - ∼5 591 - thủ công
NASNet-A [13] 26 8.4 5.3 564 1800 RL
NASNet-B [13] 27.2 8.7 5.3 488 1800 RL
NASNet-C [13] 27.5 9 4.9 558 1800 RL
AKD [55] 27.9 8.3 - - - RL
AmoebaNet-A [11] 25.5 8 5.1 555 3150 evolution
AmoebaNet-B [11] 26 8.5 5.3 555 3150 evolution
AmoebaNet-C [11] 24.3 7.6 6.4 570 3150 evolution
FairNAS-A [56] 24.7 7.6 4.6 388 12 evolution
PNAS [44] 25.8 8.1 5.1 588 225 SMBO
MnasNet-92 [57] 25.2 8 4.4 388 - RL
DARTS(2ndorder) [41] 26.7 8.7 4.7 574 4.0 gradient
SNAS (mild) [45] 27.3 9.2 4.3 522 1.5 gradient
ProxylessNAS [46] 24.9 7.5 7.1 465 8.3 gradient
P-DARTS [22] 24.4 7.4 4.9 557 0.3 gradient
PC-DARTS [18] 25.1 7.8 5.3 586 0.1 gradient
β-DARTS [47] 24.2 7.1 5.5 609 0.4 gradient
SGAS (Cri.1 avg.) [19] 24.41 7.29 5.3 579 0.25 gradient
GDAS [48] 26.0 8.5 5.3 581 0.21 gradient
SDARTS-RS [21] 25.6 8.2 - - - gradient
SDARTS-ADV [21] 25.2 7.8 - - - gradient
DARTS-PT [24] 25.5 8 4.6 - 0.8 gradient
U-DARTS [49] 26.1 8.1 4.9 582 3 gradient
DARTS-PAP [50] 25.41 8.04 5.4 629 0.4 gradient
SD-DARTS 25.0 7.62 4.7 534 0.37 gradient

4.3. Đánh giá Kiến trúc trên ImageNet
Trong tiểu mục này, chúng tôi sử dụng ILSVRC2012 [58] để đánh giá khả
năng chuyển giao của phương pháp chúng tôi. ILSVRC2012 là một phiên
bản nhẹ của ImageNet với 1,000 lớp. Nó chứa 1.28M hình ảnh huấn luyện
và 50K hình ảnh hợp lệ, và mỗi hình ảnh có độ phân giải không gian là
224 ×224.

Hình 4 hiển thị các tế bào được tìm kiếm bởi phương pháp chúng tôi trên
CIFAR-10. Chúng tôi xếp chồng các tế bào để xây dựng một mạng và đánh
giá hiệu suất của mạng trên ImageNet. Cài đặt huấn luyện của phương pháp
chúng tôi trên ImageNet giống như DARTS. Chúng tôi sử dụng 12 tế bào
bình thường và 2 tế bào giảm kích thước để xây dựng mạng, và các tế bào
giảm kích thước được chèn vào 1/3 và 2/3 của tổng độ sâu của mạng. Kích
thước kênh ban đầu của mạng là 48. Chúng tôi huấn luyện mạng trong 250
epochs với kích thước batch 1024 trên 3 ×NVIDIA GeForce GTX 3090 GPUs.
Chúng tôi sử dụng SGD để tối ưu hóa mạng với tỷ lệ học ban đầu 0.5,
momentum 0.9, và weight decay 3 ×10−5. Hơn nữa, chúng tôi sử dụng một
tháp mất mát phụ trợ để giúp huấn luyện mạng của chúng tôi với trọng số
phụ trợ 0.4. Chúng tôi dành ba GPU ngày để huấn luyện mạng trên 3 ×NVIDIA
GeForce GTX 3090 GPUs.

Bảng 3 chứng minh kết quả đánh giá của mạng chúng tôi trên tập dữ liệu
ImageNet, nơi chúng tôi đạt được tỷ lệ lỗi kiểm tra 25.0%. Hiệu suất này
chỉ ra rằng khả năng chuyển giao của phương pháp chúng tôi vượt trội so
với DARTS và một số biến thể của nó. Đáng chú ý là một số biến thể DARTS
thể hiện lỗi kiểm tra thấp hơn so với phương pháp của chúng tôi. Tuy nhiên,
kiến trúc của chúng tôi có ít tham số hơn so với hầu hết các biến thể của nó.
Mặc dù có thể có các phương pháp cơ sở với kết quả tốt hơn, điều quan
trọng cần xem xét là mục tiêu của chúng có thể khác với chúng tôi. Ví dụ,
các phương pháp như SGAS [19] và P-DARTS [22] giải quyết việc chia sẻ
trọng số và khoảng cách độ sâu giữa các tình huống tìm kiếm và đánh giá,
tương ứng. Ngược lại, trọng tâm của chúng tôi là làm phẳng địa hình mất
mát của siêu mạng. Hơn nữa, phương pháp của chúng tôi vượt trội hơn các
công trình đồng thời của chúng tôi về cả hiệu suất và hiệu quả chi phí tìm
kiếm. Những kết quả này chứng minh giá trị và hiệu quả của phương pháp
chúng tôi.

Mặc dù kết quả thí nghiệm của chúng tôi trên tập dữ liệu ImageNet có thể
không mạnh như mong muốn, chúng tôi tin rằng chúng vẫn cung cấp những
hiểu biết có giá trị về hiệu suất của phương pháp chúng tôi. Các công trình
trước đó như SDARTS [21] và SAM [37] cũng đã chứng minh rằng giá trị
riêng chiếm ưu thế của chuẩn Hessian là một chỉ số đáng tin cậy để đặc
trưng cho độ sắc nhọn của địa hình mất mát. Quan sát rằng giá trị riêng
chiếm ưu thế của chuẩn Hessian của siêu mạng trong phương pháp chúng
tôi nhỏ hơn so với DARTS tiêu chuẩn, cũng như phân tích cho thấy rằng tự
chưng cất có thể làm phẳng địa hình mất mát, cung cấp bằng chứng mạnh
mẽ rằng thuật toán đề xuất của chúng tôi giảm hiệu quả khoảng cách rời
rạc hóa. Kết hợp những mảnh bằng chứng này, chúng tôi có thể tự tin khẳng
định rằng phương pháp của chúng tôi thành công trong việc giảm thiểu khoảng
cách rời rạc hóa và làm phẳng địa hình mất mát của siêu mạng. Các công
trình trước đó như R-DARTS [20] và SDARTS [21] đã chỉ ra rằng mức độ
sắc nhọn cao trong địa hình mất mát của siêu mạng có thể có tác động có
hại đến hiệu suất của DARTS. Những công trình này đã đề xuất các kỹ thuật
như dừng sớm và nhiễu loạn tham số để giải quyết vấn đề này. Tuy nhiên,
dừng sớm có thể hạn chế việc khám phá các kiến trúc với hiệu suất tốt hơn,
và việc tạo ra nhiễu loạn tham số có thể tốn thời gian. So sánh, phương
pháp của chúng tôi hiệu quả hơn vì chúng tôi chỉ cần bảo tồn các logits từ
các bước huấn luyện trước đó. Phương pháp này cho phép chúng tôi tận dụng
kiến thức tích lũy trong các bước trước đó mà không cần nhiễu loạn tham số
đắt đỏ. Bằng cách tận dụng tự chưng cất và giáo viên bỏ phiếu, phương
pháp của chúng tôi làm phẳng hiệu quả địa hình mất mát của siêu mạng và
đạt được hiệu suất tốt hơn một cách hiệu quả hơn so với các công trình
trước đó như SDARTS.

--- TRANG 20 ---
Bảng 4: So sánh hiệu suất trên benchmark NAS-Bench-201 [59]. Lưu ý rằng SD-
DARTS chỉ tìm kiếm trên tập dữ liệu CIFAR-10, nhưng có thể đạt được SOTA mới trên CIFAR-10,
CIFAR-100 và ImageNet16-120.
Phương phápCIFAR-10 CIFAR-100 ImageNet16-120
valid test valid test valid test
DARTS(1st) [41] 39.77 ±0.00 54.30 ±0.00 15.03 ±0.00 15.61 ±0.00 16.43 ±0.00 16.32 ±0.00
DARTS(2nd) [41] 39.77 ±0.00 54.30 ±0.00 15.03 ±0.00 15.61 ±0.00 16.43 ±0.00 16.32 ±0.00
GDAS [48] 89.89 ±0.08 93.61 ±0.09 71.34 ±0.04 70.70 ±0.30 41.59 ±1.33 41.71 ±0.98
SNAS [45] 90.10 ±1.04 92.77 ±0.83 69.69 ±2.39 69.34 ±1.98 42.84 ±1.79 43.16 ±2.64
DSNAS [60] 89.66 ±0.29 93.08 ±0.13 30.87 ±16.40 31.01 ±16.38 40.61 ±0.09 41.07 ±0.09
PC-DARTS [18] 89.96 ±0.15 93.41 ±0.30 67.12 ±0.39 67.48 ±0.89 40.83 ±0.08 41.31 ±0.22
iDARTS [61] 89.86 ±0.60 93.58 ±0.32 70.57 ±0.24 70.83 ±0.48 40.38 ±0.59 40.89 ±0.68
DARTS- [62] 91.03 ±0.44 93.80 ±0.40 71.36±1.51 71.53±1.51 44.87 ±1.46 45.12 ±0.82
SD-DARTS 91.21 ±0.11 93.91 ±0.08 70.87±0.33 71.88±0.76 45.75 ±0.26 46.03 ±0.59

4.4. Kết quả trên Không gian Tìm kiếm NAS-Bench-201
NAS-Bench-201 [59] là benchmark NAS được sử dụng rộng rãi nhất. NAS-
Bench-201 chứa 4 nút nội bộ với 5 phép toán ứng viên. Không gian tìm kiếm
bao gồm 15,625 kiến trúc, và hiệu suất thực tế của mỗi kiến trúc trên CIFAR-
10, CIFAR-100 và ImageNet16-120 được cung cấp. Các cài đặt tìm kiếm của
phương pháp chúng tôi giống như DARTS trên NAS-Bench-201.

Bảng 4 hiển thị kết quả so sánh trên NAS-Bench-201. Chúng tôi tìm kiếm
trên CIFAR-10 và sử dụng genotype tìm được để truy vấn hiệu suất của các
tập dữ liệu khác nhau. Quan sát thấy rằng phương pháp của chúng tôi đạt
được kết quả cạnh tranh trên CIFAR-10, CIFAR-100, và ImageNet16-120
khi so sánh với các phương pháp NAS khác. Mặc dù hiệu suất CIFAR-100-
valid của phương pháp chúng tôi hơi thấp hơn DARTS- [27], nó vượt trội
hơn các phương pháp khác như PC-DARTS [18] và iDARTS [61] về hiệu suất
tổng thể. Những kết quả này chỉ ra rằng SD-DARTS có khả năng khám phá
các kiến trúc với hiệu suất cạnh tranh trên các tập dữ liệu khác nhau. Mặc
dù nó có thể không đạt được trạng thái tiên tiến tuyệt đối trên mọi tập dữ
liệu riêng lẻ, nó cung cấp một sự đánh đổi mạnh mẽ giữa hiệu quả tìm kiếm
và hiệu suất. Những kết quả này làm nổi bật giá trị của phương pháp chúng
tôi trong việc tìm ra các kiến trúc chất lượng cao cho một loạt ứng dụng rộng
rãi.

--- TRANG 21 ---
0 5 10 15 20 25 30 35 40
Epochs của Khởi động96.296.496.696.897.097.297.4Độ chính xác
Hình 5: Tác động của khởi động : độ chính xác của các kiến trúc được khám phá bởi SD-DARTS trong
các epochs khởi động khác nhau trên CIFAR-10. Độ chính xác tăng dần khi các epochs khởi động
tăng cho đến 25.

5. Nghiên cứu Loại bỏ
5.1. Tác động của Khởi động
Bởi vì các tham số kiến trúc và mạng được khởi tạo ngẫu nhiên, chúng tôi
cần khởi động siêu mạng. Sau khi khởi động, siêu mạng học đủ kiến thức
từ dữ liệu huấn luyện để trở thành một giáo viên đủ tiêu chuẩn. Trong tiểu
mục này, chúng tôi thực hiện một nghiên cứu loại bỏ về tầm quan trọng của
khởi động.

--- TRANG 22 ---
Chúng tôi đặt cửa sổ thời gian K là 1 trong thí nghiệm, tức là chúng tôi sử
dụng siêu mạng tại bước thời gian trước đó làm giáo viên để hướng dẫn việc
huấn luyện của siêu mạng. Mục đích của chúng tôi khi chọn một giáo viên
duy nhất là bỏ qua tác động của nhiều giáo viên. Chúng tôi chạy phương
pháp của mình nhiều lần với các epochs khác nhau cho khởi động. Sau đó,
chúng tôi đánh giá hiệu suất của kiến trúc được khám phá bởi các thí nghiệm
của chúng tôi.

Hình 5 hiển thị xu hướng độ chính xác của các kiến trúc được khám phá bởi
SD-DARTS trong các khởi động khác nhau. Chúng tôi thấy rằng độ chính
xác tăng dần khi các epochs khởi động tăng cho đến 25. Khi các epochs khởi
động không đủ, siêu mạng không thể học đủ kiến thức để trở thành một giáo
viên đủ tiêu chuẩn. Nó sẽ hướng dẫn sai việc huấn luyện siêu mạng và tạo
ra kiến trúc với hiệu suất kém. Khi các epochs khởi động tăng từ 25 epochs,
độ chính xác giảm dần. Điều này là do siêu mạng được tối ưu hóa quá mức
khi các epochs huấn luyện tăng, và siêu mạng đóng vai trò tiêu cực khi nó
được lấy làm giáo viên. Do đó, khởi động đóng vai trò thiết yếu trong phương
pháp của chúng tôi, và chúng tôi nên khởi động siêu mạng trong số epochs
thích hợp.

5.2. Tác động của Giáo viên Bỏ phiếu
Trong tiểu mục này, tác động của giáo viên bỏ phiếu được phân tích bằng
cách xem xét các cửa sổ thời gian K khác nhau, nơi K đại diện cho số lượng
giáo viên bỏ phiếu. Các epochs khởi động được đặt là 25, và kiến trúc tối
ưu được tìm kiếm bằng cách sử dụng một số siêu mạng từ K bước thời gian
trước đó làm giáo viên để hướng dẫn việc huấn luyện tự chưng cất của siêu
mạng. Hiệu suất của những kiến trúc này sau đó được đánh giá. Mục đích
của phân tích này là hiểu tác động của số lượng giáo viên bỏ phiếu đối với
hiệu suất của quá trình tìm kiếm. Bằng cách thay đổi giá trị của K, các lượng
kiến thức và thông tin khác nhau từ các siêu mạng trước đó được tích hợp
để hướng dẫn việc huấn luyện. Điều này cho phép khám phá toàn diện tác
động của việc sử dụng nhiều giáo viên trong quá trình tự chưng cất.

Hình 6 minh họa xu hướng độ chính xác trong các cửa sổ thời gian khác
nhau khi sử dụng phương pháp giáo viên bỏ phiếu. Nó cho thấy rằng hiệu
suất của các kiến trúc được tìm kiếm thay đổi theo kích thước của cửa sổ
thời gian. Thú vị, hiệu suất tốt nhất đạt được khi cửa sổ thời gian được đặt
là 2, chỉ ra rằng việc sử dụng các siêu mạng từ hai bước thời gian trước đó
làm giáo viên cung cấp hướng dẫn hiệu quả nhất cho việc huấn luyện tự
chưng cất của siêu mạng. Khi kích thước của cửa sổ thời gian tăng vượt quá
2, hiệu suất của các kiến trúc bắt đầu giảm. Điều này có thể được quy cho
thực tế là thông tin từ các siêu mạng trong các bước thời gian sớm hơn trở
nên ít liên quan hơn

--- TRANG 23 ---
1 2 3 4 5 6
Kích thước Cửa sổ Thời gian97.197.297.397.497.5Độ chính xác
Hình 6: Tác động của giáo viên bỏ phiếu : độ chính xác của các kiến trúc được khám phá bởi
SD-DARTS trong các cửa sổ thời gian khác nhau trên CIFAR-10 khi các epochs khởi động là 25.
Khi kích thước cửa sổ thời gian là 2, nó có thể đạt được độ chính xác cao nhất. Tuy nhiên, khi
kích thước cửa sổ thời gian trở nên lớn hơn, độ chính xác giảm chậm.

và tương quan với siêu mạng tại bước thời gian hiện tại. Do đó, việc sử dụng
các siêu mạng từ những bước thời gian sớm hơn này làm giáo viên bỏ phiếu
có thể giới thiệu thông tin sai lầm và cản trở hiệu quả của việc huấn luyện
tự chưng cất. Quan sát này làm nổi bật sự đánh đổi giữa số lượng giáo viên
bỏ phiếu và tương quan giữa các giáo viên và học sinh. Nghiên cứu tương lai
có thể tập trung vào việc tìm ra sự cân bằng giữa những yếu tố này để tận
dụng đầy đủ thông tin từ các siêu mạng tại các bước thời gian khác nhau và
tối ưu hóa hướng dẫn được cung cấp bởi các giáo viên bỏ phiếu trong quá
trình tự chưng cất. Điều này sẽ cho phép một quá trình tìm kiếm kiến trúc
tối ưu hiệu quả và chính xác hơn.

6. Kết luận
Tóm lại, phương pháp SD-DARTS được đề xuất, kết hợp tự chưng cất và
giáo viên bỏ phiếu, nhắm đến việc cải thiện hiệu suất của DARTS bằng cách
giải quyết độ sắc nhọn của địa hình mất mát. Nó đạt được hiệu suất tiên tiến
trên các tập dữ liệu CIFAR-10 và ImageNet bằng cách giảm khoảng cách rời
rạc hóa giữa siêu mạng và mạng con tối ưu. Mặc dù SD-DARTS chứng minh
kết quả đầy hứa hẹn, nó cũng có hạn chế liên quan đến việc sử dụng thông
tin từ các siêu mạng tại các bước thời gian khác nhau để hướng dẫn việc

--- TRANG 24 ---
huấn luyện của siêu mạng. Trong nghiên cứu tương lai, sẽ rất có giá trị khi
khám phá các phương pháp thay thế, như sử dụng trung bình hàm mũ suy
giảm, để đạt được sự cân bằng tốt hơn giữa số lượng giáo viên bỏ phiếu và
tương quan giữa các giáo viên và học sinh. Điều này có thể nâng cao hiệu
suất và hiệu quả của SD-DARTS hơn nữa.

Lời cảm ơn
Công trình của Jian Li được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên
Trung Quốc (Số 62106257), Quỹ Khoa học Thậc sĩ sau đại học Trung Quốc
(Số 2023T160680), và Chương trình Nhân tài Xuất sắc của Viện Kỹ thuật
Thông tin, CAS. Công trình của Yong Liu được hỗ trợ một phần bởi Quỹ
Khoa học Tự nhiên Trung Quốc (Số 62076234), Chương trình Nhà khoa học
Trẻ Xuất sắc Bắc Kinh (Số BJJWZYJH012019100020098), Kế hoạch Hợp
tác Hệ sinh thái Đổi mới Unicom, và Quỹ CCF-Huawei Populus Grove.

Tài liệu tham khảo
[1]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
large-scale hierarchical image database, in: IEEE conference on computer
vision and pattern recognition (CVPR), 2009, pp. 248–255.
[2]A. Yilmaz, O. Javed, M. Shah, Object tracking: A survey, Acm comput-
ing surveys (CSUR) 38 (4) (2006) 13–es.
[3]M. van Heel, G. Harauz, E. V. Orlova, R. Schmidt, M. Schatz, A new
generation of the imagic image processing system, Journal of structural
biology 116 (1) (1996) 17–24.
[4]H. Li, D. Doermann, O. Kia, Automatic text detection and tracking
in digital video, IEEE transactions on image processing 9 (1) (2000)
147–156.
[5]K. Simonyan, A. Zisserman, Very deep convolutional networks for large-
scale image recognition, in: Y. Bengio, Y. LeCun (Eds.), International
Conference on Learning Representations (ICLR), 2015.

--- TRANG 25 ---
[6]K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image
recognition, in: Proceedings of the IEEE conference on computer vision
and pattern recognition (CVPR), 2016, pp. 770–778.
[7]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
 L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural
information processing systems (NeurIPS) 30 (2017).
[8]J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep
bidirectional transformers for language understanding, in: J. Burstein,
C. Doran, T. Solorio (Eds.), Proceedings of the Conference of the North
American Chapter of the Association for Computational Linguistics
(NAACL), 2019, pp. 4171–4186.
[9]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models
are few-shot learners, Advances in neural information processing systems
(NeurIPS) 33 (2020) 1877–1901.
[10]B. Zoph, Q. V. Le, Neural architecture search with reinforcement learning,
in: International Conference on Learning Representations (ICLR), 2017.
[11]E. Real, A. Aggarwal, Y. Huang, Q. V. Le, Regularized evolution for
image classifier architecture search, in: Proceedings of the aaai conference
on artificial intelligence (AAAI), Vol. 33, 2019, pp. 4780–4789.
[12]K. Ostad-Ali-Askari, M. Shayan, Subsurface drain spacing in the unsteady
conditions by hydrus-3d and artificial neural networks, Arabian Journal
of Geosciences 14 (2021) 1–14.
[13]B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le, Learning transferable
architectures for scalable image recognition, in: Proceedings of the IEEE
conference on computer vision and pattern recognition (CVPR), 2018,
pp. 8697–8710.
[14]K. Ostad-Ali-Askari, M. Shayannejad, H. Ghorbanizadeh-Kharazi, Arti-
ficial neural network for modeling nitrate pollution of groundwater in
marginal area of zayandeh-rood river, isfahan, iran, KSCE Journal of
Civil Engineering 21 (2017) 134–140.

--- TRANG 26 ---
[15]H. Pham, M. Guan, B. Zoph, Q. Le, J. Dean, Efficient neural architecture
search via parameters sharing, in: International conference on machine
learning (ICML), 2018, pp. 4095–4104.
[16]Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, J. Sun, Single path
one-shot neural architecture search with uniform sampling, in: A. Vedaldi,
H. Bischof, T. Brox, J. Frahm (Eds.), European Conference on Computer
Vision (ECCV), Vol. 12361, 2020, pp. 544–560.
[17]R. Luo, F. Tian, T. Qin, E. Chen, T.-Y. Liu, Neural architecture opti-
mization, Advances in neural information processing systems (NeurIPS)
31 (2018).
[18]Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, H. Xiong, PC-DARTS:
partial channel connections for memory-efficient architecture search, in:
International Conference on Learning Representations (ICLR), 2020.
[19]G. Li, G. Qian, I. C. Delgadillo, M. Muller, A. Thabet, B. Ghanem, Sgas:
Sequential greedy architecture search, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2020,
pp. 1620–1630.
[20]A. Zela, T. Elsken, T. Saikia, Y. Marrakchi, T. Brox, F. Hutter, Under-
standing and robustifying differentiable architecture search, in: Interna-
tional Conference on Learning Representations (ICLR), 2020.
[21]X. Chen, C.-J. Hsieh, Stabilizing differentiable architecture search via
perturbation-based regularization, in: International conference on ma-
chine learning (ICML), 2020, pp. 1554–1565.
[22]X. Chen, L. Xie, J. Wu, Q. Tian, Progressive DARTS: bridging the
optimization gap for NAS in the wild, Int. J. Comput. Vis. 129 (3) (2021)
638–655.
[23]H. Liang, S. Zhang, J. Sun, X. He, W. Huang, K. Zhuang, Z. Li, DARTS+:
improved differentiable architecture search with early stopping, CoRR
abs/1909.06035 (2019).
[24]R. Wang, M. Cheng, X. Chen, X. Tang, C. Hsieh, Rethinking architecture
selection in differentiable NAS, in: International Conference on Learning
Representations (ICLR), 2021.

--- TRANG 27 ---
[25]Y. Tian, C. Liu, L. Xie, Q. Ye, et al., Discretization-aware architecture
search, Pattern Recognition 120 (2021) 108186.
[26]K. Bi, L. Xie, X. Chen, L. Wei, Q. Tian, GOLD-NAS: gradual, one-level,
differentiable, CoRR abs/2007.03331 (2020).
[27]X. Chu, X. Wang, B. Zhang, S. Lu, X. Wei, J. Yan, DARTS-: robustly
stepping out of performance collapse without indicators, in: International
Conference on Learning Representations (ICLR), 2021.
[28]Y. Lin, C. Wang, C. Chang, H. Sun, An efficient framework for counting
pedestrians crossing a line using low-cost devices: the benefits of distilling
the knowledge in a neural network, Multim. Tools Appl. 80 (3) (2021)
4037–4051.
[29]L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, K. Ma, Be your own
teacher: Improve the performance of convolutional neural networks
via self distillation, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), 2019, pp. 3713–3722.
[30]S. Yun, J. Park, K. Lee, J. Shin, Regularizing class-wise predictions via
self-knowledge distillation, in: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition (CVPR), 2020, pp. 13876–
13885.
[31]A. Tarvainen, H. Valpola, Mean teachers are better role models: Weight-
averaged consistency targets improve semi-supervised deep learning re-
sults, Advances in neural information processing systems (NeurIPS) 30
(2017).
[32]K. Li, S. Wang, L. Yu, P. A. Heng, Dual-teacher++: Exploiting intra-
domain and inter-domain knowledge with reliable transfer for cardiac
segmentation, IEEE Transactions on Medical Imaging 40 (10) (2020)
2771–2782.
[33]Z. Zhao, F. Zhou, K. Xu, Z. Zeng, C. Guan, S. K. Zhou, Le-uda: Label-
efficient unsupervised domain adaptation for medical image segmentation,
IEEE Transactions on Medical Imaging 42 (3) (2022) 633–646.
[34]S. Li, Z. Zhao, K. Xu, Z. Zeng, C. Guan, Hierarchical consistency
regularized mean teacher for semi-supervised 3d left atrium segmentation,

--- TRANG 28 ---
in: International Conference of the IEEE Engineering in Medicine &
Biology Society (EMBC), IEEE, 2021, pp. 3395–3398.
[35]Z. Zhao, A. Zhu, Z. Zeng, B. Veeravalli, C. Guan, Act-net: Asymmetric
co-teacher network for semi-supervised memory-efficient medical image
segmentation, in: IEEE International Conference on Image Processing
(ICIP), 2022, pp. 1426–1430.
[36]H.-R. Wei, S. Huang, R. Wang, X. Dai, J. Chen, Online distilling
from checkpoints for neural machine translation, in: Proceedings of
the Conference of the North American Chapter of the Association for
Computational Linguistics (NAACL), 2019, pp. 1932–1941.
[37]P. Foret, A. Kleiner, H. Mobahi, B. Neyshabur, Sharpness-aware mini-
mization for efficiently improving generalization, in: International Con-
ference on Learning Representations (ICLR), 2021.
[38]J. Kwon, J. Kim, H. Park, I. K. Choi, Asam: Adaptive sharpness-aware
minimization for scale-invariant learning of deep neural networks, in:
Proceedings of the 38th International Conference on Machine Learning
(ICML), Vol. 139, 2021, pp. 5905–5914.
[39]Y. Zhao, H. Zhang, X. Hu, Penalizing gradient norm for efficiently
improving generalization in deep learning, in: International Conference
on Machine Learning (ICML), Vol. 162, 2022, pp. 26982–26992.
[40]J. Du, D. Zhou, J. Feng, V. Tan, J. T. Zhou, Sharpness-aware training
for free, in: NeurIPS, 2022.
[41]H. Liu, K. Simonyan, Y. Yang, DARTS: Differentiable architecture search,
in: International Conference on Learning Representations (ICLR), 2019.
[42]A. Krizhevsky, G. Hinton, et al., Learning multiple layers of features
from tiny images (2009).
[43]G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely
connected convolutional networks, in: Proceedings of the IEEE conference
on computer vision and pattern recognition (CVPR), 2017, pp. 4700–
4708.

--- TRANG 29 ---
[44]C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
A. Yuille, J. Huang, K. Murphy, Progressive neural architecture search,
in: Proceedings of the European conference on computer vision (ECCV),
2018, pp. 19–34.
[45]S. Xie, H. Zheng, C. Liu, L. Lin, SNAS: stochastic neural architecture
search, in: International Conference on Learning Representations (ICLR),
2019.
[46]H. Cai, L. Zhu, S. Han, Proxylessnas: Direct neural architecture search
on target task and hardware, in: International Conference on Learning
Representations (ICLR), 2019.
[47]P. Ye, B. Li, Y. Li, T. Chen, J. Fan, W. Ouyang, β-darts: Beta-decay
regularization for differentiable architecture search, in: IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR), 2022, pp.
10864–10873.
[48]X. Dong, Y. Yang, Searching for a robust neural architecture in four
gpu hours, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), 2019, pp. 1761–1770.
[49]L. Huang, S. Sun, J. Zeng, W. Wang, W. Pang, K. Wang, U-DARTS:
uniform-space differentiable architecture search, Inf. Sci. 628 (2023) 339–
349.
[50]Y. Li, S. Li, Z. Yu, DARTS-PAP: differentiable neural architecture search
by polarization of instance complexity weighted architecture parameters,
in: MultiMedia Modeling (MMM), Vol. 13834, 2023, pp. 277–288.
[51]C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-
han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in:
Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR), 2015, pp. 1–9.
[52]A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,
M. Andreetto, H. Adam, Mobilenets: Efficient convolutional neural
networks for mobile vision applications, CoRR abs/1704.04861 (2017).
[53]X. Zhang, X. Zhou, M. Lin, J. Sun, Shufflenet: An extremely efficient
convolutional neural network for mobile devices, in: Proceedings of the

--- TRANG 30 ---
IEEE conference on computer vision and pattern recognition (CVPR),
2018, pp. 6848–6856.
[54]N. Ma, X. Zhang, H.-T. Zheng, J. Sun, Shufflenet v2: Practical guidelines
for efficient cnn architecture design, in: Proceedings of the European
conference on computer vision (ECCV), 2018, pp. 116–131.
[55]Y. Liu, X. Jia, M. Tan, R. Vemulapalli, Y. Zhu, B. Green, X. Wang,
Search to distill: Pearls are everywhere but not the eyes, in: 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR, 2020, pp. 7536–7545.
[56]X. Chu, B. Zhang, R. Xu, Fairnas: Rethinking evaluation fairness
of weight sharing neural architecture search, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV), 2021,
pp. 12239–12248.
[57]M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, Q. V.
Le, Mnasnet: Platform-aware neural architecture search for mobile, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2019, pp. 2820–2828.
[58]O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual
recognition challenge, International journal of computer vision 115 (3)
(2015) 211–252.
[59]X. Dong, Y. Yang, Nas-bench-201: Extending the scope of reproducible
neural architecture search, in: International Conference on Learning
Representations (ICLR), 2020.
[60]S. Hu, S. Xie, H. Zheng, C. Liu, J. Shi, X. Liu, D. Lin, Dsnas: Direct
neural architecture search without parameter retraining, in: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2020.
[61]M. Zhang, S. W. Su, S. Pan, X. Chang, E. M. Abbasnejad, R. Haffari,
idarts: Differentiable architecture search with stochastic implicit gradi-
ents, in: Proceedings of the 38th International Conference on Machine
Learning (ICML), Vol. 139, 2021, pp. 12557–12566.

--- TRANG 31 ---
[62]X. Chu, X. Wang, B. Zhang, S. Lu, X. Wei, J. Yan, {DARTS }-: Robustly
stepping out of performance collapse without indicators, in: International
Conference on Learning Representations (ICLR), 2021.

--- TRANG 32 ---
