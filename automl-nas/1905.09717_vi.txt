# 1905.09717.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/automl-nas/1905.09717.pdf
# Kích thước tệp: 631395 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Tỉa Mạng thông qua
Tìm kiếm Kiến trúc Có thể Biến đổi
Xuanyi Dongyz, Yi Yangy
yPhòng thí nghiệm ReLER, Đại học Công nghệ Sydney,zBaidu Research
xuanyi.dong@student.uts.edu.au; yi.yang@uts.edu.au
Tóm tắt
Tỉa mạng giảm chi phí tính toán của một mạng quá tham số hóa mà không làm hỏng hiệu suất. Các thuật toán tỉa phổ biến xác định trước chiều rộng và độ sâu của các mạng được tỉa, sau đó chuyển tham số từ mạng chưa tỉa sang mạng đã tỉa. Để phá vỡ giới hạn cấu trúc của các mạng được tỉa, chúng tôi đề xuất áp dụng tìm kiếm kiến trúc mạng nơ-ron để tìm kiếm trực tiếp một mạng với kích thước kênh và lớp linh hoạt. Số lượng các kênh/lớp được học bằng cách tối thiểu hóa mất mát của các mạng được tỉa. Bản đồ đặc trưng của mạng được tỉa là một tập hợp của K mảnh bản đồ đặc trưng (được tạo bởi K mạng có kích thước khác nhau), được lấy mẫu dựa trên phân phối xác suất. Mất mát có thể được lan truyền ngược không chỉ đến trọng số mạng, mà còn đến phân phối tham số hóa để điều chỉnh rõ ràng kích thước của các kênh/lớp. Cụ thể, chúng tôi áp dụng nội suy theo kênh để giữ cho bản đồ đặc trưng với các kích thước kênh khác nhau được căn chỉnh trong quá trình tập hợp. Xác suất tối đa cho kích thước trong mỗi phân phối phục vụ như chiều rộng và độ sâu của mạng được tỉa, có tham số được học bằng chuyển giao kiến thức, ví dụ, chưng cất kiến thức, từ các mạng ban đầu. Các thí nghiệm trên CIFAR-10, CIFAR-100 và ImageNet chứng minh hiệu quả của góc nhìn mới về tỉa mạng so với các thuật toán tỉa mạng truyền thống. Các phương pháp tìm kiếm và chuyển giao kiến thức khác nhau được tiến hành để cho thấy hiệu quả của hai thành phần. Mã có tại: https://github.com/D-X-Y/NAS-Projects .

1 Giới thiệu
Các mạng nơ-ron tích chập sâu (CNN) đã trở nên rộng hơn và sâu hơn để đạt được hiệu suất cao trên các ứng dụng khác nhau [17,22,48]. Mặc dù có thành công to lớn, việc triển khai chúng lên các thiết bị có tài nguyên hạn chế, như thiết bị di động và máy bay không người lái, là không thực tế.

Huấn luyện một CNN lớn T -> Tỉa bộ lọc, nhận được một CNN nhỏ S -> Tinh chỉnh CNN S -> Một CNN hiệu quả S
(a) Mô hình Tỉa Truyền thống

Huấn luyện một CNN lớn T -> Tìm kiếm chiều rộng và độ sâu của CNN S -> Chuyển giao kiến thức từ T sang S -> Một CNN hiệu quả S
(b) Mô hình Tỉa Được Đề xuất

Hình 1: So sánh giữa mô hình tỉa điển hình và mô hình được đề xuất.

Một giải pháp đơn giản để giải quyết vấn đề này là sử dụng tỉa mạng [29,12,13,20,18] để giảm chi phí tính toán của các CNN quá tham số hóa. Một quy trình điển hình cho tỉa mạng, như được chỉ ra trong Hình 1(a), được thực hiện bằng cách loại bỏ các bộ lọc dư thừa và sau đó tinh chỉnh các mạng đã cắt, dựa trên các mạng ban đầu. Các tiêu chí khác nhau về tầm quan trọng của các bộ lọc được áp dụng, chẳng hạn như chuẩn L2 của bộ lọc [30], lỗi tái tạo [20], và hệ số tỷ lệ có thể học [32]. Cuối cùng, các nhà nghiên cứu áp dụng các chiến lược tinh chỉnh khác nhau [30,18] cho mạng được tỉa để chuyển giao hiệu quả các tham số của các mạng chưa tỉa và tối đa hóa hiệu suất của các mạng được tỉa.

Công việc này được thực hiện khi Xuanyi Dong là thực tập sinh nghiên cứu tại Baidu Research.
Hội nghị lần thứ 33 về Hệ thống Xử lý Thông tin Nơ-ron (NeurIPS 2019), Vancouver, Canada.
arXiv:1905.09717v5 [cs.CV] 16 Oct 2019

--- TRANG 2 ---
Các phương pháp tỉa mạng truyền thống đạt được tác động hiệu quả trong việc nén mạng trong khi duy trì độ chính xác. Cấu trúc mạng của chúng được thiết kế một cách trực quan, ví dụ, tỉa 30% bộ lọc trong mỗi lớp [30,18], dự đoán tỷ lệ thưa [15] hoặc tận dụng điều chuẩn hóa [2]. Độ chính xác của mạng được tỉa bị giới hạn trên bởi các cấu trúc thiết kế thủ công hoặc các quy tắc cho cấu trúc. Để phá vỡ giới hạn này, chúng tôi áp dụng Tìm kiếm Kiến trúc Nơ-ron (NAS) để biến việc thiết kế cấu trúc kiến trúc thành một quy trình học tập và đề xuất một mô hình mới cho tỉa mạng như được giải thích trong Hình 1(b).

Các phương pháp NAS phổ biến [31,48,8,4,40] tối ưu hóa tôpô mạng, trong khi trọng tâm của bài báo này là kích thước mạng tự động. Để đáp ứng các yêu cầu và thực hiện so sánh công bằng giữa các chiến lược tỉa trước đây, chúng tôi đề xuất một sơ đồ NAS mới được gọi là Tìm kiếm Kiến trúc Có thể Biến đổi (TAS). TAS nhằm tìm kiếm kích thước tốt nhất của một mạng thay vì tôpô, được điều chuẩn bằng tối thiểu hóa chi phí tính toán, ví dụ, các phép toán dấu phẩy động (FLOP). Các tham số của các mạng được tìm kiếm/tỉa sau đó được học bằng chuyển giao kiến thức [21, 44, 46].

TAS là một thuật toán tìm kiếm có thể vi phân, có thể tìm kiếm chiều rộng và độ sâu của các mạng một cách hiệu quả và có hiệu suất. Cụ thể, các ứng viên khác nhau của kênh/lớp được gắn với một xác suất có thể học. Phân phối xác suất được học bằng cách lan truyền ngược mất mát được tạo bởi các mạng được tỉa, có bản đồ đặc trưng là một tập hợp của K mảnh bản đồ đặc trưng (đầu ra của các mạng có kích thước khác nhau) được lấy mẫu dựa trên phân phối xác suất. Các bản đồ đặc trưng này có kích thước kênh khác nhau được tập hợp với sự trợ giúp của nội suy theo kênh. Xác suất tối đa cho kích thước trong mỗi phân phối phục vụ như chiều rộng và độ sâu của mạng được tỉa.

Trong các thí nghiệm, chúng tôi cho thấy rằng kiến trúc được tìm kiếm với các tham số được chuyển giao bằng chưng cất kiến thức (KD) vượt trội hơn các phương pháp tỉa tiên tiến trước đây trên CIFAR-10, CIFAR-100 và ImageNet. Chúng tôi cũng thử nghiệm các phương pháp chuyển giao kiến thức khác nhau trên các kiến trúc được tạo bởi các phương pháp tỉa thiết kế thủ công truyền thống [30,18] và phương pháp tìm kiếm kiến trúc ngẫu nhiên [31]. Những cải thiện nhất quán trên các kiến trúc khác nhau chứng minh tính tổng quát của chuyển giao kiến thức.

2 Các Nghiên cứu Liên quan
Tỉa mạng [29,33] là một kỹ thuật hiệu quả để nén và tăng tốc CNN, và do đó cho phép chúng ta triển khai các mạng hiệu quả trên các thiết bị phần cứng có tài nguyên lưu trữ và tính toán hạn chế. Nhiều kỹ thuật đã được đề xuất, chẳng hạn như phân tích ma trận thứ hạng thấp [47], tỉa trọng số [14, 29,13,12], tỉa kênh [18,33], tính toán động [9,7] và lượng tử hóa [23,1]. Chúng nằm trong hai phương thức: tỉa không có cấu trúc [29, 9, 7, 12] và tỉa có cấu trúc [30, 20, 18, 33].

Các phương pháp tỉa không có cấu trúc [29,9,7,12] thường ép buộc các trọng số tích chập [29,14] hoặc bản đồ đặc trưng [7,9] trở nên thưa. Các tiên phong của tỉa không có cấu trúc, LeCun et al. [29] và Hassibi et al. [14], đã nghiên cứu việc sử dụng thông tin đạo hàm bậc hai để tỉa trọng số của các CNN nông. Sau khi mạng sâu ra đời vào năm 2012 [28], Han et al. [12,13,11] đã đề xuất một loạt các công trình để có được các CNN sâu được nén cao dựa trên điều chuẩn hóa L2. Sau sự phát triển này, nhiều nhà nghiên cứu đã khám phá các kỹ thuật điều chuẩn hóa khác nhau để cải thiện độ thưa trong khi bảo toàn độ chính xác, chẳng hạn như điều chuẩn hóa L0 [35] và độ nhạy đầu ra [41]. Vì các phương pháp không có cấu trúc này làm cho một mạng lớn trở nên thưa thay vì thay đổi toàn bộ cấu trúc của mạng, chúng cần thiết kế chuyên dụng cho các phụ thuộc [11] và phần cứng cụ thể để tăng tốc quy trình suy luận.

Các phương pháp tỉa có cấu trúc [30,20,18,33] nhắm đến việc tỉa các bộ lọc tích chập hoặc toàn bộ các lớp, và do đó các mạng được tỉa có thể được phát triển và áp dụng dễ dàng. Các công trình đầu tiên trong lĩnh vực này [2,42] đã tận dụng Lasso nhóm để cho phép độ thưa có cấu trúc của các mạng sâu. Sau đó, Li et al. [30] đã đề xuất mô hình tỉa ba giai đoạn điển hình (huấn luyện một mạng lớn, tỉa, huấn luyện lại). Các thuật toán tỉa này coi các bộ lọc có chuẩn nhỏ là không quan trọng và có xu hướng tỉa chúng, nhưng giả định này không đúng trong các mạng phi tuyến sâu [43]. Do đó, nhiều nhà nghiên cứu tập trung vào tiêu chí tốt hơn cho các bộ lọc thông tin. Ví dụ, Liu et al. [32] đã tận dụng điều chuẩn hóa L1; Ye et al. [43] đã áp dụng phạt ISTA; và He et al. [19] đã sử dụng tiêu chí dựa trên trung vị hình học.

Trái ngược với các quy trình tỉa trước đây, phương pháp của chúng tôi cho phép số lượng kênh/lớp được tối ưu hóa rõ ràng để cấu trúc được học có hiệu suất cao và chi phí thấp.

Bên cạnh các tiêu chí cho các bộ lọc thông tin, tầm quan trọng của cấu trúc mạng đã được đề xuất trong [33]. Một số phương pháp ngầm tìm một kiến trúc cụ thể cho dữ liệu [42,2,15], bằng cách tự động xác định

--- TRANG 3 ---
logit -> hình ảnh -> lấy mẫu hai lựa chọn kênh: 3 và 4 qua p1 -> CWI += -> đầu ra thực tế của conv-1 -> CNN chưa tỉa

CWI += -> CWI += -> ×p13 -> ×p14 -> ×p21 -> ×p23 -> ×p31 -> ×p32 -> p1=[p11,p12,p13,p14] -> lấy mẫu hai lựa chọn kênh: 1 và 3 qua p2 -> lấy mẫu hai lựa chọn kênh: 1 và 2 qua p3 -> kênh=4 bản đồ đặc trưng -> kênh=4 bản đồ đặc trưng -> kênh=4 bản đồ đặc trưng

xác suất C=1 2 3 4 p11 p12 p13 p14 -> C=1 2 3 4 p21 p22 p23 p24 xác suất -> C=1 2 3 4 p31 p32 p33 p34 xác suất -> phân phối xác suất của #kênh -> lớp thứ 1 -> lớp thứ 2 -> lớp thứ 3 -> CNN được tỉa - lớp thứ 1 -> CNN được tỉa - lớp thứ 2 -> CNN được tỉa - lớp thứ 3 -> CNN được tỉa - logit -> hình ảnh -> hình ảnh -> hình ảnh -> hình ảnh

Hình 2: Tìm kiếm chiều rộng của một CNN được tỉa từ một CNN ba lớp chưa tỉa. Mỗi lớp tích chập được trang bị một phân phối có thể học cho kích thước của các kênh trong lớp này, được chỉ ra bằng pi ở phía bên trái. Bản đồ đặc trưng cho mỗi lớp được xây dựng tuần tự bởi các lớp, như được hiển thị ở phía bên phải. Đối với một lớp cụ thể, K (2 trong ví dụ này) bản đồ đặc trưng có kích thước khác nhau được lấy mẫu theo phân phối tương ứng và kết hợp bằng nội suy theo kênh (CWI) và tổng có trọng số. Bản đồ đặc trưng tập hợp này được đưa làm đầu vào cho lớp tiếp theo.

tỷ lệ tỉa và nén của mỗi lớp. Ngược lại, chúng tôi khám phá rõ ràng kiến trúc bằng cách sử dụng NAS. Hầu hết các thuật toán NAS trước đây [48,8,31,40] tự động khám phá cấu trúc tôpô của một mạng nơ-ron, trong khi chúng tôi tập trung vào tìm kiếm độ sâu và chiều rộng của một mạng nơ-ron. Các phương pháp dựa trên học tăng cường (RL) [48,3] hoặc các phương pháp dựa trên thuật toán tiến hóa [40] có thể tìm kiếm các mạng với chiều rộng và độ sâu linh hoạt, tuy nhiên, chúng yêu cầu tài nguyên tính toán khổng lồ và không thể được sử dụng trực tiếp trên các tập dữ liệu mục tiêu quy mô lớn. Các phương pháp có thể vi phân [8,31,4] giảm đáng kể chi phí tính toán nhưng chúng thường giả định rằng số lượng kênh trong các ứng viên tìm kiếm khác nhau là giống nhau. TAS là một phương pháp NAS có thể vi phân, có thể tìm kiếm hiệu quả cho các mạng có thể biến đổi với chiều rộng và độ sâu linh hoạt.

Biến đổi mạng [5,10,3] cũng đã nghiên cứu độ sâu và chiều rộng của các mạng. Chen et al. [5] đã mở rộng và làm sâu thêm một mạng theo cách thủ công, và đề xuất Net2Net để khởi tạo mạng lớn hơn. Ariel et al. [10] đã đề xuất một chiến lược phỏng đoán để tìm chiều rộng phù hợp của các mạng bằng cách luân phiên giữa thu nhỏ và mở rộng. Cai et al. [3] đã sử dụng một tác nhân RL để phát triển độ sâu và chiều rộng của CNN, trong khi TAS của chúng tôi là một phương pháp có thể vi phân và có thể không chỉ mở rộng mà còn thu nhỏ CNN.

Chuyển giao kiến thức đã được chứng minh là hiệu quả trong tài liệu về tỉa. Các tham số của các mạng có thể được chuyển giao từ khởi tạo đã được huấn luyện trước [30,18]. Minnehan et al. [37] đã chuyển giao kiến thức của mạng chưa nén qua mất mát tái tạo theo khối. Trong bài báo này, chúng tôi áp dụng một phương pháp KD đơn giản [21] để thực hiện chuyển giao kiến thức, đạt được hiệu suất mạnh mẽ cho các kiến trúc được tìm kiếm.

3 Phương pháp luận
Phương pháp tỉa của chúng tôi bao gồm ba bước: (1) huấn luyện mạng lớn chưa tỉa bằng quy trình huấn luyện phân loại tiêu chuẩn. (2) tìm kiếm độ sâu và chiều rộng của một mạng nhỏ qua TAS được đề xuất. (3) chuyển giao kiến thức từ mạng lớn chưa tỉa sang mạng nhỏ được tìm kiếm bằng một phương pháp KD đơn giản [21]. Chúng tôi sẽ giới thiệu lý thuyết nền, hiển thị chi tiết của TAS, và giải thích quy trình chuyển giao kiến thức.

3.1 Tìm kiếm Kiến trúc Có thể Biến đổi
Tỉa kênh mạng nhằm giảm số lượng kênh trong mỗi lớp của một mạng. Cho một hình ảnh đầu vào, một mạng nhận nó làm đầu vào và tạo ra xác suất trên mỗi lớp mục tiêu. Giả sử X và O là các tensor đặc trưng đầu vào và đầu ra của lớp tích chập thứ l (chúng tôi lấy tích chập 3×3 làm ví dụ), lớp này tính toán quy trình sau:

Oj = Σ(k=1 đến cin) Xk,:,: * Wj,k,:,: trong đó 1 ≤ j ≤ cout; (1)

--- TRANG 4 ---
trong đó W ∈ R^(cout×cin×3×3) chỉ ra trọng số nhân tích chập, cin là kênh đầu vào, và cout là kênh đầu ra. Wj,k,:,: tương ứng với kênh đầu vào thứ k và kênh đầu ra thứ j. * biểu thị phép toán tích chập. Các phương pháp tỉa kênh có thể giảm số lượng cout, và do đó, cin trong lớp tiếp theo cũng được giảm.

Tìm kiếm chiều rộng. Chúng tôi sử dụng các tham số α ∈ R^|C| để chỉ ra phân phối của số lượng kênh có thể có trong một lớp, được chỉ ra bởi C và max(C) ≤ cout. Xác suất chọn ứng viên thứ j cho số lượng kênh có thể được công thức hóa như:

pj = exp(αj) / Σ(k=1 đến |C|) exp(αk) trong đó 1 ≤ j ≤ |C|; (2)

Tuy nhiên, phép toán lấy mẫu trong quy trình trên là không thể vi phân, điều này ngăn chúng ta lan truyền ngược gradient qua pj đến αj. Được thúc đẩy bởi [8], chúng tôi áp dụng Gumbel-Softmax [26,36] để làm mềm quy trình lấy mẫu để tối ưu hóa α:

p̂j = exp((log(pj) + oj)/τ) / Σ(k=1 đến |C|) exp((log(pk) + ok)/τ) s.t. oj = -log(-log(u)) & u ~ U(0,1); (3)

trong đó U(0,1) có nghĩa là phân phối đồng đều giữa 0 và 1. τ là nhiệt độ softmax. Khi τ → 0, p̂ = [p̂1, ..., p̂j, ...] trở thành one-shot, và phân phối Gumbel-softmax được rút ra từ p̂ trở nên giống hệt với phân phối phân loại. Khi τ → ∞, phân phối Gumbel-softmax trở thành phân phối đồng đều trên C. Bản đồ đặc trưng trong phương pháp của chúng tôi được định nghĩa là tổng có trọng số của các mảnh bản đồ đặc trưng ban đầu với các kích thước khác nhau, trong đó trọng số là p̂. Các bản đồ đặc trưng với các kích thước khác nhau được căn chỉnh bằng nội suy theo kênh (CWI) để thực hiện phép toán tổng có trọng số. Để giảm chi phí bộ nhớ, chúng tôi chọn một tập con nhỏ với các chỉ số I ⊆ [|C|] để tập hợp thay vì sử dụng tất cả các ứng viên. Ngoài ra, các trọng số được chuẩn hóa lại dựa trên xác suất của các kích thước được chọn, được công thức hóa như:

Ô = Σ(j∈I) [exp((log(pj) + oj)/τ) / Σ(k∈I) exp((log(pk) + ok)/τ)] * CWI(O1:Cj,:,:, max(CI)) s.t. I ~ T_p̂; (4)

trong đó T_p̂ chỉ ra phân phối xác suất đa thức được tham số hóa bởi p̂. CWI được đề xuất là một phép toán tổng quát để căn chỉnh các bản đồ đặc trưng với các kích thước khác nhau. Nó có thể được thực hiện bằng nhiều cách, chẳng hạn như một biến thể 3D của mạng biến đổi không gian [25] hoặc phép toán pooling thích ứng [16]. Trong bài báo này, chúng tôi chọn phép toán pooling trung bình thích ứng 3D [16] làm CWI², bởi vì nó không mang lại tham số thêm và chi phí thêm không đáng kể. Chúng tôi sử dụng Batch Normalization [24] trước CWI để chuẩn hóa các mảnh khác nhau. Hình 2 minh họa quy trình trên bằng cách lấy |I| = 2 làm ví dụ.

Thảo luận w.r.t. chiến lược lấy mẫu trong Eq. (4). Chiến lược này nhằm giảm phần lớn chi phí bộ nhớ và thời gian huấn luyện xuống mức có thể chấp nhận được bằng cách chỉ lan truyền ngược gradient của các kiến trúc được lấy mẫu thay vì tất cả các kiến trúc. So với lấy mẫu qua phân phối đồng đều, phương pháp lấy mẫu được áp dụng (lấy mẫu dựa trên xác suất) có thể làm yếu sự khác biệt gradient gây ra bởi việc lấy mẫu mỗi lần lặp sau nhiều lần lặp.

Tìm kiếm độ sâu. Chúng tôi sử dụng các tham số β ∈ R^L để chỉ ra phân phối của số lượng lớp có thể có trong một mạng với L lớp tích chập. Chúng tôi sử dụng một chiến lược tương tự để lấy mẫu số lượng lớp theo Eq. (3) và cho phép β có thể vi phân giống như α, sử dụng phân phối lấy mẫu q̂l cho độ sâu l. Sau đó chúng tôi tính toán đặc trưng đầu ra cuối cùng của các mạng được tỉa như một tập hợp từ tất cả các độ sâu có thể, có thể được công thức hóa như:

Oout = Σ(l=1 đến L) q̂l * CWI(Ôl, Cout); (5)

trong đó Ôl chỉ ra bản đồ đặc trưng đầu ra qua Eq. (4) tại lớp thứ l. Cout chỉ ra kênh tối đa được lấy mẫu trong số tất cả Ôl. Bản đồ đặc trưng đầu ra cuối cùng Oout được đưa vào lớp phân loại cuối cùng để đưa ra dự đoán. Theo cách này, chúng tôi có thể lan truyền ngược gradient đến cả tham số chiều rộng α và tham số độ sâu β.

²Công thức của CWI được chọn: giả sử B = CWI(A, Cout), trong đó B ∈ R^(Cout×H×W) và A ∈ R^(C×H×W); thì Bi,h,w = mean(As:e-1,h,w), trong đó s = ⌊iC/Cout⌋ và e = ⌊(i+1)C/Cout⌋. Chúng tôi đã thử các dạng khác của CWI, ví dụ, nội suy song tuyến và tam tuyến. Chúng đạt được độ chính xác tương tự nhưng chậm hơn nhiều so với lựa chọn của chúng tôi.

--- TRANG 5 ---
Mục tiêu tìm kiếm. Kiến trúc cuối cùng A được suy ra bằng cách chọn ứng viên có xác suất tối đa, được học bởi các tham số kiến trúc A, bao gồm α cho mỗi lớp và β. Mục tiêu của TAS của chúng tôi là tìm một kiến trúc A với mất mát validation tối thiểu Lval sau khi được huấn luyện bằng cách tối thiểu hóa mất mát training Ltrain như:

min_A Lval(ω*_A; A) s.t. ω*_A = arg min_ω Ltrain(ω; A); (6)

trong đó ω*_A chỉ ra các trọng số được tối ưu hóa của A. Mất mát training là mất mát phân loại cross-entropy của các mạng. Các phương pháp NAS phổ biến [31,48,8,4,40] tối ưu hóa A trên các ứng viên mạng với các cấu trúc tôpô khác nhau, trong khi TAS của chúng tôi tìm kiếm trên các ứng viên có cùng cấu trúc tôpô cũng như chiều rộng và độ sâu nhỏ hơn. Kết quả là, mất mát validation trong quy trình tìm kiếm của chúng tôi bao gồm không chỉ mất mát validation phân loại mà còn phạt cho chi phí tính toán:

Lval = -log(exp(zy) / Σ(j=1 đến |z|) exp(zj)) + λcost * Lcost; (7)

trong đó z là một vector biểu thị các logit đầu ra từ các mạng được tỉa, y chỉ ra lớp sự thật cơ bản của một đầu vào tương ứng, và λcost là trọng số của Lcost. Mất mát chi phí khuyến khích chi phí tính toán của mạng (ví dụ, FLOP) hội tụ đến một mục tiêu R để chi phí có thể được điều chỉnh động bằng cách đặt R khác nhau. Chúng tôi đã sử dụng mất mát chi phí tính toán từng khúc như:

Lcost = {log(Ecost(A)) nếu Fcost(A) > (1 + t)R
         0 nếu (1-t)R < Fcost(A) < (1 + t)R
         -log(Ecost(A)) nếu Fcost(A) < (1-t)R; (8)

trong đó Ecost(A) tính toán kỳ vọng của chi phí tính toán, dựa trên các tham số kiến trúc A. Cụ thể, nó là tổng có trọng số của chi phí tính toán cho tất cả các mạng ứng viên, trong đó trọng số là xác suất lấy mẫu. Fcost(A) chỉ ra chi phí thực tế của kiến trúc được tìm kiếm, có chiều rộng và độ sâu được suy ra từ A. t ∈ [0,1] biểu thị tỷ lệ dung sai, làm chậm

Thuật toán 1 Quy trình TAS
Đầu vào: chia tập training thành hai tập rời rạc: Dtrain và Dval
1: while chưa hội tụ do
2:   Lấy mẫu dữ liệu batch Dt từ Dtrain
3:   Tính Ltrain trên Dt để cập nhật trọng số mạng
4:   Lấy mẫu dữ liệu batch Dv từ Dval
5:   Tính Lval trên Dv qua Eq. (7) để cập nhật A
6: end while
7: Suy ra mạng được tìm kiếm từ A
8: Khởi tạo ngẫu nhiên mạng được tìm kiếm và tối ưu hóa nó bằng KD qua Eq. (10) trên tập training

tốc độ thay đổi của kiến trúc được tìm kiếm. Lưu ý rằng chúng tôi sử dụng FLOP để đánh giá chi phí tính toán của một mạng, và nó dễ dàng thay thế FLOP bằng các metric khác, chẳng hạn như độ trễ [4].

Chúng tôi hiển thị thuật toán tổng thể trong Alg. 1. Trong quá trình tìm kiếm, chúng tôi truyền thuận mạng sử dụng Eq. (5) để làm cho cả trọng số và tham số kiến trúc có thể vi phân. Chúng tôi luân phiên tối thiểu hóa Ltrain trên tập training để tối ưu hóa trọng số của các mạng được tỉa và Lval trên tập validation để tối ưu hóa các tham số kiến trúc A. Sau khi tìm kiếm, chúng tôi chọn số lượng kênh có xác suất tối đa làm chiều rộng và số lượng lớp có xác suất tối đa làm độ sâu. Mạng được tìm kiếm cuối cùng được xây dựng bởi chiều rộng và độ sâu được chọn. Mạng này sẽ được tối ưu hóa qua KD, và chúng tôi sẽ giới thiệu chi tiết trong Sec. 3.2.

3.2 Chuyển giao Kiến thức
Chuyển giao kiến thức quan trọng để học một mạng được tỉa mạnh mẽ, và chúng tôi sử dụng một thuật toán KD đơn giản [21] trên một kiến trúc mạng được tìm kiếm. Thuật toán này khuyến khích các dự đoán z của mạng nhỏ khớp với các mục tiêu mềm từ mạng chưa tỉa qua mục tiêu sau:

Lmatch = Σ(i=1 đến |z|) [exp(ẑi/T) / Σ(j=1 đến |z|) exp(ẑj/T)] * log[exp(zi/T) / Σ(j=1 đến |z|) exp(zj/T)]; (9)

trong đó T là nhiệt độ, và ẑi chỉ ra vector đầu ra logit từ mạng chưa tỉa đã được huấn luyện trước. Ngoài ra, nó sử dụng mất mát softmax với cross-entropy để khuyến khích mạng nhỏ dự đoán các mục tiêu đúng. Mục tiêu cuối cùng của KD như sau:

LKD = -log(exp(zy) / Σ(j=1 đến |z|) exp(zj)) + (1-λ) * Lmatch s.t. 0 ≤ λ ≤ 1; (10)

--- TRANG 6 ---
[BIỂU ĐỒ: Các biểu đồ hiển thị FLOP và độ lệch trung bình theo epoch cho các chiến lược khác nhau]

0 100 200 300 400 500
Chỉ số epoch huấn luyện
0 30 60 90 120
FLOPs (MB)
sample w/ CWI
sample w/o CWI
mixture w/ CWI
mixture w/o CWI
target FLOPs

(a) FLOP của mạng được tìm kiếm theo epoch khi chúng tôi không ràng buộc FLOP (λcost = 0).

0 100 200 300 400 500
Chỉ số epoch huấn luyện
0.0 0.1 0.2 0.3 0.4 0.5
Độ lệch
sample w/ CWI
sample w/o CWI
mixture w/ CWI
mixture w/o CWI

(b) Độ lệch trung bình theo epoch khi chúng tôi không ràng buộc FLOP (λcost = 0).

0 100 200 300 400 500
Chỉ số epoch huấn luyện
0 15 30 45 60
FLOPs (MB)
sample w/ CWI
sample w/o CWI
mixture w/ CWI
mixture w/o CWI
target FLOPs

(c) FLOP của mạng được tìm kiếm theo epoch khi chúng tôi ràng buộc FLOP (λcost = 2).

0 100 200 300 400 500
Chỉ số epoch huấn luyện
0.0 0.1 0.2 0.3 0.4 0.5
Độ lệch
sample w/ CWI
sample w/o CWI
mixture w/ CWI
mixture w/o CWI

(d) Độ lệch trung bình theo epoch khi chúng tôi ràng buộc FLOP (λcost = 2).

Hình 3: Tác động của các lựa chọn khác nhau để làm cho các tham số kiến trúc có thể vi phân.

trong đó y chỉ ra lớp mục tiêu đúng của một đầu vào tương ứng. λ là trọng số của mất mát để cân bằng mất mát phân loại tiêu chuẩn và mất mát khớp mềm. Sau khi chúng tôi có được mạng được tìm kiếm (Sec. 3.1), chúng tôi trước tiên huấn luyện trước mạng chưa tỉa và sau đó tối ưu hóa mạng được tìm kiếm bằng cách chuyển giao từ mạng chưa tỉa qua Eq. (10).

4 Phân tích Thực nghiệm
Chúng tôi giới thiệu thiết lập thực nghiệm trong Sec. 4.1. Chúng tôi đánh giá các khía cạnh khác nhau của TAS trong Sec. 4.2, chẳng hạn như siêu tham số, chiến lược lấy mẫu, các phương pháp chuyển giao khác nhau, v.v. Cuối cùng, chúng tôi so sánh TAS với các phương pháp tỉa tiên tiến khác trong Sec. 4.3.

4.1 Tập dữ liệu và Cài đặt
Tập dữ liệu. Chúng tôi đánh giá phương pháp của mình trên CIFAR-10, CIFAR-100 [27] và ImageNet [6]. CIFAR-10 chứa 50K hình ảnh huấn luyện và 10K hình ảnh test với 10 lớp. CIFAR-100 tương tự như CIFAR-10 nhưng có 100 lớp. ImageNet chứa 1.28 triệu hình ảnh huấn luyện và 50K hình ảnh test với 1000 lớp. Chúng tôi sử dụng tăng cường dữ liệu điển hình của ba tập dữ liệu này. Trên CIFAR-10 và CIFAR-100, chúng tôi cắt ngẫu nhiên patch 32×32 với 4 pixel padding ở mỗi viền, và chúng tôi cũng áp dụng lật ngang ngẫu nhiên. Trên ImageNet, chúng tôi sử dụng cắt có kích thước thay đổi ngẫu nhiên điển hình, thay đổi ngẫu nhiên độ sáng/tương phản/độ bão hòa, và lật ngang ngẫu nhiên để tăng cường dữ liệu. Trong quá trình đánh giá, chúng tôi thay đổi kích thước hình ảnh thành 256×256 và cắt giữa patch 224×224.

Cài đặt tìm kiếm. Chúng tôi tìm kiếm số lượng kênh trên {0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} của số lượng ban đầu trong mạng chưa tỉa. Chúng tôi tìm kiếm độ sâu trong mỗi giai đoạn tích chập.

--- TRANG 7 ---
Chúng tôi lấy mẫu |I| = 2 ứng viên trong Eq. (4) để giảm chi phí bộ nhớ GPU trong quá trình tìm kiếm. Chúng tôi đặt R theo FLOP của các thuật toán tỉa được so sánh và đặt λcost là 2. Chúng tôi tối ưu hóa

Bảng 1: Độ chính xác trên CIFAR-100 khi tỉa khoảng 40% FLOP của ResNet-32.
                    FLOPs    accuracy
Pre-defined         41.1 MB  68.18%
Pre-defined w/ Init 41.1 MB  69.34%
Pre-defined w/ KD   41.1 MB  71.40%
Random Search       42.9 MB  68.57%
Random Search w/ Init 42.9 MB 69.14%
Random Search w/ KD 42.9 MB  71.71%
TAS†               42.5 MB  68.95%
TAS† w/ Init       42.5 MB  69.70%
TAS† w/ KD (TAS)   42.5 MB  72.41%

trọng số qua SGD và các tham số kiến trúc qua Adam. Đối với trọng số, chúng tôi bắt đầu tốc độ học từ 0.1 và giảm nó bằng bộ lập lịch cosine [34]. Đối với các tham số kiến trúc, chúng tôi sử dụng tốc độ học không đổi là 0.001 và weight decay là 0.001. Trên cả CIFAR-10 và CIFAR-100, chúng tôi huấn luyện mô hình trong 600 epoch với batch size là 256. Trên ImageNet, chúng tôi huấn luyện ResNet [17] trong 120 epoch với batch size là 256. Tỷ lệ dung sai t luôn được đặt là 5%. τ trong Eq. (3) được giảm tuyến tính từ 10 xuống 0.1.

Huấn luyện. Đối với các thí nghiệm CIFAR, chúng tôi sử dụng SGD với momentum là 0.9 và weight decay là 0.0005. Chúng tôi huấn luyện mỗi mô hình trong 300 epoch, bắt đầu tốc độ học ở 0.1, và giảm nó bằng bộ lập lịch cosine [34]. Chúng tôi sử dụng batch size là 256 và 2 GPU. Khi sử dụng KD trên CIFAR, chúng tôi sử dụng λ là 0.9 và nhiệt độ T là 4 theo [46]. Đối với các mô hình ResNet trên ImageNet, chúng tôi tuân theo hầu hết các siêu tham số như CIFAR, nhưng sử dụng weight decay là 0.0001. Chúng tôi sử dụng 4 GPU để huấn luyện mô hình trong 120 epoch với batch size là 256. Khi sử dụng KD trên ImageNet, chúng tôi đặt λ là 0.5 và T là 4 trên ImageNet.

4.2 Nghiên cứu Trường hợp
Trong phần này, chúng tôi đánh giá các khía cạnh khác nhau của TAS được đề xuất. Chúng tôi cũng so sánh nó với thuật toán tìm kiếm khác nhau và phương pháp chuyển giao kiến thức để chứng minh hiệu quả của TAS.

Tác động của các chiến lược khác nhau để vi phân α. Chúng tôi áp dụng TAS của chúng tôi trên CIFAR-100 để tỉa ResNet-56. Chúng tôi thử hai phương pháp tập hợp khác nhau, tức là sử dụng CWI được đề xuất để căn chỉnh các bản đồ đặc trưng hoặc không. Chúng tôi cũng thử hai loại trọng số tập hợp khác nhau, tức là lấy mẫu Gumbel-softmax như Eq. (3) (được ký hiệu là "sample" trong Hình 3) và vanilla-softmax như Eq. (2) (được ký hiệu là "mixture" trong Hình 3). Do đó, có bốn chiến lược khác nhau, tức là có/không có CWI kết hợp với Gumbel-softmax/vanilla-softmax. Giả sử chúng tôi không ràng buộc chi phí tính toán, thì các tham số kiến trúc nên được tối ưu hóa để tìm chiều rộng và độ sâu tối đa. Điều này là do mạng như vậy sẽ có năng lực tối đa và dẫn đến hiệu suất tốt nhất trên CIFAR-100. Chúng tôi thử tất cả bốn chiến lược có và không có sử dụng ràng buộc chi phí tính toán. Chúng tôi hiển thị kết quả trong

Bảng 2: Kết quả của các cấu hình khác nhau khi tỉa ResNet-32 trên CIFAR-10 với một GPU V100. "#SC" chỉ ra số lượng kênh được chọn. "H" chỉ ra giờ.
#SC  Search Time  Memory  Train Time  FLOPs    Accuracy
|I|=1   2.83 H     1.5GB   0.71 H     23.59 MB  89.85%
|I|=2   3.83 H     2.4GB   0.84 H     38.95 MB  92.98%
|I|=3   4.94 H     3.4GB   0.67 H     39.04 MB  92.63%
|I|=5   7.18 H     5.1GB   0.60 H     37.08 MB  93.18%
|I|=8   10.64 H    7.3GB   0.81 H     38.28 MB  92.65%

Hình 3c và Hình 3a. Khi chúng tôi không ràng buộc FLOP, TAS của chúng tôi có thể thành công tìm ra kiến trúc tốt nhất nên có chiều rộng và độ sâu tối đa. Tuy nhiên, ba chiến lược khác đã thất bại. Khi chúng tôi sử dụng ràng buộc FLOP, chúng tôi có thể thành công ràng buộc chi phí tính toán trong phạm vi mục tiêu. Chúng tôi cũng điều tra độ lệch giữa xác suất cao nhất và xác suất cao thứ hai trong Hình 3d và Hình 3b. Về mặt lý thuyết, độ lệch cao hơn chỉ ra rằng mô hình tự tin hơn để chọn một chiều rộng nhất định, trong khi độ lệch thấp hơn có nghĩa là mô hình bối rối và không biết ứng viên nào để chọn. Như được hiển thị trong Hình 3d, với quá trình huấn luyện diễn ra, TAS của chúng tôi trở nên tự tin hơn để chọn chiều rộng phù hợp. Ngược lại, các chiến lược không có CWI không thể tối ưu hóa các tham số kiến trúc; và "mixture with CWI" hiển thị độ lệch tệ hơn so với của chúng tôi.

So sánh w.r.t. cấu trúc được tạo bởi các phương pháp khác nhau trong Bảng 1. "Pre-defined" có nghĩa là tỉa một tỷ lệ cố định ở mỗi lớp [30]. "Random Search" chỉ ra một baseline NAS được sử dụng trong [31]. "TAS†" là thuật toán tìm kiếm có thể vi phân được đề xuất của chúng tôi. Chúng tôi có hai quan sát: (1) tìm kiếm có thể tìm một cấu trúc tốt hơn bằng cách sử dụng các phương pháp chuyển giao kiến thức khác nhau; (2) TAS của chúng tôi vượt trội hơn baseline NAS ngẫu nhiên.

--- TRANG 8 ---
Bảng 3: So sánh các thuật toán tỉa khác nhau cho ResNet trên CIFAR. "Acc" = accuracy, "FLOPs" = FLOPs (tỷ lệ tỉa), "TAS (D)" = tìm kiếm độ sâu, "TAS (W)" = tìm kiếm chiều rộng, "TAS" = tìm kiếm cả chiều rộng và độ sâu.

[THIS IS TABLE: Detailed comparison table showing different pruning methods for ResNet on CIFAR-10 and CIFAR-100, with columns for Depth, Method, Prune Acc, Acc Drop, FLOPs for both datasets. The table includes multiple rows for depths 20, 32, 56, 110, and 164, comparing methods like LCCL, SFP, FPGM, TAS variants.]

So sánh w.r.t. các phương pháp chuyển giao kiến thức khác nhau trong Bảng 1. Dòng đầu tiên trong mỗi khối không sử dụng bất kỳ phương pháp chuyển giao kiến thức nào. "w/ Init" chỉ ra sử dụng mạng chưa tỉa đã được huấn luyện trước làm khởi tạo. "w/ KD" chỉ ra sử dụng KD. Từ Bảng 1, các phương pháp chuyển giao kiến thức có thể cải thiện nhất quán độ chính xác của mạng được tỉa, ngay cả khi một phương pháp đơn giản được áp dụng (Init). Bên cạnh đó, KD mạnh mẽ và cải thiện mạng được tỉa hơn 2% độ chính xác trên CIFAR-100.

Tìm kiếm chiều rộng so với tìm kiếm độ sâu. Chúng tôi thử (1) chỉ tìm kiếm độ sâu ("TAS (D)"), (2) chỉ tìm kiếm chiều rộng ("TAS (W)"), và (3) tìm kiếm cả độ sâu và chiều rộng ("TAS") trong Bảng 3. Kết quả của chỉ tìm kiếm độ sâu tệ hơn kết quả của chỉ tìm kiếm chiều rộng. Nếu chúng tôi tìm kiếm chung cho cả độ sâu và chiều rộng, chúng tôi có thể đạt được độ chính xác tốt hơn với FLOP tương tự so với cả tìm kiếm độ sâu và tìm kiếm chiều rộng riêng lẻ.

Tác động của việc chọn số lượng mẫu kiến trúc khác nhau I trong Eq. (4). Chúng tôi so sánh số lượng kênh được chọn khác nhau trong Bảng 2 và thực hiện thí nghiệm trên một NVIDIA Tesla V100 duy nhất. Thời gian tìm kiếm và việc sử dụng bộ nhớ GPU sẽ tăng tuyến tính theo |I|. Khi |I|=1, vì xác suất được chuẩn hóa lại trong Eq. (4) trở thành một vô hướng không đổi là 1, gradient của các tham số α sẽ trở thành 0 và việc tìm kiếm thất bại. Khi |I|>1, hiệu suất cho |I| khác nhau là tương tự.

Lợi ích tăng tốc. Như được hiển thị trong Bảng 2, TAS có thể hoàn thành quy trình tìm kiếm của ResNet-32 trong khoảng 3.8 giờ trên một GPU V100 duy nhất. Nếu chúng tôi sử dụng các phương pháp chiến lược tiến hóa (ES) hoặc tìm kiếm ngẫu nhiên, chúng tôi cần huấn luyện mạng với nhiều cấu hình ứng viên khác nhau lần lượt và sau đó đánh giá chúng để tìm ra tốt nhất. Theo cách này, chi phí tính toán nhiều hơn so với TAS của chúng tôi là

--- TRANG 9 ---
Bảng 4: So sánh các thuật toán tỉa khác nhau cho các ResNet khác nhau trên ImageNet.

[THIS IS TABLE: Comparison table showing different pruning algorithms for ResNets on ImageNet, with columns for Model, Method, Top-1 accuracy, Top-5 accuracy, FLOPs, and Prune Ratio. Contains data for ResNet-18 and ResNet-50 models with various methods like LCCL, SFP, FPGM, TAS, etc.]

được yêu cầu. Một giải pháp có thể để tăng tốc các phương pháp ES hoặc tìm kiếm ngẫu nhiên là chia sẻ tham số của các mạng với các cấu hình khác nhau [39, 45], điều này nằm ngoài phạm vi của bài báo này.

4.3 So sánh với tiên tiến nhất
Kết quả trên CIFAR trong Bảng 3. Chúng tôi tỉa các ResNet khác nhau trên cả CIFAR-10 và CIFAR-100. Hầu hết các thuật toán trước đây hoạt động kém trên CIFAR-100, trong khi TAS của chúng tôi vượt trội nhất quán hơn 2% độ chính xác trong hầu hết các trường hợp. Trên CIFAR-10, TAS của chúng tôi vượt trội hơn các thuật toán tiên tiến trên ResNet-20,32,56,110. Ví dụ, TAS đạt được 72.25% độ chính xác bằng cách tỉa ResNet-56 trên CIFAR-100, cao hơn 69.66% của FPGM [19]. Đối với việc tỉa ResNet-32 trên CIFAR-100, chúng tôi đạt được độ chính xác lớn hơn và FLOP ít hơn so với mạng chưa tỉa. Chúng tôi đạt được hiệu suất hơi tệ hơn LCCL [7] trên ResNet-164. Điều này là do có 8×16×3×18×3 cấu trúc mạng ứng viên để tìm kiếm cho việc tỉa ResNet-164. Việc tìm kiếm trên không gian tìm kiếm khổng lồ như vậy là thách thức, và mạng rất sâu có vấn đề over-fitting trên CIFAR-10 [17].

Kết quả trên ImageNet trong Bảng 4. Chúng tôi tỉa ResNet-18 và ResNet-50 trên ImageNet. Đối với ResNet-18, cần khoảng 59 giờ để tìm kiếm mạng được tỉa trên 4 NVIDIA Tesla V100 GPU. Thời gian huấn luyện của ResNet-18 chưa tỉa mất khoảng 24 giờ, và do đó thời gian tìm kiếm là có thể chấp nhận được. Với nhiều máy hơn và triển khai được tối ưu hóa, chúng tôi có thể hoàn thành TAS với chi phí thời gian ít hơn. Chúng tôi hiển thị kết quả cạnh tranh so với các thuật toán tỉa tiên tiến khác. Ví dụ, TAS tỉa ResNet-50 bằng 43.5% FLOP, và mạng được tỉa đạt được 76.20% độ chính xác, cao hơn FPGM 0.7. Những cải thiện tương tự có thể được tìm thấy khi tỉa ResNet-18. Lưu ý rằng chúng tôi trực tiếp áp dụng các siêu tham số trên CIFAR-10 để tỉa các mô hình trên ImageNet, và do đó TAS có thể đạt được kết quả tốt hơn bằng cách điều chỉnh cẩn thận các tham số trên ImageNet.

TAS được đề xuất của chúng tôi là một công trình sơ bộ cho quy trình tỉa mạng mới. Quy trình này có thể được cải thiện bằng cách thiết kế thuật toán tìm kiếm hiệu quả hơn và phương pháp chuyển giao kiến thức. Chúng tôi hy vọng rằng công trình tương lai để khám phá hai thành phần này sẽ mang lại các mạng nén mạnh mẽ.

5 Kết luận
Trong bài báo này, chúng tôi đề xuất một mô hình mới cho tỉa mạng, bao gồm hai thành phần. Đối với thành phần đầu tiên, chúng tôi đề xuất áp dụng NAS để tìm kiếm độ sâu và chiều rộng tốt nhất của một mạng. Vì hầu hết các phương pháp NAS trước đây tập trung vào tôpô mạng thay vì kích thước mạng, chúng tôi đặt tên sơ đồ NAS mới này là Tìm kiếm Kiến trúc Có thể Biến đổi (TAS). Hơn nữa, chúng tôi đề xuất một phương pháp TAS có thể vi phân để tìm hiệu quả và có hiệu suất độ sâu và chiều rộng phù hợp nhất của một mạng. Đối với thành phần thứ hai, chúng tôi đề xuất tối ưu hóa mạng được tìm kiếm bằng cách chuyển giao kiến thức từ mạng chưa tỉa. Trong bài báo này, chúng tôi áp dụng một thuật toán KD đơn giản để thực hiện chuyển giao kiến thức, và tiến hành các phương pháp chuyển giao khác để chứng minh hiệu quả của thành phần này. Kết quả của chúng tôi cho thấy rằng những nỗ lực mới tập trung vào tìm kiếm và chuyển giao có thể dẫn đến những đột phá mới trong tỉa mạng.

--- TRANG 10 ---
Tài liệu tham khảo
[1] M. Alizadeh, J. Fernández-Marqués, N. D. Lane, và Y. Gal. An empirical study of binary neural networks' optimisation. Trong International Conference on Learning Representations (ICLR), 2019.

[2] J. M. Alvarez và M. Salzmann. Learning the number of neurons in deep networks. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 2270–2278, 2016.

[3] H. Cai, T. Chen, W. Zhang, Y. Yu, và J. Wang. Efficient architecture search by network transformation. Trong AAAI Conference on Artificial Intelligence (AAAI), trang 2787–2794, 2018.

[4] H. Cai, L. Zhu, và S. Han. ProxylessNAS: Direct neural architecture search on target task and hardware. Trong International Conference on Learning Representations (ICLR), 2019.

[5] T. Chen, I. Goodfellow, và J. Shlens. Net2net: Accelerating learning via knowledge transfer. Trong International Conference on Learning Representations (ICLR), 2016.

[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei. ImageNet: A large-scale hierarchical image database. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 248–255, 2009.

[7] X. Dong, J. Huang, Y. Yang, và S. Yan. More is less: A more complicated network with less inference complexity. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 5840–5848, 2017.

[8] X. Dong và Y. Yang. Searching for a robust neural architecture in four gpu hours. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 1761–1770, 2019.

[9] M. Figurnov, M. D. Collins, Y. Zhu, L. Zhang, J. Huang, D. Vetrov, và R. Salakhutdinov. Spatially adaptive computation time for residual networks. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 1039–1048, 2017.

[10] A. Gordon, E. Eban, O. Nachum, B. Chen, H. Wu, T.-J. Yang, và E. Choi. MorphNet: Fast & simple resource-constrained structure learning of deep networks. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 1586–1595, 2018.

[11] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, và W. J. Dally. EIE: efficient inference engine on compressed deep neural network. Trong The ACM/IEEE International Symposium on Computer Architecture (ISCA), trang 243–254, 2016.

[12] S. Han, H. Mao, và W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. Trong International Conference on Learning Representations (ICLR), 2015.

[13] S. Han, J. Pool, J. Tran, và W. Dally. Learning both weights and connections for efficient neural network. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 1135–1143, 2015.

[14] B. Hassibi và D. G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 164–171, 1993.

[15] J. L. Z. L. H. W. L.-J. L. He, Yihui và S. Han. AMC: Automl for model compression and acceleration on mobile devices. Trong Proceedings of the European Conference on Computer Vision (ECCV), trang 183–202, 2018.

[16] K. He, X. Zhang, S. Ren, và J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 37(9):1904–1916, 2015.

[17] K. He, X. Zhang, S. Ren, và J. Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 770–778, 2016.

[18] Y. He, G. Kang, X. Dong, Y. Fu, và Y. Yang. Soft filter pruning for accelerating deep convolutional neural networks. Trong International Joint Conference on Artificial Intelligence (IJCAI), trang 2234–2240, 2018.

[19] Y. He, P. Liu, Z. Wang, và Y. Yang. Pruning filter via geometric median for deep convolutional neural networks acceleration. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 4340–4349, 2019.

[20] Y. He, X. Zhang, và J. Sun. Channel pruning for accelerating very deep neural networks. Trong Proceedings of the IEEE International Conference on Computer Vision (ICCV), trang 1389–1397, 2017.

[21] G. Hinton, O. Vinyals, và J. Dean. Distilling the knowledge in a neural network. Trong The Conference on Neural Information Processing Systems Workshop (NeurIPS-W), 2014.

[22] G. Huang, Z. Liu, L. Van Der Maaten, và K. Q. Weinberger. Densely connected convolutional networks. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 4700–4708, 2017.

--- TRANG 11 ---
[23] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, và Y. Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research (JMLR), 18(1):6869–6898, 2017.

[24] S. Ioffe và C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Trong The International Conference on Machine Learning (ICML), trang 448–456, 2015.

[25] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 2017–2025, 2015.

[26] E. Jang, S. Gu, và B. Poole. Categorical reparameterization with gumbel-softmax. Trong International Conference on Learning Representations (ICLR), 2017.

[27] A. Krizhevsky và G. Hinton. Learning multiple layers of features from tiny images. Báo cáo kỹ thuật, Citeseer, 2009.

[28] A. Krizhevsky, I. Sutskever, và G. E. Hinton. ImageNet classification with deep convolutional neural networks. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 1097–1105, 2012.

[29] Y. LeCun, J. S. Denker, và S. A. Solla. Optimal brain damage. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 598–605, 1990.

[30] H. Li, A. Kadav, I. Durdanovic, H. Samet, và H. P. Graf. Pruning filters for efficient convnets. Trong International Conference on Learning Representations (ICLR), 2017.

[31] H. Liu, K. Simonyan, và Y. Yang. Darts: Differentiable architecture search. Trong International Conference on Learning Representations (ICLR), 2019.

[32] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, và C. Zhang. Learning efficient convolutional networks through network slimming. Trong Proceedings of the IEEE International Conference on Computer Vision (ICCV), trang 2736–2744, 2017.

[33] Z. Liu, M. Sun, T. Zhou, G. Huang, và T. Darrell. Rethinking the value of network pruning. Trong International Conference on Learning Representations (ICLR), 2018.

[34] I. Loshchilov và F. Hutter. SGDR: Stochastic gradient descent with warm restarts. Trong International Conference on Learning Representations (ICLR), 2017.

[35] C. Louizos, M. Welling, và D. P. Kingma. Learning sparse neural networks through l_0 regularization. Trong International Conference on Learning Representations (ICLR), 2018.

[36] C. J. Maddison, A. Mnih, và Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. Trong International Conference on Learning Representations (ICLR), 2017.

[37] B. Minnehan và A. Savakis. Cascaded projection: End-to-end network compression and acceleration. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 10715–10724, 2019.

[38] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, và J. Kautz. Importance estimation for neural network pruning. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 11264–11272, 2019.

[39] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, và J. Dean. Efficient neural architecture search via parameter sharing. Trong The International Conference on Machine Learning (ICML), trang 4092–4101, 2018.

[40] E. Real, A. Aggarwal, Y. Huang, và Q. V. Le. Regularized evolution for image classifier architecture search. Trong AAAI Conference on Artificial Intelligence (AAAI), 2019.

[41] E. Tartaglione, S. Lepsøy, A. Fiandrotti, và G. Francini. Learning sparse neural networks via sensitivity-driven regularization. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 3878–3888, 2018.

[42] W. Wen, C. Wu, Y. Wang, Y. Chen, và H. Li. Learning structured sparsity in deep neural networks. Trong The Conference on Neural Information Processing Systems (NeurIPS), trang 2074–2082, 2016.

[43] J. Ye, X. Lu, Z. Lin, và J. Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. Trong International Conference on Learning Representations (ICLR), 2018.

[44] J. Yim, D. Joo, J. Bae, và J. Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 4133–4141, 2017.

[45] J. Yu và T. Huang. Network slimming by slimmable networks: Towards one-shot architecture search for channel numbers. arXiv preprint arXiv:1903.11728, 2019.

[46] S. Zagoruyko và N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Trong International Conference on Learning Representations (ICLR), 2017.

--- TRANG 12 ---
[47] X. Zhang, J. Zou, K. He, và J. Sun. Accelerating very deep convolutional networks for classification and detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 38(10):1943–1955, 2016.

[48] B. Zoph và Q. V. Le. Neural architecture search with reinforcement learning. Trong International Conference on Learning Representations (ICLR), 2017.
