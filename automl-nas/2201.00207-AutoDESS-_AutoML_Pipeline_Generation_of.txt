# 2201.00207.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/automl-nas/2201.00207.pdf
# File size: 11712113 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AutoDESS: AutoML Pipeline Generation of
Classication with Dynamic Ensemble Strategy
Selection
Yunpu Zhaoa, Rui Zhangb,, Xiaqing Lib
aUniversity of Science and Techonology of China
bSKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of
Sciences
Abstract
Automated machine learning has achieved remarkable technological develop-
ments in recent years, and building an automated machine learning pipeline is
now an essential task. However, existing AutoML pipeline approaches adopt
monotonous ensemble strategies across dierent machine learning classication
tasks. They ignore the fact that no single ensemble strategy can perform best
on all of the various classication tasks, and thus cannot meet the performance
requirements in practice. To this end, we propose AutoDESS, an ecient Au-
toML approach to identifying the best ensemble strategy for machine learning
classication tasks by combining dierent ensemble strategies' strengths. To
our best knowledge, AutoDESS is the rst trial in AutoML aiming at searching
and optimizing ensemble strategies. In the comparison experiments, AutoDESS
outperforms the state-of-the-art AutoML approaches while with the same CPU
time in 42 classication datasets from the OpenML platform. In addition, ab-
lation experiments also validate the eectiveness of AutoDESS.
Keywords: Automated Machine Learning, Dynamic Ensemble Selection,
Feature Engineering, Bayesian Optimization
Corresponding author
Email address: zhangrui@ict.ac.cn (Rui Zhang)
Preprint submitted to Knowledge-Based Systems July 21, 2022arXiv:2201.00207v2  [cs.LG]  20 Jul 2022

--- PAGE 2 ---
1. Introduction
Machine Learning has achieved remarkable developments in a wide range of
applications. Especially, ensemble learning plays a key role as the core approach
for the model ensemble part of the AutoML framework. Two representative
ensemble strategies are emerged in the last decades, including stacked gener-
alization [1] and static selection. Both of them typically adopt xed ensemble
strategies to exploit the high performance of the classication tasks. Concretely,
auto-sklearn uses static selection, and TPOT [2], AutoGluon [3], FLAML [4],
H2O-AutoML [5] are all based on stacked generalization, as shown in the upper
part of Figure 1. While some approaches focus on the ensemble part of AutoML
[6, 7], they are not the improvements from an ensemble strategy perspective but
rather add complexity to an existing strategy (They build complex multi-layer
stacked generalization automatically, but still stacked generalization). How-
ever, although delivering good performance, existing approaches just use one or
two strategies in all of the dierent cases, which cannot consistently oer high
performance in machine learning classication tasks.
The main reason is that each ensemble strategy has pros and cons which
is known as No Free Lunch Theorem[8]. For example, stacked generalization
often delivers better performance than a single classier. But due to the high
complexity, the stacked generalization is easy to be overtted, thus leading
to great performance degradation, especially in the case of small datasets[9].
Besides, it is quite hard for practitioners to choose the best for a given task in
practice. The diverse research advance of ensemble learning brings diculties
for applying machine learning. Such diversity further increases the diculty of
the above choices. As such, selecting the optimal ensemble strategy is signicant
as well as challenging.
Therefore, to ease the choosing process, AutoML is proposed to identify
the optimal ensemble strategy automatically. AutoML aims to reduce or even
circumvent the involvement of human experts in the design process of machine
learning[10][11]. In recent years, researchers have attempted to automate the
2

--- PAGE 3 ---
whole process to solve various machine learning tasks, which is called AutoML
pipeline. AutoML pipeline is the same as the general machine learning, mainly
divided into data preprocessing, feature engineering, and model selection. The
choice of ensemble strategy should also be part of this. Unfortunately, existing
AutoML pipeline generation approaches adopt monotonous ensemble strategies
across dierent machine learning classication tasks. They ignore the fact that
no single ensemble strategy can perform best on all of the various classication
tasks, and thus cannot meet the performance requirements in practice.
Figure 1: Dierences between the current AutoML framework (upper) and our approach
(below)
To this end, we aim to enrich and innovate the ensemble module of AutoML
and thus propose AutoDESS, an ecient AutoML approach to identifying the
best ensemble strategy for machine learning classication tasks by combing dif-
ferent ensemble strategies' strengths. As shown in the below part of Figure 1,
AutoDESS can be divided into three modules. Firstly, it preprocessed the input
data and feature engineering, where the algorithms used are selected (Data Pre-
rprocesssing and Feature Engineering). Secondly, AutoDESS selects a subset of
classiers for the ensemble and chooses the appropriate strategy and hyperpa-
rameters (Hyperparameter Optimization for Classiers and Ensemble Strategy).
Specically, AutoDESS deeply integrates Bayesian Optimization to conduct
3

--- PAGE 4 ---
the optimization of the three modules. We compare AutoDESS on 42 public
datasets of the OpenML platform with the current state-of-the-art approaches,
including TPOT, Auto-sklearn, FLAML, and mljarsupervised[2][4][12][13]. The
experimental result shows that AutoDESS outperforms the state-of-the-art Au-
toML approaches in most datasets while with the same CPU time (1800 seconds)
in various metrics. Based on the average of the results, AutoDESS consistently
outperforms existing approaches over most datasets, with f1 (accuracy) improve-
ment of up to 3.4% (0.6%) against the best baseline. Also, our approach has
the highest average ranking on all datasets.
To our best knowledge, AutoDESS is the rst trial that searches and opti-
mizes ensemble strategy in AutoML. The technical contribution of this paper is
three-fold:
â€¢This paper is the rst work to introduce dynamic ensemble selection
(DES)-related research advances into AutoML eld.
â€¢We propose the AutoDESS approach. This method enables a signicant
improvement in model performance, especially for the imbalance problem,
by constructing a rich pool of ensemble strategies and using Bayesian
optimization technique.
â€¢We conduct comprehensive performance evaluation and analysis on dif-
ferent datasets, indicating that AutoDESS is able to identify the best
or near-the-best strategy that delivers better performance while with the
same CPU runtime, compared with existing approaches. We also did hy-
pothesis testing and ablation experiments to demonstrate the eectiveness
of the approach.
The rest of the paper is organized as follows. In Section 2, we briey re-
view the related work on the ensemble in AutoML pipeline, dierent ensemble
strategy especially dynamic ensemble selection. Section 3 introduces some back-
ground concepts and motivation underlying our novel approach. Our proposed
approach is described in detail in Section 4. Section 5 presents the experimen-
4

--- PAGE 5 ---
tal results including comparisons with some baselines, ablation study and some
extra exploration of the results. Finally, Section 6 concludes the paper.
2. Related Work
In this section, we briey review the Ensemble part in current AutoML
Pipeline and the concept of dynamic ensemble selection (DES).
2.1. Ensemble in AutoML Pipeline
AutoML is a general term used to describe the process of automating the se-
lection and optimization of ML algorithms and corresponding hyperparameters.
As mentioned above, the AutoML pipeline is divided into data preprocessing,
feature engineering, model selection, and model ensemble. AutoML Pipeline
refers to automating the entire process of generating machine learning, rather
than focusing on automating just one part of it. Existing approaches to the
model ensemble are similar in their treatment, as shown in Table 1. It has to be
admitted that stacked generalization is a powerful ensemble technique widely
used in data science competitions. However, it also suers from the tendency
to overt and the high complexity of training. Our approach oers not only a
lightweight alternative to the stacked generalization but also more possibilities
for the model ensemble.
Table 1: Dierence of current AutoML framework in ensemble part
AutoML Framework Ensemble Method
Auto-sklearn Static Selection
TPOT Stacked Generalization
H2O AutoML[5] Stacked Generalization + Bagging
AutoGluon Stacked Generalization (Multi-Layer)
FLAML Stacked Generalization (Optional)
mljarsupervised Static Selection + Stacked Generalization
5

--- PAGE 6 ---
2.2. Ensemble Strategy
In recent years, there has been much work focusing on advances in ensemble
learning and multiple classier systems[14][15]. In addition to the traditional
stacked generalization, bagging and boosting methods, dynamic ensemble se-
lection techniques have been developed. Dynamic ensemble selection (DES) is
a subclass of model ensemble methods. DES techniques work by estimating
the competence level of each classier from a pool of classiers. Only the most
competent or an ensemble containing the most competent classiers is selected
to predict the label of a specic test sample. The technique's rationale is that
no single classier is an expert in classifying all samples, and it is natural to use
dierent models to predict dierent kinds of samples. In dynamic selections, the
Figure 2: Dierences between DES and static selection
key is how to pick the most competitive classier for any given sample. Usu-
ally, the capability of a classier is estimated by the local region of the feature
space of a given sample. This region can be dened by dierent methods, lead-
ing to various DES methods, such as applying the KNN technique to nd the
neighborhood of the query sample or using clustering approaches[16][17]. Then,
the competence level of the base classiers is estimated, considering only the
samples belong to the same region as the query sample according to selection
6

--- PAGE 7 ---
criteria such as accuracy, ranking, or probabilistic models[18].
However, the wide variety of DES methods poses great diculties for ma-
chine learning researchers in selecting the appropriate DES strategy for a par-
ticular problem. In order to determine the best method, a lot of practice is
generally needed to make a selection one by one, and the selection needs to be
followed by tuning the hyperparameters. In our work, we consider DES with
dierent methods as dierent ensemble strategies, and also consider traditional
stacked generalization and static selection as independent ensemble strategies.
Our work is essentially to select the appropriate strategies from a rich pool of
ensemble strategies and nd the corresponding hyperparameters and base clas-
sier pool for ensembling. We should also mention here that [19] provides a
great tool for using DES.
3. Preliminaries
This section will formalize the problem and describe the basics of AutoML
pipeline generation and Bayesian Optimization.
3.1. Pipeline Creation Problem
Let a triplet ( g;~A;~) dene an ML pipeline with ga directed acyclic graph,
~Aa vector consisting of the selected algorithm for each node and ~a vector com-
prising the hyperparameters of all selected algorithms. The pipeline is denoted
asPg;~A;~.
Let a trained pipeline Pbe given. Given a dataset Dof sizemand a loss
metricL, the performance ofPis calculated as:
(Pg;~A;~;D) =1
mmX
i=1L( ^yi;yi) (1)
with ^yibeing the predicted output of P.
Let a set of algorithm Awith an according domain of hyperparameters 
and a set of valid pipeline structure Gbe given. Furthermore, let a dataset Dbe
given. Then, the pipeline creation problem consists of nding pipeline structure
7

--- PAGE 8 ---
together with a joint algorithm and hyperparameter selection that minimizes
the loss:
g;~A;~2arg min(Pg;~A;~;D)
g2G;~A2Ajgj;~2(2)
As equation 2 formulated, the pipeline creation problem is formulated as a
black-box optimization problem. We can consider various optimization methods
to solve such a problem. In this paper we use the classical Bayesian optimization
using Gaussian processes.
3.2. Bayesian Optimization
Bayesian Optimization (BO) is an iterative algorithm that is popularly used
for HPO problems. It determines the future evaluation point based on the
previously-obtained results[20]. BO uses two components: the surrogate model
and the acquisition function. The model is a regression model that ts all
the currently-observed points into the objective function. After obtaining the
predictive distribution of the probabilistic surrogate model, the acquisition func-
tion determines the usage of dierent points by balancing the trade-o between
exploration and exploitation.
The reason for using BO as the optimization technique for our approach is
that, on the one hand, compared to grid search and random search, it is much
more scalable for higher dimension and is unlikely to end with local optima
rather than global optima. On the other hand, compared to evolutionary op-
timization, BO does not require a great number of training cycles and are not
noisy as well. Its scales well with utmost resource utilization, handling noisy
data well exploiting non continuous spaces to attain global minima.
Then specically for the proposed approach, we use the Gaussian process
(GP) as the surrogate model in BO. Assuming that the function fwith a mean
and a covariance 2is a realization of a GP, the prediction follows a normal
distribution:
p(yjx;D) =N(yj^;^2) (3)
8

--- PAGE 9 ---
where D is the conguration space of hyperparameters, and y=f(x)is the
evaluation result of each hyperparameter value x. After a set of predicted data
is obtained, the next point to be evaluated is selected from the BO-GP model's
condence intervals. The data from each new test is added to the sample record,
and the BO-GP model is the rebuilt with the new information. This process is
repeated until the end. The loop of BO can be stated as: For t= 1 :T:
1. Given observations( xi;yi=f(xi)) fori= 1 :t, build a probabilistic model
for the objective. Integrate out all possible true functions, using Gaussian
process regression.
2. Optimize a cheap acquisition function ubased on the posterior distribution
for sampling the next point. xt+1= arg minu(x) Exploit uncertainty to
balance exploration against exploitation.
3. Sample the next observation yt+1atxt+1
There are also many options of acquisition function u(x), such as the most
commonly used expected improvement (  EI(x) = E[f(x) f(x+
t)]), lower
condence bound ( LCB (x) =GP(x) +GP(x)) or probability of improve-
ment ( PI(x) = P(f(x)f(x+
t) +)),is hyperparameter for controlling
the trade-o between exploration and exploitation. In our approach, we use
gphedge, a acquisition function that probabilistically choose one of the above
three acquisition functions at every iteration. First, The gains giare initialized
to zero. Then, at every iteration,
â€¢Each acquisition function is optimized independently to propose an can-
didate point xi.
â€¢Out of all these candidates, the next point xbestis selected by softmax (gi).
â€¢After tting the surrogate model with ( xtest;ytest), the gains are updated
such thatgi=(Xi)
AutoDESS aims to optimize the model's performance on the validation
dataset to nd the best or near-the-best ensemble strategy that oers high
9

--- PAGE 10 ---
performance for a given machine learning classication tasks. In our problem,
we have little knowledge about the objective performance model. There are two
reasons for that. First, the relationship between the ensemble part and model
performance is uncertain. Meanwhile, the strategies themselves are correlated
in an unknown way, which increases such uncertainty further. Second, a lim-
ited amount of information supports the performance model since evaluations
of an ensemble learning model require an amount of runtime, and so we can
only aord a few of them. Therefore, we cannot give an accurate performance
model based on the unknown relationship and limited information. Still, the
information is sucient identify a desirable strategy within a few tuning steps.
Therefore, instead of proposing an accurate performance model, we need a
model that can guide AutoDESS in identifying the best strategy with the high-
est accuracy from the rest. Bayesian Optimization is an ecient approach to
solve such an optimization problem, where the performance of the validation
dataset is a black-box function that can be assumed through a few evaluations.
By applying BO to it, AutoDESS is able to calculate the condence interval
according to samples taken from the training dataset. As the number of sam-
ples increases, the condence interval area decreases, and the estimation of the
performance of the validation dataset improves. In addition, AutoDESS can
smartly suggest which strategy with its corresponding hyperparameters should
be sample next to minimize the uncertainty in current modeling and more closer
to the best one by using the predened acquisition function as mentioned above.
4. Proposed Method
4.1. Overview
As shown in Figure 3, we have built an AutoML pipeline based on our
approach. The input to the entire framework requires primitives sets on feature
engineering, classiers, and ensemble strategy, in addition to training sets and
test data. The specic setting for primitives will be mentioned later.
10

--- PAGE 11 ---
Figure 3: The overall framework of proposed AutoDESS
There are three dierent optimization components in our framework. The
rst part is optimizing the feature engineering step, that is, nding the opti-
mal combination of algorithms to perform the feature transformation, including
matrix decomposition and dimensionality reduction. The second part is the
optimization of the hyperparameters for every single model in the classier
primitives. After obtaining several tuned models, the third part is nding the
optimal ensemble strategy and the corresponding hyperparameters, including
which classiers will be selected for ensemble learning. In the following, we
will expand on the rst and third parts, and the second part is not dierent
from other AutoML approaches. We optimize feature engineering and ensem-
ble separately because the dimensionality of the decision variables optimized by
Bayesian optimization techniques should ideally not be too high; otherwise, it
would reduce the eciency of the search process.
4.2. Automated Feature Engineering
The steps of automated feature engineering are shown in Figure 4. Before
this, the data is preprocessed by the framework's preprocessing module, which
focuses on missing value imputation and one-hot encoding of categorical fea-
tures.
Feature Engineering may be the most important thing in a standard ML
pipeline because it determines the upper bound of ML and algorithms can only
approximate this limit. In our approach, we divide feature engineering into two
11

--- PAGE 12 ---
Figure 4: A simple design for automated feature engineering
main parts: feature transformation and feature selection[21][22]. The former
is to transform the raw data into features that better represent the underly-
ing problem of the prediction model, while the latter is to avoid the "curse of
dimensionality" and reducing the computational complexity as well.
For the feature transformation, we further divide it into scaler, feature gener-
ation, matrix decomposition and feature union. The data is duplicated into two
copies after the scaler. One copy of the data goes through a module for feature
generation and the other for matrix decomposition. The two data are united
and passed through the feature selection module to obtain the nal dataset for
training.
The next question is determining which algorithm to use for each module.
Each of these modules has a corresponding selection of each module as a decision
variable. In addition, we use a simple classier as a surrogate model. The score
of the surrogate model after the cross-validation on the training set can be
considered the objective function of the optimization problem. This approach
assumes that a promising feature engineering pipeline will already allow simple
models to achieve relatively good results. It is worth noting that the meaning
of the surrogate function referred to here is not the surrogate function used in
Bayesian optimization. The former is used as a tool for generating values for
the objective function and can be any machine learning model, while the latter
is a surrogate model to approximate the objective function during optimization,
usually using a Gaussian process model with Matern kernel.
12

--- PAGE 13 ---
Next, we introduce each module in more detail.
Scaler. Scaler in our approach refers to a collective term for methods such as
standardization and normalization of data. The data needs to obey certain rule
after the scaler. For example, StandardScaler aims to standardize features by
removing the mean and scaling to unit variance and MaxAbsScaler aims to scale
each feature by its maximum absolute value. Also, RobustScaler can be used
with the data with a lot of outliers that other methods are likely to not work
very well. These are common methods used in machine learning task that can
benet learning process sometimes.
Feature Generation. Sometimes feature generation can be called feature con-
struction as well. It is a process that constructs new features from the basic
feature space or raw data to enhance the robustness and generalizability of the
model. Essentially, this is done to increase the representative ability of the
original features[10]. For example, PolynomialFeatures generates a new feature
matrix consisting of all polynomial combinations of the features with degrees
less than or equal to the specic degree (often set to 2). KBinsDiscretizer can
bin continuous data into intervals. SplineTransformer can generate univariate
B-spline bases of features[23].
Matrix Decomposition. We can call it signal composition either. The goal is
extraction and separation of signal components from composite signals. Signal
decomposition methods are closely related to classication of underlying fea-
tures, which characterize the component to be separated[24]. In AutoDESS we
consider it as a means of feature transformation and combine it with the fea-
ture matrix after feature generation. Some classical methods are included in
this category such as principle component analysis, factor analysis and trun-
cated singular value decomposition.
Feature Selection. Feature selection builds a feature subset based on the origi-
nal feature set by reducing irrelevant or redundant features. Feature selection
can simplify the model that avoid over-tting and improve model performance.
13

--- PAGE 14 ---
This technique is especially important when the dataset has a large dimension-
ality. In our approach, the appropriate one is automatically selected among
multiple feature selection methods. For example, removing features with low
variance according to a threshold, removing all but a user-specied highest
scoring percentage of feature, removing features according to false positive rate,
false discovery rate or family wise error. In addition, recursive feature elimi-
nation is a very strong approach for feature selection even though it is more
time-consuming.
4.3. Automated Ensemble Strategy Selection
Figure 5: The design of Automated Ensemble Strategy Selection
The steps of automated ensemble strategy selection are shown in Figure 5.
The input of this component is the tted model primitives whose hyperparame-
ters have been optimized. We then select a subset of classier primitives based
on the decision variables and perform a probability calibration for each classier
in the subset. The score of the ensemble model on the validation set is used
as the objective function for the Bayesian optimization. In summary, there are
three things to be optimized: what subset of classier primitives we should se-
lect (selection of classier pool), what strategy we should use to ensemble the
model for this subset (Ensemble Strategy Selection), and how the hyperparam-
eters of this strategy should be tuned (The Setting of Decision Variables and
Probability Calibration). We will introduce each of these steps in detail.
14

--- PAGE 15 ---
4.3.1. Decision Variable in Model Selection
The decision variables indicate the optimization process of the component.
Each classier primitive corresponds to a Boolean variable in the subset selec-
tion: True corresponds to joining the subset and vice versa. The ensemble also
has a variable whose value represents which strategy in the ensemble primitives
is used. Finally, we set up decision variables regarding the internal hyperpa-
rameters of the ensemble strategy. Since most of our ensemble strategies belong
to the DES technique, and most of the DES techniques use the KNN model to
calculate the region of competence, kis a critical decision variable that we op-
timize. In addition, we set an extra Boolean variable which, when true, we use
the dynamic frienemy pruning technique (DFP)[25], which will be mentioned
later.
4.3.2. Probability Calibration
Classication models can be divided into probabilistic and non-probabilistic
models: the former outputs the probability that the sample belongs to dierent
classes, and the later gives a denite result through decision function. To use
both non-probabilistic models (support vector machine, ridge classier, etc.)
and probability-based ensemble methods[18][26][27], we use the probability cal-
ibration technique [28] to support probability prediction with non-probabilistic
models.
In our approach, the probability calibration uses Platt's logistic model. Take
a particular model on binary classication as an example. Let the output of
a learning method be f(x). To get calibrated probabilities, pass the output
through a sigmoid:
P(y= 1jf) =1
1 + exp(Af+B)(4)
where the parameters AandBare tted using maximum likelihood estimation
from a tting training set ( fi;yi). Gradient descent is used to nd AandB
such that they are the solution to:
arg min
A;B X
iyilog(pi) + (1 yi) log(1 pi) (5)
15

--- PAGE 16 ---
where
pi=1
1 + exp(Afi+B)(6)
For multiclass predictions, we calibrate each class separately in a one-vs-rest
fashion. When predicting probabilities, the calibrated probabilities for each
class are predicted independently. As those probabilities do not necessarily sum
to one, postprocessing is performed to normalize them.
4.3.3. Ensemble Strategy
Our ensemble strategy primitives fall into three main categories: static se-
lection, dynamic classier selection, and DES. The latter two can be collectively
referred to as DES.
For the static ensemble methods such as static selection and stacked gener-
alization, they take the parameters of the classier pool and are used to build
the ensemble model. The static selection output the results of the ensemble
with the majority voting rule. Stacked generalization is a method for combing
estimators to reduce their biases. Its scheme uses a number of diverse models,
each of which is trained on independent cross-validation examples of the orig-
inal dataset. The outputs of these models, along with the original input data
(optional), are then used as inputs to other generalizers, at a higher level in the
stacking structure.
For the DES, in addition to the classier pool, there is hyperparameter kfor
DES methods because KNN is the key to determining the region of competence
in the DES technique. The Boolean variable DFP is used to decide whether to
use dynamic frienemy pruning. The details and types of DES methods are too
much to be expanded here in this paper, as they can be found in [14][15].
4.3.4. Dynamic Frienemy Pruning
DFP is a technique introduced in [25] that can be used as a pre-selector
to keep only based classiers with decision boundaries crossing the region of
competence of a test sample, helping to dene more precisely the concept of
16

--- PAGE 17 ---
"local competence" evaluation of base classiers of the classication of the test
sample.
This method pre-selects the base classiers by dening the frienemy. For the
classication of a test sample, two samples are frienemies if:
(1) two samples are located in the region of competence of the test sample.
(2) two samples have dierent classes.
The ow of the algorithm for the DFP method is shown below, which is ref-
erenced from the original literature. In our approach, the DFP technique is
controlled through a decision variable optimized in the component.
Algorithm 1: DFP Method
Input: pool of classiers C, region of competence of the test sample  
Output: pool of classier after pruning Cpruned
Cpruned empty ensemble of classier
F all pairs of frienemies in  
forciinCdo
 samples in  correctly classied by ci
Fi frienemies in  ifjFij1then
Cpruned Cpruned[ci
end
end
ifjCprunedj= 0then
Cpruned C
end
5. Experiments
Our experimental section is divided into four parts. In the rst part, we
present the relevant settings for the experiment. In the second part, we intro-
duced the method compared to our proposed approach. After that, we oer
the results and analyze them, and at last, we do additional ablation studies to
validate and analyze the proposed approach.
17

--- PAGE 18 ---
5.1. Experimental Settings
5.1.1. Dataset
We select 42 classical classication datasets from the OpenML platform.
These datasets are diverse, with instances from 500 to 5000 and feature numbers
ranging from 5 to 50, with half being binary and the rest being multiclass. The
datasets include numerical and categorical and may contain missing values and
sample imbalance issues. Table 2 shows the information of the 42 datasets. Here
we need to introduce the "imbalance ratio" index. This is a well know index for
measuring class balance:
IR=Nmaj
Nmin(7)
whereNmajis the sample size of the majority class and Nminis the sample
size of the minority class. If the problem has a high imbalance ratio, it can be
considered more complex than a problem for which the ratio is smaller. There
are also some other variants, and the most classic denition is used here[ ?].
5.1.2. Evaluation Metrics
In addition to the most commonly used accuracy score, we add additional
comparison tests using the F1measure.F1is the harmonic average of recall and
precision, which can consider the accuracy and recall rate of majority and mi-
nority classes simultaneously. Under the experimental conditions in this paper,
F1is a more appropriate evaluation indicator.
Precision =TP
TP+FPRecall =TP
TP+FN(8)
F1=2PrecisionRecall
Precision +Recall(9)
5.1.3. Primitives Conguration
This section shows the specic setup of the proposed method in the experi-
ment.
18

--- PAGE 19 ---
Table 2: Datasets description. Column Nis the number of instances in the dataset, column
Feat is the number of features, column kis the number of classes, column IR is the imbalance
ratio of the dataset.
Dataset N Feat k IR
analcatdata-authorship 841 71 4 5.76
analcatdata-dmft 797 5 6 1.26
autoUniv-au6-750 750 41 8 2.89
autoUniv-au7-1100 1100 13 5 1.99
balance-scale 625 5 3 5.88
banknote-authentication 1372 5 2 1.25
blood-transfusion-service-center 748 5 2 3.20
breast-w 699 10 2 1.90
cardiotocography 2126 36 10 10.92
climate-model-simulation-crashes 540 21 2 10.74
cmc 1473 10 3 1.89
credit-g 1000 21 2 2.33
diabetes 768 9 2 1.87
eucalyptus 736 20 5 2.04
fri-c1-1000-10 1000 11 2 1.29
fri-c2-1000-10 1000 11 2 1.38
ilpd 583 11 2 2.49
kc1 2109 22 2 5.47
mfeat-fourier 2000 77 10 1.00
mfeat-karhunen 2000 65 10 1.00
mfeat-morphological 2000 7 10 1.00
mfeat-zernike 2000 48 10 1.00
monks-problem-2 601 7 2 1.92
pbcseq 1945 19 2 1.00
pc1 1109 22 2 13.4
pc3 1563 38 2 8.77
pc4 1458 38 2 7.19
phoneme 5404 6 2 2.41
qsar-biodeg 1055 42 2 1.96
quake 2178 4 2 1.25
segment 2310 20 7 1
steel-plates-fault 1941 28 7 12.24
stock 950 10 2 1.06
tokyo1 959 45 2 1.77
vehicle 846 19 4 1.10
volcanoes-a4 1515 4 5 47.07
vowel 990 14 2 10
wdbc 569 31 2 1.68
wilt 4839 6 2 17.54
yeast 1484 9 10 92.6
credit-approval 690 16 2 1.25
boston 506 14 2 1.4219

--- PAGE 20 ---
Data Preprocessing & Feature Engineering . Our data preprocessing module
automatically one-hot encodes all categorical features and imputes the missing
value using the average. The table below shows our primitives for each module in
the automated feature engineering. The surrogate classier used in automated
feature engineering is an extra-tree classier. The process is cross-validated on
the training set with a 5-fold.
Table 3: Primitives used in automated feature engineering
Module Primitives
ScalerStandardScaler MaxAbsScaler RobustScaler
Normalizer
Feature GenerationPolynomialFeature KBinsDiscretizer
SplineTransformer Nystroem RBFSampler
Matrix DecompositionFastICA IncrementalICA PCA SparsePCA
TruncatedSVD Factoranalysis
Feature SelectionSelectFwe SelectFdr SelectFpr SelectPercentile
VarianceThreshold RFE
Classier HPO & Automated Ensemble Strategy Selection. We set 18 dierent
classiers and perform HPO before making ensemble selection. It is worth
noting here that the HPO can be shorter in terms of search time because the
performance of a single model is not critical in our approach. We have chosen 23
diernt ensemble strategies as primitives, as shown below. For more information
about primitives, please refer to the relevant open-source library123.
5.2. Compared Methods
It is important to stress that we run all baselines for 1800 seconds on the
same machine to ensure fairness. However, for mljarsupervised, it terminates
1https://scikit-learn.org/stable/
2https://deslib.readthedocs.io/en/latest/
3https://imbalanced-ensemble.readthedocs.io/en/latest/index.html
20

--- PAGE 21 ---
Table 4: Primitives used in automated ensemble strategy selection
Classier PrimitivesHistGradientBoostingClassier
RidgeClassier GaussianNB BernoulliNB
BaggingClassier DecisionTreeClassier
ExtraTreesClassier
RandomForestClassier
GradientBoostingClassier
KNeighborsClassier LinearSVC
SGDClassier LogisticRegression
Perceptron MLPClassier LGBMClassier
PassiveAggressiveClassier
RUSBoostClassier
Ensemble Strategies PrimitivesSingleBest StackedGeneralization
StaticSelection LCA MCB OLA Rank
DESClustering DESKNN DESMI
KNORAE KNORAU KNOP METADES
DESKL Exponential Logarithmic RRC
MinimumDierence APriori APosteriori
21

--- PAGE 22 ---
early. Our search space was far smaller compared to the baselines. Under such
circumstances, the experiments is enough to demonstrate the eectiveness of
the proposed approach. All baselines are run using the AutoML-benchmark
designed by OpenML4.
Auto-sklearn. Auto-sklearn[12] is a tool for building machine learning pipelines.
The pipeline all have a xed structure: a xed set of data cleaning steps in-
cluding optional categorical encoding, imputation, removing variables with low
variance, and optional scaling is executed. Then, an optional preprocessing and
mandatory model algorithm are selected and tuned via the SMAC optimization
method[29].
TPOT. TPOT[2] is a framework for building and tuning arbitrary machine
learning pipelines. It uses genetic programming to construct exible pipelines
and selects an algorithm in each pipeline stage. Regarding HPO, TPOT can
only handle categorical parameters, so all continuous hyperparameters have to
be discretized. TPOT's ability to create arbitrary complex pipelines makes it
prone to overtting. TPOT optimizes a combination of high performance and
low pipeline complexity. Therefore, pipelines are selected from the Pareto front
using a multi-objective selection strategy. But, this approach is quite time-
consuming.
FLAML. FLAML[4] leverages the search space structure to choose a search or-
der optimized for both cost and error. It iteratively decides the learner, hyper-
parameters, sample size, and resampling strategy while leveraging their pound
impact on both cost and error as the search proceeds. The search gradually
moves from cheap trials and accurate models to expensive trials and accurate
models. It is designed for robustly adapting to an ad-hoc dataset out of the box
without relying on expensive preparation such as meta-learning.
4https://openml.github.io/automlbenchmark/
22

--- PAGE 23 ---
MLjar-supervised. mljar-supervised[13] is an AutoML python package that works
with tabular data. It uses many algorithms and can compute ensembles based
on the greedy algorithm. It is a very lightweight AutoML framework, and we
select it as a baseline.
5.3. Results
For each dataset, we use ten dierent random seeds for the training-test split
and ten dierent random seeds for each split for testing, which circumvents both
split and test randomness. Meanwhile, we performed hypothesis testing on the
dierences of the ve algorithms.
5.3.1. AutoDES Performance Analysis
The experimental results for the average accuracy score and F1score on
all datasets are shown in Figure 6. Figures 7 and 8 show the results of each
method of testing on each of our datasets. We use the box-plot to visualize the
overall results, and the box extends from the rst quartile to the third quartile
of the data, with a line at the median. In the scatterplot, the scatter of our
approach is adjusted to be the largest to highlight our ranking on each dataset.
Table 5 shows the specic values of the results while we report the running
time as well. In the table, we bolded the best result among all baselines, and
underlined the second best. We also show the raw data for the experiment in
Table 6. When using equal time budgets, AutoDESS clearly outperforms every
competitor on the majority of the datasets. It is worth noting that the TPOT
framework made errors when processing the four datasets and mljar-supervised
made on one dataset, so the scores in the scatterplot are zero. We did not
consider these failed datasets when calculating the average score. The results
also show that our accuracy is slightly better than baselines. However, the F1
scores are signicantly higher than baselines, indicating the relative advantage
of our method when dealing with imbalanced datasets.
To verify the correctness of the conclusions, we performed hypothesis testing
of the results, as shown in Table 7 and Table 8. We used the Wilcoxon signed-
23

--- PAGE 24 ---
rank test[ ?] as the hypothesis test for this paper because the data obtained
from the 42 datasets did not satisfy a normal distribution and each dataset
existed in a paired form among the ve algorithms. The results show that our
method is signicantly better than baselines, except that there is no signicant
dierence between Auto-sklearn and our method in terms of accuracy, and the
remaining hypothesis tests reject the original hypothesis.
Table 5: Quantication of results
TPOTAuto-
sklearnFLAML mljar AutoDESS
Average ACC 0.82433 0.83260 0.83053 0.81803 0.83804
AverageF1 0.71351 0.73663 0.75107 0.74244 0.77722
Average ACC Rank 2.47368 2.38095 2.78571 3.21951 2.16667
AverageF1Rank 2.88095 2.71429 2.85714 3.19048 2.04762
Average Time (s) 1793 1830 1802 143 998
Figure 6: The boxplot of the comparison result
24

--- PAGE 25 ---
Figure 7: Accuracy score on 42 datasets
25

--- PAGE 26 ---
Figure 8: F1score on 42 datasets
26

--- PAGE 27 ---
Table 6: The raw data of the comparison experiment
dataset name mljarsupervised acc mljarsupervised f1 TPOT acc TPOT f1 auto-sklearn acc auto-sklearn f1 aml acc aml f1 Ours acc Ours f1
analcatdata authorship 0.976471 0.976421 0.976471 0.976421 0.988235 0.988226 0.964706 0.96429 1 1
analcatdata dmft 0.1875 0.164157 0.1875 0.179621 0.225 0.216597 0.2 0.19376 0.225 0.203125
autoUniv-au6-750 0.28 0.196211 0.373333 0.268848 0.253333 0.192689 0.36 0.275965 0.326667 0.286667
autoUniv-au7-1100 0.354545 0.326684 0.390909 0.376153 0.481818 0.463337 0.4 0.376962 0.422727 0.403653
balance-scale 0.929712 0.9241 1 1 0.968051 0.969123 0.961661 0.960817 1 1
banknote-authentication 1 1 1 1 1 1 0.992754 0.99187 1 1
blood-transfusion-service-center 0.733333 0.230769 0.76 0.357143 0.76 0.357143 0.76 0.357143 0.8 0.423077
breast-w 0.957143 0.935622 0 0 0.934286 0.897778 0.957143 0.937759 0.95 0.911111
cardiotocography 1 1 1 1 1 1 1 1 1 1
climate-model-simulation-crashes 0.888889 0.941176 0.833333 0.909091 0.907407 0.951456 0.888889 0.941176 0.888889 0.940594
cmc 0.554953 0.547815 0.548168 0.542057 0.525102 0.516149 0.541384 0.532524 0.501695 0.499904
credit-g 0.726 0.801161 0.742 0.527473 0.738 0.498084 0.768 0.843243 0.765 0.844884
diabetes 0.776042 0.711409 0.755208 0.594828 0.763021 0.588235 0.75 0.571429 0.785714 0.753247
eucalyptus 0.635135 0.607096 0.581081 0.551666 0.635135 0.607241 0.621622 0.613512 0.668919 0.653288
fric1100010 0.92 0.906977 0.94 0.946429 0.93 0.940171 0.94 0.931818 0.93 0.917647
fric2100010 0.93 0.917647 0.95 0.957983 0.94 0.947368 0.93 0.917647 0.945 0.945
ilpd 0.610169 0.342857 0.644068 0.086957 0.694915 0.307692 0.694915 0.181818 0.717949 0.146341
kc1 0.85782 0.347826 0.862559 0.431373 0.862559 0.325581 0.872038 0.425532 0.845972 0.86019
mfeat-fourier 0.82 0.817768 0.835 0.833316 0.856 0.855636 0.828 0.827709 0.875 0.875862
mfeat-karhunen 0.936 0.93613 0 0 0.975 0.975077 0.972 0.971972 0.97 0.970088
mfeat-morphological 0.744 0.735372 0 0 0.713 0.709908 0.728 0.709041 0.755 0.750774
mfeat-zernike 0.84 0.837109 0 0 0.823 0.820441 0.794 0.789394 0.875 0.875068
monks-problems-2 0.983607 0.976744 1 1 1 1 1 1 1 1
pbcseq 0.789744 0.783069 0.835897 0.829787 0.85641 0.852632 0.897436 0.9 0.791774 0.782609
pc1 0.945946 0.4 0.954955 0.444444 0.954955 0.444444 0.945946 0.25 0.954955 0.545455
pc3 0.853503 0.30303 0.88535 0.25 0.898089 0.111111 0.878981 0.24 0.897764 0.25
pc4 0.883562 0.622222 0.876712 0.526316 0.910959 0.606061 0.938356 0.709677 0.90411 0.474576
phoneme 0.900185 0.833333 0.907579 0.845679 0.896488 0.820513 0.914972 0.855346 0.880666 0.878816
qsar-biodeg 0.896226 0.84058 0.849057 0.777778 0.896226 0.84507 0.858491 0.788732 0.886256 0.846343
quake 0.568807 0.65942 0.555046 0 0.555046 0.23622 0.518349 0.672897 0.529817 0.682853
segment 0.977489 0.977536 0.986147 0.986145 0.984416 0.984417 0.991342 0.991336 0.987013 0.974124
steel-plates-fault 1 1 1 1 1 1 1 1 1 1
stock 0.978947 0.979167 0.989474 0.989247 0.968421 0.967742 0.968421 0.968421 0.963158 0.963158
tokyo1 0 0 0.9375 0.952381 0.927083 0.944 0.947917 0.95935 0.9375 0.95082
vehicle 0.758865 0.755618 0.775414 0.77971 0.78487 0.779419 0.756501 0.753764 0.817647 0.810864
volcanoes-a4 0.907895 0.871393 0.907895 0.87944 0.914474 0.883915 0.901316 0.870751 0.943894 0.905206
vowel 1 1 1 1 1 1 1 1 1 1
wdbc 0.982456 0.976744 0.982456 0.976744 0.964912 0.954545 0.982456 0.976744 1 1
wilt 0.975207 0.76 0.983471 0.833333 0.985537 0.862745 0.979339 0.791667 0.987603 0.88
yeast 0.697987 0.686872 0.697987 0.686777 0.711409 0.704862 0.691275 0.68277 0.612795 0.606061
credit-approval 0.898551 0.906667 0.898551 0.911392 0.884058 0.894737 0.884058 0.9 0.913043 0.912674
boston 0.882353 0.903226 0.921569 0.904762 0.901961 0.918033 0.901961 0.918033 0.941176 0.919118
27

--- PAGE 28 ---
Table 7: Hypothesis Test of Accuracy between our approach and baselines
Median (P25;P75) Median Dierence Statistic Z values p
Our Acc vs TPOT 0.842(0.6,0.9) 0.045 2.482 0.013*
Auto-sklearn 0.896(0.7,0.9) -0.009 1.83 0.067
FLAML 0.886(0.8,0.9) 0.001 2.023 0.043*
mljar 0.870(0.7,0.9) 0.017 3.357 0.001**
Table 8: Hypothesis Test of F1 between our approach and baselines
Median (P25;P75) Median Dierence Statistic Z values p
Our F1 vs TPOT 0.732(0.4,0.9) 0.135 3.315 0.001**
Auto-sklearn 0.833(0.5,0.9) 0.035 2.11 0.035*
FLAML 0.810(0.6,0.9) 0.058 2.386 0.017*
mljar 0.792(0.6,0.9) 0.076 2.777 0.005**
5.3.2. Further Analysis
To ensure the eectiveness of our method, we do two additional ablation
experiments. Since our approach is to enrich the diversity of ensemble strategy,
then we x the ensemble strategy to the stacked generalization approach popular
in current AutoML frameworks and compare it. In addition, we compare our
approach without enabling the DFP method. The results are shown in Figures
9 and 10. As shown, the vertical axis is the score of the framework of the
proposed method, while the horizontal axis is the score of the experiment after
ablation. Data points above the line of the y=xfunction represent better
performance before ablation. It can be seen that the module of our proposed
ensemble strategy selection method does improve the performance and the DFP
method also enhances the performance of the model.
To demonstrate the superiority of our method in dealing with the sample
imbalance problem of the dataset, we singled out and compared the datasets
with imbalance ratio greater than 2. The results show that our method achieves
a more signicant advantage in the imbalance datasets.
28

--- PAGE 29 ---
Figure 9: Ablation study of DES selection, the result for accuracy (left) and F1(right)
Figure 10: Ablation study of DFP, the result for accuracy (left) and F1(right)
29

--- PAGE 30 ---
Table 9: Comparison on datasets with imbalance issue
dataset name mljarsupervised f1 TPOT F1 auto-sklearn f1 FLAML f1 AutoDESS f1
analcatdata authorship 0.976421 0.976421 0.988226 0.96429 1
autoUniv-au6-750 0.196211 0.268848 0.192689 0.275965 0.286667
balance-scale 0.9241 1 0.969123 0.960817 1
blood-transfusion-service-center 0.230769 0.357143 0.357143 0.357143 0.423077
cardiotocography 1 1 1 1 1
climate-model-simulation-crashes 0.941176 0.909091 0.951456 0.941176 0.940594
credit-g 0.801161 0.527473 0.498084 0.843243 0.844884
eucalyptus 0.607096 0.551666 0.607241 0.613512 0.653288
ilpd 0.342857 0.086957 0.307692 0.181818 0.146341
kc1 0.347826 0.431373 0.325581 0.425532 0.86019
pc1 0.4 0.444444 0.444444 0.25 0.545455
pc3 0.30303 0.25 0.111111 0.24 0.25
pc4 0.622222 0.526316 0.606061 0.709677 0.474576
phoneme 0.833333 0.845679 0.820513 0.855346 0.878816
steel-plates-fault 1 1 1 1 1
volcanoes-a4 0.871393 0.87944 0.883915 0.870751 0.905206
vowel 1 1 1 1 1
wilt 0.76 0.833333 0.862745 0.791667 0.88
yeast 0.686872 0.686777 0.704862 0.68277 0.606061
The last two gures show our overall statistics for the pipeline generated
for all datasets. Figure 11 is a histogram of the ensemble strategies, which
shows that a wide variety of strategies were selected for the ensemble. Stacked
generalization is indeed a very powerful method resulting in the highest number
of selections. Figure 12 shows the heatmap for the combination of classiers.
Each block represents the number of times that combination appears in all pool
classiers. Here we only made a simple visualization that illustrates the method
while revealing the intrinsic relationships between models. It also inspires us
to improve by performing further data analysis on the generated pipeline or by
using meta-learning techniques further.
30

--- PAGE 31 ---
Figure 11: Statistics on ensemble strategies in the generated pipeline
Figure 12: Heatmap of classier primitives combination
31

--- PAGE 32 ---
6. Conclusion
We proposed AutoDESS, an automatic approach of learning ensembles with
many dierent single classiers with diverse strategies. Compared to 42 dif-
ferent datasets, AutoDESS was able to outperform the current state-of-the-art
AutoML for the large majority of the datasets in the same CPU time. Fur-
thermore, we prove that our pro-posed approach is eective in an additional
ablation study.
Our method can be further enhanced in some aspects. In literature[30], it is
pointed out that some aspects and steps of data preprocessing can have some
inuence on the model results, however, data preprocessing is not automated in
our method. Besides, the parameter search space of primitives can be further
increased to achieve better performance.
References
[1] D. H. Wolpert, Stacked generalization, Neural Networks 5 (2) (1992) 241{
259. doi:10.1016/S0893-6080(05)80023-1 .
[2] R. S. Olson, J. H. Moore, TPOT: A tree-based pipeline optimization tool
for automating machine learning, in: F. Hutter, L. Kottho, J. Vanschoren
(Eds.), Automated Machine Learning - Methods, Systems, Challenges, The
Springer Series on Challenges in Machine Learning, Springer, 2019, pp.
151{160. doi:10.1007/978-3-030-05318-5\_8 .
[3] N. Erickson, J. Mueller, A. Shirkov, H. Zhang, P. Larroy, M. Li, A. J. Smola,
Autogluon-tabular: Robust and accurate automl for structured data, CoRR
abs/2003.06505. arXiv:2003.06505 .
URL https://arxiv.org/abs/2003.06505
[4] C. Wang, Q. Wu, M. Weimer, E. Zhu, FLAML: A fast and lightweight
automl library, in: A. Smola, A. Dimakis, I. Stoica (Eds.), Proceedings of
Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9, 2021,
mlsys.org, 2021.
32

--- PAGE 33 ---
[5] E. LeDell, S. Poirier, H2o automl: Scalable automatic machine learning,
in: Proceedings of the AutoML Workshop at ICML, Vol. 2020, 2020.
[6] B. Chen, H. Wu, W. Mo, I. Chattopadhyay, H. Lipson, Autostacker: A
compositional evolutionary learning system, in: Proceedings of the genetic
and evolutionary computation conference, 2018, pp. 402{409.
[7] M. Wistuba, N. Schilling, L. Schmidt-Thieme, Automatic frankensteining:
Creating complex ensembles autonomously, in: Proceedings of the 2017
SIAM International Conference on Data Mining, SIAM, 2017, pp. 741{749.
[8] D. H. Wolpert, W. G. Macready, No free lunch theorems for optimization,
IEEE transactions on evolutionary computation 1 (1) (1997) 67{82.
[9] S. Reid, G. Grudic, Regularized linear models in stacked generalization, in:
International Workshop on Multiple Classier Systems, Springer, 2009, pp.
112{121.
[10] X. He, K. Zhao, X. Chu, Automl: A survey of the state-of-the-art,
Knowledge-Based Systems 212 (2021) 106622.
[11] M.-A. Z oller, M. F. Huber, Benchmark and survey of automated machine
learning frameworks, Journal of articial intelligence research 70 (2021)
409{472.
[12] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, F. Hut-
ter, Ecient and robust automated machine learning, Advances in neural
information processing systems 28.
[13] A. P lo nska, P. P lo nski, Mljar: State-of-the-art automated machine learning
framework for tabular data. version 0.10.3 (2021).
URL https://github.com/mljar/mljar-supervised
[14] A. S. Britto Jr, R. Sabourin, L. E. Oliveira, Dynamic selection of classi-
ers|a comprehensive review, Pattern recognition 47 (11) (2014) 3665{
3680.
33

--- PAGE 34 ---
[15] R. M. Cruz, R. Sabourin, G. D. Cavalcanti, Dynamic classier selection:
Recent advances and perspectives, Information Fusion 41 (2018) 195{216.
[16] L. I. Kuncheva, Clustering-and-selection model for classier combination,
in: KES'2000. Fourth International Conference on Knowledge-Based Intel-
ligent Engineering Systems and Allied Technologies. Proceedings (Cat. No.
00TH8516), Vol. 1, IEEE, 2000, pp. 185{188.
[17] R. G. Soares, A. Santana, A. M. Canuto, M. C. P. de Souto, Using accuracy
and diversity to select classiers to build ensembles, in: The 2006 IEEE In-
ternational Joint Conference on Neural Network Proceedings, IEEE, 2006,
pp. 1310{1316.
[18] T. Woloszynski, M. Kurzynski, P. Podsiadlo, G. W. Stachowiak, A mea-
sure of competence based on random classication for dynamic ensemble
selection, Information Fusion 13 (3) (2012) 207{213.
[19] R. M. Cruz, L. G. Hafemann, R. Sabourin, G. D. Cavalcanti, Deslib: A
dynamic ensemble selection library in python., J. Mach. Learn. Res. 21 (8)
(2020) 1{5.
[20] L. Yang, A. Shami, On hyperparameter optimization of machine learning
algorithms: Theory and practice, Neurocomputing 415 (2020) 295{316.
[21] H. Motoda, H. Liu, Feature selection, extraction and construction, Com-
munication of IICM (Institute of Information and Computing Machinery,
Taiwan) 5 (67-72) (2002) 2.
[22] M. Dash, H. Liu, Feature selection for classication, Intelligent data anal-
ysis 1 (1-4) (1997) 131{156.
[23] A. Perperoglou, W. Sauerbrei, M. Abrahamowicz, M. Schmid, A review of
spline function procedures in r, BMC medical research methodology 19 (1)
(2019) 1{16.
34

--- PAGE 35 ---
[24] J.-R. Ohm, Signal Decomposition, Springer Berlin Heidelberg, Berlin, Hei-
delberg, 2004, pp. 417{442. doi:10.1007/978-3-642-18750-6_10 .
URL https://doi.org/10.1007/978-3-642-18750-6_10
[25] D. V. Oliveira, G. D. Cavalcanti, R. Sabourin, Online pruning of base
classiers for dynamic ensemble selection, Pattern Recognition 72 (2017)
44{58.
[26] L. Didaci, G. Giacinto, F. Roli, G. L. Marcialis, A study on the perfor-
mances of dynamic classier selection based on local accuracy estimation,
Pattern recognition 38 (11) (2005) 2188{2191.
[27] T. Woloszynski, M. Kurzynski, A probabilistic model of classier com-
petence for dynamic ensemble selection, Pattern Recognition 44 (10-11)
(2011) 2656{2668.
[28] A. Niculescu-Mizil, R. Caruana, Predicting good probabilities with super-
vised learning, in: Proceedings of the 22nd international conference on
Machine learning, 2005, pp. 625{632.
[29] M. Lindauer, K. Eggensperger, M. Feurer, A. Biedenkapp, D. Deng, C. Ben-
jamins, T. Ruhkopf, R. Sass, F. Hutter, Smac3: A versatile bayesian op-
timization package for hyperparameter optimization, Journal of Machine
Learning Research 23 (54) (2022) 1{9.
URL http://jmlr.org/papers/v23/21-0888.html
[30] P. Li, X. Rao, J. Blase, Y. Zhang, X. Chu, C. Zhang, Cleanml: A
study for evaluating the impact of data cleaning on ML classication
tasks, in: 37th IEEE International Conference on Data Engineering,
ICDE 2021, Chania, Greece, April 19-22, 2021, IEEE, 2021, pp. 13{24.
doi:10.1109/ICDE51399.2021.00009 .
URL https://doi.org/10.1109/ICDE51399.2021.00009
35
