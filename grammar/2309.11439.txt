# 2309.11439.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/grammar/2309.11439.pdf
# File size: 294514 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Controlled Generation with Prompt Insertion for Natural Language
Explanations in Grammatical Error Correction
Masahiro Kaneko1,2Naoaki Okazaki2
1MBZUAI
2Tokyo Institute of Technology
Masahiro.Kaneko@mbzuai.ac.ae okazaki@c.titech.ac.jp
Abstract
In Grammatical Error Correction (GEC), it is
crucial to ensure the user’s comprehension of a
reason for correction. Existing studies present
tokens, examples, and hints as to the basis for
correction but do not directly explain the rea-
sons for corrections. Although methods that
use Large Language Models (LLMs) to pro-
vide direct explanations in natural language
have been proposed for various tasks, no such
method exists for GEC. Generating explana-
tions for GEC corrections involves aligning in-
put and output tokens, identifying correction
points, and presenting corresponding explana-
tions consistently. However, it is not straightfor-
ward to specify a complex format to generate
explanations, because explicit control of gen-
eration is difficult with prompts. This study
introduces a method called controlled genera-
tion with Prompt Insertion (PI) so that LLMs
can explain the reasons for corrections in nat-
ural language. In PI, LLMs first correct the
input text, and then we automatically extract
the correction points based on the rules. The
extracted correction points are sequentially in-
serted into the LLM’s explanation output as
prompts, guiding the LLMs to generate ex-
planations for the correction points. We also
create an Explainable GEC (XGEC) dataset
of correction reasons by annotating NUCLE,
CoNLL2013, and CoNLL20141. Although
generations from GPT-3 and ChatGPT using
original prompts miss some correction points,
the generation control using PI can explicitly
guide to describe explanations for all correction
points, contributing to improved performance
in generating correction reasons.
1 Introduction
Grammatical Error Correction (GEC) is the task of
correcting grammatical errors in a text. In GEC,
various methods have been proposed from a wide
1https://github.com/kanekomasahiro/
gec-explanation
Figure 1: How to generate an explanation of the pro-
posed method PI.
range of perspectives, including correction per-
formance (Grundkiewicz and Junczys-Dowmunt,
2019; Chollampatt et al., 2019; Omelianchuk et al.,
2020; Kaneko et al., 2020; Qorib et al., 2022), con-
trolling (Hotate et al., 2019; Yang et al., 2022;
Loem et al., 2023), diversity (Xie et al., 2018;
Hotate et al., 2020; Han and Ng, 2021), and ef-
ficiency (Malmi et al., 2019; Chen et al., 2020).
It is also important in GEC for the model to pro-
vide explanations that allow users to understand the
reasons behind the corrections. Improving explain-
ability leads to a better judgment of whether the
correction reflects the intended result, learning of
grammatical knowledge, and overall enhancement
of GEC systems.
Kaneko et al. (2022) introduced a method of
presenting the retrieved examples as the basis for
correction, in contrast to a method of retrieving
data similar to the correction target from the train-
ing data set and using it for prediction. Fei et al.
(2023) proposed a method that presents the token
positions that are the basis of errors and error types,
and showed that they are useful for learners. Na-
gata (2019) proposed the task of generating usefularXiv:2309.11439v1  [cs.CL]  20 Sep 2023

--- PAGE 2 ---
hints and feedback for language learning on essays
written by language learners. This task does not
necessarily generate a correction result or reason,
because it is not intended for correction. Since
these existing studies do not directly explain the
reason for the correction, the user must infer the
reason for the correction from the system output.
Large Language Models (LLMs) such as Chat-
GPT (OpenAI, 2023) and GPT-3 (Brown et al.,
2020) have advanced language capabilities and can
explain the inference reasons in natural language
in various tasks (Wei et al., 2022; Wiegreffe et al.,
2022; Kaneko et al., 2023). With natural language,
the model can directly explain the details of the
inference reasons to the user. LLMs are also ef-
fective in GEC, achieving state-of-the-art in both
unsupervised (Loem et al., 2023) and supervised
settings (Kaneko and Okazaki, 2023). Explicabil-
ity in GEC first requires the alignment of input
and output tokens and identifies all error and cor-
rection pairs. Then, it is necessary to generate an
explanation for each of the extracted pairs. How-
ever, it is hard to control the generation of prompts
in a specified format for GEC. Fang et al. (2023)
showed that ChatGPT improves performance by us-
ing natural language to generate step-by-step error
detection and correction processes for each span. It
was found that it is difficult for ChatGPT to gener-
ate step-by-step according to the specified format
with simple prompt instructions. Loem et al. (2023)
showed that prompting did not contribute signifi-
cantly to the control of correction style for GPT-3.
In this study, we introduce a method to explain
the reason for correction in natural language by a
controlled generation with prompt insertion (PI).
As shown in Figure 1, we guide LLMs to the de-
sired format output by inserting prompts during in-
ference. First, LLM corrects grammatical errors in
the input text. Then, we automatically align the er-
ror and correction points from the input and output
text using rules and extract error-correction pairs.
By inserting these error-correction pairs as addi-
tional prompts, we explicitly control the LLM’s
explanation of the reasons for all pairs. Further-
more, we created an Explainable GEC (XGEC)
dataset for explaining correction reasons in natural
language by annotating NUCLE, CoNLL2013, and
CoNLL2014 datasets (Dahlmeier et al., 2013; Ng
et al., 2013, 2014).
In our experiments on GPT-3 and ChatGPT, we
found that the original prompt-based generation re-sulted in pair omissions and ambiguity as to which
pair the explanation was for. On the other hand,
the control of generation by PI can explicitly con-
trol the LLM to generate explanations for all the
corrections, which contributes to the performance
improvement of the explanation of correction rea-
sons.
2 PI to Generate Natural Language
Explanations
Existing methods to generate an explanation use
only instructions to guide LLMs in generating ex-
planations (Wei et al., 2022; Wiegreffe et al., 2022;
Chen et al., 2023; Kaneko et al., 2023). LLMs with
instruction alone do not necessarily generate the
explanation in the proper format covering all ed-
its. Our method solves this problem by inserting
prompts of edits into the input during generation
and explicitly guiding the LLM to generate expla-
nations for all edits.
Specifically, we give the LLM the instruction
to rewrite the input text (e.g. “What is the differ-
ence between genetic disorder and other disorders
.”) into grammatically correct text (e.g. “What is
the difference between genetic disorders and other
disorders ?”) and explain the corrections. We com-
pute the token alignment between the input and
output text and extract the edits such as ( “disorder
→disorders” ) or ( “.→?”). The extracted edits
are given to the LLM one by one as additional in-
put, causing the LLM to generate an explanation
corresponding to each edit. We assign numbers to
edits, such as ( “1. disorder →disorders:” ) or ( “2.
.→?:”). We assign numbers to the edits sequen-
tially from the beginning, such as ( “1. disorder →
disorders:” ) or ( “2. .→?:”).
3 Creating XGEC Dataset
The XGEC dataset includes incorrect texts, correct
texts, and explanations for each edit within the
correct texts. We annotated explanations for the
original edits in existing GEC datasets to create
training, development, and test datasets.
NUCLE (Dahlmeier et al., 2013),
CoNLL2013 (Ng et al., 2013), and
CoNLL2014 (Ng et al., 2014) datasets are
used as training, development, and test datasets,
respectively. NUCLE and CoNLL2013 contain
only one correct text per incorrect text. For
NUCLE and CoNLL2013, we randomly selected
362 correct texts and annotated explanations for

--- PAGE 3 ---
XGECa XGECb
Precision Recall F1 Precision Recall F1
ChatGPTPost w/ PI 83.2 85.5 84.3 83.9 84.5 84.2
Post w/o PI 62.1 79.6 70.0 62.6 78.2 69.6
Pre w/o PI 60.9 75.2 68.1 61.1 74.4 67.7
GPT-3.5Post w/ IP 81.2 83.8 82.4 82.0 83.0 82.5
Post w/o IP 61.2 79.4 69.1 61.8 78.1 69.0
Pre w/o IP 59.9 75.6 67.7 60.7 75.5 68.1
Table 1: The BERTScore of GPT-3.5 and ChatGPT in generating explanations with and without PI on the XGEC
test datasets.
them. CoNLL2014 consists of aandbdatasets,
which were created by different annotators and
are commonly used to evaluating GEC models.
Consequently, we also use CoNLL2014 aandb
for the XGEC test dataset. To reduce the number
of cases in the test dataset where our annotators
disagree with the original edit, we selected edits
that are widely considered appropriate by most
humans. CoNNL2014 also includes additional 8
annotations (Bryant and Ng, 2015). Therefore, we
annotated explanations for only those edits that
appeared in at least 7 out of 10 correct texts within
CoNLL2014 aandb, resulting in a total of 444
correct texts
Two native English speakers2were responsible
for annotating explanations for the edits. Anno-
tators were provided with incorrect texts, correct
texts, and the corresponding edits, and they were
tasked with writing an explanation for each edit
in a free-writing format. For the 10 correct texts
that were not included in the annotation dataset, we
provided example explanations, and the annotators
used these examples as references. In the case of
NUCLE and CoNLL2013, two annotators were as-
signed to write explanations for each half of the
correct text. For CoNLL2014, two annotators were
designated to write explanations, resulting in the
creation of two references. In total, we obtained
888 texts.
4 Experiment
4.1 Setting
We used the following text as the instruction: “Cor-
rect the input text grammatically and explain the
reason for each correction. If the input text is gram-
matically correct, only the input text should be gen-
erated as is. ” . We used text-davinci-003 for
GPT-3.5 and gpt-3.5-turbo-16k for ChatGPT in
2We compensated each annotator with a payment of $4 per
explanation.OpenAI API3. The number of examples for few-
shot is 16. The examples contain input texts, cor-
rect texts, edits, and explanations. We used the
ERRANT (Felice et al., 2016; Bryant et al., 2017)4
as the token alignment. We automatically evaluated
the performance to generate explanations with the
BERTScore (Zhang et al., 2019) of reference text
and output text on CoNLL2014.
We compare our method with two baselines
that generate explanations without inserting edit
prompts. The first baseline generates a corrected
sentence and an explanation all at once. We demon-
strate the effectiveness of explicitly providing edits
and generating explanations through a comparison
with this baseline. The second baseline generates
an explanation all at once and then generates a
corrected sentence.
The second baseline generates an explanation
all at once and then generates a corrected sentence.
We compare a baseline model that generates edits
and explanations step by step before generating the
entire corrected text, like a chain of thought, with a
model that generates explanations after the entire
corrected text. This demonstrates the effectiveness
of generating explanations after correction.5
4.2 The Performance of Generating
Explanations
Table 1 shows precision, recall, and F1 scores with
BERTScore of GPT-3.5 and ChatGPT in generat-
ing explanations with and without PI on XGECa
and XGECb datasets. The scores of the GPT-3.5
and ChatGPT with PI are better than the models
without PI in all scores on both datasets. The per-
formance improvement is believed to result from
3https://platform.openai.com/docs/models/
overview
4https://github.com/chrisjbryant/errant
5The proposed method cannot be applied to the process of
generating explanations before correction, as it requires edits
extracted from correction to generate explanations.

--- PAGE 4 ---
Validity Coverage
ChatGPTPost w/ PI 81.5 100.0
Post w/o PI 78.0 77.5
Pre w/o PI 76.5 71.5
GPT-3.5Post w/ PI 86.5 100.0
Post w/o PI 83.5 72.0
Pre w/o PI 83.5 69.5
Table 2: Human evaluations of GPT-3.5 and ChatGPT
with and without PI on the XGEC test dataset.
enhanced coverage of edits included in the explana-
tions generated by the PI. Moreover, it can be seen
from the results of Post w/o PI and Pre w/o PI that
generating explanations after correction is more
effective than generating them before correction.
5 Analysis
5.1 Human Evaluation
We examine the quality of LLM-generated expla-
nations by human evaluation. We sample 200 ex-
planations from CoNLL2013, and four human an-
notators evaluate those explanations from validity
and coverage perspectives. The validity perspective
refers to the accuracy and usefulness of grammati-
cal information in LLM-generated explanations for
language learners. It is scored on three levels: 0 if
the explanation for more than half of corrections
is incorrect and unuseful, 1 if the explanation for
more than half of corrections is correct and useful
but not perfect, 2 if the explanation for all correc-
tions is perfect. The coverage perspective means
that the LLM-generated explanation mentions all
grammatical corrections. It is scored on three lev-
els: 0 if the explanation does not cover more than
half of the corrections, 1 if the explanation covers
more than half of the corrections but not all correc-
tions, 2 if the explanation covers all corrections.
Table 2 shows the results of validity and cover-
age scores from human annotators for GPT-3.5 and
ChatGPT, both with and without PI. Both the valid-
ity and coverage scores for GPT-3.5 and ChatGPT
using PI are better than those not using PI. The PI
makes it clear to LLM the corrections that need to
be explained, and allows for specific explanations
tied to each correction, improving the quality of
LLM’s explanations. The coverage scores show
that by explicitly instructing correction positions
using the proposed method, LLM can generate ex-
planations that completely cover the edits. More-
over, comparing the post-generating models andthe pre-generation model demonstrates that gener-
ating an explanation before a correction has more
negative effects in terms of the coverage of edits
than generating an explanation after a correction.
5.2 Impact of Explanation on GEC
performance
Providing explanations in addition to gold answers
to the LLM as few-shot examples improves perfor-
mance for tasks (Wei et al., 2022; Kaneko et al.,
2023). We evaluate a model’s ability to generate
explanations by assessing their impact on GEC
performance. It is believed that if the quality of
the generated explanation is high, the GEC per-
formance will improve to the same extent as with
human explanations. Conversely, if the quality
is poor, the performance will not be as good as
with human explanations. We randomly sample
8 instances from the XGEC valid dataset to use
as few-shot examples. To include more generated
explanatory text for evaluation, we perform ran-
dom sampling for each instance in the test data to
select few-shot examples. These examples consist
of human-written explanations and explanations
generated by the PI, inserted both before and af-
ter the corrected text, allowing us to compare their
effectiveness, respectively.
Table 3 displays the GEC performance of GPT-
3.5 and ChatGPT using explanatory texts as ex-
amples for few-shot learning in the CoNLL2014,
W&I, and JFLEG test datasets. Comparing the
results without explanations to the results with ex-
planations, it is evident that using explanations as
examples for few-shot learning improves GEC per-
formance. When comparing the results of human-
authored explanatory text and text generated by
the PI, both achieve nearly equivalent GEC per-
formance. This suggests that the explanatory text
generated by the PI is of the same quality as the
explanatory text authored by humans. Furthermore,
it can be observed that adding explanatory text be-
fore or after correction for few-shot learning has
little influence.
6 Conclusion
In this study, we introduce a method for generating
comprehensive and high-quality explanatory text
in LLMs by explicitly instructing the edits. Addi-
tionally, we have created the XGEC dataset for ex-
planatory text generation. The experimental results
demonstrate that our approach, compared to meth-

--- PAGE 5 ---
CoNLL2014 W&I JFLEG
ChatGPTPre Human 55.2 51.2 61.7
Post Human 54.8 51.5 61.5
Pre PI 54.9 51.7 61.5
Post PI 54.7 49.7 61.8
No explanation 52.3 40.1 55.3
GPT-3.5Pre Human 54.0 44.2 57.8
Post Human 54.5 44.0 57.3
Pre PI 53.7 44.2 57.1
Post PI 54.1 39.9 57.1
No explanation 50.1 35.8 53.7
Table 3: The GEC performance of GPT-3.5 and ChatGPT when using explanation text as examples for few-shot
methods.
ods that do not explicitly provide edits to LLMs
for explanatory text generation, yields benefits in
both human evaluation and automated evaluation.
In future work, we plan to investigate the impact
of LLM-generated explanatory text on language
learners.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Christopher Bryant, Mariano Felice, and Ted Briscoe.
2017. Automatic annotation and evaluation of error
types for grammatical error correction. In Proceed-
ings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 793–805, Vancouver, Canada. Association for
Computational Linguistics.
Christopher Bryant and Hwee Tou Ng. 2015. How far
are we from fully automatic high quality grammatical
error correction? In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 697–707, Beijing, China. Association
for Computational Linguistics.
Mengyun Chen, Tao Ge, Xingxing Zhang, Furu Wei,
and Ming Zhou. 2020. Improving the efficiency of
grammatical error correction with erroneous span de-
tection and correction. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 7162–7169, On-
line. Association for Computational Linguistics.
Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao,
He He, Jacob Steinhardt, Zhou Yu, and Kathleen
McKeown. 2023. Do models explain themselves?
counterfactual simulatability of natural language ex-
planations. arXiv preprint arXiv:2307.08678 .Shamil Chollampatt, Weiqi Wang, and Hwee Tou Ng.
2019. Cross-sentence grammatical error correction.
InProceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics , pages 435–
445, Florence, Italy. Association for Computational
Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS corpus of learner English. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications , pages
22–31, Atlanta, Georgia. Association for Computa-
tional Linguistics.
Tao Fang, Shu Yang, Kaixin Lan, Derek F Wong, Jin-
peng Hu, Lidia S Chao, and Yue Zhang. 2023. Is
chatgpt a highly fluent grammatical error correction
system? a comprehensive evaluation. arXiv preprint
arXiv:2304.01746 .
Yuejiao Fei, Leyang Cui, Sen Yang, Wai Lam, Zhen-
zhong Lan, and Shuming Shi. 2023. Enhancing gram-
matical error correction systems with explanations.
arXiv preprint arXiv:2305.15676 .
Mariano Felice, Christopher Bryant, and Ted Briscoe.
2016. Automatic extraction of learner errors in ESL
sentences using linguistically enhanced alignments.
InProceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers , pages 825–835, Osaka, Japan. The
COLING 2016 Organizing Committee.
Roman Grundkiewicz and Marcin Junczys-Dowmunt.
2019. Minimally-augmented grammatical error cor-
rection. In Proceedings of the 5th Workshop on Noisy
User-generated Text (W-NUT 2019) , pages 357–363,
Hong Kong, China. Association for Computational
Linguistics.
Wenjuan Han and Hwee Tou Ng. 2021. Diversity-driven
combination for grammatical error correction. In
2021 IEEE 33rd International Conference on Tools
with Artificial Intelligence (ICTAI) , pages 972–979.
IEEE.

--- PAGE 6 ---
Kengo Hotate, Masahiro Kaneko, Satoru Katsumata,
and Mamoru Komachi. 2019. Controlling grammati-
cal error correction using word edit rate. In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics: Student Research Work-
shop, pages 149–154, Florence, Italy. Association for
Computational Linguistics.
Kengo Hotate, Masahiro Kaneko, and Mamoru Ko-
machi. 2020. Generating diverse corrections with
local beam search for grammatical error correction.
InProceedings of the 28th International Conference
on Computational Linguistics , pages 2132–2137,
Barcelona, Spain (Online). International Committee
on Computational Linguistics.
Masahiro Kaneko, Masato Mita, Shun Kiyono, Jun
Suzuki, and Kentaro Inui. 2020. Encoder-decoder
models can benefit from pre-trained masked language
models in grammatical error correction. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 4248–4254, On-
line. Association for Computational Linguistics.
Masahiro Kaneko, Graham Neubig, and Naoaki
Okazaki. 2023. Solving nlp problems through
human-system collaboration: A discussion-based ap-
proach. arXiv preprint arXiv:2305.11789 .
Masahiro Kaneko and Naoaki Okazaki. 2023. Re-
ducing sequence length by predicting edit opera-
tions with large language models. arXiv preprint
arXiv:2305.11862 .
Masahiro Kaneko, Sho Takase, Ayana Niwa, and Naoaki
Okazaki. 2022. Interpretability for language learners
using example-based grammatical error correction.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 7176–7187, Dublin, Ireland.
Association for Computational Linguistics.
Mengsay Loem, Masahiro Kaneko, Sho Takase, and
Naoaki Okazaki. 2023. Exploring effectiveness of
gpt-3 in grammatical error correction: A study on per-
formance and controllability in prompt-based meth-
ods. arXiv preprint arXiv:2305.18156 .
Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil
Mirylenka, and Aliaksei Severyn. 2019. Encode, tag,
realize: High-precision text editing. In Proceedings
of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 5054–5065, Hong Kong,
China. Association for Computational Linguistics.
Ryo Nagata. 2019. Toward a task of feedback comment
generation for writing learning. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3206–3215, Hong Kong,
China. Association for Computational Linguistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task , pages 1–14,
Baltimore, Maryland. Association for Computational
Linguistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
InProceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1–12, Sofia, Bulgaria. Association for
Computational Linguistics.
Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem
Chernodub, and Oleksandr Skurzhanskyi. 2020.
GECToR – grammatical error correction: Tag, not
rewrite. In Proceedings of the Fifteenth Workshop
on Innovative Use of NLP for Building Educational
Applications , pages 163–170, Seattle, WA, USA →
Online. Association for Computational Linguistics.
OpenAI. 2023. Introducing ChatGPT. Accessed on
2023-05-10.
Muhammad Qorib, Seung-Hoon Na, and Hwee Tou
Ng. 2022. Frustratingly easy system combination
for grammatical error correction. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1964–1974,
Seattle, United States. Association for Computational
Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,
Mark Riedl, and Yejin Choi. 2022. Reframing
human-AI collaboration for generating free-text ex-
planations. In Proceedings of the 2022 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 632–658, Seattle, United States.
Association for Computational Linguistics.
Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew
Ng, and Dan Jurafsky. 2018. Noising and denois-
ing natural language: Diverse backtranslation for
grammar correction. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers) ,
pages 619–628, New Orleans, Louisiana. Associa-
tion for Computational Linguistics.
Liner Yang, Chengcheng Wang, Yun Chen, Yongping
Du, and Erhong Yang. 2022. Controllable data syn-
thesis method for grammatical error correction. Fron-
tiers of Computer Science , 16:1–10.

--- PAGE 7 ---
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv:1904.09675 .
