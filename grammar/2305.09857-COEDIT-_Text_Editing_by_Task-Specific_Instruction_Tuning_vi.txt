# 2305.09857.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/grammar/2305.09857.pdf
# Kích thước tệp: 625666 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
COEDIT: Chỉnh sửa văn bản bằng điều chỉnh hướng dẫn theo tác vụ cụ thể
Vipul Raheja♣Dhruv Kumar♣Ryan Koo♢Dongyeop Kang♢
♣Grammarly♢Đại học Minnesota
♣first.last@grammarly.com♢{koo00017, dongyeop}@umn.edu

Tóm tắt
Chúng tôi giới thiệu COEDIT, một hệ thống chỉnh sửa văn bản tiên tiến để hỗ trợ viết lách. COEDIT nhận hướng dẫn từ người dùng chỉ định các thuộc tính của văn bản mong muốn, chẳng hạn như "Làm cho câu đơn giản hơn" hoặc "Viết theo phong cách trung tính hơn," và xuất ra văn bản đã chỉnh sửa. Chúng tôi trình bày một mô hình ngôn ngữ lớn được điều chỉnh tinh trên một bộ sưu tập đa dạng các hướng dẫn cụ thể theo tác vụ để chỉnh sửa văn bản (tổng cộng 82K hướng dẫn). Mô hình của chúng tôi (1) đạt được hiệu suất tiên tiến trên các bộ đánh giá chỉnh sửa văn bản khác nhau, (2) cạnh tranh với các LLM có sẵn công khai có kích thước lớn nhất được huấn luyện trên hướng dẫn trong khi nhỏ hơn khoảng 60 lần, (3) có khả năng tổng quát hóa cho các hướng dẫn chỉnh sửa chưa thấy, và (4) thể hiện khả năng tổng quát hóa cho các hướng dẫn phức hợp chứa các kết hợp khác nhau của các hành động chỉnh sửa. Thông qua phân tích định tính và định lượng rộng rãi, chúng tôi cho thấy rằng người viết ưa thích các chỉnh sửa được đề xuất bởi COEDIT so với các mô hình chỉnh sửa văn bản tiên tiến khác¹.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã có tiến bộ đáng kể trong việc tạo ra văn bản mạch lạc trong nhiều tác vụ và lĩnh vực khác nhau để hỗ trợ viết lách (Du et al., 2022a; Mallinson et al., 2022; Schick et al., 2023), chẳng hạn như sửa lỗi ngữ pháp (Wu et al., 2023), đơn giản hóa văn bản (Štajner et al., 2022), diễn giải (Chowdhury et al., 2022), và chuyển đổi phong cách (Reif et al., 2022). Một trong những khả năng mới nổi của LLM là khả năng tổng quát hóa cho các tác vụ chưa thấy bằng cách tuân theo các hướng dẫn mới hoặc tổng hợp. Điều chỉnh hướng dẫn, trong đó LLM được điều chỉnh tinh trên một bộ sưu tập các tác vụ được diễn đạt dưới dạng hướng dẫn, làm cho các mô hình trở nên thành thạo hơn trong việc diễn giải và tuân theo hướng dẫn, giảm nhu cầu về các ví dụ few-shot (Sanh et al., 2022; Ouyang et al., 2022b; Wei et al., 2022; Chung et al., 2022b).

¹Mã, dữ liệu và mô hình có sẵn tại https://github.com/vipulraheja/coedit

[Hình 1: So sánh mô hình theo tham số huấn luyện so với hiệu suất trung bình trên tất cả các bộ đánh giá chỉnh sửa văn bản được báo cáo trong Bảng 2 và 11. Các mô hình có sẵn công khai được ký hiệu bằng (*).]

Chỉnh sửa văn bản là một tác vụ phức tạp bởi vì người viết không thể đồng thời nắm bắt nhiều yêu cầu và ràng buộc của tác vụ và có xu hướng lặp lại và sửa đổi công việc của họ nhiều lần (Flower, 1980; Collins và Gentner, 1980; Vaughan và McDonald, 1986). Điều này đặt ra một thách thức đáng kể cho các trợ lý viết thông minh.

Trong công việc này, chúng tôi nhằm cải thiện khả năng của các mô hình được điều chỉnh hướng dẫn để chỉnh sửa văn bản bằng cách tận dụng điều chỉnh hướng dẫn từ các tác vụ đa dạng của các bộ đánh giá chỉnh sửa văn bản. Trong khi nhiều công trình trước đây đã cố gắng phát triển các mô hình chỉnh sửa văn bản đa năng sử dụng LLM, chúng hoặc không được huấn luyện với điều chỉnh hướng dẫn (Du et al., 2022c; Kim et al., 2022), được huấn luyện trên các mô hình nhỏ hơn nhiều hoặc không được huấn luyện trên các bộ dữ liệu cụ thể theo tác vụ (Mallinson et al., 2022; Schick et al., 2023), hoặc không có sẵn công khai (Schick et al., 2023), điều này hạn chế hiệu quả, hiệu suất hoặc khả năng sử dụng của chúng.

Chúng tôi giới thiệu COEDIT, một hệ thống chỉnh sửa văn bản được thiết kế để cung cấp hỗ trợ viết với giao diện ngôn ngữ tự nhiên. Người dùng có thể sử dụng COEDIT bằng cách cung cấp hướng dẫn ngôn ngữ tự nhiên như "Diễn giải câu" hoặc "Sửa ngữ pháp". Các thí nghiệm của chúng tôi cho thấy rằng điều chỉnh tinh hướng dẫn cho các tác vụ cụ thể hiệu quả hơn học đa tác vụ và điều chỉnh hướng dẫn đa năng. Chúng tôi phỏng đoán rằng các hướng dẫn cụ thể theo tác vụ làm tăng mật độ của không gian hướng dẫn, củng cố các hiệu ứng bổ sung của nhiều tác vụ và tạo thuận lợi cho việc tổng quát hóa của chúng sang các tác vụ chỉnh sửa văn bản phức hợp và mới, như thể hiện trong Hình 2.

[Hình 2: Điều chỉnh hướng dẫn đa năng (trái) so với cụ thể theo tác vụ (phải).]

Để xây dựng COEDIT, chúng tôi điều chỉnh tinh một mô hình sequence-to-sequence được huấn luyện trước trên một kho ngữ liệu song song của 82K cặp đầu vào-đầu ra dựa trên hướng dẫn. Các đầu vào và đầu ra được lấy từ các kho ngữ liệu có sẵn công khai cho các tác vụ chỉnh sửa văn bản khác nhau, và các hướng dẫn được xây dựng dựa trên các quy tắc giới thiệu các biến thể từ vựng và ngữ nghĩa.

Các đóng góp chính của chúng tôi như sau:
• Chúng tôi đạt được hiệu suất tiên tiến trên nhiều tác vụ chỉnh sửa văn bản: sửa lỗi ngữ pháp, đơn giản hóa văn bản, kết hợp câu, chỉnh sửa văn bản lặp lại, và ba tác vụ chỉnh sửa phong cách (chuyển đổi phong cách trang trọng, trung tính hóa, và diễn giải).
• Chúng tôi thấy rằng ngay cả mô hình được điều chỉnh hướng dẫn nhỏ nhất của chúng tôi cũng vượt trội hơn các mô hình chỉnh sửa văn bản được giám sát khác, các mô hình được điều chỉnh hướng dẫn, và các LLM đa năng với số tham số lớn hơn gần 60 lần, trên cả đánh giá thủ công và tự động.
• COEDIT tổng quát hóa tốt cho các tác vụ mới, kề cận chưa thấy trong quá trình điều chỉnh tinh, cũng như các hướng dẫn phức hợp với nhiều đặc điểm kỹ thuật tác vụ.
• Dữ liệu và mô hình của chúng tôi sẽ có sẵn công khai.

2 Công việc liên quan
Mô hình ngôn ngữ lớn cho chỉnh sửa văn bản Nói chung, công việc của chúng tôi liên quan đến nhiều công trình trước đây tận dụng LLM; ví dụ, điều chỉnh tinh T5 (Raffel et al., 2020a) trên các cặp văn bản gốc và đã chỉnh sửa (Faltings et al., 2021; Reid và Neubig, 2022; Mallinson et al., 2022; Du et al., 2022a,b; Kim et al., 2022). Tuy nhiên, các công trình nêu trên hoặc không dựa trên điều chỉnh hướng dẫn, sử dụng các kỹ thuật mô hình khác nhau như gán nhãn chuỗi dựa trên thẻ, hoặc không đủ tổng quát để hoạt động trên nhiều tác vụ chỉnh sửa văn bản. Hơn nữa, một số LLM được huấn luyện để giải quyết chỉ các tác vụ cụ thể, chẳng hạn như lỗi ngữ pháp (Mallinson et al., 2022; Fang et al., 2023), đơn giản hóa văn bản (Štajner et al., 2022), tạo diễn giải (Chowdhury et al., 2022), hoặc chuyển đổi phong cách (Reif et al., 2022), điều này hạn chế khả năng tổng quát hóa của chúng.

Điều chỉnh hướng dẫn cho hỗ trợ viết Việc dạy các mô hình một cách rõ ràng cách tuân theo hướng dẫn ngôn ngữ tự nhiên có liên quan chặt chẽ đến công việc gần đây về điều chỉnh tinh các mô hình sử dụng bộ dữ liệu lớn các hướng dẫn do con người viết (Wei et al., 2022; Mishra et al., 2022; Sanh et al., 2022; Ouyang et al., 2022a; Wang et al., 2022; Iyer et al., 2022; Bach et al., 2022; Longpre et al., 2023). Gần đây, việc tăng cường dữ liệu nâng cao và điều chỉnh hướng dẫn, bắt đầu với các mô hình Flan (Chung et al., 2022b), đã cho thấy rằng kết quả mạnh mẽ xuất phát từ cả bộ tác vụ lớn hơn và đa dạng hơn. Ngoài ra, việc làm phong phú đa dạng tác vụ và cân bằng nguồn tác vụ (Sanh et al., 2022) được chỉ ra là quan trọng đối với hiệu suất, gợi ý rằng các mô hình được điều chỉnh hướng dẫn cung cấp một điểm khởi đầu hiệu quả hơn về mặt tính toán cho các ứng dụng hạ lưu, xác nhận Liu et al. (2022) và Aribandi et al. (2022).

Về điều chỉnh hướng dẫn cho hỗ trợ viết, công việc của chúng tôi liên quan chặt chẽ đến PEER (Schick et al., 2023), người đã điều chỉnh tinh các LLM dựa trên T5 bằng cách tuân theo các kế hoạch chỉnh sửa văn bản do người dùng cung cấp để thực hiện các chỉnh sửa đó. Có một vài khác biệt đáng kể trong cách tiếp cận của chúng tôi so với PEER. Trong khi PEER cố gắng tạo ra hoặc tận dụng một kế hoạch do người dùng cung cấp, thực hiện các chỉnh sửa có điều kiện trên kế hoạch, và cố gắng giải thích kế hoạch, chúng tôi chỉ tập trung vào các phần kế hoạch và chỉnh sửa của quy trình. Ngay cả khi nói đến việc xử lý các kế hoạch chỉnh sửa dưới dạng hướng dẫn ngôn ngữ tự nhiên, công việc của chúng tôi tập trung vào các chỉnh sửa không thêm thông tin mới. Do đó, chúng tôi chỉ so sánh các mô hình của chúng tôi với các mô hình PEER-Edit. Cuối cùng, không có công trình nào trước đây, theo hiểu biết tốt nhất của chúng tôi, đã điều tra khả năng của các LLM được điều chỉnh hướng dẫn để chỉnh sửa văn bản tổng quát hóa cho các hướng dẫn phức hợp.

3 COEDIT
3.1 Bộ dữ liệu huấn luyện
Việc tạo bộ dữ liệu của chúng tôi dựa trên bộ dữ liệu ITERATER+ được đề xuất bởi Kim et al. (2022) người đã kết hợp các bộ dữ liệu từ các tác vụ chỉnh sửa văn bản khác nhau (Xem Bảng 1). Công việc của họ, lần lượt, dựa trên Du et al. (2022b), người đã phân loại mỗi chỉnh sửa thành THAY ĐỔI Ý NGHĨA hoặc KHÔNG THAY ĐỔI Ý NGHĨA. Các chỉnh sửa thuộc nhóm sau được gán thêm vào TRÔI CHẢY, MẠCH LẠC, RÕ RÀNG, hoặc PHONG CÁCH.

Phân loại ý định chỉnh sửa nêu trên từ ITERATER phản ánh ý định chung của người viết đằng sau việc sửa đổi của họ, cung cấp thông tin chi tiết hơn so với chỉ các phép toán chỉnh sửa bề ngoài, chẳng hạn như THÊM và XÓA.

Tương tự như Kim et al. (2022), công việc của chúng tôi tập trung vào các chỉnh sửa KHÔNG THAY ĐỔI Ý NGHĨA. Chúng tôi coi những chỉnh sửa đó là những chỉnh sửa không thêm thông tin mới hoặc thực hiện cập nhật sự thật. Vì các chỉnh sửa PHONG CÁCH khá chủ quan về bản chất, chúng tôi cho phép khả năng thay đổi ý nghĩa để đáp ứng nhu cầu thực hiện chỉnh sửa phong cách, nhưng chúng tôi ràng buộc các tác vụ chỉnh sửa để đảm bảo các văn bản đã chỉnh sửa tương tự về ngữ nghĩa với nguồn, nhưng không đến mức thêm thông tin mới hoặc cập nhật sự thật.

Với suy nghĩ này, chúng tôi mở rộng danh mục ý định chỉnh sửa PHONG CÁCH từ ITERATER+ để bao gồm ba ý định phụ mới: Diễn giải, Chuyển đổi phong cách trang trọng (hoặc Trang trọng hóa), và Trung tính hóa.

Phân loại bộ dữ liệu ITERATER nêu trên dễ dàng được biểu đạt dưới dạng hướng dẫn ngôn ngữ tự nhiên và cho phép chúng tôi tự nhiên biểu thức chúng thành các lời nhắc hướng dẫn (Xem Bảng 1). Chúng tôi viết lại mỗi ý định chỉnh sửa thành một tập hợp các lời nhắc hướng dẫn ngôn ngữ tự nhiên để tạo ra bộ dữ liệu COEDIT. Để cho phép các mô hình thích ứng với các biến thể ngôn ngữ của hướng dẫn, chúng tôi cũng bao gồm các diễn giải của các mẫu hướng dẫn, ví dụ, thay vì "Viết" chúng tôi cũng sử dụng "Tạo" hoặc "Viết lại," hoặc thay vì "Diễn giải văn bản" chúng tôi sử dụng "Viết lại văn bản với cách diễn đạt khác," v.v. Cho mỗi tác vụ, chúng tôi phát triển nhiều lời nhắc hướng dẫn đa dạng như vậy và ngẫu nhiên lấy mẫu một hướng dẫn từ nhóm các ứng viên hướng dẫn cụ thể theo tác vụ nêu trên để được thêm vào đầu nguồn nhằm tạo thành một cặp dữ liệu <hướng dẫn: nguồn, đích>. Chúng tôi cung cấp danh sách đầy đủ các lời nhắc hướng dẫn của chúng tôi trong §C. Tổng cộng, bộ dữ liệu huấn luyện của chúng tôi bao gồm khoảng 82K cặp <hướng dẫn: nguồn, đích>.

Chúng tôi giữ các phần chia train-validation-test gốc nhất quán như các bộ dữ liệu gốc nhưng đa dạng hóa các phần chia train và validation với các tăng cường diễn giải. Chi tiết của các bộ dữ liệu và hướng dẫn được sử dụng để huấn luyện các mô hình của chúng tôi được mô tả trong §A.

3.2 Mô hình chỉnh sửa văn bản
Chúng tôi điều chỉnh tinh các phiên bản khác nhau của các mô hình FLANT5 (Chung et al., 2022a) được huấn luyện trước trên bộ dữ liệu COEDIT. Cụ thể, chúng tôi sử dụng các mô hình FLANT5-L (770M tham số), FLANT5-XL (3B tham số), FLANT5-XXL (11B tham số), từ đây được gọi là COEDIT-L, COEDIT-XL, và COEDIT-XXL tương ứng. Chi tiết huấn luyện được tóm tắt trong §D.

4 Thiết lập thí nghiệm
Chúng tôi tiến hành thí nghiệm để xác định liệu một mô hình ngôn ngữ được điều chỉnh hướng dẫn tiêu chuẩn được điều chỉnh tinh sử dụng dữ liệu cụ thể theo tác vụ có thể cải thiện hiệu suất chỉnh sửa văn bản hay không và liệu nó có thể tổng quát hóa thành một mô hình chỉnh sửa văn bản đa năng có khả năng tuân theo hướng dẫn do con người viết và xử lý một loạt các tác vụ chỉnh sửa rộng hơn, chẳng hạn như hướng dẫn chưa thấy và phức hợp. Cụ thể, chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau:

• RQ1: COEDIT có thể tuân theo hướng dẫn chỉnh sửa văn bản và thực hiện chỉnh sửa chất lượng cao trên nhiều tác vụ khác nhau không?
• RQ2: COEDIT có thể tổng quát hóa để thực hiện chỉnh sửa chất lượng cao cho các loại hướng dẫn chỉnh sửa văn bản mới không?
• RQ3: COEDIT có làm cho quá trình viết hiệu quả và hiệu suất hơn cho người viết không?

Chúng tôi trả lời các câu hỏi này thông qua phân tích định lượng đầu ra mô hình (Phần 5) và thông qua phân tích định tính và đánh giá con người về đầu ra mô hình (Phần 6). Hơn nữa, chúng tôi điều tra RQ2 theo hai chiều: (1) tổng quát hóa cho hướng dẫn phức hợp chứa kết hợp của nhiều loại chỉnh sửa khác nhau và (2) tổng quát hóa ngoài miền cho hướng dẫn với yêu cầu tác vụ mới trên dữ liệu chưa thấy trước đây.

4.1 Mô hình
Baseline không chỉnh sửa Đầu tiên chúng tôi đánh giá baseline không chỉnh sửa, trong đó đầu ra chỉ đơn giản là bản sao của đầu vào nguồn không có hướng dẫn. Chiến lược này hoạt động khá tốt trên các tác vụ mà đầu ra đích chủ yếu trùng lặp với đầu vào (ví dụ, GEC).

Mô hình chỉnh sửa văn bản được giám sát Chúng tôi cũng đánh giá các LLM hiện có để chỉnh sửa văn bản không được điều chỉnh tinh với dữ liệu cụ thể theo hướng dẫn. Cụ thể, để hiểu tác động của điều chỉnh tinh cụ thể theo tác vụ, chúng tôi đánh giá so với các mô hình T5² (Raffel et al., 2020b) là các lựa chọn thay thế chính cho các mô hình FLAN-T5 của chúng tôi. Chúng tôi cũng so sánh các mô hình của chúng tôi với ITERATER (Du et al., 2022b) và DELITERATER (Kim et al., 2022), đã cho thấy hiệu suất mạnh mẽ trên nhiều tác vụ chỉnh sửa văn bản.³

LLM được điều chỉnh hướng dẫn Một nhóm so sánh chính của chúng tôi là với các LLM được điều chỉnh hướng dẫn:
• So sánh chính của chúng tôi là với PEER (Schick et al., 2023), chủ yếu dựa trên biến thể LM Adapted của T5. Vì trọng tâm công việc của chúng tôi là cải thiện chất lượng sửa đổi (Phần 2), chúng tôi so sánh với PEER-EDIT (cả phiên bản 3B và 11B).
• T0, T0++ (Sanh et al., 2022) và Tk-Instruct (Wang et al., 2022), tất cả đều được khởi tạo từ biến thể LM Adapted của T5, và được điều chỉnh tinh sử dụng PromptSource (Bach et al., 2022), và bộ dữ liệu Super-NaturalInstructions (Wang et al., 2022), tương ứng.
• Alpaca (Taori et al., 2023) là phiên bản được điều chỉnh hướng dẫn của mô hình LLaMA-7B (Touvron et al., 2023) được huấn luyện trên 52K minh chứng tuân theo hướng dẫn được tạo bởi GPT3.
• Chúng tôi cũng so sánh InstructGPT (Ouyang et al., 2022a), một biến thể của GPT3 được điều chỉnh tinh thông qua học tăng cường trên một bộ dữ liệu lớn hướng dẫn và đầu ra do con người viết.⁴
• GPT3.5 (từ đây gọi là ChatGPT), là phiên bản cải tiến của InstructGPT được tối ưu hóa cho chat. Chúng tôi sử dụng API của OpenAI cho tất cả các tác vụ suy luận.⁵
• GPT3 cũng cung cấp API chỉnh sửa văn bản⁶ (chúng tôi gọi là GPT3-Edit), có thể sử dụng cho các tác vụ chỉnh sửa thay vì hoàn thành, làm cho nó trực tiếp so sánh được với các tác vụ chúng tôi huấn luyện COEDIT.

Mô hình decoder-only được huấn luyện trước lớn Chúng tôi so sánh với các LLM không có điều chỉnh hướng dẫn trong hai thiết lập – zero-shot và few-shot (chi tiết trong Phần 5.1):
• Mô hình GPT3 175B (Brown et al., 2020) không được điều chỉnh hướng dẫn cho thấy khả năng sửa đổi văn bản đa năng mạnh mẽ.
• LLaMA (Touvron et al., 2023) là mô hình ngôn ngữ đa năng của Meta AI chỉ được huấn luyện trên dữ liệu có sẵn công khai. Chúng tôi sử dụng mô hình 7B do hạn chế tính toán.

Đầu ra của tất cả các mô hình được tạo ra sử dụng giải mã tham lam trừ khi được chỉ định khác.

4.2 Bộ dữ liệu kiểm tra
Để đánh giá khả năng chỉnh sửa của COEDIT, chúng tôi thực hiện đánh giá trên các bộ kiểm tra tiêu chuẩn được lấy từ nhiều bộ đánh giá tác vụ chỉnh sửa văn bản khác nhau, đáng chú ý nhất là EDITEVAL (Dwivedi-Yu et al., 2022). Do sự trùng lặp của công việc chúng tôi với PEER, chúng tôi giữ các bộ dữ liệu đánh giá và các chỉ số đánh giá càng gần với của họ càng tốt để đảm bảo tính nhất quán: Chúng tôi sử dụng JFLEG (Napoles et al., 2017) cho bộ sưu tập lỗi ngữ pháp, TurkCorpus (Xu et al., 2016) và ASSET (Alva-Manchego et al., 2020) cho đơn giản hóa văn bản, phần Coherence của ITERATER (Du et al., 2022b) và bộ dữ liệu DISCOFUSE (Geva et al., 2019) cho mạch lạc, và ITERATER (Du et al., 2022b) cho sửa đổi văn bản lặp lại. Cho các chỉnh sửa liên quan đến Phong cách, chúng tôi sử dụng GYAFC (Rao và Tetreault, 2018) cho phong cách trang trọng, WNC (Pryzant et al., 2020) cho trung tính hóa, và MRPC (Dolan và Brockett, 2005), STS (Cer et al., 2017), và QQP cho diễn giải. Mô tả chi tiết của mỗi bộ dữ liệu và các chỉ số đánh giá của nó có trong §B.

5 Kết quả định lượng
5.1 Hiệu suất chỉnh sửa văn bản
Bảng 2 giúp chúng tôi trả lời RQ1 bằng cách so sánh hiệu suất của COEDIT với các mô hình khác trên các tác vụ chỉnh sửa văn bản khác nhau. Đầu tiên chúng tôi trình bày kết quả từ các bộ đánh giá nổi tiếng hơn ở đây và trình bày kết quả bổ sung (tức là, các tác vụ phụ và bộ dữ liệu bổ sung) trong Bảng 11.

Chúng tôi phân chia các mô hình thành bảy nhóm. Nhóm đầu tiên (a) bao gồm baseline sao chép và baseline T5-LARGE được điều chỉnh tinh với prefix-tuning (mỗi điểm dữ liệu được thêm tiền tố với thẻ cụ thể theo tác vụ thay vì hướng dẫn), trong khi nhóm thứ hai (b) bao gồm các mô hình dựa trên T5 được điều chỉnh tinh hướng dẫn trên các tác vụ không chỉnh sửa văn bản. Chúng tôi thấy rằng COEDIT vượt trội đáng kể so với các mô hình này trên tất cả các tác vụ.

Hai nhóm tiếp theo (c, d) cho thấy các LLM khác nhau từ 7B đến 176B tham số về kích thước, được đánh giá trong thiết lập zero-shot. Những mô hình trong nhóm (c) là các mô hình decoder-only, trong khi những mô hình trong nhóm (d) được điều chỉnh hướng dẫn. Chúng tôi thấy rằng COEDIT vượt trội hơn tất cả các LLM so sánh với kích thước mô hình của nó (ví dụ, Alpaca và LLaMA) trên tất cả các tác vụ, cũng như trên hầu hết các tác vụ so với các mô hình lớn hơn nhiều lần, chẳng hạn như ChatGPT và InstructGPT. Điều này cho thấy rằng các mô hình đa năng và được điều chỉnh hướng dẫn hiện tại đang bị underfitted, và việc tăng mật độ không gian tác vụ/hướng dẫn có lợi hơn là mở rộng kích thước mô hình.

Mặc dù các mô hình như Alpaca và các mô hình dựa trên T5 (Tk-instruct, T0, T0++) trước đây đã cho thấy khả năng mạnh mẽ cho các tác vụ zero-shot, chúng cho thấy hiệu suất yếu hơn so với COEDIT. Chúng tôi cũng thấy rằng các mô hình decoder-only (ví dụ, GPT3 và LLaMA) thường lặp lại đầu vào cho các tác vụ phức tạp hơn, chẳng hạn như những tác vụ thuộc nhóm ý định Style. Điều này có thể được gán cho khó khăn trong việc hiểu tác vụ được nhắc, dẫn đến các mô hình hoặc lặp lại câu đầu vào hoặc tạo ra phần tiếp theo không liên quan đến tác vụ.

Tiếp theo, trong nhóm thứ năm (e), chúng tôi đánh giá các LLM trong thiết lập few-shot. Như đã đề cập trong Phần 4.1, chúng tôi tiến hành các thí nghiệm này trong thiết lập đánh giá 4-shot, trong đó các đầu vào ví dụ được xây dựng bằng cách ngẫu nhiên lấy mẫu bốn đầu vào cho mỗi tác vụ từ bộ dữ liệu COEDIT sao cho tất cả các ví dụ được chọn sẽ phù hợp trong cửa sổ đầu vào cho tất cả các mô hình như thấy trong (Brown et al., 2020). Câu đầu vào và tham chiếu đã sửa đổi tương ứng của nó được thêm vào đầu lời nhắc hướng dẫn. Chúng tôi tiến hành đánh giá few-shot cho các LLM decoder-only (GPT3) và ba LLM được điều chỉnh hướng dẫn (InstructGPT, ChatGPT, và Alpaca). Đầu ra của tất cả các mô hình được tạo ra sử dụng giải mã tham lam trừ khi được chỉ định khác.

Chúng tôi quan sát thấy rằng việc đưa ra các ví dụ cụ thể cải thiện hiệu suất trong tất cả các mô hình cho tất cả các tác vụ ngoại trừ MRPC cho GPT3. Điều này có thể là do GPT3 vẫn thể hiện một số hành vi tương tự trong việc lặp lại các thế hệ của nó liên tục, dẫn đến điểm BLEU thấp nhưng độ tương tự ngữ nghĩa cũng thấp. Chúng tôi không trình bày bất kỳ thí nghiệm nào cho GPT3-Edit trong thiết lập few-shot, vì điểm số có xu hướng giữ nguyên trên tất cả các tác vụ – ngụ ý rằng GPT3-Edit có thể không có khả năng học trong ngữ cảnh tốt như vậy. Nhìn chung, chúng tôi thấy rằng ngay cả mô hình 770M tham số nhỏ nhất của chúng tôi cũng cạnh tranh với các LLM được đánh giá trong thiết lập few-shot trong hầu hết các tác vụ.

Trong nhóm cuối cùng (f), chúng tôi so sánh các mô hình của chúng tôi với các mô hình chỉnh sửa văn bản cụ thể theo tác vụ như ITERATER, DELITERATER, và PEER. ITERATER và DELITERATER hoạt động tương đối kém hơn so với các điểm số được báo cáo trong bài báo gốc vì chúng tôi trình bày các đầu vào khác nhau và khó khăn hơn, chỉ thêm hướng dẫn vào đầu vào trong khi ITERATER và DELITERATER được huấn luyện với thẻ cụ thể theo tác vụ. Hơn nữa, chúng được huấn luyện sử dụng BART và Pegasus, tương ứng, cả hai đều có mục tiêu huấn luyện trước tóm tắt, và không được huấn luyện để tuân theo hướng dẫn. Trung bình, COEDIT đánh bại PEER trên tất cả các đánh giá được báo cáo ngoại trừ bộ đánh giá ITERATER. Điều này có thể được gán chủ yếu cho sự khác biệt trong điều chỉnh tinh cụ thể theo tác vụ vì PEER sử dụng Wikipedia làm nguồn dữ liệu chỉnh sửa hướng dẫn.

5.2 Nghiên cứu tách biệt
Bảng 3 cho thấy hiệu suất của các baseline khác nhau, mà chúng tôi thảo luận chi tiết trong phần này.

Điều chỉnh hướng dẫn. Để hiểu hiệu quả của điều chỉnh hướng dẫn, chúng tôi điều chỉnh tinh mô hình T5 3B tham số (T5-XL) và so sánh nó với COEDIT-XL, đối tác FLANT5 của nó trên cùng các bộ huấn luyện và validation. Thay đổi duy nhất là các lời nhắc hướng dẫn cho các bộ dữ liệu huấn luyện được thay thế bằng các tiền tố cụ thể theo tác vụ. Cụ thể, 82k cặp <hướng dẫn: nguồn, đích> trong bộ dữ liệu huấn luyện được sử dụng để huấn luyện các mô hình COEDIT được sửa đổi thành <tác vụ: nguồn, đích>⁸. Chúng tôi quan sát (Bảng 3(a)) rằng các mô hình COEDIT được điều chỉnh hướng dẫn luôn vượt trội hơn các mô hình T5 được điều chỉnh tiền tố, cho thấy hiệu quả của điều chỉnh hướng dẫn so với điều chỉnh tiền tố.

Huấn luyện cụ thể theo tác vụ. Một đóng góp cốt lõi của công việc này là thúc đẩy hiệu suất của các LLM nhỏ (<1B tham số) đến trung bình (1-10B tham số) cho các tác vụ chỉnh sửa văn bản phổ biến. Điều này thúc đẩy nhu cầu điều chỉnh tinh trên các bộ dữ liệu cụ thể theo tác vụ. Tác động của việc tăng cường dữ liệu cụ thể theo tác vụ này cho các tác vụ chỉnh sửa văn bản đã được thể hiện trong Kim et al. (2022). Cho công việc này, chúng tôi so sánh các mô hình được điều chỉnh tinh cụ thể theo tác vụ của chúng tôi với các đối tác FLANT5 chưa được điều chỉnh của chúng được gọi là FLANT5-XL (Bảng 3(b)). Chúng tôi thấy một khoảng cách đáng kể giữa hai bên cho tất cả các bộ dữ liệu và kích thước mô hình, do đó, xác nhận các phát hiện trước đây.

Chất lượng hướng dẫn. Trong khi chúng tôi phát triển với một tập hợp hạn chế các lời nhắc hướng dẫn cụ thể theo tác vụ, đã có công việc rộng rãi về độ nhạy lời nhắc của LLM, đặc biệt là với khả năng mô hình ngày càng tăng (Lu et al., 2022). Để đánh giá tính mạnh mẽ của các mô hình COEDIT trên các lời nhắc hướng dẫn, chúng tôi huấn luyện một baseline COEDIT-XL khác với các hướng dẫn cụ thể theo tác vụ được ngẫu nhiên hóa (từ đây gọi là COEDIT-XL-R). Cụ thể, toàn bộ bộ dữ liệu huấn luyện được ngẫu nhiên hóa, trong đó một hướng dẫn từ một tác vụ được thay thế ngẫu nhiên bằng một hướng dẫn từ tác vụ khác. Bảng 3(c) cho thấy kết quả cho thí nghiệm này. Chúng tôi quan sát thấy rằng trong khi COEDIT-XL-R đạt được điểm số cao hơn FLANT5-XL không được điều chỉnh cụ thể theo tác vụ (đặc biệt là trên các chỉ số dựa trên chỉnh sửa như SARI), nó tụt lại đáng kể so với COEDIT-XL trên những chỉ số đó, cũng như trên các chỉ số độ chính xác phong cách như độ chính xác chuyển đổi trang trọng và độ tương tự ngữ nghĩa diễn giải. Điều này cho thấy rằng trong khi cấu trúc hướng dẫn của các đầu vào và huấn luyện cụ thể theo tác vụ làm cho mô hình học cách thực hiện chỉnh sửa (điều này thúc đẩy điểm SARI lên), tuy nhiên, độ chính xác của các chỉnh sửa đó bị ảnh hưởng vì chúng được huấn luyện với hướng dẫn sai hầu hết thời gian. Nhìn chung, các cải thiện làm nổi bật tác động tích cực của huấn luyện cụ thể theo tác vụ, và các khoảng cách hiệu suất làm nổi bật tác động tiêu cực của việc thiếu điều chỉnh hướng dẫn thích hợp.

6 Kết quả định tính
Bây giờ chúng tôi giải quyết RQ2 và RQ3 (Phần 4). Chúng tôi cho thấy rằng COEDIT thể hiện khả năng tổng quát hóa cho các tác vụ liền kề không thấy trong quá trình điều chỉnh tinh và có thể tổng quát hóa cho hướng dẫn phức hợp chứa kết hợp của các tác vụ. Hơn nữa, các nghiên cứu đánh giá con người của chúng tôi cho thấy rằng các đánh giá viên chuyên gia con người thấy văn bản được tạo bởi COEDIT có chất lượng cao hơn so với một LLM được điều chỉnh hướng dẫn lớn hơn nhiều.

6.1 Chất lượng chỉnh sửa văn bản
Vì chỉnh sửa văn bản thường mang tính chủ quan, và các chỉ số tự động không phải lúc nào cũng chính xác trong việc đo lường liệu một hướng dẫn có được thỏa mãn hay không, chúng tôi tiến hành đánh giá con người cho các đầu ra mô hình của chúng tôi bởi các chuyên gia ngôn ngữ trên 50 đầu vào kiểm tra để đảm bảo chúng đáp ứng các ràng buộc hướng dẫn. Dựa trên kết quả đánh giá tự động trong Phần 5, chúng tôi so sánh mô hình COEDIT-XL 3B tham số của chúng tôi với LLM được điều chỉnh hướng dẫn 175B lớn nhất so sánh được cho chỉnh sửa văn bản GPT3-EDIT. Cụ thể, chúng tôi tiến hành so sánh theo cặp: mỗi người chú thích được hiển thị một đầu vào hướng dẫn và đầu ra từ cả hai mô hình (họ không biết đầu ra nào được tạo bởi mô hình nào). Sau đó họ được yêu cầu đánh giá tính trôi chảy, độ chính xác, và bảo tồn ý nghĩa của các văn bản đã chỉnh sửa và chọn đầu ra chất lượng cao hơn ("không cái nào" và "hòa" cũng là các lựa chọn hợp lệ). Chúng tôi thu thập ba chú thích cho mỗi câu hỏi và sử dụng phiếu bầu đa số làm phán quyết cuối cùng.

Bảng 4 cho thấy kết quả đánh giá. Các người chú thích ưa thích mô hình COEDIT của chúng tôi cho 64% đầu vào, trong khi đối với 10% đầu vào, đầu ra của GPT3-EDIT được ưa thích. Trong 4% trường hợp, cả hai mô hình tạo ra đầu ra tốt như nhau, trong khi đối với 22% đầu vào, cả hai mô hình tạo ra đầu ra không chấp nhận được. Bảng 12 cung cấp so sánh cạnh nhau của các đầu ra được tạo bởi hai mô hình.

6.2 Khả năng tổng quát hóa cho các tác vụ liền kề
Chúng tôi phân tích khả năng tổng quát hóa của các mô hình bằng cách đánh giá chúng trên một vài tác vụ liên quan không tồn tại trong dữ liệu điều chỉnh tinh. Cụ thể, chúng tôi chọn hai tác vụ NLP tiêu chuẩn – nén câu (SC) (Filippova và Altun, 2013) và chuyển đổi lịch sự (PT) (Madaan và Yang, 2021). Đáng chú ý rằng trong khi các mô hình của chúng tôi không được điều chỉnh tinh trên các tác vụ chính xác này, chúng tôi chọn chúng để các mô hình vẫn có thể hiểu chúng dựa trên các tác vụ khác mà chúng được điều chỉnh tinh. Chúng tôi định nghĩa chúng là các tác vụ liền kề, vẫn tồn tại trong phạm vi của các tác vụ hiện có nhưng chưa được thấy trong quá trình điều chỉnh tinh (các đường màu xanh trong Hình 2).

Tương tự như thí nghiệm trước, ngoài GPT3-EDIT, chúng tôi so sánh COEDIT-XL với mô hình được điều chỉnh tiền tố có kích thước tương tự (T5-XL) và mô hình FLANT5-XL không được huấn luyện cụ thể theo tác vụ (các mô hình giống như những mô hình được sử dụng trong Bảng 3 (a) và (b)). Để đánh giá, chúng tôi tuyển chọn một tập hợp các lời nhắc hướng dẫn mới hướng tới cả hai tác vụ mới (chi tiết trong Phụ lục C). Chúng tôi đánh giá các mô hình trên các bộ dữ liệu kiểm tra tương ứng từ Filippova và Altun (2013) và Madaan và Yang (2021).

Bảng 5 cho thấy kết quả của COEDIT-XL so với các mô hình khác nhau trên các tác vụ nén câu và chuyển đổi lịch sự. Cho SC, chúng tôi báo cáo chỉ số SARI cho chất lượng viết lại và tỷ lệ nén (CR) cho chất lượng cụ thể theo tác vụ. Cho PT, chúng tôi báo cáo Self-BLEU (Zhu et al., 2018) cho chất lượng viết lại⁹ và Độ chính xác chuyển đổi (TA) cho chất lượng cụ thể theo tác vụ. Chúng tôi quan sát thấy rằng COEDIT luôn vượt trội hơn các mô hình khác trên cả hai tác vụ, điều này cho thấy khả năng tổng quát hóa của nó trên các tác vụ liền kề mới và chưa thấy này. Đáng chú ý rằng GPT3-EDIT hoạt động khá tốt ngay lập tức trên PT, nhưng không tốt lắm trên tác vụ SC.

6.3 Khả năng tổng quát hóa cho hướng dẫn phức hợp
Cuối cùng, chúng tôi cũng khám phá khả năng của mô hình trong việc hiểu hướng dẫn ngôn ngữ tự nhiên phức hợp. Hướng dẫn phức hợp được tạo thành từ kết hợp của các tác vụ. Ví dụ, cho hướng dẫn phức hợp, "Làm văn bản đơn giản hơn, diễn giải nó, và làm cho nó trang trọng", mô hình cần đồng thời thực hiện đơn giản hóa, diễn giải và trang trọng hóa câu đầu vào.

Vì không có bộ dữ liệu có sẵn công khai cho hướng dẫn phức hợp, chúng tôi tạo bộ dữ liệu COEDIT-COMPOSITE bằng cách mở rộng bộ dữ liệu COEDIT lên tổng cộng 90k cặp. Ngoài các hướng dẫn tác vụ đơn, chúng tôi sử dụng bảy kết hợp hướng dẫn mới làm phần của bộ huấn luyện của chúng tôi, với mỗi hướng dẫn phức hợp có hai hoặc ba tác vụ. Cụ thể, đây là GEC-Diễn giải, GEC-Đơn giản hóa, GEC-Diễn giải-Đơn giản hóa, Trang trọng-Diễn giải, Trang trọng-Đơn giản hóa, Trang trọng-Diễn giải-Đơn giản hóa, và Diễn giải-Đơn giản hóa (chi tiết hơn trong §A). Sau đó chúng tôi điều chỉnh tinh mô hình FLANT5-XL trên COEDIT-COMPOSITE (được gọi là COEDIT-XL-C). Chi tiết huấn luyện được tóm tắt trong §D.

Chúng tôi đánh giá COEDIT-XL-C trên cả hướng dẫn đơn và phức hợp. Cho các hướng dẫn đơn, chúng tôi sử dụng cùng thiết lập đánh giá như trong Bảng 2 và thấy rằng hiệu suất tổng thể của COEDIT-XL-C ngang bằng với COEDIT-XL (Bảng 6). Điều này cho thấy rằng việc huấn luyện mô hình thêm trên các lời nhắc phức hợp không có tác động tiêu cực đến hiệu suất tác vụ đơn.

Cho hướng dẫn phức hợp, chúng tôi tiến hành đánh giá con người vì không có bộ dữ liệu kiểm tra tiêu chuẩn có sẵn. Chúng tôi sử dụng ba kết hợp tác vụ mới ngoài bảy được thấy trong quá trình huấn luyện để đánh giá khả năng tổng quát hóa của mô hình. Đây là Mạch lạc-Diễn giải, Mạch lạc-Đơn giản hóa, và Mạch lạc-Đơn giản hóa-Diễn giải. Cụ thể, chúng tôi tiến hành hai bộ chú thích theo cặp (thiết lập tương tự như trong Phần 6.1) so sánh COEDIT-XL-C với GPT3-EDIT và COEDIT-XL (được hiển thị trong Bảng 7) trên 30 hướng dẫn phức hợp. Để so sánh công bằng với COEDIT-XL, chúng tôi chuẩn bị một quy trình chuỗi¹⁰ bằng cách phân tách hướng dẫn phức hợp thành một chuỗi nhiều hướng dẫn đơn và thực hiện chúng từng cái một. Trong 38% trường hợp, các chuyên gia thể hiện sự ưa thích cho COEDIT-XL-C, so với 34% cho GPT3-EDIT. Trong 3% trường hợp, cả hai mô hình được ưa thích như nhau, trong khi đối với 25% trường hợp, không có mô hình nào được ưa thích. Các chuyên gia ưa thích COEDIT-XL-C cho 34% trường hợp so với 21% cho baseline chuỗi. Cả hai đầu ra được ưa thích như nhau trong 14% trường hợp, trong khi đối với 31% trường hợp, cả hai mô hình tạo ra dự đoán không chấp nhận được. Bảng 13 cung cấp so sánh cạnh nhau của các đầu ra được tạo bởi các mô hình này.

7 Kết luận
Chúng tôi trình bày COEDIT– một bộ dữ liệu mã nguồn mở và tập hợp các mô hình ngôn ngữ lớn được điều chỉnh hướng dẫn có thể hoạt động như một trợ lý viết bằng cách tuân theo hướng dẫn ngôn ngữ tự nhiên để thực hiện các chỉnh sửa văn bản khác nhau bằng cách loại bỏ, cập nhật, hoặc thêm từ, cụm từ, và câu. COEDIT đạt được hiệu suất tiên tiến trên nhiều bộ đánh giá chỉnh sửa văn bản, bao gồm các yêu cầu chỉnh sửa cú pháp, ngữ nghĩa, và phong cách. Thông qua các thí nghiệm rộng rãi, chúng tôi đã cho thấy rằng COEDIT có khả năng tổng quát hóa thêm cho các hướng dẫn chưa thấy, liền kề, và phức hợp để thực hiện chỉnh sửa theo nhiều chiều trong một lượt. Trong các đánh giá con người của chúng tôi, chúng tôi quan sát thấy rằng COEDIT có thể hỗ trợ người viết với các khía cạnh khác nhau của quy trình sửa đổi văn bản ở quy mô bằng cách tuân theo hướng dẫn ngôn ngữ tự nhiên.

Hạn chế
Mặc dù COEDIT đạt được hiệu suất tiên tiến trên nhiều bộ đánh giá chỉnh sửa văn bản, chúng tôi thừa nhận một số hạn chế của cách tiếp cận và phương pháp đánh giá của chúng tôi. Điều chỉnh tinh cụ thể theo tác vụ của chúng tôi (như hầu hết các công trình khác) chủ yếu tập trung vào các tác vụ chỉnh sửa cấp câu, và hiệu quả của nó trên các chuỗi văn bản dài hơn nhiều phù hợp hơn với các thiết lập chỉnh sửa thế giới thực vẫn còn phải xem. Ngoài ra, hệ thống của chúng tôi chủ yếu tập trung vào các chỉnh sửa văn bản không thay đổi ý nghĩa, do đó, có thể hạn chế tiện ích của mô hình của chúng tôi đối với các tình huống thế giới thực hơn nơi cần chỉnh sửa hoặc sửa chữa dựa trên sự thật. Một hạn chế khác của công việc chúng tôi liên quan đến độ nhạy lời nhắc. Trong khi chúng tôi xây dựng các đầu vào của chúng tôi bằng cách ngẫu nhiên chọn từ một nhóm verbalizer cho mỗi tác vụ, chúng tôi thừa nhận rằng các lời nhắc khác nhau có thể gây ra các chỉnh sửa tốt hơn hoặc tệ hơn, và vì chúng tôi đánh giá mỗi đầu vào với một verbalizer ngẫu nhiên, so sánh được kiểm soát hoàn toàn cho mỗi lời nhắc có sẵn trên tất cả các mô hình không được thực hiện. Hơn nữa, định dạng nhắc được giữ đồng nhất trên tất cả các mô hình được đánh giá, trong khi một số mô hình có thể hoạt động tốt hơn với định dạng nhắc khác. Chúng tôi dự định giải quyết điều này trong công việc tương lai. Cuối cùng, yêu cầu tài nguyên tính toán có thể gây ra một số khó khăn trong việc sao chép kết quả (mà chúng tôi cố gắng giải quyết bằng cách chia sẻ các mô hình của chúng tôi công khai).

Tuyên bố đạo đức
Vì công việc của chúng tôi chủ yếu tập trung vào các chỉnh sửa văn bản không thay đổi ý nghĩa, chúng tôi có thể tránh nhiều vấn đề liên quan đến việc tạo ra văn bản có hại. Mặc dù vẫn có khả năng thay đổi ý nghĩa nhỏ cho các tác vụ phong cách do thiếu bối cảnh cụ thể của người dùng (Kulkarni và Raheja, 2023), chúng tôi cố gắng giảm cơ hội ảo giác bằng cách ràng buộc việc tạo ra các tác vụ chỉnh sửa nghiêm ngặt để giảm cơ hội thêm bất kỳ thông tin mới nào hoặc duy trì thiên lệch.

Lời cảm ơn
Chúng tôi chân thành cảm ơn Alice Kaiser-Schatzlein, Robyn Perry, Maya Barzilai, và Claudia Leacock đã cung cấp chuyên môn ngôn ngữ vô giá và phản hồi sâu sắc với các đánh giá. Chúng tôi cũng cảm ơn Max Gubin, Leonardo Neves, và Vivek Kulkarni vì các đề xuất hữu ích của họ.

[Phần tài liệu tham khảo và các bảng số liệu chi tiết được duy trì như trong bản gốc]
