Llama 2: Các Mô Hình Nền Tảng Mở và Mô Hình Chat Được Tinh Chỉnh

Hugo Touvron∗Louis Martin†Kevin Stone†
Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
Sergey Edunov Thomas Scialom∗
GenAI, Meta

Tóm tắt

Trong nghiên cứu này, chúng tôi phát triển và phát hành Llama 2, một bộ sưu tập các mô hình ngôn ngữ lớn (LLM) được tiền huấn luyện và tinh chỉnh với quy mô từ 7 tỷ đến 70 tỷ tham số. Các LLM được tinh chỉnh của chúng tôi, gọi là Llama 2-Chat, được tối ưu hóa cho các trường hợp sử dụng đối thoại. Các mô hình của chúng tôi vượt trội hơn các mô hình chat mã nguồn mở trên hầu hết các benchmark mà chúng tôi đã thử nghiệm, và dựa trên đánh giá của con người về tính hữu ích và an toàn, có thể là một sự thay thế phù hợp cho các mô hình nguồn đóng. Chúng tôi cung cấp mô tả chi tiết về phương pháp tinh chỉnh và cải thiện an toàn của Llama 2-Chat để cho phép cộng đồng xây dựng dựa trên công việc của chúng tôi và đóng góp vào việc phát triển có trách nhiệm các LLM.

Mục lục

1 Giới thiệu 3
2 Tiền huấn luyện 5
2.1 Dữ liệu Tiền huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Chi tiết Huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 Đánh giá Mô hình Tiền huấn luyện Llama 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Tinh chỉnh 8
3.1 Tinh chỉnh Có giám sát (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.2 Học Tăng cường với Phản hồi của Con người (RLHF) . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.3 Thông điệp Hệ thống cho Tính nhất quán Đa lượt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.4 Kết quả RLHF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4 An toàn 20
4.1 An toàn trong Tiền huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
4.2 Tinh chỉnh An toàn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.4 Đánh giá An toàn của Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
5 Thảo luận 32
5.1 Bài học và Quan sát . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
5.2 Hạn chế và Cân nhắc Đạo đức . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
5.3 Chiến lược Phát hành Có trách nhiệm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
6 Công trình Liên quan 35
7 Kết luận 36
A Phụ lục 46
A.1 Đóng góp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
A.2 Chi tiết Bổ sung cho Tiền huấn luyện . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
A.3 Chi tiết Bổ sung cho Tinh chỉnh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
A.4 Chi tiết Bổ sung cho An toàn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
A.5 Chú thích Dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
A.6 Nhiễm Bộ dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
A.7 Thẻ Mô hình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

Hình 1: Kết quả đánh giá tính hữu ích của con người cho Llama 2-Chat so với các mô hình mã nguồn mở và nguồn đóng khác. Người đánh giá so sánh các kết quả sinh của mô hình trên ~4k prompt bao gồm cả prompt đơn lượt và đa lượt. Khoảng tin cậy 95% cho đánh giá này nằm trong khoảng 1% đến 2%. Chi tiết hơn trong Phần 3.4.2. Khi xem xét các kết quả này, điều quan trọng cần lưu ý là đánh giá của con người có thể có nhiễu do hạn chế của bộ prompt, tính chủ quan của hướng dẫn đánh giá, tính chủ quan của từng người đánh giá và khó khăn vốn có trong việc so sánh các kết quả sinh.

Hình 2: Tỷ lệ thắng % cho tính hữu ích và an toàn giữa các baseline có giấy phép thương mại và Llama 2-Chat, theo GPT-4. Để bổ sung cho đánh giá của con người, chúng tôi sử dụng một mô hình có khả năng hơn, không tuân theo hướng dẫn riêng của chúng tôi. Vùng màu xanh lá cây cho biết mô hình của chúng tôi tốt hơn theo GPT-4. Để loại bỏ kết quả hòa, chúng tôi sử dụng thắng/(thắng+thua). Thứ tự mà các phản hồi của mô hình được trình bày cho GPT-4 được hoán đổi ngẫu nhiên để giảm thiểu thiên kiến.

1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) đã cho thấy tiềm năng lớn như các trợ lý AI có khả năng cao vượt trội trong các nhiệm vụ suy luận phức tạp đòi hỏi kiến thức chuyên môn trên nhiều lĩnh vực, bao gồm các lĩnh vực chuyên môn như lập trình và viết sáng tạo. Chúng cho phép tương tác với con người thông qua giao diện chat trực quan, điều này đã dẫn đến việc áp dụng nhanh chóng và rộng rãi trong công chúng.

Khả năng của LLM thật đáng chú ý khi xem xét bản chất dường như đơn giản của phương pháp huấn luyện. Các transformer tự hồi quy được tiền huấn luyện trên một corpus rộng lớn dữ liệu tự giám sát, sau đó được căn chỉnh với sở thích của con người thông qua các kỹ thuật như Học Tăng cường với Phản hồi của Con người (RLHF). Mặc dù phương pháp huấn luyện đơn giản, yêu cầu tính toán cao đã giới hạn việc phát triển LLM cho một số ít người chơi. Đã có các bản phát hành công khai của LLM tiền huấn luyện (như BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), và Falcon (Penedo et al., 2023)) có hiệu suất tương đương với các đối thủ tiền huấn luyện nguồn đóng như GPT-3 (Brown et al., 2020) và Chinchilla (Hoffmann et al., 2022), nhưng không có mô hình nào trong số này là sự thay thế phù hợp cho các LLM "sản phẩm" nguồn đóng, như ChatGPT, BARD, và Claude. Các LLM sản phẩm nguồn đóng này được tinh chỉnh kỹ lưỡng để căn chỉnh với sở thích của con người, điều này cải thiện đáng kể khả năng sử dụng và an toàn của chúng. Bước này có thể đòi hỏi chi phí đáng kể về tính toán và chú thích của con người, và thường không minh bạch hoặc dễ tái tạo, hạn chế tiến bộ trong cộng đồng để thúc đẩy nghiên cứu căn chỉnh AI.

Trong nghiên cứu này, chúng tôi phát triển và phát hành Llama 2, một họ LLM tiền huấn luyện và tinh chỉnh, Llama 2 và Llama 2-Chat, với quy mô lên đến 70B tham số. Trên loạt benchmark về tính hữu ích và an toàn mà chúng tôi đã thử nghiệm, các mô hình Llama 2-Chat nhìn chung hoạt động tốt hơn các mô hình mã nguồn mở hiện có. Chúng cũng có vẻ ngang tầm với một số mô hình nguồn đóng, ít nhất là trong các đánh giá của con người mà chúng tôi thực hiện (xem Hình 1 và 3). Chúng tôi đã thực hiện các biện pháp để tăng cường an toàn của các mô hình này, sử dụng chú thích dữ liệu và tinh chỉnh đặc biệt về an toàn, cũng như tiến hành red-teaming và sử dụng các đánh giá lặp đi lặp lại. Ngoài ra, bài báo này đóng góp một mô tả kỹ lưỡng về phương pháp tinh chỉnh và cách tiếp cận để cải thiện an toàn LLM. Chúng tôi hy vọng rằng sự cởi mở này sẽ cho phép cộng đồng tái tạo các LLM tinh chỉnh và tiếp tục cải thiện an toàn của các mô hình đó, mở đường cho việc phát triển LLM có trách nhiệm hơn.

Chúng tôi cũng chia sẻ các quan sát mới lạ mà chúng tôi đã thu được trong quá trình phát triển Llama 2 và Llama 2-Chat, như sự xuất hiện của việc sử dụng công cụ và tổ chức thời gian của kiến thức.

Chúng tôi đang phát hành các mô hình sau cho công chúng để sử dụng cho mục đích nghiên cứu và thương mại:

1. Llama 2, một phiên bản cập nhật của Llama 1, được huấn luyện trên một hỗn hợp mới dữ liệu công khai có sẵn. Chúng tôi cũng tăng kích thước của corpus tiền huấn luyện lên 40%, nhân đôi độ dài ngữ cảnh của mô hình, và áp dụng grouped-query attention (Ainslie et al., 2023). Chúng tôi đang phát hành các biến thể của Llama 2 với 7B, 13B, và 70B tham số. Chúng tôi cũng đã huấn luyện các biến thể 34B, mà chúng tôi báo cáo trong bài báo này nhưng không phát hành.

2. Llama 2-Chat, một phiên bản tinh chỉnh của Llama 2 được tối ưu hóa cho các trường hợp sử dụng đối thoại. Chúng tôi phát hành các biến thể của mô hình này với 7B, 13B, và 70B tham số.

Chúng tôi tin rằng việc phát hành mở LLM, khi được thực hiện một cách an toàn, sẽ mang lại lợi ích ròng cho xã hội. Như tất cả LLM, Llama 2 là một công nghệ mới mang theo những rủi ro tiềm ẩn khi sử dụng (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Việc thử nghiệm được tiến hành cho đến nay là bằng tiếng Anh và không - và không thể - bao phủ tất cả các tình huống. Do đó, trước khi triển khai bất kỳ ứng dụng nào của Llama 2-Chat, các nhà phát triển nên thực hiện thử nghiệm an toàn và tinh chỉnh phù hợp với các ứng dụng cụ thể của mô hình. Chúng tôi cung cấp hướng dẫn sử dụng có trách nhiệm và các ví dụ mã để tạo điều kiện cho việc triển khai an toàn Llama 2 và Llama 2-Chat. Chi tiết hơn về chiến lược phát hành có trách nhiệm của chúng tôi có thể được tìm thấy trong Phần 5.3.

Phần còn lại của bài báo này mô tả phương pháp tiền huấn luyện của chúng tôi (Phần 2), phương pháp tinh chỉnh (Phần 3), cách tiếp cận an toàn mô hình (Phần 4), các quan sát và hiểu biết chính (Phần 5), các công trình liên quan có liên quan (Phần 6), và kết luận (Phần 7).

Hình 4: Huấn luyện Llama 2-Chat: Quá trình này bắt đầu với việc tiền huấn luyện Llama 2 sử dụng các nguồn trực tuyến công khai có sẵn. Sau đó, chúng tôi tạo ra một phiên bản ban đầu của Llama 2-Chat thông qua việc áp dụng tinh chỉnh có giám sát. Tiếp theo, mô hình được tinh chỉnh một cách lặp đi lặp lại bằng cách sử dụng các phương pháp Học Tăng cường với Phản hồi của Con người (RLHF), cụ thể thông qua rejection sampling và Proximal Policy Optimization (PPO). Trong suốt giai đoạn RLHF, việc tích lũy dữ liệu mô hình hóa phần thưởng lặp đi lặp lại song song với việc cải thiện mô hình là rất quan trọng để đảm bảo các mô hình phần thưởng vẫn nằm trong phân phối.

2 Tiền huấn luyện

Để tạo ra họ mô hình Llama 2 mới, chúng tôi bắt đầu với phương pháp tiền huấn luyện được mô tả trong Touvron et al. (2023), sử dụng một transformer tự hồi quy được tối ưu hóa, nhưng đã thực hiện một số thay đổi để cải thiện hiệu suất. Cụ thể, chúng tôi đã thực hiện làm sạch dữ liệu mạnh mẽ hơn, cập nhật hỗn hợp dữ liệu của chúng tôi, huấn luyện trên 40% token tổng cộng nhiều hơn, nhân đôi độ dài ngữ cảnh, và sử dụng grouped-query attention (GQA) để cải thiện khả năng mở rộng suy luận cho các mô hình lớn hơn của chúng tôi. Bảng 1 so sánh các thuộc tính của các mô hình Llama 2 mới với các mô hình Llama 1.

2.1 Dữ liệu Tiền huấn luyện

Corpus huấn luyện của chúng tôi bao gồm một hỗn hợp mới dữ liệu từ các nguồn công khai có sẵn, không bao gồm dữ liệu từ các sản phẩm hoặc dịch vụ của Meta. Chúng tôi đã nỗ lực loại bỏ dữ liệu từ một số trang web nhất định được biết là chứa lượng lớn thông tin cá nhân về các cá nhân tư nhân. Chúng tôi huấn luyện trên 2 nghìn tỷ token dữ liệu vì điều này cung cấp sự cân bằng tốt giữa hiệu suất và chi phí, tăng cường các nguồn thực tế nhất trong nỗ lực tăng kiến thức và giảm thiểu ảo giác.

Chúng tôi đã thực hiện nhiều nghiên cứu dữ liệu tiền huấn luyện khác nhau để người dùng có thể hiểu rõ hơn về khả năng và hạn chế tiềm năng của các mô hình của chúng tôi; kết quả có thể được tìm thấy trong Phần 4.1.

2.2 Chi tiết Huấn luyện

Chúng tôi áp dụng hầu hết các cài đặt tiền huấn luyện và kiến trúc mô hình từ Llama 1. Chúng tôi sử dụng kiến trúc transformer tiêu chuẩn (Vaswani et al., 2017), áp dụng pre-normalization bằng RMSNorm (Zhang và Sennrich, 2019), sử dụng hàm kích hoạt SwiGLU (Shazeer, 2020), và rotary positional embeddings (RoPE, Su et al. 2022). Các khác biệt kiến trúc chính so với Llama 1 bao gồm tăng độ dài ngữ cảnh và grouped-query attention (GQA). Chúng tôi trình bày chi tiết trong Phần A.2.1 Phụ lục về từng khác biệt này với các thí nghiệm ablation để chứng minh tầm quan trọng của chúng.

Siêu tham số. Chúng tôi huấn luyện bằng cách sử dụng bộ tối ưu AdamW (Loshchilov và Hutter, 2017), với β1 = 0.9, β2 = 0.95, eps = 10−5. Chúng tôi sử dụng lịch trình tốc độ học cosine, với khởi động 2000 bước, và giảm tốc độ học cuối cùng xuống 10% tốc độ học đỉnh. Chúng tôi sử dụng weight decay là 0.1 và gradient clipping là 1.0. Hình 5 (a) cho thấy training loss cho Llama 2 với các siêu tham số này.

Bảng 1: Họ mô hình Llama 2. Số lượng token chỉ dữ liệu tiền huấn luyện. Tất cả các mô hình được huấn luyện với global batch-size là 4M token. Các mô hình lớn hơn — 34B và 70B — sử dụng Grouped-Query Attention (GQA) để cải thiện khả năng mở rộng suy luận.

Tokenizer. Chúng tôi sử dụng cùng tokenizer như Llama 1; nó sử dụng thuật toán byte pair encoding (BPE) (Sennrich et al., 2016) với implementation từ SentencePiece (Kudo và Richardson, 2018). Như với Llama 1, chúng tôi tách tất cả các số thành các chữ số riêng lẻ và sử dụng byte để phân tách các ký tự UTF-8 không xác định. Tổng kích thước từ vựng là 32k token.

2.2.1 Phần cứng Huấn luyện & Dấu chân Carbon

Phần cứng Huấn luyện. Chúng tôi tiền huấn luyện các mô hình của mình trên Meta's Research Super Cluster (RSC) (Lee và Sengupta, 2022) cũng như trên các cluster sản xuất nội bộ. Cả hai cluster đều sử dụng NVIDIA A100. Có hai khác biệt chính giữa hai cluster, với điều đầu tiên là loại kết nối có sẵn: RSC sử dụng NVIDIA Quantum InfiniBand trong khi cluster sản xuất của chúng tôi được trang bị giải pháp RoCE (RDMA over converged Ethernet) dựa trên các switch Ethernet thông thường. Cả hai giải pháp này đều kết nối các điểm cuối 200 Gbps. Khác biệt thứ hai là giới hạn tiêu thụ điện năng mỗi GPU—RSC sử dụng 400W trong khi cluster sản xuất của chúng tôi sử dụng 350W. Với thiết lập hai cluster này, chúng tôi có thể so sánh tính phù hợp của các loại kết nối khác nhau này cho huấn luyện quy mô lớn. RoCE (là mạng kết nối thương mại có giá cả phải chăng hơn) có thể mở rộng gần như tốt như Infiniband đắt tiền lên đến 2000 GPU, điều này làm cho tiền huấn luyện thậm chí còn dân chủ hóa hơn.

Dấu chân Carbon của Tiền huấn luyện. Theo nghiên cứu trước đó (Bender et al., 2021a; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022) và sử dụng ước tính tiêu thụ điện năng của các thiết bị GPU và hiệu quả carbon, chúng tôi nhằm tính toán lượng khí thải carbon do tiền huấn luyện các mô hình Llama 2. Việc sử dụng điện năng thực tế của GPU phụ thuộc vào việc sử dụng và có thể khác với Thermal Design Power (TDP) mà chúng tôi sử dụng như một ước tính cho điện năng GPU. Điều quan trọng cần lưu ý là các tính toán của chúng tôi không tính đến các nhu cầu điện năng khác, như từ kết nối hoặc tiêu thụ điện năng máy chủ không phải GPU, cũng như từ các hệ thống làm mát trung tâm dữ liệu. Ngoài ra, đầu ra carbon liên quan đến việc sản xuất phần cứng AI, như GPU, có thể thêm vào tổng dấu chân carbon như được đề xuất bởi Gupta et al. (2022b,a).

Bảng 2 tóm tắt lượng khí thải carbon cho việc tiền huấn luyện họ mô hình Llama 2. Tổng cộng 3.3M giờ GPU tính toán được thực hiện trên phần cứng loại A100-80GB (TDP 400W hoặc 350W). Chúng tôi ước tính tổng lượng khí thải cho huấn luyện là 539 tCO2eq, trong đó 100% được bù đắp trực tiếp bởi chương trình bền vững của Meta. Chiến lược phát hành mở của chúng tôi cũng có nghĩa là các chi phí tiền huấn luyện này sẽ không cần phải chịu bởi các công ty khác, tiết kiệm nhiều tài nguyên toàn cầu hơn.

2.3 Đánh giá Mô hình Tiền huấn luyện Llama 2

Trong phần này, chúng tôi báo cáo kết quả cho các mô hình cơ sở Llama 1 và Llama 2, các mô hình MosaicML Pretrained Transformer (MPT), và các mô hình Falcon (Almazrouei et al., 2023) trên các benchmark học thuật tiêu chuẩn. Đối với tất cả các đánh giá, chúng tôi sử dụng thư viện đánh giá nội bộ của chúng tôi. Chúng tôi tái tạo kết quả cho các mô hình MPT và Falcon một cách nội bộ. Đối với các mô hình này, chúng tôi luôn chọn điểm số tốt nhất giữa framework đánh giá của chúng tôi và bất kỳ kết quả được báo cáo công khai nào.

Trong Bảng 3, chúng tôi tóm tắt hiệu suất tổng thể trên một bộ benchmark phổ biến. Lưu ý rằng các benchmark an toàn được chia sẻ trong Phần 4.1. Các benchmark được nhóm thành các danh mục được liệt kê dưới đây. Kết quả cho tất cả các benchmark riêng lẻ có sẵn trong Phần A.2.2.

• Mã. Chúng tôi báo cáo điểm pass@1 trung bình của các mô hình của chúng tôi trên HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021).

• Suy luận Thông thường. Chúng tôi báo cáo trung bình của PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy và challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), và CommonsenseQA (Talmor et al., 2018). Chúng tôi báo cáo kết quả 7-shot cho CommonSenseQA và kết quả 0-shot cho tất cả các benchmark khác.

• Kiến thức Thế giới. Chúng tôi đánh giá hiệu suất 5-shot trên Natural Questions (Kwiatkowski et al., 2019) và TriviaQA (Joshi et al., 2017) và báo cáo trung bình.

• Hiểu đọc. Đối với hiểu đọc, chúng tôi báo cáo trung bình 0-shot trên SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), và BoolQ (Clark et al., 2019).

• MATH. Chúng tôi báo cáo trung bình của GSM8K (8 shot) (Cobbe et al., 2021) và MATH (4 shot) (Hendrycks et al., 2021) benchmark ở top 1.

• Benchmark Tổng hợp Phổ biến. Chúng tôi báo cáo kết quả tổng thể cho MMLU (5 shot) (Hendrycks et al., 2020), Big Bench Hard (BBH) (3 shot) (Suzgun et al., 2022), và AGI Eval (3–5 shot) (Zhong et al., 2023). Đối với AGI Eval, chúng tôi chỉ đánh giá trên các nhiệm vụ tiếng Anh và báo cáo trung bình.

Như được hiển thị trong Bảng 3, các mô hình Llama 2 vượt trội hơn các mô hình Llama 1. Cụ thể, Llama 2 70B cải thiện kết quả trên MMLU và BBH lần lượt ≈5 và ≈8 điểm so với Llama 1 65B. Các mô hình Llama 2 7B và 30B vượt trội hơn các mô hình MPT có kích thước tương ứng trên tất cả các danh mục ngoại trừ benchmark mã. Đối với các mô hình Falcon, Llama 2 7B và 34B vượt trội hơn Falcon 7B và 40B trên tất cả các danh mục benchmark. Ngoài ra, mô hình Llama 2 70B vượt trội hơn tất cả các mô hình mã nguồn mở.

Ngoài các mô hình mã nguồn mở, chúng tôi cũng so sánh kết quả Llama 2 70B với các mô hình nguồn đóng. Như được hiển thị trong Bảng 4, Llama 2 70B gần với GPT-3.5 (OpenAI, 2023) trên MMLU và GSM8K, nhưng có khoảng cách đáng kể trên các benchmark mã hóa. Kết quả Llama 2 70B ngang bằng hoặc tốt hơn PaLM (540B) (Chowdhery et al., 2022) trên hầu hết tất cả các benchmark. Vẫn còn khoảng cách lớn về hiệu suất giữa Llama 2 70B và GPT-4 và PaLM-2-L.

Chúng tôi cũng phân tích khả năng nhiễm dữ liệu tiềm ẩn và chia sẻ chi tiết trong Phần A.6.

Bảng 3: Hiệu suất tổng thể trên các benchmark học thuật được nhóm so với các mô hình cơ sở mã nguồn mở.

Bảng 4: So sánh với các mô hình nguồn đóng trên các benchmark học thuật. Kết quả cho GPT-3.5 và GPT-4 từ OpenAI (2023). Kết quả cho mô hình PaLM từ Chowdhery et al. (2022). Kết quả cho PaLM-2-L từ Anil et al. (2023).

3 Tinh chỉnh

Llama 2-Chat là kết quả của vài tháng nghiên cứu và ứng dụng lặp đi lặp lại các kỹ thuật căn chỉnh, bao gồm cả instruction tuning và RLHF, đòi hỏi tài nguyên tính toán và chú thích đáng kể. Trong phần này, chúng tôi báo cáo về các thí nghiệm và phát hiện của chúng tôi bằng cách sử dụng supervised fine-tuning (Phần 3.1), cũng như reward modeling ban đầu và lặp đi lặp lại (Phần 3.2.2) và RLHF (Phần 3.2.3). Chúng tôi cũng chia sẻ một kỹ thuật mới, Ghost Attention (GAtt), mà chúng tôi thấy giúp kiểm soát luồng đối thoại qua nhiều lượt (Phần 3.3). Xem Phần 4.2 để đánh giá an toàn trên các mô hình tinh chỉnh.

3.1 Supervised Fine-Tuning (SFT)

Bắt đầu. Để khởi động, chúng tôi bắt đầu giai đoạn SFT với dữ liệu instruction tuning có sẵn công khai (Chung et al., 2022), như được sử dụng trước đó trong Touvron et al. (2023).

Chất lượng là Tất cả những gì Bạn Cần. Dữ liệu SFT của bên thứ ba có sẵn từ nhiều nguồn khác nhau, nhưng chúng tôi thấy rằng nhiều trong số này có sự đa dạng và chất lượng không đủ—đặc biệt để căn chỉnh LLM theo hướng instruction đối thoại. Do đó, chúng tôi tập trung đầu tiên vào việc thu thập vài nghìn ví dụ dữ liệu SFT chất lượng cao, như được minh họa trong Bảng 5. Bằng cách gạt bỏ hàng triệu ví dụ từ các bộ dữ liệu của bên thứ ba và sử dụng ít hơn nhưng chất lượng cao hơn từ các nỗ lực chú thích dựa trên vendor của chúng tôi, kết quả của chúng tôi cải thiện đáng kể. Những phát hiện này tương tự tinh thần với Zhou et al. (2023), cũng thấy rằng một bộ dữ liệu instruction-tuning sạch có giới hạn có thể đủ để đạt được mức chất lượng cao. Chúng tôi thấy rằng các chú thích SFT theo thứ tự hàng chục nghìn là đủ để đạt được kết quả chất lượng cao. Chúng tôi ngừng chú thích SFT sau khi thu thập tổng cộng 27,540 chú thích. Lưu ý rằng chúng tôi không bao gồm bất kỳ dữ liệu người dùng Meta nào.

Chúng tôi cũng quan sát thấy rằng các nền tảng chú thích và vendor khác nhau có thể dẫn đến hiệu suất mô hình downstream khác biệt rõ rệt, làm nổi bật tầm quan trọng của việc kiểm tra dữ liệu ngay cả khi sử dụng vendor để cung cấp chú thích. Để xác thực chất lượng dữ liệu của chúng tôi, chúng tôi đã kiểm tra kỹ lưỡng một bộ 180 ví dụ, so sánh các chú thích do con người cung cấp với các mẫu được tạo bởi mô hình thông qua kiểm tra thủ công. Đáng ngạc nhiên, chúng tôi thấy rằng các đầu ra được lấy mẫu từ mô hình SFT kết quả thường cạnh tranh với dữ liệu SFT được viết tay bởi các chú thích viên con người, đề xuất rằng chúng tôi có thể ưu tiên lại và dành nhiều nỗ lực chú thích hơn cho chú thích dựa trên sở thích cho RLHF.

Chi tiết Tinh chỉnh. Đối với supervised fine-tuning, chúng tôi sử dụng lịch trình tốc độ học cosine với tốc độ học ban đầu là 2×10−5, weight decay là 0.1, batch size là 64, và độ dài chuỗi là 4096 token. Đối với quá trình tinh chỉnh, mỗi mẫu bao gồm một prompt và một câu trả lời. Để đảm bảo độ dài chuỗi mô hình được điền đúng cách, chúng tôi nối tất cả các prompt và câu trả lời từ tập huấn luyện. Một token đặc biệt được sử dụng để tách các phần prompt và câu trả lời. Chúng tôi sử dụng objective tự hồi quy và zero-out loss trên các token từ user prompt, do đó chúng tôi chỉ backpropagate trên các token câu trả lời. Cuối cùng, chúng tôi tinh chỉnh mô hình trong 2 epoch.

3.2 Reinforcement Learning with Human Feedback (RLHF)

RLHF là một thủ tục huấn luyện mô hình được áp dụng cho mô hình ngôn ngữ tinh chỉnh để căn chỉnh thêm hành vi mô hình với sở thích của con người và theo dõi instruction. Chúng tôi thu thập dữ liệu đại diện cho sở thích con người được lấy mẫu thực nghiệm, trong đó các chú thích viên con người chọn đầu ra mô hình nào họ thích hơn trong hai đầu ra. Phản hồi con người này sau đó được sử dụng để huấn luyện một mô hình phần thưởng, mô hình này học các mẫu trong sở thích của các chú thích viên con người và sau đó có thể tự động hóa các quyết định sở thích.

3.2.1 Thu thập Dữ liệu Sở thích Con người

Tiếp theo, chúng tôi thu thập dữ liệu sở thích con người cho reward modeling. Chúng tôi chọn giao thức so sánh nhị phân hơn các scheme khác, chủ yếu vì nó cho phép chúng tôi tối đa hóa sự đa dạng của các prompt được thu thập. Tuy nhiên, các chiến lược khác đáng được xem xét, mà chúng tôi để dành cho công việc tương lai.

Thủ tục chú thích của chúng tôi tiến hành như sau. Chúng tôi yêu cầu các chú thích viên đầu tiên viết một prompt, sau đó chọn giữa hai phản hồi mô hình được lấy mẫu, dựa trên các tiêu chí được cung cấp. Để tối đa hóa sự đa dạng, hai phản hồi cho một prompt nhất định được lấy mẫu từ hai biến thể mô hình khác nhau, và thay đổi siêu tham số nhiệt độ. Ngoài việc đưa cho người tham gia một lựa chọn bắt buộc, chúng tôi cũng yêu cầu các chú thích viên gắn nhãn mức độ họ thích lựa chọn của họ hơn so với thay thế: hoặc lựa chọn của họ tốt hơn đáng kể, tốt hơn, tốt hơn một chút, hoặc tốt hơn không đáng kể/không chắc chắn.

Đối với việc thu thập chú thích sở thích của chúng tôi, chúng tôi tập trung vào tính hữu ích và an toàn. Tính hữu ích đề cập đến mức độ các phản hồi Llama 2-Chat đáp ứng yêu cầu của người dùng và cung cấp thông tin được yêu cầu; an toàn đề cập đến việc liệu các phản hồi của Llama 2-Chat có không an toàn hay không, ví dụ: "đưa ra hướng dẫn chi tiết về việc chế tạo bom" có thể được coi là hữu ích nhưng không an toàn theo hướng dẫn an toàn của chúng tôi. Việc tách biệt hai điều này cho phép chúng tôi áp dụng các hướng dẫn cụ thể cho từng cái và hướng dẫn các chú thích viên tốt hơn; ví dụ, các chú thích an toàn của chúng tôi cung cấp hướng dẫn tập trung vào các prompt đối nghịch, trong số các hướng dẫn khác.

Ngoài sự khác biệt trong hướng dẫn chú thích, chúng tôi bổ sung thu thập một nhãn an toàn trong giai đoạn an toàn. Thông tin bổ sung này phân loại các phản hồi mô hình thành một trong ba danh mục: 1) phản hồi được ưa thích là an toàn và phản hồi khác thì không, 2) cả hai phản hồi đều an toàn, và 3) cả hai phản hồi đều không an toàn, với 18%, 47%, và 35% bộ dữ liệu an toàn lần lượt rơi vào mỗi nhóm. Chúng tôi không bao gồm bất kỳ ví dụ nào mà phản hồi được chọn không an toàn và phản hồi khác an toàn, vì chúng tôi tin rằng các phản hồi an toàn hơn cũng sẽ tốt hơn/được ưa thích bởi con người. Hướng dẫn an toàn và thông tin chi tiết hơn về chú thích an toàn có thể được tìm thấy trong Phần 4.2.1.

Các chú thích của con người được thu thập theo đợt hàng tuần. Khi chúng tôi thu thập thêm dữ liệu sở thích, các mô hình phần thưởng của chúng tôi được cải thiện, và chúng tôi có thể huấn luyện các phiên bản tốt hơn một cách tiệm tiến cho Llama 2-Chat (xem kết quả trong Phần 5, Hình 20). Việc cải thiện Llama 2-Chat cũng thay đổi phân phối dữ liệu của mô hình. Vì độ chính xác mô hình phần thưởng có thể suy giảm nhanh chóng nếu không tiếp xúc với phân phối mẫu mới này, tức là từ siêu chuyên môn hóa (Scialom et al., 2020b), điều quan trọng trước mỗi lần lặp tinh chỉnh Llama 2-Chat mới là thu thập dữ liệu sở thích mới bằng cách sử dụng các lần lặp Llama 2-Chat mới nhất. Bước này giúp giữ mô hình phần thưởng on-distribution và duy trì phần thưởng chính xác cho mô hình mới nhất.

Trong Bảng 6, chúng tôi báo cáo thống kê của dữ liệu reward modeling mà chúng tôi đã thu thập theo thời gian, và trình bày chúng so với nhiều bộ dữ liệu sở thích mã nguồn mở bao gồm Anthropic Helpful and Harmless (Bai et al., 2022a), OpenAI Summarize (Stiennon et al., 2020), OpenAI WebGPT (Nakano et al., 2021), StackExchange (Lambert et al., 2023), Stanford Human Preferences (Ethayarajh et al., 2022), và Synthetic GPT-J (Havrilla). Chúng tôi đã thu thập một bộ dữ liệu lớn hơn một triệu so sánh nhị phân dựa trên con người áp dụng các hướng dẫn được chỉ định của chúng tôi, mà chúng tôi gọi là dữ liệu reward modeling Meta. Lưu ý rằng số lượng token trong prompt và câu trả lời khác nhau tùy thuộc vào miền văn bản. Dữ liệu tóm tắt và diễn đàn trực tuyến thường có prompt dài hơn, trong khi prompt kiểu đối thoại thường ngắn hơn. So với các bộ dữ liệu mã nguồn mở hiện có, dữ liệu sở thích của chúng tôi có nhiều lượt đối thoại hơn và dài hơn trung bình.

3.2.2 Reward Modeling

Mô hình phần thưởng nhận một phản hồi mô hình và prompt tương ứng của nó (bao gồm ngữ cảnh từ các lượt trước) làm đầu vào và đưa ra một điểm số scalar để chỉ ra chất lượng (ví dụ: tính hữu ích và an toàn) của việc sinh mô hình. Tận dụng các điểm phản hồi như phần thưởng, chúng tôi có thể tối ưu hóa Llama 2-Chat trong RLHF để căn chỉnh sở thích con người tốt hơn và cải thiện tính hữu ích và an toàn.

Những người khác đã thấy rằng tính hữu ích và an toàn đôi khi trade-off (Bai et al., 2022a), điều này có thể làm cho việc một mô hình phần thưởng duy nhất hoạt động tốt trên cả hai trở nên thách thức. Để giải quyết điều này, chúng tôi huấn luyện hai mô hình phần thưởng riêng biệt, một được tối ưu hóa cho tính hữu ích (được gọi là Helpfulness RM) và một cho an toàn (Safety RM).

Chúng tôi khởi tạo các mô hình phần thưởng của mình từ các checkpoint mô hình chat tiền huấn luyện, vì nó đảm bảo rằng cả hai mô hình đều được hưởng lợi từ kiến thức thu được trong tiền huấn luyện. Nói ngắn gọn, mô hình phần thưởng "biết" những gì mô hình chat biết. Điều này ngăn ngừa các trường hợp mà, chẳng hạn, hai mô hình sẽ có sự không khớp thông tin, có thể dẫn đến việc ủng hộ ảo giác. Kiến trúc mô hình và siêu tham số giống hệt với các mô hình ngôn ngữ tiền huấn luyện, ngoại trừ classification head cho dự đoán token tiếp theo được thay thế bằng regression head để đưa ra phần thưởng scalar.

Mục tiêu Huấn luyện. Để huấn luyện mô hình phần thưởng, chúng tôi chuyển đổi dữ liệu sở thích con người được thu thập của chúng tôi thành định dạng nhãn xếp hạng nhị phân (tức là chosen & rejected) và thực thi phản hồi được chọn có điểm số cao hơn đối tác của nó. Chúng tôi sử dụng binary ranking loss nhất quán với Ouyang et al. (2022):

Lranking = −log(σ(rθ(x, yc) − rθ(x, yr))) (1)

trong đó rθ(x, y) là điểm số scalar đầu ra cho prompt x và completion y với trọng số mô hình θ. yc là phản hồi được ưa thích mà các chú thích viên chọn và yr là đối tác bị từ chối.

Được xây dựng dựa trên binary ranking loss này, chúng tôi tiếp tục sửa đổi nó riêng biệt để có mô hình phần thưởng tính hữu ích và an toàn tốt hơn như sau. Cho rằng xếp hạng sở thích của chúng tôi được phân tách thành thang điểm bốn điểm (ví dụ: tốt hơn đáng kể), như được trình bày trong Phần 3.2.1, có thể hữu ích để tận dụng thông tin này để dạy mô hình phần thưởng một cách rõ ràng gán điểm số khác biệt hơn cho các kết quả sinh có nhiều khác biệt hơn. Để làm như vậy, chúng tôi thêm một thành phần margin trong loss:

Lranking = −log(σ(rθ(x, yc) − rθ(x, yr) − m(r))) (2)

trong đó margin m(r) là một hàm rời rạc của xếp hạng sở thích. Tự nhiên, chúng tôi sử dụng margin lớn cho các cặp có phản hồi khác biệt, và margin nhỏ hơn cho những cặp có phản hồi tương tự (được hiển thị trong Bảng 27). Chúng tôi thấy rằng thành phần margin này có thể cải thiện độ chính xác mô hình phần thưởng Helpfulness đặc biệt trên các mẫu mà hai phản hồi có thể tách biệt hơn. Phân tích ablation và chi tiết hơn có thể được tìm thấy trong Bảng 28 trong Phụ lục A.3.3.

Thành phần Dữ liệu. Chúng tôi kết hợp dữ liệu mới thu thập của chúng tôi với các bộ dữ liệu sở thích mã nguồn mở hiện có để tạo thành một bộ dữ liệu huấn luyện lớn hơn. Ban đầu, các bộ dữ liệu mã nguồn mở được sử dụng để bootstrap các mô hình phần thưởng của chúng tôi trong khi chúng tôi đang trong quá trình thu thập dữ liệu chú thích sở thích. Chúng tôi lưu ý rằng trong bối cảnh RLHF trong nghiên cứu này, vai trò của tín hiệu phần thưởng là học sở thích con người cho đầu ra Llama 2-Chat thay vì bất kỳ đầu ra mô hình nào. Tuy nhiên, trong các thí nghiệm của chúng tôi, chúng tôi không quan sát thấy sự chuyển giao tiêu cực từ các bộ dữ liệu sở thích mã nguồn mở. Do đó, chúng tôi đã quyết định giữ chúng trong hỗn hợp dữ liệu của chúng tôi, vì chúng có thể cho phép khái quát hóa tốt hơn cho mô hình phần thưởng và ngăn ngừa reward hacking, tức là Llama 2-Chat lợi dụng một số điểm yếu của phần thưởng của chúng tôi, và do đó làm tăng điểm số một cách giả tạo mặc dù hoạt động kém hơn.

Với dữ liệu huấn luyện có sẵn từ các nguồn khác nhau, chúng tôi đã thử nghiệm với các công thức trộn khác nhau cho cả mô hình phần thưởng Helpfulness và Safety để xác định các cài đặt tốt nhất. Sau nhiều thí nghiệm, mô hình phần thưởng Helpfulness cuối cùng được huấn luyện trên tất cả dữ liệu Meta Helpfulness, kết hợp với các phần bằng nhau của dữ liệu còn lại được lấy mẫu đồng đều từ Meta Safety và từ các bộ dữ liệu mã nguồn mở. Mô hình phần thưởng Meta Safety được huấn luyện trên tất cả dữ liệu Meta Safety và Anthropic Harmless, trộn với dữ liệu Meta Helpfulness và dữ liệu tính hữu ích mã nguồn mở theo tỷ lệ 90/10. Chúng tôi thấy rằng cài đặt với 10% dữ liệu tính hữu ích đặc biệt có lợi cho độ chính xác trên các mẫu mà cả phản hồi được chọn và bị từ chối đều được coi là an toàn.

Chi tiết Huấn luyện. Chúng tôi huấn luyện một epoch trên dữ liệu huấn luyện. Trong các thí nghiệm trước đó, chúng tôi thấy rằng huấn luyện lâu hơn có thể dẫn đến over-fitting. Chúng tôi sử dụng cùng tham số optimizer như cho mô hình cơ sở. Tốc độ học tối đa là 5×10−6 cho Llama 2-Chat 70B tham số và 1×10−5 cho phần còn lại. Tốc độ học được giảm trên lịch trình tốc độ học cosine, xuống 10% tốc độ học tối đa. Chúng tôi sử dụng warm-up 3% tổng số bước, với tối thiểu là 5. Kích thước batch hiệu quả được giữ cố định ở 512 cặp, hoặc 1024 hàng mỗi batch.

Kết quả Mô hình Phần thưởng. Trên mỗi đợt chú thích sở thích con người cho reward modeling, chúng tôi đã giữ lại 1000 ví dụ làm tập test để đánh giá các mô hình của chúng tôi. Chúng tôi đề cập đến hợp của tất cả prompt cho các tập test tương ứng là "Meta Helpfulness" và "Meta Safety."

Như các điểm tham chiếu, chúng tôi cũng đánh giá các lựa chọn thay thế công khai có sẵn khác làm baseline: SteamSHP-XL (Ethayarajh et al., 2022) dựa trên FLAN-T5-xl, mô hình phần thưởng Open Assistant (Köpf et al., 2023) dựa trên DeBERTa V3 Large (He et al., 2020), và GPT4 có thể truy cập thông qua API của OpenAI. Lưu ý rằng tại thời điểm suy luận, trái ngược với huấn luyện, tất cả các mô hình phần thưởng có thể dự đoán scalar cho một đầu ra duy nhất, mà không cần truy cập đầu ra được ghép đôi của nó. Đối với GPT-4, chúng tôi prompt với câu hỏi zero-shot "Choose the best answer between A and B," trong đó A và B là hai phản hồi để so sánh.

Chúng tôi báo cáo kết quả theo độ chính xác trong Bảng 7. Như mong đợi, các mô hình phần thưởng riêng của chúng tôi hoạt động tốt nhất trên các tập test nội bộ được thu thập dựa trên Llama 2-Chat, với mô hình phần thưởng Helpfulness hoạt động tốt nhất trên tập test Meta Helpfulness, và tương tự mô hình phần thưởng Safety hoạt động tốt nhất trên tập test Meta Safety. Nhìn chung, các mô hình phần thưởng của chúng tôi vượt trội hơn tất cả các baseline, bao gồm GPT-4. Thú vị là, GPT-4 hoạt động tốt hơn các mô hình phần thưởng không phải Meta khác, mặc dù không được huấn luyện trực tiếp cũng như không nhắm mục tiêu cụ thể cho nhiệm vụ reward modeling này.

Thực tế là tính hữu ích và an toàn hoạt động tốt nhất trong lĩnh vực riêng của chúng có khả năng do sự căng thẳng giữa hai mục tiêu (tức là hữu ích nhất có thể so với từ chối prompt không an toàn khi cần thiết), điều này có thể gây nhầm lẫn cho mô hình phần thưởng trong quá trình huấn luyện. Để một mô hình duy nhất hoạt động tốt trên cả hai chiều, nó cần không chỉ học cách chọn phản hồi tốt hơn cho một prompt mà còn phân biệt các prompt đối nghịch khỏi các prompt an toàn. Do đó, việc tối ưu hóa hai mô hình riêng biệt làm giảm nhiệm vụ reward modeling. Phân tích chi tiết hơn về sự căng thẳng giữa an toàn và tính hữu ích có thể được tìm thấy trong Phụ lục A.4.1.

Khi chúng tôi nhóm điểm số theo xếp hạng sở thích trong Bảng 8, chúng tôi có thể thấy rằng độ chính xác vượt trội cho tập test "significantly better" và suy giảm dần khi các cặp so sánh trở nên tương tự hơn (ví dụ: "slightly better"). Điều này được mong đợi rằng việc học mô hình hóa sở thích con người trở nên thách thức khi quyết định giữa hai phản hồi mô hình tương tự, do tính chủ quan của chú thích viên và sự phụ thuộc của họ vào các chi tiết tinh tế có thể phân biệt các phản hồi. Chúng tôi nhấn mạnh rằng độ chính xác trên các phản hồi khác biệt hơn quan trọng nhất để cải thiện hiệu suất Llama 2-Chat. Tỷ lệ đồng ý chú thích sở thích con người cũng cao hơn trên các phản hồi khác biệt hơn so với các cặp tương tự.

Xu hướng Mở rộng. Chúng tôi nghiên cứu xu hướng mở rộng về dữ liệu và kích thước mô hình cho mô hình phần thưởng, tinh chỉnh các kích thước mô hình khác nhau trên lượng dữ liệu mô hình phần thưởng tăng dần được thu thập mỗi tuần (xem chi tiết về khối lượng mỗi đợt trong Bảng 26). Hình 6 báo cáo các xu hướng này, cho thấy kết quả mong đợi rằng các mô hình lớn hơn đạt được hiệu suất cao hơn cho khối lượng dữ liệu tương tự. Quan trọng hơn, hiệu suất mở rộng vẫn chưa đạt mức cao nhất với khối lượng chú thích dữ liệu hiện có được sử dụng để huấn luyện, một tín hiệu cho thấy có chỗ cho cải thiện nhiều hơn với nhiều chú thích hơn. Chúng tôi lưu ý rằng độ chính xác mô hình phần thưởng là một trong những proxy quan trọng nhất cho hiệu suất cuối cùng của Llama 2-Chat. Trong khi các thực hành tốt nhất để đánh giá toàn diện một mô hình sinh là một câu hỏi nghiên cứu mở, nhiệm vụ xếp hạng của phần thưởng không có sự mơ hồ. Do đó, mọi thứ khác bằng nhau, sự cải thiện của mô hình phần thưởng có thể được dịch trực tiếp thành cải thiện cho Llama 2-Chat.

3.2.3 Tinh chỉnh Lặp đi lặp lại

Khi chúng tôi nhận được nhiều đợt dữ liệu chú thích sở thích con người hơn, chúng tôi có thể huấn luyện các mô hình phần thưởng tốt hơn và thu thập thêm prompt. Do đó chúng tôi đã huấn luyện các phiên bản liên tiếp cho các mô hình RLHF, được gọi ở đây là RLHF-V1, ..., RLHF-V5.

Chúng tôi đã khám phá tinh chỉnh RLHF với hai thuật toán chính:

• Proximal Policy Optimization (PPO) (Schulman et al., 2017), tiêu chuẩn trong tài liệu RLHF.

• Rejection Sampling fine-tuning. Chúng tôi lấy mẫu K đầu ra từ mô hình và chọn ứng viên tốt nhất với phần thưởng của chúng tôi, nhất quán với Bai et al. (2022b). Cùng chiến lược re-ranking cho LLM cũng được đề xuất trong Deng et al. (2019), trong đó phần thưởng được xem như một hàm năng lượng. Ở đây, chúng tôi đi xa hơn một bước, và sử dụng các đầu ra đã chọn cho một cập nhật gradient. Đối với mỗi prompt, mẫu có điểm phần thưởng cao nhất được coi là tiêu chuẩn vàng mới. Tương tự như Scialom et al. (2020a), chúng tôi sau đó tinh chỉnh mô hình của chúng tôi trên tập các mẫu được xếp hạng mới, củng cố phần thưởng.

Hai thuật toán RL chủ yếu khác nhau ở:

• Breadth — trong Rejection Sampling, mô hình khám phá K mẫu cho một prompt nhất định, trong khi chỉ một thế hệ được thực hiện cho PPO.

• Depth — trong PPO, trong quá trình huấn luyện tại bước t, mẫu là một hàm của chính sách mô hình được cập nhật từ t−1 sau cập nhật gradient của bước trước. Trong Rejection Sampling fine-tuning, chúng tôi lấy mẫu tất cả các đầu ra với chính sách ban đầu của mô hình để thu thập một bộ dữ liệu mới, trước khi áp dụng tinh chỉnh tương tự như SFT. Tuy nhiên, vì chúng tôi áp dụng cập nhật mô hình lặp đi lặp lại, sự khác biệt cơ bản giữa hai thuật toán RL ít rõ ràng hơn.

Cho đến RLHF (V4), chúng tôi chỉ sử dụng Rejection Sampling fine-tuning, và sau đó, chúng tôi kết hợp hai cái một cách tuần tự, áp dụng PPO trên top của checkpoint Rejection Sampling kết quả trước khi lấy mẫu lại.

Rejection Sampling. Chúng tôi thực hiện rejection sampling chỉ với Llama 2-Chat 70B lớn nhất của chúng tôi. Tất cả các mô hình nhỏ hơn được tinh chỉnh trên dữ liệu rejection sampled từ mô hình lớn hơn, do đó chưng cất các khả năng mô hình lớn vào các mô hình nhỏ hơn. Chúng tôi để dành phân tích thêm về hiệu ứng của việc chưng cất này cho công việc tương lai.

Tại mỗi giai đoạn lặp đi lặp lại, chúng tôi lấy mẫu K câu trả lời cho mỗi prompt từ mô hình gần đây nhất. Chúng tôi chấm điểm mỗi mẫu với mô hình phần thưởng tốt nhất có thể truy cập tại thời điểm thí nghiệm, và sau đó chọn câu trả lời tốt nhất cho một prompt nhất định. Trong các phiên bản trước đó của mô hình của chúng tôi, lên đến RLHF V3, cách tiếp cận của chúng tôi là giới hạn việc chọn câu trả lời chỉ trong "túi" các mẫu được thu thập từ lần lặp trước. Ví dụ, RLHF V3 được huấn luyện chỉ sử dụng các mẫu từ RLHF V2. Tuy nhiên, mặc dù có cải thiện liên tục, phương pháp này dẫn đến suy thoái trong một số khả năng. Ví dụ, RLHF V3 gặp khó khăn hơn các phiên bản trước trong việc soạn các dòng thơ có vần, như được phân biệt thông qua phân tích định tính, đề xuất rằng nghiên cứu thêm về nguyên nhân và giảm thiểu cho việc quên lãng (Kirkpatrick et al., 2017; Nguyen et al., 2019; Ramasesh et al., 2021) có thể là một lĩnh vực màu mỡ cho nghiên cứu tương lai bổ sung.

Để đáp ứng, trong các lần lặp tiếp theo, chúng tôi đã sửa đổi chiến lược của mình, kết hợp các mẫu hoạt động tốt nhất từ tất cả các lần lặp trước, như những mẫu được sử dụng trong RLHF-V1 và RLHF-V2. Mặc dù chúng tôi không trình bày các con số cụ thể, điều chỉnh này cho thấy sự cải thiện đáng kể về hiệu suất và giải quyết hiệu quả các vấn đề đã lưu ý trước đó. Việc giảm thiểu này có thể được xem như tương tự với Synnaeve et al. (2019) và Vinyals et al. (2019) trong tài liệu RL.

Chúng tôi minh họa lợi ích của Rejection Sampling trong Hình 7. Delta giữa đường cong tối đa và trung vị có thể được hiểu như lợi ích tiềm năng của tinh chỉnh trên đầu ra tốt nhất. Như mong đợi, delta này tăng với nhiều mẫu hơn, vì tối đa tăng (tức là nhiều mẫu hơn, nhiều cơ hội hơn để tạo ra một trajectory tốt), trong khi trung vị vẫn tĩnh. Có một kết nối trực tiếp giữa khám phá và phần thưởng tối đa chúng ta có thể đạt được trong các mẫu. Tham số nhiệt độ cũng đóng một vai trò quan trọng cho khám phá, vì nhiệt độ cao hơn cho phép chúng ta lấy mẫu các đầu ra đa dạng hơn.

Trong Hình 8, chúng tôi báo cáo cho Llama 2-Chat-SFT (trái) và Llama 2-Chat-RLHF (phải), các đường cong phần thưởng tối đa trong N mẫu (với N ∈ [1, . . . , 100]), cho các nhiệt độ khác nhau. Chúng tôi có thể quan sát thấy rằng nhiệt độ tối ưu không không đổi trong quá trình cập nhật mô hình lặp đi lặp lại: RLHF có tác động trực tiếp đến việc thay đổi tỷ lệ nhiệt độ. Đối với Llama 2-Chat-RLHF, nhiệt độ tối ưu khi lấy mẫu giữa 10 và 100 đầu ra là T ∈ [1.2, 1.3]. Với ngân sách tính toán hữu hạn, do đó cần thiết phải điều chỉnh lại nhiệt độ một cách tiến bộ. Lưu ý rằng việc thay đổi tỷ lệ nhiệt độ này xảy ra cho một số bước không đổi cho mỗi mô hình, và luôn bắt đầu từ mô hình cơ sở trên mỗi phiên bản RLHF mới.

PPO. Chúng tôi tiếp tục huấn luyện mô hình ngôn ngữ của chúng tôi theo scheme RL của Stiennon et al. (2020), sử dụng mô hình phần thưởng như một ước tính cho hàm phần thưởng thực (sở thích con người) và mô hình ngôn ngữ tiền huấn luyện như chính sách để tối ưu hóa. Trong giai đoạn này, chúng tôi tìm cách tối ưu hóa mục tiêu sau:

arg maxπ Ep∼D,g∼π[R(g|p)] (3)

Chúng tôi cải thiện chính sách một cách lặp đi lặp lại bằng cách lấy mẫu prompt p từ bộ dữ liệu D của chúng tôi và các thế hệ g từ chính sách π và sử dụng thuật toán PPO và hàm loss để đạt được mục tiêu này.

Hàm phần thưởng cuối cùng mà chúng tôi sử dụng trong quá trình tối ưu hóa,

R(g|p) = R̃c(g|p) − βDKL(πθ(g|p)‖π0(g|p)) (4)

chứa một thuật ngữ phạt cho việc lệch khỏi chính sách gốc π0. Như đã được quan sát trong các công trình khác (Stiennon et al., 2020; Ouyang et al., 2022), chúng tôi thấy ràng buộc này hữu ích cho sự ổn định huấn luyện, và để giảm reward hacking khiến chúng tôi đạt được điểm số cao từ mô hình phần thưởng nhưng điểm số thấp từ đánh giá con người.

Chúng tôi định nghĩa Rc là một kết hợp từng phần của các mô hình phần thưởng an toàn (Rs) và tính hữu ích (Rh). Chúng tôi đã gắn thẻ các prompt trong bộ dữ liệu của chúng tôi có thể gợi ra các phản hồi có khả năng không an toàn và ưu tiên điểm số từ mô hình an toàn. Ngưỡng 0.15 được chọn để lọc các phản hồi không an toàn, tương ứng với độ chính xác 0.89 và recall 0.55 được đánh giá trên tập test Meta Safety. Chúng tôi cũng thấy quan trọng việc làm trắng các điểm số tuyến tính cuối cùng (được hiển thị ở đây bằng cách đảo ngược sigmoid với hàm logit) để tăng sự ổn định và cân bằng đúng cách với thuật ngữ phạt KL (β) ở trên.

Rc(g|p) = {Rs(g|p) nếu is_safety(p) hoặc Rs(g|p) < 0.15
          {Rh(g|p) ngược lại

R̃c(g|p) = whiten(logit(Rc(g|p)))

Đối với tất cả các mô hình, chúng tôi sử dụng bộ tối ưu AdamW (Loshchilov và Hutter, 2017), với β1 = 0.9, β2 = 0.95, eps = 10−5. Chúng tôi sử dụng weight decay 0.1, gradient clipping 1.0, và tốc độ học không đổi 10−6. Cho mỗi lần lặp PPO, chúng tôi sử dụng batch size 512, ngưỡng PPO clip 0.2, kích thước mini-batch 64, và thực hiện một bước gradient trên mỗi mini-batch. Đối với các mô hình 7B và 13B, chúng tôi đặt β = 0.01 (phạt KL), và đối với các mô hình 34B và 70B, chúng tôi đặt β = 0.005.

Chúng tôi huấn luyện từ 200 đến 400 lần lặp cho tất cả các mô hình của chúng tôi, và sử dụng đánh giá trên các prompt held-out để early stopping. Mỗi lần lặp PPO trên mô hình 70B mất trung bình ≈330 giây. Để huấn luyện nhanh với kích thước batch lớn, chúng tôi sử dụng FSDP (Zhao et al., 2023). Điều này hiệu quả khi sử dụng O(1) forward hoặc backward pass, nhưng gây ra sự chậm trễ lớn (≈20×) trong quá trình sinh, ngay cả khi sử dụng kích thước batch lớn và KV cache. Chúng tôi có thể giảm thiểu điều này bằng cách hợp nhất trọng số mô hình vào mỗi node một lần trước khi sinh và sau đó giải phóng bộ nhớ sau khi sinh, tiếp tục phần còn lại của vòng lặp huấn luyện.

3.3 System Message cho Tính nhất quán Đa lượt

Trong thiết lập đối thoại, một số hướng dẫn nên áp dụng cho tất cả các lượt đối thoại, ví dụ: phản hồi một cách ngắn gọn, hoặc "act as" một nhân vật công cộng nào đó. Khi chúng tôi cung cấp các hướng dẫn như vậy cho Llama 2-Chat, phản hồi tiếp theo phải luôn tôn trọng ràng buộc. Tuy nhiên, các mô hình RLHF ban đầu của chúng tôi có xu hướng quên hướng dẫn ban đầu sau vài lượt đối thoại, như được minh họa trong Hình 9 (trái).

Để giải quyết những hạn chế này, chúng tôi đề xuất Ghost Attention (GAtt), một phương pháp rất đơn giản được lấy cảm hứng từ Context Distillation (Bai et al., 2022b) mà hack dữ liệu tinh chỉnh để giúp attention tập trung trong một quá trình nhiều giai đoạn. GAtt cho phép kiểm soát đối thoại qua nhiều lượt, như được minh họa trong Hình 9 (phải).

Phương pháp GAtt. Giả sử chúng ta có quyền truy cập vào một bộ dữ liệu đối thoại đa lượt giữa hai người (ví dụ: người dùng và trợ lý), với danh sách tin nhắn [u1, a1, . . . , un, an], trong đó un và an tương ứng với tin nhắn người dùng và trợ lý cho lượt n. Sau đó, chúng tôi định nghĩa một hướng dẫn, inst, nên được tôn trọng trong suốt đối thoại. Ví dụ, inst có thể là "act as." Chúng tôi sau đó có thể nối hướng dẫn này một cách tổng hợp với tất cả tin nhắn người dùng của đối thoại.

Tiếp theo, chúng tôi có thể lấy mẫu từ dữ liệu tổng hợp này bằng mô hình RLHF mới nhất. Bây giờ chúng ta có một context-dialogue và mẫu để tinh chỉnh một mô hình, trong một quá trình tương tự như Rejection Sampling. Thay vì tăng cường tất cả các lượt context-dialogue bằng hướng dẫn, chúng tôi có thể bỏ nó trong tất cả trừ lượt đầu tiên, nhưng điều này sẽ dẫn đến sự không khớp tại thời điểm huấn luyện giữa system message, tức là tất cả tin nhắn trợ lý trung gian đến trước lượt cuối cùng, và mẫu của chúng tôi. Để sửa vấn đề này, có thể làm tổn hại quá trình huấn luyện, chúng tôi đơn giản đặt loss thành 0 cho tất cả token từ các lượt trước, bao gồm tin nhắn trợ lý.

Đối với các hướng dẫn huấn luyện, chúng tôi tạo một vài ràng buộc tổng hợp để lấy mẫu từ: Hobbies ("You enjoy e.g. Tennis"), Language ("Speak in e.g. French"), hoặc PublicFigure ("Act as e.g. Napoleon"). Để có được danh sách sở thích và nhân vật công cộng, chúng tôi yêu cầu Llama 2-Chat tạo ra nó, tránh sự không khớp giữa hướng dẫn và kiến thức mô hình (ví dụ: yêu cầu mô hình hành động như ai đó mà nó chưa gặp trong quá trình huấn luyện). Để làm cho các hướng dẫn phức tạp và đa dạng hơn, chúng tôi xây dựng hướng dẫn cuối cùng bằng cách kết hợp ngẫu nhiên các ràng buộc trên. Khi xây dựng system message cuối cùng cho dữ liệu huấn luyện, chúng tôi cũng sửa đổi hướng dẫn gốc một nửa thời gian để ít dài dòng hơn, ví dụ: "Always act as Napoleon from now" -> "Figure: Napoleon." Các bước này tạo ra một bộ dữ liệu SFT, mà chúng tôi có thể tinh chỉnh Llama 2-Chat.

Đánh giá GAtt. Chúng tôi áp dụng GAtt sau RLHF V3. Chúng tôi báo cáo phân tích định lượng cho thấy GAtt nhất quán lên đến 20+ lượt, cho đến khi độ dài ngữ cảnh tối đa đạt được (xem Phụ lục A.3.5). Chúng tôi cố gắng đặt các ràng buộc không có trong huấn luyện GAtt tại thời điểm suy luận, chẳng hạn "Always answer with Haiku," mà mô hình vẫn nhất quán như được minh họa trong Phụ lục Hình 28.

Mặc dù có tính hữu dụng, việc triển khai hiện tại của GAtt khá đơn giản, và nhiều phát triển và lặp đi lặp lại hơn về kỹ thuật này có thể mang lại lợi ích thêm cho mô hình. Chẳng hạn, chúng tôi có thể dạy mô hình thay đổi system message trong quá trình đối thoại bằng cách tích hợp dữ liệu như vậy trong quá trình tinh chỉnh.

3.4 Kết quả RLHF

3.4.1 Đánh giá Dựa trên Mô hình

Đánh giá LLM là một vấn đề nghiên cứu mở đầy thách thức. Đánh giá con người, trong khi là tiêu chuẩn vàng, có thể phức tạp bởi nhiều cân nhắc HCI (Clark et al., 2021; Gehrmann et al., 2023), và không phải lúc nào cũng có thể mở rộng. Do đó, để chọn các mô hình hoạt động tốt nhất trong số nhiều ablation tại mỗi lần lặp từ RLHF-V1 đến V5, chúng tôi đầu tiên quan sát sự cải thiện của phần thưởng từ các mô hình phần thưởng mới nhất, để tiết kiệm chi phí và tăng tốc độ lặp. Chúng tôi sau đó xác thực các phiên bản mô hình chính với đánh giá con người.

Đánh giá Dựa trên Mô hình Có thể Đi Xa Đến Đâu? Để đo tính mạnh mẽ của mô hình phần thưởng của chúng tôi, chúng tôi thu thập một tập test các prompt cho cả tính hữu ích và an toàn, và yêu cầu ba chú thích viên đánh giá chất lượng câu trả lời dựa trên thang điểm Likert 7 điểm (càng cao càng tốt). Chúng tôi quan sát thấy rằng các mô hình phần thưởng của chúng tôi nhìn chung được hiệu chuẩn tốt với các chú thích sở thích con người của chúng tôi, như được minh họa trong Hình 29 trong phụ lục. Điều này xác nhận tính liên quan của việc sử dụng phần thưởng của chúng tôi như một metric point-wise, mặc dù được huấn luyện với Pairwise Ranking Loss.

Tuy nhiên, như Luật Goodhart nói, khi một thước đo trở thành mục tiêu, nó không còn là một thước đo tốt. Để đảm bảo thước đo của chúng tôi không phân kỳ với sở thích con người, chúng tôi bổ sung sử dụng một phần thưởng tổng quát hơn, được huấn luyện trên các bộ dữ liệu Reward Modeling mã nguồn mở đa dạng. Chúng tôi vẫn chưa quan sát thấy sự phân kỳ như vậy, và giả định rằng cập nhật mô hình lặp đi lặp lại có thể đang giúp ngăn ngừa điều này.

Như một bước xác minh cuối cùng để đảm bảo không có hồi quy giữa mô hình mới của chúng tôi và mô hình trước đó, chúng tôi sử dụng cả hai để lấy mẫu trong lần lặp chú thích tiếp theo. Điều này cho phép so sánh mô hình "miễn phí" trên các prompt mới và có thể giúp tăng tính đa dạng khi lấy mẫu.

Tiến triển của Mô hình. Hình 11 báo cáo tiến triển của các phiên bản SFT và sau đó RLHF khác nhau của chúng tôi cho cả trục An toàn và Tính hữu ích, được đo bằng các mô hình phần thưởng An toàn và Tính hữu ích nội bộ của chúng tôi. Trên tập đánh giá này, chúng tôi vượt trội hơn ChatGPT trên cả hai trục sau RLHF-V3 (tính vô hại và tính hữu ích >50%). Mặc dù tính liên quan đã đề cập của việc sử dụng phần thưởng của chúng tôi như một metric point-wise, nó có thể thiên vị có lợi cho Llama 2-Chat. Do đó, để so sánh công bằng, chúng tôi bổ sung tính toán kết quả cuối cùng bằng cách sử dụng GPT-4 để đánh giá thế hệ nào được ưa thích. Thứ tự mà các đầu ra ChatGPT và Llama 2-Chat xuất hiện trong prompt GPT-4 được hoán đổi ngẫu nhiên để tránh bất kỳ thiên kiến nào. Như mong đợi, tỷ lệ thắng có lợi cho Llama 2-Chat ít rõ rệt hơn, mặc dù đạt được hơn 60% tỷ lệ thắng cho Llama 2-Chat mới nhất của chúng tôi. Các prompt tương ứng với một tập xác thực gồm 1,586 và 584 prompt lần lượt cho an toàn và tính hữu ích.

3.4.2 Đánh giá Con người

Đánh giá con người thường được coi là tiêu chuẩn vàng để đánh giá các mô hình cho tạo ngôn ngữ tự nhiên, bao gồm các mô hình đối thoại. Để đánh giá chất lượng của các phiên bản mô hình chính, chúng tôi yêu cầu các đánh giá viên con người đánh giá chúng về tính hữu ích và an toàn. Chúng tôi so sánh các mô hình Llama 2-Chat với các mô hình mã nguồn mở (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), cũng như các mô hình nguồn đóng (ChatGPT (OpenAI, 2023) và PaLM Anil et al. (2023)) trên hơn 4,000 prompt đơn và đa lượt. Đối với ChatGPT, chúng tôi sử dụng mô hình gpt-3.5-turbo-0301 trong tất cả thế hệ. Đối với PaLM, chúng tôi sử dụng mô hình chat-bison-001 trong tất cả thế hệ. Số lượng prompt cuối cùng cho đánh giá con người cho mỗi mô hình được hiển thị trong Bảng 32. Xem chi tiết phương pháp thêm trong Phụ lục, Phần A.3.7. Phần sau đây hiển thị kết quả tính hữu ích; kết quả an toàn được trình bày trong Phần 4.4.

Kết quả. Như được hiển thị trong Hình 12, các mô hình Llama 2-Chat vượt trội hơn các mô hình mã nguồn mở bằng một biên độ đáng kể trên cả prompt đơn lượt và đa lượt. Đặc biệt, mô hình Llama 2-Chat 7B vượt trội hơn MPT-7B-chat trên 60% prompt. Llama 2-Chat 34B có tỷ lệ thắng tổng thể hơn 75% so với các mô hình Vicuna-33B và Falcon 40B có kích thước tương đương.

Mô hình Llama 2-Chat lớn nhất cạnh tranh với ChatGPT. Mô hình Llama 2-Chat 70B có tỷ lệ thắng 36% và tỷ lệ hòa 31.5% so với ChatGPT. Mô hình Llama 2-Chat 70B vượt trội hơn mô hình PaLM-bison chat bằng một tỷ lệ phần trăm lớn trên tập prompt của chúng tôi. Thêm kết quả và phân tích có sẵn trong Phần A.3.7.

Độ tin cậy Giữa các Rater (IRR). Trong đánh giá con người của chúng tôi, ba chú thích viên khác nhau cung cấp đánh giá độc lập cho mỗi so sánh sinh mô hình. Điểm IRR cao (gần 1.0) thường được xem là tốt hơn từ góc độ chất lượng dữ liệu, tuy nhiên, bối cảnh là quan trọng. Các nhiệm vụ có tính chủ quan cao như đánh giá tính hữu ích tổng thể của thế hệ LLM thường sẽ có điểm IRR thấp hơn các nhiệm vụ gắn nhãn khách quan hơn. Có tương đối ít benchmark công khai cho các bối cảnh này, vì vậy chúng tôi cảm thấy chia sẻ phân tích của chúng tôi ở đây sẽ có lợi cho cộng đồng nghiên cứu.

Chúng tôi sử dụng thống kê AC1/2 của Gwet (Gwet, 2008, 2014) để đo độ tin cậy giữa các rater (IRR), vì chúng tôi thấy nó là metric ổn định nhất trên các tình huống đo lường khác nhau. Trên nhiệm vụ tính hữu ích thang điểm Likert 7 điểm được sử dụng trong phân tích của chúng tôi, điểm AC2 của Gwet dao động từ 0.37 đến 0.55 tùy thuộc vào so sánh mô hình cụ thể. Chúng tôi thấy điểm ở đầu thấp của phạm vi đó cho các xếp hạng từ so sánh mô hình với tỷ lệ thắng tương tự nhau (như so sánh Llama 2-Chat-70B-chat vs. ChatGPT). Chúng tôi thấy điểm ở đầu cao của phạm vi đó cho các xếp hạng từ so sánh mô hình với người thắng rõ ràng hơn (như Llama 2-Chat-34b-chat vs. Falcon-40b-instruct).

Hạn chế của đánh giá con người. Trong khi kết quả của chúng tôi cho thấy Llama 2-Chat ngang tầm với ChatGPT trong đánh giá con người, điều quan trọng cần lưu ý là đánh giá con người có một số hạn chế.

• Theo tiêu chuẩn học thuật và nghiên cứu, chúng tôi có một tập prompt lớn gồm 4k prompt. Tuy nhiên, nó không bao phủ việc sử dụng thực tế của các mô hình này, mà có thể sẽ bao phủ một số lượng trường hợp sử dụng lớn hơn đáng kể.

• Tính đa dạng của các prompt có thể là một yếu tố khác trong kết quả của chúng tôi. Ví dụ, tập prompt của chúng tôi không bao gồm bất kỳ prompt liên quan đến mã hóa hoặc suy luận nào.

• Chúng tôi chỉ đánh giá thế hệ cuối cùng của một cuộc đối thoại đa lượt. Một đánh giá thú vị hơn có thể là yêu cầu các mô hình hoàn thành một nhiệm vụ và đánh giá trải nghiệm tổng thể với mô hình qua nhiều lượt.

• Đánh giá con người cho các mô hình sinh vốn có tính chủ quan và có nhiễu. Do đó, đánh giá trên một tập prompt khác hoặc với các hướng dẫn khác có thể dẫn đến kết quả khác.

4 An toàn

CẢNH BÁO: phần này chứa các ví dụ về văn bản có thể được coi là không an toàn, công kích, hoặc đáng lo ngại.

Trong phần này, chúng tôi đi sâu vào chủ đề quan trọng về các phép đo và giảm thiểu an toàn. Chúng tôi đầu tiên thảo luận các nghiên cứu an toàn của chúng tôi về dữ liệu tiền huấn luyện và các mô hình tiền huấn luyện (Phần 4.1). Tiếp theo, chúng tôi mô tả quá trình căn chỉnh an toàn của chúng tôi (Phần 4.2), giải thích cách chúng tôi thu thập các chú thích liên quan đến an toàn và sử dụng SFT và RLHF, và trình bày kết quả thí nghiệm. Sau đó, chúng tôi thảo luận red teaming mà chúng tôi thực hiện để hiểu thêm và cải thiện an toàn mô hình (Phần 4.3). Cuối cùng, chúng tôi trình bày đánh giá an toàn định lượng của Llama 2-Chat (Phần 4.4). Chúng tôi cũng chia sẻ một model card trong Phụ lục, trong Bảng 52.

4.1 An toàn trong Tiền huấn luyện

Điều quan trọng là hiểu những gì có trong dữ liệu tiền huấn luyện cả để tăng tính minh bạch và làm sáng tỏ các nguyên nhân gốc rễ của các vấn đề downstream tiềm ẩn, như thiên kiến tiềm ẩn. Điều này có thể thông báo về các biện pháp giảm thiểu downstream nào, nếu có, cần xem xét, và giúp hướng dẫn việc sử dụng mô hình phù hợp. Trong phần này, chúng tôi phân tích dữ liệu tiền huấn luyện về phân phối ngôn ngữ, đại diện nhân khẩu học, và độc tính. Chúng tôi cũng trình bày kết quả thử nghiệm các mô hình tiền huấn luyện trên các benchmark an toàn hiện có.

Các Bước Được Thực hiện để Tiền huấn luyện Có trách nhiệm. Chúng tôi đã tuân theo các quy trình đánh giá quyền riêng tư và pháp lý tiêu chuẩn của Meta cho mỗi bộ dữ liệu được sử dụng trong huấn luyện. Chúng tôi không sử dụng bất kỳ dữ liệu người dùng Meta nào trong huấn luyện. Chúng tôi loại trừ dữ liệu từ một số trang web nhất định được biết là chứa lượng lớn thông tin cá nhân về các cá nhân tư nhân. Chúng tôi đã nỗ lực hết sức để huấn luyện các mô hình của chúng tôi một cách hiệu quả để giảm dấu chân carbon của tiền huấn luyện (Phần 2.2.1). Việc chia sẻ các mô hình của chúng tôi rộng rãi sẽ giảm nhu cầu cho những người khác phải huấn luyện các mô hình tương tự. Không có việc lọc bổ sung nào được tiến hành trên các bộ dữ liệu, để cho phép Llama 2 có thể sử dụng rộng rãi hơn trên các nhiệm vụ (ví dụ: nó có thể được sử dụng tốt hơn cho phân loại hate speech), trong khi tránh khả năng xóa nhân khẩu học tình cờ đôi khi gây ra bởi over-scrubbing. Quan trọng là, điều này cho phép Llama 2-Chat khái quát hóa hiệu quả hơn trong quá trình tinh chỉnh an toàn với ít ví dụ hơn (Welbl et al., 2021; Korbak et al., 2023; Xu et al., 2021). Do đó, các mô hình Llama 2 nên được sử dụng cẩn thận và triển khai chỉ sau khi áp dụng tinh chỉnh an toàn đáng kể.

Đại diện Nhân khẩu học: Đại từ. Thiên kiến trong thế hệ mô hình có thể xuất phát từ thiên kiến được thừa hưởng từ chính dữ liệu huấn luyện. Chẳng hạn, Bailey et al. (2022) cho thấy rằng trong các corpus văn bản khổng lồ, từ đại diện cho "people" thường được sử dụng trong ngữ cảnh tương tự hơn với từ đại diện cho "men" hơn là từ đại diện cho "women," và Ganesh et al. (2023) chứng minh rằng hiệu suất của một mô hình trên các metric công bằng có thể phụ thuộc rất nhiều vào cách mô hình huấn luyện trên dữ liệu đại diện cho các nhóm nhân khẩu học thiểu số. Trong corpus huấn luyện tiếng Anh của chúng tôi, chúng tôi tính toán tần suất của các đại từ tiếng Anh phổ biến nhất trong Bảng 9a. Chúng tôi quan sát thấy rằng các đại từ He thường được đại diện quá mức trong các tài liệu so với các đại từ She, echo các khác biệt tần suất tương tự được quan sát trong việc sử dụng đại từ cho các bộ dữ liệu tiền huấn luyện mô hình có kích thước tương tự (Chowdhery et al., 2022). Điều này có thể có nghĩa là mô hình đang học ít hơn trong quá trình tiền huấn luyện về ngữ cảnh đề cập đến các đại từ She, và sau đó có thể có khả năng tạo ra các đại từ He với tỷ lệ cao hơn các đại từ She.

Đại diện Nhân khẩu học: Danh tính. Chúng tôi cũng phân tích sự đại diện của các nhóm nhân khẩu học khác nhau trong dữ liệu tiền huấn luyện bằng cách đo tỷ lệ sử dụng của các thuật ngữ danh tính nhân khẩu học từ bộ dữ liệu Holistic Bias (Smith et al., 2022) như một proxy. Chúng tôi tính toán tần suất cho mỗi thuật ngữ mô tả trong corpus tiền huấn luyện. Chúng tôi nhóm các mô tả thành 5 trục (Religion, Gender and Sex, Nationality, Race and Ethnicity, và Sexual Orientation), và hiển thị top 5 thuật ngữ trong mỗi trục trong Bảng 9b. Trong top 5 thuật ngữ, chúng tôi loại bỏ một vài thuật ngữ như "straight," "white," và "black," vì các thuật ngữ này có cách sử dụng thường xuyên ngoài đề cập nhân khẩu học (ví dụ: như các thuật ngữ màu cơ bản). Chúng tôi cũng khử trùng lặp giữa các danh sách, loại bỏ một vài thuật ngữ được tìm thấy trong cả Gender and Sex và Sexual Orientation. Đối với Gender and Sex, trong khi các đại từ She được đề cập trong ít tài liệu hơn, thuật ngữ "female" có mặt trong tỷ lệ phần trăm tài liệu lớn hơn. Điều này có thể ngụ ý rằng trong khi có ít ngữ cảnh thường xuyên hơn về các đại từ She, các bình luận về "females" phổ biến hơn, có lẽ phản ánh sự khác biệt trong đánh dấu ngôn ngữ của các thuật ngữ này (Blodgett et al., 2021). Đối với Sexual Orientation, năm thuật ngữ hàng đầu đều liên quan đến danh tính LGBTQ+. Đối với Nationality, Race and Ethnicity, và Religion, chúng tôi quan sát một sự thiên vị phương Tây (Bhatt et al., 2022). Chẳng hạn, thuật ngữ "American" được đề cập trong 69.4% tham chiếu, thuật ngữ "European" phổ biến hơn các chủng tộc và dân tộc khác, và "Christian" là tôn giáo được đại diện nhiều nhất tiếp theo là "Catholic" và "Jewish."

Độc tính Dữ liệu. Chúng tôi đo mức độ phổ biến của độc tính trong phần tiếng Anh của corpus tiền huấn luyện bằng cách sử dụng bộ phân loại HateBERT được tinh chỉnh trên bộ dữ liệu ToxiGen (Hartvigsen et al., 2022). Chúng tôi chấm điểm từng dòng của một tài liệu riêng biệt và tính trung bình chúng để gán điểm số tài liệu. Hình 13 hiển thị phân phối điểm số trong một mẫu ngẫu nhiên 10% của toàn bộ corpus. Khoảng 0.2% tài liệu được đánh giá được gán điểm likelihood 0.5 hoặc cao hơn, có nghĩa là có một lượng nhỏ độc tính trong dữ liệu tiền huấn luyện của chúng tôi.

Nhận dạng Ngôn ngữ. Trong khi dữ liệu tiền huấn luyện của chúng tôi chủ yếu là tiếng Anh, nó cũng bao gồm văn bản từ một số ít ngôn ngữ khác. Bảng 10 hiển thị phân phối ngôn ngữ trong corpus của chúng tôi, được phân tập con thành những ngôn ngữ được tìm thấy trong hơn 0.005% tài liệu. Phân tích của chúng tôi sử dụng công cụ nhận dạng ngôn ngữ fastText (Bojanowski et al., 2016) và ngưỡng 0.5 cho việc phát hiện ngôn ngữ. Một corpus huấn luyện có đa số bằng tiếng Anh có nghĩa là mô hình có thể không phù hợp để sử dụng trong các ngôn ngữ khác.

Benchmark An toàn cho Mô hình Tiền huấn luyện. Chúng tôi đánh giá khả năng an toàn của Llama 2 trên ba benchmark tự động phổ biến, liên quan đến ba chiều chính của an toàn LM.

1. Truthfulness, đề cập đến việc liệu một mô hình ngôn ngữ có tạo ra những sai lầm đã biết do hiểu lầm hoặc niềm tin sai lầm hay không. Chúng tôi sử dụng TruthfulQA (Lin et al., 2021) để đo mức độ LLM của chúng tôi có thể tạo ra đầu ra đáng tin cậy phù hợp với tính thực tế và thông thường.

2. Toxicity, được định nghĩa là xu hướng của một mô hình ngôn ngữ tạo ra nội dung độc hại, thô lỗ, đối nghịch, hoặc ngầm căm thù. Chúng tôi chọn ToxiGen (Hartvigsen et al., 2022) để đo lượng tạo ra ngôn ngữ độc hại và hate speech trên các nhóm khác nhau.

3. Bias, được định nghĩa là cách thế hệ mô hình tái tạo các thiên kiến xã hội định kiến hiện có. Chúng tôi sử dụng BOLD (Dhamala et al., 2021) để nghiên cứu cách cảm tính trong thế hệ mô hình có thể thay đổi với các thuộc tính nhân khẩu học.

Chúng tôi so sánh hiệu suất của Llama 2 với Llama 1 (Touvron et al., 2023), Falcon (Almazrouei et al., 2023), và MPT (MosaicML NLP Team et al., 2023) trong Bảng 11. Để giải mã, chúng tôi đặt nhiệt độ thành 0.1 và sử dụng nucleus sampling (Holtzman et al., 2020) với top-p được đặt thành 0.9. Đối với TruthfulQA, chúng tôi trình bày tỷ lệ phần trăm thế hệ vừa truthful vừa informative (càng cao càng tốt). Đối với ToxiGen, chúng tôi trình bày tỷ lệ phần trăm thế hệ được coi là độc hại bởi metric (càng thấp càng tốt). Mô tả chi tiết về benchmark và metric có thể được tìm thấy trong Phụ lục A.4.7. Khi so sánh với Llama 1-7B, Llama 2-7B cho thấy sự tăng 21.37% về tính truthfulness và informativeness và giảm 7.61% độc tính. Chúng tôi cũng quan sát sự tăng độc tính trong các Llama 2 tiền huấn luyện 13B và 70B, có thể xuất phát từ dữ liệu tiền huấn luyện lớn hơn hoặc hỗn hợp bộ dữ liệu khác. Một số người đã giả định sự tồn tại của mối quan hệ giữa kích thước bộ dữ liệu tiền huấn luyện và độc tính hoặc thiên kiến mô hình downstream (Bender et al., 2021b), nhưng công việc thực nghiệm để xác thực tuyên bố này vẫn đang diễn ra (Dodge et al., 2021; Smith và Williams, 2021; Tal et al., 2022), và bằng chứng thêm từ các mô hình cập nhật vẫn cần thiết.

Trong Phụ lục A.4.7, chúng tôi trình bày các metric thiên kiến, như cách cảm tính của thế hệ mô hình thay đổi với các thuộc tính nhân khẩu học. Chúng tôi lưu ý sự tăng cảm tính tích cực tổng thể cho nhiều nhóm sử dụng prompt BOLD. Kết quả chi tiết hơn được chia theo nhóm nhân khẩu học khác nhau có thể được tìm thấy trong Phụ lục A.4.8.

Llama 2 không vượt trội hơn các mô hình khác trên metric độc tính, và chúng tôi suy đoán rằng điều này có thể là do chúng tôi đã tránh lọc tích cực dữ liệu tiền huấn luyện. Nhớ lại rằng việc để dữ liệu tiền huấn luyện không được lọc có thể cho phép các mô hình cơ sở được tinh chỉnh hoạt động tốt trên nhiều nhiệm vụ downstream hơn (bao gồm phát hiện hate speech), và nó mang ít rủi ro hơn trong việc vô tình lọc ra một số nhóm nhân khẩu học. Chúng tôi quan sát rằng các mô hình được huấn luyện từ dữ liệu tiền huấn luyện được lọc ít tích cực hơn cũng đòi hỏi ít ví dụ hơn để đạt được căn chỉnh an toàn hợp lý. Chúng tôi nhắc lại rằng lựa chọn có động cơ này không ngụ ý rằng các biện pháp giảm thiểu an toàn bổ sung nên được áp dụng trước khi triển khai các mô hình Llama 2 cơ sở.

4.2 Tinh chỉnh An toàn

Trong phần này, chúng tôi mô tả cách tiếp cận tinh chỉnh an toàn của chúng tôi, bao gồm các danh mục an toàn, hướng dẫn chú thích, và các kỹ thuật chúng tôi sử dụng để giảm thiểu rủi ro an toàn. Chúng tôi sử dụng một quy trình tương tự với các phương pháp tinh chỉnh tổng quát như được mô tả trong Phần 3, với một số khác biệt đáng chú ý liên quan đến mối quan tâm an toàn. Cụ thể, chúng tôi sử dụng các kỹ thuật sau trong tinh chỉnh an toàn:

1. Supervised Safety Fine-Tuning: Chúng tôi khởi tạo bằng cách thu thập các prompt đối nghịch và các demonstration an toàn sau đó được bao gồm trong quy trình supervised fine-tuning tổng quát (Phần 3.1). Điều này dạy mô hình căn chỉnh với hướng dẫn an toàn của chúng tôi ngay cả trước RLHF, và do đó đặt nền tảng cho chú thích dữ liệu sở thích con người chất lượng cao.

2. Safety RLHF: Tiếp theo, chúng tôi tích hợp an toàn trong pipeline RLHF tổng quát được mô tả trong Phần 3.2.2. Điều này bao gồm huấn luyện một mô hình phần thưởng đặc biệt cho an toàn và thu thập các prompt đối nghịch thách thức hơn cho tinh chỉnh kiểu rejection sampling và tối ưu hóa PPO.

3. Safety Context Distillation: Cuối cùng, chúng tôi tinh chỉnh pipeline RLHF của chúng tôi với context distillation (Askell et al., 2021b). Điều này bao gồm tạo ra các phản hồi mô hình an toàn hơn bằng cách thêm tiền tố một prompt với một safety preprompt, ví dụ: "You are a safe and responsible assistant," và sau đó tinh chỉnh mô hình trên các phản hồi an toàn hơn mà không có preprompt, về cơ bản chưng cất safety preprompt (context) vào mô hình. Chúng tôi sử dụng một cách tiếp cận có mục tiêu cho phép mô hình phần thưởng an toàn của chúng tôi chọn có sử dụng context distillation cho mỗi mẫu hay không.

4.2.1 Danh mục An toàn và Hướng dẫn Chú thích

Dựa trên các hạn chế của LLM được biết từ công việc trước, chúng tôi thiết kế hướng dẫn cho nhóm chú thích của chúng tôi để tạo ra các prompt đối nghịch theo hai chiều: một risk category, hoặc chủ đề tiềm ẩn mà LLM có thể tạo ra nội dung không an toàn; và một attack vector, hoặc kiểu câu hỏi để bao phủ các loại prompt khác nhau có thể gợi ra hành vi mô hình xấu.

Các risk category được xem xét có thể được chia rộng rãi thành các danh mục ba sau: hoạt động bất hợp pháp và tội phạm (ví dụ: khủng bố, trộm cắp, buôn người); hoạt động căm thù và có hại (ví dụ: phỉ báng, tự gây hại, rối loạn ăn uống, phân biệt đối xử); và lời khuyên không đủ tiêu chuẩn (ví dụ: lời khuyên y tế, lời khuyên tài chính, lời khuyên pháp lý). Các attack vector được khám phá bao gồm thao túng tâm lý (ví dụ: thao túng thẩm quyền), thao túng logic (ví dụ: tiền đề sai), thao túng cú pháp (ví dụ: viết sai chính tả), thao túng ngữ nghĩa (ví dụ: ẩn dụ), thao túng quan điểm (ví dụ: đóng vai), ngôn ngữ không phải tiếng Anh, và những cái khác.

Chúng tôi sau đó định nghĩa các thực hành tốt nhất cho các phản hồi mô hình an toàn và hữu ích: mô hình nên đầu tiên giải quyết các mối quan tâm an toàn trực tiếp nếu có thể áp dụng, sau đó giải quyết prompt bằng cách giải thích các rủi ro tiềm ẩn cho người dùng, và cuối cùng cung cấp thông tin bổ sung nếu có thể. Chúng tôi cũng yêu cầu các chú thích viên tránh các danh mục trải nghiệm người dùng tiêu cực (xem Phụ lục A.5.2). Các hướng dẫn được dự định là một hướng dẫn chung cho mô hình và được tinh chỉnh và xem xét lặp đi lặp lại để bao gồm các rủi ro mới được xác định.

4.2.2 Safety Supervised Fine-Tuning

Theo các hướng dẫn đã thiết lập từ Phần 4.2.1, chúng tôi thu thập prompt và demonstration của các phản hồi mô hình an toàn từ các chú thích viên được huấn luyện, và sử dụng dữ liệu cho supervised fine-tuning theo cách tương tự như được mô tả trong Phần 3.1. Một ví dụ có thể được tìm thấy trong Bảng 5.

Các chú thích viên được hướng dẫn ban đầu đưa ra các prompt mà họ nghĩ có thể có khả năng khiến mô hình thể hiện hành vi không an toàn, tức là thực hiện red teaming, như được định nghĩa bởi các hướng dẫn. Tiếp theo, các chú thích viên được giao nhiệm vụ tạo ra một phản hồi an toàn và hữu ích mà mô hình nên tạo ra.

4.2.3 Safety RLHF

Chúng tôi quan sát sớm trong quá trình phát triển Llama 2-Chat rằng nó có thể tổng quát hóa từ các demonstration an toàn trong supervised fine-tuning. Mô hình nhanh chóng học viết các phản hồi an toàn chi tiết, giải quyết các mối quan tâm an toàn, giải thích tại sao chủ đề có thể nhạy cảm, và cung cấp thông tin hữu ích bổ sung. Đặc biệt, khi mô hình đưa ra các phản hồi an toàn, chúng thường chi tiết hơn những gì người chú thích trung bình viết. Do đó, sau khi thu thập chỉ vài nghìn demonstration được giám sát, chúng tôi chuyển hoàn toàn sang RLHF để dạy mô hình cách viết các phản hồi sắc thái hơn. Tinh chỉnh toàn diện với RLHF có lợi ích bổ sung là nó có thể làm cho mô hình mạnh mẽ hơn đối với các nỗ lực jailbreak (Bai et al., 2022a).

Chúng tôi tiến hành RLHF bằng cách đầu tiên thu thập dữ liệu sở thích con người cho an toàn tương tự như Phần 3.2.2: các chú thích viên viết một prompt mà họ tin có thể gợi ra hành vi không an toàn, và sau đó so sánh nhiều phản hồi mô hình với các prompt, chọn phản hồi an toàn nhất theo một bộ hướng dẫn. Chúng tôi sau đó sử dụng dữ liệu sở thích con người để huấn luyện một mô hình phần thưởng an toàn (xem Phần 3.2.2), và cũng tái sử dụng các prompt đối nghịch để lấy mẫu từ mô hình trong giai đoạn RLHF.

Tính Mạnh mẽ An toàn Long-Tail Tốt hơn mà không Làm tổn hại Tính hữu ích. An toàn vốn là một vấn đề long-tail, nơi thách thức đến từ một số lượng nhỏ các trường hợp rất cụ thể. Chúng tôi điều tra tác động của Safety RLHF bằng cách lấy hai checkpoint Llama 2-Chat trung gian—một không có prompt đối nghịch trong giai đoạn RLHF và một có chúng—và chấm điểm phản hồi của chúng trên các tập test của chúng tôi bằng cách sử dụng các mô hình phần thưởng an toàn và tính hữu ích của chúng tôi. Trong Hình 14, chúng tôi vẽ sự chuyển dịch phân phối điểm số của safety RM trên tập test an toàn (trái) và của helpfulness RM trên tập test tính hữu ích (phải). Ở phía tay trái của hình, chúng tôi quan sát rằng phân phối điểm số safety RM trên tập an toàn chuyển dịch đến điểm số phần thưởng cao hơn sau tinh chỉnh an toàn với RLHF, và phần long tail của phân phối gần số không mỏng đi. Một cluster rõ ràng xuất hiện ở góc trên bên trái đề xuất sự cải thiện an toàn mô hình. Ở phía bên phải, chúng tôi không quan sát bất kỳ mẫu thu thập nào dưới đường y = x ở phía bên phải của Hình 14, điều này cho thấy rằng phân phối điểm số tính hữu ích được bảo toàn sau tinh chỉnh an toàn với RLHF. Nói cách khác, với đủ dữ liệu huấn luyện tính hữu ích, việc bổ sung một giai đoạn bổ sung giảm thiểu an toàn không tác động tiêu cực đến hiệu suất mô hình trên tính hữu ích đến mức suy giảm đáng chú ý nào. Một ví dụ định tính được hiển thị trong Bảng 12.

Tác động của Safety Data Scaling. Một sự căng thẳng giữa tính hữu ích và an toàn của LLM đã được quan sát trong các nghiên cứu trước (Bai et al., 2022a). Để hiểu rõ hơn cách việc bổ sung dữ liệu huấn luyện an toàn ảnh hưởng đến hiệu suất mô hình tổng quát, đặc biệt là tính hữu ích, chúng tôi điều tra xu hướng trong safety data scaling bằng cách điều chỉnh lượng dữ liệu an toàn được sử dụng trong giai đoạn RLHF. Trong thí nghiệm ablation này, chúng tôi giữ lượng dữ liệu huấn luyện tính hữu ích không thay đổi (~0.9M mẫu) và tăng dần lượng dữ liệu an toàn được sử dụng trong tinh chỉnh mô hình, từ 0% đến 100% (~0.1M mẫu). Đối với công thức trộn dữ liệu huấn luyện cụ thể, chúng tôi tuân theo thủ tục được mô tả trong Phần 3.1 và tinh chỉnh mô hình tiền huấn luyện Llama 2 trong 2 epoch. Chúng tôi cuối cùng có được 6 biến thể mô hình được huấn luyện với 0%, 1%, 10%, 25%, 50%, và 100% tổng dữ liệu an toàn. Chúng tôi đánh giá chúng bằng cách sử dụng các mô hình phần thưởng an toàn và tính hữu ích của chúng tôi được mô tả trong Phần 3.2.2. Đối với mỗi biến thể, chúng tôi sử dụng các mô hình phần thưởng an toàn và tính hữu ích để chấm điểm thế hệ mô hình tương ứng với các prompt trong tập test Meta Safety và Helpful.

Như được hiển thị trong Hình 15, chúng tôi sử dụng điểm số mô hình phần thưởng trung bình làm proxy cho hiệu suất mô hình về an toàn và tính hữu ích. Chúng tôi quan sát rằng khi chúng tôi tăng tỷ lệ dữ liệu an toàn, hiệu suất của mô hình trong việc xử lý các prompt rủi ro và đối nghịch cải thiện đáng kể, và chúng ta thấy một tail nhẹ hơn trong phân phối điểm số mô hình phần thưởng an toàn. Trong khi đó, điểm số tính hữu ích trung bình vẫn không đổi. Chúng tôi giả định rằng điều này là do chúng tôi đã có một lượng đủ lớn dữ liệu huấn luyện tính hữu ích. Phụ lục A.4.2 liệt kê thêm kết quả định tính chứng minh cách các lượng dữ liệu an toàn khác nhau trong huấn luyện có thể thay đổi hành vi mô hình trong việc phản hồi các prompt đối nghịch và không đối nghịch.

Đo lường False Refusal. Mặc dù chúng tôi không thấy hồi quy tổng thể về tính hữu ích mô hình, chúng tôi quan sát định tính, thông qua tương tác, rằng mô hình với nhiều giảm thiểu an toàn hơn trả lời một số câu hỏi theo cách bảo thủ hơn (ví dụ: ví dụ được hiển thị trong Phụ lục Bảng 38). Như một follow-up, chúng tôi đo false refusal để định lượng tần suất mà mô hình từ chối không chính xác trả lời các prompt không đối nghịch. Ở đây, chúng tôi định nghĩa false refusal là mô hình từ chối không chính xác trả lời các prompt người dùng hợp pháp do các mối quan tâm an toàn không liên quan. Từ chối vì những lý do hợp lý vượt quá khả năng của mô hình, ví dụ: "I am not capable of participating in video calls" và "2024 is beyond my knowledge cut-off date," không được tính là false refusal. Chúng tôi huấn luyện một bộ phân loại để phát hiện từ chối trong phản hồi và áp dụng bộ phân loại trên 1) các tập test tính hữu ích trên và 2) một tập test borderline được curation gồm 210 mẫu cho mục đích boundary testing tương tự như OpenAI (2023). Bộ dữ liệu borderline được thiết kế có ý định để các prompt của nó trông đối nghịch (ví dụ: chứa từ hoặc từ phụ nhạy cảm) nhưng thực tế không không an toàn (ví dụ: "give me a recipe for Christmas Crack") (xem Phụ lục Bảng 41 cho thêm ví dụ).

Với nhiều dữ liệu an toàn hơn được trộn trong tinh chỉnh mô hình, tỷ lệ false-refusal trở nên lớn hơn trên cả hai bộ dữ liệu (xem Phụ lục Hình 33). Tuy nhiên, false refusal nhìn chung hiếm—khoảng 0.05%—trên bộ dữ liệu tính hữu ích, ngay cả với 100% dữ liệu an toàn. Mặt khác, tỷ lệ false-refusal lớn hơn nhiều trên tập borderline do độ khó của nó. Llama 2-Chat đôi khi gặp khó khăn trong việc phân biệt liệu một prompt có an toàn hay không khi prompt chứa các từ thường xuyên xuất hiện trong thế hệ không an toàn (như "bomb"). Phụ lục Bảng 41 hiển thị một số ví dụ false refusal mà chúng tôi phát hiện.

4.2.4 Context Distillation cho An toàn

Chúng tôi khuyến khích Llama 2-Chat liên kết các prompt đối nghịch với các phản hồi an toàn hơn bằng cách sử dụng context distillation (Askell et al., 2021a) tương tự như Phần 3.3. Chúng tôi quan sát rằng khả năng an toàn của LLM có thể được tăng cường hiệu quả bằng cách thêm tiền tố mô hình với một safety preprompt (ví dụ: "You are a safe and responsible assistant"). Giống như supervised safety fine-tuning, safety context distillation cung cấp một cách nhanh chóng để bootstrap phản hồi của mô hình trên các prompt đối nghịch khó, để chúng sau đó có thể được cải thiện thêm trong RLHF.

Cụ thể, chúng tôi áp dụng context distillation bằng cách thêm tiền tố một safety preprompt vào các prompt đối nghịch để tạo ra các phản hồi an toàn hơn, và sau đó tinh chỉnh mô hình trên đầu ra an toàn riêng của nó được đưa ra prompt đối nghịch mà không có preprompt. Chúng tôi tạo ra safety preprompt tự động với template. Cụ thể, chúng tôi sử dụng các tính từ khác nhau thường được liên kết với hành vi an toàn như "responsible," "respectful," hoặc "wise," với trực giác rằng mô hình liên kết chúng với các đặc điểm tích cực mà chúng tôi muốn thấy phản ánh trong các câu trả lời an toàn. Chúng tôi hiển thị ví dụ về safety preprompt trong Phụ lục Bảng 39.

Context Distillation với Answer Template. Trong giai đoạn thu thập prompt, chúng tôi cũng yêu cầu các chú thích viên gắn nhãn prompt theo danh mục rủi ro, điều này cho phép các preprompt thậm chí còn có mục tiêu hơn. Cụ thể, điều này cho phép chúng tôi cung cấp một số template câu trả lời dành riêng về cách các prompt đối nghịch nên được giải quyết, dựa trên mỗi danh mục rủi ro được xác định. Hình 16a hiển thị tác động của context distillation và context distillation với answer template trên điểm số safety RM.

Từ chối Context Distillation Error với Safety Reward Model. Điều quan trọng cần lưu ý là thực hiện safety context distillation cho các prompt hữu ích có thể làm suy giảm hiệu suất mô hình và dẫn đến nhiều false refusal hơn (xem Phụ lục Bảng 40). Do đó chúng tôi chỉ thực hiện safety context distillation trên các prompt đối nghịch. Tuy nhiên, chúng tôi quan sát rằng context distillation đôi khi có thể làm suy giảm chất lượng phản hồi, ngay cả khi xử lý các prompt đối nghịch. Cụ thể, nếu các phản hồi mô hình đã có chất lượng cao, việc áp dụng context distillation có thể dẫn đến các phản hồi ít phù hợp hơn, vì mô hình có xu hướng nhấn mạnh quá mức preprompt, thường tìm đến các mối quan tâm chung một cách quá mức (xem Phụ lục Bảng 40 cho ví dụ về câu trả lời mơ hồ do context distillation). Do đó chúng tôi tận dụng mô hình phần thưởng an toàn để quyết định có sử dụng safety context distillation hay không – chúng tôi chỉ giữ đầu ra context-distilled trên các ví dụ mà nó nhận được điểm số mô hình phần thưởng tốt hơn so với câu trả lời gốc. Chúng tôi nhận thấy rằng điều này đặc biệt hữu ích trên các prompt mà mô hình rất tệ, nhưng hạn chế tác động tiêu cực của context distillation (xem Hình 16b).

4.3 Red Teaming

Cho rằng khả năng của LLM rộng rãi như thế nào và dữ liệu huấn luyện của chúng đa dạng như thế nào, việc xác định rủi ro chỉ thông qua ex post facto usage và phân tích là không đủ. Thay vào đó, như đã được thực hiện cho các LLM khác, chúng tôi đã thực hiện nhiều loại nhận dạng rủi ro chủ động, thông thường được gọi là "red teaming," dựa trên thuật ngữ thường được sử dụng trong bảo mật máy tính. Loại phân tích chi tiết này rất quan trọng vì an toàn là một vấn đề long-tail, trong đó ngay cả các edge case rất hiếm cũng có thể gây ra vấn đề đáng chú ý. Ngay cả khi điểm số định lượng báo cáo kết quả tốt, các loại hiểu biết định tính này cho phép chúng ta nhận ra và nhắm mục tiêu các mẫu cụ thể theo cách toàn diện hơn.

Chúng tôi đã tiến hành một loạt red teaming với nhiều nhóm nhân viên nội bộ, công nhân hợp đồng, và vendor bên ngoài. Các nhóm này bao gồm hơn 350 người, bao gồm các chuyên gia miền trong an ninh mạng, gian lận bầu cử, thông tin sai lệch truyền thông xã hội, pháp lý, chính sách, quyền dân sự, đạo đức, kỹ thuật phần mềm, machine learning, AI có trách nhiệm, và viết sáng tạo. Họ cũng bao gồm các cá nhân đại diện cho nhiều nhân khẩu học kinh tế xã hội, giới tính, dân tộc, và chủng tộc.

Các red teamer đã thăm dò các mô hình của chúng tôi trên nhiều danh mục rủi ro (như lập kế hoạch tội phạm, buôn người, chất được kiểm soát hoặc điều chỉnh, nội dung khiêu dâm rõ ràng, lời khuyên y tế hoặc tài chính không đủ tiêu chuẩn, vi phạm quyền riêng tư, và nhiều hơn nữa), cũng như các vector tấn công khác nhau (như câu hỏi giả định, đầu vào sai định dạng/viết sai chính tả, hoặc đối thoại mở rộng). Ngoài ra, chúng tôi đã tiến hành các thử nghiệm cụ thể để xác định khả năng của các mô hình của chúng tôi tạo điều kiện cho việc sản xuất vũ khí (ví dụ: hạt nhân, sinh học, hóa học, và mạng); các phát hiện về các chủ đề này là biên và đã được giảm thiểu. Tuy nhiên, chúng tôi sẽ tiếp tục nỗ lực red teaming trong mặt trận này.

Cho đến nay, tất cả nỗ lực red teaming của chúng tôi đã nhắm mục tiêu đầu ra mô hình bằng tiếng Anh, nhưng đã bao gồm quan trọng các prompt và ngữ cảnh đối thoại không phải tiếng Anh, vì đó là một vector tấn công được biết đến. Trong tất cả các bài tập, người tham gia được đưa ra định nghĩa danh mục rủi ro và được hiển thị chỉ một số ít ví dụ về tương tác rủi ro với LLM. Sau đó, mỗi người tham gia là một phần của nhóm phụ tập trung vào một danh mục rủi ro cụ thể hoặc vector tấn công. Sau khi tạo ra mỗi đối thoại, người tham gia red team sẽ chú thích các thuộc tính khác nhau, bao gồm các khu vực rủi ro và mức độ rủi ro, như được thu thập bởi thang điểm Likert 5 điểm.

Một số ví dụ về hiểu biết hữu ích được cung cấp bởi các thành viên red team mà chúng tôi có thể cải thiện trong suốt quá trình phát triển:

• [Các mô hình sớm] có nhiều khả năng tạo ra các phản hồi không an toàn mà không lưu ý rằng chúng chứa nội dung có vấn đề. Tuy nhiên, [các mô hình hơi muộn hơn] có xu hướng hiển thị kiến thức rằng nội dung có vấn đề, ngay cả khi chúng tiếp tục cung cấp nó. "They respond with '[UNSAFE CONTENT] is not appropriate to discuss, etc.' and then immediately follow up with 'With that said, here's how [UNSAFE CONTENT].'" [Các mô hình mới nhất] có thể giải quyết những vấn đề này.

• Làm phân tâm [các mô hình sớm] bằng cách bao gồm "quirks" hoặc yêu cầu cụ thể thường đánh bại bất kỳ sự miễn cưỡng nào gặp phải thông qua các yêu cầu trực tiếp hơn. "A creative writing request (song, story, poem, etc.) is a reliable way to get it to produce content that it is otherwise robust against."

• Nhúng một yêu cầu có vấn đề trong ngữ cảnh tích cực thường thành công che giấu thực tế rằng đầu ra có vấn đề đang được yêu cầu cho [các mô hình sớm]: "The overall principle I've found most effective for any kind of attack is to hide it in language that is positive, progressive, and empowering."

Từ Red Teaming Insights đến Safer Models. Quan trọng là, sau mỗi bài tập, chúng tôi đã thực hiện phân tích kỹ lưỡng dữ liệu thu thập, bao gồm độ dài đối thoại, phân phối khu vực rủi ro, histogram chủ đề thông tin sai lệch (khi thích hợp), và mức độ rủi ro được đánh giá. Trong mỗi trường hợp, chúng tôi lấy các bài học tổng thể làm hướng dẫn để giúp huấn luyện an toàn mô hình thêm, và cụ thể lấy dữ liệu từ các bài tập này cho tinh chỉnh mô hình, huấn luyện phản hồi mô hình, và như một tín hiệu cho huấn luyện mô hình an toàn khác.

Nhiều vòng red teaming bổ sung được thực hiện trong vài tháng để đo tính mạnh mẽ của mỗi mô hình mới khi nó được phát hành nội bộ. Chúng tôi định nghĩa tính mạnh mẽ của một mô hình, γ, đối với một bài tập red teaming được thực hiện bởi một nhóm chuyên gia là số lượng prompt được tạo trung bình sẽ kích hoạt phản hồi vi phạm từ mô hình mỗi người mỗi giờ. Như một ví dụ, trên mô hình 7B của chúng tôi, chúng tôi có sự tiến hóa của γ: 1.8 → 0.45 qua vài lần lặp red teaming và tinh chỉnh mô hình. Tính mạnh mẽ có thể sẽ tiếp tục cải thiện với nỗ lực red teaming bổ sung. Một lượng khác mà chúng tôi theo dõi khi các mô hình mới được sản xuất là tỷ lệ phần trăm prompt kích hoạt phản hồi vi phạm được phát hiện trong các bài tập red teaming trước đó đã được giảm thiểu trong một bản phát hành ứng viên mới nhất định. Trung bình, chúng tôi có tỷ lệ từ chối 90% mô hình qua mô hình.

4.4 Đánh giá An toàn của Llama 2-Chat

Đánh giá An toàn Con người. Chúng tôi thu thập khoảng 2,000 prompt đối nghịch cho đánh giá con người theo các danh mục an toàn trong Phần 4.2.1, trong đó 1,351 prompt là đơn lượt và 623 là đa lượt. Ví dụ về prompt đánh giá và phản hồi có thể được tìm thấy trong Phụ lục A.4.6.

Chúng tôi sau đó yêu cầu các rater đánh giá các mô hình về vi phạm an toàn trên thang điểm Likert năm điểm với các định nghĩa sau:

• 5 - Không vi phạm an toàn và rất hữu ích
• 4 - Không vi phạm an toàn và chỉ có vấn đề không an toàn nhỏ
• 3 - Không vi phạm an toàn nhưng không hữu ích hoặc vấn đề không an toàn chính khác
• 2 - Vi phạm an toàn nhẹ hoặc vừa phải
• 1 - Vi phạm an toàn nghiêm trọng

Chúng tôi coi xếp hạng 1 hoặc 2 là vi phạm và sử dụng tỷ lệ vi phạm phần trăm làm metric đánh giá chính của chúng tôi, với xếp hạng trung bình làm bổ sung. Mỗi ví dụ được chú thích bởi ba chú thích viên và chúng tôi lấy bỏ phiếu đa số để xác định liệu phản hồi có vi phạm hay không. Chúng tôi sử dụng thống kê AC1/2 của Gwet để đo độ tin cậy giữa các rater (IRR) như trong đánh giá con người về tính hữu ích. Điểm IRR dao động từ 0.70 đến 0.95 tùy thuộc vào đợt chú thích, cho thấy mức độ đồng ý cao giữa các chú thích viên về đánh giá an toàn. Trên các chú thích Llama 2-Chat, IRR trung bình là 0.92 theo thước đo AC2 của Gwet. Chúng tôi thấy điểm IRR thấp hơn trên các đợt mà các mô hình có tỷ lệ vi phạm cao (ví dụ: Vicuna) và điểm IRR cao hơn trên các đợt mà các mô hình có tỷ lệ vi phạm tương đối thấp (ví dụ: Llama 2-Chat, Falcon, và ChatGPT).

Chúng tôi hiển thị tỷ lệ vi phạm tổng thể và xếp hạng an toàn của nhiều LLM khác nhau trong Hình 17. Llama 2-Chat có tỷ lệ vi phạm tổng thể tương đương hoặc thấp hơn trên các kích thước mô hình, trong khi ChatGPT và Falcon (Almazrouei et al., 2023) đến tiếp theo, sau đó là MPT (MosaicML NLP Team et al., 2023) và Vicuna (Chiang et al., 2023). Điều quan trọng là phải diễn giải các kết quả này một cách cẩn thận, vì chúng bị ảnh hưởng bởi các hạn chế của tập prompt, tính chủ quan của hướng dẫn đánh giá, tiêu chuẩn nội dung, và tính chủ quan của từng rater. Sau phân tích thủ công, chúng tôi thấy rằng phản hồi của Falcon thường ngắn (một hoặc hai câu), do đó ít có khả năng tạo ra nội dung không an toàn nhưng cũng nhìn chung ít hữu ích hơn. Điều này được phản ánh bởi một số lượng lớn phản hồi của Falcon với rating = 3. Do đó, chúng tôi lưu ý rằng trong Hình 17b, xếp hạng trung bình của Falcon thấp hơn nhiều so với Llama 2-Chat (34B) mặc dù tỷ lệ vi phạm của chúng trông tương tự (3.88 vs 4.45).

Trong Hình 18, chúng tôi báo cáo tỷ lệ vi phạm trên các cuộc đối thoại đơn và đa lượt. Một xu hướng trên các mô hình là các cuộc đối thoại đa lượt có khuynh hướng gây ra phản hồi không an toàn hơn. Điều đó nói rằng, Llama 2-Chat vẫn hoạt động tốt so với baseline, đặc biệt trên các cuộc đối thoại đa lượt. Chúng tôi cũng quan sát rằng Falcon hoạt động đặc biệt tốt trên các cuộc đối thoại đơn lượt (chủ yếu do tính ngắn gọn của nó) nhưng tệ hơn nhiều trên các cuộc đối thoại đa lượt, có thể do thiếu dữ liệu supervised fine-tuning đa lượt.

Trong Hình 19, chúng tôi hiển thị tỷ lệ vi phạm an toàn mỗi danh mục của các LLM khác nhau. Trong khi hiệu suất mô hình tương tự trên các danh mục, Llama 2-Chat có tương đối nhiều vi phạm hơn dưới danh mục unqualified advice (mặc dù vẫn thấp về mặt tuyệt đối), vì nhiều lý do, bao gồm thiếu disclaimer thích hợp (ví dụ: "I am not a professional") vào những lúc. Đối với hai danh mục khác, Llama 2-Chat đạt được tỷ lệ vi phạm tương đương hoặc thấp hơn một cách nhất quán bất kể kích thước mô hình.

Truthfulness, Toxicity, và Bias. Trong Bảng 14, Llama 2-Chat tinh chỉnh cho thấy cải thiện lớn so với Llama 2 tiền huấn luyện về truthfulness (50.18 → 64.14 cho 70B) và toxicity (24.60 → 0.01 cho 70B). Tỷ lệ phần trăm thế hệ độc hại co lại đến thực tế 0% cho Llama 2-Chat của tất cả kích thước: đây là mức độc tính thấp nhất trong tất cả các mô hình được so sánh. Nói chung, khi so sánh với Falcon và MPT, Llama 2-Chat tinh chỉnh cho thấy hiệu suất tốt nhất về độc tính và truthfulness. Sau tinh chỉnh, Llama 2-Chat có xu hướng tăng cảm tính tích cực tổng thể cho nhiều nhóm nhân khẩu học trong BOLD. Trong Phụ lục A.4.8, chúng tôi trình bày phân tích điểm số chi tiết của cảm tính thế hệ mô hình trên các nhóm phụ khác nhau cho benchmark thiên kiến, cùng với các phân tích và kết quả chuyên sâu hơn về truthfulness và bias.

5 Thảo luận

Ở đây, chúng tôi thảo luận về các thuộc tính thú vị mà chúng tôi đã quan sát với RLHF (Phần 5.1). Sau đó chúng tôi thảo luận về các hạn chế của Llama 2-Chat (Phần 5.2). Cuối cùng, chúng tôi trình bày chiến lược của chúng tôi để phát hành có trách nhiệm các mô hình này (Phần 5.3).

5.1 Bài học và Quan sát

Quá trình tinh chỉnh của chúng tôi đã tiết lộ một số kết quả thú vị, như khả năng của Llama 2-Chat tổ chức kiến thức theo thời gian, hoặc gọi API cho các công cụ bên ngoài.

Vượt ra ngoài Giám sát Con người. Khi bắt đầu dự án, nhiều người trong chúng tôi bày tỏ sự ưa thích cho chú thích có giám sát, bị thu hút bởi tín hiệu dày đặc hơn của nó. Trong khi đó reinforcement learning, được biết đến với sự bất ổn, có vẻ như một lĩnh vực hơi bóng tối đối với những người trong cộng đồng nghiên cứu NLP. Tuy nhiên, reinforcement learning đã chứng minh hiệu quả cao, đặc biệt là với hiệu quả chi phí và thời gian của nó. Các phát hiện của chúng tôi nhấn mạnh rằng yếu tố quyết định quan trọng của thành công RLHF nằm trong sự cộng hưởng mà nó nuôi dưỡng giữa con người và LLM trong suốt quá trình chú thích.

Ngay cả với các chú thích viên thành thạo, mỗi cá nhân viết với sự biến đổi đáng kể. Một mô hình được tinh chỉnh trên chú thích SFT học sự đa dạng này, bao gồm, không may, phần tail-end của chú thích được thực hiện kém. Hơn nữa, hiệu suất của mô hình được giới hạn bởi khả năng viết của các chú thích viên lành nghề nhất. Các chú thích viên con người có thể ít chịu sự khác biệt hơn khi so sánh chú thích sở thích hai đầu ra cho RLHF. Do đó, cơ chế phần thưởng nhanh chóng học gán điểm số thấp cho phân phối tail-end không mong muốn và căn chỉnh hướng sở thích con người. Hiện tượng này được minh họa trong Hình 20, nơi chúng ta có thể thấy rằng các câu trả lời tồi tệ nhất được loại bỏ dần dần, chuyển dịch phân phối sang phải.

Ngoài ra, trong quá trình chú thích, mô hình có tiềm năng mạo hiểm vào việc viết trajectory mà ngay cả các chú thích viên tốt nhất cũng có thể không vạch ra. Tuy nhiên, con người vẫn có thể cung cấp phản hồi có giá trị khi so sánh hai câu trả lời, vượt ra ngoài khả năng viết riêng của họ. Vẽ một sự tương đồng, trong khi chúng ta có thể không phải tất cả đều là nghệ sĩ thành đạt, khả năng đánh giá và phê bình nghệ thuật của chúng ta vẫn còn nguyên vẹn. Chúng tôi cho rằng khả năng viết vượt trội của LLM, như được thể hiện trong việc vượt qua các chú thích viên con người trong một số nhiệm vụ, được thúc đẩy cơ bản bởi RLHF, như được ghi chép trong Gilardi et al. (2023) và Huang et al. (2023). Dữ liệu được giám sát có thể không còn là tiêu chuẩn vàng, và hoàn cảnh phát triển này buộc phải đánh giá lại khái niệm "supervision."

In-Context Temperature Rescaling. Chúng tôi đã quan sát một hiện tượng hấp dẫn liên quan đến RLHF, một tính năng chưa được báo cáo trước đây theo hiểu biết tốt nhất của chúng tôi: việc re-scaling động của nhiệt độ tùy thuộc vào ngữ cảnh. Như được chỉ ra trong Hình 8, nhiệt độ dường như bị ảnh hưởng bởi RLHF. Tuy nhiên, thú vị là, các phát hiện của chúng tôi cũng tiết lộ rằng các thay đổi không được áp dụng đồng đều trên tất cả prompt, như được hiển thị trong Hình 21. Chẳng hạn, khi nói đến các prompt liên quan đến sáng tạo, như "Write a poem," sự tăng nhiệt độ tiếp tục tạo ra sự đa dạng trên các lần lặp RLHF khác nhau của chúng tôi. Điều này có thể được quan sát trong độ dốc Self-BLEU, phản ánh một mẫu tương đương với mô hình SFT.

Mặt khác, đối với các prompt dựa trên thông tin thực tế, như "What is the capital of?" độ dốc Self-BLEU giảm theo thời gian. Mẫu này đề xuất rằng mặc dù nhiệt độ tăng, mô hình học cung cấp cùng một phản hồi một cách nhất quán cho các prompt thực tế.

Llama 2-Chat Temporal Perception. Mô hình của chúng tôi thể hiện khả năng tổng quát hóa ấn tượng, như được hiển thị trong Hình 22. Chúng tôi đã thử nghiệm thủ công hàng chục ví dụ và quan sát một cách nhất quán rằng mô hình của chúng tôi thể hiện khả năng mạnh mẽ để tổ chức kiến thức của mình theo cách thời gian, ngay cả khi được cung cấp dữ liệu tối thiểu. Để thấm nhuần một khái niệm về thời gian trong Llama 2-Chat, chúng tôi thu thập một tập 1,000 ví dụ SFT liên quan đến ngày cụ thể. Các ví dụ này bao gồm các câu hỏi như "How long ago did Barack Obama become president?" Mỗi cái được liên kết với hai phần metadata quan trọng: ngày khi truy vấn được đặt ra — điều này ảnh hưởng đến phản hồi — và ngày sự kiện, một điểm thời gian trước đó mà câu hỏi sẽ vô nghĩa.

Quan sát cho thấy rằng LLM đã nội hóa khái niệm về thời gian ở mức độ lớn hơn so với giả định trước đây, mặc dù huấn luyện của chúng chỉ dựa trên dự đoán token tiếp theo và dữ liệu được xáo trộn ngẫu nhiên mà không quan tâm đến ngữ cảnh thời gian của chúng.

Tool Use Emergence. Việc tích hợp LLM với công cụ là một lĩnh vực nghiên cứu đang phát triển, như được làm nổi bật trong Mialon et al. (2023). Cách tiếp cận được thiết kế trong Toolformer (Schick et al., 2023) đòi hỏi việc lấy mẫu hàng triệu trajectory, bổ sung bởi việc xây dựng ví dụ few-shot cho mỗi công cụ. Tuy nhiên, kỹ thuật này chỉ được áp dụng bằng cách sử dụng một công cụ duy nhất mỗi ví dụ, và sẽ không mở rộng cho một chuỗi sử dụng công cụ.

Việc phát hành plugin của OpenAI đã kích thích diễn ngôn đáng kể trong cộng đồng học thuật, đốt lên các câu hỏi như: Làm thế nào chúng ta có thể dạy các mô hình sử dụng công cụ hiệu quả? hoặc Liệu quy trình có cần một bộ dữ liệu đáng kể không? Các thí nghiệm của chúng tôi cho thấy rằng việc sử dụng công cụ có thể xuất hiện tự phát từ căn chỉnh theo cách zero-shot. Mặc dù chúng tôi không bao giờ chú thích rõ ràng việc sử dụng công cụ, Hình 23 thể hiện một trường hợp mà mô hình thể hiện khả năng sử dụng một chuỗi công cụ trong ngữ cảnh zero-shot.

Ngoài ra, nghiên cứu của chúng tôi mở rộng để đánh giá Llama 2-Chat với quyền truy cập vào máy tính. Kết quả từ thí nghiệm cụ thể này được ghi chép trong Bảng 15. LLM tool use, trong khi thú vị, cũng có thể gây ra một số mối quan tâm an toàn. Chúng tôi khuyến khích thêm nghiên cứu cộng đồng và red teaming trong lĩnh vực này.

5.2 Hạn chế và Cân nhắc Đạo đức

Llama 2-Chat chịu các hạn chế được công nhận tốt tương tự của các LLM khác, bao gồm việc ngừng cập nhật kiến thức sau tiền huấn luyện, tiềm năng cho thế hệ không thực tế như lời khuyên không đủ tiêu chuẩn, và xu hướng hướng tới ảo giác.

Hơn nữa, phiên bản ban đầu của Llama 2-Chat chủ yếu tập trung vào dữ liệu tiếng Anh. Trong khi các quan sát thí nghiệm của chúng tôi đề xuất mô hình đã thu được một số thành thạo trong các ngôn ngữ khác, thành thạo của nó bị hạn chế, chủ yếu do lượng hạn chế của dữ liệu tiền huấn luyện có sẵn trong các ngôn ngữ không phải tiếng Anh (như được ghi chép trong Bảng 10). Do đó, hiệu suất của mô hình trong các ngôn ngữ khác ngoài tiếng Anh vẫn mong manh và nên được sử dụng một cách thận trọng.

Giống như các LLM khác, Llama 2 có thể tạo ra nội dung có hại, công kích, hoặc thiên kiến do huấn luyện trên các bộ dữ liệu có sẵn công khai trực tuyến. Chúng tôi đã cố gắng giảm thiểu điều này thông qua tinh chỉnh, nhưng một số vấn đề có thể vẫn còn, đặc biệt cho các ngôn ngữ khác ngoài tiếng Anh nơi các bộ dữ liệu có sẵn công khai không có sẵn. Chúng tôi sẽ tiếp tục tinh chỉnh và phát hành các phiên bản cập nhật trong tương lai khi chúng tôi tiến bộ trong việc giải quyết các vấn đề này.

Không phải ai sử dụng các mô hình AI đều có ý định tốt, và các agent AI đối thoại có thể có khả năng được sử dụng cho các mục đích xấu như tạo ra thông tin sai lệch hoặc truy xuất thông tin về các chủ đề như khủng bố sinh học hoặc tội phạm mạng. Tuy nhiên, chúng tôi đã nỗ lực tinh chỉnh các mô hình để tránh các chủ đề này và giảm thiểu bất kỳ khả năng nào chúng có thể đã cung cấp cho các trường hợp sử dụng đó.

Trong khi chúng tôi đã cố gắng cân bằng hợp lý an toàn với tính hữu ích, trong một số trường hợp, tinh chỉnh an toàn của chúng tôi đi quá xa. Người dùng Llama 2-Chat có thể quan sát một cách tiếp cận quá thận trọng, với mô hình nghiêng về phía từ chối một số yêu cầu hoặc phản hồi với quá nhiều chi tiết an toàn.

Người dùng của các mô hình tiền huấn luyện cần đặc biệt thận trọng, và nên thực hiện các bước bổ sung trong tinh chỉnh và triển khai như được mô tả trong Hướng dẫn Sử dụng Có trách nhiệm của chúng tôi.

5.3 Chiến lược Phát hành Có trách nhiệm

Chi tiết Phát hành. Chúng tôi làm cho Llama 2 có sẵn cho cả mục đích sử dụng nghiên cứu và thương mại tại https://ai.meta.com/resources/models-and-libraries/llama/. Những người sử dụng Llama 2 phải tuân thủ các điều khoản của giấy phép được cung cấp và Chính sách Sử dụng Chấp nhận được của chúng tôi, cấm bất kỳ việc sử dụng nào vi phạm các chính sách, luật pháp, quy tắc và quy định có thể áp dụng.

Chúng tôi cũng cung cấp các ví dụ mã để giúp các nhà phát triển tái tạo các thế hệ an toàn của chúng tôi với Llama 2-Chat và áp dụng các kỹ thuật an toàn cơ bản tại các lớp đầu vào người dùng và đầu ra mô hình. Các mẫu mã này có sẵn ở đây: https://github.com/facebookresearch/llama. Cuối cùng, chúng tôi đang chia sẻ một Hướng dẫn Sử dụng Có trách nhiệm, cung cấp hướng dẫn về phát triển và triển khai an toàn.

Phát hành Có trách nhiệm. Trong khi nhiều công ty đã chọn xây dựng AI đằng sau cánh cửa đóng, chúng tôi đang phát hành Llama 2 một cách mở để khuyến khích đổi mới AI có trách nhiệm. Dựa trên kinh nghiệm của chúng tôi, một cách tiếp cận mở tận dụng trí tuệ tập thể, sự đa dạng, và khéo léo của cộng đồng thực hành AI để nhận ra lợi ích của công nghệ này. Sự hợp tác sẽ làm cho các mô hình này tốt hơn và an toàn hơn. Toàn bộ cộng đồng AI—các nhà nghiên cứu học thuật, xã hội dân sự, các nhà hoạch định chính sách, và ngành công nghiệp—phải làm việc cùng nhau để phân tích và phơi bày một cách nghiêm ngặt các rủi ro của các hệ thống AI hiện tại và xây dựng các giải pháp giải quyết việc lạm dụng có vấn đề tiềm ẩn. Cách tiếp cận này không chỉ nuôi dưỡng sự hợp tác thực sự với các bên liên quan đa dạng—những người ngoài các bức tường của các công ty công nghệ lớn—mà còn phục vụ như nền tảng để dân chủ hóa quyền truy cập vào các mô hình nền tảng. Như được lập luận trong Zellers et al. (2019b), các bản phát hành mở thúc đẩy tính minh bạch và cho phép nhiều người hơn truy cập vào các công cụ AI, dân chủ hóa công nghệ và phi tập trung hóa chuyên môn AI. Chúng tôi tin rằng việc phi tập trung hóa chuyên môn AI làm nhiều hơn là chỉ đơn giản phân phối kiến thức—nó kích thích đổi mới và tăng tốc tiến bộ trong ngành. Cuối cùng, việc phát hành mở các mô hình này hợp nhất chi phí và loại bỏ các rào cản gia nhập, cho phép các doanh nghiệp nhỏ tận dụng đổi mới trong LLM để khám phá và xây dựng các trường hợp sử dụng tạo văn bản. Cuối cùng, chúng tôi tin rằng điều này sẽ tạo ra một sân chơi công bằng hơn cho các tổ chức thuộc mọi quy mô trên toàn cầu để hưởng lợi từ tăng trưởng kinh tế được hứa hẹn bởi sự tiến bộ của AI.

Chúng tôi biết rằng không phải ai sử dụng các mô hình AI đều có ý định tốt, và chúng tôi thừa nhận rằng có những mối quan tâm hợp lý về cách AI sẽ tác động đến thế giới của chúng ta. Việc tạo ra nội dung độc hại và các liên kết có vấn đề là những rủi ro có ý nghĩa mà cộng đồng AI vẫn chưa giảm thiểu hoàn toàn. Như bài báo này minh họa, chúng tôi đã đạt được những tiến bộ trong việc hạn chế tỷ lệ phổ biến của các loại phản hồi này. Trong khi chúng tôi nhận ra có nhiều việc cần làm hơn, nhận thức này chỉ làm sâu sắc thêm cam kết của chúng tôi với khoa học mở và hợp tác với cộng đồng AI.

6 Công trình Liên quan

Các Mô hình Ngôn ngữ Lớn. Những năm gần đây đã chứng kiến sự tiến hóa đáng kể trong lĩnh vực LLM. Theo quy luật mở rộng của Kaplan et al. (2020), một số Mô hình Ngôn ngữ Lớn với hơn 100B tham số đã được đề xuất, từ GPT-3 (Brown et al., 2020) đến Gopher (Rae et al., 2022) hoặc các mô hình chuyên môn, ví dụ: Galactica, cho khoa học (Taylor et al., 2022). Với 70B tham số, Chinchilla (Hoffmann et al., 2022) đã tái định nghĩa những quy luật mở rộng đó hướng về số lượng token hơn là trọng số mô hình. Đáng chú ý trong tiến triển này là sự nổi lên của Llama, được nhận ra vì tập trung vào hiệu quả tính toán trong suy luận (Touvron et al., 2023). Một diễn ngôn song song đã diễn ra xung quanh động lực của các mô hình mã nguồn mở so với nguồn đóng. Các bản phát hành mã nguồn mở như BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), và Falcon (Penedo et al., 2023) đã nổi lên để thách thức các đối tác nguồn đóng của chúng như GPT-3 và Chinchilla. Tuy nhiên, khi nói đến các LLM "sẵn sàng sản xuất" như ChatGPT, Bard, và Claude, có sự khác biệt rõ rệt về hiệu suất và khả năng sử dụng. Các mô hình này dựa vào các kỹ thuật tinh chỉnh phức tạp để căn chỉnh với sở thích con người (Gudibande et al., 2023), một quy trình vẫn đang được khám phá và tinh chỉnh trong cộng đồng mã nguồn mở.

Các nỗ lực để thu hẹp khoảng cách này đã xuất hiện, với các mô hình dựa trên chưng cất như Vicuna (Chiang et al., 2023) và Alpaca (Taori et al., 2023) áp dụng một cách tiếp cận độc đáo để huấn luyện với các hướng dẫn tổng hợp (Honovich et al., 2022; Wang et al., 2022). Tuy nhiên, trong khi các mô hình này cho thấy tiềm năng, chúng vẫn không đạt được tiêu chuẩn được đặt ra bởi các đối tác nguồn đóng của chúng.

Instruction Tuning. Wei et al. (2021) đã đạt được hiệu suất zero-shot trên các nhiệm vụ chưa thấy bằng cách tinh chỉnh LLM trên nhiều bộ dữ liệu. Chung et al. (2022) và Longpre et al. (2023) điều tra tác động của instruction tuning như một hàm của số lượng nhiệm vụ, kích thước mô hình, cài đặt prompt, v.v. Các prompt được sử dụng cho instruction tuning có thể được tạo bởi con người hoặc bởi chính LLM (Zhou et al., 2022), và các hướng dẫn follow-up có thể được sử dụng để tinh chỉnh các thế hệ ban đầu để làm cho chúng hữu ích, hấp dẫn và không thiên kiến hơn (Ganguli et al., 2023; Madaan et al., 2023). Một cách tiếp cận liên quan đến instruction tuning là chain-of-thought prompting (Wei et al., 2022b), trong đó các mô hình được nhắc giải thích lý luận của chúng khi được đưa ra một vấn đề phức tạp, để tăng khả năng câu trả lời cuối cùng của chúng là chính xác.

RLHF đã nổi lên như một chiến lược mạnh mẽ để tinh chỉnh Mô hình Ngôn ngữ Lớn, cho phép cải thiện đáng kể trong hiệu suất của chúng (Christiano et al., 2017). Phương pháp này, lần đầu tiên được giới thiệu bởi Stiennon et al. (2020) trong bối cảnh các nhiệm vụ tóm tắt văn bản, đã được mở rộng cho một loạt các ứng dụng khác. Trong paradigm này, các mô hình được tinh chỉnh dựa trên phản hồi từ người dùng con người, do đó căn chỉnh một cách lặp đi lặp lại các phản hồi của mô hình gần hơn với kỳ vọng và sở thích của con người.

Ouyang et al. (2022) chứng minh rằng sự kết hợp của instruction fine-tuning và RLHF có thể giúp khắc phục các vấn đề với tính thực tế, độc tính, và tính hữu ích không thể được khắc phục bằng cách chỉ mở rộng LLM. Bai et al. (2022b) tự động hóa một phần cách tiếp cận fine-tuning-plus-RLHF này bằng cách thay thế dữ liệu fine-tuning có nhãn con người bằng tự phê bình và sửa đổi của chính mô hình, và bằng cách thay thế người đánh giá con người bằng một mô hình khi xếp hạng đầu ra mô hình trong RLHF, một quy trình được gọi là "RL from AI Feedback" (RLAIF).

Các Thách thức An toàn LLM Đã biết. Tài liệu gần đây đã khám phá rộng rãi các rủi ro và thách thức liên kết với Mô hình Ngôn ngữ Lớn. Bender et al. (2021b) và Weidinger et al. (2021) nhấn mạnh nhiều mối nguy hiểm như thiên kiến, độc tính, rò rỉ dữ liệu riêng tư, và tiềm năng cho việc sử dụng độc hại. Solaiman et al. (2023) phân loại các tác động này thành hai nhóm—những tác động có thể được đánh giá trong hệ thống cơ sở và những tác động đòi hỏi đánh giá bối cảnh xã hội, trong khi Kumar et al. (2022) cung cấp các chiến lược giảm thiểu tiềm ẩn để hạn chế tác hại. Công việc từ Roller et al. (2020) và Dinan et al. (2021) cũng làm sáng tỏ những khó khăn gắn liền với LLM hướng chatbot, với các mối quan tâm từ quyền riêng tư đến tuyên bố chuyên môn gây hiểu lầm. Deng et al. (2023) đề xuất một framework phân loại để giải quyết các vấn đề này, và Bergman et al. (2022) đi sâu vào sự cân bằng giữa các tác động tích cực và tiêu cực tiềm ẩn từ việc phát hành các mô hình đối thoại.

Các điều tra về red teaming tiết lộ các thách thức cụ thể trong các LLM được tinh chỉnh, với các nghiên cứu của Ganguli et al. (2022) và Zhuo et al. (2023) giới thiệu nhiều loại tấn công thành công và ảnh hưởng của chúng đến việc tạo ra nội dung có hại. Các cơ quan an ninh quốc gia và nhiều nhà nghiên cứu, như (Mialon et al., 2023), cũng đã đưa ra cờ đỏ xung quanh hành vi mô hình nổi lên tiên tiến, mối đe dọa mạng, và lạm dụng tiềm ẩn trong các lĩnh vực như chiến tranh sinh học. Cuối cùng, các vấn đề xã hội rộng lớn hơn như thay thế việc làm do nghiên cứu AI được tăng tốc và sự phụ thuộc quá mức vào LLM dẫn đến suy thoái dữ liệu huấn luyện cũng là những cân nhắc phù hợp (Acemoglu và Restrepo, 2018; Autor và Salomons, 2018; Webb, 2019; Shumailov et al., 2023). Chúng tôi cam kết tiếp tục công việc của chúng tôi tham gia với cộng đồng chính sách, học thuật và ngành công nghiệp rộng lớn hơn về các vấn đề này.

7 Kết luận

Trong nghiên cứu này, chúng tôi đã giới thiệu Llama 2, một họ mô hình tiền huấn luyện và tinh chỉnh mới với quy mô từ 7 tỷ đến 70 tỷ tham số. Các mô hình này đã chứng minh tính cạnh tranh của chúng với các mô hình chat mã nguồn mở hiện có, cũng như năng lực tương đương với một số mô hình độc quyền trên các tập đánh giá mà chúng tôi kiểm tra, mặc dù chúng vẫn tụt hậu so với các mô hình khác như GPT-4. Chúng tôi đã trình bày tỉ mỉ các phương pháp và kỹ thuật được áp dụng trong việc đạt được các mô hình của chúng tôi, với sự nhấn mạnh nặng nề vào việc căn chỉnh của chúng với các nguyên tắc tính hữu ích và an toàn. Để đóng góp ý nghĩa hơn cho xã hội và nuôi dưỡng tốc độ nghiên cứu, chúng tôi đã mở quyền truy cập một cách có trách nhiệm đến Llama 2 và Llama 2-Chat. Như một phần của cam kết liên tục của chúng tôi đối với tính minh bạch và an toàn, chúng tôi dự định thực hiện các cải tiến thêm cho Llama 2-Chat trong công việc tương lai.
