# 2307.08072.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl-papers/2307.08072.pdf
# Kích thước tệp: 523056 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khả năng Emergent có tồn tại trong Mô hình Ngôn ngữ Lớn được Lượng tử hóa không:
Một Nghiên cứu Thực nghiệm
Peiyu Liu1,2, Zikang Liu1,2, Ze-Feng Gao1, Dawei Gao3,
Wayne Xin Zhao1,2∗,Yaliang Li3,Bolin Ding3, và Ji-Rong Wen1,2,4
1Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
2Phòng thí nghiệm Trọng điểm Bắc Kinh về Quản lý và Phân tích Dữ liệu Lớn
3Tập đoàn Alibaba,4Trường Thông tin, Đại học Nhân dân Trung Quốc
liupeiyustu@163.com,jason8121@foxmail.com,batmanfly@gmail.com
{zfgao,jrwen}@ruc.edu.cn,{gaodawei.gdw,yaliang.li,bolin.ding}@alibaba-inc.com

Tóm tắt
Mặc dù có hiệu suất vượt trội, các Mô hình Ngôn ngữ Lớn (LLMs) yêu cầu tài nguyên tính toán đáng kể để triển khai và sử dụng. Để vượt qua vấn đề này, các phương pháp lượng tử hóa đã được áp dụng rộng rãi để giảm dung lượng bộ nhớ của LLMs cũng như tăng tốc độ suy luận. Tuy nhiên, một thách thức lớn là các phương pháp lượng tử hóa bit thấp thường dẫn đến suy giảm hiệu suất. Việc hiểu cách lượng tử hóa ảnh hưởng đến khả năng của LLMs là quan trọng. Khác với các nghiên cứu trước tập trung vào hiệu suất tổng thể, công việc này nhằm điều tra tác động của lượng tử hóa đến các khả năng emergent, là những đặc điểm quan trọng phân biệt LLMs với các mô hình ngôn ngữ nhỏ. Đặc biệt, chúng tôi kiểm tra các khả năng học trong ngữ cảnh, lý luận chuỗi tư duy, và tuân theo hướng dẫn trong LLMs đã lượng tử hóa. Các thí nghiệm thực nghiệm của chúng tôi cho thấy các khả năng emergent này vẫn tồn tại trong các mô hình lượng tử hóa 4-bit, trong khi các mô hình 2-bit gặp phải suy giảm hiệu suất nghiêm trọng trong việc kiểm tra các khả năng này. Để cải thiện hiệu suất của các mô hình bit thấp, chúng tôi tiến hành hai thí nghiệm đặc biệt: (1) phân tích tác động chi tiết nghiên cứu thành phần (hoặc cấu trúc con) nào nhạy cảm hơn với lượng tử hóa, và (2) bù đắp hiệu suất thông qua điều chỉnh mô hình. Công việc của chúng tôi rút ra một loạt các phát hiện quan trọng để hiểu tác động của lượng tử hóa đến các khả năng emergent, và làm sáng tỏ khả năng lượng tử hóa bit cực thấp cho LLMs.

1 Giới thiệu
Gần đây, Trí tuệ Nhân tạo (AI) đã chứng kiến tiến bộ đáng kể do sự xuất hiện của các Mô hình Ngôn ngữ Lớn (LLMs) (Brown et al., 2020; Zhao et al., 2023). So với các mô hình ngôn ngữ kích thước nhỏ, LLMs, với việc mở rộng lớn kích thước mô hình và kích thước tập dữ liệu huấn luyện, đã thể hiện các hành vi rất khác biệt khi được kích hoạt bởi các prompt được thiết kế đặc biệt. Nói chung, LLMs có thể có được các khả năng vượt trội hơn, như học trong ngữ cảnh (ICL, Brown et al. 2020) và lý luận chuỗi tư duy (CoT, Wei et al. 2022), có thể không có mặt trong các mô hình ngôn ngữ kích thước nhỏ. Những khả năng như vậy thường được gọi chính thức là các khả năng emergent (Wei et al., 2022)1.

Mặc dù có hiệu suất vượt trội, việc triển khai LLMs trong các ứng dụng thực tế rất tốn kém do kích thước mô hình khổng lồ. Đối mặt với vấn đề này, lượng tử hóa mô hình (Dettmers et al., 2022; Frantar et al., 2022; Yao et al., 2023a) đã trở thành một cách tiếp cận được sử dụng rộng rãi để giảm dung lượng bộ nhớ của LLMs. Ý tưởng cơ bản của lượng tử hóa là ánh xạ các số dấu phẩy động thành các số nguyên bit thấp (ví dụ, BF16 thành INT8), để giảm tổng số bit của mô hình. Thông thường, các phương pháp hiện có áp dụng cách tiếp cận lượng tử hóa sau huấn luyện (PTQ) (Frantar et al., 2022; Dettmers et al., 2022) mà không cần huấn luyện lại các tham số mô hình. Tuy nhiên, các phương pháp PTQ hiện có thường gặp phải suy giảm hiệu suất trong lượng tử hóa bit thấp.

Để sử dụng LLMs đã lượng tử hóa một cách hiệu quả, việc hiểu mức độ hiệu suất có thể đạt được ở độ chính xác bit khác nhau là quan trọng, ví dụ, độ chính xác bit thấp nhất để lượng tử hóa đạt hiệu suất tốt trên một nhiệm vụ cụ thể là gì? Gần đây, một số nghiên cứu đã tiến hành các thí nghiệm đánh giá toàn diện về tác động của lượng tử hóa mô hình đến hiệu suất của LLMs (Yao et al., 2023b; Dettmers and Zettlemoyer, 2022). Tuy nhiên, họ chủ yếu phân tích hiệu suất tổng quát của LLMs đã lượng tử hóa (ví dụ, mô hình hóa ngôn ngữ), thiếu điều tra sâu sắc về khả năng của LLM trên các nhiệm vụ phức tạp.

Trong công việc này, chúng tôi tập trung vào việc kiểm tra hiệu suất của LLMs đã lượng tử hóa trong việc giải quyết các nhiệm vụ phức tạp, để khám phá tác động của lượng tử hóa đến các khả năng emergent của LLMs. Như đã được chứng minh trong các nghiên cứu trước (Wei et al., 2022), có sự phụ thuộc mạnh mẽ giữa các khả năng emergent và quy mô tham số. Thật tò mò liệu các khả năng emergent có biến mất dưới cài đặt độ chính xác bit thấp mặc dù kích thước mô hình vẫn ở quy mô ban đầu. Ngoài ra, việc khám phá các yếu tố (ví dụ, cấu trúc mô hình) có thể ảnh hưởng đến các khả năng emergent cũng quan trọng. Hơn nữa, chúng tôi cũng quan tâm đến các cách tiếp cận tiềm năng để tăng cường hiệu suất của các mô hình bit thấp.

Đặc biệt, chúng tôi nhằm trả lời hai câu hỏi sau: (1) Các khả năng emergent có tồn tại trong các mô hình ngôn ngữ lớn đã lượng tử hóa không? Nếu có, nó có thể đạt được mức độ hiệu suất nào? (2) Làm thế nào để tăng cường hiệu suất của các mô hình bit thấp? Để trả lời hai câu hỏi chính, chúng tôi đánh giá ba khả năng chính, cụ thể là học trong ngữ cảnh (ICL), lý luận chuỗi tư duy (CoT), và khả năng Tuân theo Hướng dẫn (IF), trên một tập hợp các mô hình LLaMA (Touvron et al., 2023) được sử dụng rộng rãi làm mô hình nền tảng. Chúng tôi tiến hành các thí nghiệm thực nghiệm mở rộng, nhằm có được hiểu biết tốt hơn về hiệu suất mô hình của LLMs đã lượng tử hóa.

Đối với câu hỏi đầu tiên, chúng tôi đánh giá các mô hình LLaMA ở bốn kích thước (tức là 7B, 13B, 30B, và 65B), kiểm tra hiệu suất của chúng qua một loạt các mức độ chính xác: 2-bit, 4-bit, 8-bit, và 16-bit. Các thí nghiệm của chúng tôi chỉ ra rằng độ chính xác 4-bit mang lại sự cân bằng thuận lợi nhất giữa hiệu suất mô hình và dung lượng bộ nhớ, đạt được kết quả vượt trội với cùng lượng tổng bit được phân bổ. Tuy nhiên, tất cả các mô hình ở các kích thước khác nhau đều gặp phải suy giảm nghiêm trọng ở độ chính xác 2-bit.

Về câu hỏi thứ hai, chúng tôi kiểm tra cẩn thận độ nhạy cảm lượng tử hóa của các thành phần mô hình khác nhau (hoặc cấu trúc con), đặc biệt là attention và mạng feed-forward (FFN). Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng FFN đóng vai trò quan trọng trong việc duy trì hiệu suất mô hình cho lượng tử hóa bit thấp. Chúng tôi cũng đánh giá tác động của các chiều outlier, là các chiều cụ thể thể hiện giá trị cao hơn đáng kể so với các chiều khác trong kích hoạt đặc trưng. Chúng tôi thấy các chiều outlier ảnh hưởng đến hầu hết các lớp Transformer chịu trách nhiệm chính cho sự suy giảm hiệu suất lượng tử hóa, và chúng chủ yếu tập trung vào các phép chiếu xuống của FFN. Những quan sát này thúc đẩy chúng tôi thiết kế các chiến lược lượng tử hóa cấu trúc con chi tiết hơn để cải thiện hiệu suất của các mô hình bit thấp.

Hơn nữa, chúng tôi nghiên cứu cách tăng cường hiệu suất của các mô hình lượng tử hóa thông qua điều chỉnh tinh. Chúng tôi đánh giá tác động của các phương pháp điều chỉnh tinh khác nhau được thực hiện trước và sau lượng tử hóa. Kết quả thí nghiệm của chúng tôi tiết lộ rằng điều chỉnh tinh hiệu quả tham số sau lượng tử hóa có thể đạt hiệu suất đáng khen ngợi với tài nguyên tính toán giảm đáng kể. Cách tiếp cận của chúng tôi có thể điều chỉnh tinh một mô hình LLaMA-65B 2-bit trên một NVIDIA A100 duy nhất, vượt qua hiệu suất của mô hình LLaMA-13B 16-bit trên bộ dữ liệu MMLU zero-shot.

2 Nền tảng
Trong phần này, chúng tôi giới thiệu nền tảng cho các khả năng emergent và lượng tử hóa sau huấn luyện.

Khả năng Emergent Với sự gia tăng của các tham số mô hình và tập dữ liệu huấn luyện, LLMs thể hiện một số khả năng đặc biệt có thể không có mặt trong các mô hình ngôn ngữ kích thước nhỏ, được gọi là các khả năng emergent (Wei et al., 2022). Các khả năng emergent là một dấu hiệu quan trọng của hiệu suất vượt trội của LLMs, đã nhận được nhiều sự chú ý trong cộng đồng nghiên cứu. Theo cuộc khảo sát về LLMs (Zhao et al., 2023), chúng tôi tập trung thảo luận ba khả năng emergent chính, cụ thể là học trong ngữ cảnh, lý luận chuỗi tư duy, và tuân theo hướng dẫn. Tiếp theo, chúng tôi sẽ giới thiệu ngắn gọn từng khả năng.

• Học trong Ngữ cảnh (ICL) được giới thiệu bởi GPT-3 (Brown et al., 2020) để giải quyết các nhiệm vụ phức tạp thông qua các prompt được thiết kế đặc biệt. Nó có thể hướng dẫn hiệu quả LLMs tạo ra đầu ra mong muốn cho các ví dụ kiểm tra bằng cách tận dụng các hướng dẫn ngôn ngữ tự nhiên và/hoặc các minh chứng nhiệm vụ, mà không cần huấn luyện bổ sung hoặc cập nhật gradient.

• Lý luận Chuỗi tư duy (CoT) là một chiến lược prompting đặc biệt giải quyết các nhiệm vụ phức tạp bao gồm nhiều bước lý luận, như các bài toán từ toán học. Nó kết hợp các bước lý luận trung gian cho mỗi minh chứng trong prompt, do đó kích thích khả năng giải quyết các nhiệm vụ phức tạp thông qua lý luận từng bước.

• Tuân theo Hướng dẫn (IF) đề cập đến khả năng vượt trội mà một LLM tuân theo các hướng dẫn của con người và hoàn thành nhiệm vụ đích theo yêu cầu. Mặc dù nó chia sẻ định dạng tương tự với ICL bằng cách sử dụng các hướng dẫn ngôn ngữ tự nhiên, nó thường không bao gồm minh chứng và yêu cầu điều chỉnh cụ thể (tức là điều chỉnh hướng dẫn) để kích thích khả năng này.

Lưu ý rằng các khả năng emergent có thể được định nghĩa trên các nhiệm vụ hoặc cài đặt khác nhau. Chúng tôi chọn ba khả năng để nghiên cứu, chủ yếu vì chúng được sử dụng rộng rãi để giải quyết các nhiệm vụ phức tạp.

Lượng tử hóa Sau Huấn luyện Do số lượng tham số khổng lồ, việc tiến hành điều chỉnh tinh toàn bộ trên các tham số mô hình thường không khả thi. Do đó, các phương pháp lượng tử hóa sau huấn luyện (PTQ) (Dettmers et al., 2022; Frantar et al., 2022; Yao et al., 2023b) được sử dụng rộng rãi cho LLMs. Đối với các phương pháp PTQ, chúng thường chỉ dựa vào dữ liệu hiệu chuẩn nhỏ để điều chỉnh các tham số lượng tử hóa, rất hiệu quả trong việc triển khai. Trong công việc này, chúng tôi áp dụng một phương pháp lượng tử hóa phổ biến, GTPQ (Frantar et al., 2022), để tiến hành các thí nghiệm. Đặc biệt, GPTQ sử dụng hàm mất mát tái tạo theo lớp để giảm thiểu sự khác biệt giữa các trọng số gốc (W) và các trọng số đã lượng tử hóa (cW) thông qua tối ưu hóa mục tiêu sau: arg min cW∥WX−cWX∥2_2. Nó có thể đạt được kết quả rất hứa hẹn cho lượng tử hóa 4-bit trên LLMs, và cũng cung cấp hỗ trợ cho độ chính xác bit thấp hơn cho lượng tử hóa trọng số.

Ngoài trọng số mô hình, các kích hoạt cũng được xem xét để lượng tử hóa. Tuy nhiên, do sự hiện diện của các chiều outlier (Dettmers et al., 2022) trong các giá trị kích hoạt đặc trưng, việc lượng tử hóa các kích hoạt ở độ chính xác bit thấp được thừa nhận rộng rãi là một nhiệm vụ thách thức. Những chiều outlier này thể hiện giá trị cao hơn đáng kể so với các chiều khác và trở nên đặc biệt nổi bật khi quy mô mô hình tăng.

3 Các khả năng Emergent có tồn tại trong LLMs đã Lượng tử hóa không?
Trong phần này, chúng tôi nhằm điều tra sự tồn tại của các khả năng emergent trong LLMs đã lượng tử hóa, đặc biệt tập trung vào học trong ngữ cảnh (ICL), lý luận chuỗi tư duy (CoT), và tuân theo hướng dẫn (IF). Tiếp theo, chúng tôi trước tiên giới thiệu thiết lập thí nghiệm và sau đó trình bày các phát hiện chính.

3.1 Thiết lập thí nghiệm
Kiểm tra Học trong Ngữ cảnh Để đánh giá khả năng ICL, chúng tôi sử dụng hai bộ dữ liệu được sử dụng rộng rãi để đánh giá LLMs: MMLU (Hendrycks et al., 2021) và BBH (Srivastava et al., 2022a). MMLU phục vụ như một điểm chuẩn toàn diện để đánh giá hiểu biết kiến thức đa nhiệm vụ trong các lĩnh vực khác nhau, bao gồm các lĩnh vực như toán học, khoa học máy tính, nhân văn, và khoa học xã hội. Ngoài ra, BBH là một biến thể thách thức của Big-Bench (Srivastava et al., 2022b), được đề xuất để tập trung vào việc điều tra các nhiệm vụ hiện tại không thể giải quyết được của LLMs. Sau đó, chúng tôi tiến hành đánh giá trên các bộ dữ liệu MMLU (tức là five-shot và zero-shot) và BBH (tức là three-shot và zero-shot), tương ứng.

Kiểm tra Lý luận Chuỗi tư duy Để đánh giá khả năng CoT của mô hình, chúng tôi sử dụng bộ dữ liệu GSM8K được sử dụng rộng rãi. GSM8K là một bộ dữ liệu lý luận bao gồm 8K vấn đề cùng nhau đánh giá khả năng của mô hình trong lý luận số học và thành phần của các bước toán học. Theo phương pháp được giới thiệu trong Fu et al. (2023), chúng tôi tiến hành đánh giá sử dụng cài đặt few-shot, trong đó các minh chứng được cung cấp. Mỗi minh chứng được định dạng như <input, CoT, output>, cho phép nó kích thích khả năng của mô hình để lý luận và tạo ra các chuỗi tư duy mạch lạc.

Kiểm tra Tuân theo Hướng dẫn Để đánh giá khả năng tuân theo hướng dẫn, chúng tôi tham khảo cách tiếp cận được đề xuất trong Vicuna (Chiang et al., 2023) và tiến hành đánh giá tự động dựa trên GPT3.5 (viết tắt là AutoEval). Cụ thể, chúng tôi sử dụng bộ dữ liệu trong Vicuna bao gồm 80 câu hỏi trải rộng 8 danh mục khác biệt. Sau đó, mỗi mô hình được giao nhiệm vụ tạo ra một phản hồi cho mỗi câu hỏi trong bộ dữ liệu.

Cài đặt Lượng tử hóa Để đánh giá hiệu suất của các khả năng emergent nói trên của lượng tử hóa, chúng tôi tiến hành một loạt thí nghiệm toàn diện. Các thử nghiệm của chúng tôi được tiến hành dựa trên việc triển khai GPTQ-for-LLaMA2, chỉ tập trung vào lượng tử hóa trọng số và bao gồm tất cả các thành phần mô hình (tức là các ma trận phép chiếu query, key, value, output trong mô-đun attention và các ma trận phép chiếu gate, up, down trong mạng feed-forward). Về kích thước mô hình, chúng tôi bao gồm một tập hợp các mô hình LLaMA có 7B, 13B, 30B, và 65B tham số. Chúng tôi xem xét lượng tử hóa ở độ chính xác 2-bit, 4-bit, 8-bit, và không lượng tử hóa (16-bit). Những cấu hình đa dạng này nhằm đánh giá kỹ lưỡng tác động của các cài đặt lượng tử hóa khác nhau đến hiệu suất mô hình.

Chỉ số Đánh giá Để đánh giá hiệu suất, chúng tôi áp dụng độ chính xác trên các bộ kiểm tra làm chỉ số đánh giá cho các kiểm tra ICL và CoT. Đối với IF, chúng tôi so sánh các phản hồi được tạo ra bởi hai mô hình side-by-side và có được một "điểm" cho mỗi mô hình bởi GPT3.5. Ngoài ra, để định lượng chi phí bộ nhớ, chúng tôi theo Dettmers and Zettlemoyer (2022) và tính toán tổng số bit (mô hình) bằng cách nhân tổng số tham số với số bit biểu diễn thực tế.

3.2 Kết quả và Phân tích
Trong phần này, chúng tôi trình bày kết quả thí nghiệm và phân tích tương ứng.

Nhìn chung, ba loại khả năng emergent dường như ít bị ảnh hưởng với lượng tử hóa 4-bit. Bảng 1 trình bày kết quả kiểm tra của các mô hình sử dụng độ chính xác 2-bit, 4-bit, 8-bit và 16-bit trên nhiều bộ dữ liệu, bao gồm MMLU, BBH cho ICL, GSM8K cho CoT, AutoEval cho IF và WikiText cho khả năng mô hình hóa ngôn ngữ tổng quát. Như chúng ta có thể thấy, các kết quả thu được bằng cách sử dụng lượng tử hóa 4-bit và 8-bit rất tương tự với hiệu suất ban đầu (tức là số dấu phẩy động 16-bit). Tuy nhiên, một sự suy giảm đáng kể được quan sát thấy khi sử dụng lượng tử hóa 2-bit, với kết quả tiếp cận mức gần ngẫu nhiên, ví dụ, khoảng 0.25 trong các nhiệm vụ phân loại 4 lựa chọn cho MMLU và BBH và 0.0 cho GSM8K. Điều này cho thấy lượng tử hóa 4-bit có thể giữ lại hiệu quả các khả năng emergent trên các bộ dữ liệu kiểm tra này.

Độ chính xác 4-bit thể hiện sự cân bằng thuận lợi về cả tổng số bit và hiệu suất. Như được hiển thị trong Bảng 1, có thể quan sát thấy lượng tử hóa 4-bit cung cấp một sự giảm đáng kể trong chi phí bộ nhớ. Để kiểm tra thêm mối quan hệ giữa hiệu suất mô hình và sử dụng tài nguyên, chúng tôi theo Dettmers and Zettlemoyer (2022) để giới thiệu thước đo tổng số bit bằng cách nhân số tham số và số bit, và báo cáo kết quả kiểm tra trong Hình 1 bằng cách thay đổi số tổng bit. Từ bốn đường cong độ chính xác tương ứng với độ chính xác bit khác nhau, chúng ta có thể thấy rằng độ chính xác 4-bit liên tục thể hiện độ chính xác mô hình cao hơn dưới cùng lượng tổng bit mô hình. Do đó, lượng tử hóa 4-bit được khuyến nghị sử dụng để có sự cân bằng thuận lợi giữa chi phí bộ nhớ và hiệu suất mô hình trong thực tế.

Hiệu ứng mở rộng phụ thuộc vào các nhiệm vụ cụ thể, và việc tăng quy mô mô hình có lợi nhất cho nhiệm vụ CoT. Chúng tôi đã tiến hành một cuộc điều tra, như được mô tả trong Hình 1, để kiểm tra tác động của việc mở rộng tổng số bit đến hiệu suất của một mô hình bit thấp trên nhiều nhiệm vụ. Nhìn chung, phân tích của chúng tôi tiết lộ rằng đối với độ chính xác 2-bit, việc tăng tổng số bit (tức là kích thước mô hình lớn hơn) không mang lại những cải thiện đáng kể, đặc biệt là cho MMLU và GSM8K, vì các kết quả thu được không thể hiện sự vượt trội so với điểm ngẫu nhiên (tức là 0.25 trên MMLU và 0.0 trên GSM8K). Thật vậy, việc giảm thiểu hiệu quả các lỗi do lượng tử hóa trong các mô hình 2-bit vẫn là một nhiệm vụ thách thức. Đối với các mô hình độ chính xác 4-bit (hoặc cao hơn), chúng tôi quan sát thấy những cải thiện đáng kể trên các nhiệm vụ CoT khi tăng tổng số bit, không đáng kể đối với kiểm tra ICL. Hơn nữa, đối với kiểm tra IF, một quy mô mô hình nhỏ có thể đủ để đạt được hiệu suất rất tốt trong các thí nghiệm kiểm tra của chúng tôi3.

Hiệu suất lượng tử hóa bit thấp được hưởng lợi từ các minh chứng trong các kiểm tra ICL. Đối với các nhiệm vụ phức tạp, chúng ta có thể cung cấp các minh chứng few-shot để cải thiện hiệu suất mô hình. Để kiểm tra điều này, trong Bảng 1, chúng tôi cũng trình bày kết quả với các minh chứng few-shot cho ICL. Chúng tôi có thể quan sát thấy một lợi thế đáng kể của cài đặt five-shot so với cài đặt zero-shot, đặc biệt là đối với độ chính xác 2-bit trên LLaMA-30B (tức là 26.1 so với 3.7). Điều này cho thấy hiệu suất lượng tử hóa bit thấp của LLMs có thể được cải thiện đáng kể khi các minh chứng phù hợp được sử dụng. Tuy nhiên, sự cải thiện như vậy không đáng kể đối với độ chính xác 2-bit trong LLaMA-7B (tức là 3.8 so với 2.3), cho thấy quy mô tham số phải đạt đến một mức độ nhất định cho khả năng này.

Đối với các kiểm tra CoT, lượng tử hóa 2-bit cực đoan yêu cầu một quy mô mô hình lớn. Từ Bảng 1, chúng tôi thấy rằng khả năng CoT đối với độ chính xác 2-bit không còn tồn tại đối với các mô hình 7B và 13B trên các bộ dữ liệu kiểm tra của chúng tôi, vì cả hai đều đạt độ chính xác 0.0 trên GSM8K trong khi 30B đạt 0.2. Điều này cho thấy một kích thước mô hình đủ lớn là cần thiết cho khả năng CoT đối với lượng tử hóa 2-bit. Để điều tra thêm hiện tượng này, chúng tôi tiến hành phân tích nghiên cứu trường hợp cho các mô hình LLaMA với 7B, 13B và 30B trên các bộ kiểm tra GSM8K và hiển thị một số ví dụ kiểm tra trong Bảng 2. Từ những ví dụ này, chúng ta có thể thấy rằng mô hình 7B gần như không có khả năng tạo ra đầu ra văn bản chính xác, dẫn đến đầu ra lộn xộn. Mặc dù mô hình 13B có thể tạo ra phản hồi bình thường nhưng thất bại trong việc tạo ra chuỗi lý luận chính xác. Để so sánh, mô hình 30B thành công trong việc tạo ra chuỗi lý luận chính xác, mặc dù với kết quả suy luận không chính xác.

4 Làm thế nào để Tăng cường Hiệu suất của các Mô hình Bit thấp?
Để khám phá các chiến lược đạt hiệu suất cao hơn với lượng tử hóa sau huấn luyện (PTQ) bit thấp, chúng tôi tiếp theo tiến hành các thí nghiệm phân tích để điều tra các yếu tố ảnh hưởng đến hiệu suất lượng tử hóa. Đầu tiên, chúng tôi phân tích độ nhạy cảm lượng tử hóa của các cấu trúc mô hình chi tiết. Thứ hai, chúng tôi kiểm tra tác động của bù đắp hiệu suất qua điều chỉnh mô hình.

4.1 Phân tích Độ nhạy cảm Lượng tử hóa
4.1.1 Thiết lập Thí nghiệm
Như đã thảo luận trong các nghiên cứu trước (Dettmers et al., 2022; Yao et al., 2023b), các thành phần mô hình khác nhau (hoặc chiều đặc trưng) có thể thể hiện độ nhạy cảm khác nhau đối với lượng tử hóa, tức là các mức độ suy giảm hiệu suất khác nhau. Trong phần này, chúng tôi chủ yếu tập trung vào lượng tử hóa bit thấp, và thiết lập ba thí nghiệm sau về độ nhạy cảm lượng tử hóa (Bảng 3):

• Phân tích lượng tử hóa thành phần. Trong thí nghiệm này, chúng tôi kiểm tra độ nhạy cảm của hai thành phần chính trong kiến trúc Transformer, tức là các lớp attention và mạng feed-forward (FFN). Cụ thể, chúng tôi xem xét đánh giá hiệu suất của hai biến thể được ký hiệu là "¬ATT" và "¬FFN", trong đó một trong hai thành phần attention hoặc FFN được bảo toàn ở độ chính xác FP16, trong khi các thành phần còn lại được lượng tử hóa thành bit thấp. Nó nhằm phân tích mức độ suy giảm hiệu suất cho từng loại thành phần mô hình.

• Phân tích lượng tử hóa outlier. Như đã phát hiện trong các nghiên cứu trước (Dettmers et al., 2022), việc lượng tử hóa các chiều đặc trưng có độ lớn lớn (được gọi là outlier) có thể làm hỏng độ chính xác lượng tử hóa, đặc biệt là khi các outlier xuất hiện trong tất cả các lớp Transformer. Do đó, chúng tôi đầu tiên sắp xếp các chiều outlier dựa trên số lượng lớp chúng ảnh hưởng và tập trung vào n chiều hàng đầu. Cụ thể, chúng tôi đầu tiên chọn các chiều outlier hàng đầu trong kích hoạt (được bảo toàn ở độ chính xác FP16 trong phương pháp LLM.int8() (Dettmers et al., 2022)), và lượng tử hóa những chiều thuộc về n chiều hàng đầu và các chiều không phải outlier khác thành độ chính xác INT8. Kết quả sau đó được so sánh với phương pháp LLM.int8() tiêu chuẩn. Cách tiếp cận này cho phép chúng tôi điều tra tác động của các chiều đặc trưng outlier về các khả năng emergent.

• Phân tích lượng tử hóa cấu trúc con. Trong công việc hiện có, họ nghiên cứu tác động ở cấp độ thành phần hoặc cấp độ đặc trưng đến hiệu suất lượng tử hóa. Ngoài ra, chúng tôi cũng phát hiện thực nghiệm rằng các cấu trúc con khác nhau trong một thành phần có tầm quan trọng khác nhau đối với LLMs đã lượng tử hóa. Ví dụ, như sẽ được thảo luận trong Phần 4.1.2, các chiều outlier chủ yếu tồn tại trong các phép chiếu xuống của các thành phần FFN. Do đó, chúng tôi xem xét lượng tử hóa chi tiết hơn ở cấp độ cấu trúc con. Đặc biệt, các cấu trúc con quan trọng trong một thành phần được bảo toàn ở mức độ chính xác FP16. Kết quả được báo cáo là "¬ trọng số quan trọng", trong đó các ma trận trọng số quan trọng có lỗi lượng tử hóa cao có thể được xác định dựa trên các thuật toán lượng tử hóa hiện có.

4.1.2 Kết quả và Phân tích
Thành phần FFN có ý nghĩa quan trọng đối với lượng tử hóa 2-bit. Chúng tôi tiến hành các thí nghiệm kiểm tra để đánh giá độ nhạy cảm lượng tử hóa của các thành phần mô hình khác nhau, đặc biệt là các thành phần attention và FFN. Vì lượng tử hóa 4-bit có thể giữ lại hiệu suất ban đầu trong khi các mô hình 2-bit gặp phải suy giảm nghiêm trọng, chúng tôi tập trung vào phân tích trường hợp 2-bit cực đoan. Kết quả trong Hình 2 chứng minh thành phần FFN thể hiện ý nghĩa quan trọng đối với các mô hình 2-bit. Giữ FFN trong FP16 cải thiện hiệu suất của LLaMA-7B-2bit từ 0.038 đến 0.225 và hiệu suất của LLaMA-13B-2bit từ 0.148 đến 0.286. Những cải thiện này cho thấy tầm quan trọng của các thành phần FFN trong việc duy trì hiệu suất, cần xem xét cụ thể dưới lượng tử hóa 2-bit cực đoan.

Chiều outlier ảnh hưởng đến hầu hết các lớp chịu trách nhiệm chính cho sự suy giảm hiệu suất. Ngoài các thành phần quan trọng, chúng tôi tiếp tục phân tích tác động của các chiều outlier đến hiệu suất mô hình bit thấp. Như đã quan sát trong Dettmers et al. (2022) rằng các outlier đặc trưng xuất hiện trong tất cả các lớp Transformer rất quan trọng đối với hiệu suất mô hình, chúng tôi do đó tập trung vào những outlier ảnh hưởng đến hầu hết các lớp. Đặc biệt, chúng tôi đầu tiên xác định các chiều outlier hàng đầu theo số lượng lớp chúng ảnh hưởng. Sau đó, chúng tôi đánh giá tác động của các chiều outlier top-1 và top-3 bằng cách lượng tử hóa chúng thành bit thấp trong khi giữ các chiều outlier khác như FP16. Ngoài ra, chúng tôi cũng lượng tử hóa các chiều không phải outlier như trong LLM.int8(). Kết quả đánh giá của LLaMA-7B và LLaMA-13B được trình bày trong Hình 3. Chúng ta có thể thấy rằng những outlier hàng đầu này có tác động đáng kể đến hiệu suất lượng tử hóa, đặc biệt là kết quả CoT và điểm PPL. Thú vị là, LLaMA-13B gặp phải suy giảm hiệu suất nghiêm trọng hơn so với mô hình 7B bằng cách lượng tử hóa chiều outlier top-1. Điều này cho thấy việc lượng tử hóa các outlier quan trọng có tác động đáng kể hơn trên các mô hình lớn hơn. Một phát hiện quan trọng khác là các chiều outlier dường như xuất hiện trên cấu trúc con đặc biệt của một thành phần. Ví dụ, các outlier chủ yếu xảy ra trong phép chiếu xuống của các thành phần FFN đối với LLaMA-7B.

Hiệu suất của mô hình 2-bit có thể được tăng cường thêm với lượng tử hóa cấu trúc con chi tiết. Trong Hình 2, chúng tôi báo cáo kết quả bảo toàn độ chính xác FP16 cho các trọng số cụ thể của các cấu trúc con quan trọng, được ký hiệu là "¬ trọng số quan trọng". Như đã thảo luận trước đó, chúng tôi đầu tiên xem xét các phép chiếu xuống của FFN như các trọng số quan trọng. Ngoài ra, chúng tôi cũng xem xét bảo toàn các cấu trúc con quan trọng hơn từ thành phần attention, và chọn hai loại phép chiếu với lỗi lượng tử hóa theo lớp cao nhất trong thành phần attention dựa trên GPTQ. Đặc biệt, chúng tôi chọn các phép chiếu query và key cho mô hình LLaMA-7B, các phép chiếu key và output cho mô hình LLaMA-13B. Kết quả cho thấy những cải thiện nhất quán so với biến thể đơn giản bảo toàn toàn bộ thành phần FFN (được ký hiệu bởi ¬FFN). Mặc dù chúng tôi bảo toàn các cấu trúc con trong cả thành phần attention và FFN, chúng tôi vẫn có dung lượng bộ nhớ giảm so với biến thể ¬FFN (xem đường nét đứt màu xanh). Kết quả khác của GSM8K và WikiText được báo cáo trong Hình 4 trong Phụ lục A.1. Những quan sát này cho thấy ý nghĩa của việc khám phá chiến lược lượng tử hóa chi tiết trong lượng tử hóa 2-bit cực đoan.

4.2 Phân tích Bù đắp Điều chỉnh tinh
4.2.1 Thiết lập Thí nghiệm
Gần đây, có một số nỗ lực sử dụng điều chỉnh tinh để đạt được bù đắp hiệu suất lượng tử hóa (Yao et al., 2023b; Dettmers et al., 2023). Được lấy cảm hứng từ những nghiên cứu này, chúng tôi cũng xem xét kiểm tra tác động của điều chỉnh tinh đối với hiệu suất lượng tử hóa, và thiết lập hai thí nghiệm tương ứng: điều chỉnh tinh trước lượng tử hóa và điều chỉnh tinh dựa trên các trọng số mô hình đã lượng tử hóa. Trong cả hai cài đặt, chúng tôi chủ yếu xem xét lượng tử hóa 2-bit và 4-bit cho trọng số mô hình. Đối với kích thước mô hình, chúng tôi thực hiện điều chỉnh tinh trên các mô hình LLaMA 7B và 13B trong cài đặt đầu tiên. Trong cài đặt thứ hai, chúng tôi tiến hành điều chỉnh tinh trên các mô hình LLaMA đã lượng tử hóa 7B, 13B và 65B. Trong suốt các thí nghiệm của phần này, chúng tôi báo cáo kết quả thu được trên các nhiệm vụ MMLU, GSM8K, và AutoEval. Tiếp theo, chúng tôi chi tiết phương pháp điều chỉnh tinh riêng biệt.

Điều chỉnh tinh Trước Lượng tử hóa Trong thí nghiệm này, chúng tôi xem xét một cài đặt phổ biến trong đó một mô hình đã tối ưu hóa cần được lượng tử hóa để triển khai thực tế. Đối với kiểm tra khả năng ICL, chúng tôi theo Dettmers et al. (2023) và đánh giá tác động của điều chỉnh tinh sử dụng bộ dữ liệu Alpaca (Taori et al., 2023). Đối với kiểm tra khả năng CoT, chúng tôi theo Chung et al. (2022) và sử dụng bộ sưu tập CoT, một hỗn hợp của chín bộ dữ liệu với chú thích CoT được viết bởi các người đánh giá con người. Đối với kiểm tra khả năng IF, chúng tôi theo (Taori et al., 2023) để điều chỉnh tinh các mô hình LLaMA trên bộ dữ liệu Alpaca vì nó được báo cáo có lợi cho các mô hình LLaMA trong việc tuân theo hướng dẫn. Ngoài ra, chúng tôi kết hợp LoRA (Hu et al., 2022) để khám phá tác động của điều chỉnh tinh hiệu quả tham số trên LLMs.

Điều chỉnh tinh Sau Lượng tử hóa Chúng tôi sau đó khám phá lợi ích của điều chỉnh tinh để giải quyết sự suy giảm hiệu suất trong mô hình sau lượng tử hóa. Mục tiêu của chúng tôi là đánh giá mức độ hiệu quả của điều chỉnh tinh trong việc giảm thiểu tác động tiêu cực của lượng tử hóa đến hiệu suất mô hình. Để đạt được điều này, chúng tôi tạo ra một công cụ chuyên dụng cho điều chỉnh tinh hiệu quả tham số của các mô hình LLaMA sau lượng tử hóa trọng số. Công cụ này cho phép chúng tôi điều chỉnh tinh các mô hình LLaMA-65B ở độ chính xác 2-bit chỉ sử dụng một A100 80G duy nhất và vượt trội hơn mô hình LLaMA-13B 16-bit trước điều chỉnh tinh, được đo bằng MMLU (5-shot). Việc tối ưu hóa trực tiếp các trọng số đã lượng tử hóa là thách thức và thường yêu cầu các kỹ thuật tối ưu hóa chuyên dụng như Huấn luyện Nhận thức Lượng tử hóa (QAT) (Liu et al., 2023). Để vượt qua trở ngại này, chúng tôi lấy cảm hứng từ cách tiếp cận LoRA, bao gồm các ma trận phân tách hạng có thể huấn luyện để điều chỉnh tinh. Tuy nhiên, cách tiếp cận LoRA ban đầu được thiết kế cho các trọng số huấn luyện trước cố định và có thể không phù hợp cho các mô hình đã lượng tử hóa.

Để giải quyết vấn đề này, chúng tôi điều chỉnh cách tiếp cận LoRA bằng cách thay thế các trọng số huấn luyện trước của nó bằng các trọng số đã lượng tử hóa được tạo ra bởi GPTQ. Chúng tôi thực hiện việc điều chỉnh này sử dụng các trọng số huấn luyện trước từ các mô hình LLaMA ở nhiều quy mô khác nhau, như 7B, 13B, 30B, và 65B, được lượng tử hóa ở các mức độ chính xác 2-bit, 4-bit, và 8-bit với GPTQ. Bằng cách kết hợp các trọng số đã lượng tử hóa vào framework LoRA, chúng tôi đạt được sự giảm ấn tượng trong tiêu thụ bộ nhớ. Đặc biệt đáng chú ý là việc điều chỉnh tinh mô hình LLaMA-65B, nơi chúng tôi đạt được mức tiêu thụ cực kỳ thấp chỉ 17.8 GiB, làm nổi bật việc sử dụng tham số hiệu quả cao. Mã cho công việc này được triển khai sử dụng GPTQ và LoRA và có sẵn như một dự án mã nguồn mở tại https://github.com/RUCAIBox/QuantizedEmpirical.

4.2.2 Kết quả và Phân tích
Lợi ích của điều chỉnh tinh trước lượng tử hóa gặp phải suy giảm đáng kể ở độ chính xác 2-bit. Chúng tôi tiến hành các thí nghiệm so sánh bao gồm điều chỉnh tinh tham số đầy đủ (FFT) và điều chỉnh tinh hiệu quả tham số với LoRA trên mô hình FP16, tiếp theo là lượng tử hóa với GPTQ. Kết quả được tóm tắt trong Bảng 4. So với mô hình cơ sở, cách tiếp cận FFT mang lại những cải thiện đáng kể trên MMLU, GSM8K, và AutoEval. Khi sử dụng lượng tử hóa 4-bit, chúng tôi quan sát thấy rằng các lợi ích thu được từ FFT được giữ lại, với gần như không có suy giảm hiệu suất trên MMLU và AutoEval. Tuy nhiên, khi sử dụng lượng tử hóa 2-bit cực đoan, các lợi ích từ FFT giảm đáng kể, đặc biệt là trong trường hợp GSM8K (tức là 2.6 cho LLaMA-7B và 2.0 cho LLaMA-13B). Đáng chú ý là khả năng CoT của LLM bị tổn hại đáng kể trong trường hợp này (tức là 0.0 cho cả LLaMA-7B và LLaMA-13B). Điều này cho thấy điều chỉnh tinh trước lượng tử hóa không thể bù đắp hiệu quả sự suy giảm hiệu suất cho các mô hình bit thấp trên các nhiệm vụ phức tạp.

Điều chỉnh tinh hiệu quả tham số vẫn tụt hậu so với điều chỉnh tinh tham số đầy đủ, đặc biệt là trên các nhiệm vụ ICL và CoT. Điều chỉnh tinh hiệu quả tham số đã trở nên phổ biến do khả năng giảm số lượng tham số điều chỉnh tinh trong khi duy trì hiệu suất tốt. Chúng tôi bao gồm kết quả của điều chỉnh tinh LoRA trong cột "LoRA" của Bảng 4. Chúng ta có thể thấy rằng LoRA có thể dẫn đến cải thiện đáng kể so với các mô hình cơ sở trong hầu hết các trường hợp, và sự vượt trội hiệu suất từ điều chỉnh tinh cũng duy trì cho lượng tử hóa 4-bit nhưng không phải lúc nào cũng giữ được cho lượng tử hóa 2-bit. Hơn nữa, LoRA vẫn có khoảng cách đáng kể so với FFT (ví dụ, 25.8 so với 38.0 trên GSM8K). Một phát hiện khác là điều chỉnh tinh LoRA giảm đáng kể trên GSM8K dưới lượng tử hóa 4-bit, cho thấy các mô hình được điều chỉnh tinh tham số đầy đủ có thể phù hợp hơn để xem xét lượng tử hóa trên các nhiệm vụ suy luận phức tạp.

Điều chỉnh tinh sau lượng tử hóa mang lại cải thiện hiệu suất đáng kể đồng thời có thể được thực hiện theo cách nhẹ nhàng. Để điều chỉnh tinh một mô hình đã lượng tử hóa, chúng tôi thực hiện hai sửa đổi chính dựa trên phương pháp LoRA ban đầu. Đầu tiên, chúng tôi sử dụng GPTQ để lượng tử hóa mô hình FP16 thành 2/4 bit. Tiếp theo, chúng tôi thay thế các trọng số huấn luyện trước của phương pháp LoRA bằng các trọng số đã lượng tử hóa. Các bước còn lại giữ nguyên như LoRA ban đầu. Kết quả thí nghiệm được trình bày trong cột "LoRA q" của Bảng 5. Nhìn chung, cách tiếp cận này có thể giảm đáng kể chi phí bộ nhớ trong quá trình điều chỉnh tinh (xem cột "Mem."), cho phép điều chỉnh tinh mô hình 65B trên một NVIDIA A100 duy nhất. Một so sánh kết quả với mô hình cơ sở cho thấy hiệu ứng tăng cường của LoRA q đặc biệt rõ rệt ở 2 bit (ví dụ, 44.4 so với 22.6 cho cài đặt five-shot). Đáng chú ý, dưới ít tổng số bit hơn, hiệu ứng 2-bit của mô hình 65B vượt qua mô hình 13B không điều chỉnh tinh với độ chính xác FP16 trên cài đặt zero-shot (tức là 42.0 so với 41.4). Những phát hiện này chứng minh rằng ngay cả sau lượng tử hóa 2-bit, các mô hình lớn có thể được tăng cường hiệu quả thông qua điều chỉnh tinh.

5 Công việc Liên quan
Trong phần này, chúng tôi thảo luận về công việc liên quan trong hai khía cạnh chính.

Khả năng Emergent Nghiên cứu gần đây đã tiết lộ rằng một số khả năng vượt trội trong Mô hình Ngôn ngữ Lớn (LLMs) có thể không có mặt trong các mô hình nhỏ, khơi dậy sự quan tâm lớn đến khả năng của chúng (Wei et al., 2022). Có nhiều nghiên cứu thảo luận hoặc khám phá tác động của các khả năng emergent đến các nhiệm vụ khác nhau. Ví dụ, ICL cho phép học few-shot mà không cần cập nhật tham số, như được thể hiện bởi GPT-3 (Brown et al., 2020), cho phép tiêm kiến thức nhiệm vụ (Liu et al., 2022) hoặc triển khai LLMs trong mô hình dịch vụ (Sun et al., 2022). CoT chia nhỏ lý luận phức tạp thành các chuỗi tư duy mạch lạc. Các mô hình tận dụng CoT đã cho thấy hiệu suất mạnh mẽ vượt qua con người trên các điểm chuẩn lý luận (Fu et al., 2023; OpenAI, 2023). IF nhằm thực hiện chính xác các hướng dẫn của con người, như được hiển thị trong ChatGPT mạnh mẽ. Khả năng trò chuyện mạnh mẽ và tổng quát hóa cho các nhiệm vụ chưa thấy của chúng chứng minh hiểu biết nhiệm vụ mạnh mẽ (Taori et al., 2023; Chung et al., 2022). Mặc dù các khả năng emergent đã được nghiên cứu rộng rãi, có ít công việc toàn diện tập trung vào đánh giá chúng trên LLMs đã lượng tử hóa. Để lấp đầy khoảng trống này trong nghiên cứu, công việc của chúng tôi nhằm cung cấp phân tích chi tiết về cách các khả năng emergent tồn tại trên LLMs đã lượng tử hóa.

Lượng tử hóa Sau Huấn luyện Lượng tử hóa sau huấn luyện (PTQ) đã được sử dụng rộng rãi để giảm tiêu thụ bộ nhớ và chi phí tính toán trong mạng neural. Một số nghiên cứu đã khám phá việc sử dụng PTQ trên LLMs, bao gồm lượng tử hóa trọng số mô hình (Frantar et al., 2022; Dettmers and Zettlemoyer, 2022) và kích hoạt đặc trưng (Dettmers et al., 2022; Yao et al., 2023b), do khả năng giảm yêu cầu huấn luyện trong khi giảm thiểu tác động hiệu suất. Tuy nhiên, vẫn thiếu các nghiên cứu thực nghiệm toàn diện đánh giá các khả năng emergent của LLMs đã lượng tử hóa. Các nghiên cứu liên quan nhất với công việc này là Yao et al. (2023b) và Dettmers and Zettlemoyer (2022). Cụ thể, Yao et al. (2023b) trình bày phân tích chi tiết của các chiến lược khác nhau trong các phương pháp PTQ trên LLMs, và Yao et al. (2023b) khám phá các quy luật mở rộng suy luận cho hiệu suất zero-shot đối với lượng tử hóa k-bit. Hai nghiên cứu này chủ yếu tập trung vào phân tích khả năng tổng thể, trong khi chúng tôi có quan điểm đặc biệt để nghiên cứu các khả năng emergent trong LLMs đã lượng tử hóa.

6 Kết luận
Trong công việc này, chúng tôi đã tiến hành một nghiên cứu thực nghiệm để kiểm tra tác động của lượng tử hóa sau huấn luyện đến các khả năng emergent của LLMs. Các phát hiện của chúng tôi tiết lộ rằng các mô hình lớn (được điều chỉnh tinh hoặc không) có thể giữ lại tốt các khả năng emergent với lượng tử hóa trọng số 4-bit, nhưng trải qua suy giảm đáng kể ở độ chính xác 2-bit. Hơn nữa, chúng tôi đi sâu vào các thành phần và cấu trúc con chi tiết để nghiên cứu độ nhạy cảm lượng tử hóa. Kết quả của chúng tôi cho thấy LLMs có thể được tăng cường bằng cách bảo toàn hiệu quả các thành phần, chiều đặc trưng và cấu trúc con quan trọng hơn cho lượng tử hóa bit thấp. Ngoài ra, chúng tôi cũng đã kiểm tra tác động của điều chỉnh tinh để cải thiện hiệu suất của các mô hình đã lượng tử hóa. Kết quả thí nghiệm chứng minh rằng điều chỉnh tinh có thể giảm bớt sự suy giảm hiệu suất từ lượng tử hóa bit thấp, cho thấy tiềm năng lớn để tăng cường khả năng của LLMs đã lượng tử hóa.

Tài liệu tham khảo
[Phần này chứa danh sách tài liệu tham khảo dài với các citation học thuật]

[Phần phụ lục chứa các bảng và hình ảnh bổ sung với dữ liệu chi tiết cho các thí nghiệm]
