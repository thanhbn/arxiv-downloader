# 2308.01421.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl-papers/2308.01421.pdf
# Kích thước tệp: 2904427 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
BẢN THẢO
Chính quy hóa, dừng sớm và mơ: một thiết lập
giống Hopfield để giải quyết khái quát hóa và
quá khớp
E. Agliari,a,1F. Alemanno,b,1M. Aquaro,a,1A. Fachechia,1
aTrường Toán học "Guido Castelnuovo", Đại học Sapienza Roma, Italia
bTrường Toán học, Đại học Bologna, Italia
1GNFM-INdAM, Nhóm Quốc gia Vật lý Toán học (Viện Toán học Cao cấp Quốc gia), Italia
E-mail: agliari@mat.uniroma1.it
Tóm tắt: Trong công trình này, chúng tôi tiếp cận mạng nơ-ron hút dẫn từ góc độ học máy:
chúng tôi tìm kiếm các tham số mạng tối ưu bằng cách áp dụng descent gradient trên một hàm mất mát được chính quy hóa.
Trong khung này, các ma trận tương tác nơ-ron tối ưu hóa ra là một lớp ma trận tương ứng
với các kernel Hebbian được xem xét lại bởi một giao thức bỏ học lặp lại. Đáng chú ý, mức độ
của việc bỏ học này được chứng minh có liên quan đến siêu tham số chính quy hóa của hàm mất mát
và thời gian huấn luyện. Do đó, chúng tôi có thể thiết kế các chiến lược để tránh quá khớp được diễn đạt
dưới dạng điều chỉnh chính quy hóa và dừng sớm. Khả năng khái quát hóa của các mạng hút dẫn này
cũng được khảo sát: kết quả phân tích được thu được cho các tập dữ liệu tổng hợp ngẫu nhiên, tiếp theo,
bức tranh nổi lên được củng cố bởi các thí nghiệm số làm nổi bật sự tồn tại của một số
chế độ (tức là, quá khớp, thất bại và thành công) khi các tham số tập dữ liệu thay đổi.
Từ khóa: Mạng nơ-ron hút dẫn, quá khớp, spin-glassesarXiv:2308.01421v2  [cs.LG]  20 Feb 2024

--- TRANG 2 ---
Mục lục
1 Giới thiệu 1
2 Từ điều kiện ổn định đến bài toán tối thiểu hóa 3
3 Mơ như chính quy hóa, chính quy hóa như dừng sớm 5
4 Sự xuất hiện của khái quát hóa và quá khớp trong các mô hình giống Hopfield 9
4.1 Một tập dữ liệu tổng hợp 9
4.2 Các trạng thái giả của dữ liệu huấn luyện tạo điều kiện cho khái quát hóa 10
5 Thí nghiệm số 12
5.1 Sơ đồ khái quát hóa 12
5.2 Một phép tương tự với thuật toán phân cụm 16
6 Kết luận 17
7 Lời cảm ơn 19
A Sự hội tụ của quy trình descent gradient 19
B Phương pháp 20
B.1 Thiết kế huấn luyện và gán tham số 20
B.2 Công thức của các chỉ số hiệu suất 21
B.3 Dấu hiệu số của quá khớp 22
C Một cách nhanh hơn để tính thời gian dừng sớm 23
D Chi tiết về các trạng thái giả 24
1 Giới thiệu
Mô hình Hopfield có lẽ là ví dụ nổi tiếng nhất về mạng nơ-ron hút dẫn [1–3]: đây được
tạo thành bởi một tập hợp N đơn vị nhị phân, được gọi là nơ-ron, tương tác theo cặp và có trạng thái
được cập nhật lặp đi lặp lại bởi một hàm kích hoạt phi tuyến, sao cho trạng thái mới của nơ-ron
i phụ thuộc vào tín hiệu tác động lên i và bắt nguồn từ các nơ-ron lân cận. Một lựa chọn thích hợp
của ma trận tương tác nơ-ron, ký hiệu là J∈RN×N, có thể đảm bảo tính hút dẫn của một số
mẫu, mà chúng ta muốn lưu trữ và truy xuất. Chính xác hơn, người ta khởi tạo cấu hình nơ-ron
bằng cách đặt các giá trị của các đơn vị gần với một mẫu ξ1∈ {− 1,+1}N, cấu hình này đại diện cho
đầu vào được cung cấp cho máy và có thể tương ứng với một phiên bản bị hỏng của ξ1; các cập nhật lặp lại của
nơ-ron sau đó được thực hiện cho đến khi hội tụ đến một điểm cố định và, nếu điều này khớp với ξ1, trạng thái này được
diễn giải như việc truy xuất thông tin được mã hóa bởi ξ1. Điều tương tự được mong đợi xảy ra với bất kỳ
mẫu đích nào, chẳng hạn ξµ, với µ= 1, ..., P.

--- TRANG 3 ---
Sự phổ biến của mô hình Hopfield cũng do thực tế là nó có thể được xử lý phân tích
và, đặc biệt, nó có thể được công nhận như một spin-glass, sao cho nó có thể hưởng lợi
từ một bộ sưu tập rộng lớn các kỹ thuật được phát triển để giải quyết các hệ thống rối loạn. Thật vậy, trong những
thập kỷ qua, mô hình đã được khảo sát chuyên sâu, và vô số biến thể về chủ đề này cũng đã được
tính đến1. Đáng chú ý, một phần đáng kể của các công trình này đã làm nổi bật cấu trúc của
ma trận tương tác nơ-ron: trong mô hình Hopfield tiêu chuẩn, điều này dựa trên quy tắc Hebb được gọi là
[19]J=ξ·ξT
N và các sửa đổi thích hợp của quy tắc này có thể dẫn đến hiệu suất tốt hơn của mô hình trong
điều kiện số lượng các mẫu có thể lưu trữ và truy xuất [20]. Một lớp thành công của những sửa đổi này áp dụng
các giao thức bỏ học (xem ví dụ, [21–30]), mục đích của chúng là làm suy yếu tính hút dẫn của các cấu hình
không tương ứng với bất kỳ mẫu nào được lưu trữ (sự hội tụ đến những cấu hình đó, đôi khi
được gọi là các trạng thái giả, được diễn giải như một sai lầm của máy). Gần đây hơn, quy tắc Hebb
cũng đã được sửa đổi để làm cho nó gần gũi hơn với một thuật toán học: "thực tế" mà chúng ta muốn truy xuất
bây giờ không thể tiếp cận được, thay vào đó, một mẫu bị hỏng, chẳng hạn {ξµ,1,ξµ,2, ...} có sẵn và được sử dụng để xây dựng J
(xem ví dụ, [31–34]). Bằng cách này, mẫu dữ liệu có sẵn có thể được diễn giải như một tập huấn luyện và
mô hình Hopfield có thể được sử dụng cho các nhiệm vụ khái quát hóa. Cầu nối giữa một kịch bản truy xuất và
một thiết lập học máy cũng đã được tăng cường bằng cách tận dụng sự tương đương giữa mô hình
Hopfield và máy Boltzmann (xem ví dụ, [35–40]). Tuy nhiên, một ánh xạ đầy đủ cho phép vai trò
của các tham số chính quy hóa, sự xuất hiện của hiện tượng quá khớp hoặc thiếu khớp vẫn đang được
xây dựng (xem ví dụ, [41–46]).
Trong bài báo này, chúng tôi cố gắng đóng góp vào việc lấp đầy khoảng trống này, tập trung vào một bài toán tái tạo
không giám sát: tập dữ liệu được tạo thành từ một tập hợp các mục thuộc về các lớp khác nhau (nhãn bị che giấu) và
chúng tôi giới thiệu một hàm mất mát cho ma trận tương tác nơ-ron. Nghiệm của bài toán chúng tôi tương ứng
với một kernel Hebbian chịu một lượng td nhất định của các lần lặp bỏ học và chúng tôi chứng minh rằng td
liên quan đến siêu tham số chính quy hóa, mà lần lượt, có thể liên quan đến thời gian huấn luyện
trong phiên bản không chính quy hóa của bài toán. Khung này cho phép chúng tôi kiểm tra sự xuất hiện của
hiện tượng quá khớp và do đó để hình thành các công thức cho một thời gian huấn luyện tối ưu. Cụ thể,
hệ thống lưu trữ từng mẫu như một điểm tối thiểu của hàm Lyapunov liên kết với động lực học nơ-ron
(điều này có thể được diễn giải như một hàm chi phí hoặc như một hàm năng lượng); các điểm tối thiểu tương ứng với cùng
lớp tạo thành một cụm, và – khi số lượng ví dụ mỗi lớp đủ lớn – những điểm tối thiểu này
thực sự kết hợp thành một điểm tối thiểu duy nhất. Trong kịch bản này, xuất hiện cả tương quan trong lớp và giữa các lớp
và chúng tôi thấy rằng vai trò của td (hoặc, tương đương, của chính quy hóa hoặc của thời gian huấn luyện)
là phân tách những tương quan như vậy bắt đầu từ những tương quan thấp nhất: khi td tăng, các điểm tối thiểu
tương ứng với các lớp khác nhau được dịch chuyển, sự chồng lấp của chúng giảm và hệ thống có khả năng
khái quát hóa từ các ví dụ; bằng cách tiếp tục tăng td, các điểm tối thiểu tương ứng với các mẫu thuộc về
cùng các lớp cũng bị dịch chuyển và hệ thống bắt đầu quá khớp.
Trong phần sau, chúng tôi chi tiết những kết quả này bằng cách đầu tiên giới thiệu hàm mất mát liên kết với
bài toán của chúng tôi và chỉ ra rằng ma trận tương tác nơ-ron tối thiểu hóa hàm mất mát tương ứng
với kernel Hebbian được xem xét lại được nghiên cứu trong [27, 47] (Mục 2). Tiếp theo, chúng tôi tìm một mối quan hệ giữa chính quy hóa, thời gian huấn luyện và thời gian bỏ học, và chúng tôi trình bày các thí nghiệm số về các tập dữ liệu
không có cấu trúc và có cấu trúc (Mục 3). Những kết quả này tạo thành tiền đề cho một cuộc thảo luận kỹ lưỡng
về sự xuất hiện của hiện tượng quá khớp (Mục 4) và một sự củng cố bằng số học (Mục 5). Cuối cùng,
1Những điều này bao gồm, chẳng hạn, sự hiện diện của độ lệch hoặc khuyết trong các mục mẫu (xem ví dụ, [4–6]), các tổng quát hóa của
kernel Hebbian để bao gồm tương quan thời gian giữa các mẫu (xem ví dụ, [7–9]) hoặc nhiễu synapse (xem ví dụ, [10–12]),
kiến trúc cơ bản không tầm thường (xem ví dụ, [13–15]), hoặc tương tác bậc cao (xem ví dụ, [16–18]). Mặc dù những
khía cạnh này không liên quan trực tiếp đến công việc hiện tại, đáng chú ý là sự quan tâm rộng rãi, lâu dài được thu hút bởi
mô hình, hơn nữa, cách tiếp cận hiện tại có thể được mở rộng để bao gồm những tính năng như vậy.

--- TRANG 4 ---
chúng tôi kết thúc bài báo của mình bằng cách đưa ra một cái nhìn tổng quan ngắn gọn và những nhận xét cuối cùng (Mục 6). Các chi tiết kỹ thuật được
thu thập trong các Phụ lục.
2 Từ điều kiện ổn định đến bài toán tối thiểu hóa
Nguyên lý cơ bản cơ bản của một mạng nơ-ron hút dẫn là mỗi mẫu tạo nên tập hợp
ξ={ξµ}P
µ=1 và mã hóa thông tin có liên quan được liên kết với một điểm cố định hút dẫn cho
động lực học mạng. Chúng tôi giả định rằng các mẫu là các vectơ nhị phân N-chiều và các đơn vị mạng
σ= (σ1, . . . , σ N)∈ {− 1,+1}N tương tác theo cặp như được chỉ định bởi ma trận liên kết đối xứng2
J, sau đó, chúng tôi thiết lập sự tiến hóa mạng như
σ(t+ 1) = sign( φ(σ(t)), (2.1)
trong đó φi(σ(t))≡P
j̸=iJijσj(t) đại diện cho tín hiệu đến đơn vị i tại thời điểm t và hàm sign
hoạt động theo từng thành phần. Động lực học này được áp dụng tuần tự và thể hiện hàm Lyapunov
sau (xem ví dụ, [48])
E(σ) =−1
2X
i,jσiJijσj. (2.2)
Trong công việc này, chúng tôi sẽ giữ lại một sự tiến hóa xác định3 và, hiện tại, chúng tôi lấy J như được thiêu kết
và chúng tôi loại trừ sự hiện diện của độ lệch. Trong khung này, điều kiện ổn định cho một cấu hình truy xuất,
ví dụ, σ(t) =ξµ mà không mất tính tổng quát, đọc như
ξµ
iφi(ξµ)≥0,∀i= 1, . . . , N, (2.3)
sao cho, nếu tại thời điểm t hệ thống được chuẩn bị hoặc xảy ra chính xác trong trạng thái đó, nó
sẽ bị mắc kẹt ở đó cho tất cả t′≥t. Tất nhiên, việc hoàn thành (2.3) ngụ ý rằng J phải là một
hàm thích hợp của ξ. Tổng quát hơn, người ta quan tâm đến việc đánh giá sự hội tụ đến một cấu hình truy xuất
ngay cả khi chất lượng đầu vào tương đối thấp, cụ thể, ngay cả khi cấu hình ban đầu
tương đối xa (theo nghĩa Hamming) từ mẫu đích. Trong trường hợp này, người ta yêu cầu một
điều kiện mạnh hơn, đó là
ξµ
iφi(ξµ)≥κ >0,∀i= 1, . . . , N, (2.4)
có nghĩa là, mặc dù σ(t) hiển thị một số khác biệt so với ξµ, động lực học (2.1) vẫn
được đảm bảo hội tụ đến ξµ. Trong bất đẳng thức này (tương ứng với yêu cầu cơ bản trong lý thuyết
Gardner [49–51]), κ kiểm soát độ rộng của các lưu vực hút dẫn, đó là, nếu σ thuộc về một quả cầu Hamming
B(ξµ, R(κ)) có tâm trong ξµ với bán kính R(κ), phản ứng mạng sẽ là f(σ) =ξµ, với f là
hàm truyền f(σ) = lim n→∞Tn(σ) và T(σ) = sgn( φi(σ)) động lực học 1-bước. Tăng
κ, tiêu chí ổn định sẽ được thỏa mãn trong một quả cầu với bán kính R(κ) ngày càng lớn bao quanh
các mẫu, nhưng điều này có giá là một lượng nhỏ hơn các vectơ thông tin có thể lưu trữ, dẫn đến một
khả năng lưu trữ quan trọng thấp hơn. Đặc biệt, đối với các mạng đối xứng ( J=JT) số lượng lớn nhất
các mẫu có thể được truy xuất là N[49]. Để thỏa mãn ràng buộc bất đẳng thức (2.4), chúng ta có thể
áp đặt một điều kiện đẳng thức (mạnh hơn) yêu cầu rằng, cho γ≥κ,
ξµ
iX
j̸=iJijξµ
j=γ,∀i= 1, . . . , N, (2.5)
với γ giống nhau cho tất cả các mẫu. Điểm sau có thể xuất hiện như một giả định khá mạnh,
nhưng – ít nhất trong lý thuyết ngẫu nhiên, nơi các mẫu đều tương đương, vì các mục của chúng là i.i.d. –
nó là hợp lý. Yêu cầu (2.5) có những hệ quả kỹ thuật quan trọng. Đầu tiên, nếu các mẫu
là Boolean, nó có thể được viết lại dưới dạng minh bạch hơn làP
j̸=iJijξµ
j=γξµ
i, đó không gì khác
ngoài tiêu chí ổn định của Personnaz et al [22, 51]. Hơn nữa, chúng ta có thể loại bỏ ràng buộc về
sự vắng mặt của tự tương tác và cho phép j=i trong tổng cuối4, do đó viết lại biểu thức trước đó
như một bài toán trị riêng như J·ξµ=γξµ; điều này có nghĩa là ma trận liên kết được thiết kế sao cho
các mẫu là vectơ riêng với trị riêng suy biến γ.
Trong trường hợp tổng quát, chúng ta có thể thêm một trường bên ngoài bằng cách thay thế trường nội bộ cục bộ trong (2.4) với
trường tổng, tức là, φi(σ)→P
jJijσj+hi. Các lập luận trước đó vẫn giữ và, cho µ= 1, . . . , P,
bài toán của chúng ta có dạng(
J·ξµ+h=γξµ
J=JT. (2.6)
Do đó, trong bối cảnh này, huấn luyện một mạng ngụ ý tìm một sắp xếp cho J và h, sao cho
(2.6) giữ và điều này có thể được viết lại thành việc tối thiểu hóa Sai số Bình phương Trung bình (MSE) của dạng
(J·ξµ+h−γξµ)2, sao cho chúng ta có thể thiết lập quy trình tối thiểu hóa cho một hàm mất mát
đọc như5
Lξ(J,h) =1
2PX
i,µX
jJijξµ
j+hi−γξµ
i2
+1
2PX
j,µX
iJijξµ
i+hj−γξµ
j2
+ϵJX
i,jJ2
ij+ϵhX
ih2
i,
(2.7)
với ϵJ, ϵh∈[0,+∞]. Thuật ngữ thứ hai trong r.h.s. được thu được bắt đầu từ thuật ngữ đầu tiên và
đảo ngược vai trò của i và j; hai đóng góp này tính đến tiêu chí ổn định và
ràng buộc đối xứng6 theo cách không cứng nhắc. Hai đóng góp tiếp theo là các thuật ngữ chính quy hóa L2
cho, tương ứng, J và h, bảo vệ chuẩn của những tham số này khỏi phân kỳ trong
huấn luyện khi chúng bị giới hạn bởi các tiềm năng bậc hai.
Dạng tường minh của nghiệm của hệ ràng buộc (2.6) có thể đạt được thông qua phương pháp
descent gradient ˙J=−∇JLξ và ˙h=−∇hLξ, mang lại
−˙J=JΩ+ΩJ+h¯ξT+¯ξhT−2γΩ+ 2ϵJJ, (2.8)
−1
2˙h=J+JT
2¯ξ+ (1 + ϵh)h−γ¯ξ, (2.9)
trong đó
Ωij≡1
PX
µξµ
iξµ
j,¯ξi≡1
PX
µξµ
i,
2Ràng buộc đối xứng đối với ma trận liên kết J được giới thiệu để nhất quán với hình ảnh cơ học thống kê và để đảm bảo rằng động lực học tuần tự (2.1) thể hiện các điểm cố định.
3Các thực hiện ngẫu nhiên của động lực học (2.1) hoạt động theo những cách tương tự, ngoại trừ thực tế là hệ thống không được cố định
trong cấu hình chính xác được đưa ra bởi mẫu thứ µ, nhưng nó có thể tự do khám phá (ở một mức độ nào đó được đưa ra bởi mức độ của
tính ngẫu nhiên) lưu vực hút dẫn liên kết. Điều này đúng nghiêm ngặt nếu hai điều kiện giữ: i.cấu hình ban đầu
gần (theo nghĩa Hamming) với mẫu đích và ii.trong giới hạn nhiệt động lực học, nơi phá vỡ ergodicity
xảy ra (trong trường hợp kích thước hữu hạn, sự chuyển đổi giữa các hút dẫn khác nhau là nhỏ theo cấp số mũ trong kích thước hệ thống nhưng
không biến mất).
4Điều này có những ảnh hưởng định lượng đến ước tính khả năng lưu trữ quan trọng, có thể được bỏ qua ở đây vì trọng tâm
là khá về khả năng khái quát hóa trong một kịch bản không giám sát, xem ví dụ [22].
5Trường hợp không có tự tương tác có thể được khôi phục bằng cách thêm một đóng gópP
iθiJii trong các hàm mất mát, trong đó
θi là các nhân tử Lagrange đảm bảo rằng các mục đường chéo được đặt thành không.
6Chúng tôi nhớ lại rằng ràng buộc đối xứng cho ma trận liên kết J được giới thiệu để nhất quán với hình ảnh cơ học thống kê và để đảm bảo rằng động lực học tuần tự (2.1) thể hiện các điểm cố định.

--- TRANG 5 ---
lần lượt là kernel Hebbian7 và giá trị trung bình của mục thứ i trên các mẫu. Lưu ý
rằng các ma trận J và JT thỏa mãn cùng phương trình vi phân (2.8) vì Ω đối xứng, do đó,
nếu chúng ta xem xét các điều kiện ban đầu sao cho J(0) = J(0)T, tính đối xứng được bảo tồn cho bất kỳ t >0 bởi
các lập luận duy nhất. Do đó, chúng ta có thể an toàn thay thế J thành phần đối xứng của nó trong (2.9). Điều kiện hội tụ
của dạng rời rạc của những phương trình động lực học này được thảo luận trong Phụ lục A.
Trước khi kết thúc Mục này, đáng chú ý rằng, như tiêu chuẩn, thư giãn nơ-ron
(2.1) và động lực học huấn luyện (2.8)-(2.9) hoạt động trên các thang thời gian khác nhau, cái trước nhanh hơn nhiều. Về mặt sinh học, điều này được thúc đẩy bởi thực tế là tính dẻo synapse chậm hơn nhiều so với kích hoạt nơ-ron và, trong các mạng nơ-ron nhân tạo, bởi thực tế là máy được huấn luyện trước và sau đó được sử dụng để hoàn thành nhiệm vụ. Để nhất quán với (2.1) do đó người ta nên viết ˙J=dJ
dtτ−1
J và
˙h=dh
dtτ−1
h, với τJ,h≫1. Tuy nhiên, dưới giả thuyết đoạn nhiệt này, chúng ta có thể xem xét các tham số mạng J,h như được trồng trong động lực học nơ-ron và tách hai bài toán động lực học sao cho, khi tập trung vào sự tiến hóa synapse, τJ,h có thể được đặt như đơn vị mà không có sự mơ hồ.
3 Mơ như chính quy hóa, chính quy hóa như dừng sớm
Điểm tối thiểu toàn cục cho Lξ(J,h) trong (2.7) có thể được thu được bằng cách yêu cầu các điều kiện ổn định ˙J=0
và ˙h=0 trong Eqs. (2.8)-(2.9) mang lại
JΩ+ΩJ+h¯ξT+¯ξhT−2γΩ+ 2ϵJJ= 0, (3.1)
J¯ξ+ (1 + ϵh)h−γ¯ξ= 0, (3.2)
có nghiệm đọc như
h=1
1 +ϵh(γ1−J)¯ξ, (3.3)
J=1
Pˆξγ
C+ϵJ1ˆξT, (3.4)
trong đó
ˆξµ
i≡ξµ
i−
1−rϵh
1 +ϵh
¯ξi, C µν≡1
PX
iˆξµ
iˆξν
i.
Bằng cách kiểm tra Eq. (3.3) người ta có thể thấy rằng trường bên ngoài bắt nguồn từ sự hiện diện của độ lệch trong
dữ liệu đầu vào, tức là ¯ξi̸= 0, sao cho việc tái tâm hóa các mẫu bởi ξµ
i→ξµ
i−¯ξµ
i dẫn đến
h=0. Do đó, miễn là dữ liệu được tiền xử lý theo cách này, các trường bên ngoài không cần thiết. Hơn nữa, bởi
nhìn vào Eq. (3.4), người ta có thể thấy rằng nghiệm của chúng ta khôi phục ma trận tương tác của "Mô hình
Hopfield Mơ" (DHM) [27, 39, 47]
J(D)≡1
Pξtd
I+CtdξT, (3.5)
khi đặt γ= 1 và xác định td như nghịch đảo của siêu tham số ϵJ8, đó là, td=ϵ−1
J. Chính xác hơn, trong DHM, kernel đọc như ˜J(D)≡1
Nξtd+1
I+CtdξT và nó được thu được từ
7Đối với ký hiệu tiêu chuẩn ở đây hệ số tiền 1 /N được thay thế bằng 1 /P.
8Việc tối thiểu hóa MSE được chính quy hóa như được định nghĩa trong (2.7) là một thiết lập đặc biệt của thiết lập thông thường của lý thuyết hồi quy ridge [52, 53], với phản ứng đích của mạng là phép nhân của các vectơ đầu vào ξµ bởi hằng số γ.
Bỏ qua vectơ độ lệch, ước lượng ridge tối thiểu hóa hàm mất mát thực sự trùng với ma trận liên kết J(D),
trong đó td đóng vai trò của tham số chính quy hóa Tichonov [54]. Hồi quy ridge, cùng với sự tổng quát hóa của chúng
đến các bài toán hồi quy phi tuyến với các kỹ thuật kernel [55–57], là một chủ đề trung tâm trong lý thuyết học thống kê, tập trung
đặc biệt vào vai trò của siêu tham số tương ứng (xem ví dụ [58–60]) cũng như các hiện tượng chính quy hóa ngầm
nổi lên trong thống kê chiều cao [61, 62].

--- TRANG 6 ---
kernel Hebbian tiêu chuẩn bằng cách áp dụng lặp đi lặp lại một giao thức bỏ học dựa trên sự tương tác của các cơ chế loại bỏ
và củng cố được lấy cảm hứng từ những cơ chế xảy ra trong giấc ngủ ở não động vật có vú [21, 63];
vì sự tương tự này, thời gian td, đo số lần lặp bỏ học, được gọi là
"thời gian mơ". Khả năng lưu trữ quan trọng của DHM đã được chỉ ra tăng đơn điệu với
td, đạt, trong giới hạn td→ ∞, cận trên lý thuyết được biết đến cho các mạng đối xứng và
tương ứng với số lượng mẫu có thể truy xuất bằng số lượng nơ-ron, tức là, P=N. Ngoài ra,
DHM đã được chứng minh vượt trội hơn mô hình Hopfield tiêu chuẩn về khả năng khái quát hóa
[64]. Sự khác biệt giữa J(D) và ˜J(D) chỉ nằm ở một yếu tố tiền P/N và trong một dịch chuyển td→td+ 1
mà, miễn là chúng hữu hạn và không biến mất, chỉ mang lại một hiệu chỉnh định lượng.9
Hơn nữa, cho td→ ∞ (hoặc, tương đương, cho ϵJ→0), chúng ta khôi phục ma trận máy chiếu của Kohonen [65]
J(P)≡1
PξC−1ξT.
Bây giờ chúng ta tiến lên và lưu ý rằng nghiệm J(D) thu được bởi một quá trình
L2-chính quy hóa được huấn luyện đầy đủ ( t→ ∞ ) ( ϵJ̸= 0) có thể liên quan đến nghiệm của một quá trình không chính quy hóa ( ϵJ= 0)
được chạy đến một thời gian hữu hạn t∗; như chúng ta sẽ thấy, mối quan hệ này cho phép chúng ta ánh xạ thời gian mơ
td vào thời gian huấn luyện và do đó diễn giải cơ chế mơ như một huấn luyện. Để
thiết lập mối quan hệ này, chúng tôi tiếp tục bài toán động lực học với các mẫu được tái tâm hóa, sao cho
trường được suy ra biến mất10 và Eq. (2.8) đơn giản đọc như
−˙J=J(Ω+ϵJ1) + (Ω+ϵJ1)J−2γΩ. (3.6)
Điều này có thể được viết lại trong cơ sở của các vectơ riêng của Ω (ký hiệu với a, b= 1, . . . , N các chỉ số tương ứng) như
−˙Jab= (λa+λb)Jab+ 2ϵJJab−2γλaδab, (3.7)
trong đó Jab là phần tử của J trong cơ sở hiện tại và σ(Ω) ={λa}N
a=1 là phổ Ω. Bằng cách giải
Eq. (3.7) chúng ta thấy rằng các thuật ngữ không chéo tiệm cận đến không như Jab∼exp[−t(λa+λb+2ϵJ)]
cho bất kỳ điều kiện ban đầu nào, sao cho, tại điểm cân bằng, ma trận liên kết là chéo.
Ở đây, chúng tôi chọn chuẩn bị hệ thống trong một cấu hình nơi không có thông tin nào được lưu trữ, tức là, một thiết lập
tabula rasa J(t= 0) = 0, bằng cách này các mục không chéo vẫn bị mắc kẹt ở không tại bất kỳ thời điểm
t. Đáng chú ý, cấu trúc chéo của J(t), khi được thể hiện trong cơ sở của các vectơ riêng của Ω,
ngụ ý rằng hai ma trận chia sẻ cùng các vectơ riêng. Đối với các mục trên đường chéo chính,
9Đặc biệt, hai mô hình giữ sự tương đương cơ học thống kê khi hàm phân vùng của mô hình hiện tại
Zβ(J(D)) :=P
{σ}exp
−βP
i,jJ(D)
ijσiσj
có thể được chuyển thành của DHM gốc bằng cách tái tỷ lệ β→P
N1+td
tdβ,
trong đó β điều chỉnh mức độ ngẫu nhiên trong hệ thống, đó là, nó điều chỉnh độ rộng của phân phối của các cấu hình nơ-ron hoặc, trong một biệt ngữ vật lý, nó đóng vai trò như nhiệt độ nghịch đảo.
10Tương đương, chúng ta có thể chọn làm việc mà không tái tỷ lệ các mẫu, do đó cũng bao gồm các trường bên ngoài. Trong
trường hợp này, chúng ta có thể đơn giản hóa việc phân tích bài toán động lực học bằng cách yêu cầu rằng τJ˙J=−∇JL và τh˙h=−∇hL,
và xem xét trường hợp τh≪τJ. Dưới giả định này, sự biến đổi của các trường bên ngoài nhanh hơn nhiều so với
sự tiến hóa điển hình của ma trận liên kết, vì vậy – khi xử lý hành vi thời gian của cái sau – các trường thực sự thư giãn
tức thì về phía điểm cố định của chúng tại J(t) cố định:
h∞[J(t)] =1
1 +ϵh(γ1−J(t))¯ξ.
Kết quả là, động lực học synapse được mô tả bởi phương trình
−˙J=J(ˆΩ+ϵJ1) + ( ˆΩ+ϵJ1)J−2γˆΩ,
với ˆΩij= Ωij−Mij=P−1P
µˆξµ
iˆξµ
j và Mij= (1 + ϵh)−1¯ξi¯ξj. Khi xử lý các mẫu có cấu trúc, chúng tôi sẽ
bảo tồn trường được suy ra để không làm thay đổi diện mạo đồ họa của dữ liệu; điều này tất nhiên hoàn toàn không liên quan
khi xử lý một tập dữ liệu có trung bình bằng không.

--- TRANG 7 ---
cho điều kiện ban đầu được đề cập ở trên, nghiệm của phương trình vi phân liên kết là
Jaa(t) =γλa
ϵJ+λan
1−exp
−2t(λa+ϵJ)o
. (3.8)
Như mong đợi, trong giới hạn thời gian huấn luyện lớn, chúng ta khôi phục J(D), đó là
Jaa(t) =
t→∞γλa
λa+ϵJ=J(D)
aa, (3.9)
trong khi, mở rộng tại t nhỏ, chúng ta có
Jaa(t) =γλa
ϵJ+λan
1−exp
−2t(λa+ϵJ)o
≈
t≪12tγλa (3.10)
tương ứng với J≈2tγΩ: điều này có nghĩa là, mặc dù điều kiện ban đầu trống, tại khởi đầu
của huấn luyện, kernel J gần với một cấu trúc Hebbian.
Mặt khác, bằng cách đặt ϵJ= 0 trong Eq. (3.8), chúng ta thấy rằng các thuật ngữ chéo tiến hóa như
Jaa(t) =γ[1−exp(−2λat)]. (3.11)
Bây giờ, chúng ta so sánh hai dạng tường minh của ma trận liên kết, tức là cái được chính quy hóa tại t→ ∞
(3.9) và cái phụ thuộc t với ϵJ= 0 (3.11), và tìm kiếm thời gian đặc trưng t∗ tại đó
cái sau gần nhất có thể với cái trước. Để làm điều này, chúng ta hãy xem xét lượng
δ(λ, t, ϵ J) =γ2hλ
λ+ϵJ−1 + exp( −2tλ)i2
,
đo sự khác biệt bình phương giữa các thành phần trong hai thực hiện tại trị riêng λ cố định. Sau đó, chúng ta lấy trung bình trên phổ Ω, tức là
¯δ(t, ϵJ) =Z
dλ δ(λ, t, ϵ J)ρE(λ),
trong đó ρE(λ) =1
NP
λa∈σ(Ω)δ(λ−λa) là phân phối phổ thực nghiệm của Ω. Lưu ý rằng ¯δ
không gì khác ngoài khoảng cách Frobenius bình phương giữa kernel Mơ và ma trận liên kết không chính quy hóa. Lượng này được tối thiểu hóa cho
t∗(ϵJ) = argmin
t¯δ(t, ϵJ). (3.12)
Mối quan hệ này cung cấp một biểu thức cho thời gian t∗ tại đó descent gradient không chính quy hóa trên
Lξ(J,h) nên được gián đoạn nếu chúng ta muốn một ma trận liên kết gần với J(D) tương ứng với
descent gradient được chính quy hóa, thư giãn đầy đủ. Sự tương đương giữa hai kịch bản được xác nhận cho
các tập dữ liệu tổng hợp, MNIST [66] và Fashion-MNIST [67] như được báo cáo trong Hình 1.
Mối quan hệ chức năng giữa td=ϵ−1
J và t∗ được làm nổi bật trong Eq. (3.12) được mô tả trong Hình 2. Hành vi
logarithmic được biện minh phân tích trong Phụ lục C, nơi, bằng cách mở rộng (3.12) xung quanh trị riêng trung bình, chúng tôi thu được một xấp xỉ bậc nhất của thời gian dừng sớm t∗(ϵJ) phụ thuộc vào
td và vào vết của Ω.
Một cách khác để thấy sự tương đương giữa chính quy hóa và dừng sớm là như sau. Bắt đầu
từ Eq. (3.8), chúng ta nhận thấy rằng ϵJ cung cấp một thang đo tự nhiên như
J(D)
aa≈

γ
1−ϵJ
λa
nếu λa≫ϵJ
γλa
ϵJ nếu λa≪ϵJ.

--- TRANG 8 ---
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.00.10.20.30.4¯d(ξ,σ(∞))NGẪU NHIÊN
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.000.050.100.150.20MNIST
P/N = 0.2
P/N = 0.3
P/N = 0.5
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.000.050.100.150.20FASHION-MNIST
Mơ
GD-Dừng sớmMơ
GD-Dừng sớmHình 1 :Hiệu suất truy xuất của kernel mơ so với dừng sớm. Ba bảng
hiển thị so sánh giữa nghiệm được huấn luyện đầy đủ (3.4) với ϵJ̸= 0 và nghiệm của quy trình huấn luyện dừng sớm với ϵJ= 0; đối với cái sau, thời gian huấn luyện cuối cùng được chọn theo
(3.12). Trong bảng bên trái nhất, tập dữ liệu ξ được tạo thành từ P vectơ Rademacher tự nhiên
hiển thị trung bình bằng không, trong khi ở bảng trung tâm và bên phải nhất, tập dữ liệu ξ được tạo thành từ P mục
được rút ngẫu nhiên từ, tương ứng, các tập dữ liệu MNIST và Fashion-MNIST, và những vectơ này
được tiền xử lý bằng phương pháp Otsu [68] để làm chúng nhị phân. Các mục trong những tập dữ liệu này được sử dụng
để xây dựng các ma trận tương tác J(D) và J(t∗). Đối với tập dữ liệu ngẫu nhiên N= 200 và γ= 1,
trong khi đối với MNIST và Fashion-MNIST N= 784 và γ= 1, ngoài ra, các giá trị khác nhau của tỷ lệ
P/N được xem xét như được báo cáo trong chú giải chung. Hiệu suất của hệ thống được đo
bằng khoảng cách Hamming chuẩn hóa d(ξµ,σ(∞)) giữa mẫu đích ξµ và
cấu hình cuối cùng σ(∞), thu được bằng cách khởi tạo hệ thống trong một phiên bản bị hỏng của ξµ (thu được
bằng cách lật các mục mẫu với xác suất q= 0.1) và lặp (2.1) đến hội tụ. Bằng cách lấy trung bình trên tất cả P mẫu chúng ta thu được ¯d(ξ,σ(∞)) =1
PP
µd(ξµ,σ(∞)), được vẽ so với
thời gian mơ. Chúng tôi tham khảo Phụ lục B để biết thêm chi tiết về số học.
Do đó, trong cách tiếp cận chính quy hóa, tham số ϵJ ngăn cản sự bão hòa của tất cả các mục chéo
của J(D) đến giá trị γ (tương ứng với máy chiếu Kohonen J(P)[22, 51, 65, 69]); trên thực tế, miễn là ϵJ>0 (hoặc td hữu hạn), chỉ các mục tương ứng với các trị riêng hàng đầu của Ω gần với giá trị giới hạn này, trong khi những cái khác vẫn gần với điều kiện ban đầu (tức là, Jaa= 0 cho bất kỳ a= 1, ..., N ).
Mặt khác, dừng sớm tính đến động lực học cho việc lọc như vậy, vì thời gian t∗ tại đó
chúng ta dừng huấn luyện được chọn sao cho chỉ một tập con của các mục chéo bão hòa đến điểm cố định
J(D)
aa=γ của hệ thống động lực học (3.6), trong khi tất cả những cái khác không được thay đổi một cách đáng kể
w.r.t. điều kiện ban đầu; trên thực tế, như được làm nổi bật bởi (3.11), thời gian đặc trưng cho bão hòa là
phụ thuộc mục và được đưa ra bởi (2 λa)−1, do đó các mục tương ứng với các trị riêng lớn của Ω nhanh hơn.
Những gì chúng tôi đã trình bày cho đến nay áp dụng cho một tập hợp chung các vectơ Boolean, vì ma trận liên kết
J tự nhiên phát sinh như điểm cố định của một thuật toán descent gradient, giả thuyết duy nhất mà chúng tôi
đã thực hiện trên các vectơ ξ, mà chúng ta muốn lưu trữ như các hút dẫn cho động lực học nơ-ron, là chúng
có cùng chiều dài và chúng chia sẻ cùng "tầm quan trọng" γ. Đặc biệt, các mục tập dữ liệu
có thể đại diện cho các thực hiện (nhiễu) của một số mẫu chân lý cơ bản mà chúng ta không có
truy cập trực tiếp. Trong bối cảnh này, chính quy hóa – ngăn cản các tham số mạng khỏi việc có được
chuẩn lớn trong quá trình học – cũng cho phép giảm sự chuyên môn hóa của mô hình trên tập huấn luyện. Mối quan hệ td=ϵ−1
J do đó gợi ý rằng các vấn đề quá khớp có thể phát sinh đối với td quá lớn, như chúng

--- TRANG 9 ---
10−210−1100101102
td0.00.51.01.52.02.5t∗NGẪU NHIÊN
MNIST
FASHION-MNISTHình 2 :Thời gian dừng như một hàm của thời gian mơ. Biểu đồ hiển thị thời gian dừng sớm t∗ như một hàm của thời gian mơ td, thu được bằng một ước tính số (đường liền) từ
Eq. (3.12) và bằng một fit (đường nét đứt) dựa trên mối quan hệ chức năng t∗(td) =alog(1 + b td), được gợi ý
bởi các phát hiện phân tích được trình bày trong Phụ lục C. Các tham số mạng cho ba trường hợp là γ= 1,
N= 784, và P/N = 0.2. Cặp hệ số ( a, b) được ước tính thông qua bình phương tối thiểu tuyến tính là
(a= 0.66, b= 0.54),(a= 0.11, b= 3.25),(a= 0.19, b= 1.67) cho các tập dữ liệu ngẫu nhiên, MNIST và Fashion-MNIST, tương ứng.
sẽ thảo luận chi tiết hơn trong Mục 4. Hơn nữa, thời gian mơ td có thể liên quan, thông qua
td=ϵ−1
J và Eq. (3.12), đến thời gian dừng t∗ trong các phiên bản không chính quy hóa của thuật toán descent gradient. Mối quan hệ này nhất quán với nhận xét trước đó vì các kỹ thuật dừng sớm thực sự
được thiết kế để tránh quá khớp11 và, một lần nữa, nhớ lại mối quan hệ đơn điệu giữa td và t∗, chúng ta mong đợi rằng các vấn đề quá khớp có thể phát sinh đối với td quá lớn. Trong mục tiếp theo, chúng tôi sẽ sử dụng
khung được phác thảo trong mục này và, cụ thể, của ma trận tương tác tối ưu J(D), để
giải quyết khả năng khái quát hóa của các mô hình như vậy hoặc, ngược lại, sự xuất hiện của quá khớp.
4 Sự xuất hiện của khái quát hóa và quá khớp trong các mô hình giống Hopfield
4.1 Một tập dữ liệu tổng hợp
Các kết quả được suy ra trong Mục 2 và 3 được thu được mà không đưa ra các giả định cụ thể về các vectơ nhị phân {ξµ}P
µ=1, tuy nhiên, để đi xa hơn trong các khảo sát phân tích, một số giả thuyết bổ sung
là theo thứ tự. Trên thực tế, trong các nghiên cứu lý thuyết, người ta thường giả định rằng các mục mẫu được
trích xuất theo một phân phối xác suất được quy định cho phép làm việc ra một lý thuyết có thể kiểm soát. Chẳng hạn, khi xử lý mô hình Hopfield, một lựa chọn phổ biến là lấy các mục mẫu
như các biến ngẫu nhiên Rademacher i.i.d., và do đó đối xử với các mẫu như chân lý cơ bản để được
tái tạo bắt đầu từ một phiên bản bị hỏng của chúng. Tuy nhiên, trong các ứng dụng thực tế, người ta
không có truy cập trực tiếp đến các mẫu chân lý cơ bản, mà chỉ đến các thực hiện thực nghiệm tạo thành tập dữ liệu từ đó chúng ta muốn trích xuất thông tin. Trong một kịch bản có giám sát, người ta biết trước cách
các mục khác nhau tạo nên tập dữ liệu được phân vùng giữa các lớp, sao cho có thể
định nghĩa các nguyên mẫu lớp (ví dụ, lấy trung bình của các ví dụ thuộc về cùng danh mục)
được lấy làm đại diện của các vectơ cơ bản, xem ví dụ, [31, 33, 71]. Trong trường hợp không giám sát, điều này rõ ràng không thể được thực hiện, và cách đơn giản nhất để tiến hành là bao gồm tất cả các ví dụ,
đồng nhất, trong việc xử lý. Đây là con đường sẽ được theo đuổi trong mục này và sau này
chúng tôi chi tiết thiết lập không giám sát này bằng cách xem xét một tập dữ liệu tổng hợp.
Cho {ζµ}K
µ=1∈ {− 1,+1}N×K là các mẫu cơ bản mà chúng ta chỉ có quyền truy cập thông qua
các thực hiện thực nghiệm được gọi là ξµ,A với A= 1, . . . , M cho mỗi µ. Chúng tôi giả định rằng các ví dụ
trong tập dữ liệu huấn luyện này được thu được từ các mẫu cơ bản với một nhiễu nhân,
đó là, ξµ,A=χµ,Aζµ (với phép nhân theo từng mục), với
Prob[ χµ,A
i=±1] =1±r
2,
trong đó r∈[0,1] là tham số định lượng chất lượng của ví dụ (tức là, nó đo tương quan giữa ví dụ và mẫu cơ bản tương ứng). Tập dữ liệu huấn luyện của chúng tôi do đó
được tạo thành bởi tập hợp S={ξµ,A}A=1,...,M
µ=1,...,K và chúng tôi phân biệt hai tải: α:=KM/N, tức là, tỷ lệ
giữa số lượng ví dụ trong tập huấn luyện và kích thước mạng, và η:=K/N≤1, tức là,
tỷ lệ giữa số lượng lớp trong tập dữ liệu (về nguyên tắc là không biết) và kích thước mạng. Với loại thông tin có sẵn này, chúng ta muốn huấn luyện hệ thống để làm cho nó
có thể khái quát hóa, cụ thể là tái tạo các chân lý ẩn ζµ, bắt đầu từ một dữ liệu đầu vào
σ(0) là một đại diện bị hỏng của ζµ. Vì chúng ta không có truy cập trực tiếp đến các chân lý cơ bản, một
quy trình tối thiểu hóa lỗi trực tiếp không khả thi trong trường hợp này. Tuy nhiên, chúng ta có thể bao gồm từng mục đơn lẻ trong hàm mất mát của chúng ta và tận dụng các hiện tượng nổi lên trong các mô hình giống Hopfield: như chúng ta
sẽ thấy, đối với một tập dữ liệu đủ lớn, một sự dư thừa của các trạng thái giả xuất hiện và, tùy thuộc vào các
tham số kiểm soát của hệ thống, những điều này có thể ủng hộ sự xuất hiện của một pha khái quát hóa. Trong kịch bản này, cơ chế chính quy hóa đóng một vai trò quan trọng, ngăn cản nghiệm của hệ thống tầm thường hóa hoặc quá chuyên môn hóa trên tập huấn luyện. Trong thiết lập không giám sát này, ma trận tương tác do đó
được thu được bằng cách cắm vào (3.5), đó vẫn là nghiệm của descent gradient Eq. (2.6), thực hiện thực nghiệm
của kernel Hebbian với các mục Ω ij=1
PP
µ,Aξµ,A
iξµ,A
j, và ma trận tương quan thực nghiệm, có kích thước là ( KM)×(KM) và có các mục là C(µA),(νB)=1
KMP
iξµ,A
iξν,B
i.
Để đánh giá hiệu suất của mạng, chúng tôi tạo ra một tập kiểm tra ˜S={˜ξµ,A}A=1,...,M
µ=1,...,K , được lấy mẫu
theo cách tương tự như tập huấn luyện, và khởi tạo mạng với các cấu hình của tập kiểm tra, chẳng hạn σ(0)=˜ξµ,A, cái sau, theo cấu trúc, là một phiên bản nhiễu của ζµ với chất lượng r. Tiếp theo,
chúng tôi kiểm tra xem phản ứng mạng có phải là f(σ(0)) =ζµ, một kết quả mà chúng tôi diễn giải như một khái quát hóa đúng; ngược lại, việc truy xuất một trong các mục huấn luyện, chẳng hạn f(σ(0)) =ξµ,A, được diễn giải như quá khớp. Trong tiểu mục sau, chúng tôi thảo luận vai trò của các trạng thái giả trong sự xuất hiện của khái quát hóa và quá khớp.
4.2 Các trạng thái giả của dữ liệu huấn luyện tạo điều kiện cho khái quát hóa
Trong thiết lập Hopfield cổ điển, các trạng thái giả (tức là, các cấu hình là sự kết hợp của dữ liệu được lưu trữ)
được biết là làm suy yếu khả năng truy xuất của mô hình và nên được xử lý thích hợp để
giảm tính hút dẫn của chúng, xem ví dụ, [21, 25–27, 39, 47, 63, 72]. Trên thực tế, các cơ chế mơ
được đề cập trong Mục 3 chính xác nhằm mục đích này và việc thực hiện chúng cải thiện khả năng truy xuất
của mạng. Mặt khác, khi xử lý một tập dữ liệu được tạo thành từ các ví dụ không được gắn nhãn, tình hình khá khác nhau, và các hút dẫn giả là rất quan trọng cho sự xuất hiện của
khả năng khái quát hóa của mạng, như chúng ta sẽ thảo luận.
Hãy giả sử rằng tập huấn luyện được tạo thành từ một số lượng lớn M dữ liệu cho mỗi lớp, và hãy
xem xét một cấu hình giả được đưa ra bởi một sự kết hợp đối xứng của L ví dụ thuộc về

--- TRANG 10 ---
rc5c10Mẫu cơ bảnL=10L=5Ví dụHình 3 :Đại diện sơ đồ của các điểm huấn luyện, sự kết hợp giả và mẫu cơ bản. Hình ảnh phác thảo tổ chức của các cấu hình hút dẫn trong mỗi lớp trong
tập dữ liệu. Lớp được đại diện bởi một mẫu cơ bản ζ (chấm đỏ ở trung tâm), trong khi các điểm huấn luyện
được đặt tại khoảng cách (1 −r)/2 từ nó (tức là chúng có tương quan r). Các sự kết hợp giả của
các điểm huấn luyện bản thân chúng là các điểm hút dẫn, và tương quan cL(r) của chúng tăng với số lượng
L của các điểm huấn luyện liên quan đến sự kết hợp. Đối với M đủ lớn, cảnh quan kết quả bao gồm
nhiều điểm tối thiểu cục bộ rất gần nhau, sao cho chúng kết hợp và tạo thành các thung lũng phẳng xung quanh
mẫu cơ bản.
cùng lớp, đó là,
ξµ
L= sgnLX
l=1ξµ,Al
, (4.1)
là A1, . . . , A L∈ {1, . . . , M } các chỉ số của các ví dụ mà chúng ta đang trộn. Ký hiệu với cL(r) :=
ξµ
L·ζµ/N tương quan giữa ξµ
L và chân lý cơ bản liên quan, chúng ta nhận thấy rằng, miễn là L
tương đối lớn, ξµ
L hiển thị một tương quan với mẫu cơ bản ζµ lớn hơn tương quan r
hiển thị bởi bất kỳ mục huấn luyện nào, đó là r < c L(r)→
L≫11, xem Phụ lục D để biết thêm chi tiết.
Bây giờ, các cấu hình giả của dạng (4.1) trong các mô hình giống Hopfield có thể là các hút dẫn ổn định, sao cho
chạy động lực học (2.1) chúng ta có thể kết thúc trong một trong những điểm tối thiểu này và đạt được một truy xuất tương đối
công bằng. Thật vậy, mỗi cấu hình này bắt nguồn như một sự kết hợp của các hút dẫn liên kết
với các vectơ được lưu trữ. Trong kịch bản của chúng ta, tăng M, sẽ dẫn đến số lượng tăng của các cấu hình giả nội lớp mà, khi L tăng, thực sự thể hiện tương quan ngày càng lớn với
các chân lý cơ bản tương ứng như được phác thảo trong Hình 3. Đối với M đủ lớn, sau đó hợp lý
để mong đợi rằng các điểm tối thiểu của các ví dụ huấn luyện và các cấu hình giả kết hợp với nhau, sao cho
cảnh quan kết quả bao gồm một điểm tối thiểu rộng tập trung trong chân lý cơ bản ζµ, ủng hộ
việc tái tạo các mẫu ẩn.
Cho một cảnh quan như vậy, vai trò của td (hoặc, tương đương, của ϵJ) là gì? Như được nhớ lại tại đầu
của tiểu mục này, trong một mô hình Hopfield nơi chúng ta lưu trữ các mẫu cơ bản, các cơ chế mơ
giảm (và cuối cùng loại bỏ, nếu tải không quá cao) tính ổn định của các hỗn hợp giả giữa
các mẫu độc lập và điều này được thu được bằng cách co lại và nâng các lưu vực hút dẫn liên kết
với các mẫu. Chuyển sang một thiết lập không giám sát, chúng ta nhận ra rằng có hai loại hỗn hợp,
tùy thuộc vào việc chúng có liên quan đến các ví dụ thuộc về các lớp khác nhau hay các ví dụ thuộc về

--- TRANG 11 ---
cùng lớp; cái trước, giống như trong mô hình Hopfield, làm suy yếu truy xuất và nên được loại bỏ,
trong khi cái sau, như đã đề cập ở trên, có thể có lợi cho khái quát hóa. Do đó, trong trường hợp này,
cơ chế mơ nên hoạt động trong việc loại bỏ chỉ loại tương quan đầu tiên. Trên thực tế, bằng cách
tăng td chúng ta đang phân tách các điểm tối thiểu tương ứng với các mẫu được lưu trữ và quá trình này
ảnh hưởng dần dần các điểm tối thiểu với sự chồng lấp ngày càng lớn. Tương quan giữa các lớp thường nhỏ hơn
tương quan trong lớp – mức độ của chúng liên quan đến, tương ứng, K/N và r– và các giá trị tương đối nhỏ của td có thể đủ để tách các lưu vực hút dẫn liên quan đến các mẫu cơ bản khác nhau.
Tuy nhiên, nếu chúng ta để cơ chế mơ hoạt động quá lâu, sự hỗn hợp của các điểm tối thiểu nội lớp
có thể bị tách ra cũng như và chúng bị phân mảnh thành nhiều điểm tối thiểu năng lượng, mỗi cái liên kết với một
ví dụ đơn lẻ. Kết quả là, chúng ta không thể truy xuất các nguyên mẫu được hình thành tự phát, mà chỉ
các ví dụ đơn lẻ: hệ thống được chuyên môn hóa trên tập huấn luyện, do đó kết thúc trong một chế độ quá khớp.
Bức tranh này được củng cố bởi các kết quả số được báo cáo trong Hình 4. Ở đây, chúng tôi tập trung vào hành vi của hệ thống khi được chuẩn bị trong lân cận của một ví dụ huấn luyện hoặc của một cấu hình giả nội lớp. Đầu tiên, chúng ta nhận thấy rằng, đối với td đủ thấp, ngay cả khi hệ thống được khởi tạo
trong một cấu hình bao gồm một nhiễu loạn nhẹ của một trong các ví dụ được lưu trữ ( L= 1), động lực học nơ-ron sẽ đưa nó về phía một cấu hình cuối cùng với khoảng cách tương đối w.r.t. ví dụ tham chiếu
là (1 −r)/2 (chính xác là khoảng cách giữa các ví dụ được lưu trữ và chân lý cơ bản tương ứng). Đối với các trạng thái giả, tình hình tương tự, với khoảng cách tương đối giữa trạng thái cuối cùng và hỗn hợp tham chiếu trở nên thấp hơn và thấp hơn khi L tăng (khi tương quan cL(r) với chân lý cơ bản tăng đơn điệu với L). Một td lớn hơn một cách vừa phải mang lại một tính hút dẫn lớn hơn của các mẫu cơ bản. Bức tranh này nhất quán với tuyên bố của chúng tôi về sự kết hợp
của dân số các lưu vực hút dẫn trong một điểm tối thiểu rộng tập trung tại ζµ. Bằng cách tiếp tục tăng td, các điểm huấn luyện trở nên hút dẫn, với độ rộng lưu vực phụ thuộc vào số lượng ví dụ mỗi lớp.
Điều này báo hiệu rằng, tùy thuộc vào thiết lập, cơ chế mơ có thể tăng cường khái quát hóa
hoặc ủng hộ quá khớp.
5 Thí nghiệm số
Trong Mục này, chúng tôi cung cấp bằng chứng số cho các phát hiện lý thuyết của chúng tôi. Đầu tiên chúng tôi kiểm tra các vùng
trong không gian tham số ( α, td, r) nơi hệ thống được trang bị ma trận tương tác J(D) có thể
khái quát hóa thành công, đó là, khi được kiểm tra với các ví dụ không được bao gồm trong tập huấn luyện (nhưng chia sẻ
với chúng cùng thống kê cơ bản), nó có thể tái tạo công bằng mẫu cơ bản. Tiếp theo, chúng tôi
củng cố bức tranh này bằng cách áp dụng một thuật toán phân cụm cho các đầu ra mạng và chỉ ra rằng,
trong vùng của không gian tham số nơi hệ thống được mong đợi khái quát hóa (tương ứng quá khớp), số lượng lớp được ước tính tốt (được ước tính quá cao). Chi tiết về số học được thu thập trong Phụ lục B.
5.1 Sơ đồ khái quát hóa
Trong phần đầu của các thí nghiệm số, chúng tôi xem xét cả các tập dữ liệu không có cấu trúc và có cấu trúc,
để xác nhận và kiểm tra tính mạnh mẽ của các kết quả lý thuyết của chúng tôi. Các tập dữ liệu không có cấu trúc được xây dựng
tổng hợp như sau: chúng tôi ban đầu tạo ra một tập hợp K mẫu cơ bản Rademacher G={ζµ}µ=1,...,K,
từ đó chúng tôi thu được một tập hợp các ví dụ huấn luyện S={ξµ,A}A=1,...,M
µ=1,...,K (được đặc trưng bởi một chất lượng r như
được chỉ định trong Mục 4.1), được sử dụng để xây dựng J(D), theo Eq. (3.5). Tiếp theo, chúng tôi tạo ra một tập kiểm tra ˜S={˜ξµ,A}A=1,...,M
µ=1,...,K , áp dụng cùng quy trình được sử dụng cho tập huấn luyện, đó là, mỗi mục
˜ξµ,A thể hiện một chất lượng r w.r.t. mẫu cơ bản liên quan ζµ. Đối với các tập dữ liệu có cấu trúc, chúng tôi xem xét
các tiêu chuẩn MNIST [66] và Fashion-MNIST [67] và định nghĩa các mẫu cơ bản như trung bình lớp

--- TRANG 12 ---
0 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.4
0 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.4
0 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.4Hình 4 :Thư giãn đến các điểm cố định từ các ví dụ huấn luyện và các trạng thái
giả bị nhiễu loạn. Các biểu đồ hiển thị khả năng truy xuất của mô hình được khởi tạo trong một cấu hình bao gồm
một phiên bản bị nhiễu loạn của các ví dụ huấn luyện (tức là, L= 1) hoặc trong một cấu hình giả nội lớp
ξL (như được đưa ra bởi Eq. (4.1)). Kích thước mạng được cố định ở N= 500, số lượng lớp là K= 10,
và chất lượng của tập dữ liệu là r= 0.8, trong khi các giá trị khác nhau của thời gian mơ (từ trái sang phải
td= 0.1,2,10) và của tải (từ trên xuống dưới α= 0.4,0.6,0.8, đó là, M= 20,30,40) được xem xét.
Việc phân tích được thực hiện bằng cách lấy các cấu hình tham chiếu ξL (với cho L= 1,3,5,20, như được giải thích
bởi chú giải) và áp dụng một nhiễu loạn bao gồm việc lật ngẫu nhiên một phần q của các mục;
chuẩn bị hệ thống trong cấu hình σ(0) này, chúng tôi cập nhật mạng đến hội tụ về phía điểm cố định σ(∞). Sau đó, chúng tôi so sánh các khoảng cách trung bình ¯d(ξL,σ(0)) và ¯d(ξL,σ(∞)) giữa
các cấu hình tham chiếu và, tương ứng, các cấu hình ban đầu và cuối cùng. Các đường nét đứt đen
tương ứng với khoảng cách giữa các ví dụ huấn luyện được sử dụng để xây dựng J(D) và các chân lý cơ bản liên kết. Các kết quả được lấy trung bình trên 50 thực hiện khác nhau của tập dữ liệu.
, sau đó, các tập huấn luyện và kiểm tra được tạo thành từ M mục, được rút từ toàn bộ tập dữ liệu
(tổng thể được tạo thành từ, tương ứng, 60000 và 10000 thể hiện), sao cho hai tập có
giao điểm rỗng.
Dù tập dữ liệu là gì, chúng tôi khởi tạo hệ thống trong một cấu hình σ(0) thuộc về tập kiểm tra, chúng tôi
chạy động lực học (2.1) và thu thập cấu hình cuối cùng σ(∞)=f(σ(0)). Nói cách khác, σ(0) và
σ(∞) đại diện, tương ứng, đầu vào và đầu ra của hệ thống. Tiếp theo, chúng tôi đánh giá các

--- TRANG 13 ---
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8NGẪU NHIÊN
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8NGẪU NHIÊN
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8NGẪU NHIÊN
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8NGẪU NHIÊN
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST
td1+tdtd1+tdtd1+tdHình 5 :Truy xuất trên các tập dữ liệu tổng hợp và có cấu trúc. Hiệu suất truy xuất được đo
bằng khoảng cách Hamming chuẩn hóa d giữa cấu hình cuối cùng σ(∞) và
ví dụ huấn luyện gần nhất ξ (đường cong chấm, xem Eq. (5.1)) và chân lý cơ bản gần nhất ζ (đường cong liền, xem Eq. (5.2)); các kết quả được trình bày đã được lấy trung bình trên K×M cấu hình ban đầu khác nhau
tạo thành tập kiểm tra (xem Phụ lục B để biết thêm chi tiết). Các tham số mạng cho
tập dữ liệu ngẫu nhiên là N= 200, K= 10 và r= 0.8, trong khi đối với các tập dữ liệu có cấu trúc chúng là
N= 784 , K= 10. Đối với tất cả các tập dữ liệu, chúng tôi báo cáo kết quả cho các lựa chọn khác nhau của α= 0.1, 0.2 và
0.8, giữ η=K/N cố định, do đó, nhớ lại rằng α=KM/N , chúng tôi thay đổi α bằng cách tăng M.
lượng:
dξ= min
ξ∈Sd(ξ,σ(∞)), (5.1)
dζ= min
ζ∈Gd(ζ,σ(∞)), (5.2)
trong đó d:{−1,+1}N×{−1,+1}N→[0,1] là khoảng cách Hamming chuẩn hóa, đo phần
của các mục không cùng hướng giữa hai cấu hình được so sánh. Chúng tôi nhấn mạnh rằng, mặc dù không
được làm nổi bật trong Eqs. (5.1)-(5.2), những lượng này phụ thuộc vào cấu hình ban đầu vì σ(∞) phụ thuộc vào σ(0). Những khoảng cách này sau đó được lấy trung bình trên mẫu ˜S, để có, tương ứng, ¯dξ và ¯dζ.
Trong Hình 5, chúng tôi so sánh hành vi của ¯dξ và ¯dζ so với td, cho các lựa chọn khác nhau của M, trong khi K
được cố định. Chúng tôi thấy rằng, trong bất kỳ trường hợp nào, ¯dξ và ¯dζ đang giảm đơn điệu với td miễn là td
tương đối nhỏ, tiếp theo, hành vi của chúng phụ thuộc vào M. Đặc biệt, đối với tập dữ liệu ngẫu nhiên, khi M

--- TRANG 14 ---
nhỏ, chúng ta luôn có ¯dξ<¯dζ chứng tỏ khả năng khái quát hóa kém; khi M lớn hơn chúng ta có thể
tận dụng td để tăng cường khả năng khái quát hóa và có ¯dζ<¯dξ, tuy nhiên khi td lớn quá lớn các đường cong cắt nhau; cuối cùng, khi M lớn, tại thời gian mơ trung gian, ¯dξ và ¯dζ thể hiện một bình nguyên và
tại các giá trị lớn của td chúng tăng, độ cao của bình nguyên (tương ứng, ≈(1−r)/2 và ≈0) gợi ý rằng ở đó cấu hình cuối cùng gần với mẫu cơ bản, trong khi sự tăng trưởng cuối cùng gợi ý một
ảnh hưởng có thể có hại của một thời gian mơ lớn. Đối với các tập dữ liệu có cấu trúc, ¯dζ thể hiện một điểm tối thiểu
tại các giá trị trung gian của td, tương ứng với một hiệu suất khái quát hóa tối ưu và, một lần nữa, cho
thời gian mơ lớn, ¯dζ tăng. Đặc biệt, đối với tập dữ liệu MNIST, cho các giá trị tương đối nhỏ (tương ứng lớn) của td, chúng ta có ¯dζ<¯dξ (tương ứng ¯dζ>¯dξ), gợi ý khả năng khái quát hóa tốt (bị suy giảm).
Trước khi tiếp tục, chúng tôi cũng nhấn mạnh rằng, đối với tất cả các tập dữ liệu được xem xét, khi td≫1 và khi
M tương đối lớn, cả ¯dζ và ¯dξ đều tăng. Điều này là do thực tế là số lượng ví dụ
tương đối lớn để phát sinh các hút dẫn giả, nhưng không đủ lớn để làm cho những hút dẫn này gần với chân lý cơ bản; điểm này được khảo sát thêm trong phần sau.
Bây giờ chúng tôi tập trung vào tập dữ liệu tổng hợp và tóm tắt khả năng xử lý thông tin của
hệ thống thành "sơ đồ pha". Để mục đích này, chúng tôi phân biệt giữa các kết quả khác nhau như sau:
•Thành công : điều này tương ứng với ¯dζ<¯dξ và ¯dζ<1−r
2. Yêu cầu đầu tiên đảm bảo rằng
hệ thống thư giãn trong một cấu hình có tương quan nhiều hơn với mẫu cơ bản hơn với
các điểm huấn luyện; điều kiện thứ hai, thay vào đó, đảm bảo rằng động lực học kết thúc trong
quả cầu Hamming tập trung trong chân lý cơ bản với bán kính (1 −r)/2 và do đó hệ thống
đã di chuyển gần hơn đến chân lý cơ bản.
•Quá khớp : điều này tương ứng với ¯dζ≥¯dξ và ¯dξ<1−r
2. Điều kiện đầu tiên tuyên bố rằng cấu hình cuối cùng
gần hơn với một trong các điểm huấn luyện hơn với cơ bản; điều kiện thứ hai,
đảm bảo rằng, lần này, động lực học kết thúc trong quả cầu Hamming tập trung trong
điểm huấn luyện gần nhất với bán kính (1 −r)/2 và do đó hệ thống đã di chuyển gần hơn đến
một mục huấn luyện cụ thể.
•Thất bại : ngược lại. Trong trường hợp này hệ thống không đủ gần với một mẫu cơ bản cũng không
với một ví dụ.
Đối với một lựa chọn cho trước của K và r, di chuyển trong mặt phẳng ( α, td), chúng tôi do đó mô tả các sơ đồ khái quát hóa
cho các tập dữ liệu tổng hợp. Các kết quả được báo cáo trong Hình 6 và được thảo luận sau đây.
Bắt đầu từ bảng a (K= 10 và r= 0.7), chúng tôi thấy rằng, tại td rất thấp, hệ thống luôn
trong một chế độ thất bại. Điều này không đáng ngạc nhiên vì, ở đó, ma trận tương tác rất gần với
đơn thuốc Hebbian và, đối với lựa chọn này của các tham số tập dữ liệu, sự can thiệp giữa các ví dụ
đáng kể đủ cho hệ thống có thể kết thúc trong các trạng thái giả giữa các lớp, do đó chúng ta mong đợi rằng các cấu hình cân bằng là các trạng thái không truy xuất. Tăng td, những can thiệp như vậy (có thể)
được loại bỏ. Nếu tham số tải α thấp (có nghĩa là số lượng ví dụ thấp), hệ thống
đi vào một chế độ quá khớp vì các điểm tối thiểu tương ứng với các ví dụ đủ thưa thớt để được
dễ dàng phân tách. Mặt khác, bằng cách tăng α (cụ thể bằng cách tăng số lượng ví dụ
mỗi lớp), các hút dẫn kết hợp và các cấu hình tương ứng với các mẫu cơ bản trở nên
ngày càng hút dẫn hơn, sao cho hệ thống bắt đầu khái quát hóa tốt. Khi td≫1, khi
α tăng, sự chuyển đổi từ quá khớp sang thành công không còn trực tiếp như, đối với các giá trị trung gian
của α, chúng ta có thể kết thúc trong các trạng thái giả vẫn quá thưa thớt để đảm bảo một khái quát hóa âm thanh; vùng này có thể được co lại bằng cách tăng cường chất lượng tập dữ liệu. Trên thực tế, bằng cách tăng chất lượng tập dữ liệu như
trong bảng b (K= 10 và r= 0.8), bức tranh định tính giống nhau, chỉ với một sự mở rộng của
vùng thành công do thực tế là các ví dụ nội lớp bây giờ gần nhau hơn. Tiếp theo, chúng tôi di chuyển

--- TRANG 15 ---
0.000.350.701.051.400.20.40.60.8td1+td(a)K= 10,r=0.7
0.001.052.103.154.200.20.40.60.8(c)K= 30,r=0.7
0.000.350.701.051.40Æ0.20.40.60.8td1+td(b)K= 10,r=0.8
0.001.052.103.154.20Æ0.20.40.60.8(d)K= 30,r=0.8
F
S
OHình 6 :Sơ đồ khái quát hóa. Bốn sơ đồ hiển thị các kết quả khái quát hóa của
mạng nơ-ron nơi ma trận tương tác J(D) được xây dựng trên một mẫu các ví dụ ngẫu nhiên, tổng hợp
{ξµ,A}A=1,...,M
µ=1,...,K với N= 200 và M có thể điều chỉnh ( M=αN/K ). Trong mặt phẳng ( α, td), cho các giá trị khác nhau
của K và r, chúng tôi phác thảo ba vùng: thành công (S), quá khớp (O), thất bại (F). Trong bất kỳ trường hợp nào các điều kiện ban đầu σ(0) được lấy như các phiên bản bị nhiễu loạn của các chân lý cơ bản được lấy mẫu với cùng chất lượng r
như các ví dụ huấn luyện.
đến các bảng c và d, nơi một η lớn hơn (cụ thể, một số lượng lớp lớn hơn K= 30, với N cố định) có lợi
cho vùng thất bại, vì trong trường hợp này các cụm của các điểm tối thiểu liên kết với mỗi lớp gần hơn và
các cụm của các điểm tối thiểu tương ứng với các lớp khác nhau bây giờ trình bày các chồng lấp không tầm thường, sao cho
sự thư giãn của hệ thống có thể kết thúc xa lớp mà điều kiện ban đầu được
tạo ra.
5.2 Một phép tương tự với thuật toán phân cụm
Trong mục này, chúng tôi sử dụng một cách tiếp cận khác để kiểm tra sự xuất hiện của quá khớp và các chế độ khái quát hóa
khi các tham số hệ thống được điều chỉnh. Ý tưởng là sử dụng một thuật toán phân cụm không giám sát
để phân vùng các cấu hình cuối cùng σ(∞) thu được bằng cách áp dụng động lực học (2.1) cho các cấu hình kiểm tra; ở đây không giám sát đề cập đến thực tế là thuật toán phân cụm không biết về số lượng
các cụm hiệu quả.
Chúng tôi bắt đầu thí nghiệm bằng cách tạo ra một tập dữ liệu tổng hợp ngẫu nhiên G được tạo thành từ K mẫu cơ bản Rademacher, từ đó chúng tôi xây dựng một tập huấn luyện S và một tập kiểm tra ˜S, cả hai đều được đặc trưng bởi một chất lượng r và

--- TRANG 16 ---
một kích thước M. Chúng tôi sử dụng cái trước để xây dựng J(D) và cái sau để khởi tạo cấu hình nơ-ron. Chúng tôi
thu thập các cấu hình cuối cùng σ(∞) thu được bằng cách lặp động lực học nơ-ron và chúng tôi mong đợi rằng, nếu
mạng khái quát hóa đúng, thuật toán phân cụm được áp dụng cho các cấu hình cuối cùng sẽ trả về
một số lượng ước tính của các cụm ˆK là (xấp xỉ) K và mỗi cụm chứa một số lượng
mục là (xấp xỉ) M. Ngược lại, nếu mạng quá khớp, chúng tôi mong đợi rằng ˆK > K .
Để định lượng khả năng của những kết quả này, chúng tôi giới thiệu độ chính xác ˆM
M×K∈[0,1], trong đó
ˆM là tổng số ví dụ được phân cụm đúng bởi thuật toán. Thuật toán phân cụm không giám sát
được xem xét ở đây dựa trên cấu trúc dữ liệu Disjoint Set Union [73], hoạt động như
sau. Ban đầu, nó liên kết với mỗi mục σ(∞) một lớp khác nhau, sao cho, ở giai đoạn này, số lượng lớp ước tính là M×K. Tiếp theo, chúng tôi xem xét tất cả M
2
cặp cấu hình σ(∞)
và kiểm tra xem khoảng cách Hamming chuẩn hóa của chúng có nhỏ hơn một ngưỡng d∗(r) và, nếu vậy, hai
mục được hợp nhất trong cùng lớp. Một khi tất cả các cặp đã được khảo sát, thuật toán dừng.
Giá trị ngưỡng được chọn bằng điểm tối thiểu của khoảng cách Hamming chuẩn hóa giữa tất cả
các cặp ví dụ thuộc về tập kiểm tra:
d∗(r) = min
(σ1,σ2)∈˜S×˜S
σ1̸=σ2d(σ1,σ2).
Ý tưởng cơ bản của lựa chọn này là, sau khi áp dụng động lực học cho các ví dụ kiểm tra, nếu mạng
hoạt động tốt, các ví dụ thuộc về cùng lớp gần hơn với mẫu cơ bản chung,
khoảng cách của chúng giảm và được mong đợi nhỏ hơn d∗(r); mặt khác, đối với hai ví dụ
thuộc về các lớp khác nhau, khoảng cách được mong đợi không thay đổi đáng kể và vẫn lớn hơn
d∗(r).
Các kết quả của thí nghiệm này, được lặp lại cho các tải khác nhau và các thời gian mơ khác nhau, được
báo cáo trong Hình 7. Các bảng trong hàng đầu tiên hiển thị độ chính xác (trục trái) và sự khác biệt
giữa ˆK và số lượng lớp thực K (trục phải) như một hàm của td
1+td, trong khi các bảng
trong hàng thứ hai hiển thị các khoảng cách trung bình ¯dζ và ¯dξ giữa σ(∞) và, tương ứng, chân lý cơ bản
và ví dụ gần nhất, như một hàm của td
1+td. Các màu sắc trong nền tương ứng với
các chế độ khác nhau của mạng và chúng tôi đã sử dụng cùng bản đồ màu đã được áp dụng trước đó trong Hình 6, để
làm nổi bật tính nhất quán. Trên thực tế, trong vùng thất bại, độ chính xác thấp và số lượng cụm
được ước tính thấp; trong vùng thành công, độ chính xác là đơn vị và ˆK=K; trong vùng quá khớp,
độ chính xác là không tối ưu và số lượng cụm được ước tính quá cao. Hơn nữa, những kết quả này được
phản ánh đẹp mắt bởi hành vi của dζ và dξ: sự chuyển đổi từ thất bại sang thành công tương ứng với một
giảm đột ngột của dζ để lại dξ phía sau; sự chuyển đổi giữa thành công và quá khớp tương ứng với
dξ vượt qua dζ.
6 Kết luận
Các kết quả chính thu được trong công việc này được liệt kê sau đây:
•Chúng tôi đã giới thiệu một hàm mất mát được chính quy hóa, có điểm tối thiểu cung cấp ma trận tương tác J của
một mạng nơ-ron liên kết; theo tập dữ liệu được cung cấp, mạng nơ-ron được trang bị
với nghiệm J có thể truy xuất một tập hợp các mẫu cơ bản được lưu trữ hoặc khái quát hóa bắt đầu
từ một phiên bản bị hỏng của các mẫu cơ bản không biết.
•Chúng tôi đã chứng minh rằng nghiệm J của hàm mất mát tương ứng với một kernel giống Hebbian J(D),
được biết đến như kernel Hebbian mơ và được tham số hóa bởi "thời gian mơ" td, miễn là td
được xác định với nghịch đảo của tham số chính quy hóa ϵJ.

--- TRANG 17 ---
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.00.10.2¯dζ,¯dξ
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.00.10.2¯dξ¯dζ0.0 0.2 0.4 0.6 0.8 1.00.60.81.0độ chính xácα= 0.15
0.0 0.2 0.4 0.6 0.8 1.00.60.81.0α= 0.3
ˆK−K
độ chính xác
024
ˆK−K
024Hình 7 :Phân cụm tập kiểm tra. Áp dụng thuật toán phân cụm trong trường hợp một
tập dữ liệu tổng hợp ngẫu nhiên với các tham số K= 10, r= 0.8, N= 200 cho hai giá trị tải khác nhau: α= 0.15
và α= 0.3 giữ cố định số lượng chân lý cơ bản K. Trong mô phỏng với α= 0.15
vùng quá khớp xảy ra khi ¯dξ bị vượt qua bởi ¯dζ, trong vùng này thuật toán phân cụm không còn
có thể phân cụm đúng các ví dụ và số lượng cụm ước tính bắt đầu tăng. Trong
mô phỏng với α= 0.3, số lượng ví dụ mỗi chân lý cơ bản trong tập huấn luyện gấp đôi
của mô phỏng trước, ¯dζ luôn thấp hơn của ví dụ huấn luyện gần nhất ¯dξ và
vùng quá khớp không còn xuất hiện.
•Trong trường hợp không có chính quy hóa ( ϵJ= 0) nghiệm J(D) có thể được khôi phục bằng cách áp dụng một
chiến lược dừng sớm cho descent gradient trên hàm mất mát. Điều này gợi ý rằng td (hoặc,
tương đương, ϵJ= 0) đóng một vai trò trong việc ngăn cản quá chuyên môn hóa trên tập huấn luyện.
•Tập trung vào trường hợp của một tập huấn luyện được tạo thành từ các phiên bản bị hỏng của một số mẫu cơ bản không biết, chúng tôi đã tìm thấy bằng chứng số mạnh mẽ rằng các giá trị tương đối lớn của td và
các tập huấn luyện tương đối thưa thớt có thể mang lại quá khớp.
•Sự xuất hiện của quá khớp liên quan đến cấu trúc của hàm Lyapunov liên kết với
động lực học nơ-ron và bức tranh này cho phép chúng tôi suy đoán về các thiết lập tối ưu cho các siêu tham số mất mát và/hoặc cho thời gian huấn luyện. Bức tranh này được phác thảo trong Hình 8.
Để kết luận, các kết quả của chúng tôi làm nổi bật cơ chế có liên quan cho phép sự xuất hiện của khả năng khái quát hóa của các mạng giống Hopfield: điều này được xác định như sự kết hợp của các hút dẫn
liên kết với các điểm huấn luyện dẫn đến các điểm tối thiểu rộng xung quanh các chân lý cơ bản (đó thực sự là một thành phần quan trọng cho các mô hình chung thể hiện các tính chất khái quát hóa mạnh mẽ, xem ví dụ [74, 75] và các tài liệu tham khảo trong đó). Trong kịch bản này, chúng tôi đưa ra một đặc trưng toàn diện của
khái quát hóa và quá khớp cho các tập dữ liệu ngẫu nhiên tổng hợp. Các phát triển của công việc hiện tại sẽ
yêu cầu các phần mở rộng cho dữ liệu có cấu trúc, cũng như một đặc trưng cơ học thống kê của các
hiện tượng tập thể có liên quan.

--- TRANG 18 ---
Đặc trưng cơ bảnCác trạng thái giả nội lớpCác điểm huấn luyện(hút dẫn)
Các hút dẫn chính (ví dụ)Các hút dẫn thứ cấp (trạng thái giả)
Chế độ khái quát hóaChế độ quá khớpCác chân lý cơ bảnCảnh quan kết quảHình 8 :Sự xuất hiện của các điểm cố định cho một hệ thống được huấn luyện mà không có giám sát. Bức tranh sơ đồ này hiển thị sự tiến hóa của cảnh quan được tạo ra bởi J(D) khi td thay đổi. Khi
td tương đối thấp (trái), sự hỗn hợp của các điểm tối thiểu đã được phân tách một phần: sự can thiệp yếu giữa các lớp
được loại bỏ, trong khi các điểm tối thiểu tương ứng với các ví dụ của cùng lớp vẫn được
phân cụm; khi td tương đối lớn (phải) sự can thiệp giữa các ví dụ huấn luyện đã được
hoàn toàn dịch chuyển. Bức tranh này tương thích với các kết quả số của chúng tôi được báo cáo trong Hình 4.
7 Lời cảm ơn
E.A. và A.F. thừa nhận hỗ trợ tài chính từ dự án PNRR MUR PE0000013-FAIR và từ
Đại học Sapienza Roma (RM120172B8066CB0, AR2221815D7192C1, AR1221815EA97525).
F.A. và A.F. đã được hỗ trợ đầy đủ bởi dự án PNRR MUR PE0000013-FAIR.
E.A., M.A., F.A., A.F. thừa nhận môi trường nghiên cứu kích thích được cung cấp bởi sự kiện Theory and Methods Challenge Fortnights của Viện Alan Turing "Physics-informed Machine Learning".
A Sự hội tụ của quy trình descent gradient
Việc rời rạc hóa phương trình động lực học (3.6) đọc như
J(n+ 1) = J(n)−ϵ
J(n)(Ω+ϵJ1) + (Ω+ϵJ1)J(n)−2γΩ
, (A.1)
với điều kiện ban đầu J(n= 0) = 0. Để đơn giản, chúng tôi cố định γ= 1 mà không mất tính tổng quát, vì nó
chỉ tính đến một tái tỷ lệ toàn cục của ma trận liên kết. Phương trình động lực học Eq. (A.1) có thể được
viết lại như một phương trình điểm cố định:
J(n+ 1) = Gϵ(J(n)),

--- TRANG 19 ---
với
Gϵ(J) =J−ϵ(J(Ω+ϵJ1) + (Ω+ϵJ1)J−2γΩ).
Cho J và K hai ma trận N×N, chúng ta có
Gϵ(J)−Gϵ(K) =1
2(J−K)(1−2ϵ(Ω+ϵJ1)) +1
2(1−2ϵ(Ω+ϵJ1))(J−K).
Lấy chuẩn (toán tử) của cả hai bên, chúng ta có
∥Gϵ(J)−Gϵ(K)∥≤ ∥J−K∥·∥1−2ϵ(Ω+ϵJ1)∥=∥J−K∥(1−2ϵ(λ1+ϵJ)),
trong đó λ1 là trị riêng lớn nhất của Ω. Điều này có nghĩa là hàm Gϵ(J) là một ánh xạ co,
với điều kiện rằng
ϵ≤1
2(ϵJ+λ1). (A.2)
Do đó, tận dụng Định lý Điểm Cố định Banach, thuật toán hội tụ đến nghiệm của hệ thống.
Hơn nữa, bằng định lý Gershgorin, chúng ta có thể có được cận sau
λ1≤X
ij|Ωij|. (A.3)
Do đó, đối với bất kỳ ϵ sao cho
ϵ≤1
2(ϵJ+P
ij|Ωij|), (A.4)
yêu cầu hội tụ (A.2) được thỏa mãn một cách tầm thường.
B Phương pháp
Phụ lục này được dành cho mô tả chi tiết của các thí nghiệm số. Chúng tôi nhớ lại rằng các thí nghiệm của chúng tôi
bao gồm một giai đoạn huấn luyện, trong đó chúng tôi huấn luyện ma trận liên kết của mạng với một
tập huấn luyện, và một giai đoạn tái tạo, trong đó chúng tôi thực hiện một động lực học tuần tự trên cấu hình nơ-ron bắt đầu từ một mục thuộc về một tập kiểm tra. Các tập huấn luyện và kiểm tra được mô tả trong Mục 4.1
và Mục 5: các tập dữ liệu tổng hợp (mẫu cơ bản Rademacher) và chuẩn (MNIST và Fashion-MNIST)
được xem xét. Để tạo điều kiện cho tính tái tạo của nghiên cứu, ở đây chúng tôi trình bày các thuật toán
và mã giả của các thí nghiệm số (Tiểu mục B.1) cùng với các chỉ số hiệu suất để
đánh giá chất lượng của các trạng thái nơ-ron cuối cùng (Tiểu mục B.2); bằng chứng thêm về sự xuất hiện của
quá khớp cũng được cung cấp (Tiểu mục B.3). Hơn nữa, chúng tôi chỉ ra rằng tất cả các mô phỏng được
thực hiện bằng cách sử dụng ngôn ngữ lập trình động lực cao Julia và được chạy trên một máy tính cá nhân với bộ xử lý Intel Core i7.
B.1 Thiết kế huấn luyện và gán tham số
Trong giai đoạn huấn luyện, chúng tôi tìm biểu thức của ma trận liên kết J và của trường bên ngoài h
tối thiểu hóa hàm mất mát Lξ(J,h) được đưa ra trong phương trình (2.7) thông qua phương pháp descent gradient. Các
tham số xuất hiện trong hàm mất mát được tóm tắt và mô tả trong Bảng 1. Cụ thể,
N là kích thước của đầu vào và bằng độ dài của các mẫu huấn luyện, P là kích thước của
tập huấn luyện (hoặc tổng số mẫu huấn luyện), ϵJ là hằng số chính quy hóa cho ma trận liên kết, ϵh là hằng số chính quy hóa cho trường h và, cuối cùng, t là thời gian huấn luyện. Trong

--- TRANG 20 ---
Tham số Mô tả
N kích thước của đầu vào
P kích thước của tập huấn luyện
ϵJ thuật ngữ chính quy hóa L2 cho J
ϵh thuật ngữ chính quy hóa L2 cho h
t thời gian huấn luyện
Bảng 1 : Các tham số huấn luyện.
Thuật toán 1 Huấn luyện ma trận liên kết J
Đầu vào : Tập huấn luyện ξ={ξµ}P
µ=1∈ {− 1,+1}N×P, thời gian dừng t, chính quy hóa ϵJ
Thiết lập cho t và ϵJ:
Chính quy hóa L2: t=∞ và ϵJ>0
Chính quy hóa dừng sớm: t=t∗ như được đưa ra bởi (3.12) và ϵJ= 0
Đầu ra :J(t),h(t)
1:J(0)=0N,N khởi tạo ma trận liên kết
2:ˆξµ=ξµ−1
PPP
µ=1ξµ tâm hóa mẫu
3:ˆΩ=1
P(ˆξ·ˆξT) tính toán kernel Hebbian
4:∆t=1
2[ϵJ+PN
i,j=1|ˆΩij|]−1 thời gian rời rạc hóa như được đưa ra bởi (A.4)
5:iters =l
t
∆tm
số lần lặp cần chạy
6:n=0
7:lặp lại
8: J=J−∆t
2ϵJJ+J′ˆΩ+ˆΩ′J−2γˆΩ
9: n=n+ 1
10:cho đến khi n=iters hoặc J đã đạt một điểm cố định.
11:trả về J,h= (γ1−J)1
PPP
µ=1ξµ
các mô phỏng với các tập dữ liệu có cấu trúc như MNIST và Fashion-MNIST, vì chúng tôi muốn tái tạo
tập dữ liệu gốc chứ không phải phiên bản được tâm hóa của nó, chúng tôi bảo tồn trường h trong động lực học và, vì
sự thuận tiện và đơn giản, chúng tôi đặt ϵh= 0 trong tất cả các mô phỏng. Mã giả được trình bày trong
Thuật toán 1 làm nổi bật các bước được theo dõi để huấn luyện mô hình.
Nếu chúng tôi chạy Thuật toán 1 đến hội tụ, ma trận tương tác kết quả khôi phục kernel mơ,
ký hiệu là J(D) và được báo cáo trong Eq. (3.5), miễn là chúng tôi đặt ϵJ=t−1
d và γ= 1. Do đó, nếu
chúng tôi quan tâm đến mô hình được chính quy hóa, huấn luyện đầy đủ, chúng tôi có thể trực tiếp đặt J=J(D) mà không cần
chạy quy trình huấn luyện. Nếu chúng tôi đặt ϵJ=ϵh= 0 và chúng tôi dừng descent gradient tại
huấn luyện t=t∗ được đưa ra trong (3.12), thì ma trận liên kết kết quả J(t∗) gần nhất có thể với
ma trận tương tác J(D).
B.2 Công thức của các chỉ số hiệu suất
Một khi huấn luyện kết thúc và chúng tôi có biểu thức mong muốn của ma trận liên kết J, khả năng truy xuất
của máy được khảo sát. Cấu hình ban đầu σ(0) được lấy như một phiên bản bị hỏng của một trong các mẫu huấn luyện (với một phần các mục bị lật w.r.t. cấu hình tham chiếu

--- TRANG 21 ---
Thuật toán 2 Động lực học tuần tự
Đầu vào : Liên kết J∈RN×N, trường bên ngoài h∈RN, đầu vào σ(0)∈ {− 1,1}N
Đầu ra : Cấu hình nơ-ron cuối cùng σ(∞)
1:Loại bỏ các thuật ngữ đường chéo từ J
2:Đặt n= 0
3:lặp lại
4: lấy mẫu một số nguyên ngẫu nhiên i đồng nhất trong tập hợp {1,2, . . . , N }
5: cập nhật spin thứ i σi theo σ(n+1)
i =sign(PN
j=1Jijσ(n)
j+hiσ(n)
i)
6: n=n+ 1
7:cho đến khi σ đã đạt một điểm cố định.
ration) hoặc như một mục của một tập kiểm tra (có các phần tử tương tự thống kê với các mẫu huấn luyện,
nhưng không tham gia vào quy trình huấn luyện); nói chung, chúng tôi ký hiệu với Q mẫu các cấu hình ban đầu. Sau đó, hệ thống thư giãn theo quy tắc tiến hóa được giới thiệu trong Eq. (2.1) và
được báo cáo ở đây trong ký hiệu thời gian rời rạc
σ(n+1)
i = sign(X
jJijσ(n)
j+hiσ(n)
i). (B.1)
Mã giả cho động lực học có thể được tìm thấy trong Thuật toán 2. Sau khi thư giãn về phía một
cấu hình cân bằng σ(∞), chúng tôi kiểm tra sự gần gũi của nó với một cấu hình cụ thể ξ∗ bằng cách tận dụng
khoảng cách Hamming chuẩn hóa như một chỉ số hiệu suất. Cái sau là số lượng mục không cùng hướng
giữa hai cấu hình σ1,σ2∈ {− 1,1}N chia cho N, đó là
d(σ1,σ2) :=1
2NNX
i=1|σ1
i−σ2
i|.
Sau đó, chúng tôi đánh giá d(σ(∞),ξ∗), phụ thuộc vào cấu hình ban đầu, do thực tế là
σ(∞) phụ thuộc vào σ(0). Tiếp theo, chúng tôi lấy trung bình d(σ(∞),ξ∗) trên |Q| thực hiện khác nhau của điều kiện ban đầu; khi σ(0) được hiểu như một mục của tập kiểm tra, hoạt động này tương ứng với một trung bình hàng loạt.
Bằng cách này, chúng tôi có được khoảng cách trung bình, được định nghĩa là
¯dξ∗=1
|Q|X
σ(0)∈Qd(σ(∞)(σ(0)),ξ∗), (B.2)
được mong đợi phụ thuộc vào điểm tham chiếu ξ∗ và vào các tham số hệ thống.
Cuối cùng, chúng tôi nhận thấy rằng d(σ(∞),ξ∗) không gì khác ngoài lỗi tuyệt đối (chuẩn hóa) được tạo ra bởi
máy xuất ra σ(∞), khi được yêu cầu tái tạo ξ∗.
B.3 Dấu hiệu số của quá khớp
Trong mục này, chúng tôi sẽ đưa ra thêm dấu hiệu số của sự xuất hiện quá khớp cho mô hình chúng tôi đang
xử lý. Cụ thể, chúng tôi kiểm tra rằng quá khớp có thể diễn ra trong quy trình huấn luyện không chính quy hóa. Để làm điều này, chúng tôi theo dõi sự tiến hóa của các hàm mất mát học tập và xác thực
sau theo thời gian huấn luyện, cụ thể các hàm mất mát có cấu trúc sau
Lξ(J(t),h(t)) =1
2PX
i,µ(X
jJij(t)ξµ
j+hi(t)−ξµ
i)2+1
2PX
j,µ(X
iJij(t)ξµ
i+hj(t)−ξµ
j)2,(B.3)

--- TRANG 22 ---
0.00 0.06 0.26 0.89 2.86 9.00
t0.00.10.20.30.4LHuấn luyện
NGẪU NHIÊN
MNIST
FASHION-MNIST
0.00 0.06 0.26 0.89 2.86 9.00
t0.20.30.40.5Kiểm traHình 9 :Mất mát huấn luyện và kiểm tra như một hàm của thời gian huấn luyện. Hình
hiển thị so sánh giữa hàm mất mát huấn luyện và các hàm xác thực cho tập dữ liệu tổng hợp (đường cong xanh), MNIST (đường cong cam) và Fashion-MNIST (đường cong xanh lá) cho một quy trình huấn luyện không có
thuật ngữ chính quy hóa.
trong đó
hi(t) =1
PPX
µ=1(ξµ
i−NX
j=1Jij(t)ξµ
j) (B.4)
Trong các mô phỏng số sau, Jij(t) tiến hóa theo Thuật toán 1 với các tham số ϵJ.
Phương trình (B.3) là mất mát được đưa ra trong Eq. (2.7) của văn bản chính với ϵJ=ϵh= 0, γ= 1. Các hàm mất mát học tập và xác thực được thu được bằng cách thay thế vào các phương trình trước đó {ξ}P
µ=1 với
các đặc trưng của tập dữ liệu huấn luyện và kiểm tra tương ứng. Đối với trường hợp tập dữ liệu ngẫu nhiên tổng hợp và MNIST
và Fashion-MNIST, chúng tôi thấy rằng, ngay cả khi mất mát huấn luyện đi đến không, các mất mát xác thực thực sự thể hiện một điểm tối thiểu toàn cục tại thời gian huấn luyện hữu hạn, và sau đó bắt đầu tăng, do đó báo hiệu một
sự xấu đi trong hiệu suất khái quát hóa. Những kết quả này được báo cáo trong Hình 9.
C Một cách nhanh hơn để tính thời gian dừng sớm
Ước tính thời gian dừng sớm được báo cáo trong Eq. (3.12) dựa trên việc tính toán của
phân phối phổ thực nghiệm ρE của Ω, đó là một ma trận N×N. Đối với các tập dữ liệu chiều cao, điều này
có thể tạo thành một nút cổ chai trong quy trình huấn luyện, vì vậy các tiêu chí hoạt động cần được cung cấp. Để
làm điều này, chúng ta có thể khai triển Taylor lượng δ(λ, t, ϵ J) xung quanh trị riêng trung bình của phân phối thực nghiệm, tức là
δ(λ, t, ϵ J) =∞X
k=01
k!δ(¯λ, t, ϵ J)(k)(λ−¯λ)k,
do đó
¯δ(t, ϵJ) =∞X
k=01
k!δ(¯λ, t, ϵ J)(k)Z
dλ
λ−¯λkρE(λ) =∞X
k=01
k!δ(¯λ, t, ϵ J)(k)1
NETr (Ω−¯λ1)k.

--- TRANG 23 ---
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ1)NGẪU NHIÊN
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNIST
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20FASHION-MNISTMơGD-Dừng sớmÆ=0.2Æ=0.3Æ=0.5
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ(1))NGẪU NHIÊN
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNISTP/ N=0.2P/ N=0.3P/ N=0.50.00.20.40.60.81.0tdtd+10.000.050.100.150.20FASHION-MNISTMơGD-Dừng sớmMơGD-Dừng sớm
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ(1))NGẪU NHIÊN
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNISTP/ N=0.2P/ N=0.3P/ N=0.50.00.20.40.60.81.0tdtd+10.000.050.100.150.20FASHION-MNISTMơGD-Dừng sớmMơGD-Dừng sớm
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ(1))NGẪU NHIÊN
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNISTP/ N=0.2P/ N=0.3P/ N=0.50.00.20.40.60.81.0tdtd+10.000.050.100.150.20FASHION-MNISTMơGD-Dừng sớmMơGD-Dừng sớmHình 10 :So sánh giữa dừng sớm và kernel mơ với thời gian xấp xỉ. Ba biểu đồ hiển thị so sánh giữa hiệu suất truy xuất của kernel Mơ và quy trình huấn luyện dừng sớm. Nội dung hoàn toàn tương tự với Hình 1, với ngoại lệ duy nhất là thời gian dừng sớm ở đây được tính với xấp xỉ bậc nhất (C.1).
Dừng tại bậc nhất trong λ−¯λ và sau đó giải bài toán tối thiểu hóa, chúng ta còn lại với
đơn thuốc
t∗(td)≈1
21
NETrΩlog
1 +td1
NETrΩ
. (C.1)
Các kết quả được báo cáo trong Hình 10, một lần nữa hiển thị một sự thỏa thuận đáng kể giữa quy trình dừng sớm và kịch bản kernel Mơ. Lưu ý rằng tiêu chí này chính xác tính đến
sự phụ thuộc logarithmic đúng của thời gian dừng sớm w.r.t. thời gian mơ td. Rõ ràng,
đơn thuốc này có thể cung cấp, trong trường hợp chung, một ước tính thô của thời gian dừng sớm. Trong trường hợp đó, người ta có thể quyết định cũng làm việc với các bậc phụ trong λ−¯λ: bất kể bậc mà
việc tính toán được thực hiện, ước tính số của t∗ dựa trên việc tính toán các mô-men bậc thấp của kernel Hebbian (được sửa đổi) ˆΩ, dễ dàng hơn nhiều so với tính toán toàn bộ phân phối phổ thực nghiệm.
D Chi tiết về các trạng thái giả
Trong Phụ lục này, chúng tôi báo cáo một số chi tiết về các cấu hình giả của các ví dụ huấn luyện trong
tập dữ liệu ngẫu nhiên tổng hợp. Chúng tôi nhớ lại rằng ξµ,A=χµ,Aζµ (với phép nhân theo từng mục), với các
biến χµ,A
i được trích xuất như
Prob[ χµ,A
i=±1] =1±r
2.
Các cấu hình giả có liên quan trong thiết lập này là của loại Hopfield, sao cho chúng ta có thể xem xét sự kết hợp
của dạng
ξµ
L= sgnLX
l=1ξµ,Al
. (D.1)
Tương quan của cấu hình mới này với mẫu cơ bản ζµ là
cL=1
NX
iξµ
L,iζµ
i=1
NX
isgnLX
l=1ξµ,Al
iζµ
i
=1
NX
isgnLX
l=1χµ,Al
i
.

--- TRANG 24 ---
Đối với L lớn, biến ngẫu nhiên trong hàm sign là, bởi định lý giới hạn trung tâm (CLT), được phân phối Gaussian,
với trung bình r và phương sai p
(1−r2)/L, do đó
cL=1
NX
isgn
1 +r
1−r2
Lr2zi
,
với z∼ N (0,1), nơi chúng tôi cũng sử dụng r > 0. Lập luận của hàm sign dương với
xác suất
p=P
zi≥ −r
Lr2
1−r2
= 1−1
2erfchs
Lr2
2(1−r2)i
.
Do đó, lượng cL không gì khác ngoài một bước đi ngẫu nhiên của các bước đơn vị với xác suất p nhảy về
bên phải. Trong giới hạn N lớn, chúng ta do đó có
cL(r) =1
NX
isgn
1 +r
1−r2
Lr2zi
≈2p−1 = erfhs
Lr2
2(1−r2)i
.
mang lại
cL(r)> r ⇒ L >2[erf−1(r)]2(1−r2)
r2.
Bây giờ, r.h.s. của bất đẳng thức cuối nằm trong [0 , π/2], trong khi biểu thức xấp xỉ cho cL(r)
được thu được dưới CLT, yêu cầu L≫1, do đó đủ để có được cL(r)> r.
Đối với L cố định, có M
L
cấu hình có thể có của dạng (4.1). Chỉ để đưa ra một ví dụ về
độ lớn ở đây liên quan, đối với M= 50 và r= 0.6, chúng ta có 1225 sự kết hợp giả của L= 2
ví dụ hiển thị một tương quan với đặc trưng cơ bản c2≈0.8884, chúng ta có ∼2·106 sự kết hợp
với L= 5 hiển thị một tương quan c5≈0.9993, và cứ thế.
Tài liệu tham khảo
[1] S.-I. Amari. Learning patterns and pattern sequences by self-organizing nets of threshold elements.
IEEE Transactions. , 21:1197–1206, 1972.
[2] W. A. Little. The existence of persistent states in the brain. Mathematical Biosciences , 19(1-2):101–120,
1974.
[3] J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences of the United States of America , 79:2554–2558, 1982.
[4] D.J. Amit, H. Gutfreund, and H. Sompolinsky. Information storage in neural networks with low levels
of activity. Physical Review A , 35(5):2293, 1987.
[5] E. Agliari, F.E. Leonelli, and C. Marullo. Storing, learning and retrieving biased patterns. Applied
Mathematics and Computation , 415:126716, 2022.
[6] E. Agliari, A. Barra, A. Galluzzi, F. Guerra, and F Moauro. Multitasking associative networks. Physical
Review Letters , 109:268101, 2012.
[7] L. Cugliandolo. Correlated attractors from uncorrelated stimuli. Neural Computation , 6:220, 1993.
[8] E. Agliari, A. Barra, A. De Antoni, and A. Galluzzi. Parallel retrieval of correlated patterns: From
hopfield networks to boltzmann machines. Neural Networks , 38:52–63, 2013.
[9] E. Agliari, A. Fachechi, and C. Marullo. The relativistic hopfield model with correlated patterns.
Journal of Mathematical Physics , 61(123301), 2020.

--- TRANG 25 ---
[10] E. Agliari, F. Alemanno, A. Barra, M. Centonze, and A. Fachechi. Neural networks with a redundant
representation: Detecting the undetectable. Physical Review Letters , 124:028301, 2020.
[11] E. Agliari and G. De Marzo. Tolerance versus synaptic noise in dense associative memories. European
Physical Journal Plus , 135:883, 2020.
[12] F. Camilli, P. Contucci, and E. Mingione. An inference problem in a mismatched setting: a spin-glass
model with mattis interaction. SciPost Phys. , 12:125, 2022.
[13] B. Wemmenhove and A.C.C. Coolen. Finite connectivity attractor neural networks. Journal of Physics
A, 36(9617), 2003.
[14] E. Agliari, A. Annibale, A. Barra, A.C.C. Coolen, and D. Tantari. Immune networks: multitasking
capabilities near saturation. Journal of Physics A , 46:415003, 2013.
[15] E. Agliari, D. Migliozzi, and D. Tantari. Non-convex multi-species Hopfield models. Journal of
Statistical Physics , 172:1247–1269, 2018.
[16] P. Baldi and S. S. Venkatesh. Number of stable points for spin-glasses and neural networks of higher
orders. Physical Review Letters , 58, 1987.
[17] D. Krotov and J.J. Hopfield. Dense associative memory for pattern recognition. Advances in Neural
Information Processing Systems , pages 1180–1188, 2016.
[18] E. Agliari, F. Alemanno, A. Barra, and A. Fachechi. Generalized Guerra's interpolation schemes for
dense associative neural networks. Neural Networks , 128:254–267, 2020.
[19] D.O. Hebb. The Organization of Behavior: A Neuropsychological Theory. New York, NY: John Wiley
& Sons, 1949.
[20] D.J. Amit. Modeling brain function: The world of attractor neural networks . Cambridge university
press, 1989.
[21] J.J. Hopfield, D.I. Feinstein, and R.G. Palmer. Unlearning has a stabilizing effect in collective
memories. Nature Letters , 304:280158, 1983.
[22] I. Kanter and H. Sompolinsky. Associative recall of memory without errors. Physical Review A ,
35(1):380, 1987.
[23] P. Del Giudice, S. Franz, and M.A. Virasoro. Perceptron beyond the limit of capacity. Journal of
Physics France , 50:121–134, 1989.
[24] S. Franz, D.J. Amit, and M. A. Virasoro. Prosopagnosia in high capacity neural networks storing
uncorrelated classes. Journal of Physics France , 51:387–408, 1990.
[25] V. Dotsenko, N. D. Yarunin, and E. A. Dorotheyev. Statistical mechanics of Hopfield-like neural
networks with modified interactions. Journal of Physics A , 24:2419, 1991.
[26] A. Y. Plakhov, S. A. Semenov, and I. B. Shuvalova. Convergent unlearning algorithm for the Hopfield
neural network. IEE Computation Society Press , 2(95):30, 1995.
[27] A. Fachechi, E. Agliari, and A. Barra. Dreaming neural networks: Forgetting spurious memories and
reinforcing pure ones. Neural Networks , 112:24–40, 2019.
[28] E. Marinari. Forgetting memories and their attractiveness. Neural Computation , 31(3):503–516, 2019.
[29] M. Benedetti, E. Ventura, E. Marinari, G. Ruocco, and F. Zamponi. Supervised perceptron learning vs
unsupervised Hebbian unlearning: Approaching optimal memory retrieval in Hopfield-like networks.
Journal of Chemical Physics , 156:104107, 2022.

--- TRANG 26 ---
[30] L. Serricchio, C. Chilin, D. Bocchi, R. Marino, M. Negri, C. Cammarota, and F. Ricci-Tersenghi.
Daydreaming hopfield networks and their surprising effectiveness on correlated data. In Associative
Memory & Hopfield Networks in 2023 , 2023.
[31] J.F. Fontanari. Generalization in a Hopfield network. Journal of Physics France , 51:2421–2430, 1990.
[32] E. Agliari, F. Alemanno, A. Barra, and G. De Marzo. The emergence of a concept in shallow neural
networks. Neural Networks , 148:232–253, 2022.
[33] M. Aquaro, F. Alemanno, I. Kanter, A. Barra, and E. Agliari. Supervised Hebbian learning.
Europhysics Letters - Perspective , 141:11001, 2023.
[34] M. Negri, C. Lauditi, G. Perugini, C. Lucibello, and E. Malatesta. Storage and learning phase
transitions in the random-features hopfield model. Physical Review Letters , 131(25):257301, 2023.
[35] A. Barra, A. Bernacchia, E. Santucci, and P. Contucci. On the equivalence of Hopfield networks and
Boltzmann Machines. Neural Networks , 34:1–9, 2012.
[36] S. Cocco, R. Monasson, and V. Sessak. High-dimensional inference with the generalized Hopfield model:
principal component analysis and corrections. Physical Review E , 83:051123, 2011.
[37] M. M´ ezard. Mean-field message-passing equations in the Hopfield model and its generalizations.
Physical Review E , 95, 2017.
[38] F. E. Leonelli, E. Agliari, L. Albanese, and A. Barra. On the effective initialisation for restricted
Boltzmann machines via duality with Hopfield model. Neural Networks , 143:314–326, 2021.
[39] A. Fachechi, A. Barra, E. Agliari, and F. Alemanno. Outperforming RBM feature-extraction
capabilities by "dreaming" mechanism. IEEE Transactions on Neural Networks and Learning Systems ,
pages 1–10, 6 2022.
[40] E. Agliari and C. Marullo. Boltzmann machines as generalized hopfield networks: a review on recent
results and outlooks. Entropy , 23(1):34, 2021.
[41] F. Camilli, D. Tieplova, and J. Barbier. Fundamental limits of overparametrized shallow neural
networks for supervised learning. arXiv:2307.05635 , 2023.
[42] J. Barbier, F. Camilli, M. Mondelli, and M. Saenz. Bayes-optimal limits in structured PCA, and how to
reach them. arXiv:2210.01237 , 2022.
[43] E. Agliari, M. Aquaro, A.C.C. Coolen, and A. Fachechi. A replica approach to RBM in generative
mode. in preparation , 2023.
[44] N. E. Zamri, S. A. Azhar, M. A. Mansor, A. Alway, and M. S. M. Kasihmuddin. Weighted random k
satisfiability for k= 1, 2 (r2sat) in discrete hopfield neural network. Applied Soft Computing ,
126:109312, 2022.
[45] E. Ventura, S. Cocco, R. Monasson, and F. Zamponi. Unlearning regularization for Boltzmann
Machines. https://arxiv.org/pdf/2311.09418.pdf , 16:1065–1095, 2023.
[46] N. E. Zamri, M. A. Mansor, M. S. M. Kasihmuddin, S. S. Sidik, A. Alway, N. A. Romli, Y. Guo, and
S. Z. M. Jamaludin. A modified reverse-based analysis logic mining model with weighted random 2
satisfiability logic in discrete hopfield neural network and multi-objective training of modified niched
genetic algorithm. Expert Systems with Applications , 240:122307, 2024.
[47] E. Agliari, F. Alemanno, A. Barra, and A. Fachechi. Dreaming neural networks: rigorous results.
Journal of Statistical Mechanics , page 083503, 2019.
[48] A.C.C. Coolen, R. K¨ uhn, and P. Sollich. Theory of neural information processing systems . Oxford
University Press, 2005.

--- TRANG 27 ---
[49] E. Gardner. The space of interactions in neural network models. Journal of Physics A , 21(1):257, 1988.
[50] E. Gardner and B. Derrida. Three unfinished works on the optimal storage capacity of networks.
Journal of Physics A , 22(12):1983, 1989.
[51] L. Personnaz, I. Guyon, and G. Dreyfus. Information storage and retrieval in spin-glass like neural
networks. Journal of Physics Letters , 46:359–365, 1985.
[52] A.E. Hoerl. Application of ridge analysis to regression problems. Chemical Engineering Progress ,
58:54–59, 1962.
[53] A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems.
Technometrics , 12(1):55–67, 1970.
[54] A.N. Tikhonov and V.I.A. Arsenin. Solutions of Ill-posed Problems . Halsted Press book. Winston, 1977.
[55] V.N. Vapnik. Statistical Learning Theory . A Wiley-Interscience publication. Wiley, 1998.
[56] B. Sch¨ olkopf and A. J. Smola. Learning with kernels: support vector machines, regularization,
optimization, and beyond . MIT press, 2002.
[57] V. Vovk. Kernel ridge regression. In Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik ,
pages 105–116. Springer, 2013.
[58] G. Meanti, L. Carratino, E. De Vito, and L. Rosasco. Efficient hyperparameter tuning for large scale
kernel ridge regression. In International Conference on Artificial Intelligence and Statistics , pages
6554–6572. PMLR, 2022.
[59] G. S. Alberti, E. De Vito, M. Lassas, L. Ratti, and M. Santacesaria. Learning the optimal tikhonov
regularizer for inverse problems. Advances in Neural Information Processing Systems , 34:25205–25216,
2021.
[60] D. Wu and J. Xu. On the optimal weighted l2regularization in overparameterized linear regression.
Advances in Neural Information Processing Systems , 33:10112–10123, 2020.
[61] T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least
squares interpolation. Annals of statistics , 50(2):949, 2022.
[62] P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression.
Proceedings of the National Academy of Sciences , 117(48):30063–30070, 2020.
[63] F. Crick and G. Mitchison. The function of dream sleep. Nature , 304(5922):111–114, 1983.
[64] E. Agliari, F. Alemanno, M. Aquaro, A. Barra, F. Durante, and I. Kanter. Hebbian dreaming for small
datasets. Neural Networks , page 106174, 2024.
[65] T. O. Kohonen. Self-organization and associative memory. Berlin: Springer, 1984.
[66] L. Deng. The mnist database of handwritten digit images for machine learning research [best of the
web]. IEEE signal processing magazine , 29(6):141–142, 2012.
[67] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
[68] N. Otsu. A threshold selection method from gray-level histograms. IEEE transactions on systems, man,
and cybernetics , 9(1):62–66, 1979.
[69] T.O. Kohonen and M. Ruohonen. Representation of Associated Data by Matrix Operators. IEEE
Transactions on Computers , C-22(7):701–702, 1973.
[70] S. B¨ os. Statistical mechanics approach to early stopping and weight decay. Physical Review E ,
58(1):833, 1998.

--- TRANG 28 ---
[71] E. Agliari, F. Alemanno, A. Barra, and G. De Marzo. The emergence of a concept in shallow neural
networks. Neural Networks , 148:232–253, 2022.
[72] G.A. Christos. Investigation of the crick-mitchison reverse-learning dream sleep hypothesis in a
dynamical setting. Neural Networks , 9(3):427–434, 1996.
[73] A. Yadav, H. Shokeen, and J. Yadav. Disjoint set union for trees. In 2021 12th International Conference
on Computing Communication and Networking Technologies (ICCCNT) , pages 1–6. IEEE, 2021.
[74] C. Baldassi, C. Lauditi, E. Malatesta, G. Perugini, and R. Zecchina. Unveiling the structure of wide flat
minima in neural networks. Physical Review Letters , 127:278301, 2021.
[75] C. Baldassi, F. Pittorino, and R. Zecchina. Shaping the learning landscape in neural networks around
wide flat minima. Proceedings of the National Academy of Sciences , 117(1):161–170, 2020.
