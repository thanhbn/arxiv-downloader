# Điểm chuẩn BELEBELE:
một Bộ dữ liệu Đọc hiểu Song song trong 122 Biến thể Ngôn ngữ

Lucas Bandarkar*§, Davis Liang*†, Benjamin Muller*,
Mikel Artetxe*‡, Satya Narayan Shukla*, Donald Husa*, Naman Goyal*,
Abhinandan Krishnan*, Luke Zettlemoyer*, Madian Khabsa*
Meta AI* Abridge AI† University of California, Los Angeles§ Reka AI‡

## Tóm tắt
Chúng tôi trình bày BELEBELE, một bộ dữ liệu đọc hiểu máy (MRC) trắc nghiệm nhiều lựa chọn bao trùm 122 biến thể ngôn ngữ. Mở rộng đáng kể phạm vi ngôn ngữ của các điểm chuẩn hiểu ngôn ngữ tự nhiên (NLU), bộ dữ liệu này cho phép đánh giá các mô hình văn bản trong các ngôn ngữ có tài nguyên cao, trung bình và thấp. Mỗi câu hỏi dựa trên một đoạn văn ngắn từ bộ dữ liệu FLORES-200 và có bốn câu trả lời trắc nghiệm. Các câu hỏi đã được tuyển chọn cẩn thận để phân biệt giữa các mô hình có các mức độ hiểu ngôn ngữ tổng quát khác nhau. Riêng bộ dữ liệu tiếng Anh đã đủ khó để thách thức các mô hình ngôn ngữ tiên tiến. Là hoàn toàn song song, bộ dữ liệu này cho phép so sánh trực tiếp hiệu suất của mô hình trên tất cả các ngôn ngữ. Chúng tôi sử dụng bộ dữ liệu này để đánh giá khả năng của các mô hình ngôn ngữ có mặt nạ đa ngôn ngữ (MLM) và các mô hình ngôn ngữ lớn (LLM). Chúng tôi trình bày các kết quả và phát hiện rộng rãi, đáng chú ý là mặc dù có sự truyền đạt đa ngôn ngữ đáng kể trong các LLM lấy tiếng Anh làm trung tâm, các MLM nhỏ hơn nhiều được đào tạo trước trên dữ liệu đa ngôn ngữ cân bằng vẫn hiểu nhiều ngôn ngữ hơn nhiều. Nhìn chung, BELEBELE mở ra những hướng mới để đánh giá và phân tích khả năng đa ngôn ngữ của các hệ thống NLP.

## 1 Giới thiệu
Việc thiếu các điểm chuẩn đánh giá song song chất lượng cao là một trở ngại lớn trong việc đánh giá khả năng hiểu văn bản của các mô hình đa ngôn ngữ. Các bộ dữ liệu NLP có phạm vi ngôn ngữ rộng tồn tại, như FLORES-200 (NLLB et al., 2022), nhưng chủ yếu tập trung vào dịch máy. Các điểm chuẩn đánh giá đa ngôn ngữ phổ biến, như trả lời câu hỏi đa ngôn ngữ (Lewis et al., 2020; Clark et al., 2020), suy luận ngôn ngữ tự nhiên (NLI) (Conneau et al., 2018), và tóm tắt (Ladhak et al., 2020; Hasan et al., 2021), tất cả chỉ bao phủ khoảng 30 ngôn ngữ. Và trong khi các dịch vụ văn bản hiểu và sinh được sử dụng trên toàn cầu với hơn 100 ngôn ngữ, việc thiếu dữ liệu được gán nhãn cung cấp một trở ngại lớn cho việc xây dựng các hệ thống chức năng trong hầu hết các ngôn ngữ.

Đồng thời, các mô hình ngôn ngữ lớn (LLM) đã trở nên ngày càng phổ biến. Một số LLM nhất định, như BLOOM (Scao et al., 2022), được đào tạo trên dữ liệu đa ngôn ngữ và khoe khoang khả năng đa ngôn ngữ bẩm sinh của chúng. Những mô hình khác như GPT-3 (Brown et al., 2020) và LLAMA (Touvron et al., 2023a) đã thể hiện năng lực đa ngôn ngữ mặc dù dữ liệu đào tạo của chúng chủ yếu bằng tiếng Anh. Tuy nhiên, các LLM được hưởng lợi từ dữ liệu đào tạo trước có tính đa dạng ngôn ngữ, có chủ ý hoặc không, cũng như từ việc truyền đạt đa ngôn ngữ (Zoph et al., 2016; Artetxe et al., 2020; Muller et al., 2021b). Nhưng các mô hình này đa ngôn ngữ đến mức nào thực sự? Ngoài LLM, cần có tiến bộ khoa học đáng kể trước khi các hệ thống NLP có thể được xây dựng một cách hiệu quả và hiệu suất trong các ngôn ngữ có tài nguyên thấp. Nhiều kỹ thuật mô hình hóa đang được trình bày như độc lập với ngôn ngữ nhưng chỉ thực sự được đánh giá trong một số lượng nhỏ ngôn ngữ (Bender, 2011), có nguy cơ không áp dụng được cho các hiện tượng đa dạng về mặt loại hình học (Bender, 2009). Chúng tôi tin rằng các bộ dữ liệu quy mô lớn, song song và phân biệt là quan trọng để nghiên cứu khả năng đa ngôn ngữ của các mô hình như vậy và hiểu cách sự chênh lệch công nghệ giữa các ngôn ngữ có tài nguyên cao và thấp đang phát triển.

Trong bài báo này, chúng tôi trình bày một điểm chuẩn hiểu ngôn ngữ tự nhiên cơ bản để đánh giá các mô hình ngôn ngữ trên 122 biến thể ngôn ngữ từ khắp nơi trên thế giới¹, gọi là BELEBELE². Bộ dữ liệu chứa 900 đoạn văn và câu hỏi đọc hiểu trắc nghiệm độc nhất. Các câu hỏi đã được thiết kế cẩn thận để phân biệt giữa các mô hình có năng lực khác nhau trong hiểu ngôn ngữ. Mặc dù các câu hỏi không nhất thiết yêu cầu mức độ kiến thức hoặc lý luận cao hơn, chúng ủng hộ các mô hình NLU có thể tổng quát hóa và cố tình trừng phạt các mô hình thiên lệch. Riêng các câu hỏi tiếng Anh đã tạo ra một thách thức đáng kể cho nhiều mô hình, trong khi con người có khả năng trả lời các câu hỏi với độ chính xác gần như hoàn hảo.

Đầu tiên về quy mô này, BELEBELE là song song trên tất cả các ngôn ngữ, tạo điều kiện so sánh trực tiếp hiệu suất mô hình trên tất cả các ngôn ngữ. Bộ dữ liệu bao phủ các ngôn ngữ đa dạng về mặt loại hình học trên 29 hệ thống chữ viết và 27 họ ngôn ngữ. Bảy ngôn ngữ được bao gồm trong hai hệ thống chữ viết riêng biệt, dẫn đến một trong những điểm chuẩn NLP đầu tiên cho các biến thể được La tinh hóa của tiếng Hindi, Urdu, Bengali, Nepal và Sinhala. Chúng tôi tiếp tục chi tiết quy trình thu thập dữ liệu và kho tài liệu kết quả trong Phần 3.

Bộ dữ liệu cho phép đánh giá các mô hình đơn và đa ngôn ngữ, nhưng bản chất song song cũng cho phép một số thiết lập đánh giá đa ngôn ngữ. Chúng tôi đánh giá một số mô hình ngôn ngữ có mặt nạ (MLM) sau khi tinh chỉnh trên một tập đào tạo tiếng Anh cũng như với sự hỗ trợ của dịch máy (Translate-Train-All). Đối với LLM, chúng tôi đánh giá một số mô hình sử dụng Học trong Ngữ cảnh và cũng các mô hình được điều chỉnh hướng dẫn qua Zero-Shot. Chúng tôi thảo luận về kết quả của chúng tôi trong Phần 5.

## 2 Bối cảnh

### 2.1 Điểm chuẩn Đánh giá Đa ngôn ngữ
Có một số bộ dữ liệu cho NLU là song song trên nhiều ngôn ngữ và cho phép đánh giá đơn ngôn ngữ, đa ngôn ngữ, hoặc đa ngôn ngữ. Bao gồm XNLI (Conneau et al., 2018), XQUAD (Artetxe et al., 2020), và MLQA (Lewis et al., 2020). MINTAKA (Sen et al., 2022) được thiết kế với LLM trong tâm trí, trình bày một nhiệm vụ QA khó hơn trong 9 ngôn ngữ. Ngoài QA, XL-SUM (Hasan et al., 2021) là một bộ dữ liệu tương tự trong lĩnh vực tóm tắt trừu tượng. Tuy nhiên, tất cả các bộ dữ liệu này cùng nhau bao phủ dưới 30 ngôn ngữ, hầu hết trong đó là có tài nguyên cao hoặc trung bình. MASSIVE (FitzGerald et al., 2023) là một bộ dữ liệu NLU lớn bao phủ 51 ngôn ngữ, nhưng trong lĩnh vực các tác nhân hội thoại nói. NER (Pan et al., 2017) có phạm vi ngôn ngữ rộng rãi và TYDIQA (Clark et al., 2020) là một điểm chuẩn đa ngôn ngữ phổ biến nhưng không phải là song song.

Công việc của chúng tôi thực hiện thách thức mở rộng các đánh giá đa ngôn ngữ hiện có lên 122 ngôn ngữ, nhiều trong số đó hiện tại thiếu bất kỳ điểm chuẩn NLU nào.

### 2.2 Đọc hiểu Máy không phải tiếng Anh
Mặc dù phần trả lời câu hỏi khác nhau, các nhiệm vụ đọc hiểu máy (MRC) được định nghĩa bởi đoạn văn sách kín được cung cấp để trả lời mỗi câu hỏi. Tất nhiên, phần lớn các bộ dữ liệu MRC là bằng tiếng Anh, như TRIVIA QA (Joshi et al., 2017) và BABItasks (Weston et al., 2016).

Tuy nhiên, nhu cầu về các bộ dữ liệu MRC cho các ngôn ngữ khác đã dẫn đến sự gia tăng các bộ dữ liệu MRC sách kín đơn ngôn ngữ trong những năm gần đây (Mozannar et al., 2019; Hardalov et al., 2019; d'Hoffschmidt et al., 2020; Möller et al., 2021; Anuranjana et al., 2019; Gupta et al., 2018; Croce et al., 2018; Efimov et al., 2020; Shavrina et al., 2020; Sun et al., 2021). Hầu hết được tạo ra bằng cách sử dụng dịch thuật và vì vậy là song song với một bộ dữ liệu tiếng Anh, thường là SQUAD (Rajpurkar et al., 2016). Tuy nhiên, BELEBELE nhằm mục đích bao phủ các ngôn ngữ này và nhiều hơn nữa trong một bộ dữ liệu nhất quán.

### 2.3 QA Trắc nghiệm
So với QA trích xuất, trắc nghiệm là một dạng ít phổ biến hơn của các bộ dữ liệu MRC. Một số, như RACE (Lai et al., 2017), được tạo ra từ các câu hỏi thi cho người học tiếng Anh, trong khi những cái khác được xây dựng cụ thể cho các hệ thống NLU, như MCTest (Richardson et al., 2013) và MultiRC (Khashabi et al., 2018). Mặc dù hầu hết được dự định là sách kín, SCIQ (Welbl et al., 2017) và OPENBOOK QA (Mihaylov et al., 2018) yêu cầu truy xuất thông tin mở. Những cái khác, như COPA (Roemmele et al., 2011), SWAG (Zellers et al., 2018), và RECLOR (Yu et al., 2020), yêu cầu lý luận hiểu biết thông thường cấp cao hơn để trả lời. Đối với các hệ thống đa ngôn ngữ, EXAMS (Hardalov et al., 2020) là một bộ dữ liệu QA trắc nghiệm song song bao phủ 28 ngôn ngữ. Tuy nhiên, không có đoạn văn nào được cung cấp và việc trả lời câu hỏi yêu cầu chuyển giao kiến thức đa ngôn ngữ và lý luận.

### 2.4 FLORES-200
Các đoạn văn trong kho tài liệu BELEBELE được lấy trực tiếp từ Điểm chuẩn Dịch máy FLORES-200 (Goyal et al., 2022; NLLB et al., 2022). Bộ dữ liệu song song được xây dựng bằng cách lấy nguồn các đoạn văn tiếng Anh từ Wikinews, Wikijunior, và WikiVoyage. Các bản dịch được thực hiện bởi người bản xứ với trình độ tiếng Anh cao và kinh nghiệm dịch thuật. Các dịch giả được hướng dẫn duy trì nội dung thông tin và chuẩn hóa trong khi xử lý các thực thể được đặt tên, từ viết tắt, thành ngữ và đại từ một cách thích hợp.

## 3 Bộ dữ liệu BELEBELE
Chúng tôi đã lựa chọn tạo ra các câu hỏi và câu trả lời trắc nghiệm bằng tiếng Anh và sau đó dịch, thay vì tạo ra các tài nguyên bản địa trong mỗi ngôn ngữ. Nhiều lợi ích của cách tiếp cận này được nêu trong Conneau et al. (2018) vẫn còn. Quan trọng nhất, điều này dẫn đến các tập mẫu tương tự đáng kể hơn trên các ngôn ngữ, cho phép so sánh điểm số trực tiếp. Quy trình tạo bộ dữ liệu được tóm tắt trong Hình 2.

### 3.1 Tạo Câu hỏi và Câu trả lời Trắc nghiệm
Để tạo bộ dữ liệu BELEBELE, đầu tiên chúng tôi xây dựng một bộ dữ liệu hỏi đáp bằng tiếng Anh.

Trong số các nhiệm vụ đọc hiểu máy, chúng tôi chọn các câu hỏi trắc nghiệm (MCQ) bởi vì nó sẽ dẫn đến đánh giá công bằng nhất trên các ngôn ngữ. Các nhiệm vụ liên quan, như trích xuất span, nhạy cảm hơn với sự khác biệt hình thái học, làm cho việc mở rộng quy mô lên nhiều ngôn ngữ trở nên khó khăn (Lewis et al., 2020). Ngoài ra, MCQ cho phép chúng tôi tập trung các câu hỏi vào thông tin được nêu rõ ràng trong đoạn văn, vì các câu hỏi có/không hoặc kéo theo (NLI) có thể dễ trả lời hơn với kiến thức bên ngoài được giữ trong các mô hình đã được đào tạo trước. Để các câu hỏi phân biệt chỉ giữa các mức độ hiểu ngôn ngữ khác nhau, chúng tôi có chủ ý tạo ra các câu hỏi không yêu cầu các mức độ xử lý thông tin cao hơn, như lý luận đa bước hoặc hiểu biết thông thường.

Việc xây dựng MCQ chất lượng cao phụ thuộc quan trọng nhất vào việc tạo ra các lựa chọn sai mạnh mẽ không quá rõ ràng là sai cũng không có thể đúng (Agarwal và Mannem, 2011; Richardson et al., 2013). Chúng tôi không muốn bộ dữ liệu đủ dễ cho các mô hình thiên lệch (ví dụ: các mô hình sử dụng lối tắt hoặc khớp mẫu) (Boyd-Graber và Börschinger, 2020). Trong việc thiết lập chú thích này, chúng tôi xem xét các giao thức được đề xuất trong Bowman et al. (2020) và các cảnh báo từ Malaviya et al. (2022). Chúng tôi thực hiện một quy trình lặp với Nhà cung cấp Dịch vụ Ngôn ngữ (LSP) cho nhiệm vụ thu thập dữ liệu phức tạp này, tương tự như từ Nangia et al. (2021). Chúng tôi tham gia vào tổng cộng 5 lần lặp, cung cấp và nhận phản hồi trong mỗi lần. Các chú thích viên được hướng dẫn về những điểm tương đồng và khác biệt về cách các mô hình ML tiếp cận các bộ dữ liệu QA so với con người, điều mà chúng tôi cảm thấy cải thiện đáng kể chất lượng của dữ liệu.

Các hướng dẫn cuối cùng của chúng tôi bao gồm cả những điểm quan trọng như có phản hồi đúng là không mơ hồ, cũng như các quy tắc cụ thể như không có phủ định kép (Mihaylov et al., 2018). Đối với mỗi quy tắc, chúng tôi cung cấp cho các chú thích viên một ví dụ tốt và xấu để minh họa. Một phiên bản rút gọn của các hướng dẫn của chúng tôi có thể được tìm thấy trong Phụ lục A.5.1.

### 3.2 Đảm bảo Chất lượng
Ở mỗi lần lặp, chúng tôi đánh giá xem các mẫu được trả về có đáp ứng tiêu chuẩn chất lượng tối thiểu hay không thông qua việc kiểm tra thủ công và kiểm tra tự động. Ở mỗi bước, chúng tôi kiểm tra thủ công một mẫu câu hỏi để hiểu mức độ đồng tình của các chú thích viên với chúng tôi về các hướng dẫn. Mặc dù tốn thời gian, việc kiểm tra thủ công là cách chắc chắn nhất để cung cấp phản hồi cụ thể cho các chú thích viên, đáng chú ý về độ khó của các câu hỏi được tạo ra. Khi chúng tôi dần dần phối hợp với các chú thích viên, chúng tôi được yêu cầu xem xét nhiều mẫu hơn để cung cấp phản hồi.

Để bổ sung cho việc kiểm tra thủ công một tập con câu hỏi, chúng tôi sử dụng các phương pháp lập trình để đánh giá tất cả các câu hỏi từ góc độ thống kê. Dựa trên các phát hiện trong Malaviya et al. (2022), chúng tôi tạo ra các đặc trưng mức thấp để xác định các câu hỏi quá dễ hoặc các chiến lược nỗ lực thấp được các chú thích viên sử dụng. Ví dụ, chúng tôi đánh giá sự trùng lặp từ vựng giữa các kết hợp khác nhau của các văn bản liên quan đến một câu hỏi để đánh giá xem câu hỏi có thể trả lời được bởi một mô hình thiên lệch hay không. Điều này cho phép chúng tôi xem liệu câu hỏi có thể được trả lời mà không cần đoạn văn, không cần câu hỏi, hoặc chỉ với một câu trong đoạn văn. Chúng tôi cũng xác định các mẫu liên quan đến khả năng giải quyết theo kinh nghiệm, như các câu trả lời sai ít thường xuyên được trích xuất từ đoạn văn. Chúng tôi chi tiết các đặc trưng này trong Phụ lục A.6.

Các đặc trưng mức thấp này cho phép chúng tôi (1) xác định xem một lần lặp chú thích có đạt tiêu chuẩn hay không, (2) lọc ra các câu hỏi thất bại trong các kiểm tra kinh nghiệm này (cho lần lặp cuối cùng, khoảng 20% đã được lọc ra), và (3) so sánh với các bộ dữ liệu MCQ khác. Chúng tôi chạy các kiểm định t thống kê để đảm bảo phân phối của các đặc trưng này cho các câu trả lời đúng không khác biệt so với các câu trả lời sai. So với MCTEST mà phần lớn thất bại trong kiểm định t này (p-value < 0.01), bộ sưu tập cuối cùng của chúng tôi có p-value 0.81. Chúng tôi cũng đào tạo một mô hình hồi quy logistic để trả lời chỉ sử dụng các biểu diễn túi từ và thấy rằng điều tốt nhất mà mô hình ngây thơ có thể đạt được là độ chính xác 0.28 trên 900 câu hỏi của chúng tôi. Điều này chỉ tốt hơn một chút so với ngẫu nhiên (0.25) và thấp hơn nhiều so với những gì đã đạt được trên MCTEST, 0.44.

### 3.3 Dịch Kho tài liệu
BELEBELE được tạo ra từ đầu đến cuối mà không sử dụng công nghệ dịch máy, chỉ dựa vào các chuyên gia thông thạo tiếng Anh và ngôn ngữ đích.

Đối với tất cả các ngôn ngữ được bao gồm trong kho tài liệu, các đoạn văn ngữ cảnh được lấy trực tiếp từ bộ dữ liệu FLORES-200, ngoại trừ tiếng Hindi, Bengali, Urdu, Nepal và Sinhala trong chữ Latin. Mặc dù biến thể La tinh hóa của 5 ngôn ngữ Ấn-Aryan này rất phổ biến trên Internet hiện đại, việc La tinh hóa của chúng không được bao gồm trong FLORES-200. Do đó, chúng tôi có các chú thích viên chuyển chữ từ chữ bản địa sang chữ Latin với sự hỗ trợ của IndicXlit (Madhani et al., 2023). Kết quả là, giống như tiếng Ả Rập Chuẩn hiện đại, các ngôn ngữ này có mặt dưới hai hình thức trong kho tài liệu.

Để các câu hỏi và câu trả lời ghép đôi đúng cách với các đoạn văn FLORES đã được dịch, đoạn văn sau đã được cung cấp cho các chú thích viên. Chúng tôi cụ thể hướng dẫn các chú thích viên phối hợp các bản dịch có thể mơ hồ với các đoạn văn gốc. Trong khi Clark et al. (2020) cảnh báo rằng việc buộc phối hợp này có thể tăng 'translationese', nó là cần thiết để đảm bảo độ khó câu hỏi tương đương trên các ngôn ngữ. Các sửa đổi cho các hướng dẫn dịch có thể được tìm thấy trong Phụ lục A.5.2. Tất cả các bản dịch đều được đọc lại và chỉnh sửa bởi một chú thích viên bổ sung.

### 3.4 Dữ liệu Đào tạo tiếng Anh
BELEBELE dự định được sử dụng như một tập kiểm tra, và không phải để đào tạo. Do đó, đối với các mô hình yêu cầu tinh chỉnh nhiệm vụ bổ sung, thay vào đó chúng tôi lắp ráp một tập đào tạo bao gồm các mẫu từ các bộ dữ liệu QA trắc nghiệm tiếng Anh (Xem Phụ lục A.2).

### 3.5 Bộ dữ liệu BELEBELE Tóm tắt
BELEBELE chứa 900 câu hỏi, mỗi câu có 4 câu trả lời trắc nghiệm và một câu trả lời đúng. Hầu hết các đoạn văn có hai câu hỏi liên quan, nhưng một số chỉ có một. Tổng cộng, có 488 đoạn văn riêng biệt, không thuộc tập kiểm tra ẩn FLORES. Song song trên 122 ngôn ngữ, kho tài liệu chứa tổng cộng 109.800 hàng. Trong số các biến thể ngôn ngữ, có 29 hệ thống chữ viết độc nhất và 27 họ ngôn ngữ được đại diện (xem Hình 4). Một số thống kê văn bản và ngôn ngữ được hiển thị trong Bảng 1 và chúng tôi hiển thị một đoạn văn mẫu trong bốn ngôn ngữ trong Hình 1.

Bởi vì quy trình chú thích cẩn thận và kiểm tra chất lượng, các MCQ phân biệt năng lực hiểu văn bản. Nó thường bao gồm diễn giải và các lựa chọn sai mạnh mẽ để tránh các mô hình khớp mẫu đơn giản. Các câu hỏi thường bổ sung yêu cầu hiểu nhiều câu. Tuy nhiên, việc trả lời không yêu cầu các giả định hoặc kiến thức bên ngoài như được yêu cầu trong các bộ dữ liệu lý luận khó khăn hơn. Ví dụ, Q1 trong Hình 1 là không mơ hồ. Thức ăn, bạn đời, và bay đều được đề cập trong đoạn văn, nhưng việc đọc cẩn thận tiết lộ việc gấp cánh về phía sau chỉ liên quan đến nơi trú ẩn. Để tự tin loại trừ các câu trả lời ứng viên khác, cần phải hiểu ba câu. Nói chung, chúng tôi thấy tất cả các câu hỏi có thể trả lời được bởi con người thông thạo ngôn ngữ đích, nhưng không phải mà không cần đọc tập trung (xem Phần 5.1).

Như có thể thấy trong Hình 1, các đoạn văn, câu hỏi và câu trả lời được phối hợp về ý nghĩa ngữ nghĩa và tính trang trọng. Do đó BELEBELE đặt ra một thách thức tương đương trong tất cả các ngôn ngữ. Nó cũng cho phép các mô hình có sự phối hợp đa ngôn ngữ trong không gian biểu diễn ngữ nghĩa trả lời các câu hỏi khi đoạn văn, câu hỏi và câu trả lời được hoán đổi sang các ngôn ngữ khác nhau. Vì FLORES bao gồm các đoạn văn trong 83 ngôn ngữ bổ sung, chúng tôi thậm chí có thể đánh giá đọc hiểu trong các ngôn ngữ này bằng cách đặt câu hỏi bằng tiếng Anh.

## 4 Thí nghiệm
Nhờ BELEBELE, chúng tôi có thể đánh giá nhiều mô hình và thiết lập hiệu suất cơ bản trên 122 biến thể ngôn ngữ. Chúng tôi so sánh hiệu suất giữa các MLM và LLM đa ngôn ngữ phổ biến trong một số thiết lập. Đối với tất cả, độ chính xác là thước đo trung tâm. Với 4 câu trả lời ứng viên cho mỗi câu hỏi, độ chính xác dự kiến cho các mô hình phân loại chuỗi đoán ngẫu nhiên là 0.25.³

### 4.1 Các Mô hình Được Đánh giá

**Mô hình Ngôn ngữ Có mặt nạ (MLM)** Chúng tôi đánh giá ba mô hình khác nhau, XLM-V (Liang et al., 2023), INFOXLM (Chi et al., 2021), và XLM-R (Conneau et al., 2020a). Tất cả các MLM được đánh giá đều được đào tạo trước trên các kho tài liệu đa ngôn ngữ có chủ ý bao gồm khoảng 100 ngôn ngữ. Dữ liệu đào tạo trước trong các ngôn ngữ có tài nguyên cao thường được lấy mẫu xuống trong khi các ngôn ngữ có tài nguyên thấp được lấy mẫu lên để ủng hộ hiệu suất đa ngôn ngữ (Conneau et al., 2020a). Ngoài ra, tất cả các tokenizer từ phụ của chúng (Kudo và Richardson, 2018) đều được đào tạo trên các kho tài liệu đa ngôn ngữ, làm cho chúng phù hợp hơn cho văn bản đa ngôn ngữ.

**Mô hình Ngôn ngữ Lớn** Chúng tôi đánh giá GPT3.5-TURBO, FALCON, và LLAMA (1 và 2). GPT3.5-TURBO là một mô hình được tối ưu hóa cho trò chuyện dựa trên GPT-3 (Brown et al., 2020) có sẵn thông qua API OpenAI⁴. Các chi tiết hạn chế đã được tiết lộ về dữ liệu đào tạo trước và tinh chỉnh.⁵ LLAMA 1 (Touvron et al., 2023a) là một bộ sưu tập các mô hình transformer chỉ giải mã được đào tạo trên 1T (cho 7B, 13B) hoặc 1.4T (cho 30B, 65B) token của dữ liệu trực tuyến có sẵn công khai, trong khi LLAMA 2 (Touvron et al., 2023b) được đào tạo trước trên khoảng 2T. Chúng tôi đánh giá tất cả bốn checkpoint đã được đào tạo trước cho LLAMA 1. Chúng tôi đánh giá cả phiên bản đã được đào tạo trước của LLAMA 2 70B và phiên bản trò chuyện được tinh chỉnh hướng dẫn cho mục đích đối thoại an toàn (a.k.a. LLAMA-2-CHAT). Chúng tôi cũng đánh giá FALCON 40B, được đào tạo trước trên 1T mẫu được thu thập từ web được lọc rộng rãi (Penedo et al., 2023).

Mặc dù LLAMA 1 được báo cáo là được đào tạo trong 20 ngôn ngữ với hệ thống chữ viết Latin và Cyrillic, văn bản không phải tiếng Anh chiếm ít hơn 4.5% kho tài liệu đào tạo trước (Touvron et al., 2023a). Dữ liệu đào tạo trước LLAMA 2 được tạo thành từ 89.7% dữ liệu tiếng Anh, 8.4% không xác định, và 1.9% nhỏ thuộc về 26 ngôn ngữ khác⁶ (Touvron et al., 2023b). Cả hai series đều sử dụng cùng các tokenizer dựa trên BPE (Kudo và Richardson, 2018). Việc chia các ký tự unicode thành byte cũng giúp LLAMA tránh lỗi ngoài từ vựng.

Trong Bảng 2, chúng tôi trình bày kết quả cho BLOOMZ-7B (Muennighoff et al., 2023), được đào tạo trước và tinh chỉnh hướng dẫn trên dữ liệu đa ngôn ngữ nặng và có khả năng từ vựng đáng kể. Tuy nhiên, chúng tôi không thảo luận về nó trong các phân tích của chúng tôi, vì nó được tinh chỉnh cho dịch thuật trên FLORES-200, làm suy yếu đánh giá công bằng.

### 4.2 Thiết lập Đánh giá
Các chi tiết cụ thể hơn được cung cấp trong Phụ lục A.4.

**Tinh chỉnh Mô hình Đầy đủ** Để đánh giá MLM, chúng tôi thêm một đầu phân loại trắc nghiệm và tinh chỉnh toàn bộ mô hình. Chúng tôi tinh chỉnh trong hai thiết lập, (1) bằng tiếng Anh và đánh giá chuyển giao đa ngôn ngữ zero-shot và (2) trên các mẫu được dịch máy của tập đào tạo sang tất cả các ngôn ngữ đích và đánh giá mỗi ngôn ngữ (Translate-Train-All).

**Học Trong Ngữ cảnh Năm Shot** Chúng tôi đánh giá LLAMA 1 và 2 đã được đào tạo trước cũng như FALCON 40B trong thiết lập năm shot. Các ví dụ được lấy mẫu từ tập đào tạo tiếng Anh và được nhắc cho mô hình. Để dự đoán, chúng tôi chọn câu trả lời có xác suất cao nhất và báo cáo điểm số tích lũy trung bình trong 3 lần chạy.

**Đánh giá Zero-Shot** Chúng tôi đánh giá GPT3.5 và LLAMA-2-CHAT (70B) trong Zero-Shot bằng cách mô tả nhiệm vụ với các hướng dẫn ngôn ngữ tự nhiên (bằng tiếng Anh). Chúng tôi trình bày đoạn văn, câu hỏi, và bốn câu trả lời có thể, và hướng dẫn mô hình cung cấp chữ cái của câu trả lời. Chúng tôi hậu xử lý câu trả lời và chấp nhận nhiều định dạng.⁷

Ngoài ra, chúng tôi nhắc LLAMA-2-CHAT với các hướng dẫn được dịch máy sang ngôn ngữ đích từ tiếng Anh. Ngược lại, chúng tôi đánh giá việc dịch máy các đoạn văn, câu hỏi, và câu trả lời trở lại tiếng Anh và nhắc chúng cho mô hình (Translate-Test). Thiết lập này cho phép chúng tôi so sánh hiểu trong ngôn ngữ với cách tiếp cận phổ biến cascading với dịch máy.

## 5 Kết quả
Chúng tôi cung cấp kết quả tóm tắt trong Bảng 2 và kết quả chi tiết trong Phụ lục A.7.

### 5.1 BELEBELE khó đến mức nào?
Như đã thảo luận trong Phần 3, các câu hỏi trong BELEBELE có chủ ý khó. Mặc dù thách thức chính của bộ dữ liệu này là tính đa ngôn ngữ của nó, chúng tôi thấy rằng về mặt thực nghiệm, các câu hỏi tiếng Anh có thể làm sáng tỏ các khả năng NLU khác nhau của các mô hình. Với tinh chỉnh đầy đủ, chúng tôi đạt được độ chính xác tối đa 71.7 bằng tiếng Anh với mô hình ROBERTA-base, đáng kể ít hơn 90.9 đạt được bởi LLAMA 2 70B trong năm shot. Giữa các biến thể LLAMA 1, chúng tôi thấy một loạt kết quả rộng, với mô hình 7B chỉ đạt được 37.3. Vì vậy, trong khi khó khăn chính của BELEBELE là tính đa ngôn ngữ của nó, chúng tôi thấy một loạt hiệu suất rộng giữa các loại và kích thước mô hình khác nhau.

Ngoài ra, tất cả các mô hình được đánh giá đều hoạt động thoải mái dưới mức con người. Để thiết lập hiệu suất con người, 4 tác giả mỗi người ngẫu nhiên lấy mẫu khoảng 30 MCQ tiếng Anh và trả lời trong một bài kiểm tra mù, đạt được độ chính xác trung bình 97.6⁸. Điều này cao hơn nhiều so với bất kỳ mô hình nào được đánh giá, ngụ ý rằng nhiệm vụ đặt ra một thách thức cụ thể cho các mô hình và có chỗ để cải thiện. Để so sánh, Nangia và Bowman (2019) ước tính bảo thủ rằng hiệu suất con người là 92.8 trên phần tiếng Anh của XNLI (tức là MNLI (Williams et al., 2018)).

Khi so sánh hiệu suất mô hình với XNLI, chúng tôi thấy tương quan rất cao. Trong thiết lập Translate-Train-All, XLM-V, INFOXLM, và XLM-R đều hoạt động thấp hơn khoảng 10 điểm độ chính xác trên BELEBELE so với trên XNLI Translate-Train⁹ được báo cáo trong Liang et al. (2023) và Chi et al. (2021). Tuy nhiên, trên tất cả 15 ngôn ngữ và ba mô hình, chúng tôi thấy tương quan điểm số r = 0.85.

### 5.2 Khả năng tổng quát hóa Đa ngôn ngữ của MLM và LLM trên BELEBELE
Một cách lược đồ, hiệu suất của một mô hình ngôn ngữ trong một ngôn ngữ nhất định liên quan đến hai yếu tố chính. (i) Đầu tiên, lượng dữ liệu đào tạo trước trong ngôn ngữ đích. Như được dự đoán bởi các quy luật mở rộng (Kaplan et al., 2020), hiệu suất trong một ngôn ngữ tăng đơn điệu với lượng token đào tạo trước. (ii) Thứ hai, việc chuyển giao đa ngôn ngữ xảy ra giữa các ngôn ngữ trong dữ liệu đào tạo trước và ngôn ngữ đích tại thời điểm suy luận (Conneau et al., 2020a,b). Việc chuyển giao này được tác động bởi sự kết hợp của sự tương đồng loại hình học, hệ thống chữ viết, và từ vựng giữa các ngôn ngữ đào tạo trước và ngôn ngữ đích (Muller et al., 2021a, 2023). Hai yếu tố này khó tách rời do quy mô (lên đến ∼1T token) và các rò rỉ ngôn ngữ tiềm năng của các kho tài liệu đào tạo trước quy mô lớn (Kreutzer et al., 2022). Nhờ chất lượng và quy mô của BELEBELE, chúng tôi cung cấp bằng chứng chi tiết về cả hai tác động đến khả năng tổng quát hóa đa ngôn ngữ của các mô hình.

**Tác động của Phân phối Ngôn ngữ Đào tạo trước**
Một trong những khác biệt chính giữa các MLM và LLM được đánh giá là phân phối dữ liệu đào tạo trước và kích thước tham số của chúng, giải thích sự khác biệt hiệu suất lớn giữa chúng. Ví dụ, LLAMA 2 vượt trội đáng kể so với XLM-R trên các ngôn ngữ có tài nguyên cao, nhưng chỉ đạt được độ chính xác 50 trên khoảng một nửa số lượng ngôn ngữ so với XLM-R (Xem Bảng 2). Sự khác biệt này giữa các MLM và LLM được đánh giá được minh họa trong Hình 3. Tuy nhiên, mặc dù có khoảng cách này, tất cả các LLM được đánh giá đều hoạt động tốt đáng ngạc nhiên trên một số lượng lớn ngôn ngữ. Ví dụ, LLAMA-2-CHAT có độ chính xác trên 35 (tức là 10 trên ngẫu nhiên) cho 59 ngôn ngữ. Điều này cho thấy rằng các LLM lấy tiếng Anh làm trung tâm là một điểm khởi đầu đầy hứa hẹn để xây dựng các mô hình đa ngôn ngữ.

**Dịch máy cho Zero-Shot** Các đánh giá Translate-Test của chúng tôi cho thấy rằng việc sử dụng dịch máy sang tiếng Anh vượt trội mạnh mẽ so với hiệu suất LLAMA-2-CHAT (70B) trong ngôn ngữ đích gốc. Trên 91 ngôn ngữ được đánh giá, chỉ có 2 tốt hơn một cách không tầm thường trong ngôn ngữ (tiếng Đức và tiếng Ý), so với 68 (không có ngôn ngữ nào có tài nguyên cao) mà việc dịch sang tiếng Anh tốt hơn, không có ngôn ngữ nào trong số đó được coi là có tài nguyên cao. So với LLAMA-2-CHAT có độ chính xác zero-shot trên 50% cho 33 ngôn ngữ, nó có 71 trong Translate-Test (xem Phụ lục A.7.4).

Ngoài ra, chúng tôi đánh giá việc dịch máy các hướng dẫn nhiệm vụ sang ngôn ngữ đích. Đối với khoảng 25 ngôn ngữ, các hướng dẫn được dịch không được hiểu rõ (tức là độ chính xác ít hơn ngẫu nhiên), tương quan mạnh với các ngôn ngữ đã có điểm số thấp. Đối với phần còn lại, hiệu suất tương đối so với việc sử dụng hướng dẫn tiếng Anh là hỗn hợp, mặc dù các ngôn ngữ đã có điểm số cao có sự tăng độ chính xác lớn nhất từ hướng dẫn trong ngôn ngữ. Mặc dù việc dịch máy hướng dẫn ít hiệu quả hơn so với hướng dẫn trong ngôn ngữ chất lượng, các kết quả này không gợi ý rằng việc sử dụng hướng dẫn tiếng Anh là lý do chính tại sao hiệu suất trên các ngôn ngữ không phải tiếng Anh tụt hậu so với tiếng Anh một cách đáng kể.

**Tác động của Tokenization Từ phụ** Chúng tôi tái khẳng định tương quan giữa việc tăng kích thước từ vựng và hiệu suất trên các ngôn ngữ có tài nguyên thấp hơn (Liang et al., 2023). XLM-V có một từ vựng khổng lồ 900k token phân bổ khả năng cho mỗi ngôn ngữ riêng lẻ và giảm nhấn mạnh việc chia sẻ token giữa các ngôn ngữ. XLM-V vượt trội so với XLM-R và INFOXLM (kích thước từ vựng 250k) trên các ngôn ngữ có tài nguyên thấp mặc dù tất cả đều có cùng kiến trúc và được đào tạo trên cùng bộ dữ liệu (CC-100). GPT3.5-TURBO (kích thước từ vựng 100k)¹⁰, FALCON (kích thước từ vựng 65k), và LLAMA 2 (kích thước từ vựng 32k) đều giảm đột ngột cho các ngôn ngữ có tài nguyên trung bình và thấp. Kích thước từ vựng lớn hơn có thể giải thích tại sao FALCON 40B hoạt động tương đương với LLAMA 1 30B mặc dù được đào tạo trước trên ít token không phải tiếng Anh hơn.

**Hiệu ứng Mở rộng quy mô trên Khả năng tổng quát hóa Đa ngôn ngữ**
Chúng tôi báo cáo trong Hình 4 tác động của kích thước mô hình đến hiệu suất trên điểm chuẩn BELEBELE trên sáu họ ngôn ngữ và tiếng Anh. Chúng tôi thấy rằng quy mô là quan trọng đối với LLAMA 1 để thực hiện đọc hiểu vì checkpoint 7B hoạt động chỉ hơi trên cơ hội ngay cả bằng tiếng Anh. Khi kích thước tham số tăng, hiệu suất trên toàn bộ tăng đáng kể. Chỉ các checkpoint 30B và 65B mới có thể hoạt động không tầm thường trong các họ ngôn ngữ không được báo cáo có trong kho tài liệu đào tạo trước (tiếng Nhật và tiếng Hy Lạp). Tuy nhiên, không giống như các họ ngôn ngữ khác như Romance và Germanic, hiệu suất trở nên không tầm thường chỉ với các checkpoint 30B và 65B. Các kết quả như thế này gợi ý rằng việc tổng quát hóa đến các ngôn ngữ xa xôi sau khi đào tạo trước lấy tiếng Anh làm trung tâm yêu cầu nhiều tham số hơn.

**Tác động của Hệ thống chữ viết** So sánh các phiên bản La tinh hóa với các hệ thống chữ viết gốc cho tiếng Hindi, Urdu, Bengali, Sinhala, Nepal, và tiếng Ả Rập Chuẩn hiện đại, chúng tôi thấy rằng tất cả các mô hình ngoại trừ FALCON hoạt động mạnh hơn trong hệ thống chữ viết bản địa hơn trong hệ thống chữ viết Latin (xem Phụ lục A.7.3). Tuy nhiên, các hệ thống chữ viết bản địa được cho là không có trong dữ liệu đào tạo trước cho LLAMA 2 và FALCON. Đối với các ngôn ngữ Ấn-Aryan, chúng tôi giả thuyết rằng việc chuyển giao đa ngôn ngữ sẽ cao hơn trong biến thể Latin vì tokenization sẽ phù hợp hơn và có cơ hội cho các từ phụ được chia sẻ (điểm neo) (Conneau et al., 2020b; Muller et al., 2020; Pfeiffer et al., 2021; Muller et al., 2021a). Tuy nhiên, điều này dường như chỉ đúng với FALCON. Các kết quả nói chung gợi ý rằng các mô hình được đào tạo trước trên các mẫu đáng kể trong hệ thống chữ viết bản địa (có thể do chuyển đổi mã hoặc nhận dạng ngôn ngữ kém).

## 6 Kết luận
Một hạn chế cơ bản đối với việc tiến hành đánh giá đúng đắn về khả năng của các mô hình ngôn ngữ trong các ngôn ngữ có tài nguyên thấp, hoặc thậm chí vừa phải, là sự sẵn có của các điểm chuẩn được chú thích. Bài báo này trình bày một bộ dữ liệu khổng lồ, BELEBELE, bao gồm các đoạn văn và câu hỏi trắc nghiệm đánh giá đọc hiểu trong 122 ngôn ngữ. Điểm chuẩn này cho phép đánh giá quan trọng về khả năng đọc hiểu của các LLM bằng tiếng Anh và các ngôn ngữ hàng đầu. Ngoài ra, bộ dữ liệu là đầu tiên thuộc loại này trong nhiều ngôn ngữ có tài nguyên trung bình và thấp, cho phép hiểu biết chưa từng có về khả năng đa ngôn ngữ của các mô hình ngôn ngữ. Chúng tôi trình bày kết quả từ một số MLM và LLM phổ biến trong các thiết lập đánh giá khác nhau và thấy rằng mặc dù kích thước từ vựng lớn và dữ liệu đào tạo trước cân bằng tương quan với hiệu suất mô hình cao nhất trên các ngôn ngữ có tài nguyên trung bình và thấp, ngay cả các LLM lấy tiếng Anh làm trung tâm cũng có thể đi xa và tổng quát hóa đến hơn 30 ngôn ngữ.

Đối với công việc tương lai, chúng tôi hy vọng rằng nhiều đánh giá và thí nghiệm hiện có thể sẽ cho phép nghiên cứu sâu hơn về các mô hình ngôn ngữ hiện tại. BELEBELE cũng có thể bổ sung cho các điều tra về khả năng mô hình cụ thể (ví dụ: lý luận), dẫn đến hiểu biết rộng hơn về mối quan hệ giữa các khả năng như vậy và tính đa ngôn ngữ. Kết quả là, chúng tôi tin rằng BELEBELE sẽ sớm tiết lộ thêm những hiểu biết góp phần vào sự phát triển của các hệ thống NLP vượt ra ngoài các ngôn ngữ có tài nguyên cao.

## Hạn chế

**Tài liệu Đào tạo trước** Các phân tích mô hình của chúng tôi bị hạn chế bởi tài liệu không nhất quán về thành phần của các kho tài liệu đào tạo trước được sử dụng. Chúng tôi trình bày kết quả từ BLOOMZ (Muennighoff et al., 2023) chỉ theo cách rất hạn chế bởi vì tài liệu của nó cho thấy nó được tinh chỉnh cho dịch thuật trên FLORES-200. Tuy nhiên, như đã ám chỉ trong Phần 4.1, chúng tôi trình bày kết quả GPT3.5-TURBO trên BELEBELE mặc dù chúng tôi không thể xác minh dữ liệu đào tạo trước hoặc tinh chỉnh của nó. Bởi vì điều này, việc so sánh kết quả trên GPT3.5-TURBO với các mô hình khác có thể không công bằng do thiếu minh bạch về dữ liệu đào tạo. Để cho phép hiểu biết sâu hơn về sự tương tác của văn bản đa ngôn ngữ trong quá trình đào tạo, chúng tôi chỉ ra hai hướng nghiên cứu quan trọng để cho phép tiến bộ trong lĩnh vực này. Đầu tiên, (i) các hệ thống nhận dạng ngôn ngữ tốt hơn: các mô hình nhận dạng ngôn ngữ phổ biến được đào tạo trên một số lượng hạn chế ngôn ngữ và lĩnh vực và chỉ hoạt động ở mức câu (Bojanowski et al., 2017), hạn chế khả năng theo dõi ngôn ngữ trong dữ liệu chuyển đổi mã và văn bản nhúng. Thứ hai, (ii) chúng tôi khuyến khích các nhà phát triển LLM cải thiện báo cáo về phân phối ngôn ngữ đào tạo trước. Điều này là cần thiết để cộng đồng nghiên cứu hiểu khả năng chuyển giao đa ngôn ngữ của các LLM và cải thiện thiết kế hệ thống NLP cho các ngôn ngữ có tài nguyên thấp.

**Lỗi trong FLORES** Như đã đề cập ngắn gọn trong Phần 3.3, các chú thích viên đã phát hiện ra một số vấn đề chất lượng với các bản dịch FLORES (tức là các chú thích gốc được thực hiện trước công việc này). Một số trong số chúng có thể do sự khác biệt về phong cách/phương言 giữa các chú thích viên, nhưng nhiều cái dường như không phải vậy. Nó khá hiếm, nhờ các vòng lặp đảm bảo chất lượng rộng rãi được thực hiện bởi nhóm NLLB và LSP. Tuy nhiên, trên quy mô 122 ngôn ngữ, một số lượng công bằng các vấn đề đã phát sinh, đặc biệt là trong các ngôn ngữ có tài nguyên thấp hơn. Vì việc cập nhật bộ dữ liệu FLORES cơ bản không nằm trong phạm vi của dự án này, chúng tôi đã cân nhắc từng cái với LSP để tối đa hóa cả tính thích hợp và tính nhất quán đa ngôn ngữ của các bản dịch câu hỏi/câu trả lời.

**Translationese** Ngay cả với đảm bảo chất lượng rộng rãi của chúng tôi, chúng tôi cảnh báo rằng "translationese" có thể thay đổi bản chất của nhiệm vụ trên các ngôn ngữ. Trong nhiều trường hợp, bản dịch hoàn hảo không tồn tại. Điều này có thể khiến độ chính xác trên các ngôn ngữ không phải tiếng Anh không thể so sánh trực tiếp với tiếng Anh.

## Tuyên bố Đạo đức

**Mã nguồn mở** Quyết định mã nguồn mở BELEBELE của chúng tôi có thể làm tổn hại đến việc đánh giá điểm chuẩn trong tương lai vì các mẫu có thể được thu thập vào các kho tài liệu đào tạo trước lớn, làm suy yếu so sánh công bằng. Điều này đặc biệt đúng với đánh giá zero- hoặc few-shot. Tuy nhiên, chúng tôi có ý thức xác định rằng giá trị của việc mã nguồn mở toàn bộ bộ dữ liệu vượt xa những cân nhắc này.

**Chủ nghĩa lấy tiếng Anh làm trung tâm** BELEBELE được thiết kế để đo lường khả năng đọc hiểu của các hệ thống NLP trên 122 ngôn ngữ. Chúng tôi cụ thể phối hợp càng nhiều càng tốt với các lựa chọn dịch thuật được thực hiện trong việc tạo ra FLORES. Do đó, theo thiết kế, các mẫu được thu thập không nắm bắt các hiện tượng cụ thể về ngôn ngữ và văn hóa như tính trang trọng (Ersoy et al., 2023), giá trị (Kováč et al., 2023), và chủ đề (Hershcovich et al., 2022). Mặc dù có ý thức về chủ nghĩa Tây phương này, BELEBELE được thiết kế để ưu tiên khả năng so sánh trên các ngôn ngữ. Sau BELEBELE, việc xây dựng các hệ thống NLP bao gồm tất cả các nền văn hóa và ngôn ngữ sẽ yêu cầu phát hành các điểm chuẩn nắm bắt các hiện tượng này.
