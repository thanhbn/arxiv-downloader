# 2308.10379.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl-papers/2308.10379.pdf
# Kích thước tệp: 953713 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Thuật toán của Tư duy:
Tăng cường Khám phá Ý tưởng trong Mô hình Ngôn ngữ Lớn
Bilgehan Sel1Ahmad Al-Tawaha1Vanshaj Khattar1Ruoxi Jia1Ming Jin1

Tóm tắt
Văn học hiện tại, nhằm vượt qua phương pháp "Chain-
of-Thought", thường sử dụng các phương thức hoạt động bên ngoài
bao gồm dừng, sửa đổi, và sau đó tiếp tục
quá trình tạo sinh để tăng cường khả năng lý luận của
Mô hình Ngôn ngữ Lớn (LLMs). Do góc nhìn cận thị của họ, họ
làm tăng số lượng yêu cầu truy vấn, dẫn đến
tăng chi phí, bộ nhớ và tải tính toán. Để giải quyết
vấn đề này, chúng tôi đề xuất Thuật toán của Tư duy - một chiến lược
mới thúc đẩy LLMs thông qua các đường dẫn lý luận
thuật toán. Bằng cách sử dụng các ví dụ thuật toán
hoàn toàn trong ngữ cảnh, góc nhìn toàn diện này về toàn bộ
quá trình khai thác động lực tái diễn bẩm sinh
của LLMs, mở rộng khám phá ý tưởng của họ chỉ với
một hoặc một vài truy vấn. Kỹ thuật của chúng tôi vượt trội
hơn các phương pháp truy vấn đơn trước đó và thậm chí
các chiến lược đa truy vấn gần đây hơn sử dụng
thuật toán tìm kiếm cây mở rộng trong khi sử dụng
ít token hơn đáng kể. Thú vị là, kết quả của chúng tôi
cho thấy rằng hướng dẫn LLM sử dụng thuật toán
có thể dẫn đến hiệu suất vượt qua chính thuật toán đó,
gợi ý khả năng bẩm sinh của LLM trong việc kết hợp
trực giác của mình vào các tìm kiếm được tối ưu hóa. Chúng tôi
nghiên cứu sâu vào nền tảng của hiệu quả phương pháp
và các sắc thái trong ứng dụng.
Mã và nội dung liên quan có thể tìm thấy tại:
algorithm-of-thoughts.github.io.

1. Giới thiệu
Các phát triển gần đây trong mô hình ngôn ngữ lớn (Chowdh-
ery et al., 2022; Thoppilan et al., 2022; Liu et al., 2023,
trong số những người khác) đã làm nổi bật hiệu quả của họ trong giải quyết
vấn đề tổng quát (Huang & Chang, 2022; Suzgun et al., 2022),

1Khoa Kỹ thuật Điện và Máy tính, Đại học
Virginia Tech, Blacksburg, USA. Liên hệ: Bilgehan Sel
<bsel@vt.edu>.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy,
Vienna, Áo. PMLR 235, 2024. Bản quyền 2024 thuộc về
(các) tác giả.

tạo mã (Chen et al., 2021; Austin et al., 2021),
và tuân theo hướng dẫn (Ouyang et al., 2022; Bai et al.,
2022). Trong khi các mô hình sớm dựa vào chiến lược
trả lời trực tiếp (Brown et al., 2020), nghiên cứu đương đại đã
chuyển sang các đường dẫn lý luận tuyến tính (Wei et al., 2022b; Kojima
et al., 2022; Zhang et al., 2022) bằng cách chia nhỏ vấn đề
thành các nhiệm vụ phụ để khám phá giải pháp, hoặc sử dụng các cơ chế
bên ngoài để thay đổi việc tạo token bằng cách thay đổi ngữ cảnh
(Zhou et al., 2022a; Drozdov et al., 2022; Yao et al., 2023).

Tương tự như nhận thức con người (Sloman, 1996; Kahneman,
2011), các chiến lược LLM sớm dường như bắt chước
Hệ thống 1 tức thời, đặc trưng bởi việc ra quyết định
bốc đồng. Ngược lại, các phương pháp gần đây hơn như chuỗi
tư duy (CoT) (Wei et al., 2022b) và prompting từ ít đến nhiều
(L2M) (Zhou et al., 2022a; Drozdov et al., 2022)
phản ánh bản chất phân tích của Hệ thống 2. Đáng chú ý, việc tích hợp
các bước lý luận trung gian đã mang lại cải thiện
trong các nhiệm vụ lý luận số học (Srivastava et al., 2022; Liang
et al., 2022).

Tuy nhiên, khi các nhiệm vụ chuyển sang lập kế hoạch sâu hơn và
khám phá tư duy mở rộng, những phương pháp này có vẻ hạn chế.
Mặc dù CoT được tích hợp với Tự nhất quán (CoT-SC)
(Wang et al., 2022) thu hút nhiều đầu ra LLM để đạt
đồng thuận, việc thiếu đánh giá tỉ mỉ có thể dẫn đến
hướng sai của mô hình. "Cây Tư duy" (Yao et al., 2023;
Long, 2023) nổi lên như một giải pháp đáng chú ý. Trong khi một LLM
được dành riêng cho việc tạo ý tưởng, một LLM khác bước vào để đánh giá
giá trị của những ý tưởng này, theo chu kỳ dừng-đánh giá-
tiếp tục. Quá trình lặp đi lặp lại này, dựa trên tìm kiếm cây,
đã cho thấy hiệu quả rõ rệt, đặc biệt trong các nhiệm vụ
có độ rộng của phần tiếp theo. Chúng tôi thấy sự tiến bộ này
giống như con người sử dụng công cụ để vượt qua các hạn chế
bộ nhớ làm việc, đóng vai trò như một sự tăng cường bên ngoài
cho LLMs (Mialon et al., 2023; Sel et al., 2023; Gu et al.,
2024a).

Mặt khác, phương pháp LLM nâng cao này không phải không có
những cạm bẫy. Một nhược điểm nổi bật là sự gia tăng đáng kể
số lượng truy vấn và yêu cầu tính toán. Mỗi
truy vấn đến các API LLM trực tuyến như GPT-4—một điểm tập trung
của nghiên cứu chúng tôi—phát sinh chi phí tiền tệ (Chen et al., 2023)
nhưng cũng góp phần vào độ trễ, một hạn chế đáng kể
đặc biệt quan trọng trong các ứng dụng thời gian thực. Độ trễ tích lũy
từ những truy vấn này có thể ảnh hưởng đến hiệu quả giải pháp.
Về mặt cơ sở hạ tầng, các tương tác liên tục có thể gây áp lực lên
hệ thống, dẫn đến các hạn chế tiềm năng về băng thông và giảm
khả năng sẵn có của mô hình (Aminabadi et al., 2022). Hơn nữa,
những tác động môi trường không thể bỏ qua; việc truy vấn không ngừng
làm tăng tiêu thụ năng lượng của các trung tâm dữ liệu vốn đã tiêu
tốn nhiều điện năng, làm trầm trọng thêm dấu chân carbon (Wu
et al., 2022; Dhar, 2020; Khattar & Jin, 2023).

Với điều này trong tâm trí, mục tiêu của chúng tôi là giảm đáng kể
số lượng truy vấn được sử dụng bởi các phương pháp lý luận
đa truy vấn đương đại trong khi duy trì hiệu suất cho các nhiệm vụ
đòi hỏi sử dụng khéo léo kiến thức thế giới, từ đó hướng
tới việc sử dụng tài nguyên AI có trách nhiệm và thành thạo hơn.
Thú vị là, mục tiêu của chúng tôi thực sự đã dẫn đến việc vượt qua
hiệu suất của những kỹ thuật như vậy trong khi yêu cầu
ít token hơn đáng kể cho prompting và tạo sinh.

Phản ánh về sự tiến hóa của LLMs từ Hệ thống 1 đến Hệ
thống 2, một thành phần thiết yếu được làm sáng tỏ: thuật toán
(Sel et al., 2021; Al-Tawaha et al., 2023; Jin et al., 2023a;

Gu et al., 2024b). Được đặc trưng bởi bản chất có phương pháp,
góc nhìn thuật toán cung cấp một con đường để khám phá
không gian vấn đề một cách sắc bén, thực hiện chiến lược và công thức hóa giải pháp
(Al-Tawaha et al., 2021; Helie & Pizlo, 2022; Banerjee et al.,
2022; Sel et al., 2022; Khattar et al., 2022). Trong khi phần lớn
văn học hiện tại coi thuật toán như bên ngoài đối với
LLMs (Lin et al.), với khả năng tái diễn tạo sinh bẩm sinh của LLMs,
liệu chúng ta có thể hướng logic lặp đi lặp lại này để nội hóa một
thuật toán?

Dựa trên cả những sắc thái phức tạp của lý luận con người
và độ chính xác có kỷ luật của các phương pháp thuật toán,
công trình của chúng tôi nhằm hợp nhất hai yếu tố này để tăng cường
khả năng lý luận trong LLMs. Nghiên cứu hiện tại
nhấn mạnh rằng con người, khi điều hướng các vấn đề phức tạp,
bản năng dựa vào những nỗ lực trong quá khứ, đảm bảo một sự
chiêm nghiệm toàn diện hơn là tập trung hẹp (Monsell,
2003; Holyoak & Morrison, 2005; Baddeley, 2003). LLMs,
với khoảng tạo sinh chỉ bị giới hạn bởi giới hạn token, có vẻ
sẵn sàng phá vỡ các rào cản của bộ nhớ làm việc con người. Được thúc đẩy
bởi quan sát này, chúng tôi đã điều tra liệu

LLMs có thể phản ánh một khám phá ý tưởng nhiều tầng tương tự,
tham chiếu các bước trung gian trước đó để sàng lọc các tùy chọn
không khả thi, tất cả trong chu kỳ tạo sinh lặp đi lặp lại của họ.
Và trong khi con người xuất sắc với cái nhìn sâu sắc trực quan của họ, thuật toán
nổi bật với việc khám phá có tổ chức, có hệ thống. Các kỹ thuật
hiện tại, như CoT, thường tránh tiềm năng hiệp lực này,
áp đặt áp lực không đáng có lên LLMs để có độ chính xác
ngay lập tức. Bằng cách tận dụng khả năng đệ quy của LLMs,
chúng tôi bắt chước một phương pháp lai con người-thuật toán. Điều này
được thực hiện thông qua việc sử dụng các ví dụ thuật toán
nắm bắt bản chất của khám phá, từ ứng cử viên ban đầu
đến các giải pháp được xác thực. Do đó xuất hiện khái niệm
Thuật toán của Tư duy (AoT) của chúng tôi, như minh họa trong Hình 1 và 2.

Một cách rộng rãi hơn, phương pháp của chúng tôi đánh dấu một mô hình mới
của học trong ngữ cảnh. Thay vì khuôn mẫu "học có giám sát"
truyền thống của [VẤN ĐỀ, GIẢI PHÁP] hoặc [VẤN ĐỀ,
CÁC BƯỚC LIÊN TIẾP ĐẾN GIẢI PHÁP], chúng tôi trình bày một cấu trúc
mới bao gồm [VẤN ĐỀ, QUÁ TRÌNH TÌM KIẾM, GIẢI
PHÁP]. Tự nhiên, khi hướng dẫn LLM sử dụng thuật toán,
kỳ vọng nghiêng về việc LLM đơn giản
bắt chước tư duy lặp đi lặp lại của thuật toán. Tuy nhiên, điều
xuất hiện như thú vị là khả năng của LLM trong việc truyền "trực giác"
của riêng mình để đạt được hiệu quả tìm kiếm thậm chí vượt qua
chính thuật toán đó (xem Hình 5).

Trong các phần tiếp theo, trước tiên chúng tôi đặt công trình của mình trong
văn học hiện có, tiếp theo là thảo luận về ý tưởng
chính của chúng tôi. Sau đó chúng tôi trình bày kết quả thực nghiệm và khám phá
một loạt giả thuyết liên quan đến khả năng mới nổi này
của LLM trước khi kết thúc với kết luận.

2. Các Nghiên cứu Liên quan

Prompting Chuẩn. Còn được gọi là prompting đầu vào-đầu ra,
nó cung cấp một vài ví dụ đầu vào-đầu ra của nhiệm vụ
trước khi nhận một câu trả lời cho mẫu thử nghiệm từ
mô hình ngôn ngữ (Brown et al., 2020). Mặc dù phương pháp này
rất tổng quát và không cần bất kỳ chiến lược prompting đặc biệt nào,
hiệu suất cũng tệ hơn so với các phương pháp
tiên tiến hơn (Shao et al., 2023; Wei et al., 2022a; Lyu
et al., 2023).

Chuỗi Tư duy. Trong CoT, LLMs được trình bày với các ví dụ
trong đó một câu hỏi cho trước x được triển khai thông qua một chuỗi
các mảnh lý luận trung gian c1, . . . , cn để đạt được một câu
trả lời y, được biểu diễn như x→c1→. . .→cn→y (Wei
et al., 2022b; Lyu et al., 2023). Bằng cách bắt chước các ví dụ
trong ngữ cảnh, LLM tự động chia giải pháp
thành các bước tuyến tính đơn giản hơn để đi đến câu trả lời, cải thiện
hiệu suất trên nhiều benchmark lý luận. Tự nhất quán (Wang et al., 2022) là một chiến lược giải mã được sử dụng rộng rãi
nhằm tạo ra nhiều đường dẫn lý luận bằng cách
chọn câu trả lời cuối cùng thông qua bỏ phiếu đa số, mặc dù
điều này đòi hỏi các thế hệ bổ sung. CoT có thể được cải thiện
thêm với việc tích hợp lý luận thuật toán chi tiết
(Zhou et al., 2022b). Chúng tôi cũng sử dụng các ví dụ thuật toán
trong AoT, tuy nhiên, chúng dành cho việc xuất hiện heuristic bẩm sinh
của LLMs để dẫn dắt tìm kiếm và không được thiết kế để tuân theo
một mã giả được chỉ định, hoặc là trên các nhiệm vụ ngôn ngữ, ví dụ, viết
sáng tạo. Ngược lại với tiến triển tuyến tính của CoT, phương pháp của chúng tôi
xoay quanh khía cạnh khám phá của LLMs. Chúng tôi tái khái niệm
chuỗi c1, . . . , cn, không chỉ như các bước liên tiếp
hướng tới một giải pháp, mà như một con đường động, có thể
biến đổi tương tự như một tìm kiếm thuật toán, cho phép
khám phá, tái hiệu chỉnh và tiến triển phi tuyến tính.

Prompting Từ ít đến nhiều (L2M). Lấy gợi ý từ tâm lý
giáo dục (Libby et al., 2008), prompting L2M hướng dẫn
LLM phân tách vấn đề trung tâm thành các vấn đề phụ
nhỏ hơn. Mỗi vấn đề phụ được giải quyết theo
trình tự, với kết quả được thêm vào trước khi tiến triển đến
vấn đề tiếp theo (Zhou et al., 2022a; Drozdov et al., 2022). Trong khi
sự phân định có cấu trúc này có lợi cho việc tổng quát hóa
rộng hơn, nó hoạt động trên tiền đề tìm kiếm một sự phân tách
gần như hoàn hảo trong một nỗ lực duy nhất—lý tưởng cho các vấn đề
có cấu trúc rõ ràng. Tuy nhiên, khi các nhiệm vụ đan xen
với sự phức tạp phân tách của chúng (như trò chơi 24),
tính không linh hoạt của phương pháp này trở nên rõ ràng. Ngược lại,
AoT không chỉ nhấn mạnh vấn đề phụ đang hoạt động (như được hiển thị
trong Hình 1), mà còn cho phép một phương pháp chiêm nghiệm hơn
bằng cách xem xét các tùy chọn khác nhau cho mỗi vấn đề phụ, trong khi
duy trì hiệu quả ngay cả với các prompt tối thiểu.

Cây Tư duy (ToT). Trong những trường hợp mà mỗi vấn đề phụ
có nhiều tùy chọn khả thi để khám phá, các đường dẫn lý luận
tuyến tính từ CoT hoặc L2M hạn chế đáng kể phạm vi bao phủ
của không gian tư duy. Xem xét các tùy chọn có thể cho mỗi
vấn đề phụ, cây quyết định có thể được khám phá bằng các cơ chế
tìm kiếm cây bên ngoài (ví dụ, BFS, DFS) (Yao et al., 2023;
Jin et al., 2023b; Sel et al., 2024). Khả năng đánh giá
của LLMs cũng có thể được sử dụng để hướng dẫn tìm kiếm bằng cách cắt tỉa
các nút vô vọng để tăng hiệu quả. Tuy nhiên, ToT,
do yêu cầu của nó đối với nhiều truy vấn đến LLM cho
một giải pháp, đòi hỏi tính toán nhiều hơn đáng kể so với
AoT. Ngoài ra, nó đòi hỏi đánh giá tiềm năng
của mỗi nút tìm kiếm trong các ví dụ trong ngữ cảnh và viết
các hàm chuyên biệt để trích xuất thông tin từ phản hồi của mô hình
để duy trì cấu trúc cây bên ngoài. Ngược lại hoàn toàn, AoT
chỉ yêu cầu một prompt duy nhất và không có kỹ năng lập trình,
dân chủ hóa đáng kể việc sử dụng LLM cho các vấn đề phức tạp.

3. Thuật toán của Tư duy

Chiến lược của chúng tôi xoay quanh việc nhận ra một thiếu sót cốt lõi
của các mô hình học trong ngữ cảnh hiện tại. CoT, trong khi tăng cường
sự mạch lạc của các liên kết tư duy dẫn đến giải pháp,
đôi khi lảo đảo, trình bày các bước trung gian không chính xác
(Zelikman et al., 2022; Turpin et al., 2023; Lanham et al.,
2023). CoT Trung thực (Lyu et al., 2023) phải sửa đổi điều này
bằng cách gợi ra các chuỗi lý luận biểu tượng trong đó đầu ra của LLM
giống như mã giả đặc trưng nhiệm vụ, được chuẩn bị cho
thực thi xác định như Python. Ý định chỉ là
sử dụng các quá trình tư duy chứ không phải đầu ra và đầu vào
của mỗi liên kết vì chúng có xu hướng không đáng tin cậy.
Nhưng, những sai lệch thỉnh thoảng của CoT có thể không nhất thiết là
do khả năng tính toán không chính xác của LLM. LLM,
khi đối mặt với các câu hỏi khớp chặt chẽ với điều kiện
của các ví dụ trong ngữ cảnh trước đó, có thể ưa chuộng việc lặp lại
những đầu ra đó hơn việc tạo ra các câu hỏi phù hợp. Để
làm sáng tỏ hiện tượng này, chúng tôi đã thiết kế một thí nghiệm.

Truy vấn text-davinci-003 cho các nhiệm vụ số học (ví dụ, '11−
2 ='), chúng tôi đặt tiền tố cho chúng với nhiều phương trình trong ngữ cảnh
hội tụ về một đầu ra giống hệt nhau (ví dụ '15−5 = 10, 8+2 =
10'). Kết quả của chúng tôi, được trình bày trong Hình 3, cho thấy sự suy giảm
mạnh về độ chính xác, cho thấy rằng chỉ sự hiện diện của lý luận
đúng trong ngữ cảnh có thể vô tình làm tổn hại
ngay cả những kỹ năng số học cơ bản.

0.0 2.5 5.0 7.5 10.0 12.5
# của Phương trình 0.0 0.2 0.4 0.6 0.8 1.0 Xác suất Token Đúng

Hình 3. Xác suất tạo ra token đúng khi chúng tôi thêm
các ví dụ trong ngữ cảnh nhiều hơn mà đúng nhưng có
đầu ra giống hệt nhau.

Để bù đắp cho thiên vị này, việc đa dạng hóa đầu ra của các ví dụ
có thể dường như là một giải pháp khả thi, nhưng điều này có thể
làm lệch phân phối đầu ra một cách tinh tế. Chỉ thêm các thử nghiệm
không thành công, giống như một tìm kiếm ngẫu nhiên, có thể vô tình khuyến khích
mô hình thử lại thay vì thực sự giải quyết. Nắm bắt
bản chất thực sự của hành vi thuật toán, nơi cả tìm kiếm
thất bại và việc phục hồi và học hỏi từ những nỗ lực
như vậy đều đóng một vai trò, chúng tôi kết hợp các ví dụ trong ngữ cảnh
được mô hình hóa theo các thuật toán tìm kiếm, đặc biệt là tìm kiếm
theo chiều sâu đầu tiên (DFS) và tìm kiếm theo chiều rộng đầu tiên (BFS). Xem Hình 1 để có
ví dụ.

Bài báo này tập trung vào một lớp rộng các nhiệm vụ gợi nhớ
các vấn đề tìm kiếm cây. Những nhiệm vụ này đòi hỏi chia nhỏ
vấn đề chính, tạo ra các giải pháp khả thi cho mỗi
phần, và đưa ra quyết định về các con đường để theo đuổi
hoặc từ bỏ, với tùy chọn đánh giá lại các phân đoạn
hứa hẹn hơn. Thay vì đặt ra các truy vấn riêng biệt cho
mỗi tập hợp con, chúng tôi tận dụng khả năng lặp đi lặp lại của
LLM để giải quyết chúng trong một lần quét tạo sinh thống nhất. Bằng cách
giới hạn bản thân trong một hoặc hai tương tác LLM, phương pháp này
tự nhiên kết hợp những hiểu biết từ ứng cử viên
ngữ cảnh tiền nhiệm và giải quyết các vấn đề phức tạp đòi hỏi
khám phá sâu sắc miền giải pháp. Chúng tôi cũng đưa ra
những hiểu biết về việc những suy nghĩ đó nên nhỏ hay lớn như thế nào
và loại ví dụ trong ngữ cảnh nào nên được đưa ra cho
LLM để thúc đẩy hiệu quả token. Tiếp theo, chúng tôi

phác thảo các thành phần chính của thuật toán tìm kiếm cây và sự
biểu hiện của chúng trong khung của chúng tôi.

1. Chia tìm kiếm thành các bước. Tương tự như việc tạo
giải pháp từng bước trong CoT hoặc L2M, chúng tôi cũng cần xác định
các lớp tìm kiếm trung gian. Điều này giống như tạo ví dụ
cho CoT, đặc biệt là cho các vấn đề tìm kiếm cây, trong đó
con đường lý luận đúng giống với một giải pháp CoT. Thách thức
nằm ở việc chọn chuỗi đúng từ nhiều ứng cử viên
tại mỗi lớp để đạt được câu trả lời cuối cùng. Do đó, trọng tâm của chúng tôi
sẽ tập trung hơn vào việc tạo ra quá trình tìm kiếm cho các ví dụ
trong ngữ cảnh thay vì cách giải quyết mỗi vấn đề phụ
sau khi chọn chuỗi tiếp theo.

Năm số nguyên tố đầu tiên:
Hoàn thành Văn bản
2 = 87.6%
1 = 12.3%
...
...
2, 3, 5, 7, 11
xác suất cho token đầu tiên

Hình 4. Một ví dụ làm nổi bật nhược điểm của việc lấy mẫu
cô lập các ý tưởng có trình tự. Đầu vào được ký hiệu bằng màu xanh,
với text-davinci-003 cung cấp các phần hoàn thành màu xanh lá cây.

2. Đề xuất Giải pháp cho Các Vấn đề Phụ. Một phương pháp
thống trị trong các nghiên cứu hiện tại bao gồm việc lấy mẫu trực tiếp
từ xác suất đầu ra token của LLM (Wang et al., 2022; Yao
et al., 2023). Mặc dù hiệu quả cho các câu trả lời một lần (Ka-
davath et al., 2022), phương pháp này không đáp ứng trong các tình huống
đòi hỏi một chuỗi mẫu được tích hợp hoặc đánh giá
trong các prompt tiếp theo (Robinson & Wingate,
2022). Để giảm thiểu các truy vấn mô hình, chúng tôi áp dụng một
quá trình tạo giải pháp không bị gián đoạn. Ở đây, chúng tôi trực tiếp và
liên tục tạo ra các giải pháp cho vấn đề phụ hiện tại
mà không có bất kỳ tạm dừng tạo sinh nào.

Lợi ích là ba mặt. Đầu tiên, với tất cả các giải pháp được tạo ra
tồn tại trong một ngữ cảnh chung, không cần
các truy vấn mô hình riêng lẻ cho mỗi đánh giá giải pháp. Thứ hai,
mặc dù có thể dường như phản trực giác, xác suất token cô lập hoặc
nhóm token có thể không luôn mang lại
lựa chọn có ý nghĩa. Một minh họa đơn giản được tìm thấy trong Hình 4. Khi
được đánh giá độc lập, token có xác suất cao thứ hai cho
số đầu tiên của chúng tôi là '1'—không đủ điều kiện là số nguyên tố. Nhưng,
khi việc tạo sinh vẫn không bị gián đoạn, chuỗi được suy ra là
đúng. Sự bất nhất này chỉ ra bản chất hạn chế
của tính chất Markov trong mô hình hóa chuỗi. Cốt lõi của
quan điểm chúng tôi là tiền đề rằng đối với các nhiệm vụ tuần tự như
tìm kiếm thuật toán, LLMs khéo léo hơn trong việc tạo ra toàn bộ
chuỗi thay vì tạm dừng và khởi tạo lại
quá trình lấy mẫu token một cách gián đoạn.

3. Đánh giá Triển vọng của một Vấn đề Phụ. Các kỹ thuật
hiện tại dựa vào prompting bổ sung để nhận biết tiềm năng
của các nút cây, hỗ trợ quyết định về hướng khám phá.
Quan sát của chúng tôi cho thấy rằng nếu các tuyến đường hứa hẹn nhất
được đóng gói trong các ví dụ trong ngữ cảnh,
LLMs vốn có xu hướng ưu tiên những ứng cử viên
hứa hẹn đó. Điều này giảm thiểu nhu cầu kỹ thuật prompt
phức tạp và cho phép kết hợp các heuristic phức tạp,
dù trực quan hay dựa trên kiến thức. Một lần nữa, việc
không có các prompt rời rạc trong phương pháp của chúng tôi cho phép
đánh giá ngay lập tức về tính khả thi của ứng cử viên trong cùng
một thế hệ.

4. Quay lại một Nút Hứa hẹn hơn. Quyết định
nút nào để khám phá tiếp theo (bao gồm việc quay trở lại
nút trước đó) vốn phụ thuộc vào thuật toán tìm kiếm cây
được chọn. Trong khi các nghiên cứu trước đây (Yao et al., 2023) đã
sử dụng phương tiện bên ngoài, chẳng hạn như các cơ chế được mã hóa cho
quá trình tìm kiếm, điều này hạn chế sức hấp dẫn rộng hơn của nó và đòi hỏi
tùy chỉnh bổ sung. Thiết kế của chúng tôi chủ yếu áp dụng
phương pháp DFS được bổ sung bằng cắt tỉa. Mục đích là để
duy trì sự gần gũi giữa các nút chia sẻ cùng cha mẹ,
từ đó khuyến khích LLM ưu tiên các tính năng cục bộ hơn tính năng xa.
Ngoài ra, chúng tôi trình bày các chỉ số hiệu suất cho
phương pháp AoT dựa trên BFS. Sự phụ thuộc của chúng tôi vào
khả năng bẩm sinh của mô hình để thu thập hiểu biết từ các ví dụ trong ngữ cảnh
làm bỏ qua sự cần thiết của các cơ chế bổ sung.

Tính Biểu đạt của LLMs với AoT. Các nghiên cứu gần đây đã
điều tra tính biểu đạt của transformers với prompting chuẩn
và CoT (Chiang et al., 2023; Schuurmans, 2023;
Merrill & Sabharwal, 2023; Feng et al., 2023). Chúng tôi cung cấp
kết quả lý thuyết sau đây cho AoT, ngụ ý rằng
nó có thể giải quyết các vấn đề NP, mở rộng từ các vấn đề P mà
COT's.

Hệ quả 3.1 (Không chính thức). Xem xét TIME(an) như lớp
các vấn đề mà một máy Turing tồn tại hoạt động
trong độ phức tạp thời gian O(an) cho một số a≥1. Nếu một
transformer có thể tạo ra an token trung gian để giải quyết
vấn đề khi được prompted bởi AoT, chúng ta có

TIME(an)⊆AOT(n), (1)

trong đó AOT(n) đề cập đến các bước giải mã bởi AoT khi
đầu vào có n token.

Chứng minh của hệ quả trên được đưa ra trong phụ lục.

4. Thực nghiệm

Chúng tôi cho thấy rằng AoT vượt qua hiệu suất của các phương pháp
prompt đơn khác (ví dụ, chuẩn, prompting CoT/-SC) và thậm chí
của các chiến lược sử dụng các cơ chế bên ngoài, chẳng hạn như
ToT, trên các benchmark mà chúng tôi đã thử nghiệm. Chúng tôi trình bày
kết quả cho nhiệm vụ viết sáng tạo trong phụ lục. Ngoài ra,
chúng tôi cho thấy rằng AoT tiếp tục có lợi thế
hơn prompting chuẩn hoặc CoT ngay cả sau khi fine-tuning. Điều này
ngụ ý rằng vấn đề với LLMs không chỉ đơn giản là sự
lệch hướng nhỏ hoặc thiếu hụt trong chuyên môn lĩnh vực. Thay vào đó,
nó nhấn mạnh sự cần thiết của prompting AoT. Phương pháp này
là quan trọng vì bản chất của các nhiệm vụ mà chúng tôi đánh giá
vốn đòi hỏi khám phá kỹ lưỡng các đường dẫn giải pháp,
một yêu cầu vượt ra ngoài các điều chỉnh fine-tuning đơn giản. Đối với việc tạo ra
các ví dụ trong ngữ cảnh, chúng tôi đã yêu cầu các tác giả viết ra
quá trình tìm kiếm của họ và chọn ngẫu nhiên từ danh sách đó. Chúng tôi đã
viết lại chúng một cách có cấu trúc đơn giản để có sự thống nhất
giữa các ví dụ để tạo ra các prompt AoT của chúng tôi. Thêm chi tiết
về quá trình này cho mỗi nhiệm vụ được đưa ra trong các phần thiết lập
AoT.

4.1. Trò chơi 24

Trò chơi 24 là một trò chơi bài toán học trong đó người chơi
được cho bốn số và phải sử dụng phép cộng, trừ,
nhân và chia (mỗi phép toán có thể được sử dụng nhiều lần) để thao tác
những số đó để đạt được tổng là 24. Ví dụ, đối với các số
'8 8 5 4', một giải pháp có thể là '8∗(5−(8/4)) = 24'. Thoạt nhìn,
trò chơi có thể xuất hiện đơn giản. Tuy nhiên, một tính toán
sơ bộ cho thấy có gần 13.000 biểu thức riêng biệt
có thể cho bất kỳ tập hợp bốn số nào, làm cho nó
trở thành một thách thức khó khăn đối với các LLMs ngày nay.

Thiết lập Nhiệm vụ. Tuân thủ thiết lập được mô tả chi tiết trong (Yao et al.,
2023), chúng tôi sử dụng các trò chơi từ chỉ số 901-1000, được lấy từ
1362 trò chơi được xếp hạng theo độ khó tương đối tại 4nums.com.
Một nỗ lực được coi là thành công nếu nó có thể đạt được
tổng là 24 sử dụng chính xác các số được cung cấp và chỉ các
phép toán được phép.

Baseline. Prompting chuẩn và CoT được sử dụng trong thiết lập
5-shot, với CoT tích hợp 3 bước cho các phép toán.
Những phương pháp này được lấy mẫu 100 lần, và tỷ lệ thành công
trung bình từ những mẫu này được báo cáo. CoT-SC cũng
được thử nghiệm với 100 phiếu bầu trong thiết lập của chúng tôi. Đối với ToT, chúng tôi sử dụng
độ rộng là 5.

Thiết lập AoT. Chúng tôi sử dụng cùng thiết lập 5-shot như trong
thiết lập baseline prompting chuẩn và CoT. Các mẫu trong ngữ cảnh
của chúng tôi tận dụng thuật toán tìm kiếm kiểu DFS, đó là
phiên bản tương tự được sử dụng khi so sánh với DFS truyền thống
trong Hình 5. Trong mỗi khám phá cây con, được gọi là
'bước đầu tiên' hoặc 'phép toán đầu tiên', chúng tôi chọn hai số—
được minh họa bằng việc chọn 8 và 6 trong 'bước đầu tiên' thứ ba
(tức là, cây con được gán nhãn '3') của Hình 1—và một phép toán
tương ứng (ví dụ, 8−6). Phép toán này tạo ra một số mới,
2, để lại cho chúng ta tổng cộng ba số. Một việc lược bỏ
kỹ lưỡng của ba số này đạt đỉnh trong 19 nút lá, tất cả
hiển thị dưới cây con '3' trong Hình 1. Để tạo ra
các ví dụ trong ngữ cảnh của chúng tôi, chúng tôi đã chọn ngẫu nhiên các trò chơi
không xuất hiện tại thời điểm thử nghiệm. Chúng tôi yêu cầu các tác giả viết
các bước tìm kiếm mà họ đã sử dụng cho đến khi họ đi đến câu trả lời.
Đây chính xác là các bước chọn nút, mở rộng nút với
heuristic bẩm sinh của các cá nhân. Sau đó, chúng tôi đã
chọn ngẫu nhiên từ những giải pháp này và viết chúng
theo cách có cấu trúc tầm thường để đảm bảo sự thống nhất giữa
các ví dụ. Các prompt chính xác mà chúng tôi sử dụng được đưa ra trong
phần Prompts dưới tiểu mục 'AoT (DFS)' trong
phụ lục. Chúng tôi nhằm đánh giá hai khía cạnh: khả năng của
LLM trong việc xác định các phép toán đầu tiên hứa hẹn, điều này trực tiếp
ảnh hưởng đến số lượng nút lá được giải quyết, và hiệu suất của nó
chống lại một DFS thông thường. Chi tiết về các prompt được
cung cấp trong phụ lục. Vì phương pháp của chúng tôi nhấn mạnh
việc tạo sinh tuần tự hơn là lấy mẫu quỹ đạo, chúng tôi hoạt động
với thiết lập nhiệt độ là 0.

Kết quả. Từ Bảng 1, rõ ràng là prompting chuẩn
kết hợp với CoT/-SC tụt hậu đáng kể so với các phương pháp
tìm kiếm cây khi được sử dụng với LLMs. Kết quả "Standard +
Refine" cho thấy tỷ lệ thành công 27%, được tham chiếu
từ (Yao et al., 2023). Phương pháp này bao gồm việc lặp đi lặp lại
yêu cầu LLM (lên đến 10 lần lặp) tinh chỉnh câu trả lời của nó nếu
câu trả lời ban đầu không chính xác. Trong khi đó, ToT bị giới hạn
tối đa 100 lần thăm nút, dịch sang vài trăm
truy vấn LLM cho mỗi ví dụ. Đáng chú ý, AoT đạt được
kết quả của nó chỉ với một truy vấn duy nhất! Mặc dù giảm số
yêu cầu hơn một hệ số 100, AoT vẫn
vượt trội hơn ToT trong nhiệm vụ này. Hơn nữa, AoT cũng
hiệu quả hơn ToT về tổng số token prompt
được đưa cho LLM và token hoàn thành mà nó tạo ra.

Phương pháp Thành công Truy vấn PTs CTs
I/O 7.3% 1 164 18
CoT 4.0% 1 421 46.2
CoT-SC 9.0% 100 42,100 4,620
I/O + Refine 27% 10 458 360
ToT (b=5) 69% 109.1 13,900 5,500
AoT (của chúng tôi) 71% 1 5,450 998.4

Bảng 1. Trò chơi 24: tỷ lệ thành công và số lượng truy vấn LLM
trung bình cho mỗi ví dụ. Chúng tôi đưa ra số lượng truy vấn trung bình,
token prompt (PT), và token hoàn thành được tạo ra bởi LLM
(CT).

Phân tích Lỗi. Sử dụng phương pháp hoàn toàn tập trung vào LLM—
tránh bất kỳ công cụ bên ngoài hoặc chỉnh sửa—chúng tôi tìm cách phân loại
những sai lầm được quan sát trong trò chơi 24. Điều này hỗ trợ
làm nổi bật các lĩnh vực để tinh chỉnh khi chỉ triển khai
LLMs. Chúng tôi đã phân loại những lỗi này thành bốn loại
riêng biệt:

loại: 1) Lỗi hết token: LLM đạt đến ngưỡng token
tối đa mà không xác định được giải pháp.
2) Sai lệch biểu đạt: LLM có logic hoặc bước đúng nhưng
thất bại khi cố gắng biểu đạt hoặc công thức hóa chúng
thành một câu trả lời mạch lạc. 3) Lỗi không kết thúc: LLM
khám phá ra giải pháp nhưng tiếp tục tìm kiếm mà không
hợp nhất phát hiện. 4) Lỗi khác: Thuật ngữ bao trùm này
bao gồm những sai lầm khác như lỗi tính toán dẫn đến
bỏ sót giải pháp hoặc cung cấp câu trả lời không chính xác. Để chỉ
thể hiện khả năng tìm kiếm của AoT, chúng tôi cũng trình bày
phiên bản AoT + Giải quyết Thủ công. Ở đây, một khi LLM
xác định một giải pháp, sự diễn đạt cuối cùng của nó được xử lý
thủ công—một chiến lược cũng được phương pháp ToT sử dụng. Như được
chứng minh trong Bảng 2, 7% sai lầm đáng chú ý xuất phát từ
các yếu tố phi thuật toán như không kết thúc và sai lệch
biểu đạt. Thực tế, với giải quyết thủ công, AoT đạt được
tỷ lệ thành công 78%, vượt qua ToT.
Điều này nhấn mạnh tiềm năng để tinh chỉnh prompt của chúng tôi,
đặc biệt trong các lĩnh vực liên quan đến nhận biết và biểu đạt
giải quyết vấn đề thành công. Ngoài ra, hạn chế token
nhấn mạnh sức hấp dẫn của việc mở rộng cửa sổ ngữ cảnh
tạo sinh, điều này có thể tăng cường thêm lý luận đệ quy của LLMs
khi tham gia với các ví dụ thuật toán.

Loại Lỗi Lỗi
Lỗi hết token 9%
Sai lệch biểu đạt 4%
Lỗi không kết thúc 3%
Khác 13%

Phương pháp Thành công
ToT 69%
AoT 71%
AoT + Giải quyết Thủ công 78%

Bảng 2. Trò chơi 24: phân tích lỗi AoT.

4.2. Ô chữ Mini

Ô chữ mini 5×5 là một câu đố từ nhỏ gọn có
lưới 25 ô vuông được sắp xếp trong cấu hình 5-by-5.
Người chơi được giao nhiệm vụ điền vào lưới dựa trên các gợi ý được cung cấp
cho mỗi từ. Các gợi ý được đưa ra cho các từ chạy cả
ngang (theo chiều ngang) và dọc (theo chiều dọc). Các từ giao nhau
tại các chữ cái nhất định, cung cấp gợi ý bổ sung để hoàn thành
câu đố.

Thiết lập Nhiệm vụ. Tuân thủ thiết lập được phác thảo trong (Yao et al.,
2023), chúng tôi rút các prompt của mình từ các trò chơi 136, 141, 146, 151,
và 156 trong số 156 trò chơi có sẵn trên goobix.com. Việc thử nghiệm
của chúng tôi tập trung vào một tập hợp 20 trò chơi, cụ thể là các trò chơi 1,
6, . . ., 91, và 96.

Baseline. Như đã làm trong trò chơi 24, chúng tôi so sánh phương pháp
của mình với các kỹ thuật đã được thiết lập: prompting chuẩn,
CoT, và ToT. Đối với prompting chuẩn, chúng tôi cung cấp cả
ô chữ và các giải pháp tương ứng của chúng như các ví dụ trong ngữ cảnh.
CoT tăng cường điều này bằng cách prompting việc truy xuất
từ cho mỗi trong số mười gợi ý—được chia đều giữa
hướng ngang và dọc. Chúng tôi trực tiếp trích xuất tỷ lệ thành công
của ToT từ bài báo của họ để so sánh.

Thiết lập AoT. Chúng tôi chia quá trình thành hai bước, mỗi bước
bao gồm một truy vấn. Ban đầu, chúng tôi giao nhiệm vụ cho LLM đề xuất
năm từ tiềm năng cho mỗi hàng và cột. Sau đó chúng tôi
xác định các ứng cử viên từ bắt đầu có độ tương thích
cao nhất với các từ khác trong khung ô chữ.
Giai đoạn sơ bộ này phản ánh chuỗi 'khởi động' trong
khởi tạo thuật toán. Trong bước tiếp theo, chúng tôi chỉ
tận dụng sức mạnh lý luận thuật toán của LLM,
bắt đầu với từ được chọn trước. Phương pháp này bao gồm
việc chọn tuần hoàn một tùy chọn có khả năng để chèn, tạo ra
các từ ứng cử viên, và đánh giá khả năng tương thích của chúng với
các từ đã có trên bảng. Nếu không tìm thấy kết quả phù hợp, quá
trình chuyển tập trung sang một ứng cử viên hứa hẹn khác. Nếu không,
từ được thêm vào ô chữ, và tìm kiếm tiếp tục.
Chu kỳ kết thúc khi bảng được điền đầy đủ
hoặc không thể tìm thấy thêm từ phù hợp, điều này có thể do
các từ hiện tại không chính xác hoặc việc không có từ
khớp. Đáng chú ý, toàn bộ quá trình này diễn ra trong một
cửa sổ tạo sinh duy nhất. Các ví dụ thuật toán trong prompt của chúng tôi
(được mô tả chi tiết trong Phụ lục) bao gồm ba ví dụ đạt được
hoàn thành trò chơi và hai ví dụ chủ yếu điền vào
ô chữ, điền 8 hoặc 9 ô.

Kết quả. Bảng 3 nhấn mạnh thành thạo của AoT trong
nhiệm vụ ô chữ mini, thể hiện tỷ lệ thành công từ—một
thước đo được sử dụng trong các nghiên cứu hiện tại để đại diện cho tỷ lệ phần trăm
từ được hoàn thành chính xác trong tổng số—vượt qua
các phương pháp trước đó dựa vào các kỹ thuật prompting khác nhau. Nó
cũng vượt trội hơn ToT. Một quan sát quan trọng là khối lượng
truy vấn lớn mà ToT sử dụng, vượt quá của AoT hơn
một hệ số 100. AoT cũng tận hưởng việc giảm 25 lần tổng token
cần thiết so với ToT, một lợi ích của việc có mọi thứ
trong ngữ cảnh.

Phương pháp W. Thành công Truy vấn PTs CTs
I/O 14% 1 790.3 30.5
CoT-SC 15.6% 1 1,400 1,600
ToT 46.5% >200 96,700 21.8k
AoT (của chúng tôi) 52% 2 3,800 975.6

Bảng 3. Ô chữ mini 5×5: tỷ lệ thành công từ và số lượng
truy vấn LLM trung bình cho mỗi ví dụ. Chúng tôi đưa ra số lượng truy vấn
trung bình, token prompt (PT), và token hoàn thành được tạo ra bởi LLM
(CT).

Phân tích Lỗi. Để hiểu những sai lầm phổ biến
mà AoT tạo ra, chúng tôi đã phân loại các lỗi thành bốn loại
riêng biệt. Trong phân tích của chúng tôi cho mỗi trò chơi, chúng tôi tập trung vào
lỗi ban đầu mà LLM tạo ra khi vạch ra con đường lý luận
của nó, với điều kiện là một lỗi sớm thường dẫn đến
thất bại tiếp theo. 1) Không có lựa chọn trước: LLM thất bại trong việc tạo ra
các từ tương thích cần thiết cho giai đoạn khởi động ấm. Với
một từ được chọn trước chính xác, giai đoạn thứ hai cho lý luận
đệ quy có thể thể hiện lỗi bao gồm: 2) Sai lệch biểu đạt: LLM
lầm tưởng rằng nó đã cạn kiệt tất cả lựa chọn và nhảy đến câu trả lời
sớm. 3) Trích xuất mẫu không chính xác: LLM sai lầm trích xuất
một mẫu dựa trên bố cục bảng hiện tại. 4) Đặt từ sai:
Mặc dù nhận ra mẫu đúng, LLM chọn một từ không khớp
hoặc bỏ lỡ các lựa chọn thay thế phù hợp hơn. Điều hướng độ phức tạp
ô chữ phát sinh từ các thuật ngữ lỗi thời và tham chiếu bí truyền.
Chủ yếu, các lỗi được quan sát là do việc đặt từ sai
tiếp theo là sự diễn giải mẫu sai. Ngoài ra, LLM dường như
bị thách thức trong việc căn chỉnh các chữ cái tại các chỉ số chính xác để tạo
cấu trúc từ—một trở ngại được vượt qua bởi một cơ chế
bên ngoài trong khung ToT.

Loại Lỗi Lỗi
Không có lựa chọn trước 15.8%
Sai lệch biểu đạt 5.3%
Trích xuất mẫu không chính xác 26.3%
Đặt từ sai 52.6%

Bảng 4. Phân tích lỗi trong ô chữ mini 5×5 với AoT.
Các số cho biết tỷ lệ phần trăm tương đối của mỗi loại lỗi trong
tất cả các lỗi.

4.3. Fine-tuning

Để loại bỏ khả năng rằng các thí nghiệm trước đó thiếu
kiến thức lĩnh vực hoặc bị lệch hướng với nhiệm vụ,
ngay cả sau khi prompting few-shot thông qua prompting chuẩn hoặc
CoT, chúng tôi cũng đã fine-tune GPT-3.5-Turbo sử dụng API của OpenAI
với 900 ví dụ với CoT và AoT. Trong Bảng 5, chúng ta có thể
thấy rằng mặc dù GPT-3.5-Turbo có tỷ lệ giải pháp tương tự
cho Trò chơi 24 với CoT và AoT, fine-tuning AoT
cải thiện mô hình 60% so với 8% cho CoT. Điều này
cho thấy rằng chỉ fine-tuning alone không thể xuất hiện tư duy
phi tuyến tính ngầm, và LLMs vẫn yêu cầu khám phá rõ ràng
các tùy chọn có thể để đi đến giải pháp. Điều này tương tự
như các bậc thầy cờ vua có thể tìm ra các nước đi tốt hơn người khác
ngay cả khi họ chơi mà không suy nghĩ sâu sắc.
Tuy nhiên, để tìm ra những nước đi thực sự tuyệt vời, họ cũng cần
cố ý khám phá khả năng của không gian.

Phương pháp không có fine-tuning có fine-tuning
CoT 3% 12%
AoT 3% 63%

Bảng 5. Lợi thế của AoT tiếp tục ngay cả sau fine-tuning. Kết quả
fine-tuning trên Trò chơi 24 trên 900 ví dụ với prompting CoT và AoT.

5. Thảo luận

Trong phần này, chúng tôi đi sâu vào các khía cạnh quan trọng cần xem xét
khi tạo prompt cho AoT, sử dụng trò chơi 24 làm nghiên cứu
trường hợp chính.

AoT có thể vượt qua DFS mà nó được mô hình hóa theo không?
Một truy vấn cốt lõi của chúng tôi là để xác định liệu LLM có
khả năng không chỉ phản ánh mà còn vượt qua hiệu quả của
thuật toán được giới thiệu trong ngữ cảnh. Như được chứng minh trong Hình 5,
AoT một cách hệ thống điều hướng ít nút hơn so với
đối tác DFS của nó. Trong khi DFS sử dụng chiến lược thống nhất khi
chọn cây con tiếp theo để điều tra, LLM của AoT
tích hợp heuristic bẩm sinh của nó. Sự khuếch đại này so với
thuật toán cơ sở minh chứng lợi thế của khả năng lý luận
đệ quy của LLM.

0 200 400 600 800 1000
# của Nút Đã Thăm 0 4 8 12 16 20 # của Trò chơi DFS
AoT

Hình 5. Biểu đồ cho thấy số lượng nút đã thăm cho AoT
và DFS trong Trò chơi 24.

Làm thế nào số lượng bước tìm kiếm trong ví dụ thuật toán
điều chỉnh hành vi của AoT? Chúng tôi bắt đầu với prompt AoT
chuẩn và sửa đổi các khám phá cây con.
Trong AoT (Ngắn), mỗi ví dụ trong ngữ cảnh sử dụng một hoặc hai
bước để đạt được giải pháp, trong khi AoT (Dài) kết hợp
ba đến năm khám phá cây con bổ sung. Tác động lên
tổng số bước tìm kiếm được minh họa trong Hình 6. Quan sát của chúng tôi
làm nổi bật việc tạo sinh dài hơn cho AoT (Dài) và ngắn hơn
cho AoT (Ngắn) so với AoT gốc. Điều này
cho thấy rằng số lượng bước tìm kiếm giới thiệu một thiên vị ngầm
lên tốc độ tìm kiếm của LLM. Đáng chú ý, ngay cả khi
điều hướng các bước không chính xác, điều quan trọng là nhấn mạnh
khám phá các hướng hứa hẹn.

0 100 200 300 400
# của Nút Đã Thăm 0 20 40 60 80 100 # của Trò chơi AoT (Ngắn)
AoT
AoT (Dài)

Hình 6. So sánh AoT với các phiên bản AoT được prompted với
ví dụ trong ngữ cảnh ngắn hơn và dài hơn: số lượng tích lũy các trò chơi
cho số lượng nút đã thăm.

AoT có thể được sử dụng cho các nhiệm vụ hỏi-đáp không?
Để trả lời câu hỏi này, chúng tôi đã tuân theo cùng cấu trúc
với nhiệm vụ Viết Sáng tạo (được đưa ra trong phụ lục của
bài báo chúng tôi) để đánh giá AoT và các baseline trên 100 câu hỏi
đầu tiên của các benchmark GSM8K và StrategyQA nổi tiếng.
Tóm lại, chúng tôi đã triển khai prompt AoT zero-shot cho
StrategyQA và GSM8K đề xuất 3 chiến lược và
mở rộng chúng với chi tiết để chọn ra cái tốt nhất. Như được thấy trong
Bảng 6, chúng ta thấy một sự tăng nhẹ về nhiệm vụ này do GPT-4 với CoT
đã thành thạo.

Phương pháp GSM8K StrategyQA
IO 51% 73%
CoT 86% 82%
ToT 90% 83%
AoT 89% 84%

Bảng 6. So sánh hiệu suất của các phương pháp khác nhau trên
nhiệm vụ hỏi-đáp sử dụng các benchmark GSM8K và StrategyQA. Mô hình
AoT cho thấy hiệu suất cạnh tranh, đặc biệt khi so sánh
với các phương pháp CoT và ToT.

AoT có thể hoạt động như các phương pháp lập trình động khác không?
Chúng tôi cũng đã thử nghiệm AoT và các baseline trên
các vấn đề lập trình động truyền thống Coin Change
và Edit Distance, nơi DFS và BFS có độ phức tạp
bùng nổ. Tuy nhiên, một phương pháp DP khác có tên tabulation
có thể giải quyết những vấn đề này dễ dàng hơn. Vì ToT không thể
lấy dạng của tabulation, và phải là DFS hoặc BFS,
chúng tôi quyết định không bao gồm những kết quả kém đó để công bằng. Tuy nhiên,
người ta có thể sử dụng một nút lá duy nhất với prompt CoT để
giải quyết những vấn đề này. Ở đó, hiệu suất của ToT có thể được
coi là giống với của CoT. Vui lòng tham khảo Bảng
7 cho kết quả chi tiết.

AoT có thể giúp các LLMs SOTA khác không? Chúng tôi điều tra AoT
cho các LLMs SOTA khác, Claude 3 và Gemini 1.5 Pro, để
thấy liệu nó có cung cấp một sự tăng cường đáng kể cho họ cũng vậy trên
trò chơi 24. Chúng ta thấy rằng cả Claude 3 và Gemini 1.5

Vấn đề Coin Change Edit Distance
I/O 72% 61%
CoT 76% 64%
AoT 96% 90%

Bảng 7. So sánh hiệu suất của AoT với các phương pháp lập trình
động truyền thống trong việc giải quyết các vấn đề Coin Change và Edit Distance.

Pro đều có lợi ích đáng kể. Chúng tôi không thể chạy Gemini 1.5
Pro trên CoT-SC do quyền truy cập API chưa có sẵn.
Vui lòng tham khảo Bảng 8 cho kết quả chi tiết.

Phương pháp GPT-4 Claude 3 Gemini 1.5 Pro
IO 7% 6% 6%
CoT-SC 9% 9% -
AoT 71% 68% 55%

Bảng 8. Kết quả mô hình ngôn ngữ bổ sung cho các phương pháp AoT, CoT-SC,
và IO trên các mô hình khác nhau.

6. Kết luận

Bài báo này trình bày Thuật toán của Tư duy, một chiến lược
prompting tiên phong để điều hướng các con đường lý luận trong
LLMs sử dụng các truy vấn tối thiểu. Những phát hiện của chúng tôi tiết lộ rằng phương pháp này
không chỉ vượt qua đáng kể các kỹ thuật truy vấn đơn trước đó
mà còn vượt trội hơn các triển khai tìm kiếm cây bên ngoài. Một
phương pháp như vậy tăng cường tiềm năng để hợp lý hóa
khám phá ý tưởng trong LLMs, cân bằng cả chi phí và
yêu cầu tính toán. Công việc tương lai bao gồm thiết kế
các ví dụ thuật toán hiệu quả token, phát triển các cơ chế thích ứng
cho việc kích hoạt "tầm nhìn hầm" để đẩy nhanh
tìm kiếm, và làm sâu sắc thêm hiểu biết về chế độ học
trong ngữ cảnh mới này từ các góc độ lý thuyết.

7. Hạn chế

Trong khi AoT giảm đáng kể số lượng truy vấn
so với ToT, yêu cầu tài nguyên của nó vượt quá của
prompting chuẩn và CoT, một hệ quả của việc khám phá
rộng rãi ý tưởng thông qua việc tạo token. Việc tạo ra các ví dụ
thuật toán hiệu quả token là một hướng nghiên cứu tương lai.
Cũng cần lưu ý rằng chúng tôi đã tiến hành
các thử nghiệm của mình độc quyền với GPT-4. Mặc dù đắt hơn
các LLMs khác, khả năng tiên tiến của GPT-4 xuất hiện then chốt
cho hoạt động tối ưu của AoT; các mô hình có năng lực
kém hơn có thể không mang lại tăng cường hiệu suất tương đương từ AoT.
