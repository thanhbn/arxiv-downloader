# LiCROM: Mô hình Giảm Bậc Liên tục Không gian Con Tuyến tính với Trường Neural

Yue Chang
University of Toronto
Canada
changyue.chang@mail.utoronto.ca

Peter Yichen Chen∗
MIT CSAIL
USA
pyc@csail.mit.edu

Zhecheng Wang
University of Toronto
Canada
zhecheng@cs.toronto.edu

Maurizio M. Chiaramonte
Meta Reality Labs Research
USA
mchiaram@meta.com

Kevin Carlberg
Meta Reality Labs Research
USA
carlberg@meta.com

Eitan Grinspun∗
University of Toronto
Canada
eitan@cs.toronto.edu

## TÓM TẮT

Mô hình giảm bậc tuyến tính (ROM) đơn giản hóa các mô phỏng phức tạp bằng cách xấp xỉ hành vi của một hệ thống sử dụng một biểu diễn động học đơn giản hóa. Thông thường, ROM được đào tạo trên các mô phỏng đầu vào được tạo với một rời rạc hóa không gian cụ thể, và sau đó phục vụ để tăng tốc các mô phỏng với cùng rời rạc hóa đó. Sự phụ thuộc rời rạc hóa này là hạn chế.

Việc trở nên độc lập với một rời rạc hóa cụ thể sẽ cung cấp tính linh hoạt để kết hợp và kết hợp độ phân giải lưới, kết nối và loại (tứ diện, lục diện) trong dữ liệu đào tạo; để tăng tốc các mô phỏng với các rời rạc hóa mới chưa được thấy trong quá trình đào tạo; và để tăng tốc các mô phỏng thích ứng thay đổi rời rạc hóa theo thời gian hoặc tham số.

Chúng tôi trình bày một phương pháp linh hoạt, độc lập với rời rạc hóa cho mô hình giảm bậc. Giống như ROM truyền thống, chúng tôi biểu diễn cấu hình như một tổ hợp tuyến tính của các trường chuyển vị. Không giống ROM truyền thống, các trường chuyển vị của chúng tôi là các ánh xạ liên tục từ mọi điểm trên miền tham chiếu đến một vectơ chuyển vị tương ứng; các ánh xạ này được biểu diễn như các trường neural ngầm.

Với ROM liên tục tuyến tính (LiCROM), tập đào tạo của chúng tôi có thể bao gồm nhiều hình học trải qua nhiều điều kiện tải trọng, độc lập với rời rạc hóa của chúng. Điều này mở ra cánh cửa cho các ứng dụng mới của mô hình giảm bậc. Chúng tôi hiện có thể tăng tốc các mô phỏng sửa đổi hình học tại thời gian chạy, ví dụ thông qua cắt, đục lỗ, và thậm chí hoán đổi toàn bộ lưới. Chúng tôi cũng có thể tăng tốc các mô phỏng của các hình học chưa được thấy trong quá trình đào tạo. Chúng tôi chứng minh khái quát hóa một lần, đào tạo trên một hình học duy nhất và sau đó mô phỏng các hình học chưa được thấy khác nhau.

## KHÁI NIỆM CCS
•Phương pháp tính toán → Mô phỏng vật lý.

## TỪ KHÓA
Mô phỏng vật lý, Mô hình giảm bậc, Biểu diễn neural ngầm, Trường Neural

## 1 GIỚI THIỆU

Mô hình giảm bậc (ROM) sử dụng không gian con tuyến tính để xấp xỉ không gian nghiệm có thể tăng tốc mô phỏng vật thể biến dạng theo cấp số nhân. Ý tưởng là tạo ra một số mẫu quỹ đạo mô phỏng, và sau đó xác định một cơ sở chiều thấp xấp xỉ các chuyển vị mẫu. Chúng tôi sau đó tính toán động lực học bằng cách chỉ phát triển số lượng nhỏ các hệ số của cơ sở này, được gọi là tọa độ giảm hoặc biến ẩn.

Các phương pháp ROM cổ điển giả định rằng các mẫu đầu vào và động lực học đầu ra đều được biểu diễn bởi một rời rạc hóa không gian nhất định, chẳng hạn như một lưới của miền Ω⊂R³. Sự phụ thuộc này vào một rời rạc hóa cụ thể có thể hạn chế.

Việc không bị ràng buộc từ một rời rạc hóa cụ thể là mong muốn khi các mẫu đầu vào được tạo ra sử dụng các lưới khác nhau (ví dụ, kết nối hoặc độ phân giải khác nhau); đầu ra mô phỏng được mong muốn cho các lưới khác nhau; chúng ta muốn tạo ra đầu ra mô phỏng thích ứng lưới theo thời gian hoặc tham số để phù hợp với biến dạng (ví dụ, tái lưới động, mô phỏng Lagrangian-Eulerian tùy ý).

Thật vậy, các biến thể không cần bị giới hạn ở kết nối và độ phân giải lưới: có thể chúng ta muốn thay đổi loại lưới (ví dụ, lưới tứ giác so với lưới tứ diện) hoặc thậm chí loại rời rạc hóa (ví dụ, lưới, tập điểm với bình phương tối thiểu di động tổng quát, hàm cơ sở radial, rời rạc hóa quang phổ).

Chúng tôi trình bày một phương pháp độc lập với rời rạc hóa như vậy cho mô hình giảm bậc. Phương pháp của chúng tôi giữ lại tính tuyến tính của không gian con của các phương pháp ROM thông thường, nhưng thay thế biểu diễn rời rạc của mỗi trường cơ sở chuyển vị bằng tương tự liên tục của nó.

Để làm cho mọi thứ cụ thể, hãy xem xét một phương pháp ROM cổ điển đơn giản gắn với một lưới có n đỉnh. Chúng tôi ký hiệu chuyển vị thay đổi theo thời gian của lưới từ cấu hình tham chiếu của nó bằng ū(t) với ū:T→R³ⁿ, trong đó T(⊆R) ký hiệu miền thời gian.

Chúng tôi sẽ đặt một thanh (ví dụ, ū(t)) trên những đại lượng phụ thuộc vào rời rạc hóa không gian, tức là những cái có chỉ số trong khoảng 1...n.

Trong ROM cổ điển, chúng tôi xấp xỉ chuyển vị thay đổi theo thời gian của lưới như một tổ hợp tuyến tính ū(t)≈Uq(t) của một số r≪n chiều, cơ sở độc lập thời gian U, trong đó q(t):T→Q là quỹ đạo giảm hoặc ẩn trong không gian con ẩn Q⊂Rʳ, và U∈M₃ₙₓᵣ(R) thường được tìm thấy thông qua Phân tích Trực giao Thích hợp¹ (POD) trên một tập đào tạo dữ liệu mô phỏng (chuỗi thời gian của các trường chuyển vị); M_{m×n}(A) ký hiệu tập hợp các ma trận m×n trên trường A. Mỗi cột U_k là một trường chuyển vị rời rạc cụ thể trên n đỉnh; các cột trực giao lẫn nhau {U₁...Uᵣ} tạo thành cơ sở cho không gian con chuyển vị rời rạc. Chúng tôi sẽ sử dụng kiểu chữ sans serif (U,q) để ký hiệu các đại lượng phụ thuộc vào chiều không gian con r.

¹POD cũng được biết đến như biến đổi Karhunen-Loève và có liên quan chặt chẽ đến Phân tích Thành phần Chính (PCA).

Bây giờ đây là trọng tâm của vấn đề: "kiến trúc" rời rạc của U được neo bất di bất dịch vào rời rạc hóa ban đầu. Hàng thứ j của U là cơ sở cho bậc tự do thứ j, trong đó 1≤j≤n. Thật vậy, đối với rời rạc hóa lưới, sự phát triển thời gian của ba bậc tự do liên quan đến đỉnh thứ i được cho bởi

ū_i(t) = W_i q(t), (1)

trong đó W_i∈M₁ₓᵣ(R³) là một ma trận 1×r (một vectơ hàng) của các hệ số có giá trị R³, tức là một vectơ chuyển vị cho mỗi r mode không gian con. Các hệ số 3×r của W_i được rút ra từ những 3 hàng của U tương ứng với đỉnh i. (Chúng tôi sẽ sử dụng chữ đậm để ký hiệu các mục có giá trị R³.)

Xếp chồng các vectơ hàng W_i của tất cả các đỉnh cho W∈M_{n×r}(R³), một ma trận n×r với các mục có giá trị R³, ánh xạ ū(t)=Wq(t). Về cơ bản, W mã hóa ánh xạ tuyến tính bất biến thời gian từ cấu hình ẩn q(t) đến chuyển vị không gian đầy đủ ū(t).

Chúng tôi gần như sẵn sàng cho bước mới của chúng tôi, chuyển đổi sang thiết lập mượt. Chúng tôi xem W=i↦W_i:{1,...,n}→M₁ₓᵣ(R³) như một ánh xạ từ chỉ số đỉnh đến vectơ hàng của trọng số không gian con. Đây là một ánh xạ rời rạc, và đó là những gì chúng tôi bây giờ sẽ làm mượt.

Thay cho ánh xạ rời rạc W, chúng tôi đề xuất thay vào đó sử dụng một ánh xạ liên tục W=X↦W(X):Ω→M₁ₓᵣ(R³) lấy một điểm X∈Ω trong miền tham chiếu đến trọng số không gian con của nó, sao cho

u(X,t) = W(X)q(t). (2)

So sánh với (1), chỉ số rời rạc phụ thuộc rời rạc hóa i được thay thế bằng điểm tham chiếu liên tục độc lập với rời rạc hóa X (xem Hình 2). Trường chuyển vị thay đổi theo thời gian và không gian u(X,t) là một tổ hợp tuyến tính của các trường chuyển vị thay đổi theo không gian, bất biến thời gian, có trọng số thay đổi theo thời gian, bất biến không gian được cho bởi q(t).

Để hỗ trợ trực giác, chúng ta cũng có thể so sánh các cột của U, W, và W. Trong tất cả các trường hợp, cột thứ k là một biểu diễn của một chuyển vị cụ thể - một phần tử cơ sở của không gian con xấp xỉ - như một trường trên toàn bộ miền Ω; sự khác biệt là W_k là một trường liên tục, trong khi những cái khác là các vectơ cột rời rạc.

Phương trình 2 là cơ sở cho ROM liên tục tuyến tính (LiCROM). Bây giờ tập đào tạo có thể bao trùm nhiều rời rạc hóa của cùng một hình học, hoặc thậm chí nhiều hình học. Điều này tạo điều kiện và mở rộng khả năng áp dụng của mô hình giảm bậc: Như chúng tôi sẽ chỉ ra, với LiCROM chúng ta có thể tính toán động lực học ẩn trên các hình học chưa được thấy trong quá trình đào tạo; các mô phỏng sửa đổi hình học tại thời gian chạy thông qua cắt, đục lỗ, hoặc hoán đổi toàn bộ lưới (Hình 3), mà không cần khởi tạo lại các tọa độ giảm.

## 2 CÔNG TRÌNH LIÊN QUAN

**Mô hình giảm bậc tuyến tính.** Các kỹ thuật giảm mô hình [Benner et al. 2015] đã được chứng minh là một công cụ mạnh mẽ để cho phép các mô hình độ trung thực cao chạy trong thời gian thực. Chúng đã được áp dụng thành công cho các vấn đề trong nhiều lĩnh vực, như động lực học chất lỏng [Bergmann et al. 2005; Carlberg et al. 2017, 2013; Hall et al. 2000; Kim et al. 2019; Kim and Delaney 2013; Lieu et al. 2006; Mainini and Willcox 2015; Treuille et al. 2006; Wiewel et al. 2019; Willcox and Peraire 2002], cơ học vật rắn [An et al. 2008a; Barbič and Zhao 2011; Barbič and James 2005; James et al. 2006; Kim and James 2009; Xu et al. 2015; Yang et al. 2015], chuyển động thứ cấp cho hoạt hình rigged [Benchekroun et al. 2023; Xu and Barbič 2016] và robot học [Katzschmann et al. 2019; Tan et al. 2020].

Thông thường, không gian giảm được học từ các mẫu đào tạo [Barbič and James 2005; Berkooz et al. 1993; Fulton et al. 2019], hoặc được xác định theo cách "không có dữ liệu" từ các nguyên lý đầu tiên về năng lượng [Pentland and Williams 1989; Shabana 2012; Sharp et al. 2023; Yang et al. 2015]. Các phương pháp "trực tuyến" cập nhật cơ sở tại thời gian chạy dựa trên quỹ đạo quan sát được [Kim and James 2009; Mukherjee et al. 2016; Ryckelynck 2005]; một phương pháp liên quan là nội suy giữa các cơ sở được tính toán trước [Xu and Barbič 2016]. Chúng tôi học một cơ sở cố định từ các mẫu mô phỏng.

Hầu hết các phương pháp giảm mô hình sử dụng một xấp xỉ không gian con tuyến tính cho động học. Các xấp xỉ như vậy chính xác cho các vấn đề hiển thị n-width Kolmogorov giảm nhanh [Pinkus 2012]. Tuy nhiên, gần như tất cả chúng hoạt động với một biểu diễn rời rạc; những cái hoạt động với biểu diễn liên tục (ví dụ, các phương pháp cơ sở giảm) về bản chất gắn với một sơ đồ rời rạc hóa không gian cơ bản. Đã có một vài phương pháp áp dụng các xấp xỉ động học phi tuyến, mà chúng tôi sẽ thảo luận dưới đây. Quan trọng là, hầu hết chúng cũng hoạt động trên một biểu diễn rời rạc, ngoại trừ CROM [Chen et al. 2023a,b], đã được áp dụng cho phương pháp điểm vật liệu và cho các phương trình vi phân riêng khác nhau.

Chúng tôi lấp đầy khoảng trống trong tài liệu bằng cách phát triển xấp xỉ động học tuyến tính đầu tiên cũng độc lập với bất kỳ rời rạc hóa không gian nào.

**Mô hình giảm bậc dựa trên học sâu.** Lee và Carlberg [2018] giới thiệu khung đầu tiên sử dụng autoencoder để nắm bắt các đa tạp phi tuyến. Fulton et al. [2019] mở rộng ý tưởng này, kết hợp nó với POD cho động lực học vật rắn biến dạng. Trong một phương pháp bổ sung, Shen et al. [2021] sử dụng autoencoder phi tuyến để thực hiện hiệu quả động lực học không gian ẩn dựa trên Hessian bằng cách tính toán chính xác các đạo hàm mạng neural bậc cao. Hơn nữa, Romero et al. [2021] giới thiệu hiệu chỉnh biến dạng do tiếp xúc gây ra với các mode không gian con tuyến tính. Trong khi đó, Luo et al. [2020] tập trung vào hiệu chỉnh chuyển vị, nhằm biến đổi phản ứng đàn hồi tuyến tính thành những phản ứng cấu thành phức tạp hơn.

**Biểu diễn độc lập với rời rạc hóa.** Gần đây, các biểu diễn neural ngầm đã trở thành một lĩnh vực khám phá thú vị trong nhiều lĩnh vực, bao gồm mô hình hóa hình dạng [Chen and Zhang 2019; Park et al. 2019], tái tạo 3D [Mescheder et al. 2019; Mildenhall et al. 2021], biểu diễn và tạo hình ảnh [Chen et al. 2021; Shaham et al. 2021; Skorokhodov et al. 2021], và các vấn đề bị ràng buộc PDE [Chen et al. 2022; Raissi et al. 2019; Yang et al. 2021; Zehnder et al. 2021].

Aigerman et al. [2022] đề xuất một khung để dự đoán chính xác các ánh xạ tuyến tính từng phần của các lưới tùy ý sử dụng mạng neural. Nó hoạt động với các bộ sưu tập lưới không đồng nhất mà không yêu cầu tam giác hóa chung. Những người khác nhằm học biểu diễn không gian ẩn của các vectơ liên tục: Chen et al. [2023a] đề xuất một phương pháp giảm mô hình cho phương pháp điểm vật liệu, trong khi Chen et al. [2023b] và Pan et al. [2023] học một không gian ẩn độc lập với rời rạc hóa cho PDE. Theo hiểu biết tốt nhất của chúng tôi, cấu trúc phân tích nguyên mẫu của ROM tuyến tính, W(X)q(t), chưa được xem xét trong bối cảnh biểu diễn liên tục độc lập với rời rạc hóa cho giảm mô hình.

## 3 HỌC KHÔNG GIAN CON MÙ RỜI RẠC HÓA

Chúng tôi đào tạo LiCROM trên một quỹ đạo quan sát được của một vật thể biến dạng. Để đơn giản hóa ký hiệu, giả sử một quỹ đạo được lấy mẫu tại các thời điểm {t₁,...,t_m}, mặc dù phương pháp một cách tầm thường tổng quát hóa để lấy mẫu nhiều quỹ đạo hoặc nhiều vật thể với các quỹ đạo song song.

Gọi X={(X̃₁,ũ₁),...,(X̃_m,ũ_m)} là tập đào tạo, trong đó (X̃_j,ũ_j) thu thập các quan sát của trường chuyển vị tại thời gian t_j. Cụ thể, ũ_j={u_j¹,u_j²,...}⊂R³ bao gồm một số hữu hạn các quan sát u_j^i≡u(X_j^i,t_j) của trường chuyển vị tại các vị trí tham chiếu X̃_j≡{X_j¹,X_j²,...}. Chúng tôi không giả định một cấu trúc nhất quán giữa các đám điểm, tức là các vị trí mẫu X_j^i và X_{j+1}^i không cần bằng nhau, cũng không phải số lượng mẫu |X̃_j| và |X̃_{j+1}|.

Chúng tôi tìm kiếm một không gian con chiều thấp bao trùm tất cả các trường quan sát được (X̃_j,ũ_j). Cụ thể, chúng tôi tìm kiếm một phép chiếu P:(X̃_j,ũ_j)↦q_j∈Q, và một cơ sở tương ứng W (độc lập với j) sao cho

W(X_i)P(X̃_j,ũ_j) ≈ u_j^i, ∀(X̃_j,ũ_j) ∈ X, ∀X_i ∈ X̃_j. (3)

Chúng tôi áp dụng một dạng tham số cho P và W, cụ thể là một encoder PointNet [Qi et al. 2017] và trường ngầm neural [Mescheder et al. 2019; Park et al. 2019], tương ứng, và tối ưu hóa các tham số để giảm thiểu residual chuẩn bình phương của (3), như được mô tả trong Hình 4.

### 3.1 Phương pháp Đào tạo

Trong các thí nghiệm của chúng tôi, chúng tôi tạo ra tập đào tạo sử dụng các mô phỏng dựa trên rời rạc hóa lưới tứ diện. Tuy nhiên, quan sát rằng mạng không trực tiếp "biết" rằng đầu vào được tạo bởi một lưới, chỉ rằng một trường chuyển vị được lấy mẫu được tạo ra bằng cách nào đó. Mạng nhằm tìm một cơ sở giảm có thể tái tạo tất cả các trường chuyển vị quan sát được, mà không xem xét tải trọng, điều kiện biên, hình học, hoặc rời rạc hóa.

Để tạo ra tập đào tạo của chúng tôi, trước tiên chúng tôi tạo một lưới tứ diện thể tích cho mỗi hình học sử dụng TetWild [Hu et al. 2018], và sau đó thực hiện mô phỏng không gian đầy đủ mong muốn sử dụng triển khai taichi dựa trên CPU [Hu et al. 2019] mà theo sát bộ tích phân FEM ngầm mặc định trong warp [Macklin 2022]. Chúng tôi lặp lại quá trình này để tạo ra một tập kết quả mô phỏng. Triển khai của chúng tôi sử dụng cùng số lượng ñ=|(X̃_j,ũ_j)| cho các chuyển vị dựa trên đỉnh được lấy mẫu ngẫu nhiên của mỗi khung hình động, điều này đơn giản hóa xử lý hàng loạt trong PyTorch. Chúng tôi thấy rằng đào tạo encoder PointNet P có thể tốn kém khi (ñ>5000), tuy nhiên sử dụng một số lượng lớn hữu ích cho độ bao phủ của miền. Do đó, chúng tôi tiếp tục lấy mẫu phụ ≈n<ñ đỉnh cho encoder PointNet.

Chúng tôi xác định các tham số cho P và W bằng cách giảm thiểu mất mát tái tạo L₂

L = Σ(j=1 to m) Σ(i=1 to ñ) ||W(X_i)P∘S_{≈n}(X̃_j,ũ_j) - u_j^i||², (4)

trong đó S_{≈n} là toán tử lấy mẫu phụ. Chúng tôi sử dụng ≈n=2500 cho tất cả các ví dụ.

**Kiến trúc PointNet.** Encoder PointNet P bất biến dưới hoán vị của các điểm đầu vào, một tính năng mong muốn cho các tập không có thứ tự của chúng tôi. Một PointNet tiêu chuẩn cũng bất biến dưới các biến đổi đầu vào do mạng biến đổi đặc trưng giai đoạn đầu vào của nó; chúng tôi đã loại bỏ giai đoạn này vì các biến ẩn không gian ẩn không bất biến dưới các biến đổi của chuyển vị. Đầu vào cho PointNet là một tập không có thứ tự các điểm (X_i,u_i)∈R³×R³≡R⁶ và đầu ra là q.

**Kiến trúc trường neural.** Kiến trúc cho trường neural W là một perceptron đa lớp (MLP) 5 lớp có độ rộng 60 với các hàm kích hoạt ELU [Clevert et al. 2016]. Chúng tôi sử dụng kiến trúc này cho tất cả các ví dụ được trình bày, tuy nhiên, chúng tôi thấy rằng các lựa chọn thay thế như SIREN [Sitzmann et al. 2020] cũng có thể tạo ra kết quả tốt.

**Học các tham số mạng.** Chúng tôi sử dụng PyTorch Lightning để triển khai toàn bộ pipeline đào tạo [Falcon and The PyTorch Lightning team 2019]. Chúng tôi áp dụng bộ tối ưu Adam [Kingma and Ba 2017] và áp dụng khởi tạo Xavier. Chúng tôi đào tạo mô hình trong 3750 epoch với tốc độ học cơ bản lr=10⁻³. Sau 1250 epoch đầu tiên, chúng tôi chia tốc độ học cho 5, sau đó chúng tôi tiếp tục chia nó cho 10 sau 1250 epoch nữa. Chúng tôi sử dụng kích thước lô 16 cho đầu vào của mạng, vì vậy kích thước lô là 16·ñ cho W.

## 4 ĐỘNG LỰC HỌC THÔNG QUA TÍCH PHÂN NGẦM

Chúng tôi công thức hóa một bước thời gian ngầm trong khung của các bộ tích phân thời gian tối ưu hóa [Martin et al. 2011; Pan et al. 2015; Stuart and Humphries 1996], được sử dụng gần đây cho động lực học không gian ẩn bởi Fulton et al. [2019]. Cấu hình q ở cuối bước thời gian thứ (j+1) giảm thiểu

E(q) = ∫_{X∈Ω} [1/(2h²)||W(X)q - u_{pred}||²_g + Ψ(X + W(X)q)] dVol, (5)

trong đó h là thời lượng của bước thời gian, g là chuẩn năng lượng động học², và Ψ(x) là mật độ năng lượng đàn hồi, trong triển khai của chúng tôi là stable neohookean [Smith et al. 2018]. Dự đoán tường minh cho bước thời gian thứ (j+1)

u_{pred}^{j+1} = u^j + hv^n + h²M^{-1}f_{ext} (6)

yêu cầu vận tốc không gian đầy đủ được cho bởi sai phân hữu hạn

v^n = (u^n - u^{n-1})/h = W(X)(q^n - q^{n-1})/h = W(X)q̇^n, (7)

trong đó (bởi tính tuyến tính của không gian con) q̇^n = (q^n - q^{n-1})/h.

Chúng tôi xấp xỉ tích phân miền (5) thông qua cubature

E(q) ≈ Σ_i [w_i/(2h²)||W(X_i)q - u_{pred}||²_g + w_iΨ(X_i + W(X_i)q)], (8)

trong đó w_i là trọng số của điểm cubature thứ i X_i. Triển khai của chúng tôi thực hiện tính toán cubature và mật độ năng lượng sử dụng một lưới, được thúc đẩy bởi các phương pháp có sẵn cho các biến dạng thể tích [An et al. 2008b], mặc dù toán học không gắn với cubature dựa trên lưới.

²Chuẩn năng lượng động học ||v(X)||_g = ∫_Ω ½ρ(X)v(X)² dVol, trong đó ρ(X) là mật độ khối lượng.

Bất kể, lưới cubature không và không cần gắn với biểu diễn của dữ liệu đào tạo. Hơn nữa, lưới cubature không cần giống nhau qua các bước thời gian, vì trạng thái được mang qua các bước thời gian bởi cấu hình ẩn q. Cubature nên được chọn để kiểm soát đầy đủ xấp xỉ (8) và để thực thi các điều kiện biên thiết yếu.

Tự do này làm cho các kịch bản có thay đổi kết nối (ví dụ, gãy, cắt), và thay đổi cấu trúc tôpô (ví dụ, đục lỗ, phát triển khoảng trống) trở nên dễ dàng một cách sảng khoái: chúng ta chỉ đơn giản chọn một sơ đồ cubature thích hợp cho bước thời gian tiếp theo. Ví dụ, nếu một lỗ được đục tức thì, chúng ta chỉ đơn giản kiềm chế việc tích phân trên miền bị cắt bỏ, bằng cách chuyển sang một lưới cubature phản ánh cấu trúc tôpô được sửa đổi và điều kiện biên được sửa đổi.

Một lựa chọn thay thế cho việc chuyển đổi lưới sẽ là bỏ qua các điểm cubature nằm trong khoảng trống. Điểm mũi chốt là có rất nhiều tự do trong phương pháp - thậm chí qua các bước thời gian - để tích phân của tích phân miền (8), bởi vì biểu diễn của cấu hình, q, được tách biệt khỏi biểu diễn của cubature.

## 5 GIẢM THIỂU THÔNG QUA CUBATURE

**Giảm thiểu.** Chúng tôi giảm thiểu E(q) sử dụng gradient descent [Macklin 2022]. Chúng tôi khởi tạo gia số tại mỗi điểm cubature với dự đoán bước thời gian tường minh Δu_i = hv_j + h²M^{-1}f_{ext}. Tại mỗi lần lặp descent, chúng tôi tính toán các gia số tại tất cả các điểm cubature, và sau đó tìm gia số khớp nhất với cấu hình ẩn. Gia số descent tại điểm cubature thứ i là

Δu_i = αM/(h²)(W(X_i)q - q_{pred}) + ∂Ψ(X_i + W(X_i)q)/∂W(X_i)q. (9)

Sau khi đánh giá gia số không gian đầy đủ tại mỗi điểm cubature, chúng tôi chiếu để tìm gia số không gian con khớp nhất bằng cách giảm thiểu bậc hai

Δq = arg min_{Δq} Σ_i w_i ||W(X_i)Δq - Δu_i||², (10)

điều này tương đương với việc giải một hệ tuyến tính xác định dương đối xứng. Ma trận chỉ phụ thuộc vào vị trí và trọng số của các điểm cubature, và trong khi những điều này bất biến, một phân tách Cholesky duy nhất cho phép các phép chiếu lặp lại thông qua thay thế ngược. Khi tập cubature thay đổi, chúng tôi lắp ráp lại ma trận hệ thống.

Vì W(X_i) là một hàm của X_i, chúng tôi cache nó tại mỗi điểm cubature, loại bỏ suy luận mạng W(X_i) ngoại trừ tại các điểm cubature được giới thiệu mới.

**Lấy mẫu Cubature.** Lấy mẫu cubature trước đây [An et al. 2008b; von Tycowicz et al. 2013] cung cấp kết quả hứa hẹn. Người ta tạo ra một tập các tư thế đào tạo cho quá trình tiền xử lý tối ưu hóa cubature. Quá trình tiền xử lý này xác định các điểm cubature mong muốn và các trọng số không âm liên quan để đạt được xấp xỉ năng lượng chính xác trên các tư thế đào tạo.

Nhưng còn về tích phân động lực học không gian con trên các lưới mới chưa được thấy trong quá trình đào tạo? Trong trường hợp này, phương pháp nói trên không trực tiếp áp dụng được. Chúng tôi triển khai một sơ đồ cubature ngây thơ, mà chúng tôi thấy thỏa đáng cho các ví dụ mà chúng tôi đã thử nghiệm. Chúng tôi (1) chọn m đỉnh ngẫu nhiên từ lưới tứ diện, và (2) bổ sung, chọn tất cả các đỉnh liên kết đến m đỉnh. Các bước này tạo ra các điểm cubature có trọng số bằng nhau {X_i}.

Trong tất cả các kết quả được trình bày không liên quan đến tái lưới, chúng tôi tính toán trước sơ đồ cubature. Đối với các ví dụ tái lưới, các điểm cubature về nguyên tắc sẽ thay đổi (cục bộ) khi lưới được thay đổi (cục bộ). Trong minh chứng đơn giản hóa của chúng tôi về tái lưới, nơi chúng tôi biết trước chuỗi các lưới, chúng tôi tính toán trước các điểm cubature và W(X_i) cho tất cả các lưới.

## 6 KẾT QUẢ

Chúng tôi tiến hành các thí nghiệm để đánh giá các tính năng độc đáo của LiCROM. Chúng tôi hỏi liệu một cơ sở neural, (W,q), có thể

(1) được đào tạo trên các đầu vào đa dạng được tạo bởi các lưới khác nhau?
(2) tái tạo các biến dạng trên các hình học được thấy trong quá trình đào tạo?
(3) và trên các hình học mới chưa được thấy trong quá trình đào tạo?
(4) tạo điều kiện cho các thay đổi kết nối lưới và cấu trúc tôpô?

### 6.1 Khả năng độc đáo của ROM liên tục

**Đào tạo với các hình dạng khác nhau.** Chúng tôi đào tạo một không gian con neural (W,q) sử dụng một tập đào tạo bao gồm các hình dạng khác nhau biến dạng dưới tải trọng tương tự, và hỏi liệu động lực học không gian con có tái tạo các hành vi khác nhau của các hình dạng được bao gồm trong tập đào tạo. Chúng tôi tạo ra năm hình dạng trải dài từ khối vuông đến hình cầu, với các khối vuông bao quanh bằng nhau, [±0.5,±0.5,±0.5]. Chúng tôi quy định chuyển vị nén bằng nhau: cho mỗi đỉnh có vị trí không biến dạng gần đỉnh (y<0.45) hoặc đáy (y>-0.45) chúng tôi quy định vận tốc hướng xuống bằng nhau (-2m/s) hoặc tĩnh (0m/s). Một tập đào tạo duy nhất bao gồm các biến dạng động không gian đầy đủ cho năm lưới này. Chúng tôi mô phỏng cùng năm hình dạng trong mô hình giảm (xem Hình 5), ghi nhận sự thỏa thuận với dữ liệu đào tạo.

Chúng tôi lặp lại thí nghiệm này, lần này với động lực học không gian con trên các hình dạng mới chưa được thấy trong quá trình đào tạo (xem Hình 7). Chúng tôi quan sát thỏa thuận tốt cho biến dạng tổng thể, mặc dù đôi khi thiếu các chi tiết bề mặt, khi những điều này không được thấy trong quá trình đào tạo.

**Xé động học.** Bởi vì lưới cubature không mang trạng thái, nó không cần gắn với việc lưới được sử dụng trong các bước thời gian trước hoặc giai đoạn đào tạo. Kết hợp với khả năng đào tạo trên các lưới có kết nối khác nhau, những đặc điểm này làm cho mô hình không gian con của xé và gãy dễ dàng hơn (xem Hình 6). Trong quá trình tính toán trước, chúng tôi đào tạo một cơ sở neural sử dụng một tập đào tạo duy nhất bao gồm hai mô phỏng không gian đầy đủ: (1) một tấm kẹp sa xuống dưới trọng lực; (2) cùng tấm đó, với một vết cắt hình Y, sa xuống dưới trọng lực.

Trong giai đoạn trực tuyến, chúng tôi mô hình hóa việc xé tấm sử dụng động lực học không gian con. Theo thời gian, chúng tôi từng bước định nghĩa lại lưới cubature để phát triển một vết cắt hình Y (xem Hình 6). Các vết cắt được giới thiệu trong lưới cubature có hiệu quả mong muốn trên tính toán lực, nhưng chúng không yêu cầu chuyển giao các biến trạng thái từ lưới trước đó. Nhớ lại rằng tập đào tạo chỉ bao gồm hình học nguyên vẹn và hoàn toàn bị cắt; các biến dạng cho vết cắt một phần phát sinh (như trong tất cả các phương pháp không gian con tuyến tính) từ một tổng có trọng số của các trường chuyển vị được tính toán trước.

Một câu hỏi tự nhiên sau đó là "trường chuyển vị neural liên tục nắm bắt tốt như thế nào một biến dạng không liên tục?" Điều này đặc biệt nọn bén khi triển khai của chúng tôi sử dụng các hàm kích hoạt ELU mượt. Chúng tôi hình dung trường chuyển vị cơ sở W(X) (xem Hình 8), quan sát sự không liên tục.

Vì cơ sở neural không có "kiến thức" về hình học, cubature chịu trách nhiệm đầy đủ cho việc cung cấp kiến thức hình học, và do đó tạo ra động lực học khác biệt cho các hình học khác biệt. Lấy mẫu dưới tạo ra các artifact (xem Hình 9). Sử dụng 3713 mẫu ngẫu nhiên (so với 20k đỉnh trong dữ liệu gốc) đủ để có được tăng tốc 29× so với mô phỏng không gian đầy đủ.

**Đục lỗ.** Ngoài việc mô phỏng gãy, phương pháp của chúng tôi có khả năng mô phỏng quá trình đục khối vuông và tạo ra khoảng trống trong thời gian thực. Trong ví dụ được hiển thị trong Hình 10, chúng tôi chạy mô phỏng trên 5 lưới với đáy cố định dưới trọng lực. Sau khi đào tạo, chúng tôi có thể mô phỏng quá trình khối vuông bị "hư hại" (tức là các lỗ được cắt ra) bởi tái lưới thời gian chạy. Lưu ý rằng sau mỗi lần tái lưới, vị trí biến dạng của phần còn lại của khối vuông nhất quán với khung trước khi tái lưới, ngoại trừ phần trống được tạo mới.

**Động vật lăn.** Phương pháp của chúng tôi có thể mô phỏng va chạm và ma sát giữa các động vật và mặt phẳng nghiêng tĩnh. Đối với ví dụ được hiển thị trong Hình 11, khi tạo dữ liệu đào tạo, chúng tôi mô phỏng một động vật đàn hồi dưới trọng lực tĩnh g=-9.8m/s. Trong mỗi khung, chúng tôi kiểm tra xem có đỉnh nào giao với một mặt phẳng vô hạn có pháp tuyến [0,√2/2,√2/2]. Nếu một giao điểm xảy ra, chúng tôi áp dụng một lực phạt dọc theo pháp tuyến của mặt phẳng để xử lý va chạm và đặt vận tốc trực giao với pháp tuyến mặt phẳng về không (lực ma sát vô hạn). Kết quả cho thấy rằng động lực học không gian ẩn của chúng tôi có thể tái tạo tương tác va chạm và lăn giữa các động vật khác nhau và mặt phẳng.

**Nội suy động vật.** Sau khi đào tạo trên ba động vật trong Hình 12, chúng tôi nội suy giữa ba lưới này thông qua khoảng cách Wasserstein [Solomon et al. 2015]. Nhờ bản chất độc lập với rời rạc hóa của phương pháp chúng tôi, chúng ta có thể dễ dàng triển khai mô hình đã đào tạo trước đó với tất cả các lưới này. Hình 12 chứng minh động lực học không gian ẩn tương ứng cho mỗi lưới.

### 6.2 Ứng dụng tương tác

Chúng tôi đào tạo một cơ sở neural trên các biến dạng được tạo ra bởi việc kéo tại armadillo (xem Hình 3). Các mô phỏng không gian đầy đủ và giảm yêu cầu 335ms và 6ms mỗi bước thời gian, tương ứng, trên Intel Core i7-10750. Tăng tốc 56× cho phép thao tác tương tác ở 30 khung hình mỗi giây.

Người dùng cũng có thể tải các mô hình hình học chưa được thấy trước đó có thể được hoán đổi cho armadillo, giữa mô phỏng, mà không cần thiết lập lại cấu hình động học hoặc động lượng. Lưu ý rằng phản ứng vật lý được đánh giá trên hình học hiện tại. Mặc dù đào tạo động học chỉ được tiến hành duy nhất trên armadillo, phản ứng vật lý phản ánh hình học, như rõ ràng, ví dụ, trong các dao động tần số cao hơn của các cánh tay mỏng hơn. Điều này chứng minh tiềm năng khái quát hóa một lần của LiCROM. Theo hiểu biết tốt nhất của chúng tôi, đây là minh chứng tốc độ tương tác đầu tiên của giảm mô hình bao gồm thay thế trực tuyến của mô hình hình học, bao gồm các mô hình hình học chưa được thấy trước đó. Thật vậy, bằng cách đào tạo trên một hình học duy nhất, phương pháp của chúng tôi khái quát hóa cho các hình học khác, hiệu quả đạt được khái quát hóa một lần.

### 6.3 So sánh với CROM phi tuyến

Phương pháp của chúng tôi chia sẻ động lực tương tự với Mô hình Giảm Bậc Liên tục (CROM) [Chen et al. 2023a,b]. Cả hai đều tìm kiếm sự độc lập với rời rạc hóa. Trong CROM, một decoder phi tuyến (q,X)↦u ánh xạ cấu hình giảm và vị trí tham chiếu đến vị trí biến dạng tương ứng. So với một cơ sở tuyến tính, một phương pháp phi tuyến có thể phức tạp hơn để triển khai, phân tích và tính toán, hoặc dữ liệu đào tạo được chọn cẩn thận hơn để tránh overfitting. Chúng tôi quan sát các artifact khi áp dụng CROM cho mô phỏng biến dạng, điều này thúc đẩy điều tra của chúng tôi vào một không gian con tuyến tính (xem Hình 14).

LiCROM cung cấp một lợi thế quan trọng so với CROM trong phép chiếu (10), mà, do tính tuyến tính của cơ sở, trở thành một giảm thiểu đơn giản của một bậc hai, tức là nghiệm của một hệ tuyến tính mà, bằng phân tách trước, có thể được tái sử dụng cùng với các điểm cubature. Ngược lại, tính phi tuyến của cơ sở CROM [Chen et al. 2023b] không cho phép một phép chiếu tầm thường như vậy.

Chúng tôi tận dụng phép chiếu nhanh này (tương đương với chỉ thay thế ngược trên ma trận được phân tách trước) để triển khai bước thời gian ngầm, yêu cầu các phép chiếu lặp lại mỗi bước thời gian. Trong CROM phi tuyến, mỗi phép chiếu như vậy sẽ yêu cầu nhiều đánh giá Jacobian mạng tốn kém.

## 7 THẢO LUẬN

Chúng tôi đã trình bày phương pháp giảm mô hình tuyến tính độc lập với rời rạc hóa đầu tiên, theo nghĩa là cơ sở không gian con không lưu trữ, tham chiếu, hoặc dựa vào các chi tiết cụ thể của các rời rạc hóa được sử dụng để tạo ra tập đào tạo, tích phân các lực, hoặc xuất ra hoạt hình kết quả.

Sự độc lập với rời rạc hóa này đạt được bằng cách định nghĩa các vectơ cơ sở không gian con như các trường chuyển vị liên tục trên miền tham chiếu, mà chúng tôi triển khai sử dụng các trường ngầm neural. Do đó, chúng tôi có thể chứng minh rằng một mô hình không gian con duy nhất có thể được đào tạo từ các rời rạc hóa khác nhau hoặc thậm chí các hình học khác nhau. Cơ sở đã học có thể tăng tốc mô phỏng khoảng 20-50× trong khi hỗ trợ các hiện tượng thường không được thấy trong các phương pháp không gian con, như các hiện tượng thường yêu cầu tái lưới (ví dụ, cắt), thay đổi cấu trúc tôpô (ví dụ, đục lỗ), hoặc hình học mới chưa được thấy trong quá trình đào tạo.

**Hạn chế.** Những tính năng mới này đi kèm với những hạn chế mới. Đầu tiên, không gian con đã đào tạo tất nhiên bị giới hạn bởi dữ liệu quan sát được. Đối với một trường ngầm neural, hạn chế thông thường này đi kèm với một khía cạnh mới: trường sẽ không ngần ngại "ảo tưởng" một kết quả ngoại suy trong các phần của miền tham chiếu Ω có ít hoặc không có quan sát dữ liệu. Như một hệ quả, nếu chúng ta đào tạo một cơ sở chuyển vị trên một hình học mỏng, cơ sở này có thể không phù hợp cho một hình học dày, nơi một số điểm cubature sẽ lấy mẫu một ngoại suy có thể không phù hợp của trường chuyển vị. Sẽ thú vị khi kết hợp các regularizer cho ngoại suy như vậy [Liu et al. 2022]. Hình 15 (a) và (b) chứng minh hai mode thất bại khái quát hóa của phương pháp chúng tôi: điều kiện tải trọng và kích thước hình học rất khác nhau.

Thật vậy, vì việc đào tạo của không gian con không có kiến thức tường minh về hình học, không gian con đã đào tạo có thể thất bại trong việc tái tạo một số chi tiết bề mặt nhất định khi được thử nghiệm trên hình học mới không được bao gồm trong dữ liệu đào tạo, như được hiển thị trong Hình 7(b). Sẽ thú vị khi cải thiện hạn chế này bằng cách giới thiệu một "mã hình học" tường minh khi đào tạo và sau đó sử dụng mạng.

Thứ hai, việc đào tạo kết hợp của một trường ngầm neural và PointNet tốn kém so với POD, yêu cầu nhiều giờ. Đây là chi phí chúng tôi đánh đổi cho lợi ích của tính bất biến hoán vị của PointNet. Thú vị là, nếu tính bất biến hoán vị này bị loại bỏ để ưu tiên một decoder đơn giản hơn, phụ thuộc hoán vị, một số khía cạnh của sự độc lập với rời rạc hóa sẽ vẫn còn. Cụ thể, trong khi embedding kết quả sẽ không còn độc lập với rời rạc hóa đầu vào, cơ sở trường chuyển vị kết quả vẫn sẽ liên tục và do đó không áp đặt bất kỳ rời rạc hóa nào trên sơ đồ cubature hoặc đầu ra động lực học không gian con. Công việc tương lai có thể liên quan đến việc tăng tốc đào tạo trong khi giữ lại tính bất biến hoán vị.

Mô hình của chúng tôi cũng chia sẻ những thiếu sót và lợi ích của các phương pháp giảm mô hình không gian con tuyến tính: chiều của không gian con thường vượt quá so với các phương pháp phi tuyến, bất kể trường chuyển vị được mã hóa như một trường rời rạc [Fulton et al. 2019] hay liên tục [Chen et al. 2023a,b]. Tuy nhiên - ngoại trừ các phương pháp được phát triển trong cộng đồng toán tính toán [Lee and Carlberg 2018] - trạng thái nghệ thuật trong các phương pháp phi tuyến (đặc biệt trong đồ họa) vẫn dường như dựa vào các không gian con tuyến tính để regularization [Fulton et al. 2019; Shen et al. 2021]; có lẽ những regularization tương tự có thể được áp dụng trong miền liên tục, ví dụ, bằng cách regularizing CROM với LiCROM.

Không giống các ROM không gian con tuyến tính khác, của chúng tôi không được đào tạo sử dụng POD, cũng không có mục tiêu đào tạo yêu cầu tường minh tính trực giao. Tính trực giao tối ưu hóa điều kiện của cơ sở, và mong muốn để giảm lỗi trong quá trình chiếu; chúng tôi không quan sát bất kỳ thách thức nào với phép chiếu. Chúng tôi dự định đánh giá góc giữa các vectơ cơ sở và báo cáo điều này trong tương lai gần.

**Công việc tương lai.** Triển khai sơ bộ của chúng tôi để lại nhiều bước tức thì. Chúng tôi sử dụng một phương pháp lấy mẫu cubature ngẫu nhiên với trọng số bằng nhau, chỉ vì tính đơn giản và tức thì của nó. Nhớ lại rằng vết xé hình Y yêu cầu 3.7k mẫu ngẫu nhiên. Có vẻ hợp lý khi mong đợi rằng một phương pháp lấy mẫu nhận biết dữ liệu theo tinh thần của An et al. [2008b] có thể giảm số lượng điểm cubature. Vì các ví dụ của chúng tôi bao gồm các hình học chưa được thấy trong quá trình đào tạo, chiến lược lấy mẫu sẽ phải được thích ứng với dữ liệu tại thời gian chạy.

Theo warp, chúng tôi sử dụng gradient descent để giảm thiểu năng lượng, tuy nhiên, có nhiều lựa chọn thay thế. Ví dụ, triển khai của chúng tôi ngay lập tức phù hợp để kết hợp một solver (L-)BFGS, xấp xỉ phương pháp Newton mà không sử dụng Hessian. Thật vậy, do tính tuyến tính của không gian con, việc tính toán Hessian năng lượng giảm, như yêu cầu cho một phương pháp Newton chính xác, rất đơn giản thông qua Hess_q Ψ(X+u(q)) = W(X)^T Hess_u Ψ(X + W(X)q)W(X), có thể được lắp ráp thông qua cubature tại {X_i}. Lưu ý rằng việc đánh giá Hessian chính xác không yêu cầu vi phân qua mạng neural, điều mà sẽ là trường hợp cho một không gian con phi tuyến [Chen et al. 2023b; Fulton et al. 2019].

Mặc dù chúng tôi bắt đầu với warp trên GPU, cuối cùng chúng tôi triển khai động lực học không gian con trực tuyến của chúng tôi hoàn toàn trên CPU với taichi. Trong các miền ứng dụng dự định của chúng tôi (thực tế ảo, trò chơi) có sự cạnh tranh đáng kể về tăng tốc GPU, chủ yếu được dành cho rendering. Đạt được tốc độ tương tác trên CPU, mặc dù hạn chế hơn, là một tiêu chí quan trọng. Tuy nhiên, cho các trường hợp sử dụng khác, một triển khai GPU nhanh vẫn mong muốn, và chúng tôi dự định tái triển khai phương pháp này trên GPU.

**Mã nguồn mở.** Triển khai LiCROM của chúng tôi sẽ được phát hành.

## LỜI CẢM ƠN

Chúng tôi muốn cảm ơn Otman Benchekroun, Jonathan Panuelos, Kateryna Starovoit, và Mengfei Liu cho phản hồi của họ về Hình 1. Chúng tôi cũng muốn cảm ơn quản trị viên hệ thống phòng thí nghiệm của chúng tôi, John Hancock, và nhân viên tài chính của chúng tôi, Xuan Dam, cho sự hỗ trợ hành chính vô giá của họ trong việc làm cho nghiên cứu này có thể. Dự án này được tài trợ một phần bởi Meta và Hội đồng Nghiên cứu Khoa học Tự nhiên và Kỹ thuật của Canada (Discovery RGPIN-2021-03733). Chúng tôi cảm ơn các nhà phát triển và cộng đồng đằng sau PyTorch, ngôn ngữ lập trình Taichi, và NVIDIA Warp vì đã trao quyền cho nghiên cứu này. Các lưới trong Hình 3 được dẫn xuất từ các mục 133568, 133078 và 170179 của bộ dữ liệu Thingi10k [Zhou and Jacobson 2016].

## TÀI LIỆU THAM KHẢO

[Tài liệu tham khảo được giữ nguyên như trong bản gốc do tính chất kỹ thuật và các tiêu chuẩn trích dẫn quốc tế]
