# 2309.01252.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl-papers/2309.01252.pdf
# Kích thước tệp: 7001835 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
S2RF: Trường Bức Xạ Được Cách Điệu Hóa Theo Ngữ Nghĩa
Moneish Kumar∗Neeraj Panse∗
Viện Robotics, Đại học Carnegie Mellon
{moneishk, npanse, dishanil }@andrew.cmu.eduDishani Lahiri∗

Tóm tắt
Chúng tôi trình bày phương pháp của mình để chuyển giao phong cách từ bất kỳ hình ảnh tùy ý nào sang các đối tượng trong một cảnh 3D. Mục tiêu chính của chúng tôi là cung cấp nhiều kiểm soát hơn trong việc cách điệu hóa cảnh 3D, tạo điều kiện thuận lợi cho việc tạo ra các hình ảnh cảnh có thể tùy chỉnh và được cách điệu hóa từ các góc nhìn tùy ý. Để đạt được điều này, chúng tôi đề xuất một phương pháp mới kết hợp tổn thất dựa trên láng giềng gần nhất, cho phép tái tạo cảnh 3D linh hoạt đồng thời nắm bắt hiệu quả các chi tiết phong cách phức tạp và đảm bảo tính nhất quán đa góc nhìn.

1. Giới thiệu
Trong nhiều thập kỷ, việc khôi phục thông tin ba chiều (3D) từ hình ảnh hai chiều (2D) đã đặt ra một thách thức dai dẳng trong lĩnh vực thị giác máy tính. Với sự ra đời của các phương pháp kết xuất khả vi tiên tiến [17, 24], những phương thức mới thú vị đã xuất hiện, cho phép tái tạo các cảnh 3D có độ trung thực cao [21, 11, 7] và hiệu quả [4, 9, 7].

Khi những tiến bộ làm cho việc tái tạo 3D trở nên dễ tiếp cận hơn, có một nhu cầu ngày càng tăng về việc chỉnh sửa và thao tác các cảnh này. Khả năng chỉnh sửa cảnh 3D trao quyền cho các nhà sáng tạo vượt qua ranh giới của trước tượng và độ chính xác. Một ứng dụng chỉnh sửa như vậy là chuyển giao phong cách 3D, nhằm chuyển giao các đặc trưng nghệ thuật từ một hình ảnh 2D duy nhất sang một cảnh 3D thế giới thực. Nhiều công trình đáng chú ý [6, 8, 10, 12, 20, 25] đã thành công trong việc đạt được mục tiêu này. Tuy nhiên, những phương pháp này chủ yếu tập trung vào việc cách điệu hóa toàn bộ cảnh bằng cách chỉ sử dụng một hình ảnh phong cách duy nhất.

Mục tiêu chính của bài báo này là nâng cao mức độ kiểm soát trong khi cách điệu hóa cảnh 3D. Với phương pháp của chúng tôi, các hình ảnh cảnh được cách điệu hóa có thể tùy chỉnh cao có thể được tạo ra từ các góc nhìn mới tùy ý. Nó không chỉ tạo điều kiện thuận lợi cho việc cách điệu hóa các đối tượng riêng lẻ mà còn đảm bảo rằng các hình ảnh được kết xuất duy trì tính nhất quán không gian. Hình 1 cung cấp một bản tóm tắt về phương pháp cách điệu hóa của chúng tôi. Theo hiểu biết tốt nhất của chúng tôi, đây là phương pháp đầu tiên cung cấp một khung duy nhất cho việc chuyển giao phong cách ở cấp độ ngữ nghĩa và instance cho các đối tượng trong cảnh 3D.

Tương tự như các phương pháp trước đây [25, 12, 2, 26, 8, 6], giải quyết chuyển giao phong cách trong 3D, chúng tôi đã áp dụng một phương pháp dựa trên tối ưu hóa. Những phương pháp này nhằm mục đích tối thiểu hóa (i) tổn thất nội dung, đánh giá sự khác biệt giữa hình ảnh được cách điệu hóa được kết xuất và hình ảnh gốc được chụp, và (ii) tổn thất phong cách, định lượng sự khác biệt giữa hình ảnh được kết xuất và hình ảnh phong cách. Tuy nhiên, không giống như những phương pháp này, chúng tôi duy nhất áp dụng tổn thất nội dung và phong cách độc quyền cho các đối tượng liên quan trong hình ảnh, trao cho chúng tôi sự kiểm soát vượt trội hơn so với cảnh 3D được tạo ra. Phương pháp này cho phép cách điệu hóa chính xác và có mục tiêu, trao quyền cho chúng tôi đạt được kết quả phù hợp và tinh tế hơn.

Chúng tôi trình bày kết quả của mình trên một phạm vi đa dạng các tình huống 3D, thể hiện cách phương pháp của chúng tôi đóng vai trò như một bước đệm hướng tới việc đạt được tạo sinh cảnh 3D có thể kiểm soát được hơn.

2. Công trình liên quan
Có rất nhiều công trình trong chuyển giao phong cách cảnh cho NeRF [25, 12, 10, 19, 23, 15, 8]. Trong phần lớn các pipeline chuyển giao phong cách, một khung đào tạo hai giai đoạn được sử dụng. Giai đoạn đầu tiên bao gồm việc đào tạo một cảnh 3D ảnh thực tế, trong khi giai đoạn thứ hai liên quan đến việc tinh chỉnh hoặc sửa đổi biểu diễn cảnh 3D bằng cách sử dụng hình ảnh phong cách. Một số phương pháp biểu diễn cảnh 3D dưới dạng mesh [8, 15, 23], một số dưới dạng point cloud [10, 19] trong khi những phương pháp khác sử dụng trường bức xạ ẩn [25, 12, 18].

Những phương pháp này cũng khác nhau trong cách chúng tinh chỉnh hoặc sửa đổi biểu diễn cảnh 3D. Một số công trình sử dụng một mạng hyper riêng biệt [6, 12] trong khi những phương pháp khác thay đổi chính các biểu diễn ẩn [25]. Trong lĩnh vực cách điệu hóa cảnh 3D, việc giải quyết tính nhất quán không gian nổi lên như một trong những thách thức chính cần được giải quyết. Ví dụ, StyleMesh [8] áp dụng phương pháp cách điệu hóa kết hợp, sử dụng tất cả hình ảnh đầu vào để cách điệu hóa cảnh 3D và tối ưu hóa một texture tường minh để tái tạo cảnh chính xác. Trong khi Trường Bức Xạ Nghệ Thuật [25] sử dụng chuyển giao màu sắc riêng biệt để đảm bảo tính nhất quán góc nhìn.

Việc kiểm soát chuyển giao phong cách và hạn chế nó đối với các đối tượng do người dùng chỉ định là một nhiệm vụ thách thức và một lĩnh vực nghiên cứu tích cực. Các phương pháp tiên tiến hiện tại thực hiện chuyển giao phong cách dựa trên instance nhưng chất lượng của việc kết xuất không hấp dẫn về mặt thị giác và chứa các artifacts. Một công trình thú vị Sine [2], yêu cầu một hình ảnh từ một cảnh được chỉnh sửa bởi người dùng và có thể tạo ra một góc nhìn 3D của cảnh với các đối tượng được chỉnh sửa. Trong trường hợp này, các prior hình học cũng được sử dụng để ràng buộc và duy trì các thành phần hình học của các đối tượng trong cảnh. Mặc dù linh hoạt, phương pháp này yêu cầu người dùng chỉnh sửa một hình ảnh và các chỉnh sửa cũng bị ràng buộc về mặt ngữ nghĩa, không giống như phương pháp của chúng tôi nơi bất kỳ phong cách nào cũng có thể gắn bó trung thành với một đối tượng lựa chọn.

3. Phương pháp
Tổng quan về phương pháp của chúng tôi được thể hiện trong Hình 2. Cho một tập hợp các hình ảnh được hiệu chỉnh của một cảnh và một tập hợp các đối tượng do người dùng định nghĩa trong cảnh, chúng tôi nhằm tạo ra một hình ảnh thực tế và nhất quán về mặt hình học từ bất kỳ góc nhìn tùy ý nào trong cảnh trong đó chỉ các đối tượng do người dùng định nghĩa được cách điệu hóa theo hình ảnh phong cách tham chiếu. Khung của chúng tôi bao gồm ba giai đoạn: tạo trường bức xạ, phát hiện đối tượng, và cách điệu hóa trường bức xạ.

3.1. Tạo trường bức xạ
Phương pháp của chúng tôi sử dụng trường bức xạ (RF) để biểu diễn cảnh trong 3D. Cho một tập hợp các hình ảnh được hiệu chỉnh của một cảnh, trường bức xạ này được tối ưu hóa bằng cách sử dụng tổn thất kết xuất trên các tia đào tạo. Phương pháp này không phụ thuộc vào cách trường bức xạ được biểu diễn nhưng để hiệu quả, chúng tôi sử dụng lưới voxel thưa thớt của Plenoxel [24] (V) để biểu diễn cảnh 3D. Mỗi voxel được chiếm giữ lưu trữ một độ mờ vô hướng σ và một vector các hệ số hài cầu cho mỗi kênh màu. Trường bức xạ được định nghĩa bằng cách sử dụng nội suy tam tuyến tính trên lưới voxel thưa thớt.

L(x, w) = ϕ(x,V) (1)

Trong đó x là điểm được truy vấn trong không gian 3D, w là vector hướng đơn vị được truy vấn, V là lưới voxel và hàm ϕ là nội suy tam tuyến tính.

Nó sử dụng mô hình kết xuất thể tích khả vi được sử dụng trong NeRF [17]. Màu sắc của tia được xác định bằng cách tích hợp tất cả các điểm dọc theo tia.

Ĉ(r) = ∑[i=1 to N] Ti(1−e^(−σiδi))ci (2)

Ti = e^(−∑[j=1 to i−1] σjδj) (3)

Trong đó Ti biểu diễn lượng ánh sáng truyền dọc theo tia r, δi là độ mờ của mẫu i, ci là màu sắc của mẫu i.

Độ mờ và hệ số hài cầu của lưới voxel được tối ưu hóa bằng cách sử dụng lỗi bình phương trung bình (Lmse) trên các pixel được kết xuất cùng với các bộ điều chỉnh biến thiên tổng (Ltv) [22], phân phối beta (Lβ) và prior thưa thớt (Ls)[13]. Hàm tổn thất tổng thể (Lrf) cho tối ưu hóa trường bức xạ như sau:

Lrf = Lmse + λtvLtv + λβLβ + λSLS (4)

Lmse = 1/|R| ∑[r∈R] ||C(r)−Ĉ(r)||²₂ (5)

Ltv = 1/|V| ∑[v∈V] ∑[d∈D] ||∆(v, d)||² (6)

LS = ∑i ∑k log(1 + 2σ(ri(tk))²) (7)

Lβ = ∑r (log(TFG(r)) + log(1−TFG(r))) (8)

--- TRANG 3 ---
Hình 2. Tổng quan về phương pháp của chúng tôi. Chúng tôi bắt đầu bằng việc tái tạo một trường bức xạ ảnh thực tế và phân đoạn tất cả các đối tượng từ một tập hợp hình ảnh cảnh. Tiếp theo, chúng tôi áp dụng cách điệu hóa cho việc tái tạo này bằng cách sử dụng tổn thất phong cách Đối Sánh Đặc Trưng Láng Giềng Gần Nhất có mặt nạ (mNNFM) với sự trợ giúp của các hình ảnh phong cách mẫu. Khi quá trình cách điệu hóa hoàn tất, chúng tôi có thể tạo ra các kết xuất được cách điệu hóa từ góc nhìn tự do nhất quán. Để hiểu sâu hơn về kết quả của chúng tôi, chúng tôi mời độc giả xem các video bổ sung.

Trong đó C(r) là màu sắc của tia ground truth, Ĉ(r) là màu sắc ước tính của tia, ||∆(v, d)||² là khoảng cách bình phương giữa các giá trị thứ d trong các voxel. σ(ri(tk))²) là độ mờ của mẫu k dọc theo tia i. log(TFG(r)) biểu thị độ trong suốt foreground tích lũy của tia r. λtv, λβ và λS là trọng số của các thành phần tổn thất tương ứng.

3.2. Phát hiện đối tượng
Giai đoạn thứ hai của khung của chúng tôi nhằm cung cấp cho người dùng một lựa chọn các đối tượng trong cảnh mà phong cách có thể được chuyển giao. Cho một tập hợp hình ảnh của một cảnh, đầu ra là một tập hợp tất cả các cặp đối tượng và mặt nạ O={(oi, mi)} (trong đó i∈[0, N] và N là số lượng đối tượng) trong cảnh.

Chúng tôi sử dụng một bộ phát hiện đối tượng dựa trên transformer, DEtection TRansformer (DETR) [3] để phát hiện đối tượng. Mặt nạ phân đoạn được thu được bằng cách sử dụng Segment Anything Model (SAM) [1] cho tất cả hình ảnh trong cảnh. Cho một hình ảnh đầu vào, DETR tạo ra một danh sách toàn diện các hộp đối tượng, mỗi hộp được liên kết với một tag danh mục và tọa độ hộp bao tương ứng. SAM nhận đầu vào là một hình ảnh cùng với các hộp đối tượng và xuất ra các mặt nạ phân đoạn tương ứng với từng truy vấn đối tượng đầu vào. Các mặt nạ phân đoạn được tạo ra bằng cách sử dụng prompt hộp tốt hơn nhiều so với những mặt nạ được tạo ra bằng cách sử dụng các prompt khác [5, 14], do đó chúng tôi sử dụng DETR trước SAM.

Tại thời điểm này, chúng tôi có một danh sách các đối tượng (với mặt nạ phân đoạn) trong cảnh và các hình ảnh phong cách tương ứng cần được chuyển giao. Để đảm bảo độ tin cậy của các đối tượng được phát hiện, chúng tôi chỉ giữ lại những đối tượng xuất hiện trong ít nhất 80% các khung hình trong suốt các hình ảnh cảnh.

3.3. Cách điệu hóa trường bức xạ
Cho một trường bức xạ ảnh thực tế được tái tạo bằng cách sử dụng phương pháp trong mục 3.1 và một tập hợp các đối tượng và mặt nạ được thu được bằng cách sử dụng phương pháp trong mục 3.2, khung của chúng tôi tinh chỉnh trường bức xạ ảnh thực tế, trong đó các đối tượng được cách điệu hóa theo hình ảnh phong cách 2D tương ứng của chúng. Chúng tôi đạt được điều này bằng cách áp dụng tổn thất Đối Sánh Đặc Trưng Láng Giềng Gần Nhất (NNFM) [25] cho từng đối tượng riêng lẻ.

Tổn thất NNFM nhằm mục đích tối thiểu hóa khoảng cách cosine của mỗi đặc trưng trong bản đồ đặc trưng của hình ảnh được kết xuất với đặc trưng láng giềng gần nhất của nó trong bản đồ đặc trưng của hình ảnh phong cách. Hình ảnh được kết xuất từ trường bức xạ được ký hiệu bằng Ir và hình ảnh phong cách được ký hiệu bằng Is. Các bản đồ đặc trưng VGG được trích xuất từ cả hai hình ảnh này là Fr và Fs tương ứng. Tổn thất NNFM được cho bởi:

LNNFM = 1/N ∑[i,j] min[k,l] δ(Fr(i, j), Fs(k, l)) (9)

trong đó F*(i, j) biểu thị vector đặc trưng tại vị trí pixel (i, j) cho bản đồ đặc trưng F* và hàm δ(v1, v2) tính toán khoảng cách cosine giữa các vector v1 và v2.

Chúng tôi độc quyền áp dụng tổn thất NNFM (phương trình 9) cho các pixel tương ứng với từng đối tượng riêng biệt. Việc áp dụng có chọn lọc này được thực hiện bằng cách sử dụng mặt nạ thu được trong mục 3.2. Mặt nạ cho phép giới hạn hiệu quả việc chuyển giao phong cách đến các đối tượng quan tâm cụ thể. Tổn thất NNFM có mặt nạ (mNNFM) như sau:

ρ = ∑[i,j] min[k,l] mo(i, j)D(Fr(i, j), Fo_s(k, l)) (10)

LmNNFM = 1/N ∑[o=1 to N] (ρ) (11)

mo biểu thị mặt nạ cụ thể cho đối tượng o, trong khi Fo_s biểu thị bản đồ đặc trưng được trích xuất từ hình ảnh phong cách dự định để chuyển giao lên đối tượng o. ρ biểu thị tổn thất mNNFM trên một đối tượng duy nhất và tổn thất tổng là trung bình trên tất cả các đối tượng trong cảnh.

--- TRANG 4 ---
Hình 3. Thể hiện các ví dụ về trường bức xạ được cách điệu hóa trong hai kịch bản. 1) Instance đối tượng đơn (ghế) được cách điệu hóa (Hàng trên). 2) Nhiều instance của cùng một đối tượng (hoa) đã được cách điệu hóa (Hàng dưới). Hình ảnh bên trái thể hiện một trong những hình ảnh đầu vào cho cảnh cùng với đối tượng cần được cách điệu hóa và hình ảnh phong cách (trên-trái). Hình ảnh bên phải thể hiện một hình ảnh của hình ảnh được cách điệu hóa.

Kết hợp tổn thất mNNFM có mặt nạ với tổn thất được đề cập trong mục 3.1, tổn thất tổng thể mà chúng tôi tối ưu hóa là:

L = Lrf + LmNNFM (12)

Tổn thất NNFM được sửa đổi đóng vai trò quan trọng trong việc tinh chỉnh quá trình tạo trường bức xạ, đảm bảo rằng phong cách được áp dụng tuân thủ chính xác với các đối tượng được chọn. Phương pháp này nâng cao sức hấp dẫn thị giác tổng thể và tính nhất quán ngữ cảnh của đầu ra cuối cùng, làm cho nó thuyết phục và thực tế hơn.

4. Thí nghiệm
Để đánh giá hiệu quả của phương pháp, chúng tôi tiến hành đánh giá định tính, thể hiện kết quả từ các cảnh thực tế đa dạng nơi các đối tượng bị ảnh hưởng bởi các hình ảnh phong cách khác nhau. Chúng tôi chứng minh cách phương pháp của chúng tôi thành công trong việc áp dụng các phong cách khác nhau cho các đối tượng trong bối cảnh thế giới thực, cung cấp bằng chứng trực quan về tính đa dạng và hiệu suất của nó.

Tập dữ liệu. Chúng tôi tiến hành các thí nghiệm trên nhiều cảnh thế giới thực bao gồm: Flower, Xmaschair, Room từ [16]. Tất cả các cảnh này đều được chụp từ phía trước. Các hình ảnh phong cách bao gồm một tập hợp đa dạng các hình ảnh lấy từ [25].

Kết quả định tính của chúng tôi được trình bày trong Hình 3,4. Chúng tôi khám phá bốn kịch bản khác nhau cho việc chuyển giao phong cách:

Chuyển giao phong cách trên một instance duy nhất của một đối tượng: Trong kịch bản này, chúng tôi áp dụng một hình ảnh phong cách cho một đối tượng duy nhất trong cảnh 3D. Hình 3 (Hàng trên) thể hiện kết quả của việc áp dụng hình ảnh phong cách lên chiếc ghế.

Chuyển giao phong cách trên tất cả các instance của một đối tượng duy nhất: Trong trường hợp này, chúng tôi chuyển giao một phong cách duy nhất cho tất cả các instance của một đối tượng duy nhất. Hình 3 (Hàng dưới) thể hiện việc áp dụng phong cách đã cho lên tất cả các instance hoa trong cảnh.

Chuyển giao phong cách trên tất cả các instance của nhiều đối tượng: Trong ví dụ này, chúng tôi chuyển giao các phong cách khác nhau cho các đối tượng riêng biệt trong cảnh. Hình 4 (Hàng trên) thể hiện việc áp dụng các phong cách khác nhau lên bàn và ghế trong cảnh phòng 3D.

Chuyển giao phong cách trên nhiều instance của một đối tượng duy nhất: Trong trường hợp này, chúng tôi chuyển giao các phong cách khác nhau cho các instance riêng biệt của cùng một đối tượng. Chúng tôi áp dụng hai phong cách khác nhau lên hai bông hoa riêng biệt trong cùng một cảnh như thể hiện trong Hình 4 (Hàng dưới).

Chúng tôi khuyến khích độc giả xem các video bổ sung và phụ lục A để xem kết quả tốt hơn.

5. Thảo luận
Chúng tôi đề xuất một phương pháp mới để tái tạo trường bức xạ được cách điệu hóa từ trường bức xạ ảnh thực tế. Nền tảng của phương pháp chúng tôi nằm ở việc áp dụng tổn thất mNNFM có mặt nạ, cho phép chuyển giao phong cách có thể kiểm soát được hơn. Phương pháp của chúng tôi hiệu quả đạt được chuyển giao phong cách ở cả cấp độ ngữ nghĩa và instance, thành công trong việc áp dụng các phong cách riêng biệt cho nhiều đối tượng trong một cảnh duy nhất. Mặc dù điều này đóng vai trò như một chứng minh khái niệm thuyết phục, một đánh giá toàn diện hơn được yêu cầu để xác thực đầy đủ phương pháp của chúng tôi. Các đánh giá tương lai nên bao gồm một phạm vi rộng hơn các cảnh, bao gồm các môi trường 360 độ và các cảnh với số lượng đối tượng tăng lên. Ngoài ra, việc tiến hành đánh giá định lượng để đánh giá kỹ lưỡng hiệu quả của phương pháp chúng tôi là rất quan trọng.

--- TRANG 5 ---
Tài liệu tham khảo
[1] Segment anything. 2023.
[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20919–20929, 2023.
[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers, 2020.
[4] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16569–16578, 2023.
[5] Dongjie Cheng, Ziyuan Qin, Zekun Jiang, Shaoting Zhang, Qicheng Lao, and Kang Li. Sam on medical images: A comprehensive study on three prompt modes. arXiv preprint arXiv:2305.00035, 2023.
[6] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-Sheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via implicit representation and hypernetwork. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1475–1484, 2022.
[7] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. arXiv preprint arXiv:2103.10380, 2021.
[8] Lukas Höllein, Justin Johnson, and Matthias Nießner. Stylemesh: Style transfer for indoor 3d scene reconstructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6198–6208, 2022.
[9] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia. Efficientnerf efficient neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12902–12911, 2022.
[10] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, and Ming-Hsuan Yang. Learning to stylize novel views. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13869–13878, 2021.
[11] Xudong Huang, Wei Li, Jie Hu, Hanting Chen, and Yunhe Wang. Refsr-nerf: Towards high fidelity and super resolution view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8244–8253, 2023.
[12] Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, and Lin Gao. Stylizednerf: consistent 3d scene stylization as stylized nerf via 2d-3d mutual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18342–18352, 2022.
[13] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. arXiv preprint arXiv:1906.07751, 2019.
[14] Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, and Yixin Zhang. Segment anything model for medical image analysis: an experimental study. Medical Image Analysis, page 102918, 2023.
[15] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13492–13502, 2022.
[16] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines, 2019.
[17] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.
[18] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstantinos G Derpanis, Jonathan Kelly, Marcus A Brubaker, Igor Gilitschenski, and Alex Levinshtein. Spin-nerf: Multiview segmentation and perceptual inpainting with neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20669–20679, 2023.
[19] Fangzhou Mu, Jian Wang, Yicheng Wu, and Yin Li. 3d photo stylization: Learning to generate stylized novel views from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16273–16282, 2022.
[20] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: stylized neural implicit representations for 3d scenes. arXiv preprint arXiv:2207.02363, 2022.
[21] Zhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, and Liefeng Bo. 4k-nerf: High fidelity neural radiance fields at ultra high resolutions, 2023.
[22] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7467–7477, 2020.
[23] Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and Sanja Fidler. 3dstylenet: Creating 3d shapes with geometric and texture style variations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12456–12465, 2021.
[24] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. arXiv preprint arXiv:2112.05131, 2021.
[25] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic radiance fields. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXI, pages 717–733. Springer, 2022.
[26] Yuechen Zhang, Zexin He, Jinbo Xing, Xufeng Yao, and Jiaya Jia. Ref-npr: Reference-based non-photorealistic radiance fields for controllable scene stylization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4242–4251, 2023.

A. Kết quả định tính
Hình 5. Thể hiện kết quả định tính với các ví dụ về chuyển giao phong cách với nhiều đối tượng và phong cách. Hình ảnh trong cột đầu tiên thể hiện một trong những hình ảnh đầu vào của cảnh cùng với đối tượng cần được cách điệu hóa và hình ảnh phong cách (trên-trái). Hình ảnh bên phải thể hiện ba hình ảnh từ cảnh được cách điệu hóa.
