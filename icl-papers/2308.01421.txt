# 2308.01421.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl-papers/2308.01421.pdf
# File size: 2904427 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PREPRINT
Regularization, early-stopping and dreaming: a
Hopfield-like setup to address generalization and
overfitting
E. Agliari,a,1F. Alemanno,b,1M. Aquaro,a,1A. Fachechia,1
aDipartimento di Matematica “Guido Castelnuovo”, Sapienza Universit` a di Roma, Italy
bDipartimento di Matematica, Universit` a di Bologna, Italy
1GNFM-INdAM, Gruppo Nazionale di Fisica Matematica (Istituto Nazionale di Alta Matematica), Italy
E-mail: agliari@mat.uniroma1.it
Abstract: In this work we approach attractor neural networks from a machine learning perspective:
we look for optimal network parameters by applying a gradient descent over a regularized loss function.
Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which
correspond to Hebbian kernels revised by a reiterated unlearning protocol. Remarkably, the extent
of such unlearning is proved to be related to the regularization hyperparameter of the loss function
and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in
terms of regularization and early-stopping tuning. The generalization capabilities of these attractor
networks are also investigated: analytical results are obtained for random synthetic datasets, next,
the emerging picture is corroborated by numerical experiments that highlight the existence of several
regimes (i.e., overfitting, failure and success) as the dataset parameters are varied.
Keywords: Attractor neural networks, overfitting, spin-glassesarXiv:2308.01421v2  [cs.LG]  20 Feb 2024

--- PAGE 2 ---
Contents
1 Introduction 1
2 From the stability condition to the minimization problem 3
3 Dreaming as regularization, regularization as early-stopping 5
4 Emergence of generalization and overfitting in Hopfield-like models 9
4.1 A synthetic dataset 9
4.2 Spurious states of training data enable generalization 10
5 Numerical experiments 12
5.1 Generalization diagrams 12
5.2 An analogy with a clustering algorithm 16
6 Conclusions 17
7 Acknowledgments 19
A Convergence of the gradient descent procedure 19
B Methods 20
B.1 Training design and parameters assignment 20
B.2 Formulation of the performance metrics 21
B.3 Numerical signatures of overfitting 22
C A faster way to compute the early-stopping time 23
D Details on spurious states 24
1 Introduction
The Hopfield model is probably the best-known example of attractor neural network [1–3]: this is
constituted by a set of Nbinary units, referred to as neurons, that interact pairwise and whose state
is iteratively updated by a non-linear activation function, in such a way that the new state of neuron
idepends on the signal acting on iand stemming from the neighbouring neurons. A suitable choice
of the neuron interaction matrix, denoted as J∈RN×N, can ensure the attractivity of a number of
patterns, that we want to store and retrieve. More precisely, one initializes the neural configuration
by setting the values of the units close to a pattern ξ1∈ {− 1,+1}N, this configuration represents the
input supplied to the machine and may correspond to a corrupted version of ξ1; repeated updates of
neurons are then performed until convergence to a fixed point and, if this matches ξ1, this state is
interpreted as the retrieval of the information codified by ξ1. The same is expected to occur for any
target pattern, say ξµ, with µ= 1, ..., P .
– 1 –

--- PAGE 3 ---
The popularity of the Hopfield model is also due to the fact that it is feasible of an analytical
treatment and, in particular, it can be recognized as a spin-glass, in such a way that it can benefit
from a broad collection of techniques developed to address disordered systems. Indeed, in the last
decades, the model has been intensively investigated, and countless variations on theme have also
been accounted for1. Remarkably, a significant fraction of these works spotlighted the structure of the
neural interaction matrix: in the standard Hopfield model this is based on the so-called Hebb’s rule
[19]J=ξ·ξT
Nand suitable revisions of this rule can give rise to better performances of the model in
terms of number of storable and retrievable patterns [20]. A successful class of these revisions apply
unlearning protocols (see e.g., [21–30]), whose aim is to impair the attractiveness of configurations that
do not correspond to any of the stored patterns (the convergence to those configurations, sometimes
referred to as spurious states, is interpreted as a mistake of the machine). More recently, Hebb’s rule
has also been revised to make it closer to a learning algorithm: the “reality” that we want to retrieve
is now not accessible, instead, a corrupted sample, say {ξµ,1,ξµ,2, ...}is available and used to build J
(see e.g., [31–34]). This way, the available sample of data can be interpreted as a training set and the
Hopfiled model can be employed for generalization tasks. The bridge between a retrieval scenario and
a machine learning setting has also been strengthened by leveraging the equivalence between Hopfield
model and Boltzmann machines (see e.g., [35–40]). However, a full mapping allowing for the role
of regularization parameters, the emergence of overfitting or underfitting phenomena is still under
construction (see e.g., [41–46]).
In this paper, we try to contributed in filling this gap, focusing on an unsupervised reconstruction
problem: the dataset is made of a set of items belonging to different classes (the label being veiled) and
we introduce a loss function for the neuron interaction matrix. The solution of our problem corresponds
to a Hebbian kernel subjected to a certain amount tdof unlearning iterations and we prove that td
is related to the regularization hyperparameter, which, in turn, can be related to the training time
in the un-regularized version of the problem. This framework allows us to inspect the emergence of
overfitting phenomena and therefore to conceive recipes for an optimal training time. Specifically, the
system stores each pattern as a minimum of the Lyapunov function associated to the neural dynamics
(this can be interpreted as a cost function or as an energy function); minima corresponding to the same
class form a cluster, and – when the number of examples per class is large enough – these minima
do coalesce into a single minimum. In this scenario, there emerge both intra-class and inter-class
correlations and we find that the role of td(or, equivalently, of the regularization or of the training
time) is to disentangle such correlations starting from the lowest ones: as tdis increased, the minima
corresponding to different classes are shifted, their overlap is reduced and the system gets able to
generalize from examples; by further increasing td, minima corresponding to patterns belonging to the
same classes get shifted too and the system starts to overfit.
In what follows, we detail these results by first introducing the loss function associated with our
problem and showing that the neural interaction matrix that minimizes the loss function corresponds
to the revised Hebbian kernel studied in [27, 47] (Sec. 2). Subsequently, we find a relation between
regularization, training time and unlearning time, and we present numerical experiments on structure-
less and structured datasets (Sec. 3). These results constitute the premises for a thorough discussion
on the emergence of overfitting phenomena (Sec. 4) and a corroboration by numerics (Sec. 5). Lastly,
1These include, for instance, the presence of biases or vacancies in pattern entries (see e.g., [4–6]), generalizations of
the Hebbian kernel to include temporal correlations among patterns (see e.g., [7–9]) or synaptic noise (see e.g., [10–12]),
non-trivial underlying architectures (see e.g., [13–15]), or high-order interactions (see e.g., [16–18]). Although these
aspects are not directly related to the current work, it is worth stressing the long-standing, broad interest attracted by
the model, further, the current approach can be extended to include such features.
– 2 –

--- PAGE 4 ---
we conclude our paper by offering a concise outlook and final remarks (Sec. 6). Technical details are
collected in the Appendices.
2 From the stability condition to the minimization problem
The basic principle underlying an attractor neural network is that each pattern making up the set
ξ={ξµ}P
µ=1and encoding relevant information is associated to an attracting fixed point for the
network dynamics. We assume that patterns are N-dimensional binary vectors and that the network
unitsσ= (σ1, . . . , σ N)∈ {− 1,+1}Ninteract pairwise as specified by the symmetric2coupling matrix
J, then, we set up the network evolution as
σ(t+ 1) = sign( φ(σ(t)), (2.1)
where φi(σ(t))≡P
j̸=iJijσj(t) represents the signal reaching unit iat time tand the sign function
acts component-wise. This dynamics is applied sequentially and exhibits the following Lyapunov
function (see e.g., [48])
E(σ) =−1
2X
i,jσiJijσj. (2.2)
In this work, we will retain a deterministic3evolution and, for the moment, we take Jas quenched
and we exclude the presence of biases. In this framework, the stability condition for a retrieval
configuration, e.g., σ(t) =ξµwithout loss of generality, reads as
ξµ
iφi(ξµ)≥0,∀i= 1, . . . , N, (2.3)
in such a way that, if at time tthe system is prepared or occurs to be precisely in that state, it
will be there trapped for all t′≥t. Of course, the fulfilment of (2.3) implies that Jmust be a
suitable functional of ξ. More generally, one is interested in assessing the convergence to a retrieval
configuration even when the input quality is relatively low, namely, even when the initial configuration
is relatively far (in Hamming sense) from the target pattern. In this case, one asks for a stronger
condition, that is
ξµ
iφi(ξµ)≥κ >0,∀i= 1, . . . , N, (2.4)
which means that, although σ(t) displays some discrepancies with respect to ξµ, the dynamics (2.1) is
still ensured to converge to ξµ. In this inequality (corresponding to the basic requirement in Gardner’s
theory [49–51]), κcontrols the width of the attraction basins, that is, if σbelongs to a Hamming ball
B(ξµ, R(κ)) centered in ξµwith radius R(κ), the network response will be f(σ) =ξµ, with fbeing
the transfer function f(σ) = lim n→∞Tn(σ) and T(σ) = sgn( φi(σ)) the 1-step dynamics. Increasing
κ, the stability criterion will be satisfied in a ball with larger and larger radius R(κ) surrounding
the patterns, but this goes at cost of a smaller amount of storable information vectors, resulting in a
2The symmetry constraint on the coupling matrix is traditionally adopted so that, in the statistical mechanical ap-
proach, detailed balance principle holds and this directly implies the stochastic relaxation to an equilibrium distribution
in Boltzmann-Gibbs form, see e.g., [48].
3Stochastic realizations of the dynamics (2.1) work in similar ways, apart from the fact that the system is not fixed
in the precise configuration given by the µ-th pattern, but it is free to explore (at some level given by the degree of
stochasticity) the associated attraction basin. This is strictly true if two conditions hold: i.the initial configuration
is close (in the Hamming sense) to the target pattern and ii.in the thermodynamic limit, where ergodicity breaking
occurs (in the finite-size case, the transition between different attractors is exponentially small in the system size but
non-vanishing).
– 3 –

--- PAGE 5 ---
lower critical storage capacity. In particular, for symmetric networks ( J=JT) the largest number
of patterns that can be retrieved is N[49]. In order to satisfy the inequality constraint (2.4), we can
impose a (stronger) equality condition requiring that, given γ≥κ,
ξµ
iX
j̸=iJijξµ
j=γ,∀i= 1, . . . , N, (2.5)
with γbeing the same for all the patterns. The latter point could appear a rather strong assumption,
but – at least in the random theory, where patterns are all equivalent, as their entries are i.i.d. –
it is reasonable. The requirement (2.5) has important technical consequences. First, if the patterns
are Boolean, it can be rewritten in a more transparent form asP
j̸=iJijξµ
j=γξµ
i, which is nothing
but the Personnaz et al’s stability criterion [22, 51]. Further, we can remove the constraint on the
absence of self-interactions and allow for j=iin the last sum4, thus recasting the previous expression
as an eigenvalue problem as J·ξµ=γξµ; this means that the coupling matrix is designed so that the
patterns are eigenvectors with degenerate eigenvalue γ.
In the general case, we can add an external field by replacing the local internal field in (2.4) with
the total field, i.e., φi(σ)→P
jJijσj+hi. The previous arguments still hold and, for µ= 1, . . . , P ,
our problem takes the form(
J·ξµ+h=γξµ
J=JT. (2.6)
Therefore, in this context, training a network implies finding an arrangement for Jandh, such that
(2.6) hold and this can be recast into the minimization of a Mean-Squared Error (MSE) of the form
(J·ξµ+h−γξµ)2, in such a way that we can set up the minimization procedure for a loss function
reading as5
Lξ(J,h) =1
2PX
i,µX
jJijξµ
j+hi−γξµ
i2
+1
2PX
j,µX
iJijξµ
i+hj−γξµ
j2
+ϵJX
i,jJ2
ij+ϵhX
ih2
i,
(2.7)
with ϵJ, ϵh∈[0,+∞]. The second term in the r.h.s. is obtained starting from the first one and
reverting the roles of iandj; these two contributions account for the stability criterion and the
symmetry constraint6in a non-rigid way. The successive two contributions are L2-regularization
terms for, respectively, Jandh, that protect the norms of these parameters from divergence during
training as they are confined by the quadratic potentials.
The explicit form of the solution of the constrained system (2.6) can be achieved via gradient
descent method ˙J=−∇JLξand˙h=−∇hLξ, which yields
−˙J=JΩ+ΩJ+h¯ξT+¯ξhT−2γΩ+ 2ϵJJ, (2.8)
−1
2˙h=J+JT
2¯ξ+ (1 + ϵh)h−γ¯ξ, (2.9)
where
Ωij≡1
PX
µξµ
iξµ
j,¯ξi≡1
PX
µξµ
i,
4This has quantitative effects on the estimate of the critical storage capacity, that can be neglected here as the focus
is rather on the generalization capabilities in an unsupervised scenario, see e.g. [22].
5The case without self-interactions can be recovered by adding a contributionP
iθiJiiin the loss functions, where
θiare Lagrange multipliers ensuring that the diagonal entries are set to zero.
6We recall that the symmetry constraint for the coupling matrix Jis introduced for consistency with the statistical
mechanics picture and to ensure that the sequential dynamics (2.1) exhibits fixed points.
– 4 –

--- PAGE 6 ---
are, respectively, the Hebbian kernel7and the mean value of the i-th entry over patterns. Notice
that the matrices JandJTsatisfy the same differential equation (2.8) since Ωis symmetric, thus,
if we consider initial conditions such that J(0) = J(0)T, the symmetry is preserved for any t >0 by
uniqueness arguments. Hence, we can safely replace Jto its symmetric part in (2.9). The convergence
condition of the discrete form of these dynamical equations is discussed in App. A.
Before concluding this Section, it is worth highlighting that, as standard, the neural relaxation
(2.1) and the training dynamics (2.8)-(2.9) operate on different time scales, the former being much
faster. Biologically, this is motivated by the fact that synaptic plasticity is much slower that neural
activation and, in artificial neural networks, by the fact that the machine is first trained and later
used for task accomplishment. For consistency with (2.1) one should therefore write ˙J=dJ
dtτ−1
Jand
˙h=dh
dtτ−1
h, with τJ,h≫1. However, under this adiabatic hypothesis, we can consider the network
parameters J,has planted during the neural dynamics and separate the two dynamical problems in
such a way that, when focusing on the synaptic evolution, τJ,hcan be set as unitary without ambiguity.
3 Dreaming as regularization, regularization as early-stopping
The global minimum for Lξ(J,h) in (2.7) can be obtained by requiring the stability conditions ˙J=0
and˙h=0in Eqs. (2.8)-(2.9) that give
JΩ+ΩJ+h¯ξT+¯ξhT−2γΩ+ 2ϵJJ= 0, (3.1)
J¯ξ+ (1 + ϵh)h−γ¯ξ= 0, (3.2)
whose solution reads as
h=1
1 +ϵh(γ1−J)¯ξ, (3.3)
J=1
Pˆξγ
C+ϵJ1ˆξT, (3.4)
where
ˆξµ
i≡ξµ
i−
1−rϵh
1 +ϵh
¯ξi, C µν≡1
PX
iˆξµ
iˆξν
i.
By inspecting Eq. (3.3) one can see that the external field stems from the presence of biases in
the input data, i.e. ¯ξi̸= 0, in such a way that re-centering the patterns by ξµ
i→ξµ
i−¯ξµ
iresults in
h=0. Thus, as long as data are pre-processed in this way, external fields are not needed. Further, by
looking at Eq. (3.4), one can see that our solution recovers the interaction matrix of the “Dreaming
Hopfield model” (DHM) [27, 39, 47]
J(D)≡1
Pξtd
I+CtdξT, (3.5)
upon setting γ= 1 and identifying tdas the inverse of the hyperparameter ϵJ8, that is, td=ϵ−1
J. More
precisely, in the DHM, the kernel reads as ˜J(D)≡1
Nξtd+1
I+CtdξTand it was obtained from the standard
7With respect to the standard notation here the prefactor 1 /Nis replaced with 1 /P.
8The minimization of the regularized-MSE as defined in (2.7) is a special setting of the usual setup of ridge regression
theory [52, 53], with the target response of the network being the multiplication of the input vectors ξµby the constant γ.
Neglecting the bias vector, the ridge estimator minimizing the loss-function does coincide with the coupling matrix J(D),
where tdplays the role of Tichonov regularization parameter [54]. Ridge regression, together with their generalization
to non-linear regression problems with kernel techniques [55–57], is a central topic in statistical learning theory, focusing
in particular on the role of the corresponding hyper parameter (see e.g. [58–60]) as well as the implicit regularization
phenomena emerging in high-dimensional statistics [61, 62].
– 5 –

--- PAGE 7 ---
Hebbian kernel by iteratively applying an unlearning protocol based on an interplay of remotion
and consolidation mechanisms inspired by those occurring during sleep in mammals’ brain [21, 63];
because of this analogy the time td, which measures the number of unlearning iterations, is referred to
as “dreaming time”. The critical storage of the DHM has been shown to increase monotonically with
td, reaching, in the td→ ∞ limit, the theoretical upper bound known for symmetric networks and
corresponding to a number of retrievable patterns equal to the number of neurons, i.e., P=N. Also,
the DHM has been proved to outperform the standard Hopfield model as for generalization abilities
[64]. The difference between J(D)and˜J(D)just lays in a pre-fractor P/N and in a shift td→td+ 1
which, as long as they are finite and non-vanishing, yield only a quantitative correction.9
Moreover, for td→ ∞ (or, equivalently, for ϵJ→0), we recover Kohonen’s projector matrix [65]
J(P)≡1
PξC−1ξT.
Let us now move forward and notice that the solution J(D)obtained by a fully-trained ( t→ ∞ )
L2-regularized ( ϵJ̸= 0) process can be related to the solution of an unregularized ( ϵJ= 0) process
which is run up to a finite time t∗; as we will see, this relation allows us to map the dreaming time
tdinto the training time and therefore interpret the dreaming mechanism as a training. In order to
establish this relation, we resume the dynamical problem with recentered patterns, in such a way that
the inferred field is vanishing10and Eq. (2.8) simply reads as
−˙J=J(Ω+ϵJ1) + (Ω+ϵJ1)J−2γΩ. (3.6)
This can be recast in the basis of the eigenvectors of Ω(denoting with a, b= 1, . . . , N the corresponding
indices) as
−˙Jab= (λa+λb)Jab+ 2ϵJJab−2γλaδab, (3.7)
where Jabis the element of Jin the current basis and σ(Ω) ={λa}N
a=1is the Ωspectrum. By solving
Eq. (3.7) we find that the non-diagonal terms asymptotically go to zero as Jab∼exp[−t(λa+λb+2ϵJ)]
for any initial condition, in such a way that, at the equilibrium point, the coupling matrix is diagonal.
Here, we choose to prepare the system in a configuration where no information is stored, i.e., a tabula
rasa setting J(t= 0) = 0, in this way the off-diagonal entries remain stuck at zero at any time
t. Remarkably, the diagonal structure of J(t), when expressed in the basis of the eigenvectors of Ω,
implies that the two matrices share the same eigenvectors. As for the entries on the principal diagonal,
9In particular, the two models hold statistical mechanical equivalence as the partition function of the present model
Zβ(J(D)) :=P
{σ}exp
−βP
i,jJ(D)
ijσiσj
can be turned into that of the original DHM by rescaling β→P
N1+td
tdβ,
where βtunes the degree of stochasticity in the system, that is, it tunes the broadness of the distribution of the neural
configurations or, in a physical jargon, it plays as the inverse temperature.
10Equivalently, we can choose to work without rescaling the patterns, thus also including the external fields. In
this case, we can simplify the analysis of the dynamical problem by requiring that τJ˙J=−∇JLandτh˙h=−∇hL,
and consider the case τh≪τJ. Under this assumption, the variation of the external fields is much faster than the
typical evolution of the coupling matrix, so – when dealing with the temporal behavior of the latter – the fields do relax
instantaneously towards their fixed point at fixed J(t):
h∞[J(t)] =1
1 +ϵh(γ1−J(t))¯ξ.
As a consequence, the synaptic dynamics is described by the equation
−˙J=J(ˆΩ+ϵJ1) + ( ˆΩ+ϵJ1)J−2γˆΩ,
with ˆΩij= Ωij−Mij=P−1P
µˆξµ
iˆξµ
jandMij= (1 + ϵh)−1¯ξi¯ξj. When dealing with structured patterns, we will
preserve the inferred field in order not to alter the graphical appearance of the data; this is of course completely irrelevant
when dealing with a zero-mean data set.
– 6 –

--- PAGE 8 ---
given the above-mentioned initial condition, the solution of the associated differential equation is
Jaa(t) =γλa
ϵJ+λan
1−exp
−2t(λa+ϵJ)o
. (3.8)
As expected, in the limit of large training-time, we recover J(D), that is
Jaa(t) =
t→∞γλa
λa+ϵJ=J(D)
aa, (3.9)
while, expanding at small t, we get
Jaa(t) =γλa
ϵJ+λan
1−exp
−2t(λa+ϵJ)o
≈
t≪12tγλa (3.10)
which corresponds to J≈2tγΩ: this means that, despite the blank initial condition, at the very start
of the training, the kernel Jis close to an Hebbian structure.
On the other hand, by setting ϵJ= 0 in Eq. (3.8), we find that the diagonal terms evolve as
Jaa(t) =γ[1−exp(−2λat)]. (3.11)
Now, we compare the two explicit forms of the coupling matrix, i.e. the regularized one at t→ ∞
(3.9) and the t-dependent one with ϵJ= 0 (3.11), and search for the characteristic time t∗at which
the latter is as close as possible to the former. To do this, let us consider the quantity
δ(λ, t, ϵ J) =γ2hλ
λ+ϵJ−1 + exp( −2tλ)i2
,
measuring the squared difference between the components in the two realizations at fixed eigenvalue
λ. Then, we take the average over the Ωspectrum, i.e.
¯δ(t, ϵJ) =Z
dλ δ(λ, t, ϵ J)ρE(λ),
where ρE(λ) =1
NP
λa∈σ(Ω)δ(λ−λa) is the empirical spectral distribution of Ω. Notice that ¯δ
is nothing but the squared Frobenius distance between the Dreaming kernel and the unregularized
coupling matrix. This quantity is minimized for the following
t∗(ϵJ) = argmin
t¯δ(t, ϵJ). (3.12)
This relation provides an expression for the time t∗at which the unregularized gradient descent over
Lξ(J,h) should be interrupted if we want a coupling matrix close to J(D)corresponding to the
fully-relaxed, regularized gradient-descent. The equivalence between the two scenarios is validated for
synthetic, MNIST [66] and Fashion-MNIST [67] datasets as reported in Fig. 1.
The functional relation between td=ϵ−1
Jandt∗highlighted in Eq. (3.12) is depicted in Fig. 2. The
logarithmic behavior is justified analytically in App. C, where, by expanding (3.12) around the mean
eigenvalue, we obtain a first-order approximation of the early-stopping time t∗(ϵJ) which depends on
tdand on the trace of Ω.
Another way to see the equivalence between regularization and early-stop is the following. Starting
from Eq. (3.8), we notice that ϵJprovides a natural scale as
J(D)
aa≈

γ
1−ϵJ
λa
ifλa≫ϵJ
γλa
ϵJifλa≪ϵJ.
– 7 –

--- PAGE 9 ---
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.00.10.20.30.4¯d(ξ,σ(∞))RANDOM
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.000.050.100.150.20MNIST
P/N = 0.2
P/N = 0.3
P/N = 0.5
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.000.050.100.150.20FASHION-MNIST
Dreaming
GD-Early stoppingDreaming
GD-Early stoppingFigure 1 :Retrieval performance of dreaming kernel versus early-stopping. The three panels
show a comparison between the fully-trained solution (3.4) with ϵJ̸= 0 and the solution of the early-
stopped training procedure with ϵJ= 0; for the latter the final training time is chosen according
to (3.12). In the leftmost panel, the dataset ξis made of PRademacher vectors that naturally
display zero mean, while in the central and in the rightmost panels the dataset ξis made of Pitems
randomly drawn from, respectively, the MNIST and the Fashion-MNIST datasets, and these vectors
were pre-processed by Otsu method [68] to make them binary. The items in these datasets were used
to build up the interaction matrices J(D)andJ(t∗). For the random dataset N= 200 and γ= 1,
whereas for the MNIST and Fashion-MNIST N= 784 and γ= 1, also, different values of the ratio
P/N are considered as reported in the common legend. The performance of the system is measured
in terms of the normalized Hamming distance d(ξµ,σ(∞)) between the target pattern ξµand the
final configuration σ(∞), obtained by initializing the system in a corrupted version of ξµ(obtained
by flipping the pattern entries with probability q= 0.1) and iterating (2.1) up to convergence. By
averaging over all the Ppatterns we obtain ¯d(ξ,σ(∞)) =1
PP
µd(ξµ,σ(∞)), which is plotted versus
the dreaming time. We refer to App. B for further details on numerics.
Therefore, in the regularization approach, the parameter ϵJprevents the saturation of all the diagonal
entries of J(D)to the value γ(corresponding to Kohonen’s projector J(P)[22, 51, 65, 69]); in fact, as
long as ϵJ>0 (or tdfinite), only the entries corresponding to the top eigenvalues of Ωget close to this
limiting value, while the others remain close to the initial condition (i.e., Jaa= 0 for any a= 1, ..., N ).
On the other hand, the early-stop dynamically accounts for such a filtering, since the time t∗at which
we stop the training is chosen so that only a subset of the diagonal entries saturate to the fixed point
J(D)
aa=γof the dynamical system (3.6), while all the others are not changed in a substantial way
w.r.t. the initial condition; in fact, as highlighted by (3.11), the characteristic time for saturation is
entry-dependent and given by (2 λa)−1, thus entries corresponding to large eigenvalues of Ωare faster.
What we presented so far does apply to a general set of Boolean vectors, as the coupling matrix
Jnaturally arises as the fixed point of a gradient descent algorithm, the only hypothesis that we
made on the vectors ξ, that we want to store as attractors for the neural dynamics, being that they
are of the same length and that they share the same “importance” γ. In particular, the dataset
items could represent (noisy) realizations of some unknown ground-truth patterns to which we have
no direct access. In this context, regularization – preventing the network parameters from acquiring
large norms during learning – also allows for a reduction of the model specialization on the training
set. The relation td=ϵ−1
Jtherefore suggests that overfitting issues may arise for too large td, as we
– 8 –

--- PAGE 10 ---
10−210−1100101102
td0.00.51.01.52.02.5t∗RANDOM
MNIST
FASHION-MNISTFigure 2 :Stopping time as a function of dreaming time. The plot shows the early-stopping
time t∗as a function of the dreaming time td, obtained by a numerical estimate (solid line) from
Eq. (3.12) and by a fit (dashed line) based on the functional relation t∗(td) =alog(1 + b td), suggested
by the analytical findings presented in App. C. The network parameters for the three cases are γ= 1,
N= 784, and P/N = 0.2. The couple of coefficients ( a, b) estimated via linear least squares are
(a= 0.66, b= 0.54),(a= 0.11, b= 3.25),(a= 0.19, b= 1.67) for the random, MNIST and Fashion-
MNIST datasets, respectively.
are going to discuss in more details in Sec. 4. Further, the dreaming time tdcan be related, through
td=ϵ−1
Jand Eq. (3.12), to the stopping time t∗in unregularized versions of the gradient descent
algorithm. This relation is consistent with the previous remark since early-stop techniques are indeed
designed for avoiding overfitting11and, again, recalling the monotonic relation between tdandt∗, we
expect that overfitting issues may arise for too large td. In the next section we will make use of the
framework outlined in this section and, specifically, of the optimal interaction matrix J(D), in order
to address the generalization capabilities of such models or, conversely, the emergence of overfitting.
4 Emergence of generalization and overfitting in Hopfield-like models
4.1 A synthetic dataset
The results derived in Secs. 2 and 3 were obtained without making specific assumptions on the binary
vectors {ξµ}P
µ=1, however, in order to go further in the analytical investigations, some additional
hypothesis are in order. In fact, in theoretical studies one usually assumes that pattern entries are
extracted according to a prescribed probability distribution that allows working out a controllable
theory. For instance, when dealing with the Hopfield model, a common choice is to take pattern
entries as i.i.d. Rademacher random variables, and thus treat the patterns as ground truths to be
reconstructed starting from a corrupted version of them. However, in practical applications, one
has no direct access to the ground-truth patterns, but only to empirical realizations constituting the
dataset from which we want to extract information. In a supervised scenario, one knows a priori how
the different items making up the dataset are partitioned between classes, so that it is possible to
define class archetypes (for example, taking the average of examples belonging to the same category)
which are taken as representatives of the ground vectors, see e.g., [31, 33, 71]. In the unsupervised
11This is executed by following the behaviour of training and validation losses, abruptly stopping the training proce-
dure when the latter exhibits a general growing behavior, see e.g. [70].
– 9 –

--- PAGE 11 ---
case, this clearly cannot be done, and the simplest way to proceed is to include all the examples,
homogeneously, in the treatment. This is the path that shall be pursued in this section and hereafter
we detail this unsupervised setting by considering a synthetic dataset.
Let{ζµ}K
µ=1∈ {− 1,+1}N×Kbe the ground patterns to which we have access only through
empirical realizations referred to as ξµ,Awith A= 1, . . . , M for each µ. We assume that the examples
in this training dataset are obtained from the ground patterns with a multiplicative noise, that is,
ξµ,A=χµ,Aζµ(with entry-wise multiplication), with
Prob[ χµ,A
i=±1] =1±r
2,
where r∈[0,1] is the parameter quantifying the quality of the example (i.e., it measures the correla-
tion between the example and the corresponding ground-pattern). Our training dataset is therefore
constituted by the set S={ξµ,A}A=1,...,M
µ=1,...,K and we distinguish two loads: α:=KM/N , i.e., the ratio
between the number of examples in the training set and the network size, and η:=K/N≤1, i.e.,
the ratio between the number of classes in the dataset (which is in principle unknown) and the net-
work size. With this kind of available information, we want to train the system in order to make it
able to generalize, namely to reconstruct the hidden ground-truths ζµ, starting from an input data
σ(0)that is a corrupted representation of ζµ. Since we have no direct access to the ground-truths, a
direct error minimization procedure is not feasible in this case. However, we can include each single
item in our loss function and take advantage of emergent phenomena in Hopfield-like models: as we
will see, for a sufficiently large dataset, a plethora of spurious states appear and, depending on the
control parameters of the system, these can favour the appearance of a generalization phase. In this
scenario, regularization mechanism plays a key role, preventing the solution of the system to trivialize
or overspecialize on the training set. In this unsupervised setting, the interaction matrix is therefore
obtained by plugging in (3.5), that is still solution of the gradient descent Eq. (2.6), the empirical
realization of the Hebbian kernel with entries Ω ij=1
PP
µ,Aξµ,A
iξµ,A
j, and the empirical correlation
matrix, whose size is ( KM)×(KM) and whose entries are C(µA),(νB)=1
KMP
iξµ,A
iξν,B
i.
To evaluate the performance of the network, we generate a test set ˜S={˜ξµ,A}A=1,...,M
µ=1,...,K , sampled
in the same way as the training set, and initialise the network with the configurations of the test
set, say σ(0)=˜ξµ,A, the latter being, by construction, a noisy version of ζµwith quality r. Next,
we check whether the network response is f(σ(0)) =ζµ, an outcome that we interpret as a correct
generalization; conversely, the retrieval of one of the training items, say f(σ(0)) =ξµ,A, is interpreted
as overfitting. In the following subsection, we discuss the role of spurious states in the emergence of
generalization and overfitting.
4.2 Spurious states of training data enable generalization
In the classical Hopfield setup, spurious states (i.e., configurations that are combinations of stored data)
are known to impair the retrieval capabilities of the model and should be suitably treated in order
to reduce their attractiveness, see e.g., [21, 25–27, 39, 47, 63, 72]. In fact, the dreaming mechanisms
mentioned in Sec. 3 are precisely aimed at this purpose and their implementation improves the retrieval
capabilities of the network. On the other hand, when dealing with a dataset made of unlabelled
examples, the situation is quite different, and spurious attractors are crucial for the emergence of
generalization capabilities of the network, as we are going to discuss.
Let us suppose that the training set is made of a large number Mof data for each class, and let
us consider a spurious configuration given by a symmetric combination of Lexamples pertaining to
– 10 –

--- PAGE 12 ---
rc5c10Ground-patternsL=10L=5ExamplesFigure 3 :Schematic representation of training points, spurious combinations and ground-
pattern. The figure sketches the organization of attracting configurations within each class in the
dataset. The class is represented by a ground-pattern ζ(the red dot in the center), while the training
points are located at distance (1 −r)/2 from it (i.e. they have correlation r). Spurious combinations of
training points are themselves attracting points, and their correlation cL(r) increases with the number
Lof training points involved in the combination. For large enough M, the resulting landscape consists
in many local minima very close to each other, so that they coalesce and form flat valleys around the
ground pattern.
the same class, that is,
ξµ
L= sgnLX
l=1ξµ,Al
, (4.1)
being A1, . . . , A L∈ {1, . . . , M }the indices of the examples that we are mixing. Denoting with cL(r) :=
ξµ
L·ζµ/Nthe correlation between ξµ
Land the related ground truth, we notice that, as long as Lis
relatively large, ξµ
Ldisplays a correlation with the ground pattern ζµthat is larger than the correlation
rdisplayed by any training item, that is r < c L(r)→
L≫11, see App. D for more details.
Now, spurious configurations of the form (4.1) in Hopfield-like models can be stable attractors, so
that running the dynamics (2.1) we could end in one of these minima and reach a relatively fair
retrieval. Indeed, each of these configurations originates as a combination of attractors associated
to stored vectors. In our scenario, increasing M, would result in an increasing number of intra-class
spurious configurations which, as Lis increased, do exhibit larger and larger correlation with the
corresponding ground-truths as sketched in Fig. 3. For sufficiently large M, it is then reasonable
to expect that minima of training examples and spurious configurations coalesce together, so that
the resulting landscape consists in a wide minima centered in the ground-truth ζµ, favoring the
reconstruction of the hidden patterns.
Given such a landscape, which is the role of td(or, equivalently, of ϵJ)? As recalled at the begin-
ning of this subsection, in a Hopfield model where we store ground patterns, dreaming mechanisms
reduce (and ultimately remove, if the load is not too high) the stability of spurious mixtures between
independent patterns and this is obtained by shrinking and lifting the attraction basins associated
to the patterns. Moving to an unsupervised setting, we realize that there are two kinds of mixture,
according to whether they involve examples belonging to different classes or examples belonging to
– 11 –

--- PAGE 13 ---
the same class; the former, just like in the Hopfield model, impair retrieval and should be removed,
while the latter, as mentioned above, can be beneficial for generalization. Therefore, in this case,
the dreaming mechanism should operate in removing only the first type of correlation. In fact, by
increasing tdwe are disentangling the minima corresponding to the stored patterns and this process
affects progressively minima with larger and larger overlap. Inter-class correlation is typically smaller
than intra-class correlation – their extents being related to, respectively, K/N andr– and relatively
small values of tdcan be sufficient to detach the attraction basins related to different ground-patterns.
Yet, if we let the dreaming mechanism operate for too long a time, the m´ elange of intra-class minima
can be separated as well and they get fragmented in many energetic minima, each associated to a
single example. As a consequence, we cannot retrieve spontaneously-formed archetypes, but only the
single examples: the system is specialized over the training set, thus ending in an overfitting regime.
This picture is corroborated by the numerical results reported in Fig. 4. Here, we focus on the be-
havior of the system when prepared in the neighborhood of a training example or of an intra-class
spurious configuration. First, we notice that, for low enough td, even when the system is initialised
in a configuration consisting in a mild perturbation of one of the stored examples ( L= 1), the neural
dynamics will drive it towards a final configuration with the relative distance w.r.t. the reference
example being (1 −r)/2 (precisely the distance between the stored examples and the corresponding
ground-truth). For spurious states, the situation is similar, with the relative distance between the
final state and the reference mixture getting lower and lower as Lis increased (as the correlation
cL(r) with the ground-truth increases monotonically with L). A moderately larger tdyields a larger
attractivity of the ground patterns. This picture is consistent with our claim about the coalescence
of the population of attraction basins in a wide minima centered at ζµ. By further increasing td, the
training points get attractive, with a basin width depending on the number of examples per class.
This signals that, according to the setting, the dreaming mechanism can either enhance generalisation
or favour overfitting.
5 Numerical experiments
In this Section, we provide numerical evidence to our theoretical findings. We first inspect the regions
in the space of parameters ( α, td, r) where the system equipped with the interaction matrix J(D)can
successfully generalize, that is, when tested with examples not included in the training set (but sharing
with them the same underlying statistics), it is able to fairly reconstruct the ground pattern. Next, we
corroborate this picture by applying a clustering algorithm to the network outputs and showing that,
in the region of the parameter space where the system is expected to generalize (resp. overfit), the
number of classes is nicely estimated (overestimated). Details on numerics are collected in App. B.
5.1 Generalization diagrams
In the first part of the numerical experiments we consider both structureless and structured datasets,
to confirm and check the robustness of our theoretical results. The structureless datasets are built
synthetically as follows: we initially generate a set of KRademacher ground patterns G={ζµ}µ=1,...,K,
whence we obtain a set of training examples S={ξµ,A}A=1,...,M
µ=1,...,K (characterized by a quality ras
specified in Sec. 4.1), which are used to build J(D), according to Eq. (3.5). Next, we generate a test
set˜S={˜ξµ,A}A=1,...,M
µ=1,...,K , applying the same procedure used for the training set, that is, each item
˜ξµ,Aexhibits a quality rw.r.t. the related ground pattern ζµ. For structured datasets, we consider
the MNIST [66] and the Fashion-MNIST [67] benchmarks and define the ground patterns as the class
– 12 –

--- PAGE 14 ---
0 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.4
0 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.4
0 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.40 0.1 0.2 0.3 0.400.20.4Figure 4 :Relaxation to fixed points from perturbed training examples and spurious
states. The plots show the retrieval capabilities of the model initialized in a configuration consisting
in a perturbed version of the training examples (i.e., L= 1) or in an intra-class spurious configuration
ξL(as given by Eq. (4.1)). The network size is fixed to N= 500, the number of classes is K= 10,
and the quality of the dataset is r= 0.8, while different values of dreaming time (from left to right
td= 0.1,2,10) and of load (from top to bottom α= 0.4,0.6,0.8, that is, M= 20,30,40) are considered.
The analysis is performed by taking a reference configurations ξL(with for L= 1,3,5,20, as explained
by the legend) and applying a perturbation that consists in randomly flipping a fraction qof the entries;
preparing the system in this configuration σ(0), we update the network up to convergence towards the
fixed point σ(∞). Then, we compare the average distances ¯d(ξL,σ(0)) and ¯d(ξL,σ(∞)) between the
reference configurations and, respectively, the initial and the final configurations. The dashed black
lines correspond to the distance between the training examples used to build J(D)and the associated
ground-truths. The results are averaged over 50 different realizations of the dataset.
averages, then, the training and the test sets are made of Mitems, drawn from the whole datasets
(overall made of, respectively, 60000 and 10000 instances), in such a way that the two sets have null
intersection.
Whatever the dataset, we initialize the system in a configuration σ(0)belonging to the test set, we
run the dynamics (2.1) and collect the final configuration σ(∞)=f(σ(0)). In other words, σ(0)and
σ(∞)represent, respectively, the input and the output of the system. Next, we evaluate the following
– 13 –

--- PAGE 15 ---
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8RANDOM
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8RANDOM
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8RANDOM
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.0
0.00.20.40.60.81.00.00.20.40.60.81.00.000.250.500.751.000.000.070.14¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.000.070.14Æ=0.2
0.000.250.500.751.000.000.070.14Æ=0.8RANDOM
0.000.250.500.751.000.060.090.12¯dª,¯d≥Æ=0.1
0.000.250.500.751.000.060.090.12Æ=0.2
0.000.250.500.751.000.060.090.12Æ=0.8MNIST
0.000.250.500.751.000.060.130.20¯dª,¯d≥Æ=0.1¯dª¯d≥0.000.250.500.751.000.060.130.20Æ=0.2
0.000.250.500.751.000.060.130.20Æ=0.8FASHION-MNIST
td1+tdtd1+tdtd1+tdFigure 5 :Retrieval on synthetic and structured datasets. The retrieval performance is mea-
sured in terms of the normalized Hamming distance dbetween the final configuration σ(∞)and the
nearest training example ξ(dotted curve, see Eq. (5.1)) and the nearest ground-truth ζ(solid curve,
see Eq. (5.2)); the results presented have been averaged over the K×Mdifferent initial configura-
tions which constitute the test set (see App. B for further details). The network parameters for the
random dataset are N= 200, K= 10 and r= 0.8, whereas for the structured datasets they are
N= 784 , K= 10. For all the datasets, we reported results for different choices of α= 0.1, 0.2 and
0.8, retaining η=K/N fixed, therefore, recalling that α=KM/N , we varied αby increasing M.
quantities:
dξ= min
ξ∈Sd(ξ,σ(∞)), (5.1)
dζ= min
ζ∈Gd(ζ,σ(∞)), (5.2)
where d:{−1,+1}N×{−1,+1}N→[0,1] is the normalized Hamming distance, measuring the fraction
of misaligned entries among the two configurations that are compared. We stress that, despite not
highlighted in Eqs. (5.1)-(5.2), these quantities depend on the initial configuration as σ(∞)does depend
onσ(0). These distances are then averaged over the sample ˜S, to get, respectively, ¯dξand¯dζ.
In Fig. 5, we compare the behaviour of ¯dξand ¯dζversus td, for different choices of M, while K
is fixed. We find that, in any case, ¯dξand ¯dζare monotonically decreasing with tdas long as tdis
relatively small, next, their behavior depends on M. In particular, for the random dataset, when Mis
– 14 –

--- PAGE 16 ---
small, we always have ¯dξ<¯dζthat evidences poor generalization capabilities; when Mis larger we can
leverage tdto enhance the generalization capabilities and get ¯dζ<¯dξ, yet when large tdis too large the
curves cross; finally, when Mis large, at intermediate dreaming time, ¯dξand¯dζexhibit a plateau and
at large values of tdthey grow, the height of the plateau (respectively, ≈(1−r)/2 and ≈0) suggests
that there the final configuration is close to the ground pattern, while the final growth suggests a
possible harmful effect of a large dreaming time. As for structured datasets, ¯dζexhibits a minimum
at intermediate values of td, corresponding to an optimal generalization performance and, again, for
large dreaming times, ¯dζgrows. In particular, for the MNIST dataset, for relatively small (resp. large)
values of td, we get ¯dζ<¯dξ(resp. ¯dζ>¯dξ), suggesting good (impoverished) generalization capabilities.
Before proceeding, we also emphasize that, for all the datasets considered, when td≫1 and when
Mis relatively large, both ¯dζand ¯dξgrow. This is due to the fact that the number of examples is
relatively large to give rise to spurious attractors, but not large enough to make these attractors close
to the ground truths; this point is further examined in the following.
We now focus on the synthetic dataset and summarize the information processing capabilities of
the system into “phase diagrams”. To this aim, we distinguish between different outcomes as follows:
•Success : this corresponds to ¯dζ<¯dξand ¯dζ<1−r
2. The first requirement ensures that the
system relaxes in a configuration which is more correlated with the ground pattern than with the
training points; the second condition, instead, guarantees that the dynamics ends up within the
Hamming ball centered in the ground-truth with radius (1 −r)/2 and therefore that the system
has moved closer to the ground-truth.
•Overfitting : this corresponds to ¯dζ≥¯dξand¯dξ<1−r
2. The first condition states that the final
configuration is closer to one of the training points than to the ground; the second condition,
guarantees that, this time, the dynamics ends up within the Hamming ball centered in the
nearest training points with radius (1 −r)/2 and therefore that the system has moved closer to
a specific training item.
•Failure : otherwise. In this case the system is neither sufficiently close to a ground-pattern nor
to an example.
For a given choice of Kandr, moving within the ( α, td) plane, we thus depict the generalization
diagrams for synthetic datasets. The results are reported in Fig. 6 and discussed hereafter.
Starting from panel a(K= 10 and r= 0.7), we find that, at very low td, the system is always
in a failure regime. This is not surprising since, there, the interaction matrix is very close to the
Hebbian prescription and, for this choice of the dataset parameters, interferences among examples are
significant enough for the system to be likely to end up in inter-class spurious states, thus we expect
that equilibrium configurations are non-retrieval states. Increasing td, such interferences are (possibly)
removed. If the load parameter αis low (meaning that the number of examples is low), the system
enters in an overfitting regime because the minima corresponding to examples are sparse enough to be
easily disentangled. On the other hand, by increasing α(namely by increasing the number of examples
per class), the attractors coalesce and configurations corresponding to the ground patterns become
more and more attractive, in such a way that the system starts to well-generalize. When td≫1, as
αis increased, the transition from overfitting to success is no longer direct as, for intermediate values
ofα, we can end up into spurious states that are still too sparse to ensure a sound generalization; this
region can be shrunk by enhancing the dataset quality. In fact, by increasing the dataset quality as
in panel b(K= 10 and r= 0.8), the qualitative picture is the same, with just an expansion of the
success region due to the fact that intra-class examples are now closer to each other. Next, we move
– 15 –

--- PAGE 17 ---
0.000.350.701.051.400.20.40.60.8td1+td(a)K= 10,r=0.7
0.001.052.103.154.200.20.40.60.8(c)K= 30,r=0.7
0.000.350.701.051.40Æ0.20.40.60.8td1+td(b)K= 10,r=0.8
0.001.052.103.154.20Æ0.20.40.60.8(d)K= 30,r=0.8
F
S
OFigure 6 :Generalization diagrams. The four diagrams show the generalization outcomes of the
neural network where the interaction matrix J(D)is built on a sample of random, synthetic examples
{ξµ,A}A=1,...,M
µ=1,...,K with N= 200 and Mtunable ( M=αN/K ). In the ( α, td) plane, for various values
ofKandr, we outline three regions: success (S), overfitting (O), failure (F). In any case the initial
conditions σ(0)are taken as perturbed versions of the ground-truths sampled with the same quality r
as the training examples.
to panels candd, where a larger η(namely, a larger number of classes K= 30, with fixed N) benefits
the failure region, since in this case the clusters of minima associated to each class are closer and
clusters of minima corresponding to different classes now present non-trivial overlaps, in such a way
that the relaxation of the system could end far away from the class for which the initial condition was
generated.
5.2 An analogy with a clustering algorithm
In this section, we use another approach to check the emergence of overfitting and generalization
regimes as the system parameters are tuned. The idea is to use an unsupervised clustering algorithm
to partition the final configurations σ(∞)obtained by applying the dynamics (2.1) to the test config-
urations; here unsupervised refers to the fact that the clustering algorithm is unaware of the number
of effective clusters.
We start the experiment by generating a random synthetic datasets Gmade of KRademacher
ground patterns, whence we build a training set Sand a test set ˜S, both characterized by a quality rand
– 16 –

--- PAGE 18 ---
a size M. We use the former to construct J(D)and the latter to initialize the neural configuration. We
collect the final configurations σ(∞)obtained by iterating the neural dynamics and we expect that, if
the network correctly generalizes, the clustering algorithm applied to the final configurations will return
an estimated number of clusters ˆKwhich is (approximately) Kand each cluster contains a number
of items which is (approximately) M. Conversely, if the network overfits, we expect that ˆK > K .
In order to quantify the likelihood of these outcomes we introduce the accuracyˆM
M×K∈[0,1], where
ˆMis the total number of examples correctly clustered by the algorithm. The unsupervised clustering
algorithm considered here is based on the Disjoint Set Union data structure [73], which works as
follows. Initially, it associates to each item σ(∞)a different class, in such a way that, at this stage, the
number of estimated classes is M×K. Next, we consider all the M
2
couples of configurations σ(∞)
and check whether their normalized Hamming distance is smaller than a threshold d∗(r) and, if so, the
two items are merged in the same class. Once all the couples have been examined the algorithm stops.
The threshold value is chosen equal to the minimum of the normalised Hamming distance between all
pairs of examples belonging to the test set:
d∗(r) = min
(σ1,σ2)∈˜S×˜S
σ1̸=σ2d(σ1,σ2).
The idea underlying this choice is that, after applying the dynamics to the test examples, if the network
performs well, the examples belonging to the same class get closer to the common ground pattern,
their distance is reduced and expected to be smaller than d∗(r); on the other hand, for two examples
belonging to different classes the distance is expected not to vary significantly and remain larger than
d∗(r).
The results of this experiment, repeated for different loads and different dreaming times, are
reported in Fig. 7. The panels in the first row show the accuracy (left axis) and the difference
between ˆKand the true number of classed K(right axis) as a function oftd
1+td, while the panels
in the second row show the average distances ¯dζand ¯dξbetween σ(∞)and, respectively, the ground
truth and the nearest example, as a function oftd
1+td. The colours in the background correspond to
the different regimes of the network and we used the same colormap previously adopted in Fig. 6, to
highlight the consistency. In fact, in the failure region the accuracy is low and the number of clusters
is underestimated; in the success region the accuracy is unitary and ˆK=K; in the overfitting region
the accuracy is suboptimal and the number of cluster is overestimated. Further, these outcomes are
nicely mirrored by the behavior of dζanddξ: the transition from failure to success corresponds to an
abrupt decrease of dζthat leaves dξbehind; the transition between success to overfit corresponds to
dξoutpacing dζ.
6 Conclusions
The main results obtained in this work are listed hereafter:
•We introduced a regularized loss function, whose minimum provides the interaction matrix Jof
an associative neural network; according to the dataset provided, the neural network equipped
with the solution Jis able to retrieve a set of stored ground patterns or to generalize starting
from a corrupted version of unknown ground patterns.
•We proved that the solution Jof the loss function corresponds to a Hebbian-like kernel J(D),
known as dreaming Hebbian kernel and parametrized by the “dreaming time” td, as long as td
is identified with the inverse of the regularization parameter ϵJ.
– 17 –

--- PAGE 19 ---
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.00.10.2¯dζ,¯dξ
0.0 0.2 0.4 0.6 0.8 1.0
td
td+10.00.10.2¯dξ¯dζ0.0 0.2 0.4 0.6 0.8 1.00.60.81.0accuracyα= 0.15
0.0 0.2 0.4 0.6 0.8 1.00.60.81.0α= 0.3
ˆK−K
accuracy
024
ˆK−K
024Figure 7 :Clustering of the test set. Application of the clustering algorithm in case of a random
synthetic dataset with parameters K= 10, r= 0.8, N= 200 for two different loads values: α= 0.15
andα= 0.3 keeping fixed the number of ground-truths K. In the simulation with α= 0.15 the
overfitting region occurs when ¯dξis overcome by ¯dζ, in this region the clustering algorithm is no
longer able to correctly cluster the examples and the number of clusters estimated starts to grow. In
the simulation with α= 0.3, the number of examples per ground truth in the training set is double
that of the previous simulation, ¯dζis always lower than that of the nearest training example ¯dξand
the overfitting region is no longer present.
•In the absence of regularization ( ϵJ= 0) the solution J(D)can be recovered by applying an
early-stop strategy to the gradient descent over the loss function. This suggests that td(or,
equivalently, ϵJ= 0) plays a role in preventing overspecialization on the training set.
•Focusing on the case of a training set made of corrupted versions of some unknown ground
patterns, we found robust numerical evidence that relatively large values of tdand relatively
sparse training sets can yield overfitting.
•The emergence of overfitting is related to the structure of the Lyapunov function associated to
the neural dynamics and this picture allowed us to speculate on optimal settings for the loss
hyperparameters and/or for the training time. This picture is sketched in Fig. 8.
To conclude, our results highlight the relevant mechanism allowing for the emergence of gen-
eralization capabilities of Hopfield-like networks: this is identified as the coalescence of attractors
associated to training points giving rise to wide minima around the underlying ground truths (which
is, indeed, a crucial ingredient for general models exhibiting robust generalization properties, see for
example [74, 75] and references therein). In this scenario we give a comprehensive characterization of
generalization and overfitting for synthetic random datasets. Developments of the present work would
require the extensions to structured data, as well as a statistical mechanics characterization of relevant
collective phenomena.
– 18 –

--- PAGE 20 ---
Ground-featuresIntra-class spurious statesTraining points(attractors)
Primary attractors (examples)Secondary attractors (spurious states)
Generalization regimeOverfitting regimeGround-truthsResulting landscapeFigure 8 :Emergence of fixed points for a system trained without supervision . This
schematic picture shows the evolution of the landscape generated by J(D)astdis varied. When
tdis relatively low (left), the m´ elange of minima has been partially disentangled: the weak inter-
class interference is removed, while the minima corresponding to examples of the same class are still
clusterized; when tdis relatively large (right) the interference among training examples has been
completely shifted. This picture is compatible with our numerical results reported in Fig. 4.
7 Acknowledgments
E.A. and A.F. acknowledge financial support from PNRR MUR project PE0000013-FAIR and from
Sapienza University of Rome (RM120172B8066CB0, AR2221815D7192C1, AR1221815EA97525).
F.A. and A.F. have been fully supported by PNRR MUR project PE0000013-FAIR.
E.A., M.A., F.A., A.F. acknowledge the stimulating research environment provided by the Alan Turing
Institute’s Theory and Methods Challenge Fortnights event “Physics-informed Machine Learning”.
A Convergence of the gradient descent procedure
The discretization of the dynamical equation (3.6) reads as
J(n+ 1) = J(n)−ϵ
J(n)(Ω+ϵJ1) + (Ω+ϵJ1)J(n)−2γΩ
, (A.1)
with the initial condition J(n= 0) = 0. For simplicity, we fix γ= 1 without loss of generality, as it
only accounts for a global rescaling of the coupling matrix. The dynamical equation Eq. (A.1) can be
rewritten as a fixed-point equation:
J(n+ 1) = Gϵ(J(n)),
– 19 –

--- PAGE 21 ---
with
Gϵ(J) =J−ϵ(J(Ω+ϵJ1) + (Ω+ϵJ1)J−2γΩ).
Given JandKtwoN×Nmatrices, we have
Gϵ(J)−Gϵ(K) =1
2(J−K)(1−2ϵ(Ω+ϵJ1)) +1
2(1−2ϵ(Ω+ϵJ1))(J−K).
Taking the (operator) norm of both sides, we have
∥Gϵ(J)−Gϵ(K)∥≤ ∥J−K∥·∥1−2ϵ(Ω+ϵJ1)∥=∥J−K∥(1−2ϵ(λ1+ϵJ)),
where λ1is the largest eigenvalue of Ω. This means that the function Gϵ(J) is a contraction map,
provided that
ϵ≤1
2(ϵJ+λ1). (A.2)
Thus, exploiting Banach Fixed Point Theorem, the algorithm converges to the solution of the system.
Further, by Gershgorin’s theorem, we can get the following bound
λ1≤X
ij|Ωij|. (A.3)
Thus, for any ϵsuch that
ϵ≤1
2(ϵJ+P
ij|Ωij|), (A.4)
the convergence requirement (A.2) is trivially satisfied.
B Methods
This appendix is devoted to a detailed description of numerical experiments. We recall that our ex-
periments encompass a training phase, in which we train the coupling matrix of the network with a
training set, and a reconstruction phase, in which we perform a sequential dynamics on the neural con-
figuration starting from an item belonging to a test set. Training and test sets are described in Sec. 4.1
and Sec. 5: synthetic (Rademacher ground patterns) and benchmark (MNIST and Fashion-MNIST)
datasets are considered. To facilitate the reproducibility of the research, here we present algorithms
and pseudocodes of the numerical experiments (Subsec. B.1) along with the performance metrics to
evaluate the quality of the final neuron states (Subsec. B.2); further evidence on the emergence of
overfitting is also provided (Subsec. B.3). Moreover, we point out that all the simulations were carried
out using the high-performance, dynamic programming language Julia and were run on a personal
computer with an Intel Core i7 processor.
B.1 Training design and parameters assignment
In the training phase we find the expression of the coupling matrix Jand of the external field h
which minimize the loss function Lξ(J,h) given in equation (2.7) via gradient descent method. The
parameters appearing in the loss function are summarized and described in Tab. 1. Specifically,
Nis the size of the input and is equal to the length of the training patterns, Pis the size of the
training set (or total number of training patterns), ϵJis the regularization constant for the coupling
matrix, ϵhis the regularization constant for the field hand, finally, tis the training time. In the
– 20 –

--- PAGE 22 ---
Parameter Description
N size of the input
P size of the training set
ϵJ L2-regularization term for J
ϵh L2-regularization term for h
t training time
Table 1 : Training parameters.
Algorithm 1 Training of the coupling matrix J
Input : Training set ξ={ξµ}P
µ=1∈ {− 1,+1}N×P, stopping time t, regularizator ϵJ
Settings for tandϵJ:
L2-regularization: t=∞andϵJ>0
Early-stop regularization: t=t∗as given by (3.12) andϵJ= 0
Output :J(t),h(t)
1:J(0)=0N,Ninizialization of the coupling matrix
2:ˆξµ=ξµ−1
PPP
µ=1ξµpattern centering
3:ˆΩ=1
P(ˆξ·ˆξT)computation of the Hebbian kernel
4:∆t=1
2[ϵJ+PN
i,j=1|ˆΩij|]−1discretization time as given by (A.4)
5:iters =l
t
∆tm
number of iterations to be run
6:n=0
7:repeat
8: J=J−∆t
2ϵJJ+J′ˆΩ+ˆΩ′J−2γˆΩ
9: n=n+ 1
10:until n=iters orJhas reached a fixed point.
11:return J,h= (γ1−J)1
PPP
µ=1ξµ
simulations with structured datasets such as MNIST and Fashion-MNIST, since we want to reconstruct
the original dataset and not its centered version, we preserve the field hin the dynamics and, for the
sake of convenience and simplicity we set ϵh= 0 in all the simulations. The pseudo-code presented in
Algorithm 1 highlights the steps followed to train the model.
If we run Algorithm 1 up to convergence, the resulting interaction matrix recovers the dreaming
kernel, denoted as J(D)and reported in Eq. (3.5), as long as we set ϵJ=t−1
dandγ= 1. Thus, if
we are interested in the regularized, full-trained model, we can directly pose J=J(D)without the
need of running the training procedure. If we set ϵJ=ϵh= 0 and we stop the gradient descent at the
training t=t∗given in (3.12), then the resulting coupling matrix J(t∗) is the closest possible to the
interaction matrix J(D).
B.2 Formulation of the performance metrics
Once the training is over and we have the desired expression of the coupling matrix J, the retrieval
capabilities of the machine are investigated. The initial configuration σ(0)is taken as a corrupted
version of one of the training patterns (with a fraction of flipped entries w.r.t. the reference configu-
– 21 –

--- PAGE 23 ---
Algorithm 2 Sequential dynamic
Input : Couplings J∈RN×N, external fields h∈RN, input σ(0)∈ {− 1,1}N
Output : Final neural configuration σ(∞)
1:Remove the diagonal terms from J
2:Setn= 0
3:repeat
4: sample a random integer iuniformly in the set {1,2, . . . , N }
5: update the i-th spin σiaccording to σ(n+1)
i =sign(PN
j=1Jijσ(n)
j+hiσ(n)
i)
6: n=n+ 1
7:until σhas reached a fixed point.
ration) or as an item of a test set (whose elements are statistically analogous to the training patterns,
but were not involved in the training procedure); in general, we denote with Qthe sample of initial
configurations. Then, the system relaxes according to the evolution rule introduced in Eq. (2.1) and
reported hereafter in a discrete-time notation
σ(n+1)
i = sign(X
jJijσ(n)
j+hiσ(n)
i). (B.1)
The pseudo-code for the dynamics can be found in Algorithm 2. After the relaxation towards an
equilibrium configuration σ(∞), we check its proximity to a specific configurations ξ∗by exploiting
the normalized Hamming distance as a performance metrics. The latter is the number of misaligned
entries between two configurations σ1,σ2∈ {− 1,1}Ndivided by N, that is
d(σ1,σ2) :=1
2NNX
i=1|σ1
i−σ2
i|.
Then, we evaluate d(σ(∞),ξ∗), which depends on the initial configuration, due to the fact that
σ(∞)depends on σ(0). Next, we average d(σ(∞),ξ∗) over the |Q|different realizations of the initial
condition; when σ(0)is meant as an item of the test set, this operation corresponds to a batch average.
This way, we get the average distance, defined as
¯dξ∗=1
|Q|X
σ(0)∈Qd(σ(∞)(σ(0)),ξ∗), (B.2)
that is expected to depend on the reference point ξ∗and on the system parameters.
Finally, we notice that d(σ(∞),ξ∗) is nothing but the (normalized) absolute error made by the
machine outputting σ(∞), when asked to reconstruct ξ∗.
B.3 Numerical signatures of overfitting
In this section, we will give more numerical indications of overfitting emergence for the model we are
dealing with. Specifically, we check that overfitting can take place in the non-regularized training
procedure. To do this, we follow the evolution of the following learning and validation loss functions
over training time, in particular the loss functions have the following structure
Lξ(J(t),h(t)) =1
2PX
i,µ(X
jJij(t)ξµ
j+hi(t)−ξµ
i)2+1
2PX
j,µ(X
iJij(t)ξµ
i+hj(t)−ξµ
j)2,(B.3)
– 22 –

--- PAGE 24 ---
0.00 0.06 0.26 0.89 2.86 9.00
t0.00.10.20.30.4LTraining
RANDOM
MNIST
FASHION-MNIST
0.00 0.06 0.26 0.89 2.86 9.00
t0.20.30.40.5TestingFigure 9 :Training and test losses as a function of the training time. The figure shows
the comparison between the training loss function and the validation ones for synthetic dataset (blue
curve), MNIST (orange curve) and Fashion-MNIST (green curve) for a training procedure without
regularization term.
where
hi(t) =1
PPX
µ=1(ξµ
i−NX
j=1Jij(t)ξµ
j) (B.4)
In the following numerical simulations, Jij(t) evolves according to Algorithm 1 with parameters ϵJ.
Equation (B.3) is the loss given in Eq. (2.7) of the main text with ϵJ=ϵh= 0, γ= 1. The learning
and validation loss functions are obtained by substituting into the previous equations {ξ}P
µ=1with the
features of the training and test dataset respectively. For the synthetic random dataset and MNIST
and Fashion-MNIST cases we found that, even if the training loss goes to zero, the validation ones
do exhibit a global minimum at finite training time, and then is start to increase, thus signalizing a
worsening in generalization performances. These results are reported in Fig. 9.
C A faster way to compute the early-stopping time
The estimate of the early-stopping time reported in Eq. (3.12) is based on the computation of the
empirical spectral distribution ρEofΩ, which is a N×Nmatrix. For high-dimension datasets, this
can constitute a bottleneck in the training procedure, so operative criteria needs to be provided. To
do this, we can Taylor expand the quantity δ(λ, t, ϵ J) around the average eigenvalue of the empirical
distribution, i.e.
δ(λ, t, ϵ J) =∞X
k=01
k!δ(¯λ, t, ϵ J)(k)(λ−¯λ)k,
thus
¯δ(t, ϵJ) =∞X
k=01
k!δ(¯λ, t, ϵ J)(k)Z
dλ 
λ−¯λkρE(λ) =∞X
k=01
k!δ(¯λ, t, ϵ J)(k)1
NETr (Ω−¯λ1)k.
– 23 –

--- PAGE 25 ---
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ1)RANDOM
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNIST
0.00.20.40.60.81.0tdtd+10.000.050.100.15FASHION-MNISTDreamingGD-Early stoppingÆ=0.2Æ=0.3Æ=0.5
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ(1))RANDOM
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNISTP/ N=0.2P/ N=0.3P/ N=0.50.00.20.40.60.81.0tdtd+10.000.050.100.150.20FASHION-MNISTDreamingGD-Early stoppingDreamingGD-Early stopping
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ(1))RANDOM
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNISTP/ N=0.2P/ N=0.3P/ N=0.50.00.20.40.60.81.0tdtd+10.000.050.100.150.20FASHION-MNISTDreamingGD-Early stoppingDreamingGD-Early stopping
0.00.20.40.60.81.0tdtd+10.00.10.20.30.4¯d(ª,æ(1))RANDOM
0.00.20.40.60.81.0tdtd+10.000.050.100.150.20MNISTP/ N=0.2P/ N=0.3P/ N=0.50.00.20.40.60.81.0tdtd+10.000.050.100.150.20FASHION-MNISTDreamingGD-Early stoppingDreamingGD-Early stoppingFigure 10 :Comparison between early-stopping and dreaming kernel with approximated
time. The three plots show the comparison between the retrieval performances of the Dreaming kernel
and the early-stopped training procedure. The content is perfectly specular to Fig. 1, with the only
exception that the early-stopping time here is computed with the first-order approximation (C.1).
Stopping at the first order in λ−¯λand then solving the minimization problem, we are left with the
prescription
t∗(td)≈1
21
NETrΩlog
1 +td1
NETrΩ
. (C.1)
The results are reported in Fig. 10, which shows again a substantial agreement between the early-
stopping procedure and the Dreaming kernel scenario. Notice that this criterion precisely accounts for
the correct logarithmic dependence of the early-stopping time w.r.t. the dreaming time td. Clearly,
this prescription can provide, in the general case, a rough estimate of the early-stopping time. In that
case, one can decide also to work out sub-leading orders in λ−¯λ: regardless of the order at which
the computation is performed, the numerical estimate of t∗is based on the computation of low-order
moments of the (modified) Hebbian kernel ˆΩ, which is far easier than to compute the whole empirical
spectral distribution.
D Details on spurious states
In this Appendix, we report some details about spurious configurations of training examples in the
synthetic random dataset. We recall that ξµ,A=χµ,Aζµ(with entry-wise multiplication), with the
χµ,A
ivariables extracted as
Prob[ χµ,A
i=±1] =1±r
2.
Relevant spurious configurations in this setup are of Hopfield-type, so that we can consider combination
of the form
ξµ
L= sgnLX
l=1ξµ,Al
. (D.1)
The correlation of this new configuration with the ground-pattern ζµis
cL=1
NX
iξµ
L,iζµ
i=1
NX
isgn LX
l=1ξµ,Al
iζµ
i
=1
NX
isgn LX
l=1χµ,Al
i
.
– 24 –

--- PAGE 26 ---
For large L, the random variable in the sign function is, by central limit theorem (CLT), Gaussian
distributed, with mean rand variancep
(1−r2)/L, thus
cL=1
NX
isgn 
1 +r
1−r2
Lr2zi
,
with z∼ N (0,1), where we also used r > 0. The argument of the sign function is positive with
probability
p=P 
zi≥ −r
Lr2
1−r2
= 1−1
2erfchs
Lr2
2(1−r2)i
.
Thus, the quantity cLis nothing but a random walk of unitary steps with probability pto jump on
the right. In the large Nlimit, we thus have
cL(r) =1
NX
isgn 
1 +r
1−r2
Lr2zi
≈2p−1 = erfhs
Lr2
2(1−r2)i
.
which yields
cL(r)> r ⇒ L >2[erf−1(r)]2(1−r2)
r2.
Now, the r.h.s. of the last inequality ranges in [0 , π/2], while the approximated expression for cL(r)
is obtained under the CLT, requiring L≫1, which is therefore sufficient for obtaining cL(r)> r.
For fixed Lthere are M
L
possible configurations of the form (4.1). Just to give an example of the
magnitudes here involved, for M= 50 and r= 0.6, we have 1225 spurious combinations of L= 2
examples displaying a correlation with ground-feature c2≈0.8884, we have ∼2·106combinations
with L= 5 displaying a correlation c5≈0.9993, and so on.
References
[1] S.-I. Amari. Learning patterns and pattern sequences by self-organizing nets of threshold elements.
IEEE Transactions. , 21:1197–1206, 1972.
[2] W. A. Little. The existence of persistent states in the brain. Mathematical Biosciences , 19(1-2):101–120,
1974.
[3] J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences of the United States of America , 79:2554–2558, 1982.
[4] D.J. Amit, H. Gutfreund, and H. Sompolinsky. Information storage in neural networks with low levels
of activity. Physical Review A , 35(5):2293, 1987.
[5] E. Agliari, F.E. Leonelli, and C. Marullo. Storing, learning and retrieving biased patterns. Applied
Mathematics and Computation , 415:126716, 2022.
[6] E. Agliari, A. Barra, A. Galluzzi, F. Guerra, and F Moauro. Multitasking associative networks. Physical
Review Letters , 109:268101, 2012.
[7] L. Cugliandolo. Correlated attractors from uncorrelated stimuli. Neural Computation , 6:220, 1993.
[8] E. Agliari, A. Barra, A. De Antoni, and A. Galluzzi. Parallel retrieval of correlated patterns: From
hopfield networks to boltzmann machines. Neural Networks , 38:52–63, 2013.
[9] E. Agliari, A. Fachechi, and C. Marullo. The relativistic hopfield model with correlated patterns.
Journal of Mathematical Physics , 61(123301), 2020.
– 25 –

--- PAGE 27 ---
[10] E. Agliari, F. Alemanno, A. Barra, M. Centonze, and A. Fachechi. Neural networks with a redundant
representation: Detecting the undetectable. Physical Review Letters , 124:028301, 2020.
[11] E. Agliari and G. De Marzo. Tolerance versus synaptic noise in dense associative memories. European
Physical Journal Plus , 135:883, 2020.
[12] F. Camilli, P. Contucci, and E. Mingione. An inference problem in a mismatched setting: a spin-glass
model with mattis interaction. SciPost Phys. , 12:125, 2022.
[13] B. Wemmenhove and A.C.C. Coolen. Finite connectivity attractor neural networks. Journal of Physics
A, 36(9617), 2003.
[14] E. Agliari, A. Annibale, A. Barra, A.C.C. Coolen, and D. Tantari. Immune networks: multitasking
capabilities near saturation. Journal of Physics A , 46:415003, 2013.
[15] E. Agliari, D. Migliozzi, and D. Tantari. Non-convex multi-species Hopfield models. Journal of
Statistical Physics , 172:1247–1269, 2018.
[16] P. Baldi and S. S. Venkatesh. Number of stable points for spin-glasses and neural networks of higher
orders. Physical Review Letters , 58, 1987.
[17] D. Krotov and J.J. Hopfield. Dense associative memory for pattern recognition. Advances in Neural
Information Processing Systems , pages 1180–1188, 2016.
[18] E. Agliari, F. Alemanno, A. Barra, and A. Fachechi. Generalized Guerra’s interpolation schemes for
dense associative neural networks. Neural Networks , 128:254–267, 2020.
[19] D.O. Hebb. The Organization of Behavior: A Neuropsychological Theory. New York, NY: John Wiley
& Sons, 1949.
[20] D.J. Amit. Modeling brain function: The world of attractor neural networks . Cambridge university
press, 1989.
[21] J.J. Hopfield, D.I. Feinstein, and R.G. Palmer. Unlearning has a stabilizing effect in collective
memories. Nature Letters , 304:280158, 1983.
[22] I. Kanter and H. Sompolinsky. Associative recall of memory without errors. Physical Review A ,
35(1):380, 1987.
[23] P. Del Giudice, S. Franz, and M.A. Virasoro. Perceptron beyond the limit of capacity. Journal of
Physics France , 50:121–134, 1989.
[24] S. Franz, D.J. Amit, and M. A. Virasoro. Prosopagnosia in high capacity neural networks storing
uncorrelated classes. Journal of Physics France , 51:387–408, 1990.
[25] V. Dotsenko, N. D. Yarunin, and E. A. Dorotheyev. Statistical mechanics of Hopfield-like neural
networks with modified interactions. Journal of Physics A , 24:2419, 1991.
[26] A. Y. Plakhov, S. A. Semenov, and I. B. Shuvalova. Convergent unlearning algorithm for the Hopfield
neural network. IEE Computation Society Press , 2(95):30, 1995.
[27] A. Fachechi, E. Agliari, and A. Barra. Dreaming neural networks: Forgetting spurious memories and
reinforcing pure ones. Neural Networks , 112:24–40, 2019.
[28] E. Marinari. Forgetting memories and their attractiveness. Neural Computation , 31(3):503–516, 2019.
[29] M. Benedetti, E. Ventura, E. Marinari, G. Ruocco, and F. Zamponi. Supervised perceptron learning vs
unsupervised Hebbian unlearning: Approaching optimal memory retrieval in Hopfield-like networks.
Journal of Chemical Physics , 156:104107, 2022.
– 26 –

--- PAGE 28 ---
[30] L. Serricchio, C. Chilin, D. Bocchi, R. Marino, M. Negri, C. Cammarota, and F. Ricci-Tersenghi.
Daydreaming hopfield networks and their surprising effectiveness on correlated data. In Associative
Memory & Hopfield Networks in 2023 , 2023.
[31] J.F. Fontanari. Generalization in a Hopfield network. Journal of Physics France , 51:2421–2430, 1990.
[32] E. Agliari, F. Alemanno, A. Barra, and G. De Marzo. The emergence of a concept in shallow neural
networks. Neural Networks , 148:232–253, 2022.
[33] M. Aquaro, F. Alemanno, I. Kanter, A. Barra, and E. Agliari. Supervised Hebbian learning.
Europhysics Letters - Perspective , 141:11001, 2023.
[34] M. Negri, C. Lauditi, G. Perugini, C. Lucibello, and E. Malatesta. Storage and learning phase
transitions in the random-features hopfield model. Physical Review Letters , 131(25):257301, 2023.
[35] A. Barra, A. Bernacchia, E. Santucci, and P. Contucci. On the equivalence of Hopfield networks and
Boltzmann Machines. Neural Networks , 34:1–9, 2012.
[36] S. Cocco, R. Monasson, and V. Sessak. High-dimensional inference with the generalized Hopfield model:
principal component analysis and corrections. Physical Review E , 83:051123, 2011.
[37] M. M´ ezard. Mean-field message-passing equations in the Hopfield model and its generalizations.
Physical Review E , 95, 2017.
[38] F. E. Leonelli, E. Agliari, L. Albanese, and A. Barra. On the effective initialisation for restricted
Boltzmann machines via duality with Hopfield model. Neural Networks , 143:314–326, 2021.
[39] A. Fachechi, A. Barra, E. Agliari, and F. Alemanno. Outperforming RBM feature-extraction
capabilities by “dreaming” mechanism. IEEE Transactions on Neural Networks and Learning Systems ,
pages 1–10, 6 2022.
[40] E. Agliari and C. Marullo. Boltzmann machines as generalized hopfield networks: a review on recent
results and outlooks. Entropy , 23(1):34, 2021.
[41] F. Camilli, D. Tieplova, and J. Barbier. Fundamental limits of overparametrized shallow neural
networks for supervised learning. arXiv:2307.05635 , 2023.
[42] J. Barbier, F. Camilli, M. Mondelli, and M. Saenz. Bayes-optimal limits in structured PCA, and how to
reach them. arXiv:2210.01237 , 2022.
[43] E. Agliari, M. Aquaro, A.C.C. Coolen, and A. Fachechi. A replica approach to RBM in generative
mode. in preparation , 2023.
[44] N. E. Zamri, S. A. Azhar, M. A. Mansor, A. Alway, and M. S. M. Kasihmuddin. Weighted random k
satisfiability for k= 1, 2 (r2sat) in discrete hopfield neural network. Applied Soft Computing ,
126:109312, 2022.
[45] E. Ventura, S. Cocco, R. Monasson, and F. Zamponi. Unlearning regularization for Boltzmann
Machines. https://arxiv.org/pdf/2311.09418.pdf , 16:1065–1095, 2023.
[46] N. E. Zamri, M. A. Mansor, M. S. M. Kasihmuddin, S. S. Sidik, A. Alway, N. A. Romli, Y. Guo, and
S. Z. M. Jamaludin. A modified reverse-based analysis logic mining model with weighted random 2
satisfiability logic in discrete hopfield neural network and multi-objective training of modified niched
genetic algorithm. Expert Systems with Applications , 240:122307, 2024.
[47] E. Agliari, F. Alemanno, A. Barra, and A. Fachechi. Dreaming neural networks: rigorous results.
Journal of Statistical Mechanics , page 083503, 2019.
[48] A.C.C. Coolen, R. K¨ uhn, and P. Sollich. Theory of neural information processing systems . Oxford
University Press, 2005.
– 27 –

--- PAGE 29 ---
[49] E. Gardner. The space of interactions in neural network models. Journal of Physics A , 21(1):257, 1988.
[50] E. Gardner and B. Derrida. Three unfinished works on the optimal storage capacity of networks.
Journal of Physics A , 22(12):1983, 1989.
[51] L. Personnaz, I. Guyon, and G. Dreyfus. Information storage and retrieval in spin-glass like neural
networks. Journal of Physics Letters , 46:359–365, 1985.
[52] A.E. Hoerl. Application of ridge analysis to regression problems. Chemical Engineering Progress ,
58:54–59, 1962.
[53] A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems.
Technometrics , 12(1):55–67, 1970.
[54] A.N. Tikhonov and V.I.A. Arsenin. Solutions of Ill-posed Problems . Halsted Press book. Winston, 1977.
[55] V.N. Vapnik. Statistical Learning Theory . A Wiley-Interscience publication. Wiley, 1998.
[56] B. Sch¨ olkopf and A. J. Smola. Learning with kernels: support vector machines, regularization,
optimization, and beyond . MIT press, 2002.
[57] V. Vovk. Kernel ridge regression. In Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik ,
pages 105–116. Springer, 2013.
[58] G. Meanti, L. Carratino, E. De Vito, and L. Rosasco. Efficient hyperparameter tuning for large scale
kernel ridge regression. In International Conference on Artificial Intelligence and Statistics , pages
6554–6572. PMLR, 2022.
[59] G. S. Alberti, E. De Vito, M. Lassas, L. Ratti, and M. Santacesaria. Learning the optimal tikhonov
regularizer for inverse problems. Advances in Neural Information Processing Systems , 34:25205–25216,
2021.
[60] D. Wu and J. Xu. On the optimal weighted l2regularization in overparameterized linear regression.
Advances in Neural Information Processing Systems , 33:10112–10123, 2020.
[61] T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least
squares interpolation. Annals of statistics , 50(2):949, 2022.
[62] P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression.
Proceedings of the National Academy of Sciences , 117(48):30063–30070, 2020.
[63] F. Crick and G. Mitchison. The function of dream sleep. Nature , 304(5922):111–114, 1983.
[64] E. Agliari, F. Alemanno, M. Aquaro, A. Barra, F. Durante, and I. Kanter. Hebbian dreaming for small
datasets. Neural Networks , page 106174, 2024.
[65] T. O. Kohonen. Self-organization and associative memory. Berlin: Springer, 1984.
[66] L. Deng. The mnist database of handwritten digit images for machine learning research [best of the
web]. IEEE signal processing magazine , 29(6):141–142, 2012.
[67] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
[68] N. Otsu. A threshold selection method from gray-level histograms. IEEE transactions on systems, man,
and cybernetics , 9(1):62–66, 1979.
[69] T.O. Kohonen and M. Ruohonen. Representation of Associated Data by Matrix Operators. IEEE
Transactions on Computers , C-22(7):701–702, 1973.
[70] S. B¨ os. Statistical mechanics approach to early stopping and weight decay. Physical Review E ,
58(1):833, 1998.
– 28 –

--- PAGE 30 ---
[71] E. Agliari, F. Alemanno, A. Barra, and G. De Marzo. The emergence of a concept in shallow neural
networks. Neural Networks , 148:232–253, 2022.
[72] G.A. Christos. Investigation of the crick-mitchison reverse-learning dream sleep hypothesis in a
dynamical setting. Neural Networks , 9(3):427–434, 1996.
[73] A. Yadav, H. Shokeen, and J. Yadav. Disjoint set union for trees. In 2021 12th International Conference
on Computing Communication and Networking Technologies (ICCCNT) , pages 1–6. IEEE, 2021.
[74] C. Baldassi, C. Lauditi, E. Malatesta, G. Perugini, and R. Zecchina. Unveiling the structure of wide flat
minima in neural networks. Physical Review Letters , 127:278301, 2021.
[75] C. Baldassi, F. Pittorino, and R. Zecchina. Shaping the learning landscape in neural networks around
wide flat minima. Proceedings of the National Academy of Sciences , 117(1):161–170, 2020.
– 29 –
