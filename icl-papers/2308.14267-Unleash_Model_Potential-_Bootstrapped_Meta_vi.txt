# 2308.14267.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl-papers/2308.14267.pdf
# Kích thước file: 782531 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Giải phóng tiềm năng mô hình: Học tự giám sát Meta có Bootstrap
Jingyao Wang, Zeen Song, Wenwen Qiang*, Changwen Zheng
Viện Phần mềm Viện Hàn lâm Khoa học Trung Quốc
{wangjingyao2023, songzeen, qiangwenwen, changwen}@iscas.ac.cn

Tóm tắt
Mục tiêu dài hạn của học máy là học các biểu diễn thị giác tổng quát từ một lượng nhỏ dữ liệu mà không cần giám sát, mô phỏng ba lợi thế của nhận thức con người: i) không cần nhãn, ii) mạnh mẽ với dữ liệu khan hiếm, và iii) học từ kinh nghiệm. Học tự giám sát và meta-learning là hai kỹ thuật đầy hứa hẹn để đạt được mục tiêu này, nhưng cả hai chỉ bắt giữ được một phần các lợi thế và không thể giải quyết tất cả các vấn đề. Học tự giám sát gặp khó khăn trong việc vượt qua nhược điểm của dữ liệu khan hiếm, đồng thời bỏ qua kiến thức tiền nghiệm có thể tạo thuận lợi cho việc học và tổng quát hóa. Meta-learning dựa vào thông tin có giám sát và gặp phải tắc nghẽn do việc học không đủ. Để giải quyết những vấn đề này, chúng tôi đề xuất một khung Học tự giám sát Meta có Bootstrap (BMSSL) mới nhằm mô phỏng quá trình học của con người. Trước tiên, chúng tôi phân tích mối quan hệ chặt chẽ giữa meta-learning và học tự giám sát. Dựa trên hiểu biết này, chúng tôi tái cấu trúc các nhiệm vụ để tận dụng điểm mạnh của cả hai mô hình, đạt được lợi thế i và ii. Hơn nữa, chúng tôi sử dụng khung tối ưu hóa hai cấp độ luân phiên giữa việc giải quyết các nhiệm vụ cụ thể với khả năng đã học (cấp độ thứ nhất) và cải thiện khả năng này (cấp độ thứ hai), đạt được lợi thế iii. Để khai thác đầy đủ sức mạnh của nó, chúng tôi giới thiệu mục tiêu bootstrap dựa trên meta-gradient để làm cho mô hình trở thành giáo viên của chính nó. Chúng tôi xác nhận hiệu quả của phương pháp với nghiên cứu lý thuyết và thực nghiệm toàn diện.

1 Giới thiệu
Con người có thể hiểu thế giới với ba lợi thế [35,8]: i) không cần thông tin giám sát; ii) chỉ cần một số lượng nhỏ mẫu để nhận biết nhiệm vụ phân loại; và iii) học dựa trên kiến thức tiền nghiệm hiện có về thế giới. Tương ứng, mục tiêu cuối cùng của học máy là tận dụng kiến thức tiền nghiệm và học các biểu diễn có thể chuyển giao giữa các nhiệm vụ khác nhau mà không cần bất kỳ giám sát nào, ngay cả khi dữ liệu khan hiếm.

Học tự giám sát (SSL) là một phương pháp đầy hứa hẹn để đạt được mục tiêu này, vì nó có thể học các biểu diễn tổng quát mà không cần giám sát và tổng quát hóa cho các nhiệm vụ downstream [28,44,46,4]. SSL áp dụng các tăng cường dữ liệu khác nhau [45,56] để tạo ra các góc nhìn khác nhau của cùng một hình ảnh và khuyến khích chúng có các embedding tương tự trong khi khác biệt với các góc nhìn thu được từ các hình ảnh khác. SSL được coi là một xấp xỉ gần với việc học của con người trong học máy [1,33,21]. Tuy nhiên, chúng tôi cho rằng nó chỉ bắt giữ một phần lợi thế thứ nhất của việc học con người và không thể giải quyết hai lợi thế còn lại. Cụ thể, chúng tôi chỉ ra rằng tăng cường dữ liệu không thể hoàn toàn bù đắp cho việc thiếu đa dạng dữ liệu và thậm chí có thể gây hại cho hiệu suất khi sử dụng quá mức (xem Hình 1). Hơn nữa, chúng tôi chỉ ra rằng SSL dựa vào một tiền nghiệm cố định dựa trên dữ liệu đơn lẻ, ví dụ như phân phối dữ liệu cần thỏa mãn phân phối đều, có thể gây ra thiên vị khi dữ liệu khan hiếm và hạn chế khả năng thích ứng với các tình huống mới. Do đó, SSL vẫn phải đối mặt với những thách thức đáng kể trong việc vượt qua rào cản dữ liệu thấp và kết hợp kiến thức tiền nghiệm linh hoạt như con người.

Preprint. Đang xem xét.arXiv:2308.14267v1 [cs.LG] 28 Aug 2023

--- TRANG 2 ---

Độ chính xác Top-1 ImageNet (%)80
78
76
74
72
70
Lượng dữ liệu (%)100 70 80 90

Hình 1: Độ chính xác với đánh giá tuyến tính cho các quy mô dữ liệu và tăng cường khác nhau. "SimCLR-Nx" có nghĩa là nhiều tăng cường dữ liệu được sử dụng ngẫu nhiên để mở rộng dữ liệu lên N lần.

…
…
…Anchor
…
…Positive
Negative
NegativeCùng lớp
Lớp khác
Một batch gốc của Học tự giám sát Nhiệm vụ Meta-learning

Hình 2: Tái cấu trúc dữ liệu SSL thành các nhiệm vụ meta-learning. Các góc nhìn được tăng cường từ cùng một "mèo" với Anchor là positive (cùng lớp), trong khi các góc nhìn của các mẫu khác ("xe hơi" và "lá") là negative (lớp khác).

Meta-learning là một phương pháp đầy hứa hẹn khác cho mục tiêu này, nhằm vượt qua rào cản dữ liệu thấp bằng cách học để nhanh chóng thích ứng với các nhiệm vụ mới với dữ liệu hạn chế [20,10,18,48,51]. Meta-learning mô phỏng việc học con người bằng cách sử dụng cấu trúc vòng lặp kép: vòng lặp trong tối ưu hóa mô hình cụ thể cho nhiệm vụ dựa trên khả năng học hiện tại, và vòng lặp ngoài cập nhật khả năng này dựa trên phản hồi từ nhiều nhiệm vụ. Tuy nhiên, meta-learning vẫn dựa vào giám sát để cập nhật khả năng, điều này vi phạm lợi thế thứ nhất của việc học con người. Một số công trình gần đây cố gắng sử dụng nhãn giả tự giám sát cho meta-learning để tránh giám sát [2,13,5], nhưng chiến lược này tốn kém về mặt tính toán và có thể không tổng quát hóa tốt cho các nhiệm vụ khác nhau. Hơn nữa, meta-learning gặp phải hai hạn chế: i) vòng lặp trong: mô hình cụ thể cho nhiệm vụ f chỉ được cập nhật trong L bước, có thể không bắt giữ được trạng thái tối ưu cho nhiệm vụ; ii) vòng lặp ngoài: việc cập nhật khả năng học dựa trên cùng một mục tiêu và hình học với f, có thể lan truyền các lỗi của f và làm tổn hại kết quả cuối cùng. Do đó, meta-learning vẫn có khoảng cách lớn với việc đạt được khả năng học giống con người.

Trong công trình này, chúng tôi tiết lộ một kết nối đáng ngạc nhiên giữa học tự giám sát và meta-learning: chúng có thể được thống nhất bằng cách xem batch các lớp được lấy mẫu trong meta-learning như các góc nhìn tăng cường của các mẫu được tạo ra trong học tự giám sát. Dựa trên hiểu biết này, chúng tôi đề xuất một khung Học tự giám sát Meta có Bootstrap (BMSSL) mới nhằm mô phỏng ba lợi thế của việc học con người. Để đạt được lợi thế thứ nhất và thứ hai, chúng tôi trình bày một phương pháp đơn giản và tổng quát để tái cấu trúc các nhiệm vụ có thể tận dụng cả học tự giám sát và meta-learning, vượt qua những hạn chế của nhãn và khối lượng dữ liệu. Hình 2 minh họa ngắn gọn ý tưởng của chúng tôi. Để đạt được lợi thế thứ ba, chúng tôi sử dụng cấu trúc meta-learning hai cấp độ với tối ưu hóa dựa trên gradient để cập nhật các tham số ban đầu dựa trên phản hồi từ nhiều nhiệm vụ. Để giải quyết các tắc nghẽn hiện tại của meta-learning, chúng tôi tiếp tục giới thiệu mục tiêu bootstrap dựa trên meta-gradient để cho phép mô hình học từ chính nó. Đóng góp của chúng tôi như sau:

• Chúng tôi thảo luận về mối quan hệ chặt chẽ giữa meta-learning và học tự giám sát, và xác minh thực nghiệm tính khả thi của các ý tưởng quy nạp.
• Chúng tôi đề xuất một khung Học tự giám sát Meta có Bootstrap (BMSSL) mới để mô phỏng quá trình học của con người.
• Chúng tôi tiến hành phân tích lý thuyết và nghiên cứu thực nghiệm về khung đề xuất để xác minh hiệu quả của nó.

2 Công trình liên quan
Học tự giám sát. Học tự giám sát (SSL) cho phép học các biểu diễn thị giác tổng quát bằng cách áp đặt một ràng buộc bổ sung giữa các góc nhìn khác nhau của dữ liệu đầu vào thô mà không cần truy cập bất kỳ dữ liệu được chú thích nào [12,32,3,16,50]. Gần đây, các phương pháp chủ đạo dựa vào việc xây dựng các điểm nhìn tích cực và tiêu cực cho các ví dụ thông qua tăng cường [42,21,42] để học các nhiệm vụ pretext và tổng quát hóa cho các nhiệm vụ downstream: các mẫu tích cực thường là các góc nhìn tăng cường của cùng một thể hiện tham chiếu, trong khi các ví dụ tiêu cực được định nghĩa là bất kỳ góc nhìn nào từ một thể hiện khác. Quá trình học dựa trên phương pháp NCE tiên phong [38], sử dụng mất mát đối lập [63,64] để thực thi sự phân biệt giữa các điểm nhìn tích cực và tiêu cực của mỗi thể hiện, dẫn đến việc học các ngữ nghĩa hữu ích. Mô hình đã học cung cấp các biểu diễn phân biệt thị giác được phân tán đều trong không gian đặc trưng. Tuy nhiên, các phương pháp này khó tổng quát hóa khi dữ liệu khan hiếm, và chỉ học dựa trên một giai đoạn dữ liệu thay vì nhanh chóng thích ứng với các nhiệm vụ mới từ kinh nghiệm với nhiều quá trình học như con người.

Meta-learning. Meta-learning nhằm học một mô hình có thể nhanh chóng thích ứng với các nhiệm vụ mới với dữ liệu hạn chế và tổng quát hóa cho các ví dụ chưa thấy. Các phương pháp meta-learning có thể được chia thành hai loại: i) học khởi tạo tối ưu để thích ứng với các nhiệm vụ mới một cách nhanh chóng [14,37,23]; ii) học một không gian embedding được chia sẻ và amortizing inference [52,47,49,62]. Gần đây, meta-learning đã đạt được hiệu suất vượt trội trong các ứng dụng khác nhau, chẳng hạn như phân loại few-shot [58,41], học tăng cường [40,60], và tối ưu hóa siêu tham số [59]. Các mô hình này thiết kế khéo léo các nhiệm vụ dựa vào một vài mẫu được gán nhãn để học một đơn vị biểu diễn thị giác tổng quát, nhưng thường không thể cung cấp ước tính độ không chắc chắn đáng tin cậy khi chỉ có một vài nhiệm vụ meta-training hoặc khi có giám sát được cung cấp [34,20]. Một số phương pháp áp dụng cách tiếp cận vá víu để giải quyết vấn đề này: sử dụng mô hình không giám sát để xây dựng nhãn giả, sau đó sử dụng chúng làm giám sát cho meta-learning. Tuy nhiên, mặc dù phương pháp này có thể học các biểu diễn từ dữ liệu hạn chế mà không cần tiền nghiệm của con người, nó dẫn đến việc sử dụng tài nguyên tính toán khổng lồ trong khi khó đảm bảo độ chính xác [39]. Đồng thời, do những hạn chế về cận thị và độ cong của meta-learning [15], nó vẫn không thể đáp ứng kỳ vọng về việc mô phỏng việc học con người.

3 Mối quan hệ giữa Meta-Learning và Học tự giám sát
Trong phần này, chúng tôi nghiên cứu mối quan hệ chặt chẽ giữa học tự giám sát và meta-learning. Trước tiên, chúng tôi xem xét các mô hình học của cả hai khung và làm nổi bật những điểm tương đồng từ ba góc độ. Dựa trên những hiểu biết này, chúng tôi đề xuất một phương pháp dựa trên meta-learning đơn giản để tái cấu trúc các nhiệm vụ tự giám sát và đánh giá hiệu quả của nó một cách thực nghiệm.

Các quy trình huấn luyện của meta-learning và học tự giám sát được thể hiện trong Hình 3. Đối với học tự giám sát: i) lấy mẫu {x}N từ phân phối của không gian dữ liệu thô X; ii) áp dụng nhiều tăng cường dữ liệu cho {x}N, ví dụ như thay đổi tỷ lệ ngẫu nhiên, xoay và cắt, thu được A = {ax1, ..., axN}M; iii) học một biểu diễn thị giác tổng quát dựa trên việc tối thiểu hóa mất mát học tự giám sát, ví dụ như mất mát đối lập; iv) sử dụng các mô hình được pre-train để trích xuất đặc trưng và xác minh chúng trong các nhiệm vụ downstream. Đối với meta-learning: i) lấy mẫu xN từ phân phối của không gian dữ liệu thô X; ii) xây dựng các nhiệm vụ {T}K bằng cách phân vùng xN; iii) tìm các tham số khởi tạo tốt nhất dựa trên mất mát meta (vòng lặp ngoài): tối thiểu hóa mất mát gradient tích lũy cho tất cả các mô hình cụ thể cho nhiệm vụ (vòng lặp trong); iv) sử dụng mô hình đã huấn luyện để thích ứng nhanh với các nhiệm vụ mới. Về mặt khái niệm, chúng tôi thấy rằng quá trình huấn luyện của học tự giám sát tương tự như meta-learning và bao gồm một số điểm tương đồng:

• Cả hai đều nhằm học các biểu diễn có thể tổng quát hóa và nhanh chóng thích ứng với các nhiệm vụ mới: học tự giám sát nhằm phân biệt giữa các hình ảnh chưa thấy; meta learning nhằm phân biệt giữa các nhiệm vụ chưa thấy.
• Cả hai đều học một lượng thông tin cố định có thể được chuyển giao cho các nhiệm vụ mới: học tự giám sát tận dụng sự tương đồng và khác biệt giữa nhiều góc nhìn của các mẫu; meta-learning tận dụng sự tương đồng của các thể hiện trong mỗi nhiệm vụ.
• Cả hai đều sử dụng batch làm đơn vị xử lý dữ liệu: học tự giám sát xem tất cả các góc nhìn được tạo ra bởi một tăng cường duy nhất như một batch; meta learning xem mỗi nhiệm vụ bao gồm K nhiệm vụ n-way-m-shot như một batch.

Được truyền cảm hứng từ điều này, chúng tôi đề xuất một mô hình tổng quát để thống nhất học tự giám sát và meta-learning. Ý tưởng là biến đổi học tự giám sát thành một phân phối nhiệm vụ phù hợp với tối ưu hóa meta-learning và học từ nó. Mô hình bao gồm các bước sau: i) lấy mẫu ngẫu nhiên một batch gồm N hình ảnh đầu vào {xi}N ∈ X từ pool ứng viên; ii) chia X thành K khối, mỗi khối chứa N/K hình ảnh; iii) áp dụng nhiều tăng cường dữ liệu trên x ∈ X của mỗi khối, thu được a ∈ A; iv) tạo một bài toán phân loại N-way cho mỗi khối: tất cả dữ liệu được tạo ra từ cùng một xi được coi là một danh mục, với z = {a, y} ∈ Z là dữ liệu và y ∈ Y là nhãn giả; v) tích hợp các nhiệm vụ tương ứng với K khối trong một batch, thu được phân phối nhiệm vụ p(T); vi) cập nhật mô hình cụ thể cho nhiệm vụ w cho nhiệm vụ Ti dựa trên các tham số khởi tạo meta θ ∈ ζ bằng cách tính toán mất mát l(·) (vòng lặp trong); vii) sử dụng hàm mục tiêu meta L(·, θ) để cập nhật θ, được tính toán thông qua arg min 1/K ∑(i=1 to K) l(wi) (vòng lặp ngoài). Trước khi mở rộng thêm mô hình này, chúng tôi nêu các giả định cần thiết cho mô hình này. Chi tiết phân tích lý thuyết có thể tìm thấy trong Phần 4.3 và Phụ lục A.

Giả định 3.1 Chúng tôi giả định rằng Z là một không gian Polish (tức là hoàn chỉnh, có thể tách biệt và có thể metric hóa), và với bất kỳ i nào, p(Ti) là một phân phối xác suất không nguyên tử trên (Z, L), trong đó L(·, θ) là một σ-algebra Borel trên Z.

Giả định 3.2 Tham số sự thật cơ bản θ* độc lập với X và thỏa mãn Cov[θ*] = (R²/d)Id, trong đó R là một hằng số và d là chiều của tham số mô hình.

Giả định 3.3 Với bất kỳ z ∈ Z, hàm L(·, θ) có thể vi phân liên tục hai lần và thỏa mãn các thuộc tính sau với bất kỳ z ∈ Z, wi, wj ∈ Rᵈ:
• Hàm L(·, θ) bị chặn K trên W với chuẩn gradient bị chặn đều bởi G, tức là ∥▽l(z, wi)∥ ≤ G;
• Hàm L(·, θ) là L-trơn trên Rᵈ, tức là ∥▽l(z, wi) - ▽l(z, wj)∥ - L(wi - wj) ≤ 0;
• Hàm L(·, θ) là μ-lồi mạnh, tức là ∥▽l(z, wi) - ▽l(z, wj)∥ - μ(wi - wj) ≥ 0.

Để đánh giá tính khả thi của mô hình tổng quát được đề xuất, chúng tôi so sánh hiệu suất của các khung tự giám sát điển hình, SimCLR [9], Barlow Twins [57], và MoCo [17], trong việc học các biểu diễn dưới các cài đặt khác nhau. Chúng tôi đo độ chính xác top-1 trên các bộ dữ liệu ImageNet1K [53] và CIFAR-10 [29] mà không áp đặt bất kỳ hạn chế dữ liệu nào, nơi các khung tự giám sát thuần túy có thể hoạt động tốt. Chúng tôi mong đợi rằng các biến thể của mô hình dựa trên mô hình được đề xuất có thể hưởng lợi từ nó hoặc ít nhất không suy giảm hiệu suất trong những môi trường như vậy. Kết quả trong Bảng 1 và 2 xác nhận kỳ vọng này. Chúng tôi quan sát thấy rằng các biểu diễn học tự giám sát dựa trên mô hình được đề xuất có thể đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với các khung gốc trên cả hai bộ dữ liệu. Hơn nữa, vì sự cải thiện bị hạn chế trong cài đặt này, chúng tôi tiến hành một đánh giá toàn diện hơn từ nhiều góc độ trong Phần 5.

--- TRANG 3 ---

--- TRANG 4 ---

Hình ảnh
x1
x2
x3
p(·|·)
Nhãn giả
0
1
2
Tăng cường
Pool ứng viên
X Y
(X,Y)
Biểu diễn
Chiếu
Mất mát đối lập
H P

Hình ảnh
Pool ứng viên
X
x1
x2
x3
Nhãn
Lớp
"Mèo" c1
"Xe hơi"
"Lá"
Y Z=(X,Y)
c2
c3
Tham số khởi tạo θ
+
+
+
=
=
=
Z
Biểu diễn
Chiếu
H P
Tham số khởi tạo θ
Mất mát Meta
...

(a)
(b)

Hình 3: (a) Quy trình huấn luyện của học đối lập. Các góc nhìn tăng cường từ hình ảnh X được tạo ra bằng cách áp dụng các phép biến đổi ngẫu nhiên cho cùng một batch đầu vào, thu được nhãn giả Y tương ứng. H và P là các vector thông qua một backbone cho biểu diễn và một projector được học thông qua các nhiệm vụ dự đoán đối lập. (b) Quy trình huấn luyện của meta-learning. Đầu vào của nó là dữ liệu được gán nhãn Z, và học một mô hình cụ thể cho nhiệm vụ dựa trên tham số khởi tạo có thể học θ. Việc xử lý tiếp theo của mỗi nhiệm vụ tương tự như học tự giám sát, nhưng θ được cập nhật dựa trên gradient tích lũy của tất cả các nhiệm vụ.

Bảng 1: Độ chính xác(%) trên ImageNet1K trên các baseline SSL ("-o" có nghĩa là sử dụng cài đặt dựa trên meta-learning, trong khi "-x" có nghĩa là không).

Phương pháp | Backbone | Top-1 Acc
SimCLR-x | ResNet-50 | 64.561
SimCLR-o | ResNet-50 | 65.156
Barlow Twins-x | ResNet-50 | 66.561
Barlow Twins-o | ResNet-50 | 66.952
Moco-x | ResNet-50 | 59.382
Moco-o | ResNet-50 | 59.156

Bảng 2: Độ chính xác(%) trên CIFAR-10 trên các baseline SSL ("-o" có nghĩa là sử dụng cài đặt dựa trên meta-learning, trong khi "-x" có nghĩa là không).

Phương pháp | Backbone | Top-1 Acc
SimCLR-x | ResNet-18 | 89.416
SimCLR-o | ResNet-18 | 91.516
Barlow Twins-x | ResNet-18 | 92.513
Barlow Twins-o | ResNet-18 | 92.789
Moco-x | ResNet-18 | 82.465
Moco-o | ResNet-18 | 83.165

4 Khung Học tự giám sát Meta có Bootstrap

Trong phần này, chúng tôi giới thiệu khung Học tự giám sát Meta có Bootstrap (BMSSL) được đề xuất, được truyền cảm hứng từ mô hình nhận thức con người. Ý tưởng chính của chúng tôi bao gồm: i) xây dựng các nhiệm vụ phân loại few-shot bằng cách sử dụng hàng đợi đa góc nhìn được tạo ra bởi tăng cường dữ liệu trong học tự giám sát (Phần 4.1); ii) sử dụng cấu trúc tối ưu hóa hai cấp độ dựa trên gradient để học biểu diễn thị giác phổ quát (Phần 4.2): vòng lặp trong sử dụng học đối lập để học nhiệm vụ cụ thể (tối ưu hóa thứ nhất); vòng lặp ngoài tìm các tham số khởi tạo tối ưu cho vòng lặp trong với meta gradient có bootstrap thông qua việc tối thiểu hóa khoảng cách đến mục tiêu tự-bootstrap, cho phép mô hình trở thành giáo viên của chính nó (tối ưu hóa thứ hai). Mã giả của khung của chúng tôi được cung cấp trong Phụ lục A. Đáng chú ý là BMSSL được thiết kế dựa trên quan sát trong Phần 3, do đó áp dụng ký hiệu và cài đặt nhất quán.

4.1 Xây dựng nhiệm vụ trực tuyến

Bây giờ chúng tôi mô tả cách tái cấu trúc nhiệm vụ tự giám sát dựa trên meta-learning cho BMSSL. Trước tiên, chúng tôi chọn ngẫu nhiên N dữ liệu không gán nhãn x ∈ X từ pool ứng viên của dữ liệu huấn luyện Dpool để tạo thành D = {xi}N. Thứ hai, chúng tôi áp dụng tăng cường dữ liệu cho D, thu được ˆD = {{ax1}M, {ax2}M, ..., {axN}M}, trong đó {axj}M là kết quả của việc áp dụng M lần tăng cường trên xj. Tiếp theo, chúng tôi chia ˆD thành K khối ˆD = {ˆD1, ..., ˆDK}, và mỗi khối chứa (N/K) × M hình ảnh. Đối với con người, các hình ảnh khác nhau của cùng một đối tượng dễ dàng được phân loại vì sự tương đồng cao của chúng. Tương tự, vì có sự tương đồng thực thể đáng kể trong dữ liệu được tăng cường từ cùng một hình ảnh, chúng tôi gán cùng một nhãn yj ∈ Y cho dữ liệu tăng cường của cùng một hình ảnh xj để mô phỏng tư duy phân loại của con người. Chúng tôi sử dụng z = {a, y} ∈ Z để biểu diễn dữ liệu được phân loại theo cách trên, trong đó y biểu diễn nhãn của mẫu tăng cường a của x, tức là bộ dữ liệu ˆD = {zi}N×M. Dựa trên phân tích trong Phần 3, mỗi khối trong ˆDi có thể được coi là một nhiệm vụ N/K-way Ti, trong đó ˆD = {Ti}K. Lấy ˆD1 = {zi}(N/K)×M làm ví dụ, chúng tôi chia ˆD1 thành hai phần cho support set ˆDs1 và query set ˆDq1, tương ứng chứa (N/K) × M1 và (N/K) × M2 hình ảnh (M1 + M2 = M). Phương pháp này biến đổi một batch dữ liệu không gán nhãn thành K nhiệm vụ phân loại N/K-way-M1-shot, thực hiện các lợi thế i và ii của việc học con người.

4.2 Học tự giám sát Meta có Bootstrap

Bây giờ chúng tôi mô tả cách sử dụng các nhiệm vụ few-shot được xây dựng cho meta-learning. Như con người, chúng ta học kiến thức cụ thể và xây dựng bản đồ tư duy thông qua hai cấp độ trừu tượng, cho phép chúng ta nhanh chóng nhận biết những thứ tương tự. Do đó, chúng tôi tập trung vào các phương pháp dựa trên gradient [14] sử dụng hai vòng lặp cấp độ để hạn chế các cập nhật cho người học cụ thể cho nhiệm vụ và meta-learner để có được các tham số khởi tạo tối ưu θ, tương tự như kinh nghiệm con người để thích ứng nhanh hơn với các nhiệm vụ mới.

Đối với vòng lặp trong (tối ưu hóa thứ nhất), mục tiêu của người học là học f(w) bằng cách tối thiểu hóa mục tiêu học [l(f(w); θ, D)] trên dữ liệu huấn luyện D, trong đó D biểu diễn một mini-batch của nguồn dữ liệu không gán nhãn, f biểu diễn một mạng nơ-ron bao gồm bộ trích xuất đặc trưng, projection head (học đối lập), và classifier (cross entropy). Chúng tôi ký hiệu tham số của f(w) là w. Dựa trên Phần 4.1, D có thể được mở rộng thành K nhiệm vụ N/K-way-M1-shot. Xem xét tác động của lỗi tạo nhãn, hàm mất mát cụ thể cho nhiệm vụ bao gồm mất mát cross-entropy lce(·) và mất mát đối lập lcl(·), được biểu diễn như:

l(f(w); θ, D) = lce(f(w); θ, D) + λlcl(f(w); θ; D)

s.t. lce(·) = -∑j=1^N ∑i=1^M yj log·axj_i
lcl(·) = -∑j=1^N,i=1^M log(∑r=1^M,r≠i exp(sim(*axj_i, *axj_r)/τ))/(∑r=1^M,r≠i exp(sim(*axj_i, *axj_r)) + ∑p=1^N,p≠j ∑o=1^M exp(sim(*axj_i, *axo_p)/τ))    (1)

trong đó λ mô tả tầm quan trọng của lcl(·), ·axj_i là đầu ra của classifier, và *axj_i là đầu ra của projection head. Cuối cùng, dựa trên tham số khởi tạo θ0 của w, chúng tôi huấn luyện một mô hình cụ thể cho nhiệm vụ với tốc độ học α và thu được trọng số w = θ0 - α▽wl(w; θ0, D).

Đối với vòng lặp ngoài (tối ưu hóa thứ hai), mục tiêu của meta-learner là học mô hình F, ví dụ như F(D) = f(w), có nghĩa là tạo ra mô hình cụ thể cho nhiệm vụ tối ưu f(w) cho bộ dữ liệu D. Trong học máy, f cần được thu được thông qua một chuỗi các cập nhật gradient descent dựa trên hàm mất mát l(·), bắt đầu từ một tham số khởi tạo, và rất khó đạt được kết quả tối ưu trong một bước duy nhất. Do đó, mục tiêu của meta-learner được biến đổi thành việc tìm một khởi tạo tối ưu có thể học θ ∈ ζ để cho f(w) nhanh chóng có được tham số tối ưu w ∈ W. Vì vậy, meta-learning xây dựng hàm mục tiêu của việc học F là minθ l(f(w*(θ)); θ, D), s.t., w*(θ) = arg minw l(f(w); θ, D). Chúng ta có thể thấy rằng meta-gradient tiêu chuẩn trước tiên tối ưu hóa điều kiện ràng buộc w*(θ) = arg minw l(f(w); θ, D) bằng cách thực hiện L bước cập nhật và sau đó đánh giá w*(θ) dưới l(f(w*(θ)); θ, D), do đó thu được quy tắc cập nhật của θ:

θ' = θ - β▽θl(f(w*(θ)); θ, D)    (2)

Tuy nhiên, mặc dù phương pháp này có thể giới thiệu kinh nghiệm tiền nghiệm θ như con người và nhanh chóng thích ứng với các nhiệm vụ mới, quá trình học của θ vẫn có những hạn chế: i) nó phụ thuộc cao vào f(w), trong khi tư duy con người không bị hạn chế; ii) nó dựa trên các cập nhật trong một số bước hạn chế L, trong khi việc học con người có thể mở rộng. Do đó, xem xét rằng quy nạp con người dựa trên sự tương đồng thực thể, chúng tôi chuyển đổi việc học kinh nghiệm này thành một metric trong mô hình: sử dụng mục tiêu bootstrap để di chuyển wL(θ) gần hơn với phiên bản L+δ của nó wL+δ(θ). Chúng tôi coi wL(θ) và wL+δ(θ) như hai phân phối đều rời rạc đối với các đơn vị cấu thành của chúng. Để mở rộng góc nhìn cập nhật θ, đối với vòng lặp ngoài, chúng tôi sử dụng divergence KL để đưa phân phối πw(L) thu được bởi wL(θ) bước gần hơn với phân phối π⃗w thu được bởi wL+δ(θ) đến trạng thái bootstrap wL(θ) mở rộng thành wL+δ(θ), do đó khuyến khích meta-learner đạt được các trạng thái tương lai trên quỹ đạo của nó nhanh hơn. Chúng ta có:

⃗θ = θ - β▽θDKL(π⃗w, πwL)    (3)

trong đó wL(θ) là cập nhật L bước dựa trên w*(θ) = arg minw l(f(w); θ, D), trong khi wL+δ(θ) là cập nhật L+δ bước.

Biện pháp này loại bỏ sự phụ thuộc của việc cập nhật ngoài θ vào mô hình cụ thể cho nhiệm vụ f, và làm cho πwL liên tục tiếp cận π⃗w chứa thông tin tương lai để đạt được hội tụ và vượt qua giới hạn của các cập nhật hạn chế. Mô hình này làm cho mô hình trở thành giáo viên của chính nó.

4.3 Phân tích lý thuyết

Bây giờ chúng tôi tiến hành phân tích lý thuyết về BMSSL để đảm bảo hiệu suất. Chúng tôi hoãn tất cả các chứng minh và phân tích thêm đến Phụ lục A.

Trước tiên, xem xét xây dựng nhiệm vụ được mô tả trong Phần 4.1, mục tiêu là xác định một biểu diễn cho phép chúng ta xấp xỉ nhiều "lựa chọn hợp lý" khác nhau bằng g. Nó có thể nhóm các góc nhìn tăng cường của các thực thể tương tự lại với nhau, tức là mọi g thỏa mãn giả định sau:

Giả định 4.1 (Bất biến góc nhìn xấp xỉ): Ước tính tốt nhất của nhãn y xấp xỉ bất biến với việc lựa chọn các góc nhìn tăng cường khác nhau a của cùng một x. Mỗi hàm mục tiêu g: A → Rⁿ thỏa mãn:

Ep+(a1,a2)[(g(a1) - g(a2))²] ≤ ε    (4)

trong đó p+(a1, a2) = ∑x p(a1|x)p(a2|x)p(x) khi cố định ε ∈ [0, ∞).

Sau đó chúng ta có thể hạn chế lỗi xấp xỉ g với một tập con nhỏ các eigenfunction bằng cách hạn chế mỗi hệ số theo đóng góp của nó vào tổng sự khác biệt cặp dương. Chúng tôi tập trung vào một lớp các predictor tuyến tính trên biểu diễn k-chiều r: A → Rⁿ, trong đó biểu diễn rd = {p1(a), p2(a), ..., pd(a)} chứa d eigenfunction của chuỗi Markov cặp dương với các giá trị riêng lớn nhất là lựa chọn tốt nhất và được sử dụng cho huấn luyện cụ thể nhiệm vụ (Phương trình 1).

Định lý 4.2 (Đảm bảo hiệu suất cụ thể nhiệm vụ) Cho Gε là các hàm thỏa mãn Giả định 4.1, và Gr = {a ↦ ĝν(a) = νᵀr(a)} là không gian con của các predictor tuyến tính tối đa hóa tính bất biến góc nhìn của predictor có chuẩn đơn vị ít bất biến nhất Grd với hiệu ứng điều hóa ngầm:

Grd = arg minG maxĝ∈G Ep+(a1,a2)[(ĝ(a1) - ĝ(a2))²], s.t. dim(G) = d, E[ĝ(a)²] = 1    (5)

nó được ngụ ý trong G, tối thiểu hóa lỗi xấp xỉ của hàm mục tiêu trường hợp xấu nhất f:

Grd = arg minG maxg∈Gε minĝ∈G Ep+(a1,a2)[(ĝ(a1) - ĝ(a2))²]    (6)

Định lý 4.2 nói rằng lớp hàm chúng tôi xây dựng (Phương trình 1) là lựa chọn tốt nhất cho xấp xỉ bình phương tối thiểu thỏa mãn Giả định 4.1. Tiếp theo, chúng tôi chuyển đến huấn luyện meta bootstrap trong vòng lặp ngoài, nơi chúng tôi sử dụng Giả định 3.1-3.3 được đề cập trong Phần 3.

Định lý 4.3 (Đảm bảo hiệu suất huấn luyện Meta Bootstrap) Cho w và θ được cho bởi các Phương trình được đề cập trong Phần 4.2, quá trình cập nhật thỏa mãn:

f(wL(⃗θ)) - f(wL(θ)) = β/α(DKL(⃗w, wL - α∇wf(wL)) - DKL(⃗w, wL)) + o(β(α + β))    (7)

Cho ⃗θ và θ' được cho bởi Phương trình (3) và (2) tương ứng, f(wL(⃗θ)) - f(wL(θ)) ≤ 0 khi (α, β) đủ nhỏ, trong khi f(wL(θ')) - f(wL(θ)) ≤ 0 khi β đủ nhỏ. Với trạng thái ⃗w được bootstrap từ wL với δ bước cung cấp phân phối tương lai (tốt hơn), quá trình cập nhật sẽ trở thành:

f(wL(⃗θ)) - f(wL(θ)) = -β/α DKL(⃗w, wL) + o(β(α + β)) < 0    (8)

Do đó, so với meta-learning tiêu chuẩn, BMSSL cho phép các mô hình đạt được kết quả tối ưu nhanh hơn trong khi đạt được hội tụ mà không sử dụng cập nhật gradient.

5 Nghiên cứu thực nghiệm

Trong phần này, chúng tôi tiến hành một số thí nghiệm để đánh giá và phân tích BMSSL, bao gồm học few-shot tự giám sát tiêu chuẩn (Phần 5.1), học few-shot tự giám sát cross-domain (Phần 5.2), và nghiên cứu ablation (Phần 5.3). Chi tiết về triển khai có sẵn tại Phụ lục B. Chúng tôi bỏ qua khoảng tin cậy trong phần này để rõ ràng, và kết quả đầy đủ với chúng được cung cấp trong Phụ lục E. Mục tiêu chính của chúng tôi là chứng minh hiệu quả của BMSSL bằng cách khám phá hai câu hỏi chính: i) BMSSL có thể được áp dụng thành công trong các tình huống phân loại few-shot tự giám sát và đạt được hiệu suất tổng quát hóa vượt trội không? ii) Bằng cách mô phỏng cách con người học, BMSSL có thể đạt được kết quả học mạnh mẽ hơn không?

5.1 Học few-shot tự giám sát tiêu chuẩn

Thiết lập. Chúng tôi đánh giá BMSSL trên ba benchmark few-shot tiêu chuẩn của meta-learning không giám sát: Omniglot [30], miniImageNet [52], và CIFAR-FS [6]. Theo [22], chúng tôi so sánh hiệu suất của BMSSL với các phương pháp meta-learning không giám sát [19,24,25,31,26,22], các phương pháp học tự giám sát [9,57,17,7], và các phương pháp meta-learning có giám sát [14,47,43]. Để khám phá hiệu quả của việc mô phỏng cách con người học, chúng tôi giới thiệu mô hình học tự giám sát meta tiêu chuẩn (MetaSSL) được đề cập trong Phần 4, cũng là cấu trúc hai lớp nhưng không được tối ưu hóa bởi mục tiêu bootstrap. Xem Phụ lục C và D để biết chi tiết về các benchmark và baseline.

Kết quả. Bảng 3 trình bày kết quả phân loại few-shot trên các nhiệm vụ (way, shot) khác nhau cho ba bộ dữ liệu benchmark được đề cập ở trên. Chúng tôi có ba quan sát sau: i) hiệu suất xuất sắc: BMSSL đạt được hiệu suất xuất sắc trên cả ba benchmark, vượt qua các mô hình SOTA meta-learning không giám sát trước đây. Ví dụ, chúng tôi có được mức tăng độ chính xác 3.763% trong bài kiểm tra 20-way-1-shot. ii) cải thiện tổng quát hóa: hiệu suất của nó có khả năng cạnh tranh ngay cả trong cài đặt không giám sát so với meta-learning có giám sát và các baseline tự giám sát. iii) hiệu quả của việc mô phỏng học con người: chúng tôi so sánh phương pháp của chúng tôi với MetaSSL tiêu chuẩn và đạt được mức tăng trung bình 4.045%.

--- TRANG 5 ---

--- TRANG 6 ---

--- TRANG 7 ---

Bảng 3: Độ chính xác (%) phân loại few-shot tiêu chuẩn trên các benchmark Omniglot, miniImageNet, và CIFAR-FS. Các giá trị trong bảng này là độ chính xác trung bình trên 2000 nhiệm vụ "(way, shot)" cho BMSSL và các baseline theo [22]. Các mục in đậm chỉ ra tốt nhất cho các nhiệm vụ không giám sát. Màu xanh chỉ ra BMSSL của chúng tôi trước và sau khi giới thiệu bootstrap được đề cập trong Phần 4.2.

Phương pháp | Omniglot | miniImageNet | CIFAR-FS
---|---|---|---
 | (5,1) | (5,5) | (20,1) | (5,1) | (5,5) | (20,1) | (5,1) | (5,5) | (20,1)
Train từ đầu | 50.29 | 72.82 | 26.20 | 24.20 | 38.84 | 16.29 | 31.12 | 44.89 | 20.32
**Meta-learning không giám sát**
CACTUs[19] | 65.29 | 86.25 | 49.54 | 39.32 | 53.54 | 31.99 | 40.02 | 58.16 | 35.88
UMTRA[24] | 83.32 | 94.23 | 75.84 | 39.23 | 51.78 | 30.27 | 41.61 | 60.55 | 37.10
LASIUM[25] | 82.38 | 95.11 | 70.23 | 42.12 | 54.98 | 34.26 | 45.33 | 62.65 | 38.40
Meta-SVEBM[26] | 87.07 | 94.13 | 73.33 | 44.74 | 58.38 | 39.71 | 47.24 | 63.10 | 40.10
Meta-GMVAE[31] | 90.89 | 96.05 | 81.51 | 42.28 | 56.97 | 39.83 | 47.45 | 63.20 | 41.55
PsCo[22] | 96.18 | 98.22 | 89.32 | 46.35 | 63.05 | 40.84 | 51.77 | 69.66 | 45.08
MetaSSL | 91.36 | 95.35 | 88.64 | 45.82 | 62.14 | 39.48 | 49.09 | 66.54 | 43.70
**BMSSL** | **96.02** | **99.56** | **91.41** | **49.98** | **64.59** | **45.28** | **52.20** | **69.64** | **49.84**
**Học tự giám sát**
SimCLR[9] | 90.83 | 97.67 | 81.67 | 42.32 | 51.10 | 36.36 | 49.44 | 60.02 | 39.29
MoCo[17] | 87.83 | 95.52 | 80.03 | 40.56 | 49.41 | 36.52 | 45.35 | 58.11 | 37.89
SwAV[7] | 91.28 | 97.21 | 82.02 | 44.39 | 54.91 | 37.13 | 49.39 | 62.20 | 40.19
**Meta-learning có giám sát**
MAML[14] | 93.22 | 97.53 | 82.36 | 45.84 | 63.25 | 36.77 | 48.25 | 58.00 | 39.52
ProtoNet[47] | 95.83 | 99.29 | 92.80 | 46.58 | 63.20 | 40.11 | 51.28 | 69.55 | 46.65
CNAPs[43] | 91.28 | 95.98 | 87.09 | 43.21 | 62.87 | 36.55 | 52.07 | 70.38 | 43.30

5.2 Học few-shot tự giám sát cross-domain

Thiết lập. Chúng tôi so sánh hiệu quả của BMSSL và baseline được mô tả trong Phần 5.1 trên các benchmark phân loại few-shot cross-domain, được chia thành hai loại dựa trên sự tương đồng với ImageNet: i) tương đồng cao: CUB [55], Cars [27], và Places [61]; ii) tương đồng thấp: CropDiseases [36], ISIC [11], và ChestX [54].

Kết quả. Bảng 4 trình bày hiệu suất của mô hình được huấn luyện trên miniImageNet cho meta-learning trên các bộ dữ liệu benchmark được đề cập ở trên. Bằng quan sát, chúng tôi tiếp tục xác nhận hiệu suất của BMSSL đề xuất: i) Hiệu quả: đạt được kết quả tương tự hoặc thậm chí tốt hơn so với các thuật toán baseline tiên tiến trên tất cả các bộ dữ liệu benchmark; ii) Tổng quát hóa: đạt được gần 3% cải thiện so với meta-learning có giám sát và học tự giám sát trên các bộ dữ liệu có sự khác biệt đáng kể so với giai đoạn huấn luyện; iii) Mạnh mẽ: đạt được kết quả tương tự với PsCo [22] giới thiệu các mẫu ngoài phân phối, mặc dù chúng tôi không xem xét rõ ràng các mẫu ngoài phân phối trên các bộ dữ liệu có sự khác biệt đáng kể.

5.3 Nghiên cứu Ablation

Tăng cường trong xây dựng nhiệm vụ. Mặc dù chúng tôi đã chỉ ra trong Hình 1 rằng tăng cường không thể bù đắp tác động của dữ liệu khan hiếm, chúng tôi chưa khám phá hiệu quả của các mức độ tăng cường khác nhau đối với xây dựng nhiệm vụ SSL, điều này trực tiếp liên quan đến sự đa dạng và tương đồng đặc trưng của các mẫu trong nhiệm vụ. Chúng tôi chia các phương pháp tăng cường thành bốn cấp độ với số lượng khác nhau (năm loại/một loại) và cường độ (nhẹ/mạnh, chẳng hạn như ghép nối diện tích lớn/nhỏ), và áp dụng chúng để đánh giá tác động đến mô hình. Kết quả thí nghiệm trong Bảng 5 cho thấy rằng lợi ích của sự đa dạng dữ liệu đối với mô hình bị hạn chế, và các chiến lược tăng cường có ít ảnh hưởng đến mô hình.

Hiệu quả của tối ưu hóa hai cấp độ. BMSSL giới thiệu kinh nghiệm học thông qua hai vòng lặp cập nhật gradient và cho mô hình khả năng tối ưu hóa và hạn chế hai lần. Để đánh giá hiệu quả của nó, chúng tôi cố định cấu trúc ràng buộc của vòng lặp trong và so sánh nó dưới ba cài đặt sau: i) tối ưu hóa một lần + không giới thiệu kinh nghiệm (M1): chỉ chứa mô hình cụ thể cho nhiệm vụ với khởi tạo ngẫu nhiên; ii) tối ưu hóa một lần + giới thiệu kinh nghiệm (M2): chỉ cập nhật metric learning của mô hình cho các nhiệm vụ cụ thể, và sử dụng nó làm kinh nghiệm; iii) tối ưu hóa hai lần + giới thiệu kinh nghiệm (M3): cấu trúc tối ưu hóa hai cấp độ của BMSSL. Kết quả của thí nghiệm ablation này được thể hiện trong Bảng 6. BMSSL đạt được gần 4% cải thiện, chứng minh lợi ích từ tiền nghiệm và cấu trúc mô hình đối với thuật toán.

Huấn luyện L và δ. Để tìm các tham số tối ưu của mô hình, chúng tôi kiểm tra mô hình trên miniImageNet với các cài đặt khác nhau của L và δ. Bảng 7 cho thấy độ chính xác của mô hình và hiệu quả chạy khi L = 5, chạy trên GPU NVIDIA V100. Chúng tôi thấy δ = 5 có thể là lựa chọn tốt nhất khi việc tăng thêm có ít ảnh hưởng đến độ chính xác, nhưng hiệu suất hoạt động giảm đáng kể. Việc giới thiệu δ có thể điều chỉnh hiệu quả của mô hình và đạt được hội tụ nhanh hơn thông qua xấp xỉ phân phối này. Kết quả đầy đủ và phân tích thêm có sẵn tại Phụ lục E.

Bảng 5: Độ chính xác(%) trên miniImageNet với bốn cấp độ tăng cường dữ liệu được biểu diễn như {Ai}4.

Cấp độ | Top-1 Acc(%)
A1 | 49.730 ±0.303
A2 | 49.990 ±0.238
A3 | 49.789 ±0.210
A4 | 49.832 ±0.199

Bảng 6: Độ chính xác(%) trên miniImageNet dưới ba loại cấu trúc mô hình được đề cập trong Phần 5.3.

Phương pháp | Top-1 Acc(%)
M1 | 39.583 ±0.482
M2 | 46.184 ±0.298
M3 | 49.987 ±0.283

Bảng 7: Độ chính xác(%) và các bước meta-training (/s) khi L = 5 trên miniImageNet với δ khác nhau.

δ | Top-5 ACC(%) | Bước(/s)
1 | 63.832 | 4.3
5 | 64.443 | 3.2
10 | 64.592 | 2.6
15 | 64.588 | 2.1
20 | 64.605 | 1.7

6 Kết luận

Trong công trình này, chúng tôi đề xuất một khung Học tự giám sát Meta có Bootstrap mới mô phỏng ba lợi thế của việc học con người: i) không cần giám sát, ii) không bị hạn chế bởi dữ liệu, và iii) học từ kinh nghiệm. Chúng tôi thảo luận về mối quan hệ giữa học tự giám sát và meta-learning, và tận dụng các phát hiện của chúng tôi để đề xuất một phương pháp đơn giản nhưng khéo léo để tái cấu trúc các nhiệm vụ tự giám sát. Ngoài ra, chúng tôi sử dụng tối ưu hóa hai cấp độ để giới thiệu kinh nghiệm cho việc học, và sử dụng meta-gradient để tạo ra mục tiêu bootstrap để làm cho mô hình trở thành giáo viên của chính nó. Thông qua phân tích lý thuyết và thí nghiệm rộng rãi, chúng tôi chứng minh hiệu suất vượt trội của khung của chúng tôi.

Tác động rộng hơn và Hạn chế. Công trình này cung cấp một cách đáng tin cậy để máy móc mô phỏng việc học con người, cung cấp tiến bộ công nghệ trong học máy. Chúng tôi không cần bị ràng buộc bởi dữ liệu hoặc huấn luyện dài hạn như các phương pháp trước đây. Nhưng nó có một hạn chế là việc đánh giá tập trung vào các nhiệm vụ thị giác, mà không xem xét hiệu quả học của các lĩnh vực khác (ví dụ như học tăng cường và nhận dạng ngôn ngữ) hoặc các nhiệm vụ khác (ví dụ như hồi quy, tạo sinh).

--- TRANG 8 ---

--- TRANG 9 ---

Bảng 4: Độ chính xác (%) phân loại few-shot cross-domain với hai loại được đề cập trong Phần 5.2. Chúng tôi chuyển giao các mô hình được huấn luyện trên miniImageNet cho mỗi benchmark. Ý nghĩa của "(way, shot)", "Bold" và "Blue" trong bảng nhất quán với Bảng 3.

Phương pháp | CUB | Cars | Places | CropDiseases | ISIC | ChestX
---|---|---|---|---|---|---
 | (5,5) | (5,20) | (5,5) | (5,20) | (5,5) | (5,20) | (5,5) | (5,20) | (5,5) | (5,20) | (5,5) | (5,20)
**Meta-learning không giám sát**
Meta-SVEBM | 45.893 | 54.823 | 33.530 | 44.622 | 50.516 | 61.561 | 71.652 | 84.515 | 37.106 | 48.001 | 27.238 | 29.652
Meta-GMVAE | 48.783 | 55.651 | 30.205 | 39.946 | 55.361 | 65.520 | 72.683 | 80.777 | 30.630 | 37.574 | 24.522 | 26.239
PsCo | 56.365 | 69.298 | 44.632 | 56.990 | 64.501 | 73.516 | 89.565 | 95.492 | 43.632 | 54.886 | 21.907 | 24.182
MetaSSL | 54.238 | 65.031 | 45.341 | 56.526 | 62.538 | 70.022 | 83.922 | 90.058 | 40.140 | 50.209 | 24.827 | 25.238
**BMSSL** | **57.543** | **69.561** | **49.636** | **59.511** | **67.250** | **75.834** | **87.524** | **95.950** | **46.518** | **56.293** | **29.463** | **30.389**
**Học tự giám sát**
SimCLR | 51.389 | 60.011 | 38.639 | 52.412 | 59.523 | 68.419 | 80.360 | 89.161 | 44.669 | 51.823 | 26.556 | 30.982
MoCo | 52.843 | 61.204 | 39.504 | 50.108 | 60.291 | 69.033 | 81.606 | 90.366 | 44.328 | 52.398 | 24.198 | 27.893
SwAV | 51.250 | 61.645 | 36.352 | 51.153 | 58.789 | 68.512 | 80.055 | 89.917 | 43.200 | 50.109 | 21.252 | 28.270
**Meta-learning có giám sát**
MAML | 57.296 | 64.005 | 44.934 | 49.561 | 62.502 | 71.741 | 78.202 | 85.247 | 46.405 | 56.293 | 22.435 | 24.238
ProtoNet | 56.237 | 64.829 | 40.893 | 48.123 | 59.887 | 69.207 | 76.651 | 84.164 | 40.028 | 49.289 | 22.219 | 25.839

--- TRANG 10 ---

--- TRANG 11 ---

--- TRANG 12 ---

--- TRANG 13 ---

Tài liệu tham khảo

[1] Saleh Albelwi. Survey on self-supervised learning: auxiliary pretext tasks and contrastive learning methods in imaging. Entropy, 24(4):551, 2022.

[2] Mustafa Sercan Amac, Ahmet Sencan, Bugra Baran, Nazli Ikizler-Cinbis, và Ramazan Gokberk Cinbis. Masksplit: Self-supervised meta-learning for few-shot semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1067–1077, 2022.

[3] Philip Bachman, R Devon Hjelm, và William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019.

[4] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, và Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In International Conference on Machine Learning, pages 1298–1312. PMLR, 2022.

[5] Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, và Andrew McCallum. Self-supervised meta-learning for few-shot natural language classification tasks. arXiv preprint arXiv:2009.08445, 2020.

[6] Luca Bertinetto, Joao F Henriques, Philip HS Torr, và Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.

[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, và Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912–9924, 2020.

[8] John B Carroll et al. Human cognitive abilities: A survey of factor-analytic studies. Number 1. Cambridge University Press, 1993.

[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, và Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020.

[10] Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, và Xiaolong Wang. Meta-baseline: Exploring simple meta-learning for few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9062–9071, 2021.

[11] Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018), pages 168–172. IEEE, 2018.

[12] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 44(10):7112–7127, 2021.

[13] Xiaomin Fang, Jizhou Huang, Fan Wang, Lihang Liu, Yibo Sun, và Haifeng Wang. Ssml: Self-supervised meta-learner for en route travel time estimation at baidu maps. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2840–2848, 2021.

[14] Chelsea Finn, Pieter Abbeel, và Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126–1135. PMLR, 2017.

[15] Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, và Satinder Singh. Bootstrapped meta-learning. arXiv preprint arXiv:2109.04504, 2021.

[16] Adam Foster, Rattana Pukdee, và Tom Rainforth. Improving transformation invariance in contrastive representation learning. arXiv preprint arXiv:2010.09515, 2020.

[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, và Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729–9738, 2020.

[18] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, và Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149–5169, 2021.

[19] Kyle Hsu, Sergey Levine, và Chelsea Finn. Unsupervised learning via meta-learning. arXiv preprint arXiv:1810.02334, 2018.

[20] Mike Huisman, Jan N Van Rijn, và Aske Plaat. A survey of deep meta-learning. Artificial Intelligence Review, 54(6):4483–4541, 2021.

[21] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, và Fillia Makedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2020.

[22] Huiwon Jang, Hankook Lee, và Jinwoo Shin. Unsupervised meta-learning via few-shot pseudo-supervised contrastive learning. arXiv preprint arXiv:2303.00996, 2023.

[23] Chia-Hsiang Kao, Wei-Chen Chiu, và Pin-Yu Chen. Maml is a noisy contrastive learner in classification. arXiv preprint arXiv:2106.15367, 2021.

[24] Siavash Khodadadeh, Ladislau Boloni, và Mubarak Shah. Unsupervised meta-learning for few-shot image classification. Advances in neural information processing systems, 32, 2019.

[25] Siavash Khodadadeh, Sharare Zehtabian, Saeed Vahidian, Weijia Wang, Bill Lin, và Ladislau Bölöni. Unsupervised meta-learning through latent-space interpolation in generative models. arXiv preprint arXiv:2006.10236, 2020.

[26] Deqian Kong, Bo Pang, và Ying Nian Wu. Unsupervised meta-learning via latent space energy-based model of symbol vector coupling. In Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems, 2021.

[27] Jonathan Krause, Michael Stark, Jia Deng, và Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554–561, 2013.

[28] Rayan Krishnan, Pranav Rajpurkar, và Eric J Topol. Self-supervised learning in medicine and healthcare. Nature Biomedical Engineering, pages 1–7, 2022.

[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

[30] Brenden M Lake, Ruslan Salakhutdinov, và Joshua B Tenenbaum. The omniglot challenge: a 3-year progress report. Current Opinion in Behavioral Sciences, 29:97–104, 2019.

[31] Dong Bok Lee, Dongchan Min, Seanie Lee, và Sung Ju Hwang. Meta-gmvae: Mixture of gaussian vae for unsupervised meta-learning. In International Conference on Learning Representations, 2021.

[32] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, và Tomas Pfister. Cutpaste: Self-supervised learning for anomaly detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9664–9674, 2021.

[33] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, và Jie Tang. Self-supervised learning: Generative or contrastive. IEEE Transactions on Knowledge and Data Engineering, 35(1):857–876, 2021.

[34] Shuai Luo, Yujie Li, Pengxiang Gao, Yichuan Wang, và Seiichi Serikawa. Meta-seg: A survey of meta-learning for image segmentation. Pattern Recognition, page 108586, 2022.

[35] John J McArdle và Richard W Woodcock. Human cognitive abilities in theory and practice. Psychology Press, 2014.

[36] Sharada P Mohanty, David P Hughes, và Marcel Salathé. Using deep learning for image-based plant disease detection. Frontiers in plant science, 7:1419, 2016.

[37] Alex Nichol và John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2(3):4, 2018.

[38] Aaron van den Oord, Yazhe Li, và Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.

[39] Sangwoo Park, Osvaldo Simeone, và Joonhyuk Kang. Meta-learning to communicate: Fast end-to-end training for fading channels. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5075–5079. IEEE, 2020.

[40] Alexandra C Pike và Oliver J Robinson. Reinforcement learning in patients with mood and anxiety disorders vs control individuals: A systematic review and meta-analysis. JAMA psychiatry, 2022.

[41] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, và Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprint arXiv:1803.00676, 2018.

[42] Sucheng Ren, Huiyu Wang, Zhengqi Gao, Shengfeng He, Alan Yuille, Yuyin Zhou, và Cihang Xie. A simple data mixing prior for improving self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14595–14604, 2022.

[43] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, và Richard E Turner. Fast and flexible multi-task classification using conditional neural adaptive processes. Advances in Neural Information Processing Systems, 32, 2019.

[44] Madeline C Schiappa, Yogesh S Rawat, và Mubarak Shah. Self-supervised learning for videos: A survey. ACM Computing Surveys, 2022.

[45] Connor Shorten và Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of big data, 6(1):1–48, 2019.

[46] Saeed Shurrab và Rehab Duwairi. Self-supervised learning methods and applications in medical imaging analysis: A survey. PeerJ Computer Science, 8:e1045, 2022.

[47] Jake Snell, Kevin Swersky, và Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017.

[48] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, và Bernt Schiele. Meta-transfer learning for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 403–412, 2019.

[49] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, và Timothy M Hospedales. Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1199–1208, 2018.

[50] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, và Phillip Isola. What makes for good views for contrastive learning? Advances in neural information processing systems, 33:6827–6839, 2020.

[51] Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.

[52] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016.

[53] Haoqi Wang, Zhizhong Li, Litong Feng, và Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4921–4930, 2022.

[54] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, và Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2097–2106, 2017.

[55] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, và Pietro Perona. Caltech-ucsd birds 200. 2010.

[56] Suorong Yang, Weikang Xiao, Mengcheng Zhang, Suhan Guo, Jian Zhao, và Furao Shen. Image data augmentation for deep learning: A survey. arXiv preprint arXiv:2204.08610, 2022.

[57] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, và Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pages 12310–12320. PMLR, 2021.

[58] Xueting Zhang, Debin Meng, Henry Gouk, và Timothy M Hospedales. Shallow bayesian meta learning for real-world few-shot recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 651–660, 2021.

[59] Yabin Zhang, Hui Tang, và Kui Jia. Fine-grained visual categorization using meta-learning optimization with sample selection of auxiliary data. In Proceedings of the european conference on computer vision (ECCV), pages 233–248, 2018.

[60] Tony Z Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess, Jon Scholz, Stefan Schaal, và Sergey Levine. Offline meta-reinforcement learning for industrial insertion. In 2022 International Conference on Robotics and Automation (ICRA), pages 6386–6393. IEEE, 2022.

[61] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, và Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464, 2017.

[62] Qing Zhu, Qirong Mao, Hongjie Jia, Ocquaye Elias Nii Noi, và Juanjuan Tu. Convolutional relation network for facial expression recognition in the wild with few-shot learning. Expert Systems with Applications, 189:116046, 2022.

[63] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, và Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pages 2069–2080, 2021.

[64] Roland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, và Wieland Brendel. Contrastive learning inverts the data generating process. In International Conference on Machine Learning, pages 12979–12990. PMLR, 2021.
