# 2309.13308.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl-papers/2309.13308.pdf
# Kích thước tệp: 875796 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Preprint
HIỆU CHỈNH BỘ ĐÁNH GIÁ DỰA TRÊN LLM
Yuxuan Liu†∗, Tianchi Yang‡, Shaohan Huang♯‡, Zihan Zhang‡, Haizhen Huang‡,
Furu Wei‡, Weiwei Deng‡, Feng Sun‡, Qi Zhang‡
†Đại học Bắc Kinh ‡Tập đoàn Microsoft
yx.liu@stu.pku.edu.cn

TÓM TẮT
Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLM) về mô hình hóa ngôn ngữ và khả năng nổi lên khiến chúng trở thành một bộ đánh giá không cần tham chiếu đầy hứa hẹn cho chất lượng sinh ngôn ngữ tự nhiên, và là một thay thế có năng lực cho đánh giá của con người. Tuy nhiên, bị cản trở bởi nguồn đóng hoặc nhu cầu tính toán cao để lưu trữ và điều chỉnh, thiếu thực hành để tiếp tục hiệu chỉnh một bộ đánh giá dựa trên LLM có sẵn hướng tới sự liên kết tốt hơn với con người. Trong công trình này, chúng tôi đề xuất AUTOCALIBRATE, một phương pháp đa giai đoạn, không cần gradient để tự động hiệu chỉnh và liên kết một bộ đánh giá dựa trên LLM hướng tới sở thích của con người. Thay vì mô hình hóa rõ ràng sở thích của con người, trước tiên chúng tôi bao gồm ngầm chúng trong một tập hợp các nhãn của con người. Sau đó, một tập hợp tiêu chí chấm điểm ban đầu được soạn thảo bởi chính mô hình ngôn ngữ, tận dụng học tập trong ngữ cảnh trên các ví dụ few-shot khác nhau. Để tiếp tục hiệu chỉnh tập hợp tiêu chí này, chúng tôi chọn những người thực hiện tốt nhất và soạn thảo lại chúng với tự-tinh chỉnh. Các thí nghiệm của chúng tôi trên nhiều tập dữ liệu đánh giá chất lượng văn bản minh họa một cải thiện đáng kể trong tương quan với đánh giá chuyên gia thông qua hiệu chỉnh. Phân tích định tính toàn diện của chúng tôi truyền đạt những trực giác và quan sát sâu sắc về bản chất của tiêu chí chấm điểm hiệu quả.

1 GIỚI THIỆU
Sự xuất hiện của các mô hình ngôn ngữ lớn đang kêu gọi sự tập trung lớn hơn và tầm quan trọng hơn về chất lượng đánh giá sinh ngôn ngữ tự nhiên. Với sự cải thiện nhanh chóng của các mô hình ngôn ngữ, mục tiêu của chúng đã vượt ra ngoài việc đơn giản là khớp đầu ra với một số lượng mẫu đã cho đến một sự liên kết con người rộng lớn hơn. Các chỉ số đánh giá truyền thống như BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) và CIDEr (Vedantam et al., 2015) thường yêu cầu đầu ra tham chiếu được tuyển chọn, việc ứng dụng của chúng bị hạn chế khi không gian đầu ra mở và đa dạng, và cho thấy tương quan thấp với phán đoán của con người (Freitag et al., 2022). Trong khi các bộ đánh giá dựa trên mô hình tinh vi như BERTScore (Zhang* et al., 2020) và COMET (Rei et al., 2020) mang lại cải thiện tương quan, hiệu suất của chúng vẫn bị hạn chế bởi chất lượng của tham chiếu. Do đó, có một nhu cầu tăng cao về các bộ đánh giá liên kết con người, không cần tham chiếu cho đánh giá NLG.

Về mặt này, các nghiên cứu gần đây đã khám phá việc tận dụng các mô hình ngôn ngữ lớn (LLM) hiện đại làm bộ đánh giá không cần tham chiếu trên các tác vụ NLG khác nhau (Kocmi & Federmann, 2023; Fu et al., 2023; Wang et al., 2023a; Liu et al., 2023). Cho rằng LLM được tối ưu hóa để tuân theo hướng dẫn của con người (Ouyang et al., 2022) cũng như hiệu suất hiện đại của chúng trong mô hình hóa ngôn ngữ (OpenAI, 2023), chúng có thể thực hiện tác vụ đánh giá khi được prompt phù hợp. Nhiều bằng chứng cho thấy LLM đầy hứa hẹn có năng lực trong việc đánh giá các mô hình được điều chỉnh theo hướng dẫn như Alpaca (Taori et al., 2023) và Vicuna (Zheng et al., 2023), và là một thay thế khả thi cho đánh giá chuyên gia của con người (Zheng et al., 2023; Dubois et al., 2023).

Bất chấp những kết quả đầy hứa hẹn này, các nghiên cứu mới nổi đang nêu lên mối quan tâm về tính hợp lệ của các bộ đánh giá dựa trên LLM - liệu cơ chế chấm điểm cơ bản của LLM có phù hợp với hướng dẫn và sở thích của con người hay không (Liu et al., 2023). Các bộ đánh giá dựa trên LLM hiện tại bao gồm văn bản ứng cử viên cùng với tác vụ đánh giá vào một prompt hướng dẫn. Trong khi paradigm này thành công trong việc trình bày tác vụ, nó làm nổi lên một số vấn đề chưa được giải quyết, bao gồm độ nhạy cảm và thiên vị đối với không gian đầu ra (Wang et al., 2023a), thứ tự mẫu (Wang et al., 2023b), và định dạng prompt (Zheng et al., 2023). Thêm vào đó, vì prompt chấm điểm cũng được viết bởi con người, nó cũng có thể kết hợp thiên vị tiềm ẩn vào LLM.

Để giải quyết vấn đề này, chúng tôi nghiên cứu việc hiệu chỉnh một bộ đánh giá dựa trên LLM hướng tới sự liên kết tốt hơn với con người. Chúng tôi bắt đầu từ một sự hồi tưởng về các bộ đánh giá dựa trên LLM hiện tại và khám phá ra chúng gặp phải prompting không đủ, trong đó các hướng dẫn chấm điểm vắng mặt và chỉ các không gian đầu ra (ví dụ: 0-100) được cung cấp, dẫn đến các đánh giá không nhất quán và không liên kết (Lu et al., 2023). Chúng tôi lập luận rằng vấn đề như vậy có thể được giảm thiểu bằng cách làm rõ tiêu chí chấm điểm. Và bằng cách hoàn thiện tiêu chí chấm điểm, một sự đồng thuận có thể đạt được giữa con người và LLM, như một phương tiện liên kết.

Tuy nhiên, không phải là tầm thường để có được tiêu chí đầy đủ², vì nó có thể yêu cầu kiến thức chuyên môn cấp độ chuyên gia để gán rubric và ngăn chặn thiên vị cá nhân. Rút ra cảm hứng từ khả năng học tập trong ngữ cảnh (Dong et al., 2022) của LLM, chúng tôi đề xuất AUTOCALIBRATE, một framework để tự động liên kết và hiệu chỉnh một bộ đánh giá dựa trên LLM thông qua sự liên kết con người. Để giải quyết thách thức của việc tuyển chọn tiêu chí chấm điểm, chúng tôi áp dụng một phương pháp hướng dữ liệu để soạn thảo, lọc và tinh chỉnh rubric sử dụng LLM, dựa trên nhãn chuyên gia của con người. Bằng cách kết hợp các rubric được khai thác và hiệu chỉnh vào hướng dẫn chấm điểm, chúng tôi đạt được những cải thiện đáng kể trong sự liên kết con người khi đánh giá tóm tắt văn bản, sinh dữ liệu thành văn bản, và ảo giác. Hơn nữa, chúng tôi phát hành các tập hợp tiêu chí chấm điểm tối ưu được khai thác cho các tác vụ trên, và trình bày phân tích định tính và định lượng chi tiết để khám phá bản chất tạo nên một tiêu chí hiệu quả.

²Kết quả trong Chen et al. (2023) cho thấy rằng tiêu chí được tuyển chọn kém làm giảm liên quan với chấm điểm chuyên gia của con người. Tiêu chí ngẫu nhiên chưa được hiệu chỉnh sẽ đưa ra thiên vị bổ sung như một sự không liên kết giữa các tiêu chuẩn được sử dụng cho các chuyên gia con người. Và các rubric được gán không đúng cách có thể làm giảm sự khác biệt giữa mỗi điểm số.

--- TRANG 2 ---
Preprint
Vui lòng đánh giá chất lượng tóm tắt của một bài báo về tính nhất quán với bài báo gốc.
Tiêu chí để đánh giá tính nhất quán: [Nội dung tiêu chí]
Vui lòng trả về điểm đánh giá của bạn trên thang điểm từ 1 đến 5, với 1 là thấp nhất.
Bài báo: [Nội dung bài báo]
Tóm tắt: [Nội dung tóm tắt]

Mẫu Prompt Đánh Giá
Hướng dẫn
Khía cạnh
Tiêu chí
Không gian đầu ra
Mẫu dữ liệu để đánh giá

Hình 2: Ví dụ về mẫu prompt đánh giá được áp dụng bởi một bộ đánh giá dựa trên LLM.

2 PHƯƠNG PHÁP

2.1 TỔNG QUAN VỀ AUTOCALIBRATE
Hình 1 minh họa framework tổng thể của AUTOCALIBRATE. Để hiệu chỉnh một bộ đánh giá dựa trên LLM, chúng tôi tập trung vào tối ưu hóa mẫu prompt đánh giá T được áp dụng để cải thiện tương quan và sự liên kết giữa điểm số của LLM và sở thích của con người. Cụ thể, chúng tôi khai thác và điều chỉnh tiêu chí chấm điểm trong việc theo đuổi sự liên kết như vậy. Để biểu hiện sở thích của con người, trước tiên chúng tôi xây dựng một tập hợp vàng D*, chứa các cặp mẫu-nhãn sự thật từ người ghi nhãn chuyên gia của con người. Sau đó chúng tôi tuân theo một quy trình đa giai đoạn mới để tối ưu hóa tiêu chí chấm điểm ứng cử viên, bao gồm soạn thảo và xem xét lại. Các bản thảo tiêu chí ban đầu trước tiên được suy ra từ nhãn trong ngữ cảnh và một prompt quy nạp, đánh giá và lọc trên nhãn chuyên gia, và sau đó được tinh chỉnh để phù hợp với các đánh giá sai lệch.

2.2 CÔNG THỨC HÓA VẤN ĐỀ
Trong phần này, chúng tôi trình bày chi tiết về phương tiện hiệu chỉnh và mục tiêu của AUTOCALIBRATE - tiêu chí chấm điểm. Ký hiệu D là tập dữ liệu chứa nhiều mẫu để đánh giá. Dựa trên các tác vụ khác nhau, một mẫu di ∈ D có thể chứa các thành phần khác nhau: văn bản đơn, cho các tác vụ như đánh giá tính đúng ngữ pháp; nguồn-đích, cho phần lớn các sinh có điều kiện, và đa lượt, như đánh giá các cuộc trò chuyện đa lượt.

Để hướng dẫn LLM đánh giá chất lượng của mẫu di, các prompt được áp dụng để cung cấp hướng dẫn và làm rõ đầy đủ về tác vụ. Để hiệu chỉnh mẫu prompt T được áp dụng trong quá trình đánh giá, chúng tôi quy định nó bằng cách phân tách thành các khối xây dựng sau: hướng dẫn, tiêu chí, khía cạnh, định dạng đầu ra, và mẫu dữ liệu để đánh giá, như được minh họa trong Hình 2. Đối với một mẫu tùy ý di ∈ D, cho một mẫu prompt T (hướng dẫn LLM thực hiện đánh giá về chất lượng NLG), tiêu chí chấm điểm C, khía cạnh đánh giá a (ví dụ: lưu loát, mạch lạc, nhất quán) và một mô hình ngôn ngữ lớn LLM(·), chất lượng NLG của di có thể được đánh giá như

ŝi,a = LLM(T(di, C, a)). (1)

Ký hiệu D* là tập hợp vàng bao gồm các cặp mẫu-nhãn được tuyển chọn (d*i, si,a) từ các chuyên gia con người, và f(·) là một chỉ số tương quan. Trong AUTOCALIBRATE, chúng tôi tập trung vào hiệu chỉnh tiêu chí chấm điểm C để tối đa hóa tương quan giữa nhãn dự đoán và nhãn chuyên gia của con người, như

C = arg max_C f(∪d*i∼D* (ŝi,a, si,a)). (2)

2.3 AUTOCALIBRATE

Ghi nhãn dữ liệu như sở thích của con người Để hiệu chỉnh một bộ đánh giá dựa trên LLM, một câu hỏi chính là: làm thế nào để biểu diễn và mô hình hóa sở thích của các chuyên gia con người. Về các phương pháp hiện tại, các bộ đánh giá dựa trên mô hình tinh vi như COMET (Rei et al., 2020) trực tiếp huấn luyện trên nhãn của con người, trong khi nhãn của con người dựa trên thứ hạng được áp dụng rộng rãi trong RLHF để mô hình hóa sở thích của con người (Ouyang et al., 2022). Tuy nhiên, các phương pháp mô hình hóa sở thích dựa trên mô hình này yêu cầu fine-tuning bổ sung, điều này làm cho chúng tốn kém về mặt tính toán và không thực tế đối với các LLM dựa trên API. Để giảm thiểu những hạn chế này, chúng tôi mã hóa ngầm sở thích chuyên gia của con người thành một tập hợp các cặp mẫu-nhãn và tạo thành một tập hợp vàng D*. So với việc tuyển chọn tiêu chí chấm điểm hoàn thiện và hướng dẫn với các công việc chuyên gia con người chung, việc thu thập nhãn tận dụng việc gửi crowdsourcing là khả thi hơn, và cũng dễ dàng hơn để xác thực và hợp nhất ý kiến từ các chuyên gia khác nhau.

Soạn thảo tiêu chí Sau khi xây dựng tập hợp nhãn chuyên gia D*, chúng tôi sử dụng khả năng tuân theo hướng dẫn và học tập trong ngữ cảnh của LLM để suy ra một cách độc lập tiêu chí chấm điểm C từ các mẫu few-shot. Một phần quan trọng ở đây là đảm bảo tính đa dạng của tiêu chí được nhớ lại. Để giảm thiểu thiên vị nhãn và thiên vị vị trí của học tập trong ngữ cảnh (Zhao et al., 2021), chúng tôi xây dựng các mẫu Monte-Carlo khác nhau từ D* để có được các mẫu trong ngữ cảnh few-shot. Cho mẫu prompt soạn thảo TD và một tập hợp mẫu few-shot Ds = ∪(d*i, si,a) ⊂ D*, một tiêu chí tương ứng được suy ra như

Ĉ = arg max_C Pθ(C|TD(Ds, a)), (3)

trong đó a biểu thị khía cạnh đánh giá. Lấy mẫu nhiệt độ cũng được áp dụng để rút ra tiêu chí chấm điểm trong các bài thuyết trình đa dạng từ LLM. Các mẫu prompt ví dụ được cung cấp trong Phụ lục D.1.

Theo quy trình này, chúng tôi có được tập hợp tiêu chí chấm điểm ban đầu để đánh giá và tinh chỉnh.

Xem xét lại tiêu chí Được suy ra từ các mẫu few-shot khác nhau, tiêu chí trong tập hợp bản thảo ban đầu được đa dạng hóa, nhưng có thể là tối ưu phụ hoặc chứa thiên vị tiềm ẩn (ví dụ: đối với các nhãn chấm điểm cụ thể). Để lọc ra các ứng cử viên chất lượng cao, trước tiên chúng tôi xem xét lại chúng tận dụng D* và chọn những ứng cử viên hoạt động tốt nhất w.r.t liên quan của con người³. Để giảm thiểu sự bất đồng giữa các chuyên gia con người và tiêu chí được soạn thảo, chúng tôi prompt LLM tinh chỉnh (Madaan et al., 2023) tiêu chí được tạo trước đó bằng cách cung cấp cho chúng các mẫu với sự bất đồng mạnh trong điểm số của chúng. Khi tinh chỉnh tiêu chí, chúng tôi đề xuất các hoạt động chỉnh sửa nguyên tử sau thông qua prompting đến LLM⁴:

• Sửa đổi: Điều chỉnh một số phần của tiêu chí để tăng tương quan của nó.
• Diễn giải: Nếu một tiêu chí đủ tốt, hãy diễn giải nó để làm cho nó rõ ràng và ngắn gọn hơn.
• Thêm khía cạnh hoặc chi tiết: Khi LLM khám phá các quy tắc chấm điểm cơ bản mới không được bao phủ bởi tiêu chí hiện tại, hãy xem xét thêm chúng như một dòng mới vào tiêu chí hiện tại, nhưng đảm bảo không làm cho tiêu chí quá dài và dư thừa.
• Hiệu chỉnh: Bất kỳ sửa đổi nào khác mà LLM coi là hữu ích.

Như được minh họa trong Hình 1, sau khi có được tiêu chí ứng cử viên được tinh chỉnh, trước tiên chúng tôi lọc chúng với D* và sau đó kết hợp chúng với tiêu chí bản thảo được lọc trước để có được một tập hợp các quy tắc chấm điểm được hiệu chỉnh.

Kết luận và thảo luận Kết hợp những điều trên, chúng tôi có được AUTOCALIBRATE, một pipeline tự động trong việc hiệu chỉnh các bộ đánh giá dựa trên LLM. Quy trình tổng thể được tóm tắt trong Thuật toán 1.

Lợi ích của việc chọn tiêu chí làm phương tiện hiệu chỉnh là đa dạng. Đầu tiên, chúng tôi không yêu cầu gradient hoặc truy cập vào tham số mô hình, điều này làm cho AUTOCALIBRATE có thể áp dụng cho các LLM dựa trên API. Thứ hai, vì tiêu chí vẫn ở dạng ngôn ngữ tự nhiên (so với soft prompt-tuning), việc hiệu chỉnh tiêu chí là cần thiết để đạt được một thỏa thuận giữa con người và LLM. Do đó, quá trình này có thể giải thích được và có thể kiểm soát hơn (ví dụ: người ta có thể thực hiện các điều chỉnh con người-trong-vòng lặp đối với tiêu chí chấm điểm trong trường hợp thay đổi sở thích, hoặc để tránh các trường hợp góc).

3 THIẾT LẬP THÍ NGHIỆM

3.1 TÁC VỤ VÀ TẬP DỮ LIỆU
Chúng tôi đánh giá AUTOCALIBRATE trên ba tác vụ đánh giá chất lượng văn bản, bao gồm tóm tắt văn bản, sinh dữ liệu thành văn bản, và đánh giá ảo giác. Chúng tôi chọn các tác vụ theo các công trình nghiên cứu trước đây (Zhong et al., 2022; Fu et al., 2023). Chúng tôi chọn hai tập dữ liệu cho mỗi tác vụ, bao gồm 6 tập dữ liệu tổng cộng, mỗi tập chứa nhãn chuyên gia con người cho các mẫu ứng cử viên. Cụ thể, chúng tôi chọn NewsRoom (Grusky et al., 2018) và SummEval (Fabbri et al., 2021) để đánh giá tóm tắt máy; SFRES (Wen et al., 2015) và SFHOT (Wen et al., 2015) cho tác vụ dữ liệu thành văn bản, QAGS-XSUM và QAGS-CNN (Wang et al., 2020b) để đánh giá ảo giác. Để đánh giá sự liên kết giữa chấm điểm từ LLM và các chuyên gia con người, chúng tôi thực hiện một meta-đánh giá theo (Zhong et al., 2022). Chi tiết về chiến lược đánh giá được liệt kê trong Phụ lục A.

3.2 MÔ HÌNH VÀ BASELINE
Để triển khai AUTOCALIBRATE, chúng tôi chọn mô hình GPT-4 của OpenAI (GPT-4-32K) làm LLM cho bộ đánh giá. Chúng tôi liệt kê các mẫu prompt cho soạn thảo tiêu chí, đánh giá và tinh chỉnh cho mỗi tác vụ trong Phụ lục D. Chúng tôi đặt nhiệt độ là 0 trong quá trình đánh giá, và 1 khi có được các bản thảo tiêu chí ban đầu và các phiên bản tinh chỉnh của chúng. Vui lòng tham khảo Phụ lục C để biết các cấu hình chi tiết của mỗi tác vụ.

Chúng tôi so sánh AUTOCALIBRATE với các bộ đánh giá hiện đại và/hoặc được áp dụng rộng rãi khác nhau. Đầu tiên chúng tôi bao gồm ROUGE (Lin, 2004), một chỉ số đánh giá dựa trên n-gram được áp dụng rộng rãi cho tóm tắt văn bản. Sau đó chúng tôi chọn các bộ đánh giá khác nhau dựa trên các mô hình neural (ngôn ngữ) nhỏ hơn, bao gồm BERTScore (Zhang* et al., 2020), MoverScore (Zhao et al., 2019), PRISM (Thompson & Post, 2020), BartScore (Yuan et al., 2021), CTC (Deng et al., 2021), và UniEval (Zhong et al., 2022). Cuối cùng, chúng tôi so sánh các bộ đánh giá dựa trên LLM hiện đại (ví dụ: GPT-3.5 và GPT-4), bao gồm GPTScore (Fu et al., 2023), ChatGPT⁵ (Wang et al., 2023a), và GPT-Eval (Liu et al., 2023).

4 KẾT QUẢ THÍ NGHIỆM

4.1 KẾT QUẢ CHO TÓM TẮT
Chúng tôi tiến hành phân tích meta-tương quan trên benchmark NewsRoom và SummEval để đánh giá hiệu suất của AUTOCALIBRATE trong việc hiệu chỉnh một bộ đánh giá dựa trên LLM về tóm tắt văn bản. Theo Liu et al. (2021), chúng tôi thực hiện phân tích tương quan Spearman và Kendall cấp độ tóm tắt trên mỗi trong số 4 chỉ số đánh giá với đánh giá chuyên gia con người. Để đại diện cho hiệu suất của LLM backbone chưa được hiệu chỉnh của chúng tôi, chúng tôi thêm một baseline GPT-4, có đánh giá được thu thập với một lần gọi sử dụng prompt đánh giá mà tiêu chí chấm điểm bị bỏ qua⁶.

Kết quả trên benchmark NewsRoom và SummEval được liệt kê trong Bảng 1 và 2, tương ứng. Trên benchmark NewsRoom (Bảng 1), AUTOCALIBRATE của chúng tôi vượt trội đáng kể so với bộ đánh giá ChatGPT dựa trên LLM. Nó cũng vượt qua bộ đánh giá GPT-4 vanilla với biên độ lớn (với cải thiện 10.4% trên Spearman và 11% trên tương quan Kendall), chứng minh tầm quan trọng và hiệu quả của quy trình hiệu chỉnh. Trong khi BartScore đạt được hiệu suất có thẩm quyền trên NewsRoom, nó thất bại trên SummEval. Chúng tôi suy đoán rằng vì nó sử dụng một mô hình nhỏ hơn, tính nhất quán trong chấm điểm của nó có thể bị cản trở do phân phối của corpus fine-tuning của nó.

³Một phương pháp meta-đánh giá f(·) được áp dụng ở đây để thực hiện meta-đánh giá về tương quan giữa phán đoán của con người và LLM. Để biết giải thích và định nghĩa chi tiết, vui lòng tham khảo Phụ lục A.
⁴Các ví dụ prompt chi tiết được cung cấp trong Phụ lục D.3.
⁵Bộ đánh giá 'ChatGPT' bao gồm nhiều phiên bản theo các mẫu prompt khác nhau, và chúng tôi đánh dấu các biến thể này với dấu ngoặc. Chúng tôi khuyến khích độc giả kiểm tra các công trình gốc để biết thông tin chi tiết.
⁶Để có sự so sánh công bằng, sự khác biệt duy nhất là việc loại bỏ tiêu chí khỏi prompt. Chúng tôi giữ phần còn lại giống hệt nhau.

--- TRANG 3 ---
[Tiếp tục dịch phần còn lại của tài liệu...]

Trái lại, AUTOCALIBRATE của chúng tôi đã chứng minh sự cải thiện liên quan con người nhất quán trên cả hai tập dữ liệu tóm tắt, vì kiến thức pretraining trong LLM sâu sắc và có thể tổng quát hóa hơn.

Trên SummEval, AUTOCALIBRATE cải thiện tương quan con người của đánh giá GPT-4 bằng 7.3%, và cũng vượt trội hơn baseline mạnh G-EVAL-4 cũng sử dụng GPT-4. Đáng chú ý, G-EVAL-4 yêu cầu 20 lần gọi từ LLM để có được điểm số trung bình nhằm giảm thiểu các đánh giá lặp lại. Trong khi điều này cải thiện tương quan Spearman bằng cách tạo ra phân phối liên tục hơn, nó làm giảm hệ số thứ hạng. Ngược lại, bằng cách làm rõ quy tắc chấm điểm với tiêu chí đã hiệu chỉnh, AUTOCALIBRATE cải thiện cả hệ số Spearman (2.9%) và Kendall (13.4%) chỉ với một lần gọi forward.

4.2 KẾT QUẢ CHO DỮ LIỆU THÀNH VĂN BẢN

Chúng tôi xem xét tập dữ liệu SFRES và SFHOT để đánh giá tác vụ sinh dữ liệu thành văn bản và tuân theo Fu et al. (2023) để tiến hành meta-đánh giá cấp độ tập dữ liệu về sự liên kết con người. Kết quả được liệt kê trong Bảng 3. Như được minh họa trong bảng, AUTOCALIBRATE vượt trội đáng kể so với bộ đánh giá được huấn luyện có thẩm quyền nhất (UNIEVAL) hơn 30%, và mang lại cải thiện hơn 20% và 10% về tương quan Spearman so với GPT-SCORE (dựa trên LLM GPT-3.5 175B) và bộ đánh giá GPT-4 chưa hiệu chỉnh, tương ứng. Những kết quả này cho thấy rằng các quy trình được đề xuất trong AUTOCALIBRATE có thể nhanh chóng tuyển chọn tiêu chí chấm điểm đầy đủ cho các tác vụ NLG khác nhau và phân phối mẫu.

4.3 KẾT QUẢ CHO ĐÁNH GIÁ ẢO GIÁC

Ảo giác là một vấn đề quan trọng trong các mô hình NLG khi đầu ra dựa trên các sự kiện bịa đặt, không có căn cứ hoặc lệch khỏi ngữ cảnh trước đó, và nó đang trở thành một chủ đề ngày càng quan trọng cho các LLM đáng tin cậy (Ji et al., 2023). Để kiểm tra AUTOCALIBRATE về đánh giá ảo giác, chúng tôi chọn tập dữ liệu QAGS-CNNDM và QAGS-XSUM và thực hiện meta-phân tích cấp độ tập dữ liệu theo Liu et al. (2023). Như được trình bày trong Bảng 4, AUTOCALIBRATE nâng cao tương quan Spearman trung bình 15% so với G-EVAL-4. Đáng chú ý, vì được fine-tuned trên dữ liệu CNN, BartScore đạt được sự liên quan con người đầy hứa hẹn trên QAGS-CNN, nhưng thất bại đáng kể trên QAGS-XSUM, trong khi AUTOCALIBRATE dựa trên LLM hoạt động nhất quán trên cả hai tập dữ liệu. Điều này chỉ ra thêm rằng các LLM, với kiến thức khổng lồ thu được trong quá trình pre-training, là những ứng cử viên mạnh mẽ cho các bộ đánh giá tổng quát, và hiệu suất của chúng có thể được tăng cường thêm với hiệu chỉnh phù hợp.

4.4 THÍ NGHIỆM ABLATION

Chúng tôi tiến hành nghiên cứu ablation về quy trình của AUTOCALIBRATE để điều tra tốt hơn sự đóng góp của mỗi quá trình trong việc hiệu chỉnh bộ đánh giá dựa trên LLM. Các thí nghiệm ablation chính được liệt kê trong Bảng 5. Như được minh họa trong bảng, việc loại bỏ tiêu chí trong prompt làm giảm đáng kể tương quan con người của GPT-4. Điều này chứng thực lập luận của chúng tôi rằng trước đây các LLM gặp khó khăn từ một nguyên tắc chấm điểm được định nghĩa mơ hồ, và điều này có thể được hiệu chỉnh để tăng sự liên kết con người của các bộ đánh giá LLM. Quá trình tự-tinh chỉnh cũng đóng góp tích cực vào những cải thiện trong sự liên kết con người. Điều này chỉ ra rằng các LLM có thể điều chỉnh tương ứng hiệu quả của tiêu chí chấm điểm. Phân tích định tính chi tiết được trình bày trong Chương 5.

5 PHÂN TÍCH

5.1 BẢN CHẤT CỦA TIÊU CHÍ HIỆU QUẢ

Trong chương này, chúng tôi trình bày phân tích thống kê về nhóm ứng cử viên bản thảo của tiêu chí chấm điểm, và khai thác bản chất có thể đóng góp vào tiêu chí chấm điểm hiệu quả với sự liên quan con người cao cho các bộ đánh giá dựa trên LLM. Kết quả chính được trình bày trong Hình 3.

Tác động của kích thước ví dụ Few-Shot Chúng tôi nghiên cứu độ nhạy của AUTOCALIBRATE đối với kích thước mẫu của các sampler trong ngữ cảnh few-shot. Như được minh họa trong Hình 3(A), kích thước của các mẫu trong ngữ cảnh few-shot không mang lại tác động đáng kể ngoại trừ QAGS-CNN. Kết quả cho thấy rằng AUTOCALIBRATE hầu như mạnh mẽ đối với kích thước của các mẫu trong ngữ cảnh. Nhờ kiến thức tiền nghiệm đầy đủ thu được trong quá trình pretraining bởi LLM, AUTOCALIBRATE có khả năng suy ra tiêu chí cơ bản chỉ sử dụng một vài ví dụ trong ngữ cảnh. Như được minh họa trong hình, kích thước few-shot từ 8 đến 12 là đủ để khai thác tiêu chí hiệu quả trên tất cả các tác vụ. Tính năng hấp dẫn này cho phép giảm không gian tìm kiếm để giảm chi phí khi triển khai.

Tác động của độ dài tiêu chí Phân phối độ dài của tiêu chí được tạo ra và sự liên quan con người của chúng được minh họa trong Hình 3(B). Hầu hết các tiêu chí đánh giá được soạn thảo và tinh chỉnh với AUTOCALIBRATE nằm trong phạm vi từ 60 đến 600 từ. Chúng tôi khám phá các xu hướng khác nhau về sở thích của AUTOCALIBRATE đối với các độ dài tiêu chí khác nhau. Trong khi các chỉ số lưu loát và mạch lạc về tóm tắt văn bản nghiêng về tiêu chí ngắn hơn, các phiên bản dài hơn được ưa chuộng bởi chỉ số tính thông tin về dữ liệu thành văn bản và đánh giá ảo giác. Bất chấp sự khác biệt này, AUTOCALIBRATE có khả năng tạo ra tiêu chí hiệu quả ở mỗi độ dài. Chúng tôi suy đoán sự tinh tế này được gây ra bởi sự phức tạp nội tại của khía cạnh cần đánh giá: có thể đơn giản để định nghĩa tính lưu loát, nhưng có thể thách thức hơn để giải quyết ảo giác.

Mẫu của tiêu chí Chúng tôi quan sát hai mẫu đáng kể về tiêu chí được soạn thảo bởi GPT-4: toàn diện và cụ thể. Cái trước thường đặc trưng cho các tính năng chung được sở hữu bởi các mẫu chất lượng cao và thấp, trong khi cái sau tạo ra một phân đoạn của rubric tương ứng cho mỗi điểm số đánh giá (ví dụ: 1 đến 5). Một ví dụ ngẫu nhiên về các mẫu tiêu chí này được liệt kê trong Bảng 6. Hai mẫu này xuất hiện trên tất cả các tập thí nghiệm trên các benchmark khác nhau. Phân phối hiệu suất của hai mẫu này trên tất cả các tập dữ liệu được minh họa trong Hình 4. Như được minh họa trong hình, không có sự khác biệt đáng kể về tương quan chuyên gia con người giữa các mẫu toàn diện và cụ thể, cho thấy rằng cả hai mẫu được tạo ra từ AUTOCALIBRATE đều có chất lượng cao. Do đó, hiệu suất của AUTOCALIBRATE mạnh mẽ đối với các mẫu tiêu chí được tạo ra.

5.2 NGHIÊN CỨU TRƯỜNG HỢP

Để điều tra tác động của tinh chỉnh tiêu chí, chúng tôi trình bày một nghiên cứu trường hợp trong Bảng 7. Như được chứng minh trong bảng, khi được prompt với các trường hợp đánh giá không liên kết trước đó và các phương tiện sửa đổi có thể (Phần 2.3), LLM tự động suy ra các mẫu mới của nguyên tắc chấm điểm cơ bản, và nhanh chóng thích ứng tiêu chí hiện tại để phù hợp với chúng. Như được minh họa trong bảng, AUTOCALIBRATE khám phá rằng thể loại và định dạng là quan trọng đối với tính lưu loát của tóm tắt từ các ví dụ trong ngữ cảnh được cung cấp, điều chỉnh tiêu chí tương ứng, và đạt được sự liên quan con người cao hơn. Những phát hiện này chứng thực với Madaan et al. (2023) rằng LLM có khả năng tự-tinh chỉnh, và mở ra một hướng nghiên cứu tương lai về hiệu chỉnh đa lượt, lặp lại của các bộ đánh giá dựa trên LLM.

6 CÔNG TRÌNH LIÊN QUAN

Đánh giá NLG tự động Việc tự động đánh giá sinh ngôn ngữ tự nhiên đã là một nỗ lực dài và khó khăn. Đoạn này vạch ra các chỉ số đánh giá tự động trước kỷ nguyên LLM. (1) Chỉ số dựa trên N-gram: là phương pháp được áp dụng rộng rãi nhất, các chỉ số dựa trên n-gram đo lường chất lượng của văn bản ứng cử viên bằng sự chồng chập của phần từ vựng của nó giữa các tham chiếu. Là hai trong số những chỉ số được sử dụng rộng rãi nhất, BLEU (Papineni et al., 2002) và ROUGE (Lin, 2004) chuyên biệt về độ chính xác cho dịch máy và recall cho tóm tắt văn bản, tương ứng. Bất chấp được áp dụng rộng rãi, sự liên quan con người của chúng là không mong muốn (Freitag et al., 2022). (2) Chỉ số dựa trên embedding: dòng phương pháp này tận dụng một mô hình ngôn ngữ được pre-trained (ví dụ: BERT (Devlin et al., 2019)) để đo lường sự tương tự giữa word embedding của văn bản ứng cử viên và tham chiếu (Zhang* et al., 2020; Zhao et al., 2019). Hạn chế chính của chúng nằm ở paradigm dựa trên sự tương tự và sự phụ thuộc cao vào chất lượng và tính đa dạng của tham chiếu. (3) Bộ đánh giá neural được huấn luyện: nghiên cứu gần đây hơn tập trung vào việc chuyên biệt hóa các PLM bằng cách fine-tuning trên nhãn con người (Rei et al., 2020) hoặc tổng hợp (Zhong et al., 2022), hoặc pretraining trên các tài liệu liên quan đến miền (Yuan et al., 2021). Tuy nhiên, các chỉ số này hoặc tập trung vào một chiều duy nhất (Wang et al., 2020a; Huang et al., 2020) hoặc bị hạn chế trong sự liên quan con người (Mehri & Eskenazi, 2020; Zhong et al., 2022).

Đánh giá NLG dựa trên LLM Với sự xuất hiện của LLM, các công trình nghiên cứu gần đây tập trung vào các bộ đánh giá dựa trên LLM với khả năng tuân theo hướng dẫn và tổng quát hóa đầy hứa hẹn của chúng. Một dòng công trình đầu tiên đi qua các khám phá sơ bộ về các bộ đánh giá dựa trên LLM, bao gồm các phương pháp prompting và biến thể mô hình (Fu et al., 2023; Kocmi & Federmann, 2023; Wang et al., 2023a; Chen et al., 2023; Liu et al., 2023). Nghiên cứu tiếp theo tập trung vào các khía cạnh khác nhau của việc cải thiện các bộ đánh giá dựa trên LLM, bao gồm tính thực tế (Min et al., 2023), khả năng diễn giải (Lu et al., 2023), giảm thiểu thiên vị vị trí (Wang et al., 2023b), và thỏa thuận với đánh giá con người (Zheng et al., 2023). Khác với các phương pháp trên, chúng tôi tập trung vào một phương pháp tổng quát để hiệu chỉnh một LLM có sẵn với các phương pháp không cần gradient, để cải thiện sự liên kết của nó với sở thích con người trên một tác vụ mong muốn.

7 KẾT LUẬN

Trong công trình này, chúng tôi tập trung vào một câu hỏi quan trọng: làm thế nào để hiệu chỉnh và liên kết một bộ đánh giá dựa trên LLM có sẵn hướng tới sự liên kết con người theo cách không cần gradient. Đầu tiên chúng tôi có một sự hồi tưởng vào các bộ đánh giá NLG dựa trên LLM hiện tại và khám phá ra chúng gặp khó khăn từ prompting không đủ, trong đó các hướng dẫn chấm điểm vắng mặt và chỉ các không gian đầu ra được cung cấp, dẫn đến các đánh giá không nhất quán và không liên kết. Chúng tôi nhấn mạnh tầm quan trọng của tiêu chí chấm điểm liên kết như một sự đồng thuận giữa con người và LLM và đề xuất AUTOCALIBRATE để tự động hiệu chỉnh một bộ đánh giá dựa trên LLM thông qua soạn thảo và tinh chỉnh tiêu chí. Được suy ra từ nhãn chuyên gia con người và được tinh chỉnh theo các mẫu không liên kết trước đó bởi LLM, các tiêu chí được tuyển chọn bởi AUTOCALIBRATE chứng minh những cải thiện đáng kể trong tương quan con người trên việc đánh giá tóm tắt văn bản, dữ liệu thành văn bản, và ảo giác. Phân tích định tính của chúng tôi truyền đạt những trực giác và quan sát sâu sắc về bản chất của tiêu chí chấm điểm hiệu quả.

8 THẢO LUẬN

Hạn chế và tác động rộng lớn Công trình này nghiên cứu về việc hiệu chỉnh một bộ đánh giá dựa trên LLM mạnh hướng tới sự liên kết con người tốt hơn. Ngoài kỹ thuật prompt thủ công, AUTOCALIBRATE tự động hóa quá trình hiệu chỉnh của các bộ đánh giá dựa trên LLM và cung cấp một nghiên cứu thí nghiệm đầu tiên về cách các bộ đánh giá dựa trên LLM tiếp theo có thể được tăng cường với prompting tốt hơn. Chúng tôi hình dung AUTOCALIBRATE có thể được áp dụng cho một phổ tác vụ rộng hơn trong NLG và xa hơn nữa.

Hạn chế chính là chỉ tiêu chí được khai thác để cải thiện sự liên kết. Sau khi phân tích cẩn thận các prompt, chúng tôi kết luận rằng tiêu chí là quan trọng nhất, vì chúng có tính nhân quả nhất đối với điểm số được đưa ra, và có thể được coi là một sự đồng thuận chung giữa con người và LLM do dạng ngôn ngữ tự nhiên của chúng. Thêm vào đó, phần tiêu chí là khó tuyển chọn nhất so với các phần khác của mẫu prompt (ví dụ: thang chấm điểm, định nghĩa tác vụ), mà chúng tôi chủ yếu tập trung vào. Bên cạnh đó, một nghiên cứu toàn diện hơn về việc thúc đẩy và đánh giá các thành phần khác của prompt để hiệu chỉnh một bộ đánh giá dựa trên LLM, và thích ứng nó với các tác vụ và ngôn ngữ rộng hơn được mở ra cho công việc tương lai.

--- TRANG 12 ---
TÀI LIỆU THAM KHẢO

Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, và Ruifeng Xu. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. arXiv preprint arXiv:2304.00723, 2023.

Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing, và Zhiting Hu. Compression, transduction, and creation: A unified framework for evaluating natural language generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7580–7605, 2021.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186. Association for Computational Linguistics, 2019.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, và Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, và Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, và Dragomir Radev. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391–409, 2021.

Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, và André FT Martins. Results of wmt22 metrics shared task: Stop using bleu–neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 46–68, 2022.

Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, và Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.

Max Grusky, Mor Naaman, và Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of NAACL-HLT, pp. 708–719, 2018.

Lishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, và Xiaodan Liang. Grade: Automatic graph-enhanced coherence metric for evaluating open-domain dialogue systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9230–9240, 2020.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, và Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38, 2023.

Tom Kocmi và Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520, 2023.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004.

Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, và Graham Neubig. Explainaboard: An explainable leaderboard for nlp. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 280–289, 2021.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, và Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.

Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, và Dacheng Tao. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. arXiv preprint arXiv:2303.13809, 2023.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, và cộng sự. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.

Shikib Mehri và Maxine Eskenazi. Usr: An unsupervised and reference free evaluation metric for dialog generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 681–707, 2020.

Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, và Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.

Shashi Narayan, Shay B Cohen, và Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797–1807, 2018.

OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, và cộng sự. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311–318, 2002.

Ricardo Rei, Craig Stewart, Ana C Farinha, và Alon Lavie. Comet: A neural framework for mt evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2685–2702, 2020.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

Brian Thompson và Matt Post. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 90–121, 2020.

Ramakrishna Vedantam, C Lawrence Zitnick, và Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566–4575, 2015.

Alex Wang, Kyunghyun Cho, và Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5008–5020, 2020a.

Alex Wang, Kyunghyun Cho, và Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5008–5020, 2020b.

Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, và Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048, 2023a.

Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, và Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, và cộng sự. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-Hao Su, David Vandyke, và Steve Young. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1711–1721, 2015.

Weizhe Yuan, Graham Neubig, và Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263–27277, 2021.

Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, và Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.

Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, và Steffen Eger. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 563–578, 2019.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, và Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697–12706. PMLR, 2021.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, và cộng sự. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.

Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, và Jiawei Han. Towards a unified multi-dimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2023–2038, 2022.

--- TRANG 13 ---
[Tiếp tục dịch phần còn lại...]

A CHIẾN LƯỢC ĐÁNH GIÁ

Trong phần này, chúng tôi giới thiệu các chiến lược meta-đánh giá để đánh giá sự liên kết con người được áp dụng trong công trình này. Chúng tôi chọn các chiến lược đánh giá chủ yếu theo các công trình trước đây (Zhong et al., 2022; Fu et al., 2023; Liu et al., 2023). Cho một tập dữ liệu D bao gồm các mẫu NLG từ M hệ thống đa dạng và J mẫu văn bản nguồn, chỉ số đánh giá f(·) (ví dụ: BLEU (Papineni et al., 2002)) và chỉ số tương quan g(·), chúng tôi có thể thực hiện meta-đánh giá ở cấp độ mẫu hoặc tập dữ liệu.

Cấp độ mẫu Đối với meta-đánh giá cấp độ mẫu, đầu tiên chúng tôi tính toán các giá trị tương quan trên nhiều phản hồi ứng cử viên (từ mỗi hệ thống) đến một mẫu riêng lẻ, sau đó lấy trung bình trên tất cả các mẫu:

f_sample = (1/J) ∑_{i=1}^J (g([ŝ_{i,1}, ..., ŝ_{i,M}], [s_{i,1}, ..., s_{i,M}])), (4)

trong đó ŝ_{u,v} và s_{u,v} biểu thị kết quả đánh giá (nếu không, chuyển đổi thành giá trị số) cho phản hồi thứ v đối với mẫu thứ u từ bộ đánh giá f(·) và các chuyên gia con người, tương ứng.

Cấp độ tập dữ liệu Đối với meta-đánh giá cấp độ tập dữ liệu, chúng tôi đánh giá tương quan trên tất cả các mẫu trong tập dữ liệu (với tổng cộng M×J mẫu), như sau:

f_dataset = g([ŝ_{i,1}, ..., ŝ_{J,M}], [s_{i,1}, ..., s_{J,M}]). (5)

B VỀ HIỆU SUẤT CỦA VIỆC THÊM CHAIN-OF-THOUGHTS

Chain-of-thought (CoT) (Wei et al., 2022) prompting làm nổi bật lý luận trong các mô hình ngôn ngữ lớn bằng cách khuyến khích các mô hình tạo ra lý do của chúng trước khi có được câu trả lời. Như được nghiên cứu trong nghiên cứu gần đây (Liu et al., 2023), chain-of-thoughts có lợi cho việc cải thiện sự liên kết con người trong đánh giá NLG, nếu được kết hợp trong mẫu prompt chấm điểm. Do đó, chúng tôi nghiên cứu liệu AUTOCALIBRATE có thể hưởng lợi thêm từ việc thêm CoT vào các prompt chấm điểm đã hiệu chỉnh của chúng tôi.

Để có được CoT cho mỗi khía cạnh chấm điểm, chúng tôi tuân theo Liu et al. (2023), và kết quả được minh họa trong Bảng 8. Như được thể hiện trong hình, việc thêm CoT vào các prompt đã hiệu chỉnh của chúng tôi mang lại sự khác biệt không đáng kể. Chúng tôi suy đoán tính hiệu quả của 'CoT' bị giảm thiểu bởi việc cung cấp tiêu chí chấm điểm thông tin và hướng dẫn. Trái ngược với toán học, việc đánh giá chất lượng văn bản không phải là một quá trình lý luận dây chuyền nghiêm ngặt, vì vậy việc cung cấp CoT về cơ bản là làm rõ các rubric đánh giá, điều này phù hợp với ý nghĩa của tiêu chí trong bài báo này, và do đó không đạt được lợi ích bổ sung. Có thể, 'CoT' ở đây hoạt động để làm sáng tỏ các quy tắc chấm điểm, thay vì cung cấp đường dẫn lý luận để tuân theo.

C CHI TIẾT CẤU HÌNH

Trong phần này, chúng tôi liệt kê chi tiết cấu hình của AUTOCALIBRATE cho mỗi thí nghiệm. Các cấu hình chi tiết cho AUTOCALIBRATE được liệt kê trong Bảng 9. Chúng tôi áp dụng cùng một tập hợp cấu hình cho mỗi trong hai tập dữ liệu trong một tác vụ.

D DANH SÁCH CÁC MẪU PROMPT

Trong phần này, chúng tôi liệt kê các mẫu prompt được áp dụng trong suốt nghiên cứu này, bao gồm các mẫu quy nạp cho soạn thảo tiêu chí, các mẫu đánh giá sử dụng tiêu chí chấm điểm được tạo ra, và các mẫu cho việc tự-tinh chỉnh tiêu chí.

D.1 CÁC MẪU SOẠN THẢO TIÊU CHÍ

Các mẫu prompt cho soạn thảo tiêu chí được liệt kê trong Hình 5, 6 và 7. [Aspect] biểu thị chỗ dành cho các khía cạnh cần đánh giá (ví dụ: mạch lạc, nhất quán, v.v.), và các mẫu trong ngữ cảnh few-shot được lấy mẫu được đặt tại [In-Context Few-Shot Samples], bao gồm các mẫu và điểm số chuyên gia của chúng.

D.2 CÁC MẪU ĐÁNH GIÁ

Các mẫu prompt cho đánh giá được liệt kê trong Hình 8, 9 và 10. [Aspect] biểu thị chỗ dành cho các khía cạnh cần đánh giá (ví dụ: mạch lạc, nhất quán, v.v.). Các mẫu đánh giá và tiêu chí chấm điểm đã hiệu chỉnh cho mỗi khía cạnh được điền vào các chỗ dành tương ứng trong quá trình đánh giá.

D.3 CÁC MẪU TINH CHỈNH TIÊU CHÍ

Một mẫu prompt ví dụ cho tinh chỉnh tiêu chí có thể được tìm thấy trong Hình 11. Như được minh họa trong hình, đầu tiên chúng tôi điền khía cạnh và tác vụ vào hướng dẫn, sau đó prompt LLM với tiêu chí trước đó, các mẫu trong ngữ cảnh few-shot của các đánh giá không liên kết, cùng với các phương tiện sửa đổi được đề xuất để có được phiên bản sửa đổi của tiêu chí chấm điểm cho tác vụ này.

E NGHIÊN CỨU TRƯỜNG HỢP MỞ RỘNG

E.1 DANH SÁCH CÁC TIÊU CHÍ

Trong phần này, chúng tôi trình bày một nghiên cứu trường hợp về tiêu chí chấm điểm được tạo ra bởi AUTOCALIBRATE cho mỗi khía cạnh đánh giá của mỗi benchmark trong suốt nghiên cứu này trong Bảng 10, 11, 12 và 13. Như được minh họa trong các bảng, tiêu chí chấm điểm được tạo ra với AUTOCALIBRATE có tính thông tin, bao phủ các rubric đáng kể để đánh giá một khía cạnh đã cho của tác vụ NLG mục tiêu.

[Tiếp tục với các bảng 10-13 chứa các tiêu chí chi tiết cho từng khía cạnh đánh giá]
