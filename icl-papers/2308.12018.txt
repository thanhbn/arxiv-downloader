# 2308.12018.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl-papers/2308.12018.pdf
# File size: 597396 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Bias-Aware Minimisation: Understanding and
Mitigating Estimator Bias in Private SGD
Moritz Knolle1,2, Robert Dorfman3, Alexander Ziller1, Daniel Rueckert1,2,4and Georgios Kaissis1,2,4
1Institute for AI in Medicine, Technical University of Munich
2Konrad Zuse School for Excelllence in Reliable AI
3V7 Labs
4Imperial College London
August 24, 2023
Abstract
Differentially private SGD (DP-SGD) holds the promise of
enabling the safe and responsible application of machine learn-
ing to sensitive datasets. However, DP-SGD only provides a
biased, noisy estimate of a mini-batch gradient. This renders
optimisation steps less effective and limits model utility as a
result. With this work, we show a connection between per-
sample gradient norms and the estimation bias of the private
gradient oracle used in DP-SGD. Here, we propose Bias-Aware
Minimisation (BAM) that allows for the provable reduction
of private gradient estimator bias. We show how to efficiently
compute quantities needed for BAM to scale to large neural
networks and highlight similarities to closely related methods
such as Sharpness-Aware Minimisation. Finally, we provide
empirical evidence that BAM not only reduces bias but also sub-
stantially improves privacy-utility trade-offs on the CIFAR-10,
CIFAR-100, and ImageNet-32 datasets.
1 Introduction
The application of machine learning models to datasets con-
taining sensitive information necessitates meaningful privacy
guarantees, as it has been shown that the public release of
unsecured models trained on sensitive data can pose serious
threats to data-contributing individuals [ 1,2,3]. To this end,
differential privacy (DP) [ 4], the widely accepted gold stan-
dard approach to privacy-preserving data analysis, has been ex-
tended to many machine learning methods. Most significantly,
for modern machine learning, differentially private stochastic
gradient descent (DP-SGD) [ 5,6] has enabled the application
of powerful deep neural networks to sensitive datasets with
meaningful privacy guarantees.
In practice, however, DP-SGD comes with a substantial utility
penalty. As we demonstrate below, this is because DP-SGD
only provides a noisy, biased gradient estimate, causing opti-
mization to settle in regions of the loss landscape that yield
poorly performing models. We show that (ignoring sampling
noise), the variance of a private gradient estimate is fixed while
its bias is not. In this work, we attempt to improve private
gradient estimators by minimising their bias. The bias of ang1
clip(g1)
clip(g2)ˆg
ˆgclip
(a)Bias(ˆgpriv,ˆg,)
(b)
Figure 1: (a)Clipping per-sample gradients g1andg2yields a
biased gradient estimate ˆgclipof the true mini-batch gradient
ˆgin expectation. Below we show this bias, represented by the
vector Bias (ˆgpriv,ˆg,)in(b), to depend on ∥g1∥2and∥g2∥2.
estimate ˆpof the true value punderlying a process generating
observations xis defined as:
Bias(ˆ p, p) =Ex[ˆp]−p. (1)
Thus, when an estimator has zero bias, it is called unbiased
and its expected value is equal to the quantity being estimated.
When estimating gradients, unbiasedness is a useful property as
it implies that gradient estimates –in expectation– point towards
the correct descent direction.
Bias in DP-SGD results from the per-sample gradient clipping
operation, used to enforce a sensitivity bound on the gradients
(illustrated above in Fig. 1). However, despite its importance,
the nature of this bias and how it affects parameter updates
has remained poorly understood. With this work, we show
that private gradient estimator bias is intimately related to the
per-sample gradient norms and can further be decomposed
into a magnitude anddirectional component (See Appendix
C). We develop Bias-Aware Minimisation ( BAM ) and provide
empirical evidence that our method effectively (I) reduces bias
magnitude and (II) increases classification performance on a
range of challenging vision benchmarks.
1arXiv:2308.12018v1  [cs.LG]  23 Aug 2023

--- PAGE 2 ---
2 Related Work
The bias-variance tradeoff of the private gradient oracle was
first discussed in [ 7]. More generally, bias associated with
gradient clipping in SGD has been studied by Zhang et al.
[8], Qian et al. [9]and also in the private setting, where Song
et al. [10] demonstrated the importance of the correct choice
of clipping threshold in convex models, while Chen et al. [11]
showed that no bias is incurred when a symmetricity condition
on the gradient distribution holds.
Concurrent work [ 12,13] has provided empirical evidence that
Sharpness-Aware-Minisation (SAM) [ 14] can improve privacy-
utility tradeoffs. Park et al. [12] suggest that the magnitude
of the bias vector (referred to as the effect of clipping in their
work) is bounded by the sharpness. With this work, we provide
evidence that sharpness, while closely related, might not be
the root cause determining private gradient bias but rather the
per-sample gradient norms in a mini-batch.
3 Preliminaries
Differential privacy [15] (DP), is a stability notion on ran-
domised mechanisms over sensitive databases. Let DandD′
be two databases (datasets) that differ exactly in one individ-
ual’s data. We denote this relationship (i.e. database adjacency)
through the symbol ≃and use the standard remove/add one
relation throughout.
Definition 1 (Differential privacy) .A randomised mechanism
Mexecuted on the result of a query function q(i.e.M(q(·)))
preserves (ε,δ)-DP if, for all pairs of adjacent databases D
andD′, and all subsets SofM(q(·))’s range:
p[M(q(D))∈ S]≤eεp[M(q(D′))∈ S] +δ, (2)
where the relationship between DandD′is symmetric. The
guarantee is given over the randomness of M.
To achieve (ε, δ)-DP using Gaussian noise perturbation (i.e. the
Gaussian mechanism M), one needs to calibrate the magnitude
of the noise to the query function’s (global) sensitivity:
∆2(q) = sup
D≃D′∥q(D)−q(D′)∥2,
or∆for short. Since obtaining a (non-vacuous) sensitivity
bound is generally not possible for deep neural network gra-
dients, it is common practice to enforce a bound manually by
projecting per-sample gradients to the L2-ball [ 6]. Together,
this operation, more commonly known as clipping , followed
by appropriately scaled Gaussian perturbation, then yields the
DP-SGD algorithm, which can be thought of as simply query-
ing a private gradient oracle at each optimisation step as seen
in Algorithm 1:
Where lis the expected batch size due to the sub-sampling am-
plification requirement that Bis constructed through a Poisson
sample of the Dataset D.Algorithm 1 Private gradient oracle ψ
Input: (Poisson-sampled) mini-batch B =
{(x1, y1), ...,(xl, yl)}, Clipping Bound C, noise multi-
plierσ, loss function L, parameters θ∈Rd
Output ; private gradient estimate
1:for(xi, yi)inBdo
2: gi← ∇ θL(fθ(xi), yi)
3: ¯gi←gi/max(1 ,∥gi∥2
C) ▷clip
4:ˆgpriv←1
lPl
i=1¯gi+N(0, σ2C2Id)
▷perturb
5:return ˆgpriv
3.1 Setting
We focus on supervised learning where, given a dataset D=
{(xi, yi), ...,(xn, yn)}drawn i.i.d from a product distribution
X ×Y , we wish to find a mapping f:X → Y , realised through
a neural network. This neural network has parameters θ∈Θ∈
Rdand is trained by minimising the empirical loss function
L(θ) :=1
nPn
i=1L(θ, xi, yi)using DP-SGD, whereby at each
optimisation step the private gradient oracle is queried to obtain
a privatised gradient estimate.
θ(t+1)=θ(t)−γ(t)ˆg(t)
priv (3)
ˆg(t)
priv=ψ(B, C, σ, L, θt) (4)
We drop the superscript tfor notational simplicity, assuming
we are on step tin the following analyses. We further denote
the clipped minibatch gradient as ˆgclip= 1/lPl
i¯gi.
4Clipping dominates private gradient
bias
Since we are interested in obtaining an unbiased version of
DP-SGD, we first study the bias introduced by constructing
a private estimate for the mini-batch gradient ˆg. Our analysis
focuses on the quantity:
Bias(ˆgpriv,ˆg) =E[ˆgpriv]−ˆg,
where we view ˆgas fixed, that is, the gradient constructed
for an already observed batch of data to be used in a step of
conventional SGD. This enables us to isolate the bias introduced
through private estimation of ˆg. Note that when using Ewithout
a subscript, we take the expectation over all randomness present.
In the case of ˆgpriv, this randomness is –by assumption– only
due to the Gaussian mechanism. We first observe that this
gradient perturbation does not introduce additional bias:
Lemma 1. The bias of the private gradient estimate,
Bias( ˆgpriv,ˆg), is unaffected by the noise addition in the Gaus-
sian mechanism. That is,
Bias(ˆ gpriv,ˆg) = Bias(ˆ gclip,ˆg)
2

--- PAGE 3 ---
Proof.
Bias(ˆ gpriv,ˆg) =Eh1
llX
i=1
clip(gi) +N(0, σ2C2Id)i
−ˆg
(5)
=1
llX
i=1
clip(gi) +E
N(0, σ2C2Id)
−ˆg (6)
=ˆgclip−ˆg (7)
=Bias(ˆ gclip,ˆg) (8)
This holds since ˆgandˆgclipare viewed as constructed from
an observed mini-batch of data and the zero centred Gaussian
random variables are independent of one another.
Thus, private gradient bias is caused by the clipping operation
alone. Next, we develop an objective function that provably
minimises the aforementioned bias.
5 A bias-aware objective
We propose LBAO, an objective that, when minimised, prov-
ably reduces the bias of private gradient estimates ˆgprivby
encouraging small per-sample gradient norms:
LBAO(θ, x, y ) =L(θ, x, y )|{z}
original loss+λ 
1
llX
i=1∥gi∥2!
| {z }
regularising term(9)
which can be sub-sampled as follows:
LBAO(θ, xi, yi) =L(θ, xi, yi) +λ∥gi∥2(10)
To motivate this optimisation objective, we will now demon-
strate the primary dependence of Bias( ˆgpriv,ˆg)on the per-
sample gradient norms for a fixed clipping threshold C.
Lemma 2. A smaller per-sample gradient norm ∥gi∥2of the
i-th sample in a mini-batch decreases Bias (ˆgpriv,ˆg).
Proof. First note that
clip(gi) =gi
max(1 ,∥gi∥2)
C)=(
gi if∥gi∥2≤C
Cgi
∥gi∥2if∥gi∥2> C.
Thus, if ∥gi∥2≤Cfor every iin the mini-batch, then by
Lemma 1:
Bias(ˆ gpriv,ˆg) =1
llX
i=1gi−ˆg= 0.
That is, if the L2-norm of the gradients of the mini-batch are
all below the clipping threshold, the bias of the private gradient
estimate reduces to zero. On the other hand, if ∥gi∥2> C:
Bias(ˆ gpriv,ˆg) =1
llX
i=1C
∥gi∥2gi−ˆg. (11)Therefore, the bias is dependent on the ratiosC
∥gi∥2∀i: as a
per-sample gradient norm ∥gi∥2gets increasingly larger than
C, the bias grows (note that here “grows” means gets further
from the origin). Conversely, if we minimise ∥gi∥2for any i,
the bias will shrink until all ∥gi∥2in the mini-batch are less
thanC, at which point the estimate becomes unbiased.
Notice that (9)is similar to the gradient norm penalty, used by
[16] to encourage flatness of the loss landscape:
LZ(θ) =L(θ) +∥∇θL(θ)∥2, (12)
It can be shown that (9)upper-bounds LZ(see Appendix B).
This means that a reduction in LBAO implies a reduction in LZ,
but the converse is not generally true. Note also that LZdoes
not allow for the required per-sample DP analysis, precluding
the simple application of DP-SGD [12].
5.1 Efficient Computation
Na¨ıvely implementing LBAO is problematic, as the gradient
computation now involves computing a Hessian-Vector Product
(HVP) for every sample:
∇θLBAO(θ, x, y ) =∇θL(θ, x, y )+
λ1
nlX
i=1∇2
θL(θ, xi, yi)∇θL(θ, xi, yi)
∥∇θL(θ, xi, yi)∥2| {z }
HVP.(13)
This objective is thus expensive to compute for deep networks
using reverse-mode automatic differentiation (AD) [ 17]. Fortu-
nately, however, prior work has shown that HVPs can be com-
puted efficiently, in this case, either exactly using a combination
of forward and reverse mode AD [ 18,19], or approximately
using the local gradient ascent step of SAM [16].
6 Method
To reduce the bias of the private gradient oracle in DP-SGD,
we optimise our bias-aware objective LBAO and approximate
the necessary per-sample gradients. Concretely, we perform the
local gradient ascent step of SAM, at the sample level before
computing the gradient:
∇θLBAO(θ, xi, yi)≈ ∇ θL(θ, xi, yi)
θ=θ+λ∇θL(θ,xi,yi)
∥∇θL(θ,xi,yi)∥2.
(14)
The entire training procedure is summarised in Algorithm 2.
Note that, since ∇θLBAO(θ, xi, yi)only depends on sample-
level statistics, the sampling process is identical to DP-SGD
and all quantities are privatised as in DP-SGD, every itera-
tiontof Algorithm 2 satisfies (ε, δ)-DP with identical privacy
parameters as DP-SGD.
3

--- PAGE 4 ---
Dataset ε δ DP-SGD DP-SAT BAM (ours)
CIFAR-101.0
10−560.9±0.49 60 .9±0.6261.4±0.48
2.0 67 .1±0.10 67 .2±0.3068.2±0.27
10.0 78 .6±0.08 78 .1±0.6979.7±0.13
CIFAR-1001.0
10−518.1±0.10 18 .2±0.1318.5±0.04
2.0 24 .9±0.46 24 .9±0.3525.4±0.40
10.0 40 .3±0.21 40 .1±0.1940.8±0.06
ImageNet32 10.0 8×10−714.97 14 .70 20.67
Table 1: Test accuracy (mean ±SD %) for CIFAR-10 ,CIFAR-100 andImageNet32 computed over three random seeds
at different (ε, δ). Due to computational resource constraints, we report only a single training run for Imagenet32.
Algorithm 2 Bias-Aware Minimisation (BAM)
1:fort∈1,2, ..., T do
2: B←Poisson sample of Dwith probability q
3: for(xi, yi)∈Bdo
4: θ′(t)←θ(t)+λ(t)∇θL(θ(t),xi,yi)
∥∇θL(θ(t),xi,yi)∥2▷SAM step
5: g(t)
i← ∇ θ′LBAO(θ′(t), xi, yi)
6: ¯g(t)
i←g(t)
i/max
1,g(t)
i
2
C
▷clip
7: ˆg(t)
priv=1
lPl
ih
¯g(t)
i+N(0, σ2C2Id)i
▷perturb
8: θ(t+1)=θ(t)−γ(t)ˆg(t)
priv
7 Results
To evaluate our proposed approach and compare its perfor-
mance to DP-SGD and DP-SAT [ 12], we perform a range of
experiments on challenging computer vision datasets. Results
forCIFAR-10 /100[20] and ImageNet32 [21] are reported above
in Table 1. We employ state-of-the-art (SOTA) training prac-
tices for DP-SGD [ 22], namely: weight standardisation, group
normalisation, large batch sizes and augmentation multiplicity.
Full experimental details and hyperparameter values can be
found in Table 2, Appendix A.
0 200 400 600 800
Step0246810/bardblBias( ˆgpriv,ˆg)/bardbl2BAM
DP-SAT
DP-SGD
Figure 2: BAM effectively minimises bias: The magnitude of
the bias vector measured on CIFAR-10 at a batch size of 512is
substantially lower for BAM, while DP-SAT incurs the same
bias as DP-SGD.
We find that BAM effectively minimises private gradient bias in
practice (see Fig. 2 above). On the other hand, DP-SAT largelyhas no effect on estimator bias for values of the regularisation
parameter λthat yield high-performing models. Further empir-
ical run-time comparisons (data shown in Appendix A) reveal
that while both exact and approximate gradient computations
for BAM do incur higher computational costs than DP-SAT
(and DP-SGD), this burden is manageable for most practically
sized networks with less than 200 layers. Finally, our empirical
performance comparison reveals that when using very large
batch sizes and the other SOTA practices of De et al. [22], per-
formance gains on more challenging datasets realised through
DP-SAT are smaller than reported in the original publication
[12]. In contrast, BAM consistently improves performance
across different privacy budgets (Table 1).
8 Discussion
After deriving the bias of the private gradient oracle from first
principles, we developed a bias-aware regularisation objective
LBAO that was empirically confirmed effective at minimising
the bias vector associated with private gradient estimation. We
further demonstrated that using the SAM approximation as
suggested by [ 16], our objective and its gradient are computable
with manageable computational overhead.
Our method performs the local gradient ascent step (SAM step)
at the per-sample level and is thus closely related to the DP-
SAT method of [ 12], which performs the ascent step with the
previous iteration’s privatised mini-batch gradient. Our experi-
ments show that our method outperforms DP-SAT at minimis-
ing bias, which is mirrored in the superior accuracy of models
trained with BAM. Notably, our method yields a more than
5%accuracy increase on the most challenging dataset tested,
ImageNet32. Based on our results, we hypothesise that, with
modern private training practices [ 22] (very large batch size,
large learning rate), the noisy gradient of the previous iteration
(ˆg(t−1)
priv ), is a poor approximation for finding the maximum loss
in the local neighbourhood around the current iteration’s pa-
rameter value θ(t). This is corroborated by a simple experiment
(data shown in Fig. 4, Appendix A) that shows the ascent step
vector used in DP-SAT to point in slightly different directions
to the current iteration’s (un-privatised) mini-batch gradient
conventionally used in SAM.
Our results, while not without limitations (BAM incurs slight
computational overhead), have shown that reducing private
4

--- PAGE 5 ---
gradient bias can lead to effective performance increases on
challenging image datasets. More extensive empirical eval-
uation, especially on large-scale datasets such as (full-size)
ImageNet, is ongoing work and required to fully assess its ben-
efits. Finally, future work should investigate the effectiveness
of alternative methods that encourage smoothness [ 23] and look
into connections to and impact on model fairness with respect
to sub-groups [24].
9 Acknowledgments
This paper was supported by the DAAD programme Konrad
Zuse Schools of Excellence in Artificial Intelligence, sponsored
by the Federal Ministry of Education and Research.
References
[1]Jonas Geiping, Hartmut Bauermeister, Hannah Dr ¨oge,
and Michael Moeller. Inverting gradients-how easy is it to
break privacy in federated learning? Advances in Neural
Information Processing Systems , 33:16937–16947, 2020.
[2]Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
Jagielski, Ariel Herbert-V oss, Katherine Lee, Adam
Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al.
Extracting training data from large language models. In
30th USENIX Security Symposium (USENIX Security 21) ,
pages 2633–2650, 2021.
[3]Nicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej
Kos, and Dawn Song. The secret sharer: Evaluating and
testing unintended memorization in neural networks. In
28th USENIX Security Symposium (USENIX Security 19) ,
pages 267–284, 2019.
[4]Cynthia Dwork, Aaron Roth, et al. The algorithmic foun-
dations of differential privacy. Foundations and Trends ®
in Theoretical Computer Science , 9(3–4):211–407, 2014.
[5]Reza Shokri and Vitaly Shmatikov. Privacy-preserving
deep learning. In Proceedings of the 22nd ACM SIGSAC
conference on computer and communications security ,
pages 1310–1321, 2015.
[6]Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan
McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In Proceedings
of the 2016 ACM SIGSAC conference on computer and
communications security , pages 308–318, 2016.
[7]H Brendan McMahan, Daniel Ramage, Kunal Talwar, and
Li Zhang. Learning differentially private recurrent lan-
guage models. In International Conference on Learning
Representations , 2018.
[8]Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jad-
babaie. Why gradient clipping accelerates training: A
theoretical justification for adaptivity. In International
Conference on Learning Representations , 2019.[9]Jiang Qian, Yuren Wu, Bojin Zhuang, Shaojun Wang, and
Jing Xiao. Understanding gradient clipping in incremental
gradient methods. In International Conference on Artifi-
cial Intelligence and Statistics , pages 1504–1512. PMLR,
2021.
[10] Shuang Song, Om Thakkar, and Abhradeep Thakurta.
Characterizing private clipped gradient descent on
convex generalized linear problems. arXiv preprint
arXiv:2006.06783 , 2020.
[11] Xiangyi Chen, Steven Z Wu, and Mingyi Hong. Under-
standing gradient clipping in private sgd: A geometric
perspective. Advances in Neural Information Processing
Systems , 33:13773–13782, 2020.
[12] Jinseong Park, Hoki Kim, Yujin Choi, and Jaewook Lee.
Differentially private sharpness-aware training. In Inter-
national Conference on Machine Learning , pages XXXX–
YYYY . PMLR, 2023.
[13] Yifan Shi, Yingqi Liu, Kang Wei, Li Shen, Xueqian Wang,
and Dacheng Tao. Make landscape flatter in differen-
tially private federated learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 24552–24562, 2023.
[14] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and
Behnam Neyshabur. Sharpness-aware minimization for
efficiently improving generalization. arXiv preprint
arXiv:2010.01412 , 2020.
[15] Cynthia Dwork, Aaron Roth, et al. The algorithmic foun-
dations of differential privacy. 2014.
[16] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing
gradient norm for efficiently improving generalization in
deep learning. In International Conference on Machine
Learning , pages 26982–26992. PMLR, 2022.
[17] Ryo Karakida, Tomoumi Takase, Tomohiro Hayase, and
Kazuki Osawa. Understanding gradient regularization in
deep learning: Efficient finite-difference computation and
implicit bias. arXiv preprint arXiv:2210.02720 , 2022.
[18] Alex Wiltschko and Matthew Johnson. Jax- au-
todiff cookbook, n.d. Accessed June 20, 2023.
https://jax.readthedocs.io/en/latest/notebooks/
autodiff cookbook.html.
[19] Barak A Pearlmutter. Fast exact multiplication by the
hessian. Neural computation , 6(1):147–160, 1994.
[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multi-
ple layers of features from tiny images. 2009.
[21] Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A
downsampled variant of imagenet as an alternative to the
cifar datasets. arXiv preprint arXiv:1707.08819 , 2017.
[22] Soham De, Leonard Berrada, Jamie Hayes, Samuel L
Smith, and Borja Balle. Unlocking high-accuracy differ-
entially private image classification through scale. arXiv
preprint arXiv:2204.13650 , 2022.
5

--- PAGE 6 ---
[23] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol
Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park.
Swad: Domain generalization by seeking flat minima.
Advances in Neural Information Processing Systems , 34:
22405–22418, 2021.
[24] Cuong Tran, My Dinh, and Ferdinando Fioretto. Dif-
ferentially private empirical risk minimization under the
fairness lens. Advances in Neural Information Processing
Systems , 34:27555–27565, 2021.
[25] Timothy Dozat. Incorporating nesterov momentum into
adam. 2016.
[26] James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
[27] Sepp Hochreiter and J ¨urgen Schmidhuber. Flat minima.
Neural computation , 9(1):1–42, 1997.
AExperimental details & further re-
sults
To obtain suitable hyperparameter values for the compared
methods (reported above in Table 2), a random search was first
employed using DP-SGD, after which a separate search for op-
timal λ-values was performed for both DP-SAT and BAM with
N=200 random trials each. For Imagenet32, only a very small-
scale hyperparameter search was employed due to the high
computational cost. Augmentation multiplicity, as described
in [22], was employed with probability p= 0.5(for each aug-
mentation) across all compared methods with random pixel
shifts (up to 4pixels) and random vertical flips. All models
were trained for 75epochs with the NAdam[ 25] optimizer and,
besides the learning rate (LR), otherwise, default hyperparame-
ters. To meet the different (ε, δ)privacy budget requirements
the noise multiplier σ2was adjusted accordingly.
A.1 Computational aspects
0 100 200 300 400 500
Depth0.00.20.40.6Wall-time(s)/step
DP-SGD
BAM (FWD-REV)
BAM (SAM)
DP-SAT
Figure 3: Per-step wall-time (mean ±SD) for different compu-
tational approaches to bias mitigation in DP-SGD for increasing
network depth. Results were computed over ten trials and five
repetitions.We evaluate the empirical run-time complexity of the two pre-
viously mentioned approaches to compute ∇θLBAO(θ, xi, yi).
Concretely, we compare forward-over-reverse mode AD (BAM
FWD-REV) and the SAM (BAM SAM) approximation to com-
pute necessary gradients for BAM and compare to DP-SAT
[12]. DP-SAT uses the previous step’s privatised mini-batch
gradient to approximate (12) at no additional privacy cost and
little computational overhead. Figure 3 showcases the results
of an empirical run-time comparison on a toy dataset, imple-
mented in jax [26] and compiled with XLA to obtain a fair
comparison between methods.
A.2 Gradient ascent step effectiveness
To investigate the effectiveness the gradient ascent step in DP-
SAT and BAM, we investigate the cosine similarity between the
ascent direction used by the respective method and the (ground-
truth) ascent direction as used in SAM, that is the un-privatised,
current mini-batch gradient. Our findings (below in Fig. 4)
indicate that the per-sample gradient ascent step of DP-SAT is
substantially better aligned with the non-private SAM ascent
step than the ascent step with the previous iteration’s privatised
gradient of the DP-SAT method. Concretely, we find DP-SAT
ascent steps to point in slightly opposite directions, as indicated
by a small, but negative cosine similarity.
0 200 400 600 800
Step−0.10.00.10.2CCSBAM
DP-SAT
Figure 4: Effectiveness of the ascent step throughout train-
ing on CIFAR-10 (three repetitions) with DP-SAT and BAM
as measured by the cosine similarity cos(ˆg(t−1)
priv,ˆg(t))and
1/lPl
icos(g(t)
i,ˆg(t))respectively.
A.3 C. Batch size and its effects on bias and
directional error
To investigate the empirical success of recent works using ex-
tremely large batch sizes (or even full-batch training), we inves-
tigate the impact of batch size on private gradient bias and its
directional component.
6

--- PAGE 7 ---
Dataset Method λ Model Batch-size LR CMultiplicity
CIFAR-10DP-SGD 0.0
ResNet-9 32×128 2e-3 116 DP-SAT 0.086
BAM 0.02
CIFAR-100DP-SGD 0.0
ResNet-9 32×128 2e-3 116 DP-SAT 0.086
BAM 0.01
ImageNet32DP-SGD 0.0
WideResNet-16-4 8×512 1e-3 14 DP-SAT 0.07
BAM 0.005
Table 2: Hyperparameter values for experiments on CIFAR-10, CIFAR-100 and ImageNet32.
1024 2048 4096 8192 16384 32768
Batch-size3.003.253.503.754.001
T/summationtextT
i/bardblBias( ˆg(i)
priv,ˆg(i))/bardbl2
Figure 5: Bigger batches reduce private gradient bias mag-
nitude. The figure showcases bias magnitude averaged over
training iterations when training on CIFAR-10 with DP-SGD
with different batch sizes.
1024 2048 4096 8192 16384 32768
Batch-size0.50.60.70.80.91
T/summationtextT
iccs/parenleftBig
ˆg(i),/hatwideg(i)
clip/parenrightBig
Figure 6: Bigger batches reduce the directional component
of private gradient bias as measured by an increased average
cosine similarity between clipped and unclipped mini-batch
gradients during training on CIFAR-10.
We find large batches to reduce both: the magnitude of the bias
vector and the directional components of the bias vector. The
latter is indicated by an increased cosine similarity between
clipped and unclipped gradients.
BBAM and effects on loss landscape
flatness
Minimising (9)has the additional benefit of smoothing the op-
timisation landscape around the minima ( flat minima ) which,as originally argued by Hochreiter and Schmidhuber [27], is
widely believed to offer better generalisation performance than
sharp minima. To find such flat minima, Zhao et al. [16] pro-
pose the objective:
LZ(θ) =L(θ) +λ∇θL(θ)
2(15)
=L(θ) +λ1
nnX
i=1gi
2(16)
This objective finds flat minima because ∥∇θL(θ)∥2approxi-
mates the Lipschitz constant of the loss function L. The Lips-
chitz constant is an upper bound on how much the magnitude
of a change in the parameter space changes the magnitude of
the loss. This is a way of capturing “flatness”: the smaller the
Lipschitz constant of the loss function, the flatter the minima
(since the same change in θwould lead to a smaller bound
on the change in the loss magnitude). Hence, by decreasing
the Lipschitz constant of the loss function, the solution will be
driven towards a flatter minimum.
However, objective (15) cannot be used in our case because
it does not allow for the per-sample analysis required for DP.
Nonetheless, we can show the following:
Lemma 3. The bias aware objective LBAO (9) upper-bounds
the objective LZ(15) of [16].
Proof. This follows from the triangle inequality:
L(θ) +λ1
nnX
i=1gi
2=L(θ) +λ1
nnX
i=1gi
2(17)
≤ L(θ) +λ1
nnX
i=1∥gi∥2. (18)
Thus, minimising LBAO will additionally drive the solution to
a flat minimum by minimising an upper bound on (15).
7

--- PAGE 8 ---
CA meaningful decomposition of the
private gradient
Bias(ˆgpriv,ˆg,)
(a)(a−1)ˆg
c
(b)
Figure 7: Illustration of the proposed bias (a) decomposition
into orthogonal vector components (b): magnitude aand direc-
tionc.
With the aim of better capturing the pathological nature of
clipping during private stochastic optimisation, we propose a
bias decomposition into orthogonal components that allow for
the isolation of magnitude and directional estimation error.
In our case, we have an estimator ˆgprivofˆg. We now show a
decomposition of ˆgprivin terms of ˆg, which will allow us to
differentiate harmful from harmless bias.
Theorem 1. The private gradient estimate ˆgprivcan be decom-
posed as:
ˆgpriv=a·ˆg+c,
where a∈R, c∈Rd. We call athemagnitude error andcthe
directional error that arise through (private) estimation.
Proof. We first relate each per-sample gradient gito the mini-
batch gradient ˆgthrough a vector decomposition into orthogo-
nal components.
gi= projˆggi+τi
where projˆggi=⟨gi,ˆg⟩
∥ˆg∥2·ˆgandτiis a suitable vector orthogonal
to the projection. Letting ηi=⟨gi,ˆg⟩
∥ˆg∥2, the decomposition is:
projˆggi=ηi·ˆg+τi. (19)
Utilising this decomposition inside the clipfunction gives us:
clip(gi) =gi
max(1 ,C
||ˆgi||2)=ηiˆg+τi
max(1 ,C
||gi||2). (20)
Now, let Mi= max(1 ,C
||gi||2)so that we can simplify the
above as
clip(gi) =ηi
Miˆg+τi
Mi. (21)Averaging the clipped gradients across the mini-batch yields:
ˆgclip=1
llX
i=1clip(gi) (22)
=1
llX
i=1ηi
Miˆg+τi
Mi
(23)
= 
1
llX
i=1ηi
Mi!
·ˆg+ 
1
llX
i=1τi
Mi!
. (24)
Finally, perturbing each giwith a vector βi∼ N(0, CσI d)we
obtain the desired decomposition in terms of magnitude error a
and directional error c:
ˆgpriv=1
llX
i=1(clip( gi) +βi) (25)
= 
1
llX
i=1ηi
Mi!
|{z}
a·ˆg+ 
1
llX
i=1τi
Mi+βi!
| {z }
c(26)
This allows us to now effectively isolate directional from mag-
nitude components of the private gradient bias:
Bias(ˆ gpriv,ˆg) =E[ˆgpriv]−ˆg (27)
=E[a·ˆg+c]−ˆg. (28)
8
