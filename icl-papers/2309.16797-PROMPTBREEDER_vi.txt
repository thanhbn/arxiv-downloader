# 2309.16797.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl-papers/2309.16797.pdf
# Kích thước tệp: 819158 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
PROMPTBREEDER:
TỰ CẢI THIỆN TỰ THAM CHIẾU
THÔNG QUA TIẾN HÓA PROMPT

Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, Tim Rockt ¨aschel
Google DeepMind
{chrisantha,dylski,henrykm,osindero,rocktaschel }@google.com

TÓM TẮT

Các chiến lược prompt phổ biến như Chain-of-Thought Prompting có thể cải thiện đáng kể khả năng suy luận của các Mô hình Ngôn ngữ Lớn (LLM) trong nhiều lĩnh vực khác nhau. Tuy nhiên, những chiến lược prompt được tạo thủ công này thường không tối ưu. Trong bài báo này, chúng tôi trình bày PROMPTBREEDER, một cơ chế tự cải thiện tự tham chiếu có mục đích chung để phát triển và thích ứng các prompt cho một lĩnh vực nhất định. Được điều khiển bởi một LLM, Promptbreeder biến đổi một quần thể các task-prompt, đánh giá chúng về độ phù hợp trên tập huấn luyện, và lặp lại quá trình này qua nhiều thế hệ để phát triển các task-prompt. Quan trọng là, việc biến đổi các task-prompt này được điều khiển bởi các mutation-prompt mà LLM tạo ra và cải thiện trong suốt quá trình tiến hóa theo cách tự tham chiếu. Tức là, Promptbreeder không chỉ cải thiện task-prompt mà còn cải thiện các mutation-prompt để cải thiện các task-prompt này. Promptbreeder vượt trội hơn các chiến lược prompt tiên tiến như Chain-of-Thought và Plan-and-Solve Prompting trên các benchmark suy luận số học và thông thức thường được sử dụng. Hơn nữa, Promptbreeder có thể phát triển các task-prompt phức tạp cho bài toán thách thức phân loại ngôn từ thù địch.

1 GIỚI THIỆU

Prompting là trung tâm của hiệu suất downstream của các mô hình nền tảng. Ví dụ, các chiến lược prompt khác nhau có thể có tác động đáng kể đến khả năng suy luận của mô hình (Wei et al., 2022; Nye et al., 2021; Zhou et al., 2022; Wang et al., 2022; Zhou et al., 2023; Wang et al., 2023b), khả năng xử lý đa phương thức (Yang et al., 2023b; Wang et al., 2023d), hoặc khả năng sử dụng công cụ (Yao et al., 2022; Schick et al., 2023). Hơn nữa, prompting có thể cải thiện việc chưng cất mô hình (Wang et al., 2023c; Hsieh et al., 2023) và có thể được sử dụng để mô phỏng hành vi tác nhân (Wang et al., 2023a; Park et al., 2023; Wu et al., 2023). Tuy nhiên, những chiến lược prompt này được thiết kế thủ công. Vì cách diễn đạt cụ thể của một prompt có thể có hiệu ứng đáng kể đến tính hữu dụng của nó (Madaan & Yazdanbakhsh, 2022), điều này đặt ra câu hỏi liệu kỹ thuật prompt có thể được tự động hóa hay không. Automatic Prompt Engineer (APE, Zhou et al., 2023) cố gắng giải quyết vấn đề này bằng cách tạo ra một phân phối ban đầu của các prompt sử dụng prompt khác suy ra bài toán từ một số ví dụ input-output từ tập dữ liệu. Tuy nhiên, Zhou et al. nhận thấy "lợi ích giảm dần đối với các vòng lựa chọn tiếp theo vì chất lượng dường như ổn định sau ba vòng", và do đó từ bỏ việc sử dụng APE lặp đi lặp lại.

Chúng tôi đề xuất một giải pháp cho vấn đề lợi ích giảm dần thông qua một thuật toán tiến hóa duy trì đa dạng cho việc tự cải thiện tự tham chiếu của các prompt cho LLM.

Schmidhuber (1990) lưu ý rằng "chương trình của một mạng nơ-ron là ma trận trọng số của nó". Do đó, "chương trình" này có thể được thay đổi theo cách tự tham chiếu bởi chính mạng nơ-ron đó (Schmidhuber, 1993; Irie et al., 2022). Một mạng nơ-ron như vậy tự cải thiện bản thân cũng như cải thiện cách nó tự cải thiện có thể là một bước đệm quan trọng hướng tới việc tự cải thiện tự tham chiếu mở rộng của AI (Schmidhuber, 2003). Tuy nhiên, việc tự cải thiện thông qua ma trận trọng số tự tham chiếu rất tốn kém vì nó yêu cầu các tham số bổ sung để sửa đổi tất cả các

1Xem Phụ lục A để biết định nghĩa thuật ngữ.

1arXiv:2309.16797v1 [cs.CL] 28 Sep 2023

--- TRANG 2 ---

[Bảng so sánh hiệu suất các phương pháp trên nhiều tập dữ liệu được dịch sang tiếng Việt]

Phương pháp | LLM | MultiArith* | SingleEq* | AddSub* | SVAMP* | SQA | CSQA | AQuA-RAT | GSM8K
Zero-shot CoT | text-davinci-003 | (83.8) | (88.1) | (85.3) | (69.9) | (63.8) | (65.2) | (38.9) | (56.4)
PoT | text-davinci-003 | (92.2) | (91.7) | (85.1) | (70.8) | – | – | (43.9) | (57.0)
PS | text-davinci-003 | (87.2) | (89.2) | (88.1) | (72.0) | – | – | (42.5) | (58.2)
PS+ | text-davinci-003 | (91.8) | (94.7) | (92.2) | (75.7) | (65.4) | (71.9) | (46.0) | (59.3)
PS | PaLM 2-L | 97.7 | 90.6 | 72.4 | 83.8 | 50.0 | 77.9 | 40.2 | 59.0
PS+ | PaLM 2-L | 92.5 | 94.7 | 74.4 | 86.3 | 50.1 | 73.3 | 39.4 | 60.5
APE | PaLM 2-L | 95.8 | 82.2 | 72.2 | 73.0 | 38.4 | 67.3 | 45.7 | 77.9
OPRO | PaLM 2-L | – | – | – | – | – | – | – | 80.2
PB (của chúng tôi) | PaLM 2-L | 99.7 | 96.4 | 87.8 | 90.2 | 71.8 | 85.4 | 62.2 | 83.9

Few-shot
Manual-CoT | text-davinci-003 | (93.6) | (93.5) | (91.6) | (80.3) | (71.2) | (78.3) | (48.4) | (58.4)
Auto-CoT | text-davinci-003 | (95.5) | (92.1) | (90.8) | (78.1) | – | – | (41.7) | (57.1)
PB (của chúng tôi) | PaLM 2-L | 100.0 | 98.9 | 87.1 | 93.7 | 80.2 | 85.9 | 64.6 | 83.5

Bảng 1: So sánh Promptbreeder (PB) với Chain-of-Thought (Manual-CoT, Wei et al., 2022), Zero-shot CoT (Kojima et al., 2022), Program-of-Thoughts (PoT, Chen et al., 2022), Auto-CoT (Zhang et al., 2023b), OPRO (Yang et al., 2023a), Automatic Prompt Engineer Zero-shot prompt (APE, Zhou et al., 2023), Plan-and-Solve có (PS+) và không có prompt cải thiện (PS, Wang et al., 2023b) và sử dụng PaLM 2-L (Anil et al., 2023) làm LLM cơ sở (APE, PS PaLM 2-L /PS+ PaLM 2-L). Kết quả tốt nhất trong cả hai danh mục zero-shot và few-shot được tô đậm. Kết quả trong ngoặc được lấy trực tiếp từ bài báo Plan-and-Solve sử dụng text-davinci-003 (Brown et al., 2020). Đối với các tập dữ liệu có dấu hoa thị (MultiArith*, SingleEq*, AddSub*, và SVAMP*), chúng tôi ngẫu nhiên lấy một nửa ví dụ để huấn luyện và báo cáo độ chính xác trên tập kiểm tra còn lại. Xem Phần 4 và Phụ lục I để biết chi tiết về prompt và tập dữ liệu.

tham số của mô hình. Vì hành vi và khả năng của LLM bị ảnh hưởng đáng kể bởi các prompt mà chúng ta cung cấp cho chúng, chúng ta có thể tương tự nghĩ về prompt như là chương trình của LLM (Zhou et al., 2023). Trong quan điểm này, việc thay đổi chiến lược prompt như phương pháp Scratchpad (Nye et al., 2021) hoặc Chain-of-Thought Prompting (Wei et al., 2022) tương ứng với việc thay đổi "chương trình" của LLM. Đi xa hơn trong phép tương tự này, chúng ta có thể sử dụng chính LLM để thay đổi prompt của nó, cũng như cách nó thay đổi những prompt này, đưa chúng ta đến gần hơn với hệ thống tự cải thiện hoàn toàn tự tham chiếu dựa trên LLM.

Trong bài báo này, chúng tôi giới thiệu PROMPTBREEDER (PB) cho việc tự cải thiện tự tham chiếu của LLM. Cho một tập hạt giống của mutation-prompt (tức là hướng dẫn để sửa đổi task-prompt), thinking-style (tức là mô tả văn bản về các heuristic nhận thức chung), và mô tả bài toán cụ thể cho lĩnh vực, PB tạo ra các biến thể của task-prompt và mutation-prompt, khai thác sự thật là LLM có thể được prompt để hoạt động như các toán tử biến đổi (Meyerson et al., 2023). Dựa trên độ phù hợp của các task-prompt đã tiến hóa được đo trên tập huấn luyện, chúng tôi chọn một tập con các đơn vị tiến hóa bao gồm task-prompt và mutation-prompt liên quan của chúng, để truyền cho các thế hệ tương lai. Qua nhiều thế hệ của PB, chúng tôi quan sát thấy các prompt thích ứng với lĩnh vực hiện tại. Ví dụ, trong lĩnh vực toán học, PB đã phát triển task-prompt "Hiển thị tất cả cách làm của bạn. II. Bạn nên sử dụng ký hiệu và từ vựng toán học chính xác, khi thích hợp. III. Bạn nên viết câu trả lời bằng câu đầy đủ và bằng từ ngữ. IV. Bạn nên sử dụng ví dụ để minh họa quan điểm và chứng minh câu trả lời. V. Cách làm của bạn nên gọn gàng và dễ đọc" trên GSM8K (xem Phụ lục J). Trên một loạt các benchmark thường được sử dụng bao gồm suy luận thông thức, số học và đạo đức, chúng tôi thấy rằng PB vượt trội hơn các phương pháp tiên tiến như Chain-of-Thought (Wei et al., 2022) và Plan-and-Solve (Wang et al., 2023b) prompting. Vì PB không yêu cầu cập nhật tham số nào cho việc tự cải thiện tự tham chiếu, chúng tôi tin rằng cách tiếp cận này chỉ ra một tương lai thú vị nơi các LLM lớn hơn và có khả năng hơn có thể khuếch đại thêm lợi ích của phương pháp chúng tôi.

Tóm lại, bài báo này đóng góp những điểm chính sau: (i) chúng tôi giới thiệu Promptbreeder, một phương pháp tự cải thiện tự tham chiếu cho LLM phát triển prompt cho lĩnh vực hiện tại, cũng như cải thiện cách nó phát triển những prompt này, (ii) chúng tôi báo cáo cải thiện so với các chiến lược prompt tiên tiến trên một loạt các benchmark suy luận số học và thông thức thường được sử dụng, và (iii) chúng tôi điều tra các thành phần tự tham chiếu khác nhau của Promptbreeder và đóng góp của chúng vào kết quả.

--- TRANG 3 ---

[Hình 1 - Sơ đồ tổng quan về Promptbreeder được dịch sang tiếng Việt]

Các Phong cách Suy nghĩ:
"Hãy suy nghĩ từng bước" + "Thay đổi hướng dẫn này để làm cho nó thú vị hơn" +
"HƯỚNG DẪN:" + "Giải quyết bài toán từ toán học này" + "HƯỚNG DẪN ĐỘT BIẾN ="

Các Prompt Biến đổi:
Lấy mẫu | Lấy mẫu

Mô tả Bài toán
cụ thể cho GSM8K, AQuA, ETHOS, SVAMP, v.v.

LLM → "Tạo ra một câu trả lời có hệ thống khiến bạn trông khá thông minh"

P: "Tạo ra một câu trả lời có hệ thống khiến bạn trông khá thông minh"
M: "Thay đổi hướng dẫn này để làm cho nó thú vị hơn"
P: "Vẽ sơ đồ thể hiện bài toán toán học"
M: "Biến đổi prompt bằng một khúc quanh bất ngờ"
P: "Hãy suy nghĩ từng bước qua bài toán toán học này"
M: "Sửa đổi hướng dẫn như không có LLM tự trọng nào sẽ làm"
P: "GIẢI PHÁP:"
M: "Cân nhắc cách một giáo viên giỏi hơn sẽ diễn đạt điều này"

Quần thể N

[Sơ đồ tiếp theo với các mũi tên và hộp thể hiện quá trình khởi tạo quần thể, các toán tử biến đổi, và thay thế]

Hình 1: Tổng quan về Promptbreeder. Cho một mô tả bài toán và một tập ban đầu "thinking-style" và mutation-prompt chung, Promptbreeder tạo ra một quần thể các đơn vị tiến hóa, mỗi đơn vị thường bao gồm hai task-prompt và một mutation-prompt. Sau đó chúng tôi chạy một thuật toán di truyền thi đấu nhị phân tiêu chuẩn (Harvey, 2011). Để xác định độ phù hợp của task-prompt, chúng tôi đánh giá hiệu suất của nó trên một batch dữ liệu huấn luyện ngẫu nhiên. Qua nhiều thế hệ, Promptbreeder tiếp tục biến đổi task-prompt cũng như mutation-prompt sử dụng năm lớp toán tử biến đổi khác nhau. Điều trước dẫn đến các task-prompt ngày càng thích ứng với lĩnh vực trong khi điều sau phát triển các mutation-prompt ngày càng hữu dụng theo cách tự tham chiếu.

2 CÔNG TRÌNH LIÊN QUAN

Prompting một LLM đúng cách là thiết yếu cho hiệu suất downstream của nó (Moradi & Samwald, 2021; Madaan & Yazdanbakhsh, 2022; Zhou et al., 2023). Thực tế, ngay cả thứ tự mà các prompt được trình bày cũng có thể ảnh hưởng mạnh đến hiệu suất LLM (Lu et al., 2022). Một số công trình gần đây đã tập trung vào việc thiết kế các chiến lược prompt tốt hơn, hoặc thậm chí tự động hóa kỹ thuật prompt như vậy.

Prompting: Chain-of-Thought Prompting (CoT, Wei et al., 2022) là một chiến lược prompt phổ biến cung cấp các bước suy luận trung gian như few-shot prompt cho LLM, do đó cải thiện đáng kể khả năng suy luận số học, thông thức và tượng trưng của nó. Đáng chú ý, lợi ích của CoT rõ ràng hơn đối với các LLM mạnh hơn. Điều này thú vị vì nó chỉ ra khả năng các cơ chế tự cải thiện ngày càng có khả năng (và có thể mở rộng) dựa trên các LLM thành thạo—một giả thuyết mà Promptbreeder xây dựng trực tiếp. Thay vì few-shot CoT prompting, Kojima et al. (2022) chứng minh rằng LLM cũng có thể được prompt zero-shot (ví dụ "Hãy suy nghĩ từng bước") để tạo ra chuỗi suy nghĩ riêng của chúng (Zero-shot CoT) cải thiện khả năng suy luận. Self-Consistency (CoT-SC, Wang et al., 2022) mở rộng CoT bằng cách lấy mẫu một tập đa dạng các cách làm và chọn câu trả lời nhất quán nhất. Tree of Thoughts (ToT, Yao et al., 2023) tổng quát hóa CoT thành nhiều cách làm có thể được mở rộng hoặc quay lại. Graph of Thoughts (GoT, Besta et al., 2023) là một tổng quát hóa tiếp theo thành cấu trúc đồ thị tùy ý. Plan-and-Solve Prompting (PS, Wang et al., 2023b) khuyến khích LLM trước tiên lập kế hoạch để giải quyết bài toán trước khi cố gắng giải quyết. Tương tự, Least-to-Most Prompting (Zhou et al., 2022) khuyến khích LLM phân tách bài toán thành các phần con, và sau đó giải quyết từng phần riêng lẻ trước khi tổng hợp câu trả lời. Self-Refine (Madaan et al., 2023) prompt LLM tạo ra phản hồi, cung cấp phản hồi về phản hồi, và cuối cùng tinh chỉnh giải pháp.

--- TRANG 4 ---

Trái ngược với các cách tiếp cận không dùng gradient ở trên, các cách tiếp cận Soft Prompting (ví dụ, Liu et al., 2021; Qin & Eisner, 2021; Lester et al., 2021) trực tiếp fine-tune các biểu diễn prompt liên tục. Huang et al. (2022) sử dụng CoT và CoT-SC trên tập dữ liệu câu hỏi không nhãn, và sau đó fine-tune LLM dựa trên các giải pháp được tạo ra. Tương tự, Zelikman et al. (2022) sử dụng CoT để tạo ra các lý lẽ và fine-tune LLM dựa trên những ví dụ và lý lẽ đó đã đưa ra câu trả lời đúng. Tuy nhiên, như được Zhou et al. (2023) lập luận, bất kỳ cách tiếp cận nào cập nhật tất cả hoặc một phần các tham số LLM sẽ không mở rộng khi các mô hình ngày càng lớn và hơn nữa, sẽ không hoạt động với số lượng LLM ngày càng tăng được ẩn sau API.

Tất cả các cách tiếp cận kỹ thuật prompt ở trên đều bất khả tri lĩnh vực nhưng được thiết kế thủ công. Trọng tâm của công trình chúng tôi là giả thuyết rằng chúng ta có thể làm tốt hơn bằng cách sử dụng một quy trình tự cải thiện tự động có thể thích ứng prompt với lĩnh vực hiện tại. Auto-CoT (Zhang et al., 2023b) và Automatic-CoT (Shum et al., 2023) tự động tìm chuỗi suy luận cho Few-Shot CoT. Automatic Prompt Engineer (APE, Zhou et al., 2023) sử dụng một generator-prompt để tạo ra các ứng viên prompt, và một mutation-prompt khác để biến đổi chúng. Trái ngược với APE, công trình của chúng tôi thực hiện khởi tạo mutation-prompt cụ thể cho tác vụ theo thành phần, biến đổi online mutation-prompt tiếp theo, sử dụng các toán tử biến đổi đặc biệt tính đến toàn bộ quần thể và lịch sử elite, và sử dụng các phương pháp duy trì đa dạng—tất cả đều giúp tránh vấn đề lợi ích giảm dần và mất đa dạng mà APE gặp phải.

Đồng thời với công trình chúng tôi, Yang et al. (2023a) đã phát triển Optimization by PROmpting (OPRO), một phương pháp tối ưu hóa prompt biến đổi prompt sử dụng một mutation prompt phức tạp duy nhất, và đánh giá các prompt mới được tạo ra trên một tập huấn luyện cố định nhỏ các bài toán. Trái ngược, Promptbreeder tự động phát triển nhiều mutation-prompt do LLM tạo ra cũng như task-prompt, và đánh giá độ phù hợp trên các tập con ngẫu nhiên từ toàn bộ tập huấn luyện trong quá trình tiến hóa. Tại thời điểm phát hành, OPRO đạt được điểm 80.2% thông qua prompt zero-shot được tối ưu hóa "Hít thở sâu và làm việc với bài toán này từng bước" trên GSM8K. Promptbreeder vượt qua điều này với 83.9% trong setting zero-shot với prompt đơn giản một cách không trực quan "GIẢI PHÁP" —bằng chứng tiếp theo cho độ nhạy cảm của LLM với prompt và tầm quan trọng của việc tìm prompt hiệu quả tự động. Cũng đồng thời với công trình chúng tôi, Guo et al. (2023) đã phát triển EvoPrompt, sử dụng một mutation (và crossover) prompt cố định, cũng như một prompt yêu cầu một mutant của sự khác biệt giữa hai prompt cha mẹ, để tạo ra prompt con cái. EvoPrompt được khởi tạo với toàn bộ quần thể các prompt ban đầu được thiết kế thủ công phù hợp với tác vụ thay vì một mô tả bài toán duy nhất như chúng tôi làm. Trái ngược với hai cách tiếp cận trên, Promptbreeder sử dụng LLM để tự tham chiếu cải thiện mutation-prompt, và nó có thể phát triển context cũng như.

Tự cải thiện tự tham chiếu: Phát triển một hệ thống mở rộng có thể tự cải thiện cũng như cải thiện cách nó tự cải thiện (Schmidhuber, 1993; 2003) là một bài toán mở lâu dài trong nghiên cứu AI. Schmidhuber (1993) giới thiệu một mạng nơ-ron "nội tâm" với ma trận trọng số tự tham chiếu có thể sửa đổi trọng số riêng của nó và do đó cũng sửa đổi những trọng số điều khiển cách trọng số riêng của nó được sửa đổi. Gần đây, Irie et al. (2022) đề xuất một ma trận trọng số tự tham chiếu có thể mở rộng hơn lấy cảm hứng từ fast weight programmer (Schmidhuber, 1992). Kirsch & Schmidhuber (2022) đề xuất một cách tiếp cận meta-learning tự tham chiếu, kết hợp ma trận trọng số tự tham chiếu với ý tưởng từ Gödel Machines (Schmidhuber, 2003), tức là phân bổ nhiều tài nguyên tính toán hơn cho các giải pháp hiệu suất tốt hơn. Tuy nhiên, vì những cách tiếp cận này trực tiếp sửa đổi tham số của mô hình, không rõ cách mở rộng chúng lên số lượng tham số ngày càng tăng trong các LLM hiện đại. Trái ngược, đối với Promptbreeder, nền tảng của tự cải thiện tự tham chiếu là ngôn ngữ tự nhiên, tránh hoàn toàn các cập nhật tham số tốn kém.

Tính mở rộng và LLM: Promptbreeder sử dụng quan sát của Lehman et al. (2022), Meyerson et al. (2023) và Chen et al. (2023) rằng LLM hiệu quả trong việc tạo ra các biến đổi từ ví dụ. Ngoài ra, LLM mã hóa khái niệm con người về tính thú vị và có thể được sử dụng để tự động định lượng tính mới lạ (Zhang et al., 2023a). Promptbreeder liên quan đến Picbreeder (Secretan et al., 2008), một hệ thống mở rộng có con người trong vòng lặp phát triển hình ảnh ngày càng thú vị. Trong khi Picbreeder khám phá không gian hình ảnh, Promptbreeder khám phá không gian prompt và làm điều đó mà không có con người trong vòng lặp. Vì Promptbreeder đề xuất các prompt biến đổi cho chính nó, đó là một ví dụ về hệ thống chuyển đổi từ "học từ dữ liệu" sang "học dữ liệu gì để học từ" (Jiang et al., 2022).

--- TRANG 5 ---

3 PROMPTBREEDER

Chúng tôi giới thiệu Promptbreeder, một hệ thống tiến hóa prompt có thể tự động khám phá các prompt cho một lĩnh vực nhất định và có thể tìm ra các task-prompt cải thiện khả năng của LLM trong việc tạo ra câu trả lời cho các câu hỏi trong lĩnh vực đó. Promptbreeder có mục đích chung vì cùng một hệ thống có thể thích ứng với nhiều lĩnh vực khác nhau.

Promptbreeder sử dụng quan sát rằng LLM có thể được dùng để tạo ra các biến thể của văn bản đầu vào (Lehman et al., 2022; Meyerson et al., 2023; Chen et al., 2023). Hình 1 đưa ra tổng quan về phương pháp của chúng tôi. Chúng tôi quan tâm đến việc phát triển các task-prompt. Một task-prompt P là một chuỗi được sử dụng để điều kiện hóa context của LLM trước một số đầu vào tiếp theo Q, nhằm đảm bảo phản hồi tốt hơn so với việc Q được trình bày mà không có P. Để đánh giá độ phù hợp của mỗi task-prompt đã phát triển, chúng tôi lấy mẫu một batch gồm 100 cặp Q&A từ toàn bộ tập huấn luyện của lĩnh vực hiện tại².

Promptbreeder tạo ra các task-prompt theo thuật toán tiến hóa. Toán tử biến đổi cho thuật toán này chính là một LLM, được điều kiện hóa trên mutation-prompt M. Tức là, một task prompt biến đổi P' được định nghĩa bởi P' = LLM(M + P) trong đó '+' tương ứng với phép nối chuỗi. Nhiều mutation-prompt như vậy được mô tả trong Phần 3.2.

Cơ chế tự tham chiếu chính của Promptbreeder xuất phát từ việc áp dụng thuật toán tiến hóa không chỉ cho task-prompt mà còn cho mutation-prompt. Toán tử biến đổi cho thuật toán meta-level này lại là một LLM, bây giờ được điều kiện hóa trên hyper-mutation prompt H. Tức là, chúng ta có được mutation-prompt biến đổi M' thông qua M' = LLM(H + M).

Cho một tập "thinking styles" T và một tập mutation-prompt ban đầu M, cũng như mô tả bài toán cụ thể lĩnh vực D, Promptbreeder khởi tạo một quần thể các task-prompt biến đổi (xem Phần 3.1). Để làm rõ, một đơn vị tiến hóa bao gồm một tập task-prompt, một mutation-prompt và trong trường hợp few-shot, một tập các workings out chính xác (tức là các bước suy luận từng bước hoặc "chains-of-thought" dẫn đến câu trả lời đúng). Điều này có nghĩa là task-prompt và mutation-prompt có quan hệ 1:1. Để phát triển quần thể này, chúng tôi sử dụng framework thuật toán di truyền thi đấu nhị phân (Harvey, 2011): chúng tôi lấy mẫu hai cá thể từ quần thể, lấy cá thể có độ phù hợp cao hơn, biến đổi nó (xem phần tiếp theo) và ghi đè kẻ thua cuộc bằng bản sao biến đổi của người thắng.

3.1 KHỞI TẠO PROMPTBREEDER

Để đưa ra một ví dụ cụ thể, hãy xem xét các bước khởi tạo được sử dụng để tạo ra task-prompt và mutation-prompt cho GSM8K (một tập dữ liệu bài toán từ toán 'cấp tiểu học'). Mô tả bài toán là "Giải quyết bài toán từ toán, đưa ra câu trả lời dưới dạng số A-rập". Vì Plan-and-Solve (Wang et al., 2023b) sử dụng hai task-prompt, chúng tôi cũng phát triển hai task-prompt (cộng với một mutation-prompt) cho mỗi đơn vị tiến hóa. Để thúc đẩy đa dạng trong các prompt ban đầu, chúng tôi tạo ra các task-prompt ban đầu bằng cách nối (cho mỗi task-prompt) một 'mutation-prompt' được rút ngẫu nhiên (ví dụ "Tạo một biến thể của prompt.") và một 'thinking-style' được rút ngẫu nhiên (ví dụ "Hãy suy nghĩ từng bước") với mô tả bài toán, và cung cấp cho LLM để tạo ra phần tiếp theo, dẫn đến một task-prompt ban đầu. Chúng tôi thực hiện điều này hai lần để tạo ra hai task-prompt ban đầu cho mỗi đơn vị. Cả mutation-prompt và thinking-style đều được lấy mẫu ngẫu nhiên từ một tập mutation-prompt ban đầu và một tập thinking-style (xem Phụ lục C, D và G để biết tập đầy đủ). Mutation-prompt được thêm vào đơn vị tiến hóa và do đó liên kết với task-prompt cụ thể của nó trong suốt quá trình tiến hóa.

Đối với ví dụ trên, chuỗi đầu vào đầy đủ cho LLM để tạo một task-prompt ban đầu có thể là "Tạo một biến thể của prompt. Hãy suy nghĩ từng bước. HƯỚNG DẪN: Giải quyết bài toán từ toán, đưa ra câu trả lời dưới dạng số A-rập. HƯỚNG DẪN ĐỘT BIẾN:". Lưu ý cách các chuỗi điều khiển "HƯỚNG DẪN" và "HƯỚNG DẪN ĐỘT BIẾN" được thêm vào để khuyến khích một phần tiếp theo phù hợp. Bảng 4 trong Phụ lục E cho thấy ví dụ về các prompt ban đầu được tạo ra theo cách này.

²Chiến lược prompt của chúng tôi áp dụng tuần tự hai task-prompt. Task-prompt đầu tiên + câu hỏi tạo ra phần tiếp theo. Phần tiếp theo + task-prompt thứ hai tạo ra câu trả lời cuối cùng.

--- TRANG 6 ---

3.2 CÁC TOÁN TỬ BIẾN ĐỔI

Như được hiển thị trong Hình 1, có chín toán tử thuộc năm lớp rộng thúc đẩy việc khám phá các chiến lược prompt. Đối với mỗi sự kiện nhân bản, chỉ một trong chín toán tử biến đổi được áp dụng (chúng tôi lấy mẫu với xác suất đồng đều trên chín toán tử để quyết định toán tử biến đổi nào áp dụng). Lý do sử dụng tập toán tử đa dạng này là để cho phép LLM khám phá một không gian rộng lớn các phương pháp nhận thức tự đặt câu hỏi bằng ngôn ngữ, bằng cách liên tục thay đổi cách đóng khung bài toán cũng như truy xuất các mô hình tinh thần được thể hiện bằng ngôn ngữ tự nhiên có thể giúp giải quyết thách thức suy luận nhất định. Các nghiên cứu từ học tập insight mạnh mẽ cho thấy rằng việc mô tả lại đại diện đa dạng là chìa khóa để giải quyết vấn đề (Öllinger & Knoblich, 2009)—một nguyên tắc mà chúng tôi cố gắng tái tạo thông qua tự cải thiện tự tham chiếu với ngôn ngữ tự nhiên làm nền tảng. Hình 2 minh họa cách Promptbreeder tự tham chiếu (xem Phụ lục F để biết giải thích chi tiết hơn).

3.2.1 BIẾN ĐỔI TRỰC TIẾP

Lớp toán tử biến đổi đơn giản nhất trực tiếp tạo ra task-prompt mới P' từ một task-prompt P hiện có (tạo prompt bậc một) hoặc từ một prompt chung khuyến khích tạo ra task-prompt mới dạng tự do—tức là không sử dụng parent hiện có, do đó tạo prompt bậc không.

Tạo Prompt Bậc Không: Chúng tôi tạo ra task-prompt mới bằng cách nối mô tả bài toán D (ví dụ "Giải quyết bài toán từ toán, đưa ra câu trả lời dưới dạng số A-rập") với prompt "Danh sách 100 gợi ý:", khuyến khích LLM đưa ra gợi ý mới có thể giúp giải quyết bài toán trong lĩnh vực nhất định. Chúng tôi trích xuất gợi ý đầu tiên được tạo ra làm task-prompt mới. Quan trọng là, task-prompt mới này không phụ thuộc vào bất kỳ task-prompt nào được tìm thấy trước đó. Thay vào đó, nó được tạo lại từ mô tả bài toán mỗi lần. Lý do chúng tôi bao gồm toán tử bậc không này là khi tiến hóa prompt phân tán, toán tử này cho phép chúng tôi tạo ra các task-prompt mới liên quan chặt chẽ đến mô tả bài toán gốc, tương tự như việc lấy mẫu lại đồng nhất trong các cách tiếp cận học tập curriculum tự động (Jiang et al., 2021b;a; Park et al., 2023; Parker-Holder et al., 2022).

Tạo Prompt Bậc Một: Chúng tôi nối mutation-prompt (màu đỏ) với task-prompt cha (màu xanh) và truyền cho LLM để tạo ra task-prompt biến đổi. Ví dụ "Nói lại hướng dẫn đó theo cách khác. ĐỪNG sử dụng bất kỳ từ nào trong hướng dẫn gốc, đó là anh bạn tốt. HƯỚNG DẪN: Giải quyết bài toán từ toán, đưa ra câu trả lời dưới dạng số A-rập. HƯỚNG DẪN ĐỘT BIẾN:". Quy trình này giống hệt với phương pháp khởi tạo, ngoại trừ việc chuỗi thinking-style được lấy mẫu ngẫu nhiên không được sử dụng. Tạo prompt bậc một là toán tử biến đổi vô tính tiêu chuẩn của Promptbreeder, và đó là cốt lõi của mọi thuật toán di truyền—lấy một genotype cha mẹ (task-prompt) và áp dụng biến đổi vào nó (trong trường hợp này bị ảnh hưởng bởi mutation-prompt).

3.2.2 BIẾN ĐỔI ƯỚC LƯỢNG PHÂN PHỐI

Lớp toán tử biến đổi tiếp theo điều kiện hóa không chỉ trên không hoặc một cha mẹ, mà thay vào đó trên một tập cha mẹ. Như vậy, chúng có thể biểu cảm hơn bằng cách xem xét các mẫu trong quần thể.

Biến đổi Ước lượng Phân phối (EDA): Lấy cảm hứng từ Hauschild & Pelikan (2011), chúng tôi cung cấp một danh sách được lọc và đánh số của quần thể task-prompt hiện tại cho LLM và yêu cầu nó tiếp tục danh sách này với các task-prompt mới. Chúng tôi lọc quần thể prompt dựa trên độ tương tự cosine embedding BERT (Devlin et al., 2019) giữa chúng với nhau—một cá thể không được bao gồm trong danh sách nếu nó tương tự hơn 0.95 với bất kỳ mục nào khác trong danh sách, do đó khuyến khích đa dạng (so sánh các phương pháp quality-diversity (Lehman & Stanley, 2011b;a; Mouret & Clune, 2015)). Các prompt được liệt kê theo thứ tự ngẫu nhiên và chúng tôi không cung cấp cho LLM quyền truy cập vào giá trị độ phù hợp của các cá thể trong quần thể—chúng tôi thấy trong các thử nghiệm sơ bộ rằng LLM không hiểu những giá trị độ phù hợp này³ và chuyển sang tạo ra bản sao của các mục trong danh sách.

³Điều này trái ngược với các phát hiện gần đây của Mirchandani et al. (2023). Chúng tôi để lại cho công việc tương lai việc xem xét lại liệu LLM có thể diễn giải giá trị độ phù hợp để cải thiện tiến hóa prompt hay không.

--- TRANG 7 ---

[Hình 2 được dịch sang tiếng Việt - mô tả các biến thể của tiến hóa prompt tự tham chiếu]

Trực tiếp
P → P′ LLM
(a) Hướng dẫn bởi Mutation-Prompt
P → P′ LLM ← M
(b) Siêu Biến đổi  
P → P′ LLM ← M → M′ ← H LLM
(c) Promptbreeder
P → P′ LLM ← M → M′ ← H LLM
(d) T∼ T, D, M∼ M, LLM

Hình 2: Tổng quan về nhiều biến thể của tiến hóa prompt tự tham chiếu. Trong (a), LLM được sử dụng trực tiếp để tạo ra các biến thể P' của chiến lược prompt P (so sánh Meyerson et al., 2023). Sử dụng mutation prompt M, chúng ta có thể prompt LLM một cách rõ ràng để tạo ra các biến thể (b). Bằng cách sử dụng hyper mutation prompt H, chúng ta cũng có thể phát triển chính mutation prompt, biến hệ thống thành một hệ thống tự tham chiếu (c). Promptbreeder (d) cải thiện tính đa dạng của các prompt đã phát triển và mutation prompt bằng cách tạo ra quần thể ban đầu của các chiến lược prompt từ một tập thinking-style hạt giống T, mutation-prompt M, cũng như mô tả cấp cao D của lĩnh vực vấn đề.

Biến đổi EDA Rank và Index: Đây là một biến thể của phương pháp trên trong đó các task-prompt được liệt kê theo thứ tự độ phù hợp. Các thử nghiệm sơ bộ cho thấy LLM có khả năng tạo ra các mục tương tự với các phần tử xuất hiện sau trong danh sách. Điều này phù hợp với các phát hiện tương tự về hiệu ứng gần đây trong LLM (Liu et al., 2023). Do đó, sau khi lọc theo cách tương tự như trước, chúng tôi sắp xếp các task-prompt trong quần thể theo thứ tự tăng dần của độ phù hợp. Đầu danh sách được thêm tiền tố bằng prompt sau: "HƯỚNG DẪN: " + <<mutation-prompt>> + " \n Danh sách Phản hồi theo thứ tự điểm giảm dần." + <<chỉ số cuối + 1>> + "là phản hồi tốt nhất. Nó giống với" + <<chỉ số cuối>> + "hơn là với (1)". Lưu ý rằng chúng tôi đã 'nói dối' với LLM bằng cách nói với nó rằng thứ tự đang giảm dần. Điều này là vì nếu không nó quá thiên vị vào việc tạo ra một mục mới quá tương tự với mục cuối cùng. Mâu thuẫn giữa sắp xếp tăng dần và tuyên bố rằng đó là thứ tự giảm dần dường như cải thiện tính đa dạng của việc lấy mẫu. Lý do cho toán tử này lại là để biểu diễn phân phối hiện tại sao cho các phép ngoại suy có độ phù hợp cao nhưng đa dạng được LLM gợi ý.

Biến đổi Dựa trên Dòng dõi: Đối với mỗi đơn vị tiến hóa, chúng tôi lưu trữ lịch sử của các cá thể trong dòng dõi của nó là những cá thể tốt nhất trong quần thể, tức là, một danh sách lịch sử theo thời gian của các elite. Danh sách này được cung cấp cho LLM theo thứ tự thời gian (không được lọc theo đa dạng), với tiêu đề "GENOTYPE ĐƯỢC TÌM THẤY THEO THỨ TỰ CHẤT LƯỢNG TĂNG DẦN" để tạo ra một prompt mới dưới dạng tiếp theo. Lý do cho toán tử này là chúng tôi mong đợi tín hiệu của các genotype prompt đang cải thiện có thể mạnh hơn tín hiệu từ các prompt trong quần thể hiện tại vì chúng cung cấp gradient từ tệ đến tốt có thể được theo dõi (giả sử tín hiệu này có thể được LLM sử dụng).

3.2.3 HYPERMUTATION: BIẾN ĐỔI CỦA MUTATION-PROMPT

Trong khi các toán tử biến đổi ở trên có thể đã khám phá các task-prompt đa dạng, một hệ thống tự cải thiện lý tưởng cũng nên cải thiện cách nó tự cải thiện theo cách tự tham chiếu. Lớp thứ ba các toán tử biến đổi của chúng tôi bao gồm các toán tử siêu biến đổi liên quan đến tiến hóa của khả năng tiến hóa (Dawkins, 2003; Pigliucci, 2008; Payne & Wagner, 2019; Gajewski et al., 2019)—những toán tử sửa đổi quá trình tìm kiếm/khám phá thay vì trực tiếp quá trình thu nhận phần thưởng nhiệm vụ⁴.

Siêu Biến đổi Bậc Không: Chúng tôi nối mô tả bài toán ban đầu với một thinking-style được lấy mẫu ngẫu nhiên, và đưa vào LLM để tạo ra mutation-prompt mới. Mutation-prompt kết quả được áp dụng vào task-prompt để tạo ra biến thể của task-prompt như trong Tạo Prompt Bậc Một (xem Phần 3.2.1). Lưu ý rằng toán tử meta-mutation bậc không này giống hệt với toán tử được sử dụng trong quá trình khởi tạo. Lý do cho toán tử này là tạo ra các toán tử biến đổi theo cách tương tự như khởi tạo, đồng thời cũng mang lại kiến thức từ tập thinking style.

Siêu Biến đổi Bậc Một: Chúng tôi nối hyper-mutation-prompt "Vui lòng tóm tắt và cải thiện hướng dẫn sau:" với mutation-prompt để LLM tạo ra mutation-prompt mới. Mutation-prompt mới được tạo ra này sau đó được áp dụng vào task-prompt của đơn vị đó (xem Tạo Prompt Bậc Một trong Phần 3.2.1). Bằng cách này, chúng tôi có thể đánh giá ảnh hưởng của siêu biến đổi thông qua mutation-prompt mới được tạo ra của nó đối với chất lượng của task-prompt downstream đã tiến hóa cùng một lúc.

⁴Điều này tương tự như population based training (Jaderberg et al., 2017a)—thay vì áp dụng cho các siêu tham số như learning rate, nó áp dụng cho các mutation-prompt của Promptbreeder.

--- TRANG 8 ---

3.2.4 BIẾN ĐỔI LAMARCKIAN

Đối với lớp toán tử biến đổi này, chúng tôi bắt chước quá trình Lamarckian. Chúng tôi muốn sử dụng một phenotype thành công (tức là cách làm cụ thể được sử dụng để tạo ra câu trả lời đúng được gây ra bởi một task-prompt đã tiến hóa) để tạo ra genotype mới (tức là task-prompt đột biến). Một số quy trình dạng này đã xuất hiện trong tài liệu về LLM, ví dụ STaR (Zelikman et al., 2022), APO (Pryzant et al., 2023), và APE (Zhou et al., 2023).

Từ Working Out sang Task-Prompt: Đây là toán tử biến đổi 'Lamarckian' tương tự như instruction induction trong APE. Chúng tôi cung cấp cho LLM một working out đã được tạo ra trước đó dẫn đến câu trả lời đúng thông qua prompt sau: "Tôi đã đưa cho một người bạn một hướng dẫn và một số lời khuyên. Dưới đây là các ví dụ chính xác về cách làm của anh ấy + <<working out chính xác>> + Hướng dẫn là:". Đây thực chất là kỹ thuật ngược để tạo ra task-prompt từ working out đã cho. Một ví dụ hiệu quả về điều này được hiển thị trong Phụ lục H. Loại toán tử này rất quan trọng khi mô tả bài toán vắng mặt, không đủ hoặc gây hiểu lầm.

3.2.5 PROMPT CROSSOVER VÀ CONTEXT SHUFFLING

Lớp toán tử biến đổi cuối cùng của chúng tôi là các toán tử crossover và toán tử để xáo trộn các ví dụ ngữ cảnh few-shot có trong các đơn vị tiến hóa.

Prompt Crossover: Sau khi một toán tử biến đổi được áp dụng, với xác suất 10%, một task-prompt được thay thế bằng task-prompt được chọn ngẫu nhiên từ thành viên khác của quần thể. Thành viên này được chọn theo lựa chọn tỷ lệ với độ phù hợp. Crossover không được áp dụng cho mutation-prompt, chỉ cho task-prompt.

Context Shuffling: Promptbreeder có thể đồng thời phát triển task-prompt, mutation-prompt và tập working out chính xác được gọi là ngữ cảnh few-shot. Để đạt được điều sau, chúng tôi điền vào ngữ cảnh few-shot chỉ với các working out dẫn đến câu trả lời đúng. Trong quá trình đánh giá, chúng tôi cung cấp ngữ cảnh few-shot này trước task-prompt, cung cấp hướng dẫn về dạng working out mong muốn. Nếu danh sách ngữ cảnh few-shot đã đầy, một working out chính xác mới được lấy mẫu ngẫu nhiên sẽ thay thế working out hiện có từ danh sách sau khi đánh giá độ phù hợp của đơn vị trên một tập câu hỏi mới. Ngoài ra, với xác suất 10%, chúng tôi lấy mẫu lại toàn bộ danh sách ngữ cảnh với xác suất nghịch đảo với độ dài danh sách ngữ cảnh tối đa.

4 THÍ NGHIỆM

Chúng tôi đã sử dụng kích thước quần thể 50 đơn vị, tiến hóa qua thường 20-30 thế hệ, trong đó một thế hệ bao gồm việc tạo thành các cặp ngẫu nhiên của tất cả cá thể trong quần thể và cho họ thi đấu với nhau. Để đánh giá Promptbreeder, chúng tôi sử dụng các tập dữ liệu từ các chiến lược prompt tiên tiến như Plan-and-Solve, bao gồm suy luận số học với GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy & Roth, 2016), AddSub (Hosseini et al., 2014), AQuA-RAT (Ling et al., 2017), và SingleEq (Koncel-Kedziorski et al., 2015), suy luận thông thức với CommonsenseQA (CSQA, Talmor et al., 2019) và StrategyQA (SQA, Geva et al., 2021), các tác vụ instruction induction từ (Honovich et al., 2023), và phân loại ngôn từ thù địch trên tập dữ liệu ETHOS (Mollas et al., 2022). Xem Phụ lục I để biết chi tiết.

5 KẾT QUẢ VÀ THẢO LUẬN

Chúng tôi trình bày kết quả của Promptbreeder (PB) so sánh với các chiến lược prompt tiên tiến trên một loạt các benchmark suy luận thường được sử dụng trong Bảng 1. PB vượt trội hơn PS+, kỹ thuật prompting Plan-and-Solve tốt nhất (Wang et al., 2023b). Lưu ý rằng hiệu suất của PS+ được cải thiện bằng cách sử dụng PaLM 2-L (Anil et al., 2023) làm LLM cơ sở (PS+ PaLM 2-L) trên tất cả các tập dữ liệu ngoại trừ ADDSUB so với kết quả text-davinci-003 trong bài báo gốc. Trên tất cả các tập dữ liệu khác, độ chính xác zero-shot PB cao hơn PS+, với cải thiện tiếp theo trong trường hợp few-shot khi các ví dụ về giải pháp đã khám phá được bao gồm với các prompt. Trong Bảng 6 ở Phụ lục J, chúng tôi hiển thị các prompt zero-shot tốt nhất đã tiến hóa. Các ứng viên few-shot tốt nhất được hiển thị trong Phụ lục J.5 trở đi. Phụ lục K hiển thị kết quả few-shot và các đối chứng của chúng trên các tác vụ Instruction Induction từ bài báo APE. Để điều tra khả năng của Promptbreeder phát triển các prompt phức tạp cụ thể lĩnh vực cho tác vụ downstream, chúng tôi đã áp dụng nó vào bài toán Phân loại Ngôn từ Thù địch ETHOS (Mollas et al., 2022). Promptbreeder có thể phát triển một chiến lược prompt bao gồm hai prompt tương đối dài được áp dụng tuần tự (xem Phụ lục J.1) đạt điểm 89% trên ETHOS—một cải thiện so với prompt được thiết kế thủ công "Xác định liệu văn bản có chứa ngôn từ thù địch hay không" chỉ đạt 80%. Điều này chứng minh rằng Promptbreeder có khả năng thích ứng lĩnh vực phức tạp với tác vụ hiện tại. Phụ lục B hiển thị một lần chạy tiến hóa điển hình và các prompt đã tiến hóa, cho thấy rằng không giống như APE lặp đi lặp lại, độ phù hợp tiếp tục tăng trong suốt quá trình chạy.

Chúng tôi đã phân tích các mutation-prompt tốt nhất được sử dụng trong một lần chạy cho GSM8K. Bảng 7 trong Phụ lục J.3 hiển thị các mutation prompt tiến hóa tốt nhất theo điểm số của chúng (tỷ lệ lần mà khi mutation-prompt được áp dụng vào task-prompt trong một đơn vị, một task-prompt tốt hơn được tạo ra). Bảng 8 trong Phụ lục J.4 hiển thị theo thứ tự giảm dần, tỷ lệ phần trăm các lần mà các loại toán tử biến đổi khác nhau dẫn đến cải thiện khi được áp dụng vào task-prompt trong quần thể. Nó chứng minh rằng tất cả các toán tử biến đổi đều quan trọng để Promptbreeder hoạt động, bao gồm các toán tử siêu biến đổi dẫn đến tự cải thiện tự tham chiếu.

Chúng tôi đã đo lường tác động của các toán tử tự tham chiếu trên tất cả các tập dữ liệu toán học và tập dữ liệu ETHOS. Chi tiết về quy trình loại bỏ và kết quả của nó có thể được tìm thấy trong Phụ lục L. Việc loại bỏ bất kỳ toán tử tự tham chiếu nào đều có hại trong hầu hết mọi trường hợp, lợi ích lớn nhất là việc mô tả lại ban đầu các task-prompt khi khởi tạo. Chúng tôi chỉ thấy một toán tử biến đổi có hại cho một tác vụ cụ thể: rút ngẫu nhiên từ tập mutation-prompt khi khởi tạo làm giảm hiệu suất trên GSM8K.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi đã giới thiệu PROMPTBREEDER (PB), một hệ thống tự cải thiện tự tham chiếu có thể tự động phát triển các prompt cụ thể lĩnh vực hiệu quả cho lĩnh vực hiện tại. PB tự tham chiếu ở chỗ nó không chỉ phát triển task-prompt mà còn phát triển mutation-prompt điều khiển cách PB sửa đổi task-prompt. Do đó, nó không chỉ cải thiện prompt mà còn cải thiện cách nó cải thiện prompt.

Tiến xa hơn, có thể thú vị khi sử dụng chính LLM để đánh giá và thúc đẩy sự đa dạng của các prompt được tạo ra (xem Zhang et al., 2023a), hoặc sử dụng nó để xác định độ phù hợp của toàn bộ "quá trình tư duy", ví dụ một chiến lược N-prompt trong đó các prompt được áp dụng có điều kiện thay vì áp dụng vô điều kiện như trong Promptbreeder. Ví dụ, một "quá trình tư duy" phức tạp hơn là sử dụng PB trong chế độ tự đấu để phát triển pre-prompt cho các chính sách dựa trên LLM cạnh tranh với nhau, tức là, trong một cuộc đối thoại Socratic cạnh tranh⁵.

PB vẫn còn hạn chế so với tính mở rộng của các quy trình tư duy con người. Đầu tiên, cấu trúc topology của prompting vẫn cố định (xem Hình 2)—chúng tôi chỉ thích ứng nội dung prompt chứ không phải thuật toán prompting. Một cách diễn giải về tư duy là nó là một quá trình tự prompting mở rộng có thể cấu hình lại. Nếu như vậy, làm thế nào để phát triển các chiến lược tư duy phức tạp? Rõ ràng là cần thiết phải tạo ra và đánh giá chúng, và trong khi một quy trình tiến hóa đơn giản cung cấp một framework mà trong đó một chiến lược tư duy có thể được phát triển, trải nghiệm thực tế của con người chúng ta gợi ý nhiều quy trình chọn lọc phân cấp chồng chéo đang hoạt động. Hơn nữa, ngoài ngôn ngữ, tư duy con người bao gồm ngữ điệu, hình ảnh, v.v., trong một hệ thống đa phương thức.

Chúng tôi tin rằng PB chỉ ra một tương lai thú vị nơi các hệ thống tự cải thiện tự tham chiếu ngày càng mở rộng có thể trực tiếp sử dụng ngôn ngữ làm nền tảng để cải thiện thay vì dựa vào bất kỳ cập nhật tham số nào. Điều này thú vị vì cách tiếp cận này có thể sẽ tiếp tục mở rộng với các LLM ngày càng lớn và có khả năng hơn trong tương lai.

⁵https://princeton-nlp.github.io/SocraticAI/

--- TRANG 9 ---

---

[Phần tiếp theo sẽ bao gồm các phần Acknowledgments, References, và các Appendix - tất cả được dịch sang tiếng Việt]

LỜI CẢM ƠN

Chúng tôi cảm ơn Edward Hughes và Tom Schaul đã phản hồi về bản thảo đầu của bài báo. Chúng tôi cũng cảm ơn Tom Schaul, Chengrun Yang, và Denny Zhou đã có những cuộc thảo luận bổ ích, cũng như Gavin Buttimore, Simon Green, Keith Anderson, Joss Moore, Ollie Purkiss, John Quan, và Francesco Visin đã hỗ trợ trong việc chạy một số thí nghiệm.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch sang tiếng Việt...]

---

PHỤ LỤC

A THUẬT NGỮ

Thuật toán Ước lượng Phân phối: Một thuật toán tối ưu hóa lặp đi lặp lại tinh chỉnh mô hình xác suất của các giải pháp đầy hứa hẹn, thường sử dụng toàn bộ quần thể làm hướng dẫn.

Lựa chọn Tỷ lệ với Độ phù hợp: Còn được gọi là Lựa chọn Bánh xe Roulette, một cá thể được chọn tỷ lệ với độ phù hợp của nó trong quần thể.

Mutation Prompt: Prompt văn bản khi được nối với task-prompt nhằm tạo ra phần tiếp theo là task-prompt được cải thiện.

Mô tả bài toán: Mô tả văn bản ban đầu của bài toán có thể được sử dụng làm task-prompt ban đầu. Người dùng có thể thực hiện nỗ lực tốt nhất để tạo ra mô tả bài toán hiệu quả, đó là điểm khởi đầu của Promptbreeder.

Chiến lược Prompt: Một tập task-prompt và quy tắc áp dụng chúng tại thời điểm suy luận trong quá trình đánh giá độ phù hợp. Trong trường hợp tối thiểu, chiến lược prompt chỉ là một task-prompt duy nhất. Thông thường các chiến lược prompt của chúng tôi bao gồm hai task-prompt được áp dụng tuần tự.

Phenotype/Workings out/Context/Reasoning Path: Được sử dụng thay thế cho nhau để chỉ đầu ra của LLM trên một câu hỏi hoặc bài toán cụ thể khi được prompt với task-prompt được nối với câu hỏi.

Quần thể: Tập hợp các đơn vị tiến hóa (ví dụ 50).

Đơn vị tiến hóa: Cấu trúc thông tin đang được phát triển, ở đây bao gồm một tập task-prompt (thường là 2), một mutation-prompt, và trong trường hợp few-shot là một tập 2-3 ngữ cảnh (workings out).

B MỘT LẦN CHẠY TIẾN HÓA ĐIỂN HÌNH

Tác vụ từ trong ngữ cảnh là một trong 24 tác vụ instruction induction được sử dụng trong APE. Cho hai câu và một từ đồng âm, LLM phải xác định xem từ đồng âm có được sử dụng với cùng nghĩa trong cả hai câu hay không. Hình 3 hiển thị một lần chạy tiến hóa trong đó các chấm xanh là các đánh giá độ phù hợp cá nhân và đường màu đỏ là trung bình quần thể. Qua 2000 đánh giá, độ phù hợp tăng đáng kể. Các cặp Prompt 1 và Prompt 2 tốt nhất đã tiến hóa (được đánh giá trên tập huấn luyện) được hiển thị ở bên phải.

[Tiếp tục với phần còn lại của tài liệu...]
