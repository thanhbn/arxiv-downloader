# 2002.10957.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2002.10957.pdf
# File size: 583637 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MINILM: Deep Self-Attention Distillation for
Task-Agnostic Compression of Pre-Trained Transformers
Wenhui Wang Furu Wei Li Dong Hangbo Bao Nan Yang Ming Zhou
Microsoft Research
{wenwan,fuwei,lidong1,t-habao,nanya,mingzhou}@microsoft.com
Abstract
Pre-trained language models (e.g., BERT (Devlin
et al., 2018) and its variants) have achieved re-
markable success in varieties of NLP tasks. How-
ever, these models usually consist of hundreds of
millions of parameters which brings challenges
for ﬁne-tuning and online serving in real-life ap-
plications due to latency and capacity constraints.
In this work, we present a simple and effective ap-
proach to compress large Transformer (Vaswani
et al., 2017) based pre-trained models, termed
as deep self-attention distillation. The small
model (student) is trained by deeply mimicking
the self-attention module, which plays a vital
role in Transformer networks, of the large model
(teacher). Speciﬁcally, we propose distilling the
self-attention module of the last Transformer layer
of the teacher, which is effective and ﬂexible for
the student. Furthermore, we introduce the scaled
dot-product between values in the self-attention
module as the new deep self-attention knowledge,
in addition to the attention distributions (i.e., the
scaled dot-product of queries and keys) that have
been used in existing works. Moreover, we show
that introducing a teacher assistant (Mirzadeh
et al., 2019) also helps the distillation of large
pre-trained Transformer models. Experimental
results demonstrate that our monolingual model1
outperforms state-of-the-art baselines in different
parameter size of student models. In particular, it
retains more than 99% accuracy on SQuAD 2.0
and several GLUE benchmark tasks using 50%
of the Transformer parameters and computations
of the teacher model. We also obtain competitive
results in applying deep self-attention distillation
to multilingual pre-trained models.
Correspondence to: Furu Wei <fuwei@microsoft.com>.
1The code and models are publicly available at https://
aka.ms/minilm .1. Introduction
Language model (LM) pre-training has achieved remarkable
success for various natural language processing tasks (Pe-
ters et al., 2018; Howard & Ruder, 2018; Radford et al.,
2018; Devlin et al., 2018; Dong et al., 2019; Yang et al.,
2019; Joshi et al., 2019; Liu et al., 2019). The pre-trained
language models, such as BERT (Devlin et al., 2018) and its
variants, learn contextualized text representations by predict-
ing words given their context using large scale text corpora,
and can be ﬁne-tuned with additional task-speciﬁc layers to
adapt to downstream tasks. However, these models usually
contain hundreds of millions of parameters which brings
challenges for ﬁne-tuning and online serving in real-life
applications for latency and capacity constraints.
Knowledge distillation (Hinton et al., 2015; Romero et al.,
2015) (KD) has been proven to be a promising way to com-
press a large model (called the teacher model) into a small
model (called the student model), which uses much fewer
parameters and computations while achieving competitive
results on downstream tasks. There have been some works
that task-speciﬁcally distill pre-trained large LMs into small
models (Tang et al., 2019; Turc et al., 2019b; Sun et al.,
2019a; Aguilar et al., 2019). They ﬁrst ﬁne-tune the pre-
trained LMs on speciﬁc tasks and then perform distillation.
Task-speciﬁc distillation is effective, but ﬁne-tuning large
pre-trained models is still costly, especially for large datasets.
Different from task-speciﬁc distillation, task-agnostic LM
distillation mimics the behavior of the original pre-trained
LMs and the student model can be directly ﬁne-tuned on
downstream tasks (Tsai et al., 2019; Sanh et al., 2019; Jiao
et al., 2019; Sun et al., 2019b).
Previous works use soft target probabilities for masked lan-
guage modeling predictions or intermediate representations
of the teacher LM to guide the training of the task-agnostic
student. DistilBERT (Sanh et al., 2019) employs a soft-label
distillation loss and a cosine embedding loss, and initializes
the student from the teacher by taking one layer out of two.
But each Transformer layer of the student is required to
have the same architecture as its teacher. TinyBERT (Jiao
et al., 2019) and MOBILE BERT (Sun et al., 2019b) utilizearXiv:2002.10957v2  [cs.CL]  6 Apr 2020

--- PAGE 2 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
more ﬁne-grained knowledge, including hidden states and
self-attention distributions of Transformer networks, and
transfer these knowledge to the student model layer-to-layer.
To perform layer-to-layer distillation, TinyBERT adopts a
uniform function to determine the mapping between the
teacher and student layers, and uses a parameter matrix to
linearly transform student hidden states. MOBILE BERT
assumes the teacher and student have the same number of
layers and introduces the bottleneck module to keep their
hidden size the same.
In this work, we propose the deep self-attention distillation
framework for task-agnostic Transformer based LM distil-
lation. The key idea is to deeply mimic the self-attention
modules which are the fundamentally important compo-
nents in the Transformer based teacher and student models.
Speciﬁcally, we propose distilling the self-attention module
of the last Transformer layer of the teacher model. Com-
pared with previous approaches, using knowledge of the
last Transformer layer rather than performing layer-to-layer
knowledge distillation alleviates the difﬁculties in layer map-
ping between the teacher and student models, and the layer
number of our student model can be more ﬂexible. Further-
more, we introduce the scaled dot-product between values
in the self-attention module as the new deep self-attention
knowledge, in addition to the attention distributions (i.e., the
scaled dot-product of queries and keys) that has been used
in existing works. Using scaled dot-product between self-
attention values also converts representations of different
dimensions into relation matrices with the same dimensions
without introducing additional parameters to transform stu-
dent representations, allowing arbitrary hidden dimensions
for the student model. Finally, we show that introducing a
teacher assistant (Mirzadeh et al., 2019) helps the distilla-
tion of large pre-trained Transformer based models and the
proposed deep self-attention distillation can further boost
the performance.
We conduct extensive experiments on downstream NLP
tasks. Experimental results demonstrate that our monolin-
gual model outperforms state-of-the-art baselines in dif-
ferent parameter size of student models. Speciﬁcally, the
6-layer model of 768 hidden dimensions distilled from
BERT BASE is2:0faster, while retaining more than 99% ac-
curacy on SQuAD 2.0 and several GLUE benchmark tasks.
Moreover, our multilingual model distilled from XLM-R Base
also achieves competitive performance with much fewer
Transformer parameters.
2. Preliminary
Multi-layer Transformers (Vaswani et al., 2017) have been
the most widely-used network structures in state-of-the-art
pre-trained models. In this section, we present a brief intro-
duction to the Transformer network and the self-attentionmechanism, which is the core component of the Transformer.
We also present the existing approaches on knowledge distil-
lation for Transformer networks, particularly in the context
of distilling a large Transformer based pre-trained model
into a small Transformer model.
2.1. Input Representation
Texts are tokenized to subword units by WordPiece (Wu
et al., 2016) in BERT (Devlin et al., 2018). For example, the
word “ forecasted ” is split to “ forecast ” and “ ##ed ”, where
“##” indicates the pieces are belong to one word. A spe-
cial boundary token [SEP] is used to separate segments
if the input text contains more than one segment. At the
beginning of the sequence, a special token [CLS] is added
to obtain the representation of the whole input. The vec-
tor representations ( fxigjxj
i=1) of input tokens are computed
via summing the corresponding token embedding, absolute
position embedding, and segment embedding.
2.2. Backbone Network: Transformer
Transformer (Vaswani et al., 2017) is used to encode contex-
tual information for input tokens. The input vectors fxigjxj
i=1
are packed together into H0= [x1;;xjxj]. Then stacked
Transformer blocks compute the encoding vectors as:
Hl= Transformer l(Hl 1); l2[1;L] (1)
whereLis the number of Transformer layers, and the ﬁnal
output is HL= [hL
1;;hL
jxj]. The hidden vector hL
iis
used as the contextualized representation of xi. Each Trans-
former layer consists of a self-attention sub-layer and a fully
connected feed-forward network. Residual connection (He
et al., 2016) is employed around each of the two sub-layers,
followed by layer normalization (Ba et al., 2016).
Self-Attention In each layer, Transformer uses multiple
self-attention heads to aggregate the output vectors of the
previous layer. For the l-th Transformer layer, the output of
a self-attention head AOl;a; a2[1;Ah]is computed via:
Ql;a=Hl 1WQ
l;a;Kl;a=Hl 1WK
l;a;Vl;a=Hl 1WV
l;a
(2)
Al;a= softmax(Ql;aK|
l;apdk) (3)
AOl;a=Al;aVl;a (4)
where the previous layer’s output Hl 12Rjxjdhis lin-
early projected to a triple of queries, keys and values using
parameter matrices WQ
l;a;WK
l;a;WV
l;a2Rdhdk, respec-
tively. Al;a2Rjxjjxjindicates the attention distributions,
which is computed by the scaled dot-product of queries
and keys.Ahrepresents the number of self-attention heads.
dkAhis equal to the hidden dimension dhin BERT.

--- PAGE 3 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Transformer Block L
Transformer Block 3
Transformer Block 2…
Transformer Block 4
𝑥1𝑥2 𝑥5 𝑥3𝑥4…
Queries
(ℝ𝐴ℎ×𝑥×𝑑𝑘)
…
Keys
(ℝ𝐴ℎ×𝑥×𝑑𝑘)
…
TeacherLast Layer
Self-Attention VectorsValues -Values
Scaled Dot -Product
(ℝ𝐴ℎ×𝑥×|𝑥|)Queries -Keys
Scaled Dot -Product
(ℝ𝐴ℎ×𝑥×|𝑥|)Attention 
Transfer
Queries -Keys
Scaled Dot -Product
(ℝ𝐴ℎ×𝑥×|𝑥|)
Value -Relation
Transfer
Values -Values
Scaled Dot -Product
(ℝ𝐴ℎ×𝑥×|𝑥|)…
…
…
Last Layer
Self-Attention VectorsTransformer Block M
Transformer Block 3
Transformer Block 2…
StudentValues
(ℝ𝐴ℎ×𝑥×𝑑𝑘)Queries
(ℝ𝐴ℎ×𝑥×𝑑𝑘′)
Keys
(ℝ𝐴ℎ×𝑥×𝑑𝑘′)
Values
(ℝ𝐴ℎ×𝑥×𝑑𝑘′) 𝑥1𝑥2 𝑥5 𝑥3𝑥4(KL-Divergence)
(KL-Divergence)Transformer Block 1Transformer Block 1
Figure 1. Overview of Deep Self-Attention Distillation. The student is trained by deeply mimicking the self-attention behavior of the last
Transformer layer of the teacher. In addition to the self-attention distributions, we introduce the self-attention value-relation transfer to
help the student achieve a deeper mimicry. Our student models are named as M INILM.
2.3. Transformer Distillation
Knowledge distillation (Hinton et al., 2015; Romero et al.,
2015) is to train the small student model Son a transfer
feature set with soft labels and intermediate representations
provided by the large teacher model T. Knowledge distil-
lation is modeled as minimizing the differences between
teacher and student features:
LKD=X
e2DL(fS(e);fT(e)) (5)
WhereDdenotes the training data, fS()andfT()indicate
the features of student and teacher models respectively, L()
represents the loss function. The mean squared error (MSE)
and KL-divergence are often used as loss functions.
For Transformer based LM distillation, soft target probabili-
ties for masked language modeling predictions, embedding
layer outputs, self-attention distributions and outputs (hid-
den states) of each Transformer layer of the teacher model
are used as features to help the training of the student. Soft
labels and embedding layer outputs are used in DistillBERT.
TinyBERT and MOBILE BERT further utilize self-attention
distributions and outputs of each Transformer layer. For
MOBILE BERT , the student is required to have the same
number of layers as its teacher to perform layer-to-layer dis-
tillation. Besides, bottleneck and inverted bottleneck mod-
ules are introduced to keep the hidden size of the teacher
and student are also the same. To transfer knowledge layer-
to-layer, TinyBERT employs a uniform-function to map
teacher and student layers. Since the hidden size of the
student can be smaller than its teacher, a parameter matrixis introduced to transform the student features.
3. Deep Self-Attention Distillation
Figure 1 gives an overview of the deep self-attention dis-
tillation. The key idea is three-fold. First, we propose to
train the student by deeply mimicking the self-attention
module, which is the vital component in the Transformer,
of the teacher’s last layer. Second, we introduce transfer-
ring the relation between values (i.e., the scaled dot-product
between values) to achieve a deeper mimicry, in addition
to performing attention distributions (i.e., the scaled dot-
product of queries and keys) transfer in the self-attention
module. Moreover, we show that introducing a teacher as-
sistant (Mirzadeh et al., 2019) also helps the distillation of
large pre-trained Transformer models when the size gap
between the teacher model and student model is large.
3.1. Self-Attention Distribution Transfer
The attention mechanism (Bahdanau et al., 2015) has been a
highly successful neural network component for NLP tasks,
which is also crucial for pre-trained LMs. Some works show
that self-attention distributions of pre-trained LMs capture
a rich hierarchy of linguistic information (Jawahar et al.,
2019; Clark et al., 2019). Transferring self-attention distri-
butions has been used in previous works for Transformer
distillation (Jiao et al., 2019; Sun et al., 2019b; Aguilar et al.,
2019). We also utilize the self-attention distributions to help
the training of the student. Speciﬁcally, we minimize the
KL-divergence between the self-attention distributions of

--- PAGE 4 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 1. Comparison with previous task-agnostic Transformer based LM distillation approaches.
Approach Teacher Model Distilled KnowledgeLayer-to-Layer
DistillationRequirements on
the number of
layers of studentsRequirements on
the hidden
size of students
DistillBERT BERT BASESoft target probabilities
Embedding outputsX
TinyBERT BERT BASEEmbedding outputs
Hidden states
Self-Attention distributionsX
MOBILE BERT IB-BERT LARGESoft target probabilities
Hidden states
Self-Attention distributionsX X X
MINILM BERT BASESelf-Attention distributions
Self-Attention value relation
the teacher and student:
LAT=1
AhjxjAhX
a=1jxjX
t=1DKL(AT
L;a;tkAS
M;a;t)(6)
WherejxjandAhrepresent the sequence length and the
number of attention heads. LandMrepresent the number
of layers for the teacher and student. AT
LandAS
Mare the
attention distributions of the last Transformer layer for the
teacher and student, respectively. They are computed by the
scaled dot-product of queries and keys.
Different from previous works which transfer teacher’s
knowledge layer-to-layer, we only use the attention maps
of the teacher’s last Transformer layer. Distilling attention
knowledge of the last Transformer layer allows more ﬂexi-
bility for the number of layers of our student models, avoids
the effort of ﬁnding the best layer mapping.
3.2. Self-Attention Value-Relation Transfer
In addition to the attention distributions, we propose using
the relation between values in the self-attention module
to guide the training of the student. The value relation is
computed via the multi-head scaled dot-product between
values. The KL-divergence between the value relation of
the teacher and student is used as the training objective:
VRT
L;a= softmax(VT
L;aVT|
L;apdk) (7)
VRS
M;a= softmax(VS
M;aVS|
M;ap
d0
k) (8)
LVR=1
AhjxjAhX
a=1jxjX
t=1DKL(VRT
L;a;tkVRS
M;a;t)(9)
Where VT
L;a2RjxjdkandVS
M;a2Rjxjd0
kare the values
of an attention head in self-attention module for the teacher’sand student’s last Transformer layer. VRT
L2RAhjxjjxj
andVRS
M2RAhjxjjxjare the value relation of the last
Transformer layer for teacher and student, respectively.
The training loss is computed via summing the attention
distribution transfer loss and value-relation transfer loss:
L=LAT+LVR (10)
Introducing the relation between values enables the stu-
dent to deeply mimic the teacher’s self-attention behavior.
Moreover, using the scaled dot-product converts vectors of
different hidden dimensions into the relation matrices with
the same size, which allows our students to use more ﬂex-
ible hidden dimensions and avoids introducing additional
parameters to transform the student’s representations.
3.3. Teacher Assistant
Following Mirzadeh et al. (2019), we introduce a teacher
assistant (i.e., intermediate-size student model) to further
improve the model performance of smaller students.
Assuming the teacher model consists of L-layer Transformer
withdhhidden size, the student model has M-layer Trans-
former with d0
hhidden size. For smaller students ( M1
2L,
d0
h1
2dh), we ﬁrst distill the teacher into a teacher assistant
withL-layer Transformer and d0
hhidden size. The assistant
model is then used as the teacher to guide the training of the
ﬁnal student. The introduction of a teacher assistant bridges
the size gap between teacher and smaller student models,
helps the distillation of Transformer based pre-trained LMs.
Moreover, combining deep self-attention distillation with
a teacher assistant brings further improvements for smaller
student models.
3.4. Comparison with Previous Work
Table 1 presents the comparison with previous ap-
proaches (Sanh et al., 2019; Jiao et al., 2019; Sun et al.,

--- PAGE 5 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 2. Comparison between the publicly released 6-layer models with 768 hidden size distilled from BERT BASE. We compare task-
agnostic distilled models without task-speciﬁc distillation and data augmentation. We report F1 for SQuAD 2.0, and accuracy for other
datasets. The GLUE results of DistillBERT are taken from Sanh et al. (2019). We report the SQuAD 2.0 result by ﬁne-tuning their
released model3. For TinyBERT, we ﬁne-tune the latest version of their public model4for a fair comparison. The results of our ﬁne-tuning
experiments are an average of 4runs for each task.
Model #Param SQuAD2 MNLI-m SST-2 QNLI CoLA RTE MRPC QQP Average
BERT BASE 109M 76.8 84.5 93.2 91.7 58.9 68.6 87.3 91.3 81.5
DistillBERT 66M 70.7 79.0 90.7 85.3 43.6 59.9 87.5 84.9 75.2
TinyBERT 66M 73.1 83.5 91.6 90.5 42.8 72.2 88.4 90.6 79.1
MINILM 66M 76.4 84.0 92.0 91.0 49.2 71.5 88.4 91.0 80.4
2019b). MOBILE BERT proposes using a specially de-
signed inverted bottleneck model, which has the same model
size as BERT LARGE , as the teacher. The other methods uti-
lize BERT BASE to conduct experiments. For the knowledge
used for distillation, our method introduces the scaled dot-
product between values in the self-attention module as the
new knowledge to deeply mimic teacher’s self-attention be-
havior. TinyBERT and MOBILE BERT transfer knowledge
of the teacher to the student layer-to-layer. MOBILE BERT
assumes the student has the same number of layers as its
teacher. TinyBERT employs a uniform strategy to deter-
mine its layer mapping. DistillBERT initializes the student
with teacher’s parameters, therefore selecting layers of the
teacher model is still needed. MINILM distills the self-
attention knowledge of the teacher’s last Transformer layer,
which allows the ﬂexible number of layers for the students
and alleviates the effort of ﬁnding the best layer mapping.
Student hidden size of DistillBERT and MOBILE BERT is
required to be the same as its teacher. TinyBERT uses a
parameter matrix to transform student hidden states. Using
value relation allows our students to use arbitrary hidden
size without introducing additional parameters.
4. Experiments
We conduct distillation experiments in different parameter
size of student models, and evaluate the distilled models on
downstream tasks including extractive question answering
and the GLUE benchmark.
4.1. Distillation Setup
We use the uncased version of BERT BASE as our teacher.
BERT BASE (Devlin et al., 2018) is a 12-layer Transformer
with768hidden size, and 12attention heads, which contains
about 109M parameters. The number of heads of attention
distributions and value relation are set to 12for student
models. We use documents of English Wikipedia2and
BookCorpus (Zhu et al., 2015) for the pre-training data,
following the preprocess and the WordPiece tokenization
2Wikipedia version: enwiki-20181101.of Devlin et al. (2018). The vocabulary size is 30;522. The
maximum sequence length is 512. We use Adam (Kingma &
Ba, 2015) with 1= 0:9,2= 0:999. We train the 6-layer
student model with 768hidden size using 1024 as the batch
size and 5e-4 as the peak learning rate for 400;000steps.
For student models of other architectures, the batch size
and peak learning rate are set to 256and 3e-4, respectively.
We use linear warmup over the ﬁrst 4;000steps and linear
decay. The dropout rate is 0:1. The weight decay is 0:01.
We also use an in-house pre-trained Transformer model
in the BERT BASE size as the teacher model, and distill it
into12-layer and 6-layer student models with 384hidden
size. For the 12-layer model, we use Adam (Kingma & Ba,
2015) with1= 0:9,2= 0:98. The model is trained
using 2048 as the batch size and 6e-4 as the peak learning
rate for 400;000steps. The batch size and peak learning
rate are set to 512and 4e-4 for the 6-layer model. The
rest hyper-parameters are the same as above BERT based
distilled models.
For the training of multilingual MINILMmodels, we use
Adam (Kingma & Ba, 2015) with 1= 0:9,2= 0:999.
We train the 12-layer student model using 256as the batch
size and 3e-4 as the peak learning rate for 1;000;000steps.
The6-layer student model is trained using 512as the batch
size and 6e-4 as the peak learning rate for 400;000steps.
We distill our student models using 8V100 GPUs with
mixed precision training. Following Sun et al. (2019a)
and Jiao et al. (2019), the inference time is evaluated on
the QNLI training set with the same hyper-parameters. We
report the average running time of 100batches on a single
P100 GPU.
4.2. Downstream Tasks
Following previous language model pre-training (Devlin
et al., 2018; Liu et al., 2019) and task-agnostic pre-trained
language model distillation (Sanh et al., 2019; Jiao et al.,
2019; Sun et al., 2019b), we evaluate our distilled models
on the extractive question answering and GLUE benchmark.

--- PAGE 6 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 3. Comparison between student models of different architectures distilled from BERT BASE.Mandd0
hindicate the number of layers
and hidden dimension of the student model. TA indicates teacher assistant5. The ﬁne-tuning results are averaged over 4runs.
Architecture #Param Model SQuAD 2.0 MNLI-m SST-2 Average
M=6;d0
h=384 22MMLM-KD (Soft-Label Distillation) 67.9 79.6 89.8 79.1
TinyBERT 71.6 81.4 90.2 81.1
MINILM 72.4 82.2 91.0 81.9
MINILM (w/ TA) 72.7 82.4 91.2 82.1
M=4;d0
h=384 19MMLM-KD (Soft-Label Distillation) 65.3 77.7 88.8 77.3
TinyBERT 66.7 79.2 88.5 78.1
MINILM 69.4 80.3 90.2 80.0
MINILM (w/ TA) 69.7 80.6 90.6 80.3
M=3;d0
h=384 17MMLM-KD (Soft-Label Distillation) 59.9 75.2 88.0 74.4
TinyBERT 63.6 77.4 88.4 76.5
MINILM 66.2 78.8 89.3 78.1
MINILM (w/ TA) 66.9 79.1 89.7 78.6
Table 4. The number of Embedding (Emd) and Transformer (Trm)
parameters, and inference time for different models.
#LayersHidden
Size#Param
(Emd)#Param
(Trm)Inference
Time
12 768 23.4M 85.1M 93.1s (1.0 )
6 768 23.4M 42.5M 46.9s (2.0 )
12 384 11.7M 21.3M 34.8s (2.7 )
6 384 11.7M 10.6M 17.7s (5.3 )
4 384 11.7M 7.1M 12.0s (7.8 )
3 384 11.7M 5.3M 9.2s (10.1 )
Extractive Question Answering Given a passage P, the
task is to select a contiguous span of text in the passage by
predicting its start and end positions to answer the question
Q. We evaluate on SQuAD 2.0 (Rajpurkar et al., 2018),
which has served as a major question answering benchmark.
Following BERT (Devlin et al., 2018), we pack the question
and passage tokens together with special tokens, to form the
input: “ [CLS]Q[SEP]P[SEP] ". Two linear output
layers are introduced to predict the probability of each token
being the start and end positions of the answer span. The
questions that do not have an answer are treated as having
an answer span with start and end at the [CLS] token.
GLUE The General Language Understanding Evaluation
(GLUE) benchmark (Wang et al., 2019) consists of nine
sentence-level classiﬁcation tasks, including Corpus of Lin-
guistic Acceptability (CoLA) (Warstadt et al., 2018), Stan-
ford Sentiment Treebank (SST) (Socher et al., 2013), Mi-
crosoft Research Paraphrase Corpus (MRPC) (Dolan &
Brockett, 2005), Semantic Textual Similarity Benchmark
(STS) (Cer et al., 2017), Quora Question Pairs (QQP) (Chen
et al., 2018), Multi-Genre Natural Language Inference(MNLI) (Williams et al., 2018), Question Natural Language
Inference (QNLI) (Rajpurkar et al., 2016), Recognizing Tex-
tual Entailment (RTE) (Dagan et al., 2006; Bar-Haim et al.,
2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and
Winograd Natural Language Inference (WNLI) (Levesque
et al., 2012). We add a linear classiﬁer on top of the [CLS]
token to predict label probabilities.
4.3. Main Results
Previous works (Sanh et al., 2019; Sun et al., 2019a; Jiao
et al., 2019) usually distill BERT BASE into a 6-layer student
model with 768hidden size. We ﬁrst conduct distillation
experiments using the same student architecture. Results on
SQuAD 2.0 and GLUE dev sets are presented in Table 2.
Since MOBILE BERT distills a specially designed teacher
with the inverted bottleneck modules, which has the same
model size as BERT LARGE , into a 24-layer student using
the bottleneck modules, we do not compare our models
with MOBILE BERT .MINILMoutperforms DistillBERT3
and TinyBERT4across most tasks. Our model exceeds the
two state-of-the-art models by 3:0+%F1 on SQuAD 2.0 and
5:0+%accuracy on CoLA. We present the inference time for
models in different parameter size in Table 4. Our 6-layer
768-dimensional student model is 2:0faster than original
BERT BASE, while retaining more than 99% performance on
a variety of tasks, such as SQuAD 2.0 and MNLI.
We also conduct experiments for smaller student models.
We compare MINILM with our implemented MLM-KD
3The public model of DistillBERT is obtained from https:
//github.com/huggingface/transformers/tree/
master/examples/distillation
4We use the 2nd version TinyBERT from https://github.
com/huawei-noah/Pretrained-Language-Model/
tree/master/TinyBERT

--- PAGE 7 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 5. Effectiveness of self-attention value-relation (Value-Rel)
transfer. The ﬁne-tuning results are averaged over 4runs.
Architecture Model SQuAD2 MNLI-m SST-2
M=6;d0
h=384MINILM 72.4 82.2 91.0
-Value-Rel 71.0 80.9 89.9
M=4;d0
h=384MINILM 69.4 80.3 90.2
-Value-Rel 67.5 79.0 89.2
M=3;d0
h=384MINILM 66.2 78.8 89.3
-Value-Rel 64.2 77.8 88.3
(knowledge distillation using soft target probabilities for
masked language modeling predictions) and TinyBERT,
which are trained using the same data and hyper-parameters.
The results on SQuAD 2.0, MNLI and SST-2 dev sets are
shown in Table 3. MINILM outperforms soft label distil-
lation and our implemented TinyBERT on the three tasks.
Deep self-attention distillation is also effective for smaller
models. Moreover, we show that introducing a teacher as-
sistant5is also helpful in Transformer based pre-trained LM
distillation, especially for smaller models. Combining deep
self-attention distillation with a teacher assistant achieves
further improvement for smaller student models.
4.4. Ablation Studies
We do ablation tests on several tasks to analyze the contribu-
tion of self-attention value-relation transfer. The dev results
of SQuAD 2.0, MNLI and SST-2 are illustrated in Table 5,
using self-attention value-relation transfer positively con-
tributes to the ﬁnal results for student models in different
parameter size. Distilling the ﬁne-grained knowledge of
value relation helps the student model deeply mimic the self-
attention behavior of the teacher, which further improves
model performance.
We also compare different loss functions over values in the
self-attention module. We compare our proposed value re-
lation with mean squared error (MSE) over the teacher and
student values. An additional parameter matrix is introduced
to transform student values if the hidden dimension of the
student is smaller than its teacher. The dev results on three
tasks are presented in Table 6. Using value relation achieves
better performance. Speciﬁcally, our method brings about
1:0%F1 improvement on the SQuAD benchmark. More-
over, there is no need to introduce additional parameters for
our method. We have also tried to transfer the relation be-
tween hidden states. But we ﬁnd the performance of student
models are unstable for different teacher models.
To show the effectiveness of distilling self-attention knowl-
5The teacher assistant is only introduced for the model
MINILM(w/ TA). The model MINILMin different tables is di-
rectly distilled from its teacher model.Table 6. Comparison between different loss functions: KL-
divergence over the value relation (the scaled dot-product between
values) and mean squared error (MSE) over values. A parameter
matrix is introduced to transform student values to have the same
dimensions as the teacher values (Jiao et al., 2019). The ﬁne-tuning
results are an average of 4runs for each task.
Architecture Model SQuAD2 MNLI-m SST-2
M=6;d0
h=384MINILM 72.4 82.2 91.0
Value-MSE 71.4 82.0 90.8
M=4;d0
h=384MINILM 69.4 80.3 90.2
Value-MSE 68.3 80.1 89.9
M=3;d0
h=384MINILM 66.2 78.8 89.3
Value-MSE 65.5 78.4 89.3
edge of the teacher’s last Transformer layer, we compare
our method with layer-to-layer distillation. We transfer the
same knowledge and adopt a uniform strategy as in Jiao
et al. (2019) to map teacher and student layers to perform
layer-to-layer distillation. The dev results on three tasks are
presented in Table 7. MINILM achieves better results. It
also alleviates the difﬁculties in layer mapping between the
teacher and student. Besides, distilling the teacher’s last
Transformer layer requires less computation than layer-to-
layer distillation, results in faster training speed.
5. Discussion
5.1. Better Teacher Better Student
We report the results of MINILMdistilled from an in-house
pre-trained Transformer model following UNILM (Dong
et al., 2019; Bao et al., 2020) in the BERT BASE size. The
teacher model is trained using similar pre-training datasets
as in RoBERTa BASE (Liu et al., 2019), which includes
160GB text corpora from English Wikipedia, BookCor-
pus (Zhu et al., 2015), OpenWebText6, CC-News (Liu et al.,
2019), and Stories (Trinh & Le, 2018). We distill the teacher
model into 12-layer and 6-layer models with 384hidden
size using the same corpora. The 12x384model is used as
the teacher assistant to train the 6x384model. We present
the dev results of SQuAD 2.0 and GLUE benchmark in
Table 8, the results of MINILMare signiﬁcantly improved.
The12x384MINILM achieves 2:7speedup while per-
forms competitively better than BERT BASE in SQuAD 2.0
and GLUE benchmark datasets.
5.2. M INILM for NLG Tasks
We also evaluate MINILMon natural language generation
tasks, such as question generation and abstractive sum-
marization. Following Dong et al. (2019), we ﬁne-tune
6skylion007.github.io/OpenWebTextCorpus

--- PAGE 8 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 7. Comparison between distilling knowledge of the teacher’s last Transformer layer and layer-to-layer distillation. We adopt a
uniform strategy as in Jiao et al. (2019) to determine the mapping between teacher and student layers. The ﬁne-tuning results are an
average of 4runs for each task.
Architecture Model SQuAD 2.0 MNLI-m SST-2 Average
M=6;d0
h=384MINILM 72.4 82.2 91.0 81.9
+Layer-to-Layer Distillation 71.6 81.8 90.6 81.3
M=4;d0
h=384MINILM 69.4 80.3 90.2 80.0
+Layer-to-Layer Distillation 67.6 79.9 89.6 79.0
M=3;d0
h=384MINILM 66.2 78.8 89.3 78.1
+Layer-to-Layer Distillation 64.8 77.7 88.6 77.0
Table 8. The results of MINILM distilled from an in-house pre-trained Transformer model (BERT BASE size, 12-layer Transformer,
768-hidden size, and 12self-attention heads) on SQuAD 2.0 and GLUE benchmark. We report our 12-layeraand6-layerbmodels with
384hidden size. The ﬁne-tuning results are averaged over 4runs.
Model #Param SQuAD2 MNLI-m SST-2 QNLI CoLA RTE MRPC QQP Average
BERT BASE 109M 76.8 84.5 93.2 91.7 58.9 68.6 87.3 91.3 81.5
MINILMa33M 81.7 85.7 93.0 91.5 58.5 73.3 89.5 91.3 83.1
MINILMb(w/ TA) 22M 75.6 83.3 91.5 90.5 47.5 68.8 88.9 90.6 79.6
Table 9. Question generation results of our 12-layeraand6-layerb
models with 384 hidden size on SQuAD 1.1. The ﬁrst block
follows the data split in Du & Cardie (2018), while the second
block is the same as in Zhao et al. (2018). MTR is short for
METEOR, RG for ROUGE, and B for BLEU.
#Param B-4 MTR RG-L
(Du & Cardie, 2018) 15.16 19.12 -
(Zhang & Bansal, 2019) 18.37 22.65 46.68
UNILM LARGE 340M 22.78 25.49 51.57
MINILMa33M 21.07 24.09 49.14
MINILMb(w/ TA) 22M 20.31 23.43 48.21
(Zhao et al., 2018) 16.38 20.25 44.48
(Zhang & Bansal, 2019) 20.76 24.20 48.91
UNILM LARGE 340M 24.32 26.10 52.69
MINILMa33M 23.27 25.15 50.60
MINILMb(w/ TA) 22M 22.01 24.24 49.51
MINILMas a sequence-to-sequence model by employing a
speciﬁc self-attention mask.
Question Generation We conduct experiments for the
answer-aware question generation task (Du & Cardie, 2018).
Given an input passage and an answer, the task is to gen-
erate a question that asks for the answer. The SQuAD 1.1
dataset (Rajpurkar et al., 2016) is used for evaluation. The
results of MINILM,UNILM LARGE and several state-of-the-
art models are presented in Table 9, our 12x384and6x384
distilled models achieve competitive performance on the
question generation task.Abstractive Summarization We evaluate MINILM
on two abstractive summarization datasets, i.e.,
XSum (Narayan et al., 2018), and the non-anonymized
version of CNN/DailyMail (See et al., 2017). The
generation task is to condense a document into a concise
and ﬂuent summary, while conveying its key information.
We report ROUGE scores (Lin, 2004) on the datasets.
Table 10 presents the results of MINILM, baseline, several
state-of-the-art models and pre-trained Transformer models.
Our 12x384 model outperforms BERT based method
BERTS UMABS(Liu & Lapata, 2019) and the pre-trained
sequence-to-sequence model MASS BASE (Song et al.,
2019) with much fewer parameters. Moreover, our 6x384
MINILM also achieves competitive performance.
5.3. Multilingual M INILM
We conduct experiments on task-agnostic knowledge dis-
tillation of multilingual pre-trained models. We use the
XLM-R Base7(Conneau et al., 2019) as the teacher and distill
the model into 12-layer and 6-layer models with 384 hidden
size using the same corpora. The 6x384model is trained
using the 12x384model as the teacher assistant. Given
the vocabulary size of multilingual pre-trained models is
much larger than monolingual models (30k for monolin-
gual BERT, 250k for XLM-R), soft-label distillation for
multilingual pre-trained models requires more computation.
MINILM only uses the deep self-attention knowledge of
the teacher’s last Transformer layer. The training speed
7We use the v 0version of XLM-R Basein our distillation and
ﬁne-tuning experiments.

--- PAGE 9 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 10. Abstractive summarization results of our 12-layeraand6-layerbmodels with 384hidden size on CNN/DailyMail and XSum.
The evaluation metric is the F1 version of ROUGE (RG) scores.
Model #ParamCNN/DailyMail XSum
RG-1 RG-2 RG-L RG-1 RG-2 RG-L
LEAD-3 40.42 17.62 36.67 16.30 1.60 11.95
PTRNET(See et al., 2017) 39.53 17.28 36.38 28.10 8.02 21.72
Bottom-Up (Gehrmann et al., 2018) 41.22 18.68 38.34 - - -
UNILM LARGE (Dong et al., 2019) 340M 43.08 20.43 40.34 - - -
BART LARGE (Lewis et al., 2019a) 400M 44.16 21.28 40.90 45.14 22.27 37.25
T511B(Raffel et al., 2019) 11B 43.52 21.55 40.69 - - -
MASS BASE(Song et al., 2019) 123M 42.12 19.50 39.01 39.75 17.24 31.95
BERTS UMABS(Liu & Lapata, 2019) 156M 41.72 19.39 38.76 38.76 16.33 31.15
T5 BASE(Raffel et al., 2019) 220M 42.05 20.34 39.40 - - -
MINILMa33M 42.66 19.91 39.73 40.43 17.72 32.60
MINILMb(w/ TA) 22M 41.57 19.21 38.64 38.79 16.39 31.10
Table 11. Cross-lingual classiﬁcation results of our 12-layeraand6-layerbmultilingual models with 384hidden size on XNLI. We
report the accuracy on each of the 15 XNLI languages and the average accuracy. Results of mBERT, XLM-100 and XLM-R Baseare
from Conneau et al. (2019).
Model #Layers #Hidden en fr es de el bg ru tr ar vi th zh hi sw ur Avg
mBERT 12 768 82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.0 66.3
XLM- 100 16 1280 83.2 76.7 77.7 74.0 72.7 74.1 72.7 68.7 68.6 72.9 68.9 72.5 65.6 58.2 62.4 70.7
XLM-R Base 12 768 84.6 78.4 78.9 76.8 75.9 77.3 75.4 73.2 71.5 75.4 72.5 74.9 71.1 65.2 66.5 74.5
MINILMa12 384 81.5 74.8 75.7 72.9 73.0 74.5 71.3 69.7 68.8 72.1 67.8 70.0 66.2 63.3 64.2 71.1
MINILMb(w/ TA) 6 384 79.2 72.3 73.1 70.3 69.1 72.0 69.1 64.5 64.9 69.0 66.0 67.8 62.9 59.0 60.6 68.0
Table 12. The number of Transformer (Trm) and Embedding
(Emd) parameters for different multilingual pre-trained models
and our distilled models.
Model #LayersHidden
Size#Vocab#Param
(Trm)#Param
(Emd)
mBERT 12 768 110k 85M 85M
XLM-15 12 1024 95k 151M 97M
XLM-100 16 1280 200k 315M 256M
XLM-R Base 12 768 250k 85M 192M
MINILMa12 384 250k 21M 96M
MINILMb6 384 250k 11M 96M
ofMINILM is much faster than soft-label distillation for
multilingual pre-trained models.
We evaluate the student models on cross-lingual natural
language inference (XNLI) benchmark (Conneau et al.,
2018) and cross-lingual question answering (MLQA) bench-
mark (Lewis et al., 2019b).
XNLI Table 11 presents XNLI results of our distilled stu-
dents and several pre-trained LMs. Following Conneau et al.
(2019), we select the best single model on the joint dev
set of all the languages. We present the number of Trans-
former and embedding parameters for different multilingualpre-trained models and our distilled models in Table 12.
MINILMachieves competitive performance on XNLI with
much fewer Transformer parameters. Moreover, the 12x384
MINILMcompares favorably with mBERT (Devlin et al.,
2018) and XLM (Lample & Conneau, 2019) trained on the
MLM objective.
MLQA Table 13 shows cross-lingual question answering
results. Following Lewis et al. (2019b), we adopt SQuAD
1.1 as training data and use MLQA English development
data for early stopping. The 12x384MINILM performs
competitively better than mBERT and XLM. Our 6-layer
MINILM also achieves competitive performance.
6. Related Work
6.1. Pre-trained Language Models
Unsupervised pre-training of language models (Peters et al.,
2018; Howard & Ruder, 2018; Radford et al., 2018; Devlin
et al., 2018; Baevski et al., 2019; Song et al., 2019; Dong
et al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al.,
2019; Lewis et al., 2019a; Raffel et al., 2019) has achieved
signiﬁcant improvements for a wide range of NLP tasks.
Early methods for pre-training (Peters et al., 2018; Radford
et al., 2018) were based on standard language models. Re-

--- PAGE 10 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 13. Cross-lingual question answering results of our 12-layeraand6-layerbmultilingual models with 384hidden size on MLQA.
We report the F1 and EM (exact match) scores on each of the 7 MLQA languages. Results of mBERT and XLM- 15are taken from Lewis
et al. (2019b). yindicates results of XLM-R Basetaken from Conneau et al. (2019). We also report our ﬁne-tuned results ( z) of XLM-R Base.
Model #Layers #Hidden en es de ar hi vi zh Avg
mBERT 12 768 77.7 / 65.2 64.3 / 46.6 57.9 / 44.3 45.7 / 29.8 43.8 / 29.7 57.1 / 38.6 57.5 / 37.3 57.7 / 41.6
XLM- 15 12 1024 74.9 / 62.4 68.0 / 49.8 62.2 / 47.6 54.8 / 36.3 48.8 / 27.3 61.4 / 41.8 61.1 / 39.6 61.6 / 43.5
XLM-R Basey 12 768 77.8 / 65.3 67.2 / 49.7 60.8 / 47.1 53.0 / 34.7 57.9 / 41.7 63.1 / 43.1 60.2 / 38.0 62.9 / 45.7
XLM-R Basez 12 768 80.3 / 67.4 67.0 / 49.2 62.7 / 48.3 55.0 / 35.6 60.4 / 43.7 66.5 / 45.9 62.3 / 38.3 64.9 / 46.9
MINILMa12 384 79.4 / 66.5 66.1 / 47.5 61.2 / 46.5 54.9 / 34.9 58.5 / 41.3 63.1 / 42.1 59.0 / 33.8 63.2 / 44.7
MINILMb(w/ TA) 6 384 75.5 / 61.9 55.6 / 38.2 53.3 / 37.7 43.5 / 26.2 46.9 / 31.5 52.0 / 33.1 48.8 / 27.3 53.7 / 36.6
cently, BERT (Devlin et al., 2018) proposes to use a masked
language modeling objective to train a deep bidirectional
Transformer encoder, which learns interactions between left
and right context. Liu et al. (2019) show that very strong
performance can be achieved by training the model longer
over more data. Joshi et al. (2019) extend BERT by masking
contiguous random spans. Yang et al. (2019) predict masked
tokens auto-regressively in a permuted order.
To extend the applicability of pre-trained Transformers for
NLG tasks. Dong et al. (2019) extend BERT by utilizing spe-
ciﬁc self-attention masks to jointly optimize bidirectional,
unidirectional and sequence-to-sequence masked language
modeling objectives. Raffel et al. (2019) employ an encoder-
decoder Transformer and perform sequence-to-sequence
pre-training by predicting the masked tokens in the encoder
and decoder. Different from Raffel et al. (2019), Lewis et al.
(2019a) predict tokens auto-regressively in the decoder.
6.2. Knowledge Distillation
Knowledge distillation has proven a promising way to com-
press large models while maintaining accuracy. It transfers
the knowledge of a large model or an ensemble of neural
networks (teacher) to a single lightweight model (student).
Hinton et al. (2015) ﬁrst propose transferring the knowledge
of the teacher to the student by using its soft target distri-
butions to train the distilled model. Romero et al. (2015)
introduce intermediate representations from hidden layers
of the teacher to guide the training of the student. Knowl-
edge of the attention maps (Zagoruyko & Komodakis, 2017;
Hu et al., 2018) is also introduced to help the training.
In this work, we focus on task-agnostic knowledge dis-
tillation of large pre-trained Transformer based language
models. There have been some works that task-speciﬁcally
distill the ﬁne-tuned language models on downstream tasks.
Tang et al. (2019) distill ﬁne-tuned BERT into an extremely
small bidirectional LSTM. Turc et al. (2019a) initialize the
student with a small pre-trained LM during task-speciﬁc dis-
tillation. Sun et al. (2019a) introduce the hidden states from
everyklayers of the teacher to perform knowledge distilla-
tion layer-to-layer. Aguilar et al. (2019) further introduce
the knowledge of self-attention distributions and proposeprogressive and stacked distillation methods. Task-speciﬁc
distillation requires to ﬁrst ﬁne-tune the large pre-trained
LMs on downstream tasks and then perform knowledge
transfer. The procedure of ﬁne-tuning large pre-trained LMs
is costly and time-consuming, especially for large datasets.
For task-agnostic distillation, the distilled model mimics the
original large pre-trained LM and can be directly ﬁne-tuned
on downstream tasks. In practice, task-agnostic compres-
sion of pre-trained LMs is more desirable. MiniBERT (Tsai
et al., 2019) uses the soft target distributions for masked
language modeling predictions to guide the training of the
multilingual student model and shows its effectiveness on
sequence labeling tasks. DistillBERT (Sanh et al., 2019)
uses the soft label and embedding outputs of the teacher to
train the student. TinyBERT (Jiao et al., 2019) and MOBILE -
BERT (Sun et al., 2019b) further introduce self-attention
distributions and hidden states to train the student. MOBILE -
BERT employs inverted bottleneck and bottleneck modules
for teacher and student to make their hidden dimensions
the same. The student model of MOBILE BERT is required
to have the same number of layers as its teacher to per-
form layer-to-layer distillation. Besides, MOBILE BERT
proposes a bottom-to-top progressive scheme to transfer
teacher’s knowledge. TinyBERT uses a uniform-strategy to
map the layers of teacher and student when they have dif-
ferent number of layers, and a linear matrix is introduced to
transform the student hidden states to have the same dimen-
sions as the teacher. TinyBERT also introduces task-speciﬁc
distillation and data augmentation for downstream tasks,
which brings further improvements.
Different from previous works, our method employs the self-
attention distributions and value relation of the teacher’s last
Transformer layer to help the student deeply mimic the self-
attention behavior of the teacher. Using knowledge of the
last Transformer layer instead of layer-to-layer distillation
avoids restrictions on the number of student layers and the
effort of ﬁnding the best layer mapping. Distilling rela-
tion between self-attention values allows the hidden size of
students to be more ﬂexible and avoids introducing linear
matrices to transform student representations.

--- PAGE 11 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
7. Conclusion
In this work, we propose a simple and effective knowledge
distillation method to compress large pre-trained Trans-
former based language models. The student is trained
by deeply mimicking the teacher’s self-attention modules,
which are the vital components of the Transformer networks.
We propose using the self-attention distributions and value
relation of the teacher’s last Transformer layer to guide the
training of the student, which is effective and ﬂexible for
the student models. Moreover, we show that introducing a
teacher assistant also helps pre-trained Transformer based
LM distillation, and the proposed deep self-attention distil-
lation can further boost the performance. Our student model
distilled from BERT BASE retains high accuracy on SQuAD
2.0 and the GLUE benchmark tasks, and outperforms state-
of-the-art baselines. The deep self-attention distillation can
also be applied to compress pre-trained models in larger
size. We leave it as our future work.
References
Aguilar, G., Ling, Y ., Zhang, Y ., Yao, B., Fan, X., and Guo,
E. Knowledge distillation from internal representations.
CoRR , abs/1910.03723, 2019. URL http://arxiv.
org/abs/1910.03723 .
Ba, L. J., Kiros, J. R., and Hinton, G. E. Layer normalization.
CoRR , abs/1607.06450, 2016. URL http://arxiv.
org/abs/1607.06450 .
Baevski, A., Edunov, S., Liu, Y ., Zettlemoyer, L., and Auli,
M. Cloze-driven pretraining of self-attention networks.
arXiv preprint arXiv:1903.07785 , 2019.
Bahdanau, D., Cho, K., and Bengio, Y . Neural machine
translation by jointly learning to align and translate. In
3rd International Conference on Learning Representa-
tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings , 2015.
Bao, H., Dong, L., Wei, F., Wang, W., Yang, N., Liu,
X., Wang, Y ., Piao, S., Gao, J., Zhou, M., and Hon,
H.-W. Unilmv2: Pseudo-masked language models for
uniﬁed language model pre-training. arXiv preprint
arXiv:2002.12804 , 2020.
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., and Giampic-
colo, D. The second PASCAL recognising textual entail-
ment challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment ,
01 2006.
Bentivogli, L., Dagan, I., Dang, H. T., Giampiccolo, D.,
and Magnini, B. The ﬁfth pascal recognizing textual
entailment challenge. In In Proc Text Analysis Conference
(TAC’09 , 2009.Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia,
L. Semeval-2017 task 1: Semantic textual similarity-
multilingual and cross-lingual focused evaluation. arXiv
preprint arXiv:1708.00055 , 2017.
Chen, Z., Zhang, H., Zhang, X., and Zhao, L. Quora ques-
tion pairs. 2018.
Clark, K., Khandelwal, U., Levy, O., and Manning, C. D.
What does BERT look at? an analysis of bert’s attention.
CoRR , abs/1906.04341, 2019. URL http://arxiv.
org/abs/1906.04341 .
Conneau, A., Lample, G., Rinott, R., Williams, A., Bowman,
S. R., Schwenk, H., and Stoyanov, V . Xnli: Evaluating
cross-lingual sentence representations. In Proceedings of
the 2018 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , 2018.
Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V .,
Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer,
L., and Stoyanov, V . Unsupervised cross-lingual repre-
sentation learning at scale. CoRR , abs/1911.02116, 2019.
URLhttp://arxiv.org/abs/1911.02116 .
Dagan, I., Glickman, O., and Magnini, B. The pas-
cal recognising textual entailment challenge. In Pro-
ceedings of the First International Conference on Ma-
chine Learning Challenges: Evaluating Predictive Un-
certainty Visual Object Classiﬁcation, and Recognizing
Textual Entailment , MLCW’05, pp. 177–190, Berlin, Hei-
delberg, 2006. Springer-Verlag. ISBN 3-540-33427-0,
978-3-540-33427-9. doi: 10.1007/11736790_9. URL
http://dx.doi.org/10.1007/11736790_9 .
Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. CoRR , abs/1810.04805, 2018.
Dolan, W. B. and Brockett, C. Automatically construct-
ing a corpus of sentential paraphrases. In Proceedings
of the Third International Workshop on Paraphrasing
(IWP2005) , 2005.
Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y .,
Gao, J., Zhou, M., and Hon, H.-W. Uniﬁed language
model pre-training for natural language understanding
and generation. In 33rd Conference on Neural Informa-
tion Processing Systems (NeurIPS 2019) , 2019.
Du, X. and Cardie, C. Harvesting paragraph-level question-
answer pairs from wikipedia. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July 15-20,
2018, Volume 1: Long Papers , pp. 1907–1917, 2018.

--- PAGE 12 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Gehrmann, S., Deng, Y ., and Rush, A. Bottom-up ab-
stractive summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pp. 4098–4109, Brussels, Belgium,
October-November 2018. Association for Computational
Linguistics. URL https://www.aclweb.org/
anthology/D18-1443 .
Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. The
third PASCAL recognizing textual entailment challenge.
InProceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing , pp. 1–9, Prague, June 2007.
Association for Computational Linguistics. URL https:
//www.aclweb.org/anthology/W07-1401 .
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV , USA, June 27-30, 2016 , pp. 770–778,
2016.
Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowl-
edge in a neural network. CoRR , abs/1503.02531, 2015.
URLhttp://arxiv.org/abs/1503.02531 .
Howard, J. and Ruder, S. Universal language model ﬁne-
tuning for text classiﬁcation. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp. 328–339, Mel-
bourne, Australia, July 2018. Association for Compu-
tational Linguistics. URL https://www.aclweb.
org/anthology/P18-1031 .
Hu, M., Peng, Y ., Wei, F., Huang, Z., Li, D., Yang, N.,
and Zhou, M. Attention-guided answer distillation for
machine reading comprehension. In Proceedings of
the 2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October 31 -
November 4, 2018 , pp. 2077–2086, 2018. URL https:
//www.aclweb.org/anthology/D18-1232/ .
Jawahar, G., Sagot, B., and Seddah, D. What does BERT
learn about the structure of language? In Proceedings
of the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers , pp. 3651–
3657, 2019. URL https://www.aclweb.org/
anthology/P19-1356/ .
Jiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L.,
Wang, F., and Liu, Q. Tinybert: Distilling BERT for natu-
ral language understanding. CoRR , abs/1909.10351, 2019.
URLhttp://arxiv.org/abs/1909.10351 .
Joshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,
L., and Levy, O. Spanbert: Improving pre-training
by representing and predicting spans. arXiv preprint
arXiv:1907.10529 , 2019.Kingma, D. P. and Ba, J. Adam: A method for stochas-
tic optimization. In 3rd International Conference on
Learning Representations , San Diego, CA, 2015. URL
http://arxiv.org/abs/1412.6980 .
Lample, G. and Conneau, A. Cross-lingual language model
pretraining. Advances in Neural Information Processing
Systems (NeurIPS) , 2019.
Levesque, H., Davis, E., and Morgenstern, L. The winograd
schema challenge. In Thirteenth International Confer-
ence on the Principles of Knowledge Representation and
Reasoning , 2012.
Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L.
BART: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461 , 2019a.
Lewis, P. S. H., Oguz, B., Rinott, R., Riedel, S., and
Schwenk, H. MLQA: evaluating cross-lingual extrac-
tive question answering. CoRR , abs/1910.07475, 2019b.
URLhttp://arxiv.org/abs/1910.07475 .
Lin, C.-Y . ROUGE: A package for automatic evalua-
tion of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop , pp. 74–81,
Barcelona, Spain, July 2004. Association for Compu-
tational Linguistics. URL https://www.aclweb.
org/anthology/W04-1013 .
Liu, Y . and Lapata, M. Text summarization with pretrained
encoders. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language
Processing , pp. 3730–3740, Hong Kong, China, 2019.
Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .
RoBERTa: A robustly optimized BERT pretraining ap-
proach. arXiv preprint arXiv:1907.11692 , 2019.
Mirzadeh, S., Farajtabar, M., Li, A., and Ghasemzadeh,
H. Improved knowledge distillation via teacher assistant:
Bridging the gap between student and teacher. CoRR ,
abs/1902.03393, 2019. URL http://arxiv.org/
abs/1902.03393 .
Narayan, S., Cohen, S. B., and Lapata, M. Don’t give me the
details, just the summary! topic-aware convolutional neu-
ral networks for extreme summarization. In Proceedings
of the 2018 Conference on Empirical Methods in Natural
Language Processing , pp. 1797–1807, Brussels, Belgium,
October-November 2018. Association for Computational
Linguistics. doi: 10.18653/v1/D18-1206. URL https:
//www.aclweb.org/anthology/D18-1206 .

--- PAGE 13 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., and Zettlemoyer, L. Deep contextualized
word representations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers) , pp. 2227–2237, New
Orleans, Louisiana, June 2018. Association for Computa-
tional Linguistics. URL http://www.aclweb.org/
anthology/N18-1202 .
Radford, A., Narasimhan, K., Salimans, T., and
Sutskever, I. Improving language understand-
ing by generative pre-training. 2018. URL
https://s3-us-west-2.amazonaws.
com/openaiassets/research-covers/
language-unsupervised/
languageunderstandingpaper.pdf .
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683 , 2019.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
SQuAD: 100,000+ questions for machine comprehen-
sion of text. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Processing ,
pp. 2383–2392, Austin, Texas, November 2016. Asso-
ciation for Computational Linguistics. doi: 10.18653/
v1/D16-1264. URL https://www.aclweb.org/
anthology/D16-1264 .
Rajpurkar, P., Jia, R., and Liang, P. Know what you don’t
know: Unanswerable questions for SQuAD. In Proceed-
ings of the 56th Annual Meeting of the Association for
Computational Linguistics, ACL 2018, Melbourne, Aus-
tralia, July 15-20, 2018, Volume 2: Short Papers , pp.
784–789, 2018.
Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta,
C., and Bengio, Y . Fitnets: Hints for thin deep nets.
In3rd International Conference on Learning Repre-
sentations, ICLR 2015, San Diego, CA, USA, May 7-
9, 2015, Conference Track Proceedings , 2015. URL
http://arxiv.org/abs/1412.6550 .
Sanh, V ., Debut, L., Chaumond, J., and Wolf, T. Distilbert,
a distilled version of BERT: smaller, faster, cheaper and
lighter. CoRR , abs/1910.01108, 2019. URL http://
arxiv.org/abs/1910.01108 .
See, A., Liu, P. J., and Manning, C. D. Get to the point:
Summarization with pointer-generator networks. In Pro-
ceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers) ,
pp. 1073–1083, Vancouver, Canada, July 2017. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https://www.aclweb.org/
anthology/P17-1099 .
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A., and Potts, C. Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 conference on empirical methods
in natural language processing , pp. 1631–1642, 2013.
Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y . Mass:
Masked sequence to sequence pre-training for language
generation. arXiv preprint arXiv:1905.02450 , 2019.
Sun, S., Cheng, Y ., Gan, Z., and Liu, J. Patient knowledge
distillation for BERT model compression. In Proceedings
of the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International Joint
Conference on Natural Language Processing, EMNLP-
IJCNLP 2019, Hong Kong, China, November 3-7, 2019 ,
pp. 4322–4331, 2019a.
Sun, Z., Yu, H., Song, X., Liu, R., Yang, Y ., and Zhou,
D. Mobilebert: Task-agnostic compression of bert by
progressive knowledge transfer, 2019b. URL https:
//openreview.net/pdf?id=SJxjVaNKwB .
Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,
Z. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pp. 2818–2826,
2016. doi: 10.1109/cvpr.2016.308.
Tang, R., Lu, Y ., Liu, L., Mou, L., Vechtomova, O., and Lin,
J. Distilling task-speciﬁc knowledge from BERT into
simple neural networks. CoRR , abs/1903.12136, 2019.
URLhttp://arxiv.org/abs/1903.12136 .
Trinh, T. H. and Le, Q. V . A simple method for common-
sense reasoning. ArXiv , abs/1806.02847, 2018.
Tsai, H., Riesa, J., Johnson, M., Arivazhagan, N., Li, X.,
and Archer, A. Small and practical BERT models for
sequence labeling. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on
Natural Language Processing, EMNLP-IJCNLP 2019,
Hong Kong, China, November 3-7, 2019 , pp. 3630–3634,
2019.
Turc, I., Chang, M., Lee, K., and Toutanova, K. Well-read
students learn better: The impact of student initialization
on knowledge distillation. CoRR , abs/1908.08962, 2019a.
URLhttp://arxiv.org/abs/1908.08962 .
Turc, I., Chang, M., Lee, K., and Toutanova, K. Well-read
students learn better: The impact of student initialization
on knowledge distillation. CoRR , abs/1908.08962, 2019b.
URLhttp://arxiv.org/abs/1908.08962 .

--- PAGE 14 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit,
J., Jones, L., Gomez, A. N., Kaiser, Ł., and
Polosukhin, I. Attention is all you need. In
Advances in Neural Information Processing Sys-
tems 30 , pp. 5998–6008. Curran Associates, Inc.,
2017. URL http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf .
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. In
International Conference on Learning Representations ,
2019. URL https://openreview.net/forum?
id=rJ4km2R5t7 .
Warstadt, A., Singh, A., and Bowman, S. R. Neu-
ral network acceptability judgments. arXiv preprint
arXiv:1805.12471 , 2018.
Williams, A., Nangia, N., and Bowman, S. A broad-
coverage challenge corpus for sentence understanding
through inference. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers) , pp. 1112–
1122, New Orleans, Louisiana, June 2018. Associa-
tion for Computational Linguistics. doi: 10.18653/
v1/N18-1101. URL https://www.aclweb.org/
anthology/N18-1101 .
Wu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M.,
Macherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey,
K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,
L., Gouws, S., Kato, Y ., Kudo, T., Kazawa, H., Stevens,
K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J.,
Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes,
M., and Dean, J. Google’s neural machine translation
system: Bridging the gap between human and machine
translation. CoRR , abs/1609.08144, 2016. URL http:
//arxiv.org/abs/1609.08144 .
Yang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,
R., and Le, Q. V . XLNet: Generalized autoregressive
pretraining for language understanding. In 33rd Confer-
ence on Neural Information Processing Systems (NeurIPS
2019) , 2019.
Zagoruyko, S. and Komodakis, N. Paying more atten-
tion to attention: Improving the performance of con-
volutional neural networks via attention transfer. In
5th International Conference on Learning Representa-
tions, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings , 2017. URL https:
//openreview.net/forum?id=Sks9_ajex .Zhang, S. and Bansal, M. Addressing semantic drift in ques-
tion generation for semi-supervised question answering.
CoRR , abs/1909.06356, 2019.
Zhao, Y ., Ni, X., Ding, Y ., and Ke, Q. Paragraph-
level neural question generation with maxout pointer
and gated self-attention networks. In Proceedings of
the 2018 Conference on Empirical Methods in Natural
Language Processing , pp. 3901–3910, Brussels, Bel-
gium, October-November 2018. Association for Com-
putational Linguistics. URL https://www.aclweb.
org/anthology/D18-1424 .
Zhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-
sun, R., Torralba, A., and Fidler, S. Aligning books and
movies: Towards story-like visual explanations by watch-
ing movies and reading books. In Proceedings of the
IEEE international conference on computer vision , pp.
19–27, 2015.
A. GLUE Benchmark
The summary of datasets used for the General Language Un-
derstanding Evaluation (GLUE) benchmark8(Wang et al.,
2019) is presented in Table 14.
We present the dataset statistics and metrics of SQuAD
2.09(Rajpurkar et al., 2018) in Table 15.
B. Fine-tuning Hyper-parameters
Extractive Question Answering For SQuAD 2.0, the
maximum sequence length is 384and a sliding window
of size 128if the lengths are longer than 384. For the 12-
layer model distilled from our in-house pre-trained model,
we ﬁne-tune 3epochs using 48as the batch size and 4e-5 as
the peak learning rate. The rest distilled models are trained
using 32as the batch size and 6e-5 as the peak learning rate
for3epochs.
GLUE The maximum sequence length is 128 for the
GLUE benchmark. We set batch size to 32, choose learning
rates from {2e-5, 3e-5, 4e-5, 5e-5} and epochs from { 3,4,
5} for student models distilled from BERT BASE. For student
models distilled from our in-house pre-trained model, the
batch size is chosen from { 32,48}. We ﬁne-tune several
tasks (CoLA, RTE and MRPC) with longer epochs (up to 10
epochs), which brings slight improvements. For the 12-layer
model, the learning rate used for CoLA, RTE and MRPC
tasks is 1.5e-5.
8https://gluebenchmark.com/
9http://stanford-qa.com

--- PAGE 15 ---
MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
Table 14. Summary of the GLUE benchmark.
Corpus #Train #Dev #Test Metrics
Single-Sentence Tasks
CoLA 8.5k 1k 1k Matthews Corr
SST-2 67k 872 1.8k Accuracy
Similarity and Paraphrase Tasks
QQP 364k 40k 391k Accuracy/F1
MRPC 3.7k 408 1.7k Accuracy/F1
STS-B 7k 1.5k 1.4k Pearson/Spearman Corr
Inference Tasks
MNLI 393k 20k 20k Accuracy
RTE 2.5k 276 3k Accuracy
QNLI 105k 5.5k 5.5k Accuracy
WNLI 634 71 146 Accuracy
Table 15. Dataset statistics and metrics of SQuAD 2.0.
#Train #Dev #Test Metrics
130,319 11,873 8,862 Exact Match/F1
C. SQuAD 2.0
Question Generation For the question generation task,
we set batch size to 32, and total length to 512. The maxi-
mum output length is 48. The learning rates are 3e-5 and
8e-5 for the 12-layer and 6-layer models, respectively. They
are both ﬁne-tuned for 25epochs. We also use label smooth-
ing (Szegedy et al., 2016) with rate of 0:1. During decod-
ing, we use beam search with beam size of 5. The length
penalty (Wu et al., 2016) is 1:3.
Abstractive Summarization For the abstractive summa-
rization task, we set batch size to 64, and the rate of label
smoothing to 0:1. For the CNN/DailyMail dataset, the total
length is 768and the maximum output length is 160. The
learning rates are 1e-4 and 1.5e-4 for the 12-layer and 6-
layer models, respectively. They are both ﬁne-tuned for 25
epochs. During decoding, we set beam size to 5, and the
length penalty to 0:7. For the XSum dataset, the total length
is512and the maximum output length is 48. The learning
rates are 1e-4 and 1.5e-4 for the 12-layer and 6-layer models,
respectively. We ﬁne-tune 30epochs for the 12-layer model
and50epochs for the 6-layer model. During decoding, we
use beam search with beam size of 5. The length penalty is
set to 0:9.
Cross-lingual Natural Language Inference The maxi-
mum sequence length is 128 for XNLI. We ﬁne-tune 5
epochs using 128as the batch size, choose learning rates
from {3e-5, 4e-5, 5e-5, 6e-5}.
Cross-lingual Question Answering For MLQA, the
maximum sequence length is 512and a sliding windowof size 128if the lengths are longer than 512. We ﬁne-tune
3epochs using 32as the batch size. The learning rates are
chosen from {3e-5, 4e-5, 5e-5, 6e-5}.
