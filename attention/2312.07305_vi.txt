# 2312.07305.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2312.07305.pdf
# File size: 519403 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SCCA: Shifted Cross Chunk Attention cho việc mở rộng ngữ nghĩa ngữ cảnh dài
Yuxiang Guo
Đại học Beihang
irisg@buaa.edu.cn
Tóm tắt
Attention thưa thớt như một phương pháp hiệu quả có thể giảm đáng kể chi phí tính toán, nhưng attention thưa thớt hiện tại có xu hướng dựa vào window self attention gây cản trở luồng thông tin toàn cục. Đối với vấn đề này, chúng tôi trình bày Shifted Cross Chunk Attention (SCCA), sử dụng chiến lược shifting KV khác nhau để mở rộng trường tương ứng trong mỗi lớp attention. Ngoài ra, chúng tôi kết hợp Dilated Attention(DA) và Dilated Neighborhood Attention(DNA) để trình bày Shifted Dilated Attention(SDA). Cả SCCA và SDA đều có thể tích lũy kết quả attention trong multi head attention để có được trường tương ứng gần đúng trong full attention. Trong bài báo này, chúng tôi tiến hành các thí nghiệm mô hình hóa ngôn ngữ sử dụng các mẫu khác nhau của SCCA và sự kết hợp của SCCA và SDA. Shifted cross chunk attention (SCCA) được đề xuất có thể mở rộng hiệu quả các mô hình ngôn ngữ lớn (LLMs) đến ngữ cảnh dài hơn kết hợp với Positional interpolation(PI) và LoRA so với attention thưa thớt hiện tại. Đáng chú ý, SCCA áp dụng LLaMA2 7B từ ngữ cảnh 4k lên 8k trong một V100 duy nhất. Mẫu attention này có thể cung cấp phương pháp fine-tuning Plug-and-play để mở rộng ngữ cảnh của mô hình trong khi giữ nguyên kiến trúc gốc, và tương thích với hầu hết các kỹ thuật hiện có, như FlashAttention-2.

1 Giới thiệu
Kiến trúc Transformer đang nhanh chóng trở thành một trong những kiến trúc deep learning được áp dụng rộng rãi nhất, và sự xuất hiện của các Mô hình Ngôn ngữ Lớn (LLMs) sử dụng Transformer đã mang lại cải tiến cho nhiều tác vụ. Tuy nhiên, một thách thức đáng kể nằm ở độ phức tạp tính toán bậc hai được giới thiệu bởi vanilla transformer, điều này cản trở việc tăng độ dài đầu vào.

Một số nhà nghiên cứu lựa chọn sử dụng các mẫu attention thưa thớt để giảm độ phức tạp tính toán và tiết kiệm bộ nhớ. Trong khi các sparse transformer như local attention (Qiu et al., 2020) và sliding window context(Beltagy et al., 2020) dựa trên kích thước cửa sổ được đề xuất, các mẫu attention này đối mặt với hạn chế trong luồng thông tin trong cửa sổ hoặc chunk. Các cách tiếp cận khác, như dilated window attention (Beltagy et al., 2020) và sparse Transformer (Child et al., 2019), yêu cầu thay đổi cấu trúc mô hình và thiếu triển khai CUDA-friendly tương ứng. Swin Transformer (Liu et al., 2021) và Dilated Neighborhood Attention (DNA) cung cấp mẫu attention cross-layer trong chunk-based attention, giới thiệu luồng thông tin giữa các chunk hoặc cửa sổ khác nhau. Tuy nhiên, luồng thông tin toàn cục vẫn còn thiếu trong các phương pháp này.

Trong khi các LLMs hiện tại đã cách mạng hóa mô hình hóa ngôn ngữ và thể hiện hiệu suất tác vụ ấn tượng (Dasigi et al., 2021; Cohan et al., 2018; Koˇcisk´y et al., 2018; Shi et al., 2023; Huang et al., 2021; Shaham et al., 2022; Bai et al., 2023), chúng bị hạn chế bởi kích thước cửa sổ ngữ cảnh được định trước. Hiệu suất giảm đáng kể khi token đầu vào vượt quá độ dài ngữ cảnh giới hạn này. Extrapolation ngữ cảnh trực tiếp trong LLMs sử dụng positional embedding, như RoPE, có thể dẫn đến hậu quả thảm khốc. Để giải quyết vấn đề out-of-distribution này, các thuật toán Position Interpolation khác nhau (Chen et al., 2023a; Peng and Quesnelle, 2023; Peng et al., 2023) đã được giới thiệu. Trong khi các phương pháp này hiệu quả trong việc extrapolate độ dài của LLMs sử dụng RoPE, full fine-tuning vẫn được yêu cầu.

Longlora (Chen et al., 2023b) giới thiệu một góc nhìn mới rằng sparse attention có thể được sử dụng trong quá trình fine-tuning để extrapolate độ dài ngữ cảnh của LLMs, dẫn đến tiết kiệm tính toán không tầm thường với hiệu suất tương tự như full fine-tuning. Tuy nhiên, việc thiếu luồng thông tin vẫn tồn tại trong quá trình mở rộng độ dài, nhấn mạnh tầm quan trọng của một mẫu attention hiệu quả thông tin.

Trong bài báo này, chúng tôi đề xuất Shifted Cross Chunk Attention (SCCA), sử dụng các chiến lược shift key-value (KV) khác nhau để cho phép các query trực tiếp attend bên ngoài cùng một cửa sổ. Chúng tôi cung cấp hai chiến lược shifting, SCCA fixed và SCCA flow, để giới thiệu các luồng thông tin khác nhau, với SCCA flow đạt được full attention gần đúng trong trường tương ứng của nó trong quá trình tích lũy các kết quả attention head khác nhau với độ phức tạp tuyến tính. Ngoài ra, chúng tôi kết hợp Dilated Attention (DA) và Dilated Neighborhood Attention (DNA) để trình bày Shifted Dilated Attention (SDA). Cả SCCA và SDA đều có thể tích lũy kết quả attention trong multi-head attention để có được trường tương ứng gần đúng trong full attention. Để đánh giá hiệu quả của mẫu attention trong việc mở rộng độ dài ngữ cảnh của LLMs, chúng tôi tiến hành các thí nghiệm mô hình hóa ngôn ngữ sử dụng các mẫu SCCA khác nhau và sự kết hợp của SCCA và SDA trên PG19 validation split và Proof test split. SCCA được đề xuất có thể mở rộng LLMs đến ngữ cảnh dài hơn một cách hiệu quả hơn, kết hợp với Positional Interpolation (PI) và LoRA, so với S2attention được sử dụng trong Longlora (Chen et al., 2023b). Cả SCCA và SDA đều là các phương pháp fine-tuning plug-and-play có thể mở rộng ngữ cảnh mô hình trong khi giữ nguyên kiến trúc gốc.

2 Công trình liên quan
2.1 Sparse attention
Sparse attention có xu hướng thực hiện phép toán self attention trên một tập con token của một chuỗi để giảm thời gian tính toán và bộ nhớ. Blockwise attention, còn được gọi là local attention (Qiu et al., 2020) chia một chuỗi với N token thành n cửa sổ không chồng lấp với N/n token. Local attention cho phép một query attend đến các token trong cùng một cửa sổ. Dựa trên ngữ cảnh cửa sổ này, các mẫu thưa thớt khác nhau được đề xuất. Sliding window attention (Beltagy et al., 2020) thích ứng sliding window để thực hiện local attention. Dilated sliding window tiếp tục tăng trường receptive trong cách "dilated" sliding window (Beltagy et al., 2020). Điều này tương tự như dilated CNNs (Oord et al., 2016) nơi cửa sổ có khoảng cách với kích thước dilation d. Mẫu cố định của sparse Transformer(Child et al., 2019) bao gồm một local attention và một strided attention. Strided attention cho phép query Q attend đến các token không ở trong cùng một cửa sổ. Swin transformer(Liu et al., 2021) cung cấp shifted window attention để cho phép tính toán self-attention cả đối với các cửa sổ cục bộ không chồng lấp và kết nối cross-window. Tương tự như dilated window attention, LongNet (Ding et al., 2023) và Dilated Neighborhood Attention(Hassani and Shi, 2023) mở rộng kích thước cửa sổ khác nhau và thích ứng các khoảng cách khác nhau với kích thước dilation d.

2.2 Length extrapolation trong LLMs
Length extrapolation nhằm đảm bảo rằng mô hình tiếp tục hoạt động tốt, ngay cả khi số lượng token đầu vào trong quá trình suy luận vượt quá kích thước của cửa sổ ngữ cảnh mà mô hình được huấn luyện (Press et al., 2021). Trong khi một số kỹ thuật như ALiBi (Press et al., 2022) và LeX (Sun et al., 2023) cho phép length extrapolation của Transformers, tức là huấn luyện trên cửa sổ ngữ cảnh ngắn và suy luận trên cửa sổ dài hơn, nhiều LLMs được huấn luyện trước hiện có, bao gồm LLaMA (Touvron et al., 2023), sử dụng positional encodings có tính chất extrapolation yếu (ví dụ, RoPE (Su et al., 2021)). Một câu hỏi tồn tại trong các LLMs này là việc trực tiếp extrapolate độ dài ngữ cảnh trong quá trình suy luận có thể mang lại hiệu suất thảm khốc và việc huấn luyện LLMs với ngữ cảnh dài từ đầu là quá đắt đỏ đối với hầu hết các nhà nghiên cứu. Position Interpolation (Chen et al., 2023a) giới thiệu một sửa đổi trên RoPE và mở rộng độ dài ngữ cảnh của LLaMA lên 32768. Sau đó, một loạt các chiến lược Positional Interpolation (PI) như NTK ((Peng and Quesnelle, 2023)) và YaRN (Peng et al., 2023) đã được giới thiệu. Trong khi các phương pháp này làm cho length extrapolation của LLMs sử dụng RoPE hiệu quả, full fine-tuning vẫn được yêu cầu. Longlora (Chen et al., 2023b) đề xuất một góc nhìn mới rằng sparse attention có thể được sử dụng trong quá trình fine-tuning để extrapolate ngữ cảnh, dẫn đến tiết kiệm tính toán không tầm thường với hiệu suất tương tự như fine-tuning. Khác với huấn luyện trong full-length, một số nhà nghiên cứu lựa chọn thiết kế chiến lược huấn luyện phù hợp để mở rộng độ dài ngữ cảnh trong cửa sổ ngữ cảnh gốc. PoSE (Zhu et al., 2023) quản lý để tách rời train / target length, chỉ yêu cầu kích thước ngữ cảnh gốc cho fine-tuning.

2.3 LongBench
Lĩnh vực NLP từ lâu đã tìm cách trao cho máy móc khả năng hiểu và suy luận trên ngữ cảnh dài (Dasigi et al., 2021). Các tác vụ như tóm tắt (Cohan et al., 2018) và trả lời câu hỏi (Ko ˇcisk´y et al., 2018) dựa trên sách, báo cáo (Huang et al., 2021), và tài liệu (Pang et al., 2022), và tạo mã ở cấp độ repository đòi hỏi khả năng mô hình hóa các chuỗi ngữ cảnh dài trải dài hàng nghìn hoặc thậm chí hàng chục nghìn token về độ dài ( ?). LongBench là benchmark song ngữ, đa tác vụ đầu tiên được thiết kế riêng cho hiểu biết ngữ cảnh dài. LongBench (Bai et al., 2023) bao gồm 6 danh mục tác vụ chính và 21 tác vụ khác nhau, bao quát các kịch bản ứng dụng văn bản dài chính bao gồm multi-document QA, single-document QA, tóm tắt, few-shot learning, hoàn thành mã, và các tác vụ tổng hợp. LongBench chứa 4,750 trường hợp thử nghiệm, với độ dài trung bình 6,711 từ cho các trường hợp tiếng Anh (bao gồm mã).

3 Shifted cross chunk attention
Standard self attention sử dụng softmax để tính toán trọng số attention của Query Q={Q1, Q2, ..., Q h} attending Key K={K1, K2, ..., K h}, sau đó dot Value V={V1, V2, ..., V h} theo phương trình (1), h là số head, QiKi và Qi đại diện cho vector head thứ i trong multi head attention. N đại diện cho số token trong một chuỗi. ki,vi và qi đại diện cho vector token thứ i trong một head.

Attention (Q, K, V ) =softmax (QKT
√
d)V (1)

Chúng tôi đầu tiên chia vector QKV thành m chunk, mỗi chunk chứa w token, trong đó m=N/w. Khác với S2attention, redistricts window bằng cách shift Q,K và V, chúng tôi chỉ shift K và V và giữ nguyên phân vùng cửa sổ để làm cho query Qci attend Kcj trong đó 1<=j <=m. Hình 1 hiển thị hai mẫu khác nhau trong Shifted cross chunk attention (viết tắt là SCCA) trong kịch bản multi head attention. Hình 1(a) đại diện cho mẫu SCCA fixed, trong đó một nửa head chỉ có thể attend trong cửa sổ và nửa head còn lại có thể attend đến cửa sổ khác bằng cách sử dụng SCCA. Hình 1(b) hiển thị mẫu SCCA flow, mỗi cửa sổ có thể attend đến các cửa sổ khác bằng cách shifting KV ở khoảng cách khác nhau trong các head khác nhau. Trong đó g=w/2.

3.1 Fixed shifted cross chunk attention
Ki và Vi trong head thứ i với shifting sẽ được sắp xếp lại thành SKi và SVi như phương trình (2) và phương trình (3)

SKi={kN−g−1, kN−g, ..., k N−1, k0, K1, ..., k N−g}(2)
SVi={vN−g−1, vN−g, ..., v N−1, v0, v1, ..., v N−g}(3)

Sau khi shifting vector KV, chúng ta cần chia chúng thành các chunk khác nhau dựa trên cửa sổ, hình 1 là một ví dụ chứa bốn chunk, và mỗi chunk bao gồm bốn token. Sau đó ma trận KV có thể được mô tả thành Phương trình (4) và Phương trình (5).

K={SK 1, SK 2, ..., SK h/2, Kh/2+1, Kh/2+2, , ..., K h}(4)
V={SV1, SV 2, ..., SV h/2, Vh/2+1, Vh/2+2, ..., V h}(5)

Qci,Kci và Vci tương ứng đại diện cho chunk thứ i trong vector multi head Q K V, và mỗi chunk chứa các token KV được shift và không được shift. Sau khi chia chuỗi dài thành các chunk, SCCA thực hiện phép toán attention trong mỗi chunk theo Phương trình (6).

Attention (Q, K, V ) =
softmax (Qc1KT
c1√
d)Vc1
softmax (Qc2KT
c2√
d)Vc2
...
softmax (Qc3KT
c3√
d)Vc3
(6)

3.2 Flow shifted cross chunk attention
Khác với phần trước chúng ta chỉ shift fix nửa kích thước nhóm, phần này chúng tôi đề xuất một mẫu shift mới trong đó các head khác nhau shift kích thước chunk khác nhau để khám phá trường receptive trong một lớp.

Hình 1(b) hiển thị quá trình nhất định trong SCCA flow trong quá trình shifting tất cả các head ở khoảng cách shift khác nhau. Trong tình huống này, mẫu shift tuân theo số nhóm. Mục tiêu của mẫu này là mô phỏng trường receptive của full attention thông qua cơ chế multi head. Thuật toán ?? hiển thị pseudocode triển khai trong SCCA flow. Shifting vector KV và giữ nguyên query như Phương trình (7) (8) và Phương trình (9) có thể khám phá trường tương ứng trong một lớp attention bằng cách tích lũy kết quả tính toán của nhiều head. Trong đó w đại diện cho kích thước nhóm trong mỗi chunk, và m=N/w, có nghĩa là một chuỗi có thể được chia thành m chunk. t=h/m đại diện cho số head có cùng khoảng cách shift.

Qij = {qjw+1, qiw+1, qiw+1, ..., q (j+1)w} có nghĩa là vector query chunk thứ j trong head i,
Kij = {kjw+1, kiw+1, kiw+1, ..., k (j+1)w} có nghĩa là vector key chunk thứ j trong head i,
Vij={vjw+1, viw+1, viw+1, ..., v (j+1)w} có nghĩa là vector value chunk thứ j trong head i.

Q=
Q11, Q12, Q13, ..., Q 1m
Q21, Q22, Q23, ..., Q 2m
...
Qh1, Qh2, Qh3, ..., Q hm
(7)

K=
K11, K12, ..., K 1m−1, K1m
...
Kt1, Ktc2, ..., K tcm−1, Ktm
Kt+12, Kt+13, ..., K t+1m, Kt+11
...
K2t2, K2t3, ..., K 2tm, K2t1
...
...
Kh−t+1m, Kh−t+11, ..., K h−t+1m−2, Kh−t+1m−1
...
Khm, Kh1, ..., K hm−2, Khm−1
(8)

V=
V11, V12, ..., V 1m−1, V1m
...
Vt1, Vtc2, ..., V tcm−1, Vtm
Vt+12, Vt+13, ..., V t+1m, Vt+11
...
V2t2, V2t3, ..., V 2tm, V2t1
...
...
Vh−t+1m, Vh−t+11, ..., V h−t+1m−2, Vh−t+1m−1
...
Vhm, Vh1, ..., V hm−2, Vhm−1
(9)

Pseudocode của SCCA flow theo phong cách PyTorch.
# B: batch size; H: head number; N: sequence length; D: dimension of each attention head
# index : number of heads conduct same shift pattern
# w: group size; # H: number of attention heads;
# K và V có shape (B, H, N, D)
# Key line 2: mỗi index heads shift KV i*w trên chuỗi độ dài N
for i in range(num group):
kv[:, i*index:(i+1)*index] = qkv[:, i*index:(i+1)*index].roll(w*i, dims=2)
kv=kv.reshape(B,H,N/w,w,D)
Sau khi shifting KV chúng ta cần chia shifted KV thành N/w chunk

--- TRANG 4 ---
(a) Dilated distance=2
 (b) Dilated distance=4
Hình 2: Minh họa hai mẫu khác nhau trong DAT, DAT thực hiện sliding window attention trong mỗi head. Hình trên hiển thị mẫu dilated distance=2, và các token head liền kề bắt đầu từ 1 và 2 tương ứng, sau đó lặp lại quá trình này h/2 lần. Hình dưới hiển thị mẫu dilated distance=4, và các token head liền kề bắt đầu từ 1, 2, 3, 4 tương ứng, sau đó lặp lại quá trình này h/4 lần trong quá trình multi head attention

Sau khi shifting chúng ta chia thành sequence và thực hiện window attention như Phương trình (6) để giảm chi phí bộ nhớ và thời gian tính toán.

4 LongMixed
Trong phần này chúng tôi đề xuất một kết hợp mới mà các sparse attention khác nhau có thể được kết hợp để cải thiện hiệu suất mô hình trong quá trình fine-tuning để extrapolate độ dài ngữ cảnh trong LLMs.

Lấy cảm hứng từ DAT (Hassani and Shi, 2023) và LongNet (Ding et al., 2023), chúng tôi đề xuất Shifted Dilated Attention (SDA), một sparse global attention. Hình 2 hiển thị hai mẫu của SDA. Tương tự như DAT, chúng tôi chọn các token tính toán trong không gian toàn cục, khác với DAT thực hiện shifted computing trong các lớp attention khác nhau, chúng tôi shifting vị trí bắt đầu trong mỗi head, và phép toán này tương tự như Dilated Attention (DA) trong LongNet. Sự khác biệt với LongNet là DA chọn các token dilated trong một segment chứa một tập con của các token toàn cục, và chúng tôi trực tiếp thực hiện DA trong toàn bộ không gian toàn cục và không chia bất kỳ segment hoặc chunk nào. Hình 2(a) hiển thị mẫu attention SDA nơi dilated distance bằng 2 và 2(b) là mẫu nơi dilated distance bằng 4. Phương pháp này thực hiện sliding dilated token selection trong một chuỗi trong các head khác nhau, và start index bắt đầu từ 1,2,3, ..., θ, trong đó θ là dilated distance.

Các mẫu attention khác nhau có thể được kết hợp trong quá trình fine-tuning để extrapolate độ dài ngữ cảnh. Trong phần này, chúng tôi kết hợp SCCA fixed và SDA thành LongMixed.

5 Thí nghiệm
5.1 Thiết lập
Datasets, chúng tôi sử dụng một tập con của dataset RedPajama (Computer, 2023) cho tác vụ fine-tuning next token prediction, chúng tôi chọn các mẫu huấn luyện có độ dài token lớn hơn 8192 bằng cách sử dụng LLaMA tokenizer. Số lượng tổng mẫu huấn luyện là 21768. Chúng tôi đánh giá perplexity trên PG19 validation split và Proof-pile dataset (Zhangir Azerbayev and Piotrowski, 2022) test split.

Model Chúng tôi chọn mô hình base LLaMA2-7B làm mô hình đánh giá của chúng tôi và so sánh với mẫu attention tương tự nhất S2attention. Cả hai mẫu attention đều thực hiện cùng thiết lập huấn luyện.

Thiết lập mẫu attention Đối với SCCA fixed và SCCA flow, chúng tôi đặt số chunk m= 4 và SCCA fixed right shift một nửa N/m token trong nửa head, SCCA flow shift iw token trong head khác nhau. Đối với LogMixted, 8 head được chọn để thực hiện SDA 2 và 16 head được chọn để thực hiện SDA 4, các head khác thực hiện SCCA fixed.

Thiết lập huấn luyện và đánh giá Chúng tôi sử dụng DeepSpeed (Rasley et al., 2020) trong Stage 3 trong quá trình fine-tuning và thiết lập LoRA(Hu et al., 2022) giống như Longlora (Chen et al., 2023b). Chúng tôi sử dụng Adamw Optimizer và learning rate được đặt là 2e-5, chúng tôi sử dụng constant và linear learning rate với warmup, warmup step là 20. Chúng tôi đặt per-device batch size là 1 trong 32G 8*V100, có nghĩa là global batch size là 8. Chúng tôi fine-tune 1 epoch trong 21768 mẫu huấn luyện trong RedPajama. Chúng tôi đánh giá điểm perplexity ở các kích thước cửa sổ ngữ cảnh đánh giá khác nhau, từ 1024 đến 8192. Để hiệu quả đánh giá, chúng tôi đặt stride của sliding window là 256 và sử dụng kỹ thuật quantization 4-bit.

5.2 Kết quả mô hình hóa ngôn ngữ
Trong Bảng 2, chúng tôi báo cáo perplexity cho các mô hình của chúng tôi và baseline S2attention trên các dataset Proof-pile và PG19. Dưới độ dài ngữ cảnh huấn luyện nhất định, SCCA fixed và LongMixed đạt được perplexity tốt hơn với 1024,2048,4096 thậm chí trong ngữ cảnh 8192 so với S2attention. Điều này cho thấy hiệu quả của mẫu attention hiệu quả của chúng tôi. Trong Bảng 2, đối với các trường hợp độ dài ngữ cảnh huấn luyện và đánh giá giống nhau, perplexity giảm khi kích thước ngữ cảnh tăng. chúng tôi tìm thấy một số suy giảm perplexity trên kích thước ngữ cảnh nhỏ đối với các mô hình được mở rộng. Đây là một hạn chế đã biết của Position Interpolation.

--- TRANG 5 ---
Bảng 1: Perplexity của các mô hình được mở rộng đến kích thước ngữ cảnh 8k thông qua PI và mẫu sparse attention khác nhau trên tập validation PG19
attention 8192
LLaMA 2 1000
S2(Chen et al., 2023b) 9.41
SCCA fixed 9.17
SCCA flow 9.47
LongMixed 8.73

--- TRANG 6 ---
Bảng 2: Perplexity của các mô hình được mở rộng đến kích thước ngữ cảnh 8k thông qua PI và mẫu sparse attention khác nhau trên tập validation PG19 và tập test Proof. Dataset huấn luyện đến từ một tập con của RedPajama. Chúng tôi cho thấy rằng mẫu attention được đề xuất của chúng tôi có hiệu suất tốt hơn trong ngữ cảnh 8k so với S2attention

PG19
attention 1024 2048 4096 8192
S2(Chen et al., 2023b) 11.71 10.73 9.98 9.41
SCCA fixed 11.26 10.33 9.63 9.17
SCCA flow 11.59 10.64 9.94 9.47
LongMixed 10.49 9.65 9.10 8.73

Proof
1024 2048 4096 8192
3.99 3.83 3.15 2.96
3.95 3.43 3.09 2.88
3.99 3.47 3.13 2.91
3.96 3.46 3.12 2.90

Tài liệu tham khảo
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508.

Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a. Extending context window of large language models via positional interpolation.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Longlora: Efficient fine-tuning of long-context large language models.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana. Association for Computational Linguistics.

Together Computer. 2023. Redpajama: An open source recipe to reproduce llama training dataset. https://github.com/togethercomputer/ RedPajama-Data.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. pages 4599–4610.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens.

Ali Hassani and Humphrey Shi. 2023. Dilated neighborhood attention transformer.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419–1436, Online. Association for Computational Linguistics.

Tom´aˇs Koˇcisk´y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G ´abor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9992–10002.

--- TRANG 7 ---
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.

Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336–5358, Seattle, United States. Association for Computational Linguistics.

Bowen Peng and Jeffrey Quesnelle. 2023. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. https://www.reddit.com/r/LocalLLaMA/ comments/14lz7j5/ntkaware scaled rope allows llama models tohave.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models.

Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations.

Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. 2020. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555–2565, Online. Association for Computational Linguistics.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD '20, page 3505–3506, New York, NY, USA. Association for Computing Machinery.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: Standardized CompaRison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007–12021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Shuming Shi, Enbo Zhao, Wei Bi, Deng Cai, Leyang Cui, Xinting Huang, Haiyun Jiang, Duyu Tang, Kaiqiang Song, Longyue Wang, Chenyan Huang, Guoping Huang, Yan Wang, and Piji Li. 2023. Efficdit: An assistant for improving writing efficiency. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 508–515, Toronto, Canada. Association for Computational Linguistics.

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2023. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590–14604, Toronto, Canada. Association for Computational Linguistics.

Edward Ayers Zhangir Azerbayev and Bartosz Piotrowski. 2022. Proof-pile. https://github.com/ zhangir-azerbayev/proof-pile.

Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training.

A Phụ lục Ví dụ
Sẽ tiếp tục.
