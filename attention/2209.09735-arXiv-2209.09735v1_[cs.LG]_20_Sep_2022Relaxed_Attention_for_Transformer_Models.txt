# 2209.09735.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2209.09735.pdf
# File size: 351245 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2209.09735v1  [cs.LG]  20 Sep 2022Relaxed Attention for Transformer Models
Timo Lohrenz Björn Möller Zhengyang Li Tim Fingscheidt
Technische Universität Braunschweig, Germany
{t.lohrenz, bjoern.moeller, zheli, t.fingscheidt}@tu-b raunschweig.de
Abstract
The powerful modeling capabilities of all-attention-base d transformer architec-
tures often cause overﬁtting and—for natural language proc essing tasks—lead to
an implicitly learned internal language model in the autore gressive transformer
decoder complicating the integration of external language models. In this paper,
we explore relaxed attention, a simple and easy-to-impleme nt smoothing of the
attention weights, yielding a two-fold improvement to the g eneral transformer ar-
chitecture: First, relaxed attention provides regulariza tion when applied to the
self-attention layers in the encoder. Second, we show that i t naturally supports the
integration of an external language model as it suppresses t he implicitly learned
internal language model by relaxing the cross attention in t he decoder. We demon-
strate the beneﬁt of relaxed attention across several tasks with clear improvement
in combination with recent benchmark approaches. Speciﬁca lly, we exceed the
former state-of-the-art performance of 26.90% word error r ate on the largest pub-
lic lip-reading LRS3 benchmark with a word error rate of 26.31% , as well as we
achieve a top-performing BLEU score of 37.67 on the IWSLT14 (DE →EN) ma-
chine translation task without external language models an d virtually no additional
model parameters. Code and models will be made publicly avai lable.
1 Introduction
Early encoder-decoder models emerged from machine transla tion, where the encoder compressed
the entire source language sentence into a ﬁxed-length embe dding vector [14]. This is particu-
larly difﬁcult for very long sentences [13], as the ﬁxed-len gth embedding vector is only a limited-
capacity representation. The use of attention, introduced in [6], enabled the computation of variable-
length weight distributions over the input sequence and soo n turned out to be advantageous for
far more applications than just neural machine translation (NMT), e.g., automatic speech recogni-
tion (ASR) [15, 11, 5], language modeling and understanding [19], object detection [8], and image
classiﬁcation [20, 36]. Soon the most prominent attention- based encoder-decoder (AED) model
emerged, namely the transformer [55] model. Without the use of any recurrency, it entirely relies
on self-attention in the encoder to model temporal dependen cies in the input and cross attention in
the decoder to extract relevant timesteps thereof during th e autoregressive decoding process. While
transformers in language modeling tasks are well-suited fo r upscaling the model size and depth
without any saturation when large amounts of data are presen t [19, 7, 28, 22], they are also suscepti-
ble to overﬁt and require strong regularization to learn at a ll [61, 47, 53]. In a study exclusively on
ASR [37], it was shown that regularization by smoothing the a ttention weights in the decoder’s cross
attention, dubbed relaxed attention, improves performanc e when the transformer model is combined
with an external language model but, for reasons yet to be exp lored, does not help without a language
model.
In this work, we take on the idea of relaxed attention to expan d it to the self-attention layers in
the encoder, regularizing already the encoder. Thereby, we increase the method’s versatility as
it becomes applicable to encoder-only transformers, which are common in several non-sequence
Preprint. Under review.

--- PAGE 2 ---
tasks such as image classiﬁcation or pre-trained bidirecti onal encoder representation by transformer
(BERT , [19]) models. Our main contributions are summarized as fol lows:
• We introduce relaxed self-attention in the transformer encoder to improve generalization
and develop fuzzy relaxation as a variant thereof.
• Beyond relaxed self-attention, we extensively investiga te the capability of relaxed cross
attention in the decoder of sequence-to-sequence transformer models and show that t he im-
provement is due to better external language model integrat ion as it suppresses the inﬂuence
of the internal language model.
• We show improvements of the relaxed attention approaches o n a variety of tasks including
automatic speech recognition, lip-reading, machine trans lation, and image classiﬁcation.
On the lip-reading and machine translation task we report a n ew state of the art and top-
performing result, respectively.
The paper is structured as follows: After a summary of relate d work in Section 2, we introduce the
relaxed attention approach in Section 3, followed by the exp erimental evaluation including results
and discussion in Section 4. Section 5 concludes the paper.
2 Related work
Regularization of transformer models In this work, we introduce a regularization method to the
self-attention function [55], which is fundamental to tran sformer models. Several regularization ap-
proaches proposed for such networks in the past are related t o the network output of transformer
models by modifying the loss computation, either through la bel smoothing [42], or by introduc-
ing additional loss terms. This could be a CTC-loss computed on the encoder outputs [29, 12] for
monotonous tasks such as ASR, or a divergence term between ou tput softmax distributions of two
forward passes with different dropout masks [34, 48]. Relat ed to the network input, several—mostly
application-dependent—data augmentation approaches suc h as spectral augmentation for ASR [46],
or cutoff for machine translation [48] have proven to be effe ctive. Another set of regularization meth-
ods, speciﬁc to transformer models, adds a loss term to encou rage attention heads to yield diverse
outputs [33] or is based on the dropout technique [51] and ran domly masks attention heads [63, 54]
or entire en-/decoder block layers (LayerDrop) [21] during training. Relaxed attention in [37] was
used to prevent too narrow attention weight distributions i n the cross attention during training which
only yielded improvements with an external language model i n ASR. We, however, apply this ap-
proach to the self-attention function to reduce over-ﬁttin g already in the encoder and show that
relaxed self-attention also helps when applied during both training and test. In addition we include
a variety of the aforementioned—proven to be effective—reg ularization methods as baselines and
show that relaxed attention is able to further improve perfo rmance yielding complementarity to other
regularization methods.
Very early once attention-based encoder-decoder networks were introduced to ASR, Chorowski et al.
[15] proposed a modiﬁed softmax function to smooth the atten tion weights in the cross attention be-
tween encoder and decoder by replacing the exponential func tion in the standard softmax function
with a sigmoid. Thereby, they compressed the probability-l ike outputs, but didn’t take into account
the input sequence length, despite the authors’ observatio n that longer sentences require less smooth-
ing of the attention weights. Even though this method dubbed smoothed focus was so far only applied
to recurrent neural network (RNN)-based AED models, we incl ude it as a reference method in our
simulations as it is the closest to the relaxed attention app roach.
Internal language model handling For many sequence-to-sequence tasks the integration of lan -
guage models (LMs) to AED models is of dual use: First, LMs lev erage the use of large amounts
of additional text-only data to improve performance. Secon d, LMs can be utilized to adapt acoustic
models to domains which differ from the original acoustic tr aining data domain. Several techniques
exist to combine language models with AED models, such as sha llow fusion [23], deep fusion [23],
and cold fusion [50], whereas shallow fusion still is the mos t common solution due to its simplicity
and ﬂexibility. However, AED models tend to learn an interna l language model in the autoregres-
sive decoder [40], which can either be suppressed by subtrac ting an additional LM trained only on
text transcriptions from the acoustic training data (e.g., density ratio fusion [40]) or—as was shown
2

--- PAGE 3 ---
4×
Attention
Weights
Fully
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
Connected4××Matrix MultiplicationSoftmaxRelaxation (1)Matrix MultiplicationConcatenationFully Connected
1√
d
K
B×T×dQ
B×˜L×dV
B×T×dB×˜L×d/4
B×d/4×TYi4B×T×d/4
Gi4B×˜L×T˜Gi4B×˜L×T4B×˜L×d/4 ZiB×˜L×dB×˜L×dZ
Multi-Head
Attention
Figure 1: Multi-head attention (MHA) as used in encoder and d ecoder blocks of transformer models
withNh=4attention heads. The proposed relaxed attention (red block) is presented in Section 3.
more recently—can be adapted to a new domain [41] requiring a dditional retraining. For the speciﬁc
application of automatic speech recognition, in a small stu dy, the authors of [37] have investigated
relaxed cross attention, whereby performance improvement s were only achieved with external lan-
guage models. In our work, we investigate the hypothesis tha t relaxed cross attention successfully
suppresses the internal language model, which in contrast t o the aforementioned methods does not—
apart from a single hyperparameter—require any additional models ([40]), parameters ([50]), or
adaptation trainings ([41]), but weakens the internal lang uage model during training of the trans-
former thus supporting shallow fusion [23]. In addition, we will introduce relaxed self-attention,
which improves performance in many applications even witho ut the use of an explicit LM.
3 Proposed relaxed attention
Scaled dot-product multi-head attention (MHA, see Figure 1 ) is typically used in two variants in
the encoder-decoder transformer model [55]: First, it is us ed in the encoder as self-attention to
model positional (e.g., temporal) dependencies in the prep rocessed input sequence indexed with
t∈{1,...,T}of length T. Second, it is used in the decoder as cross attention (also often referred to
as encoder-decoder or source target attention), which draw s the decoder’s attention to relevant parts
in the encoded input sequence hT
1∈RT×dfor decoding at output sequence index ℓ∈{1,...,L}
with model dimension d. In case of self-attention, all MHA inputs (key K, valueV, queryQ)
are the same, i.e., KVQ, with query input Q∈R˜L×dof length ˜L=T. For cross attention,
key and value inputs, K∈RT×dandV∈RT×d, respectively, stem from the encoder output hT
1
yieldingKVhT
1, while the query input Qcomes from the previous decoder layer with ˜L1
during inference and ˜LLduring training, where for the latter all Ltokens of the target sequence
are processed in parallel. Details of the entire typical enc oder-decoder transformer architectures are
recapitulated in Appendix A.1. The attention weights Gi(Q,K)∈I˜L×Tfor the scaled dot-product
MHA sum up to one across the query input length ˜Lafter the softmax activation function and thus
can be interpreted as a probabilistic weighting applied to t he value input projection Yi∈RT×d/Nh,
withNhbeing the number of attention heads each indexed with i∈Nh. To prevent overly sharp
attention distributions applied to the encoded input seque nce, i.e., to introduce some stress into
the (training) process, our relaxed attention weights for scaled dot-product attention are deﬁ ned as
3

--- PAGE 4 ---
simple as (see Figure 1, red box)
˜Gi(Q,K) =/bracketleftbigg
(1−γ)Gi(Q,K)+γ1
T/bracketrightbigg
, i∈ Nh, (1)
gradually injecting a uniform distribution (with 1here being an ˜L×Tmatrix of ones) into the stan-
dard attention weights, controlled by a relaxation coefﬁci entγ∈[0,1], which is a constant single
hyperparameter for all respective attention layers. While the authors of [37] only investigated relaxed
cross attention, only for automatic speech recognition and only d uring training, in our work (i) we
propose relaxed cross attention and self-attention, (ii) d uring training and during inference ( matched
inference ), (iii) we investigate their application to automatic spee ch recognition, lip-reading, ma-
chine translation, and image classiﬁcation, and (iv) we int roduce fuzzy relaxation for the image
classiﬁcation task, where we randomly draw the relaxation c oefﬁcient from a normal distribution
γ∼N(x;µγ0, σ2), with the initially set γ0being the mean µ. For this speciﬁc task, the variable
sequence length Tin equation (1) is substituted by a constant number of image patch tokens M2
(see equation (3) in Appendix A.2), thereby omitting a certa in natural variation. Fuzzy relaxation re-
establishes this variation of the relaxation by randomizin gγduring training, while for the matched
inference case, the relaxation coefﬁcient is kept ﬁxed at γµγ0during inference. Details for the
encoder-only transformer used for the image classiﬁcation task are given in Appendix A.2.
4 Experimental validation and discussion
4.1 Application to automatic speech recognition
Task and datasets Automatic speech recognition transforms recorded speech s ignals into a se-
quence of text tokens. We investigate our relaxed attention method on the Librispeech dataset [43]
with theclean andother conditions of the devandtest subsets. We measure system performance
in terms of word error rate WER=1 −N−D−I−S
N, depending on the number of words N, deletions
D, insertions Iand substitutions S. All raw speech signals are sampled at 16 kHz and analyzed wit h
a 25 ms window at a frame shift of 10 ms. As common in ASR, we also use an external language
model trained on the text labels of the 960h training set as well as on the text-only Librispeech
language model training corpus, the latter containing sent ences from a total amount of 14,500 books
from project Gutenberg [43] which are accessible under publ ic-domain. The Librispeech ASR cor-
pus is available under the very permissive Creative Commons BY 4.0 license.
Models and training For training with 100 h and 960 h of training data, we use the st andard
encoder-decoder transformer model from [55] with a small an d a large conﬁguration, comprising
19.3M and 69.8M parameters, respectively. As common for ASR , ﬁlterbank features are extracted
for each time frame tand then preprocessed by a four-layer convolutional neural network, each
using 3×3ﬁlter kernels (cf. preprocessing block in Figure 2, Appendi x A.1). All hyperparameters
were set according to the recipes available in the fairseq basedespresso toolkit1[57] except the
relaxation coefﬁcients γ, which have been tuned on the joint clean andother portions of the dev
set for both, relaxed cross attention and relaxed self-atte ntion. As additional regularization we use
SpecAugment [45], label smoothing [42] and dropout [51] dur ing training. See Appendix A.3.1 for
more details.
Results and discussion For both, the 100 h and 960 h training data cases in Table 1, the resim-
ulated baselines (using training scripts from [57]) yield s imilar results as in [37] using a standard
transformer approach. The smoothed focus method [15] has a h igher WER compared to the base-
line on the small training data case, but yields small improv ements on some clean settings for the
960 h training data case. Compared to smoothed focus, relaxe d self- andcross attention adapt to
the length Tof the input sequence, with the latter yielding solid WER red uction across all devand
test conditions when an LM is used (right-hand side of Table 1), th ereby conﬁrming the results
of Lohrenz et al. [37]. In Appendix A.4, we show that the stron g improvement with LM using
relaxed cross attention is due to improved internal languag e model suppression. Without an LM,
both the resimulated baseline andrelaxed cross attention approaches are outperformed by our new
1ASR training recipes at https://github.com/freewym/espresso
4

--- PAGE 5 ---
Training
dataApproachwithout LM with LM
dev test dev test
clean other clean other clean other clean other
100hBaseline ([37], resimulated) 13.98 28.71 14.82 29.31 10.62 24.19 12.06 25.56
+ smoothed focus [15] 14.60 28.73 15.50 30.78 10.83 24.86 12. 11 26.46
+ relaxed cross attention 13.91 28.70 14.70 30.10 9.33 22.16 10.62 23.04
+ matched inference 14.30 29.03 15.15 30.09 11.04 25.19 12.1 6 26.36
+ relaxed self-attention 13.48 27.87 14.20 28.96 10.22 23.53 11.04 24.55
+ matched inference 13.43 28.00 14.46 29.23 10.01 23.96 11.19 25.32
960hBaseline ([37], resimulated) 3.92 9.00 4.47 9.23 3.73 8.52 4 .40 8.95
+ smoothed focus [15] 4.11 9.42 4.35 9.63 3.70 9.18 4.31 9.33
+ relaxed cross attention 3.95 9.33 4.28 9.45 3.44 7.74 3.58 8.35
+ matched inference 3.96 9.29 4.20 9.40 3.69 8.95 4.21 9.46
+ relaxed self-attention 3.82 8.50 4.05 8.71 3.52 8.03 4.17 8.51
+ matched inference 3.79 9.12 4.09 9.07 3.35 8.28 3.91 8.50
Table 1: Automatic speech recognition results in terms of WER (%) on the Librispeech task using
standard encoder-decoder transformer models. Attention relaxation is applied in training only,
except for "matched inference" (attention relaxation in tr aining and test). We separately use the
100h and960h training datasets and highlight the respective best resul ts for each size in bold font.
relaxed self-attention in all devandtest conditions for both training data cases. Speciﬁcally, the
WER across the test conditions of the 960 h case for relaxed self-attention impr oved by a relative
9% (clean ) and 5% ( other ) compared to the resimulated baseline, yielding complemen tary regular-
ization of our method to the other employed regularization m ethods. Note that in all aforementioned
cases, relaxed attention is best when used only in training . Only in a very speciﬁc case on the dev
set, however, "matched inference", i.e., relaxed self-att ention in training andtest, is slightly ahead
of using it in training only.
4.2 Application to lip-reading
Task and datasets Automatic lip-reading strives to process an image sequence from recordings
of talking faces. We evaluate lip-reading performance in te rms of WER on the test partition of the
Lip Reading Sentences 3 (LRS3) dataset consisting of a total of 1,321 recorded videos of English
utterances sourced from TED talks [3]. To investigate the pe rformance of the relaxed attention
approach on recently successful self-supervised learning approaches, we closely follow the training
setup from [49] and use audio-visual hidden unit BERT (AV-HuBERT ) encoder models pre-trained
on the English subset of the V oxceleb2 dataset [16], contain ing a total amount of 1,326 hours of
unlabeled video recordings. For some experiments we also us e an external language model trained
on the joint text data from LRS3 and the Librispeech language model training corpus. LRS3 is
publicly available under the TED terms of use as well as the Cr eative Commons BY-NC-ND 4.0
license.
Models and training We useAV-HuBERT models2, introduced recently in [49], which receive im-
age and acoustic frames for pre-training by unlabeled train ing data to iteratively learn contextualized
feature representations hT
1. For ﬁne-tuning and inference, only the video input is used a nd prepro-
cessed (cf. preprocessing layer in Figure 2, Appendix A.1) w ith a 3D convolutional layer and a
subsequent ResNet-18 [25, 52] architecture. The models ﬁne-tuned on 30 h of LRS3 tr aining data
use thebase conﬁguration of the downloaded AV-HuBERT encoder and have a total of 160M parame-
ters. Models ﬁne-tuned on 433 h of LRS3 training data (with or without self-training) use the large
AV-HuBERT encoder and comprise 477M parameters in total. As additiona l regularization methods
we use label smoothing [42], LayerDrop [21], as well as dropo ut [51]. For ﬁnal experiments, we use
the self-training [64] method, where an AV-HuBERT model ﬁne-tuned on 433 h of LRS3 training data
2Pre-trained AV-HuBERT models and ﬁne-tuning code downloaded from
https://github.com/facebookresearch/av _hubert
5

--- PAGE 6 ---
Unlabeled
data
(pre-training)Labeled
data
(ﬁne-tuning)Approachwithout LM with LM
dev test dev test
334 h 433 h Afouras et al. [4], 2020 — — — 59.80
— 1,362 h +157 h Afouras et al. [2], 2018 — 59.90 — 58.90
— 433 h + 157 h Xu et al. [60], 2020 — 57.80 — —
— 433 h Ma et al. [38], 2021 — — — 46.90
— 433 h + 157 h Ma et al. [38], 2021 — — — 43.30
— 33,000 h Makino et al. [39], 2019 — 33.60 — —
1,326h30hBaseline (Shi et al. [49]) — 46.10 — —
Baseline ([49], resimulated) 47.36 45.90 47.61 45.33
+ smoothed focus [15] 47.08 45.80 46.69 45.38
+ relaxed cross attention 45.92 44.00 45.11 42.68
+ matched inference 46.55 45.25 46.46 45.39
+ relaxed self-attention 46.90 45.47 46.95 44.64
+ matched inference 46.85 45.04 46.68 44.69
433hBaseline (Shi et al. [49]) — 28.60 — —
Baseline ([49], resimulated) 21.90 29.52 21.61 28.97
+ smoothed focus [15] 21.87 29.25 21.29 28.86
+ relaxed cross attention 22.12 29.49 21.05 28.05
+ matched inference 22.11 29.20 21.55 28.55
+ relaxed self-attention 21.89 28.96 21.25 28.55
+ matched inference 21.86 28.84 21.24 28.48
433h
+
1,326hBaseline (Shi et al. [49]) — 26.90 — —
Baseline ([49], resimulated) 17.71 26.73 17.18 26.50
+ smoothed focus [15] 17.42 26.78 17.22 26.29
+ relaxed cross attention 17.40 26.57 16.92 25.51
+ matched inference 17.71 26.43 17.48 25.95
+ relaxed self-attention 17.54 26.31 17.12 26.17
+ matched inference 17.65 26.40 17.16 26.06
Table 2: Automatic lip-reading results in terms of WER (%) on the LRS3 task using various
sequence-to-sequence topologies (top segment baselines) orAV-HuBERT encoders (lower three seg-
ments) pre-trained on unlabeled English data from V oxceleb 2 and ﬁne-tuned with a joint transformer
decoder on the given amount of ﬁne-tuning training data. We a lso use self-training (bottom segment)
by creating pseudo-labels for the 1,326 h of unlabeled data a nd using these for ﬁne-tuning. Attention
relaxation is applied in training only, except for "matched inference" (attention relaxation in training
andtest). Best results for each of the three ﬁne-tuning setups a re in bold font.
is inferred to generate pseudo-labels for the 1,326 h of unla beled V oxceleb2 data. These were then
used together with the true labels from the LRS3 training dat a to ﬁne-tune the pre-trained AV-HuBERT
model. Relaxed attention was only used during this ﬁnal ﬁne- tuning, and relaxation coefﬁcient γ
of each relaxed attention approach was optimized on the deve lopment set for each corresponding
amount of ﬁne-tuning data. See Appendix A.3.2 for more detai ls.
Results and discussion The upper segment of Table 2 shows various baselines on LRS3,
whereby Makino et al. [39] reached 33.60% WER w/o LM, using 33 ,000 h of YouTube training data,
and Ma et al. [38] achieved 43.40% with LM and 157 h of addition al data from the Lip Reading in
the Wild dataset [17]. By leveraging pre-training of AV-HuBERT models, Shi et al. [49] report state of
the art so far on LRS3 in three cases with 1,326 h unlabeled pre -training data plus 30 h, plus 433 h,
plus 433 h + 1,326 h of ﬁne-tuning data, respectively, the lat ter using self-training to leverage the
pre-training data using pseudo-labels. See also our resimu lated numbers of that approach. Note that
as it is common practice on the LRS3 task to not even report per formance on devcondition, we also
formulate performance claims on the test set. Smoothed focu s [15] helps a bit in 4 out of the 6 total
test conditions. Without a language model—adding virtually no p arameters and only marginally
6

--- PAGE 7 ---
Approachwithout LMwith LM
(training transcripts only)with extended LM
(additional data)
test test test
Vaswani et al. [55] 34.40 — —
Fan et al. [21] 34.50 — —
Wu et al. [58] 35.20 — —
Wu et al. [59] 36.88 — —
Liang et al. [34] 37.25 — —
Shen et al. [48] 37.60 — —
Baseline ([48], resimulated) 37.42 37.42 37.62
+ smoothed focus [15] 37.42 37.52 37.67
+ relaxed cross attention 37.56 37.53 37.85
+ matched inference 37.60 37.64 37.57
+ relaxed self-attention 37.57 37.49 37.74
+ matched inference 37.67 37.67 37.71
Table 3: Neural machine translation results in terms of BLEU scores on the IWSLT14 task (DE
→EN) using encoder-decoder transformer models with cutoff augmentation [48]. Attention
relaxation is applied in training only, except for "matched inference" (attention relaxation in training
andtest). Best results across all approaches are in bold font, second best underlined.
more complexity during training—our relaxed self-attenti on achieves WERs of 45.04% vs. 45.90%
resimulated [49], and 28.84% vs. 29.52% resimulated [49] in the 30 h and 433 h ﬁne-tuning cases,
respectively, with matched inference (relaxation in train ing and test). With self-training (433 h +
1,326 h), relaxed self-attention without matched inferenc e even achieves 26.31% WER compared to
the best lip-reading WER of 26.90% from [49] thus setting a ne w state of the art for LRS3. With
an additional LM, similar to the ASR task in Section 4.1, rela xed cross attention yields consistent
improvement on the test set compared to the resimulated baseline in all three ﬁne-tu ning cases (i.e.,
42.68% vs. 45.33%, 28.05% vs. 28.97%, and 25.51% vs. 26.50%, respectively). We show that this
is also caused by the improved internal language model handl ing for this task in Appendix A.4.
4.3 Application to machine translation
Task and datasets Neural machine translation (NMT) models use neural network s that translate
an input text sequence from a source language to a different t arget language. For our particular exper-
iments on relaxed attention we use data from the well-known I WSLT14 translation challenge [10],
choosing the German-to-English (DE →EN) subtask and report performance in terms of BLEU [44]
scores. For training of an external LM we either use the IWSLT 14 target language transcripts (160k
utterances) or the MuST-C dataset, the latter contains 47% a dditional transcripts (236k utterances)
from TED talks and is available under the Creative Commons BY –NC–ND 4.0 international li-
cense [9].
Models and training For training we use the standard encoder-decoder transform er model
from [55] in the base conﬁguration with 36.7M parameters and apply cutoff augmen tation,
which ﬁrst randomly masks input positions and feature dimen sions of the embedded input tokens
and second uses a divergence loss to minimize the difference in predictions for different input
masks3[48]. The joint dictionary for source and target language co mprises 10k tokens generated
withSentencePiece [31] and preprocessed with an embedding layer (cf. preproce ssing layer in
Figure 2, Appendix A.1). As in the previous tasks, to investi gate relaxed attention with LM, we
trained two transformer LMs of equal size: One LM trained wit h IWSLT14 training transcripts
and an extended LM trained on the MuST-C dataset, respective ly. For both, relaxed cross attention
and relaxed self-attention, the relaxation coefﬁcient γhas been tuned on the development set. See
Appendix A.3.3 for more details.
3Code available from https://github.com/dinghanshen/cutoff
7

--- PAGE 8 ---
Results and discussion In the upper segment of Table 3, we report BLEU scores for rece nt
transformer-based approaches to NMT, whereof we choose the strong approach from Shen et al.
[48] using cutoff augmentation as a baseline and report a som ewhat lower BLEU score of 37.42 in
our resimulation. Smoothed focus here achieves comparable performance to that baseline with small
gains when LMs are used. We observe that a LM trained only with the target language training tran-
scripts of the translation model yields no additional infor mation compared to the internally learned
language model and thus does not improve performance for mos t approaches, even the relaxed cross
attention that has been strong (with LM) in previous tasks. H owever, in case of a strong extended
LM trained with additional data, relaxed cross attention (o nly during training again) yields the best
performance of 37.85 BLEU, as it suppresses the internal LM. The best performance for the com-
mon case without LM is achieved with our relaxed self-attent ion approach applied during training
andtest, slightly outperforming the previous state-of-the-a rt BLEU score without additional training
data (37.60, Shen et al. [48]), with a score of 37.67 , exceeding the resimulated baseline even by 0.25
BLEU. We note, that in [27] (only available as preprint) the author s also chose the model of Shen
et al. [48] as baseline but were able to reproduce the result o f 37.60 BLEU. They report a BLEU
score of 37.78 by simply applying a modiﬁed learning rate sch edule achieving a somewhat smaller
improvement of 0.18 BLEU absolute vs. their baseline. Witho ut claiming a new state of the art, we
note that both, our and their method are top-performing on th e IWSLT14 task.
4.4 Application to image classiﬁcation
Task and datasets Image classiﬁcation is a fundamental task in computer visio n aiming at rec-
ognizing the primary content of images and differs signiﬁca ntly from the previous sequence-to-
sequence tasks as it uses a type of an attention-based encode r-only (decoder-less) transformer model,
which recently dominate vision benchmarks. To investigate if relaxed attention is also applicable to
such tasks, we evaluate performance in terms of classiﬁcati on accuracy (%) on the computationally
less demanding CIFAR-100 dataset [1]. For each of its 100 cla sses, it contains 500 and 100 images
for training and test, respectively, and is publicly availa ble without a speciﬁed license. As initializa-
tion, we use a model pre-trained on the ImageNet-1k dataset [ 18], which contains 1.28M training
images from 1,000 classes and is also available for research purposes upon agreement of the terms
of access.
Models and training For our experiments we use the vanilla Swin-T transformer model [36]
as baseline—a recently established vision transformer com prising 29M parameters using localized
attention. Details on the architecture (including ﬁgures) are given in Appendix A.2. For training
settings we follow [36]. For some experiments we downloaded the ofﬁcial ImageNet-1k pre-trained
model4and report results after ﬁne-tuning for 100 epochs on CIFAR- 100 training data. With or
without pre-training, relaxed self-attention is applied o nly during ﬁne-tuning. We investigate the
interaction of our relaxed self-attention approach with ot her regularization methods by omitting
already employed (i.e., the well-known stochastic depth me thod [26]) or adding recently proposed
(i.e., the dense relative localization loss Ldrloc [35]) approaches. For fair comparison and following
common practice as in [32, 35, 56], we report results of our re laxed self-attention approaches after
roughly optimizing test accuracy with a small grid search over γvalues (and σ2for fuzzy relaxation
after the optimal γ0was found) separately for each batch size (1024 and 128) with pre-training,
applying the found values to experiments without pre-train ing. See Appendix A.3.4 for more details.
Results and discussion The ﬁrst segment of Table 4 shows results for reference visio n transformer
models ranging from 87.10% accuracy for the pure attention- basedViT-S-16 [20] to 91.70% accu-
racy for the convolution attention hybrid model CMT-S [24]. The second table segment presents
baselines and experimental results for Swin-T transformer models where we chose the vanilla ar-
chitecture [36] to resimulate a baseline for our experiment s. Omitting stochastic depth [26] causes
a severe loss of performance with pre-training but clearly h elps when training from scratch. For
the dense relative localization loss Ldrloc [35], we conﬁrm performance gains with and especially
without pre-training. Smoothed focus helps for the small ba tch size using pre-training and performs
remarkably good for a large batch size when training from scr atch. Without pre-training we observe
4ImageNet-1k pre-trained Swin transformer models and ﬁne-tuning code downloaded from
https://github.com/microsoft/Swin-Transformer .
8

--- PAGE 9 ---
Approachw/ pre-training w/o pre-training
batch size batch size
1024 128 1024 128
test test test test
Other
transformersDosovitskiy et al. [20], ViT-S-16 87.10 — — —
Yuan et al. [62], T2T-ViT-14 88.40 — — —
Guo et al. [24], CMT-S 91.70 — — —
Swin-T
transformersLiu et al. [36], Swin-T (vanilla) 88.22 — 53.28 —
Liu et al. [35], Swin-T (+Ldrloc) 88.40 — 66.23 —
Baseline ([36], resimulated) 88.53 89.16 53.12 64.10
- stochastic depth [26] 87.62 88.42 57.62 68.08
+Ldrloc [35] 88.63 89.27 61.72 66.29
+ smoothed focus [15] 88.44 89.53 57.02 64.15
+ relaxed self-attention 88.64 89.21 52.89 63.72
+ matched inference 88.73 89.39 53.15 63.52
- stochastic depth [26] 87.49 88.42 56.99 67.91
+Ldrloc [35] 88.55 89.29 61.37 65.90
+ fuzzy relaxation 88.63 89.60 52.51 63.58
Table 4: Image classiﬁcation results in terms of accuracy (%) on the CIFAR-100 task using encoder-
only transformer models. Relaxed self-attention is applied in training only , except for "matched
inference" (relaxation in training andtest). All reference methods have roughly the same model siz e
and complexity. Best results across all Swin-T approaches are in bold font, second best underlined.
that relaxed self-attention doesn’t help. This might be due to the limited number of training epochs
and a slower convergence caused by the additional relaxed se lf-attention regularization, similar to
the effect of stochastic depth in the resimulated baseline. When applying relaxed attention after pre-
training, however, relaxed self-attention alone slightly outperforms the baseline but achieves even
higher accuracies when used with matched inference ( 88.73 % vs. 88.53%) and (89.39% vs. 89.16%)
for the large and small batch sizes, respectively. Matched i nference turned out to be advantageous on
this task in most cases, thus we continue to report based ther eon. Also, we note that the combination
with stochastic depth seems to be beneﬁcial for relaxed self -attention. Our new fuzzy relaxation with
matched inference turns out to be useful only on smaller batc h sizes after pre-training, achieving a
strong accuracy of 89.60 % outperforming the baseline ([36], resimulated) at 89.16% .
5 Conclusions
In this work we broadly explored the idea of relaxed attentio n for transformer architectures, a simple
smoothing method of the attention weights in the attention l ayers. We conﬁrmed the advantage of
relaxed cross attention when combined with strong external language models and introduced relaxed
self-attention, thereby regularizing already in the encod er and increasing the versatility of relaxed
attention to different transformer variants. We show impro vements when applying relaxed attention
to automatic speech recognition, lip-reading, machine tra nslation, and image classiﬁcation. On the
LRS3 lip-reading task in particular we achieve a word error r ate of 26.31% (vs. the former state
of the art of 26.90%) as well as a top-performing BLEU score of 37.67 on the IWSLT14 machine
translation task.
Acknowledgements
The research leading to these results has received funding f rom the Deutsche Forschungsgemein-
schaft (DFG, German Research Foundation) for project numbe r 414091002, as well as from the
Bundesministerium für Wirtschaft und Energie (BMWi) under funding code 01MK20011T.
9

--- PAGE 10 ---
References
[1] A. Krizhevsky. Learning Multiple Layers of Features fro m Tiny Images.
https://www.cs.toronto.edu/~kriz/cifar.html , 2009.
[2] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Ziss erman. Deep Audio-Visual Speech
Recognition. IEEE Transactions on Pattern Analysis and Machine Intellig ence (Early Access) ,
pages 1–11, 2018.
[3] T. Afouras, J. S. Chung, and A. Zisserman. LRS3-TED: A Lar ge-Scale Dataset for Visual
Speech Recognition. arXiv:1809.00496 , October 2018.
[4] T. Afouras, J. S. Chung, and A. Zisserman. ASR is All You Ne ed: Cross-Modal Distillation
for Lip Reading. In Proc. of ICASSP , pages 2143–2147, Barcelona, Spain, May 2020.
[5] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . B engio. End-to-End Attention-
Based Large V ocabulary Speech Recognition. In Proc. of ICASSP , pages 4945–4949, Shanghai,
China, March 2016.
[6] D. Bahdanau, K. Cho, and Y . Bengio. Neural Machine Transl ation by Jointly Learning to
Align and Translate. In Proc. of ICLR , pages 1–18, San Diego, CA, USA, May 2015.
[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dh ariwal, A. Neelakantan, P.
Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child,
A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. S igler, M. Litwin, S. Gray, B.
Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sut skever, and D. Amodei. Language
Models are Few-Shot Learners. In Proc. of NeurIPS , volume 33, pages 1877–1901, virtual,
December 2020.
[8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov , and S. Zagoruyko. End-to-End
Object Detection With Transformers. In Proc. of ECCV , page 213–229, Glasgow, UK, August
2020.
[9] R. Cattoni, M. A. Di Gangi, L. Bentivogli, M. Negri, and M. Turchi. MuST-C: A Multilingual
Corpus for End-to-End Speech Translation. Computer Speech and Language , 66, October
2021.
[10] M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, and M. Federico. Report on the 11th IWSLT
Evaluation Campaign. In Proc. of the IWSLT , pages 2–17, Lake Tahoe, CA, USA, December
2014.
[11] W. Chan, N. Jaitly, Q. Le, and O. Vinyals. Listen, Attend and Spell: A Neural Network for
Large V ocabulary Conversational Speech Recognition. In Proc. of ICASSP , pages 4960–4964,
Shanghai, China, March 2016.
[12] N. Chen, P. Zelasko, J. Villalba, and N. Dehak. Focus on t he Present: A Regularization Method
for the ASR Source-Target Attention Layer. In Proc. of ICASSP , pages 5994–5998, Toronto,
ON, Canada, June 2021.
[13] K. Cho, B. van Merriënboer, D. Bahdanau, and Y . Bengio. O n the Properties of Neural Machine
Translation: Encoder-Decoder Approaches. In Proc. of SSST , pages 103–111, Doha, Qatar,
October 2014.
[14] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y .
Bengio. Learning Phrase Representations Using RNN Encoder -Decoder for Statistical Ma-
chine Translation. In Proc. of EMNLP , pages 1724–1734, Doha, Qatar, October 2014.
[15] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Ben gio. Attention-Based Models for
Speech Recognition. In Proc. of NIPS , pages 577–585, Montréal, Canada, December 2015.
[16] J. S. Chung, A. Nagrani, and A. Zisserman. V oxCeleb2: De ep Speaker Recognition. In Proc.
of Interspeech , pages 1086–1090, Hyderabad, India, September 2018.
10

--- PAGE 11 ---
[17] J. S. Chung and A. Zisserman. Lip Reading in the Wild. In Proc. of ACCV , pages 87–103,
Taipei, Taiwan, November 2016.
[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fe i. ImageNet: A Large-Scale
Hierarchical Image Database. In Proc. of CVPR , pages 248–255, Miami, FL, USA, June 2009.
[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: P re-Training of Deep Bidirec-
tional Transformers for Language Understanding. In Proc. of NAACL-HLT , pages 4171–4186,
Minneapolis, MN, USA, June 2019.
[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenbor n, X. Zhai, T. Unterthiner, M. De-
hghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An Image is Worth
16x16 Words: Transformers for Image Recognition at Scale. I nProc. of ICLR , pages 1–21,
virtual, May 2021.
[21] A. Fan, E. Grave, and A. Joulin. Reducing Transformer De pth on Demand With Structured
Dropout. In Proc. of ICLR , pages 1–16, virtual, April 2020.
[22] W. Fedus, B. Zoph, and N. Shazeer. Switch Transformers: Scaling to Trillion Parameter Mod-
els With Simple and Efﬁcient Sparsity. Journal of Machine Learning Research , 23:2–40, 2022.
[23] Ç. Gülçehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin, F. Bougares, H. Schwenk, and Y .
Bengio. On Using Monolingual Corpora in Neural Machine Tran slation. arXiv:1503.03535 ,
March 2015.
[24] J. Guo, K. Han, H. Wu, C. Xu, Y . Tang, C. Xu, and Y . Wang. CMT : Convolutional Neural
Networks Meet Vision Transformers. arXiv:2107.06263 , July 2021.
[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learnin g for Image Recognition. In Proc.
of CVPR , pages 770–778, Las Vegas, NV , USA, June 2016.
[26] G. Huang, Y . Sun, Z. Liu, D. Sedra, and K. Q. Weinberger. D eep Networks With Stochastic
Depth. In Proc. of ECCV , pages 646–661, Amsterdam, Netherlands, October 2016.
[27] N. Iyer, V . Thejas, N. Kwatra, R. Ramjee, and M. Sivathan u. Wide-minima Density Hypothesis
and the Explore-Exploit Learning Rate Schedule. arXiv:2003.03977 , June 2021.
[28] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Ch ess, R. Child, S. Gray, A. Radford,
J. Wu, and D. Amodei. Scaling Laws for Neural Language Models .arXiv:2001.08361 , January
2020.
[29] S. Karita, N. E. Y . Soplin, S. Watanabe, M. Delcroix, A. O gawa, and T. Nakatani. Improving
Transformer-Based End-to-End Speech Recognition With Con nectionist Temporal Classiﬁca-
tion and Language Model Integration. In Proc. of Interspeech , pages 1408–1412, Graz, Austria,
September 2019.
[30] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Opti mization. In Proc. of ICLR ,
pages 1–15, San Diego, CA, USA, May 2015.
[31] T. Kudo and J. Richardson. SentencePiece: A Simple and L anguage Independent Subword
Tokenizer and Detokenizer for Neural Text Processing. arXiv:1808.06226 , August 2018.
[32] J. Kwon, J. Kim, H. Park, and I. K. Choi. Asam: Adaptive Sh arpness-Aware Minimization
for Scale-Invariant Learning of Deep Neural Networks. In Proc. of ICML , pages 5905–5914,
virtual, July 2021.
[33] J. Li, Z. Tu, B. Yang, M. R. Lyu, and T. Zhang. Multi-Head A ttention With Disagreement
Regularization. In Proc. of EMNLP , pages 2897–2903, Brussels, Belgium, October 2018.
[34] X. Liang, L. Wu, J. Li, Y . Wang, Q. Meng, T. Qin, W. Chen, M. Zhang, and T.-Y . Liu. R-
Drop: Regularized Dropout for Neural Networks. In Proc. of NeurIPS , pages 1–21, virtual,
December 2021.
11

--- PAGE 12 ---
[35] Y . Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, and M. D. Na dai. Efﬁcient Training of Visual
Transformers With Small Datasets. In Proc. of NeurIPS , pages 1–13, virtual, December 2021.
[36] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B . Guo. Swin Transformer:
Hierarchical Vision Transformer Using Shifted Windows. In Proc. of ICCV , pages 10012–
10022, virtual, October 2021.
[37] T. Lohrenz, P. Schwarz, Z. Li, and T. Fingscheidt. Relax ed Attention: A Simple Method to
Boost Performance of End-to-End Automatic Speech Recognit ion. In Proc. of ASRU , pages
177–184, Cartagena, Colombia, December 2021.
[38] P. Ma, S. Petridis, and M. Pantic. End-To-End Audio-Vis ual Speech Recognition With Con-
formers. In Proc. of ICASSP , pages 7613–7617, Toronto, ON, Canada, June 2021.
[39] T. Makino, H. Liao, Y . Assael, B. Shillingford, B. Garci a, O. Braga, and O. Siohan. Recurrent
Neural Network Transducer for Audio-Visual Speech Recogni tion. In Proc. of ASRU , pages
905–912, Singapore, Singapore, December 2019.
[40] E. McDermott, H. Sak, and E. Variani. A Density Ratio App roach to Language Model Fusion
in End-To-End Automatic Speech Recognition. In Proc. of ASRU , pages 434–441, Singapore,
Singapore, December 2019.
[41] Z. Meng, Y . Gaur, N. Kanda, J. Li, X. Chen, Y . Wu, and Y . Gon g. Internal Language Model
Adaptation With Text-Only Data for End-to-End Speech Recog nition. arXiv:2110.05354 ,
March 2022.
[42] R. Müller, S. Kornblith, and G. Hinton. When Does Label S moothing Help?
arXiv:1906.02629 , June 2020.
[43] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur. Libri speech: An ASR Corpus Based on
Public Domain Audio Books. In Proc. of ICASSP , pages 5206–5210, South Brisbane, QLD,
Australia, April 2015.
[44] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: A Me thod for Automatic Evaluation
of Machine Translation. In Proc. of ACL , pages 311–318, Philadelphia, PA, USA, July 2002.
[45] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cu buk, and Q. V . Le. SpecAug-
ment: A Simple Data Augmentation Method for Automatic Speec h Recognition. In Proc. of
Interspeech , pages 2613–2617, Graz, Austria, September 2019.
[46] W. Park, D. Kim, Y . Lu, and M. Cho. Relational Knowledge D istillation. In Proc. of CVPR ,
pages 3967–3976, Long Beach, CA, USA, June 2019.
[47] M. Popel and O. Bojar. Training Tips for the Transformer Model. The Prague Bulletin of
Mathematical Linguistics , 110:43–70, 2018.
[48] D. Shen, M. Zheng, Y . Shen, Y . Qu, and W. Chen. A Simple But Tough-to-Beat Data Aug-
mentation Approach for Natural Language Understanding and Generation. arXiv:2009.13818 ,
October 2020.
[49] B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed. Learning Audio-Visual Speech Represen-
tation by Masked Multimodal Cluster Prediction. In Proc. of ICLR , pages 1–24, virtual, April
2022.
[50] A. Sriram, H. Jun, S. Satheesh, and A. Coates. Cold Fusio n: Training Seq2Seq Models To-
gether With Language Models. In Proc. of Interspeech , pages 387–391, Hyderabad, India,
September 2018.
[51] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A Sim-
ple Way to Prevent Neural Networks from Overﬁtting. Journal of Machine Learning Research ,
15:1929–1958, June 2014.
[52] T. Stafylakis and G. Tzimiropoulos. Combining Residua l Networks With LSTMs for Lipread-
ing. In Proc. of Interspeech , pages 3652–3656, Stockholm, Sweden, August 2017.
12

--- PAGE 13 ---
[53] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszk oreit, and L. Beyer. How to Train
Your ViT? Data, Augmentation, and Regularization in Vision Transformers. arXiv:2106.10270 ,
June 2021.
[54] Z. Sun, S. Huang, X. Dai, and J. Chen. Alleviating the Ine quality of Attention Heads for Neural
Machine Translation. arXiv:2009.09672 , September 2020.
[55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jone s, A. N. Gomez, L. Kaiser, and I.
Polosukhin. Attention Is All You Need. arXiv:1706.03762 , December 2017.
[56] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wan g, and X. Tang. Residual
Attention Network for Image Classiﬁcation. In Proc. of CVPR , pages 3156–3164, Honulu, HI,
USA, July 2017.
[57] Y . Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y . Shao, N. Peng, L. Xie, S. Watanabe, and S.
Khudanpur. Espresso: A Fast End-to-End Neural Speech Recog nition Toolkit. In Proc. of
ASRU , pages 136–143, Singapore, Singapore, December 2019.
[58] F. Wu, A. Fan, A. Baevski, Y . Dauphin, and M. Auli. Pay Les s Attention With Lightweight
and Dynamic Convolutions. In Proc. of ICLR , pages 1–16, New Orleans, LA, USA, March
2019.
[59] Z. Wu, L. Wu, Q. Meng, Y . Xia, S. Xie, T. Qin, X. Dai, and T.- Y . Liu. UniDrop: A Simple
Yet Effective Technique to Improve Transformer Without Ext ra Cost. In Proc. of NAACL-HLT ,
pages 3865–3878, virtual, June 2021.
[60] B. Xu, C. Lu, Y . Guo, and J. Wang. Discriminative Multi-M odality Speech Recognition. In
Proc. of CVPR , pages 14421–14430, Los Alamitos, CA, USA, June 2020.
[61] Z. Xu, M. Strake, and T. Fingscheidt. Deep Noise Suppres sion With Non-Intrusive PESQNet
Supervision Enabling the Use of Real Training Data. In Proc. of Interspeech , pages 2806–2810,
Brno, Czech Republic, August 2021.
[62] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. H. T ay, J. Feng, and S. Yan. Tokens-
to-Token ViT: Training Vision Transformers from Scratch on ImageNet. In Proc. of ICCV ,
pages 538–547, virtual, October 2021.
[63] W. Zhou, T. Ge, F. Wei, M. Zhou, and K. Xu. Scheduled DropH ead: A Regularization Method
for Transformer Models. In Proc. of EMNLP , pages 1971–1980, virtual, November 2020.
[64] B. Zoph, G. Ghiasi, T.-Y . Lin, Y . Cui, H. Liu, E. D. Cubuk, and Q. Le. Rethinking Pre-Training
and Self-Training. In Proc. of NeurIPS , pages 3833–3845, virtual, December 2020.
13

--- PAGE 14 ---
A Appendix
x˜T
1Preprocessing+positional
encodingEncoder Block...in totalNe
encoder blocksEncoder Block
input
sequenceB× ··· × ˜T×FB×T×dB×T×dB×T×dhT
1
Embedding
cℓ−1+positional
encodingDecoder Block...in totalNd
decoder blocksDecoder BlockLayer NormFully Connected
+ Softmax
previous
output tokenB×1B×1×dB×1×dB×1×dB×1×D Pℓ output token probs
Encoder DecoderNetwork
D()
Figure 2: Standard encoder-decoder transformer during inference as used for sequence-to-
sequence tasks (i.e., automatic speech recognition, lip-r eading, and machine translation).
A.1 Model speciﬁcs for the sequence-to-sequence transform er
In this section we brieﬂy review the original transformer ar chitecture from [55] consisting of encoder
anddecoder as shown in Figure 2. Please note that here we describ e the transformer architecture ex-
actly as used for the investigated sequence-to-sequence ta sks (i.e., automatic speech recognition, lip-
reading, and machine translation), employing task-depend ent individual preprocessing steps, while
the encoder-only Swin transformer, used for the image classiﬁcation task, is sepa rately described
and shown in Appendix A.2.
A.1.1 Encoder-decoder transformer
The input sequence x˜T
1of length ˜T(and more optional dimensions, e.g., for lip-reading: vide o chan-
nel, height, and width, or for ASR: acoustic feature dimensi onF) is entirely fed to the transformer’s
encoder and auto-regressively transformed (by the decoder modelD()) into an output token se-
quencecL
1= (c1,c2,...,c L)withcℓ∈C={c(1),c(2),...,c(D)}being a single output token (i.e.,
grapheme-based characters or (sub-) word units [31]) at out put sequence index ℓ∈ {1,...,L}
from a vocabulary of size D. Speciﬁcally, the original input sequence x˜T
1is ﬁrst subject to a task-
dependent preprocessing that outputs a feature sequence of t∈{1,...,T}frames, optionally sub-
sampled with T≤˜T. For each decoding step (starting at ℓ= 1), the transformer decoder uses
the entire encoded input sequence hT
1and the previous output token cℓ−1to ﬁnally output a vector
Pℓ=D(hT
1,cℓ−1)comprising probabilities of all Dpossible output tokens. These probabilities
are then subject to a beam search algorithm which, step-by-s tep, invokes the decoder until an end-
of-sentence (EOS) threshold is exceeded and the ﬁnal set of h ypotheses is emitted. Considering
regularization, the standard encoder-decoder transforme r model employs three different variants of
dropout [51]: Residual dropout applied to sub-layer output s before the residual connection is added,
activation dropout applied after the rectiﬁed linear unit ( ReLU) activation, and attention dropout
which is applied to the attention weights inside the MHA func tion (all layers, where dropout might
be applied to the respective outputs, are shown as dashed box es in Figures 1 and 3).
14

--- PAGE 15 ---
Layer NormMulti-Head
Attention+Layer NormFully Connected
+ ReLUFully Connected+
B×T×dQKVB×T×dB×T×dB×T×4dB×T×d
Encoder Block
(a) Single encoder block with self-attention.Layer NormMasked Multi-
Head Attention+Layer NormMulti-Head
Attention+Layer NormFully Connected
+ ReLUFully Connected+
B×1×dQ′K′V′B×1×dQ
B×1×dKVfrom
encoder
B×T×dZB×1×dB×1×dB×1×4dB×1×d
Decoder Block
(b) Single decoder block with cross attention.
Figure 3: Encoder and decoder blocks as used in the transformer model (Figure 2) during inference .
Multi-head attention blocks which may exhibit relaxed attention are colored yellow. Details thereof
are shown in Figure 1. Layers, where dropout [51] might be app lied to the outputs, are depicted as
dashed-line boxes.
A.1.2 Scaled dot-product attention
Besides other variants of the original attention function i ntroduced in [6], in this work we focus
on scaled dot-product multi-head attention (MHA), introdu ced together with the orignal encoder-
decoder transformer model [55]. As shown in Figure 1 (withou t the red block), the standard MHA
employs multiple (i.e., Nh) attention heads
Zi(Q,K,V) =softmax
QW(Q)
i/parenleftig
KW(K)
i/parenrightigT
√
d

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
attention weights
=Gi(Q,K)·VW(V)
i
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
value projections
=Yi(V)∈R˜L×d
Nh (2)
withW(Q)
i,W(K)
i,W(V)
i∈Rd×d
Nhbeing linear projection weight matrices for the query Q, keyK,
and value Vinputs,i∈ Nh={1...Nh}being the index of the in total Nhattention heads, and d
is the feature vector size being used in most layers of the tra nsformer model often referred to as the
model dimension. Considering cross attention, key and valu e inputs stem from the encoder’s last
layer, yielding KVhT
1and the entries in each of the ˜L=Lrows of the attention weight matrix
Gi(Q,K)∈I˜L×T, withI=[0,1], sum up to one and are treated as probabilities that correspo nd to
the relevance of a time frame tto theℓ-th ort-th position in the query input for cross attention or
self-attention, respectively. The outputs Ziof allNhseparate attention heads are concatenated and
subject to a fully connected output layer, yielding the MHA o utputZ∈R˜L×d. Note that for brevity
of notation the attention dropout commonly applied to the at tention weights in transformer models
is not shown in (2).
15

--- PAGE 16 ---
xconv(4×4,C)/(4,4)Swin Transformer
BlockPatch MergingSwin Transformer
BlockPatch MergingSwin Transformer
Block...Patch MergingSwin Transformer
BlockLayer Norm
+ Avg. PoolingFully Connected
+ Softmax
B×H×W×3B×H
4×W
4×CB×H
4×W
4×CB×H
8×W
8×2CB×H
8×W
8×2CB×H
16×W
16×4CB×H
16×W
16×4CB×H
32×W
32×8CB×H
32×W
32×8ChB×8CB×DP
2×2×N×2×
input imageoutput posterior vector
Figure 4: Swin transformer as used for the image classiﬁcation tasks.
A.2 Model speciﬁcs for the vision transformer
As attention-based model for the image classiﬁcation task w e employ the recently successful
encoder-only transformer, dubbed the Swin transformer [36], as shown in Figure 4. The Swin trans-
former is a hierarchical vision transformer, which uses a sh ifting window scheme to compute its
feature representations hand can be used as a general purpose backbone for various visi on tasks. In
contrast to the sequence-based tasks, where a whole decoder is employed to yield sequential output,
here, a single fully connected layer with softmax activatio n (and preceding layer normalization and
adaptive average pooling) is used after the Swin transformer blocks to assign probabilities Pto the
Dclasses for each image inside a batch B.
As input, the Swin transformer receives an RGB input image x∈IH×W×3,I= [0,1]of height H
and width W, which is divided into non-overlapping image patches of siz e4×4by a convolutional
16

--- PAGE 17 ---
Layer Norm(Shifted)
Window-Based
Multi-Head Attention+Layer NormFully Connected
+ GELUFully Connected+
B×(h×w)×cB×(h×w)×cB×(h×w)×4cB×(h×w)×c
Swin Transformer
Block
(a) Single Swin transformer block as used in
theSwin transformer model (cf. Figure 4) dur-
ing training and inference. Instead of dropout
in the encoder-decoder transformer, here we ap-
ply stochastic-depth [26] to randomly drop layers
parallel to the residual connections during train-
ing. (Shifted) window-based multi-head attention
blocks which may exhibit relaxed attention are col-
ored yellow. Details thereof are shown in Fig-
ure 5b.4×
Attention
Weights
Fully1
ConnectedFully
ConnectedFully3
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
ConnectedFully
Connected4×
Reshape
"Window Partitioning"×Matrix Multiplication+SoftmaxRelaxation (3)Matrix MultiplicationConcatenationFully ConnectedReshape
"Window Merge"
B×(w×h)×chw
M2B×M2×c1√
c/4
hw
M2B×M2×c/4hw
M2B×M2×c/4hw
M2B×M2×M2Rpos4hw
M2B×M2×c/4
V K QGi(Q,K)˜Gi4hw
M2B×M2×M24hw
M2B×M2×c/4hw
M2B×M2×chw
M2B×M2×cB×(w×h)×c
(Shifted) Window-based
Multi-Head Attention
(b)(Shifted) window-based multi-head attention
(MHA) as used in the Swin transformer block Fig-
ure 5a with Nh= 4 attention heads. The proposed
relaxed attention (red block) is presented in Sec-
tion A.2.
Figure 5: Details of a Swin transformer block and the (shifted) window-based multi-head atten-
tion (MHA) , where relaxed attention (red block) is applied for the imag e classiﬁcation task.
layer with a stride of (4,4)and is thereby embedded into a feature representation of dim ensionC.
The hierarchical structure of the Swin transformer consists then of four stages each depicted as a
dashed box in Figure 4. In each stage, the patch merging modul es ﬁrst reduce the spatial resolution
and double the feature dimensionality (n·C→2n·C), while dimensions remain constant for
the subsequent processing of that speciﬁc stage through the speciﬁed number of Swin transformer
blocks for that speciﬁc stage.
TheSwin transformer block, shown in Figure 5a, is based on the origin al standard transformer
block ([55], see also Figure 3a), but replaces the ReLU activ ation with a Gaussian error linear
unit (GELU) activation function after the ﬁrst fully connec ted layer and, more importantly, uses
a (shifted) window-based multi-head attention module, sho wn in Figure 5b. There, the window
partitioning limits the self-attention computation to non -overlapping local M×Mwindows ( M=
7), which are shifted in position every other Swin transformer block. Once the features are split into
windows, they are treated as separate batch instances yield ing a temporary batch size ofhw
M2Bwith
Bbeing the original batch size. Different to the standard mul ti-head attention, a relative position bias
Rpos∈RM2×M2is added before softmax activation. The attention weights Gi(Q,K)∈RM2×M2
17

--- PAGE 18 ---
inside the shifted window-based MHA contain probabilities for relevant entries in these windows
and are then subject to the herein investigated relaxation ( see red box in Figure 5b), yielding
˜Gi(Q,K) =/bracketleftbigg
(1−γ)Gi(Q,K)+γ1
M2/bracketrightbigg
, i∈ Nh, (3)
withM2being the ﬁxed amount of features in a single window. See Sect ion 3 for the sequence-based
relaxed attention approach as well as for the fuzzy relaxati on which randomly varies the relaxation
coefﬁcient γto compensate for the now constant M2term in (3).
A.3 Experimental details
In this section we will list additional experimental detail s for all of the investigated tasks, thereby
starting with general settings that apply to multiple tasks and then providing details for the experi-
ments of each individual task that has been investigated in t his work. Please note that for all tasks we
used publicly available code as baselines and did not change any hyper-parameters unless explicitly
mentioned (e.g., for ablation studies).
In experiments where an additional language model was inclu ded, we used the common shallow
fusion method [23] for language model fusion. Speciﬁcally, shallow fusion combines the output
token probability vector Pℓat the output of the transformer model (cf. Figure 2) for each decoding
timestepℓwith the same D-length output token probabilities P(LM)
ℓin the logarithmic domain to
gather a joint output token probability log˜Pℓ= logPℓ+λlogP(LM)
ℓ. The language model weight
is used to steer the inﬂuence of the language model during dec oding and is gathered individually for
each task.
In all investigated tasks, the smoothed focus method from [1 5] is applied as a reference method
that smoothes the attention weights in the cross attention l ayer by modyiﬁng the softmax function.
Deﬁning the scaled dot-product of query and key projections , which is input to the softmax function
in (2), as Ei=1√
dQW(Q)
i/parenleftig
KW(K)
i/parenrightigT
= (ei,ℓ,t)∈RL×Twithei,ℓ,tbeing elements thereof, the
single elements gi,ℓ,tof the attention weights Gi(Q,K)with smoothed focus are computed as
gi,ℓ,t(Q,K) =σsig(ei,ℓ,t)/summationtextT
t1σsig(ei,ℓ,t), (4)
withσsigbeing the sigmoid function, which for smoothed focus replac es the unbounded exponential
function from the standard softmax function. Please note th at for the Swin transformer the softmax
input can be deﬁned analogously as Ei=Rpos+1√
c/4QW(Q)
i/parenleftig
KW(K)
i/parenrightigT
∈RM2×M2.
A.3.1 Automatic speech recognition
The speciﬁc model architecture for the trainings with 100 h a nd 960 h of training data, are stan-
dard encoder-decoder transformer models with a small ( NeNd6,Nh4,d512) and a large
(Ne12,Nd6,Nh4,d512) conﬁguration, respectively, with Ne,Nd, andNhbeing the number
of encoder blocks, decoder blocks, and attention heads, res pectively, and dis the model dimension
(i.e., the amount of nodes used in most layers of the model). T he external RNN language model con-
sists of shared input and output embedding layers with four L STM layer in between each comprising
800 nodes yielding a total of 24.5M parameters.
Training of ASR models was done using the espresso toolkit, which is an extension of the PyTorch -
basedfairseq toolkit. We performed a small grid search on the joint dev clean anddev other
datasets among values {0.0001,0.001,0.01,0.05,0.1}and{0.1,0.15,0.2,0.25,0.3}for the relax-
ation coefﬁcient γand found γ= 0.01andγ= 0.25to be optimal for relaxed self-attention and
relaxed cross attention, respectively. Optimal values wer e used for both, 100h and960h training
data conﬁgurations. All remaining training hyper-paramet ers were adopted from the recipes avail-
able athttps://github.com/freewym/espresso with commit id 390ad6f . Speciﬁcally, we train
all models for 100 epochs using the Adam optimizer [30] with a learning rate of 0.001. All dropout
layers (i.e., residual, activation, and attention dropout ) used the dropout rate p0.2and the label
18

--- PAGE 19 ---
smoothing coefﬁcient was set to α= 0.1. Models for 100 h of training data were trained using a
singleRTX2080ti GPU, while larger models on 960 h of training data were traine d on a single A100
GPU.
Inference was done using a beam search with beam size of 60 and the language model weight λwas
ﬁxed at0.4, following recipes from [57] for all experiments with LM, wi thout further optimization.
A.3.2 Lip-reading
The speciﬁc model architecture for ﬁne-tuning with 30 h of la beled data, is a pre-trained
base AV-HuBERT encoder model with an appended standard transformer decode r model
(Ne12,Nd6,Nh12,d768) while for the 433 h and 433 h + 1,326 h setups a largeAV-HuBERT
encoder with a larger decoder was used ( Ne24,Nd9,Nh16,d1024 ). The external transformer
language model comprises 16 decoder blocks with d= 512 (cf. Figure 3b, but without the cross at-
tention layer) and uses a shared input/output embedding of t he in total D= 1000 subword units,
resulting in a language model size of 51M parameters.
Training of lip-reading models was done using the PyTorch -basedfairseq toolkit. We performed
a small grid search on the development dataset among values {0.005,0.01,0.02,0.05,0.1}and
{0.1,0.15,0.2,0.25,0.3}for the relaxation coefﬁcient of relaxed self-attention an d relaxed cross
attention, respectively. For the 30 h ﬁne-tuning case we fou ndγ=0.001andγ=0.25, for 433 h
we found γ=0.005andγ=0.25, and for the 433 h+1,326 h case we found γ=0.005andγ=0.2
to be optimal. All remaining training hyper-parameters wer e adopted from the recipes available at
https://github.com/facebookresearch/av _hubert with commit id cd1fd24 . Residual, acti-
vation, and attention dropout layers were using a dropout ra tepof0.1,0.1, and0.0, respectively.
The label smoothing coefﬁcient was set to α= 0.1. Models for the smaller 30 h ﬁne-tuning data
setup were trained using a single RTX3080 GPU, while for all other settings a single A100 GPU was
used for training.
Inference was done using a beam search with beam size of 50 and the language model weight λwas
optimzied for each approach by searching optimal values on t he development data among values of
{0.05, 0.1, 0.15, 0.2}.
A.3.3 Machine translation
The standard encoder-decoder transformer from [55] was use d in the base conﬁguration
(NeNd6,Nh4,d512). The external transformer language model consists of shar ed input
and output embedding layers of the in total D= 10000 subword units with 6 decoder blocks (cf.
Figure 3b, but without the cross attention layer) in between and comprises 24.1M parameters.
Training of the machine translation transformer models was done using the PyTorch -based
fairseq toolkit. We performed a small grid search on the development dataset among val-
ues{0.005,0.01,0.02,0.05,0.1}and{0.1,0.15,0.2,0.25,0.3}for the relaxation coefﬁcient and
foundγ= 0.05andγ= 0.1optimal for relaxed self-attention and relaxed cross atten tion, re-
spectively. For approaches with LM the language model weigh tλwas tuned among values
{0,0.05,0.1,0.15,0.2}. All remaining training hyper-parameters were adopted fro m the recipes
available at https://github.com/dinghanshen/Cutoff with commit id 4978563 . Residual, ac-
tivation, and attention dropout layers were set to 0.3,0.1, and0.1, respectively. All models were
trained using a single RTX2080ti GPU.
Inference was done using a beam search with beam size of 10 and the language model weight λwas
optimized for each approach by searching optimal values on t he development dataset among values
of {0.05, 0.1, 0.15, 0.2}.
A.3.4 Image classiﬁcation
We chose the Swin transformer as the speciﬁc model architecture for training s with and without pre-
training. It is a multi-purpose backbone for various vision tasks and can be conﬁgured in terms of
size and complexity. Speciﬁcally, we use the tiny conﬁgurat ion of the model dubbed Swin-T , which
is deﬁned by an initial feature embedding dimensionality C= 96 and comprises N= 6Swin trans-
former blocks in the third stage, resulting in a total of Ne= 12Swin transformer blocks. The num-
19

--- PAGE 20 ---
ber of attention heads Nhdoubles with each consecutive stage, yielding an amount of {3,6,12,24}
attention heads for the respective stages. In total, the mod el comprises 29M parameters.
Training of image classiﬁcation models was done using the PyTorch toolkit. We performed a
small grid search among values {0.005,0.01,0.05,0.1,0.15,0.2}and{0.01,0.02,0.03}for the
relaxation coefﬁcient of relaxed self-attention and σ2for fuzzy relaxation, respectively. Follow-
ing common practice on the CIFAR-100 task (see, e.g., [32, 35 , 56]), parameter search was con-
ducted on the test dataset. For the training with pre-training we found γ= 0.1andσ= 0.03
to be optimal. Both found relaxation hyper-parameters were also applied for experiments with-
out pre-training. All remaining training hyper-parameter s were adopted from the recipes available
athttps://github.com/microsoft/Swin-Transformer with commit id 5d2aede . For some
trainings we use an auxiliary dense relative localization l ossLdrloc, which encourages vision trans-
formers to learn spatial information between image patches and thereby boosts convergence, espe-
cially for small datasets [35]. For the Ldrloc loss, we adopted the ofﬁcial Swin -based code from
https://github.com/yhlleo/VTs-Drloc with commit id b69adb6 . Speciﬁcally we train all
models for 100 epochs using the Adam optimizer [30] with a lea rning rate of 0.000125 for a batch
size of 128 and 0.001for a batch size of 1024. Stochastic depth [26], which random ly drops layers
in the transformer block, is a standard method for training t he baseline model and was used with a
drop probability of 0.2. Label smoothing was used with a value of 0.1. AllSwin transformer models
were trained using a single RTX2080ti GPU.
A.4 Internal language model suppression
Approachabsolute WER LM-induced WER reduction
dev test dev test
clean other clean other clean other clean other
Baseline ([37], resimulated), no LM 3.92 9.00 4.47 9.23 — — — —
+ LM (training transcripts only) 3.92 8.90 4.44 9.20 0.00 0.1 0 0.03 0.03
+ LM (additional data, from Tab. 1) 3.73 8.52 4.40 8.95 0.19 0. 48 0.07 0.28
Relaxed cross attention, no LM 3.95 9.33 4.28 9.45 — — — —
+ LM (training transcripts only) 3.91 9.26 4.23 9.30 0.04 0.0 7 0.05 0.15
+ LM (additional data, from Tab. 1) 3.44 7.74 3.58 8.35 0.51 1.59 0.70 1.10
Table 5: Automatic speech recognition results in terms of WER (%) on the Librispeech task using
standard encoder-decoder transformer models. The 960 h training dataset is used, see also Table 1.
As shown in Table 1 for automatic speech recognition, we achi eved superior results with relaxed
cross attention only when the transformer was combined with an external language model that is
trained with large amounts of additional text-only data. Th is ﬁnding is in line with Lohrenz et al.
[37], but [37] does not provide a sound reason for such behavi or. Different to hybrid ASR ap-
proaches, the output token posterior Pℓof a trained transformer model cannot technically be decom-
posed into an acoustic model P(xT
1|cL
1)and language model P(cL
1), since the latter is also implicitly
learned on the training transcripts by the transformer deco der that in addition to the encoder output
autoregressively receives previous output tokens as it is t he case for language models.
Here, we investigate whether the improvement by relaxed cro ss attention might be due to a sup-
pression of the internal language model . To accomplish this, in Table 5, we measure the WER
improvement achieved by using an LM when the transformer was trained with and without relaxed
cross attention, respectively. Both trained transformer m odels are combined with two language
models, one trained only from the text transcripts of the acoustic training data, and one trained with
additional text-only data. Note that both, resimulated bas eline results and results for the LM with
additional data, are taken from Table 1. We observe that for b oth, the baseline and the relaxed cross
attention model, the improvements with the training transcript only LM (rows 2 and 5) vs. the no
LM methods are about equally small. In contrast, when combin ed with the LM model trained on
additional data, the model trained with relaxed cross atten tion yields far more WER reduction as
if this strong LM would be used with the baseline. In any case i t exceeds an absolute reduction of
20

--- PAGE 21 ---
0.5% (nowhere reached with the baseline), and for the (noisy )other condition it is more than 1%
absolute WER reduction if relaxed cross attention is employ ed.
Approachabsolute WERLM-induced
WER reduction
dev test dev test
Baseline ([49], resimulated), no LM 17.71 26.73 — —
+ LM (training transcripts only) 17.83 27.22 -0.12 -0.49
+ LM (additional data, from Tab. 2) 17.18 26.50 0.53 0.23
Relaxed cross attention, no LM 17.40 26.57 — —
+ LM (training transcripts only) 17.48 26.52 -0.08 0.05
+ LM (additional data, from Tab. 2) 16.92 25.51 0.48 1.01
Table 6: Automatic lip-reading results in terms of WER (%) on the LRS3 task using standard
encoder-decoder transformer models with pre-trained AV-HuBERT encoders. For ﬁne-tuning
433 h + 1,326 h of labeled data are used, see also Table 2.
For the automatic lip-reading task we observe similar behav ior in Table 6. Here the integration
of the training transcripts only LM is even harmful for the ba seline model (row 2), while for the
relaxed cross attention approach, WERs remain roughly the s ame compared to the relaxed cross
attention-trained model without LM (row 4 vs. 5). In combina tion with the strong LM, both baseline
and relaxed cross attention models take proﬁt on the devset, while on the test set, relaxed cross
attention yields a more than four-fold WER reduction by LM fu sion (1.01% absolute) compared to
the baseline approach (0.23% absolute).
Overall, we observe that relaxed cross attention does not ye t help when the LM was trained only
with the text transcript data that was already exposed to the ASR transformer model during training
of acoustic data. We conclude, however, that relaxed cross a ttention particularly helps when the
LM has been trained with additional text data and seems to sup press the internal model bias, thus
suppressing the inﬂuence of the internally learned (usuall y poor) language model. Note that the
same behavior is observed in Table 3 for neural machine trans lation.
A.5 Robustness to different initialization seeds
In Table 7, we investigate the inﬂuence of different initial ization seeds on our experiments. While
for the main experiments in Section 4 we experimented on an un changed and non-optimized seed
for random number generation, here—since both of our SOTA co ntributions are based on the novel
self-attention—we analyze the best relaxed self-attentio n schemes of each task w.r.t. statistical sig-
niﬁcance when using 5 different random seeds.
We note that in these experiments, we achieve signiﬁcant imp rovement for all three sequence-based
tasks including those where we claim state-of-the-art and t op performance (i.e., lip-reading and ma-
chine translation). In addition, not shown here, the relaxe d cross attention method yielded even better
performance on all three sequence-based tasks, outperform ing relaxed self-attention, but we do not
formulate performance claims in this particular analysis a s it implies extra computational complex-
ity due to the requirement of a language model as well as addit ional unpaired text training data. For
the image classiﬁcation task, note that we reach a clear impr ovement using the non-optimized stan-
dard seed for initialization of our main experiments (see Ta ble 4). Here, however, with additional
seeds for initialization, we observe the baseline and the fu zzy relaxation approach to differ without
statistical signiﬁcance. We suspect this is due to non-dete rministic operations in the original base-
line code from [36], which might have ﬂawed the tuning proces s for the relaxation coefﬁcients for
fuzzy relaxation. However, as the average accuracy with fuz zy relaxation is still higher (89.45% vs.
89.29%), we feel encouraged to further expand the relaxed se lf-attention approach to attention-based
approaches for computer vision tasks.
21

--- PAGE 22 ---
TaskAutomatic
Speech Recognition
(Librispeech)Automatic
Lip-Reading
(LRS3)Machine
Translation
(IWSLT14)Image
Classiﬁcation
(CIFAR-100)
Setting100 h
training data
w/o LM433 h + 1,326 h
labeled data
w/o LMw/o LM,
matched
inferencew/ pre-training,
batchsize 128
fuzzy relaxation
Metric WER (%) WER (%) BLEU Acc. (%)
Data subset test clean test other test test test
Baseline (resimulated) 14.89±0.17 29.66±0.34 26.92±0.21 37.49±0.10 89 .29±0.12
Relaxed self-attention 14.25±0.2928.63±0.54 26.36±0.2237.66±0.0289.45±0.17
Table 7: Sensitivity to different initialization of the var ious tasks. Training of the models for the
baseline and best relaxed self-attention approach was repeated 5 times. Results are shown in
terms of average and standard deviation values of the respec tive metrics.
TaskAutomatic
Speech Recognition
(Librispeech)Machine
Translation
(IWSLT14)
Settingw/ LM
100 h training dataw/o LM,
Relaxation type (*) cross attentionself-attention
matched inference
Metric WER (%) ↓ BLEU↑
Data subsetdev
cleandev
othertest
cleantest
other test
Baseline (resimulated) 10.62 24.19 12.06 25.56 37.42
- attention dropout 11.02 25.24 11.89 26.88 37.51
+ relaxed (*) attention 9.33 22.16 10.62 23.04 37.67
- attention dropout 9.68 21.38 10.91 23.16 37.47
Table 8: Ablation study on attention dropout [51] for exempl aryautomatic speech recognition and
neural machine translation tasks. Best results across approaches are in bold font and arrows ( ↓↑)
point to the direction of better metric values for each task.
A.6 Ablation study on attention dropout
Depicted as dashed boxes in Figures 1 and 3, the well-known dr opout method [51] is employed to
the standard encoder-decoder transformer in three differe nt variations: Residual dropout, activation
dropout, and—most relevant for our study—attention dropou t, where the latter is either applied to the
attention weights Giafter the softmax layer (baseline) or to the modiﬁed attenti on weights ˜Giafter
the relaxation operation (relaxed attention approaches, s ee (1)). In Table 8, we investigate how these
regularization operations interfere with each other for tw o different tasks that incorporate attention
dropout during training. Therefore, in this ablation, we re moved attention dropout throughout the
encoder and the decoder of the transformer model for both app roaches with and without speciﬁed
types of relaxed attention. Note that the employed models fo r the lip-reading and image recognition
tasks did not use attention dropout (following the respecti ve baseline recipes from [49] and [36], see
experimental details in Appendices A.3.2 and A.3.4) and are thus omitted for this ablation. Speciﬁc
values for attention dropout are given for each task in Appen dix A.3.
We note that relaxed attention nicely combines with attenti on dropout [51] as in the test conditions
of both tasks the combination of relaxed self-/cross attent ion with attention dropout yields the best
results, which are also reported in the main experiments for both speciﬁc tasks in Section 4. Interest-
ingly, attention dropout did even harm the baseline perform ance for machine translation, as omitting
it yields an 0.09 absolute increase in BLEU score, while it im proves the advantageous relaxed self-
attention even further. In summary, we observe that both pro posed relaxed attention approaches
22

--- PAGE 23 ---
seem to go along with other regularization approaches, such as attention dropout [51], providing
complementary regularization to the attention layers.
A.7 Sensitiveness of relaxation coefﬁcient γ
In Figures 6 and 7 we investigate the sensitiveness of the rel axation coefﬁcient γfor the automatic
speech recognition and for the neural machine translation t ask respectively. As introduced in Sec-
tion 3 the constant relaxation coefﬁcient γ∈[0,1]is a single hyperparameter to control the addition
of an uniform distribution to the unmodiﬁed attention weigh ts over the temporal dimension of the
input sequence. For both exemplary tasks we investigate the inﬂuence of γfor relaxing either the
encoder self-attention layers (Figures 6a and 7a) or the dec oder cross attention layers (Figures 6b
and 7b).
0 .0001.001 .01 .05 .1 .2 .32020.52121.522
Relaxation coefﬁcient γWord error rate (%)
(a)Relaxed self-attention, without LM0 .05 .1 .15 .2 .25 .315.51616.51717.5
Relaxation coefﬁcient γWord error rate (%)
(b)Relaxed cross attention, with LM
Figure 6: Sensitiveness of the automatic speech recognition results with respect to the relaxation
coefﬁcient γin terms of combined WER (%) on the joint clean andother portions of the dev
dataset of the Librispeech task.
0 .01 .02 .05 .138.338.438.538.638.7
Relaxation coefﬁcient γBLEU score
(a)Relaxed self-attention, w/o LM, matched in-
fer.0 .05 .1 .2 .2539.039.139.239.339.4
Relaxation coefﬁcient γBLEU score
(b)Relaxed cross attention, with LM
Figure 7: Sensitiveness of the neural machine translation results with respect to the relaxation
coefﬁcient γin terms of BLEU scores on the development dataset of the IWSLT14 task (DE →
EN).
Both, Figures 6 and 7 show the task-speciﬁc performance on th e respective development sets, that
were used for optimization of the γhyperparameter. In all shown cases, we make the following
observations: (i) While relaxed self-attention performs b est with smaller γvalues, relaxed cross at-
tention reaches best performance with somewhat higher valu es, (ii) there is a smooth and substantial
range where relaxed self- and cross attention improves over the resimulated baselines with γ= 0
thus showing that the contribution of our method is insensit ive with respect to the choice of γin
these ranges.
23

--- PAGE 24 ---
A.8 Statement on potential negative societal impacts
Our method itself applies to the general transformer model a nd is—as we have demonstrated—
applicable to a variety of applications. Out of these applic ations, we identify that automatic lip-
reading can be used for malicious purposes such as eavesdrop ping on private civilian conversation
in video surveillance footage. The dataset we use for automa tic lip-reading consists of professionally
recorded speakers that are aware of being recorded, are at a c lose distance, have well illuminated
faces while speaking, and are mostly facing towards the came ra. These conditions are very unlikely
in a malicious surveillance scenario where it is unlikely th at the methods and models we developed
in our work are of large beneﬁt. In addition, we believe that t he positive impact of lip-reading
applications clearly outweighs the possible negative appl ications. Examples of such applications
are (i) improving speech recognition in case audio is corrup ted, (ii) helping in crime investigations,
(iii) enabling people suffering from aphonia to communicat e, (iv) silence dictations, and (v) uttering
silent alarms or passphrases for security.
24
