# 2407.12866.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2407.12866.pdf
# Kích thước tệp: 3070836 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2407.12866v1  [cs.CL]  13 Jul 2024Vượt Ra Ngoài KV Caching: Shared Attention cho LLM Hiệu Quả
Liao Bingli1, Danilo Vasconcellos Vargas1
1Đại học Kyushu, Fukuoka, Nhật Bản
{liao.bingli.734@s, vargas@inf }.kyushu-u.ac.jp
Tóm Tắt
Hiệu quả của các mô hình ngôn ngữ lớn (LLMs) vẫn là một thách thức quan trọng, đặc biệt trong các bối cảnh mà tài nguyên tính toán bị hạn chế. Các cơ chế attention truyền thống trong những mô hình này, mặc dù mạnh mẽ, đòi hỏi tài nguyên tính toán và bộ nhớ đáng kể do sự cần thiết phải tính toán lại và lưu trữ trọng số attention qua các lớp khác nhau. Bài báo này giới thiệu một cơ chế Shared Attention (SA) mới, được thiết kế để tăng cường hiệu quả của LLMs bằng cách chia sẻ trực tiếp các trọng số attention đã tính toán qua nhiều lớp. Không giống như các phương pháp trước đó tập trung vào việc chia sẻ các cache Key-Value (KV) trung gian, cách tiếp cận của chúng tôi sử dụng các xu hướng đẳng hướng của phân phối attention được quan sát trong các LLM tiên tiến sau khi pretraining để giảm cả số phép tính toán và kích thước cache KV cần thiết trong quá trình suy luận. Chúng tôi chứng minh thực nghiệm rằng việc triển khai SA qua các LLM khác nhau dẫn đến sự mất mát độ chính xác tối thiểu trên các benchmark tiêu chuẩn. Các phát hiện của chúng tôi cho thấy SA không chỉ bảo tồn tài nguyên tính toán mà còn duy trì hiệu suất mô hình mạnh mẽ, từ đó tạo điều kiện cho việc triển khai các LLM hiệu quả hơn trong các môi trường hạn chế tài nguyên.
Code: https://github.com/metacarbon/shareAtt
Giới Thiệu
Sự phát triển nhanh chóng của các mô hình ngôn ngữ lớn (LLM) đã mang lại những thách thức đáng kể về mặt hiệu quả tính toán và bộ nhớ trong quá trình suy luận. Các cách tiếp cận truyền thống, như Multi-Query Attention (MQA) (Shazeer 2019) và Grouped-Query Attention (GQA) (Ainslie et al. 2023), đã đạt được tiến bộ trong việc giảm kích thước cache key-value (KV) bằng cách chia sẻ keys và values qua nhiều heads trong một lớp. Gần đây hơn, Cross-Layer Attention (CLA) đã mở rộng khái niệm này bằng cách chia sẻ keys và values qua các lớp liền kề, tiếp tục giảm yêu cầu bộ nhớ mà không ảnh hưởng đáng kể đến hiệu suất mô hình (Brandon et al. 2024). Mặc dù có những tiến bộ này, nhu cầu về các phương pháp hiệu quả hơn tiếp tục tăng lên, đặc biệt khi các mô hình mở rộng quy mô và được triển khai trong các môi trường hạn chế tài nguyên.

Trong bài báo này, chúng tôi giới thiệu một phương pháp mới được gọi là Shared Attention (SA), giảm đáng kể yêu cầu cache KV và tải tính toán trong quá trình suy luận cho LLMs. Không giống như các phương pháp trước đó tập trung vào việc chia sẻ cache KV hoặc trong cùng một lớp hoặc giữa các lớp liền kề, cách tiếp cận của chúng tôi được truyền cảm hứng bởi sự tương tự vốn có của phân phối trọng số attention qua các lớp, và việc chia sẻ những trọng số này trực tiếp có thể tiếp tục giảm nhu cầu tính toán key và value lặp lại. Cách tiếp cận sáng tạo này không chỉ giảm kích thước cache KV mà còn bỏ qua nhu cầu cho thao tác softmax tính toán đắt đỏ, dẫn đến một quá trình suy luận hiệu quả hơn.

Các đóng góp chính của công trình chúng tôi được tóm tắt như sau:
1. Chúng tôi đề xuất một cơ chế Shared Attention mới giảm chi phí tính toán và bộ nhớ bằng cách chia sẻ trực tiếp các trọng số attention đã tính toán sẵn qua nhiều lớp trong LLMs.
2. Chúng tôi xác nhận thực nghiệm hiệu quả của Shared Attention bằng cách triển khai nó qua các benchmark khác nhau và chứng minh rằng nó đạt được độ chính xác tương đương.
3. Phân tích của chúng tôi về tính đẳng hướng attention qua các LLM đã được pretrain cung cấp insights về cách các cơ chế attention ổn định và trở nên đồng nhất hơn qua các lớp khi quá trình huấn luyện tiến triển. Hiểu biết này thông báo cho các phạm vi lớp tối ưu để áp dụng Shared Attention.

Shared Attention
Trong phần này chúng tôi chứng minh động lực, phương pháp Shared Attention (SA), và so sánh với các cơ chế chia sẻ KV hiện có.

Động Lực
Cơ chế self-attention trong các mô hình transformer thường được định nghĩa là softmax (QKT/√d)V, trong đó Q, K, và V đại diện cho các ma trận query, key, và value tương ứng, và d là chiều của các vector key. Công thức này đòi hỏi việc tính toán lại trọng số attention ở mỗi lớp, một nhiệm vụ tính toán chuyên sâu, đặc biệt khi mô hình được triển khai ở chế độ suy luận. Để giảm thiểu điều này, khái niệm KV-cache được sử dụng, giảm nhu cầu tính toán lại các ma trận K và V cho các token đã gặp trước đó.

Trong khi các phương pháp luận trước đó đã tập trung vào việc chia sẻ cache KV ở các cấp độ khác nhau để giảm thiểu chi phí bộ nhớ, chúng chủ yếu hoạt động dưới giả định rằng trọng số attention khác biệt đáng kể qua các lớp, do đó đòi hỏi các tính toán riêng lẻ để nắm bắt hiệu quả các phụ thuộc ngữ cảnh đa dạng. Giả định này thúc đẩy một

--- TRANG 2 ---
Hình 1: Minh họa các thuật toán chia sẻ khác nhau. Các phương pháp MQA và GQA chia sẻ cache Key và Value với Query trong cùng một lớp để giảm sử dụng bộ nhớ. Phương pháp CLA mở rộng điều này bằng cách chia sẻ cache Key và Value qua các lớp khác nhau. Phương pháp của chúng tôi, Shared Attention, tiến xa hơn khái niệm này bằng cách chia sẻ trọng số attention qua nhiều lớp.

câu hỏi quan trọng: Liệu các trọng số attention có thực sự khác biệt rõ rệt qua các lớp, hay sự biến thiên này đủ tối thiểu để cho phép một cách tiếp cận thống nhất qua nhiều lớp?

Để khám phá điều này, chúng tôi đã tiến hành phân tích thực nghiệm về phân phối trọng số attention qua các lớp khác nhau của mô hình. Dựa trên mô hình Llama2-7B-chat, chúng tôi xử lý tập dữ liệu Massive Multitask Language Understanding (MMLU) (Hendrycks et al. 2020) để trích xuất các ma trận attention, softmax (QKT/√d), cho mỗi lớp. Do sự biến thiên trong độ dài chuỗi, chúng tôi chuẩn hóa những ma trận này thành kích thước đồng nhất bằng cách áp dụng zero-padding để căn chỉnh chúng theo hình dạng nhất quán là maxlen × maxlen.

Phân tích của chúng tôi sử dụng độ đo cosine similarity để so sánh các ma trận attention của tất cả các lớp, tiết lộ một mức độ tương tự cao đáng chú ý qua hầu hết các lớp, đặc biệt từ chỉ số 3 đến 30. Ngược lại, các lớp ban đầu (0 và 1) và lớp đầu ra cuối cùng (31) thể hiện điểm tương tự thấp hơn đáng kể so với các lớp giữa. Quan sát này là trực quan vì các lớp đầu gần hơn với các embedding token đầu vào, đòi hỏi điều chỉnh thường xuyên phân phối attention của chúng để trừu tượng hóa chính xác ý nghĩa ngữ nghĩa từ các đầu vào đa dạng. Tương tự, vai trò độc đáo của lớp cuối trong việc dự đoán token tiếp theo biện minh cho mẫu attention riêng biệt của nó.

Được truyền cảm hứng bởi những phát hiện này, chúng tôi đưa ra giả thuyết rằng sự tương tự cao trong trọng số attention qua đa số các lớp có thể cho phép một biểu diễn chia sẻ của những trọng số này, do đó loại bỏ nhu cầu cho các tính toán softmax riêng biệt trong mỗi lớp và giảm kích thước cache key. Một chiến lược như vậy không chỉ có thể hợp lý hóa quá trình suy luận mà còn tăng cường hiệu quả tính toán một cách đáng kể.

Dựa trên tính đồng nhất được quan sát trong trọng số attention, chúng tôi đề xuất một thuật toán mới như được hiển thị trong Thuật toán 1, Shared Attention, sử dụng một ma trận attention chia sẻ duy nhất qua nhiều lớp. Cách tiếp cận này về cơ bản tái định nghĩa paradigm hoạt động bằng cách duy trì một cơ chế attention nhất quán qua các lớp ngữ cảnh khác nhau, từ đó giảm sự dư thừa và tăng cường tốc độ suy luận.

So Sánh với Các Cách Tiếp Cận Hiện Có
Cơ chế self-attention ban đầu trong Transformers, được đặc trưng bởi mô hình Multi-Head Attention (MHA), đòi hỏi việc cache các keys (K) và values (V) trong mỗi head và lớp để tăng tốc suy luận (Vaswani et al. 2017). Yêu cầu này theo lịch sử đã áp đặt một chi phí bộ nhớ đáng kể, thúc đẩy một loạt các đổi mới nhằm giảm gánh nặng này.

Trong số này, Multi-Query Attention (MQA) và đối tác tổng quát hóa hơn của nó, Grouped-Query Attention (GQA), củng cố cache KV bằng cách cho phép nhiều query heads trong cùng một lớp chia sẻ một tập hợp K và V duy nhất. Cách tiếp cận này hiệu quả giảm số lượng cặp key và value độc đáo phải được lưu trữ và truy xuất trong quá trình tính toán. Tiếp theo, Cross-Layer Attention (CLA) mở rộng khái niệm này bằng cách tạo điều kiện cho việc chia sẻ ma trận K và V qua các lớp khác nhau, từ đó cung cấp thêm sự giảm trong dấu chân bộ nhớ cần thiết để lưu trữ KV.

Tuy nhiên, phương pháp của chúng tôi giới thiệu một paradigm về cơ bản khác biệt trong việc giải quyết các thách thức của self-attention.

--- TRANG 3 ---
Hình 2: Sự tương tự theo lớp của trọng số attention qua các LLM khác nhau. Trục x và trục y đại diện cho các chỉ số lớp, trong khi trục z mô tả các giá trị cosine similarity. Các mẫu tương tự riêng biệt cho thấy các vai trò chức năng cụ thể mà mỗi nhóm lớp đóng trong kiến trúc tổng thể.

Thuật toán 1: Thuật toán Shared Attention
Đầu vào: Tập các lớp L, token đầu vào X
Tham số: Phạm vi attention S (ví dụ: lớp 23 đến 30)
Đầu ra: Trọng số attention được cập nhật qua các lớp được chỉ định

1: Khởi tạo trọng số attention A ← ∅
2: foreach lớp l ∈ S do
3:    if lớp đầu tiên trong S then
4:        Tính toán trọng số attention ban đầu Al ← softmax(QlKTl/√dk)
5:        Đặt A ← Al
6:    else
7:        Chia sẻ trọng số attention Al ← A
8:    end if
9:    Áp dụng shared attention để tính toán đầu ra Ol ← Al · Vl
10: end for
11: Điều chỉnh đầu vào lớp tiếp theo sử dụng đầu ra từ S
12: Tiếp tục xử lý các lớp còn lại với attention tiêu chuẩn
13: return Đầu ra cuối cùng sau khi xử lý tất cả các lớp

Trong khi các phương pháp trước đây đã tập trung vào việc giảm sự dư thừa trong việc lưu trữ ma trận K và V, cách tiếp cận của chúng tôi tập trung vào việc tối ưu hóa tính toán của chính các trọng số attention. Trong thực hành tiêu chuẩn, các keys được cache (K) chủ yếu được sử dụng để tính toán trọng số attention kết hợp với các queries (Q). Thay vì gián tiếp tạo điều kiện cho tương tác này thông qua các ma trận KV chia sẻ, phương pháp của chúng tôi đề xuất việc chia sẻ trực tiếp các trọng số attention kết quả—cụ thể, các điểm số được chuẩn hóa softmax.

Điều này không chỉ giảm yêu cầu bộ nhớ bằng cách loại bỏ nhu cầu lưu trữ các tập hợp keys riêng biệt cho mỗi lớp mà còn giảm đáng kể độ phức tạp tính toán. Bằng cách chia sẻ kết quả softmax đã tính toán sẵn qua các lớp, cách tiếp cận của chúng tôi bỏ qua việc tính toán lặp lại softmax, thường là một trong những thao tác tính toán chuyên sâu nhất trong cơ chế attention. Lợi ích hiệu quả này được phản ánh trong việc giảm đáng kể số lượng phép toán dấu phẩy động (FLOPs) cần thiết trong quá trình suy luận mô hình, tăng cường cả tốc độ và khả năng mở rộng của việc triển khai Transformer.

Không giống như các phương pháp truyền thống tối ưu hóa sử dụng bộ nhớ bằng cách chia sẻ keys và values vật lý qua các lớp hoặc heads, mô hình Shared Attention của chúng tôi đổi mới trên chính quá trình tính toán, khai thác các mẫu nhất quán trong trọng số attention để hợp lý hóa các hoạt động qua nhiều lớp của kiến trúc Transformer.

Phân Phối Attention Đẳng Hướng
Trong một phân tích mở rộng về trọng số attention cụ thể theo lớp qua một phổ các LLM, chúng tôi đã khám phá động lực attention trong các mô hình như Llama2-7B-chat, Llama3-8B-instruct, Llama3-70B-instruct, Baichuan2-7B-chat, Qwen2-7B-instruct, và Qwen2-72B-instruct (Touvron et al. 2023; Yang et al. 2023; Bai et al. 2023). Những mô hình này được đánh giá sử dụng MMLU.

Các nghiên cứu của chúng tôi tiết lộ một mẫu tự tổ chức trong trọng số attention qua những mô hình đa dạng này. Như được mô tả trong Hình 2, tồn tại một mẫu tương tự toàn cục nhất quán trong trọng số attention của các lớp qua tất cả các mô hình được thử nghiệm. Mẫu này gợi ý một đặc tính cấu trúc vốn có trong cách LLMs xử lý thông tin, có thể được phân đoạn rộng rãi thành bốn nhóm riêng biệt:

• Nhóm 1: Bao gồm các lớp ban đầu (chỉ số 0 và 1), nhóm này nằm gần nhất với các token đầu vào và chủ yếu tập trung vào việc trừu tượng hóa thông tin ngữ nghĩa cấp token. Những lớp này thể hiện các mẫu attention phụ thuộc dữ liệu quan trọng cho việc xử lý ngữ nghĩa ban đầu của các đầu vào.

• Nhóm 2: Nhóm này bao gồm các lớp ngay sau nhóm đầu tiên và mở rộng đến chỉ số lớp 5. Các lớp trong phân đoạn này thể hiện sự tương tự nội bộ cao trong trọng số attention nhưng khác biệt rõ rệt so với những nhóm khác. Những lớp này có thể phục vụ như các vùng chuyển tiếp nơi các đặc trưng ngữ nghĩa trung gian được tinh chỉnh.

--- TRANG 4 ---
• Nhóm 3: Bao gồm các lớp sau Nhóm 2 và mở rộng đến lớp gần cuối, đây là nhóm lớn nhất cả về số lượng lớp và vai trò của chúng trong kiến trúc. Các lớp trong nhóm này hiển thị mức độ tương tự cao, gợi ý một tính đẳng hướng trong cơ chế attention nơi các đặc trưng được tinh chỉnh được sử dụng nhất quán để thông báo cho hiểu biết ngữ cảnh sâu hơn của mô hình.

• Nhóm 4: Nhóm cuối cùng, chỉ bao gồm lớp đầu ra, xử lý thông tin ngữ cảnh tổng hợp một cách riêng biệt để tạo ra đầu ra. Trọng số attention của lớp này khác biệt so với những gì quan sát được ở các lớp khác, nhấn mạnh vai trò chuyên biệt của nó trong quá trình ra quyết định cuối cùng.

Các mẫu trọng số attention riêng biệt được xác định qua những nhóm này củng cố khái niệm chuyên môn hóa chức năng trong LLMs. Sự phân đoạn này không chỉ làm nổi bật các vai trò đa dạng của các lớp khác nhau trong việc xử lý đầu vào mà còn hỗ trợ tiềm năng tối ưu hóa các chiến lược tính toán, như phương pháp Shared Attention được đề xuất của chúng tôi, bằng cách thao tác những mẫu vốn có này để giảm sự dư thừa tính toán.

Động Lực Trong Quá Trình Pretraining
Để làm rõ sự hình thành và tiến hóa của các mẫu trọng số attention trong giai đoạn pretraining của LLMs, chúng tôi sử dụng các checkpoint trung gian của mô hình Baichuan 7B, được cung cấp bởi các nhà phát triển mô hình. Những checkpoint này, trải dài từ 0.2T đến 2.6T token được xử lý, cung cấp một góc nhìn độc đáo để quan sát những thay đổi động trong cơ chế attention khi mô hình tiếp xúc với lượng dữ liệu ngày càng tăng.

Chúng tôi áp dụng một độ đo nhất quán để đo sự tương tự của trọng số attention qua các lớp tại mỗi checkpoint pretraining. Ngoài ra, mô hình chat cuối cùng, được fine-tune để phù hợp với các phản hồi tham chiếu của con người, được bao gồm để đánh giá sự tiến hóa so với kết quả ứng dụng thực tế. Động lực của những trọng số attention này được minh họa trong Hình 3, thể hiện sự khác biệt và ổn định tiến bộ của các mẫu attention qua các lớp của mô hình.

Như quan sát trong giai đoạn pretraining sớm tại 0.2T token, Nhóm 1 và 2 xuất hiện được hợp nhất, chỉ ra một chiến lược xử lý ít khác biệt hơn qua những lớp ban đầu này. Sự kết hợp này gợi ý rằng sớm trong quá trình huấn luyện, mô hình không tách biệt rõ ràng việc xử lý ngữ nghĩa cấp token khỏi việc tinh chỉnh ngữ nghĩa trung gian. Tuy nhiên, khi mô hình tiến triển đến 1.0T token, một sự phân chia rõ ràng xuất hiện giữa Nhóm 1 và 2. Sự tách biệt này phù hợp với việc mô hình bắt đầu hình thành các chiến lược chuyên biệt và hiệu quả hơn để xử lý các loại thông tin khác nhau qua kiến trúc của nó.

Sự tương tự trong Nhóm 3, bao gồm phần lớn các lớp của mô hình, cho thấy một cải thiện rõ rệt từ điểm tương tự 0.8 đến 0.9. Sự tăng này cho thấy cơ chế attention của mô hình đang ổn định và trở nên nhất quán hơn trong cách tiếp cận xử lý phần lớn thông tin ngữ cảnh.

Những tiến bộ huấn luyện quan sát được qua các checkpoint pretraining không chỉ chứng minh những thay đổi đáng kể trong cấu trúc nội bộ của cơ chế attention của mô hình mà còn tương quan tích cực với cải thiện hiệu suất trên nhiều benchmark. Điều này bao gồm kết quả trên các bài kiểm tra độ chính xác 5-shot MMLU, CMMLU (Li et al. 2023), và C-Eval (Huang et al. 2024), được báo cáo đã cải thiện từ độ chính xác cơ sở 0.25 đến 0.50 (Yang et al. 2023). Sự tăng cường đáng chú ý này nhấn mạnh mối liên kết nội tại giữa việc tinh chỉnh cơ chế attention trong LLMs và khả năng nâng cao của chúng trong các nhiệm vụ hiểu ngôn ngữ tự nhiên.

Hơn nữa, việc kiểm tra thêm sự phát triển của mô hình, như quan sát trong tài liệu bổ sung, tiết lộ rằng sự tương tự trong Nhóm 3—bao gồm các lớp xử lý ngữ cảnh cốt lõi của mô hình—tiếp tục tăng cường sau giai đoạn alignment. Quan sát này gợi ý rằng quá trình alignment, thường nhằm fine-tune mô hình để phản ánh chặt chẽ hơn hiểu biết và tạo phản hồi giống con người, cũng góp phần vào sự ổn định của cơ chế attention của mô hình.

Thí Nghiệm và Thảo Luận
Để xác nhận hiệu quả của phương pháp Shared Attention (SA) được đề xuất, chúng tôi đã tiến hành một loạt thí nghiệm. Những thí nghiệm này được thiết kế để kiểm tra độ mạnh mẽ của SA dưới các cấu hình khác nhau và để đánh giá hiệu suất của nó trên các benchmark được công nhận rộng rãi.

Ban đầu, chúng tôi áp dụng cơ chế SA trực tiếp vào các LLM tiên tiến mà không có bất kỳ huấn luyện trước nào để đánh giá tác động của nó đối với các mô hình đã được pretrain. Thí nghiệm này nhằm hiểu tác động ngay lập tức của SA khi được tích hợp vào các kiến trúc mô hình hiện có. Chúng tôi đánh giá hiệu suất của những mô hình này trên các benchmark LLM tiêu chuẩn, bao gồm GLUE (General), GSM8k (Arithmetic), HellaSwag (Reasoning), và MMLU (Knowledge) (Wang et al. 2018; Cobbe et al. 2021; Zellers et al. 2019). Như dự đoán, việc áp dụng trực tiếp SA dẫn đến mất mát độ chính xác trên một số benchmark. Kết quả này phù hợp với kỳ vọng của chúng tôi do thiếu việc huấn luyện lại để thích ứng các mô hình hoàn toàn với các sắc thái của cơ chế Shared Attention. Do hạn chế tính toán, việc pretrain một LLM từ đầu kết hợp SA là không thực tế cho nhóm chúng tôi.

Để tiếp tục thăm dò khả năng của SA dưới một chế độ huấn luyện, chúng tôi fine-tune các LLM cơ sở được trang bị Shared Attention trên tập dữ liệu Instruct có sẵn công khai (Taori et al. 2023). Sau fine-tuning, những mô hình này được kiểm tra so với cùng các benchmark để tìm ra bất kỳ thay đổi hiệu suất nào. Cách tiếp cận này cho phép chúng tôi đo khả năng thích ứng của SA khi các mô hình được huấn luyện để thích ứng với động lực của nó.

Những thí nghiệm này chung minh tập thể tiềm năng của Shared Attention để điều chỉnh cơ chế attention truyền thống trong LLMs, cho thấy một hướng đi đầy hứa hẹn để giảm nhu cầu tính toán trong khi duy trì, và trong một số trường hợp tăng cường, hiệu suất mô hình. Kết quả chi tiết và thảo luận thêm về mỗi benchmark và tập dữ liệu được cung cấp trong các phần tiếp theo.

--- TRANG 5 ---
Hình 3: Sự tiến hóa của sự tương tự trọng số attention lớp trong suốt giai đoạn pretraining của mô hình Baichuan2 7B, khi nó xử lý các token được huấn luyện từ 220 tỷ đến 2.6 nghìn tỷ. Gradient màu trong visualization đại diện cho cosine similarity, minh họa hiệu quả sự chuyển đổi trong các mẫu attention từ giai đoạn ban đầu đến giai đoạn tiên tiến của pretraining.

Thiết Lập Thí Nghiệm
Cho các thí nghiệm fine-tuning, chúng tôi sử dụng các mô hình cơ sở Llama2-7B và Llama3-8B. Những thí nghiệm này được tiến hành trên một cấu hình phần cứng mạnh mẽ bao gồm hai GPU NVIDIA A100 80GB. Tối ưu hóa các mô hình được thực hiện sử dụng trình tối ưu AdamW, với tốc độ học ban đầu được đặt ở 2×10⁻⁵. Chúng tôi sử dụng kiểu dữ liệu bf16 cho các tham số mô hình, giúp tăng cường phạm vi số và độ ổn định trong quá trình backpropagation, quan trọng để duy trì độ chính xác trong việc huấn luyện mô hình lớn.

Mỗi GPU xử lý kích thước micro-batch là 16, tận dụng các kỹ thuật tích lũy gradient để quản lý hiệu quả tải tính toán. Ngoài ra, chúng tôi sử dụng DeepSpeed Zero Stage 3 để tối ưu hóa phân phối các tham số mô hình và trình tối ưu và tăng cường quản lý bộ nhớ qua các GPU, đảm bảo sử dụng hiệu quả các tài nguyên có sẵn. Quá trình fine-tuning kéo dài hai epoch và sử dụng định dạng chỉ dẫn Alpaca tiêu chuẩn, được thiết kế để cải thiện khả năng phản hồi và độ chính xác của các mô hình trong việc xử lý các nhiệm vụ dựa trên chỉ dẫn.

Áp Dụng Trực Tiếp Shared Attention
Việc áp dụng SA được kiểm tra qua các phân đoạn lớp riêng biệt trong các mô hình Llama2-7B và Llama3-8B, mỗi mô hình bao gồm tổng cộng 32 lớp. Để đánh giá độ mạnh mẽ và khả năng thích ứng của SA như được hiển thị trong Hình 4, nó được triển khai trong các phân đoạn lớp khác nhau, từ khoảng cách hẹp hơn như bốn lớp (ví dụ: SA:15∼18) đến khoảng cách rộng hơn như tám lớp (ví dụ: SA:23∼30).

Đánh giá sơ bộ về SA trong các lớp sớm hơn của Llama2-7B (ví dụ: lớp 3 đến 6) dẫn đến sự bùng nổ perplexity, chỉ ra những gián đoạn đáng kể trong khả năng dự đoán chính xác các token tiếp theo của mô hình. Hiện tượng này nhấn mạnh vai trò quan trọng mà sự biến thiên điểm attention đóng trong các giai đoạn sớm của việc xử lý mô hình, điều quan trọng cho việc thiết lập ngữ cảnh ban đầu và trích xuất đặc trưng. Để đánh giá định lượng tác động của phương sai attention trong suốt mô hình, chúng tôi tiến hành phân tích phương sai chi tiết. Chúng tôi áp dụng cùng phương pháp tính toán được sử dụng để có được điểm trung bình attention để tính toán phương sai của trọng số attention trong Llama2-7B và Llama3-8B trong khi xử lý tập dữ liệu MMLU. Chúng tôi tiếp tục khám phá ảnh hưởng tiềm năng của phương sai attention trong các lớp downstream

Hình 4: Hình minh họa việc triển khai Shared Attention trong các phân đoạn lớp cụ thể của mô hình. Shared Attention trải dài từ lớp 27 đến 30 cho phân đoạn bốn lớp và từ lớp 23 đến 30 cho phân đoạn tám lớp.

bằng cách tính toán một phương sai tích lũy có trọng số. Độ đo này tổng hợp các phương sai của tất cả các lớp downstream bắt đầu từ mỗi lớp cụ thể, được trọng số bởi trung bình của những phương sai tổng này. Như được minh họa trong Hình 5, phân tích tiết lộ rằng các lớp sớm thể hiện phương sai có trọng số cao hơn đáng kể so với các lớp sau. Phương sai này có xu hướng giảm khi tiến qua kiến trúc của mô hình, gợi ý sự ổn định của cơ chế attention trong các lớp sau. Dựa trên những kết quả này, các thí nghiệm của chúng tôi chủ yếu tập trung vào việc áp dụng SA trong các lớp sau, nơi những phương sai như vậy xuất hiện ổn định.

Kết quả của những thí nghiệm này, như được tóm tắt trong Bảng 1, tiết lộ các mẫu thú vị. Đối với mô hình Llama2-7B, việc triển khai SA trong các lớp sau (ví dụ: SA:23∼26 và SA:27∼30) duy trì hiệu suất tương đối ổn định qua nhiều benchmark, bao gồm GLUE và MMLU. Ngược lại, việc mở rộng phạm vi của SA để bao gồm nhiều lớp hơn, đặc biệt là các lớp cấp độ giữa như SA:15∼18, dẫn đến sự suy giảm đáng chú ý trong các nhiệm vụ đòi hỏi lý luận toán học (GSM8K).

So sánh, mô hình Llama3-8B, vốn có sự tương tự attention theo lớp cao hơn như đã thảo luận

--- TRANG 6 ---
Mô hình | GLUE | GSM8K 5-shot | HellaSwag | MMLU
Llama2-7B | 0.4050±0.0019 | 0.1395±0.0095 | 0.5713±0.0049 | 0.4119±0.0041
Llama2-7B SA:23∼30 | 0.3819±0.0019 | 0.0728±0.0072 | 0.5575±0.0050 | 0.3794±0.0040
Llama2-7B SA:27∼30 | 0.3882±0.0019 | 0.1243±0.0091 | 0.5616±0.0050 | 0.4056±0.0041
Llama2-7B SA:23∼26 | 0.4351±0.0019 | 0.1122±0.0087 | 0.5681±0.0049 | 0.3994±0.0040
Llama2-7B SA:19∼22 | 0.3996±0.0019 | 0.0834±0.0076 | 0.5553±0.0050 | 0.3926±0.0040
Llama2-7B SA:15∼18 | 0.3731±0.0019 | 0.0220±0.0040 | 0.4790±0.0050 | 0.3378±0.0047
Llama2-7B-Instruct-SFT | 0.5372±0.0019 | 0.1440±0.0097 | 0.5772±0.0049 | 0.3722±0.0040
Llama2-7B-Instruct-SFT SA:23∼30 | 0.5401±0.0019 | 0.0758±0.0073 | 0.5671±0.0049 | 0.3717±0.0040
Llama3-8B | 0.4804±0.0019 | 0.5155±0.0138 | 0.6009±0.0049 | 0.6198±0.0038
Llama3-8B SA:23∼30 | 0.5595±0.0019 | 0.3275±0.0129 | 0.6011±0.0049 | 0.6122±0.0038
Llama3-8B SA:27∼30 | 0.5532±0.0019 | 0.4526±0.0137 | 0.6060±0.0049 | 0.6163±0.0038
Llama3-8B SA:23∼26 | 0.5024±0.0019 | 0.4556±0.0137 | 0.5993±0.0049 | 0.6189±0.0038
Llama3-8B SA:19∼22 | 0.5115±0.0019 | 0.3745±0.0133 | 0.5829±0.0049 | 0.6181±0.0038
Llama3-8B SA:15∼18 | 0.4685±0.0019 | 0.0136±0.0032 | 0.5307±0.0050 | 0.3019±0.0038

Bảng 1: Các độ đo hiệu suất cho các mô hình khác nhau qua các nhiệm vụ

Hình 5: Hình hiển thị phương sai tích lũy có trọng số cho các mô hình Llama2-7B-chat và Llama3-8B-instruct. Hai trục dưới đại diện cho cấu trúc của mô hình: trục trái chi tiết 32 lớp, và trục phải hiển thị 32 heads trong mỗi lớp. Trục z đại diện cho các giá trị phương sai.

trong các phần trước, thể hiện sự suy giảm hiệu suất ít hơn khi SA được áp dụng. Sau khi triển khai SA trong các lớp gần hơn với đầu ra của mô hình (ví dụ: SA:27∼30), Llama3-8B thậm chí vượt trội so với cấu hình ban đầu của nó trên benchmark GLUE, gợi ý rằng việc đặt SA một cách chiến lược có thể tiềm năng tăng cường hiệu suất của mô hình trong các nhiệm vụ hiểu ngôn ngữ tự nhiên phức tạp.

Fine-Tuning trên Tập Dữ Liệu Instruct
Do những hạn chế tính toán ngăn cản việc pretraining LLMs với SA từ đầu, chúng tôi chấp nhận fine-tune các LLM hiện có để đánh giá liệu fine-tuning có thể cải thiện những thiếu hụt hiệu suất quan sát được với việc áp dụng trực tiếp SA hay không. Cách tiếp cận này đặc biệt nhằm hiểu khả năng thích ứng của SA dưới một chế độ học có kiểm soát hơn.

Fine-tuning được tiến hành trên tập dữ liệu Instruct có sẵn công khai, được thiết kế để đánh giá các mô hình trên các nhiệm vụ đòi hỏi tuân theo các chỉ dẫn phức tạp. Tập dữ liệu này được chọn vì nó thách thức các mô hình sử dụng hiệu quả các biểu diễn đã học của chúng, làm cho nó trở thành một benchmark lý tưởng để kiểm tra hiệu quả của các điều chỉnh như SA.

Kết quả, như được tóm tắt trong Bảng 1, chứng minh một khoảng cách hiệu suất thu hẹp giữa các mô hình ban đầu và những mô hình được điều chỉnh với SA. Ví dụ, trong khi mô hình Llama2-7B ban đầu vượt trội so với phiên bản SA trong các bài kiểm tra áp dụng trực tiếp, Llama2-7B SA:23∼30 được fine-tune cho thấy cải thiện đáng kể qua nhiều độ đo. Điều này gợi ý rằng fine-tuning cho phép mô hình tích hợp và tận dụng tốt hơn cơ chế Shared Attention, hiệu quả lấy lại một số hiệu suất bị mất được ghi nhận trong việc áp dụng ban đầu của SA.

Những phát hiện này chỉ ra tiềm năng của fine-tuning như một phương pháp khả thi để tích hợp những thay đổi kiến trúc mới như SA vào các mô hình hiện có. Sự phục hồi trong hiệu suất chỉ ra rằng với việc huấn luyện đầy đủ, những bất lợi ban đầu của việc áp dụng trực tiếp SA có thể được giảm thiểu, dẫn đến khả năng mô hình nâng cao phù hợp chặt chẽ hơn với hoặc thậm chí vượt quá các cấu hình ban đầu của chúng.

Hướng Tương Lai
Các nghiên cứu thí nghiệm của chúng tôi đã chứng minh rằng việc triển khai Shared Attention (SA) qua nhiều lớp sau trong LLMs gây ra sự mất mát độ chính xác tối thiểu, làm cho nó trở thành một cách tiếp cận đầy hứa hẹn để tăng cường hiệu quả mô hình. Hơn nữa, phân tích của chúng tôi tiết lộ một xu hướng hướng tới các mẫu attention đẳng hướng trong quá trình pretraining, chỉ ra rằng các cơ chế attention của mô hình có xu hướng ổn định khi chúng xử lý nhiều dữ liệu hơn.

Dựa trên những insight này, việc tích hợp SA từ giai đoạn pretraining xuất hiện như một chiến lược đặc biệt có lợi. Sự tích hợp sớm này có thể cho phép các mô hình thích ứng tốt hơn với cơ chế attention được hợp lý hóa, tiềm năng cải thiện hiệu suất và hiệu quả qua các nhiệm vụ khác nhau. Việc nhúng nền tảng của SA có thể đơn giản hóa các thích ứng sau này và vốn dĩ hỗ trợ động lực attention hiệu quả.

--- TRANG 7 ---
Một hướng nghiên cứu đầy hứa hẹn khác liên quan đến việc khám phá sự kết hợp giữa SA và các chiến lược chia sẻ attention khác như Cross-Layer Attention (CLA). Kết hợp SA với các phương pháp như CLA có thể khai thác điểm mạnh của cả hai cách tiếp cận, dẫn đến một cơ chế attention mạnh mẽ và linh hoạt hơn. Cách tiếp cận toàn diện này đối với quản lý attention có thể cung cấp một giải pháp tổng thể tối đa hóa cả hiệu quả tính toán và khả năng mở rộng mô hình.

Bằng cách theo đuổi những hướng này, nghiên cứu tương lai không chỉ có thể tinh chỉnh việc áp dụng Shared Attention trong LLMs mà còn khám phá tiềm năng đầy đủ của nó trong việc tăng cường hiệu quả kiến trúc và hoạt động của các mô hình ngôn ngữ thế hệ tiếp theo. Những nỗ lực này có thể dẫn đến các mô hình được trang bị tốt hơn để xử lý độ phức tạp và đa dạng ngày càng tăng của các nhiệm vụ trong xử lý ngôn ngữ tự nhiên.

Công Trình Liên Quan
Quản lý bộ nhớ hiệu quả trong transformers là một lĩnh vực nghiên cứu quan trọng với các mục tiêu đa dạng từ việc giảm băng thông bộ nhớ và yêu cầu lưu trữ đến tối ưu hóa chi phí tính toán trong cả giai đoạn huấn luyện và suy luận. Đáng chú ý, công trình của chúng tôi tập trung vào việc giảm thiểu kích thước cache Key-Value (KV) suy luận tồn tại giữa các lần chuyển mô hình, từ đó tăng cường hiệu quả mô hình mà không có sự thỏa hiệp đáng kể trong hiệu suất.

Hiệu Quả Bộ Nhớ trong Cơ Chế Attention
Những nỗ lực đáng kể đã được thực hiện để giải quyết hiệu quả của cache KV sau huấn luyện. Các kỹ thuật như nén cache KV đã được khám phá rộng rãi. Ví dụ, các phương pháp như KVQuant (Hooper et al. 2024) và KIVI (Liu et al. 2024b) sử dụng các chiến lược quantization để giảm dấu chân bộ nhớ của các cặp KV xuống chỉ vài bit. Hơn nữa, các công trình như AttentionSink (Xiao et al. 2023) và Scissorhands (Liu et al. 2024a) giới thiệu tính thưa thớt vào cache KV bằng cách lưu trữ có chọn lọc các phần tử dựa trên độ gần hoặc tầm quan trọng đối với token tạo, do đó giảm yêu cầu lưu trữ tổng thể.

Đổi Mới Kiến Trúc để Giảm Cache KV
Các điều chỉnh kiến trúc nhằm giảm kích thước cache KV là quan trọng trong việc tăng cường hiệu quả của các mô hình ngôn ngữ lớn. Những chiến lược như vậy bao gồm việc giới hạn độ dài chuỗi hiệu quả, như được thấy trong Sparse Attention (Child et al. 2019), ràng buộc attention vào các cửa sổ cục bộ để giảm cả tải tính toán và chi phí bộ nhớ. Một cách tiếp cận khác liên quan đến việc thay thế attention softmax truyền thống bằng các lựa chọn thay thế có thể mở rộng như linear attention (Katharopoulos et al. 2020), duy trì độ phức tạp không gian không đổi và cung cấp khả năng mở rộng graceful hơn đối với số lượng token. Ngoài ra, các phương pháp như Grouped-Query Attention (GQA) (Ainslie et al. 2023) và Multi-Query Attention (MQA) (Shazeer 2019) tổng hợp attention qua nhiều queries, giảm đáng kể dấu chân bộ nhớ bằng cách chia sẻ các cặp KV qua các attention heads. Những đổi mới này tập thể góp phần giảm sự dư thừa trong tính toán attention và liên quan trực tiếp đến công trình của chúng tôi, thông báo cho sự phát triển của chúng tôi về cơ chế Shared Attention tiếp tục tối ưu hóa sử dụng bộ nhớ bằng cách chia sẻ trọng số attention qua các lớp.

Kết Luận
Trong bài báo này, chúng tôi đã khám phá động lực attention trong các LLM tiên tiến và quan sát rằng phân phối attention qua các lớp có xu hướng đẳng hướng hóa sau khi pretraining rộng rãi. Mẫu attention đẳng hướng này, nơi các lớp thể hiện các cơ chế attention tương tự, đã truyền cảm hứng cho một cách tiếp cận mới về chia sẻ attention khác biệt so với các phương pháp thông thường.

Truyền thống, các phương pháp như MQA và CLA đã tập trung vào việc chia sẻ cache KV để giảm chi phí bộ nhớ nhưng vẫn yêu cầu tính toán trọng số attention độc lập qua mỗi lớp. Phương pháp Shared Attention (SA) được đề xuất của chúng tôi bỏ qua sự dư thừa này bằng cách chia sẻ trực tiếp các trọng số attention đã tính toán qua nhiều lớp. Cách tiếp cận này không chỉ giảm đáng kể kích thước cache KV mà còn giảm FLOPs tính toán cần thiết trong quá trình suy luận mô hình.

Việc giới thiệu Shared Attention đại diện cho một sự thay đổi paradigm trong thiết kế các cơ chế attention trong mạng neural, nhấn mạnh hiệu quả mà không thỏa hiệp hiệu suất của mô hình. Bằng cách giảm cả gánh nặng tính toán và yêu cầu bộ nhớ, SA cho phép triển khai LLM có thể mở rộng và hiệu quả hơn, đặc biệt trong các môi trường nơi tài nguyên bị hạn chế.

Nghiên cứu này mở đường cho các khám phá thêm về kiến trúc mô hình hiệu quả và mở ra các khả năng mới cho việc áp dụng LLMs qua một phổ rộng hơn của các nhiệm vụ và tập dữ liệu. Công trình tương lai sẽ tập trung vào việc mở rộng khả năng áp dụng của Shared Attention, khám phá sự tích hợp của nó trong các giai đoạn ban đầu của việc huấn luyện mô hình, và kết hợp nó với các kỹ thuật tối ưu hóa khác để tối đa hóa hiệu quả hoạt động của LLMs.

Tài Liệu Tham Khảo
Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy, Y.; Lebrón, F.; and Sanghai, S. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245.

Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan, Y.; Ge, W.; Han, Y.; Huang, F.; et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.

Brandon, W.; Mishra, M.; Nrusimha, A.; Panda, R.; and Kelly, J. R. 2024. Reducing Transformer Key-Value Cache Size with Cross-Layer Attention. arXiv preprint arXiv:2405.12981.

Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.

Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

--- TRANG 8 ---
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.

Hooper, C.; Kim, S.; Mohammadzadeh, H.; Mahoney, M. W.; Shao, Y. S.; Keutzer, K.; and Gholami, A. 2024. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079.

Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.; Zhang, Y.; Fu, Y.; et al. 2024. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36.

Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, 5156–5165. PMLR.

Li, H.; Zhang, Y.; Koto, F.; Yang, Y.; Zhao, H.; Gong, Y.; Duan, N.; and Baldwin, T. 2023. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212.

Liu, Z.; Desai, A.; Liao, F.; Wang, W.; Xie, V.; Xu, Z.; Kyrillidis, A.; and Shrivastava, A. 2024a. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36.

Liu, Z.; Yuan, J.; Jin, H.; Zhong, S.; Xu, Z.; Braverman, V.; Chen, B.; and Kivi, X. H. 2024b. A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750.

Shazeer, N. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150.

Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html, 3(6): 7.

Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.

Xiao, G.; Tian, Y.; Chen, B.; Han, S.; and Lewis, M. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453.

Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv, C.; Pan, D.; Wang, D.; Yan, D.; et al. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305.

Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830.
