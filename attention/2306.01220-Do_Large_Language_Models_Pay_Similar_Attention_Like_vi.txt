# Liệu các Mô hình Ngôn ngữ Lớn có Chú ý Tương tự như Lập trình viên Con người Khi Tạo Code Không?

BONAN KOU, Đại học Purdue, Hoa Kỳ
SHENGMAI CHEN∗, Đại học Brown, Hoa Kỳ  
ZHIJIE WANG, Đại học Alberta, Canada
LEI MA, Đại học Tokyo, Nhật Bản và Đại học Alberta, Canada
TIANYI ZHANG, Đại học Purdue, Hoa Kỳ

Các Mô hình Ngôn ngữ Lớn (LLMs) gần đây đã được sử dụng rộng rãi cho việc tạo code. Do tính phức tạp và không minh bạch của LLMs, ít được biết về cách những mô hình này tạo ra code. Chúng tôi đã thực hiện nỗ lực đầu tiên để thu hẹp khoảng cách kiến thức này bằng cách điều tra liệu LLMs có chú ý đến cùng những phần của mô tả nhiệm vụ như lập trình viên con người trong quá trình tạo code hay không. Phân tích sáu LLMs, bao gồm GPT-4, trên hai tiêu chuẩn tạo code phổ biến đã tiết lộ sự không đồng bộ nhất quán giữa sự chú ý của LLMs và lập trình viên. Chúng tôi đã phân tích thủ công 211 đoạn code không chính xác và tìm thấy năm mẫu chú ý có thể được sử dụng để giải thích nhiều lỗi tạo code. Cuối cùng, một nghiên cứu người dùng cho thấy sự chú ý của mô hình được tính toán bằng phương pháp dựa trên nhiễu loạn thường được lập trình viên con người ưa chuộng. Các phát hiện của chúng tôi nhấn mạnh nhu cầu về LLMs phù hợp với con người để có khả năng diễn giải tốt hơn và sự tin tưởng của lập trình viên.

CCS Concepts: •Software and its engineering ;•Computing methodologies →Natural language processing ;

Additional Key Words and Phrases: Tạo Code, Mô hình Ngôn ngữ Lớn, Chú ý

ACM Reference Format:
Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2024. Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?. Proc. ACM Softw. Eng. 1, FSE, Article 100 (July 2024), 24 pages. https://doi.org/10.1145/3660807

## 1 GIỚI THIỆU

Các Mô hình Ngôn ngữ Lớn (LLMs) đã có tiến bộ đáng kể trong việc tạo code trong những năm gần đây [4,5,26,27,29,32,36,43,87]. Một nghiên cứu gần đây [14] cho thấy GPT-4, LLM hiện đại nhất với 1.7 nghìn tỷ tham số, có thể giải quyết chính xác 84% các nhiệm vụ lập trình Python từ tiêu chuẩn HuamnEval [27]. Mặc dù có tiến bộ lớn này, vẫn chưa rõ tại sao và làm thế nào LLMs có thể tạo ra code chính xác từ các mô tả ngôn ngữ tự nhiên.

Phân tích chú ý của mô hình là một phương pháp luận phổ biến để hiểu cách một mô hình hoạt động. Nó đã được áp dụng rộng rãi trong thị giác máy tính [31,38,42,44,59,63,80,101] để điều tra liệu một mô hình có chú ý đến những phần nổi bật của hình ảnh khi đưa ra quyết định hay không. Đặc biệt, các nghiên cứu gần đây thấy rằng việc căn chỉnh sự chú ý của mô hình với sự chú ý của con người có thể nâng cao hiệu suất mô hình một cách hiệu quả [8,34,42]. Ví dụ, Huang et al. [42] cho thấy hiệu suất của các mô hình phân loại hình ảnh dựa trên Conv-4 có thể tăng lên đến 23% khi chúng được huấn luyện để phù hợp với sự chú ý của con người. Hơn nữa, các nghiên cứu trước đây cũng gợi ý rằng người dùng có thêm sự tự tin và tin tưởng vào các mô hình phù hợp với con người [13,40,75,89]. Ví dụ, Boggust et al. [13] thấy rằng người dùng xác định độ tin cậy của một mô hình bằng cách kiểm tra liệu mô hình có đưa ra dự đoán dựa trên các đặc điểm mà họ coi là quan trọng hay không.

∗Công việc này được thực hiện khi Shengmai Chen là sinh viên đại học tại Đại học Purdue.

Những phát hiện này dẫn đến một câu hỏi khoa học quan trọng cho việc tạo code dựa trên LLM—liệu LLMs có chú ý đến những phần tương tự của mô tả nhiệm vụ như lập trình viên con người trong việc tạo code không? Chúng tôi chọn so sánh sự chú ý của mô hình với sự chú ý của con người, vì nó có thể giúp chúng tôi xác định liệu LLMs có nắm bắt được ngữ nghĩa sâu trong mô tả nhiệm vụ như con người hay liệu chúng chỉ học các mẫu bề mặt từ dữ liệu huấn luyện, điều này được biết đến như một vấn đề phổ biến trong học máy. Hơn nữa, bằng cách so sánh các mẫu chú ý của con người và mô hình, chúng tôi tìm cách điều tra liệu sự khác biệt về chú ý có thể được sử dụng để giải thích một số lỗi tạo code và thông báo các cơ hội mới để cải thiện LLMs cho việc tạo code.

Để thu hẹp khoảng cách kiến thức, chúng tôi đã thực hiện nỗ lực đầu tiên để tiết lộ quá trình tạo code của LLMs bằng cách phân tích những phần nào của ngôn ngữ con người mà một LLM chú ý đến khi tạo code. Chúng tôi trình bày một nghiên cứu quy mô lớn kiểm tra sự căn chỉnh chú ý giữa sáu LLMs và lập trình viên con người trên hai tiêu chuẩn tạo code phổ biến—tiêu chuẩn HumanEval của OpenAI [27] và tiêu chuẩn MBPP của Google [7]. Giả thuyết là LLMs nên tạo code dựa trên các từ nổi bật từ mô tả NL tương tự như lập trình viên con người, thay vì tạo code dựa trên các token tầm thường như giới từ và dấu phân cách. Cụ thể, chúng tôi đã điều tra các câu hỏi nghiên cứu sau trong nghiên cứu này:

RQ1 Sự chú ý của mô hình được căn chỉnh với sự chú ý của con người ở mức độ nào?
RQ2 Sự chú ý có thể giải thích lỗi của các mô hình tạo code không?
RQ3 Tác động của các phương pháp tính toán chú ý khác nhau đối với việc căn chỉnh chú ý là gì?
RQ4 Phương pháp tính toán chú ý nào được lập trình viên ưa chuộng nhất?

Vì không có tiêu chuẩn tạo code hiện có nào chứa thông tin chú ý của lập trình viên, chúng tôi đã tạo ra bộ dữ liệu chú ý lập trình viên đầu tiên cho các nhiệm vụ lập trình từ HumanEval và MBPP (tổng cộng 1,138 nhiệm vụ). Chúng tôi ghi lại sự chú ý của lập trình viên bằng cách yêu cầu hai lập trình viên có kinh nghiệm gắn nhãn thủ công các từ và cụm từ mà họ coi là thiết yếu để giải quyết mỗi nhiệm vụ lập trình. Các nhãn của họ được xác thực bởi một lập trình viên thứ ba. Mặt khác, để ghi lại sự chú ý của mô hình, chúng tôi đã triển khai và thử nghiệm với mười hai phương pháp tính toán chú ý khác nhau trong ba danh mục—dựa trên self-attention, dựa trên gradient, và dựa trên nhiễu loạn. Để đảm bảo các phát hiện của chúng tôi tổng quát hóa trên các LLMs khác nhau, chúng tôi đã phân tích sự chú ý của sáu LLMs với các kích thước khác nhau, bao gồm GPT-4 [2], InCoder-1.3B [32], CoderGen-2.7B [62], PolyCoder-2.7B [91], CodeParrot-1.5B [1], và GPT-J-6B [87].

Nghiên cứu của chúng tôi tiết lộ một số hiểu biết quan trọng về quá trình tạo code của LLMs. Đầu tiên, chúng tôi thấy sự không đồng bộ chú ý nhất quán trong tất cả sáu LLMs, bất kể phương pháp tính toán chú ý. Hơn nữa, chúng tôi đã thực hiện phân tích chuyên sâu về các mẫu chú ý của 211 code không chính xác được tạo bởi hai mô hình tốt nhất trong nghiên cứu của chúng tôi, CodeGen-2.7B và GPT-4. Chúng tôi thấy rằng 27% lỗi tạo code có thể được giải thích bằng năm mẫu chú ý. Cuối cùng, các phương pháp dựa trên nhiễu loạn tạo ra điểm chú ý được căn chỉnh với sự chú ý của con người tổng thể hơn so với các phương pháp khác. Chúng cũng được lập trình viên con người ưa chuộng, theo nghiên cứu người dùng với 22 người tham gia. Các phát hiện của chúng tôi nhấn mạnh nhu cầu phát triển LLMs phù hợp với con người và cung cấp hướng dẫn thực tế để cải thiện việc tạo code dựa trên LLM và tính toán sự chú ý của mô hình.

Tóm lại, bài báo này đóng góp như sau:

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

●Chúng tôi đã tiến hành nghiên cứu thực nghiệm đầu tiên về sự căn chỉnh chú ý của LLMs và lập trình viên con người trên các nhiệm vụ tạo code.

●Chúng tôi đã tiến hành phân tích so sánh các phương pháp tính toán chú ý khác nhau cho các mô hình tạo code thông qua cả thí nghiệm định lượng và nghiên cứu người dùng.

●Chúng tôi đã công khai bộ dữ liệu chú ý lập trình viên đầu tiên của 1,138 nhiệm vụ Python, có thể được sử dụng để phát triển các mô hình phù hợp với con người mới và đánh giá các phương pháp diễn giải cho các mô hình tạo code. Code và dữ liệu của chúng tôi đã được cung cấp trong kho GitHub của chúng tôi [47].

## 2 ĐỘNG LỰC VÀ KIẾN THỨC NỀN TẢNG

Trong phần này, trước tiên chúng tôi giải thích động lực để thực hiện phân tích chú ý cho các mô hình tạo code. Sau đó, chúng tôi giới thiệu các tiêu chuẩn và số liệu phổ biến cho việc tạo code. Cuối cùng, chúng tôi định nghĩa sự chú ý của mô hình và mô tả các loại phương pháp tính toán chú ý khác nhau cho LLMs.

### 2.1 Động lực

Phân tích chú ý của mô hình là một nhiệm vụ phổ biến trong một số lĩnh vực, như thị giác máy tính [31,44,59,63], dịch máy thần kinh [18], và lái xe tự động [38,46,80]. Cụ thể, một số nghiên cứu cho thấy việc căn chỉnh sự chú ý của mô hình với sự chú ý của con người trong quá trình huấn luyện có thể cải thiện đáng kể hiệu suất mô hình [8,34,42]. Ví dụ, Huang et al. [42] cho thấy bằng cách căn chỉnh sự chú ý của các mô hình dựa trên Conv-4 và ResNet với sự chú ý của con người trên hình ảnh, hiệu suất của những mô hình này trong phân loại hình ảnh có thể tăng lên đến 23% trong thiết lập một lần và 10% trong thiết lập năm lần. Trong lái xe tự động, nhiều nghiên cứu đã chứng minh rằng việc thêm các ràng buộc căn chỉnh chú ý có thể giúp hệ thống lái xe tự động lái xe an toàn hơn [38,46,80]. Điều này thúc đẩy chúng tôi điều tra sự căn chỉnh chú ý giữa LLMs và lập trình viên con người trong lĩnh vực tạo code.

Một số nghiên cứu gần đây đã phân tích sự chú ý của các mô hình thần kinh cho tóm tắt code, sửa chữa chương trình, và dự đoán tên phương thức [8,64,68]. Paltenghi et al. [64] nghiên cứu sự căn chỉnh chú ý giữa sự chú ý của mô hình thần kinh và sự chú ý của lập trình viên trong tóm tắt code. Bansal et al. [8] cho thấy việc căn chỉnh sự chú ý của các mô hình tóm tắt code thần kinh với sự chú ý của con người có thể cải thiện hiệu suất mô hình một cách hiệu quả. Rabin et al. [68] thấy rằng các mô hình code được huấn luyện trước phụ thuộc rất nhiều vào chỉ một vài đặc điểm cú pháp trong các gợi ý để thực hiện dự đoán tên phương thức và phát hiện sử dụng sai biến. Theo hiểu biết tốt nhất của chúng tôi, không có nghiên cứu hiện có nào đã điều tra các nhiệm vụ tạo code hoặc LLMs với hàng tỷ tham số. Nghiên cứu của chúng tôi lấp đầy khoảng trống này bằng cách phân tích các mẫu chú ý của sáu LLMs trong các nhiệm vụ tạo code.

Cuối cùng, một động lực khác đằng sau nghiên cứu này là thực tế rằng vẫn chưa có sự đồng thuận về cách tính toán điểm chú ý của các mô hình code dựa trên LLM. Điều này phần lớn được quy cho tính phức tạp của cơ chế chú ý đa đầu, đa lớp được áp dụng bởi những mô hình này. Ví dụ, một số nghiên cứu chỉ xem xét sự chú ý của mô hình từ lớp transformer đầu tiên và tuyên bố rằng lớp đầu tiên mã hóa thông tin ở mức từ vựng [10,97], trong khi các nghiên cứu khác tổng hợp sự chú ý từ tất cả các lớp transformer để kết hợp các mối quan hệ token khoảng cách xa [56,85]. Để thu hẹp khoảng cách này, chúng tôi đã tiến hành nghiên cứu toàn diện đầu tiên về 12 phương pháp tính toán chú ý và đánh giá chúng một cách có hệ thống với các thí nghiệm định lượng và nghiên cứu người dùng với 22 người tham gia.

### 2.2 Tiêu chuẩn và Số liệu Tạo Code

Trong công việc này, chúng tôi tập trung vào các nhiệm vụ tạo code tạo ra một hàm từ mô tả ngôn ngữ tự nhiên. Kể từ đột phá của mô hình Codex của OpenAI vào năm 2021 [27], loại nhiệm vụ tạo code này đã trở nên ngày càng phổ biến trong cộng đồng nghiên cứu. Trong thiết lập nhiệm vụ này, cho một header hàm và mô tả nhiệm vụ bằng ngôn ngữ tự nhiên, một LLM được mong đợi hoàn thành hàm theo mô tả nhiệm vụ. Hình 1 cho thấy một ví dụ.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Hình 1. Một hàm Python được tạo bởi CodeGen-2.7B [62]. Code được tạo được làm nổi bật màu xanh lá cây.

Các mô hình tạo code thường được đánh giá trên các tiêu chuẩn lập trình được tạo bởi cộng đồng. OpenAI đã phát triển một tiêu chuẩn lập trình gọi là HumanEval và sử dụng nó để đánh giá mô hình Codex gốc và các biến thể của nó [27]. HumanEval [27] bao gồm 164 nhiệm vụ lập trình Python và các giải pháp chuẩn. Cho đến nay đây là tiêu chuẩn phổ biến nhất và đã được sử dụng để đánh giá hầu hết các mô hình tạo code dựa trên LLM. Bên cạnh đó, MBPP là một tiêu chuẩn lớn [7] với 974 nhiệm vụ lập trình và giải pháp được tạo bởi cộng đồng. Cả HumanEval và MBPP đều bao gồm các test case để đánh giá tính chính xác chức năng của code được tạo.

Hai loại số liệu thường được sử dụng để đánh giá hiệu suất của các mô hình tạo code dựa trên LLM. Đầu tiên, nếu một tiêu chuẩn tạo code bao gồm các test case, người ta có thể đơn giản chạy các test case để đánh giá tính chính xác của code được tạo. Một số liệu điển hình trong danh mục này là Pass@k. Pass@k ban đầu được giới thiệu bởi Kulal et al. [50]. Nó đo lường tỷ lệ phần trăm các nhiệm vụ lập trình được giải quyết chính xác trong đó k đề cập đến số lượng mẫu code được tạo bởi LLM. Nếu bất kỳ mẫu nào trong k mẫu vượt qua tất cả test case trong một nhiệm vụ, nhiệm vụ đó được coi là được giải quyết chính xác. OpenAI sau đó giới thiệu một phiên bản không thiên vị của Pass@k để giảm phương sai, được sử dụng rộng rãi để đánh giá các mô hình tạo code ngày nay [27]. Trong thực tế, nhiều nhiệm vụ lập trình không có test case hiện có và một số giải pháp code không có giao diện hàm được định nghĩa rõ ràng để kiểm tra, ví dụ như một dòng code duy nhất không có đầu vào và đầu ra rõ ràng. Do đó, công việc trước đây cũng đo lường sự tương tự giữa code được tạo và giải pháp chuẩn như một proxy cho tính chính xác. BLEU [65] và CodeBLEU [69] là các số liệu tương tự được áp dụng phổ biến. BLEU là một số liệu thường được sử dụng để đánh giá chất lượng văn bản được tạo bởi máy. Nó đánh giá chất lượng văn bản được tạo bởi máy bằng cách so sánh sự hiện diện của n-gram trong văn bản được tạo với những văn bản tham chiếu. BLEU tính toán một điểm số từ 0 đến 1, với 1 cho thấy sự tương tự hoàn hảo. CodeBLEU được thiết kế để điều chỉnh BLEU cụ thể cho việc đánh giá code được tạo. Trong khi BLEU chủ yếu xem xét sự tương tự ở mức từ, CodeBLEU xem xét cấu trúc và tính chính xác của code được tạo, là những khía cạnh quan trọng trong các nhiệm vụ tạo code.

### 2.3 Sự Chú ý của Mô hình

Trong công việc này, chúng tôi sử dụng sự chú ý của mô hình để đề cập đến mức độ quan trọng của một token trong mô tả nhiệm vụ ngôn ngữ tự nhiên được coi là bởi một LLM trong quá trình tạo code. Nó ngụ ý những phần nào của đầu vào mà mô hình "chú ý" đến trong quá trình tạo code. Ý tưởng này giống với tầm quan trọng đặc điểm [41], bản đồ nổi bật [61], và quy kết đặc điểm [60] trong tài liệu XAI. Chúng tôi mô tả ba loại phương pháp tính toán chú ý của mô hình như sau.

#### 2.3.1 Phương pháp dựa trên Self-attention. 

Cơ chế self-attention cho phép transformer cân nhắc tầm quan trọng của các phần khác nhau của đầu vào khi đưa ra dự đoán. Bằng cách tập trung vào các token có liên quan nhất với điểm self-attention cao nhất, transformer có thể đưa ra dự đoán tốt hơn bằng cách ghi lại các mối quan hệ và phụ thuộc trong đầu vào.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Hình 2. Ma trận chú ý của đầu chú ý đầu tiên của lớp transformer trong CodeGen-2.7B

Một LLM bao gồm nhiều lớp transformer, mỗi lớp bao gồm nhiều đầu chú ý. Những đầu chú ý này tính toán độc lập điểm self-attention giữa các token đầu vào khác nhau. Do đó, một mô hình transformer có thể có nhiều nguồn chú ý mô hình từ các lớp transformer khác nhau và các đầu chú ý khác nhau trong mỗi lớp. Ví dụ, Hình 2 cho thấy điểm self-attention của ba đầu chú ý đầu tiên của lớp transformer đầu tiên trong CodeGen-2.7B khi tạo token "numbers" từ chuỗi đầu vào "Return the max between two". Điểm self-attention được tính toán bởi các đầu chú ý khác nhau khác nhau cho cùng một token. Các đầu khác nhau đại diện cho các loại "tập trung" khác nhau từ mô hình.

Để tính toán sự chú ý của mô hình trên chuỗi đầu vào, các vector điểm self-attention từ các lớp chú ý khác nhau và các đầu chú ý khác nhau trong mỗi lớp cần được tổng hợp thành một vector duy nhất để đại diện cho tầm quan trọng tổng thể của mỗi token đầu vào đối với dự đoán mô hình. Tuy nhiên, mặc dù điểm self-attention đã được sử dụng chung như sự chú ý của mô hình [20,33,52,84], vẫn chưa có sự đồng thuận về cách tổng hợp điểm self-attention từ các lớp và đầu chú ý khác nhau. Ví dụ, một số nghiên cứu tổng hợp điểm chú ý từ tất cả các lớp transformer và tất cả đầu chú ý trong mỗi lớp [99] như điểm chú ý cuối cùng. Những nghiên cứu này lập luận rằng chiến lược tổng hợp này có thể ghi lại các mối quan hệ token khoảng cách xa. Một số nghiên cứu khác chỉ sử dụng điểm chú ý từ lớp transformer đầu tiên và lập luận rằng lớp đầu tiên ghi lại các phụ thuộc ở mức từ vựng [10, 97].

Để tiết lộ cách các cách khác nhau để tổng hợp self-attention ảnh hưởng đến sự căn chỉnh giữa chú ý của mô hình và con người, chúng tôi đã thử nghiệm với sáu phương pháp dựa trên self-attention trong nghiên cứu này (chi tiết trong Phần 4.2.1).

#### 2.3.2 Phương pháp dựa trên Gradient. 

Các phương pháp dựa trên gradient tận dụng gradient của dự đoán mô hình liên quan đến các đặc điểm đầu vào để tính toán sự chú ý của mô hình. Việc tính toán các phương pháp dựa trên gradient bao gồm hai bước khác nhau: (1) thực hiện một lượt forward của đầu vào quan tâm, và (2) tính toán gradient bằng cách sử dụng backpropagation qua các lớp của mạng thần kinh. Bằng cách phân tích độ lớn của những gradient này, những phương pháp này có thể xác định token đầu vào nào có ảnh hưởng nhất trong việc xác định đầu ra. Ví dụ, Integrated Gradients là một phương pháp dựa trên gradient tính toán tích phân của gradient của đầu ra mô hình liên quan đến mỗi đặc điểm đầu vào [23,76,81]. Trong nghiên cứu này, chúng tôi đã thử nghiệm với hai phương pháp dựa trên gradient được sử dụng trong công việc trước [76, 78] với chi tiết trong Phần 4.2.2.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

#### 2.3.3 Phương pháp dựa trên Nhiễu loạn. 

Khác với hai danh mục phương pháp trước đó, các phương pháp dựa trên nhiễu loạn [84,90] là model-agnostic. Nói cách khác, chúng không yêu cầu truy cập vào thông tin nội bộ của một mô hình. Các phương pháp dựa trên nhiễu loạn đặc biệt hữu ích để tính toán sự chú ý của các mô hình thương mại như GPT-4, vì những mô hình này không tiết lộ các lớp self-attention hoặc gradient của chúng cho người dùng.

Các phương pháp dựa trên nhiễu loạn đầu tiên biến đổi đầu vào và sau đó tính toán sự chú ý của mô hình dựa trên sự khác biệt đầu ra. LIME [70] và SHAP [57] là hai phương pháp dựa trên nhiễu loạn phổ biến. LIME [70] tạo ra một giải thích cục bộ bằng cách xấp xỉ các dự đoán mô hình cụ thể với một mô hình đơn giản hơn (ví dụ, một bộ phân loại tuyến tính). SHAP [57] nâng cao LIME bằng cách nhiễu loạn đầu vào dựa trên lý thuyết trò chơi và sử dụng giá trị Shapely để ước tính tầm quan trọng của các token khác nhau.

Một hạn chế của hai phương pháp này là chúng thường yêu cầu một số lượng lớn mẫu nhiễu loạn để đảm bảo độ chính xác ước tính. Điều này tốn kém để tính toán sự chú ý cho GPT-4, vì chúng tôi cần truy vấn GPT-4 nhiều lần. Hơn nữa, LIME và SHAP chỉ biến đổi đầu vào bằng cách xóa token, điều này có thể thay đổi đáng kể ý nghĩa hoặc cấu trúc của đầu vào. Để giải quyết hạn chế này, các phương pháp dựa trên nhiễu loạn gần đây hơn chọn thay thế token bằng token tương tự hoặc có liên quan về mặt ngữ nghĩa trong ngữ cảnh [54,90]. Chúng thường sử dụng mô hình ngôn ngữ được che (masked) như BERT [25] để dự đoán token tương tự hoặc có liên quan về mặt ngữ nghĩa để thay thế token hiện có trong đầu vào. Sau đó, chúng đo lường ảnh hưởng của những thay thế này đối với đầu ra. Trong nghiên cứu này, chúng tôi đã thử nghiệm với SHAP [57] và phương pháp dựa trên che BERT [90] (chi tiết trong Phần 4.2.3).

## 3 XÂY DỰNG BỘ DỮ LIỆU CHÚ Ý LẬP TRÌNH VIÊN

Vì không có tiêu chuẩn tạo code hiện có nào chứa thông tin chú ý của lập trình viên (tức là từ hoặc cụm từ nào mà lập trình viên coi là quan trọng khi viết code), chúng tôi đã tạo ra bộ dữ liệu chú ý lập trình viên đầu tiên dựa trên 1,138 nhiệm vụ lập trình, bao gồm tất cả 164 prompt từ HumanEval [27] và 974 prompt từ MBPP [7]. Chúng tôi đã chọn hai bộ dữ liệu này vì chúng được sử dụng rộng rãi để đánh giá các mô hình tạo code và chúng cũng cung cấp test case cho mỗi nhiệm vụ lập trình, điều này quan trọng để tính toán tính chính xác của code được tạo bởi mô hình.

Hai tác giả đầu tiên, có hơn năm năm kinh nghiệm lập trình Python, đã gắn nhãn thủ công các từ và cụm từ mà họ coi là quan trọng để giải quyết nhiệm vụ lập trình trong mỗi mô tả nhiệm vụ. Trước quá trình gắn nhãn, hai người gắn nhãn đã xem qua các nhiệm vụ lập trình trong HumanEval để làm quen với các nhiệm vụ lập trình và các giải pháp code. Sau đó, họ đầu tiên độc lập gắn nhãn 20 mô tả nhiệm vụ đầu tiên trong HumanEval. Vòng gắn nhãn đầu tiên này có điểm Cohen's Kappa là 0.68. Hai người gắn nhãn đã thảo luận về sự bất đồng và tóm tắt bốn loại từ khóa mà cả hai đều coi là quan trọng. Bốn loại từ khóa được tóm tắt dưới đây:

●Kiểu dữ liệu: Từ hoặc cụm từ mô tả các loại dữ liệu mà code nên nhập hoặc xuất, như "string", "number", hoặc "list".

●Toán tử: Từ hoặc cụm từ mô tả các phép toán mà code nên thực hiện trên dữ liệu, như "compare", "sort", "filter", hoặc "search".

●Điều kiện: Từ hoặc cụm từ chỉ định các điều kiện mà code nên thực thi, như các cụm từ sau "if" và "when" trong mô tả nhiệm vụ.

●Thuộc tính: Các thuộc tính quan trọng của dữ liệu và phép toán được thao tác, như từ định lượng (ví dụ: "all", "one"), tính từ (ví dụ: "first", "closer"), và trạng từ (ví dụ: "every", "none").

Mặc dù chỉ có bốn loại từ khóa, mỗi loại được thiết kế để ở mức cao và bao gồm. Ví dụ, loại "toán tử" đề cập đến bất kỳ loại phép toán nào trên dữ liệu, như "sort a list", "connect a database", và "plot a graph". Với tiêu chuẩn gắn nhãn này, hai người gắn nhãn

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Hình 3. Hai ví dụ về prompt được gắn nhãn từ bộ dữ liệu của chúng tôi.

tiếp tục gắn nhãn 144 mô tả nhiệm vụ còn lại trong bộ dữ liệu HumanEval. Điểm Cohen's Kappa của vòng gắn nhãn này tăng lên 0.72, cho thấy sự đồng thuận đáng kể [58]. Sau đó, họ thảo luận và giải quyết tất cả bất đồng.

Để xác minh những nhãn này, tác giả thứ ba, không tham gia vào quá trình gắn nhãn trước đó, đã độc lập gắn nhãn 164 mô tả nhiệm vụ từ bộ dữ liệu HumanEval. Vì Cohen's Kappa chỉ có thể tính toán mức độ đồng thuận giữa hai người gắn nhãn, chúng tôi đã sử dụng Fleiss' Kappa để đo lường sự đồng thuận giữa người gắn nhãn thứ ba và các nhãn ban đầu từ hai người gắn nhãn đầu tiên. Điểm Fleiss' Kappa là 0.64, cho thấy sự đồng thuận đáng kể [30]. Kết quả này cho thấy các nhãn được tạo bởi hai người gắn nhãn đầu tiên là hợp lý và có thể được chấp nhận bởi các lập trình viên khác. Sau đó, hai người gắn nhãn đầu tiên tiếp tục gắn nhãn 974 nhiệm vụ lập trình trong bộ dữ liệu MBPP một cách độc lập, kết quả trong điểm Cohen's Kappa là 0.73. Cuối cùng, họ giải quyết tất cả bất đồng và sử dụng bộ nhãn cuối cùng làm bộ dữ liệu chú ý của lập trình viên. Toàn bộ quá trình gắn nhãn mất 192 giờ người.

Trung bình, mỗi mô tả nhiệm vụ có 29.6 từ, trong đó 7 từ được coi là quan trọng bởi cả hai người gắn nhãn. Trong tất cả bốn loại từ khóa, từ khóa thuộc tính (45.2%) được gắn nhãn thường xuyên nhất bởi hai người gắn nhãn, tiếp theo là toán tử (27.2%), điều kiện (25%), và kiểu dữ liệu (2.6%). Hình 3 cho thấy hai ví dụ về mô tả nhiệm vụ được gắn nhãn. Bốn loại từ khóa được gắn nhãn bằng các màu khác nhau.

## 4 PHƯƠNG PHÁP LUẬN

Phần này mô tả thiết kế nghiên cứu để trả lời các câu hỏi nghiên cứu được liệt kê trong Phần 1.

### 4.1 Mô hình Tạo Code

Trong nghiên cứu này, chúng tôi chọn sáu LLMs với các kích thước khác nhau và hiệu suất mô hình khác nhau trên các nhiệm vụ tạo code. Bảng 1 cho thấy kích thước, số lớp self-attention, số đầu chú ý trong mỗi lớp, và hiệu suất mô hình trên bộ dữ liệu kết hợp của HumanEval và MBPP theo Pass@1. Chúng tôi mô tả mỗi mô hình dưới đây.

●InCoder-1.3B [32] là một mô hình ngôn ngữ code mã nguồn mở từ Meta AI. Nó được huấn luyện trên 159 GB code được cấp phép cho phép từ GitHub, GitLab và Stack Overflow. So với các LLMs khác, nó áp dụng một mục tiêu che nhân quả mới, cho phép nó điền các khối code dựa trên ngữ cảnh trái và phải tùy ý. Chúng tôi đã sử dụng mô hình được huấn luyện trước lớn nhất được phát hành bởi Meta, bao gồm 1.3B tham số và 24 lớp transformer.

●PolyCoder-2.7B [91] là một mô hình mã nguồn mở từ CMU. Nó dựa trên kiến trúc GPT-2 và được thiết kế để trở thành đối tác mã nguồn mở của OpenAI Codex [27], vì Codex không được mã nguồn mở. Nó được huấn luyện trên 249GB code và có 2.7B tham số.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Bảng 1. Các mô hình tạo code được bao gồm trong nghiên cứu này.

| Mô hình | Lớp | Đầu | Pass@1 |
|---------|-----|-----|--------|
| InCoder-1.3B | 24 | 32 | 15.20% |
| PolyCoder-2.7B | 32 | 32 | 5.59% |
| CodeGen-2.7B | 32 | 32 | 23.70% |
| CodeParrot-1.5B | 48 | 25 | 3.58% |
| GPT-J-6B | 28 | 16 | 11.62% |
| GPT-4 | - | - | 67% |

●CodeGen-Mono-2.7B [62] là một mô hình mã nguồn mở từ Salesforce Research. Nó tuân theo kiến trúc mô hình autoregressive transformer tiêu chuẩn với embedding vị trí xoay.

●CodeParrot-1.5B [1] là một nỗ lực mã nguồn mở khác để huấn luyện một mô hình GPT-2 cho việc tạo code. Nó được huấn luyện trên 180GB Python Code và có 1.5B tham số.

●GPT-J-6B [87] là một mô hình mã nguồn mở từ EleutherAI. Nó áp dụng kiến trúc transformer tương tự GPT-3. Nó được huấn luyện trên 825 GB dữ liệu văn bản, bao gồm 95GB code từ GitHub.

●GPT-4 [2] là mô hình ngôn ngữ hiện đại nhất được phát triển bởi OpenAI. Vì cấu trúc nội bộ của GPT-4 không được công bố công khai, chúng tôi không bao gồm số lượng lớp và đầu của GPT-4 trong Bảng 1. Được báo cáo rằng GPT-4 có khoảng 1.76 nghìn tỷ tham số [3]. Chúng tôi đã sử dụng API (gpt-4) do OpenAI cung cấp để truy vấn GPT-4.

### 4.2 Tính toán Chú ý Mô hình

Chúng tôi đã thử nghiệm với mười hai phương pháp tính toán chú ý từ ba danh mục khác nhau: sáu phương pháp dựa trên self-attention, bốn phương pháp dựa trên gradient, và hai phương pháp dựa trên nhiễu loạn.

#### 4.2.1 Phương pháp dựa trên Self-attention. 

Cho rằng LLMs có nhiều lớp chú ý, hiện tại không có sự đồng thuận về cách đúng để tổng hợp những self-attention đó để giải thích LLMs. Zeng et al. [97] cho thấy lớp chú ý đầu tiên có tính chỉ thị về token nào mà mô hình chú ý đến, trong khi Wan et al. [85] cho thấy các lớp chú ý sâu hơn tốt hơn trong việc ghi lại các phụ thuộc khoảng cách xa và cấu trúc chương trình. Để thực hiện phân tích toàn diện, chúng tôi quyết định thử nghiệm với ba thiết lập: (1) chỉ sử dụng lớp chú ý đầu tiên (ký hiệu là first), (2) chỉ sử dụng lớp chú ý cuối cùng (ký hiệu là last), và (3) sử dụng tất cả các lớp chú ý (ký hiệu là all).

Để tổng hợp self-attention trên các đầu chú ý khác nhau trong một lớp, chúng tôi tuân theo công việc trước [99] bằng cách tổng các giá trị chú ý từ các đầu khác nhau. Cuối cùng, vì LLMs tạo code theo cách autoregressive, sự chú ý của chúng thay đổi trong mỗi bước khi chúng đọc thêm token từ đầu vào và khi chúng tạo thêm code. Chúng tôi tò mò về token đầu vào nào mà mô hình chú ý cao khi chúng đọc đầu vào và token đầu vào nào mà mô hình chú ý cao khi chúng tạo code. Vì vậy chúng tôi xem xét hai thiết lập thử nghiệm: (1) tổng hợp điểm chú ý được gán cho mỗi token trong quá trình đọc đầu vào (ký hiệu là READING), và (2) tổng hợp điểm chú ý được gán cho mỗi token trong quá trình tạo code (ký hiệu là CODING).

Cho ba thiết lập trong tổng hợp chú ý theo lớp và hai thiết lập trong tổng hợp chú ý theo bước, chúng tôi có tổng cộng sáu thiết lập thử nghiệm: READING_first, CODING_first, READING_last, CODING_last, READING_all, và CODING_all.

#### 4.2.2 Phương pháp dựa trên Gradient. 

Chúng tôi xem xét hai phương pháp khác nhau để tính toán chú ý mô hình dựa trên gradient: (1) Saliency [78], và (2) Input×Gradient [76]. Phương pháp saliency tính toán sự chú ý của mô hình bằng cách tính toán gradient mô hình đối với đầu vào. Cho một LLM ℱ, giả sử một đầu vào là X = (x₁, x₂, ..., xₙ), trong đó n là độ dài của đầu vào. Sự chú ý sᵢ trên xᵢ được tính toán là sᵢ = ∂ℱ(X)/∂xᵢ. Khác với phương pháp saliency, Input×Gradient còn nhân gradient với các giá trị embedding của đầu vào. Sự chú ý sᵢ trên xᵢ được tính toán là sᵢ = xᵢ · ∂ℱ(X)/∂xᵢ.

Tương tự như self-attention, gradient cũng thay đổi liên tục tại mỗi bước tạo. Do đó, chúng tôi cũng đã thử nghiệm với hai thiết lập tổng hợp chú ý theo bước như trong các phương pháp self-attention. Sự kết hợp của hai phương pháp tính toán gradient với hai thiết lập tổng hợp theo bước tạo ra bốn phương pháp dựa trên gradient—Input×Gradient_reading, Input×Gradient_coding, Saliency_reading, và Saliency_coding.

#### 4.2.3 Phương pháp dựa trên Nhiễu loạn. 

Chúng tôi xem xét hai phương pháp dựa trên nhiễu loạn khác nhau: (1) SHAP [57], che đầu vào thông qua việc xóa một số token, và (2) BERT Masking [90], che đầu vào thông qua việc thay thế một token bằng kết quả dự đoán mô hình ngôn ngữ che của nó từ BERT.

●SHAP. Chúng tôi sử dụng thư viện SHAP chính thức với số lượng nhiễu loạn bằng 50 để tính toán sự chú ý của mô hình (điểm SHAP) trên các token khác nhau.¹ Chúng tôi tổng hợp điểm SHAP trong việc dự đoán các token khác nhau bằng cách tổng chúng lại.

●BERT Masking. Vì code từ bài báo BERT Masking gốc [90] không có sẵn công khai, chúng tôi đã tái triển khai phương pháp này dựa trên mô tả trong bài báo. Cụ thể, cho mỗi token trong một prompt đầu vào, phương pháp này che nó và sử dụng mô hình BERT được huấn luyện trước từ HuggingFace để dự đoán token có khả năng nhất ở vị trí bị che. Sau đó, nó thay thế token gốc bằng token được dự đoán và yêu cầu LLM tạo lại giải pháp code. Phương pháp này sau đó tính toán sự chú ý của mô hình trên token này bằng cách tính toán điểm BLEU [65] giữa giải pháp code gốc và giải pháp mới. Chúng tôi lặp qua tất cả token trong các prompt đầu vào để có được sự chú ý của mô hình trên các token khác nhau.

### 4.3 Đo lường Căn chỉnh Chú ý

Chúng tôi đo lường sự căn chỉnh chú ý giữa mô hình và lập trình viên con người bằng cách sử dụng hai số liệu mạnh mẽ—số liệu chồng chéo token của San Martino [22] và alpha của Krippendorff [49]. Chúng tôi cũng đã thử nghiệm với hai số liệu đơn giản, kappa của Cohen và tỷ lệ bao phủ từ khóa, và thu được kết quả tương tự. Do giới hạn trang, chúng tôi chỉ báo cáo kết quả của hai số liệu đầu tiên trong bài báo này. Kết quả của các số liệu khác được bao gồm trong kho GitHub [47].

#### 4.3.1 Số liệu Chồng chéo Token của San Martino [22] 

tính toán sự chồng chéo giữa hai tập token về precision, recall, và điểm F-1. Ban đầu nó được thiết kế cho các nhiệm vụ gắn nhãn chuỗi trong NLP [22] và gần đây đã được áp dụng trong các nghiên cứu SE như một số liệu mạnh mẽ để phát hiện độc tính trong bình luận đánh giá code [72]. Cụ thể, nó gán điểm một phần cho sự chồng chéo một phần giữa hai tập token, điều này làm cho nó trở thành một lựa chọn phù hợp cho nhiệm vụ của chúng tôi. Do đó, chúng tôi tuân theo [22] để so sánh các từ nổi bật được chọn bởi con người và mô hình. Đối với sự chú ý của con người, một token được coi là được chú ý cao nếu nó được gắn nhãn là một từ quan trọng bởi lập trình viên con người, như được mô tả trong Phần 3. Đối với sự chú ý của mô hình, vì các phương pháp tính toán chú ý trong Phần 4.2 tính toán điểm liên tục cho mỗi token, khó xác định ngưỡng phổ quát để quyết định token nào được mô hình chú ý cao. Do đó, chúng tôi xếp hạng các token trong mô tả nhiệm vụ lập trình dựa trên điểm chú ý của chúng và chọn K token hàng đầu làm token được mô hình chú ý cao. Vì trung bình các từ quan trọng được gắn nhãn bởi con người là 7 cho mỗi mô tả nhiệm vụ, chúng tôi thử nghiệm với K = 5, 10, 20 trong nghiên cứu của chúng tôi. Cho một tập token được lập trình viên con người chú ý cao và một tập token được mô hình chú ý cao, chúng tôi tuân theo các phương trình trong [72] để tính toán precision, recall, và điểm F-1. Chúng tôi báo cáo tất cả ba điểm trong Bảng 2.

¹https://shap.readthedocs.io/en/latest/

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Hình 4. Ánh xạ từ NL đến sub-token LLM

#### 4.3.2 Alpha của Krippendorff [48] 

là một thước đo thống kê mạnh mẽ cho sự đồng thuận giữa người gắn nhãn, trong đó α = 1 cho thấy sự đồng thuận hoàn hảo, α = 0 cho thấy không có sự đồng thuận, và α = -1 cho thấy sự bất đồng hoàn toàn. Nó có thể được sử dụng để đo lường mức độ đồng thuận giữa lập trình viên con người và LLMs về các từ quan trọng trong mô tả nhiệm vụ lập trình. So với các số liệu đồng thuận giữa người đánh giá khác như kappa của Cohen [21], alpha của Krippendorff mạnh mẽ hơn đối với số lượng người mã hóa, dữ liệu thiếu, và kích thước mẫu. Để tính toán alpha của Krippendorff, nhãn của con người và mô hình phải được lưu trữ trong các vector có cùng độ dài. Điều này khó khăn vì LLMs thực hiện tokenization từ phụ để xử lý các từ ngoài từ vựng với kích thước từ vựng có thể quản lý. Ví dụ, trong Hình 4, từ "separate" được tokenize thành hai token: "separ" và "ate" thông qua mã hóa cặp byte (BPE). Trong quá trình suy luận, một LLM sẽ tính toán điểm chú ý riêng biệt cho hai subtoken này, mặc dù chúng từ cùng một từ tiếng Anh.

Để giải quyết thách thức này, chúng tôi ánh xạ các từ NL trở lại token LLM. Nếu mô hình tokenize một từ ngôn ngữ tự nhiên thành nhiều token, tất cả sub-token sẽ được coi là được chọn bởi người gắn nhãn con người. Sử dụng cùng ví dụ từ Hình 4, nếu từ ngôn ngữ tự nhiên "separate" được chọn bởi người gắn nhãn con người, cả hai sub-token "separ" và "ate" sẽ được coi là được chọn và được đại diện bởi 1 trong các vector được sử dụng để tính toán alpha của Krippendorff.

### 4.4 Thiết kế Nghiên cứu Người dùng

Để trả lời RQ4, chúng tôi đã tiến hành một nghiên cứu người dùng để đánh giá các phương pháp tính toán chú ý khác nhau.² Chúng tôi tuyển dụng 22 sinh viên (18 nam và 4 nữ) thông qua danh sách gửi thư của khoa trong một khoa CS. Theo nghiên cứu người dùng của chúng tôi, tất cả người tham gia có trung bình 5.62 năm kinh nghiệm lập trình. Tất cả người tham gia đều có hiểu biết cơ bản về sự chú ý của mô hình trong học máy.

Chúng tôi đã chọn ngẫu nhiên 8 mô tả nhiệm vụ từ bộ dữ liệu của chúng tôi. Đối với mỗi nhiệm vụ, chúng tôi đã tận dụng CodeGen-2.7B, mô hình mã nguồn mở hoạt động tốt nhất trên HumanEval trong thí nghiệm của chúng tôi, để tính toán sự chú ý của mô hình. Chúng tôi không xem xét GPT-4 trong nghiên cứu người dùng này, vì nó là mã nguồn đóng và chúng tôi không thể tính toán sự chú ý của nó bằng các phương pháp dựa trên self-attention và các phương pháp dựa trên gradient. Chúng tôi đã chọn một phương pháp tính toán chú ý từ mỗi danh mục (dựa trên self-attention, dựa trên nhiễu loạn, và dựa trên gradient): CODING_last, Input×Gradient_coding, và SHAP.

Trong mỗi nghiên cứu người dùng, người tham gia đầu tiên đọc mô tả nhiệm vụ để hiểu nhiệm vụ lập trình và sau đó đọc code được tạo bởi CodeGen. Đối với mỗi phương pháp chú ý, chúng tôi hiển thị một phiên bản được làm nổi bật của mô tả nhiệm vụ tương tự như Hình 3, trong đó 10 token được chú ý hàng đầu được làm nổi bật dựa trên điểm chú ý được tính toán bởi phương pháp này. Chúng tôi chọn hiển thị 10 từ khóa quan trọng hàng đầu vì nó gần với số lượng từ quan trọng trung bình (7) được gắn nhãn trong bộ dữ liệu của chúng tôi.

²Bảng câu hỏi nghiên cứu người dùng và phản hồi của người tham gia có sẵn trong kho GitHub của chúng tôi:
https://github.com/BonanKou/Attention-Alignment-Empirical-Study.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Bảng 2. Sự căn chỉnh chú ý giữa con người và mô hình được tính toán bằng các phương pháp khác nhau theo Precision (P), Recall (R), điểm F1 (F1) của San Martino, và điểm alpha của Krippendorff (KA). Trong số 12 phương pháp tính toán chú ý khác nhau, phương pháp tạo ra kết quả căn chỉnh nhất cho mỗi mô hình dưới mỗi thiết lập K trong mỗi số liệu được làm nổi bật màu vàng. Lưu ý rằng chúng tôi chỉ thử nghiệm với các phương pháp dựa trên nhiễu loạn trên GPT-4, vì GPT-4 không cung cấp truy cập vào các lớp self-attention và gradient của nó.

(a) Phương pháp dựa trên nhiễu loạn.

[Bảng chi tiết với các số liệu cho BERT_masking và SHAP trên các mô hình khác nhau]

(b) Phương pháp dựa trên gradient

[Bảng chi tiết với các số liệu cho Input×Gradient_reading, Input×Gradient_coding, Saliency_reading, Saliency_coding]

(c) Phương pháp dựa trên self-attention.

[Bảng chi tiết với các số liệu cho READING_first, CODING_first, READING_last, CODING_last, READING_all, CODING_all]

Người tham gia sau đó được yêu cầu đánh giá mỗi phương pháp chú ý bằng cách chỉ ra sự đồng ý của họ với ba câu sau trên thang Likert 7 điểm (1—hoàn toàn không đồng ý, 7—hoàn toàn đồng ý).

Q1 Các từ khóa được mô hình chú ý phù hợp với sự chú ý của tôi khi đọc mô tả nhiệm vụ ngôn ngữ tự nhiên.

Q2 Sự chú ý này giải thích tại sao mô hình thành công hoặc thất bại trong việc tạo code chính xác.

Q3 Tôi muốn thấy sự chú ý này khi làm việc với các mô hình tạo code trong cuộc sống thực.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Thứ tự của các phương pháp tính toán chú ý khác nhau được ngẫu nhiên hóa để giảm thiểu hiệu ứng học tập. Chúng tôi cũng không tiết lộ tên của các phương pháp tính toán chú ý để giảm thiểu thiên vị. Ở cuối nghiên cứu người dùng, người tham gia trả lời ba câu hỏi mở về các phương pháp tính toán chú ý khác nhau và thiết kế nghiên cứu người dùng. Chúng tôi hỏi những câu hỏi mở này để nghiên cứu mối tương quan giữa khả năng giải thích mô hình và sự tin tưởng của người dùng. Những câu hỏi này bao gồm:

Q4 Bạn có quan tâm đến việc biết LLM tạo code như thế nào không?

Q5 Bạn muốn tìm hiểu gì về quá trình tạo code nội bộ trong LLM?

Q6 Bạn có tin tưởng LLM này không? Bạn cần biết gì để cải thiện sự tin tưởng vào LLM?

## 5 KẾT QUẢ

### 5.1 RQ1: Sự chú ý của mô hình được căn chỉnh với sự chú ý của con người ở mức độ nào?

Để trả lời câu hỏi này, chúng tôi thu thập K từ khóa hàng đầu mà sáu LLMs chú ý đến và so sánh chúng với các từ khóa được gắn nhãn là quan trọng trong bộ dữ liệu chú ý lập trình viên. Như được mô tả trong Phần 4.3, chúng tôi sử dụng số liệu chồng chéo token của San Martino [22] và alpha của Krippendorff [49] để đo lường sự căn chỉnh chú ý giữa LLMs và lập trình viên con người. Khi chạy các mô hình mà độ dài đầu ra tối đa có thể được thiết lập, chúng tôi đặt giới hạn token bằng số token của sự thật cơ bản cộng 20 để chịu đựng sự dư thừa vừa phải.

Bảng 2a, Bảng 2b, và Bảng 2c trình bày kết quả căn chỉnh chú ý khi tính toán điểm chú ý với các phương pháp dựa trên nhiễu loạn, dựa trên gradient, và dựa trên self-attention, tương ứng. Vì GPT-4 không tiết lộ trạng thái nội bộ của nó trong thời gian chạy, chỉ có các phương pháp dựa trên nhiễu loạn có thể áp dụng. Do đó, phần này thảo luận về kết quả của phương pháp dựa trên nhiễu loạn tốt nhất, BERT_masking. Phần 5.3 so sánh các phương pháp tính toán chú ý khác nhau một cách chi tiết.

Với phương pháp BERT_masking, từ K = 5 đến K = 10, điểm F1 của tất cả mô hình tăng vì khi ngày càng nhiều token được chọn bởi các mô hình, chúng tôi quan sát recall rất cao (khoảng 90%), bù đắp cho precision giảm. Tuy nhiên, từ K = 10 đến K = 20, điểm F1 vẫn ổn định do sự giảm nhanh trong điểm precision. Mặt khác, điểm alpha của Krippendorff vẫn ổn định từ K = 5 đến K = 10 (trừ GPT-4) nhưng giảm nhanh từ K = 10 đến K = 20.

Nhìn chung, đối với tất cả mô hình và tất cả giá trị K, alpha của Krippendorff không thay đổi nhiều và vẫn dưới 0.3, và điểm F1 vẫn dưới 0.6, cho thấy ít sự đồng thuận giữa sự chú ý của mô hình và con người [58]. Trong số những mô hình này, GPT-4 cho thấy sự chồng chéo chú ý thấp nhất với sự chú ý của con người về cả hai số liệu. Một giải thích có thể là các mô hình siêu lớn như GPT-4 đã phát triển một chiến lược lý luận khác với lập trình viên con người. Những kết quả này gợi ý sự không đồng bộ chú ý nhất quán giữa LLMs và lập trình viên khi tạo code.

**Phát hiện 1**
Có sự không đồng bộ nhất quán giữa sự chú ý của LLM và lập trình viên trong tất cả thiết lập, cho thấy LLMs không lý luận các nhiệm vụ lập trình như lập trình viên con người.

### 5.2 RQ2: Sự chú ý có thể giải thích lỗi của các mô hình tạo code không?

Để trả lời câu hỏi này, hai tác giả đầu tiên đã phân tích thủ công các lỗi tạo code được thực hiện bởi hai mô hình tốt nhất trong nghiên cứu của chúng tôi—GPT-4 và CodeGen-2.7B. Tổng cộng, hai mô hình này đã tạo ra 920 giải pháp code không chính xác trên hai tiêu chuẩn. Chúng tôi đã lấy mẫu ngẫu nhiên 211 giải pháp không chính xác,

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

bao gồm 172 giải pháp không chính xác từ CodeGen-2.7B và 39 giải pháp không chính xác từ GPT-4. Kích thước mẫu có ý nghĩa thống kê với mức độ tin cậy 90% và biên sai 5%.

Hai tác giả đầu tiên bắt đầu với 50 lỗi tạo code đầu tiên và độc lập kiểm tra liệu mẫu chú ý của mỗi code được tính toán bởi BERT_masking, cho kết quả căn chỉnh nhất trong các thí nghiệm định lượng, có thể giải thích lỗi trong đó hay không. Đối với các nhiệm vụ đơn giản, mất khoảng năm phút để kiểm tra mỗi cái. Đối với các nhiệm vụ phức tạp (ví dụ, các nhiệm vụ yêu cầu hiểu biết về các khái niệm toán học cụ thể), mất khoảng 10 đến 15 phút, vì các tác giả cần debug code thủ công và kiểm tra các giá trị runtime của nó để hiểu lỗi trước. Sau khi phân tích 50 lỗi, họ đã thảo luận những lỗi này với các tác giả khác và tóm tắt sáu mẫu chú ý phổ biến có thể được sử dụng để giải thích lỗi tạo code:

●Thiếu chú ý đến các điều kiện quan trọng. Mô tả nhiệm vụ đề cập đến các điều kiện hoặc trường hợp góc cạnh nhất định để xử lý. Tuy nhiên, mô hình bỏ lỡ hoặc xử lý không đúng một hoặc nhiều điều kiện như vậy vì nó không chú ý đến các từ hoặc cụm từ mô tả các điều kiện tương ứng.

●Thiếu chú ý đến các từ mô tả quan trọng của một phép toán hoặc đối tượng dữ liệu. Mô tả nhiệm vụ đề cập đến một thuộc tính quan trọng của một phép toán hoặc đối tượng dữ liệu, như "largest element" và "ascending order". Tuy nhiên, mô hình không chú ý đến những từ mô tả này và do đó tạo ra giải pháp code với logic không chính xác.

●Thiếu chú ý đến mô tả phép toán. Mô tả nhiệm vụ đề cập đến một phép toán hoặc hành động, như open a file và sort a list. Tuy nhiên, mô hình không chú ý đến các từ hoặc cụm từ động từ. Do đó, code được tạo thực hiện hành động sai hoặc không thực hiện hành động.

●Thiếu chú ý đến kiểu dữ liệu. Các nhiệm vụ lập trình thường đề cập rõ ràng đến loại đầu vào và đầu ra mong đợi. Một số mô tả nhiệm vụ cũng đề cập đến kiểu dữ liệu của một số kết quả trung gian. Tuy nhiên, mô hình không chú ý đến một số mô tả kiểu dữ liệu. Điều này có thể dẫn đến các loại lỗi khác nhau, ví dụ, trả về kiểu dữ liệu sai, gọi phương thức trên kiểu đối tượng sai, v.v.

●Ánh xạ không chính xác giữa từ NL và các yếu tố code. Trong một số trường hợp, chúng tôi quan sát mô hình chú ý đến một từ hoặc cụm từ quan trọng một cách chính xác nhưng mô hình ánh xạ nó đến một lời gọi phương thức, biến, tham số, giá trị hoặc logic sai, có thể do một số hiểu lầm khái niệm về ý nghĩa ngữ nghĩa của các từ NL.

Sau đó, họ độc lập gắn nhãn các lỗi còn lại, thảo luận việc gắn nhãn của họ với nhau, và giải quyết các xung đột. Tổng cộng, chúng tôi thấy rằng lỗi trong 57 trong số 211 giải pháp không chính xác (27%) có thể được giải thích bằng một trong năm mẫu chú ý được đề cập ở trên. Cụ thể, 54 trong số 172 giải pháp không chính xác từ CodeGen-2.7B (31%) có thể giải thích và 3 trong số 39 giải pháp không chính xác từ GPT-4 (8%) có thể giải thích. Phát hiện này gợi ý rằng phân tích chú ý thần kinh có thể được áp dụng để định vị và sửa chữa một phần không tầm thường của lỗi trong code được tạo bởi LLM. Cho rằng chỉ có 3 lỗi được tạo bởi GPT-4 có thể được giải thích bằng các mẫu không đồng bộ chú ý, điều này ngụ ý rằng các mô hình yếu hơn như CodeGen-2.7B có nhiều khả năng bị ảnh hưởng bởi sự không đồng bộ chú ý và do đó các lỗi tạo của chúng có thể giải thích hơn. Đây là một quan sát thú vị vì GPT-4 cũng gặp phải sự không đồng bộ chú ý, như được thể hiện trong Bảng 2, nhưng các lỗi tạo của nó không thể dễ dàng được giải thích bằng phân tích chú ý. Điều này cho thấy khi các mô hình ngôn ngữ trở nên đủ lớn, chúng có thể đã phát triển một cách khác để diễn giải prompt đầu vào và tạo nội dung. Điều này đòi hỏi nghiên cứu mới để hiểu tại sao các mô hình ngôn ngữ siêu lớn như GPT-4 tạo ra code không chính xác. Bên cạnh đó, chúng tôi thừa nhận rằng nhiều lỗi không thể dễ dàng được giải thích bằng sự chú ý của mô hình. Những lỗi như vậy bao gồm lỗi cú pháp, tên biến không xác định, sử dụng API không chính xác, chỉ số mảng không chính xác, vòng lặp vô hạn, v.v. Kết quả này cho thấy cần có phân tích thêm để hiểu nguyên nhân gốc rễ của những lỗi này.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Chúng tôi cho thấy phân phối và ví dụ cho mỗi loại lỗi dưới đây. Trong những ví dụ này, chúng tôi làm nổi bật các từ khóa với điểm chú ý cao từ mô hình màu xanh lam.

#### 5.2.1 Thiếu Chú ý đến Điều kiện Quan trọng. 

Prompt này yêu cầu một hàm khớp một chuỗi với ký tự "a" theo sau bởi không hoặc nhiều ký tự "b". Tuy nhiên, CodeGen-2.7B tạo ra một hàm chỉ khớp các từ với một "a" theo sau bởi một hoặc nhiều "b". Phân tích chú ý của chúng tôi cho thấy cụm từ điều kiện "followed by zero or more" không chứa bất kỳ token được chú ý nhất nào, điều này giải thích tại sao mô hình bỏ qua điều kiện mà ký tự "a" có thể được theo sau không có "b" hoặc nhiều hơn một "b". 6 giải pháp không chính xác thuộc danh mục này.

```python
#Write a function that matches a string that has an ``a'' followed by zero or more ``b''s.
def function(string):
    return string.count('ab') >= 1
```

#### 5.2.2 Thiếu Chú ý đến Từ Mô tả Quan trọng của Phép toán hoặc Đối tượng Dữ liệu. 

Prompt dưới đây yêu cầu một hàm tính toán lũy thừa thứ n của tất cả các phần tử trong một danh sách. Tuy nhiên, code được tạo bởi CodeGen-2.7B tính toán lũy thừa thứ n của một phần tử cụ thể mà không sử dụng hàm lambda. Phân tích chú ý của chúng tôi cho thấy cụm từ thuộc tính "individual items" không chứa bất kỳ từ được chú ý nhất nào trong prompt. Điều này giải thích tại sao mô hình không thực hiện phép toán được yêu cầu trên tất cả item thay vì một item cụ thể. 13 giải pháp không chính xác thuộc danh mục này.

```python
# Write a function to find the n-th power of individual elements in a list using lambda function.
def function(list, n):
    return list[n-1] ** n
```

#### 5.2.3 Thiếu Chú ý đến Mô tả Phép toán. 

Prompt dưới đây yêu cầu một hàm đếm có bao nhiêu cặp phần tử tại cùng chỉ số trong hai mảng cho trước bằng nhau. Tuy nhiên, CodeGen-2.7B tạo ra một hàm kiểm tra liệu hai chuỗi có bằng nhau hay không. Phân tích chú ý của chúng tôi cho thấy từ khóa toán tử "count" không phải là một trong những từ được chú ý nhất trong prompt, điều này giải thích tại sao code được tạo không thực hiện đếm như được yêu cầu. 6 giải pháp không chính xác thuộc danh mục này.

```python
# Write a function to count the same pair in two given lists using map function.
def function(x,y):
    return x == y
```

#### 5.2.4 Thiếu Chú ý đến Kiểu Dữ liệu. 

Prompt này yêu cầu một hàm khớp một từ chứa "z" nhưng không có "z" ở đầu hoặc cuối. Tuy nhiên, CodeGen-2.7B tạo ra một hàm kiểm tra liệu chuỗi thu được từ việc loại bỏ ký tự đầu tiên từ từ cho trước có bằng chuỗi thu được từ việc loại bỏ ký tự cuối cùng từ từ cho trước hay không. Phân tích chú ý của chúng tôi cho thấy từ khóa kiểu dữ liệu "z" không phải là một trong những từ được chú ý nhất trong prompt. Điều này giải thích tại sao code được tạo không kiểm tra sự tồn tại của một ký tự cụ thể trong từ cho trước. 7 lỗi thuộc danh mục này.

```python
#Write a function that matches a word containing 'z', not at the start or end of the word.
def function(word):
    return word[1:] == word[:-1]
```

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

#### 5.2.5 Ánh xạ Không chính xác Giữa Từ NL và Yếu tố Code. 

Prompt này yêu cầu một hàm tính toán số bell thứ n. Tuy nhiên, thay vì gọi công thức đúng cho số bell, CodeGen-2.7B tính toán giai thừa của n. Phân tích chú ý của chúng tôi cho thấy từ khóa "bell" là một trong những từ được chú ý nhất trong prompt. Tuy nhiên, nó vẫn tạo ra không chính xác cho prompt này vì nó không hiểu ý nghĩa ngữ nghĩa của "bell number". 25 giải pháp không chính xác thuộc danh mục này.

```python
#Write a python function to find nth bell number.
def function(n):
    if n == 1:
        return 1
    else:
        return n * function(n - 1)
```

**Phát hiện 2**
Phân tích chú ý có thể giải thích 27% lỗi tạo trong tiêu chuẩn 211 lỗi. Những lỗi này cung cấp hiểu biết về các hạn chế của LLMs.

### 5.3 RQ3: Tác động của các phương pháp tính toán chú ý đối với sự căn chỉnh là gì?

Để so sánh các phương pháp tính toán chú ý khác nhau, chúng tôi đã tính toán điểm F-1 trung bình của San Martino và alpha của Krippendorff của tất cả năm mô hình (trừ GPT-4) cho tất cả K (Bảng 3). Bảng 3 cho thấy BERT_masking cho sự căn chỉnh cao nhất trong cả hai số liệu cho K = 5, 10. Tuy nhiên, phương pháp dựa trên nhiễu loạn khác, SHAP, không vượt trội hơn các phương pháp khác như BERT_masking đã làm. Mặt khác, các phương pháp dựa trên gradient nhìn chung tốt hơn các phương pháp dựa trên self-attention, đặc biệt là cho K = 5 theo cả hai số liệu. Đáng ngạc nhiên, điểm chú ý được tính toán bằng các phương pháp dựa trên self-attention (ví dụ, CODING_first) ít được căn chỉnh nhất với sự chú ý của con người, đặc biệt là cho các giá trị K nhỏ hơn. Một lý do có thể là các đơn vị tính toán khác trong kiến trúc transformer, như các lớp feed-forward, cũng đóng vai trò quan trọng trong quá trình tạo code. Ví dụ, Geva et al. thấy rằng các lớp feed-forward ảnh hưởng đến dự đoán mô hình bằng cách thúc đẩy các khái niệm trong không gian từ vựng [35]. Do đó, chỉ xem xét các lớp self-attention không hoàn toàn ghi lại ảnh hưởng của mỗi token đầu vào đối với dự đoán mô hình.

Bảng 3. Điểm F-1 trung bình của San Martino và Alpha của Krippendorff trên tất cả năm mô hình (loại trừ GPT-4) trong các thiết lập K khác nhau. Điểm cao nhất trong mỗi cột được làm nổi bật màu vàng.

[Bảng chi tiết với các điểm số cho các phương pháp khác nhau]

Hơn nữa, trong các phương pháp dựa trên gradient, chúng tôi quan sát rằng việc tính toán phân phối chú ý trong giai đoạn coding (ví dụ, Saliency_coding) cho sự căn chỉnh tốt hơn so với phân phối chú ý trong giai đoạn reading (ví dụ, Saliency_reading). Mẫu này có thể được quan sát cho tất cả giá trị K (tức là, K = 5, 10). Tuy nhiên, sự khác biệt giữa Input×Gradient và Saliency là tầm thường.

Cuối cùng, như đã thảo luận trong Phần 2.3.1, không có sự đồng thuận về cách tổng hợp điểm self-attention [10,97,99]. Chúng tôi đã thử nghiệm với sáu phương pháp tổng hợp self-attention khác nhau được sử dụng trong công việc trước để tìm phương pháp tổng hợp self-attention tốt nhất. Đầu tiên, trong ba tùy chọn—(1) chỉ tổng điểm chú ý trong

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

Hình 5. Lựa chọn của người tham gia về các phương pháp tính toán chú ý khác nhau trong ba chiều.

lớp đầu tiên, (2) chỉ tổng điểm chú ý trong lớp cuối cùng, và (3) tổng điểm chú ý từ tất cả các lớp—chỉ tổng điểm chú ý trong lớp cuối cùng tạo ra các phân phối chú ý được căn chỉnh nhất với các phân phối chú ý của con người trong cả giai đoạn reading và coding, trừ khi K được đặt thành 5 trong giai đoạn coding. Điều này gợi ý rằng nghiên cứu tương lai nên xem xét lớp cuối cùng khi tận dụng self-attention để diễn giải các mô hình ngôn ngữ code. Tương tự như phát hiện trong các phương pháp dựa trên gradient, đối với các phương pháp dựa trên self-attention, các phân phối chú ý tại giai đoạn coding (ví dụ, CODING_first) được căn chỉnh nhất quán hơn với sự chú ý của con người so với những phân phối tại giai đoạn reading (ví dụ, READING_first).

**Phát hiện 3**
BERT_masking tạo ra sự căn chỉnh chú ý tốt nhất với lập trình viên con người trong tất cả các phương pháp. Các phương pháp dựa trên gradient nhìn chung tốt hơn các phương pháp dựa trên self-attention. Đối với các phương pháp dựa trên gradient và dựa trên self-attention, các phân phối chú ý trong giai đoạn coding tạo ra sự căn chỉnh cao hơn. Đối với các phương pháp dựa trên self-attention, các phân phối chú ý trong lớp cuối cùng tạo ra sự căn chỉnh cao nhất.

### 5.4 RQ4: Phương pháp tính toán chú ý nào được ưa chuộng nhất?

Hình 5 cho thấy đánh giá của người tham gia về SHAP, Input×Gradient_coding, và CODING_all về sự căn chỉnh với sự chú ý của họ, khả năng giải thích của sự chú ý được tính toán, và sự ưa chuộng của họ (Q1-Q3 trong Phần 4.4). Nhìn chung, phương pháp dựa trên nhiễu loạn, SHAP, được ưa chuộng hơn hai phương pháp khác trong tất cả ba khía cạnh. Điểm đánh giá trung bình về sự căn chỉnh chú ý của SHAP là 5.13, trong khi trung bình cho InputXGradient_coding và CODING_all lần lượt là 4.59 và 3.62.

Hơn nữa, người tham gia gợi ý rằng SHAP có khả năng giải thích tốt hơn InputXGradient_coding (khác biệt trung bình: 0.59, t-test của Welch, p = 0.02) và CODING_all (khác biệt trung bình: 1.26, t-test của Welch, p = 0.00001). Tuy nhiên, theo phản hồi của người tham gia về sự tin tưởng của họ vào LLMs (Q6 trong Phần 4.4), 14 trong số 22 người tham gia bày tỏ thiếu tin tưởng và niềm tin, ngay cả sau khi thấy các giải thích dựa trên chú ý. Ví dụ, P3 viết, "Tôi muốn biết liệu mô hình LLM có thể cung cấp lý do tại sao họ tạo code hay không. Ví dụ, code tham chiếu." Người tham gia cũng yêu cầu phân tích chú ý chi tiết hơn. Cụ thể, họ muốn thấy phần nào của đầu vào chịu trách nhiệm tạo ra phần nào của đầu ra. Ví dụ, P10 nói, "[Tôi muốn biết] LLM xác định đầu vào và đầu ra như thế nào, các từ phân biệt nào thúc đẩy các thế hệ khác nhau." Người tham gia cũng cho thấy sự ưa chuộng nhiều hơn đối với SHAP so với

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

hai phương pháp khác. Sự khác biệt ưa chuộng trung bình giữa SHAP và InputXGradient_coding và giữa SHAP và CODING_all là 5.04 so với 4.56 (t-test của Welch, p = 0.05) và 5.04 so với 3.48 (t-test của Welch, p = 0.00009).

**Phát hiện 4**
Nhìn chung, người tham gia ưa chuộng phương pháp dựa trên nhiễu loạn hơn các phương pháp dựa trên gradient và dựa trên self-attention. Tuy nhiên, người tham gia vẫn cảm thấy thiếu tin tưởng vào LLMs sau khi thấy các giải thích dựa trên chú ý và muốn thấy các giải thích phong phú hơn như code tham chiếu và ánh xạ chú ý chi tiết giữa văn bản và code.

## 6 HÀM Ý VÀ CƠ HỘI

Nghiên cứu của chúng tôi có một số hàm ý quan trọng có lợi cho việc phát triển các LLMs đáng tin cậy và chính xác hơn cho việc tạo code trong tương lai.

Trong RQ1, chúng tôi đã khám phá sự không đồng bộ nhất quán giữa sự chú ý của con người và mô hình trong tất cả sáu mô hình. Mặc dù có thể LLMs sử dụng một phương pháp hoàn toàn khác để lý luận về mô tả nhiệm vụ so với lập trình viên con người, có thể tranh luận liệu phương pháp như vậy có thực sự tốt cho lập trình viên hay không do lo ngại về khả năng diễn giải, độ mạnh mẽ và sự tin tưởng. Nhiều nghiên cứu đã cho thấy các mô hình phù hợp với con người được coi là đáng tin cậy hơn bởi con người [8,34,38,42,46,80]. Hơn nữa, trong thực tế, LLMs ngày nay có thể giải quyết đáng tin cậy một số lượng hạn chế các nhiệm vụ lập trình đơn giản và gặp khó khăn để xử lý các nhiệm vụ phức tạp hoặc tùy chỉnh hơn, điều này cho thấy một không gian lớn để cải thiện. Do đó, chúng tôi tin rằng việc điều tra các mẫu chú ý của LLMs là một nỗ lực đáng giá để giúp chúng ta hiểu cách LLMs tạo code và tại sao LLMs mắc một số lỗi và cũng thông báo các cơ hội mới để cải thiện LLMs.

Trong RQ2, chúng tôi đã phân tích thủ công sự chú ý của 211 thế hệ không chính xác của hai mô hình tốt nhất trong bài báo của chúng tôi (GPT-4 và CodeGen-2.7B) và thấy 27% lỗi có thể được giải thích bằng sự căn chỉnh chú ý không chính xác. Phát hiện của chúng tôi gợi ý tiềm năng sửa chữa những lỗi tạo này bằng cách điều chỉnh sự căn chỉnh chú ý. Tương tự, công việc trước đây trong Thị giác Máy tính đã cho thấy hiệu suất của các mô hình thần kinh có thể được cải thiện nếu chúng ta buộc sự chú ý của chúng phù hợp với con người [31,44,59,63]. Một giải pháp tiềm năng là mở rộng hàm mất mát của LLMs với một thuật ngữ phạt đo lường sự phân kỳ KL giữa sự chú ý của mô hình và con người. Trong quá trình huấn luyện, mất mát sẽ tăng khi sự chú ý của mô hình lệch khỏi sự chú ý của con người. Kết quả là, LLM sẽ được huấn luyện để phân phối chú ý theo cách tương tự như lập trình viên con người. Chúng tôi đã mã nguồn mở bộ dữ liệu chú ý con người của chúng tôi để tạo điều kiện cho công việc tương lai về căn chỉnh chú ý.

Hơn nữa, các mẫu chú ý mô hình trong RQ3 có thể giúp cải thiện độ mạnh mẽ của các mô hình tạo code bằng cách phát triển các phương pháp huấn luyện đối kháng dựa trên chú ý mới. Hầu hết các phương pháp huấn luyện đối kháng chỉ áp dụng các nhiễu loạn nhỏ cho các token ngẫu nhiên trong một prompt, mà không xem xét tầm quan trọng của một token đối với mô hình [11,79,92,98]. Kết quả của chúng tôi cho thấy rằng đáng để ưu tiên các token quan trọng trong quá trình nhiễu loạn. Việc nhiễu loạn những từ quan trọng này và yêu cầu mô hình tạo code cho những prompt bị nhiễu loạn này có thể tiết lộ các vấn đề về độ mạnh mẽ hiệu quả hơn.

Trong RQ3, chúng tôi đã so sánh 12 phương pháp tính toán chú ý với các thí nghiệm định lượng (Bảng 2). Do đó, nghiên cứu của chúng tôi cung cấp hướng dẫn thực tế để chọn phương pháp tính toán chú ý cho nghiên cứu tương lai về các mô hình tạo code dựa trên LLM. Kết quả thí nghiệm của chúng tôi cho thấy các nhà phát triển muốn đo lường sự chú ý của các mô hình tạo code nên trước tiên xem xét BERT_masking vì nó nhất quán đạt được sự căn chỉnh tốt nhất với sự chú ý của con người trong tất cả trừ hai thiết lập (Bảng 3). Giữa các phương pháp dựa trên self-attention và dựa trên gradient, các nhà nghiên cứu nên ưu tiên

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

các phương pháp dựa trên gradient mà nhìn chung cho sự căn chỉnh tốt hơn và ổn định hơn trong hầu hết các thiết lập (Bảng 3). Đối với các phương pháp dựa trên gradient và dựa trên self-attention, việc tính toán phân phối chú ý của giai đoạn coding cho sự căn chỉnh tốt hơn (Bảng 3). Cuối cùng, khi tận dụng self-attention để diễn giải các mô hình tạo code dựa trên transformer, các nhà nghiên cứu nên xem xét sử dụng điểm self-attention được tính toán từ lớp cuối cùng, được chứng minh là phù hợp hơn với sự chú ý của con người (Bảng 3).

Trong RQ4, chúng tôi đã tiến hành nghiên cứu người dùng để so sánh nhận thức của nhà phát triển về các phương pháp giải thích khác nhau. Hầu hết người tham gia coi SHAP là phương pháp XAI tốt nhất cho các mô hình tạo code LLM. Phát hiện này gợi ý rằng các nhà nghiên cứu tương lai có thể muốn sử dụng phương pháp dựa trên nhiễu loạn làm phương pháp XAI mặc định cho việc tạo code dựa trên LLM. Hơn nữa, trong khảo sát sau nghiên cứu, người tham gia cũng yêu cầu phân tích chú ý chi tiết hơn, như tiết lộ mối liên quan giữa các token đầu vào và đầu ra riêng lẻ. Điều này nhấn mạnh nhu cầu về các phương pháp XAI mới để diễn giải các mô hình tạo code dựa trên LLM.

Trong tương lai, chúng tôi cũng muốn khám phá cách dạy con người cách LLMs diễn giải code. Hiểu cách LLMs diễn giải code có thể truyền cảm hứng cho lập trình viên con người tạo ra các prompt dễ hiểu hơn bởi LLMs và do đó hướng dẫn LLMs tạo ra code tốt hơn.

## 7 HIỂM HỌA ĐỐI VỚI TÍNH HỢP LỆ

Tính hợp lệ nội bộ. Một hiểm họa tiềm ẩn nằm trong quá trình gắn nhãn thủ công. Hai lập trình viên đã gắn nhãn thủ công các từ quan trọng để hiểu mô tả nhiệm vụ và triển khai hàm chính xác. Vì tiêu chí từ khóa này rất chủ quan, các từ được chọn bởi hai người gắn nhãn có thể không đại diện cho lựa chọn của một nhóm lớn lập trình viên. Để giảm thiểu hiểm họa này, các tác giả đã thiết lập tiêu chuẩn gắn nhãn thông qua thảo luận và đạt được sự đồng thuận đáng kể về việc gắn nhãn. Hơn nữa, chúng tôi mời một người gắn nhãn thứ ba để xác thực bộ dữ liệu được chú thích của chúng tôi bằng cách độc lập gắn nhãn 164 prompt từ bộ dữ liệu HumanEval. Chúng tôi đã tính toán điểm Fleiss' Kappa giữa các nhãn của ba người gắn nhãn và kết quả (0.64) cho thấy sự đồng thuận đáng kể.

Tính hợp lệ ngoại vi. Một hiểm họa tiềm ẩn đối với tính hợp lệ ngoại vi là chúng tôi chỉ thử nghiệm với một ngôn ngữ lập trình, Python. Chúng tôi không thể đảm bảo rằng các phát hiện của chúng tôi tổng quát hóa sang ngôn ngữ khác.

Tính hợp lệ cấu trúc. Một hiểm họa tiềm ẩn đối với tính hợp lệ cấu trúc nằm trong thiết kế khảo sát. Như chúng tôi biết, các vấn đề lập trình đòi hỏi về mặt tinh thần để giải quyết và lập trình viên có thể có quan điểm khác nhau về phần nào của prompt mà mô hình nên chú ý đến. Tuy nhiên, trong thiết kế hiện tại, chỉ có 22 người tham gia được tham gia. Kích thước mẫu nhỏ có thể dẫn đến các phát hiện không thể tổng quát hóa. Để chống lại hiểm họa này, chúng tôi chỉ mời những người tham gia có kinh nghiệm trong lập trình Python và đã sử dụng LLMs tạo code ít nhất một lần để kiểm soát chất lượng của nghiên cứu người dùng.

## 8 CÔNG VIỆC LIÊN QUAN

### 8.1 Tạo Code từ Ngôn ngữ Tự nhiên

Kể từ CodeBERT [28], đã có một lượng lớn tài liệu trong đó Các Mô hình Ngôn ngữ Lớn (LLMs) được sử dụng trong việc tạo code [5,27,29,32,37,62,66,74,82,86,88,91,96]. Cả Guo et al. [37] và Zeng et al. [96] đều sử dụng mô hình BERT [25] được huấn luyện trước để mã hóa các câu hỏi NL và lược đồ cơ sở dữ liệu cho việc tạo text-to-SQL. CodeBERT áp dụng cùng kiến trúc mô hình như BERT nhưng được huấn luyện với mục tiêu lai trên dữ liệu code và văn bản từ các kho GitHub [28]. Codex, CodeGPT, và GraphCodeBERT cải thiện CodeBERT bằng cách tận dụng luồng dữ liệu trong giai đoạn huấn luyện trước [36]. Gần đây, các LLMs hiện đại như GPT-4 và Google Bard xuất sắc trong các nhiệm vụ tạo code trên các tiêu chuẩn khác nhau [14,24]. Các đoạn code chính xác và phong phú về ngữ cảnh được tạo bởi những mô hình này cho phép tự động hóa các nhiệm vụ lập trình khác nhau.

Ngoài việc phát triển LLMs mới cho code, công việc trước đây cũng đã trình bày các phương pháp mới để nâng cao LLMs cho việc tạo code chính xác và mạnh mẽ hơn [15,16,51,66,74,94,100]. Thay vì trực tiếp tạo code từ văn bản, REDCODER [66] đầu tiên truy xuất các đoạn code tương tự từ một kho và sau đó chuyển chúng cùng với mô tả văn bản đến một LLM để tạo code. Shen et al. [74] đề xuất tận dụng kiến thức lĩnh vực từ tài liệu và bình luận code để hỗ trợ việc tạo code. Chakraborty et al. đề xuất một nhiệm vụ huấn luyện trước mới gọi là naturalizing của code nguồn (tức là dịch code được tạo nhân tạo thành dạng được viết bởi con người) để giúp LLM học cách tạo code tự nhiên [15]. Chen et al. đề xuất sử dụng LLM để tự động tạo test case để kiểm tra code được tạo bởi cùng LLM [16]. Zan et al. đề xuất đầu tiên phân tách LLM thành hai LLMs, một để tạo bản phác thảo chương trình và cái khác để điền vào bản phác thảo [94]. SkCoder [51] được thiết kế để bắt chước hành vi tái sử dụng code của nhà phát triển bằng cách đầu tiên truy xuất một ví dụ code liên quan đến một nhiệm vụ cho trước, trích xuất bản phác thảo từ nó, và chỉnh sửa bản phác thảo dựa trên mô tả nhiệm vụ.

### 8.2 Nghiên cứu Thực nghiệm về Tạo Code

Gần đây, nhiều nghiên cứu đã đánh giá các mô hình tạo code dựa trên LLM trong các khía cạnh khác nhau, bao gồm hiệu suất [27,39,55,71,97], độ mạnh mẽ [56,103], bảo mật [6,67,93], mùi code [77], khả năng sử dụng [9,12,83,91], và cấp phép [19]. Những nghiên cứu liên quan nhất đến chúng tôi là những nghiên cứu điều tra khả năng giải thích của LLMs cho code [45,53,85,99]. Karmakar et al. nghiên cứu những gì các mô hình dựa trên BERT được huấn luyện trước như CodeBERT và GraphCodeBERT đã học về code bằng cách sử dụng các nhiệm vụ thăm dò [45]. Một nhiệm vụ thăm dò về cơ bản là một nhiệm vụ dự đoán trên một thuộc tính code cụ thể, như độ dài code và độ phức tạp cyclomatic, để kiểm tra liệu mô hình có nhạy cảm với thuộc tính đó hay không. So với công việc của chúng tôi, họ không phân tích quá trình tạo code, ví dụ, tại sao và làm thế nào code nhất định được tạo dựa trên mô tả văn bản. Liguori et al. đề xuất một phương pháp dựa trên nhiễu loạn để đánh giá các mô hình NMT cho việc tạo code [53]. Ngoài ra, Zhang et al. điều tra quá trình tạo code bằng cách phân tích các lớp self-attention trong CodeBERT và GraphCodeBERT [99]. Wan et al. đã thực hiện phân tích self-attention tương tự và cũng thiết kế một nhiệm vụ thăm dò mới trên cấu trúc code để phân tích liệu LLMs có học được thông tin về cấu trúc code hay không [85]. So với những nghiên cứu này, công việc của chúng tôi khác biệt bằng cách phân tích sự nhất quán giữa sự chú ý của LLMs và sự chú ý của lập trình viên trên mô tả NL của một nhiệm vụ lập trình.

### 8.3 Phân tích Chú ý Mô hình trong Các Lĩnh vực Khác

Nhiều phương pháp tính toán chú ý đã được phát triển để giải thích các mô hình trong các lĩnh vực khác. Ví dụ, trong lĩnh vực CV, Selvaraju et al. [73] đề xuất sử dụng gradient của mạng thần kinh tích chập (CNN) để chỉ ra tầm quan trọng của mỗi pixel trong hình ảnh cho một dự đoán cụ thể. Tương tự, Zhou et al. [102] sử dụng cơ chế global average pooling để tính toán tầm quan trọng của mỗi vùng để CNN phân loại hình ảnh một cách chính xác. Lái xe tự động là một lĩnh vực khác mà nhu cầu về khả năng giải thích rất mạnh. Ví dụ, Zeiler et al. [95] sử dụng các lớp deconvolution để hiểu cách xe tự hành ghi lại các phân đoạn hình ảnh thời gian thực bằng CNN. Trong một công việc khác, Chen et al. [17] đề xuất một phương pháp học chính sách hiệu quả dữ liệu gọi là Semantic Predictive Control (SPC) giải thích cách các trạng thái môi trường được nhận thức được ánh xạ đến các hành động.

## 9 KẾT LUẬN

Bài báo này trình bày một nghiên cứu thực nghiệm về sự căn chỉnh chú ý giữa các mô hình tạo code dựa trên LLM và lập trình viên con người. Kết quả của chúng tôi tiết lộ rằng có sự không đồng bộ nhất quán giữa sự chú ý của LLMs và lập trình viên. Trong số 12 phương pháp tính toán chú ý, các phương pháp dựa trên nhiễu loạn tạo ra điểm chú ý được căn chỉnh tốt hơn với sự chú ý của con người và cũng được người tham gia nghiên cứu người dùng ưa chuộng hơn. Dựa trên kết quả nghiên cứu của chúng tôi, chúng tôi tiếp tục thảo luận một số hàm ý và cơ hội nghiên cứu tương lai để diễn giải và cải thiện hiệu suất tốt hơn của các mô hình tạo code dựa trên LLM.

## 10 TÍNH CÓ SẴN CỦA DỮ LIỆU

Code và dữ liệu của chúng tôi có sẵn trên kho GitHub [47].

## Tài liệu tham khảo

[1] 2022. CodeParrot. https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot.
[2] 2023. ChatGPT. http://chat.openai.com.
[3] 2023. GPT-4 Parameters: Unlimited guide NLP's Game-Changer. https://medium.com/@mlubbad/the-ultimate-guide-to-gpt-4-parameters-everything-you-need-to-know-about-nlps-game-changer-109b8767855a.
[4] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2655–2668.
[5] Alex Andonian, Quentin Anthony, et al. 2021. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch. https://doi.org/10.5281/zenodo.5879544
[6] Owura Asare, Meiyappan Nagappan, and N Asokan. 2022. Is github's copilot as bad as humans at introducing vulnerabilities in code? arXiv preprint arXiv:2204.04741 (2022).
[7] Jacob Austin, Augustus Odena, et al. 2021. Program Synthesis with Large Language Models. arXiv preprint arXiv:2108.07732 (2021).
[8] Aakash Bansal, Bonita Sharif, and Collin McMillan. 2023. Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization. Proceedings of the ACM on Human-Computer Interaction 7, ETRA (2023), 1–19.
[9] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with code-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85–111.
[10] Joshua Bensemann et al. 2022. Eye gaze and self-attention: How humans and transformers attend words in sentences. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics. 75–87.
[11] Pavol Bielik and Martin Vechev. 2020. Adversarial robustness for code. In International Conference on Machine Learning. PMLR, 896–907.
[12] Christian Bird et al. 2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools. Queue 20, 6 (2022), 35–57.
[13] Angie Boggust, Benjamin Hoover, Arvind Satyanarayan, and Hendrik Strobelt. 2022. Shared interest: Measuring human-ai alignment to identify recurring patterns in model behavior. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 1–17.
[14] Sébastien Bubeck et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).
[15] Saikat Chakraborty et al. 2022. NatGen: generative pre-training by "naturalizing" source code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 18–30.
[16] Bei Chen et al. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).
[17] Jianyu Chen, Shengbo Eben Li, and Masayoshi Tomizuka. 2021. Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems 23, 6 (2021), 5068–5078.
[18] Wenhu Chen, Evgeny Matusov, Shahram Khadivi, and Jan-Thorsten Peter. 2016. Guided alignment training for topic-aware neural machine translation. arXiv preprint arXiv:1607.01628 (2016).
[19] Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota. 2022. To what extent do deep learning-based code recommenders generate predictions by cloning code from the training set?. In Proceedings of the 19th International Conference on Mining Software Repositories. 167–178.
[20] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What Does BERT Look at? An Analysis of BERT's Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 276–286.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

[21] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement 20, 1 (1960), 37–46.
[22] Giovanni Da San Martino et al. 2019. Fine-grained analysis of propaganda in news article. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). Association for Computational Linguistics, 5636–5646.
[23] Misha Denil, Alban Demiraj, and Nando De Freitas. 2014. Extraction of salient sentences from labelled documents. arXiv preprint arXiv:1412.6815 (2014).
[24] Giuseppe Destefanis, Silvia Bartolucci, and Marco Ortu. 2023. A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5 and Bard AI Models for Java Functions. arXiv preprint arXiv:2305.09402 (2023).
[25] Jacob Devlin et al. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 4171–4186.
[26] Ahmed Elnaggar et al. 2021. CodeTrans: Towards Cracking the Language of Silicon's Code Through Self-Supervised Deep Learning and High Performance Computing. arXiv preprint arXiv:2104.02443 (2021).
[27] Mark Chen et al. 2021. Evaluating Large Language Models Trained on Code. (2021). arXiv:2107.03374 [cs.LG]
[28] Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of table consolidation and transformation tasks from examples. ACM SIGPLAN Notices 52, 6 (2017), 422–436.
[29] Zhangyin Feng et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Language Processing. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 307–316.
[30] Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin 76, 5 (1971), 378.
[31] Ruth C Fong, Walter J Scheirer, and David D Cox. 2018. Using human brain activity to guide machine learning. Scientific reports 8, 1 (2018), 5397.
[32] Daniel Fried et al. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=hQwb-lbM6EL
[33] Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020. Attention in natural language processing. IEEE transactions on neural networks and learning systems 32, 10 (2020), 4291–4308.
[34] Yuyang Gao et al. 2022. Aligning eyes between humans and deep neural network through interactive attention alignment. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2 (2022), 1–28.
[35] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680 (2022).
[36] Daya Guo et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020).
[37] Jiaqi Guo et al. 2019. Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 4524–4535.
[38] Christopher Hazard et al. 2022. Importance is in your attention: agent importance prediction for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2532–2535.
[39] Dan Hendrycks et al. 2021. Measuring Coding Challenge Competence With APPS. NeurIPS (2021).
[40] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning AI with Shared Human Values. arXiv preprint arXiv:2008.02275 (2020).
[41] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2018. Evaluating feature importance estimates. (2018).
[42] Siteng Huang, Min Zhang, Yachen Kang, and Donglin Wang. 2021. Attributes-guided and pure-visual attention alignment for few-shot recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 7840–7847.
[43] Paras Jain and Ajay Jain. 2021. Contrastive Code Representation Learning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.
[44] Shaohua Jia et al. 2018. Biometric recognition through eye movements using a recurrent neural network. In 2018 IEEE International Conference on Big Knowledge (ICBK). IEEE, 57–64.
[45] Anjan Karmakar and Romain Robbes. 2021. What do pre-trained code models know about code?. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1332–1336.
[46] Iuliia Kotseruba, Amir Rasouli, and John K Tsotsos. 2016. Joint attention in autonomous driving (JAAD). arXiv preprint arXiv:1609.04741 (2016).
[47] Bonan Kou. 2024. Attention-Alignment-Empirical-Study. https://github.com/BonanKou/Attention-Alignment-Empirical-Study.
[48] Klaus Krippendorff. 2004. Reliability in content analysis: Some common misconceptions and recommendations. Human communication research 30, 3 (2004), 411–433.
[49] Klaus Krippendorff. 2018. Content analysis: An introduction to its methodology. Sage publications.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

[50] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. 2019. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems 32 (2019).
[51] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. 2023. SkCoder: A Sketch-based Approach for Automatic Code Generation. arXiv preprint arXiv:2302.06144 (2023).
[52] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220 (2016).
[53] Pietro Liguori et al. 2022. Can NMT understand me? towards perturbation-based evaluation of NMT models for code generation. In 2022 IEEE/ACM 1st International Workshop on Natural Language-Based Software Engineering (NLBSE). IEEE, 59–66.
[54] Shusen Liu et al. 2018. Nlize: A perturbation-driven visual interrogation tool for analyzing and interpreting natural language inference models. IEEE transactions on visualization and computer graphics 25, 1 (2018), 651–660.
[55] Xiaodong Liu, Ying Xia, and David Lo. 2020. An Empirical Study on the Usage of Transformer Models for Code Completion. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 408–418.
[56] Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, and Li Li. 2023. On the Reliability and Explainability of Automated Code Generation Approaches. arXiv preprint arXiv:2302.09587 (2023).
[57] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural information processing systems 30 (2017).
[58] Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica 22, 3 (2012), 276–282.
[59] Cristina Melício et al. 2018. Object detection and localization with artificial foveal visual attention. In 2018 Joint IEEE 8th international conference on development and learning and epigenetic robotics (ICDL-EpiRob). IEEE, 101–106.
[60] Christoph Molnar. 2020. Interpretable machine learning. Lulu.com.
[61] Ernst Niebur. 2007. Saliency map. Scholarpedia 2, 8 (2007), 2675.
[62] Erik Nijkamp, Bo Pang, et al. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh International Conference on Learning Representations.
[63] Afonso Nunes, Rui Figueiredo, and Plinio Moreno. 2020. Learning to search for objects in images from human gaze sequences. In Image Analysis and Recognition: 17th International Conference. Springer, 280–292.
[64] Matteo Paltenghi and Michael Pradel. 2021. Thinking like a developer? comparing the attention of humans with neural models of code. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 867–879.
[65] Kishore Papineni et al. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318.
[66] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented Code Generation and Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021. 2719–2734.
[67] Hammond Pearce et al. 2022. Asleep at the keyboard? assessing the security of github copilot's code contributions. In 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 754–768.
[68] Md Rafiqul Islam Rabin, Vincent J Hellendoorn, and Mohammad Amin Alipour. 2021. Understanding neural code intelligence through program simplification. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 441–452.
[69] Shuo Ren et al. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020).
[70] Marco Tulio Ribeiro et al. 2016. "Why should i trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135–1144.
[71] Rafael R Rodrigues et al. 2021. Studying the usage of text-to-text transfer transformer to support code-related tasks. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 327–336.
[72] Jaydeb Sarker, Sayma Sultana, Steven R Wilson, and Amiangshu Bosu. 2023. ToxiSpanSE: An Explainable Toxicity Detection in Code Review Comments. In 2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM). IEEE, 1–12.
[73] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision. 618–626.
[74] Sijie Shen, Xiang Zhu, Yihong Dong, Qizhi Guo, Yankun Zhen, and Ge Li. 2022. Incorporating domain knowledge through task augmentation for front-end JavaScript code generation. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1533–1543.
[75] Donghee Shin. 2021. The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. International Journal of Human-Computer Studies 146 (2021), 102551.

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

[76] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. In International conference on machine learning. PMLR, 3145–3153.
[77] Mohammed Latif Siddiq et al. 2022. An Empirical Study of Code Smells in Transformer-based Code Generation Techniques. In 2022 IEEE 22nd International Working Conference on Source Code Analysis and Manipulation (SCAM). IEEE, 71–82.
[78] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).
[79] Shashank Srikant et al. 2020. Generating Adversarial Computer Programs using Optimized Obfuscations. In International Conference on Learning Representations.
[80] Andrea Stocco et al. 2022. Thirdeye: Attention maps for safe autonomous driving systems. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. 1–12.
[81] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International conference on machine learning. PMLR, 3319–3328.
[82] Lewis Tunstall, Leandro von Werra, and Thomas Wolf. 2022. Natural Language Processing with Transformers: Building Language Applications with Hugging Face. O'Reilly Media, Incorporated.
[83] Priyan Vaithilingam et al. 2022. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In CHI conference on human factors in computing systems extended abstracts. 1–7.
[84] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention interpretability across nlp tasks. arXiv preprint arXiv:1909.11218 (2019).
[85] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022. What do they capture? a structural analysis of pre-trained language models for source code. In Proceedings of the 44th International Conference on Software Engineering. 2377–2388.
[86] Bailin Wang et al. 2020. RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7567–7578.
[87] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.
[88] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 8696–8708.
[89] Katharina Weitz et al. 2019. "Do you trust me?" Increasing user-trust by integrating virtual agents in explainable AI interaction design. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents. 7–9.
[90] Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020. Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4166–4176.
[91] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. 1–10.
[92] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of code. Proceedings of the ACM on Programming Languages 4, OOPSLA (2020), 1–30.
[93] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot's code generation. In Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering. 62–71.
[94] Daoguang Zan, Bei Chen, et al. 2022. CERT: Continual Pre-training on Sketches for Library-oriented Code Generation. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, Luc De Raedt (Ed.). ijcai.org, 2369–2375. https://doi.org/10.24963/ijcai.2022/329
[95] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer, 818–833.
[96] Yu Zeng et al. 2020. RECPARSER: A Recursive Semantic Parsing Framework for Text-to-SQL Task.. In IJCAI. 3644–3650.
[97] Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang, and Lingming Zhang. 2022. An extensive study on pre-trained models for program understanding and generation. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. 39–51.
[98] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. 2020. Generating adversarial examples for holding robustness of source code processing models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1169–1176.
[99] Kechi Zhang, Ge Li, and Zhi Jin. 2022. What does Transformer learn about source code? arXiv preprint arXiv:2207.08466 (2022).

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

[100] Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. 2022. Diet code is healthy: Simplifying programs for pre-trained models of code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1073–1084.
[101] Haiying Zhao, Wei Zhou, Xiaogang Hou, and Hui Zhu. 2020. Double attention for multi-label image classification. IEEE Access 8 (2020), 225539–225550.
[102] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2921–2929.
[103] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023. On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 1090–1102.

Received 2023-09-28; accepted 2024-04-16

Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.
