# 2009.14794.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2009.14794.pdf
# Kích thước tệp: 5880177 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
TÍNH TOÁN LẠI ATTENTION VỚI PERFORMERS
Krzysztof Choromanski1, Valerii Likhosherstov2, David Dohan1, Xingyou Song1
Andreea Gane1, Tamas Sarlos1, Peter Hawkins1, Jared Davis3, Afroz Mohiuddin1
Lukasz Kaiser1, David Belanger1, Lucy Colwell1;2, Adrian Weller2;4
1Google2University of Cambridge3DeepMind4Alan Turing Institute

TÓM TẮT
Chúng tôi giới thiệu Performers, kiến trúc Transformer có thể ước lượng các Transformer attention full-rank thông thường (softmax) với độ chính xác có thể chứng minh, nhưng chỉ sử dụng độ phức tạp không gian và thời gian tuyến tính (thay vì bậc hai), không dựa vào bất kỳ giả định nào như sparsity hoặc low-rankness. Để xấp xỉ softmax attention-kernels, Performers sử dụng phương pháp Fast Attention Via positive Orthogonal Random features mới (FAVOR+), có thể có ý nghĩa độc lập cho các phương pháp kernel có thể mở rộng. FAVOR+ cũng có thể được sử dụng để mô hình hóa hiệu quả các cơ chế attention kernelizable ngoài softmax. Khả năng biểu diễn này rất quan trọng để so sánh chính xác softmax với các kernel khác lần đầu tiên trên các tác vụ quy mô lớn, vượt ra ngoài khả năng của các Transformer thông thường, và điều tra các attention-kernel tối ưu. Performers là kiến trúc tuyến tính hoàn toàn tương thích với Transformer thông thường và có đảm bảo lý thuyết mạnh: ước lượng không thiên lệch hoặc gần như không thiên lệch của ma trận attention, hội tụ đồng nhất và phương sai ước lượng thấp. Chúng tôi đã thử nghiệm Performers trên một tập hợp tác vụ phong phú từ dự đoán pixel đến mô hình văn bản và mô hình hóa chuỗi protein. Chúng tôi chứng minh kết quả cạnh tranh với các phương pháp attention sparse và dense hiệu quả khác được kiểm tra, thể hiện hiệu quả của mô hình học attention mới được tận dụng bởi Performers.

1 GIỚI THIỆU VÀ CÔNG TRÌNH LIÊN QUAN
Transformers (Vaswani et al., 2017; Dehghani et al., 2019) là kiến trúc mạng nơ-ron mạnh mẽ đã trở thành SOTA trong một số lĩnh vực học máy bao gồm xử lý ngôn ngữ tự nhiên (NLP) (ví dụ: nhận dạng giọng nói (Luo et al., 2020)), dịch máy nơ-ron (NMT) (Chen et al., 2018), tạo/tóm tắt tài liệu, dự đoán chuỗi thời gian, mô hình tạo sinh (ví dụ: tạo ảnh (Parmar et al., 2018)), tạo nhạc (Huang et al., 2019), và tin sinh học (Rives et al., 2019; Madani et al., 2020; Ingraham et al., 2019; Elnaggar et al., 2019; Du et al., 2020). Transformers dựa vào cơ chế attention có thể huấn luyện để xác định các phụ thuộc phức tạp giữa các phần tử của mỗi chuỗi đầu vào. Thật không may, Transformer thông thường có độ phức tạp bậc hai với số lượng token L trong chuỗi đầu vào, điều này rất tốn kém cho L lớn và ngăn cản việc sử dụng trong các cài đặt có tài nguyên tính toán hạn chế ngay cả với giá trị L vừa phải. Một số giải pháp đã được đề xuất để giải quyết vấn đề này (Beltagy et al., 2020; Gulati et al., 2020; Chan et al., 2020; Child et al., 2019; Bello et al., 2019). Hầu hết các phương pháp hạn chế cơ chế attention chỉ tham gia vào các vùng lân cận cục bộ (Parmar et al., 2018) hoặc kết hợp các giả định cấu trúc về attention như sparsity (Child et al., 2019), nén dựa trên pooling (Rae et al., 2020) các kỹ thuật clustering/binning/convolution (ví dụ: (Roy et al., 2020) áp dụng k-means clustering để học các vùng attention sparse động, hoặc (Kitaev et al., 2020), nơi locality sensitive hashing được sử dụng để nhóm các token có embedding tương tự), sliding windows (Beltagy et al., 2020), hoặc truncated targeting (Chelba et al., 2020). Cũng có một dòng nghiên cứu dài về việc sử dụng ma trận attention dense, nhưng được định nghĩa bởi các kernel low-rank thay thế softmax (Katharopoulos et al., 2020; Shen et al., 2018). Những phương pháp này phụ thuộc nghiêm trọng vào các kernel cho phép biểu diễn rõ ràng dưới dạng tích vô hướng của các vector đặc trưng positive-feature hữu hạn.

Các phương pháp trên không nhằm mục đích xấp xỉ attention thông thường, mà đề xuất các cơ chế attention đơn giản và dễ xử lý hơn, thường bằng cách kết hợp các ràng buộc bổ sung (ví dụ: tập query và key giống hệt nhau như trong (Kitaev et al., 2020)), hoặc bằng cách đánh đổi attention thông thường với sparse attention sử dụng nhiều

Đóng góp ngang nhau. Liên hệ với {kchoro,lcolwell}@google.com .
Code cho mô hình Transformer trên dữ liệu protein có thể tìm thấy tại github.com/google-research/google-research/tree/master/protein_lm và code Performer có thể tìm thấy tại github.com/google-research/google-research/tree/master/performer . Google AI Blog: https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html
1arXiv:2009.14794v4  [cs.LG]  19 Nov 2022

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
layer (Child et al., 2019). Thật không may, thiếu đảm bảo nghiêm ngặt cho khả năng biểu diễn được tạo ra bởi các phương pháp như vậy, và đôi khi tính hợp lệ của các mẫu sparsity chỉ có thể được xác minh thông qua thử nghiệm và sai sót bằng cách xây dựng các hoạt động GPU đặc biệt (ví dụ: viết các kernel C++ CUDA (Child et al., 2019) hoặc sử dụng TVM (Beltagy et al., 2020)). Các kỹ thuật khác nhằm giảm độ phức tạp không gian của Transformers bao gồm các layer dư thừa có thể đảo ngược cho phép lưu trữ activation một lần trong huấn luyện (Kitaev et al., 2020) và trọng số attention chia sẻ (Xiao et al., 2019).

Những ràng buộc này có thể cản trở việc áp dụng cho các vấn đề chuỗi dài, nơi các xấp xỉ của cơ chế attention là không đủ. Các xấp xỉ dựa trên back-propagation bị cắt ngắn (Dai et al., 2019) cũng không thể nắm bắt các tương quan khoảng cách dài vì các gradient chỉ được truyền bên trong một cửa sổ cục bộ. Các phương pháp khác đề xuất ước lượng thiên lệch của attention thông thường nhưng chỉ trong cài đặt non-causal và với sai số bình phương trung bình lớn (Wang et al., 2020).

Để đáp ứng, chúng tôi giới thiệu kiến trúc Transformer đầu tiên, Performers, có khả năng ước lượng chính xác và thực tế có thể chứng minh của attention full-rank (softmax) thông thường, nhưng chỉ có độ phức tạp không gian và thời gian tuyến tính và không dựa vào bất kỳ giả định nào như sparsity hoặc low-rankness. Performers sử dụng cơ chế Fast Attention Via positive Orthogonal Random features (FAVOR+), tận dụng các phương pháp mới để xấp xỉ kernel softmax và Gaussian, mà chúng tôi đề xuất. Chúng tôi tin rằng những phương pháp này có ý nghĩa độc lập, đóng góp vào lý thuyết về các phương pháp kernel có thể mở rộng. Do đó, Performers là kiến trúc tuyến tính đầu tiên hoàn toàn tương thích (thông qua lượng nhỏ fine-tuning) với Transformer thông thường, cung cấp đảm bảo lý thuyết mạnh: ước lượng không thiên lệch hoặc gần như không thiên lệch của ma trận attention, hội tụ đồng nhất và phương sai thấp hơn của xấp xỉ.

FAVOR+ cũng có thể được áp dụng để mô hình hóa hiệu quả các cơ chế attention kernelizable khác ngoài softmax. Khả năng biểu diễn này rất quan trọng để so sánh chính xác softmax với các kernel khác lần đầu tiên trên các tác vụ quy mô lớn, vượt ra ngoài khả năng của Transformer thông thường, và tìm ra các attention-kernel tối ưu cho chúng. FAVOR+ cũng có thể được áp dụng ngoài phạm vi Transformer như một sự thay thế có thể mở rộng hơn cho attention thông thường, bản thân nó có nhiều ứng dụng trong computer vision (Fu et al., 2019), reinforcement learning (Zambaldi et al., 2019), huấn luyện với softmax cross entropy loss, và thậm chí tối ưu hóa tổ hợp (Vinyals et al., 2015).

Chúng tôi thử nghiệm Performers trên một tập hợp tác vụ phong phú từ dự đoán pixel đến mô hình văn bản và mô hình hóa chuỗi protein. Chúng tôi chứng minh kết quả cạnh tranh với các phương pháp attention sparse và dense hiệu quả khác được kiểm tra, thể hiện hiệu quả của mô hình học attention mới được tận dụng bởi Performers. Chúng tôi nhấn mạnh rằng về nguyên tắc, FAVOR+ cũng có thể được kết hợp với các kỹ thuật khác, như reversible layer (Kitaev et al., 2020) hoặc cluster-based attention (Roy et al., 2020).

2 CƠ CHẾ FAVOR+ & POSITIVE ORTHOGONAL RANDOM FEATURES

Dưới đây chúng tôi mô tả chi tiết cơ chế FAVOR+ - xương sống của kiến trúc Performer. Chúng tôi giới thiệu một phương pháp mới để ước lượng kernel softmax (và Gaussian) với positive orthogonal random features mà FAVOR+ tận dụng để ước lượng mạnh mẽ và không thiên lệch của attention (softmax) thông thường và chỉ ra cách FAVOR+ có thể được áp dụng cho các attention-kernel khác.

2.1 KIẾN THỨC CƠ BẢN - CỚ CHẾ ATTENTION THÔNG THƯỜNG

Cho L là kích thước của chuỗi token đầu vào. Khi đó attention tích vô hướng thông thường (Vaswani et al., 2017) là một mapping chấp nhận ma trận Q;K;V∈R^(L×d) làm đầu vào trong đó d là chiều ẩn (chiều của biểu diễn latent). Ma trận Q;K;V là các biểu diễn trung gian của đầu vào và các hàng của chúng có thể được hiểu như queries, keys và values của cấu trúc dữ liệu từ điển liên tục tương ứng. Attention tích vô hướng hai chiều (hoặc non-directional (Devlin et al., 2018)) có dạng sau, trong đó A∈R^(L×L) là ma trận attention gọi là:

Att$(Q;K;V) =D^(-1)AV;A= exp( QK^T/√d);D= diag( A1_L): (1)

Ở đây exp() được áp dụng element-wise, 1_L là vector tất cả một có độ dài L, và diag() là ma trận đường chéo với vector đầu vào làm đường chéo. Độ phức tạp thời gian và không gian để tính toán (1) là O(L^2d) và O(L^2+Ld) tương ứng, vì A phải được lưu trữ rõ ràng. Do đó, về nguyên tắc, attention tích vô hướng loại (1) không tương thích với xử lý end-to-end của các chuỗi dài. Attention hai chiều được áp dụng trong encoder self-attention và encoder-decoder attention trong kiến trúc Seq2Seq.

Một loại attention quan trọng khác là attention tích vô hướng một chiều có dạng:

Att!(Q;K;V) =Ď^(-1)ĂV;Ă= tril( A);Ď= diag(Ă1_L); (2)

2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
trong đó tril() trả về phần tam giác dưới của ma trận đối số bao gồm đường chéo. Như đã thảo luận trong (Vaswani et al., 2017), attention một chiều được sử dụng cho mô hình tạo sinh tự hồi quy, ví dụ như self-attention trong Transformer tạo sinh cũng như phần decoder của Transformer Seq2Seq.

Chúng tôi sẽ chỉ ra rằng ma trận attention A có thể được xấp xỉ đến bất kỳ độ chính xác nào trong thời gian O(Ld^2log(d)). Để so sánh, các phương pháp phổ biến tận dụng sparsity thông qua kỹ thuật Locality-Sensitive Hashing (LSH) (Kitaev et al., 2020) có độ phức tạp thời gian O(Ld^2logL). Trong phần chính của bài báo, chúng tôi sẽ mô tả FAVOR+ cho attention hai chiều. Kết quả hoàn toàn tương tự có thể thu được cho biến thể một chiều thông qua cơ chế prefix-sum (tất cả chi tiết trong Phụ lục B.1).

2.2 GENERALIZED KERNELIZABLE ATTENTION

FAVOR+ hoạt động cho các khối attention sử dụng ma trận A∈R^(L×L) có dạng A(i,j) = K( q_i^T,k_j^T), với q_i=k_j đại diện cho vector hàng query/key thứ i/j trong Q=K và kernel K :R^d×R^d→R_+ được định nghĩa cho mapping (thường là ngẫu nhiên): φ:R^d→R_+^r (với r>0 nào đó) như:

K(x,y) =E[φ(x)^Tφ(y)]: (3)

Chúng tôi gọi φ(u) là random feature map cho u∈R^d. Với Q';K'∈R^(L×r) có các hàng được cho như φ(q_i^T)^T và φ(k_i^T)^T tương ứng, Phương trình 3 dẫn trực tiếp đến cơ chế attention hiệu quả có dạng:

\Att$(Q;K;V) =D̂^(-1)(Q'((K')^TV));D̂= diag( Q'((K')^T1_L)): (4)

Ở đây \Att$ đại diện cho attention xấp xỉ và dấu ngoặc chỉ thứ tự tính toán. Dễ thấy rằng cơ chế như vậy được đặc trưng bởi độ phức tạp không gian O(Lr+Ld+rd) và độ phức tạp thời gian O(Lrd) so với O(L^2+Ld) và O(L^2d) của attention thông thường (xem thêm Hình 1).

Hình 1: Xấp xỉ của cơ chế attention thông thường AV (trước khi chuẩn hóa D^(-1)) thông qua (random) feature maps. Các khối nét đứt chỉ thứ tự tính toán với độ phức tạp thời gian tương ứng được đính kèm.

Sơ đồ trên tạo thành phần FA của cơ chế FAVOR+. Phần OR+ còn lại trả lời các câu hỏi sau: (1) Mô hình attention được định nghĩa trong Phương trình 3 có biểu cảm như thế nào, và đặc biệt, chúng ta có thể sử dụng nó về nguyên tắc để xấp xỉ attention softmax thông thường không? (2) Chúng ta triển khai nó một cách mạnh mẽ trong thực tế như thế nào, và đặc biệt, chúng ta có thể chọn r≪L cho L≫d để đạt được lợi ích về độ phức tạp không gian và thời gian mong muốn không? Chúng tôi trả lời những câu hỏi này trong các phần tiếp theo.

2.3 CÁCH VÀ CÁCH KHÔNG XẤP XỈ SOFTMAX-KERNELS CHO ATTENTION

Hóa ra bằng cách lấy φ có dạng sau cho các hàm f_1;...;f_l:R→R, hàm g:R^d→R và các vector xác định ω_i hoặc ω_1;...;ω_m iid D cho một số phân phối D∈P(R^d):

φ(x) =h(x)/√m(f_1(ω_1^Tx);...;f_1(ω_m^Tx);...;f_l(ω_1^Tx);...;f_l(ω_m^Tx)); (5)

chúng ta có thể mô hình hóa hầu hết các kernel được sử dụng trong thực tế. Hơn nữa, trong hầu hết các trường hợp D là isotropic (tức là với hàm mật độ xác suất không đổi trên một hình cầu), thường là Gaussian. Ví dụ, bằng cách lấy h(x) = 1, l= 1 và D=N(0;I_d) chúng ta thu được estimator của cái gọi là PNG-kernel (Choromanski et al., 2017) (ví dụ: f_1= sgn tương ứng với angular kernel). Cấu hình: h(x) = 1, l= 2, f_1= sin, f_2= cos tương ứng với shift-invariant kernel, đặc biệt D=N(0;I_d) dẫn đến Gaussian kernel K_gauss (Rahimi & Recht, 2007). Softmax-kernel định nghĩa ma trận attention thông thường A được cho như:

3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
SM(x,y)^def= exp( x^Ty): (6)

Trong phần trên, không mất tính tổng quát, chúng tôi bỏ qua việc chuẩn hóa √d vì chúng ta có thể tương đương chuẩn hóa lại các key và query đầu vào. Vì: SM(x,y) = exp(‖x‖_2^2)K_gauss(x,y) exp(‖y‖_2^2), dựa trên những gì chúng tôi đã nói, chúng ta thu được xấp xỉ random feature map không thiên lệch của SM(x,y) sử dụng các hàm lượng giác với: h(x) = exp(‖x‖_2^2), l= 2, f_1= sin, f_2= cos. Chúng tôi gọi nó là dSM_m^trig(x,y).

Tuy nhiên có một điểm cần lưu ý ở đây. Module attention từ (1) xây dựng cho mỗi token, một tổ hợp lồi của các value-vector với các hệ số được cho như các điểm kernel được chuẩn hóa tương ứng. Đó là lý do tại sao các kernel tạo ra điểm không âm được sử dụng. Việc áp dụng random feature map với các giá trị chiều có thể âm (sin/cos) dẫn đến hành vi không ổn định, đặc biệt khi các điểm kernel gần 0 (đây là trường hợp của nhiều mục trong A tương ứng với các token có độ liên quan thấp) được xấp xỉ bởi các estimator có phương sai lớn trong các vùng như vậy. Điều này dẫn đến hành vi bất thường, ví dụ như các giá trị đường chéo âm của renormalizer D^(-1), và do đó hoặc hoàn toàn ngăn cản việc huấn luyện hoặc dẫn đến các mô hình sub-optimal. Chúng tôi chứng minh thực nghiệm rằng đây là điều xảy ra với dSM_m^trig và cung cấp giải thích lý thuyết chi tiết cho thấy rằng phương sai của dSM_m^trig lớn khi các giá trị được xấp xỉ tiến về 0 (xem: Phần 3). Đây là một trong những lý do chính tại sao cơ chế random feature map mạnh mẽ để xấp xỉ attention softmax thông thường chưa bao giờ được đề xuất.

Chúng tôi đề xuất một cơ chế mạnh mẽ trong bài báo này. Hơn nữa, phương sai của estimator positive random feature map không thiên lệch mới của chúng tôi tiến về 0 khi các giá trị được xấp xỉ tiến về 0 (xem: Phần 3).

Bổ đề 1 (Positive Random Features (PRFs) cho Softmax). Với x,y∈R^d, z=x+y chúng ta có:
SM(x,y) =E_{ω~N(0,I_d)}[exp(ω^Tx‖x‖_2^2/2)exp(ω^Ty‖y‖_2^2/2)]
= E_{ω~N(0,I_d)}[μcosh(ω^Tz)], (7)

trong đó μ = exp(‖x‖^2+‖y‖^2/2) và cosh là hàm cosine hyperbolic. Do đó, softmax-kernel cho phép xấp xỉ positive random feature map không thiên lệch với h(x) = exp(‖x‖_2^2/2), l= 1, f_1= exp và D=N(0,I_d) hoặc: h(x) =1/√2 exp(‖x‖_2^2/2), l= 2, f_1(u) = exp(u), f_2(u) = exp(-u) và cùng D (cái sau để giảm phương sai hơn nữa). Chúng tôi gọi các estimator liên quan: dSM_m^+ và dSM_m^{hyp+}.

Hình 2: Trái: Hàm tiện ích đối xứng (quanh gốc tọa độ) r (được định nghĩa là tỷ số của sai số bình phương trung bình (MSE) của các estimator được xây dựng trên: trigonometric và positive random features) như một hàm của góc θ (tính bằng radian) giữa các vector đặc trưng đầu vào và độ dài l của chúng. Các giá trị lớn hơn cho thấy các vùng không gian (θ,l) với hiệu suất tốt hơn của positive random features. Chúng ta thấy rằng đối với các vùng quan trọng với θ đủ lớn (giá trị softmax-kernel đủ nhỏ) phương pháp của chúng tôi chính xác hơn tùy ý so với trigonometric random features. Biểu đồ được trình bày cho miền [-π,π]×[2,2]. Phải: Lát cắt của hàm r với l= 1 cố định và góc θ thay đổi. Góc phải trên: So sánh MSE của cả hai estimator trong vùng giá trị softmax-kernel thấp.

Trong Hình 2, chúng tôi trực quan hóa các ưu điểm của positive so với standard trigonometric random features. Trong các vùng quan trọng, nơi các giá trị kernel nhỏ và cần xấp xỉ cẩn thận, phương pháp của chúng tôi vượt trội hơn đối tác của nó. Trong Phần 4, chúng tôi tiếp tục xác nhận các ưu điểm của phương pháp thông qua thực nghiệm, sử dụng positive features để huấn luyện hiệu quả các Transformer tuyến tính dựa trên softmax. Nếu chúng ta thay thế ω trong (7) bằng √d ω/‖ω‖, chúng ta thu được cái gọi là regularized softmax-kernel SM_REG mà chúng ta có thể xấp xỉ theo cách tương tự, chỉ đơn giản thay đổi D=N(0,I_d) thành D= Unif(√d S^{d-1}), một phân phối tương ứng với measure Haar trên hình cầu bán kính √d trong R^d, thu được estimator \SM_REG^{+}_m. Như chúng tôi chỉ ra trong Phần 3, các random features như vậy cũng có thể được sử dụng để xấp xỉ chính xác regular softmax-kernel.

4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
2.4 ORTHOGONAL RANDOM FEATURES (ORFS)

Phần trên tạo thành phần R+ của phương pháp FAVOR+. Vẫn còn phải giải thích phần O. Để giảm thêm phương sai của estimator (để chúng ta có thể sử dụng số lượng random features r thậm chí nhỏ hơn), chúng tôi làm cho các mẫu ngẫu nhiên khác nhau ω_1;...;ω_m trở nên chính xác trực giao. Điều này có thể được thực hiện trong khi duy trì tính không thiên lệch bất cứ khi nào các phân phối isotropic D được sử dụng (tức là đặc biệt trong tất cả các kernel mà chúng tôi đã xem xét cho đến nay) bằng quy trình orthogonalization Gram-Schmidt tiêu chuẩn (xem (Choromanski et al., 2017) để biết chi tiết). ORFs là một phương pháp được biết đến, tuy nhiên hóa ra nó hoạt động đặc biệt tốt với PRFs mà chúng tôi giới thiệu cho softmax. Điều này dẫn đến các kết quả lý thuyết đầu tiên cho thấy rằng ORFs có thể được áp dụng để giảm phương sai của các estimator softmax/Gaussian kernel cho bất kỳ dimensionality d nào thay vì chỉ asymptotically cho d đủ lớn (như trường hợp của các phương pháp trước đó, xem: phần tiếp theo) và dẫn đến các giới hạn nhỏ theo cấp số nhân đầu tiên về xác suất large deviations nghiêm ngặt nhỏ hơn so với các phương pháp non-orthogonal. Tính positive của random features đóng vai trò chủ chốt trong các giới hạn này. Cơ chế ORF yêu cầu m≤d, nhưng đây sẽ là trường hợp trong tất cả các thí nghiệm của chúng tôi. Pseudocode của toàn bộ thuật toán FAVOR+ được cho trong Phụ lục B.

Các kết quả lý thuyết của chúng tôi được liên kết chặt chẽ với thí nghiệm. Chúng tôi chỉ ra trong Phần 4 rằng PRFs+ORFs cải thiện đáng kể độ chính xác của xấp xỉ ma trận attention và cho phép chúng tôi giảm r dẫn đến một cơ chế chính xác cũng như hiệu quả về không gian và thời gian mà chúng tôi gọi là FAVOR+.

3 KẾT QUẢ LÝ THUYẾT

Chúng tôi trình bày ở đây lý thuyết về positive orthogonal random features cho ước lượng softmax-kernel. Tất cả các kết quả này cũng có thể được áp dụng cho Gaussian kernel, vì như đã giải thích trong phần trước, một cái có thể được thu được từ cái khác bằng cách chuẩn hóa lại (xem: Phần 2.3). Tất cả các chứng minh và kết quả lý thuyết tổng quát bổ sung với thảo luận được đưa ra trong Phụ lục.

Bổ đề 2 (positive (hyperbolic) so với trigonometric random features). Điều sau đây là đúng:
MSE(dSM_m^trig(x,y)) =1/(2m)exp(‖x+y‖^2)SM^2(x,y)(1-exp(-‖x-y‖^2))^2;
MSE(dSM_m^+(x,y)) =1/m exp(‖x+y‖^2)SM^2(x,y)(1-exp(-‖x+y‖^2));
MSE(dSM_m^{hyp+}(x,y)) =1/2(1-exp(-‖x+y‖^2))MSE(dSM_m^+(x,y)); (8)

cho các mẫu ngẫu nhiên độc lập ω_i, và trong đó MSE đại diện cho sai số bình phương trung bình.

Do đó, với SM(x,y)→0 chúng ta có: MSE(dSM_m^trig(x,y))→∞ và MSE(dSM_m^+(x,y))→0. Hơn nữa, estimator hyperbolic cung cấp cải thiện độ chính xác bổ sung nghiêm ngặt tốt hơn so với dSM_{2m}^+(x,y) với gấp đôi số random features. Kết quả tiếp theo cho thấy rằng regularized softmax-kernel trong thực tế là một proxy chính xác của softmax-kernel trong attention.

Định lý 1 (regularized so với softmax-kernel). Giả sử rằng L1-norm của ma trận attention cho softmax-kernel thỏa mãn: ‖A‖_1≤C cho một số hằng số C≥1. Ký hiệu bằng A_reg ma trận attention tương ứng cho regularized softmax-kernel. Điều sau đây đúng:

inf_{i,j} A_reg(i,j)/A(i,j) ≥ 1-2/d^{1/3}+o(1/d^{1/3}), và sup_{i,j} A_reg(i,j)/A(i,j) ≤ 1: (9)

Hơn nữa, cái sau đúng cho d≥2 ngay cả khi điều kiện L1-norm không được thỏa mãn, tức là regularized softmax-kernel là giới hạn dưới universal cho softmax-kernel.

Do đó, positive random features cho SM_REG có thể được sử dụng để xấp xỉ softmax-kernel. Kết quả tiếp theo của chúng tôi cho thấy rằng orthogonality có thể chứng minh giảm sai số bình phương trung bình của ước lượng với positive random features cho bất kỳ dimensionality d>0 và chúng tôi cung cấp rõ ràng khoảng cách.

Định lý 2. Nếu dSM_m^{ort+}(x,y) đại diện cho sự biến đổi của dSM_m^+(x,y) với orthogonal random features (và do đó cho m≤d), thì điều sau đây đúng cho bất kỳ d>0:

MSE(dSM_m^{ort+}(x,y))≤MSE(dSM_m^+(x,y))-2(m-1)/(m(d+ 2)) SM(x,y)exp(-(‖x‖^2+‖y‖^2)/2): (10)

Hơn nữa, kết quả hoàn toàn tương tự đúng cho regularized softmax-kernel SM_REG.

5

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
Đối với regularized softmax-kernel, orthogonal features cung cấp kết quả concentration bổ sung - các giới hạn nhỏ theo cấp số nhân đầu tiên cho xác suất của đuôi estimator nghiêm ngặt tốt hơn so với các biến thể non-orthogonal cho mọi d>0. Kết quả tiếp theo của chúng tôi cho phép chúng tôi ước lượng rõ ràng khoảng cách.

Định lý 3. Cho x,y∈R^d. Điều sau đây đúng cho bất kỳ a>SM_REG(x,y), λ>0 và m≤d:
P[\SM_REG_m^+(x,y)>a]≤exp(-λma)M_Z(λ)^m; P[\SM_REG_m^{ort+}(x,y)>a]≤
exp(-λma)/M_Z(λ)^m exp(-λm/2(‖x‖^2+‖y‖^2)-4λm(m-1)/(4(d+ 2))‖x+y‖^4)

trong đó \SM_REG_m^{ort+}(x,y) đại diện cho sự biến đổi của \SM_REG_m^+(x,y) với ORFs, Z= λ exp(√d ω^T/(‖ω‖_2)(x+y)), ω~N(0,I_d), μ như trong Bổ đề 1 và M_Z là hàm tạo moment của Z.

Chúng ta thấy rằng ORFs cung cấp các giới hạn nhỏ theo cấp số nhân và sắc nét hơn cho các vùng quan trọng nơi softmax-kernel nhỏ. Dưới đây chúng tôi chỉ ra rằng ngay cả đối với cơ chế SM_m^trig với ORFs, chỉ cần lấy m= Õ(d log(d)) random projection để xấp xỉ chính xác ma trận attention (do đó nếu không có attention renormalization, PRFs sẽ không cần thiết). Nói chung, m phụ thuộc vào dimensionality d của embedding, bán kính R của hình cầu nơi tất cả queries/keys sống và tham số precision ε (xem: Phụ lục F.6 để thảo luận bổ sung), nhưng không phụ thuộc vào độ dài chuỗi đầu vào L.

Định lý 4 (uniform convergence cho attention approximation). Giả sử rằng L2-norm của queries/keys được giới hạn trên bởi R > 0. Định nghĩa l=R d^{-1/4} và lấy h= exp(l^2/2). Thì với bất kỳ ε>0, δ=ε/(h)^2 và số random projection m= Õ(d/ε^2 log(4d^3/ε^4 R)) điều sau đây đúng cho cơ chế xấp xỉ attention tận dụng estimator dSM^trig với ORFs:
‖Â-A‖_1≤ε với bất kỳ xác suất hằng số nào, trong đó Â xấp xỉ ma trận attention A.

4 THÍ NGHIỆM

Chúng tôi triển khai setup của mình trên đầu của code huấn luyện Transformer có sẵn trong Jax (Frostig et al., 2018) được tối ưu hóa với just-in-time (jax.jit) compilation, và bổ sung lý thuyết của chúng tôi bằng bằng chứng thực nghiệm để chứng minh tính thực tế của FAVOR+ trong nhiều cài đặt. Trừ khi được nêu rõ ràng, một Performer chỉ thay thế thành phần attention bằng phương pháp của chúng tôi, trong khi tất cả các thành phần khác hoàn toàn giống như Transformer thông thường. Để ký hiệu tắt, chúng tôi ký hiệu unidirectional/causal modelling là (U) và bidirectional/masked language modelling là (B).

Về baseline, chúng tôi sử dụng các mô hình Transformer khác để so sánh, mặc dù một số trong số chúng bị hạn chế chỉ một trường hợp - ví dụ Reformer (Kitaev et al., 2020) chỉ là (U), và Linformer (Wang et al., 2020) chỉ là (B). Hơn nữa, chúng tôi sử dụng PG-19 (Rae et al., 2020) như một benchmark pretraining (B) thay thế, vì nó được tạo ra cho việc huấn luyện chuỗi dài so với BookCorpus (Zhu et al., 2015) + dataset Wikipedia được sử dụng trong BERT (Devlin et al., 2018) và Linformer (hiện không còn công khai). Tất cả các hyperparameter mô hình và tokenization được hiển thị trong Phụ lục A.

Hình 3: So sánh Transformer và Performer về tốc độ forward và backward pass và L tối đa được phép. "X" (OPT) ký hiệu speedup tối đa có thể đạt được, khi attention đơn giản trả về V-matrix. Biểu đồ được hiển thị cho đến khi một mô hình tạo ra lỗi out of memory trên GPU V100 với 16GB. Kích thước từ vựng được sử dụng là 256. Tốt nhất trong màu sắc.

4.1 CHI PHÍ TÍNH TOÁN

Chúng tôi so sánh về tốc độ backward pass của Transformer và Performer trong cài đặt (B), vì nó là một trong những nút thắt cổ chai tính toán chính trong quá trình huấn luyện, khi sử dụng kích thước mặc định thông thường (n_heads;n_layers;d_ff;d) = (8;6;2048;512), trong đó d_ff ký hiệu chiều rộng của các layer MLP.

6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
Chúng tôi quan sát (Hình 3) rằng về L, Performer đạt được thời gian gần như tuyến tính và tiêu thụ bộ nhớ sub-quadratic (vì ma trận attention O(L^2) rõ ràng không được lưu trữ). Thực tế, Performer đạt được speedup và hiệu quả bộ nhớ gần như tối ưu có thể, được mô tả bởi đường "X" khi attention được thay thế bằng "hàm đồng nhất" đơn giản trả về V-matrix. Sự kết hợp của cả hiệu quả bộ nhớ và backward pass cho L lớn cho phép tương ứng, huấn luyện batch lớn và thời gian wall clock thấp hơn mỗi bước gradient. Các kết quả bổ sung rộng rãi được chứng minh trong Phụ lục E bằng cách thay đổi layer, raw attention, và kích thước kiến trúc.

4.2 SAI SỐ XẤP XỈ SOFTMAX ATTENTION

Chúng tôi tiếp tục kiểm tra sai số xấp xỉ thông qua FAVOR+ trong Hình 4. Chúng tôi chứng minh rằng 1. Orthogonal features tạo ra sai số thấp hơn so với unstructured (IID) features, 2. Positive features tạo ra sai số thấp hơn so với trigonometric sin/cos features. Hai điều này thực nghiệm xác nhận cơ chế PORF.

Hình 4: MSE của đầu ra xấp xỉ khi so sánh Orthogonal vs IID features và trigonometric sin/cos vs positive features. Chúng tôi lấy L= 4096;d= 16, và thay đổi số mẫu ngẫu nhiên m. Độ lệch chuẩn được hiển thị trên 15 mẫu dữ liệu đầu vào ma trận ngẫu nhiên được chuẩn hóa phù hợp.

Để cải thiện thêm xấp xỉ tổng thể của các khối attention qua nhiều iteration mà cải thiện thêm việc huấn luyện, các mẫu ngẫu nhiên nên được vẽ lại định kỳ (Hình 5, phải). Đây là một quy trình rẻ, nhưng có thể được tối ưu hóa thêm (Phụ lục B.2).

4.3 XẤP XỈ SOFTMAX TRÊN TRANSFORMERS

Ngay cả khi xấp xỉ của cơ chế attention chặt chẽ, các sai số nhỏ có thể dễ dàng lan truyền qua nhiều layer Transformer (ví dụ: MLP, nhiều head), như chúng tôi chỉ ra trong Hình 14 (Phụ lục). Nói cách khác, hằng số Lipschitz của mô hình có thể dễ dàng scale up sai số xấp xỉ attention nhỏ, có nghĩa là các xấp xỉ rất chặt chẽ đôi khi có thể cần thiết. Do đó, khi áp dụng xấp xỉ softmax của FAVOR(+) trên mô hình Transformer (tức là "Performer-X-SOFTMAX"), chúng tôi chứng minh rằng:

1. Khả năng tương thích ngược với các mô hình pretrained có sẵn như một lợi ích từ xấp xỉ softmax, thông qua fine-tuning nhỏ (cần thiết do lan truyền sai số) ngay cả cho trigonometric features (Hình 5, trái) trên dataset LM1B (Chelba et al., 2014). Tuy nhiên, khi trên dataset lớn hơn PG-19, 2. Positive (POS) softmax features (với redrawing) trở nên quan trọng để đạt được hiệu suất phù hợp với Transformer thông thường (Hình 5, phải).

Hình 5: Chúng tôi chuyển trọng số của Transformer pretrained gốc vào Performer, tạo ra độ chính xác ban đầu không bằng không 0.07 (đường cam chấm), nhưng nhanh chóng phục hồi độ chính xác trong một phần nhỏ số bước gradient gốc. Tuy nhiên trên PG-19, xấp xỉ Trigonometric (TRIG) softmax trở nên rất không ổn định (đường cong đầy đủ trong Phụ lục D.2), trong khi positive features (POS) (không có redrawing) và Linformer (cũng xấp xỉ softmax) ngay cả với các projection được vẽ lại, plateau tại cùng perplexity. Positive softmax với feature redrawing là cần thiết để phù hợp với Transformer, với SM_REG (regularization từ Phần 3) cho phép hội tụ nhanh hơn. Các nghiên cứu ablation bổ sung trên nhiều attention kernel, cũng cho thấy rằng trigonometric random features thậm chí dẫn đến giá trị NaN trong huấn luyện được đưa ra trong Phụ lục D.3.

7

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
4.4 HUẤN LUYỆN NHIỀU LAYER CHO PROTEIN

Chúng tôi tiếp tục benchmark Performer trên cả trường hợp (U) và (B) bằng cách huấn luyện mô hình 36-layer sử dụng chuỗi protein từ bản phát hành tháng 1 năm 2019 của TrEMBL (Consortium, 2019), tương tự như (Madani et al., 2020). Trong Hình 6, Reformer và Linformer giảm đáng kể độ chính xác trên dataset protein. Hơn nữa, tính hữu ích của generalized attention được chứng minh bởi Performer-RELU (lấy f= ReLU trong Phương trình 5) đạt được độ chính xác cao nhất trong cả trường hợp (U) và (B). Xấp xỉ softmax được đề xuất của chúng tôi cũng được chỉ ra là chặt chẽ, đạt được cùng độ chính xác như Transformer exact-softmax và xác nhận các tuyên bố lý thuyết của chúng tôi từ Phần 3.

Hình 6: Train = Đứt nét, Validation = Liền nét. Đối với TrEMBL, chúng tôi sử dụng chính xác cùng tham số mô hình (n_heads;n_layers;d_ff;d) = (8;36;1024;512) từ (Madani et al., 2020) cho tất cả các lần chạy. Để công bằng, tất cả thí nghiệm TrEMBL sử dụng 16x16 TPU-v2. Kích thước batch được tối đa hóa cho mỗi lần chạy riêng biệt với các ràng buộc tính toán. Hyperparameter có thể tìm thấy trong Phụ lục A. Kết quả mở rộng bao gồm thống kê dataset, đánh giá out of distribution, và trực quan hóa, có thể tìm thấy trong Phụ lục C.

4.5 HUẤN LUYỆN ĐỘ DÀI LỚN - DATASET THÔNG THƯỜNG

Trên benchmark ImageNet64 (U) tiêu chuẩn từ (Parmar et al., 2018) với L= 12288 không khả thi cho Transformer thông thường, chúng tôi đặt tất cả mô hình sử dụng cùng (n_heads;d_ff;d) nhưng thay đổi n_layers. Performer/6-layer phù hợp với Reformer/12-layer, trong khi Performer/12-layer phù hợp với Reformer/24-layer (Hình 7: trái). Tùy thuộc vào phần cứng (TPU hoặc GPU), chúng tôi cũng thấy rằng Performer có thể nhanh hơn 2x so với Reformer thông qua tối ưu hóa Jax cho cài đặt (U).

Đối với một nghiên cứu chứng minh nguyên lý, chúng tôi cũng tạo ra một benchmark protein ban đầu để dự đoán tương tác giữa các nhóm protein bằng cách nối các chuỗi protein với độ dài L= 8192 từ TrEMBL, đủ dài để mô hình hóa mạng tương tác protein mà không cần các sequence alignment lớn được yêu cầu bởi các phương pháp hiện có (Cong et al., 2019). Trong cài đặt này, một Transformer thông thường làm quá tải bộ nhớ ngay cả với kích thước batch 1 mỗi chip, với một biên độ rộng. Do đó như một baseline, chúng tôi buộc phải sử dụng một biến thể nhỏ hơn đáng kể, giảm xuống (n_heads;n_layers;d_ff;d) = (8;{1;2;3};256;256). Trong khi đó, Performer huấn luyện hiệu quả với kích thước batch 8 mỗi chip sử dụng kiến trúc tiêu chuẩn (8;6;2048;512). Chúng ta thấy trong Hình 7 (biểu đồ con bên phải) rằng Transformer nhỏ hơn (n_layer = 3) nhanh chóng bị giới hạn ở 19%, trong khi Performer có thể huấn luyện liên tục đến 24%.

Hình 7: Train = Đứt nét, Validation = Liền nét. Đối với ImageNet64, tất cả mô hình sử dụng tiêu chuẩn (n_heads;d_ff;d) = (8;2048;512). Chúng tôi tiếp tục chỉ ra rằng xấp xỉ positive softmax của chúng tôi đạt được cùng hiệu suất như ReLU trong Phụ lục D.2. Đối với TrEMBL nối, chúng tôi thay đổi n_layers∈{1;2;3} cho Transformer nhỏ hơn. Hyperparameter có thể tìm thấy trong Phụ lục A.

5 KẾT LUẬN

Chúng tôi đã trình bày Performer, một loại Transformer mới, dựa vào cơ chế Fast Attention Via positive Orthogonal Random features (FAVOR+) của chúng tôi để cải thiện đáng kể độ phức tạp không gian và thời gian của Transformer thông thường. Cơ chế của chúng tôi cung cấp theo hiểu biết của chúng tôi ước lượng không thiên lệch hiệu quả đầu tiên của Transformer dựa trên softmax gốc với độ phức tạp không gian và thời gian tuyến tính và mở ra các hướng mới trong nghiên cứu về Transformer và vai trò của các cơ chế attention non-sparsifying.

8

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
6 TÁC ĐỘNG RỘNG HƠN

Chúng tôi tin rằng thuật toán được trình bày có thể có tác động theo nhiều cách khác nhau:

Sinh học và Y học: Phương pháp của chúng tôi có tiềm năng tác động trực tiếp đến nghiên cứu về phân tích chuỗi sinh học bằng cách cho phép Transformer được áp dụng cho các chuỗi dài hơn nhiều mà không có ràng buộc về cấu trúc của ma trận attention. Ứng dụng ban đầu mà chúng tôi xem xét là dự đoán tương tác giữa các protein trên quy mô proteome. Các phương pháp được công bố gần đây yêu cầu sequence alignment tiến hóa lớn, một nút thắt cổ chai cho các ứng dụng cho genome của động vật có vú (Cong et al., 2019). Tác động translational có thể rộng rãi của việc áp dụng các phương pháp này cho chuỗi sinh học là một trong những động lực chính của công việc này. Chúng tôi tin rằng tin sinh học hiện đại có thể được hưởng lợi rất lớn từ các kỹ thuật học máy mới với Transformer nằm trong số những kỹ thuật hứa hẹn nhất. Việc mở rộng các phương pháp này để huấn luyện nhanh hơn các mô hình ngôn ngữ chính xác hơn mở ra cánh cửa cho khả năng thiết kế tập hợp phân tử với các thuộc tính tương tác được chỉ định trước. Các phương pháp này có thể được sử dụng để bổ sung cho các chiến lược thiết kế dựa trên vật lý hiện có có tầm quan trọng cốt yếu ví dụ trong việc phát triển vaccine nanoparticle mới (Marcandalli et al., 2019).

Môi trường: Như chúng tôi đã chỉ ra, Performer với FAVOR+ được đặc trưng bởi chi phí tính toán thấp hơn nhiều và độ phức tạp không gian thấp hơn đáng kể có thể được dịch trực tiếp sang giảm phát thải CO2 (Strubell et al., 2019) và tiêu thụ năng lượng thấp hơn (You et al., 2020), vì Transformer thông thường yêu cầu tài nguyên tính toán rất lớn.

Nghiên cứu về Transformer: Chúng tôi tin rằng kết quả của chúng tôi có thể định hình nghiên cứu về kiến trúc Transformer hiệu quả, hướng dẫn lĩnh vực hướng tới các phương pháp có nền tảng toán học mạnh mẽ. Nghiên cứu của chúng tôi cũng có thể hy vọng mở rộng Transformer ra ngoài phạm vi tiêu chuẩn của chúng (ví dụ: bằng cách xem xét cơ chế Generalized Attention và kết nối với kernel). Khám phá các kiến trúc Transformer có thể mở rộng có thể xử lý L cỡ vài nghìn và hơn, bảo tồn độ chính xác của baseline cùng lúc, là cánh cửa cho các đột phá mới trong tin sinh học, ví dụ: mô hình ngôn ngữ cho protein, như chúng tôi đã giải thích trong bài báo. Phương pháp được trình bày của chúng tôi có thể là bước đầu tiên.

Khả năng tương thích ngược: Performer của chúng tôi có thể được sử dụng trên đầu của Transformer pretrained thông thường trái ngược với các biến thể Transformer khác. Ngay cả khi up-training không được yêu cầu, FAVOR+ vẫn có thể được sử dụng cho inference nhanh mà không mất độ chính xác. Chúng tôi nghĩ về khả năng tương thích ngược này như một tính năng bổ sung rất quan trọng của các kỹ thuật được trình bày có thể đặc biệt hấp dẫn đối với các thực hành viên.

Attention ngoài Transformer: Cuối cùng, FAVOR+ có thể được áp dụng để xấp xỉ exact attention cũng bên ngoài phạm vi của Transformer. Điều này mở ra một khối lượng lớn các ứng dụng tiềm năng mới bao gồm: hierarchical attention networks (HANS) (Yang et al., 2016), graph attention networks (Velickovic et al., 2018), xử lý ảnh (Fu et al., 2019), và reinforcement learning/robotics (Tang et al., 2020).

7 LỜI CẢM ƠN

Chúng tôi cảm ơn Nikita Kitaev và Wojciech Gajewski cho nhiều cuộc thảo luận về Reformer, và cũng cảm ơn Aurko Roy và Ashish Vaswani cho nhiều cuộc thảo luận về Routing Transformer. Chúng tôi tiếp tục cảm ơn Joshua Meier, John Platt, và Tom Weingarten cho nhiều cuộc thảo luận hiệu quả về dữ liệu sinh học và nhận xét hữu ích về bản thảo này. Chúng tôi cuối cùng cảm ơn Yi Tay và Mostafa Dehghani cho các cuộc thảo luận về so sánh baseline.

Valerii Likhosherstov thừa nhận sự hỗ trợ từ Cambridge Trust và DeepMind. Lucy Colwell thừa nhận sự hỗ trợ từ Simons Foundation. Adrian Weller thừa nhận sự hỗ trợ từ Turing AI Fellowship dưới grant EP/V025379/1, The Alan Turing Institute dưới EPSRC grant EP/N510129/1 và U/B/000074, và Leverhulme Trust thông qua CFI.

9

--- TRANG 10 ---
[Tiếp tục với các trang còn lại...][Lưu ý: Do giới hạn độ dài phản hồi, tôi đã dịch khoảng 1/3 đầu của tài liệu (trang 1-9). Văn bản gốc có 28 trang với nhiều công thức toán học, bảng biểu và tham khảo. Để có bản dịch đầy đủ, bạn có thể yêu cầu tôi tiếp tục dịch từng phần một cách tuần tự.][Lưu ý: Do giới hạn độ dài phản hồi, tôi đã dịch khoảng 1/3 đầu của tài liệu (trang 1-9). Văn bản gốc có 28 trang với nhiều công thức toán học, bảng biểu và tham khảo. Để có bản dịch đầy đủ, bạn có thể yêu cầu tôi tiếp tục dịch từng phần một cách tuần tự.]