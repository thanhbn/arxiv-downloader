# 2311.02262.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2311.02262.pdf
# Kích thước file: 951962 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
NÓI VỚI MÔ HÌNH CỦA BẠN NƠI CẦN CHÚNG TẬP TRUNG:
ĐIỀU KHIỂN SỰ CHÚ Ý SAU XỬ LÝ CHO CÁC LLM
Qingru Zhang†∗, Chandan Singh⋄, Liyuan Liu⋄, Xiaodong Liu⋄, Bin Yu‡,
Jianfeng Gao⋄,Tuo Zhao†
†Viện Công nghệ Georgia‡Đại học California, Berkeley⋄Microsoft Research
{qingru.zhang,tourzhao }@gatech.edu
binyu@berkeley.edu
{chansingh,lucliu,xiaodl,jfgao }@microsoft.com
TÓM TẮT
Trong các bài viết do con người viết, chúng ta thường tận dụng những tinh tế của phong cách văn bản, chẳng hạn như chữ đậm và chữ nghiêng, để hướng dẫn sự chú ý của người đọc. Những nhấn mạnh văn bản này rất quan trọng để người đọc nắm bắt thông tin được truyền đạt. Khi tương tác với các mô hình ngôn ngữ lớn (LLMs), chúng ta có nhu cầu tương tự – điều khiển mô hình để chú ý nhiều hơn đến thông tin do người dùng chỉ định, ví dụ như một hướng dẫn. Tuy nhiên, các phương pháp hiện tại bị hạn chế chỉ xử lý văn bản thuần túy và không hỗ trợ cơ chế như vậy. Điều này thúc đẩy chúng tôi giới thiệu PASTA – Phương pháp Điều khiển Sự chú ý Sau xử lý, một phương pháp cho phép LLMs đọc văn bản với các dấu nhấn mạnh do người dùng chỉ định. Để đạt được điều này, PASTA xác định một tập con nhỏ các đầu chú ý và áp dụng việc tính trọng số lại chú ý chính xác trên chúng, hướng sự chú ý của mô hình đến các phần do người dùng chỉ định. Giống như prompting, PASTA được áp dụng tại thời điểm suy luận và không yêu cầu thay đổi bất kỳ tham số mô hình nào. Các thí nghiệm chứng minh rằng PASTA có thể cải thiện đáng kể khả năng của LLM trong việc tuân theo hướng dẫn của người dùng hoặc tích hợp kiến thức mới từ đầu vào của người dùng, dẫn đến cải thiện hiệu suất đáng kể trên nhiều tác vụ khác nhau, ví dụ như cải thiện độ chính xác trung bình 22% cho LLAMA-7B. Mã nguồn của chúng tôi được công khai tại https://github.com/QingruZhang/PASTA .

1 GIỚI THIỆU
Sự ra đời của các mô hình ngôn ngữ lớn (LLMs) đã đánh dấu một cột mốc quan trọng trong xử lý ngôn ngữ tự nhiên (NLP) và trí tuệ nhân tạo (AI), thể hiện hiệu suất đặc biệt trên nhiều tác vụ (Vaswani et al., 2017; Brown et al., 2020a; OpenAI, 2023). Những nỗ lực tinh chỉnh thêm các mô hình này đã không ngừng nghỉ, nhằm cho phép chúng xử lý và phản hồi ngôn ngữ tự nhiên và lập trình với chuyên môn giống con người (Stiennon et al., 2020; Yao et al., 2023).

Mặc dù có những thành tựu đáng kể, LLMs thường gặp khó khăn trong việc hiểu đầu vào ngữ cảnh của chúng trong quá trình tương tác với người dùng (Shen et al., 2023; Lu et al., 2021). Khó khăn này trở nên đặc biệt rõ ràng khi chúng được trình bày các prompts¹ chứa ngữ cảnh nền rộng lớn hoặc hướng dẫn phức tạp của người dùng. Ngữ cảnh dài có thể áp đảo LLMs, vì các mô-đun chú ý của chúng, được học từ dữ liệu, không thể nắm bắt đầy đủ các chi tiết quan trọng (Liu et al., 2023). Các hướng dẫn phức tạp có thể tiếp tục cản trở mô hình tập trung vào ý định của người dùng, dẫn đến các đầu ra không mong muốn (Wei et al., 2022). Ngoài ra, đối với dữ liệu nhạy cảm theo thời gian, chẳng hạn như bài báo tin tức, có thể tồn tại kiến thức thực tế trong ngữ cảnh, mâu thuẫn với niềm tin trước đó của mô hình được tạo ra từ việc tiền huấn luyện lỗi thời. Kết quả là, một mô hình có thể tạo ra đầu ra được điều kiện hóa trên niềm tin có sẵn của nó thay vì chú ý đến các sự kiện mới trong ngữ cảnh (Meng et al., 2022a;b; Mitchell et al., 2022; Hernandez et al., 2023). Tất cả những thách thức này góp phần làm cho LLMs gặp khó khăn trong việc hiểu ý định của người dùng.

So với LLMs, người đọc hiếm khi gặp khó khăn trong việc hiểu những nhấn mạnh của bài viết và ý định của người viết. Các nhà văn thường tận dụng nhiều phong cách văn bản khác nhau, chẳng hạn như chữ đậm và chữ nghiêng, để nhấn mạnh nội dung cụ thể. Cơ chế này cho phép các nhà văn hướng dẫn và duy trì sự chú ý của

∗Công việc được hoàn thành trong thời gian thực tập của Qingru Zhang tại Microsoft Research.
¹Chúng tôi sử dụng prompts để chỉ tất cả các đầu vào văn bản LLM, bao gồm hướng dẫn của người dùng, và các thông tin nền khác (mà chúng tôi gọi là ngữ cảnh).
1arXiv:2311.02262v2  [cs.CL]  1 Oct 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Đầu ra gốc:Mary là một chuyên gia làm việcĐầu 1Lớp 1Lớp 2…Lớp L…Đầu vào được điều khiển của người dùng:Mary là một bác sĩ...*Trả về nghề nghiệp của cô ấy theo định dạng json*Nhấn mạnh điểm số chú ý cho các vị trí token được chọn…Đối với mỗi đầu được chọn:Đầu 2Đầu hĐầu ra được điều khiển: {"Name": "Mary",  "Occupation": "Doctor"}Đầu vào gốc của người dùng: Mary là một bác sĩ...Trả về nghề nghiệp của cô ấy theo định dạng jsonLLM……
Chú ý đến 'định dạng json'
Hình 1: PASTA sử dụng một phần do người dùng chỉ định của đầu vào để điều khiển việc tạo mô hình phù hợp với ý định của người dùng. PASTA sửa đổi các điểm số chú ý được tạo ra trong quá trình suy luận, bằng cách nhấn mạnh các điểm số được tạo ra tại các vị trí token tương ứng với phần do người dùng chỉ định của ngữ cảnh.

người đọc, đảm bảo rằng thông tin dự định được nắm bắt chính xác. Trong các tương tác giữa người dùng và LLMs, người dùng cũng cần làm nổi bật thông tin cụ thể cho mô hình. Do đó, việc tạo mô hình có thể được thiên vị hiệu quả theo hướng dẫn của người dùng, từ đó giải quyết những thách thức được đề cập trước đó. Tính năng này đặc biệt cần thiết khi thiết kế giao diện người dùng-AI, và có thể được áp dụng thường xuyên trong các cuộc hội thoại dài giữa người dùng và mô hình. Tuy nhiên, các phương pháp hiện tại không hỗ trợ cơ chế như vậy. LLMs về bản chất bị hạn chế trong việc xử lý văn bản thuần túy, không có bất kỳ tín hiệu phong cách hoặc dấu nhấn mạnh nào (Brown et al., 2020b; Liu et al., 2021; Wei et al., 2022). Ngay cả khi các dấu nhấn mạnh được thêm vào prompts, các LLMs tiên tiến thường gặp khó khăn trong việc phân biệt các tín hiệu yếu từ một vài token đánh dấu (Xem bằng chứng trong Phần 5.1).

Được thúc đẩy bởi nhu cầu truyền đạt sự nhấn mạnh của người dùng, chúng tôi giới thiệu PASTA (Phương pháp Điều khiển Sự chú ý Sau xử lý), một phương pháp sau xử lý² cho phép người dùng làm nổi bật thông tin cụ thể, ví dụ như một hướng dẫn như trong Hình 1, và điều khiển mô hình để diễn giải văn bản được nhấn mạnh như người đọc. Cụ thể, PASTA chọn một tập con nhỏ các đầu chú ý và áp dụng việc tính trọng số lại chú ý chính xác trên chúng. Như được minh họa trong Hình 1, PASTA tăng trọng số các điểm số chú ý của các token do người dùng chỉ định trong khi giảm trọng số các token khác tại các đầu chú ý cụ thể. Phương pháp của chúng tôi được lấy cảm hứng từ quan sát rằng các mô-đun chú ý thể hiện nhiều mẫu chú ý token khác nhau trên các đầu khác nhau (Michel et al., 2019; V oita et al., 2019; Clark et al., 2019). Những mẫu chú ý này có thể được diễn giải là mã hóa thông tin ngữ nghĩa hoặc cú pháp đa dạng, và việc thay đổi chúng có thể ảnh hưởng đáng kể đến hành vi của mô hình (Shi et al., 2023a; Hu et al., 2021b). Thông qua điều khiển các mô-đun chú ý, PASTA hướng mô hình chú ý chặt chẽ đến các phần do người dùng chỉ định và do đó tạo ra đầu ra mong muốn phù hợp với nội dung được làm nổi bật. Đáng chú ý là, PASTA được áp dụng sau khi huấn luyện và không yêu cầu thay đổi bất kỳ tham số mô hình nào; PASTA chỉ yêu cầu truy cập vào các điểm số chú ý của các đầu cụ thể của một LLM.

Vì các đầu chú ý có thể phục vụ các chức năng khác nhau (Tenney et al., 2019; Deb et al., 2023), chúng tôi giới thiệu một thuật toán lập hồ sơ mô hình hiệu quả để xác định đầu nào hiệu quả cho việc điều khiển. Cụ thể, chúng tôi lấy mẫu phụ các tập huấn luyện nhỏ từ nhiều tác vụ và đánh giá hiệu suất của việc điều khiển chú ý cho từng đầu riêng lẻ trên các tác vụ này. PASTA chọn các đầu chú ý mà khi được điều khiển, thường cải thiện hiệu suất đa tác vụ. Chúng tôi quan sát thực nghiệm rằng việc điều khiển các đầu này không chỉ có lợi cho các tác vụ hiện tại mà còn tăng cường hiệu suất trên các tác vụ chưa thấy. Đáng chú ý là, việc lập hồ sơ mô hình chỉ được thực hiện một lần cho một LLM. Các đầu chú ý được chọn có thể được coi là một hồ sơ cấp mô hình, hiệu quả cho việc điều khiển LLM trên các tác vụ chưa thấy.

Chúng tôi tiến hành thí nghiệm trên các tác vụ đa dạng để chứng minh hiệu quả của PASTA. Cụ thể, chúng tôi đánh giá PASTA sử dụng GPT-J-6B (Wang & Komatsuzaki, 2021) và LLAMA-7B (Touvron et al., 2023) trên các tác vụ bao gồm hướng dẫn phức tạp, ngữ cảnh dài và xung đột kiến thức trong ngữ cảnh. Kết quả chứng minh rằng PASTA liên tục cung cấp cải thiện hiệu suất đáng kể so với các chiến lược prompting cơ bản. Ví dụ, PASTA đạt được cải thiện độ chính xác trung bình 22% so với few-shot prompting cho LLAMA-7B trên 4 tác vụ thách thức.

²Sau xử lý có nghĩa là phương pháp của chúng tôi không cập nhật trọng số mô hình.
2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

2 BỐI CẢNH
Mô tả vấn đề Trong prompting LLM tiêu chuẩn, chúng ta được cho một LLM được tiền huấn luyện và một prompt văn bản x. Trong thiết lập của chúng tôi, chúng ta bổ sung yêu cầu (i) truy cập vào các điểm số chú ý được tạo ra bởi các mô-đun chú ý trong LLM³ và (ii) chúng ta được cung cấp một tập con do người dùng chỉ định của prompt xg⊂x để được nhấn mạnh.

Như trong ví dụ trong Hình 1, x có thể là một chuỗi kết thúc bằng một hướng dẫn, chẳng hạn như Mary là một bác sĩ nhưng từng là một y tá...Trả về nghề nghiệp của cô ấy theo định dạng json. Nếu người dùng nhấn mạnh hướng dẫn, xg đơn giản có thể là hướng dẫn cuối cùng Trả về nghề nghiệp của cô ấy theo định dạng json. Trong các tập dữ liệu đánh giá, chúng tôi giả định rằng phần do người dùng chỉ định của mỗi ví dụ đã được cung cấp bằng cách bao quanh ở cả hai đầu trong một số dấu nhấn mạnh, như dấu '∗' trong Markdown. Việc tạo ra những dữ liệu có cấu trúc tốt này thường phát sinh chi phí ít. Ví dụ, trong tập dữ liệu được thiết kế để đánh giá khả năng của mô hình trong việc tuân theo hướng dẫn của người dùng, chúng ta có thể đơn giản đánh dấu hướng dẫn cuối cùng cho mỗi ví dụ, là cố định và được chia sẻ giữa các ví dụ. Khi nói đến giao diện người dùng-LLM, người dùng có thể chỉ định xg bằng cách bao quanh nó với các dấu nhấn mạnh tương tự. xg có thể được chỉ định một cách linh hoạt. Cụ thể, nó không cần phải là một khoảng liên tục, và có thể được sử dụng để nhấn mạnh thông tin đa dạng.

Chú ý Đa-đầu. Một mô hình transformer điển hình bao gồm L lớp xếp chồng, trong đó mỗi lớp chứa hai mô-đun con: một chú ý đa-đầu (MHA) và một mạng feed-forward kết nối đầy đủ (FFN). Cho đầu vào X∈Rn×d, MHA của lớp l thực hiện hàm chú ý song song H đầu: MHA(l)(X) =Concat (H(l,1), ...,H(l,H))Wo trong đó
H(l,h)=A(l,h)V=Softmax
QK⊤/p
dh
V (1)
trong đó Q=XW qh,K=XW kh,V=XW vh và Wqh,Wkh,Wvh∈Rd×dh là các ma trận chiếu có thể học được của đầu h. dh thường được đặt là d/H. Cụ thể, ký hiệu các điểm số chú ý tại đầu h của lớp thứ l là A(l,h).

3 PHƯƠNG PHÁP
PASTA (Thuật toán 1) bao gồm hai thành phần: (i) điều khiển chú ý sau xử lý, nhấn mạnh các phần do người dùng chỉ định của đầu vào trong quá trình suy luận, xem Phần 3.1 và (ii) lập hồ sơ mô hình đa tác vụ, chọn các đầu chú ý hiệu quả cho việc điều khiển, xem Phần 3.2.

Thuật toán 1 PASTA: Phương pháp Điều khiển Sự chú ý Sau xử lý
Lập hồ sơ mô hình đa tác vụ (Phần 3.2)
1:Đầu vào: các tập huấn luyện nhỏ {D(i)}m
i=1, các siêu tham số α,k;
2:for1≤i≤mdo
3: for1≤l≤L,1≤h≤Hdo
4: Đánh giá hiệu suất mô hình trên D(i) khi điều khiển đầu (l, h) bằng (2);
5: Trả về kết quả đánh giá của việc điều khiển (l, h) trên D(i);
6: end for
7: Thu thập kết quả điều khiển của tất cả các đầu và trả về lập hồ sơ tác vụ R(i);
8:end for
9:Đầu ra: Tập đầu chú ý H=∩m
i=1R(i)
1:k.
Điều khiển tại thời điểm suy luận (Phần 3.1)
1:Đầu vào: đầu vào văn bản x, các đoạn được gạch chân bởi người dùng G, hệ số α;
2:Đầu ra: các thế hệ mô hình trong khi điều khiển mỗi đầu (l, h) trong H bằng (2).

3.1 ĐIỀU KHIỂN CHÚ Ý SAU XỬ LÝ
PASTA nhấn mạnh tập con đầu vào do người dùng chỉ định bằng cách giảm trọng số các điểm số chú ý của các token không được chỉ định bởi người dùng. Cụ thể, cho tập chỉ số của các khoảng đầu vào được làm nổi bật là G, PASTA nhấn mạnh các token do người dùng chỉ định này bằng một phép chiếu chú ý T:
H(l,h)=T(A(l,h))V,trong đó [T(A)]ij=
αAij/Ci nếu j∈ G−
Aij/Ci ngược lại .(2)

³Chúng tôi không cần truy cập trọng số mô hình cũng như đầu ra trung gian từ các mô-đun khác như FFNs.
3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

trong đó 0≤α <1 là một hệ số tỷ lệ và G−= [n]− G là tập chỉ số của các token không nằm trong G. Thuật ngữ Ci=P
j∈GAij+P
j∈G−αAij chuẩn hóa các điểm số để chúng tổng bằng một. Việc điều khiển chú ý (2) được tiến hành trong thời gian suy luận và không yêu cầu bất kỳ huấn luyện nào.

(2) điều khiển sự chú ý của mô hình bằng cách thu nhỏ các điểm số của các token không được làm nổi bật bởi người dùng. Khi hệ số α được đặt rất nhỏ, các đoạn do người dùng chỉ định được làm nổi bật do các điểm số chú ý tăng của chúng sau khi chuẩn hóa lại. Do đó, chúng ta có thể hướng mô hình tập trung nhiều hơn vào các token do người dùng chỉ định, thiên vị việc tạo ra để phù hợp với nội dung được chỉ định.

PASTA thu nhỏ các điểm số chú ý của các token không được chỉ định bằng α. Như việc chuẩn hóa lại được theo sau, nó tương đương với việc mở rộng các điểm số chú ý của các token do người dùng chỉ định bằng 1/α. Lý do của việc chọn (2) là nó có thể ổn định về mặt số học hơn so với việc mở rộng điểm số. Thay vào đó, người ta cũng có thể tỷ lệ các điểm số chú ý bằng cách thêm một hằng số dương vào các token được gạch chân G. Lý do chúng tôi chọn phép nhân trong (2) thay vì phép cộng là nó bảo tồn sự khác biệt về độ lớn chú ý giữa các token được làm nổi bật. Như vậy, thao tác điều khiển chỉ điều chỉnh tỷ lệ chú ý tổng thể của hai nhóm token. Ngược lại, việc cộng bằng một hằng số lớn vào các token được làm nổi bật dẫn đến các điểm số chú ý của chúng được phân phối gần như đồng đều, dẫn đến mất thông tin không cần thiết và suy thoái hiệu suất.

3.2 LẬP HỒ SƠ MÔ HÌNH ĐA TÁC VỤ
Thực nghiệm, chúng tôi thấy rằng việc áp dụng điều khiển chú ý trong (2) cho tất cả các đầu chú ý hoạt động kém hơn so với việc áp dụng nó chỉ cho các đầu cụ thể (xem Phần 5.3). Điều quan trọng là phải chỉ định các đầu chú ý đúng, cho rằng các đầu khác nhau phục vụ các vai trò khác biệt trong việc mã hóa thông tin ngữ nghĩa/cú pháp.

Để đạt được điều này, chúng tôi đề xuất một thuật toán lập hồ sơ mô hình đa tác vụ để xác định các đầu chú ý hiệu quả cho việc điều khiển. Cụ thể, cho m tác vụ liên quan đến nhấn mạnh của người dùng, chúng tôi lấy mẫu phụ một tập huấn luyện nhỏ D(i)(ví dụ,|D(i)|= 1000) từ mỗi tác vụ i. Sau đó, chúng tôi đánh giá hiệu suất của việc điều khiển mỗi đầu chú ý riêng lẻ (l, h)(1≤l≤L,1≤h≤H) trên mỗi tập con nhỏ D(i)(1≤i≤m).

Đối với mỗi tác vụ i, chúng tôi xếp hạng tất cả các đầu theo hiệu suất điều khiển của chúng trên D(i) và coi thứ hạng R(i)= [(l1, h1),(l2, h2), . . .] là lập hồ sơ của tác vụ i. Sau đó chúng tôi đặt tập đầu chú ý H cho việc điều khiển là giao của k đầu hiệu suất cao nhất, H=∩m
i=1R(i)
1:k (xem Phần 5.3 cho các lựa chọn thay thế). Một cách trực quan, chúng ta mong đợi hiệu suất được cải thiện khi số lượng tác vụ m tăng.

Giống như điều khiển chú ý, lập hồ sơ mô hình chỉ yêu cầu truy cập vào các điểm số chú ý, ngoài các đầu vào và đầu ra của nó (trọng số và gradient mô hình không được yêu cầu). Quan trọng là, quá trình này chỉ cần được thực hiện một lần cho một LLM, tương tự như finetuning. Tuy nhiên, không giống như finetuning, điều khiển mô hình không sửa đổi trọng số mô hình và quan trọng hơn, tổng quát hóa cho các tác vụ mới. Tập đầu H kết quả có thể được coi là một hồ sơ cấp mô hình. Một khi nó được xác định, chúng ta có thể áp dụng việc điều khiển chú ý trên H cho cả các tác vụ hiện tại và các tác vụ chưa thấy để tăng cường hiểu biết ngữ cảnh của mô hình và có lợi cho hiệu suất downstream.

4 THIẾT LẬP THÍ NGHIỆM
Các tác vụ và chỉ số đánh giá. Chúng tôi triển khai PASTA cho hai mô hình được tiền huấn luyện: GPT-J (6 tỷ tham số, (Wang & Komatsuzaki, 2021)) và LLaMA-7B (7 tỷ tham số, (Touvron et al., 2023)). Chúng tôi đánh giá hiệu quả của PASTA tại (i) xử lý hướng dẫn phức tạp của người dùng, (ii) diễn giải ngữ cảnh dài, và (iii) giải quyết xung đột kiến thức trong ngữ cảnh. Đối với (i), chúng tôi giới thiệu hai tác vụ mới: Định dạng JSON và Thay đổi đại từ. Đối với (ii) và (iii), chúng tôi nghiên cứu Bias in Bios (De-Arteaga et al., 2019) và CounterFact (Meng et al., 2022a). Đối với mỗi tác vụ, chúng tôi cung cấp một mô tả, mô tả phần nào của đầu vào chúng tôi nhấn mạnh, và các chỉ số nào chúng tôi sử dụng để đánh giá (xem Phụ lục A để biết chi tiết đầy đủ về tập dữ liệu).

•Định dạng JSON là một tác vụ mới đánh giá khả năng của LLM trong việc tạo ra đầu ra theo định dạng mong muốn của người dùng (JSON). Đây là một trường hợp sử dụng quan trọng cho LLMs khi đầu ra của chúng được sử dụng trong một quá trình downstream. Tác vụ này sử dụng dữ liệu tiểu sử từ BiasBios (được mô tả bên dưới) nhưng nối thêm một hướng dẫn khác vào cuối ngữ cảnh: trả lời nghề nghiệp của {person} và tạo ra câu trả lời theo định dạng JSON. Hướng dẫn này thúc đẩy các mô hình tạo ra đầu ra theo định dạng JSON.
Chúng tôi nhấn mạnh hướng dẫn cuối cùng
4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Chỉ số: (a) Độ chính xác định dạng (F. Acc.) đo độ chính xác trong việc tạo ra JSON hợp lệ. (b) Độ chính xác dự đoán (P. Acc.) đo độ chính xác trong việc tạo ra mục tiêu đúng trong các giá trị JSON sau khi tải các thế hệ định dạng JSON.

•Thay đổi đại từ là một tác vụ mới đánh giá khả năng của LLM trong việc tuân theo một hướng dẫn khó khăn của người dùng. Nó lại sử dụng các ngữ cảnh tiểu sử từ BiasBios nhưng thay vào đó hướng dẫn các mô hình: thay thế 'she' và 'he' bằng 'they' và tạo ra nghề nghiệp của {person} sau khi thay đổi đại từ.
Chúng tôi nhấn mạnh hướng dẫn cuối cùng.
Chỉ số: (a) Độ chính xác đánh giá tỷ lệ 'she/he' được thay đổi thành công thành 'they' trong các thế hệ mô hình. (b) Độ chính xác tất cả được thay đổi (A. Acc.) là tỷ lệ mà các mô hình thay thế tất cả các đại từ tương ứng, tức là thay đổi she/he/her/him/hers/his thành they/them/their/theirs.

•CounterFact đo khả năng của LLM trong việc tạo ra văn bản nhất quán với một sự kiện mới. Mỗi ví dụ bao gồm (subject, relation, old target, new target), ví dụ như (Kevin Garnett, is a professional, basketball player, baseball player). Chúng tôi trình bày mô hình cả sự kiện cũ và mới theo prompt: Trước đây, {old fact}, nhưng hiện tại, {new fact}. {question}. Sự thay đổi này trong các sự kiện theo thời gian thường làm bối rối LLMs, dẫn đến các phỏng đoán ngẫu nhiên trên hai trong số chúng khi trả lời {question}.
Chúng tôi nhấn mạnh khoảng đầu vào chứa sự kiện mới.
Chỉ số: chúng tôi đánh giá các chỉ số theo (Meng et al., 2022a): (a) Điểm hiệu quả (ES) là phần các trường hợp mà mô hình có PLLM(new target) > PLLM(old target); (b) Điểm paraphrase (PS) giống như ES nhưng thay đổi {question} với một tập các câu hỏi được diễn đạt lại để đánh giá khả năng tổng quát hóa

•BiasBios bao gồm các tiểu sử nghề nghiệp của những người không nổi tiếng, ban đầu được giới thiệu để điều tra thiên vị giới tính trong các nghề nghiệp. Mỗi ví dụ bao gồm ngữ cảnh tiểu sử và một nhãn của nghề nghiệp mục tiêu. Câu đầu tiên đề cập đến nghề nghiệp của người đó, và các câu tiếp theo mô tả lịch sử nghề nghiệp của cá nhân nhưng có thể không liên quan trực tiếp đến dự đoán, có khả năng làm mất tập trung sự chú ý của mô hình. Ở cuối ngữ cảnh, chúng tôi nối thêm câu hỏi: {person} có nghề nghiệp là.
Chúng tôi nhấn mạnh câu đầu tiên, vì nó mang thông tin nhiều nhất về nghề nghiệp.
Chỉ số: theo (Hernandez et al., 2023), chúng tôi tính Độ chính xác bằng cách kiểm tra xem xác suất được gán cho nghề nghiệp mục tiêu có cao nhất trong số 28 nghề nghiệp ứng viên hay không.

Đối với Thay đổi đại từ, CounterFact, và BiasBios, chúng tôi bổ sung đo Độ trôi chảy như entropy bi-gram và tri-gram trung bình của các thế hệ, được thiết kế để thấp cho các văn bản thoái hóa hoặc lặp lại (Meng et al., 2022a). Chúng tôi lọc ra bất kỳ kết quả nào nhận được độ trôi chảy dưới 3.0 (xem kết quả đầy đủ bao gồm độ trôi chảy trong Phụ lục B.1).

Các baseline. Chúng tôi so sánh PASTA với các baseline sau:
•Zero-shot prompting là cách tiếp cận phổ biến nhất để tương tác với LLMs, trong đó người dùng cung cấp cho mô hình một prompt chứa ngữ cảnh nền và một hướng dẫn hoặc câu hỏi của người dùng.
•Marked prompting thay đổi các prompts được sử dụng trong zero-shot prompting bằng cách bao quanh các khoảng đầu vào do người dùng chỉ định với các dấu nhấn mạnh, ví dụ như dấu hoa thị, như được thực hiện trong các tệp markdown để nhấn mạnh, hoặc dấu ngoặc kép, như được thực hiện trong ngôn ngữ tự nhiên.
•Few-shot prompting bao gồm các minh họa (ví dụ đầu vào và đầu ra mục tiêu) ở đầu prompt được cung cấp cho LLM. Few-shot prompting thường cải thiện hiệu suất trong các tác vụ mới, nhưng tăng chi phí tính toán của suy luận do độ dài prompt tăng, đặc biệt khi các minh họa dài (Dong et al., 2023); ở đây chúng tôi sử dụng 3 minh họa trong ngữ cảnh.

Cài đặt PASTA Chúng tôi nghiên cứu PASTA trong 2 cài đặt: đa tác vụ và bất khả tri tác vụ. Trong cài đặt đa tác vụ, tác vụ đánh giá j được bao gồm để lập hồ sơ, trong khi trong cài đặt bất khả tri tác vụ, tác vụ đánh giá bị loại trừ (thay vào đó, chúng tôi lập hồ sơ trên 3 tập dữ liệu ngoài j). Cài đặt đa tác vụ cải thiện hiệu suất nhưng yêu cầu các mẫu huấn luyện được gắn nhãn cho tác vụ được đánh giá, điều này có thể khó có được trong thực tế.

Thực nghiệm, chúng tôi thấy rằng PASTA không nhạy cảm với hệ số tỷ lệ α (xem Phần 5.3) và cố định nó ở 0.01 trong các thí nghiệm của chúng tôi. Chúng tôi chọn 1000 mẫu huấn luyện từ mỗi trong 4 tác vụ trên cho việc lập hồ sơ mô hình. Sau khi lập hồ sơ mô hình, chúng tôi chọn k từ {300, 400, 500} cho LLAMA-7B
5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

để có số lượng đầu được điều khiển |H| là {25, 53, 86}. Chúng tôi thấy rằng PASTA đạt hiệu suất tốt nhất trên LLAMA-7B khi 50≤ |H| ≤ 100, tức là k= 400 hoặc k= 500. Đối với GPT-J, chúng tôi chọn k từ {250, 275, 300, 350} để có |H| là {52, 72, 111, 153}. Đối với mỗi tác vụ, chúng tôi chia dữ liệu thành các tập train/validation/test theo (Hernandez et al., 2023) (Xem Phụ lục A) và chọn |H| bằng cross validation. Đối với tất cả các tác vụ, đầu ra mô hình được tạo ra với tìm kiếm tham lam.

5 KẾT QUẢ
5.1 KẾT QUẢ CHÍNH: PASTA CẢI THIỆN VIỆC TẠO MÔ HÌNH
Bảng 1 và 2 trình bày các kết quả chính cho PASTA được áp dụng cho LLAMA-7B và GPT-J tương ứng. Few-shot prompting là baseline mạnh nhất, và PASTA bất khả tri tác vụ vượt trội hơn nó trên chỉ số chính cho mỗi tác vụ cho tất cả các cài đặt ngoại trừ Định dạng JSON với GPT-J. PASTA đa tác vụ vượt trội hơn tất cả các baseline trên tất cả các cài đặt.

PASTA có thể cải thiện việc tuân theo hướng dẫn của LLM. Kết quả từ các tác vụ Định dạng JSON và Thay đổi đại từ cho thấy rằng, bằng cách làm nổi bật hướng dẫn của người dùng ở cuối đầu vào, PASTA hiệu quả điều khiển các mô hình tập trung vào ý định của người dùng, từ đó thiên vị việc tạo ra của chúng để thực hiện các yêu cầu hoặc định dạng cụ thể. Ví dụ, trong khi GPT-J chỉ đạt được 39.9% các thế hệ zero-shot của nó tuân thủ yêu cầu của người dùng trên tác vụ Thay đổi đại từ, PASTA mang lại cải thiện độ chính xác đáng kể 53% bằng cách nhấn mạnh hướng dẫn. Hơn nữa, PASTA đạt được 96.64% độ chính xác định dạng ấn tượng và 85.09% độ chính xác dự đoán khi được áp dụng cho LLAMA-7B trên tác vụ Định dạng JSON. Hiệu suất này vượt quá few-shot prompting 11%, mặc dù few-shot prompting cung cấp cho mô hình một cách rõ ràng các ví dụ JSON đúng thông qua các minh họa bổ sung. Bảng 3 trình bày một vài ví dụ được tạo ra bởi LLAMA-7B khi áp dụng PASTA.

PASTA có thể giúp các mô hình nắm bắt thông tin ngữ cảnh quan trọng. Trong trường hợp các tác vụ BiasBios và CounterFact, chúng tôi áp dụng PASTA để nhấn mạnh các khoảng ngữ cảnh cụ thể cho LLMs. Do đó, các mô hình được hướng dẫn chú ý chặt chẽ đến thông tin ngữ cảnh cụ thể hoặc các sự kiện mới trong
6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 3: Ví dụ tạo của LLAMA-7B trên các tác vụ Định dạng JSON và Thay đổi đại từ.
Tác vụ Prompt Tạo Zero-shot Tạo PASTA
JSON
FormatWinnie là một nhiếp ảnh gia người Mỹ
sống ở New York. Chuyên về nhiếp
ảnh thời trang và chân dung, cô áp
dụng tài năng của mình trên cả con
người và động vật . . . {instruction}✗Winnie là một nhiếp ảnh gia thời trang
và chân dung sống ở New York, chuyên
về nhiếp ảnh thời trang và chân dung,
người áp dụng tài năng của cô trên cả
con người và động vật.✓{"name": "Winnie", "occupation":
"photographer"}

Prons.
ChangeMary là một Phó Giáo sư tại Khoa
Chương trình Giảng dạy tại Đại học
St. John; cô có bằng tiến sĩ về Đọc/
Viết/Chữ viết từ Đại học Pennsylvania.
Cô gần đây đã xuất bản một bài báo
trong "Urban Education" với . . . {instruction}✗Mary là một giáo sư tại Khoa Chương
trình Giảng dạy tại Đại học St. John;
cô có bằng tiến sĩ về Đọc/Viết/Chữ
viết từ Đại học Pennsylvania. Cô gần
đây đã xuất bản một bài báo trong "Urban Education" với . . .✓Mary là một phó giáo sư tại khoa
chương trình giảng dạy và hướng dẫn
tại Đại học St. John; họ có bằng tiến
sĩ về đọc/viết/chữ viết từ Đại học
Pennsylvania. Họ gần đây đã xuất bản
một bài báo trong . . .

ngữ cảnh. Kết quả từ hai tác vụ này minh họa rằng PASTA có thể hướng các mô hình diễn giải thông tin quan trọng hoặc giải quyết các xung đột kiến thức trong ngữ cảnh, mang lại cải thiện đáng kể trong hiệu suất dự đoán của cả hai tác vụ. Ví dụ, PASTA đạt được độ chính xác dự đoán 94.96% cho GPT-J trên tác vụ BiasBios, cao hơn baseline tốt nhất 16.32%.

Bảng 1 và 2 cũng gợi ý rằng marked prompting, một baseline làm nổi bật các văn bản cụ thể giống như các nhà văn, gặp khó khăn trong việc truyền đạt hiệu quả sự nhấn mạnh cho LLMs. Một lý do có thể là các dấu nhấn mạnh này hiếm khi xuất hiện trong dữ liệu tiền huấn luyện khổng lồ. Ngược lại, few-shot prompting đôi khi dẫn đến cải thiện trong hiệu suất mô hình. Tuy nhiên, một nhược điểm của few-shot prompting là tính không ổn định của nó, tức là hiệu suất của nó thể hiện phương sai cao trên các mẫu khác nhau trong minh họa (Xem Phụ lục B).

5.2 PASTA CÓ THỂ GIẢM THIỂU ĐỘ NHẠY CẢM CỦA PROMPTS

Bảng 4: Kết quả về độ nhạy cảm của hiệu suất mô hình đối với việc diễn đạt lại prompt trên tác vụ Định dạng JSON. Cho các hướng dẫn được diễn đạt lại trong mẫu prompt, PASTA có thể cải thiện hiệu suất zero-shot cho tất cả các prompts.

Hướng dẫn PhươngphápLLAMA-7B GPT-J
Trung bình Định dạng JSON
F. Acc / P. AccThay đổi Prons.
Acc / A. AccĐịnh dạng JSON
F. Acc / P. AccThay đổi Prons.
Acc / A. Acc

GốcZero-shot 60.0 / 54.9 71.8 / 66.3 28.8 / 25.1 39.9 / 36.2 47.9
PASTA 96.6 / 85.1 96.4 / 95.8 91.5 / 18.6 93.0 / 91.3 83.5

Rút gọnZero-shot 36.0 / 32.4 49.2 / 42.6 25.4 / 17.1 56.5 / 54.8 39.3
PASTA 87.4 / 65.9 89.0 / 86.9 54.1 / 37.0 94.0 / 93.7 76.0

Diễn đạt lạiZero-shot 57.9 / 54.2 82.3 / 79.6 63.3 / 50.3 76.0 / 72.8 67.1
PASTA 97.1 / 87.1 89.6 / 89.0 77.5 / 68.1 94.8 / 92.3 86.9

Điều được biết rộng rãi là hiệu suất của LLMs có thể nhạy cảm với những thay đổi nhỏ trong prompts, chẳng hạn như diễn đạt lại và định dạng lại, ngay cả khi những prompts này truyền đạt cùng một ý nghĩa (Reynolds & McDonell, 2021; Liu et al., 2021). Chúng tôi thấy rằng PASTA có thể làm giảm độ nhạy cảm của hiệu suất mô hình đối với các prompts khác nhau. Cụ thể, Bảng 4 đánh giá hiệu suất của LLAMA-7B và GPT-J trên tác vụ Định dạng JSON và Thay đổi đại từ cho các hướng dẫn khác nhau trong mẫu prompt, tất cả đều truyền đạt cùng một ý nghĩa (xem các prompts chính xác trong Phụ lục A.1). Kết quả cho thấy hiệu suất zero-shot nhạy cảm với các prompts khác nhau và có thể giảm đáng kể với các mẫu được tạo kém. Ngược lại, PASTA liên tục cải thiện hiệu suất mô hình so với zero-shot prompting cho tất cả các prompts, hiệu quả giảm thiểu độ nhạy cảm đối với các biến thể trong prompts.

5.3 PHÂN TÍCH VÀ ABLATIONS
Trong phần này, chúng tôi điều tra các lựa chọn siêu tham số khác nhau và quyết định mô hình hóa ảnh hưởng đến hiệu suất của PASTA.

Lập hồ sơ mô hình Hình 2 trình bày kết quả về tầm quan trọng của lập hồ sơ mô hình được giới thiệu trong Phần 3.2. Chúng tôi so sánh PASTA khi điều khiển các đầu được chọn với các lựa chọn hợp lý khác: điều khiển (i) tất cả đầu, (ii) toàn bộ lớp, hoặc (iii) các đầu riêng lẻ trên tác vụ Định dạng JSON (Xem
7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

1 4 8 12 16 20 24 28 32
Lớp0255075100F . Acc
Zero-shot
PASTA
Điều khiển tất cả đầu
Điều khiển toàn bộ lớp
Điều khiển đầu đơn

Hình 2: Hiệu suất của LLAMA-7B trên tác vụ Định dạng JSON khi chúng tôi điều khiển (i) tất cả đầu (xanh lá); (ii) toàn bộ lớp (vàng); và (iii) một đầu riêng lẻ trong một lớp (biểu đồ violin xanh). Hiệu suất khác nhau đáng kể giữa các lớp và giữa các đầu của một lớp.

Phụ lục B.3 cho các so sánh trên các tác vụ còn lại). Việc chọn đầu thông qua lập hồ sơ mô hình trong PASTA (đường đỏ) vượt trội đáng kể so với các cách tiếp cận khác. Điều khiển tất cả đầu (đường xanh đứt nét) làm giảm hiệu suất so với hiệu suất zero-shot cơ bản (đường đen đứt nét). Điều này có thể là do điều khiển tất cả đầu khuếch đại quá mức thông tin do người dùng chỉ định với chi phí của thông tin thiết yếu khác cần thiết cho việc tạo ra và dự đoán hiệu quả. Thú vị là, chúng tôi thấy rằng hiệu suất khác nhau đáng kể khi điều khiển các lớp khác nhau (vàng) hoặc đầu (biểu đồ violin xanh). Như đã đề cập trong Phần 1, các đầu chú ý đóng vai trò khác biệt trong việc mã hóa thông tin ngữ nghĩa và cú pháp đa dạng (Tenney et al., 2019). Khi điều khiển các đầu, được liên quan một cách thích hợp trong việc mã hóa thông tin do người dùng chỉ định, mô hình có thể được hướng dẫn để nắm bắt và củng cố những tín hiệu cụ thể này. Ngược lại, việc sửa đổi sự chú ý của các đầu không liên quan không chỉ không thể nhấn mạnh thông tin mong muốn mà còn can thiệp vào các chức năng ban đầu của chúng, dẫn đến suy thoái hiệu suất. Do đó, điều quan trọng là phải xác định các đầu hiệu quả thông qua lập hồ sơ mô hình trước khi áp dụng việc điều khiển.

Các chiến lược khác nhau để chọn đầu trong quá trình lập hồ sơ. Như được mô tả trong Phần 5.3, lập hồ sơ mô hình của chúng tôi chọn Giao của các đầu hiệu suất k hàng đầu để điều khiển trên nhiều tác vụ. Thay vào đó, khi đánh giá trên tác vụ j, chúng ta có thể chọn đầu để điều khiển với các chiến lược khác nhau: (i) Cụ thể theo tác vụ – điều khiển k2 đầu hiệu suất hàng đầu chỉ của tác vụ j, tức là R(j)
1:k2; hoặc (ii) Hợp – hợp của những đầu này trên nhiều tác vụ, tức là ∪m
i=1R(i)
1:k2. Bảng 5 so sánh hiệu suất của chúng. Sử dụng các đầu cụ thể theo tác vụ thay vì các đầu được chọn bằng giao đôi khi mang lại hiệu suất cải thiện, nhưng yêu cầu chọn một tập đầu khác nhau cho mỗi tác vụ mới.

Bảng 5: Các chiến lược chọn đầu khác nhau giữa các đầu cụ thể theo tác vụ hàng đầu, hợp trên nhiều tác vụ, và giao (mặc định được sử dụng trong PASTA).

PASTAJSON Format Prons. Changing BiasBios CounterFact All
F. Acc / P. Acc Acc / A.Acc Acc ES / PS Avg.
LLAMACụ thể theo tác vụ 95.56 / 86.83 98.52 / 98.02 97.62 99.18 / 99.24 96.57
Hợp 88.42 / 74.49 92.12 / 91.44 96.36 99.24 / 99.35 92.22
Giao 96.64 / 85.09 96.42 / 95.84 95.28 99.60 / 99.57 95.46
GPT-JCụ thể theo tác vụ 85.71 / 79.39 94.74 / 92.54 97.64 99.26 / 99.34 93.29
Hợp 72.61 / 64.89 89.68 / 87.76 95.56 99.82 / 99.83 88.21
Giao 91.50 / 18.63 92.96 / 91.34 94.96 98.62 / 98.79 85.22

Thay đổi số lượng đầu được điều khiển. Hình 3a và 3b minh họa hiệu suất của PASTA khi điều khiển số lượng đầu khác nhau trên hai tác vụ. Kết quả gợi ý rằng khi nhiều đầu hơn được bao gồm để điều khiển, mô hình tuân theo người dùng thậm chí còn chặt chẽ hơn, đạt hiệu quả cao hơn (JSON Format Acc. và Pron. Change Acc.). Tuy nhiên, tại một điểm nào đó, điều này dẫn đến giảm trong các chỉ số phản ánh chất lượng tạo (JSON Pred. Acc và Fluency). Do đó, có một sự đánh đổi giữa việc nhấn mạnh hiệu quả và chất lượng tạo. Việc nhấn mạnh quá mức có thể dẫn mô hình tập trung chỉ vào việc thỏa mãn các yêu cầu của người dùng và bỏ qua các phần khác. Do đó, chúng tôi khuyến nghị áp dụng PASTA cho một số lượng vừa phải các đầu (thường từ 50 đến 150), đạt được sự cân bằng giữa hiệu quả và chất lượng tạo.

Thay đổi hệ số tỷ lệ α. Hình 3c trình bày hiệu suất của PASTA trên hai tác vụ khi chúng tôi thay đổi hệ số tỷ lệ α. Kết quả cho thấy rằng PASTA khá mạnh mẽ đối với siêu
8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

2 10 25 53 86 168 294
# Heads7580859095100JSON Format Acc.
020406080
JSON Pred. Acc.
JSON Format Acc.
JSON Pred. Acc.
(a) JSON Format

2 10 25 53 86 168 294
# Heads80859095100Pron. Change Acc.
23456
Fluency
Pron. Change Acc.
Fluency (b) Prons. Change

0.05 0.01 0.002 10−310−40
a20406080100JSON Format Acc.
80859095100
Pron. Change Acc.
 JSON Format Acc.
Pron. Change Acc. (c) Varying α

Hình 3: Hiệu suất của việc áp dụng PASTA cho LLAMA-7B trên các tác vụ Định dạng JSON và Thay đổi đại từ khi thay đổi số lượng đầu được điều khiển |H|(3a,3b); và thay đổi hệ số tỷ lệ α(3c).

tham số này; trong thực tế, chúng tôi cố định nó là 0.01. Lưu ý rằng việc đặt α thành không nên được tránh, vì điều này dẫn đến việc loại bỏ hoàn toàn các ngữ cảnh quan trọng khác tại các đầu được điều khiển, dẫn đến suy thoái hiệu suất.

6 CÔNG TRÌNH LIÊN QUAN
Phương pháp chính để kiểm soát LLMs đã là thông qua prompting, thường mang lại những cải thiện ấn tượng trong hiệu suất (Brown et al., 2020b; Liu et al., 2021; Wei et al., 2022) và thúc đẩy một dòng công việc nhằm làm cho prompting dễ dàng hơn, ví dụ như (Strobelt et al., 2022; Bach et al., 2022; Shin et al., 2020; Deng et al., 2022; Singh et al., 2023b). Tuy nhiên, LLMs vẫn cực kỳ nhạy cảm với những sắc thái trong prompts (Webson & Pavlick, 2021; Lu et al., 2021); PASTA bổ sung cho những cách tiếp cận này bằng cách làm cho người dùng dễ dàng hơn trong việc chỉ định một prompt trong các tình huống khó khăn.

Một dòng công việc khác nhằm làm cho LLMs dễ tiếp cận hơn với prompting bằng cách sửa đổi chúng trong quá trình huấn luyện. Nổi bật nhất trong những cách tiếp cận này là instruction finetuning (Wei et al., 2021; Chung et al., 2022), Reinforcement Learning from Human Feedback (Ziegler et al., 2019; Ouyang et al., 2022), và các phương pháp liên quan khác, ví dụ như (Lee et al., 2023). Cũng có một vài phương pháp để chỉ định trực tiếp phần nào của đầu vào quan trọng trong quá trình huấn luyện, ví dụ như (Ross et al., 2017; Rieger et al., 2019; Schramowski et al., 2020; Krishna et al., 2023). PASTA có thể được sử dụng bổ sung cho những cách tiếp cận này để cải thiện một số khía cạnh của khả năng điều khiển mô hình (ví dụ như tuân theo hướng dẫn).

PASTA liên quan đến nhiều phương pháp khác nhau để thích ứng với các tác vụ mới, bao gồm LoRA (Hu et al., 2021a), AdaLoRA (Zhang et al., 2023), QLoRA (Dettmers et al., 2023), và TOAST (Shi et al., 2023b). PASTA cũng liên quan đến nhiều nghiên cứu về chỉnh sửa mô hình, ví dụ như ROME (Meng et al., 2022a), MEMIT (Meng et al., 2022b), MEND (Mitchell et al., 2022), và REMEDI (Hernandez et al., 2023). Không giống như những công trình này, PASTA bảo tồn khả năng của LLMs chuyển giao sang các tác vụ mới sử dụng prompts và thông tin được chọn bởi con người, thay vì sử dụng các ví dụ được gắn nhãn mới.

Cuối cùng, PASTA cũng được thúc đẩy bởi các công trình nhằm hiểu cơ học các điểm số chú ý (Zou et al., 2023), ví dụ như bằng cách nghiên cứu chúng thông qua tầm quan trọng của đặc trưng (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019; Deb et al., 2023), probing (Conneau et al., 2018; Liu & Avci, 2019), visualization (Karpathy et al., 2015; Olah et al., 2017), localizing knowledge (Meng et al., 2022a; Dai et al., 2021), categorizing directions in representation space (Kim et al., 2017; Schwettmann et al., 2021), hoặc natural-language explanations (Bills et al., 2023; Singh et al., 2023a).

7 KẾT LUẬN
Trong nghiên cứu này, chúng tôi đề xuất PASTA, một cách tiếp cận mới nhằm cho phép LLMs vượt qua những hạn chế của văn bản thuần túy và hiệu quả nhận biết hướng dẫn của người dùng được thể hiện như các phần được làm nổi bật của prompts. Bằng cách thực hiện các điều chỉnh chính xác đối với các điểm số chú ý trong các đầu được chọn, PASTA hướng sự tập trung của mô hình vào ngữ cảnh liên quan, phản ánh cách con người hưởng lợi từ các tín hiệu văn bản. Không giống như các phương pháp fine-tuning truyền thống, PASTA được áp dụng tại thời điểm suy luận và không yêu cầu cập nhật tham số cũng như tính toán gradient; PASTA chỉ yêu cầu chọn đầu chú ý nào để áp dụng việc tính trọng số lại, một thao tác lập hồ sơ một lần cho một LLM. Kết quả thí nghiệm cho thấy rằng PASTA có thể cải thiện đáng kể hiệu suất mô hình trên nhiều tác vụ khác nhau. Trong tương lai, chúng tôi dự định tích hợp PASTA với nhiều phương pháp khác nhau, chẳng hạn như few-shot in-context learning, nhằm làm nổi bật các ví dụ hiệu quả để tăng cường tính ổn định của nó.
9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

TÀI LIỆU THAM KHẢO
Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma,
Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An integrated development environment
and repository for natural language prompts. arXiv preprint arXiv:2202.01279 , 2022.

Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan
Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. URL
https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05.
2023) , 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,
Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-
pher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems , volume 33, pp. 1877–1901. Curran Associates,
Inc., 2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020b.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint
arXiv:2210.11416 , 2022.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an
analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Inter-
preting Neural Networks for NLP , pp. 276–286, Florence, Italy, August 2019. Association for Computational
Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology.org/W19-4828 .

Alexis Conneau, German Kruszewski, Guillaume Lample, Lo ¨ıc Barrault, and Marco Baroni. What you can cram
into a single vector: Probing sentence embeddings for linguistic properties. arXiv preprint arXiv:1805.01070 ,
2018.

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained
transformers. arXiv preprint arXiv:2104.08696 , 2021.

Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Choulde-
chova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios: A case study of semantic
representation bias in a high-stakes setting. In proceedings of the Conference on Fairness, Accountability,
and Transparency , pp. 120–128, 2019.

Mayukh Deb, Bj ¨orn Deiseroth, Samuel Weinbach, Patrick Schramowski, and Kristian Kersting. Atman:
Understanding transformer predictions through memory efficient attention manipulation. arXiv preprint
arXiv:2301.08110 , 2023.

Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing,
and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint
arXiv:2205.12548 , 2022.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized
llms, 2023.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and
Zhifang Sui. A survey on in-context learning, 2023.

Evan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing knowledge representations in
language models, 2023.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021a.

J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv preprint abs:2106.09685 , 2021b.
10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Sarthak Jain and Byron C Wallace. Attention is not explanation. arXiv preprint arXiv:1902.10186 , 2019.

Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv
preprint arXiv:1506.02078 , 2015.

Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres.
Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). arXiv
preprint arXiv:1711.11279 , 2017.

Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.
Post hoc explanations of language models can improve language models. arXiv preprint arXiv:2305.11426 ,
2023.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune,
and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv
preprint arXiv:2309.00267 , 2023.

Frederick Liu and Besim Avci. Incorporating priors with feature attribution on text classification. arXiv preprint
arXiv:1906.08286 , 2019.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
Lost in the middle: How language models use long contexts, 2023.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt,
and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint
arXiv:2107.13586 , 2021.

Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts
and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786 ,
2021.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in
gpt. Advances in Neural Information Processing Systems , 35:17359–17372, 2022a.

Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a
transformer. arXiv preprint arXiv:2210.07229 , 2022b.

Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one?
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch ´e-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates,
Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/
2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf .

Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at
scale, 2022.

Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill , 2(11):e7, 2017.

OpenAI. Gpt-4 technical report, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch ´e-Buc, Emily B. Fox, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp. 8024–8035,
2019.

Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot
paradigm, 2021.

Laura Rieger, Chandan Singh, W James Murdoch, and Bin Yu. Interpretations are useful: penalizing explanations
to align neural networks with prior knowledge. arXiv preprint arXiv:1909.13584 , 2019.
11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. Right for the right reasons: Training
differentiable models by constraining their explanations. arXiv preprint arXiv:1703.03717 , 2017.

Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao,
Hans-Georg Luigs, Anne-Katrin Mahlein, and Kristian Kersting. Making deep neural networks right for the
right scientific reasons by interacting with their explanations. Nature Machine Intelligence , 2(8):476–486,
2020.

Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, and Antonio Torralba. Toward
a visual concept vocabulary for gan latent space. In Proceedings of the IEEE/CVF International Conference
on Computer Vision , pp. 6804–6812, 2021.

Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and
Deyi Xiong. Large language model alignment: A survey, 2023.

Baifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Toast: Transfer learning via attention steering. arXiv
preprint abs:2305.15542 , 2023a.

Baifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Refocusing is key to transfer learning. arXiv preprint
arXiv:2305.15542 , 2023b.

Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt: Eliciting
knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 ,
2020.

Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G Huth, Bin Yu, and Jianfeng Gao. Ex-
plaining black box text modules in natural language with language models. arXiv preprint arXiv:2305.09863 ,
2023a.

Chandan Singh, John X. Morris, Jyoti Aneja, Alexander M. Rush, and Jianfeng Gao. Explaining patterns in data
with language models via interpretable autoprompting, 2023b.

Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea V oss, Alec Radford, Dario
Amodei, and Paul Christiano. Learning to summarize from human feedback. arXiv preprint abs:2009.01325 ,
2020.

Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and
Alexander M. Rush. Interactive and visual prompt engineering for ad-hoc task adaptation with large language
models, 2022.

Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4593–4601, Florence,
Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL https:
//aclanthology.org/P19-1452 .

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat
models. arXiv preprint arXiv:2307.09288 , 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_
files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .

Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention:
Specialized heads do the heavy lifting, the rest can be pruned, July 2019. URL https://aclanthology.
org/P19-1580 .

Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:
//github.com/kingoflolz/mesh-transformer-jax , May 2021.

Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts?
arXiv preprint arXiv:2109.01247 , 2021.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,
and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.
12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems , 35:24824–24837, 2022.

Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv preprint arXiv:1908.04626 , 2019.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural
language processing. arXiv preprint arXiv:1910.03771 , 2019.

Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad
Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly
Smith, L A Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He.
Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint
abs:2308.01320 , 2023.

Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.
Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on
Learning Representations , 2023. URL https://openreview.net/forum?id=lq62uWRJjiY .

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and
Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 ,
2019.

Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin,
Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang,
Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.
Representation engineering: A top-down approach to ai transparency, 2023.
13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

PHỤ LỤC

A CHI TIẾT THÍ NGHIỆM
Chúng tôi triển khai tất cả các thuật toán sử dụng PyTorch (Paszke et al., 2019) và Huggingface (Wolf et al., 2019) và chạy thí nghiệm trên GPU NVIDIA V100 và GPU NVIDIA A6000.

Bảng 6 cung cấp thống kê chi tiết của các tập dữ liệu trong thí nghiệm của chúng tôi.

Bảng 6: Thống kê của các tập dữ liệu.

Tác vụ Train Valid Test
CounterFact 1000 1000 5000
BiasBios 1000 1000 5000
JSON Formatting 1000 1000 5000
Pronouns Changing 1000 1000 5000

A.1 MẪU PROMPT CHI TIẾT CỦA MỖI TÁC VỤ
Đối với mỗi tác vụ, các mẫu prompt trong kết quả của chúng tôi như sau:

•Định dạng JSON:
–(Gốc) {context}. Trả lời nghề nghiệp của {person} và tạo ra câu trả lời theo định dạng json. Đây là một ví dụ: {"name": , "occupation": , }. Bây giờ tạo ra câu trả lời.
–(Rút gọn trong Phần 5.2) {context}. Trả lời nghề nghiệp của {person} và tạo ra câu trả lời theo định dạng json.
–(Diễn đạt lại trong Phần 5.2) Trả lời nghề nghiệp của {person} và tạo ra câu trả lời theo định dạng json. Đây là một ví dụ: {"name": , "occupation": , }.{context}. Bây giờ tạo ra câu trả lời.

•Thay đổi đại từ:
–(Gốc): {context}. Đối với văn bản được đề cập trước đó, thay thế 'she' và 'he' bằng 'they' và tạo ra nghề nghiệp của {person} sau khi thay đổi đại từ.
–(Rút gọn trong Phần 5.2): {context}. Thay đổi 'she' và 'he' bằng 'they' và trả lời nghề nghiệp của {person} sau khi thay thế đại từ
–(Diễn đạt lại trong Phần 5.2): {context}. Đối với các mô tả được đề cập trước đó, thay thế 'she' và 'he' bằng 'they' trong văn bản được đề cập trước đó và tạo ra văn bản mới sau khi thay thế đại từ.

•BiasBios: {context}.{person} có nghề nghiệp là.
•CounterFact: Trước đây, {old fact}. Hiện tại, {new fact}.{question}

A.2 CHI TIẾT ĐÁNH GIÁ CỦA PASTA
Bảng 7 trình bày số lượng đầu được điều khiển bởi PASTA cho LLAMA-7B và GPT-J-6B trên mỗi tác vụ.

Bảng 7: Số lượng đầu được điều khiển bởi PASTA.

Tác vụ LLAMA-7B GPT-J-6B
JSON Formatting 53 153
Pronouns Changing 86 72
BiasBios 86 111
CounterFact 86 52
14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

B KẾT QUẢ MỞ RỘNG
B.1 KẾT QUẢ MỞ RỘNG VỚI ĐỘ TRÔI CHẢY
Trong phần này, chúng tôi bao gồm các kết quả mở rộng, bao gồm các chỉ số độ trôi chảy. Điểm độ trôi chảy là entropy bi-gram và tri-gram trung bình của các thế hệ, được thiết kế để thấp cho các văn bản thoái hóa hoặc lặp lại (Meng et al., 2022a). Chỉ số này có thể được coi là chỉ số tham chiếu của chất lượng tạo. Thông thường, các thế hệ của mô hình ngôn ngữ đáng tin cậy miễn là điểm độ trôi chảy của chúng không quá thấp. Ở đây, chúng tôi lọc ra bất kỳ kết quả nào nhận được điểm độ trôi chảy dưới 3.0. Bảng 8, 9 và 10 bao gồm tất cả kết quả và đánh giá độ trôi chảy.

Bảng 8: Kết quả chính của LLAMA-7B để chứng minh rằng PASTA có thể cải thiện khả năng của mô hình trong việc (i) tuân theo hướng dẫn của người dùng (JSON Format và Prons. Changing); (ii) diễn giải thông tin ngữ cảnh (BiasBios); (iii) giải quyết xung đột kiến thức (CounterFact). Đối với tất cả điểm số, cao hơn là tốt hơn. Kết quả tốt nhất được in đậm.

PhươngphápJSON Format Prons. Changing BiasBios CounterFact
F. Acc / P. Acc Acc / A.Acc / Flue. Acc / Flue. ES / PS /Flue.
PromptingZero-shot 60.00 / 54.94 71.84 / 66.28 / 6.10 87.36 / 3.98 58.50 / 52.03 / 4.96
∗-marked 18.55 / 12.71 39.14 / 35.17 / 6.03 90.62 / 3.89 57.74 / 50.52 / 5.12
""-marked 4.56 / 4.20 20.55 / 18.19 / 5.13 89.82 / 3.97 58.14 / 51.70 / 5.13
Few-shot 84.85 / 73.58 59.06 / 55.27 / 5.95 88.79 / 4.19 87.45 / 49.82 / 5.68
PASTATask-agnostic 88.16 / 49.08 83.65 / 81.31 / 4.62 93.54 / 3.03 98.82 / 99.03 / 4.78
Multi-task 96.64 /85.09 96.42 /95.84 / 5.43 95.28 / 4.05 99.60 /99.57 / 4.89

Bảng 9: Kết quả chính của GPT-J để chứng minh rằng PASTA có thể cải thiện khả năng của mô hình trong việc (i) tuân theo hướng dẫn của người dùng (JSON Format và Prons. Changing); (ii) diễn giải thông tin ngữ cảnh (BiasBios); (iii) giải quyết xung đột kiến thức (CounterFact). Đối với tất cả điểm số, cao hơn là tốt hơn. Kết quả tốt nhất được in đậm.

PhươngphápJSON Format Prons. Changing BiasBios CounterFact
F. Acc / P. Acc Acc / A.Acc / Flue. Acc / Flue. ES / PS /Flue.
PromptingZero-shot 28.83 / 25.09 39.88 / 36.19 / 5.91 72.76 / 5.06 42.14 / 42.02 / 5.01
∗-marked 4.44 / 4.10 41.25 / 37.57 / 4.76 74.14 / 5.01 44.50 / 45.09 / 5.22
""-marked 8.81 / 5.62 6.12 / 5.72 / 5.43 78.64 / 4.96 45.54 / 41.84 / 5.16
Few-shot 84.15 / 72.65 35.77 / 32.08 / 6.46 72.98 / 4.82 68.34 / 38.23 / 5.67
PASTATask-agnostic 46.68 / 34.71 91.62 / 88.60 / 3.00 80.84 / 4.92 99.54 /99.57 / 5.11
Multi-task 91.50 / 18.63 92.96 /91.34 / 4.91 94.96 / 4.87 98.62 / 98.79 / 5.11

Bảng 10: Các chiến lược chọn đầu khác nhau giữa các đầu cụ thể theo tác vụ hàng đầu, hợp trên nhiều tác vụ, và giao (mặc định được sử dụng trong PASTA).

PASTAJSON Format Prons. Changing BiasBios CounterFact
F. Acc / P. Acc Acc / A.Acc / Flue. Acc / Flue. ES / PS /Flue.LLAMACụ thể theo tác vụ 95.56 / 86.83 98.52 / 98.02 / 5.92 97.62 / 4.18 99.18 / 99.24 / 4.93
hợp 88.42 / 74.49 92.12 / 91.44 / 4.88 96.36 / 4.13 99.24 / 99.35 / 4.53
giao 96.64 / 85.09 96.42 / 95.84 / 5.43 95.28 / 4.05 99.60 / 99.57 / 4.89GPT-JCụ thể theo tác vụ 85.71 / 79.39 94.74 / 92.54 / 5.07 97.64 / 5.06 99.26 / 99.34 / 4.94
Hợp 72.61 / 64.89 89.68 / 87.76 / 3.92 95.56 / 5.02 99.82 / 99.83 / 5.03
Giao 91.50 / 18.63 92.96 / 91.34 / 4.91 94.96 / 4.87 98.62 / 98.79 / 5.11

B.2 PHƯƠNG SAI CỦA HIỆU SUẤT FEW-SHOT
Few-shot prompting đôi khi dẫn đến cải thiện trong hiệu suất mô hình. như cung cấp rõ ràng các ví dụ trong các minh họa bổ sung. Tuy nhiên, một nhược điểm của few-shot prompting là tính không ổn
15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

định của nó, có nghĩa là hiệu suất của nó thể hiện phương sai cao trên các mẫu khác nhau trong minh họa. Trong phần này, chúng tôi trình bày kết quả để cho thấy rằng hiệu suất của few-shot prompting hiển thị phương sai cao về việc lấy mẫu các minh họa few-shot khác nhau.

Bảng 11: Hiệu suất few-shot (Acc. / A. Acc. / Fluency) trên tác vụ Thay đổi đại từ.

Ví dụ few-shot LLAMA-7B GPT-J-6B
Minh họa 1 84.87 / 90.09 / 4.74 43.82 / 40.36 / 6.43
Minh họa 2 57.24 / 53.98 / 6.22 40.68 / 37.86 / 6.44
Minh họa 3 57.08 / 53.22 / 6.02 33.13 / 29.21 / 6.48
Minh họa 4 52.26 / 48.30 / 6.42 25.47 / 20.89 / 6.44
Minh họa 5 43.86 / 40.78 / 6.43 11.90 / 8.63 / 6.51

B.3 KẾT QUẢ LẬP HỒ SƠ MÔ HÌNH
Trong Phần này, chúng tôi cung cấp thêm kết quả về hiệu suất của LLAMA-7B trên tất cả các tác vụ khi điều khiển: (i) tất cả đầu; (ii) toàn bộ lớp; (iii) một đầu riêng lẻ của một lớp.

1 4 8 12 16 20 24 28 32
Lớp20406080100Acc
Zero-shot
PASTA
Điều khiển tất cả đầu
Điều khiển toàn bộ lớp
Điều khiển đầu đơn

Hình 4: Hiệu suất của LLAMA-7B trên tác vụ Thay đổi đại từ khi chúng tôi điều khiển (i) tất cả đầu (xanh lá); (ii) toàn bộ lớp (vàng); và (iii) đầu riêng lẻ trong một lớp (biểu đồ violin xanh). Hiệu suất khác nhau đáng kể giữa các lớp và giữa các đầu của một lớp.

1 4 8 12 16 20 24 28 32
Lớp60708090Acc
Zero-shot
PASTA
Điều khiển tất cả đầu
Điều khiển toàn bộ lớp
Điều khiển đầu đơn

Hình 5: Hiệu suất của LLAMA-7B trên tác vụ BiasBios khi chúng tôi điều khiển (i) tất cả đầu (xanh lá); (ii) toàn bộ lớp (vàng); và (iii) đầu riêng lẻ trong một lớp (biểu đồ violin xanh). Hiệu suất khác nhau đáng kể giữa các lớp và giữa các đầu của một lớp.

1 4 8 12 16 20 24 28 32
Lớp406080100ES
 Zero-shot
PASTA
Điều khiển tất cả đầu
Điều khiển toàn bộ lớp
Điều khiển đầu đơn

Hình 6: Hiệu suất của LLAMA-7B trên tác vụ CounterFact khi chúng tôi điều khiển (i) tất cả đầu (xanh lá); (ii) toàn bộ lớp (vàng); và (iii) đầu riêng lẻ trong một lớp (biểu đồ violin xanh). Hiệu suất khác nhau đáng kể giữa các lớp và giữa các đầu của một lớp.
16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

C KẾT QUẢ TRÊN NHIỀU MÔ HÌNH HỖN
C.1 KÍCH THƯỚC MÔ HÌNH LỚN HỖN
Chúng tôi tiến hành thí nghiệm với LLAMA-13B để đánh giá thêm hiệu quả của PASTA trên tất cả các tác vụ. Bảng sau trình bày so sánh hiệu suất cho LLAMA-13B.

Bảng 12: Kết quả của LLAMA-13B. Đối với tất cả điểm số, cao hơn là tốt hơn. Kết quả tốt nhất được in đậm.

PhươngphápJSON Format Prons. Changing BiasBios CounterFact All
F. Acc / P. Acc Acc / A.Acc Acc ES / PS Ave.
Zero-shot prompting 45.48 / 43.16 65.03 / 60.90 85.80 47.86 / 44.14 56.05
Few-shot prompting 39.80 / 3.56 82.33 / 80.71 88.38 90.63 / 65.49 64.41
PASTA (Multi-task) 98.74 / 89.88 97.56 / 96.78 95.34 99.38 / 99.30 96.71

C.2 ĐIỀU KHIỂN CÁC MÔ HÌNH ĐƯỢC TINH CHỈNH HƯỚNG DẪN MÀ KHÔNG CẦN LẬP HỒ SƠ LẠI
Chúng tôi thử nghiệm thêm khả năng áp dụng của PASTA cho Vicuna-7B-v1.3, được tinh chỉnh hướng dẫn từ LLAMA-7B. Chúng tôi áp dụng PASTA sử dụng các đầu chú ý được chọn từ lập hồ sơ LLAMA-7B (bao gồm các đầu đa tác vụ và cụ thể theo tác vụ). Bằng cách này, chúng tôi đánh giá xem các đầu được chọn từ mô hình cơ sở có thể chuyển giao được cho mô hình được tinh chỉnh hướng dẫn hay không, từ đó tránh việc lập hồ sơ lại. Bảng dưới đây trình bày hiệu suất của Vicuna trên tất cả các tác vụ.

Bảng 13: Kết quả của Vicuna-7B. Đối với tất cả điểm số, cao hơn là tốt hơn. Kết quả tốt nhất được in đậm.

PhươngphápJSON Format Prons. Changing BiasBios CounterFact
F. Acc / P. Acc Acc / A.Acc Acc ES / PS
LLAMA-7B Zero-shot 60.00 / 54.94 71.84 / 66.28 87.36 58.50 / 52.03
LLAMA-7B PASTA(multi-task) 96.64 / 85.09 96.42 / 95.84 95.28 99.60 / 99.57
Vicuna Zero-shot 65.41 / 61.78 95.74 / 94.74 90.74 61.10 / 52.46
Vicuna PASTA(multi-task) 66.09 / 56.00 98.82 / 98.08 96.44 99.80 / 99.80
Vicuna PASTA(task-specific) 90.54 / 86.53 98.62 / 98.04 97.42 99.82 / 99.74

Kết quả chứng minh rằng các đầu chú ý được chọn cho LLAMA-7B hiệu quả điều khiển Vicuna-7B, cho thấy rằng việc lập hồ sơ lại không cần thiết cho các mô hình được tinh chỉnh hướng dẫn. Đáng chú ý là, khi điều khiển các đầu cụ thể theo tác vụ được chọn từ lập hồ sơ LLAMA, PASTA cải thiện đáng kể hiệu suất của Vicuna trên tất cả các tác vụ. Bằng chứng này cho thấy rằng PASTA có thể bổ sung cho việc tinh chỉnh hướng dẫn mà không cần thiết việc lập hồ sơ lại.

C.3 ABLATION VỀ SỐ LƯỢNG VÍ DỤ CHO LẬP HỒ SƠ
Tính mạnh mẽ của thứ hạng hiệu suất đầu đối với phương sai mẫu cho phép chúng ta giảm thêm kích thước mẫu cho lập hồ sơ (ví dụ, |D|= 200). Bảng dưới đây trình bày hiệu suất PASTA trên tác vụ Định dạng JSON khi lập hồ sơ lại với |D|= 200 mẫu. Chúng ta có thể thấy PASTA vẫn đạt hiệu suất vượt trội khi lập hồ sơ với ít ví dụ hơn nhiều.

Bảng 14: Hiệu suất của PASTA với kích thước mẫu |D| khác nhau của lập hồ sơ mô hình.

Mô hình Kích thước mẫu JSON Format Acc JSON Pred. Acc
LLAMA-7B Zero-shot N/A 60.00 54.94
LLAMA-7B w. PASTA 1000 95.56 86.83
LLAMA-7B w. PASTA 200 95.04 84.17
Vicuna-7B Zero-shot N/A 65.41 61.78
Vicuna-7B w. PASTA 200 98.56 94.56
17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

D VÍ DỤ VỀ CÁC THẾ HỆ PASTA
Bảng 15: Ví dụ tạo của LLAMA-7B trên Định dạng JSON.

Prompt Tạo PASTA
Winnie là một nhiếp ảnh gia người Mỹ sống ở New York. Chuyên về nhiếp ảnh thời trang và chân dung, cô áp dụng tài năng của mình trên cả con người và động vật. Trong loạt ảnh này, Tất cả đều về việc nhân cách hóa những con chó này trong ánh đèn sân khấu. Toàn bộ điều này mang lại kết quả tuyệt vời. Nhiều ảnh và thông tin hơn trên portfolio của cô. {instruction}{"name": "Winnie", "occupation": "photographer"}\n{"name": "Winnie", "occupation": "photographer"}. . .

Jon là một kỹ sư phần mềm trong nhóm Core Processing tại comScore, một công ty phân tích và đo lường khán giả trực tuyến. Anh thích làm hầu hết mã hóa của mình trong Pig. Trước khi làm việc tại comScore, anh đã viết phần mềm để tối ưu hóa chuỗi cung ứng và phân bổ chứng khoán thu nhập cố định. {instruction}{"name": "Jon", "occupation": "Software Engineer"}\n{"name": "Jon", "occupation": "Software Engineer"}. . .

Juris là một phó giáo sư tại Khoa Thông tin, Đại học Toronto. Anh là đồng tác giả của Dịch vụ Tư vấn Đọc giả trong Thư viện Công cộng Bắc Mỹ, 1870-2005 (McFarland, 2007); Sự Tiến hóa của Đối tác Thư viện và Bảo tàng: Tiền đề Lịch sử, Biểu hiện Đương đại, và Hướng Tương lai (Libraries Unlimited, 2004); và Đọc và Thủ thư Tham khảo: Tầm quan trọng đối với Dịch vụ Thư viện của Thói quen Đọc Nhân viên (McFarland, 2004). {instruction}{"name": "Juris", "occupation": "Associate Professor"}\n{"name": "Juris", "occupation": "Associate Professor"}. . .

Gregory là một nhà tâm lý giáo dục có mối quan tâm chính là cách mọi người học, tức là phát triển kỹ năng và kiến thức, đặc biệt trong môi trường giáo dục. Trọng tâm của anh là một nền tảng nghiên cứu (thống kê) mạnh mẽ vượt qua các lĩnh vực như lý thuyết học tập xã hội, xử lý thông tin, và cách tiếp cận nhận thức đối với các yếu tố cảm xúc. {instruction}{"name": "Gregory", "occupation": "Educational Psychologist"}\n{"name": "Gregory", "occupation": "Educational Psychologist"}. . .

Asif là một Kiến trúc sư Giải pháp với Amazon Web Services. Anh cung cấp hướng dẫn kỹ thuật, lời khuyên thiết kế và lãnh đạo tư tưởng cho một số khách hàng và đối tác AWS lớn nhất và thành công nhất trên hành tinh. Chuyên môn sâu nhất của anh trải rộng kiến trúc ứng dụng, containers, devops, bảo mật, học máy và ứng dụng kinh doanh SaaS. Trong 12 năm qua, anh đã mang lại sự tập trung khách hàng mãnh liệt cho các vai trò thách thức và sâu về mặt kỹ thuật trong nhiều ngành công nghiệp. Anh có một số bằng sáng chế và đã thành công dẫn dắt phát triển sản phẩm, kiến trúc và tương tác khách hàng. {instruction}{"name": "Asif", "occupation": "Solutions Architect"}\n{"name": "Asif", "occupation": "Solutions Architect"}. . .

Graham là một Phó Giáo sư về Quản lý Xây dựng tại Đại học Newcastle, Úc, nơi anh được bổ nhiệm lần đầu vào cuối năm 1999. Anh hiện là Phó Trưởng Khoa Kiến trúc và Môi trường Xây dựng, và trước đây là Trưởng Ngành, Xây dựng. Trong thời gian này anh đã đảm nhận nhiều vai trò hành chính ở cấp Khoa, Học viện và Đại học, và hiện là thành viên của Học viện Hàn lâm. {instruction}{"name": "Graham", "occupation": "Associate Professor in Construction Management"}\n{"name": "Graham", "occupation": "Associate Professor in Construction Management"}. . .

Katherine là một Phó Giáo sư trong Khoa Bệnh lý học. Cô nhận bằng MD từ Trường Y khoa Đại học Colorado nơi cô cũng hoàn thành khóa đào tạo cư trú AP/CP về bệnh lý học và học bổng trong Di truyền học Tế bào. Cô hoàn thành học bổng trong Bệnh lý học Di truyền Phân tử trong Khoa Bệnh lý học vào năm 2009. Cô là Giám đốc Y khoa trong Di truyền học Tế bào, Di truyền học Tế bào Phân tử, và Ung thư học Phân tử tại Phòng thí nghiệm ARUP. {instruction}{"name": "Katherine", "occupation": "Assistant Professor in the Department of Pathology", "answer": "Katherine is an Assistant Professor in the Department of Pathology. She received her MD from the University of Colorado School of Medicine where she also completed her AP/CP residency training in pathology and a fellowship in Cytogenetics.",

Peter là một luật sư di trú với ít nhất ba liên kết đến chính phủ Mexico. Ngoài việc đứng đầu Trung tâm Nhân quyền và Luật Hiến pháp, anh cũng điều hành vocesunidas.org, mà cho đến gần đây được gọi là "Dự án Mexico". Nó đã trải qua một thay đổi thú vị kể từ đó. {instruction}{"name": "Peter", "occupation": "Immigration Attorney"}\nCâu trả lời là một đối tượng json. {"name": "Peter", "occupation": "Immigration Attorney"}\n{"name": "Peter", "occupation": "Immigration Attorney"}. . .

Delvin là một nhà báo tự do. Anh cũng là Chuyên gia Dầu khí của Ban Chippewa Núi Rùa. Anh là người đóng góp thường xuyên cho tờ báo của bộ lạc, The Turtle Mountain Times, cũng như indianz.com và Indian Country Today Media Network, cả hai đều là cửa sổ tin tức quốc gia cho Indian Country. {instruction}{"name": "Delvin", "occupation": "Freelance Journalist"}\n{"name": "Delvin", "occupation": "Freelance Journalist", "age": 25}\n{"name": "Delvin", "occupation": "Freelance Journalist", "age": 25, "gender": "male"}\n{"name": "Delvin", "occupation": "Freelance Journalist", "age": 25, "gender": "male", "hair": "black", "eyes": "brown"}. . .

Robert là một nhiếp ảnh gia rock có trụ sở tại Los Angeles. Anh bắt đầu sự nghiệp của mình với tư cách là nhiếp ảnh gia nhân viên cho tạp chí Creem huyền thoại. Anh đã chụp ảnh The Stooges tại một số buổi diễn sớm nhất của họ và đã duy trì mối quan hệ làm việc chặt chẽ với nhóm. Tác phẩm của anh đã xuất hiện trong Playboy, The New York Times, Los Angeles Times, Rolling Stone, và Mojo, cũng như trên hơn 200 album, bao gồm album gần đây nhất của The Stooges, The Weirdness. Anh giám sát việc trở lại của tạp chí CREEM năm 2001, cũng như tuyển tập bìa cứng năm 2007 CREEM: America˘2019s Only Rock˘2018n˘2019 Roll Magazine. {instruction}{"name": "Robert Matheu", "occupation": "Rock Photographer"}\n{"name": "Robert Matheu", "occupation": "Rock Photographer"}. . .
18

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 16: Ví dụ tạo của LLAMA-7B trên Thay đổi đại từ.

Prompt Tạo PASTA Flue./Cons.
Jon là một kỹ sư phần mềm trong nhóm Core Processing tại comScore, một công ty phân tích và đo lường khán giả trực tuyến. Anh thích làm hầu hết mã hóa của mình trong Pig. Trước khi làm việc tại comScore, anh đã viết phần mềm để tối ưu hóa chuỗi cung ứng và phân bổ chứng khoán thu nhập cố định. {instruction}Jon là một kỹ sư phần mềm trong nhóm Core Processing tại comScore, một công ty phân tích và đo lường khán giả trực tuyến. Họ thích làm hầu hết mã hóa của họ trong Pig. Trước khi làm việc tại comScore họ đã viết phần mềm để tối ưu hóa chuỗi cung ứng và phân bổ chứng khoán thu nhập cố định. \nVăn bản trên là một ví dụ tốt về một câu trung tính giới. \nVăn bản sau đây là một ví dụ tốt về một câu trung tính giới: \nJon là một kỹ sư phần mềm trong nhóm Core Processing tại comScore, một công ty phân tích và đo lường khán giả trực tuyến. Họ thích làm hầu hết mã hóa của họ trong Pig.5.95 / 19.98

Fred là một nhiếp ảnh gia cũng điều hành một doanh nghiệp sơn và dán giấy dán tường ở Brooklyn. Anh đã trưng bày tác phẩm của mình trong các triển lãm nhóm ở Brooklyn và Manhattan tại nhà hàng Hope & Anchor, Phòng trưng bày Object Image, Phòng trưng bày Foley và BAX (Brooklyn Arts Exchange). Anh chủ yếu tự học và đã học tại New School và Trung tâm Nhiếp ảnh Quốc tế. {instruction}:Fred là một nhiếp ảnh gia cũng điều hành một doanh nghiệp sơn và dán giấy dán tường ở Brooklyn. Họ đã trưng bày tác phẩm của họ trong các triển lãm nhóm ở Brooklyn và Manhattan tại nhà hàng Hope & Anchor, Phòng trưng bày Object Image, Phòng trưng bày Foley và BAX (Brooklyn Arts Exchange). Họ chủ yếu tự học và đã học tại New School và Trung tâm Nhiếp ảnh Quốc tế. \nVăn bản bây giờ là trung tính giới. \nVăn bản bây giờ là trung tính giới. Các đại từ đã được thay đổi thành 'they' và 'them' và nghề nghiệp đã được thay đổi thành 'photographer'.6.39 / 28.16

Ann là một nhà báo viết về du lịch và nuôi dạy con cho The New York Times Magazine, Vogue, Conde Nast Traveler và nhiều ấn phẩm khác. Adrienne Hartman là một họa sĩ minh họa sống ở New York. Tác phẩm của cô đã được xuất bản bởi Gourmet, Entreee, Book of the Month Club, và các ấn phẩm và sách khác. {instruction}Ann là một nhà báo viết về du lịch và nuôi dạy con cho The New York Times Magazine, Vogue, Conde Nast Traveler và nhiều ấn phẩm khác. Họ là một nhà báo viết về du lịch và nuôi dạy con cho The New York Times Magazine, Vogue, Conde Nast Traveler và nhiều ấn phẩm khác. \nAnn là một nhà báo viết về du lịch và nuôi dạy con cho The New York Times Magazine, Vogue, Conde Nast Traveler và nhiều ấn phẩm khác.4.90 / 20.91

Justine là một nhiếp ảnh gia. Photo District News đã đặt tên cô là một trong 30 nhiếp ảnh gia mới và nổi bật hàng đầu cần theo dõi năm 2011. Cô cũng gần đây đã nhận được một khoản tài trợ Queens Community Arts Fund từ Queens Council on the Arts. Cô đã có nhiều chương trình nghệ sĩ thường trú, gần đây nhất là tại Lower Manhattan Cultural Council. Tác phẩm của cô đã được trưng bày tại nhiều địa điểm, bao gồm Phòng trưng bày Michael Mazzeo và El Museo del Barrio ở New York. Cô đã có bằng Thạc sĩ Mỹ thuật từ Viện Nghệ thuật San Francisco. {instruction}Justine là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. \nJustine là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia.\nJustine là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. \nJustine là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. \nJustine là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia. Họ là một nhiếp ảnh gia2.89 / 14.18

Courtney là một nhà báo tự do sống người trước đây sống ở Buenos Aires. Cô viết cho True/Slant, Pocket-Lint, Flavorwire, Relix, GBQ, Wander-Argentina & Forbes. Cô yêu robot, torrontes, yoga và nhạc sống. Cô được đặt tên là một trong Business Insider, Most Important Women Under 30' và thành lập audience.io, một công ty xuyên Đại Tây Dương có mục tiêu giúp các startup có trụ sở tại New York City và London hợp tác quốc tế. Cô thậm chí có trang Wikipedia riêng của mình.Courtney là một nhà báo tự do sống người trước đây sống ở Buenos Aires. Họ viết cho True/Slant, Pocket-Lint, Flavorwire, Relix, GBQ, Wander-Argentina & Forbes. Họ yêu robot, torrontes, yoga và nhạc sống. Họ được đặt tên là một trong Business Insider, Most Important Women Under 30' và thành lập audience.io, một công ty xuyên Đại Tây Dương có mục tiêu giúp các startup có trụ sở tại New York City và London hợp tác quốc tế. Họ thậm chí có Wikipedia riêng của họ.6.33 / 12.17

Gavin là một nhiếp ảnh gia Sydney tập trung vào con người và địa điểm. Anh chuyên về nhiếp ảnh thương mại và biên tập và thường xuyên được ủy thác làm việc trên nhiều dự án công ty, biên tập, kiến trúc, công nghiệp, quảng cáo và báo cáo thường niên. Anh là một nhiếp ảnh gia chân dung và địa điểm rất linh hoạt. {instruction}Gavin là một nhiếp ảnh gia Sydney tập trung vào con người và địa điểm. Họ chuyên về nhiếp ảnh thương mại và biên tập và thường xuyên được ủy thác làm việc trên nhiều dự án công ty, biên tập, kiến trúc, công nghiệp, quảng cáo và báo cáo thường niên. Họ là một nhiếp ảnh gia chân dung và địa điểm rất linh hoạt.5.07 / 13.72

Eric là một nhiếp ảnh gia chuyên nghiệp. Scott Allen Perry là một diễn viên, một nhà làm phim độc lập, và một nhà văn. Anh là nhà văn và đạo diễn của "Bobbi's Bedroom," "MC Outdoorz," và phim tài liệu "The Outdoorsmen". Cả hai đều sống ở Los Angeles. Adam Mock là chủ tịch và giám đốc sáng tạo của Medallion Media Group. Anh sống ở Montgomery, Illinois. Họ là nhiếp ảnh gia và tác giả của "Mime Very Own Book." {instruction}Eric là một nhiếp ảnh gia chuyên nghiệp. Họ là nhiếp ảnh gia và tác giả của "Mime Very Own Book." \nNghề nghiệp của họ là một nhiếp ảnh gia chuyên nghiệp. \nNghề nghiệp của họ là một nhiếp ảnh gia chuyên nghiệp. Họ là nhiếp ảnh gia và tác giả của "Mime Very Own Book."4.53 / 17.31

Christopher là một nhà báo tự do lớn lên ở Michigan. Anh có bằng báo chí từ Đại học Central Michigan và dành sáu năm viết cho tờ báo hàng ngày lớn nhất của Upper Peninsula. Anh là một nhà thám hiểm và khám phá nghiệp dư và biết vị trí bí mật của nghĩa trang Big Boy của Marquette County. Anh thiết kế báo để kiếm sống.{instruction}Christopher là một nhà báo tự do lớn lên ở Michigan. Họ có bằng báo chí từ Đại học Central Michigan và dành sáu năm viết cho tờ báo hàng ngày lớn nhất của Upper Peninsula. Họ là một nhà thám hiểm và khám phá nghiệp dư và biết vị trí bí mật của nghĩa trang Big Boy của Marquette County. Họ thiết kế báo để kiếm sống.5.58 / 13.85
19

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

E CÁC CHỈ SỐ ĐÁNH GIÁ BỔ SUNG
Chúng tôi hiểu tầm quan trọng của việc bảo tồn độ trôi chảy và chất lượng tạo sinh trong khi tăng cường hiệu suất cụ thể theo tác vụ với PASTA. Để đảm bảo điều này, chúng tôi sử dụng hai chỉ số để đánh giá chất lượng của các thế hệ PASTA trên ba tác vụ tạo ngôn ngữ tự nhiên (Prons. Changing, BiasBios, và CounterFact).

•Đánh giá Độ trôi chảy (Meng et al., 2022a): Như đã đề cập trong Phần 5, chúng tôi đánh giá độ trôi chảy của tất cả các thế hệ (entropy bigram và trigram trung bình của các thế hệ), và loại trừ các kết quả có điểm độ trôi chảy dưới 3.0. Bước này hiệu quả loại bỏ các thế hệ thoái hóa hoặc lặp lại khỏi xem xét.

•Chỉ số Nhất quán: Chúng tôi sử dụng một chỉ số nhất quán bổ sung (được giới thiệu bởi Hernandez et al. (2023)), đo sự tương tự tf-idf trung bình giữa văn bản được tạo ra và văn bản tham chiếu của tập dữ liệu đầy đủ. Chỉ số này giúp chúng tôi đo lường mức độ văn bản được tạo ra phù hợp với các đầu vào ngữ cảnh tổng thể về mặt nội dung và phong cách (cao hơn là tốt hơn).

Bảng 16 trình bày các ví dụ về thế hệ LLAMA-7B với PASTA và điểm độ trôi chảy và nhất quán của chúng trên tác vụ Thay đổi đại từ. Chúng ta có thể thấy rằng các thế hệ lặp lại hoặc vô nghĩa nhận được độ trôi chảy thấp (dưới 3.0) và nhất quán (dưới 8.0). Các thế hệ có độ trôi chảy cao (khoảng 4.5) và nhất quán (trên 13) có ý nghĩa và có thể đọc được. Bảng sau trình bày đánh giá độ trôi chảy và nhất quán trung bình trên các tác vụ được đề cập:

Bảng 17: Kết quả đánh giá độ trôi chảy và nhất quán trên LLAMA-7B.

PhươngphápProns. Changing BiasBios CounterFact
Acc / Cons. / Flue. Acc / Cons. / Flue. ES / PS / Cons. / Flue.
Zero-shot 71.84 / 22.29 / 6.10 87.36 / 13.02 / 3.98 58.50 / 52.03 / 11.64 / 4.96
PASTA 92.30 / 22.37 / 6.07 95.28 / 14.25 / 4.05 99.60 / 99.57 / 19.29 / 4.89

Kết quả cho thấy rằng PASTA đạt được điểm nhất quán và độ trôi chảy tương đương với zero-shot prompting. Điều này cho thấy rằng PASTA hiệu quả duy trì chất lượng và độ trôi chảy tạo sinh trong khi cải thiện đáng kể hiệu quả tác vụ.
20
