# 2111.05498.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2111.05498.pdf
# Kích thước tệp: 10933451 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Attention Xấp Xỉ Sparse Distributed Memory
Trenton Bricken
Systems, Synthetic and Quantitative Biology
Harvard University
trentonbricken@g.harvard.edu

Cengiz Pehlevan
Applied Mathematics
Harvard University
cpehlevan@seas.harvard.edu

Tóm tắt
Trong khi Attention đã trở thành một cơ chế quan trọng trong deep learning, vẫn còn hạn chế về trực giác tại sao nó hoạt động tốt như vậy. Ở đây, chúng tôi chỉ ra rằng Transformer Attention có thể có mối liên hệ chặt chẽ dưới những điều kiện dữ liệu nhất định với Sparse Distributed Memory (SDM) của Kanerva, một mô hình bộ nhớ kết hợp có tính khả thi sinh học. Chúng tôi xác nhận rằng những điều kiện này được thỏa mãn trong các mô hình Transformer GPT2 đã được tiền huấn luyện. Chúng tôi thảo luận về những tác động của ánh xạ Attention-SDM và cung cấp những diễn giải tính toán và sinh học mới về Attention.

Giới thiệu
Được sử dụng nổi bật nhất trong Transformer, Attention đã giúp deep learning có thể nói là tiếp cận hiệu suất ở mức con người trong nhiều nhiệm vụ khác nhau với các mô hình lớn hơn tiếp tục tăng cường hiệu suất [1,2,3,4,5,6,7,8,9]. Tuy nhiên, những động lực heuristic đã tạo ra Attention để lại câu hỏi mở về lý do tại sao nó hoạt động tốt như vậy [1,10]. Những hiểu biết sâu sắc về lý do Attention hiệu quả như vậy sẽ không chỉ làm cho nó có thể diễn giải hơn mà còn hướng dẫn những cải tiến trong tương lai.

Đã có nhiều nghiên cứu để cố gắng giải thích thành công của Attention, bao gồm công trình cho thấy rằng các biểu diễn Transformer ánh xạ gần gũi hơn với các bản ghi não bộ con người và thiên hướng quy nạp so với các mô hình khác [11,12]. Công trình của chúng tôi thực hiện một bước khác theo hướng này bằng cách chỉ ra mối quan hệ tiềm năng giữa Attention và xử lý thần kinh có tính khả thi sinh học ở mức độ kết nối thần kinh, cung cấp một góc nhìn cơ chế mới đằng sau hoạt động Attention. Mối quan hệ tiềm năng này được tạo ra bằng cách chỉ ra về mặt toán học rằng Attention xấp xỉ chặt chẽ Sparse Distributed Memory (SDM).

SDM là một mô hình bộ nhớ kết hợp được phát triển vào năm 1988 để giải quyết "Bài toán Khớp Tốt nhất", nơi chúng ta có một tập hợp các bộ nhớ và muốn nhanh chóng tìm ra sự khớp "tốt nhất" với bất kỳ truy vấn nào được cho [13,14]. Trong quá trình phát triển giải pháp của nó, SDM đã tôn trọng những ràng buộc sinh học cơ bản, chẳng hạn như luật Dale, rằng các khớp thần kinh được cố định là kích thích hoặc ức chế và không thể chuyển đổi một cách động (xem Phần 1 để có cái nhìn tổng quan về SDM và [13] hoặc [15] để có đánh giá sâu hơn). Mặc dù được phát triển độc lập với giải phẫu thần kinh, giải pháp có tính khả thi sinh học của SDM ánh xạ một cách đáng kinh ngạc lên tiểu não [13, 16].

Trừu tượng, mối quan hệ giữa SDM và Attention tồn tại bởi vì hoạt động đọc của SDM sử dụng các giao điểm giữa các hypersphere nhiều chiều cao mà xấp xỉ hàm mũ trên tổng của các hàm mũ là hàm softmax của Attention (Phần 2). Thiết lập rằng Attention xấp xỉ SDM về mặt toán học, sau đó chúng tôi kiểm tra nó trong các mô hình Transformer GPT2 đã được tiền huấn luyện [3] (Phần 3) và mô phỏng (Phụ lục B.7). Chúng tôi sử dụng biến thể Query-Key Normalized Transformer [22] để trực tiếp chỉ ra rằng mối quan hệ với SDM tồn tại tốt. Sau đó chúng tôi sử dụng các mô hình GPT2 gốc để giúp xác nhận kết quả này và làm cho nó tổng quát hơn.

Mối quan hệ tiểu não này thêm hấp dẫn bởi thực tế rằng giải phẫu thần kinh giống tiểu não tồn tại ở nhiều sinh vật khác bao gồm nhiều loài côn trùng (ví dụ: Drosophila Mushroom Body) và có thể cả động vật chân đầu [17, 18, 19, 20, 21].

--- TRANG 2 ---
Sử dụng framework SDM, chúng ta có thể vượt ra ngoài Attention và diễn giải toàn bộ kiến trúc Transformer, cung cấp trực giác sâu sắc hơn (Phần 4). Được thúc đẩy bởi ánh xạ này giữa Attention và SDM, chúng tôi thảo luận về cách Attention có thể được triển khai trong não bộ bằng cách tóm tắt mối quan hệ của SDM với tiểu não (Phần 5). Trong công trình liên quan (Phần 6), chúng tôi liên kết SDM với các mô hình bộ nhớ khác [23,24], bao gồm cách SDM là sự tổng quát hóa của Hopfield Networks và đến lượt nó, cách kết quả của chúng tôi mở rộng công trình liên quan Hopfield Networks với Attention [25,26]. Cuối cùng, chúng tôi thảo luận về những hạn chế và hướng nghiên cứu tương lai có thể tận dụng công trình của chúng tôi (Phần 7).

1 Đánh giá SDM của Kanerva

Ở đây, chúng tôi trình bày một cái nhìn tổng quan ngắn gọn về SDM. Một đánh giá sâu hơn về động lực đằng sau SDM và những đặc điểm làm cho nó có tính khả thi sinh học có thể được tìm thấy trong [13,15]. SDM cung cấp một thuật toán về cách các bộ nhớ (mẫu) được lưu trữ trong và truy xuất từ các nơ-ron trong não. Có ba thành phần nguyên thủy đều tồn tại trong không gian của các vector nhị phân n chiều:

Mẫu (p) - có hai thành phần: địa chỉ mẫu, p^a ∈ {0,1}^n, là biểu diễn vector của một bộ nhớ; "con trỏ" mẫu, p^p ∈ {0,1}^n, được ràng buộc với địa chỉ và trỏ đến chính nó khi tự kết hợp hoặc đến một địa chỉ mẫu khác khi dị kết hợp. Một ví dụ dị kết hợp là ghi nhớ bảng chữ cái nơi địa chỉ mẫu cho chữ cái a trỏ đến địa chỉ mẫu b, b trỏ đến c, v.v. Để có tính khả thi trong việc phân tích SDM, chúng tôi giả định các địa chỉ mẫu và con trỏ của chúng tôi là ngẫu nhiên. Có m mẫu và chúng được đánh chỉ số bằng chỉ số trên ∈ {1, ..., m}.

Nơ-ron (x) - trong việc hiển thị mối quan hệ của SDM với Attention, đủ để biết có r nơ-ron với các địa chỉ cố định x^a ∈ {0,1}^n lưu trữ một tập hợp tất cả các mẫu được ghi vào chúng. Mỗi nơ-ron sẽ tổng hợp tập hợp các mẫu của nó để tạo ra một chồng chất. Điều này tạo ra sự can thiệp tiếng ồn tối thiểu giữa các mẫu do bản chất nhiều chiều cao của không gian vector và cho phép tất cả các mẫu được lưu trữ trong một vector lưu trữ n chiều được ký hiệu x^v ∈ Z_+^n, bị ràng buộc với các số nguyên dương. Các đặc điểm có tính khả thi sinh học của chúng được nêu trong [13,15]. Khi chúng ta giả định các mẫu của chúng ta là ngẫu nhiên, chúng ta cũng giả định các địa chỉ nơ-ron được phân bố ngẫu nhiên. Trong số 2^n vector có thể có trong không gian vector nhị phân của chúng ta, SDM là "thưa thớt" vì nó giả định rằng r ≪ 2^n nơ-ron tồn tại trong không gian.

Truy vấn (θ) - là đầu vào cho SDM, được ký hiệu θ ∈ {0,1}^n. Mục tiêu trong Bài toán Khớp Tốt nhất là trả về con trỏ mẫu được lưu trữ tại địa chỉ mẫu gần nhất với truy vấn. Chúng ta thường sẽ quan tâm đến mức tham nhũng tiếng ồn tối đa có thể được áp dụng cho truy vấn của chúng ta, trong khi vẫn để nó đọc ra mẫu chính xác. Một ví dụ tự kết hợp là muốn nhận ra các khuôn mặt quen thuộc trong ánh sáng yếu. Hình ảnh của các khuôn mặt chúng ta đã thấy trước đây là các mẫu được lưu trữ trong bộ nhớ và truy vấn của chúng ta là một biểu diễn nhiễu của một trong những khuôn mặt. Chúng ta muốn SDM trả về phiên bản không nhiễu của khuôn mặt được truy vấn, giả định nó được lưu trữ trong bộ nhớ.

SDM sử dụng metric khoảng cách Hamming giữa hai vector bất kỳ được định nghĩa: d(a,b) := 1^T_n |a-b|. Vector toàn số một 1_n có n chiều và |a-b| lấy giá trị tuyệt đối của hiệu theo từng phần tử giữa các vector nhị phân. Khi rõ ràng về hai vector nào mà khoảng cách Hamming được tính giữa chúng, đôi khi chúng tôi sẽ sử dụng ký hiệu rút gọn d_v := d(a,b).

Khoảng cách Hamming là quan trọng để xác định có bao nhiêu nơ-ron mà các hoạt động đọc và ghi được phân bố qua. Khoảng cách Hamming tối ưu cho các vòng tròn đọc và ghi được ký hiệu d, phụ thuộc vào số lượng và phân bố các mẫu trong không gian vector và mục đích sử dụng bộ nhớ (ví dụ: tối đa hóa số lượng bộ nhớ có thể được lưu trữ so với tính mạnh mẽ của hệ thống bộ nhớ đối với tiếng ồn truy vấn). Chúng tôi cung cấp ba giá trị d tham chiếu hữu ích, sử dụng các phương trình được nêu trong Phụ lục B.5. Tỷ lệ Tín hiệu trên Nhiễu (SNR) tối ưu d_SNR tối đa hóa xác suất một truy vấn không nhiễu sẽ trả về mẫu mục tiêu của nó [15]. Dung lượng bộ nhớ tối ưu d_Mem tối đa hóa số lượng bộ nhớ có thể được lưu trữ với một xác suất truy xuất nhất định và cũng giả định một truy vấn không nhiễu. Khoảng cách tới hạn d_CD tối đa hóa, cho một số lượng mẫu nhất định, lượng nhiễu có thể được áp dụng cho một truy vấn sao cho nó sẽ hội tụ đến mẫu chính xác của nó [15].

Những d này chỉ là những điểm tham chiếu xấp xỉ cho các so sánh sau này với Transformer Attention, trước hết và quan trọng nhất bởi vì chúng giả định các mẫu ngẫu nhiên để làm cho các phép dẫn xuất của chúng khả thi. Ngoài ra, Transformer Attention sẽ không chỉ tối ưu hóa cho một trong những mục tiêu này, và có thể nội suy giữa những d tối ưu này khi nó muốn có cả khoảng cách tới hạn tốt để xử lý các truy vấn nhiễu và dung lượng bộ nhớ hợp lý. Những d tối ưu này là một hàm của n, r và m. Đối với setting Transformer Attention [1], nơi n = 64, r = 2^n và m ≪ 1024, d_SNR = 11, d_Mem = 5, d_CD = 15, như đã dẫn xuất trong Phụ lục B.5.

[Hình 1]: Tóm tắt các hoạt động đọc và ghi SDM. Hàng trên cho thấy ba mẫu được ghi vào các nơ-ron gần đó. 1. Hoạt động ghi đầu tiên; 2. Các mẫu được lưu trữ bên trong các nơ-ron gần đó và vị trí mẫu gốc được hiển thị; 3. Ghi mẫu thứ hai; 4. Ghi mẫu thứ ba và các nơ-ron lưu trữ một chồng chất của nhiều mẫu. Hàng dưới cho thấy hai góc nhìn đẳng cấu của hoạt động đọc. Góc nhìn nơ-ron (trái) cho thấy truy vấn đọc từ các nơ-ron gần đó với phần nhỏ cho thấy số lần mỗi mẫu được đọc. Bốn mẫu màu xanh là đa số sẽ dẫn đến hội tụ một bước. Góc nhìn mẫu (phải) là quan trọng để liên hệ SDM với Attention và được định nghĩa trong Phương trình 1 dưới đây. Chúng ta trừu tượng hóa các nơ-ron bằng cách giả định chúng được phân bố đều trong không gian. Điều này cho phép chúng ta xem xét giao điểm vòng tròn giữa truy vấn và các vị trí gốc của mỗi mẫu nơi màu xanh có giao điểm vòng tròn lớn nhất.

1.1 Hoạt động Đọc SDM

Đối với kết nối với Attention, chúng tôi tập trung vào hoạt động đọc SDM và tóm tắt ngắn gọn hoạt động ghi: tất cả các mẫu ghi con trỏ p^p của chúng theo cách phân tán đến tất cả các địa chỉ nơ-ron nằm trong khoảng cách Hamming d. Điều này có nghĩa là mỗi nơ-ron sẽ lưu trữ một chồng chất của các con trỏ mẫu từ những địa chỉ mẫu trong d: x^v = Σ_{p: d(p^a,x^a) ≤ d, ∀} p^p. Sau khi lưu trữ các mẫu theo cách phân tán qua các nơ-ron gần đó, hoạt động đọc của SDM truy xuất các con trỏ mẫu đã lưu trữ từ tất cả các nơ-ron trong khoảng cách d của truy vấn và tính trung bình chúng. Trung bình này được trọng số hóa hiệu quả bởi vì cùng các mẫu đã có lưu trữ phân tán qua nhiều nơ-ron được đọc từ đó. Trọng số mẫu sẽ cao hơn cho những mẫu có địa chỉ gần truy vấn hơn bởi vì chúng đã ghi con trỏ của chúng vào nhiều nơ-ron hơn mà truy vấn đọc từ đó. Về mặt hình học, trọng số hóa này của mỗi mẫu có thể được diễn giải như giao điểm của các vòng tròn bán kính d được trung tâm hóa trên truy vấn và mỗi địa chỉ mẫu p^a cho tất cả ℓ. Một cái nhìn tổng quan cao về các hoạt động đọc và ghi SDM được hiển thị trong Hình 1.

2^n nơ-ron có thể có mà cả đã lưu trữ con trỏ p^p của mẫu này và được đọc bởi ℓ là: |O_n(p^a; d) ∩ O_n(θ; d)|, nơi |·| là toán tử cardinality và O_n(ξ; d) = {x^a ∈ {0,1}^n : d(ξ, x^a) ≤ d} là tập hợp tất cả các địa chỉ nơ-ron có thể x^a trong bán kính d của ξ. Về mặt toán học,

Trong không gian nhị phân này, khoảng cách Hamming xung quanh một vector thực tế là một hypercube nhưng các đỉnh của một khối n chiều đơn vị nằm trên bề mặt của một hypersphere n chiều với bán kính √(n/2) và chúng tôi gọi đây là một vòng tròn vì các sơ đồ hai chiều của chúng tôi. Chúng tôi áp dụng sự tương tự hữu ích này, được lấy từ sách của Kanerva về SDM [13], xuyên suốt bài báo.

--- TRANG 3 ---
hoạt động đọc của SDM tổng hợp trên con trỏ của mỗi mẫu, được trọng số hóa bởi giao điểm vòng tròn truy vấn của nó:

θ^new = g(Σ_{p∈P} |O_n(p^a;d) ∩ O_n(θ;d)| p^p / Σ_{p∈P} |O_n(p^a;d) ∩ O_n(θ;d)|); g(e) = {1, nếu e > 1/2; 0, ngược lại} (1)

và g hoạt động theo từng phần tử trên các vector. Mẫu số chuẩn hóa tất cả các trọng số để chúng tổng bằng 1 trong tử số và cho phép tính toán nếu giá trị đa số theo từng phần tử là 0 hay 1, sử dụng hàm g(·). Trực quan, truy vấn sẽ hội tụ đến mẫu "tốt nhất" gần nhất bởi vì nó sẽ có trọng số giao điểm vòng tròn lớn nhất. Đầu ra của hoạt động đọc SDM được viết như việc cập nhật truy vấn θ → θ^new để nó có thể (nhưng không bắt buộc) áp dụng hoạt động đọc một cách lặp lại nếu hội tụ đầy đủ đến mẫu "khớp tốt nhất" của nó được mong muốn và không đạt được trong một lần cập nhật.

Giao điểm vòng tròn (được dẫn xuất trong Phụ lục B.1) được tính như một hàm của bán kính Hamming cho các hoạt động đọc và ghi d, chiều n, và khoảng cách vector giữa truy vấn và mẫu: d_v = d(p^a, θ), vì vậy chúng tôi sử dụng ký hiệu rút gọn I(d_v; d; n):

I(d_v; d; n) := |O_n(p^a;d) ∩ O_n(θ;d)| = Σ_{a=n-d-⌊d_v/2⌋}^{d_v-(n-d-a)} Σ_{c=max(0,n-d-a)}^{n-d_v-a} (n-d_v choose a)(d_v choose c) (2)

Phương trình 2 tổng hợp trên số lượng các vector nhị phân có thể có thể tồn tại tại mọi vị trí bên trong giao điểm vòng tròn. Lấy cảm hứng từ [27], đây là một dẫn xuất mới và có thể diễn giải hơn của giao điểm vòng tròn so với cái được phát triển ban đầu [13]. Phương trình 2 xấp xỉ mũ cho những mẫu gần nhất, quan trọng nhất nơi d(p^a, θ) ≤ 2d, điều này là quan trọng đối với cách SDM xấp xỉ Attention. Điều này được hiển thị cho một trường hợp đại diện của SDM trong Hình 2. Chi tiết của sự xấp xỉ này được cung cấp trong Phụ lục B.2, nhưng ở mức độ cao, các hệ số nhị thức có thể được biểu diễn như các phân phối nhị thức và sau đó được xấp xỉ bởi các phân phối chuẩn chứa các hàm mũ. Với các hằng số được chọn đúng, c_1 và c_2, mà độc lập với khoảng cách vector d(p^a, θ), chúng ta có thể thực hiện xấp xỉ sau:

I(d(p^a, θ); d; n) ≈ c_1 exp(-c_2 d(p^a, θ)) (3)

[Hình 2]: (Trái) Hoạt động đọc của SDM sử dụng bán kính Hamming d (cho đọc và ghi) và khoảng cách vector d_v = d(θ, p^a). Nhớ lại rằng trong hoạt động ghi, các địa chỉ mẫu p^a ghi con trỏ mẫu p^p của chúng vào các nơ-ron nằm tại các địa chỉ x^a (được ký hiệu ở đây như các chấm đen) trong bán kính d. Trong hoạt động đọc, truy vấn đọc từ mỗi nơ-ron trong bán kính d, do đó tạo ra một giao điểm. (Phải) Khi d_v giữa truy vấn và mẫu tăng (trục x), kích thước giao điểm vòng tròn của chúng giảm xấp xỉ mũ (trục y). Chúng tôi sử dụng Phương trình 2 với n = 64 và d_SNR = 11, trong khi thay đổi d_v lên đến khoảng cách d_v = 2d ngoài điểm đó không có giao điểm vòng tròn. Chúng tôi vẽ trục y trên thang logarit để hiển thị cách, bởi vì đường cong xấp xỉ tuyến tính, giao điểm vòng tròn xấp xỉ mũ. Xem Phụ lục B.2 để có phân tích chính thức về sự xấp xỉ mũ mà giao điểm vòng tròn tạo ra mạnh mẽ qua các tham số n, d, và r.

--- TRANG 4 ---
2 Attention Xấp Xỉ SDM

Để có thể xử lý một số lượng lớn các mẫu, chúng ta để ma trận địa chỉ mẫu với mỗi mẫu như một cột là: P_a = [p_a^1, p_a^2, ..., p_a^m] với các con trỏ P_p = [p_p^1, p_p^2, ..., p_p^m].

Quy tắc cập nhật Attention [1] sử dụng ký hiệu gốc của nó là:

θ^new = V softmax(K^T Q) = (W_v Y) softmax((W_k Y)^T (W_q q)),

nơi K, V, và Q tượng trưng cho các ma trận "key", "value", và "query", tương ứng. q là một vector truy vấn đơn và Y biểu diễn các mẫu thô được lưu trữ trong bộ nhớ. Hàm softmax(x) = exp(x)/Σ_{i=1}^n exp(x_i), nơi hàm mũ hoạt động theo từng phần tử và Attention đặt β = 1/√n. Softmax chuẩn hóa một vector các giá trị để tổng bằng 1 và cho các giá trị lớn nhất trọng số nhiều nhất do hàm mũ, đến mức độ nào tùy thuộc vào β. Chúng ta có thể viết lại điều này sử dụng ký hiệu của chúng ta, bao gồm phân biệt các vector liên tục trong R^n từ các vector nhị phân bằng cách đặt dấu ngã trên chúng:

θ̃^new = P̃_p softmax(P̃_a^T θ̃). (4)

Chúng ta viết K = W_k Y = P̃_a khi các mẫu đầu vào thô Y được chiếu bởi ma trận trọng số đã học W_k vào không gian vector SDM để trở thành các địa chỉ P̃_a. Tương tự, V = W_v Y = P̃_p và Q = W_q q = θ̃.

Hiển thị sự xấp xỉ giữa SDM Phương trình 1 và Attention Phương trình 4 yêu cầu hai bước: (i) Attention phải chuẩn hóa L2 các vector của nó. Đây là một bước nhỏ bởi vì Transformer đã sử dụng LayerNorm [28] trước và sau hoạt động Attention của nó mà chúng ta sau này liên hệ với chuẩn hóa L2; (ii) Một hệ số β cho hàm mũ softmax phải được chọn sao cho nó xấp xỉ chặt chẽ sự suy giảm gần như mũ của tính toán giao điểm vòng tròn của SDM.

Để tiến hành, chúng ta định nghĩa một ánh xạ từ các vector nhị phân a, b đến các vector liên tục đã chuẩn hóa L2 â, b̂, h(a) = â, sao cho đối với bất kỳ cặp địa chỉ mẫu nào điều sau đây giữ:

d(a,b) = ⌊n/2(1 - â^T b̂)⌋, (5)

nơi ⌊⌋ là toán tử floor. Chúng ta giả định rằng ánh xạ này tồn tại, ít nhất là xấp xỉ. Ánh xạ này cho phép chúng ta liên hệ giao điểm vòng tròn SDM nhị phân (Phương trình 2) với hàm mũ được sử dụng trong Attention (Phương trình 4) bằng cách cắm nó vào sự xấp xỉ mũ của Phương trình 3:

I(d(p^a, θ); d; n) = I(⌊n/2(1 - p̂_a^T θ̂)⌋; d; n) ≈ c_1 exp(-c_2⌊n/2(1 - p̂_a^T θ̂)⌋) ≈ c_3 exp(β p̂_a^T θ̂), (6)

nơi c_3 bao gồm các hằng số bên ngoài hàm mũ. Chúng ta thay thế các hằng số còn lại trong hàm mũ bằng β, đó là một hàm của n và d và là một xấp xỉ do phép toán floor.

Cuối cùng, những kết quả này cho phép chúng ta hiển thị mối quan hệ giữa Attention và SDM:

θ̃^new = P̂_p softmax(P̂_a^T θ̂) = Σ_{p∈P} exp(β p̂_a^T θ̂) p̂_p / Σ_{p∈P} exp(β p̂_a^T θ̂) ≈ Σ_{p∈P} I(⌊n/2(1 - p̂_a^T θ̂)⌋; d; n) p̂_p / Σ_{p∈P} I(⌊n/2(1 - p̂_a^T θ̂)⌋; d; n). (7)

Thay vì chuyển đổi cosine similarity thành khoảng cách Hamming để sử dụng giao điểm vòng tròn Phương trình 2 trong không gian vector nhị phân, chúng ta có thể mở rộng SDM để hoạt động với các địa chỉ mẫu và nơ-ron đã chuẩn hóa L2 (Phụ lục B.3). Giao điểm vòng tròn SDM liên tục này khớp chặt chẽ với đối tác nhị phân của nó trong việc xấp xỉ mũ:

I_c(p̂_a^T θ̂; 1-2d/n; n) ≈ c_4 exp(β̃ p̂_a^T θ̂). (8)

Chúng ta sử dụng I_c để ký hiệu giao điểm liên tục này, sử dụng Phương trình 5 để ánh xạ d Hamming của chúng ta vào cosine similarity, và sử dụng các hệ số c_4 và β̃ để thừa nhận các giá trị hơi khác nhau của chúng. Sau đó, chúng ta cũng có thể liên hệ Attention như chúng ta đã làm trong Phương trình 7 với SDM liên tục như sau:

θ̃^new = P̂_p softmax(P̂_a^T θ̂) = Σ_{p∈P} exp(β p̂_a^T θ̂) p̂_p / Σ_{p∈P} exp(β p̂_a^T θ̂) ≈ Σ_{p∈P} I_c(p̂_a^T θ̂; 1-2d/n; n) p̂_p / Σ_{p∈P} I_c(p̂_a^T θ̂; 1-2d/n; n). (9)

Chúng ta đã viết Attention với các vector đã chuẩn hóa L2 và mở rộng hoạt động softmax để hiển thị rằng nó được xấp xỉ khi chúng ta thay thế các trọng số mũ bằng giao điểm vòng tròn SDM nhị phân hoặc liên tục (Phương trình 7 và 9, tương ứng). Vế phải của Phương trình (7) giống hệt với Phương trình 2 ngoài việc sử dụng các vector liên tục, đã chuẩn hóa L2 và bỏ hàm đa số theo từng phần tử g(·) đảm bảo đầu ra của chúng ta là một vector nhị phân. Trong Transformer, trong khi phương trình Attention không chứa bất kỳ hàm hậu xử lý nào cho việc cập nhật truy vấn θ̃^new của nó, sau đó nó được hậu xử lý bằng cách đi qua một phép chiếu tuyến tính và LayerNorm [1] và có thể được liên hệ với g(·).

Để phù hợp với SDM nhị phân, chúng ta chuyển đổi các khoảng cách Hamming thành cosine similarity sử dụng Phương trình 5 và sử dụng hồi quy log tuyến tính đơn biến:

log(I(d(p^a, θ); d; n)) ≈ log(c_3) + β(p̂_a^T θ̂). (10)

Chúng ta mong đợi hành vi mũ bị phá vỡ tại một điểm nào đó, nếu chỉ vì lý do rằng nếu d(p^a, θ) > 2d thì giao điểm vòng tròn trở thành không. Tuy nhiên, các mẫu gần hơn là những mẫu nhận được trọng số lớn nhất và "attention" sao cho chúng thống trị trong quy tắc cập nhật và là quan trọng nhất.

Trong Hình 3, chúng tôi vẽ sự xấp xỉ softmax cho SDM nhị phân và liên tục cho d_Mem = 5 nhỏ nhất và d_CD = 15 lớn nhất của chúng ta để hiển thị không chỉ chất lượng của các xấp xỉ mà còn có bao nhiêu bậc độ lớn nhỏ hơn các trọng số đã chuẩn hóa khi d(p^a, θ) > d. Đối với các biểu đồ này, chúng tôi cắm vào phương trình giao điểm vòng tròn nhị phân của chúng ta mỗi khoảng cách Hamming có thể từ 0 đến 64 khi n = 64 và chuyển đổi khoảng cách Hamming thành cosine similarity, làm tương tự cho giao điểm vòng tròn liên tục của chúng ta. Ở đây sử dụng các giá trị giao điểm nhị phân của chúng ta để fit β, tạo ra sự xấp xỉ mũ. Để tập trung sự xấp xỉ mũ của chúng ta vào các mẫu gần nhất, quan trọng nhất, chúng tôi fit hồi quy của chúng ta với những mẫu d(p^a, θ) < d và cho phép nó ngoại suy đến các giá trị còn lại. Sau đó chúng tôi chuẩn hóa các giá trị và vẽ chúng cùng với một biểu đồ nhỏ hơn trong không gian log để hiển thị tốt hơn mối quan hệ mũ. Trong cả hai biểu đồ, nhìn vào biểu đồ nhỏ log trước, điểm mà giao điểm vòng tròn màu xanh ngừng tồn tại hoặc là mũ tương ứng với một điểm trong biểu đồ chính đã chuẩn hóa nơi các trọng số là ≈ 0.

Số lượng nơ-ron r và mức độ chúng bao phủ đa tạp mẫu là những cân nhắc quan trọng sẽ xác định hiệu suất SDM và mức độ xấp xỉ với Attention. Tăng số lượng nơ-ron trong giao điểm vòng tròn có thể được thực hiện bằng cách tăng số lượng nơ-ron tồn tại, đảm bảo chúng bao phủ đa tạp mẫu, và giảm chiều của đa tạp để tăng mật độ nơ-ron. Trong công thức gốc của SDM, người ta giả định rằng các địa chỉ nơ-ron được phân bố ngẫu nhiên và cố định vị trí, tuy nhiên, các mở rộng của SDM [29] đã đề xuất các thuật toán học cạnh tranh có tính khả thi sinh học để học đa tạp [30]. Để đảm bảo các xấp xỉ với SDM là chặt chẽ, chúng tôi kiểm tra các mẫu ngẫu nhiên và tương quan trong một nhiệm vụ truy xuất tự kết hợp qua các số lượng nơ-ron khác nhau và các biến thể SDM (Phụ lục B.7). Những biến thể này bao gồm SDM được triển khai sử dụng các nơ-ron mô phỏng và sự xấp xỉ Attention với β đã fitted. Để tóm tắt, Attention xấp xỉ chặt chẽ quy tắc cập nhật SDM khi nó sử dụng các vector liên tục đã chuẩn hóa L2 và β được chọn đúng.

3 Attention Đã Huấn luyện Hội tụ với SDM

Đối với nhiều hiện thân của SDM, tồn tại một β có thể được tìm thấy qua hồi quy log tuyến tính Phương trình 10 làm cho Attention xấp xỉ nó tốt. Tuy nhiên, tùy thuộc vào nhiệm vụ đang thực hiện, có những hiện thân của SDM tốt hơn những cái khác như được làm nổi bật bởi các giá trị d tối ưu khác nhau. Nếu Attention trong mô hình Transformer đang triển khai SDM, chúng ta nên mong đợi cho Attention đã huấn luyện sử dụng β tương ứng với các trường hợp SDM hợp lý. Chúng tôi sử dụng như các điểm tham chiếu những d tối ưu này.

[Hình 4]: Biểu đồ tần số cho thấy các hệ số β đã học cho tất cả các đầu Attention qua các lớp cho 5 nhiệm vụ dịch thuật được sử dụng trong [22]. Chúng tôi vẽ các giá trị β cho Attention xấp xỉ các định nghĩa d khác nhau cho thấy cách các β đang nội suy giữa chúng. CD là tối ưu cho khoảng cách tới hạn (nhiễu tối đa cho mỗi truy vấn); SNR là tối ưu cho tỷ lệ Tín hiệu trên Nhiễu. Điều này giả định không có nhiễu truy vấn và SDM muốn giảm thiểu nhiễu từ các truy vấn khác. Mem tối đa hóa dung lượng bộ nhớ và cũng giả định không có nhiễu truy vấn.

Attention học các biểu diễn mẫu hữu ích cách xa ngẫu nhiên vì vậy SDM này phù hợp với các d tối ưu chỉ là một tham chiếu yếu cho những giá trị β nào có thể hợp lý. Tuy nhiên, bởi vì những định nghĩa d tối ưu này kéo dài từ tối đa hóa hội tụ với nhiễu truy vấn, đến tối đa hóa dung lượng bộ nhớ với các truy vấn không nhiễu, chúng ta nên mong đợi Transformer xử lý các truy vấn nhiễu và muốn truy xuất đáng tin cậy nội suy giữa những giá trị d này.

Ở đây, chúng tôi cung cấp bằng chứng thực nghiệm rằng đây thực sự là trường hợp. Chúng tôi phân tích các hệ số β được học bởi biến thể "Query-Key Normalization" Transformer Attention [22]. Query-Key Normalization làm cho việc tìm β trở nên đơn giản vì nó được học qua backpropagation và dễ diễn giải vì nó sử dụng cosine similarity giữa các vector query và key. Để đánh giá thêm sự hội tụ giữa các hệ số β Attention và SDM và làm cho nó tổng quát hơn, chúng tôi cũng điều tra kiến trúc GPT2 [3]. Tuy nhiên, trong trường hợp này chúng ta cần suy luận các giá trị β "hiệu quả" từ kích thước các tích vô hướng query key trong softmax. Điều này làm cho những kết quả này, được nêu trong Phụ lục A.2, xấp xỉ hơn nhưng chúng vẫn phần lớn đồng ý với các β đã học của Query-Key Norm.

Các đầu Attention Query-Key Norm học β ∈ [10, 25] như hiển thị trong Hình 4. Lưu ý rằng toàn bộ phạm vi β ∈ [10, 25] nội suy giữa các giá trị d tốt, đặc biệt với khoảng cách tới hạn tối ưu mà thực tế giả định các truy vấn nhiễu và SNR tối ưu, nơi có tỷ lệ tín hiệu trên nhiễu cao là mong muốn cho cả dung lượng bộ nhớ và khoảng cách tới hạn (xem Phụ lục B.5).

Trong việc yêu cầu rằng các vector Attention được chuẩn hóa L2 và β được fitted, SDM đã dự đoán Query-Key Norm. Điều này thú vị vì Query-Key Norm có bằng chứng về việc cải thiện hiệu suất Transformer và tốc độ huấn luyện [22, 31]. Chúng tôi nói thêm về những ưu điểm và cảnh báo của nó trong Phụ lục A.1.

--- TRANG 5 ---
4 Các Thành phần Transformer Được Diễn giải với SDM

Chúng ta có thể tận dụng cách Attention xấp xỉ SDM để diễn giải nhiều thành phần của kiến trúc Transformer. Bài tập này chứng minh sức mạnh giải thích của SDM và, trong việc liên hệ nó với các thành phần Transformer độc đáo bổ sung, mở rộng cây cầu mà trên đó các ý tưởng liên quan đến SDM và khoa học thần kinh có thể chuyển sang deep learning và ngược lại.

Một thành phần quan trọng của Transformer là lớp Feed Forward (FF) được xen kẽ với các lớp Attention và sử dụng khoảng 2/3 ngân sách tham số của Transformer [1]. Việc triển khai Transformer của Attention và SDM khác biệt quan trọng trong việc Attention sử dụng các mẫu nhất thời từ trường tiếp nhận hiện tại. Để mô hình hóa các phụ thuộc thời gian ngoài trường tiếp nhận, chúng ta muốn Attention có thể lưu trữ các bộ nhớ bền vững. Công trình đã chỉ ra một cách thuyết phục rằng các lớp FF lưu trữ những bộ nhớ bền vững này [32,33,34]. SDM có thể được diễn giải như lớp FF này vì trong [32] lớp FF đã được thay thế bằng các vector key và value bổ sung, bền vững mà Attention học độc lập thay vì chiếu từ các đầu vào hiện tại của nó. Sự thay thế này hoạt động ngang hàng với lớp FF mà, kết hợp với phân tích sâu hơn trong [33], cho thấy các lớp FF đang thực hiện Attention với các bộ nhớ dài hạn và do đó có thể được diễn giải trực tiếp như SDM.

Một thành phần quan trọng khác của Transformer là việc sử dụng LayerNorm [28,35,1]. LayerNorm có diễn giải tự nhiên bởi SDM như việc triển khai một ràng buộc tương tự với chuẩn hóa L2. Tuy nhiên, trong khi nó đảm bảo tất cả các vector ở cùng tỷ lệ sao cho các tích vô hướng của chúng có thể so sánh với nhau, nó không ràng buộc những tích vô hướng này vào khoảng [-1, 1] như cosine similarity. Ngoài việc cung cấp diễn giải cho tầm quan trọng của LayerNorm, sự khác biệt này đã dẫn đến hai hiểu biết: Thứ nhất, như đã đề cập trước đây, nó dự đoán rằng Query-Key Normalization có thể là một thiên hướng quy nạp hữu ích (chi tiết thêm trong Phụ lục A.1). Thứ hai, nó đã cung cấp cảnh báo cho các diễn giải về trọng số Attention. Hình 8 của Phụ lục A.3 cho thấy rằng có nhiều vector value nhận được lượng attention rất nhỏ nhưng có chuẩn L2 lớn thống trị việc tổng hợp có trọng số của các vector value. Điều này làm cho việc diễn giải trực tiếp trọng số Attention mà không chuẩn hóa L2 các vector value trở nên gây hiểu lầm và điều này chưa được thực hiện trong công trình bao gồm [1,36,37,38].

Ngoài việc giúp các diễn giải Attention trong tương lai, chúng tôi đã kiểm tra các vector value đã chuẩn hóa L2 như một thiên hướng quy nạp có thể hữu ích bằng cách huấn luyện một mô hình GPT2 với nó. Kết quả của chúng tôi cho thấy rằng điều này không thay đổi hiệu suất nhưng chuẩn hóa L2 vẫn nên được thực hiện trong các trường hợp mà trọng số Attention sẽ được diễn giải. Xem Phụ lục A.3 để có thảo luận đầy đủ.

Cuối cùng, multi-headed Attention là một thành phần Transformer nơi nhiều hiện thân của Attention hoạt động ở cùng mức độ phân cấp và có đầu ra của chúng được kết hợp. Multi-heading cho phép SDM mô hình hóa các đầu ra xác suất, cung cấp diễn giải cho lý do nó có lợi cho Attention. Ví dụ, nếu chúng ta đang học một chuỗi có thể đi "A → B" và "A → Z" với xác suất bằng nhau, chúng ta có thể có một mô-đun SDM học mỗi chuyển đổi. Bằng cách kết hợp các dự đoán của chúng, chúng ta gán đúng xác suất bằng nhau cho mỗi cái. Diễn giải xác suất này có thể giải thích bằng chứng cho thấy rằng các đầu Attention chú ý đến các đầu vào khác nhau và tại sao một số dư thừa sau huấn luyện [39, 40, 10].

Một sự khác biệt quan trọng giữa SDM và Transformer còn lại cần được hòa giải là trong việc xếp chồng phân cấp Attention của Transformer. Điều này là vì, không giống như trong setting SDM truyền thống nơi các địa chỉ mẫu (key) và con trỏ mẫu (value) được biết trước và được ghi vào bộ nhớ, điều này không thể được thực hiện cho các lớp SDM ngoài lớp đầu tiên sẽ cần học các biểu diễn tiềm ẩn cho các địa chỉ mẫu và con trỏ của nó (key và value). Transformer Attention giải quyết vấn đề này bằng cách học các key và value ở mức cao hơn của nó, coi mỗi token đầu vào như truy vấn riêng của nó để tạo ra một biểu diễn tiềm ẩn mới sau đó được chiếu vào key và value [1]. Điều này không có nghĩa là SDM sẽ thất bại trong việc hưởng lợi từ phân cấp. Như một ví dụ cụ thể, các hoạt động của SDM liên quan đến các hoạt động phân cấp của [41]. Rộng hơn, chúng tôi tin rằng suy nghĩ về cách học các key và value tiềm ẩn cho các mức cao hơn của SDM có thể trình bày các cải tiến Transformer mới.

Một bước đột phá chính của kiến trúc Performer gần đây làm nổi bật tính tùy ý của giải pháp Transformer gốc là việc sử dụng một tập hợp giảm các key và value tiềm ẩn [42].

5 Một Triển khai Có Tính Khả thi Sinh học của Attention

Ở đây, chúng tôi cung cấp cái nhìn tổng quan về tính khả thi sinh học của SDM để cung cấp một triển khai có tính khả thi sinh học của Attention. Các hoạt động đọc và ghi của SDM có yêu cầu kết nối không tầm thường được mô tả trong [13,15]. Mỗi nơ-ron phải: (i) biết kích hoạt nếu nó trong khoảng cách Hamming d của một đầu vào; (ii) cập nhật duy nhất mỗi phần tử của vector lưu trữ của nó khi ghi một mẫu mới; (iii) xuất vector lưu trữ của nó trong khi đọc sử dụng các đường đầu ra chia sẻ để tất cả đầu ra nơ-ron có thể được tổng hợp lại với nhau.

Các đặc điểm kiến trúc độc đáo của vỏ não tiểu não có thể triển khai tất cả những yêu cầu này, cụ thể thông qua sự hội tụ ba chiều giữa các tế bào hạt, sợi leo và tế bào Purkinje: (i) tất cả các tế bào hạt nhận đầu vào từ cùng các sợi rêu để kiểm tra xem chúng có trong d của truy vấn hoặc mẫu đến không; (ii) mỗi tế bào hạt có một sợi song song rất dài lưu trữ bộ nhớ trong các khớp thần kinh với hàng nghìn tế bào Purkinje [43], được cập nhật bởi LTP/LTD (Long Term Potentiation/Depression) từ kích hoạt chung với các sợi leo; (iii) tất cả các tế bào hạt xuất các bộ nhớ đã lưu trữ của chúng qua các khớp thần kinh của chúng đến các tế bào Purkinje thực hiện hoạt động tổng hợp và sử dụng ngưỡng kích hoạt của chúng để xác định xem bit đa số là 1 hay 0, xuất truy vấn mới [13,15]. Hơn nữa, Drosophila mushroom body rất tương tự với tiểu não và các nhãn tế bào trước đây cho mỗi chức năng có thể được thay thế bằng các tế bào Kenyon, nơ-ron dopaminergic, và các nơ-ron đầu ra mushroom body, tương ứng [17].

Trong khi SDM phù hợp tốt với các đặc điểm độc đáo của tiểu não, kết nối này có những hạn chế. Các giải thích cho một số hạn chế của mô hình gốc đã được đưa ra để giải thích cho các kết nối dendrite thưa thớt của các tế bào Granule [44] và chức năng của ít nhất hai trong ba nơ-ron ức chế trung gian: tế bào Golgi, Stellate và Basket [29,45]. Tuy nhiên, vẫn còn những thách thức khác, bao gồm giải thích tốt hơn về các đầu vào cho các sợi rêu và leo và đầu ra từ các tế bào Purkinje; đặc biệt, cách các đầu vào sợi rêu và leo đồng bộ hóa cho tính dẻo phụ thuộc thời gian đột phóng đúng [46]. Một hiện tượng khác mà SDM không giải thích được là khả năng của các tế bào Purkinje lưu trữ các khoảng thời gian liên quan đến bộ nhớ [47]. Nghiên cứu thêm là cần thiết để cập nhật tình trạng tính khả thi sinh học của SDM với các phát hiện khoa học thần kinh hiện đại.

6 Công trình Liên quan

Công trình trước đây đã chỉ ra rằng Hopfield Network hiện đại, khi được làm liên tục và tối ưu hóa khác nhau, trở thành Attention [25,26]. Kết quả này là một động lực cho công trình này vì Hopfield Networks là một mô hình bộ nhớ kết hợp khác. Thực tế, đã được chỉ ra rằng SDM là một sự tổng quát hóa của Hopfield Network gốc (Phụ lục B.6) [29]. Trong khi SDM là một sự tổng quát hóa của Hopfield Networks, những khác biệt cụ thể của chúng cung cấp các góc nhìn khác nhau về Attention. Đáng chú ý nhất, Hopfield Networks giả định trọng số đối xứng tạo ra một cảnh quan năng lượng, có thể được sử dụng mạnh mẽ trong các chứng minh hội tụ, bao gồm chỉ ra rằng hội tụ một bước là có thể cho Hopfield Network hiện đại, và bằng proxy, Attention và SDM khi nó là một xấp xỉ chặt chẽ [25,26]. Tuy nhiên, những trọng số đối xứng này đến với chi phí của tính khả thi sinh học mà SDM cung cấp ngoài framework hình học và mối quan hệ với Vector Symbolic Architectures [29, 48].

Các công trình khác đã cố gắng diễn giải lại hoặc loại bỏ hoạt động softmax từ Attention vì hằng số chuẩn hóa có thể tốn kém để tính toán [49,50]. Tuy nhiên, trong khi giảm chi phí tính toán, những bài báo này cho thấy rằng loại bỏ hoạt động softmax làm hại hiệu suất. Trong khi đó, SDM không chỉ cho thấy cách Attention có thể được viết như một mô hình Feedforward [15] mà còn tiết lộ rằng thông qua các hoạt động đọc và ghi nhị phân đơn giản, (nơ-ron hoặc ở trong khoảng cách Hamming/cosine hoặc không) hàm softmax nổi lên mà không có chi phí tính toán bổ sung.

Kể từ khi SDM được xuất bản, đã có một số tiến bộ không chỉ đối với SDM cụ thể, mà còn thông qua việc tạo ra các thuật toán bộ nhớ kết hợp liên quan dưới tên "Vector Symbolic Architectures" [51]. Các tiến bộ đối với SDM bao gồm sử dụng các vector số nguyên thay vì nhị phân [52], xử lý các mẫu tương quan [29], và lưu trữ dữ liệu phân cấp [53]. Vector Symbolic Architectures, đáng chú ý nhất là Holographic Reduced Representations, có những ý tưởng có thể được liên hệ ngược lại với SDM và Transformer theo những cách có thể có lợi [54, 55, 56, 57, 58, 59].

Việc sử dụng các mô-đun bộ nhớ bên ngoài trong mạng nơ-ron đã được khám phá đáng chú ý nhất với Neural Turing Machine (NTM) và phần tiếp theo của nó, Differentiable Neural Computer (DNC) [23,60]. Để có các hoạt động đọc và ghi khả vi đến bộ nhớ bên ngoài, chúng sử dụng hàm softmax. Điều này, kết hợp với việc sử dụng cosine similarity giữa truy vấn và các vị trí bộ nhớ, làm cho cả hai mô hình liên quan chặt chẽ đến SDM. Một cải tiến gần đây hơn cho NTM và DNC được trực tiếp lấy cảm hứng từ SDM là Kanerva Machine [24,61,62,63]. Tuy nhiên, Kanerva Machine vẫn khác biệt với SDM và Attention vì nó không áp dụng ngưỡng khoảng cách Hamming trên cosine similarity giữa truy vấn và nơ-ron của nó. Độc lập với những khác biệt này, chúng tôi tin rằng việc liên hệ những mô-đun bộ nhớ bên ngoài thay thế này với SDM trình bày một số ý tưởng thú vị sẽ được khám phá trong công trình tương lai.

7 Thảo luận

Kết quả rằng Attention xấp xỉ SDM nên cho phép nhiều ý tưởng chéo hơn giữa khoa học thần kinh, các mô hình lý thuyết về học kết hợp, và deep learning. Xem xét các hướng cho nghiên cứu deep learning tương lai, mối quan hệ của SDM với Vector Symbolic Architectures đặc biệt hấp dẫn vì chúng có thể áp dụng các hoạt động logic và tượng trưng trên bộ nhớ làm cho SDM mạnh mẽ hơn [55,64,65,66,67]. SDM và mối quan hệ của nó với não có thể truyền cảm hứng cho nghiên cứu mới không chỉ trong deep learning mà còn trong khoa học thần kinh, vì thành công thực nghiệm của Transformer và mối quan hệ của nó với tiểu não, thông qua SDM.

Kết quả của chúng tôi phục vụ như một ví dụ mới về cách các hoạt động deep learning phức tạp có thể được xấp xỉ bởi và ánh xạ lên các thuộc tính chức năng và mẫu kết nối của các quần thể nơ-ron. Tại thời điểm nhiều công cụ khoa học thần kinh mới đang lập bản đồ các vùng thần kinh chưa được khám phá, chúng tôi hy vọng rằng nhiều khám phá theo hướng công trình này kết nối deep learning với não sẽ được thực hiện [68, 69, 70].

Hạn chế Trong khi công trình của chúng tôi cho thấy một số hội tụ giữa SDM, Attention, và các mô hình Transformer đầy đủ, những mối quan hệ này vẫn xấp xỉ. Sự xấp xỉ chính là liên kết giữa SDM và Attention tồn tại không chỉ trong giao điểm vòng tròn của SDM là xấp xỉ mũ mà còn trong việc sử dụng không gian nhị phân thay vì liên tục. Một xấp xỉ khác là giữa các bán kính Hamming d SDM tối ưu và các hệ số β Attention. Điều này là vì chúng ta giả định các mẫu là ngẫu nhiên để dẫn xuất các giá trị d. Ngoài ra, trong các mô hình Transformer GPT2, chúng ta phải suy luận các giá trị β hiệu quả của chúng. Cuối cùng, chỉ có mối quan hệ xấp xỉ giữa SDM và kiến trúc Transformer đầy đủ, cụ thể với các thành phần Feed Forward và LayerNorm của nó.

8 Kết luận

Chúng tôi đã chỉ ra rằng quy tắc cập nhật Attention xấp xỉ chặt chẽ SDM khi nó chuẩn hóa L2 các vector của nó và có hệ số β thích hợp. Kết quả này đã được chỉ ra đúng trong cả lý thuyết và đánh giá thực nghiệm của các mô hình Transformer đã huấn luyện. SDM dự đoán rằng Transformers nên chuẩn hóa các vector key, query và value của chúng, đi trước việc phát triển Query-Key Normalization và thêm sắc thái vào diễn giải trọng số Attention. Chúng tôi ánh xạ SDM lên toàn bộ kiến trúc Transformer, liên hệ nó với các triển khai bộ nhớ bên ngoài khác, và làm nổi bật các mở rộng cho SDM. Bằng cách thảo luận về cách SDM có thể được ánh xạ lên các kiến trúc não cụ thể, chúng tôi cung cấp một triển khai sinh học tiềm năng của Transformer Attention. Do đó, công trình của chúng tôi làm nổi bật một liên kết khác giữa deep learning và khoa học thần kinh.

Lời cảm ơn
Cảm ơn Dr. Gabriel Kreiman, Alex Cuozzo, Miles Turpin, Dr. Pentti Kanerva, Joe Choo-Choy, Dr. Beren Millidge, Jacob Zavatone-Veth, Blake Bordelon, Nathan Rollins, Alan Amin, Max Farrens, David Rein, Sam Eure, Grace Bricken, và Davis Brown vì đã cung cấp cảm hứng, thảo luận và phản hồi vô giá. Cảm ơn đặc biệt Miles Turpin vì sự giúp đỡ trong việc làm việc với các thí nghiệm mô hình Transformer. Chúng tôi cũng muốn cảm ơn các nhà đóng góp phần mềm mã nguồn mở đã giúp làm cho nghiên cứu này có thể thực hiện được, bao gồm nhưng không giới hạn ở: Numpy, Pandas, Scipy, Matplotlib, PyTorch, HuggingFace, và Anaconda. Công trình được tài trợ bởi Chương trình Tiến sĩ Systems, Synthetic, and Quantitative Biology của Harvard.

[Phần còn lại của tài liệu bao gồm các tham khảo và phụ lục sẽ được dịch tiếp theo nếu cần...]
