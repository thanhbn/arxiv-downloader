# 2310.11685.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2310.11685.pdf
# File size: 967241 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Superiority of Softmax: Unveiling the Performance Edge Over
Linear Attention
Yichuan Deng∗Zhao Song†Tianyi Zhou‡
Abstract
Large transformer models have achieved state-of-the-art results in numerous natural language
processing tasks. Among the pivotal components of the transformer architecture, the attention
mechanism plays a crucial role in capturing token interactions within sequences through the
utilization of softmax function.
Conversely, linear attention presents a more computationally efficient alternative by ap-
proximating the softmax operation with linear complexity. However, it exhibits substantial
performance degradation when compared to the traditional softmax attention mechanism.
In this paper, we bridge the gap in our theoretical understanding of the reasons behind the
practical performance gap between softmax and linear attention. By conducting a comprehen-
sive comparative analysis of these two attention mechanisms, we shed light on the underlying
reasons for why softmax attention outperforms linear attention in most scenarios.
∗ycdeng@cs.washington.edu . The University of Washington.
†zsong@adobe.com . Adobe Research.
‡tzhou029@usc.edu . University of Southern California.arXiv:2310.11685v1  [cs.CL]  18 Oct 2023

--- PAGE 2 ---
1 Introduction
Large language models (LLMs) like Transformer [VSP+17], BERT [DCLT18], RoBERTa [LOG+19],
XLNet [YDY+19], GPT-3 [BMR+20], OPT [ZRG+22], PaLM [CND+22], Llama [TLI+23], Llama2
[TMS+23], Adobe firefly [Inc23], and BARD [Man23] have proven to be efficient methods for ad-
dressing complex natural language tasks such as content creation, summarization, and dialogue
systems [TDFH+22, YCRI22, WTB+22]. The utilization of attention mechanisms has revolution-
ized the landscape of computer vision and natural language processing, significantly enhancing
network performance. However, a critical challenge lies in the escalating memory and computa-
tional demands associated with the prevalent dot-product attention mechanism, particularly as the
increase of the input length. The quadratic computational complexity of this attention mechanism
with respect to the number of tokens has historically hindered its applicability to processing lengthy
sequences.
Recent efforts in the research community have been dedicated to developing efficient attention
architectures, aiming to mitigate computation complexity and memory usage. Linear attention is
one of the proposed method that has been widely studied [LSDZ20, KVPF20, SZZ+21a, ZWK22,
LCZ+23]. In [LSDZ20], they propose a Linear Attention Mechanism which is approximate to
dot-product attention with much less memory and computational costs based on first-order ap-
proximation of Taylor expansion.
In practical applications, softmax attention consistently demonstrates superior performance
when compared to linear attention. We hypothesize that for Transformer,
there exist some datasets that can only be effectively classified using softmax attention, while
linear attention proves inadequate.
In this paper, we delve into a comprehensive comparison between softmax attention and linear
attention, examining their attributes from both experimental and theoretical perspectives.
Our construction is inspired by the hardness proofs in fine-grained complexity [BIS17, Che18,
CJW19, ACSS20, AS23a, AS23b, ALS+23, JX23]. Let K∈Rd×d, Q∈Rd×d, V∈Rd×ddenote
the learned key matrix, query matrix and the value matrix respectively in the attention layer. Let
A1, A2denote the input sequences. The formulation of the softmax cross attention is defined as:
Att(X, Y ) =D(X)−1exp(A1XA⊤
2)A2Y (1)
where X∈Rd×ddenotes the combined parameter X:=QK⊤,Y∈Rd×d:=VandD(X) :=
diag(exp( A1XA⊤
2)1n)∈Rd×ddenotes the softmax normalization. For linear cross attention, we
replace the exp with lin, which stands for linear. Note that, self attention is a special case for the
cross attention, i.e., A2=A1. Let Fexprepresent the neural network employing softmax attention
and ReLU activation, while Flindenotes the neural network utilizing linear attention. Next, we
state our main results.
1.1 Our Result
For the self-attention, we need A1=A2in Eq.(1).
Theorem 1.1 (Self-attention, informal of Section D) .There exists two (self-attention) datasets
D0⊂Rn×dandD1⊂Rn×d. There exists two four-layer neural networks: Fexp:Rn×d→Rwhich
uses softmax units and ReLU units, and Flin:Rn×d→Rwhich uses linear attention units and
ReLU units such that with high probability (the randomness is over the weights of network)
•Fexpcan distinguish D0andD1
1

--- PAGE 3 ---
•Flincannot distinguish D0andD1
For cross attention, we need A1̸=A2in Eq. (1), and thus the function of neural network in
fact takes two n×dmatrices as inputs.
Theorem 1.2 (Cross-attention, informal of Section F) .There exists two (cross-attention) datasets
D0⊂R2n×dandD1⊂R2n×d. There exists two four-layer neural networks: Fexp:Rn×d×Rn×d→R
which uses softmax units and ReLU units, and Flin:Rn×d×Rn×d→Rwhich uses linear attention
units and ReLU units such that with high probability (the randomness is over the weights of network)
•Fexpcan distinguish D0andD1
•Flincannot distinguish D0andD1
2 Related Work
The efficacy of Transformer architectures in particular relies entirely on a self-attention mechanism
[PTDU16, LFS+17] to compute a series of context-informed vector-space representations of the
symbols in its input and output, which are then used to predict distributions over subsequent
symbols as the model predicts the output sequence symbol-by-symbol. Concrete evidence from
studies [TDP19, VB19, HL19, Bel22] underscores the pivotal role of attention with multilayer
perceptron in transmitting crucial information, facilitating diverse probing assignments.
Contemporary investigations have delved deep into Transformers’ capabilities. Topics explored
range from Turing completeness [PMB19, BPG20], functional representation [YBR+20, CDW+21],
to the representation of formal languages [BAG20, EGZ20, YPPN21] and the mastering of abstract
algebraic operations [ZBB+22]. Some research alludes to the potential of Transformers to serve
as universal adaptors for sequence-based tasks and even mimic the capabilities of Turing machines
[PMB19, BPG20].
However, the quadratic computational complexity of this attention mechanism with respect to
the number of tokens has historically hindered its applicability to processing lengthy sequences.
[DCL+22] introduced the Pixelated Butterfly model, a strategy employing a consistent sparsity
pattern to speed up Transformer training. Performer [CLD+20] is an example of the low-rank vari-
ant, which uses kernelization to speed up the computation. There are also work that approximate
attention computations during the inference phase, provided the precision is adequately maintained.
[LWD+23] highlights the phenomenon of contextual sparsity in LLM and its predictability. They
leveraged this insight to accelerate LLM inference without compromising on output quality.
Numerous investigations, encompassed by references such as [CGRS19, KKL20, WLK+20,
DKOD20, KVPF20, CDW+21, CDL+22], have illuminated various facets of this domain. Sub-
sequent research, represented by works like [ZHDK23, AS23a, BSZ23, DMS23, KMZ23, AS23b,
HJK+23, AG23, MDA22], have delved deeply into the computation of attention matrices, high-
lighting its intricacies and advocating for optimized algorithms.
Furthermore, significant strides have been made in understanding the power of attention mecha-
nisms within Transformers, as illustrated by studies [DGV+18, VBC20, ZKV+20, EGKZ21, SZKS21,
WCM21, DSX23, DLMS23]. The work of [ZPGA23] underscored the potential of mid-scale masked
language models to recognize syntactic components, presenting possibilities for partial parse tree
reconstructions. This innovative concept, postulated by [ZPGA23], facilitated [DGS23] in their
exploration of tensor cycle rank approximation challenges. [GMS23] subsequently turned their lens
towards the exponential regression in the context of the over-parameterized neural tangent kernel.
2

--- PAGE 4 ---
While [LSZ23] engaged in evaluating a regularized version of the exponential regression, a no-
table omission was the normalization factor. In a distinct approach, [DLS23] emphasized softmax
regression, encompassing this normalization factor, thereby differentiating their work from prior
exponential regression research [GMS23, LSZ23].
Roadmap We first provide a toy example, which simplify the attention formulation, before
we propose our main results. In Section 3 we provide some tools that used in our proof and the
formulation of our toy model. In Section 4, we provide some properties of the dataset that use in
the proof of the toy example. In Section 5, we provide the theoritical analysis of the performance
of different models in binary classification task. In Section 6, we proposed our main results for
both self-attention and cross-attention. In Section 7, we show some experiments that shows the
robustness of our theoretical results.
3 Preliminary
Here in this section, we provide some preliminaries to be used.
Notations.
For a positive integer n, the set {1,2,···, n}is denoted by [ n]. Given vectors uandv, their
inner product is represented as ⟨u, v⟩. For any u∈Rn, the vector with entries exp( x)i= exp( xi)
is given by exp( u)∈Rn. A vector of length nwith all entries being one is represented as 1n.
Considering a matrix A∈Rn×d, its i-th column is referred to as A∗, ifor every i∈[d]. The
element-wise product of two vectors uandvis denoted by u◦v, where the i-th entry is uivi. We
useE[] to denote expectation. We use Pr[] to denote the probability.
3.1 Probability Tools
Lemma 3.1 (Hoeffding bound [Hoe63]) .LetX1,···, Xndenote nindependent bounded variables
in[ai, bi]. Let X=Pn
i=1Xi, then we have
Pr[|X−E[X]| ≥t]≤2 exp
−2t2
Pn
i=1(bi−ai)2
.
3.2 Definitions of Functions
Definition 3.2 (Linear functions) .We define ulin:Rn×d→Rnulin(A;x) := Ax. We de-
fineαlin:Rn×d→Rαlin(A;x) :=⟨ulin(A, x),1n⟩. We define flin:Rn×d→Rnflin(A;x) :=
αlin(A, x)−1ulin(A, x).
Definition 3.3 (Softmax functions) .We define uexp:Rn×d→Rnuexp(A;x) := exp( Ax).. We de-
fineαexp(A, x) :=⟨uexp(A, x),1n⟩. We define fexp(x)as follows fexp(A, x) :=αexp(A, x)−1uexp(A, x).
Definition 3.4 (ReLU) .We define ϕ(z) := max {z,0}.
More generally, for parameter τ, we define ϕτ(z) := max {z−τ,0}.
Definition 3.5. Letτ >0denote a parameter. We define a three-layer neural network (with soft-
max attention and ReLU activation) Fexp:Rn×d→RFexp(A;x, y) :=ϕ(Pm
j=1ϕτ(⟨fexp(A, x), yj⟩)).
Definition 3.6. We define a four-layer neural network(with linear attention and ReLU activation)
Flin:Rn×d→RFexp(A;x, y) :=ϕ(Pm
j=1ϕτ(⟨flin(A, x), yj⟩))
3

--- PAGE 5 ---
f1
f2
fn−1
fnσ
σσ
σσ
σσ
σ1
1A
A
A
A1
2
...
n−1
n1
...
moLayer #1 Layer #2 Layer #3 Layer #4Figure 1: Visualization of our neural network in Section 4. Here m=O(logn).
Block #1
Block #2
Block #3
Block #4A c(A, X)
Attention
Computationy1
y2oLayer #1 Layer #2 Layer #3 Layer #4
Figure 2: This is structure for self-attention. Let A=A1=A2=A3∈Rn×d. Let A=A1⊗A2.
Letn= 4,d= 3,m= 2. Let y1∈Rd. Let c(A, x)∈Rn×dwhich is R4×3. Let c(A, x)1∈R3denote
the first block in layer 2. For the first node in first block in layer 3, the computation is based on
ReLU of ⟨c(A, X)1, y1⟩.
Definition 3.7. LetC > 1denote some constant. Let m=Clog(n/δ). Let y∈Rn×m, for
each j∈[m], we use yjto denote the j-th column of y. For each entry in yjwe sampled it from
Rademacher random variable distribution.
The settings of the above neural network are natural in deep learning theory [ZSJ+17, LL18,
DZPS19, AZLS19a, AZLS19b, SY19, SZZ21b, BPSW21, ALS+23, MOSW22].
4

--- PAGE 6 ---
3.3 Definition of Datasets for Binary Classification
Definition 3.8 (Binary classification) .Given two sets of datasets D0andD1.
•For each A∈Rn×dfromD0, we assume that (Ax)i∈[logn,1.4 logn]for all i∈[n]
•For each A∈Rn×dfromD1, we assume that there is one index j∈[n]such that (Ax)j=
4 lognand for all i∈[n]\{j}, we have (Ax)i∈[logn,1.4 logn]
4 Property of Dataset
500 1000 1500 2000
n0.00.20.40.60.81.0Ratio of Success
(a) Ratio of Success with respect to n
0 20 40 60 80 100
m0.00.20.40.60.81.0Ratio of Success (b) Ratio of Success with respect to m
Figure 3: Figure for the softmax regression model
Lemma 4.1. Letσ∼ {− 1,+1}ndenote a random sign vector. Let C > 1be a sufficiently large
constant. Let δ∈(0,0.1). Then, for each A∈ D 0, we have Part 1. Prσ[|Pn
i=1flin(A, x)iσi| ≤
C√
log(n/δ)√n]≥1−δ/poly( n)Part 2. Prσ[|Pn
i=1fexp(A, x)iσi| ≤C√
log(n/δ)
n0.1]≥1−δ/poly( n)
Proof. Proof of Part 1. Note that we can show that
|flin(A, x)i|=αlin(A, x)−1|ulin(A, x)i|
≤(nlogn)−1·(1.4 logn)
≤2
n
where the first step follows from the definition of α, the second step follows from Definition 3.8 and
the last step follows from simple algebra.
Thus, applying Hoeffding inequality, we can get with probability 1 −δ/poly( n),
|nX
i=1flin(A, x)iσi| ≤Cp
nlog(n/δ)
n=Cp
log(n/δ)√n
Proof of Part 2. For all i∈[n], we know that
|fexp(A, x)i|=αexp(A, x)−1· |uexp(A, x)i|
5

--- PAGE 7 ---
≤(n·n)−1·(n1.4)
=1
n0.6
Thus, applying Hoeffding inequality, we can get with probability 1 −δ/poly( n),
|nX
i=1fexp(A, x)iσi| ≤Cp
nlog(n/δ)
n0.6
≤Cp
log(n/δ)
n0.1
Lemma 4.2. Letσ∼ {− 1,+1}ndenote a random sign vector. Let C > 1be a sufficiently large
constant. Let δ∈(0,0.1). Then, for each A∈ D 1, we have
•Part 1. Prσ[|Pn
i=1flin(A, x)iσi| ≤Cp
log(n/δ)/n]≥1−δ/poly( n)
•Part 2. Prσ[|Pn
i=1fexp(A, x)iσi| ≥1/4]≥1
•Part 3. Pr[Pn
i=1fexp(A, x)iσi>0] = 1 /2andPr[Pn
i=1fexp(A, x)iσi<0] = 1 /2
Proof. Proof of Part 1. we can show that
|flin(A, x)i|=αlin(A, x)−1|u(A, x)i|
≤((n−1) log n+ 4 log n)−1·(4 log n)
=4
n+ 3
≤4
n
where the first step follows from the definition of flin, the second step follows from Definition 3.8,
the third step follows from simple algebra and the last step follows from simple algebra.
Applying Hoeffding inequality again, we finish the proof.
Proof of Part 2.
There is one index j∈[n], we know that
|fexp(A, x)j|=αexp(A, x)−1· |uexp(A, x)j|
≥(n1.4·(n−1) +n4)−1·(n4)
≥1
2
For all the i∈[n]\{j}, we know that
|fexp(A, x)i|=αexp(A, x)−1· |uexp(A, x)i|
≤(n·(n−1) +n4)−1·(n1.4)
≤1
n2.6
Thus, it is obvious that
|nX
i=1fexp(A, x)iσi| ≥1
2−(n−1)·1
n2.6≥1/4
6

--- PAGE 8 ---
where the last step follows from n≥4.
Proof of Part 3. The sign is completely decided by σj, thus it has 1 /2 chance to be positive
and 1 /2 chance to be negative.
5 Binary Classification
In this section, we provide an overview of theoretical analysis of the performance of different models
in binary classification task.
5.1 Softmax Attention
Lemma 5.1. For each data AfromD0,Fexp(A) = 0 with probability at least 1−δ/poly( n).
Proof. Note that all the ylare independent, for each l∈[m], we call Part 2 of Lemma 4.1, we can
show that ϕτ(⟨fexp(A, x), yl⟩) = 0.
Since m≤δ/poly( n), we are allowed to take union bound over all l∈[m]. Thus, we have
Flin(A, x) =ϕ(Pm
l=0ϕτ(⟨fexp(A, x), yl⟩)) = 0 .
Lemma 5.2. For each data AfromD1,Fexp(A)>0with probability at least 1−δ/poly( n).
Proof. By Part 2 and 3 of Lemma 4.2, we have ϕτ(⟨fexp(A, x), yl⟩)>1/4 with probability 1 /2.
Since all l∈[m] are independent, thus there exists one l∈[m] such that
ϕτ(⟨fexp(A, x), yl⟩)>1/4
the probability is 1 −(1/2)m≥1−δ/poly( n).
Thus, with probability 1 −δ/poly( n), we have
Fexp(A, x) =ϕ(mX
l=0ϕτ(⟨fexp(A, x), yl⟩))>0
holds.
5.2 Linear Attention
Lemma 5.3. For each data AfromD0,Flin(A) = 0 with probability at least 1−δ/poly( n).
Proof. Note that all the ylare independent, for each l∈[m], we call Part 1 of Lemma 4.1, we can
show that ϕτ(⟨flin(A, x), yl⟩) = 0.
Since m≤δ/poly( n), we are allowed to take union bound over all l∈[m]. Thus, we have
Flin(A, x) =ϕ(Pm
l=0ϕτ(⟨flin(A, x), yl⟩)) = 0.
Lemma 5.4. For each data AfromD1,Flin(A) = 0 with probability at least 1−δ/poly( n).
Proof. Note that all the ylare independent, for each l∈[m], we call Part 1 of Lemma 4.2, we can
show that ϕτ(⟨flin(A, x), yl⟩) = 0.
Since m≤δ/poly( n), we are allowed to take union bound over all l∈[m].
Thus, we have Flin(A, x) =ϕ(Pm
l=0ϕτ(⟨flin(A, x), yl⟩)) = 0
7

--- PAGE 9 ---
6 Main Results
We provide more details for self-attention result, for details of cross-attention, we refer the readers
to appendix.
Definition 6.1 (Self-Attention dataset distribution, informal version of Definition D.1) .We define
a0∈(0,0.1). We denote a1≥0.7. Let b+c= 1forb≥0.1, c≥0.1. Assume n= (d−2)twhere t
is a positive integer.
Given two sets of datasets D0andD1. For each A1∈ D 0, we have the first column of A1
isej3·a0√lognfor some j3∈[n]. From the second column to the (d−1)-th columns of A1are
Id−2
Id−2
...
Id−2
·b√logn. The last column of A1is1n·c√logn.
Assume n= (d−2)twhere tis a positive integer. For each A1∈ D 1, we have Type I column:
the first column of A1isej3·a1√lognforj3∈[n]. Type II column: from the second column to
the(d−1)-th columns of A1are
Id−2
Id−2
...
Id−2
·b√logn. Type III column: the last column of A1is
1n·c√logn.
200 400 600 800 1000
n0.00.20.40.60.81.0Ratio of Success
(a) Vary with n
0 10 20 30 40 50 60
d0.00.20.40.60.81.0Ratio of Success (b) Var with d
0 20 40 60 80
m0.00.20.40.60.81.0Ratio of Success (c) Vary with m
0.5 0.6 0.7 0.8 0.9
c0.00.20.40.60.81.0Ratio of Success
(d) Vary with c
0.0 0.2 0.4 0.6 0.8
a_00.00.20.40.60.81.0Ratio of Success (e) Vary with a0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
a_10.00.20.40.60.81.0Ratio of Success (f) Vary with a1
Figure 4: Self-Attention when X= vec( Id). Let ndenote the length of input sentence. Let d
denote the embedding size. Let mdenotes the width of the second layer. Let a1, a0, cdenote the
parameters of dataset D0andD1.
8

--- PAGE 10 ---
200 400 600 800 1000
n0.00.20.40.60.81.0Ratio of Success(a) Vary with n
5 10 15 20 25 30 35
d0.00.20.40.60.81.0Ratio of Success (b) Vary with d
0 20 40 60 80
m0.00.20.40.60.81.0Ratio of Success (c) Vary with m
0.5 0.6 0.7 0.8 0.9
c0.00.20.40.60.81.0Ratio of Success
(d) Vary with c
0.0 0.2 0.4 0.6 0.8
a_00.00.20.40.60.81.0Ratio of Success (e) Vary with a0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
a_10.00.20.40.60.81.0Ratio of Success (f) Vary with a1
Figure 5: Self-Attention when X=1n2. Let ndenote the length of input sentence. Let ddenote the
embedding size. Let mdenotes the width of the second layer. Let a1, a0, cdenote the parameters
of dataset D0andD1.
Theorem 6.2 (Main results, informal version of Section D) .Letτ= (c+ 0.1)√lognLetd∈
[ω(n0.02), n]Letm=O(log(n/δ))Letx=1d2Let
Fexp(A1) :=ϕ(nX
j0=1mX
l=1ϕτ(⟨Attj0,∗, yl⟩))
where Attj0,∗denote the j0-th line of Eq. (1). For any A1fromD1(Definition D.1) With high
probability 1−δ/poly( n), we have
Fexp(A1)>0, Fexp(A1) = 0
For any A1fromD0(Definition D.1) With high probability 1−δ/poly( n), we have
Flin(A1) = 0 , Flin(A1) = 0
7 Numerical Experiments
Here in this section, we present our numerical experiments of our proof. We ran simulation exper-
iments on the softmax regression model, the self attention model and the cross attention model.
We deploy all our experiments on an Apple MacBook Pro with M2 chip and 16GB of memory. The
Python we use is version 3.9.12.
9

--- PAGE 11 ---
7.1 Softmax Regression Model
We set the dataset as described in Section 4. For a pair of inputs ( A0, x0)∈ D 0and ( A1, x1)∈ D 1,
we define the event Esuccess as
Esuccess :=Fexp(A1, x1)>0∧Flin(A1, x1) = 0
∧Fexp(A0, x0) = 0∧Flin(A0, x0) = 0 .
We divide our numerical experiments for parameters nandm. To be specific,
•We deploy experiments for n∈[100,2000] with a step size of 10. For each n, we set m= log n.
For each group of the parameters, we generated the models for 1000 times and count the times
Esuccess happens. The result can be found in Figure (a) in Figure 3.
•We fix n= 1000, and deploy experiments for m∈[1,100] with a step size of 1. For each
group of the parameters, we generated the models for 1000 times and count the counts Esuccess
happens. The result can be found in Figure (b) in Figure 3.
7.2 Experiments for Self-Attention
We set the dataset as described in Section D.1. For a pair of inputs ( A01, A02, A03)∈ D 0and
(A11, A12, A13)∈ D 1, we define the event Esuccess as
Esuccess :=Fexp(A11, A12, A13)>0
∧Flin(A11, A12, A13) = 0
∧Fexp(A01, A02, A03) = 0
∧Flin(A01, A02, A03) = 0 .
7.2.1 For X= vec( Id)
We first fix X= vec( Id), and deploy the numerical experiments for parameters n,d,m,c,a0and
a1. To be specific,
•We deploy experiments for n∈[200,1000] with a step size of 10. For each n, we set d= 12,
δ= 0.01,m= max {log(n/δ),15},a0= 0.01,a1= 1.2,b= 0.2,c= 0.8. For each set
of parameters, we iteratively generated models 100 times and recorded the occurrences of
successful events denoted as Esuccess . The result can be found in Figure (a) in Figure 4.
•We deploy experiments for d∈ {4,6,10,18,34,66}. For each d, we set n= 256, δ= 0.01,
m= max {log(n/δ),15},a0= 0.01,a1= 1.2,b= 0.2,c= 0.8. For each set of parameters,
we iteratively generated models 100 times and recorded the occurrences of successful events
denoted as Esuccess . The result can be found in Figure (b) in Figure 4.
•We deploy experiments for m∈[1,80] with a step size of 1. For each m, we set n= 200,
d= 22, δ= 0.01,a0= 0.01,a1= 1.2,b= 0.2,c= 0.8. For each set of parameters,
we iteratively generated models 100 times and recorded the occurrences of successful events
denoted as Esuccess . The result can be found in Figure (c) in Figure 4.
•We deploy experiments for a1∈[0.1,3] with a step size of 0 .036. For each a1, we set n= 200,
d= 22, δ= 0.01,m= max {log(n/δ),15},a0= 0.01,b= 0.2,c= 0.8. For each set
of parameters, we iteratively generated models 100 times and recorded the occurrences of
successful events denoted as Esuccess . The result can be found in Figure (d) in Figure 4.
10

--- PAGE 12 ---
•We deploy experiments for a0∈[0.01,0.8] with a step size of 0 .01. For each a0, we set n= 200,
d= 22, δ= 0.01,m= max {log(n/δ),15},a0= 0.01,a1= 1.2,b= 0.2,c= 0.8. For each
set of parameters, we iteratively generated models 100 times and recorded the occurrences of
successful events denoted as Esuccess . The result can be found in Figure (e) in Figure 4.
•We deploy experiments for c∈[0.5,0.9] with a step size of 0 .005. For each c, we set n= 200,
d= 22, δ= 0.01,m= max {log(n/δ),15},a1= 1.2,b= 0.2. For each set of parameters,
we iteratively generated models 100 times and recorded the occurrences of successful events
denoted as Esuccess . The result can be found in Figure (f) in Figure 4.
7.2.2 For X=1n2
For the setting that X=1n2, we similarly deploy the experiments on parameters n,d,m,c,a0
anda1. The parameter setting and choice are the same as above. The experiment result is shown
in Figure 5.
8 Conclusion
The transformer architecture, propelled by its attention mechanism, has revolutionized natural
language processing tasks. Particularly, the softmax function utilized in the attention mechanism
plays a crucial role in grasping the token interactions within sequences. On the flip side, linear
attention, while computationally more efficient, falls short in its performance compared to softmax
attention. This paper delves into the core reasons for this observed discrepancy. Through metic-
ulous comparative analysis, it has been elucidated that softmax-based neural networks are more
adept at distinguishing between certain datasets compared to their linear attention counterparts,
both in self-attention and cross-attention scenarios. This pivotal revelation offers profound insights
into the intrinsic workings of attention mechanisms, thereby guiding the path for more optimized
future model developments.
Acknowledgments
The authors would like to thank Majid Daliri and Chenyang Li for helpful discussions.
11

--- PAGE 13 ---
Appendix
Roadmap. We organize the appendix as follows. Section A provides some preliminaries. We
discuss more related work in Section B. Section C describes the attention model setting. Section D
gives analysis for self attention when QK⊤=1d×dwhile Section E gives a further discussion when
QK⊤=Id. Section F provides analysis for cross attention model, with the result figure of our
experiment.
A Preliminary
Notations. We used Rto denote real numbers. We use A∈Rn×dto denote an n×dsize matrix
where each entry is a real number. We use ejto denote the unit vector where j-th entry is 1
and others are 0. For any positive integer n, we use [ n] to denote {1,2,···, n}. For a matrix
A∈Rn×d, we use ai,jto denote the an entry of Awhich is in i-th row and j-th column of A,
for each i∈[n],j∈[d]. For two vectors a, b, we use ⟨a, b⟩to denote their inner product. We
use1nto denote a length- nvector where all the entries are ones, and use 1n×nto denote a n×n
matrix where all the entries are ones. For a vector xor a matrix A, we use exp( x) and exp( A) to
denote the entry-wise exponential operation on them. For matrices A∈Rn1×d1andB∈Rn2×d2,
we use A⊗B∈Rn1n2×d1d2to denote a matrix such that its (( i1−1)n2+i2,(j1−i)d2+j2)-th
entry is Ai1,j1·Bi2,j2for all i1∈[n1], j1∈[d1], i2∈[n2], j2∈[d2]. For a matrix A∈Rn×d, we use
vec(A)∈Rndto denote the vectorization of A. We use Idto denote the d×didentity matrix. We
useA⊤to denote the transpose of a matrix A.
Using standard tensor-trick [GSX23, GSY23, GSWY23, AS23a, AS23b], we know that
Fact A.1 (Tensor-trick) .LetA1, A2∈Rn×d, letQ∈Rd×d, letK∈Rd×d, we have
vec(exp( A1QK⊤A⊤
2)) = exp( Avec(QK⊤))
where A:=A1⊗A2∈Rn2×d2.
B More Related Works
Theory of Transformer. With the ascent of LLMs, there’s a heightened interest in compre-
hending their learning prowess and deepening the theoretical underpinnings of transformer models.
A key focus is the in-context learning ability of transformers. [GTLV22] empirically demonstrated
that transformers can adeptly learn linear function classes in-context. [ASA+22] linked in-context
learning in transformers to conventional learning algorithms, a connection further validated through
linear regression. [ZFB23] explored the in-context learning of a single-head self-attention layer, not-
ing its strengths and weaknesses. [WZW23] viewed LLMs through a Bayesian lens, interpreting
in-context learning as a Bayesian selection process. Delving into the transformer’s architecture,
[PSZA23] introduced the ”skill location” concept, emphasizing how minor parameter tweaks dur-
ing fine-tuning can drastically impact performance and continual learning. On the capabilities
front, [SHT23] analyzed the strengths and limitations of transformers, highlighting their varying
growth rates in different tasks. [BCE+23] provided an in-depth analysis of GPT-4 [Ope23], lauding
its versatility across domains and advocating for more advanced models. Our research now pivots
to a unique 2-layer regression problem, inspired by the transformer paradigm.
12

--- PAGE 14 ---
Boosting Computation of Attention. The task of fine-tuning pre-trained LLMs presents chal-
lenges, primarily due to their extensive parameter sets. Efforts have been directed towards devising
efficient methods for computing the attention module. The use of locality sensitive hashing (LSH)
for attention approximation has been a topic of discussion in several studies, as seen in [KKL20]
and [CLP+21]. Specifically, [KKL20] introduced two methods to enhance computational efficiency.
They employed LSH as an alternative to dot product attention, resulting in a notable reduction in
time complexity. Additionally, they adopted a reversible residual layer in place of the conventional
residual layer. On the other hand, [CLP+21] refined the approximation technique, noting that LSH
doesn’t consistently demand updates to model parameters. In a different approach, [PMXA23]
proposed approximation methods that leverage a transformer-in-transformer (TinT) model to em-
ulate both the forward pass and back-propagation of a transformer, leading to enhanced parameter
efficiency. [MGN+23] delved into the efficient fine-tuning of LLMs, especially those with substantial
memory requirements. Building upon the traditional ZO-SCD optimizer, they crafted the memory-
efficient MeZO gradient estimator that operates using only the forward pass. Furthermore, [AS23a]
established a stringent bound for static attention, while [BSZ23] validated results concerning the
dynamic attention issue. Lastly, [GSYZ23] unveiled a quantum algorithm tailored for attention
computation.
Optimizer for LLMs. Gradient-based algorithms remain a cornerstone in machine learning.
In recent times, there has been a surge in research efforts aimed at devising efficient optimizers
tailored for LLM-centric optimization challenges. [CLMY21] explored large-scale optimization sce-
narios where basic vector operations on decision variables become unfeasible. To address this,
they employed a block gradient estimator, formulating an algorithm that markedly cuts down both
query and computational complexities per iteration. Taking a different approach, [RSM+23] in-
troduced the Direct Preference Optimization algorithm. This method fine-tunes LLMs directly
using a specified human preference dataset, bypassing the need for explicit reward models or re-
inforcement learning techniques. In another significant contribution, [LLH+23] unveiled an adept
second-order optimizer for LLMs. This optimizer is grounded in diagonal Hessian approximation
coupled with a clipping mechanism. Interestingly, they also eased the stipulation that the Hessian
must be Lipschitz continuous in their convergence proof. Drawing inspiration from this innovative
perspective, our research employs a congruent proof methodology, especially when addressing the
ReLU function within our regression analysis.
C Analysis for Q,K,V Attention
In Section C.1, we give the definition of linear attention. In Section C.2, we give the definition of
softmax attention.
C.1 Linear Attention
Definition C.1 (Linear Attention) .Given A1, A2, A3∈Rn×d.
Letx= vec( QK⊤)∈Rd2. Let A=A1⊗A2∈Rn2×d2.
LetV∈Rd×d.
LetAj0∈Rn×d2denote the j0-th block of A.
For each j0∈[n], we define ulin(A, x)j0∈Rnas follows
ulin(A, x)j0:=Aj0x
13

--- PAGE 15 ---
We define αlin(A, x)j0as follows
αlin(A, x)j0:=⟨ulin(A, x)j0,1n⟩
We define flin(A, x)j0as follows
flin(A, x)j0=αlin(A, x)−1
j0·ulin(A, x)j0
We define clin(A, x)j0∈Rdas follows
clin(A, x)j0,i0:=⟨flin(A, x)j0|{z}
n×1,(A3V)i0|{z}
n×1⟩,∀i0∈[d]
Definition C.2. Lety∈Rd×m. Let yl∈Rddenote the l-th column of y. We define
Flin(A1, A2, A3) :=ϕ(nX
j0=1mX
l=1ϕτ(⟨clin(A, x)j0|{z}
d×1, yl|{z}
d×1⟩))
c(A, x)∈Rn×d
y∈ {± 1}m×d
c(A, x)j0
yj1(j0, j1)Σj0∈[n],j1∈[m]ϕτ(⟨cj0, yj1⟩)
F(A, x)⟨·,·⟩
=
ϕ(·)
Figure 6: Visualization of our attention computation.
Claim C.3 (Equivalence Formula) .If the following conditions hold,
•Letx= vec( QK⊤)
•Let(A1)j0,∗denote the j0-th row of A1∈Rn×d
•LetA=A1⊗A2
Then, we have
•ulin(A, x)j0= ((A1)j0,∗QK⊤A⊤
2)⊤
•αlin(A, x)j0=⟨((A1)j0,∗QK⊤A⊤
2)⊤,1n⟩
Proof. The proofs are directly following from tensor-trick (Fact A.1).
14

--- PAGE 16 ---
C.2 Softmax Attention
Definition C.4 (Softmax Attention) .Given A1, A2, A3∈Rn×d.
Letx= vec( QK⊤)∈Rd2. Let A=A1⊗A2∈Rn2×d2.
LetAj0∈Rn×d2denote the j0-th block of A.
For each j0∈[n], we define uexp(A, x)j0∈Rnas follows
uexp(A, x)j0:= exp( Aj0x)
We define αexp(A, x)j0as follows
αexp(A, x)j0:=⟨uexp(A, x)j0,1n⟩
We define fexp(A, x)j0as follows
fexp(A, x)j0=αexp(A, x)−1
j0·uexp(A, x)j0
We define cexp(A, x)j0∈Rdas follows
cexp(A, x)j0,i0:=⟨fexp(A, x)j0,(A3V)i0⟩,∀i0∈[d]
Claim C.5 (Equivalence Formula) .If the following conditions hold,
•Letx= vec( QK⊤)
•Let(A1)j0,∗denote the j0-th row of A1∈Rn×d
•LetA=A1⊗A2
Then, we have
•uexp(A, x)j0= (exp(( A1)j0,∗QK⊤A⊤
2))⊤
•αexp(A, x)j0=⟨(exp(( A1)j0,∗QK⊤A2)⊤)⊤,1n⟩
Proof. The proofs are directly following from tensor-trick (Fact A.1).
Definition C.6. Lety∈Rn×m. Let yl∈Rndenote the l-th column of y. We define
Fexp(A1, A2, A3) :=ϕ(nX
j0=1mX
l=1ϕτ(⟨cexp(A, x)j0, yl⟩))
D Self-Attention Dataset
In Section D.1, we give the definition of the dataset used in the following sections for self-attention.
In Section D.2, we show that the output of the Fexpis greater than 0 with high probability. In
Section D.3, we show that the output of Fexpis equal to 0 with high probability. In Section D.4,
we show that the output of the Flinis equal to 0 with high probability. In Section D.5, we show
that the output of Flinis equal to 0 with high probability.
15

--- PAGE 17 ---
D.1 Definition of Dataset
Definition D.1 (Self-Attention dataset distribution) .We define
•a0∈(0,0.1)
•Leta1≥0.7
•b+c= 1forb≥0.1, c≥0.1
Given two sets of datasets D0andD1
•For each {A1, A2, A3} ∈ D 0, we have
–A1=A2=A3∈Rn×d
–Assume n= (d−2)twhere tis a positive integer
–the first column of A1isej3·a0√lognfor some j3∈[n]
–from the second column to the (d−1)-th columns of A1are
Id−2
Id−2
...
Id−2
·b√logn
–the last column of A1is1n·c√logn
•For each {A1, A2, A3} ∈ D 1, we have
–A1=A2=A3∈Rn×d
–Assume n= (d−2)twhere tis a positive integer
–Type I column: the first column of A1isej3·a1√lognforj3∈[n]
–Type II column: from the second column to the (d−1)-th columns of A1are
Id−2
Id−2
...
Id−2
·
b√logn
–Type III column: the last column of A1is1n·c√logn
D.2 Dataset 1 with Fexp
In Section D.2.1 we analyse the property of dataset 1 with respect to function uexpandfexp. In
Section D.2.2 we analyse the property of dataset 1 with respect to function cexp. In Section D.2.3 we
analyse the property of dataset 1 with respect to function cexpwith random signs. In Section D.2.4
we show the property of dataset 1 with respect to the output of Fexp.
D.2.1 Dataset 1 Property when applying function uexpand fexp
Lemma D.2. If the following conditions hold
•Letb+c= 1
•Let{A1, A2, A3}from dataset D1(see Definition D.1)
16

--- PAGE 18 ---
•LetQK⊤=1d×d(This is the major difference compared to Lemma E.1)
Then, for uexp(A, x)j0,j1andfexp(A, x)j0,j1entry we have
•Part 1. Forj0=j3,
– Part 1a. Forj1=j3, then uexp(A, x)j0,j1=n(1+a1)2.
– Part 1b. Forj1̸=j3, then uexp(A, x)j0,j1=n1+a1.
•Part 2. Forj0̸=j3
– Part 2a. Forj1=j3, then uexp(A, x)j0,j1=n1+a1
– Part 2b. Forj1̸=j3, then uexp(A, x)j0,j1=n.
•Part 3. Forj0=j3,
– Part 3a. Forj1=j3, then fexp(A, x)j0,j1≥1/2(if(1 +a1)2>2 +a1)
– Part 3b. Forj1̸=j3, then fexp(A, x)j0,j1≤1/n
•Part 4. Forj0̸=j3,
– Part 4a. Forj1=j3, then fexp(A, x)j0,j1≤1/n1−a1
– Part 4b. Forj0̸=j3, then fexp(A, x)j0,j1≤1/n
Proof. Proof of Part 1.
Proof of Part 1a. Forj0=j3andj1=j3, by computing the tensor product of two 3-sparse
vector
uexp(A, x)j0,j1
= exp( ⟨(a1p
logn,0,···,0, bp
logn,0,···,0, cp
logn)⊗(a1p
logn,0,···,0, bp
logn,0,···,0, cp
logn),1d×d⟩)
= exp(( a1p
logn+bp
logn+cp
logn)2)
= exp(( a1+b+c)2logn)
=n(1+a1)2
where the first step follows from Definition D.1, the second step follows from algebra, the second
step follows from simple algebra and the last step follows from simple algebra.
Proof of Part 1b. Forj0=j3andj1̸=j3.
uexp(A, x)j0,j1= exp(( a1p
logn+bp
logn+cp
logn)(bp
logn+cp
logn))
= exp((( b+c)2+a1(b+c)) log n)
=n1+a1
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from simple algebra.
Proof of Part 2.
Proof of Part 2a.
Forj0̸=j3andj1=j3, this case is same as Part 1b.
Proof of Part 2b.
Forj0̸=j3andj1̸=j3,
17

--- PAGE 19 ---
For the case that we tensor a 2-sparse vector with another 2-sparse vector, we have
uexp(A, x)j0,j1= exp(( bp
logn+cp
logn)2)
= exp(( b+c)2logn)
=n
Proof of Part 3.
Proof of Part 3a. We can show
fexp(A, x)j0,j1=⟨uexp(A, x)j0,1n⟩−1·uexp(A, x)j0,j1
=1
n(a1+1)2+ (n−1)n1+a1·n(1+a1)2
≥1
2
where the first step follows from the definition of fexp, the second step follows from Part 1 and
Part 2 and the last step follows from ( a1+ 1)2>2 +a1.
Proof of Part 3b.
fexp(A, x)j0,j1=⟨uexp(A, x)j0,1n⟩−1·uexp(A, x)j0,j1
=1
n(a1+1)2+ (n−1)n1+a1·n(1+a1)
≤1/n
Proof of Part 4.
Proof of Part 4a.
We can show
fexp(A, x)j0,j1=n(1+a1)
n1+a1+ (n−1)·n
≤1
n1−a1
where the first step follows from Part 1 andPart 2 and the last step follows from simple algebra.
Proof of Part 4b.
We can show
fexp(A, x)j0,j1=n
n1+a1+ (n−1)·n
≤1
n
where the first step follows from Part 1 andPart 2 and the last step follows from simple algebra.
D.2.2 Dataset 1 Property when applying function cexp
Lemma D.3. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition D.1)
•LetQK⊤=1d×d
18

--- PAGE 20 ---
•LetV=Id
•Letn=t(d−2)
Then for cexp(A, x)j0,i0entry we have
•Part 1. Forj0=j3andi0= 1, we have cexp(A, x)j0,i0≥1
2·a1√logn
•Part 2. Forj0=j3andi0∈ {2,···, d−1}
–There is only one i0, we have cexp(A, x)j0,i0≥1
2b√logn
–For the rest of i0, we have cexp(A, x)j0,i0≤t
nb√logn
•Part 3. Forj0=j3andi0=d, we have cexp(A, x)j0,i0=c√logn
•Part 4. Forj0̸=j3andi0= 1, we have cexp(A, x)j0,i0≤1
n1−a1a1√logn
•Part 5. Forj0̸=j3andi0∈ {2,···, d−1}, we have cexp(A, x)j0,i0≤t
nb√logn
•Part 6. Forj0̸=j3andi0=d, we have cexp(A, x)j0,i0=c√logn
Proof. Proof of Part 1.
It follows from Part 3 of Lemma D.2 and Type I column in A3(Definition D.1).
We can show for one-hot vector ej3∈Rn,
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, ej3·a1p
logn⟩
≥1
2a1p
logn
where the first step follows from Type I column in A3(Definition D.1) and the last step follows
from Part 3 of Lemma D.2.
Proof of Part 2.
It follows from Part 3 of Lemma D.2 and Type II column in A3(Definition D.1).
There are two cases we need to consider, there is one ( A3V)i0’sj3coordinate is 1, in this
situation, we know
⟨fexp(A, x)j0,(A3V)i0⟩ ≥1
2·bp
logn
For the other case, ( A3V)i0’sj3coordinate is 0, in this case we use the property that ( A3V)i0
ist-sparse vector. Then, we have
⟨fexp(A, x)j0,(A3V)i0⟩ ≤t·1
n·bp
logn
≤t
n·bp
logn,
where the first step follows from entries in fexp(A, x)j0are bounded by1
nand at most tentries in
(A3V)i0areb√lognand rest are zeros, the second step follows from simple algebra.
Proof of Part 3.
It follows from the fact that fexp(A, x)j0is a normalized vector (so ⟨fexp(A, x)j0,1n⟩= 1) and
Type III column in A3(Definition D.1).
19

--- PAGE 21 ---
We can show
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, cp
logn·1n⟩
=⟨fexp(A, x)j0,1n⟩ ·cp
logn
=cp
logn
Proof of Part 4.
It follows from Part 4 of Lemma D.2 and Type I column in A3(Definition D.1).
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, ej3·a1p
logn⟩
≤1
n1−a1a1p
logn
where the first step follows from Type I column in A3and the last step follows from, for j0̸=j3,
then fexp(A, x)j0,j1≤1/n1−a1(Part 4 of Lemma D.2).
Proof of Part 5.
It follows from Part 4 of Lemma D.2 and Type II column in A3(Definition D.1).
⟨fexp(A, x)j0,(A3V)i0⟩=tbp
logn·1
n
≤t
nbp
logn
where the first step follows from Type II column in A3and the last step follows from, for j0̸=j3,
then fexp(A, x)j0,j1≤1/n(Part 4 of Lemma D.2).
Proof of Part 6.
It follows from the fact that fexp(A, x)j0is a normalized vector and Type III column in A3
(Definition D.1).
We can show
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, cp
logn·1n⟩
=⟨fexp(A, x)j0,1n⟩ ·cp
logn
=cp
logn
where the first step follows from Type III column in A3(Definition D.1), the second step
follows from simple algebra and the last step follows from the fact that fexp(A, x)j0is a normalized
vector.
D.2.3 Dataset 1 Property when applying function cexpwith random signs
Lemma D.4. If the following conditions hold
•Let{A1, A2, A3}be from dataset D1(see Definition D.1)
•Lett√
d=o(n0.99)(since t(d−2) = n, then we know√
d=ω(n0.01), this implies d=ω(n0.02))
.
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[⟨cexp(A, x)j0, σ⟩ ≥(c+ 0.1)√logn]≥1/10
20

--- PAGE 22 ---
•Part 2. Ifj0̸=j3, then Pr[⟨cexp(A, x)j0, σ⟩<(c+ 0.1)√logn]≥1−δ/poly( n)
Proof. Proof of Part 1.
Forj0=j3, following Part 1,2,3 of Lemma D.3, there are three cases for cexp(A, x)j0,i0.
Fori0= 1, we have cexp(A, x)j0,i0≥1
2a1√logn
Fori0∈ {2,···, d−1}, there is one i0such that cexp(A, x)j0,i0≥1
2b√logn. For the rest of i0,
we have cexp(A, x)j0,i0≤t
nb√logn.
Fori0=d, we have cexp(A, x)j0,i0=c√logn.
LetS={1, i′
0, d}denote a set of three indices, and i′
0denote that special index.
By hoeffding inequality (Lemma 3.1), we know that
|⟨cexp(A, x)j0,[d]\S, σ⟩| ≤O(p
log(n/δ))·t√
d−3
n·bp
logn
≤0.1p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99) and poly(log n)≤
n0.01.
It is obvious that, with probability at least 1 /8, we have
⟨cexp(A, x)j0,S, σ⟩ ≥1
2a1p
logn+1
2bp
logn+cp
logn≥(c+ 0.2)·p
logn
Since the probability that σi0= 1 for all three cases is1
8.
Hence, combining above two events, we have
Pr[⟨cexp(A, x)j0, σ⟩ ≥(c+ 0.1)p
logn]≥1/10
Proof of Part 2. It follows from Part 4,5,6 of Lemma D.3 and Hoeffding inequality (Lemma 3.1).
LetS={1, d}.
By hoeffding inequality (Lemma 3.1), we know that
|⟨cexp(A, x)j0,[d]\S, σ⟩| ≤O(p
log(n/δ))·t√
d
n·bp
logn
≤0.05p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99) and poly(log n)≤
n0.01.
It is obvious that
|cexp(A, x)j0,S, σ⟩| ≤cp
logn+ 0.05p
logn
Thus,
Pr[⟨cexp(A, x)j0, σ⟩ ≤(c+ 0.1)p
logn]≥1−δ/poly( n)
Now, we complete the proof.
21

--- PAGE 23 ---
D.2.4 Dataset 1 Property when applying function Fexp
Theorem D.5. If the following conditions hold
•Letd∈[ω(n0.02), n]
•Letτ= (c+ 0.1)√logn
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD1(Definition D.1)
•Letx=1d2
•LetFexp(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨cexp(A, x), yj1⟩))
Then we have
•With high probability 1−δ/poly( n),Fexp(A1, A2, A3)>0
Proof. It follows from using Lemma D.4.
D.3 Dataset 0 with Fexp
In Section D.3.1 we analyse the property of dataset 0 with respect to function uexpandfexp. In
Section D.3.2 we analyse the property of dataset 0 with respect to function cexp. In Section D.3.3 we
analyse the property of dataset 0 with respect to function cexpwith random signs. In Section D.3.4
we show the property of dataset 0 with respect to the output of Fexp.
D.3.1 Dataset 0 Property when applying function uexpand fexp
Lemma D.6. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition D.1)
•LetQK⊤=1d×d
Then, for uexp(A, x)j0,j1andfexp(A, x)j0,j1entry we have
•Part 1. Forj0=j3,
– Part 1a. Forj1=j3, then uexp(A, x)j0,j1=n(1+a0)2.
– Part 1b. Forj1̸=j3, then uexp(A, x)j0,j1=n(1+a0).
•Part 2. Forj0̸=j3,
– Part 2a. Forj1=j3, then uexp(A, x)j0,j1=n1+a0.
– Part 2b. Forj1̸=j3, then uexp(A, x)j0,j1=n.
•Part 3. Forj0=j3,
– Part 3a. Forj1=j3, then fexp(A, x)j0,j1≤1/n1−2a0. (ifa0∈(0,0.1))
– Part 3b. Forj1̸=j3, then fexp(A, x)j0,j1≤1/n.
•Part 4. Forj0̸=j3,
22

--- PAGE 24 ---
– Part 4a. Forj1=j3, then fexp(A, x)j0,j1≤1/n1−a0.
– Part 4b. Forj0̸=j3, then fexp(A, x)j0,j1≤1/n.
Proof. Proof of Part 1.
Proof of Part 1a. Forj0=j3andj1=j3, by computing the tensor of two 3-sparse vector
uexp(A, x)j0,j1= exp(( a0p
logn+bp
log + cp
logn)2)
= exp((1 + a0)2logn)
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from simple algebra.
Proof of Part 1b. Forj0=j3andj1̸=j3.
In this case we tensor a 2-sparse vector with another 3-sparse vector, we get
uexp(A, x)j0,j1= exp(( bp
logn+cp
logn)·(a0p
logn+bp
logn+cp
logn))
= exp((( b+c)2+a0(b+c)) log n)
=n1+a0
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from simple algebra.
Proof of Part 2.
Proof of Part 2a.
Forj0̸=j3andj1=j3, this case is same as Part 1b.
Proof of Part 2b.
Forj0̸=j3andj1̸=j3,
For the case that we tensor a 2-sparse vector with another 2-sparse vector, we have
uexp(A, x)j0,j1= exp(( bp
logn+cp
logn)2)
= exp(( b+c)2logn)
=n
Proof of Part 3.
Proof of Part 3a. We can show
fexp(A, x)j0,j1=⟨uexp(A, x)j0,1n⟩−1·uexp(A, x)j0,j1
=1
n(a0+1)2+ (n−1)n1+a0·n(1+a0)2
≤n1+2a0+a2
0
n2+a0
=1
n1−a0−a2
0
≤1
n1−2a0
where the first step follows from the definition of fexp, the second step follows from Part 1 and
Part 2 , the third step follows from ( a0+ 1)2≥(a0+ 1), and the last step follows from a2
0≤a0.
Proof of Part 3b.
fexp(A, x)j0,j1=⟨uexp(A, x)j0,1n⟩−1·uexp(A, x)j0,j1
23

--- PAGE 25 ---
=1
n(a0+1)2+ (n−1)n1+a0·n(1+a0)
≤1/n
Proof of Part 4.
Proof of Part 4a.
We can show
fexp(A, x)j0,j1=n(1+a0)
n1+a0+ (n−1)·n
≤1
n1−a0
where the first step follows from Part 1 andPart 2 and the last step follows from simple algebra.
Proof of Part 4b.
We can show
fexp(A, x)j0,j1=n
n1+a0+ (n−1)·n
≤1
n
where the first step follows from Part 1 andPart 2 and the last step follows from simple algebra.
D.3.2 Dataset 0 Property when applying function cexp
Lemma D.7. If the following conditions hold
•Let{A1, A2, A3}from dataset D0(see Definition D.1)
•LetQK⊤=1d×d
•LetV=Id
•Letn=t(d−2)
Then for cexp(A, x)j0,i0entry we have
•Part 1. Forj0=j3andi0= 1, we have cexp(A, x)j0,i0≤1
n1−2a0a0√logn
•Part 2. Forj0=j3andi0∈ {2,···, d−1}, we have cexp(A, x)j0,i0≤t
n1−2a0b√logn
–there is one index i0such that ≤(1
n1−2a0+t−1
n)b√logn
–the other indices i0are≤t
nb√logn
•Part 3. Forj0=j3andi0=d, we have cexp(A, x)j0,i0=c√logn
•Part 4. Forj0̸=j3andi0= 1, we have cexp(A, x)j0,i0≤1
n1−a0a0√logn
•Part 5. Forj0̸=j3andi0∈ {2,···, d−1}, we have cexp(A, x)j0,i0≤t
n1−a0b√logn
–there is one index i0such that ≤(1
n1−a0+t−1
n)b√logn
–the other indices i0are≤t
nb√logn
24

--- PAGE 26 ---
•Part 6. Forj0̸=j3andi0=d, we have cexp(A, x)j0,i0=c√logn
Proof. Proof of Part 1.
It follows from Part 3 of Lemma D.6, and Type I column in A3.
Proof of Part 2.
It follows from Part 3 of Lemma D.6, and Type II column in A3.
There are two types of entries in fexp(A, x)j0:
•there is one entry at most 1 /n1−2a0
•there are n−1 entries at most 1 /n
For ( A3V)i0∈Rn, the sparsity is t.
Thus there is one index i0
⟨fexp(A, x)j0,(A3V)i0⟩ ≤(1
n1−2a0+t−1
n)bp
logn.
For the rest of indices i0, we have
⟨fexp(A, x)j0,(A3V)i0⟩ ≤t
nbp
logn.
Proof of Part 3.
It follows from Part 3 of Lemma D.6, and Type III column in A3.
Proof of Part 4.
It follows from Part 4 of Lemma D.6, and Type I column in A3.
Proof of Part 5.
It follows from Part 4 of Lemma D.6, and Type II column in A3. The proof is similar to Part
2 of this Lemma.
Proof of Part 6.
It follows from Part 4 of Lemma D.6, and Type III column in A3.
D.3.3 Dataset 0 Property when applying function cexpwith random signs
Lemma D.8. If the following conditions hold
•Let{A1, A2, A3,}be from dataset D0(see Definition D.1)
•Lett√
d=o(n1−2a0−0.01)(since t(d−2) = n, then this implies d=ω(n4a0+0.02))
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[|⟨cexp(A, x)j0, σ⟩|<(c+ 0.1)√logn]≥1−δ/poly( n)
•Part 2. Ifj0̸=j3, then Pr[|⟨cexp(A, x)j0, σ⟩|<(c+ 0.1)√logn]≥1−δ/poly( n)
Proof. Proof of Part 1. It follows from Part 1,2,3 of Lemma D.3, random sign distribution.
Proof of Part 2. It follows from Part 4,5,6 of Lemma D.3 and Hoeffding inequality.
By hoeffding inequality, we know that
|⟨cexp(A, x)j0, σ⟩ −cp
logn| ≤O(p
log(n/δ))·t√
d
n1−2a0bp
logn
≤0.1p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n1−2a0−0.01) and and
poly(log n)≤n0.01.
25

--- PAGE 27 ---
D.3.4 Dataset 0 Property when applying function Fexp
Theorem D.9. If the following conditions hold
•Letd∈[ω(n4a0+0.02), n]
•Letτ= (c+ 0.1)√logn
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD1(Definition D.1)
•LetFexp(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨cexp(A, x)j0, yj1))
Then we have
•With high probability 1−δ/poly( n),Fexp(A1, A2, A3) = 0 .
Proof. It follows from using Lemma D.8.
D.4 Dataset 1 with Flin
In Section D.4.1 we analyse the property of dataset 1 with respect to function ulinandflin. In
Section D.4.2 we analyse the property of dataset 1 with respect to function clin. In Section D.4.3 we
analyse the property of dataset 1 with respect to function clinwith random signs. In Section D.4.4
we show the property of dataset 1 with respect to the output of Flin.
D.4.1 Dataset 1 Property when applying function ulinand flin
Lemma D.10. If the following conditions hold
•Letb+c= 1
•Let{A1, A2, A3}from dataset D1(see Definition D.1)
•LetQK⊤=1d×d
Then, for ulin(A, x)j0,j1andflin(A, x)j0,j1entry we have
•Part 1. Forj0=j3,
– Part 1a. Forj1=j3, then ulin(A, x)j0,j1= (1 + a1)2logn.
– Part 1b. Forj1̸=j3, then ulin(A, x)j0,j1= (1 + a1) logn.
•Part 2. Forj0̸=j3
– Part 2a. Forj1=j3, then ulin(A, x)j0,j1= (1 + a1) logn
– Part 2b. Forj1̸=j3, then ulin(A, x)j0,j1= log n.
•Part 3. Forj0=j3,
– Part 3a. Forj1=j3, then flin(A, x)j0,j1≤1+a1
n
– Part 3b. Forj1̸=j3, then flin(A, x)j0,j1≤1
n
•Part 4. Forj0̸=j3,
26

--- PAGE 28 ---
– Part 4a. Forj1=j3, then flin(A, x)j0,j1≤1+a1
n
– Part 4b. Forj0̸=j3, then flin(A, x)j0,j1≤1
n
Proof. Proof of Part 1.
Proof of Part 1a.
Forj0=j3andj1=j3, by computing the circ product of two 3-sparse vector
ulin(A, x)j0,j1= (a1p
logn+bp
logn+cp
logn)2
= (a1+b+c)2logn
= (a1+ 1)2logn
where the first step follows from Definition D.1, the second step follows from simpel algebra and
the last step follows from c+b= 1.
Proof of Part 1b.
Forj0=j3andj1̸=j3.
ulin(A, x)j0,j1= (a1p
logn+bp
logn+cp
logn)(bp
logn+cp
logn)
= ((b+c)2+a1(b+c)) log n
= (1 + a1) logn
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from c+b= 1.
Proof of Part 2.
Proof of Part 2a.
Forj0̸=j3andj1=j3, this case is same as Part 1b.
Proof of Part 2b. Forj0̸=j3andj1̸=j3,
For the case that we tensor a 2-sparse vector with another 2-sparse vector, we have
ulin(A, x)j0,j1= (bp
logn+cp
logn)2
= (b+c)2logn
= log n
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from b+c= 1.
Proof of Part 3.
Proof of Part 3a.
We can show
flin(A, x)j0,j1=⟨ulin(A, x)j0,1n⟩−1·ulin(A, x)j0,j1
=1
(a1+ 1)2logn+ (n−1)(1 + a1) logn·(a1+ 1)2logn
≤1
(a1+ 1) log n+ (n−1)(1 + a1) logn·(a1+ 1)2logn
=a1+ 1
n
where the first step follows from the definition of flin, the second step follows from Part 1 , the
third step follows from simple algebra and the last step follows from simple algebra.
27

--- PAGE 29 ---
Proof of Part 3b.
flin(A, x)j0,j1=⟨ulin(A, x)j0,1n⟩−1·ulin(A, x)j0,j1
=1
(a1+ 1)2logn+ (n−1)(1 + a1) logn·(1 +a1) logn
=1
(a1+ 1) + ( n−1)
≤1/n
where the first step follows from the definition of flin, the second step follows from Part 1 , the
third step follows from simple algebra and the last step follows from simple algebra. Proof of
Part 4.
Proof of Part 4a. We can show
flin(A, x)j0,j1≤(1 +a1) logn
(1 +a1) logn+ (n−1)·logn
≤1 +a1
n
Proof of Part 4b.
We can show
flin(A, x)j0,j1≤logn
(1 +a1) logn+ (n−1)·logn
≤1
n
D.4.2 Dataset 1 Property when applying function clin
Lemma D.11. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition D.1)
•LetQK⊤=1d×d
•LetV=Id
•Letn=t(d−2)
Then for clin(A, x)j0,i0entry we have
•Part 1. Forj0=j3andi0= 1, we have clin(A, x)j0,i0≤(1+a1)
n·a1√logn
•Part 2. Forj0=j3andi0∈ {2,···, d−1}, we have clin(A, x)j0,i0≤(1+a1)
n·t·b√logn
•Part 3. Forj0=j3andi0=d, we have clin(A, x)j0,i0=c√logn
•Part 4. Forj0̸=j3andi0= 1, we have clin(A, x)j0,i0≤(1+a1)
n·a1√logn
•Part 5. Forj0̸=j3andi0∈ {2,···, d−1}, we have clin(A, x)j0,i0≤(1+a1)
n·t·b√logn
28

--- PAGE 30 ---
•Part 6. Forj0̸=j3andi0=d, we have clin(A, x)j0,i0=c√logn
Proof. Proof of Part 1.
It follows from Part 3 of Lemma D.10, and Type I column in A3.
Proof of Part 2.
It follows from Part 3 of Lemma D.10, and Type II column in A3.
We know that each entry in flin(A, x)j0is at least 0 and is at most1+a1
n.
We know that each entry in ( A3V)i0∈Rnis at least 0 and at most b√lognand it is t-sparse.
Thus,
⟨flin(A, x)j0,(A3V)i0⟩ ≤(1 +a1)t
nbp
logn
Proof of Part 3.
It follows from Part 3 of Lemma D.10, and Type III column in A3.
Proof of Part 4.
It follows from Part 4 of Lemma D.10, and Type I column in A3.
Proof of Part 5.
It follows from Part 4 of Lemma D.10, and Type II column in A3.
Proof of Part 6.
It follows from Part 4 of Lemma D.10, and Type III column in A3.
D.4.3 Dataset 1 Property when applying function clinwith random signs
Lemma D.12. If the following conditions hold
•Let{A1, A2, A3,}be from dataset D1(see Definition D.1)
•Lett√
d=o(n0.99)(since t(d−2) = n, then this implies d=ω(n0.02))
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<(c+ 0.1)√logn]≥1−δ/poly( n)
•Part 2. Ifj0̸=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<(c+ 0.1)√logn]≥1−δ/poly( n)
Proof. Proof of Part 1. It follows from Part 1,2,3 of Lemma D.11 and Hoeffding inequality
(Lemma 3.1).
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩ −cp
logn| ≤O(p
log(n/δ))·(1 +a1)t√
d
nbp
logn
≤0.1p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99),a1=O(1),
b=O(1), and poly(log n)≤n0.01.
Proof of Part 2. It follows from Part 4,5,6 of Lemma D.11 and Hoeffding inequality (Lemma 3.1).
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩ −cp
logn| ≤O(p
log(n/δ))·(1 +a1)t√
d
nbp
logn
≤0.1p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99),a1=O(1),
b=O(1), and poly(log n)≤n0.01.
29

--- PAGE 31 ---
D.4.4 Dataset 1 Property when applying function Flin
Theorem D.13. If the following conditions hold
•Letd∈[ω(n0.02), n]
•Letτ= (c+ 0.1)√logn
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD1(Definition D.1)
•LetFlin(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨clin(A, x)j0, yj1))
Then we have
•With high probability 1−δ/poly( n),Flin(A1, A2, A3) = 0
Proof. It follows from using Lemma D.4.
D.5 Dataset 0 with Flin
In Section D.5.1 we analyse the property of dataset 0 with respect to function ulinandflin. In
Section D.5.2 we analyse the property of dataset 0 with respect to function clin. In Section D.5.3 we
analyse the property of dataset 0 with respect to function clinwith random signs. In Section D.5.4
we show the property of dataset 0 with respect to the output of Flin.
D.5.1 Dataset 0 Property when applying function ulinand flin
Lemma D.14. If the following conditions hold
•Let{A1, A2, A3}from dataset D0(see Definition D.1)
•LetQK⊤=Id
Then, for ulin(A, x)j0,j1andflin(A, x)j0,j1entry we have
•Part 1. Forj0=j3,
– Part 1a. Forj1=j3, then ulin(A, x)j0,j1= (1 + a0)2logn.
– Part 1b. Forj1̸=j3, then ulin(A, x)j0,j1= (1 + a0) logn.
•Part 2. Forj0̸=j3
– Part 2a. Forj1=j3, then ulin(A, x)j0,j1= (1 + a0) logn
– Part 2b. Forj1̸=j3, then ulin(A, x)j0,j1= log n.
•Part 3. Forj0=j3,
– Part 3a. Forj1=j3, then flin(A, x)j0,j1≤1+a0
n
– Part 3b. Forj1̸=j3, then flin(A, x)j0,j1≤1
n
•Part 4. Forj0̸=j3,
– Part 4a. Forj1=j3, then flin(A, x)j0,j1≤1+a0
n
30

--- PAGE 32 ---
– Part 4b. Forj0̸=j3, then flin(A, x)j0,j1≤1
n
Proof. Proof of Part 1.
Proof of Part 1a.
Forj0=j3andj1=j3, by computing the circ product of two 3-sparse vector
ulin(A, x)j0,j1= (a0p
logn+bp
logn+cp
logn)2
= (a0+b+c)2logn
= (a0+ 1)2logn
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from c+b= 1.
Proof of Part 1b.
Forj0=j3andj1̸=j3.
ulin(A, x)j0,j1= (a0p
logn+bp
logn+cp
logn)(bp
logn+cp
logn)
= ((b+c)2+a0(b+c)) log n
= (1 + a0) logn
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from c+b= 1.
Proof of Part 2.
Proof of Part 2a.
Forj0̸=j3andj1=j3, this case is same as Part 1b.
Proof of Part 2b. Forj0̸=j3andj1̸=j3,
For the case that we tensor a 2-sparse vector with another 2-sparse vector, we have
ulin(A, x)j0,j1= (bp
logn+cp
logn)2
= (b+c)2logn
= log n
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from b+c= 1.
Proof of Part 3.
Proof of Part 3a.
We can show
flin(A, x)j0,j1=⟨ulin(A, x)j0,1n⟩−1·ulin(A, x)j0,j1
=1
(a0+ 1)2logn+ (n−1)(1 + a0) logn·(a0+ 1)2logn
≤1
(a0+ 1) log n+ (n−1)(1 + a0) logn·(a0+ 1)2logn
=a0+ 1
n
where the first step follows from the definition of flin, the second step follows from Part 1 , the
third step follows from simple algebra and the last step follows from simple algebra.
31

--- PAGE 33 ---
Proof of Part 3b.
flin(A, x)j0,j1=⟨ulin(A, x)j0,1n⟩−1·ulin(A, x)j0,j1
=1
(a0+ 1)2logn+ (n−1)(1 + a0) logn·(1 +a0) logn
=1
(a0+ 1) + ( n−1)
≤1/n
where the first step follows from the definition of flin, the second step follows from Part 1 , the
third step follows from simple algebra and the last step follows from simple algebra. Proof of
Part 4.
Proof of Part 4a. We can show
flin(A, x)j0,j1≤(1 +a0) logn
(1 +a0) logn+ (n−1)·logn
≤1 +a0
n
Proof of Part 4b.
We can show
flin(A, x)j0,j1≤logn
(1 +a0) logn+ (n−1)·logn
≤1
n
D.5.2 Dataset 0 Property when applying function clin
Lemma D.15. If the following conditions hold
•Let{A1, A2, A3}from dataset D0(see Definition D.1)
•LetQK⊤=1d×d
•LetV=Id
•Letn=t(d−2)
Then for clin(A, x)j0,i0entry we have
•Part 1. Forj0=j3andi0= 1, we have clin(A, x)j0,i0≤(1+a0)
n·a0√logn
•Part 2. Forj0=j3andi0∈ {2,···, d−1}, we have clin(A, x)j0,i0≤(1+a0)
n·t·b√logn
•Part 3. Forj0=j3andi0=d, we have clin(A, x)j0,i0=c√logn
•Part 4. Forj0̸=j3andi0= 1, we have clin(A, x)j0,i0≤(1+a0)
n·a0√logn
•Part 5. Forj0̸=j3andi0∈ {2,···, d−1}, we have clin(A, x)j0,i0≤(1+a0)
n·t·b√logn
32

--- PAGE 34 ---
•Part 6. Forj0̸=j3andi0=d, we have clin(A, x)j0,i0=c√logn
Proof. Proof of Part 1.
It follows from Part 3 of Lemma D.14, and Type I column in A3.
Proof of Part 2.
It follows from Part 3 of Lemma D.14, and Type II column in A3.
We know that each entry in flin(A, x)j0is at least 0 and is at most1+a0
n.
We know that each entry in ( A3V)i0∈Rnis at least 0 and at most b√lognand it is t-sparse.
Thus,
⟨fexp(A, x)j0,(A3V)i0⟩ ≤(1 +a0)t
nbp
logn
Proof of Part 3.
It follows from Part 3 of Lemma D.14, and Type III column in A3.
Proof of Part 4.
It follows from Part 4 of Lemma D.14, and Type I column in A3.
Proof of Part 5.
It follows from Part 4 of Lemma D.14, and Type II column in A3.
Proof of Part 6.
It follows from Part 4 of Lemma D.14, and Type III column in A3.
D.5.3 Dataset 0 Property when applying function clinwith random signs
Lemma D.16. If the following conditions hold
•Let{A1, A2, A3,}be from dataset D0(see Definition D.1)
•Lett√
d=o(n0.99)(since t(d−2) = n, then this implies d=ω(n0.02))
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<(c+ 0.1)√logn]≥1−δ/poly( n)
•Part 2. Ifj0̸=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<(c+ 0.1)√logn]≥1−δ/poly( n)
Proof. Proof of Part 1. It follows from Part 1,2,3 of Lemma D.15 and Hoeffding inequality
(Lemma 3.1).
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩ −cp
logn| ≤O(p
log(n/δ))·(1 +a0)t√
d
nbp
logn
≤0.1p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99),a0=O(1),
b=O(1), and poly(log n)≤n0.01.
Proof of Part 2. It follows from Part 4,5,6 of Lemma D.15 and Hoeffding inequality (Lemma 3.1).
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩ −cp
logn| ≤O(p
log(n/δ))·(1 +a0)t√
d
nbp
logn
≤0.1p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99),a0=O(1),
b=O(1), and poly(log n)≤n0.01.
33

--- PAGE 35 ---
D.5.4 Dataset 0 Property when applying function Fexp
Theorem D.17. If the following conditions hold
•Letd∈[ω(n0.02), n]
•Letτ= (c+ 0.1)√logn
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD0(Definition D.1)
•LetFexp(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨cexp(A, x)j0, yj1))
Then we have
•With high probability 1−δ/poly( n),Flin(A1, A2, A3) = 0
Proof. It follows from using Lemma D.16.
E Self-Attention Dataset, More discussion
In this section instead of choosing QK⊤=1d×d, we choose QK⊤=Id. The proofs are similar, we
only provide the proofs for Fexpoutputs >0 forD1. We omitted the proofs for Fexpoutputs 0 for
D0andFlinoutputs 0 for D1andD0. In Section E.1 we analyse the dataset 1 with the function
Fexp.
E.1 Dataset 1 with Fexp
In Section E.1.1 we analyse the property of dataset 1 with respect to function uexpandfexp. In
Section E.1.2 we analyse the property of dataset 1 with respect to function cexp. In Section E.1.3 we
analyse the property of dataset 1 with respect to function cexpwith random signs. In Section E.1.4
we show the property of dataset 1 with respect to the output of Fexp.
E.1.1 Dataset 1 Property when applying function uexpand fexp
Lemma E.1. If the following conditions hold
•Leta1≥1
•Letb2+c2= 1
•Let{A1, A2, A3}from dataset D1(see Definition D.1)
•LetQK⊤=Id(This is the major difference compared to Lemma D.2)
Then, for uexp(A, x)j0,j1andfexp(A, x)j0,j1entry we have
•Part 1. Forj0=j3,
– Part 1a. Forj1=j3, then uexp(A, x)j0,j1=n(1+a1)2.
– Part 1b. Forj1̸=j3, then uexp(A, x)j0,j1∈[nc2, nb2+c2].
∗There t−1indices equal nb2+c2
34

--- PAGE 36 ---
∗There are n−tindices equal to nc2
•Part 2. Forj0̸=j3, for all j1∈[n], then uexp(A, x)j0,j1∈[nc2, nb2+c2]
–There t−1indices equal nb2+c2
–There are n−tindices equal to nc2
•Part 3. Forj0=j3,
– Part 3a. Forj1=j3, then fexp(A, x)j0,j1≥1/2(ifa1>1)
– Part 3b. Forj1̸=j3, then fexp(A, x)j0,j1≤1/n1−b2
•Part 4. Forj0̸=j3, for all j1∈[n],uexp(A, x)j0,j1has the following properties
–There t−1indices ≤1/n1−b2
–There are n−tindices ≤1/n
Proof. Proof of Part 1.
Proof of Part 1a. Forj0=j3andj1=j3, by computing the circ product of two 3-sparse
vector
uexp(A, x)j0,j1= exp(( a1p
logn)2+ (bp
logn)2+ (cp
logn)2)
= exp(( a2
1+b2+c2) logn)
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from simple algebra.
Proof of Part 1b. Forj0=j3andj1̸=j3. There are ( n−1) indices j1in this case.
There are tindices, we have
uexp(A, x)j0,j1= exp(( bp
logn+cp
logn)(bp
logn+cp
logn)2)
= exp(( b2+c2) logn)
=nb2+c2
where the first step follows from Definition D.1, the second step follows from simple algebra and
the last step follows from simple algebra.
There are n−t−1 indices, we have
uexp(A, x)j0,j1= exp(( cp
logn)2)
=nc2
Proof of Part 2.
Proof is similar to Part 1b.
Proof of Part 3.
Proof of Part 3a. We can show
fexp(A, x)j0,j1=⟨uexp(A, x)j0,1n⟩−1·uexp(A, x)j0,j1
=1
n(a2
1+b2+c2)+ (n−1)nb2+c2·n(a2
1+b2+c2)
≥1
2
35

--- PAGE 37 ---
where the first step follows from the definition of fexp, the second step follows from Part 1 and
Part 2 and the last step follows from a1>1.
Proof of Part 3b.
fexp(A, x)j0,j1=⟨uexp(A, x)j0,1n⟩−1·uexp(A, x)j0,j1
≤1
n(a2
1+b2+c2)+ (n−1)nc2·n(b2+c2)
≤1
nc2+ (n−1)nc2·n(b2+c2)
≤1/n1−b2
Proof of Part 4.
Proof of Part 4a.
There are tindices we have:
fexp(A, x)j0,j1≤nb2+c2
t·nb2+c2+ (n−t)·nc2
≤nb2+c2
t·nc2+ (n−t)·nc2
≤1
n1−b2
where the first step follows from Part 1 andPart 2 and the last step follows from simple algebra.
Proof of Part 4b. There are n−tindices we have:
fexp(A, x)j0,j1≤nc2
t·nb2+c2+ (n−t)·nc2
≤nc2
t·nc2+ (n−t)·nc2
≤1
n
where the first step follows from Part 1 andPart 2 and the last step follows from simple algebra.
E.1.2 Dataset 1 Property when applying function cexp
Lemma E.2. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition D.1)
•LetQK⊤=1d×d
•LetV=Id
•Letn=t(d−2)
Then for cexp(A, x)j0,i0entry we have
•Part 1. Forj0=j3andi0= 1, we have cexp(A, x)j0,i0≥1
2·a1√logn
36

--- PAGE 38 ---
•Part 2. Forj0=j3andi0∈ {2,···, d−1}
–There is only one i0, we have cexp(A, x)j0,i0≥1
2·b√logn
–For the rest of i0, we have cexp(A, x)j0,i0≤t
n1−b2·b√logn
•Part 3. Forj0=j3andi0=d, we have cexp(A, x)j0,i0=c√logn
•Part 4. Forj0̸=j3andi0= 1, we have cexp(A, x)j0,i0≤1
n1−b2·a1√logn
•Part 5. Forj0̸=j3andi0∈ {2,···, d−1}, we have cexp(A, x)j0,i0≤t
n1−b2·b√logn
•Part 6. Forj0̸=j3andi0=d, we have cexp(A, x)j0,i0=c√logn
Proof. Proof of Part 1.
It follows from Part 3 of Lemma E.1 and Type I column in A3(Definition D.1).
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, ej3·a1p
logn⟩
≥1
2a1p
logn
where the first step follows from Type I column in A3(Definition D.1) and the last step follows
from Part 3 of Lemma E.1.
Proof of Part 2.
It follows from Part 3 of Lemma E.1 and Type II column in A3(Definition D.1).
There are two cases we need to consider, there is one ( A3V)i0’sj3coordinate is 1, in this
situation, we know
⟨fexp(A, x)j0,(A3V)i0⟩ ≥1
2·bp
logn
For the other case, ( A3V)i0’sj3coordinate is 0, in this case we use the property that ( A3V)i0
ist-sparse vector. Then, we have
⟨fexp(A, x)j0,(A3V)i0⟩ ≤t·1
n1−b2·bp
logn
≤t
n1−b2·bp
logn
Proof of Part 3.
It follows from the fact that fexp(A, x)j0is a normalized vector and Type III column in A3
(Definition D.1).
We can show
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, cp
logn·1n⟩
=⟨fexp(A, x)j0,1n⟩ ·cp
logn
=cp
logn
Proof of Part 4.
It follows from Part 4 of Lemma E.1 and Type I column in A3(Definition D.1).
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, ej3·a1p
logn⟩
37

--- PAGE 39 ---
≤1
n1−b2·a1p
logn
where the first step follows from Type I column in A3and the last step follows from, for j0̸=j3,
then fexp(A, x)j0,j1≤1/n1−a1(Part 5 of Lemma E.1).
Proof of Part 5.
It follows from Part 4 of Lemma E.1 and Type II column in A3(Definition D.1).
⟨fexp(A, x)j0,(A3V)i0⟩=tbp
logn· ·1
n
≤t
n1−b2bp
logn
where the first step follows from Type II column in A3and the last step follows from, for j0̸=j3,
then fexp(A, x)j0,j1≤1/n1−b2(Part 4 of Lemma E.1).
Proof of Part 6.
It follows from the fact that fexp(A, x)j0is a normalized vector and Type III column in A3
(Definition D.1).
We can show
⟨fexp(A, x)j0,(A3V)i0⟩=⟨fexp(A, x)j0, cp
logn·1n⟩
=⟨fexp(A, x)j0,1n⟩ ·cp
logn
=cp
logn
where the first step follows from Type III column in A3(Definition D.1), the second step
follows from simple algebra and the last step follows from the fact that fexp(A, x)j0is a normalized
vector.
E.1.3 Dataset 1 Property when applying function cexpwith random signs
Lemma E.3. If the following conditions hold
•Let{A1, A2, A3}be from dataset D1(see Definition D.1)
•Lett√
d=o(n1−b2−0.01)(since t(d−2) = n, then we know√
d=ω(nb2+0.01), this implies
d=ω(n2b2+0.02)) .
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[⟨cexp(A, x)j0, σ⟩ ≥(c+ 0.1)√logn]≥1/10
•Part 2. Ifj0̸=j3, then Pr[⟨cexp(A, x)j0, σ⟩<(c+ 0.1)√logn]≥1−δ/poly( n)
Proof. Proof of Part 1.
Forj0=j3, following Part 1,2,3 of Lemma E.2, there are three cases for cexp(A, x)j0,i0.
Fori0= 1, we have cexp(A, x)j0,i0≥1
2a1√logn
Fori0∈ {2,···, d−1}, there is one i0such that cexp(A, x)j0,i0≥1
2b√logn. For the rest of i0,
we have cexp(A, x)j0,i0≤t
n1−b2b√logn.
Fori0=d, we have cexp(A, x)j0,i0=c√logn.
LetS={1, i′
0, d}denote a set of three indices, and i′
0denote that special index.
38

--- PAGE 40 ---
By hoeffding inequality (Lemma 3.1), we know that
|⟨cexp(A, x)j0,[d]\S, σ⟩| ≤O(p
log(n/δ))·t√
d−3
n1−b2·bp
logn
≤0.1p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n1−b2−0.01) and
poly(log n)≤n0.01.
It is obvious that, with probability at least 1 /8, we have
⟨cexp(A, x)j0,S, σ⟩ ≥1
2a1p
logn+1
2bp
logn+cp
logn≥(c+ 0.2)·p
logn
Since the probability that σi0= 1 for all three cases is1
8.
Hence, combining above two events, we have
Pr[⟨cexp(A, x)j0, σ⟩ ≥(c+ 0.1)p
logn]≥1/8−δ/poly( n)≥1/10
Proof of Part 2. It follows from Part 4,5,6 of Lemma E.2 and Hoeffding inequality (Lemma 3.1).
LetS={1, d}.
By hoeffding inequality (Lemma 3.1), we know that
|⟨cexp(A, x)j0,[d]\S, σ⟩| ≤O(p
log(n/δ))·t√
d
n1−b2·bp
logn
≤0.0.5p
logn
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n1−b2−0.01) and
poly(log n)≤n0.01.
It is obvious that
|cexp(A, x)j0,S, σ⟩| ≤cp
logn+ 0.05p
logn
Thus,
Pr[⟨cexp(A, x)j0, σ⟩ ≤(c+ 0.1)p
logn]≥1−δ/poly( n)
Now, we complete the proof.
E.1.4 Dataset 1 Property when applying function Fexp
Theorem E.4. If the following conditions hold
•Leta1≥1
•Letb2+c2= 1
•Letb∈(0,0.2)
•Letd∈[ω(n2b2+0.02), n]
•Letτ= (c+ 0.1)√logn
•Letm=O(log(n/δ))
39

--- PAGE 41 ---
•For any {A1, A2, A3}fromD1(Definition D.1)
•LetQK⊤=Id
•LetFexp(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨cexp(A, x)j0, yj1⟩))
Then we have
•With high probability 1−δ/poly( n),Fexp(A1, A2, A3)>0
Proof. It follows from using Lemma E.3.
F Cross-Attention Dataset
In Section F.1, we give the definition of the dataset used in the following parts for the cross-
attention.. In Section F.2, we show that the output of the Fexpis greater than 0 with high
probability. In Section F.3, we show that the output of Fexpis equal to 0 with high probability. In
Section F.4, we show that the output of the Flinis equal to 0 with high probability. In Section F.5,
we show that the output of Flinis equal to 0 with high probability.
F.1 Definition of Dataset
Definition F.1. Assume the follow parameters,
•Leta0∈(0,0.1)
•Leta1>1
Given two sets of datasets D0andD1.
•For each {A1, A2, A3} ∈ D 0, we have
–A1̸=A2=A3∈Rn×d
–Assume n= (d−1)twhere tis a positive integer
–There are two special index j2∈[n]andj3∈[n]
–The first column of A1is,j2-th location is a0lognand rest locations are 0
–For the second column to d−1columns in A1, all the entries are logn
–the first column of A2isej3(this is a one-sparse vector where only j3-th location is 1)
–the second column to d−1columns in A2are
Id−1
Id−1
...
Id−1

•For each {A1, A2, A3} ∈ D 1, we have
–A1̸=A2=A3∈Rn×d
–Assume n= (d−1)twhere tis a positive integer
–There are two special index j2∈[n]andj3∈[n]
–The first column of A1is,j2-th location is a1·lognand rest locations are 0
40

--- PAGE 42 ---
–For the second column to d−1columns in A1, all the entries are logn
–the first column of A2isej3∈Rn(this is a one-sparse vector where only j3-th location
is1)
–the second column to d−1columns in A2are
Id−1
Id−1
...
Id−1

F.2 Dataset 1 with Fexp
In Section F.2.1 we analyse the property of dataset 1 with respect to function uexpandfexp. In
Section F.2.2 we analyse the property of dataset 1 with respect to function cexp. In Section F.2.3 we
analyse the property of dataset 1 with respect to function cexpwith random signs. In Section F.2.4
we show the property of dataset 1 with respect to the output of Fexp.
F.2.1 Dataset 1 Property when applying function uexpand fexp
Lemma F.2. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition F.1)
•LetQK⊤=Id
Then, for uexp(A, x)j0,j1andfexp(A, x)j0,j1entry we have
•Part 1. Forj0=j2,
– Part 1a. Forj1=j3, then uexp(A, x)j0,j1=n1+a1.
– Part 1b. Forj1̸=j3, then uexp(A, x)j0,j1=n,
•Part 2. Forj0̸=j2
– Part 2a. Forj1=j3, then uexp(A, x)j0,j1=n.
– Part 2b. Forj1̸=j3, then uexp(A, x)j0,j1=n.
•Part 3. Forj0=j2,
– Part 3a. Forj1=j3, then fexp(A, x)j0,j1≥1/2(ifa1>1)
– Part 3b. Forj1̸=j3, then fexp(A, x)j0,j1≤1/n
•Part 4. Forj0=j2,
– Part 4a. Forj0=j2, then fexp(A, x)j0,j1= 1/n
– Part 4b. Forj0̸=j2, then fexp(A, x)j0,j1= 1/n
Proof. Proof of Part 1. Proof of Part 1a.
By computing the inner product we know
uexp(A, x)j0,j1= exp( a1logn+ log n)
= exp((1 + a1) logn)
41

--- PAGE 43 ---
=n1+a1
Proof of Part 1b.
We have
uexp(A, x)j0,j1= exp(log n)
=n
Proof of Part 2. Proofs are same as Part 1b.
Proof of Part 3.
Proof of Part 3a.
We can show
fexp(A, x)j0,j1≥n1+a1
n1+a1+ (n−1)n
≥1
2
Proof of Part 3b.
We can show
fexp(A, x)j0,j1≤n
n1+a1+ (n−1)·n
≤n
n2
≤1
n
Proof of Part 4.
We can show
fexp(A, x)j0,j1=n
n·n
=1
n
F.2.2 Dataset 1 Property when applying function cexp
Lemma F.3. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition F.1)
•LetQK⊤=Id
•LetV=Id
•Letn=t(d−1)
Then for cexp(A, x)j0,i0entry we have
•Part 1. Forj0=j2andi0= 1, we have cexp(A, x)j0,i0≥1
2
•Part 2. Forj0=j2andi0∈ {2,···, d}
42

--- PAGE 44 ---
–There is only one i0, we have cexp(A, x)j0,i0≥1
2
–For the rest of i0, we have cexp(A, x)j0,i0≤t
n
•Part 3. Forj0̸=j2andi0= 1, we have cexp(A, x)j0,i0≤1
n
•Part 4. Forj0̸=j2andi0∈ {2,···, d}, we have cexp(A, x)j0,i0≤t
n
Proof. Proof of Part 1.
It follows from Part 3 of Lemma F.2 and Type I column in A3(Definition F.1).
Proof of Part 2.
It follows from Part 3 of Lemma F.2 and Type II column in A3(Definition F.1).
Proof of Part 3.
It follows from Part 4 of Lemma F.2 and Type I column in A3(Definition F.1).
Proof of Part 4.
It follows from Part 4 of Lemma F.2 and Type II column in A3(Definition F.1).
F.2.3 Dataset 1 Property when applying function cexpwith random signs
Lemma F.4. If the following conditions hold
•Let{A1, A2, A3,}be from dataset D1(see Definition F.1)
•Lett√
d=o(n0.99)(since t(d−1) = n, then this implies d=ω(n0.02))
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[⟨cexp(A, x)j0, σ⟩ ≥1−0.1] = 1 /10
•Part 2. Ifj0̸=j3, then Pr[|⟨cexp(A, x)j0, σ⟩|<0.5] = 1 −δ/poly( n)
Proof. Proof of Part 1. It follows from Part 1,2,3 of Lemma F.3, random sign distribution.
There are two large coordinates, both of them multiplying with a positive sign, the chance of
that is probability 1 /4.
Thus,
Pr[⟨cexp(A, x)j0, σ⟩ ≥1
2+1
2−0.1]≥1/4−δ/poly( n)≥1/10
Proof of Part 2. It follows from Part 4,5,6 of Lemma F.3 and Hoeffding inequality (Lemma 3.1).
By hoeffding inequality (Lemma 3.1), we know that
|⟨cexp(A, x)j0, σ⟩| ≤O(p
log(n/δ))·t√
d
n
≤0.1
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99) and poly(log n)≤
n0.01.
43

--- PAGE 45 ---
F.2.4 Dataset 1 Property when applying function Fexp
Theorem F.5. If the following conditions hold
•Letd∈[ω(n0.02), n]
•Letτ= 0.9
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD1(Definition F.1)
•LetFexp(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨cexp(A, x), yj1))
Then we have
•With high probability 1−δ/poly( n),Fexp(A1, A2, A3)>0
Proof. It follows from using Lemma F.4.
F.3 Dataset 0 with Fexp
In Section F.3.1 we analyse the property of dataset 0 with respect to function uexpandfexp. In
Section F.3.2 we analyse the property of dataset 0 with respect to function cexp. In Section F.3.3 we
analyse the property of dataset 0 with respect to function cexpwith random signs. In Section F.3.4
we show the property of dataset 0 with respect to the output of Fexp.
F.3.1 Dataset 0 Property when applying function uexpand fexp
Lemma F.6. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition F.1)
•LetQK⊤=Id
Then, for uexp(A, x)j0,j1andfexp(A, x)j0,j1entry we have
•Part 1. Forj0=j2,
– Part 1a. Forj1=j3, then uexp(A, x)j0,j1=n1+a0.
– Part 1b. Forj1̸=j3, then uexp(A, x)j0,j1=n,
•Part 2. Forj0̸=j2
– Part 2a. Forj1=j3, then uexp(A, x)j0,j1=n.
– Part 2b. Forj1̸=j3, then uexp(A, x)j0,j1=n.
•Part 3. Forj0=j2,
– Part 3a. Forj1=j3, then fexp(A, x)j0,j1≤1/n1−a0(ifa0<1)
– Part 3b. Forj1̸=j3, then fexp(A, x)j0,j1≤1/n
•Part 4.
– Part 4a. Forj0=j2, then fexp(A, x)j0,j1= 1/n
44

--- PAGE 46 ---
– Part 4b. Forj0̸=j2, then fexp(A, x)j0,j1= 1/n
Proof. Proof of Part 1.
Proof of Part 1a.
By computing the inner product we know
uexp(A, x)j0,j1= exp( a0logn+ log n)
= exp((1 + a0) logn)
=n1+a0
Proof of Part 1b.
We can show
uexp(A, x)j0,j1= exp(log n)
=n
Proof of Part 2.
We can show
uexp(A, x)j0,j1= exp(log n)
=n
Proof of Part 3.
Proof of Part 3a. We can show
fexp(A, x)j0,j1≤n1+a0
n1+a0+ (n−1)n
≤n1+a0
n2
≤1
1−a0
Proof of Part 3b.
We can show
fexp(A, x)j0,j1≤n
n1+a0+ (n−1)·n
≤n
n2
=1
n
Proof of Part 4.
We can show
fexp(A, x)j0,j1=n
n·n
= 1/n
45

--- PAGE 47 ---
F.3.2 Dataset 0 Property when applying function cexp
Lemma F.7. If the following conditions hold
•Let{A1, A2, A3}from dataset D0(see Definition F.1)
•LetQK⊤=Id
•LetV=Id
•Letn=t(d−1)
Then for cexp(A, x)j0,i0entry we have
•Part 1. Forj0=j3andi0= 1, we have cexp(A, x)j0,i0≤1
n1−a0
•Part 2. Forj0=j3andi0∈ {2,···, d}, we have cexp(A, x)j0,i0≤t
n1−a0
•Part 4. Forj0̸=j3andi0= 1, we have cexp(A, x)j0,i0≤1
n
•Part 5. Forj0̸=j3andi0∈ {2,···, d}, we have cexp(A, x)j0,i0≤t
n
Proof. Proof of Part 1.
It follows from Part 3 of Lemma F.6, and Type I column in A3.
Proof of Part 2.
It follows from Part 3 of Lemma F.6, and Type II column in A3.
We know that each entry in fexp(A, x)j0is at least 0 and is at most1
n1−a0.
We know that each entry in ( A3V)i0∈Rnis at least 0 and at most 1 and it is t-sparse.
Thus,
⟨fexp(A, x)j0,(A3V)i0⟩ ≤1
n1−a0
Proof of Part 3.
It follows from Part 4 of Lemma F.6, and Type I column in A3.
Proof of Part 4.
It follows from Part 4 of Lemma F.6, and Type II column in A3.
F.3.3 Dataset 0 Property when applying function cexpwith random signs
Lemma F.8. If the following conditions hold
•Let{A1, A2, A3,}be from dataset D0(see Definition F.1)
•Lett√
d=o(n1−a0−0.01)(since t(d−1) = n, then this implies d=ω(n2a0+0.02))
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[|⟨cexp(A, x)j0, σ⟩|<0.5] = 1 −δ/poly( n)
•Part 2. Ifj0̸=j3, then Pr[|⟨cexp(A, x)j0, σ⟩|<0.5] = 1 −δ/poly( n)
46

--- PAGE 48 ---
Proof. Proof of Part 1. It follows from Part 1,2,3 of Lemma F.3, random sign distribution.
Proof of Part 2. It follows from Part 4,5,6 of Lemma F.3 and Hoeffding inequality.
By hoeffding inequality, we know that
|⟨cexp(A, x)j0, σ⟩| ≤O(p
log(n/δ))·t√
d
n1−a0
≤0.5
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n1−a0−0.01) and and
poly(log n)≤n0.01.
F.3.4 Dataset 0 Property when applying function Fexp
Theorem F.9. If the following conditions hold
•Letd∈[ω(n2a0+0.02), n]
•Leta0∈(0,0.1)
•Letτ= 0.9
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD1(Definition F.1)
•LetFexp(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨cexp(A, x), yj1))
Then we have
•With high probability 1−δ/poly( n),Fexp(A1, A2, A3) = 0
Proof. It follows from using Lemma F.4.
F.4 Dataset 1 with Flin
In Section F.4.1 we analyse the property of dataset 1 with respect to function ulinandflin. In
Section F.4.2 we analyse the property of dataset 1 with respect to function clin. In Section F.4.3 we
analyse the property of dataset 1 with respect to function clinwith random signs. In Section F.4.4
we show the property of dataset 1 with respect to the output of Flin.
F.4.1 Dataset 1 Property when applying function ulinand flin
Lemma F.10. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition F.1)
•LetQK⊤=Id
Then, for ulin(A, x)j0,j1andflin(A, x)j0,j1entry we have
•Part 1. Forj0=j2, and j1=j3, then ulin(A, x)j0,j1= (1 + a1) logn.
•Part 2. Forj0̸=j2orj1̸=j3, then ulin(A, x)j0,j1= log n.
•Part 3. Forj0=j2, and j1=j3, then flin(A, x)j0,j1≤(1 +a1)/n.
47

--- PAGE 49 ---
•Part 4. Forj0=j2, and j1̸=j3, then flin(A, x)j0,j1≤1/n.
•Part 5. Forj0̸=j2, then flin(A, x)j0,j1= 1/n.
Proof. Proof of Part 1.
By computing the inner product we know
ulin(A, x)j0,j1=a1logn+ log n
= (1 + a1) logn
where the last step follows from simple algebra.
Proof of Part 2.
We can show
ulin(A, x)j0,j1= log n
Proof of Part 3.
We can show
flin(A, x)j0,j1≤(1 +a1) logn
(1 +a1) logn+ (n−1)·logn
≤1 +a1
n
Proof of Part 4.
We can show
flin(A, x)j0,j1≤logn
(1 +a1) logn+ (n−1)·logn
≤1
n
Proof of Part 5.
We can show
flin(A, x)j0,j1=logn
n·logn
=1
n
F.4.2 Dataset 1 Property when applying function clin
Lemma F.11. If the following conditions hold
•Let{A1, A2, A3}from dataset D0(see Definition F.1)
•LetQK⊤=Id
•LetV=Id
•Letn=t(d−1)
Then for clin(A, x)j0,i0entry we have
48

--- PAGE 50 ---
•Part 1. Forj0=j3andi0= 1, we have clin(A, x)j0,i0≤1+a1
n
•Part 2. Forj0=j3andi0∈ {2,···, d}, we have clin(A, x)j0,i0≤(1+a1)t
n
•Part 3. Forj0̸=j3andi0= 1, we have clin(A, x)j0,i0≤1
n
•Part 4. Forj0̸=j3andi0∈ {2,···, d}, we have clin(A, x)j0,i0≤t
n
Proof. Proof of Part 1.
It follows from Part 3 and Part 4 of Lemma F.10, and Type I column in A3.
Proof of Part 2.
It follows from Part 3 and Part 4 of Lemma F.10, and Type II column in A3.
We know that each entry in flin(A, x)j0is at least 0 and is at most(1+a1)
n.
We know that each entry in ( A3V)i0∈Rnis at least 0 and at most 1 and it is t-sparse.
Thus,
⟨flin(A, x)j0,(A3V)i0⟩ ≤(1 +a1)t
n
Proof of Part 3.
It follows from Part 5 of Lemma F.10, and Type I column in A3.
Proof of Part 4.
It follows from Part 5 of Lemma F.10, and Type II column in A3.
F.4.3 Dataset 1 Property when applying function clinwith random signs
Lemma F.12. If the following conditions hold
•Let{A1, A2, A3,}be from dataset D0(see Definition F.1)
•Lett√
d=o(n0.99)(since t(d−1) = n, then this implies d=ω(n0.02))
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<0.5] = 1 −δ/poly( n)
•Part 2. Ifj0̸=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<0.5] = 1 −δ/poly( n)
Proof. Proof of Part 1. It follows from Part 1,2,3 of Lemma F.11, random sign distribution.
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩| ≤O(p
log(n/δ))·(1 +a1)t√
d
n
≤0.5
with probability at least 1 −δ/poly( n). Here a1=O(1).
Proof of Part 2. It follows from Part 4,5,6 of Lemma F.11 and Hoeffding inequality (Lemma 3.1).
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩| ≤O(p
log(n/δ))·2t√
d
n
≤0.5
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99) and and
poly(log n)≤n0.01.
49

--- PAGE 51 ---
F.4.4 Dataset 1 Property when applying function Flin
Theorem F.13. If the following conditions hold
•Letd∈[ω(n0.02), n]
•Letτ= 0.9
•Leta1=O(1)
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD1(Definition F.1)
•LetFlin(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨clin(A, x), yj1))
Then we have
•With high probability 1−δ/poly( n),Flin(A1, A2, A3) = 0
Proof. It follows from using Lemma F.4.
F.5 Dataset 0 with Flin
In Section F.5.1 we analyse the property of dataset 0 with respect to function ulinandflin. In
Section F.5.2 we analyse the property of dataset 0 with respect to function clin. In Section F.5.3 we
analyse the property of dataset 0 with respect to function clinwith random signs. In Section F.5.4
we show the property of dataset 0 with respect to the output of Flin.
F.5.1 Dataset 0 Property when applying function ulinand flin
Lemma F.14. If the following conditions hold
•Let{A1, A2, A3}from dataset D1(see Definition F.1)
•LetQK⊤=Id
Then, for ulin(A, x)j0,j1andflin(A, x)j0,j1entry we have
•Part 1. Forj0=j3, and j1=j3, then ulin(A, x)j0,j1= (1 + a0) logn.
•Part 2. Forj0̸=j3andj1̸=j3, then ulin(A, x)j0,j1= log n.
•Part 3. Forj0=j3, and j1=j3, then flin(A, x)j0,j1≤(1 +a0)/n.
•Part 4. Forj0=j3, and j1̸=j3, then flin(A, x)j0,j1≤1/n.
•Part 5. Forj0̸=j3, for all j1∈[n]then flin(A, x)j0,j1= 1/n.
Proof. Proof of Part 1.
By computing the inner product we know
ulin(A, x)j0,j1=a0logn+ log n
= (1 + a0) logn
where the last step follows from simple algebra.
50

--- PAGE 52 ---
Proof of Part 2.
We have
ulin(A, x)j0,j1= log n
Proof of Part 3.
We can show
flin(A, x)j0,j1≤(1 +a0) logn
(1 +a0) logn+ (n−1)·logn
≤1 +a0
n
Proof of Part 4.
We can show
flin(A, x)j0,j1≤logn
(1 +a0) logn+ (n−1)·logn
≤1
n
Proof of Part 5.
We can show
flin(A, x)j0,j1=logn
n·logn
=1
n
F.5.2 Dataset 0 Property when applying function clin
Lemma F.15. If the following conditions hold
•Let{A1, A2, A3}from dataset D0(see Definition F.1)
•LetQK⊤=Id
•LetV=Id
•Letn=t(d−2)
Then for clin(A, x)j0,i0entry we have
•Part 1. Forj0=j2andi0= 1, we have clin(A, x)j0,i0≤1+a0
n
•Part 2. Forj0=j2andi0∈ {2,···, d}, we have clin(A, x)j0,i0≤(1+a0)t
n
•Part 3. Forj0̸=j2andi0= 1, we have clin(A, x)j0,i0≤1
n
•Part 4. Forj0̸=j2andi0∈ {2,···, d}, we have clin(A, x)j0,i0≤t
n
51

--- PAGE 53 ---
Proof. Proof of Part 1.
It follows from Part 3 and Part 4 of Lemma F.14, and Type I column in A3.
Proof of Part 2.
It follows from Part 3 and Part 4 of Lemma F.14, and Type II column in A3.
We know that each entry in flin(A, x)j0is at least 0 and is at most1+a0
n.
We know that each entry in ( A3V)i0∈Rnis at least 0 and at most 1 and it is t-sparse.
Thus,
⟨flin(A, x)j0,(A3V)i0⟩ ≤(1 +a0)t
n
Proof of Part 3.
It follows from Part 5 of Lemma F.14, and Type I column in A3.
Proof of Part 4.
It follows from Part 5 of Lemma F.14, and Type II column in A3.
F.5.3 Dataset 0 Property when applying function clinwith random signs
Lemma F.16. If the following conditions hold
•Let{A1, A2, A3,}be from dataset D0(see Definition F.1)
•Lett√
d=o(n0.99)(since t(d−1) = n, then this implies d=ω(n0.02))
Then, for each random string σ∈ {− 1,+1}d, we have
•Part 1. Ifj0=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<0.5] = 1 −δ/poly( n)
•Part 2. Ifj0̸=j3, then Pr[|⟨clin(A, x)j0, σ⟩|<0.5] = 1 −δ/poly( n)
Proof. Proof of Part 1. It follows from Part 1,2,3 of Lemma F.15, random sign distribution.
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩| ≤O(p
log(n/δ))·(1 +a0)t√
d
n
≤0.5
with probability at least 1 −δ/poly( n). Here a0=O(1)
Proof of Part 2. It follows from Part 4,5,6 of Lemma F.15 and Hoeffding inequality (Lemma 3.1).
By hoeffding inequality (Lemma 3.1), we know that
|⟨clin(A, x)j0, σ⟩| ≤O(p
log(n/δ))·t√
d
n
≤0.5
with probability at least 1 −δ/poly( n). Here the last step due to t√
d=o(n0.99) and and
poly(log n)≤n0.01.
52

--- PAGE 54 ---
F.5.4 Dataset 0 Property when applying function Flin
Theorem F.17. If the following conditions hold
•Letd∈[ω(n0.02), n]
•Letτ= 0.9
•Leta0∈(0,0.1)
•Letm=O(log(n/δ))
•For any {A1, A2, A3}fromD0(Definition F.1)
•LetFlin(A1, A2, A3) :=ϕ(Pn
j0=1Pm
j1=1ϕτ(⟨clin(A, x), yj1))
Then we have
•With high probability 1−δ/poly( n),Flin(A1, A2, A3) = 0
Proof. It follows from using Lemma F.4.
F.6 Experiments for Cross-Attention
200 400 600 800 1000
n0.00.20.40.60.81.0Ratio of Success
(a) Cross-Attention n
0 10 20 30 40 50
d0.00.20.40.60.81.0Ratio of Success (b) Cross-Attention d
0 20 40 60 80
m0.00.20.40.60.81.0Ratio of Success (c) Cross-Attention m
0.0 0.5 1.0 1.5 2.0 2.5 3.0
a_10.00.20.40.60.81.0Ratio of Success
(d) Cross-Attention a1
0.0 0.2 0.4 0.6 0.8
a_00.00.20.40.60.81.0Ratio of Success (e) Cross-Attention a0
Figure 7: Cross-Attention
We set the dataset as described in Section F.1. For a pair of inputs ( A01, A02, A03)∈ D 0and
(A11, A12, A13)∈ D 1, we define the event Esuccess as
Esuccess :=Fexp(A11, A12, A13)>0
53

--- PAGE 55 ---
∧Flin(A11, A12, A13) = 0
∧Fexp(A01, A02, A03) = 0
∧Flin(A01, A02, A03) = 0 .
We first fix X= vec( Id), and deploy the numerical experiments for parameters n,d,m,a0and
a1. To be specific,
•We deploy experiments for n∈[200,1000] with a step size of 10. For each n, we set d= 11,
δ= 0.01,m= log( n/δ),a0= 0.01,a1= 3. For each set of parameters, we iteratively
generated models 100 times and recorded the occurrences of successful events denoted as
Esuccess . The result can be found in Figure (a) in Figure 7.
•We deploy experiments for d∈ {2,3,5,6,9,11,21,26,41,51}. For each d, we set n= 200,
δ= 0.01,m= log( n/δ),a0= 0.01,a1= 3. For each set of parameters, we iteratively
generated models 100 times and recorded the occurrences of successful events denoted as
Esuccess . The result can be found in Figure (b) in Figure 7.
•We deploy experiments for m∈[1,80] with a step size of 1. For each m, we set n= 200,
d= 11, δ= 0.01,m= log( n/δ),a0= 0.01,a1= 3. For each set of parameters, we iteratively
generated models 100 times and recorded the occurrences of successful events denoted as
Esuccess . The result can be found in Figure (c) in Figure 7.
•We deploy experiments for a1∈[0.1,3] with a step size of 0 .036. For each a1, we set n= 200,
d= 12, δ= 0.01,m= log( n/δ),a0= 0.01. For each set of parameters, we iteratively
generated models 100 times and recorded the occurrences of successful events denoted as
Esuccess . The result can be found in Figure (d) in Figure 7.
•We deploy experiments for a0∈[0.01,0.9] with a step size of 0 .01. For each a0, we set
n= 200, d= 12, δ= 0.01,m= log( n/δ),a0= 0.01. For each set of parameters, we
iteratively generated models 100 times and recorded the occurrences of successful events
denoted as Esuccess . The result can be found in Figure (e) in Figure 7.
References
[ACSS20] Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness
for linear algebra on geometric graphs. In 2020 IEEE 61st Annual Symposium on
Foundations of Computer Science (FOCS) , pages 541–552. IEEE, 2020.
[AG23] Pranjal Awasthi and Anupam Gupta. Improving length-generalization in transformers
via task hinting. arXiv preprint arXiv:2310.00726 , 2023.
[ALS+23] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo. Bypass ex-
ponential time preprocessing: Fast neural network training via weight-data correlation
preprocessing. In NeurIPS . arXiv preprint arXiv:2211.14227, 2023.
[AS23a] Josh Alman and Zhao Song. Fast attention requires bounded entries. In NeurIPS .
arXiv preprint arXiv:2302.13214, 2023.
[AS23b] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing
matrix softmax attention to kronecker computation. arXiv preprint arXiv:2310.04064 ,
2023.
54

--- PAGE 56 ---
[ASA+22] Ekin Aky¨ urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What
learning algorithm is in-context learning? investigations with linear models. arXiv
preprint arXiv:2211.15661 , 2022.
[AZLS19a] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning
via over-parameterization. In International Conference on Machine Learning , pages
242–252. PMLR, 2019.
[AZLS19b] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training
recurrent neural networks. Advances in neural information processing systems , 32,
2019.
[BAG20] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the Ability and Limita-
tions of Transformers to Recognize Formal Languages. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages
7096–7116, Online, November 2020. Association for Computational Linguistics.
[BCE+23] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha
Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2305.12712 , 2023.
[Bel22] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Com-
putational Linguistics , 48(1):207–219, March 2022.
[BIS17] Arturs Backurs, Piotr Indyk, and Ludwig Schmidt. On the fine-grained complexity of
empirical risk minimization: Kernel methods and neural networks. Advances in Neural
Information Processing Systems (NeurIPS) , 30, 2017.
[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in neural information processing
systems , 33:1877–1901, 2020.
[BPG20] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of
transformers and its implications in sequence modeling. In Proceedings of the 24th
Conference on Computational Natural Language Learning , pages 455–475, Online,
November 2020. Association for Computational Linguistics.
[BPSW21] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over-
parametrized) neural networks in near-linear time. In ITCS , 2021.
[BSZ23] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic
attention maintenance in large language models. arXiv preprint arXiv:2304.02207 ,
2023.
[CDL+22] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and
Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural
network models. In International Conference on Learning Representations (ICLR) ,
2022.
55

--- PAGE 57 ---
[CDW+21] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R´ e. Scat-
terbrain: Unifying sparse and low-rank attention. Advances in Neural Information
Processing Systems (NeurIPS) , 34:17413–17426, 2021.
[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences
with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.
[Che18] Lijie Chen. On the hardness of approximate and exact (bichromatic) maximum inner
product. In CCC . arXiv preprint arXiv:1802.02325, 2018.
[CJW19] Lijie Chen, Ce Jin, and R Ryan Williams. Hardness magnification for all sparse
np languages. In 2019 IEEE 60th Annual Symposium on Foundations of Computer
Science (FOCS) , pages 1240–1255. IEEE, 2019.
[CLD+20] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,
et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794 , 2020.
[CLMY21] HanQin Cai, Yuchen Lou, Daniel Mckenzie, and Wotao Yin. A zeroth-order block
coordinate descent algorithm for huge-scale black-box optimization. arXiv preprint
arXiv:2102.10707 , 2021.
[CLP+21] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao,
Zhao Song, Anshumali Shrivastava, and Re.Mongoose Christopher. A learnable lsh
framework for efficient neural network training. International Conference on Learning
Representation , 2021.
[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
arXiv:2204.02311 , 2022.
[DCL+22] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and
Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural
network models. In ICLR . arXiv preprint arXiv:2112.00029, 2022.
[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018.
[DGS23] Yichuan Deng, Yeqi Gao, and Zhao Song. Solving tensor low cycle rank approximation.
arXiv preprint arXiv:2304.06594 , 2023.
[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and  Lukasz
Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.
[DKOD20] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-
efficient attention using asymmetric clustering. Advances in Neural Information Pro-
cessing Systems (NeurIPS) , 33:6476–6489, 2020.
[DLMS23] Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th order algo-
rithm for softmax attention optimization. arXiv preprint arXiv:2307.08352 , 2023.
56

--- PAGE 58 ---
[DLS23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regres-
sion. arXiv preprint arXiv:2304.10411 , 2023.
[DMS23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic
attention sparsification algorithms for over-parameterized feature dimension. arxiv
preprint: arxiv 2304.03426 , 2023.
[DSX23] Yichuan Deng, Zhao Song, and Shenghao Xie. Convergence of two-layer regression
with nonlinear units. arXiv preprint arXiv:2308.08358 , 2023.
[DZPS19] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054 , 2019.
[EGKZ21] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases
and variable creation in self-attention mechanisms. arXiv preprint arXiv:2110.10090 ,
2021.
[EGZ20] Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. How can self-attention networks recog-
nize Dyck-n languages? In Findings of the Association for Computational Linguistics:
EMNLP 2020 , pages 4301–4306, Online, November 2020. Association for Computa-
tional Linguistics.
[GMS23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential
regression. arXiv preprint arXiv:2303.16504 , 2023.
[GSWY23] Yeqi Gao, Zhao Song, Weixin Wang, and Junze Yin. A fast optimization view: Refor-
mulating single layer attention in llm based on tensor and svm trick, and solving it in
matrix multiplication time. arXiv preprint arXiv:2309.07418 , 2023.
[GSX23] Yeqi Gao, Zhao Song, and Shenghao Xie. In-context learning for attention scheme:
from single softmax regression to multiple softmax regression via a tensor trick. arXiv
preprint arXiv:2307.02419 , 2023.
[GSY23] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized
large language models. arXiv preprint arXiv:2308.10502 , 2023.
[GSYZ23] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for
attention computation. arXiv preprint arXiv:2307.08045 , 2023.
[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can trans-
formers learn in-context? a case study of simple function classes. arXiv preprint
arXiv:2208.01066 , 2022.
[HJK+23] Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, and
Amir Zandieh. Hyperattention: Long-context attention in near-linear time. arXiv
preprint arXiv:2310.05869 , 2023.
[HL19] John Hewitt and Percy Liang. Designing and interpreting probes with control tasks.
InProceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 2733–2743, Hong Kong, China, November 2019. Association
for Computational Linguistics.
57

--- PAGE 59 ---
[Hoe63] Wassily Hoeffding. Probability inequalities for sums of bounded random variables.
Journal of the American Statistical Association , 58(301):13–30, 1963.
[Inc23] Adobe Inc. Adobe firefly. https://www.adobe.com/sensei/generative-ai/firefly.html,
2023.
[JX23] Ce Jin and Yinzhan Xu. Removing additive structure in 3sum-based reductions. In
Proceedings of the 55th Annual ACM Symposium on Theory of Computing , pages 405–
418, 2023.
[KKL20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient trans-
former. arXiv preprint arXiv: 2001.04451 , 2020.
[KMZ23] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast trans-
formers via sketches for polynomial kernels. arXiv preprint arXiv:2310.01655 , 2023.
[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran¸ cois Fleuret. Trans-
formers are rnns: Fast autoregressive transformers with linear attention. In Interna-
tional conference on machine learning , pages 5156–5165. PMLR, 2020.
[LCZ+23] Langming Liu, Liu Cai, Chi Zhang, Xiangyu Zhao, Jingtong Gao, Wanyu Wang, Yifu
Lv, Wenqi Fan, Yiqi Wang, Ming He, et al. Linrec: Linear attention mechanism for
long-term sequential recommender systems. In Proceedings of the 46th International
ACM SIGIR Conference on Research and Development in Information Retrieval , pages
289–299, 2023.
[LFS+17] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv
preprint arXiv:1703.03130 , 2017.
[LL18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via
stochastic gradient descent on structured data. Advances in neural information pro-
cessing systems , 31, 2018.
[LLH+23] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable
stochastic second-order optimizer for language model pre-training. arXiv preprint
arXiv:2305.14342 , 2023.
[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.
[LSDZ20] Rui Li, Jianlin Su, Chenxi Duan, and Shunyi Zheng. Linear attention mechanism: An
efficient attention for semantic segmentation. arXiv preprint arXiv:2007.14902 , 2020.
[LSZ23] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh
regression problems. arXiv preprint arXiv:2303.15725 , 2023.
[LWD+23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi Chen. Deja vu:
Contextual sparsity for efficient llms at inference time. In ICML , 2023.
58

--- PAGE 60 ---
[Man23] James Manyika. An overview of bard: an early experiment with generative ai. Tech-
nical report, Tech. rep., Technical report, Google AI, 2023.
[MDA22] Gary Marcus, Ernest Davis, and Scott Aaronson. A very preliminary analysis of dall-e
2.arXiv preprint arXiv:2204.13807 , 2022.
[MGN+23] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi
Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes.
arXiv preprint arXiv:2305.17333 , 2023.
[MOSW22] Alexander Munteanu, Simon Omlor, Zhao Song, and David Woodruff. Bounding the
width of neural networks via coupled initialization a worst case analysis. In Interna-
tional Conference on Machine Learning , pages 16083–16122. PMLR, 2022.
[Ope23] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.
[PMB19] Jorge P´ erez, Javier Marinkovi´ c, and Pablo Barcel´ o. On the turing completeness of
modern neural network architectures. arXiv preprint arXiv:1901.03429 , 2019.
[PMXA23] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora. Trainable
transformer in transformer. arXiv preprint arXiv:2307.01189 , 2023.
[PSZA23] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific
skill localization in fine-tuned language models. arXiv preprint arXiv:2302.06600 ,
2023.
[PTDU16] Ankur P Parikh, Oscar T¨ ackstr¨ om, Dipanjan Das, and Jakob Uszkoreit. A decompos-
able attention model for natural language inference. arXiv preprint arXiv:1606.01933 ,
2016.
[RSM+23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher
D.Manning, and Chelsea Finn. Direct preference optimization: Your language model
is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023.
[SHT23] Clayton Sanford, Daniel Hsu, and Telgarsky. Representational strengths and limita-
tions of transformers. arXiv preprint arXiv:2306.02896 , 2023.
[SY19] Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix
chernoff bound. arXiv preprint arXiv:1906.03593 , 2019.
[SZKS21] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how
single head attention learns. arXiv preprint arXiv:2103.07601 , 2021.
[SZZ+21a] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient
attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 3531–3539, 2021.
[SZZ21b] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized
neural network in subquadratic time. arXiv preprint arXiv:2112.07628 , 2021.
[TDFH+22] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-
shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda:
Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022.
59

--- PAGE 61 ---
[TDP19] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP
pipeline. In Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 4593–4601, Florence, Italy, July 2019. Association for
Computational Linguistics.
[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971 , 2023.
[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 , 2023.
[VB19] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer
language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages 63–76, Florence, Italy, August 2019.
Association for Computational Linguistics.
[VBC20] James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical
theory of attention. arXiv preprint arXiv:2007.02876 , 2020.
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in
neural information processing systems , 30, 2017.
[WCM21] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation:
a case study on approximating turing machines with transformers. arXiv preprint
arXiv:2107.13163 , 2021.
[WLK+20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,
Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent
abilities of large language models. arXiv preprint arXiv:2206.07682 , 2022.
[WZW23] Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are
implicitly topic models: Explaning and finding good demonstrations for in-context
learning. arXiv preprint arXiv:2301.11916 , 2023.
[YBR+20] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv
Kumar. Are transformers universal approximators of sequence-to-sequence functions?
InInternational Conference on Learning Representations , 2020.
[YCRI22] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing
with large language models. In 27th International Conference on Intelligent User
Interfaces , pages 841–852, 2022.
[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and
Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding.
Advances in neural information processing systems , 32, 2019.
60

--- PAGE 62 ---
[YPPN21] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-
attention networks can process bounded hierarchical languages. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Pa-
pers) , pages 3770–3785, Online, August 2021. Association for Computational Linguis-
tics.
[ZBB+22] Yi Zhang, Arturs Backurs, S´ ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, and
Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task, 2022.
[ZFB23] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear
models in-context. arXiv preprint arXiv:2306.09927 , 2023.
[ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating
transformers via kernel density estimation. In ICML . arXiv preprint arXiv:2302.02451,
2023.
[ZKV+20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention
models? Advances in Neural Information Processing Systems , 33:15383–15393, 2020.
[ZPGA23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse
while predicting the masked word? arXiv preprint arXiv:2303.08117 , 2023.
[ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.
[ZSJ+17] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recov-
ery guarantees for one-hidden-layer neural networks. In International conference on
machine learning , pages 4140–4149. PMLR, 2017.
[ZWK22] Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-
attention mechanism. In International Conference on Machine Learning , pages 27011–
27041. PMLR, 2022.
61
