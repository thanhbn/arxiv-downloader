# 2207.03341.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2207.03341.pdf
# File size: 11226920 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
International Journal of Computer Vision
https://doi.org/10.1007/s11263-024-02035-5
Softmax-free Linear Transformers
Jiachen Lu1,·Junge Zhang1,·Xiatian Zhu2,·Jiafeng Feng1,·Tao Xiang2,·Li Zhang1/enve♀e
Abstract Vision transformers (ViTs) have pushed the
state-of-the-art for visual perception tasks. The self-
attention mechanism underpinning the strength of ViTs
has a quadratic complexity in both computation and
memory usage. This motivates the development of ap-
proximating the self-attention at linear complexity. How-
ever, an in-depth analysis in this work reveals that ex-
isting methods are either theoretically flawed or empir-
ically ineffective for visual recognition. We identify that
their limitations are rooted in the inheritance of soft-
max based self-attention during approximations, that
is, normalizing the scaled dot-product between token
feature vectors using the softmax function. As preserv-
ing the softmax operation challenges any subsequent
linearization efforts. By this insight, a family of SOftmax-
Free Transformers (SOFT ) are proposed. Specifically,
a Gaussian kernel function is adopted to replace the
dot-product similarity, enabling a full self-attention ma-
trix to be approximated under low-rank matrix decom-
position. For computational robustness, we estimate the
Moore-Penrose inverse using an iterative Newton-Raphson
method in the forward process only, while calculating
its theoretical gradients only once in the backward pro-
cess. To further expand applicability ( e.g., dense predic-
tion tasks), an efficient symmetric normalization tech-
nique is introduced. Extensive experiments on ImageNet,
COCO and ADE20K show that our SOFT significantly
improves the computational efficiency of existing ViT
variants. With linear complexity, much longer token se-
quences are permitted by SOFT, resulting in superior
trade-off between accuracy and complexity. Code and
models are available at https://github.com/fudan-zvg/
SOFT .
Corresponding author: Li Zhang
E-mail: lizhangfd@fudan.edu.cn
1School of Data Science, Fudan University, Shanghai, China
2University of Surrey, Guildford, UKKeywords Transformer ·linear complexity ·softmax
normalization ·softmax-free ·Gaussian attention
1 Introduction
Recently the step change brought by Transformers (Vaswani
et al., 2017) in natural language processing (NLP) (Brown
et al., 2020, Devlin et al., 2019) seems to have arrived in
vision (Dosovitskiy et al., 2021, Yuan et al., 2021, Zheng
et al., 2021, Zhu et al., 2021). Indeed, with less inductive
bias in its architecture design than Convolution neu-
ral networks (CNNs), pure Vision Transformer (ViT)
(Dosovitskiy et al., 2021) and its variants have shown
to be able to outperform CNNs on various vision tasks
(d’Ascoli et al., 2021, Jaegle et al., 2021). However,
there is a bottleneck in any Transformer based model,
namely its quadratic complexity in both computation
and memory usage. This is intrinsic to the self-attention
mechanism: given a sequence of tokens ( e.g., words or
image patches) as input, the self-attention module it-
eratively learns the feature representations by relating
one token to all other tokens. This results in a quadratic
complexity O(n2) with the token sequence length nin
both computation (time) and memory (space) since an
n×nsized attention matrix needs to be computed
and saved during inference. This problem is particularly
acute in vision: a 2D image after tokenization will pro-
duce a far longer sequence than those in NLP even with
a moderate spatial resolution. This quadratic complex-
ity thus prevents a ViT model from modeling images
at high spatial resolutions, which are often crucial for
visual recognition tasks.
A natural solution is to reduce the complexity of
self-attention computation via approximation. Indeed,
there have been a number of attempts in NLP (Choro-
manski et al., 2021, Kitaev et al., 2020, Wang et al.,
2020, Xiong et al., 2021). For example, (Wang et al.,arXiv:2207.03341v3  [cs.CV]  15 Mar 2024

--- PAGE 2 ---
2 Jiachen Lu1, et al.
10 20 30 40 50 60 70 80 90
Parameter (M)7072747678808284Imagenet T op-1 Accuracy (%)
PVT-TinyPVT-SmallPVT-MediumPVT-Large
ResNet-18ResNet-50ResNet-101T2T-ViT-19T2T-ViT-24
DeiT-B
ViT-BSwin-TSwin-SSwin-B
T wins-SVT-S
SAN10CoAtNet-1CoAtNet-2
SOFT-TinySOFT-SmallSOFT-MediumSOFT-Large
(a)
784×1784×2784×3784×4784×5784×6784×7784×8
/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000056/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000004f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013/uni00000013/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c
/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni0000002f/uni0000004c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000031/uni0000005c/uni00000056/uni00000057/uni00000055/uni00000052/uni00000050/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000036/uni00000032/uni00000029/uni00000037 (b)
Fig. 1 Top-1 classification accuracy on ImageNet (Deng et al., 2009) validation set with respect to parameters and the memory
usage corresponding to the token sequence length in practice compared to other methods. (a) Comparison with CNN models:
ResNet (He et al., 2016) and CoAtNet (Dai et al., 2021) Transformer models: PVT (Wang et al., 2021), Swin (Liu et al.,
2021), DeiT (Touvron et al., 2021a), ViT (Dosovitskiy et al., 2021), T2T-ViT (Yuan et al., 2021), Twins-SVT (Chu et al.,
2021) and SAN10 (Zhao et al., 2020); (b) Comparison with Transformer (Vaswani et al., 2017), Linformer (Wang et al., 2020),
Nystr¨ oformer (Xiong et al., 2021) and Performer (Choromanski et al., 2021). The memory usage is measured with a batch size
of 1 on a 16GB Tesla V100.
2020) takes a naive approach by shortening the length
of Key and Value via learnable projections. Such a coarse
approximation would inevitably cause performance degra-
dation. In contrast, (Choromanski et al., 2021, Katharopou-
los et al., 2020) both leverage the kernel mechanism
to approximate softmax normalization to linearize the
computation in self-attention. (Kitaev et al., 2020) in-
stead adopts a hashing strategy to selectively compute
the most similar pairs. Recently, (Xiong et al., 2021)
uses Nystr¨ om matrix decomposition to reconstruct the
full attention matrix with polynomial iteration for ap-
proximating the pseudo-inverse of the landmark ma-
trix. Nonetheless, softmax normalization is simply du-
plicated across the matrix decomposition process, which
is theoretically unsound. We empirically found that none
of these methods are effective when applied to vision
(see Sec. 4.1).
In this work, we identify that the limitations of ex-
isting efficient Transformers are caused by the use of
softmax self-attention , and for the first time propose a
softmax-free Transformer. More specifically, in all ex-
isting Transformers (with or without linearization), a
softmax normalization is needed on top of scaled dot-
product between token feature vectors (Vaswani et al.,
2017). Keeping this softmax operation challenges any
subsequent linearization efforts. To overcome this ob-
stacle, we introduce a novel softmax-free self-attention
mechanism, named as SOFT, with linear complexity
O(n) in both space and time. Specifically, SOFT uses
Gaussian kernel to define the similarity (self-attention)function without the need for subsequent softmax nor-
malization. With this softmax-free attention matrix, we
further introduce a novel low-rank matrix decomposi-
tion algorithm for approximation. The robustness of the
approximation is theoretically guaranteed by employing
a Newton-Raphson method for reliably computing the
Moore-Penrose inverse of the matrix.
We make the following contributions .(I)We in-
troduce a novel softmax-free Transformer with linear
space and time complexity. (II) Our attention matrix
approximation is achieved through a novel matrix de-
composition algorithm with theoretical guarantee. (III)
To evaluate our method for visual recognition tasks, we
design a family of generic backbone architectures with
varying capacities using SOFT as the core self-attention
component. Extensive experiments show that with a
linear complexity (Figure 1b), our SOFT models can
take in as input much longer image token sequences.
As a result, with the same model size, our SOFT out-
performs the state-of-the-art CNNs and ViT variants
on ImageNet (Deng et al., 2009) classification in the
accuracy/complexity trade-off (Figure 1a).
A preliminary version of this work was presented
in NeurIPS 2021 spotlight (Lu et al., 2021). In this
paper, we have further extended our conference ver-
sion as follows: (i) We improve the efficiency and ro-
bustness in computing Moore-Penrose inverse by us-
ing an iterative method in the forward process only
while calculating its theoretical gradient only once in
the backward propagation. (ii) We analyze the limita-

--- PAGE 3 ---
Softmax-free Linear Transformers 3
tions of our preliminary SOFT from a matrix spectral
norm perspective, revealing the importance of normal-
ization for enhancing the model’s task generalizability.
(iii) We prove that the preliminary SOFT experiences
a second-order increase in matrix spectral norm rela-
tive to the matrix size, making it fail in dense vision
problems. (iv) To address these limitations, we propose
a normalized softmax-free self-attention, keeping linear
complexity while enhancing performance, supported by
both theoretical proof and extensive experiments. (v)
The improved SOFT outperforms the state-of-the-art
CNNs and ViTs for classification on ImageNet (Deng
et al., 2009), object detection on COCO (Lin et al.,
2014) and semantic segmentation on ADE20K (Zhou
et al., 2019).
2 Related work
2.1 Vision Transformers
There is a surge of research interests recently in ex-
ploiting Transformers for visual recognition tasks (Guo
et al., 2022, Touvron et al., 2021a, Wang et al., 2021,
2018, Yuan et al., 2021, Zhang et al., 2020), inspired by
their remarkable success in NLP (Brown et al., 2020,
Devlin et al., 2019, Vaswani et al., 2017). Core to these
NLP and vision transformers is the same self-attention
mechanism (Vaswani et al., 2017) that computes a self-
attention matrix by exhaustively comparing token pairs.
This means a quadratic complexity with the sequence
length in both space and time, which thus limits the
scalability of Transformers in dealing with long sequences.
This limitation is more serious in vision than NLP:
To process an image with at least thousands of pixels,
patch-wise tokenization is a must for Transformers to
control the computational cost. Given higher resolution
images, the patch size also needs to be enlarged propor-
tionally sacrificing the spatial resolution. This limits the
capability of Transformers, e.g., learning fine-grained
feature representation as required in many visual recog-
nition tasks.
2.2 Linear Transformers
Recently, there have been a number of linear/efficient
variants (Choromanski et al., 2021, Kasai et al., 2021,
Katharopoulos et al., 2020, Kitaev et al., 2020, Peng
et al., 2021, Tay et al., 2023, Wang et al., 2020) of
Transformers in NLP. For example, (Wang et al., 2020)
learns to shrink the length of Key and Value based on
a low-rank assumption. (Kitaev et al., 2020) adoptsa hashing strategy to selective the most similar pairs
and only compute attention among them. (Choroman-
ski et al., 2021, Katharopoulos et al., 2020) utilize differ-
ent kernel functions for approximating softmax-based
self-attention matrix. (Peng et al., 2021) applies ran-
dom feature mapping on the sequences to approach
the original softmax function. (Kasai et al., 2021) de-
creases the time and memory consumption of the at-
tention matrix by replacing the softmax function with
its linear-complexity recurrent alternative. When ap-
plied to visual recognition tasks, however, we show that
these models have considerable performance degrada-
tion compared to the standard Transformers (Vaswani
et al., 2017) (see Sec. 4.1).
The most related work to SOFT is (Xiong et al.,
2021) which uses the Nystr¨ om matrix decomposition
to avoid computing the full attention matrix. However,
this method suffers from several theoretical defects: (1)
As the standard self-attention needs to apply row-wise
softmax normalization on the full attention matrix, a
direct application of matrix decomposition is infeasible.
As a workaround without solid theoretical support, soft-
max is simply applied to all the ingredient matrices in
(Xiong et al., 2021). Such an approximation is not guar-
anteed theoretically. (2) With a polynomial iteration
method, it is not guaranteed that the generalized atten-
tion matrix inverse can be computed when the matrix
is a nearly singular one in practice. In contrast to all the
above methods, in this paper we propose a softmax-free
self-attention mechanism that facilitates matrix decom-
position for complexity minimization with theoretical
guarantees.
3 Method
3.1 Softmax-free self-attention
formulation
A schematic illustration of our model is given in Figure
2. Let’s first look at our attention module design. Given
a sequence of ntokens X∈Rn×dwith each token repre-
sented by a d-dimensional feature vector, self-attention
(Vaswani et al., 2017) aims to discover the correlations
of all token pairs exhaustively.
Formally, Xis first linearly projected into three de-
dimensional spaces ( query ,key, and values ) as:
Q=XWq∈Rn×de, K=XWk∈Rn×de, V=XWv∈Rn×de,
(1)
where Wq, Wk, Wv∈Rd×deare learnable matrices. For
equation simplicity, we omit the multi-head notation in
self-attention operations. However, it should be noted

--- PAGE 4 ---
4 Jiachen Lu1, et al.
that multi-head mechanisms are employed throughout.
Self-attention can be expressed in a generic formulation
as:
yi,:=nX
j=1α(Qi,:, Kj,:)⊙Vj,:, (2)
where ⊙is the Hadamard product, and i, j∈ {1,···, n}
index the tokens. The key self-attention function α:
Rde×Rde→Ris composed of a nonlinear function
β:R→Rand a relation function γ:Rde×Rde→R.
A dominant instantiation of αis the scaled dot-product
based softmax self-attention (Vaswani et al., 2017), de-
fined as
β(·) = softmax( ·), γ(Qi,:, Kj,:) =1√de·Q⊤
i,:Kj,:.(3)
Whilst this softmax self-attention has been the de facto
choice and seldomly questioned, as discussed earlier it is
not necessarily suited for linearization. To facilitate the
design of linear self-attention, we introduce a softmax-
free self-attention function with the dot-product replaced
by a Gaussian kernel as:
β′(·) = exp( ·), γ′(Qi,:, Kj,:) =−1
2√de·∥Qi,:−Kj,:∥2
2.
(4)
A dissection of Transformers by Tsai et al. (Tsai et al.,
2019) reveals negligible performance differences between
asymmetric and symmetric kernels. To maintain the
symmetric properties of the attention matrix as defined
in Eq. (3), we opt for identical projection matrices Wq
andWkin Eq. (1), effectively setting Q=K. To further
investigate the impact of symmetric kernels, additional
experiments are conducted in our ablation studies of
Section 4.4. Our self-attention matrix is then written
as:
Si,j= exp
−1
2√de· ∥Qi,:−Kj,:∥2
2
. (5)
For notation simplicity, we define the matrix formula-
tion as: S= exp ( Q⊖K).
Remarks: Our self-attention matrix Shas three im-
portant properties: (1) It is symmetric; (2) All the el-
ements lie in a unit range of [0 ,1]; (3) All diagonal el-
ements hold the largest value 1 (self-reinforced), with
the bottom ones (corresponding to most dissimilar to-
ken pairs) being close to 0. As Gaussian kernel is a
positive definite kernel (Fasshauer, 2011), Sis deemed
a Gram matrix. However, we find that when using our
kernel-based self-attention matrix Swithout lineariza-
tion, the training of a transformer fails to converge.
More discussion can be found in Section 3.3.3.2 Low-rank regularization via matrix
decomposition with linear complexity
To solve the convergence and quadratic complexity prob-
lems, we leverage matrix decomposition as a unified so-
lution with low-rank regularization. In particular, we
consider Nystr¨ om (Williams and Seeger, 2000), which is
originally a low-rank matrix approximation algorithm.
This enables our model’s complexity to be reduced sig-
nificantly without computing the full self-attention ma-
trixS.
We make this choice because our Sis positive semi-
definite ( i.e., a Gram matrix) without follow-up nor-
malization which are all necessary conditions for Nystr¨ om.
In contrast, (Xiong et al., 2021) totally ignores these
requirements, leading to theoretical flaw in its approx-
imation.
To define the Nystr¨ om method formally, let us ex-
press S= exp ( Q⊖K) as a block matrix:
S=A B
B⊤C
∈Rn×n, (6)
where A∈Rm×m,B∈Rm×(n−m),C∈R(n−m)×(n−m)
with m≪n. Through Nystr¨ om decomposition (see
derivative details in Appendix A), an approximation
can be represented as:
ˆS=A
B⊤
A†
A B
=P⊤A†P, where P=
A B
,
(7)
andA†is the Moore-Penrose (a generalized) inverse of
A.
Algorithm 1: SOFT: Softmax-free attention
Input: Q∈Rn×de, sampling function fs
Sampling eQ←fs(Q) ;
A←exp(eQ⊖eQ),P←exp(eQ⊖Q);
ˆS←P⊤NR(A)P;
Output: ˆS
Algorithm 2: NR: Newton-Raphson iteration
Input: A∈Rm×m, andT ∈Z+
α= 2/∥A∥2
1.Initialize A0←αA;
forkfrom 1toTdo
Ak←2Ak−1−Ak−1AAk−1
end
Output: AT

--- PAGE 5 ---
Softmax-free Linear Transformers 5
Fig. 2 Schematic illustration of the proposed softmax-free self-attention (SOFT) method. P.E.: Position embedding. Dash
lines: linear projection. dh: the hidden dim of each attention head. ◦denotes the matrix dot product.
Sampling: In the standard Nystr¨ om formulation, A
andBare sub-matrices of Sobtained by randomly sam-
pled mtokens, denoted as eQ. We call the sampled eQ
asbottleneck tokens . However, we find empirically that
random sampling is considerably sensitive to the choice
ofm. We hence explore two additional options to lever-
age the structural prior of visual data: (1) Using one
convolutional layer with kernel size kand stride kto
learneQ, and (2) Using average pooling with kernel size
kand stride kto generate eQ. For both, we need to
reshape Qto the form of RH×W×de. Each slide of con-
volution or pooling produces a token. We set kaccord-
ing to the length of Qsuch that mtokens can be ob-
tained. Our experiments show that a convolution layer
performs better in accuracy. We therefore use a convo-
lution layer by default.
AsKis identical to Q, we have eK=eQ. Given these
mtokens, we then compute bottleneck attention Aand
Pas:
A= exp( eQ⊖eK), P = exp( eQ⊖K). (8)
We finally obtain a regularized self-attention matrix ˆS
of SOFT as:
ˆS= exp
Q⊖eK
exp
eQ⊖eK†
exp
eQ⊖K
.(9)
The overall SOFT is summarized in Algorithm 1. The
low-rank regularization is conducted as follows. For com-
puting the attention score between any two tokens, we
first correlate each of them with sampled tokens us-
ing our self-attention function (Equation (5)); With
this correlation representation we then compute their
similarity under the modulation of the generalized in-
verse of eQ’s correlation matrix. Similar as the standard
Nystr¨ om, our design associates the input tokens w.r.t. asmall space spanned by a set of sampled tokens, giving
a proper estimation of the original attention relation-
ships subject to a low-rank constraint. This method is
proved in Appendix A.
Moore-Penrose inverse: An accurate and commonly
used way to calculate the Moore-Penrose inverse is to
use Singular Value Decomposition (SVD). Given A∈
Rm×mand its SVD form A=UΣV⊤where U, V are
m×munitary matrices and Σis a m×mdiago-
nal matrix, the Moore-Penrose inverse of AisA†=
V Σ†U⊤. Nevertheless, SVD is not friendly to the train-
ing process on GPU hence harming the model train-
ing efficiency. To solve this issue, we adopt the New-
ton–Raphson method. It is an iterative algorithm with
the (k+ 1)-th iteration formulated given the previous
iteration as:
Ak+1= 2Ak−AkAAk,and A0=αA. (10)
We now prove that Akfinally converges to Moore-Penrose
inverse of Am×m, ifαis sufficiently small (Ben-Israel
and Cohen, 1966).
Proposition 1 When αis sufficiently small, Ak+1=
2Ak−AkAAk,Akconverges to A†.
The proposition is proved in Appendix B. Though α=
2/∥A∥2
1which ensures good convergence behavior in Al-
gorithm 2, in practice, we find that using an alternative
form gives more stable training and faster convergence.
Specifically, in ∥I−A2βn
∥A∥2
1∥1≤1 where βequals to 0 .5,
we find the smallest nithat holds this inequality. Then,
we initialize αasα=2βni
∥A∥2
1.
The following proposition comes with the proof of
Proposition 1:
Proposition 2 ∥AAkA−A∥and∥Ak−A†∥decreases
to0monotonously, if αis sufficiently small.

--- PAGE 6 ---
6 Jiachen Lu1, et al.
Proof Note that when we multiply Aon both sides of
(33), the equation turns to be:
A−AAk+1A=A(A†−Ak)A(A†−Ak)A
= (AA†−AAk)(A−AAkA).(11)
Similarly norm both sides of (11), considering that ∥AA†−
AAk∥ → 0 and ∥AA†−AAk∥<1 always holds, ∥A−
AAkA∥monotonically decreases to 0. The inequality
(34) implies that ∥Ak−A†∥decreases to 0 monotonously
.
Note although ∥A−AAkA∥monotonically decreases to
0,∥AkAAk−Ak∥cannot be proved to be so yet.
This ensures that our estimated inverse is sufficiently
accurate for matrix decomposition, subject to that our
SOFT attention is regularized. In training stage, we
find that bottleneck matrix Ais always non-singular
in practice and its inverse A−1thus exists. Therefore,
the iteration can be avoided in back propagation be-
cause the differential of matrix inverse can be explicitly
expressed as
∇xL=−Y⊤(∇YL)Y⊤, (12)
where X∈Rm×mis an non-singular matrix and Yis
the inverse of X,i.e.,Y=X−1,∇YLis the gradient
of loss LonYand∇XLis the gradient of loss Lon
X. This can accelerate the training, as validated in Sec.
4.1. The theoretical proof is shown in Appendix B.
Complexity: We summarize the complexity of SOFT
in space and time. For time complexity , it involves: (1)
Sampling: O(nde). (2) Calculating three decomposed
matrices: O(nmd e+mnd e+m2de) =O(2mnd e+m2de);
(3) Moore-Penrose inverse: O(T × m3) =O(Tm3),
where Tis the iteration steps. (4) All matrix multi-
plication: O(nm2+mnd e+mnd e) =O(nm2+2mnd e).
The total time complexity is O((de+ 4mde+m2)n+
Tm3+dem2). The space complexity is decided by four
decomposed matrices with O(n×m) +O(m×m) +
O(m×n) +O(n×de) =O((2m+de)n+m2). As we
keep m(m≪n) a fixed constant in our model, both
time and space complexity are O(n), making SOFT a
linear self-attention.
3.3 Attention normalization
Whilst the above SOFT formulation is competitive for
image classification, transferring the pre-trained model
to downstream tasks with different input token sequence
lengths is limited. We conduct analysis on the model’s
sensitivity against input perturbation. As suggested in
(Yoshida and Miyato, 2017), all parts of a model should
have small spectral norm (i.e., matrix 2-norm or thelargest singular value of a matrix). Concretely, by the
triangle inequality ∥XY∥2≤ ∥X∥2∥Y∥2where Xand
Yare any real matrix, any part with large spectral
norm could lead to significant error accumulation. This
also applies for self-attention matrix, since it is often
symmetric and non-negative definite and its spectral
norm corresponds to the largest eigenvalue. We provide
more theoretical analysis below. Note that both scale
dot-product kernel (linear kernel) and Gaussian kernel
are positive definite kernels, so they have non-negative
eigenvalues.
Proposition 3 In scaled dot-product based softmax self-
attention, assume λ1≥λ2≥ ··· λn≥0are eigenvalues
of self-attention matrix Ssoftmax ∈Rn×n, then λ1≤1.
Proof We rewrite the softmax self-attention as
Ssoftmax =D−1A, (13)
where A∈Rn×nis a real symmetric matrix, and D=
diag( A 1n). We consider the graph Laplacian Lof ma-
trixAdefined by L=D−A, then the noramlized graph
Laplacian can be expressed as
Lrw=D−1L=D−1(D−A) =I−D−1A, (14)
where, Iis an identity matrix. According to (Von Luxburg,
2007), Lrwis a real semi-definite matrix, so all the
eigenvalue of Lrwis non-negative. Therefore, eigenval-
ues of I−D−1Aare non-negative, which leads to the
fact that eigenvalues λ1,···, λnofD−1Ais less or equal
to 1.
This proposition means that softmax normalization is
useful in restricting the range of self-attention matrix’s
eigenvalues to [0 ,1] and eventually the effect of error
accumulation. This is an important role softmax op-
eration plays in improving generalizability with stan-
dard self-attention. However, this is not the case for
our softmax-free self-attention as formulated above.
Proposition 4 In Gaussian kernel-based self-attention,
ifλ1≥λ2≥ ··· λn≥0are eigenvalues of self-attention
matrix Sgaussian ∈Rn×n, then λ1≤n.
Proof
nX
i=1λi= Tr( A) =n,
since the diagonal elements of Gaussian kernel-based
self-attention is always 1. Therefore, with the fact that
all the eigenvalues are positive, we have λ1≤n.

--- PAGE 7 ---
Softmax-free Linear Transformers 7
A larger upper bound of eigenvalue with our pro-
posed self-attention could thus lead to less generaliz-
ability, due to a trend of higher error accumulation.
Specifically, for a softmax-free self-attention matrix, ˆS=
P⊤A†P, we have
∥ˆS∥2=∥P⊤A†P∥2≤ ∥P∥2
2∥A†∥2. (15)
For the bottleneck matrix of softmax-free attention A∈
Rm×m, we assume it is k-connected ( k << m ),i.e.,
there are kfields disconnected with each other. This is
because a field represents a semantic part of an image
and the number is often small.
Proposition 5 Assume the bottleneck matrix of softmax-
free attention , A∈Rm×misk-connected. If λ1≥λ2≥
···λm≥0are eigenvalues of A†, then λ1=O(m2)and
∥A†∥2=O(m2).
The proposition is proved in Appendix C. This propo-
sition indicates that ∥ˆS∥2is proportional quadratically
with the length mof bottleneck token sequence. It im-
plies that the above SOFT formula would be limited to
the applications with short bottleneck token sequences
(e.g., image classification).
Proposition 6 For the bottleneck matrix of SOFT self-
attention A∈Rm×m, we have
∥D−1/2A†D−1/2∥2=O(m), (16)
where D=diag(A 1m)and 1mis an all one m-D vector.
Proof
∥D−1/2A†D−1/2∥2≤ ∥D−1/2∥2
2∥A†∥2=∥D−1∥2∥A†∥2,
also, Dis a diagonal matrix,
∥D−1∥2∥A†∥2=∥D−1A†∥2=∥A†
n∥2=O(m)
This proposition suggests that symmetric normaliza-
tion can lower the largest eigenvalue of A†, reducing
the spectral norm of ˆS.
Attention normalization: In light of the above the-
orem, we further introduce the normalization of SOFT
as:
ˆS= exp
Q⊖eK
D−1
2
exp
eQ⊖eK†
D−1
2exp
eQ⊖K
,
(17)
where D= diag
exp
eQ⊖eK
1m
. This yields a tiny
increase of O(m2) in the computational complexity, which
can be further reduced to O(logm) on GPU by parallelreduction algorithm (Cheng et al., 2014). This discov-
ery unleashes new possibilities for SOFT and paves the
way for its expanded usage, particularly in applications
that demand dense information reasoning (such as ob-
ject detection and semantic segmentation). This results
in our SOFT++ formulation.
3.4 Instantiations
Figure 2 shows how our proposed softmax-free self-attention
block ( SOFT block ) can be implemented in a neural
network. We replace the self-attention block with our
SOFT block in the traditional Transformer, that is, we
stack a SOFT block with a feed forward residual block
(Dosovitskiy et al., 2021) to form a softmax-free Trans-
former layer ( SOFT layer ).
Focusing on the general image recognition tasks, we
integrate our SOFT layer into the recent pyramidal
Transformer architecture (Wang et al., 2021) to form
our final model SOFT . Further, several improvements
are introduced in patch embedding ( i.e., tokenization).
Specifically, unlike (Wang et al., 2021) that uses a com-
bination of non-overlapping convolution and layer nor-
malization (Ba et al., 2016), we adopt a stack of over-
lapping convolutions, batch normalization (Ioffe and
Szegedy, 2015) and ReLU non-linearity. Concretely, the
STEM is implemented by 3 units of 3x3 Conv →BN→ReLU ,
with the stride of 2, 1, 2 respectively. Then, one such
unit is applied to each of three following down-sampling
operations with stride of 2 in the multi-stage architec-
ture.
The architecture hyper-parameters of SOFT are: d:
the input channel dimension of SOFT layer. de: the em-
bedding dimension of tokens in SOFT block. In prac-
tice, we set de=d.h: the head number of SOFT block.
dh: the channel dimension of each head and dh=de/h.
n: the input token sequence length of a SOFT block. m:
the bottleneck token sequence length of SOFT block.
sp: the sampling ratio of token sequence length sam-
pling, which is the ratio between input token sequence
length and the bottleneck token sequence length. e: the
expansion ratio of the 2-layer feed forward block. In
SOFT, for all the stages we set dh= 32, e= 4 and
m= 49, spvaries in each stage according to the in-
put token sequence length. Table 1 details the family of
our SOFT configurations with varying capacities (depth
and width).

--- PAGE 8 ---
8 Jiachen Lu1, et al.
Table 1 Architecture specifications of SOFT variants. sp.: sampling ratio. -d: the hidden dimension. -h: the number of heads
in the self-attention block. C33-BN-ReLU : three 3x3 Conv-BN-ReLU, with the stride of 2, 1, 2 respectively. C31-BN-ReLU :
one 3x3 Conv-BN-ReLU, with a stride of 2.
Tiny Small Medium Large
Stage 1C33-BN-ReLU, 64-d
sp. 8x8,
64-d, 2-h
x 2
sp. 8x8,
96-d, 3-h
x 2
sp. 8x8,
96-d, 3-h
x 2
sp. 8x8,
128-d, 4-h
x 2
Stage 2C31-BN-ReLU, 128-d
sp. 4x4,
128-d, 4-h
x 2
sp. 4x4,
192-d, 6-h
x 2
sp. 4x4,
192-d, 6-h
x 2
sp. 4x4,
256-d, 8-h
x 2
Stage 3C31-BN-ReLU, 256-d
sp. 2x2,
320-d, 10-h
x 5
sp. 2x2,
384-d, 12-h
x 5
sp. 2x2,
384-d, 12-h
x 18
sp. 2x2,
512-d, 16-h
x 18
Stage 4
w. cls tokenC31-BN-ReLU, 512-d
sp. 1x1,
512-d, 16-h
x 2
sp. 1x1,
768-d, 24-h
x 2
sp. 1x1,
768-d, 24-h
x 2
sp. 1x1,
1024-d, 32-h
x 2
Table 2 Comparison of different linear/efficient transformer variants on ImageNet (Deng et al., 2009), based on our multi-
stage Tiny configuration (see Table 1). The memory usage is measured with the batch size of 1024 which is our standard
training setting. Transformer is tested at a batch size of 256, which is the maximal number possible with the GPU resource at
our disposal. Throughput is in format as Train throughput /inference throughput.
Methods Memory Params FLOPs Throughput (img/s) Top-1 %
Transformer (Vaswani et al., 2017) 19.0GB † 13M 3.9G 1073 / 3240 80.0
Linformer (Wang et al., 2020) 11.7GB 13M 1.9G 2767 / 3779 78.2
Performer (Choromanski et al., 2021) 15.0GB 13M 2.2G 2037 / 3657 76.1
Nystr¨ omformer (Xiong et al., 2021) 17.2GB 13M 2.0G 1891 / 3518 80.1
SOFT 15.8GB 13M 1.9G 1730 / 3436 80.9
SOFT++ 15.8GB 13M 1.9G 1730 / 3436 80.9
4 Experiments
4.1 Image classification
Dataset: We evaluate the proposed SOFT and SOFT++
on the ILSVRC-2012 ImageNet-1K dataset (Deng et al.,
2009) with 1.28M training images and 50K validation
images from 1,000 classes. Following the common prac-
tice, we train a model on the training set and evaluate
on the validation set.
Metrics: For model performance, the top-1 accuracy
on a single crop is reported. To assess the cost-effectiveness,
we also report the model size and floating point opera-
tions ( i.e., FLOPs).
Implementation details: We use the code base (Wight-
man, 2019) with the default setting to train and test all
the models. Specifically, we use weight decay of 0.05 and
10 epochs of linear warm-up. We conduct 300 epochs
training with an AdamW optimizer and decreasing learn-ing rate with the cosine annealing schedule. During
training, random flipping, mixup (Zhang et al., 2018)
and cutmix (Yun et al., 2019) are adopted for data aug-
mentation. Label smoothing (Szegedy et al., 2016) is
used for loss calculation. All our variants are trained
with a batch size of 1024 on 32G NVIDIA V100 GPUs.
We also implement our method using the Mindspore
(Mindspore, 2020).
Comparison with existing linear Transformers:
We compare our method with three existing linear Trans-
former models: Linformer (Wang et al., 2020), Performer
(Choromanski et al., 2021), Nystr¨ omformer (Xiong et al.,
2021) in terms of model complexity and accuracy.
Two experimental settings are adopted. Under the
first setting, for all methods we use the same Tiny (Ta-
ble 1) architecture for a fair comparison. That is, we re-
place the core self-attention block in SOFT/SOFT++
with each baseline’s own attention block with the rest
of the architecture unchanged. Note that the spatial re-

--- PAGE 9 ---
Softmax-free Linear Transformers 9
Table 3 Evaluation results on ILSVRC-2012 ImageNet-1K (Deng et al., 2009) validation set. We report the results using
the input size of 224x224 pixels center cropped from resized images with 256x256 pixels. M.S.Out.stands for whether the model
is designed for multi-scale output. †: Corrected FLOPs by taking into account the cost of attention matrix multiplication
overlooked in the origin paper.
Model Style Resolution M.S. Out.? Params FLOPs Top-1 %.
ResNet-18 (He et al., 2016) Convolution 2242✓ 11M 1.9G 69.8
PVT-Tiny (Wang et al., 2021) Transformers 2242✓ 13M 1.9G † 75.1
ConViT-Ti+ (d’Ascoli et al., 2021) Transformers 2242% 10M 2.0G 76.7
Coat-Lite Mini (Xu et al., 2021) Transformers 2242✓ 11M 2.0G 78.9
LambdaNets-50 (Bello, 2021) Transformers 2242✓ 16M - 78.9
ViP-Ti (Sun et al., 2021) Transformers 2242✓ 13M 1.7G 79.0
SOFT-Tiny SOFT 2242✓ 13M 1.9G 80.9
SOFT++-Tiny SOFT++ 2242✓ 13M 1.9G 80.9
ResNet-50 (He et al., 2016) Convolution 2242✓ 25M 4.1G 78.5
PVT-Small (Wang et al., 2021) Transformer 2242✓ 24M 4.0G † 79.8
Swin-T (Liu et al., 2021) Transformer 2242✓ 29M 4.5G 81.3
Twins-SVT-S (Chu et al., 2021) Hybrid 2242✓ 24M 3.7G 81.7
CoAtNet-0 (Dai et al., 2021) Hybrid 2242✓ 25M 4.2G 81.6
DeiT III-S (Touvron et al., 2022) Transformer 2242% 22M 4.6G 81.4
SwinV2-T (Liu et al., 2022a) Transformer 2562✓ 29M 4.5G 81.7
ConvNext-T (Liu et al., 2022b) Hybrid 2242✓ 29M 4.5G 82.1
SOFT-Small SOFT 2242✓ 27M 4.5G 82.5
SOFT++-Small SOFT++ 2242✓ 27M 4.5G 82.6
ResNet-101 (He et al., 2016) Convolution 2242✓ 44M 7.9G 79.8
PVT-Medium (Wang et al., 2021) Transformer 2242✓ 44M 7.0G † 81.2
ViT-Small/16 (Dosovitskiy et al., 2021) Transformer 2242% 48M 9.9G 80.8
Swin-S (Liu et al., 2021) Transformer 2242✓ 50M 8.7G 83.0
ConvNext-S (Liu et al., 2022b) Hybrid 2242✓ 50M 8.7G 83.1
CoAtNet-1 (Dai et al., 2021) Transformer 2242✓ 42M 8.4G 83.3
SwinV2-S (Liu et al., 2022a) Transformer 2562✓ 50M 8.7G 83.6
SOFT-Medium SOFT 2242✓ 48M 8.7G 83.2
SOFT++-Medium SOFT++ 2242✓ 48M 8.7G 83.7
CaiT-S36(Touvron et al., 2021b) Transformer 2242✓ 88M 13.9G 83.3
Swin-B(Liu et al., 2021) Transformer 2242✓ 88M 15.4G 83.3
Twins-SVT-L (Chu et al., 2021) Hybrid 2242✓ 99M 14.8G 83.3
DeiT III-B (Touvron et al., 2022) Transformer 2242% 87M 15.5G 83.8
ConvNext-S (Liu et al., 2022b) Hybrid 2242✓ 89M 15.4G 83.8
CoAtNet-2 (Dai et al., 2021) Hybrid 2242✓ 75M 15.7G 84.1
SwinV2-B (Liu et al., 2022a) Transformer 2562✓ 88M 15.4G 84.1
SOFT-Large SOFT 2242✓ 85M 15.4G 83.6
SOFT++-Large SOFT++ 2242✓ 85M 15.4G 84.1
duction module of (Wang et al., 2021) is a special case
of Linformer (Wang et al., 2020). We set the reduction
ratio to be identical to ours. With the same uniformsampling idea, we replace the 1D window averaging of
Nystr¨ omformer (Xiong et al., 2021) (for NLP tasks)
with 2D average pooling (for images). The downsam-

--- PAGE 10 ---
10 Jiachen Lu1, et al.
Table 4 Object detection (RetinaNet (Lin et al., 2017)) results on COCO (Lin et al., 2014) val2007 . We report the results by
training 12 epoch (1 ×) schedule. #Pstands for parameter size. APbrepresents bounding box AP. Note, SOFT cannot converge
in training.
BackboneRetinaNet 1 ×
#P AP AP 50 AP75 APS APM APL
ResNet18 (He et al., 2016) 21M 31.8 49.6 33.6 16.3 34.3 43.2
PVT-Tiny (Wang et al., 2021) 23M 36.7 56.9 38.9 22.6 38.8 50.0
PVTv2-B1 (Wang et al., 2022) 23M 41.2 61.9 43.9 25.4 44.5 54.3
SOFT++-Tiny 23M 41.9 62.7 44.7 27.8 45.4 55.6
ResNet50 (He et al., 2016) 38M 36.3 55.3 38.6 19.3 40.0 48.8
PVT-Small (Wang et al., 2021) 34M 40.4 61.3 43.0 25.0 42.9 55.7
ViL-S (Zhang et al., 2021) 35M 41.6 62.5 44.1 24.9 44.6 56.2
Swin-T(Liu et al., 2021) 38M 41.7 61.2 43.2 26.0 44.3 54.5
SOFT++-Small 38M 43.7 64.9 46.8 28.7 47.4 57.6
ResNet101 (He et al., 2016) 57M 38.5 57.8 41.2 21.4 42.6 51.1
PVT-Medium (Wang et al., 2021) 54M 41.9 63.1 44.3 25.0 44.9 57.6
ViL-M (Zhang et al., 2021) 50M 42.9 64.0 45.4 27.0 46.1 57.2
Swin-S(Liu et al., 2021) 60M 43.0 63.8 45.7 27.1 46.9 57.2
SOFT++-Medium 59M 44.3 64.7 47.4 29.0 48.2 59.9
ResNeXt101 (Xie et al., 2017) 95M 41.0 60.9 44.0 23.9 45.2 54.0
PVT-Large (Wang et al., 2021) 71M 42.6 63.7 45.4 25.8 46.0 58.4
Swin-B (Liu et al., 2021) 98M 44.7 65.9 49.2 - - -
SOFT++-Large 98M 47.0 67.8 50.4 30.2 50.9 62.0
200 300 400 500 600 700 800
Throughput (image/s)81.381.782.182.583.083.584.0Imagenet T op-1 Accuracy (%)
Swin-TSwin-SSwin-B
ConvNeXt-TConvNeXt-SConvNeXt-B
Soft-SmallSOFT-MediumSOFT-Large
Fig. 3 A comparison of Top-1 classification accuracy on the
ImageNet validation set (Deng et al., 2009) with respect to
inference throughput for various models. Our comparison in-
cludes CNN models such as ConvNext (Liu et al., 2022b),
as well as Transformer models like Swin (Liu et al., 2021).
In this comparison, models positioned closer to the top-right
indicate superior performance, balancing both accuracy and
throughput effectively. Inference throughput is measured on
a V100 GPU, following (Liu et al., 2021, 2022b).pling ratio remains identical to ours. It is also worth
mentioning that there is no official code released for Re-
former (Kitaev et al., 2020) and the local Sensitive Hash
(LSH) module has strict requirements on the length of
input tokens. We thus do not include this method in
our comparison.
From Table 3.4 we can make the following obser-
vations: (i) Linear Transformer methods substantially
reduce the memory and FLOPs while maintain similar
parameter size comparing to the Transformer on the
Tiny architecture; (ii) Our approach SOFT/SOFT++
achieve the best classification accuracy among all the
linearization methods. (iii) Our inference speed is on-
par with other compared linear Transformers and our
training speed is slightly slower than Nystromformer
and both are slower than Performer and Linformer.
Note that the slow training speed of our model is mostly
due to the Newton-Raphson iteration which can only be
applied sequentially for ensuring the accuracy of Moore-
Penrose inverse. In summary, due to the on-par infer-
ence speed we consider the training cost increase is a
price worth paying for our superior accuracy.
Under the second setting, we focus on the mem-
ory efficiency of SOFT against the baselines. Here we

--- PAGE 11 ---
Softmax-free Linear Transformers 11
Table 5 Instance segmentation (Mask R-CNN (He et al., 2017)) results on COCO (Lin et al., 2014) val2007 . We report the
results by training 12 epoch (1 ×) schedule. #Pstands for parameter size. APmrepresents mask AP respectively. Note, SOFT
cannot converge in training.
BackboneMask R-CNN 1 ×
#P APbAPb
50 APb
75 APmAPm
50 APm
75
ResNet18 (He et al., 2016) 31M 34.0 54.0 36.7 31.2 51.0 32.7
PVT-Tiny (Wang et al., 2021) 33M 36.7 59.2 39.3 35.1 56.7 37.3
PVTv2-B1 (Wang et al., 2022) 33M 41.8 64.3 45.9 38.8 61.2 41.6
SOFT++-Tiny 32M 41.2 63.7 44.7 38.2 61.0 41.0
ResNet50 (He et al., 2016) 44M 38.0 58.6 41.4 34.4 55.1 36.7
PVT-Small (Wang et al., 2021) 44M 40.4 62.9 43.8 37.8 60.1 40.3
ViL-S (Zhang et al., 2021) 45M 41.8 64.1 45.1 38.5 61.1 41.4
Swin-T(Liu et al., 2021) 48M 42.7 65.2 46.8 39.3 62.2 42.2
SOFT++-Small 48M 43.8 66.0 47.5 40.1 63.0 43.0
ResNet101 (He et al., 2016) 63M 40.4 61.1 44.2 36.4 57.7 38.8
PVT-Medium (Wang et al., 2021) 64M 42.0 64.4 45.6 39.0 61.6 42.1
ViL-M (Zhang et al., 2021) 60M 43.4 65.9 47.0 39.7 62.8 42.1
Swin-S(Liu et al., 2021) 69M 45.6 67.4 50.0 41.2 64.5 44.3
SOFT++-Medium 69M 46.6 67.8 51.2 42.0 64.8 45.2
ResNeXt101 (Xie et al., 2017) 101M 42.8 63.8 47.3 38.4 60.6 41.3
PVT-Large (Wang et al., 2021) 81M 42.9 65.0 46.6 39.5 61.9 42.5
Swin-B (Liu et al., 2021) 107M 45.5 - - 41.3 - -
SOFT++-Large 106M 47.0 68.3 51.7 42.2 65.2 45.4
follow the ViT (Dosovitskiy et al., 2021) network struc-
ture, stacking 12 attention layers with hidden dimen-
siond= 384, heads h= 12, bottleneck token sequence
length m= 49. Different attention blocks from the
three linearized Transformer variants, Linformer (Wang
et al., 2020), Performer (Choromanski et al., 2021),
and Nystr¨ omformer (Xiong et al., 2021) are studied.
For each Transformer variant, we adjust its token se-
quence length nin a linear increment. Specifically, we
use a token sequence length of 784 ×pwhere p=
1,2,3,4,5,6,7,8 and set batch size 1 to verify whether
the memory consumption increases “quadratically” or
“linearly”. Figure 1b shows all compared transformer
variants including our SOFT indeed have a linear mem-
ory usage complexity. This is in contrast with the stan-
dard Transformer which cannot cope with long token
sequences with a quadratic complexity.
Comparison with state-of-the-art CNNs and ViTs:
We compare with state-of-the-art alternatives and re-
port the top-1 accuracy on the ImageNet-1K validation
set. FLOPs are calculated at batch size 1. From Figure
1a and Table 3, the following observations are made:
(i) Overall, ViT and its variants yield better classifi-
cation accuracy over CNNs. (ii) We achieve the bestperformance among the recent pure vision Transformer
based methods including ViT (Dosovitskiy et al., 2021)
and DeiT (Touvron et al., 2021a), as well as the state-
of-the-art CNN RegNet (Radosavovic et al., 2020). (iii)
Our SOFT and SOFT++ outperform the most sim-
ilar (in architecture configuration) Transformer coun-
terparts PVT (Wang et al., 2021) at all variants. Since
the attention module is the main difference, this vali-
dates directly the effectiveness of our model. (iv) We
can also beat the latest ViT variants Twins (Chu et al.,
2021) which is designed to address the efficiency limita-
tion of ViT. We have done so with less parameters and
fewer float point computation.
To gain insights into how attention is learned us-
ing our SOFT and the alternatives, Figure 4 shows the
attention masks. For each model, we visualize the first
two attention heads. It is evident that SOFT exhibits
robustness and versatility in capturing local and long
distance relations among pixels. We note that, although
SOFT is trained for object categorization (ImageNet
(Deng et al., 2009)), it seems to be able to learn both
semantic concepts shared across instances in the same
category and instance specific features. For instance, in
the bottom-right example of a bird class, one attention

--- PAGE 12 ---
12 Jiachen Lu1, et al.
Fig. 4 Comparison of attention heatmaps for a selected query patch (indicated by a cross ”+”) against all patches in an
image. Heatmaps are derived from the first head’s corresponding row in the attention maps, as calculated by Equation 17.
These heatmaps are normalized to a 0-1 scale, with warmer colors indicating higher relevance. The model variants compared
are:(a)Transformer (Vaswani et al., 2017), (b)Performer (Choromanski et al., 2021), (c)Nystromformer (Xiong et al., 2021),
and(d)Our SOFT approach. For additional examples, refer to Appendix E.
head focuses on the black bird only, while the other at-
tends to both birds in the image. More examples are
given in Appendix E.
Throughputs comparison: Figure 3 illustrates that
our model achieves a superior balance between speed
and performance compared to the Swin Transformer
(Liu et al., 2021), and maintains a comparable balance
with the current CNN state-of-the-art, ConvNext (Liu
et al., 2022b). As detailed in Table 3.4, we employ a
series-computed Newton iteration method to ensure nu-
merical accuracy. While this approach slightly reduces
speed, it preserves the accuracy of our model.
Comparison between SOFT and SOFT++ As
shown in Table 3, SOFT++ performs equally well as
SOFT for small models, and outperforms the prelim-
inary version for larger models. Further, Table 6 vali-
dates the superiority of SOFT++ over SOFT in pro-
cessing long token sequences in model deployment.
4.2 Object detection on COCO
Dataset: We evaluate the object detection perfor-
mance of our SOFT++ on the COCO benchmark (Lin
et al., 2014) including the train2017 (118k images) and
val2017 (5k images) sets.
Implementation details: We consider two represen-
tative detectors: RetinaNet (Lin et al., 2017) and Mask
R-CNN (He et al., 2017). We use AdamW optimizer with
base learning rate of 1 ×10−4and weight decay of 0 .01.
We train all the model with batch size 16 on 8 V100
GPUs. Following the practices of MMDetection (Chen
et al., 2019), we adopt the 1 ×and 3×training sched-
ules. In the training stage without multi-scale, images
are resized to make the shorter sides at 800 pixels and
the longer sides no exceeding 1333 pixels. In the train-ing stage with multi-scale, the shorter sides of images
are randomly resized to between 480 to 800.
Comparison with state-of-the-art CNNs and ViTs:
We compare with the state-of-the-art alternatives on
RetinaNet (Lin et al., 2017) in Table 4 and Mask R-
CNN (He et al., 2017) in Table 5. Our SOFT++ out-
performs the CNN ResNet (He et al., 2016) and Trans-
former counterparts PVT (Wang et al., 2021) across
all complexity groups in both object detection and in-
stance segmentation, validating the superior trade-off
between model complexity and performance by our de-
sign.
4.3 Semantic segmentation
Dataset: We evaluate the semantic segmentation per-
formance of our SOFT++ on ADE20K (Zhou et al.,
2019) and Cityscapes (Cordts et al., 2016).
Implementation details: UperNet (Xiao et al., 2018)
is used as the framework. AdamW optimizer with 6 ×10−4
learning rate is applied to train ADE20K for 160k it-
erations and Cityscapes for 40k iterations. Images are
cropped as 512 ×512 for ADE20K and 768 ×768 for
Cityscapes during training. Multi-scale training and test
time augmentation are not used.
Comparison with state-of-the-art CNNs and ViTs:
Table 7 shows that SOFT++ surpasses ResNet clearly,
and the newest designed vision transformer Swin (Liu
et al., 2021) and CovNext (Liu et al., 2022b) slightly
under the similar parameters and float point computa-
tion cost.
4.4 Ablation studies
Pyramidal architecture: Unlike the earlier non-pyramidal
vision Transformers ( e.g., ViT (Dosovitskiy et al., 2021)),

--- PAGE 13 ---
Softmax-free Linear Transformers 13
Table 6 The evaluation results on the validation set of ImageNet-1K (Deng et al., 2009) at various input sizes. The models
were trained on the size of 224 ×224. The down-sampling ratio of the patch embedding remains at 4.
Test input size 224 ×224 384 ×384 512 ×512
Token sequence length 3136 9216 16384
SOFT 82.5 33.7 0.14
SOFT++ 82.6 82.6 80.2
Table 7 Semantic segmentation performance using our HLG Transformer with UperNet (Xiao et al., 2018) and SETR-PUP on
Cityscapes validation set. Single-scale inference and 40k training schedules are used. Note, SOFT cannot converge in training.
Method Backbone #PCityscapes ADE20K
FLOPs mIoU FLOPs mIoU
FCN (Long et al., 2015) ResNet-101 68M 619G 76.6 276G 39.9
PSPNet (Zhao et al., 2017) ResNet-101 68M 576G 78.5 256G 44.4
DLabV3+ (Chen et al., 2018) ResNet-101 62M 571G 79.3 262G 46.9
CCNet (Huang et al., 2019) ResNet-101 68M 625G 80.2 278G 43.7
SETR (Zheng et al., 2021) ViT-Large 318M 1340G 79.2 602G 48.5
OCRNet (Yuan et al., 2020) HRNet-W48 70M 972G 81.1 164G 43.2
UperNet (Xiao et al., 2018) Swin-T 60M - - 236G 44.5
UperNet (Xiao et al., 2018) Swin-S 81M - - 259G 47.6
UperNet (Xiao et al., 2018) Swin-B 121M - - 297G 48.1
UperNet (Xiao et al., 2018) ConvNext 60M - - 235G 46.0
UperNet (Xiao et al., 2018) ConvNext 82M - - 256G 48.7
UperNet (Xiao et al., 2018) ConvNext 122M - - 293G 49.1
UperNet (Xiao et al., 2018) SOFT++-Small 60M 545G 81.2 237G 46.5
UperNet (Xiao et al., 2018) SOFT++-Med 81M 607G 82.0 260G 48.9
UperNet (Xiao et al., 2018) SOFT++-Large 121M 899G 82.6 301G 49.2
most recent pyramidal (multi-scale) Transformers ( e.g.,
PVT (Wang et al., 2021)) use convolution layers to re-
duce the spatial resolution ( i.e., token sequence length)
between stages. In this study, we ablate SOFT++ with
a pyramidal architecture (our default SOFT++- Small ),
SOFT++ w/o a pyramidal architecture and DeiT-S
(Touvron et al., 2021a) (no pyramidal architecture ei-
ther). We replace the Transformer layer with a SOFT++
layer to get SOFT++ w/o a pyramidal architecture.
Note all three variants have similar parameters and
FLOPs. Table 11a shows that the conv-based pyramidal
architecture is clearly superior to a non-pyramidal de-
sign, and our non-pyramidal counterpart is even slightly
better than DeiT-S (Touvron et al., 2021a) whilst en-
joying linear complexity.
Symmetric kernels: Tsai et al. (Tsai et al., 2019)
conducted a comprehensive analysis of Transformers,
revealing minimal performance disparities between asym-
metric and symmetric kernels in NLP tasks, such as
neural machine translation (NMT) and sequence pre-diction (SP). In contrast, our research, detailed in Ta-
ble 8, extends this investigation to the domain of com-
puter vision, specifically focusing on image classification
tasks. Our findings indicate that while symmetric ker-
nels do have a marginal detrimental effect in computer
vision tasks, this impact is relatively limited. Therefore,
maintaining a symmetric kernel represents a balanced
approach for enabling effective decomposition in our
model.
Bottleneck token sequence length: In this study,
we examine how the bottleneck token sequence length
m, sampled from ntokens, influences the model’s per-
formance. We change the bottleneck token sequence
length in all stages to 36 ,49,64,81. Table 9 shows that
longer bottleneck token would increase the memory cost
and the computational overhead. m= 49 seems to give
the best trade-off between the performance and compu-
tational overhead. The memory usage is measured with
the batch size of 128.

--- PAGE 14 ---
14 Jiachen Lu1, et al.
Table 8 Ablations on assessing the Impact of Symmetric Kernels on ImageNet-1K (Deng et al., 2009) image classification. As
baselines for our study, we employ two of the most common backbones: ViT-small/16 (Dosovitskiy et al., 2021) and Swin-T (Liu
et al., 2021).
Model Asymmetric ( Q̸=K) Symmetric ( Q=K)
ViT-Small/16 (Dosovitskiy et al., 2021) 80.8 80.5
Swin-T (Liu et al., 2021) 81.3 80.9
Table 9 Ablations on bottleneck token sequence length.
Bottleneck Memory FLOPs Top-1 %
36 15.1GB 1.9G 80.5
49 15.8GB 1.9G 80.9
64 16.9GB 2.0G 80.9
81 18.5GB 2.1G 80.4
Table 10 Ablations on sampling methods.
Sampling methods Params FLOPs Top-1 %
Convolution 13.07M 2.0G 80.9
Random sampling 12.96M 1.9G 80.8
Biased sampling 12.96M 1.9G 79.9
Average pooling 12.96M 1.9G 80.8
Table 11 (a) Ablations on pyramidal architecture. (b) Ablations on overlapped convolution.
Methods Pyramidal? Top-1 % Methods Overlapped? Top-1 %
DeiT-S (Touvron et al., 2021a) % 79.8 PVT (Wang et al., 2021) % 75.1
SOFT++ % 80.1 SOFT++ % 78.4
SOFT++ ✓ 82.4 SOFT++ ✓ 80.9
Token sampling: The sampling function in SOFT++
can assume different forms. Convolution: The sequence
Q∈Rn×deis first reshaped to a feature map RH×W×de.
r×rconvolution kernel with stride of ris applied for
downsampling, where r=√sp. The output channel size
is also kept and no bias is used. At last, the feature map
is reshaped back to the sequence. Average pooling:
using a r×rkernel and rstride, where r=√sp.Ran-
dom sampling: mtokens are randomly picked from n
tokens. Biased sampling: We pick mtokens with a
biased policy. Here, the first mtokens are picked. Table
10 shows that both convolution yields the best perfor-
mance. Biased sampling can miss the most salient sam-
ples, and there is no guarantee that random sampling
can keep the uniformity of the chosen samples. This
result thus justifies the choice of using convolution in
SOFT++.
Overlapped convolution: We ablate SOFT++ with
overlapped convolution (our default choice, same as manyrecent works) and with non-overlapped convolution in
ourTiny configuration. Table 11b shows that overlapped
convolution is a better choice. Our non-overlapped con-
volution variant still outperforms PVT (Wang et al.,
2021) with the same non-overlapped convolution by a
clear margin.
Newton-Raphson’s convergence: We study how many
iterations the Newton-Raphson method needs to con-
verge when computing the Moore-Penrose inverse A†.
We use ∥AAkA−A∥p/∥A∥pwithp= 2 (see Proposition
2) as the convergence metric to quantify the difference
between AkandA†. Figure 5 shows that our approxi-
mation converges within 20 iterations across all stages.
Effect of attention normalization: Our attention
normalization enables the model to perform more chal-
lenging vision tasks such as object detection and seg-
mentation. This is because the leading eigenvalue of
Gaussian kernel self-attention is conditioned on the to-
ken sequence length (Proposition 4), as shown in Figure

--- PAGE 15 ---
Softmax-free Linear Transformers 15
0 5 10 15 20 25 30 35
Iterations0.00.10.20.30.40.50.60.70.80.9||AAkAA||2/||A||2
ImageNet
0 5 10 15 20 25 30 35
Iterations0.00.10.20.30.40.50.60.70.80.9COCO
0 5 10 15 20 25 30 35
Iterations0.00.10.20.30.40.50.60.70.80.9ADE20K
Fig. 5 Convergence analysis for the approximation of Moore-Penrose inverse on ImageNet, COCO and ADE20K separately.
SOFT-Tiny is used. We measure ∥AAkA−A∥p/∥A∥2for 100 input images on each dataset. The solid line shows the average
convergence metric, while the shallow area indicates the upper bound and lower bound.
Table 12 Comparison of different linear/efficient Transformer variants on Long Range Arena (Tay et al., 2020), based on its
official configuration. Our SOFT surpasses previous efficient methods on three tasks.
Methods Listops(2K) Text(4K) Retrieval(4K) Image(1K) Avg. %
Transformer (Vaswani et al., 2017) 37.10 65.02 79.35 38.20 54.92
Reformer (Kitaev et al., 2020) 19.05 64.88 78.64 43.29 51.47
Linformer (Wang et al., 2020) 37.25 55.91 79.37 37.84 52.59
Performer (Choromanski et al., 2021) 18.80 63.81 78.62 37.07 49.58
Nystr¨ omformer (Xiong et al., 2021) 37.15 65.52 79.56 41.58 55.95
SOFT 37.40 63.49 81.77 46.91 57.39
6a. And this effect can be clearly mitigated by our nor-
malization scheme, as indicated in Figure 6b.
4.5 Additional experiments on NLP
tasks
We compare our method with previous linear counter-
parts on four tasks of the Long Range Arena (LRA)
(Tay et al., 2020) benchmark, including Listops (Nan-
gia and Bowman, 2018), byte-level IMDb reviews text
classification (Maas et al., 2011), byte-level document
retrieval (Radev et al., 2013), and image classification
on sequences of pixels (Krizhevsky, 2009).
Implementations. We use the Pytorch version of LRA
(Tay et al., 2020) benchmark, provided by (Xiong et al.,
2021). For the evaluation protocol, we strictly follow
(Tay et al., 2020, Xiong et al., 2021). We omit the
Pathfinder(1K) task as we cannot replicate the result
of Nystr¨ omformer (Xiong et al., 2021). For our SOFT,
we simply use the average pooling with window size 4,
stride 4 to sample the bottlenecks. We follow the config-
urations of (Xiong et al., 2021) with 2 layers, 64 and 128
hidden dimension respectively, and 2 attention heads.
Table 12 shows that our SOFT outperforms both thestandard and alternative efficient Transformers on three
out of four tasks, as well as the average performance.
5 Conclusions
In this work, we have introduced a novel softmax-free
self-attention (SOFT) mechanism for linearizing Trans-
former’s complexity in space and time. Unlike exist-
ing linear Transformers that aim to approximate the
conventional softmax based self-attention, SOFT em-
ploys a Gaussian kernel based attention which elimi-
nates the need for softmax normalization. This design
enables a full self-attention matrix to be approximated
via low-rank matrix decomposition. The robustness of
this proposed approximation is achieved by calculat-
ing its Moore-Penrose inverse using a Newton-Raphson
method and an efficient symmetric attention normal-
ization. Extensive experiments show that SOFT yields
superior trade-off in accuracy and complexity on a va-
riety of vision and language tasks.
Acknowledgements This work was supported in part by
STI2030-Major Projects (Grant No. 2021ZD0200204), Na-
tional Natural Science Foundation of China (Grant No. 62106050
and 62376060), Natural Science Foundation of Shanghai (Grant

--- PAGE 16 ---
16 Jiachen Lu1, et al.
0 200 400 600 800 1000
Eigenvalue index0100200300400500600Eigenvalue
(a)
100 200 300 400 500 600 700 800
Bottleneck token sequence length0123456Spectral norm1e8
Without attention normalization
With attention normalization (b)
Fig. 6 (a) Eigenvalues of a specific bottleneck matrix A∈R1024×1024. The auxiliary (red dash) line connects all the eigen-
values. (b) Comparing the spectral norm of self-attention matrix A†(without normalization, blue) and D−1/2A†D−1/2(with
normalization, orange) under a variety of bottleneck token sequence lengths. “+” represents individual samples; The lines
connect the average norm at each token sequence length.
No. 22ZR1407500) and Lingang Laboratory (Grant No. LG-
QS-202202-07).
Data availability statement The datasets generated dur-
ing and/or analysed during the current study are available in
the Imagenet (Deng et al., 2009) ( https://www.image-net.
org/), COCO (Lin et al., 2014) ( https://cocodataset.org ),
ADE20K (Zhou et al., 2019) ( https://groups.csail.mit.
edu/vision/datasets/ADE20K/ ), Cityscapes (Cordts et al.,
2016) ( https://www.cityscapes-dataset.com ), Long Range
Arena (Tay et al., 2020) ( https://github.com/google-research/
long-range-arena ) repositories.
A Nystr¨ om method
Nystr¨ om method Williams and Seeger (2000) aims to calcu-
late a low-rank approximation for a Gram matrix. For Trans-
formers, the self-attention matrix can be viewed as a Gram
matrix Swith a Gaussian kernel kapplied to the query Q,
with each element Sijexpressed as:
Sij=k 
Qi,:, Qj,:
= exp( −∥Qi,:−Qj,:∥2
2
2√
d), (18)
k(x, y) means operating Gaussian kernel kto (x, y), which
can be written in the feature space as:
k(x, y) =nX
i=1λiϕi(x)ϕi(y), (19)
nis the dimension of a feature space, λidenotes the eigen-
value and ϕidenotes the eigenfunction of kernel k. According
to the eigenfunction’s definition, we can get:
Z
k(y, x)ϕi(x)p(x)dx=λiϕi(y), (20)where p(x) is the probability distribution of x. And {ϕi(x)}
arep-orthogonal:
Z
ϕi(x)ϕj(x)p(x)dx=δij. (21)
δijis 0 when i̸=j, 1 when i=j. To get an approximation
of the eigenfunctions, we sample {x1, x2,···, xq}from p(x),
then:
1
qqX
t=1k(y, xt)ϕi(xt)≈λiϕi(y), (22)
1
qqX
t=1ϕi(xt)ϕj(xt)≈δij. (23)
This inspires us to approximate the Gram matrix S. Let
S(m)be a submatrix of S, consisting of m×melements from
S. Gram matrix is a symmetric positive semi-definite matrix,
so it has a spectral decomposition:
S(m)U(m)=U(m)Λ(m), (24)
where U(m)is column orthogonal and Λ(m)is a diagonal ma-
trix with the diagonal elements as the eigenvalues of S(m).
Substituting the ytoxjand applying the approximation
above to S, we can get:
ϕi(xj)≈√mU(m)
j,i, λi≈λ(m)
i
m, (25)
ϕi(y)≈√m
λ(m)
imX
t=1k(y, xt)ϕi(xt), (26)
λiis eigenvalue of Sandλ(m)
iis the eigenvalue of S(m).
Denote ˜Sas the rank- mapproximation of Sand ˜U,˜Λas the

--- PAGE 17 ---
Softmax-free Linear Transformers 17
approximation for spectral decomposition of S. Now we can
get an approximation of Swith rank m:
˜S=˜U˜Λ˜UT=mX
t=1˜λt(n)˜u(n)
t(˜u(n)
t)T. (27)
Similarly, we have:
ϕi(xj)≈√nUj,i(n), λi≈˜λi(n)
n. (28)
Thus
˜λi(n)≈nλ(m)
i
m, (29)
˜u(n)
t≈rm
n1
λ(m)
tSn,mu(m)
t. (30)
Then we get an approximation of S:˜S≈Sn,mS†
m,mSm,n.
Shas a block representation below:
S=
Sm,m Sm,n−m
Sn−m,m Sn−m,n−m
. (31)
B Newton method
Proof of Proposition 1 When αis sufficiently small, Ak+1=
2Ak−AkAAk,Akconverges to A†.
Proof Ais a symmetric positive semi-definite matrix and
Aij≤1,∀1≤i, j≤n,Aii= 1,1≤i≤nin our
case. A0is chosen to be αA, so the Akcan be written as
Ak=CkA=ADkfor some matrix Ck, Dk, leading to the
fact that
A†AAk=Ak, A kAA†=Ak. (32)
This is because Ak+1=Ak(2In−AAk) = (2 In−AkA)Ak
andA0=αA. We make a difference between A†andAk+1:
A†−Ak+1=A†−2Ak+AkAAk
=A†−AkAA†−A†AAk+AkAAk
= (A†−Ak)A(A†−Ak). (33)
We norm both sides of the equation above:
∥A†−Ak+1∥=∥(A†−Ak)A(A†−Ak)∥
≤ ∥A†−Ak∥∥A(A†−Ak)∥. (34)
And we left multiply Aon the both sides of (Eq 33), then
norm the equation:
∥AA†−AAk+1∥=∥A(A†−Ak)A(A†−Ak)∥
≤ ∥AA†−AAk∥2. (35)
We choose αsufficiently small so that the initial value sat-
isfy∥AA†−AA0∥<1. We set α=2
∥A∥2
1to ensure it is
small enough Ben-Israel and Cohen (1966). Then the ∥AA†−
AAk∥ → 0, when k→ ∞ . The inequality (34) implies that
Ak→A†, k→ ∞ .
Proof of Equation 12
∇xL=−Y⊤(∇YL)Y⊤,Proof Since XY=I, we take derivatives on the both sides
Y dX +XdY = 0
dY=−Y dXY (36)
For the loss function L, we have
dL=⟨∇XL, dX⟩=⟨∇YL, dY⟩
Recall that the inner product ⟨A, B⟩= Tr( A⊤B) and Tr( ABC ) =
Tr(CAB ) = Tr( BCA ), we have
⟨∇XL, dX⟩=⟨∇YL, dY⟩
=⟨∇YL,−Y dXY ⟩
=−Tr(∇YL⊤Y dXY )
=⟨−Y⊤∇YLY⊤, dX⟩
Therefore, Eq (12) is proved.
C Attention normalization
Proof of Porpostion 5 Assume the bottleneck matrix of
softmax-free attention , A∈Rm×misk-connected. If λ1≥
λ2≥ ··· λm≥0 are eigenvalues of A†, then λ1=O(m2) and
∥A†∥2=O(m2).
To prove Proposition 5, we first introduce the following
lemmas.
Lemma 1 IfFis a non-singular and symmetric matrix,
then∥F∥2≤ ∥F∥1.
Proof According to the definition of 2-Norm, we have FTFz=
µ2zwith µ=∥F∥2. Therefore, µ2∥z∥1=∥FTFz∥2≤
∥FT∥1∥F∥1∥z∥1=∥F∥∞∥∥F1∥z∥1. Since Ais non-singular
and symmetric,
∥F∥2≤p
∥F∥∞∥F∥1=∥F∥1 (37)
Lemma 2 IfFis a real matrix and ∥F∥p<1, then
(I−F)−1=∞X
k=0Fk(38)
with
∥(I−F)−1∥p≤1
1− ∥F∥p(39)
Proof
 NX
k=0Fk!
(I−F) =I−FN+1
Since∥F∥p<1, lim k→∞Fk= 0. Thus,
 
lim
N→∞NX
k=0Fk!
(I−F) =I
Multiply the equation by ( I−F)−1we obtain Eq (38). From
that, we can easily get
∥(I−F)−1∥p≤∞X
k=0∥F∥k
p=1
1− ∥F∥p

--- PAGE 18 ---
18 Jiachen Lu1, et al.
Now we begin to prove Proposition 5.
Proof Since the bottleneck matrix softmax-free attention ∥A∥1
is not less than 1, we normalize the matrix as An=D−1A,
where D= diag( A 1m). From Lemma 2, it follows that
∥A−1
n∥1≤1
1− ∥1−An∥1
Since we assume the bottleneck matrix is k-connected, and
k << m ,
∥I−An∥1≤(m−k)
m
Then,
∥A−1
n∥1≤1
1−(m−k)
m=m
k=O(m)
Note that we D=O(m) by assumption that the bottleneck
matrix is k-connected, it follows that
∥A−1∥1=∥A−1
nD∥1≤ ∥A−1
n∥1∥D∥1=O(m2)
D Non-linearized Gaussian kernel
attention
Instead of directly calculating the Gaussian kernel weights, in
our formulation they are approximated. More specifically, the
relation between any two tokens is reconstructed via sampled
bottleneck tokens. As the number m(e.g., 49) of bottleneck
tokens is much smaller than the entire token sequence length,
our attention matrix is of low-rank. This brings about two
favorable consequences: (I)The model now focuses the at-
tentive learning on latent salient information captured by the
mbottleneck tokens. (II) The model becomes more robust
against the underlying token noise due to the auto-encoder
style reconstruction Geng et al. (2021). This explains why
a model with an approximated gram matrix performs bet-
ter than one with a directly computed matrix. Further, we
find that exact Gaussian kernel attention leads to training
difficulties. As Proposition 4 reveals, this is due to lacking
normalization that leads to explosion of the spectral norm
especially in long token sequence cases. Big spectral norm
could jeopardize the training and tend to collapse the model.
E Attention visualization
Figure 7 shows more visualization of the attention masks
by various Transformers Choromanski et al. (2021), Vaswani
et al. (2017), Xiong et al. (2021) and our SOFT. For each
model, we show the output from the first two attention heads
(up and down row). It is noteworthy that SOFT exhibits
better semantic diversity of the multi-head mechanism than
other methods. Moreover, when we sample the patch at the
boundary of multiple objects, SOFT is able to more precisely
capture all these objects.

--- PAGE 19 ---
Softmax-free Linear Transformers 19
Fig. 7 Comparison of attention heatmaps for a selected query patch (indicated by a cross ”+”) against all patches in an
image. Heatmaps are derived from the first head’s corresponding row in the attention maps, as calculated by Equation 17.
These heatmaps are normalized to a 0-1 scale, with warmer colors indicating higher relevance. The model variants compared
are:(a)Transformer (Vaswani et al., 2017), (b)Performer (Choromanski et al., 2021), (c)Nystromformer (Xiong et al., 2021),
and(d)Our SOFT approach.

--- PAGE 20 ---
20 Jiachen Lu1, et al.
References
Ba JL, Kiros JR, Hinton GE (2016) Layer normaliza-
tion. arXiv preprint arXiv:160706450
Bello I (2021) Lambdanetworks: Modeling long-range
interactions without attention. In: International Con-
ference on Learning Representations
Ben-Israel A, Cohen D (1966) On iterative computa-
tion of generalized inverses and associated projec-
tions. SIAM Journal on Numerical Analysis 3(3):410–
419
Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD,
Dhariwal P, Neelakantan A, Shyam P, Sastry G,
Askell A, et al. (2020) Language models are few-shot
learners. Advances in Neural Information Processing
Systems 33:1877–1901
Chen K, Wang J, Pang J, Cao Y, Xiong Y, Li X, Sun
S, Feng W, Liu Z, Xu J, et al. (2019) Mmdetec-
tion: Open mmlab detection toolbox and benchmark.
arXiv preprint arXiv:190607155
Chen LC, Zhu Y, Papandreou G, Schroff F, Adam H
(2018) Encoder-decoder with atrous separable convo-
lution for semantic image segmentation. In: European
Conference on Computer Vision, pp 801–818
Cheng J, Grossman M, McKercher T (2014) Profes-
sional CUDA c programming
Choromanski K, Likhosherstov V, Dohan D, Song X,
Gane A, Sarlos T, Hawkins P, Davis J, Mohiuddin
A, Kaiser L, et al. (2021) Rethinking attention with
performers. In: International Conference on Learning
Representations
Chu X, Tian Z, Wang Y, Zhang B, Ren H, Wei X,
Xia H, Shen C (2021) Twins: Revisiting the design
of spatial attention in vision transformers. Advances
in Neural Information Processing Systems 34:9355–
9366
Cordts M, Omran M, Ramos S, Rehfeld T, Enzweiler
M, Benenson R, Franke U, Roth S, Schiele B (2016)
The cityscapes dataset for semantic urban scene un-
derstanding. In: IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp 3213–3223
Dai Z, Liu H, Le QV, Tan M (2021) Coatnet: Marry-
ing convolution and attention for all data sizes. In:
Advances in Neural Information Processing Systems,
vol 34, pp 3965–3977
Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009)
Imagenet: A large-scale hierarchical image database.
In: IEEE Conference on Computer Vision and Pat-
tern Recognition, pp 248–255
Devlin J, Chang MW, Lee K, Toutanova K (2019) Bert:
Pre-training of deep bidirectional transformers for
language understanding. In: Conference of the North
American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies,
NAACL-HLT, pp 4171–4186
Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D,
Zhai X, Unterthiner T, Dehghani M, Minderer M,
Heigold G, Gelly S, et al. (2021) An image is worth
16x16 words: Transformers for image recognition at
scale. In: International Conference on Learning Rep-
resentations
d’Ascoli S, Touvron H, Leavitt ML, Morcos AS, Biroli
G, Sagun L (2021) Convit: Improving vision trans-
formers with soft convolutional inductive biases.
In: International Conference on Machine Learning,
PMLR, pp 2286–2296
Fasshauer GE (2011) Positive definite kernels: past,
present and future. Dolomites Research Notes on Ap-
proximation 4:21–63
Geng Z, Guo MH, Chen H, Li X, Wei K, Lin Z (2021) Is
attention better than matrix decomposition? In: In-
ternational Conference on Learning Representations
Guo MH, Liu ZN, Mu TJ, Hu SM (2022) Beyond self-
attention: External attention using two linear layers
for visual tasks. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 45(5):5436–5447
He K, Zhang X, Ren S, Sun J (2016) Deep residual
learning for image recognition. In: IEEE Conference
on Computer Vision and Pattern Recognition, pp
770–778
He K, Gkioxari G, Doll´ ar P, Girshick R (2017) Mask r-
cnn. In: IEEE International Conference on Computer
Vision, pp 2961–2969
Huang Z, Wang X, Huang L, Huang C, Wei Y, Liu
W (2019) Ccnet: Criss-cross attention for semantic
segmentation. In: IEEE International Conference on
Computer Vision, pp 603–612
Ioffe S, Szegedy C (2015) Batch normalization: Accel-
erating deep network training by reducing internal
covariate shift. In: International Conference on Ma-
chine Learning, pmlr, pp 448–456
Jaegle A, Gimeno F, Brock A, Vinyals O, Zisserman A,
Carreira J (2021) Perceiver: General perception with
iterative attention. In: International Conference on
Machine Learning, PMLR, pp 4651–4664
Kasai J, Peng H, Zhang Y, Yogatama D, Ilharco G,
Pappas N, Mao Y, Chen W, Smith NA (2021) Fine-
tuning pretrained transformers into rnns. In: Con-
ference on Empirical Methods in Natural Language
Processing, pp 10630–10643
Katharopoulos A, Vyas A, Pappas N, Fleuret F (2020)
Transformers are rnns: Fast autoregressive trans-
formers with linear attention. In: International Con-
ference on Machine Learning, PMLR, pp 5156–5165
Kitaev N, Kaiser  L, Levskaya A (2020) Reformer: The
efficient transformer. In: International Conference on

--- PAGE 21 ---
Softmax-free Linear Transformers 21
Learning Representations
Krizhevsky A (2009) Learning multiple layers of
features from tiny images. URL https://api.
semanticscholar.org/CorpusID:18268744
Lin TY, Maire M, Belongie S, Hays J, Perona P, Ra-
manan D, Doll´ ar P, Zitnick CL (2014) Microsoft coco:
Common objects in context. In: European Confer-
ence on Computer Vision, Springer, pp 740–755
Lin TY, Goyal P, Girshick R, He K, Doll´ ar P (2017)
Focal loss for dense object detection. In: IEEE Inter-
national Conference on Computer Vision, pp 2980–
2988
Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S,
Guo B (2021) Swin transformer: Hierarchical vision
transformer using shifted windows. In: Proceedings of
the IEEE/CVF international conference on computer
vision, pp 10012–10022
Liu Z, Hu H, Lin Y, Yao Z, Xie Z, Wei Y, Ning J, Cao Y,
Zhang Z, Dong L, et al. (2022a) Swin transformer v2:
Scaling up capacity and resolution. In: IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pp 12009–12019
Liu Z, Mao H, Wu CY, Feichtenhofer C, Darrell T, Xie
S (2022b) A convnet for the 2020s. In: IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pp 11976–11986
Long J, Shelhamer E, Darrell T (2015) Fully convolu-
tional networks for semantic segmentation. In: IEEE
Conference on Computer Vision and Pattern Recog-
nition, pp 3431–3440
Lu J, Yao J, Zhang J, Zhu X, Xu H, Gao W, Xu C,
Xiang T, Zhang L (2021) Soft: softmax-free trans-
former with linear complexity. Advances in Neural
Information Processing Systems 34:21297–21309
Maas A, Daly RE, Pham PT, Huang D, Ng AY, Potts
C (2011) Learning word vectors for sentiment anal-
ysis. In: Proceedings of the 49th annual meeting of
the association for computational linguistics: Human
language technologies, pp 142–150
Mindspore (2020) https://www.mindspore.cn/
Nangia N, Bowman SR (2018) Listops: A diagnostic
dataset for latent tree learning. In: Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Student Research Workshop,
pp 92–99
Peng H, Pappas N, Yogatama D, Schwartz R, Smith
NA, Kong L (2021) Random feature attention. In: In-
ternational Conference on Learning Representations
Radev DR, Muthukrishnan P, Qazvinian V, Abu-Jbara
A (2013) The acl anthology network corpus. Lan-
guage Resources and Evaluation 47:919–944
Radosavovic I, Kosaraju RP, Girshick R, He K, Doll´ ar
P (2020) Designing network design spaces. In: IEEEConference on Computer Vision and Pattern Recog-
nition, pp 10428–10436
Sun S, Yue X, Bai S, Torr P (2021) Visual parser: Rep-
resenting part-whole hierarchies with transformers.
arXiv preprint arXiv:210705790
Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z
(2016) Rethinking the inception architecture for com-
puter vision. In: IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp 2818–2826
Tay Y, Dehghani M, Abnar S, Shen Y, Bahri D, Pham
P, Rao J, Yang L, Ruder S, Metzler D (2020) Long
range arena: A benchmark for efficient transformers.
In: International Conference on Learning Represen-
tations
Tay Y, Dehghani M, Bahri D, Metzler D (2023) Effi-
cient transformers: A survey. ACM Computing Sur-
veys 55(6):109:1–109:28
Touvron H, Cord M, Douze M, Massa F, Sablayrolles A,
J´ egou H (2021a) Training data-efficient image trans-
formers & distillation through attention. In: Interna-
tional Conference on Machine Learning, PMLR, pp
10347–10357
Touvron H, Cord M, Sablayrolles A, Synnaeve G, J´ egou
H (2021b) Going deeper with image transformers. In:
IEEE Conference on Computer Vision and Pattern
Recognition, pp 32–42
Touvron H, Cord M, J´ egou H (2022) Deit iii: Revenge
of the vit. In: European Conference on Computer Vi-
sion, pp 516–533
Tsai YHH, Bai S, Yamada M, Morency LP, Salakhut-
dinov R (2019) Transformer dissection: A unified un-
derstanding of transformer’s attention via the lens of
kernel. In: Conference on Empirical Methods in Natu-
ral Language Processing and International Joint Con-
ference on Natural Language Processing (EMNLP-
IJCNLP), pp 4344–4353
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones
L, Gomez AN, Kaiser  L, Polosukhin I (2017) Atten-
tion is all you need. Advances in Neural Information
Processing Systems 30
Von Luxburg U (2007) A tutorial on spectral clustering.
Statistics and computing 17:395–416
Wang S, Li BZ, Khabsa M, Fang H, Ma H (2020) Lin-
former: Self-attention with linear complexity. arXiv
preprint arXiv:200604768
Wang W, Xie E, Li X, Fan DP, Song K, Liang D, Lu T,
Luo P, Shao L (2021) Pyramid vision transformer:
A versatile backbone for dense prediction without
convolutions. In: IEEE International Conference on
Computer Vision, pp 568–578
Wang W, Xie E, Li X, Fan DP, Song K, Liang D, Lu
T, Luo P, Shao L (2022) Pvt v2: Improved baselines
with pyramid vision transformer. Computational Vi-

--- PAGE 22 ---
22 Jiachen Lu1, et al.
sual Media 8(3):415–424
Wang X, Girshick R, Gupta A, He K (2018) Non-local
neural networks. In: IEEE Conference on Computer
Vision and Pattern Recognition, pp 7794–7803
Wightman R (2019) Pytorch image models. https://
github.com/rwightman/pytorch-image-models
Williams C, Seeger M (2000) Using the nystr¨ om method
to speed up kernel machines. Advances in Neural In-
formation Processing Systems 13
Xiao T, Liu Y, Zhou B, Jiang Y, Sun J (2018) Unified
perceptual parsing for scene understanding. In: Eu-
ropean Conference on Computer Vision, pp 418–434
Xie S, Girshick R, Doll´ ar P, Tu Z, He K (2017) Aggre-
gated residual transformations for deep neural net-
works. In: IEEE Conference on Computer Vision and
Pattern Recognition, pp 1492–1500
Xiong Y, Zeng Z, Chakraborty R, Tan M, Fung G,
Li Y, Singh V (2021) Nystr¨ omformer: A nystr¨ om-
based algorithm for approximating self-attention. In:
AAAI Conference on Artificial Intelligence, vol 35,
pp 14138–14148
Xu W, Xu Y, Chang T, Tu Z (2021) Co-scale conv-
attentional image transformers. In: Proceedings of
the IEEE/CVF International Conference on Com-
puter Vision, pp 9981–9990
Yoshida Y, Miyato T (2017) Spectral norm regulariza-
tion for improving the generalizability of deep learn-
ing. arXiv preprint arXiv:170510941
Yuan L, Chen Y, Wang T, Yu W, Shi Y, Jiang ZH, Tay
FE, Feng J, Yan S (2021) Tokens-to-token vit: Train-
ing vision transformers from scratch on imagenet. In:
IEEE International Conference on Computer Vision,
pp 558–567
Yuan Y, Chen X, Wang J (2020) Object-contextual rep-
resentations for semantic segmentation. In: European
Conference on Computer Vision, pp 173–190
Yun S, Han D, Oh SJ, Chun S, Choe J, Yoo Y (2019)
Cutmix: Regularization strategy to train strong clas-
sifiers with localizable features. In: IEEE Interna-
tional Conference on Computer Vision, pp 6023–6032
Zhang H, Ciss´ e M, Dauphin YN, Lopez-Paz D (2018)
mixup: Beyond empirical risk minimization. In: In-
ternational Conference on Learning Representations
Zhang L, Xu D, Arnab A, Torr PH (2020) Dynamic
graph message passing networks. In: IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pp 3726–3735
Zhang P, Dai X, Yang J, Xiao B, Yuan L, Zhang L, Gao
J (2021) Multi-scale vision longformer: A new vision
transformer for high-resolution image encoding. In:
IEEE International Conference on Computer Vision,
pp 2998–3008Zhao H, Shi J, Qi X, Wang X, Jia J (2017) Pyramid
scene parsing network. In: IEEE Conference on Com-
puter Vision and Pattern Recognition, pp 2881–2890
Zhao H, Jia J, Koltun V (2020) Exploring self-attention
for image recognition. In: IEEE Conference on Com-
puter Vision and Pattern Recognition, pp 10076–
10085
Zheng S, Lu J, Zhao H, Zhu X, Luo Z, Wang Y, Fu Y,
Feng J, Xiang T, Torr PH, et al. (2021) Rethinking
semantic segmentation from a sequence-to-sequence
perspective with transformers. In: IEEE Conference
on Computer Vision and Pattern Recognition, pp
6881–6890
Zhou B, Zhao H, Puig X, Xiao T, Fidler S, Barriuso A,
Torralba A (2019) Semantic understanding of scenes
through the ade20k dataset. International Journal of
Computer Vision 127:302–321
Zhu X, Su W, Lu L, Li B, Wang X, Dai J (2021) De-
formable detr: Deformable transformers for end-to-
end object detection. In: International Conference on
Learning Representations
