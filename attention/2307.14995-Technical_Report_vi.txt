**TÀI LIỆU THAM KHẢO**

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. Falcon-40b: một mô hình ngôn ngữ lớn mở với hiệu suất tiên tiến. Báo cáo kỹ thuật, Báo cáo kỹ thuật, Viện Đổi mới Công nghệ, 2023.

Baichuan. Baichuan 2: Các mô hình ngôn ngữ quy mô lớn mở. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/abs/2309.10305.

[Tiếp tục với tất cả các tài liệu tham khảo...]

**PHỤ LỤC**

**A MÔ HÌNH**

Chúng tôi trình bày các biến thể mô hình riêng biệt của kiến trúc TransNormerLLM, mô tả các cấu hình tương ứng của chúng về tham số, lớp, attention head và chiều ẩn. Các đặc tả chi tiết được lập bảng tỉ mỉ trong Bảng 10.

Bảng 10: Các Biến thể Mô hình TransNormerLLM.
| Kích thước Mô hình | Tham số Không-Embedding | Lớp | Chiều Ẩn | Head | Mô hình Tương đương |
|-------------------|-------------------------|-----|-----------|------|-------------------|
| 385M | 384,974,848 | 24 | 1024 | 8 | Pythia-410M |
| 1B | 992,165,888 | 16 | 2048 | 16 | Pythia-1B |
| 3B | 2,876,006,400 | 32 | 2560 | 20 | Pythia-2.8B |
| 7B | 6,780,547,072 | 30 | 4096 | 32 | LLAMA-6.7B |
| 13B | 12,620,195,840 | 36 | 5120 | 40 | LLAMA-13B |
| 65B | 63,528,009,728 | 72 | 8192 | 64 | LLAMA-65B |
| 175B | 173,356,498,944 | 88 | 12288 | 96 | GPT-3 |

**B LIGHTNING ATTENTION**

Chúng tôi trình bày chi tiết thuật toán của Lightning Attention bao gồm forward pass và backward pass trong Thuật toán 3 và 4, tương ứng.

Thuật toán 3 Lightning Attention Forward Pass
Đầu vào: Q,K,V∈Rn×d, attention mask M∈Rn×n, kích thước khối Bc, Br;
Khởi tạo: O=0∈Rn×d;
Chia Q thành Tr=n/Br khối Q1,Q2, ...QTr với kích thước Br×d mỗi khối.
Chia K,V thành Tc=n/Bc khối K1,K2, ...KTc,V1,V2, ...VTc với kích thước Bc×d mỗi khối.
Chia O thành Tr=n/Br khối O1,O2, ...OTr với kích thước Br×d mỗi khối.
Chia M thành Tr×Tc khối M11,M12, ...MTr,Tc với kích thước Br×Bc mỗi khối.
for 1≤i≤Tr do
    Tải Qi∈RBr×d từ HBM đến on-chip SRAM.
    Khởi tạo Oi=0∈RBr×d trên SRAM.
    for 1≤j≤Tc do
        Tải Kj,Vj∈RBc×d từ HBM đến on-chip SRAM.
        Tải Mij∈RBc×Bc từ HBM đến on-chip SRAM.
        Trên chip, tính Aij= [QiK⊤j]⊙Mij∈RBr×Bc.
        Trên chip, tính Oi=Oi+AijVj∈RBr×d.
    end for
    Ghi Oi vào HBM như khối thứ i của O.
end for
return O

**C CHỨNG MINH THUẬT TOÁN SUY LUẬN MẠNH MẼ**

Chúng tôi sẽ sử dụng quy nạp để chứng minh: [kv]t=λ−t[kv]t.

Trường hợp cơ sở (n=1):
[kv]1= ([kv]0+k1λ−1v⊤1)
=λ−1(k1v⊤1)
=λ−1[kv]1.(18)

[Tiếp tục với chứng minh đầy đủ...]

**D CORPUS**

Chúng tôi thu thập một corpus rộng lớn văn bản có thể truy cập công khai từ internet, tổng cộng hơn 700TB về kích thước. Dữ liệu được thu thập được xử lý bởi quy trình tiền xử lý dữ liệu của chúng tôi như được thể hiện trong Hình 5, để lại một corpus đã làm sạch 6TB với khoảng 2 nghìn tỷ token. Chúng tôi phân loại các nguồn dữ liệu của mình để cung cấp tính minh bạch và hiểu biết tốt hơn. Các chi tiết cụ thể của những danh mục này được nêu trong Bảng 11.

**D.1 TIỀN XỬ LÝ DỮ LIỆU**

Quy trình tiền xử lý dữ liệu của chúng tôi bao gồm ba bước: 1). lọc dựa trên quy tắc, 2). khử trùng lặp, và 3). một lược đồ tự làm sạch. Trước khi được thêm vào corpus huấn luyện, corpus đã làm sạch cần được đánh giá bởi con người.

[Tiếp tục với tất cả các chi tiết về tiền xử lý dữ liệu...]

**E KẾT QUẢ THỰC NGHIỆM BỔ SUNG**

**E.1 SONG SONG MÔ HÌNH TRÊN TRANS NORMER LLM**

Chúng tôi tiến hành một loạt thí nghiệm với mô hình TransNormerLLM 7B để điều tra hiệu suất của song song mô hình trên TransNormerLLM về tốc độ và bộ nhớ. Những thử nghiệm này được thực hiện trên một node Nvidia DGX duy nhất chứa tám GPU A100 80G được liên kết bởi NVLink. Trong thí nghiệm này, FSDP được kích hoạt và Flash Attention (Dao et al., 2022a) được sử dụng trên Transformer. Bảng 12 thể hiện kết quả về tốc độ huấn luyện và tiêu thụ bộ nhớ.

[Tiếp tục với tất cả các kết quả thí nghiệm bổ sung...]
