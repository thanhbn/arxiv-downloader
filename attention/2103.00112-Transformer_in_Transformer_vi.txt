# 2103.00112.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2103.00112.pdf
# Kích thước tệp: 5541910 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformer trong Transformer
Kai Han1;2An Xiao2Enhua Wu1;3Jianyuan Guo2Chunjing Xu2Yunhe Wang2
1Phòng thí nghiệm trọng điểm nhà nước về Khoa học máy tính, ISCAS & UCAS
2Phòng thí nghiệm Noah's Ark, Huawei Technologies
3Đại học Macau
{hankai,weh}@ios.ac.cn, yunhe.wang@huawei.com
Tóm tắt
Transformer là một loại kiến trúc mạng neural mới mã hóa dữ liệu đầu vào thành các đặc trưng mạnh mẽ thông qua cơ chế attention. Về cơ bản, các transformer thị giác trước tiên chia hình ảnh đầu vào thành nhiều patch cục bộ và sau đó tính toán cả biểu diễn và mối quan hệ của chúng. Vì hình ảnh tự nhiên có độ phức tạp cao với thông tin chi tiết và màu sắc phong phú, độ chi tiết của việc chia patch không đủ tinh vi để khai thác các đặc trưng của các đối tượng ở các tỷ lệ và vị trí khác nhau. Trong bài báo này, chúng tôi chỉ ra rằng attention bên trong các patch cục bộ này cũng rất quan trọng để xây dựng các transformer thị giác có hiệu suất cao và chúng tôi khám phá một kiến trúc mới, cụ thể là Transformer iN Transformer (TNT). Cụ thể, chúng tôi coi các patch cục bộ (ví dụ: 16×16) là "câu thị giác" và trình bày để tiếp tục chia chúng thành các patch nhỏ hơn (ví dụ: 4×4) là "từ thị giác". Attention của mỗi từ sẽ được tính toán với các từ khác trong câu thị giác đã cho với chi phí tính toán không đáng kể. Các đặc trưng của cả từ và câu sẽ được tổng hợp để tăng cường khả năng biểu diễn. Các thí nghiệm trên một số benchmark chứng minh hiệu quả của kiến trúc TNT được đề xuất, ví dụ, chúng tôi đạt được độ chính xác top-1 81.5% trên ImageNet, cao hơn khoảng 1.7% so với transformer thị giác tiên tiến nhất với chi phí tính toán tương tự. Mã PyTorch có sẵn tại https://github.com/huawei-noah/CV-Backbones, và mã MindSpore có sẵn tại https://gitee.com/mindspore/models/tree/master/research/cv/TNT.

1 Giới thiệu
Trong thập kỷ qua, các kiến trúc mạng neural sâu chính được sử dụng trong thị giác máy tính (CV) chủ yếu được thiết lập dựa trên mạng neural tích chập (CNN) [19,13,12]. Khác biệt, transformer là một loại mạng neural chủ yếu dựa trên cơ chế self-attention [39], có thể cung cấp mối quan hệ giữa các đặc trưng khác nhau. Transformer được sử dụng rộng rãi trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP), ví dụ như các mô hình BERT [9] và GPT-3 [2] nổi tiếng. Sức mạnh của các mô hình transformer này truyền cảm hứng cho toàn bộ cộng đồng nghiên cứu việc sử dụng transformer cho các tác vụ thị giác.

Để sử dụng các kiến trúc transformer cho việc thực hiện các tác vụ thị giác, một số nhà nghiên cứu đã khám phá việc biểu diễn thông tin chuỗi từ các dữ liệu khác nhau. Ví dụ, Wang và cộng sự khám phá cơ chế self-attention trong mạng non-local [41] để nắm bắt các phụ thuộc tầm xa trong nhận dạng video và hình ảnh. Carion và cộng sự trình bày DETR [3], coi phát hiện đối tượng như một bài toán dự đoán tập hợp trực tiếp và giải quyết nó bằng kiến trúc encoder-decoder transformer. Chen và cộng sự đề xuất iGPT [6], là công việc tiên phong áp dụng mô hình transformer thuần túy (tức là không có tích chập) trên nhận dạng hình ảnh bằng tiền huấn luyện tự giám sát.

Tác giả liên hệ.
Hội nghị lần thứ 35 về Hệ thống xử lý thông tin neural (NeurIPS 2021).arXiv:2103.00112v3 [cs.CV] 26 Oct 2021

--- TRANG 2 ---
…21 2 3
4 5 6
7 8 9
11 2 3
4 5 6
7 8 991 2 3
4 5 6
7 8 9
Phép chiếu tuyến tính của Câu và Từ thị giác0*Đầu ra
Khối 
Transformer 
nội bộ
Khối 
Transformer 
nội bộ
Khối 
Transformer 
nội bộ
+ + +Khối Transformer ngoại vi L×
1
1*Mã hóa vị trí câu
Nhúng câu
Nhúng từMã hóa vị trí từMã thông báo lớp ……

Hình 1: Minh họa khung làm việc Transformer-iN-Transformer (TNT) được đề xuất. Khối transformer nội bộ được chia sẻ trong cùng một tầng. Mã hóa vị trí từ được chia sẻ giữa các câu thị giác.

Khác với dữ liệu trong các tác vụ NLP, tồn tại khoảng cách ngữ nghĩa giữa hình ảnh đầu vào và nhãn thực tế trong các tác vụ CV. Để đạt được mục đích này, Dosovitskiy và cộng sự phát triển ViT [10], mở đường cho việc chuyển giao thành công của các mô hình NLP dựa trên transformer. Cụ thể, ViT chia hình ảnh đã cho thành nhiều patch cục bộ như một chuỗi thị giác. Sau đó, attention có thể được tính toán tự nhiên giữa bất kỳ hai patch hình ảnh nào để tạo ra các biểu diễn đặc trưng hiệu quả cho tác vụ nhận dạng. Tiếp theo, Touvron và cộng sự khám phá huấn luyện hiệu quả dữ liệu và chưng cất để tăng cường hiệu suất của ViT trên benchmark ImageNet và đạt được độ chính xác top-1 ImageNet khoảng 81.8%, có thể so sánh với các mạng tích chập tiên tiến nhất. Chen và cộng sự tiếp tục coi các tác vụ xử lý hình ảnh (ví dụ: khử nhiễu và siêu phân giải) như một chuỗi các bản dịch và phát triển mô hình IPT để xử lý nhiều vấn đề thị giác máy tính cấp thấp [4]. Ngày nay, các kiến trúc transformer đã được sử dụng trong một số lượng ngày càng tăng các tác vụ thị giác máy tính [11] như nhận dạng hình ảnh [7, 44, 33], phát hiện đối tượng [50], và phân đoạn [47, 42].

Mặc dù các transformer thị giác đã đề cập ở trên đã nỗ lực rất nhiều để nâng cao hiệu suất của các mô hình, hầu hết các công việc hiện tại đều tuân theo sơ đồ biểu diễn thông thường được sử dụng trong ViT, tức là chia hình ảnh đầu vào thành các patch. Một mô hình tinh tế như vậy có thể nắm bắt hiệu quả thông tin tuần tự thị giác và ước tính attention giữa các patch hình ảnh khác nhau. Tuy nhiên, sự đa dạng của hình ảnh tự nhiên trong các benchmark hiện đại rất cao, ví dụ, có hơn 120 triệu hình ảnh với 1000 danh mục khác nhau trong bộ dữ liệu ImageNet [30]. Như được hiển thị trong Hình 1, biểu diễn hình ảnh đã cho thành các patch cục bộ có thể giúp chúng ta tìm ra mối quan hệ và sự tương tự giữa chúng. Tuy nhiên, cũng có một số sub-patch bên trong chúng có độ tương tự cao. Do đó, chúng tôi được thúc đẩy khám phá một phương pháp chia hình ảnh thị giác tinh tế hơn để tạo ra các chuỗi thị giác và cải thiện hiệu suất.

Trong bài báo này, chúng tôi đề xuất một kiến trúc Transformer-iN-Transformer (TNT) mới cho nhận dạng thị giác như được hiển thị trong Hình 1. Để tăng cường khả năng biểu diễn đặc trưng của các transformer thị giác, chúng tôi trước tiên chia hình ảnh đầu vào thành nhiều patch là "câu thị giác" và sau đó tiếp tục chia chúng thành các sub-patch là "từ thị giác". Ngoài các khối transformer thông thường để trích xuất đặc trưng và attention của các câu thị giác, chúng tôi tiếp tục nhúng một sub-transformer vào kiến trúc để khai thác các đặc trưng và chi tiết của các từ thị giác nhỏ hơn. Cụ thể, các đặc trưng và attention giữa các từ thị giác trong mỗi câu thị giác được tính toán độc lập bằng một mạng chia sẻ để lượng tham số và FLOP (phép toán điểm nổi) tăng thêm là không đáng kể. Sau đó, các đặc trưng của từ sẽ được tổng hợp vào câu thị giác tương ứng. Token lớp cũng được sử dụng cho tác vụ nhận dạng thị giác tiếp theo thông qua một đầu fully-connected. Thông qua mô hình TNT được đề xuất, chúng ta có thể trích xuất thông tin thị giác với độ chi tiết tinh vi và cung cấp các đặc trưng với nhiều chi tiết hơn. Sau đó chúng tôi tiến hành một loạt thí nghiệm trên benchmark ImageNet và các tác vụ downstream để chứng minh tính ưu việt của nó và phân tích kỹ lưỡng tác động của kích thước để chia các từ thị giác. Kết quả cho thấy TNT của chúng tôi có thể đạt được sự cân bằng độ chính xác và FLOP tốt hơn so với các mạng transformer tiên tiến nhất.
2

--- TRANG 3 ---
2 Phương pháp
Trong phần này, chúng tôi mô tả kiến trúc transformer-in-transformer được đề xuất và phân tích chi tiết độ phức tạp tính toán và tham số.

2.1 Kiến thức cơ bản
Chúng tôi trước tiên mô tả ngắn gọn các thành phần cơ bản trong transformer [39], bao gồm MSA (Multi-head Self-Attention), MLP (Multi-Layer Perceptron) và LN (Layer Normalization).

MSA. Trong mô-đun self-attention, các đầu vào X∈R^(n×d) được biến đổi tuyến tính thành ba phần, tức là truy vấn Q∈R^(n×d_k), khóa K∈R^(n×d_k) và giá trị V∈R^(n×d_v) trong đó n là độ dài chuỗi, d, d_k, d_v là các chiều của đầu vào, truy vấn (khóa) và giá trị, tương ứng. Scaled dot-product attention được áp dụng trên Q, K, V:

Attention(Q,K,V) = softmax(QK^T/√d_k)V. (1)

Cuối cùng, một lớp tuyến tính được sử dụng để tạo ra đầu ra. Multi-head self-attention chia truy vấn, khóa và giá trị thành h phần và thực hiện hàm attention song song, và sau đó các giá trị đầu ra của mỗi head được nối và chiếu tuyến tính để tạo thành đầu ra cuối cùng.

MLP. MLP được áp dụng giữa các lớp self-attention cho việc biến đổi đặc trưng và phi tuyến tính:

MLP(X) = FC(σ(FC(X))); FC(X) = XW + b, (2)

trong đó W và b là trọng số và hạng tử bias của lớp fully-connected tương ứng, và σ(·) là hàm kích hoạt như GELU [14].

LN. Layer normalization [1] là một phần quan trọng trong transformer để huấn luyện ổn định và hội tụ nhanh hơn. LN được áp dụng trên mỗi mẫu x∈R^d như sau:

LN(x) = γ ⊙ (x - μ)/σ + β (3)

trong đó μ∈R, σ∈R là giá trị trung bình và độ lệch chuẩn của đặc trưng tương ứng, ⊙ là tích element-wise, và γ∈R^d, β∈R^d là các tham số biến đổi affine có thể học được.

2.2 Transformer trong Transformer
Cho một hình ảnh 2D, chúng tôi chia đều nó thành n patch X = [X_1; X_2; ...; X_n] ∈ R^(n×p×p×3), trong đó (p,p) là độ phân giải của mỗi patch hình ảnh. ViT [10] chỉ sử dụng một transformer tiêu chuẩn để xử lý chuỗi các patch làm hỏng cấu trúc cục bộ của một patch, như được hiển thị trong Hình 1(a). Thay vào đó, chúng tôi đề xuất kiến trúc Transformer-iN-Transformer (TNT) để học cả thông tin toàn cục và cục bộ trong một hình ảnh. Trong TNT, chúng tôi coi các patch là câu thị giác đại diện cho hình ảnh. Mỗi patch được chia tiếp thành m sub-patch, tức là một câu thị giác được cấu thành từ một chuỗi các từ thị giác:

X_i → [x_{i,1}; x_{i,2}; ...; x_{i,m}], (4)

trong đó x_{i,j} ∈ R^(s×s×3) là từ thị giác thứ j của câu thị giác thứ i, (s,s) là kích thước không gian của sub-patch, và j = 1,2,...,m. Với một phép chiếu tuyến tính, chúng tôi biến đổi các từ thị giác thành một chuỗi các embedding từ:

Y_i = [y_{i,1}; y_{i,2}; ...; y_{i,m}]; y_{i,j} = FC(Vec(x_{i,j})), (5)

trong đó y_{i,j} ∈ R^c là embedding từ thứ j, c là chiều của embedding từ, và Vec(·) là phép toán vector hóa.

Trong TNT, chúng tôi có hai luồng dữ liệu trong đó một luồng hoạt động trên các câu thị giác và luồng khác xử lý các từ thị giác bên trong mỗi câu. Đối với các embedding từ, chúng tôi sử dụng một khối transformer để khám phá mối quan hệ giữa các từ thị giác:

Y'^i_l = Y^i_{l-1} + MSA(LN(Y^i_{l-1})), (6)

Y^i_l = Y'^i_l + MLP(LN(Y'^i_l)). (7)

3

--- TRANG 4 ---
trong đó l = 1,2,...,L là chỉ số của khối thứ l, và L là tổng số khối xếp chồng. Đầu vào của khối đầu tiên Y^i_0 chỉ là Y_i trong Phương trình 5. Tất cả các embedding từ trong hình ảnh sau biến đổi là Y_l = [Y^1_l; Y^2_l; ...; Y^n_l]. Điều này có thể được xem như một khối transformer nội bộ, ký hiệu là T_in. Quá trình này xây dựng mối quan hệ giữa các từ thị giác bằng cách tính toán tương tác giữa bất kỳ hai từ thị giác nào. Ví dụ, trong một patch khuôn mặt người, một từ tương ứng với mắt có liên quan nhiều hơn với các từ khác của mắt trong khi tương tác ít với phần trán.

Đối với cấp độ câu, chúng tôi tạo ra bộ nhớ embedding câu để lưu trữ chuỗi biểu diễn cấp độ câu: Z_0 = [Z_class; Z^1_0; Z^2_0; ...; Z^n_0] ∈ R^{(n+1)×d} trong đó Z_class là token lớp tương tự như ViT [10], và tất cả chúng được khởi tạo là zero. Trong mỗi lớp, chuỗi các embedding từ được biến đổi thành miền của embedding câu bởi phép chiếu tuyến tính và được thêm vào embedding câu:

Z^i_{l-1} = Z^i_{l-1} + FC(Vec(Y^i_l)), (8)

trong đó Z^i_{l-1} ∈ R^d và lớp fully-connected FC làm cho chiều khớp để cộng. Với phép toán cộng ở trên, biểu diễn của embedding câu được tăng cường bởi các đặc trưng cấp độ từ. Chúng tôi sử dụng khối transformer tiêu chuẩn để biến đổi các embedding câu:

Z'_l = Z_{l-1} + MSA(LN(Z_{l-1})), (9)

Z_l = Z'_l + MLP(LN(Z'_l)). (10)

Khối transformer ngoại vi T_out này được sử dụng để mô hình hóa mối quan hệ giữa các embedding câu.

Tóm lại, các đầu vào và đầu ra của khối TNT bao gồm các embedding từ thị giác và embedding câu như được hiển thị trong Hình 1(b), do đó TNT có thể được công thức hóa như

Y_l; Z_l = TNT(Y_{l-1}; Z_{l-1}). (11)

Trong khối TNT của chúng tôi, khối transformer nội bộ được sử dụng để mô hình hóa mối quan hệ giữa các từ thị giác để trích xuất đặc trưng cục bộ, và khối transformer ngoại vi nắm bắt thông tin nội tại từ chuỗi các câu. Bằng cách xếp chồng các khối TNT L lần, chúng tôi xây dựng mạng transformer-in-transformer. Cuối cùng, token phân loại phục vụ như biểu diễn hình ảnh và một lớp fully-connected được áp dụng cho phân loại.

Mã hóa vị trí. Thông tin không gian là một yếu tố quan trọng trong nhận dạng hình ảnh. Đối với embedding câu và embedding từ, chúng tôi đều thêm mã hóa vị trí tương ứng để giữ lại thông tin không gian như được hiển thị trong Hình 1. Mã hóa vị trí 1D có thể học được tiêu chuẩn được sử dụng ở đây. Cụ thể, mỗi câu được gán với một mã hóa vị trí:

Z_0 ← Z_0 + E_sentence, (12)

trong đó E_sentence ∈ R^{(n+1)×d} là mã hóa vị trí câu. Đối với các từ thị giác trong một câu, một mã hóa vị trí từ được thêm vào mỗi embedding từ:

Y^i_0 ← Y^i_0 + E_word, i = 1,2,...,n (13)

trong đó E_word ∈ R^{m×c} là mã hóa vị trí từ được chia sẻ giữa các câu. Theo cách này, mã hóa vị trí câu có thể duy trì thông tin không gian toàn cục, trong khi mã hóa vị trí từ được sử dụng để bảo tồn vị trí tương đối cục bộ.

2.3 Phân tích độ phức tạp
Một khối transformer tiêu chuẩn bao gồm hai phần, tức là multi-head self-attention và multi-layer perceptron. FLOP của MSA là 2nd(d_k + d_v) + n^2(d_k + d_v), và FLOP của MLP là 2nd_v rd_v trong đó r là tỷ lệ mở rộng chiều của lớp ẩn trong MLP. Tổng thể, FLOP của một khối transformer tiêu chuẩn là

FLOPs_T = 2nd(d_k + d_v) + n^2(d_k + d_v) + 2nd_d r. (14)

Vì r thường được đặt là 4, và các chiều của đầu vào, khóa (truy vấn) và giá trị thường được đặt giống nhau, tính toán FLOP có thể được đơn giản hóa thành

FLOPs_T = 2nd(6d + n). (15)

4

--- TRANG 5 ---
Số lượng tham số có thể được tính như

Params_T = 12d^2. (16)

Khối TNT của chúng tôi bao gồm ba phần: một khối transformer nội bộ T_in, một khối transformer ngoại vi T_out và một lớp tuyến tính. Độ phức tạp tính toán của T_in và T_out lần lượt là 2nmc(6c + m) và 2nd(6d + n). Lớp tuyến tính có FLOP là nmcd. Tổng cộng, FLOP của khối TNT là

FLOPs_TNT = 2nmc(6c + m) + nmcd + 2nd(6d + n). (17)

Tương tự, độ phức tạp tham số của khối TNT được tính như

Params_TNT = 12c^2 + mcd + 12d^2. (18)

Mặc dù chúng tôi thêm hai thành phần nữa trong khối TNT của mình, việc tăng FLOP là nhỏ vì c ≪ d và O(m) ≪ O(n) trong thực tế. Ví dụ, trong cấu hình DeiT-S, chúng ta có d = 384 và n = 196. Chúng tôi đặt c = 24 và m = 16 trong cấu trúc TNT-S tương ứng. Từ Phương trình 15 và Phương trình 17, chúng ta có thể thu được FLOPs_T = 376M và FLOPs_TNT = 429M. Tỷ lệ FLOP của khối TNT so với khối transformer tiêu chuẩn là khoảng 1:14. Tương tự, tỷ lệ tham số là khoảng 1:08. Với một ít tăng chi phí tính toán và bộ nhớ, khối TNT của chúng tôi có thể mô hình hóa hiệu quả thông tin cấu trúc cục bộ và đạt được sự cân bằng tốt hơn nhiều giữa độ chính xác và độ phức tạp như được chứng minh trong các thí nghiệm.

2.4 Kiến trúc mạng
Chúng tôi xây dựng các kiến trúc TNT của mình bằng cách tuân theo cấu hình cơ bản của ViT [10] và DeiT [35]. Kích thước patch được đặt là 16×16. Số lượng sub-patch được đặt là m = 4×4 = 16 theo mặc định. Các giá trị kích thước khác được đánh giá trong các nghiên cứu ablation. Như được hiển thị trong Bảng 1, có ba biến thể của mạng TNT với các kích thước mô hình khác nhau, cụ thể là TNT-Ti, TNT-S và TNT-B. Chúng bao gồm 6.1M, 23.8M và 65.6M tham số tương ứng. FLOP tương ứng để xử lý một hình ảnh 224×224 lần lượt là 1.4B, 5.2B và 14.1B.

Bảng 1: Các biến thể của kiến trúc TNT của chúng tôi. 'Ti' có nghĩa là tiny, 'S' có nghĩa là small, và 'B' có nghĩa là base. FLOP được tính cho hình ảnh ở độ phân giải 224×224.

Mô hình | Độ sâu | Transformer nội bộ | Transformer ngoại vi | Params | FLOPs
        |        | dim c | #heads | MLP r | dim d | #heads | MLP r | (M)   | (B)
TNT-Ti  | 12     | 12    | 2      | 4     | 192   | 3      | 4     | 6.1   | 1.4
TNT-S   | 12     | 24    | 4      | 4     | 384   | 6      | 4     | 23.8  | 5.2
TNT-B   | 12     | 40    | 4      | 4     | 640   | 10     | 4     | 65.6  | 14.1

3 Thí nghiệm
Trong phần này, chúng tôi tiến hành các thí nghiệm rộng rãi trên các benchmark thị giác để đánh giá hiệu quả của kiến trúc TNT được đề xuất.

Bảng 2: Chi tiết của các bộ dữ liệu thị giác được sử dụng.

Bộ dữ liệu | Loại | Kích thước huấn luyện | Kích thước val | #Lớp
ImageNet [30] | Tiền huấn luyện | 1,281,167 | 50,000 | 1,000
Oxford 102 Flowers [25] | Phân loại | 2,040 | 6,149 | 102
Oxford-IIIT Pets [26] | | 3,680 | 3,669 | 37
iNaturalist 2019 [38] | | 265,240 | 3,003 | 1,010
CIFAR-10 [18] | | 50,000 | 10,000 | 10
CIFAR-100 [18] | | 50,000 | 10,000 | 100
COCO2017 [22] | Phát hiện | 118,287 | 5,000 | 80
ADE20K [49] | Phân đoạn | 20,210 | 2,000 | 150

5

--- TRANG 6 ---
3.1 Bộ dữ liệu và thiết lập thí nghiệm
Bộ dữ liệu. ImageNet ILSVRC 2012 [30] là một benchmark phân loại hình ảnh bao gồm 1.2M hình ảnh huấn luyện thuộc về 1000 lớp, và 50K hình ảnh xác thực với 50 hình ảnh mỗi lớp. Chúng tôi áp dụng cùng chiến lược tăng cường dữ liệu như trong DeiT [35] bao gồm random crop, random clip, Rand-Augment [8], Random Erasing [48], Mixup [46] và CutMix [45]. Đối với giấy phép của bộ dữ liệu ImageNet, vui lòng tham khảo http://www.image-net.org/download.

Ngoài ImageNet, chúng tôi cũng thử nghiệm trên các tác vụ downstream với transfer learning để đánh giá khả năng tổng quát hóa của TNT. Chi tiết của các bộ dữ liệu thị giác được sử dụng được liệt kê trong Bảng 2. Chiến lược tăng cường dữ liệu của các bộ dữ liệu phân loại hình ảnh giống như của ImageNet. Đối với COCO và ADE20K, chiến lược tăng cường dữ liệu tuân theo như trong PVT [40]. Đối với giấy phép của các bộ dữ liệu này, vui lòng tham khảo các bài báo gốc.

Chi tiết triển khai. Chúng tôi sử dụng chiến lược huấn luyện được cung cấp trong DeiT [35]. Các công nghệ tiên tiến chính ngoài các thiết lập thông thường [13] bao gồm AdamW [23], label smoothing [31], DropPath [20], và repeated augmentation [15]. Chúng tôi liệt kê các siêu tham số trong Bảng 3 để hiểu rõ hơn. Tất cả các mô hình được triển khai với PyTorch [27] và MindSpore [17] và được huấn luyện trên GPU NVIDIA Tesla V100. Các tác động tiêu cực tiềm năng đối với xã hội có thể bao gồm tiêu thụ năng lượng và phát thải carbon dioxide từ tính toán GPU.

Bảng 3: Các siêu tham số huấn luyện mặc định được sử dụng trong phương pháp của chúng tôi, trừ khi được nêu khác.

Epochs | Optimizer | Batch size | Learning rate | LR decay | Weight decay | Warmup epochs | Label smooth | Drop path | Repeated Aug
300 | AdamW | 1024 | 1e-3 | cosine | 0.05 | 5 | 0.1 | 0.1 | p

3.2 TNT trên ImageNet
Chúng tôi huấn luyện các mô hình TNT của mình với cùng thiết lập huấn luyện như DeiT [35]. Các mô hình dựa trên transformer gần đây như ViT [10] và DeiT [35] được so sánh. Để hiểu rõ hơn về tiến bộ hiện tại của các transformer thị giác, chúng tôi cũng bao gồm các mô hình dựa trên CNN đại diện như ResNet [13], RegNet [28] và EfficientNet [32]. Kết quả được hiển thị trong Bảng 4. Chúng ta có thể thấy rằng mô hình dựa trên transformer của chúng tôi, tức là TNT vượt trội hơn tất cả các mô hình transformer thị giác khác. Đặc biệt, TNT-S đạt được độ chính xác top-1 81.5% cao hơn 1.7% so với mô hình baseline DeiT-S, cho thấy lợi ích của khung TNT được giới thiệu để bảo tồn thông tin cấu trúc cục bộ bên trong patch. So với CNN, TNT có thể vượt trội hơn ResNet và RegNet được sử dụng rộng rãi. Lưu ý rằng tất cả các mô hình dựa trên transformer vẫn kém hơn EfficientNet sử dụng các tích chập depth-wise đặc biệt, vì vậy việc đánh bại EfficientNet bằng transformer thuần túy vẫn là một thách thức.

Bảng 4: Kết quả của TNT và các mạng khác trên ImageNet.

Mô hình | Độ phân giải | Params (M) | FLOPs (B) | Top-1 | Top-5
Dựa trên CNN
ResNet-50 [13] | 224×224 | 25.6 | 4.1 | 76.2 | 92.9
ResNet-152 [13] | 224×224 | 60.2 | 11.5 | 78.3 | 94.1
RegNetY-8GF [28] | 224×224 | 39.2 | 8.0 | 79.9 | -
RegNetY-16GF [28] | 224×224 | 83.6 | 15.9 | 80.4 | -
EfficientNet-B3 [32] | 300×300 | 12.0 | 1.8 | 81.6 | 94.9
EfficientNet-B4 [32] | 380×380 | 19.0 | 4.2 | 82.9 | 96.4
Dựa trên Transformer
DeiT-Ti [35] | 224×224 | 5.7 | 1.3 | 72.2 | -
TNT-Ti | 224×224 | 6.1 | 1.4 | 73.9 | 91.9
DeiT-S [35] | 224×224 | 22.1 | 4.6 | 79.8 | -
PVT-Small [40] | 224×224 | 24.5 | 3.8 | 79.8 | -
T2T-ViT_t-14 [44] | 224×224 | 21.5 | 5.2 | 80.7 | -
TNT-S | 224×224 | 23.8 | 5.2 | 81.5 | 95.7
ViT-B/16 [10] | 384×384 | 86.4 | 55.5 | 77.9 | -
DeiT-B [35] | 224×224 | 86.4 | 17.6 | 81.8 | -
T2T-ViT_t-24 [44] | 224×224 | 63.9 | 13.2 | 82.2 | -
TNT-B | 224×224 | 65.6 | 14.1 | 82.9 | 96.3

6

--- TRANG 7 ---
[Biểu đồ so sánh hiệu suất]

Hình 2: So sánh hiệu suất của các mạng backbone thị giác đại diện trên ImageNet.

Chúng tôi cũng vẽ biểu đồ đường độ chính xác-tham số và độ chính xác-FLOP trong Hình 2 để có so sánh trực quan về các mô hình này. Các mô hình TNT của chúng tôi liên tục vượt trội hơn các mô hình dựa trên transformer khác với một khoảng cách đáng kể.

Tốc độ suy luận. Triển khai các mô hình transformer trên thiết bị rất quan trọng cho các ứng dụng thực tế, vì vậy chúng tôi thử nghiệm tốc độ suy luận của mô hình TNT. Theo [35], throughput được đo trên GPU NVIDIA V100 và PyTorch, với kích thước đầu vào 224×224. Vì độ phân giải và nội dung bên trong patch nhỏ hơn so với toàn bộ hình ảnh, chúng ta có thể cần ít khối hơn để học biểu diễn của nó. Do đó, chúng tôi có thể giảm các khối TNT được sử dụng và thay thế một số bằng các khối transformer vanilla. Từ kết quả trong Bảng 5, chúng ta có thể thấy rằng TNT của chúng tôi hiệu quả hơn DeiT và PVT bằng cách đạt được độ chính xác cao hơn với tốc độ suy luận tương tự.

Bảng 5: So sánh throughput GPU của các mô hình vision transformer.

Mô hình | Chỉ số của khối TNT | FLOPs (B) | Throughput (hình ảnh/s) | Top-1
DeiT-S [35] | - | 4.6 | 907 | 79.8
DeiT-B [35] | - | 17.6 | 292 | 81.8
PVT-Small [40] | - | 3.8 | 820 | 79.8
PVT-Medium [40] | - | 6.7 | 526 | 81.2
TNT-S | [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] | 5.2 | 428 | 81.5
TNT-S-1 | [1, 4, 8, 12] | 4.8 | 668 | 81.4
TNT-S-2 | [1, 6, 12] | 4.7 | 704 | 81.3
TNT-S-3 | [1, 6] | 4.7 | 757 | 81.1
TNT-S-4 | [1] | 4.6 | 822 | 80.8

3.3 Nghiên cứu Ablation

Bảng 6: Ảnh hưởng của mã hóa vị trí.

Mã hóa vị trí
Mô hình | Cấp độ câu | Cấp độ từ | Top-1
TNT-S | % | % | 80.5
" | % | " | 80.8
% | " | % | 80.7
" | " | " | 81.5

Ảnh hưởng của mã hóa vị trí. Thông tin vị trí rất quan trọng cho nhận dạng hình ảnh. Trong cấu trúc TNT, mã hóa vị trí câu là để duy trì thông tin không gian toàn cục, và mã hóa vị trí từ được sử dụng để bảo tồn vị trí tương đối cục bộ. Chúng tôi xác minh ảnh hưởng của chúng bằng cách loại bỏ chúng riêng biệt. Như được hiển thị trong Bảng 6, chúng ta có thể thấy rằng TNT-S với cả mã hóa vị trí patch và mã hóa vị trí từ hoạt động tốt nhất bằng cách đạt được độ chính xác top-1 81.5%. Loại bỏ mã hóa vị trí câu/từ dẫn đến giảm độ chính xác 0.8%/0.7% tương ứng, và loại bỏ tất cả mã hóa vị trí làm giảm mạnh độ chính xác 1.0%.

Số lượng head. Ảnh hưởng của #head trong transformer tiêu chuẩn đã được nghiên cứu trong nhiều công việc [24,39] và độ rộng head 64 được khuyến nghị cho các tác vụ thị giác [10,35]. Chúng tôi áp dụng độ rộng head 64 trong khối transformer ngoại vi trong mô hình của chúng tôi. Số lượng head trong khối transformer nội bộ là một siêu tham số khác để nghiên cứu. Chúng tôi đánh giá ảnh hưởng của #head trong khối transformer nội bộ (Bảng 7). Chúng ta có thể thấy rằng một số lượng head phù hợp (ví dụ: 2 hoặc 4) đạt được hiệu suất tốt nhất.

Bảng 7: Ảnh hưởng của #head trong khối transformer nội bộ trong TNT-S.

#heads | 1 | 2 | 4 | 6 | 8
Top-1 | 81.0 | 81.4 | 81.5 | 81.3 | 81.1

Bảng 8: Ảnh hưởng của #từ m.

m | c | Params | FLOPs | Top-1
64 | 6 | 23.8M | 5.1B | 81.0
16 | 24 | 23.8M | 5.2B | 81.5
4 | 96 | 25.1M | 6.0B | 81.1

Số lượng từ thị giác. Trong TNT, hình ảnh đầu vào được chia thành một số patch 16×16 và mỗi patch được chia tiếp thành m sub-patch (từ thị giác) có kích thước (s,s) để đạt hiệu quả tính toán. Ở đây chúng tôi thử nghiệm ảnh hưởng của siêu tham số m trên kiến trúc TNT-S. Khi chúng ta thay đổi m, chiều embedding c cũng thay đổi tương ứng để kiểm soát FLOP. Như được hiển thị trong Bảng 8, chúng ta có thể thấy rằng giá trị của m có ảnh hưởng nhẹ đến hiệu suất, và chúng tôi sử dụng m = 16 theo mặc định cho hiệu quả của nó, trừ khi được nêu khác.

[Hình ảnh trực quan hóa đặc trưng]

Hình 3: Trực quan hóa các đặc trưng của DeiT-S và TNT-S.

3.4 Trực quan hóa
Trực quan hóa bản đồ đặc trưng. Chúng tôi trực quan hóa các đặc trưng đã học của DeiT và TNT để hiểu rõ hơn về ảnh hưởng của phương pháp đề xuất. Để trực quan hóa tốt hơn, hình ảnh đầu vào được thay đổi kích thước thành 1024×1024. Các bản đồ đặc trưng được hình thành bằng cách reshape các embedding patch theo vị trí không gian của chúng. Các bản đồ đặc trưng trong khối thứ 1, 6 và 12 được hiển thị trong Hình 3(a) trong đó 12 bản đồ đặc trưng được lấy mẫu ngẫu nhiên cho mỗi khối này. Trong TNT, thông tin cục bộ được bảo tồn tốt hơn so với DeiT. Chúng tôi cũng trực quan hóa tất cả 384 bản đồ đặc trưng trong khối thứ 12 bằng t-SNE [37] (Hình 3(b)). Chúng ta có thể thấy rằng các đặc trưng của TNT đa dạng hơn và chứa thông tin phong phú hơn so với DeiT. Những lợi ích này có được nhờ việc giới thiệu khối transformer nội bộ để mô hình hóa các đặc trưng cục bộ.

Ngoài các đặc trưng cấp patch, chúng tôi cũng trực quan hóa các embedding cấp pixel của TNT trong Hình 4. Đối với mỗi patch, chúng tôi reshape các embedding từ theo vị trí không gian của chúng để tạo thành các bản đồ đặc trưng và sau đó lấy trung bình các bản đồ đặc trưng này theo chiều kênh. Các bản đồ đặc trưng trung bình tương ứng với các patch 14×14 được hiển thị trong Hình 4. Chúng ta có thể thấy rằng thông tin cục bộ được bảo tồn tốt trong các lớp nông, và các biểu diễn trở nên trừu tượng hơn dần khi mạng đi sâu hơn.

Trực quan hóa bản đồ attention. Có hai lớp self-attention trong khối TNT của chúng tôi, tức là một inner self-attention và một outer self-attention để mô hình hóa mối quan hệ giữa các từ thị giác và câu tương ứng. Chúng tôi hiển thị các bản đồ attention của các truy vấn khác nhau trong transformer nội bộ trong Hình 5. Đối với một từ thị giác truy vấn đã cho, các giá trị attention của các từ thị giác có ngoại hình tương tự cao hơn, cho thấy các đặc trưng của chúng sẽ tương tác phù hợp hơn với truy vấn. Những tương tác này bị bỏ lỡ trong ViT và DeiT, v.v. Các bản đồ attention trong transformer ngoại vi có thể được tìm thấy trong tài liệu bổ sung.

8

--- TRANG 8 ---
Hình 4: Trực quan hóa các embedding từ trung bình của TNT-S.

Hình 5: Bản đồ attention của các truy vấn khác nhau trong transformer nội bộ. Ký hiệu chữ thập đỏ biểu thị vị trí truy vấn.

3.5 Transfer Learning
Để chứng minh khả năng tổng quát hóa mạnh mẽ của TNT, chúng tôi chuyển các mô hình TNT-S, TNT-B được huấn luyện trên ImageNet sang các tác vụ downstream.

Phân loại hình ảnh Transformer thuần túy. Theo DeiT [35], chúng tôi đánh giá các mô hình của mình trên 4 bộ dữ liệu phân loại hình ảnh với kích thước tập huấn luyện từ 2,040 đến 50,000 hình ảnh. Các bộ dữ liệu này bao gồm phân loại đối tượng cấp độ superordinate (CIFAR-10 [18], CIFAR-100 [18]) và phân loại đối tượng fine-grained (Oxford-IIIT Pets [26], Oxford 102 Flowers [25] và iNaturalist 2019 [38]), được hiển thị trong Bảng 2. Tất cả các mô hình được fine-tune với độ phân giải hình ảnh 384×384. Chúng tôi áp dụng cùng thiết lập huấn luyện như ở giai đoạn tiền huấn luyện bằng cách bảo tồn tất cả các chiến lược tăng cường dữ liệu. Để fine-tune ở độ phân giải khác nhau, chúng tôi cũng nội suy các embedding vị trí của các patch mới. Đối với CIFAR-10 và CIFAR-100, chúng tôi fine-tune các mô hình trong 64 epoch, và đối với các bộ dữ liệu fine-grained, chúng tôi fine-tune các mô hình trong 300 epoch. Bảng 9 so sánh kết quả transfer learning của TNT với ViT, DeiT và các mạng tích chập khác. Chúng tôi thấy rằng TNT vượt trội hơn DeiT trong hầu hết các bộ dữ liệu với ít tham số hơn, cho thấy tính ưu việt của việc mô hình hóa mối quan hệ cấp pixel để có được biểu diễn đặc trưng tốt hơn.

Bảng 9: Kết quả trên các tác vụ phân loại hình ảnh downstream với tiền huấn luyện ImageNet. "384 biểu thị fine-tuning với độ phân giải 384×384.

Mô hình | Params (M) | ImageNet | CIFAR10 | CIFAR100 | Flowers | Pets | iNat-19
Dựa trên CNN
Grafit ResNet-50 [36] | 25.6 | 79.6 | - | - | 98.2 | - | 75.9
Grafit RegNetY-8GF [36] | 39.2 | - | - | - | 99.1 | - | 80.0
EfficientNet-B5 [32] | 30 | 83.6 | 98.7 | 91.1 | 98.5 | - | -
Dựa trên Transformer
ViT-B/16@384 [10] | 86.4 | 77.9 | 98.1 | 87.1 | 89.5 | 93.8 | -
DeiT-B@384 [35] | 86.4 | 83.1 | 99.1 | 90.8 | 98.4 | - | -
TNT-S@384 | 23.8 | 83.1 | 98.7 | 90.1 | 98.8 | 94.7 | 81.4
TNT-B@384 | 65.6 | 83.9 | 99.1 | 91.1 | 99.0 | 95.0 | 83.2

Phát hiện đối tượng Transformer thuần túy. Chúng tôi xây dựng một pipeline phát hiện đối tượng transformer thuần túy bằng cách kết hợp TNT và DETR [3] của chúng tôi. Để so sánh công bằng, chúng tôi áp dụng thiết lập huấn luyện và thử nghiệm trong PVT [40] và thêm average pooling 2×2 để làm cho kích thước đầu ra của backbone TNT giống như của PVT và ResNet. Tất cả các mô hình được so sánh được huấn luyện bằng AdamW [23] với batch size 16 trong 50 epoch. Các hình ảnh huấn luyện được thay đổi kích thước ngẫu nhiên để có cạnh ngắn hơn trong khoảng [640,800] và cạnh dài hơn trong vòng 1333 pixel. Để thử nghiệm, cạnh ngắn hơn được đặt là 800 pixel. Kết quả trên COCO val2017 được hiển thị trong Bảng 10. Dưới cùng thiết lập, DETR với backbone TNT-S vượt trội hơn detector transformer thuần túy đại diện DETR+PVT-Small 3.5 AP với tham số tương tự.

9

--- TRANG 9 ---
Bảng 10: Kết quả phát hiện đối tượng trên tập COCO2017 val với tiền huấn luyện ImageNet. †Kết quả từ triển khai của chúng tôi.

Backbone | Params | Epochs | AP | AP50 | AP75 | APS | APM | APL
ResNet-50 [40] | 41M | 50 | 32.3 | 53.9 | 32.3 | 10.7 | 33.8 | 53.0
DeiT-S† [35] | 38M | 50 | 33.9 | 54.7 | 34.3 | 11.0 | 35.4 | 56.6
PVT-Small [40] | 40M | 50 | 34.7 | 55.7 | 35.4 | 12.0 | 36.4 | 56.7
PVT-Medium [40] | 57M | 50 | 36.4 | 57.9 | 37.2 | 13.0 | 38.7 | 59.1
TNT-S | 39M | 50 | 38.2 | 58.9 | 39.4 | 15.5 | 41.1 | 58.8

Bảng 11: Kết quả phân đoạn ngữ nghĩa trên tập ADE20K val với tiền huấn luyện ImageNet. †Kết quả từ triển khai của chúng tôi.

Backbone | Params | FLOPs | Steps | mIoU
ResNet-50 [42] | 56.1M | 79.3G | 40k | 39.7
DeiT-S† [35] | 30.3M | 27.2G | 40k | 40.5
PVT-Small [47] | 32.1M | 31.6G | 40k | 42.6
TNT-S | 32.1M | 30.4G | 40k | 43.6

Phân đoạn ngữ nghĩa Transformer thuần túy. Chúng tôi áp dụng framework phân đoạn của Trans2Seg [42] để xây dựng phân đoạn ngữ nghĩa transformer thuần túy dựa trên backbone TNT. Chúng tôi tuân theo cấu hình huấn luyện và thử nghiệm trong PVT [40] để so sánh công bằng. Tất cả các mô hình được so sánh được huấn luyện bởi optimizer AdamW với learning rate ban đầu 1e-4 và lịch trình decay polynomial. Chúng tôi áp dụng random resize và crop 512×512 trong quá trình huấn luyện. Kết quả ADE20K với single scale testing được hiển thị trong Bảng 11. Với tham số tương tự, Trans2Seg với backbone TNT-S đạt được 43.6% mIoU, cao hơn 1.0% so với backbone PVT-small và cao hơn 2.8% so với backbone DeiT-S.

4 Kết luận
Trong bài báo này, chúng tôi đề xuất một kiến trúc mạng Transformer-iN-Transformer (TNT) mới cho nhận dạng thị giác. Đặc biệt, chúng tôi chia đều hình ảnh thành một chuỗi các patch (câu thị giác) và coi mỗi patch như một chuỗi các sub-patch (từ thị giác). Chúng tôi giới thiệu một khối TNT trong đó một khối transformer ngoại vi được sử dụng để xử lý các embedding câu và một khối transformer nội bộ được sử dụng để mô hình hóa mối quan hệ giữa các embedding từ. Thông tin của các embedding từ thị giác được thêm vào embedding câu thị giác sau phép chiếu của một lớp tuyến tính. Chúng tôi xây dựng kiến trúc TNT của mình bằng cách xếp chồng các khối TNT. So với các transformer thị giác thông thường (ViT) làm hỏng cấu trúc cục bộ của patch, TNT của chúng tôi có thể bảo tồn và mô hình hóa tốt hơn thông tin cục bộ cho nhận dạng thị giác. Các thí nghiệm rộng rãi trên ImageNet và các tác vụ downstream đã chứng minh hiệu quả của kiến trúc TNT được đề xuất.

Lời cảm ơn
Công việc này được hỗ trợ bởi NSFC (62072449, 61632003), Guangdong-Hongkong-Macao Joint Research Grant (2020B1515130004) và Macao FDCT (0018/2019/AKP, 0015/2019/AKP).

A Phụ lục
A.1 Trực quan hóa bản đồ Attention
Attention giữa các Patch. Trong Hình 6, chúng tôi vẽ các bản đồ attention từ mỗi patch đến tất cả các patch. Chúng ta có thể thấy rằng đối với cả DeiT-S và TNT-S, nhiều patch liên quan hơn khi lớp đi sâu hơn. Điều này là do thông tin giữa các patch đã được giao tiếp đầy đủ với nhau trong các lớp sâu hơn. Về sự khác biệt giữa DeiT và TNT, attention của TNT có thể tập trung vào các patch có ý nghĩa trong Block-12, trong khi DeiT vẫn chú ý đến cây không liên quan đến gấu trúc.

Attention giữa Class Token và Patch. Trong Hình 7, chúng tôi vẽ các bản đồ attention giữa class token đến tất cả các patch cho một số hình ảnh được lấy mẫu ngẫu nhiên. Chúng ta có thể thấy rằng đặc trưng đầu ra chủ yếu tập trung vào các patch liên quan đến đối tượng cần nhận dạng.

10

--- TRANG 10 ---
Hình 6: Trực quan hóa các bản đồ attention giữa tất cả các patch trong khối transformer ngoại vi.

Hình 7: Ví dụ bản đồ attention từ output token đến không gian đầu vào.

A.2 Khám phá mô-đun SE trong TNT
Lấy cảm hứng từ mạng squeeze-and-excitation (SE) cho CNN [16], chúng tôi đề xuất khám phá attention theo kênh cho transformer. Chúng tôi trước tiên lấy trung bình tất cả các embedding câu (từ) và sử dụng MLP hai lớp để tính toán các giá trị attention. Attention được nhân với tất cả các embedding. Mô-đun SE chỉ mang lại một vài tham số bổ sung nhưng có thể thực hiện attention theo chiều cho việc tăng cường đặc trưng. Từ kết quả trong Bảng 12, việc thêm mô-đun SE vào TNT có thể cải thiện thêm độ chính xác một chút.

Bảng 12: Khám phá mô-đun SE trong TNT.

Mô hình | Độ phân giải | Params (M) | FLOPs (B) | Top-1 (%) | Top-5 (%)
TNT-S | 224×224 | 23.8 | 5.2 | 81.5 | 95.7
TNT-S + SE | 224×224 | 24.7 | 5.2 | 81.7 | 95.7

11

--- TRANG 11 ---
A.3 Phát hiện đối tượng với Faster RCNN
Như một mạng backbone tổng quát, TNT cũng có thể được áp dụng với các mô hình thị giác đa tỷ lệ như Faster RCNN [29]. Chúng tôi trích xuất các đặc trưng từ các lớp khác nhau của TNT để xây dựng các đặc trưng đa tỷ lệ. Cụ thể, FPN lấy 4 mức đặc trưng (1/4, 1/8, 1/16, 1/32) làm đầu vào, trong khi độ phân giải đặc trưng của mỗi khối TNT là 1/16. Chúng tôi chọn 4 lớp từ nông đến sâu (thứ 3, 6, 9, 12) để tạo thành biểu diễn đa cấp. Để khớp hình dạng đặc trưng, chúng tôi chèn các lớp deconvolution/convolution với stride phù hợp. Chúng tôi đánh giá TNT-S và DeiT-S trên Faster RCNN với FPN [21]. Mô hình DeiT được sử dụng theo cách tương tự. Kết quả COCO2017 val được hiển thị trong Bảng 13. TNT đạt được hiệu suất tốt hơn nhiều so với backbone ResNet và DeiT, cho thấy khả năng tổng quát hóa của nó cho framework giống FPN.

Bảng 13: Kết quả phát hiện đối tượng Faster RCNN trên tập COCO minival với tiền huấn luyện ImageNet. †Kết quả từ triển khai của chúng tôi.

Backbone | Params (M) | Epochs | AP | AP50 | AP75 | APS | APM | APL
ResNet-50 [21, 5] | 41.5 | 12 | 37.4 | 58.1 | 40.4 | 21.2 | 41.0 | 48.1
DeiT-S† [35] | 46.4 | 12 | 39.9 | 62.8 | 42.6 | 23.4 | 42.5 | 54.0
TNT-S | 48.1 | 12 | 41.5 | 64.1 | 44.5 | 25.7 | 44.6 | 55.4

Tài liệu tham khảo
[1] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. Trong NeurIPS, 2020.
[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. Trong ECCV, 2020.
[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, và Wen Gao. Pre-trained image processing transformer. Trong CVPR, 2021.
[5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, và cộng sự. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, và Ilya Sutskever. Generative pretraining from pixels. Trong ICML, 2020.
[7] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, và Huaxia Xia. Do we really need explicit position encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.
[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, và Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. Trong CVPR Workshops, 2020.
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. Trong NAACL-HLT (1), 2019.
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, và cộng sự. An image is worth 16x16 words: Transformers for image recognition at scale. Trong ICLR, 2021.
[11] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, và cộng sự. A survey on vision transformer. arXiv preprint arXiv:2012.12556, 2020.
[12] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, và Chang Xu. Ghostnet: More features from cheap operations. Trong CVPR, 2020.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong CVPR, 2016.
[14] Dan Hendrycks và Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.
[15] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, và Daniel Soudry. Augment your batch: Improving generalization through instance repetition. Trong CVPR, 2020.
[16] Jie Hu, Li Shen, và Gang Sun. Squeeze-and-excitation networks. Trong CVPR, 2018.
[17] Huawei. Mindspore. https://www.mindspore.cn/, 2020.

12

--- TRANG 12 ---
[18] Alex Krizhevsky, Geoffrey Hinton, và cộng sự. Learning multiple layers of features from tiny images. 2009.
[19] Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Trong NeurIPS, trang 1097–1105, 2012.
[20] Gustav Larsson, Michael Maire, và Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016.
[21] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, và Serge Belongie. Feature pyramid networks for object detection. Trong CVPR, 2017.
[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. Microsoft coco: Common objects in context. Trong ECCV, trang 740–755, 2014.
[23] Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
[24] Paul Michel, Omer Levy, và Graham Neubig. Are sixteen heads really better than one? Trong NeurIPS, 2019.
[25] Maria-Elena Nilsback và Andrew Zisserman. Automated flower classification over a large number of classes. Trong 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, trang 722–729. IEEE, 2008.
[26] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, và CV Jawahar. Cats and dogs. Trong CVPR, trang 3498–3505. IEEE, 2012.
[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, và cộng sự. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019.
[28] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, và Piotr Dollár. Designing network design spaces. Trong CVPR, 2020.
[29] Shaoqing Ren, Kaiming He, Ross Girshick, và Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Trong Advances in neural information processing systems, trang 91–99, 2015.
[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, và cộng sự. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.
[31] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, và Zbigniew Wojna. Rethinking the inception architecture for computer vision. Trong CVPR, 2016.
[32] Mingxing Tan và Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. Trong ICML, 2019.
[33] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, và Yunhe Wang. Augmented shortcuts for vision transformers. arXiv preprint arXiv:2106.15941, 2021.
[34] Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, và Chang Xu. Manifold regularized dynamic network pruning. Trong CVPR, trang 5018–5028, 2021.
[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. Trong ICML, 2021.
[36] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, và Hervé Jégou. Grafit: Learning fine-grained image representations with coarse labels. arXiv preprint arXiv:2011.12982, 2020.
[37] Laurens Van der Maaten và Geoffrey Hinton.. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.
[38] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, và Serge Belongie. The inaturalist species classification and detection dataset. Trong CVPR, trang 8769–8778, 2018.
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. NeurIPS, 2017.
[40] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, và Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. Trong ICCV, 2021.
[41] Xiaolong Wang, Ross Girshick, Abhinav Gupta, và Kaiming He. Non-local neural networks. Trong CVPR, trang 7794–7803, 2018.
[42] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, và Ping Luo. Segmenting transparent object in the wild with transformer. Trong IJCAI, 2021.
[43] Yixing Xu, Yunhe Wang, Kai Han, Yehui Tang, Shangling Jui, Chunjing Xu, và Chang Xu. Renas: Relativistic evaluation of neural architecture search. Trong CVPR, trang 4411–4420, 2021.

13

--- TRANG 13 ---
[44] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, và Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.
[45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, và Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. Trong ICCV, 2019.
[46] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, và David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.
[47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, và cộng sự. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Trong CVPR, 2021.
[48] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, và Yi Yang. Random erasing data augmentation. Trong AAAI, tập 34, trang 13001–13008, 2020.
[49] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, và Antonio Torralba. Scene parsing through ade20k dataset. Trong CVPR, 2017.
[50] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, và Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. Trong ICLR, 2021.

14
