# 2408.08454.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2408.08454.pdf
# File size: 1063809 bytes

===============================================
PDF FILE CONTENT
===============================================


--- TRANG 1 ---
Vượt ra ngoài Phân phối Truy vấn Đồng nhất: Cơ chế Chú ý Nhóm Truy vấn Dẫn hướng bởi Key
Zohaib Khan*, Muhammad Khaquan*, Omer Tafveez, Burhanuddin Samiwala, Agha Ali Raza
Lahore University of Management Sciences
{24100074, 25100095, 25020254, 25100255, agha.ali.raza }@lums.edu.pk
Tóm tắt
Kiến trúc Transformer đã cách mạng hóa deep learning thông qua cơ chế Self-Attention, có thể nắm bắt thông tin ngữ cảnh một cách hiệu quả. Tuy nhiên, lượng bộ nhớ của Self-Attention đặt ra những thách thức đáng kể cho các tác vụ chuỗi dài. Grouped Query Attention (GQA) giải quyết vấn đề này bằng cách nhóm các truy vấn và tính trung bình pooling các key-value heads tương ứng—giảm số lượng tham số tổng thể và yêu cầu bộ nhớ một cách linh hoạt mà không ảnh hưởng xấu đến độ chính xác của mô hình. Trong công trình này, chúng tôi giới thiệu các cải tiến cho GQA, tập trung vào hai phương pháp mới lạ khác biệt với tính chất tĩnh của việc nhóm: Key-Distributed GQA (KDGQA) và Dynamic Key-Distributed GQA (DGQA), tận dụng thông tin từ các norm của key heads để hướng dẫn phân bổ truy vấn. Cụ thể, KDGQA xem xét tỷ lệ của các norm của key heads trong mỗi lần forward pass, trong khi DGQA kiểm tra tỷ lệ của các norm khi chúng phát triển qua quá trình huấn luyện. Ngoài ra, chúng tôi trình bày Perturbed GQA (PGQA) như một nghiên cứu tình huống, giới thiệu sự biến đổi trong việc hình thành nhóm (tĩnh) thông qua việc trừ nhiễu từ các bản đồ chú ý. Các thí nghiệm với Vision Transformers được up-train, cho Phân loại Hình ảnh trên các bộ dữ liệu như CIFAR-10, CIFAR-100, Food101, và Tiny ImageNet, chứng minh tiềm năng của các biến thể này trong việc cải thiện GQA gốc thông qua các cơ chế nhóm có thông tin và thích ứng hơn: cụ thể ViT-L đạt được mức tăng độ chính xác lên đến 8% khi sử dụng DGQA so với GQA và các biến thể khác. Chúng tôi tiếp tục phân tích tác động của số lượng Key-Value Heads đến hiệu suất, nhấn mạnh tầm quan trọng của việc sử dụng ái lực query-key. Code có sẵn trên GitHub.1

1 Giới thiệu
Kể từ khi ra đời năm 2017, Transformers (Vaswani et al. 2023) đã trở thành phương pháp tiên tiến nhất trong các tác vụ mô hình hóa ngôn ngữ (Brown et al. 2020; Touvron et al. 2023). Gần đây hơn, chúng đã được điều chỉnh cho các tác vụ computer vision như phân loại hình ảnh và phân đoạn, vượt qua sự phụ thuộc lớn vào convolutions (Dosovitskiy et al. 2021; Liu et al. 2021; Dong et al. 2022). Khả năng mở rộng của Transformers cũng đã được thiết lập thực nghiệm bởi các định luật scaling (Hoffmann et al. 2022) cho thấy lớn hơn thực sự là tốt hơn trong paradigm này. Sự mở rộng về kích thước mô hình này thường xảy ra bằng cách tăng số lượng layers, hoặc chiều của các projections.

*Những tác giả này đóng góp như nhau.
1https://github.com/zohaib-khan5040/key-driven-gqa

Hình 1: Tổng quan về các cơ chế liên quan. MHA liên kết một truy vấn với một key-value head. GQA liên kết một key-value head duy nhất với các nhóm con của truy vấn sao cho kích thước nhóm vẫn không đổi và việc nhóm được thực hiện theo cách tĩnh/đồng nhất. Các phương pháp của chúng tôi, KDGQA/DGQA, thực hiện việc nhóm theo các norm của key heads theo cách không đồng nhất - lưu ý cách các keys tối hơn (cho thấy norm cao hơn) có nhiều truy vấn được gán hơn so với những cái sáng hơn.

Tăng kích thước mô hình tuy nhiên đi kèm với chi phí tăng yêu cầu bộ nhớ và tính toán. Các mô hình lớn hơn cũng gây ra tắc nghẽn suy luận vì độ phức tạp tính toán của Self-Attention tăng bậc hai với số lượng token. Những hạn chế này, cùng với thực tế rằng hầu hết các mô hình transformer lớn đôi khi được tham số hóa quá mức (Hoffmann et al. 2022) đã tạo ra bối cảnh cho các kỹ thuật nén mô hình khác nhau nhắm mục tiêu cụ thể vào module Self-Attention.

Một trong những scheme phổ biến đầu tiên là Multi-Query Attention (Shazeer 2019) gợi ý rằng trong mỗi khối self-attention, việc chỉ sử dụng một key head duy nhất giảm đáng kể các tham số mà không gây ra sụt giảm đáng kể về độ chính xác. Sau đó, Grouped-Query Attention (GQA) đã được giới thiệu (Ainslie et al. 2023) như một phép nội suy giữa Multi-Head Attention (MHA) và Multi-Query Attention (MQA). GQA được chứng minh đạt hiệu suất gần với MHA, trong khi gần như nhanh bằng MQA, chỉ sử dụng một key và value head duy nhất cho mỗi nhóm con của query heads. Kể từ đó, đã có nhiều phương pháp sửa đổi kiến trúc GQA để giảm thiểu thêm sự sụt giảm độ chính xác (Javadi et al. 2023; Chinnakonduru and Mohapatra 2024; Chen et al. 2024).

Chúng tôi đề xuất các mở rộng riêng cho kiến trúc GQA cố gắng làm mềm sự sụt giảm độ chính xác bằng cách làm cho cơ chế nhóm trở nên động và có thông tin hơn, được áp dụng cho phân loại hình ảnh cho Vision Transformers (Dosovitskiy et al. 2021). Cụ thể, chúng tôi khám phá khả năng và tiềm năng của việc sử dụng một heuristic đơn giản như các L2-norms (như các thước đo tầm quan trọng) của key heads để hướng dẫn quá trình phân bổ nhóm, trong quá trình huấn luyện, trong các phương pháp chúng tôi gọi là Key-Distributed GQA (KDGQA) và Dynamic Key-Distributed GQA (DGQA) - cái trước giới hạn kiến thức về các norm trong lần forward pass hiện tại, trong khi cái sau kiểm tra các tỷ lệ khi chúng phát triển qua quá trình huấn luyện (Hình 1 minh họa sự khác biệt trong việc nhóm). Chúng tôi cũng xem xét Perturbed GQA (PGQA) như một trường hợp thú vị của việc giới thiệu Gaussian Noise vào các bản đồ chú ý GQA.

2 Nghiên cứu liên quan
Tính chất toàn cục của Self-Attention, nơi có số lượng truy vấn và keys/values bằng nhau, đã làm cho việc huấn luyện và triển khai transformers ngày càng tốn kém về mặt tính toán. GQA đã giảm thiểu điều này ở một mức độ nào đó, cho phép hình thành các nhóm có kích thước cố định của query heads và liên kết một key-value head duy nhất với mỗi nhóm (Ainslie et al. 2023): bài báo thảo luận về việc sử dụng các checkpoint MHA hiện có, và chuyển đổi chúng sang kiến trúc GQA bằng cách mean-pooling các key và value heads liên kết với mỗi nhóm, tương ứng. Nhiều công trình gần đây đã nổi lên tìm hiểu về kết nối và động lực forward pass của cơ chế Attention, vắt kiệt hiệu suất hơn từ mô hình, thường với cùng số lượng tham số.

Grouped Query Key Value Attention (Javadi et al. 2023) nổi lên như một phương pháp thống nhất bao gồm các biến thể như Multi-Key Value Attention (MKVA) và Grouped Key Value Attention (GKVA). Trong khi MKVA và GKVA giới thiệu các chiến lược mới bằng cách nhóm keys và values thành các nhóm riêng biệt và chia sẻ truy vấn trong các nhóm này, GQKVA phân chia truy vấn và cặp key-value, tối ưu hóa cả số lượng tham số và hiệu quả tính toán. Nó khám phá phương pháp tổ hợp chia ma trận Q thành g nhóm và ma trận K, V thành gkv nhóm, cho phép số lượng linh hoạt của truy vấn, keys, và values trong mỗi phân vùng. Phương pháp này sử dụng dot-product attention cho mỗi kết hợp duy nhất của các nhóm này (Qi, KVj), đảm bảo đầu ra riêng biệt trên nhiều heads.

Chen et al. (2024) đề xuất AsymGQA và điều tra việc phân bổ không đồng nhất của heads bằng cách kết hợp các projections tương tự vào các clusters được xây dựng đồng nhất. Ví dụ, xem xét một tình huống nơi tập hợp keys K được phân chia thành ba nhóm, mỗi nhóm chứa hai keys. Trong phương pháp này, sự tương tự của một key được chọn ngẫu nhiên được tính toán đối với tất cả keys trong các nhóm khác. Key sau đó được chèn vào nhóm có sự tương tự cao nhất. Điều này cũng đã được thử nghiệm ở cấp độ activation, nơi đầu ra của mỗi layer được sử dụng để tính toán sự tương tự để hình thành clusters của heads, hoạt động tốt hơn clustering cấp Weight.

Tương tự, Joshi et al. (2024) đề xuất Quality and Capacity-aware GQA (QCQA), sử dụng một loss tối thiểu hóa được gọi là Weight-Sharing Error, là khoảng cách giữa các phân phối head K và V để tối thiểu hóa sự sụt giảm độ chính xác do việc nhóm truy vấn. Phương pháp này tuân theo framework tìm kiếm hai giai đoạn, đầu tiên hình thành các nhóm truy vấn Q của mỗi layer một cách độc lập. Điều này được theo sau bằng việc đánh giá các nhóm của mỗi layer sử dụng Weight Sharing Error và KV cache như các hàm fitness để loại bỏ các layers có tác động lớn nhất đến độ chính xác. Các layers thể hiện thiệt hại đáng kể đến độ chính xác được giữ lại như các dạng MHA trong khi những cái khác được nhóm, hoàn thiện việc lựa chọn các layers sẽ bảo tồn các nhóm này với các layers không được chọn được cấu hình thành MHA. Giai đoạn đầu tập trung vào tối ưu hóa nhóm cho mỗi layer riêng lẻ, trong khi giai đoạn thứ hai sử dụng Non-dominated Sorting Genetic Algorithm II để tinh chỉnh các nhóm này một cách lặp đi lặp lại.

Weighted GQA (Chinnakonduru and Mohapatra 2024) giới thiệu một tham số scalar hoặc vector có thể học được cho các heads K và V cho (w1,k, w2,k..wh,k) và (w1,v, w2,v..wh,v). Nói cách khác, các keys và values tổng hợp được hình thành nơi mỗi cột của K và V là một kết hợp tuyến tính của các ma trận key và value K1, V1 thông qua Kh, Vh nơi kết hợp được kiểm soát bởi một tập hợp cụ thể của vector trọng số. Tổng có trọng số của phép toán này được sử dụng để sửa đổi K, V, sau đó được sử dụng cho self-attention.

3 Phương pháp
Phần này mô tả các phương pháp của chúng tôi để khác biệt với tính chất đồng nhất mặc định của việc phân bổ truy vấn trong Grouped-Query Attention.

3.1 Key-Distributed GQA (KDGQA)
Phương pháp này là nỗ lực đầu tiên của chúng tôi trong việc phân bổ query heads cho mỗi nhóm theo cách động hơn. Trong KDGQA, thay vì gán truy vấn đồng nhất cho các nhóm, chúng tôi sử dụng độ lớn của các key vectors, được đo bằng L2-norms của chúng như một proxy cho tầm quan trọng của chúng, để thông báo cho việc phân phối truy vấn qua các nhóm khác nhau. Chúng tôi đặt giả thuyết rằng việc phân bổ một số query heads tỷ lệ với tầm quan trọng tương đối của chúng (trong layer đó) là có lợi cho hiệu suất mô hình.

Cho một tập hợp key vectors K∈RG×dk, nơi G là số lượng nhóm (hoặc key-value heads) và dk là chiều của key vectors, norm của mỗi key vector có thể được tính toán như sau:

ng=∥Kg∥2=vuutdkX
i=1K2
g,i cho g= 1, . . . , G (1)

Ở đây, ng là norm của key vector thứ g Kg.

Để đảm bảo rằng các norm được scaled trong phạm vi [0,1] để dễ diễn giải và phân bổ hơn, chúng tôi áp dụng min-max scaling cho vector của norms n= [n1, n2, . . . , nG]:

ˆng=ng−min(n)
max(n)−min(n)(2)

--- TRANG 2 ---
Nơi:
•ˆng là norm được scaled cho nhóm thứ g.
•min(n) và max(n) đại diện cho các giá trị tối thiểu và tối đa trong vector n.

Chúng tôi thấy rằng min-max scaling như được mô tả trong phương trình 2 giúp nén các norm thành một phạm vi chặt hơn, khuếch đại sự khác biệt và dẫn đến kích thước nhóm không đồng nhất hơn, so với việc trực tiếp lấy tỷ lệ.

Cuối cùng, số lượng truy vấn, Qg, được phân bổ cho mỗi key head có thể được xác định bằng cách chuẩn hóa các norm đã scaled và nhân với tổng số truy vấn Nq:

Qg=$
ˆng·NqPG
g=1ˆng%
(3)

Nơi:
•Nq là tổng số truy vấn.
•Qg đại diện cho số lượng truy vấn được phân bổ cho nhóm thứ g.

KDGQA thực hiện quy trình này để tìm số lượng query heads cần phân bổ, sau đó hình thành các nhóm theo cách từ trái sang phải, trong mỗi lần forward pass, bất kể huấn luyện hay suy luận, gây ra một overhead nhỏ mà chúng tôi thảo luận sau (Phần 5.2).

3.2 Dynamic Key-Distributed GQA (DGQA)
Dynamic Key-Distributed Grouped Query Attention (DGQA) mở rộng các nguyên tắc của KDGQA bằng cách không chỉ xem xét các norm của key vectors tại một thời điểm, mà bằng cách điều chỉnh động các thước đo tầm quan trọng này trong quá trình huấn luyện. Ý tưởng cốt lõi của DGQA là cho phép việc nhóm truy vấn thích ứng dựa trên các đặc tính phát triển của key vectors khi mô hình học. Sự thích ứng động này nhằm nắm bắt và tận dụng hiệu quả hơn cấu trúc cơ bản của dữ liệu, dẫn đến hiệu suất cải thiện trong các cơ chế attention.

Cơ chế cơ bản dựa vào việc định nghĩa một kích thước cửa sổ: một số lượng iteration cố định ở đầu mà chúng tôi cập nhật các phân bổ truy vấn và duy trì trong suốt khoảng thời gian đó. Để lượng hóa một số thước đo về cách các keys phát triển (dựa trên độ lớn của chúng), chúng tôi đề xuất hai phương pháp:

1.Difference-based Evolution.
Chúng tôi cache các norm của key heads (như được tính toán trong phương trình 1) ở đầu mỗi cửa sổ/interval t,c(t)g. Khi quá trình huấn luyện tiến triển, chúng tôi lượng hóa tầm quan trọng của mỗi nhóm bằng cách đo sự khác biệt tuyệt đối giữa norm được cache (đến từ interval trước) và cái hiện tại:

∆n(t)g=n(t)g−c(t−1)g cho g= 1, . . . , G (4)

nơi ∆n(t)g đại diện cho sự thay đổi trong norm cho key head thứ g và interval thứ t.

Sau đó chúng tôi sử dụng sự khác biệt này để hình thành thước đo tầm quan trọng của chúng tôi, dg, cho mỗi key head:

dg=∆n(t)g (5)

Các giá trị được cache sau đó được cập nhật khi interval tiếp theo bắt đầu:

c(t)g=n(t)g cho g= 1, . . . , G (6)

2.Exponential Moving Average-based Evolution.
Phương pháp này rất tương tự với cái trước về mặt thiết lập window-size, sử dụng cache của các norm, và cập nhật cache. Sự khác biệt chính nằm ở thước đo tầm quan trọng, dg: thay vì lấy sự khác biệt tuyệt đối của các norm, chúng tôi lấy exponential moving average (EMA) của hai cái:

c(t)g=α·n(t)g+(1−α)·c(t−1)g cho g= 1, . . . , G (7)

nơi α là một hyperparameter xác định tầm quan trọng của giá trị hiện tại so với moving average trong việc xác định average mới.

Giá trị cached mới này sẽ được sử dụng để xác định các phân bổ truy vấn (lưu ý chúng tôi không cần lấy giá trị tuyệt đối ở đây):

dg=c(t)g cho g= 1, . . . , G (8)

Cả hai phương pháp đều đơn giản tìm cách tính toán dg (g= 1, . . . , G). Sử dụng điều này, số lượng truy vấn được phân bổ trong mỗi nhóm, Qg, có thể được tính toán theo cùng cách như phương trình 3, thay thế cho ˆng.

Các thí nghiệm của chúng tôi đã thiết lập sự vượt trội của biến thể EMA so với biến thể difference: 84% vs 71% tương ứng, cho kích thước cửa sổ 300, trên CIFAR-100 cho 10 epochs, từ một checkpoint MHA đã chuyển đổi. Điều này có thể được quy cho những điều sau (Hình 2):

•Smooth Adaptation to Changing Magnitudes
Hiệu ứng averaging của EMA giúp giảm thiểu ảnh hưởng của các outliers hoặc thay đổi đột ngột, dẫn đến phân bổ head mạnh mẽ và đáng tin cậy hơn so với phương pháp difference, đặc biệt trong các tình huống nơi các norm của keys phát triển nhanh chóng.

•Reduced Sensitivity to Noise
Bằng cách kết hợp thông tin quá khứ vào cập nhật hiện tại, EMA giảm độ nhạy cảm với các pattern nhiễu hoặc thoáng qua trong dữ liệu.

Cho phần còn lại của bài báo, lưu ý rằng trừ khi được chỉ định khác, phương pháp DGQA đề cập đến biến thể EMA. Chúng tôi thực hiện tìm kiếm lưới để tìm kích thước cửa sổ hoạt động tốt nhất trên CIFAR-100, được tìm thấy là 300 - điều này có nghĩa là scheme phân bổ truy vấn của mô hình được cập nhật mỗi 300 bước huấn luyện, có thể bị ảnh hưởng bởi kích thước của bộ dữ liệu. Chúng tôi chọn bỏ qua tìm kiếm tinh hơn này do hạn chế thời gian.

3.3 Perturbed GQA (PGQA)
Thông qua các thí nghiệm, MHA có bằng chứng rõ ràng về việc mỗi head tương tự với chính nó hơn so với các heads khác, trong khi GQA không duy trì được sự mạch lạc này. Thường thì, đầu ra attention của một head trong GQA có thể tương tự với các heads khác trong nhóm của nó hơn, chính nó dẫn đến bias tương tự intra-group. Mặc dù các định luật scaling của các mô hình lớn cho thấy ít hoặc không có sự suy giảm hiệu suất, bias tương tự góp phần làm cho GQA tụt lại so với MHA về hiệu suất.

Do đó, chúng tôi đề xuất Perturbed GQA loại bỏ bias tương tự này bằng cách giới thiệu một ma trận Gaussian Noise trong đầu ra attention.

Ma trận nhiễu được tính toán riêng biệt cho mỗi nhóm g bằng cách tính mean µg và standard deviation σg cho đầu ra attention ˆA của mỗi nhóm. Một ma trận ngẫu nhiên Rg được tạo ra có cùng hình dạng với ˆAg và được chuẩn hóa để phù hợp với thống kê của ˆAg như sau:

Gaussiang=σg∗(Rg−µ(Rg))
σ(Rg)+µg cho g= 1, . . . , G
(9)

nơi Gaussiang là ma trận Gaussian Noise được tính toán cho mỗi nhóm. Để duy trì tính thưa thớt và tránh làm xáo trộn các pattern attention, chúng tôi đặt các phần tử đường chéo của ma trận bằng không.

Ma trận Gaussian Noise thu được được trừ khỏi đầu ra attention của mỗi nhóm. Trong khi đó, việc phân bổ nhóm được thực hiện với cùng cách từ trái-phải như GQA.

Hình 3 cho thấy PGQA giúp chúng tôi giảm thiểu bias tương tự intra-group. Tuy nhiên, việc giảm thiểu bias này cũng khiến sự tương tự của mỗi head với chính nó giảm đi. Trong Phụ lục C, chúng tôi cũng thảo luận về các pattern tương tự head được quan sát trong DGQA mà xấp xỉ một trung bình có trọng số của các heat maps từ MHA và GQA.

4 Thí nghiệm
4.1 Mô hình và Bộ dữ liệu
Chúng tôi chọn sử dụng Vision Transformers (Dosovitskiy et al. 2021) trong các thí nghiệm, tuân theo một scheme thí nghiệm tương tự Javadi et al. (2023). Lựa chọn này thuận lợi vì Vision Transformers được pretrain đi kèm với ba kích thước mô hình, có thể dễ dàng phù hợp với phần cứng cấp độ consumer. Các giá trị xấp xỉ cho kích thước mô hình, và số lượng heads chúng được cấu hình để huấn luyện, được hiển thị trong Bảng 1.

Hình 3: PGQA giảm thiểu bias tương tự từ GQA, tuy nhiên nó xóa bỏ các pattern tự tương tự được thấy trong attention map của MHA.

Sự linh hoạt này cho phép đánh giá mạnh mẽ hơn khi kết hợp với lựa chọn bộ dữ liệu của chúng tôi: CIFAR-10, CIFAR-100, Food101, và Tiny ImageNet (Krizhevsky (2009); Bossard, Guillaumin, and Van Gool (2014); mnmoustafa (2017)).

4.2 Chuyển đổi Checkpoint và Uptraining
Scheme "uptraining" được đề xuất trong bài báo GQA bởi Ainslie et al. (2023) mô tả việc điều chỉnh các checkpoint MHA đã pretrain sang kiến trúc GQA: điều này bao gồm việc xây dựng key và value heads của mỗi nhóm bằng cách mean-pooling tất cả các heads gốc trong nhóm đó, theo sau bởi việc tiếp tục pretrain để thu hẹp khoảng cách giữa GQA và MHA. Chúng tôi thực hiện chuyển đổi checkpoint sử dụng các checkpoint MHA của ViT từ thư viện timm (Wightman 2019).

Với mô hình đã chuyển đổi, chúng tôi thực hiện up-training theo cách tương tự như đã được mô tả trong bài báo ViT gốc. Chúng tôi sử dụng optimizer AdamW với learning rate 1×10−4, với β1= 0.9, β2= 0.999, ϵ= 1×10−8 và weight decay 0.01. Chúng tôi huấn luyện trong 1 epoch trên bộ dữ liệu ImageNet-1k (Russakovsky et al. 2015), với hơn 1.3M hình ảnh huấn luyện. Điều này được thực hiện với batch size 32 trên một GPU NVIDIA GeForce RTX 4090 24GB duy nhất.

Lưu ý rằng vì tất cả các mô hình có cùng số lượng tham số, giữ kích thước (small, base, large) và bộ dữ liệu cố định, sự khác biệt duy nhất trong uptraining FLOPs xuất phát từ các tính toán bổ sung trong việc tìm số lượng truy vấn cần phân bổ cho mỗi nhóm.

Để up-training, chúng tôi áp dụng augmentation ngẫu nhiên cho các hình ảnh trước khi chúng được resize thành kích thước hình ảnh (224,224,3). Điều này để giảm thiểu overfitting là một thách thức lớn khi huấn luyện ViTs - các augmentations được ưa thích hơn so với các chiến lược regularization như đã thảo luận bởi Steiner et al. (2022).

Lưu ý rằng cho uptraining và cho fine-tuning, chúng tôi chọn đối đầu các phiên bản tốt nhất của biến thể chúng tôi với phiên bản tốt nhất của GQA, tức là với số lượng key-value heads tối đa. Điều này cho phép thu hẹp khoảng cách tối đa giữa GQA (cộng với các biến thể của nó) và MHA: với H heads cho một mô hình đã cho, chúng tôi đi với H/2 key-value heads để đảm bảo công bằng, vì đó là ceiling của những gì được phép với GQA. Chúng tôi cũng kiểm tra tác động của việc thay đổi số lượng key-value heads đến hiệu suất trong Hình 4.

4.3 Fine-tuning
Để đánh giá các mô hình, chúng tôi load các checkpoint mô hình từ giai đoạn uptraining. Điều quan trọng cần lưu ý là tất cả các mô hình trải qua cùng số lượng training steps, vì vậy không có lợi thế nào được trao cho các biến thể của chúng tôi mặc dù có các sửa đổi được thực hiện. Không có cơ hội bổ sung nào được trao cho các biến thể của chúng tôi để khôi phục hiệu suất mô hình.

Mỗi mô hình được fine-tuned trong 5 epochs. Optimizer, phần cứng, chiến lược augmentation, và batch size giống với các lần uptraining, ngoại trừ learning rate được đặt thành 1×10−5 cho fine-tuning.

5 Kết quả và Thảo luận
5.1 Đánh giá
Kết quả từ các lần uptraining được hiển thị trong Bảng 2. Kết quả cốt lõi của chúng tôi được hiển thị trong Bảng 3, nơi chúng tôi uptrain mỗi mô hình và biến thể, và Bảng 4, nơi chúng tôi sử dụng checkpoint GQA đã uptrain và chỉ giới thiệu các sửa đổi của chúng tôi trong quá trình fine-tuning. Cho các bảng này, GQA và các biến thể của nó có cấu hình sau: ViT-S sử dụng 3 key-value heads, ViT-B sử dụng 6 key-value heads, và ViT-L sử dụng 8 key-value heads. Benchmark của chúng tôi là biến thể GQA, và hiệu suất của các biến thể MHA đã được cung cấp để tham khảo.

Mô hình	Biến thể	Độ chính xác
ViT-S	GQA	65.95
	KDGQA	51.85
	DGQA	65.66
	PGQA	59.55
ViT-B	GQA	48.43
	KDGQA	43.69
	DGQA	45.53
	PGQA	44.59
ViT-L	GQA	52.49
	KDGQA	37.21
	DGQA	59.13
	PGQA	49.94

Bảng 2: Độ chính xác validation sau 1 epoch uptraining trên ImageNet-1k. Các mô hình vượt trội hơn GQA trong danh mục của chúng được highlight in đậm.

Từ Bảng 3, điểm quan trọng nhất là DGQA luôn vượt trội hơn benchmark GQA khi sử dụng kiến trúc ViT-L đã uptrain, và với biên độ khá lớn, lên đến 8% như trường hợp của Tiny ImageNet.

Rất thú vị khi lưu ý rằng đối với ViT-S và ViT-B, DGQA luôn kém hiệu suất so với GQA - chúng tôi muốn bao gồm những thí nghiệm thất bại này trong đánh giá của chúng tôi để làm nổi bật tác động của số lượng key-value heads và kích thước mô hình đến sự thay đổi hiệu suất. Vì DGQA được thiết kế như một cải tiến so với KDGQA, không có gì ngạc nhiên khi cái sau hoạt động kém trong đánh giá cuối cùng của chúng tôi, cũng như pattern tương tự trong các thí nghiệm sơ bộ.

Một pattern thú vị khác là độ phức tạp của tác vụ phân loại: nếu chúng tôi tập trung vào ViT-L, chúng tôi thấy (xấp xỉ) rằng có mức tăng 1.5% về độ chính xác cho CIFAR-10, mức tăng 5% cho CIFAR-100 và Food101 (lưu ý chúng có 100 và 101 lớp tương ứng), và 8% cho Tiny ImageNet (có 200 lớp). Điều này, cùng với lưu ý từ đoạn trước, dẫn chúng tôi đến giả thuyết rằng lợi ích thực sự của các phương pháp mới, được dẫn xuất từ việc tách nhánh từ một base đã pretrain, chỉ xuất hiện ở quy mô lớn hơn, tức là với các mô hình lớn hơn và bộ dữ liệu lớn hơn.

Trong khi PGQA có thể kém hiệu suất so với benchmark trong các đánh giá của chúng tôi, nó tuân theo pattern tương tự ở trên (biên độ hẹp hơn khi mở rộng), và trình bày một nghiên cứu tình huống hấp dẫn. Mặc dù hiệu suất kém, PGQA cung cấp những hiểu biết có giá trị về sự (thiếu) kiên cường của các cơ chế attention.

--- TRANG 3 ---
Mô hình	Biến thể	Độ chính xác
		CIFAR-10	CIFAR-100	Food101	Tiny ImageNet
ViT-S	MHA	98.42	90.94	87.29	86.51
	GQA	97.27	84.70	80.95	80.23
	KDGQA	94.05	71.32	72.04	63.66
	DGQA	97.27	84.55	80.77	80.16
	PGQA	95.81	78.42	76.55	73.81
ViT-B	MHA	98.67	91.91	90.17	91.04
	GQA	93.32	71.78	72.42	62.87
	KDGQA	91.33	67.84	68.01	57.90
	DGQA	91.95	69.63	70.59	59.55
	PGQA	91.84	67.86	68.31	59.27
ViT-L	MHA	99.10	94.05	91.94	92.60
	GQA	94.81	76.41	74.57	67.73
	KDGQA	88.87	64.50	62.90	52.49
	DGQA	96.36	81.67	79.93	75.83
	PGQA	93.21	75.03	73.33	66.78

Bảng 3: Độ chính xác test của các lần fine-tuning, post-uptraining cho mỗi biến thể. Các mô hình vượt trội hơn GQA trong danh mục của chúng được highlight in đậm.

Mô hình	Biến thể	Độ chính xác
		CIFAR-10	CIFAR-100	Food101	Tiny ImageNet
ViT-S	MHA	98.42	90.94	87.29	86.51
	GQA	97.27	84.70	80.95	80.23
	KDGQA	93.57	64.34	71.02	65.83
	DGQA	97.09	84.54	80.81	80.03
	PGQA	95.81	78.68	75.44	75.79
ViT-B	MHA	98.67	91.91	90.17	91.04
	GQA	93.32	71.78	72.42	62.87
	KDGQA	90.36	65.64	65.99	56.42
	DGQA	92.80	70.17	71.70	61.42
	PGQA	91.85	68.19	68.73	59.75
ViT-L	MHA	99.10	94.05	91.94	92.60
	GQA	94.81	76.41	74.57	67.73
	KDGQA	86.69	63.34	67.18	61.67
	DGQA	93.77	75.83	73.79	66.04
	PGQA	94.19	73.66	71.91	66.84

Bảng 4: Độ chính xác test của các lần fine-tuning, post-uptraining chỉ sử dụng checkpoint GQA.

anisms. Nó gợi ý rằng ngay cả những perturbations nhỏ cũng có thể tác động đến các phân phối attention, đặc biệt khi mô hình đã được điều chỉnh để hoạt động theo cách cụ thể. Phát hiện này nhấn mạnh tầm quan trọng của uptraining rộng rãi khi thực hiện những sửa đổi như vậy đối với các mô hình đã pretrain, vì động lực forward pass bị thay đổi đáng kể, đòi hỏi những điều chỉnh bổ sung để khôi phục hiệu suất mô hình.

Từ Bảng 4, có thể thấy rằng các biến thể của chúng tôi hoạt động tệ hơn ở quy mô tuyệt đối, bất kể so sánh với GQA. Điều này làm nổi bật rằng uptraining đặc biệt phải được tiến hành cho các biến thể là sửa đổi của GQA, đến từ chuyển đổi sang checkpoint MHA.

5.2 Thời gian Suy luận
Bảng 5 trình bày so sánh thời gian suy luận cho các mô hình base với GQA làm baseline. PGQA thể hiện mức tăng cao nhất về thời gian suy luận là 3.31%, trong khi DGQA và KDGQA cho thấy mức tăng tối thiểu là 0.45% và 0.21% tương ứng.

5.3 Độ nhạy cảm với Huấn luyện
Kết quả của chúng tôi cho thấy rõ ràng rằng các nhóm bất đối xứng và phân bổ thông tin activation có thể giúp cải thiện hiệu suất. Tuy nhiên, có nhiều cân nhắc khi nhằm cải thiện các cơ chế cho các mô hình hiện có.

Từ phần trước, chúng tôi lại chỉ ra rằng quy mô/kích thước của bộ dữ liệu và mô hình đóng vai trò quan trọng trong việc xác định liệu các biến thể có khả năng vượt trội hơn benchmark hay không. Với các mô hình lớn hơn, có số lượng heads lớn hơn, có nghĩa là có ceiling lớn hơn về số lượng key-value heads mà chúng ta có thể tìm hiểu. Số lượng key-value heads lớn hơn cho phép các cơ chế phân bổ thông tin thực sự tỏa sáng, vì có quá ít key-value heads ngụ ý có biên độ lỗi hẹp hơn nhiều trong việc hình thành nhóm. Điều này có thể sẽ ít là vấn đề khi áp dụng cho Transformers cho các tác vụ ngôn ngữ tự nhiên vì nhiều benchmarks và mô hình phổ biến thường rất lớn về quy mô (Hoffmann et al. 2022).

Đây không phải là sự xuất hiện cô lập của độ nhạy cảm của một phương pháp mới đối với quy mô của mô hình và bộ dữ liệu. Dosovitskiy et al. (2021) đưa ra những lưu ý tương tự trong bài báo của họ về Vision Transformer khi thảo luận về yêu cầu dữ liệu pretraining: ViT-L kém hiệu suất so với ViT-B trên ImageNet-1k, hoạt động tương tự trên ImageNet-21k, và vượt trội trên JFT-300M. Chúng tôi giả thuyết rằng pattern này có thể được ngoại suy sang nghiên cứu của chúng tôi trong đó cần nhiều hiệu chuẩn hơn để các phương pháp nhóm thông tin activation vượt qua phương pháp nhóm tĩnh.

Cũng đáng chỉ ra lại rằng KDGQA và DGQA có cùng ý tưởng cơ bản, nhưng cái sau có góc nhìn hơi khác trong một bước của tính toán. Mặc dù sự khác biệt nhỏ này, rất rõ ràng rằng DGQA tốt hơn đáng kể so với KDGQA, và chỉ với biến thể EMA.

Một thách thức khác mà chúng tôi gặp phải là learning rates. Chúng tôi thấy rằng learning rate 1×10−3 quá cao cho các biến thể của chúng tôi cụ thể, phá vỡ hiệu suất của chúng trên tất cả bộ dữ liệu. Learning rate thấp hơn và phổ biến hơn là 3×10−4 hoạt động tốt hơn cho PGQA, nhưng vẫn phá vỡ DGQA. Cuối cùng chúng tôi bắt đầu thấy kết quả hợp lý với 1×10−4, và ổn định với việc sử dụng 1×10−5 (xem Phụ lục A).

Thành phần cuối cùng trong công thức huấn luyện một biến thể của mô hình (đã chuyển đổi) đã pretrain là bộ dữ liệu uptraining. Cho các thí nghiệm sơ bộ, chúng tôi tìm hiểu việc huấn luyện GQA và các biến thể của nó

1. Từ đầu,
2. Chuyển đổi từ checkpoint MHA nhưng không uptraining,
3. Uptraining trên CINIC-10 trong 10 epochs,
4. Uptraining trên ImageNet-1k trong 1 epoch.

Chúng tôi thấy rằng kết quả sử dụng các biến thể của chúng tôi khá kém khi các mô hình không được uptrain đúng cách: cụ thể khi chúng tôi sử dụng phiên bản sửa đổi của CINIC-10 (Darlow et al. 2018), có 270000 hình ảnh huấn luyện. Việc thiếu đa dạng và độ phân giải thô thấp đã làm cho điều này trở thành lựa chọn kém, trở nên rõ ràng khi chúng tôi uptrain trên ImageNet-1k chỉ trong một epoch. Mặc dù số lượng training steps trong thiết lập thứ tư ít hơn một nửa so với thứ ba, tác động của chất lượng uptraining là đáng kể vì đó là khi chúng tôi bắt đầu thấy các biến thể của chúng tôi vượt trội hơn benchmark GQA. Do hạn chế thời gian và tài nguyên, chúng tôi không thể thí nghiệm với uptraining trên ImageNet-21k hoặc JFT-300M, hoặc hiệu chuẩn dài hơn trên ImageNet-1k.

Chúng tôi khám phá ý tưởng này sâu hơn trong Phụ lục B.

6 Kết luận
Chúng tôi giới thiệu KDGQA, DGQA, và PGQA: các chiến lược để vượt qua chiến lược nhóm tĩnh như đã được tìm thấy trong GQA. Chúng tôi thấy rằng DGQA, là phiên bản tinh chỉnh của KDGQA, vượt trội đáng kể so với GQA đã uptrain trên ViT-L, và cho thấy mức tăng hiệu suất lớn hơn cho các bộ dữ liệu tinh vi hơn. Điều này cho thấy tiềm năng của hiệu chuẩn cho các cơ chế nhóm thông tin và thích ứng hơn.

7 Nghiên cứu Tương lai
Vì các thí nghiệm của chúng tôi đã chứng minh tính khả thi của việc cải thiện hiệu suất của Transformers sử dụng GQA thông qua cơ chế nhóm thông tin hơn và khác biệt với các phân bổ đồng nhất, bước đầu tiên sẽ là mở rộng những điều này sang Decoders. Với sự phổ biến của các tác vụ Ngôn ngữ Tự nhiên ngày nay, sẽ là một loạt thí nghiệm thú vị để tìm hiểu với các LLMs có hàng tỷ tham số và xử lý các tác vụ có chiều nội tại cao hơn nhiều, như mô hình hóa ngôn ngữ. Dựa trên kinh nghiệm của chúng tôi, chúng tôi tin rằng sự kết hợp này có thể dẫn đến mức tăng hiệu suất thậm chí lớn hơn với việc sử dụng các cơ chế nhóm thông tin.

Ngay cả trong framework model-agnostic, có những hướng thú vị mà chúng tôi dự định khám phá. Thứ nhất, chúng tôi lưu ý rằng việc sử dụng các norm của Keys, hoặc thao tác các attention maps, vẫn không thực sự cho phép các activations giao tiếp với nhau. Cụ thể, chúng tôi dự định khám phá liệu có thể tận dụng ái lực giữa queries và keys, và keys và values, để cải thiện cơ chế nhóm thậm chí nhiều hơn. Chúng tôi cũng có thể chuyển hướng từ chỉ các phương pháp thông tin activation, và xem xét các phương pháp thông tin weight, để quyết định các phân bổ độc lập với input. Đây là những hướng trong cùng khu vực với Chen et al. (2024), nhưng chúng tôi muốn có phương pháp ít ngẫu nhiên hơn để tìm các nhóm tối ưu và trao cho cả các biến thể và benchmark cùng số lượng training steps cho đánh giá công bằng.

Tài liệu tham khảo
Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy, Y.; Lebrón, F.; and Sanghai, S. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv:2305.13245.

--- TRANG 4 ---
Hình 2: Standard deviation của biến thể EMA phát sinh ít spiky hơn, hỗ trợ ý tưởng rằng nó mạnh mẽ hơn đối với nhiễu thoáng qua.

Mô hình	Số lượng Tham số (M)	Kích thước (MB)	Số lượng Heads
ViT-S	30	77	6
ViT-B	75	300	12
ViT-L	300	1060	16

Bảng 1: Tham số mô hình, kích thước, và số lượng heads. Lưu ý rằng (1) số lượng heads đến từ cấu hình mô hình đã pretrain, vì nó là một hyperparameter được đặt trước khi pretrain, và (2) Kích thước trong MB đến từ các thí nghiệm uptraining của chúng tôi trên ImageNet-1k.

Hình 4: Scaling số lượng Key-Value heads, và cách nó tác động đến loss. Mỗi điểm dữ liệu đại diện cho một biến thể trên ViT-B, với 12 heads, được finetuned trên CIFAR-100 trong 5 epochs, không có uptraining nào vì lý do thời gian. Rõ ràng là các mô hình hoạt động tốt hơn khi chúng có số lượng Key-Value heads tối đa.

Mô hình	Thời gian Suy luận tính bằng Giây	% Khác biệt
GQA	0.192415	-
DGQA	0.193276	0.4477%
KDGQA	0.192812	0.2064%
PGQA	0.198783	3.3098%

Bảng 5: So sánh thời gian suy luận giữa các biến thể ViT-B trung bình qua batch size 288, GPU NVIDIA GeForce RTX 4090 24GB duy nhất.

--- TRANG 5 ---
Bossard, L.; Guillaumin, M.; and Van Gool, L. 2014. Food-101 – Mining Discriminative Components with Random Forests. In European Conference on Computer Vision.

Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165.

Chen, Y.; Zhang, C.; Gao, X.; Mullins, R. D.; Constantinides, G. A.; and Zhao, Y. 2024. Optimised Grouped-Query Attention Mechanism for Transformers. arXiv:2406.14963.

Chinnakonduru, S. S.; and Mohapatra, A. 2024. Weighted Grouped Query Attention in Transformers. arXiv:2407.10855.

Darlow, L. N.; Crowley, E. J.; Antoniou, A.; and Storkey, A. J. 2018. CINIC-10 is not ImageNet or CIFAR-10. arXiv:1810.03505.

Dong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.; Chen, D.; and Guo, B. 2022. CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. arXiv:2107.00652.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929.

Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.; Welbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.; van den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.; Simonyan, K.; Elsen, E.; Rae, J. W.; Vinyals, O.; and Sifre, L. 2022. Training Compute-Optimal Large Language Models. arXiv:2203.15556.

Javadi, F.; Ahmed, W.; Hajimolahoseini, H.; Ataiefard, F.; Hassanpour, M.; Asani, S.; Wen, A.; Awad, O. M.; Liu, K.; and Liu, Y. 2023. GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values. arXiv:2311.03426.

Joshi, V.; Laddha, P.; Sinha, S.; Omer, O. J.; and Subramoney, S. 2024. QCQA: Quality and Capacity-aware grouped Query Attention. arXiv:2406.10247.

Krizhevsky, A. 2009. Learning multiple layers of features from tiny images. Technical report.

Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv:2103.14030.

mnmoustafa, M. A. 2017. Tiny ImageNet.

Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; Berg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3): 211–252.

Shazeer, N. 2019. Fast Transformer Decoding: One Write-Head is All You Need. arXiv:1911.02150.

Steiner, A.; Kolesnikov, A.; Zhai, X.; Wightman, R.; Uszkoreit, J.; and Beyer, L. 2022. How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. arXiv:2106.10270.

Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2023. Attention Is All You Need. arXiv:1706.03762.

Wightman, R. 2019. PyTorch Image Models. https://github.com/rwightman/pytorch-image-models.

--- TRANG 6 ---
A Learning Rates
Theo các lưu ý từ đánh giá chính của chúng tôi, chúng tôi muốn cho thấy tác động của learning rates đến các mô hình và biến thể. Learning rate 1×10−4 là nơi huấn luyện bắt đầu ổn định trên tất cả mô hình và biến thể - chúng tôi thấy rằng learning rates thấp hơn (lên đến 1×10−5) có lợi hơn cho fine-tuning.

Bảng 6 cho thấy kết quả từ cùng thiết lập như Bảng 3, nhưng với learning rate cao hơn. Chúng tôi thấy rằng một số biến thể ViT-S chỉ vượt trội hơn benchmark GQA một chút, mặc dù không có biên độ đáng kể, và DGQA vượt trội hơn GQA một cách nhất quán khi sử dụng ViT-L.

Nếu các số được so sánh qua hai bảng này, có thể thấy rằng các mô hình hoạt động tốt hơn với learning rates thấp hơn - mức tăng hiệu suất bởi DGQA cũng đáng kể hơn, cho thấy rằng những mô hình này nhạy cảm hơn nhiều với thiết lập huấn luyện.

B Tác động của Phương pháp Uptraining
Trong các thí nghiệm sơ bộ, chúng tôi khám phá tính khả thi của việc giảm chi phí tính toán của uptraining bằng cách sử dụng bộ dữ liệu CINIC-10 để hiệu chuẩn mô hình, thay vì ImageNet-1k tốn kém tài nguyên hơn. Chúng tôi uptrain trên một biến thể của CINIC-10, chứa 270000 hình ảnh, trong 10 epochs, chỉ hơi dưới gấp đôi số lượng training steps cần thiết cho 1 epoch trên ImageNet-1k.

Độ phân giải thấp, thiếu đa dạng trong các lớp, và kích thước nhỏ của bộ dữ liệu được chứng minh là xa tối ưu cho các mô hình của chúng tôi. Để ngắn gọn, chúng tôi chọn đánh giá tất cả mô hình chỉ sử dụng các checkpoint GQA tương ứng, như có thể thấy trong Bảng 7.

Vì CINIC-10 chứa cùng 10 lớp từ CIFAR-10, chúng tôi thấy rằng các mô hình rõ ràng hoạt động tốt hơn trên CIFAR-10. Tuy nhiên, những thiếu sót của bộ dữ liệu được phản ánh khi chúng tôi chuyển sang ba bộ dữ liệu còn lại, nơi các mô hình thất bại trong việc tổng quát hóa một cách kinh khủng, trải qua sự sụt giảm đáng kể về độ chính xác khi so sánh với Bảng 4.

Thí nghiệm này cho thấy rằng chất lượng uptraining rất quan trọng, và đó là bước cần thiết để chuyển sang ImageNet-1k. Chúng tôi tin rằng uptraining trên ImageNet-21k và JFT-300M có thể cải thiện chất lượng của các mô hình đã fine-tuned cuối cùng thậm chí nhiều hơn, nhưng do hạn chế tài nguyên và thời gian, chúng tôi không thể khám phá hướng này.

C Head Similarity Bias
Trong MHA, đầu ra của mỗi head không cho thấy sự tương tự chủ đạo với đầu ra từ bất kỳ head nào khác. Thực tế, chúng ta thấy sự không tương tự giữa đầu ra của một head so với các heads xa hơn.

Tuy nhiên trong GQA, đầu ra của heads rất tương tự với các heads khác trong nhóm gây ra bias tương tự intra-group mà, trong số những lý do khác, có thể góp phần vào sự sụt giảm độ chính xác trong GQA so với MHA.

Chúng tôi ban đầu giới thiệu PGQA như một cách để giảm thiểu bias tương tự intra-group này, tuy nhiên chúng tôi quan sát trong Hình 3 rằng PGQA cũng xóa bỏ các xu hướng khác được quan sát trong heat map của MHA. Mặt khác, DGQA chứng minh là một trung gian thú vị giữa GQA và MHA (Hình 5).

Chúng tôi cho thấy trong Hình 6 rằng heat map của DGQA gần với trung bình có trọng số của GQA và MHA. Theo cách này, DGQA có thể giảm thiểu phần lớn bias tương tự intra-group trong khi cũng giữ lại các xu hướng không tương tự giữa các heads như được thấy trong heat map của MHA.

D Dữ liệu Phức tạp Cần nhiều Nhóm Không đồng nhất hơn
Chúng tôi huấn luyện cùng ViT từ đầu với cùng hyper parameters (Batch Size 32, learning rate 1e−4, optimizer AdamW, và window size 100) trong 1000 training steps trên bộ dữ liệu MNIST và CIFAR-10. Các thí nghiệm của chúng tôi trong Hình 7 cho thấy rằng bộ dữ liệu CIFAR-10 phức tạp hơn, giàu tính năng hơn chọn một tỷ lệ cao hơn của các phân bổ của nó (thay đổi mỗi 100 training steps) là không đồng nhất.

Thí nghiệm đơn giản này cho thấy sự phụ thuộc của phân bổ nhóm vào bộ dữ liệu, tuy nhiên tính chất tĩnh của GQA không cho phép các phân bổ động này được cung cấp bởi cơ chế DGQA của chúng tôi.

E Tương quan giữa Số lượng Tham số và Nhóm Không đồng nhất
Chúng tôi lặp lại các thí nghiệm từ Phần D trong phụ lục nhưng chúng tôi tăng dần số lượng tham số bằng cách tăng embedding dimensionality nhưng giữ số lượng layers không đổi. Hình 8 cho thấy rằng trong khi số lượng layers vẫn không đổi, việc chỉ tăng tham số không nhất thiết dẫn đến sự gia tăng số lượng nhóm không đồng nhất so với tổng phân bổ nhóm.

Tuy nhiên, khi chúng tôi lặp lại những thí nghiệm này một lần nữa nhưng lần này, chúng tôi giữ embedding dimensionality không đổi nhưng tăng số lượng layers và do đó số lượng tham số cũng vậy. Hình 9 cho thấy xu hướng tăng mạnh hơn nhiều mô tả rằng việc tăng số lượng layers dẫn đến tỷ lệ phần trăm lớn hơn của các phân bổ nhóm không đồng nhất.

--- TRANG 7 ---
Mô hình	Biến thể	Độ chính xác
		CIFAR-10	CIFAR-100	Food101	Tiny ImageNet
ViT-S	MHA	96.29	85.87	84.93	78.61
	GQA	95.73	83.77	82.22	76.38
	KDGQA	93.14	76.53	77.78	66.64
	DGQA	96.10	83.57	82.30	76.16
	PGQA	95.19	81.03	80.03	73.02
ViT-B	MHA	95.08	81.86	81.13	73.04
	GQA	92.80	73.58	73.75	62.99
	KDGQA	91.61	71.35	71.34	59.60
	DGQA	91.91	73.42	73.29	60.98
	PGQA	90.93	72.06	72.02	60.65
ViT-L	MHA	96.08	84.97	83.24	78.19
	GQA	94.05	76.61	76.46	65.62
	KDGQA	87.33	61.24	66.85	54.33
	DGQA	95.11	80.85	79.08	71.91
	PGQA	92.71	70.37	72.61	65.20

Bảng 6: Độ chính xác test của các lần fine-tuning, post-uptraining trên ImageNet-1k với learning rate 1×10−4, sử dụng các checkpoint cụ thể cho biến thể. Các mô hình vượt trội hơn GQA trong danh mục của chúng được highlight in đậm.

Mô hình	Biến thể	Độ chính xác
		CIFAR-10	CIFAR-100	Food101	Tiny ImageNet
ViT-S	GQA	98.13	72.78	61.83	53.88
	KDGQA	95.01	62.71	53.83	44.61
	DGQA	98.15	72.91	60.88	53.49
	PGQA	97.68	66.42	54.02	48.37
ViT-B	GQA	96.74	67.36	56.46	49.26
	KDGQA	95.23	61.28	50.14	41.74
	DGQA	96.57	66.44	54.83	46.72
	PGQA	96.25	63.52	50.34	44.41
ViT-L	GQA	97.95	77.31	66.80	59.85
	KDGQA	97.08	71.38	60.17	54.37
	DGQA	97.78	76.77	65.53	59.31
	PGQA	97.77	74.11	63.08	56.77

Bảng 7: Độ chính xác test của các lần fine-tuning, post-uptraining trên CINIC-10 với learning rate 1×10−5, sử dụng checkpoint chỉ-GQA.

Hình 5: DGQA là một trung gian thú vị giữa GQA và MHA. Không giống PGQA, heat map của DGQA không xóa bỏ các patterns được quan sát trong MHA.

--- TRANG 8 ---
Hình 6: DGQA gần như xấp xỉ một trung bình có trọng số giữa GQA và MHA. DGQA bảo tồn các xu hướng từ MHA trong khi giảm thiểu đáng kể bias tương tự intra-group ảnh hưởng đến GQA.

Hình 7: Kiến trúc mô hình giống nhau, có vẻ như CIFAR10 phức tạp hơn ưa thích nhiều nhóm không đồng nhất hơn trong quá trình huấn luyện.

Hình 8: % của các nhóm không đồng nhất không cho thấy pattern rõ ràng khi chúng tôi tăng tham số trong khi giữ layers không đổi.

Hình 9: % của các nhóm không đồng nhất tăng khi chúng tôi tăng số lượng layers trong mô hình.
