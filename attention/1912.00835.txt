# 1912.00835.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/1912.00835.pdf
# File size: 800089 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Compact Multi-Head Self-Attention for Learning Supervised
Text Representations
Sneha Mehta
Virginia Tech, USA
snehamehta@cs.vt.eduHuzefa Rangwala
George Mason University, USA
rangwala@cs.gmu.eduNaren Ramakrishnan
Virginia Tech, USA
naren@cs.vt.edu
ABSTRACT
Effective representation learning from text has been an active area of
research in the ﬁelds of NLP and text mining. Attention mechanisms
have been at the forefront in order to learn contextual sentence repre-
sentations. Current state-of-the-art approaches for many NLP tasks
use large pre-trained language models such as BERT, XLNet and so
on for learning representations. These models are based on the Trans-
former architecture that involves recurrent blocks of computation
consisting of multi-head self-attention and feedforward networks.
One of the major bottlenecks largely contributing to the computa-
tional complexity of the Transformer models is the self-attention
layer, that is both computationally expensive and parameter inten-
sive. In this work, we introduce a novel multi-head self-attention
mechanism operating on GRUs that is shown to be computationally
cheaper and more parameter efﬁcient than self-attention mechanism
proposed in Transformers for text classiﬁcation tasks. The efﬁciency
of our approach mainly stems from two optimizations; 1) we use
low-rank matrix factorization of the afﬁnity matrix to efﬁciently get
multiple attention distributions instead of having separate parameters
for each head 2) attention scores are obtained by querying a global
context vector instead of densely querying all the words in the sen-
tence. We evaluate the performance of the proposed model on tasks
such as sentiment analysis from movie reviews, predicting business
ratings from reviews and classifying news articles into topics. We
ﬁnd that the proposed approach matches or outperforms a series
of strong baselines and is more parameter efﬁcient than compara-
ble multi-head approaches. We also perform qualitative analyses
to verify that the proposed approach is interpretable and captures
context-dependent word importance.
KEYWORDS
neural networks, text classiﬁcation, attention
1 INTRODUCTION
Learning effective language representation is important for a variety
of text analysis tasks including sentiment analysis, news classiﬁca-
tion, natural language inference and question answering. Supervised
learning using neural networks commonly entails learning inter-
mediate sentence representations followed by a task speciﬁc layer.
For text classiﬁcation tasks; this is usually a fully connected layer
followed by an N-way softmax where Nis the number of classes.
Learning self-supervised language representations has made sub-
stantial progress in recent years with the introduction of new tech-
niques for language modeling combined with deep models like
ELMo [ 32], ULMFit [ 15] and more recently BERT [ 8] and GPT-
2 [33]. There has been a surge of BERT-based language models
that use larger data for pretraining and different pretraining methodssuch as XL-Net [ 41], RoBERTa [ 24] while others combine different
modalities [ 37]. These methods have enabled transfer of learned
representations via pre-training to downstream tasks. Although
these models work well on a variety of tasks there are two major
limitations: 1) they are computationally expensive to train 2) they
usually have a large number of parameters that greatly increases the
model size and memory requirements. For instance, the multilingual
BERT-base cased model has 110M parameters, the small GPT-2
model has 117M parameters [ 33] and the RoBERTa model was
trained on 160GB of data [ 24]. Recently, researchers have proposed
‘lighter’ BERT models that leverage knowledge distillation during
the pre-training phase and reduce the size of the BERT model [ 40]
or try to improve the parameter efﬁciency of the BERT model by
optimizations at the embedding layer and by parameter sharing [ 21].
However, all of the above models are based on the Transformer archi-
tecture [ 39] the major component of which is the scaled dot-product
self-attention mechanism .
This layer has a computational complexity of O¹n2dºthat scales
quadratically with the length of the input ( n) and linearly with the
length of the model hidden size ( dº. It is natural to see how task
speciﬁc training or ﬁne-tuning can be limiting when the training
data and computational resources are scarce and sequences are long.
Further, running inference on and storing such models can also be
difﬁcult in low resource scenarios such as IoT devices or low-latency
use cases. Hence, supervised learning for task-speciﬁc architectures
which are trained from scratch, especially where domain speciﬁc
training data is available are useful. They are light-weight and easy to
deploy. In this work, we propose low-rank f actorization based multi-
head attention mechanism (LAMA), a lean attention mechanism
which is computationally cheaper and more parameter efﬁcient than
prior approaches and exceeds or matches the performance of state-
of-the-art baselines including large pretrained models, with fewer
parameters.
Contrary to previous approaches [ 12,23] that are based on ad-
ditive attention mechanism [ 2], LAMA is based on multiplicative
attention [ 25] which replaced the additive attention by dot product
for faster computation. We further introduce a bilinear projection
while computing the dot product to capture similarities between
a global context vector and each word in the sentence [ 25]. The
function of the bilinear projection is to capture nuanced context
dependent word-importance as corroborated by previous works [ 3].
Next, unlike previous methods we use low-rank formulation of the
bilinear projection matrix based on hadarmard product [ 17,42] to
produce multiple attentions by querying the global context vector
for each word as opposed to having a different learned vector [ 23] or
matrix [ 43]. In effect, we cast score computation between a word rep-
resentation and a context vector as unimodal feature fusion to a low
dimensional space akin to its multimodal counterparts [ 22,43]. EacharXiv:1912.00835v2  [cs.CL]  10 Aug 2020

--- PAGE 2 ---
dimension of this low-dimensional feature space can be considered
as the contribution of the word to a different attention head. By con-
trolling the dimension of this feature space new heads can be added
or removed. Finally, we devise a mechanism to obtain context-aware
supervised sentence embeddings from the obtained attention distri-
butions for downstream classiﬁcation tasks. We evaluate our model
on tasks such as sentiment analysis, predicting business ratings and
news classiﬁcation. We show that the proposed model learns context-
dependent word importance like other attention models. Moreover,
the proposed model is 3 times more parameter efﬁcient than other
comparable attention models especially the encoder of a Transformer
model [ 39]. In terms of performance, the proposed model is compet-
itive and matches or exceeds the performance of strong baselines,
attention and non-attention based. Further, we present some ancillary
analyses on model efﬁciency and need for multiple attention heads.
In summary, our results show that the proposed model can be reliably
used as a leaner multi-head attention alternative for supervised text
classiﬁcation tasks.
The organization of the rest of the paper is as follows: the next
section ( §2) discusses connections with related work, followed by
the description of the proposed model ( §3), followed by the task
descriptions and the experimental evaluation ( §4). Finally, in the fol-
lowing sections the results are presented along with their discussion
(§5), before concluding (§6).
2 RELATED WORK
Spearheaded by their success in neural machine translation [ 2,25]
attention mechanisms are now ubiquitously used in problems such
as question answering [ 10,13,34], text summarization [ 3,31], event
extraction [ 27], and training large language models [ 8,33]. In
sequence modeling, attention mechanisms allow the decoder to learn
which part of the sequence it should “attend” to based on the input
sequence and the output it has generated so far [2].
2.1 Self-Attention
Self-attention, sometimes called intra-attention is an attention mech-
anism relating different positions of a single sequence in order to
compute a representation of the sequence [ 5]. Self-attention has been
used successfully in a variety of tasks including reading compre-
hension, abstractive summarization, textual entailment and learning
task-independent sentence representations [ 5,23,30,31]. Tradi-
tionally, the above methods have depended on Recurrent Neural
Networks(RNNs) [ 7,14] to model sequential dependencies and
attention mechanisms were proposed to alleviate the vanishing gradi-
ent problem by establishing shorter connections between the source
and target positions. The inherently sequential nature of RNNs pre-
cludes parallelization within training examples which becomes a
memory bottleneck for batching across examples. Recently Trans-
former model was proposed [ 39] that replaced the dependence on
RNNs and relies completely on the self-attention mechanism. In
this approach, every position attends to every other position and
adjusts its embedding accordingly. However, this dense computation
is quadratic in the length of the input and is resource intensive. The
proposed approach on the contrary leverages RNNs for modeling
sequential dependencies and uses a single global query vector for
each input word. We show that our approach is computationallymore efﬁcient in comparison to encoder of the Transformer model
on text classiﬁcation tasks and exceeds in performance.
2.2 Multi-Head Attention
Models have been proposed that compute multiple attention distribu-
tions over a single sequence of words. Multi-view networks [ 12] use
a different set of parameters for each view which leads to an increase
in the number of parameters. Lin et al. [ 23] use the additive atten-
tion mechanism, which is a more general approach of computing
attention between query and key vectors and modify it to produce
multiple attentions to obtain a matrix sentence embedding. Scaled
dot product attention proposed by Vaswani et. al. [ 39] is a direct
approach and is based on dot product between the query and key
vectors. This approach has been shown to be very effective in ma-
chine translation [ 39] and pretraining language models [ 8]. However
to compute multiple attention heads different transformation param-
eters are learned for different heads leading to increased parameters.
In this work, on the other hand, score between the key (context)
and the query (word representation) is computed using a bilinear
projection matrix followed by an approach inspired by multi-modal
low rank bilinear pooling [ 17] to factorize the matrix into two low
rank matrices to compute multiple attention distributions over words.
We ﬁnd that this is a more parameter efﬁcient way of computing mul-
tiple attentions. Contrary to Guo et al. [ 12] and Vaswani et al. [ 39]
we use matrix factorization to alleviate the problem of increasing
parameters with increasing heads and the proposed model performs
superior to their approach.
2.3 Low-Rank Factorization
Low-rank factorization has been a popular approach to reduce the
size of the hidden layers [ 4,38]. Recent work has achieved signiﬁ-
cant improvements in computational efﬁciency through factorization
tricks [ 35] and conditional computation [ 20]. Recently, factorization
was also employed at the embedding layer of large pretrained lan-
guage models such as BERT [ 8] to reduce the model size [ 21]. In
this work, we employ factorization for unimodal feature fusion for
computing attention scores for multiple attention heads. Hadamard
product formulation for matrix factorization is used to compact-
ify the multi-head attention layer. This formulation can be viewed
as low-rank bilinear pooling for unimodal features based on the
corresponding idea of multimodal feature fusion [22, 43].
3 PROPOSED MODEL
In this section, we give an overview of the proposed model followed
by a detailed description of each model component.
A document (a product review or a news article) is ﬁrst tokenized
and converted to a word embedding via a lookup into a pretrained
embedding matrix. The embedding of each token is encoded via a
bi-directional Gated Recurrent Unit [ 6] (bi-GRU) sentence encoder
to get a contextual annotation of each word in that document. The
LAMA attention mechanism then obtains multiple attention distri-
butions over those words by computing an alignment score of their
hidden representation with a word-level context vector. Sum of the
word representations weighted by the scores from multiple attention
2

--- PAGE 3 ---
distributions then forms a matrix document embedding. The ma-
trix embedding is then ﬂattened and passed onto downstream layers
(either a classiﬁer or another encoder depending on the task).
In the rest of the paper, capital bold letters indicate matrices, small
bold letters indicate vectors and small letters indicate scalars.
3.1 Sequence Encoder
We use the GRU [ 2] RNN as the sequence encoder. GRU uses a
gating mechanism to track the state of the sequences. There are two
types of gates: the reset gate rtand the update gate zt. The update
gate decides how much past information is kept and how much new
information is added. At time t, the GRU computes its new state as:
ht=¹1 ztºht 1+zt˜h (1)
and the update gate ztis updated as:
zt=σ¹Wzxt+Uzht 1+bzº (2)
The RNN candidate state ˜htis computed as:
˜ht=tanh¹Whxt+rt¹Uhht 1+bhº (3)
Here rtis the reset gate which controls how much the past state
contributes to the candidate state. If rtis zero, then it forgets the
previous state. The reset gate is updated as follows:
rt=σ¹Wrxt+Urht 1+brº (4)
Consider a document Dicontaining Twords. Di=fw1; :::;wt; :::;wTg.
Let each word be denoted by wt,t2»0;T¼where every word is
converted to a real valued word vector xtusing a pre-trained em-
bedding matrix We=RdjVj,xt=Wewt,t2»1;T¼where dis
the embedding dimension and Vis the vocabulary. The embedding
matrix Weis ﬁne-tuned during training. Note that we have dropped
the subscript ias all the derivations are for the ithdocument and it is
assumed implicit in the following sections unless otherwise stated.
We encode the document using a bi-GRU that summarizes infor-
mation in both directions along the text to get a contextual annotation
of a word. In a bi-GRU the hidden state at time step tis represented
as a concatenation of hidden states in the forward and backward
direction. The forward GRU denoted by   !GRU processes the docu-
ment from w1towTwhereas the backward GRU denoted by    GRU
processes it from wTtow1.
xt=Wewt (5)
  !ht=   !GRU¹xt;h¹t 1º;θº (6a)
  ht=    GRU¹xt;h¹t+1º;θº (6b)
Here the word annotation htis obtained by concatenating the for-
ward hidden state  !htand the backward hidden state  ht.
3.2 Single-Head Attention
To alleviate the burden of remembering long term dependencies
from GRUs we use the global attention mechanism [ 25] in which
the document representation is computed by attending to all words
in the document. Let htbe the annotation corresponding to the word
xt. First we transform htusing a one layer Multi-Layer PerceptronTable 1: Important notations. Capital bold letters indicate ma-
trices, small bold letters indicate vectors, small letters indicate
scalars.
Notation Meaning
N Corpus size
T # of words tokens in a sample
m # of attention heads
ft alignment score
αt attention weight
ut word hidden representation
c global context vector
h GRU hidden state dimension
(MLP) to obtain its hidden representation ut. We assume Gaussian
priors with 0mean and 0:1standard deviation on Wwandbw.
ut=tanh¹Wwht+bwº (7)
Next, to compute the importance of the word in the current context
we calculate its relevance to a global context vector cusing a bilinear
projection.
ft=c>Wiut (8)
Here, Wi2I R2h2h, is a bilinear projection matrix which is ran-
domly initialized and jointly learned with other parameters during
training. his the dimension of the GRU hidden state and ut&care
both of dimension 2h1since we’re using a bi-GRU. The mean of
the word embeddings provides a good initial approximation of the
global context of the document. We initialize c=1
TÍT
t=1wtwhich
is then updated during training. We use a bilinear projection because
they are more effective in learning pairwise interactions as shown
in previous works [ 3]. The attention weight for the word xtis then
computed using a softmax function where summation is taken over
all the words in the document.
αt=exp¹ftºÍ
t0exp¹ft0º(9)
3.3 Low-Rank Factorization based Multi-Head
Attention
In this section, we describe the novel low-rank factorization based
multi-head attention mechanism (LAMA). The attention distribution
in Eq. 9 above usually focuses on a speciﬁc component of the docu-
ment, like a special set of trigger words. So it is expected to reﬂect
an aspect, or component of the semantics in a document. This type
of attention is useful for smaller pieces of texts such as tweets or
short reviews. For larger reviews there can be multiple aspects that
describe that review. For this we introduce a novel way of computing
multiple heads of attention that capture different aspects.
Suppose mheads of attention are to be computed, we need m
alignment scores between each word hidden representation utand
the context vector c. To obtain an mdimensional output ft, we
need to learn mweight matrices given by W=»W1; :::;Wm¼2
Rm2h2has demonstrated in previous works. Although this strategy
might be effective in capturing pairwise interactions for each aspect
it also introduces a huge number of parameters that may lead to
overﬁtting and also incur a high computational cost especially for
a large mor a large h. To address this, the rank of matrix Wcan
3

--- PAGE 4 ---
∈ ̃ IR ×1∈ ̃ IR2ℎ  
tanh ∈IR2ℎ×1
∈ ̃   IR ×1 ∘ ˜ ˜ 
 norm 2∈ ̃ IR2ℎ  
∈   IR ∈   IR2ℎ×1
... ...
 1  2 ... ... ... ...`
   1
... ...
bestpad thai i
have ever tasted 2   
MLP MLP MLP MLP MLP MLP MLP
ℎ1 ℎ2ℎ ... ... ... ... 
 1
 2
...
  
...
  
Multi-Head AttentionSentence
 Encoder 
 1 2...............  
Structured Sentence
Embedding 
5
stars
MLP
Classiﬁer
HFigure 1: Figure shows a schematic of the model architecture and its major components including the Sentence Encoder, proposed
multi-head attention mechanism LAMA, Structured Sentence Embedding and ﬁnally the MLP classiﬁer. The attention computation
is demonstrated for a single word.
be reduced by using low-rank bilinear method to have less number
of parameters [ 17,43]. Consider one head; the bilinear projection
matrix Wiin Eq. 8 can be factorized into two low rank matrices P
&Q.
ft=c>PQ>ut=kÕ
d=1c>pdq>
dut= 1>¹P>cQ>utº (10)
where P=»p1; :::;pk¼2I R2hkandQ=»q1; :::;qk¼2I R2hkare
two low-rank matrics, is the Hadamard product or the element-
wise multiplication of two vectors, 12I Rkis an all-one vector and
kis the latent dimensionality of the factorized matrices.
To obtain mscores, by Eq.10, the weights to be learned are
two three-order tensors P=»P1; :::;Pm¼ 2I R2hkmandQ=
»Q1; :::;Qm¼2I R2hkmaccordingly. Without loss of generality
PandQcan be reformulated as 2-D matrices ˜P2I R2hkmand
˜Q2I R2hkmrespectively with simple reshape operations. Setting
k=1, which corresponds to rank-1 factorization. Eq.10 can be
written as:
ft=˜P>c˜Q>ut (11)
This brings the two feature vectors ut2R2h, the word hidden
representation and c2R2h, the global context vector in a common
subspace and are given by ˜utand˜crespectively. ft2I Rmcan be
viewed as a multi-head alignment vector for the word xtwhere each
dimension of the vector can be viewed as the score of the word w.r.t.
a different attention head. For computing attention for one head, this
is equivalent to replacing the projection matrix Wiin Eq. 8 by the
outer product of vectors ˜Piand ˜Qi; rows of the matrices ˜Pand ˜Q
respectively and rewriting it as the Hadamard product. As a result
each row of matrices ˜Piand ˜Qirepresent the vectors for computing
the score for a different head.
The multi-head attention vector αt2I Rmis obtained by comput-
ing a softmax function along the sentence length:
αt=exp¹ftºÍ
t0exp¹ft0º(12)Before computing softmax, similar to previous works [ 17,43], to fur-
ther increase the model capacity we apply the tanh nonlinearlity to ft.
Since element-wise multiplication is introduced the variance of the
model might increase, so we apply an l2normalization layer across
themdimension. Although l2is not strictly necessary since both c
andutare in the same modality empirically we do see improvement
after applying l2. Each component kofαtis the contribution of the
word xtto the kthhead.
Next, we describe how this computation can be vectorized for
each word in the document. Let H=¹h1;h2; :::hTºbe a matrix of
all word annotations in the sentence; H2I RT2h. The attention
matrix for the sentence can be computed as:
A=sof tmax¹l2¹tanh¹˜P>C/afii10069.ital˜Q>H>ººº (13)
where, C/afii10069.ital2I R2hTiscrepeated Ttimes, once for each word,
l2¹xº=x
jjxjjandsoftmax is applied row-wise. A2I RmTis the
attention matrix between the sentence and the global context with
each row representing attention for one aspect.
Given A=»α1;α2; :::αT¼, the multi-head attention matrix for
the sentence; A2I RmT. The document representation for a head
jgiven by αj=fαj1;αj2; ::αjTgcan be computed by taking a
weighted sum of all word annotations.
sj=TÕ
k=1hkαjk (14)
Similarly, document representation can be computed for all heads
and is given in a compact form by:
S=AH (15)
Here S2I Rm2his a matrix sentence embedding and contains as
many rows as the number of heads. Each row contains an attention
distribution for a new head. It is ﬂattened by concatenating all
rows to obtain the document representation d. From the document
representation, the class probabilities are obtained as follows.
ˆy=sof tmax¹Wcd+bcº (16)
4

--- PAGE 5 ---
Loss is computed using cross entropy.
L¹y;ˆyº= CÕ
c=1/y.altclo/afii10069.ital¹ˆ/y.altcº (17)
where Cis the number of classes and ˆ/y.altcis the probability of the
class c.
3.4 Disagreement Regularization
To reduce the variance of model introduced due to point-wise multi-
plication and to encourage diversity among multiple attention heads
we introduce an auxiliary regularization term.
/J.alt¹θº=ar/afii10069.italminθfL¹y;ˆyº λD¹Ajx;y;θºg (18)
where Ais the attention matrix, θrepresents model parameters, λis a
hyper-parameter and is empirically set to 0.2 in this paper. L¹ˆy;yºis
the cross-entropy loss and D¹Ajx;y;θºis the auxiliary regularization
term that represents the disagreement between different attentions. It
guides the related attention components to capture different features
from the corresponding projected subspaces. We try two different
regularizations i) regularization over attended positions ii) regular-
ization over document embeddings. For the ﬁrst type we adapt
the penalization term in [ 23] to represent disagreement between
attention distributions (Eq. 19a). Next, we directly regularize the
sentence embeddings resulting from different attention distributions
represented using their cosine similarity (Eq. 19b). The more similar
the embeddings, lesser the disagreement.
Dpenal = kAA> Ik2
F(19a)
Demb= 1
m2mÕ
i=1mÕ
i=1sisj
ksikksjk(19b)
The ﬁnal training loss is given by Eq. 18 summed over all documents
in a minibatch. We use the minibatch stochastic gradient descent
algorithm [ 16] with momentum and weight decay for optimizing the
loss function and the backpropogation algorithm is used to compute
the gradients.
Fig. 1 shows a single document and its ﬂow through various
model components. The middle block illustrates the proposed at-
tention mechanism for one word wtof the document. It is ﬁrst
transformed through Eq. 7 to ut. In parallel, a context vector cis
initialized. Eq. 11 is then used to compute the multi-head attention
for this word. Vectorized attention computation can be performed
for all words using Eq. 13 to obtain the attention matrix Awhich is
then multiplied by the hidden state matrix Hto obtain an embedding
for the document which is then passed to the MLP classiﬁer.
3.5 Hyperparameters
We use a word embedding size of 100. The embedding matrix Weis
pretrained on the corpus using word2vec [ 29]. All words appearing
less than 5 times are discarded. The GRU hidden state is set to
h=50, MLP hidden state to 512and we apply a dropout of 0:4to
the hidden layer. Batch size is set to 32for training and an initial
learning rate of 0:05is used. For early stopping we use patience =5.
Momentum is set to 0.9 and weight decay to 0.0001. We will open
source the code on acceptance.Table 2: Per-layer complexity for different layer types. nis the docu-
ment length, mis the number of attention heads and dis the represen-
tation dimension.
Layer Type Complexity per Layer
Self Attention (LAMA) O¹nmhº
Self-Attention (TE) O¹n2dº
Table 3: Dataset statistics. # words indicates the average num-
ber of tokens per document in the corresponding datasets.
Dataset # Classes # Train # Test # words
YELP 5 499,976 4,000 118
YELP-L 5 175,844 1,378 226
YELP-P 2 560,000 38,000 137
IMDB 2 25,000 25,000 221
Reuters 8 4,484 2,189 102
News 4 151,328 32,428 352
3.6 Computational Complexity
Other variables such as document encoder and dimension of the hid-
den state held constant, the computational complexity of the model
depends on the attention layer. It can be seen that the computational
complexity of LAMA is linear in input and linear in the number of
attention heads. Where as, the attention mechanism in the encoder
of a Transformer model [ 39] is quadratic in the length of the input
(Table 2). For cases where nmn2, which is a common scenario,
attention mechanism LAMA is computationally more efﬁcient than
self-attention in encoder of the Transformer model for sequence
classiﬁcation tasks.
4 EV ALUATION
4.1 Datasets
We evaluate the performance of the proposed model on tasks of
predicting business ratings from Yelp, sentiment prediction from
movie reviews and classifying news articles into topics. Table 3
gives an overview of the datasets and their statistics.
4.1.1 Y elp. The Yelp dataset1consists of 2.7M Yelp reviews
and user ratings from 1 to 5. Given a review the goal is to predict the
rating assigned by the user to the corresponding business store. We
treat the task as 5-way text classiﬁcation where each class indicates
the user rating. We randomly sampled 500K review-star pairs as
training set and 4,000 for test set. Reviews were tokenized using
the Spacy tokenizer2. 100-dimensional word embeddings were
trained from scratch on the train dataset using the gensim3software
package.
4.1.2 Y elp-Long. Multi-head attention capturing multiple as-
pects is more useful for classifying ratings that are more subjective
i.e. longer reviews where people express their experiences in detail.
We create a subset of the YELP dataset containing all longer reviews
1https://www.yelp.com/dataset challenge
2https://spacy.io/
3https://radimrehurek.com/gensim/
5

--- PAGE 6 ---
i.e. reviews containing longer than 118 tokens which we found to be
the mean length of the reviews in the dataset. The training set con-
sists of 175,844 reviews, and the test set consists of 1,378 reviews.
The goal is to predict the ratings from the above subset of the Yelp
dataset. We refer to this dataset as Yelp-L (Yelp-Long) in the rest
of the paper since it consists of all longer reviews. We hypothesize
that having multi-head attention would beneﬁt in this setting where
more intricate foraging of information from different parts of the
text is required to make a prediction. The model hyperparameters
and training settings remain the same as the above.
4.1.3 Y elp-Polarity. The Yelp reviews polarity dataset [ 44] is
constructed by considering stars 1 and 2 negative, and 3 and 4
positive from the Yelp dataset. For each polarity 280,000 training
samples and 19,000 testing samples are taken randomly. In total
there are 560,000 training samples and 38,000 testing samples. This
datset is referred as Yelp-P in the paper.
4.1.4 Movie Reviews. The large Movie Review dataset [ 26]
contains movie reviews along with their associated binary sentiment
polarity labels. It contains 50,000 highly polar reviews ( score4
out of 10 for negative reviews and score >=7out of 10 for positive
reviews) split evenly into 25K train and 25K test sets. The overall
distribution of labels is balanced (25K pos and 25K neg). In the
entire collection, no more than 30 reviews are allowed for any given
movie because reviews for the same movie tend to have correlated
ratings. Further, the train and test sets contain a disjoint set of
movies, so no signiﬁcant performance is obtained by memorizing
movie-unique terms and their associated with observed labels. We
refer to this dataset as IMDB in the rest of the paper.
4.1.5 News Aggregator. This dataset [ 9] contains headlines,
URLs, and categories for news stories collected by a web aggregator
between March 10th, 2014 and August 10th, 2014. News categories
included in this dataset include business; science and technology;
entertainment; and health. Different news articles that refer to the
same news item are also categorized together. Given a news article
the task is to classify it into one of the four categories. Training
dataset consits of 151,328 articles and test dataset consits of 32, 428.
The average token length is 352.
4.1.6 Reuters. This dataset4is taken from Reuters-21578
Text Categorization Collection. This is a collection of documents
that appeared on Reuters newswire in 1987. The documents were
assembled and indexed with categories. We evaluate on the Reuters8
dataset consisting of news articles about 8 topics including acq,
crude, earn, grain, interest, money-fx, ship,trade.
4.2 Comparative Methods
To benchmark the proposed model against existing methods we use a
variety of model architectures as our comparative baselines. We use
BERT [ 8] as one of the baselines. In our experiments, we used the
pretrained bert-base uncased model which has 12-layers, 768-hidden
state size, 12-heads, 110M parameters and is trained on lower-cased
English text. We ﬁnetuned it on our datasets for 2 epochs using the
ADAM optimizer [19] with a learning rate of 5e 6.
4https://www.cs.umb.edu/smimarog/textmining/datasets/It has been shown that average of word embeddings can make for
a very strong baseline [ 36]. We use this as another baseline and refer
to it as A VG in the paper.
We use a variety of models with and without attention as other
baselines. Strong representative baselines for different model archi-
tectures are chosen; such as CNN with max-over-time pooling [ 18],
bidirectional GRU [ 7] model with maxpooling referred as BiGRU.
We experimented with n=1andn=2GRU layers and found that
n=1converged faster and led to a better performance. For the CNN
baseline we used 3 kernels of sizes 3, 4 and 5 with 100 ﬁlters each.
Among supervised attention-based multi-head models we use the
Self Attention Network proposed by Lin et al. [ 23]. We refer to this
baseline as SAN. For each task, we empirically ﬁnd the number of
heads that give the best performance. We use a 1-layer Encoder of
the Transformer model (TE) [ 39] with 8 attention heads as another
baseline. We use dmodel =512such that dmodelheads =64.
We use two variations of the proposed model to compare the
performance with the above baselines. For the ﬁrst variation we
initialize cwith mean of word embeddings in the sentence to provide
the global context of the sentence. We call this baseline LAMA+ctx .
In another variation, we randomly initialize cand jointly learn it
with other model parameters ( LAMA ). Our model with embedding
regularization is referred as LAMA+ctx +deand our model with
regularization over positions is referred as LAMA+ctx +dp. For the
models LAMA andLAMA+ctx we empirically identify the optimal
number of attention heads to get the best performance.
5 RESULTS
Table 4 reports the accuracy of the best model on the test set after
performing 3-fold cross-validation. The proposed model with global
context initialization LAMA+ctx outperforms the SAN model [ 23]
on all tasks from 3.3% (Reuters) to 8.2% (IMDB). This is due
to the fact that during attention computation the proposed model
architecture has the provision to access the global context of the
sentence whereas for SAN no such provision is available.
Extrapolating the attention over larger chunks of text we get
uniform attention over all words, which is equivalent to no attention
or equal preference for all words. This is what a simple BiGRU
effects to (in a contextual setting and average of word embeddings
in a non-contextual setting). We note that LAMA+ctx outperforms
BiGRU by 2.0% (News), 12.2% (Reuters) 7.9% (Yelp) and 9.4 %
(Yelp-L) and 2.7 % (IMDB).
Our models outperform the Transformer Encoder (TE) on all
tasks. It should be noted that this performance improvement also
comes with fewer parameters than TE (Table 5). When compared to
large ﬁne-tuned pre-trained language models such as BERT we ﬁnd
that LAMA outperforms BERT on News, Reuters, Yelp and IMDB
datasets. On YELP-L and YELP-P datasets BERT outperforms
LAMA. However, it should be noted that besides being trained
on large-scale text corpora and having a large memory footprint
compared to LAMA, BERT models also take a longer time for
pretraining. For instance, for Yelp-P it took 12.5 hours to train the
model just for 1 epoch as compared to 20 mins for LAMA and for
Yelp it took 8.5 hours as opposed to 25 mins for LAMA on one
Nvidia Tesla P100 GPU.
6

--- PAGE 7 ---
Table 4: Table reports the accuracy of the proposed models ( LAMA ,LAMA+ctx ) against various baselines on sentiment analysis and news classiﬁca-
tion tasks. +Dprefers to LAMA + Ctx with position-wise regularization whereas +Derefers to LAMA + Ctx with regularization over embeddings.
Methods News Reuters Yelp IMDB Yelp-L Yelp-P
SAN (Lin et al. [23]) 0.876 0.942 0.68 0.831 0.638 0.945
BiGRU 0.905 0.867 0.663 0.876 0.608 0.943
CNN (Kim et al. [18]) 0.914 0.96 0.693 0.874 0.672 0.953
TE (Vaswani et al. [39]) 0.899 0.901 0.655 0.817 0.569 0.925
BERT (Devlin et al. [8]) 0.92 0.97 0.715 0.894 0.672 0.97
A VG (Arora et al. [1]) 0.91 0.795 0.653 0.874 0.652 0.928
LAMA 0.922 0.965 0.697 0.895 0.653 0.947
LAMA + Ctx 0.923 0.973 0.716 0.90 0.665 0.952
+Dp 0.903 0.948 0.711 0.874 0.656 0.948
+De 0.91 0.831 0.677 0.805 0.619 0.893
For the non-contextual baseline of average of word embeddings
there’s an improvement of 1.4% (News), 22.4% (Reuters) 9.8%
(Yelp), 11.4% (Yelp-L) and 3.0% (IMDB) which shows that con-
textual information captured by BiGRU or CNN models are indeed
important for the tasks.
Compared to CNN models an improvement of 1% (News), 1.4%
(Reuters) and 3.3% (Yelp), and 3.0 % (IMDB) can be observed. On
the Yelp-L dataset our model and CNN perform similarly. However,
the proposed model is more interpretable and gives an option for
inspecting the attended keywords.
Finally, our results suggest that using disagreement regularization
on LAMA worsens the performance in general.
From the above results it can be noted that LAMA is a competitive,
interpretable and lean supervised model for text classiﬁcation tasks.
5.1 Parameter vs Heads
In Transformer-based models attention layer can prove to be a mem-
ory bottleneck when resources are constrained. In this section, we
compare the number of trainable parameters of the proposed model
(LAMA) and Transformer Encoder (TE). Table 5 shows the increase
in number of parameters (y-axis) when the number of attention
heads are varied as 2, 4, 8, 16, 32, 64. Powers of 2 are picked
because TE requires number of attention heads to be divisble by
dmodel [39]. Note that reported parameters also contain GRU &
embedding parameters for LAMA and feed forward layer & position
embedding parameters for TE, although these parameters don’t de-
pend on number of attention heads. To ensure a fair comparison we
pick BiGRU hidden dimension and dmodel both as 512. The hidden
layer dimension of the ﬁnal layer sentence classiﬁer is 1024, which
is the same for both models. It can be observed that the number of
parameters in TE is constant for all heads which is consistent with
its deﬁnition, because with increasing heads; Key, Value and Query
projections parameters are scaled down. For LAMA, the number of
parameters increases only slightly with increasing number of heads
because P&Qare the only parameter matrices that are dependent
on the number of attention heads mand for which the size increases
(linearly). More importantly, it should be noted that for the number
of heads for most practical use cases, the proposed model LAMA is
almost 3 times more compact than Transformer Encoder.Table 5: Comparison of number of trainable parameters in the
attention layer of LAMA and TE with increasing number of
attention heads. Number of parameters increase linearly for
LAMA whereas for TE they are constant. Overall LAMA is
more parameter efﬁcient than TE.
# heads# parameters (LAMA)
(in millions)# parameters (TE)
(in millions)
2 6.403 18.465
4 6.405 18.465
8 6.409 18.465
16 6.418 18.465
32 6.434 18.465
64 6.468 18.465
5.2 Runtime
It should be noted that since LAMA is applied on top of RNN the
computation is sequential and attention cannot be computed unless
all RNN hidden states are available. Hence, the computational
time increases linearly with input length and quadratically with
hidden state dimension ¹O¹nh2ºº. Once RNN hidden states are
available the complexity of the attention layer LAMA is O¹nmhº.
In comparison, the complexity of the self-attention mechanism in
Transformer is O¹n2dº, where dis the hidden state dimension. It
increases quadratically with increasing input length and linearly with
hidden state dimension. In this section, we compare the runtimes of
LAMA and a 1-layer Transformer Encoder. For a fair comparison
we compare the runtimes of LAMA free from RNN. We propose,
LAMA Encoder – a model that doesn’t use RNN to model sequential
dependencies and directly operates on the word embeddings of the
inputs. That is, the hidden state matrix Hin Eq. 13 is populated by
the pretrained word embeddings. Fig 2 shows the average runtime
per epoch (averaged over 10 epochs) of LAMA Encoder (LE) and
Transformer Encoder (TE) when sequence lengths are increased
from 50 to 250 for IMDB and Reuters datasets. It can be seen from
the ﬁgure that TE is computationally more expensive per epoch than
LE. LE is also a competitive model with test accuracies of 83.0 %
on IMDB and 93.9% on Reuters as compared to 81.7% (IMDB) and
90.1% (REUTERS) of TE.
7

--- PAGE 8 ---
50 75 100 125 150 175 200 225 250
Input Length510152025Time (sec)IMDB
LAMA
TE
50 75 100 125 150 175 200 225 250
Input Length1.52.02.53.03.54.04.55.0Time (sec)Reuters
LAMA
TEFigure 2: Figure shows the average run time per epoch in seconds (averaged over 10 epochs) for LAMA Encoder (LE) and Transformer Encoder
(TE) models as a function of input sequence length (50 to 250). It can be seen that TE (green) is more computationally expensive compared to LAMA
(blue). LE is LAMA attention mechanism applied directly to word embeddings without computing GRU hidden states.
107108
# parameters9394959697AccuracyLAMA
SAN
TEBERT
Figure 3: Figure shows the accuracy (y-axis) of the models LAMA,
SAN, TE and BERT models on YELP-P dataset when viewed against
the model parameters (x-axis). LAMA outperforms SAN and TE while
also being more parameter efﬁcient. BERT outperforms LAMA by
1.8% but by more than an order of magnitude increase in parameters.
5.3 Parameters vs Accuracy
On the YELP-P dataset our model is outperformed by BERT by
1.8 %. However it it worth noting the cost of this performance im-
provement. Fig. 3 shows the accuracy of different models and their
corresponding parameters on the YELP-P dataset. It can be seen that
LAMA+ctx outperforms SAN & TE with fewer parameters (differ-
ence being linear). BERT slightly outperforms LAMA+ctx however
with more than an order of magnitude increase in parameters.
5.4 Contextual Attention Weights
To verify that our model captures context dependent word impor-
tance and is interpretable we peform qualitative analysis. We plot the
distribution of the attention weights of the positive words ‘amazing’,
‘happy’ and ‘recommended’ and negative words ‘poor’, ‘terrible’
and ‘worst’ from the test split of the Yelp data set as shown in Figure
4. We plot the distribution when conditioned on the ratings of the
reviews from 1 to 5. It can be seen from Figure 4 that the weight
of positive words concentrates on the low end in the reviews with
rating 1 (blue curve). As the rating increases, the weight distribution
shifts to the right. This indicates that positive words play a moreimportant role for reviews with higher ratings. The trend is opposite
for the negative words where words with negative connotation have
lower attention weights for reviews with rating 5 (purple curve).
However, there are a few exceptions. For example, it is intuitive that
‘amazing’ gets a high weight for reviews with high ratings but it also
gets a high weight for reviews with rating 2 (orange curve). This is
because, inspecting the Yelp dataset we ﬁnd that ‘amazing’ occurs
quite frequently with the word ‘not’ in the reviews with rating 2;
‘above average but not amazing’, ‘was ok but not amazing’. Our
model captures this phrase-level context and assigns similar weights
to ‘not’ and ‘amazing’. ‘not’ being a negative word gets a high
weight for lower ratings and hence so does ‘amazing’. Similarly,
other exceptions such as ‘terrible’ for rating 4 can be explained due
to the fact that customers might dislike one aspect of a business such
as their service but like another aspect such as food.
To further illustrate context-dependent word importance Table 6
lists top attended keywords for Yelp and Reuters datasets. Note
that superlatives such ‘100 stars’ appear in the list which are strong
indicators of the sentiment of a review.
5.5 Why Multiple Heads
0 3 6 9 12 15 180.600.620.640.660.68AccuracyYelp-L
m=1
m=5
m=10
m=15
m=20
0 3 6 9 12 15 180.7000.7250.7500.7750.8000.8250.8500.875IMDB
m=1
m=5
m=10
m=15
m=20
Epoch
Figure 5: Figure shows the effect of using multiple attention heads.
Validation accuracy of LAMA+Ctx is plotted for different values of m
for the Yelp-L dataset(left) and the IMDB dataset(right). x-axis indi-
cates the number of epochs, /y.alt-axis indicates the accuracy. Accuracy
peaks at m=15for both Yelp-L and IMDB.
8

--- PAGE 9 ---
Table 6: Top attended words for Yelp dataset from reviews with ratings
1 and 5 (indicated in paratheses) and Reuters(r8) Dataset.
Yelp(1) Yelp(5) r8(ship) r8(money)
inconsiderate
rudest
goodnight
worst
ever
boycott
loud
livid
some
hassle
friendly
ugh
dealership
brutal
rather
1
pizza
slow
torture
absoluterecommend
trust
referred
downside
professional
100
happy
attentive
stars
please
delicious
safe
worth
very
would
blown
sensitive
removed
impressed
lookskuwait
gulf
south
tanker
cargo
warships
pentagon
says
begin
iranian
shipping
from
demand
salvage
trade
production
japan
gulf
india
combatcurrencies
monetary
miyazawa
stoltenberg
accord
louvre
fed
cooperation
rate
poehl
exchange
stability
currency
german
reagan
pact
policy
support
trade
deﬁcit
0.00 0.02 0.040255075amazing
0.00 0.02 0.040204060happy
0.00 0.02 0.04020406080recommend
0.00 0.02 0.04
poor0255075
0.00 0.02 0.04
terrible050100150200
0.00 0.02 0.04
worst050100150
1 2 3 4 5
Figure 4: Attention weight ( x-axis ) distribution of the positive words
‘amazing’, ‘happy’ & ‘recommend’ and negative words ‘poor’, ‘terri-
ble’, & ‘worst’. Positive words tend to get higher weights in reviews
with higher ratings (3-5) whereas negative words get higher weights for
lower ratings (1-2) Example ‘terrible’ and ‘poor’ get a very low weight
for reviews with ratings 5 and ‘recommend’ and ‘amazing’ get high at-
tention weights for reviews with ratings 4 and 5.
Previous works have shown that more heads does not necessarily
lead to better performance for machine translation tasks [ 28]. In this
analysis, we seek to answer a similar question for text classiﬁcation.
We evaluate the model performance as we vary the number of
attention heads mfrom 1 to 25. Speciﬁcally, we plot the validation
accuracy vs. epochs for different values of m, for the Yelp-L andIMDB datasets. We vary mfrom 1 to 20 to get 5 models with
m=1,m=5,m=10,m=15andm=20. The plots are
shown in Figure 5. From the ﬁgure we can see that for the Yelp-
L dataset model performance peaks for m=15and then starts
falling for m=20. We can clearly see a signiﬁcant difference
between m=1andm=20, showing that having a multi-aspect
attention mechanism helps. For the IMDB dataset model with m=
15performs the best whereas model with m=1performs the worst
although performances for m=5;15;20are similar.
6 CONCLUSION
In this paper we presented a novel compact multi-head attention
mechanism and illustrated its effectiveness on text classiﬁcation
benchmarks. The proposed method computes multiple attention
distributions over words which leads to contextual sentence rep-
resentations. The results showed that this mechanism performed
better than several other approaches with fewer parameters. We
also veriﬁed that the model captured context-dependent word impor-
tance. We envision two directions for future work – 1) Currently, the
model relies on RNNs that makes it slower due to their sequential
computation. We seek to investigate ways of adapting the proposed
mechanism in Transformer-style self-supervised language models
such as BERT, XLNet etc. without dependency on RNNs by incorpo-
rating positional embeddings [ 11] for faster and efﬁcient learning of
language representations; 2) currently, the model uses a single global
context vector as the query vector that conﬂates the entire sentence
into one vector which could lead to information loss. Transformer
models on the other hand use ﬁne-grained context by querying every
word in the sequence for each candidate word. Even though this may
help develop direct connections between relevant words it might get
redundant. In the future work, we seek to investigate ways of incor-
porating phrase-level queries as a middle ground between a single
global context vector like the proposed approach and ﬁne-grained
queries like Transformer providing a balance between complexity
and context.
REFERENCES
[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A Simple but Tough-to-Beat
Baseline for Sentence Embeddings. (2017).
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine
Translation by Jointly Learning to Align and Translate. CoRR abs/1409.0473
(2014).
[3]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading
Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-
pers) . Association for Computational Linguistics, Vancouver, Canada, 1870–1879.
https://doi.org/10.18653/v1/P17-1171
[4]Ting Chen, Ji Lin, Tian Lin, Song Han, Chong Wang, and Denny Zhou. 2018.
Adaptive mixture of low-rank factorizations for compact neural modeling. (2018).
[5] Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long Short-Term Memory-
Networks for Machine Reading. In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing . Association for Computational
Linguistics, Austin, Texas, 551–561. https://doi.org/10.18653/v1/D16-1053
[6]Kyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
Representations using RNN Encoder–Decoder for Statistical Machine Translation.
InProceedings of the 2014 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) . Association for Computational Linguistics, Doha,
Qatar, 1724–1734. https://doi.org/10.3115/v1/D14-1179
[7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
InNIPS 2014 Workshop on Deep Learning, December 2014 .
9

--- PAGE 10 ---
[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for Language Under-
standing. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) . Association for Computational Linguistics,
Minneapolis, Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423
[9] Dua Dheeru and Eﬁ Karra Taniskidou. 2017. UCI Machine Learning Repository.
http://archive.ics.uci.edu/ml
[10] Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, and Ruslan Salakhut-
dinov. 2017. Gated-Attention Readers for Text Comprehension. In Proceedings
of the 55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) . Association for Computational Linguistics, Vancouver,
Canada, 1832–1846. https://doi.org/10.18653/v1/P17-1168
[11] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.
2017. Convolutional Sequence to Sequence Learning. In Proceedings of the
34th International Conference on Machine Learning - Volume 70 (ICML’17) .
JMLR.org, 1243–1252.
[12] Hongyu Guo, Colin Cherry, and Jiang Su. 2017. End-to-end multi-view networks
for text classiﬁcation. arXiv preprint arXiv:1704.05907 (2017).
[13] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will
Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and
comprehend. In Advances in neural information processing systems . 1693–1701.
[14] Sepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Comput. 9, 8 (Nov. 1997), 1735–1780. https://doi.org/10.1162/neco.1997.
9.8.1735
[15] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-
tuning for Text Classiﬁcation. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) . Association
for Computational Linguistics, Melbourne, Australia, 328–339. https://doi.org/
10.18653/v1/P18-1031
[16] Jack Kiefer, Jacob Wolfowitz, et al .1952. Stochastic estimation of the maximum
of a regression function. The Annals of Mathematical Statistics 23, 3 (1952),
462–466.
[17] Jin-Hwa Kim et al .2017. Hadamard Product for Low-rank Bilinear Pooling. In
ICLR . OpenReview.net. https://openreview.net/forum?id=r1rhWnZkg
[18] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classiﬁcation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP) . Association for Computational Linguistics, Doha, Qatar,
1746–1751. https://doi.org/10.3115/v1/D14-1181
[19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic
Optimization. In 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,
Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980
[20] Oleksii Kuchaiev and Boris Ginsburg. 2017. Factorization tricks for LSTM
networks. CoRR abs/1703.10722 (2017). arXiv:1703.10722 http://arxiv.org/abs/
1703.10722
[21] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning
of language representations. arXiv preprint arXiv:1909.11942 (2019).
[22] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. 2015. Bilinear cnn
models for ﬁne-grained visual recognition. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision . 1449–1457.
[23] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang,
Bowen Zhou, and Yoshua Bengio. 2017. A Structured Self-Attentive Sentence
Embedding. https://openreview.net/forum?id=BJC jUqxe
[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:
A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[25] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective
Approaches to Attention-based Neural Machine Translation. In Proceedings
of the 2015 Conference on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics, Lisbon, Portugal, 1412–1421.
https://doi.org/10.18653/v1/D15-1166
[26] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng,
and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In
Proceedings of the 49th annual meeting of the association for computational lin-
guistics: Human language technologies-volume 1 . Association for Computational
Linguistics, 142–150.
[27] Sneha Mehta, Mohammad Raihanul Islam, Huzefa Rangwala, and Naren Ramakr-
ishnan. 2019. Event Detection Using Hierarchical Multi-Aspect Attention. In The
World Wide Web Conference (WWW ’19) . ACM, New York, NY , USA, 3079–3085.
https://doi.org/10.1145/3308558.3313659
[28] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really
Better than One?. In Advances in Neural Information Processing Systems . 14014–
14024.
[29] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. InAdvances in neural information processing systems . 3111–3119.
[30] Ankur P Parikh, Oscar T ¨ackstr ¨om, Dipanjan Das, and Jakob Uszkoreit. 2016. A
decomposable attention model for natural language inference. arXiv preprint
arXiv:1606.01933 (2016).
[31] Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced
model for abstractive summarization. arXiv preprint arXiv:1705.04304 (2017).
[32] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Represen-
tations. In Proceedings of the 2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers) . Association for Computational Linguistics, New Orleans,
Louisiana, 2227–2237. https://doi.org/10.18653/v1/N18-1202
[33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. [n.d.]. Language models are unsupervised multitask learners. ([n. d.]).
[34] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017.
Bidirectional Attention Flow for Machine Comprehension. In 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings . OpenReview.net. https://openreview.net/
forum?id=HJ0UKP9ge
[35] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The
sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017).
[36] Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang
Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, and Lawrence Carin. 2018.
Baseline Needs More Love: On Simple Word-Embedding-Based Models and
Associated Pooling Mechanisms. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) . Association
for Computational Linguistics, Melbourne, Australia, 440–450. https://doi.org/
10.18653/v1/P18-1041
[37] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In
International Conference on Learning Representations . https://openreview.net/
forum?id=SygXPaEYvH
[38] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, and E. Weinan. 2016. Convolu-
tional neural networks with low-rank regularization. 4th International Conference
on Learning Representations, ICLR 2016 ; Conference date: 02-05-2016 Through
04-05-2016.
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. In Advances in neural information processing systems . 5998–6008.
[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al .
2019. Transformers: State-of-the-art Natural Language Processing. arXiv preprint
arXiv:1910.03771 (2019).
[41] Zhilin Yang et al .2019. Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. In Advances in neural information processing systems .
5754–5764.
[42] Mo Yu, Matthew R Gormley, and Mark Dredze. 2015. Combining word embed-
dings and feature embeddings for ﬁne-grained relation extraction. In Proceedings
of the 2015 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies . 1374–1379.
[43] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. 2017. Multi-modal factorized
bilinear pooling with co-attention learning for visual question answering. In
Proceedings of the IEEE international conference on computer vision . 1821–
1830.
[44] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional
networks for text classiﬁcation. In Advances in neural information processing
systems . 649–657.
10
