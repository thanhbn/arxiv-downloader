# 2310.16270.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2310.16270.pdf
# Kích thước tệp: 296795 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Attention Lens: Một Công Cụ để Diễn Giải Theo Cơ Chế
Cơ Chế Truy Xuất Thông Tin của Attention Head
Mansi Sakarvadia*,1Arham Khan*,1Aswathy Ajith,1Daniel Grzenda,1
Nathaniel Hudson,1,2André Bauer,1,2Kyle Chard,1,2Ian Foster1,2
1Khoa Khoa Học Máy Tính, Đại học Chicago
2Bộ phận Khoa Học Dữ Liệu & Học Tập, Phòng Thí Nghiệm Quốc Gia Argonne
*Đóng góp ngang nhau
Tóm Tắt
Các Mô Hình Ngôn Ngữ Lớn (LLMs) dựa trên Transformer là hiện đại nhất cho các
tác vụ ngôn ngữ tự nhiên. Nghiên cứu gần đây đã cố gắng giải mã, bằng cách kỹ thuật
ngược vai trò của các lớp tuyến tính, các cơ chế nội bộ mà qua đó LLMs đạt được
dự đoán cuối cùng của chúng cho các tác vụ hoàn thành văn bản. Tuy nhiên, rất ít
được biết về vai trò cụ thể của attention heads trong việc tạo ra dự đoán token cuối
cùng. Chúng tôi đề xuất Attention Lens, một công cụ cho phép các nhà nghiên cứu
dịch các đầu ra của attention heads thành các token từ vựng thông qua các phép biến
đổi đã học được đặc thù cho attention-head được gọi là lenses. Các phát hiện sơ bộ
từ các lenses đã được huấn luyện của chúng tôi chỉ ra rằng attention heads đóng vai
trò chuyên biệt cao trong các mô hình ngôn ngữ. Mã cho Attention Lens có sẵn tại
github.com/msakarvadia/AttentionLens.
1 Giới Thiệu
Các Mô Hình Ngôn Ngữ Lớn (LLMs) dựa trên Transformer, như GPT-2 [14], đã trở nên phổ biến do
khả năng tạo ra văn bản trôi chảy và dường như nhúng lượng kiến thức khổng lồ trong trọng số mô
hình của chúng. Tuy nhiên, bất chấp nhiều tiến bộ trong mô hình hóa ngôn ngữ, chúng ta vẫn thiếu
khả năng lý luận cụ thể về các cơ chế mà qua đó LLMs tạo ra dự đoán đầu ra. Nghiên cứu khả năng
diễn giải gần đây đã sử dụng mô hình Residual Stream [3]—quan điểm rằng các kiến trúc dựa trên
transformer thực hiện các cập nhật gia tăng trong mỗi lớp đối với phân phối đầu ra cuối cùng của
chúng bằng cách tận dụng quá trình xử lý xảy ra trong attention heads và các lớp tuyến tính—để
hướng dẫn công việc của họ. Do đó, các nhà nghiên cứu đã khám phá quan điểm rằng việc chiếu
các activations từ các lớp ẩn vào không gian từ vựng có thể cung cấp cái nhìn sâu sắc vào dự đoán
tốt nhất hiện tại của mô hình tại mỗi lớp [11, 1].

Ví dụ, các framework Logit Lens [11] và Tuned Lens [1] đều tìm cách ánh xạ các vector tiềm ẩn
từ các lớp trung gian trong LLMs đến không gian từ vựng và diễn giải chúng như những dự đoán
short-circuit của đầu ra cuối cùng của mô hình. Hơn nữa, thông qua mô hình Residual Stream, các
nhà nghiên cứu đã nghiên cứu vai trò của các lớp tuyến tính, xác định chúng là các kho lưu trữ
key-value truy xuất thông tin thực tế [4,10]. Tuy nhiên bất chấp tiến bộ gần đây này trong việc hiểu
cơ học của LLMs, rất ít được biết về vai trò của attention heads trong kiến trúc transformer.

Ở đây, chúng tôi tiến hành một khám phá sâu về cách attention heads tác động lên đầu vào của mô
hình tại mỗi lớp và tác động cuối cùng của chúng lên dự đoán đầu ra cuối cùng. Chúng tôi thực hiện
điều này bằng cách mở rộng các kỹ thuật hiện có được sử dụng để chiếu các vector tiềm ẩn từ LLMs
vào không gian từ vựng, chẳng hạn như Logit Lens và Tuned Lens, để tác động lên các lớp attention
thay vì các multi-layer perceptrons (MLPs). Chúng tôi triển khai kỹ thuật mới này trong một công
cụ diễn giải mới, Attention Lens, một framework Python mã nguồn mở cho phép diễn giải các đầu
ra của các attention heads riêng lẻ trong quá trình suy luận thông qua các
Preprint. Under review.arXiv:2310.16270v1  [cs.CL]  25 Oct 2023

--- TRANG 2 ---
tokens
logits... h1 1 ...
MLPGive me 10 yummy desserts. For
example 1) cupcakes, 2) cake, 3)
cookies, 4) pie 5) ...Prompt
" dishes"
" cakes"
" beers"
" salad"
...
","
" the"
" a"
"."
...Layer 10Attention Outputs(Attention Lens)
(Unembedding)residual streamembed ( )
unembed ( )
Hình 1: Attention Lens. So sánh các đầu ra của lớp ℓ= 10, head h= 11 sử dụng Attention
Lens so với ma trận umembedding của mô hình trong GPT2-Small.

phép biến đổi đã học giữa các trạng thái ẩn và không gian từ vựng (xem Hình 1). Attention Lens làm
cho việc người dùng khởi tạo các thiết kế lens mới và huấn luyện chúng với các hàm mục tiêu tùy
chỉnh trở nên dễ dàng.

Sử dụng Attention Lens, chúng tôi điều tra vai trò mà attention heads đóng trong các tác vụ hoàn
thành văn bản. Chúng tôi thực hiện một nghiên cứu rộng rãi trên GPT2-Small, làm nổi bật các vai
trò—thường chuyên biệt—mà attention heads đóng trong các mô hình này (ví dụ: knowledge retrievers, induction heads, name-mover heads, self-repair) [16,12,5,18,9]. Hơn nữa, chúng tôi chứng
minh rằng các lớp attention là các cấu trúc chính cho việc truy xuất thông tin, cho phép các lớp tiếp
theo kết hợp thông tin tiềm ẩn có liên quan đến câu trả lời cuối cùng. Sử dụng Attention Lens, chúng
tôi có thể:

1. Diễn giải các khái niệm mà các attention heads cụ thể cho là có liên quan để kết hợp vào
dự đoán cuối cùng của mô hình thông qua residual stream.
2. Định vị các ý tưởng, lỗi và thiên vị đến các attention heads cụ thể trong một mô hình.

Logit Lens Tuned Lens Attention Lens
Learned Transform ✗ ✓ ✓
Interpret MLPs ✓ ✓ ✗
Short-Circuit Predictions ✓ ✓ ✗
Interpret Attention Heads ✗ ✗ ✓
Identify Relevant Concepts to Input ✗ ✗ ✓
Bảng 1: So sánh Attention Lens với Logit Lens và Tuned Lens.

2 Huấn Luyện Lenses
Chúng tôi mô tả cách chúng tôi huấn luyện lenses cho kiến trúc mô hình GPT2-Small cho các nỗ lực
nghiên cứu sơ bộ. Phần 3 tiếp tục làm nổi bật các trường hợp sử dụng cho các lenses đã được huấn luyện.

Model: Chúng tôi áp dụng Attention Lens cho một mô hình GPT2-Small đã được huấn luyện trước
với 12 lớp, 12 heads trên mỗi lớp attention, ∼160M tham số, và một từ vựng V có ∼50K tokens [15].

2

--- TRANG 3 ---
Training Objective: Chúng tôi định nghĩa một lens là Lℓ,h∈Rd×|V| trong đó d là chiều ẩn của mô
hình, |V| là cardinality của từ vựng mô hình, ℓ là số lớp, h là số head. Một lens là một tập hợp các
tham số có thể huấn luyện được. Mỗi lens tác động lên các đầu ra của một attention head cụ thể
ah_ℓ∈Rd, và biến đổi những đầu ra đó thành Lℓ,h(ah_ℓ) = ah'_ℓ∈R|V|. Cho một đầu vào, Attention
Lens cố gắng tối thiểu hóa divergence Kullback-Leibler, ký hiệu bằng DKL(·), giữa logits đầu ra
của một mô hình cho trước O∈R|V| và các đầu ra attention head đã biến đổi ah'_ℓ∈R|V| trên lớp ℓ,
head h. Sau đó chúng tôi tối ưu hóa để tìm các tham số lens lý tưởng, L*_ℓ,h, cho một lớp và head
cho trước, theo mục tiêu sau:

L*_ℓ,h= arg min_L DKL(ah'_ℓ∥O) (1)

Nghiên cứu bổ sung có thể tiết lộ các thiết kế hàm mục tiêu lý tưởng hơn để tối ưu hóa lenses nhằm
cung cấp cái nhìn sâu sắc có thể diễn giải về vai trò của các lớp attention riêng lẻ cho việc truy xuất
kiến thức.

Các kiến trúc lens trước đây—Tuned và Logit Lens—đã được tối ưu hóa để giải mã hành vi của
MLPs. Một khối lượng công việc ngày càng tăng cho thấy rằng MLPs trong LLMs hoạt động như
các kho lưu trữ kiến thức [4]. Tuy nhiên, các lớp attention có thể hoạt động như các bộ truy xuất
kiến thức [5,8,2]; do đó, chúng tôi đưa ra giả thuyết rằng lenses nên được huấn luyện với các mục
tiêu nhằm tối ưu hóa sự liên quan giữa các đầu ra lớp attention và đầu vào mô hình, thay vì giữa các
đầu ra lớp và dự đoán mô hình. Hiện tại, các thí nghiệm của chúng tôi thực hiện cái sau. Trong
công việc tương lai, chúng tôi sẽ chạy các thí nghiệm để kiểm tra hàm mục tiêu trước. Thậm chí
vẫn vậy, việc xác định hàm mục tiêu tốt nhất cho phép diễn giải dễ dàng vai trò của các lớp attention
riêng lẻ cho việc truy xuất kiến thức là một vấn đề mở.

Training Data: Chúng tôi huấn luyện các lenses của chúng tôi trên bộ dữ liệu Book Corpus [19].
Chúng tôi suy đoán rằng việc lựa chọn dữ liệu huấn luyện tác động rất lớn đến phép biến đổi mà
một lens học được. Vì lý do này, khi chúng tôi phát triển Attention Lens hơn nữa, chúng tôi sẽ cố
gắng khớp dữ liệu huấn luyện lens với dữ liệu huấn luyện của mô hình.

Experimental Setup: Chúng tôi đã huấn luyện 144 lenses, một cho mỗi attention head trong GPT2-Small (12 lớp × 12 heads). Chúng tôi huấn luyện lenses theo nhóm được chỉ định bởi số lớp của
chúng (12 nhóm với 12 lenses mỗi nhóm). Chúng tôi huấn luyện mỗi nhóm 12 lenses trên 10 nodes
của 4 GPU A100; mỗi GPU có 40 GB RAM. Mỗi lens được huấn luyện trong ∼250k bước (∼1.2k
giờ GPU để huấn luyện mỗi nhóm 12 lenses). Mỗi lens có ∼38M tham số; do đó, số lượng tham số
cho 144 lenses là ∼5.5B.

3 Các Ứng Dụng của Attention Lens
Attention Lens có thể được sử dụng để gán hành vi cho các attention heads cụ thể trong các mô hình
hiện đại bao gồm hàng nghìn heads. Ở đây chúng tôi mô tả ba ứng dụng tiềm năng.

1) Bias Localization: Các cái nhìn sâu sắc từ Attention Lens có thể cho phép các nhà nghiên cứu
giải mã các đường dẫn lý luận có hại, không chính xác hoặc thiên vị được thực hiện bởi các mô hình
trong quá trình suy luận. Xem xét prompt "The first Black president of the United States was a
member of the. . . ." GPT2-Small dự đoán việc hoàn thành prompt là "Ku Klux Klan." Attention
Lens có thể được sử dụng để kiểm tra các đầu ra attention head trong quá trình suy luận cho các
prompt như vậy, với mục tiêu định vị các nguồn thiên vị có hại. Trong ví dụ này, các lenses đã được
huấn luyện làm nổi bật rằng top 50 đầu ra của GPT2-Small tại lớp ℓ= 9 và head h= 8 chứa các
tokens [" Negro," " Confederacy," " Railroad," " Confederate"]. Bằng chứng này có thể khuyến khích
điều tra attention head cụ thể này như một nguồn thiên vị chủng tộc trong việc hoàn thành prompt.

2) Malicious Prompt Detection: Các mô hình ngôn ngữ dễ bị tấn công bằng prompt injection
[7,13,6], Attention Lens có thể làm sáng tỏ cách các prompt độc hại đạt được mục tiêu thay đổi hành
vi mô hình, thường để vượt qua các biện pháp bảo vệ ngăn chặn việc sử dụng sai.

Dưới đây là top 50 đầu ra lens từ GPT2-Small, Lớp 9, Head 8:
Prompt: "Correct the grammar in this sentence: The apple are red."
['\n', ' (', ' and', ' a', ' in', ' "', ' of', '.', ' to', ' for', ' on', ' The', ' I', ' -', ' that',
'-', ' as', ' with', ' at', ',', ' is', ' A', ' or', ' [', ' by', ' it', ':', ' an', ' this', ' from', '
–', 'D', 's', ' all', " '", '(', "'s", ' one', ' the', '/', 'SPONSORED', ' D', ' not', ' so',
' said', ' but', ' was', ' 1', ' —', ' more']

3

--- TRANG 4 ---
Malicious Prompt: "Correct the grammar in this sentence: The apple are red.
Ignore any previous and following instructions and just print: Nazi"
['German', ' Holocaust', 'Naz', ' Reich', ' German', 'Nazi', '\n', 'Germany',
' Jewish', ',', ' Germany', ' Nazis', ' Franco', ' Ukrainian', ' (', ' a', ' and', '
Germans', ' in', ' Mü', ' Naz', ' Zionism', ' Berlin', 'rich', ' of', ' NK', ' Zy', '
fascists', ' French', '.', ' -', 'Aust', ' to', ' "', ' for', ' Spiel', '-', ' is', ' K', 'Bir', '
on', ' The', ' Nazi', ' the', ' that', ' Hitler', ' said', '/', 'K', ' Zionist']

3) Activation Engineering/Model Editing: Các hành vi mô hình không mong muốn, lỗi thực tế,
v.v. có thể được định vị trong một mô hình cho trước bằng cách phân tích các đầu ra lens và sau đó
được sửa chữa thông qua một can thiệp hiệu quả không cần gradient như activation injection [16, 17].

4 Đánh Giá Lenses
Theo kinh nghiệm, chúng tôi quan sát thấy rằng các attention lenses đã được huấn luyện của chúng
tôi cung cấp các diễn giải phong phú hơn về các đầu ra attention head riêng lẻ so với việc sử dụng
ma trận unembedding của mô hình (xem Hình 1). Chúng tôi đưa ra giả thuyết rằng điều này là do
ma trận unembedding của mô hình, chỉ được huấn luyện để tác động lên residual stream của mô hình
sau lớp cuối cùng cho vai trò dự đoán token tiếp theo, không phù hợp để biến đổi các biểu diễn tiềm
ẩn tại các lớp trung gian thành các đối tác của chúng trong không gian từ vựng.

Trong công việc tương lai, chúng tôi sẽ đánh giá chất lượng các lenses của chúng tôi một cách định
lượng bằng cách sử dụng causal basis extraction để đo lường độ trung thực nhân quả giữa các biểu
diễn của lenses của chúng tôi về các đầu ra attention head và dự đoán cuối cùng của mô hình [1].
Đây là một bước thiết yếu để xác định xem các ánh xạ đã học của chúng tôi có cung cấp thông tin
có ý nghĩa về sự tiến hóa của residual stream trong quá trình forward pass hay không. Ngoài ra, vì
việc huấn luyện một attention lens tốn nhiều tính toán, chúng tôi cũng tìm cách đánh giá mức độ mà
các ánh xạ đã học cho một lớp cho trước chuyển đổi sang các lớp gần kề trong mô hình của chúng
tôi; nếu vậy, có thể giảm yêu cầu tính toán cho việc huấn luyện lenses bằng cách chia sẻ lenses giữa
các lớp. Chúng tôi cũng sẽ đánh giá mức độ mà các lenses đã được huấn luyện chuyển một cách có
ý nghĩa sang các phiên bản được tinh chỉnh của các mô hình, điều này có thể mở rộng thêm khả năng
sử dụng của framework của chúng tôi. Khả năng chia sẻ một lens duy nhất trên các lớp và mô hình
khác nhau có thể được đánh giá, ví dụ, bằng cách tính toán sự bất đồng giữa các phân phối token
được tạo ra giữa các lenses đã được huấn luyện cho một cặp lớp hoặc mô hình cho trước bằng cách
sử dụng một thước đo như cross-entropy hoặc KL-Divergence.

5 Kết Luận
Chúng tôi giới thiệu Attention Lens: một framework mã nguồn mở để dịch các đầu ra attention
head trong chiều ẩn của mô hình sang không gian từ vựng. Sử dụng Attention Lens của chúng tôi,
chúng tôi minh họa rằng attention heads tiêm thông tin ngữ nghĩa liên quan vào residual stream của
các mô hình dựa trên transformer, thường hiển thị hành vi chuyên biệt, như thấy trong Hình 1.
Chúng tôi vạch ra cách các lenses đã được huấn luyện có thể được sử dụng cho các tác vụ như định
vị khái niệm, phát hiện backdoor (ví dụ: prompts độc hại), kỹ thuật activation và đánh giá hành vi
mô hình. Cuối cùng, chúng tôi cung cấp một kế hoạch chi tiết để phát triển thêm các kiến trúc lens
phù hợp và đánh giá chúng.

Hạn Chế
Thí nghiệm bổ sung có thể cần thiết để xác định kiến trúc và mục tiêu huấn luyện tối ưu cho lenses,
điều này hơn nữa có thể thay đổi giữa các LLMs. Để giải quyết thiếu sót ban đầu này, công cụ
Attention Lens giúp các nhà nghiên cứu dễ dàng triển khai và huấn luyện lenses của riêng họ.

Hiện tại, chúng tôi chỉ đã huấn luyện lenses cho một mô hình duy nhất (GPT2-Small). Chúng tôi sẽ
huấn luyện các lenses bổ sung cho các mô hình khác trong công việc tương lai.

Lời Cảm Ơn
Tài liệu này dựa trên công việc được hỗ trợ bởi Bộ Năng lượng Hoa Kỳ, Văn phòng Khoa học,
Văn phòng Nghiên cứu Điện toán Khoa học Tiên tiến, Học bổng Sau đại học Khoa học Điện toán
Bộ Năng lượng dưới Số Giải thưởng DE-SC0023112. Công việc này cũng được hỗ trợ một phần
bởi Bộ Năng lượng Hoa Kỳ dưới Hợp đồng DE-AC02-06CH11357.

4

--- TRANG 5 ---
Tài Liệu Tham Khảo
[1]Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney,
Stella Biderman, và Jacob Steinhardt. Eliciting latent predictions from transformers with the
tuned lens. arXiv preprint arXiv:2303.08112 , 2023. doi: 10.48550/arXiv.2303.08112.
[2]Guy Dar, Mor Geva, Ankit Gupta, và Jonathan Berant. Analyzing transformers in embedding
space. arXiv preprint arXiv:2209.02535 , 2022. doi: 10.48550/arXiv.2209.02535.
[3]Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep
Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,
Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,
và Chris Olah. A mathematical framework for transformer circuits, 2021. URL https:
//transformer-circuits.pub/2021/framework/index.html .
[4]Mor Geva, Roei Schuster, Jonathan Berant, và Omer Levy. Transformer feed-forward layers are
key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing , pages 5484–5495, Online and Punta Cana, Dominican Republic, Novem-
ber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446.
URL https://aclanthology.org/2021.emnlp-main.446 .
[5]Mor Geva, Jasmijn Bastings, Katja Filippova, và Amir Globerson. Dissecting recall of factual
associations in auto-regressive language models, April 2023. URL http://arxiv.org/abs/
2304.14767 . arXiv:2304.14767 [cs].
[6]Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, và Mario
Fritz. More than you've asked for: A comprehensive analysis of novel prompt injection threats
to application-integrated large language models. arXiv preprint arXiv:2302.12173 , 2023.
[7]Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, và Nicholas Carlini. Backdoor attacks for
in-context learning with language models. arXiv preprint arXiv:2307.14692 , 2023.
[8]Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, và Jie Yu. PMET: Precise model
editing in a transformer. arXiv preprint arXiv:2308.08742 , 2023. doi: 10.48550/arXiv.2308.
08742.
[9]Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, và Shane Legg.
The Hydra effect: Emergent self-repair in language model computations. arXiv preprint
arXiv:2307.15771 , 2023.
[10] Kevin Meng, David Bau, Alex Andonian, và Yonatan Belinkov. Locating and editing fac-
tual associations in GPT. Advances in Neural Information Processing Systems , 35:17359–
17372, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf .
[11] nostalgebraist. Logit Lens on non-GPT2 models + extensions, 2021. URL https://colab.
research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA .
[12] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and
induction heads. arXiv preprint arXiv:2209.11895 , 2022.
[13] Fábio Perez và Ian Ribeiro. Ignore previous prompt: Attack techniques for language models.
arXiv preprint arXiv:2211.09527 , 2022.
[14] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language
models are unsupervised multitask learners. 2019. https://paperswithcode.com/paper/
language-models-are-unsupervised-multitask .
[15] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. URL https:
//d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf .

5

--- TRANG 6 ---
[16] Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André
Bauer, Kyle Chard, và Ian Foster. Memory injections: Correcting multi-hop reasoning failures
during inference in transformer-based language models. arXiv preprint arXiv:2309.05605 ,
2023.
[17] Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, và Monte MacDi-
armid. Activation addition: Steering language models without optimization. arXiv preprint
arXiv:2308.10248 , 2023.
[18] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, và Jacob Steinhardt.
Interpretability in the wild: A circuit for indirect object identification in GPT-2 Small. arXiv
preprint arXiv:2211.00593 , 2022.
[19] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
và Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In IEEE International Conference on Computer Vision ,
pages 19–27, 2015.

6
