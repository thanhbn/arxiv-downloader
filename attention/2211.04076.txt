# 2211.04076.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2211.04076.pdf
# File size: 92932 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2211.04076v1  [cs.LG]  8 Nov 2022Linear Self-Attention Approximation via
Trainable Feedforward Kernel
Uladzislau Yorsh and Alexander Kovalenko[0000−0002−7194−1874]
Faculty of Information Technology, Czech Technical Univer sity in Prague
Prague, Czech Republic
{yorshula, kovalale }@fit.cvut.cz
Restrictive limitation of Transformers [18] due to the quadratic com plexity
of self-attention mechanism motivated a new research ﬁeld of eﬃcient Trans-
formers [17], which approximate the original architecture with asymptotically
faster models.
Despite the fact that Transformers are pervasive, unbiased and able to virtu-
ally handle arbitrarilylong dependencies, the quadraticspace and tim e complex-
ity limit Transformer applications on long sequences. In this connect ion, various
ﬁndings onapproximatingtheattention with asymptoticallyfasterm oduleshave
been made in order to tackle longer sequences. However, given the absence of a
uniﬁed and systematic benchmark, overall evaluating remained unc ertain until
Tay et al. [16] published the benchmark for eﬃcient Transformer mo dels called
”Long Range Arena”, that consists of task of various data types .
In pursue of the faster computation, Eﬃcient Transformers dem onstrate an
impressivevarietyofapproaches—modelsattainingsub-quadratic attentioncom-
plexity can utilize a notion of sparsity [2,3,15] or a low-rank approxim ation of
inputs [19,20] to reduce an amount of attended keys; another wa ys to reduce
complexity include locality-sensitive hashing [12], key pooling [21], additio nal
memory to store information in compacted form [7,14] or hybridizat ion with
another architectures, such as CNNs [1,8].
Often based on strong mathematical basis, kernelized approache s allow to
approximate an attention with linear complexity while retaining high acc uracy.
The work by Katharopoulos et al. [11] describes an approximation c onsisting of
computing an attention by a dot product of projected queries and keys. Conse-
quently, the work by Choromanski et al.k [4] demonstrated that su ch an approx-
imation can be arbitrarily precise and mathematically robust, while the work
by Chowdhury et al. [5] reported that the projection can be learne d. Therefore,
in the present paper we aim to expand the idea of trainable kernel me thods to
approximate self-attention mechanism of the Transformer archit ecture.
Our contribution: given that feedforward neural network with at least one
hidden layer [6], arbitrary non-linearity, and arbitrary number of n eurons is able
toapproximateanywell-behavedfunctiontoanyaccuracythatgiv esfeedforward
neural network the potential of being universal approximator [9 ]. Therefore, we
propose that trainable kernel function φ(·) can approximate traditional softmax
attention eﬃciently. Therefore, we study the possibility of using th e feedforward
neuralnetworktorepresent φ(·). Weexperimentwiththearchitectureof φ(·) and
test its performance on the three Long Range Arena tasks—text classiﬁcation,

--- PAGE 2 ---
2 Yorsh et al.
document matching and ListOps, following the instruction on limitation [16] the
number of trainable parameters in the model to provide comparable metrics.
Kernelized Model. Kernelized models are based on the following factor-
ization of an attention operation:
Att(qi,K,V) =L/summationdisplay
j=1κ(qi,kj)/summationtextL
j′=1κ(qi,kj′)vj≈φ(qi)T/summationtextL
j=1φ(kj)vT
j
φ(qi)T/summationtextL
j=1φ(kj)
whereqiis a query token, KandVare key and value matrices, κ(q,k) is a kernel
function to model (exp( qTk) for a base Transformer) and φ(·) is a projection
function we approximate.The φ(·) is requiredto be positive to maintain numeric
stability, and can vary from simple functions like ELU + 1 to stochastic softmax
kernel approximations. In our work, instead of approximation str ategies with
strong priors we employ a general function such as feedforward N N.
Feedforward Kernel. We start with a single-layer FFN, deﬁned as:
φ(X) =Softplus (XW)
whereW∈Rn×nis the layer weight matrix. Surprisingly, this model already
shows the notable performance gain over the Performer and come s close to the
leader of the original LRA paper. Following the [10], we can boost perf ormance
by forcing the orthogonality via orthogonal initialization and regular ization.
We also tried to stack more layers, but observed no performance g ain – with
or without normalization layers in between. We tried the GELU and logis tic
sigmoid non-linearities.
GLU Kernel. Gated Linear Units are deﬁned as:
GLU(X) =XWf⊙σ(XWg)
whereσ(·) is a logistic sigmoid and Wf,Wgare weight matrices. This layer pro-
vides and element-wise nonlinearity and may represent more complex functions,
but requires a doubled parametrization compared to a linear one.
For the purposes of our model, we need to modify the last GLU to for ce the
positive output:
GLUoutput(X) =Softplus (XWf)⊙σ(XWg)
We also force the orthogonality of Wfin these units in the same way as in the
previous subsection. We refer this regularized units as O(rthogon al)GLU.
To mitigate the parametrization growth we apply transforms head- wise, and
suggest that gating does not require that amount of information a s the input
transform. Thus, we can approximate the Wgwith, say, two low-rank matrices
of sizesn×rresp.r×nwherer <n
2is theWgapproximation rank. We refer
this unit as A(pproximated)OGLU.

--- PAGE 3 ---
Linear Self-Attention Approximation 3
Gating. Compared to the orthogonalsingle-layer FFN, a single-layerOGLU
model converges even faster and shows signiﬁcantly less score va riance between
runs. These units can also be sequentially stacked with a beneﬁt, up to some
extent. On the other hand, the doubled parametrization will not allo w stacking
more than two units without going beyond the 10% of additional para meters.
By approximating the gating weight matrix, we are able to stack more units
– but according to the Table 1, this brings no advantage with the high er compu-
tational costs. We used the matrices of rank r=n/4 to approximate the gate,
reducing the layer parametrization by 25%.
Experiments. Following the recommendations from [16], we replicate the
learning schedule and all the hyperparameters that relate to our m odel, while
keeping additional parametrization below 10% Due to the limitation in co mpu-
tational power, we restrict ourselves only to the three LRA tasks —BPE text
classiﬁcation, BPE text matching and ListOps, with input lengths 4 K/4K/2K
respectively. To provide comparable and reproducible and results, we used the
gradient accumulation in order to simulate larger batch sizes. Each m odel was
trainedﬁvetimestoobservemodelbehaviorandtoavoidso-called black swans —
random seeds that give radically diﬀerent results [13]. Mean and best results are
reported in Table 1
Model Complexity Classif. Matching ListOps
Transformer O(L2) 64.27 57.46 36.37
Linear kernel†O(CL) 65.77 73.51 18.54
1×GLU O(CL) 65.82 72.17 18.67
2×GLU O(CL) 65.99 73.36 18.42
3×GLU O(CL) 65.87 72.60 18.68
Orth. linear kernel O(CL) 65.86 72.63 18.19
1×OGLU O(CL) 65.95 72.50 18.45
2×OGLU O(CL) 66.02 72.96 18.32
3×AOGLU O(CL) 66.06 72.57 18.45
Table 1: Results of our models on the chosen LRA tasks, mean result s for ﬁve
runs. We denote by †models that show signiﬁcant variance in results.
Acknowledgment
ThisresearchissupportedbytheCzechMinistryofEducation,You thandSports
fromthe CzechOperationalProgrammeResearch,Development, andEducation,
under grant agreement No. CZ.02.1.01/0.0/0.0/15003/0000421.

--- PAGE 4 ---
4 Yorsh et al.
References
1. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Att ention augmented convo-
lutional networks (2020)
2. Beltagy, I., Peters, M.E., Cohan, A.: Longformer: The lon g-document transformer
(2020)
3. Child, R., Gray, S., Radford, A., Sutskever, I.: Generati ng long sequences with
sparse transformers (2019)
4. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger , D., Colwell, L., Weller,
A.: Rethinking attention with performers (2021)
5. Chowdhury, S.P., Solomou, A., Dubey, A., Sachan, M.: On le arning the transformer
kernel (2021)
6. Cybenko, G.: Approximation by superpositions of a sigmoi dal function. Mathe-
matics of control, signals and systems 2(4), 303–314 (1989)
7. Dai, Z., Yang, Z., Yang, Y., Carbonell, J.G., Le, Q.V., Sal akhutdinov, R.:
Transformer-xl: Attentive language models beyond a ﬁxed-l ength context. CoRR
abs/1901.02860 (2019), http://arxiv.org/abs/1901.02860
8. Gulati, A., Qin, J., Chiu, C.C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S.,
Zhang, Z., Wu, Y., Pang, R.: Conformer: Convolution-augmen ted transformer for
speech recognition (2020)
9. Hornik, K.: Approximation capabilities of multilayer fe edforward networks. Neural
networks 4(2), 251–257 (1991)
10. Jia, K., Li, S., Wen, Y., Liu, T., Tao, D.: Orthogonal deep neural networks (2019)
11. Katharopoulos, A., Vyas, A., Pappas, N., Fleuret, F.: Tr ansformers are rnns: Fast
autoregressive transformers with linear attention (2020)
12. Kitaev, N., /suppress Lukasz Kaiser, Levskaya, A.: Reformer: The eﬃcient transformer (2020)
13. Picard, D.: Torch.manual seed(3407) is all you need: On the inﬂuence of random
seeds in deep learning architectures for computer vision (2 021)
14. Rae, J.W., Potapenko, A., Jayakumar, S.M., Lillicrap, T .P.: Compressive trans-
formers for long-range sequence modelling (2019)
15. Roy, A., Saﬀar, M., Vaswani, A., Grangier, D.: Eﬃcient co ntent-based sparse at-
tention with routing transformers (2020)
16. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pha m, P., Rao, J., Yang, L.,
Ruder, S., Metzler, D.: Long range arena: A benchmark for eﬃc ient transformers
(2020)
17. Tay, Y., Dehghani, M., Bahri, D., Metzler, D.: Eﬃcient tr ansformers: A survey
(2020)
18. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jon es, L., Gomez, A.N., Kaiser,
L., Polosukhin, I.: Attention is all you need (2017)
19. Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linforme r: Self-
attention with linear complexity. CoRR abs/2006.04768 (2020),
https://arxiv.org/abs/2006.04768
20. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., L i, Y., Singh, V.:
Nystr¨ omformer: A nystr¨ om-based algorithm for approxima ting self-attention.
CoRRabs/2102.03902 (2021), https://arxiv.org/abs/2102.03902
21. Zhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., Che n, W.: Poolingformer:
Long document modeling with pooling attention. CoRR abs/2105.04371 (2021),
https://arxiv.org/abs/2105.04371
