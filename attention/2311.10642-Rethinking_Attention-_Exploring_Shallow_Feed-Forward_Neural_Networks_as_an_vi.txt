# 2311.10642.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2311.10642.pdf
# Kích thước tệp: 940153 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Suy Nghĩ Lại Về Attention: Khám Phá Mạng Neural Feed-Forward Nông Như Một 
Giải Pháp Thay Thế Cho Các Lớp Attention Trong Transformers
Vukasin Bozic, Danilo Dordevic, Daniele Coppola, Joseph Thommes, Sidak Pal Singh
ETH Zurich
{vbozic, ddordevic, dcoppola, jthommes }@student.ethz.ch, sidak.singh@inf.ethz.ch
Tóm tắt
Công trình này trình bày một phân tích về hiệu quả của việc sử-
dụng các mạng feed-forward nông tiêu chuẩn để mô phỏng 
hành vi của cơ chế attention trong mô hình Transformer gốc, 
một kiến trúc tối ưu cho các tác vụ sequence-to-sequence. Chúng 
tôi thay thế các thành phần chính của cơ chế attention trong 
Transformer bằng các mạng feed-forward đơn giản, được huấn 
luyện sử dụng các thành phần gốc thông qua chưng cất kiến thức. 
Các thí nghiệm của chúng tôi, được thực hiện trên bộ dữ liệu 
IWSLT2017, tiết lộ khả năng của những "Transformers không 
attention" này để cạnh tranh với hiệu suất của kiến trúc gốc. 
Thông qua các nghiên cứu loại bỏ nghiêm ngặt, và thử nghiệm 
với các loại và kích thước mạng thay thế khác nhau, chúng tôi 
đưa ra những hiểu biết hỗ trợ tính khả thi của phương pháp của 
chúng tôi. Điều này không chỉ làm sáng tỏ khả năng thích ứng 
của các mạng feed-forward nông trong việc mô phỏng các cơ 
chế attention mà còn nhấn mạnh tiềm năng của chúng trong 
việc đơn giản hóa các kiến trúc phức tạp cho các tác vụ 
sequence-to-sequence.

Giới thiệu
Bài báo có tính đột phá (Vaswani et al. 2017) đã giới thiệu 
mô hình Transformer đã thay đổi cơ bản bối cảnh của các tác 
vụ mô hình hóa sequence-to-sequence. Nó đã thiết lập các tiêu 
chuẩn mới cho dịch thuật ngôn ngữ, được đo bằng điểm BLEU 
(Papineni et al. 2002). Cơ chế attention của Transformer cho 
phép thiết lập các phụ thuộc dài hạn trong dữ liệu tuần tự, 
cho phép nó chú ý đến mọi phần tử trong một chuỗi, một thành 
tựu mà các kiến trúc mạng trước đó đã gặp khó khăn để đạt 
được mà không có chi phí tính toán đáng kể.

Được truyền cảm hứng bởi các công trình trước đây (Ba and 
Caruana 2014), (Urban et al. 2017) khám phá tính khả thi 
của việc huấn luyện các mạng feed-forward nông để mô phỏng 
hành vi của các mạng tích chập sâu với các mạng sâu làm giáo 
viên, chúng tôi tiến hành một cuộc điều tra tương tự trên 
Transformer gốc được trình bày trong (Vaswani et al. 2017). 
Trọng tâm của chúng tôi là dịch thuật ngôn ngữ, sử dụng bộ 
dữ liệu IWSLT2017 (Cettolo et al. 2017). Chúng tôi nhằm 
đánh giá mức độ mà các mạng feed-forward nông tiêu chuẩn 
có thể mô hình hóa các cơ chế attention bằng cách thay thế 
các thành phần attention chính bằng các mạng feed-forward 
được huấn luyện để sao chép hành vi của chúng.

Công trình này cung cấp bằng chứng thực nghiệm hỗ trợ 
quan niệm rằng các mạng feed-forward nông có thể học hiệu 
quả các hành vi của các mô-đun attention Transformer và 
thay thế chúng mà không ảnh hưởng đáng kể đến hiệu suất 
tổng thể của nó. Mặc dù nó không giới thiệu một lợi thế 
cạnh tranh so với các phương pháp đã thiết lập, nó cung cấp 
một phân tích khái niệm về các kỹ thuật hiện có và các giải 
pháp thay thế tiềm năng.

Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

Hình 1: Các phương pháp thay thế encoder self-attention khác 
nhau được trình bày.

Mô hình và Phương pháp
Kiến trúc Transformer được cấu tạo từ các khối encoder và 
decoder chồng lên nhau, sử dụng attention để xử lý dữ liệu 
đầu vào. Lớp encoder có một khối self-attention, trong khi 
lớp decoder bao gồm cả khối self-attention và cross-attention, 
kết hợp dữ liệu được xử lý bởi encoder và chính nó. Mô hình 
này được sử dụng làm baseline, tức là mô hình giáo viên, 
nơi các kích hoạt trung gian của các khối của nó được sử 
dụng cho chưng cất kiến thức (Hinton, Vinyals, and Dean 
2015) trong việc huấn luyện các mạng feed-forward.

Thay thế encoder self-attention. Trong phương pháp được 
đề xuất, một nghiên cứu loại bỏ kỹ lưỡng về các phương 
pháp thay thế tiềm năng đã được tiến hành. Các thí nghiệm 
được thực hiện trên các lớp self-attention trong tất cả 6 
khối encoder.

Chúng tôi đã giới thiệu bốn cấp độ trừu tượng khác nhau 
để thay thế encoder attention gốc:
Attention Layer Replacement (ALR), Attention Layer with 
Residual Connection Replacement (ALRR), Attention Separate 
Heads Layer Replacement (ASLR) và Encoder Layer Replacement 
(ELR), như được mô tả trong Hình 1. Hơn nữa, arXiv:2311.10642v4  [cs.CL]  4 Feb 2024

--- TRANG 2 ---
tất cả các kiến trúc này đã được huấn luyện trong 5 kích 
thước khác nhau, từ "XS" đến "L".

Thay thế toàn bộ Transformer attention. Vì ALR được thấy 
là phương pháp hiệu quả nhất trong trường hợp thay thế 
encoder attention, có cả hiệu suất cao và số lượng tham số 
nhỏ, toàn bộ quy trình đã được tái tạo cho việc thay thế 
decoder self-attention và cross-attention. Điều này đòi hỏi 
các thích ứng của các kiến trúc đã giới thiệu trước đó, do 
các loại attention khác nhau trong decoder. Thêm chi tiết về 
động lực và lựa chọn các mạng thay thế được đưa ra trong 
Phụ lục A, trong khi chi tiết triển khai và huấn luyện cụ 
thể của tất cả các mạng thay thế FF được cung cấp trong 
Phụ lục B.

Kết quả
Metric BLEU được sử dụng cho mục đích đánh giá trong công 
trình này, vì nó đại diện cho một metric tiêu chuẩn cho các 
tác vụ dịch thuật ngôn ngữ. Kết quả cho cả nghiên cứu thay 
thế encoder self-attention và toàn bộ Transformer trải rộng 
trên 4 tập con của bộ dữ liệu IWSLT2017. Hơn nữa, điểm BLEU 
tương đối so với điểm baseline (vanilla Transformer) của 
mỗi thí nghiệm đã được tính toán và sau đó lấy trung bình 
trên các bộ dữ liệu. Kết quả thí nghiệm một phần được trình 
bày trong Hình 2 và 3, trong khi kết quả đầy đủ có sẵn trong 
Phụ lục C. Chúng tôi cung cấp mã triển khai trên Github1.

Thảo luận
Trong trường hợp thay thế encoder, tất cả các phương pháp 
được đề xuất đạt được kết quả cạnh tranh so với baseline, 
như thấy trong Hình 2. Trong số bốn phương pháp, ELR hoạt 
động tệ nhất, được gây ra bởi tính đơn giản của mô hình thay 
thế, loại bỏ tất cả các cấu trúc encoder hỗ trợ huấn luyện.

Hơn nữa, phương pháp thay thế toàn bộ Transformer, nơi chỉ 
sử dụng phương pháp ALR, đã mang lại kết quả thể hiện tiềm 
năng của các mạng feed-forward để sao chép thành công hành 
vi decoder self-attention, trong khi hiệu suất trên decoder 
cross-attention tương đối tệ hơn, như được trình bày trong 
Hình 3. Lý do tiềm năng cho hành vi này có thể là thiếu khả 
năng biểu đạt của mạng feed-forward cần thiết để mô tả việc 
ánh xạ và tương tác phức tạp hơn giữa các chuỗi được sử dụng 
trong khối cross-attention, điều này cũng ảnh hưởng đến điểm 
đánh giá cuối cùng cho Transformer hoàn toàn "không attention".

Tuy nhiên, tất cả các phương pháp thay thế đều có chi phí 
đáng kể là có nhiều tham số hơn. Một nhược điểm khác của việc 
thay thế attention bằng mạng feed-forward kích thước cố định 
là thiếu tính linh hoạt sắp xảy ra của mô hình về độ dài của 
các chuỗi mà mô hình có thể hoạt động.

1https://github.com/vulus98/Rethinking-attention.git

Hình 2: Điểm BLEU tương đối [%] (tương đối so với baseline 
Transformer), phụ thuộc vào kích thước mạng FF. Encoder 
self-attention được thay thế sử dụng các phương pháp thay 
thế khác nhau.

Hình 3: Điểm BLEU tương đối [%] (tương đối so với baseline), 
phụ thuộc vào kích thước mạng FF. Phương pháp ALR được sử 
dụng để thay thế các phần attention khác nhau của transformer.

Kết luận
Bằng chứng thực nghiệm cho thấy rằng các phương pháp được 
đề xuất có khả năng đạt được hiệu suất tương đương với 
Transformer gốc, chứng minh rằng Transformers không nhất 
thiết phải có attention. Những kết luận này cũng chỉ ra 
những thiếu sót của các phương pháp tối ưu hóa hiện tại, 
không thể huấn luyện những "Transformers không attention" 
này từ đầu mà cần các kỹ thuật tiên tiến hơn, chẳng hạn như 
chưng cất kiến thức để hội tụ thành các cấu hình tham số 
mong muốn. Kết luận này nhấn mạnh rằng với những tiến bộ 
trong các kỹ thuật tối ưu hóa, các kiến trúc ít chuyên biệt 
hơn như các mạng feed-forward có thể được sử dụng cho các 
tác vụ tiên tiến, hiện được dành riêng cho các kiến trúc 
chuyên biệt cao.

--- TRANG 3 ---
Lời cảm ơn
Chúng tôi muốn bày tỏ lòng biết ơn chân thành đến phòng thí 
nghiệm Data Analytics của ETH Zurich vì đã cung cấp các tài 
nguyên và hỗ trợ cần thiết trong suốt quá trình của dự án 
này; môi trường hợp tác và làm giàu của phòng thí nghiệm 
đã đóng góp đáng kể vào sự thành công của nghiên cứu này, 
và chúng tôi thực sự biết ơn vì sự hỗ trợ vô giá của họ. 
Ngoài ra, chúng tôi xin gửi lời cảm ơn chân thành đến 
G-research vì sự tài trợ hào phóng của họ, điều này đã làm 
cho chúng tôi có thể tham dự hội nghị và trình bày bài báo này.

Tài liệu tham khảo
Ba, L. J.; and Caruana, R. 2014. Do Deep Nets Really Need
to be Deep? ArXiv:1312.6184 [cs].
Cettolo, M.; Federico, M.; Bentivogli, L.; Niehues, J.;
St¨uker, S.; Sudoh, K.; Yoshino, K.; and Federmann, C. 2017.
Overview of the IWSLT 2017 Evaluation Campaign.
Hinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the
Knowledge in a Neural Network. ArXiv:1503.02531 [cs,
stat].
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
BLEU: A Method for Automatic Evaluation of Machine
Translation.
Snoek, J.; Larochelle, H.; and Adams, R. P. 2012. Practical
Bayesian Optimization of Machine Learning Algorithms.
arXiv:1206.2944.
Urban, G.; Geras, K. J.; Kahou, S. E.; Aslan, O.; Wang, S.;
Caruana, R.; Mohamed, A.; Philipose, M.; and Richardson,
M. 2017. Do Deep Convolutional Nets Really Need to be
Deep and Convolutional? ArXiv:1603.05691 [cs, stat].
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-
tention Is All You Need. arXiv:1706.03762.

--- TRANG 4 ---
Phụ lục
Phụ lục A: Lựa chọn các mạng thay thế
Ở đây chúng tôi cung cấp mô tả chi tiết hơn về việc tích hợp các mạng feed-forward (FF) vào kiến trúc Transformer, và việc thay thế tiếp theo các mô-đun attention. Tất cả các phương pháp thay thế đều dựa trên việc thay thế các phần khác nhau của mô-đun attention:
•Attention Layer Replacement (ALR) : thay thế chỉ khối multi-head attention (MHA) bằng mạng FF, giữ nguyên kết nối dư và layer normalization.
•Attention Layer with Residual Connection Replacement (ALRR) : Mô-đun MHA, cùng với kết nối dư được thay thế bằng mạng FF. Phương pháp này hiệu quả loại bỏ kết nối dư khi Mạng FF được thay thế trong Transformer.
•Attention Separate heads Layer Replacement (ASLR) : Như một biến thể của ALR, phương pháp này thay thế mỗi head đơn lẻ của mô-đun MHA bằng một mạng FF riêng biệt.
•Encoder Layer Replacement(ELR) : Thay thế toàn bộ lớp Encoder.
Sau đây là mô tả, động lực và đặc điểm của mỗi phương pháp nêu trên. Các phương pháp ALR và ALRR được thiết kế theo cách tách biệt các tác động và lợi ích của lớp attention khỏi lợi ích mang lại bởi kết nối dư. Hơn nữa, mô phỏng hành vi của mỗi head riêng biệt, được đặc trưng trong phương pháp ASLR, làm cho việc thay thế FF giống với phương pháp MHA gốc hơn. Cuối cùng, như mức độ trừu tượng cao nhất, toàn bộ khối encoder được thay thế bằng Mạng FF trong phương pháp ELR. Điều này về cơ bản đảo lộn kiến trúc encoder gốc, biến nó thành một chuỗi các mạng FF - một cho mỗi khối của encoder. Bằng cách kiểm tra tất cả các mức độ thay thế thông qua thí nghiệm, chúng tôi đã có được một nghiên cứu loại bỏ có căn cứ về các thay thế cơ chế attention có thể.
Như một mô-đun thay thế trong tất cả các trường hợp này, mạng FF một lớp ẩn nông đơn giản đã được sử dụng. Đối với tất cả các phương pháp nêu trên, các mạng FF có kích thước khác nhau đã được thiết kế, như được đưa ra trong Bảng 1. Số lượng tham số gốc cho lớp attention (60,000 tham số trong trường hợp của chúng tôi) chủ yếu bị vượt quá bởi các mạng thay thế, chủ yếu do đầu vào và đầu ra kích thước cố định, và định dạng xử lý được yêu cầu bởi mạng FF.

XS S M L
ALR
320K 640K 10M 41M ALRR
ELR
ASLR 290K 1.5M 11.5M 46M
Bảng 1: Số lượng tham số của các kiến trúc được đề xuất. Số được đặc trưng là số tham số cho một mạng FF đơn lẻ trong một lớp. Việc mở rộng thêm các mạng này không mang lại cải thiện trong điểm BLEU. Số lượng tham số cho phương pháp ASLR được trình bày trên cơ sở mỗi attention-head.

Phụ lục B: Chi tiết triển khai
Điểm khởi đầu của quy trình của chúng tôi là việc huấn luyện mô hình vanilla Transformer, bao gồm sáu encoder và sáu decoder. Để giảm thời gian huấn luyện và làm cho việc kiểm tra nhanh hơn, chúng tôi đã giảm kích thước embedding từ 512 xuống 128. Những thay đổi này không làm giảm điểm tổng thể quá nhiều so với điểm BLEU gốc nhưng dẫn đến nhu cầu năng lực tính toán thấp hơn đáng kể. Mô hình Transformer này sau đó được sử dụng làm mô hình giáo viên để huấn luyện các mạng feedforward.
Trong Hình 4, bản chất của phương pháp huấn luyện và đánh giá của chúng tôi được trình bày, trên ví dụ về thay thế ALRR, trong khi các phương pháp khác tương tự. Như bước khởi đầu, các kích hoạt trung gian (cặp đầu vào-đầu ra) được trích xuất từ Transformer đã huấn luyện và được sử dụng làm dữ liệu huấn luyện cho mạng thay thế Feed-Forward mong muốn. Sau đó, chúng phải được thích ứng thêm, như được mô tả dưới đây.
Các biến đổi dữ liệu cần thiết Mỗi lớp attention biến đổi biểu diễn từ đầu vào của một câu thành một tổ hợp tuyến tính của các giá trị được trích xuất bởi biểu diễn đầu vào. Để mô phỏng hành vi này, mạng FF nhận vào các biểu diễn từ nối tiếp của một câu làm đầu vào và tạo ra các biểu diễn từ cập nhật làm đầu ra trong một lần chuyển. Để xử lý các câu đầu vào có độ dài khác nhau, chúng tôi đã quyết định pad tất cả các câu đến độ dài cố định tối đa và che các giá trị được pad bằng số không để ngăn chúng ảnh hưởng đến suy luận của mô hình. Quá trình này được minh họa trong Hình 5.

--- TRANG 5 ---
Hình 4: Minh họa các chu kỳ huấn luyện và đánh giá của phương pháp ALRR trong encoder self-attention. Thay thế trong lớp self-attention và cross-attention tương tự. Các phương pháp thay thế khác tuân theo nguyên tắc tương tự, với sự khác biệt là dữ liệu đầu vào và nhãn giáo viên được lấy từ các khối khác nhau của encoder, tùy thuộc vào cấu trúc của chúng.
Chúng tôi cũng đã áp dụng một giới hạn trên cố định cho độ dài câu, mà chúng tôi đặt là 50. Giới hạn này do đó hạn chế kích thước của các mạng FF thay thế của chúng tôi. Vì 96% các mẫu trong bộ dữ liệu có độ dài 50 hoặc ít hơn, các bộ dữ liệu không bị thu hẹp đáng kể.
Sau khi huấn luyện thành công các mạng, chúng được chèn vào kiến trúc Transformer, thay thế các lớp hiện tại thừa, và việc đánh giá được chạy, như thể hiện trong Hình 4.
Các mạng thay thế decoder Sự khác biệt chính giữa phương pháp thay thế được sử dụng cho self-attention trong encoder và phương pháp được sử dụng cho self-attention trong decoder là các mạng thay thế FF trong decoder xuất ra các word embedding từng cái một, theo masked self-attention gốc. Mạng xử lý mỗi từ riêng lẻ bằng cách đưa toàn bộ biểu diễn câu qua mạng và che biểu diễn của các từ đến sau từ được xử lý. Điều này được thực hiện để tính đến khái niệm nhân quả, nơi chỉ các từ trước đó trong câu có thể ảnh hưởng đến ý nghĩa của từ hiện tại.
Cross-attention trong decoder chấp nhận cả biểu diễn từ từ các lớp encoder và decoder, tích hợp chúng lại với nhau. Để sao chép quá trình này, biểu diễn từ từ cả encoder và decoder đã được nối tiếp lại với nhau và được pad, có kích thước đầu vào gấp đôi so với các mạng thay thế self-attention. Kích thước đầu ra vẫn giữ nguyên, một lần nữa theo thiết kế cross-attention ban đầu.
Chi tiết huấn luyện mạng FF Mỗi phương pháp và mỗi kích thước mạng FF đòi hỏi huấn luyện 6 mạng độc lập cho mỗi khối self-attention hoặc cross-attention. Các mạng được huấn luyện trong 20 epoch sử dụng Adam làm optimizer lựa chọn. Tốc độ học được đặt là 0.001, trong khi kích thước batch là 1400. Cài đặt huấn luyện được giữ giống nhau cho tất cả các mạng.

Phụ lục C: Bộ dữ liệu và Kết quả
Đối với tất cả các quy trình kiểm tra, bộ dữ liệu IWSLT2017 đã được sử dụng. Nó cung cấp nhiều tập con cho dịch thuật ngôn ngữ, trong số đó chúng tôi đã sử dụng các tập con French-English (F2E), English-French (E2F), German-English (G2E), và English-German (E2G). Trung bình, các tập con này bao gồm 200000 câu huấn luyện và 1000 câu kiểm tra, thu được sau khi thu nhỏ kích thước tối đa của câu xuống 50 từ, như được giải thích ở trên.
Metric được sử dụng cho tất cả các kiểm tra là điểm BLEU, vì nó cung cấp một so sánh trực quan với bản dịch do con người cung cấp, đặt chất lượng dịch thuật trên thang điểm chuẩn hóa 0-1, và đại diện cho metric kiểm tra chính cho nhiều công trình tương tự trong lĩnh vực này.
Trong phần tóm tắt, chúng tôi đã cung cấp kết quả trung bình trên tất cả 4 bộ dữ liệu, chứng minh tính linh hoạt và mạnh mẽ của các phương pháp được đề xuất. Kết quả thô được hiển thị trong Bảng 3, trong khi các điểm được trình bày trong phần tóm tắt là tương đối so với điểm baseline của Transformer gốc, trên mỗi bộ dữ liệu tương ứng. Các điểm baseline được trình bày trong Bảng 2.
Chúng tôi đã thay thế cơ chế attention theo các cách khác nhau trong encoder, decoder, và cả hai. Kết quả thu được trong quá trình thử nghiệm rộng rãi cho thấy rằng chướng ngại vật duy nhất để thay thế hoàn toàn các cơ chế attention trong Transformer là cross-attention. Như rõ ràng từ bảng 3, điểm BLEU thấp hơn đáng kể nếu cơ chế cross-attention được thay thế

--- TRANG 6 ---
Hình 5: Minh họa về tiền xử lý và hậu xử lý dữ liệu cần thiết trước và sau khi truyền qua mạng Feed-forward.
bởi các mạng FF. Chúng tôi đã kiểm tra việc thay thế theo ba cách: chỉ trong decoder cross-attention, chỉ trong encoder và decoder (không thay thế cross-attention), và thay thế hoàn toàn. Trong tất cả các trường hợp mà decoder cross-attention được thay thế cho các mạng FF, mô hình cuối cùng hoạt động tệ hơn đáng kể, bất kể các phần khác có được thay thế hay không.
Điều này cho thấy rằng các mạng nông được đề xuất không thể nắm bắt các tương tác phức tạp và phức tạp hơn giữa các chuỗi khác nhau đi vào cơ chế cross-attention. Mặt khác, self-attention đã được mô hình hóa và học thành công.

Phụ lục D: Công trình tương lai
Bằng cách khớp hiệu suất của Transformer gốc, rất có thể là việc tối ưu hóa thêm các siêu tham số của mạng FF sử dụng tìm kiếm tham số tiên tiến (ví dụ: sử dụng tối ưu hóa Bayesian (Snoek, Larochelle, and Adams 2012)) có thể mang lại kết quả tốt hơn về chất lượng dịch thuật và có thể thậm chí cho phép sử dụng các mạng FF nhỏ hơn cho việc thay thế, vì kích thước của các mạng đại diện cho một trong những nút thắt cổ chai chính cho việc triển khai những Transformers 'không attention' này trong thực tế.
Hơn nữa, một hướng tiềm năng khác nằm trong việc huấn luyện các mạng FF phức tạp hơn cho mục đích mô hình hóa mô-đun cross-attention của decoder, vì mạng nông hiện tại cho thấy rằng, trái ngược với self-attention mà chúng có thể học thành công, cross-attention chứng minh là thách thức hơn do tính phức tạp của nó.

--- TRANG 7 ---
E2G G2E E2F F2E
Transformer 0.257 0.324 0.276 0.292
Bảng 2: Điểm BLEU transformer baseline trên tất cả 4 bộ dữ liệu dịch thuật ngôn ngữ.

E2G G2E E2F F2E
ALREnc
SAXS 0.180 0.235 0.226 0.218
S 0.196 0.257 0.244 0.240
M 0.245 0.320 0.275 0.284
L 0.252 0.327 0.276 0.288
Dec
SAXS 0.227 0.305 0.251 0.273
S 0.240 0.313 0.255 0.281
M 0.252 0.322 0.267 0.290
L 0.253 0.323 0.273 0.291
Dec
CAXS 0.035 0.036 0.035 0.039
S 0.054 0.058 0.042 0.053
M 0.104 0.125 0.089 0.108
L 0.115 0.130 0.109 0.115
E-D
SAXS 0.163 0.222 0.219 0.204
S 0.187 0.247 0.235 0.227
M 0.244 0.313 0.265 0.277
L 0.246 0.321 0.270 0.284
FullXS 0.026 0.027 0.026 0.032
S 0.041 0.053 0.048 0.044
M 0.102 0.122 0.083 0.107
L 0.105 0.134 0.117 0.116
ALRREnc
SAXS 0.013 0.010 0.003 0.001
S 0.018 0.013 0.008 0.012
M 0.158 0.181 0.153 0.150
L 0.243 0.315 0.263 0.276
ASLREnc
SAXS 0.245 0.319 0.257 0.272
S 0.250 0.323 0.260 0.285
M 0.251 0.326 0.269 0.289
L 0.252 0.326 0.271 0.290
ELREnc
SAXS 0.012 0.010 0.011 0.011
S 0.016 0.015 0.132 0.012
M 0.116 0.120 0.124 0.110
L 0.194 0.248 0.225 0.219
Bảng 3: Điểm BLEU của tất cả các phương pháp được đề xuất, trong tất cả các kích thước và trên tất cả các bộ dữ liệu được sử dụng. Các từ viết tắt được sử dụng cho mục đích rõ ràng, và được giải thích trong văn bản sau: "Enc" viết tắt của encoder, "Dec" viết tắt của decoder, "SA" viết tắt của self-attention, "CA" viết tắt của cross-attention, và E-D viết tắt của encoder and decoder.
