# S3: Tăng cường Sử dụng GPU trong Quá trình Suy luận Sinh tạo để Đạt Thông lượng Cao hơn

Yunho Jin
Đại học HarvardChun-Feng Wu
Đại học Quốc gia Yang Ming
Chiao TungDavid Brooks
Đại học HarvardGu-Yeon Wei
Đại học Harvard

## Tóm tắt
Tạo văn bản với mô hình ngôn ngữ lớn (LLM) tiêu thụ lượng bộ nhớ khổng lồ. Ngoài các tham số mô hình đã rất lớn, bộ nhớ đệm key/value (KV) lưu trữ thông tin về các token trước đó trong một chuỗi có thể phát triển thậm chí lớn hơn cả mô hình. Vấn đề này trở nên nghiêm trọng hơn trong một trong những framework phục vụ LLM hiện tại khi dành trước độ dài chuỗi tối đa của bộ nhớ cho bộ nhớ đệm KV để đảm bảo tạo ra một chuỗi hoàn chỉnh vì chúng không biết độ dài chuỗi đầu ra. Điều này hạn chế chúng ta sử dụng kích thước batch nhỏ hơn dẫn đến việc sử dụng GPU thấp hơn và trên hết là thông lượng thấp hơn. Chúng tôi lập luận rằng thiết kế một hệ thống với kiến thức tiên nghiệm về chuỗi đầu ra có thể giảm thiểu vấn đề này. Để đạt được điều này, chúng tôi đề xuất S3, dự đoán độ dài chuỗi đầu ra, lập lịch các truy vấn tạo dựa trên dự đoán để tăng việc sử dụng tài nguyên thiết bị và thông lượng, và xử lý những dự đoán sai. Phương pháp được đề xuất của chúng tôi đạt được thông lượng cao gấp 6,49× so với những hệ thống giả định trường hợp xấu nhất cho độ dài chuỗi đầu ra.

## 1 Giới thiệu
Tạo văn bản đã trở nên ngày càng phổ biến trong các dịch vụ khác nhau, dẫn đến sự gia tăng trong việc sử dụng các mô hình ngôn ngữ lớn (LLM) nổi tiếng với độ chính xác cao. LLM có những đặc điểm riêng biệt khiến chúng khác biệt với các mô hình không dựa trên Transformer, bao gồm yêu cầu về lượng lớn tài nguyên tính toán và bộ nhớ. Cụ thể, chúng tôi quan sát thấy rằng các LLM dựa trên Transformer thường bị giới hạn bởi dung lượng và băng thông bộ nhớ, dẫn đến việc sử dụng kém đáng kể các tài nguyên tính toán. Khi phục vụ GPT-J trên GPU NVIDIA A100, việc sử dụng tài nguyên tính toán GPU có thể thấp tới 0,4%. Điều này làm nổi bật bản chất bị giới hạn bởi bộ nhớ của các LLM và nhu cầu sử dụng bộ nhớ hiệu quả để tăng việc sử dụng tài nguyên tính toán GPU.

Một cách tiếp cận phổ biến để tăng việc sử dụng GPU và nâng cao thông lượng là tăng kích thước batch. Điều này là do các đầu vào trong một batch chia sẻ cùng các trọng số mô hình, do đó GPU chỉ cần tải trọng số mô hình từ bộ nhớ băng thông cao (HBM) lên SRAM trên chip một lần và tái sử dụng cho tất cả các đầu vào trong batch. GPU sử dụng nhiều hơn các tài nguyên tính toán khi xử lý cùng các trọng số mô hình. Tăng kích thước batch là một kỹ thuật tối ưu hóa đơn giản và do đó, thường được sử dụng trong việc phục vụ các mô hình mạng neural tích chập và kết nối đầy đủ [1, 2].

Tuy nhiên, lớp self-attention trong các LLM tạo văn bản dựa trên Transformer đặt ra thách thức cho việc tối ưu hóa đơn giản này do bản chất tự hồi quy của nó. Cụ thể, khi tạo một token mới trong một chuỗi, mô hình cần chú ý đến tất cả các token trước đó trong chuỗi, yêu cầu mô hình giữ lại tất cả thông tin từ các token trước đó và lưu trữ chúng trong HBM. Chúng tôi gọi vùng trong HBM này chứa thông tin là bộ nhớ đệm key/value (KV cache). Kích thước của bộ nhớ đệm KV tăng lên với kích thước batch lớn hơn và chuỗi dài hơn, điều này giới hạn kích thước batch tối đa, từ đó làm giảm việc sử dụng GPU và cuối cùng giảm thông lượng. Để hỗ trợ kích thước bộ nhớ đệm KV ngày càng tăng, thư viện Transformers của Huggingface [3] liên tục cấp phát bộ nhớ mới tại mỗi lần tạo token và phát sinh độ trễ overhead liên quan đến cấp phát bộ nhớ. Điều này cải thiện khả năng sử dụng vì người dùng không phải biết độ dài chuỗi đầu ra nhưng gặp phải độ trễ suy luận dài. Thay vào đó, thư viện FasterTransformer của NVIDIA [4] cấp phát trước bộ nhớ cho độ dài chuỗi tối đa, điều này kết thúc bằng việc lãng phí bộ nhớ bằng cách cấp phát nhiều hơn mức cần thiết. Cách tiếp cận này giới hạn kích thước batch và ưu tiên độ trễ hơn thông lượng. Xu hướng hướng tới độ dài chuỗi tối đa dài trong các mô hình gần đây (ví dụ: 8K hoặc 32K) [5] khuếch đại overhead của bộ nhớ đệm KV, đòi hỏi việc sử dụng bộ nhớ hiệu quả hơn trong việc phục vụ các mô hình tạo văn bản dựa trên Transformer.

Nhận thức được những thách thức này, chúng tôi đề xuất S3, lập lịch chuỗi với suy đoán, một framework tối đa hóa thông lượng thông qua việc dự đoán độ dài chuỗi đầu ra và giảm lãng phí bộ nhớ. Việc cấp phát bộ nhớ thường xuyên trong Transformers của Huggingface và kích thước batch hạn chế trong FasterTransformer xuất phát từ thực tế là chúng ta thiếu kiến thức trước về độ dài chuỗi đầu ra. S3 giải quyết những vấn đề này bằng cách dự đoán độ dài chuỗi đầu ra dự kiến và cấp phát lượng bộ nhớ tương ứng cho mỗi truy vấn. Nó cũng lập lịch các chuỗi để tăng việc sử dụng GPU. Cuối cùng, S3 chạy một supervisor trong nền để phát hiện những dự đoán sai và điều chỉnh kích thước bộ nhớ được cấp phát để chính xác hơn. Bằng cách tích hợp những thành phần này với nhau, S3 tối ưu hóa việc sử dụng bộ nhớ và lập lịch để tối đa hóa thông lượng trong quá trình triển khai các LLM dựa trên Transformer cho tạo văn bản trên GPU.

Có hai loại kịch bản triển khai LLM: trực tuyến và ngoại tuyến. Các kịch bản trực tuyến như chatbot [7, 8] yêu cầu các nhà cung cấp dịch vụ tạo ra một chuỗi trong ràng buộc mục tiêu cấp độ dịch vụ (SLO) độ trễ chặt chẽ. Các kịch bản ngoại tuyến bao gồm các ứng dụng như chấm điểm [9] hoặc xử lý dữ liệu [10], và có SLO độ trễ lỏng lẻo, nhấn mạnh thông lượng hơn độ trễ đầu cuối. Ngược lại, FaterTransformers [4] và xFormers [11] ưu tiên giảm độ trễ. Chúng tôi lập luận rằng việc đảm bảo độ trễ vẫn dưới ràng buộc SLO khiến việc ưu tiên giảm độ trễ thêm trở nên không cần thiết. Để đạt được điều này, chúng tôi thiết kế S3 để đạt được thông lượng cao hơn dưới những SLO độ trễ đó. Hình 1 làm nổi bật S3 có thể cải thiện thông lượng bao nhiêu khi đánh đổi độ trễ. Đối với các kịch bản trực tuyến, chúng tôi giả định một SLO độ trễ được đặt theo tốc độ đọc trung bình của người đọc tiếng Anh, 4 từ mỗi giây [12] và 0,1875 giây mỗi token [13]. Các mô hình ở hình bên trái nhỏ hơn 100 tỷ tham số thỏa mãn SLO này cho tất cả các kích thước batch có thể. SLO này cung cấp cho các nhà cung cấp dịch vụ để cải thiện thông lượng cho tất cả các mô hình. Thực tế, đối với LLAMA-33B [14], có cơ hội để có được lợi ích thông lượng mà không có penalty độ trễ. Hình bên phải, quét qua số lượng GPU khác nhau, cho thấy cơ hội để tối đa hóa thông lượng trong các kịch bản ngoại tuyến có SLO độ trễ lỏng lẻo.

Chúng tôi đánh giá S3, đánh giá cả thông lượng và hiệu quả chi phí của nó. Phân tích của chúng tôi bao gồm cả kịch bản ngoại tuyến và trực tuyến. Trong các kịch bản trực tuyến dưới ràng buộc SLO độ trễ tốc độ đọc trung bình, chúng tôi thấy rằng S3 có thể tạo ra tới 6,49× nhiều chuỗi hơn trong khi vẫn tuân thủ cùng ràng buộc SLO. Trong các kịch bản ngoại tuyến, chúng tôi quan sát thấy rằng S3 đạt được tăng tốc lên tới 6,49× cho các mô hình khác nhau. S3 không ảnh hưởng đến perplexity của các mô hình vì nó không thay đổi kiến trúc của các mô hình. Hơn nữa, chúng tôi đánh giá hiệu quả chi phí của S3 và thấy rằng sử dụng 6 GPU, S3 cung cấp thông lượng gần như giống hệt so với hệ thống vanilla với 10 GPU.

Tóm lại, chúng tôi đóng góp những điều sau:
• Chúng tôi tăng kích thước batch có thể đạt được dưới SLO độ trễ dài hơn và cho phép các nhà cung cấp dịch vụ phục vụ với thông lượng cao hơn trong cả kịch bản trực tuyến và ngoại tuyến bằng cách sử dụng kích thước batch lớn hơn.
• Chúng tôi fine-tune một mô hình Distillbert để dự đoán độ dài chuỗi đầu ra cho một prompt đầu vào.
• Chúng tôi cung cấp một cơ chế để phục hồi từ những dự đoán sai. S3 preempt các chuỗi vượt quá bộ nhớ được cấp phát của chúng và retrain predictor để học từ những sai lầm của nó.

## 2 Bối cảnh và Động lực

### 2.1 Các Mô hình AI Sinh tạo
Một mô hình sinh tạo dựa trên Transformer là tự hồi quy. Nó dự đoán token có khả năng nhất dựa trên các token trong quá khứ. Vì mô hình tạo ra một token tại một thời điểm, nó phải lặp lại chính nó n lần để tạo ra một chuỗi dài n-token. Một lần lặp bao gồm một token đầu vào đi qua mô hình là một stack các lớp transformer chứa một attention, hai layer norm, và hai lớp feed-forward. Đặc biệt, lớp self-attention sử dụng thông tin về các token trong quá khứ để tạo ra token tiếp theo.

Ví dụ, mô hình tại lần lặp thứ i chú ý token hiện tại (ti) với mọi token mà nó đã tạo ra (t0, ...ti−1) trong lớp self-attention. Chúng ta có thể biểu diễn lớp self-attention như:
hout=softmax (qi·KT/√dh)·V
trong đó dh là chiều ẩn của mô hình, hout, qi∈Rdh là vector ẩn đầu ra, vector truy vấn hiện tại, tương ứng, và K, V∈Ri×dh là các ma trận key và value. Hàng thứ j trong các ma trận K và V biểu diễn các vector key và value của tj, tương ứng. Hai tích vô hướng chú ý ti đến tất cả các vector key và value trong chuỗi hiện tại.

Mô hình lưu trữ các ma trận K và V như bộ nhớ đệm key/value (KV cache) để tránh phải tạo ra các vector key và value tại mỗi lần lặp. Nếu không, nó phải lưu trữ các trạng thái ẩn của mọi token trước đó và nhân nó với ma trận trọng số WQKV∈Rdh×2dh tại mọi lớp transformer. Điều này sẽ yêu cầu 2(i−1)dh FLOP mỗi lớp gần như giống hệt 2idh FLOP cho lớp self-attention tại ti. Kích thước của bộ nhớ đệm KV là 4ldh byte mỗi token khi sử dụng số half-precision. Cache sử dụng 2 byte cho mọi số cho cả bộ nhớ đệm key và value trong đó l là số lượng lớp transformer trong một mô hình. Ví dụ, GPT-NEOX [15], một mô hình 20 tỷ tham số, có 44 lớp và 6144 chiều ẩn và do đó sử dụng 1MB mỗi KV cache mỗi token.

### 2.2 Quản lý KV Cache trên GPU
Bộ nhớ đệm KV tương đối nhỏ (ví dụ: vài MB) và có thể dễ dàng được lưu trữ trong HBM GPU (bộ nhớ băng thông cao) khi chuỗi ngắn vì cache lưu trữ thông tin về các token trước đó trong chuỗi. Nó tăng lên khi mô hình tạo ra nhiều token hơn. Sử dụng bản chất động này, Transformers của Huggingface [3] (HF-Transformers) liên tục cấp phát thêm bộ nhớ cho bộ nhớ đệm KV và tạm dừng tính toán cho đến khi hoàn thành. Cách tiếp cận này cho phép thư viện cấp phát chính xác lượng bộ nhớ cho mỗi cache với chi phí truy cập bộ nhớ thường xuyên.

Để giảm thiểu điều này, thư viện FasterTransformer của NVIDIA dành trước bộ nhớ cho độ dài chuỗi tối đa cho mọi chuỗi [4, 16]. Nó loại bỏ các truy cập bộ nhớ dư thừa bằng cách đơn giản điền vào bộ nhớ được dành trước theo cách chỉ-thêm. Tuy nhiên, cách tiếp cận này đi kèm với nhược điểm riêng vì nó dành trước nhiều bộ nhớ hơn mức cần thiết cho các chuỗi. Đối với GPT-NEOX với độ dài chuỗi tối đa 2048 token, FasterTransformer dành trước 2048 token ngay cả cho các chuỗi cuối cùng chỉ dài 50 token. Kích thước batch tối đa mà FasterTransformer có thể sử dụng cho mô hình này với GPU A100 80GB là ít hơn 20 với kích thước mô hình 40GB và 2,2GB mỗi chuỗi. Kích thước batch nhỏ sử dụng kém các tài nguyên tính toán khổng lồ trong GPU. Lý do để dành trước lượng bộ nhớ độ dài chuỗi tối đa ngay cả với vấn đề sử dụng kém là để đảm bảo rằng nó tạo ra các chuỗi đầy đủ và nâng cao trải nghiệm người dùng.

### 2.3 Quan sát
Các mô hình ngôn ngữ bị giới hạn bởi bộ nhớ Chúng tôi chứng minh mức độ sử dụng kém tài nguyên GPU khi chúng tôi chạy GPT-J với 6B tham số trên GPU A100. Mô hình tương đối nhỏ hơn cho thấy phổ rộng hơn của các kích thước batch và độ dài chuỗi khác nhau vì chúng ta có thể vừa các batch lớn hơn với chuỗi dài hơn trong GPU. Hình 2 (a) cho thấy việc sử dụng GPU được quét qua các kích thước batch và độ dài chuỗi khác nhau. Như hình chỉ ra, tăng kích thước batch đạt được việc sử dụng cao hơn nhưng cuối cùng gặp phải vách đá bộ nhớ, nơi lỗi hết bộ nhớ (OOM) giết chết quá trình. Lấy các batch với độ dài chuỗi 1024 làm ví dụ. 8 chuỗi là kích thước batch tối đa và do đó 12,19% là việc sử dụng tối đa mà chúng ta có thể đạt được với độ dài chuỗi này. Hình 2 (b) cho thấy sự sử dụng kém tương tự trong mô hình GPT-NEOX lớn hơn do vấn đề vách đá bộ nhớ. Mô hình này gặp phải vách đá bộ nhớ với kích thước batch nhỏ hơn và chuỗi ngắn hơn vì mô hình tiêu thụ nhiều bộ nhớ GPU hơn. Xin lưu ý rằng HF-Transformers vẫn cần biết độ dài chuỗi đầu ra trước khi batching các đầu vào để tránh vách đá bộ nhớ.

Như các hình minh họa, tăng kích thước batch có thể nâng cao thông lượng trong mạng neural. Cách tiếp cận này không yêu cầu tối ưu hóa phức tạp và cho phép GPU tải trọng số mô hình từ HBM của nó lên SRAM trên chip chỉ một lần và chia sẻ nó giữa số lượng lớn hơn các đầu vào. Bằng cách làm như vậy, GPU có thể kích hoạt các tài nguyên tính toán nhàn rỗi của nó và đồng thời xử lý nhiều đầu vào. Tuy nhiên, vách đá bộ nhớ đặt ra thách thức, hạn chế việc sử dụng tài nguyên bổ sung. Tuy nhiên, như chúng tôi elaborat, vấn đề này có thể được giải quyết.

Lý do đằng sau sự kém hiệu quả Cả việc cấp phát bộ nhớ thường xuyên trong HF-Transformers và kích thước batch hạn chế trong FasterTransformer đều xuất phát từ thực tế là chúng ta không biết về độ dài chuỗi được tạo ra. Nếu chúng ta biết độ dài chính xác của chuỗi được tạo ra, chúng ta có thể cấp phát bộ nhớ chính xác cho mỗi chuỗi và giải quyết các vấn đề dành trước bộ nhớ lặp lại và cấp phát bộ nhớ không cần thiết trong HF-Transformers và FasterTransformer, tương ứng.

## 3 Thiết kế S3
S3 là một framework co-design hệ thống-thuật toán tối đa hóa việc sử dụng GPU với dự đoán độ dài chuỗi để đạt được thông lượng cao hơn. S3 có ba thành phần như được hiển thị trong Hình 3: 1) predictor, 2) scheduler, và 3) supervisor. Một truy vấn tạo văn bản đến trong một request pool trong DRAM host. Predictor sau đó dự đoán độ dài chuỗi đầu ra của nó mà scheduler sử dụng để batch các request. Scheduler gửi batch đến GPU và mô hình text generator tạo ra văn bản trong batch. Supervisor giám sát việc sử dụng GPU và xử lý những dự đoán sai. Chúng tôi mô tả chi tiết từng thành phần và cách chúng tương tác với nhau trong phần này.

Predictor độ dài chuỗi đầu ra Chúng tôi sử dụng một predictor để dự đoán độ dài chuỗi đầu ra và giải quyết các vấn đề cấp phát bộ nhớ thường xuyên và dư thừa. Cụ thể, chúng tôi fine-tune một mô hình Distilbert [17] đã được huấn luyện cho phân loại chuỗi để phân loại bucket độ dài mà độ dài chuỗi đầu ra rơi vào. Chúng tôi bucketize độ dài chuỗi vì đã biết rằng các mô hình machine learning không có khả năng tìm kiếm "dặm cuối" so với việc thu hẹp các ứng viên xuống dặm cuối. Mỗi bucket được cấp phát phạm vi max sequence length/number of buckets và chúng tôi sử dụng 10 bucket.

Để đạt được điều này, chúng tôi fine-tune mô hình trên dataset Alpaca [18], một trong những dataset hỏi-đáp đại diện, và sử dụng các câu hỏi làm đầu vào và độ dài của các câu trả lời làm nhãn. Chúng tôi quan sát thấy rằng predictor này dự đoán bucket đúng với độ chính xác 98,61%. Khoảng cách trung bình giữa các dự đoán sai và bucket đúng là 1,03 có nghĩa là lỗi hội tụ về 0 khi chúng ta nhân đôi kích thước bucket. Chúng tôi cũng đánh giá predictor trên một mô hình được fine-tune với dataset Google Natural-Question [19] và quan sát độ chính xác 77,13%. Nó tạo ra những sai lầm nhỏ hơn thường xuyên hơn những sai lầm lớn hơn. Để hoàn thiện, chúng tôi fine-tune một mô hình trên dataset Pile [20], một dataset không phải hỏi-đáp, và thấy độ chính xác 65,6%. Predictor cho thấy độ chính xác đáng ngạc nhiên cao so với việc đoán ngẫu nhiên các bin vì cái sau chỉ đúng 10% thời gian.

Chúng tôi chọn Distilbert, một mô hình 66 triệu tham số cho kích thước nhỏ và suy luận nhanh của nó. Kích thước mô hình là không đáng kể vì nó nhỏ hơn thậm chí một lớp transformer duy nhất trong các mô hình tỷ-scale (ví dụ: 214 triệu cho mô hình GPT-J 6 tỷ [21]). Độ trễ cũng không đáng kể vì mô hình predictor chỉ chạy một lần khi một request đến máy chủ trong khi mô hình tạo văn bản chạy n lần để tạo ra một chuỗi đầu ra dài n-token. Chúng tôi đo được rằng Distilbert mất 3,7ms để chạy so với vài giây cho các mô hình tạo văn bản trên GPU NVIDIA A100.

Scheduler chuỗi nhận biết độ dài Scheduler batch và lập lịch các chuỗi dựa trên các kết quả dự đoán để tối đa hóa việc sử dụng GPU mà không vượt quá dung lượng HBM GPU. Chúng ta có thể công thức hóa vấn đề này như một biến thể của bài toán bin packing với một bin duy nhất. Ràng buộc dung lượng của bin là dung lượng HBM và trọng số item là kích thước của bộ nhớ đệm KV cho mỗi chuỗi.

Chúng tôi sử dụng thuật toán decreasing first fit [22] như giải pháp cho bài toán bin packing vì tính đơn giản của nó. Scheduler đầu tiên sắp xếp các chuỗi trong request pool chứa các chuỗi được lập lịch theo thứ tự giảm dần. Nó lặp qua pool và kiểm tra xem bộ nhớ đệm KV của chuỗi hiện tại không vượt quá HBM có sẵn. Nếu vậy, nó bao gồm chuỗi trong batch hiện tại và giảm HBM có sẵn bằng kích thước của bộ nhớ đệm KV. Scheduler tiếp tục quá trình này cho đến khi không còn HBM có sẵn hoặc nó đã lặp qua toàn bộ request pool.

Batch kết quả có hình dạng không đều nơi một số chuỗi dài hơn những chuỗi khác. Thật không may, các framework hiện tại hoặc không hỗ trợ batch có hình dạng không đều [3, 4] hoặc hỗ trợ nó với hiệu suất hạn chế [23]. Những framework không hỗ trợ chức năng này pad các chuỗi ngắn với padding token để khớp độ dài của các chuỗi trong cùng batch. Các padding token lãng phí cả tính toán và bộ nhớ vì chúng không chứa bất kỳ thông tin hữu ích nào. ORCA [16] giới thiệu một giải pháp thú vị cho vấn đề này. Nó đầu tiên xác định rằng các đầu vào cho các lớp nhất định (ví dụ: feed-forward) trong Transformer có cùng hình dạng bất kể độ dài chuỗi và batch chúng mà không pad các chuỗi ngắn. Nó serialize các đầu vào cho các lớp (tức là self-attention) yêu cầu các đầu vào có hình dạng giống hệt nhau thay vì batch-process chúng. ORCA cho thấy rằng điều này có tác động không đáng kể đến độ trễ vì các đầu vào cho các lớp self-attention không chia sẻ trọng số và không hưởng lợi từ batching. Do đó, chúng tôi theo ORCA và sử dụng kỹ thuật selective batching của nó.

Cũng mượn từ ORCA kỹ thuật lập lịch iteration-level, S3 không đợi cho đến khi tất cả các chuỗi trong batch hiện tại kết thúc tạo. Thay vào đó, nó kiểm tra xem có chuỗi nào trong batch đã kết thúc tạo tại mọi lần lặp. Điều này cấp cho S3 tính linh hoạt lập lịch cao hơn và loại bỏ bất kỳ thời gian chờ dư thừa nào. Cuối cùng, nếu một mô hình không thể vừa trong một GPU, S3 sử dụng pipeline parallelism và chia các mô hình trong granularity lớp Transformer.

Supervisor Supervisor chịu trách nhiệm giám sát việc sử dụng GPU và xử lý những dự đoán sai. Supervisor chạy trong nền để kiểm tra không gian có sẵn trong HBM và chuyển thông tin cho scheduler. Scheduler sau đó thêm một chuỗi vào batch đang chạy nếu bộ nhớ có sẵn đủ lớn cho chuỗi.

Supervisor cũng chịu trách nhiệm xử lý những dự đoán sai. Trong trường hợp dự đoán ngắn, supervisor preempt những chuỗi vượt quá bộ nhớ được dành trước của chúng. Nó giám sát độ dài của các chuỗi đầu ra hiện tại và đuổi chúng nếu chúng chưa kết thúc nhưng đã sử dụng hết bộ nhớ được dành trước. Nó di chuyển bất đồng bộ trạng thái hiện tại của những chuỗi đó bao gồm bộ nhớ đệm KV và các token được tạo ra đến request pool và giải phóng bộ nhớ GPU. Bây giờ các ma trận K và V được phân mảnh với các hàng trống nơi bộ nhớ đệm KV bị đuổi ban đầu được lưu trữ. Supervisor dịch chuyển các hàng bên dưới hàng trống để tất cả các hàng được lưu trữ liền kề. Định dạng bộ nhớ này được yêu cầu bởi các thư viện hiện tại [24,25] và cũng giải quyết vấn đề phân mảnh. Cuối cùng, supervisor nhân đôi bộ nhớ được gán cho các chuỗi bị đuổi để sửa dự đoán ngắn sai.

Cuối cùng, supervisor liên tục huấn luyện predictor trong nền. Nó sử dụng các chuỗi mà predictor đã nhầm để huấn luyện predictor để nó có thể học từ những sai lầm của mình. Thời gian huấn luyện này tương đối ngắn và đo lường của chúng tôi cho thấy rằng mỗi lần lặp huấn luyện mất trung bình 11ms trong khi tạo chuỗi mất vài giây hoặc hơn. Điều này ngụ ý rằng overhead retraining ít hơn 10% ngay cả khi chúng ta huấn luyện predictor trong 10 epoch.

Kết hợp tất cả lại với nhau Chúng tôi tóm tắt phần này với một giải thích về cách S3 sử dụng từng thành phần để phục vụ một yêu cầu tạo-chuỗi. Đầu tiên, các yêu cầu tạo-văn bản đến tại request pool. Predictor dự đoán độ dài chuỗi đầu ra của các chuỗi trong pool. Supervisor chạy trong nền và kiểm tra việc sử dụng HBM hiện tại. Tiếp theo, scheduler sử dụng cả dự đoán và HBM có sẵn để batch các yêu cầu cho việc sử dụng GPU tối đa. Nó kết thúc công việc của mình bằng cách lập lịch batch đó cho GPU để tạo ra các chuỗi được lập lịch.

## 4 Đánh giá
Chúng tôi cho thấy rằng S3 đạt được thông lượng cao hơn bằng cách dự đoán độ dài chuỗi đầu ra. Chúng tôi cũng cho thấy rằng S3 có thể giảm chi phí phục vụ các mô hình bằng cách sử dụng ít GPU hơn.

Môi trường Chúng tôi chạy đánh giá của mình trên GPU NVIDIA 80GB A100 được kết nối với DRAM host qua PCIe 4.0 ×8 trong Máy chủ Lenovo ThinkSystem SD650-N V2 [27].

Baseline Chúng tôi so sánh với ORCA [16], một hệ thống phục vụ Transformer tăng thông lượng bằng lập lịch iteration level và selective batching. ORCA phải cấp phát bộ nhớ độ dài chuỗi tối đa khi nó không biết về độ dài chuỗi đầu ra để đảm bảo tạo chuỗi đầy đủ. S3 dự đoán độ dài chuỗi đầu ra và cấp phát bộ nhớ dựa trên dự đoán. Chúng tôi triển khai các hệ thống trên đầu FasterTransformer [4] vì thư viện này nhanh hơn HF-Transformers [3] do nhiều tối ưu hóa hơn. Chúng tôi cũng so sánh S3 với một hệ thống lý tưởng với predictor hoàn hảo mà chúng tôi gọi là Oracle.

Các mô hình Chúng tôi sử dụng các mô hình từ 6 tỷ tham số đến 175 tỷ tham số cho đánh giá. Các chi tiết cụ thể của những mô hình này được giải thích trong bảng 1.

### 4.1 Phân tích Thông lượng
Chúng tôi đánh giá thông lượng của S3 sử dụng dataset Alpaca [18], một dataset hỏi-đáp nguồn mở phổ biến. Cụ thể, chúng tôi truy vấn S3 với các câu hỏi và yêu cầu nó tạo ra các câu trả lời.

Kịch bản ngoại tuyến: Thông lượng tối đa Hình 4 (a) báo cáo thông lượng tối đa trong chuỗi mỗi giây cho các mô hình khác nhau. Chúng tôi đo thông lượng sử dụng kích thước batch tối đa của mỗi cấu hình. Nó cho thấy rằng S3 vượt trội ORCA từ 1,13× và lên tới 6,49× và khớp chặt chẽ với Oracle, khác biệt từ 9,34% và lên tới 40,52%. Sự khác biệt được khuếch đại với các mô hình lớn hơn vì kích thước batch bị giới hạn ngay cả đối với S3 vì hầu hết HBM được sử dụng để giữ trọng số mô hình. Kích thước batch trong S3 bị cắt trước khi làm bão hòa thông lượng như được hiển thị trong Hình 1.

Chúng ta có thể nhận thấy rằng thông lượng tối đa tăng 6,49× trong khi kích thước batch tăng gần 10× cho mọi mô hình. Điều này xuất phát từ hành vi độc đáo của Transformer. Các lớp Feed-forward trong các mô hình hưởng lợi từ việc batching các lớp trong khi các lớp self-attention thì không. Điều này là do các đầu vào trong feed-forward chia sẻ cùng trọng số trong khi các đầu vào trong self-attention chú ý đến các chuỗi riêng của chúng. Lợi ích hiệu suất của S3 được tăng lên khi phần song song lớn hơn phần serial. Tuy nhiên, tăng kích thước batch đòi hỏi thêm nhiều đầu vào được serialize từ đó làm tăng phần serial. GPT-J là một ví dụ tốt về đặc tính này cho thấy một bước nhảy thông lượng tương tự là 1,13× từ ORCA đến S3 và 1,09× từ S3 và Oracle trong khi kích thước batch khác biệt lần lượt là 8,69× và 1,97×.

Kịch bản trực tuyến: Thông lượng nhận biết SLO Bây giờ chúng tôi xem xét một kịch bản với ràng buộc SLO độ trễ. Chúng tôi đặt SLO là 0,1875 giây để tạo ra một token cho biết rằng người đọc tiếng Anh trung bình có thể đọc với tốc độ 4 từ mỗi giây [12] hoặc 0,25 giây mỗi từ, và 0,75 từ mỗi token [13]. Chúng tôi tiếp theo tính toán SLO độ trễ cho mỗi chuỗi bằng cách nhân 0,1875 với độ dài chuỗi của nó (ví dụ: 11,25s cho một chuỗi với 60 token).

Hình 4b báo cáo số lượng chuỗi mà mỗi mô hình tạo ra sử dụng ORCA, S3, và Oracle. Oracle vượt quá SLO cho GPT-J, LLAMA 13B, và GPT-NEOX khi nó chọn kích thước batch tối đa. Vì vậy chúng tôi giới hạn kích thước batch cho những mô hình này trong đánh giá này. S3 tạo ra số lượng chuỗi tương tự với trường hợp lý tưởng và từ 1,13× đến 6,49× nhiều chuỗi hơn so với ORCA. Tăng thông lượng tương tự với kịch bản ngoại tuyến vì S3 đáp ứng SLO trong hầu hết các trường hợp với kích thước batch tối đa của nó. Tuy nhiên, sự khác biệt giữa S3 và Oracle giảm 10% so với các kịch bản ngoại tuyến vì chúng tôi giới hạn kích thước batch do đó thông lượng của Oracle.

### 4.2 Phân tích Chi phí
Chúng tôi đánh giá S3 với số lượng GPU khác nhau. Chúng tôi phân vùng GPT-3 thành 6 và 8 GPU theo cách pipeline-parallel, cấp phát 16 và 12 lớp transformer mỗi GPU cho mỗi cấu hình, tương ứng. Chúng tôi cũng đánh giá trên cài đặt 10 GPU nơi chúng tôi cấp phát 10 lớp cho 9 GPU và 6 cho GPU còn lại. S3 pipeline mỗi batch và lập lịch một batch bất cứ khi nào GPU xử lý phân vùng đầu tiên chuyển kết quả của nó đến batch tiếp theo. Điều này giảm thời gian nhàn rỗi GPU bằng cách có mọi GPU xử lý các batch đồng thời.

Hình 5 báo cáo thông lượng tối đa sử dụng số lượng GPU khác nhau. Đầu tiên, chúng ta có thể thấy rằng S3 đạt được thông lượng tương tự sử dụng 6 GPU so với ORCA với 10 GPU. ORCA cho thấy thông lượng cao hơn 0,92% so với S3 để cụ thể. Nhiều GPU hơn chia mô hình thành các mảnh mịn hơn và để lại nhiều không gian hơn để lưu trữ bộ nhớ đệm KV, cho phép chúng ta tăng kích thước batch. Bảng 3 hỗ trợ tuyên bố này bằng cách báo cáo kích thước batch lớn hơn với nhiều GPU hơn. S3 có thể đạt được hiệu ứng tương tự với ít GPU hơn bằng cách tối ưu hóa chiến lược cấp phát bộ nhớ.

Tự nhiên, nó dẫn đến một câu hỏi tương tự với câu hỏi trong đánh giá thông lượng về tại sao S3 với 6 GPU cho thấy thông lượng tương tự với ORCA với 10 GPU ngay cả với kích thước batch gấp 2,33×. Câu trả lời nằm trong việc thực thi pipeline và bản chất tuần tự của các lớp self-attention trong các mô hình dựa trên Transformer như được giải thích trong 4.1. Các hệ thống hoàn thành một batch tại mỗi ⌈l/number of GPUs⌉ thay vì tại mỗi l lớp. Ví dụ, S3 với 6 GPU hoàn thành một batch tại mỗi 16 lớp thay vì 96 cho GPT3. Tương tự, ORCA với 10 GPU hoàn thành tại mỗi 10 lớp và do đó xử lý nhanh hơn. Sự tăng độ trễ phủ nhận lợi ích thông lượng từ S3 sao cho hai hệ thống cho thấy thông lượng gần như giống hệt ngay cả với kích thước batch lớn gấp 2,33× khi sử dụng S3.

### 4.3 Overhead: Phân tích Độ trễ
Chúng tôi đánh giá độ trễ runtime của mỗi thành phần trong S3. Chúng tôi phân loại độ trễ runtime thành ba loại: generation, penalty, và overhead. Generation là thời gian S3 dành để xử lý mô hình để tạo ra token. Penalty biểu thị thời gian cần để S3 preempt bộ nhớ đệm KV và các trạng thái ẩn từ GPU và tải nó trở lại GPU. Overhead bao gồm thời gian cần cho predictor, scheduler, và supervisor, kết hợp. Hình 6 cho thấy rằng penalty và overhead kết hợp là không đáng kể (tức là trung bình 11%) so với generation. Tất nhiên, penalty sẽ tăng nếu predictor kém chính xác hơn và kích hoạt nhiều traffic dữ liệu hơn giữa GPU và host. Ngược lại, overhead sẽ tăng nếu chúng ta sử dụng một predictor chính xác hơn nhưng nặng hơn. Với một predictor tốt, overhead này sẽ được khấu hao khi mô hình tạo văn bản tạo ra các chuỗi dài hơn.

## 5 Các Công trình Liên quan
Các hệ thống phục vụ machine learning Sự quan tâm cao đến machine learning đã khơi nguồn nhiều nghiên cứu trong các nền tảng dịch vụ của nó [1, 4, 6, 16, 28–38]. Đặc biệt, hiệu suất đáng ngạc nhiên của các mô hình ngôn ngữ dựa trên Transformer đã hướng nhiều nhà nghiên cứu phát triển các hệ thống phục vụ cụ thể cho Transformer [4, 6, 34, 37, 38]. Hầu hết các hệ thống tập trung vào giảm độ trễ mà không quan tâm nhiều đến thông lượng với các ngoại lệ của [6,31,39]. Các hệ thống hướng thông lượng sử dụng hệ thống phân cấp bộ nhớ để lưu trữ các phần của mô hình trong bộ nhớ chậm hơn và để tăng kích thước batch. Tuy nhiên, tất cả đều cấp phát cùng bộ nhớ cho mọi chuỗi và không xem xét preempt một chuỗi dựa trên dự đoán. S3 có thể cải thiện những hệ thống này bằng cách giảm kích thước bộ nhớ yêu cầu. Điều này sẽ loại bỏ nhu cầu cho các bộ nhớ lớn hơn nhưng chậm hơn như SSD và cũng giảm chi phí tài chính của bộ nhớ tổng thể.

Giảm overhead bộ nhớ đệm KV Vấn đề của các lớp attention trong Transformer yêu cầu tính toán và bộ nhớ bậc hai đối với độ dài chuỗi đã được nghiên cứu rộng rãi. Nhiều cách tiếp cận khác nhau đã được đề xuất để giải quyết vấn đề này. Xấp xỉ low-rank [40, 41] và khai thác sparsity [42–44] là trong số các phương pháp nhằm giảm thiểu vấn đề. Một cách tiếp cận khác, được gọi là multi-query attention [37, 45], đề xuất giảm kích thước cache bằng cách sử dụng một attention head mỗi bộ nhớ đệm key-value (KV) thay vì sử dụng nhiều attention head, như được minh họa trong Bảng 1. Ngoài ra, các công trình tập trung vào giảm kích thước mô hình thông qua nén [46] và quantization [6, 47, 48] cũng có thể đóng góp vào việc giảm kích thước của bộ nhớ đệm KV. Những cách tiếp cận này bổ sung cho công việc của chúng tôi và có thể được sử dụng để giảm penalty gây ra bởi những dự đoán sai, cho phép sử dụng một predictor kém chính xác hơn nhưng nhẹ hơn.

Dự đoán độ dài chuỗi Trong khi có các công trình hạn chế dự đoán độ dài chuỗi đầu ra dựa trên một chuỗi đầu vào, Yan et al. [49] đề xuất một mạng nhỏ dựa trên convolution với các lớp embedding để dự báo độ dài chuỗi đầu ra trong dịch máy. Trong cách tiếp cận của chúng tôi, chúng tôi sử dụng một chiến lược tương tự nhưng sử dụng một predictor nhỏ dựa trên Transformer để ước tính độ dài chuỗi đầu ra trong các tác vụ tạo văn bản. Predictor này cho phép chúng tôi dự đoán chính xác độ dài chuỗi đầu ra cho các workload tạo văn bản.

## 6 Giới hạn và Kết luận
Giới hạn Chúng tôi đưa ra những giả định sau trong công việc này. Chúng tôi giả định rằng các trace yêu cầu tạo văn bản mô phỏng các dataset hỏi-đáp có sẵn công khai vì không có trace có sẵn công khai. Phân tích trace thực tế sẽ tạo điều kiện triển khai S3 trong các workload thương mại. Tương tự, tác vụ tạo văn bản không có bất kỳ ràng buộc SLO độ trễ được chuẩn hóa nào như trong các workload machine learning khác [50] vì nó là một dịch vụ tương đối mới. Vì vậy chúng tôi giả định tốc độ đọc trung bình của một người đọc tiếng Anh làm SLO của chúng tôi. Chúng tôi sẽ có thể đánh giá S3 trong các kịch bản rộng hơn nếu các tổ chức công bố công khai các SLO khác nhau cho các ứng dụng tạo văn bản khác nhau.

Kết luận Tóm lại, chúng tôi giới thiệu S3, một framework được thiết kế để đạt được thông lượng cao trong việc phục vụ các mô hình sinh tạo dựa trên Transformer. S3 tận dụng một predictor để ước tính độ dài đầu ra của các chuỗi được tạo ra và lập lịch chúng tương ứng để tối đa hóa thông lượng. Ngoài ra, S3 xử lý các lỗi dự đoán tiềm năng để đảm bảo độ tin cậy. Bằng cách cấp phát kích thước bộ nhớ khác nhau cho các đầu vào khác nhau, S3 thừa nhận rằng không phải tất cả các chuỗi đều nên được đối xử như nhau. Cách tiếp cận này mở rộng sự đánh đổi thông thường giữa độ trễ và thông lượng frontier, mở đường cho những khả năng mới trong tối ưu hóa sự đánh đổi độ trễ-thông lượng.
