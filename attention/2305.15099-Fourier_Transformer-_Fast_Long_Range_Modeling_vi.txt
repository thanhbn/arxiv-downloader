# 2305.15099.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2305.15099.pdf
# Kích thước tệp: 429609 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Fourier Transformer: Mô hình hóa Tầm xa Nhanh
bằng cách Loại bỏ Dư thừa Chuỗi với Toán tử FFT
Ziwei He♢, Meng Yang†, Minwei Feng†, Jingcheng Yin†,
Xinbing Wang♢,Jingwen Leng♢và Zhouhan Lin♢∗
♢Đại học Giao thông Thượng Hải †Netease BizEase
{ziwei.he, xwang8, leng-jw}@sjtu.edu.cn∗lin.zhouhan@gmail.com
Tóm tắt
Mô hình transformer được biết đến là đòi hỏi
tính toán cao, và cực kỳ tốn kém cho các chuỗi
dài, vì mô-đun self-attention sử dụng độ phức
tạp thời gian và không gian bậc hai so với độ
dài chuỗi. Nhiều nhà nghiên cứu đã tập trung
vào việc thiết kế các hình thức mới của self-
attention hoặc giới thiệu các tham số mới để
vượt qua hạn chế này, tuy nhiên một phần lớn
trong số họ cấm mô hình kế thừa trọng số từ
các mô hình pretrained lớn. Trong công trình
này, tính không hiệu quả của transformer đã
được xử lý từ một góc độ khác. Chúng tôi đề
xuất Fourier Transformer, một phương pháp
đơn giản nhưng hiệu quả bằng cách loại bỏ
dần các dư thừa trong chuỗi ẩn sử dụng toán
tử Fast Fourier Transform (FFT) có sẵn để
thực hiện Discrete Cosine Transformation
(DCT). Fourier Transformer có thể giảm đáng
kể chi phí tính toán trong khi vẫn giữ khả năng
kế thừa từ các mô hình pretrained lớn khác
nhau. Các thí nghiệm cho thấy mô hình của
chúng tôi đạt được hiệu suất tiên tiến nhất
trong tất cả các mô hình dựa trên transformer
trên benchmark mô hình hóa tầm xa LRA với
cải thiện đáng kể về cả tốc độ và không gian.
Đối với các tác vụ seq-to-seq sinh tạo bao gồm
CNN/DailyMail và ELI5, bằng cách kế thừa
trọng số BART, mô hình của chúng tôi vượt
trội hơn BART tiêu chuẩn và các mô hình hiệu
quả khác.¹
1 Giới thiệu
Transformers (Vaswani et al., 2017), đặc biệt là
khi được trang bị pre-training quy mô lớn (De-
vlin et al., 2018; Lewis et al., 2019; Raffel et al.,
2020) đã trở thành kiến trúc cốt lõi trong hầu hết
các tác vụ trong xử lý ngôn ngữ tự nhiên (NLP),
bao gồm cả các tác vụ chỉ encoder như phân loại
câu, gắn thẻ chuỗi (Liu et al., 2019), và các tác vụ
encoder-decoder như tóm tắt văn bản
∗Zhouhan Lin là tác giả liên hệ.
¹Mã của chúng tôi được công khai tại https://github.com/
LUMIA-Group/FourierTransformervà trả lời câu hỏi (Lewis et al., 2019). Tuy
nhiên, do độ phức tạp bậc hai của mô-đun self-
attention (Lin et al., 2017), việc áp dụng các mô
hình này trên các chuỗi dài có thể cực kỳ tốn kém.
Do đó, nhiều nỗ lực đã được đầu tư vào việc phát
triển các biến thể Transformer hiệu quả khác nhau
(Tay et al., 2020b), cũng như thiết lập các bộ thử
nghiệm chuẩn hóa cho các chuỗi dài như Long
Range Arena (LRA) (Tay et al., 2020a).

Hầu hết các Transformers hiệu quả đều thiết kế
các biến thể attention đặc biệt để giảm độ phức tạp
của nó (Tay et al., 2020b). Một số trong số chúng
đạt được điều này bằng cách chiếu các thành phần
trong self-attention thành các xấp xỉ hạng thấp hơn
(Wang et al., 2020; Zhu et al., 2021; Winata et al.,
2020, trong số những cái khác), hoặc dựa vào
kernelization để tính ngầm ma trận attention
(Katharopoulos et al., 2020; Choromanski et al.,
2020b; Peng et al., 2021; Choromanski et al.,
2020a, trong số những cái khác).

Do việc giới thiệu các ma trận chiếu hoặc tham số
bổ sung, các mô hình này không thể kế thừa các
tham số mô hình pretrained. Tuy nhiên, vì các mô
hình ngôn ngữ lớn pretrained (LLMs) đã ảnh hưởng
cơ bản đến cộng đồng NLP, việc lệch hướng kiến
trúc mô hình khỏi LLMs đòi hỏi pre-training từ đầu
trên mô hình được thiết kế, điều này đòi hỏi tài
nguyên cực kỳ lớn đối với hầu hết các nhà thực hành.

Các phương pháp khác nhắm vào việc tính toán
một phần của ma trận attention, bằng cách tuân
theo một số mẫu được định nghĩa trước (Child et al.,
2019; Qiu et al., 2020; Ho et al., 2019, trong số
những cái khác). Một số trong số chúng cho phép
mẫu có thể học được (Sukhbaatar et al., 2019; Roy
et al., 2021, trong số những cái khác). Hầu hết các
mẫu đòi hỏi các kernels CUDA tùy chỉnh hoặc các
toán tử đặc biệt để đạt được tăng tốc được tuyên bố
(Wu et al., 2019; Child et al., 2019; Beltagy et al.,
2020), điều này tạo ra thêm thách thức trong việc
triển khai các mô hình này trên các thiết bị biên
hoặc phần cứng đặc biệt như TPUs.

Hơn nữa, một số phương pháp liên quan đến các
bước tính toán bổ sung đáng kể, mà trong
thực tế có thể cân bằng độ phức tạp thời gian và
bộ nhớ mà chúng giảm, đặc biệt đối với các chuỗi
ngắn và trung bình (Kitaev et al., 2020; Roy
et al., 2021).

Một yếu tố cốt lõi đằng sau các phương pháp khác
nhau là sự tồn tại của dư thừa trong các ma trận
attention và trạng thái ẩn. Ví dụ, Wang et al. (2020)
cung cấp phân tích phổ trên ma trận self-attention,
chỉ ra rằng ma trận attention học cách có hạng thấp,
điều này cho phép họ học một xấp xỉ hạng thấp của
ma trận attention. Được truyền cảm hứng từ hướng
nghiên cứu này, trong công trình này, chúng tôi
phân tích phổ công suất của các trạng thái ẩn trong
chiều thời gian qua các lớp khác nhau trong Hình 1,
và cho thấy phổ công suất ngày càng tập trung vào
các bin tần số thấp hơn khi lớp càng sâu.

Trong công trình này, chúng tôi đề xuất Fourier
Transformer, mà thậm chí không yêu cầu học ma
trận chiếu để xấp xỉ self-attention. Fourier
Transformer tận dụng quan sát của chúng tôi về
phổ công suất của các trạng thái ẩn, nó loại bỏ dần
các dư thừa chuỗi qua các lớp khác nhau bằng cách
downsampling các trạng thái ẩn với Discrete
Cosine Transform (DCT), một biến thể của biến
đổi Fourier tạo ra các giá trị thực.

DCT trong Fourier Transformer được đề xuất của
chúng tôi có thể được thực hiện với toán tử Fast
Fourier Transform (FFT). Nhờ ứng dụng sâu rộng
trong nén ảnh và xử lý tín hiệu, nó là một trong
những toán tử được sử dụng rộng rãi nhất và được
tối ưu hóa cao trong nhiều framework khác nhau
và thậm chí trên các thiết bị biên, cung cấp độ phức
tạp O(nlogn) và lên đến O(logn) trong các triển
khai song song với overhead không đáng kể. Do
đó, Fourier Transformer dễ dàng triển khai trên
một loạt các thiết bị, không cần thiết phải thiết kế
các kernels CUDA đặc biệt. Ngoài ra, kết quả thí
nghiệm trên các tác vụ LRA cho thấy nó hoạt động
nhanh hơn đáng kể so với nhiều Transformers hiệu
quả khác, trong khi đạt được hiệu suất tiên tiến
nhất trong các mô hình hiệu quả dựa trên
Transformer.

Mặt khác, vì DCT là một phép biến đổi tuyến tính,
có thể đảo ngược, và self-attention không bị can
thiệp trong mô hình của chúng tôi, Fourier
Transformer được đề xuất có thể kế thừa trọng số
pretrained từ các mô hình ngôn ngữ lớn mà không
làm tổn hại đến hiệu suất. Kết quả thí nghiệm trên
CNN-DailyMail (Hermann et al., 2015) và ELI5
(Fan et al., 2019c) cho thấy mô hình của chúng tôi
có thể vượt trội hơn BART (Lewis et al., 2019) và
các Transformers hiệu quả khác bằng cách kế thừa
và fine-tuning trên BART. Hơn nữa, với một lượng
nhỏ further pretraining trước khi fine-tuning,
hiệu suất của nó có thể được cải thiện thêm.

2 Công trình liên quan
Downsampling các trạng thái ẩn Không có nhiều
công trình downsample độ dài chuỗi cho ngôn ngữ
tự nhiên. Công trình gần nhất là Funnel Trans-
former (Dai et al., 2020), loại bỏ dần độ dài chuỗi
query thông qua strided mean pooling, trong khi
giữ nguyên độ dài chuỗi key và value. Fourier
Transformer nén cả ba chuỗi với nhau và cung cấp
tăng tốc tính toán nhiều hơn so với Funnel
Transformer. Lưu ý rằng Funnel Transformer cần
tái đầu tư các tính toán đã tiết kiệm để xây dựng
một mô hình lớn hơn để đạt được hiệu suất tốt hơn,
điều này vô hiệu hóa khả năng kế thừa trọng số
pretrained của nó.

Đối với các công trình khác, Charformer (Tay et al.,
2021b) thiết kế một mô-đun tokenization có thể
vi phân cũng dựa vào strided mean pooling để
downsample chuỗi byte của nó. Nyströmformer
(Xiong et al., 2021) xấp xỉ ma trận attention thông
qua phương pháp Nyström, mà hiệu quả downsample
các chuỗi query và key. Do convolution depth-wise
bổ sung, nó một lần nữa không thể tận dụng các
mô hình pretrained.

Trong góc nhìn rộng hơn, downsampling đã được
ưa chuộng hơn trong thị giác máy tính. (Chen et al.,
2020) downsample một cách quyết liệt đầu vào thô
thành một vector 1D. Perceiver (Jaegle et al., 2021)
áp dụng một cơ chế attention bất đối xứng để chưng
cất đầu vào thành một bottleneck latent chặt chẽ.
Hầu như tất cả các mô hình thị giác này được thiết
kế cho các tác vụ thị giác chỉ encoder thay vì các
tác vụ NLP kiểu encoder-decoder.

Biến đổi Fourier cho Transformer Có nhiều công
trình gần đây kết hợp biến đổi Fourier vào
Transformer. FNet (Lee-Thorp et al., 2021) thực
hiện một phương pháp cực đoan hơn bằng cách thay
thế toàn bộ self-attention bằng 2D FFT, loại bỏ
toàn bộ phần ảo để tránh số phức. Performer
(Choromanski et al., 2020a) giới thiệu các đặc
trưng Fourier ngẫu nhiên trực giao để xấp xỉ
softmax attention. FSAT (Zhuang et al., 2022) sử
dụng 1D FFT dọc theo chiều chuỗi để học cấu trúc
thưa thớt của ma trận attention. DCT-Former
(Scribano et al., 2022) dịch các chuỗi vào miền tần
số và thực hiện self-attention ở đó trước khi chiếu
chúng trở lại, do tính phi tuyến trong mạng, self-
attention được huấn luyện trong miền tần số lệch
đáng kể so với trong miền thời gian. Do đó, tất cả
các mô hình đã

--- TRANG 3 ---
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 0 (word embedding)
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 1
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 2
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 3
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 4
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 5
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 6
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 7
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 8
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 9
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 10
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 11Hình 1: Phổ công suất của các trạng thái ẩn đầu vào từ các lớp khác nhau trong mô hình RoBERTa pretrained (Liu et al., 2019). Các trục ngang biểu thị các bin tần số, bắt đầu từ các thành phần tần số thấp bên trái. Các trục dọc là các biên độ tương ứng. Biên độ được tính trung bình trên tất cả các chiều ẩn và trên toàn bộ tập validation của Wiki-103 (Merity et al., 2016). Vì các đầu vào là số thực, các thành phần tần số dương và âm là liên hợp theo cặp. Do đó chúng tôi chỉ vẽ biên độ của nửa dương của các tần số.

thảo luận ở trên cũng mất khả năng kế thừa trọng số pretrained.

3 Kiến thức cơ bản
3.1 Discrete Cosine Transform
Discrete Cosine Transform (DCT) biểu diễn một chuỗi các số thực dưới dạng tổng của các hàm cosine với tần số khác nhau. Vì DCT chỉ tạo ra các giá trị thực, nó là một sự thay thế cho biến đổi Fourier trong lĩnh vực số thực. Nó đã là phép biến đổi cốt lõi đằng sau định dạng nén ảnh có mất mát JPEG².

Một cách chính thức, cho một chuỗi N số thực {xₙ}={x₀, x₁, ...xₙ₋₁}, DCT biến đổi nó vào miền tần số thông qua³:

yₖ = αₖ ∑ᴺ⁻¹ₙ₌₀ xₙcos(πk(2n+1)/2N)  (1)

trong đó k ∈ {0, ..., N-1} và αₖ là một hệ số liên quan đến k:

αₖ = { √(1/N) nếu k = 0,
      √(2/N) ngược lại        (2)

Chuỗi gốc {xₙ} có thể được khôi phục với DCT nghịch đảo (IDCT):

xₙ = ∑ᴺ⁻¹ₖ₌₀ αₖyₖcos(πk(2n+1)/2N)  (3)

mà chúng tôi sẽ ký hiệu là {xₙ} = IDCT({yₖ}).

²https://jpeg.org/jpeg/
³Có một số biến thể hơi khác nhau của DCT. Ở đây chúng tôi sử dụng biến thể type-II phổ biến nhất trong bài báo này.

Trên thực tế, DCT có thể được tính toán bằng cách sử dụng toán tử FFT. Đầu tiên, cho {uₙ} là {xₙ} được xáo trộn bằng cách xen kẽ các giá trị của nó ở vị trí chẵn và lẻ. Một cách chính thức, khi N là một số nguyên lẻ, {uₙ} được cho bởi

{uₙ} = {x₀, x₂, ..., xₙ₋₁, xₙ₋₂, xₙ₋₄, ..., x₁}  (4)

Khi N chẵn, một sự xáo trộn tương tự áp dụng. Sau đó chúng ta biến đổi {uₙ} vào miền tần số của nó thông qua FFT:

{vₖ} = FFT({uₙ})  (5)

trong đó k ∈ {0, ..., N-1} và {vₖ} là một chuỗi có độ dài N. DCT của chuỗi gốc {xₙ} do đó có thể được tính từ {vₖ}:

yₖ = cos(πk/2N)Re(vₖ) - sin(πk/2N)Im(vₖ)  (6)

trong đó Re(·) và Im(·) đại diện cho phần thực và phần ảo, tương ứng.

3.2 Phổ Công suất của Trạng thái Ẩn Transformer
Phổ công suất của một chuỗi rời rạc mô tả sự phân bố công suất tín hiệu w.r.t. các thành phần tần số, đó là biên độ của các thành phần tần số được tạo ra bởi biến đổi Fourier. Đối với một lớp nhất định trong Transformer, các trạng thái ẩn của nó có thể được coi như một chuỗi các vector ẩn, dọc theo chiều thời gian. Để phân tích phổ công suất của lớp, chúng tôi thực hiện biến đổi Fourier 1D một cách độc lập dọc theo chiều thời gian

--- TRANG 4 ---
IDCT
DCT
hₙhₙ
Block 1Block 2Block 3
Upsample
Upsample
Residual+
+Cho Thiết lập Chỉ EncoderSoftmaxXác suất 
Lớp
Đến classification head
Cho Thiết lập Encoder-Decoder
DecoderSpectral Filter
Spectral Filter
Truncate~
InputHình 2: Kiến trúc Mô hình Tổng thể

chiều cho các vector ẩn, tính toán các biên độ tương ứng, và tính trung bình trên tất cả các chiều trong lớp đó. Ngoài ra, chúng tôi tính toán phổ trung bình trên nhiều chuỗi văn bản để loại bỏ nhiễu theo từng ví dụ.

Hình 1 cho thấy phổ công suất cho các lớp khác nhau trong mô hình RoBERTa-base pretrained (Liu et al., 2019). Hình con trên-trái cho thấy phổ công suất của word embeddings tương đối phẳng, phân bố năng lượng của nó gần như đồng đều trên tất cả các thành phần tần số với một số đỉnh trong tần số thấp. Khi lớp càng sâu, năng lượng bắt đầu tập trung về phía tần số thấp và các đỉnh bắt đầu làm mịn, để lại một đuôi dài ở phía tần số cao. Xu hướng này chỉ ra rằng các trạng thái ẩn trong các lớp sâu hơn có tương quan cục bộ nhiều hơn, điều này để lại không gian cho biến đổi Fourier để vắt kiệt các dư thừa.

4 Fourier Transformer
4.1 Kiến trúc Mô hình
Kiến trúc tổng thể của Fourier Transformer được mô tả trong Hình 2. Nhìn chung, chúng tôi chèn các bộ lọc phổ giữa các lớp trong Transformer, bên trong đó chúng tôi sử dụng DCT và IDCT để downsample độ dài chuỗi. Nhiều bộ lọc phổ có thể làm việc cùng nhau để chia các lớp Transformer thành các block khác nhau, do đó giảm dần độ dài chuỗi. Chúng tôi để nguyên self-attention để giữ lại khả năng kế thừa trọng số pretrained của nó.

Đối với bộ lọc phổ, nó bao gồm ba bước, tức là, transform, truncate, và reverse. Một cách chính thức, đối với một chuỗi ẩn đến {hₙ}, 0 < n < N-1 chứa N vector ẩn hₙ ∈ ℝᴰ trong đó D là kích thước ẩn của mô hình, bộ lọc phổ

đầu tiên biến đổi nó vào miền tần số thông qua 1D-DCT:

{yₖ} = DCT({hₙ}), 0 < k < N-1  (7)

Lưu ý rằng DCT được áp dụng độc lập trên tất cả chiều trong {hₙ}, do đó chỉ biến đổi dọc theo chiều thời gian.

Tiếp theo, {yₖ} được truncate bằng cách cắt bỏ các chiều cuối ở phía tần số cao. Đối với các chuỗi có độ dài khác nhau, chúng tôi cố định một tỉ lệ r ∈ (0,1), đây là một siêu tham số, để xác định số lượng thành phần tần số cần giữ lại. Do đó độ dài của {yₖ} được truncate từ N thành ⌈rN⌉.⁴

Cuối cùng, chuỗi ngắn hơn kết quả {yₖ}, 0 < k < ⌈rN⌉-1 có thể được biến đổi trở lại miền thời gian thông qua IDCT, tạo ra một chuỗi ngắn hơn của {h̃ₙ}:

{h̃ₙ} = IDCT({yₖ}), 0 < n < ⌈rN⌉-1  (8)

Một lần nữa, IDCT cũng được thực hiện chỉ trong chiều thời gian. Các trạng thái ẩn ngắn hơn kết quả được truyền đến các lớp trên.

Tùy thuộc vào loại tác vụ, các phần tiếp theo khác nhau. Chúng tôi sẽ làm rõ chúng trong các thiết lập chỉ encoder và encoder-decoder.

Thiết lập Chỉ Encoder Đối với các tác vụ chỉ encoder như phân loại văn bản, đầu ra cuối cùng của

⁴chúng tôi đã thử với nhiều cách truncation khác nhau ở đây, như cắt bỏ các thành phần tần số thấp, cắt bỏ những thành phần có biên độ trung bình thấp hơn, loại bỏ các thành phần DC và chuẩn hóa lại chuỗi, trừ một giá trị chung trên tất cả các thành phần và chuẩn hóa lại, hoặc giữ lại các thành phần tương ứng với các đỉnh. Thú vị là cách khá cổ điển là chỉ đơn giản cắt bỏ những thành phần tần số cao lại hoạt động tốt nhất.

--- TRANG 5 ---
encoder được kỳ vọng là một vector có kích thước cố định, sau đó được đưa vào hồi quy logistic để dự đoán xác suất lớp. Trong công trình này, trong khi mô hình được huấn luyện từ đầu, chúng tôi đơn giản sử dụng mean pooling trên toàn bộ chuỗi đầu ra để tạo ra vector này; ngược lại khi mô hình kế thừa một token [CLS] từ các mô hình pretrained, chúng tôi sử dụng embedding tại token đó thay thế.

Thiết lập Encoder-Decoder Đối với các tác vụ sinh tạo ngôn ngữ bao gồm cả encoder và decoder, có một encoder-decoder attention quan tâm đến các trạng thái encoder tại mỗi bước decoder. Tuy nhiên, encoder-decoder attention yêu cầu độ phân giải vị trí chi tiết để hoạt động tốt. Do đó chúng tôi theo Dai et al. (2020) để upsample các chuỗi ngắn hơn trở lại độ dài gốc của chúng, và cộng các chuỗi ẩn được upsample tại tất cả các block với nhau trước khi đưa chúng vào decoder. Cụ thể hơn, chúng tôi sử dụng phép nội suy láng giềng gần nhất không có tham số để upsample, và chúng tôi chuẩn hóa lại chuỗi sau khi cộng các chuỗi được upsample.

4.2 Further Pretraining
Vì DCT có thể đảo ngược thông qua IDCT, mô hình được đề xuất xấp xỉ một cách liền mạch vanilla Transformer khi r tăng lên. Hình 3 cho thấy rằng trong khi fine-tuning trực tiếp trên trọng số BART (Lewis et al., 2019), mô hình hoạt động tương đối tốt khi lên đến 70% thành phần tần số bị truncate. Tuy nhiên, vì upsampling và việc cộng các chuỗi được upsample vẫn khác với Transformer gốc, chúng ta vẫn có thể vắt giọt cuối cùng bằng cách áp dụng một lượng nhỏ further pretraining trước khi fine-tuning, và cải thiện thêm hiệu suất mô hình. Loại further pretraining này thuận lợi hơn nhiều so với pretraining tùy chỉnh từ đầu, có thể tốn một lượng tài nguyên tính toán khổng lồ.

Như một ví dụ cụ thể, further pretraining mô hình của chúng tôi trên BART-Large tiêu thụ khoảng 10GB dữ liệu và mất khoảng 4 ngày trên 2 GPU NVidia A100, trong khi pretraining BART từ đầu cần tiêu thụ 160GB dữ liệu, mất khoảng 1000 ngày với cùng thiết bị. So với pretraining tùy chỉnh từ đầu, việc tận dụng trọng số BART và further pretraining mất ít hơn 2 bậc độ lớn tài nguyên tính toán, trong khi vẫn có thể đưa mô hình đến hiệu suất tương tự hoặc thậm chí tốt hơn.

4.3 Phân tích Độ phức tạp
Đối với một lớp Transformer tiêu chuẩn với chiều mô hình D, bao gồm self-attention và 2 lớp feed-forward, độ phức tạp thời gian và bộ nhớ của việc xử lý một chuỗi đầu vào có độ dài N là O(N²D + ND²) và O(N² + ND), tương ứng. Với toán tử FFT, mô hình của chúng tôi có thể nén độ dài chuỗi từ N thành ⌈rN⌉ trong độ phức tạp thời gian O(NlogN). Do đó Fourier Transformer hưởng độ phức tạp thời gian và bộ nhớ O(r²N²D + rND² + NlogN) và O(r²N² + rND) mỗi khi độ dài chuỗi bị giảm. Thực tế, cho triển khai song song của FFT, số hạng độ phức tạp thời gian O(NlogN) bổ sung là không đáng kể so với hai số hạng khác. Tăng tốc có thể trở nên ấn tượng hơn khi độ dài chuỗi tương đối dài. Chúng tôi giới thiệu độc giả đến Phần 5.1 để biết thêm chi tiết.

5 Thí nghiệm
Trong phần này, chúng tôi thử nghiệm với mô hình của chúng tôi trong cả hai thiết lập chỉ encoder và encoder-decoder trong các tập dữ liệu khác nhau liên quan đến các chuỗi dài.

5.1 Các Tác vụ Chỉ Encoder
Để kiểm tra khả năng của mô hình trên các tác vụ chỉ encoder, chúng tôi chọn 5 tác vụ trong benchmark Long Range Arena (LRA) được sử dụng rộng rãi (Tay et al., 2020a). LRA được thiết kế để đánh giá các transformers hiệu quả trong tình huống ngữ cảnh dài, với độ dài chuỗi đầu vào dao động từ 1K đến 8K. Các tập dữ liệu trong LRA đến từ các nguồn phong phú, bao gồm ngôn ngữ tự nhiên, pixel hình ảnh, biểu thức toán học, v.v. Cụ thể hơn, chúng là:

ListOps Một tập dữ liệu các biểu thức toán học yêu cầu mô hình tính toán giá trị đầu ra của một biểu thức toán học với độ dài chuỗi lên đến 2K.

Text Một tác vụ phân loại văn bản cấp byte, với độ dài chuỗi cố định 4K yêu cầu mô hình xử lý tính tổng hợp.

Retrieval Một tác vụ truy xuất tài liệu cấp byte với độ dài tối đa 8K kiểm tra khả năng nén các chuỗi dài của mô hình.

Image Một tác vụ phân loại hình ảnh yêu cầu mô hình học các mối quan hệ không gian 2D giữa các pixel đầu vào bằng cách đọc tuần tự các pixel. Độ dài chuỗi được cố định ở 1K.

--- TRANG 6 ---
Mô hình | ListOps | Text | Retrieval | Image | Pathfinder | Avg.
Transformer (Vaswani et al., 2017) | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | 54.39
Longformer (Beltagy et al., 2020) | 35.63 | 62.85 | 56.89 | 42.22 | 69.71 | 53.46
Linformer (Wang et al., 2020) | 35.70 | 53.94 | 52.27 | 38.56 | 76.34 | 51.36
Reformer (Kitaev et al., 2020) | 37.27 | 56.10 | 53.40 | 38.07 | 68.50 | 50.67
Synthesizer (Tay et al., 2021a) | 36.99 | 61.68 | 54.67 | 41.61 | 69.45 | 52.88
BigBird (Zaheer et al., 2020) | 36.05 | 64.02 | 59.29 | 40.83 | 74.87 | 55.01
Performer (Choromanski et al., 2020a) | 18.01 | 65.40 | 53.82 | 42.77 | 77.50 | 51.41
FNet (Lee-Thorp et al., 2021) | 35.55 | 65.11 | 59.61 | 38.67 | 77.80 | 55.30
Nyström (Xiong et al., 2021) | 37.15 | 65.52 | 79.56 | 41.58 | 70.94 | 58.95
Luna-256 (Ma et al., 2021) | 37.25 | 64.57 | 79.29 | 47.38 | 77.32 | 61.24
FSAT (Zhuang et al., 2022) | 46.85 | 65.95 | 81.11 | 49.97 | 77.32 | 64.24
Fourier Transformer (của chúng tôi) | 40.73 | 75.02 | 85.35 | 53.17 | 83.43 | 67.54

Bảng 1: Kết quả trên benchmark LRA. Chúng tôi báo cáo độ chính xác phân loại cho mỗi tác vụ và độ chính xác trung bình trên tất cả các tác vụ. Kết quả từ Longformer đến Performer được lấy từ Tay et al. (2020a), phần còn lại được lấy từ các bài báo tương ứng của chúng. Đối với mô hình FSAT trên tác vụ Text, chúng tôi chỉ xem xét kết quả không có convolutions.

Bước trên giây ↑ | Sử dụng Bộ nhớ Đỉnh ↓
Mô hình | 1K | 2K | 3K | 4K | 1K | 2K | 3K | 4K
Transformer | 1.0x | 1.0x | 1.0x | 1.0x | 1.0x | 1.0x | 1.0x | 1.0x
Reformer | 0.5x | 0.4x | 0.7x | 0.8x | 0.56x | 0.37x | 0.28x | 0.24x
BigBird | 0.9x | 0.8x | 1.2x | 1.1x | 0.91x | 0.56x | 0.4x | 0.3x
Synthesizer | 1.1x | 1.2x | 2.9x | 1.4x | 0.76x | 0.75x | 0.74x | 0.74x
FSAT | 1.1x | 1.5x | 2x | 2.5x | 0.53x | 0.27x | 0.21x | 0.16x
Linformer | 1.2x | 1.9x | 3.7x | 5.5x | 0.44x | 0.21x | 0.18x | 0.1x
Performer | 1.2x | 1.9x | 3.8x | 5.7x | 0.44x | 0.22x | 0.15x | 0.11x
Fourier Transformer (của chúng tôi) | 6.9x | 12.2x | 16.8x | 17.7x | 0.23x | 0.19x | 0.18x | 0.18x

Bảng 2: Tốc độ và tiêu thụ bộ nhớ trên benchmark LRA cho tác vụ Text với độ dài đầu vào 1K, 2K, 3K và 4K. Kết quả từ Reformer đến Performer được lấy từ Zhuang et al. (2022). Tốc độ và tiêu thụ bộ nhớ được liệt kê dưới dạng tỉ lệ w.r.t. vanilla Transformer.

Pathfinder Một tác vụ phân loại hình ảnh tổng hợp với độ dài đầu vào cố định 1K yêu cầu mô hình nắm bắt các phụ thuộc không gian tầm xa.

5.1.1 Chi tiết Triển khai
Chúng tôi chạy thí nghiệm trên benchmark LRA tuân theo chặt chẽ các cấu hình trong (Tay et al., 2020a), bao gồm tiền xử lý dữ liệu, chia dữ liệu, kiến trúc mô hình, siêu tham số (số lượng lớp, chiều ẩn, v.v.). Chúng tôi đánh giá theo độ chính xác phân loại. Triển khai của chúng tôi dựa trên (Xiong et al., 2021). Vì sự đơn giản, chúng tôi báo cáo kết quả của mô hình với cùng ngân sách nén trên năm tác vụ. Chúng tôi giảm mạnh 80% độ dài chuỗi đầu vào tại lớp đầu tiên.

5.1.2 Hiệu suất & Hiệu quả
Kết quả trên 5 tác vụ đã nêu được tóm tắt trong Bảng 1. Chúng tôi so sánh Fourier Transformer với một loạt các mô hình dựa trên Transformer đã được công bố trước đó, và nó đạt được kết quả tiên tiến nhất mới trên 4 trong 5 tác vụ. Mô hình được đề xuất của chúng tôi cải thiện so với mô hình SOTA trước đó (Zhuang et al., 2022) trên Text, Retrieval, Image và Pathfinder lần lượt là 9.07%, 4.24%, 3.20%, 6.11% giá trị tuyệt đối, đây là một biên độ lớn. Đáng chú ý, mô hình của chúng tôi không đánh bại FSAT (Zhuang et al., 2022) trên tác vụ ListOps và xếp thứ 2 trong danh sách. Chúng tôi phỏng đoán rằng đó là do các giá trị biểu thức toán học nhạy cảm hơn

đối với các token riêng lẻ trong chuỗi, do đó nhạy cảm hơn với downsampling.

Tiếp theo, lấy tác vụ phân loại văn bản cấp byte (tập dữ liệu Text) làm testbed, chúng tôi đánh giá định lượng hiệu quả thời gian và bộ nhớ của mô hình và các mô hình cạnh tranh khác trên các độ dài đầu vào khác nhau. Kết quả được tóm tắt trong Bảng 2. Lưu ý rằng, do hạn chế bộ nhớ GPU cho vanilla Transformer, kết quả trên độ dài 1K, 2K và 3K được chạy với kích thước batch 32, và 4K với kích thước batch 16. Chúng tôi tính toán các tỉ lệ tương ứng của mô hình w.r.t. vanilla Transformer trên các thiết lập batch giống hệt nhau, và tính thời gian trên GPU NVidia A100-80G. So với các transformers hiệu quả khác, Fourier Transformer giảm đáng kể tiêu thụ thời gian trên cả chuỗi ngắn và dài, để lại các mô hình khác phía sau với một biên độ lớn, trong khi duy trì tiết kiệm bộ nhớ ổn định khi độ dài chuỗi tăng.

5.2 Các Tác vụ Encoder-Decoder
Mô hình cho các tác vụ encoder-decoder được trang bị decoder để thực hiện sinh tạo văn bản. Cho thiết lập này, chúng tôi chọn hai tập dữ liệu văn bản dài trong các tác vụ tóm tắt và trả lời câu hỏi, tức là, CNN/DailyMail (Hermann et al., 2015) và ELI5 (Fan et al., 2019c), với độ dài chuỗi trung bình lần lượt là 0.8K và 5K.

CNN/DailyMail Một tập dữ liệu tóm tắt chứa hơn 280K bài báo tin tức (trung bình 766 token) từ các câu chuyện tin tức trong các trang web CNN và Daily Mail được ghép đôi với các tóm tắt do con người tạo ra (trung bình 53 token). Chúng tôi tuân theo quy ước và đánh giá hiệu suất theo điểm Rouge (Rouge-1, Rouge-2, Rouge-L) (Lin, 2004).

ELI5 Một tập dữ liệu trả lời câu hỏi chứa hơn 270K cặp câu hỏi-câu trả lời phức tạp, đa dạng và có độ dài đoạn văn được thu thập từ các subreddits, số lượng token trung bình cho đầu vào và mục tiêu lần lượt là 5140 và 693. Tuân theo quy ước, chúng tôi đánh giá nó bằng cả điểm Rouge-L và F1.

5.2.1 Chi tiết Triển khai
Vì trên cả hai tập dữ liệu, các mô hình pretrained để lại một khoảng cách lớn so với những mô hình không pretrained, việc báo cáo kết quả mà không pretrained ít ý nghĩa hơn. Do đó, chúng tôi báo cáo kết quả của mô hình kế thừa trọng số BART-large (Lewis et al., 2019). Chúng tôi thường kiểm tra hai thiết lập, đó là: 1) fine-tune trực tiếp

mô hình của chúng tôi trên tập dữ liệu, và 2) thực hiện further pretraining trước khi fine-tuning. Để thuận tiện, chúng tôi gọi chúng là Fourier-BART và Fourier-BART-FP tương ứng trong phần còn lại của bài báo.

Fourier-BART có cùng kiến trúc với BART-large. Nó đơn giản áp dụng thiết kế 2-block, block đầu tiên chứa 2 lớp transformer liên tiếp đầu tiên, 10 lớp còn lại thuộc block thứ hai. Đối với CNN/DailyMail, 50% thành phần tần số bị truncate, trong khi đối với ELI5 70% bị truncate vì nó có độ dài chuỗi dài hơn nhiều.

Fourier-BART-FP có cùng thiết lập với Fourier-BART, ngoại trừ việc trước khi fine-tuning trên các tác vụ downstream, nó được further pretrained trong 1 epoch trên 10GB văn bản với các mục tiêu pretraining BART gốc. Văn bản được cắt ngẫu nhiên từ corpus Pile (Gao et al., 2020).

5.2.2 Hiệu suất & Hiệu quả
CNN/DailyMail Trên tác vụ tóm tắt, trong phạm vi các mô hình hiệu quả, chúng tôi so sánh mô hình với BigBird (Zaheer et al., 2020), ST-MoE (Zoph et al., 2022) và Switch Transformer (Fedus et al., 2021), là các baseline mạnh từ tài liệu gần đây. Cả ST-MoE và Switch Transformer đều nhắm vào việc kích hoạt chỉ một phần tham số để cải thiện hiệu quả. Bigbird xấp xỉ ma trận attention đầy đủ với một ma trận thưa để cải thiện FLOPs. Ngoài ra, chúng tôi đặt hiệu suất BART tiêu chuẩn (Lewis et al., 2019) làm baseline.

Kết quả được liệt kê trong Bảng 3. Fourier-BART được đề xuất của chúng tôi kế thừa thành công lợi thế của BART, đạt được hiệu suất ở mức mô hình pretrained. Với lượng nhỏ further pretraining, nó đạt được hiệu suất tốt nhất trong tất cả các đối thủ cạnh tranh. Lưu ý rằng Fourier-BART được xây dựng dựa trên BART và chia sẻ cùng kích thước mô hình với BART-400M với ít tính toán hơn nhiều, tuy nhiên nó có thể vượt trội hơn BART-400M tiêu chuẩn với một biên độ hợp lý.

Về hiệu quả, gần như không thể tái tạo tất cả các mô hình được liệt kê trong Bảng 3 và điều tra hiệu quả của chúng, vì vậy chúng tôi chọn chỉ đánh giá BART-400M tiêu chuẩn và Fourier-BART-400M được đề xuất theo FLOPs. Như được làm rõ trong Phần 5.2.1, chúng tôi loại bỏ 50% từ chuỗi ẩn trên lớp transformer thứ ba, mặc dù hai mô hình có chính xác cùng kích thước, FLOPs đầu tư trong BART-400M tiêu chuẩn gấp 1.6 lần Fourier-BART-400M. Do upsampling và giải mã tự động hồi quy, việc giảm tổng thể

--- TRANG 8 ---
Mô hình | R-1 | R-2 | R-L
BART-400M | 44.16 | 21.28 | 40.90
ST-MOE-L-770M | - | 20.7 | -
Switch Trans.-223M | - | 19.6 | -
BigBird-Large | 43.84 | 21.11 | 40.74
Fourier-BART-400M | 44.65 | 21.48 | 41.30
Fourier-BART-FP-400M | 44.76 | 21.55 | 41.34

Bảng 3: Điểm Rouge trên CNN/DailyMail. Kết quả đều được lấy từ các bài báo tương ứng của chúng. R-1 và R-L của ST-MOE và Switch Transformer không được báo cáo trong bài báo của chúng. Số sau tên mô hình biểu thị kích thước mô hình. Kích thước mô hình cho BigBird không may không được đề cập trong bài báo của chúng.

Mô hình | RL | F1
LayerDrop-240M | 23.4 | -
E-MCA-240M | 24.0 | -
c-REALM*-596M | 23.2 | 22.9
EMAT*-446M | 20.91 | 19.03
KID*-406M | 26.3 | -
BART-large-400M | 26.8 | 26.6
Fourier-BART-400M | 26.2 | 25.98
Fourier-BART-FP-400M | 26.9 | 26.73

Bảng 4: Hiệu suất mô hình trên ELI5. Kết quả từ E-MCA đến KID được lấy từ các bài báo tương ứng của chúng. * biểu thị kết quả sử dụng benchmark Kilt (Petroni et al., 2020), có tập dev và test nhỏ hơn.

trong tính toán không đáng kể như những trên LRA.

ELI5 Trên tác vụ trả lời câu hỏi, chúng tôi so sánh mô hình với LayerDrop (Fan et al., 2019b), E-MCA (Fan et al., 2019a), c-REALMS (Krishna et al., 2021), EMAT (Wu et al., 2022) và KID (Liu et al., 2022). Để cung cấp so sánh công bằng, kết quả của BART-large là kết quả được tái tạo của chúng tôi trên phiên bản bleeding-edge của fairseq (Ott et al., 2019), cao hơn nhiều so với kết quả được báo cáo trong bài báo BART gốc. Lưu ý rằng ở đây chúng tôi thậm chí đang so sánh với các mô hình nhạy cảm về hiệu suất, vì trong danh sách chỉ EMAT và LayerDrop tập trung vào việc giảm độ phức tạp. Như được hiển thị trong Bảng 4, Fourier-BART-FP của chúng tôi đã vượt qua tất cả các mô hình cạnh tranh trên cả điểm Rouge-L và F1.

Về hiệu quả, khi loại bỏ 70% thành phần tần số (được làm rõ trong Phần 5.2.1), FLOPs đầu tư trong BART tiêu chuẩn gấp 1.9 lần Fourier-BART.

10% 30% 50% 70% 90% 100%
Tỉ lệ Giữ lại23.023.524.024.525.025.526.026.5GiátrịRL
F1Hình 3: R1, R2, RL và F1 trên ELI5. trục x biểu thị tỉ lệ giữ lại r.

5.3 Phân tích về Tỉ lệ Giữ lại r
Một câu hỏi quan trọng nảy sinh là mô hình nhạy cảm như thế nào w.r.t. tỉ lệ giữ lại các thành phần tần số. Để điều tra điều này, chúng tôi thử nghiệm mô hình trong tập dữ liệu ELI5 bằng cách quét r từ 0.1 đến 1. Chúng tôi không thực hiện further pretraining trên mỗi thiết lập do giới hạn tính toán. Kết quả được hiển thị trong Hình 3. Hiệu suất vẫn khá tốt cho đến khi ít hơn 30% thành phần tần số được giữ lại. Khi chúng tôi cố gắng truncate nhiều thành phần hơn vượt qua tỉ lệ đó, hiệu suất bắt đầu giảm đáng kể. Đây là một kết quả khá hài lòng cho thấy mô hình hoạt động ổn định một cách đáng tin cậy trong một phạm vi rộng của các r hợp lý.

6 Kết luận
Trong công trình này, chúng tôi giới thiệu discrete cosine transformation để downsample dần các trạng thái ẩn trong mô hình Transformer bằng cách tận dụng các tương quan cục bộ giữa các trạng thái ẩn trong các lớp trên. Phương pháp của chúng tôi có thể giảm đáng kể tính toán yêu cầu bởi vanilla Transformer, trong khi có thể đạt được hiệu suất thậm chí tốt hơn trong các tác vụ khác nhau. Hơn nữa, nó có thể kế thừa trọng số mô hình pretrained, đây là một lợi thế đáng chú ý so với hầu hết các Transformers hiệu quả.

7 Hạn chế
Mặc dù phương pháp của chúng tôi thể hiện tăng tốc tuyệt vời trong các thiết lập chỉ encoder, nó không mang lại tăng tốc ấn tượng như vậy trong thiết lập encoder-decoder. Điều này là do các bước giải mã tự động hồi quy trong decoder, phải được thực hiện tuần tự. Tăng tốc điều đó với DCT yêu cầu cập nhật tăng dần đầu ra DCT từng bước dựa trên đầu ra của các timesteps trước đó, điều này về mặt lý thuyết có thể nhưng không dễ tối ưu hóa hiệu quả của nó. Chúng tôi dự định tăng tốc thêm theo hướng này trong công việc tương lai.

Lời cảm ơn
Công trình này được tài trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) grant (No. 62106143), và Chương trình Pujiang Thượng Hải (No. 21PJ1405700).

Tài liệu tham khảo
Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, và Ilya Sutskever. 2020. Generative pretraining from pixels. In International conference on machine learning, pages 1691–1703. PMLR.

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, et al. 2020a. Masked language modeling for proteins via linearly scalable long-context transformers. arXiv preprint arXiv:2006.03555.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020b. Rethinking attention with performers. arXiv preprint arXiv:2009.14794.

Zihang Dai, Guokun Lai, Yiming Yang, và Quoc Le. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271–4282.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Angela Fan, Claire Gardent, Chloé Braud, và Antoine Bordes. 2019a. Using local knowledge graph construction to scale seq2seq models to multi-document inputs. arXiv preprint arXiv:1910.08435.

Angela Fan, Edouard Grave, và Armand Joulin. 2019b. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556.

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, và Michael Auli. 2019c. ELI5: long form question answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3558–3567. Association for Computational Linguistics.

William Fedus, Barret Zoph, và Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, và Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.

Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, và Tim Salimans. 2019. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180.

Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, và Joao Carreira. 2021. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651–4664. PMLR.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156–5165. PMLR.

Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451.

Kalpesh Krishna, Aurko Roy, và Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. arXiv preprint arXiv:2103.06332.

James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, và Santiago Ontanon. 2021. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, và Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.

Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81.

--- TRANG 10 ---
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, và Yoshua Bengio. 2017. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130.

Ruibo Liu, Guoqing Zheng, Shashank Gupta, Radhika Gaonkar, Chongyang Gao, Soroush Vosoughi, Milad Shokouhi, và Ahmed Hassan Awadallah. 2022. Knowledge infused decoding. arXiv preprint arXiv:2204.03084.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, và Luke Zettlemoyer. 2021. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441–2453.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, và Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations.

Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, và Lingpeng Kong. 2021. Random feature attention. arXiv preprint arXiv:2103.02143.

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2020. Kilt: a benchmark for knowledge intensive language tasks. arXiv preprint arXiv:2009.02252.

Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, và Jie Tang. 2020. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555–2565.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68.

Carmelo Scribano, Giorgia Franchini, Marco Prato, và Marko Bertogna. 2022. Dct-former: Efficient self-attention with discrete cosine transform. arXiv preprint arXiv:2203.01178.

Sainbayar Sukhbaatar, Édouard Grave, Piotr Bojanowski, và Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335.

Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, và Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In International conference on machine learning, pages 10183–10192. PMLR.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. 2020a. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006.

Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. 2020b. Efficient transformers: A survey. ACM Computing Surveys (CSUR).

Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, và Donald Metzler. 2021b. Charformer: Fast character transformers via gradient-based subword tokenization. arXiv preprint arXiv:2106.12672.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, và Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.

Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, và Pascale Fung. 2020. Lightweight and efficient end-to-end speech recognition using low-rank transformer. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6144–6148. IEEE.

Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, và Michael Auli. 2019. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430.

Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, và Sebastian Riedel. 2022. An efficient memory-augmented transformer for knowledge-intensive nlp tasks. arXiv preprint arXiv:2210.16773.

Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, và Vikas Singh. 2021. Nyströmformer: A nyström-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14138–14148.

--- TRANG 11 ---
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297.

Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, và Bryan Catanzaro. 2021. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34:17723–17736.

Yimeng Zhuang, Jing Zhang, và Mei Tu. 2022. Long-range sequence modeling with predictable sparse attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 234–243.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, và William Fedus. 2022. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906.
