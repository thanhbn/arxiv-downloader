# 1905.10650.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1905.10650.pdf
# Kích thước tệp: 576411 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Liệu Mười Sáu Đầu Có Thực Sự Tốt Hơn Một Đầu?
Paul Michel
Viện Công Nghệ Ngôn Ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
pmichel1@cs.cmu.eduOmer Levy
Nghiên cứu Trí tuệ Nhân tạo Facebook
Seattle, WA
omerlevy@fb.com

Graham Neubig
Viện Công Nghệ Ngôn Ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
gneubig@cs.cmu.edu

Tóm tắt
Attention là một cơ chế mạnh mẽ và phổ biến cho phép các mô hình neural tập trung vào những phần thông tin nổi bật cụ thể bằng cách lấy trung bình có trọng số khi đưa ra dự đoán. Cụ thể, multi-headed attention là động lực thúc đẩy nhiều mô hình xử lý ngôn ngữ tự nhiên (NLP) tiên tiến gần đây như các mô hình MT dựa trên Transformer và BERT. Những mô hình này áp dụng nhiều cơ chế attention song song, với mỗi "head" attention có thể tập trung vào các phần khác nhau của đầu vào, điều này giúp có thể biểu diễn các hàm phức tạp vượt ra ngoài trung bình có trọng số đơn giản. Trong bài báo này, chúng tôi đưa ra quan sát bất ngờ rằng ngay cả khi các mô hình đã được huấn luyện sử dụng nhiều head, trong thực tế, một tỷ lệ lớn các attention head có thể bị loại bỏ tại thời điểm kiểm tra mà không ảnh hưởng đáng kể đến hiệu suất. Thực tế, một số lớp thậm chí có thể được giảm xuống còn một head duy nhất. Chúng tôi tiếp tục kiểm tra các thuật toán tham lam để cắt tỉa mô hình, và các cải thiện tiềm năng về tốc độ, hiệu quả bộ nhớ và độ chính xác có thể đạt được. Cuối cùng, chúng tôi phân tích các kết quả liên quan đến việc những phần nào của mô hình phụ thuộc nhiều hơn vào việc có nhiều head, và cung cấp bằng chứng sơ bộ rằng động lực huấn luyện đóng vai trò trong những lợi ích được cung cấp bởi multi-head attention¹.

1 Giới thiệu
Transformer (Vaswani et al., 2017) đã cho thấy hiệu suất tốt nhất trên nhiều tác vụ NLP, bao gồm nhưng không giới hạn ở dịch máy (Vaswani et al., 2017; Ott et al., 2018), trả lời câu hỏi (Devlin et al., 2018), phân loại văn bản (Radford et al., 2018), và gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018). Trọng tâm trong những cải tiến kiến trúc, Transformer mở rộng cơ chế attention tiêu chuẩn (Bahdanau et al., 2015; Cho et al., 2014) thông qua multi-headed attention (MHA), nơi attention được tính toán độc lập bởi Nh cơ chế attention song song (head). Đã được chứng minh rằng ngoài việc cải thiện hiệu suất, MHA có thể giúp với sự đồng thuận chủ ngữ-động từ (Tang et al., 2018) và một số head có thể dự đoán cấu trúc phụ thuộc (Raganato và Tiedemann, 2018). Kể từ đó, một số mở rộng cho phương pháp chung đã được đề xuất (Ahmed et al., 2017; Shen et al., 2018).

¹Mã để tái tạo các thí nghiệm của chúng tôi được cung cấp tại https://github.com/pmichel31415/are-16-heads-really-better-than-1

Hội nghị lần thứ 33 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2019), Vancouver, Canada.arXiv:1905.10650v3 [cs.CL] 4 Nov 2019

--- TRANG 2 ---
Tuy nhiên, vẫn chưa hoàn toàn rõ ràng: nhiều head trong những mô hình này mang lại cho chúng ta điều gì? Trong bài báo này, chúng tôi đưa ra quan sát bất ngờ rằng – trong cả mô hình dựa trên Transformer cho dịch máy và BERT (Devlin et al., 2018) suy luận ngôn ngữ tự nhiên – hầu hết các attention head có thể được loại bỏ riêng lẻ sau khi huấn luyện mà không có bất kỳ tác động tiêu cực đáng kể nào về hiệu suất kiểm tra (§3.2). Đáng chú ý, nhiều lớp attention thậm chí có thể được giảm xuống riêng lẻ thành một attention head duy nhất mà không ảnh hưởng đến hiệu suất kiểm tra (§3.3).

Dựa trên quan sát này, chúng tôi tiếp tục đề xuất một thuật toán đơn giản cắt tỉa tham lam và lặp lại các attention head mà có vẻ đóng góp ít hơn cho mô hình. Bằng cách loại bỏ cùng lúc các attention head từ toàn bộ mạng, mà không hạn chế việc cắt tỉa ở một lớp duy nhất, chúng tôi thấy rằng các phần lớn của mạng có thể được loại bỏ với ít hoặc không có hậu quả, nhưng phần lớn các head phải được giữ lại để tránh sự sụt giảm hiệu suất thảm khốc (§4). Chúng tôi cũng thấy rằng điều này có lợi ích đáng kể cho hiệu quả thời gian suy luận, dẫn đến tăng tốc độ suy luận lên đến 17,5% cho mô hình dựa trên BERT.

Sau đó chúng tôi đi sâu vào phân tích thêm. Một cái nhìn gần hơn về trường hợp dịch máy cho thấy rằng các lớp attention encoder-decoder đặc biệt nhạy cảm với việc cắt tỉa, nhiều hơn so với các lớp self-attention, cho thấy rằng multi-headedness đóng vai trò quan trọng trong thành phần này (§5). Cuối cùng, chúng tôi cung cấp bằng chứng rằng sự khác biệt giữa các head quan trọng và không quan trọng tăng lên khi quá trình huấn luyện tiến triển, cho thấy sự tương tác giữa multi-headedness và động lực huấn luyện (§6).

2 Nền tảng: Attention, Multi-headed Attention, và Masking
Trong phần này, chúng tôi đặt ra nền tảng ký hiệu về attention, và cũng mô tả phương pháp của chúng tôi để che (mask) các attention head.

2.1 Single-headed Attention
Chúng tôi nhắc lại ngắn gọn cách vanilla attention hoạt động. Chúng tôi tập trung vào scaled bilinear attention (Luong et al., 2015), biến thể được sử dụng phổ biến nhất trong các lớp MHA. Cho một chuỗi n vector d-chiều x=x₁;:::;xₙ∈Rᵈ, và một vector truy vấn q∈Rᵈ, lớp attention được tham số hóa bởi Wₖ;Wq;Wᵥ;Wₒ∈Rᵈˣᵈ tính toán tổng có trọng số:

Att_{Wₖ;Wq;Wᵥ;Wₒ}(x;q) = Wₒ∑ᵢ₌₁ⁿ αᵢWᵥxᵢ

trong đó αᵢ = softmax(q^T W_q^T Wₖxᵢ/√d)

Trong self-attention, mỗi xᵢ được sử dụng như truy vấn q để tính toán một chuỗi biểu diễn mới, trong khi ở các mô hình sequence-to-sequence, q thường là trạng thái decoder trong khi x tương ứng với đầu ra encoder.

2.2 Multi-headed Attention
Trong multi-headed attention (MHA), Nₕ lớp attention được tham số hóa độc lập được áp dụng song song để có được kết quả cuối cùng:

MHAtt(x;q) = ∑ₕ₌₁^{Nₕ} Att_{Wₖʰ;Wqʰ;Wᵥʰ;Wₒʰ}(x;q)  (1)

trong đó Wₖʰ;Wqʰ;Wᵥʰ∈Rᵈʰˣᵈ và Wₒʰ∈Rᵈˣᵈʰ. Khi dₕ=d, MHA có tính biểu diễn chặt chẽ hơn vanilla attention. Tuy nhiên, để giữ số tham số không đổi, dₕ thường được đặt thành d/Nₕ, trong trường hợp này MHA có thể được xem như một ensemble của các lớp vanilla attention có hạng thấp². Trong phần tiếp theo, chúng tôi sử dụng Attₕ(x) như một cách viết tắt cho đầu ra của head h trên đầu vào x.

Để cho phép các attention head khác nhau tương tác với nhau, transformer áp dụng một mạng feed-forward phi tuyến trên đầu ra của MHA, tại mỗi lớp transformer (Vaswani et al., 2017).

²Ký hiệu này, tương đương với công thức "concatenation" từ Vaswani et al. (2017), được sử dụng để dễ dàng giải thích trong các phần tiếp theo.

--- TRANG 3 ---
35.5 35.6 35.7 35.8 35.9 36.0 36.1 36.2 36.3
BLEU1020304050607080#headsBaseline
BLEU(a) WMT

0.824 0.826 0.828 0.830 0.832 0.834 0.836 0.838
Accuracy10203040#headsBaseline
accuracy (b) BERT

Hình 1: Phân phối của các head theo điểm mô hình sau khi masking.

2.3 Masking Attention Heads
Để thực hiện các thí nghiệm ablation trên các head, chúng tôi sửa đổi công thức cho MHAtt:

MHAtt(x;q) = ∑ₕ₌₁^{Nₕ} ξₕAtt_{Wₖʰ;Wqʰ;Wᵥʰ;Wₒʰ}(x;q)

trong đó ξₕ là các biến mask với giá trị trong {0;1}. Khi tất cả ξₕ bằng 1, điều này tương đương với công thức trong Phương trình 1. Để mask head h, chúng tôi chỉ cần đặt ξₕ = 0.

3 Tất Cả Attention Head Có Quan Trọng Không?
Chúng tôi thực hiện một chuỗi thí nghiệm trong đó chúng tôi loại bỏ một hoặc nhiều attention head từ một kiến trúc cho trước tại thời điểm kiểm tra, và đo lường sự khác biệt hiệu suất. Trước tiên chúng tôi loại bỏ một attention head duy nhất tại mỗi thời điểm (§3.2) và sau đó loại bỏ mọi head trong toàn bộ lớp trừ một head (§3.3).

3.1 Thiết lập Thí nghiệm
Trong tất cả các thí nghiệm sau, chúng tôi xem xét hai mô hình đã được huấn luyện:

WMT Đây là kiến trúc transformer "large" gốc từ Vaswani et al. 2017 với 6 lớp và 16 head trên mỗi lớp, được huấn luyện trên corpus WMT2014 English to French. Chúng tôi sử dụng mô hình pretrained của Ott et al. 2018³ và báo cáo điểm BLEU trên tập kiểm tra newstest2013. Phù hợp với Ott et al. 2018, chúng tôi tính điểm BLEU trên đầu ra đã được tokenize của mô hình sử dụng Moses (Koehn et al., 2007). Ý nghĩa thống kê được kiểm tra với paired bootstrap resampling (Koehn, 2004) sử dụng compare-mt⁴ (Neubig et al., 2019) với 1000 lần resample. Một đặc điểm riêng của mô hình này là nó có 3 cơ chế attention riêng biệt: encoder self-attention (Enc-Enc), encoder-decoder attention (Enc-Dec) và decoder self-attention (Dec-Dec), tất cả đều sử dụng MHA.

BERT BERT (Devlin et al., 2018) là một transformer đơn được pre-train trên tác vụ "masked language modeling" không giám sát kiểu cloze-style và sau đó được fine-tune trên các tác vụ cụ thể. Tại thời điểm ra đời, nó đạt được hiệu suất tốt nhất trên nhiều tác vụ NLP. Chúng tôi sử dụng mô hình pretrained base-uncased của Devlin et al. 2018 với 12 lớp và 12 attention head mà chúng tôi fine-tune và đánh giá trên MultiNLI (Williams et al., 2018). Chúng tôi báo cáo độ chính xác trên tập validation "matched". Chúng tôi kiểm tra ý nghĩa thống kê sử dụng t-test. Trái ngược với mô hình WMT, BERT chỉ có một cơ chế attention (self-attention trong mỗi lớp).

3.2 Ablating One Head
Để hiểu đóng góp của một attention head cụ thể h, chúng tôi đánh giá hiệu suất của mô hình trong khi mask head đó (tức là thay thế Attₕ(x) bằng không). Nếu hiệu suất không có h kém hơn đáng kể so với mô hình đầy đủ, h rõ ràng là quan trọng; nếu hiệu suất tương đương, h thừa so với phần còn lại của mô hình.

Hình 1a và 1b cho thấy phân phối của các head theo điểm mô hình sau khi mask nó, cho WMT và BERT tương ứng. Chúng tôi quan sát rằng phần lớn các attention head có thể được loại bỏ mà không lệch quá nhiều so với điểm ban đầu. Đáng ngạc nhiên, trong một số trường hợp việc loại bỏ một attention head dẫn đến sự gia tăng BLEU/accuracy.

³https://github.com/pytorch/fairseq/tree/master/examples/translation
⁴https://github.com/neulab/compare-mt

--- TRANG 4 ---
LayerHead1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
1 0.03 0.07 0.05 -0.06 0.03 -0.53 0.09 -0.33 0.06 0.03 0.11 0.04 0.01 -0.04 0.04 0.00
2 0.01 0.04 0.10 0.20 0.06 0.03 0.00 0.09 0.10 0.04 0.15 0.03 0.05 0.04 0.14 0.04
3 0.05 -0.01 0.08 0.09 0.11 0.02 0.03 0.03 -0.00 0.13 0.09 0.09 -0.11 0.24 0.07 -0.04
4 -0.02 0.03 0.13 0.06 -0.05 0.13 0.14 0.05 0.02 0.14 0.05 0.06 0.03 -0.06 -0.10 -0.06
5 -0.31 -0.11 -0.04 0.12 0.10 0.02 0.09 0.08 0.04 0.21 -0.02 0.02 -0.03 -0.04 0.07 -0.02
6 0.06 0.07 -0.31 0.15 -0.19 0.15 0.11 0.05 0.01 -0.08 0.06 0.01 0.01 0.02 0.07 0.05

Bảng 1: Sự khác biệt về điểm BLEU cho mỗi head của cơ chế self attention của encoder. Các số được gạch chân chỉ ra rằng thay đổi có ý nghĩa thống kê với p < 0.01. Điểm BLEU cơ sở là 36.05.

Layer Enc-Enc Enc-Dec Dec-Dec
1 -1.31 0.24 -0.03
2 -0.16 0.06 0.12
3 0.12 0.05 0.18
4 -0.15 -0.24 0.17
5 0.02 -1.55 -0.04
6 -0.36 -13.56 0.24

Bảng 2: Delta BLEU tốt nhất theo lớp khi chỉ một head được giữ lại trong mô hình WMT. Các số được gạch chân chỉ ra rằng thay đổi có ý nghĩa thống kê với p < 0.01.

Layer Layer
1 -0.01% 7 0.05%
2 0.10% 8 -0.72%
3 -0.14% 9 -0.96%
4 -0.53% 10 0.07%
5 -0.29% 11 -0.19%
6 -0.52% 12 -0.12%

Bảng 3: Delta accuracy tốt nhất theo lớp khi chỉ một head được giữ lại trong mô hình BERT. Không có kết quả nào trong số này có ý nghĩa thống kê với p < 0.01.

Để có cái nhìn chi tiết hơn về những kết quả này, chúng tôi phóng to vào các lớp encoder self-attention của mô hình WMT trong Bảng 1. Đáng chú ý, chúng tôi thấy rằng chỉ 8 (trong số 96) head gây ra thay đổi có ý nghĩa thống kê về hiệu suất khi chúng được loại bỏ khỏi mô hình, một nửa trong số đó thực sự dẫn đến điểm BLEU cao hơn. Điều này dẫn chúng tôi đến quan sát đầu tiên: tại thời điểm kiểm tra, hầu hết các head là thừa so với phần còn lại của mô hình.

3.3 Ablating All Heads but One
Quan sát này dẫn đến câu hỏi: liệu có cần nhiều hơn một head? Do đó, chúng tôi tính toán sự khác biệt về hiệu suất khi tất cả các head trừ một được loại bỏ, trong một lớp duy nhất. Trong Bảng 2 và Bảng 3, chúng tôi báo cáo điểm tốt nhất cho mỗi lớp trong mô hình, tức là điểm khi giảm toàn bộ lớp xuống head quan trọng nhất duy nhất.

Chúng tôi thấy rằng, đối với hầu hết các lớp, một head thực sự đủ tại thời điểm kiểm tra, mặc dù mạng đã được huấn luyện với 12 hoặc 16 attention head. Điều này đáng chú ý vì những lớp này có thể được giảm xuống single-headed attention chỉ với 1/16 (tương ứng 1/12) số tham số của một lớp vanilla attention. Tuy nhiên, một số lớp đòi hỏi nhiều attention head; ví dụ, việc thay thế lớp cuối cùng trong encoder-decoder attention của WMT bằng một head duy nhất làm giảm hiệu suất ít nhất 13.5 điểm BLEU. Chúng tôi phân tích thêm về khi nào các thành phần mô hình khác nhau phụ thuộc vào nhiều head hơn trong §5.

Ngoài ra, chúng tôi xác minh rằng kết quả này vẫn đúng ngay cả khi chúng tôi không có quyền truy cập vào tập đánh giá khi chọn head "tốt nhất riêng lẻ". Với mục đích này, chúng tôi chọn head tốt nhất cho mỗi lớp trên tập validation (newstest2013 cho WMT và một tập con 5,000 được chọn ngẫu nhiên từ tập huấn luyện của MNLI cho BERT) và đánh giá hiệu suất của mô hình trên tập kiểm tra

--- TRANG 5 ---
35.5 35.6 35.7 35.8 35.9 36.0 36.1 36.2 36.3
BLEU on newstest201330.531.031.532.032.533.0BLEU on MTNT

Head ablation scores
(Pearson r=0.56)
Original BLEU scores(a) BLEU trên newstest2013 và MTNT khi các head riêng lẻ được loại bỏ khỏi WMT. Lưu ý rằng các phạm vi trên trục X và Y không giống nhau vì có vẻ như có nhiều biến động hơn trên MTNT.

0.823 0.825 0.828 0.830 0.833 0.835 0.838 0.840
Accuracy on MNLI-matched0.8250.8280.8300.8330.8350.8380.8400.843Accuracy on MNLI-mismatched

Head ablation accuracies
(Pearson r=0.68)
Original accuracies(b) Độ chính xác trên MNLI-matched và -mismatched khi các head riêng lẻ được loại bỏ khỏi BERT. Ở đây các điểm vẫn nằm trong cùng phạm vi giá trị gần đúng.

Hình 2: Phân tích cross-task về tác động của pruning lên độ chính xác

(newstest2014 cho WMT và tập validation MNLI-matched cho BERT). Chúng tôi quan sát rằng những phát hiện tương tự vẫn đúng: chỉ giữ lại một head không dẫn đến thay đổi có ý nghĩa thống kê về hiệu suất cho 50% (tương ứng 100%) các lớp của WMT (tương ứng BERT). Kết quả chi tiết có thể được tìm thấy trong Phụ lục A.

3.4 Các Head Quan Trọng Có Giống Nhau Trên Các Bộ Dữ Liệu Không?
Có một cạm bẫy trong hai thí nghiệm trước của chúng tôi: những kết quả này chỉ có giá trị trên các tập kiểm tra cụ thể (và khá nhỏ), gây nghi ngờ về khả năng tổng quát hóa của chúng lên các bộ dữ liệu khác. Như bước đầu tiên để hiểu liệu một số head có quan trọng phổ quát hay không, chúng tôi thực hiện cùng một nghiên cứu ablation trên tập kiểm tra thứ hai, ngoài domain. Cụ thể, chúng tôi xem xét tập validation "mismatched" của MNLI cho BERT và tập kiểm tra MTNT English to French (Michel và Neubig, 2018) cho mô hình WMT, cả hai đều được tập hợp với mục đích cung cấp các bộ kiểm tra tương phản, ngoài domain cho các tác vụ tương ứng của chúng.

Chúng tôi thực hiện cùng một nghiên cứu ablation như §3.2 trên mỗi bộ dữ liệu này và báo cáo kết quả trong Hình 2a và 2b. Chúng tôi nhận thấy rằng có một mối tương quan dương, >0.5 (p < 0.01) giữa tác động của việc loại bỏ một head trên cả hai bộ dữ liệu. Hơn nữa, các head có tác động cao nhất lên hiệu suất trên một domain có xu hướng có cùng tác động trên domain khác, điều này cho thấy rằng những head quan trọng nhất từ §3.2 thực sự là "phổ quát" quan trọng.

4 Cắt Tỉa Lặp Lại Các Attention Head
Trong các thí nghiệm ablation của chúng tôi (§3.2 và §3.3), chúng tôi quan sát tác động của việc loại bỏ một hoặc nhiều head trong một lớp duy nhất, mà không xem xét điều gì sẽ xảy ra nếu chúng tôi thay đổi hai hoặc nhiều lớp khác nhau cùng một lúc. Để kiểm tra tác động tổng hợp của việc cắt tỉa nhiều head từ toàn bộ mô hình, chúng tôi sắp xếp tất cả các attention head trong mô hình theo điểm quan trọng proxy (được mô tả bên dưới), và sau đó loại bỏ các head từng cái một. Chúng tôi sử dụng phương pháp lặp lại, heuristic này để tránh tìm kiếm tổ hợp, điều này không thực tế do số lượng head và thời gian cần thiết để đánh giá mỗi mô hình.

--- TRANG 6 ---
0% 20% 40% 60% 80% 100%
Percentage pruned05101520253035BLEU

(a) Sự tiến triển của điểm BLEU trên newstest2013 khi các head được cắt tỉa từ WMT.

0% 20% 40% 60% 80% 100%
Percentage pruned0.00.20.40.60.8Accuracy

(b) Sự tiến triển của độ chính xác trên tập validation MultiNLI-matched khi các head được cắt tỉa từ BERT.

Hình 3: Sự tiến triển của độ chính xác theo số lượng head được cắt tỉa theo Ih (đường xanh liền nét) và hiệu suất oracle cá nhân (đường xanh lá đứt nét).

4.1 Điểm Quan Trọng Head để Cắt Tỉa
Như một điểm proxy cho tầm quan trọng của head, chúng tôi xem xét độ nhạy kỳ vọng của mô hình đối với các biến mask ξh được định nghĩa trong §2.3:

Ih = Ex∈X |∂L(x)/∂ξh|  (2)

trong đó X là phân phối dữ liệu và L(x) là loss trên mẫu x. Trực quan, nếu Ih có giá trị cao thì việc thay đổi ξh có khả năng có tác động lớn lên mô hình. Cụ thể, chúng tôi thấy giá trị tuyệt đối là quan trọng để tránh các điểm dữ liệu có đóng góp âm hoặc dương cao triệt tiêu lẫn nhau trong tổng. Thay Phương trình 1 vào Phương trình 2 và áp dụng quy tắc chuỗi cho ra biểu thức cuối cùng sau cho Ih:

Ih = Ex∈X |Atth(x)T ∂L(x)/∂Atth(x)|

Công thức này gợi nhớ đến kho tàng tài liệu phong phú về cắt tỉa mạng neural (LeCun et al., 1990; Hassibi và Stork, 1993; Molchanov et al., 2017, trong số những người khác). Cụ thể, nó tương đương với phương pháp Taylor expansion từ Molchanov et al. (2017).

Về mặt hiệu suất, việc ước tính Ih chỉ yêu cầu thực hiện một lần forward và backward pass, và do đó không chậm hơn so với huấn luyện. Trong thực tế, chúng tôi tính toán kỳ vọng trên dữ liệu huấn luyện hoặc một tập con của nó⁵. Như được khuyến nghị bởi Molchanov et al. (2017), chúng tôi chuẩn hóa các điểm quan trọng theo lớp (sử dụng chuẩn ℓ2).

4.2 Tác Động của Cắt Tỉa lên BLEU/Accuracy
Hình 3a (cho WMT) và 3b (cho BERT) mô tả tác động của việc cắt tỉa attention-head lên hiệu suất mô hình trong khi loại bỏ dần 10% tổng số head theo thứ tự tăng dần của Ih tại mỗi bước. Chúng tôi cũng báo cáo kết quả khi thứ tự cắt tỉa được xác định bởi sự khác biệt điểm từ §3.2 (trong đường đứt nét), nhưng thấy rằng sử dụng Ih nhanh hơn và cho kết quả tốt hơn.

Chúng tôi quan sát rằng phương pháp này cho phép chúng tôi cắt tỉa lên đến 20% và 40% head từ WMT và BERT (tương ứng), mà không gây ra bất kỳ tác động tiêu cực đáng chú ý nào. Hiệu suất giảm mạnh khi cắt tỉa thêm, có nghĩa là cả hai mô hình đều không thể được giảm xuống thành mô hình attention hoàn toàn single-head mà không cần huấn luyện lại hoặc gánh chịu những tổn thất đáng kể về hiệu suất. Chúng tôi tham khảo Phụ lục B cho các thí nghiệm trên bốn bộ dữ liệu bổ sung.

⁵Đối với mô hình WMT, chúng tôi sử dụng tất cả các tập newstest20[09-12] để ước tính I.

--- TRANG 7 ---
4.3 Tác Động của Cắt Tỉa lên Hiệu Quả
Ngoài hiệu suất tác vụ downstream, có những lợi thế nội tại của việc cắt tỉa head. Đầu tiên, mỗi head đại diện cho một tỷ lệ không nhỏ của tổng tham số trong mỗi lớp attention (6.25% cho WMT, 8.34% cho BERT), và do đó của tổng mô hình (nói chung, trong cả hai mô hình của chúng tôi, khoảng một phần ba của tổng số tham số được dành cho MHA trên tất cả các lớp)⁶. Điều này hấp dẫn trong bối cảnh triển khai mô hình trong các môi trường hạn chế bộ nhớ.

Batch size 1 4 16 64
Original 17.0±0.3 67.3±1.3 114.0±3.6 124.7±2.9
Pruned (50%) 17.3±0.6 69.1±1.3 134.0±3.6 146.6±3.4
(+1.9%) (+2.7%) (+17.5%) (+17.5%)

Bảng 4: Tốc độ suy luận trung bình của BERT trên tập validation MNLI-matched tính bằng ví dụ trên giây (± độ lệch chuẩn). Tốc độ tăng tương đối so với mô hình gốc được chỉ ra trong ngoặc đơn.

Hơn nữa, chúng tôi thấy rằng việc thực sự cắt tỉa các head (chứ không chỉ masking) dẫn đến sự gia tăng đáng kể về tốc độ suy luận. Bảng 4 báo cáo số lượng ví dụ trên giây được xử lý bởi BERT, trước và sau khi cắt tỉa 50% tất cả attention head. Các thí nghiệm được thực hiện trên hai máy khác nhau, cả hai đều được trang bị GPU GeForce GTX 1080Ti. Mỗi thí nghiệm được lặp lại 3 lần trên mỗi máy (tổng cộng 6 điểm dữ liệu cho mỗi cài đặt). Chúng tôi thấy rằng việc cắt tỉa một nửa số head của mô hình tăng tốc suy luận lên đến 17.5% cho các batch size cao hơn (sự khác biệt này biến mất đối với các batch size nhỏ hơn).

5 Khi Nào Nhiều Head Quan Trọng? Trường Hợp Dịch Máy
Như được thể hiện trong Bảng 2, không phải tất cả các lớp MHA đều có thể được giảm xuống một attention head duy nhất mà không ảnh hưởng đáng kể đến hiệu suất. Để có ý tưởng tốt hơn về mức độ mỗi phần của mô hình dịch dựa trên transformer phụ thuộc vào multi-headedness, chúng tôi lặp lại thí nghiệm cắt tỉa heuristic từ §4 cho mỗi loại attention riêng biệt (Enc-Enc, Enc-Dec, và Dec-Dec).

Hình 4 cho thấy rằng hiệu suất giảm nhanh hơn nhiều khi các head được cắt tỉa từ các lớp attention Enc-Dec. Cụ thể, việc cắt tỉa hơn 60% các attention head Enc-Dec sẽ dẫn đến sự suy giảm hiệu suất thảm khốc, trong khi các lớp encoder và decoder self-attention vẫn có thể tạo ra các bản dịch hợp lý (với điểm BLEU khoảng 30) chỉ với 20% số attention head ban đầu. Nói cách khác, encoder-decoder attention phụ thuộc vào multi-headedness nhiều hơn so với self-attention.

0% 20% 40% 60% 80% 100%
Percentage pruned05101520253035BLEU
Enc-Enc
Enc-Dec
Dec-Dec

Hình 4: BLEU khi cắt tỉa dần các head từ mỗi loại attention trong mô hình WMT.

⁶Hơi nhiều hơn trong WMT vì có attention Enc-Dec.

--- TRANG 8 ---
1
[3.0]2
[6.2]3
[18.1]5
[26.9]10
[30.6]20
[34.5]30
[34.7]40
[34.9]

Epoch
[un-pruned BLEU score]0%20%40%60%80%100%Percentage of un-pruned
model BLEU score

Percentage of
heads pruned
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%0%25%50%75%100%
Percentage pruned0%25%50%75%100%Percentage of original BLEUEpoch 1
Epoch 2
Epoch 35
Epoch 40

Hình 5: Bên trái: mối quan hệ giữa tỷ lệ phần trăm head được cắt tỉa và mức giảm điểm tương đối trong quá trình huấn luyện mô hình IWSLT. Chúng tôi báo cáo các epoch trên thang logarit. Điểm BLEU của mô hình gốc, không cắt tỉa được chỉ ra trong ngoặc vuông. Bên phải: tập trung vào sự khác biệt trong hành vi ở đầu (epoch 1 và 2) và cuối (epoch 35 và 40) quá trình huấn luyện.

6 Động Lực Quan Trọng Head Trong Quá Trình Huấn Luyện
Các phần trước cho chúng tôi biết rằng một số head quan trọng hơn những head khác trong các mô hình đã được huấn luyện. Để có thêm hiểu biết về động lực của tầm quan trọng head trong quá trình huấn luyện, chúng tôi thực hiện cùng một thí nghiệm cắt tỉa dần từ §4.2 tại mỗi epoch. Chúng tôi thực hiện thí nghiệm này trên một phiên bản nhỏ hơn của mô hình WMT (6 lớp và 8 head trên mỗi lớp), được huấn luyện cho dịch German sang English trên bộ dữ liệu IWSLT 2014 nhỏ hơn Cettolo et al. (2015). Chúng tôi gọi mô hình này là IWSLT.

Hình 5 báo cáo, cho mỗi mức cắt tỉa (theo từng đoạn 10% — 0% tương ứng với mô hình gốc), sự tiến triển của điểm mô hình (trên newstest2013) cho mỗi epoch. Để dễ đọc hơn, chúng tôi hiển thị các epoch trên thang logarit, và chỉ báo cáo điểm mỗi 5 epoch sau epoch thứ 10). Để làm cho các điểm có thể so sánh được qua các epoch, trục Y báo cáo sự suy giảm tương đối của điểm BLEU so với mô hình không cắt tỉa tại mỗi epoch. Đáng chú ý, chúng tôi thấy rằng có hai chế độ riêng biệt: trong các epoch rất sớm (đặc biệt là 1 và 2), hiệu suất giảm tuyến tính theo tỷ lệ cắt tỉa, tức là mức giảm hiệu suất tương đối độc lập với Ih, cho thấy rằng hầu hết các head đều ít nhiều quan trọng như nhau. Từ epoch 10 trở đi, có sự tập trung của các head không quan trọng có thể được cắt tỉa trong khi vẫn duy trì trong 85-90% điểm BLEU ban đầu (lên đến 40% tổng số head).

Điều này cho thấy rằng các head quan trọng được xác định sớm (nhưng không ngay lập tức) trong quá trình huấn luyện. Hai giai đoạn huấn luyện gợi nhớ đến phân tích của Shwartz-Ziv và Tishby (2017), theo đó việc huấn luyện mạng neural phân tách thành một giai đoạn "tối thiểu hóa rủi ro thực nghiệm", nơi mô hình tối đa hóa thông tin tương hỗ của các biểu diễn trung gian với nhãn, và một giai đoạn "nén" nơi thông tin tương hỗ với đầu vào được tối thiểu hóa. Một nghiên cứu có nguyên tắc hơn về hiện tượng này được để lại cho công việc tương lai.

7 Công Trình Liên Quan
Việc sử dụng cơ chế attention trong NLP và đặc biệt trong dịch máy neural (NMT) có thể được truy nguyên về Bahdanau et al. (2015) và Cho et al. (2014), và hầu hết các triển khai đương thời đều dựa trên công thức từ Luong et al. (2015). Attention đã được nhanh chóng thích nghi (thành công) cho các tác vụ NLP khác, thường đạt được hiệu suất tốt nhất khi đó trong đọc hiểu (Cheng et al., 2016), suy luận ngôn ngữ tự nhiên (Parikh et al., 2016) hoặc tóm tắt trừu tượng (Paulus et al., 2017) để nêu một vài ví dụ. Multi-headed attention lần đầu tiên được giới thiệu bởi Vaswani et al. (2017) cho NMT và phân tích cú pháp thành phần tiếng Anh, và sau đó được áp dụng cho transfer learning (Radford et al., 2018; Devlin et al., 2018), mô hình hóa ngôn ngữ (Dai et al., 2019; Radford et al., 2019), hoặc gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018), trong số những tác vụ khác.

Có một kho tàng tài liệu phong phú về cắt tỉa mạng neural đã được huấn luyện, có nguồn gốc từ LeCun et al. (1990) và Hassibi và Stork (1993) vào đầu những năm 90 và được tái sinh sau sự ra đời của deep learning, với hai phương pháp trực giao: cắt tỉa mịn "từng trọng số" (Han et al., 2015) và cắt tỉa có cấu trúc (Anwar et al., 2017; Li et al., 2016; Molchanov et al., 2017), trong đó toàn bộ các phần của mô hình được cắt tỉa. Trong NLP, cắt tỉa có cấu trúc để tự động điều chỉnh kích thước các mô hình ngôn ngữ feed-forward lần đầu tiên được nghiên cứu bởi Murray và Chiang (2015). Gần đây hơn, các phương pháp cắt tỉa mịn đã được phổ biến bởi See et al. (2016) và Kim và Rush (2016) (chủ yếu trên NMT).

Đồng thời với công trình của chúng tôi, Voita et al. (2019) đã có quan sát tương tự về multi-head attention. Phương pháp của họ bao gồm việc sử dụng LRP (Binder et al., 2016) để xác định các head quan trọng và xem xét các thuộc tính cụ thể như attend đến các vị trí liền kề, từ hiếm hoặc từ có liên quan cú pháp. Họ đề xuất một cơ chế cắt tỉa thay thế dựa trên việc thực hiện gradient descent trên các biến mask ξh. Mặc dù phương pháp và kết quả của họ bổ sung cho bài báo này, nghiên cứu của chúng tôi cung cấp bằng chứng bổ sung về hiện tượng này ngoài NMT, cũng như phân tích về động lực huấn luyện của việc cắt tỉa attention head.

8 Kết Luận
Chúng tôi đã quan sát thấy rằng MHA không phải lúc nào cũng tận dụng tính biểu diễn vượt trội về mặt lý thuyết so với vanilla attention một cách tối đa. Cụ thể, chúng tôi đã chứng minh rằng trong nhiều thiết lập khác nhau, một số head có thể được loại bỏ khỏi các mô hình transformer đã được huấn luyện mà không suy giảm có ý nghĩa thống kê về hiệu suất kiểm tra, và một số lớp có thể được giảm xuống chỉ còn một head. Ngoài ra, chúng tôi đã chỉ ra rằng trong các mô hình dịch máy, các lớp encoder-decoder attention phụ thuộc vào multi-headedness nhiều hơn so với các lớp self-attention, và cung cấp bằng chứng rằng tầm quan trọng tương đối của mỗi head được xác định trong các giai đoạn đầu của quá trình huấn luyện. Chúng tôi hy vọng rằng những quan sát này sẽ thúc đẩy sự hiểu biết của chúng tôi về MHA và truyền cảm hứng cho các mô hình đầu tư tham số và attention của chúng một cách hiệu quả hơn.

Lời Cảm Ơn
Các tác giả muốn gửi lời cảm ơn đến các reviewer ẩn danh vì những phản hồi sâu sắc của họ. Chúng tôi cũng đặc biệt biết ơn Thomas Wolf từ Hugging Face, người mà những nỗ lực tái tạo độc lập đã cho phép chúng tôi tìm ra và sửa một lỗi trong các thí nghiệm so sánh tốc độ của chúng tôi. Nghiên cứu này được hỗ trợ một phần bởi một món quà từ Facebook.

Tài Liệu Tham Khảo
Karim Ahmed, Nitish Shirish Keskar, và Richard Socher. Weighted transformer network for machine translation. arXiv preprint arXiv:1711.02132, 2017.

Sajid Anwar, Kyuyeon Hwang, và Wonyong Sung. Structured pruning of deep convolutional neural networks. J. Emerg. Technol. Comput. Syst., pages 32:1–32:18, 2017.

Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.

Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, Klaus-Robert Müller, và Wojciech Samek. Layer-wise relevance propagation for neural networks with local renormalization layers. In International Conference on Artificial Neural Networks, pages 63–71, 2016.

Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, và Marcello Federico. Report on the 11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the 2014 International Workshop on Spoken Language Translation (IWSLT), 2015.

Jianpeng Cheng, Li Dong, và Mirella Lapata. Long short-term memory-networks for machine reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 551–561, 2016.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, và Yoshua Bengio. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, 2014.

--- TRANG 10 ---
Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2018.

William B. Dolan và Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the The 3rd International Workshop on Paraphrasing (IWP), 2005. URL http://aclweb.org/anthology/I05-5002.

Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. In Proceedings of the 29th Annual Conference on Neural Information Processing Systems (NIPS), pages 1135–1143, 2015.

Babak Hassibi và David G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Proceedings of the 5th Annual Conference on Neural Information Processing Systems (NIPS), pages 164–171, 1993.

Yoon Kim và Alexander M. Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1317–1327, 2016.

Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395, 2004.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, và Evan Herbst. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 177–180, 2007.

Yann LeCun, John S. Denker, và Sara A. Solla. Optimal brain damage. In Proceedings of the 2nd Annual Conference on Neural Information Processing Systems (NIPS), pages 598–605, 1990.

Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.

Thang Luong, Hieu Pham, và Christopher D. Manning. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1412–1421, 2015.

Paul Michel và Graham Neubig. MTNT: A testbed for machine translation of noisy text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 543–553, 2018.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. Pruning convolutional neural networks for resource efficient inference. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.

Kenton Murray và David Chiang. Auto-sizing neural networks: With applications to n-gram language models. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 908–916, 2015.

Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi, và Xinyi Wang. compare-mt: A tool for holistic comparison of language generation systems. In Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL) Demo Track, Minneapolis, USA, June 2019. URL http://arxiv.org/abs/1903.07926.

Myle Ott, Sergey Edunov, David Grangier, và Michael Auli. Scaling neural machine translation. In Proceedings of the 3rd Conference on Machine Translation (WMT), pages 1–9, 2018.

--- TRANG 11 ---
Ankur Parikh, Oscar Täckström, Dipanjan Das, và Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2249–2255, 2016.

Romain Paulus, Caiming Xiong, và Richard Socher. A deep reinforced model for abstractive summarization. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.

Alec Radford, Karthik Narasimhan, Time Salimans, và Ilya Sutskever. Improving language understanding with unsupervised learning. Technical report, Technical report, OpenAI, 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1:8, 2019.

Alessandro Raganato và Jörg Tiedemann. An analysis of encoder representations in transformer-based machine translation. In Proceedings of the Workshop on BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287–297, 2018.

Abigail See, Minh-Thang Luong, và Christopher D. Manning. Compression of neural machine translation models via pruning. In Proceedings of the Computational Natural Language Learning (CoNLL), pages 291–301, 2016.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, và Chengqi Zhang. Disan: Directional self-attention network for rnn/cnn-free language understanding. In Proceedings of the 32nd Meeting of the Association for Advancement of Artificial Intelligence (AAAI), 2018.

Ravid Shwartz-Ziv và Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, và Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642, 2013.

Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, và Andrew McCallum. Linguistically-informed self-attention for semantic role labeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5027–5038, 2018.

Gongbo Tang, Mathias Müller, Annette Rios, và Rico Sennrich. Why self-attention? a targeted evaluation of neural machine translation architectures. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4263–4272, 2018.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NIPS), pages 5998–6008, 2017.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, và Titov Ivan. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), page to appear, 2019.

Alex Warstadt, Amanpreet Singh, và Samuel R. Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.

Adina Williams, Nikita Nangia, và Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1112–1122, 2018.

A Ablating All Heads but One: Thí Nghiệm Bổ Sung.
Bảng 5 và 6 báo cáo sự khác biệt về hiệu suất khi chỉ một head được giữ lại cho bất kỳ lớp nào. Head được chọn là head tốt nhất riêng lẻ trên một bộ dữ liệu riêng biệt.

--- TRANG 12 ---
Layer Enc-Enc Enc-Dec Dec-Dec
1 -1.96 0.02 0.03
2 -0.57 0.09 -0.13
3 -0.45 -0.42 0.00
4 -0.30 -0.60 -0.31
5 -0.32 -2.75 -0.66
6 -0.67 -18.89 -0.03

Bảng 5: Delta BLEU tốt nhất theo lớp trên newstest2014 khi chỉ head tốt nhất (được đánh giá trên newstest2013) được giữ lại trong mô hình WMT. Các số được gạch chân chỉ ra rằng thay đổi có ý nghĩa thống kê với p < 0.01.

Layer Layer
1 -0.01% 7 0.05%
2 -0.02% 8 -0.72%
3 -0.26% 9 -0.96%
4 -0.53% 10 0.07%
5 -0.29% 11 -0.19%
6 -0.52% 12 -0.15%

Bảng 6: Delta accuracy tốt nhất theo lớp trên tập validation của MNLI-matched khi chỉ head tốt nhất (được đánh giá trên 5,000 ví dụ huấn luyện) được giữ lại trong mô hình BERT. Không có kết quả nào trong số này có ý nghĩa thống kê với p < 0.01.

B Thí Nghiệm Cắt Tỉa Bổ Sung
Chúng tôi báo cáo kết quả bổ sung cho phương pháp cắt tỉa theo tầm quan trọng từ Phần 4 trên 4 bộ dữ liệu bổ sung:

SST-2: Phiên bản GLUE của Stanford Sentiment Treebank (Socher et al., 2013). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

CoLA: Phiên bản GLUE của Corpus of Linguistic Acceptability (Warstadt et al., 2018). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

MRPC: Phiên bản GLUE của Microsoft Research Paraphrase Corpus (Dolan và Brockett, 2005). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

IWSLT: Bộ dữ liệu dịch German sang English từ IWSLT 2014 (Cettolo et al., 2015). Chúng tôi sử dụng cùng mô hình nhỏ hơn được mô tả trong Phần 6.

Hình 6 cho thấy rằng trong một số trường hợp lên đến 60% (SST-2) hoặc 50% (CoLA, MRPC) các head có thể được cắt tỉa mà không có tác động đáng chú ý đến hiệu suất.

--- TRANG 13 ---
0% 20% 40% 60% 80% 100%
Percentage pruned0.00.20.40.60.8Accuracy

(a) Sự tiến triển của độ chính xác trên tập validation của SST-2 khi các head được cắt tỉa từ BERT theo Ih.

0% 20% 40% 60% 80% 100%
Percentage pruned0.00.20.40.6Matthew's correlation

(b) Sự tiến triển của tương quan Matthew trên tập validation của CoLA khi các head được cắt tỉa từ BERT theo Ih.

0% 20% 40% 60% 80% 100%
Percentage pruned0.00.20.40.60.8F-1

(c) Sự tiến triển của điểm F-1 trên tập validation của MRPC khi các head được cắt tỉa từ BERT theo Ih.

0% 20% 40% 60% 80% 100%
Percentage pruned05101520253035BLEU

(d) Sự tiến triển của điểm BLEU của mô hình IWSLT khi các head được cắt tỉa theo Ih (đường xanh liền nét).

Hình 6: Sự tiến triển của điểm theo tỷ lệ phần trăm head được cắt tỉa.
