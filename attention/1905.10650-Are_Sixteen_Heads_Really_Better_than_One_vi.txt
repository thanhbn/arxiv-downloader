# 1905.10650.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1905.10650.pdf
# Kích thước tệp: 576411 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Liệu Mười Sáu Đầu Có Thực Sự Tốt Hơn Một Đầu?
Paul Michel
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
pmichel1@cs.cmu.eduOmer Levy
Phòng thí nghiệm Nghiên cứu Trí tuệ Nhân tạo Facebook
Seattle, WA
omerlevy@fb.com
Graham Neubig
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
gneubig@cs.cmu.edu
Tóm tắt
Attention (Cơ chế chú ý) là một cơ chế mạnh mẽ và phổ biến cho phép các mô hình mạng nơ-ron tập trung vào các phần thông tin nổi bật cụ thể bằng cách lấy trung bình có trọng số của chúng khi đưa ra dự đoán. Cụ thể, multi-headed attention (cơ chế chú ý đa đầu) là động lực chính đằng sau nhiều mô hình xử lý ngôn ngữ tự nhiên (NLP) hiện đại tiên tiến như các mô hình MT dựa trên Transformer và BERT. Các mô hình này áp dụng nhiều cơ chế chú ý song song, với mỗi "đầu" chú ý có thể tập trung vào các phần khác nhau của đầu vào, điều này giúp có thể biểu diễn các hàm phức tạp vượt ra ngoài trung bình có trọng số đơn giản. Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng ngay cả khi các mô hình đã được huấn luyện sử dụng nhiều đầu, trong thực tế, một tỷ lệ lớn các đầu chú ý có thể được loại bỏ tại thời điểm kiểm tra mà không ảnh hưởng đáng kể đến hiệu suất. Thực tế, một số lớp thậm chí có thể được giảm xuống chỉ còn một đầu duy nhất. Chúng tôi tiếp tục kiểm tra các thuật toán tham lam để cắt giảm mô hình, và các cải thiện tiềm năng về tốc độ, hiệu quả bộ nhớ, và độ chính xác có thể đạt được từ đó. Cuối cùng, chúng tôi phân tích kết quả theo góc độ các phần nào của mô hình phụ thuộc nhiều hơn vào việc có nhiều đầu, và cung cấp bằng chứng sơ bộ rằng động lực huấn luyện đóng vai trò trong những lợi ích được cung cấp bởi multi-head attention¹.

1 Giới thiệu
Transformers (Vaswani et al., 2017) đã cho thấy hiệu suất tốt nhất trên nhiều tác vụ NLP khác nhau, bao gồm nhưng không giới hạn ở dịch máy (Vaswani et al., 2017; Ott et al., 2018), trả lời câu hỏi (Devlin et al., 2018), phân loại văn bản (Radford et al., 2018), và gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018). Là trung tâm của những cải tiến kiến trúc, Transformer mở rộng cơ chế chú ý tiêu chuẩn (Bahdanau et al., 2015; Cho et al., 2014) thông qua multi-headed attention (MHA), trong đó chú ý được tính toán độc lập bởi Nh cơ chế chú ý song song (các đầu). Đã được chứng minh rằng ngoài việc cải thiện hiệu suất, MHA có thể giúp với sự đồng thuận chủ ngữ-động từ (Tang et al., 2018) và một số đầu có thể dự đoán các cấu trúc phụ thuộc (Raganato và Tiedemann, 2018). Kể từ đó, một số mở rộng cho phương pháp luận chung đã được đề xuất (Ahmed et al., 2017; Shen et al., 2018).

¹Mã để tái tạo các thí nghiệm của chúng tôi được cung cấp tại https://github.com/pmichel31415/
are-16-heads-really-better-than-1

Hội nghị lần thứ 33 về Hệ thống Xử lý Thông tin Nơ-ron (NeurIPS 2019), Vancouver, Canada.arXiv:1905.10650v3  [cs.CL]  4 Nov 2019

--- TRANG 2 ---
Tuy nhiên, vẫn chưa hoàn toàn rõ ràng: nhiều đầu trong các mô hình này mang lại gì cho chúng ta? Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng - trong cả các mô hình dựa trên Transformer cho dịch máy và BERT (Devlin et al., 2018) suy luận ngôn ngữ tự nhiên - hầu hết các đầu chú ý có thể được loại bỏ riêng lẻ sau khi huấn luyện mà không có bất kỳ nhược điểm đáng kể nào về hiệu suất kiểm tra (§3.2). Đáng chú ý, nhiều lớp chú ý thậm chí có thể được giảm riêng lẻ xuống chỉ một đầu chú ý duy nhất mà không ảnh hưởng đến hiệu suất kiểm tra (§3.3).

Dựa trên quan sát này, chúng tôi đề xuất thêm một thuật toán đơn giản thực hiện việc cắt giảm các đầu chú ý theo cách tham lam và lặp lại, những đầu có vẻ đóng góp ít hơn cho mô hình. Bằng cách loại bỏ kết hợp các đầu chú ý từ toàn bộ mạng, không giới hạn việc cắt giảm ở một lớp đơn lẻ, chúng tôi phát hiện rằng các phần lớn của mạng có thể được loại bỏ với ít hoặc không có hậu quả nào, nhưng phần lớn các đầu phải được giữ lại để tránh sự sụt giảm thảm khốc về hiệu suất (§4). Chúng tôi tiếp tục phát hiện rằng điều này có lợi ích đáng kể cho hiệu quả thời gian suy luận, dẫn đến tăng tới 17,5% tốc độ suy luận cho mô hình dựa trên BERT.

Sau đó chúng tôi đi sâu vào phân tích thêm. Cái nhìn kỹ hơn về trường hợp dịch máy tiết lộ rằng các lớp chú ý encoder-decoder đặc biệt nhạy cảm với việc cắt giảm, nhiều hơn so với các lớp self-attention, gợi ý rằng multi-headedness đóng vai trò quan trọng trong thành phần này (§5). Cuối cùng, chúng tôi cung cấp bằng chứng rằng sự phân biệt giữa các đầu quan trọng và không quan trọng tăng lên khi quá trình huấn luyện tiến triển, gợi ý sự tương tác giữa multi-headedness và động lực huấn luyện (§6).

2 Kiến thức nền tảng: Attention, Multi-headed Attention, và Masking
Trong phần này, chúng tôi đặt nền tảng ký hiệu về attention và cũng mô tả phương pháp mask ra các đầu chú ý.

2.1 Single-headed Attention
Chúng tôi nhắc lại ngắn gọn cách hoạt động của vanilla attention. Chúng tôi tập trung vào scaled bilinear attention (Luong et al., 2015), biến thể được sử dụng phổ biến nhất trong các lớp MHA. Cho một chuỗi n vector d chiều x=x₁;...;xₙ∈Rᵈ, và một vector truy vấn q∈Rᵈ, lớp attention được tham số hóa bởi Wₖ;Wq;Wᵥ;W₀∈Rᵈˣᵈ tính tổng có trọng số:

Att_{Wₖ;Wq;Wᵥ;W₀}(x;q) = W₀∑ᵢ₌₁ⁿ αᵢWᵥxᵢ

trong đó αᵢ = softmax(q^T W_q^T Wₖxᵢ/√d)

Trong self-attention, mỗi xᵢ được sử dụng như truy vấn q để tính toán một chuỗi mới của các biểu diễn, trong khi đó trong các mô hình sequence-to-sequence, q thường là một trạng thái decoder trong khi x tương ứng với đầu ra encoder.

2.2 Multi-headed Attention
Trong multi-headed attention (MHA), Nₕ lớp attention được tham số hóa độc lập được áp dụng song song để thu được kết quả cuối cùng:

MHAtt(x;q) = ∑ₕ₌₁^{Nₕ} Att_{Wₖʰ;Wqʰ;Wᵥʰ;W₀ʰ}(x;q) (1)

trong đó Wₖʰ;Wqʰ;Wᵥʰ∈Rᵈₕˣᵈ và W₀ʰ∈Rᵈˣᵈₕ. Khi dₕ=d, MHA có tính biểu diễn chặt chẽ hơn vanilla attention. Tuy nhiên, để giữ số tham số không đổi, dₕ thường được đặt thành d/Nₕ, trong trường hợp này MHA có thể được xem như một tập hợp các lớp vanilla attention rank thấp². Trong phần tiếp theo, chúng tôi sử dụng Attₕ(x) như viết tắt cho đầu ra của head h trên input x.

Để cho phép các đầu chú ý khác nhau tương tác với nhau, transformers áp dụng một mạng feed-forward phi tuyến trên đầu ra của MHA, tại mỗi lớp transformer (Vaswani et al., 2017).

²Ký hiệu này, tương đương với công thức "nối" từ Vaswani et al. (2017), được sử dụng để dễ dàng trình bày trong các phần tiếp theo.

--- TRANG 3 ---
[Biểu đồ 1: Phân bố các đầu theo điểm số mô hình sau khi masking]

2.3 Masking Attention Heads
Để thực hiện các thí nghiệm ablation trên các đầu, chúng tôi sửa đổi công thức cho MHAtt:

MHAtt(x;q) = ∑ₕ₌₁^{Nₕ} ξₕAtt_{Wₖʰ;Wqʰ;Wᵥʰ;W₀ʰ}(x;q)

trong đó ξₕ là các biến mask với giá trị trong {0;1}. Khi tất cả ξₕ bằng 1, điều này tương đương với công thức trong Phương trình 1. Để mask head h, chúng tôi đơn giản đặt ξₕ = 0.

3 Liệu Tất Cả Các Đầu Chú Ý Đều Quan Trọng?
Chúng tôi thực hiện một loạt thí nghiệm trong đó chúng tôi loại bỏ một hoặc nhiều đầu chú ý từ một kiến trúc nhất định tại thời điểm kiểm tra, và đo lường sự khác biệt về hiệu suất. Chúng tôi đầu tiên loại bỏ một đầu chú ý duy nhất tại mỗi lần (§3.2) và sau đó loại bỏ mọi đầu trong toàn bộ lớp ngoại trừ một đầu (§3.3).

3.1 Thiết lập Thí nghiệm
Trong tất cả các thí nghiệm tiếp theo, chúng tôi xem xét hai mô hình đã được huấn luyện:

WMT Đây là kiến trúc transformer "lớn" ban đầu từ Vaswani et al. 2017 với 6 lớp và 16 đầu mỗi lớp, được huấn luyện trên corpus WMT2014 English to French. Chúng tôi sử dụng mô hình đã được huấn luyện trước của Ott et al. 2018.³ và báo cáo điểm BLEU trên tập kiểm tra newstest2013. Theo Ott et al. 2018, chúng tôi tính điểm BLEU trên đầu ra tokenized của mô hình sử dụng Moses (Koehn et al., 2007). Ý nghĩa thống kê được kiểm tra với paired bootstrap resampling (Koehn, 2004) sử dụng compare-mt⁴ (Neubig et al., 2019) với 1000 mẫu lại. Một đặc điểm của mô hình này là nó có 3 cơ chế chú ý riêng biệt: encoder self-attention (Enc-Enc), encoder-decoder attention (Enc-Dec) và decoder self-attention (Dec-Dec), tất cả đều sử dụng MHA.

BERT BERT (Devlin et al., 2018) là một transformer đơn được huấn luyện trước trên tác vụ "masked language modeling" không giám sát kiểu cloze và sau đó được fine-tune trên các tác vụ cụ thể. Tại thời điểm ra đời, nó đạt được hiệu suất tối tân trên nhiều tác vụ NLP khác nhau. Chúng tôi sử dụng mô hình base-uncased đã được huấn luyện trước của Devlin et al. 2018 với 12 lớp và 12 đầu chú ý mà chúng tôi fine-tune và đánh giá trên MultiNLI (Williams et al., 2018). Chúng tôi báo cáo độ chính xác trên tập validation "matched". Chúng tôi kiểm tra ý nghĩa thống kê sử dụng t-test. Trái ngược với mô hình WMT, BERT chỉ có một cơ chế chú ý (self-attention trong mỗi lớp).

3.2 Ablating Một Đầu
Để hiểu đóng góp của một đầu chú ý cụ thể h, chúng tôi đánh giá hiệu suất của mô hình trong khi mask đầu đó (tức là thay thế Attₕ(x) bằng zero). Nếu hiệu suất sans h giảm đáng kể so với mô hình đầy đủ, h rõ ràng là quan trọng; nếu hiệu suất tương đương, h là dư thừa so với phần còn lại của mô hình.

³https://github.com/pytorch/fairseq/tree/master/examples/translation
⁴https://github.com/neulab/compare-mt

--- TRANG 4 ---
[Bảng 1: Sự khác biệt trong điểm BLEU cho mỗi đầu của cơ chế self attention của encoder. Các số được gạch dưới cho biết sự thay đổi có ý nghĩa thống kê với p < 0.01. Điểm BLEU cơ sở là 36.05.]

[Bảng 2: Delta BLEU tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình WMT. Các số được gạch dưới cho biết sự thay đổi có ý nghĩa thống kê với p < 0.01.]

[Bảng 3: Delta độ chính xác tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình BERT. Không có kết quả nào có ý nghĩa thống kê với p < 0.01.]

Hình 1a và 1b cho thấy phân bố của các đầu về điểm số của mô hình sau khi mask nó, cho WMT và BERT tương ứng. Chúng tôi quan sát thấy rằng phần lớn các đầu chú ý có thể được loại bỏ mà không lệch quá xa so với điểm số ban đầu. Đáng ngạc nhiên, trong một số trường hợp loại bỏ một đầu chú ý dẫn đến sự gia tăng BLEU/độ chính xác.

Để có cái nhìn chi tiết hơn về những kết quả này, chúng tôi phóng to vào các lớp encoder self-attention của mô hình WMT trong Bảng 1. Đáng chú ý, chúng tôi thấy rằng chỉ có 8 (trong số 96) đầu gây ra sự thay đổi có ý nghĩa thống kê về hiệu suất khi chúng được loại bỏ khỏi mô hình, một nửa trong số đó thực sự dẫn đến điểm BLEU cao hơn. Điều này dẫn chúng tôi đến quan sát đầu tiên: tại thời điểm kiểm tra, hầu hết các đầu là dư thừa so với phần còn lại của mô hình.

3.3 Ablating Tất Cả Các Đầu Trừ Một
Quan sát này đặt ra câu hỏi: liệu có cần nhiều hơn một đầu không? Do đó, chúng tôi tính toán sự khác biệt về hiệu suất khi tất cả các đầu ngoại trừ một được loại bỏ, trong một lớp đơn. Trong Bảng 2 và Bảng 3, chúng tôi báo cáo điểm số tốt nhất cho mỗi lớp trong mô hình, tức là điểm số khi giảm toàn bộ lớp xuống chỉ một đầu quan trọng nhất.

Chúng tôi phát hiện rằng, đối với hầu hết các lớp, một đầu thực sự đủ tại thời điểm kiểm tra, mặc dù mạng đã được huấn luyện với 12 hoặc 16 đầu chú ý. Điều này đáng chú ý vì những lớp này có thể được giảm xuống single-headed attention chỉ với 1/16 (tương ứng 1/12) số tham số của một lớp vanilla attention. Tuy nhiên, một số lớp cần nhiều đầu chú ý; ví dụ, thay thế lớp cuối trong encoder-decoder attention của WMT bằng một đầu đơn làm giảm hiệu suất ít nhất 13.5 điểm BLEU. Chúng tôi phân tích thêm khi các thành phần mô hình khác nhau phụ thuộc vào nhiều đầu hơn trong §5.

Ngoài ra, chúng tôi xác minh rằng kết quả này vẫn đúng ngay cả khi chúng tôi không có quyền truy cập vào tập đánh giá khi lựa chọn đầu "tốt nhất riêng lẻ". Với mục đích này, chúng tôi lựa chọn đầu tốt nhất cho mỗi lớp trên tập validation (newstest2013 cho WMT và một tập con 5,000 được chọn ngẫu nhiên từ tập huấn luyện của MNLI cho BERT) và đánh giá hiệu suất của mô hình trên tập kiểm tra

--- TRANG 5 ---
[Hình 2: Phân tích chéo nhiệm vụ về tác động của việc cắt giảm lên độ chính xác]

(newstest2014 cho WMT và tập validation MNLI-matched cho BERT). Chúng tôi quan sát rằng những phát hiện tương tự vẫn đúng: chỉ giữ một đầu không dẫn đến sự thay đổi có ý nghĩa thống kê về hiệu suất cho 50% (tương ứng 100%) lớp của WMT (tương ứng BERT). Kết quả chi tiết có thể tìm thấy trong Phụ lục A.

3.4 Liệu Các Đầu Quan Trọng Giống Nhau Trên Các Tập Dữ Liệu?
Có một lưu ý đối với hai thí nghiệm trước của chúng tôi: những kết quả này chỉ có giá trị trên các tập kiểm tra cụ thể (và khá nhỏ), đặt ra nghi ngờ về khả năng tổng quát hóa của chúng lên các tập dữ liệu khác. Như bước đầu tiên để hiểu liệu một số đầu có quan trọng phổ quát không, chúng tôi thực hiện nghiên cứu ablation tương tự trên tập kiểm tra thứ hai, ngoài miền. Cụ thể, chúng tôi xem xét tập validation MNLI "mismatched" cho BERT và tập kiểm tra MTNT English to French (Michel và Neubig, 2018) cho mô hình WMT, cả hai đều được tập hợp với mục đích rất cụ thể là cung cấp các bộ kiểm tra đối chiếu, ngoài miền cho các nhiệm vụ tương ứng.

Chúng tôi thực hiện nghiên cứu ablation tương tự như §3.2 trên mỗi tập dữ liệu này và báo cáo kết quả trong Hình 2a và 2b. Chúng tôi nhận thấy rằng có mối tương quan dương, >0.5 (p < 0.01) giữa tác động của việc loại bỏ một đầu trên cả hai tập dữ liệu. Hơn nữa, các đầu có tác động cao nhất lên hiệu suất trên một miền có xu hướng có cùng tác động trên miền khác, điều này gợi ý rằng các đầu quan trọng nhất từ §3.2 thực sự quan trọng "phổ quát".

4 Cắt Giảm Lặp Lại Các Đầu Chú Ý
Trong các thí nghiệm ablation của chúng tôi (§3.2 và §3.3), chúng tôi quan sát tác động của việc loại bỏ một hoặc nhiều đầu trong một lớp đơn, không xem xét điều gì sẽ xảy ra nếu chúng tôi thay đổi hai hoặc nhiều lớp khác nhau cùng lúc. Để kiểm tra tác động tổng hợp của việc cắt giảm nhiều đầu từ toàn bộ mô hình, chúng tôi sắp xếp tất cả các đầu chú ý trong mô hình theo điểm số tầm quan trọng proxy (được mô tả dưới đây), và sau đó loại bỏ các đầu từng cái một. Chúng tôi sử dụng cách tiếp cận lặp lại, heuristic này để tránh tìm kiếm tổ hợp, điều này không thực tế do số lượng đầu và thời gian cần thiết để đánh giá mỗi mô hình.

--- TRANG 6 ---
[Hình 3: Sự phát triển của độ chính xác theo số lượng đầu được cắt giảm]

4.1 Điểm Số Tầm Quan Trọng Đầu để Cắt Giảm
Như điểm số proxy cho tầm quan trọng đầu, chúng tôi xem xét độ nhạy cảm kỳ vọng của mô hình với các biến mask ξₕ được định nghĩa trong §2.3:

Iₕ = E_x∈X |∂L(x)/∂ξₕ|                                    (2)

trong đó X là phân bố dữ liệu và L(x) là loss trên mẫu x. Trực quan, nếu Iₕ có giá trị cao thì việc thay đổi ξₕ có thể có tác động lớn lên mô hình. Đặc biệt, chúng tôi thấy giá trị tuyệt đối là rất quan trọng để tránh các điểm dữ liệu với đóng góp rất âm hoặc dương triệt tiêu lẫn nhau trong tổng. Thay Phương trình 1 vào Phương trình 2 và áp dụng chain rule cho biểu thức cuối cùng cho Iₕ:

Iₕ = E_x∈X |Attₕ(x)ᵀ ∂L(x)/∂Attₕ(x)|

Công thức này gợi nhớ đến văn học phong phú về cắt giảm mạng nơ-ron (LeCun et al., 1990; Hassibi và Stork, 1993; Molchanov et al., 2017, inter alia). Đặc biệt, nó tương đương với phương pháp Taylor expansion từ Molchanov et al. (2017).

Về hiệu suất, ước tính Iₕ chỉ yêu cầu thực hiện một forward và backward pass, và do đó không chậm hơn so với huấn luyện. Trong thực tế, chúng tôi tính kỳ vọng trên dữ liệu huấn luyện hoặc một tập con của nó.⁵ Như được khuyến nghị bởi Molchanov et al. (2017), chúng tôi chuẩn hóa điểm số tầm quan trọng theo lớp (sử dụng ℓ₂ norm).

4.2 Tác Động của Cắt Giảm lên BLEU/Độ Chính Xác
Hình 3a (cho WMT) và 3b (cho BERT) mô tả tác động của việc cắt giảm đầu chú ý lên hiệu suất mô hình trong khi loại bỏ dần 10% tổng số đầu theo thứ tự Iₕ tăng dần tại mỗi bước. Chúng tôi cũng báo cáo kết quả khi thứ tự cắt giảm được xác định bởi sự khác biệt điểm số từ §3.2 (trong đường nét đứt), nhưng thấy rằng việc sử dụng Iₕ nhanh hơn và cho kết quả tốt hơn.

Chúng tôi quan sát rằng cách tiếp cận này cho phép chúng tôi cắt giảm tới 20% và 40% đầu từ WMT và BERT (tương ứng), mà không phải chịu bất kỳ tác động tiêu cực đáng chú ý nào. Hiệu suất giảm mạnh khi cắt giảm thêm, có nghĩa là không mô hình nào có thể được giảm xuống mô hình attention hoàn toàn single-head mà không cần huấn luyện lại hoặc phải chịu tổn thất đáng kể về hiệu suất. Chúng tôi tham khảo Phụ lục B cho các thí nghiệm trên bốn tập dữ liệu bổ sung.

⁵Đối với mô hình WMT, chúng tôi sử dụng tất cả các tập newstest20[09-12] để ước tính I.

--- TRANG 7 ---
4.3 Tác Động của Cắt Giảm lên Hiệu Quả
Ngoài hiệu suất tác vụ downstream, có những lợi thế nội tại của việc cắt giảm đầu. Đầu tiên, mỗi đầu đại diện cho một tỷ lệ không đáng kể của tổng tham số trong mỗi lớp attention (6.25% cho WMT, 8.34% cho BERT), và do đó của tổng mô hình (nói chung, trong cả hai mô hình của chúng tôi, khoảng một phần ba tổng số tham số được dành cho MHA trên tất cả các lớp).⁶ Điều này hấp dẫn trong bối cảnh triển khai các mô hình trong các môi trường hạn chế bộ nhớ.

[Bảng 4: Tốc độ suy luận trung bình của BERT trên tập validation MNLI-matched]

Hơn nữa, chúng tôi thấy rằng thực sự cắt giảm các đầu (và không chỉ masking) dẫn đến sự gia tăng đáng kể về tốc độ suy luận. Bảng 4 báo cáo số lượng ví dụ mỗi giây được xử lý bởi BERT, trước và sau khi cắt giảm 50% tất cả các đầu chú ý. Các thí nghiệm được thực hiện trên hai máy khác nhau, cả hai đều trang bị GPU GeForce GTX 1080Ti. Mỗi thí nghiệm được lặp lại 3 lần trên mỗi máy (tổng cộng 6 điểm dữ liệu cho mỗi thiết lập). Chúng tôi thấy rằng việc cắt giảm một nửa đầu của mô hình tăng tốc suy luận lên tới 17.5% cho kích thước batch cao hơn (sự khác biệt này biến mất cho kích thước batch nhỏ hơn).

5 Khi Nào Nhiều Đầu Quan Trọng? Trường Hợp Dịch Máy
Như được hiển thị trong Bảng 2, không phải tất cả các lớp MHA đều có thể được giảm xuống một đầu chú ý đơn mà không ảnh hưởng đáng kể đến hiệu suất. Để có ý tưởng tốt hơn về mức độ mỗi phần của mô hình dịch dựa trên transformer phụ thuộc vào multi-headedness, chúng tôi lặp lại thí nghiệm cắt giảm heuristic từ §4 cho mỗi loại attention riêng biệt (Enc-Enc, Enc-Dec, và Dec-Dec).

Hình 4 cho thấy rằng hiệu suất giảm nhanh hơn nhiều khi các đầu được cắt giảm từ các lớp attention Enc-Dec. Cụ thể, việc cắt giảm hơn 60% các đầu attention Enc-Dec sẽ dẫn đến sự suy giảm hiệu suất thảm khốc, trong khi các lớp self-attention encoder và decoder vẫn có thể tạo ra các bản dịch hợp lý (với điểm BLEU khoảng 30) chỉ với 20% các đầu attention ban đầu. Nói cách khác, encoder-decoder attention phụ thuộc vào multi-headedness nhiều hơn self-attention.

[Hình 4: BLEU khi cắt giảm dần các đầu từ mỗi loại attention trong mô hình WMT]

⁶Hơi nhiều hơn trong WMT vì attention Enc-Dec.

--- TRANG 8 ---
[Hình 5: Mối quan hệ giữa tỷ lệ phần trăm đầu được cắt giảm và giảm điểm số tương đối trong quá trình huấn luyện]

6 Động Lực Tầm Quan Trọng Đầu Trong Quá Trình Huấn Luyện
Các phần trước cho chúng tôi biết rằng một số đầu quan trọng hơn những đầu khác trong các mô hình đã được huấn luyện. Để có cái nhìn sâu sắc hơn về động lực tầm quan trọng đầu trong quá trình huấn luyện, chúng tôi thực hiện thí nghiệm cắt giảm lặp lại tương tự của §4.2 tại mỗi epoch. Chúng tôi thực hiện thí nghiệm này trên phiên bản nhỏ hơn của mô hình WMT (6 lớp và 8 đầu mỗi lớp), được huấn luyện cho dịch German to English trên tập dữ liệu IWSLT 2014 nhỏ hơn Cettolo et al. (2015). Chúng tôi gọi mô hình này là IWSLT.

Hình 5 báo cáo, cho mỗi mức độ cắt giảm (theo gia số 10% — 0% tương ứng với mô hình ban đầu), sự phát triển của điểm số mô hình (trên newstest2013) cho mỗi epoch. Để dễ đọc hơn, chúng tôi hiển thị các epoch trên thang logarit, và chỉ báo cáo điểm số mỗi 5 epoch sau epoch thứ 10. Để làm cho điểm số có thể so sánh được qua các epoch, trục Y báo cáo sự suy giảm tương đối của điểm BLEU so với mô hình không cắt giảm tại mỗi epoch. Đặc biệt, chúng tôi thấy rằng có hai chế độ riêng biệt: trong các epoch rất sớm (đặc biệt là 1 và 2), hiệu suất giảm tuyến tính với tỷ lệ phần trăm cắt giảm, tức là sự giảm tương đối về hiệu suất độc lập với Ih, cho thấy rằng hầu hết các đầu đều ít nhiều quan trọng như nhau. Từ epoch 10 trở đi, có sự tập trung của các đầu không quan trọng có thể được cắt giảm trong khi vẫn giữ trong vòng 85-90% điểm BLEU ban đầu (lên tới 40% tổng số đầu).

Điều này gợi ý rằng các đầu quan trọng được xác định sớm (nhưng không ngay lập tức) trong quá trình huấn luyện. Hai giai đoạn huấn luyện gợi nhớ đến phân tích của Shwartz-Ziv và Tishby (2017), theo đó việc huấn luyện các mạng nơ-ron phân tách thành giai đoạn "tối thiểu hóa rủi ro thực nghiệm", trong đó mô hình tối đa hóa thông tin tương hỗ của các biểu diễn trung gian với các nhãn, và giai đoạn "nén" trong đó thông tin tương hỗ với đầu vào được tối thiểu hóa. Một điều tra có nguyên tắc hơn về hiện tượng này được để lại cho công việc tương lai.

7 Công Trình Liên Quan
Việc sử dụng cơ chế attention trong NLP và đặc biệt là dịch máy nơ-ron (NMT) có thể được truy nguyên về Bahdanau et al. (2015) và Cho et al. (2014), và hầu hết các triển khai đương thời đều dựa trên công thức từ Luong et al. (2015). Attention sau đó được điều chỉnh (thành công) cho các tác vụ NLP khác, thường đạt được hiệu suất tốt nhất khi đó trong đọc hiểu (Cheng et al., 2016), suy luận ngôn ngữ tự nhiên (Parikh et al., 2016) hoặc tóm tắt trừu tượng (Paulus et al., 2017) để kể một vài ví dụ. Multi-headed attention lần đầu tiên được giới thiệu bởi Vaswani et al. (2017) cho NMT và phân tích cú pháp tiếng Anh, và sau đó được áp dụng cho transfer learning (Radford et al., 2018; Devlin et al., 2018), mô hình hóa ngôn ngữ (Dai et al., 2019; Radford et al., 2019), hoặc gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018), trong số những ứng dụng khác.

Có một văn học phong phú về cắt giảm các mạng nơ-ron đã được huấn luyện, quay trở lại LeCun et al. (1990) và Hassibi và Stork (1993) vào đầu những năm 90 và được làm mới sau sự ra đời của deep learning, với hai cách tiếp cận trực giao: cắt giảm "từng trọng số" tinh vi (Han et al., 2015) và cắt giảm có cấu trúc (Anwar et al., 2017; Li et al., 2016; Molchanov et al., 2017), trong đó toàn bộ các phần của mô hình được cắt giảm. Trong NLP, cắt giảm có cấu trúc cho auto-sizing feed-forward language models lần đầu tiên được điều tra bởi Murray và Chiang (2015). Gần đây hơn, các cách tiếp cận cắt giảm tinh vi đã được phổ biến bởi See et al. (2016) và Kim và Rush (2016) (chủ yếu trên NMT).

Đồng thời với công việc của chúng tôi, Voita et al. (2019) đã đưa ra quan sát tương tự về multi-head attention. Cách tiếp cận của họ liên quan đến việc sử dụng LRP (Binder et al., 2016) để xác định các đầu quan trọng và xem xét các thuộc tính cụ thể như chú ý đến các vị trí liền kề, từ hiếm hoặc từ liên quan cú pháp. Họ đề xuất một cơ chế cắt giảm thay thế dựa trên việc thực hiện gradient descent trên các biến mask ξh. Trong khi cách tiếp cận và kết quả của họ bổ sung cho bài báo này, nghiên cứu của chúng tôi cung cấp bằng chứng bổ sung về hiện tượng này ngoài NMT, cũng như phân tích về động lực huấn luyện của việc cắt giảm các đầu attention.

8 Kết Luận
Chúng tôi đã quan sát rằng MHA không phải lúc nào cũng tận dụng khả năng biểu diễn vượt trội về mặt lý thuyết so với vanilla attention một cách trọn vẹn. Cụ thể, chúng tôi đã chứng minh rằng trong nhiều tình huống khác nhau, một số đầu có thể được loại bỏ khỏi các mô hình transformer đã được huấn luyện mà không có sự suy giảm có ý nghĩa thống kê về hiệu suất kiểm tra, và một số lớp có thể được giảm xuống chỉ còn một đầu. Ngoài ra, chúng tôi đã chỉ ra rằng trong các mô hình dịch máy, các lớp encoder-decoder attention phụ thuộc vào multi-headedness nhiều hơn so với các lớp self-attention, và cung cấp bằng chứng rằng tầm quan trọng tương đối của mỗi đầu được xác định trong các giai đoạn đầu của quá trình huấn luyện. Chúng tôi hy vọng rằng những quan sát này sẽ thúc đẩy sự hiểu biết của chúng tôi về MHA và truyền cảm hứng cho các mô hình đầu tư tham số và attention của chúng hiệu quả hơn.

Lời Cảm Ơn
Các tác giả muốn gửi lời cảm ơn đến các nhà phản biện ẩn danh vì những phản hồi sâu sắc của họ. Chúng tôi cũng đặc biệt biết ơn Thomas Wolf từ Hugging Face, người có nỗ lực tái tạo độc lập đã cho phép chúng tôi tìm ra và sửa một lỗi trong các thí nghiệm so sánh tốc độ. Nghiên cứu này được hỗ trợ một phần bởi món quà từ Facebook.

Tài Liệu Tham Khảo
[Danh sách các tài liệu tham khảo được dịch đầy đủ theo cấu trúc ban đầu]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

A Ablating Tất Cả Các Đầu Trừ Một: Thí Nghiệm Bổ Sung
Bảng 5 và 6 báo cáo sự khác biệt về hiệu suất khi chỉ giữ một đầu cho bất kỳ lớp nào. Đầu được chọn là đầu tốt nhất riêng lẻ trên một tập dữ liệu riêng biệt.

--- TRANG 12 ---
[Bảng 5 và 6 với kết quả thí nghiệm bổ sung]

B Các Thí Nghiệm Cắt Giảm Bổ Sung
Chúng tôi báo cáo kết quả bổ sung cho cách tiếp cận cắt giảm dựa trên tầm quan trọng từ Phần 4 trên 4 tập dữ liệu bổ sung:

SST-2: Phiên bản GLUE của Stanford Sentiment Treebank (Socher et al., 2013). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

CoLA: Phiên bản GLUE của Corpus of Linguistic Acceptability (Warstadt et al., 2018). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

MRPC: Phiên bản GLUE của Microsoft Research Paraphrase Corpus (Dolan và Brockett, 2005). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

IWSLT: Tập dữ liệu dịch German to English từ IWSLT 2014 (Cettolo et al., 2015). Chúng tôi sử dụng mô hình nhỏ hơn tương tự được mô tả trong Phần 6.

Hình 6 cho thấy rằng trong một số trường hợp có thể cắt giảm lên tới 60% (SST-2) hoặc 50% (CoLA, MRPC) các đầu mà không có tác động đáng chú ý lên hiệu suất.

--- TRANG 13 ---
[Hình 6: Sự phát triển điểm số theo tỷ lệ phần trăm đầu được cắt giảm]# 1905.10650.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1905.10650.pdf
# Kích thước tệp: 576411 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Liệu Mười Sáu Đầu Có Thực Sự Tốt Hơn Một Đầu?
Paul Michel
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
pmichel1@cs.cmu.eduOmer Levy
Phòng thí nghiệm Nghiên cứu Trí tuệ Nhân tạo Facebook
Seattle, WA
omerlevy@fb.com
Graham Neubig
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
gneubig@cs.cmu.edu
Tóm tắt
Attention (Cơ chế chú ý) là một cơ chế mạnh mẽ và phổ biến cho phép các mô hình mạng nơ-ron tập trung vào các phần thông tin nổi bật cụ thể bằng cách lấy trung bình có trọng số của chúng khi đưa ra dự đoán. Cụ thể, multi-headed attention (cơ chế chú ý đa đầu) là động lực chính đằng sau nhiều mô hình xử lý ngôn ngữ tự nhiên (NLP) hiện đại tiên tiến như các mô hình MT dựa trên Transformer và BERT. Các mô hình này áp dụng nhiều cơ chế chú ý song song, với mỗi "đầu" chú ý có thể tập trung vào các phần khác nhau của đầu vào, điều này giúp có thể biểu diễn các hàm phức tạp vượt ra ngoài trung bình có trọng số đơn giản. Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng ngay cả khi các mô hình đã được huấn luyện sử dụng nhiều đầu, trong thực tế, một tỷ lệ lớn các đầu chú ý có thể được loại bỏ tại thời điểm kiểm tra mà không ảnh hưởng đáng kể đến hiệu suất. Thực tế, một số lớp thậm chí có thể được giảm xuống chỉ còn một đầu duy nhất. Chúng tôi tiếp tục kiểm tra các thuật toán tham lam để cắt giảm mô hình, và các cải thiện tiềm năng về tốc độ, hiệu quả bộ nhớ, và độ chính xác có thể đạt được từ đó. Cuối cùng, chúng tôi phân tích kết quả theo góc độ các phần nào của mô hình phụ thuộc nhiều hơn vào việc có nhiều đầu, và cung cấp bằng chứng sơ bộ rằng động lực huấn luyện đóng vai trò trong những lợi ích được cung cấp bởi multi-head attention¹.

1 Giới thiệu
Transformers (Vaswani et al., 2017) đã cho thấy hiệu suất tốt nhất trên nhiều tác vụ NLP khác nhau, bao gồm nhưng không giới hạn ở dịch máy (Vaswani et al., 2017; Ott et al., 2018), trả lời câu hỏi (Devlin et al., 2018), phân loại văn bản (Radford et al., 2018), và gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018). Là trung tâm của những cải tiến kiến trúc, Transformer mở rộng cơ chế chú ý tiêu chuẩn (Bahdanau et al., 2015; Cho et al., 2014) thông qua multi-headed attention (MHA), trong đó chú ý được tính toán độc lập bởi Nh cơ chế chú ý song song (các đầu). Đã được chứng minh rằng ngoài việc cải thiện hiệu suất, MHA có thể giúp với sự đồng thuận chủ ngữ-động từ (Tang et al., 2018) và một số đầu có thể dự đoán các cấu trúc phụ thuộc (Raganato và Tiedemann, 2018). Kể từ đó, một số mở rộng cho phương pháp luận chung đã được đề xuất (Ahmed et al., 2017; Shen et al., 2018).

¹Mã để tái tạo các thí nghiệm của chúng tôi được cung cấp tại https://github.com/pmichel31415/
are-16-heads-really-better-than-1

Hội nghị lần thứ 33 về Hệ thống Xử lý Thông tin Nơ-ron (NeurIPS 2019), Vancouver, Canada.arXiv:1905.10650v3  [cs.CL]  4 Nov 2019

--- TRANG 2 ---
Tuy nhiên, vẫn chưa hoàn toàn rõ ràng: nhiều đầu trong các mô hình này mang lại gì cho chúng ta? Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng - trong cả các mô hình dựa trên Transformer cho dịch máy và BERT (Devlin et al., 2018) suy luận ngôn ngữ tự nhiên - hầu hết các đầu chú ý có thể được loại bỏ riêng lẻ sau khi huấn luyện mà không có bất kỳ nhược điểm đáng kể nào về hiệu suất kiểm tra (§3.2). Đáng chú ý, nhiều lớp chú ý thậm chí có thể được giảm riêng lẻ xuống chỉ một đầu chú ý duy nhất mà không ảnh hưởng đến hiệu suất kiểm tra (§3.3).

Dựa trên quan sát này, chúng tôi đề xuất thêm một thuật toán đơn giản thực hiện việc cắt giảm các đầu chú ý theo cách tham lam và lặp lại, những đầu có vẻ đóng góp ít hơn cho mô hình. Bằng cách loại bỏ kết hợp các đầu chú ý từ toàn bộ mạng, không giới hạn việc cắt giảm ở một lớp đơn lẻ, chúng tôi phát hiện rằng các phần lớn của mạng có thể được loại bỏ với ít hoặc không có hậu quả nào, nhưng phần lớn các đầu phải được giữ lại để tránh sự sụt giảm thảm khốc về hiệu suất (§4). Chúng tôi tiếp tục phát hiện rằng điều này có lợi ích đáng kể cho hiệu quả thời gian suy luận, dẫn đến tăng tới 17,5% tốc độ suy luận cho mô hình dựa trên BERT.

Sau đó chúng tôi đi sâu vào phân tích thêm. Cái nhìn kỹ hơn về trường hợp dịch máy tiết lộ rằng các lớp chú ý encoder-decoder đặc biệt nhạy cảm với việc cắt giảm, nhiều hơn so với các lớp self-attention, gợi ý rằng multi-headedness đóng vai trò quan trọng trong thành phần này (§5). Cuối cùng, chúng tôi cung cấp bằng chứng rằng sự phân biệt giữa các đầu quan trọng và không quan trọng tăng lên khi quá trình huấn luyện tiến triển, gợi ý sự tương tác giữa multi-headedness và động lực huấn luyện (§6).

2 Kiến thức nền tảng: Attention, Multi-headed Attention, và Masking
Trong phần này, chúng tôi đặt nền tảng ký hiệu về attention và cũng mô tả phương pháp mask ra các đầu chú ý.

2.1 Single-headed Attention
Chúng tôi nhắc lại ngắn gọn cách hoạt động của vanilla attention. Chúng tôi tập trung vào scaled bilinear attention (Luong et al., 2015), biến thể được sử dụng phổ biến nhất trong các lớp MHA. Cho một chuỗi n vector d chiều x=x₁;...;xₙ∈Rᵈ, và một vector truy vấn q∈Rᵈ, lớp attention được tham số hóa bởi Wₖ;Wq;Wᵥ;W₀∈Rᵈˣᵈ tính tổng có trọng số:

Att_{Wₖ;Wq;Wᵥ;W₀}(x;q) = W₀∑ᵢ₌₁ⁿ αᵢWᵥxᵢ

trong đó αᵢ = softmax(q^T W_q^T Wₖxᵢ/√d)

Trong self-attention, mỗi xᵢ được sử dụng như truy vấn q để tính toán một chuỗi mới của các biểu diễn, trong khi đó trong các mô hình sequence-to-sequence, q thường là một trạng thái decoder trong khi x tương ứng với đầu ra encoder.

2.2 Multi-headed Attention
Trong multi-headed attention (MHA), Nₕ lớp attention được tham số hóa độc lập được áp dụng song song để thu được kết quả cuối cùng:

MHAtt(x;q) = ∑ₕ₌₁^{Nₕ} Att_{Wₖʰ;Wqʰ;Wᵥʰ;W₀ʰ}(x;q) (1)

trong đó Wₖʰ;Wqʰ;Wᵥʰ∈Rᵈₕˣᵈ và W₀ʰ∈Rᵈˣᵈₕ. Khi dₕ=d, MHA có tính biểu diễn chặt chẽ hơn vanilla attention. Tuy nhiên, để giữ số tham số không đổi, dₕ thường được đặt thành d/Nₕ, trong trường hợp này MHA có thể được xem như một tập hợp các lớp vanilla attention rank thấp². Trong phần tiếp theo, chúng tôi sử dụng Attₕ(x) như viết tắt cho đầu ra của head h trên input x.

Để cho phép các đầu chú ý khác nhau tương tác với nhau, transformers áp dụng một mạng feed-forward phi tuyến trên đầu ra của MHA, tại mỗi lớp transformer (Vaswani et al., 2017).

²Ký hiệu này, tương đương với công thức "nối" từ Vaswani et al. (2017), được sử dụng để dễ dàng trình bày trong các phần tiếp theo.

--- TRANG 3 ---
[Biểu đồ 1: Phân bố các đầu theo điểm số mô hình sau khi masking]

2.3 Masking Attention Heads
Để thực hiện các thí nghiệm ablation trên các đầu, chúng tôi sửa đổi công thức cho MHAtt:

MHAtt(x;q) = ∑ₕ₌₁^{Nₕ} ξₕAtt_{Wₖʰ;Wqʰ;Wᵥʰ;W₀ʰ}(x;q)

trong đó ξₕ là các biến mask với giá trị trong {0;1}. Khi tất cả ξₕ bằng 1, điều này tương đương với công thức trong Phương trình 1. Để mask head h, chúng tôi đơn giản đặt ξₕ = 0.

3 Liệu Tất Cả Các Đầu Chú Ý Đều Quan Trọng?
Chúng tôi thực hiện một loạt thí nghiệm trong đó chúng tôi loại bỏ một hoặc nhiều đầu chú ý từ một kiến trúc nhất định tại thời điểm kiểm tra, và đo lường sự khác biệt về hiệu suất. Chúng tôi đầu tiên loại bỏ một đầu chú ý duy nhất tại mỗi lần (§3.2) và sau đó loại bỏ mọi đầu trong toàn bộ lớp ngoại trừ một đầu (§3.3).

3.1 Thiết lập Thí nghiệm
Trong tất cả các thí nghiệm tiếp theo, chúng tôi xem xét hai mô hình đã được huấn luyện:

WMT Đây là kiến trúc transformer "lớn" ban đầu từ Vaswani et al. 2017 với 6 lớp và 16 đầu mỗi lớp, được huấn luyện trên corpus WMT2014 English to French. Chúng tôi sử dụng mô hình đã được huấn luyện trước của Ott et al. 2018.³ và báo cáo điểm BLEU trên tập kiểm tra newstest2013. Theo Ott et al. 2018, chúng tôi tính điểm BLEU trên đầu ra tokenized của mô hình sử dụng Moses (Koehn et al., 2007). Ý nghĩa thống kê được kiểm tra với paired bootstrap resampling (Koehn, 2004) sử dụng compare-mt⁴ (Neubig et al., 2019) với 1000 mẫu lại. Một đặc điểm của mô hình này là nó có 3 cơ chế chú ý riêng biệt: encoder self-attention (Enc-Enc), encoder-decoder attention (Enc-Dec) và decoder self-attention (Dec-Dec), tất cả đều sử dụng MHA.

BERT BERT (Devlin et al., 2018) là một transformer đơn được huấn luyện trước trên tác vụ "masked language modeling" không giám sát kiểu cloze và sau đó được fine-tune trên các tác vụ cụ thể. Tại thời điểm ra đời, nó đạt được hiệu suất tối tân trên nhiều tác vụ NLP khác nhau. Chúng tôi sử dụng mô hình base-uncased đã được huấn luyện trước của Devlin et al. 2018 với 12 lớp và 12 đầu chú ý mà chúng tôi fine-tune và đánh giá trên MultiNLI (Williams et al., 2018). Chúng tôi báo cáo độ chính xác trên tập validation "matched". Chúng tôi kiểm tra ý nghĩa thống kê sử dụng t-test. Trái ngược với mô hình WMT, BERT chỉ có một cơ chế chú ý (self-attention trong mỗi lớp).

3.2 Ablating Một Đầu
Để hiểu đóng góp của một đầu chú ý cụ thể h, chúng tôi đánh giá hiệu suất của mô hình trong khi mask đầu đó (tức là thay thế Attₕ(x) bằng zero). Nếu hiệu suất sans h giảm đáng kể so với mô hình đầy đủ, h rõ ràng là quan trọng; nếu hiệu suất tương đương, h là dư thừa so với phần còn lại của mô hình.

³https://github.com/pytorch/fairseq/tree/master/examples/translation
⁴https://github.com/neulab/compare-mt

--- TRANG 4 ---
[Bảng 1: Sự khác biệt trong điểm BLEU cho mỗi đầu của cơ chế self attention của encoder. Các số được gạch dưới cho biết sự thay đổi có ý nghĩa thống kê với p < 0.01. Điểm BLEU cơ sở là 36.05.]

[Bảng 2: Delta BLEU tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình WMT. Các số được gạch dưới cho biết sự thay đổi có ý nghĩa thống kê với p < 0.01.]

[Bảng 3: Delta độ chính xác tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình BERT. Không có kết quả nào có ý nghĩa thống kê với p < 0.01.]

Hình 1a và 1b cho thấy phân bố của các đầu về điểm số của mô hình sau khi mask nó, cho WMT và BERT tương ứng. Chúng tôi quan sát thấy rằng phần lớn các đầu chú ý có thể được loại bỏ mà không lệch quá xa so với điểm số ban đầu. Đáng ngạc nhiên, trong một số trường hợp loại bỏ một đầu chú ý dẫn đến sự gia tăng BLEU/độ chính xác.

Để có cái nhìn chi tiết hơn về những kết quả này, chúng tôi phóng to vào các lớp encoder self-attention của mô hình WMT trong Bảng 1. Đáng chú ý, chúng tôi thấy rằng chỉ có 8 (trong số 96) đầu gây ra sự thay đổi có ý nghĩa thống kê về hiệu suất khi chúng được loại bỏ khỏi mô hình, một nửa trong số đó thực sự dẫn đến điểm BLEU cao hơn. Điều này dẫn chúng tôi đến quan sát đầu tiên: tại thời điểm kiểm tra, hầu hết các đầu là dư thừa so với phần còn lại của mô hình.

3.3 Ablating Tất Cả Các Đầu Trừ Một
Quan sát này đặt ra câu hỏi: liệu có cần nhiều hơn một đầu không? Do đó, chúng tôi tính toán sự khác biệt về hiệu suất khi tất cả các đầu ngoại trừ một được loại bỏ, trong một lớp đơn. Trong Bảng 2 và Bảng 3, chúng tôi báo cáo điểm số tốt nhất cho mỗi lớp trong mô hình, tức là điểm số khi giảm toàn bộ lớp xuống chỉ một đầu quan trọng nhất.

Chúng tôi phát hiện rằng, đối với hầu hết các lớp, một đầu thực sự đủ tại thời điểm kiểm tra, mặc dù mạng đã được huấn luyện với 12 hoặc 16 đầu chú ý. Điều này đáng chú ý vì những lớp này có thể được giảm xuống single-headed attention chỉ với 1/16 (tương ứng 1/12) số tham số của một lớp vanilla attention. Tuy nhiên, một số lớp cần nhiều đầu chú ý; ví dụ, thay thế lớp cuối trong encoder-decoder attention của WMT bằng một đầu đơn làm giảm hiệu suất ít nhất 13.5 điểm BLEU. Chúng tôi phân tích thêm khi các thành phần mô hình khác nhau phụ thuộc vào nhiều đầu hơn trong §5.

Ngoài ra, chúng tôi xác minh rằng kết quả này vẫn đúng ngay cả khi chúng tôi không có quyền truy cập vào tập đánh giá khi lựa chọn đầu "tốt nhất riêng lẻ". Với mục đích này, chúng tôi lựa chọn đầu tốt nhất cho mỗi lớp trên tập validation (newstest2013 cho WMT và một tập con 5,000 được chọn ngẫu nhiên từ tập huấn luyện của MNLI cho BERT) và đánh giá hiệu suất của mô hình trên tập kiểm tra

--- TRANG 5 ---
[Hình 2: Phân tích chéo nhiệm vụ về tác động của việc cắt giảm lên độ chính xác]

(newstest2014 cho WMT và tập validation MNLI-matched cho BERT). Chúng tôi quan sát rằng những phát hiện tương tự vẫn đúng: chỉ giữ một đầu không dẫn đến sự thay đổi có ý nghĩa thống kê về hiệu suất cho 50% (tương ứng 100%) lớp của WMT (tương ứng BERT). Kết quả chi tiết có thể tìm thấy trong Phụ lục A.

3.4 Liệu Các Đầu Quan Trọng Giống Nhau Trên Các Tập Dữ Liệu?
Có một lưu ý đối với hai thí nghiệm trước của chúng tôi: những kết quả này chỉ có giá trị trên các tập kiểm tra cụ thể (và khá nhỏ), đặt ra nghi ngờ về khả năng tổng quát hóa của chúng lên các tập dữ liệu khác. Như bước đầu tiên để hiểu liệu một số đầu có quan trọng phổ quát không, chúng tôi thực hiện nghiên cứu ablation tương tự trên tập kiểm tra thứ hai, ngoài miền. Cụ thể, chúng tôi xem xét tập validation MNLI "mismatched" cho BERT và tập kiểm tra MTNT English to French (Michel và Neubig, 2018) cho mô hình WMT, cả hai đều được tập hợp với mục đích rất cụ thể là cung cấp các bộ kiểm tra đối chiếu, ngoài miền cho các nhiệm vụ tương ứng.

Chúng tôi thực hiện nghiên cứu ablation tương tự như §3.2 trên mỗi tập dữ liệu này và báo cáo kết quả trong Hình 2a và 2b. Chúng tôi nhận thấy rằng có mối tương quan dương, >0.5 (p < 0.01) giữa tác động của việc loại bỏ một đầu trên cả hai tập dữ liệu. Hơn nữa, các đầu có tác động cao nhất lên hiệu suất trên một miền có xu hướng có cùng tác động trên miền khác, điều này gợi ý rằng các đầu quan trọng nhất từ §3.2 thực sự quan trọng "phổ quát".

4 Cắt Giảm Lặp Lại Các Đầu Chú Ý
Trong các thí nghiệm ablation của chúng tôi (§3.2 và §3.3), chúng tôi quan sát tác động của việc loại bỏ một hoặc nhiều đầu trong một lớp đơn, không xem xét điều gì sẽ xảy ra nếu chúng tôi thay đổi hai hoặc nhiều lớp khác nhau cùng lúc. Để kiểm tra tác động tổng hợp của việc cắt giảm nhiều đầu từ toàn bộ mô hình, chúng tôi sắp xếp tất cả các đầu chú ý trong mô hình theo điểm số tầm quan trọng proxy (được mô tả dưới đây), và sau đó loại bỏ các đầu từng cái một. Chúng tôi sử dụng cách tiếp cận lặp lại, heuristic này để tránh tìm kiếm tổ hợp, điều này không thực tế do số lượng đầu và thời gian cần thiết để đánh giá mỗi mô hình.

--- TRANG 6 ---
[Hình 3: Sự phát triển của độ chính xác theo số lượng đầu được cắt giảm]

4.1 Điểm Số Tầm Quan Trọng Đầu để Cắt Giảm
Như điểm số proxy cho tầm quan trọng đầu, chúng tôi xem xét độ nhạy cảm kỳ vọng của mô hình với các biến mask ξₕ được định nghĩa trong §2.3:

Iₕ = E_x∈X |∂L(x)/∂ξₕ|                                    (2)

trong đó X là phân bố dữ liệu và L(x) là loss trên mẫu x. Trực quan, nếu Iₕ có giá trị cao thì việc thay đổi ξₕ có thể có tác động lớn lên mô hình. Đặc biệt, chúng tôi thấy giá trị tuyệt đối là rất quan trọng để tránh các điểm dữ liệu với đóng góp rất âm hoặc dương triệt tiêu lẫn nhau trong tổng. Thay Phương trình 1 vào Phương trình 2 và áp dụng chain rule cho biểu thức cuối cùng cho Iₕ:

Iₕ = E_x∈X |Attₕ(x)ᵀ ∂L(x)/∂Attₕ(x)|

Công thức này gợi nhớ đến văn học phong phú về cắt giảm mạng nơ-ron (LeCun et al., 1990; Hassibi và Stork, 1993; Molchanov et al., 2017, inter alia). Đặc biệt, nó tương đương với phương pháp Taylor expansion từ Molchanov et al. (2017).

Về hiệu suất, ước tính Iₕ chỉ yêu cầu thực hiện một forward và backward pass, và do đó không chậm hơn so với huấn luyện. Trong thực tế, chúng tôi tính kỳ vọng trên dữ liệu huấn luyện hoặc một tập con của nó.⁵ Như được khuyến nghị bởi Molchanov et al. (2017), chúng tôi chuẩn hóa điểm số tầm quan trọng theo lớp (sử dụng ℓ₂ norm).

4.2 Tác Động của Cắt Giảm lên BLEU/Độ Chính Xác
Hình 3a (cho WMT) và 3b (cho BERT) mô tả tác động của việc cắt giảm đầu chú ý lên hiệu suất mô hình trong khi loại bỏ dần 10% tổng số đầu theo thứ tự Iₕ tăng dần tại mỗi bước. Chúng tôi cũng báo cáo kết quả khi thứ tự cắt giảm được xác định bởi sự khác biệt điểm số từ §3.2 (trong đường nét đứt), nhưng thấy rằng việc sử dụng Iₕ nhanh hơn và cho kết quả tốt hơn.

Chúng tôi quan sát rằng cách tiếp cận này cho phép chúng tôi cắt giảm tới 20% và 40% đầu từ WMT và BERT (tương ứng), mà không phải chịu bất kỳ tác động tiêu cực đáng chú ý nào. Hiệu suất giảm mạnh khi cắt giảm thêm, có nghĩa là không mô hình nào có thể được giảm xuống mô hình attention hoàn toàn single-head mà không cần huấn luyện lại hoặc phải chịu tổn thất đáng kể về hiệu suất. Chúng tôi tham khảo Phụ lục B cho các thí nghiệm trên bốn tập dữ liệu bổ sung.

⁵Đối với mô hình WMT, chúng tôi sử dụng tất cả các tập newstest20[09-12] để ước tính I.

--- TRANG 7 ---
4.3 Tác Động của Cắt Giảm lên Hiệu Quả
Ngoài hiệu suất tác vụ downstream, có những lợi thế nội tại của việc cắt giảm đầu. Đầu tiên, mỗi đầu đại diện cho một tỷ lệ không đáng kể của tổng tham số trong mỗi lớp attention (6.25% cho WMT, 8.34% cho BERT), và do đó của tổng mô hình (nói chung, trong cả hai mô hình của chúng tôi, khoảng một phần ba tổng số tham số được dành cho MHA trên tất cả các lớp).⁶ Điều này hấp dẫn trong bối cảnh triển khai các mô hình trong các môi trường hạn chế bộ nhớ.

[Bảng 4: Tốc độ suy luận trung bình của BERT trên tập validation MNLI-matched]

Hơn nữa, chúng tôi thấy rằng thực sự cắt giảm các đầu (và không chỉ masking) dẫn đến sự gia tăng đáng kể về tốc độ suy luận. Bảng 4 báo cáo số lượng ví dụ mỗi giây được xử lý bởi BERT, trước và sau khi cắt giảm 50% tất cả các đầu chú ý. Các thí nghiệm được thực hiện trên hai máy khác nhau, cả hai đều trang bị GPU GeForce GTX 1080Ti. Mỗi thí nghiệm được lặp lại 3 lần trên mỗi máy (tổng cộng 6 điểm dữ liệu cho mỗi thiết lập). Chúng tôi thấy rằng việc cắt giảm một nửa đầu của mô hình tăng tốc suy luận lên tới 17.5% cho kích thước batch cao hơn (sự khác biệt này biến mất cho kích thước batch nhỏ hơn).

5 Khi Nào Nhiều Đầu Quan Trọng? Trường Hợp Dịch Máy
Như được hiển thị trong Bảng 2, không phải tất cả các lớp MHA đều có thể được giảm xuống một đầu chú ý đơn mà không ảnh hưởng đáng kể đến hiệu suất. Để có ý tưởng tốt hơn về mức độ mỗi phần của mô hình dịch dựa trên transformer phụ thuộc vào multi-headedness, chúng tôi lặp lại thí nghiệm cắt giảm heuristic từ §4 cho mỗi loại attention riêng biệt (Enc-Enc, Enc-Dec, và Dec-Dec).

Hình 4 cho thấy rằng hiệu suất giảm nhanh hơn nhiều khi các đầu được cắt giảm từ các lớp attention Enc-Dec. Cụ thể, việc cắt giảm hơn 60% các đầu attention Enc-Dec sẽ dẫn đến sự suy giảm hiệu suất thảm khốc, trong khi các lớp self-attention encoder và decoder vẫn có thể tạo ra các bản dịch hợp lý (với điểm BLEU khoảng 30) chỉ với 20% các đầu attention ban đầu. Nói cách khác, encoder-decoder attention phụ thuộc vào multi-headedness nhiều hơn self-attention.

[Hình 4: BLEU khi cắt giảm dần các đầu từ mỗi loại attention trong mô hình WMT]

⁶Hơi nhiều hơn trong WMT vì attention Enc-Dec.

--- TRANG 8 ---
[Hình 5: Mối quan hệ giữa tỷ lệ phần trăm đầu được cắt giảm và giảm điểm số tương đối trong quá trình huấn luyện]

6 Động Lực Tầm Quan Trọng Đầu Trong Quá Trình Huấn Luyện
Các phần trước cho chúng tôi biết rằng một số đầu quan trọng hơn những đầu khác trong các mô hình đã được huấn luyện. Để có cái nhìn sâu sắc hơn về động lực tầm quan trọng đầu trong quá trình huấn luyện, chúng tôi thực hiện thí nghiệm cắt giảm lặp lại tương tự của §4.2 tại mỗi epoch. Chúng tôi thực hiện thí nghiệm này trên phiên bản nhỏ hơn của mô hình WMT (6 lớp và 8 đầu mỗi lớp), được huấn luyện cho dịch German to English trên tập dữ liệu IWSLT 2014 nhỏ hơn Cettolo et al. (2015). Chúng tôi gọi mô hình này là IWSLT.

Hình 5 báo cáo, cho mỗi mức độ cắt giảm (theo gia số 10% — 0% tương ứng với mô hình ban đầu), sự phát triển của điểm số mô hình (trên newstest2013) cho mỗi epoch. Để dễ đọc hơn, chúng tôi hiển thị các epoch trên thang logarit, và chỉ báo cáo điểm số mỗi 5 epoch sau epoch thứ 10. Để làm cho điểm số có thể so sánh được qua các epoch, trục Y báo cáo sự suy giảm tương đối của điểm BLEU so với mô hình không cắt giảm tại mỗi epoch. Đặc biệt, chúng tôi thấy rằng có hai chế độ riêng biệt: trong các epoch rất sớm (đặc biệt là 1 và 2), hiệu suất giảm tuyến tính với tỷ lệ phần trăm cắt giảm, tức là sự giảm tương đối về hiệu suất độc lập với Ih, cho thấy rằng hầu hết các đầu đều ít nhiều quan trọng như nhau. Từ epoch 10 trở đi, có sự tập trung của các đầu không quan trọng có thể được cắt giảm trong khi vẫn giữ trong vòng 85-90% điểm BLEU ban đầu (lên tới 40% tổng số đầu).

Điều này gợi ý rằng các đầu quan trọng được xác định sớm (nhưng không ngay lập tức) trong quá trình huấn luyện. Hai giai đoạn huấn luyện gợi nhớ đến phân tích của Shwartz-Ziv và Tishby (2017), theo đó việc huấn luyện các mạng nơ-ron phân tách thành giai đoạn "tối thiểu hóa rủi ro thực nghiệm", trong đó mô hình tối đa hóa thông tin tương hỗ của các biểu diễn trung gian với các nhãn, và giai đoạn "nén" trong đó thông tin tương hỗ với đầu vào được tối thiểu hóa. Một điều tra có nguyên tắc hơn về hiện tượng này được để lại cho công việc tương lai.

7 Công Trình Liên Quan
Việc sử dụng cơ chế attention trong NLP và đặc biệt là dịch máy nơ-ron (NMT) có thể được truy nguyên về Bahdanau et al. (2015) và Cho et al. (2014), và hầu hết các triển khai đương thời đều dựa trên công thức từ Luong et al. (2015). Attention sau đó được điều chỉnh (thành công) cho các tác vụ NLP khác, thường đạt được hiệu suất tốt nhất khi đó trong đọc hiểu (Cheng et al., 2016), suy luận ngôn ngữ tự nhiên (Parikh et al., 2016) hoặc tóm tắt trừu tượng (Paulus et al., 2017) để kể một vài ví dụ. Multi-headed attention lần đầu tiên được giới thiệu bởi Vaswani et al. (2017) cho NMT và phân tích cú pháp tiếng Anh, và sau đó được áp dụng cho transfer learning (Radford et al., 2018; Devlin et al., 2018), mô hình hóa ngôn ngữ (Dai et al., 2019; Radford et al., 2019), hoặc gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018), trong số những ứng dụng khác.

Có một văn học phong phú về cắt giảm các mạng nơ-ron đã được huấn luyện, quay trở lại LeCun et al. (1990) và Hassibi và Stork (1993) vào đầu những năm 90 và được làm mới sau sự ra đời của deep learning, với hai cách tiếp cận trực giao: cắt giảm "từng trọng số" tinh vi (Han et al., 2015) và cắt giảm có cấu trúc (Anwar et al., 2017; Li et al., 2016; Molchanov et al., 2017), trong đó toàn bộ các phần của mô hình được cắt giảm. Trong NLP, cắt giảm có cấu trúc cho auto-sizing feed-forward language models lần đầu tiên được điều tra bởi Murray và Chiang (2015). Gần đây hơn, các cách tiếp cận cắt giảm tinh vi đã được phổ biến bởi See et al. (2016) và Kim và Rush (2016) (chủ yếu trên NMT).

Đồng thời với công việc của chúng tôi, Voita et al. (2019) đã đưa ra quan sát tương tự về multi-head attention. Cách tiếp cận của họ liên quan đến việc sử dụng LRP (Binder et al., 2016) để xác định các đầu quan trọng và xem xét các thuộc tính cụ thể như chú ý đến các vị trí liền kề, từ hiếm hoặc từ liên quan cú pháp. Họ đề xuất một cơ chế cắt giảm thay thế dựa trên việc thực hiện gradient descent trên các biến mask ξh. Trong khi cách tiếp cận và kết quả của họ bổ sung cho bài báo này, nghiên cứu của chúng tôi cung cấp bằng chứng bổ sung về hiện tượng này ngoài NMT, cũng như phân tích về động lực huấn luyện của việc cắt giảm các đầu attention.

8 Kết Luận
Chúng tôi đã quan sát rằng MHA không phải lúc nào cũng tận dụng khả năng biểu diễn vượt trội về mặt lý thuyết so với vanilla attention một cách trọn vẹn. Cụ thể, chúng tôi đã chứng minh rằng trong nhiều tình huống khác nhau, một số đầu có thể được loại bỏ khỏi các mô hình transformer đã được huấn luyện mà không có sự suy giảm có ý nghĩa thống kê về hiệu suất kiểm tra, và một số lớp có thể được giảm xuống chỉ còn một đầu. Ngoài ra, chúng tôi đã chỉ ra rằng trong các mô hình dịch máy, các lớp encoder-decoder attention phụ thuộc vào multi-headedness nhiều hơn so với các lớp self-attention, và cung cấp bằng chứng rằng tầm quan trọng tương đối của mỗi đầu được xác định trong các giai đoạn đầu của quá trình huấn luyện. Chúng tôi hy vọng rằng những quan sát này sẽ thúc đẩy sự hiểu biết của chúng tôi về MHA và truyền cảm hứng cho các mô hình đầu tư tham số và attention của chúng hiệu quả hơn.

Lời Cảm Ơn
Các tác giả muốn gửi lời cảm ơn đến các nhà phản biện ẩn danh vì những phản hồi sâu sắc của họ. Chúng tôi cũng đặc biệt biết ơn Thomas Wolf từ Hugging Face, người có nỗ lực tái tạo độc lập đã cho phép chúng tôi tìm ra và sửa một lỗi trong các thí nghiệm so sánh tốc độ. Nghiên cứu này được hỗ trợ một phần bởi món quà từ Facebook.

Tài Liệu Tham Khảo
[Danh sách các tài liệu tham khảo được dịch đầy đủ theo cấu trúc ban đầu]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

A Ablating Tất Cả Các Đầu Trừ Một: Thí Nghiệm Bổ Sung
Bảng 5 và 6 báo cáo sự khác biệt về hiệu suất khi chỉ giữ một đầu cho bất kỳ lớp nào. Đầu được chọn là đầu tốt nhất riêng lẻ trên một tập dữ liệu riêng biệt.

--- TRANG 12 ---
[Bảng 5 và 6 với kết quả thí nghiệm bổ sung]

B Các Thí Nghiệm Cắt Giảm Bổ Sung
Chúng tôi báo cáo kết quả bổ sung cho cách tiếp cận cắt giảm dựa trên tầm quan trọng từ Phần 4 trên 4 tập dữ liệu bổ sung:

SST-2: Phiên bản GLUE của Stanford Sentiment Treebank (Socher et al., 2013). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

CoLA: Phiên bản GLUE của Corpus of Linguistic Acceptability (Warstadt et al., 2018). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

MRPC: Phiên bản GLUE của Microsoft Research Paraphrase Corpus (Dolan và Brockett, 2005). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

IWSLT: Tập dữ liệu dịch German to English từ IWSLT 2014 (Cettolo et al., 2015). Chúng tôi sử dụng mô hình nhỏ hơn tương tự được mô tả trong Phần 6.

Hình 6 cho thấy rằng trong một số trường hợp có thể cắt giảm lên tới 60% (SST-2) hoặc 50% (CoLA, MRPC) các đầu mà không có tác động đáng chú ý lên hiệu suất.

--- TRANG 13 ---
[Hình 6: Sự phát triển điểm số theo tỷ lệ phần trăm đầu được cắt giảm]