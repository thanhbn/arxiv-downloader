# 2112.00029.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2112.00029.pdf
# File size: 2521209 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pixelated ButterÔ¨Çy: Simple and EÔ¨Écient Sparse Training for
Neural Network Models
Tri Dao‚àóy, Beidi Chen‚àóy, Kaizhao Liang, Jiaming Yang, Zhao Songx, Atri Rudraz, and
Christopher R√©y
yDepartment of Computer Science, Stanford University
SambaNova Systems, Inc
Department of Probability and Statistics, Peking University
xAdobe Research
zDepartment of Computer Science and Engineering, University at BuÔ¨Äalo, SUNY
{trid,beidic}@stanford.edu ,kaizhao.liang@sambanovasystems.com ,edwinyjmpku@gmail.com ,
zsong@adobe.com ,atri@buffalo.edu ,chrismre@cs.stanford.edu
May 12, 2022
Abstract
Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like
to reduce their computational cost while retaining their generalization beneÔ¨Åts. Sparse model training
is a simple and promising approach to achieve this, but there remain challenges as existing methods
struggle with accuracy loss, slow training runtime, or diÔ¨Éculty in sparsifying all model components.
The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is diÔ¨Écult
and expensive. To address this, our main insight is to optimize over a continuous superset of sparse
matrices with a Ô¨Åxed structure known as products of butterÔ¨Çy matrices. As butterÔ¨Çy matrices are not
hardware eÔ¨Écient, we propose simple variants of butterÔ¨Çy (block and Ô¨Çat) to take advantage of modern
hardware. Our method (Pixelated ButterÔ¨Çy) uses a simple Ô¨Åxed sparsity pattern based on Ô¨Çat block
butterÔ¨Çy and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically
validate that Pixelated ButterÔ¨Çy is 3faster than butterÔ¨Çy and speeds up training to achieve favorable
accuracy‚ÄìeÔ¨Éciency tradeoÔ¨Äs. On the ImageNet classiÔ¨Åcation and WikiText-103 language modeling tasks,
our sparse models train up to 2.5 faster than the dense MLP-Mixer, Vision Transformer, and GPT-2
medium with no drop in accuracy.
1 Introduction
Recent results suggest that overparameterized neural networks generalize well [Belkin et al., 2019], but
they are expensive to train [Kaplan et al., 2020]. An ideal model should use less compute and memory
while retaining the generalization beneÔ¨Åts of large models. The simplest and most popular direction is to
sparsify these models. This idea has a long history in machine learning [LeCun et al., 1990] and has driven
fundamental progress in other Ô¨Åelds such as statistics [Tibshirani, 1996], neuroscience [Foldiak, 2003], and
signal processing [Candes et al., 2006]. However, despite signiÔ¨Åcant eÔ¨Äorts, speeding up sparse training in
wall-clock time without degrading accuracy remains an unresolved problem.
While sparse training is an active research area, it has not seen wide adoption. First, it is diÔ¨Écult and
expensive to Ô¨Ånd the sparsity pattern (the possible locations of the nonzeros) that could maintain the same
level of accuracy of dense models. Many methods (pruning [Lee et al., 2018], lottery tickets [Frankle and
Carbin,2018], hashing[Kitaevetal.,2020])maintaindynamicsparsitymasks. However, thelargeoverheadof
‚àóEqual contribution. Order determined by coin Ô¨Çip.
1arXiv:2112.00029v2  [cs.LG]  11 May 2022

--- PAGE 2 ---
evolving the sparsity mask can signiÔ¨Åcantly slow down training and complicate the implementation. Indeed,
these methods either require long cycles of pruning and retraining [Frankle and Carbin, 2018]1or maintain
expensive hash tables [Chen et al., 2019]. Second, most existing methods adopt unstructured sparsity,
which may be eÔ¨Écient in theory, but do not take into account the eÔ¨Éciency of training hardware such as
GPUs (optimized for dense computation)2. Finally, most methods target a single type of operation such as
attention[Childetal.,2019,Zaheeretal.,2020], whereasneuralnetwork(NN)modelsoftencomposediÔ¨Äerent
modules (attention, MLP), and in many applications the MLP layers are the main training bottleneck [Wu
et al., 2020].
A better sparse training method should (i) be simple yet accurate, ideally with a static sparsity pattern,
(ii) be fast by aligning sparsity pattern with available hardware, and (iii) have wide coverage of operators
that applies to most NN layers. There are three technical challenges. First, we show that given a budget
(e.g., total non-zeros in a matrix), it is NP-hard to Ô¨Ånd the optimal static sparsity pattern for a NN module
to minimize the approximation error to the dense model. Second, for each sparsity pattern, we need to take
into account hardware block-oriented eÔ¨Éciency (accessing each element in memory takes the same time as
accessing the block of adjacent elements [Cook, 2012], illustrated in Fig. 2). Common theoretical measures
of eÔ¨Éciency (e.g., number of non-zeros, FLOPs) do not map well to modern hardware designed for block
computation. Last, every diÔ¨Äerent NN module might require diÔ¨Äerent sparsity patterns, which makes the
problem even more complicated.
In our early exploration, we empirically study many sparsity patterns proposed in the literature to Ô¨Ånd
those patterns that can closely approximate the dense model (Details in Appendix K). We found that one
sparsity pattern, namely butterÔ¨Çy + low-rank, consistently outperforms the others. This sparsity pattern
closely connects to two lines of work in matrix structures: (i) sparse + low-rank matrices, which can capture
global and local information [Cand√®s et al., 2011, Udell and Townsend, 2019, Chen et al., 2021], and (ii) but-
terÔ¨Çy matrices [Parker, 1995, Dao et al., 2019] whose products can tightly represent any sparse matrix [De Sa
et al., 2018, Dao et al., 2020]. Using the Ô¨Åxed sparsity pattern from butterÔ¨Çy matrices, with the addition of
a low-rank term, would address two of the three challenges above and yield a simple way to sparsify most
NN layers (that are based on matrix multiply).
However, butterÔ¨Çy matrices are ineÔ¨Écient on modern hardware: (i) they are diÔ¨Écult to parallelize as they
contain sequential products of many factors, and (ii) they are not hardware-friendly because the sparsity
patterns are not block-aligned. We propose two simple changes to make ButterÔ¨Çy eÔ¨Écient while retaining
their favorable properties. Our proposal, Pixelated ButterÔ¨Çy (PixelÔ¨Çy), combines Ô¨Çat block butterÔ¨Çy and
low-rank matrices to yield a simple and eÔ¨Écient sparse training method.
‚Ä¢We design an extremely simple sparsity pattern inspired by butterÔ¨Çy + low-rank matrices, which takes into
account the hardware‚Äôs block-oriented eÔ¨Éciency. We propose block butterÔ¨Çy matrices that are eÔ¨Écient
as their sparsity patterns align with hardware blocks. We then introduce Ô¨Çat butterÔ¨Çy, a Ô¨Årst-order
approximation of butterÔ¨Çy with residual connection, that turns the original product of factors into a sum.
Flat butterÔ¨Çy matrix multiplications are easy to parallelize. PixelÔ¨Çy, uses the Ô¨Åxed sparsity pattern from
Ô¨Çat & block butterÔ¨Çy, along with a low-rank term, to produce a sparse network.
‚Ä¢We prove that block butterÔ¨Çy retains the expressiveness of butterÔ¨Çy matrices and can thus tightly capture
sparse matrices. We show that Ô¨Çat butterÔ¨Çy matrices can closely approximate large classes of matrices that
butterÔ¨Çy matrices capture. Moreover, we demonstrate that Ô¨Çat block butterÔ¨Çy + low-rank matrices are
strictly more expressive than sparse or low-rank matrices alone. Finally, leveraging the recent advance in
the neural tangent kernel (NTK), we adapt existing techniques to prove the global convergence of gradient
descent on training sparse and wide ReLU networks.
‚Ä¢Our proposed PixelÔ¨Çy can be applied to all network modules that rely on matrix multiplication (e.g., linear
layer, attention, MLP). To sparsify a full network, we simply need to allocate compute budget for each
layer based on matrix and hardware block size.
We empirically validate that PixelÔ¨Çy can speed up the training of models (Transformers, ViT, MLP-
Mixer) without quality drop compared to baselines on a wide range of domains and tasks. On CIFAR10/100
& ImageNet classiÔ¨Åcation, PixelÔ¨Çy achieve 2.3 training time speedup compared to dense ViT, MLP-Mixer
models, and other sparse training baselines, while preserving the same accuracy. On the WikiText-103
1State-of-the-art sparse training methods require up to 5 more training epochs compared to dense models [Evci et al.,
2020]
2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]
2

--- PAGE 3 ---
Pixelated Butterfly  
Attention
MLP Flat Block Butterfly+Model Schema  
Compute Allocation+
Low-rankAttention Mask
MLP MaskSparse Masks  
Figure 1: PixelÔ¨Çy targets GEMM-based networks (networks whose computation is dominated by matrix multiply),
which it views as a series of matrix multiplication. For each matrix multiply from Model Schema, it (1) allocates
compute budget based on dimension and layer type, (2) the budget decides a mapping (hyper-parameter) to our
proposed Ô¨Çat block butterÔ¨Çy sparsity patterns, (3) outputs a hardware-aware sparse mask. Note since the hardware
is a block device, one memory access to an element in a block leads to the access to the full block.
language modeling task, we speed up GPT-2 Medium training by 2.5 and achieve the same perplexity. On
the Long Range Arena benchmark, we maintain the same accuracy as Transformer with 5:2faster training
than a dense model, 2faster than Sparse transformer, and 6faster than non-block-aligned sparse methods
(Reformer). Our ablation studies highlight the importance of each of our components: our butterÔ¨Çy sparsity
improves on existing hand-crafted patterns by up to 2% of accuracy on ImageNet, our hardware-aware block-
sparsity yields up to 5 speedup, and the balanced compute budget allocation brings 2 speedup compared
to baselines that only sparsify attention.3
2 Problem Setting
We Ô¨Årst deÔ¨Åne the problem as sparse matrix approximation with a simple hardware cost model. Then we
brieÔ¨Çy introduce butterÔ¨Çy and sparse + low-rank matrices.
Memory Access  
Figure 2: Visualization of
memory access for a hard-
ware with block size 4: ac-
cessing the one (red) location
means accessing the full 44
block (blue).ProblemFormulation: WefocusonthetrainingofGEMM-basedmodels,
which can be viewed as a series of matrix multiplies (Given A;B2Rnd,
computeC=ABT). Speeding up training while maintaining model quality
can be mapped to Ô¨Ånding an approximation procedure fwhich reduces the
timeTof computing Cwhile minimizing error E[kf(A;B) ABTk2
F]. Since
the hardware is a block device, accessing any individual element within a block
ofmemoryisthesameasaccessingthefullblock[Cook,2012](Fig.2). Asimple
cost model of Ton hardware with block size bwould depend on the number of
b-blocks being accessed and compute time (formal deÔ¨Ånition in Appendix A).
Our experiment (Appendix L.5) reveals that when the non-zeros are grouped
into blocks, picking the smallest block size supported by hardware can speed
up operations by 10 compared to sparsity patterns that are not block-aligned.
ButterÔ¨Çy, Sparse + Low-rank Matrices: ButterÔ¨Çy matrices have been used in numerical linear
algebra [Parker, 1995, Li et al., 2015] and machine learning [Mathieu and LeCun, 2014, Jing et al., 2017,
Munkhoeva et al., 2018, Dao et al., 2019, Choromanski et al., 2019]. They encode the recursive divide-and-
conquer structure of the fast Fourier transform (FFT) algorithm [Cooley and Tukey, 1965] and provably
capture any sparse matrix with near-optimal space and time complexity. Sparse and Low-rank structures
have been studied in Robust PCA [Cand√®s et al., 2011], graph clustering [Jalali et al., 2011], and co-variance
estimation [Luo, 2011]. Recently it has been adopted in attention approximation for Transformers [Chen
et al., 2021].
3 ButterÔ¨Çy matrices and Pixelated ButterÔ¨Çy
ButterÔ¨Çy matrices [Parker, 1995, Dao et al., 2019] are expressive and theoretically eÔ¨Écient. As they contain
the set of sparse matrices, we choose to search for the sparsity pattern in this larger class due to their Ô¨Åxed
3PixelÔ¨Çy code is available at https://github.com/HazyResearch/pixelfly
3

--- PAGE 4 ---
Butterfly  
Flat Butterfly  + + +
++
Flat Block Butterfly  
Block Butterfly  
+ + ++
+Figure 3: Visualization of Flat, Block, and Flat Block butterÔ¨Çy.
sparsity structure. However, there are three technical challenges. We highlight them here along with our
approaches to address them:
1. Slow speed: butterÔ¨Çy matrices are not friendly to modern hardware as their sparsity patterns are not
block-aligned, thus are slow. We introduce a variant of butterÔ¨Çy matrices, block butterÔ¨Çy , which operate
at the block level, yielding a block-aligned sparsity pattern.
2. DiÔ¨Éculty of parallelization: the sequential nature of butterÔ¨Çy matrices as products of many factors makes
it hard to parallelize the multiplication. We propose another class of matrices, Ô¨Çat butterÔ¨Çy matrices, that
are the Ô¨Årst-order approximation of butterÔ¨Çy with residual connections. Flat butterÔ¨Çy turns the product
of factors into a sum, facilitating parallelization.
3. Reduced expressiveness of Ô¨Çat butterÔ¨Çy: even though Ô¨Çat butterÔ¨Çy matrices can approximate butterÔ¨Çy
matrices with residual connections, they are necessarily high-rank and cannot represent low-rank matri-
ces [Udell and Townsend, 2019]. We propose to add a low-rank matrix (that is also block-aligned) to Ô¨Çat
butterÔ¨Çy to increase their expressiveness.
Combining these three approaches (Ô¨Çat & block butterÔ¨Çy + low-rank), our proposal (Pixelated ButterÔ¨Çy) is
a very simple method to train sparse networks.
3.1 Block ButterÔ¨Çy Matrices
We propose a block version of butterÔ¨Çy matrices, which is more hardware-friendly than the regular butterÔ¨Çy.
The regular butterÔ¨Çy matrices Dao et al. [2019, 2020] will be a special case of block butterÔ¨Çy with block size
b= 1. We omitbin the notation if b= 1.
DeÔ¨Ånition 3.1. Ablock butterÔ¨Çy factor (denoted as Bk;b) of sizekb(wherek2) and block size bis a
matrix of the form Bk;b=D1D2
D3D4
where each Diis ak
2k
2block diagonal matrix of block size bof the
form diag 
Di;1;:::;Di;k=2
whereDi;j2Rbb. We restrict kto be a power of 2.
DeÔ¨Ånition 3.2. Ablock butterÔ¨Çy factor matrix (denoted as B(n;b)
k) of sizenbwith stride kand block
sizebis a block diagonal matrix ofn
k(possibly diÔ¨Äerent) butterÔ¨Çy factors of size kband block size b:
B(n;b)
k= diag
[Bk;b]1;[Bk;b]2;:::; [Bk;b]n
k
DeÔ¨Ånition 3.3. Ablock butterÔ¨Çy matrix of sizenbwith block size b(denoted as B(n;b)) is a matrix that
can be expressed as a product of butterÔ¨Çy factor matrices: B(n;b)=B(n;b)
nB(n;b)
n
2:::B(n;b)
2:DeÔ¨ÅneBbas the
set of all matrices that can be expressed in the form B(n;b)(for somen).
3.2 Flat butterÔ¨Çy matrices
In most applications of butterÔ¨Çy matrices to neural networks, one multiplies the O(logn)butterÔ¨Çy factors.
However, this operation is hard to be eÔ¨Éciently implemented on parallel hardware (e.g., GPUs) due to
4

--- PAGE 5 ---
the sequential nature of the operation4. We instead propose to use a sum of butterÔ¨Çy factors that can
approximate the products of the factors. This sum of factors results in one sparse matrix with a Ô¨Åxed
sparsity pattern, which yields up to 3 faster multiplication on GPUs (Appendix J).
Residual connections have been proposed to connect the butterÔ¨Çy factors [Vahid et al., 2020]. We show
that residual products of butterÔ¨Çy matrices have a Ô¨Årst-order approximation as a sparse matrix with a Ô¨Åxed
sparsity. Let Mbe a matrix in the set of butterÔ¨Çy matrices B. In residual form, for some 2R:
M= (I+B(n)
n)(I+B(n)
n=2):::(I+B(n)
2): (1)
Note that this form can represent the same matrices in the class of butterÔ¨Çy matrices B, since any B(n)
k
contains the identity matrix I.
Assuming that is small, we can expand the residual and collect the terms5:
M=I+(B(n)
2+B(n)
4++B(n)
n) +eO(2):
DeÔ¨Ånition 3.4. Flat butterÔ¨Çy matrices of maximum stride k(forka power of 2) are those of the form
I+(B(n)
2+B(n)
4++B(n)
k).
Flat butterÔ¨Çy matrices of maximum stride nare the Ô¨Årst-order approximation of butterÔ¨Çy matrices in
residual form (Eq. (1)). Notice that Ô¨Çat butterÔ¨Çy of maximum stride kare sparse matrices with O(nlogk)
nonzeros with a Ô¨Åxed sparsity pattern, as illustrated in Fig. 3. We call this sparsity pattern the Ô¨Çat butterÔ¨Çy
pattern.
Flat block butterÔ¨Çy matrices are block versions of Ô¨Çat butterÔ¨Çy in Section 3.2 (shown in Fig. 3). We
empirically validate that Ô¨Çat block butterÔ¨Çy matrices are up to 3 faster than block butterÔ¨Çy or regular
butterÔ¨Çy (Appendix J).
Since Ô¨Çat butterÔ¨Çy matrices approximate the residual form of butterÔ¨Çy matrices, they have high rank if
is small (Section 4). This is one of the motivations for the addition of the low-rank term in our method.
3.3 Pixelated ButterÔ¨Çy: Flat Block ButterÔ¨Çy + Low-rank for EÔ¨Écient Sparse
Training
We present Pixelated ButterÔ¨Çy, an eÔ¨Écient sparse model with a simple and Ô¨Åxed sparsity pattern based on
butterÔ¨Çy and low-rank matrices. Our method targets GEMM-based neural networks, which are networks
whose computation is dominated by general matrix multiplies (GEMM), such as Transformer and MLP-
Mixer. As a result, we can view the network as a series of matrix multiplies.
Given a model schema (layer type, number of layers, matrix dimension) and a compute budget, Pixelated
ButterÔ¨Çy has three steps: compute budget allocation per layer, sparsity mask selection from the Ô¨Çat butterÔ¨Çy
pattern, and model sparsiÔ¨Åcation. We describe these steps in more details:
1.Compute budget allocation : based on our cost model (Appendix A), given the layer type, number
of layers, and matrix dimension, we can Ô¨Ånd the density (fraction of nonzero weights) of each layer type
to minimize the projected compute cost. Continuing our goal for a simple method, we propose to use a
simple rule of thumb: allocate sparsity compute budget proportional to the compute fraction of the layer.
For example, if the MLP layer and attention layers are projected to takes 60% and 40% the compute time
respectively, then allocate 60% of the sparsity compute budget to MLP and 40% to attention. We verify
in Appendix I that this simple rule of thumb produces similar results to solving for the density from the
cost model.
2.Sparsity mask selection : given a layer and a sparsity compute budget for that layer, we use one-
quarter to one-third of the budget for the low-rank part as a simple rule of thumb. We pick the rank as
a multiple of the smallest supported block size of the device (e.g., 32) so that the low-rank matrices are
also block-aligned. The remaining compute budget is used to select the sparsity mask from the Ô¨Çat block
butterÔ¨Çy sparsity pattern: we choose the butterÔ¨Çy block size as the smallest supported block size of the
device (e.g., 32), and pick the maximum stride of the Ô¨Çat block butterÔ¨Çy (DeÔ¨Ånition 3.4) to Ô¨Åll up the
budget.
4Even with a very specialized CUDA kernel, butterÔ¨Çy matrix multiply ( O(nlogn)complexity) is only faster than dense
matrix multiply ( O(n2)complexity) for large values of n(around 1024) [Dao et al., 2019].
5We make the approximation rigorous in Section 4.
5

--- PAGE 6 ---
3.Model sparsiÔ¨Åcation : The resulting sparse model is simply a model whose weights or attention follow
the Ô¨Åxed sparsity mask chosen in step 2, with the additional low-rank terms (rank also chosen in step 2).
In particular, we parameterize each weight matrix6as:W=B+ (1 )UV>, whereBis a Ô¨Çat block
butterÔ¨Çy matrix (which is sparse), UV>is the low-rank component, and is a learnable parameter. We
train the model from scratch as usual.
Ourmethodisverysimple, butcompetitivewithmorecomplicatedproceduresthatsearchforthesparsity
pattern (Appendix K). We expect more sophisticated techniques (dynamic sparsity, a better approximation
of butterÔ¨Çy) to improve the accuracy of the method.
4 Theoretical analysis
We characterize the expressiveness of the matrices used in our method. In particular, we prove that block
butterÔ¨Çy retains the expressiveness of butterÔ¨Çy, and that Ô¨Çat butterÔ¨Çy can accurately approximate the
residual form of butterÔ¨Çy. Moreover, Ô¨Çat block butterÔ¨Çy + low-rank (an instance of sparse + low-rank) is
more expressive than sparse or low-rank matrices alone. Finally, we analyze the training convergence and
generalization of networks with sparse weights. All proofs are in the Appendix.
4.1 Expressiveness of Block ButterÔ¨Çy
We Ô¨Årst prove the expressiveness of block butterÔ¨Çy matrices.
Theorem 4.1. The set B2bofnnblock butterÔ¨Çy matrices with block size 2bcontains the set Bbofnn
block butterÔ¨Çy matrices of block size b.
By a recursive argument, the set of block butterÔ¨Çy matrices whose block size is a power of 2 contains the
set of regular butterÔ¨Çy matrices.
Dao et al. [2020] show that butterÔ¨Çy matrices can tightly represent all structured matrices, such as sparse
matrices and many fast transforms. As a result, block butterÔ¨Çy matrices can also represent those structured
matrices. In particular,
Corollary 4.2. For any constant block size bthat is a power of 2, any nbnbspare matrix with snonzeros
can be written as products of block butterÔ¨Çy matrices with block size band their transposes, with O(slogn)
parameters.
4.2 Expressiveness of Flat ButterÔ¨Çy
We now characterize how the Ô¨Çat butterÔ¨Çy matrices approximate butterÔ¨Çy matrices. In particular, assuming
thateachbutterÔ¨Çyfactorhasboundednorm, weshowthatÔ¨Çat-butterÔ¨Çymatricescanaccuratelyapproximate
the residual form of butterÔ¨Çy with error scaling as eO(2).
Theorem 4.3. LetMbe a matrix of the form in DeÔ¨Ånition 3.4 where k=n, withBmax:= maxiB(n)
i
F
andjjcp
lognBmaxfor some constant 0<c1
2and some>0. Then
M 
I+(B(n)
2+B(n)
4++B(n)
n)
F:
We show that Ô¨Çat butterÔ¨Çy matrices must have high-rank if is small. This is the motivation for the
addition of the low-rank term in PixelÔ¨Çy (Section 3).
Theorem 4.4. LetMbe as in Eq. (1), withBmax:= maxiB(n)
i
Fandjjcp
lognBmaxfor some constant
0<c1
4and some>0. LetB1
max= maxikBik1. Assuming B1
maxBmax. Then
rank(I+(B(n)
2++B(n)
n)) = 
0
@Bmax
B1max2
logn
log
Bmax
B1max1
A:
6We describe how to add sparse and low-rank for attention in Appendix I
6

--- PAGE 7 ---
4.3 Expressiveness of Flat Block ButterÔ¨Çy + Low-rank
Chen et al. [2021] prove that there is a natural class of input sequences (generated by a clustering process)
whose attention matrix can only be approximated well by sparse + low-rank matrices, and not sparse or
low-rank matrices alone. We adapt their technique to show a similar result for the class of matrices we use
in PixelÔ¨Çy.
We require an extra assumption on the clustering process compared to Chen et al. [2021]: the elements
in the input sequence form clusters with the same size. Then their attention matrix will have a large block
diagonal component well-approximated by Ô¨Çat butterÔ¨Çy, while the rest of the attention matrix is of medium
size and is well-approximated by low-rank.
Theorem4.5 (Informal) .Thereexists aclass ofinput sequences whoseattention matricesare well-approximated
by Ô¨Çat block butterÔ¨Çy + low-rank (a special case of sparse + low-rank) but not by sparse or low-rank alone.
The formal theorem statement and proof are in Appendix B.3.
4.4 Convergence and Generalization of Sparse Networks
There are natural questions about the training and generalization of sparse models: do they train similarly
to dense models, is their generalization close to that of dense models, and can one successfully train them
with gradient descent? Our analysis theoretically shows that the answers are yes.
Our analysis relies on the neural tangent kernel (NTK) [Jacot et al., 2018] of the network. The NTK
of two data points xandymeasures the similarity between the gradient of the network when evaluated at
xcompared to the gradient when evaluated at y. This kernel governs the dynamics of the neural network
output function f(;)throughout the training and its generalization. We build on the great literature of
NTK [Li and Liang, 2018, Du et al., 2019, Allen-Zhu et al., 2019b]. The standard result [Song and Yang,
2019] implies the following, if the NTK of the sparse model is close to the NTK of the dense model, then
(i) their training convergence speed is similar, (ii) their generalization bounds are similar. For completeness,
we state the formal result in Appendix F.
Though this result does not capture the possible regularization eÔ¨Äect of sparsity, it shows that sparse
models with small NTK diÔ¨Äerence from dense NTK preserve the generalization ability of dense models, a
subject that has been studied more extensively, both from empirical and from theoretical perspectives. We
also show that training wide and sparse networks with gradient descent converges globally, similar to the
result for wide dense networks [Du et al., 2019, Allen-Zhu et al., 2019b] in Appendix H.
5 Experiments
In this section, our goal is to demonstrate that an extremely simple Ô¨Åxed sparsity pattern can actually speed
up sparse model training in wall-clock time without degrading model quality. SpeciÔ¨Åcally, we empirically
validate three claims that suggest PixelÔ¨Çy can improve training speed of diÔ¨Äerent model architectures while
retaining model quality on a wide range of domains and tasks.
1. Section 5.1: for image classiÔ¨Åcation tasks, we Ô¨Årst show the empirical NTK of Ô¨Çat block butterÔ¨Çy +
low-rank sparsity pattern is closer to dense NKT than other baselines. Then we demonstrate our superior
end-to-end performance. SpeciÔ¨Åcally, we achieve training speed up on both MLP-Mixer and ViT models
by up to 2.3wall-clock time with no drop in accuracy compared to the dense model and up to 4 
compared to RigL, BigBird and other sparse baselines.
2. Section 5.2: for language modeling and text classiÔ¨Åcation tasks, we can speed up GPT-2 small dense
model training by 2.1 , achieving a perplexity of 22.5 on wikitext-103. In addition, on Long Range
Arena (LRA) benchmark, we maintain the same accuracy but have 5:2speed-up in training.
3. Section 5.3: we show the necessity of block Ô¨Çat butterÔ¨Çy and low-rank structures, hardware-alignment
and wide coverage of most network layers with ablation studies on these three components of PixelÔ¨Çy.
5.1 Image ClassiÔ¨Åcation
7

--- PAGE 8 ---
0255075100Epoch0.00.10.20.30.40.50.60.70.8AccuracyCIFAR100 Training DynamicsDenseBigBirdPixelÔ¨ÇyNTK Distance = 0.15NTK Distance = 0.35Figure 4: NTK Comparison with
Dense Model.We evaluate the quality and eÔ¨Éciency of PixelÔ¨Çy through three metrics:
(1) distance to training dynamic of the dense model: compare the distance
between empirical NTK kernel7of the models with candidate patterns, in-
cluding BigBird [Zaheer et al., 2020], ButterÔ¨Çy [Dao et al., 2020], and that
of the dense model, (2) upstream accuracy: compare the accuracy and train-
ing time of the PixelÔ¨Çy, the dense counterpart, and other baselines on same
image classiÔ¨Åcation tasks, (3) downstream accuracy: compare the accuracy
of our pretrained PixelÔ¨Çy and dense model Ô¨Åne-tuned on downstream tasks
(Appendix L.4). The empirical NTK of the model with Ô¨Çat block butterÔ¨Çy
+ low-rank, picked by PixelÔ¨Çy, is closer to the NTK of the dense model.
PixelÔ¨Çy MLP-mixer and ViT models also retain the same top-1 accuracy of
the original dense models while achieving up to 2.3 speed up.
Setup: We use three popular vision benchmarks, CIFAR-10/100 [Krizhevsky et al., 2009] and Ima-
geNet [Deng et al., 2009]. We choose recent popular Vision Transformer [Dosovitskiy et al., 2020], T2T-
ViT [Yuan et al., 2021] and MLP-Mixer [Tolstikhin et al., 2021] as representative base models. Their major
computation bottlenecks are in diÔ¨Äerent components, e.g. MLP only, attention, or both so we can evaluate
the end-to-end applicability of PixelÔ¨Çy more clearly.
Figure5: TheperformanceofPixelÔ¨ÇyandViTorMLP-MixeronCIFAR10, CI-
FAR100andImageNetbenchmarks. Wemeasuretheaccuracyandthetraining
time speedup (on ImageNet) compared to the dense model.
Model CIFAR10 CIFAR100 ImageNet Speedup
Mixer-S/16 86.4 58.7 72.4 -
PixelÔ¨Çy-Mixer-S/16 89.8 62.9 72.6 1.7 
Mixer-B/16 87.6 59.5 75.6 -
PixelÔ¨Çy-Mixer-B/16 90.6 65.4 76.3 2.3 
ViT-S/16 89.5 65.1 77.7 -
PixelÔ¨Çy-ViT-S/16 91.3 66.8 77.5 1.9 
ViT-B/16 89.9 61.9 78.5 -
PixelÔ¨Çy-ViT-B/16 92.2 65.1 78.6 2.0 Empirical NTK: To charac-
terize the training dynamic of the
sparse networks, we compute the
empirical NTK kernels for dense
Vision Transformer on CIFAR-
100. Then, we show the rela-
tive diÔ¨Äerences between kernels
of models with diÔ¨Äerent sparsity
patternsandthatofthedenseone
in Fig. 4. SpeciÔ¨Åcally, we pick
a popular sparsity pattern com-
bination ‚Äì Bigbird pattern [Za-
heer et al., 2020] for attention
layer and random (magnitude-
based sparsity at initialization equals to random) for MLP layer, as a representative baseline. The plot
indicates that our designed pattern, Ô¨Çat block butterÔ¨Çy + low-rank is the closest one to that of the dense one
among all the patterns. Hence, we expect them to enjoy the most beneÔ¨Åts of their dense overparameterized
counterparts in real tasks. More details on measuring empirical NTK are covered in the Appendix L.3.
Figure 6: Comparison with a representative sparse
training baseline RigL [Evci et al., 2020].
Model ImageNet (Acc) Speedup
Mixer-S/32 58.56 -
RigL [Evci et al., 2020] 56.10 0.8
PixelÔ¨Çy (ours) 59.61 2.1Training from scratch: We validate that PixelÔ¨Çy
trains up to 2.3and 2.0faster than dense MLP-Mixer
andViTmodelsfromscratch, withthesameaccuracyunder
the same setting (batch size, epochs). SpeciÔ¨Åcally, we spar-
sify the models with PixelÔ¨Çy and train them on three com-
monly used vision benchmarking datasets, CIFAR-10/100
and ImageNet. We measure their Top-1 accuracy wall-clock
training time. To summarize the general trend, Fig. 5 high-
lights that our sparse vision models consistently retain the accuracy of their dense counterparts in terms of
accuracy and achieve training-time speed-up.
Furthermore, we have discussed in Section 1 that current sparse training algorithms aim to dynamic
search what could be good sparsity for eÔ¨Écient inference but do not speed up training in wall-clock time.
But we still present the comparison results in Fig. 6 for completeness. For a fair comparison, we conduct
the experiment on Mixer-S/32 model for 100 epochs because RigL aims for sparsity on weights, while we
aim for both weights & attention. As expected, RigL does not speed up training (the pioneering work has
unstructured sparsity and does not achieve speed up on GPU) but surprisingly PixelÔ¨Çy outperforms both
dense and RigL in terms of accuracy while achieving 2:1speedup.
7There is an emerging consensus that the NTK is an informative measure of how training and convergence behaviors of two
models are similar.
8

--- PAGE 9 ---
Figure 7: Comparison with representative sparse attention
baselines.
Model ImageNet (Acc) Speedup
T2T-ViT 81.7 -
BigBird 81.5 0.9
Sparse Transformer 81.4 1.3
PixelÔ¨Çy 81.7 1.4Finally, we compare PixelÔ¨Çy with BigBird
and Sparse Transformer pattern. For a fair com-
parison, we choose T2T-ViT as the base model
becauseitsmajorbottleneckisontheT2Tatten-
tion module (our baselines are eÔ¨Écient attention
variants). We can see from Fig. 7 that PixelÔ¨Çy
is the only one that can maintain the accuracy
and have actual speed up. Further more, PixelÔ¨Çy
speeds up T2T module (large attention) by 1.4 compare to dense.
5.2 Language Modeling and Text ClassiÔ¨Åcation
In this section, we aim to evaluate the eÔ¨Äectiveness of PixelÔ¨Çy in the text domain, on a language modeling
task and Long Range Arena (LRA [Tay et al., 2020]) benchmarks. On WikiText-103 [Merity et al., 2016],
PixelÔ¨Çy achieves 22.5 perplexity, which is around the same perplexity as GPT-2 small [Radford et al., 2019]
but trains 2.1faster. On LRA, PixelÔ¨Çy obtains almost the same accuracy as the full model but gains up
to5:2speed-up.
Setup: We use WikiText-103 for language modeling and LRA for classiÔ¨Åcation tasks. We use GPT-2
small and vanilla Transformer as the base dense models. The computational bottleneck of GPT-2 small
for moderate sequence length, e.g. 512, would be on both attention and MLP layers, while the bottleneck
of transformer on LRA task is on attention since the benchmark is designed to evaluate models under
long-context scenarios.
Figure 8: The performance of PixelÔ¨Çy, BigBird and GPT-
2-Small, Medium on WikiText-103. We measure the per-
plexity and the training speed up.
Model WikiText-103 (ppl) Speedup
GPT-2-Small 22.2 -
BigBird 23.3 0.96
PixelÔ¨Çy 22.5 2.1
GPT-2-Medium 20.9 -
BigBird 21.5 1.1
PixelÔ¨Çy 21.0 2.5GPT-2-Small, Medium on WikiText-103:
We show training GPT-2-Small, Medium and its
PixelÔ¨Çy model from scratch on a commonly used
NLPbenchmarkingdataset, wikiText-103. Wemea-
sure their perplexity on that dataset, and our train-
ing speed up. All setup and Ô¨Ånetuning hyperparam-
eters follow the ones in the original paper [Radford
et al., 2019]. We present the results in Fig. 8. It
is not hard to see that PixelÔ¨Çy models have great
advantages in accuracy-eÔ¨Éciency tradeoÔ¨Äs since it
maintains the same perplexity as the dense model
but achieve up to 2.5 speed-up in training.
Figure 9: The performance of PixelÔ¨Çy, Reformer and vanilla transformer on Long-
Range-Arena benchmarks. We measure the accuracy and training speed.
Model ListOps TextRetrieval ImagePathÔ¨Ånder AvgSpeedup
Transformer 36.54 63.12 80.33 41.56 73.49 59.01 -
Reformer 36.85 58.12 78.36 28.30 67.95 53.90 0.8
PixelÔ¨Çy 37.65 66.78 80.55 42.35 72.01 59.86 5.2Vanilla Transformer on
LRA:We compare vanilla
transformer and its PixelÔ¨Çy
models trained from scratch
on LRA benchmark. We mea-
sure the accuracy, through-
put, and training time of both
models. Each task has a dif-
ferent sequence length varying between 1024 and 4096. We follow the implementation and experimental
setting in [Xiong et al., 2021]. We compare the performance of PixelÔ¨Çy against the dense transformer and
report the results in Fig. 9. We also include the numbers of other baselines from the same repository in the
appendix. We can see PixelÔ¨Çy cause almost no drop in accuracy while achieving 5.2 speed-up in time.
5.3 Ablation Study
We conduct ablation studies on each component of PixelÔ¨Çy (Details in Appendix L.5). SpeciÔ¨Åcally, we
present (i) how Ô¨Çat block butterÔ¨Çy and low-rank aÔ¨Äect the model quality, (ii) how diÔ¨Äerent block size would
aÔ¨Äect the training speed, (iii) how budget allocation aÔ¨Äects the end-to-end speed up.
Necessity of Flat Block ButterÔ¨Çy and Low-rank: (i) We apply diÔ¨Äerent parameter allocation of
Ô¨Çat block butterÔ¨Çy and Low-rank component in PixelÔ¨Çy Mixer-S model on CIFAR-10 under the diÔ¨Äerent
9

--- PAGE 10 ---
density varying in [0.05, 0.1, 0.2]. We found that similar to what was reported in [Chen et al., 2021], using
around1
4budget on Low-rank and3
4on Ô¨Çat block butterÔ¨Çy achieves the best accuracy. (ii) We also compare
PixelÔ¨Çy with baseline sparsity patterns and show it is 2:7faster than dense, 3faster than ButterÔ¨Çy, 3:2
faster than BigBird under 10%density.
Block Size: We study the accuracy-eÔ¨Éciency trade-oÔ¨Ä for Ô¨Çat block butterÔ¨Çy and random sparsity
pattern with diÔ¨Äerent block sizes from 1-32 ( Table 7). We found that Ô¨Årst, under the same density, the
same sparsity patterns covered with diÔ¨Äerent block sizes could have a big diÔ¨Äerence in eÔ¨Éciency. Under the
same block, the pattern with more locality can be more eÔ¨Écient. Last, the density can seem very small,
but actually memory access could be up to 100%of the matrix. Therefore, we always want to make full
utilization of the smallest block size that the hardware (or compiler) supported.
Budget Allocation: We sparsify diÔ¨Äerent components of ViT-small separately, including attention and
MLP. We show that their compute ratio is approximately 1 : 2, so if only sparsify one of them, the other
one will be the bottleneck preventing end-to-end speed up. Therefore, it is necessary to have an algorithm
that can sparsify all layers.
6 Related Work
Lottery Ticket Hypothesis. Models proposed in our work can be roughly seen as a class of manually
constructed lottery tickets. Lottery tickets [Frankle and Carbin, 2018] are a set of small sub-networks derived
fromalargerdensenetwork, whichoutperformstheirparentnetworks. Manyinsightfulstudies[Morcosetal.,
2019, Orseau et al., 2020, Frankle et al., 2019, 2020, Malach et al., 2020, Pensia et al., 2020] are carried out
to analyze these tickets, but it remains diÔ¨Écult to generalize to large models due to training cost. In an
attempt, follow-up works [Wang et al., 2020, Tanaka et al., 2020] show that one can Ô¨Ånd tickets without
training labels. We draw inspiration from one of them, Liu and Zenke [2020], which uses the NTK to
avoid using labels in sparsifying networks. Other recent works use specialized hardware to accelerate sparse
training [Goli and Aamodt, 2020, Raihan and Aamodt, 2020].
Neural Pruning. Our work is loosely related to neural network pruning. By iteratively eliminating neu-
rons and connections, pruning has seen great success in compressing complex models. Pioneering work [Han
et al., 2015a,b] shows that pruning can produce signiÔ¨Åcantly smaller and faster models for inference. Subse-
quent methods [Li et al., 2016, Lin et al., 2017, Dong et al., 2017, Sanh et al., 2020, Lagunas et al., 2021, Zhu
and Gupta, 2017] improve on the quality of the pruned models. While both our and the pruning methods
aim to produce sparse models, we target training eÔ¨Éciency, whereas pruning mostly focuses on inference
eÔ¨Éciency at the cost of sacriÔ¨Åcing training speed.
Overparameterized Models and NTK. Our analysis for sparse model convergence relies heavily
on recent advance in neural tangent kernel (NTK) [Jacot et al., 2018]. NTK is a tool which has been
widely used in analyzing overparameterized models‚Äô convergence [Li and Liang, 2018, Du et al., 2019, Allen-
Zhu et al., 2019b,c, Song and Yang, 2019], generalization [Allen-Zhu et al., 2019a], connection to data
separability [Oymak and Soltanolkotabi, 2020], and cost per iteration [Brand et al., 2021]). Deep Double
Descent [Nakkiran et al., 2019, d‚ÄôAscoli et al., 2020] conjectures that the generalization error improves as the
parameter count grows. It is not surprising that the community is racing to break the record of the largest
parameter counts [Radford et al., 2019, Brown et al., 2020, Dosovitskiy et al., 2020, Tolstikhin et al., 2021,
Zhang et al., 2021, Naumov et al., 2019, Jumper et al., 2021].
We provide extended related work in Appendix M.
7 Conclusion
In our early exploration of many sparsity patterns with complex training procedures, we found that a simple
pattern (butterÔ¨Çy + low-rank) consistently (though not always) performed among the best. This motivated
us to propose Pixelated ButterÔ¨Çy, a simple and eÔ¨Écient sparse training method. In our quest for simplicity
and eÔ¨Éciency, we have chosen to use Ô¨Åxed sparsity that aligns with modern hardware, which was suÔ¨Écient
to yield wall-clock training time speedup without sacriÔ¨Åcing accuracy. We are excited about several future
directions. Inspired by the remarkable success of model pruning for inference, it is possible that dynamic
block sparse mask could be made eÔ¨Écient yet still accurate. Our Ô¨Çat butterÔ¨Çy is a simple Ô¨Årst order
approximation of the rich class of butterÔ¨Çy matrices, and there could be more sophisticated approximations
10

--- PAGE 11 ---
that retain more expressiveness. Our method is a Ô¨Årst step towards the goal of making sparse models train
faster than dense models and make them more accessible to the general machine learning community.
Acknowledgments
We thank Laurel Orr, Xun Huang, Sarah Hooper, Sen Wu, Megan Leszczynski, and Karan Goel for their
helpful discussions and feedback on early drafts of the paper.
We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under
No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying
Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP,
Xilinx, LETI-CEA,Intel, IBM,Microsoft, NEC,Toshiba, TSMC,ARM,Hitachi, BASF,Accenture, Ericsson,
Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce,
Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI),
and members of the Stanford DAWN project: Facebook, Google, and VMWare. The Mobilize Center is a
Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging and
Bioengineering through Grant P41EB027060. The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions,
Ô¨Åndings, and conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reÔ¨Çect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the
U.S. Government. Atri Rudra‚Äôs research is supported by NSF grant CCF-1763481.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, goingbeyondtwolayers. In Advances in neural information processing systems , pages6155‚Äì6166,
2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pages 242‚Äì252. PMLR, 2019b.
ZeyuanAllen-Zhu, YuanzhiLi, andZhaoSong. Ontheconvergencerateoftrainingrecurrentneuralnetworks.
InNeurIPS , 2019c.
Noga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics, Probability
and Computing , 18(1-2):3‚Äì15, 2009.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration
by overparameterization. In International Conference on Machine Learning , pages 244‚Äì253. PMLR, 2018.
SanjeevArora,SimonDu,WeiHu,ZhiyuanLi,andRuosongWang. Fine-grainedanalysisofoptimizationand
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning , pages 322‚Äì332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact
computation with an inÔ¨Ånitely wide neural net. arXiv preprint arXiv:1904.11955 , 2019b.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias‚Äìvariance trade-oÔ¨Ä. Proceedings of the National Academy of Sciences , 116(32):15849‚Äì
15854, 2019.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258 , 2021.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized) neural
networks in near-linear time. In ITCS, 2021.
11

--- PAGE 12 ---
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165 , 2020.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and
inaccurate measurements. Communications on Pure and Applied Mathematics: A Journal Issued by the
Courant Institute of Mathematical Sciences , 59(8):1207‚Äì1223, 2006.
Emmanuel J Cand√®s, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal
of the ACM (JACM) , 58(3):1‚Äì37, 2011.
YuanCaoandQuanquanGu. Generalizationerrorboundsofgradientdescentforlearningover-parameterized
deep relu networks. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 34, pages
3349‚Äì3356, 2020.
Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, and Anshumali Shrivastava. Slide:
In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. arXiv
preprint arXiv:1903.03129 , 2019.
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R√©. Scatterbrain: Unifying
sparse and low-rank attention. In NeurIPS , 2021.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse trans-
formers. arXiv preprint arXiv:1904.10509 , 2019.
Krzysztof Choromanski, Mark Rowland, Wenyu Chen, and Adrian Weller. Unifying orthogonal Monte Carlo
methods. In International Conference on Machine Learning , pages 1203‚Äì1212, 2019.
DC Collins and ES Angel. The diagonal decomposition technique applied to the dynamic programming
solution of elliptic partial diÔ¨Äerential equations. Journal of Mathematical Analysis and Applications , 33
(3):467‚Äì481, 1971.
Shane Cook. CUDA Programming: A Developer‚Äôs Guide to Parallel Computing with GPUs . Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA, 1st edition, 2012. ISBN 9780124159334.
James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series.
Mathematics of computation , 19(90):297‚Äì301, 1965.
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R√©. Learning fast algorithms for
linear transforms using butterÔ¨Çy factorizations. In International conference on machine learning , pages
1517‚Äì1527. PMLR, 2019.
Tri Dao, Nimit S Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra,
and Christopher R√©. Kaleidoscope: An eÔ¨Écient, learnable representation for all structured linear maps.
InInternational conference on representation learning , 2020.
St√©phane d‚ÄôAscoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overÔ¨Åtting: Where
& why do they appear? arXiv preprint arXiv:2006.03509 , 2020.
Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R√©, and Atri Rudra. A two-pronged progress
in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms , pages 1060‚Äì1079. SIAM, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248‚Äì255.
Ieee, 2009.
Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. arXiv preprint arXiv:1705.07565 , 2017.
12

--- PAGE 13 ---
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In ICLR.https://arxiv.org/pdf/1810.02054 , 2019.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making
all tickets winners. In International Conference on Machine Learning , pages 2943‚Äì2952. PMLR, 2020.
Peter Foldiak. Sparse coding in the primate cortex. The handbook of brain theory and neural networks , 2003.
Dean Foster, Howard KarloÔ¨Ä, and Justin Thaler. Variable selection is hard. In Conference on Learning
Theory, pages 696‚Äì709. PMLR, 2015.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 , 2018.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery
ticket hypothesis. arXiv preprint arXiv:1903.01611 , 2019.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity
and the lottery ticket hypothesis. In International Conference on Machine Learning , pages 3259‚Äì3269.
PMLR, 2020.
Negar Goli and Tor M. Aamodt. Resprop: Reuse sparsiÔ¨Åed backpropagation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020.
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. arXiv preprint
arXiv:1711.09224 , 3, 2017.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huÔ¨Äman coding. arXiv preprint arXiv:1510.00149 , 2015a.
Song Han, JeÔ¨Ä Pool, John Tran, and William J Dally. Learning both weights and connections for eÔ¨Écient
neural networks. arXiv preprint arXiv:1506.02626 , 2015b.
SouÔ¨Åane Hayou, Arnaud Doucet, and Judith Rousseau. Training dynamics of deep networks using stochastic
gradient descent via neural tangent kernel. arXiv preprint arXiv:1905.13654 , 2019.
Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489 , 2020.
Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. arXiv preprint arXiv:1806.07572 , 2018.
Ali Jalali, Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via convex
optimization. In ICML, 2011.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and Marin
Soljaciƒá. Tunable eÔ¨Écient unitary neural networks (EUNN) and their application to RNNs. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70 , pages 1733‚Äì1741. JMLR. org, 2017.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin ≈Ω√≠dek, Anna Potapenko, et al. Highly accurate protein structure
prediction with alphafold. Nature, 596(7873):583‚Äì589, 2021.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, JeÔ¨Ärey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.
13

--- PAGE 14 ---
Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The eÔ¨Écient transformer. In The Interna-
tional Conference on Machine Learning (ICML) , 2020.
Alex Krizhevsky, GeoÔ¨Ärey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Fran√ßois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning for faster transformers.
arXiv preprint arXiv:2109.04838 , 2021.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information
processing systems , pages 598‚Äì605, 1990.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
JeÔ¨Ärey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
Advances in neural information processing systems , 32:8572‚Äì8583, 2019.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on
connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning Ô¨Ålters for eÔ¨Écient
convnets. arXiv preprint arXiv:1608.08710 , 2016.
Yingzhou Li, Haizhao Yang, Eileen R. Martin, Kenneth L. Ho, and Lexing Ying. ButterÔ¨Çy factorization.
Multiscale Modeling & Simulation , 13(2):714‚Äì732, 2015.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent
on structured data. In NeurIPS , 2018.
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf .
Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent transfer. In
International Conference on Machine Learning , pages 6336‚Äì6347. PMLR, 2020.
Xi Luo. High dimensional low rank and sparse covariance matrix estimation via convex minimization. arXiv
preprint arXiv:1111.1133 , 199, 2011.
Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis:
Pruning is all you need. In International Conference on Machine Learning , pages 6682‚Äì6691. PMLR, 2020.
Michael Mathieu and Yann LeCun. Fast approximation of rotations and Hessians matrices. arXiv preprint
arXiv:1404.7195 , 2014.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
arXiv preprint arXiv:1609.07843 , 2016.
Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: generalizing
lottery ticket initializations across datasets and optimizers. arXiv preprint arXiv:1906.02773 , 2019.
Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, and Ivan Oseledets. Quadrature-based features
for kernel approximation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages 9165‚Äì9174. Curran
Associates, Inc., 2018.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double
descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292 , 2019.
14

--- PAGE 15 ---
Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jong-
soo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning rec-
ommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 ,
2019.
Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Logarithmic pruning is all you need. Advances in
Neural Information Processing Systems , 33, 2020.
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence
guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory ,
1(1):84‚Äì105, 2020.
D Stott Parker. Random butterÔ¨Çy transformations with applications in computational linear algebra. 1995.
Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos. Op-
timal lottery tickets via subsetsum: Logarithmic over-parameterization is suÔ¨Écient. arXiv preprint
arXiv:2006.07990 , 2020.
Alec Radford, JeÔ¨Ärey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Md Aamir Raihan and Tor M Aamodt. Sparse weight activation training. arXiv preprint arXiv:2001.01969 ,
2020.
Ilya Razenshteyn, Zhao Song, and David P WoodruÔ¨Ä. Weighted low rank approximations with provable
guarantees. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing , pages
250‚Äì263, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision , 115(3):211‚Äì252, 2015.
Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by Ô¨Åne-tuning.
arXiv preprint arXiv:2005.07683 , 2020.
Zhao Song and Xin Yang. Quadratic suÔ¨Éces for over-parametrization via matrix chernoÔ¨Ä bound. arXiv
preprint arXiv:1906.03593 , 2019.
Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks without
any data by iteratively conserving synaptic Ô¨Çow. arXiv preprint arXiv:2006.05467 , 2020.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eÔ¨Écient transformers. arXiv
preprint arXiv:2011.04006 , 2020.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini. The penn treebank: an overview. Treebanks , pages
5‚Äì22, 2003.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological) , 58(1):267‚Äì288, 1996.
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-
sica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for
vision.arXiv preprint arXiv:2105.01601 , 2021.
Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal
on Mathematics of Data Science , 1(1):144‚Äì160, 2019.
Keivan Alizadeh Vahid, Anish Prabhu, Ali Farhadi, and Mohammad Rastegari. ButterÔ¨Çy transform: An
eÔ¨Écient Ô¨Ät based neural architecture design. In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 12021‚Äì12030. IEEE, 2020.
15

--- PAGE 16 ---
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving
gradient Ô¨Çow. arXiv preprint arXiv:2002.07376 , 2020.
Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range
attention. arXiv preprint arXiv:2004.11886 , 2020.
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas
Singh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint
arXiv:2102.03902 , 2021.
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint
arXiv:2101.11986 , 2021.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.
Advances in Neural Information Processing Systems , 33, 2020.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv
preprint arXiv:2106.04560 , 2021.
Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji,
Jian Guan, et al. Cpm: A large-scale generative chinese pre-trained language model. AI Open , 2:93‚Äì99,
2021.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the eÔ¨Écacy of pruning for model
compression. arXiv preprint arXiv:1710.01878 , 2017.
16

--- PAGE 17 ---
A Problem Formulation
We formulate the problem of sparse model training as sparse matrix approximation with a simple hardware
cost model (Section 2).
We Ô¨Årst describe our simple cost model for sparse matrix multiplication to reÔ¨Çect the fact that parallel
hardware such as GPUs are block-oriented [Cook, 2012, Gray et al., 2017]: accessing one single element
from memory costs the same as accessing one whole block of elements. We then formulate the sparse matrix
approximation in the forward pass and the backward pass. The cost model necessitates narrowing the
sparsity pattern candidates to those that are block-aligned.
Cost model We model the time cost of an operation based on the number of Ô¨Çoating point operations
and memory access. The main feature is that our cost model takes into account memory coalescing , where
accessing a memory location costs the same as accessing the whole block of belements around it (typically
b= 16or32depending on the hardware).
LetCost membe the memory access cost (either read or write) for a block of bcontiguous elements.
Accessing any individual element within that block also costs Cost memtime. Let Cost opbe the compute
cost of a Ô¨Çoating point operation. Let Nblockmem be the number of block memory access, and Nopbe the
number of Ô¨Çoating point operations. Then the total cost of the operation is
Totalcost = Cost memNblockmem + Cost opNop:
This cost model is a Ô¨Årst order approximation of the runtime on modern hardware (GPUs), ignoring the
eÔ¨Äect of caching.
Block-aligned sparsity pattern, Block cover, and Memory access cost As the memory access cost
depends on the number of block of memory being accessed, we describe how the number of nonzero elements
in a sparse matrix relates to the number of blocks being accessed. We Ô¨Årst deÔ¨Åne a block cover of a sparse
mask.
DeÔ¨Ånition A.1. A sparse mask M2f0;1gmnis(b1;b2)-block-aligned if for any index i;jwhereMij= 1,
we also have Mi0j0= 1where:
i0=b1bi=b1c+r1;j0=b2bj=b2c+r2for allr1= 0;1;:::;b 1 1andr2= 0;1;:::;b 2 1:
The(b1;b2)-blockcover of a sparse mask M2f0;1gmnis the (b1;b2)-block-aligned mask M02f0;1gmn
with the least number of nonzeros such that MijM0
ijfor alli;j.
We omit the block size (b1;b2)if it is clear from context.
A sparse mask Mbeing (b1;b2)block-aligned means that if we divide Minto blocks of size b1b2, then
each block is either all zeros or all ones. To get the (b1;b2)-block cover of a sparse mask M, we simply divide
Minto blocks of size b1b2and set each block to all ones if any location in that block is one.
For a sparse matrix with sparse mask Mon a device with block size b, the number of block memory
accessNblockmem is the number of nonzero blocks in its (1;b)-block cover M0(assuming row-major storage).
This corresponds to the fact that to access a memory location on modern hardware (GPUs), the device needs
to load a whole block of belements around that location.
Fast sparse matrices means block-aligned sparsity pattern For sparsity patterns that are not block-
aligned, such as the random sparse pattern where each location is independently zero or nonzero, its (1;b)-
block cover might increase the density by a factor of close to btimes (we show this more rigorously in the
Appendix). As memory access often dominates the computation time, this means that non block-aligned
sparsity will often result is btimes slower execution than block-aligned ones. In other words, exploiting
hardware locality is crucial to obtain speed up.
Therefore, this cost model indicates that instead of searching over sparsity patterns whose total cost is
less than some budget C, we can instead search over block-aligned patterns whose number of nonzeros is less
than some limit k. For our theoretical analysis, we consider sparsity patterns that are (1;b)-block-aligned.
17

--- PAGE 18 ---
In practice, since we need to access both the matrix and its transpose (in the forward and backward pass),
we require the sparsity pattern to be both (1;b)-block-aligned and (b;1)-block-aligned. This is equivalent to
the condition that the sparsity pattern is (b;b)-block-aligned.
Sparsematrixapproximationintheforwardpass Wenowformulatethesparsematrixapproximation
in the forward pass. That is, we have weight matrix Awith input Band we would like to sparsify Awhile
minimizing the diÔ¨Äerence in the output. For easier exposition, we focus on the case where number of nonzeros
in each row is the same.
DeÔ¨Ånition A.2 (Forward regression) .Given four positive integers mndk1, matricesA2Rmd
andB2Rdn. The goal is to Ô¨Ånd a (1;b)-block-aligned binary mask matrix M2f0;1gmdthat satisÔ¨Åes
min
M2f0;1gmdkAB (AM)Bk1
s:t:kMik0=k;8i2[d]
whereMiis thei-th row ofM.
Sparse matrix approximation in the backward pass In the backward pass to compute the gradient
wrt to the weight matrix A, we would like to sparsify the gradient CB>while preserving as much of the
gradient magnitude as possible.
DeÔ¨ÅnitionA.3 (Backwardregression) .Given four positive integers mndk1, matricesB2Rdn
andC2Rmn. The goal is to Ô¨Ånd a (1;b)-block-aligned binary mask matrix M2f0;1gmdsuch that
min
M2f0;1gmdkCB> (CB>)Mk1
s:t:kMik0=k;8i2[d]
whereMiis thei-th row ofM.
Without making any assumptions, such problems are in general computationally hard Foster et al. [2015],
Razenshteyn et al. [2016].
18

--- PAGE 19 ---
B Analysis of ButterÔ¨Çy Variants
We present formal versions of theorems in Section 4 regarding variants of butterÔ¨Çy matrices. We provide
full proofs of the results here.
B.1 Block ButterÔ¨Çy Analysis
Proof of Theorem 4.1. LetMbe annnblock butterÔ¨Çy matrix with block size b. We want to show that
Malso has a representation as an nnblock butterÔ¨Çy matrix with block size 2b.
By DeÔ¨Ånition 3.3, Mhas the form:
M=B(n
b;b)
n
bB(n
b;b)
n
2b:::B(n
b;b)
4B(n
b;b)
2:
Notice that we can combine that last two terms to form a matrix of the form B(n
2b;2b)
2(see Fig. 3). Moreover,
other terms in the product of the form B(n
b;b)
n
2ibcan also be written as B(n
2b;2b)
n
2i 12b(see Fig. 3). Thus Malso has
the form:
M=B(n
2b;2b)
n
2bB(n
2b;2b)
n
4b:::B(n
2b;2b)
2:
In other words, Mis also annnblock butterÔ¨Çy matrix with block size 2b.
Proof of Corollary 4.2. Dao et al. [2020, Theorem 3] states that any nnsparse matrix with snonzeros
can be represented as products of butterÔ¨Çy matrices and their transposes, with O(slogn)parameters.
For a constant block size bthat is a power of 2, the set of nnblock butterÔ¨Çy matrices of block size
bcontains the set of regular butterÔ¨Çy matrices by Theorem 4.1. Therefore any such nnsparse matrix
also has a representation has products of block butterÔ¨Çy matrices of block size band their transposes, with
O(slogn)parameters.
B.2 Flat ButterÔ¨Çy Analysis
We prove Theorem 4.3, which relates the Ô¨Årst-order approximation in the form of a Ô¨Çat butterÔ¨Çy matrix
with the original butterÔ¨Çy matrix.
Proof of Theorem 4.3. Letn= 2mand letB1;:::;Bm2Rnnbe thembutterÔ¨Çy factor matrices (we
rename them here for simplicity of notation).
Let
E=mY
i=1(I+Bi)  
I+mX
i=1Bi!
:
Our goal is to show that kEkF.
We Ô¨Årst recall some properties of Frobenius norm. For any matrices AandC, we havekACkF
kAkFkCkFandkA+CkFkAkF+kCkF.
Expanding the terms of the product in E, we have
E=mX
i=2iX
s2[m];jsj=iY
j2sBj:
19

--- PAGE 20 ---
Using the above properties of Frobenius norm, we can bound E:
kEkFmX
i=2iX
s2[m];jsj=iY
j2skBjkF
mX
i=2iX
s2[m];jsj=iY
j2sBmax
=mX
i=22mi 
Bi
max
=mX
i=2(mB max)i
mX
i= 
cpi
c21X
i=0(cp)i
c2
1 cp
;
where in the last step we use the assumption that c1
2.
We now bound the rank of the Ô¨Årst-order approximation.
Proof of Theorem 4.4. LetM=I+Pm
i=1Bi. Note that any entry inPm
i=1Bihas absolute value at
most
mB1
maxcpB1
max
Bmax1
4;
where we use the assumption that B1
maxBmaxandc1
4.
Thus any diagonal entry in Mhas absolute value at least 1 1
4=3
4and the oÔ¨Ä-diagonal entries are at
mostcpB1
max
bm.
Alon [2009, Theorem 1.1] states that: there exists some c >0such that for any real M2Rnn, if the
diagonal elements have absolute values at least1
2and the oÔ¨Ä-diagonal elements have absolute values at most
where1
2pn1
4, then rank(M)clogn
2log 1=.
Applying this theorem to our setting, we have that
rank(M)
0
@Bmax
B1max2
m
log
Bmax
B1max1
A:
We just need to show thatB1
max
Bmax1
2cpnto satisfy the condition of the theorem.
Indeed, we have that 1Bmax
B1maxp
2nas eachkBik02n. Combining the two conditions onBmax
B1max, we
have shown that 1Bmax
B1max2cpn. This concludes the proof.
B.3 Flat Block ButterÔ¨Çy + Low-rank Analysis
We show that Ô¨Çat butterÔ¨Çy + low-rank (an instance) of sparse + low-rank, is more expressive than either
sparse or low-rank alone. We adapt the argument from Chen et al. [2021] to show a generative process where
the attention matrix can be well approximated by a Ô¨Çat butterÔ¨Çy + low-rank matrix, but not by a sparse or
low-rank alone.
We describe here a generative model of an input sequence to attention, parameterized by the inverse
temperature 2Rand the intra-cluster distance 2R.
20

--- PAGE 21 ---
Process 1. LetQ2Rnd, whered
(log3=2(n)), with every row of Qgenerated randomly as follows:
1. ForC= 
(n), sampleCnumber of cluster centers c1;:::;cC2Rdindependently from N(0;Id=p
d).
2. For each cluster around ci, sampleni=bnumber of elements around ci, of the form zij=ci+rij
forj= 1;:::;niwhererijN (0;Id=p
d). Assume that the total number of elements is n=cband
O(1=log1=4n).
LetQbe the matrix whose rows are the vectors zijwherei= 1;:::;Candj= 1;:::;ni. LetA=QQ>and
let the attention matrix be M= exp(A).
Theorem B.1. LetM, be the attention matrix in Process 1. Fix 2(0;1). LetR2Rnnbe a matrix.
Consider low-rank, sparse, and sparse + low-rank approximations to M. Assume (1 2) logn
O(logn).
1.Flat butterÔ¨Çy + low-rank : There exists a Ô¨Çat butterÔ¨Çy + low-rank Rwithn1+o(1)parameters with
kM RkFn.
2.Low-rank : IfRis such that n rank(R) = 
(n), thenkM RkF
(n).
3.Sparse: IfRhas sparsity o(n2), thenkM RkF
(n).
Proof sketch. As the argument is very similar to that of Chen et al. [2021, Theorem 1], we describe here the
modiÔ¨Åcations needed to adapt their proof.
The main diÔ¨Äerence between our generative process and that of Chen et al. [2021] is that each cluster
has the same number of elements, which is the same as the block size. The resulting attention matrix will
have a large block diagonal component, similar to that Chen et al. [2021]. However, all the blocks in the
block diagonal component has the same block size, which is b. Moreover, a Ô¨Çat block butterÔ¨Çy of block size
bcontains a block diagonal component of block size b. Therefore, this Ô¨Çat block butterÔ¨Çy matrix plays the
same role as the sparse matrix in the proof of Chen et al. [2021]. The rest of the argument follows that of
theirs.
21

--- PAGE 22 ---
Roadmap Theanalysisofsparsenetworksisorganizedasfollows. InSectionCwelistsomebasicnotations
that will be used. In Section D we consider the problem of adding sparsity on W, and we achieve polynomial
solving time. In Section E we prove that the gradient descent can be done fast under the sparsity assumption.
In Section G we consider the problem of adding sparsity on a, and we show that minimizing the dropout
loss is equivalent with a kernel ridge regression problem. In Section H we analyze the dynamics of gradient
Ô¨Çow and prove the convergence result.
C Notations
For a vector x, we usekxkpto denote its `pnorm, and we mainly consider p= 1;2in this paper. For a
matrixA, we usekAk0;kAk1;kAkFto denote the `0norm, entry-wise `1norm and Frobenius norm of A
respectively. For two matrices A;B2Rdm, we useABto denote their Hadamard product. We use
Tmat(n;d;m )to denote the time of multiplying ndmatrix with another dmmatrix. For a symmetric
matrixA, we usemin(A)to denote its minimum eigenvalue. We also let vec(A)be the vectorization of
a matrixAin column Ô¨Årst order. We use h;ito denote standard Euclidean inner product between two
vectors.
Moreover, we use N(;)to denote the Gaussian distribution with mean and covariance . We denote
the ReLU function by (z) = maxfz;0g. For an event E, we use 1fEgor1Eto denote its indicator function.
D Sparsity on hidden layer weights
D.1 Applying masks before multiplication
Given matrix A2Rnd,B2Rdn, naively computing ABtakesTmat(n;d;n ). Note that, we can also
consider the case where AandBhave diÔ¨Äerent size. For simplicity, let us consider the case where matrix A
and matrix B>have the same size.
Our goal is to Ô¨Ånd ‚Äúoptimal" binary mask matrix W2f0;1gdnsuch that,
min
Wkf(AB) f(A(WB))k1
s.t.kWB;ik0=k;8i2[n]
Remark D.1. In the practical applications we care about, the function fis the activation function of neural
network, e.g., ReLU(z) = maxfz;0g.
We deÔ¨Åne a sparse targeted regression problem:
DeÔ¨Ånition D.2 (Sparse mark regression, `1version).Given a matrix B2Rdn, and a vector a2Rd, the
goal is to Ô¨Ånd a k-sparse binary vector w2f0;1gdto minimize the following problem:
min
wka>B (a>w>)Bk1:
Naively, the above problem can be solved in ndO(k)via guess all the d
k
choices.
Lemma D.3. The targeted sparse mask regression problem can be solved in ndO(k).
Proof.We need to guess d
k
times, which becomes dO(k). Each time it takes ndoperations, thus the total
time is
nddO(k)=ndO(k):
DeÔ¨Ånition D.4 (`1version).Given three positive integers mndk1, matricesA2Rmdand
B2Rdn. We deÔ¨Åne our problem as Ô¨Ånding the binary matrix W2f0;1gmdthat satisÔ¨Åes
min
WkAB (AW)Bk1
s:t:kWik0=k;8i2[m]:
whereWiis thei-th row ofW.
22

--- PAGE 23 ---
Theorem D.5. The problem being deÔ¨Åned as DeÔ¨Ånition D.4 can be solved in mndO(k)time.
Proof.Our problem can be decomposed into msub-problems as follows:
kAB (AW)Bk1=mX
i=1(AB)i ((AW)B)i
1
=mX
i=1AiB (AW)iB
1
=mX
i=1AiB (AiWi)B
1
whereAimeans thei-th row of matrix A. By applying Lemma D.3, each sub-problem
min
WikAiB (AiWi)Bk1
can be solved in ndO(k)time. Then the problem deÔ¨Åned in DeÔ¨Ånition D.4 can be solved in
mndO(k)=mndO(k)
time in total. Thus we Ô¨Ånish the proof.
In the above Theorem, we show that solving the sparse mask regression problem is NP-hard. However, if
we add some mild assumptions and consider minimizing `1norm, then we can solve the regression problem
in polynomial time, as the following parts show.
DeÔ¨Ånition D.6 (`1version).Given a matrix B2Rdn
0, and a vector a2Rd
0, the goal is to Ô¨Ånd a k-sparse
binary vector w2f0;1gdto solve
min
wka>B (a>w>)Bk1
Lemma D.7. The targeted `1version sparse mask regression problem can be solved in
O(nd+nlogn)
which is polynomial time.
Proof.We Ô¨Årst consider the situation when a2f0;1gd. In this case, we have
ka>B (a>w>)Bk1+k(a>w>)Bk1=ka>Bk1
whereka>Bk1is Ô¨Åxed. So we only need to consider the following problem:
max
wk(a>w>)Bk1:
For simplicity we assume ai= 1;8i2[d], and we only need to solve
max
wkw>Bk1
wherewhaskelements equal to 1andd kelements equal to 0. Fori2[d], we compute Si=Pn
j=1Bij
which is the summation of i-th row ofB, and sort them as S(1)S(2)S(n). Then we only need to
letw(i)= 1fori2[k]and other elements equal to 0. Computing all SitakesO(nd)time, sorting Sitakes
O(nlogn)time, thus the total time consumption is O(nd+nlogn)in this case.
Next, we consider the general case when a2Rd
0. We let
Bi=aiBiandai=(
1; ai>0
0; ai= 0;8i2[d]
23

--- PAGE 24 ---
whereBiis thei-th row ofB. Then our optimization problem is equivalent to
min
wka>B (a>w>)Bk1
whereB2Rdn
0anda2f0;1gd. Thus we turn this case into the Ô¨Årst case. Constructing Bandatakes
O(nd)time, thus the total time consumption is also O(nd+nlogn)in this case.
DeÔ¨Ånition D.8 (`1version).Given three positive integers mndk1, matricesA2Rmd
0and
B2Rdn
0. We deÔ¨Åne our problem as Ô¨Ånding the binary matrix W2f0;1gmdthat satisÔ¨Åes
min
WkAB (AW)Bk1
s:t:kWik0=k;8i2[m]:
whereWiis thei-th row ofW.
Theorem D.9. The problem being deÔ¨Åned as DeÔ¨Ånition D.8 can be solved in
O(mnd +mnlogn)
time.
Proof.Our problem can be decomposed into msub-problems as follows:
kAB (AW)Bk1=mX
i=1(AB)i ((AW)B)i
1
=mX
i=1AiB (AW)iB
1
=mX
i=1AiB (AiWi)B
1
whereAimeans thei-th row of matrix A. By applying Lemma D.7, each sub-problem
min
WikAiB (AiWi)Bk1
can be solved in O(nd+nlogn)time. Then the problem deÔ¨Åned in DeÔ¨Ånition D.8 can be solved in
mO(nd+nlogn) =O(mnd +mnlogn)
time in total. Thus we Ô¨Ånish the proof.
D.2 Applying Masks After Multiplication
DeÔ¨Ånition D.10. Given matrix B2Rdn,C2Rmn. The goal is to Ô¨Ånd a mask W2f0;1gmdwhere
each column of Wisk-sparse
min
W2f0;1gmdkCB> (CB>)Wk1
Remark D.11. TheBdeÔ¨Åned in DeÔ¨Ånition D.4 is the same as the BdeÔ¨Åned in DeÔ¨Ånition D.10. Bis
corresponding to the Xin the neural network setting.
24

--- PAGE 25 ---
E Gradient computation
In this section we consider a neural network with one hidden layer and mneurons in this hidden layer.
Supposex2Rdis the input, W= (w1;;wm)2Rdmis the weight matrix of the Ô¨Årst layer, a2Rm
is the output weight, and M2f0;1gdmis the mask matrix with each column having at most knon-zero
entries. The neural network f:Rd!Ris deÔ¨Åned as
f(x) =a>
(MW)>x
:
For simplicity, we only optimize Wand Ô¨Åxa. Consider the mean square loss
L(W) =1
2nX
i=1(f(xi) yi)2=1
2nX
i=1(a>((MW)>xi) yi)2:
In the forward computation, for a batch of data points x1;;xn2Rd, letX2Rdndenote the input data
points matrix. For convenience, we deÔ¨Åne
W(t) =W(t+ 1) W(t) = @L(W(t))
@W(t)
whereis the step size. We deÔ¨Åne function gt:Rd!Rmas
gt(x) = (f(x) y)diagf0((MW(t))>x)ga
and also denote gt(X) = (gt(x1);;gt(xn))2Rmn.
Lemma E.1. We can express W(t)as
W(t) = (Xg>
t(X))M;
and each column of W(t)has at most knon-zero entries.
Proof.From the deÔ¨Ånition, we know
W(t) = @L(W(t))
@W(t)
= nX
i=1(f(xi) yi) diagf0((MW(t))>xi)g|{z}
mma|{z}
m1x>
i|{z}
1d>
M|{z}
dm
= (nX
i=1gt(xi)x>
i)>M
= (X|{z}
dng>
t(X)|{z}
nm)M|{z}
dm:
Since each column of Mhas at most knon-zero entries, we easily know each column of W(t)also has at
mostknon-zero entries.
Lemma E.2. Suppose that matrices M2Rdm,W(t)2RdmandW(t)2Rdmare given and pre-
computed, then we can compute ft+1(X)in
O(mnk)
time. (Here ft+1(X)is the evaluation of fatW(t+ 1).)
25

--- PAGE 26 ---
Proof.The goal is to compute
ft+1(X) =a>((M|{z}
dmW(t+ 1)|{z}
dm)>X):
By using Lemma E.1, we have
(MW(t+ 1))>X= (M(W(t) + W(t)))>X
= (MW(t))>X+ (MW(t))>X
= (MW(t))>X (M(Xg>
t(X))M)>X
= (MW(t))>X ((Xg>
t(X))M)>X
= (MW(t))>X+ (W(t))>X:
Notice that we have already computed (MW(t))>X2Rmdfrom previous iteration, so we only need to
compute (W(t))>Xwhere W(t)2RdmandX2Rdn. By using Lemma E.1, each row of (W(t))>
has at most knon-zero entries, thus we can compute (W(t))>XinO(mnk)time.
Lemma E.3. Suppose that matrices M2Rdm;W(t)2Rdmandft(X)are given and pre-computed, then
we can compute@L(W(t))
@W(t)inO(mnk)time.
Proof.By using Lemma E.1, we have
@L(W(t))
@W(t)= (Xg>
t(X))M
wheregt(x) = (f(x) y)diagf0((MW(t))>x)ga2Rmandgt(X) = (gt(x1);;gt(xn))2Rmn.
We Ô¨Årst compute MW(t)inO(mk)time, then we can construct gt(X)2RmninnO(mk)time. Given
gt(X), since we only need to compute kmentries ofXg>
t(X), where each entry can be computed in O(n)
time, thus we can compute@L(W(t))
@W(t)inO(mnk)time.
Algorithm 1 The sparse training algorithm
1:procedure Sparse Training (fxi;yigi2[n])
2:Initialization ar;wr(0)N(0;Id)forr2[m].
3:fort= 1!Tdo
4: /*forward computation*/
5: ComputeMW(t) .TakesO(mk)time.
6:fori= 1!ndo
7: ft(xi) a>((MW(t))>xi) .TakesO(mk)time.
8: gt(xi) (f(xi) yi)diag0((MW(t))>xi)a . TakesO(mk)time.
9:end for
10: /*backward computation*/
11:gt(X) (gt(x1);;gt(xn)).
12:@L(W(t))
@W(t)= (Xg>
t(X))M . TakesO(mnk)time.
13:W(t+ 1) =W(t) + W(t) .W(t) = @L(W(t))
@W(t).
14:end for
15:end procedure
26

--- PAGE 27 ---
F Neural Tangent Kernel, Convergence, and Generalization
Our analysis relies on the neural tangent kernel (NTK) [Jacot et al., 2018] of the network.
DeÔ¨Ånition F.1. Letf(;):Rd!Rbe the function speciÔ¨Åed by a neural network with parameters 2Rp
and input dimension d. The parameter is initialized randomly from a distribution P. Then its neural
tangent kernel (NTK) [Jacot et al., 2018] is a kernel K:RdRd!RdeÔ¨Åned by:
K(x;y) =E
P@f(x;)
@;@f(y;)
@
:
We can relate the training and generalization behavior of dense and sparse models through their NTK.
The standard result [Song and Yang, 2019] implies the following.
Proposition F.2. Letfdensedenote a ReLU neural network with Llayers with dense weight matrices dense
with NTK Kdense, and letfsparsebe the ReLU neural network with the same architecture and with weight
matricessparsewhose rows are k-sparse, and with NTK Ksparse. Letx1;:::;xNbe the inputs sampled
from some distribution PX. Suppose that the empirical NTK matrices Kd=Kdense(xi;xj)andKs=
Ksparse (xi;xj)for(i;j)2[N][N]satisfykKd Ksk.
Training. We knew the the number of iterations of dense network is min(Kd) 2n2log(1=)to reach the
training loss. For sparse network we need (min(Kd) ) 2n2log(1=).
Generalization. We knew the the number of iterations of dense network is min(Kd) 2n2log(1=)to
reach the generalization error training loss. For sparse network we need (min(Kd) ) 2n2log(1=).
These results relate the generalization bound of sparse models to that of dense models.
27

--- PAGE 28 ---
G Dropout Neural Network and KRR
We consider a two layer neural network with ReLU activation function, and write
f(W;x) :=1pmmX
r=1ar(w>
rx) =1pmmX
r=1arw>
rx1w>
rx0 (2)
wherewr(0)N(0;Id)2Rd,arunif(f 1;+1g)and all randomnesses are independent. We will Ô¨Åx ar
during the training process and use1pmnormalization factor, both of which are in the literature of Du et al.
[2019], Song and Yang [2019], Brand et al. [2021].
Suppose the training data are (x1;y1);:::; (xn;yn)2RdR, we deÔ¨Åne the classical objective function bL
as follows:
bL(W) :=1
2nX
i=1(f(W;xi) yi)2:
The gradient with respect to loss function bLis
@bL
@wr=1pmnX
i=1(f(W;xi) yi)arxi1w>rxi0:
We consider the eÔ¨Äect of dropout on network training. For each r2[m], we introduce the mask by
deÔ¨Åning random variable ras follows:
r=(
0; with probability 1  q;
1=q; with probability q:
It is easy to see that E[r] = 0(1 q) + (1=q)q= 1andE[2
r] = 02(1 q) + (1=q)2q= 1=q. We
assumeiandjare independent for any i6=j, then E[ij] =E[i]E[j] = 1. Let= (1;;m), we
deÔ¨Åne our dropout neural net as
F(W;x; ) :=1pmmX
r=1arr(w>
rx) =1pmmX
r=1arrw>
rx1w>rx0: (3)
Dropout explicitly change the target function, since we need to minimize the `2distance between F(W;x; )
andy, instead of f(W;x)andy. Formally, we deÔ¨Åne the dropout loss as
L(W) :=1
2E
"nX
i=1(F(W;xi;) yi)2#
: (4)
We Ô¨Årst give an explicit formulation of Lwhich also shows the diÔ¨Äerence between LandbL.
Lemma G.1. The dropout loss deÔ¨Åned in Eq. (4)can be expressed as the sum of classical loss bLand a
regularization term as
L(W) =bL(W) +1 q
2mqnX
i=1mX
r=1(w>
rxi)2: (5)
Proof.Since E[r] = 1, we have
E
[F(W;xi;)] =1pmE
[mX
r=1arr(w>
rx)] =1pmmX
r=1ar(w>
rxi) =f(W;xi) (6)
28

--- PAGE 29 ---
holds for any i2[n]. Next, we show the diÔ¨Äerence between LandbL:
2(L(W) bL(W))
=E
"nX
i=1(F(W;xi;) yi)2#
 nX
i=1(f(W;xi) yi)2
=nX
i=1
E
h
(F(W;xi;) yi)2i
 (f(W;xi) yi)2
=nX
i=1
E

F(W;xi;)2
 f(W;xi)2
=nX
i=10
@1
mX
r1;r22[m]E[ar1ar2r1r2(w>
r1xi)(w>
r2xi)] 1
mX
r1;r22[m]ar1ar2(w>
r1xi)(w>
r2xi)1
A
=1
m1 q
qnX
i=1mX
r=1a2
r(w>
rxi)2
=1
m1 q
qnX
i=1mX
r=1(w>
rxi)2(7)
where the Ô¨Årst step follows from deÔ¨Ånition, the second step follows from the linearity of expectation, the third
step follows from Eq. (6), the forth step follows from expansion, the Ô¨Åfth step follows from E[r1r2] = 1for
r16=r2andE[2
r1] =1
q, and the last step follows from a2
r= 1. Thus we have
L(W) =bL(W) +1 q
2mqnX
i=1mX
r=1(w>
rxi)2
and Ô¨Ånish the proof.
Before we move on, we introduce some extra notations and deÔ¨Ånitions. We denote
W= vec(W) =2
6664w1
w2
...
wm3
77752Rmd; and Y =2
6664y1
y2
...
yn3
77752Rn:
DeÔ¨Ånition G.2. We deÔ¨Åne matrix G12Rnnwhich can be viewed as a Gram matrix from a kernel
associated with ReLU function as follows:
G1
ij(X) = E
wN(0;I)[x>
ixj1w>xi0;w>xj0];8i;j2[n][n]
and assume 0=min(G1)>08.
DeÔ¨Ånition G.3. We deÔ¨Åne the masked matrix W(X;)2Rnmdas
W(X;) :=1pm2
6664(x1;)
(x2;)
...
(xn;)3
7775
=1pm2
6664a111hw1;x1i0x>
1a221hw2;x1i0x>
1::: amm1hwm;x1i0x>
1
a111hw1;x2i0x>
2a221hw2;x2i0x>
2::: amm1hwm;x2i0x>
2
............
a111hw1;xni0x>
na221hw2;xni0x>
n::: amm1hwm;xni0x>
n3
7775
8According to Theorem 3.1 in Du et al. [2019], the assumption holds when xiis not parallel with xjfori6=j, which is
reasonable in reality.
29

--- PAGE 30 ---
and also deÔ¨Åne the unmasked matrix bW(X)2Rnmdas
bW(X) :=1pm2
6664a11hw1;x1i0x>
1a21hw2;x1i0x>
1::: am1hwm;x1i0x>
1
a11hw1;x2i0x>
2a21hw2;x2i0x>
2::: am1hwm;x2i0x>
2
............
a11hw1;xni0x>
na21hw2;xni0x>
n::: am1hwm;xni0x>
n3
7775:
DeÔ¨Ånition G.4. We deÔ¨Åne the masked block diagonal matrix 	W(X;)2Rmdmdas
	W(X;) :=1
mdiag
 1; 2;; m
:
where8r2[m], r2Rddis deÔ¨Åned as
 r:=a2
r2
rnX
i=1xix>
i12
hwr;xii0=2
rnX
i=1xix>
i1hwr;xii0:
We also deÔ¨Åne the unmasked block diagonal matrix b	W(X)2Rmdmdas
b	W(X) :=1
mdiag
b 1;b 2;;b m
:
where8r2[m],b r2Rddis deÔ¨Åned as
b r:=nX
i=1xix>
i1hwr;xii0:
Lemma G.5. It is easy to verify that
W(X;) =bW(X)Dand 	W(X;) =b	W(X)D2

where
D:= diag(1;;1|{z}
d;;m;;m|{z}
d)2Rmdmd:
For convenience, we will simply denote W= W(X;)and	W= 	W(X;). Then by using the above
notations, we can express our dropout loss as L(W) =1
2E[kWW Yk2
2].
Lemma G.6. If we denote =1 q
q0, then we have
L(W) =1
2kbWW Yk2
2+
2W>b	WW:
Proof.As for the Ô¨Årst term, we have
kbWW Yk2
2=nX
i=1(1pmmX
r=1ar1hwr;xii0x>
iwr yi)2
=nX
i=1(1pmmX
r=1ar(w>
rxi) yi)2
=nX
i=1(f(W;xi) yi)2
= 2bL(W):
30

--- PAGE 31 ---
As for the second term, since b	Wis a block diagonal matrix, we have
W>b	WW=1
mmX
r=1
w>
r 
a2
rnX
i=1xix>
i12
hwr;xii0
wr
=1
mmX
r=1nX
i=1 
(w>
rxi)(w>
rxi)>12
hwr;xii0
=1
mnX
i=1mX
r=1(w>
rxi)2:
Thus by using Lemma G.1, we have
L(W) =bL(W) +1 q
2mqnX
i=1mX
r=1(w>
rxi)2
=1
2kbWW Yk2
2+
2W>b	WW
and Ô¨Ånish the proof.
Remark G.7. A classical kernel ridge regression problem can be deÔ¨Åned as
min
W1
2k(X)>W Yk2
2+
2kWk2
2
where:Rd!Fis a feature map. Note that Lemma G.6 breaks the dropout loss into two parts: the Ô¨Årst
part is an error term, and the second part can be seen as a regularization term. Thus the task of minimizing
the dropout loss L(W)is equivalent to a kernel ridge regression (KRR) problem.
31

--- PAGE 32 ---
H Dynamics of Kernel Methods (Continuous Gradient Flow)
The NTK also allows us to analyze the training convergence of sparse networks. We show that gradient
descent converges globally when training wide sparse networks. This convergence speed is similar to that of
dense models [Du et al., 2019, Allen-Zhu et al., 2019b].
In this section we will discuss the dynamics of kernel method under the mask , which adds sparsity in
the output layer. Our problem will be considered in over-parameterized scheme. First we introduce some
additionaldeÔ¨Ånitionsandnotations. WedeÔ¨ÅnesymmetricGrammatrix G(W)asG(W) :=bWb>
W2Rnn.
For alli;j2[n][n], we have
G(W)ij=1
mmX
r=1a2
r1hwr;xii0;hwr;xji0x>
ixj=1
mx>
ixjmX
r=11hwr;xii0;hwr;xji0:
We deÔ¨Åne block symmetric matrix H(W)asH(W) =b>
WbW2Rmdmd. Then for all i;j2[m][m], the
(i;j)-th block of H(W)is
H(W)ij=1
maiajnX
k=1xkx>
k1hwi;xki0;hwj;xki02Rdd:
By using Lemma G.6, we consider the corresponding kernel regression problem:
min
WLk(W) = min
W1
2kbW Yk2
2+
2W>b	W (8)
whereb2Rnmd,W2Rmd1,Y2Rn1andb	2Rmdmd. The main diÔ¨Äerence from neural network is
that we assume b(related to NTK, e.g., see DeÔ¨Ånition G.3) and b	(related to regularization term, e.g., see
DeÔ¨Ånition G.4) do not change during the training process.
The gradient of Lkcan be expressed as
rWLk(W) =b>bW b>Y+b	W: (9)
We useW?to denote the optimal solution of Eq. (8), and it satisÔ¨Åes
rWLk(W)
W=W?= (b>b +b	)W? b>Y= 0: (10)
Sinceb	is a positive diagonal matrix, b 1
2exists, thus we have
W?= (b>b +b	) 1b>Y:
Next, we consider the question from a continuous gradient Ô¨Çow aspect. In time t, we denote W(t) =
vec(W(t));b(t) =bW(t);b	(t) =b	W(t). We also denote G(t) =G(W(t))andH(t) =H(W(t)). Following
the literature of Du et al. [2019], we consider the ordinary diÔ¨Äerential equation deÔ¨Åned by
dwr(t)
dt= @Lk(W(t))
@wr(t): (11)
Lemma H.1 (Lemma 3.1 in Du et al. [2019]) .Ifm= 
(n2
2
0log(n
)), we have with probability at least 1 ,
kG(0) G1k20
4andmin(G(0))3
40.
Lemma H.2 (Lemma 3.2 in Du et al. [2019]) .Ifw1;;wmare i.i.d generated from N(0;Id), then with
probability at least 1 , the following holds. For any set of weight vectors w1;;wm2Rdthat satisfy
for anyr2[m];kwr wr(0)k2c0
n2for some small positive constant c, then matrix G2RddsatisÔ¨Åes
kG G(0)k2<0
4andmin(G)>0
2.
The above lemma shows that for Wthat is close to W(0), the Gram matrix Galso stays close to the
initial Gram matrix G(0), and its minimal eigenvalue is lower bounded.
32

--- PAGE 33 ---
Lemma H.3 (Gradient Flow) .If we assume min(b	)0>0, then with probability at least 1 , for
w1;;wm2Rdthat satisfy8r2[m];kwr wr(0)k2c0
n2, we have
dkbW bW?k2
2
dt kbW bW?k2
2
holds some constant  >0.
Proof.By using Eq. (9) and Eq. (11), we can expressdW
dtas
dW
dt= rWLk(W) = (b>bW b>Y+b	W): (12)
Then we have
dkbW bW?k2
2
dt
=dkbW bW?k2
2
dWdW
dt
= 2(bW bW?)>b( (b>bW b>Y+b	W))
= 2(bW bW?)>b(b>bW b>Y+b	W)
= 2(bW bW?)>b(b>bW b>bW? b	W?+b	W)
= 2(bW bW?)>bb>(bW bW?) 2(bW bW?)>b(b	W b	W?)
   20kbW bW?k2
2 2(W W?)>b>bb	(W W?) (13)
where the second step follows from Eq. (12), the fourth step follows from Eq. (10), and the last step follows
from the deÔ¨Ånition that 0=min(G) =min(bb>).
As for the second term in the Eq. (13), we have
2(W W?)>b>bb	(W W?)
= 2(Wb>b W?b>b)>b	(W W?)
20(W W?)>b>b(W W?)
= 20kbW bW?k2
2 (14)
Thus by Eq. (13) and Eq. (14) we have
dkbW bW?k2
2
dt (20+ 20)kbW bW?k2
2:
By letting= 20+ 20we Ô¨Ånish the proof.
For convenience, we denote u(t) =b(t)W(t)2Rn. Then it is easy to verify that
ui(t) =1pmmX
r=1ar(w>
rxi) =f(W(t);xi);8i2[n];
showing that u(t)is the prediction in time t.
Lemma H.4 (Convergence rate) .If we assume min(G(s))0
2holds for 0st, then we have
1.ku(t) Yk2
2e (0+2=m)tku(0) Yk2
2;
2.8r2[m];kwr(t) wr(0)k2pnku(0) Yk2
0pm:
33

--- PAGE 34 ---
Proof.From Eq. (9), we can express the dynamics by using u(t)as
du(t)
dt= b(b>bW b>Y+b	W)
=G(t)(Y u(t)) bb	W: (15)
Thus we have
dku(t) Yk2
2
dt= 2(u(t) Y)> 
G(t)(Y u(t)) bb	W
= 2(u(t) Y)>G(t)(u(t) Y) 2(u(t) Y)>bb	W
  0ku(t) Yk2
2 2(u(t) Y)>bb	W: (16)
As for the second term, we have
2(u(t) Y)>bb	W=2
m(u(t) Y)>b[b 1w1;;b mwm]>
=2
m(u(t) Y)>b[nX
i=1xi(w>
1xi);;nX
i=1xi(w>
mxi)]>
=2
m(u(t) Y)>[U1(t);;Un(t)]>(17)
where forj2[n],Uj(t)2Rcan be expressed as
Uj(t) =1pmmX
r=1 
ar1hwr;xji0x>
jnX
i=1xi(w>
rxi)
=1pmmX
r=1nX
i=1arx>
j(xix>
i)wr1hwr;xii0;hwr;xji0
=1pmmX
r=1
arx>
jwr1hwr;xji0nX
i=11hwr;xii0
:
We denote U(t) = [U1(t);;Un(t)]>2Rnand have
2(u(t) Y)>bb	W=2
m(u(t) Y)>U(t) (18)
and our dynamics becomes
dku(t) Yk2
2
dt  0ku(t) Yk2
2 2
m(u(t) Y)>U(t)
   (0+2
m)ku(t) Yk2
2 (19)
showing thatd
dt 
e(0+2=m)tku(t) Yk2
2
0. Thuse(0+2=m)tku(t) Yk2
2is a decreasing function with
respect tot, and we have
ku(t) Yk2
2e (0+2=m)tku(0) Yk2
2:
As for bounding kwr(t) wr(0)k2, we use the same method as in Lemma 3.3 of Du et al. [2019]. Thus we
complete the proof.
Finally, by combining Lemma H.1, H.2, H.3 and H.4, we have the following convergence result.
TheoremH.5 (ConvergenceofgradientÔ¨Çow) .Suppose0>0,m= poly(n;1=0;1=), then with probability
at least 1 over the randomness of initialization, we have
ku(t) Yk2
2e (0+2=m)tku(0) Yk2
2:
34

--- PAGE 35 ---
The above theorem shows that in the over-parameterized setting (when mis large enough), the training
loss of the kernel ridge regression problem deÔ¨Åne in Eq. (8) converges to 0in a linear rate. By comparing our
Theorem H.5 with Theorem 3.2 in Du et al. [2019], we can Ô¨Ånd that the introducing of regularization term
makes the convergence speed faster, though the improvement is limited. Further notice that in Section G
we prove the equivalence between minimizing the dropout loss and the kernel ridge regression problem. So
we conclude our results as:
The introducing of sparsity into neural network makes the convergence speed faster, but the improvement is
limited due to the over-parameterized scheme.
35

--- PAGE 36 ---
I Method Details
We describe some details of our method.
I.1 Compute budget allocation
We describe here a procedure to compute the budget allocation based on our cost model. This procedure is
more complicated than our simple rule of thumb in Section 3.3, and tend to produce the same allocation.
For completeness, we include the procedure here for the interested reader.
Given a parameter budget B, we Ô¨Ånd the density of each layer type that minimize the models‚Äô total cost
of matrix multiplication. For example, in Transformers, let daanddmbe the density of the attention and
the MLP layers. Let sbe the sequence length and dbe the feature size. The attention layer with density da
will costda(n2+nd), and the fully connected layers with density dmwill cost 2dmnd. We then set daand
dmto minimize the total cost while maintaining the parameter budget:
minimizea;ma(n2+nd) + 2mndsubject to #of trainable parameters B: (20)
As this is a problem with two variables, we can solve it in closed form.
I.2 Low-rank in Attention
In Section 3.3, we describe how to use the sparsity pattern from Ô¨Çat block butterÔ¨Çy and the low-rank term
for weight matrices. This applies to the linear layer in MLP and the projection steps in the attention.
We also use the sparse + low-rank structure in the attention step itself. Chen et al. [2021] describes a
general method to combine sparse and low-rank attention, where one uses the sparse component to discount
the contribution from the low-rank component to ensure accurate approximation of the attention matrix.
We follow a simpler procedure, which in practice yields similar performance. We use a restricted version
of low-rank of the form a ‚Äúglobal‚Äù sparsity mask (as shown in Fig. 12). Indeed, a sparse matrix whose sparsity
pattern follows the ‚Äúglobal‚Äù pattern is a sum of two sparse matrices, one containing the ‚Äúhorizontal‚Äù global
components and one containing the ‚Äúvertical‚Äù components. Let wbe the width of each of those components,
then each of them has rank at most w. Therefore, this sparse matrix has rank at most 2w, and is low-rank
(for smallw).
We also make the global component block-aligned (i.e., set wto be a multiple of the smallest supported
block size such as 32) for hardware eÔ¨Éciency.
I.3 Comparison to Other Sparsity Patterns for Attention
In the context of sparse attention, other sparsity patterns such as BigBird and Longformer also contain a
‚Äúglobal‚Äù component, analogous to our low-rank component. Their ‚Äúlocal‚Äù component is contained in the
block diagonal part of the Ô¨Çat block butterÔ¨Çy sparsity pattern.
The main diÔ¨Äerence that we do not use the random components (e.g., BigBird), and the diagonal strides
from Ô¨Çat block butterÔ¨Çy are not found in BigBird or Longformer. Moreover, we apply the same sparsity
pattern (+ low-rank) to the linear layers in the MLP and the projection step in attention as well, allowing
our method to target most neural network layers, not just the attention layer.
I.4 Sparsity Mask for Rectangular Matrices
We have described the sparsity masks from Ô¨Çat block butterÔ¨Çy for square matrices. For rectangular weight
matrices, we simply ‚Äústretch‚Äù the sparsity mask. The low-rank component applies to both square and
rectangular matrices (as shown in Fig. 10). We have found this to work consistently well across tasks.
36

--- PAGE 37 ---
Square Flat Block Butterfly++
++Rectangular Flat Block Butterfly  Figure 10: Sparsity Mask for Rectangular Matrices.
J Benchmarking of ButterÔ¨Çy Multiply
We validate that Ô¨Çat butterÔ¨Çy matrices (sum of factors) can speed up multiplication on GPUs compared to
butterÔ¨Çy matrices (products of factors).
Consider the matrix M2Rnnthat can be written as products of butterÔ¨Çy factors of strides of up k(a
power of 2), with residual connection:
M= (I+B(n)
k)(I+B(n)
k=2):::(I+B(n)
2):
The Ô¨Årst-order approximation of Mhas the form of a Ô¨Çat butterÔ¨Çy matrix with maximum stride k(Sec-
tion 3.2):
Mat=I+(B(n)
2++B(n)
k=2+B(n)
k):
Notice that Mis a product of log2kfactors, each has 2nnonzeros, so multiplying Mby a input vector
xcostsO(nlogk)operations (by sequentially multiplying xby the factors of M). The Ô¨Çat version Mat
is a sparse matrix with O(nlogk)nonzeros as well, and the cost of multiplying Matxis alsoO(nlogk).
However, in practice, multiplying Matxis much more eÔ¨Écient on GPUs than multiplying Mxbecause of
the ease of parallelization.
We measure the total time of forward and backward passes of multiplying either Matxand compare to
that of multiplying Mxfor diÔ¨Äerent maximum strides, as shown in Fig. 11. We see that ‚ÄúÔ¨Çattening‚Äù the
products brings up to 3 speedup.
We use matrix size 10241024with block size 32. The input batch size is 2048. We use the block sparse
matrix multiply library from https://github.com/huggingface/pytorch_block_sparse . The speed mea-
surement is done on a V100 GPU.
37

--- PAGE 38 ---
2 4 8 16 32
Maximum stride123Speedup (x)Figure 11: Speedup of multiplying Matxcompared to multiplying Mx. Flattening the products yields up
3speedup.
Local Global (Low-rank) Butterfly Random
Figure 12: Sparsity pattern candidate components: Local corresponds to local interaction of neighboring
elements; Global (low-rank) involves the interaction between all elements and a small subset of elements;
ButterÔ¨Çy captures the interaction between elements that are some Ô¨Åxed distance apart; Random is common
in the pruning literature.
K ExhaustedSearchingSparsityPatternsforEÔ¨ÉcientSparseTrain-
ing
We describe here our early exploration of searching among diÔ¨Äerent sparsity patterns that has been proposed
in the literature. We use a metric derived from the NTK, which has emerged as one of the standard metric
to predict the training and generalization of the model. We consistently found the butterÔ¨Çy + low-rank
pattern to perform among the best.
In Appendix K.1, we describe the challenges of selecting sparsity patterns for every model components
using the a metric derived from the NTK, followed by our approaches. Then in , we describe details of empir-
ical NTK computation, which is an important step in our method implementation. Last, in Appendix K.3,
we highlight important properties of our method ‚Äì it rediscovers several classical sparsity patterns, and the
sparse models can inherit the training hyperparamters of the dense models, reducing the need for hyperpa-
rameters tuning.
K.1 Challenges and Approaches
Challenge 1: We seek sparsity patterns for each model components that can closely mimic the training
dynamics of the dense counterpart. As mentioned in Theorem D.9, it is NP-hard to Ô¨Ånd the optimal sparse
38

--- PAGE 39 ---
Algorithm 2 Model SparsiÔ¨Åcation
1:Input: model schema 
, compute budget B, dataset subset X, sparsity mask candidate set C.
2:Kdense NTK (f; X). .Eq. (22)
3:output sparsity mask assignment sout,dmin inf
4:forM1; : : : ; Mj
j2Cj
jdo .Enumerate all sparsity mask candidate combinations
5:Letsbe the sparsity mask assignment (ti; ri; mi; ni)!Mi.
6: ifTotalCompute (s)< Bthen .Eq. (21), Check if masks satisfy budget constraint
7: LetMsbe the Ô¨Çattened sparse masks
8: Ksparse NTK (fMs; X)
9: ds Distance (Kdense; Ksparse ) .Eq. (22)
10: ifdmin> dsthen
11: dmin ds,sout s
12: end if
13: end if
14:end for
15:return sout .Return sparsity mask assignment
matrix approximation. Although NTK provides insights and measurement on the ‚Äúright‚Äù sparse model,
bruteforcely computing NTK for one-layer models with all sparsity patterns is still infeasible.
Approach 1: Sparsity Pattern Candidates. To address the above challenge, we design our search
space to be a limited set of sparsity pattern candidates, each is either a component visualized in Fig. 12
or the combination of any two of them. These components encompass the most common types of sparsity
pattern used, and can express We provide the intuition behind these sparsity components:
‚Ä¢Local: this block-diagonal component in the matrix corresponds to local interaction of neighboring ele-
ments. This has appeared in classical PDE discretization [Collins and Angel, 1971], and has been redis-
covered in Longformer and BigBird attention patterns.
‚Ä¢Global: this component involves interaction between all elements and a small subset of elements (i.e.,
‚Äúglobal‚Äù elements). This global pattern is low-rank, and this sparse + low-rank structure is common in
data science [Udell and Townsend, 2019], and rediscovered in Longformer and BigBird patterns as well.
‚Ä¢ButterÔ¨Çy: this component corresponds to interaction between elements that are some Ô¨Åxed distance apart.
The many divide-and-conquer algorithms, such as the classical fast Fourier transform [Cooley and Tukey,
1965], uses this pattern at each step. ButterÔ¨Çy matrices reÔ¨Çects this divide-and-conquer structure, and
hence this sparsity component. The sparse transformer [Child et al., 2019] also found this pattern helpful
for attention on image data.
‚Ä¢Random: this component is a generalization of sparsity patterns found in one-shot magnitude, gradient,
or momentum based pruning [Lee et al., 2018]. Note that at network initialization, they are equivalent to
random sparsity.
Challenge 2: Even with a Ô¨Åxed pool of sparsity patterns for each layer, if the model has many layers,
the number of possible layer-pattern assignments is exponentially large.
Approach 2: To further reduce the search space, we constrain each layer type (attention, MLP) to have
the same sparsity pattern. For example, if there are 10 patterns and 2 layer types, the candidate pool is
102= 100combinations.
Challenge 3: Computing the empirical NTK on the whole dataset is expensive in time and space, as it
scales quadratically in the dataset size.
Approach 3: We compute the empirical NTK on a randomly chosen subset of the data (i.e., a principal
submatrix of the empirical NTK matrix). In our experiments, we verify that increasing the subset size
beyond 1000 does not change the choices picked by the NTK heuristic. The subsampled empirical NTK can
be computed within seconds or minutes.
K.2 Algorithm Description
Our method targets GEMM-based neural networks, which are networks whose computation is dominated
by general matrix multiplies (GEMM), such as Transformer and MLP-Mixer. As a result, we can view the
network as a series of matrix multiplies. We Ô¨Årst deÔ¨Åne:
39

--- PAGE 40 ---
‚Ä¢Model schema: a list of layer types t(e.g., attention, linear layers in MLP), number of layers rof that type,
anddimensionofthematrixmultiplies mn. Wedenoteitas 
 =f(t1;r1;m1;n1);:::; (tj
j;rj
j;mj
j;nj
j)g.
‚Ä¢AmaskMof dimension mnis a binary matrix f0;1gmn. The compute of a mask is the total number
of ones in the matrix: compute(M) =P
i;jMij.
‚Ä¢Asparsity pattern Pmnfor matrix dimension mnis a set of masks fM1;:::;MjPjg, each of dimension
mn.
‚Ä¢Asparsity mask assignment is a mapping from a model schema 
to masksMbelonging to some sparsity
patternP:s: (t;r;m;n )!M.
‚Ä¢Given a set of sparsity patterns P1;:::;Pk, the set of sparsity mask candidate Cis the union of sparsity
masks in each of Pi:C=[Pi
‚Ä¢A sparsity pattern assignment ssatisÔ¨Åes the compute budget Bif:
TotalCompute( s) :=X
layer typelcompute(s(t;r;m;n ))B: (21)
‚Ä¢LetbetheÔ¨Çattenedvectorcontainingthemodelparameters,andlet MsbetheÔ¨Çattenedvectorcontaining
the sparsity mask by the sparsity mask assignment s. Letf(x)be the output of the dense network with
parameterand inputx. Then the output of the sparse network is fMs(x).
‚Ä¢The empirical NTK of a network fon a data subset X=fx1;:::;xjXjgis a matrix of size jXjjXj:
NTK(f;X)i;j=@f(xi)
@;@f(xj)
@
: (22)
The formal algorithm to assign the sparsity mask to each layer type is described in Algorithm 2. The
main idea is that, as the set of sparsity mask candidate is Ô¨Ånite, we can enumerate all possible sparsity
mask assignment satisfying the budget and pick the one with the smallest NTK distance to the dense NTK.
In practice, we can use strategies to avoid explicitly enumerating all possible sparsity mask, e.g. for each
sparsity pattern, we can choose the largest sparse mask that Ô¨Åts under the budget.
K.3 Method Properties: Rediscovering Classical Sparsity Patterns, No Addi-
tional Hyperparameter Tuning
When applied to the Transformer architecture, among the sparsity components described in Appendix K.1,
the NTK-guided heuristic consistently picks the local and global components for boththe attention and
MLP layers. Moreover, the butterÔ¨Çy component is also consistently picked for image data, reÔ¨Çecting the 2D
inductive bias in this component9. While some of these patterns have been proposed for sparse attention,
it is surprising that they are also picked for the MLP layers. The most popular type of sparsity pattern
in MLP layers is top-k (in magnitude or gradient, which at initialization is equivalent to random sparsity).
We have proved that lower NTK diÔ¨Äerence results in better generalization bound for the sparse model. As
expected, we observe that this allows the sparse model to use the same hyperparamters (optimizer, learning
rate, scheduler) as the dense model (Section 5).
9Convolution (commonly used in image data) can be written in terms of the fast Fourier transform, which has this same
sparse pattern at each step of the algorithm
40

--- PAGE 41 ---
L Experiment Details
L.1 Datasets
‚Ä¢Cifar10 [Krizhevsky et al., 2009] consists of 60000 coloured images of resolution 32 32. Each of
them belong to one of 10 classes, including airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships,
and trucks. Among these, 50000 images are allocated to be the training set and 10000 images the
testing set.
‚Ä¢Cifar100 [Krizhevsky et al., 2009] is similar to Cifar10. It also consists of images of resolution 32 
32. In total, there are 60000 images, each of which belongs to one of 100 classes. Each of the 100
classes has 500 images in training set and 100 images in testing set.
‚Ä¢ImageNet1K [Russakovsky et al., 2015] spans 1000 object classes, containing 1,281,167 training im-
ages, 50,000 validation images and 100,000 test images. Although images are collected in diÔ¨Äerent
resolutions, in practice they are generally reshaped and cropped into 224 224.
‚Ä¢WikiText-103 [Merity et al., 2016] contains articles from the wikipedia page. It extracts veriÔ¨Åed
articles from Wikipedia, which add up to over 100 million tokens. Compared to other datasets, such
as Penn Treebank (PTB) [Taylor et al., 2003], WikiText features a larger vocabulary and preserves
original upper/lower cases, punctuation and numbers.
L.2 Model ConÔ¨Ågurations and Hyperparameter
We summarize the details required to replicate our experiments below.
Baseline Model: Except for dense model. We choose our baselines for each experiment base on the
following. RigL aims to sparsify model weights/parameters, so we use it as a baseline in MLP-based models
(Mixer). BigBird focuses on attention matrices, so we used it as a baseline in Transformer-based models
(ViT, GPT-2).
L.2.1 Image ClassiÔ¨Åcation
Table 1: ConÔ¨Åguration of the Cifar10 experiments.
Model Optimizer Weight Decay Learning Rate Drop Path Warmup/Epoch
ViT-Small AdamW 0.05 0.0005 0.1 5/300
PixelÔ¨Çy-ViT-Small AdamW 0.05 0.0005 0 5/300
ViT-Base AdamW 0.05 0.0005 0.1 5/300
PixelÔ¨Çy-ViT-Base AdamW 0.05 0.0005 0 5/300
Mixer-Small AdamW 0.1 0.0005 0.1 5/300
PixelÔ¨Çy-Mixer-Small AdamW 0.1 0.0005 0 5/300
Mixer-Base AdamW 0.1 0.0005 0.1 5/300
PixelÔ¨Çy-Mixer-Base AdamW 0.1 0.0005 0 5/300
We report more details on the models, including number of parameters and FLOPs, in Table 4.
We follow the naming convention in the Vision Transformer paper and MLP-Mixer paper. In particular,
ViT-S and ViT-B refers to the small and base ViT models respectively, and 16 refers to the patch size of
16x16. The MLP-Mixer models follows the same convention.
L.2.2 Language Modeling
We report more details on the models, including number of parameters and FLOPs, in Table 5 and Table 6.
41

--- PAGE 42 ---
Model Optimizer Weight Decay Learning Rate Drop Path Warmup/Epoch
ViT-Small AdamW 0.05 0.0005 0.1 5/300
PixelÔ¨Çy-ViT-Small AdamW 0.05 0.0005 0 5/300
ViT-Base AdamW 0.05 0.0005 0.1 5/300
PixelÔ¨Çy-ViT-Base AdamW 0.05 0.0005 0 5/300
Mixer-Small AdamW 0.1 0.0005 0.1 5/300
PixelÔ¨Çy-Mixer-Small AdamW 0.1 0.0005 0 5/300
Mixer-Base AdamW 0.1 0.0005 0.1 5/300
PixelÔ¨Çy-Mixer-Base AdamW 0.1 0.0005 0 5/300
Table 2: ConÔ¨Åguration of the Cifar100 experiments
Model Optimizer Weight Decay Learning Rate Drop Path Warmup/Epoch
ViT-Small AdamW 0.05 0.001 0.1 5/300
PixelÔ¨Çy-ViT-Small AdamW 0.05 0.001 0 5/300
ViT-Base AdamW 0.05 0.001 0.1 5/300
PixelÔ¨Çy-ViT-Base AdamW 0.05 0.001 0 5/300
Mixer-Small AdamW 0.1 0.001 0.1 5/300
PixelÔ¨Çy-Mixer-Small AdamW 0.1 0.001 0 5/300
Mixer-Base AdamW 0.1 0.001 0.1 5/300
PixelÔ¨Çy-Mixer-Base AdamW 0.1 0.001 0 5/300
Table 3: ConÔ¨Åguration of the ImageNet experiment
L.3 Measuring Empirical NTK
The Empirical NTK is a rough estimation of the real NTK, in which the width of the neural net goes to
inÔ¨Ånity. As the width grows, the kernel gets closer to its inÔ¨Ånite-width limit. Fortunately, both our models
of interest, MLP-Mixer and Vision Transformer, are wide and overly parameterized. Therefore they are
only one step away from the real NTK domain. This allows us to use the Empirical NTK to approximately
predict their training behaviors.
As described in equation 22, we Ô¨Årst compute the gradient of each data sample, then we compute pair-
wise product to construct the Empirical NTK. Although we use a relatively small dataset, it‚Äôs still expensive
to build a kernel for large models, such as ViTs and MLP-Mixers. In practice, we Ô¨Ånd that it‚Äôs suÔ¨Écient to
compute kernels for a subsampled dataset.
MLP-Mixer and Vision Transformer each represent one type of module of interest for our sparsiÔ¨Åcation.
In MLP-Mixer, we study the sparse behavior of the Linear module, whereas, in Vision Transformer, we
mainly focus on sparsifying attention. All models are Ô¨Årst sparsiÔ¨Åed to around 10%of the original dense
compute. Then we compare their NTK kernels with their original dense kernel. We run three random seeds
to eliminate noise, i.e., three diÔ¨Äerent initializations for each pair of conÔ¨Ågurations. We report the mean
relative diÔ¨Äerence between the kernels with respect to the norm of the dense kernel.
L.4 Transfer Learning Experiments
We conduct extended experiments to test the generalization of our pretrained sparse models on downstream
tasks. SpeciÔ¨Åcally, we Ô¨Ånetune PixelÔ¨Çy pretrained model (ImageNet) on CIFAR-10 and show that it get
99.03% accuracy compared to 98.77% on our pretrained dense ViT-B/16 model. In addition, we see more
than 2speed up on downstream task Ô¨Åne-tuning process as well.
L.5 Microbenchmarking
In this section, we perform microbenchmarking on a 4K 4K sparse matrix multiplication. We aim to
show that PixelÔ¨Çy patterns are far more hardware friendly than random patterns. For a 4K 4K matrix,
42

--- PAGE 43 ---
Table 4: The performance of PixelÔ¨Çy and ViT or MLP-Mixer on the ImageNet benchmarks, including the
number of parameters and FLOPs. We measure the accuracy and the training time speedup (on ImageNet)
compared to the dense model.
Model ImageNet top-1 acc. Speedup Params FLOPs
Mixer-S/16 72.4 - 18.5M 3.8G
PixelÔ¨Çy-Mixer-S/16 72.6 1.7 5.9M 1.3G
Mixer-B/16 75.6 - 59.9M 12.6G
PixelÔ¨Çy-Mixer-B/16 76.3 2.3 17.4M 4.3G
ViT-S/16 77.7 - 48.8M 9.9G
PixelÔ¨Çy-ViT-S/16 77.5 1.9 16.9M 3.6G
ViT-B/16 78.5 - 86.6M 17.6G
PixelÔ¨Çy-ViT-B/16 78.6 2.0 28.2M 6.1G
Table 5: The performance of PixelÔ¨Çy, BigBird and GPT-2-Small on WikiText-103, including the number of
parameters and FLOPs. We measure the perplexity and the training speed up.
Model WikiText-103 (ppl) Speedup Params FLOPS
GPT-2-Small 22.2 - 117M 48.4G
BigBird 23.3 0.96 117M 40.2G
PixelÔ¨Çy 22.5 2.1 68M 18.5G
GPT-2-Medium 20.9 - 345 M 168G
BigBird 21.5 1.1 345 M 134G
PixelÔ¨Çy 21.0 2.5 203M 27G
Table 6: ConÔ¨Åguration of the WikiText103 experiments
Model Optimizer Weight Decay Learning Rate Dropout Warmup/Epoch
GPT-2-Small Adam 0.1 0.0001 0.1 5/100
PixelÔ¨Çy Adam 0.1 0.0001 0.1 5/100
expected density is the number of non-zero entries/(4K 4K) ; actual density is the number of accessed
entries/(4K4K), e.g. even if there is only one non-zero, 32 32 entries would be accessed because the
hardware block size is 32 32.
When random patterns are generated with small block size, (e.g 11,22), the resources, such as
memory access and computes(denoted by Actual Density), required to compute a random sparse matrix
of density 1:25%are equivalent to computing a dense matrix multiplication. This is further reÔ¨Çected in
the latency: As pattern block size shrinks, deviating from the hardware block size of 3232, the random
patterns‚Äô latency worsens, whereas the PixelÔ¨Çy remains eÔ¨Écient. Vanilla ButterÔ¨Çy is 5 slower than PixelÔ¨Çy
as expected, because (1) it does not take advantage of the hardware property ‚Äì not structured sparsity(2) it
is a series of products.
L.6 EÔ¨Écient Implementation of PixelÔ¨Çy
We run all of our experiments on V100 GPUs. We rely on eÔ¨Écient implementation of block sparse matrix
multiply and block sparse attention from the libraries Triton ( https://github.com/openai/triton ) and
https://github.com/huggingface/pytorch_block_sparse . For the low-rank part, we rely on eÔ¨Écient
(dense) matrix multiplies from cuBLAS. In particular, to multiply the input xby the low-rank matrix UV>,
we multiply U(V>x).
We keep the same number of training epochs as that of the dense models (e.g., on ImageNet, the dense
43

--- PAGE 44 ---
Pattern Block size Expected Density Actual Density Latency(ms)
11 1.25% 100% 9.4
22 2.5% 99.84% 9.3
44 5% 96.24% 9.04
Random 66 10% 93.66% 8.8
88 20% 81.89% 7.7
1616 40% 34.52% 3.3
3232 80% 10.15% 1.0
ButterÔ¨Çy 11 10% 62.50% 5.2
11 1.25% 4.62% 0.48
22 2.5% 5.38% 0.56
44 5% 6.13% 0.63
PixelÔ¨Çy 66 10% 9.64% 0.96
88 10% 10.58% 1.05
1616 10% 11.30% 1.12
3232 10% 10.58% 1.04
Table 7: Microbenchmarking of diÔ¨Äerent patterns. Given GPU processes the matrix block by block of size
3232, random block pattern‚Äôs latency increases as the block size shrinks, while PixelÔ¨Çy remains eÔ¨Écient.
We measure the latency by averaging 100 runs of batch size 4096 for each conÔ¨Åguration.
model and the PixelÔ¨Çy model are trained for 300 epochs). The training speedup of the PixelÔ¨Çy models is
due to faster time per epoch.
We do not use 2:4 sparsity (available on Ampere GPUs such as A100). Such Ô¨Åne-grained sparsity is
orthogonal to our approach, and we expect that future work incorporating both 2:4 sparsity and block
sparsity to yield further speedup.
L.7 Ablation: Speed-Accuracy TradeoÔ¨Ä of PixelÔ¨Çy
Weconductanablationexperimenttoexaminethespeed-accuracytradeofPixelÔ¨Çy: ontheImageNetdataset
andthe Mixer-B/16model, wereplace thedensematriceswithÔ¨Çat blockbutterÔ¨Çy+low-rankmatrices, while
varying the compute / parameter budget. We plot the speed-accuracy tradeoÔ¨Ä in Fig. 13.
L.8 Comparison Against Original ButterÔ¨Çy
We compare PixelÔ¨Çy against original ButterÔ¨Çy matrices [Dao et al., 2020] on the ImageNet dataset and
Mixer-B/16 dense model. We present the results in Table 8. We notice another beneÔ¨Åt of PixelÔ¨Çy compared
to ButterÔ¨Çy: it trains more stably and requires less careful initialization. Since ButterÔ¨Çy is a product of
many factors, it requires careful initialization, otherwise the activation and gradient will be very large or
very small.
Table 8: The performance of PixelÔ¨Çy and original ButterÔ¨Çy on MLP-Mixer on the ImageNet benchmarks.
Model ImageNet top-1 acc. Speedup Params FLOPs
Mixer-B/16 75.6 - 59.9M 12.6G
ButterÔ¨Çy-Mixer-B/16 76.1 0.8 17.4M 4.3G
PixelÔ¨Çy-Mixer-B/16 76.3 2.3 17.4M 4.3G
44

--- PAGE 45 ---
1.0 1.5 2.0 2.5
Training speedup compared to dense model75.075.576.076.5ImageNet1k top-1 accuracyMixer-B/16 (dense)
Pixelfly Mixer-B/16 (block sparse)Figure 13: Speed-accuracy tradeoÔ¨Ä of PixelÔ¨Çy on ImageNet classiÔ¨Åcation, with Mixer-B/16 as the dense
model. PixelÔ¨Çy maintains or exceeds the accuracy of the dense model, up to around 2.3 speedup (or
around 30% of the number of parameters). Performance degrades when the PixelÔ¨Çy model has fewer than
30% of the number of parameters.
M Extended Related Work
In this section, we extend the related works referenced in the main paper and discuss them in detail.
M.1 Neural Pruning
Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections,
pruning has seen great success in compressing complex models.Han et al. [2015a,b] put forth two naive but
eÔ¨Äective algorithms to compress models up to 49x and maintain comparable accuracy. Li et al. [2016] em-
ploy Ô¨Ålter pruning to reduce the cost of running convolution models up to 38 %, Lin et al. [2017] prunes the
network at runtime, hence retaining the Ô¨Çexibility of the full model. Dong et al. [2017] prunes the network
locally in a layer by layer manner. Sanh et al. [2020] prunes with deterministic Ô¨Årst-order information, which
is more adaptive to pretrained model weights. Lagunas et al. [2021] prunes transformers models with block
sparsity pattern during Ô¨Åne-tuning, which leads to real hardware speed up while maintaining the accuracy.
Zhu and Gupta [2017] Ô¨Ånds large pruned sparse network consistently outperform the small dense networks
with the same compute and memory footprints. Although both our and all the pruning methods are aiming
to produce sparse models, we diÔ¨Äer in our emphasis on the overall eÔ¨Éciency, whereas pruning mostly focuses
on inference eÔ¨Éciency and disregards the cost in Ô¨Ånding the smaller model.
M.2 Lottery Ticket Hypothesis
Models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery
tickets Frankle and Carbin [2018] are a set of small sub-networks derived from a larger dense network, which
outperforms their parent networks in convergence speed and potentially in generalization. A huge number
of studies are carried out to analyze these tickets both empirically and theoretically: Morcos et al. [2019]
proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with
the specialized lottery tickets; Frankle et al. [2019] improves the stability of the lottery tickets by iterative
45

--- PAGE 46 ---
pruning; Frankle et al. [2020] found that subnetworks reach full accuracy only if they are stable against SGD
noise during training; Orseau et al. [2020] provides a logarithmic upper bound for the number of parameters
it takes for the optimal sub-networks to exist; Pensia et al. [2020] suggests a way to construct the lottery
ticket by solving the subset sum problem and it‚Äôs a proof by construction for the strong lottery ticket hy-
pothesis. Furthermore, follow-up works [Liu and Zenke, 2020, Wang et al., 2020, Tanaka et al., 2020] show
that we can Ô¨Ånd tickets without any training labels.
M.3 Neural Tangent Kernel
Our work rely heavily on neural tangent kernel in theoretical analysis. Neural Tangent Kernel Jacot et al.
[2018] is Ô¨Årst proposed to analyse the training dynamic of inÔ¨Ånitely wide and deep networks. The kernel is
deterministic with respect to the initialization as the width and depth go to inÔ¨Ånity, which provide an unique
mathematicaltoanalyzedeepoverparameterizednetworks. Couplesoftheoreticalworksarebuiltbasedupon
this: Lee et al. [2019] extend on the previous idea and prove that Ô¨Ånite learning rate is enough for the model
to follow NTK dynamic. Arora et al. [2019b] points out that there is still a gap between NTK and the real
Ô¨Ånite NNs. Cao and Gu [2020] sheds light on the good generalization behavior of overparameterized deep
neural networks. Arora et al. [2019a] is the Ô¨Årst one to show generalization bound independent of the network
size. Later, some works reveal the training dynamic of models of Ô¨Ånite width, pointing out the importance
of width in training: Hayou et al. [2019] analyzes stochastic gradient from the stochastic diÔ¨Äerential equa-
tions‚Äô point of view; Based on these results, we formulate and derive our theorems on sparse network training.
M.4 Overparameterized Models
Our work mainly targets overparameterized models. In Nakkiran et al. [2019], the double descendent phe-
nomenon was observed. Not long after that, d‚ÄôAscoli et al. [2020] discover the triple descendent phenomenon.
It‚Äôs conjectured in both works that the generalization error improves as the parameter count grows. On top
of that, Arora et al. [2018] speculates that overparameterization helps model optimization, and without
"enough" width, training can be stuck at local optimum. Given these intuitions, it‚Äôs not surprising that
the practitioning community is racing to break the record of the largest parameter counts: The two large
language models, GPT-2 and GPT-3 [Radford et al., 2019, Brown et al., 2020], are pushing the boundary
on text generation and understanding; Their amazing zero-shot ability earn them the title of foundation
models [Bommasani et al., 2021]. On the computer vision side, Dosovitskiy et al. [2020], Tolstikhin et al.
[2021], Zhai et al. [2021] push the top-1 accuracy on various vision benchmarks to new highs after scaling
up to 50 times the parameters; Naumov et al. [2019] shows impressive results on recommendation with a 21
billion large embedding; Jumper et al. [2021] from DeepMind solve a 50 year old grand challenge in protein
research with a 46-layer Evoformer. In our work, we show that there is a more eÔ¨Écient way to scale up
model training through sparsiÔ¨Åcation and double descent only implies the behavior of the dense networks.
46
