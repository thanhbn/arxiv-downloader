# 2112.00029.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2112.00029.pdf
# File size: 2521209 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pixelated Butterﬂy: Simple and Eﬃcient Sparse Training for
Neural Network Models
Tri Dao∗y, Beidi Chen∗y, Kaizhao Liang, Jiaming Yang, Zhao Songx, Atri Rudraz, and
Christopher Réy
yDepartment of Computer Science, Stanford University
SambaNova Systems, Inc
Department of Probability and Statistics, Peking University
xAdobe Research
zDepartment of Computer Science and Engineering, University at Buﬀalo, SUNY
{trid,beidic}@stanford.edu ,kaizhao.liang@sambanovasystems.com ,edwinyjmpku@gmail.com ,
zsong@adobe.com ,atri@buffalo.edu ,chrismre@cs.stanford.edu
May 12, 2022
Abstract
Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like
to reduce their computational cost while retaining their generalization beneﬁts. Sparse model training
is a simple and promising approach to achieve this, but there remain challenges as existing methods
struggle with accuracy loss, slow training runtime, or diﬃculty in sparsifying all model components.
The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is diﬃcult
and expensive. To address this, our main insight is to optimize over a continuous superset of sparse
matrices with a ﬁxed structure known as products of butterﬂy matrices. As butterﬂy matrices are not
hardware eﬃcient, we propose simple variants of butterﬂy (block and ﬂat) to take advantage of modern
hardware. Our method (Pixelated Butterﬂy) uses a simple ﬁxed sparsity pattern based on ﬂat block
butterﬂy and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically
validate that Pixelated Butterﬂy is 3faster than butterﬂy and speeds up training to achieve favorable
accuracy–eﬃciency tradeoﬀs. On the ImageNet classiﬁcation and WikiText-103 language modeling tasks,
our sparse models train up to 2.5 faster than the dense MLP-Mixer, Vision Transformer, and GPT-2
medium with no drop in accuracy.
1 Introduction
Recent results suggest that overparameterized neural networks generalize well [Belkin et al., 2019], but
they are expensive to train [Kaplan et al., 2020]. An ideal model should use less compute and memory
while retaining the generalization beneﬁts of large models. The simplest and most popular direction is to
sparsify these models. This idea has a long history in machine learning [LeCun et al., 1990] and has driven
fundamental progress in other ﬁelds such as statistics [Tibshirani, 1996], neuroscience [Foldiak, 2003], and
signal processing [Candes et al., 2006]. However, despite signiﬁcant eﬀorts, speeding up sparse training in
wall-clock time without degrading accuracy remains an unresolved problem.
While sparse training is an active research area, it has not seen wide adoption. First, it is diﬃcult and
expensive to ﬁnd the sparsity pattern (the possible locations of the nonzeros) that could maintain the same
level of accuracy of dense models. Many methods (pruning [Lee et al., 2018], lottery tickets [Frankle and
Carbin,2018], hashing[Kitaevetal.,2020])maintaindynamicsparsitymasks. However, thelargeoverheadof
∗Equal contribution. Order determined by coin ﬂip.
1arXiv:2112.00029v2  [cs.LG]  11 May 2022

--- PAGE 2 ---
evolving the sparsity mask can signiﬁcantly slow down training and complicate the implementation. Indeed,
these methods either require long cycles of pruning and retraining [Frankle and Carbin, 2018]1or maintain
expensive hash tables [Chen et al., 2019]. Second, most existing methods adopt unstructured sparsity,
which may be eﬃcient in theory, but do not take into account the eﬃciency of training hardware such as
GPUs (optimized for dense computation)2. Finally, most methods target a single type of operation such as
attention[Childetal.,2019,Zaheeretal.,2020], whereasneuralnetwork(NN)modelsoftencomposediﬀerent
modules (attention, MLP), and in many applications the MLP layers are the main training bottleneck [Wu
et al., 2020].
A better sparse training method should (i) be simple yet accurate, ideally with a static sparsity pattern,
(ii) be fast by aligning sparsity pattern with available hardware, and (iii) have wide coverage of operators
that applies to most NN layers. There are three technical challenges. First, we show that given a budget
(e.g., total non-zeros in a matrix), it is NP-hard to ﬁnd the optimal static sparsity pattern for a NN module
to minimize the approximation error to the dense model. Second, for each sparsity pattern, we need to take
into account hardware block-oriented eﬃciency (accessing each element in memory takes the same time as
accessing the block of adjacent elements [Cook, 2012], illustrated in Fig. 2). Common theoretical measures
of eﬃciency (e.g., number of non-zeros, FLOPs) do not map well to modern hardware designed for block
computation. Last, every diﬀerent NN module might require diﬀerent sparsity patterns, which makes the
problem even more complicated.
In our early exploration, we empirically study many sparsity patterns proposed in the literature to ﬁnd
those patterns that can closely approximate the dense model (Details in Appendix K). We found that one
sparsity pattern, namely butterﬂy + low-rank, consistently outperforms the others. This sparsity pattern
closely connects to two lines of work in matrix structures: (i) sparse + low-rank matrices, which can capture
global and local information [Candès et al., 2011, Udell and Townsend, 2019, Chen et al., 2021], and (ii) but-
terﬂy matrices [Parker, 1995, Dao et al., 2019] whose products can tightly represent any sparse matrix [De Sa
et al., 2018, Dao et al., 2020]. Using the ﬁxed sparsity pattern from butterﬂy matrices, with the addition of
a low-rank term, would address two of the three challenges above and yield a simple way to sparsify most
NN layers (that are based on matrix multiply).
However, butterﬂy matrices are ineﬃcient on modern hardware: (i) they are diﬃcult to parallelize as they
contain sequential products of many factors, and (ii) they are not hardware-friendly because the sparsity
patterns are not block-aligned. We propose two simple changes to make Butterﬂy eﬃcient while retaining
their favorable properties. Our proposal, Pixelated Butterﬂy (Pixelﬂy), combines ﬂat block butterﬂy and
low-rank matrices to yield a simple and eﬃcient sparse training method.
•We design an extremely simple sparsity pattern inspired by butterﬂy + low-rank matrices, which takes into
account the hardware’s block-oriented eﬃciency. We propose block butterﬂy matrices that are eﬃcient
as their sparsity patterns align with hardware blocks. We then introduce ﬂat butterﬂy, a ﬁrst-order
approximation of butterﬂy with residual connection, that turns the original product of factors into a sum.
Flat butterﬂy matrix multiplications are easy to parallelize. Pixelﬂy, uses the ﬁxed sparsity pattern from
ﬂat & block butterﬂy, along with a low-rank term, to produce a sparse network.
•We prove that block butterﬂy retains the expressiveness of butterﬂy matrices and can thus tightly capture
sparse matrices. We show that ﬂat butterﬂy matrices can closely approximate large classes of matrices that
butterﬂy matrices capture. Moreover, we demonstrate that ﬂat block butterﬂy + low-rank matrices are
strictly more expressive than sparse or low-rank matrices alone. Finally, leveraging the recent advance in
the neural tangent kernel (NTK), we adapt existing techniques to prove the global convergence of gradient
descent on training sparse and wide ReLU networks.
•Our proposed Pixelﬂy can be applied to all network modules that rely on matrix multiplication (e.g., linear
layer, attention, MLP). To sparsify a full network, we simply need to allocate compute budget for each
layer based on matrix and hardware block size.
We empirically validate that Pixelﬂy can speed up the training of models (Transformers, ViT, MLP-
Mixer) without quality drop compared to baselines on a wide range of domains and tasks. On CIFAR10/100
& ImageNet classiﬁcation, Pixelﬂy achieve 2.3 training time speedup compared to dense ViT, MLP-Mixer
models, and other sparse training baselines, while preserving the same accuracy. On the WikiText-103
1State-of-the-art sparse training methods require up to 5 more training epochs compared to dense models [Evci et al.,
2020]
2An unstructured sparse model with 1% nonzero weights can be as slow as a dense model [Hooker, 2020]
2

--- PAGE 3 ---
Pixelated Butterfly  
Attention
MLP Flat Block Butterfly+Model Schema  
Compute Allocation+
Low-rankAttention Mask
MLP MaskSparse Masks  
Figure 1: Pixelﬂy targets GEMM-based networks (networks whose computation is dominated by matrix multiply),
which it views as a series of matrix multiplication. For each matrix multiply from Model Schema, it (1) allocates
compute budget based on dimension and layer type, (2) the budget decides a mapping (hyper-parameter) to our
proposed ﬂat block butterﬂy sparsity patterns, (3) outputs a hardware-aware sparse mask. Note since the hardware
is a block device, one memory access to an element in a block leads to the access to the full block.
language modeling task, we speed up GPT-2 Medium training by 2.5 and achieve the same perplexity. On
the Long Range Arena benchmark, we maintain the same accuracy as Transformer with 5:2faster training
than a dense model, 2faster than Sparse transformer, and 6faster than non-block-aligned sparse methods
(Reformer). Our ablation studies highlight the importance of each of our components: our butterﬂy sparsity
improves on existing hand-crafted patterns by up to 2% of accuracy on ImageNet, our hardware-aware block-
sparsity yields up to 5 speedup, and the balanced compute budget allocation brings 2 speedup compared
to baselines that only sparsify attention.3
2 Problem Setting
We ﬁrst deﬁne the problem as sparse matrix approximation with a simple hardware cost model. Then we
brieﬂy introduce butterﬂy and sparse + low-rank matrices.
Memory Access  
Figure 2: Visualization of
memory access for a hard-
ware with block size 4: ac-
cessing the one (red) location
means accessing the full 44
block (blue).ProblemFormulation: WefocusonthetrainingofGEMM-basedmodels,
which can be viewed as a series of matrix multiplies (Given A;B2Rnd,
computeC=ABT). Speeding up training while maintaining model quality
can be mapped to ﬁnding an approximation procedure fwhich reduces the
timeTof computing Cwhile minimizing error E[kf(A;B) ABTk2
F]. Since
the hardware is a block device, accessing any individual element within a block
ofmemoryisthesameasaccessingthefullblock[Cook,2012](Fig.2). Asimple
cost model of Ton hardware with block size bwould depend on the number of
b-blocks being accessed and compute time (formal deﬁnition in Appendix A).
Our experiment (Appendix L.5) reveals that when the non-zeros are grouped
into blocks, picking the smallest block size supported by hardware can speed
up operations by 10 compared to sparsity patterns that are not block-aligned.
Butterﬂy, Sparse + Low-rank Matrices: Butterﬂy matrices have been used in numerical linear
algebra [Parker, 1995, Li et al., 2015] and machine learning [Mathieu and LeCun, 2014, Jing et al., 2017,
Munkhoeva et al., 2018, Dao et al., 2019, Choromanski et al., 2019]. They encode the recursive divide-and-
conquer structure of the fast Fourier transform (FFT) algorithm [Cooley and Tukey, 1965] and provably
capture any sparse matrix with near-optimal space and time complexity. Sparse and Low-rank structures
have been studied in Robust PCA [Candès et al., 2011], graph clustering [Jalali et al., 2011], and co-variance
estimation [Luo, 2011]. Recently it has been adopted in attention approximation for Transformers [Chen
et al., 2021].
3 Butterﬂy matrices and Pixelated Butterﬂy
Butterﬂy matrices [Parker, 1995, Dao et al., 2019] are expressive and theoretically eﬃcient. As they contain
the set of sparse matrices, we choose to search for the sparsity pattern in this larger class due to their ﬁxed
3Pixelﬂy code is available at https://github.com/HazyResearch/pixelfly
3

--- PAGE 4 ---
Butterfly  
Flat Butterfly  + + +
++
Flat Block Butterfly  
Block Butterfly  
+ + ++
+Figure 3: Visualization of Flat, Block, and Flat Block butterﬂy.
sparsity structure. However, there are three technical challenges. We highlight them here along with our
approaches to address them:
1. Slow speed: butterﬂy matrices are not friendly to modern hardware as their sparsity patterns are not
block-aligned, thus are slow. We introduce a variant of butterﬂy matrices, block butterﬂy , which operate
at the block level, yielding a block-aligned sparsity pattern.
2. Diﬃculty of parallelization: the sequential nature of butterﬂy matrices as products of many factors makes
it hard to parallelize the multiplication. We propose another class of matrices, ﬂat butterﬂy matrices, that
are the ﬁrst-order approximation of butterﬂy with residual connections. Flat butterﬂy turns the product
of factors into a sum, facilitating parallelization.
3. Reduced expressiveness of ﬂat butterﬂy: even though ﬂat butterﬂy matrices can approximate butterﬂy
matrices with residual connections, they are necessarily high-rank and cannot represent low-rank matri-
ces [Udell and Townsend, 2019]. We propose to add a low-rank matrix (that is also block-aligned) to ﬂat
butterﬂy to increase their expressiveness.
Combining these three approaches (ﬂat & block butterﬂy + low-rank), our proposal (Pixelated Butterﬂy) is
a very simple method to train sparse networks.
3.1 Block Butterﬂy Matrices
We propose a block version of butterﬂy matrices, which is more hardware-friendly than the regular butterﬂy.
The regular butterﬂy matrices Dao et al. [2019, 2020] will be a special case of block butterﬂy with block size
b= 1. We omitbin the notation if b= 1.
Deﬁnition 3.1. Ablock butterﬂy factor (denoted as Bk;b) of sizekb(wherek2) and block size bis a
matrix of the form Bk;b=D1D2
D3D4
where each Diis ak
2k
2block diagonal matrix of block size bof the
form diag 
Di;1;:::;Di;k=2
whereDi;j2Rbb. We restrict kto be a power of 2.
Deﬁnition 3.2. Ablock butterﬂy factor matrix (denoted as B(n;b)
k) of sizenbwith stride kand block
sizebis a block diagonal matrix ofn
k(possibly diﬀerent) butterﬂy factors of size kband block size b:
B(n;b)
k= diag
[Bk;b]1;[Bk;b]2;:::; [Bk;b]n
k
Deﬁnition 3.3. Ablock butterﬂy matrix of sizenbwith block size b(denoted as B(n;b)) is a matrix that
can be expressed as a product of butterﬂy factor matrices: B(n;b)=B(n;b)
nB(n;b)
n
2:::B(n;b)
2:DeﬁneBbas the
set of all matrices that can be expressed in the form B(n;b)(for somen).
3.2 Flat butterﬂy matrices
In most applications of butterﬂy matrices to neural networks, one multiplies the O(logn)butterﬂy factors.
However, this operation is hard to be eﬃciently implemented on parallel hardware (e.g., GPUs) due to
4

--- PAGE 5 ---
the sequential nature of the operation4. We instead propose to use a sum of butterﬂy factors that can
approximate the products of the factors. This sum of factors results in one sparse matrix with a ﬁxed
sparsity pattern, which yields up to 3 faster multiplication on GPUs (Appendix J).
Residual connections have been proposed to connect the butterﬂy factors [Vahid et al., 2020]. We show
that residual products of butterﬂy matrices have a ﬁrst-order approximation as a sparse matrix with a ﬁxed
sparsity. Let Mbe a matrix in the set of butterﬂy matrices B. In residual form, for some 2R:
M= (I+B(n)
n)(I+B(n)
n=2):::(I+B(n)
2): (1)
Note that this form can represent the same matrices in the class of butterﬂy matrices B, since any B(n)
k
contains the identity matrix I.
Assuming that is small, we can expand the residual and collect the terms5:
M=I+(B(n)
2+B(n)
4++B(n)
n) +eO(2):
Deﬁnition 3.4. Flat butterﬂy matrices of maximum stride k(forka power of 2) are those of the form
I+(B(n)
2+B(n)
4++B(n)
k).
Flat butterﬂy matrices of maximum stride nare the ﬁrst-order approximation of butterﬂy matrices in
residual form (Eq. (1)). Notice that ﬂat butterﬂy of maximum stride kare sparse matrices with O(nlogk)
nonzeros with a ﬁxed sparsity pattern, as illustrated in Fig. 3. We call this sparsity pattern the ﬂat butterﬂy
pattern.
Flat block butterﬂy matrices are block versions of ﬂat butterﬂy in Section 3.2 (shown in Fig. 3). We
empirically validate that ﬂat block butterﬂy matrices are up to 3 faster than block butterﬂy or regular
butterﬂy (Appendix J).
Since ﬂat butterﬂy matrices approximate the residual form of butterﬂy matrices, they have high rank if
is small (Section 4). This is one of the motivations for the addition of the low-rank term in our method.
3.3 Pixelated Butterﬂy: Flat Block Butterﬂy + Low-rank for Eﬃcient Sparse
Training
We present Pixelated Butterﬂy, an eﬃcient sparse model with a simple and ﬁxed sparsity pattern based on
butterﬂy and low-rank matrices. Our method targets GEMM-based neural networks, which are networks
whose computation is dominated by general matrix multiplies (GEMM), such as Transformer and MLP-
Mixer. As a result, we can view the network as a series of matrix multiplies.
Given a model schema (layer type, number of layers, matrix dimension) and a compute budget, Pixelated
Butterﬂy has three steps: compute budget allocation per layer, sparsity mask selection from the ﬂat butterﬂy
pattern, and model sparsiﬁcation. We describe these steps in more details:
1.Compute budget allocation : based on our cost model (Appendix A), given the layer type, number
of layers, and matrix dimension, we can ﬁnd the density (fraction of nonzero weights) of each layer type
to minimize the projected compute cost. Continuing our goal for a simple method, we propose to use a
simple rule of thumb: allocate sparsity compute budget proportional to the compute fraction of the layer.
For example, if the MLP layer and attention layers are projected to takes 60% and 40% the compute time
respectively, then allocate 60% of the sparsity compute budget to MLP and 40% to attention. We verify
in Appendix I that this simple rule of thumb produces similar results to solving for the density from the
cost model.
2.Sparsity mask selection : given a layer and a sparsity compute budget for that layer, we use one-
quarter to one-third of the budget for the low-rank part as a simple rule of thumb. We pick the rank as
a multiple of the smallest supported block size of the device (e.g., 32) so that the low-rank matrices are
also block-aligned. The remaining compute budget is used to select the sparsity mask from the ﬂat block
butterﬂy sparsity pattern: we choose the butterﬂy block size as the smallest supported block size of the
device (e.g., 32), and pick the maximum stride of the ﬂat block butterﬂy (Deﬁnition 3.4) to ﬁll up the
budget.
4Even with a very specialized CUDA kernel, butterﬂy matrix multiply ( O(nlogn)complexity) is only faster than dense
matrix multiply ( O(n2)complexity) for large values of n(around 1024) [Dao et al., 2019].
5We make the approximation rigorous in Section 4.
5

--- PAGE 6 ---
3.Model sparsiﬁcation : The resulting sparse model is simply a model whose weights or attention follow
the ﬁxed sparsity mask chosen in step 2, with the additional low-rank terms (rank also chosen in step 2).
In particular, we parameterize each weight matrix6as:W=B+ (1 )UV>, whereBis a ﬂat block
butterﬂy matrix (which is sparse), UV>is the low-rank component, and is a learnable parameter. We
train the model from scratch as usual.
Ourmethodisverysimple, butcompetitivewithmorecomplicatedproceduresthatsearchforthesparsity
pattern (Appendix K). We expect more sophisticated techniques (dynamic sparsity, a better approximation
of butterﬂy) to improve the accuracy of the method.
4 Theoretical analysis
We characterize the expressiveness of the matrices used in our method. In particular, we prove that block
butterﬂy retains the expressiveness of butterﬂy, and that ﬂat butterﬂy can accurately approximate the
residual form of butterﬂy. Moreover, ﬂat block butterﬂy + low-rank (an instance of sparse + low-rank) is
more expressive than sparse or low-rank matrices alone. Finally, we analyze the training convergence and
generalization of networks with sparse weights. All proofs are in the Appendix.
4.1 Expressiveness of Block Butterﬂy
We ﬁrst prove the expressiveness of block butterﬂy matrices.
Theorem 4.1. The set B2bofnnblock butterﬂy matrices with block size 2bcontains the set Bbofnn
block butterﬂy matrices of block size b.
By a recursive argument, the set of block butterﬂy matrices whose block size is a power of 2 contains the
set of regular butterﬂy matrices.
Dao et al. [2020] show that butterﬂy matrices can tightly represent all structured matrices, such as sparse
matrices and many fast transforms. As a result, block butterﬂy matrices can also represent those structured
matrices. In particular,
Corollary 4.2. For any constant block size bthat is a power of 2, any nbnbspare matrix with snonzeros
can be written as products of block butterﬂy matrices with block size band their transposes, with O(slogn)
parameters.
4.2 Expressiveness of Flat Butterﬂy
We now characterize how the ﬂat butterﬂy matrices approximate butterﬂy matrices. In particular, assuming
thateachbutterﬂyfactorhasboundednorm, weshowthatﬂat-butterﬂymatricescanaccuratelyapproximate
the residual form of butterﬂy with error scaling as eO(2).
Theorem 4.3. LetMbe a matrix of the form in Deﬁnition 3.4 where k=n, withBmax:= maxiB(n)
i
F
andjjcp
lognBmaxfor some constant 0<c1
2and some>0. Then
M 
I+(B(n)
2+B(n)
4++B(n)
n)
F:
We show that ﬂat butterﬂy matrices must have high-rank if is small. This is the motivation for the
addition of the low-rank term in Pixelﬂy (Section 3).
Theorem 4.4. LetMbe as in Eq. (1), withBmax:= maxiB(n)
i
Fandjjcp
lognBmaxfor some constant
0<c1
4and some>0. LetB1
max= maxikBik1. Assuming B1
maxBmax. Then
rank(I+(B(n)
2++B(n)
n)) = 
0
@Bmax
B1max2
logn
log
Bmax
B1max1
A:
6We describe how to add sparse and low-rank for attention in Appendix I
6

--- PAGE 7 ---
4.3 Expressiveness of Flat Block Butterﬂy + Low-rank
Chen et al. [2021] prove that there is a natural class of input sequences (generated by a clustering process)
whose attention matrix can only be approximated well by sparse + low-rank matrices, and not sparse or
low-rank matrices alone. We adapt their technique to show a similar result for the class of matrices we use
in Pixelﬂy.
We require an extra assumption on the clustering process compared to Chen et al. [2021]: the elements
in the input sequence form clusters with the same size. Then their attention matrix will have a large block
diagonal component well-approximated by ﬂat butterﬂy, while the rest of the attention matrix is of medium
size and is well-approximated by low-rank.
Theorem4.5 (Informal) .Thereexists aclass ofinput sequences whoseattention matricesare well-approximated
by ﬂat block butterﬂy + low-rank (a special case of sparse + low-rank) but not by sparse or low-rank alone.
The formal theorem statement and proof are in Appendix B.3.
4.4 Convergence and Generalization of Sparse Networks
There are natural questions about the training and generalization of sparse models: do they train similarly
to dense models, is their generalization close to that of dense models, and can one successfully train them
with gradient descent? Our analysis theoretically shows that the answers are yes.
Our analysis relies on the neural tangent kernel (NTK) [Jacot et al., 2018] of the network. The NTK
of two data points xandymeasures the similarity between the gradient of the network when evaluated at
xcompared to the gradient when evaluated at y. This kernel governs the dynamics of the neural network
output function f(;)throughout the training and its generalization. We build on the great literature of
NTK [Li and Liang, 2018, Du et al., 2019, Allen-Zhu et al., 2019b]. The standard result [Song and Yang,
2019] implies the following, if the NTK of the sparse model is close to the NTK of the dense model, then
(i) their training convergence speed is similar, (ii) their generalization bounds are similar. For completeness,
we state the formal result in Appendix F.
Though this result does not capture the possible regularization eﬀect of sparsity, it shows that sparse
models with small NTK diﬀerence from dense NTK preserve the generalization ability of dense models, a
subject that has been studied more extensively, both from empirical and from theoretical perspectives. We
also show that training wide and sparse networks with gradient descent converges globally, similar to the
result for wide dense networks [Du et al., 2019, Allen-Zhu et al., 2019b] in Appendix H.
5 Experiments
In this section, our goal is to demonstrate that an extremely simple ﬁxed sparsity pattern can actually speed
up sparse model training in wall-clock time without degrading model quality. Speciﬁcally, we empirically
validate three claims that suggest Pixelﬂy can improve training speed of diﬀerent model architectures while
retaining model quality on a wide range of domains and tasks.
1. Section 5.1: for image classiﬁcation tasks, we ﬁrst show the empirical NTK of ﬂat block butterﬂy +
low-rank sparsity pattern is closer to dense NKT than other baselines. Then we demonstrate our superior
end-to-end performance. Speciﬁcally, we achieve training speed up on both MLP-Mixer and ViT models
by up to 2.3wall-clock time with no drop in accuracy compared to the dense model and up to 4 
compared to RigL, BigBird and other sparse baselines.
2. Section 5.2: for language modeling and text classiﬁcation tasks, we can speed up GPT-2 small dense
model training by 2.1 , achieving a perplexity of 22.5 on wikitext-103. In addition, on Long Range
Arena (LRA) benchmark, we maintain the same accuracy but have 5:2speed-up in training.
3. Section 5.3: we show the necessity of block ﬂat butterﬂy and low-rank structures, hardware-alignment
and wide coverage of most network layers with ablation studies on these three components of Pixelﬂy.
5.1 Image Classiﬁcation
7

--- PAGE 8 ---
0255075100Epoch0.00.10.20.30.40.50.60.70.8AccuracyCIFAR100 Training DynamicsDenseBigBirdPixelﬂyNTK Distance = 0.15NTK Distance = 0.35Figure 4: NTK Comparison with
Dense Model.We evaluate the quality and eﬃciency of Pixelﬂy through three metrics:
(1) distance to training dynamic of the dense model: compare the distance
between empirical NTK kernel7of the models with candidate patterns, in-
cluding BigBird [Zaheer et al., 2020], Butterﬂy [Dao et al., 2020], and that
of the dense model, (2) upstream accuracy: compare the accuracy and train-
ing time of the Pixelﬂy, the dense counterpart, and other baselines on same
image classiﬁcation tasks, (3) downstream accuracy: compare the accuracy
of our pretrained Pixelﬂy and dense model ﬁne-tuned on downstream tasks
(Appendix L.4). The empirical NTK of the model with ﬂat block butterﬂy
+ low-rank, picked by Pixelﬂy, is closer to the NTK of the dense model.
Pixelﬂy MLP-mixer and ViT models also retain the same top-1 accuracy of
the original dense models while achieving up to 2.3 speed up.
Setup: We use three popular vision benchmarks, CIFAR-10/100 [Krizhevsky et al., 2009] and Ima-
geNet [Deng et al., 2009]. We choose recent popular Vision Transformer [Dosovitskiy et al., 2020], T2T-
ViT [Yuan et al., 2021] and MLP-Mixer [Tolstikhin et al., 2021] as representative base models. Their major
computation bottlenecks are in diﬀerent components, e.g. MLP only, attention, or both so we can evaluate
the end-to-end applicability of Pixelﬂy more clearly.
Figure5: TheperformanceofPixelﬂyandViTorMLP-MixeronCIFAR10, CI-
FAR100andImageNetbenchmarks. Wemeasuretheaccuracyandthetraining
time speedup (on ImageNet) compared to the dense model.
Model CIFAR10 CIFAR100 ImageNet Speedup
Mixer-S/16 86.4 58.7 72.4 -
Pixelﬂy-Mixer-S/16 89.8 62.9 72.6 1.7 
Mixer-B/16 87.6 59.5 75.6 -
Pixelﬂy-Mixer-B/16 90.6 65.4 76.3 2.3 
ViT-S/16 89.5 65.1 77.7 -
Pixelﬂy-ViT-S/16 91.3 66.8 77.5 1.9 
ViT-B/16 89.9 61.9 78.5 -
Pixelﬂy-ViT-B/16 92.2 65.1 78.6 2.0 Empirical NTK: To charac-
terize the training dynamic of the
sparse networks, we compute the
empirical NTK kernels for dense
Vision Transformer on CIFAR-
100. Then, we show the rela-
tive diﬀerences between kernels
of models with diﬀerent sparsity
patternsandthatofthedenseone
in Fig. 4. Speciﬁcally, we pick
a popular sparsity pattern com-
bination – Bigbird pattern [Za-
heer et al., 2020] for attention
layer and random (magnitude-
based sparsity at initialization equals to random) for MLP layer, as a representative baseline. The plot
indicates that our designed pattern, ﬂat block butterﬂy + low-rank is the closest one to that of the dense one
among all the patterns. Hence, we expect them to enjoy the most beneﬁts of their dense overparameterized
counterparts in real tasks. More details on measuring empirical NTK are covered in the Appendix L.3.
Figure 6: Comparison with a representative sparse
training baseline RigL [Evci et al., 2020].
Model ImageNet (Acc) Speedup
Mixer-S/32 58.56 -
RigL [Evci et al., 2020] 56.10 0.8
Pixelﬂy (ours) 59.61 2.1Training from scratch: We validate that Pixelﬂy
trains up to 2.3and 2.0faster than dense MLP-Mixer
andViTmodelsfromscratch, withthesameaccuracyunder
the same setting (batch size, epochs). Speciﬁcally, we spar-
sify the models with Pixelﬂy and train them on three com-
monly used vision benchmarking datasets, CIFAR-10/100
and ImageNet. We measure their Top-1 accuracy wall-clock
training time. To summarize the general trend, Fig. 5 high-
lights that our sparse vision models consistently retain the accuracy of their dense counterparts in terms of
accuracy and achieve training-time speed-up.
Furthermore, we have discussed in Section 1 that current sparse training algorithms aim to dynamic
search what could be good sparsity for eﬃcient inference but do not speed up training in wall-clock time.
But we still present the comparison results in Fig. 6 for completeness. For a fair comparison, we conduct
the experiment on Mixer-S/32 model for 100 epochs because RigL aims for sparsity on weights, while we
aim for both weights & attention. As expected, RigL does not speed up training (the pioneering work has
unstructured sparsity and does not achieve speed up on GPU) but surprisingly Pixelﬂy outperforms both
dense and RigL in terms of accuracy while achieving 2:1speedup.
7There is an emerging consensus that the NTK is an informative measure of how training and convergence behaviors of two
models are similar.
8

--- PAGE 9 ---
Figure 7: Comparison with representative sparse attention
baselines.
Model ImageNet (Acc) Speedup
T2T-ViT 81.7 -
BigBird 81.5 0.9
Sparse Transformer 81.4 1.3
Pixelﬂy 81.7 1.4Finally, we compare Pixelﬂy with BigBird
and Sparse Transformer pattern. For a fair com-
parison, we choose T2T-ViT as the base model
becauseitsmajorbottleneckisontheT2Tatten-
tion module (our baselines are eﬃcient attention
variants). We can see from Fig. 7 that Pixelﬂy
is the only one that can maintain the accuracy
and have actual speed up. Further more, Pixelﬂy
speeds up T2T module (large attention) by 1.4 compare to dense.
5.2 Language Modeling and Text Classiﬁcation
In this section, we aim to evaluate the eﬀectiveness of Pixelﬂy in the text domain, on a language modeling
task and Long Range Arena (LRA [Tay et al., 2020]) benchmarks. On WikiText-103 [Merity et al., 2016],
Pixelﬂy achieves 22.5 perplexity, which is around the same perplexity as GPT-2 small [Radford et al., 2019]
but trains 2.1faster. On LRA, Pixelﬂy obtains almost the same accuracy as the full model but gains up
to5:2speed-up.
Setup: We use WikiText-103 for language modeling and LRA for classiﬁcation tasks. We use GPT-2
small and vanilla Transformer as the base dense models. The computational bottleneck of GPT-2 small
for moderate sequence length, e.g. 512, would be on both attention and MLP layers, while the bottleneck
of transformer on LRA task is on attention since the benchmark is designed to evaluate models under
long-context scenarios.
Figure 8: The performance of Pixelﬂy, BigBird and GPT-
2-Small, Medium on WikiText-103. We measure the per-
plexity and the training speed up.
Model WikiText-103 (ppl) Speedup
GPT-2-Small 22.2 -
BigBird 23.3 0.96
Pixelﬂy 22.5 2.1
GPT-2-Medium 20.9 -
BigBird 21.5 1.1
Pixelﬂy 21.0 2.5GPT-2-Small, Medium on WikiText-103:
We show training GPT-2-Small, Medium and its
Pixelﬂy model from scratch on a commonly used
NLPbenchmarkingdataset, wikiText-103. Wemea-
sure their perplexity on that dataset, and our train-
ing speed up. All setup and ﬁnetuning hyperparam-
eters follow the ones in the original paper [Radford
et al., 2019]. We present the results in Fig. 8. It
is not hard to see that Pixelﬂy models have great
advantages in accuracy-eﬃciency tradeoﬀs since it
maintains the same perplexity as the dense model
but achieve up to 2.5 speed-up in training.
Figure 9: The performance of Pixelﬂy, Reformer and vanilla transformer on Long-
Range-Arena benchmarks. We measure the accuracy and training speed.
Model ListOps TextRetrieval ImagePathﬁnder AvgSpeedup
Transformer 36.54 63.12 80.33 41.56 73.49 59.01 -
Reformer 36.85 58.12 78.36 28.30 67.95 53.90 0.8
Pixelﬂy 37.65 66.78 80.55 42.35 72.01 59.86 5.2Vanilla Transformer on
LRA:We compare vanilla
transformer and its Pixelﬂy
models trained from scratch
on LRA benchmark. We mea-
sure the accuracy, through-
put, and training time of both
models. Each task has a dif-
ferent sequence length varying between 1024 and 4096. We follow the implementation and experimental
setting in [Xiong et al., 2021]. We compare the performance of Pixelﬂy against the dense transformer and
report the results in Fig. 9. We also include the numbers of other baselines from the same repository in the
appendix. We can see Pixelﬂy cause almost no drop in accuracy while achieving 5.2 speed-up in time.
5.3 Ablation Study
We conduct ablation studies on each component of Pixelﬂy (Details in Appendix L.5). Speciﬁcally, we
present (i) how ﬂat block butterﬂy and low-rank aﬀect the model quality, (ii) how diﬀerent block size would
aﬀect the training speed, (iii) how budget allocation aﬀects the end-to-end speed up.
Necessity of Flat Block Butterﬂy and Low-rank: (i) We apply diﬀerent parameter allocation of
ﬂat block butterﬂy and Low-rank component in Pixelﬂy Mixer-S model on CIFAR-10 under the diﬀerent
9

--- PAGE 10 ---
density varying in [0.05, 0.1, 0.2]. We found that similar to what was reported in [Chen et al., 2021], using
around1
4budget on Low-rank and3
4on ﬂat block butterﬂy achieves the best accuracy. (ii) We also compare
Pixelﬂy with baseline sparsity patterns and show it is 2:7faster than dense, 3faster than Butterﬂy, 3:2
faster than BigBird under 10%density.
Block Size: We study the accuracy-eﬃciency trade-oﬀ for ﬂat block butterﬂy and random sparsity
pattern with diﬀerent block sizes from 1-32 ( Table 7). We found that ﬁrst, under the same density, the
same sparsity patterns covered with diﬀerent block sizes could have a big diﬀerence in eﬃciency. Under the
same block, the pattern with more locality can be more eﬃcient. Last, the density can seem very small,
but actually memory access could be up to 100%of the matrix. Therefore, we always want to make full
utilization of the smallest block size that the hardware (or compiler) supported.
Budget Allocation: We sparsify diﬀerent components of ViT-small separately, including attention and
MLP. We show that their compute ratio is approximately 1 : 2, so if only sparsify one of them, the other
one will be the bottleneck preventing end-to-end speed up. Therefore, it is necessary to have an algorithm
that can sparsify all layers.
6 Related Work
Lottery Ticket Hypothesis. Models proposed in our work can be roughly seen as a class of manually
constructed lottery tickets. Lottery tickets [Frankle and Carbin, 2018] are a set of small sub-networks derived
fromalargerdensenetwork, whichoutperformstheirparentnetworks. Manyinsightfulstudies[Morcosetal.,
2019, Orseau et al., 2020, Frankle et al., 2019, 2020, Malach et al., 2020, Pensia et al., 2020] are carried out
to analyze these tickets, but it remains diﬃcult to generalize to large models due to training cost. In an
attempt, follow-up works [Wang et al., 2020, Tanaka et al., 2020] show that one can ﬁnd tickets without
training labels. We draw inspiration from one of them, Liu and Zenke [2020], which uses the NTK to
avoid using labels in sparsifying networks. Other recent works use specialized hardware to accelerate sparse
training [Goli and Aamodt, 2020, Raihan and Aamodt, 2020].
Neural Pruning. Our work is loosely related to neural network pruning. By iteratively eliminating neu-
rons and connections, pruning has seen great success in compressing complex models. Pioneering work [Han
et al., 2015a,b] shows that pruning can produce signiﬁcantly smaller and faster models for inference. Subse-
quent methods [Li et al., 2016, Lin et al., 2017, Dong et al., 2017, Sanh et al., 2020, Lagunas et al., 2021, Zhu
and Gupta, 2017] improve on the quality of the pruned models. While both our and the pruning methods
aim to produce sparse models, we target training eﬃciency, whereas pruning mostly focuses on inference
eﬃciency at the cost of sacriﬁcing training speed.
Overparameterized Models and NTK. Our analysis for sparse model convergence relies heavily
on recent advance in neural tangent kernel (NTK) [Jacot et al., 2018]. NTK is a tool which has been
widely used in analyzing overparameterized models’ convergence [Li and Liang, 2018, Du et al., 2019, Allen-
Zhu et al., 2019b,c, Song and Yang, 2019], generalization [Allen-Zhu et al., 2019a], connection to data
separability [Oymak and Soltanolkotabi, 2020], and cost per iteration [Brand et al., 2021]). Deep Double
Descent [Nakkiran et al., 2019, d’Ascoli et al., 2020] conjectures that the generalization error improves as the
parameter count grows. It is not surprising that the community is racing to break the record of the largest
parameter counts [Radford et al., 2019, Brown et al., 2020, Dosovitskiy et al., 2020, Tolstikhin et al., 2021,
Zhang et al., 2021, Naumov et al., 2019, Jumper et al., 2021].
We provide extended related work in Appendix M.
7 Conclusion
In our early exploration of many sparsity patterns with complex training procedures, we found that a simple
pattern (butterﬂy + low-rank) consistently (though not always) performed among the best. This motivated
us to propose Pixelated Butterﬂy, a simple and eﬃcient sparse training method. In our quest for simplicity
and eﬃciency, we have chosen to use ﬁxed sparsity that aligns with modern hardware, which was suﬃcient
to yield wall-clock training time speedup without sacriﬁcing accuracy. We are excited about several future
directions. Inspired by the remarkable success of model pruning for inference, it is possible that dynamic
block sparse mask could be made eﬃcient yet still accurate. Our ﬂat butterﬂy is a simple ﬁrst order
approximation of the rich class of butterﬂy matrices, and there could be more sophisticated approximations
10

--- PAGE 11 ---
that retain more expressiveness. Our method is a ﬁrst step towards the goal of making sparse models train
faster than dense models and make them more accessible to the general machine learning community.
Acknowledgments
We thank Laurel Orr, Xun Huang, Sarah Hooper, Sen Wu, Megan Leszczynski, and Karan Goel for their
helpful discussions and feedback on early drafts of the paper.
We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under
No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying
Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP,
Xilinx, LETI-CEA,Intel, IBM,Microsoft, NEC,Toshiba, TSMC,ARM,Hitachi, BASF,Accenture, Ericsson,
Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce,
Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI),
and members of the Stanford DAWN project: Facebook, Google, and VMWare. The Mobilize Center is a
Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging and
Bioengineering through Grant P41EB027060. The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions,
ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not
necessarily reﬂect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the
U.S. Government. Atri Rudra’s research is supported by NSF grant CCF-1763481.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural
networks, goingbeyondtwolayers. In Advances in neural information processing systems , pages6155–6166,
2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning , pages 242–252. PMLR, 2019b.
ZeyuanAllen-Zhu, YuanzhiLi, andZhaoSong. Ontheconvergencerateoftrainingrecurrentneuralnetworks.
InNeurIPS , 2019c.
Noga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics, Probability
and Computing , 18(1-2):3–15, 2009.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration
by overparameterization. In International Conference on Machine Learning , pages 244–253. PMLR, 2018.
SanjeevArora,SimonDu,WeiHu,ZhiyuanLi,andRuosongWang. Fine-grainedanalysisofoptimizationand
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning , pages 322–332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact
computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955 , 2019b.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias–variance trade-oﬀ. Proceedings of the National Academy of Sciences , 116(32):15849–
15854, 2019.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258 , 2021.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized) neural
networks in near-linear time. In ITCS, 2021.
11

--- PAGE 12 ---
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165 , 2020.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and
inaccurate measurements. Communications on Pure and Applied Mathematics: A Journal Issued by the
Courant Institute of Mathematical Sciences , 59(8):1207–1223, 2006.
Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal
of the ACM (JACM) , 58(3):1–37, 2011.
YuanCaoandQuanquanGu. Generalizationerrorboundsofgradientdescentforlearningover-parameterized
deep relu networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pages
3349–3356, 2020.
Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, and Anshumali Shrivastava. Slide:
In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. arXiv
preprint arXiv:1903.03129 , 2019.
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying
sparse and low-rank attention. In NeurIPS , 2021.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse trans-
formers. arXiv preprint arXiv:1904.10509 , 2019.
Krzysztof Choromanski, Mark Rowland, Wenyu Chen, and Adrian Weller. Unifying orthogonal Monte Carlo
methods. In International Conference on Machine Learning , pages 1203–1212, 2019.
DC Collins and ES Angel. The diagonal decomposition technique applied to the dynamic programming
solution of elliptic partial diﬀerential equations. Journal of Mathematical Analysis and Applications , 33
(3):467–481, 1971.
Shane Cook. CUDA Programming: A Developer’s Guide to Parallel Computing with GPUs . Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA, 1st edition, 2012. ISBN 9780124159334.
James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series.
Mathematics of computation , 19(90):297–301, 1965.
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. Learning fast algorithms for
linear transforms using butterﬂy factorizations. In International conference on machine learning , pages
1517–1527. PMLR, 2019.
Tri Dao, Nimit S Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra,
and Christopher Ré. Kaleidoscope: An eﬃcient, learnable representation for all structured linear maps.
InInternational conference on representation learning , 2020.
Stéphane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overﬁtting: Where
& why do they appear? arXiv preprint arXiv:2006.03509 , 2020.
Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, and Atri Rudra. A two-pronged progress
in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms , pages 1060–1079. SIAM, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255.
Ieee, 2009.
Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. arXiv preprint arXiv:1705.07565 , 2017.
12

--- PAGE 13 ---
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In ICLR.https://arxiv.org/pdf/1810.02054 , 2019.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making
all tickets winners. In International Conference on Machine Learning , pages 2943–2952. PMLR, 2020.
Peter Foldiak. Sparse coding in the primate cortex. The handbook of brain theory and neural networks , 2003.
Dean Foster, Howard Karloﬀ, and Justin Thaler. Variable selection is hard. In Conference on Learning
Theory, pages 696–709. PMLR, 2015.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 , 2018.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery
ticket hypothesis. arXiv preprint arXiv:1903.01611 , 2019.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity
and the lottery ticket hypothesis. In International Conference on Machine Learning , pages 3259–3269.
PMLR, 2020.
Negar Goli and Tor M. Aamodt. Resprop: Reuse sparsiﬁed backpropagation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020.
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. arXiv preprint
arXiv:1711.09224 , 3, 2017.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huﬀman coding. arXiv preprint arXiv:1510.00149 , 2015a.
Song Han, Jeﬀ Pool, John Tran, and William J Dally. Learning both weights and connections for eﬃcient
neural networks. arXiv preprint arXiv:1506.02626 , 2015b.
Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. Training dynamics of deep networks using stochastic
gradient descent via neural tangent kernel. arXiv preprint arXiv:1905.13654 , 2019.
Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489 , 2020.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. arXiv preprint arXiv:1806.07572 , 2018.
Ali Jalali, Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via convex
optimization. In ICML, 2011.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and Marin
Soljacić. Tunable eﬃcient unitary neural networks (EUNN) and their application to RNNs. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70 , pages 1733–1741. JMLR. org, 2017.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure
prediction with alphafold. Nature, 596(7873):583–589, 2021.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.
13

--- PAGE 14 ---
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient transformer. In The Interna-
tional Conference on Machine Learning (ICML) , 2020.
Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
François Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. Block pruning for faster transformers.
arXiv preprint arXiv:2109.04838 , 2021.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information
processing systems , pages 598–605, 1990.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeﬀrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
Advances in neural information processing systems , 32:8572–8583, 2019.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on
connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for eﬃcient
convnets. arXiv preprint arXiv:1608.08710 , 2016.
Yingzhou Li, Haizhao Yang, Eileen R. Martin, Kenneth L. Ho, and Lexing Ying. Butterﬂy factorization.
Multiscale Modeling & Simulation , 13(2):714–732, 2015.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent
on structured data. In NeurIPS , 2018.
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf .
Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent transfer. In
International Conference on Machine Learning , pages 6336–6347. PMLR, 2020.
Xi Luo. High dimensional low rank and sparse covariance matrix estimation via convex minimization. arXiv
preprint arXiv:1111.1133 , 199, 2011.
Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis:
Pruning is all you need. In International Conference on Machine Learning , pages 6682–6691. PMLR, 2020.
Michael Mathieu and Yann LeCun. Fast approximation of rotations and Hessians matrices. arXiv preprint
arXiv:1404.7195 , 2014.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
arXiv preprint arXiv:1609.07843 , 2016.
Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: generalizing
lottery ticket initializations across datasets and optimizers. arXiv preprint arXiv:1906.02773 , 2019.
Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, and Ivan Oseledets. Quadrature-based features
for kernel approximation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages 9165–9174. Curran
Associates, Inc., 2018.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double
descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292 , 2019.
14

--- PAGE 15 ---
Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jong-
soo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning rec-
ommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 ,
2019.
Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Logarithmic pruning is all you need. Advances in
Neural Information Processing Systems , 33, 2020.
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence
guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory ,
1(1):84–105, 2020.
D Stott Parker. Random butterﬂy transformations with applications in computational linear algebra. 1995.
Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos. Op-
timal lottery tickets via subsetsum: Logarithmic over-parameterization is suﬃcient. arXiv preprint
arXiv:2006.07990 , 2020.
Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Md Aamir Raihan and Tor M Aamodt. Sparse weight activation training. arXiv preprint arXiv:2001.01969 ,
2020.
Ilya Razenshteyn, Zhao Song, and David P Woodruﬀ. Weighted low rank approximations with provable
guarantees. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing , pages
250–263, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision , 115(3):211–252, 2015.
Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by ﬁne-tuning.
arXiv preprint arXiv:2005.07683 , 2020.
Zhao Song and Xin Yang. Quadratic suﬃces for over-parametrization via matrix chernoﬀ bound. arXiv
preprint arXiv:1906.03593 , 2019.
Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks without
any data by iteratively conserving synaptic ﬂow. arXiv preprint arXiv:2006.05467 , 2020.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eﬃcient transformers. arXiv
preprint arXiv:2011.04006 , 2020.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini. The penn treebank: an overview. Treebanks , pages
5–22, 2003.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological) , 58(1):267–288, 1996.
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-
sica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for
vision.arXiv preprint arXiv:2105.01601 , 2021.
Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal
on Mathematics of Data Science , 1(1):144–160, 2019.
Keivan Alizadeh Vahid, Anish Prabhu, Ali Farhadi, and Mohammad Rastegari. Butterﬂy transform: An
eﬃcient ﬀt based neural architecture design. In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 12021–12030. IEEE, 2020.
15

--- PAGE 16 ---
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving
gradient ﬂow. arXiv preprint arXiv:2002.07376 , 2020.
Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range
attention. arXiv preprint arXiv:2004.11886 , 2020.
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas
Singh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint
arXiv:2102.03902 , 2021.
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint
arXiv:2101.11986 , 2021.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.
Advances in Neural Information Processing Systems , 33, 2020.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv
preprint arXiv:2106.04560 , 2021.
Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji,
Jian Guan, et al. Cpm: A large-scale generative chinese pre-trained language model. AI Open , 2:93–99,
2021.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the eﬃcacy of pruning for model
compression. arXiv preprint arXiv:1710.01878 , 2017.
16

--- PAGE 17 ---
A Problem Formulation
We formulate the problem of sparse model training as sparse matrix approximation with a simple hardware
cost model (Section 2).
We ﬁrst describe our simple cost model for sparse matrix multiplication to reﬂect the fact that parallel
hardware such as GPUs are block-oriented [Cook, 2012, Gray et al., 2017]: accessing one single element
from memory costs the same as accessing one whole block of elements. We then formulate the sparse matrix
approximation in the forward pass and the backward pass. The cost model necessitates narrowing the
sparsity pattern candidates to those that are block-aligned.
Cost model We model the time cost of an operation based on the number of ﬂoating point operations
and memory access. The main feature is that our cost model takes into account memory coalescing , where
accessing a memory location costs the same as accessing the whole block of belements around it (typically
b= 16or32depending on the hardware).
LetCost membe the memory access cost (either read or write) for a block of bcontiguous elements.
Accessing any individual element within that block also costs Cost memtime. Let Cost opbe the compute
cost of a ﬂoating point operation. Let Nblockmem be the number of block memory access, and Nopbe the
number of ﬂoating point operations. Then the total cost of the operation is
Totalcost = Cost memNblockmem + Cost opNop:
This cost model is a ﬁrst order approximation of the runtime on modern hardware (GPUs), ignoring the
eﬀect of caching.
Block-aligned sparsity pattern, Block cover, and Memory access cost As the memory access cost
depends on the number of block of memory being accessed, we describe how the number of nonzero elements
in a sparse matrix relates to the number of blocks being accessed. We ﬁrst deﬁne a block cover of a sparse
mask.
Deﬁnition A.1. A sparse mask M2f0;1gmnis(b1;b2)-block-aligned if for any index i;jwhereMij= 1,
we also have Mi0j0= 1where:
i0=b1bi=b1c+r1;j0=b2bj=b2c+r2for allr1= 0;1;:::;b 1 1andr2= 0;1;:::;b 2 1:
The(b1;b2)-blockcover of a sparse mask M2f0;1gmnis the (b1;b2)-block-aligned mask M02f0;1gmn
with the least number of nonzeros such that MijM0
ijfor alli;j.
We omit the block size (b1;b2)if it is clear from context.
A sparse mask Mbeing (b1;b2)block-aligned means that if we divide Minto blocks of size b1b2, then
each block is either all zeros or all ones. To get the (b1;b2)-block cover of a sparse mask M, we simply divide
Minto blocks of size b1b2and set each block to all ones if any location in that block is one.
For a sparse matrix with sparse mask Mon a device with block size b, the number of block memory
accessNblockmem is the number of nonzero blocks in its (1;b)-block cover M0(assuming row-major storage).
This corresponds to the fact that to access a memory location on modern hardware (GPUs), the device needs
to load a whole block of belements around that location.
Fast sparse matrices means block-aligned sparsity pattern For sparsity patterns that are not block-
aligned, such as the random sparse pattern where each location is independently zero or nonzero, its (1;b)-
block cover might increase the density by a factor of close to btimes (we show this more rigorously in the
Appendix). As memory access often dominates the computation time, this means that non block-aligned
sparsity will often result is btimes slower execution than block-aligned ones. In other words, exploiting
hardware locality is crucial to obtain speed up.
Therefore, this cost model indicates that instead of searching over sparsity patterns whose total cost is
less than some budget C, we can instead search over block-aligned patterns whose number of nonzeros is less
than some limit k. For our theoretical analysis, we consider sparsity patterns that are (1;b)-block-aligned.
17

--- PAGE 18 ---
In practice, since we need to access both the matrix and its transpose (in the forward and backward pass),
we require the sparsity pattern to be both (1;b)-block-aligned and (b;1)-block-aligned. This is equivalent to
the condition that the sparsity pattern is (b;b)-block-aligned.
Sparsematrixapproximationintheforwardpass Wenowformulatethesparsematrixapproximation
in the forward pass. That is, we have weight matrix Awith input Band we would like to sparsify Awhile
minimizing the diﬀerence in the output. For easier exposition, we focus on the case where number of nonzeros
in each row is the same.
Deﬁnition A.2 (Forward regression) .Given four positive integers mndk1, matricesA2Rmd
andB2Rdn. The goal is to ﬁnd a (1;b)-block-aligned binary mask matrix M2f0;1gmdthat satisﬁes
min
M2f0;1gmdkAB (AM)Bk1
s:t:kMik0=k;8i2[d]
whereMiis thei-th row ofM.
Sparse matrix approximation in the backward pass In the backward pass to compute the gradient
wrt to the weight matrix A, we would like to sparsify the gradient CB>while preserving as much of the
gradient magnitude as possible.
DeﬁnitionA.3 (Backwardregression) .Given four positive integers mndk1, matricesB2Rdn
andC2Rmn. The goal is to ﬁnd a (1;b)-block-aligned binary mask matrix M2f0;1gmdsuch that
min
M2f0;1gmdkCB> (CB>)Mk1
s:t:kMik0=k;8i2[d]
whereMiis thei-th row ofM.
Without making any assumptions, such problems are in general computationally hard Foster et al. [2015],
Razenshteyn et al. [2016].
18

--- PAGE 19 ---
B Analysis of Butterﬂy Variants
We present formal versions of theorems in Section 4 regarding variants of butterﬂy matrices. We provide
full proofs of the results here.
B.1 Block Butterﬂy Analysis
Proof of Theorem 4.1. LetMbe annnblock butterﬂy matrix with block size b. We want to show that
Malso has a representation as an nnblock butterﬂy matrix with block size 2b.
By Deﬁnition 3.3, Mhas the form:
M=B(n
b;b)
n
bB(n
b;b)
n
2b:::B(n
b;b)
4B(n
b;b)
2:
Notice that we can combine that last two terms to form a matrix of the form B(n
2b;2b)
2(see Fig. 3). Moreover,
other terms in the product of the form B(n
b;b)
n
2ibcan also be written as B(n
2b;2b)
n
2i 12b(see Fig. 3). Thus Malso has
the form:
M=B(n
2b;2b)
n
2bB(n
2b;2b)
n
4b:::B(n
2b;2b)
2:
In other words, Mis also annnblock butterﬂy matrix with block size 2b.
Proof of Corollary 4.2. Dao et al. [2020, Theorem 3] states that any nnsparse matrix with snonzeros
can be represented as products of butterﬂy matrices and their transposes, with O(slogn)parameters.
For a constant block size bthat is a power of 2, the set of nnblock butterﬂy matrices of block size
bcontains the set of regular butterﬂy matrices by Theorem 4.1. Therefore any such nnsparse matrix
also has a representation has products of block butterﬂy matrices of block size band their transposes, with
O(slogn)parameters.
B.2 Flat Butterﬂy Analysis
We prove Theorem 4.3, which relates the ﬁrst-order approximation in the form of a ﬂat butterﬂy matrix
with the original butterﬂy matrix.
Proof of Theorem 4.3. Letn= 2mand letB1;:::;Bm2Rnnbe thembutterﬂy factor matrices (we
rename them here for simplicity of notation).
Let
E=mY
i=1(I+Bi)  
I+mX
i=1Bi!
:
Our goal is to show that kEkF.
We ﬁrst recall some properties of Frobenius norm. For any matrices AandC, we havekACkF
kAkFkCkFandkA+CkFkAkF+kCkF.
Expanding the terms of the product in E, we have
E=mX
i=2iX
s2[m];jsj=iY
j2sBj:
19

--- PAGE 20 ---
Using the above properties of Frobenius norm, we can bound E:
kEkFmX
i=2iX
s2[m];jsj=iY
j2skBjkF
mX
i=2iX
s2[m];jsj=iY
j2sBmax
=mX
i=22mi 
Bi
max
=mX
i=2(mB max)i
mX
i= 
cpi
c21X
i=0(cp)i
c2
1 cp
;
where in the last step we use the assumption that c1
2.
We now bound the rank of the ﬁrst-order approximation.
Proof of Theorem 4.4. LetM=I+Pm
i=1Bi. Note that any entry inPm
i=1Bihas absolute value at
most
mB1
maxcpB1
max
Bmax1
4;
where we use the assumption that B1
maxBmaxandc1
4.
Thus any diagonal entry in Mhas absolute value at least 1 1
4=3
4and the oﬀ-diagonal entries are at
mostcpB1
max
bm.
Alon [2009, Theorem 1.1] states that: there exists some c >0such that for any real M2Rnn, if the
diagonal elements have absolute values at least1
2and the oﬀ-diagonal elements have absolute values at most
where1
2pn1
4, then rank(M)clogn
2log 1=.
Applying this theorem to our setting, we have that
rank(M)
0
@Bmax
B1max2
m
log
Bmax
B1max1
A:
We just need to show thatB1
max
Bmax1
2cpnto satisfy the condition of the theorem.
Indeed, we have that 1Bmax
B1maxp
2nas eachkBik02n. Combining the two conditions onBmax
B1max, we
have shown that 1Bmax
B1max2cpn. This concludes the proof.
B.3 Flat Block Butterﬂy + Low-rank Analysis
We show that ﬂat butterﬂy + low-rank (an instance) of sparse + low-rank, is more expressive than either
sparse or low-rank alone. We adapt the argument from Chen et al. [2021] to show a generative process where
the attention matrix can be well approximated by a ﬂat butterﬂy + low-rank matrix, but not by a sparse or
low-rank alone.
We describe here a generative model of an input sequence to attention, parameterized by the inverse
temperature 2Rand the intra-cluster distance 2R.
20

--- PAGE 21 ---
Process 1. LetQ2Rnd, whered
(log3=2(n)), with every row of Qgenerated randomly as follows:
1. ForC= 
(n), sampleCnumber of cluster centers c1;:::;cC2Rdindependently from N(0;Id=p
d).
2. For each cluster around ci, sampleni=bnumber of elements around ci, of the form zij=ci+rij
forj= 1;:::;niwhererijN (0;Id=p
d). Assume that the total number of elements is n=cband
O(1=log1=4n).
LetQbe the matrix whose rows are the vectors zijwherei= 1;:::;Candj= 1;:::;ni. LetA=QQ>and
let the attention matrix be M= exp(A).
Theorem B.1. LetM, be the attention matrix in Process 1. Fix 2(0;1). LetR2Rnnbe a matrix.
Consider low-rank, sparse, and sparse + low-rank approximations to M. Assume (1 2) logn
O(logn).
1.Flat butterﬂy + low-rank : There exists a ﬂat butterﬂy + low-rank Rwithn1+o(1)parameters with
kM RkFn.
2.Low-rank : IfRis such that n rank(R) = 
(n), thenkM RkF
(n).
3.Sparse: IfRhas sparsity o(n2), thenkM RkF
(n).
Proof sketch. As the argument is very similar to that of Chen et al. [2021, Theorem 1], we describe here the
modiﬁcations needed to adapt their proof.
The main diﬀerence between our generative process and that of Chen et al. [2021] is that each cluster
has the same number of elements, which is the same as the block size. The resulting attention matrix will
have a large block diagonal component, similar to that Chen et al. [2021]. However, all the blocks in the
block diagonal component has the same block size, which is b. Moreover, a ﬂat block butterﬂy of block size
bcontains a block diagonal component of block size b. Therefore, this ﬂat block butterﬂy matrix plays the
same role as the sparse matrix in the proof of Chen et al. [2021]. The rest of the argument follows that of
theirs.
21

--- PAGE 22 ---
Roadmap Theanalysisofsparsenetworksisorganizedasfollows. InSectionCwelistsomebasicnotations
that will be used. In Section D we consider the problem of adding sparsity on W, and we achieve polynomial
solving time. In Section E we prove that the gradient descent can be done fast under the sparsity assumption.
In Section G we consider the problem of adding sparsity on a, and we show that minimizing the dropout
loss is equivalent with a kernel ridge regression problem. In Section H we analyze the dynamics of gradient
ﬂow and prove the convergence result.
C Notations
For a vector x, we usekxkpto denote its `pnorm, and we mainly consider p= 1;2in this paper. For a
matrixA, we usekAk0;kAk1;kAkFto denote the `0norm, entry-wise `1norm and Frobenius norm of A
respectively. For two matrices A;B2Rdm, we useABto denote their Hadamard product. We use
Tmat(n;d;m )to denote the time of multiplying ndmatrix with another dmmatrix. For a symmetric
matrixA, we usemin(A)to denote its minimum eigenvalue. We also let vec(A)be the vectorization of
a matrixAin column ﬁrst order. We use h;ito denote standard Euclidean inner product between two
vectors.
Moreover, we use N(;)to denote the Gaussian distribution with mean and covariance . We denote
the ReLU function by (z) = maxfz;0g. For an event E, we use 1fEgor1Eto denote its indicator function.
D Sparsity on hidden layer weights
D.1 Applying masks before multiplication
Given matrix A2Rnd,B2Rdn, naively computing ABtakesTmat(n;d;n ). Note that, we can also
consider the case where AandBhave diﬀerent size. For simplicity, let us consider the case where matrix A
and matrix B>have the same size.
Our goal is to ﬁnd “optimal" binary mask matrix W2f0;1gdnsuch that,
min
Wkf(AB) f(A(WB))k1
s.t.kWB;ik0=k;8i2[n]
Remark D.1. In the practical applications we care about, the function fis the activation function of neural
network, e.g., ReLU(z) = maxfz;0g.
We deﬁne a sparse targeted regression problem:
Deﬁnition D.2 (Sparse mark regression, `1version).Given a matrix B2Rdn, and a vector a2Rd, the
goal is to ﬁnd a k-sparse binary vector w2f0;1gdto minimize the following problem:
min
wka>B (a>w>)Bk1:
Naively, the above problem can be solved in ndO(k)via guess all the d
k
choices.
Lemma D.3. The targeted sparse mask regression problem can be solved in ndO(k).
Proof.We need to guess d
k
times, which becomes dO(k). Each time it takes ndoperations, thus the total
time is
nddO(k)=ndO(k):
Deﬁnition D.4 (`1version).Given three positive integers mndk1, matricesA2Rmdand
B2Rdn. We deﬁne our problem as ﬁnding the binary matrix W2f0;1gmdthat satisﬁes
min
WkAB (AW)Bk1
s:t:kWik0=k;8i2[m]:
whereWiis thei-th row ofW.
22

--- PAGE 23 ---
Theorem D.5. The problem being deﬁned as Deﬁnition D.4 can be solved in mndO(k)time.
Proof.Our problem can be decomposed into msub-problems as follows:
kAB (AW)Bk1=mX
i=1(AB)i ((AW)B)i
1
=mX
i=1AiB (AW)iB
1
=mX
i=1AiB (AiWi)B
1
whereAimeans thei-th row of matrix A. By applying Lemma D.3, each sub-problem
min
WikAiB (AiWi)Bk1
can be solved in ndO(k)time. Then the problem deﬁned in Deﬁnition D.4 can be solved in
mndO(k)=mndO(k)
time in total. Thus we ﬁnish the proof.
In the above Theorem, we show that solving the sparse mask regression problem is NP-hard. However, if
we add some mild assumptions and consider minimizing `1norm, then we can solve the regression problem
in polynomial time, as the following parts show.
Deﬁnition D.6 (`1version).Given a matrix B2Rdn
0, and a vector a2Rd
0, the goal is to ﬁnd a k-sparse
binary vector w2f0;1gdto solve
min
wka>B (a>w>)Bk1
Lemma D.7. The targeted `1version sparse mask regression problem can be solved in
O(nd+nlogn)
which is polynomial time.
Proof.We ﬁrst consider the situation when a2f0;1gd. In this case, we have
ka>B (a>w>)Bk1+k(a>w>)Bk1=ka>Bk1
whereka>Bk1is ﬁxed. So we only need to consider the following problem:
max
wk(a>w>)Bk1:
For simplicity we assume ai= 1;8i2[d], and we only need to solve
max
wkw>Bk1
wherewhaskelements equal to 1andd kelements equal to 0. Fori2[d], we compute Si=Pn
j=1Bij
which is the summation of i-th row ofB, and sort them as S(1)S(2)S(n). Then we only need to
letw(i)= 1fori2[k]and other elements equal to 0. Computing all SitakesO(nd)time, sorting Sitakes
O(nlogn)time, thus the total time consumption is O(nd+nlogn)in this case.
Next, we consider the general case when a2Rd
0. We let
Bi=aiBiandai=(
1; ai>0
0; ai= 0;8i2[d]
23

--- PAGE 24 ---
whereBiis thei-th row ofB. Then our optimization problem is equivalent to
min
wka>B (a>w>)Bk1
whereB2Rdn
0anda2f0;1gd. Thus we turn this case into the ﬁrst case. Constructing Bandatakes
O(nd)time, thus the total time consumption is also O(nd+nlogn)in this case.
Deﬁnition D.8 (`1version).Given three positive integers mndk1, matricesA2Rmd
0and
B2Rdn
0. We deﬁne our problem as ﬁnding the binary matrix W2f0;1gmdthat satisﬁes
min
WkAB (AW)Bk1
s:t:kWik0=k;8i2[m]:
whereWiis thei-th row ofW.
Theorem D.9. The problem being deﬁned as Deﬁnition D.8 can be solved in
O(mnd +mnlogn)
time.
Proof.Our problem can be decomposed into msub-problems as follows:
kAB (AW)Bk1=mX
i=1(AB)i ((AW)B)i
1
=mX
i=1AiB (AW)iB
1
=mX
i=1AiB (AiWi)B
1
whereAimeans thei-th row of matrix A. By applying Lemma D.7, each sub-problem
min
WikAiB (AiWi)Bk1
can be solved in O(nd+nlogn)time. Then the problem deﬁned in Deﬁnition D.8 can be solved in
mO(nd+nlogn) =O(mnd +mnlogn)
time in total. Thus we ﬁnish the proof.
D.2 Applying Masks After Multiplication
Deﬁnition D.10. Given matrix B2Rdn,C2Rmn. The goal is to ﬁnd a mask W2f0;1gmdwhere
each column of Wisk-sparse
min
W2f0;1gmdkCB> (CB>)Wk1
Remark D.11. TheBdeﬁned in Deﬁnition D.4 is the same as the Bdeﬁned in Deﬁnition D.10. Bis
corresponding to the Xin the neural network setting.
24

--- PAGE 25 ---
E Gradient computation
In this section we consider a neural network with one hidden layer and mneurons in this hidden layer.
Supposex2Rdis the input, W= (w1;;wm)2Rdmis the weight matrix of the ﬁrst layer, a2Rm
is the output weight, and M2f0;1gdmis the mask matrix with each column having at most knon-zero
entries. The neural network f:Rd!Ris deﬁned as
f(x) =a>
(MW)>x
:
For simplicity, we only optimize Wand ﬁxa. Consider the mean square loss
L(W) =1
2nX
i=1(f(xi) yi)2=1
2nX
i=1(a>((MW)>xi) yi)2:
In the forward computation, for a batch of data points x1;;xn2Rd, letX2Rdndenote the input data
points matrix. For convenience, we deﬁne
W(t) =W(t+ 1) W(t) = @L(W(t))
@W(t)
whereis the step size. We deﬁne function gt:Rd!Rmas
gt(x) = (f(x) y)diagf0((MW(t))>x)ga
and also denote gt(X) = (gt(x1);;gt(xn))2Rmn.
Lemma E.1. We can express W(t)as
W(t) = (Xg>
t(X))M;
and each column of W(t)has at most knon-zero entries.
Proof.From the deﬁnition, we know
W(t) = @L(W(t))
@W(t)
= nX
i=1(f(xi) yi) diagf0((MW(t))>xi)g|{z}
mma|{z}
m1x>
i|{z}
1d>
M|{z}
dm
= (nX
i=1gt(xi)x>
i)>M
= (X|{z}
dng>
t(X)|{z}
nm)M|{z}
dm:
Since each column of Mhas at most knon-zero entries, we easily know each column of W(t)also has at
mostknon-zero entries.
Lemma E.2. Suppose that matrices M2Rdm,W(t)2RdmandW(t)2Rdmare given and pre-
computed, then we can compute ft+1(X)in
O(mnk)
time. (Here ft+1(X)is the evaluation of fatW(t+ 1).)
25

--- PAGE 26 ---
Proof.The goal is to compute
ft+1(X) =a>((M|{z}
dmW(t+ 1)|{z}
dm)>X):
By using Lemma E.1, we have
(MW(t+ 1))>X= (M(W(t) + W(t)))>X
= (MW(t))>X+ (MW(t))>X
= (MW(t))>X (M(Xg>
t(X))M)>X
= (MW(t))>X ((Xg>
t(X))M)>X
= (MW(t))>X+ (W(t))>X:
Notice that we have already computed (MW(t))>X2Rmdfrom previous iteration, so we only need to
compute (W(t))>Xwhere W(t)2RdmandX2Rdn. By using Lemma E.1, each row of (W(t))>
has at most knon-zero entries, thus we can compute (W(t))>XinO(mnk)time.
Lemma E.3. Suppose that matrices M2Rdm;W(t)2Rdmandft(X)are given and pre-computed, then
we can compute@L(W(t))
@W(t)inO(mnk)time.
Proof.By using Lemma E.1, we have
@L(W(t))
@W(t)= (Xg>
t(X))M
wheregt(x) = (f(x) y)diagf0((MW(t))>x)ga2Rmandgt(X) = (gt(x1);;gt(xn))2Rmn.
We ﬁrst compute MW(t)inO(mk)time, then we can construct gt(X)2RmninnO(mk)time. Given
gt(X), since we only need to compute kmentries ofXg>
t(X), where each entry can be computed in O(n)
time, thus we can compute@L(W(t))
@W(t)inO(mnk)time.
Algorithm 1 The sparse training algorithm
1:procedure Sparse Training (fxi;yigi2[n])
2:Initialization ar;wr(0)N(0;Id)forr2[m].
3:fort= 1!Tdo
4: /*forward computation*/
5: ComputeMW(t) .TakesO(mk)time.
6:fori= 1!ndo
7: ft(xi) a>((MW(t))>xi) .TakesO(mk)time.
8: gt(xi) (f(xi) yi)diag0((MW(t))>xi)a . TakesO(mk)time.
9:end for
10: /*backward computation*/
11:gt(X) (gt(x1);;gt(xn)).
12:@L(W(t))
@W(t)= (Xg>
t(X))M . TakesO(mnk)time.
13:W(t+ 1) =W(t) + W(t) .W(t) = @L(W(t))
@W(t).
14:end for
15:end procedure
26

--- PAGE 27 ---
F Neural Tangent Kernel, Convergence, and Generalization
Our analysis relies on the neural tangent kernel (NTK) [Jacot et al., 2018] of the network.
Deﬁnition F.1. Letf(;):Rd!Rbe the function speciﬁed by a neural network with parameters 2Rp
and input dimension d. The parameter is initialized randomly from a distribution P. Then its neural
tangent kernel (NTK) [Jacot et al., 2018] is a kernel K:RdRd!Rdeﬁned by:
K(x;y) =E
P@f(x;)
@;@f(y;)
@
:
We can relate the training and generalization behavior of dense and sparse models through their NTK.
The standard result [Song and Yang, 2019] implies the following.
Proposition F.2. Letfdensedenote a ReLU neural network with Llayers with dense weight matrices dense
with NTK Kdense, and letfsparsebe the ReLU neural network with the same architecture and with weight
matricessparsewhose rows are k-sparse, and with NTK Ksparse. Letx1;:::;xNbe the inputs sampled
from some distribution PX. Suppose that the empirical NTK matrices Kd=Kdense(xi;xj)andKs=
Ksparse (xi;xj)for(i;j)2[N][N]satisfykKd Ksk.
Training. We knew the the number of iterations of dense network is min(Kd) 2n2log(1=)to reach the
training loss. For sparse network we need (min(Kd) ) 2n2log(1=).
Generalization. We knew the the number of iterations of dense network is min(Kd) 2n2log(1=)to
reach the generalization error training loss. For sparse network we need (min(Kd) ) 2n2log(1=).
These results relate the generalization bound of sparse models to that of dense models.
27

--- PAGE 28 ---
G Dropout Neural Network and KRR
We consider a two layer neural network with ReLU activation function, and write
f(W;x) :=1pmmX
r=1ar(w>
rx) =1pmmX
r=1arw>
rx1w>
rx0 (2)
wherewr(0)N(0;Id)2Rd,arunif(f 1;+1g)and all randomnesses are independent. We will ﬁx ar
during the training process and use1pmnormalization factor, both of which are in the literature of Du et al.
[2019], Song and Yang [2019], Brand et al. [2021].
Suppose the training data are (x1;y1);:::; (xn;yn)2RdR, we deﬁne the classical objective function bL
as follows:
bL(W) :=1
2nX
i=1(f(W;xi) yi)2:
The gradient with respect to loss function bLis
@bL
@wr=1pmnX
i=1(f(W;xi) yi)arxi1w>rxi0:
We consider the eﬀect of dropout on network training. For each r2[m], we introduce the mask by
deﬁning random variable ras follows:
r=(
0; with probability 1  q;
1=q; with probability q:
It is easy to see that E[r] = 0(1 q) + (1=q)q= 1andE[2
r] = 02(1 q) + (1=q)2q= 1=q. We
assumeiandjare independent for any i6=j, then E[ij] =E[i]E[j] = 1. Let= (1;;m), we
deﬁne our dropout neural net as
F(W;x; ) :=1pmmX
r=1arr(w>
rx) =1pmmX
r=1arrw>
rx1w>rx0: (3)
Dropout explicitly change the target function, since we need to minimize the `2distance between F(W;x; )
andy, instead of f(W;x)andy. Formally, we deﬁne the dropout loss as
L(W) :=1
2E
"nX
i=1(F(W;xi;) yi)2#
: (4)
We ﬁrst give an explicit formulation of Lwhich also shows the diﬀerence between LandbL.
Lemma G.1. The dropout loss deﬁned in Eq. (4)can be expressed as the sum of classical loss bLand a
regularization term as
L(W) =bL(W) +1 q
2mqnX
i=1mX
r=1(w>
rxi)2: (5)
Proof.Since E[r] = 1, we have
E
[F(W;xi;)] =1pmE
[mX
r=1arr(w>
rx)] =1pmmX
r=1ar(w>
rxi) =f(W;xi) (6)
28

--- PAGE 29 ---
holds for any i2[n]. Next, we show the diﬀerence between LandbL:
2(L(W) bL(W))
=E
"nX
i=1(F(W;xi;) yi)2#
 nX
i=1(f(W;xi) yi)2
=nX
i=1
E
h
(F(W;xi;) yi)2i
 (f(W;xi) yi)2
=nX
i=1
E

F(W;xi;)2
 f(W;xi)2
=nX
i=10
@1
mX
r1;r22[m]E[ar1ar2r1r2(w>
r1xi)(w>
r2xi)] 1
mX
r1;r22[m]ar1ar2(w>
r1xi)(w>
r2xi)1
A
=1
m1 q
qnX
i=1mX
r=1a2
r(w>
rxi)2
=1
m1 q
qnX
i=1mX
r=1(w>
rxi)2(7)
where the ﬁrst step follows from deﬁnition, the second step follows from the linearity of expectation, the third
step follows from Eq. (6), the forth step follows from expansion, the ﬁfth step follows from E[r1r2] = 1for
r16=r2andE[2
r1] =1
q, and the last step follows from a2
r= 1. Thus we have
L(W) =bL(W) +1 q
2mqnX
i=1mX
r=1(w>
rxi)2
and ﬁnish the proof.
Before we move on, we introduce some extra notations and deﬁnitions. We denote
W= vec(W) =2
6664w1
w2
...
wm3
77752Rmd; and Y =2
6664y1
y2
...
yn3
77752Rn:
Deﬁnition G.2. We deﬁne matrix G12Rnnwhich can be viewed as a Gram matrix from a kernel
associated with ReLU function as follows:
G1
ij(X) = E
wN(0;I)[x>
ixj1w>xi0;w>xj0];8i;j2[n][n]
and assume 0=min(G1)>08.
Deﬁnition G.3. We deﬁne the masked matrix W(X;)2Rnmdas
W(X;) :=1pm2
6664(x1;)
(x2;)
...
(xn;)3
7775
=1pm2
6664a111hw1;x1i0x>
1a221hw2;x1i0x>
1::: amm1hwm;x1i0x>
1
a111hw1;x2i0x>
2a221hw2;x2i0x>
2::: amm1hwm;x2i0x>
2
............
a111hw1;xni0x>
na221hw2;xni0x>
n::: amm1hwm;xni0x>
n3
7775
8According to Theorem 3.1 in Du et al. [2019], the assumption holds when xiis not parallel with xjfori6=j, which is
reasonable in reality.
29

--- PAGE 30 ---
and also deﬁne the unmasked matrix bW(X)2Rnmdas
bW(X) :=1pm2
6664a11hw1;x1i0x>
1a21hw2;x1i0x>
1::: am1hwm;x1i0x>
1
a11hw1;x2i0x>
2a21hw2;x2i0x>
2::: am1hwm;x2i0x>
2
............
a11hw1;xni0x>
na21hw2;xni0x>
n::: am1hwm;xni0x>
n3
7775:
Deﬁnition G.4. We deﬁne the masked block diagonal matrix 	W(X;)2Rmdmdas
	W(X;) :=1
mdiag
 1; 2;; m
:
where8r2[m], r2Rddis deﬁned as
 r:=a2
r2
rnX
i=1xix>
i12
hwr;xii0=2
rnX
i=1xix>
i1hwr;xii0:
We also deﬁne the unmasked block diagonal matrix b	W(X)2Rmdmdas
b	W(X) :=1
mdiag
b 1;b 2;;b m
:
where8r2[m],b r2Rddis deﬁned as
b r:=nX
i=1xix>
i1hwr;xii0:
Lemma G.5. It is easy to verify that
W(X;) =bW(X)Dand 	W(X;) =b	W(X)D2

where
D:= diag(1;;1|{z}
d;;m;;m|{z}
d)2Rmdmd:
For convenience, we will simply denote W= W(X;)and	W= 	W(X;). Then by using the above
notations, we can express our dropout loss as L(W) =1
2E[kWW Yk2
2].
Lemma G.6. If we denote =1 q
q0, then we have
L(W) =1
2kbWW Yk2
2+
2W>b	WW:
Proof.As for the ﬁrst term, we have
kbWW Yk2
2=nX
i=1(1pmmX
r=1ar1hwr;xii0x>
iwr yi)2
=nX
i=1(1pmmX
r=1ar(w>
rxi) yi)2
=nX
i=1(f(W;xi) yi)2
= 2bL(W):
30

--- PAGE 31 ---
As for the second term, since b	Wis a block diagonal matrix, we have
W>b	WW=1
mmX
r=1
w>
r 
a2
rnX
i=1xix>
i12
hwr;xii0
wr
=1
mmX
r=1nX
i=1 
(w>
rxi)(w>
rxi)>12
hwr;xii0
=1
mnX
i=1mX
r=1(w>
rxi)2:
Thus by using Lemma G.1, we have
L(W) =bL(W) +1 q
2mqnX
i=1mX
r=1(w>
rxi)2
=1
2kbWW Yk2
2+
2W>b	WW
and ﬁnish the proof.
Remark G.7. A classical kernel ridge regression problem can be deﬁned as
min
W1
2k(X)>W Yk2
2+
2kWk2
2
where:Rd!Fis a feature map. Note that Lemma G.6 breaks the dropout loss into two parts: the ﬁrst
part is an error term, and the second part can be seen as a regularization term. Thus the task of minimizing
the dropout loss L(W)is equivalent to a kernel ridge regression (KRR) problem.
31

--- PAGE 32 ---
H Dynamics of Kernel Methods (Continuous Gradient Flow)
The NTK also allows us to analyze the training convergence of sparse networks. We show that gradient
descent converges globally when training wide sparse networks. This convergence speed is similar to that of
dense models [Du et al., 2019, Allen-Zhu et al., 2019b].
In this section we will discuss the dynamics of kernel method under the mask , which adds sparsity in
the output layer. Our problem will be considered in over-parameterized scheme. First we introduce some
additionaldeﬁnitionsandnotations. WedeﬁnesymmetricGrammatrix G(W)asG(W) :=bWb>
W2Rnn.
For alli;j2[n][n], we have
G(W)ij=1
mmX
r=1a2
r1hwr;xii0;hwr;xji0x>
ixj=1
mx>
ixjmX
r=11hwr;xii0;hwr;xji0:
We deﬁne block symmetric matrix H(W)asH(W) =b>
WbW2Rmdmd. Then for all i;j2[m][m], the
(i;j)-th block of H(W)is
H(W)ij=1
maiajnX
k=1xkx>
k1hwi;xki0;hwj;xki02Rdd:
By using Lemma G.6, we consider the corresponding kernel regression problem:
min
WLk(W) = min
W1
2kbW Yk2
2+
2W>b	W (8)
whereb2Rnmd,W2Rmd1,Y2Rn1andb	2Rmdmd. The main diﬀerence from neural network is
that we assume b(related to NTK, e.g., see Deﬁnition G.3) and b	(related to regularization term, e.g., see
Deﬁnition G.4) do not change during the training process.
The gradient of Lkcan be expressed as
rWLk(W) =b>bW b>Y+b	W: (9)
We useW?to denote the optimal solution of Eq. (8), and it satisﬁes
rWLk(W)
W=W?= (b>b +b	)W? b>Y= 0: (10)
Sinceb	is a positive diagonal matrix, b 1
2exists, thus we have
W?= (b>b +b	) 1b>Y:
Next, we consider the question from a continuous gradient ﬂow aspect. In time t, we denote W(t) =
vec(W(t));b(t) =bW(t);b	(t) =b	W(t). We also denote G(t) =G(W(t))andH(t) =H(W(t)). Following
the literature of Du et al. [2019], we consider the ordinary diﬀerential equation deﬁned by
dwr(t)
dt= @Lk(W(t))
@wr(t): (11)
Lemma H.1 (Lemma 3.1 in Du et al. [2019]) .Ifm= 
(n2
2
0log(n
)), we have with probability at least 1 ,
kG(0) G1k20
4andmin(G(0))3
40.
Lemma H.2 (Lemma 3.2 in Du et al. [2019]) .Ifw1;;wmare i.i.d generated from N(0;Id), then with
probability at least 1 , the following holds. For any set of weight vectors w1;;wm2Rdthat satisfy
for anyr2[m];kwr wr(0)k2c0
n2for some small positive constant c, then matrix G2Rddsatisﬁes
kG G(0)k2<0
4andmin(G)>0
2.
The above lemma shows that for Wthat is close to W(0), the Gram matrix Galso stays close to the
initial Gram matrix G(0), and its minimal eigenvalue is lower bounded.
32

--- PAGE 33 ---
Lemma H.3 (Gradient Flow) .If we assume min(b	)0>0, then with probability at least 1 , for
w1;;wm2Rdthat satisfy8r2[m];kwr wr(0)k2c0
n2, we have
dkbW bW?k2
2
dt kbW bW?k2
2
holds some constant  >0.
Proof.By using Eq. (9) and Eq. (11), we can expressdW
dtas
dW
dt= rWLk(W) = (b>bW b>Y+b	W): (12)
Then we have
dkbW bW?k2
2
dt
=dkbW bW?k2
2
dWdW
dt
= 2(bW bW?)>b( (b>bW b>Y+b	W))
= 2(bW bW?)>b(b>bW b>Y+b	W)
= 2(bW bW?)>b(b>bW b>bW? b	W?+b	W)
= 2(bW bW?)>bb>(bW bW?) 2(bW bW?)>b(b	W b	W?)
   20kbW bW?k2
2 2(W W?)>b>bb	(W W?) (13)
where the second step follows from Eq. (12), the fourth step follows from Eq. (10), and the last step follows
from the deﬁnition that 0=min(G) =min(bb>).
As for the second term in the Eq. (13), we have
2(W W?)>b>bb	(W W?)
= 2(Wb>b W?b>b)>b	(W W?)
20(W W?)>b>b(W W?)
= 20kbW bW?k2
2 (14)
Thus by Eq. (13) and Eq. (14) we have
dkbW bW?k2
2
dt (20+ 20)kbW bW?k2
2:
By letting= 20+ 20we ﬁnish the proof.
For convenience, we denote u(t) =b(t)W(t)2Rn. Then it is easy to verify that
ui(t) =1pmmX
r=1ar(w>
rxi) =f(W(t);xi);8i2[n];
showing that u(t)is the prediction in time t.
Lemma H.4 (Convergence rate) .If we assume min(G(s))0
2holds for 0st, then we have
1.ku(t) Yk2
2e (0+2=m)tku(0) Yk2
2;
2.8r2[m];kwr(t) wr(0)k2pnku(0) Yk2
0pm:
33

--- PAGE 34 ---
Proof.From Eq. (9), we can express the dynamics by using u(t)as
du(t)
dt= b(b>bW b>Y+b	W)
=G(t)(Y u(t)) bb	W: (15)
Thus we have
dku(t) Yk2
2
dt= 2(u(t) Y)> 
G(t)(Y u(t)) bb	W
= 2(u(t) Y)>G(t)(u(t) Y) 2(u(t) Y)>bb	W
  0ku(t) Yk2
2 2(u(t) Y)>bb	W: (16)
As for the second term, we have
2(u(t) Y)>bb	W=2
m(u(t) Y)>b[b 1w1;;b mwm]>
=2
m(u(t) Y)>b[nX
i=1xi(w>
1xi);;nX
i=1xi(w>
mxi)]>
=2
m(u(t) Y)>[U1(t);;Un(t)]>(17)
where forj2[n],Uj(t)2Rcan be expressed as
Uj(t) =1pmmX
r=1 
ar1hwr;xji0x>
jnX
i=1xi(w>
rxi)
=1pmmX
r=1nX
i=1arx>
j(xix>
i)wr1hwr;xii0;hwr;xji0
=1pmmX
r=1
arx>
jwr1hwr;xji0nX
i=11hwr;xii0
:
We denote U(t) = [U1(t);;Un(t)]>2Rnand have
2(u(t) Y)>bb	W=2
m(u(t) Y)>U(t) (18)
and our dynamics becomes
dku(t) Yk2
2
dt  0ku(t) Yk2
2 2
m(u(t) Y)>U(t)
   (0+2
m)ku(t) Yk2
2 (19)
showing thatd
dt 
e(0+2=m)tku(t) Yk2
2
0. Thuse(0+2=m)tku(t) Yk2
2is a decreasing function with
respect tot, and we have
ku(t) Yk2
2e (0+2=m)tku(0) Yk2
2:
As for bounding kwr(t) wr(0)k2, we use the same method as in Lemma 3.3 of Du et al. [2019]. Thus we
complete the proof.
Finally, by combining Lemma H.1, H.2, H.3 and H.4, we have the following convergence result.
TheoremH.5 (Convergenceofgradientﬂow) .Suppose0>0,m= poly(n;1=0;1=), then with probability
at least 1 over the randomness of initialization, we have
ku(t) Yk2
2e (0+2=m)tku(0) Yk2
2:
34

--- PAGE 35 ---
The above theorem shows that in the over-parameterized setting (when mis large enough), the training
loss of the kernel ridge regression problem deﬁne in Eq. (8) converges to 0in a linear rate. By comparing our
Theorem H.5 with Theorem 3.2 in Du et al. [2019], we can ﬁnd that the introducing of regularization term
makes the convergence speed faster, though the improvement is limited. Further notice that in Section G
we prove the equivalence between minimizing the dropout loss and the kernel ridge regression problem. So
we conclude our results as:
The introducing of sparsity into neural network makes the convergence speed faster, but the improvement is
limited due to the over-parameterized scheme.
35

--- PAGE 36 ---
I Method Details
We describe some details of our method.
I.1 Compute budget allocation
We describe here a procedure to compute the budget allocation based on our cost model. This procedure is
more complicated than our simple rule of thumb in Section 3.3, and tend to produce the same allocation.
For completeness, we include the procedure here for the interested reader.
Given a parameter budget B, we ﬁnd the density of each layer type that minimize the models’ total cost
of matrix multiplication. For example, in Transformers, let daanddmbe the density of the attention and
the MLP layers. Let sbe the sequence length and dbe the feature size. The attention layer with density da
will costda(n2+nd), and the fully connected layers with density dmwill cost 2dmnd. We then set daand
dmto minimize the total cost while maintaining the parameter budget:
minimizea;ma(n2+nd) + 2mndsubject to #of trainable parameters B: (20)
As this is a problem with two variables, we can solve it in closed form.
I.2 Low-rank in Attention
In Section 3.3, we describe how to use the sparsity pattern from ﬂat block butterﬂy and the low-rank term
for weight matrices. This applies to the linear layer in MLP and the projection steps in the attention.
We also use the sparse + low-rank structure in the attention step itself. Chen et al. [2021] describes a
general method to combine sparse and low-rank attention, where one uses the sparse component to discount
the contribution from the low-rank component to ensure accurate approximation of the attention matrix.
We follow a simpler procedure, which in practice yields similar performance. We use a restricted version
of low-rank of the form a “global” sparsity mask (as shown in Fig. 12). Indeed, a sparse matrix whose sparsity
pattern follows the “global” pattern is a sum of two sparse matrices, one containing the “horizontal” global
components and one containing the “vertical” components. Let wbe the width of each of those components,
then each of them has rank at most w. Therefore, this sparse matrix has rank at most 2w, and is low-rank
(for smallw).
We also make the global component block-aligned (i.e., set wto be a multiple of the smallest supported
block size such as 32) for hardware eﬃciency.
I.3 Comparison to Other Sparsity Patterns for Attention
In the context of sparse attention, other sparsity patterns such as BigBird and Longformer also contain a
“global” component, analogous to our low-rank component. Their “local” component is contained in the
block diagonal part of the ﬂat block butterﬂy sparsity pattern.
The main diﬀerence that we do not use the random components (e.g., BigBird), and the diagonal strides
from ﬂat block butterﬂy are not found in BigBird or Longformer. Moreover, we apply the same sparsity
pattern (+ low-rank) to the linear layers in the MLP and the projection step in attention as well, allowing
our method to target most neural network layers, not just the attention layer.
I.4 Sparsity Mask for Rectangular Matrices
We have described the sparsity masks from ﬂat block butterﬂy for square matrices. For rectangular weight
matrices, we simply “stretch” the sparsity mask. The low-rank component applies to both square and
rectangular matrices (as shown in Fig. 10). We have found this to work consistently well across tasks.
36

--- PAGE 37 ---
Square Flat Block Butterfly++
++Rectangular Flat Block Butterfly  Figure 10: Sparsity Mask for Rectangular Matrices.
J Benchmarking of Butterﬂy Multiply
We validate that ﬂat butterﬂy matrices (sum of factors) can speed up multiplication on GPUs compared to
butterﬂy matrices (products of factors).
Consider the matrix M2Rnnthat can be written as products of butterﬂy factors of strides of up k(a
power of 2), with residual connection:
M= (I+B(n)
k)(I+B(n)
k=2):::(I+B(n)
2):
The ﬁrst-order approximation of Mhas the form of a ﬂat butterﬂy matrix with maximum stride k(Sec-
tion 3.2):
Mat=I+(B(n)
2++B(n)
k=2+B(n)
k):
Notice that Mis a product of log2kfactors, each has 2nnonzeros, so multiplying Mby a input vector
xcostsO(nlogk)operations (by sequentially multiplying xby the factors of M). The ﬂat version Mat
is a sparse matrix with O(nlogk)nonzeros as well, and the cost of multiplying Matxis alsoO(nlogk).
However, in practice, multiplying Matxis much more eﬃcient on GPUs than multiplying Mxbecause of
the ease of parallelization.
We measure the total time of forward and backward passes of multiplying either Matxand compare to
that of multiplying Mxfor diﬀerent maximum strides, as shown in Fig. 11. We see that “ﬂattening” the
products brings up to 3 speedup.
We use matrix size 10241024with block size 32. The input batch size is 2048. We use the block sparse
matrix multiply library from https://github.com/huggingface/pytorch_block_sparse . The speed mea-
surement is done on a V100 GPU.
37

--- PAGE 38 ---
2 4 8 16 32
Maximum stride123Speedup (x)Figure 11: Speedup of multiplying Matxcompared to multiplying Mx. Flattening the products yields up
3speedup.
Local Global (Low-rank) Butterfly Random
Figure 12: Sparsity pattern candidate components: Local corresponds to local interaction of neighboring
elements; Global (low-rank) involves the interaction between all elements and a small subset of elements;
Butterﬂy captures the interaction between elements that are some ﬁxed distance apart; Random is common
in the pruning literature.
K ExhaustedSearchingSparsityPatternsforEﬃcientSparseTrain-
ing
We describe here our early exploration of searching among diﬀerent sparsity patterns that has been proposed
in the literature. We use a metric derived from the NTK, which has emerged as one of the standard metric
to predict the training and generalization of the model. We consistently found the butterﬂy + low-rank
pattern to perform among the best.
In Appendix K.1, we describe the challenges of selecting sparsity patterns for every model components
using the a metric derived from the NTK, followed by our approaches. Then in , we describe details of empir-
ical NTK computation, which is an important step in our method implementation. Last, in Appendix K.3,
we highlight important properties of our method – it rediscovers several classical sparsity patterns, and the
sparse models can inherit the training hyperparamters of the dense models, reducing the need for hyperpa-
rameters tuning.
K.1 Challenges and Approaches
Challenge 1: We seek sparsity patterns for each model components that can closely mimic the training
dynamics of the dense counterpart. As mentioned in Theorem D.9, it is NP-hard to ﬁnd the optimal sparse
38

--- PAGE 39 ---
Algorithm 2 Model Sparsiﬁcation
1:Input: model schema 
, compute budget B, dataset subset X, sparsity mask candidate set C.
2:Kdense NTK (f; X). .Eq. (22)
3:output sparsity mask assignment sout,dmin inf
4:forM1; : : : ; Mj
j2Cj
jdo .Enumerate all sparsity mask candidate combinations
5:Letsbe the sparsity mask assignment (ti; ri; mi; ni)!Mi.
6: ifTotalCompute (s)< Bthen .Eq. (21), Check if masks satisfy budget constraint
7: LetMsbe the ﬂattened sparse masks
8: Ksparse NTK (fMs; X)
9: ds Distance (Kdense; Ksparse ) .Eq. (22)
10: ifdmin> dsthen
11: dmin ds,sout s
12: end if
13: end if
14:end for
15:return sout .Return sparsity mask assignment
matrix approximation. Although NTK provides insights and measurement on the “right” sparse model,
bruteforcely computing NTK for one-layer models with all sparsity patterns is still infeasible.
Approach 1: Sparsity Pattern Candidates. To address the above challenge, we design our search
space to be a limited set of sparsity pattern candidates, each is either a component visualized in Fig. 12
or the combination of any two of them. These components encompass the most common types of sparsity
pattern used, and can express We provide the intuition behind these sparsity components:
•Local: this block-diagonal component in the matrix corresponds to local interaction of neighboring ele-
ments. This has appeared in classical PDE discretization [Collins and Angel, 1971], and has been redis-
covered in Longformer and BigBird attention patterns.
•Global: this component involves interaction between all elements and a small subset of elements (i.e.,
“global” elements). This global pattern is low-rank, and this sparse + low-rank structure is common in
data science [Udell and Townsend, 2019], and rediscovered in Longformer and BigBird patterns as well.
•Butterﬂy: this component corresponds to interaction between elements that are some ﬁxed distance apart.
The many divide-and-conquer algorithms, such as the classical fast Fourier transform [Cooley and Tukey,
1965], uses this pattern at each step. Butterﬂy matrices reﬂects this divide-and-conquer structure, and
hence this sparsity component. The sparse transformer [Child et al., 2019] also found this pattern helpful
for attention on image data.
•Random: this component is a generalization of sparsity patterns found in one-shot magnitude, gradient,
or momentum based pruning [Lee et al., 2018]. Note that at network initialization, they are equivalent to
random sparsity.
Challenge 2: Even with a ﬁxed pool of sparsity patterns for each layer, if the model has many layers,
the number of possible layer-pattern assignments is exponentially large.
Approach 2: To further reduce the search space, we constrain each layer type (attention, MLP) to have
the same sparsity pattern. For example, if there are 10 patterns and 2 layer types, the candidate pool is
102= 100combinations.
Challenge 3: Computing the empirical NTK on the whole dataset is expensive in time and space, as it
scales quadratically in the dataset size.
Approach 3: We compute the empirical NTK on a randomly chosen subset of the data (i.e., a principal
submatrix of the empirical NTK matrix). In our experiments, we verify that increasing the subset size
beyond 1000 does not change the choices picked by the NTK heuristic. The subsampled empirical NTK can
be computed within seconds or minutes.
K.2 Algorithm Description
Our method targets GEMM-based neural networks, which are networks whose computation is dominated
by general matrix multiplies (GEMM), such as Transformer and MLP-Mixer. As a result, we can view the
network as a series of matrix multiplies. We ﬁrst deﬁne:
39

--- PAGE 40 ---
•Model schema: a list of layer types t(e.g., attention, linear layers in MLP), number of layers rof that type,
anddimensionofthematrixmultiplies mn. Wedenoteitas 
 =f(t1;r1;m1;n1);:::; (tj
j;rj
j;mj
j;nj
j)g.
•AmaskMof dimension mnis a binary matrix f0;1gmn. The compute of a mask is the total number
of ones in the matrix: compute(M) =P
i;jMij.
•Asparsity pattern Pmnfor matrix dimension mnis a set of masks fM1;:::;MjPjg, each of dimension
mn.
•Asparsity mask assignment is a mapping from a model schema 
to masksMbelonging to some sparsity
patternP:s: (t;r;m;n )!M.
•Given a set of sparsity patterns P1;:::;Pk, the set of sparsity mask candidate Cis the union of sparsity
masks in each of Pi:C=[Pi
•A sparsity pattern assignment ssatisﬁes the compute budget Bif:
TotalCompute( s) :=X
layer typelcompute(s(t;r;m;n ))B: (21)
•Letbetheﬂattenedvectorcontainingthemodelparameters,andlet Msbetheﬂattenedvectorcontaining
the sparsity mask by the sparsity mask assignment s. Letf(x)be the output of the dense network with
parameterand inputx. Then the output of the sparse network is fMs(x).
•The empirical NTK of a network fon a data subset X=fx1;:::;xjXjgis a matrix of size jXjjXj:
NTK(f;X)i;j=@f(xi)
@;@f(xj)
@
: (22)
The formal algorithm to assign the sparsity mask to each layer type is described in Algorithm 2. The
main idea is that, as the set of sparsity mask candidate is ﬁnite, we can enumerate all possible sparsity
mask assignment satisfying the budget and pick the one with the smallest NTK distance to the dense NTK.
In practice, we can use strategies to avoid explicitly enumerating all possible sparsity mask, e.g. for each
sparsity pattern, we can choose the largest sparse mask that ﬁts under the budget.
K.3 Method Properties: Rediscovering Classical Sparsity Patterns, No Addi-
tional Hyperparameter Tuning
When applied to the Transformer architecture, among the sparsity components described in Appendix K.1,
the NTK-guided heuristic consistently picks the local and global components for boththe attention and
MLP layers. Moreover, the butterﬂy component is also consistently picked for image data, reﬂecting the 2D
inductive bias in this component9. While some of these patterns have been proposed for sparse attention,
it is surprising that they are also picked for the MLP layers. The most popular type of sparsity pattern
in MLP layers is top-k (in magnitude or gradient, which at initialization is equivalent to random sparsity).
We have proved that lower NTK diﬀerence results in better generalization bound for the sparse model. As
expected, we observe that this allows the sparse model to use the same hyperparamters (optimizer, learning
rate, scheduler) as the dense model (Section 5).
9Convolution (commonly used in image data) can be written in terms of the fast Fourier transform, which has this same
sparse pattern at each step of the algorithm
40

--- PAGE 41 ---
L Experiment Details
L.1 Datasets
•Cifar10 [Krizhevsky et al., 2009] consists of 60000 coloured images of resolution 32 32. Each of
them belong to one of 10 classes, including airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships,
and trucks. Among these, 50000 images are allocated to be the training set and 10000 images the
testing set.
•Cifar100 [Krizhevsky et al., 2009] is similar to Cifar10. It also consists of images of resolution 32 
32. In total, there are 60000 images, each of which belongs to one of 100 classes. Each of the 100
classes has 500 images in training set and 100 images in testing set.
•ImageNet1K [Russakovsky et al., 2015] spans 1000 object classes, containing 1,281,167 training im-
ages, 50,000 validation images and 100,000 test images. Although images are collected in diﬀerent
resolutions, in practice they are generally reshaped and cropped into 224 224.
•WikiText-103 [Merity et al., 2016] contains articles from the wikipedia page. It extracts veriﬁed
articles from Wikipedia, which add up to over 100 million tokens. Compared to other datasets, such
as Penn Treebank (PTB) [Taylor et al., 2003], WikiText features a larger vocabulary and preserves
original upper/lower cases, punctuation and numbers.
L.2 Model Conﬁgurations and Hyperparameter
We summarize the details required to replicate our experiments below.
Baseline Model: Except for dense model. We choose our baselines for each experiment base on the
following. RigL aims to sparsify model weights/parameters, so we use it as a baseline in MLP-based models
(Mixer). BigBird focuses on attention matrices, so we used it as a baseline in Transformer-based models
(ViT, GPT-2).
L.2.1 Image Classiﬁcation
Table 1: Conﬁguration of the Cifar10 experiments.
Model Optimizer Weight Decay Learning Rate Drop Path Warmup/Epoch
ViT-Small AdamW 0.05 0.0005 0.1 5/300
Pixelﬂy-ViT-Small AdamW 0.05 0.0005 0 5/300
ViT-Base AdamW 0.05 0.0005 0.1 5/300
Pixelﬂy-ViT-Base AdamW 0.05 0.0005 0 5/300
Mixer-Small AdamW 0.1 0.0005 0.1 5/300
Pixelﬂy-Mixer-Small AdamW 0.1 0.0005 0 5/300
Mixer-Base AdamW 0.1 0.0005 0.1 5/300
Pixelﬂy-Mixer-Base AdamW 0.1 0.0005 0 5/300
We report more details on the models, including number of parameters and FLOPs, in Table 4.
We follow the naming convention in the Vision Transformer paper and MLP-Mixer paper. In particular,
ViT-S and ViT-B refers to the small and base ViT models respectively, and 16 refers to the patch size of
16x16. The MLP-Mixer models follows the same convention.
L.2.2 Language Modeling
We report more details on the models, including number of parameters and FLOPs, in Table 5 and Table 6.
41

--- PAGE 42 ---
Model Optimizer Weight Decay Learning Rate Drop Path Warmup/Epoch
ViT-Small AdamW 0.05 0.0005 0.1 5/300
Pixelﬂy-ViT-Small AdamW 0.05 0.0005 0 5/300
ViT-Base AdamW 0.05 0.0005 0.1 5/300
Pixelﬂy-ViT-Base AdamW 0.05 0.0005 0 5/300
Mixer-Small AdamW 0.1 0.0005 0.1 5/300
Pixelﬂy-Mixer-Small AdamW 0.1 0.0005 0 5/300
Mixer-Base AdamW 0.1 0.0005 0.1 5/300
Pixelﬂy-Mixer-Base AdamW 0.1 0.0005 0 5/300
Table 2: Conﬁguration of the Cifar100 experiments
Model Optimizer Weight Decay Learning Rate Drop Path Warmup/Epoch
ViT-Small AdamW 0.05 0.001 0.1 5/300
Pixelﬂy-ViT-Small AdamW 0.05 0.001 0 5/300
ViT-Base AdamW 0.05 0.001 0.1 5/300
Pixelﬂy-ViT-Base AdamW 0.05 0.001 0 5/300
Mixer-Small AdamW 0.1 0.001 0.1 5/300
Pixelﬂy-Mixer-Small AdamW 0.1 0.001 0 5/300
Mixer-Base AdamW 0.1 0.001 0.1 5/300
Pixelﬂy-Mixer-Base AdamW 0.1 0.001 0 5/300
Table 3: Conﬁguration of the ImageNet experiment
L.3 Measuring Empirical NTK
The Empirical NTK is a rough estimation of the real NTK, in which the width of the neural net goes to
inﬁnity. As the width grows, the kernel gets closer to its inﬁnite-width limit. Fortunately, both our models
of interest, MLP-Mixer and Vision Transformer, are wide and overly parameterized. Therefore they are
only one step away from the real NTK domain. This allows us to use the Empirical NTK to approximately
predict their training behaviors.
As described in equation 22, we ﬁrst compute the gradient of each data sample, then we compute pair-
wise product to construct the Empirical NTK. Although we use a relatively small dataset, it’s still expensive
to build a kernel for large models, such as ViTs and MLP-Mixers. In practice, we ﬁnd that it’s suﬃcient to
compute kernels for a subsampled dataset.
MLP-Mixer and Vision Transformer each represent one type of module of interest for our sparsiﬁcation.
In MLP-Mixer, we study the sparse behavior of the Linear module, whereas, in Vision Transformer, we
mainly focus on sparsifying attention. All models are ﬁrst sparsiﬁed to around 10%of the original dense
compute. Then we compare their NTK kernels with their original dense kernel. We run three random seeds
to eliminate noise, i.e., three diﬀerent initializations for each pair of conﬁgurations. We report the mean
relative diﬀerence between the kernels with respect to the norm of the dense kernel.
L.4 Transfer Learning Experiments
We conduct extended experiments to test the generalization of our pretrained sparse models on downstream
tasks. Speciﬁcally, we ﬁnetune Pixelﬂy pretrained model (ImageNet) on CIFAR-10 and show that it get
99.03% accuracy compared to 98.77% on our pretrained dense ViT-B/16 model. In addition, we see more
than 2speed up on downstream task ﬁne-tuning process as well.
L.5 Microbenchmarking
In this section, we perform microbenchmarking on a 4K 4K sparse matrix multiplication. We aim to
show that Pixelﬂy patterns are far more hardware friendly than random patterns. For a 4K 4K matrix,
42

--- PAGE 43 ---
Table 4: The performance of Pixelﬂy and ViT or MLP-Mixer on the ImageNet benchmarks, including the
number of parameters and FLOPs. We measure the accuracy and the training time speedup (on ImageNet)
compared to the dense model.
Model ImageNet top-1 acc. Speedup Params FLOPs
Mixer-S/16 72.4 - 18.5M 3.8G
Pixelﬂy-Mixer-S/16 72.6 1.7 5.9M 1.3G
Mixer-B/16 75.6 - 59.9M 12.6G
Pixelﬂy-Mixer-B/16 76.3 2.3 17.4M 4.3G
ViT-S/16 77.7 - 48.8M 9.9G
Pixelﬂy-ViT-S/16 77.5 1.9 16.9M 3.6G
ViT-B/16 78.5 - 86.6M 17.6G
Pixelﬂy-ViT-B/16 78.6 2.0 28.2M 6.1G
Table 5: The performance of Pixelﬂy, BigBird and GPT-2-Small on WikiText-103, including the number of
parameters and FLOPs. We measure the perplexity and the training speed up.
Model WikiText-103 (ppl) Speedup Params FLOPS
GPT-2-Small 22.2 - 117M 48.4G
BigBird 23.3 0.96 117M 40.2G
Pixelﬂy 22.5 2.1 68M 18.5G
GPT-2-Medium 20.9 - 345 M 168G
BigBird 21.5 1.1 345 M 134G
Pixelﬂy 21.0 2.5 203M 27G
Table 6: Conﬁguration of the WikiText103 experiments
Model Optimizer Weight Decay Learning Rate Dropout Warmup/Epoch
GPT-2-Small Adam 0.1 0.0001 0.1 5/100
Pixelﬂy Adam 0.1 0.0001 0.1 5/100
expected density is the number of non-zero entries/(4K 4K) ; actual density is the number of accessed
entries/(4K4K), e.g. even if there is only one non-zero, 32 32 entries would be accessed because the
hardware block size is 32 32.
When random patterns are generated with small block size, (e.g 11,22), the resources, such as
memory access and computes(denoted by Actual Density), required to compute a random sparse matrix
of density 1:25%are equivalent to computing a dense matrix multiplication. This is further reﬂected in
the latency: As pattern block size shrinks, deviating from the hardware block size of 3232, the random
patterns’ latency worsens, whereas the Pixelﬂy remains eﬃcient. Vanilla Butterﬂy is 5 slower than Pixelﬂy
as expected, because (1) it does not take advantage of the hardware property – not structured sparsity(2) it
is a series of products.
L.6 Eﬃcient Implementation of Pixelﬂy
We run all of our experiments on V100 GPUs. We rely on eﬃcient implementation of block sparse matrix
multiply and block sparse attention from the libraries Triton ( https://github.com/openai/triton ) and
https://github.com/huggingface/pytorch_block_sparse . For the low-rank part, we rely on eﬃcient
(dense) matrix multiplies from cuBLAS. In particular, to multiply the input xby the low-rank matrix UV>,
we multiply U(V>x).
We keep the same number of training epochs as that of the dense models (e.g., on ImageNet, the dense
43

--- PAGE 44 ---
Pattern Block size Expected Density Actual Density Latency(ms)
11 1.25% 100% 9.4
22 2.5% 99.84% 9.3
44 5% 96.24% 9.04
Random 66 10% 93.66% 8.8
88 20% 81.89% 7.7
1616 40% 34.52% 3.3
3232 80% 10.15% 1.0
Butterﬂy 11 10% 62.50% 5.2
11 1.25% 4.62% 0.48
22 2.5% 5.38% 0.56
44 5% 6.13% 0.63
Pixelﬂy 66 10% 9.64% 0.96
88 10% 10.58% 1.05
1616 10% 11.30% 1.12
3232 10% 10.58% 1.04
Table 7: Microbenchmarking of diﬀerent patterns. Given GPU processes the matrix block by block of size
3232, random block pattern’s latency increases as the block size shrinks, while Pixelﬂy remains eﬃcient.
We measure the latency by averaging 100 runs of batch size 4096 for each conﬁguration.
model and the Pixelﬂy model are trained for 300 epochs). The training speedup of the Pixelﬂy models is
due to faster time per epoch.
We do not use 2:4 sparsity (available on Ampere GPUs such as A100). Such ﬁne-grained sparsity is
orthogonal to our approach, and we expect that future work incorporating both 2:4 sparsity and block
sparsity to yield further speedup.
L.7 Ablation: Speed-Accuracy Tradeoﬀ of Pixelﬂy
Weconductanablationexperimenttoexaminethespeed-accuracytradeofPixelﬂy: ontheImageNetdataset
andthe Mixer-B/16model, wereplace thedensematriceswithﬂat blockbutterﬂy+low-rankmatrices, while
varying the compute / parameter budget. We plot the speed-accuracy tradeoﬀ in Fig. 13.
L.8 Comparison Against Original Butterﬂy
We compare Pixelﬂy against original Butterﬂy matrices [Dao et al., 2020] on the ImageNet dataset and
Mixer-B/16 dense model. We present the results in Table 8. We notice another beneﬁt of Pixelﬂy compared
to Butterﬂy: it trains more stably and requires less careful initialization. Since Butterﬂy is a product of
many factors, it requires careful initialization, otherwise the activation and gradient will be very large or
very small.
Table 8: The performance of Pixelﬂy and original Butterﬂy on MLP-Mixer on the ImageNet benchmarks.
Model ImageNet top-1 acc. Speedup Params FLOPs
Mixer-B/16 75.6 - 59.9M 12.6G
Butterﬂy-Mixer-B/16 76.1 0.8 17.4M 4.3G
Pixelﬂy-Mixer-B/16 76.3 2.3 17.4M 4.3G
44

--- PAGE 45 ---
1.0 1.5 2.0 2.5
Training speedup compared to dense model75.075.576.076.5ImageNet1k top-1 accuracyMixer-B/16 (dense)
Pixelfly Mixer-B/16 (block sparse)Figure 13: Speed-accuracy tradeoﬀ of Pixelﬂy on ImageNet classiﬁcation, with Mixer-B/16 as the dense
model. Pixelﬂy maintains or exceeds the accuracy of the dense model, up to around 2.3 speedup (or
around 30% of the number of parameters). Performance degrades when the Pixelﬂy model has fewer than
30% of the number of parameters.
M Extended Related Work
In this section, we extend the related works referenced in the main paper and discuss them in detail.
M.1 Neural Pruning
Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections,
pruning has seen great success in compressing complex models.Han et al. [2015a,b] put forth two naive but
eﬀective algorithms to compress models up to 49x and maintain comparable accuracy. Li et al. [2016] em-
ploy ﬁlter pruning to reduce the cost of running convolution models up to 38 %, Lin et al. [2017] prunes the
network at runtime, hence retaining the ﬂexibility of the full model. Dong et al. [2017] prunes the network
locally in a layer by layer manner. Sanh et al. [2020] prunes with deterministic ﬁrst-order information, which
is more adaptive to pretrained model weights. Lagunas et al. [2021] prunes transformers models with block
sparsity pattern during ﬁne-tuning, which leads to real hardware speed up while maintaining the accuracy.
Zhu and Gupta [2017] ﬁnds large pruned sparse network consistently outperform the small dense networks
with the same compute and memory footprints. Although both our and all the pruning methods are aiming
to produce sparse models, we diﬀer in our emphasis on the overall eﬃciency, whereas pruning mostly focuses
on inference eﬃciency and disregards the cost in ﬁnding the smaller model.
M.2 Lottery Ticket Hypothesis
Models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery
tickets Frankle and Carbin [2018] are a set of small sub-networks derived from a larger dense network, which
outperforms their parent networks in convergence speed and potentially in generalization. A huge number
of studies are carried out to analyze these tickets both empirically and theoretically: Morcos et al. [2019]
proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with
the specialized lottery tickets; Frankle et al. [2019] improves the stability of the lottery tickets by iterative
45

--- PAGE 46 ---
pruning; Frankle et al. [2020] found that subnetworks reach full accuracy only if they are stable against SGD
noise during training; Orseau et al. [2020] provides a logarithmic upper bound for the number of parameters
it takes for the optimal sub-networks to exist; Pensia et al. [2020] suggests a way to construct the lottery
ticket by solving the subset sum problem and it’s a proof by construction for the strong lottery ticket hy-
pothesis. Furthermore, follow-up works [Liu and Zenke, 2020, Wang et al., 2020, Tanaka et al., 2020] show
that we can ﬁnd tickets without any training labels.
M.3 Neural Tangent Kernel
Our work rely heavily on neural tangent kernel in theoretical analysis. Neural Tangent Kernel Jacot et al.
[2018] is ﬁrst proposed to analyse the training dynamic of inﬁnitely wide and deep networks. The kernel is
deterministic with respect to the initialization as the width and depth go to inﬁnity, which provide an unique
mathematicaltoanalyzedeepoverparameterizednetworks. Couplesoftheoreticalworksarebuiltbasedupon
this: Lee et al. [2019] extend on the previous idea and prove that ﬁnite learning rate is enough for the model
to follow NTK dynamic. Arora et al. [2019b] points out that there is still a gap between NTK and the real
ﬁnite NNs. Cao and Gu [2020] sheds light on the good generalization behavior of overparameterized deep
neural networks. Arora et al. [2019a] is the ﬁrst one to show generalization bound independent of the network
size. Later, some works reveal the training dynamic of models of ﬁnite width, pointing out the importance
of width in training: Hayou et al. [2019] analyzes stochastic gradient from the stochastic diﬀerential equa-
tions’ point of view; Based on these results, we formulate and derive our theorems on sparse network training.
M.4 Overparameterized Models
Our work mainly targets overparameterized models. In Nakkiran et al. [2019], the double descendent phe-
nomenon was observed. Not long after that, d’Ascoli et al. [2020] discover the triple descendent phenomenon.
It’s conjectured in both works that the generalization error improves as the parameter count grows. On top
of that, Arora et al. [2018] speculates that overparameterization helps model optimization, and without
"enough" width, training can be stuck at local optimum. Given these intuitions, it’s not surprising that
the practitioning community is racing to break the record of the largest parameter counts: The two large
language models, GPT-2 and GPT-3 [Radford et al., 2019, Brown et al., 2020], are pushing the boundary
on text generation and understanding; Their amazing zero-shot ability earn them the title of foundation
models [Bommasani et al., 2021]. On the computer vision side, Dosovitskiy et al. [2020], Tolstikhin et al.
[2021], Zhai et al. [2021] push the top-1 accuracy on various vision benchmarks to new highs after scaling
up to 50 times the parameters; Naumov et al. [2019] shows impressive results on recommendation with a 21
billion large embedding; Jumper et al. [2021] from DeepMind solve a 50 year old grand challenge in protein
research with a 46-layer Evoformer. In our work, we show that there is a more eﬃcient way to scale up
model training through sparsiﬁcation and double descent only implies the behavior of the dense networks.
46
