Based on the text content provided in your message, I'll now provide the complete Vietnamese translation:

# 2311.05884.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2311.05884.pdf
# Kích thước file: 1847620 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Hiformer: Học Tương tác Đặc trưng Dị chủng với Transformers cho Hệ thống Gợi ý
Huan Gui
Google DeepMind
Mountain View, California, USA
hgui@google.comRuoxi Wang
Google DeepMind
Mountain View, California, USA
ruoxi@google.comKe Yin
Google Inc
Mountain View, California, USA
keyin@google.com
Long Jin
Google Inc
Mountain View, California, USA
longjin@google.comMaciej Kula
Google DeepMind
Mountain View, California, USA
maciejkula@google.comTaibai Xu
Google Inc
Mountain View, California, USA
taibaixu@google.com
Lichan Hong
Google DeepMind
Mountain View, California, USA
lichan@google.comEd H. Chi
Google DeepMind
Mountain View, California, USA
edchi@google.com

TÓM TẮT
Học tương tác đặc trưng là xương sống quan trọng để xây dựng hệ thống gợi ý. Trong các ứng dụng quy mô web, việc học tương tác đặc trưng cực kỳ thách thức do không gian đặc trưng đầu vào thưa thớt và lớn; đồng thời, việc tạo ra thủ công các tương tác đặc trưng hiệu quả là không khả thi vì không gian giải pháp theo cấp số nhân. Chúng tôi đề xuất tận dụng kiến trúc dựa trên Transformer với các lớp attention để tự động nắm bắt tương tác đặc trưng. Kiến trúc Transformer đã chứng kiến thành công lớn trong nhiều lĩnh vực, chẳng hạn như xử lý ngôn ngữ tự nhiên và thị giác máy tính. Tuy nhiên, chưa có nhiều việc áp dụng kiến trúc Transformer cho mô hình hóa tương tác đặc trưng trong ngành công nghiệp. Chúng tôi hướng đến việc thu hẹp khoảng cách này. Chúng tôi xác định hai thách thức chính khi áp dụng kiến trúc Transformer vanilla cho hệ thống gợi ý quy mô web: (1) Kiến trúc Transformer không thể nắm bắt các tương tác đặc trưng dị chủng trong lớp self-attention; (2) Độ trễ phục vụ của kiến trúc Transformer có thể quá cao để triển khai trong hệ thống gợi ý quy mô web. Đầu tiên chúng tôi đề xuất một lớp self-attention dị chủng, đây là một sửa đổi đơn giản nhưng hiệu quả cho lớp self-attention trong Transformer, để tính đến tính dị chủng của tương tác đặc trưng. Sau đó chúng tôi giới thiệu Hiformer (Heterogeneous Interaction Trans former) để cải thiện thêm khả năng biểu đạt của mô hình. Với xấp xỉ hạng thấp và cắt tỉa mô hình, Hiformer có thể suy luận nhanh cho triển khai trực tuyến. Kết quả thí nghiệm offline mở rộng xác nhận tính hiệu quả và hiệu suất của mô hình Hiformer. Chúng tôi đã triển khai thành công mô hình Hiformer vào một ứng dụng thực tế

Quyền được cấp để tạo bản sao kỹ thuật số hoặc bản cứng toàn bộ hoặc một phần của tác phẩm này cho mục đích cá nhân hoặc lớp học được cấp miễn phí với điều kiện các bản sao không được tạo hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu. Bản quyền cho các thành phần của tác phẩm này thuộc sở hữu của những bên khác ngoài ACM phải được tôn trọng. Việc tóm tắt có ghi tín dụng được cho phép. Để sao chép theo cách khác, hoặc xuất bản lại, đăng trên máy chủ hoặc phân phối lại thành danh sách, cần có sự cho phép cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.
Conference'17, July 2017, Washington, DC, USA
©2023 Association for Computing Machinery.
ACM ISBN 123-4567-24-567/08/06. . . $15.00
https://doi.org/10.475/123_4quy mô lớn mô hình xếp hạng App tại Google Play, với cải thiện đáng kể trong các chỉ số tương tác chính (lên đến +2.66%).

TỪ KHÓA
Tương tác Đặc trưng Dị chủng, Transformer, Hệ thống Gợi ý

Định dạng Tham chiếu ACM:
Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, và Ed H. Chi. 2023. Hiformer: Học Tương tác Đặc trưng Dị chủng với Transformers cho Hệ thống Gợi ý. Trong Kỷ yếu Hội nghị ACM (Conference'17). ACM, New York, NY, USA, 10 trang.
https://doi.org/10.475/123_4

1 GIỚI THIỆU
Internet bị ngập tràn bởi vô số thông tin, khiến người dùng khó có thể điều hướng hiệu quả và tìm kiếm nội dung liên quan. Hệ thống gợi ý lọc thông tin và trình bày nội dung phù hợp nhất cho từng người dùng [35,49]. Việc hình thành hệ thống gợi ý như một bài toán học máy có giám sát là phổ biến, với mục tiêu tăng cường tương tác tích cực của người dùng với các gợi ý, chẳng hạn như nhấp chuột [40,45,46], xem [50], hoặc mua hàng [36]. Do đó, việc triển khai hệ thống gợi ý với độ chính xác dự đoán cao có tầm quan trọng tối cao vì nó có thể có ảnh hưởng trực tiếp đến hiệu suất tài chính của doanh nghiệp, chẳng hạn như doanh số và doanh thu [3–5, 49].

Tương tác đặc trưng là nơi nhiều đặc trưng có tác động cộng tác phức tạp đến việc dự đoán kết quả [43,46]. Đây là một trong những thành phần quan trọng của hệ thống gợi ý. Ví dụ, người ta quan sát thấy người dùng thường xuyên tải xuống ứng dụng giao đồ ăn trong giờ ăn [8], cho thấy tương tác đặc trưng của id ứng dụng (ví dụ: ứng dụng giao đồ ăn) và bối cảnh thời gian (ví dụ: giờ ăn) có thể cung cấp tín hiệu quan trọng để dự đoán hành vi người dùng và đưa ra gợi ý cá nhân hóa. Tuy nhiên, việc mô hình hóa tương tác đặc trưng trong hệ thống gợi ý quy mô web đặt ra ba thách thức đáng kể. Trước hết, các tương tác đặc trưng phù hợp thường cụ thể theo miền, việc tạo thủ công các tương tác đặc trưng tốn thời gian và đòi hỏi kiến thức chuyên môn. Thứ hai, đối với

--- TRANG 2 ---
Conference'17, July 2017, Washington, DC, USA Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, và Ed H. Chi

các ứng dụng quy mô web, do số lượng lớn các đặc trưng thô, việc tìm kiếm các tương tác đặc trưng có thể có không gian giải pháp theo cấp số nhân. Điều này khiến việc trích xuất thủ công tất cả các tương tác trở nên không khả thi. Cuối cùng nhưng không kém quan trọng, các tương tác đặc trưng được trích xuất có thể không tổng quát hóa tốt cho các tác vụ khác. Do tầm quan trọng của nó, tương tác đặc trưng tiếp tục thu hút sự chú ý ngày càng tăng từ cả học thuật và công nghiệp [4, 5, 27, 33, 49].

Trong những năm gần đây, mạng nơ-ron sâu (DNN) đã thu hút sự chú ý đáng kể trong các lĩnh vực nghiên cứu khác nhau, nhờ hiệu suất mô hình đặc biệt cũng như khả năng học biểu diễn của chúng [22,49]. DNN đã được áp dụng rộng rãi cho hệ thống gợi ý để học biểu diễn đặc trưng đầu vào thưa thớt [30] và học tương tác đặc trưng [1,3,8,26,29,32,40,45,46,49]. Nó đã trở thành một công cụ mạnh mẽ để ánh xạ đầu vào lớn và thưa thớt vào không gian ngữ nghĩa chiều thấp. Một số công trình gần đây [8,24, 29,32,46] trong học tương tác đặc trưng dựa trên các hàm tương tác đặc trưng rõ ràng, chẳng hạn như Factorization Machine [26,34], tích trong [32], kernel Factorization Machine [33], và Cross Network [45,46]. DNN cũng được tận dụng để học các tương tác đặc trưng ngầm với nhiều lớp nơ-ron nhân tạo có hàm kích hoạt phi tuyến [3, 29].

Một hướng nghiên cứu khác dựa trên cơ chế attention, đặc biệt là kiến trúc dựa trên Transformer [44] cho việc học tương tác đặc trưng. Kiến trúc Transformer đã trở thành mô hình tiên tiến (SOTA) cho nhiều tác vụ khác nhau, bao gồm thị giác máy tính (CV) [9,21] và xử lý ngôn ngữ tự nhiên (NLP) [19]. AutoInt [40] và InterHAt [25] đề xuất tận dụng lớp multi-head self-attention cho việc học tương tác đặc trưng. Tuy nhiên, không giống như lĩnh vực NLP, ngữ nghĩa đặc trưng trong hệ thống gợi ý là động dưới các bối cảnh khác nhau.

Giả sử chúng ta đang gợi ý ứng dụng giao đồ ăn cho người dùng và chúng ta có các đặc trưng: app_id, hour_of_day, và user_country. Trong trường hợp này, app_id có thể có nghĩa này đối với hour_of_day và nghĩa khác đối với user_country. Để tương tác hiệu quả các đặc trưng khác nhau, chúng ta cần nhận thức ngữ nghĩa và căn chỉnh không gian ngữ nghĩa. Các mô hình Transformer rất tiếc không xem xét điều này. Trong lớp attention vanilla, các đặc trưng được chiếu qua cùng các ma trận chiếu (tức là WQ và Wk) được chia sẻ trên tất cả các đặc trưng. Đây là thiết kế tự nhiên cho các ứng dụng mà ngữ nghĩa đặc trưng (token văn bản) độc lập với bối cảnh; tuy nhiên đối với hệ thống gợi ý mà ngữ nghĩa đặc trưng thường phụ thuộc vào bối cảnh, thiết kế đồng nhất này sẽ dẫn đến khả năng biểu đạt mô hình hạn chế cho việc học tương tác đặc trưng.

Bên cạnh hạn chế không thể nắm bắt tương tác đặc trưng dị chủng, một hạn chế khác của mô hình Transformer là về độ trễ. Trong các ứng dụng quy mô web, thường xuyên có số lượng lớn người dùng và yêu cầu. Điều quan trọng là các mô hình có khả năng quản lý số lượng truy vấn cực kỳ cao mỗi giây, để mang lại trải nghiệm người dùng tối ưu. Tuy nhiên, chi phí suy luận của kiến trúc Transformer tăng bậc hai với độ dài đầu vào, điều này khiến việc suy luận thời gian thực sử dụng kiến trúc Transformer trở nên bất khả thi trong hệ thống gợi ý quy mô web.

Bất chấp những hạn chế, chúng tôi cho rằng việc nghiên cứu và cải thiện kiến trúc dựa trên Transformer cho tương tác đặc trưng là quan trọng. Trước hết, do thành công của kiến trúc Transformer trong nhiều lĩnh vực, một số lượng đáng kể các tiến bộ và sửa đổi đã được thực hiện đối với kiến trúc để cải thiện hiệu suất và khả năng của nó. Việc cho phép các mô hình tương tác đặc trưng dựa trên Transformer cho hệ thống gợi ý có thể đóng vai trò như một cây cầu, để cho phép các tiến bộ gần đây trong kiến trúc Transformer được áp dụng trong lĩnh vực gợi ý. Thứ hai, với việc áp dụng rộng rãi, thiết kế phần cứng (ví dụ: thiết kế chip [28]) có thể ưu tiên kiến trúc giống Transformer. Do đó, hệ thống gợi ý với kiến trúc dựa trên Transformer cũng có thể tận hưởng các tối ưu hóa do phần cứng mang lại. Thứ ba, vì mô hình Transformer cho việc học tương tác đặc trưng dựa trên cơ chế attention, chúng cung cấp khả năng giải thích mô hình tốt [40]. Xây dựng mô hình gợi ý với cơ chế attention trong các ứng dụng quy mô web có thể mở ra các cơ hội nghiên cứu mới.

Trong bài báo này, chúng tôi xem xét các hạn chế của kiến trúc mô hình Transformer vanilla và đề xuất một mô hình mới: Hiformer (Heterogeneous Feature Interaction Trans former) cho việc học tương tác đặc trưng trong hệ thống gợi ý. Hiformer nhận thức ngữ nghĩa đặc trưng và quan trọng hơn, hiệu quả cho triển khai trực tuyến. Để cung cấp nhận thức ngữ nghĩa đặc trưng trong việc học tương tác đặc trưng, chúng tôi thiết kế một lớp attention dị chủng mới. Để tuân thủ các yêu cầu độ trễ nghiêm ngặt của các ứng dụng quy mô web, chúng tôi tận dụng các kỹ thuật xấp xỉ hạng thấp và cắt tỉa mô hình để cải thiện hiệu suất.

Chúng tôi sử dụng mô hình xếp hạng App quy mô lớn tại Google Play làm nghiên cứu trường hợp. Chúng tôi thực hiện cả thí nghiệm offline và online với Hiformer để xác nhận tính hiệu quả trong mô hình hóa tương tác đặc trưng và hiệu suất trong phục vụ. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi cho thấy lần đầu tiên rằng kiến trúc dựa trên Transformer (tức là Hiformer) có thể vượt trội hơn các mô hình gợi ý SOTA cho việc học tương tác đặc trưng. Chúng tôi đã triển khai thành công Hiformer như mô hình sản xuất.

Tóm lại, các đóng góp của bài báo như sau:
• Chúng tôi đề xuất mô hình Hiformer với một lớp attention dị chủng mới để nắm bắt các tác động cộng tác phức tạp của các đặc trưng trong việc học tương tác đặc trưng. So với các phương pháp dựa trên Transformer vanilla hiện có, mô hình của chúng tôi có nhiều khả năng biểu đạt mô hình hơn.
• Chúng tôi tận dụng xấp xỉ hạng thấp và cắt tỉa mô hình để giảm độ trễ phục vụ của mô hình Hiformer, mà không ảnh hưởng đến chất lượng mô hình.
• Chúng tôi tiến hành so sánh offline mở rộng với tập dữ liệu quy mô web để chứng minh tầm quan trọng của việc nắm bắt tương tác đặc trưng dị chủng. Chúng tôi cho thấy rằng hiformer, như một kiến trúc dựa trên Transformer, có thể vượt trội hơn các mô hình gợi ý SOTA.
• Chúng tôi thực hiện thử nghiệm A/B trực tuyến để đo lường tác động của các mô hình khác nhau đối với các chỉ số tương tác chính, và mô hình Hiformer cho thấy lợi ích trực tuyến đáng kể so với mô hình cơ sở với mức tăng độ trễ hạn chế.

2 ĐỊNH NGHĨA VẤN ĐỀ
Mục tiêu của hệ thống gợi ý là cải thiện tương tác tích cực của người dùng, chẳng hạn như nhấp chuột, chuyển đổi, v.v. Tương tự như các nghiên cứu trước đây [26,27], chúng tôi hình thành nó như một tác vụ học máy có giám sát cho dự đoán tương tác.

Định nghĩa 2.1 (Hệ thống Gợi ý). Cho x∈R^{d_x} biểu thị các đặc trưng của một cặp (người dùng, mục). Có các đặc trưng phân loại x^C

--- TRANG 3 ---
Hiformer: Học Tương tác Đặc trưng Dị chủng với Transformers cho Hệ thống Gợi ý Conference'17, July 2017, Washington, DC, USA

Token Tác vụ Đặc trưng Phân loại... Đặc trưng Vô hướng Dày đặc...
Learned embedding
Embedding Look-up
Embedding Look-up
Normalize, Concat, & Project emb emb emb emb emb emb emb emb emb emb
...
Learned embedding emb emb
Tác vụ 1 Tác vụ 2 Tác vụ 1 Tác vụ 2
Lớp Đầu vào Lớp Tiền xử lý Lớp Đầu ra Lớp Tương tác Đặc trưng
......

Hình 1: Có bốn thành phần trong framework: Lớp Đầu vào, Lớp Tiền xử lý, Lớp Tương tác đặc trưng, và Lớp Đầu ra. Chúng tôi tận dụng mô hình Hiformer mới cho việc học tương tác đặc trưng dị chủng.

và đặc trưng dày đặc x^D. Đặc trưng phân loại được biểu diễn bằng mã hóa one-hot. Đặc trưng dày đặc có thể được xem như đặc trưng embedding đặc biệt với chiều embedding là 1. Số lượng đặc trưng phân loại và dày đặc lần lượt là |C| và |D|. Vấn đề của hệ thống gợi ý là dự đoán liệu người dùng có tương tác với mục dựa trên các đặc trưng đầu vào x.

Như đã đề cập trước đây, việc học tương tác đặc trưng cho phép mô hình nắm bắt các mối quan hệ phức tạp hơn giữa các đặc trưng, do đó cung cấp thông tin ảo cho các gợi ý chính xác hơn. Chúng tôi định nghĩa chính thức việc học tương tác đặc trưng dị chủng.

Định nghĩa 2.2 (Tương tác Đặc trưng z-bậc Dị chủng). Cho đầu vào với |F| đặc trưng và đầu vào là x={x_i}^{|F|}_{i=1}, một tương tác đặc trưng z-bậc là học một hàm ánh xạ phi cộng tính duy nhất ρ_Z(·) ánh xạ từ danh sách đặc trưng Z với z đặc trưng (tức là [x_{i1}, ···x_{iz}]) thành một biểu diễn mới hoặc một vô hướng để nắm bắt các mối quan hệ phức tạp giữa danh sách đặc trưng Z. Khi z = 2, ρ_Z(·) nắm bắt tương tác đặc trưng bậc hai.

Ví dụ, g([x1,x2])=w_1x_1+w_2x_2 không phải là tương tác đặc trưng bậc 2, vì w_1x_1+w_2x_2 là cộng tính. Trong khi đó, g([x1,x2])= x_1^Tx_2 là tương tác đặc trưng bậc 2; và ρ_{1,2}(·)=ρ_{2,1}(·)=x^T_1x_2, có nghĩa là tương tác đặc trưng dị chủng là đối xứng.

Trước khi đi sâu vào chi tiết mô hình, chúng tôi định nghĩa ký hiệu của chúng tôi. Các vô hướng được ký hiệu bằng chữ thường (a,b,...), véc-tơ bằng chữ thường in đậm (a,b,...), ma trận bằng chữ hoa in đậm (A,B,...).

3 MÔ HÌNH
Chúng tôi sẽ trình bày tổng quan framework trong Phần 3.1. Sau đó chúng tôi đề xuất lớp attention dị chủng trong Phần 3.3, dựa trên đó chúng tôi tiếp tục giới thiệu mô hình Hiformer trong Phần 3.4.

3.1 Tổng quan
3.1.1 Lớp Đầu vào. Lớp đầu vào bao gồm các đặc trưng phân loại và đặc trưng dày đặc. Ngoài ra, chúng tôi có embedding tác vụ. Embedding tác vụ có thể được coi như token CLS [6]. Framework được đề xuất cũng có thể dễ dàng mở rộng cho đa tác vụ [2] với nhiều embedding tác vụ, và mỗi embedding tác vụ đại diện thông tin của mục tiêu huấn luyện tương ứng. Các embedding tác vụ trong lớp đầu vào là tham số mô hình và được học qua việc huấn luyện mô hình end-to-end. Chúng tôi ký hiệu số lượng embedding tác vụ là t.

3.1.2 Lớp Tiền xử lý. Chúng tôi có lớp tiền xử lý để chuyển đổi lớp đầu vào thành danh sách embedding đặc trưng, ngoài các embedding tác vụ, làm đầu vào cho các lớp tương tác đặc trưng. Chúng tôi có một lớp tiền xử lý cho mỗi loại đặc trưng.

Đặc trưng Phân loại. Các đặc trưng phân loại đầu vào thường thưa thớt và có chiều cao. Việc sử dụng trực tiếp mã hóa one-hot cho huấn luyện mô hình có thể dễ dẫn đến overfitting mô hình. Do đó, một chiến lược phổ biến là chiếu các đặc trưng phân loại vào không gian dày đặc chiều thấp. Cụ thể, chúng tôi học một ma trận chiếu W^C_i∈R^{V_i×d} liên quan đến mỗi đặc trưng phân loại, và

e_i=x^C_iW^C_i,

trong đó e_i∈R^d, x^C_i∈{0,1}^{V_i}, V_i là chiều của mã hóa one-hot cho đặc trưng x^C_i, và d là chiều mô hình. Sau đó chúng tôi sẽ sử dụng véc-tơ embedding dày đặc e_i làm biểu diễn của đặc trưng phân loại x^C_i. {W^C_i}^{|C|} là các tham số mô hình và được gọi là bảng tra cứu embedding.

Đặc trưng Vô hướng Dày đặc. Các đặc trưng vô hướng dày đặc là đặc trưng số. Vì các đặc trưng vô hướng dày đặc có thể từ các phân phối rất khác nhau, chúng tôi sẽ cần chuyển đổi các đặc trưng thành phân phối tương tự (tức là phân phối đều) để đảm bảo ổn định số [51]. Ngoài ra, trong các ứng dụng quy mô web, có một số lượng lớn đặc trưng vô hướng dày đặc, với số lượng ít hơn nhiều so với các đặc trưng phân loại. Do đó, chúng tôi tổng hợp tất cả các đặc trưng vô hướng dày đặc và chiếu chúng thành n_D embedding [29], trong đó n_D là siêu tham số được chọn theo kết quả thực nghiệm, n_D ≪ |D| để giảm tổng số đặc trưng. Hàm chiếu f_D(·) có thể là một perceptron đa lớp (MLP) với kích hoạt phi tuyến:

e_i=split_i(f_D(concat(normalize({x^D_i}))), split_size = d),

trong đó e_i∈R^d, concat(·) là hàm nối, normalize(·) là hàm chuyển đổi đặc trưng, và split_i(·,split_size) là hàm chia tensor đầu vào thành các tensor có chiều bằng nhau là split_size và lấy tensor thứ i làm đầu ra. Trong kiến trúc dựa trên Transformer, chi phí suy luận tăng bậc hai với độ dài danh sách embedding đầu vào. Bằng cách tổng hợp |D| đặc trưng vô hướng dày đặc thành n_D embedding, độ dài của embedding đặc trưng được giảm, và do đó, chi phí suy luận cũng được giảm, mặc dù có sự đánh đổi việc thêm f_D(·).

Để tóm tắt về các lớp tiền xử lý, chúng tôi chuyển đổi |C| đặc trưng phân loại, |D| đặc trưng vô hướng dày đặc, và t embedding tác vụ thành danh sách embedding với chiều mô hình d. Số lượng embedding đầu ra từ lớp tiền xử lý là L=|C|+n_D+T. Để dễ thảo luận, các embedding tác vụ cũng được coi như đặc trưng đặc biệt trong danh sách embedding. Do đó, tương tác đặc trưng không chỉ đề cập đến tương tác giữa các đặc trưng, mà còn giữa đặc trưng và tác vụ.

3.1.3 Lớp Tương tác Đặc trưng. Lớp tương tác đặc trưng được sử dụng để học các tác động cộng tác phức tạp giữa các đặc trưng,

--- TRANG 4 ---
Conference'17, July 2017, Washington, DC, USA Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, và Ed H. Chi

điều này rất quan trọng để nắm bắt sở thích của người dùng dưới các bối cảnh khác nhau. Đầu vào cho lớp tương tác đặc trưng là danh sách embedding đầu ra của lớp tiền xử lý.

3.1.4 Lớp Đầu ra và Mục tiêu Huấn luyện. Chúng tôi chỉ sử dụng các embedding tác vụ được mã hóa từ lớp tương tác đặc trưng cho dự đoán tác vụ. Chúng tôi huấn luyện một tháp MLP để chiếu embedding tác vụ được mã hóa thành dự đoán cuối cùng. Các tác vụ huấn luyện có thể được cấu hình linh hoạt dựa trên ứng dụng gợi ý. Ví dụ, tác vụ huấn luyện có thể là tác vụ phân loại nếu các nhãn là nhấp chuột [39,40], cài đặt [15,47], hoặc mua hàng [23,31], và cũng có thể là tác vụ hồi quy, chẳng hạn như trong trường hợp thời gian xem [50]. Không mất tính tổng quát, chúng tôi xem xét tác vụ phân loại, với giá trị 0 chỉ tương tác tiêu cực và 1 chỉ tương tác tích cực. Do đó, mục tiêu huấn luyện có thể được hình thành như một bài toán phân loại nhị phân với Binary Cross Entropy (tức là Log Loss):

ℓ=1/N ∑^N_{i} -y_i log(p_i)-(1-y_i)log(1-p_i),

trong đó y_i và p_i lần lượt là nhãn thực tế và xác suất dự đoán tương tác cho ví dụ i, và N là tổng số ví dụ huấn luyện.

3.2 Tương tác Đặc trưng Dị chủng
Kiến trúc Transformer đã trở thành tiêu chuẩn de-facto cho nhiều tác vụ như NLP, CV, v.v., và lớp multi-head self-attention đã đạt được hiệu suất đáng chú ý trong việc mô hình hóa các mối quan hệ phức tạp trong chuỗi [9,19,21,44]. Tuy nhiên, kiến trúc Transformer chưa được áp dụng rộng rãi cho hệ thống gợi ý quy mô web. Người ta đã quan sát thấy rằng các kiến trúc dựa trên Transformer, chẳng hạn như AutoInt [40], có thể không thể hiện hiệu suất tối ưu khi áp dụng cho hệ thống gợi ý quy mô web.

Nhớ lại rằng việc tính toán điểm attention của Transformer vanilla, việc tham số hóa việc học tương tác đặc trưng được chia sẻ trên tất cả các nhóm đặc trưng trong lớp multi-head self-attention. Thiết kế chia sẻ tham số này xuất hiện tự nhiên trong các tác vụ NLP, vì đầu vào cho Transformer trong các tác vụ NLP thường là một câu được biểu diễn bởi danh sách embedding văn bản. Embedding văn bản chủ yếu mã hóa thông tin độc lập với bối cảnh. Chúng tôi gọi thiết lập này là tương tác đặc trưng đồng nhất. Minh họa chi tiết của kiến trúc Transformer được hiển thị trong Hình 4(a).

Ngược lại hoàn toàn, có thông tin bối cảnh đa dạng trong hệ thống gợi ý. Các biểu diễn embedding đã học động hơn và bao gồm thông tin từ nhiều góc độ. Trong ví dụ gợi ý ứng dụng giao đồ ăn, chúng ta có ba đặc trưng: app_id (ví dụ: Zomato), hour_of_day (ví dụ: 12pm), và user_country (ví dụ: Ấn Độ). Biểu diễn embedding đã học cho ứng dụng Zomato mã hóa nhiều thông tin ngầm, bao gồm bản chất của ứng dụng, các quốc gia mà nó phổ biến, thời gian mà nó được sử dụng thường xuyên, và ngôn ngữ của người dùng, v.v. Không phải tất cả thông tin được mã hóa sẽ hữu ích trong việc học tương tác đặc trưng nhất định. Khi học tương tác đặc trưng giữa app_id và user_country, có thể thông tin về các quốc gia mà ứng dụng phổ biến quan trọng hơn nhiều so với các thông tin khác. Do đó, cốt lõi của việc học tương tác đặc trưng dị chủng là cung cấp nhận thức bối cảnh cho việc lựa chọn và chuyển đổi thông tin, để đạt được việc học tương tác đặc trưng chính xác hơn.

3.3 Lớp Attention Dị chủng
Chúng tôi đầu tiên giới thiệu một sửa đổi đơn giản nhưng hiệu quả cho lớp multi-head self-attention trong mô hình Transformer: lớp multi-head self-attention dị chủng, để nắm bắt tương tác đặc trưng bậc hai dị chủng. Để đơn giản, chúng tôi gọi lớp multi-head self-attention dị chủng là lớp attention dị chủng, và mô hình tương ứng là mô hình attention dị chủng. Cụ thể, chúng tôi thiết kế lại việc tính toán điểm multi-head self-attention cho đặc trưng i và j:

Att(i,j)^h=exp(φ^h_{i,j}(e_i,e_j))/∑^L_{m=1}exp(φ^h_{i,m}(e_i,e_m)), (1)

trong đó h đề cập đến head thứ h trong tổng số H head, L là độ dài danh sách embedding, φ^h_{i,j}(e_i,e_j) đo mức tương quan ngữ nghĩa giữa embedding e_i và e_j liên quan đến head h. Sau đó chúng tôi sẽ gọi lớp self-attention vanilla là lớp attention đồng nhất.

Đối với mọi cặp đặc trưng (i,j), chúng ta có một φ^h_{i,j}(·,·) duy nhất để đo tương quan ngữ nghĩa. Nói cách khác, φ^h_{i,j}(·,·) phục vụ mục đích lựa chọn thông tin phù hợp nhất từ e_i và e_j cho bối cảnh của cặp đặc trưng (i,j). Chúng ta có thể có các hàm tùy ý cho φ^h_{i,j}(·,·):[R^d,R^d]→R, chẳng hạn như một số hàm phi cộng tính rõ ràng, một mạng nơ-ron với chuyển đổi phi tuyến. Do tính đơn giản, chúng tôi chọn hàm tích chấm. Theo đó,

φ^h_{i,j}(e_i,e_j)=e_iQ^h_i(e_jK^h_j)^T/√d_k, (2)

trong đó Q_i∈R^{d×d_k}, K_j∈R^{d×d_k} lần lượt là các chiếu query và key cho đặc trưng j và j, và √d_k để chuẩn hóa độ lớn của tích chấm, thường được đặt là d_k=d/H.

Với trọng số attention được tính trong Eq (1), sau đó chúng ta có thể tính đầu ra của lớp attention dị chủng như sau:

o_i=concat{∑_j Att(i,j)^h e_jV^h_j}^H_{h=1} O_j, (3)

trong đó V_j∈R^{d×d_v}, O_j∈R^{Hd_v×d} là các chiếu value và output, và d_v thường được đặt là d_v=d/H.

Tương tự như lớp attention dị chủng, chúng tôi cũng thiết kế một Feed Forward Net (FFN) dị chủng cho mỗi đặc trưng. FFN được triển khai thông qua hai hàm chuyển đổi tuyến tính với kích hoạt Gaussian Error Linear Unit (GELU) [12, 37]:

FFN_i^{GELU}(o_i)=GELU(o_iW^1_i+b^1_i)W^2_i+b^2_i, (4)

trong đó W^1_i∈R^{d×d_f}, W^2_i∈R^{d_f×d}, b^1_i∈R^{d_f}, b^2_i∈R^d, và d_f là kích thước lớp của lớp trung gian. Theo các nghiên cứu hiện có [44], d_f được đặt là d_f=4d, d là chiều của mô hình.

Như chúng ta có thể thấy thay đổi chính của lớp attention dị chủng, so với lớp self-attention là chúng ta học các ma trận chiếu query, key, và value (QKV) riêng biệt (tức là Q,K,V) cho mỗi đặc trưng. Do đó, chúng ta tăng số lượng tham số trong lớp attention dị chủng, so với mô hình Transformer vanilla. Số lượng tham số tăng tuyến tính với

--- TRANG 5 ---
Hiformer: Học Tương tác Đặc trưng Dị chủng với Transformers cho Hệ thống Gợi ý Conference'17, July 2017, Washington, DC, USA

(a) Lớp Attention Transformer Vanilla.
(b) Lớp Attention Dị chủng.

Hình 2: Mẫu attention với các lớp tương tác đặc trưng.

độ dài của danh sách embedding đầu vào. Tuy nhiên, đáng chú ý rằng tổng số FLOP là giống nhau cho lớp attention dị chủng và lớp đồng nhất (lớp multi-head self-attention tiêu chuẩn trong Transformer). Điều này là do chúng ta có chính xác cùng các phép toán so với lớp Transformer đồng nhất, nhưng với các tham số hóa khác nhau.

Chúng tôi trực quan hóa mẫu attention đã học Att(·,·) trong Hình 2. Mẫu attention đã học với mô hình Transformer vanilla, trong Hình 2(a), tương đối thưa thớt và thể hiện mẫu đường chéo mạnh, cho thấy rằng lớp attention có thể không nắm bắt hiệu quả các tác động cộng tác giữa các đặc trưng khác nhau. Trong khi đó, đối với lớp attention dị chủng, có một mẫu attention dày đặc. Trực quan, quan sát này có thể được giải thích bởi thực tế rằng lớp attention dị chủng có thể tốt hơn trong việc nắm bắt các tác động cộng tác giữa các đặc trưng nhờ ma trận chuyển đổi để căn chỉnh ngữ nghĩa đặc trưng.

3.4 Hiformer
Bây giờ chúng tôi sẵn sàng giới thiệu mô hình Hiformer, nơi chúng tôi tiếp tục tăng khả năng biểu đạt của mô hình, bằng cách giới thiệu việc học đặc trưng tổng hợp trong các chiếu QKV. Lấy chiếu key làm ví dụ. Thay vì học chiếu đặc trưng key cho mỗi đặc trưng, chúng tôi định nghĩa lại chiếu key như sau:

[k̂^h_1,...k̂^h_L]=concat([e^h_1,...e^h_L])K̂^h, (5)

trong đó K̂^h∈R^{Ld×Ld_k}. So với các ma trận chiếu key trong Phần 3.3, [K^h_i,...,K^h_L]∈R^{d×Ld_k}, chúng tôi tăng khả năng biểu đạt của mô hình bằng cách thêm nhiều tham số hơn vào chiếu key. Chúng tôi gọi chiếu mới K̂^h là chiếu Tổng hợp. Về mặt toán học, thay vì học tương tác đặc trưng một cách rõ ràng với các cặp đặc trưng, chúng tôi đầu tiên chuyển đổi danh sách embedding đặc trưng thành các đặc trưng tổng hợp làm key và value; sau đó chúng tôi học tương tác đặc trưng dị chủng giữa các đặc trưng tổng hợp và embedding tác vụ. Tương tự, chúng tôi áp dụng chiếu chéo cho các chiếu query và value.

Tương tự như trong Eq (1), mô hình Hiformer tính điểm attention giữa query q_i và key k_i như sau:

Att_{Composite}(i,j)^h=exp(q̂^h_i(k̂^h_i)^T/√d_k)/∑^L_l exp(q̂^h_i(k̂^h_l)^T/√d_k),

Với điểm attention, sau đó chúng ta có thể tính đầu ra của lớp attention trong mô hình Hiformer như:

ô_i=concat{∑_m Att^h_{Composite}(i,j)v̂^h_i}^H_{h=1} O_j. (6)

3.5 Hiệu suất Tốt hơn cho Hiformer
So với mô hình Transformer và lớp attention dị chủng, mô hình Hiformer đi kèm với nhiều khả năng biểu đạt mô hình hơn, nhưng cũng có chi phí tính toán trong quá trình suy luận. Do lớp attention Tổng hợp được giới thiệu, các kiến trúc Transformer hiệu quả hiện có [42] không thể được áp dụng trực tiếp.

Độ dài của danh sách embedding đặc trưng là L=|C|+n_D+t. Với chiều mô hình là d, chúng ta có phân tích chi phí suy luận^1 cho Hiformer: chiếu QKV: 3L^2d^2, tính toán điểm attention: 2L^2d, chiếu output: Ld^2, mạng FFN: 8Ld^2. Kết quả là, tổng tính toán là O(L^2d^2+L^2d+Ld^2). Để triển khai Hiformer cho suy luận thời gian thực, chúng tôi giảm chi phí suy luận của Hiformer thông qua xấp xỉ hạng thấp và cắt tỉa.

3.5.1 Xấp xỉ hạng thấp. Như chúng ta có thể thấy, số hạng nổi bật cho chi phí suy luận Hiformer là chiếu QKV với độ phức tạp tính toán O(L^2d^2).

Với chiếu key Tổng hợp được định nghĩa trong (5), chúng ta có thể xấp xỉ K̂^h với xấp xỉ hạng thấp, tức là,

K̂^h=L^h_k(R^h_k)^T, (7)

trong đó L^h_v∈R^{Ld×r_v}, R^h_v∈R^{Ld_k×r_v}, và r_v là hạng của xấp xỉ hạng thấp cho K̂^h. Xấp xỉ hạng thấp giảm chi phí tính toán cho chiếu value xuống O(Lr_v(d+d_k)). Tương tự, nó có thể được áp dụng cho chiếu query và value tổng hợp với hạng r_k và r_v. Nếu r_k<Ld_k/2 và r_v<Ld_v/2, chi phí sẽ được giảm. Cấu trúc hạng thấp cũng được quan sát trong phân tích dữ liệu. Lấy V̂^h làm ví dụ, chúng tôi vẽ biểu đồ các giá trị singular trong Hình 3, nơi chúng ta có thể thấy rằng ma trận V̂^h thực sự có hạng thấp.

Hình 3: Giá trị singular cho V̂^h trong chiếu Tổng hợp.

Sau khi áp dụng xấp xỉ hạng thấp, độ phức tạp mô hình Hiformer được tính như sau: chiếu query và key: O(Lr_k(d+d_k)), chiếu value O(Lr_v(d+d_v)), lớp attention O(L^2d), chiếu output O(Ld^2), và lớp FFN O(Ld^2). Với d_k=d/H, d_v=d/H, và r_k<d_k, r_v<d_v, chúng ta có độ phức tạp tính toán là O(L^2d+Ld^2), tăng bậc hai với độ dài của danh sách embedding đầu vào L.

^1 Tính toán này dựa trên thiết lập mà d_k=d/H, d_v=d/H, và d_f=4d.

--- TRANG 6 ---
Conference'17, July 2017, Washington, DC, USA Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, và Ed H. Chi

Token Tác vụ Đặc trưng 1 Đặc trưng 2 Scaled Dot-Prod (Multi-Head) Attention Token Tác vụ Đặc trưng 1 Đặc trưng 2 FFN Add & Norm Token Tác vụ Đặc trưng 1 Đặc trưng 2 Add & Norm

Linear (Query) Linear (Key) Linear (Value)

(a) Transformer Vanilla.

Token Tác vụ Đặc trưng 1 Đặc trưng 2 Linear (Query) Linear (Key) Linear (Value) Scaled Dot-Prod (Multi-Head) Attention Token Tác vụ Đặc trưng 1 Đặc trưng 2 Add & Norm Token Tác vụ Đặc trưng 1 Đặc trưng 2 Add & Norm

Linear (Query) Linear (Key) Linear (Value) Linear (Query) Linear (Key) Linear (Value) FFN FFN FFN

(b) Transformer với Attention Dị chủng.

Linear (Query)

Token Tác vụ Đặc trưng 1 Đặc trưng 2 Scaled Dot-Prod (Multi-Head) Attention Token Tác vụ Đặc trưng 1 Đặc trưng 2 Add & Norm Token Tác vụ Đặc trưng 1 Đặc trưng 2 Add & Norm

FFN FFN FFN

Reshape Reshape Reshape

Concat Concat Concat Linear (Key) Linear (Value)

(c) Hiformer.

Hình 4: Minh họa về việc nắm bắt tương tác đặc trưng với các kiến trúc dựa trên Transformer khác nhau. với một token tác vụ và hai token đặc trưng. Trong 4(a), áp dụng Transformer vanilla cho việc học tương tác đặc trưng. Trong 4(b), chúng tôi mô hình hóa một cách rõ ràng tương tác đặc trưng dị chủng thông qua lớp tương tác dị chủng. Trong 4(c), chúng tôi đề xuất mô hình Hiformer, cải thiện thêm khả năng biểu đạt mô hình so với lớp attention dị chủng trong 4(b). d là kích thước lớp ẩn, d_q và d_v là chiều Query và Value, L là tổng độ dài đầu vào, r_* tương ứng với hạng của các chiếu QKV tương ứng (xem Eq (7)).

3.5.2 Cắt tỉa Mô hình. Nhớ lại rằng trong Lớp Đầu ra (xem Hình 1), chúng tôi chỉ sử dụng các embedding tác vụ được mã hóa cho huấn luyện và dự đoán mục tiêu cuối cùng. Do đó, chúng tôi có thể tiết kiệm một số tính toán trong lớp cuối cùng của mô hình Hiformer bằng cách cắt tỉa việc tính toán mã hóa embedding đặc trưng. Cụ thể, việc cắt tỉa chuyển thành chỉ sử dụng embedding tác vụ làm query, và sử dụng cả embedding tác vụ và embedding đặc trưng làm key và value trong lớp attention Hiformer. Do đó, chúng ta có độ phức tạp tính toán của lớp Hiformer cuối cùng: chiếu query: O(tr_k(d+d_k)), chiếu key: O(Lr_k(d+d_k)), chiếu value: O(Lr_v(d+d_v)), tính toán lớp attention: O(Ltd), chiếu output: O(td^2), lớp FFN: O(td^2). Giả sử số lượng tác vụ t(≪L) được xem như một hằng số. Chúng ta có độ phức tạp tính toán của lớp cuối cùng của Hiformer là O(L(r_k+r_v)d+Ltd+td^2).

Cắt tỉa cho phép chúng tôi giảm độ phức tạp tính toán của lớp Hiformer cuối cùng từ tăng bậc hai xuống tăng tuyến tính với L. Kỹ thuật cắt tỉa này có thể được áp dụng cho tất cả các kiến trúc dựa trên Transformer. Một số công trình gần đây, chẳng hạn như Perceiver [14], cũng đã giới thiệu ý tưởng tương tự để giảm chi phí phục vụ của mô hình Transformer từ tăng bậc hai xuống tuyến tính.

4 THÍ NGHIỆM
Chúng tôi thực hiện một cuộc kiểm tra thực nghiệm kỹ lưỡng về các kiến trúc mô hình được đề xuất với mô hình xếp hạng App quy mô web tại Google Play. Chúng tôi tiến hành cả thí nghiệm offline và online, với đó chúng tôi hướng đến việc trả lời các câu hỏi nghiên cứu sau:

Q1. Lớp attention dị chủng có thể nắm bắt các tương tác đặc trưng dị chủng để đưa ra gợi ý tốt hơn, so với mô hình Transformer vanilla không?

Q2. Với nhiều khả năng biểu đạt mô hình hơn trong mô hình Hiformer, liệu nó có thể cải thiện thêm chất lượng mô hình không?

Q3. Hiệu suất phục vụ của mô hình Hiformer so với các mô hình SOTA như thế nào?

Q4. Các thiết lập siêu tham số ảnh hưởng đến hiệu suất mô hình và độ trễ phục vụ như thế nào và làm thế nào để thực hiện đánh đổi giữa hiệu suất và hiệu quả?

4.1 Thiết lập Đánh giá Offline
4.1.1 Tập dữ liệu. Để đánh giá offline, chúng tôi sử dụng dữ liệu được ghi lại từ mô hình xếp hạng. Tương tác của người dùng là nhãn huấn luyện, 0 hoặc 1 cho biết liệu tương tác có xảy ra giữa người dùng và ứng dụng hay không và hàm mất mát huấn luyện là LogLoss (xem Eq (3.1.4)). Chúng tôi huấn luyện mô hình với dữ liệu cửa sổ trượt 35 ngày, và đánh giá trên dữ liệu ngày thứ 36. Mô hình được huấn luyện lại từ đầu hàng ngày với dữ liệu mới nhất. Có 31 đặc trưng phân loại và 30 đặc trưng vô hướng dày đặc để mô tả các ứng dụng, chẳng hạn như ID ứng dụng, tiêu đề ứng dụng, ngôn ngữ người dùng, v.v. Kích thước từ vựng của các đặc trưng phân loại thay đổi từ hàng trăm đến hàng triệu.

4.1.2 Chỉ số Đánh giá. Chúng tôi sẽ đánh giá hiệu suất mô hình bằng AUC (Diện tích Dưới Đường cong Đặc tính Hoạt động của Máy thu). Chỉ số AUC có thể được hiểu như xác suất mô hình gán cho một ví dụ tích cực được chọn ngẫu nhiên một thứ hạng cao hơn so với một ví dụ tiêu cực được chọn ngẫu nhiên. Trong tập dữ liệu ứng dụng quy mô web, do kích thước lớn của tập dữ liệu đánh giá, chúng tôi nhận thấy rằng sự cải thiện AUC ở mức 0.001 đạt được ý nghĩa thống kê. Các quan sát tương tự đã được thực hiện trong các ứng dụng quy mô web khác [3,8,15]. Ngoài ra, chúng tôi báo cáo LogLoss được chuẩn hóa với Transformer một lớp làm cơ sở.

--- TRANG 7 ---
Hiformer: Học Tương tác Đặc trưng Dị chủng với Transformers cho Hệ thống Gợi ý Conference'17, July 2017, Washington, DC, USA

Chúng tôi huấn luyện mô hình trên TPU [16,17], và chúng tôi báo cáo Truy vấn mỗi giây (QPS) huấn luyện để đo tốc độ huấn luyện. Chúng tôi ước tính độ trễ thông qua mô phỏng offline, nơi chúng tôi thực hiện suy luận của các mô hình trên 20 lô ví dụ với kích thước lô là 1024. Sau đó chúng tôi chuẩn hóa độ trễ của tất cả mô hình so với mô hình cơ sở, đó là mô hình Transformer một lớp với cắt tỉa. Do số lượng lớn yêu cầu phục vụ trực tuyến, độ trễ phục vụ quan trọng hơn nhiều so với QPS huấn luyện.

4.1.3 Kiến trúc Mô hình. Lưu ý rằng công việc của chúng tôi là về việc học tương tác đặc trưng với Transformers, khác biệt đáng kể so với mô hình hóa chuỗi lịch sử người dùng với Transformers [20,41]. Do đó, chúng tôi tập trung vào việc so sánh các mô hình được đề xuất với các mô hình tương tác đặc trưng SOTA sau đây:

• AutoInt [40] được đề xuất để học tương tác đặc trưng thông qua lớp multi-head self-attention, và sau đó học dự đoán cuối cùng thông qua các lớp Multi-layer Perceptrons (MLP).

• DLRM [29] là để học tương tác đặc trưng thông qua factorization machine, và sau đó học tương tác đặc trưng ngầm thông qua lớp MLP, cho dự đoán cuối cùng.

• DCN [45] (tức là DCN-v2) là một trong các mô hình SOTA về việc học tương tác đặc trưng trong ứng dụng quy mô web, có cross network tạo ra các chéo đặc trưng có độ giới hạn một cách rõ ràng nơi bậc tương tác đặc trưng tăng theo độ sâu lớp.

• Transformer [44] với lớp multi-head self-attention cho việc học tương tác đặc trưng. Sự khác biệt chính giữa Transformer và AutoInt là: 1. Transformer sử dụng embedding tác vụ được mã hóa cho dự đoán; trong khi AutoInt sử dụng tất cả embedding đặc trưng và tác vụ được mã hóa; 2. Có lớp FFN trong kiến trúc Transformer, nhưng không có trong lớp AutoInt.

• Transformer + PE là để thêm Position Encoding vào kiến trúc Transformer, nơi chúng tôi học một embedding bổ sung cho mỗi đặc trưng làm encoding đặc trưng.

• HeteroAtt là lớp Attention Dị chủng được đề xuất của chúng tôi để nắm bắt tương tác đặc trưng dị chủng.

• Hiformer là mô hình được đề xuất của chúng tôi với xấp xỉ hạng thấp và cắt tỉa mô hình.

4.1.4 Chi tiết Triển khai. Việc huấn luyện và phục vụ mô hình được xây dựng trên Tensorflow 2 [38] và Model Garden [48]. Để đảm bảo so sánh công bằng, chúng tôi giữ tất cả các thành phần mô hình, ngoại trừ các lớp tương tác đặc trưng, là giống nhau. Trong lớp embedding, chúng tôi đặt chiều embedding là 128 cho tất cả mô hình. Lưu ý rằng đối với kiến trúc Transformer, HeteroAtt, và Hiformer, chúng tôi chỉ sử dụng token CLS được mã hóa (tức là token tác vụ đã học) cho dự đoán tác vụ cuối cùng.

Trong các thí nghiệm của chúng tôi trên Transformers, chúng tôi đặt chiều mô hình d=128, số head trong lớp attention H=4, và kích thước lớp của FFN là d_f=512. Chúng tôi đặt d_k=16 và d_v=64, thay vì đặt d_k=d/H, d_v=d/H như trong mô hình Transform và các mô hình dựa trên Transformer khác. Đối với các phương pháp khác, chúng tôi thực hiện điều chỉnh siêu tham số để có được hiệu suất tốt nhất. Đối với thí nghiệm trực tuyến, chúng tôi điều chỉnh siêu tham số dựa trên dữ liệu một ngày (ví dụ ngày 1 tháng 2), và cố định thiết lập cho các ngày tiếp theo (tức là các ngày sau ngày 2 tháng 2).

4.2 Tương tác Đặc trưng Đồng nhất vs Dị chủng (Q1)
Trước hết, chúng tôi quan sát thấy rằng mô hình Transformer+PE hoạt động tương tự như mô hình Transformer vanilla. Điều này phù hợp với giả thuyết của chúng tôi. Mặc dù có nhiều tham số hơn, encoding đặc trưng chỉ học một số hạng bias embedding cho mỗi đặc trưng, không đủ để học chuyển đổi đặc trưng cho việc học tương tác đặc trưng dị chủng. Thứ hai, chúng tôi quan sát thấy rằng mô hình Transformer với lớp attention dị chủng (HeteroAtt) hoạt động tốt hơn đáng kể so với mô hình Transformer vanilla. Kết quả này xác nhận lập luận của chúng tôi rằng HeteroAtt có thể nắm bắt hiệu quả các tương tác đặc trưng phức tạp thông qua các ma trận chuyển đổi M_{i,j}. Hơn nữa, chúng tôi quan sát thấy rằng số tham số # của mô hình HeteroAtt hai lớp lớn hơn nhiều so với mô hình Transformer hai lớp, vì mô hình HeteroAtt có nhiều khả năng biểu đạt mô hình hơn. Do đó, chúng tôi có thể trả lời khẳng định cho Q1, rằng tương tác đặc trưng dị chủng với nhận thức bối cảnh đặc trưng tốt hơn là quan trọng để cung cấp gợi ý cá nhân hóa.

4.3 So sánh Hiệu suất Mô hình (Q2)
Việc so sánh offline của các mô hình cơ sở và mô hình được đề xuất của chúng tôi được tóm tắt trong Bảng 1. Chúng tôi báo cáo hiệu suất tốt nhất của mỗi mô hình, với số lớp tương ứng.

Chúng tôi xem xét kỹ hơn mô hình AutoInt, DLRM, và Transformer vanilla. Trong việc học tương tác đặc trưng của các mô hình này, không có nhận thức đặc trưng và căn chỉnh ngữ nghĩa. Do đó, hiệu suất của chúng tương đối yếu hơn so với các mô hình khác. Trong mô hình DCN, Cross Net ngầm tạo ra tất cả các chéo cặp và sau đó chiếu nó vào không gian chiều thấp hơn. Việc tham số hóa của tất cả các chéo cặp là khác nhau, tương tự như việc cung cấp nhận thức đặc trưng. Do đó, hiệu suất mô hình DCN có thể so sánh với các mô hình HeteroAtt của chúng tôi.

So với lớp attention dị chủng (tức là HeteroAtt), mô hình Hiformer cung cấp nhiều khả năng biểu đạt mô hình hơn trong các chiếu QKV. Nó đạt được hiệu suất mô hình tốt nhất chỉ với một lớp, vượt trội hơn mô hình HeteroAtt và các mô hình SOTA khác.

4.4 Hiệu suất Phục vụ (Q3)
Chúng tôi so sánh hiệu suất phục vụ của tất cả các mô hình. Trước hết, chúng tôi so sánh mô hình Transformer hai lớp với mô hình một lớp. Như chúng ta có thể thấy, độ trễ của mô hình hai lớp gấp hơn 2 lần mô hình một lớp. Điều này là do chúng tôi chỉ áp dụng cắt tỉa cho lớp thứ hai trong mô hình hai lớp, và chúng tôi đã áp dụng cắt tỉa cho mô hình một lớp. Tương tự, chúng ta có thể rút ra kết luận tương tự cho lớp HeteroAtt vì hai mô hình có cùng toán tử, do đó độ trễ. Tuy nhiên, kỹ thuật cắt tỉa như vậy không thể được áp dụng cho mô hình AutoInt. Do đó, mô hình AutoInt một lớp đắt hơn nhiều so với mô hình Transformer vanilla và mô hình HeteroAtt.

Thứ hai, chúng tôi so sánh mô hình Hiformer với mô hình HeteroAtt một lớp. Cả mô hình Hiformer một lớp và mô hình HeteroAtt một lớp đều được hưởng lợi từ tối ưu hóa suy luận từ cắt tỉa. Tuy nhiên, do chiếu QKV biểu đạt hơn, chúng ta thấy rằng mô hình Hiformer đắt hơn 50.05%.

--- TRANG 8 ---
Conference'17, July 2017, Washington, DC, USA Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, và Ed H. Chi

[THIS IS TABLE: Comparison table showing Model, Layer #, Parameter #, AUC, LogLoss, Train QPS, and Serving Latency for different models including AutoInt, DLRM, DCN, Transformer, Transformer+PE, HeteroAtt, and Hiformer]

Bảng 1: So sánh hiệu suất mô hình offline về số lượng tham số, AUC, và hiệu suất được chuẩn hóa. Số lượng tham số không bao gồm lớp embedding. ↑(↓) có nghĩa là càng cao (thấp) càng tốt.

Tuy nhiên, chúng tôi muốn chỉ ra rằng không có xấp xỉ hạng thấp, mô hình Hiformer có thể sẽ đắt hơn nhiều. Trong thí nghiệm của chúng tôi, chúng tôi đặt hạng của query và key là r_k=128, và value r_v=1024. Chúng tôi so sánh Hiformer có và không có xấp xỉ hạng thấp trong Bảng 2. Xấp xỉ hạng thấp cho 62.7% tiết kiệm độ trễ suy luận. Ngoài ra, không có mất mát chất lượng mô hình đáng kể, điều này được mong đợi vì cấu trúc hạng thấp được quan sát của các ma trận chiếu query, key, và value của Hiformer (ví dụ: V, được hiển thị trong Hình 3).

[THIS IS TABLE: Small comparison table showing Hiformer Model with and without low-rank approximation, showing Parameter #, AUC, and Latency]

Bảng 2: Xấp xỉ hạng thấp cho Hiformer.

4.5 Độ nhạy Tham số (Q4)
Vì một trong những thách thức là giảm độ trễ phục vụ, chúng tôi cũng quan tâm đến độ nhạy tham số của mô hình như trong RQ4. Chính thức hơn, chúng tôi muốn trả lời câu hỏi sau:

Như chúng tôi đã thảo luận trước đây trong Phần 4, thay vì sử dụng trực tiếp d_k=d/H và d_v=d/H, chúng tôi thấy rằng việc lựa chọn d_k và d_v đưa ra những đánh đổi nhất định giữa hiệu suất mô hình và độ trễ phục vụ cho mô hình HeteroAtt và Hiformer.

Đầu tiên, đối với Hd_k trong HeteroAtt trong Hình 5(a), sau khi giảm Hd_k từ d=128 xuống 64, không có mất mát chất lượng mô hình. Tuy nhiên, chúng ta có thể thu được cải thiện độ trễ miễn phí (3%). Nếu chúng ta tiếp tục giảm Hd_k, có mất mát chất lượng mô hình đáng kể. Do đó, chúng tôi chọn Hd_k=64, như được đánh dấu bằng đường màu xám. So sánh, việc tăng Hd_k cho mô hình HeteroAtt cho cải thiện chất lượng mô hình, nhưng cũng tăng độ trễ đáng kể (trong Hình 5(b)). Đối với đánh đổi chất lượng và độ trễ, chúng tôi đặt Hd_v=256.

Tương tự, chúng tôi quan sát trong Hình 5(c), đối với mô hình Hiformer, việc giảm Hd_k xuống 64 không cho mất mát chất lượng, nhưng không giống như mô hình HeteroAtt, hầu như không có lợi ích độ trễ. Điều này là do xấp xỉ hạng thấp trong chiếu QK, đặc biệt chúng tôi đặt hạng r_k=256, tương đối nhỏ, so với r_v=1024. Theo đó, chiếu query chiếm ưu thế chi phí tính toán. Với Hd_v tăng, độ trễ tăng đáng kể, được hiển thị trong Hình 5(d).

Quan sát này về đánh đổi chất lượng mô hình và độ trễ của Hd_v cung cấp tính độc đáo của mô hình HeteroAtt và Hiformer. Nó có thể đóng vai trò như một công cụ để điều chỉnh khả năng mô hình với chi phí độ trễ. Ví dụ, khi số lượng yêu cầu mỗi truy vấn tương đối nhỏ, chúng ta có thể đạt được hiệu suất mô hình tốt hơn bằng cách tăng d_v. Công cụ như vậy có thể không có sẵn cho các mô hình SOTA khác, chẳng hạn như DCN [46].

4.6 Thử nghiệm A/B Trực tuyến
Ngoài các thí nghiệm offline mở rộng, chúng tôi cũng đã tiến hành thí nghiệm thử nghiệm A/B trực tuyến với framework mô hình được đề xuất. Đối với mỗi nhóm kiểm soát và thử nghiệm, có 1% người dùng được chọn ngẫu nhiên, nhận gợi ý mục dựa trên các thuật toán xếp hạng khác nhau. Để hiểu sự cải thiện mà chúng tôi đạt được thông qua việc học tương tác đặc trưng dị chủng, mô hình cơ sở là tận dụng mô hình Transformer một lớp để nắm bắt tương tác đặc trưng. Chúng tôi thu thập các chỉ số tương tác của người dùng trong 10 ngày, như được hiển thị trong Bảng 3. Mô hình HeteroAtt một lớp vượt trội đáng kể so với mô hình Transformer một lớp, phù hợp với phân tích offline của chúng tôi. Ngoài ra, hai

(a) HeteroAtt: Hd_k.
(b) HeteroAtt: Hd_v.
(c) Hiformer: Hd_k.
(d) Hiformer: Hd_v.

Hình 5: Độ nhạy Tham số. Với d_k và d_v tăng, lợi ích chất lượng mô hình bão hòa; tuy nhiên, chi phí suy luận mô hình tăng mạnh. Các đường đứt nét màu đen đánh dấu các giá trị d_k, d_v được chọn, mang lại đánh đổi tốt nhất giữa chất lượng và chi phí suy luận.

--- TRANG 9 ---
Hiformer: Học Tương tác Đặc trưng Dị chủng với Transformers cho Hệ thống Gợi ý Conference'17, July 2017, Washington, DC, USA

lớp mô hình HeteroAtt cải thiện thêm so với một lớp. Hiformer hoạt động tốt nhất trong tất cả mô hình, bao gồm cả mô hình DCN. Quan sát như vậy cho thấy rằng kiến trúc dựa trên Transformer có thể vượt trội hơn các mô hình tương tác đặc trưng SOTA. Chúng tôi đã triển khai thành công mô hình Hiformer vào sản xuất.

[THIS IS TABLE: Bảng 3 showing online engagement metrics for different models]
Mô hình           Số Lớp    Chỉ số Tương tác
Transformer       1         +0.00%
HeteroAtt (ours)  1         +1.27%*
HeteroAtt (ours)  2         +2.33%*
Hiformer (ours)   1         +2.66%*
DCN               1         +2.20%*

Bảng 3: Chỉ số tương tác trực tuyến và độ trễ phục vụ của các mô hình khác nhau. * cho thấy có ý nghĩa thống kê.

5 CÔNG TRÌNH LIÊN QUAN
Trước kỷ nguyên học sâu, người ta thường tạo thủ công các đặc trưng chéo và thêm chúng vào mô hình hồi quy logistic để cải thiện hiệu suất. Một số cũng tận dụng các mô hình dựa trên cây quyết định [11] để học các chéo đặc trưng. Sau đó, sự phát triển của các kỹ thuật embedding đã dẫn đến thiết kế Factorization Machines (FM) [34], được đề xuất để mô hình hóa tương tác đặc trưng bậc hai với tích trong của hai véc-tơ tiềm ẩn.

Gần đây, DNN đã trở thành xương sống của nhiều mô hình trong ngành công nghiệp. Để cải thiện hiệu quả của việc học chéo đặc trưng, nhiều công trình mô hình hóa một cách rõ ràng tương tác đặc trưng bằng cách thiết kế một hàm g(x_i,x_j) trong khi tận dụng các chéo đặc trưng ngầm được học bởi DNN. Mô hình Wide & deep [3] kết hợp mô hình DNN với thành phần wide bao gồm các chéo của đặc trưng thô. Kể từ đó, nhiều công trình đã được đề xuất để tự động hóa công việc chéo đặc trưng thủ công của wide & deep bằng cách giới thiệu các hoạt động giống FM (tích trong) hoặc tích Hadamard. DeepFM [8] và DLRM [29] áp dụng FM trong mô hình, Neural FM [10] tổng quát hóa FM bằng cách kết hợp tích Hadamard, PNN [32] cũng sử dụng tích trong và tích ngoài để nắm bắt tương tác đặc trưng cặp. Tuy nhiên, các phương pháp này chỉ có thể mô hình hóa đến tương tác đặc trưng bậc hai. Cũng có công trình có thể mô hình hóa tương tác đặc trưng bậc cao hơn. Deep Cross Network (DCN) [45,46] thiết kế cross network để học một cách rõ ràng tương tác đặc trưng bậc cao nơi bậc tương tác tăng theo độ sâu lớp. DCN-V2 [46] làm cho DCN biểu đạt hơn và thực tế trong thiết lập công nghiệp quy mô lớn. xDeepFM [26] cũng cải thiện khả năng biểu đạt của cross network trong DCN và dựa vào tích Hadamard để nắm bắt các chéo đặc trưng bậc cao.

Một hướng nghiên cứu khác tận dụng cơ chế attention trong mô hình Transformer để nắm bắt tương tác đặc trưng. AutoInt [40] được đề xuất để nắm bắt tương tác đặc trưng thông qua lớp multi-head self-attention. Tuy nhiên, vì Transformer được thiết kế để mô hình hóa các mối quan hệ độc lập với bối cảnh, chẳng hạn như token văn bản, việc áp dụng trực tiếp lớp multi-head self-attention cho tương tác đặc trưng có khả năng biểu đạt mô hình rất hạn chế. Tương tự, [25] dựa trên cơ chế attention với chuyển đổi tích chéo để nắm bắt attention phân cấp, không cung cấp nhận thức đặc trưng và căn chỉnh ngữ nghĩa.

Công trình Heterogeneous Graph Transformer [13] liên quan đến của chúng tôi, nơi các loại nút được xem xét trong lớp mutual attention dị chủng. Mặc dù chúng tôi chia sẻ một số điểm tương đồng trong động lực, công việc của chúng tôi khác biệt đáng kể vì mô hình Hiformer được đề xuất để nắm bắt tương tác đặc trưng cho hệ thống gợi ý quy mô web, và thiết kế của lớp attention Hiformer có nhiều khả năng biểu đạt hơn. Công trình field-aware factorization machine [18] cũng liên quan. Trong [18], nhận thức đặc trưng của việc học tương tác đặc trưng được triển khai thông qua các bảng tra cứu embedding riêng biệt, trong khi các phương pháp được đề xuất của chúng tôi thông qua chuyển đổi thông tin trong lớp attention dị chủng. PNN [33] tận dụng kernel Factorization Machine để xem xét động lực đặc trưng trong việc học tương tác đặc trưng, chia sẻ sự tương đồng với lớp attention dị chủng. Tuy nhiên, lớp attention dị chủng dựa trên attention trong khi PNN được phát triển từ factorization machine. Ngoài ra, chúng tôi tận dụng chiếu Query và Key để học tương tác đặc trưng dị chủng trong khi PNN dựa trên ma trận kernel. Hơn nữa, chúng tôi tiếp tục tăng khả năng biểu đạt mô hình trong Hiformer với giảm chi phí.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Trong công trình này, chúng tôi đề xuất lớp attention dị chủng, đây là một sửa đổi đơn giản nhưng hiệu quả của lớp self-attention để cung cấp nhận thức đặc trưng cho việc học tương tác đặc trưng. Sau đó chúng tôi tiếp tục cải thiện khả năng biểu đạt mô hình, và đề xuất Hiformer để cải thiện việc học tương tác đặc trưng. Chúng tôi cũng cải thiện hiệu suất phục vụ của Hiformer sao cho nó có thể đáp ứng yêu cầu độ trễ phục vụ cho các ứng dụng quy mô web. Thay đổi chính trong phương pháp của chúng tôi là xác định các mối quan hệ khác nhau giữa các đặc trưng khác nhau trong cơ chế attention, sao cho chúng ta có thể nắm bắt mức độ phù hợp ngữ nghĩa cho thông tin động trong embedding đặc trưng đã học. Cả thí nghiệm offline và online trên nền tảng dịch vụ phân phối kỹ thuật số hàng đầu thế giới đều chứng minh tính hiệu quả và hiệu suất của phương pháp của chúng tôi. Kết quả thí nghiệm này cho thấy rằng mô hình dựa trên Transformer cho việc học tương tác đặc trưng có thể hoạt động tốt hơn các phương pháp SOTA trong các ứng dụng quy mô web.

Công việc của chúng tôi đóng vai trò như một cây cầu để đưa kiến trúc dựa trên Transformer đến các ứng dụng trong hệ thống gợi ý quy mô web. Đối với công việc tương lai, chúng tôi quan tâm đến việc các tiến bộ gần đây trong kiến trúc Transformer [9] trong các lĩnh vực khác như NLP và CVR, có thể được chuyển đổi sang hệ thống gợi ý như thế nào. Với thành phần tương tác đặc trưng mới được giới thiệu trong bài báo này, nó cũng có thể được tận dụng trong tìm kiếm kiến trúc nơ-ron [7] để cải thiện thêm hệ thống gợi ý.

--- TRANG 10 ---
Conference'17, July 2017, Washington, DC, USA Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, và Ed H. Chi

TÀI LIỆU THAM KHẢO
[1] Beutel, A., Covington, P., Jain, S., Xu, C., Li, J., Gatto, V., và Chi, E. H. Latent cross: Making use of context in recurrent recommender systems. Trong Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (2018), tr. 46–54.
[2] Caruana, R. Multitask learning. Machine learning 28, 1 (1997), 41–75.
[3] Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., Anderson, G., Corrado, G., Chai, W., Ispir, M., et al. Wide & deep learning for recommender systems. Trong Proceedings of the 1st workshop on deep learning for recommender systems (2016), tr. 7–10.
[4] Covington, P., Adams, J., và Sargin, E. Deep neural networks for youtube recommendations. Trong Proceedings of the 10th ACM conference on recommender systems (2016), tr. 191–198.
[5] Davidson, J., Liebald, B., Liu, J., Nandy, P., Van Vleet, T., Gargi, U., Gupta, S., He, Y., Lambert, M., Livingston, B., et al. The youtube video recommendation system. Trong Proceedings of the fourth ACM conference on Recommender systems (2010), tr. 293–296.
[6] Devlin, J., Chang, M.-W., Lee, K., và Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[7] Elsken, T., Metzen, J. H., và Hutter, F. Neural architecture search: A survey. The Journal of Machine Learning Research 20, 1 (2019), 1997–2017.
[8] Guo, H., Tang, R., Ye, Y., Li, Z., và He, X. Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247 (2017).
[9] Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C., Xu, Y., et al. A survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence (2022).
[10] He, X., Liao, L., Zhang, H., Nie, L., Hu, X., và Chua, T.-S. Neural collaborative filtering. Trong Proceedings of the 26th international conference on world wide web (2017), tr. 173–182.
[11] He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y., Atallah, A., Herbrich, R., Bowers, S., et al. Practical lessons from predicting clicks on ads at facebook. Trong Proceedings of the eighth international workshop on data mining for online advertising (2014), tr. 1–9.
[12] Hendrycks, D., và Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016).
[13] Hu, Z., Dong, Y., Wang, K., và Sun, Y. Heterogeneous graph transformer. Trong Proceedings of the web conference 2020 (2020), tr. 2704–2710.
[14] Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., và Carreira, J. Perceiver: General perception with iterative attention. Trong International conference on machine learning (2021), PMLR, tr. 4651–4664.
[15] Joglekar, M. R., Li, C., Chen, M., Xu, T., Wang, X., Adams, J. K., Khaitan, P., Liu, J., và Le, Q. V. Neural input search for large scale recommendation models. Trong Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2020), tr. 2387–2397.
[16] Jouppi, N., Young, C., Patil, N., và Patterson, D. Motivation for and evaluation of the first tensor processing unit. ieee Micro 38, 3 (2018), 10–19.
[17] Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. Trong Proceedings of the 44th annual international symposium on computer architecture (2017), tr. 1–12.
[18] Juan, Y., Zhuang, Y., Chin, W.-S., và Lin, C.-J. Field-aware factorization machines for ctr prediction. Trong Proceedings of the 10th ACM conference on recommender systems (2016), tr. 43–50.
[19] Kalyan, K. S., Rajasekharan, A., và Sangeetha, S. Ammus: A survey of transformer-based pretrained models in natural language processing. arXiv preprint arXiv:2108.05542 (2021).
[20] Kang, W.-C., và McAuley, J. Self-attentive sequential recommendation. Trong 2018 IEEE international conference on data mining (ICDM) (2018), IEEE, tr. 197–206.
[21] Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., và Shah, M. Transformers in vision: A survey. ACM computing surveys (CSUR) 54, 10s (2022), 1–41.
[22] LeCun, Y., Bengio, Y., và Hinton, G. Deep learning. nature 521, 7553 (2015), 436–444.
[23] Li, D., Hu, B., Chen, Q., Wang, X., Qi, Q., Wang, L., và Liu, H. Attentive capsule network for click-through rate and conversion rate prediction in online advertising. Knowledge-Based Systems 211 (2021), 106522.
[24] Li, M., Liu, Z., Smola, A. J., và Wang, Y.-X. Difacto: Distributed factorization machines. Trong Proceedings of the Ninth ACM International Conference on Web Search and Data Mining (2016), tr. 377–386.
[25] Li, Z., Cheng, W., Chen, Y., Chen, H., và Wang, W. Interpretable click-through rate prediction through hierarchical attention. Trong Proceedings of the 13th International Conference on Web Search and Data Mining (2020), tr. 313–321.
[26] Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., và Sun, G. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. Trong Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (2018), tr. 1754–1763.
[27] Lu, J., Wu, D., Mao, M., Wang, W., và Zhang, G. Recommender system application developments: a survey. Decision Support Systems 74 (2015), 12–32.
[28] Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W., Songhori, E., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Nazi, A., et al. A graph placement methodology for fast chip design. Nature 594, 7862 (2021), 207–212.
[29] Naumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sundaraman, N., Park, J., Wang, X., Gupta, U., Wu, C.-J., Azzolini, A. G., et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019).
[30] Okura, S., Tagami, Y., Ono, S., và Tajima, A. Embedding-based news recommendation for millions of users. Trong Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (2017), tr. 1933–1942.
[31] Pan, X., Li, M., Zhang, J., Yu, K., Wen, H., Wang, L., Mao, C., và Cao, B. Metacvr: Conversion rate prediction via meta learning in small-scale recommendation scenarios. Trong Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (2022), tr. 2110–2114.
[32] Qu, Y., Cai, H., Ren, K., Zhang, W., Yu, Y., Wen, Y., và Wang, J. Product-based neural networks for user response prediction. Trong 2016 IEEE 16th International Conference on Data Mining (ICDM) (2016), IEEE, tr. 1149–1154.
[33] Qu, Y., Fang, B., Zhang, W., Tang, R., Niu, M., Guo, H., Yu, Y., và He, X. Product-based neural networks for user response prediction over multi-field categorical data. ACM Transactions on Information Systems (TOIS) 37, 1 (2018), 1–35.
[34] Rendle, S. Factorization machines. Trong 2010 IEEE International conference on data mining (2010), IEEE, tr. 995–1000.
[35] Resnick, P., và Varian, H. R. Recommender systems. Communications of the ACM 40, 3 (1997), 56–58.
[36] Schafer, J. B., Konstan, J., và Riedl, J. Recommender systems in e-commerce. Trong Proceedings of the 1st ACM conference on Electronic commerce (1999), tr. 158–166.
[37] Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 (2020).
[38] Singh, P., Manure, A., Singh, P., và Manure, A. Introduction to tensorflow 2.0. Learn TensorFlow 2.0: Implement Machine Learning and Deep Learning Models with Python (2020), 1–24.
[39] Song, Q., Cheng, D., Zhou, H., Yang, J., Tian, Y., và Hu, X. Towards automated neural interaction discovery for click-through rate prediction. Trong Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2020), tr. 945–955.
[40] Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., và Tang, J. Autoint: Automatic feature interaction learning via self-attentive neural networks. Trong Proceedings of the 28th ACM International Conference on Information and Knowledge Management (2019), tr. 1161–1170.
[41] Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., và Jiang, P. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. Trong Proceedings of the 28th ACM international conference on information and knowledge management (2019), tr. 1441–1450.
[42] Tay, Y., Dehghani, M., Bahri, D., và Metzler, D. Efficient transformers: A survey. ACM Computing Surveys 55, 6 (2022), 1–28.
[43] Tsang, M., Cheng, D., và Liu, Y. Detecting statistical interactions from neural network weights. arXiv preprint arXiv:1705.04977 (2017).
[44] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., và Polosukhin, I. Attention is all you need. Advances in neural information processing systems 30 (2017).
[45] Wang, R., Fu, B., Fu, G., và Wang, M. Deep & cross network for ad click predictions. Trong Proceedings of the ADKDD'17. 2017, tr. 1–7.
[46] Wang, R., Shivanna, R., Cheng, D., Jain, S., Lin, D., Hong, L., và Chi, E. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. Trong Proceedings of the Web Conference 2021 (2021), tr. 1785–1797.
[47] Yang, J., Yi, X., Zhiyuan Cheng, D., Hong, L., Li, Y., Xiaoming Wang, S., Xu, T., và Chi, E. H. Mixed negative sampling for learning two-tower neural networks in recommendations. Trong Companion Proceedings of the Web Conference 2020 (2020), tr. 441–447.
[48] Yu, H., Chen, C., Du, X., Li, Y., Rashwan, A., Hou, L., Jin, P., Yang, F., Liu, F., Kim, J., et al. Tensorflow model garden. GitHub (2020).
[49] Zhang, S., Yao, L., Sun, A., và Tay, Y. Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys (CSUR) 52, 1 (2019), 1–38.
[50] Zhao, Z., Hong, L., Wei, L., Chen, J., Nath, A., Andrews, S., Kumthekar, A., Sathiamoorthy, M., Yi, X., và Chi, E. Recommending what video to watch next: a multitask ranking system. Trong Proceedings of the 13th ACM Conference on Recommender Systems (2019), tr. 43–51.
[51] Zhuang, H., Wang, X., Bendersky, M., và Najork, M. Feature transformation for neural ranking models. Trong Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval (2020), tr. 1649–1652.
