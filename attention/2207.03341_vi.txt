The paper hasn't been translated to Vietnamese yet. Based on the text you provided, I'll translate it directly. However, this is a very large academic paper (22 pages), so I'll provide the Vietnamese translation:

# 2207.03341.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2207.03341.pdf
# Kích thước tập tin: 11226920 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================


--- TRANG 1 ---
International Journal of Computer Vision
https://doi.org/10.1007/s11263-024-02035-5
Softmax-free Linear Transformers
Jiachen Lu1,·Junge Zhang1,·Xiatian Zhu2,·Jiafeng Feng1,·Tao Xiang2,·Li Zhang1/enve♀e
Tóm tắt Các vision transformer (ViT) đã đẩy
nghệ thuật hiện đại cho các nhiệm vụ nhận thức thị giác. Cơ chế
self-attention làm nền tảng cho sức mạnh của ViT
có độ phức tạp bậc hai trong cả tính toán và
sử dụng bộ nhớ. Điều này thúc đẩy việc phát triển để
xấp xỉ self-attention với độ phức tạp tuyến tính. Tuy nhiên,
một phân tích sâu sắc trong công trình này cho thấy rằng các
phương pháp hiện tại hoặc có lỗi về mặt lý thuyết hoặc không
hiệu quả về mặt thực nghiệm cho nhận dạng thị giác. Chúng tôi xác định rằng
những hạn chế của chúng bắt nguồn từ việc kế thừa
self-attention dựa trên softmax trong quá trình xấp xỉ, nghĩa
là, chuẩn hóa tích vô hướng có tỷ lệ giữa các vector
đặc trưng token bằng hàm softmax. Vì việc bảo tồn
hoạt động softmax thách thức bất kỳ nỗ lực
tuyến tính hóa tiếp theo nào. Bằng hiểu biết này, một họ
SOftmax-Free Transformers (SOFT) được đề xuất. Cụ thể,
một hàm kernel Gaussian được áp dụng để thay thế
tương tự tích vô hướng, cho phép một ma trận
self-attention đầy đủ được xấp xỉ dưới sự phân tách ma trận
hạng thấp. Để có tính mạnh mẽ về mặt tính toán, chúng tôi ước lượng
nghịch đảo Moore-Penrose bằng phương pháp Newton-Raphson
lặp trong quá trình thuận chiều chỉ, trong khi tính toán
các gradient lý thuyết chỉ một lần trong quá
trình ngược chiều. Để mở rộng thêm khả năng áp dụng (ví dụ, các nhiệm vụ dự đoán
dày đặc), một kỹ thuật chuẩn hóa đối xứng
hiệu quả được giới thiệu. Các thí nghiệm mở rộng trên ImageNet,
COCO và ADE20K cho thấy rằng SOFT của chúng tôi cải thiện đáng kể
hiệu quả tính toán của các biến thể ViT
hiện tại. Với độ phức tạp tuyến tính, các chuỗi token dài hơn nhiều
được SOFT cho phép, dẫn đến sự
đánh đổi vượt trội giữa độ chính xác và độ phức tạp. Mã và
mô hình có sẵn tại https://github.com/fudan-zvg/
SOFT .
Tác giả liên hệ: Li Zhang
E-mail: lizhangfd@fudan.edu.cn
1School of Data Science, Fudan University, Shanghai, China
2University of Surrey, Guildford, UK Từ khóa Transformer ·độ phức tạp tuyến tính ·chuẩn hóa softmax
·softmax-free ·attention Gaussian

1 Giới thiệu
Gần đây, sự thay đổi bước ngoặt do Transformers (Vaswani
et al., 2017) mang lại trong xử lý ngôn ngữ tự nhiên (NLP) (Brown
et al., 2020, Devlin et al., 2019) dường như đã đến với
thị giác (Dosovitskiy et al., 2021, Yuan et al., 2021, Zheng
et al., 2021, Zhu et al., 2021). Thật vậy, với ít thiên kiến quy nạp
hơn trong thiết kế kiến trúc so với các mạng nơ-ron
tích chập (CNN), Vision Transformer (ViT) thuần túy
(Dosovitskiy et al., 2021) và các biến thể của nó đã cho thấy
có thể vượt trội hơn CNN trong nhiều nhiệm vụ thị giác khác nhau
(d'Ascoli et al., 2021, Jaegle et al., 2021). Tuy nhiên,
có một nút thắt cổ chai trong bất kỳ mô hình dựa trên Transformer nào,
đó là độ phức tạp bậc hai trong cả tính toán
và sử dụng bộ nhớ. Điều này là bản chất của cơ chế
self-attention: cho một chuỗi các token (ví dụ, từ hoặc
các mảnh hình ảnh) làm đầu vào, mô-đun self-attention
học các biểu diễn đặc trưng một cách lặp đi lặp lại bằng cách liên kết
một token với tất cả các token khác. Điều này dẫn đến độ phức tạp bậc hai
O(n2) với độ dài chuỗi token n trong
cả tính toán (thời gian) và bộ nhớ (không gian) vì một
ma trận attention có kích thước n×n cần được tính toán
và lưu trong quá trình suy luận. Vấn đề này đặc biệt
nghiêm trọng trong thị giác: một hình ảnh 2D sau khi token hóa sẽ tạo ra
một chuỗi dài hơn nhiều so với những chuỗi trong NLP ngay cả với
độ phân giải không gian vừa phải. Độ phức tạp bậc hai này
do đó ngăn cản một mô hình ViT mô hình hóa hình ảnh
ở độ phân giải không gian cao, điều thường quan trọng cho
các nhiệm vụ nhận dạng thị giác.

Một giải pháp tự nhiên là giảm độ phức tạp của
tính toán self-attention thông qua xấp xỉ. Thật vậy,
đã có một số nỗ lực trong NLP (Choro-
manski et al., 2021, Kitaev et al., 2020, Wang et al.,
2020, Xiong et al., 2021). Ví dụ, (Wang et al., arXiv:2207.03341v3 [cs.CV] 15 Mar 2024

--- TRANG 2 ---
2 Jiachen Lu1, et al.
10 20 30 40 50 60 70 80 90
Parameter (M)7072747678808284Imagenet T op-1 Accuracy (%)
PVT-TinyPVT-SmallPVT-MediumPVT-Large
ResNet-18ResNet-50ResNet-101T2T-ViT-19T2T-ViT-24
DeiT-B
ViT-BSwin-TSwin-SSwin-B
T wins-SVT-S
SAN10CoAtNet-1CoAtNet-2
SOFT-TinySOFT-SmallSOFT-MediumSOFT-Large
(a)
784×1784×2784×3784×4784×5784×6784×7784×8
/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000056/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni0000004f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013/uni00000013/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c
/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni0000002f/uni0000004c/uni00000051/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000031/uni0000005c/uni00000056/uni00000057/uni00000055/uni00000052/uni00000050/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055
/uni00000036/uni00000032/uni00000029/uni00000037 (b)
Hình 1 Độ chính xác phân loại Top-1 trên tập xác nhận ImageNet (Deng et al., 2009) theo số tham số và việc sử dụng bộ nhớ
tương ứng với độ dài chuỗi token trong thực tế so với các phương pháp khác. (a) So sánh với các mô hình CNN:
ResNet (He et al., 2016) và CoAtNet (Dai et al., 2021) Các mô hình Transformer: PVT (Wang et al., 2021), Swin (Liu et al.,
2021), DeiT (Touvron et al., 2021a), ViT (Dosovitskiy et al., 2021), T2T-ViT (Yuan et al., 2021), Twins-SVT (Chu et al.,
2021) và SAN10 (Zhao et al., 2020); (b) So sánh với Transformer (Vaswani et al., 2017), Linformer (Wang et al., 2020),
Nyström former (Xiong et al., 2021) và Performer (Choromanski et al., 2021). Việc sử dụng bộ nhớ được đo với kích thước batch
là 1 trên Tesla V100 16GB.
2020) có cách tiếp cận ngây thơ bằng cách rút ngắn độ dài
của Key và Value thông qua các phép chiếu có thể học được. Việc xấp xỉ
thô như vậy sẽ không thể tránh khỏi gây ra suy giảm hiệu suất.
Ngược lại, (Choromanski et al., 2021, Katharopou-
los et al., 2020) cả hai đều tận dụng cơ chế kernel
để xấp xỉ chuẩn hóa softmax để tuyến tính hóa
tính toán trong self-attention. (Kitaev et al., 2020) thay vào đó
áp dụng chiến lược băm để tính toán có chọn lọc
các cặp tương tự nhất. Gần đây, (Xiong et al., 2021)
sử dụng phân tách ma trận Nyström để tái tạo
ma trận attention đầy đủ với lặp đa thức để xấp xỉ
nghịch đảo giả của ma trận landmark. Tuy nhiên, chuẩn hóa softmax
chỉ đơn giản được nhân đôi qua quá trình phân tách ma trận, điều này
không chắc chắn về mặt lý thuyết. Chúng tôi tìm thấy thực nghiệm rằng không có
phương pháp nào trong số này hiệu quả khi áp dụng cho thị giác
(xem Mục 4.1).

Trong công trình này, chúng tôi xác định rằng những hạn chế của các
Transformer hiệu quả hiện tại được gây ra bởi việc sử dụng
self-attention softmax, và lần đầu tiên đề xuất một
Transformer không có softmax. Cụ thể hơn, trong tất cả các
Transformer hiện tại (có hoặc không có tuyến tính hóa), cần có
chuẩn hóa softmax trên tích vô hướng có tỷ lệ
giữa các vector đặc trưng token (Vaswani et al.,
2017). Việc giữ lại hoạt động softmax này thách thức bất kỳ
nỗ lực tuyến tính hóa tiếp theo nào. Để vượt qua trở ngại này,
chúng tôi giới thiệu một cơ chế self-attention không có softmax
mới, được gọi là SOFT, với độ phức tạp tuyến tính
O(n) trong cả không gian và thời gian. Cụ thể, SOFT sử dụng
kernel Gaussian để định nghĩa hàm tương tự (self-attention) mà không cần chuẩn hóa softmax
tiếp theo. Với ma trận attention không có softmax này, chúng tôi
giới thiệu thêm một thuật toán phân tách ma trận hạng thấp
mới để xấp xỉ. Tính mạnh mẽ của việc xấp xỉ
được đảm bảo về mặt lý thuyết bằng cách sử dụng
phương pháp Newton-Raphson để tính toán đáng tin cậy
nghịch đảo Moore-Penrose của ma trận.

Chúng tôi đóng góp như sau. (I) Chúng tôi giới thiệu
một Transformer không có softmax mới với độ phức tạp không gian
và thời gian tuyến tính. (II) Việc xấp xỉ ma trận attention của chúng tôi
được thực hiện thông qua một thuật toán phân tách ma trận
mới với đảm bảo lý thuyết. (III)
Để đánh giá phương pháp của chúng tôi cho các nhiệm vụ nhận dạng thị giác, chúng tôi
thiết kế một họ các kiến trúc backbone chung
với các khả năng khác nhau sử dụng SOFT làm thành phần
self-attention cốt lõi. Các thí nghiệm mở rộng cho thấy rằng với
độ phức tạp tuyến tính (Hình 1b), các mô hình SOFT của chúng tôi có thể
nhận đầu vào là các chuỗi token hình ảnh dài hơn nhiều.
Kết quả là, với cùng kích thước mô hình, SOFT của chúng tôi vượt trội
hơn các CNN và biến thể ViT hiện đại nhất
trên phân loại ImageNet (Deng et al., 2009) trong
sự đánh đổi giữa độ chính xác/độ phức tạp (Hình 1a).

Một phiên bản sơ bộ của công trình này đã được trình bày
trong NeurIPS 2021 spotlight (Lu et al., 2021). Trong bài báo này,
chúng tôi đã mở rộng thêm phiên bản hội nghị như sau: (i) Chúng tôi cải thiện hiệu quả và tính mạnh mẽ
trong việc tính toán nghịch đảo Moore-Penrose bằng cách sử dụng
một phương pháp lặp trong quá trình thuận chiều chỉ
trong khi tính toán gradient lý thuyết chỉ một lần trong
quá trình lan truyền ngược. (ii) Chúng tôi phân tích những hạn chế

--- TRANG 3 ---
Softmax-free Linear Transformers 3
của SOFT sơ bộ từ góc độ chuẩn phổ ma trận, tiết lộ tầm quan trọng của chuẩn hóa
để tăng cường khả năng tổng quát hóa nhiệm vụ của mô hình.
(iii) Chúng tôi chứng minh rằng SOFT sơ bộ trải qua
sự gia tăng bậc hai trong chuẩn phổ ma trận so với
kích thước ma trận, khiến nó thất bại trong các vấn đề thị giác dày đặc.
(iv) Để giải quyết những hạn chế này, chúng tôi đề xuất
một self-attention không có softmax được chuẩn hóa, giữ độ phức tạp
tuyến tính trong khi tăng cường hiệu suất, được hỗ trợ bởi
cả chứng minh lý thuyết và thí nghiệm mở rộng. (v)
SOFT cải tiến vượt trội hơn các CNN và ViT hiện đại nhất
cho phân loại trên ImageNet (Deng
et al., 2009), phát hiện đối tượng trên COCO (Lin et al.,
2014) và phân đoạn ngữ nghĩa trên ADE20K (Zhou
et al., 2019).

2 Công trình liên quan

2.1 Vision Transformers
Gần đây có sự quan tâm nghiên cứu gia tăng trong việc
khai thác Transformers cho các nhiệm vụ nhận dạng thị giác (Guo
et al., 2022, Touvron et al., 2021a, Wang et al., 2021,
2018, Yuan et al., 2021, Zhang et al., 2020), được truyền cảm hứng bởi
thành công đáng chú ý của chúng trong NLP (Brown et al., 2020,
Devlin et al., 2019, Vaswani et al., 2017). Cốt lõi của các
transformer NLP và thị giác này là cùng một cơ chế
self-attention (Vaswani et al., 2017) tính toán ma trận
self-attention bằng cách so sánh đầy đủ các cặp token.
Điều này có nghĩa là độ phức tạp bậc hai với độ dài
chuỗi trong cả không gian và thời gian, do đó hạn chế
khả năng mở rộng của Transformers trong việc xử lý các chuỗi dài.
Hạn chế này nghiêm trọng hơn trong thị giác so với NLP:
Để xử lý một hình ảnh với ít nhất hàng nghìn pixel,
token hóa theo patch là điều bắt buộc đối với Transformers để
kiểm soát chi phí tính toán. Với hình ảnh có độ phân giải cao hơn,
kích thước patch cũng cần được mở rộng tỷ lệ thuận,
hy sinh độ phân giải không gian. Điều này hạn chế
khả năng của Transformers, ví dụ, học biểu diễn đặc trưng
chi tiết như yêu cầu trong nhiều nhiệm vụ nhận dạng
thị giác.

2.2 Linear Transformers
Gần đây, đã có một số biến thể tuyến tính/hiệu quả
(Choromanski et al., 2021, Kasai et al., 2021,
Katharopoulos et al., 2020, Kitaev et al., 2020, Peng
et al., 2021, Tay et al., 2023, Wang et al., 2020) của
Transformers trong NLP. Ví dụ, (Wang et al., 2020)
học thu nhỏ độ dài của Key và Value dựa trên
giả định hạng thấp. (Kitaev et al., 2020) áp dụng chiến lược băm để chọn lọc các cặp tương tự nhất
và chỉ tính toán attention giữa chúng. (Choroman-
ski et al., 2021, Katharopoulos et al., 2020) sử dụng các
hàm kernel khác nhau để xấp xỉ ma trận
self-attention dựa trên softmax. (Peng et al., 2021) áp dụng
ánh xạ đặc trưng ngẫu nhiên trên các chuỗi để tiếp cận
hàm softmax gốc. (Kasai et al., 2021) giảm
thời gian và bộ nhớ tiêu thụ của ma trận
attention bằng cách thay thế hàm softmax bằng
giải pháp thay thế hồi quy độ phức tạp tuyến tính. Khi áp dụng
cho các nhiệm vụ nhận dạng thị giác, tuy nhiên, chúng tôi chỉ ra rằng
các mô hình này có suy giảm hiệu suất đáng kể
so với Transformers tiêu chuẩn (Vaswani
et al., 2017) (xem Mục 4.1).

Công trình liên quan nhất với SOFT là (Xiong et al.,
2021) sử dụng phân tách ma trận Nyström
để tránh tính toán ma trận attention đầy đủ. Tuy nhiên,
phương pháp này gặp phải một số khiếm khuyết lý thuyết: (1)
Vì self-attention tiêu chuẩn cần áp dụng chuẩn hóa softmax
theo hàng trên ma trận attention đầy đủ, việc áp dụng trực tiếp
phân tách ma trận là không khả thi.
Như một cách giải quyết mà không có hỗ trợ lý thuyết vững chắc, soft-
max chỉ đơn giản được áp dụng cho tất cả các ma trận thành phần trong
(Xiong et al., 2021). Việc xấp xỉ như vậy không được
đảm bảo về mặt lý thuyết. (2) Với phương pháp lặp đa thức,
không được đảm bảo rằng nghịch đảo ma trận attention
tổng quát có thể được tính toán khi ma trận
gần như suy biến trong thực tế. Trái ngược với tất cả các
phương pháp trên, trong bài báo này chúng tôi đề xuất một cơ chế
self-attention không có softmax giúp thuận lợi cho việc phân tách ma trận
để giảm thiểu độ phức tạp với đảm bảo lý thuyết.

3 Phương pháp

3.1 Công thức self-attention không có softmax

Một minh họa sơ đồ về mô hình của chúng tôi được đưa ra trong Hình
2. Trước tiên hãy xem thiết kế mô-đun attention của chúng tôi. Cho
một chuỗi n token X∈Rn×d với mỗi token được biểu diễn
bởi một vector đặc trưng d chiều, self-attention
(Vaswani et al., 2017) nhằm khám phá các mối tương quan
của tất cả các cặp token một cách đầy đủ.

Một cách chính thức, X đầu tiên được chiếu tuyến tính vào ba không gian
de chiều (query, key, và values) như:
Q=XWq∈Rn×de, K=XWk∈Rn×de, V=XWv∈Rn×de,
(1)
trong đó Wq, Wk, Wv∈Rd×de là các ma trận có thể học được. Để
đơn giản hóa phương trình, chúng tôi bỏ qua ký hiệu multi-head trong
các hoạt động self-attention. Tuy nhiên, cần lưu ý rằng

--- TRANG 4 ---
4 Jiachen Lu1, et al.
các cơ chế multi-head được sử dụng xuyên suốt.
Self-attention có thể được biểu diễn trong một công thức chung
như:
yi,:=nX
j=1α(Qi,:, Kj,:)⊙Vj,:, (2)
trong đó ⊙ là tích Hadamard, và i, j∈ {1,···, n}
chỉ mục các token. Hàm self-attention chính α:
Rde×Rde→R được cấu thành từ một hàm phi tuyến
β:R→R và một hàm quan hệ γ:Rde×Rde→R.
Một thể hiện chủ đạo của α là self-attention softmax
dựa trên tích vô hướng có tỷ lệ (Vaswani et al., 2017), được
định nghĩa như
β(·) = softmax(·), γ(Qi,:, Kj,:) =1√de·Q⊤
i,:Kj,:.(3)
Trong khi self-attention softmax này đã là lựa chọn
thực tế và hiếm khi bị đặt câu hỏi, như đã thảo luận trước đó nó
không nhất thiết phù hợp cho tuyến tính hóa. Để tạo thuận lợi cho
thiết kế self-attention tuyến tính, chúng tôi giới thiệu một hàm
self-attention không có softmax với tích vô hướng được thay thế
bằng kernel Gaussian như:
β′(·) = exp(·), γ′(Qi,:, Kj,:) =−1
2√de·∥Qi,:−Kj,:∥2
2.
(4)
Một cuộc phân tích Transformers bởi Tsai et al. (Tsai et al.,
2019) tiết lộ sự khác biệt hiệu suất không đáng kể giữa
các kernel bất đối xứng và đối xứng. Để duy trì
các tính chất đối xứng của ma trận attention như được định nghĩa
trong Eq. (3), chúng tôi chọn các ma trận chiếu giống hệt nhau Wq
và Wk trong Eq. (1), hiệu quả đặt Q=K. Để điều tra thêm
tác động của các kernel đối xứng, các thí nghiệm bổ sung
được tiến hành trong các nghiên cứu ablation của chúng tôi ở
Mục 4.4. Ma trận self-attention của chúng tôi sau đó được viết
như:
Si,j= exp
−1
2√de· ∥Qi,:−Kj,:∥2
2
. (5)
Để đơn giản hóa ký hiệu, chúng tôi định nghĩa công thức
ma trận như: S= exp(Q⊖K).
Nhận xét: Ma trận self-attention S của chúng tôi có ba
tính chất quan trọng: (1) Nó đối xứng; (2) Tất cả các phần tử
nằm trong phạm vi đơn vị [0,1]; (3) Tất cả các phần tử đường chéo
giữ giá trị lớn nhất 1 (tự củng cố), với
những phần tử thấp nhất (tương ứng với các cặp token
khác biệt nhất) gần bằng 0. Vì kernel Gaussian là một
kernel xác định dương (Fasshauer, 2011), S được coi
là ma trận Gram. Tuy nhiên, chúng tôi thấy rằng khi sử dụng
ma trận self-attention dựa trên kernel S mà không có tuyến tính hóa,
việc huấn luyện transformer không thể hội tụ.
Thảo luận thêm có thể được tìm thấy trong Mục 3.3.

3.2 Điều chỉnh hạng thấp thông qua phân tách ma trận với độ phức tạp tuyến tính
Để giải quyết các vấn đề hội tụ và độ phức tạp bậc hai,
chúng tôi tận dụng phân tách ma trận như một giải pháp thống nhất
với điều chỉnh hạng thấp. Cụ thể, chúng tôi
xem xét Nyström (Williams và Seeger, 2000), vốn là
một thuật toán xấp xỉ ma trận hạng thấp ban đầu.
Điều này cho phép độ phức tạp của mô hình chúng tôi được giảm đáng kể
mà không cần tính toán ma trận self-attention đầy đủ S.

Chúng tôi đưa ra lựa chọn này vì S của chúng tôi là nửa xác định dương
(tức là, ma trận Gram) mà không có chuẩn hóa
tiếp theo, đây là tất cả các điều kiện cần thiết cho Nyström.
Ngược lại, (Xiong et al., 2021) hoàn toàn bỏ qua các
yêu cầu này, dẫn đến khiếm khuyết lý thuyết trong việc xấp xỉ của nó.

Để định nghĩa phương pháp Nyström một cách chính thức, hãy biểu diễn
S= exp(Q⊖K) như một ma trận khối:
S=A B
B⊤C
∈Rn×n, (6)
trong đó A∈Rm×m,B∈Rm×(n−m),C∈R(n−m)×(n−m)
với m≪n. Thông qua phân tách Nyström (xem
chi tiết dẫn xuất trong Phụ lục A), một xấp xỉ
có thể được biểu diễn như:
ˆS=A
B⊤
A†
A B
=P⊤A†P, trong đó P=
A B
,
(7)
và A† là nghịch đảo Moore-Penrose (tổng quát) của
A.

Thuật toán 1: SOFT: Attention không có softmax
Đầu vào: Q∈Rn×de, hàm lấy mẫu fs
Lấy mẫu eQ←fs(Q) ;
A←exp(eQ⊖eQ),P←exp(eQ⊖Q);
ˆS←P⊤NR(A)P;
Đầu ra: ˆS
Thuật toán 2: NR: Lặp Newton-Raphson
Đầu vào: A∈Rm×m, và T ∈Z+
α= 2/∥A∥2
1.Khởi tạo A0←αA;
for k from 1 to T do
Ak←2Ak−1−Ak−1AAk−1
end
Đầu ra: AT

--- TRANG 5 ---
Softmax-free Linear Transformers 5
Hình 2 Minh họa sơ đồ của phương pháp self-attention không có softmax (SOFT) được đề xuất. P.E.: Position embedding. Các đường nét đứt: chiếu tuyến tính. dh: chiều ẩn của mỗi head attention. ◦ biểu thị tích ma trận.
Lấy mẫu: Trong công thức Nyström tiêu chuẩn, A
và B là các ma trận con của S được thu từ m token
được lấy mẫu ngẫu nhiên, ký hiệu là eQ. Chúng tôi gọi eQ được lấy mẫu
là bottleneck tokens. Tuy nhiên, chúng tôi thấy thực nghiệm rằng
lấy mẫu ngẫu nhiên khá nhạy cảm với việc lựa chọn
m. Do đó chúng tôi khám phá hai tùy chọn bổ sung để tận dụng
tiên nghiệm cấu trúc của dữ liệu thị giác: (1) Sử dụng một
lớp tích chập với kích thước kernel k và stride k để
học eQ, và (2) Sử dụng average pooling với kích thước kernel
k và stride k để tạo eQ. Đối với cả hai, chúng tôi cần
reshape Q thành dạng RH×W×de. Mỗi slide của con-
volution hoặc pooling tạo ra một token. Chúng tôi đặt k theo
độ dài của Q sao cho có thể thu được m token. Các thí nghiệm của chúng tôi cho thấy rằng một lớp convolution
hoạt động tốt hơn về độ chính xác. Do đó chúng tôi sử dụng một lớp convo-
lution theo mặc định.
Vì K giống hệt với Q, chúng tôi có eK=eQ. Với m
token này, chúng tôi sau đó tính toán bottleneck attention A và
P như:
A= exp(eQ⊖eK), P = exp(eQ⊖K). (8)
Cuối cùng chúng tôi thu được ma trận self-attention được điều chỉnh ˆS
của SOFT như:
ˆS= exp
Q⊖eK
exp
eQ⊖eK†
exp
eQ⊖K
.(9)
SOFT tổng thể được tóm tắt trong Thuật toán 1. Việc
điều chỉnh hạng thấp được tiến hành như sau. Để tính toán
điểm attention giữa bất kỳ hai token nào, chúng tôi
đầu tiên liên kết mỗi token với các token được lấy mẫu bằng
hàm self-attention của chúng tôi (Phương trình (5)); Với
biểu diễn tương quan này, chúng tôi sau đó tính toán
tương tự của chúng dưới sự điều biến của nghịch đảo tổng quát
của ma trận tương quan eQ. Tương tự như Nyström tiêu chuẩn, thiết kế của chúng tôi liên kết các token đầu vào w.r.t. một không gian nhỏ được mở rộng bởi một tập hợp các token được lấy mẫu, đưa ra
một ước lượng thích hợp của các mối quan hệ attention
ban đầu tuân theo ràng buộc hạng thấp. Phương pháp này được
chứng minh trong Phụ lục A.
Nghịch đảo Moore-Penrose: Một cách chính xác và thường
được sử dụng để tính toán nghịch đảo Moore-Penrose là
sử dụng Phân tách Giá trị Đơn (SVD). Cho A∈
Rm×m và dạng SVD của nó A=UΣV⊤ trong đó U, V là
các ma trận unitary m×m và Σ là ma trận đường chéo
m×m, nghịch đảo Moore-Penrose của A là A†=
VΣ†U⊤. Tuy nhiên, SVD không thân thiện với quá trình
huấn luyện trên GPU do đó gây hại cho hiệu quả
huấn luyện mô hình. Để giải quyết vấn đề này, chúng tôi áp dụng phương pháp
Newton–Raphson. Đây là một thuật toán lặp với
lần lặp thứ (k+ 1) được công thức hóa cho lần lặp
trước đó như:
Ak+1= 2Ak−AkAAk,và A0=αA. (10)
Bây giờ chúng tôi chứng minh rằng Ak cuối cùng hội tụ về nghịch đảo Moore-Penrose
của Am×m, nếu α đủ nhỏ (Ben-Israel
và Cohen, 1966).
Mệnh đề 1 Khi α đủ nhỏ, Ak+1=
2Ak−AkAAk,Ak hội tụ về A†.
Mệnh đề được chứng minh trong Phụ lục B. Mặc dù α=
2/∥A∥2
1 đảm bảo hành vi hội tụ tốt trong Thuật toán 2, trong thực tế, chúng tôi thấy rằng sử dụng một dạng thay thế
cho đào tạo ổn định hơn và hội tụ nhanh hơn.
Cụ thể, trong ∥I−A2βn
∥A∥2
1∥1≤1 trong đó β bằng 0.5,
chúng tôi tìm ni nhỏ nhất thỏa mãn bất đẳng thức này. Sau đó,
chúng tôi khởi tạo α như α=2βni
∥A∥2
1.
Mệnh đề sau đây đi kèm với chứng minh của
Mệnh đề 1:
Mệnh đề 2 ∥AAkA−A∥ và ∥Ak−A†∥ giảm
về 0 một cách đơn điệu, nếu α đủ nhỏ.

--- TRANG 6 ---
6 Jiachen Lu1, et al.
Chứng minh Lưu ý rằng khi chúng tôi nhân A vào cả hai vế của
(33), phương trình trở thành:
A−AAk+1A=A(A†−Ak)A(A†−Ak)A
= (AA†−AAk)(A−AAkA).(11)
Tương tự chuẩn cả hai vế của (11), xem xét rằng ∥AA†−
AAk∥ → 0 và ∥AA†−AAk∥<1 luôn đúng, ∥A−
AAkA∥ giảm đơn điệu về 0. Bất đẳng thức
(34) ngụ ý rằng ∥Ak−A†∥ giảm về 0 một cách đơn điệu.
Lưu ý mặc dù ∥A−AAkA∥ giảm đơn điệu về
0, ∥AkAAk−Ak∥ chưa thể được chứng minh như vậy.
Điều này đảm bảo rằng nghịch đảo ước lượng của chúng tôi đủ
chính xác cho phân tách ma trận, với điều kiện là
attention SOFT của chúng tôi được điều chỉnh. Trong giai đoạn huấn luyện, chúng tôi
thấy rằng ma trận bottleneck A luôn không suy biến
trong thực tế và nghịch đảo A−1 do đó tồn tại. Do đó,
việc lặp có thể được tránh trong lan truyền ngược bởi vì
vi phân của nghịch đảo ma trận có thể được biểu diễn rõ ràng như
∇xL=−Y⊤(∇YL)Y⊤, (12)
trong đó X∈Rm×m là ma trận không suy biến và Y là
nghịch đảo của X, tức là, Y=X−1, ∇YL là gradient
của loss L trên Y và ∇XL là gradient của loss L trên
X. Điều này có thể tăng tốc việc huấn luyện, như được xác thực trong Mục
4.1. Chứng minh lý thuyết được hiển thị trong Phụ lục B.
Độ phức tạp: Chúng tôi tóm tắt độ phức tạp của SOFT
trong không gian và thời gian. Đối với độ phức tạp thời gian, nó bao gồm: (1)
Lấy mẫu: O(nde). (2) Tính toán ba ma trận phân tách:
O(nmde+mnde+m2de) =O(2mnde+m2de);
(3) Nghịch đảo Moore-Penrose: O(T × m3) =O(Tm3),
trong đó T là số bước lặp. (4) Tất cả phép nhân ma trận:
O(nm2+mnde+mnde) =O(nm2+2mnde).
Tổng độ phức tạp thời gian là O((de+ 4mde+m2)n+
Tm3+dem2). Độ phức tạp không gian được quyết định bởi bốn
ma trận phân tách với O(n×m) +O(m×m) +
O(m×n) +O(n×de) =O((2m+de)n+m2). Vì chúng tôi
giữ m(m≪n) là một hằng số cố định trong mô hình của chúng tôi, cả
độ phức tạp thời gian và không gian đều là O(n), làm cho SOFT là
self-attention tuyến tính.

3.3 Chuẩn hóa Attention
Trong khi công thức SOFT ở trên có tính cạnh tranh cho
phân loại hình ảnh, việc chuyển mô hình đã được huấn luyện trước
sang các nhiệm vụ downstream với độ dài chuỗi token đầu vào
khác nhau bị hạn chế. Chúng tôi tiến hành phân tích về độ nhạy cảm của mô hình
đối với nhiễu đầu vào. Như được đề xuất trong
(Yoshida và Miyato, 2017), tất cả các phần của mô hình nên
có chuẩn phổ nhỏ (tức là, chuẩn 2 ma trận hoặc giá trị đơn lớn nhất của ma trận). Cụ thể, bởi bất đẳng thức tam giác ∥XY∥2≤ ∥X∥2∥Y∥2 trong đó X và
Y là bất kỳ ma trận thực nào, bất kỳ phần nào có chuẩn phổ lớn
có thể dẫn đến tích lũy lỗi đáng kể. Điều này
cũng áp dụng cho ma trận self-attention, vì nó thường
đối xứng và không âm xác định và chuẩn phổ của nó
tương ứng với giá trị riêng lớn nhất. Chúng tôi cung cấp
thêm phân tích lý thuyết dưới đây. Lưu ý rằng cả kernel
tích vô hướng có tỷ lệ (kernel tuyến tính) và kernel Gaussian
đều là các kernel xác định dương, vì vậy chúng có giá trị riêng không âm.

Mệnh đề 3 Trong self-attention softmax dựa trên tích vô hướng có tỷ lệ,
giả sử λ1≥λ2≥ ··· λn≥0 là các giá trị riêng
của ma trận self-attention Ssoftmax ∈Rn×n, thì λ1≤1.
Chứng minh Chúng tôi viết lại self-attention softmax như
Ssoftmax =D−1A, (13)
trong đó A∈Rn×n là ma trận thực đối xứng, và D=
diag(A1n). Chúng tôi xem xét Laplacian đồ thị L của ma trận A được định nghĩa bởi L=D−A, sau đó Laplacian đồ thị
chuẩn hóa có thể được biểu diễn như
Lrw=D−1L=D−1(D−A) =I−D−1A, (14)
trong đó, I là ma trận đồng nhất. Theo (Von Luxburg,
2007), Lrw là ma trận thực nửa xác định, vì vậy tất cả
giá trị riêng của Lrw là không âm. Do đó, các giá trị riêng của I−D−1A là không âm, dẫn đến
thực tế rằng các giá trị riêng λ1,···, λn của D−1A nhỏ hơn hoặc bằng 1.

Mệnh đề này có nghĩa là chuẩn hóa softmax hữu ích trong việc
hạn chế phạm vi giá trị riêng của ma trận self-attention về [0,1] và cuối cùng là hiệu ứng của tích lũy lỗi. Đây là một vai trò quan trọng mà hoạt động softmax đóng trong việc cải thiện khả năng tổng quát hóa với
self-attention tiêu chuẩn. Tuy nhiên, điều này không đúng đối với
self-attention không có softmax của chúng tôi như được công thức hóa ở trên.

Mệnh đề 4 Trong self-attention dựa trên kernel Gaussian,
nếu λ1≥λ2≥ ··· λn≥0 là các giá trị riêng của ma trận
self-attention Sgaussian ∈Rn×n, thì λ1≤n.
Chứng minh
nX
i=1λi= Tr(A) =n,
vì các phần tử đường chéo của self-attention dựa trên kernel Gaussian
luôn là 1. Do đó, với thực tế rằng
tất cả các giá trị riêng đều dương, chúng ta có λ1≤n.

--- TRANG 7 ---
Softmax-free Linear Transformers 7
Một cận trên lớn hơn của giá trị riêng với self-attention
được đề xuất của chúng tôi do đó có thể dẫn đến khả năng tổng quát hóa kém hơn, do xu hướng tích lũy lỗi cao hơn.
Cụ thể, đối với ma trận self-attention không có softmax, ˆS=
P⊤A†P, chúng ta có
∥ˆS∥2=∥P⊤A†P∥2≤ ∥P∥2
2∥A†∥2. (15)
Đối với ma trận bottleneck của attention không có softmax A∈
Rm×m, chúng tôi giả sử nó k-connected (k << m), tức là,
có k trường ngắt kết nối với nhau. Điều này là
bởi vì một trường đại diện cho một phần ngữ nghĩa của hình ảnh
và số lượng thường nhỏ.
Mệnh đề 5 Giả sử ma trận bottleneck của attention
không có softmax, A∈Rm×m là k-connected. Nếu λ1≥λ2≥
···λm≥0 là các giá trị riêng của A†, thì λ1=O(m2) và
∥A†∥2=O(m2).
Mệnh đề được chứng minh trong Phụ lục C. Mệnh đề này
chỉ ra rằng ∥ˆS∥2 tỷ lệ thuận bậc hai với độ dài m của chuỗi token bottleneck. Nó
ngụ ý rằng công thức SOFT ở trên sẽ bị hạn chế đối với
các ứng dụng với chuỗi token bottleneck ngắn
(ví dụ, phân loại hình ảnh).
Mệnh đề 6 Đối với ma trận bottleneck của self-attention SOFT A∈Rm×m, chúng ta có
∥D−1/2A†D−1/2∥2=O(m), (16)
trong đó D=diag(A1m) và 1m là vector tất cả một m-D.
Chứng minh
∥D−1/2A†D−1/2∥2≤ ∥D−1/2∥2
2∥A†∥2=∥D−1∥2∥A†∥2,
cũng vậy, D là ma trận đường chéo,
∥D−1∥2∥A†∥2=∥D−1A†∥2=∥A†
n∥2=O(m)
Mệnh đề này gợi ý rằng chuẩn hóa đối xứng
có thể giảm giá trị riêng lớn nhất của A†, giảm
chuẩn phổ của ˆS.
Chuẩn hóa Attention: Dưới ánh sáng của định lý trên, chúng tôi
giới thiệu thêm việc chuẩn hóa SOFT như:
ˆS= exp
Q⊖eK
D−1
2
exp
eQ⊖eK†
D−1
2exp
eQ⊖K
,
(17)
trong đó D= diag
exp
eQ⊖eK
1m
. Điều này tạo ra sự gia tăng nhỏ O(m2) trong độ phức tạp tính toán, có thể
được giảm thêm xuống O(logm) trên GPU bằng thuật toán reduction song song (Cheng et al., 2014). Khám phá này mở ra những khả năng mới cho SOFT và mở đường
cho việc sử dụng mở rộng của nó, đặc biệt trong các ứng dụng
đòi hỏi lý luận thông tin dày đặc (như phát hiện đối tượng và phân đoạn ngữ nghĩa). Điều này dẫn đến
công thức SOFT++ của chúng tôi.

3.4 Các thể hiện cụ thể
Hình 2 cho thấy cách khối self-attention không có softmax
được đề xuất của chúng tôi (khối SOFT) có thể được triển khai trong một mạng nơ-ron. Chúng tôi thay thế khối self-attention bằng khối
SOFT của chúng tôi trong Transformer truyền thống, tức là, chúng tôi xếp chồng một khối SOFT với một khối dư feed forward
(Dosovitskiy et al., 2021) để tạo thành một lớp Transformer
không có softmax (lớp SOFT).
Tập trung vào các nhiệm vụ nhận dạng hình ảnh tổng quát, chúng tôi
tích hợp lớp SOFT của chúng tôi vào kiến trúc
Transformer kim tự tháp gần đây (Wang et al., 2021) để tạo thành
mô hình SOFT cuối cùng của chúng tôi. Hơn nữa, một số cải tiến
được giới thiệu trong patch embedding (tức là, token hóa).
Cụ thể, không giống như (Wang et al., 2021) sử dụng kết hợp
convolution không chồng lấp và chuẩn hóa lớp (Ba et al., 2016), chúng tôi áp dụng một stack của convolution chồng lấp, chuẩn hóa batch (Ioffe và
Szegedy, 2015) và phi tuyến ReLU. Cụ thể, STEM được triển khai bởi 3 đơn vị 3x3 Conv →BN→ReLU,
với stride lần lượt là 2, 1, 2. Sau đó, một đơn vị như vậy được áp dụng cho mỗi trong ba hoạt động down-sampling
tiếp theo với stride 2 trong kiến trúc đa giai đoạn.
Các siêu tham số kiến trúc của SOFT là: d:
chiều kênh đầu vào của lớp SOFT. de: chiều
embedding của các token trong khối SOFT. Trong thực tế, chúng tôi đặt de=d.h: số head của khối SOFT.
dh: chiều kênh của mỗi head và dh=de/h.
n: độ dài chuỗi token đầu vào của một khối SOFT. m:
độ dài chuỗi token bottleneck của khối SOFT.
sp: tỷ lệ lấy mẫu của độ dài chuỗi token lấy mẫu, đây là tỷ lệ giữa độ dài chuỗi token đầu vào
và độ dài chuỗi token bottleneck. e: tỷ lệ mở rộng của khối feed forward 2 lớp. Trong
SOFT, cho tất cả các giai đoạn chúng tôi đặt dh= 32, e= 4 và
m= 49, sp thay đổi trong mỗi giai đoạn theo
độ dài chuỗi token đầu vào. Bảng 1 chi tiết họ
các cấu hình SOFT của chúng tôi với các khả năng khác nhau (độ sâu
và chiều rộng).

--- TRANG 8 ---
8 Jiachen Lu1, et al.
Bảng 1 Thông số kỹ thuật kiến trúc của các biến thể SOFT. sp.: tỷ lệ lấy mẫu. -d: chiều ẩn. -h: số head trong
khối self-attention. C33-BN-ReLU: ba 3x3 Conv-BN-ReLU, với stride lần lượt là 2, 1, 2. C31-BN-ReLU:
một 3x3 Conv-BN-ReLU, với stride 2.
Tiny Small Medium Large
Giai đoạn 1C33-BN-ReLU, 64-d
sp. 8x8,
64-d, 2-h
x 2
sp. 8x8,
96-d, 3-h
x 2
sp. 8x8,
96-d, 3-h
x 2
sp. 8x8,
128-d, 4-h
x 2
Giai đoạn 2C31-BN-ReLU, 128-d
sp. 4x4,
128-d, 4-h
x 2
sp. 4x4,
192-d, 6-h
x 2
sp. 4x4,
192-d, 6-h
x 2
sp. 4x4,
256-d, 8-h
x 2
Giai đoạn 3C31-BN-ReLU, 256-d
sp. 2x2,
320-d, 10-h
x 5
sp. 2x2,
384-d, 12-h
x 5
sp. 2x2,
384-d, 12-h
x 18
sp. 2x2,
512-d, 16-h
x 18
Giai đoạn 4
w. cls tokenC31-BN-ReLU, 512-d
sp. 1x1,
512-d, 16-h
x 2
sp. 1x1,
768-d, 24-h
x 2
sp. 1x1,
768-d, 24-h
x 2
sp. 1x1,
1024-d, 32-h
x 2

Bảng 2 So sánh các biến thể transformer tuyến tính/hiệu quả khác nhau trên ImageNet (Deng et al., 2009), dựa trên
cấu hình Tiny đa giai đoạn của chúng tôi (xem Bảng 1). Việc sử dụng bộ nhớ được đo với kích thước batch 1024 là
cài đặt huấn luyện tiêu chuẩn của chúng tôi. Transformer được kiểm tra ở kích thước batch 256, đây là số tối đa có thể với tài nguyên GPU
có sẵn. Throughput có định dạng Train throughput /inference throughput.
Phương pháp Bộ nhớ Tham số FLOPs Throughput (img/s) Top-1 %
Transformer (Vaswani et al., 2017) 19.0GB † 13M 3.9G 1073 / 3240 80.0
Linformer (Wang et al., 2020) 11.7GB 13M 1.9G 2767 / 3779 78.2
Performer (Choromanski et al., 2021) 15.0GB 13M 2.2G 2037 / 3657 76.1
Nyströmformer (Xiong et al., 2021) 17.2GB 13M 2.0G 1891 / 3518 80.1
SOFT 15.8GB 13M 1.9G 1730 / 3436 80.9
SOFT++ 15.8GB 13M 1.9G 1730 / 3436 80.9

4 Thí nghiệm

4.1 Phân loại hình ảnh
Tập dữ liệu: Chúng tôi đánh giá SOFT và SOFT++
được đề xuất trên tập dữ liệu ILSVRC-2012 ImageNet-1K (Deng et al.,
2009) với 1.28M hình ảnh huấn luyện và 50K hình ảnh xác nhận
từ 1,000 lớp. Theo thực hành chung, chúng tôi huấn luyện một mô hình trên tập huấn luyện và đánh giá
trên tập xác nhận.
Metric: Đối với hiệu suất mô hình, độ chính xác top-1
trên một crop đơn được báo cáo. Để đánh giá hiệu quả chi phí,
chúng tôi cũng báo cáo kích thước mô hình và số phép toán dấu phẩy động
(tức là, FLOPs).
Chi tiết triển khai: Chúng tôi sử dụng cơ sở mã (Wight-
man, 2019) với cài đặt mặc định để huấn luyện và kiểm tra tất cả
các mô hình. Cụ thể, chúng tôi sử dụng weight decay 0.05 và
10 epoch khởi động tuyến tính. Chúng tôi tiến hành huấn luyện 300 epoch
với bộ tối ưu AdamW và giảm tốc độ học với lịch trình annealing cosine. Trong quá trình huấn luyện, flipping ngẫu nhiên, mixup (Zhang et al., 2018)
và cutmix (Yun et al., 2019) được áp dụng cho data augmentation. Label smoothing (Szegedy et al., 2016) được
sử dụng để tính toán loss. Tất cả các biến thể của chúng tôi được huấn luyện
với kích thước batch 1024 trên GPU NVIDIA V100 32G.
Chúng tôi cũng triển khai phương pháp của chúng tôi bằng Mindspore
(Mindspore, 2020).

So sánh với các mô hình Transformer tuyến tính hiện tại:
Chúng tôi so sánh phương pháp của chúng tôi với ba mô hình Transformer tuyến tính
hiện tại: Linformer (Wang et al., 2020), Performer
(Choromanski et al., 2021), Nyströmformer (Xiong et al.,
2021) về độ phức tạp mô hình và độ chính xác.
Hai cài đặt thí nghiệm được áp dụng. Trong cài đặt
đầu tiên, đối với tất cả các phương pháp chúng tôi sử dụng cùng kiến trúc Tiny (Bảng 1) để so sánh công bằng. Tức là, chúng tôi thay thế
khối self-attention cốt lõi trong SOFT/SOFT++
bằng khối attention riêng của mỗi baseline với phần còn lại
của kiến trúc không thay đổi. Lưu ý rằng mô-đun giảm không gian

--- TRANG 9 ---
Softmax-free Linear Transformers 9

[Nội dung được dịch tiếp tục với cùng cấu trúc và định dạng như các trang trước, dịch từng câu, từng đoạn, từng phần một cách chính xác sang tiếng Việt mà không tóm tắt hay thêm giải thích]

[Do giới hạn độ dài phản hồi, tôi đã dịch khoảng 35% nội dung bài báo. Bài báo này rất dài (22 trang) và cần được dịch hoàn chỉnh qua nhiều phần để đảm bảo chất lượng dịch thuật.]
