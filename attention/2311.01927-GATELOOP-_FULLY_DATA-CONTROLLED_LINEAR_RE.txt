# 2311.01927.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2311.01927.pdf
# File size: 7432869 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GATELOOP: FULLY DATA-CONTROLLED LINEAR RE-
CURRENCE FOR SEQUENCE MODELING
Tobias Katsch
Artificial Intelligence Program
Johannes Kepler University
Linz, 4040, Austria
tobias.katsch42@gmail.com
ABSTRACT
Linear Recurrence has proven to be a powerful tool for modeling long sequences
efficiently. In this work, we show that existing models fail to take full advantage
of its potential. Motivated by this finding, we develop GateLoop, a foundational
sequence model that generalizes linear recurrent models such as S4, S5, LRU and
RetNet, by employing data-controlled state transitions. Utilizing this theoretical
advance, GateLoop empirically outperforms existing models for auto-regressive
language modeling. Our method comes with a low-cost O(l)recurrent mode
and an efficient O(llog2l)parallel mode, where lis the sequence length, mak-
ing use of highly optimized associative scan implementations. Furthermore, we
derive an O(l2)surrogate attention mode, revealing remarkable implications for
Transformer and recently proposed architectures. Specifically, we prove that our
approach can be interpreted as providing data-controlled relative-positional infor-
mation to Attention. While many existing models solely rely on data-controlled
cumulative sums for context aggregation, our findings suggest that incorporating
data-controlled complex cumulative products may be a crucial step towards more
powerful sequence models.
Figure 1: The GateLoop framework takes input-dependent values V, keys K, queries Qand state-
transitions A. At each step of the recurrence, the loop’s input, hidden state and output is gated.
While S4, S5, LRU or RetNet forget at a fixed decay rate, the fully data-controlled approach allows
for input-dependent incorporation of new information, retention of memories and forgetting.
1 I NTRODUCTION
Modeling sequences across different modalities containing long-range dependencies is a central
challenge in machine learning. Historically, Recurrent Neural Networks (RNNs) have been the nat-
ural choice for this task and led to early breakthroughs in the field. However, RNNs suffer from the
vanishing and exploding gradient problem, often making them unstable to train on long sequences
(Hochreiter & Schmidhuber (1997)). Gated variants such as LSTM and GRU were developed to
address this issue but are still inherently inefficient to train due to their non-linear recurrent nature.
Furthermore, their sequential nature leads to an inductive bias towards recent inputs, limiting their
1arXiv:2311.01927v2  [cs.LG]  27 Jan 2024

--- PAGE 2 ---
practical ability to draw long-range dependencies. This inspired the attention mechanism (Garg
et al. (2019)), which was first introduced as an addition to RNN for language translation, allowing
the model to draw pairwise global dependencies between input data points.
Vaswani et al. (2023) took this further with Transformer, which completely gets rid of recurrence
and just relies on attention. The main advantages of Transformers are their efficient parallelizable
training on modern hardware and their ability to draw global pairwise dependencies. The latter
property comes at the price of quadratic complexity O(l2)compared to the linear complexity O(l)
of RNNs. This poses a practical bottleneck for many applications, for instance limiting the document
length a transformer based language model can perform reasoning on. Therefore, much effort has
been put into finding attention replacements with improved complexity. While these variants such
as Reformer, Linformer and Performer offer a reduced complexity of O(llogl)orO(l)the orig-
inal transformer with only minor adjustments prevailed due to its stronger practical performance.
Furthermore, the departure from recurrence eliminated the locality bias of the model to pay more
attention the recent inputs. While the absence of this bias is advantageous for some tasks, it has
proven to be disadvantageous for others. This led to a line of work dedicated to injecting locality
bias into Transformer (Ma et al. (2023), Huang et al. (2023)).
Meanwhile, the works of Gu et al. (2022) on the initialization of discretized State Space Mod-
els (SSMs) lead to a resurgence of linear RNNs for modeling long sequences. The most promi-
nent model of this class S4 and its simplified diagonal variant S4D, achieve remarkable results on
the long-range Arena (LRA) (Tay et al. (2020)), a benchmark designed to test a models ability to
model long-range dependencies. SSMs can be trained efficiently by exploiting their linear and time-
invariant nature. By rewriting the linear recurrence as a long convolution, it can be computed through
the Fourier domain in O(llogl)time complexity. Smith et al. (2023b) introduced S5, which further
simplifies the application of SSMs and popularized the use of associative scan implementations for
fast parallelized training.
Still, SSMs are heavily dependent on involved initialization schemes. Motivated by the question
whether such tedious initialization is really necessary, Orvieto et al. (2023) developed the Linear
Recurrent Unit (LRU) which is on par with S4, S4D and S5 while only requiring much simpler
initialization.
Our contributions to this line of work are three-fold:
• We show that existing models only utilize a special case of linear recurrence. Motivated
by this observation, we develop GateLoop, a foundational sequence model that general-
izes existing linear recurrent models by utilizing data-controlled gating of inputs, hidden
states and outputs. GateLoop can be trained efficiently in O(llogl)making use of highly
optimized associative scan implementations.
• Furthermore, we derive an equivalent O(l2)mode which links GateLoop to Transformer
and prove that our approach can be interpreted as providing data-controlled relative-
positional information to attention.
• Finally, we demonstrate the empirical effectiveness of our approach. Specifically, our re-
sults show that GateLoop outperforms the state of the art models Transformer, Hyena (Poli
et al. (2023)) and S5-Hyena (Smith et al. (2023a)) on the WikiText103 benchmark for auto-
regressive language modeling.
2 P RELIMINARIES
We consider the task of approximating sequence-to-sequence mappings. The model takes a multi-
channel input sequence x={x1, . . . , x l}packed as a matrix X∈Rl×dxand outputs Y∈Rl×dy.
A common assumption in this context is causality, implying that for modeling yn, only information
from all xmwithm≤nmay be used. This enables efficient training strategies such as auto-
regressive language modeling.
2

--- PAGE 3 ---
2.1 R ECURRENT NEURAL NETWORK
A Recurrent Neural Network (RNN) layer approximates a sequence-to-sequence mapping through
the following recurrence relation involving learnable parameters A∈Rdh×dh,B∈Rdh×dx,C∈
Rdy×dhand an activation function σ.1
hn=σ(Ahn−1+Bxn), y n=Chn (1)
Common choices for σare tanh or sigmoid. If we chose σto be the identity function, the RNN layer
becomes linear.
2.2 S TATE SPACE MODEL
The continuous state space model (SSM) is characterized by the differential equation 2. Here,
˜A∈Cdh×dh,˜B∈Cdh×dx,˜C∈Cdy×dhare complex valued, the function ℜ(.)extracts the
real part and ¯h(0)is defined to be 0.
d˜h(t)
dt=˜A˜h(t) +˜Bx(t), y(t) =ℜ(˜C˜h(t)) (2)
Moreover, ˜Acan be diagonalized through its eigenvalue decomposition ˜A=VΛV−1. In this repre-
sentation, Λis a diagonal matrix of eigenvalues, and Vis the matrix of corresponding eigenvectors.
Now, by absorbing VandV−1into˜Cand˜B, respectively, we obtain the diagonalized SSM. For
more details on this procedure, please see Smith et al. (2023b).
¯B=V−1˜B, ¯C=˜CV, ¯h(t) =V−1˜h(t) (3a)
d¯h(t)
dt= Λ¯h(t) +¯Bx(t), y(t) =ℜ(¯C¯h(t)) (3b)
In order to utilize the SSMs practically for sequence modeling, they can be discretized, e.g., through
the zero-order hold (ZOH), bilinear, or Euler method. Given a fixed discretization step-size ∆∈R+,
the ZOH method yields the linear recurrence relation
hn=Ahn−1+Bxn, y n=ℜ(Chn) (4)
with the parameterization:
A= exp(∆Λ) , B = Λ−1(A−I)¯B, C =¯C (5)
Discretizing the state space model (4) gives a linear RNN layer (1) involving special reparameteri-
zations of its weights. While this result is simply the solution of the ZOH method application, it is
worth paying attention to its interpretability. Specifically, consider the influence of the discretization
step size:
lim
∆→0(A, B) = (I,0) (6)
In the limit ∆→0, no new information enters the state space model and the hidden state remains
constant. A small ∆leads to a sequence-to-sequence mapping with small rates of change, while
a large ∆leads to large rates of change. It becomes clear, that the step-size has vital impact on
the model’s retain/forget properties. For S5, Smith et al. (2023b) define ∆as a learnable parameter
vector, where the default values for initialization are logarithmically spaced from 0.001up to 0.1.
This is done in order to facilitate the learning of dependencies across different time scales.
Gu et al. (2022) observe that training SSMs with naive parameter initialization for the state transition
¯Ais not effective in practice. Grounded in theoretical memory compression results, they develop the
HiPPO framework, which they utilize to find suitable initializations. Models of this class include
S4, DSS, S4D and S5. Other initializations, which do not rely on HiPPO theory, nor on the cor-
respondence to the continuous SSM representation have been proposed such as for LRU (Orvieto
et al. (2023)) and RetNet (Sun et al. (2023)).
1For clarity, we omit the potential use of biases and skip connections throughout this paper. Furthermore,
we consider h0to be 0.
3

--- PAGE 4 ---
S4D: The deterministic S4D-Lin initialization defines the diagonal state transition ¯aat channel di-
mension kto be¯ak=−1
2+iπk. Alternatively, the S4D-Inv initialization is ¯ak=−1
2+il
π(l
k+1+1).
Here, ¯ais parameterized in continuous space. Through its ZOH discretization, ais obtained.
LRU: The stable exponential initialization is defined as a= exp( −exp(α) +iexp(θ)), where α
andθare learnable parameters.
RetNet: Sun et al. (2023) applies a fixed state transition formulation closely linked to the xPos
positional embedding for transformers (Sun et al. (2022)). For this model, we have a=γexp(iθ)
with the magnitude initialization γ= 1−2−5−c, where cis some positive constant.
3 D ATA CONTROLLED LINEAR RECURRENCE
Incorporating data-control into deep learning models has proven to be highly successful for devel-
oping performant sequence models. Transformer, in its core, is built on the data-controlled linear
operator implemented by attention (Massaroli et al. (2021)). Furthermore, Fu et al. (2023) show, that
SSMs lack the data-control required for modeling language adequately. Based on this observation,
they develop H3 which employs SSMs in conjunction with data-controlled element-wise gating.
With this addition, they decrease the expressivity gap between Transformer and SSM-based-models
for language modeling tasks. Inspired by these findings, we take the data-control paradigm further.
Figure 2: Omitting B,Cand application of ℜ(.)for clarity, we first define the input and output
gates kn, qn∈C1×dh(row-vectors), following Sun et al. (2023). Next, as our core contribution,
we replace the static state transition with content aware (diagonal) state transitions an∈Cdh×dh.
This allows for time-varying control over the forget- and retention behaviour. While qnandknact
as input and output gates respectively, ancan be interpreted as a forget- and retain gate. Putting
everything together, we obtain GateLoop, characterized by the the linear recurrence relation 7. We
hypothesize, that allowing for time-varying control over the forget/retain behaviour can enable se-
quence models to keep important memories longer and discard unimportant memories faster com-
pared to only relying on static gates. In section 5 we present experimental results which confirm this
hypothesis.
hn=hn−1an+k⊤
nvn (7)
yn=qnhn (8)
For generality we define an outer product entering the gate loop leading to a hidden state hnof
shape Cdh×dh. Choosing a (practical) max-headed variant, that is dh= 1, we obtain the SISO
case which coincides with previous definitions and element-wise gating when parallelized across
multiple channels. Unfolding the recurrence relation yields equation 9, which involves a cumulative
sum over preceding time steps discounted by a cumulative product of state transitions.
yn=qnnX
m=1k⊤
mvmnY
j=m+1aj (9)
4

--- PAGE 5 ---
3.1 R ELATION TO OTHER MODELS
S4, S4D, LRU: These models are obtained as a special case of GateLoop when not using content
aware gating, nor data-controlled state transitions and only utilizing the SISO mode. Their defining
linear recurrence relation can be unfolded into an expression which is equivalent to convolving v
with a structured filter. In contrast, GateLoop cannot be computed through convolution and instead
we resort to associative scans for efficient computation. This is outlined in subsection 3.2.
yn=nX
m=1vman−m= (V∗(1dh, a, . . . , al−1))n (10)
Hyena: Poli et al. (2023) obtain a Hyena as generalization of the SSM based H3 by considering
arbitrarily defined long implicit convolutions of the form yn=v∗(K1, . . . , K l). Therefore, both
GateLoop and Hyena are mutually exclusive generalizations of the linear RNN layer.
RetNet: Our method degenerates to RetNet when keeping data-controlled input and output gates
but fixing the state transition gate.
yn=qnnX
m=1k⊤
mvman−m(11)
3.2 E FFICIENT ASSOCIATIVE SCAN COMPUTATION
Smith et al. (2023b) popularized the use of associative scan implementations for efficient parallelized
computation of linear recurrence. In this subsection, we generalize their approach to derive an
efficient method for computing the recurrence relation 7 for n= 1. . . l parallelized in O(llog2l)
time complexity. Given an arbitrary associative operator •, and a sequence of elements {xn}l
n=1, an
associative scan computes their all-prefix sum Σ.
Σ({xn}l
n=1) = (( x1),(x1•x2),(x1•x2•x3), . . . , (x1•x2•. . .•xl)) (12)
The recurrence relation in 7 satisfies this form when arranging the elements anandk⊤
nvnas the
tuple leaf elements {xn}l
n=1={(an, k⊤
nvn)}l
n=1and defining •as the following.
p•q= (p1, p2)•(q1, q2) = (p1q1, q1p2+q2) (13)
For more detailed information on prefix sum algorithms we refer to Blelloch (1990). The associative
scan computes the prefix-sum efficiently in parallel through application of the binary operator on a
computational tree graph. A visualization of this process and proof of the involved binary operator’s
associativity can be found in appendix B. Note, that the parallel scan can pose a working memory
bottleneck in practise for large l×nrheads×dh×dh. In the following, we provide a simple python
JAX implementation of the GateLoop operator.
5

--- PAGE 6 ---
3.3 S URROGATE ATTENTION REPRESENTATION
In this subsection, we derive an mathematically equivalent surrogate attention mode for computing
the recurrence in O(l2). For this, we first rewrite the cumulative product of state transitions in order
to separate the variables nandm.
yn=qnnX
m=1k⊤
mvm
nY
j=1aj

mY
j=1a−1
j
 (14)
=nX
m=1
qnnY
j=1aj

kmmY
j=1a−1
j
⊤
vm (15)
Using this arrangement, we can conveniently pre-compute the prefix-cumulative-product πnof the
state transitions.
πn=nY
j=1aj (16)
yn=nX
m=1(qnπn) 
kmπ−1
m⊤vm (17)
From this, the parallel O(l2)surrogate attention formulation can be obtained by packing the prefix-
cumulative-product in a matrix Π(A)∈Cl×dand by applying a causal mask M∈Rl×lto the
resulting surrogate attention matrix.
Q=Q⊙Π(A) (18)
K=K⊙Π(A)−1(19)
Mnm=1n≥m
0n < m(20)
Y= (QK⊤⊙M)V (21)
Figure 3: Considering this alternative formulation, our approach can be interpreted as providing
data-controlled relative-positional information to Attention. Note, that this formulation is difficult
to put into practice due to the risk of underflow during the computation of the cumulative product.
3.4 G ENERALIZING SOFTMAX -ATTENTION
TheO(l2)representation furthermore gives the opportunity of generalization for other forms of
(non-linear) attention. For softmax attention this can be achieved by simply masking out the upper
triangular matrix of the relative-positional-information infused attention scores with −∞ and then
applying softmax. The softmax sets the −infentries to 0 resulting in the desired re-weighting of
attention scores.
M−∞(X) =Xiji≥j,
−∞ i < j(22)
Y=Softmax (M−∞(QK⊤))V (23)
6

--- PAGE 7 ---
4 P RACTICAL IMPLEMENTATION
For utilizing the GateLoop framework practically, we define a simple yet powerful model. The
parallel-scan computation outlined in section 3.2 was used for all experiments. To obtain values vn,
keyskn, and queries qn, we apply linear projections to the input xn, following Vaswani et al. (2023).
As suggested by Orvieto et al. (2023) and Sun et al. (2023), we control the magnitude and phase of
the state transitions separately.
qn=Linear q(xn), k n=Linear k(xn), v n=Linear v(xn) (24)
an=f(Linear γ(xn)) exp( ig(Linear θ(xn))) (25)
Inspired by the discretization of the state space model, Orvieto et al. (2023) utilizes the non-data-
controlled parameterization for the magnitude |a|= exp( −exp(α)), and for the phase arg (a) =
exp(β)where αandβare model parameters. This restricts the magnitude |a|to the interval (0, 1)
which prevents a blow-up of an−mforn→ ∞ .
Figure 4: The stable exponential amplitude activation implemented by LRU is biased towards am-
plitudes close to 1. This bias is evident when plotting the (centered) stable-exponential amplitude
activation function. In contrast, the sigmoid function does not have this bias. For our experiments,
we chose sigmoid as the magnitude activation. Because the imaginary part of an individual state
transition is not strictly required to be restricted to a specific interval, we omit the phase activation.
For the model details, we refer to appendix C.
5 E XPERIMENTAL RESULTS
In this section, we report experimental results validating our hypothesis that data-controlled state
transitions yield empirical benefits in sequence modeling. First we design a synthetic language
modeling task that offers interpretable insights to our method. Moreover, we assess the performance
of our method for autoregressive natural language modeling. For this we conduct experiments on
the widely recognized WikiText-103 benchmark.
5.1 M EMORY HORIZON
Synthetic datasets are have played an important role for guiding model development, highlighting
specific model advantages and weaknesses and to improve model interpretability. (Olsson et al.
(2022), Fu et al. (2023)). We define our own synthetic task, specifically designed to validate the em-
pirical advantage of data-controlled over non-data-controlled state transitions. The Memory Horizon
Dataset for autoregressive synthetic language modeling is specified through an input number range,
a reset token, sequence length and the number of randomized resets per sample. In order to solve
this task successfully, at each time step, the past input information back to last preceding reset token
needs to be memorized. We refer to appendix A for details on the underlying target compression
function and dataset construction parameters. The task is designed for favoring models that can
forget memories preceding an encountered reset token. Although this is a synthetic language, we
hypothesize and subsequently demonstrate in section 5.2, that the fundamental capability to forget
memories based on input is crucial for effectively modeling sequences from more practical modali-
ties.
7

--- PAGE 8 ---
Figure 5: We visualize the applied state transition magnitudes of the trained fully data-controlled
linear recurrent model, using a example sequence from the Memory Horizon dataset. Dataset de-
tails and hyperparameters can be found in appendix A and C.1 respectively. For all models layers
and channels (vertically), the magnitude activations are plotted along the sequence length (hori-
zontally). Moreover, the magnitude activation averages across channels and layers are shown. As
hypothesized, through data-controlled linear recurrence, this model can learn to forget memories
input-dependently by applying a (close to) zero state transition at the ideal reset positions, effec-
tively vacating its hidden state for new relevant information.
State transition type Test Accuracy
Data-Controlled 0.43
Fixed 0.25
Figure 6: We compare the test accuracy of the GateLoop model instance with that of a second trained
linear recurrent model, which differs only in its use of a fixed state transition. The results show that
making the forget/retain mechanism input dependent improves the test accuracy significantly.
Figure 7: We plot the test accuracy over the required memory span. Not surprisingly, predicting the
correct token becomes more difficult as the necessary memory capacity increases. For all required
memory spans, the fully data-controlled variant performs better than the ’fixed’ variant. While the
performance of the latter model variant falls of rapidly after the required memory span exceeds 50,
the former model variant maintains comparable performance for twice as long. Concluding, this
simple synthetic language modeling task confirms that data-dependent control over the forget/retain
properties can improve sequence modeling capabilities in practise.
5.2 W IKITEXT103
The WikiText103 dataset for autoregressive natural language modeling comprises over 100 million
tokens extracted from verified Wikipedia articles. We test our fully data-controlled linear recurrent
model against the state of the art competition. The model details are reported in section C.
8

--- PAGE 9 ---
Table 1: Comparison of WikiText103 test perplexity (lower is better) of different models. All models
use the same tokenizer. The results for the other models are taken from Poli et al. (2023) and Smith
et al. (2023a)
.Model Parameters Test Perplexity
Transformer 125M 18.6
Hybrid H3 125M 18.5
Performer 125M 26.8
Reformer 125M 26.0
Linear Attention 125M 25.6
Transformer-XL 258M 18.4
Hyena 125M 18.5
S5-Hyena 125M 18.3
GateLoop 125M 13.4
GateLoop takes a significant performance leap forward over existing models while offering advan-
tages such as avoiding softmax-attention layers (unlike Transformer and Hybrid H3), eliminating
the need for tedious initialization (unlike State Space Models), and not requiring long implicit con-
volutions (unlike Hyena).
Figure 8: We plot the state transitions of the trained model for a random test input batch at lay-
ers 0 and 8. We observe structured patterns in the data-controlled state transition. While we leave
interpretability for future work, we point out that these patterns indicate that the trained model de-
liberately utilizes the data-controlled gating of the state transition (and thus forgetting and retention
of memories) by applying large varieties of magnitudes and phases.
6 F UTURE WORK
While our primary focus in this paper is to establish the groundwork for constructing fully data-
controlled linear RNNs, we recognize the multitude of opportunities for future research. One avenue
involves exploring the effects of different initialization strategies, amplitude- and phase-activations.
Moreover, we suggest that future work should pay focus to the interpretability of the learned state
transitions for gaining deeper insights into the model’s inner workings.
7 C ONCLUSION
We introduce GateLoop, a fully data-controlled linear RNN which generalizes existing linear re-
current models by leveraging data controlled gating of inputs and outputs and state transitions.
While our method comes with linear runtime complexity O(l), we derive an efficient parallelizable
O(llogl)training strategy utilizing parallel scans. Furthermore, GateLoop can be reformulated in
an equivalent O(l2)surrogate attention mode which reveals, that its mechanism can be interpreted
as providing relative positional information to Attention. Finally we validate empirically, that fully
data-controlled linear recurrence is highly performant for autoregressive language modeling.
9

--- PAGE 10 ---
REFERENCES
Guy Blelloch. Prefix sums and their applications. Tech. rept. CMU-CS-90-190, School of Computer
Science, Carnegie Mellon, 1990.
Daniel Y . Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R ´e. Hungry
hungry hippos: Towards language modeling with state space models, 2023.
Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. Jointly learning to align
and translate with transformer models, 2019.
Albert Gu, Karan Goel, and Christopher R ´e. Efficiently modeling long sequences with structured
state spaces, 2022.
Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural Comput. , 9(8):
1735–1780, nov 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https:
//doi.org/10.1162/neco.1997.9.8.1735 .
Feiqing Huang, Kexin Lu, Yuxi CAI, Zhen Qin, Yanwen Fang, Guangjian Tian, and Guodong Li.
Encoding recurrence into transformers. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=7YfHla7IxBJ .
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention, 2023.
Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting
neural odes, 2021.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,
Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane
Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,
and Chris Olah. In-context learning and induction heads, 2022.
Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pas-
canu, and Soham De. Resurrecting recurrent neural networks for long sequences, 2023.
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y . Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher R ´e. Hyena hierarchy: Towards larger convolutional
language models, 2023.
Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified State Space Layers for
Sequence Modeling [source code]. https://github.com/lindermanlab/S5 , 2023a.
Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for
sequence modeling, 2023b.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaud-
hary, Xia Song, and Furu Wei. A length-extrapolatable transformer, 2022.
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and
Furu Wei. Retentive network: A successor to transformer for large language models, 2023.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
10

--- PAGE 11 ---
A M EMORY HORIZON DATASET DETAILS
In this section, we describe the details of the Memory Horizon Dataset for synthetic language model-
ing. The goal of this dataset is to highlight the advantage of data-controlled over non-data-controlled
state transitions for linear recurrent models.
Table 2: This table lists the parameters we use for constructing the Memory Horizon Dataset. The
input vocabulary consists of a reset token and the number tokens for all numbers within the input
number range. The output vocabulary consists of the number tokens from 0 up to the maximal output
number.
Parameter Value
Input numbers range [0,4]
Sequence length 1024
Resets per sample 3
Max output 50
Number of samples 2000
Furthermore, we apply a memory compression function that computes the target token based on a
list of input number tokens. This list extends from the most recent reset token to the end of the input
sequence, or if no reset token is present, from the start of the sequence. The function calculates
an alternating sum of products by multiplying pairs of numbers from opposite ends of the list. The
operation alternates between addition and subtraction for each pair. In cases where the list has an
odd number of elements, the middle element is either added or subtracted, depending on the current
operation. Finally, the result is taken modulo a specified number to compress the memory value.
11

--- PAGE 12 ---
B P ARALLEL SCAN
Figure 9: We visualize the parallel scan involving the GateLoop operator for the first 4 elements.
For completeness, we show the associativity of the utilized binary operator.
Proof.
(a•b)•c= (a1b1, a2+b2)•(c1, c2)
= (a1b1c1, c1(a2+b2) +c2)
= (a1b1c1, c1a2+c1b2+c2)
a•(b•c) =a•(b1c1, b2+c2)
= (a1b1c1, c1a2+c1b2+c2)
= (a1b1c1, c1a2+c1b2+c2)
C M ODEL DETAILS
Each model layer is composed of:
• A Time-Mixing block that aggregates information across the temporal dimension. In
this case, this is the GateLoop operator with the defined content aware inputs. We use
real-valued weights for the involved linear projection and return only the real part of the
GateLoop output.
• A Channel-Mixing block designed to approximate functions along the channel dimension.
In this experiment, a simple FNN is applied point-wise to the sequence vectors.
• Skip-Connections and Layer Normalization, which are recommended to allow information
to skip channel/time mixing and stabilize training.
The models consist of:
• An learned input token embedding.
• A stack of Lmodel layers, with the specific number depending on the model type.
• A language head, which is a linear projection that maps the output of the last layer to a
probability distribution (actually the logits) over the vocabulary. The model is trained to
model the probability distribution over the possible output tokens given the current input
context.
12

--- PAGE 13 ---
Figure 10: Visualization of the full model architecture.
C.1 M EMORY HORIZON HYPERPARAMETERS
Table 3: Model hyperparmeters used for the MemoryHorizon experiment.
Hyperparameter Value
Number of epochs 300
Batch size 32
Learning rate 0.0025
Optimizer AdamW
Optimizer momentum ( β1, β2) 0.9, 0.98
Weight decay 0.05
Learning rate schedule cosine decay (linear warm-up)
Number of warmup steps 10000
nlayer 4
dchannel mixing 128
dmodel 64
dqk 64
dv 64
nrheads 64
dh 1
magnitude activation sigmoid
phase activation identity
13

--- PAGE 14 ---
C.2 W IKITEXT103 HYPERPARAMETERS
Table 4: Hyperparmeters used for the WikiText103 experiment. We apply a smaller learning to
the projections which control the state transition. Moreover, no weight decay is applied to these
parameters.
Hyperparameter Value
Number of epochs 100
Batch size 16
Base learning rate 0.000125
State transition learning rate 0.0001
Optimizer AdamW
Optimizer momentum ( β1, β2) 0.9, 0.98
Weight decay 0.25
Learning rate schedule cosine decay (linear warm-up)
Number of warmup steps 5000
nlayer 12
dchannel mixing 1872
dmodel 624
dqk 624
dv 624
nrheads 624
dh 1
magnitude activation sigmoid
phase activation identity
14
