Tài liệu này quá dài để dịch hoàn toàn trong một phản hồi. Đây là một báo cáo kỹ thuật 22 trang về TransNormerLLM. Tôi có thể dịch từng phần một cách tuần tự.

Hãy để tôi bắt đầu với phần đầu:

**Báo cáo Kỹ thuật**

**TRANS NORMER LLM: MỘT MÔ HÌNH NGÔN NGỮ LỚN NHANH HƠN VÀ TỐT HƠN VỚI TRANS NORMER CẢI TIẾN**

Zhen Qin♯, Dong Li♯, Weigao Sun♯, Weixuan Sun♯, Xuyang Shen♯,
Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, Yiran Zhong∗

OpenNLPLab, Shanghai AI Laboratory
https://github.com/OpenNLPLab/TransnormerLLM

**TÓM TẮT**

Chúng tôi trình bày TransNormerLLM, Mô hình Ngôn ngữ Lớn (LLM) đầu tiên dựa trên linear attention vượt trội hơn các mô hình dựa trên softmax attention truyền thống về cả độ chính xác và hiệu quả. TransNormerLLM phát triển từ kiến trúc linear attention trước đó TransNormer (Qin et al., 2022a) bằng cách thực hiện các cải tiến tiên tiến bao gồm positional embedding, tăng tốc linear attention, cơ chế gating, chuẩn hóa tensor, và tăng tốc cùng ổn định suy luận. Cụ thể, chúng tôi sử dụng LRPE (Qin et al., 2023b) cùng với exponential decay để tránh các vấn đề dilution attention trong khi cho phép mô hình giữ lại tương tác toàn cục giữa các token. Ngoài ra, chúng tôi đề xuất Lightning Attention, một kỹ thuật tiên tiến tăng tốc linear attention hơn hai lần trong thời gian chạy và giảm sử dụng bộ nhớ đáng kể bốn lần. Để nâng cao hơn nữa hiệu suất của TransNormer, chúng tôi tận dụng cơ chế gating để làm mượt quá trình huấn luyện và một lược đồ chuẩn hóa tensor mới để tăng tốc mô hình, dẫn đến tăng tốc ấn tượng hơn 20%. Hơn nữa, chúng tôi phát triển một thuật toán suy luận mạnh mẽ đảm bảo tính ổn định số học và tốc độ suy luận nhất quán, bất kể độ dài chuỗi, thể hiện hiệu quả vượt trội trong cả giai đoạn huấn luyện và suy luận. Chúng tôi cũng triển khai một lược đồ song song mô hình hiệu quả cho TransNormerLLM, cho phép triển khai liền mạch trên các cụm quy mô lớn và tạo điều kiện mở rộng đến các mô hình rộng lớn hơn, tức là LLM với 175B tham số. Chúng tôi xác thực thiết kế mô hình thông qua một loạt các ablation và huấn luyện các mô hình với kích thước 385M, 1B, và 7B trên corpus tự thu thập của chúng tôi. Kết quả benchmark cho thấy các mô hình của chúng tôi không chỉ phù hợp với hiệu suất của các LLM tiên tiến với Transformer mà còn nhanh hơn đáng kể.

Bạn có muốn tôi tiếp tục dịch phần tiếp theo không?
