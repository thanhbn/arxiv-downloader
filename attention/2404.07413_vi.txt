# JetMoE
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2404.07413.pdf
# File size: 1052849 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
JetMoE
JetMoE: Đạt được hiệu suất Llama2 với 0.1 triệu đô la
Yikang Shen∗
MIT-IBM Watson AI Lab
yikang.shn@gmail.comZhen Guo∗
MIT EECS
zguo0525@mit.edu
Tianle Cai
Princeton University
tianle.cai@princeton.eduZengyi Qin
MyShell.ai & MIT
qinzy@mit.edu
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLM) đã đạt được kết quả đáng chú ý, nhưng
nhu cầu tài nguyên ngày càng tăng của chúng đã trở thành một trở ngại lớn đối với việc
phát triển trí tuệ siêu nhân mạnh mẽ và dễ tiếp cận. Báo cáo này
giới thiệu JetMoE-8B, một LLM mới được đào tạo với chi phí ít hơn 0.1 triệu đô la, sử dụng
1.25T token từ các kho dữ liệu mã nguồn mở được trộn cẩn thận và 30,000 giờ GPU H100. Mặc dù chi phí thấp, JetMoE-8B thể hiện hiệu suất ấn tượng, với JetMoE-8B vượt trội hơn mô hình Llama2-7B và
JetMoE-8B-Chat vượt qua mô hình Llama2-13B-Chat. Những kết quả này
cho thấy rằng việc đào tạo LLM có thể hiệu quả về chi phí hơn nhiều so với những gì
thường được nghĩ. JetMoE-8B dựa trên kiến trúc Hỗn hợp Thưa
Chuyên gia Cổng (SMoE) hiệu quả, bao gồm các chuyên gia attention và feedforward. Cả hai lớp đều được kích hoạt thưa, cho phép JetMoE-8B có
8B tham số trong khi chỉ kích hoạt 2B cho mỗi token đầu vào, giảm
tính toán suy luận khoảng 70% so với Llama2-7B. Hơn nữa,
JetMoE-8B rất mở và thân thiện với học thuật, chỉ sử dụng các bộ dữ liệu công khai
và mã đào tạo. Tất cả các tham số đào tạo và hỗn hợp dữ liệu đã được
chi tiết trong báo cáo này để tạo điều kiện cho các nỗ lực tương lai trong việc phát triển các mô
hình nền tảng mở. Tính minh bạch này nhằm khuyến khích hợp tác và
những tiến bộ hơn nữa trong lĩnh vực LLM có thể tiếp cận và hiệu quả. Các mô
hình được công khai tại https://github.com/myshell-ai/JetMoE .
1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) đã đạt được kết quả đáng chú ý, nhưng nhu cầu tài nguyên
ngày càng tăng của chúng đã trở thành một trở ngại lớn đối với việc phát triển AI mạnh mẽ và dễ tiếp cận.
Mặc dù các LLM hiện đại đã vượt qua hiệu suất của con người trong một số nhiệm vụ, chúng vẫn
không hiệu quả và thiếu linh hoạt. Hầu hết các LLM (ví dụ: Llama, Touvron et al. 2023; Pythia, Biderman
et al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) sử dụng tất cả các tham số
của chúng trong quá trình suy luận và đào tạo, được gọi là mô hình dày đặc. Xem xét
chi phí đáng kể, kiến trúc Hỗn hợp Chuyên gia (MoE) (Yuksel et al., 2012; Shazeer
et al., 2017; Du et al., 2022; Pan et al., 2024) đã nổi lên như một giải pháp phổ biến, cho phép
mở rộng tham số trong khi giữ chi phí tính toán khiêm tốn. Các ứng dụng gần đây của kiến trúc MoE
trong Transformers (Vaswani et al., 2017) đã mang lại những nỗ lực thành công trong việc
mở rộng các mô hình ngôn ngữ đến kích thước đáng kể, kèm theo hiệu suất đáng chú ý,
như Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,
2024), và DBRX (Databricks, 2024). Tuy nhiên, mặc dù những mô hình này đạt được hiệu suất
xuất sắc, chúng không thực sự mã nguồn mở vì các công thức đào tạo không được công bố và
có thể chứa các bộ dữ liệu độc quyền không thể tiếp cận bên ngoài các tập đoàn lớn. Cộng đồng mã nguồn mở
cũng đã cố gắng đào tạo các mô hình MoE, như OpenMoE (Xue et al., 2024), nhưng
hiệu suất của nó chỉ ngang bằng với các mô hình dày đặc yếu có tham số kích hoạt tương tự,
như OpenLLaMA (Geng & Liu, 2023) và TinyLLaMA (Zhang et al., 2024a).
∗Đóng góp ngang nhau.
1arXiv:2404.07413v1  [cs.CL]  11 Apr 2024

--- TRANG 2 ---
JetMoE
Hình 1: Kiến trúc JetMoE
Để tạo điều kiện cho các nỗ lực tương lai về các mô hình nền tảng mở, đặc biệt là các mô hình MoE, chúng tôi giới
thiệu JetMoE-8B, một kiến trúc MoE sáng tạo được lấy cảm hứng từ ModuleFormer (Shen et al.,
2023) mở rộng khái niệm kích hoạt thưa cho cả lớp attention và feed-forward.
Không giống như các công trình trước đây chỉ áp dụng kích hoạt thưa cho lớp feed-forward,
JetMoE-8B tận dụng kích hoạt thưa trong cả hai thành phần để giảm thêm chi phí tính toán
trong khi duy trì hiệu suất.
Đáng ấn tượng, JetMoE-8B được đào tạo với ngân sách hạn chế $100k, sử dụng 1.25T token từ
các bộ dữ liệu mã nguồn mở hỗn hợp và 30,000 giờ GPU H100. Mặc dù chi phí thấp, JetMoE-8B
vượt trội hơn mô hình Llama2-7B, và JetMoE-8B-Chat vượt qua mô hình Llama2-13B-Chat,
chứng minh rằng việc đào tạo LLM có thể hiệu quả về chi phí hơn nhiều so với những gì thường được nghĩ. Ngoài ra, JetMoE-8B có 8B tham số trong khi chỉ kích hoạt 2B cho mỗi token đầu vào, giảm tính toán suy luận khoảng 70% so với Llama2-7B.
Các lợi thế chính của JetMoE-8B bao gồm:
•Tính mở và thân thiện với học thuật : JetMoE-8B được đào tạo chỉ sử dụng các bộ dữ liệu công khai và
mã đào tạo mã nguồn mở, làm cho nó có thể tiếp cận được với nhiều cài đặt nghiên cứu học thuật.
Mô hình cũng có thể được tinh chỉnh với ngân sách tính toán hạn chế (ví dụ: GPU cấp người tiêu dùng).
•Kích hoạt thưa trên cả lớp attention và feed-forward , giảm đáng kể
chi phí đào tạo và suy luận. Chúng tôi cũng đề xuất chia sẻ phép chiếu kv trong
các chuyên gia attention để cải thiện tính ổn định đào tạo.
•Hỗn hợp dữ liệu mã nguồn mở toàn diện , đảm bảo đào tạo chất lượng cao chỉ sử dụng
các bộ dữ liệu mã nguồn mở.
Những đổi mới này trong JetMoE-8B mở đường cho các LLM dễ tiếp cận và hiệu quả hơn, có lợi
cho cộng đồng nghiên cứu AI rộng lớn hơn. Để thúc đẩy hợp tác và những tiến bộ hơn nữa,
chúng tôi đã chi tiết tất cả các tham số đào tạo và hỗn hợp dữ liệu trong báo cáo này.
2 Kiến trúc Mô hình
2.1 Hỗn hợp Chuyên gia
Một lớp Hỗn hợp Chuyên gia (MoE) bao gồm N mô-đun f1,. . .,fN và một bộ định tuyến g(e|x).
Cho một đầu vào x vào lớp MoE, bộ định tuyến dự đoán một phân phối xác suất trên N
2

--- TRANG 3 ---
JetMoE
mô-đun. Trong số này, chúng ta chọn top k chuyên gia. Khi k<N, chúng ta đang sử dụng Hỗn hợp Thưa
Chuyên gia (SMoE, Shazeer et al. 2017). Trong JetMoE này, chúng tôi sử dụng một lớp tuyến tính để mô hình hóa
bộ định tuyến
s=Wrtrx, (1)
g(e|x) =
softmax (Topk(s))i,si∈Topk(s)
0, si/∈Topk(s)(2)
trong đó Wrtrlà ma trận nhúng chuyên gia có hình dạng (N,Demb),Topklà toán tử
chọn top k logit từ s. Đầu ra cuối cùng của SMoE sau đó được cho bởi
y=N
∑
e=1g(e|x)·fe(x) (3)
Khi g(e|x) = 0,fe(x)sẽ không cần được đánh giá, do đó giảm chi phí tính toán
trong quá trình đào tạo và suy luận.
Theo thiết kế trong ModuleFormer (Shen et al., 2023), JetMoE thay thế cả lớp self-
attention và Feed-forward (FFD) bằng lớp SMoE. Điều này khác với hầu hết
các mô hình MoE mã nguồn mở (Dai et al., 2024; Xue et al., 2024), chỉ thay thế các lớp FFD.
2.2 Chuyên gia FeedFoward
Mỗi chuyên gia FFD là một MLP tiêu chuẩn 2 lớp với kích thước trạng thái ẩn Dffd:
fmlp(x) =Woutσ(Winx) (4)
Trong đó Woutlà ma trận chiếu đầu ra có hình dạng (Demb,Df f d),Winlà ma trận chiếu đầu vào
có hình dạng (2Df f d,Demb),σlà hàm kích hoạt SwiGLU.
2.3 Chuyên gia Attention
Zhang et al. (2022) đề xuất Hỗn hợp Đầu Attention (MoA), mở rộng SMOEs cho
các cơ chế attention. Chúng tôi điều chỉnh MoA cho mục đích của chúng tôi, tổng quát hóa nó để cho phép nhiều
đầu trên mỗi chuyên gia và giới thiệu định vị tương đối RoPE vào tính toán attention.
Trong JetMoE, mỗi chuyên gia attention e được tạo thành từ bốn ma trận RDemb×Datt: We
q,Wk,Wv,We
o,
trong đó Datt=H×Dhead,H là số đầu attention bên trong mỗi chuyên gia attention,
Dheadlà chiều của mỗi đầu attention. Trong số các ma trận này, We
q và We
o được
sở hữu bởi mỗi chuyên gia, nhưng Wk và Wv được chia sẻ giữa các chuyên gia để cải thiện
hiệu quả đào tạo và suy luận.
Cho một chuỗi vector đầu vào x, trước tiên chúng ta chiếu nó thành các vector khóa k và các vector giá trị v
sử dụng các ma trận chiếu khóa và giá trị được chia sẻ:
k=Wkx (5)
v=Wvx (6)
Bên trong chuyên gia e, chúng ta chiếu x thành các vector truy vấn qe, áp dụng attention đa đầu tiêu chuẩn
với RoPE (Su et al., 2024), và chiếu đầu ra attention trở lại không gian đầu vào:
qe=We
qx (7)
ae=MHA (qe,k,v) (8)
oe=We
oa (9)
Bằng cách giới thiệu MoA, chúng ta có thể mở rộng lớp attention với nhiều chuyên gia attention
trong khi duy trì cùng một lượng tính toán. Như vậy lớp attention sẽ không
trở thành một nút thắt cổ chai hiệu suất, trong khi chúng ta mở rộng các lớp MLP.
3

--- TRANG 4 ---
JetMoE
2.4 Cân bằng Tải trong quá trình Tiền huấn luyện
Để tránh SMoE liên tục sử dụng cùng một mô-đun và lãng phí khả năng bổ sung trong
các mô-đun khác, nó đòi hỏi các mất mát cân bằng tải khác nhau để điều chỉnh việc đào tạo
bộ định tuyến (Shazeer et al., 2017; Fedus et al., 2021). Trong việc đào tạo JetMoE, chúng tôi sử dụng
mất mát phụ trợ dựa trên tần suất được giới thiệu trong Fedus et al. (2021)
loss b=NN
∑
i=1fiPi (10)
trong đó N là số chuyên gia, fi là phần token được gửi đến chuyên gia i, và Pi là
phần xác suất bộ định tuyến được phân bổ cho chuyên gia i. Để cải thiện tính ổn định đào tạo,
chúng tôi cũng sử dụng z-loss bộ định tuyến được giới thiệu trong Zoph et al. (2022):
loss z=1
BB
∑
i=1 
logN
∑
j=1exp(xi
j)!2
(11)
trong đó B là số token, x là các logit được đưa ra bởi bộ định tuyến. Mất mát đào tạo cuối cùng sẽ
là tổng có trọng số của ba mất mát:
loss=loss lm+αloss b+βloss z (12)
trong đó α là trọng số cho mất mát cân bằng tải và β là trọng số cho z-loss.
3 Bộ dữ liệu Tiền huấn luyện
3.1 Bộ dữ liệu Thế giới thực
RefinedWeb là một bộ dữ liệu web chất lượng cao, chứa 5 nghìn tỷ token được trích xuất từ
CommonCrawl1 sử dụng đường ống Tinh chế Dữ liệu Vĩ mô (MDR) để cải thiện chất lượng dữ
liệu (Penedo et al., 2023). Chúng tôi sử dụng phần trích xuất 600 tỷ token của RefinedWeb có sẵn
công khai.
Dữ liệu đào tạo StarCoder có nguồn từ The Stack v1.2 với mã từ GitHub trải rộng
86 ngôn ngữ lập trình (Li et al., 2023b). Dữ liệu được tiền xử lý thông qua kiểm tra trực quan,
lọc, khử trùng lặp và tái cân bằng các ngôn ngữ dữ liệu thấp. Một phiên bản mới của
bộ dữ liệu đã được phát hành gần đây (Lozhkov et al., 2024).
Dolma là một kho dữ liệu văn bản tiếng Anh lớn, mở, đa dạng chứa 3 nghìn tỷ token được lấy mẫu từ
7 nguồn, bao gồm các trang web từ Common Crawl, mã từ The Stack, dữ liệu web được tuyển chọn
từ C4 (Raffel et al., 2020), các cuộc trò chuyện truyền thông xã hội từ Reddit, các bài báo học thuật
từ PeS2o, sách miền công cộng từ Project Gutenberg, và nội dung bách khoa toàn thư từ
Wikipedia và Wikibooks (Soldaini et al., 2024).
The Pile là một kho dữ liệu văn bản tiếng Anh mã nguồn mở 825 GB để đào tạo các mô hình ngôn ngữ
lớn (Gao et al., 2020). Nó bao gồm 22 bộ dữ liệu đa dạng, có sẵn công khai như Wikipedia,
NIH exPORTER, ArXiv, Books3, BookCorpus2, OpenSubtitles, YTSubtitles, và Enron
Emails.
3.1.1 Khác
◦Proof-Pile-2 là một bộ dữ liệu 55 tỷ token về các tài liệu toán học và khoa học
(Azerbayev et al., 2023). Chúng tôi sử dụng tập con algebraic-stack (11B token) bao
gồm tính toán số, đại số máy tính, và toán học hình thức.
◦OpenWebMath là một bộ dữ liệu mở lớn, chất lượng cao, chứa 14.7 tỷ token văn bản web
toán học tiếng Anh (Paster et al., 2023).
1http://commoncrawl.org/
4

--- TRANG 5 ---
JetMoE
◦StackMathQA là một bộ sưu tập được tuyển chọn tỉ mỉ gồm 2 triệu câu hỏi và câu trả lời toán học,
có nguồn từ các trang Stack Exchange khác nhau (Zhang, 2024).
◦OpenAssistant là một kho dữ liệu cuộc trò chuyện kiểu trợ lý được tạo bởi con người, được chú thích bởi con người
trong 35 ngôn ngữ khác nhau. Kho dữ liệu này là sản phẩm của một nỗ lực cộng đồng toàn cầu
liên quan đến hơn 13,500 tình nguyện viên (LAION-AI, 2023).
◦xP3x (Crosslingual Public Pool of Prompts eXtended) là một bộ sưu tập các lời nhắc và
bộ dữ liệu trải rộng 277 ngôn ngữ và 16 nhiệm vụ NLP (Muennighoff et al., 2023b).
◦CommitPackFT là một phiên bản được lọc 2GB của CommitPack chỉ chứa các tin nhắn commit
chất lượng cao trên các repo Github công khai giống như các hướng dẫn ngôn ngữ tự nhiên
(Muennighoff et al., 2023a).
3.2 Bộ dữ liệu Tổng hợp
OpenHermes 2.5 là một bộ biên soạn quy mô lớn, đa dạng, chất lượng cao của các bộ dữ liệu
tổng hợp mã nguồn mở và tùy chỉnh (Teknium, 2023). Nó chứa 1 triệu mẫu hướng dẫn và trò chuyện
chủ yếu được tạo tổng hợp, theo cấu trúc ShareGPT. Bộ dữ liệu được biên soạn từ các nguồn
bao gồm Airoboros 2.2 (Durbin, 2023), bộ dữ liệu chuyên gia miền CamelAI (Li et al., 2023a),
ChatBot Arena (chỉ GPT-4) (Zheng et al., 2024a), Collective Cognition (09-11-2023) (CollectiveCognition, 2023), CoT Alpaca GPT4 (Si et al., 2023), Evol
Instruct 70K và 140K (Xu et al., 2023a), Glaive Code Assistant (glaiveai, 2023), GPT4-
LLM (Peng et al., 2023), GPTeacher (Teknium1, 2023), Medical Tasks (CogStack, 2023),
MetaMath 40k (Yu et al., 2023), SlimOrca 550K (Longpre et al., 2023; Mukherjee et al., 2023;
Lian et al., 2023), Platypus (Lee et al., 2024; Lightman et al., 2023; Wang et al., 2023b),
ShareGPT (chỉ GPT4) (lm sys, 2023), và Unnatural Instructions GPT4 (Peng et al., 2023).
UltraTextbooks là một bộ sưu tập toàn diện các sách giáo khoa tổng hợp và do con người viết
chất lượng cao (Locutusque, 2024). Thành phần của bộ dữ liệu kết hợp
nhiều nguồn như nampdn-ai/mini-peS2o ,open-phi/programming books llama ,
open-phi/textbooks ,nampdn-ai/tiny-strange-textbooks , và một bộ sưu tập web chất lượng cao
được chọn lọc từ math-ai/AutoMathText .
UltraChat 200k là một tập con được lọc của bộ dữ liệu UltraChat, bao gồm 1.4M cuộc đối thoại
được tạo bởi ChatGPT (Ding et al., 2023; Tunstall et al., 2023b). Tập con được tạo bằng cách
chọn một phần nhỏ hơn của dữ liệu, viết hoa đúng văn bản để sửa lỗi ngữ pháp, và
loại bỏ các cuộc đối thoại nơi trợ lý không thích hợp tuyên bố thiếu cảm xúc hoặc ý kiến.
3.2.1 Khác
◦Bộ dữ liệu TemplateGSM là một bộ sưu tập mới và rộng lớn chứa hơn 7 triệu
bài toán toán học cấp tiểu học với các giải pháp mã và giải pháp ngôn ngữ tự nhiên
(Zhang et al., 2024b).
◦Bộ dữ liệu Magicoder-Evol-110K và Magicoder-OSS-75K được tạo bằng cách sử dụng
phương pháp OSS-INSTRUCT, tận dụng một LLM để tự động tạo các bài toán lập trình mới
bằng cách lấy cảm hứng từ các đoạn mã ngẫu nhiên được thu thập từ các dự án mã nguồn mở
(Wei et al., 2023).
◦Evol-Code Alpaca là một triển khai mã nguồn mở của Evol-Instruct được điều chỉnh cho
các hướng dẫn mã bằng cách sắp xếp hợp lý, đơn giản hóa, và thêm các hướng dẫn tiến hóa
cụ thể về mã (Luo et al., 2023).
◦Code-290k-ShareGPT là một bộ dữ liệu ở định dạng ShareGPT, bao gồm khoảng
290,000 bộ cuộc trò chuyện (ajibawa 2023, 2024). Code-290k-ShareGPT được xây dựng dựa trên
các bộ dữ liệu hiện có Python-Code-23k-ShareGPT và Code-74k-ShareGPT .
5

--- TRANG 6 ---
JetMoE
4 Tiền huấn luyện Mô hình
4.1 Cơ sở hạ tầng
Chúng tôi sử dụng Megatron (Shoeybi et al., 2019) làm khung đào tạo và tích hợp
Megablock (Gale et al., 2023) để hỗ trợ MoE. Chúng tôi tiếp tục sửa đổi khung đào tạo
để hỗ trợ MoA (Phần 2.3) và z-loss (Phần 2.4). Trái với thực hành thông thường, chúng tôi
chọn song song Pipeline được giới thiệu trong (Narayanan et al., 2021) thay vì song song chuyên gia
cho song song mô hình trong quá trình đào tạo. Điều này chủ yếu do hai lý do. Thứ nhất,
các mô hình Sparse MoE thường có trạng thái ẩn hẹp hơn so với các mô hình transformer tiêu chuẩn.
Do đó, chi phí giao tiếp cho song song pipeline là nhỏ hơn. Thứ hai, chúng tôi sử dụng
lược đồ MoE không rơi được giới thiệu trong Gale et al. (2023); Shen et al. (2023), có thể
gây ra mất cân bằng tải giữa các chuyên gia. Do đó, sử dụng song song chuyên gia sẽ gây ra
tải không cân bằng giữa các thiết bị và dẫn đến đào tạo không hiệu quả. Song song Pipeline có thể tránh
sự chậm lại này vì nó tính toán tất cả các chuyên gia bên trong một lớp trên cùng một thiết bị. Chúng tôi tiến
hành đào tạo trên một cụm chứa 12 nút và 96 H100. Bên trong mỗi nút, các gpu được
kết nối qua NVLinks. Infiniband được sử dụng để giao tiếp nhanh giữa các nút.
4.2 Siêu tham số
Ptotal Pactive nlayers Dmodel Nexperts Top- k n kvheads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632
Bảng 1: Siêu tham số JetMoE-8B.
Các siêu tham số của JetMoE-8B được chọn dựa trên thực hành thông thường cho mô hình ngôn ngữ
transformer 1B. Chúng tôi thay thế tất cả các lớp self-attention và MLP trong transformer
bằng MoA và MoE. Sau đó, chúng tôi đặt cùng số chuyên gia là 8 và top- k là 2 cho mọi
lớp. Như vậy mô hình có khoảng hai lần tính toán so với một mô hình 1B.
Theo ST-MoE (Zoph et al., 2022), trọng số cho mất mát cân bằng tải và z-loss
được đặt là 0.01 và 0.001, tương ứng. Bảng 1 hiển thị các siêu tham số chính trong JetMoE-8B.
JetMoE-8B được đào tạo với bộ tối ưu hóa AdamW (Loshchilov & Hutter, 2017) với tốc độ học tối đa
5e-4 và kích thước batch 4M token với độ dài chuỗi 4096. Chúng tôi
sử dụng lịch trình tốc độ học Warmup-Stable-Decay (WSD) được giới thiệu trong Hu et al.
(2024). Bộ lập lịch tốc độ học này được chia thành ba giai đoạn: giai đoạn khởi động (ký hiệu
bằng W, đại diện cho số bước ở cuối giai đoạn khởi động), giai đoạn đào tạo ổn định (ký hiệu
bằng S), và giai đoạn ủ (ký hiệu bằng D):
lr(s) =

s
W∗η, s<W
η, W<s<S
f(s−S)∗η,S<s<S+D(13)
trong đó 0 <f(s−S)≤1 là một hàm giảm của s, và η là tốc độ học tối đa.
Trong cài đặt của chúng tôi, giai đoạn khởi động kéo dài 10 tỷ token, và giai đoạn suy giảm trải rộng 250
tỷ token. Tốc độ học ban đầu và cuối cùng được đặt là 10% tốc độ học tối đa. Suy giảm trọng số
0.1 và cắt gradient 1.0 được áp dụng trong quá trình đào tạo.
4.3 Hỗn hợp Dữ liệu Đào tạo
JetMoE-8B được đào tạo trên 1.25T token dữ liệu chủ yếu là tiếng Anh từ tài liệu web,
toán học, và mã. Tương tự như phương pháp được ủng hộ trong miniCPM (Hu et al., 2024) và
Gemma (Team et al., 2024), chúng tôi tăng trọng số của dữ liệu chất lượng cao trong giai đoạn
suy giảm tốc độ học. Quá trình đào tạo được chia thành hai giai đoạn:
•Giai đoạn 1 (khởi động và tốc độ học ổn định): Bộ dữ liệu bao gồm RefinedWeb, Star-
coder, The Pile, peS2o từ Dolma, và OpenWebMath.
6

--- TRANG 7 ---
JetMoE
•Giai đoạn 2 (suy giảm tốc độ học): Chúng tôi bao gồm dữ liệu chất lượng cao bổ sung để cải thiện thêm
hiệu suất của mô hình.
Hỗn hợp dữ liệu chi tiết có thể được tìm thấy trong Hình 2 và Bảng 2. Điều quan trọng cần lưu ý là
do ngân sách tính toán hạn chế có sẵn, hỗn hợp dữ liệu của chúng tôi có thể không lý tưởng.
Tuy nhiên, nó phục vụ như một điểm khởi đầu tốt cho việc đào tạo JetMoE-8B và có thể được
tối ưu hóa thêm trong các lần lặp tương lai.
Hình 2: Hỗn hợp dữ liệu tiền huấn luyện
Danh mục Bộ dữ liệu Tỷ lệ phần trăm
Dữ liệu tiền huấn luyện NLRefinedweb 39.8%
Pile Wikipedia 6.7%
Pile StackExchange 4.8%
Pile arXiv 1.0%
Pile còn lại 5.1%
Dolma peS2o 1.0%
Dữ liệu SFT NLxP3x, OpenAssistant, OpenHermes7.3%UltraChat, Oasst-octopack
Sách giáo khoa UltraTextbooks 4.8%
Dữ liệu tiền huấn luyện Mã Starcoder Github 19.6%
Dữ liệu SFT MãMagicoder-OSS, Magicoder-Evol
3.8% Code-290k-ShareGPT, CommitPackFT
Evol-Code Alpaca
Dữ liệu ToánOpen-web-math, algebraic-stack5.8%TemplateGSM, StackMathQA
Bảng 2: Hỗn hợp dữ liệu chi tiết cho Giai đoạn 2
5 Căn chỉnh Mô hình
5.1 Tinh chỉnh Có giám sát Chưng cất (dSFT)
Quá trình dSFT liên quan đến việc đào tạo một mô hình ngôn ngữ học sinh để trả lời các lời nhắc của người dùng,
với dữ liệu được tạo bởi một mô hình giáo viên (như GPT-4 hoặc Claude) (Wang et al., 2022; Taori
et al., 2023; Chiang et al., 2023; Tunstall et al., 2023b). Các bước chính như sau:
7

--- TRANG 8 ---
JetMoE
1.Chưng cất Dữ liệu : Đối với một tập lời nhắc seed {x0
j}J
j=1, tạo phản hồi y0
j sử dụng
mô hình giáo viên πT, và tinh chỉnh hướng dẫn để có được C={(xj,yj)}J
j=1.
2.Điều chỉnh Hướng dẫn : Mô hình học sinh πdSFT được đào tạo bằng cách tối đa hóa khả năng
của các phản hồi được đưa ra các hướng dẫn:
πdSFT=arg max
π∑
(x,y)∈Clogπ(y|x). (14)
Lưu ý rằng kỳ vọng cho hàm khả năng được xấp xỉ bằng cách sử dụng
trung bình cộng trên một batch mẫu đào tạo.
5.2 Tối ưu hóa Sở thích Trực tiếp Chưng cất (dDPO)
dDPO tinh chỉnh mô hình dSFT bằng cách kết hợp sở thích từ một mô hình giáo viên được căn chỉnh
vào quá trình đào tạo. Nó tối ưu hóa một hàm phần thưởng phản ánh những sở thích này,
nhằm căn chỉnh các đầu ra của mô hình học sinh với các kết quả mong muốn dựa trên bộ dữ liệu
sở thích tĩnh.
1.Tối ưu hóa Ràng buộc KL : Nền tảng của dDPO nằm trong tối ưu hóa ràng buộc KL,
dẫn xuất chính sách tối ưu π∗
r tối đa hóa phần thưởng mong đợi
trong khi tối thiểu hóa phân kỳ từ một chính sách cơ sở π0 (Wang et al., 2023a):
π∗
r(y|x):=arg max
πEx∼d0h
Ey∼π(·|x)[r(x,y)]−ηKL(π(·|x)∥π0(·|x))i
(15)
trong đó η là một tham số điều chỉnh cân bằng việc tối đa hóa hàm phần thưởng
r(x,y) và tuân thủ chính sách cơ sở π0.
2.Hàm Phần thưởng Được điều khiển bởi Sở thích : dDPO kết hợp một hàm phần thưởng
phản ánh sở thích từ một mô hình giáo viên được căn chỉnh:
r∗(x,y) =ηlogπ∗(y|x)
πdSFT(y|x)
+ηlogZ(x), (16)
định lượng sở thích cho việc sản xuất phản hồi y được đưa ra đầu vào x tương đối so với
xác suất cơ sở của mô hình dSFT. η điều chỉnh ảnh hưởng của phần thưởng, và Z(x) đảm bảo
chuẩn hóa.
3.Mục tiêu Tối ưu hóa : Mục tiêu để căn chỉnh πθ với sở thích của mô hình giáo viên là:
πθ=arg max
π∑
(x,yw,yl)∈Dlogσ
ηlogπ(yw|x)
πdSFT(yw|x)−ηlogπ(yl|x)
πdSFT(yl|x)
, (17)
trong đó D bao gồm các cặp hướng dẫn-phản hồi, với yw và yl chỉ ra các phản hồi được ưa thích
và ít được ưa thích hơn tương ứng, được chấm điểm bởi mô hình giáo viên.
DPO Ngoại tuyến (Rafailov et al., 2023) trực tiếp tối ưu hóa các chính sách mô hình ngôn ngữ sử dụng dữ liệu
sở thích tĩnh, cung cấp học tập ổn định và điều chỉnh đơn giản hơn so với Học tăng cường
từ Phản hồi Con người (RLHF) (Ouyang et al., 2022; Christiano et al., 2023). Tuy nhiên,
nó đối mặt với thách thức với sự thay đổi phân phối giữa bộ dữ liệu và chính sách đang phát triển.
Các biến thể DPO trực tuyến và lặp đi lặp lại giải quyết vấn đề này với chi phí tăng độ phức tạp tính toán
(Xu et al., 2023b; Guo et al., 2024b; Xiong et al., 2024).
5.3 Chi tiết Căn chỉnh
Khung căn chỉnh của chúng tôi dựa trên Alignment Handbook (Tunstall et al., 2023a) sử dụng
Pytorch 2 (He & Yu, 2023; Ansel et al., 2024) với DeepSpeed ZeRO-3 (Rajbhandari et al.,
2020). Chúng tôi tinh chỉnh mô hình cơ sở JetMoE-8B sử dụng dSFT trên một kết hợp của các
bộ dữ liệu sau: UltraChat 200k (Ding et al., 2023; Tunstall et al., 2023b), Airoboros-3.2 (Durbin,
8

--- TRANG 9 ---
JetMoE
2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,
2024), SystemChat (abacusai, 2024), và Capybara (Daniele & Suphavadeeprasit, 2023).
Mẫu trò chuyện giống như Zephyr-7b-beta. Các siêu tham số chính cho dSFT là tốc độ học
2e-5 với bộ tối ưu hóa Adam, kích thước batch 128, và 3 epoch.
Chúng tôi tinh chỉnh thêm mô hình JetMoE-8B-SFT sử dụng dDPO trên bộ dữ liệu UltraFeedback
(Cui et al., 2023), chứa các nhãn sở thích nhị phân chỉ ra phản hồi được ưa thích
giữa hai tùy chọn. Các siêu tham số chính cho dDPO là tốc độ học 5e-7 với AdamW, kích thước batch 128, và 1 epoch. Quá trình tinh chỉnh này dẫn đến
mô hình JetMoE-8B-Chat. Toàn bộ quá trình căn chỉnh mất 60 giờ GPU H100.
6 Đánh giá
LLaMA2 DeepseekMoE Gemma JetMoE
# Tổng Tham số 7B 16B 2B 8B
# Tham số Kích hoạt 7B 2.8B 2B 2.2B
# Token Đào tạo 2T 2T 2T 1.25T
ARC-challenge 53.1 53.2 48.4 48.7
Hellaswag 78.6 79.8 71.8 80.5
MMLU 46.9 46.3 41.8 49.2
TruthfulQA 38.8 36.1 33.1 41.7
WinoGrande 74.0 73.7 66.3 70.2
GSM8k 14.5 17.3 16.9 27.8
Trung bình OpenLLM Leaderboard 51.0 51.1 46.4 53.0
MBPP (Pass@1) 20.8 34.0 28.0 34.2
HumanEval (Pass@1) 12.8 25.0 24.4 14.6
Trung bình Tất cả 45.5 47.3 43.2 47.6
Bảng 3: Kết quả bảng xếp hạng OpenLLM và điểm chuẩn mã từ bốn mô hình khác nhau.
Chúng tôi đo hiệu suất của JetMoE-8B trên các nhiệm vụ được bao gồm trong bảng xếp hạng OpenLLM2 và
từ các miền khác, bao gồm lý luận vật lý (Bisk et al., 2020), lý luận xã hội (Sap
et al., 2019), trả lời câu hỏi (Clark et al., 2019; Kwiatkowski et al., 2019), toán
học (Cobbe et al., 2021), lý luận thông thường (Sakaguchi et al., 2021), mô hình hóa ngôn ngữ
(Paperno et al., 2016), hiểu đọc (Joshi et al., 2017), và nhiều hơn nữa. Đối với hầu hết
các điểm chuẩn, chúng tôi sử dụng cùng một phương pháp đánh giá như trong bảng xếp hạng OpenLLM
để có thể so sánh với các mô hình khác.. Chúng tôi so sánh các mô hình JetMoE-8B với một số
LLM mã nguồn mở (OSS) bên ngoài, bao gồm Gemma, LLaMA2, DeepseekMoE.
Ngoài ra, chúng tôi bao gồm HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021)
để đánh giá việc tạo mã của các mô hình. Sử dụng BigCode Evaluation Har-
ness (Ben Allal et al., 2022), chúng tôi theo các công trình gần đây về Code LLMs (Rozi `ere et al., 2024; Guo
et al., 2024a) với giải mã tham lam, và báo cáo trung bình pass@1 (tỷ lệ thành công trung bình) cho
hai điểm chuẩn.
Bảng 3 hiển thị kết quả bảng xếp hạng OpenLLM và điểm chuẩn mã từ bốn mô hình khác nhau.
JetMoE-8B vượt trội hơn Gemma, LLaMA2, và DeepseekMoE trên bảng xếp hạng OpenLLM,
đạt được điểm số tốt nhất trong tất cả các nhiệm vụ ngoại trừ ARC-challenge và WinoGrande.
Ngoài ra, JetMoE-8B đạt được điểm số MBPP cao nhất trong lập trình Python.
Chúng tôi cũng đánh giá mô hình của chúng tôi trên MT-Bench (Zheng et al., 2023) với một thẩm phán LLM mạnh
(checkpoint gpt-4-0613). Cấu hình nhiệt độ, theo triển khai FastChat
chính thức, được định nghĩa như sau: các nhiệm vụ "Writing" và "Roleplay" có nhiệt độ
0.7, chỉ ra tính sáng tạo cao hơn; các nhiệm vụ "Extraction", "Math", "Coding", và "Reasoning"
2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
9

--- TRANG 10 ---
JetMoE
có nhiệt độ 0.0, gợi ý sự chính xác; và "STEM" và "Humanities" có
nhiệt độ 0.1, ngụ ý biến đổi nhiều hơn một chút so với các nhiệm vụ 0.0.
JetMoE-8B-Chat đạt được điểm số MT-Bench cao hơn Llama-2-13b-Chat sau khi căn chỉnh,
chứng minh hiệu suất vượt trội của nó. Tuy nhiên, như được hiển thị trong Hình 3, JetMoE-8B-chat
tương đối yếu trong lập trình và trích xuất so với GPT-3.5-turbo. Điều này có thể do
kích thước mô hình nhỏ hơn dẫn đến khả năng lý luận không tối ưu trong các nhiệm vụ này. Mặc dù
có hạn chế này, JetMoE-8B-chat thể hiện hiệu suất mạnh qua các chiều khác nhau khác,
làm cho nó trở thành một mô hình cạnh tranh trong cảnh quan LLM mã nguồn mở.
Mô hình Điểm số MT-Bench
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Vicuna-13b-v1.3 6.413
Wizardlm-13b 6.353
Llama-2-7b-chat 6.269
Bảng 4: So sánh điểm số MT-Bench của các mô hình khác nhau
Hình 3: Hình radar MT-Bench
7 Hạn chế và Công việc Tương lai
Do ngân sách hạn chế $100k, chúng tôi không thể có khả năng nghiên cứu loại bỏ nào cho kiến trúc mô hình.
Các siêu tham số và hỗn hợp dữ liệu cũng được chọn bằng tay dựa trên kết quả thực nghiệm
từ các công trình trước đây (Shen et al., 2023; Zoph et al., 2022; Hu et al., 2024).
Trong tương lai, sẽ thú vị khi nghiên cứu thêm về đóng góp thực tế của các
thành phần khác nhau cho kết quả cuối cùng.
8 Kết luận
Chúng tôi giới thiệu JetMoE-8B, một mô hình MoE mã nguồn mở đạt được hiệu suất tiên tiến
trong số các mô hình mã nguồn mở trong khi duy trì hiệu quả cao. Bằng cách tận dụng kích hoạt thưa
10

--- TRANG 11 ---
JetMoE
trong cả lớp attention và feed-forward, JetMoE-8B giảm chi phí tính toán
trong khi duy trì hiệu suất mạnh qua một loạt rộng các nhiệm vụ.
Được đào tạo bằng cách sử dụng phương pháp hai giai đoạn và một hỗn hợp được tuyển chọn cẩn thận của các bộ dữ liệu mã nguồn mở,
JetMoE-8B vượt trội hơn các mô hình lớn hơn và tốn nhiều tài nguyên hơn trên OpenLLM Leader-
board. Ngoài ra, JetMoE-8B-Chat thể hiện hiệu suất cạnh tranh so với
các chatbot mã nguồn mở khác.
Chúng tôi cung cấp thông tin tham số đào tạo chi tiết và hỗn hợp dữ liệu để khuyến khích tái sản xuất
và cho phép các nhà nghiên cứu xây dựng dựa trên công việc của chúng tôi. JetMoE-8B đại diện cho một bước tiến
đáng kể trong việc phát triển các mô hình ngôn ngữ mã nguồn mở, hiệu quả, và hiệu suất cao,
đóng góp vào việc dân chủ hóa các công nghệ ngôn ngữ tiên tiến.
Lời cảm ơn
Chúng tôi bày tỏ lòng biết ơn của mình đối với Shengding Hu vì lời khuyên quý giá của anh ấy về hỗn hợp dữ liệu
Giai đoạn 2. Chúng tôi cũng bày tỏ lòng biết ơn của mình đối với Exabits vì sự hỗ trợ của họ trong việc thiết lập
các cụm GPU, và đối với Lepton AI vì sự hỗ trợ của họ trong việc thiết lập demo trò chuyện.
Tài liệu tham khảo
abacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/
SystemChat .
ajibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/
ajibawa-2023/Code-290k-ShareGPT .
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-
sensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine
learning through dynamic python bytecode transformation and graph compilation, 2024.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with
large language models. arXiv preprint arXiv:2108.07732 , 2021.
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,
Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language
model for mathematics, 2023.
Loubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro
von Werra. A framework for the evaluation of code generation models. https://github.
com/bigcode-project/bigcode-evaluation-harness , 2022.
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
Raff, et al. Pythia: A suite for analyzing large language models across training and scaling.
arXiv preprint arXiv:2304.01373 , 2023.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical
commonsense in natural language. In Proceedings of the AAAI conference on artificial
intelligence , volume 34, pp. 7432–7439, 2020.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in neural information processing systems ,
33:1877–1901, 2020.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,
Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
11

--- TRANG 12 ---
JetMoE
Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-
hammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,
Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,
Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,
Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,
2021.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P . Xing.
Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
URL https://lmsys.org/blog/2023-03-30-vicuna/ .
Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
Deep reinforcement learning from human preferences, 2023.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
arXiv preprint arXiv:1905.10044 , 2019.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers
to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
CogStack. OpenGPT: A framework for creating grounded instruction based datasets and
training conversational domain expert Large Language Models (LLMs). https://github.
com/CogStack/OpenGPT , 2023.
CollectiveCognition. Collective cognition chatgpt conversations, 2023. URL https://
huggingface.co/datasets/CollectiveCognition/chats-data-2023-09-22 .
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,
Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-
quality feedback, 2023.
Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi
Li, Wangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert
specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066 ,
2024.
Luigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse
multi-turn conversations for effecient llm training. arXiv preprint arXiv:(coming soon) , 2023.
URL https://huggingface.co/datasets/LDJnr/Capybara .
Databricks. Dbrx: Resources and code examples. https://github.com/databricks/dbrx ,
2024.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu,
Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality
instructional conversations, 2023.
Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of
language models with mixture-of-experts. In International Conference on Machine Learning ,
pp. 5547–5569. PMLR, 2022.
Jon Durbin. airoboros: Customizable implementation of the self-instruct paper. https:
//github.com/jondurbin/airoboros , 2023.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity, 2021.
12

--- TRANG 13 ---
JetMoE
Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient
sparse training with mixture-of-experts. Proceedings of Machine Learning and Systems , 5,
2023.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of
diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.
Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL
https://github.com/openlm-research/open_llama .
glaiveai. Glaive-code-assistant, 2023. URL https://huggingface.co/datasets/glaiveai/
glaive-code-assistant .
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,
Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder:
When the large language model meets programming – the rise of code intelligence, 2024a.
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares,
Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu
Blondel. Direct language model alignment from online ai feedback, 2024b.
Horace He and Shangdi Yu. Transcending runtime-memory tradeoffs in checkpointing by
being fusion aware. Proceedings of Machine Learning and Systems , 5, 2023.
Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei
Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang,
Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding,
Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling
the potential of small language models with scalable training strategies, 2024.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,
Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian
Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L ´elio Renard Lavaud,
Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang,
Szymon Antoniak, Teven Le Scao, Th ´eophile Gervet, Thibaut Lavril, Thomas Wang,
Timoth ´ee Lacroix, and William El Sayed. Mixtral of experts, 2024.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large
scale distantly supervised challenge dataset for reading comprehension. arXiv preprint
arXiv:1705.03551 , 2017.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural
questions: a benchmark for question answering research. Transactions of the Association for
Computational Linguistics , 7:453–466, 2019.
LAION-AI. Open-Assistant: A chat-based assistant that understands tasks, can interact
with third-party systems, and retrieve information dynamically. https://github.com/
LAION-AI/Open-Assistant , 2023.
Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful
refinement of llms, 2024.
Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard
Ghanem. Camel: Communicative agents for "mind" exploration of large language model
society. In Thirty-seventh Conference on Neural Information Processing Systems , 2023a.
13

--- TRANG 14 ---
JetMoE
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-
hao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii
Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
Joel Lamy-Poirier, Jo ˜ao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,
Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-
tasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel,
Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi
Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Ku-
nakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire
Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer
Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,
Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu ˜noz Ferrandis, Sean Hughes,
Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the
source be with you!, 2023b.
Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,
and "Teknium". Slimorca: An open dataset of gpt-4 augmented flan reasoning traces,
with verification, 2023. URL https://https://huggingface.co/Open-Orca/SlimOrca .
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,
Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step.
preprint arXiv:2305.20050 , 2023.
lm sys. FastChat: An open platform for training, serving, and evaluating large language
model based chatbots. https://github.com/lm-sys/FastChat , 2023.
Locutusque. Ultratextbooks, 2024. URL https://huggingface.co/datasets/Locutusque/
UltraTextbooks .
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,
Quoc V . Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing
data and methods for effective instruction tuning, 2023.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 , 2017.
Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,
Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2
and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 , 2024.
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao,
Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language
models with evol-instruct, 2023.
Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math:
Unlocking the potential of slms in grade school math, 2024.
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo,
Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack:
Instruction tuning code large language models. arXiv preprint arXiv:2308.07124 , 2023a.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,
Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xian-
gru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid
Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization
through multitask finetuning, 2023b.
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi,
and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of
gpt-4, 2023.
14

--- TRANG 15 ---
JetMoE
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Pat-
wary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan
Catanzaro, et al. Efficient large-scale language model training on gpu clusters using
megatron-lm. In Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis , pp. 1–15, 2021.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob
Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul
Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions
with human feedback, 2022.
Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin
Raffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of
mixture-of-experts language models, 2024.
Denis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella
Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ´andez. The
lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint
arXiv:1606.06031 , 2016.
Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An
open dataset of high-quality mathematical web text, 2023.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro
Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and
web data only. arXiv preprint arXiv:2306.01116 , 2023.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction
tuning with gpt-4. arXiv preprint arXiv:2304.03277 , 2023.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward
model, 2023.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020.
URL http://jmlr.org/papers/v21/20-074.html .
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory
optimizations toward training trillion parameter models, 2020.
Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,
Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J ´er´emy Rapin, Artyom Kozhevnikov,
Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori,
Wenhan Xiong, Alexandre D ´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis
Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open
foundation models for code, 2024.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,
2021.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa:
Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728 , 2019.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey
Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-
of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.
15

--- TRANG 16 ---
JetMoE
Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan.
Moduleformer: Learning modular large language models from uncurated data. arXiv
preprint arXiv:2306.04640 , 2023.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and
Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using
model parallelism. arXiv preprint arXiv:1909.08053 , 2019.
Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical
study of instruction-tuning large language models in chinese, 2023.
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell
Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,
Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson,
Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Pe-
ters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant
Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh
Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus
of three trillion tokens for language model pretraining research, 2024.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063, 2024.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,
Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following
llama model. https://github.com/tatsu-lab/stanford_alpaca , 2023.
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
Shreya Pathak, Laurent Sifre, Morgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, et al.
Gemma: Open models based on gemini research and technology. arXiv preprint
arXiv:2403.08295 , 2024.
Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,
2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5 .
Teknium1. GPTeacher: A collection of modular datasets generated by GPT-4. https:
//github.com/teknium1/GPTeacher , 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 ,
2023.
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang,
Kashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https:
//github.com/huggingface/alignment-handbook , 2023a.
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes
Belkada, Shengyi Huang, Leandro von Werra, Cl ´ementine Fourrier, Nathan Habib,
Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr:
Direct distillation of lm alignment, 2023b.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.
Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl:
Generalizing direct preference optimization with diverse divergence constraints, 2023a.
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam,
Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating
college-level scientific problem-solving abilities of large language models, 2023b.
16
