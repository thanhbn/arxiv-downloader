# 2002.07028.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2002.07028.pdf
# File size: 713171 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Low-Rank Bottleneck in Multi-head Attention Models
Srinadh Bhojanapalli* Chulhee Yun†Ankit Singh Rawat‡Sashank J. Reddi§
Sanjiv Kumar¶
Abstract
Attention based Transformer architecture has enabled signiﬁcant advances in the ﬁeld of natural language processing.
In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger
embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in
the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size
requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each
head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation. We further
validate this in our experiments. As a solution we propose to set the head size of an attention unit to input sequence
length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive
power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and
with better performance scaling.
1 Introduction
Attention based architectures, such as Transformers, have been effective for sequence modelling tasks such as machine
translation [Gehring et al., 2017, Vaswani et al., 2017], question answering, sentence classiﬁcation [Radford et al., 2018,
Devlin et al., 2018] and document generation [Liu et al., 2018]. These models have emerged as better alternatives to
the recurrent models - RNNs [Sutskever et al., 2014], LSTMs [Hochreiter and Schmidhuber, 1997] and GRUs [Cho
et al., 2014]. This is mainly due to their feed forward structure, which removes the sequential processing bottleneck for
sequence data, making them easier to train compared to the recurrent models. Self attention models also have found
applications in vision [Wang et al., 2018], adversarial networks [Zhang et al., 2018], reinforcement learning [Zambaldi
et al., 2018, Li, 2017] and speech recognition [Chiu et al., 2018].
Recent advances in using the self attention models in natural language tasks have been made by ﬁrst using a
language modeling task to pre-train the models and then ﬁne tuning the learned models on speciﬁc downstream tasks.
Radford et al. [2018] and Devlin et al. [2018] used Transformers to pre-train a language model and showed that the
ﬁne tuned model outperforms LSTMs on many natural language understanding and question answering tasks. For
example, BERT [Devlin et al., 2018], a 24 layer transformer model, is shown to achieve the state of the art performance
on several NLP tasks, including on the SQuAD dataset. These advances, in addition to novel pre-training tasks, relied
on bigger models with a larger embedding size. BERT model uses an embedding size of 1024 [Devlin et al., 2018];
GPT-2 uses models with embedding size up to 1600 [Radford et al., 2019].
A single Transformer block consists of two key components: a multi-head self attention layer followed by a feed
forward layer [Vaswani et al., 2017]. A single head in a multi-head attention layer, computes self attention between
the tokens in the input sequence, which it then uses to compute a weighted average of embeddings for each token.
Each head projects the data into a lower dimensional subspace, and computes the self attention in this subspace. This
projection size for each head is commonly referred to as the head size .
*Google Research NY . bsrinadh@google.com
†MIT. Based on work performed at Google Research New York. chulheey@mit.edu
‡Google Research NY . ankitsrawat@google.com
§Google Research NY . sashank@google.com
¶Google Research NY . sanjivk@google.com
1arXiv:2002.07028v1  [cs.LG]  17 Feb 2020

--- PAGE 2 ---
# heads 8 16 32
# params 336M 336M 336M
SQuAD - F1 90.890.15 90.61 0.14 90.45 0.08
SQuAD - EM 84.10.34 83.75 0.27 83.48 0.13
MNLI 850.2 84.5 0.4 84.4 0.2
Table 1: Performance of BERT LARGE [Devlin et al., 2018], a 24 layer Transformer with an embedding size of 1024,
suffers with the increasing number of heads after 8 heads.
To keep the number of parameters ﬁxed in the attention layer regardless of the number of heads, the prevalent
heuristic is to scale the head size with 1/(number of heads). This heuristic was initially proposed in Vaswani et al. [2017]
and has become a de facto standard heuristic in multi-head attention models [Radford et al., 2018, Devlin et al., 2018].
However, increasing the number of heads decreases the head size, decreasing the expressive power of individual heads.
We prove that reducing the head size to a value below the input sequence length harms the representation power of each
head (see Theorem 1). This is because a smaller head size introduces a rank constraint on the projection matrices in each
head, and limits their representation power. We indeed notice this effect in practice: while the performance improves
with increasing the number of heads in the beginning [Devlin et al., 2018], we notice a drop in the performance once
the number of heads increases beyond a certain threshold, as seen in Table 1 and Fig. 1 (see also Table 4(A) in Vaswani
et al. [2017]).
In order to avoid hurting the performance, the existing models allow for multiple heads by increasing the embedding
size, which in turn increases the head size. However, larger embedding size, in addition to increasing the number of
parameters, makes it expensive to use the model and the learned embeddings in downstream tasks, as the downstream
model sizes scale with the embedding size of the tokens. For example, the inference time and memory required in
retrieval tasks typically increases linearly with the embedding size.
In this paper we propose setting the head size of attention units to input sequence length. While this is a simple
hyper-parameter change in the Transformer architecture, we show that it is important to set this value appropriately to
avoid the low-rank bottleneck (see Theorem 1), and to improve the representation power (see Theorem 2). This ﬁxed
head size is also independent of both the number of heads and the embedding size of the model. This allows us to train
models with a relatively smaller embedding size (hence fewer parameters) without affecting the head size. Another
advantage of the ﬁxed head size is that unlike the standard setting which requires the number of heads to be a factor of
the embedding size, we are free to set an arbitrary number of heads as required for the task.
Interestingly, we note that this simple yet novel approach of ﬁxing the head size in multi-head Transformers results
in empirically superior performance. We evaluate Transformers trained with this ﬁxed head size on language modeling
(LM1B dataset), natural language inference (MNLI dataset) and question answering tasks (SQuAD dataset). We show
that ﬁxing the head size allows us to train Transformers with a better performance scaling and smaller embedding size.
We show that with the ﬁxed head size Transformers trained with an embedding size of 512 can match the performance
of the BERT LARGE [Devlin et al., 2018], a Transformer with an embedding size of 1024 (see Fig. 2). We further present
experimental results evaluating the effect of different choices of the head size and the embedding size in Section 4.
Our contributions in this paper lie in identifying and rigorously proving the low rank bottleneck in multi-head
attention models, and showing that ﬁxing the head size to input sequence length results in a strictly better model, both
theoretically and empirically. The contributions of this paper are summarized below.
•We analyze the representation power of the multi-head self attention layer and prove the low-rank bottleneck the
head size places on the attention units (Theorem 1).
•We propose to set the head size to input sequence length, and show that ﬁxing the head size strictly improves the
expressive power of the multi-head attention layers compared to the standard heuristic for setting the head size
(Theorem 2). This allows us to both increase the number of heads per layer and decrease the embedding size, without
hurting the performance. We develop a novel construction based approach to prove this result, which can potentially
be useful in analyzing other variants of the Transformer architecture.
•We experimentally show that with a ﬁxed head size, Transformers can be trained with better performance scaling and
2

--- PAGE 3 ---
a smaller embedding size on three standard NLP tasks.
1.1 Related Works
Given the signiﬁcance of self attention models, there has been work trying to both improve the performance and speed
up the computation in Transformers. Ott et al. [2018] and You et al. [2019] reduce precision and use large batch training
to reduce the training time of the attention models. Child et al. [2019] propose sparse self attention models to speed up
the computation in the attention layer for long sequence data generation tasks. They show that these sparse attention
models can be trained on tasks with sequence length greater than 10k without sacriﬁcing the accuracy. Dehghani et al.
[2018] propose a depth recurrent Transformer network that reuses the parameters across layers. They show that this
modiﬁcation makes the Transformer networks Turing complete even with ﬁnite precision weights. Yang et al. [2019]
propose a new way to increase the effective sequence length that the Transformer attends to, by reusing the intermediate
embeddings across sequences. They show that the modiﬁed architecture performs better on tasks that require computing
context over longer sequence lengths. We note that most of these modiﬁcations rely on the multi-head self attention, the
same building block of the Transformers. Our work is studying this basic multi-head attention layer, and suggesting a
new way to set the head size, which can be easily applied along with any of the above architectural modiﬁcations.
Wu et al. [2019] propose to replace the self-attention layer with lightweight dynamic convolutions and show
improved performance on machine translation and language modeling. Even though the resulting model has faster
inference time, it still needs to use a large embedding size (1024), as big as the original attention models. We believe the
techniques in this paper can be combined with these results to realize both smaller embedding size and faster inference
time.
Sun et al. [2019] perform neural architecture search using evolutionary methods on sequence to sequence models
and ﬁnd an evolved transformer architecture, which in addition to multi-head attention units, has convolution ﬁlter and
gated linear units. Our proposed modiﬁcations stay closer to Transformers in spirit and can be used as seed units for
this architecture search.
Yang et al. [2017] have studied the effect of rank constraint caused by the small projection sizes in computing the
softmax loss. The situation in self attention layers is a bit different. While the expressive power of each head reduces
with the decreasing head size, at the same time we are increasing the number of heads, which can potentially negate this
and increase the overall capacity of the layer. As we show in Theorem 2, the prevalent head size heuristic indeed limits
the expressive power of the multi-head attention layer.
Yun et al. [2019] studied the representation power of Transformers and showed that they are universal approximators
of sequence to sequence functions. However they do not study the low rank bottleneck caused by the prevalent head
size heuristic and its connection to the embedding size.
V oita et al. [2019], Michel et al. [2019] study the importance of different heads in an attention layer. They observe
that, during inference, many of the heads in each layer can be pruned away with a little effect on the prediction. However,
they still need multiple heads during the training.
Child et al. [2019], Correia et al. [2019] impose sparsity structure on the attention layer during training to improve
both interpretability and performance. Fixing the head size will in fact make it easier to learn such sparsity patterns, as
a low rank constraint does not allow a head to express all possible sparsity patterns. Combining these techniques can
hence potentially enable training of sparse attention models with a smaller embedding size.
2 Transformer Architecture and Analysis
In this section, we present the Transformer architecture and analyze the representation power of the multi-head self
attention, a key component of the Transformer block.
The input to a Transformer network is a sequence of ntokens. Typically, each token is converted into a token
embedding of dimension dby an embedding layer. We let X2Rdnbe the embedding matrix corresponding to the n
tokens in the input sequence.
3

--- PAGE 4 ---
2.1 Single-Head Attention
The Transformer block is a combination of a self attention layer followed by a feed forward layer [Vaswani et al.,
2017]. Both layers have a skip connection and use Layer Normalization (LN) [Ba et al., 2016]. In particular, for token
embeddings X, the dot product attention is computed as follows.
Attention (X) =WvXSoftmax(WkX)T(WqX)pdk
=WvXP: (1)
HereWq2Rdqd,Wk2RdkdandWv2Rdvdrepresent the projection matrices associated with the query,
key and value respectively in an attention unit [Vaswani et al., 2017]. For a single-head attention unit, we have
dq=dk=dv=d. In the dot-product attention (cf. (1)),Paims to capture the context of the input for a given token
based on the remaining tokens in the input sequence. Subsequently, the output of the attention layer takes the following
form.
LN(X+WoAttention (X)); (2)
where LN()represents the layer-normalization operation. Given the attention module, as deﬁned in (1), it is natural to
question its ability to represent arbitrary contexts Pfor a given input sequence X.
In the following result we establish that for a large enough projection size an attention unit can represent any data
pair(X;P). We also show that the model cannot represent arbitrary context when dis smaller than n, creating a
low-rank bottleneck.
Theorem 1 (Representation Theorem) .Ifdq=dk=dn, then given any full column rank matrix X2Rdnand
an arbitrary nnpositive column stochastic matrix P, there always exists ddprojection matrices WqandWk
such that
Softmax(WkX)T(WqX)pdk
=P: (3)
Ifdq=dk=d<n , there exist XandPsuch that (3)does not hold for all WqandWk.
This result shows that the projection dimension dq=dk=dneeds to be larger than the sequence length nfor the
attention unit to be able to represent any desired context P. Even though this result describes a single example sequence
case, it highlights a fundamental property of the model architecture that decreasing the projection size below a certain
threshold introduces a bottleneck.
Proof of Theorem 1. dncase. To prove the ﬁrst part of the result, we present an explicit construction of Wkand
Wqwhich allows us to generate PfromXusing the dot product attention. Since Xhas full column rank, there exists a
left inverse Xy= (XTX) 1XT2Rndsuch that XyX=In. LetWk=~WkXyandWq=~WqXy. Then
XTWT
kWqX=XT(Xy)T~WT
k~WqXyX=In~WT
k~WqIn
=~WT
k~Wq=~Wkq: (4)
Now that the above choice of WqandWkhas handled the dependence on X, we will choose a ~Wkqdepending on P
and ﬁnish the construction. Below we express the Softmax operation on the query and key inner products. Note that the
Softmax here is a columnwise operator computing the attention scores for each query. By using (4), we obtain that
Softmax(WkX)T(WqX)pdk
= Softmax"~Wkqpdk#
= exp ~Wkqpdk!
D 1
~Wkq;
where D~Wkqis annndiagonal matrix such that
(D~Wkq)ii=nX
j=1exp 
(~Wkq)jipdk!
= 
1Texp 
(~Wkq)pdk!!
i:
4

--- PAGE 5 ---
Hence, we can establish the desired result by showing that there always exists a ~Wkqthat satisﬁes the following
ﬁxed point equation.
exp ~Wkqpdk!
=PD~Wkq: (5)
Given P, to construct such a ~Wkq, we pick an arbitrary positive diagonal matrix D0, and set
~Wkq=p
dklog (PD0): (6)
Since Pis a positive matrix, such a ~Wkqalways exists. Next, we verify that this construction indeed satisﬁes the ﬁxed
point equation (cf. (5)). Note that
D~Wkq= Diag 
1Texp 
(~Wkq)pdk!!
= Diag 
1TPD0
=D0: (7)
The last equation follows from the fact that Pis a column stochastic matrix. Now, using (6) and (7),
exp ~Wkqpdk!
=PD0=PD~Wkq:
This completes the ﬁrst part of the proof.
d<ncase. Consider the case of d= 1andn= 2. Then X2R12andWqandWk2R11. LetX= [1;0]. Then
Softmax(WkX)T(WqX)pdk
= Softmax[1;0]TWT
kWq[1;0]pdk
= SoftmaxWkWq0
0 0
:
This matrix clearly cannot be used to generate Pthat have distinct elements in the second column, e.g., P=0:5 0:75
0:5 0:25
.
2.2 Multi-Head Attention
As discussed in Section 2.1, an attention unit updates the embedding of an input token based on a weighted average
of the embeddings of all the tokens in the sequence, using the context P(cf.(1)). Vaswani et al. [2017] proposed
Multi-Head attention mechanism that increases the representation power of an attention layer, where multiple attention
units operate on different low dimensional projections of the input, with each attention unit being referred to as a
head. This is followed by concatenation of the outputs from different heads. In particular, the computation inside a
Multi-Head attention with hheads takes the following form:
head(X)i=Wi
vXSoftmax
(Wi
kX)T(Wi
qX)=p
d
h
2Rd
hn
MultiHead (X) =Concat [head(X)1;;head(X)h]2Rdn:
The output of the Multi-head attention layer then becomes
Z=LN(X+WoMultiHead (X)); (8)
where Wo2Rdd. For a model with hheads, the query, key and value projection matrices fWi
qg,fWi
kgandfWi
vg
ared
hdmatrices. Therefore, each head projects the input onto ad
h-dimensional subspace to compute the context, and
keeps the number of parameters ﬁxed per layer. Using MultiHead has resulted in empirically better performance over
the single head attention layer [Vaswani et al., 2017].
5

--- PAGE 6 ---
(a) LM1B
 (b) LM1B
Figure 1: Performance of Transformers trained with the prevalent head size heuristic ( dp=d=h) (baseline) compared
with the ﬁxed head size ( dp= 32 ) on a language modeling task (LM1B) on the test set. We train baseline models with
embedding sizes from 256 to 512. We train the ﬁxed head size models with a ﬁxed embedding size of 256 and a head
size of 32, and vary the number of heads from 4 to 70, while matching the number of parameters. The plots clearly
indicate that ﬁxing the head size allows us to train Transformers with a smaller embedding size (plot (b)), and with a
better scaling of performance (plot (a)). Note that for perplexity lower values are better.
2.3 Low-Rank Bottleneck
While increasing the number of heads seemingly gives the model more expressive power, at the same time we are
reducing the head size, which can decrease the expressive power. When the number of heads his larger thand
n, the
attention unit inside each head projects onto a dimension smaller than n, creating a low-rank bottlenck and loses
its ability to represent arbitrary context vectors (cf. Theorem 1). Interestingly, this is consistent with the empirical
observation in Table 1 that increasing hbeyond 8 results in performance degradation in BERT LARGE [Devlin et al.,
2018]; note that d= 1024 andn= 128 for most of the pre-training phase of BERT LARGE .
Since the sequence length is ﬁxed from the data/task at hand, the only way to increase the number of heads without
introducing the low-rank bottleneck is by increasing the embedding size d. This is a fundamental limitation of the
currently dominant head size heuristic, that we need to increase the embedding size in order to support more heads.
Unfortunately, increasing the embedding size leads to higher computation and memory requirements to train
and store the model. Further, since it is common to use learned embeddings from Transformer based models for
downstream tasks [Devlin et al., 2018], larger embedding size increases the model size and computation required for all
the downstream tasks as well.
3 Fixed Multi-Head Attention
In this section we propose to ﬁx the head size of the Transformer, which allows us to enjoy the advantage of higher
expressive power of multiple heads without requiring the embedding size to be large. The key is to decouple the
dependency between the projection size in a head and the embedding size of the model. The projection matrices now
project onto subspaces of a ﬁxed dimension dpirrespective of the number of heads h. This approach where dpis
independent of dandhleads to the following attention mechanism.
ﬁxedhead (X)i=Vi
vXSoftmax
(Vi
kX)T(Vi
qX)=p
dp
2Rdpn
FixedMultiHead (X) =Concat [ﬁxedhead (X)1;;ﬁxedhead (X)h]2Rdphn:
6

--- PAGE 7 ---
Note that the projection matrices used here fVi
qg,fVi
kgandfVi
vgaredpdmatrices. With Vo2Rdhdp, the
output of this new multi-head attention layer takes the following form.
Z=LN(X+VoFixedMultiHead (X)):
This modiﬁcation makes each attention head more similar to a hidden unit in a feed forward network or a ﬁlter in a
convolutional network, and allows us to vary the number of heads without worrying about reducing the representation
power per head. The downside is, unlike the standard MultiHead, the number of parameters per layer increases with
the number of heads. However, this modiﬁcation allows us to train a model with a smaller embedding size without a
low-rank bottleneck, ultimately allowing us to reduce the total number of parameters in the model.
3.1 MultiHead vs. FixedMultiHead Attention
Given a MultiHead layer, we can always represent it using a FixedMultiHead layer, whenever we have the head
sizedpd=h. While this shows that increasing the number of heads hbeyond d=dpmakes individual heads of the
FixedMultiHead as expressive as the ones in the MultiHead, it is not obvious if FixedMultiHead is strictly more
expressive. Can the FixedMultiHead layer represent functions that the standard MultiHead layer can not represent?
In this subsection we show that indeed, in the multi-head regime, the FixedMultiHead layer is strictly better than the
standard MultiHead layer in terms of expressive power.
Consider the standard multi-head attention units in (8).
fW(X) =WoMultiHead (X):
We denote the collection of all parameter matrices as W. Similarly, consider the function represented by the ﬁxed head
size attention units:
gV(X) =VoFixedMultiHead (X):
LetVbe the collection of all these parameter matrices. We deﬁne FandGto be the class of functions fW()and
gV(), respectively. As noted above, if dpd=h, we haveFG .
The following theorem shows that even for simple examples in G, functions inFfail to represent them; this already
shows thatFis astrict subset ofG.
Theorem 2. Letn2,ddp, andh> d=dp. Consider a FixedMultiHead attention layer gV()with parameters that
satisfy the following conditions:
Vo2
64V1
v
...
Vh
v3
75is full rank, and (Vi
k)TVi
q=U;for alli= 1;:::;h; where Uis a rank-dpmatrix.
Then, for any fW2F, there exists X2Rdnsuch thatfW(X)6=gV(X):
BecausekfW(X) gV(X)kis a continuous function of X, existence of such an Ximplies that the integral of the
norm of difference (i.e., approximation error) is strictly positive. We note that the assumptions on Vi
kandVi
qin the
above Theorem are made to provide a simple and constructive proof; in fact, failure of MultiHead ( F) to represent such
simple attention layers suggests that the situation is likely worse for more complex functions.
Theorem 2 shows that the expressive power of the FixedMultiHead attention function class is strictly superior to the
standard MultiHead attention function class. Hence the heuristic of reducing the head size with the number of heads is
limiting the expressive power of MultiHead, whereas using the ﬁxed head size will increase the expressive power of the
attention layers.
4 Experiments
The goal of this section is to show that setting the head size in a principled way leads to better performance than using
the prevalent heuristic. We again note that while this is a simple hyper-parameter change to the Transformer, setting this
to input sequence length as shown in our analysis, allows us to train better models with a smaller embedding size.
7

--- PAGE 8 ---
(a) SQuAD F1
 (b) SQuAD EM
 (c) MNLI
Figure 2: Comparison of 24 layer Transformer models trained with the prevalent head size heuristic BERT LARGE
(baseline) vs. the ﬁxed head size model on the SQuAD and MNLI dev sets. We vary the embedding size of the baseline
models from 512 to 1024. We train the ﬁxed head size models with a ﬁxed embedding size of 512 and a head size of
128, with a varying number of heads from 8 to 32, while matching the number of parameters. Fixing the head size
allows us to train models with a smaller embedding size of 512 and with a better performance.
(a)
 (b)
Figure 3: Ablation studies on LM1B: (a) We ﬁx the embedding size of all the models to 256 and vary the capacity of
Transformers trained with the prevalent head size heuristic (baseline) by increasing the size of the feedforward layers.
For the ﬁxed head size models we ﬁx the head size to 32, so 8 head ﬁxed head size model is the same as the 8 head
baseline model. We notice that again with the standard heuristic increasing the number of heads beyond 16 hurts the
performance, whereas with a ﬁxed head size increasing the number of heads monotonically improves the performance.
(b) We show the effect of head size on the performance with different number of heads. Both plots clearly show the
advantage in having an additional way to tune the capacity of Transformers with a ﬁxed embedding size.
In this section we present our experiments on three standard NLP tasks, language modeling (LM1B), question
answering (SQuAD), and sentence entailment (MNLI), to demonstrate: 1) Increasing the number of heads in Trans-
formers beyond a certain point hurts the performance with the prevalent head size heuristic, but always helps with the
ﬁxed head size attention layers; 2) Decoupling the head size from embedding size allows us to train models with a
smaller embedding size; and 3) Setting the head size appropriately in the Transformers allows us to train models with a
better performance scaling. We ﬁrst describe our experimental setup followed by our results and ablation studies on the
proposed modiﬁcations.
4.1 Setup and Datasets
For the language modeling task we use the one billion word benchmark dataset (LM1B) [Chelba et al., 2013]. This
dataset has around 30M training examples and around 300k examples in the test set. We use a sub-word tokenizer with
8

--- PAGE 9 ---
# heads 8 12 16 32
# params 168M 193M 218M 319M
SQuAD - F1 89.60.17 90.250.21 90.430.14 90.950.14
SQuAD - EM 82.730.21 83.180.24 83.590.06 84.40.29
MNLI 83.50.2 84.20.2 83.90.2 84.90.2
(A) Increasing number of heads
head size 32 64 128 256
# params 130M 142M 168M 218M
SQuAD - F1 88.530.06 89.510.15 89.60.17 90.330.23
SQuAD - EM 81.190.21 82.410.32 82.730.21 83.360.48
MNLI 82.50.1 83.40.3 83.50.2 83.90.2
(B) Increasing head size
Table 2: Ablation studies on SQuAD and MNLI: (A) 24 layer Transformer with a ﬁxed head size of 128 and 512
embedding size shows an improvement in the accuracy with the increasing number of heads. (B) The ﬁxed head size
model with 512 embedding size and 8 heads shows an improvement in accuracy with the increasing head size. This
shows that indeed head size is an important capacity controlling parameter in the self attention architecture.
32k vocab and cap the input to 256 sequence length. We train a 6 layer Transformer model with the ADAM optimizer
using the tensor2tensor library [Vaswani et al., 2018]. The detailed experimental setting is presented in Section C.
Multi-Genre Natural Language Inference (MNLI) is a sentence level entailment task, designed to test natural
language understanding [Williams et al., 2018]. Given a premise sentence and a hypothesis sentence, the goal is to
predict whether hypothesis entails, contradicts or is neutral to the premise. We report the classiﬁcation accuracy for
this task. Stanford Question Answering Dataset (SQuAD) is a question answering dataset, where given a paragraph
and a question, the goal is to predict the sequence of words in the paragraph that constitute the answer to the question
[Rajpurkar et al., 2016]. This is a harder word level task, compared to the sentence classiﬁcation task. We report both
Exact Match (EM) and F1 scores for this task. All results in this section are reported on the Dev set, which has not been
used in any experimental choices in this paper.
For these latter two tasks, we follow the two stage approach of ﬁrst pre-training on a language modeling task
and then ﬁne-tuning the models on the task data. We follow the same experimental setup for both pre-training and
ﬁne-tuning as BERT [Devlin et al., 2018], and use their codebase1. We ﬁrst pre-train our models using the masked
language model and the next sentence prediction objectives, and then ﬁne tune the pre-trained model for individual tasks
[Devlin et al., 2018]. For pre-training we use English Wikipedia and BooksCorpus dataset [Zhu et al., 2015]. The input
to the models is tokenized using the WordPiece representation with 30000 tokens in the vocabulary. We present the key
experiment choices in Section C, and refer the reader to Devlin et al. [2018] for a complete description of the setup.
Choice of the head size. Our proposed modiﬁcation introduces head size dpas a new model hyper-parameter. We
choose head size to be 128for our BERT experiments, as most of the pre-training is done with 128 sequence length
data. While we have ablation studies (cf. Table 2(B)) showing bigger head size improves the performance, there is a
tradeoff between increasing the head size vs number of heads vs layers. We found that having sufﬁciently large head
size, e.g., matching the pre-training sequence length, is better than having a larger embedding size.
4.2 Results
For our ﬁrst set of experiments we want to see if Transformers trained with a ﬁxed head size and a smaller embedding
size can match the performance of training with the standard head size heuristic but with a larger embedding size. As a
baseline for the language modeling task, we train Transformers with the embedding size increasing from 256 to 512
1https://github.com/google-research/bert
9

--- PAGE 10 ---
with different number of heads. We train the ﬁxed head size models with a ﬁxed embedding size of 256 and a head size
of 32, with an increasing number of heads from 4 to 70. We notice that Transformers with a ﬁxed head size and an
embedding size of 256 have better performance than the baseline models with an embedding size of 512 (see Fig. 1).
We repeat the similar experiment on the other two tasks, where for baseline we train BERT LARGE , a 24 layer, 16 head
Transformer with the standard head size heuristic, with embedding sizes from 512 to 1024. We compare it with the
ﬁxed head size model, with an embedding size of 512 and a head size of 128, with an increasing number of heads
from 8 to 32. We again notice that the Transformers trained with a ﬁxed head size and 512 embedding size have better
performance than the baseline, BERT LARGE (see Fig. 2).
Note that simply trying to increase the head size of the Transformers by decreasing the number of heads does not
improve the performance, as decreasing the number of heads reduces the expressive power of the model (see Fig. 4 in
the Appendix). Hence, both the head size and the number of heads needs to be set high enough for better performance.
4.3 Ablation
Increasing heads. From Table 1 and Fig. 1a we can see that increasing the number of heads hurts the performance of
the Transformer after a certain number. We repeat the same experiments with the ﬁxed head size Transformer, and
present the results in Table 2(A) and Fig. 3a. The results show that the performance of the modiﬁed model improves
monotonically as the number of heads increase. This is because the model capacity (a function of the head size) is no
longer reduced with the increasing number of heads.
Increasing head size. In Table 2(B) and Fig. 3b, we present comparisons between models with different head sizes.
This shows that the gains in the performance of the ﬁxed head size models indeed come from adjusting the head size of
the query, key and value layers in the attention unit. The table shows a clear trend of better performance with a larger
head size, suggesting that it indeed is an important factor in the performance of the attention models.
5 Conclusion
In this paper we studied the representation power of the multi-head self attention models and proved the low-rank
bottleneck that results from a small head size in the multi-head attention. We showed that the larger embedding size
used in the current models is a consequence of this low-rank bottleneck in multi-head attention layers. We propose to
instead use ﬁxed head size attention units, with the head size set to input sequence length, to avoid this bottleneck. We
showed that it allows us to increase the number of heads without increasing the embedding size. As a consequence we
are able to train Transformers with a smaller embedding size and fewer parameters, with better performance. In the
future, it will be interesting to experiment with varying head sizes within an attention block and across layers. This
requires further understanding of the role of each layer in computing the context, which is an interesting direction for
the future work.
References
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.
C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, and P. Koehn. One billion word benchmark for measuring progress
in statistical language modeling. CoRR , abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005 .
R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint
arXiv:1904.10509 , 2019.
C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina,
et al. State-of-the-art speech recognition with sequence-to-sequence models. In 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , pages 4774–4778. IEEE, 2018.
10

--- PAGE 11 ---
K. Cho, B. van Merrienboer, D. Bahdanau, and Y . Bengio. On the properties of neural machine translation: Encoder–
decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical
Translation , pages 103–111, 2014.
G. M. Correia, V . Niculae, and A. F. Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 2174–2184, 2019.
M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser. Universal transformers. arXiv preprint
arXiv:1807.03819 , 2018.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language
understanding. arXiv preprint arXiv:1810.04805 , 2018.
J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin. Convolutional sequence to sequence learning. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 1243–1252. JMLR. org,
2017.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780, 1997.
Y . Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274 , 2017.
P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing
long sequences. arXiv preprint arXiv:1801.10198 , 2018.
P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? arXiv preprint arXiv:1905.10650 , 2019.
M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation. In Proceedings of the Third
Conference on Machine Translation: Research Papers , pages 1–9, 2018.
A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training.
Technical report, OpenAI , 2018.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask
learners. Technical report, OpenAI , 2019.
P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv
preprint arXiv:1606.05250 , 2016.
H. Sun, X. Tan, J.-W. Gan, H. Liu, S. Zhao, T. Qin, and T.-Y . Liu. Token-level ensemble distillation for grapheme-to-
phoneme conversion. arXiv preprint arXiv:1904.03446 , 2019.
I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks. In Advances in neural
information processing systems , pages 3104–3112, 2014.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all
you need. In Advances in Neural Information Processing Systems , pages 5998–6008, 2017.
A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar,
R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2tensor for neural machine translation. CoRR , abs/1803.07416, 2018.
URLhttp://arxiv.org/abs/1803.07416 .
E. V oita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-head self-attention: Specialized heads do the
heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418 , 2019.
X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 7794–7803, 2018.
11

--- PAGE 12 ---
A. Williams, N. Nangia, and S. Bowman. A broad-coverage challenge corpus for sentence understanding through
inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 1112–1122. Association for
Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101 .
F. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions.
arXiv preprint arXiv:1901.10430 , 2019.
Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. Breaking the softmax bottleneck: A high-rank rnn language
model. arXiv preprint arXiv:1711.03953 , 2017.
Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized autoregressive pretraining
for language understanding. arXiv preprint arXiv:1906.08237 , 2019.
Y . You, J. Li, J. Hseu, X. Song, J. Demmel, and C.-J. Hsieh. Reducing bert pre-training time from 3 days to 76 minutes.
arXiv preprint arXiv:1904.00962 , 2019.
C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of
sequence-to-sequence functions? arXiv preprint arXiv:1912.10077 , 2019.
V . Zambaldi, D. Raposo, A. Santoro, V . Bapst, Y . Li, I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart,
et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830 , 2018.
H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. arXiv preprint
arXiv:1805.08318 , 2018.
Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies:
Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE
international conference on computer vision , pages 19–27, 2015.
12

--- PAGE 13 ---
A Notation
Embedding size d
Number of layers l
Number of heads h
Sequence length n
V ocab size v
Head size dp
B Proofs
Proof of Theorem 2. First let us rewrite the MultiHead and FixedMultiHead layers as follows. The MultiHead layer
can be rewritten as
fW(X) =WoMultiHead (X) =hX
i=1Wi
oWi
vXSoftmax
(Wi
kX)T(Wi
qX)=p
d
h
;
where Wi
oaredd=hmatrices and Wi
v,Wi
k, andWi
qared=hdmatrices. We denote the collection of all parameter
matrices as W.
Similarly, rewrite the ﬁxed head size attention layer as
gV(X) =VoFixedMultiHead (X) =hX
i=1Vi
oVi
vXSoftmax
(Vi
kX)T(Vi
qX)=p
dp
;
where Vi
o2Rddp, andVi
v;Vi
k;Vi
q2Rdpd. LetVbe the collection of all these matrices.
The outline of the proof is basically a case analysis: we divide possible values of Winto three categories, and show
in each case that there exists a Xsuch thatfW(X)6=gV(X). Here are the three cases:
•Case 1 :Ph
i=1Wi
oWi
v6=Ph
i=1Vi
oVi
v.
•Case 2 :Ph
i=1Wi
oWi
v=Ph
i=1Vi
oVi
v, and there exists i2f1;:::;hgsuch that U=p
dp (Wi
k)T(Wi
q)=p
d=h
is not skew-symmetric.
•Case 3 :Ph
i=1Wi
oWi
v=Ph
i=1Vi
oVi
v, and all U=p
dp (Wi
k)T(Wi
q)=p
d=hare skew-symmetric.
Case 1. In the ﬁrst case, we can choose any vsuch that (Ph
i=1Wi
oWi
v Ph
i=1Vi
oVi
v)v6=0. Choose X=v1T=
v v:::v
. Then, note that for any column stochastic matrix P, we have XP=X. Therefore,
hX
i=1Wi
oWi
vXSoftmax
(Wi
kX)T(Wi
qX)=p
d=h
 hX
i=1Vi
oVi
vXSoftmax
(Vi
kX)T(Vi
qX)=p
dp
=hX
i=1Wi
oWi
vX hX
i=1Vi
oVi
vX= (hX
i=1Wi
oWi
v hX
i=1Vi
oVi
v)v1T6=0:
Case 2. In cases wherePh
i=1Wi
oWi
v=Ph
i=1Vi
oVi
v, sincePh
i=1Vi
oVi
vis full rank by assumption and each
Wi
oWi
vis at most rank d=h, it follows that all columns in Wi
o2Rdd=hmust be linearly independent. Therefore,
for any v6=0,fWi
oWi
vv;i= 1;:::;hgis a set of linearly independent vectors, because each Wi
oWi
vvis a linear
combination of d=hcolumn vectors of Wi
othat are linearly independent of other column vectors in Wj
o,j6=i.
13

--- PAGE 14 ---
Now consider any v2Rd, andX=veT
1, where e1= (1;0;:::; 0)2Rn. Deﬁne(t) = exp(t)=(exp(t)+n 1).
Then, we have
gV(X) =hX
i=1Vi
oVi
vXSoftmax
XTUX=p
dp
=hX
i=1Vi
oVi
vXSoftmax2
66664vTUvp
dp0::: 0
0 0::: 0
............
0 0::: 03
77775
= hX
i=1Vi
oVi
v!

vTUvp
dp
vv
n:::v
n
= hX
i=1Wi
oWi
v!

vTUvp
dp
vv
n:::v
n
:
Similarly, we can calculate
fW(X) =hX
i=1Wi
oWi
vXSoftmax
(Wi
kX)T(Wi
qX)=p
d=h
=hX
i=1Wi
oWi
v

vT(Wi
k)TWi
qvp
d=h
vv
n:::v
n
:
Notice that all the columns of fW(X)andgV(X), from the second columns to the last ones, are the same. We now
compare the ﬁrst columns:
fW(X):;1 gV(X):;1=hX
i=1 
 
vT(Wi
k)TWi
qvp
d=h!
  
vTUvp
dp!!
Wi
oWi
vv:
Recall that for any v6=0,Wi
oWi
vvare linearly independent, so fW(X):;1 gV(X):;1=0if and only if
all
vT(Wi
k)TWi
qvp
d=h
 
vTUvp
dp
are zero. However, since there exists i2f1;:::;hgsuch that U=p
dp 
(Wi
k)T(Wi
q)=p
d=his not skew-symmetric, we can choose vto be one that satisﬁesvT(Wi
k)TWi
qvp
d=h6=vTUvp
dp, hence
making
vT(Wi
k)TWi
qvp
d=h
 
vTUvp
dp
6= 0, thereforefW(X):;1 gV(X):;16=0.
Case 3. Now consider any X=v1v20:::0
, where v1andv2will be chosen later. Deﬁne 1(t1;t2) =
exp(t1)=(exp(t1) + exp(t2) +n 2),2(t1;t2) = exp(t2)=(exp(t1) + exp(t2) +n 2). Then, we have
gV(X) =hX
i=1Vi
oVi
vXSoftmax2
66666664vT
1Uv1p
dpvT
1Uv2p
dp0::: 0
vT
2Uv1p
dpvT
2Uv2p
dp0::: 0
0 0 0 ::: 0
...............
0 0 0 ::: 03
77777775:
Therefore, the ﬁrst column of gV(X)can be written as
gV(X):;1= hX
i=1Wi
oWi
v!"
1 
vT
1Uv1p
dp;vT
2Uv1p
dp!
v1+2 
vT
1Uv1p
dp;vT
2Uv1p
dp!
v2#
:
14

--- PAGE 15 ---
Similarly, the ﬁrst column of fW(X)is
fW(X):;1=hX
i=1Wi
oWi
v"
1 
vT
1(Wi
k)TWi
qv1p
d=h;vT
2(Wi
k)TWi
qv1p
d=h!
v1+
2 
vT
1(Wi
k)TWi
qv1p
d=h;vT
2(Wi
k)TWi
qv1p
d=h!
v2#
:
Since U=p
dp (W1
k)T(W1
q)=p
d=his skew-symmetric by assumption, we have vT
1
Up
dp (W1
k)T(W1
q)p
d=h
v1= 0
for all v1. Recall that Uis rank-dpby assumption, so U=p
dp (W1
k)T(W1
q)=p
d=his at least rank dp d=h1,
so we can choose any v1such that
Up
dp (W1
k)T(W1
q)p
d=h
v16=0.
If bothUp
dpv1and(W1
k)T(W1
q)p
d=hv1are nonzero, We can always choose ~v2such that ~vT
2
Up
dp
v1>0and
~vT
2
(W1
k)T(W1
q)p
d=h
v1<0. This means that if we choose v2=~v2and scale!1 ,
1 
vT
1Uv1p
dp;vT
2Uv1p
dp!
!0; 2 
vT
1Uv1p
dp;vT
2Uv1p
dp!
!1;
1 
vT
1(W1
k)TW1
qv1p
d=h;vT
2(W1
k)TW1
qv1p
d=h!
!exp(vT
1(W1
k)TW1
qv1=p
d=h)
exp(vT
1(W1
k)TW1qv1=p
d=h) +n 2;
2 
vT
1(W1
k)TW1
qv1p
d=h;vT
2(W1
k)TW1
qv1p
d=h!
!0:
Then, consider the difference fW(X):;1 gV(X):;1. Recall that for any v,W1
oW1
vvis independent offWi
oWi
vv;i6=
1g. This means that, to show fW(X):;1 gV(X):;16=0, it sufﬁces to show that
"
1 
vT
1(W1
k)TW1
qv1p
d=h;vT
2(W1
k)TW1
qv1p
d=h!
 1 
vT
1Uv1p
dp;vT
2Uv1p
dp!#
W1
oW1
vv1+
"
2 
vT
1(W1
k)TW1
qv1p
d=h;vT
2(W1
k)TW1
qv1p
d=h!
 2 
vT
1Uv1p
dp;vT
2Uv1p
dp!#
W1
oW1
vv26=0:
If we scale v2=~v2with large enough , the second term will dominate the ﬁrst term and the ﬁrst term will never be
able to cancel the second one. Thus, by choosing large enough >0, we can make sure that the sum is nonzero.
Even in case where one ofUp
dpv1and(W1
k)T(W1
q)p
d=hv1is zero (say(W1
k)T(W1
q)p
d=hv1=0), we can choose ~v2=Up
dpv1
and use a similar scaling argument. By choosing large enough >0andv2=~v2, one can show that the difference
fW(X):;1 gV(X):;1is nonzero.
C Experimental settings
For our experiments with the language modeling (LM1B dataset), we train 6 layer Transformer models. We use a batch
size of 4096 and train for 250k steps. We use a learning rate of 0.1 with a linear warm up for the ﬁrst 10k steps. We
decay the learning rate with the square root of the number of steps. We train the baseline models, with the prevalent
head size heuristic, with the embedding dimension varying from 256 to 512. We ﬁx the width of the feed forward layer
in the Transformer to be 1024. In addition, we use weight decay of 0.01 and dropout with probability of 0.1 on all the
layers.
15

--- PAGE 16 ---
For our experiments with BERT, we follow the same experimental settings as in [Devlin et al., 2018]. We present
the key details here and refer the reader to [Devlin et al., 2018]. We train with a batch size of 1024 for 450k steps with
inputs of sequence length n= 128 followed by 50k steps with inputs of sequence length 512. In contrast the BERT
paper uses a batch size of 512, and does the pre-training for 900K steps with 128 sequence length inputs and 100k steps
with 512 sequence length inputs. We train using ADAM with a learning rate of 1e-4, and a linear warmup and decay
schedule as in BERT. We use 5k warmup steps for the ﬁrst stage, and a re-warmup of 3k steps for the second stage [You
et al., 2019]. Again, we use weight decay of 0.01 and dropout with probability of 0.1 on all the layers.
For the language modeling task, training is performed on 4 TPUv2 chips for a couple of hours. For BERT models
training is performed on 16 TPUv3 chips in the ﬁrst stage and 64 TPUv3 chips for the second stage. Pre-training with
this conﬁguration takes between 2 to 3 days. We did not attempt to ﬁnd the optimal hyper-parameters for the ﬁxed head
size architecture, and use the same hyper-parameters as used for training the BERT models.
D Additional experimental results
Figure 4: Performance of the Transformers trained with the prevalent head size heuristic (baseline) compared with
the ﬁxed head size ( dp) models for a language modeling task (LM1B) on the test set. Unlike Fig.1, we vary both the
embedding size and the number of heads of the baseline models to keep their head size ﬁxed to 32. We train the ﬁxed
head size models with a ﬁxed embedding size of 256 and a head size of 32, and vary the number of heads from 4 to
70, while matching the number of parameters. The plot again clearly indicates the advantage of the ﬁxed head size
models. The main issue with the baseline models is that ﬁxing the head size to 32 forces the number of heads to be
small when the embedding size is small. Reducing the number of heads below certain threshold hurts the performance
of the Transformer.
16

--- PAGE 17 ---
# heads 8 12 16 20
# params 214M 252M 290M 327M
SQuAD - F1 90.350.14 90.480.09 90.920.14 90.890.08
SQuAD - EM 83.370.12 83.670.03 84.160.35 84.290.16
MNLI 84.40.2 84.40.2 84.70.1 85.10.4
(A) Increasing number of heads
Table 3: (A): 24 layer Transformer trained with a ﬁxed head size of 128 and an embedding size of 768 shows an
improvement in the accuracy with the increasing number of heads.
17
