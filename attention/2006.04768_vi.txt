I'll translate this technical paper from English to Vietnamese while maintaining the exact same structure and content.

# 2006.04768.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2006.04768.pdf
# Kích thước file: 922879 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Linformer: Cơ chế Tự-Chú-Ý với Độ Phức Tạp Tuyến Tính
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma
Facebook AI, Seattle, WA
{sinongwang, belindali, hanfang, mkhabsa, haom}@fb.com

Tóm tắt
Các mô hình transformer lớn đã cho thấy thành công phi thường trong việc đạt được các kết quả tối ưu trong nhiều ứng dụng xử lý ngôn ngữ tự nhiên. Tuy nhiên, việc huấn luyện và triển khai các mô hình này có thể tốn kém cấm đoán cho các chuỗi dài, vì cơ chế tự-chú-ý tiêu chuẩn của Transformer sử dụng O(n²) thời gian và không gian với độ dài chuỗi. Trong bài báo này, chúng tôi chứng minh rằng cơ chế tự-chú-ý có thể được xấp xỉ bằng một ma trận hạng thấp. Chúng tôi tiếp tục khai thác khám phá này để đề xuất một cơ chế tự-chú-ý mới, giảm độ phức tạp tự-chú-ý tổng thể từ O(n²) xuống O(n) về cả thời gian và không gian. Transformer tuyến tính kết quả, Linformer, hoạt động ngang bằng với các mô hình Transformer tiêu chuẩn, trong khi hiệu quả hơn nhiều về bộ nhớ và thời gian.

1 Giới thiệu
Các mô hình Transformer (Vaswani et al., 2017) đã trở nên phổ biến cho nhiều loại vấn đề trong xử lý ngôn ngữ tự nhiên (NLP), bao gồm dịch thuật (Ott et al., 2018), phân loại văn bản, trả lời câu hỏi, và nhiều ứng dụng khác (Raffel et al., 2019; Mohamed et al., 2019). Trong vài năm qua, số lượng tham số trong các transformer NLP tiên tiến đã tăng mạnh, từ 340 triệu ban đầu được giới thiệu trong BERT-Large đến 175 tỷ trong GPT-3 (Brown et al., 2020). Mặc dù các mô hình quy mô lớn này mang lại kết quả ấn tượng trên nhiều tác vụ khác nhau, việc huấn luyện và triển khai mô hình như vậy chậm trong thực tế. Ví dụ, mô hình BERT-Large gốc (Devlin et al., 2019) mất bốn ngày để huấn luyện trên 16 Cloud TPU, và GPT-3 gần đây (Brown et al., 2020) tiêu tốn nhiều bậc độ lớn petaflops/ngày để huấn luyện so với người tiền nhiệm, GPT-2 (Radford et al., 2019). Ngoài huấn luyện, triển khai các mô hình Transformer vào ứng dụng thế giới thực cũng tốn kém, thường yêu cầu chưng cất rộng rãi (Hinton et al., 2015) hoặc nén.

Nút thắt hiệu quả chính trong các mô hình Transformer là cơ chế tự-chú-ý của nó. Ở đây, biểu diễn của mỗi token được cập nhật bằng cách chú ý đến tất cả các token khác trong lớp trước đó. Hoạt động này là chìa khóa để giữ lại thông tin dài hạn, mang lại cho Transformer lợi thế so với các mô hình tuần tự trên các chuỗi dài. Tuy nhiên, việc chú ý đến tất cả các token ở mỗi lớp gây ra độ phức tạp O(n²) với độ dài chuỗi. Do đó, trong bài báo này, chúng tôi tìm cách trả lời câu hỏi: liệu các mô hình Transformer có thể được tối ưu hóa để tránh phép toán bậc hai này, hay phép toán này cần thiết để duy trì hiệu suất mạnh?

Các công trình trước đây đã đề xuất một số kỹ thuật để cải thiện hiệu quả của tự-chú-ý. Một kỹ thuật phổ biến là đưa tính thưa thớt vào các lớp chú ý (Child et al., 2019; Qiu et al., 2019; Beltagy et al., 2020) bằng cách để mỗi token chỉ chú ý đến một tập con các token trong toàn bộ chuỗi. Điều này giảm độ phức tạp tổng thể của cơ chế chú ý xuống O(n√n) (Child et al., 2019). Tuy nhiên, như được chỉ ra trong Qiu et al. (2019), phương pháp này gặp phải sự sụt giảm hiệu suất lớn với những cải thiện hiệu quả hạn chế, tức là giảm 2% với chỉ 20% tăng tốc. Gần đây hơn, Reformer (Kitaev et al., 2020) đã sử dụng băm nhạy cảm cục bộ (LSH) để giảm độ phức tạp tự-chú-ý xuống O(n log(n)). Tuy nhiên, trong thực tế, những cải thiện hiệu quả của Reformer chỉ xuất hiện trên các chuỗi có độ dài >2048 (Hình 5 trong Kitaev et al. (2020)). Hơn nữa, phương pháp băm đa vòng của Reformer thực sự tăng số lượng các hoạt động tuần tự, điều này làm suy giảm thêm những cải thiện hiệu quả cuối cùng của họ.

Bản thảo. Đang xem xét.arXiv:2006.04768v3 [cs.LG] 14 Jun 2020

--- TRANG 2 ---
Trong nghiên cứu này, chúng tôi giới thiệu một phương pháp mới để giải quyết nút thắt tự-chú-ý trong Transformer. Phương pháp của chúng tôi được truyền cảm hứng từ quan sát quan trọng rằng tự-chú-ý có hạng thấp. Cụ thể hơn, chúng tôi chỉ ra cả về mặt lý thuyết và thực nghiệm rằng ma trận ngẫu nhiên được hình thành bởi tự-chú-ý có thể được xấp xỉ bằng một ma trận hạng thấp. Được trao quyền bởi quan sát này, chúng tôi giới thiệu một cơ chế mới giảm tự-chú-ý xuống phép toán O(n) về cả độ phức tạp không gian và thời gian: chúng tôi phân tách chú ý tích vô hướng có tỷ lệ gốc thành nhiều chú ý nhỏ hơn thông qua các phép chiếu tuyến tính, sao cho sự kết hợp của các phép toán này tạo thành một phân tích nhân tử hạng thấp của chú ý gốc. Một tóm tắt thời gian chạy cho các kiến trúc Transformer khác nhau, bao gồm của chúng tôi, có thể được tìm thấy trong Bảng 1.

Một ứng dụng chủ yếu của Transformer, đã thấy nhiều cải tiến nhất, là sử dụng chúng như các mô hình ngôn ngữ được huấn luyện trước, trong đó các mô hình đầu tiên được huấn luyện trước với mục tiêu mô hình hóa ngôn ngữ trên một corpus lớn, sau đó được tinh chỉnh trên các tác vụ đích sử dụng dữ liệu có giám sát (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2019). Theo Devlin et al. (2019), chúng tôi huấn luyện trước mô hình của chúng tôi trên BookCorpus (Zhu et al., 2015) cộng với English Wikipedia sử dụng mục tiêu mô hình hóa ngôn ngữ có mặt nạ. Chúng tôi quan sát hiệu suất huấn luyện trước tương tự với mô hình Transformer tiêu chuẩn. Sau đó chúng tôi tinh chỉnh các mô hình được huấn luyện trước của chúng tôi trên ba tác vụ từ GLUE (Wang et al., 2018) và một tác vụ phân tích cảm xúc, đánh giá IMDB (Maas et al., 2011). Trên các tác vụ này, chúng tôi thấy rằng mô hình của chúng tôi hoạt động tương đương, hoặc thậm chí tốt hơn một chút, so với Transformer được huấn luyện trước tiêu chuẩn, trong khi quan sát được những cải thiện đáng kể về tốc độ huấn luyện và suy luận.

Kiến trúc Mô hình | Độ phức tạp mỗi lớp | Phép toán tuần tự
Tái phát | O(n) | O(n)
Transformer, (Vaswani et al., 2017) | O(n²) | O(1)
Sparse Transformer, (Child et al., 2019) | O(n√n) | O(1)
Reformer, (Kitaev et al., 2020) | O(n log(n)) | O(log(n))
Linformer | O(n) | O(1)

Bảng 1: Độ phức tạp thời gian mỗi lớp và số lượng tối thiểu các phép toán tuần tự như một hàm của độ dài chuỗi (n) cho các kiến trúc khác nhau.

2 Bối cảnh và Các nghiên cứu liên quan

2.1 Transformer và Tự-Chú-Ý

Transformer được xây dựng dựa trên ý tưởng của Multi-Head Self-Attention (MHA), cho phép mô hình cùng lúc chú ý đến thông tin tại các vị trí khác nhau từ các không gian biểu diễn con khác nhau. MHA được định nghĩa là

MultiHead(Q;K;V) = Concat(head₁;head₂;...;headₕ)W^O, (1)

trong đó Q;K;V ∈ R^(n×d_m) là các ma trận nhúng đầu vào, n là độ dài chuỗi, d_m là chiều nhúng, và h là số đầu. Mỗi đầu được định nghĩa như:

head_i = Attention(QW^Q_i;KW^K_i;VW^V_i) = softmax[QW^Q_i(KW^K_i)^T/√d_k] × VW^V_i, (2)

trong đó W^Q_i;W^K_i ∈ R^(d_m×d_k); W^V_i ∈ R^(d_m×d_v); W^O ∈ R^(hd_v×d_m) là các ma trận học được và d_k;d_v là các chiều ẩn của các không gian chiếu con. Trong phần còn lại của bài báo này, chúng tôi sẽ không phân biệt giữa d_k và d_v và chỉ sử dụng d.

Tự-chú-ý được định nghĩa trong (2) ám chỉ một ma trận ánh xạ ngữ cảnh P ∈ R^(n×n). Transformer sử dụng P để nắm bắt ngữ cảnh đầu vào cho một token đã cho, dựa trên sự kết hợp của tất cả các token trong chuỗi. Tuy nhiên, tính toán P tốn kém. Nó yêu cầu nhân hai ma trận n×d_m, có độ phức tạp O(n²) về thời gian và không gian. Sự phụ thuộc bậc hai này vào độ dài chuỗi đã trở thành nút thắt cho Transformer.

2.2 Các nghiên cứu liên quan

Đã có nhiều tài liệu trước đây về cải thiện hiệu quả của Transformer, đặc biệt là nút thắt tự-chú-ý. Các kỹ thuật phổ biến nhất cho hiệu quả mô hình có thể được áp dụng cho Transformer (một số cụ thể cho Transformer, những cái khác có mục đích chung hơn) bao gồm:

--- TRANG 3 ---
Độ chính xác hỗn hợp (Micikevicius et al., 2017): Sử dụng biểu diễn nửa độ chính xác hoặc độ chính xác hỗn hợp của các điểm thực là phổ biến trong học sâu, và cũng được sử dụng rộng rãi trong huấn luyện Transformer (Ott et al., 2019). Kỹ thuật này có thể được cải thiện thêm thông qua Huấn luyện Nhận thức Lượng tử hóa (Jacob et al., 2018; Fan et al., 2020), trong đó các trọng số được lượng tử hóa trong quá trình huấn luyện và các gradient được xấp xỉ với Bộ ước lượng Thông qua Trực tiếp. Dòng công việc này trực giao với phương pháp của chúng tôi, và chúng tôi sử dụng huấn luyện độ chính xác hỗn hợp theo mặc định.

Chưng cất Kiến thức (Hinton et al., 2015): Chưng cất kiến thức nhằm chuyển giao "kiến thức" từ một mô hình giáo viên lớn sang một mô hình học sinh nhẹ. Mô hình học sinh sau đó được sử dụng trong quá trình suy luận. Tuy nhiên phương pháp này có nhược điểm: Nó không giải quyết việc tăng tốc mô hình giáo viên trong quá trình huấn luyện, và hơn nữa, các mô hình học sinh thường gặp phải sự suy giảm hiệu suất so với mô hình giáo viên. Ví dụ, khi chưng cất một BERT 12 lớp thành một BERT 6 lớp, mô hình học sinh trải qua sự sụt giảm hiệu suất trung bình 2.5% trên một số tác vụ chuẩn (Sanh et al., 2019).

Chú ý Thưa thớt (Child et al., 2019): Kỹ thuật này cải thiện hiệu quả của tự-chú-ý bằng cách thêm tính thưa thớt vào ma trận ánh xạ ngữ cảnh P. Ví dụ, Sparse Transformer (Child et al., 2019) chỉ tính toán P_ij xung quanh đường chéo của ma trận P (thay vì tất cả P_ij). Trong khi đó, tự-chú-ý theo khối (Qiu et al., 2019) chia P thành nhiều khối và chỉ tính toán P_ij trong các khối được chọn. Tuy nhiên, các kỹ thuật này cũng gặp phải sự suy giảm hiệu suất lớn, trong khi chỉ có tăng tốc bổ sung hạn chế, tức là giảm 2% với tăng tốc 20%.

Chú ý LSH (Kitaev et al., 2020): Chú ý băm nhạy cảm cục bộ (LSH) sử dụng một sơ đồ băm đa vòng khi tính toán chú ý tích vô hướng, về mặt lý thuyết giảm độ phức tạp tự-chú-ý xuống O(n log(n)). Tuy nhiên, trong thực tế, hạng tử độ phức tạp của họ có một hằng số lớn 128² và nó chỉ hiệu quả hơn transformer vani khi độ dài chuỗi cực kỳ dài.

Cải thiện Hiệu quả Bộ tối ưu hóa: Microbatching (Huang et al., 2019) chia một batch thành các microbatch nhỏ (có thể vừa với bộ nhớ), và sau đó chạy riêng biệt các lượt truyền tiến và lùi trên chúng với tích lũy gradient. Gradient checkpointing (Chen et al., 2016) tiết kiệm bộ nhớ bằng cách chỉ lưu cache các kích hoạt của một tập con các lớp. Các kích hoạt không được cache được tính toán lại trong quá trình lan truyền ngược từ checkpoint mới nhất. Cả hai kỹ thuật đều đánh đổi thời gian cho bộ nhớ, và không tăng tốc suy luận.

Như chúng tôi đã lưu ý, hầu hết các kỹ thuật phổ biến có hạn chế trong việc giảm cả thời gian huấn luyện và suy luận/tiêu thụ bộ nhớ, chúng tôi điều tra cách tối ưu hóa các lớp tự-chú-ý và giới thiệu phương pháp của chúng tôi tiếp theo.

3 Tự-Chú-Ý có Hạng Thấp

Trong phần này, chúng tôi chứng minh rằng cơ chế tự-chú-ý, tức là ma trận ánh xạ ngữ cảnh P, có hạng thấp.

Chúng tôi đầu tiên cung cấp một phân tích phổ của ma trận ánh xạ ngữ cảnh P. Chúng tôi sử dụng hai mô hình transformer được huấn luyện trước, RoBERTa-base (transformer xếp chồng 12 lớp) và RoBERTa-large (transformer xếp chồng 24 lớp) (Liu et al., 2019) trên hai tác vụ: tác vụ mô hình hóa ngôn ngữ có mặt nạ trên Wiki103 (Merity et al., 2016) và tác vụ phân loại trên IMDB (Maas et al., 2011). Trong Hình 1 (trái), chúng tôi áp dụng phân tích giá trị suy biến vào P qua các lớp và đầu khác nhau của mô hình, và vẽ giá trị suy biến tích lũy được chuẩn hóa trung bình trên 10k câu. Kết quả thể hiện một phân phối phổ đuôi dài rõ ràng qua mỗi lớp, đầu và tác vụ. Điều này ám chỉ rằng hầu hết thông tin của ma trận P có thể được khôi phục từ vài giá trị suy biến lớn nhất đầu tiên. Trong Hình 1 (phải), chúng tôi vẽ một bản đồ nhiệt của giá trị suy biến tích lũy được chuẩn hóa tại giá trị suy biến lớn thứ 128 (trong số 512). Chúng tôi quan sát rằng phân phối phổ ở các lớp cao hơn bị lệch hơn so với ở các lớp thấp hơn, có nghĩa là, ở các lớp cao hơn, nhiều thông tin được tập trung vào các giá trị suy biến lớn nhất và hạng của P thấp hơn.

Dưới đây, chúng tôi cung cấp một phân tích lý thuyết về các kết quả phổ trên.

Định lý 1. (tự-chú-ý có hạng thấp) Với bất kỳ Q;K;V ∈ R^(n×d) và W^Q_i;W^K_i;W^V_i ∈ R^(d×d), với bất kỳ vector cột w ∈ R^n của ma trận VW^V_i, tồn tại một ma trận hạng thấp P̃ ∈ R^(n×n) sao cho

Pr(‖P̃w^T - Pw^T‖ < ε‖Pw^T‖) > 1 - o(1) và rank(P̃) = O(log(n)), (3)

trong đó ma trận ánh xạ ngữ cảnh P được định nghĩa trong (2).

--- TRANG 4 ---
Chỉ số đầu | Chỉ số lớp | Chỉ số giá trị eigen
0    128    512 | 0    128    512 | Chỉ số giá trị eigen
Giá trị eigen tích lũy chuẩn hóa | Transformer 12 lớp | Transformer 24 lớp
0.96 0.94 0.92 0.90 0.88 | 0  1  2  3  4  5  6  7  8  9  10 11 | 0 1 2 3 4 5 6 7 8 9 10 11
1.0 0.9 0.8 0.7 0.6 0.5 0.4

Hình 1: Hai hình bên trái là phân tích phổ của ma trận tự-chú-ý trong mô hình transformer được huấn luyện trước (Liu et al., 2019) với n = 512. Trục Y là giá trị suy biến tích lũy chuẩn hóa của ma trận ánh xạ ngữ cảnh P, và trục X là chỉ số của giá trị eigen lớn nhất. Kết quả được dựa trên cả mô hình RoBERTa-base và large trong hai tập dữ liệu công cộng: Wiki103 và IMDB. Hình bên phải vẽ bản đồ nhiệt của giá trị eigen tích lũy chuẩn hóa tại giá trị eigen lớn thứ 128 qua các lớp và đầu khác nhau trong dữ liệu Wiki103.

Chứng minh. Dựa trên định nghĩa của ma trận ánh xạ ngữ cảnh P, chúng ta có thể viết

P = softmax[QW^Q_i(KW^K_i)^T/√d] = exp(A)D^(-1)_A, (4)

trong đó D_A là một ma trận đường chéo n×n. Ý tưởng chính của chứng minh này dựa trên bổ đề phân phối Johnson–Lindenstrauss (Lindenstrauss, 1984) (JL tắt). Chúng tôi xây dựng ma trận xấp xỉ hạng thấp như P̃ = exp(A)D^(-1)_A R^T R, trong đó R ∈ R^(k×n) với các mục i.i.d. từ N(0,1/k). Sau đó chúng ta có thể sử dụng bổ đề JL để chỉ ra rằng, với bất kỳ vector cột w ∈ R^n của ma trận VW^V_i, khi k = 5log(n)/(2ε³), chúng ta có

Pr[‖PR^T Rw^T - Pw^T‖ ≤ ε‖Pw^T‖] > 1 - o(1). (5)

Để biết thêm chi tiết, tham khảo tài liệu bổ sung.

Cho tính chất hạng thấp của ma trận ánh xạ ngữ cảnh P, một ý tưởng đơn giản là sử dụng phân tích giá trị suy biến (SVD) để xấp xỉ P với một ma trận hạng thấp P_low, như sau

P ≈ P_low = Σ^k_{i=1} σ_i u_i v^T_i = [u_1,...,u_k] diag{σ_1,...,σ_k} [v_1; ...; v_k]^T ∈ R^(n×k), (6)

trong đó σ_i, u_i và v_i là giá trị suy biến lớn thứ i và các vector suy biến tương ứng của chúng. Dựa trên kết quả trong Định lý 1 và Định lý Eckart–Young–Mirsky (Eckart & Young, 1936), người ta có thể sử dụng P_low để xấp xỉ tự-chú-ý (2) với lỗi ε và độ phức tạp thời gian và không gian O(nk). Tuy nhiên, phương pháp này yêu cầu thực hiện một phân tích SVD trong mỗi ma trận tự-chú-ý, điều này thêm độ phức tạp bổ sung. Do đó, chúng tôi đề xuất một phương pháp khác cho xấp xỉ hạng thấp tránh độ phức tạp được thêm vào này.

4 Mô hình

Trong phần này, chúng tôi đề xuất một cơ chế tự-chú-ý mới cho phép chúng tôi tính toán ánh xạ ngữ cảnh PVW^V_i trong độ phức tạp thời gian và bộ nhớ tuyến tính với độ dài chuỗi.

Ý tưởng chính của tự-chú-ý tuyến tính được đề xuất của chúng tôi (Hình 2) là thêm hai ma trận chiếu tuyến tính E_i, F_i ∈ R^(n×k) khi tính toán khóa và giá trị. Chúng tôi đầu tiên chiếu các lớp khóa và giá trị (n×d)-chiều gốc KW^K_i và VW^V_i thành các lớp khóa và giá trị chiếu (k×d)-chiều. Sau đó chúng tôi tính toán một ma trận ánh xạ ngữ cảnh (n×k)-chiều P sử dụng chú ý tích vô hướng có tỷ lệ.

head_i = Attention(QW^Q_i, E_i KW^K_i, F_i VW^V_i)
       = softmax(QW^Q_i(E_i KW^K_i)^T/√d_k) F_i VW^V_i, (7)

Cuối cùng, chúng tôi tính toán các nhúng ngữ cảnh cho mỗi đầu i sử dụng P(F_i VW^V_i). Lưu ý các phép toán trên chỉ yêu cầu độ phức tạp thời gian và không gian O(nk). Do đó, nếu chúng ta có thể chọn một chiều chiếu rất nhỏ k, sao cho k ≪ n, thì chúng ta có thể giảm đáng kể tiêu thụ bộ nhớ và không gian. Định lý sau đây nêu rằng, khi k = O(d/ε²) (độc lập với n), người ta có thể xấp xỉ PVW^V_i sử dụng tự-chú-ý tuyến tính (7) với lỗi ε.

Định lý 2. (Tự-chú-ý tuyến tính) Với bất kỳ Q_i;K_i;V_i ∈ R^(n×d) và W^Q_i;W^K_i;W^V_i ∈ R^(d×d), nếu k = min{⌈9d log(d)/ε²⌉, ⌈5(log(n)/ε²)⌉}, thì tồn tại các ma trận E_i;F_i ∈ R^(n×k) sao cho, với bất kỳ vector hàng w của ma trận QW^Q_i(KW^K_i)^T/√d, chúng ta có

Pr[‖softmax(wE^T_i)F_i VW^V_i - softmax(w)VW^V_i‖ ≤ ε‖softmax(w)‖‖VW^V_i‖] > 1 - o(1) (8)

Chứng minh. Ý tưởng chính của chứng minh dựa trên bổ đề phân phối Johnson–Lindenstrauss (Lindenstrauss, 1984). Chúng tôi đầu tiên chứng minh rằng với bất kỳ vector hàng x ∈ R^n của ma trận QW^Q_i(KW^K_i)^T/√d_k và vector cột y ∈ R^n của ma trận VW^V_i,

Pr[‖exp(xE^T_i)F_i y^T - exp(x)y^T‖ ≤ ε‖exp(x)y^T‖] > 1 - 2e^(-2ε³k/4), (9)

trong đó E_i = R và F_i = εR, trong đó R ∈ R^(k×n) với các mục i.i.d. từ N(0,1/k) và ε là một hằng số nhỏ. Áp dụng kết quả trong (9) cho mỗi vector hàng của ma trận A và mỗi vector cột của ma trận V, người ta có thể chứng minh trực tiếp rằng, với bất kỳ vector hàng A_i của ma trận A,

Pr[‖exp(A_i E^T_i)F_i V - exp(A_i)V‖ ≤ ε‖exp(A_i)V‖] > 1 - o(1), (10)

bằng cách đặt k = 5log(nd)/(2ε³). Kết quả này không sử dụng tính chất hạng thấp của ma trận A (rank(A) = d) và k kết quả có sự phụ thuộc vào độ dài chuỗi n. Chúng tôi sẽ tiếp tục sử dụng thực tế rằng rank(A) = d để chứng minh việc chọn k có thể là hằng số và độc lập với độ dài chuỗi n. Để biết thêm chi tiết, tham khảo tài liệu bổ sung.

--- TRANG 5 ---
Chú ý Tích Vô hướng Có tỷ lệ | Tuyến tính | Concat | Chú ý Tích Vô hướng Có tỷ lệ | Tuyến tính | Tuyến tính
V | K | Q | Chiếu | Chiếu | Tuyến tính | Tuyến tính | Tuyến tính | Tuyến tính
n×d_m | d_m×d_k | k×n | × | Linformer | × = k×d_k | W^O_i(W^V_i) | K(V) | Độ dài chuỗi/kích thước batch

Hình 2: Trái và dưới-phải cho thấy kiến trúc và ví dụ của tự-chú-ý tuyến tính đa đầu được đề xuất của chúng tôi. Trên phải cho thấy thời gian suy luận so với độ dài chuỗi cho các mô hình Linformer khác nhau.

Trong Hình 2 (trên phải), chúng tôi vẽ tốc độ suy luận của Linformer và Transformer tiêu chuẩn so với độ dài chuỗi, trong khi giữ tổng số token cố định. Chúng ta thấy rằng trong khi Transformer tiêu chuẩn trở nên chậm hơn ở độ dài chuỗi dài hơn, tốc độ Linformer vẫn tương đối ổn định và nhanh hơn đáng kể ở các chuỗi dài.

Các Kỹ thuật Hiệu quả Bổ sung: Một số kỹ thuật bổ sung có thể được giới thiệu trên Linformer để tối ưu hóa thêm cho cả hiệu suất và hiệu quả:

Chia sẻ tham số giữa các phép chiếu: Người ta có thể chia sẻ tham số cho các ma trận chiếu tuyến tính E_i;F_i qua các lớp và đầu. Cụ thể, chúng tôi thử nghiệm với 3 mức độ chia sẻ:

Chia sẻ theo đầu: cho mỗi lớp, chúng tôi chia sẻ hai ma trận chiếu E và F sao cho E_i = E và F_i = F qua tất cả các đầu i.

Chia sẻ khóa-giá trị: chúng tôi thực hiện chia sẻ theo đầu, với ràng buộc bổ sung của việc chia sẻ các phép chiếu khóa và giá trị. Cho mỗi lớp, chúng tôi tạo một ma trận chiếu duy nhất E sao cho E_i = F_i = E cho mỗi ma trận chiếu khóa-giá trị qua tất cả các đầu i.

Chia sẻ theo lớp: chúng tôi sử dụng một ma trận chiếu duy nhất E qua tất cả các lớp, cho tất cả các đầu, và cho cả khóa và giá trị.

Ví dụ, trong một mô hình Transformer xếp chồng 12 lớp, 12 đầu, chia sẻ theo đầu, chia sẻ khóa-giá trị và chia sẻ theo lớp sẽ giới thiệu 24, 12, và 1 ma trận chiếu tuyến tính khác biệt, tương ứng.

Chiều chiếu không đồng nhất: Người ta có thể chọn một chiều chiếu k khác nhau cho các đầu và lớp khác nhau. Như được chỉ ra trong Hình 1 (phải), các ma trận ánh xạ ngữ cảnh trong các đầu và lớp khác nhau có phân phối phổ khác biệt, và các đầu trong lớp cao hơn có xu hướng về phân phối phổ lệch hơn (hạng thấp hơn). Điều này ám chỉ người ta có thể chọn một chiều chiếu k nhỏ hơn cho các lớp cao hơn.

Phép chiếu tổng quát: Người ta cũng có thể chọn các loại phương pháp chiếu chiều thấp khác nhau thay vì một phép chiếu tuyến tính đơn giản. Ví dụ, người ta có thể chọn pooling trung bình/tối đa, hoặc tích chập trong đó kernel và stride được đặt thành n/k. Các hàm tích chập chứa các tham số yêu cầu huấn luyện.

5 Thí nghiệm

Trong phần này, chúng tôi trình bày kết quả thí nghiệm cho các kỹ thuật được mô tả ở trên. Chúng tôi phân tích các kỹ thuật từng cái một và khám phá cách chúng tác động đến hiệu suất.

5.1 Perplexity Huấn luyện trước

Chúng tôi đầu tiên so sánh hiệu suất huấn luyện trước của kiến trúc được đề xuất của chúng tôi với RoBERTa (Liu et al., 2019), dựa trên Transformer. Theo Devlin et al. (2019), chúng tôi sử dụng BookCorpus (Zhu et al., 2015) cộng với English Wikipedia làm tập huấn luyện trước của chúng tôi (3300M từ). Tất cả các mô hình được huấn luyện trước với mục tiêu mô hình hóa ngôn ngữ có mặt nạ (MLM), và huấn luyện cho tất cả các thí nghiệm được song song hóa trên 64 Tesla V100 GPU với 250k cập nhật.

Hiệu ứng của chiều chiếu: Chúng tôi thí nghiệm với các giá trị khác nhau cho chiều chiếu k. (Chúng tôi sử dụng cùng k qua tất cả các lớp và đầu của Linformer.) Trong Hình 3(a) và (b), chúng tôi vẽ các đường cong perplexity validation cho cả Transformer tiêu chuẩn và Linformer qua các k khác nhau, cho độ dài chuỗi tối đa n = 512 và n = 1024. Như mong đợi, Linformer hoạt động tốt hơn khi chiều chiếu k tăng. Tuy nhiên, thậm chí ở k = 128 cho n = 512 và k = 256 cho n = 1024, hiệu suất của Linformer đã gần như ngang bằng với Transformer gốc.

Hiệu ứng của việc chia sẻ phép chiếu: Trong Hình 3(c), chúng tôi vẽ các đường cong perplexity validation cho ba chiến lược chia sẻ tham số (theo đầu, khóa-giá trị, và theo lớp) với n = 512. Lưu ý rằng khi chúng tôi chỉ sử dụng một ma trận chiếu duy nhất (tức là cho chia sẻ theo lớp), perplexity validation của mô hình Linformer kết quả gần như khớp với mô hình không chia sẻ. Điều này gợi ý rằng chúng ta có thể giảm số lượng tham số bổ sung trong mô hình của chúng tôi, và do đó, tiêu thụ bộ nhớ của nó, mà không có nhiều tác hại đến hiệu suất.

Hiệu ứng của các chuỗi dài hơn: Chúng tôi đánh giá hiệu ứng của độ dài chuỗi trong quá trình huấn luyện trước Linformer. Trong Hình 3(d), chúng tôi vẽ perplexity validation cho Linformer với n ∈ {512;1024;2048;4096},

--- TRANG 6 ---
(c) (d) (a) (b)
Số lượng cập nhật | Số lượng cập nhật | Số lượng cập nhật | Số lượng cập nhật

Hình 3: Perplexity validation huấn luyện trước so với số lượng cập nhật.

giữ chiều chiếu k cố định ở 256. Lưu ý rằng khi độ dài chuỗi tăng, mặc dù chiều chiếu của chúng tôi được cố định, các perplexity cuối cùng sau khi hội tụ vẫn gần như giống nhau. Điều này hỗ trợ thêm bằng kinh nghiệm khẳng định của chúng tôi rằng Linformer có thời gian tuyến tính.

n | Mô hình | SST-2 | IMDB | QNLI | QQP | Trung bình
512 | Liu et al. (2019), RoBERTa-base | 93.1 | 94.1 | 90.9 | 90.9 | 92.25
512 | Linformer, 128 | 92.4 | 94.0 | 90.4 | 90.2 | 91.75
512 | Linformer, 128, shared kv | 93.4 | 93.4 | 90.3 | 90.3 | 91.85
512 | Linformer, 128, shared kv, layer | 93.2 | 93.8 | 90.1 | 90.2 | 91.83
512 | Linformer, 256 | 93.2 | 94.0 | 90.6 | 90.5 | 92.08
512 | Linformer, 256, shared kv | 93.3 | 93.6 | 90.6 | 90.6 | 92.03
512 | Linformer, 256, shared kv, layer | 93.1 | 94.1 | 91.2 | 90.8 | 92.30
512 | Devlin et al. (2019), BERT-base | 92.7 | 93.5 | 91.8 | 89.6 | 91.90
512 | Sanh et al. (2019), Distilled BERT | 91.3 | 92.8 | 89.2 | 88.5 | 90.45
1024 | Linformer, 256 | 93.0 | 93.8 | 90.4 | 90.4 | 91.90
1024 | Linformer, 256, shared kv | 93.0 | 93.6 | 90.3 | 90.4 | 91.83
1024 | Linformer, 256, shared kv, layer | 93.2 | 94.2 | 90.8 | 90.5 | 92.18

Bảng 2: Kết quả tập dev trên các tác vụ hiểu ngôn ngữ tự nhiên chuẩn. Mô hình RoBERTa-base ở đây được huấn luyện trước với cùng corpus như BERT.

5.2 Kết quả Downstream

Cho đến nay, chúng tôi chỉ xem xét các perplexity huấn luyện trước của mô hình chúng tôi. Tuy nhiên, chúng tôi muốn chỉ ra rằng kết luận của chúng tôi có hiệu lực sau khi tinh chỉnh trên các tác vụ downstream. Chúng tôi tinh chỉnh Linformer của chúng tôi trên IMDB (Maas et al., 2011) và SST-2 (Socher et al., 2013) (phân loại cảm xúc), cũng như QNLI (suy luận ngôn ngữ tự nhiên) (Rajpurkar et al., 2016), và QQP (tương tự văn bản) (Chen et al., 2018). Chúng tôi làm tương tự với RoBERTa, BERT-base 12 lớp và BERT chưng cất 6 lớp. Tất cả các mô hình của chúng tôi, bao gồm các baseline Transformer, đều được huấn luyện trước với cùng mục tiêu, corpus huấn luyện trước, và lên đến 250k cập nhật (mặc dù Linformer của chúng tôi mất ít thời gian thực tế hơn nhiều để đạt đến 250k cập nhật, và do đó được huấn luyện trong thời gian ít hơn). Kết quả được liệt kê trong Bảng 2.

--- TRANG 7 ---
Chúng tôi quan sát rằng mô hình Linformer (n = 512; k = 128) có hiệu suất downstream tương đương với mô hình RoBERTa, và thực tế thậm chí còn vượt trội hơn một chút ở k = 256. Hơn nữa, chúng tôi lưu ý rằng mặc dù chiến lược chia sẻ theo lớp của Linformer chia sẻ một ma trận chiếu duy nhất trên toàn bộ mô hình, nó thực sự thể hiện kết quả độ chính xác tốt nhất trong tất cả ba chiến lược chia sẻ tham số. Hơn nữa, Linformer được huấn luyện trước với độ dài chuỗi dài hơn (n = 1024; k = 256) có kết quả tương tự với cái được huấn luyện trước với độ dài ngắn hơn (n = 512; k = 256), điều này hỗ trợ bằng kinh nghiệm khái niệm rằng hiệu suất của mô hình Linformer chủ yếu được quyết định bởi chiều chiếu k thay vì tỷ lệ n/k.

5.3 Kết quả Hiệu quả Thời gian Suy luận

Trong Bảng 3, chúng tôi báo cáo hiệu quả suy luận của Linformer (với chia sẻ theo lớp) so với một Transformer tiêu chuẩn. Chúng tôi đánh giá tốc độ suy luận và bộ nhớ của cả hai mô hình trên card GPU Tesla V100 16GB. Chúng tôi tạo ngẫu nhiên dữ liệu lên đến một số độ dài chuỗi n và thực hiện một lượt truyền tiến đầy đủ trên nhiều batch. Chúng tôi cũng chọn kích thước batch dựa trên kích thước batch tối đa có thể vừa với bộ nhớ, và việc tiết kiệm bộ nhớ của chúng tôi được tính toán dựa trên số này.

độ dài n | chiều chiếu k
128 | 256 | 512 | 1024 | 2048
512 | 1.5x | 1.3x | - | - | -
1024 | 1.7x | 1.6x | 1.3x | - | -
2048 | 2.6x | 2.4x | 2.1x | 1.3x | -
4096 | 3.4x | 3.2x | 2.8x | 2.2x | 1.3x
8192 | 5.5x | 5.0x | 4.4x | 3.5x | 2.1x
16384 | 8.6x | 7.8x | 7.0x | 5.6x | 3.3x
32768 | 13x | 12x | 11x | 8.8x | 5.0x
65536 | 20x | 18x | 16x | 14x | 7.9x

độ dài n | chiều chiếu k
128 | 256 | 512 | 1024 | 2048
512 | 1.7x | 1.5x | - | - | -
1024 | 3.0x | 2.9x | 1.8x | - | -
2048 | 6.1x | 5.6x | 3.6x | 2.0x | -
4096 | 14x | 13x | 8.3x | 4.3x | 2.3x
8192 | 28x | 26x | 17x | 8.5x | 4.5x
16384 | 56x | 48x | 32x | 16x | 8x
32768 | 56x | 48x | 36x | 18x | 16x
65536 | 60x | 52x | 40x | 20x | 18x

Bảng 3: Cải thiện hiệu quả thời gian suy luận của Linformer so với Transformer, qua các chiều chiếu k và độ dài chuỗi n khác nhau. Bảng trái cho thấy thời gian tiết kiệm được. Bảng phải cho thấy bộ nhớ tiết kiệm được.

Từ Bảng 3, chúng ta thấy rằng thậm chí với n = 512 và k = 128, Linformer có thời gian suy luận nhanh hơn 1.5 lần và cho phép kích thước batch tối đa lớn hơn 1.7 lần so với Transformer. Khi độ dài chuỗi tăng, việc tăng tốc thời gian suy luận và tiết kiệm bộ nhớ thậm chí còn ấn tượng hơn. Chúng tôi cũng vẽ thời gian suy luận của cả Linformer và Transformer trên 100 mẫu dữ liệu ở phía trên bên phải của Hình 2.

6 Kết luận

Các mô hình Transformer nổi tiếng chậm để huấn luyện và triển khai trong thực tế vì các phép toán tự-chú-ý của chúng có độ phức tạp thời gian và không gian O(n²) với độ dài chuỗi n. Trong bài báo này, chúng tôi chứng minh, cả về mặt lý thuyết và thực nghiệm, rằng ma trận ngẫu nhiên được hình thành bởi cơ chế tự-chú-ý có hạng thấp. Chúng tôi tiếp tục tận dụng quan sát này để đề xuất một cơ chế tự-chú-ý mới, hiệu quả cao. Thông qua sự kết hợp của phân tích lý thuyết và thực nghiệm, chúng tôi chứng minh rằng phương pháp được đề xuất của chúng tôi là O(n) với độ dài chuỗi.

Tác động Rộng hơn

Công việc của chúng tôi tập trung vào việc làm cho Transformer hiệu quả hơn bằng cách giới thiệu một cơ chế giảm tự-chú-ý xuống độ phức tạp thời gian tuyến tính. Các tác động tích cực tiềm năng của các transformer hiệu quả bao gồm tăng khả năng tiếp cận của các mô hình của chúng tôi, cả cho việc triển khai trên thiết bị, cũng như trong quá trình huấn luyện cho mục đích nghiên cứu. Nó cũng có tác động tiềm năng đến việc huấn luyện transformer trên hình ảnh vì chúng tôi có thể hỗ trợ các chuỗi rất dài. Hơn nữa, có những lợi ích môi trường tích cực liên quan đến việc giảm tiêu thụ điện năng của các mô hình. Như vậy, chúng tôi không thấy tác động đạo đức hoặc xã hội tiêu cực tức thì nào của công việc chúng tôi ngoài những gì áp dụng cho các khối xây dựng cốt lõi khác của học sâu.

--- TRANG 8 ---
Tài liệu tham khảo

Rosa I Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161–182, 2006.

Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.

Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs, 2018.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.

Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218, 1936.

Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme fixed-point compression. arXiv preprint arXiv:2004.07320, 2020.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems, pp. 103–112, 2019.

Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2704–2713, 2018.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. ACL, 2019.

W Johnson J Lindenstrauss. Extensions of lipschitz maps into a hilbert space. Contemp. Math, 26: 189–206, 1984.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pp. 142–150. Association for Computational Linguistics, 2011.

--- TRANG 9 ---
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.

Abdelrahman Mohamed, Dmytro Okhonko, and Luke Zettlemoyer. Transformers with convolutional context for asr. arXiv preprint arXiv:1904.11660, 2019.

Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1–9, 2018.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48–53, 2019.

Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, 2016.

Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. CoRR, abs/1804.07461, 2018. URL http://arxiv.org/abs/1804.07461.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19–27, 2015.

--- TRANG 10 ---
A Chứng minh Định lý 1

Chứng minh. Ý tưởng chính của chứng minh dựa trên bổ đề phân phối Johnson–Lindenstrauss (Lindenstrauss, 1984) (JL, tắt), phiên bản sau đây từ (Arriaga & Vempala, 2006).

Bổ đề 1. Cho R là ma trận k×n, 1 ≤ k ≤ n, với các mục i.i.d. từ N(0,1/k). Với bất kỳ x,y ∈ R^n, chúng ta có

Pr(‖Rx‖ ≤ (1 + ε)‖x‖) > 1 - e^(-ε²-ε³)k/4, (11)

Pr[‖xR^T Ry^T - xy^T‖ ≤ ε‖x‖‖y‖] > 1 - 2e^(-ε²-ε³)k/4. (12)

Để đơn giản, chúng tôi sẽ bỏ chỉ số i cho ma trận W^K_i, W^Q_i, W^V_i, E_i và F_i. Chúng tôi sẽ coi Q như QW^Q, K như KW^K và V như VW^V. Định nghĩa

A = QW^Q_i(KW^K_i)^T/√d (13)

Dựa trên định nghĩa của ma trận ánh xạ ngữ cảnh P, chúng ta có

P = softmax[QW^Q_i(KW^K_i)^T/√d] = exp(A)D^(-1)_A, (14)

trong đó D_A là ma trận đường chéo n×n sao cho

(D_A)_ii = Σ^n_{j=1} exp(A_ji) (15)

Ở đây chúng tôi cung cấp một chứng minh xây dựng. Cho bất kỳ lỗi xấp xỉ ε > 0, định nghĩa ma trận sau.

P̃ = exp(A)D^(-1)_A R^T R, (16)

trong đó R là ma trận k×n, 1 ≤ k ≤ n, với các mục i.i.d. từ N(0,1/k). Rõ ràng hạng của ma trận P̃ thỏa mãn

rank(P̃) ≤ rank(R) = k. (17)

Chúng tôi tiếp tục chỉ ra rằng, khi k = O(log(n)), chúng ta có rằng, với bất kỳ vector cột w ∈ R^n,

Pr[‖P̃w - Pw‖ ≤ ε‖Pw‖] > 1 - o(1). (18)

Điều này kết luận định lý. Với bất kỳ vector hàng u ∈ R^n của ma trận P và bất kỳ vector cột w ∈ R^n của ma trận VW^V, áp dụng Bổ đề JL, chúng ta có thể thu được

Pr[‖uR^T Rw^T - uw^T‖ ≤ ε‖uw^T‖] > 1 - 2e^(-ε²-ε³)k/4. (19)

Do đó, chúng ta có

Pr[‖P̃w - Pw‖ ≤ ε‖Pw‖] = Pr[‖PR^T Rw - Pw‖ ≤ ε‖Pw‖]

(a) ≥ 1 - Σ_{x∈P} Pr[‖xR^T Rw^T - xw^T‖ > ε‖xw^T‖]

(b) > 1 - 2ne^(-ε²-ε³)k/4. (20)

Ở trên, bước (a) dựa trên giới hạn hợp. Bước (b) sử dụng kết quả của Bổ đề JL. Cho k = 5log(n)/(2ε³), thì định lý được chứng minh.

--- TRANG 11 ---
B Chứng minh Định lý 2

Chứng minh. Định nghĩa E = R và F = εR, trong đó R ∈ R^(n×k) với các mục i.i.d. từ N(0,1/k), ε là một hằng số với ε = 1/(2n). Chúng tôi sẽ đầu tiên chứng minh rằng với bất kỳ vector hàng x ∈ R^n của ma trận QK^T và vector cột y ∈ R^n của ma trận V,

Pr[‖exp(xE^T)Fy^T - exp(x)y^T‖ ≤ ε‖exp(x)y^T‖] > 1 - 2e^(-2ε³k/4). (21)

Dựa trên bất đẳng thức tam giác, chúng ta có

‖exp(xE^T)Fy - exp(x)y^T‖ ≤ ‖exp(xE^T)Fy - exp(x)R^T Ry‖ + ‖exp(x)R^T Ry - exp(x)y^T‖

(a) ≤ (1 + ε)‖y‖‖exp(xE^T) - exp(x)R^T‖ + ‖exp(x)R^T Ry - exp(x)y^T‖

(b) ≤ ‖exp(x)R^T Ry - exp(x)y^T‖ + o(‖exp(x)‖‖y‖)

(c) ≤ ε‖exp(x)‖‖y‖ + o(‖exp(x)‖‖y‖) (22)

Ở trên, bước (a) dựa trên bất đẳng thức Cauchy và Bổ đề JL trong (11). Bước (b) sử dụng thực tế rằng hàm mũ là liên tục Lipschitz trong một vùng compact. Sau đó chúng ta có thể chọn ε đủ nhỏ, tức là ε = (1/n) sao cho

‖exp(xR) - exp(x)R‖ = o(‖exp(x)‖) (23)

Bước (c) dựa trên Bổ đề JL được định nghĩa trong (12).

Áp dụng kết quả trong (21) cho mỗi vector hàng của ma trận A và mỗi vector cột của ma trận V, người ta có thể chứng minh trực tiếp rằng, với bất kỳ vector hàng A_i của ma trận A,

Pr[‖exp(A_i E^T)FV - exp(A_i)V‖ ≤ ε‖exp(A_i)‖‖V‖] > 1 - o(1), (24)

bằng cách đặt k = 5log(nd)/(2ε³). Kết quả này không sử dụng tính chất hạng thấp của ma trận A (rank(A) = d) và k kết quả có sự phụ thuộc vào độ dài chuỗi n. Chúng tôi sẽ tiếp tục chứng minh việc chọn k có thể là hằng số và độc lập với độ dài chuỗi n.

Dựa trên thực tế rằng rank(A) = d, chúng ta có thể tìm một ma trận con hàng A_s ∈ R^(2d×d) của ma trận exp(AE^T)FH sao cho rank(A_s) = d. Áp dụng kết quả trong (21) cho mỗi vector hàng của ma trận A_s và mỗi vector cột của ma trận V, và k = 9log(d)/(2ε³), chúng ta có thể thu được rằng, với bất kỳ vector hàng A^s_i của ma trận A_s,

Pr[‖exp(A^s_i E^T)FV - exp(A^s_i)V‖ ≤ ε‖exp(A^s_i)‖‖V‖] > 1 - o(1), (25)

Hơn nữa, định nghĩa ma trận Ψ ∈ R^(n×2d) như

Ψ = [exp(AE^T)FV / exp(A)V] [exp(A_s E^T)FV / exp(A_s)V]^(-1) (26)

Chúng ta có rằng, với bất kỳ vector hàng A_i của ma trận A, 1 ≤ i ≤ n.

‖exp(A_i E^T)FV - exp(A_i)V‖ = ‖Ψ_i [exp(A_s E^T)FV - exp(A_s)V]‖

(a) ≤ ‖[exp(A_s E^T)FV - exp(A_s)V]^T‖_2 ‖Ψ_i‖

(b) ≤ √d‖exp(A_s E^T)FV - exp(A_s)V‖_F

= √d Σ^(2d)_(i=1) ‖exp(A^s_i E^T)FV - exp(A^s_i)V‖

(c) ≤ √d Σ^(2d)_(i=1) ε‖exp(A^s_i)‖‖V‖

≤ √d ε‖exp(A_s)‖‖V‖

Ở trên, bước (a) sử dụng bất đẳng thức ‖Ax‖ ≤ ‖A‖_2‖x‖, trong đó ‖A‖_2 = √λ_max(A^T A) (λ_max(·) là giá trị eigen lớn nhất) là chuẩn phổ của ma trận A. Bước (b) dựa trên bất đẳng thức chuẩn ma trận ‖A‖_2 ≤ ‖A‖_F, trong đó ‖A‖_F = (Σ_(1≤i,j≤n) A²_ij)^(1/2) là chuẩn Frobenius của ma trận A. Bước (c) dựa trên kết quả của (24).

--- TRANG 12 ---I'll translate this technical paper from English to Vietnamese while maintaining the exact same structure and content.

# 2006.04768.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2006.04768.pdf
# Kích thước file: 922879 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Linformer: Cơ chế Tự-Chú-Ý với Độ Phức Tạp Tuyến Tính
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma
Facebook AI, Seattle, WA
{sinongwang, belindali, hanfang, mkhabsa, haom}@fb.com

Tóm tắt
Các mô hình transformer lớn đã cho thấy thành công phi thường trong việc đạt được các kết quả tối ưu trong nhiều ứng dụng xử lý ngôn ngữ tự nhiên. Tuy nhiên, việc huấn luyện và triển khai các mô hình này có thể tốn kém cấm đoán cho các chuỗi dài, vì cơ chế tự-chú-ý tiêu chuẩn của Transformer sử dụng O(n²) thời gian và không gian với độ dài chuỗi. Trong bài báo này, chúng tôi chứng minh rằng cơ chế tự-chú-ý có thể được xấp xỉ bằng một ma trận hạng thấp. Chúng tôi tiếp tục khai thác khám phá này để đề xuất một cơ chế tự-chú-ý mới, giảm độ phức tạp tự-chú-ý tổng thể từ O(n²) xuống O(n) về cả thời gian và không gian. Transformer tuyến tính kết quả, Linformer, hoạt động ngang bằng với các mô hình Transformer tiêu chuẩn, trong khi hiệu quả hơn nhiều về bộ nhớ và thời gian.

1 Giới thiệu
Các mô hình Transformer (Vaswani et al., 2017) đã trở nên phổ biến cho nhiều loại vấn đề trong xử lý ngôn ngữ tự nhiên (NLP), bao gồm dịch thuật (Ott et al., 2018), phân loại văn bản, trả lời câu hỏi, và nhiều ứng dụng khác (Raffel et al., 2019; Mohamed et al., 2019). Trong vài năm qua, số lượng tham số trong các transformer NLP tiên tiến đã tăng mạnh, từ 340 triệu ban đầu được giới thiệu trong BERT-Large đến 175 tỷ trong GPT-3 (Brown et al., 2020). Mặc dù các mô hình quy mô lớn này mang lại kết quả ấn tượng trên nhiều tác vụ khác nhau, việc huấn luyện và triển khai mô hình như vậy chậm trong thực tế. Ví dụ, mô hình BERT-Large gốc (Devlin et al., 2019) mất bốn ngày để huấn luyện trên 16 Cloud TPU, và GPT-3 gần đây (Brown et al., 2020) tiêu tốn nhiều bậc độ lớn petaflops/ngày để huấn luyện so với người tiền nhiệm, GPT-2 (Radford et al., 2019). Ngoài huấn luyện, triển khai các mô hình Transformer vào ứng dụng thế giới thực cũng tốn kém, thường yêu cầu chưng cất rộng rãi (Hinton et al., 2015) hoặc nén.

Nút thắt hiệu quả chính trong các mô hình Transformer là cơ chế tự-chú-ý của nó. Ở đây, biểu diễn của mỗi token được cập nhật bằng cách chú ý đến tất cả các token khác trong lớp trước đó. Hoạt động này là chìa khóa để giữ lại thông tin dài hạn, mang lại cho Transformer lợi thế so với các mô hình tuần tự trên các chuỗi dài. Tuy nhiên, việc chú ý đến tất cả các token ở mỗi lớp gây ra độ phức tạp O(n²) với độ dài chuỗi. Do đó, trong bài báo này, chúng tôi tìm cách trả lời câu hỏi: liệu các mô hình Transformer có thể được tối ưu hóa để tránh phép toán bậc hai này, hay phép toán này cần thiết để duy trì hiệu suất mạnh?

Các công trình trước đây đã đề xuất một số kỹ thuật để cải thiện hiệu quả của tự-chú-ý. Một kỹ thuật phổ biến là đưa tính thưa thớt vào các lớp chú ý (Child et al., 2019; Qiu et al., 2019; Beltagy et al., 2020) bằng cách để mỗi token chỉ chú ý đến một tập con các token trong toàn bộ chuỗi. Điều này giảm độ phức tạp tổng thể của cơ chế chú ý xuống O(n√n) (Child et al., 2019). Tuy nhiên, như được chỉ ra trong Qiu et al. (2019), phương pháp này gặp phải sự sụt giảm hiệu suất lớn với những cải thiện hiệu quả hạn chế, tức là giảm 2% với chỉ 20% tăng tốc. Gần đây hơn, Reformer (Kitaev et al., 2020) đã sử dụng băm nhạy cảm cục bộ (LSH) để giảm độ phức tạp tự-chú-ý xuống O(n log(n)). Tuy nhiên, trong thực tế, những cải thiện hiệu quả của Reformer chỉ xuất hiện trên các chuỗi có độ dài >2048 (Hình 5 trong Kitaev et al. (2020)). Hơn nữa, phương pháp băm đa vòng của Reformer thực sự tăng số lượng các hoạt động tuần tự, điều này làm suy giảm thêm những cải thiện hiệu quả cuối cùng của họ.

Bản thảo. Đang xem xét.arXiv:2006.04768v3 [cs.LG] 14 Jun 2020

--- TRANG 2 ---
Trong nghiên cứu này, chúng tôi giới thiệu một phương pháp mới để giải quyết nút thắt tự-chú-ý trong Transformer. Phương pháp của chúng tôi được truyền cảm hứng từ quan sát quan trọng rằng tự-chú-ý có hạng thấp. Cụ thể hơn, chúng tôi chỉ ra cả về mặt lý thuyết và thực nghiệm rằng ma trận ngẫu nhiên được hình thành bởi tự-chú-ý có thể được xấp xỉ bằng một ma trận hạng thấp. Được trao quyền bởi quan sát này, chúng tôi giới thiệu một cơ chế mới giảm tự-chú-ý xuống phép toán O(n) về cả độ phức tạp không gian và thời gian: chúng tôi phân tách chú ý tích vô hướng có tỷ lệ gốc thành nhiều chú ý nhỏ hơn thông qua các phép chiếu tuyến tính, sao cho sự kết hợp của các phép toán này tạo thành một phân tích nhân tử hạng thấp của chú ý gốc. Một tóm tắt thời gian chạy cho các kiến trúc Transformer khác nhau, bao gồm của chúng tôi, có thể được tìm thấy trong Bảng 1.

Một ứng dụng chủ yếu của Transformer, đã thấy nhiều cải tiến nhất, là sử dụng chúng như các mô hình ngôn ngữ được huấn luyện trước, trong đó các mô hình đầu tiên được huấn luyện trước với mục tiêu mô hình hóa ngôn ngữ trên một corpus lớn, sau đó được tinh chỉnh trên các tác vụ đích sử dụng dữ liệu có giám sát (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2019). Theo Devlin et al. (2019), chúng tôi huấn luyện trước mô hình của chúng tôi trên BookCorpus (Zhu et al., 2015) cộng với English Wikipedia sử dụng mục tiêu mô hình hóa ngôn ngữ có mặt nạ. Chúng tôi quan sát hiệu suất huấn luyện trước tương tự với mô hình Transformer tiêu chuẩn. Sau đó chúng tôi tinh chỉnh các mô hình được huấn luyện trước của chúng tôi trên ba tác vụ từ GLUE (Wang et al., 2018) và một tác vụ phân tích cảm xúc, đánh giá IMDB (Maas et al., 2011). Trên các tác vụ này, chúng tôi thấy rằng mô hình của chúng tôi hoạt động tương đương, hoặc thậm chí tốt hơn một chút, so với Transformer được huấn luyện trước tiêu chuẩn, trong khi quan sát được những cải thiện đáng kể về tốc độ huấn luyện và suy luận.

Kiến trúc Mô hình | Độ phức tạp mỗi lớp | Phép toán tuần tự
Tái phát | O(n) | O(n)
Transformer, (Vaswani et al., 2017) | O(n²) | O(1)
Sparse Transformer, (Child et al., 2019) | O(n√n) | O(1)
Reformer, (Kitaev et al., 2020) | O(n log(n)) | O(log(n))
Linformer | O(n) | O(1)

Bảng 1: Độ phức tạp thời gian mỗi lớp và số lượng tối thiểu các phép toán tuần tự như một hàm của độ dài chuỗi (n) cho các kiến trúc khác nhau.

2 Bối cảnh và Các nghiên cứu liên quan

2.1 Transformer và Tự-Chú-Ý

Transformer được xây dựng dựa trên ý tưởng của Multi-Head Self-Attention (MHA), cho phép mô hình cùng lúc chú ý đến thông tin tại các vị trí khác nhau từ các không gian biểu diễn con khác nhau. MHA được định nghĩa là

MultiHead(Q;K;V) = Concat(head₁;head₂;...;headₕ)W^O, (1)

trong đó Q;K;V ∈ R^(n×d_m) là các ma trận nhúng đầu vào, n là độ dài chuỗi, d_m là chiều nhúng, và h là số đầu. Mỗi đầu được định nghĩa như:

head_i = Attention(QW^Q_i;KW^K_i;VW^V_i) = softmax[QW^Q_i(KW^K_i)^T/√d_k] × VW^V_i, (2)

trong đó W^Q_i;W^K_i ∈ R^(d_m×d_k); W^V_i ∈ R^(d_m×d_v); W^O ∈ R^(hd_v×d_m) là các ma trận học được và d_k;d_v là các chiều ẩn của các không gian chiếu con. Trong phần còn lại của bài báo này, chúng tôi sẽ không phân biệt giữa d_k và d_v và chỉ sử dụng d.

Tự-chú-ý được định nghĩa trong (2) ám chỉ một ma trận ánh xạ ngữ cảnh P ∈ R^(n×n). Transformer sử dụng P để nắm bắt ngữ cảnh đầu vào cho một token đã cho, dựa trên sự kết hợp của tất cả các token trong chuỗi. Tuy nhiên, tính toán P tốn kém. Nó yêu cầu nhân hai ma trận n×d_m, có độ phức tạp O(n²) về thời gian và không gian. Sự phụ thuộc bậc hai này vào độ dài chuỗi đã trở thành nút thắt cho Transformer.

2.2 Các nghiên cứu liên quan

Đã có nhiều tài liệu trước đây về cải thiện hiệu quả của Transformer, đặc biệt là nút thắt tự-chú-ý. Các kỹ thuật phổ biến nhất cho hiệu quả mô hình có thể được áp dụng cho Transformer (một số cụ thể cho Transformer, những cái khác có mục đích chung hơn) bao gồm:

--- TRANG 3 ---
Độ chính xác hỗn hợp (Micikevicius et al., 2017): Sử dụng biểu diễn nửa độ chính xác hoặc độ chính xác hỗn hợp của các điểm thực là phổ biến trong học sâu, và cũng được sử dụng rộng rãi trong huấn luyện Transformer (Ott et al., 2019). Kỹ thuật này có thể được cải thiện thêm thông qua Huấn luyện Nhận thức Lượng tử hóa (Jacob et al., 2018; Fan et al., 2020), trong đó các trọng số được lượng tử hóa trong quá trình huấn luyện và các gradient được xấp xỉ với Bộ ước lượng Thông qua Trực tiếp. Dòng công việc này trực giao với phương pháp của chúng tôi, và chúng tôi sử dụng huấn luyện độ chính xác hỗn hợp theo mặc định.

Chưng cất Kiến thức (Hinton et al., 2015): Chưng cất kiến thức nhằm chuyển giao "kiến thức" từ một mô hình giáo viên lớn sang một mô hình học sinh nhẹ. Mô hình học sinh sau đó được sử dụng trong quá trình suy luận. Tuy nhiên phương pháp này có nhược điểm: Nó không giải quyết việc tăng tốc mô hình giáo viên trong quá trình huấn luyện, và hơn nữa, các mô hình học sinh thường gặp phải sự suy giảm hiệu suất so với mô hình giáo viên. Ví dụ, khi chưng cất một BERT 12 lớp thành một BERT 6 lớp, mô hình học sinh trải qua sự sụt giảm hiệu suất trung bình 2.5% trên một số tác vụ chuẩn (Sanh et al., 2019).

Chú ý Thưa thớt (Child et al., 2019): Kỹ thuật này cải thiện hiệu quả của tự-chú-ý bằng cách thêm tính thưa thớt vào ma trận ánh xạ ngữ cảnh P. Ví dụ, Sparse Transformer (Child et al., 2019) chỉ tính toán P_ij xung quanh đường chéo của ma trận P (thay vì tất cả P_ij). Trong khi đó, tự-chú-ý theo khối (Qiu et al., 2019) chia P thành nhiều khối và chỉ tính toán P_ij trong các khối được chọn. Tuy nhiên, các kỹ thuật này cũng gặp phải sự suy giảm hiệu suất lớn, trong khi chỉ có tăng tốc bổ sung hạn chế, tức là giảm 2% với tăng tốc 20%.

Chú ý LSH (Kitaev et al., 2020): Chú ý băm nhạy cảm cục bộ (LSH) sử dụng một sơ đồ băm đa vòng khi tính toán chú ý tích vô hướng, về mặt lý thuyết giảm độ phức tạp tự-chú-ý xuống O(n log(n)). Tuy nhiên, trong thực tế, hạng tử độ phức tạp của họ có một hằng số lớn 128² và nó chỉ hiệu quả hơn transformer vani khi độ dài chuỗi cực kỳ dài.

Cải thiện Hiệu quả Bộ tối ưu hóa: Microbatching (Huang et al., 2019) chia một batch thành các microbatch nhỏ (có thể vừa với bộ nhớ), và sau đó chạy riêng biệt các lượt truyền tiến và lùi trên chúng với tích lũy gradient. Gradient checkpointing (Chen et al., 2016) tiết kiệm bộ nhớ bằng cách chỉ lưu cache các kích hoạt của một tập con các lớp. Các kích hoạt không được cache được tính toán lại trong quá trình lan truyền ngược từ checkpoint mới nhất. Cả hai kỹ thuật đều đánh đổi thời gian cho bộ nhớ, và không tăng tốc suy luận.

Như chúng tôi đã lưu ý, hầu hết các kỹ thuật phổ biến có hạn chế trong việc giảm cả thời gian huấn luyện và suy luận/tiêu thụ bộ nhớ, chúng tôi điều tra cách tối ưu hóa các lớp tự-chú-ý và giới thiệu phương pháp của chúng tôi tiếp theo.

3 Tự-Chú-Ý có Hạng Thấp

Trong phần này, chúng tôi chứng minh rằng cơ chế tự-chú-ý, tức là ma trận ánh xạ ngữ cảnh P, có hạng thấp.

Chúng tôi đầu tiên cung cấp một phân tích phổ của ma trận ánh xạ ngữ cảnh P. Chúng tôi sử dụng hai mô hình transformer được huấn luyện trước, RoBERTa-base (transformer xếp chồng 12 lớp) và RoBERTa-large (transformer xếp chồng 24 lớp) (Liu et al., 2019) trên hai tác vụ: tác vụ mô hình hóa ngôn ngữ có mặt nạ trên Wiki103 (Merity et al., 2016) và tác vụ phân loại trên IMDB (Maas et al., 2011). Trong Hình 1 (trái), chúng tôi áp dụng phân tích giá trị suy biến vào P qua các lớp và đầu khác nhau của mô hình, và vẽ giá trị suy biến tích lũy được chuẩn hóa trung bình trên 10k câu. Kết quả thể hiện một phân phối phổ đuôi dài rõ ràng qua mỗi lớp, đầu và tác vụ. Điều này ám chỉ rằng hầu hết thông tin của ma trận P có thể được khôi phục từ vài giá trị suy biến lớn nhất đầu tiên. Trong Hình 1 (phải), chúng tôi vẽ một bản đồ nhiệt của giá trị suy biến tích lũy được chuẩn hóa tại giá trị suy biến lớn thứ 128 (trong số 512). Chúng tôi quan sát rằng phân phối phổ ở các lớp cao hơn bị lệch hơn so với ở các lớp thấp hơn, có nghĩa là, ở các lớp cao hơn, nhiều thông tin được tập trung vào các giá trị suy biến lớn nhất và hạng của P thấp hơn.

Dưới đây, chúng tôi cung cấp một phân tích lý thuyết về các kết quả phổ trên.

Định lý 1. (tự-chú-ý có hạng thấp) Với bất kỳ Q;K;V ∈ R^(n×d) và W^Q_i;W^K_i;W^V_i ∈ R^(d×d), với bất kỳ vector cột w ∈ R^n của ma trận VW^V_i, tồn tại một ma trận hạng thấp P̃ ∈ R^(n×n) sao cho

Pr(‖P̃w^T - Pw^T‖ < ε‖Pw^T‖) > 1 - o(1) và rank(P̃) = O(log(n)), (3)

trong đó ma trận ánh xạ ngữ cảnh P được định nghĩa trong (2).

--- TRANG 4 ---
Chỉ số đầu | Chỉ số lớp | Chỉ số giá trị eigen
0    128    512 | 0    128    512 | Chỉ số giá trị eigen
Giá trị eigen tích lũy chuẩn hóa | Transformer 12 lớp | Transformer 24 lớp
0.96 0.94 0.92 0.90 0.88 | 0  1  2  3  4  5  6  7  8  9  10 11 | 0 1 2 3 4 5 6 7 8 9 10 11
1.0 0.9 0.8 0.7 0.6 0.5 0.4

Hình 1: Hai hình bên trái là phân tích phổ của ma trận tự-chú-ý trong mô hình transformer được huấn luyện trước (Liu et al., 2019) với n = 512. Trục Y là giá trị suy biến tích lũy chuẩn hóa của ma trận ánh xạ ngữ cảnh P, và trục X là chỉ số của giá trị eigen lớn nhất. Kết quả được dựa trên cả mô hình RoBERTa-base và large trong hai tập dữ liệu công cộng: Wiki103 và IMDB. Hình bên phải vẽ bản đồ nhiệt của giá trị eigen tích lũy chuẩn hóa tại giá trị eigen lớn thứ 128 qua các lớp và đầu khác nhau trong dữ liệu Wiki103.

Chứng minh. Dựa trên định nghĩa của ma trận ánh xạ ngữ cảnh P, chúng ta có thể viết

P = softmax[QW^Q_i(KW^K_i)^T/√d] = exp(A)D^(-1)_A, (4)

trong đó D_A là một ma trận đường chéo n×n. Ý tưởng chính của chứng minh này dựa trên bổ đề phân phối Johnson–Lindenstrauss (Lindenstrauss, 1984) (JL tắt). Chúng tôi xây dựng ma trận xấp xỉ hạng thấp như P̃ = exp(A)D^(-1)_A R^T R, trong đó R ∈ R^(k×n) với các mục i.i.d. từ N(0,1/k). Sau đó chúng ta có thể sử dụng bổ đề JL để chỉ ra rằng, với bất kỳ vector cột w ∈ R^n của ma trận VW^V_i, khi k = 5log(n)/(2ε³), chúng ta có

Pr[‖PR^T Rw^T - Pw^T‖ ≤ ε‖Pw^T‖] > 1 - o(1). (5)

Để biết thêm chi tiết, tham khảo tài liệu bổ sung.

Cho tính chất hạng thấp của ma trận ánh xạ ngữ cảnh P, một ý tưởng đơn giản là sử dụng phân tích giá trị suy biến (SVD) để xấp xỉ P với một ma trận hạng thấp P_low, như sau

P ≈ P_low = Σ^k_{i=1} σ_i u_i v^T_i = [u_1,...,u_k] diag{σ_1,...,σ_k} [v_1; ...; v_k]^T ∈ R^(n×k), (6)

trong đó σ_i, u_i và v_i là giá trị suy biến lớn thứ i và các vector suy biến tương ứng của chúng. Dựa trên kết quả trong Định lý 1 và Định lý Eckart–Young–Mirsky (Eckart & Young, 1936), người ta có thể sử dụng P_low để xấp xỉ tự-chú-ý (2) với lỗi ε và độ phức tạp thời gian và không gian O(nk). Tuy nhiên, phương pháp này yêu cầu thực hiện một phân tích SVD trong mỗi ma trận tự-chú-ý, điều này thêm độ phức tạp bổ sung. Do đó, chúng tôi đề xuất một phương pháp khác cho xấp xỉ hạng thấp tránh độ phức tạp được thêm vào này.

4 Mô hình

Trong phần này, chúng tôi đề xuất một cơ chế tự-chú-ý mới cho phép chúng tôi tính toán ánh xạ ngữ cảnh PVW^V_i trong độ phức tạp thời gian và bộ nhớ tuyến tính với độ dài chuỗi.

Ý tưởng chính của tự-chú-ý tuyến tính được đề xuất của chúng tôi (Hình 2) là thêm hai ma trận chiếu tuyến tính E_i, F_i ∈ R^(n×k) khi tính toán khóa và giá trị. Chúng tôi đầu tiên chiếu các lớp khóa và giá trị (n×d)-chiều gốc KW^K_i và VW^V_i thành các lớp khóa và giá trị chiếu (k×d)-chiều. Sau đó chúng tôi tính toán một ma trận ánh xạ ngữ cảnh (n×k)-chiều P sử dụng chú ý tích vô hướng có tỷ lệ.

head_i = Attention(QW^Q_i, E_i KW^K_i, F_i VW^V_i)
       = softmax(QW^Q_i(E_i KW^K_i)^T/√d_k) F_i VW^V_i, (7)

Cuối cùng, chúng tôi tính toán các nhúng ngữ cảnh cho mỗi đầu i sử dụng P(F_i VW^V_i). Lưu ý các phép toán trên chỉ yêu cầu độ phức tạp thời gian và không gian O(nk). Do đó, nếu chúng ta có thể chọn một chiều chiếu rất nhỏ k, sao cho k ≪ n, thì chúng ta có thể giảm đáng kể tiêu thụ bộ nhớ và không gian. Định lý sau đây nêu rằng, khi k = O(d/ε²) (độc lập với n), người ta có thể xấp xỉ PVW^V_i sử dụng tự-chú-ý tuyến tính (7) với lỗi ε.

Định lý 2. (Tự-chú-ý tuyến tính) Với bất kỳ Q_i;K_i;V_i ∈ R^(n×d) và W^Q_i;W^K_i;W^V_i ∈ R^(d×d), nếu k = min{⌈9d log(d)/ε²⌉, ⌈5(log(n)/ε²)⌉}, thì tồn tại các ma trận E_i;F_i ∈ R^(n×k) sao cho, với bất kỳ vector hàng w của ma trận QW^Q_i(KW^K_i)^T/√d, chúng ta có

Pr[‖softmax(wE^T_i)F_i VW^V_i - softmax(w)VW^V_i‖ ≤ ε‖softmax(w)‖‖VW^V_i‖] > 1 - o(1) (8)

Chứng minh. Ý tưởng chính của chứng minh dựa trên bổ đề phân phối Johnson–Lindenstrauss (Lindenstrauss, 1984). Chúng tôi đầu tiên chứng minh rằng với bất kỳ vector hàng x ∈ R^n của ma trận QW^Q_i(KW^K_i)^T/√d_k và vector cột y ∈ R^n của ma trận VW^V_i,

Pr[‖exp(xE^T_i)F_i y^T - exp(x)y^T‖ ≤ ε‖exp(x)y^T‖] > 1 - 2e^(-2ε³k/4), (9)

trong đó E_i = R và F_i = εR, trong đó R ∈ R^(k×n) với các mục i.i.d. từ N(0,1/k) và ε là một hằng số nhỏ. Áp dụng kết quả trong (9) cho mỗi vector hàng của ma trận A và mỗi vector cột của ma trận V, người ta có thể chứng minh trực tiếp rằng, với bất kỳ vector hàng A_i của ma trận A,

Pr[‖exp(A_i E^T_i)F_i V - exp(A_i)V‖ ≤ ε‖exp(A_i)V‖] > 1 - o(1), (10)

bằng cách đặt k = 5log(nd)/(2ε³). Kết quả này không sử dụng tính chất hạng thấp của ma trận A (rank(A) = d) và k kết quả có sự phụ thuộc vào độ dài chuỗi n. Chúng tôi sẽ tiếp tục sử dụng thực tế rằng rank(A) = d để chứng minh việc chọn k có thể là hằng số và độc lập với độ dài chuỗi n. Để biết thêm chi tiết, tham khảo tài liệu bổ sung.

--- TRANG 5 ---
Chú ý Tích Vô hướng Có tỷ lệ | Tuyến tính | Concat | Chú ý Tích Vô hướng Có tỷ lệ | Tuyến tính | Tuyến tính
V | K | Q | Chiếu | Chiếu | Tuyến tính | Tuyến tính | Tuyến tính | Tuyến tính
n×d_m | d_m×d_k | k×n | × | Linformer | × = k×d_k | W^O_i(W^V_i) | K(V) | Độ dài chuỗi/kích thước batch

Hình 2: Trái và dưới-phải cho thấy kiến trúc và ví dụ của tự-chú-ý tuyến tính đa đầu được đề xuất của chúng tôi. Trên phải cho thấy thời gian suy luận so với độ dài chuỗi cho các mô hình Linformer khác nhau.

Trong Hình 2 (trên phải), chúng tôi vẽ tốc độ suy luận của Linformer và Transformer tiêu chuẩn so với độ dài chuỗi, trong khi giữ tổng số token cố định. Chúng ta thấy rằng trong khi Transformer tiêu chuẩn trở nên chậm hơn ở độ dài chuỗi dài hơn, tốc độ Linformer vẫn tương đối ổn định và nhanh hơn đáng kể ở các chuỗi dài.

Các Kỹ thuật Hiệu quả Bổ sung: Một số kỹ thuật bổ sung có thể được giới thiệu trên Linformer để tối ưu hóa thêm cho cả hiệu suất và hiệu quả:

Chia sẻ tham số giữa các phép chiếu: Người ta có thể chia sẻ tham số cho các ma trận chiếu tuyến tính E_i;F_i qua các lớp và đầu. Cụ thể, chúng tôi thử nghiệm với 3 mức độ chia sẻ:

Chia sẻ theo đầu: cho mỗi lớp, chúng tôi chia sẻ hai ma trận chiếu E và F sao cho E_i = E và F_i = F qua tất cả các đầu i.

Chia sẻ khóa-giá trị: chúng tôi thực hiện chia sẻ theo đầu, với ràng buộc bổ sung của việc chia sẻ các phép chiếu khóa và giá trị. Cho mỗi lớp, chúng tôi tạo một ma trận chiếu duy nhất E sao cho E_i = F_i = E cho mỗi ma trận chiếu khóa-giá trị qua tất cả các đầu i.

Chia sẻ theo lớp: chúng tôi sử dụng một ma trận chiếu duy nhất E qua tất cả các lớp, cho tất cả các đầu, và cho cả khóa và giá trị.

Ví dụ, trong một mô hình Transformer xếp chồng 12 lớp, 12 đầu, chia sẻ theo đầu, chia sẻ khóa-giá trị và chia sẻ theo lớp sẽ giới thiệu 24, 12, và 1 ma trận chiếu tuyến tính khác biệt, tương ứng.

Chiều chiếu không đồng nhất: Người ta có thể chọn một chiều chiếu k khác nhau cho các đầu và lớp khác nhau. Như được chỉ ra trong Hình 1 (phải), các ma trận ánh xạ ngữ cảnh trong các đầu và lớp khác nhau có phân phối phổ khác biệt, và các đầu trong lớp cao hơn có xu hướng về phân phối phổ lệch hơn (hạng thấp hơn). Điều này ám chỉ người ta có thể chọn một chiều chiếu k nhỏ hơn cho các lớp cao hơn.

Phép chiếu tổng quát: Người ta cũng có thể chọn các loại phương pháp chiếu chiều thấp khác nhau thay vì một phép chiếu tuyến tính đơn giản. Ví dụ, người ta có thể chọn pooling trung bình/tối đa, hoặc tích chập trong đó kernel và stride được đặt thành n/k. Các hàm tích chập chứa các tham số yêu cầu huấn luyện.

5 Thí nghiệm

Trong phần này, chúng tôi trình bày kết quả thí nghiệm cho các kỹ thuật được mô tả ở trên. Chúng tôi phân tích các kỹ thuật từng cái một và khám phá cách chúng tác động đến hiệu suất.

5.1 Perplexity Huấn luyện trước

Chúng tôi đầu tiên so sánh hiệu suất huấn luyện trước của kiến trúc được đề xuất của chúng tôi với RoBERTa (Liu et al., 2019), dựa trên Transformer. Theo Devlin et al. (2019), chúng tôi sử dụng BookCorpus (Zhu et al., 2015) cộng với English Wikipedia làm tập huấn luyện trước của chúng tôi (3300M từ). Tất cả các mô hình được huấn luyện trước với mục tiêu mô hình hóa ngôn ngữ có mặt nạ (MLM), và huấn luyện cho tất cả các thí nghiệm được song song hóa trên 64 Tesla V100 GPU với 250k cập nhật.

Hiệu ứng của chiều chiếu: Chúng tôi thí nghiệm với các giá trị khác nhau cho chiều chiếu k. (Chúng tôi sử dụng cùng k qua tất cả các lớp và đầu của Linformer.) Trong Hình 3(a) và (b), chúng tôi vẽ các đường cong perplexity validation cho cả Transformer tiêu chuẩn và Linformer qua các k khác nhau, cho độ dài chuỗi tối đa n = 512 và n = 1024. Như mong đợi, Linformer hoạt động tốt hơn khi chiều chiếu k tăng. Tuy nhiên, thậm chí ở k = 128 cho n = 512 và k = 256 cho n = 1024, hiệu suất của Linformer đã gần như ngang bằng với Transformer gốc.

Hiệu ứng của việc chia sẻ phép chiếu: Trong Hình 3(c), chúng tôi vẽ các đường cong perplexity validation cho ba chiến lược chia sẻ tham số (theo đầu, khóa-giá trị, và theo lớp) với n = 512. Lưu ý rằng khi chúng tôi chỉ sử dụng một ma trận chiếu duy nhất (tức là cho chia sẻ theo lớp), perplexity validation của mô hình Linformer kết quả gần như khớp với mô hình không chia sẻ. Điều này gợi ý rằng chúng ta có thể giảm số lượng tham số bổ sung trong mô hình của chúng tôi, và do đó, tiêu thụ bộ nhớ của nó, mà không có nhiều tác hại đến hiệu suất.

Hiệu ứng của các chuỗi dài hơn: Chúng tôi đánh giá hiệu ứng của độ dài chuỗi trong quá trình huấn luyện trước Linformer. Trong Hình 3(d), chúng tôi vẽ perplexity validation cho Linformer với n ∈ {512;1024;2048;4096},

--- TRANG 6 ---
(c) (d) (a) (b)
Số lượng cập nhật | Số lượng cập nhật | Số lượng cập nhật | Số lượng cập nhật

Hình 3: Perplexity validation huấn luyện trước so với số lượng cập nhật.

giữ chiều chiếu k cố định ở 256. Lưu ý rằng khi độ dài chuỗi tăng, mặc dù chiều chiếu của chúng tôi được cố định, các perplexity cuối cùng sau khi hội tụ vẫn gần như giống nhau. Điều này hỗ trợ thêm bằng kinh nghiệm khẳng định của chúng tôi rằng Linformer có thời gian tuyến tính.

n | Mô hình | SST-2 | IMDB | QNLI | QQP | Trung bình
512 | Liu et al. (2019), RoBERTa-base | 93.1 | 94.1 | 90.9 | 90.9 | 92.25
512 | Linformer, 128 | 92.4 | 94.0 | 90.4 | 90.2 | 91.75
512 | Linformer, 128, shared kv | 93.4 | 93.4 | 90.3 | 90.3 | 91.85
512 | Linformer, 128, shared kv, layer | 93.2 | 93.8 | 90.1 | 90.2 | 91.83
512 | Linformer, 256 | 93.2 | 94.0 | 90.6 | 90.5 | 92.08
512 | Linformer, 256, shared kv | 93.3 | 93.6 | 90.6 | 90.6 | 92.03
512 | Linformer, 256, shared kv, layer | 93.1 | 94.1 | 91.2 | 90.8 | 92.30
512 | Devlin et al. (2019), BERT-base | 92.7 | 93.5 | 91.8 | 89.6 | 91.90
512 | Sanh et al. (2019), Distilled BERT | 91.3 | 92.8 | 89.2 | 88.5 | 90.45
1024 | Linformer, 256 | 93.0 | 93.8 | 90.4 | 90.4 | 91.90
1024 | Linformer, 256, shared kv | 93.0 | 93.6 | 90.3 | 90.4 | 91.83
1024 | Linformer, 256, shared kv, layer | 93.2 | 94.2 | 90.8 | 90.5 | 92.18

Bảng 2: Kết quả tập dev trên các tác vụ hiểu ngôn ngữ tự nhiên chuẩn. Mô hình RoBERTa-base ở đây được huấn luyện trước với cùng corpus như BERT.

5.2 Kết quả Downstream

Cho đến nay, chúng tôi chỉ xem xét các perplexity huấn luyện trước của mô hình chúng tôi. Tuy nhiên, chúng tôi muốn chỉ ra rằng kết luận của chúng tôi có hiệu lực sau khi tinh chỉnh trên các tác vụ downstream. Chúng tôi tinh chỉnh Linformer của chúng tôi trên IMDB (Maas et al., 2011) và SST-2 (Socher et al., 2013) (phân loại cảm xúc), cũng như QNLI (suy luận ngôn ngữ tự nhiên) (Rajpurkar et al., 2016), và QQP (tương tự văn bản) (Chen et al., 2018). Chúng tôi làm tương tự với RoBERTa, BERT-base 12 lớp và BERT chưng cất 6 lớp. Tất cả các mô hình của chúng tôi, bao gồm các baseline Transformer, đều được huấn luyện trước với cùng mục tiêu, corpus huấn luyện trước, và lên đến 250k cập nhật (mặc dù Linformer của chúng tôi mất ít thời gian thực tế hơn nhiều để đạt đến 250k cập nhật, và do đó được huấn luyện trong thời gian ít hơn). Kết quả được liệt kê trong Bảng 2.

--- TRANG 7 ---
Chúng tôi quan sát rằng mô hình Linformer (n = 512; k = 128) có hiệu suất downstream tương đương với mô hình RoBERTa, và thực tế thậm chí còn vượt trội hơn một chút ở k = 256. Hơn nữa, chúng tôi lưu ý rằng mặc dù chiến lược chia sẻ theo lớp của Linformer chia sẻ một ma trận chiếu duy nhất trên toàn bộ mô hình, nó thực sự thể hiện kết quả độ chính xác tốt nhất trong tất cả ba chiến lược chia sẻ tham số. Hơn nữa, Linformer được huấn luyện trước với độ dài chuỗi dài hơn (n = 1024; k = 256) có kết quả tương tự với cái được huấn luyện trước với độ dài ngắn hơn (n = 512; k = 256), điều này hỗ trợ bằng kinh nghiệm khái niệm rằng hiệu suất của mô hình Linformer chủ yếu được quyết định bởi chiều chiếu k thay vì tỷ lệ n/k.

5.3 Kết quả Hiệu quả Thời gian Suy luận

Trong Bảng 3, chúng tôi báo cáo hiệu quả suy luận của Linformer (với chia sẻ theo lớp) so với một Transformer tiêu chuẩn. Chúng tôi đánh giá tốc độ suy luận và bộ nhớ của cả hai mô hình trên card GPU Tesla V100 16GB. Chúng tôi tạo ngẫu nhiên dữ liệu lên đến một số độ dài chuỗi n và thực hiện một lượt truyền tiến đầy đủ trên nhiều batch. Chúng tôi cũng chọn kích thước batch dựa trên kích thước batch tối đa có thể vừa với bộ nhớ, và việc tiết kiệm bộ nhớ của chúng tôi được tính toán dựa trên số này.

độ dài n | chiều chiếu k
128 | 256 | 512 | 1024 | 2048
512 | 1.5x | 1.3x | - | - | -
1024 | 1.7x | 1.6x | 1.3x | - | -
2048 | 2.6x | 2.4x | 2.1x | 1.3x | -
4096 | 3.4x | 3.2x | 2.8x | 2.2x | 1.3x
8192 | 5.5x | 5.0x | 4.4x | 3.5x | 2.1x
16384 | 8.6x | 7.8x | 7.0x | 5.6x | 3.3x
32768 | 13x | 12x | 11x | 8.8x | 5.0x
65536 | 20x | 18x | 16x | 14x | 7.9x

độ dài n | chiều chiếu k
128 | 256 | 512 | 1024 | 2048
512 | 1.7x | 1.5x | - | - | -
1024 | 3.0x | 2.9x | 1.8x | - | -
2048 | 6.1x | 5.6x | 3.6x | 2.0x | -
4096 | 14x | 13x | 8.3x | 4.3x | 2.3x
8192 | 28x | 26x | 17x | 8.5x | 4.5x
16384 | 56x | 48x | 32x | 16x | 8x
32768 | 56x | 48x | 36x | 18x | 16x
65536 | 60x | 52x | 40x | 20x | 18x

Bảng 3: Cải thiện hiệu quả thời gian suy luận của Linformer so với Transformer, qua các chiều chiếu k và độ dài chuỗi n khác nhau. Bảng trái cho thấy thời gian tiết kiệm được. Bảng phải cho thấy bộ nhớ tiết kiệm được.

Từ Bảng 3, chúng ta thấy rằng thậm chí với n = 512 và k = 128, Linformer có thời gian suy luận nhanh hơn 1.5 lần và cho phép kích thước batch tối đa lớn hơn 1.7 lần so với Transformer. Khi độ dài chuỗi tăng, việc tăng tốc thời gian suy luận và tiết kiệm bộ nhớ thậm chí còn ấn tượng hơn. Chúng tôi cũng vẽ thời gian suy luận của cả Linformer và Transformer trên 100 mẫu dữ liệu ở phía trên bên phải của Hình 2.

6 Kết luận

Các mô hình Transformer nổi tiếng chậm để huấn luyện và triển khai trong thực tế vì các phép toán tự-chú-ý của chúng có độ phức tạp thời gian và không gian O(n²) với độ dài chuỗi n. Trong bài báo này, chúng tôi chứng minh, cả về mặt lý thuyết và thực nghiệm, rằng ma trận ngẫu nhiên được hình thành bởi cơ chế tự-chú-ý có hạng thấp. Chúng tôi tiếp tục tận dụng quan sát này để đề xuất một cơ chế tự-chú-ý mới, hiệu quả cao. Thông qua sự kết hợp của phân tích lý thuyết và thực nghiệm, chúng tôi chứng minh rằng phương pháp được đề xuất của chúng tôi là O(n) với độ dài chuỗi.

Tác động Rộng hơn

Công việc của chúng tôi tập trung vào việc làm cho Transformer hiệu quả hơn bằng cách giới thiệu một cơ chế giảm tự-chú-ý xuống độ phức tạp thời gian tuyến tính. Các tác động tích cực tiềm năng của các transformer hiệu quả bao gồm tăng khả năng tiếp cận của các mô hình của chúng tôi, cả cho việc triển khai trên thiết bị, cũng như trong quá trình huấn luyện cho mục đích nghiên cứu. Nó cũng có tác động tiềm năng đến việc huấn luyện transformer trên hình ảnh vì chúng tôi có thể hỗ trợ các chuỗi rất dài. Hơn nữa, có những lợi ích môi trường tích cực liên quan đến việc giảm tiêu thụ điện năng của các mô hình. Như vậy, chúng tôi không thấy tác động đạo đức hoặc xã hội tiêu cực tức thì nào của công việc chúng tôi ngoài những gì áp dụng cho các khối xây dựng cốt lõi khác của học sâu.

--- TRANG 8 ---
Tài liệu tham khảo

Rosa I Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. Machine Learning, 63(2):161–182, 2006.

Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.

Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs, 2018.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.

Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218, 1936.

Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme fixed-point compression. arXiv preprint arXiv:2004.07320, 2020.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems, pp. 103–112, 2019.

Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2704–2713, 2018.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. ACL, 2019.

W Johnson J Lindenstrauss. Extensions of lipschitz maps into a hilbert space. Contemp. Math, 26: 189–206, 1984.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pp. 142–150. Association for Computational Linguistics, 2011.

--- TRANG 9 ---
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.

Abdelrahman Mohamed, Dmytro Okhonko, and Luke Zettlemoyer. Transformers with convolutional context for asr. arXiv preprint arXiv:1904.11660, 2019.

Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1–9, 2018.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48–53, 2019.

Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, 2016.

Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. CoRR, abs/1804.07461, 2018. URL http://arxiv.org/abs/1804.07461.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19–27, 2015.

--- TRANG 10 ---
A Chứng minh Định lý 1

Chứng minh. Ý tưởng chính của chứng minh dựa trên bổ đề phân phối Johnson–Lindenstrauss (Lindenstrauss, 1984) (JL, tắt), phiên bản sau đây từ (Arriaga & Vempala, 2006).

Bổ đề 1. Cho R là ma trận k×n, 1 ≤ k ≤ n, với các mục i.i.d. từ N(0,1/k). Với bất kỳ x,y ∈ R^n, chúng ta có

Pr(‖Rx‖ ≤ (1 + ε)‖x‖) > 1 - e^(-ε²-ε³)k/4, (11)

Pr[‖xR^T Ry^T - xy^T‖ ≤ ε‖x‖‖y‖] > 1 - 2e^(-ε²-ε³)k/4. (12)

Để đơn giản, chúng tôi sẽ bỏ chỉ số i cho ma trận W^K_i, W^Q_i, W^V_i, E_i và F_i. Chúng tôi sẽ coi Q như QW^Q, K như KW^K và V như VW^V. Định nghĩa

A = QW^Q_i(KW^K_i)^T/√d (13)

Dựa trên định nghĩa của ma trận ánh xạ ngữ cảnh P, chúng ta có

P = softmax[QW^Q_i(KW^K_i)^T/√d] = exp(A)D^(-1)_A, (14)

trong đó D_A là ma trận đường chéo n×n sao cho

(D_A)_ii = Σ^n_{j=1} exp(A_ji) (15)

Ở đây chúng tôi cung cấp một chứng minh xây dựng. Cho bất kỳ lỗi xấp xỉ ε > 0, định nghĩa ma trận sau.

P̃ = exp(A)D^(-1)_A R^T R, (16)

trong đó R là ma trận k×n, 1 ≤ k ≤ n, với các mục i.i.d. từ N(0,1/k). Rõ ràng hạng của ma trận P̃ thỏa mãn

rank(P̃) ≤ rank(R) = k. (17)

Chúng tôi tiếp tục chỉ ra rằng, khi k = O(log(n)), chúng ta có rằng, với bất kỳ vector cột w ∈ R^n,

Pr[‖P̃w - Pw‖ ≤ ε‖Pw‖] > 1 - o(1). (18)

Điều này kết luận định lý. Với bất kỳ vector hàng u ∈ R^n của ma trận P và bất kỳ vector cột w ∈ R^n của ma trận VW^V, áp dụng Bổ đề JL, chúng ta có thể thu được

Pr[‖uR^T Rw^T - uw^T‖ ≤ ε‖uw^T‖] > 1 - 2e^(-ε²-ε³)k/4. (19)

Do đó, chúng ta có

Pr[‖P̃w - Pw‖ ≤ ε‖Pw‖] = Pr[‖PR^T Rw - Pw‖ ≤ ε‖Pw‖]

(a) ≥ 1 - Σ_{x∈P} Pr[‖xR^T Rw^T - xw^T‖ > ε‖xw^T‖]

(b) > 1 - 2ne^(-ε²-ε³)k/4. (20)

Ở trên, bước (a) dựa trên giới hạn hợp. Bước (b) sử dụng kết quả của Bổ đề JL. Cho k = 5log(n)/(2ε³), thì định lý được chứng minh.

--- TRANG 11 ---
B Chứng minh Định lý 2

Chứng minh. Định nghĩa E = R và F = εR, trong đó R ∈ R^(n×k) với các mục i.i.d. từ N(0,1/k), ε là một hằng số với ε = 1/(2n). Chúng tôi sẽ đầu tiên chứng minh rằng với bất kỳ vector hàng x ∈ R^n của ma trận QK^T và vector cột y ∈ R^n của ma trận V,

Pr[‖exp(xE^T)Fy^T - exp(x)y^T‖ ≤ ε‖exp(x)y^T‖] > 1 - 2e^(-2ε³k/4). (21)

Dựa trên bất đẳng thức tam giác, chúng ta có

‖exp(xE^T)Fy - exp(x)y^T‖ ≤ ‖exp(xE^T)Fy - exp(x)R^T Ry‖ + ‖exp(x)R^T Ry - exp(x)y^T‖

(a) ≤ (1 + ε)‖y‖‖exp(xE^T) - exp(x)R^T‖ + ‖exp(x)R^T Ry - exp(x)y^T‖

(b) ≤ ‖exp(x)R^T Ry - exp(x)y^T‖ + o(‖exp(x)‖‖y‖)

(c) ≤ ε‖exp(x)‖‖y‖ + o(‖exp(x)‖‖y‖) (22)

Ở trên, bước (a) dựa trên bất đẳng thức Cauchy và Bổ đề JL trong (11). Bước (b) sử dụng thực tế rằng hàm mũ là liên tục Lipschitz trong một vùng compact. Sau đó chúng ta có thể chọn ε đủ nhỏ, tức là ε = (1/n) sao cho

‖exp(xR) - exp(x)R‖ = o(‖exp(x)‖) (23)

Bước (c) dựa trên Bổ đề JL được định nghĩa trong (12).

Áp dụng kết quả trong (21) cho mỗi vector hàng của ma trận A và mỗi vector cột của ma trận V, người ta có thể chứng minh trực tiếp rằng, với bất kỳ vector hàng A_i của ma trận A,

Pr[‖exp(A_i E^T)FV - exp(A_i)V‖ ≤ ε‖exp(A_i)‖‖V‖] > 1 - o(1), (24)

bằng cách đặt k = 5log(nd)/(2ε³). Kết quả này không sử dụng tính chất hạng thấp của ma trận A (rank(A) = d) và k kết quả có sự phụ thuộc vào độ dài chuỗi n. Chúng tôi sẽ tiếp tục chứng minh việc chọn k có thể là hằng số và độc lập với độ dài chuỗi n.

Dựa trên thực tế rằng rank(A) = d, chúng ta có thể tìm một ma trận con hàng A_s ∈ R^(2d×d) của ma trận exp(AE^T)FH sao cho rank(A_s) = d. Áp dụng kết quả trong (21) cho mỗi vector hàng của ma trận A_s và mỗi vector cột của ma trận V, và k = 9log(d)/(2ε³), chúng ta có thể thu được rằng, với bất kỳ vector hàng A^s_i của ma trận A_s,

Pr[‖exp(A^s_i E^T)FV - exp(A^s_i)V‖ ≤ ε‖exp(A^s_i)‖‖V‖] > 1 - o(1), (25)

Hơn nữa, định nghĩa ma trận Ψ ∈ R^(n×2d) như

Ψ = [exp(AE^T)FV / exp(A)V] [exp(A_s E^T)FV / exp(A_s)V]^(-1) (26)

Chúng ta có rằng, với bất kỳ vector hàng A_i của ma trận A, 1 ≤ i ≤ n.

‖exp(A_i E^T)FV - exp(A_i)V‖ = ‖Ψ_i [exp(A_s E^T)FV - exp(A_s)V]‖

(a) ≤ ‖[exp(A_s E^T)FV - exp(A_s)V]^T‖_2 ‖Ψ_i‖

(b) ≤ √d‖exp(A_s E^T)FV - exp(A_s)V‖_F

= √d Σ^(2d)_(i=1) ‖exp(A^s_i E^T)FV - exp(A^s_i)V‖

(c) ≤ √d Σ^(2d)_(i=1) ε‖exp(A^s_i)‖‖V‖

≤ √d ε‖exp(A_s)‖‖V‖

Ở trên, bước (a) sử dụng bất đẳng thức ‖Ax‖ ≤ ‖A‖_2‖x‖, trong đó ‖A‖_2 = √λ_max(A^T A) (λ_max(·) là giá trị eigen lớn nhất) là chuẩn phổ của ma trận A. Bước (b) dựa trên bất đẳng thức chuẩn ma trận ‖A‖_2 ≤ ‖A‖_F, trong đó ‖A‖_F = (Σ_(1≤i,j≤n) A²_ij)^(1/2) là chuẩn Frobenius của ma trận A. Bước (c) dựa trên kết quả của (24).

--- TRANG 12 ---