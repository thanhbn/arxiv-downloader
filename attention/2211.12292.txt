# 2211.12292.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2211.12292.pdf
# File size: 5383761 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Exemplar-free Continual Learning of Vision Transformers
via Gated Class-Attention and Cascaded Feature Drift
Compensation
Marco Cotogni, Fei Yang, Claudio Cusano,
Andrew D. Bagdanov, Joost van de Weijer
Abstract Vision transformers (ViTs) have achieved
remarkable successes across a broad range of computer
vision applications. As a consequence there has been
increasing interest in extending continual learning the-
ory and techniques to ViT architectures. We propose a
new method for exemplar-free class incremental train-
ing of ViTs. The main challenge of exemplar-free con-
tinual learning is maintaining plasticity of the learner
without causing catastrophic forgetting of previously
learned tasks. This is often achieved via exemplar re-
play which can help recalibrate previous task classifiers
to the feature drift which occurs when learning new
tasks. Exemplar replay, however, comes at the cost of
retaining samples from previous tasks which for many
applications may not be possible.
To address the problem of continual ViT training,
we first propose gated class-attention to minimize the
drift in the final ViT transformer block. This mask-
based gating is applied to class-attention mechanism of
the last transformer block and strongly regulates the
weights crucial for previous tasks. Importantly, gated
class-attention does not require the task-ID during in-
ference, which distinguishes it from other parameter
isolation methods. Secondly, we propose a new method
offeature drift compensation that accommodates fea-
ture drift in the backbone when learning new tasks.
The combination of gated class-attention and cascaded
feature drift compensation allows for plasticity to-
M. Cotogni and C. Cusano are with the Dept. of Electrical,
Computer and Biomedical Engineering, University of Pavia,
Pavia, Italy. E-mail: marco.cotogni01@universitadipavia.it,
claudio.cusano@unipv.it.
F. Yang and J. van de Weijer are with the Computer Vi-
sion Center, Universitat Aut` onoma de Barcelona, Barcelona
08193, Spain. E-mail: {fyang, joost }@cvc.uab.es.
A. D. Bagdanov is with MICC, University of Florence Flo-
rence 50134, FI, Italy. E-mail: andrew.bagdanov@unifi.itwards new tasks while limiting forgetting of previous
ones. Extensive experiments performed on CIFAR-100,
Tiny-ImageNet and ImageNet100 demonstrate that our
exemplar-free method obtains competitive results when
compared to rehearsal based ViT methods.1
Keywords Continual Learning, Vision Transformer,
Exemplar-Free, Class-Incremental.
1 Introduction
The initial excellent results of transformers for language
tasks (Vaswani et al., 2017) have encouraged its appli-
cation also for vision applications (Dosovitskiy et al.,
2020). Vision Transformers (ViTs) currently achieve ex-
cellent results for many applications (Caron et al., 2021;
Liu et al., 2021; Strudel et al., 2021). Most existing work
on ViT training assumes that all training data is jointly
available, an assumption which does not hold for real-
world applications in which data arrives in a sequence
of non-overlapping tasks. Continual learning considers
learning from a non-IID stream of data. Applying a
naive finetuning approach to such data results in a phe-
nomenon called catastrophic forgetting which results in
a drastic drop in performance on previous tasks (Good-
fellow et al., 2014). The main goal of continual learning
algorithms is to maximize the stability-plasticity trade-
off (Mermillod et al., 2013), i.e. to mitigate forgetting
of previously learned classes while maintaining the plas-
ticity required to learn new ones.
One of the most successful approaches to prevent-
ing forgetting of previous tasks is exemplar rehearsal in
which a subset of images from previous tasks is stored
and then rehearsed when learning new ones (Bang et al.,
2021; Buzzega et al., 2020, 2021; Chaudhry et al., 2019;
1Code: https://github.com/OcraM17/GCAB-CFDCarXiv:2211.12292v3  [cs.CV]  27 Jul 2023

--- PAGE 2 ---
2 Cotogni et al.
20 40 60 80 10020406080Accuracy
CIFAR100 5 Steps
10 20 30 40 50 60 70 80 90 10020406080
CIFAR100 10 Steps
20 40 60 80 100 120 140 160 180 200
Observed Classes20406080Accuracy
Tiny-ImageNet 10 Steps
10 20 30 40 50 60 70 80 90 100
Observed Classes20406080
ImageNet100 10 Steps
Ours
DyT oX-200
DyT oX-500
LVT-200
LVT-500
Fig. 1: Comparison between GCAB and transformer-
based state-of-the-art methods (Douillard et al., 2022;
Wang et al., 2022a) (number refers to number of exem-
plars in replay buffer). Our exemplar-free method ob-
tains comparable performance compared to rehearsal-
based methods on all scenarios. LVT results are only
reported after the last task, since no code has been re-
leased.
Riemer et al., 2018; Rebuffi et al., 2017; Lopez-Paz and
Ranzato, 2017). Because of its success, the rehearsal
technique has also been adopted by the initial works
on continual learning for ViTs (Douillard et al., 2022;
Wang et al., 2022a). However, for many applications
the storage of previous task data might not be possible.
This is especially true for applications with strict mem-
ory constraints and those where privacy or data use
legislation prevents the long-term storage of data. In
order to overcome this limitation, exemplar-free meth-
ods have been investigated (Kirkpatrick et al., 2017; Li
and Hoiem, 2017; Yan et al., 2021; Yu et al., 2020).
These methods do not store any data from previous
tasks, however their application to continual learning
of ViTs has not been fully explored.
In this paper, we propose one of the first exemplar-
free ViT-based methods for class-incremental learning
(CIL). One of the challenges of exemplar-free contin-
ual learning is that these models tend to forget previ-
ously learned features while they are learning new tasks.
The architecture is based on a gated class-attention
mechanism applied to the ViT decoder in order to mit-
igate the forgetting of learned features. Existing mask-
based gating methods have the major drawback that
they can only be applied to task-incremental learning,
since they require the task-ID at inference time (Abati
et al., 2020; Del Chiaro et al., 2020; Serra et al., 2018).
We propose a solution to overcome this limitation by
applying the masking mechanism only on the trans-
former decoder via multiple forward passes. This so-lution allows us to use mask-based gating for class-
incremental learning without the need for task-IDs at
inference time.
Mask-based gating prevents the drift of weights in
the decoder, however it does not mitigate the drift in
the transformer encoder. In fact, learning a stable back-
bone in the exemplar-free scenario is very difficult due
to the drift in encoder weights that occurs during the
learning of new tasks. To address this, we propose a
method for backbone regularization in combination with
a feature drift compensation mechanism that uses a cas-
cade of projection networks that map the current back-
bone features to those of the previous backbone. This
allows increased plasticity while maintaining stability
across tasks, incurring a small computational cost due
to the feature projection cascade. We also show, how-
ever, that knowledge distillation can be used to alleviate
the computational burden of the projection cascade and
the need for multiple forward passes at inference time.
The main contributions of this work are:
–a gated class-attention mechanism, called GCAB,
that mitigates weight drift in the transformer de-
coder, which is the first application of an activation-
masking method for class-incremental learning;
–a novel method for feature drift compensation that
increases plasticity towards new classes while main-
taining the stability of previously learned ones;
–a method for GCAB distillation that reduces the
computational overhead due to multiple forward
passes and the memory overhead of storing projec-
tion networks;
–experiments on multiple benchmarks demonstrate
that our exemplar-free approach achieves state-of-
the-art performance compared to other exemplar-
free methods and outperforms recent continual
learning methods developed for ViT even when
these are equipped with a small memory of exem-
plars (see Figure 1).
2 Related Work
Continual Learning. Continual learning algorithms
can be grouped in three categories (Delange et al.,
2021): regularization approaches, parameter based reg-
ularization (Kirkpatrick et al., 2017; Lee et al., 2020;
Liu et al., 2018; Zenke et al., 2017), and data based reg-
ularization (Castro et al., 2018; Dhar et al., 2019; Hou
et al., 2019; Jung et al., 2016; Li and Hoiem, 2017; Wu
et al., 2019; Zhang et al., 2020); rehearsal approaches,
which store (Chaudhry et al., 2018; Rebuffi et al., 2017)
or generate exemplars (Wang et al., 2021; Zhai et al.,

--- PAGE 3 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 3
2021); and bias-correction approaches (Castro et al.,
2018; Hou et al., 2019; Wu et al., 2019).
Continual Learning with VITs. Visual Transform-
ers recently outperforms convolutional neural networks
and in particular resnet (He et al., 2016) in several tasks
like classification (Dosovitskiy et al., 2020) or segmen-
tation (Zheng et al., 2021). Although ViTs are consid-
ered state-of-the-art models, their application in con-
tinual learning has not been fully explored. Douillard
et al. (2022) proposed a transformer-based architec-
ture, called DyTox, with a dynamic task-token expan-
sion for mitigating catastrophic forgetting. Wang et al.
(2022a) proposed an inter-task attention mechanism
for ViTs. Wang et al. (2022b) described a prompting
method for continually learning a classifier using a pre-
trained, frozen ViT backbone. Even if the performances
showed in these work are very remarkable, the challenge
of continually learning the parameters of a ViT without
storing exemplars or using a pretrained model, is still
open. Differently from previous work, we propose an
exemplar-free ViT approach to class-incremental learn-
ing. Our work is inspired by DyToX (Douillard et al.,
2022), which applies a task conditioned class-attention
block. However, DyToX shares the class-attention block
parameters between tasks which leads to forgetting, and
it therefore requires exemplars to counter this. Instead,
we replace the task token with a task-specific gating
function that prevents forgetting and does not require
exemplars. In addition, we introduce feature drift com-
pensation, which allows for more plasticity in the back-
bone.
Parameter Isolation in Continual Learning. In
this family of algorithms, learnable masks are applied
to the weights of the model in order to reduce for-
getting. Mallya et al. (2018) proposed Piggyback, a
masked-based method able to learn the weight masks
while training a backbone. The same group proposed
Packnet (Mallya and Lazebnik, 2018) which, via itera-
tive pruning and sequential re-training, is able to add
multiple tasks to a single network. Serra et al. (2018)
proposed to apply masks to layer activations in order
to limit the update of the parameters more relevant to
a specific task. Masana et al. (2021) proposed a system
of ternary-masks applied on to layer activations for pre-
venting catastrophic forgetting and backward transfer.
Yan et al. (2021) proposed a Dynamical Expandable
Representation (DER) for continual learning. In this
work, channel-level masks are used for pruning the fea-
ture extractor. Rajasegaran et al. (2019) proposed Ran-
dom Path Selection (RPS). This approach uses a pa-
rameter isolation mechanism, distillation, and a replay-
buffer to learn different paths for each task without the
need for a task-ID during inference.
Multiple ModelsFederated Online
Open World
Multiple ModalitiesActive Data Query
Task Order Discovery
Task AgnosticEpisodic MemoryGenerativeUncertaintyP
aram
etersCom
pute
tim
eMACoperationCommunication
Forgetting
Forwardtransfe
BackwardtransfO
penness
D
ata
per
task
Task
order
PertaskmetricOptimizationsteGenerateddataStoreddataMemory
GCAB GCAB-Fast DyToX
LVT A-DFig. 2: CLEVA-Compass for transformer-based meth-
ods (zoom in for a better view). In the inner circle,
GCAB, GCAB-Fast are covered by A-D, while Dytox
is covered by LVT. We do not report backward transfer
since it makes more sense for task-incremental learning.
Exemplar-Free Continual Learning. This is one
of the most challenging scenario in continual learning.
In this paradigm, it is not possible to store any ex-
emplars from the previously observed classes. Li and
Hoiem (2017) proposed an exemplar-free data regular-
ization approach to mitigate forgetting. This method
distills knowledge of the previous model into the new
one in order to prevent weight drift while learning
the new task data. Kirkpatrick et al. (2017) described
an exemplar-free weight regularization approach called
Elastic Weight Consolidation (EWC) for preventing
weight drift. Similarly, Aljundi et al. (2018) proposed a
method that accumulates the importance of each model
parameter by analyzing the effect of their change to the
predicted output. Yu et al. (2020) proposed a semantic
drift compensation mechanism to compensate for fea-
ture drift in previous tasks by approximating it with
the drift estimated with current task data. Toldo and
Ozay (2022) presented a framework for modeling the se-
mantic drift of model weights of and estimating feature
drift in the representation of previously learned classes.
Pelosin et al. (2022) proposed an attention distillation
mechanism for exemplar-free visual transformer in task-
incremental learning.
In Figure 2 we report the CLEVA-Compass Mundt
et al. (2021) comparing our method, GCAB and

--- PAGE 4 ---
4 Cotogni et al.
Patch tokenizerTransformer encoder
Cascaded feature
drift compensation
(see Fig.3 b)Gated class
tokenGCAB
Patch tokens
Classifer
Classifer
Classifer
Classifer(see Fig.3 a)
Fig. 3: Overview of the architecture. Our main contributions are the gated class-attention block (GCAB) to prevent
forgetting in the final ViT block (Section 3.2) and the cascaded feature drift compensation to compensate for feature
drift of the backbone network (Section 3.5).
GCAB-Fast, to other transformer methods (A-D, Dy-
Tox, and LVT) on several measures.
3 Method
3.1 Problem setup
Here we define the class-incremental learning setup and
the specific Vision Transformer architecture we use.
Class-incremental learning setup. In class-
incremental learning the model learns a sequence of T
tasks, where each task tintroduces a number of new
classes Ct. The data Dtof task tcontains samples
(xi, yi), where xiis input data labeled by yi∈Ct.
Note that we consider the case in which that there is
no overlap between different task label sets: Ci∩Cj=∅
ifi̸=j, as is commonly assumed (Masana et al., 2020).
The model is evaluated on all previously seen classes
C≤t=∪t′≤tCt′. Class-incremental learning differs from
task-incremental learning in that it has no access to the
task label tat inference time, and is therefore consid-
ered a more challenging setting (Delange et al., 2021).
Furthermore, we consider the more restrictive setup of
exemplar-free incremental learning in which no data
from previous tasks is saved.
Transformer architecture. We use a vision trans-
former based on the one proposed by Dosovitskiy et al.
(2020) and the recent improvements of Touvron et al.
(2021). It consists of a transformer encoder and de-
coder, each built with several multi-head attention
blocks. In Figure 3 we give a schematic diagram of our
architecture. Formally, the input image x∈RH×W×C
is passed through a patch tokenizer that splits xinto
Npatches and projects them using a 2D convolutional
layer to obtain a set of Npatch tokens x0∈RN×D.A learnable position embedding is added to the patch
tokens as in (Gehring et al., 2017). The patch tokens
x0are passed as input to a sequence of Mtransformer
encoder blocks, each yielding tensors of the same di-
mensions. Each block is composed of a multi-head self-
attention (SA) mechanism (Vaswani et al., 2017), layer
normalization and a Multi-layer Perceptron (MLP),
each with residual connections:
x′
l=xl+ SA( xl)
xl+1=x′
l+ MLP( x′
l)(1)
We follow the design of CaiT (Touvron et al., 2021), and
only insert a class token with a class-attention layer
in the last block of the decoder. The transformer de-
coder is composed of one single block. To distinguish
the various parts of the transformer network, we define
the image output prediction ˆ y=c(f(b(x;Ψ))), where
the backbone features b(x;Ψ)∈RN×Dparameterized
byΨare the output of the self-attention blocks , and
f(b(x;Ψ)) refers to the feature output of the decoder
before classifier c.
3.2 Gated class-attention
Parameter isolation methods work by isolating a lim-
ited set of parameters after learning each task (De-
lange et al., 2021; Mallya and Lazebnik, 2018; Rusu
et al., 2016; Serra et al., 2018). Inspired by Serra et al.
(2018), we design a parameter isolation method called
gated class-attention that operates on the class at-
tention block of transformers. The method allows pa-
rameters of the network used by previous tasks to be
exploited by new tasks, thereby allowing for forward
transfer, but their update is restricted to prevent for-
getting. The main strengths of this approach are the

--- PAGE 5 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 5
1
0
0
0×
×EmbeddingAttentionPatch tokens cls
1
1
1
0∗ ∗
∗
×V × ∗ ×
softmax
updated params
for current taskfree params
for future taskK Q
regularized params
for previous taskMLP
MLPMLP(a) (b)
Fig. 4: (a) The gated self-attention layer (for a single head) in GCAB. (b) Feature Drift Compensation applies a cas-
cade of feature projection networks to map the current backbone features to previous task features to compensate
for feature drift.
good forward transfer with little or no forgetting of pre-
vious tasks, together with the ability to automatically
learn which neurons to dedicate to each task within the
capacity limit of the neural network.
The forward pass of parameter isolation methods
is usually conditioned on the task (Mallya and Lazeb-
nik, 2018; Masana et al., 2021; Serra et al., 2018), and is
therefore restricted to task-incremental learning. A pos-
sible way to extend these methods to class-incremental
learning would be to run one forward pass for each task
and then combine the task predictions (e.g., by con-
catenation). However, this would increase the run time
linearly in the number of tasks. To limit computational
overhead, we propose to only apply attention masks to
the last block of the ViT. In Section 3.5, we investigate
distillation to further reduce computational overhead.
Mask-based class-attention gating. We apply the
attention-gating in the last transformer block, i.e. the
decoder, which contains class-attention (Touvron et al.,
2021). This block combines the patch tokens from pre-
vious blocks with a learnable class token θ.
The learnable masks mfor task tare defined as:
mt=σ(sAtT), (2)
where, slightly abusing notation, trepresents both a
task index and a one-hot vector identifying the current
task, Ais a learnable embedding matrix, σis the sig-
moid activation function, and sis a positive scaling pa-
rameter. Here mtrefers to the neurons that have been
selected for task t.
We can now define the forward pass through the
final class-attention block by introducing a mask for
all the activations contributing to the final class token
output. These masks correspond to: the input tokens
(mt
i), queries and keys ( mt
QK), values ( mt
V), the MLP
(m1andm2), and the class-attention output ( mo). Wecan then compute the query ( Q), key ( K), value ( V),
attention ( A) and self attention output ( O) given the
block token inputs Otand these masks:
p= [θ, b]∈R(N+1)×D
Qt=Wq(θ⊙mt
i)
Kt=Wk(p⊙mt
i)
Vt=Wv(p⊙mt
i) (3)
At= softmaxn 
Qt⊙mt
QK 
Kt⊙mt
QKT
/p
d/ho
Ot=WoAt(Vt⊙mt
V).
The gating mechanism is also applied to the MLP
(see Eq. 1) according to:
b′t=Ot+θ
ut=W1(b′t⊙mt
1)
vt=W2(ut⊙mt
2)
ft=vt+Ot(4)
We call this the Gated Class-Attention Block (GCAB).
The masks are learned during the training of task t
and their role is twofold: they select those activations
that are used to compute the task-conditioned output
and they restrict the backpropagation of future tasks,
preventing changes to weights that used by previous
tasks. In the experimental section, we show that good
results are obtained when sharing the weights and set-
tingmQK=mV=m1=mo=mi, resulting in only
two learnable masks miandm2. In Figure 4 (a) we
show the interaction between the masks miand self-
attention layer in the class-attention block.
For each task a dedicated classifier ctis added which
produces predictions ˆ yt=Wclf(ft⊙mt
1) using the vec-
torftoutput from the GCAB. We perform multiple
forward passes of patches extracted from the backbone

--- PAGE 6 ---
6 Cotogni et al.
btthrough the decoder ft(i.e. we pass it ttimes). For
each pass s, the obtained vectors which we denote by
ft
s(and are computed with the masks ms), are then
passed to the corresponding classifier cs. The classifier
outputs are then concatenated C= [c1,···, ct] and the
binary cross entropy loss LBCEis computed.
Training. During the training of the current task t
the scaling parameter sfrom equation 2, is scaled with
the batch index: s=1
smax+ (smax−1
smax)i−1
I−1, where i
current batch index and Iis the total number of batches
in an epoch. This was found to be beneficial in Serra
et al. (2018).
At the end of the current task, the learned masks
are accumulated as m<t
∗= max( mt−1
∗, m<t−1
∗), where
’∗’ stands for any of the specific mask subscripts intro-
duced above. Accumulated masks m<t
∗are used during
backpropagation to prevent updating weights consid-
ered important for the tasks observed so far. The masks
are learned by minimizing the following loss function:
LGCAB =λGCABP
xmt
x(1−m<t
x)P
x(1−m<tx), (5)
where mt
xis the mask learned at the current task tfor
component xof the GCAB, xranges over the mask
subscripts described above, and m<t
xis the cumulative
mask. λGCAB is a tunable hyperparameter controlling
the capacity of the masks learned during the tasks. This
equation encourages new task mask mt
xto be sparse,
however it permits use of activations already used by
previous tasks m<t
xat no cost.
The cumulative masks play a pivotal role during the
training of new tasks. Consider, for example, weights
Wqthat map from the input tokens (masked by mt
i)
to the queries (masked by mt
QK). We then define the
elements of the weight mask according to M<t
q,kl= 1−
min(m<t
i,k, m<t
QK,l) where m<t
i,krefers to the k-th element
ofm<t
i. The update rule for the backpropagation of the
gradient is then: Wq=Wq−λM<t
q⊙∂L
∂Wq. This update
rule prevents the updating of part of the weights learned
for previous tasks. The input mask also influences the
updating of the class token embedding which is given
byWθ=Wθ−λ(1−m<t
i)⊙∂L
∂Wθ.
For completeness, we report here the update rules
for all weight matrices in the GCAB:
WQ=WQ−λM<t
q⊙∂L
∂WqWk=Wk−λM<t
k⊙∂L
∂Wk
Wv=Wv−λM<t
v⊙∂L
∂WvWθ=Wθ−λ(1−m<t
i,k)⊙∂L
∂Wθ
Wo=Wo−λM<t
o⊙∂L
∂WoW1=W1−λM<t
1⊙∂L
∂W1
W2=W2−λM<t
2⊙∂L
∂W2Wclf=Wclf−λ(1−m<t
o)⊙∂clf
∂Wclf.
Moreover, the weight masks are defined as:
M<t
q,kl= 1−min(m<t
i,k, m<t
QK,l)M<t
k,kl= 1−min(m<t
i,k, m<t
QK,l)
M<t
v,kl= 1−min(m<t
i,k, m<t
V,l) M<t
o,kl= 1−min(m<t
V,k, m<t
1,l)
M<t
1,kl= 1−min(m<t
1,i, m<t
2,l) M<t
2,kl= 1−min(m<t
2,i, m<t
o,l).3.3 Backbone Regularization and Cascaded Feature
Drift Compensation
In the previous section, we applied gating only to the
last transformer block to limit computational overhead.
Gating ensures that only minimal changes occur to the
weights relevant to previous tasks, however, the network
can still suffer from forgetting due to backbone feature
drift.
A straightforward way to prevent forgetting in the
backbone network via regularization is feature distilla-
tion (Hou et al., 2019). Feature distillation encourages
backbone features at task tto remain close to those
at task t-1, however, this was found to limit plastic-
ity (Douillard et al., 2020). To ensure stability with-
out sacrificing plasticity, some recent works in con-
tinual learning of self-supervised representations have
proposed to learn a projector between feature extrac-
tors (Fini et al., 2022; Gomez-Villa et al., 2022). The
approach, called Projected Functional Regularization
(PFR), introduces a projection network ptthat maps
the current backbone features to those of the previous
backbone. This allows the new backbone to learn new
features without imposing a high regularization penalty
as long as the new features can still be projected back
to those of the previous backbone. The loss function is
given by:
Lpfr=λpfrEx∼Dt
S 
pt 
b 
x;Ψt
, b 
x;Ψt−1
(6)
where Sis the cosine distance, λpfris a trade-off param-
eter, and Ψtrefers to the parameters of the backbone
after learning task t.
The gained plasticity induced by PFR leads to a
misalignment of the current backbone with previous
class-attention layers and classifiers. This is not a prob-
lem during self-supervised learning (Fini et al., 2022;
Gomez-Villa et al., 2022), however for supervised learn-
ing when a previously learned classifier is applied to
the current backbone it becomes problematic. There-
fore, as the second main contribution, we propose cas-
caded feature drift compensation that extends the use of
projected feature regularization to supervised settings.
This is especially relevant for exemplar-free methods,
where alignment with previous tasks is challenging due
to the absence of replay data.
The regularization of Eq. 6 results in backbone drift,
and therefore bt̸=bt−1. However, since we have the
projection matrix ptwe can approximate the previous
backbone according to bt−1≈pt(bt). Continuing this,
we perform cascaded feature drift compensation (FDC)
(see Figure 4(b)) which applies the projection networks
from previous tasks in a cascade to align the current
backbone with the learned class-attention block at any

--- PAGE 7 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 7
Task 1 Task 2
(a) (b) (c) (d)
Fig. 5: t-SNE visualization of the output of the decoder f(·) after the first task and second task. These visualizations
are based on the task-agnostic embeddings in the 10 task scenario on CIFAR-100. (a) Results after task 1. Results
after task 2 (b) without PFR during training, (c) with PFR during training but without feature drift compensation,
and (d) with PFR during training and with feature drift compensation. More t-SNE results are provided in the
Appendix B.
previous task s≤t:
ˆys=ct(ft
s(ps+1(p···(pt−1(pt(bt(x)))))) . (7)
Note that here ft
srefers to the class attention block
output at task tcomputed based on the masks ms. The
projection networks ptmust be saved in this formula-
tion in order to compute the projection cascade that
compensates for backbone feature drift. In Section 3.5
we show how knowledge distillation can be used to elim-
inate the need for projection networks ptat inference
time.
To illustrate the effectiveness of cascaded feature
drift compensation, we analyzed the embeddings pro-
duced by the GCAB. In Figure 5 we visualize the em-
bedding produced by the GCAB using t-SNE (Van der
Maaten and Hinton, 2008). The embedding of the im-
ages from the test set of the first task are shown after
the training of the first task and the second task. Only
applying PFR during training results in a latent space
where classes no longer align with their location after
task 1 (see Figure 5(c)). This is problematic since the
previously trained classifier of task 1 no longer aligns
with them and will therefore suffer a significant drop in
performance (as verified in our ablation study). After
applying the cascaded feature drift compensation, the
class distributions are mapped back to their original
locations and align again with the classifier head (see
Figure 5(d)). In conclusion, this illustration shows that
cascaded feature drift compensation can be a strong
tool to recover from feature drift in the backbone. This
is important since it allows for high plasticity during
the continual learning process.3.4 Training Objective and Inference
The final objective we use for incremental training of
the model is the sum of the three loss functions:
L=LBCE+Lpfr+LGCAB (8)
After training the current task, an evaluation phase is
performed over all classes seen so far. At task t, the
model is tested for the tasks t′≤t. All images are
passed through the backbone to obtain tokens b. The
tokens are passed ttimes through the Gated Class-
Attention Block. Based on the task index of the for-
ward pass t′, the composition of the previously stored
projection networks is used as in Equation 7 to align
the current backbone features to those of previous task
t′. In Figure 4 (b) we give a schematic diagram of the
Feature Drift Compensation mechanism during infer-
ence. During inference the parameter sof Equation 2
is equal to smax.
3.5 GCAB Distillation
To overcome the increased computational cost due
to multiple forward passes for all tasks and the cas-
caded projection layers, we perform knowledge dis-
tillation (Hinton et al., 2015) to transfer the class-
conditioned GCAB (the teacher ) into a single class-
attention block (CAB) with the same architecture as
the GCAB but without masks, and an aggregated clas-
sifier cAt(student ). This reduces the number of pa-
rameters in the network, since the task projection net-
works are no longer needed, and eliminates the need for
multiple forward passes. We name this distilled version
GCAB-Fast . As shown in Figure 6, the CAB and cAt
are trained by minimizing the Kullback–Leibler (KL)

--- PAGE 8 ---
8 Cotogni et al.
Projection  
 networkGCABTeacherlogits
CAB
StudentlogitsKL divergence
Frozen
Learnable
Fig. 6: Illustration of GCAB distillation.
divergence between the logits output by the teacher and
student models. Note that this distillation is conducted
only with the data from task t, while the transformer
backbone band teacher hyper-classifier are frozen dur-
ing training. We also investigate the use of static masks
in the CAB to leave unused capacity in the student net-
work in order to accommodate potential future tasks
(see Section 4.7 for details).
4 Experimental Results
In this section we describe experimental results ob-
tained with GCAB and GCAB-Fast. We begin with a
description of our experimental setup and datasets used
for comparison with the state-of-the-art.
4.1 Experimental Setup and Datasets
Our network is based on the DytoX (Douillard et al.,
2022) transformer architecture. The number of trans-
former encoder blocks M= 5, each one with H= 12
heads for the multi-head self-attention mechanism. The
dimension of the embeddings is set to D= 384. We
use a 2-layer MLP for the projection network with also
a dimensionality of 384 in the middle. We train each
task for 500 epochs using Adam with lr= 1e−4and a
batch size of 128. We set hyperparameters λpfr= 0.001,
λGCAB = 0.05 and smax= 800.
For our experiments, we consider three datasets:
CIFAR-100 (Krizhevsky et al., 2009), Tiny-
ImageNet (Le and Yang, 2015) and ImageNet100 (Rus-
sakovsky et al., 2015). The CIFAR-100 dataset is
composed of 60,000 images, each 32 ×32 pixels and
divided into 100 classes. Each class has 500 training
and 100 test images. The Tiny-ImageNet is a reduced
version of the original ImageNet dataset with 200
classes. The classes are split into 500 for training, 50
for validation, and 50 for test (for a total number
of 120,000 images). The images are 64 ×64 pixels.
The ImageNet100 dataset is a selection of 100 classesfrom the larger ImageNet dataset (composed of 1000
classes). The images in this reduced version are 120,000
for training and 5000 for test. For our purposes, we
resized the images of this third dataset to 224 ×224.
4.2 Comparison with the State-of-the-art
We consider two different CIL scenarios: 5 tasks and 10
tasks equally split among all classes. For CIFAR-100,
tasks contain 20 classes for the 5 task scenario and 10
for the 10 task scenario. For Tiny-ImageNet and Im-
ageNet100 we consider only the 10-task scenario, with
20 and 10 classes in each task, respectively. For CIFAR-
100 and Tiny-ImageNet the images are split in N= 64
patches. For ImageNet100 the number of patches is
N= 196. We report the task-agnostic top-1 accuracy
over all the classes of the dataset after training the last
task: ACC TAG =1
NPN
t=1at, where Nis the total num-
ber of classes in the dataset. For the task-aware scenario
we report the mean accuracy over all the tasks after
training the last task: ACC TAW =1
TPT
t=1at, where
T is the total number of tasks. The memory buffer for
the exemplar-based methods is limited to 200 images,
which is the setting proposed by Wang et al. (2022a).
From Tables 1 and 2 we see that our method out-
performs all others in both class-incremental and task-
incremental scenarios. Our method more than doubles
the class-IL results obtained by A-D, the only other
exemplar-free ViT method, on all experiments. Espe-
cially on the more challenging Tiny-ImageNet and the
larger ImageNet100, our method obtains competitive
results compared to all the other methods. In particu-
lar, for the class incremental setting we obtain a signif-
icant improvement of 9-20% with respect to LVT even
though LVT uses 200 exemplars and we do not. Also no-
table are the improved results with respect to DyToX,
which, like LVT, is based on the same architecture as
ours but uses exemplars. Finally, our distilled version,
which requires neither multiple forward passes, nor mul-
tiple projection networks (denoted by GCAB-Fast ), ob-
tains very good results only slightly below GCAB . Note
that we are using GCAB distillation with 80% capacity
usage in the distilled GCAB-Fast .
In Figure 1 we also include results for 500 exem-
plars. We see that our exemplar-free method obtains
competitive results, even outperforming LVT and Dy-
ToX on Tiny-ImageNet and obtaining similar results as
DyToX-500 on ImageNet100. Further comparison with
500 exemplars is provided in Appendix A.

--- PAGE 9 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 9
Method Exemplar-FreeCIFAR-100 5 Tasks CIFAR-100 10 Tasks
Class-IL Task-IL Class-IL Task-IL
Joint ✓ 75.39 - 75 .39 -
ER (Riemer et al., 2018) ✗ 21.94 62 .41 14 .23 67 .57
GEM (Lopez-Paz and Ranzato, 2017) ✗ 19.73 57 .13 13 .20 62 .96
AGEM (Chaudhry et al., 2019) ✗ 17.97 53 .55 9 .44 55 .04
iCaRL (Rebuffi et al., 2017) ✗ 30.12 55 .70 22 .38 60 .81
FDR (Benjamin et al., 2018) ✗ 22.84 63 .57 14 .85 65 .88
GSS (Aljundi et al., 2019) ✗ 19.44 56 .11 11 .84 56 .24
DER++ (Buzzega et al., 2020) ✗ 27.46 62 .55 21 .76 59 .54
HAL (Chaudhry et al., 2021) ✗ 13.21 35 .61 9 .67 37 .49
ERT (Buzzega et al., 2021) ✗ 21.61 54 .75 12 .91 58 .49
RM (Bang et al., 2021) ✗ 32.23 62 .05 22 .71 66 .28
PASS (Zhu et al., 2021) ✓ 48.28 - 33 .76 -
SDC (Yu et al., 2020) ✓ 6.65 - 7.41 -
LVT†(Wang et al., 2022a) ✗ 39.68 66 .92 35 .41 72 .80
DyToX†(Douillard et al., 2022) ✗ 36.52 - 25 .94 -
A-D†(Pelosin et al., 2022) ✓ 15.61 42.82 16 .77 55 .53
GCAB†✓ 49 .86 81 .01 35 .90 82 .08
GCAB-Fast†✓ 48.85 79 .75 35 .42 81 .97
Table 1: Comparison on CIFAR-100. All non- exemplar-free methods use a memory buffer of 200 exemplars. The
accuracies reported here are the ACC TAG andACC TAW . The methods marked with†are based on ViT.
Method Exemplar-FreeTiny-ImageNet ImageNet100
Class-IL Task-IL Class-IL Task-IL
Joint ✓ 59.38 − 79.18 −
ER (Riemer et al., 2018) ✗ 8.79 39 .16 9 .58 36 .24
AGEM (Chaudhry et al., 2019) ✗ 8.28 23 .79 9 .27 25 .20
iCaRL (Rebuffi et al., 2017) ✗ 8.64 28 .41 12 .59 33 .75
FDR (Benjamin et al., 2018) ✗ 8.77 40 .15 10 .08 37 .80
DER++ (Buzzega et al., 2020) ✗ 11.16 40 .91 11 .92 31 .96
ERT (Buzzega et al., 2021) ✗ 10.85 39 .54 13 .51 36 .94
RM (Bang et al., 2021) ✗ 13.58 41 .96 16 .76 35 .18
PASS (Zhu et al., 2021) ✓ 24.23 - 25.22 -
SDC (Yu et al., 2020) ✓ 3.94 - 11.52 -
LVT†(Wang et al., 2022a) ✗ 17.34 46 .15 19 .46 41 .78
DyToX†(Douillard et al., 2022) ✗ 13.14 - 24 .82 -
A-D†(Pelosin et al., 2022) ✓ 6.10 18.02 10.92 39.13
GCAB†✓ 26 .82 65 .92 40 .10 81 .82
GCAB-Fast†✓ 26.44 65 .02 36 .22 80 .28
Table 2: Comparison on Tiny-ImageNet, and ImageNet100. All non exemplar-free methods use a memory buffer
of 200 exemplars.
Gated
Class
AttentionBackbone
Regularization
(PFR)Feature
Drift
CompensationCIFAR-100
ACCTAGTiny-ImageNet
ACCTAG
11.90 7.98
✓ 31.35 22.79
✓ ✓ 7.96 5.52
✓ ✓ ✓ 35.90 26.82
Table 3: Ablation study on the components of our archi-
tecture for 10-task scenarios on CIFAR-100 and Tiny-
ImageNet.
4.3 Ablation Study
We ablate on the importance of the different compo-
nents of our approach. In Table 3 we report four pos-
sible configurations on the 10-task split of CIFAR-100(and Tiny-ImageNet). First, we consider fine-tuning our
architecture without applying any continual learning
strategy to the base architecture, which results in very
low performance of 11.90% (7.98% Tiny-ImageNet).
Then we add gated class-attention to the transformer
decoder. This increases the average accuracy by 20%
(15% Tiny-ImageNet), showing the importance of pre-
venting forgetting in the final block. As explained in
Section 3.2, we use only two masks for gating the trans-
former decoder. This does not prevent backbone weight
drift when passing from one task to the next.
When we apply feature drift compensation, we ob-
tain better performance – notably, a more than 4% (4%
Tiny-ImageNet) increase. These results confirm the im-
portance of projecting the learned backbone features

--- PAGE 10 ---
10 Cotogni et al.
Dataset Method Exemplar-Free Model Size (M params) / (MB) Exemplars (#) / (MB) Total Size (MB) Accuracy
CIFAR100GCAB ✓ 12.4 / 47.30 0 / 0 47.30 49.86 35.90
GCAB-Fast ✓ 9.2 / 35.09 0 / 0 35.09 48.85 35.42
Dytox ✗ 10.7 / 40.82 200 / 2.34 43.16 36.52 25.94
Dytox ✗ 10.7 / 40.82 500 / 5.86 46.68 51.28 36.24
LVT ✗ 8.9 / 33.95 200 / 2.34 36.29 39.68 35.41
LVT ✗ 8.9 / 33.95 500 / 5.86 39.81 44.73 43.51
Tiny-ImagenetGCAB ✓ 12.4 / 47.30 0 / 0 47.30 26.82
GCAB-Fast ✓ 9.2 / 35.16 0 / 0 35.16 26.44
Dytox ✗ 10.7 / 40.81 200 / 9.38 50.19 13.14
Dytox ✗ 10.7 / 40.81 500 / 23.44 64.25 24.64
LVT ✗ 9.0 / 34.33 200 / 9.38 43.71 17.34
LVT ✗ 9.0 / 34.33 500 / 23.44 57.77 23.97
ImageNet100GCAB ✓ 12.7 / 48.4 0 / 0 48.4 40.10
GCAB-Fast ✓ 9.5 / 36.19 0 / 0 36.19 36.22
Dytox ✗ 11.0 / 41.96 200 / 114.84 156.80 24.82
Dytox ✗ 11.0 / 41.96 500 / 287.11 329.07 42.40
LVT ✗ 9.0 / 34.33 200 / 114.84 149.17 19.46
LVT ✗ 9.0 / 34.33 500 / 287.11 321.44 26.32
Table 4: Comparison between transformer-based methods. For CIFAR-100 we report both 5- and 10-split performance. Class-
incremental learning accuracy is reported.
to the previous feature space.2Note that only apply-
ing PFR without feature drift compensation leads to a
dramatic drop in performance to 7.96% (5.52% Tiny-
ImageNet).
4.4 Memory Requirements
To further improve comparability, we report here a
memory budget analysis in Table 4 motivated by the
paper suggested by (Zhou et al., 2022). GCAB requires
just a few MB more with respect to the other meth-
ods. On Tiny-ImageNet and ImageNet100, our method
is the one with the smaller computational burden while,
at the same time, being the most effective. However, on
CIFAR-100 with its smaller images, LVT obtains better
performance at lower total memory usage (using 500 ex-
emplars). We stress that, depending on the application,
the usage of exemplars might be forbidden.
4.5 Hyperparameter Analysis
We analyze the importance and the robustness of our
method with respect to the hyperparameters described
is Section 3. In Figure 7 we show the behavior of our
method in terms of ACC TAG on the CIFAR-100 5-task
scenario under changing hyperparameters. In particu-
lar, in the upper part of the figure the accuracy as a
function of λGCAB is shown. In this plot, the other hy-
perparameter λpfris kept fixed at 0.001. The accuracy
is stable, confirming the robustness of our method over
a wide range of values of λGCAB . Only for extremely
large values of λGCAB does the accuracy significantly
drop.
2We also applied feature distillation (Hou et al., 2019) as a
backbone regularization method. This does not increase per-
formance, yielding 30.50% (22.17% Tiny-ImageNet).
GCAB
48.048.549.049.5
PFR=0.001
103
102
101
100101
PFR
4546474849 GCAB=0.05
Fig. 7: ACC TAG on CIFAR-100 for the 5-task scenario
under varying hyperparameters. Top: for fixed λpfr, the
ACC TAG as a function of λGCAB .Bottom: for fixed
λGCAB , the ACC TAG as a function of λpfr.
In the lower part of Figure 7 we show the variation of
theACC TAG as a function of λpfrwhen λGCAB = 0.05.
For higher values of λpfr, the value of Lpfrstrongly
overpowers LBCE andLGCAB , pushing the model to
not correctly learn useful features for the classification
and the mask. For smaller values of λpfrthe magnitude
of this term of the loss function is comparable with the
others, which emphasizes the importance of correctly
learning a projector network that will be used during
inference.
4.6 Gating Capacity
In Figure 9, we show the percentage of used masked
capacity as tasks are added. The experiments were per-
formed on CIFAR-100 dataset for 5- and 10-task sce-

--- PAGE 11 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 11
20 40 60 80 100
Capacity usage (%)102030405060Accuracy (%)
CIFAR100 (task-agnostic)
Before distillation (10 splits)
After distillation (10 splits)
Before distillation (5 splits)
After distillation (5 splits)
20 40 60 80 100
Capacity usage (%)60657075808590
CIFAR100 (task-aware)
Before distillation (10 splits)
After distillation (10 splits)
Before distillation (5 splits)
After distillation (5 splits)
20 40 60 80 100
Capacity usage (%)010203040
TinyImageNet (task-agnostic)
Before distillation (10 splits)
After distillation (10 splits)
Before distillation (5 splits)
After distillation (5 splits)
20 40 60 80 100
Capacity usage (%)40455055606570
TinyImageNet (task-aware)
Before distillation (10 splits)
After distillation (10 splits)
Before distillation (5 splits)
After distillation (5 splits)
Fig. 8: Average accuracy after the last task with and without GCAB distillation.
0 20 40 60 80 100
Completed T asks (%)020406080100Capacity (%)5 splits
10 splits
Fig. 9: Gated Class-Attention Block capacity on
CIFAR-100 in the 5- and 10-split scenarios.
narios. We observe that most of the available capacity
is used during the firsts tasks. For subsequent ones the
percentage of occupied capacity is significantly lower
compared to the first ones. From the 5-task scenario
curve we that there is almost 10% of the capacity avail-
able for possible new incoming tasks. For the 10-task
scenario, however, after completing 60% of the tasks
the capacity is almost full and the last tasks are using
less than 2% of capacity each.
4.7 GCAB Distillation
We conduct the experiments with GCAB distillation in
four scenarios: CIFAR-100 with 5 and 10 tasks, and
Tiny-ImageNet with 5 and 10 tasks. We freeze the
transformer encoder, projection network, GCAB, and
classifiers after the last task and train the student CAB
(as shown in Figure 6) with an Adam optimizer and a
learning rate 5 e−3for 200 epochs with only data from
the last task. When performing GCAB distillation, we
use static binary masks at the same position as theMethod Params Time (ms) GFLOPs
Dytox 10.7 ∼16 0.775
GCAB 12.4 ∼22 0.946
GCAB-Fast 9.2 ∼7 0.510
Table 5: Comparison of runtime and computational cost
for a single forward pass (on a Quadro RTX 6000 GPU
with batch size 1) for DyTox and our method with and
without GCAB distillation after the last task on the
CIFAR-100 10 task scenario.
masks in GCAB (see Figure 4 (a)) to control the capac-
ity usage in the student CAB (this potentially allows
us to continue training on further tasks).
From Figure 8 we see that GCAB distillation ob-
tains almost the same performance as the model before
distillation when the capacity usage is higher than 80%,
and only a small performance drop at 60% capacity us-
age. As shown in Table 5, GCAB distillation (indicated
byGCAB-Fast ) can overcome the increased computa-
tional cost from multiple forward passes for each tasks
during inference.
5 Discussion and Conclusions
Within the context of the broader continual learning
literature, we believe that our paper advances theory
along two main axes. First of all, it makes a contribution
to parameter-isolation methods (Delange et al., 2021).
These methods are very popular for task-incremental
learning (Mallya and Lazebnik, 2018; Mallya et al.,
2018; Serra et al., 2018), but to the best of our
knowledge they have not yet been applied to class-
incremental learning scenarios. The main bottleneck in
applying these methods to class-incremental problems
is the fact that, without a task-ID available only in
task-incremental scenarios, these methods require mul-
tiple forward passes through the network, which makes
them computationally less attractive. In this paper, we
have addressed this problem by only applying param-
eter isolation to the last transformer block, reducing

--- PAGE 12 ---
12 Cotogni et al.
the computational overhead to multiple forward passes
only through this single transformer block. As a result,
parameter isolation can be applied to class-incremental
learning and only incur minor computational overhead.
In addition, we show that knowledge distillation can be
applied (GCAB-Fast) to replace the final block with a
single block that requires only a single forward pass.
The second contribution of this paper is the cas-
caded feature drift compensation, which is related to
semantic drift compensation (Yu et al., 2020). Since the
proposed gated class-attention only operates on the last
block of the transformer, the backbone, which is shared
among all tasks, still suffers from representation drift.
To address this, rather than preventing the drift from
happening, which is the prevailing approach in litera-
ture (Kirkpatrick et al., 2017; Li and Hoiem, 2017), we
accept that drift will occur when learning the new task
and compensate for it. We show that learned projection
networks (which are used to ensure that the new net-
work consolidates the knowledge of the previous one)
allow for a straightforward compensation of representa-
tion drift. Differently than the method proposed in (Yu
et al., 2020), our approach is not limited to prototype-
based (nearest class mean) classification and can also be
applied to multi-layer classifiers, like the applied class-
attention block used in our paper.
In conclusion, we presented an exemplar-free ap-
proach to class-incremental visual transformer training.
Our method, through the gated class-attention mech-
anism, achieves low forgetting by learning and mask-
ing important neurons for each task. High plasticity is
ensured via backbone regularization and feature drift
compensation using a cascade of feature projection
networks. Experiments on several benchmark datasets
show that our method obtains competitive results when
compared to rehearsal based ViT methods. Ours is one
of the first effective approaches to exemplar-free, class-
incremental training of ViTs.
Acknowledgments
We acknowledge the project, TED2021-132513B-I00,
PID2022-143257NB-I00 (MICINN, Spain), the CERCA
Programme of Generalitat de Catalunya and the Euro-
pean Commission funded Horizon 2020 project, grant
number 951911 (AI4Media).
Data Availability
In the interest of reproducibility, we have made
our code available at https://github.com/OcraM17/GCAB-CFDC . All experiments are conducted on publicly
available datasets; see the references cited.
References
Abati D, Tomczak J, Blankevoort T, Calderara S, Cuc-
chiara R, Bejnordi BE (2020) Conditional channel
gated networks for task-aware continual learning. In:
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp 3931–3940
Aljundi R, Babiloni F, Elhoseiny M, Rohrbach M,
Tuytelaars T (2018) Memory aware synapses: Learn-
ing what (not) to forget. In: Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pp
139–154
Aljundi R, Lin M, Goujaud B, Bengio Y (2019) Gradi-
ent based sample selection for online continual learn-
ing. Advances in neural information processing sys-
tems 32
Bang J, Kim H, Yoo Y, Ha JW, Choi J (2021) Rain-
bow memory: Continual learning with a memory of
diverse samples. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, pp 8218–8227
Benjamin AS, Rolnick D, Kording K (2018) Measuring
and regularizing networks in function space. arXiv
preprint arXiv:180508289
Buzzega P, Boschini M, Porrello A, Abati D, Calder-
ara S (2020) Dark experience for general continual
learning: a strong, simple baseline. Advances in neu-
ral information processing systems 33:15920–15930
Buzzega P, Boschini M, Porrello A, Calderara S (2021)
Rethinking experience replay: a bag of tricks for con-
tinual learning. In: 2020 25th International Confer-
ence on Pattern Recognition (ICPR), IEEE, pp 2180–
2187
Caron M, Touvron H, Misra I, J´ egou H, Mairal J, Bo-
janowski P, Joulin A (2021) Emerging properties in
self-supervised vision transformers. In: Proceedings
of the IEEE/CVF International Conference on Com-
puter Vision, pp 9650–9660
Castro FM, Mar´ ın-Jim´ enez MJ, Guil N, Schmid C, Ala-
hari K (2018) End-to-end incremental learning. In:
Proceedings of the European conference on computer
vision (ECCV), pp 233–248
Chaudhry A, Dokania PK, Ajanthan T, Torr PH (2018)
Riemannian walk for incremental learning: Under-
standing forgetting and intransigence. In: Proceed-
ings of the European Conference on Computer Vision
(ECCV), pp 532–547
Chaudhry A, Ranzato M, Rohrbach M, Elhoseiny M
(2019) Efficient lifelong learning with a-gem. In: In-
ternational Conference on Learning Representations

--- PAGE 13 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 13
Chaudhry A, Gordo A, Dokania P, Torr P, Lopez-Paz
D (2021) Using hindsight to anchor past knowledge
in continual learning. In: Proceedings of the AAAI
Conference on Artificial Intelligence, vol 35, pp 6993–
7001
Del Chiaro R, Twardowski B, Bagdanov A, Van
De Weijer J (2020) Ratt: Recurrent attention to tran-
sient tasks for continual image captioning. Advances
in Neural Information Processing Systems 33:16736–
16748
Delange M, Aljundi R, Masana M, Parisot S, Jia X,
Leonardis A, Slabaugh G, Tuytelaars T (2021) A con-
tinual learning survey: Defying forgetting in classifi-
cation tasks. IEEE Transactions on Pattern Analysis
and Machine Intelligence
Dhar P, Singh RV, Peng KC, Wu Z, Chellappa R (2019)
Learning without memorizing. In: Proceedings of the
IEEE/CVF conference on computer vision and pat-
tern recognition, pp 5138–5146
Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D,
Zhai X, Unterthiner T, Dehghani M, Minderer M,
Heigold G, Gelly S, et al. (2020) An image is worth
16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:201011929
Douillard A, Cord M, Ollion C, Robert T, Valle E
(2020) Podnet: Pooled outputs distillation for small-
tasks incremental learning. In: Computer Vision–
ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XX 16,
Springer, pp 86–102
Douillard A, Ram´ e A, Couairon G, Cord M (2022)
Dytox: Transformers for continual learning with
dynamic token expansion. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pp 9285–9295
Fini E, da Costa VGT, Alameda-Pineda X, Ricci E,
Alahari K, Mairal J (2022) Self-supervised models are
continual learners. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, pp 9621–9630
Gehring J, Auli M, Grangier D, Yarats D, Dauphin
YN (2017) Convolutional sequence to sequence learn-
ing. In: International conference on machine learning,
PMLR, pp 1243–1252
Gomez-Villa A, Twardowski B, Yu L, Bagdanov AD,
van de Weijer J (2022) Continually learning self-
supervised representations with projected functional
regularization. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, pp 3867–3877
Goodfellow IJ, Mirza M, Da X, Courville AC, Bengio
Y (2014) An empirical investigation of catastrophic
forgeting in gradient-based neural networks. In: Ben-gio Y, LeCun Y (eds) 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Pro-
ceedings, URL http://arxiv.org/abs/1312.6211
He K, Zhang X, Ren S, Sun J (2016) Deep residual
learning for image recognition. In: Proceedings of
the IEEE conference on computer vision and pattern
recognition, pp 770–778
Hinton G, Vinyals O, Dean J, et al. (2015) Distilling
the knowledge in a neural network. arXiv preprint
arXiv:150302531 2(7)
Hou S, Pan X, Loy CC, Wang Z, Lin D (2019) Learning
a unified classifier incrementally via rebalancing. In:
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp 831–839
Jung H, Ju J, Jung M, Kim J (2016) Less-forgetting
learning in deep neural networks. arXiv preprint
arXiv:160700122
Kirkpatrick J, Pascanu R, Rabinowitz N, Veness J,
Desjardins G, Rusu AA, Milan K, Quan J, Ra-
malho T, Grabska-Barwinska A, et al. (2017) Over-
coming catastrophic forgetting in neural networks.
Proceedings of the national academy of sciences
114(13):3521–3526
Krizhevsky A, Hinton G, et al. (2009) Learning multiple
layers of features from tiny images. Tech Report
Le Y, Yang X (2015) Tiny imagenet visual recognition
challenge. CS 231N 7(7):3
Lee J, Hong HG, Joo D, Kim J (2020) Continual learn-
ing with extended kronecker-factored approximate
curvature. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition,
pp 9001–9010
Li Z, Hoiem D (2017) Learning without forgetting.
IEEE transactions on pattern analysis and machine
intelligence 40(12):2935–2947
Liu X, Masana M, Herranz L, Van de Weijer J, Lopez
AM, Bagdanov AD (2018) Rotate your networks:
Better weight consolidation and less catastrophic for-
getting. In: 2018 24th International Conference on
Pattern Recognition (ICPR), IEEE, pp 2262–2268
Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S,
Guo B (2021) Swin transformer: Hierarchical vision
transformer using shifted windows. In: Proceedings
of the IEEE/CVF International Conference on Com-
puter Vision, pp 10012–10022
Lopez-Paz D, Ranzato M (2017) Gradient episodic
memory for continual learning. Advances in neural
information processing systems 30
Van der Maaten L, Hinton G (2008) Visualizing data
using t-sne. Journal of machine learning research
9(11)

--- PAGE 14 ---
14 Cotogni et al.
Mallya A, Lazebnik S (2018) Packnet: Adding multi-
ple tasks to a single network by iterative pruning. In:
Proceedings of the IEEE conference on Computer Vi-
sion and Pattern Recognition, pp 7765–7773
Mallya A, Davis D, Lazebnik S (2018) Piggyback:
Adapting a single network to multiple tasks by learn-
ing to mask weights. In: Proceedings of the European
Conference on Computer Vision (ECCV), pp 67–82
Masana M, Liu X, Twardowski B, Menta M, Bagdanov
AD, van de Weijer J (2020) Class-incremental learn-
ing: survey and performance evaluation on image
classification. arXiv preprint arXiv:201015277
Masana M, Tuytelaars T, Van de Weijer J (2021)
Ternary feature masks: zero-forgetting for task-
incremental learning. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp 3570–3579
Mermillod M, Bugaiska A, Bonin P (2013) The
stability-plasticity dilemma: Investigating the con-
tinuum from catastrophic forgetting to age-limited
learning effects
Mundt M, Lang S, Delfosse Q, Kersting K (2021) Cleva-
compass: A continual learning evaluation assessment
compass to promote research transparency and com-
parability. arXiv preprint arXiv:211003331
Pelosin F, Jha S, Torsello A, Raducanu B, van de Wei-
jer J (2022) Towards exemplar-free continual learning
in vision transformers: an account of attention, func-
tional and weight regularization. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp 3820–3829
Rajasegaran J, Hayat M, Khan SH, Khan FS, Shao L
(2019) Random path selection for continual learning.
Advances in Neural Information Processing Systems
32
Rebuffi SA, Kolesnikov A, Sperl G, Lampert CH (2017)
icarl: Incremental classifier and representation learn-
ing. In: Proceedings of the IEEE conference on Com-
puter Vision and Pattern Recognition, pp 2001–2010
Riemer M, Cases I, Ajemian R, Liu M, Rish I, Tu Y,
Tesauro G (2018) Learning to learn without forget-
ting by maximizing transfer and minimizing interfer-
ence. arXiv preprint arXiv:181011910
Russakovsky O, Deng J, Su H, Krause J, Satheesh S,
Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M,
et al. (2015) Imagenet large scale visual recognition
challenge. International journal of computer vision
115(3):211–252
Rusu AA, Rabinowitz NC, Desjardins G, Soyer H, Kirk-
patrick J, Kavukcuoglu K, Pascanu R, Hadsell R
(2016) Progressive neural networks. arXiv
Serra J, Suris D, Miron M, Karatzoglou A (2018) Over-
coming catastrophic forgetting with hard attentionto the task. In: International Conference on Machine
Learning, PMLR, pp 4548–4557
Strudel R, Garcia R, Laptev I, Schmid C (2021) Seg-
menter: Transformer for semantic segmentation. In:
Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp 7262–7272
Toldo M, Ozay M (2022) Bring evanescent representa-
tions to life in lifelong class incremental learning. In:
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp 16732–
16741
Touvron H, Cord M, Sablayrolles A, Synnaeve G, J´ egou
H (2021) Going deeper with image transformers. In:
Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp 32–42
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones
L, Gomez AN, Kaiser  L, Polosukhin I (2017) Atten-
tion is all you need. Advances in neural information
processing systems 30
Wang L, Yang K, Li C, Hong L, Li Z, Zhu J (2021)
Ordisco: Effective and efficient usage of incremental
unlabeled data for semi-supervised continual learn-
ing. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp 5383–
5392
Wang Z, Liu L, Duan Y, Kong Y, Tao D (2022a) Con-
tinual learning with lifelong vision transformer. In:
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp 171–181
Wang Z, Zhang Z, Lee CY, Zhang H, Sun R, Ren X,
Su G, Perot V, Dy J, Pfister T (2022b) Learning
to prompt for continual learning. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp 139–149
Wu Y, Chen Y, Wang L, Ye Y, Liu Z, Guo Y, Fu Y
(2019) Large scale incremental learning. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pp 374–382
Yan S, Xie J, He X (2021) Der: Dynamically expand-
able representation for class incremental learning. In:
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp 3014–3023
Yu L, Twardowski B, Liu X, Herranz L, Wang K, Cheng
Y, Jui S, Weijer Jvd (2020) Semantic drift compen-
sation for class-incremental learning. In: Proceedings
of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp 6982–6991
Zenke F, Poole B, Ganguli S (2017) Continual learning
through synaptic intelligence. In: International Con-
ference on Machine Learning, PMLR, pp 3987–3995
Zhai M, Chen L, Mori G (2021) Hyper-lifelonggan: scal-
able lifelong learning for image conditioned genera-
tion. In: Proceedings of the IEEE/CVF Conference

--- PAGE 15 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 15
on Computer Vision and Pattern Recognition, pp
2246–2255
Zhang J, Zhang J, Ghosh S, Li D, Tasci S, Heck L,
Zhang H, Kuo CCJ (2020) Class-incremental learn-
ing via deep model consolidation. In: Proceedings of
the IEEE/CVF Winter Conference on Applications
of Computer Vision, pp 1131–1140
Zheng S, Lu J, Zhao H, Zhu X, Luo Z, Wang Y, Fu Y,
Feng J, Xiang T, Torr PH, et al. (2021) Rethinking
semantic segmentation from a sequence-to-sequence
perspective with transformers. In: Proceedings of the
IEEE/CVF conference on computer vision and pat-
tern recognition, pp 6881–6890
Zhou DW, Wang QW, Ye HJ, Zhan DC (2022)
A model or 603 exemplars: Towards memory-
efficient class-incremental learning. arXiv preprint
arXiv:220513218
Zhu F, Zhang XY, Wang C, Yin F, Liu CL (2021) Pro-
totype augmentation and self-supervision for incre-
mental learning. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, pp 5871–5880
A Comparison with exemplar-based methods
using 500 exemplars
As a further comparison we analyzed the behavior of GCAB
compared to exemplar-based methods with a replay buffer of
500 exemplars. In Table 6 we report the results of the con-
sidered methods on the CIFAR-100 dataset in 5- and 10-task
scenarios. In these settings the 100 classes have been split in
5 and 10 task, each one containing 20 and 10 classes, respec-
tively. The performance of both our exemplar-free solutions
(GCAB and GCAB-Fast) are comparable with exemplar-
based methods using 500 exemplars. In detail, on the 5-task
scenario, our method reaches the second-best result. Simi-
larly, on the 10-task scenario, GCAB and GCAB-Fast achieve
very good performance at the third and fourth positions in
the comparison.
In Table 7, we report a comparison on Tiny-Imagenet
and ImageNet100. For these two datasets, we consider the
10 and 20-task scenario, where each task contains 20 and
10 classes, respectively. Like in Table 6, the exemplar-based
methods use a buffer of 500 exemplars. The performance
of GCAB and GCAB-Fast on the Tiny-ImageNet surpass
all exemplar-based methods even with a budget of 500 ex-
emplars. In ImageNet100, our method is competitive with
exemplar-based methods, reaching the second-best accuracy
after DyToX with 500 exemplars.
B Additional Embedding Visualizations
We give here visualizations of the embeddings produced with
and without the projection network cascade. In column (a)
of Figure 10, we show the embedding obtained after training
the first task. In column (b) we report the result of the em-
bedding without PFR during training. In columns (c) and (d)we show the embeddings obtained using PFR during training.
The difference between (c) and (d) is the use of the Feature
Drift Compensation mechanism during inference. The pro-
jection networks of the various tasks are used to align the
embeddings produced by the current backbone to the ones
produced by the previous ones (in column d). The different
rows of the figure show the embeddings of the task 1 data
produced by the network after training a variable number of
tasks. We see that the embeddings of Task 1 without PFR
during training (b) and only with PFR during training but
not during inference (c) are less clearly clustered, especially
after Task 10 where samples of all classes significantly overlap.
However, when using PFR during both training and inference
the data remains clearly clustered, as seen in column (d).

--- PAGE 16 ---
16 Cotogni et al.
Method Exemplar-FreeCIFAR-100 5 Tasks CIFAR-100 10 Tasks
Class-IL Task-IL Class-IL Task-IL
Joint ✓ 75.39 - 75 .39 -
ER (Riemer et al., 2018) ✗ 27.97 68 .21 21 .54 74 .97
GEM (Lopez-Paz and Ranzato, 2017) ✗ 25.44 67 .49 18 .48 72 .68
AGEM (Chaudhry et al., 2019) ✗ 18.75 58 .70 9 .72 58 .23
iCaRL (Rebuffi et al., 2017) ✗ 35.95 64 .40 30 .25 71 .02
FDR (Benjamin et al., 2018) ✗ 29.99 69 .11 22 .81 74 .22
GSS (Aljundi et al., 2019) ✗ 22.08 61 .77 13 .72 56 .32
DER++ (Buzzega et al., 2020) ✗ 38.39 70 .74 36 .15 73 .31
HAL (Chaudhry et al., 2021) ✗ 16.74 39 .70 11 .12 41 .75
ERT (Buzzega et al., 2021) ✗ 28.82 62 .85 23 .00 68 .26
RM (Bang et al., 2021) ✗ 39.47 69 .27 32 .52 73 .51
PASS (Zhu et al., 2021) ✓ 48.28 - 33 .76 -
SDC (Yu et al., 2020) ✓ 6.65 - 7.41 -
LVT†(Wang et al., 2022a) ✗ 44.73 71 .54 43 .51 76.78
DyToX†(Douillard et al., 2022) ✗ 51 .28 - 36.24 -
A-D†(Pelosin et al., 2022) ✓ 15.61 42.82 16 .77 55 .53
GCAB†✓ 49.86 81 .01 35.90 82 .08
GCAB-Fast†✓ 48.85 79 .75 35 .42 81 .97
Table 6: Comparison on CIFAR-100. All exemplar-based methods use a memory buffer of 500 exemplars. The
accuracies reported here are the ACC TAG andACC TAW computed after training the last task. Methods marked
with†are based on ViT.
Method Exemplar-FreeTiny-ImageNet ImageNet100
Class-IL Task-IL Class-IL Task-IL
Joint ✓ 59.38 − 79.18 −
ER (Riemer et al., 2018) ✗ 10.15 50 .11 11 .68 42 .04
AGEM (Chaudhry et al., 2019) ✗ 9.67 26 .79 10 .92 34 .22
iCaRL (Rebuffi et al., 2017) ✗ 10.69 35 .89 16 .44 36 .89
FDR (Benjamin et al., 2018) ✗ 10.58 49 .91 11 .78 42 .60
DER++ (Buzzega et al., 2020) ✗ 19.33 51 .90 14 .52 35 .46
ERT (Buzzega et al., 2021) ✗ 12.13 50 .87 20 .42 41 .56
RM (Bang et al., 2021) ✗ 18.96 52 .08 14 .56 38 .66
PASS (Zhu et al., 2021) ✓ 24.23 - 25.22 -
SDC (Yu et al., 2020) ✓ 3.94 - 11.52 -
LVT†(Wang et al., 2022a) ✗ 23.97 57 .39 26 .32 47 .84
DyToX†(Douillard et al., 2022) ✗ 24.64 - 42 .40 -
A-D†(Pelosin et al., 2022) ✓ 6.10 18.02 10.92 39.13
GCAB†✓ 26 .82 65 .92 40.10 81 .82
GCAB-Fast†✓ 26.44 65 .02 36 .22 80 .28
Table 7: Comparison on Tiny-ImageNet and ImageNet100. All exemplar-based methods use a memory buffer of
500 exemplars. The accuracies reported here are the ACC TAG andACC TAW computed after the training the last
task. Methods marked with†are based on ViT.

--- PAGE 17 ---
Exemplar-free Continual Learning of ViTs via Gated Class-Attention and Cascaded Feature Drift Compensation 17
Task 1Task 2
(a) (b) (c) (d)
Task 3
Task 4
Task 5
Task 10
Fig. 10: t-SNE visualization of embedding space (output of f(·)) at Tasks 1, 2, 3, 4, 5, and 10. (a) Results after
Task 1; (b) Results without PFR during training; (c) Results with PFR during training but without feature drift
compensation; (d) Results with PFR during training and with feature drift compensation.
