# 2304.02147.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2304.02147.pdf
# File size: 5717616 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ConvFormer: Parameter Reduction in Transformer Models for 3D Human
Pose Estimation by Leveraging Dynamic Multi-Headed Convolutional
Attention
Alec Diaz-Arias and Dmitriy Shin
Inseer, Inc.
Iowa City, IA 52241
alec.diaz-arias@inseer.com
April 6, 2023
Abstract
Recently, fully-transformer architectures have replaced the defacto convolutional architecture for the 3D human pose
estimation task. In this paper we propose ConvFormer , a novel convolutional transformer that leverages a new dynamic
multi-headed convolutional self-attention mechanism for monocular 3D human pose estimation. We designed a spatial
and temporal convolutional transformer to comprehensively model human joint relations within individual frames and
globally across the motion sequence. Moreover, we introduce a novel notion of temporal joints proﬁle for our temporal
ConvFormer that fuses complete temporal information immediately for a local neighborhood of joint features. We have
quantitatively and qualitatively validated our method on three common benchmark datasets: Human3.6M, MPI-INF-3DHP,
and HumanEva. Extensive experiments have been conducted to identify the optimal hyper-parameter set. These experiments
demonstrated that we achieved a signiﬁcant parameter reduction relative to prior transformer models while
attaining State-of-the-Art (SOTA) or near SOTA on all three datasets. Additionally, we achieved SOTA for Protocol III on
H36M for both GT and CPN detection inputs. Finally, we obtained SOTA on all three metrics for the MPI-INF-3DHP
dataset and for all three subjects on HumanEva under Protocol II.
1 Introduction
Monocular3D Human Pose Estimation (HPE)isaprocessoflocalizingjointlocationsandsubsequentlyabodyrepresentation
(skeletal representation) from varying input streams such as a static image or a video stream. 3D HPE receives a lot of
attention in the computer vision community and plays an essential role in many applications including motion analysis,
computer animation, action recognition, and ergonomic risk-safety assessment. Many approaches had been proposed to
solve this problem (for an extended treatment see Prior Works section).
More recently, and motivated by the ground-breaking work in [ 21] (ViT) a fully transformer architecture was introduced
for the 3D HPE task. Transformers were developed to exploit long-range dependencies and have achieved tremendous
success in NLP since their invention [12] and more recently in various CV tasks.
All of these methods have continued to push the boundaries for accuracy, however, they have continually done so
by increasing the network capacity [ 60,59,55,57] and potentially leading to over-parametrization. For instance, classic
transformers suﬀer from a known problem of redundancy which occurs due to complete connectivity. There have been
works in NLP that have sought to introduce sparsity and have seen substantial accuracy increases while reducing the
computational complexity [ 27,61,62]. Given that classic transformers are still in their infancy for CV tasks, sparsity
mechanisms have not been fully applied yet. Solving the redundancy problem of classic transformers was one of the main
motivations to introduce ConvFormer. For the 3D HPE problem and speciﬁcally for human motion, individual joints may
exhibit a high degree of inter-correlation. Thus, by generating queries, keys, and values via fully connected layers in vanilla
transformers, the networks learn redundancies leading to noisier inference. ConvFormer leverages convolutions to extract
combinations of joints via their local receptive ﬁeld that together provide a stronger signal that is less susceptible to noise,
results in fewer features for the attention computations, and extensively reduces parameter counts. Furthermore, a single
ﬁlter may be incapable of fully capturing dependencies. For this reason we introduced a dynamic aggregation mechanism
which weights the contribution of diﬀerent joint neighborhoods. We call this novel mechanism dynamic multi-headed
convolutional self-attention (DMHCSA).
Following [ 5,68] we leverage a spatial-temporal framework. However, a critical distinguishing factor and substantial
motivation for ConvFormer was how to extract temporal dependencies at the query, key, value level prior to computing
temporal attention maps. To achieve this we introduce temporal joints proﬁle . To compute them, ConvFormer extracts
correlations between joints for individual frames in the spatial ConvFormer and subsequently generates high-order temporal
proﬁles of joints present in the motion sequences with the temporal ConvFormer. More speciﬁcally for the temporal blocks,
1arXiv:2304.02147v1  [cs.CV]  4 Apr 2023

--- PAGE 2 ---
the DMHCSA mechanism extracts queries, keys, and values that have visibility across the motion sequence, which has been
coinedearly temporal fusion , leading to more complex self-attention maps that capture more intricate correlations. We
conducted extensive experiments on three standard 3D HPE datasets, i.e., Human3.6M, MPI-INF-3DHP, and HumanEva-I
[7,10,8] and compare ConvFormer against several competitive 3D HPE solution methodologies. ConvFormer achieves
state-of-the-art results by the majority of the metrics and comparable on CPN detections under Protocol I while reducing
the parameter count by more than half of models that take input sequences of equivalent length. Our contributions in this
paper are summarized as follows:
1.A signiﬁcant parameter reduction relative to other transformer models using a new architecture called ConvFormer.
ConvFormer leverages a novel multi-headed convolutional self-attention mechanism that dynamically aggregates
sub-queries, keys, and values into a richer set of cues for 3D HPE.
2.A novel notion of temporal joints proﬁle is introduced that relies on immediate fusion of complete temporal information
of the motion sequence.
3. An extensive study on factors aﬀecting the performance of ConvFormer.
2 Prior Works
At the outset of leveraging deep neural networks for 3D HPE, many methods attempted to learn mappings from monocular
RGB images to 3D skeletal representations [ 9,44]. While one-shot 3D HPE saw some success, it suﬀered from substantial
computational overhead and simultaneously poor generalizability due to Motion Capture data being acquired in staged
environments. In part due to Martinez et al. [ 1], the 3D HPE landscape shifted its focus predominantly towards the
two-stage approach by leveraging accurate performance of oﬀ-the-shelf 2D pose detectors and then building networks that
perform the 2D-to-3D lifting. A number of other works improved the performance of 3D HPE from a single monocular
image utilizing various deep learning techniques and analytical approaches (e.g., [44, 46, 78, 64, 48]).
To reduce errors, improve handling of self-occlusions, and increase generalizability of 3D HPE models, several works
exploited spatial relationships among joints. To account for these relationships, some methods incorporate “static”
anthropometric constraints and regularization procedures, while others are based on temporal architectures that infer these
dependencies across video frames [ 31,2,16,11]. For instance, people have used graph convolutional and graph attention
networks to naturally model spatial relationships between joints while building lightweight networks [75, 76, 78, 79].
Recently, Kolesnikov et al. introduced Vision Transformer (ViT) that applies a global self-attention mechanism to
eﬃciently exploit salient information from video frames [ 21]. Since ViT, transformers have seen successes in many CV tasks,
including, image recognition [21, 55], and object detection [41].
Even more recently, researchers have begun leveraging convolutions in transformers, to learn positional embeddings in
place of dense layers, replacing the dense feed forward component with a convolutional feed-forward block for sparsity, or
leveraging convolutional projections for speciﬁc tasks like video synthesis and working with unstructured point cloud data
[56,59]. Finally, with the explosion of transformer-based models and their capability to eﬀectively capture local and global
relationships, they began to be applied to the 3D HPE problem as well [54, 50, 35, 5].
In a typical setup, a model processes adjacent video frames to learn temporal representations of human body parts
during motion, and then reconstructs a human pose for an interior frame at the inference step. Zheng et al. developed a 3D
HPE method called PoseFormer that is based purely on the transformer architecture that encodes and learns both spatial
and temporal information [ 5]. Li et al. leverages a spatial-temporal framework while ﬁnding multiple feasible solutions
(given that 3D HPE is an inverse problem) and then leverages a transformer head that aggregates feasible solutions into
an optimal solution [ 68]. The framework operates by generating multiple hypothesis for pose prediction with consequent
self-hypothesis reﬁnement and computation of cross-hypothesis interactions. MHFormer achieves superior to previous
methods accuracy on MPI-INF-3DHP and Human 3.6M datasets. He et al. developed an Epipolar Transformer to take
advantage of 3D data to improve 2D pose estimation, which is challenged in the presence of occlusions and oblique viewing
angles [54]. Shuai introduced a Multi-view and Temporal Fusion transformer to adaptively process varying view numbers
and video lengths without calibration [50].
Even though transformers demonstrate strong capabilities to model complex relationships, they suﬀer from redundancy
and over-connectivity . To address this issue, researchers in NLP have begun developing diﬀerent sparsity mechanisms in
an attempt to reduce connectivity. For example, Jaszczur et al. proposed a sparse transformer model to scale learning
and inference processes [ 62]. In 3D HPE, Li et al. proposed Strided Transformer [ 71] to reduce dimensionality of the last
linear layers. However, to the best of our knowledge, no work has been done to reduce connectivity in the most "parameter
heavy" self-attention mechanism of transformers for the 3D HPE task.
For these reasons, we propose ConvFormer model, which reduces the parameter count extensively relative to [ 5], by
an approximate 60 percent. At the spatial level, our multi-scale feature aggregation mechanism is able to capture critical
correlations leading to a stronger signal that is more robust relative to [ 5]. Moreover, our convolutional self-attention
mechanism in the temporal transformer produces queries, keys, and values that extract inter-frame information leading to
more diverse attention maps. This enables us to achieve SOTA at a relatively low cost. In the next section, we provide a
comprehensive exposition of our methodology.
2

--- PAGE 3 ---
3 Method
In the following subsections, we present an overview of our solution methodology for estimating 3D poses from a sequence of
2D poses, then we describe in our global network architecture, and lastly we present our dynamic multi-headed convolutional
self-attention mechanism.
3.1 Overview
Figure 1: Panel A depicts architecture of a ConvFormer Block. Panel B presents the overall pipeline for 3D HPE from a sequence of 2D
poses. The central component of a ConvFormer Block is DMHCSA which is depicted in the panel C. A curvy blue line at the bottom of
Panel C corresponds to a part of an extracted temporal joints proﬁle of the right elbow joint (for the temporal ConvFormer block).
Panel D presents an example of convolutions during generation of Queries, Keys, and Values in a Temporal ConvFormer Block. A ﬁlter
slides across the feature dimension eﬀectively convolving full temporal proﬁles of local joint neighborhoods.
The overall architecture of our methodology is described in Figure 1. Given a sequence of 2D poses P=fPigT
i=1RJ2
whereTrepresents the number of frames in the sequence and J is the number of joints in the skeleton. We seek to reconstruct
the 3D poses in the root relative camera reference frame (i.e. the camera reference frame where the root joint sits at
the origin). Following [ 2], we predict the 3D pose for the central frame from any such sequence, i.e. ^pdi=2e2RJ3. Our
network contains two Dynamic ConvFormer blocks, one with spatial attention and the other with temporal attention. More
speciﬁcally, we leverage a spatial attention mechanism to extract frame-wise inter-joint dependencies by analyzing sections of
joints that are related. The temporal attention mechanism extracts global inter-frame relationships by analyzing correlations
between the temporal proﬁles of joints. In contrast to [ 5], which queries latent pose representations for individual frames
and then computes attention with respect to the temporal axis, our temporal joints proﬁle mechanism fuses temporal
information at the querying level prior to computing self-attention with respect to the temporal axis.
3

--- PAGE 4 ---
3.2 Network Architecture
We employ two main components in our network architecture: a spatialand a temporal ConvFormer. The spatial ConvFormer
block extracts a high dimensional feature vector for a single-frames’ encoded joint correlations. We assume our input is a
2D pose with Jjoints that is represented by two coordinates (u;v). Following [ 5] we ﬁrst map the coordinate of each joint
into a higher-dimensional feature vector with a trainable linear layer. We then apply a learned positional encoding via
summation to retain joint position information. That is, given a sequence of poses fPigT
i=1RJ2andW2R2dand
Epos2RJdwe encodePias follows:
xi=PiW+Epos; i2f1;:::;Tg. (1)
anddrepresents the dimension of the embedding, Wis the trainable linear layer, and Eposis the learned positional encoding.
Subsequently, the spatial feature sequence fxigT
i=1RJdare fed into the spatial ConvFormer which applies the attention
mechanism to the joint dimension to integrate information across the complete pose on a per frame basis. Q;K;Vare
generated via convolutions with weights of the following dimension (d;d;k )wheredis the encoded dimension and kis the
kernel size and the ﬁlter is slid over the joints dimension. The output for the i-th frame of the b-th spatial ConvFormer
block is denoted by zb
i2RJdfori= 1;:::;T.
While the spatial ConvFormer seeks to encode correlations between joints in a single frame we leveraged the temporal
model to localize sequence wise correlations between the encoded spatial features. This mechanism should be viewed
as extracting the temporal proﬁle of a neighborhood of joints, which we call temporal joints proﬁle (see Panel D in
Figure 1). An early work that leveraged this temporal fusion mechanism was [ 4] where Karpathy et al. studied diﬀerent
mechanisms for incorporating temporal information without convolving over the temporal dimension. To further clarify
the point,Q;K;Vare generated via convolutions with weights of the following dimension (T;T;k )wherekis the kernel
size and the 1D convolutions have depth the size of input sequence. Thus, one can view our network as fusing into the
queries the temporal evolution of a patch of deep joint features immediately. This is very distinct from the temporal
attention seen in [ 5] which attends complete pose encoding throughout the motion sequence. We note that the output
from the spatial ConvFormer block is a sequence fzB
igi=1;:::;TRJdwhereBis the number of spatial blocks and Tis
the number of frames in the sequence. We note that zb
ican be represented in R1Jdand thus concatenate these features
along the ﬁrst axis giving us X0=Concatenate (zB
1;:::;zB
T)2RTJd. Following this procedure we incorporate a learned
temporal embedding to retain information about the deep joint features evolution throughout time, i.e. Etemp2RTJd
andX=X0+Etempis the input into our temporal transformer. We note that the output of the b-th ConvFormer block
with temporal attention is Zb2RTJdwhere there are Bsuch layers.
Since we follow many-to-one prediction scheme ﬁrst introduced in [ 2] we ﬁrst down sample the spatial axis with a linear
projection and then perform a temporal convolution with one output channel i.e. ^p=ConvT;1(ZBW)whereW2RJd3J
andConvT;1denotes a temporal convolution with one output channel and Tinput channels.
We trained our network by minimizing the MPJPE (Mean Per Joint Position Error) during optimization. The loss
function is deﬁned as
L(p;^p) =1
JJX
i=1kpi ^pik2 (2)
wherepis the ground truth 3D pose and ^pis the predicted pose and iis indexing speciﬁc joints in the skeleton.
3.3 Dynamic Multi-Headed Convolutional Self-Attention
A core novelty of this paper is the dynamic multi-headed convolutional self-attention mechanism. This is introduced to
reduce the over connectedness witnessed in classic transformer architectures while simultaneously extracting contexts at
diﬀerent scales. An additional novelty is the type of representations being queried in our temporal ConvFormer block.
Instead of generating queries, keys, and values, that are latent pose representations for individual frames and attending the
temporal axis; we query temporal joints proﬁles eﬀectively fusing temporal information prior to the attention mechanism.
Convolutional Scaled Dot Product Attention can be described as a mapping function that maps a query matrix Q, a
key matrix K, and a value matrix V to an output attention matrix – where the matrix entries are scores representing the
strength of correlation between any two elements in the dimension being attended. We note that Q;K;V2RNdwhere N
is the length of the sequence and d is the dimension. In our Spatial ConvFormer N=Jand in the Temporal ConvFormer
N=T. The output of the scaled dot product attention can be expressed as
Attention (Q;K;V ) =Softmax (QKT=p
d)V. (3)
The query, keys, and values, are computed in the same manner for a ﬁxed ﬁlter length. We demonstrate how Qcan be
generated, and note that KandVare computed in an identical manner.
Q=Convn;dout(z) =dinX
i=1X
k=1wdout;i;kzi;n  1
2+k(4)
Here,denotes the kernel size and doutdenotes output dimension. This is juxtaposed against the classic scaled dot product
attention introduced in [12] where queries, keys, and values are generated via a linear projection
Q=WQz K =WKz V =WVz (5)
which provides global scope but causes redundancy due to the complete connectivity. In our dynamic convolutional
attention mechanism we introduce sparsity via convolutions to decrease connectivity while simultaneously fusing complete
4

--- PAGE 5 ---
temporal information prior to the scaled-dot-product-attention. ConvFormers’ ability to provide context at diﬀerent scales
is attributable to the dynamic feature aggregation method. Moreover, due to our convolution mechanism we query on
inter-frame level where we learn the temporal joints proﬁle. To this end, we use nconvolutional ﬁlter sizes to extract
diﬀerent local contexts at scales fign
i=1and then perform an averaging operation to generate the ﬁnal query, keys, and
values that we apply attention to, following ideas presented in [36]:
Q=Concat (Q1;:::;Qn)Q=nX
i=1Q(i)Qi
wherenX
i=1Q(i) = 1(6)
wherenis the number of convolution ﬁlters used, Q2Rn1is a learned parameter and Qiare generated as in equation 4.
Dynamic Multi-headed Convolutional Self-Attention (DMHCSA) leverages multiple heads to jointly model information
from multiple representation spaces. As seen in Figure 1 each head applies scaled dot-product self-attention in parallel. The
output of the DMHCSA block is the concatenation of hattention head outputs fed into a feed-forward network.
DMHCSA (Q;K;V ) =Concatenate (H1;:::;Hh)
whereHi=Attention (Qi;Ki;Vi); i2f1;:::;hg(7)
whereQi,Ki, andViare computed via the procedure deﬁned above.
Then the ConvFormer block is deﬁned by the following equations:
X0
b=DMHCSA (LN(Xb 1)) +Xb 1; b= 1;:::;B
Xb=FFN (LN(X0
b)) +X0
b; b= 1;:::;B(8)
whereLN()denotes layer normalization same as [ 21,55]. andFFNdenotes a feed forward network. Both the spatial and
temporal ConvFormer blocks consist of BspandBtempidentical blocks. The output of the spatial ConvFormer encoder
isY2RTJdwhereTis the frame sequence length, J is the number of joints, and dis the embedding dimension. The
output of the temporal ConvFormer is Y2RTJd.
4 Experiments
4.1 Datasets and Evaluation Protocols
Our proposed method is evaluated on three common datasets: Human3.6M [ 7], HumanEva [ 8], and MPI-INF-3DHP [ 10].
Human3.6M consists of approximately 2.3 million images from 4 synchronized video cameras capturing video at 50 Hz.
There are 7 subjects performing 15 distinct actions and each action is performed twice per subject. We train on subjects
(S1, S5, S6, S7, S8) and validate on subjects (S9, S11) following previous works [ 11,33,10,5,68]. We evaluate our method
on H36M under three diﬀerent protocols. The mean per joint position error (MPJPE) which is referred to as Protocol
I in many works [ 6,14,2]. Procrustes analysis or rigid alignment denoted by P-MPJPE is calculated as the Euclidean
distance between the ground-truth and the optimal SE(3)transformation aligning the predicted pose with the ground-truth.
This is referred to as Protocol II as in [ 1,15]. Lastly, we evaluate temporal smoothness via the mean per joint velocity
error, referred to as MPJVE (the mean across joints of the ﬁnite diﬀerence velocity approximations) or Protocol III as in
[2,16]. HumanEva on the other hand is a much smaller dataset with less than 50k frames and only 3 subjects (S1, S2, S3)
performing three actions. We evaluate our method with respect to Protocol II following previous works (e.g. [ 2]. Lastly,
we evaluated on MPI-INF-3DHP to assess our model’s generalizability. MPI consists of roughly 1.3 million frames. This
dataset contains more diverse motions than the previous two datasets. Following the setting in [ 11,33,10,5,68] we report
the following metrics: MPJPE, Percentage of Correct Keypoint (PCK) with the threshold of 150mm, and Area Under
Curve (AUC) for a range of PCK thresholds.
4.2 Implementation Details
We implemented our proposed solution methodology with PyTorch [ 17] and trained using two NVIDIA RTX 3090 GPUs.
We trained on H3.6M using 5 diﬀerent frame sequence lengths when conducting our experiments, T= 9;27;81;143;243.
Following [ 2] we augment our datasets with ﬂipping poses horizontally. We train our models for 60 epochs with an initial
learning rate of 1e 3and a weight decay factor of 0:95after each epoch. We set the batch size to 1024and utilize stochastic
depth [18] of0:2. We also use a dropout [ 22] rate of 0:2on the dynamic feature aggregation inside of the convolutional
self-attention mechanism. We benchmark on H3.6M using both CPN [ 24] detections following [ 2,11,16,5] and ground-truth
2D poses. Furthermore, we benchmark on HumanEva using three diﬀerent frame sequence lengths of T= 9,T= 27, and
T= 43following [ 13]. Lastly, following [ 5,68] we further assess the generalization ability of our solution methodology
on MPI-INF-3DHP dataset. We use 2D pose sequences of length T= 9as our model input and we evaluate using three
metrics, percentage of correct keypoints (PCK), area under the curve (AUC), and MPJPE.
5

--- PAGE 6 ---
(a) a
 (b) b
(c) c
 (d) d
Figure 2: Qualitative examples of S11 from H36M displaying ConvFormer’s eﬀectiveness: (a) Sitting Down action with heavy occlusion
on lower extremities, (b) demonstrates high quality reconstruction in the presence of slight occlusions, (c) heavy occlusion from camera
and ConvFormer still captures the correct pose from previous frame information (d) slight failure case in presence of occlusion from
right arm.
Figure 3: Qualitative results for ConvFormer on challenging In-The-Wild videos.
6

--- PAGE 7 ---
5 Results and Discussion
5.1 Comparison with State-of-the-Art
Table 1: The ﬁrst block reports MPJPE for GT-inputs and the second block is MPJPE for CPN detections. The third block reports
P-MPJPE for CPN detections. The fourth block reports MPJPV for CPN detections and the ﬁfth is MPJPV for GT-inputs. Best is in
Red and second is in Blue.
GT-MPJPE (mm) Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Hossain and Little [15] 35.240.837.237.4 43.2 44.0 38.9 35.6 42.344.6 39.7 39.7 40.2 32.8 35.5 39.2
Pavllo et al. [2] –––– – – – – –– – – – – –37.8
Liu eet al. [13] 34.537.133.634.2 32.9 37.1 39.6 35.8 40.741.4 33.0 33.8 33.0 26.6 26.9 34.7
Zeng et al. [30] 34.832.128.530.7 31.4 36.9 35.6 30.5 38.940.5 32.5 31.0 29.9 22.5 24.5 32.0
Chen et al. [11] –––– – – – – –– – – – – –32.3
Zheng et al. [5] 30.033.629.931.0 30.2 33.3 34.8 31.4 37.838.6 31.7 31.5 29.0 23.3 23.1 31.3
Li et al. [68] (T=351) 27.732.129.128.9 30.0 33.9 33.0 31.2 37.039.3 30.0 31.0 29.4 22.2 23.0 30.5
ConvFormer (T=143) 29.132.428.128.5 29.3 33.3 33.3 30.5 37.037.6 29.2 29.5 28.4 21.8 21.3 29.9
ConvFormer (T=243) 28.931.828.028.2 29.5 33.0 32.9 30.1 36.837.4 29.8 29.6 28.2 21.7 21.5 29.8
CPN-MPJPE (mm) Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Dabral et al. [31] 44.850.444.749.0 52.9 61.4 43.5 45.5 63.187.3 51.7 48.5 52.2 37.6 41.9 52.1
Cai et al. [32] (T=7) 44.647.445.648.8 50.8 59.0 47.2 43.9 57.961.9 49.7 46.6 51.3 37.1 39.4 48.8
Pavllo et al. [2] (T=243) 45.246.743.345.6 48.1 55.1 44.6 44.3 57.365.8 47.1 44.0 49.0 32.8 33.9 46.8
Lin and Lee [33] (T=50) 42.544.842.644.2 48.5 57.1 52.6 41.4 56.564.5 47.4 43.0 48.1 33.0 35.1 46.6
Yeh et al. [34] 44.846.143.346.4 49.0 55.2 44.6 44.0 58.362.7 47.1 43.9 48.6 32.7 33.3 46.7
Liu et al. [13] (T=243) 41.844.841.144.9 47.4 54.1 43.4 42.2 56.263.6 45.3 43.5 45.3 31.3 32.2 45.1
Zeng et al. [30] 46.647.143.941.6 45.8 49.6 46.5 40.0 53.461.1 46.1 42.6 43.1 31.5 32.6 44.8
Wang et al. [16] (T=96) 41.343.944.042.2 48.0 57.1 42.2 43.2 57.361.3 47.0 43.5 47.0 32.6 31.8 45.6
Chen et al. [11] (T=243) 41.443.240.142.9 46.6 51.9 41.7 42.3 53.960.2 45.4 41.7 46.0 31.5 32.7 44.1
Lin et al. [35] (T=1) –––– – – – – –– – – – – –54.0
Zheng et al. [5] (T=81) 41.544.839.842.5 46.5 51.6 42.1 42.0 53.360.7 45.5 43.3 46.1 31.8 32.2 44.3
Li et al. [68] (T=351) 39.243.140.140.9 44.9 51.2 40.6 41.3 53.560.3 43.7 41.1 43.8 29.8 30.6 43.0
ConvFormer (T=143) 41.843.639.343.2 44.9 52.8 42.7 41.2 53.160.9 45.0 41.9 44.7 29.7 31.1 43.7
ConvFormer (T=243) 41.043.239.042.4 44.5 52.2 41.7 40.8 53.060.6 44.8 41.3 43.7 29.6 30.9 43.2
CPN-P-MPJPE (mm) Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Pavlakos et al. [9] 34.739.841.838.6 42.5 47.5 38.0 36.6 50.756.8 42.6 39.6 43.9 32.1 36.5 41.5
Rayat et al. [15] 35.739.344.643.0 47.2 54.0 38.3 37.5 51.661.3 46.5 41.4 47.3 34.2 39.4 44.1
Cai et al. [32] (T=7) 35.737.836.940.7 39.6 45.2 37.4 34.5 46.950.1 40.5 36.1 41.0 29.6 32.3 39.0
Lin and Lee [33] (T=50) 32.535.334.336.2 37.8 43.0 33.0 32.2 45.751.8 38.4 32.8 37.5 25.8 28.9 36.8
Pavllo et al. [2] (T=243) 34.136.134.437.2 36.4 42.2 34.4 33.6 45.052.5 37.4 33.8 37.8 25.6 27.3 36.5
Liu et al. [13] (T=243) 32.335.235.634.4 36.4 42.7 31.2 32.5 45.650.2 37.3 32.8 36.3 26.0 23.9 35.5
Wang et al. [16] (T=96) 32.935.235.634.4 36.4 42.7 31.2 32.5 45.650.2 37.3 32.8 36.3 26.0 23.9 35.5
Chen et al. [11] (T=243) 32.635.132.835.4 36.3 40.4 32.4 32.3 42.749.0 36.8 32.4 36.0 24.9 26.5 35.0
Zheng et al. [5] (T=81) 32.534.832.634.6 35.3 39.5 32.1 32.0 42.848.5 34.8 32.4 35.3 24.5 26.0 34.6
Li et al. [68] (T=351) 31.534.932.833.6 35.3 39.6 32.0 32.2 43.548.7 36.4 32.6 34.3 23.9 25.1 34.4
ConvFormer (T=143) 31.934.432.235.0 34.2 40.7 32.9 31.8 42.849.1 36.0 31.5 35.0 23.6 25.2 34.5
ConvFormer (T=243) 31.434.232.035.2 34.0 40.3 32.7 31.3 42.649.0 36.2 31.3 34.8 23.4 24.9 34.2
CPN-MPJVE Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Pavllo et al. [2] (T=243) 3.03.12.23.4 2.3 2.7 2.7 3.1 2.12.9 2.3 2.4 3.7 3.1 2.8 2.8
Chen et al. [11] (T=243) 2.72.82.03.1 2.0 2.4 2.4 2.8 1.82.4 2.0 2.1 3.4 2.7 2.4 2.5
Wang et al. [16] (T=96) 2.32.52.02.7 2.0 2.3 2.2 2.5 1.82.7 1.9 2.0 3.1 2.2 2.5 2.3
ConvFormer (T=143) 2.32.31.82.6 1.8 2.1 2.1 2.5 1.42.0 1.7 1.9 3.0 2.4 2.1 2.1
GT-MPJVE Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Wang et al. [16] (T=96) 1.21.31.11.4 1.1 1.4 1.2 1.4 1.01.3 1.0 1.1 1.7 1.3 1.4 1.4
ConvFormer (T=143) 1.21.30.91.4 1.0 1.2 1.3 1.5 0.71.1 0.9 1.1 1.7 1.4 1.2 1.2
Table 2: Quantitative results on HumanEva under protocol 2 for the left part of the table and quantitative results on MPI-INF-3DHP
in the right part of the table. Best is in Red and second best is Blue.
Action Walk Jog Box Method PCK "AUC "MPJPE (mm) #
Subject S1S2S3S1S2S3S1S2S3 [10] 75.7 39.3 117.6
Martinez et al. [1] 19.717.446.826.918.218.6–––Lin et al. [33] 83.6 51.4 79.8
Pavalkos et aal. [9] 22.319.529.728.921.923.8–––Pavllo et al. [2] 86.0 51.9 84.0
Pavllo et al. [2] 13.910.246.620.913.113.823.833.732.0Li et al. [43] 81.2 46.1 99.7
Zheng et al. [5] 16.311.047.125.015.215.1–––Chen et al. [11] 87.6 54.0 78.8
ConvFormer (T=9) 12.510.125.413.312.922.631.728.629.0Zheng et al. [5] 88.6 56.4 77.1
ConvFormer (T=27) 11.49.020.119.111.811.820.828.026.1Li et al. [68] 93.8 63.3 58.0
ConvFormer (T=43) 10.77.916.016.79.310.018.225.024.3 ConvFormer 96.4 69.8 53.6
We report results for our 143and243frame models on H3.6M and we report results for our 9,27, and 43frame model
for HumanEva. We report all 15 action results for both subjects S9 and S11 using GT and CPN detections as the 2D input
under protocol I, II, and III in Table. 1 and the last column represents the average. ConvFormer’s 143 and 243-frame
models substantially reduce the parameter count by 83.4% and 65.5% respectively, relative the previous
SOTA[68]. ConvFormer’s 143and243-frame models outperforms the previous SOTA on GT inputs – achieving a 2:3%
reduction of error. ConvFormer’s 243-frame model misses SOTA on CPN inputs for Protocol I by 0:2mm while having
7

--- PAGE 8 ---
substantially lowered parameters and achieving best or second best on 11 of the 15 actions. However, it outperforms the
SOTA on some challenging actions such as SittingandWalkingDog which exhibit complex postures and rapid postural
changes. Under Protocol II ConvFormer achieves SOTA on 9 individual actions and on the average error. Lastly, for both
GT and CPN inputs ConvFormer reduces the MPJVE by 8:6% and 14:3% respectively, resulting in smoother predictions.
See Figure 2 for some qualitative results on H36M or see https://github.com/AJDA1992/ConvFormer for more examples
from challenging in-the-wild motions.
The left side of Table 2 shows the results of training ConvFormer from scratch on HumanEva. We note that our larger
receptive ﬁeld model, with 43 frames, achieves SOTA for every action, while our 27 frame receptive ﬁeld model achieves
second place for every action.
The right side of Table 2 reports the quantitative results of ConvFormer on MPI-INF-3DHP relative to other methods.
Following [ 5,68], we use 2D pose sequences of 9 frames due to fewer samples and shorter video sequences. We note that
ConvFormer increases PCK by 2:7%, AUC by 10:2%, and decreases MPJPE by 7:6%.
5.2 Ablations
First, we study the contribution of individual hyper-parameters and tune them. Second, we assess the contribution
of convolutional self-attention relative to the baseline (vanilla transformer) and then the contribution of our dynamic
self-attention mechanism. To the ﬁrst point, we perform an extensive grid search using [ 3] and report some of the results in
Tables 3, 4. We ﬁne the following hyper-parameters to be optimal: d= 32,Bsp=Btemp= 2and using the following kernel
sizes (7;7;7).
In Table 5 we analyze the eﬀect of receptive ﬁeld alongside parameter counts relative to other transformer based
methods. We ﬁx the optimal hyper-parameters found in Tables 3, 4. We ﬁnd across all receptive ﬁelds, ConvFormer reduces
parameters substantially relative [71, 5] while remaining extremely competitive on CPN inputs for Protocol I.
Finally, we analyze what improvement ConvFormer brings relative to a vanilla transformer architecture and the beneﬁt
of you using our Dynamic Multi-Headed attention mechanism. In Table 6 our baseline model following the same architecture
as ConvFormer except with class scaled dot product attention and fully-connected layers generating the queries, keys, and
values. We ﬁnd by using a single ﬁlter in our ConvFormer architecture improves on the baseline by 2mm and introducing
our Dynamic Multi-Headed Attention we reduce by another 1.1 mm.
Table 3: Analysis of spatial embedding dimension and number of spatial and temporal ConvFormer Blocks. We also perform a limited
analysis on number of attention heads. Optimal performance is marked by Red and assessed by MPJPE.
d 16161632323264323232
Bsp 2242242222
Btemp 2422422222
Params (M) 0.651.260.692.564.952.709.972.562.562.56
Heads 8888888124
MPJPE (mm) 50.851.051.749.450.649.950.152.351.049.6
6 Conclusion
In this paper we attempt to address the ever growing complexity of transformer models. For this, we introduce ConvFormer
which is based on three novel components: temporal fusion, convolutional self-attention, and dynamic feature aggregation.
To assess the eﬀectiveness of diﬀerent components we conducted extensive ablation studies. We reduced the parameter
counts relative to the previous SOTA by over 65%while achieving SOTA on H36M for Protocol I on GT inputs, Protocol
II for CPN detections, Protocol III for both GT and CPN inputs, HumanEva for all subjects, and lastly all three metrics of
MPI. Interestingly, even though graph convolutional networks and graph attention networks are light-weight and robustly
model spatial/temporal relationships, ConvFormer provides a better trade oﬀ between error reduction and computational
complexity. We believe ConvFormer will provide more ready access to high quality 3D reconstruction networks by making
the training and inference process less computationally demanding.
8

--- PAGE 9 ---
Table 4: Analysis of diﬀerent kernel conﬁgurations where performance is evaluated relative to MPJPE on CPN detections for H36M.
Best marked in Red.
BspBtempKernels MPJPE (mm) Params (M)
22 3 51.7 2.44
22 3,3 51.6 2.46
223,3,3 50.8 2.48
22 5 50.5 2.46
22 5,5 52.0 2.49
225,5,5 51.9 2.52
22 7 50.5 2.47
22 7,7 50.1 2.51
227,7,7 49.4 2.56
22 9 51.5 2.48
22 9,9 50.8 2.54
229,9,9 50.6 2.60
223,5,7 50.6 2.52
225,7,9 51.6 2.56
Table 5: Parameter count and FLOPs results with MPJPE for diﬀerent transformer architectures and graph attention networks
separated by receptive ﬁeld. The last grouping is for models with largest receptive ﬁeld. Best and second best marked with Red and
Blue respectively.
TParams (M) FLOPs (M)* MPJPE (mm)
Zheng et al. [5] 9 9.58 180 49.9
Li et al. [68] 9 19.09 340 47.8
ConvFormer 9 2.56 100 49.4
Zheng et al. [5] 27 9.59 540 47.0
Li et al. [68] 27 19.18 1040 45.9
ConvFormer 27 2.65 360 47.7
Zheng et al. [5] 81 9.60 1620 44.3
Li et al. [68] 81 19.84 3120 44.5
ConvFormer 81 3.43 1600 45.0
Liu et al. [77] 243 7.09 9700 44.9
Li et al. [68] 351 31.52 14160 43.0
ConvFormer 143 5.24 4220 43.7
ConvFormer 243 10.24 10000 43.2
Method Params (M) MPJPE (mm)
Baseline 5.2 52.4
Single-Filter ConvFormer 2.47 50.5
Dynamic ConvFormer 2.56 49.4
Table 6: Ablation study analyzing eﬀect of diﬀerent components of ConvFormer on Accuracy and Parameter Counts.
9

--- PAGE 10 ---
7 Appendices
7.1 Attention Visualization
We provide in ﬁgure 4 additional visualizations of temporal attention heads for part of our ablation on number of attention
heads with quantitative results reported in Table 3. We note that our 8head model achieves the lowest MPJPE on H3.6M.
We hypothesize that, although there is a clear visual indication that as the number of heads increases, redundancies within
the attention maps occur with subtle variations, that this redundancy acts as a noise ﬁltering mechanism by highlighting
critical information. In the NLP landscape an extensive analysis on BERT was conducted to understand the optimal
number of heads and how one can perform head pruning during test-time without substantial performance impact, [74].
We also provide visualizations of the attention heads for both the spatial and temporal ConvFormer for all attention
heads utilized in our model. We evaluate the attention heads for subject 9 from H3.6M for the Directions action. The
spatial self attention maps are seen in Figure 5 and the x-axis corresponds to the 17 joints of the H3.6M skeleton and the
y-axis corresponds to the attention output. These maps correspond to the 143 frame model and the temporal attention
maps x-axis is the 143 frames of the sequence while the y-axis is the attention at each frame. The attention heads return
diﬀerent attention magnitudes which represent either spatial correlations or frame-wise global correlations learned from the
joint temporal proﬁles.
10

--- PAGE 11 ---
Figure 4: Temporal attention for 9 frame ConvFormer trained on H36M using CPN detections as input – a) is our one head model, b)
is 2 head model, c) is 4 head and d) is the 8 head model which achieves lowest MPJPE.
11

--- PAGE 12 ---
Figure 5: Example of Attention maps, top is the spatial ConvFormer and the bottom is the temporal ConvFormer for the 143 frame
model trained on CPN detections for H3.6M. These maps were generated for S9 for the Directions action.
12

--- PAGE 13 ---
References
[1]J. Martinez, R. Hossain, J. Romero, aand J. J. Little, title=A simple yet eﬀective baseline for 3d human pose estimation,
A simple yet eﬀective baseline for 3d human pose estimation, Proceedings of the IEEE International Conference on
Computer Vision, (2017), 2640-2649.
[2]D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli, 3d human pose estimation in video with temporal convolutions
and semi-supervised training, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), (2019), 7753-7762.
[3]R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. Gonzalez, and I. Stoica, Tune: A Research Platform for Distributed
Model Selection and Training, arXiv preprint arXiv:1807.05118, (2018).
[4]A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, Large-Scale Video Classiﬁcation with
Convolutional Neural Networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), (2014), 1725-1732.
[5]C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Cheng, and Z. Ding, 3D Human Pose Estimation with Spatial and Temporal
Transformers, Proceedings of the IEEE International Conference on Computer Vision (ICCV), (2021).
[6]H. S. Faang, Y. Xu, W. Wang, X. Liu, and S. C. Zhu, Learning pose grammar to encode human body conﬁguration for
3d pose estimation, Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (2018).
[7]C. Ionescu, D. Papava, V. Olarue, C. Sminchisescu, Human3.6m: Large scale datasets and predictive methods for 3d
human sensing in natural environments, IEEE transactions on pattern analysis and machine intelligence, 36 (7), (2013),
1325-1339.
[8]L. Sigal, A. O. Balaan, and M. J. Black, Humaneva: Synchronized video and motion capture dataset and baseline
algorithm for evaluation of articulated human motion, IEEE Transactions on Pattern Analysis and Machine Intelligence,
87 (12), (2010), 4-27.
[9]G. Pavaalkos, X. Zhou, K. G. Derpanis, and K. Daniilidis, Coarse-to-ﬁne volumetric prediction for single-image 3d
human pose, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2017),
7025-7034.
[10]D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt, Monocular 3D Human Pose
Estimation In The Wild Using Improved CNN Supervision,3D Vision (3DV), 2017 Fifth International Conference on
(2017
[11], T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, and J. Luo, Anatomy-aware 3d human pose estimation with bone-based
pose decomposition, IEEE Transactions on Circuits and Systems for Video Technology, (2021).
[12]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. U. Kaiser, and I. Polosukhin, Attention is
all you need, Advances in Neural Information Processing Systems (NIPS), (2017), 5998-6008.
[13]R. Liu, J. Shen, H. Wang, C. Chen, S. C. Cheung, and V. Asari, Attention mechanism exploits temporal contexts:
Real-time 3d human pose reconstruction, Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CPRV), (2020), 5064-5073.
[14]M. Kocabas, S. Karagozz, and E. Akbas, Self-supervised learning of 3d human pose using multi-view geometry,
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2019), 1077-1086.
[15]M. Raayat Imtia Hossain and J. J. Little, Exploiting temporal information for 3d human pose estimation, Proceedings
of the European Conference on Computer Vision (ECCV), (2018), 68-84.
[16]J. Wang, S. Yan, Y. Xiong, and D. Lin, Motion guided 3d pose estimation from videos, ArXiv, arXiv:2004.13985,
(2020).
[17]A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer,
Automatic Diﬀerentiation in PyTorch, NIPS 2017 Workshop on Autodiﬀ, (2017).
[18]J. Wang, S. Yan, Y. Xiong, and D. Lin, Deep networks with stochastic depth, European Conference on Computer
Vision (ECCV), (2016), 646-661.
[19]Z. Cao, G. Hidalgo, T. Simon, S-E. Wei, and Y. Sheikh, OpenPose: realtime multi-person 2D pose estimation using
Part Aﬃnity Fields, IEEE transactions on pattern analysis and machine intelligence, 43 (1), (3019), 172-186.
[20]S-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, Convolutional pose machines, Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition, (2016), 4724-4732.
[21]A. Kolesnikov, A. Dosovitskiy, D. Weissenborn, G. Heigold, J. Uszkoreit, L. Beyer, M. Minderer, M. Dehghani, N.
Houlsby, S. Gelly, T. Unterthiner, and X. Zhai, An Image is Worth 16x16 Words: Transformers for Image Recognition
at Scale, (2021).
[22]N. Srivastava, G. Hinton, A. Krihevsky, I. Sutskever, and R. Salakhudinov, Dropout: A Simple Way to Prevent Neural
Networks from Overﬁtting, Journal of Machine Learning Research, 15 (56), (2014), 1929-1958.
[23]H-S. Fang, S. Xie, Y-W. Tai, and C. Lu, RMPE: Regional Multi-person Pose Estimation, International Conference on
Computer Vision (ICCV), (2017).
13

--- PAGE 14 ---
[24]Y. Chen, Z. Wang, X. Peng, Z. Zhang, G. Yu, and J. Sun, Cascaded Pyramid Network for Multi-person Pose Estimation,
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2018), 7103-7112.
[25]K. Sun, B. Xiao, D. Liu, and J. Wang, Deep High-Resolution Representation Learning for Human Pose Estimation,
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2019).
[26]B. Xiao, H. Wu, Y. Wei, Simple Baselines for Human Pose Estimation and Tracking, European Conference on Computer
Vision (ECCV), (2020).
[27]Z. Wu, Z. Liu, J. Lin, and S. Han, Lite transformer with long-short range attention, International Conference on
Learning Representations (ICLR), (2020).
[28]Z. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan, Convbert: Improving bert with span-based dynamic
convolution, Advances in Neural Information Processing Sytems (NeurIPS), (2020).
[29]Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, Dynamic Convolution: Attention Over Convolution Kernels,
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2020), 11027-11036.
[30]A. Zeng, X. Sun, F. Huaang, M. Liu, Q. Xu, and S. Lin, Srnet: Improving generalization in 3d human pose estimation
with a split-and-recombine approach, European Conference on Computer Vision (ECCV), (2020).
[31]R. Dabral, A. Mundhaada, U. Kusupati, S. Afaque, A. Sharma, and A. Jain, Learning 3D Human Pose from Structure
and Motion, European Conference on Computer Vision (ECCV), (2018).
[32]Y. Cai, L. Ge, J. Liu, J. Cai, T-J. Cham, J. Yuan, N. M. Thalmann, Exploiting Spatial-Temporal Relationships for 3D
Pose Estimation via Graph Convolutional Networks, 2019 IEEE/CVF International Conference on Computer Vision
(ICCV), (2019), 2272-2281.
[33]J. Lin and G. Lee, Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation, British Machine
Vision Conference (BMVC), (2019).
[34]R. Yeh, Y. T. Hu, and A. Schwing, Chirality Nets for Human Pose Regression, International Conference on Neural
Information Processing Systems (NeurIPS), (2019).
[35]K. Lin, L. Wang, aand Z. Liu, End-to-End Human Pose and Mesh Reconstruction with Transformers, IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), (2021).
[36]B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, Learning Deep Features for Discriminative Localization,
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2016), 2921-2929.
[37]G. Moon, J. Chang, and K. M. Lee, Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation
from a Single RGB Image, The IEEE Conference on International Conference on Computer Vision (ICCV), (2019).
[38]G. Pavlakos, X. Zhou, and K. Daniilidis, Ordinal depth supervision for 3D human pose estimation, IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), (2018).
[39]G. Moon and K. M. Lee, I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose estimation and
mesh estimation from a single rgb image, European Conference on Computer Vision (ECCV), (2020).
[40]S. Khan, M. Naseer, M. Hayat, S. Zamir, F. Khan, and M. Shah, Transformers in Vision: A surver, ArXiv,
arXiv:2101.01169, (2021).
[41]N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillob, and S. Zagoruyko, Ene-to-End Object Detection with
Transformers, ArXiv, http://arxiv.org/abs/2005.12872, (2020).
[42]S. Li and A. B. Chan, 3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network,
Asian Conference on Computer Vision (ACCV), (2014).
[43]S. Li, L. Ke, K. Pratama, Y. W. Tai, C. K. Tang, K. T. Cheng, Cascaded deep monocular 3D human pose estimation
with evolutionary training data, ArXiv arXiv:2006.07778 (2020).
[44] X. Sun, B. Xiao, S. Liang, Y. Wei, Integral human pose regression, ArXiv arXiv:1711.08229, (2017).
[45]A. Kanmazawa, M. J. Black, D. W. Jacobs, and J. Malik, End-to-end Recovery of Human Shape and Pose, IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), (2017).
[46]K. Zhou, X. Han, N. Jaing, K. Jia, and J. Lu, HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate
3D Human Pose Estimation, International Conference of Computer Vision (ICCV), (@019).
[47]Y. Cheng, B. Yang, B. Waang, Y. Wending, and R. T. Tang, Occlusion Aware Networks for 3D Human Pose Estimation
in Video, International Conference of Computer Vision (ICCV), (2019).
[48]K. hou, X. Han, N. Jiang, K. Jai, aand J. Lu, HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human
Pose and Shape Estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). (2021).
[49]F. Huang, A. Zeng, M. Liu, Q. Lai, and Q. Xu, DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose
Estimation from Multi-View Image, IEEE Winter Conference on Applications of Computer Vision (WACV), (2020),
418-427.
[50]H. Shuai, L. Wu, and Q. Liu, Adaptively Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation,
ArXiv abs/2110.05092, (2021).
[51]K. Iskakov, E. Burkov, V. Lempitsky, and Y. Malkov, Learnable Triangulation of Human Pose. IEEE/CVF International
Conference on Computer Vision (ICCV), (2019).
14

--- PAGE 15 ---
[52]H. Qiu, C. Wang, J. Wang, N. Wang, and W. Zeng, Cross View Fusion for 3D Human Pose Estimation, IEEE/CVF
International Conference on Computer Vision (ICCV), (2019), 4341-4359.
[53]J. Dong, W. Jiang, Q. Huang, H. Bao, X. hou, Fast and Robust Multi-Person 3D Pose Estimation from Multiple View,
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2019).
[54]Y. He, R. Yan, K. Fragkiadaki, S. Yu. Epipolr Transformers, IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), (2020), 7776-7785.
[55]L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, . Jiang, F. Tay, J. Feng, and S. Yan, Tokens-to-Token ViT: Training Vision
Transformers from Scratch on ImageNet, IEEE International Conference on Computer Vision (ICCV) (2021), 538-547
[56]Z. Liu, N. Shun, W. Li, J. Lu, Y. Wu, C. Li, aand L. Yang, ConvTransformer: A Convolutional Transformer Network
for Video Frame Synthesis, ArXiv, abs/2011.10185 (2020).
[57]H. Touvron, M. Cord, M. Doue, F. Massa, A. Sablayrolles, H. J’egou, Training data-eﬃcient image transformers and
distillation through attention, International Conference of Machine Learning (ICML), (2021).
[58]S. Gilad, N. Asaf, -M. Lihi, An Image is Worth 16x16 Words, What is a Video Worth?, ArXXiv, abs/2103.13915
(2021).
[59]K. Chaitanya, M. Joshua, D. Hang, M-S. Roderick, CpT: Convolutional Point Transformer for 3D Point Cloud
Processing, ArXXiv, abs/2111.10866 (2021).
[60] P. Paaschalis and A. Antonis, PE-former: Pose Estimation Transformer, CoRR abs/2112.04981, (2021).
[61]N. Kitaev, L. Kaiser, and A. Levskaya, Reformer: The Eﬃcient Transformer, Proceedings of International Conference
on Learning Representations (ICLR) (2020).
[62]J. Sebastian, C. Aakanksha, M. Afroz, K. Lukasz, G. Wojciech, M. Henryk, and K. Jonni, Sparse is Enough in Scaling
Transformers, (2021).
[63]V. Belagiannis, S. Amin, M. AAndriluka, B. Schiele, N. Navab, and S. Ilic, 3D Pictorial Structures Revisited: Multiple
Human Pose Estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38 (10) (2016) 1929-1942.
[64]A. Diaz-Arias, M. Messmore, D. Shin, and S. Baek, On the role of depth predictions for 3D human pose estimation,
https://arxiv.org/abs/2103.02521 (2021).
[65]X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong and W. Woo, Convolutional LSTM Network: A Machine Learning
Approach for Precipitation Nowcasting, Proceedings of the 28th International Conference on Neural Information
Processing Systems, 1 (2015) 802-810.
[66] Q. Basatiaan, rnn: a Recurrent Neural Network in R, Working Papers (2016).
[67] S. Hochreiter and H. Schmidhuber, Long short-term memory, Neural computation, 9 (8) (1997) 1735-1780.
[68]W. Li, T. Hong, W. Hao, V. Picho, and L. Van Gool, MHFormer: Multi-Hypothesis Transformer for 3D Human Pose
Estimation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2022).
[69]Z. Zou and W. Tang, Modulated graph convolutional network for 3D human pose estimation, IEEE International
Conference on Computer Vision (ICCV), (2021), 11477-11487.
[70]K. Gong, J. Zhang, and J. Feng, PoseAug: A Diﬀerentiable Pose Augmentation Framework for 3D Human Pose
Estimation, CVPR, 2021.
[71]W. Li, H. Liu, R. Ding, M. Liu, P. Wang, W. Yang Exploiting temporal contexts with strided transformer for 3d
human pose estimation, IEEE Transaactions on Multimedia, 2022.
[72]M. Zanﬁr, A. Zanﬁr, E. Bazavan, W. Freeman, R. Sukthankar, and C. Sminchisescu, Thundr: Transformer-based 3d
human reconstruction with markers, Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.
[73]V. Sovrasov, Flops counter for convolutional networks in pytorch framework, https://github.com/sovrasov/ﬂops-
counter.pytorch/, 2019-11-12.
[74]P. Michel, O. Levy, and G. Neubig, Are Sixteen Heads Really Better than One?, Proceedings of the International
Conference on Neural Information Processing Systems (NeurIPS), (2019).
[75]Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio,
Graph Attention Networks, International Conference on Learning Representations (ICLR), 2018
[76]P. -E. Sarlin, D. DeTone, T. Malisiewicz and A. Rabinovich, "SuperGlue: Learning Feature Matching With Graph
Neural Networks," 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA,
USA, 2020, pp. 4937-4946
[77]Liu, Junfa and Rojas, Juan and Li, Yihui and Liang, Zhijun and Guan, Yisheng and Xi, Ning and Zhu, Haifei,
"A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video," 2021 IEEE
International Conference on Robotics and Automation (ICRA), Xi’an, China, 2021, pp. 3374-3380
[78]Jianzhai Wu, Dewen Hu, Fengtao Xiang, Xingsheng Yuan, and Jiongming Su. 2020. 3D human pose estimation by
depth map. Vis. Comput. 36, 7 (Jul 2020), 1401–1410
[79]S. Banik, A. M. GarcÍa and A. Knoll, "3D Human Pose Regression Using Graph Convolutional Network," 2021 IEEE
International Conference on Image Processing (ICIP), Anchorage, AK, USA, 2021, pp. 924-928
15
