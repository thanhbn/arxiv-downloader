# 2309.06180.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2309.06180.pdf
# Kích thước file: 1459631 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Quản Lý Bộ Nhớ Hiệu Quả cho Việc Phục Vụ Mô Hình Ngôn Ngữ Lớn với PagedAttention
Woosuk Kwon1,∗Zhuohan Li1,∗Siyuan Zhuang1Ying Sheng1,2Lianmin Zheng1Cody Hao Yu3
Joseph E. Gonzalez1Hao Zhang4Ion Stoica1
1UC Berkeley2Stanford University3Independent Researcher4UC San Diego

Tóm tắt
Việc phục vụ với thông lượng cao của các mô hình ngôn ngữ lớn (LLM) đòi hỏi phải gộp đủ nhiều yêu cầu cùng một lúc. Tuy nhiên, các hệ thống hiện tại gặp khó khăn vì bộ nhớ cache khóa-giá trị (KV cache) cho mỗi yêu cầu rất lớn và tăng giảm động. Khi được quản lý không hiệu quả, bộ nhớ này có thể bị lãng phí đáng kể do phân mảnh và nhân bản dư thừa, hạn chế kích thước batch. Để giải quyết vấn đề này, chúng tôi đề xuất PagedAttention, một thuật toán attention được lấy cảm hứng từ kỹ thuật bộ nhớ ảo và phân trang cổ điển trong hệ điều hành. Dựa trên đó, chúng tôi xây dựng vLLM, một hệ thống phục vụ LLM đạt được (1) gần như không lãng phí bộ nhớ KV cache và (2) chia sẻ linh hoạt KV cache trong và giữa các yêu cầu để giảm thêm việc sử dụng bộ nhớ. Các đánh giá của chúng tôi cho thấy vLLM cải thiện thông lượng của các LLM phổ biến lên 2-4× với cùng mức độ độ trễ so với các hệ thống tiên tiến như FasterTransformer và Orca. Cải thiện rõ rệt hơn với các chuỗi dài hơn, mô hình lớn hơn và thuật toán giải mã phức tạp hơn. Mã nguồn của vLLM có sẵn công khai tại https://github.com/vllm-project/vllm.

1 Giới thiệu
Sự xuất hiện của các mô hình ngôn ngữ lớn (LLM) như GPT [5, 37] và PaLM [9] đã cho phép các ứng dụng mới như trợ lý lập trình [6,18] và chatbot đa năng [19,35] đang bắt đầu tác động sâu sắc đến công việc và thói quen hàng ngày của chúng ta. Nhiều công ty đám mây [34,44] đang chạy đua để cung cấp các ứng dụng này như dịch vụ được lưu trữ. Tuy nhiên, việc chạy các ứng dụng này rất tốn kém, đòi hỏi một số lượng lớn bộ tăng tốc phần cứng như GPU. Theo ước tính gần đây, việc xử lý một yêu cầu LLM có thể đắt hơn 10× so với một truy vấn từ khóa truyền thống [43]. Với chi phí cao này, việc tăng thông lượng—và do đó giảm chi phí cho mỗi yêu cầu—của các hệ thống phục vụ LLM đang trở nên quan trọng hơn.

[Hình 1 mô tả: Trái: Bố cục bộ nhớ khi phục vụ một LLM với 13B tham số trên NVIDIA A100. Các tham số (màu xám) tồn tại trong bộ nhớ GPU suốt quá trình phục vụ. Bộ nhớ cho KV cache (màu đỏ) được (hủy)cấp phát cho mỗi yêu cầu phục vụ. Một lượng nhỏ bộ nhớ (màu vàng) được sử dụng tạm thời cho activation. Phải: vLLM làm mượt đường cong tăng trưởng nhanh của bộ nhớ KV cache thấy trong các hệ thống hiện tại [31,60], dẫn đến tăng đáng kể thông lượng phục vụ.]

Cốt lõi của LLM là một mô hình Transformer tự hồi quy [53]. Mô hình này tạo ra các từ (token), từng từ một, dựa trên đầu vào (prompt) và chuỗi token đầu ra trước đó mà nó đã tạo ra cho đến nay. Đối với mỗi yêu cầu, quá trình tốn kém này được lặp lại cho đến khi mô hình xuất ra một token kết thúc. Quá trình tạo tuần tự này khiến khối lượng công việc bị ràng buộc bởi bộ nhớ, không tận dụng hết sức mạnh tính toán của GPU và hạn chế thông lượng phục vụ.

Việc cải thiện thông lượng có thể thực hiện được bằng cách gộp nhiều yêu cầu lại với nhau. Tuy nhiên, để xử lý nhiều yêu cầu trong một batch, không gian bộ nhớ cho mỗi yêu cầu phải được quản lý hiệu quả. Ví dụ, Hình 1 (trái) minh họa phân bố bộ nhớ cho một LLM 13B tham số trên GPU NVIDIA A100 với 40GB RAM. Khoảng 65% bộ nhớ được cấp phát cho trọng số mô hình, vẫn tĩnh trong quá trình phục vụ. Gần 30% bộ nhớ được sử dụng để lưu trữ các trạng thái động của các yêu cầu. Đối với Transformer, các trạng thái này bao gồm các tensor khóa và giá trị liên quan đến cơ chế attention, thường được gọi là KV cache [41], đại diện cho ngữ cảnh từ các token trước đó để tạo ra các token đầu ra mới theo chuỗi. Phần trăm nhỏ còn lại của bộ nhớ được sử dụng cho dữ liệu khác, bao gồm activation - các tensor tạm thời được tạo khi đánh giá LLM. Vì trọng số mô hình là hằng số và activation chỉ chiếm một phần nhỏ bộ nhớ GPU, cách thức quản lý KV cache là quan trọng trong việc xác định kích thước batch tối đa. Khi được quản lý không hiệu quả, bộ nhớ KV cache có thể hạn chế đáng kể kích thước batch và do đó là thông lượng của LLM, như minh họa trong Hình 1 (phải).

Trong bài báo này, chúng tôi quan sát thấy các hệ thống phục vụ LLM hiện tại [31,60] không quản lý bộ nhớ KV cache một cách hiệu quả. Điều này chủ yếu do chúng lưu trữ KV cache của một yêu cầu trong không gian bộ nhớ liền kề, vì hầu hết các framework học sâu [33,39] yêu cầu tensor được lưu trữ trong bộ nhớ liền kề. Tuy nhiên, không giống như tensor trong khối lượng công việc học sâu truyền thống, KV cache có những đặc điểm độc đáo: nó tăng và giảm động theo thời gian khi mô hình tạo ra token mới, và thời gian tồn tại cũng như độ dài của nó không được biết trước. Những đặc điểm này khiến cách tiếp cận của các hệ thống hiện tại trở nên không hiệu quả đáng kể theo hai cách:

Thứ nhất, các hệ thống hiện tại [31,60] bị phân mảnh bộ nhớ bên trong và bên ngoài. Để lưu trữ KV cache của một yêu cầu trong không gian liền kề, chúng cấp phát trước một khối bộ nhớ liền kề với độ dài tối đa của yêu cầu (ví dụ: 2048 token). Điều này có thể dẫn đến phân mảnh bên trong nghiêm trọng, vì độ dài thực tế của yêu cầu có thể ngắn hơn nhiều so với độ dài tối đa (ví dụ: Hình 11). Hơn nữa, ngay cả khi độ dài thực tế được biết trước, việc cấp phát trước vẫn không hiệu quả: Vì toàn bộ khối được dành riêng trong suốt thời gian tồn tại của yêu cầu, các yêu cầu ngắn hơn khác không thể sử dụng bất kỳ phần nào của khối hiện tại chưa được sử dụng. Bên cạnh đó, phân mảnh bộ nhớ bên ngoài cũng có thể đáng kể, vì kích thước cấp phát trước có thể khác nhau cho mỗi yêu cầu. Thực vậy, kết quả profiling của chúng tôi trong Hình 2 cho thấy chỉ 20.4% - 38.2% bộ nhớ KV cache được sử dụng để lưu trữ trạng thái token thực tế trong các hệ thống hiện tại.

Thứ hai, các hệ thống hiện tại không thể khai thác các cơ hội chia sẻ bộ nhớ. Các dịch vụ LLM thường sử dụng các thuật toán giải mã tiên tiến, như lấy mẫu song song và beam search, tạo ra nhiều đầu ra cho mỗi yêu cầu. Trong các tình huống này, yêu cầu bao gồm nhiều chuỗi có thể chia sẻ một phần KV cache của chúng. Tuy nhiên, việc chia sẻ bộ nhớ không thể thực hiện được trong các hệ thống hiện tại vì KV cache của các chuỗi được lưu trữ trong các không gian liền kề riêng biệt.

Để giải quyết những hạn chế trên, chúng tôi đề xuất PagedAttention, một thuật toán attention được lấy cảm hứng từ giải pháp của hệ điều hành (OS) cho phân mảnh và chia sẻ bộ nhớ: bộ nhớ ảo với phân trang. PagedAttention chia KV cache của yêu cầu thành các khối, mỗi khối có thể chứa khóa và giá trị attention của một số lượng token cố định. Trong PagedAttention, các khối cho KV cache không nhất thiết phải được lưu trữ trong không gian liền kề. Do đó, chúng ta có thể quản lý KV cache một cách linh hoạt hơn như trong bộ nhớ ảo của OS: có thể nghĩ về các khối như các trang, token như byte, và yêu cầu như quy trình. Thiết kế này giảm thiểu phân mảnh bên trong bằng cách sử dụng các khối tương đối nhỏ và cấp phát chúng theo yêu cầu. Hơn nữa, nó loại bỏ phân mảnh bên ngoài vì tất cả các khối có cùng kích thước. Cuối cùng, nó cho phép chia sẻ bộ nhớ ở mức độ chi tiết của một khối, giữa các chuỗi khác nhau liên quan đến cùng một yêu cầu hoặc thậm chí giữa các yêu cầu khác nhau.

Trong công trình này, chúng tôi xây dựng vLLM, một công cụ phục vụ LLM phân tán thông lượng cao dựa trên PagedAttention đạt được gần như không lãng phí bộ nhớ KV cache. vLLM sử dụng quản lý bộ nhớ cấp khối và lập lịch yêu cầu preemptive được đồng thiết kế với PagedAttention. vLLM hỗ trợ các LLM phổ biến như GPT [5], OPT [62], và LLaMA [52] với các kích thước khác nhau, bao gồm những mô hình vượt quá dung lượng bộ nhớ của một GPU đơn. Các đánh giá của chúng tôi trên nhiều mô hình và khối lượng công việc khác nhau cho thấy vLLM cải thiện thông lượng phục vụ LLM lên 2-4× so với các hệ thống tiên tiến [31,60], mà không ảnh hưởng gì đến độ chính xác mô hình. Cải thiện rõ rệt hơn với các chuỗi dài hơn, mô hình lớn hơn và thuật toán giải mã phức tạp hơn (§4.3).

Tóm lại, chúng tôi đóng góp như sau:
• Chúng tôi xác định các thách thức trong cấp phát bộ nhớ khi phục vụ LLM và định lượng tác động của chúng lên hiệu suất phục vụ.
• Chúng tôi đề xuất PagedAttention, một thuật toán attention hoạt động trên KV cache được lưu trữ trong bộ nhớ phân trang không liền kề, được lấy cảm hứng từ bộ nhớ ảo và phân trang trong OS.
• Chúng tôi thiết kế và triển khai vLLM, một công cụ phục vụ LLM phân tán được xây dựng dựa trên PagedAttention.
• Chúng tôi đánh giá vLLM trên nhiều tình huống khác nhau và chứng minh rằng nó vượt trội đáng kể so với các giải pháp tiên tiến trước đây như FasterTransformer [31] và Orca [60].

2 Bối cảnh
Trong phần này, chúng tôi mô tả quy trình tạo và phục vụ của các LLM điển hình và lập lịch cấp iteration được sử dụng trong phục vụ LLM.

--- TRANG 2 ---
[Hình 2: Tỷ lệ phần trăm trung bình lãng phí bộ nhớ trong các hệ thống phục vụ LLM khác nhau trong thí nghiệm ở §6.2.]

2.1 Mô Hình Ngôn Ngữ Lớn Dựa Trên Transformer
Nhiệm vụ mô hình hóa ngôn ngữ là mô hình hóa xác suất của một danh sách token (𝑥1,...,𝑥𝑛). Vì ngôn ngữ có thứ tự tuần tự tự nhiên, thông thường ta phân tích xác suất kết hợp trên toàn bộ chuỗi thành tích của các xác suất có điều kiện (gọi là phân tách tự hồi quy [3]):

𝑃(𝑥) = 𝑃(𝑥1) · 𝑃(𝑥2|𝑥1) ··· 𝑃(𝑥𝑛|𝑥1,...,𝑥𝑛−1). (1)

Transformer [53] đã trở thành kiến trúc tiêu chuẩn de facto để mô hình hóa xác suất trên ở quy mô lớn. Thành phần quan trọng nhất của mô hình ngôn ngữ dựa trên Transformer là các lớp self-attention. Đối với một chuỗi trạng thái ẩn đầu vào (𝑥1,...,𝑥𝑛) ∈ ℝ𝑛×𝑑, một lớp self-attention đầu tiên áp dụng biến đổi tuyến tính tại mỗi vị trí 𝑖 để có được các vector query, key và value:

𝑞𝑖 = 𝑊𝑞𝑥𝑖, 𝑘𝑖 = 𝑊𝑘𝑥𝑖, 𝑣𝑖 = 𝑊𝑣𝑥𝑖. (2)

Sau đó, lớp self-attention tính toán điểm attention 𝑎𝑖𝑗 bằng cách nhân vector query tại một vị trí với tất cả các vector key trước nó và tính đầu ra 𝑜𝑖 như trung bình có trọng số trên các vector value:

𝑎𝑖𝑗 = exp(𝑞⊤𝑖𝑘𝑗/√𝑑) / Σ𝑖𝑡=1exp(𝑞⊤𝑖𝑘𝑡/√𝑑), 𝑜𝑖 = Σ𝑖𝑗=1𝑎𝑖𝑗𝑣𝑗. (3)

Bên cạnh tính toán trong Eq. 4, tất cả các thành phần khác trong mô hình Transformer, bao gồm lớp embedding, lớp feed-forward, layer normalization [2], kết nối residual [22], tính toán logit đầu ra, và biến đổi query, key, value trong Eq. 2, đều được áp dụng độc lập theo từng vị trí dưới dạng 𝑦𝑖 = 𝑓(𝑥𝑖).

2.2 Dịch Vụ LLM & Tạo Tự Hồi Quy
Sau khi được huấn luyện, LLM thường được triển khai như một dịch vụ tạo có điều kiện (ví dụ: completion API [34] hoặc chatbot [19,35]). Một yêu cầu đến dịch vụ LLM cung cấp một danh sách token prompt đầu vào (𝑥1,...,𝑥𝑛), và dịch vụ LLM tạo ra một danh sách token đầu ra (𝑥𝑛+1,...,𝑥𝑛+𝑇) theo Eq. 1. Chúng tôi gọi sự nối của danh sách prompt và đầu ra là chuỗi.

Do phân tách trong Eq. 1, LLM chỉ có thể lấy mẫu và tạo token mới từng cái một, và quá trình tạo mỗi token mới phụ thuộc vào tất cả các token trước đó trong chuỗi đó, cụ thể là các vector key và value của chúng. Trong quá trình tạo tuần tự này, các vector key và value của các token hiện tại thường được cache để tạo các token tương lai, được gọi là KV cache. Lưu ý rằng KV cache của một token phụ thuộc vào tất cả các token trước nó. Điều này có nghĩa là KV cache của cùng một token xuất hiện ở các vị trí khác nhau trong một chuỗi sẽ khác nhau.

Cho một prompt yêu cầu, tính toán tạo trong dịch vụ LLM có thể được phân tách thành hai giai đoạn:

Giai đoạn prompt nhận toàn bộ prompt người dùng (𝑥1,...,𝑥𝑛) làm đầu vào và tính xác suất của token mới đầu tiên 𝑃(𝑥𝑛+1|𝑥1,...,𝑥𝑛). Trong quá trình này, cũng tạo ra các vector key 𝑘1,...,𝑘𝑛 và vector value 𝑣1,...,𝑣𝑛. Vì các token prompt 𝑥1,...,𝑥𝑛 đều đã biết, tính toán của giai đoạn prompt có thể được song song hóa bằng các phép toán nhân ma trận-ma trận. Do đó, giai đoạn này có thể sử dụng hiệu quả tính song song vốn có trong GPU.

Giai đoạn tạo tự hồi quy tạo ra các token mới còn lại một cách tuần tự. Tại iteration 𝑡, mô hình nhận một token 𝑥𝑛+𝑡 làm đầu vào và tính xác suất 𝑃(𝑥𝑛+𝑡+1|𝑥1,...,𝑥𝑛+𝑡) với các vector key 𝑘1,...,𝑘𝑛+𝑡 và vector value 𝑣1,...,𝑣𝑛+𝑡. Lưu ý rằng các vector key và value tại vị trí 1 đến 𝑛+𝑡−1 được cache ở các iteration trước, chỉ vector key và value mới 𝑘𝑛+𝑡 và 𝑣𝑛+𝑡 được tính toán tại iteration này. Giai đoạn này hoàn thành khi chuỗi đạt độ dài tối đa (được chỉ định bởi người dùng hoặc giới hạn bởi LLM) hoặc khi một token kết thúc chuỗi (<eos>) được phát ra. Tính toán tại các iteration khác nhau không thể được song song hóa do phụ thuộc dữ liệu và thường sử dụng phép nhân ma trận-vector, ít hiệu quả hơn. Kết quả là, giai đoạn này sử dụng không đầy đủ tính toán GPU và trở thành bị ràng buộc bởi bộ nhớ, chịu trách nhiệm cho phần lớn độ trễ của một yêu cầu đơn.

2.3 Kỹ Thuật Batching cho LLM
Việc sử dụng tính toán trong phục vụ LLM có thể được cải thiện bằng cách gộp nhiều yêu cầu. Bởi vì các yêu cầu chia sẻ cùng trọng số mô hình, chi phí di chuyển trọng số được phân bổ qua các yêu cầu trong một batch, và có thể được vượt qua bởi chi phí tính toán khi kích thước batch đủ lớn. Tuy nhiên, việc gộp các yêu cầu cho dịch vụ LLM không phải là điều đơn giản vì hai lý do. Thứ nhất, các yêu cầu có thể đến vào những thời điểm khác nhau. Chiến lược batching ngây thơ sẽ hoặc là làm cho các yêu cầu đến sớm phải chờ các yêu cầu đến muộn hoặc trì hoãn các yêu cầu đến cho đến khi các yêu cầu trước đó hoàn thành, dẫn đến độ trễ xếp hàng đáng kể. Thứ hai, các yêu cầu có thể có độ dài đầu vào và đầu ra rất khác nhau (Hình 11). Kỹ thuật batching đơn giản sẽ pad đầu vào và đầu ra của các yêu cầu để cân bằng độ dài của chúng, lãng phí tính toán và bộ nhớ GPU.

Để giải quyết vấn đề này, các cơ chế batching chi tiết, như cellular batching [16] và lập lịch cấp iteration [60], đã được đề xuất. Không giống như các phương pháp truyền thống làm việc ở cấp yêu cầu, các kỹ thuật này hoạt động ở cấp iteration. Sau mỗi iteration, các yêu cầu đã hoàn thành được loại bỏ khỏi batch, và các yêu cầu mới được thêm vào. Do đó, một yêu cầu mới có thể được xử lý sau khi chờ một iteration duy nhất, không phải chờ toàn bộ batch hoàn thành. Hơn nữa, với các kernel GPU đặc biệt, các kỹ thuật này loại bỏ nhu cầu pad đầu vào và đầu ra. Bằng cách giảm độ trễ xếp hàng và sự không hiệu quả từ padding, các cơ chế batching chi tiết tăng đáng kể thông lượng của phục vụ LLM.

--- TRANG 3 ---
[Hình 3: Quản lý bộ nhớ KV cache trong các hệ thống hiện tại. Ba loại lãng phí bộ nhớ - dự trữ, phân mảnh bên trong và phân mảnh bên ngoài - tồn tại và ngăn các yêu cầu khác vừa vào bộ nhớ. Token trong mỗi slot bộ nhớ đại diện cho KV cache của nó. Lưu ý các token giống nhau có thể có KV cache khác nhau khi ở các vị trí khác nhau.]

3 Thách Thức Bộ Nhớ trong Phục Vụ LLM
Mặc dù batching chi tiết giảm lãng phí tính toán và cho phép các yêu cầu được gộp một cách linh hoạt hơn, số lượng yêu cầu có thể được gộp cùng nhau vẫn bị hạn chế bởi dung lượng bộ nhớ GPU, đặc biệt là không gian được cấp phát để lưu trữ KV cache. Nói cách khác, thông lượng của hệ thống phục vụ bị ràng buộc bởi bộ nhớ. Vượt qua ràng buộc bộ nhớ này đòi hỏi giải quyết các thách thức sau trong quản lý bộ nhớ:

KV cache lớn. Kích thước KV cache tăng nhanh với số lượng yêu cầu. Ví dụ, đối với mô hình OPT 13B tham số [62], KV cache của một token đơn đòi hỏi 800 KB không gian, được tính như 2(vector key và value) × 5120 (kích thước trạng thái ẩn) × 40(số lớp) × 2(byte cho mỗi FP16). Vì OPT có thể tạo chuỗi lên đến 2048 token, bộ nhớ cần thiết để lưu trữ KV cache của một yêu cầu có thể lên đến 1.6 GB. GPU đồng thời có dung lượng bộ nhớ hàng chục GB. Ngay cả khi tất cả bộ nhớ có sẵn được cấp phát cho KV cache, chỉ vài chục yêu cầu có thể được chứa. Hơn nữa, quản lý bộ nhớ không hiệu quả có thể giảm thêm kích thước batch, như thể hiện trong Hình 2. Thêm vào đó, với xu hướng hiện tại, tốc độ tính toán của GPU tăng nhanh hơn dung lượng bộ nhớ [17]. Ví dụ, từ NVIDIA A100 đến H100, FLOPS tăng hơn 2x, nhưng bộ nhớ GPU vẫn ở mức tối đa 80GB. Do đó, chúng tôi tin rằng bộ nhớ sẽ trở thành một nút cổ chai ngày càng quan trọng.

Thuật toán giải mã phức tạp. Các dịch vụ LLM cung cấp một loạt thuật toán giải mã để người dùng lựa chọn, mỗi thuật toán có những ý nghĩa khác nhau về độ phức tạp quản lý bộ nhớ. Ví dụ, khi người dùng yêu cầu nhiều mẫu ngẫu nhiên từ một prompt đầu vào duy nhất, một trường hợp sử dụng điển hình trong gợi ý chương trình [18], KV cache của phần prompt, chiếm 12% tổng bộ nhớ KV cache trong thí nghiệm của chúng tôi (§6.3), có thể được chia sẻ để giảm thiểu việc sử dụng bộ nhớ. Mặt khác, KV cache trong giai đoạn tạo tự hồi quy nên không được chia sẻ do kết quả mẫu khác nhau và sự phụ thuộc của chúng vào ngữ cảnh và vị trí. Mức độ chia sẻ KV cache phụ thuộc vào thuật toán giải mã cụ thể được sử dụng. Trong các thuật toán phức tạp hơn như beam search [49], các beam yêu cầu khác nhau có thể chia sẻ các phần lớn hơn (lên đến 55% tiết kiệm bộ nhớ, xem §6.3) KV cache của chúng, và mẫu chia sẻ phát triển khi quá trình giải mã tiến triển.

Lập lịch cho độ dài đầu vào & đầu ra chưa biết. Các yêu cầu đến dịch vụ LLM thể hiện sự biến đổi trong độ dài đầu vào và đầu ra của chúng. Điều này đòi hỏi hệ thống quản lý bộ nhớ phải chứa được một loạt rộng độ dài prompt. Thêm vào đó, khi độ dài đầu ra của một yêu cầu tăng trong quá trình giải mã, bộ nhớ cần thiết cho KV cache của nó cũng mở rộng và có thể cạn kiệt bộ nhớ có sẵn cho các yêu cầu đến hoặc tạo đang diễn ra cho các prompt hiện tại. Hệ thống cần đưa ra quyết định lập lịch, như xóa hoặc swap ra KV cache của một số yêu cầu khỏi bộ nhớ GPU.

3.1 Quản Lý Bộ Nhớ trong Các Hệ Thống Hiện Tại
Vì hầu hết các toán tử trong các framework học sâu hiện tại [33,39] đòi hỏi tensor được lưu trữ trong bộ nhớ liền kề, các hệ thống phục vụ LLM trước đây [31,60] cũng lưu trữ KV cache của một yêu cầu như một tensor liền kề qua các vị trí khác nhau. Do độ dài đầu ra không thể đoán trước từ LLM, chúng cấp phát tĩnh một khối bộ nhớ cho một yêu cầu dựa trên độ dài chuỗi tối đa có thể của yêu cầu, bất kể độ dài đầu vào thực tế hoặc độ dài đầu ra cuối cùng của yêu cầu.

Hình 3 minh họa hai yêu cầu: yêu cầu A với độ dài chuỗi tối đa có thể là 2048 và yêu cầu B với tối đa 512. Sơ đồ cấp phát khối trước trong các hệ thống hiện tại có ba nguồn chính của lãng phí bộ nhớ: slot dự trữ cho các token tương lai, phân mảnh bên trong do cấp phát quá mức cho độ dài chuỗi tối đa tiềm năng, và phân mảnh bên ngoài từ bộ cấp phát bộ nhớ như buddy allocator. Phân mảnh bên ngoài sẽ không bao giờ được sử dụng cho các token được tạo, điều này được biết trước khi phục vụ một yêu cầu. Phân mảnh bên trong cũng không được sử dụng, nhưng điều này chỉ được nhận ra sau khi một yêu cầu đã hoàn thành lấy mẫu. Cả hai đều là lãng phí bộ nhớ thuần túy. Mặc dù bộ nhớ dự trữ cuối cùng được sử dụng, việc dành riêng không gian này trong suốt thời gian tồn tại của yêu cầu, đặc biệt khi không gian dự trữ lớn, chiếm không gian có thể được sử dụng để xử lý các yêu cầu khác. Chúng tôi trực quan hóa tỷ lệ phần trăm trung bình lãng phí bộ nhớ trong các thí nghiệm của mình trong Hình 2, tiết lộ rằng bộ nhớ hiệu quả thực tế trong các hệ thống trước đây có thể thấp đến 20.4%.

Mặc dù compaction [54] đã được đề xuất như một giải pháp tiềm năng cho phân mảnh, việc thực hiện compaction trong một hệ thống phục vụ LLM nhạy cảm với hiệu suất là không thực tế do KV cache khổng lồ. Ngay cả với compaction, không gian khối cấp phát trước cho mỗi yêu cầu ngăn cản việc chia sẻ bộ nhớ cụ thể cho thuật toán giải mã trong các hệ thống quản lý bộ nhớ hiện tại.

4 Phương Pháp
Trong công trình này, chúng tôi phát triển một thuật toán attention mới, PagedAttention, và xây dựng một công cụ phục vụ LLM, vLLM, để giải quyết các thách thức được nêu trong §3. Kiến trúc của vLLM được thể hiện trong Hình 4. vLLM áp dụng một scheduler tập trung để điều phối việc thực thi các worker GPU phân tán. KV cache manager quản lý hiệu quả KV cache theo cách phân trang, được kích hoạt bởi PagedAttention. Cụ thể, KV cache manager quản lý bộ nhớ KV cache vật lý trên các worker GPU thông qua các chỉ thị được gửi bởi scheduler tập trung.

Tiếp theo, chúng tôi mô tả thuật toán PagedAttention trong §4.1. Với điều đó, chúng tôi trình bày thiết kế của KV cache manager trong §4.2 và cách nó hỗ trợ PagedAttention trong §4.3, tương ứng. Sau đó, chúng tôi chỉ ra cách thiết kế này tạo điều kiện cho quản lý bộ nhớ hiệu quả cho các phương pháp giải mã khác nhau (§4.4) và xử lý các chuỗi đầu vào và đầu ra có độ dài biến đổi (§4.5). Cuối cùng, chúng tôi chỉ ra cách thiết kế hệ thống của vLLM hoạt động trong môi trường phân tán (§4.6).

[Hình 4: Tổng quan hệ thống vLLM.]

4.1 PagedAttention
Để giải quyết các thách thức bộ nhớ trong §3, chúng tôi giới thiệu PagedAttention, một thuật toán attention được lấy cảm hứng từ ý tưởng cổ điển của phân trang [25] trong hệ điều hành. Không giống như các thuật toán attention truyền thống, PagedAttention cho phép lưu trữ key và value liên tục trong không gian bộ nhớ không liền kề. Cụ thể, PagedAttention phân chia KV cache của mỗi chuỗi thành các KV block. Mỗi block chứa key và value vector cho một số lượng token cố định, chúng tôi ký hiệu là kích thước KV block (𝐵). Ký hiệu key block 𝐾𝑗 = (𝑘(𝑗−1)𝐵+1,...,𝑘𝑗𝐵) và value block 𝑉𝑗 = (𝑣(𝑗−1)𝐵+1,...,𝑣𝑗𝐵). Tính toán attention trong Eq. 4 có thể được chuyển đổi thành tính toán theo khối như sau:

𝐴𝑖𝑗 = exp(𝑞⊤𝑖𝐾𝑗/√𝑑) / Σ⌈𝑖/𝐵⌉𝑡=1exp(𝑞⊤𝑖𝐾𝑡1/√𝑑), 𝑜𝑖 = Σ⌈𝑖/𝐵⌉𝑗=1𝑉𝑗𝐴⊤𝑖𝑗, (4)

trong đó 𝐴𝑖𝑗 = (𝑎𝑖,(𝑗−1)𝐵+1,...,𝑎𝑖,𝑗𝐵) là vector hàng của điểm attention trên KV block thứ 𝑗.

Trong quá trình tính toán attention, kernel PagedAttention xác định và lấy các KV block khác nhau riêng biệt. Chúng tôi thể hiện một ví dụ về PagedAttention trong Hình 5: Các vector key và value được trải rộng qua ba block, và ba block không liền kề trên bộ nhớ vật lý. Tại mỗi thời điểm, kernel nhân vector query 𝑞𝑖 của token query ("forth") và các vector key 𝐾𝑗 trong một block (ví dụ: vector key của "Four score and seven" cho block 0) để tính điểm attention 𝐴𝑖𝑗, và sau đó nhân 𝐴𝑖𝑗 với các vector value 𝑉𝑗 trong một block để suy ra đầu ra attention cuối cùng 𝑜𝑖.

Tóm lại, thuật toán PagedAttention cho phép các KV block được lưu trữ trong bộ nhớ vật lý không liền kề, tạo điều kiện cho quản lý bộ nhớ phân trang linh hoạt hơn trong vLLM.

[Hình 5: Minh họa thuật toán PagedAttention, trong đó các vector attention key và value được lưu trữ như các block không liền kề trong bộ nhớ.]

4.2 KV Cache Manager
Ý tưởng chính đằng sau trình quản lý bộ nhớ của vLLM tương tự như bộ nhớ ảo [25] trong hệ điều hành. OS phân chia bộ nhớ thành các trang có kích thước cố định và ánh xạ các trang logic của chương trình người dùng đến các trang vật lý. Các trang logic liền kề có thể tương ứng với các trang bộ nhớ vật lý không liền kề, cho phép các chương trình người dùng truy cập bộ nhớ như thể nó liền kề. Hơn nữa, không gian bộ nhớ vật lý không cần phải được dành riêng đầy đủ trước, cho phép OS cấp phát động các trang vật lý khi cần thiết. vLLM sử dụng các ý tưởng đằng sau bộ nhớ ảo để quản lý KV cache trong dịch vụ LLM. Được kích hoạt bởi PagedAttention, chúng tôi tổ chức KV cache như các KV block có kích thước cố định, giống như các trang trong bộ nhớ ảo.

KV cache của một yêu cầu được biểu diễn như một chuỗi các logical KV block, được lấp đầy từ trái sang phải khi các token mới và KV cache của chúng được tạo ra. Các vị trí chưa lấp đầy của KV block cuối cùng được dành riêng cho các lần tạo tương lai. Trên các worker GPU, một block engine cấp phát một khối DRAM GPU liền kề và chia nó thành các physical KV block (điều này cũng được thực hiện trên CPU RAM để swapping; xem §4.5). KV block manager cũng duy trì các block table - ánh xạ giữa các logical và physical KV block của mỗi yêu cầu. Mỗi mục block table ghi lại các physical block tương ứng của một logical block và số lượng vị trí đã lấp đầy. Việc tách biệt logical và physical KV block cho phép vLLM tăng trưởng động bộ nhớ KV cache mà không cần dành riêng cho tất cả vị trí trước, điều này loại bỏ hầu hết lãng phí bộ nhớ trong các hệ thống hiện tại, như trong Hình 2.

--- TRANG 4 ---
[Hình 6: Dịch block table trong vLLM.]

4.3 Giải Mã với PagedAttention và vLLM
Tiếp theo, chúng tôi đi qua một ví dụ, như trong Hình 6, để chứng minh cách vLLM thực thi PagedAttention và quản lý bộ nhớ trong quá trình giải mã của một chuỗi đầu vào duy nhất:

1○ Như trong bộ nhớ ảo của OS, vLLM không yêu cầu dành riêng bộ nhớ cho độ dài chuỗi được tạo tối đa có thể ban đầu. Thay vào đó, nó chỉ dành riêng các KV block cần thiết để chứa KV cache được tạo trong quá trình tính toán prompt. Trong trường hợp này, prompt có 7 token, vì vậy vLLM ánh xạ 2 logical KV block đầu tiên (0 và 1) đến 2 physical KV block (7 và 1, tương ứng). Trong bước prefill, vLLM tạo KV cache của prompt và token đầu ra đầu tiên với thuật toán self-attention thông thường (ví dụ: [13]). vLLM sau đó lưu trữ KV cache của 4 token đầu tiên trong logical block 0 và 3 token tiếp theo trong logical block 1. Slot còn lại được dành riêng cho giai đoạn tạo tự hồi quy tiếp theo.

2○ Trong bước giải mã tự hồi quy đầu tiên, vLLM tạo token mới với thuật toán PagedAttention trên physical block 7 và 1. Vì một slot vẫn có sẵn trong logical block cuối cùng, KV cache mới được tạo được lưu trữ ở đó, và bản ghi #filled của block table được cập nhật.

3○ Tại bước giải mã thứ hai, khi logical block cuối cùng đã đầy, vLLM lưu trữ KV cache mới được tạo trong một logical block mới; vLLM cấp phát một physical block mới (physical block 3) cho nó và lưu trữ ánh xạ này trong block table.

Toàn cầu, cho mỗi iteration giải mã, vLLM đầu tiên chọn một tập hợp các chuỗi ứng viên để batching (thêm trong §4.5), và cấp phát các physical block cho các logical block mới cần thiết. Sau đó, vLLM nối tất cả các token đầu vào của iteration hiện tại (tức là tất cả token cho giai đoạn prompt cho các yêu cầu prompt và các token mới nhất cho các yêu cầu giai đoạn tạo) thành một chuỗi và đưa nó vào LLM. Trong quá trình tính toán của LLM, vLLM sử dụng kernel PagedAttention để truy cập KV cache trước đó được lưu trữ dưới dạng logical KV block và lưu KV cache mới được tạo vào các physical KV block. Lưu trữ nhiều token trong một KV block (kích thước block > 1) cho phép kernel PagedAttention xử lý KV cache qua nhiều vị trí hơn song song, do đó tăng việc sử dụng phần cứng và giảm độ trễ. Tuy nhiên, kích thước block lớn hơn cũng tăng phân mảnh bộ nhớ. Chúng tôi nghiên cứu tác động của kích thước block trong §7.2.

Một lần nữa, vLLM cấp phát động các physical block mới cho logical block khi nhiều token và KV cache của chúng được tạo ra. Vì tất cả các block được lấp đầy từ trái sang phải và một physical block mới chỉ được cấp phát khi tất cả các block trước đó đã đầy, vLLM giới hạn tất cả lãng phí bộ nhớ cho một yêu cầu trong một block, vì vậy nó có thể sử dụng hiệu quả tất cả bộ nhớ, như thể hiện trong Hình 2. Điều này cho phép nhiều yêu cầu hơn vừa vào bộ nhớ để batching—do đó cải thiện thông lượng. Khi một yêu cầu hoàn thành việc tạo của nó, các KV block của nó có thể được giải phóng để lưu trữ KV cache của các yêu cầu khác. Trong Hình 7, chúng tôi thể hiện một ví dụ về vLLM quản lý bộ nhớ cho hai chuỗi. Các logical block của hai chuỗi được ánh xạ đến các physical block khác nhau trong không gian được dành riêng bởi block engine trong các worker GPU. Các logical block lân cận của cả hai chuỗi không cần phải liền kề trong bộ nhớ GPU vật lý và không gian của các physical block có thể được sử dụng hiệu quả bởi cả hai chuỗi.

[Hình 7: Lưu trữ KV cache của hai yêu cầu cùng một lúc trong vLLM.]

4.4 Ứng Dụng cho Các Tình Huống Giải Mã Khác
§4.3 chỉ ra cách PagedAttention và vLLM xử lý các thuật toán giải mã cơ bản, như giải mã tham lam và lấy mẫu, nhận một prompt người dùng làm đầu vào và tạo ra một chuỗi đầu ra duy nhất. Trong nhiều ứng dụng LLM thành công [18,34], một dịch vụ LLM phải cung cấp các tình huống giải mã phức tạp hơn thể hiện các mẫu truy cập phức tạp và nhiều cơ hội chia sẻ bộ nhớ hơn. Chúng tôi thể hiện khả năng áp dụng tổng quát của vLLM trên chúng trong phần này.

Lấy mẫu song song. Trong các trợ lý chương trình dựa trên LLM [6,18], một LLM tạo ra nhiều đầu ra được lấy mẫu cho một prompt đầu vào duy nhất; người dùng có thể chọn một đầu ra yêu thích từ các ứng viên khác nhau. Cho đến nay chúng tôi đã giả định ngầm rằng một yêu cầu tạo ra một chuỗi duy nhất. Trong phần còn lại của bài báo này, chúng tôi giả định trường hợp tổng quát hơn trong đó một yêu cầu tạo ra nhiều chuỗi. Trong lấy mẫu song song, một yêu cầu bao gồm nhiều mẫu chia sẻ cùng prompt đầu vào, cho phép KV cache của prompt cũng được chia sẻ. Thông qua PagedAttention và quản lý bộ nhớ phân trang của nó, vLLM có thể thực hiện việc chia sẻ này một cách dễ dàng và tiết kiệm bộ nhớ.

[Hình 8: Ví dụ lấy mẫu song song.]

Hình 8 thể hiện một ví dụ về giải mã song song cho hai đầu ra. Vì cả hai đầu ra chia sẻ cùng prompt, chúng tôi chỉ dành riêng không gian cho một bản sao trạng thái của prompt tại giai đoạn prompt; các logical block cho prompt của cả hai chuỗi được ánh xạ đến cùng các physical block: logical block 0 và 1 của cả hai chuỗi được ánh xạ đến physical block 7 và 1, tương ứng. Vì một physical block duy nhất có thể được ánh xạ đến nhiều logical block, chúng tôi giới thiệu một số đếm tham chiếu cho mỗi physical block. Trong trường hợp này, số đếm tham chiếu cho physical block 7 và 1 đều là 2. Tại giai đoạn tạo, hai đầu ra lấy mẫu các token đầu ra khác nhau và cần lưu trữ riêng biệt cho KV cache. vLLM triển khai cơ chế copy-on-write ở mức độ chi tiết block cho các physical block cần sửa đổi bởi nhiều chuỗi, tương tự như kỹ thuật copy-on-write trong bộ nhớ ảo OS (ví dụ: khi fork một quy trình). Cụ thể, trong Hình 8, khi mẫu A1 cần ghi vào logical block cuối cùng của nó (logical block 1), vLLM nhận ra rằng số đếm tham chiếu của physical block tương ứng (physical block 1) lớn hơn 1; nó cấp phát một physical block mới (physical block 3), chỉ thị cho block engine sao chép thông tin từ physical block 1, và giảm số đếm tham chiếu xuống 1. Tiếp theo, khi mẫu A2 ghi vào physical block 1, số đếm tham chiếu đã được giảm xuống 1; do đó A2 trực tiếp ghi KV cache mới được tạo của nó vào physical block 1.

Tóm lại, vLLM cho phép chia sẻ hầu hết không gian được sử dụng để lưu trữ KV cache của prompt qua nhiều mẫu đầu ra, ngoại trừ logical block cuối cùng, được quản lý bởi cơ chế copy-on-write. Bằng cách chia sẻ physical block qua nhiều mẫu, việc sử dụng bộ nhớ có thể được giảm đáng kể, đặc biệt cho các prompt đầu vào dài.

Beam search. Trong các nhiệm vụ LLM như dịch máy [59], người dùng mong đợi đầu ra dịch thuật phù hợp nhất top-𝑘 được xuất ra bởi LLM. Beam search [49] được sử dụng rộng rãi để giải mã chuỗi đầu ra có khả năng nhất từ một LLM, vì nó giảm thiểu độ phức tạp tính toán của việc duyệt hoàn toàn không gian mẫu. Thuật toán dựa vào tham số beam width 𝑘, xác định số lượng ứng viên hàng đầu được giữ lại ở mỗi bước. Trong quá trình giải mã, beam search mở rộng mỗi chuỗi ứng viên trong beam bằng cách xem xét tất cả các token có thể, tính xác suất tương ứng của chúng bằng LLM, và giữ lại các chuỗi có khả năng nhất top-𝑘 trong số 𝑘·|𝑉| ứng viên, trong đó |𝑉| là kích thước từ vựng.

Không giống như giải mã song song, beam search tạo điều kiện chia sẻ không chỉ các block prompt ban đầu mà còn các block khác qua các ứng viên khác nhau, và các mẫu chia sẻ thay đổi động khi quá trình giải mã tiến triển, tương tự như cây quy trình trong OS được tạo bởi các fork kết hợp. Hình 9 thể hiện cách vLLM quản lý các KV block cho một ví dụ beam search với 𝑘=4. Trước iteration được minh họa như đường chấm, mỗi chuỗi ứng viên đã sử dụng 4 logical block đầy. Tất cả ứng viên beam chia sẻ block đầu tiên 0 (tức là prompt). Ứng viên 3 tách ra khỏi các ứng viên khác từ block thứ hai. Ứng viên 0-2 chia sẻ 3 block đầu tiên và tách ra ở block thứ tư. Tại các iteration tiếp theo, tất cả ứng viên có khả năng top-4 đều xuất phát từ ứng viên 1 và 2. Vì ứng viên 0 và 3 ban đầu không còn nằm trong số các ứng viên hàng đầu, các logical block của chúng được giải phóng, và số đếm tham chiếu của các physical block tương ứng được giảm. vLLM giải phóng tất cả physical block có số đếm tham chiếu đạt 0 (block 2, 4, 5, 8). Sau đó, vLLM cấp phát các physical block mới (block 9-12) để lưu trữ KV cache mới từ các ứng viên mới. Bây giờ, tất cả ứng viên chia sẻ block 0, 1, 3; ứng viên 0 và 1 chia sẻ block 6, và ứng viên 2 và 3 tiếp tục chia sẻ block 7.

[Hình 9: Ví dụ beam search.]

Các hệ thống phục vụ LLM trước đây yêu cầu sao chép bộ nhớ thường xuyên của KV cache qua các ứng viên beam. Ví dụ, trong trường hợp thể hiện trong Hình 9, sau đường chấm, ứng viên 3 sẽ cần sao chép một phần lớn KV cache của ứng viên 2 để tiếp tục tạo. Chi phí sao chép bộ nhớ thường xuyên này được giảm đáng kể bằng việc chia sẻ physical block của vLLM. Trong vLLM, hầu hết các block của các ứng viên beam khác nhau có thể được chia sẻ. Cơ chế copy-on-write chỉ được áp dụng khi các token mới được tạo nằm trong một block chia sẻ cũ, như trong giải mã song song. Điều này chỉ liên quan đến việc sao chép một block dữ liệu.

Tiền tố chia sẻ. Thông thường, người dùng LLM cung cấp một mô tả (dài) về nhiệm vụ bao gồm hướng dẫn và ví dụ đầu vào và đầu ra, còn được gọi là system prompt [36]. Mô tả được nối với đầu vào nhiệm vụ thực tế để tạo thành prompt của yêu cầu. LLM tạo đầu ra dựa trên prompt đầy đủ. Hình 10 thể hiện một ví dụ. Hơn nữa, tiền tố chia sẻ có thể được tinh chỉnh thêm, thông qua prompt engineering, để cải thiện độ chính xác của các nhiệm vụ downstream [26, 27].

[Hình 10: Ví dụ prompt chia sẻ cho dịch máy. Các ví dụ được áp dụng từ [5].]

Đối với loại ứng dụng này, nhiều prompt người dùng chia sẻ một tiền tố, do đó nhà cung cấp dịch vụ LLM có thể lưu trữ trước KV cache của tiền tố để giảm tính toán dư thừa dành cho tiền tố. Trong vLLM, điều này có thể được thực hiện thuận tiện bằng cách dành riêng một tập hợp physical block cho một tập hợp tiền tố chia sẻ được xác định trước bởi nhà cung cấp dịch vụ LLM, như cách OS xử lý thư viện chia sẻ qua các quy trình. Một prompt đầu vào người dùng với tiền tố chia sẻ có thể đơn giản ánh xạ các logical block của nó đến các physical block được cache (với block cuối cùng được đánh dấu copy-on-write). Tính toán giai đoạn prompt chỉ cần thực thi trên đầu vào nhiệm vụ của người dùng.

Phương pháp giải mã hỗn hợp. Các phương pháp giải mã được thảo luận trước đây thể hiện các mẫu chia sẻ và truy cập bộ nhớ đa dạng. Tuy nhiên, vLLM tạo điều kiện cho việc xử lý đồng thời các yêu cầu với các tùy chọn giải mã khác nhau, điều mà các hệ thống hiện tại không thể thực hiện hiệu quả. Điều này là do vLLM che giấu việc chia sẻ bộ nhớ phức tạp giữa các chuỗi khác nhau thông qua một lớp ánh xạ chung dịch các logical block thành physical block. LLM và kernel thực thi của nó chỉ thấy một danh sách ID physical block cho mỗi chuỗi và không cần xử lý các mẫu chia sẻ qua các chuỗi. So với các hệ thống hiện tại, cách tiếp cận này mở rộng các cơ hội batching cho các yêu cầu với các yêu cầu lấy mẫu khác nhau, cuối cùng tăng thông lượng tổng thể của hệ thống.

4.5 Lập Lịch và Preemption
Khi lưu lượng yêu cầu vượt quá khả năng của hệ thống, vLLM phải ưu tiên một tập hợp con các yêu cầu. Trong vLLM, chúng tôi áp dụng chính sách lập lịch first-come-first-serve (FCFS) cho tất cả các yêu cầu, đảm bảo công bằng và ngăn chặn starvation. Khi vLLM cần preempt các yêu cầu, nó đảm bảo rằng các yêu cầu đến sớm nhất được phục vụ trước và các yêu cầu mới nhất được preempt trước.

Các dịch vụ LLM đối mặt với một thách thức độc đáo: các prompt đầu vào cho một LLM có thể khác nhau đáng kể về độ dài, và độ dài đầu ra kết quả không được biết trước, phụ thuộc vào cả prompt đầu vào và mô hình. Khi số lượng yêu cầu và đầu ra của chúng tăng trưởng, vLLM có thể hết các physical block của GPU để lưu trữ KV cache mới được tạo. Có hai câu hỏi cổ điển mà vLLM cần trả lời trong bối cảnh này: (1) Nên evict block nào? (2) Làm thế nào để khôi phục các block đã evict nếu cần lại? Thông thường, các chính sách eviction sử dụng heuristics để dự đoán block nào sẽ được truy cập xa nhất trong tương lai và evict block đó. Vì trong trường hợp của chúng ta, chúng ta biết rằng tất cả các block của một chuỗi được truy cập cùng nhau, chúng tôi triển khai chính sách eviction all-or-nothing, tức là hoặc evict tất cả hoặc không evict block nào của một chuỗi. Hơn nữa, nhiều chuỗi trong một yêu cầu (ví dụ: các ứng viên beam trong một yêu cầu beam search) được lập lịch gang như một nhóm chuỗi. Các chuỗi trong một nhóm chuỗi luôn được preempt hoặc lập lịch lại cùng nhau do khả năng chia sẻ bộ nhớ qua các chuỗi đó. Để trả lời câu hỏi thứ hai về cách khôi phục một block đã evict, chúng tôi xem xét hai kỹ thuật:

Swapping. Đây là kỹ thuật cổ điển được sử dụng bởi hầu hết các triển khai bộ nhớ ảo sao chép các trang đã evict vào không gian swap trên đĩa. Trong trường hợp của chúng ta, chúng tôi sao chép các block đã evict vào bộ nhớ CPU. Như thể hiện trong Hình 4, bên cạnh GPU block allocator, vLLM bao gồm một CPU block allocator để quản lý các physical block được swap vào CPU RAM. Khi vLLM cạn kiệt các physical block tự do cho các token mới, nó chọn một tập hợp chuỗi để evict và chuyển KV cache của chúng đến CPU. Khi nó preempt một chuỗi và evict các block của nó, vLLM ngừng chấp nhận các yêu cầu mới cho đến khi tất cả các chuỗi bị preempt được hoàn thành. Khi một yêu cầu hoàn thành, các block của nó được giải phóng khỏi bộ nhớ, và các block của một chuỗi bị preempt được đưa trở lại để tiếp tục xử lý chuỗi đó. Lưu ý rằng với thiết kế này, số lượng block được swap vào CPU RAM không bao giờ vượt quá số lượng tổng physical block trong GPU RAM, vì vậy không gian swap trên CPU RAM được giới hạn bởi bộ nhớ GPU được cấp phát cho KV cache.

Tính toán lại. Trong trường hợp này, chúng tôi đơn giản tính toán lại KV cache khi các chuỗi bị preempt được lập lịch lại. Lưu ý rằng độ trễ tính toán lại có thể thấp hơn đáng kể so với độ trễ ban đầu, vì các token được tạo trong quá trình giải mã có thể được nối với prompt người dùng ban đầu như một prompt mới—KV cache của chúng tại tất cả vị trí có thể được tạo trong một iteration giai đoạn prompt.

Hiệu suất của swapping và tính toán lại phụ thuộc vào băng thông giữa CPU RAM và bộ nhớ GPU và sức mạnh tính toán của GPU. Chúng tôi kiểm tra tốc độ của swapping và tính toán lại trong §7.3.

4.6 Thực Thi Phân Tán
Nhiều LLM có kích thước tham số vượt quá dung lượng của một GPU đơn [5,9]. Do đó, cần thiết phải phân chia chúng qua các GPU phân tán và thực thi chúng theo cách song song mô hình [28,63]. Điều này đòi hỏi một trình quản lý bộ nhớ có khả năng xử lý bộ nhớ phân tán. vLLM hiệu quả trong môi trường phân tán bằng cách hỗ trợ chiến lược tensor model parallelism kiểu Megatron-LM được sử dụng rộng rãi trên Transformer [47]. Chiến lược này tuân theo lịch trình thực thi SPMD (Single Program Multiple Data), trong đó các lớp tuyến tính được phân chia để thực hiện phép nhân ma trận theo khối, và các GPU liên tục đồng bộ hóa kết quả trung gian thông qua phép toán all-reduce. Cụ thể, toán tử attention được chia trên chiều attention head, mỗi quy trình SPMD chăm sóc một tập hợp con các attention head trong multi-head attention.

Chúng tôi quan sát thấy rằng ngay cả với việc thực thi song song mô hình, mỗi model shard vẫn xử lý cùng tập hợp token đầu vào, do đó yêu cầu KV Cache cho cùng các vị trí. Do đó, vLLM có một KV cache manager duy nhất trong scheduler tập trung, như trong Hình 4. Các worker GPU khác nhau chia sẻ manager, cũng như ánh xạ từ logical block đến physical block. Ánh xạ chung này cho phép các worker GPU thực thi mô hình với các physical block được cung cấp bởi scheduler cho mỗi yêu cầu đầu vào. Mặc dù mỗi worker GPU có cùng ID physical block, một worker chỉ lưu trữ một phần của KV cache cho các attention head tương ứng của nó.

Trong mỗi bước, scheduler đầu tiên chuẩn bị thông điệp với ID token đầu vào cho mỗi yêu cầu trong batch, cũng như block table cho mỗi yêu cầu. Tiếp theo, scheduler phát sóng thông điệp điều khiển này đến các worker GPU. Sau đó, các worker GPU bắt đầu thực thi mô hình với các ID token đầu vào. Trong các lớp attention, các worker GPU đọc KV cache theo block table trong thông điệp điều khiển. Trong quá trình thực thi, các worker GPU đồng bộ hóa kết quả trung gian với primitive giao tiếp all-reduce mà không cần sự điều phối của scheduler, như trong [47]. Cuối cùng, các worker GPU gửi các token được lấy mẫu của iteration này trở lại scheduler. Tóm lại, các worker GPU không cần đồng bộ hóa về quản lý bộ nhớ vì chúng chỉ cần nhận tất cả thông tin quản lý bộ nhớ ở đầu mỗi iteration giải mã cùng với các đầu vào bước.

5 Triển Khai
vLLM là một hệ thống phục vụ end-to-end với frontend FastAPI [15] và công cụ suy luận dựa trên GPU. Frontend mở rộng giao diện OpenAI API [34], cho phép người dùng tùy chỉnh các tham số lấy mẫu cho mỗi yêu cầu, như độ dài chuỗi tối đa và beam width 𝑘. Công cụ vLLM được viết bằng 8.5K dòng Python và 2K dòng mã C++/CUDA. Chúng tôi phát triển các thành phần liên quan đến điều khiển bao gồm scheduler và block manager bằng Python trong khi phát triển các kernel CUDA tùy chỉnh cho các phép toán chính như PagedAttention. Đối với model executor, chúng tôi triển khai các LLM phổ biến như GPT [5], OPT [62], và LLaMA [52] bằng PyTorch [39] và Transformers [58]. Chúng tôi sử dụng NCCL [32] để giao tiếp tensor qua các worker GPU phân tán.

[Hình 11: Phân bố độ dài đầu vào và đầu ra của tập dữ liệu (a) ShareGPT và (b) Alpaca.]

5.1 Tối Ưu Hóa Cấp Kernel
Vì PagedAttention giới thiệu các mẫu truy cập bộ nhớ không được hỗ trợ hiệu quả bởi các hệ thống hiện tại, chúng tôi phát triển một số kernel GPU để tối ưu hóa nó. (1) Fused reshape và block write. Trong mỗi lớp Transformer, KV cache mới được chia thành các block, được reshape thành bố cục bộ nhớ được tối ưu hóa cho việc đọc block, sau đó được lưu tại các vị trí được chỉ định bởi block table. Để giảm thiểu chi phí khởi chạy kernel, chúng tôi hợp nhất chúng thành một kernel duy nhất. (2) Fusing block read và attention. Chúng tôi điều chỉnh kernel attention trong FasterTransformer [31] để đọc KV cache theo block table và thực hiện các phép toán attention ngay lập tức. Để đảm bảo truy cập bộ nhớ coalesced, chúng tôi gán một GPU warp để đọc mỗi block. Hơn nữa, chúng tôi thêm hỗ trợ cho độ dài chuỗi biến đổi trong một batch yêu cầu. (3) Fused block copy. Các phép toán sao chép block, được phát hành bởi cơ chế copy-on-write, có thể hoạt động trên các block không liên tục. Điều này có thể dẫn đến nhiều lời gọi của các chuyển động dữ liệu nhỏ nếu chúng ta sử dụng API cudaMemcpyAsync. Để giảm thiểu chi phí, chúng tôi triển khai một kernel gộp các phép toán sao chép cho các block khác nhau thành một lần khởi chạy kernel duy nhất.

5.2 Hỗ Trợ Các Thuật Toán Giải Mã Khác Nhau
vLLM triển khai các thuật toán giải mã khác nhau bằng ba phương pháp chính: fork, append, và free. Phương pháp fork tạo một chuỗi mới từ một chuỗi hiện tại. Phương pháp append thêm một token mới vào chuỗi. Cuối cùng, phương pháp free xóa chuỗi. Ví dụ, trong lấy mẫu song song, vLLM tạo nhiều chuỗi đầu ra từ chuỗi đầu vào duy nhất bằng phương pháp fork. Sau đó nó thêm các token mới vào các chuỗi này trong mỗi iteration với append, và xóa các chuỗi đáp ứng điều kiện dừng bằng free. Cùng chiến lược cũng được áp dụng trong beam search và chia sẻ tiền tố bởi vLLM. Chúng tôi tin rằng các thuật toán giải mã tương lai cũng có thể được hỗ trợ bằng cách kết hợp các phương pháp này.

6 Đánh Giá
Trong phần này, chúng tôi đánh giá hiệu suất của vLLM dưới nhiều khối lượng công việc khác nhau.

--- TRANG 5 ---
[Bảng 1: Kích thước mô hình và cấu hình máy chủ.]

6.1 Thiết Lập Thí Nghiệm
Cấu hình mô hình và máy chủ. Chúng tôi sử dụng các mô hình OPT [62] với 13B, 66B, và 175B tham số và LLaMA [52] với 13B tham số cho đánh giá của chúng tôi. 13B và 66B là các kích thước phổ biến cho LLM như thể hiện trong bảng xếp hạng LLM [38], trong khi 175B là kích thước của mô hình GPT-3 nổi tiếng [5]. Đối với tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng các instance A2 với GPU NVIDIA A100 trên Google Cloud Platform. Kích thước mô hình chi tiết và cấu hình máy chủ được thể hiện trong Bảng 1.

Khối lượng công việc. Chúng tôi tổng hợp khối lượng công việc dựa trên tập dữ liệu ShareGPT [51] và Alpaca [50], chứa văn bản đầu vào và đầu ra của các dịch vụ LLM thực tế. Tập dữ liệu ShareGPT là một bộ sưu tập các cuộc trò chuyện được người dùng chia sẻ với ChatGPT [35]. Tập dữ liệu Alpaca là một tập dữ liệu hướng dẫn được tạo bởi GPT-3.5 với self-instruct [57]. Chúng tôi tokenize các tập dữ liệu và sử dụng độ dài đầu vào và đầu ra của chúng để tổng hợp các yêu cầu khách hàng. Như thể hiện trong Hình 11, tập dữ liệu ShareGPT có prompt đầu vào dài hơn 8.4× và đầu ra dài hơn 5.8× trung bình so với tập dữ liệu Alpaca, với độ biến thiên cao hơn. Vì các tập dữ liệu này không bao gồm timestamps, chúng tôi tạo thời gian đến yêu cầu bằng phân phối Poisson với các tỷ lệ yêu cầu khác nhau.

Baseline 1: FasterTransformer. FasterTransformer [31] là một công cụ suy luận phân tán được tối ưu hóa cao cho độ trễ. Vì FasterTransformer không có scheduler riêng, chúng tôi triển khai một scheduler tùy chỉnh với cơ chế batching động tương tự như các hệ thống phục vụ hiện tại như Triton [30]. Cụ thể, chúng tôi đặt kích thước batch tối đa 𝐵 càng lớn càng tốt cho mỗi thí nghiệm, theo dung lượng bộ nhớ GPU. Scheduler lấy tối đa 𝐵 số yêu cầu đến sớm nhất và gửi batch đến FasterTransformer để xử lý.

Baseline 2: Orca. Orca [60] là một hệ thống phục vụ LLM tiên tiến được tối ưu hóa cho thông lượng. Vì Orca không có sẵn công khai để sử dụng, chúng tôi triển khai phiên bản Orca của riêng mình. Chúng tôi giả định Orca sử dụng thuật toán buddy allocation để xác định địa chỉ bộ nhớ để lưu trữ KV cache. Chúng tôi triển khai ba phiên bản của Orca dựa trên mức độ nó over-reserve không gian cho đầu ra yêu cầu:

• Orca (Oracle). Chúng tôi giả định hệ thống có kiến thức về độ dài của các đầu ra sẽ thực sự được tạo cho các yêu cầu. Điều này thể hiện hiệu suất giới hạn trên của Orca, không thể đạt được trong thực tế.

• Orca (Pow2). Chúng tôi giả định hệ thống over-reserve không gian cho đầu ra tối đa 2×. Ví dụ, nếu độ dài đầu ra thực là 25, nó dành riêng 32 vị trí cho đầu ra.

• Orca (Max). Chúng tôi giả định hệ thống luôn dành riêng không gian lên đến độ dài chuỗi tối đa của mô hình, tức là 2048 token.

Các chỉ số chính. Chúng tôi tập trung vào thông lượng phục vụ. Cụ thể, sử dụng các khối lượng công việc với các tỷ lệ yêu cầu khác nhau, chúng tôi đo độ trễ chuẩn hóa của các hệ thống, trung bình của độ trễ end-to-end của mỗi yêu cầu chia cho độ dài đầu ra của nó, như trong Orca [60]. Một hệ thống phục vụ thông lượng cao nên duy trì độ trễ chuẩn hóa thấp chống lại tỷ lệ yêu cầu cao. Đối với hầu hết các thí nghiệm, chúng tôi đánh giá các hệ thống với trace 1 giờ. Như một ngoại lệ, chúng tôi sử dụng trace 15 phút cho mô hình OPT-175B do giới hạn chi phí.

6.2 Lấy Mẫu Cơ Bản
Chúng tôi đánh giá hiệu suất của vLLM với lấy mẫu cơ bản (một mẫu cho mỗi yêu cầu) trên ba mô hình và hai tập dữ liệu. Hàng đầu tiên của Hình 12 thể hiện kết quả trên tập dữ liệu ShareGPT. Các đường cong minh họa rằng khi tỷ lệ yêu cầu tăng, độ trễ ban đầu tăng với tốc độ dần dần nhưng sau đó đột nhiên bùng nổ. Điều này có thể được quy cho thực tế rằng khi tỷ lệ yêu cầu vượt quá khả năng của hệ thống phục vụ, độ dài hàng đợi tiếp tục tăng vô hạn và độ trễ của các yêu cầu cũng vậy.

Trên tập dữ liệu ShareGPT, vLLM có thể duy trì tỷ lệ yêu cầu cao hơn 1.7×–2.7× so với Orca (Oracle) và 2.7×–8× so với Orca (Max), trong khi duy trì độ trễ tương tự. Điều này là do PagedAttention của vLLM có thể quản lý hiệu quả việc sử dụng bộ nhớ và do đó cho phép batching nhiều yêu cầu hơn Orca. Ví dụ, như thể hiện trong Hình 13a, đối với OPT-13B vLLM xử lý nhiều hơn 2.2× yêu cầu cùng một lúc so với Orca (Oracle) và nhiều hơn 4.3× yêu cầu so với Orca (Max). So với FasterTransformer, vLLM có thể duy trì tỷ lệ yêu cầu cao hơn lên đến 22×, vì FasterTransformer không sử dụng cơ chế lập lịch chi tiết và quản lý bộ nhớ không hiệu quả như Orca (Max).

[Hình 12: Tạo chuỗi đơn với các mô hình OPT trên tập dữ liệu ShareGPT và Alpaca]

[Hình 13: Số lượng yêu cầu được batch trung bình khi phục vụ OPT-13B cho trace ShareGPT (2 reqs/s) và Alpaca (30 reqs/s).]

Hàng thứ hai của Hình 12 và Hình 13b thể hiện kết quả trên tập dữ liệu Alpaca, theo xu hướng tương tự như tập dữ liệu ShareGPT. Một ngoại lệ là Hình 12 (f), trong đó lợi thế của vLLM so với Orca (Oracle) và Orca (Pow2) ít rõ rệt hơn. Điều này là do cấu hình mô hình và máy chủ cho OPT-175B (Bảng 1) cho phép không gian bộ nhớ GPU lớn có sẵn để lưu trữ KV cache, trong khi tập dữ liệu Alpaca có các chuỗi ngắn. Trong thiết lập này, Orca (Oracle) và Orca (Pow2) cũng có thể batch một số lượng lớn yêu cầu bất chấp sự không hiệu quả trong quản lý bộ nhớ của chúng. Kết quả là, hiệu suất của các hệ thống trở thành bị ràng buộc bởi tính toán thay vì bị ràng buộc bởi bộ nhớ.

6.3 Lấy Mẫu Song Song và Beam Search
Chúng tôi đánh giá hiệu quả của việc chia sẻ bộ nhớ trong PagedAttention với hai phương pháp lấy mẫu phổ biến: lấy mẫu song song và beam search. Trong lấy mẫu song song, tất cả các chuỗi song song trong một yêu cầu có thể chia sẻ KV cache cho prompt. Như thể hiện trong hàng đầu tiên của Hình 14, với số lượng chuỗi lớn hơn để lấy mẫu, vLLM mang lại cải thiện nhiều hơn so với các baseline Orca. Tương tự, hàng thứ hai của Hình 14 thể hiện kết quả cho beam search với các beam width khác nhau. Vì beam search cho phép chia sẻ nhiều hơn, vLLM thể hiện lợi ích hiệu suất thậm chí lớn hơn. Cải thiện của vLLM so với Orca (Oracle) trên OPT-13B và tập dữ liệu Alpaca tăng từ 1.3× trong lấy mẫu cơ bản lên 2.3× trong beam search với width là 6.

[Hình 14: Tạo song song và beam search với OPT-13B trên tập dữ liệu Alpaca.]

[Hình 15: Lượng tiết kiệm bộ nhớ trung bình từ việc chia sẻ KV block, khi phục vụ OPT-13B cho trace Alpaca.]

Hình 15 vẽ lượng tiết kiệm bộ nhớ, được tính bằng số block chúng tôi tiết kiệm được bằng cách chia sẻ chia cho số block tổng cộng mà không có chia sẻ. Chúng tôi thể hiện 6.1% - 9.8% tiết kiệm bộ nhớ trên lấy mẫu song song và 37.6% - 55.2% trên beam search. Trong cùng các thí nghiệm với tập dữ liệu ShareGPT, chúng tôi thấy 16.2% - 30.5% tiết kiệm bộ nhớ trên lấy mẫu song song và 44.3% - 66.3% trên beam search.

6.4 Tiền Tố Chia Sẻ
Chúng tôi khám phá hiệu quả của vLLM cho trường hợp một tiền tố được chia sẻ giữa các prompt đầu vào khác nhau, như minh họa trong Hình 10. Đối với mô hình, chúng tôi sử dụng LLaMA-13B [52], là đa ngôn ngữ. Đối với khối lượng công việc, chúng tôi sử dụng tập dữ liệu dịch WMT16 [4] Tiếng Anh-sang-Tiếng Đức và tổng hợp hai tiền tố bao gồm một hướng dẫn và một vài ví dụ dịch. Tiền tố đầu tiên bao gồm một ví dụ duy nhất (tức là one-shot) trong khi tiền tố khác bao gồm 5 ví dụ (tức là few-shot). Như thể hiện trong Hình 16 (a), vLLM đạt được thông lượng cao hơn 1.67× so với Orca (Oracle) khi tiền tố one-shot được chia sẻ. Hơn nữa, khi nhiều ví dụ được chia sẻ hơn (Hình 16 (b)), vLLM đạt được thông lượng cao hơn 3.58× so với Orca (Oracle).

[Hình 16: Khối lượng công việc dịch thuật trong đó các prompt đầu vào chia sẻ một tiền tố chung. Tiền tố bao gồm (a) 1 ví dụ với 80 token hoặc (b) 5 ví dụ với 341 token.]

6.5 Chatbot
Chatbot [8,19,35] là một trong những ứng dụng quan trọng nhất của LLM. Để triển khai chatbot, chúng tôi để mô hình tạo phản hồi bằng cách nối lịch sử trò chuyện và truy vấn người dùng cuối cùng thành một prompt. Chúng tôi tổng hợp lịch sử trò chuyện và truy vấn người dùng bằng tập dữ liệu ShareGPT. Do độ dài ngữ cảnh hạn chế của mô hình OPT-13B, chúng tôi cắt prompt đến 1024 token cuối cùng và để mô hình tạo tối đa 1024 token. Chúng tôi không lưu trữ KV cache giữa các vòng trò chuyện khác nhau vì việc này sẽ chiếm không gian cho các yêu cầu khác giữa các vòng trò chuyện.

[Hình 17: Hiệu suất trên khối lượng công việc chatbot.]

Hình 17 thể hiện rằng vLLM có thể duy trì tỷ lệ yêu cầu cao hơn 2× so với ba baseline Orca. Vì tập dữ liệu ShareGPT chứa nhiều cuộc trò chuyện dài, các prompt đầu vào cho hầu hết các yêu cầu có 1024 token. Do thuật toán buddy allocation, các baseline Orca dành riêng không gian cho 1024 token cho đầu ra yêu cầu, bất kể cách chúng dự đoán độ dài đầu ra. Vì lý do này, ba baseline Orca hoạt động tương tự. Ngược lại, vLLM có thể xử lý hiệu quả các prompt dài, vì PagedAttention giải quyết vấn đề phân mảnh và dành riêng bộ nhớ.

7 Nghiên Cứu Ablation
Trong phần này, chúng tôi nghiên cứu các khía cạnh khác nhau của vLLM và đánh giá các lựa chọn thiết kế chúng tôi đưa ra với các thí nghiệm ablation.

7.1 Microbenchmark Kernel
Ánh xạ block động trong PagedAttention ảnh hưởng đến hiệu suất của các phép toán GPU liên quan đến KV cache được lưu trữ, tức là đọc/ghi block và attention. So với các hệ thống hiện tại, các kernel GPU của chúng tôi (§5) bao gồm chi phí bổ sung của việc truy cập block table, thực thi các nhánh bổ sung, và xử lý độ dài chuỗi biến đổi. Như thể hiện trong Hình 18a, điều này dẫn đến độ trễ kernel attention cao hơn 20–26%, so với triển khai FasterTransformer được tối ưu hóa cao. Chúng tôi tin rằng chi phí này nhỏ vì nó chỉ ảnh hưởng đến toán tử attention chứ không phải các toán tử khác trong mô hình, như Linear. Bất chấp chi phí, PagedAttention làm cho vLLM vượt trội đáng kể so với FasterTransformer trong hiệu suất end-to-end (§6).

[Hình 18: Các thí nghiệm ablation.]

7.2 Tác Động của Kích Thước Block
Việc lựa chọn kích thước block có thể có tác động đáng kể đến hiệu suất của vLLM. Nếu kích thước block quá nhỏ, vLLM có thể không tận dụng đầy đủ tính song song của GPU để đọc và xử lý KV cache. Nếu kích thước block quá lớn, phân mảnh bên trong tăng và xác suất chia sẻ giảm.

Trong Hình 18b, chúng tôi đánh giá hiệu suất của vLLM với các kích thước block khác nhau, sử dụng trace ShareGPT và Alpaca với lấy mẫu cơ bản dưới tỷ lệ yêu cầu cố định. Trong trace ShareGPT, kích thước block từ 16 đến 128 dẫn đến hiệu suất tốt nhất. Trong trace Alpaca, trong khi kích thước block 16 và 32 hoạt động tốt, các kích thước block lớn hơn làm giảm đáng kể hiệu suất vì các chuỗi trở nên ngắn hơn kích thước block. Trong thực tế, chúng tôi thấy rằng kích thước block 16 đủ lớn để sử dụng hiệu quả GPU và đủ nhỏ để tránh phân mảnh bên trong đáng kể trong hầu hết các khối lượng công việc. Theo đó, vLLM đặt kích thước block mặc định là 16.

7.3 So Sánh Tính Toán Lại và Swapping
vLLM hỗ trợ cả tính toán lại và swapping như các cơ chế khôi phục của nó. Để hiểu sự đánh đổi giữa hai phương pháp, chúng tôi đánh giá hiệu suất end-to-end và microbenchmark chi phí của chúng, như trình bày trong Hình 19. Kết quả của chúng tôi tiết lộ rằng swapping gây ra chi phí quá mức với kích thước block nhỏ. Điều này là do kích thước block nhỏ thường dẫn đến nhiều chuyển dữ liệu nhỏ giữa CPU và GPU, làm hạn chế băng thông PCIe hiệu quả. Ngược lại, chi phí của tính toán lại vẫn không đổi qua các kích thước block khác nhau, vì tính toán lại không sử dụng các KV block. Do đó, tính toán lại hiệu quả hơn khi kích thước block nhỏ, trong khi swapping hiệu quả hơn khi kích thước block lớn, mặc dù chi phí tính toán lại không bao giờ cao hơn 20% độ trễ của swapping. Đối với kích thước block trung bình từ 16 đến 64, hai phương pháp thể hiện hiệu suất end-to-end tương đương.

[Hình 19: (a) Chi phí của tính toán lại và swapping cho các kích thước block khác nhau. (b) Hiệu suất khi phục vụ OPT-13B với trace ShareGPT ở cùng tỷ lệ yêu cầu.]

8 Thảo Luận
Áp dụng kỹ thuật bộ nhớ ảo và phân trang cho các khối lượng công việc GPU khác. Ý tưởng về bộ nhớ ảo và phân trang hiệu quả để quản lý KV cache trong phục vụ LLM vì khối lượng công việc yêu cầu cấp phát bộ nhớ động (vì độ dài đầu ra không được biết trước) và hiệu suất của nó bị giới hạn bởi dung lượng bộ nhớ GPU. Tuy nhiên, điều này không đúng chung cho mọi khối lượng công việc GPU. Ví dụ, trong huấn luyện DNN, các hình dạng tensor thường tĩnh, và do đó cấp phát bộ nhớ có thể được tối ưu hóa trước thời gian. Đối với một ví dụ khác, trong phục vụ DNN không phải LLM, sự gia tăng hiệu quả bộ nhớ có thể không dẫn đến cải thiện hiệu suất nào vì hiệu suất chủ yếu bị ràng buộc bởi tính toán. Trong các tình huống như vậy, việc giới thiệu các kỹ thuật của vLLM có thể làm giảm hiệu suất do chi phí bổ sung của gián tiếp bộ nhớ và bộ nhớ block không liền kề. Tuy nhiên, chúng tôi sẽ rất hào hứng thấy các kỹ thuật của vLLM được áp dụng cho các khối lượng công việc khác có tính chất tương tự như phục vụ LLM.

Các tối ưu hóa cụ thể LLM trong việc áp dụng bộ nhớ ảo và phân trang. vLLM tái diễn giải và tăng cường ý tưởng về bộ nhớ ảo và phân trang bằng cách tận dụng ngữ nghĩa cụ thể ứng dụng. Một ví dụ là chính sách swap-out all-or-nothing của vLLM, khai thác thực tế rằng việc xử lý một yêu cầu yêu cầu tất cả các trạng thái token tương ứng của nó được lưu trữ trong bộ nhớ GPU. Một ví dụ khác là phương pháp tính toán lại để khôi phục các block đã evict, không khả thi trong OS. Bên cạnh đó, vLLM giảm thiểu chi phí của gián tiếp bộ nhớ trong phân trang bằng cách hợp nhất các kernel GPU cho các phép toán truy cập bộ nhớ với các phép toán khác như attention.

9 Công Trình Liên Quan
Các hệ thống phục vụ mô hình tổng quát. Phục vụ mô hình đã là một lĩnh vực nghiên cứu tích cực trong những năm gần đây, với nhiều hệ thống được đề xuất để giải quyết các khía cạnh đa dạng của việc triển khai mô hình học sâu. Clipper [11], TensorFlow Serving [33], Nexus [45], InferLine [10], và Clockwork [20] là một số hệ thống phục vụ mô hình tổng quát sớm hơn. Chúng nghiên cứu batching, caching, placement, và lập lịch để phục vụ một hoặc nhiều mô hình. Gần đây hơn, DVABatch [12] giới thiệu batching multi-entry multi-exit. REEF [21] và Shepherd [61] đề xuất preemption để phục vụ. AlpaServe [28] sử dụng song song mô hình cho statistical multiplexing. Tuy nhiên, các hệ thống tổng quát này không tính đến tính chất tự hồi quy và trạng thái token của suy luận LLM, dẫn đến bỏ lỡ các cơ hội tối ưu hóa.

Các hệ thống phục vụ chuyên biệt cho transformer. Do tầm quan trọng của kiến trúc transformer, nhiều hệ thống phục vụ chuyên biệt cho nó đã được phát triển. Các hệ thống này sử dụng tối ưu hóa kernel GPU [1, 29, 31, 56], cơ chế batching tiên tiến [14,60], song song mô hình [1, 41,60], và chia sẻ tham số [64] để phục vụ hiệu quả. Trong số chúng, Orca [60] có liên quan nhất đến cách tiếp cận của chúng tôi.

So sánh với Orca. Lập lịch cấp iteration trong Orca [60] và PagedAttention trong vLLM là các kỹ thuật bổ sung: Trong khi cả hai hệ thống đều nhằm tăng việc sử dụng GPU và do đó thông lượng của phục vụ LLM, Orca đạt được điều này bằng cách lập lịch và xen kẽ các yêu cầu để nhiều yêu cầu hơn có thể được xử lý song song, trong khi vLLM thực hiện điều này bằng cách tăng việc sử dụng bộ nhớ để các tập hợp làm việc của nhiều yêu cầu hơn vừa vào bộ nhớ. Bằng cách giảm phân mảnh bộ nhớ và cho phép chia sẻ, vLLM chạy nhiều yêu cầu hơn trong một batch song song và đạt được tăng tốc 2-4× so với Orca. Thật vậy, lập lịch và xen kẽ chi tiết của các yêu cầu như trong Orca làm cho quản lý bộ nhớ trở nên thách thức hơn, làm cho các kỹ thuật được đề xuất trong vLLM thậm chí quan trọng hơn.

Tối ưu hóa bộ nhớ. Khoảng cách ngày càng lớn giữa khả năng tính toán và dung lượng bộ nhớ của bộ tăng tốc đã khiến bộ nhớ trở thành nút cổ chai cho cả huấn luyện và suy luận. Swapping [23,42,55], tính toán lại [7,24] và sự kết hợp của chúng [40] đã được sử dụng để giảm bộ nhớ đỉnh của huấn luyện. Đáng chú ý, FlexGen [46] nghiên cứu cách swap trọng số và trạng thái token cho suy luận LLM với bộ nhớ GPU hạn chế, nhưng nó không nhắm đến các thiết lập phục vụ trực tuyến. OLLA [48] tối ưu hóa thời gian tồn tại và vị trí của tensor để giảm phân mảnh, nhưng nó không thực hiện quản lý cấp block chi tiết hoặc phục vụ trực tuyến. FlashAttention [13] áp dụng tiling và tối ưu hóa kernel để giảm bộ nhớ đỉnh của tính toán attention và giảm chi phí I/O. Bài báo này giới thiệu ý tưởng mới về quản lý bộ nhớ cấp block trong bối cảnh phục vụ trực tuyến.

10 Kết Luận
Bài báo này đề xuất PagedAttention, một thuật toán attention mới cho phép khóa và giá trị attention được lưu trữ trong bộ nhớ phân trang không liền kề, và trình bày vLLM, một hệ thống phục vụ LLM thông lượng cao với quản lý bộ nhớ hiệu quả được kích hoạt bởi PagedAttention. Lấy cảm hứng từ hệ điều hành, chúng tôi chứng minh cách các kỹ thuật đã được thiết lập, như bộ nhớ ảo và copy-on-write, có thể được điều chỉnh để quản lý hiệu quả KV cache và xử lý các thuật toán giải mã khác nhau trong phục vụ LLM. Các thí nghiệm của chúng tôi cho thấy vLLM đạt được cải thiện thông lượng 2-4× so với các hệ thống tiên tiến.

Lời Cảm Ơn
Chúng tôi muốn cảm ơn Xiaoxuan Liu, Zhifeng Chen, Yanping Huang, các reviewer SOSP ẩn danh, và shepherd của chúng tôi, Lidong Zhou, vì phản hồi sâu sắc của họ. Nghiên cứu này được hỗ trợ một phần bởi quà tặng từ Andreessen Horowitz, Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, Uber, và VMware.

Tài Liệu Tham Khảo
[Danh sách tài liệu tham khảo từ [1] đến [64] - giữ nguyên định dạng và nội dung như bản gốc]
