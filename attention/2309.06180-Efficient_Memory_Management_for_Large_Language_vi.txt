# 2309.06180.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/attention/2309.06180.pdf
# KÃ­ch thÆ°á»›c file: 1459631 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================

--- TRANG 1 ---
Quáº£n LÃ½ Bá»™ Nhá»› Hiá»‡u Quáº£ cho Viá»‡c Phá»¥c Vá»¥ MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n vá»›i PagedAttention
Woosuk Kwon1,âˆ—Zhuohan Li1,âˆ—Siyuan Zhuang1Ying Sheng1,2Lianmin Zheng1Cody Hao Yu3
Joseph E. Gonzalez1Hao Zhang4Ion Stoica1
1UC Berkeley2Stanford University3Independent Researcher4UC San Diego

TÃ³m táº¯t
Viá»‡c phá»¥c vá»¥ vá»›i thÃ´ng lÆ°á»£ng cao cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) Ä‘Ã²i há»i pháº£i gá»™p Ä‘á»§ nhiá»u yÃªu cáº§u cÃ¹ng má»™t lÃºc. Tuy nhiÃªn, cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i gáº·p khÃ³ khÄƒn vÃ¬ bá»™ nhá»› cache khÃ³a-giÃ¡ trá»‹ (KV cache) cho má»—i yÃªu cáº§u ráº¥t lá»›n vÃ  tÄƒng giáº£m Ä‘á»™ng. Khi Ä‘Æ°á»£c quáº£n lÃ½ khÃ´ng hiá»‡u quáº£, bá»™ nhá»› nÃ y cÃ³ thá»ƒ bá»‹ lÃ£ng phÃ­ Ä‘Ã¡ng ká»ƒ do phÃ¢n máº£nh vÃ  nhÃ¢n báº£n dÆ° thá»«a, háº¡n cháº¿ kÃ­ch thÆ°á»›c batch. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t PagedAttention, má»™t thuáº­t toÃ¡n attention Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« ká»¹ thuáº­t bá»™ nhá»› áº£o vÃ  phÃ¢n trang cá»• Ä‘iá»ƒn trong há»‡ Ä‘iá»u hÃ nh. Dá»±a trÃªn Ä‘Ã³, chÃºng tÃ´i xÃ¢y dá»±ng vLLM, má»™t há»‡ thá»‘ng phá»¥c vá»¥ LLM Ä‘áº¡t Ä‘Æ°á»£c (1) gáº§n nhÆ° khÃ´ng lÃ£ng phÃ­ bá»™ nhá»› KV cache vÃ  (2) chia sáº» linh hoáº¡t KV cache trong vÃ  giá»¯a cÃ¡c yÃªu cáº§u Ä‘á»ƒ giáº£m thÃªm viá»‡c sá»­ dá»¥ng bá»™ nhá»›. CÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i cho tháº¥y vLLM cáº£i thiá»‡n thÃ´ng lÆ°á»£ng cá»§a cÃ¡c LLM phá»• biáº¿n lÃªn 2-4Ã— vá»›i cÃ¹ng má»©c Ä‘á»™ Ä‘á»™ trá»… so vá»›i cÃ¡c há»‡ thá»‘ng tiÃªn tiáº¿n nhÆ° FasterTransformer vÃ  Orca. Cáº£i thiá»‡n rÃµ rá»‡t hÆ¡n vá»›i cÃ¡c chuá»—i dÃ i hÆ¡n, mÃ´ hÃ¬nh lá»›n hÆ¡n vÃ  thuáº­t toÃ¡n giáº£i mÃ£ phá»©c táº¡p hÆ¡n. MÃ£ nguá»“n cá»§a vLLM cÃ³ sáºµn cÃ´ng khai táº¡i https://github.com/vllm-project/vllm.

1 Giá»›i thiá»‡u
Sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) nhÆ° GPT [5, 37] vÃ  PaLM [9] Ä‘Ã£ cho phÃ©p cÃ¡c á»©ng dá»¥ng má»›i nhÆ° trá»£ lÃ½ láº­p trÃ¬nh [6,18] vÃ  chatbot Ä‘a nÄƒng [19,35] Ä‘ang báº¯t Ä‘áº§u tÃ¡c Ä‘á»™ng sÃ¢u sáº¯c Ä‘áº¿n cÃ´ng viá»‡c vÃ  thÃ³i quen hÃ ng ngÃ y cá»§a chÃºng ta. Nhiá»u cÃ´ng ty Ä‘Ã¡m mÃ¢y [34,44] Ä‘ang cháº¡y Ä‘ua Ä‘á»ƒ cung cáº¥p cÃ¡c á»©ng dá»¥ng nÃ y nhÆ° dá»‹ch vá»¥ Ä‘Æ°á»£c lÆ°u trá»¯. Tuy nhiÃªn, viá»‡c cháº¡y cÃ¡c á»©ng dá»¥ng nÃ y ráº¥t tá»‘n kÃ©m, Ä‘Ã²i há»i má»™t sá»‘ lÆ°á»£ng lá»›n bá»™ tÄƒng tá»‘c pháº§n cá»©ng nhÆ° GPU. Theo Æ°á»›c tÃ­nh gáº§n Ä‘Ã¢y, viá»‡c xá»­ lÃ½ má»™t yÃªu cáº§u LLM cÃ³ thá»ƒ Ä‘áº¯t hÆ¡n 10Ã— so vá»›i má»™t truy váº¥n tá»« khÃ³a truyá»n thá»‘ng [43]. Vá»›i chi phÃ­ cao nÃ y, viá»‡c tÄƒng thÃ´ng lÆ°á»£ngâ€”vÃ  do Ä‘Ã³ giáº£m chi phÃ­ cho má»—i yÃªu cáº§uâ€”cá»§a cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM Ä‘ang trá»Ÿ nÃªn quan trá»ng hÆ¡n.

[HÃ¬nh 1 mÃ´ táº£: TrÃ¡i: Bá»‘ cá»¥c bá»™ nhá»› khi phá»¥c vá»¥ má»™t LLM vá»›i 13B tham sá»‘ trÃªn NVIDIA A100. CÃ¡c tham sá»‘ (mÃ u xÃ¡m) tá»“n táº¡i trong bá»™ nhá»› GPU suá»‘t quÃ¡ trÃ¬nh phá»¥c vá»¥. Bá»™ nhá»› cho KV cache (mÃ u Ä‘á») Ä‘Æ°á»£c (há»§y)cáº¥p phÃ¡t cho má»—i yÃªu cáº§u phá»¥c vá»¥. Má»™t lÆ°á»£ng nhá» bá»™ nhá»› (mÃ u vÃ ng) Ä‘Æ°á»£c sá»­ dá»¥ng táº¡m thá»i cho activation. Pháº£i: vLLM lÃ m mÆ°á»£t Ä‘Æ°á»ng cong tÄƒng trÆ°á»Ÿng nhanh cá»§a bá»™ nhá»› KV cache tháº¥y trong cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i [31,60], dáº«n Ä‘áº¿n tÄƒng Ä‘Ã¡ng ká»ƒ thÃ´ng lÆ°á»£ng phá»¥c vá»¥.]

Cá»‘t lÃµi cá»§a LLM lÃ  má»™t mÃ´ hÃ¬nh Transformer tá»± há»“i quy [53]. MÃ´ hÃ¬nh nÃ y táº¡o ra cÃ¡c tá»« (token), tá»«ng tá»« má»™t, dá»±a trÃªn Ä‘áº§u vÃ o (prompt) vÃ  chuá»—i token Ä‘áº§u ra trÆ°á»›c Ä‘Ã³ mÃ  nÃ³ Ä‘Ã£ táº¡o ra cho Ä‘áº¿n nay. Äá»‘i vá»›i má»—i yÃªu cáº§u, quÃ¡ trÃ¬nh tá»‘n kÃ©m nÃ y Ä‘Æ°á»£c láº·p láº¡i cho Ä‘áº¿n khi mÃ´ hÃ¬nh xuáº¥t ra má»™t token káº¿t thÃºc. QuÃ¡ trÃ¬nh táº¡o tuáº§n tá»± nÃ y khiáº¿n khá»‘i lÆ°á»£ng cÃ´ng viá»‡c bá»‹ rÃ ng buá»™c bá»Ÿi bá»™ nhá»›, khÃ´ng táº­n dá»¥ng háº¿t sá»©c máº¡nh tÃ­nh toÃ¡n cá»§a GPU vÃ  háº¡n cháº¿ thÃ´ng lÆ°á»£ng phá»¥c vá»¥.

Viá»‡c cáº£i thiá»‡n thÃ´ng lÆ°á»£ng cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c báº±ng cÃ¡ch gá»™p nhiá»u yÃªu cáº§u láº¡i vá»›i nhau. Tuy nhiÃªn, Ä‘á»ƒ xá»­ lÃ½ nhiá»u yÃªu cáº§u trong má»™t batch, khÃ´ng gian bá»™ nhá»› cho má»—i yÃªu cáº§u pháº£i Ä‘Æ°á»£c quáº£n lÃ½ hiá»‡u quáº£. VÃ­ dá»¥, HÃ¬nh 1 (trÃ¡i) minh há»a phÃ¢n bá»‘ bá»™ nhá»› cho má»™t LLM 13B tham sá»‘ trÃªn GPU NVIDIA A100 vá»›i 40GB RAM. Khoáº£ng 65% bá»™ nhá»› Ä‘Æ°á»£c cáº¥p phÃ¡t cho trá»ng sá»‘ mÃ´ hÃ¬nh, váº«n tÄ©nh trong quÃ¡ trÃ¬nh phá»¥c vá»¥. Gáº§n 30% bá»™ nhá»› Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c tráº¡ng thÃ¡i Ä‘á»™ng cá»§a cÃ¡c yÃªu cáº§u. Äá»‘i vá»›i Transformer, cÃ¡c tráº¡ng thÃ¡i nÃ y bao gá»“m cÃ¡c tensor khÃ³a vÃ  giÃ¡ trá»‹ liÃªn quan Ä‘áº¿n cÆ¡ cháº¿ attention, thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  KV cache [41], Ä‘áº¡i diá»‡n cho ngá»¯ cáº£nh tá»« cÃ¡c token trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ táº¡o ra cÃ¡c token Ä‘áº§u ra má»›i theo chuá»—i. Pháº§n trÄƒm nhá» cÃ²n láº¡i cá»§a bá»™ nhá»› Ä‘Æ°á»£c sá»­ dá»¥ng cho dá»¯ liá»‡u khÃ¡c, bao gá»“m activation - cÃ¡c tensor táº¡m thá»i Ä‘Æ°á»£c táº¡o khi Ä‘Ã¡nh giÃ¡ LLM. VÃ¬ trá»ng sá»‘ mÃ´ hÃ¬nh lÃ  háº±ng sá»‘ vÃ  activation chá»‰ chiáº¿m má»™t pháº§n nhá» bá»™ nhá»› GPU, cÃ¡ch thá»©c quáº£n lÃ½ KV cache lÃ  quan trá»ng trong viá»‡c xÃ¡c Ä‘á»‹nh kÃ­ch thÆ°á»›c batch tá»‘i Ä‘a. Khi Ä‘Æ°á»£c quáº£n lÃ½ khÃ´ng hiá»‡u quáº£, bá»™ nhá»› KV cache cÃ³ thá»ƒ háº¡n cháº¿ Ä‘Ã¡ng ká»ƒ kÃ­ch thÆ°á»›c batch vÃ  do Ä‘Ã³ lÃ  thÃ´ng lÆ°á»£ng cá»§a LLM, nhÆ° minh há»a trong HÃ¬nh 1 (pháº£i).

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i quan sÃ¡t tháº¥y cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM hiá»‡n táº¡i [31,60] khÃ´ng quáº£n lÃ½ bá»™ nhá»› KV cache má»™t cÃ¡ch hiá»‡u quáº£. Äiá»u nÃ y chá»§ yáº¿u do chÃºng lÆ°u trá»¯ KV cache cá»§a má»™t yÃªu cáº§u trong khÃ´ng gian bá»™ nhá»› liá»n ká», vÃ¬ háº§u háº¿t cÃ¡c framework há»c sÃ¢u [33,39] yÃªu cáº§u tensor Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› liá»n ká». Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° tensor trong khá»‘i lÆ°á»£ng cÃ´ng viá»‡c há»c sÃ¢u truyá»n thá»‘ng, KV cache cÃ³ nhá»¯ng Ä‘áº·c Ä‘iá»ƒm Ä‘á»™c Ä‘Ã¡o: nÃ³ tÄƒng vÃ  giáº£m Ä‘á»™ng theo thá»i gian khi mÃ´ hÃ¬nh táº¡o ra token má»›i, vÃ  thá»i gian tá»“n táº¡i cÅ©ng nhÆ° Ä‘á»™ dÃ i cá»§a nÃ³ khÃ´ng Ä‘Æ°á»£c biáº¿t trÆ°á»›c. Nhá»¯ng Ä‘áº·c Ä‘iá»ƒm nÃ y khiáº¿n cÃ¡ch tiáº¿p cáº­n cá»§a cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i trá»Ÿ nÃªn khÃ´ng hiá»‡u quáº£ Ä‘Ã¡ng ká»ƒ theo hai cÃ¡ch:

Thá»© nháº¥t, cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i [31,60] bá»‹ phÃ¢n máº£nh bá»™ nhá»› bÃªn trong vÃ  bÃªn ngoÃ i. Äá»ƒ lÆ°u trá»¯ KV cache cá»§a má»™t yÃªu cáº§u trong khÃ´ng gian liá»n ká», chÃºng cáº¥p phÃ¡t trÆ°á»›c má»™t khá»‘i bá»™ nhá»› liá»n ká» vá»›i Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a yÃªu cáº§u (vÃ­ dá»¥: 2048 token). Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n phÃ¢n máº£nh bÃªn trong nghiÃªm trá»ng, vÃ¬ Ä‘á»™ dÃ i thá»±c táº¿ cá»§a yÃªu cáº§u cÃ³ thá»ƒ ngáº¯n hÆ¡n nhiá»u so vá»›i Ä‘á»™ dÃ i tá»‘i Ä‘a (vÃ­ dá»¥: HÃ¬nh 11). HÆ¡n ná»¯a, ngay cáº£ khi Ä‘á»™ dÃ i thá»±c táº¿ Ä‘Æ°á»£c biáº¿t trÆ°á»›c, viá»‡c cáº¥p phÃ¡t trÆ°á»›c váº«n khÃ´ng hiá»‡u quáº£: VÃ¬ toÃ n bá»™ khá»‘i Ä‘Æ°á»£c dÃ nh riÃªng trong suá»‘t thá»i gian tá»“n táº¡i cá»§a yÃªu cáº§u, cÃ¡c yÃªu cáº§u ngáº¯n hÆ¡n khÃ¡c khÃ´ng thá»ƒ sá»­ dá»¥ng báº¥t ká»³ pháº§n nÃ o cá»§a khá»‘i hiá»‡n táº¡i chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng. BÃªn cáº¡nh Ä‘Ã³, phÃ¢n máº£nh bá»™ nhá»› bÃªn ngoÃ i cÅ©ng cÃ³ thá»ƒ Ä‘Ã¡ng ká»ƒ, vÃ¬ kÃ­ch thÆ°á»›c cáº¥p phÃ¡t trÆ°á»›c cÃ³ thá»ƒ khÃ¡c nhau cho má»—i yÃªu cáº§u. Thá»±c váº­y, káº¿t quáº£ profiling cá»§a chÃºng tÃ´i trong HÃ¬nh 2 cho tháº¥y chá»‰ 20.4% - 38.2% bá»™ nhá»› KV cache Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÆ°u trá»¯ tráº¡ng thÃ¡i token thá»±c táº¿ trong cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i.

Thá»© hai, cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i khÃ´ng thá»ƒ khai thÃ¡c cÃ¡c cÆ¡ há»™i chia sáº» bá»™ nhá»›. CÃ¡c dá»‹ch vá»¥ LLM thÆ°á»ng sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n giáº£i mÃ£ tiÃªn tiáº¿n, nhÆ° láº¥y máº«u song song vÃ  beam search, táº¡o ra nhiá»u Ä‘áº§u ra cho má»—i yÃªu cáº§u. Trong cÃ¡c tÃ¬nh huá»‘ng nÃ y, yÃªu cáº§u bao gá»“m nhiá»u chuá»—i cÃ³ thá»ƒ chia sáº» má»™t pháº§n KV cache cá»§a chÃºng. Tuy nhiÃªn, viá»‡c chia sáº» bá»™ nhá»› khÃ´ng thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c trong cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i vÃ¬ KV cache cá»§a cÃ¡c chuá»—i Ä‘Æ°á»£c lÆ°u trá»¯ trong cÃ¡c khÃ´ng gian liá»n ká» riÃªng biá»‡t.

Äá»ƒ giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ trÃªn, chÃºng tÃ´i Ä‘á» xuáº¥t PagedAttention, má»™t thuáº­t toÃ¡n attention Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« giáº£i phÃ¡p cá»§a há»‡ Ä‘iá»u hÃ nh (OS) cho phÃ¢n máº£nh vÃ  chia sáº» bá»™ nhá»›: bá»™ nhá»› áº£o vá»›i phÃ¢n trang. PagedAttention chia KV cache cá»§a yÃªu cáº§u thÃ nh cÃ¡c khá»‘i, má»—i khá»‘i cÃ³ thá»ƒ chá»©a khÃ³a vÃ  giÃ¡ trá»‹ attention cá»§a má»™t sá»‘ lÆ°á»£ng token cá»‘ Ä‘á»‹nh. Trong PagedAttention, cÃ¡c khá»‘i cho KV cache khÃ´ng nháº¥t thiáº¿t pháº£i Ä‘Æ°á»£c lÆ°u trá»¯ trong khÃ´ng gian liá»n ká». Do Ä‘Ã³, chÃºng ta cÃ³ thá»ƒ quáº£n lÃ½ KV cache má»™t cÃ¡ch linh hoáº¡t hÆ¡n nhÆ° trong bá»™ nhá»› áº£o cá»§a OS: cÃ³ thá»ƒ nghÄ© vá» cÃ¡c khá»‘i nhÆ° cÃ¡c trang, token nhÆ° byte, vÃ  yÃªu cáº§u nhÆ° quy trÃ¬nh. Thiáº¿t káº¿ nÃ y giáº£m thiá»ƒu phÃ¢n máº£nh bÃªn trong báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c khá»‘i tÆ°Æ¡ng Ä‘á»‘i nhá» vÃ  cáº¥p phÃ¡t chÃºng theo yÃªu cáº§u. HÆ¡n ná»¯a, nÃ³ loáº¡i bá» phÃ¢n máº£nh bÃªn ngoÃ i vÃ¬ táº¥t cáº£ cÃ¡c khá»‘i cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c. Cuá»‘i cÃ¹ng, nÃ³ cho phÃ©p chia sáº» bá»™ nhá»› á»Ÿ má»©c Ä‘á»™ chi tiáº¿t cá»§a má»™t khá»‘i, giá»¯a cÃ¡c chuá»—i khÃ¡c nhau liÃªn quan Ä‘áº¿n cÃ¹ng má»™t yÃªu cáº§u hoáº·c tháº­m chÃ­ giá»¯a cÃ¡c yÃªu cáº§u khÃ¡c nhau.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i xÃ¢y dá»±ng vLLM, má»™t cÃ´ng cá»¥ phá»¥c vá»¥ LLM phÃ¢n tÃ¡n thÃ´ng lÆ°á»£ng cao dá»±a trÃªn PagedAttention Ä‘áº¡t Ä‘Æ°á»£c gáº§n nhÆ° khÃ´ng lÃ£ng phÃ­ bá»™ nhá»› KV cache. vLLM sá»­ dá»¥ng quáº£n lÃ½ bá»™ nhá»› cáº¥p khá»‘i vÃ  láº­p lá»‹ch yÃªu cáº§u preemptive Ä‘Æ°á»£c Ä‘á»“ng thiáº¿t káº¿ vá»›i PagedAttention. vLLM há»— trá»£ cÃ¡c LLM phá»• biáº¿n nhÆ° GPT [5], OPT [62], vÃ  LLaMA [52] vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau, bao gá»“m nhá»¯ng mÃ´ hÃ¬nh vÆ°á»£t quÃ¡ dung lÆ°á»£ng bá»™ nhá»› cá»§a má»™t GPU Ä‘Æ¡n. CÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i trÃªn nhiá»u mÃ´ hÃ¬nh vÃ  khá»‘i lÆ°á»£ng cÃ´ng viá»‡c khÃ¡c nhau cho tháº¥y vLLM cáº£i thiá»‡n thÃ´ng lÆ°á»£ng phá»¥c vá»¥ LLM lÃªn 2-4Ã— so vá»›i cÃ¡c há»‡ thá»‘ng tiÃªn tiáº¿n [31,60], mÃ  khÃ´ng áº£nh hÆ°á»Ÿng gÃ¬ Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh. Cáº£i thiá»‡n rÃµ rá»‡t hÆ¡n vá»›i cÃ¡c chuá»—i dÃ i hÆ¡n, mÃ´ hÃ¬nh lá»›n hÆ¡n vÃ  thuáº­t toÃ¡n giáº£i mÃ£ phá»©c táº¡p hÆ¡n (Â§4.3).

TÃ³m láº¡i, chÃºng tÃ´i Ä‘Ã³ng gÃ³p nhÆ° sau:
â€¢ ChÃºng tÃ´i xÃ¡c Ä‘á»‹nh cÃ¡c thÃ¡ch thá»©c trong cáº¥p phÃ¡t bá»™ nhá»› khi phá»¥c vá»¥ LLM vÃ  Ä‘á»‹nh lÆ°á»£ng tÃ¡c Ä‘á»™ng cá»§a chÃºng lÃªn hiá»‡u suáº¥t phá»¥c vá»¥.
â€¢ ChÃºng tÃ´i Ä‘á» xuáº¥t PagedAttention, má»™t thuáº­t toÃ¡n attention hoáº¡t Ä‘á»™ng trÃªn KV cache Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› phÃ¢n trang khÃ´ng liá»n ká», Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« bá»™ nhá»› áº£o vÃ  phÃ¢n trang trong OS.
â€¢ ChÃºng tÃ´i thiáº¿t káº¿ vÃ  triá»ƒn khai vLLM, má»™t cÃ´ng cá»¥ phá»¥c vá»¥ LLM phÃ¢n tÃ¡n Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn PagedAttention.
â€¢ ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ vLLM trÃªn nhiá»u tÃ¬nh huá»‘ng khÃ¡c nhau vÃ  chá»©ng minh ráº±ng nÃ³ vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c giáº£i phÃ¡p tiÃªn tiáº¿n trÆ°á»›c Ä‘Ã¢y nhÆ° FasterTransformer [31] vÃ  Orca [60].

2 Bá»‘i cáº£nh
Trong pháº§n nÃ y, chÃºng tÃ´i mÃ´ táº£ quy trÃ¬nh táº¡o vÃ  phá»¥c vá»¥ cá»§a cÃ¡c LLM Ä‘iá»ƒn hÃ¬nh vÃ  láº­p lá»‹ch cáº¥p iteration Ä‘Æ°á»£c sá»­ dá»¥ng trong phá»¥c vá»¥ LLM.

--- TRANG 2 ---
[HÃ¬nh 2: Tá»· lá»‡ pháº§n trÄƒm trung bÃ¬nh lÃ£ng phÃ­ bá»™ nhá»› trong cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM khÃ¡c nhau trong thÃ­ nghiá»‡m á»Ÿ Â§6.2.]

2.1 MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n Dá»±a TrÃªn Transformer
Nhiá»‡m vá»¥ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ lÃ  mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cá»§a má»™t danh sÃ¡ch token (ğ‘¥1,...,ğ‘¥ğ‘›). VÃ¬ ngÃ´n ngá»¯ cÃ³ thá»© tá»± tuáº§n tá»± tá»± nhiÃªn, thÃ´ng thÆ°á»ng ta phÃ¢n tÃ­ch xÃ¡c suáº¥t káº¿t há»£p trÃªn toÃ n bá»™ chuá»—i thÃ nh tÃ­ch cá»§a cÃ¡c xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n (gá»i lÃ  phÃ¢n tÃ¡ch tá»± há»“i quy [3]):

ğ‘ƒ(ğ‘¥) = ğ‘ƒ(ğ‘¥1) Â· ğ‘ƒ(ğ‘¥2|ğ‘¥1) Â·Â·Â· ğ‘ƒ(ğ‘¥ğ‘›|ğ‘¥1,...,ğ‘¥ğ‘›âˆ’1). (1)

Transformer [53] Ä‘Ã£ trá»Ÿ thÃ nh kiáº¿n trÃºc tiÃªu chuáº©n de facto Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t trÃªn á»Ÿ quy mÃ´ lá»›n. ThÃ nh pháº§n quan trá»ng nháº¥t cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»±a trÃªn Transformer lÃ  cÃ¡c lá»›p self-attention. Äá»‘i vá»›i má»™t chuá»—i tráº¡ng thÃ¡i áº©n Ä‘áº§u vÃ o (ğ‘¥1,...,ğ‘¥ğ‘›) âˆˆ â„ğ‘›Ã—ğ‘‘, má»™t lá»›p self-attention Ä‘áº§u tiÃªn Ã¡p dá»¥ng biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh táº¡i má»—i vá»‹ trÃ­ ğ‘– Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c vector query, key vÃ  value:

ğ‘ğ‘– = ğ‘Šğ‘ğ‘¥ğ‘–, ğ‘˜ğ‘– = ğ‘Šğ‘˜ğ‘¥ğ‘–, ğ‘£ğ‘– = ğ‘Šğ‘£ğ‘¥ğ‘–. (2)

Sau Ä‘Ã³, lá»›p self-attention tÃ­nh toÃ¡n Ä‘iá»ƒm attention ğ‘ğ‘–ğ‘— báº±ng cÃ¡ch nhÃ¢n vector query táº¡i má»™t vá»‹ trÃ­ vá»›i táº¥t cáº£ cÃ¡c vector key trÆ°á»›c nÃ³ vÃ  tÃ­nh Ä‘áº§u ra ğ‘œğ‘– nhÆ° trung bÃ¬nh cÃ³ trá»ng sá»‘ trÃªn cÃ¡c vector value:

ğ‘ğ‘–ğ‘— = exp(ğ‘âŠ¤ğ‘–ğ‘˜ğ‘—/âˆšğ‘‘) / Î£ğ‘–ğ‘¡=1exp(ğ‘âŠ¤ğ‘–ğ‘˜ğ‘¡/âˆšğ‘‘), ğ‘œğ‘– = Î£ğ‘–ğ‘—=1ğ‘ğ‘–ğ‘—ğ‘£ğ‘—. (3)

BÃªn cáº¡nh tÃ­nh toÃ¡n trong Eq. 4, táº¥t cáº£ cÃ¡c thÃ nh pháº§n khÃ¡c trong mÃ´ hÃ¬nh Transformer, bao gá»“m lá»›p embedding, lá»›p feed-forward, layer normalization [2], káº¿t ná»‘i residual [22], tÃ­nh toÃ¡n logit Ä‘áº§u ra, vÃ  biáº¿n Ä‘á»•i query, key, value trong Eq. 2, Ä‘á»u Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»™c láº­p theo tá»«ng vá»‹ trÃ­ dÆ°á»›i dáº¡ng ğ‘¦ğ‘– = ğ‘“(ğ‘¥ğ‘–).

2.2 Dá»‹ch Vá»¥ LLM & Táº¡o Tá»± Há»“i Quy
Sau khi Ä‘Æ°á»£c huáº¥n luyá»‡n, LLM thÆ°á»ng Ä‘Æ°á»£c triá»ƒn khai nhÆ° má»™t dá»‹ch vá»¥ táº¡o cÃ³ Ä‘iá»u kiá»‡n (vÃ­ dá»¥: completion API [34] hoáº·c chatbot [19,35]). Má»™t yÃªu cáº§u Ä‘áº¿n dá»‹ch vá»¥ LLM cung cáº¥p má»™t danh sÃ¡ch token prompt Ä‘áº§u vÃ o (ğ‘¥1,...,ğ‘¥ğ‘›), vÃ  dá»‹ch vá»¥ LLM táº¡o ra má»™t danh sÃ¡ch token Ä‘áº§u ra (ğ‘¥ğ‘›+1,...,ğ‘¥ğ‘›+ğ‘‡) theo Eq. 1. ChÃºng tÃ´i gá»i sá»± ná»‘i cá»§a danh sÃ¡ch prompt vÃ  Ä‘áº§u ra lÃ  chuá»—i.

Do phÃ¢n tÃ¡ch trong Eq. 1, LLM chá»‰ cÃ³ thá»ƒ láº¥y máº«u vÃ  táº¡o token má»›i tá»«ng cÃ¡i má»™t, vÃ  quÃ¡ trÃ¬nh táº¡o má»—i token má»›i phá»¥ thuá»™c vÃ o táº¥t cáº£ cÃ¡c token trÆ°á»›c Ä‘Ã³ trong chuá»—i Ä‘Ã³, cá»¥ thá»ƒ lÃ  cÃ¡c vector key vÃ  value cá»§a chÃºng. Trong quÃ¡ trÃ¬nh táº¡o tuáº§n tá»± nÃ y, cÃ¡c vector key vÃ  value cá»§a cÃ¡c token hiá»‡n táº¡i thÆ°á»ng Ä‘Æ°á»£c cache Ä‘á»ƒ táº¡o cÃ¡c token tÆ°Æ¡ng lai, Ä‘Æ°á»£c gá»i lÃ  KV cache. LÆ°u Ã½ ráº±ng KV cache cá»§a má»™t token phá»¥ thuá»™c vÃ o táº¥t cáº£ cÃ¡c token trÆ°á»›c nÃ³. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  KV cache cá»§a cÃ¹ng má»™t token xuáº¥t hiá»‡n á»Ÿ cÃ¡c vá»‹ trÃ­ khÃ¡c nhau trong má»™t chuá»—i sáº½ khÃ¡c nhau.

Cho má»™t prompt yÃªu cáº§u, tÃ­nh toÃ¡n táº¡o trong dá»‹ch vá»¥ LLM cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ¡ch thÃ nh hai giai Ä‘oáº¡n:

Giai Ä‘oáº¡n prompt nháº­n toÃ n bá»™ prompt ngÆ°á»i dÃ¹ng (ğ‘¥1,...,ğ‘¥ğ‘›) lÃ m Ä‘áº§u vÃ o vÃ  tÃ­nh xÃ¡c suáº¥t cá»§a token má»›i Ä‘áº§u tiÃªn ğ‘ƒ(ğ‘¥ğ‘›+1|ğ‘¥1,...,ğ‘¥ğ‘›). Trong quÃ¡ trÃ¬nh nÃ y, cÅ©ng táº¡o ra cÃ¡c vector key ğ‘˜1,...,ğ‘˜ğ‘› vÃ  vector value ğ‘£1,...,ğ‘£ğ‘›. VÃ¬ cÃ¡c token prompt ğ‘¥1,...,ğ‘¥ğ‘› Ä‘á»u Ä‘Ã£ biáº¿t, tÃ­nh toÃ¡n cá»§a giai Ä‘oáº¡n prompt cÃ³ thá»ƒ Ä‘Æ°á»£c song song hÃ³a báº±ng cÃ¡c phÃ©p toÃ¡n nhÃ¢n ma tráº­n-ma tráº­n. Do Ä‘Ã³, giai Ä‘oáº¡n nÃ y cÃ³ thá»ƒ sá»­ dá»¥ng hiá»‡u quáº£ tÃ­nh song song vá»‘n cÃ³ trong GPU.

Giai Ä‘oáº¡n táº¡o tá»± há»“i quy táº¡o ra cÃ¡c token má»›i cÃ²n láº¡i má»™t cÃ¡ch tuáº§n tá»±. Táº¡i iteration ğ‘¡, mÃ´ hÃ¬nh nháº­n má»™t token ğ‘¥ğ‘›+ğ‘¡ lÃ m Ä‘áº§u vÃ o vÃ  tÃ­nh xÃ¡c suáº¥t ğ‘ƒ(ğ‘¥ğ‘›+ğ‘¡+1|ğ‘¥1,...,ğ‘¥ğ‘›+ğ‘¡) vá»›i cÃ¡c vector key ğ‘˜1,...,ğ‘˜ğ‘›+ğ‘¡ vÃ  vector value ğ‘£1,...,ğ‘£ğ‘›+ğ‘¡. LÆ°u Ã½ ráº±ng cÃ¡c vector key vÃ  value táº¡i vá»‹ trÃ­ 1 Ä‘áº¿n ğ‘›+ğ‘¡âˆ’1 Ä‘Æ°á»£c cache á»Ÿ cÃ¡c iteration trÆ°á»›c, chá»‰ vector key vÃ  value má»›i ğ‘˜ğ‘›+ğ‘¡ vÃ  ğ‘£ğ‘›+ğ‘¡ Ä‘Æ°á»£c tÃ­nh toÃ¡n táº¡i iteration nÃ y. Giai Ä‘oáº¡n nÃ y hoÃ n thÃ nh khi chuá»—i Ä‘áº¡t Ä‘á»™ dÃ i tá»‘i Ä‘a (Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh bá»Ÿi ngÆ°á»i dÃ¹ng hoáº·c giá»›i háº¡n bá»Ÿi LLM) hoáº·c khi má»™t token káº¿t thÃºc chuá»—i (<eos>) Ä‘Æ°á»£c phÃ¡t ra. TÃ­nh toÃ¡n táº¡i cÃ¡c iteration khÃ¡c nhau khÃ´ng thá»ƒ Ä‘Æ°á»£c song song hÃ³a do phá»¥ thuá»™c dá»¯ liá»‡u vÃ  thÆ°á»ng sá»­ dá»¥ng phÃ©p nhÃ¢n ma tráº­n-vector, Ã­t hiá»‡u quáº£ hÆ¡n. Káº¿t quáº£ lÃ , giai Ä‘oáº¡n nÃ y sá»­ dá»¥ng khÃ´ng Ä‘áº§y Ä‘á»§ tÃ­nh toÃ¡n GPU vÃ  trá»Ÿ thÃ nh bá»‹ rÃ ng buá»™c bá»Ÿi bá»™ nhá»›, chá»‹u trÃ¡ch nhiá»‡m cho pháº§n lá»›n Ä‘á»™ trá»… cá»§a má»™t yÃªu cáº§u Ä‘Æ¡n.

2.3 Ká»¹ Thuáº­t Batching cho LLM
Viá»‡c sá»­ dá»¥ng tÃ­nh toÃ¡n trong phá»¥c vá»¥ LLM cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n báº±ng cÃ¡ch gá»™p nhiá»u yÃªu cáº§u. Bá»Ÿi vÃ¬ cÃ¡c yÃªu cáº§u chia sáº» cÃ¹ng trá»ng sá»‘ mÃ´ hÃ¬nh, chi phÃ­ di chuyá»ƒn trá»ng sá»‘ Ä‘Æ°á»£c phÃ¢n bá»• qua cÃ¡c yÃªu cáº§u trong má»™t batch, vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c vÆ°á»£t qua bá»Ÿi chi phÃ­ tÃ­nh toÃ¡n khi kÃ­ch thÆ°á»›c batch Ä‘á»§ lá»›n. Tuy nhiÃªn, viá»‡c gá»™p cÃ¡c yÃªu cáº§u cho dá»‹ch vá»¥ LLM khÃ´ng pháº£i lÃ  Ä‘iá»u Ä‘Æ¡n giáº£n vÃ¬ hai lÃ½ do. Thá»© nháº¥t, cÃ¡c yÃªu cáº§u cÃ³ thá»ƒ Ä‘áº¿n vÃ o nhá»¯ng thá»i Ä‘iá»ƒm khÃ¡c nhau. Chiáº¿n lÆ°á»£c batching ngÃ¢y thÆ¡ sáº½ hoáº·c lÃ  lÃ m cho cÃ¡c yÃªu cáº§u Ä‘áº¿n sá»›m pháº£i chá» cÃ¡c yÃªu cáº§u Ä‘áº¿n muá»™n hoáº·c trÃ¬ hoÃ£n cÃ¡c yÃªu cáº§u Ä‘áº¿n cho Ä‘áº¿n khi cÃ¡c yÃªu cáº§u trÆ°á»›c Ä‘Ã³ hoÃ n thÃ nh, dáº«n Ä‘áº¿n Ä‘á»™ trá»… xáº¿p hÃ ng Ä‘Ã¡ng ká»ƒ. Thá»© hai, cÃ¡c yÃªu cáº§u cÃ³ thá»ƒ cÃ³ Ä‘á»™ dÃ i Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra ráº¥t khÃ¡c nhau (HÃ¬nh 11). Ká»¹ thuáº­t batching Ä‘Æ¡n giáº£n sáº½ pad Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a cÃ¡c yÃªu cáº§u Ä‘á»ƒ cÃ¢n báº±ng Ä‘á»™ dÃ i cá»§a chÃºng, lÃ£ng phÃ­ tÃ­nh toÃ¡n vÃ  bá»™ nhá»› GPU.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, cÃ¡c cÆ¡ cháº¿ batching chi tiáº¿t, nhÆ° cellular batching [16] vÃ  láº­p lá»‹ch cáº¥p iteration [60], Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t. KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng lÃ m viá»‡c á»Ÿ cáº¥p yÃªu cáº§u, cÃ¡c ká»¹ thuáº­t nÃ y hoáº¡t Ä‘á»™ng á»Ÿ cáº¥p iteration. Sau má»—i iteration, cÃ¡c yÃªu cáº§u Ä‘Ã£ hoÃ n thÃ nh Ä‘Æ°á»£c loáº¡i bá» khá»i batch, vÃ  cÃ¡c yÃªu cáº§u má»›i Ä‘Æ°á»£c thÃªm vÃ o. Do Ä‘Ã³, má»™t yÃªu cáº§u má»›i cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ sau khi chá» má»™t iteration duy nháº¥t, khÃ´ng pháº£i chá» toÃ n bá»™ batch hoÃ n thÃ nh. HÆ¡n ná»¯a, vá»›i cÃ¡c kernel GPU Ä‘áº·c biá»‡t, cÃ¡c ká»¹ thuáº­t nÃ y loáº¡i bá» nhu cáº§u pad Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra. Báº±ng cÃ¡ch giáº£m Ä‘á»™ trá»… xáº¿p hÃ ng vÃ  sá»± khÃ´ng hiá»‡u quáº£ tá»« padding, cÃ¡c cÆ¡ cháº¿ batching chi tiáº¿t tÄƒng Ä‘Ã¡ng ká»ƒ thÃ´ng lÆ°á»£ng cá»§a phá»¥c vá»¥ LLM.

--- TRANG 3 ---
[HÃ¬nh 3: Quáº£n lÃ½ bá»™ nhá»› KV cache trong cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i. Ba loáº¡i lÃ£ng phÃ­ bá»™ nhá»› - dá»± trá»¯, phÃ¢n máº£nh bÃªn trong vÃ  phÃ¢n máº£nh bÃªn ngoÃ i - tá»“n táº¡i vÃ  ngÄƒn cÃ¡c yÃªu cáº§u khÃ¡c vá»«a vÃ o bá»™ nhá»›. Token trong má»—i slot bá»™ nhá»› Ä‘áº¡i diá»‡n cho KV cache cá»§a nÃ³. LÆ°u Ã½ cÃ¡c token giá»‘ng nhau cÃ³ thá»ƒ cÃ³ KV cache khÃ¡c nhau khi á»Ÿ cÃ¡c vá»‹ trÃ­ khÃ¡c nhau.]

3 ThÃ¡ch Thá»©c Bá»™ Nhá»› trong Phá»¥c Vá»¥ LLM
Máº·c dÃ¹ batching chi tiáº¿t giáº£m lÃ£ng phÃ­ tÃ­nh toÃ¡n vÃ  cho phÃ©p cÃ¡c yÃªu cáº§u Ä‘Æ°á»£c gá»™p má»™t cÃ¡ch linh hoáº¡t hÆ¡n, sá»‘ lÆ°á»£ng yÃªu cáº§u cÃ³ thá»ƒ Ä‘Æ°á»£c gá»™p cÃ¹ng nhau váº«n bá»‹ háº¡n cháº¿ bá»Ÿi dung lÆ°á»£ng bá»™ nhá»› GPU, Ä‘áº·c biá»‡t lÃ  khÃ´ng gian Ä‘Æ°á»£c cáº¥p phÃ¡t Ä‘á»ƒ lÆ°u trá»¯ KV cache. NÃ³i cÃ¡ch khÃ¡c, thÃ´ng lÆ°á»£ng cá»§a há»‡ thá»‘ng phá»¥c vá»¥ bá»‹ rÃ ng buá»™c bá»Ÿi bá»™ nhá»›. VÆ°á»£t qua rÃ ng buá»™c bá»™ nhá»› nÃ y Ä‘Ã²i há»i giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c sau trong quáº£n lÃ½ bá»™ nhá»›:

KV cache lá»›n. KÃ­ch thÆ°á»›c KV cache tÄƒng nhanh vá»›i sá»‘ lÆ°á»£ng yÃªu cáº§u. VÃ­ dá»¥, Ä‘á»‘i vá»›i mÃ´ hÃ¬nh OPT 13B tham sá»‘ [62], KV cache cá»§a má»™t token Ä‘Æ¡n Ä‘Ã²i há»i 800 KB khÃ´ng gian, Ä‘Æ°á»£c tÃ­nh nhÆ° 2(vector key vÃ  value) Ã— 5120 (kÃ­ch thÆ°á»›c tráº¡ng thÃ¡i áº©n) Ã— 40(sá»‘ lá»›p) Ã— 2(byte cho má»—i FP16). VÃ¬ OPT cÃ³ thá»ƒ táº¡o chuá»—i lÃªn Ä‘áº¿n 2048 token, bá»™ nhá»› cáº§n thiáº¿t Ä‘á»ƒ lÆ°u trá»¯ KV cache cá»§a má»™t yÃªu cáº§u cÃ³ thá»ƒ lÃªn Ä‘áº¿n 1.6 GB. GPU Ä‘á»“ng thá»i cÃ³ dung lÆ°á»£ng bá»™ nhá»› hÃ ng chá»¥c GB. Ngay cáº£ khi táº¥t cáº£ bá»™ nhá»› cÃ³ sáºµn Ä‘Æ°á»£c cáº¥p phÃ¡t cho KV cache, chá»‰ vÃ i chá»¥c yÃªu cáº§u cÃ³ thá»ƒ Ä‘Æ°á»£c chá»©a. HÆ¡n ná»¯a, quáº£n lÃ½ bá»™ nhá»› khÃ´ng hiá»‡u quáº£ cÃ³ thá»ƒ giáº£m thÃªm kÃ­ch thÆ°á»›c batch, nhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 2. ThÃªm vÃ o Ä‘Ã³, vá»›i xu hÆ°á»›ng hiá»‡n táº¡i, tá»‘c Ä‘á»™ tÃ­nh toÃ¡n cá»§a GPU tÄƒng nhanh hÆ¡n dung lÆ°á»£ng bá»™ nhá»› [17]. VÃ­ dá»¥, tá»« NVIDIA A100 Ä‘áº¿n H100, FLOPS tÄƒng hÆ¡n 2x, nhÆ°ng bá»™ nhá»› GPU váº«n á»Ÿ má»©c tá»‘i Ä‘a 80GB. Do Ä‘Ã³, chÃºng tÃ´i tin ráº±ng bá»™ nhá»› sáº½ trá»Ÿ thÃ nh má»™t nÃºt cá»• chai ngÃ y cÃ ng quan trá»ng.

Thuáº­t toÃ¡n giáº£i mÃ£ phá»©c táº¡p. CÃ¡c dá»‹ch vá»¥ LLM cung cáº¥p má»™t loáº¡t thuáº­t toÃ¡n giáº£i mÃ£ Ä‘á»ƒ ngÆ°á»i dÃ¹ng lá»±a chá»n, má»—i thuáº­t toÃ¡n cÃ³ nhá»¯ng Ã½ nghÄ©a khÃ¡c nhau vá» Ä‘á»™ phá»©c táº¡p quáº£n lÃ½ bá»™ nhá»›. VÃ­ dá»¥, khi ngÆ°á»i dÃ¹ng yÃªu cáº§u nhiá»u máº«u ngáº«u nhiÃªn tá»« má»™t prompt Ä‘áº§u vÃ o duy nháº¥t, má»™t trÆ°á»ng há»£p sá»­ dá»¥ng Ä‘iá»ƒn hÃ¬nh trong gá»£i Ã½ chÆ°Æ¡ng trÃ¬nh [18], KV cache cá»§a pháº§n prompt, chiáº¿m 12% tá»•ng bá»™ nhá»› KV cache trong thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i (Â§6.3), cÃ³ thá»ƒ Ä‘Æ°á»£c chia sáº» Ä‘á»ƒ giáº£m thiá»ƒu viá»‡c sá»­ dá»¥ng bá»™ nhá»›. Máº·t khÃ¡c, KV cache trong giai Ä‘oáº¡n táº¡o tá»± há»“i quy nÃªn khÃ´ng Ä‘Æ°á»£c chia sáº» do káº¿t quáº£ máº«u khÃ¡c nhau vÃ  sá»± phá»¥ thuá»™c cá»§a chÃºng vÃ o ngá»¯ cáº£nh vÃ  vá»‹ trÃ­. Má»©c Ä‘á»™ chia sáº» KV cache phá»¥ thuá»™c vÃ o thuáº­t toÃ¡n giáº£i mÃ£ cá»¥ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng. Trong cÃ¡c thuáº­t toÃ¡n phá»©c táº¡p hÆ¡n nhÆ° beam search [49], cÃ¡c beam yÃªu cáº§u khÃ¡c nhau cÃ³ thá»ƒ chia sáº» cÃ¡c pháº§n lá»›n hÆ¡n (lÃªn Ä‘áº¿n 55% tiáº¿t kiá»‡m bá»™ nhá»›, xem Â§6.3) KV cache cá»§a chÃºng, vÃ  máº«u chia sáº» phÃ¡t triá»ƒn khi quÃ¡ trÃ¬nh giáº£i mÃ£ tiáº¿n triá»ƒn.

Láº­p lá»‹ch cho Ä‘á»™ dÃ i Ä‘áº§u vÃ o & Ä‘áº§u ra chÆ°a biáº¿t. CÃ¡c yÃªu cáº§u Ä‘áº¿n dá»‹ch vá»¥ LLM thá»ƒ hiá»‡n sá»± biáº¿n Ä‘á»•i trong Ä‘á»™ dÃ i Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a chÃºng. Äiá»u nÃ y Ä‘Ã²i há»i há»‡ thá»‘ng quáº£n lÃ½ bá»™ nhá»› pháº£i chá»©a Ä‘Æ°á»£c má»™t loáº¡t rá»™ng Ä‘á»™ dÃ i prompt. ThÃªm vÃ o Ä‘Ã³, khi Ä‘á»™ dÃ i Ä‘áº§u ra cá»§a má»™t yÃªu cáº§u tÄƒng trong quÃ¡ trÃ¬nh giáº£i mÃ£, bá»™ nhá»› cáº§n thiáº¿t cho KV cache cá»§a nÃ³ cÅ©ng má»Ÿ rá»™ng vÃ  cÃ³ thá»ƒ cáº¡n kiá»‡t bá»™ nhá»› cÃ³ sáºµn cho cÃ¡c yÃªu cáº§u Ä‘áº¿n hoáº·c táº¡o Ä‘ang diá»…n ra cho cÃ¡c prompt hiá»‡n táº¡i. Há»‡ thá»‘ng cáº§n Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh láº­p lá»‹ch, nhÆ° xÃ³a hoáº·c swap ra KV cache cá»§a má»™t sá»‘ yÃªu cáº§u khá»i bá»™ nhá»› GPU.

3.1 Quáº£n LÃ½ Bá»™ Nhá»› trong CÃ¡c Há»‡ Thá»‘ng Hiá»‡n Táº¡i
VÃ¬ háº§u háº¿t cÃ¡c toÃ¡n tá»­ trong cÃ¡c framework há»c sÃ¢u hiá»‡n táº¡i [33,39] Ä‘Ã²i há»i tensor Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› liá»n ká», cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM trÆ°á»›c Ä‘Ã¢y [31,60] cÅ©ng lÆ°u trá»¯ KV cache cá»§a má»™t yÃªu cáº§u nhÆ° má»™t tensor liá»n ká» qua cÃ¡c vá»‹ trÃ­ khÃ¡c nhau. Do Ä‘á»™ dÃ i Ä‘áº§u ra khÃ´ng thá»ƒ Ä‘oÃ¡n trÆ°á»›c tá»« LLM, chÃºng cáº¥p phÃ¡t tÄ©nh má»™t khá»‘i bá»™ nhá»› cho má»™t yÃªu cáº§u dá»±a trÃªn Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a cÃ³ thá»ƒ cá»§a yÃªu cáº§u, báº¥t ká»ƒ Ä‘á»™ dÃ i Ä‘áº§u vÃ o thá»±c táº¿ hoáº·c Ä‘á»™ dÃ i Ä‘áº§u ra cuá»‘i cÃ¹ng cá»§a yÃªu cáº§u.

HÃ¬nh 3 minh há»a hai yÃªu cáº§u: yÃªu cáº§u A vá»›i Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a cÃ³ thá»ƒ lÃ  2048 vÃ  yÃªu cáº§u B vá»›i tá»‘i Ä‘a 512. SÆ¡ Ä‘á»“ cáº¥p phÃ¡t khá»‘i trÆ°á»›c trong cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i cÃ³ ba nguá»“n chÃ­nh cá»§a lÃ£ng phÃ­ bá»™ nhá»›: slot dá»± trá»¯ cho cÃ¡c token tÆ°Æ¡ng lai, phÃ¢n máº£nh bÃªn trong do cáº¥p phÃ¡t quÃ¡ má»©c cho Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a tiá»m nÄƒng, vÃ  phÃ¢n máº£nh bÃªn ngoÃ i tá»« bá»™ cáº¥p phÃ¡t bá»™ nhá»› nhÆ° buddy allocator. PhÃ¢n máº£nh bÃªn ngoÃ i sáº½ khÃ´ng bao giá» Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c token Ä‘Æ°á»£c táº¡o, Ä‘iá»u nÃ y Ä‘Æ°á»£c biáº¿t trÆ°á»›c khi phá»¥c vá»¥ má»™t yÃªu cáº§u. PhÃ¢n máº£nh bÃªn trong cÅ©ng khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng, nhÆ°ng Ä‘iá»u nÃ y chá»‰ Ä‘Æ°á»£c nháº­n ra sau khi má»™t yÃªu cáº§u Ä‘Ã£ hoÃ n thÃ nh láº¥y máº«u. Cáº£ hai Ä‘á»u lÃ  lÃ£ng phÃ­ bá»™ nhá»› thuáº§n tÃºy. Máº·c dÃ¹ bá»™ nhá»› dá»± trá»¯ cuá»‘i cÃ¹ng Ä‘Æ°á»£c sá»­ dá»¥ng, viá»‡c dÃ nh riÃªng khÃ´ng gian nÃ y trong suá»‘t thá»i gian tá»“n táº¡i cá»§a yÃªu cáº§u, Ä‘áº·c biá»‡t khi khÃ´ng gian dá»± trá»¯ lá»›n, chiáº¿m khÃ´ng gian cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xá»­ lÃ½ cÃ¡c yÃªu cáº§u khÃ¡c. ChÃºng tÃ´i trá»±c quan hÃ³a tá»· lá»‡ pháº§n trÄƒm trung bÃ¬nh lÃ£ng phÃ­ bá»™ nhá»› trong cÃ¡c thÃ­ nghiá»‡m cá»§a mÃ¬nh trong HÃ¬nh 2, tiáº¿t lá»™ ráº±ng bá»™ nhá»› hiá»‡u quáº£ thá»±c táº¿ trong cÃ¡c há»‡ thá»‘ng trÆ°á»›c Ä‘Ã¢y cÃ³ thá»ƒ tháº¥p Ä‘áº¿n 20.4%.

Máº·c dÃ¹ compaction [54] Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t nhÆ° má»™t giáº£i phÃ¡p tiá»m nÄƒng cho phÃ¢n máº£nh, viá»‡c thá»±c hiá»‡n compaction trong má»™t há»‡ thá»‘ng phá»¥c vá»¥ LLM nháº¡y cáº£m vá»›i hiá»‡u suáº¥t lÃ  khÃ´ng thá»±c táº¿ do KV cache khá»•ng lá»“. Ngay cáº£ vá»›i compaction, khÃ´ng gian khá»‘i cáº¥p phÃ¡t trÆ°á»›c cho má»—i yÃªu cáº§u ngÄƒn cáº£n viá»‡c chia sáº» bá»™ nhá»› cá»¥ thá»ƒ cho thuáº­t toÃ¡n giáº£i mÃ£ trong cÃ¡c há»‡ thá»‘ng quáº£n lÃ½ bá»™ nhá»› hiá»‡n táº¡i.

4 PhÆ°Æ¡ng PhÃ¡p
Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t thuáº­t toÃ¡n attention má»›i, PagedAttention, vÃ  xÃ¢y dá»±ng má»™t cÃ´ng cá»¥ phá»¥c vá»¥ LLM, vLLM, Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c Ä‘Æ°á»£c nÃªu trong Â§3. Kiáº¿n trÃºc cá»§a vLLM Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 4. vLLM Ã¡p dá»¥ng má»™t scheduler táº­p trung Ä‘á»ƒ Ä‘iá»u phá»‘i viá»‡c thá»±c thi cÃ¡c worker GPU phÃ¢n tÃ¡n. KV cache manager quáº£n lÃ½ hiá»‡u quáº£ KV cache theo cÃ¡ch phÃ¢n trang, Ä‘Æ°á»£c kÃ­ch hoáº¡t bá»Ÿi PagedAttention. Cá»¥ thá»ƒ, KV cache manager quáº£n lÃ½ bá»™ nhá»› KV cache váº­t lÃ½ trÃªn cÃ¡c worker GPU thÃ´ng qua cÃ¡c chá»‰ thá»‹ Ä‘Æ°á»£c gá»­i bá»Ÿi scheduler táº­p trung.

Tiáº¿p theo, chÃºng tÃ´i mÃ´ táº£ thuáº­t toÃ¡n PagedAttention trong Â§4.1. Vá»›i Ä‘iá»u Ä‘Ã³, chÃºng tÃ´i trÃ¬nh bÃ y thiáº¿t káº¿ cá»§a KV cache manager trong Â§4.2 vÃ  cÃ¡ch nÃ³ há»— trá»£ PagedAttention trong Â§4.3, tÆ°Æ¡ng á»©ng. Sau Ä‘Ã³, chÃºng tÃ´i chá»‰ ra cÃ¡ch thiáº¿t káº¿ nÃ y táº¡o Ä‘iá»u kiá»‡n cho quáº£n lÃ½ bá»™ nhá»› hiá»‡u quáº£ cho cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ khÃ¡c nhau (Â§4.4) vÃ  xá»­ lÃ½ cÃ¡c chuá»—i Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cÃ³ Ä‘á»™ dÃ i biáº¿n Ä‘á»•i (Â§4.5). Cuá»‘i cÃ¹ng, chÃºng tÃ´i chá»‰ ra cÃ¡ch thiáº¿t káº¿ há»‡ thá»‘ng cá»§a vLLM hoáº¡t Ä‘á»™ng trong mÃ´i trÆ°á»ng phÃ¢n tÃ¡n (Â§4.6).

[HÃ¬nh 4: Tá»•ng quan há»‡ thá»‘ng vLLM.]

4.1 PagedAttention
Äá»ƒ giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c bá»™ nhá»› trong Â§3, chÃºng tÃ´i giá»›i thiá»‡u PagedAttention, má»™t thuáº­t toÃ¡n attention Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« Ã½ tÆ°á»Ÿng cá»• Ä‘iá»ƒn cá»§a phÃ¢n trang [25] trong há»‡ Ä‘iá»u hÃ nh. KhÃ´ng giá»‘ng nhÆ° cÃ¡c thuáº­t toÃ¡n attention truyá»n thá»‘ng, PagedAttention cho phÃ©p lÆ°u trá»¯ key vÃ  value liÃªn tá»¥c trong khÃ´ng gian bá»™ nhá»› khÃ´ng liá»n ká». Cá»¥ thá»ƒ, PagedAttention phÃ¢n chia KV cache cá»§a má»—i chuá»—i thÃ nh cÃ¡c KV block. Má»—i block chá»©a key vÃ  value vector cho má»™t sá»‘ lÆ°á»£ng token cá»‘ Ä‘á»‹nh, chÃºng tÃ´i kÃ½ hiá»‡u lÃ  kÃ­ch thÆ°á»›c KV block (ğµ). KÃ½ hiá»‡u key block ğ¾ğ‘— = (ğ‘˜(ğ‘—âˆ’1)ğµ+1,...,ğ‘˜ğ‘—ğµ) vÃ  value block ğ‘‰ğ‘— = (ğ‘£(ğ‘—âˆ’1)ğµ+1,...,ğ‘£ğ‘—ğµ). TÃ­nh toÃ¡n attention trong Eq. 4 cÃ³ thá»ƒ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh tÃ­nh toÃ¡n theo khá»‘i nhÆ° sau:

ğ´ğ‘–ğ‘— = exp(ğ‘âŠ¤ğ‘–ğ¾ğ‘—/âˆšğ‘‘) / Î£âŒˆğ‘–/ğµâŒ‰ğ‘¡=1exp(ğ‘âŠ¤ğ‘–ğ¾ğ‘¡1/âˆšğ‘‘), ğ‘œğ‘– = Î£âŒˆğ‘–/ğµâŒ‰ğ‘—=1ğ‘‰ğ‘—ğ´âŠ¤ğ‘–ğ‘—, (4)

trong Ä‘Ã³ ğ´ğ‘–ğ‘— = (ğ‘ğ‘–,(ğ‘—âˆ’1)ğµ+1,...,ğ‘ğ‘–,ğ‘—ğµ) lÃ  vector hÃ ng cá»§a Ä‘iá»ƒm attention trÃªn KV block thá»© ğ‘—.

Trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n attention, kernel PagedAttention xÃ¡c Ä‘á»‹nh vÃ  láº¥y cÃ¡c KV block khÃ¡c nhau riÃªng biá»‡t. ChÃºng tÃ´i thá»ƒ hiá»‡n má»™t vÃ­ dá»¥ vá» PagedAttention trong HÃ¬nh 5: CÃ¡c vector key vÃ  value Ä‘Æ°á»£c tráº£i rá»™ng qua ba block, vÃ  ba block khÃ´ng liá»n ká» trÃªn bá»™ nhá»› váº­t lÃ½. Táº¡i má»—i thá»i Ä‘iá»ƒm, kernel nhÃ¢n vector query ğ‘ğ‘– cá»§a token query ("forth") vÃ  cÃ¡c vector key ğ¾ğ‘— trong má»™t block (vÃ­ dá»¥: vector key cá»§a "Four score and seven" cho block 0) Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm attention ğ´ğ‘–ğ‘—, vÃ  sau Ä‘Ã³ nhÃ¢n ğ´ğ‘–ğ‘— vá»›i cÃ¡c vector value ğ‘‰ğ‘— trong má»™t block Ä‘á»ƒ suy ra Ä‘áº§u ra attention cuá»‘i cÃ¹ng ğ‘œğ‘–.

TÃ³m láº¡i, thuáº­t toÃ¡n PagedAttention cho phÃ©p cÃ¡c KV block Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› váº­t lÃ½ khÃ´ng liá»n ká», táº¡o Ä‘iá»u kiá»‡n cho quáº£n lÃ½ bá»™ nhá»› phÃ¢n trang linh hoáº¡t hÆ¡n trong vLLM.

[HÃ¬nh 5: Minh há»a thuáº­t toÃ¡n PagedAttention, trong Ä‘Ã³ cÃ¡c vector attention key vÃ  value Ä‘Æ°á»£c lÆ°u trá»¯ nhÆ° cÃ¡c block khÃ´ng liá»n ká» trong bá»™ nhá»›.]

4.2 KV Cache Manager
Ã tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau trÃ¬nh quáº£n lÃ½ bá»™ nhá»› cá»§a vLLM tÆ°Æ¡ng tá»± nhÆ° bá»™ nhá»› áº£o [25] trong há»‡ Ä‘iá»u hÃ nh. OS phÃ¢n chia bá»™ nhá»› thÃ nh cÃ¡c trang cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh vÃ  Ã¡nh xáº¡ cÃ¡c trang logic cá»§a chÆ°Æ¡ng trÃ¬nh ngÆ°á»i dÃ¹ng Ä‘áº¿n cÃ¡c trang váº­t lÃ½. CÃ¡c trang logic liá»n ká» cÃ³ thá»ƒ tÆ°Æ¡ng á»©ng vá»›i cÃ¡c trang bá»™ nhá»› váº­t lÃ½ khÃ´ng liá»n ká», cho phÃ©p cÃ¡c chÆ°Æ¡ng trÃ¬nh ngÆ°á»i dÃ¹ng truy cáº­p bá»™ nhá»› nhÆ° thá»ƒ nÃ³ liá»n ká». HÆ¡n ná»¯a, khÃ´ng gian bá»™ nhá»› váº­t lÃ½ khÃ´ng cáº§n pháº£i Ä‘Æ°á»£c dÃ nh riÃªng Ä‘áº§y Ä‘á»§ trÆ°á»›c, cho phÃ©p OS cáº¥p phÃ¡t Ä‘á»™ng cÃ¡c trang váº­t lÃ½ khi cáº§n thiáº¿t. vLLM sá»­ dá»¥ng cÃ¡c Ã½ tÆ°á»Ÿng Ä‘áº±ng sau bá»™ nhá»› áº£o Ä‘á»ƒ quáº£n lÃ½ KV cache trong dá»‹ch vá»¥ LLM. ÄÆ°á»£c kÃ­ch hoáº¡t bá»Ÿi PagedAttention, chÃºng tÃ´i tá»• chá»©c KV cache nhÆ° cÃ¡c KV block cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh, giá»‘ng nhÆ° cÃ¡c trang trong bá»™ nhá»› áº£o.

KV cache cá»§a má»™t yÃªu cáº§u Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t chuá»—i cÃ¡c logical KV block, Ä‘Æ°á»£c láº¥p Ä‘áº§y tá»« trÃ¡i sang pháº£i khi cÃ¡c token má»›i vÃ  KV cache cá»§a chÃºng Ä‘Æ°á»£c táº¡o ra. CÃ¡c vá»‹ trÃ­ chÆ°a láº¥p Ä‘áº§y cá»§a KV block cuá»‘i cÃ¹ng Ä‘Æ°á»£c dÃ nh riÃªng cho cÃ¡c láº§n táº¡o tÆ°Æ¡ng lai. TrÃªn cÃ¡c worker GPU, má»™t block engine cáº¥p phÃ¡t má»™t khá»‘i DRAM GPU liá»n ká» vÃ  chia nÃ³ thÃ nh cÃ¡c physical KV block (Ä‘iá»u nÃ y cÅ©ng Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn CPU RAM Ä‘á»ƒ swapping; xem Â§4.5). KV block manager cÅ©ng duy trÃ¬ cÃ¡c block table - Ã¡nh xáº¡ giá»¯a cÃ¡c logical vÃ  physical KV block cá»§a má»—i yÃªu cáº§u. Má»—i má»¥c block table ghi láº¡i cÃ¡c physical block tÆ°Æ¡ng á»©ng cá»§a má»™t logical block vÃ  sá»‘ lÆ°á»£ng vá»‹ trÃ­ Ä‘Ã£ láº¥p Ä‘áº§y. Viá»‡c tÃ¡ch biá»‡t logical vÃ  physical KV block cho phÃ©p vLLM tÄƒng trÆ°á»Ÿng Ä‘á»™ng bá»™ nhá»› KV cache mÃ  khÃ´ng cáº§n dÃ nh riÃªng cho táº¥t cáº£ vá»‹ trÃ­ trÆ°á»›c, Ä‘iá»u nÃ y loáº¡i bá» háº§u háº¿t lÃ£ng phÃ­ bá»™ nhá»› trong cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i, nhÆ° trong HÃ¬nh 2.

--- TRANG 4 ---
[HÃ¬nh 6: Dá»‹ch block table trong vLLM.]

4.3 Giáº£i MÃ£ vá»›i PagedAttention vÃ  vLLM
Tiáº¿p theo, chÃºng tÃ´i Ä‘i qua má»™t vÃ­ dá»¥, nhÆ° trong HÃ¬nh 6, Ä‘á»ƒ chá»©ng minh cÃ¡ch vLLM thá»±c thi PagedAttention vÃ  quáº£n lÃ½ bá»™ nhá»› trong quÃ¡ trÃ¬nh giáº£i mÃ£ cá»§a má»™t chuá»—i Ä‘áº§u vÃ o duy nháº¥t:

1â—‹ NhÆ° trong bá»™ nhá»› áº£o cá»§a OS, vLLM khÃ´ng yÃªu cáº§u dÃ nh riÃªng bá»™ nhá»› cho Ä‘á»™ dÃ i chuá»—i Ä‘Æ°á»£c táº¡o tá»‘i Ä‘a cÃ³ thá»ƒ ban Ä‘áº§u. Thay vÃ o Ä‘Ã³, nÃ³ chá»‰ dÃ nh riÃªng cÃ¡c KV block cáº§n thiáº¿t Ä‘á»ƒ chá»©a KV cache Ä‘Æ°á»£c táº¡o trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n prompt. Trong trÆ°á»ng há»£p nÃ y, prompt cÃ³ 7 token, vÃ¬ váº­y vLLM Ã¡nh xáº¡ 2 logical KV block Ä‘áº§u tiÃªn (0 vÃ  1) Ä‘áº¿n 2 physical KV block (7 vÃ  1, tÆ°Æ¡ng á»©ng). Trong bÆ°á»›c prefill, vLLM táº¡o KV cache cá»§a prompt vÃ  token Ä‘áº§u ra Ä‘áº§u tiÃªn vá»›i thuáº­t toÃ¡n self-attention thÃ´ng thÆ°á»ng (vÃ­ dá»¥: [13]). vLLM sau Ä‘Ã³ lÆ°u trá»¯ KV cache cá»§a 4 token Ä‘áº§u tiÃªn trong logical block 0 vÃ  3 token tiáº¿p theo trong logical block 1. Slot cÃ²n láº¡i Ä‘Æ°á»£c dÃ nh riÃªng cho giai Ä‘oáº¡n táº¡o tá»± há»“i quy tiáº¿p theo.

2â—‹ Trong bÆ°á»›c giáº£i mÃ£ tá»± há»“i quy Ä‘áº§u tiÃªn, vLLM táº¡o token má»›i vá»›i thuáº­t toÃ¡n PagedAttention trÃªn physical block 7 vÃ  1. VÃ¬ má»™t slot váº«n cÃ³ sáºµn trong logical block cuá»‘i cÃ¹ng, KV cache má»›i Ä‘Æ°á»£c táº¡o Ä‘Æ°á»£c lÆ°u trá»¯ á»Ÿ Ä‘Ã³, vÃ  báº£n ghi #filled cá»§a block table Ä‘Æ°á»£c cáº­p nháº­t.

3â—‹ Táº¡i bÆ°á»›c giáº£i mÃ£ thá»© hai, khi logical block cuá»‘i cÃ¹ng Ä‘Ã£ Ä‘áº§y, vLLM lÆ°u trá»¯ KV cache má»›i Ä‘Æ°á»£c táº¡o trong má»™t logical block má»›i; vLLM cáº¥p phÃ¡t má»™t physical block má»›i (physical block 3) cho nÃ³ vÃ  lÆ°u trá»¯ Ã¡nh xáº¡ nÃ y trong block table.

ToÃ n cáº§u, cho má»—i iteration giáº£i mÃ£, vLLM Ä‘áº§u tiÃªn chá»n má»™t táº­p há»£p cÃ¡c chuá»—i á»©ng viÃªn Ä‘á»ƒ batching (thÃªm trong Â§4.5), vÃ  cáº¥p phÃ¡t cÃ¡c physical block cho cÃ¡c logical block má»›i cáº§n thiáº¿t. Sau Ä‘Ã³, vLLM ná»‘i táº¥t cáº£ cÃ¡c token Ä‘áº§u vÃ o cá»§a iteration hiá»‡n táº¡i (tá»©c lÃ  táº¥t cáº£ token cho giai Ä‘oáº¡n prompt cho cÃ¡c yÃªu cáº§u prompt vÃ  cÃ¡c token má»›i nháº¥t cho cÃ¡c yÃªu cáº§u giai Ä‘oáº¡n táº¡o) thÃ nh má»™t chuá»—i vÃ  Ä‘Æ°a nÃ³ vÃ o LLM. Trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a LLM, vLLM sá»­ dá»¥ng kernel PagedAttention Ä‘á»ƒ truy cáº­p KV cache trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c lÆ°u trá»¯ dÆ°á»›i dáº¡ng logical KV block vÃ  lÆ°u KV cache má»›i Ä‘Æ°á»£c táº¡o vÃ o cÃ¡c physical KV block. LÆ°u trá»¯ nhiá»u token trong má»™t KV block (kÃ­ch thÆ°á»›c block > 1) cho phÃ©p kernel PagedAttention xá»­ lÃ½ KV cache qua nhiá»u vá»‹ trÃ­ hÆ¡n song song, do Ä‘Ã³ tÄƒng viá»‡c sá»­ dá»¥ng pháº§n cá»©ng vÃ  giáº£m Ä‘á»™ trá»…. Tuy nhiÃªn, kÃ­ch thÆ°á»›c block lá»›n hÆ¡n cÅ©ng tÄƒng phÃ¢n máº£nh bá»™ nhá»›. ChÃºng tÃ´i nghiÃªn cá»©u tÃ¡c Ä‘á»™ng cá»§a kÃ­ch thÆ°á»›c block trong Â§7.2.

Má»™t láº§n ná»¯a, vLLM cáº¥p phÃ¡t Ä‘á»™ng cÃ¡c physical block má»›i cho logical block khi nhiá»u token vÃ  KV cache cá»§a chÃºng Ä‘Æ°á»£c táº¡o ra. VÃ¬ táº¥t cáº£ cÃ¡c block Ä‘Æ°á»£c láº¥p Ä‘áº§y tá»« trÃ¡i sang pháº£i vÃ  má»™t physical block má»›i chá»‰ Ä‘Æ°á»£c cáº¥p phÃ¡t khi táº¥t cáº£ cÃ¡c block trÆ°á»›c Ä‘Ã³ Ä‘Ã£ Ä‘áº§y, vLLM giá»›i háº¡n táº¥t cáº£ lÃ£ng phÃ­ bá»™ nhá»› cho má»™t yÃªu cáº§u trong má»™t block, vÃ¬ váº­y nÃ³ cÃ³ thá»ƒ sá»­ dá»¥ng hiá»‡u quáº£ táº¥t cáº£ bá»™ nhá»›, nhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 2. Äiá»u nÃ y cho phÃ©p nhiá»u yÃªu cáº§u hÆ¡n vá»«a vÃ o bá»™ nhá»› Ä‘á»ƒ batchingâ€”do Ä‘Ã³ cáº£i thiá»‡n thÃ´ng lÆ°á»£ng. Khi má»™t yÃªu cáº§u hoÃ n thÃ nh viá»‡c táº¡o cá»§a nÃ³, cÃ¡c KV block cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i phÃ³ng Ä‘á»ƒ lÆ°u trá»¯ KV cache cá»§a cÃ¡c yÃªu cáº§u khÃ¡c. Trong HÃ¬nh 7, chÃºng tÃ´i thá»ƒ hiá»‡n má»™t vÃ­ dá»¥ vá» vLLM quáº£n lÃ½ bá»™ nhá»› cho hai chuá»—i. CÃ¡c logical block cá»§a hai chuá»—i Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n cÃ¡c physical block khÃ¡c nhau trong khÃ´ng gian Ä‘Æ°á»£c dÃ nh riÃªng bá»Ÿi block engine trong cÃ¡c worker GPU. CÃ¡c logical block lÃ¢n cáº­n cá»§a cáº£ hai chuá»—i khÃ´ng cáº§n pháº£i liá»n ká» trong bá»™ nhá»› GPU váº­t lÃ½ vÃ  khÃ´ng gian cá»§a cÃ¡c physical block cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng hiá»‡u quáº£ bá»Ÿi cáº£ hai chuá»—i.

[HÃ¬nh 7: LÆ°u trá»¯ KV cache cá»§a hai yÃªu cáº§u cÃ¹ng má»™t lÃºc trong vLLM.]

4.4 á»¨ng Dá»¥ng cho CÃ¡c TÃ¬nh Huá»‘ng Giáº£i MÃ£ KhÃ¡c
Â§4.3 chá»‰ ra cÃ¡ch PagedAttention vÃ  vLLM xá»­ lÃ½ cÃ¡c thuáº­t toÃ¡n giáº£i mÃ£ cÆ¡ báº£n, nhÆ° giáº£i mÃ£ tham lam vÃ  láº¥y máº«u, nháº­n má»™t prompt ngÆ°á»i dÃ¹ng lÃ m Ä‘áº§u vÃ o vÃ  táº¡o ra má»™t chuá»—i Ä‘áº§u ra duy nháº¥t. Trong nhiá»u á»©ng dá»¥ng LLM thÃ nh cÃ´ng [18,34], má»™t dá»‹ch vá»¥ LLM pháº£i cung cáº¥p cÃ¡c tÃ¬nh huá»‘ng giáº£i mÃ£ phá»©c táº¡p hÆ¡n thá»ƒ hiá»‡n cÃ¡c máº«u truy cáº­p phá»©c táº¡p vÃ  nhiá»u cÆ¡ há»™i chia sáº» bá»™ nhá»› hÆ¡n. ChÃºng tÃ´i thá»ƒ hiá»‡n kháº£ nÄƒng Ã¡p dá»¥ng tá»•ng quÃ¡t cá»§a vLLM trÃªn chÃºng trong pháº§n nÃ y.

Láº¥y máº«u song song. Trong cÃ¡c trá»£ lÃ½ chÆ°Æ¡ng trÃ¬nh dá»±a trÃªn LLM [6,18], má»™t LLM táº¡o ra nhiá»u Ä‘áº§u ra Ä‘Æ°á»£c láº¥y máº«u cho má»™t prompt Ä‘áº§u vÃ o duy nháº¥t; ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ chá»n má»™t Ä‘áº§u ra yÃªu thÃ­ch tá»« cÃ¡c á»©ng viÃªn khÃ¡c nhau. Cho Ä‘áº¿n nay chÃºng tÃ´i Ä‘Ã£ giáº£ Ä‘á»‹nh ngáº§m ráº±ng má»™t yÃªu cáº§u táº¡o ra má»™t chuá»—i duy nháº¥t. Trong pháº§n cÃ²n láº¡i cá»§a bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giáº£ Ä‘á»‹nh trÆ°á»ng há»£p tá»•ng quÃ¡t hÆ¡n trong Ä‘Ã³ má»™t yÃªu cáº§u táº¡o ra nhiá»u chuá»—i. Trong láº¥y máº«u song song, má»™t yÃªu cáº§u bao gá»“m nhiá»u máº«u chia sáº» cÃ¹ng prompt Ä‘áº§u vÃ o, cho phÃ©p KV cache cá»§a prompt cÅ©ng Ä‘Æ°á»£c chia sáº». ThÃ´ng qua PagedAttention vÃ  quáº£n lÃ½ bá»™ nhá»› phÃ¢n trang cá»§a nÃ³, vLLM cÃ³ thá»ƒ thá»±c hiá»‡n viá»‡c chia sáº» nÃ y má»™t cÃ¡ch dá»… dÃ ng vÃ  tiáº¿t kiá»‡m bá»™ nhá»›.

[HÃ¬nh 8: VÃ­ dá»¥ láº¥y máº«u song song.]

HÃ¬nh 8 thá»ƒ hiá»‡n má»™t vÃ­ dá»¥ vá» giáº£i mÃ£ song song cho hai Ä‘áº§u ra. VÃ¬ cáº£ hai Ä‘áº§u ra chia sáº» cÃ¹ng prompt, chÃºng tÃ´i chá»‰ dÃ nh riÃªng khÃ´ng gian cho má»™t báº£n sao tráº¡ng thÃ¡i cá»§a prompt táº¡i giai Ä‘oáº¡n prompt; cÃ¡c logical block cho prompt cá»§a cáº£ hai chuá»—i Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n cÃ¹ng cÃ¡c physical block: logical block 0 vÃ  1 cá»§a cáº£ hai chuá»—i Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n physical block 7 vÃ  1, tÆ°Æ¡ng á»©ng. VÃ¬ má»™t physical block duy nháº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡nh xáº¡ Ä‘áº¿n nhiá»u logical block, chÃºng tÃ´i giá»›i thiá»‡u má»™t sá»‘ Ä‘áº¿m tham chiáº¿u cho má»—i physical block. Trong trÆ°á»ng há»£p nÃ y, sá»‘ Ä‘áº¿m tham chiáº¿u cho physical block 7 vÃ  1 Ä‘á»u lÃ  2. Táº¡i giai Ä‘oáº¡n táº¡o, hai Ä‘áº§u ra láº¥y máº«u cÃ¡c token Ä‘áº§u ra khÃ¡c nhau vÃ  cáº§n lÆ°u trá»¯ riÃªng biá»‡t cho KV cache. vLLM triá»ƒn khai cÆ¡ cháº¿ copy-on-write á»Ÿ má»©c Ä‘á»™ chi tiáº¿t block cho cÃ¡c physical block cáº§n sá»­a Ä‘á»•i bá»Ÿi nhiá»u chuá»—i, tÆ°Æ¡ng tá»± nhÆ° ká»¹ thuáº­t copy-on-write trong bá»™ nhá»› áº£o OS (vÃ­ dá»¥: khi fork má»™t quy trÃ¬nh). Cá»¥ thá»ƒ, trong HÃ¬nh 8, khi máº«u A1 cáº§n ghi vÃ o logical block cuá»‘i cÃ¹ng cá»§a nÃ³ (logical block 1), vLLM nháº­n ra ráº±ng sá»‘ Ä‘áº¿m tham chiáº¿u cá»§a physical block tÆ°Æ¡ng á»©ng (physical block 1) lá»›n hÆ¡n 1; nÃ³ cáº¥p phÃ¡t má»™t physical block má»›i (physical block 3), chá»‰ thá»‹ cho block engine sao chÃ©p thÃ´ng tin tá»« physical block 1, vÃ  giáº£m sá»‘ Ä‘áº¿m tham chiáº¿u xuá»‘ng 1. Tiáº¿p theo, khi máº«u A2 ghi vÃ o physical block 1, sá»‘ Ä‘áº¿m tham chiáº¿u Ä‘Ã£ Ä‘Æ°á»£c giáº£m xuá»‘ng 1; do Ä‘Ã³ A2 trá»±c tiáº¿p ghi KV cache má»›i Ä‘Æ°á»£c táº¡o cá»§a nÃ³ vÃ o physical block 1.

TÃ³m láº¡i, vLLM cho phÃ©p chia sáº» háº§u háº¿t khÃ´ng gian Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÆ°u trá»¯ KV cache cá»§a prompt qua nhiá»u máº«u Ä‘áº§u ra, ngoáº¡i trá»« logical block cuá»‘i cÃ¹ng, Ä‘Æ°á»£c quáº£n lÃ½ bá»Ÿi cÆ¡ cháº¿ copy-on-write. Báº±ng cÃ¡ch chia sáº» physical block qua nhiá»u máº«u, viá»‡c sá»­ dá»¥ng bá»™ nhá»› cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ, Ä‘áº·c biá»‡t cho cÃ¡c prompt Ä‘áº§u vÃ o dÃ i.

Beam search. Trong cÃ¡c nhiá»‡m vá»¥ LLM nhÆ° dá»‹ch mÃ¡y [59], ngÆ°á»i dÃ¹ng mong Ä‘á»£i Ä‘áº§u ra dá»‹ch thuáº­t phÃ¹ há»£p nháº¥t top-ğ‘˜ Ä‘Æ°á»£c xuáº¥t ra bá»Ÿi LLM. Beam search [49] Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ giáº£i mÃ£ chuá»—i Ä‘áº§u ra cÃ³ kháº£ nÄƒng nháº¥t tá»« má»™t LLM, vÃ¬ nÃ³ giáº£m thiá»ƒu Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a viá»‡c duyá»‡t hoÃ n toÃ n khÃ´ng gian máº«u. Thuáº­t toÃ¡n dá»±a vÃ o tham sá»‘ beam width ğ‘˜, xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng á»©ng viÃªn hÃ ng Ä‘áº§u Ä‘Æ°á»£c giá»¯ láº¡i á»Ÿ má»—i bÆ°á»›c. Trong quÃ¡ trÃ¬nh giáº£i mÃ£, beam search má»Ÿ rá»™ng má»—i chuá»—i á»©ng viÃªn trong beam báº±ng cÃ¡ch xem xÃ©t táº¥t cáº£ cÃ¡c token cÃ³ thá»ƒ, tÃ­nh xÃ¡c suáº¥t tÆ°Æ¡ng á»©ng cá»§a chÃºng báº±ng LLM, vÃ  giá»¯ láº¡i cÃ¡c chuá»—i cÃ³ kháº£ nÄƒng nháº¥t top-ğ‘˜ trong sá»‘ ğ‘˜Â·|ğ‘‰| á»©ng viÃªn, trong Ä‘Ã³ |ğ‘‰| lÃ  kÃ­ch thÆ°á»›c tá»« vá»±ng.

KhÃ´ng giá»‘ng nhÆ° giáº£i mÃ£ song song, beam search táº¡o Ä‘iá»u kiá»‡n chia sáº» khÃ´ng chá»‰ cÃ¡c block prompt ban Ä‘áº§u mÃ  cÃ²n cÃ¡c block khÃ¡c qua cÃ¡c á»©ng viÃªn khÃ¡c nhau, vÃ  cÃ¡c máº«u chia sáº» thay Ä‘á»•i Ä‘á»™ng khi quÃ¡ trÃ¬nh giáº£i mÃ£ tiáº¿n triá»ƒn, tÆ°Æ¡ng tá»± nhÆ° cÃ¢y quy trÃ¬nh trong OS Ä‘Æ°á»£c táº¡o bá»Ÿi cÃ¡c fork káº¿t há»£p. HÃ¬nh 9 thá»ƒ hiá»‡n cÃ¡ch vLLM quáº£n lÃ½ cÃ¡c KV block cho má»™t vÃ­ dá»¥ beam search vá»›i ğ‘˜=4. TrÆ°á»›c iteration Ä‘Æ°á»£c minh há»a nhÆ° Ä‘Æ°á»ng cháº¥m, má»—i chuá»—i á»©ng viÃªn Ä‘Ã£ sá»­ dá»¥ng 4 logical block Ä‘áº§y. Táº¥t cáº£ á»©ng viÃªn beam chia sáº» block Ä‘áº§u tiÃªn 0 (tá»©c lÃ  prompt). á»¨ng viÃªn 3 tÃ¡ch ra khá»i cÃ¡c á»©ng viÃªn khÃ¡c tá»« block thá»© hai. á»¨ng viÃªn 0-2 chia sáº» 3 block Ä‘áº§u tiÃªn vÃ  tÃ¡ch ra á»Ÿ block thá»© tÆ°. Táº¡i cÃ¡c iteration tiáº¿p theo, táº¥t cáº£ á»©ng viÃªn cÃ³ kháº£ nÄƒng top-4 Ä‘á»u xuáº¥t phÃ¡t tá»« á»©ng viÃªn 1 vÃ  2. VÃ¬ á»©ng viÃªn 0 vÃ  3 ban Ä‘áº§u khÃ´ng cÃ²n náº±m trong sá»‘ cÃ¡c á»©ng viÃªn hÃ ng Ä‘áº§u, cÃ¡c logical block cá»§a chÃºng Ä‘Æ°á»£c giáº£i phÃ³ng, vÃ  sá»‘ Ä‘áº¿m tham chiáº¿u cá»§a cÃ¡c physical block tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c giáº£m. vLLM giáº£i phÃ³ng táº¥t cáº£ physical block cÃ³ sá»‘ Ä‘áº¿m tham chiáº¿u Ä‘áº¡t 0 (block 2, 4, 5, 8). Sau Ä‘Ã³, vLLM cáº¥p phÃ¡t cÃ¡c physical block má»›i (block 9-12) Ä‘á»ƒ lÆ°u trá»¯ KV cache má»›i tá»« cÃ¡c á»©ng viÃªn má»›i. BÃ¢y giá», táº¥t cáº£ á»©ng viÃªn chia sáº» block 0, 1, 3; á»©ng viÃªn 0 vÃ  1 chia sáº» block 6, vÃ  á»©ng viÃªn 2 vÃ  3 tiáº¿p tá»¥c chia sáº» block 7.

[HÃ¬nh 9: VÃ­ dá»¥ beam search.]

CÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM trÆ°á»›c Ä‘Ã¢y yÃªu cáº§u sao chÃ©p bá»™ nhá»› thÆ°á»ng xuyÃªn cá»§a KV cache qua cÃ¡c á»©ng viÃªn beam. VÃ­ dá»¥, trong trÆ°á»ng há»£p thá»ƒ hiá»‡n trong HÃ¬nh 9, sau Ä‘Æ°á»ng cháº¥m, á»©ng viÃªn 3 sáº½ cáº§n sao chÃ©p má»™t pháº§n lá»›n KV cache cá»§a á»©ng viÃªn 2 Ä‘á»ƒ tiáº¿p tá»¥c táº¡o. Chi phÃ­ sao chÃ©p bá»™ nhá»› thÆ°á»ng xuyÃªn nÃ y Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ báº±ng viá»‡c chia sáº» physical block cá»§a vLLM. Trong vLLM, háº§u háº¿t cÃ¡c block cá»§a cÃ¡c á»©ng viÃªn beam khÃ¡c nhau cÃ³ thá»ƒ Ä‘Æ°á»£c chia sáº». CÆ¡ cháº¿ copy-on-write chá»‰ Ä‘Æ°á»£c Ã¡p dá»¥ng khi cÃ¡c token má»›i Ä‘Æ°á»£c táº¡o náº±m trong má»™t block chia sáº» cÅ©, nhÆ° trong giáº£i mÃ£ song song. Äiá»u nÃ y chá»‰ liÃªn quan Ä‘áº¿n viá»‡c sao chÃ©p má»™t block dá»¯ liá»‡u.

Tiá»n tá»‘ chia sáº». ThÃ´ng thÆ°á»ng, ngÆ°á»i dÃ¹ng LLM cung cáº¥p má»™t mÃ´ táº£ (dÃ i) vá» nhiá»‡m vá»¥ bao gá»“m hÆ°á»›ng dáº«n vÃ  vÃ­ dá»¥ Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, cÃ²n Ä‘Æ°á»£c gá»i lÃ  system prompt [36]. MÃ´ táº£ Ä‘Æ°á»£c ná»‘i vá»›i Ä‘áº§u vÃ o nhiá»‡m vá»¥ thá»±c táº¿ Ä‘á»ƒ táº¡o thÃ nh prompt cá»§a yÃªu cáº§u. LLM táº¡o Ä‘áº§u ra dá»±a trÃªn prompt Ä‘áº§y Ä‘á»§. HÃ¬nh 10 thá»ƒ hiá»‡n má»™t vÃ­ dá»¥. HÆ¡n ná»¯a, tiá»n tá»‘ chia sáº» cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh thÃªm, thÃ´ng qua prompt engineering, Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c nhiá»‡m vá»¥ downstream [26, 27].

[HÃ¬nh 10: VÃ­ dá»¥ prompt chia sáº» cho dá»‹ch mÃ¡y. CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c Ã¡p dá»¥ng tá»« [5].]

Äá»‘i vá»›i loáº¡i á»©ng dá»¥ng nÃ y, nhiá»u prompt ngÆ°á»i dÃ¹ng chia sáº» má»™t tiá»n tá»‘, do Ä‘Ã³ nhÃ  cung cáº¥p dá»‹ch vá»¥ LLM cÃ³ thá»ƒ lÆ°u trá»¯ trÆ°á»›c KV cache cá»§a tiá»n tá»‘ Ä‘á»ƒ giáº£m tÃ­nh toÃ¡n dÆ° thá»«a dÃ nh cho tiá»n tá»‘. Trong vLLM, Ä‘iá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n thuáº­n tiá»‡n báº±ng cÃ¡ch dÃ nh riÃªng má»™t táº­p há»£p physical block cho má»™t táº­p há»£p tiá»n tá»‘ chia sáº» Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c bá»Ÿi nhÃ  cung cáº¥p dá»‹ch vá»¥ LLM, nhÆ° cÃ¡ch OS xá»­ lÃ½ thÆ° viá»‡n chia sáº» qua cÃ¡c quy trÃ¬nh. Má»™t prompt Ä‘áº§u vÃ o ngÆ°á»i dÃ¹ng vá»›i tiá»n tá»‘ chia sáº» cÃ³ thá»ƒ Ä‘Æ¡n giáº£n Ã¡nh xáº¡ cÃ¡c logical block cá»§a nÃ³ Ä‘áº¿n cÃ¡c physical block Ä‘Æ°á»£c cache (vá»›i block cuá»‘i cÃ¹ng Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u copy-on-write). TÃ­nh toÃ¡n giai Ä‘oáº¡n prompt chá»‰ cáº§n thá»±c thi trÃªn Ä‘áº§u vÃ o nhiá»‡m vá»¥ cá»§a ngÆ°á»i dÃ¹ng.

PhÆ°Æ¡ng phÃ¡p giáº£i mÃ£ há»—n há»£p. CÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ Ä‘Æ°á»£c tháº£o luáº­n trÆ°á»›c Ä‘Ã¢y thá»ƒ hiá»‡n cÃ¡c máº«u chia sáº» vÃ  truy cáº­p bá»™ nhá»› Ä‘a dáº¡ng. Tuy nhiÃªn, vLLM táº¡o Ä‘iá»u kiá»‡n cho viá»‡c xá»­ lÃ½ Ä‘á»“ng thá»i cÃ¡c yÃªu cáº§u vá»›i cÃ¡c tÃ¹y chá»n giáº£i mÃ£ khÃ¡c nhau, Ä‘iá»u mÃ  cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i khÃ´ng thá»ƒ thá»±c hiá»‡n hiá»‡u quáº£. Äiá»u nÃ y lÃ  do vLLM che giáº¥u viá»‡c chia sáº» bá»™ nhá»› phá»©c táº¡p giá»¯a cÃ¡c chuá»—i khÃ¡c nhau thÃ´ng qua má»™t lá»›p Ã¡nh xáº¡ chung dá»‹ch cÃ¡c logical block thÃ nh physical block. LLM vÃ  kernel thá»±c thi cá»§a nÃ³ chá»‰ tháº¥y má»™t danh sÃ¡ch ID physical block cho má»—i chuá»—i vÃ  khÃ´ng cáº§n xá»­ lÃ½ cÃ¡c máº«u chia sáº» qua cÃ¡c chuá»—i. So vá»›i cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i, cÃ¡ch tiáº¿p cáº­n nÃ y má»Ÿ rá»™ng cÃ¡c cÆ¡ há»™i batching cho cÃ¡c yÃªu cáº§u vá»›i cÃ¡c yÃªu cáº§u láº¥y máº«u khÃ¡c nhau, cuá»‘i cÃ¹ng tÄƒng thÃ´ng lÆ°á»£ng tá»•ng thá»ƒ cá»§a há»‡ thá»‘ng.

4.5 Láº­p Lá»‹ch vÃ  Preemption
Khi lÆ°u lÆ°á»£ng yÃªu cáº§u vÆ°á»£t quÃ¡ kháº£ nÄƒng cá»§a há»‡ thá»‘ng, vLLM pháº£i Æ°u tiÃªn má»™t táº­p há»£p con cÃ¡c yÃªu cáº§u. Trong vLLM, chÃºng tÃ´i Ã¡p dá»¥ng chÃ­nh sÃ¡ch láº­p lá»‹ch first-come-first-serve (FCFS) cho táº¥t cáº£ cÃ¡c yÃªu cáº§u, Ä‘áº£m báº£o cÃ´ng báº±ng vÃ  ngÄƒn cháº·n starvation. Khi vLLM cáº§n preempt cÃ¡c yÃªu cáº§u, nÃ³ Ä‘áº£m báº£o ráº±ng cÃ¡c yÃªu cáº§u Ä‘áº¿n sá»›m nháº¥t Ä‘Æ°á»£c phá»¥c vá»¥ trÆ°á»›c vÃ  cÃ¡c yÃªu cáº§u má»›i nháº¥t Ä‘Æ°á»£c preempt trÆ°á»›c.

CÃ¡c dá»‹ch vá»¥ LLM Ä‘á»‘i máº·t vá»›i má»™t thÃ¡ch thá»©c Ä‘á»™c Ä‘Ã¡o: cÃ¡c prompt Ä‘áº§u vÃ o cho má»™t LLM cÃ³ thá»ƒ khÃ¡c nhau Ä‘Ã¡ng ká»ƒ vá» Ä‘á»™ dÃ i, vÃ  Ä‘á»™ dÃ i Ä‘áº§u ra káº¿t quáº£ khÃ´ng Ä‘Æ°á»£c biáº¿t trÆ°á»›c, phá»¥ thuá»™c vÃ o cáº£ prompt Ä‘áº§u vÃ o vÃ  mÃ´ hÃ¬nh. Khi sá»‘ lÆ°á»£ng yÃªu cáº§u vÃ  Ä‘áº§u ra cá»§a chÃºng tÄƒng trÆ°á»Ÿng, vLLM cÃ³ thá»ƒ háº¿t cÃ¡c physical block cá»§a GPU Ä‘á»ƒ lÆ°u trá»¯ KV cache má»›i Ä‘Æ°á»£c táº¡o. CÃ³ hai cÃ¢u há»i cá»• Ä‘iá»ƒn mÃ  vLLM cáº§n tráº£ lá»i trong bá»‘i cáº£nh nÃ y: (1) NÃªn evict block nÃ o? (2) LÃ m tháº¿ nÃ o Ä‘á»ƒ khÃ´i phá»¥c cÃ¡c block Ä‘Ã£ evict náº¿u cáº§n láº¡i? ThÃ´ng thÆ°á»ng, cÃ¡c chÃ­nh sÃ¡ch eviction sá»­ dá»¥ng heuristics Ä‘á»ƒ dá»± Ä‘oÃ¡n block nÃ o sáº½ Ä‘Æ°á»£c truy cáº­p xa nháº¥t trong tÆ°Æ¡ng lai vÃ  evict block Ä‘Ã³. VÃ¬ trong trÆ°á»ng há»£p cá»§a chÃºng ta, chÃºng ta biáº¿t ráº±ng táº¥t cáº£ cÃ¡c block cá»§a má»™t chuá»—i Ä‘Æ°á»£c truy cáº­p cÃ¹ng nhau, chÃºng tÃ´i triá»ƒn khai chÃ­nh sÃ¡ch eviction all-or-nothing, tá»©c lÃ  hoáº·c evict táº¥t cáº£ hoáº·c khÃ´ng evict block nÃ o cá»§a má»™t chuá»—i. HÆ¡n ná»¯a, nhiá»u chuá»—i trong má»™t yÃªu cáº§u (vÃ­ dá»¥: cÃ¡c á»©ng viÃªn beam trong má»™t yÃªu cáº§u beam search) Ä‘Æ°á»£c láº­p lá»‹ch gang nhÆ° má»™t nhÃ³m chuá»—i. CÃ¡c chuá»—i trong má»™t nhÃ³m chuá»—i luÃ´n Ä‘Æ°á»£c preempt hoáº·c láº­p lá»‹ch láº¡i cÃ¹ng nhau do kháº£ nÄƒng chia sáº» bá»™ nhá»› qua cÃ¡c chuá»—i Ä‘Ã³. Äá»ƒ tráº£ lá»i cÃ¢u há»i thá»© hai vá» cÃ¡ch khÃ´i phá»¥c má»™t block Ä‘Ã£ evict, chÃºng tÃ´i xem xÃ©t hai ká»¹ thuáº­t:

Swapping. ÄÃ¢y lÃ  ká»¹ thuáº­t cá»• Ä‘iá»ƒn Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi háº§u háº¿t cÃ¡c triá»ƒn khai bá»™ nhá»› áº£o sao chÃ©p cÃ¡c trang Ä‘Ã£ evict vÃ o khÃ´ng gian swap trÃªn Ä‘Ä©a. Trong trÆ°á»ng há»£p cá»§a chÃºng ta, chÃºng tÃ´i sao chÃ©p cÃ¡c block Ä‘Ã£ evict vÃ o bá»™ nhá»› CPU. NhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 4, bÃªn cáº¡nh GPU block allocator, vLLM bao gá»“m má»™t CPU block allocator Ä‘á»ƒ quáº£n lÃ½ cÃ¡c physical block Ä‘Æ°á»£c swap vÃ o CPU RAM. Khi vLLM cáº¡n kiá»‡t cÃ¡c physical block tá»± do cho cÃ¡c token má»›i, nÃ³ chá»n má»™t táº­p há»£p chuá»—i Ä‘á»ƒ evict vÃ  chuyá»ƒn KV cache cá»§a chÃºng Ä‘áº¿n CPU. Khi nÃ³ preempt má»™t chuá»—i vÃ  evict cÃ¡c block cá»§a nÃ³, vLLM ngá»«ng cháº¥p nháº­n cÃ¡c yÃªu cáº§u má»›i cho Ä‘áº¿n khi táº¥t cáº£ cÃ¡c chuá»—i bá»‹ preempt Ä‘Æ°á»£c hoÃ n thÃ nh. Khi má»™t yÃªu cáº§u hoÃ n thÃ nh, cÃ¡c block cá»§a nÃ³ Ä‘Æ°á»£c giáº£i phÃ³ng khá»i bá»™ nhá»›, vÃ  cÃ¡c block cá»§a má»™t chuá»—i bá»‹ preempt Ä‘Æ°á»£c Ä‘Æ°a trá»Ÿ láº¡i Ä‘á»ƒ tiáº¿p tá»¥c xá»­ lÃ½ chuá»—i Ä‘Ã³. LÆ°u Ã½ ráº±ng vá»›i thiáº¿t káº¿ nÃ y, sá»‘ lÆ°á»£ng block Ä‘Æ°á»£c swap vÃ o CPU RAM khÃ´ng bao giá» vÆ°á»£t quÃ¡ sá»‘ lÆ°á»£ng tá»•ng physical block trong GPU RAM, vÃ¬ váº­y khÃ´ng gian swap trÃªn CPU RAM Ä‘Æ°á»£c giá»›i háº¡n bá»Ÿi bá»™ nhá»› GPU Ä‘Æ°á»£c cáº¥p phÃ¡t cho KV cache.

TÃ­nh toÃ¡n láº¡i. Trong trÆ°á»ng há»£p nÃ y, chÃºng tÃ´i Ä‘Æ¡n giáº£n tÃ­nh toÃ¡n láº¡i KV cache khi cÃ¡c chuá»—i bá»‹ preempt Ä‘Æ°á»£c láº­p lá»‹ch láº¡i. LÆ°u Ã½ ráº±ng Ä‘á»™ trá»… tÃ­nh toÃ¡n láº¡i cÃ³ thá»ƒ tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i Ä‘á»™ trá»… ban Ä‘áº§u, vÃ¬ cÃ¡c token Ä‘Æ°á»£c táº¡o trong quÃ¡ trÃ¬nh giáº£i mÃ£ cÃ³ thá»ƒ Ä‘Æ°á»£c ná»‘i vá»›i prompt ngÆ°á»i dÃ¹ng ban Ä‘áº§u nhÆ° má»™t prompt má»›iâ€”KV cache cá»§a chÃºng táº¡i táº¥t cáº£ vá»‹ trÃ­ cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o trong má»™t iteration giai Ä‘oáº¡n prompt.

Hiá»‡u suáº¥t cá»§a swapping vÃ  tÃ­nh toÃ¡n láº¡i phá»¥ thuá»™c vÃ o bÄƒng thÃ´ng giá»¯a CPU RAM vÃ  bá»™ nhá»› GPU vÃ  sá»©c máº¡nh tÃ­nh toÃ¡n cá»§a GPU. ChÃºng tÃ´i kiá»ƒm tra tá»‘c Ä‘á»™ cá»§a swapping vÃ  tÃ­nh toÃ¡n láº¡i trong Â§7.3.

4.6 Thá»±c Thi PhÃ¢n TÃ¡n
Nhiá»u LLM cÃ³ kÃ­ch thÆ°á»›c tham sá»‘ vÆ°á»£t quÃ¡ dung lÆ°á»£ng cá»§a má»™t GPU Ä‘Æ¡n [5,9]. Do Ä‘Ã³, cáº§n thiáº¿t pháº£i phÃ¢n chia chÃºng qua cÃ¡c GPU phÃ¢n tÃ¡n vÃ  thá»±c thi chÃºng theo cÃ¡ch song song mÃ´ hÃ¬nh [28,63]. Äiá»u nÃ y Ä‘Ã²i há»i má»™t trÃ¬nh quáº£n lÃ½ bá»™ nhá»› cÃ³ kháº£ nÄƒng xá»­ lÃ½ bá»™ nhá»› phÃ¢n tÃ¡n. vLLM hiá»‡u quáº£ trong mÃ´i trÆ°á»ng phÃ¢n tÃ¡n báº±ng cÃ¡ch há»— trá»£ chiáº¿n lÆ°á»£c tensor model parallelism kiá»ƒu Megatron-LM Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trÃªn Transformer [47]. Chiáº¿n lÆ°á»£c nÃ y tuÃ¢n theo lá»‹ch trÃ¬nh thá»±c thi SPMD (Single Program Multiple Data), trong Ä‘Ã³ cÃ¡c lá»›p tuyáº¿n tÃ­nh Ä‘Æ°á»£c phÃ¢n chia Ä‘á»ƒ thá»±c hiá»‡n phÃ©p nhÃ¢n ma tráº­n theo khá»‘i, vÃ  cÃ¡c GPU liÃªn tá»¥c Ä‘á»“ng bá»™ hÃ³a káº¿t quáº£ trung gian thÃ´ng qua phÃ©p toÃ¡n all-reduce. Cá»¥ thá»ƒ, toÃ¡n tá»­ attention Ä‘Æ°á»£c chia trÃªn chiá»u attention head, má»—i quy trÃ¬nh SPMD chÄƒm sÃ³c má»™t táº­p há»£p con cÃ¡c attention head trong multi-head attention.

ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng ngay cáº£ vá»›i viá»‡c thá»±c thi song song mÃ´ hÃ¬nh, má»—i model shard váº«n xá»­ lÃ½ cÃ¹ng táº­p há»£p token Ä‘áº§u vÃ o, do Ä‘Ã³ yÃªu cáº§u KV Cache cho cÃ¹ng cÃ¡c vá»‹ trÃ­. Do Ä‘Ã³, vLLM cÃ³ má»™t KV cache manager duy nháº¥t trong scheduler táº­p trung, nhÆ° trong HÃ¬nh 4. CÃ¡c worker GPU khÃ¡c nhau chia sáº» manager, cÅ©ng nhÆ° Ã¡nh xáº¡ tá»« logical block Ä‘áº¿n physical block. Ãnh xáº¡ chung nÃ y cho phÃ©p cÃ¡c worker GPU thá»±c thi mÃ´ hÃ¬nh vá»›i cÃ¡c physical block Ä‘Æ°á»£c cung cáº¥p bá»Ÿi scheduler cho má»—i yÃªu cáº§u Ä‘áº§u vÃ o. Máº·c dÃ¹ má»—i worker GPU cÃ³ cÃ¹ng ID physical block, má»™t worker chá»‰ lÆ°u trá»¯ má»™t pháº§n cá»§a KV cache cho cÃ¡c attention head tÆ°Æ¡ng á»©ng cá»§a nÃ³.

Trong má»—i bÆ°á»›c, scheduler Ä‘áº§u tiÃªn chuáº©n bá»‹ thÃ´ng Ä‘iá»‡p vá»›i ID token Ä‘áº§u vÃ o cho má»—i yÃªu cáº§u trong batch, cÅ©ng nhÆ° block table cho má»—i yÃªu cáº§u. Tiáº¿p theo, scheduler phÃ¡t sÃ³ng thÃ´ng Ä‘iá»‡p Ä‘iá»u khiá»ƒn nÃ y Ä‘áº¿n cÃ¡c worker GPU. Sau Ä‘Ã³, cÃ¡c worker GPU báº¯t Ä‘áº§u thá»±c thi mÃ´ hÃ¬nh vá»›i cÃ¡c ID token Ä‘áº§u vÃ o. Trong cÃ¡c lá»›p attention, cÃ¡c worker GPU Ä‘á»c KV cache theo block table trong thÃ´ng Ä‘iá»‡p Ä‘iá»u khiá»ƒn. Trong quÃ¡ trÃ¬nh thá»±c thi, cÃ¡c worker GPU Ä‘á»“ng bá»™ hÃ³a káº¿t quáº£ trung gian vá»›i primitive giao tiáº¿p all-reduce mÃ  khÃ´ng cáº§n sá»± Ä‘iá»u phá»‘i cá»§a scheduler, nhÆ° trong [47]. Cuá»‘i cÃ¹ng, cÃ¡c worker GPU gá»­i cÃ¡c token Ä‘Æ°á»£c láº¥y máº«u cá»§a iteration nÃ y trá»Ÿ láº¡i scheduler. TÃ³m láº¡i, cÃ¡c worker GPU khÃ´ng cáº§n Ä‘á»“ng bá»™ hÃ³a vá» quáº£n lÃ½ bá»™ nhá»› vÃ¬ chÃºng chá»‰ cáº§n nháº­n táº¥t cáº£ thÃ´ng tin quáº£n lÃ½ bá»™ nhá»› á»Ÿ Ä‘áº§u má»—i iteration giáº£i mÃ£ cÃ¹ng vá»›i cÃ¡c Ä‘áº§u vÃ o bÆ°á»›c.

5 Triá»ƒn Khai
vLLM lÃ  má»™t há»‡ thá»‘ng phá»¥c vá»¥ end-to-end vá»›i frontend FastAPI [15] vÃ  cÃ´ng cá»¥ suy luáº­n dá»±a trÃªn GPU. Frontend má»Ÿ rá»™ng giao diá»‡n OpenAI API [34], cho phÃ©p ngÆ°á»i dÃ¹ng tÃ¹y chá»‰nh cÃ¡c tham sá»‘ láº¥y máº«u cho má»—i yÃªu cáº§u, nhÆ° Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a vÃ  beam width ğ‘˜. CÃ´ng cá»¥ vLLM Ä‘Æ°á»£c viáº¿t báº±ng 8.5K dÃ²ng Python vÃ  2K dÃ²ng mÃ£ C++/CUDA. ChÃºng tÃ´i phÃ¡t triá»ƒn cÃ¡c thÃ nh pháº§n liÃªn quan Ä‘áº¿n Ä‘iá»u khiá»ƒn bao gá»“m scheduler vÃ  block manager báº±ng Python trong khi phÃ¡t triá»ƒn cÃ¡c kernel CUDA tÃ¹y chá»‰nh cho cÃ¡c phÃ©p toÃ¡n chÃ­nh nhÆ° PagedAttention. Äá»‘i vá»›i model executor, chÃºng tÃ´i triá»ƒn khai cÃ¡c LLM phá»• biáº¿n nhÆ° GPT [5], OPT [62], vÃ  LLaMA [52] báº±ng PyTorch [39] vÃ  Transformers [58]. ChÃºng tÃ´i sá»­ dá»¥ng NCCL [32] Ä‘á»ƒ giao tiáº¿p tensor qua cÃ¡c worker GPU phÃ¢n tÃ¡n.

[HÃ¬nh 11: PhÃ¢n bá»‘ Ä‘á»™ dÃ i Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a táº­p dá»¯ liá»‡u (a) ShareGPT vÃ  (b) Alpaca.]

5.1 Tá»‘i Æ¯u HÃ³a Cáº¥p Kernel
VÃ¬ PagedAttention giá»›i thiá»‡u cÃ¡c máº«u truy cáº­p bá»™ nhá»› khÃ´ng Ä‘Æ°á»£c há»— trá»£ hiá»‡u quáº£ bá»Ÿi cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t sá»‘ kernel GPU Ä‘á»ƒ tá»‘i Æ°u hÃ³a nÃ³. (1) Fused reshape vÃ  block write. Trong má»—i lá»›p Transformer, KV cache má»›i Ä‘Æ°á»£c chia thÃ nh cÃ¡c block, Ä‘Æ°á»£c reshape thÃ nh bá»‘ cá»¥c bá»™ nhá»› Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho viá»‡c Ä‘á»c block, sau Ä‘Ã³ Ä‘Æ°á»£c lÆ°u táº¡i cÃ¡c vá»‹ trÃ­ Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh bá»Ÿi block table. Äá»ƒ giáº£m thiá»ƒu chi phÃ­ khá»Ÿi cháº¡y kernel, chÃºng tÃ´i há»£p nháº¥t chÃºng thÃ nh má»™t kernel duy nháº¥t. (2) Fusing block read vÃ  attention. ChÃºng tÃ´i Ä‘iá»u chá»‰nh kernel attention trong FasterTransformer [31] Ä‘á»ƒ Ä‘á»c KV cache theo block table vÃ  thá»±c hiá»‡n cÃ¡c phÃ©p toÃ¡n attention ngay láº­p tá»©c. Äá»ƒ Ä‘áº£m báº£o truy cáº­p bá»™ nhá»› coalesced, chÃºng tÃ´i gÃ¡n má»™t GPU warp Ä‘á»ƒ Ä‘á»c má»—i block. HÆ¡n ná»¯a, chÃºng tÃ´i thÃªm há»— trá»£ cho Ä‘á»™ dÃ i chuá»—i biáº¿n Ä‘á»•i trong má»™t batch yÃªu cáº§u. (3) Fused block copy. CÃ¡c phÃ©p toÃ¡n sao chÃ©p block, Ä‘Æ°á»£c phÃ¡t hÃ nh bá»Ÿi cÆ¡ cháº¿ copy-on-write, cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng trÃªn cÃ¡c block khÃ´ng liÃªn tá»¥c. Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n nhiá»u lá»i gá»i cá»§a cÃ¡c chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u nhá» náº¿u chÃºng ta sá»­ dá»¥ng API cudaMemcpyAsync. Äá»ƒ giáº£m thiá»ƒu chi phÃ­, chÃºng tÃ´i triá»ƒn khai má»™t kernel gá»™p cÃ¡c phÃ©p toÃ¡n sao chÃ©p cho cÃ¡c block khÃ¡c nhau thÃ nh má»™t láº§n khá»Ÿi cháº¡y kernel duy nháº¥t.

5.2 Há»— Trá»£ CÃ¡c Thuáº­t ToÃ¡n Giáº£i MÃ£ KhÃ¡c Nhau
vLLM triá»ƒn khai cÃ¡c thuáº­t toÃ¡n giáº£i mÃ£ khÃ¡c nhau báº±ng ba phÆ°Æ¡ng phÃ¡p chÃ­nh: fork, append, vÃ  free. PhÆ°Æ¡ng phÃ¡p fork táº¡o má»™t chuá»—i má»›i tá»« má»™t chuá»—i hiá»‡n táº¡i. PhÆ°Æ¡ng phÃ¡p append thÃªm má»™t token má»›i vÃ o chuá»—i. Cuá»‘i cÃ¹ng, phÆ°Æ¡ng phÃ¡p free xÃ³a chuá»—i. VÃ­ dá»¥, trong láº¥y máº«u song song, vLLM táº¡o nhiá»u chuá»—i Ä‘áº§u ra tá»« chuá»—i Ä‘áº§u vÃ o duy nháº¥t báº±ng phÆ°Æ¡ng phÃ¡p fork. Sau Ä‘Ã³ nÃ³ thÃªm cÃ¡c token má»›i vÃ o cÃ¡c chuá»—i nÃ y trong má»—i iteration vá»›i append, vÃ  xÃ³a cÃ¡c chuá»—i Ä‘Ã¡p á»©ng Ä‘iá»u kiá»‡n dá»«ng báº±ng free. CÃ¹ng chiáº¿n lÆ°á»£c cÅ©ng Ä‘Æ°á»£c Ã¡p dá»¥ng trong beam search vÃ  chia sáº» tiá»n tá»‘ bá»Ÿi vLLM. ChÃºng tÃ´i tin ráº±ng cÃ¡c thuáº­t toÃ¡n giáº£i mÃ£ tÆ°Æ¡ng lai cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c há»— trá»£ báº±ng cÃ¡ch káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y.

6 ÄÃ¡nh GiÃ¡
Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a vLLM dÆ°á»›i nhiá»u khá»‘i lÆ°á»£ng cÃ´ng viá»‡c khÃ¡c nhau.

--- TRANG 5 ---
[Báº£ng 1: KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  cáº¥u hÃ¬nh mÃ¡y chá»§.]

6.1 Thiáº¿t Láº­p ThÃ­ Nghiá»‡m
Cáº¥u hÃ¬nh mÃ´ hÃ¬nh vÃ  mÃ¡y chá»§. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh OPT [62] vá»›i 13B, 66B, vÃ  175B tham sá»‘ vÃ  LLaMA [52] vá»›i 13B tham sá»‘ cho Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i. 13B vÃ  66B lÃ  cÃ¡c kÃ­ch thÆ°á»›c phá»• biáº¿n cho LLM nhÆ° thá»ƒ hiá»‡n trong báº£ng xáº¿p háº¡ng LLM [38], trong khi 175B lÃ  kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh GPT-3 ná»•i tiáº¿ng [5]. Äá»‘i vá»›i táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c instance A2 vá»›i GPU NVIDIA A100 trÃªn Google Cloud Platform. KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh chi tiáº¿t vÃ  cáº¥u hÃ¬nh mÃ¡y chá»§ Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 1.

Khá»‘i lÆ°á»£ng cÃ´ng viá»‡c. ChÃºng tÃ´i tá»•ng há»£p khá»‘i lÆ°á»£ng cÃ´ng viá»‡c dá»±a trÃªn táº­p dá»¯ liá»‡u ShareGPT [51] vÃ  Alpaca [50], chá»©a vÄƒn báº£n Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a cÃ¡c dá»‹ch vá»¥ LLM thá»±c táº¿. Táº­p dá»¯ liá»‡u ShareGPT lÃ  má»™t bá»™ sÆ°u táº­p cÃ¡c cuá»™c trÃ² chuyá»‡n Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng chia sáº» vá»›i ChatGPT [35]. Táº­p dá»¯ liá»‡u Alpaca lÃ  má»™t táº­p dá»¯ liá»‡u hÆ°á»›ng dáº«n Ä‘Æ°á»£c táº¡o bá»Ÿi GPT-3.5 vá»›i self-instruct [57]. ChÃºng tÃ´i tokenize cÃ¡c táº­p dá»¯ liá»‡u vÃ  sá»­ dá»¥ng Ä‘á»™ dÃ i Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a chÃºng Ä‘á»ƒ tá»•ng há»£p cÃ¡c yÃªu cáº§u khÃ¡ch hÃ ng. NhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 11, táº­p dá»¯ liá»‡u ShareGPT cÃ³ prompt Ä‘áº§u vÃ o dÃ i hÆ¡n 8.4Ã— vÃ  Ä‘áº§u ra dÃ i hÆ¡n 5.8Ã— trung bÃ¬nh so vá»›i táº­p dá»¯ liá»‡u Alpaca, vá»›i Ä‘á»™ biáº¿n thiÃªn cao hÆ¡n. VÃ¬ cÃ¡c táº­p dá»¯ liá»‡u nÃ y khÃ´ng bao gá»“m timestamps, chÃºng tÃ´i táº¡o thá»i gian Ä‘áº¿n yÃªu cáº§u báº±ng phÃ¢n phá»‘i Poisson vá»›i cÃ¡c tá»· lá»‡ yÃªu cáº§u khÃ¡c nhau.

Baseline 1: FasterTransformer. FasterTransformer [31] lÃ  má»™t cÃ´ng cá»¥ suy luáº­n phÃ¢n tÃ¡n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cao cho Ä‘á»™ trá»…. VÃ¬ FasterTransformer khÃ´ng cÃ³ scheduler riÃªng, chÃºng tÃ´i triá»ƒn khai má»™t scheduler tÃ¹y chá»‰nh vá»›i cÆ¡ cháº¿ batching Ä‘á»™ng tÆ°Æ¡ng tá»± nhÆ° cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ hiá»‡n táº¡i nhÆ° Triton [30]. Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c batch tá»‘i Ä‘a ğµ cÃ ng lá»›n cÃ ng tá»‘t cho má»—i thÃ­ nghiá»‡m, theo dung lÆ°á»£ng bá»™ nhá»› GPU. Scheduler láº¥y tá»‘i Ä‘a ğµ sá»‘ yÃªu cáº§u Ä‘áº¿n sá»›m nháº¥t vÃ  gá»­i batch Ä‘áº¿n FasterTransformer Ä‘á»ƒ xá»­ lÃ½.

Baseline 2: Orca. Orca [60] lÃ  má»™t há»‡ thá»‘ng phá»¥c vá»¥ LLM tiÃªn tiáº¿n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho thÃ´ng lÆ°á»£ng. VÃ¬ Orca khÃ´ng cÃ³ sáºµn cÃ´ng khai Ä‘á»ƒ sá»­ dá»¥ng, chÃºng tÃ´i triá»ƒn khai phiÃªn báº£n Orca cá»§a riÃªng mÃ¬nh. ChÃºng tÃ´i giáº£ Ä‘á»‹nh Orca sá»­ dá»¥ng thuáº­t toÃ¡n buddy allocation Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»‹a chá»‰ bá»™ nhá»› Ä‘á»ƒ lÆ°u trá»¯ KV cache. ChÃºng tÃ´i triá»ƒn khai ba phiÃªn báº£n cá»§a Orca dá»±a trÃªn má»©c Ä‘á»™ nÃ³ over-reserve khÃ´ng gian cho Ä‘áº§u ra yÃªu cáº§u:

â€¢ Orca (Oracle). ChÃºng tÃ´i giáº£ Ä‘á»‹nh há»‡ thá»‘ng cÃ³ kiáº¿n thá»©c vá» Ä‘á»™ dÃ i cá»§a cÃ¡c Ä‘áº§u ra sáº½ thá»±c sá»± Ä‘Æ°á»£c táº¡o cho cÃ¡c yÃªu cáº§u. Äiá»u nÃ y thá»ƒ hiá»‡n hiá»‡u suáº¥t giá»›i háº¡n trÃªn cá»§a Orca, khÃ´ng thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c trong thá»±c táº¿.

â€¢ Orca (Pow2). ChÃºng tÃ´i giáº£ Ä‘á»‹nh há»‡ thá»‘ng over-reserve khÃ´ng gian cho Ä‘áº§u ra tá»‘i Ä‘a 2Ã—. VÃ­ dá»¥, náº¿u Ä‘á»™ dÃ i Ä‘áº§u ra thá»±c lÃ  25, nÃ³ dÃ nh riÃªng 32 vá»‹ trÃ­ cho Ä‘áº§u ra.

â€¢ Orca (Max). ChÃºng tÃ´i giáº£ Ä‘á»‹nh há»‡ thá»‘ng luÃ´n dÃ nh riÃªng khÃ´ng gian lÃªn Ä‘áº¿n Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh, tá»©c lÃ  2048 token.

CÃ¡c chá»‰ sá»‘ chÃ­nh. ChÃºng tÃ´i táº­p trung vÃ o thÃ´ng lÆ°á»£ng phá»¥c vá»¥. Cá»¥ thá»ƒ, sá»­ dá»¥ng cÃ¡c khá»‘i lÆ°á»£ng cÃ´ng viá»‡c vá»›i cÃ¡c tá»· lá»‡ yÃªu cáº§u khÃ¡c nhau, chÃºng tÃ´i Ä‘o Ä‘á»™ trá»… chuáº©n hÃ³a cá»§a cÃ¡c há»‡ thá»‘ng, trung bÃ¬nh cá»§a Ä‘á»™ trá»… end-to-end cá»§a má»—i yÃªu cáº§u chia cho Ä‘á»™ dÃ i Ä‘áº§u ra cá»§a nÃ³, nhÆ° trong Orca [60]. Má»™t há»‡ thá»‘ng phá»¥c vá»¥ thÃ´ng lÆ°á»£ng cao nÃªn duy trÃ¬ Ä‘á»™ trá»… chuáº©n hÃ³a tháº¥p chá»‘ng láº¡i tá»· lá»‡ yÃªu cáº§u cao. Äá»‘i vá»›i háº§u háº¿t cÃ¡c thÃ­ nghiá»‡m, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c há»‡ thá»‘ng vá»›i trace 1 giá». NhÆ° má»™t ngoáº¡i lá»‡, chÃºng tÃ´i sá»­ dá»¥ng trace 15 phÃºt cho mÃ´ hÃ¬nh OPT-175B do giá»›i háº¡n chi phÃ­.

6.2 Láº¥y Máº«u CÆ¡ Báº£n
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a vLLM vá»›i láº¥y máº«u cÆ¡ báº£n (má»™t máº«u cho má»—i yÃªu cáº§u) trÃªn ba mÃ´ hÃ¬nh vÃ  hai táº­p dá»¯ liá»‡u. HÃ ng Ä‘áº§u tiÃªn cá»§a HÃ¬nh 12 thá»ƒ hiá»‡n káº¿t quáº£ trÃªn táº­p dá»¯ liá»‡u ShareGPT. CÃ¡c Ä‘Æ°á»ng cong minh há»a ráº±ng khi tá»· lá»‡ yÃªu cáº§u tÄƒng, Ä‘á»™ trá»… ban Ä‘áº§u tÄƒng vá»›i tá»‘c Ä‘á»™ dáº§n dáº§n nhÆ°ng sau Ä‘Ã³ Ä‘á»™t nhiÃªn bÃ¹ng ná»•. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho thá»±c táº¿ ráº±ng khi tá»· lá»‡ yÃªu cáº§u vÆ°á»£t quÃ¡ kháº£ nÄƒng cá»§a há»‡ thá»‘ng phá»¥c vá»¥, Ä‘á»™ dÃ i hÃ ng Ä‘á»£i tiáº¿p tá»¥c tÄƒng vÃ´ háº¡n vÃ  Ä‘á»™ trá»… cá»§a cÃ¡c yÃªu cáº§u cÅ©ng váº­y.

TrÃªn táº­p dá»¯ liá»‡u ShareGPT, vLLM cÃ³ thá»ƒ duy trÃ¬ tá»· lá»‡ yÃªu cáº§u cao hÆ¡n 1.7Ã—â€“2.7Ã— so vá»›i Orca (Oracle) vÃ  2.7Ã—â€“8Ã— so vá»›i Orca (Max), trong khi duy trÃ¬ Ä‘á»™ trá»… tÆ°Æ¡ng tá»±. Äiá»u nÃ y lÃ  do PagedAttention cá»§a vLLM cÃ³ thá»ƒ quáº£n lÃ½ hiá»‡u quáº£ viá»‡c sá»­ dá»¥ng bá»™ nhá»› vÃ  do Ä‘Ã³ cho phÃ©p batching nhiá»u yÃªu cáº§u hÆ¡n Orca. VÃ­ dá»¥, nhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 13a, Ä‘á»‘i vá»›i OPT-13B vLLM xá»­ lÃ½ nhiá»u hÆ¡n 2.2Ã— yÃªu cáº§u cÃ¹ng má»™t lÃºc so vá»›i Orca (Oracle) vÃ  nhiá»u hÆ¡n 4.3Ã— yÃªu cáº§u so vá»›i Orca (Max). So vá»›i FasterTransformer, vLLM cÃ³ thá»ƒ duy trÃ¬ tá»· lá»‡ yÃªu cáº§u cao hÆ¡n lÃªn Ä‘áº¿n 22Ã—, vÃ¬ FasterTransformer khÃ´ng sá»­ dá»¥ng cÆ¡ cháº¿ láº­p lá»‹ch chi tiáº¿t vÃ  quáº£n lÃ½ bá»™ nhá»› khÃ´ng hiá»‡u quáº£ nhÆ° Orca (Max).

[HÃ¬nh 12: Táº¡o chuá»—i Ä‘Æ¡n vá»›i cÃ¡c mÃ´ hÃ¬nh OPT trÃªn táº­p dá»¯ liá»‡u ShareGPT vÃ  Alpaca]

[HÃ¬nh 13: Sá»‘ lÆ°á»£ng yÃªu cáº§u Ä‘Æ°á»£c batch trung bÃ¬nh khi phá»¥c vá»¥ OPT-13B cho trace ShareGPT (2 reqs/s) vÃ  Alpaca (30 reqs/s).]

HÃ ng thá»© hai cá»§a HÃ¬nh 12 vÃ  HÃ¬nh 13b thá»ƒ hiá»‡n káº¿t quáº£ trÃªn táº­p dá»¯ liá»‡u Alpaca, theo xu hÆ°á»›ng tÆ°Æ¡ng tá»± nhÆ° táº­p dá»¯ liá»‡u ShareGPT. Má»™t ngoáº¡i lá»‡ lÃ  HÃ¬nh 12 (f), trong Ä‘Ã³ lá»£i tháº¿ cá»§a vLLM so vá»›i Orca (Oracle) vÃ  Orca (Pow2) Ã­t rÃµ rá»‡t hÆ¡n. Äiá»u nÃ y lÃ  do cáº¥u hÃ¬nh mÃ´ hÃ¬nh vÃ  mÃ¡y chá»§ cho OPT-175B (Báº£ng 1) cho phÃ©p khÃ´ng gian bá»™ nhá»› GPU lá»›n cÃ³ sáºµn Ä‘á»ƒ lÆ°u trá»¯ KV cache, trong khi táº­p dá»¯ liá»‡u Alpaca cÃ³ cÃ¡c chuá»—i ngáº¯n. Trong thiáº¿t láº­p nÃ y, Orca (Oracle) vÃ  Orca (Pow2) cÅ©ng cÃ³ thá»ƒ batch má»™t sá»‘ lÆ°á»£ng lá»›n yÃªu cáº§u báº¥t cháº¥p sá»± khÃ´ng hiá»‡u quáº£ trong quáº£n lÃ½ bá»™ nhá»› cá»§a chÃºng. Káº¿t quáº£ lÃ , hiá»‡u suáº¥t cá»§a cÃ¡c há»‡ thá»‘ng trá»Ÿ thÃ nh bá»‹ rÃ ng buá»™c bá»Ÿi tÃ­nh toÃ¡n thay vÃ¬ bá»‹ rÃ ng buá»™c bá»Ÿi bá»™ nhá»›.

6.3 Láº¥y Máº«u Song Song vÃ  Beam Search
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a viá»‡c chia sáº» bá»™ nhá»› trong PagedAttention vá»›i hai phÆ°Æ¡ng phÃ¡p láº¥y máº«u phá»• biáº¿n: láº¥y máº«u song song vÃ  beam search. Trong láº¥y máº«u song song, táº¥t cáº£ cÃ¡c chuá»—i song song trong má»™t yÃªu cáº§u cÃ³ thá»ƒ chia sáº» KV cache cho prompt. NhÆ° thá»ƒ hiá»‡n trong hÃ ng Ä‘áº§u tiÃªn cá»§a HÃ¬nh 14, vá»›i sá»‘ lÆ°á»£ng chuá»—i lá»›n hÆ¡n Ä‘á»ƒ láº¥y máº«u, vLLM mang láº¡i cáº£i thiá»‡n nhiá»u hÆ¡n so vá»›i cÃ¡c baseline Orca. TÆ°Æ¡ng tá»±, hÃ ng thá»© hai cá»§a HÃ¬nh 14 thá»ƒ hiá»‡n káº¿t quáº£ cho beam search vá»›i cÃ¡c beam width khÃ¡c nhau. VÃ¬ beam search cho phÃ©p chia sáº» nhiá»u hÆ¡n, vLLM thá»ƒ hiá»‡n lá»£i Ã­ch hiá»‡u suáº¥t tháº­m chÃ­ lá»›n hÆ¡n. Cáº£i thiá»‡n cá»§a vLLM so vá»›i Orca (Oracle) trÃªn OPT-13B vÃ  táº­p dá»¯ liá»‡u Alpaca tÄƒng tá»« 1.3Ã— trong láº¥y máº«u cÆ¡ báº£n lÃªn 2.3Ã— trong beam search vá»›i width lÃ  6.

[HÃ¬nh 14: Táº¡o song song vÃ  beam search vá»›i OPT-13B trÃªn táº­p dá»¯ liá»‡u Alpaca.]

[HÃ¬nh 15: LÆ°á»£ng tiáº¿t kiá»‡m bá»™ nhá»› trung bÃ¬nh tá»« viá»‡c chia sáº» KV block, khi phá»¥c vá»¥ OPT-13B cho trace Alpaca.]

HÃ¬nh 15 váº½ lÆ°á»£ng tiáº¿t kiá»‡m bá»™ nhá»›, Ä‘Æ°á»£c tÃ­nh báº±ng sá»‘ block chÃºng tÃ´i tiáº¿t kiá»‡m Ä‘Æ°á»£c báº±ng cÃ¡ch chia sáº» chia cho sá»‘ block tá»•ng cá»™ng mÃ  khÃ´ng cÃ³ chia sáº». ChÃºng tÃ´i thá»ƒ hiá»‡n 6.1% - 9.8% tiáº¿t kiá»‡m bá»™ nhá»› trÃªn láº¥y máº«u song song vÃ  37.6% - 55.2% trÃªn beam search. Trong cÃ¹ng cÃ¡c thÃ­ nghiá»‡m vá»›i táº­p dá»¯ liá»‡u ShareGPT, chÃºng tÃ´i tháº¥y 16.2% - 30.5% tiáº¿t kiá»‡m bá»™ nhá»› trÃªn láº¥y máº«u song song vÃ  44.3% - 66.3% trÃªn beam search.

6.4 Tiá»n Tá»‘ Chia Sáº»
ChÃºng tÃ´i khÃ¡m phÃ¡ hiá»‡u quáº£ cá»§a vLLM cho trÆ°á»ng há»£p má»™t tiá»n tá»‘ Ä‘Æ°á»£c chia sáº» giá»¯a cÃ¡c prompt Ä‘áº§u vÃ o khÃ¡c nhau, nhÆ° minh há»a trong HÃ¬nh 10. Äá»‘i vá»›i mÃ´ hÃ¬nh, chÃºng tÃ´i sá»­ dá»¥ng LLaMA-13B [52], lÃ  Ä‘a ngÃ´n ngá»¯. Äá»‘i vá»›i khá»‘i lÆ°á»£ng cÃ´ng viá»‡c, chÃºng tÃ´i sá»­ dá»¥ng táº­p dá»¯ liá»‡u dá»‹ch WMT16 [4] Tiáº¿ng Anh-sang-Tiáº¿ng Äá»©c vÃ  tá»•ng há»£p hai tiá»n tá»‘ bao gá»“m má»™t hÆ°á»›ng dáº«n vÃ  má»™t vÃ i vÃ­ dá»¥ dá»‹ch. Tiá»n tá»‘ Ä‘áº§u tiÃªn bao gá»“m má»™t vÃ­ dá»¥ duy nháº¥t (tá»©c lÃ  one-shot) trong khi tiá»n tá»‘ khÃ¡c bao gá»“m 5 vÃ­ dá»¥ (tá»©c lÃ  few-shot). NhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 16 (a), vLLM Ä‘áº¡t Ä‘Æ°á»£c thÃ´ng lÆ°á»£ng cao hÆ¡n 1.67Ã— so vá»›i Orca (Oracle) khi tiá»n tá»‘ one-shot Ä‘Æ°á»£c chia sáº». HÆ¡n ná»¯a, khi nhiá»u vÃ­ dá»¥ Ä‘Æ°á»£c chia sáº» hÆ¡n (HÃ¬nh 16 (b)), vLLM Ä‘áº¡t Ä‘Æ°á»£c thÃ´ng lÆ°á»£ng cao hÆ¡n 3.58Ã— so vá»›i Orca (Oracle).

[HÃ¬nh 16: Khá»‘i lÆ°á»£ng cÃ´ng viá»‡c dá»‹ch thuáº­t trong Ä‘Ã³ cÃ¡c prompt Ä‘áº§u vÃ o chia sáº» má»™t tiá»n tá»‘ chung. Tiá»n tá»‘ bao gá»“m (a) 1 vÃ­ dá»¥ vá»›i 80 token hoáº·c (b) 5 vÃ­ dá»¥ vá»›i 341 token.]

6.5 Chatbot
Chatbot [8,19,35] lÃ  má»™t trong nhá»¯ng á»©ng dá»¥ng quan trá»ng nháº¥t cá»§a LLM. Äá»ƒ triá»ƒn khai chatbot, chÃºng tÃ´i Ä‘á»ƒ mÃ´ hÃ¬nh táº¡o pháº£n há»“i báº±ng cÃ¡ch ná»‘i lá»‹ch sá»­ trÃ² chuyá»‡n vÃ  truy váº¥n ngÆ°á»i dÃ¹ng cuá»‘i cÃ¹ng thÃ nh má»™t prompt. ChÃºng tÃ´i tá»•ng há»£p lá»‹ch sá»­ trÃ² chuyá»‡n vÃ  truy váº¥n ngÆ°á»i dÃ¹ng báº±ng táº­p dá»¯ liá»‡u ShareGPT. Do Ä‘á»™ dÃ i ngá»¯ cáº£nh háº¡n cháº¿ cá»§a mÃ´ hÃ¬nh OPT-13B, chÃºng tÃ´i cáº¯t prompt Ä‘áº¿n 1024 token cuá»‘i cÃ¹ng vÃ  Ä‘á»ƒ mÃ´ hÃ¬nh táº¡o tá»‘i Ä‘a 1024 token. ChÃºng tÃ´i khÃ´ng lÆ°u trá»¯ KV cache giá»¯a cÃ¡c vÃ²ng trÃ² chuyá»‡n khÃ¡c nhau vÃ¬ viá»‡c nÃ y sáº½ chiáº¿m khÃ´ng gian cho cÃ¡c yÃªu cáº§u khÃ¡c giá»¯a cÃ¡c vÃ²ng trÃ² chuyá»‡n.

[HÃ¬nh 17: Hiá»‡u suáº¥t trÃªn khá»‘i lÆ°á»£ng cÃ´ng viá»‡c chatbot.]

HÃ¬nh 17 thá»ƒ hiá»‡n ráº±ng vLLM cÃ³ thá»ƒ duy trÃ¬ tá»· lá»‡ yÃªu cáº§u cao hÆ¡n 2Ã— so vá»›i ba baseline Orca. VÃ¬ táº­p dá»¯ liá»‡u ShareGPT chá»©a nhiá»u cuá»™c trÃ² chuyá»‡n dÃ i, cÃ¡c prompt Ä‘áº§u vÃ o cho háº§u háº¿t cÃ¡c yÃªu cáº§u cÃ³ 1024 token. Do thuáº­t toÃ¡n buddy allocation, cÃ¡c baseline Orca dÃ nh riÃªng khÃ´ng gian cho 1024 token cho Ä‘áº§u ra yÃªu cáº§u, báº¥t ká»ƒ cÃ¡ch chÃºng dá»± Ä‘oÃ¡n Ä‘á»™ dÃ i Ä‘áº§u ra. VÃ¬ lÃ½ do nÃ y, ba baseline Orca hoáº¡t Ä‘á»™ng tÆ°Æ¡ng tá»±. NgÆ°á»£c láº¡i, vLLM cÃ³ thá»ƒ xá»­ lÃ½ hiá»‡u quáº£ cÃ¡c prompt dÃ i, vÃ¬ PagedAttention giáº£i quyáº¿t váº¥n Ä‘á» phÃ¢n máº£nh vÃ  dÃ nh riÃªng bá»™ nhá»›.

7 NghiÃªn Cá»©u Ablation
Trong pháº§n nÃ y, chÃºng tÃ´i nghiÃªn cá»©u cÃ¡c khÃ­a cáº¡nh khÃ¡c nhau cá»§a vLLM vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c lá»±a chá»n thiáº¿t káº¿ chÃºng tÃ´i Ä‘Æ°a ra vá»›i cÃ¡c thÃ­ nghiá»‡m ablation.

7.1 Microbenchmark Kernel
Ãnh xáº¡ block Ä‘á»™ng trong PagedAttention áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t cá»§a cÃ¡c phÃ©p toÃ¡n GPU liÃªn quan Ä‘áº¿n KV cache Ä‘Æ°á»£c lÆ°u trá»¯, tá»©c lÃ  Ä‘á»c/ghi block vÃ  attention. So vá»›i cÃ¡c há»‡ thá»‘ng hiá»‡n táº¡i, cÃ¡c kernel GPU cá»§a chÃºng tÃ´i (Â§5) bao gá»“m chi phÃ­ bá»• sung cá»§a viá»‡c truy cáº­p block table, thá»±c thi cÃ¡c nhÃ¡nh bá»• sung, vÃ  xá»­ lÃ½ Ä‘á»™ dÃ i chuá»—i biáº¿n Ä‘á»•i. NhÆ° thá»ƒ hiá»‡n trong HÃ¬nh 18a, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n Ä‘á»™ trá»… kernel attention cao hÆ¡n 20â€“26%, so vá»›i triá»ƒn khai FasterTransformer Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cao. ChÃºng tÃ´i tin ráº±ng chi phÃ­ nÃ y nhá» vÃ¬ nÃ³ chá»‰ áº£nh hÆ°á»Ÿng Ä‘áº¿n toÃ¡n tá»­ attention chá»© khÃ´ng pháº£i cÃ¡c toÃ¡n tá»­ khÃ¡c trong mÃ´ hÃ¬nh, nhÆ° Linear. Báº¥t cháº¥p chi phÃ­, PagedAttention lÃ m cho vLLM vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i FasterTransformer trong hiá»‡u suáº¥t end-to-end (Â§6).

[HÃ¬nh 18: CÃ¡c thÃ­ nghiá»‡m ablation.]

7.2 TÃ¡c Äá»™ng cá»§a KÃ­ch ThÆ°á»›c Block
Viá»‡c lá»±a chá»n kÃ­ch thÆ°á»›c block cÃ³ thá»ƒ cÃ³ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n hiá»‡u suáº¥t cá»§a vLLM. Náº¿u kÃ­ch thÆ°á»›c block quÃ¡ nhá», vLLM cÃ³ thá»ƒ khÃ´ng táº­n dá»¥ng Ä‘áº§y Ä‘á»§ tÃ­nh song song cá»§a GPU Ä‘á»ƒ Ä‘á»c vÃ  xá»­ lÃ½ KV cache. Náº¿u kÃ­ch thÆ°á»›c block quÃ¡ lá»›n, phÃ¢n máº£nh bÃªn trong tÄƒng vÃ  xÃ¡c suáº¥t chia sáº» giáº£m.

Trong HÃ¬nh 18b, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a vLLM vá»›i cÃ¡c kÃ­ch thÆ°á»›c block khÃ¡c nhau, sá»­ dá»¥ng trace ShareGPT vÃ  Alpaca vá»›i láº¥y máº«u cÆ¡ báº£n dÆ°á»›i tá»· lá»‡ yÃªu cáº§u cá»‘ Ä‘á»‹nh. Trong trace ShareGPT, kÃ­ch thÆ°á»›c block tá»« 16 Ä‘áº¿n 128 dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t nháº¥t. Trong trace Alpaca, trong khi kÃ­ch thÆ°á»›c block 16 vÃ  32 hoáº¡t Ä‘á»™ng tá»‘t, cÃ¡c kÃ­ch thÆ°á»›c block lá»›n hÆ¡n lÃ m giáº£m Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t vÃ¬ cÃ¡c chuá»—i trá»Ÿ nÃªn ngáº¯n hÆ¡n kÃ­ch thÆ°á»›c block. Trong thá»±c táº¿, chÃºng tÃ´i tháº¥y ráº±ng kÃ­ch thÆ°á»›c block 16 Ä‘á»§ lá»›n Ä‘á»ƒ sá»­ dá»¥ng hiá»‡u quáº£ GPU vÃ  Ä‘á»§ nhá» Ä‘á»ƒ trÃ¡nh phÃ¢n máº£nh bÃªn trong Ä‘Ã¡ng ká»ƒ trong háº§u háº¿t cÃ¡c khá»‘i lÆ°á»£ng cÃ´ng viá»‡c. Theo Ä‘Ã³, vLLM Ä‘áº·t kÃ­ch thÆ°á»›c block máº·c Ä‘á»‹nh lÃ  16.

7.3 So SÃ¡nh TÃ­nh ToÃ¡n Láº¡i vÃ  Swapping
vLLM há»— trá»£ cáº£ tÃ­nh toÃ¡n láº¡i vÃ  swapping nhÆ° cÃ¡c cÆ¡ cháº¿ khÃ´i phá»¥c cá»§a nÃ³. Äá»ƒ hiá»ƒu sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a hai phÆ°Æ¡ng phÃ¡p, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t end-to-end vÃ  microbenchmark chi phÃ­ cá»§a chÃºng, nhÆ° trÃ¬nh bÃ y trong HÃ¬nh 19. Káº¿t quáº£ cá»§a chÃºng tÃ´i tiáº¿t lá»™ ráº±ng swapping gÃ¢y ra chi phÃ­ quÃ¡ má»©c vá»›i kÃ­ch thÆ°á»›c block nhá». Äiá»u nÃ y lÃ  do kÃ­ch thÆ°á»›c block nhá» thÆ°á»ng dáº«n Ä‘áº¿n nhiá»u chuyá»ƒn dá»¯ liá»‡u nhá» giá»¯a CPU vÃ  GPU, lÃ m háº¡n cháº¿ bÄƒng thÃ´ng PCIe hiá»‡u quáº£. NgÆ°á»£c láº¡i, chi phÃ­ cá»§a tÃ­nh toÃ¡n láº¡i váº«n khÃ´ng Ä‘á»•i qua cÃ¡c kÃ­ch thÆ°á»›c block khÃ¡c nhau, vÃ¬ tÃ­nh toÃ¡n láº¡i khÃ´ng sá»­ dá»¥ng cÃ¡c KV block. Do Ä‘Ã³, tÃ­nh toÃ¡n láº¡i hiá»‡u quáº£ hÆ¡n khi kÃ­ch thÆ°á»›c block nhá», trong khi swapping hiá»‡u quáº£ hÆ¡n khi kÃ­ch thÆ°á»›c block lá»›n, máº·c dÃ¹ chi phÃ­ tÃ­nh toÃ¡n láº¡i khÃ´ng bao giá» cao hÆ¡n 20% Ä‘á»™ trá»… cá»§a swapping. Äá»‘i vá»›i kÃ­ch thÆ°á»›c block trung bÃ¬nh tá»« 16 Ä‘áº¿n 64, hai phÆ°Æ¡ng phÃ¡p thá»ƒ hiá»‡n hiá»‡u suáº¥t end-to-end tÆ°Æ¡ng Ä‘Æ°Æ¡ng.

[HÃ¬nh 19: (a) Chi phÃ­ cá»§a tÃ­nh toÃ¡n láº¡i vÃ  swapping cho cÃ¡c kÃ­ch thÆ°á»›c block khÃ¡c nhau. (b) Hiá»‡u suáº¥t khi phá»¥c vá»¥ OPT-13B vá»›i trace ShareGPT á»Ÿ cÃ¹ng tá»· lá»‡ yÃªu cáº§u.]

8 Tháº£o Luáº­n
Ãp dá»¥ng ká»¹ thuáº­t bá»™ nhá»› áº£o vÃ  phÃ¢n trang cho cÃ¡c khá»‘i lÆ°á»£ng cÃ´ng viá»‡c GPU khÃ¡c. Ã tÆ°á»Ÿng vá» bá»™ nhá»› áº£o vÃ  phÃ¢n trang hiá»‡u quáº£ Ä‘á»ƒ quáº£n lÃ½ KV cache trong phá»¥c vá»¥ LLM vÃ¬ khá»‘i lÆ°á»£ng cÃ´ng viá»‡c yÃªu cáº§u cáº¥p phÃ¡t bá»™ nhá»› Ä‘á»™ng (vÃ¬ Ä‘á»™ dÃ i Ä‘áº§u ra khÃ´ng Ä‘Æ°á»£c biáº¿t trÆ°á»›c) vÃ  hiá»‡u suáº¥t cá»§a nÃ³ bá»‹ giá»›i háº¡n bá»Ÿi dung lÆ°á»£ng bá»™ nhá»› GPU. Tuy nhiÃªn, Ä‘iá»u nÃ y khÃ´ng Ä‘Ãºng chung cho má»i khá»‘i lÆ°á»£ng cÃ´ng viá»‡c GPU. VÃ­ dá»¥, trong huáº¥n luyá»‡n DNN, cÃ¡c hÃ¬nh dáº¡ng tensor thÆ°á»ng tÄ©nh, vÃ  do Ä‘Ã³ cáº¥p phÃ¡t bá»™ nhá»› cÃ³ thá»ƒ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a trÆ°á»›c thá»i gian. Äá»‘i vá»›i má»™t vÃ­ dá»¥ khÃ¡c, trong phá»¥c vá»¥ DNN khÃ´ng pháº£i LLM, sá»± gia tÄƒng hiá»‡u quáº£ bá»™ nhá»› cÃ³ thá»ƒ khÃ´ng dáº«n Ä‘áº¿n cáº£i thiá»‡n hiá»‡u suáº¥t nÃ o vÃ¬ hiá»‡u suáº¥t chá»§ yáº¿u bá»‹ rÃ ng buá»™c bá»Ÿi tÃ­nh toÃ¡n. Trong cÃ¡c tÃ¬nh huá»‘ng nhÆ° váº­y, viá»‡c giá»›i thiá»‡u cÃ¡c ká»¹ thuáº­t cá»§a vLLM cÃ³ thá»ƒ lÃ m giáº£m hiá»‡u suáº¥t do chi phÃ­ bá»• sung cá»§a giÃ¡n tiáº¿p bá»™ nhá»› vÃ  bá»™ nhá»› block khÃ´ng liá»n ká». Tuy nhiÃªn, chÃºng tÃ´i sáº½ ráº¥t hÃ o há»©ng tháº¥y cÃ¡c ká»¹ thuáº­t cá»§a vLLM Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c khá»‘i lÆ°á»£ng cÃ´ng viá»‡c khÃ¡c cÃ³ tÃ­nh cháº¥t tÆ°Æ¡ng tá»± nhÆ° phá»¥c vá»¥ LLM.

CÃ¡c tá»‘i Æ°u hÃ³a cá»¥ thá»ƒ LLM trong viá»‡c Ã¡p dá»¥ng bá»™ nhá»› áº£o vÃ  phÃ¢n trang. vLLM tÃ¡i diá»…n giáº£i vÃ  tÄƒng cÆ°á»ng Ã½ tÆ°á»Ÿng vá» bá»™ nhá»› áº£o vÃ  phÃ¢n trang báº±ng cÃ¡ch táº­n dá»¥ng ngá»¯ nghÄ©a cá»¥ thá»ƒ á»©ng dá»¥ng. Má»™t vÃ­ dá»¥ lÃ  chÃ­nh sÃ¡ch swap-out all-or-nothing cá»§a vLLM, khai thÃ¡c thá»±c táº¿ ráº±ng viá»‡c xá»­ lÃ½ má»™t yÃªu cáº§u yÃªu cáº§u táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i token tÆ°Æ¡ng á»©ng cá»§a nÃ³ Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› GPU. Má»™t vÃ­ dá»¥ khÃ¡c lÃ  phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n láº¡i Ä‘á»ƒ khÃ´i phá»¥c cÃ¡c block Ä‘Ã£ evict, khÃ´ng kháº£ thi trong OS. BÃªn cáº¡nh Ä‘Ã³, vLLM giáº£m thiá»ƒu chi phÃ­ cá»§a giÃ¡n tiáº¿p bá»™ nhá»› trong phÃ¢n trang báº±ng cÃ¡ch há»£p nháº¥t cÃ¡c kernel GPU cho cÃ¡c phÃ©p toÃ¡n truy cáº­p bá»™ nhá»› vá»›i cÃ¡c phÃ©p toÃ¡n khÃ¡c nhÆ° attention.

9 CÃ´ng TrÃ¬nh LiÃªn Quan
CÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ mÃ´ hÃ¬nh tá»•ng quÃ¡t. Phá»¥c vá»¥ mÃ´ hÃ¬nh Ä‘Ã£ lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u tÃ­ch cá»±c trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, vá»›i nhiá»u há»‡ thá»‘ng Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c khÃ­a cáº¡nh Ä‘a dáº¡ng cá»§a viá»‡c triá»ƒn khai mÃ´ hÃ¬nh há»c sÃ¢u. Clipper [11], TensorFlow Serving [33], Nexus [45], InferLine [10], vÃ  Clockwork [20] lÃ  má»™t sá»‘ há»‡ thá»‘ng phá»¥c vá»¥ mÃ´ hÃ¬nh tá»•ng quÃ¡t sá»›m hÆ¡n. ChÃºng nghiÃªn cá»©u batching, caching, placement, vÃ  láº­p lá»‹ch Ä‘á»ƒ phá»¥c vá»¥ má»™t hoáº·c nhiá»u mÃ´ hÃ¬nh. Gáº§n Ä‘Ã¢y hÆ¡n, DVABatch [12] giá»›i thiá»‡u batching multi-entry multi-exit. REEF [21] vÃ  Shepherd [61] Ä‘á» xuáº¥t preemption Ä‘á»ƒ phá»¥c vá»¥. AlpaServe [28] sá»­ dá»¥ng song song mÃ´ hÃ¬nh cho statistical multiplexing. Tuy nhiÃªn, cÃ¡c há»‡ thá»‘ng tá»•ng quÃ¡t nÃ y khÃ´ng tÃ­nh Ä‘áº¿n tÃ­nh cháº¥t tá»± há»“i quy vÃ  tráº¡ng thÃ¡i token cá»§a suy luáº­n LLM, dáº«n Ä‘áº¿n bá» lá»¡ cÃ¡c cÆ¡ há»™i tá»‘i Æ°u hÃ³a.

CÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ chuyÃªn biá»‡t cho transformer. Do táº§m quan trá»ng cá»§a kiáº¿n trÃºc transformer, nhiá»u há»‡ thá»‘ng phá»¥c vá»¥ chuyÃªn biá»‡t cho nÃ³ Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn. CÃ¡c há»‡ thá»‘ng nÃ y sá»­ dá»¥ng tá»‘i Æ°u hÃ³a kernel GPU [1, 29, 31, 56], cÆ¡ cháº¿ batching tiÃªn tiáº¿n [14,60], song song mÃ´ hÃ¬nh [1, 41,60], vÃ  chia sáº» tham sá»‘ [64] Ä‘á»ƒ phá»¥c vá»¥ hiá»‡u quáº£. Trong sá»‘ chÃºng, Orca [60] cÃ³ liÃªn quan nháº¥t Ä‘áº¿n cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i.

So sÃ¡nh vá»›i Orca. Láº­p lá»‹ch cáº¥p iteration trong Orca [60] vÃ  PagedAttention trong vLLM lÃ  cÃ¡c ká»¹ thuáº­t bá»• sung: Trong khi cáº£ hai há»‡ thá»‘ng Ä‘á»u nháº±m tÄƒng viá»‡c sá»­ dá»¥ng GPU vÃ  do Ä‘Ã³ thÃ´ng lÆ°á»£ng cá»§a phá»¥c vá»¥ LLM, Orca Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y báº±ng cÃ¡ch láº­p lá»‹ch vÃ  xen káº½ cÃ¡c yÃªu cáº§u Ä‘á»ƒ nhiá»u yÃªu cáº§u hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ song song, trong khi vLLM thá»±c hiá»‡n Ä‘iá»u nÃ y báº±ng cÃ¡ch tÄƒng viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ä‘á»ƒ cÃ¡c táº­p há»£p lÃ m viá»‡c cá»§a nhiá»u yÃªu cáº§u hÆ¡n vá»«a vÃ o bá»™ nhá»›. Báº±ng cÃ¡ch giáº£m phÃ¢n máº£nh bá»™ nhá»› vÃ  cho phÃ©p chia sáº», vLLM cháº¡y nhiá»u yÃªu cáº§u hÆ¡n trong má»™t batch song song vÃ  Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 2-4Ã— so vá»›i Orca. Tháº­t váº­y, láº­p lá»‹ch vÃ  xen káº½ chi tiáº¿t cá»§a cÃ¡c yÃªu cáº§u nhÆ° trong Orca lÃ m cho quáº£n lÃ½ bá»™ nhá»› trá»Ÿ nÃªn thÃ¡ch thá»©c hÆ¡n, lÃ m cho cÃ¡c ká»¹ thuáº­t Ä‘Æ°á»£c Ä‘á» xuáº¥t trong vLLM tháº­m chÃ­ quan trá»ng hÆ¡n.

Tá»‘i Æ°u hÃ³a bá»™ nhá»›. Khoáº£ng cÃ¡ch ngÃ y cÃ ng lá»›n giá»¯a kháº£ nÄƒng tÃ­nh toÃ¡n vÃ  dung lÆ°á»£ng bá»™ nhá»› cá»§a bá»™ tÄƒng tá»‘c Ä‘Ã£ khiáº¿n bá»™ nhá»› trá»Ÿ thÃ nh nÃºt cá»• chai cho cáº£ huáº¥n luyá»‡n vÃ  suy luáº­n. Swapping [23,42,55], tÃ­nh toÃ¡n láº¡i [7,24] vÃ  sá»± káº¿t há»£p cá»§a chÃºng [40] Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£m bá»™ nhá»› Ä‘á»‰nh cá»§a huáº¥n luyá»‡n. ÄÃ¡ng chÃº Ã½, FlexGen [46] nghiÃªn cá»©u cÃ¡ch swap trá»ng sá»‘ vÃ  tráº¡ng thÃ¡i token cho suy luáº­n LLM vá»›i bá»™ nhá»› GPU háº¡n cháº¿, nhÆ°ng nÃ³ khÃ´ng nháº¯m Ä‘áº¿n cÃ¡c thiáº¿t láº­p phá»¥c vá»¥ trá»±c tuyáº¿n. OLLA [48] tá»‘i Æ°u hÃ³a thá»i gian tá»“n táº¡i vÃ  vá»‹ trÃ­ cá»§a tensor Ä‘á»ƒ giáº£m phÃ¢n máº£nh, nhÆ°ng nÃ³ khÃ´ng thá»±c hiá»‡n quáº£n lÃ½ cáº¥p block chi tiáº¿t hoáº·c phá»¥c vá»¥ trá»±c tuyáº¿n. FlashAttention [13] Ã¡p dá»¥ng tiling vÃ  tá»‘i Æ°u hÃ³a kernel Ä‘á»ƒ giáº£m bá»™ nhá»› Ä‘á»‰nh cá»§a tÃ­nh toÃ¡n attention vÃ  giáº£m chi phÃ­ I/O. BÃ i bÃ¡o nÃ y giá»›i thiá»‡u Ã½ tÆ°á»Ÿng má»›i vá» quáº£n lÃ½ bá»™ nhá»› cáº¥p block trong bá»‘i cáº£nh phá»¥c vá»¥ trá»±c tuyáº¿n.

10 Káº¿t Luáº­n
BÃ i bÃ¡o nÃ y Ä‘á» xuáº¥t PagedAttention, má»™t thuáº­t toÃ¡n attention má»›i cho phÃ©p khÃ³a vÃ  giÃ¡ trá»‹ attention Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› phÃ¢n trang khÃ´ng liá»n ká», vÃ  trÃ¬nh bÃ y vLLM, má»™t há»‡ thá»‘ng phá»¥c vá»¥ LLM thÃ´ng lÆ°á»£ng cao vá»›i quáº£n lÃ½ bá»™ nhá»› hiá»‡u quáº£ Ä‘Æ°á»£c kÃ­ch hoáº¡t bá»Ÿi PagedAttention. Láº¥y cáº£m há»©ng tá»« há»‡ Ä‘iá»u hÃ nh, chÃºng tÃ´i chá»©ng minh cÃ¡ch cÃ¡c ká»¹ thuáº­t Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p, nhÆ° bá»™ nhá»› áº£o vÃ  copy-on-write, cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ quáº£n lÃ½ hiá»‡u quáº£ KV cache vÃ  xá»­ lÃ½ cÃ¡c thuáº­t toÃ¡n giáº£i mÃ£ khÃ¡c nhau trong phá»¥c vá»¥ LLM. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cho tháº¥y vLLM Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n thÃ´ng lÆ°á»£ng 2-4Ã— so vá»›i cÃ¡c há»‡ thá»‘ng tiÃªn tiáº¿n.

Lá»i Cáº£m Æ n
ChÃºng tÃ´i muá»‘n cáº£m Æ¡n Xiaoxuan Liu, Zhifeng Chen, Yanping Huang, cÃ¡c reviewer SOSP áº©n danh, vÃ  shepherd cá»§a chÃºng tÃ´i, Lidong Zhou, vÃ¬ pháº£n há»“i sÃ¢u sáº¯c cá»§a há». NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi quÃ  táº·ng tá»« Andreessen Horowitz, Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, Uber, vÃ  VMware.

TÃ i Liá»‡u Tham Kháº£o
[Danh sÃ¡ch tÃ i liá»‡u tham kháº£o tá»« [1] Ä‘áº¿n [64] - giá»¯ nguyÃªn Ä‘á»‹nh dáº¡ng vÃ  ná»™i dung nhÆ° báº£n gá»‘c]
