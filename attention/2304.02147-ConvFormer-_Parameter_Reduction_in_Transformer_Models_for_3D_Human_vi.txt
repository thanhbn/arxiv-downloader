# 2304.02147.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2304.02147.pdf
# Kích thước tệp: 5717616 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
ConvFormer: Giảm Tham số trong Mô hình Transformer cho Ước lượng Tư thế Con người 3D bằng cách Tận dụng Cơ chế Chú ý Tích chập Đa đầu Động

Alec Diaz-Arias và Dmitriy Shin
Inseer, Inc.
Iowa City, IA 52241
alec.diaz-arias@inseer.com
6 tháng 4, 2023

Tóm tắt
Gần đây, các kiến trúc transformer hoàn toàn đã thay thế kiến trúc tích chập thông thường cho nhiệm vụ ước lượng tư thế con người 3D. Trong bài báo này chúng tôi đề xuất ConvFormer, một transformer tích chập mới tận dụng cơ chế tự chú ý tích chập đa đầu động mới cho ước lượng tư thế con người 3D đơn mắt. Chúng tôi đã thiết kế một transformer tích chập không gian và thời gian để mô hình hóa toàn diện các mối quan hệ khớp con người trong các khung hình riêng lẻ và toàn cục qua chuỗi chuyển động. Hơn nữa, chúng tôi giới thiệu một khái niệm mới về hồ sơ khớp thời gian cho ConvFormer thời gian của chúng tôi để hợp nhất thông tin thời gian hoàn chỉnh ngay lập tức cho một vùng lân cận cục bộ của các đặc trưng khớp. Chúng tôi đã xác thực phương pháp của mình định lượng và định tính trên ba tập dữ liệu chuẩn thông thường: Human3.6M, MPI-INF-3DHP, và HumanEva. Các thí nghiệm mở rộng đã được tiến hành để xác định bộ siêu tham số tối ưu. Những thí nghiệm này đã chứng minh rằng chúng tôi đạt được sự giảm tham số đáng kể so với các mô hình transformer trước đây trong khi đạt được Hiệu suất Tốt nhất (SOTA) hoặc gần SOTA trên cả ba tập dữ liệu. Ngoài ra, chúng tôi đạt được SOTA cho Giao thức III trên H36M cho cả đầu vào phát hiện GT và CPN. Cuối cùng, chúng tôi thu được SOTA trên cả ba thang đo cho tập dữ liệu MPI-INF-3DHP và cho cả ba đối tượng trên HumanEva theo Giao thức II.

1 Giới thiệu
Ước lượng Tư thế Con người 3D Đơn mắt (HPE) là một quá trình định vị các vị trí khớp và sau đó là biểu diễn cơ thể (biểu diễn xương) từ các luồng đầu vào khác nhau như hình ảnh tĩnh hoặc luồng video. 3D HPE nhận được rất nhiều sự chú ý trong cộng đồng thị giác máy tính và đóng vai trò thiết yếu trong nhiều ứng dụng bao gồm phân tích chuyển động, hoạt hình máy tính, nhận dạng hành động, và đánh giá rủi ro an toàn công thái học. Nhiều phương pháp đã được đề xuất để giải quyết vấn đề này (để có cách xử lý mở rộng xem phần Công trình Trước đây).

Gần đây hơn, và được thúc đẩy bởi công trình đột phá trong [21] (ViT), một kiến trúc transformer hoàn toàn đã được giới thiệu cho nhiệm vụ 3D HPE. Transformers được phát triển để khai thác các phụ thuộc tầm xa và đã đạt được thành công to lớn trong NLP kể từ khi được phát minh [12] và gần đây hơn trong các nhiệm vụ CV khác nhau.

Tất cả các phương pháp này đã tiếp tục đẩy ranh giới của độ chính xác, tuy nhiên, chúng đã liên tục làm như vậy bằng cách tăng dung lượng mạng [60,59,55,57] và có thể dẫn đến tham số hóa quá mức. Ví dụ, các transformer cổ điển gặp phải vấn đề dư thừa đã biết xảy ra do kết nối hoàn toàn. Đã có các công trình trong NLP đã tìm cách giới thiệu tính thưa thớt và đã thấy sự gia tăng độ chính xác đáng kể trong khi giảm độ phức tạp tính toán [27,61,62]. Vì các transformer cổ điển vẫn đang trong giai đoạn sơ khai cho các nhiệm vụ CV, các cơ chế thưa thớt vẫn chưa được áp dụng đầy đủ. Giải quyết vấn đề dư thừa của các transformer cổ điển là một trong những động lực chính để giới thiệu ConvFormer. Đối với vấn đề 3D HPE và cụ thể cho chuyển động con người, các khớp riêng lẻ có thể thể hiện mức độ tương quan cao. Do đó, bằng cách tạo ra queries, keys, và values thông qua các lớp kết nối đầy đủ trong các transformer vanilla, các mạng học được các dư thừa dẫn đến suy luận nhiễu hơn. ConvFormer tận dụng các tích chập để trích xuất các kết hợp khớp thông qua trường tiếp nhận cục bộ của chúng mà cùng nhau cung cấp một tín hiệu mạnh hơn ít nhạy cảm với nhiễu, dẫn đến ít đặc trưng hơn cho các tính toán chú ý, và giảm đáng kể số lượng tham số. Hơn nữa, một bộ lọc duy nhất có thể không thể nắm bắt đầy đủ các phụ thuộc. Vì lý do này chúng tôi đã giới thiệu một cơ chế tổng hợp động để cân bằng đóng góp của các vùng lân cận khớp khác nhau. Chúng tôi gọi cơ chế mới này là tự chú ý tích chập đa đầu động (DMHCSA).

Theo [5,68] chúng tôi tận dụng một khung không gian-thời gian. Tuy nhiên, một yếu tố phân biệt quan trọng và động lực đáng kể cho ConvFormer là cách trích xuất các phụ thuộc thời gian ở mức query, key, value trước khi tính toán các bản đồ chú ý thời gian. Để đạt được điều này chúng tôi giới thiệu hồ sơ khớp thời gian. Để tính toán chúng, ConvFormer trích xuất các tương quan giữa các khớp cho các khung hình riêng lẻ trong ConvFormer không gian và sau đó tạo ra các hồ sơ thời gian bậc cao của các khớp có mặt trong các chuỗi chuyển động với ConvFormer thời gian. Cụ thể hơn cho các khối thời gian,

--- TRANG 2 ---
cơ chế DMHCSA trích xuất queries, keys, và values có khả năng nhìn thấy qua chuỗi chuyển động, điều này đã được gọi là hợp nhất thời gian sớm, dẫn đến các bản đồ tự chú ý phức tạp hơn nắm bắt các tương quan phức tạp hơn. Chúng tôi đã tiến hành các thí nghiệm mở rộng trên ba tập dữ liệu 3D HPE tiêu chuẩn, tức là Human3.6M, MPI-INF-3DHP, và HumanEva-I [7,10,8] và so sánh ConvFormer với một số phương pháp giải pháp 3D HPE cạnh tranh. ConvFormer đạt được kết quả hiện đại bởi phần lớn các thang đo và có thể so sánh trên các phát hiện CPN theo Giao thức I trong khi giảm số lượng tham số hơn một nửa so với các mô hình lấy chuỗi đầu vào có độ dài tương đương. Các đóng góp của chúng tôi trong bài báo này được tóm tắt như sau:

1. Một sự giảm tham số đáng kể so với các mô hình transformer khác sử dụng một kiến trúc mới gọi là ConvFormer. ConvFormer tận dụng một cơ chế tự chú ý tích chập đa đầu mới tổng hợp động các sub-queries, keys, và values thành một tập hợp gợi ý phong phú hơn cho 3D HPE.

2. Một khái niệm mới về hồ sơ khớp thời gian được giới thiệu dựa trên việc hợp nhất ngay lập tức thông tin thời gian hoàn chỉnh của chuỗi chuyển động.

3. Một nghiên cứu mở rộng về các yếu tố ảnh hưởng đến hiệu suất của ConvFormer.

2 Các Công trình Trước đây
Ngay từ đầu việc tận dụng mạng neural sâu cho 3D HPE, nhiều phương pháp đã cố gắng học các ánh xạ từ hình ảnh RGB đơn mắt đến các biểu diễn xương 3D [9,44]. Trong khi 3D HPE một lần thấy một số thành công, nó gặp phải chi phí tính toán đáng kể và đồng thời khả năng tổng quát hóa kém do dữ liệu Motion Capture được thu thập trong môi trường dàn dựng. Một phần do Martinez et al. [1], bối cảnh 3D HPE đã chuyển trọng tâm chủ yếu về phương pháp hai giai đoạn bằng cách tận dụng hiệu suất chính xác của các bộ phát hiện tư thế 2D sẵn có và sau đó xây dựng các mạng thực hiện việc nâng từ 2D lên 3D. Một số công trình khác đã cải thiện hiệu suất của 3D HPE từ một hình ảnh đơn mắt duy nhất sử dụng các kỹ thuật học sâu và phương pháp phân tích khác nhau (ví dụ, [44, 46, 78, 64, 48]).

Để giảm lỗi, cải thiện xử lý tự che khuất, và tăng khả năng tổng quát hóa của các mô hình 3D HPE, một số công trình đã khai thác các mối quan hệ không gian giữa các khớp. Để tính đến những mối quan hệ này, một số phương pháp kết hợp các ràng buộc nhân trắc học "tĩnh" và các thủ tục chính quy hóa, trong khi những phương pháp khác dựa trên các kiến trúc thời gian suy luận những phụ thuộc này qua các khung hình video [31,2,16,11]. Ví dụ, mọi người đã sử dụng mạng tích chập đồ thị và mạng chú ý đồ thị để mô hình hóa tự nhiên các mối quan hệ không gian giữa các khớp trong khi xây dựng các mạng nhẹ [75, 76, 78, 79].

Gần đây, Kolesnikov et al. đã giới thiệu Vision Transformer (ViT) áp dụng một cơ chế tự chú ý toàn cục để khai thác hiệu quả thông tin nổi bật từ các khung hình video [21]. Kể từ ViT, transformers đã thấy thành công trong nhiều nhiệm vụ CV, bao gồm, nhận dạng hình ảnh [21, 55], và phát hiện đối tượng [41].

Thậm chí gần đây hơn, các nhà nghiên cứu đã bắt đầu tận dụng tích chập trong transformers, để học nhúng vị trí thay cho các lớp dày đặc, thay thế thành phần feed forward dày đặc bằng một khối feed-forward tích chập cho tính thưa thớt, hoặc tận dụng các phép chiếu tích chập cho các nhiệm vụ cụ thể như tổng hợp video và làm việc với dữ liệu đám mây điểm không có cấu trúc [56,59]. Cuối cùng, với sự bùng nổ của các mô hình dựa trên transformer và khả năng của chúng để nắm bắt hiệu quả các mối quan hệ cục bộ và toàn cục, chúng bắt đầu được áp dụng cho vấn đề 3D HPE cũng như [54, 50, 35, 5].

Trong một thiết lập điển hình, một mô hình xử lý các khung hình video kề cận để học các biểu diễn thời gian của các bộ phận cơ thể con người trong chuyển động, và sau đó tái tạo một tư thế con người cho một khung hình bên trong tại bước suy luận. Zheng et al. đã phát triển một phương pháp 3D HPE gọi là PoseFormer dựa thuần túy trên kiến trúc transformer mã hóa và học cả thông tin không gian và thời gian [5]. Li et al. tận dụng một khung không gian-thời gian trong khi tìm nhiều giải pháp khả thi (cho rằng 3D HPE là một vấn đề nghịch đảo) và sau đó tận dụng một đầu transformer tổng hợp các giải pháp khả thi thành một giải pháp tối ưu [68]. Khung này hoạt động bằng cách tạo ra nhiều giả thuyết cho dự đoán tư thế với việc tinh chỉnh tự giả thuyết và tính toán các tương tác chéo giả thuyết tiếp theo. MHFormer đạt được độ chính xác vượt trội so với các phương pháp trước đây trên các tập dữ liệu MPI-INF-3DHP và Human 3.6M. He et al. đã phát triển một Epipolar Transformer để tận dụng dữ liệu 3D để cải thiện ước lượng tư thế 2D, điều này bị thách thức khi có che khuất và góc nhìn xiên [54]. Shuai đã giới thiệu một transformer Fusion đa góc nhìn và thời gian để xử lý thích ứng số lượng góc nhìn và độ dài video khác nhau mà không cần hiệu chuẩn [50].

Mặc dù transformers thể hiện khả năng mạnh mẽ để mô hình hóa các mối quan hệ phức tạp, chúng gặp phải vấn đề dư thừa và kết nối quá mức. Để giải quyết vấn đề này, các nhà nghiên cứu trong NLP đã bắt đầu phát triển các cơ chế thưa thớt khác nhau trong nỗ lực giảm kết nối. Ví dụ, Jaszczur et al. đã đề xuất một mô hình transformer thưa thớt để mở rộng quy trình học và suy luận [62]. Trong 3D HPE, Li et al. đã đề xuất Strided Transformer [71] để giảm chiều của các lớp tuyến tính cuối. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, chưa có công trình nào được thực hiện để giảm kết nối trong cơ chế tự chú ý "nặng tham số" nhất của transformers cho nhiệm vụ 3D HPE.

Vì những lý do này, chúng tôi đề xuất mô hình ConvFormer, giảm số lượng tham số một cách mở rộng so với [5], khoảng 60 phần trăm. Ở mức không gian, cơ chế tổng hợp đặc trưng đa quy mô của chúng tôi có thể nắm bắt các tương quan quan trọng dẫn đến một tín hiệu mạnh hơn mạnh mẽ hơn so với [5]. Hơn nữa, cơ chế tự chú ý tích chập của chúng tôi trong transformer thời gian tạo ra queries, keys, và values trích xuất thông tin giữa các khung hình dẫn đến các bản đồ chú ý đa dạng hơn. Điều này cho phép chúng tôi đạt được SOTA với chi phí tương đối thấp. Trong phần tiếp theo, chúng tôi cung cấp một trình bày toàn diện về phương pháp giải pháp của chúng tôi.

--- TRANG 3 ---
3 Phương pháp

Trong các phần phụ sau, chúng tôi trình bày tổng quan về phương pháp giải pháp của chúng tôi để ước lượng tư thế 3D từ một chuỗi tư thế 2D, sau đó chúng tôi mô tả kiến trúc mạng toàn cục của chúng tôi, và cuối cùng chúng tôi trình bày cơ chế tự chú ý tích chập đa đầu động của chúng tôi.

3.1 Tổng quan

Hình 1: Bảng A mô tả kiến trúc của một Khối ConvFormer. Bảng B trình bày pipeline tổng thể cho 3D HPE từ một chuỗi tư thế 2D. Thành phần trung tâm của một Khối ConvFormer là DMHCSA được mô tả trong bảng C. Một đường cong màu xanh ở phía dưới Bảng C tương ứng với một phần của hồ sơ khớp thời gian được trích xuất của khớp khuỷu tay phải (cho khối ConvFormer thời gian). Bảng D trình bày một ví dụ về tích chập trong quá trình tạo Queries, Keys, và Values trong một Khối ConvFormer Thời gian. Một bộ lọc trượt qua chiều đặc trưng hiệu quả tích chập các hồ sơ thời gian đầy đủ của các vùng lân cận khớp cục bộ.

Kiến trúc tổng thể của phương pháp của chúng tôi được mô tả trong Hình 1. Cho một chuỗi tư thế 2D P=fPigT i=1∈RJ×2 trong đó T đại diện cho số khung hình trong chuỗi và J là số khớp trong xương. Chúng tôi tìm cách tái tạo các tư thế 3D trong khung tham chiếu camera tương đối gốc (tức là khung tham chiếu camera nơi khớp gốc nằm tại gốc tọa độ). Theo [2], chúng tôi dự đoán tư thế 3D cho khung hình trung tâm từ bất kỳ chuỗi nào như vậy, tức là ^pd⌈T/2⌉∈RJ×3. Mạng của chúng tôi chứa hai khối Dynamic ConvFormer, một với chú ý không gian và khối khác với chú ý thời gian. Cụ thể hơn, chúng tôi tận dụng một cơ chế chú ý không gian để trích xuất các phụ thuộc liên khớp theo khung hình bằng cách phân tích các phần của khớp có liên quan. Cơ chế chú ý thời gian trích xuất các mối quan hệ liên khung hình toàn cục bằng cách phân tích các tương quan giữa các hồ sơ thời gian của khớp. Trái ngược với [5], truy vấn các biểu diễn tư thế tiềm ẩn cho các khung hình riêng lẻ và sau đó tính toán chú ý đối với trục thời gian, cơ chế hồ sơ khớp thời gian của chúng tôi hợp nhất thông tin thời gian ở mức truy vấn trước khi tính toán tự chú ý đối với trục thời gian.

--- TRANG 4 ---
3.2 Kiến trúc Mạng

Chúng tôi sử dụng hai thành phần chính trong kiến trúc mạng của chúng tôi: một ConvFormer không gian và một ConvFormer thời gian. Khối ConvFormer không gian trích xuất một vector đặc trưng chiều cao cho các tương quan khớp được mã hóa của một khung hình duy nhất. Chúng tôi giả định đầu vào của chúng tôi là một tư thế 2D với J khớp được biểu diễn bởi hai tọa độ (u,v). Theo [5] chúng tôi đầu tiên ánh xạ tọa độ của mỗi khớp thành một vector đặc trưng chiều cao hơn với một lớp tuyến tính có thể huấn luyện. Sau đó chúng tôi áp dụng một mã hóa vị trí học được thông qua phép cộng để giữ lại thông tin vị trí khớp. Tức là, cho một chuỗi tư thế fPigT i=1∈RJ×2 và W∈R2×d và Epos∈RJ×d chúng tôi mã hóa Pi như sau:

xi = PiW + Epos; i∈{1,⋯,T}. (1)

và d đại diện cho chiều của nhúng, W là lớp tuyến tính có thể huấn luyện, và Epos là mã hóa vị trí học được. Sau đó, chuỗi đặc trưng không gian fxigT i=1∈RJ×d được đưa vào ConvFormer không gian áp dụng cơ chế chú ý cho chiều khớp để tích hợp thông tin qua tư thế hoàn chỉnh trên cơ sở mỗi khung hình. Q,K,V được tạo thông qua tích chập với trọng số có chiều sau (d,d,k) trong đó d là chiều được mã hóa và k là kích thước kernel và bộ lọc được trượt qua chiều khớp. Đầu ra cho khung hình thứ i của khối ConvFormer không gian thứ b được ký hiệu bởi zbi∈RJ×d cho i=1,⋯,T.

Trong khi ConvFormer không gian tìm cách mã hóa các tương quan giữa các khớp trong một khung hình duy nhất, chúng tôi tận dụng mô hình thời gian để định vị các tương quan theo chuỗi giữa các đặc trưng không gian được mã hóa. Cơ chế này nên được xem như trích xuất hồ sơ thời gian của một vùng lân cận của khớp, mà chúng tôi gọi là hồ sơ khớp thời gian (xem Bảng D trong Hình 1). Một công trình sớm tận dụng cơ chế hợp nhất thời gian này là [4] nơi Karpathy et al. nghiên cứu các cơ chế khác nhau để kết hợp thông tin thời gian mà không tích chập qua chiều thời gian. Để làm rõ hơn điểm này, Q,K,V được tạo thông qua tích chập với trọng số có chiều sau (T,T,k) trong đó k là kích thước kernel và các tích chập 1D có độ sâu bằng kích thước của chuỗi đầu vào. Do đó, có thể xem mạng của chúng tôi như hợp nhất vào các queries sự tiến hóa thời gian của một patch các đặc trưng khớp sâu ngay lập tức. Điều này rất khác biệt so với chú ý thời gian được thấy trong [5] chú ý đến việc mã hóa tư thế hoàn chỉnh trong suốt chuỗi chuyển động. Chúng tôi lưu ý rằng đầu ra từ khối ConvFormer không gian là một chuỗi fzBigT i=1,⋯,T∈RJ×d trong đó B là số khối không gian và T là số khung hình trong chuỗi. Chúng tôi lưu ý rằng zbi có thể được biểu diễn trong R1×J×d và do đó nối các đặc trưng này dọc theo trục đầu tiên cho chúng ta X0=Concatenate(zB1,⋯,zBT)∈RT×J×d. Theo thủ tục này chúng tôi kết hợp một nhúng thời gian học được để giữ lại thông tin về sự tiến hóa đặc trưng khớp sâu trong suốt thời gian, tức là Etemp∈RT×J×d và X=X0+Etemp là đầu vào vào transformer thời gian của chúng tôi. Chúng tôi lưu ý rằng đầu ra của khối ConvFormer thứ b với chú ý thời gian là Zb∈RT×J×d trong đó có B lớp như vậy.

Vì chúng tôi theo sơ đồ dự đoán nhiều-thành-một được giới thiệu lần đầu trong [2] chúng tôi đầu tiên lấy mẫu xuống trục không gian với một phép chiếu tuyến tính và sau đó thực hiện một tích chập thời gian với một kênh đầu ra tức là ^p=ConvT,1(ZBW) trong đó W∈RJ×d×3J và ConvT,1 ký hiệu một tích chập thời gian với một kênh đầu ra và T kênh đầu vào.

Chúng tôi huấn luyện mạng của chúng tôi bằng cách giảm thiểu MPJPE (Mean Per Joint Position Error) trong quá trình tối ưu hóa. Hàm mất mát được định nghĩa là

L(p,^p) = (1/J)∑Ji=1‖pi−^pi‖2 (2)

trong đó p là tư thế 3D thực tế và ^p là tư thế được dự đoán và i đang lập chỉ mục các khớp cụ thể trong xương.

3.3 Tự Chú ý Tích chập Đa đầu Động

Một sự mới lạ cốt lõi của bài báo này là cơ chế tự chú ý tích chập đa đầu động. Điều này được giới thiệu để giảm tính kết nối quá mức được chứng kiến trong các kiến trúc transformer cổ điển trong khi đồng thời trích xuất ngữ cảnh ở các quy mô khác nhau. Một sự mới lạ bổ sung là loại biểu diễn được truy vấn trong khối ConvFormer thời gian của chúng tôi. Thay vì tạo ra queries, keys, và values, là các biểu diễn tư thế tiềm ẩn cho các khung hình riêng lẻ và chú ý đến trục thời gian; chúng tôi truy vấn các hồ sơ khớp thời gian hiệu quả hợp nhất thông tin thời gian trước cơ chế chú ý.

Convolutional Scaled Dot Product Attention có thể được mô tả như một hàm ánh xạ ánh xạ một ma trận query Q, một ma trận key K, và một ma trận value V thành một ma trận chú ý đầu ra – trong đó các mục ma trận là điểm số đại diện cho cường độ tương quan giữa hai phần tử bất kỳ trong chiều được chú ý. Chúng tôi lưu ý rằng Q,K,V∈RN×d trong đó N là độ dài của chuỗi và d là chiều. Trong ConvFormer Không gian của chúng tôi N=J và trong ConvFormer Thời gian N=T. Đầu ra của scaled dot product attention có thể được biểu diễn như

Attention(Q,K,V) = Softmax(QKT/√d)V. (3)

Query, keys, và values, được tính toán theo cách tương tự cho một độ dài bộ lọc cố định. Chúng tôi chứng minh cách Q có thể được tạo ra, và lưu ý rằng K và V được tính toán theo cách giống hệt.

Q = Convn,dout(z) = ∑dini=1∑ℓk=1wdout,i,kzi,n−⌊ℓ/2⌋+k (4)

Ở đây, ℓ ký hiệu kích thước kernel và dout ký hiệu chiều đầu ra. Điều này được đặt song song với scaled dot product attention cổ điển được giới thiệu trong [12] nơi queries, keys, và values được tạo thông qua một phép chiếu tuyến tính

Q = WQz K = WKz V = WVz (5)

cung cấp phạm vi toàn cục nhưng gây ra dư thừa do kết nối hoàn toàn. Trong cơ chế chú ý tích chập động của chúng tôi chúng tôi giới thiệu tính thưa thớt thông qua tích chập để giảm kết nối trong khi đồng thời hợp nhất thông tin thời gian hoàn chỉnh trước scaled-dot-product-attention. Khả năng của ConvFormers cung cấp ngữ cảnh ở các quy mô khác nhau là do phương pháp tổng hợp đặc trưng động. Hơn nữa, do cơ chế tích chập của chúng tôi chúng tôi truy vấn ở mức liên khung hình nơi chúng tôi học hồ sơ khớp thời gian. Với mục đích này, chúng tôi sử dụng n kích thước bộ lọc tích chập để trích xuất các ngữ cảnh cục bộ khác nhau ở các quy mô {ℓi}ni=1 và sau đó thực hiện một phép toán lấy trung bình để tạo ra query, keys, và values cuối cùng mà chúng tôi áp dụng chú ý, theo ý tưởng được trình bày trong [36]:

Q = Concat(Q1,⋯,Qn) Q̄ = ∑ni=1α(i)QiQi trong đó ∑ni=1α(i) = 1 (6)

trong đó n là số bộ lọc tích chập được sử dụng, α∈Rn×1 là một tham số học được và Qi được tạo như trong phương trình 4.

Dynamic Multi-headed Convolutional Self-Attention (DMHCSA) tận dụng nhiều đầu để mô hình hóa chung thông tin từ nhiều không gian biểu diễn. Như được thấy trong Hình 1 mỗi đầu áp dụng scaled dot-product self-attention song song. Đầu ra của khối DMHCSA là sự nối của h đầu ra đầu chú ý được đưa vào một mạng feed-forward.

DMHCSA(Q,K,V) = Concatenate(H1,⋯,Hh) trong đó Hi = Attention(Qi,Ki,Vi); i∈{1,⋯,h} (7)

trong đó Qi, Ki, và Vi được tính toán thông qua thủ tục được định nghĩa ở trên.

Sau đó khối ConvFormer được định nghĩa bởi các phương trình sau:

X'b = DMHCSA(LN(Xb−1)) + Xb−1; b = 1,⋯,B
Xb = FFN(LN(X'b)) + X'b; b = 1,⋯,B (8)

trong đó LN() ký hiệu chuẩn hóa lớp giống như [21,55]. và FFN ký hiệu một mạng feed forward. Cả khối ConvFormer không gian và thời gian đều bao gồm Bsp và Btemp khối giống hệt nhau. Đầu ra của bộ mã hóa ConvFormer không gian là Y∈RT×J×d trong đó T là độ dài chuỗi khung hình, J là số khớp, và d là chiều nhúng. Đầu ra của ConvFormer thời gian là Y∈RT×J×d.

4 Thí nghiệm

4.1 Tập dữ liệu và Giao thức Đánh giá

Phương pháp đề xuất của chúng tôi được đánh giá trên ba tập dữ liệu thông thường: Human3.6M [7], HumanEva [8], và MPI-INF-3DHP [10]. Human3.6M bao gồm khoảng 2.3 triệu hình ảnh từ 4 camera video đồng bộ quay video ở 50 Hz. Có 7 đối tượng thực hiện 15 hành động riêng biệt và mỗi hành động được thực hiện hai lần mỗi đối tượng. Chúng tôi huấn luyện trên các đối tượng (S1, S5, S6, S7, S8) và xác thực trên các đối tượng (S9, S11) theo các công trình trước đây [11,33,10,5,68]. Chúng tôi đánh giá phương pháp của chúng tôi trên H36M theo ba giao thức khác nhau. Mean per joint position error (MPJPE) được gọi là Giao thức I trong nhiều công trình [6,14,2]. Phân tích Procrustes hoặc căn chỉnh cứng được ký hiệu bởi P-MPJPE được tính như khoảng cách Euclidean giữa ground-truth và phép biến đổi SE(3) tối ưu căn chỉnh tư thế được dự đoán với ground-truth. Điều này được gọi là Giao thức II như trong [1,15]. Cuối cùng, chúng tôi đánh giá độ mượt thời gian thông qua mean per joint velocity error, được gọi là MPJVE (trung bình qua các khớp của các xấp xỉ vận tốc hiệu số hữu hạn) hoặc Giao thức III như trong [2,16]. Mặt khác HumanEva là một tập dữ liệu nhỏ hơn nhiều với ít hơn 50k khung hình và chỉ 3 đối tượng (S1, S2, S3) thực hiện ba hành động. Chúng tôi đánh giá phương pháp của chúng tôi đối với Giao thức II theo các công trình trước đây (ví dụ [2]. Cuối cùng, chúng tôi đánh giá trên MPI-INF-3DHP để đánh giá khả năng tổng quát hóa của mô hình chúng tôi. MPI bao gồm khoảng 1.3 triệu khung hình. Tập dữ liệu này chứa các chuyển động đa dạng hơn so với hai tập dữ liệu trước đây. Theo thiết lập trong [11,33,10,5,68] chúng tôi báo cáo các thang đo sau: MPJPE, Percentage of Correct Keypoint (PCK) với ngưỡng 150mm, và Area Under Curve (AUC) cho một dải ngưỡng PCK.

4.2 Chi tiết Triển khai

Chúng tôi triển khai phương pháp giải pháp đề xuất của chúng tôi với PyTorch [17] và huấn luyện sử dụng hai GPU NVIDIA RTX 3090. Chúng tôi huấn luyện trên H3.6M sử dụng 5 độ dài chuỗi khung hình khác nhau khi tiến hành thí nghiệm của chúng tôi, T = 9,27,81,143,243. Theo [2] chúng tôi tăng cường tập dữ liệu của chúng tôi bằng cách lật tư thế theo chiều ngang. Chúng tôi huấn luyện các mô hình của chúng tôi trong 60 epoch với tốc độ học ban đầu là 1e−3 và hệ số suy giảm trọng số là 0.95 sau mỗi epoch. Chúng tôi đặt kích thước batch là 1024 và sử dụng stochastic depth [18] là 0.2. Chúng tôi cũng sử dụng tỷ lệ dropout [22] là 0.2 trên tổng hợp đặc trưng động bên trong cơ chế tự chú ý tích chập. Chúng tôi đánh giá chuẩn trên H3.6M sử dụng cả phát hiện CPN [24] theo [2,11,16,5] và tư thế 2D ground-truth. Hơn nữa, chúng tôi đánh giá chuẩn trên HumanEva sử dụng ba độ dài chuỗi khung hình khác nhau T = 9, T = 27, và T = 43 theo [13]. Cuối cùng, theo [5,68] chúng tôi đánh giá thêm khả năng tổng quát hóa của phương pháp giải pháp chúng tôi trên tập dữ liệu MPI-INF-3DHP. Chúng tôi sử dụng chuỗi tư thế 2D có độ dài T = 9 như đầu vào mô hình của chúng tôi và chúng tôi đánh giá sử dụng ba thang đo, percentage of correct keypoints (PCK), area under the curve (AUC), và MPJPE.

--- TRANG 6 ---
(a) a
 (b) b
(c) c
 (d) d

Hình 2: Các ví dụ định tính của S11 từ H36M hiển thị hiệu quả của ConvFormer: (a) hành động Sitting Down với che khuất nặng ở chi dưới, (b) chứng minh tái tạo chất lượng cao khi có che khuất nhẹ, (c) che khuất nặng từ camera và ConvFormer vẫn nắm bắt tư thế đúng từ thông tin khung hình trước đó (d) trường hợp thất bại nhẹ khi có che khuất từ cánh tay phải.

Hình 3: Kết quả định tính cho ConvFormer trên các video In-The-Wild thách thức.

--- TRANG 7 ---
5 Kết quả và Thảo luận

5.1 So sánh với Hiện đại

Bảng 1: Khối đầu tiên báo cáo MPJPE cho đầu vào GT và khối thứ hai là MPJPE cho phát hiện CPN. Khối thứ ba báo cáo P-MPJPE cho phát hiện CPN. Khối thứ tư báo cáo MPJPV cho phát hiện CPN và khối thứ năm là MPJPV cho đầu vào GT. Tốt nhất màu Đỏ và thứ hai màu Xanh.

GT-MPJPE (mm) Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Hossain and Little [15] 35.2 40.8 37.2 37.4 43.2 44.0 38.9 35.6 42.3 44.6 39.7 39.7 40.2 32.8 35.5 39.2
Pavllo et al. [2] ––––––––––––––– 37.8
Liu eet al. [13] 34.5 37.1 33.6 34.2 32.9 37.1 39.6 35.8 40.7 41.4 33.0 33.8 33.0 26.6 26.9 34.7
Zeng et al. [30] 34.8 32.1 28.5 30.7 31.4 36.9 35.6 30.5 38.9 40.5 32.5 31.0 29.9 22.5 24.5 32.0
Chen et al. [11] ––––––––––––––– 32.3
Zheng et al. [5] 30.0 33.6 29.9 31.0 30.2 33.3 34.8 31.4 37.8 38.6 31.7 31.5 29.0 23.3 23.1 31.3
Li et al. [68] (T=351) 27.7 32.1 29.1 28.9 30.0 33.9 33.0 31.2 37.0 39.3 30.0 31.0 29.4 22.2 23.0 30.5
ConvFormer (T=143) 29.1 32.4 28.1 28.5 29.3 33.3 33.3 30.5 37.0 37.6 29.2 29.5 28.4 21.8 21.3 29.9
ConvFormer (T=243) 28.9 31.8 28.0 28.2 29.5 33.0 32.9 30.1 36.8 37.4 29.8 29.6 28.2 21.7 21.5 29.8

CPN-MPJPE (mm) Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Dabral et al. [31] 44.8 50.4 44.7 49.0 52.9 61.4 43.5 45.5 63.1 87.3 51.7 48.5 52.2 37.6 41.9 52.1
Cai et al. [32] (T=7) 44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4 48.8
Pavllo et al. [2] (T=243) 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8
Lin and Lee [33] (T=50) 42.5 44.8 42.6 44.2 48.5 57.1 52.6 41.4 56.5 64.5 47.4 43.0 48.1 33.0 35.1 46.6
Yeh et al. [34] 44.8 46.1 43.3 46.4 49.0 55.2 44.6 44.0 58.3 62.7 47.1 43.9 48.6 32.7 33.3 46.7
Liu et al. [13] (T=243) 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2 45.1
Zeng et al. [30] 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6 44.8
Wang et al. [16] (T=96) 41.3 43.9 44.0 42.2 48.0 57.1 42.2 43.2 57.3 61.3 47.0 43.5 47.0 32.6 31.8 45.6
Chen et al. [11] (T=243) 41.4 43.2 40.1 42.9 46.6 51.9 41.7 42.3 53.9 60.2 45.4 41.7 46.0 31.5 32.7 44.1
Lin et al. [35] (T=1) ––––––––––––––– 54.0
Zheng et al. [5] (T=81) 41.5 44.8 39.8 42.5 46.5 51.6 42.1 42.0 53.3 60.7 45.5 43.3 46.1 31.8 32.2 44.3
Li et al. [68] (T=351) 39.2 43.1 40.1 40.9 44.9 51.2 40.6 41.3 53.5 60.3 43.7 41.1 43.8 29.8 30.6 43.0
ConvFormer (T=143) 41.8 43.6 39.3 43.2 44.9 52.8 42.7 41.2 53.1 60.9 45.0 41.9 44.7 29.7 31.1 43.7
ConvFormer (T=243) 41.0 43.2 39.0 42.4 44.5 52.2 41.7 40.8 53.0 60.6 44.8 41.3 43.7 29.6 30.9 43.2

CPN-P-MPJPE (mm) Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Pavlakos et al. [9] 34.7 39.8 41.8 38.6 42.5 47.5 38.0 36.6 50.7 56.8 42.6 39.6 43.9 32.1 36.5 41.5
Rayat et al. [15] 35.7 39.3 44.6 43.0 47.2 54.0 38.3 37.5 51.6 61.3 46.5 41.4 47.3 34.2 39.4 44.1
Cai et al. [32] (T=7) 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 32.3 39.0
Lin and Lee [33] (T=50) 32.5 35.3 34.3 36.2 37.8 43.0 33.0 32.2 45.7 51.8 38.4 32.8 37.5 25.8 28.9 36.8
Pavllo et al. [2] (T=243) 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.5
Liu et al. [13] (T=243) 32.3 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5
Wang et al. [16] (T=96) 32.9 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5
Chen et al. [11] (T=243) 32.6 35.1 32.8 35.4 36.3 40.4 32.4 32.3 42.7 49.0 36.8 32.4 36.0 24.9 26.5 35.0
Zheng et al. [5] (T=81) 32.5 34.8 32.6 34.6 35.3 39.5 32.1 32.0 42.8 48.5 34.8 32.4 35.3 24.5 26.0 34.6
Li et al. [68] (T=351) 31.5 34.9 32.8 33.6 35.3 39.6 32.0 32.2 43.5 48.7 36.4 32.6 34.3 23.9 25.1 34.4
ConvFormer (T=143) 31.9 34.4 32.2 35.0 34.2 40.7 32.9 31.8 42.8 49.1 36.0 31.5 35.0 23.6 25.2 34.5
ConvFormer (T=243) 31.4 34.2 32.0 35.2 34.0 40.3 32.7 31.3 42.6 49.0 36.2 31.3 34.8 23.4 24.9 34.2

CPN-MPJVE Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Pavllo et al. [2] (T=243) 3.0 3.1 2.2 3.4 2.3 2.7 2.7 3.1 2.1 2.9 2.3 2.4 3.7 3.1 2.8 2.8
Chen et al. [11] (T=243) 2.7 2.8 2.0 3.1 2.0 2.4 2.4 2.8 1.8 2.4 2.0 2.1 3.4 2.7 2.4 2.5
Wang et al. [16] (T=96) 2.3 2.5 2.0 2.7 2.0 2.3 2.2 2.5 1.8 2.7 1.9 2.0 3.1 2.2 2.5 2.3
ConvFormer (T=143) 2.3 2.3 1.8 2.6 1.8 2.1 2.1 2.5 1.4 2.0 1.7 1.9 3.0 2.4 2.1 2.1

GT-MPJVE Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.
Wang et al. [16] (T=96) 1.2 1.3 1.1 1.4 1.1 1.4 1.2 1.4 1.0 1.3 1.0 1.1 1.7 1.3 1.4 1.4
ConvFormer (T=143) 1.2 1.3 0.9 1.4 1.0 1.2 1.3 1.5 0.7 1.1 0.9 1.1 1.7 1.4 1.2 1.2

Bảng 2: Kết quả định lượng trên HumanEva theo giao thức 2 cho phần bên trái của bảng và kết quả định lượng trên MPI-INF-3DHP ở phần bên phải của bảng. Tốt nhất màu Đỏ và tốt thứ hai màu Xanh.

Action Walk Jog Box Method PCK ↑ AUC ↑ MPJPE (mm) ↓
Subject S1 S2 S3 S1 S2 S3 S1 S2 S3 [10] 75.7 39.3 117.6
Martinez et al. [1] 19.7 17.4 46.8 26.9 18.2 18.6 ––– Lin et al. [33] 83.6 51.4 79.8
Pavalkos et aal. [9] 22.3 19.5 29.7 28.9 21.9 23.8 ––– Pavllo et al. [2] 86.0 51.9 84.0
Pavllo et al. [2] 13.9 10.2 46.6 20.9 13.1 13.8 23.8 33.7 32.0 Li et al. [43] 81.2 46.1 99.7
Zheng et al. [5] 16.3 11.0 47.1 25.0 15.2 15.1 ––– Chen et al. [11] 87.6 54.0 78.8
ConvFormer (T=9) 12.5 10.1 25.4 13.3 12.9 22.6 31.7 28.6 29.0 Zheng et al. [5] 88.6 56.4 77.1
ConvFormer (T=27) 11.4 9.0 20.1 19.1 11.8 11.8 20.8 28.0 26.1 Li et al. [68] 93.8 63.3 58.0
ConvFormer (T=43) 10.7 7.9 16.0 16.7 9.3 10.0 18.2 25.0 24.3 ConvFormer 96.4 69.8 53.6

Chúng tôi báo cáo kết quả cho các mô hình 143 và 243 khung hình của chúng tôi trên H3.6M và chúng tôi báo cáo kết quả cho các mô hình 9, 27, và 43 khung hình của chúng tôi cho HumanEva. Chúng tôi báo cáo tất cả 15 kết quả hành động cho cả hai đối tượng S9 và S11 sử dụng phát hiện GT và CPN như đầu vào 2D theo giao thức I, II, và III trong Bảng 1 và cột cuối cùng đại diện cho trung bình. Các mô hình 143 và 243 khung hình của ConvFormer giảm đáng kể số lượng tham số lần lượt 83.4% và 65.5% so với SOTA trước đây [68]. Các mô hình 143 và 243 khung hình của ConvFormer vượt trội so với SOTA trước đây trên đầu vào GT – đạt được sự giảm lỗi 2.3%. Mô hình 243 khung hình của ConvFormer bỏ lỡ SOTA trên đầu vào CPN cho Giao thức I 0.2mm trong khi có

--- TRANG 8 ---
tham số giảm đáng kể và đạt được tốt nhất hoặc tốt thứ hai trên 11 trong số 15 hành động. Tuy nhiên, nó vượt trội so với SOTA trên một số hành động thách thức như Sitting và WalkingDog thể hiện tư thế phức tạp và thay đổi tư thế nhanh. Theo Giao thức II ConvFormer đạt được SOTA trên 9 hành động riêng lẻ và trên lỗi trung bình. Cuối cùng, cho cả đầu vào GT và CPN ConvFormer giảm MPJVE lần lượt 8.6% và 14.3%, dẫn đến dự đoán mượt mà hơn. Xem Hình 2 cho một số kết quả định tính trên H36M hoặc xem https://github.com/AJDA1992/ConvFormer để có thêm ví dụ từ các chuyển động in-the-wild thách thức.

Phía bên trái của Bảng 2 hiển thị kết quả huấn luyện ConvFormer từ đầu trên HumanEva. Chúng tôi lưu ý rằng mô hình trường tiếp nhận lớn hơn của chúng tôi, với 43 khung hình, đạt được SOTA cho mọi hành động, trong khi mô hình trường tiếp nhận 27 khung hình của chúng tôi đạt được vị trí thứ hai cho mọi hành động.

Phía bên phải của Bảng 2 báo cáo kết quả định lượng của ConvFormer trên MPI-INF-3DHP so với các phương pháp khác. Theo [5,68], chúng tôi sử dụng chuỗi tư thế 2D gồm 9 khung hình do ít mẫu hơn và chuỗi video ngắn hơn. Chúng tôi lưu ý rằng ConvFormer tăng PCK 2.7%, AUC 10.2%, và giảm MPJPE 7.6%.

5.2 Phân tích Loại bỏ

Đầu tiên, chúng tôi nghiên cứu đóng góp của các siêu tham số riêng lẻ và điều chỉnh chúng. Thứ hai, chúng tôi đánh giá đóng góp của tự chú ý tích chập so với baseline (transformer vanilla) và sau đó đóng góp của cơ chế tự chú ý động của chúng tôi. Đối với điểm đầu tiên, chúng tôi thực hiện tìm kiếm lưới mở rộng sử dụng [3] và báo cáo một số kết quả trong Bảng 3, 4. Chúng tôi tìm thấy các siêu tham số sau là tối ưu: d = 32, Bsp = Btemp = 2 và sử dụng các kích thước kernel sau (7,7,7).

Trong Bảng 5 chúng tôi phân tích hiệu ứng của trường tiếp nhận cùng với số lượng tham số so với các phương pháp dựa trên transformer khác. Chúng tôi cố định các siêu tham số tối ưu được tìm thấy trong Bảng 3, 4. Chúng tôi tìm thấy qua tất cả các trường tiếp nhận, ConvFormer giảm tham số đáng kể so với [71, 5] trong khi vẫn cực kỳ cạnh tranh trên đầu vào CPN cho Giao thức I.

Cuối cùng, chúng tôi phân tích cải thiện mà ConvFormer mang lại so với kiến trúc transformer vanilla và lợi ích của việc sử dụng cơ chế Dynamic Multi-Headed attention của chúng tôi. Trong Bảng 6 mô hình baseline của chúng tôi theo cùng kiến trúc như ConvFormer ngoại trừ với scaled dot product attention lớp và các lớp kết nối đầy đủ tạo ra queries, keys, và values. Chúng tôi tìm thấy bằng cách sử dụng một bộ lọc duy nhất trong kiến trúc ConvFormer của chúng tôi cải thiện trên baseline 2mm và giới thiệu Dynamic Multi-Headed Attention của chúng tôi chúng tôi giảm thêm 1.1 mm.

Bảng 3: Phân tích chiều nhúng không gian và số Khối ConvFormer không gian và thời gian. Chúng tôi cũng thực hiện phân tích hạn chế về số đầu chú ý. Hiệu suất tối ưu được đánh dấu bằng màu Đỏ và được đánh giá bởi MPJPE.

d 16 16 16 32 32 32 64 32 32 32
Bsp 2 2 4 2 2 4 2 2 2 2
Btemp 2 4 2 2 4 2 2 2 2 2
Params (M) 0.65 1.26 0.69 2.56 4.95 2.70 9.97 2.56 2.56 2.56
Heads 8 8 8 8 8 8 8 1 2 4
MPJPE (mm) 50.8 51.0 51.7 49.4 50.6 49.9 50.1 52.3 51.0 49.6

6 Kết luận

Trong bài báo này chúng tôi cố gắng giải quyết độ phức tạp ngày càng tăng của các mô hình transformer. Vì điều này, chúng tôi giới thiệu ConvFormer dựa trên ba thành phần mới: hợp nhất thời gian, tự chú ý tích chập, và tổng hợp đặc trưng động. Để đánh giá hiệu quả của các thành phần khác nhau chúng tôi đã tiến hành các nghiên cứu loại bỏ mở rộng. Chúng tôi giảm số lượng tham số so với SOTA trước đây hơn 65% trong khi đạt được SOTA trên H36M cho Giao thức I trên đầu vào GT, Giao thức II cho phát hiện CPN, Giao thức III cho cả đầu vào GT và CPN, HumanEva cho tất cả đối tượng, và cuối cùng cả ba thang đo của MPI. Thú vị, mặc dù mạng tích chập đồ thị và mạng chú ý đồ thị nhẹ và mô hình hóa mạnh mẽ các mối quan hệ không gian/thời gian, ConvFormer cung cấp sự đánh đổi tốt hơn giữa giảm lỗi và độ phức tạp tính toán. Chúng tôi tin rằng ConvFormer sẽ cung cấp quyền truy cập sẵn sàng hơn vào các mạng tái tạo 3D chất lượng cao bằng cách làm cho quá trình huấn luyện và suy luận ít đòi hỏi tính toán hơn.

--- TRANG 9 ---
Bảng 4: Phân tích các cấu hình kernel khác nhau nơi hiệu suất được đánh giá so với MPJPE trên phát hiện CPN cho H36M. Tốt nhất được đánh dấu màu Đỏ.

Bsp Btemp Kernels MPJPE (mm) Params (M)
2 2 3 51.7 2.44
2 2 3,3 51.6 2.46
2 2 3,3,3 50.8 2.48
2 2 5 50.5 2.46
2 2 5,5 52.0 2.49
2 2 5,5,5 51.9 2.52
2 2 7 50.5 2.47
2 2 7,7 50.1 2.51
2 2 7,7,7 49.4 2.56
2 2 9 51.5 2.48
2 2 9,9 50.8 2.54
2 2 9,9,9 50.6 2.60
2 2 3,5,7 50.6 2.52
2 2 5,7,9 51.6 2.56

Bảng 5: Kết quả số lượng tham số và FLOPs với MPJPE cho các kiến trúc transformer khác nhau và mạng chú ý đồ thị được phân tách theo trường tiếp nhận. Nhóm cuối cùng là cho các mô hình có trường tiếp nhận lớn nhất. Tốt nhất và tốt thứ hai được đánh dấu màu Đỏ và Xanh tương ứng.

T Params (M) FLOPs (M)* MPJPE (mm)
Zheng et al. [5] 9 9.58 180 49.9
Li et al. [68] 9 19.09 340 47.8
ConvFormer 9 2.56 100 49.4
Zheng et al. [5] 27 9.59 540 47.0
Li et al. [68] 27 19.18 1040 45.9
ConvFormer 27 2.65 360 47.7
Zheng et al. [5] 81 9.60 1620 44.3
Li et al. [68] 81 19.84 3120 44.5
ConvFormer 81 3.43 1600 45.0
Liu et al. [77] 243 7.09 9700 44.9
Li et al. [68] 351 31.52 14160 43.0
ConvFormer 143 5.24 4220 43.7
ConvFormer 243 10.24 10000 43.2

Method Params (M) MPJPE (mm)
Baseline 5.2 52.4
Single-Filter ConvFormer 2.47 50.5
Dynamic ConvFormer 2.56 49.4

Bảng 6: Nghiên cứu loại bỏ phân tích hiệu ứng của các thành phần khác nhau của ConvFormer trên Độ chính xác và Số lượng Tham số.

--- TRANG 10 ---
7 Phụ lục

7.1 Trực quan hóa Chú ý

Chúng tôi cung cấp trong hình 4 các trực quan hóa bổ sung của các đầu chú ý thời gian cho một phần của phân tích loại bỏ của chúng tôi về số đầu chú ý với kết quả định lượng được báo cáo trong Bảng 3. Chúng tôi lưu ý rằng mô hình 8 đầu của chúng tôi đạt được MPJPE thấp nhất trên H3.6M. Chúng tôi giả thuyết rằng, mặc dù có dấu hiệu trực quan rõ ràng rằng khi số đầu tăng, các dư thừa trong các bản đồ chú ý xảy ra với các biến thể tinh tế, rằng dư thừa này hoạt động như một cơ chế lọc nhiễu bằng cách làm nổi bật thông tin quan trọng. Trong bối cảnh NLP một phân tích mở rộng về BERT đã được tiến hành để hiểu số đầu tối ưu và cách có thể thực hiện cắt tỉa đầu trong thời gian thử nghiệm mà không ảnh hưởng hiệu suất đáng kể, [74].

Chúng tôi cũng cung cấp các trực quan hóa của các đầu chú ý cho cả ConvFormer không gian và thời gian cho tất cả các đầu chú ý được sử dụng trong mô hình của chúng tôi. Chúng tôi đánh giá các đầu chú ý cho đối tượng 9 từ H3.6M cho hành động Directions. Các bản đồ tự chú ý không gian được thấy trong Hình 5 và trục x tương ứng với 17 khớp của xương H3.6M và trục y tương ứng với đầu ra chú ý. Những bản đồ này tương ứng với mô hình 143 khung hình và các bản đồ chú ý thời gian trục x là 143 khung hình của chuỗi trong khi trục y là chú ý tại mỗi khung hình. Các đầu chú ý trả về các độ lớn chú ý khác nhau đại diện cho các tương quan không gian hoặc tương quan toàn cục theo khung hình được học từ các hồ sơ thời gian khớp chung.

--- TRANG 11 ---
Hình 4: Chú ý thời gian cho ConvFormer 9 khung hình được huấn luyện trên H36M sử dụng phát hiện CPN như đầu vào – a) là mô hình một đầu của chúng tôi, b) là mô hình 2 đầu, c) là 4 đầu và d) là mô hình 8 đầu đạt được MPJPE thấp nhất.

--- TRANG 12 ---
Hình 5: Ví dụ về Bản đồ Chú ý, phía trên là ConvFormer không gian và phía dưới là ConvFormer thời gian cho mô hình 143 khung hình được huấn luyện trên phát hiện CPN cho H3.6M. Những bản đồ này được tạo ra cho S9 cho hành động Directions.

--- TRANG 13 ---
Tài liệu tham khảo

[1]J. Martinez, R. Hossain, J. Romero, aand J. J. Little, title=A simple yet effective baseline for 3d human pose estimation, A simple yet effective baseline for 3d human pose estimation, Proceedings of the IEEE International Conference on Computer Vision, (2017), 2640-2649.

[2]D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli, 3d human pose estimation in video with temporal convolutions and semi-supervised training, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2019), 7753-7762.

[3]R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. Gonzalez, and I. Stoica, Tune: A Research Platform for Distributed Model Selection and Training, arXiv preprint arXiv:1807.05118, (2018).

[4]A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, Large-Scale Video Classification with Convolutional Neural Networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2014), 1725-1732.

[5]C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Cheng, and Z. Ding, 3D Human Pose Estimation with Spatial and Temporal Transformers, Proceedings of the IEEE International Conference on Computer Vision (ICCV), (2021).

[6]H. S. Faang, Y. Xu, W. Wang, X. Liu, and S. C. Zhu, Learning pose grammar to encode human body configuration for 3d pose estimation, Thirty-Second AAAI Conference on Artificial Intelligence, (2018).

[7]C. Ionescu, D. Papava, V. Olarue, C. Sminchisescu, Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments, IEEE transactions on pattern analysis and machine intelligence, 36 (7), (2013), 1325-1339.

[8]L. Sigal, A. O. Balaan, and M. J. Black, Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion, IEEE Transactions on Pattern Analysis and Machine Intelligence, 87 (12), (2010), 4-27.

[9]G. Pavaalkos, X. Zhou, K. G. Derpanis, and K. Daniilidis, Coarse-to-fine volumetric prediction for single-image 3d human pose, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2017), 7025-7034.

[10]D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt, Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision,3D Vision (3DV), 2017 Fifth International Conference on (2017

[11], T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, and J. Luo, Anatomy-aware 3d human pose estimation with bone-based pose decomposition, IEEE Transactions on Circuits and Systems for Video Technology, (2021).

[12]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. U. Kaiser, and I. Polosukhin, Attention is all you need, Advances in Neural Information Processing Systems (NIPS), (2017), 5998-6008.

[13]R. Liu, J. Shen, H. Wang, C. Chen, S. C. Cheung, and V. Asari, Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CPRV), (2020), 5064-5073.

[14]M. Kocabas, S. Karagozz, and E. Akbas, Self-supervised learning of 3d human pose using multi-view geometry, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2019), 1077-1086.

[15]M. Raayat Imtia Hossain and J. J. Little, Exploiting temporal information for 3d human pose estimation, Proceedings of the European Conference on Computer Vision (ECCV), (2018), 68-84.

[16]J. Wang, S. Yan, Y. Xiong, and D. Lin, Motion guided 3d pose estimation from videos, ArXiv, arXiv:2004.13985, (2020).

[17]A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, Automatic Differentiation in PyTorch, NIPS 2017 Workshop on Autodiff, (2017).

[18]J. Wang, S. Yan, Y. Xiong, and D. Lin, Deep networks with stochastic depth, European Conference on Computer Vision (ECCV), (2016), 646-661.

[19]Z. Cao, G. Hidalgo, T. Simon, S-E. Wei, and Y. Sheikh, OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields, IEEE transactions on pattern analysis and machine intelligence, 43 (1), (3019), 172-186.

[20]S-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, Convolutional pose machines, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, (2016), 4724-4732.

[21]A. Kolesnikov, A. Dosovitskiy, D. Weissenborn, G. Heigold, J. Uszkoreit, L. Beyer, M. Minderer, M. Dehghani, N. Houlsby, S. Gelly, T. Unterthiner, and X. Zhai, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, (2021).

[22]N. Srivastava, G. Hinton, A. Krihevsky, I. Sutskever, and R. Salakhudinov, Dropout: A Simple Way to Prevent Neural Networks from Overfitting, Journal of Machine Learning Research, 15 (56), (2014), 1929-1958.

[23]H-S. Fang, S. Xie, Y-W. Tai, and C. Lu, RMPE: Regional Multi-person Pose Estimation, International Conference on Computer Vision (ICCV), (2017).

--- TRANG 14 ---
[24]Y. Chen, Z. Wang, X. Peng, Z. Zhang, G. Yu, and J. Sun, Cascaded Pyramid Network for Multi-person Pose Estimation, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2018), 7103-7112.

[25]K. Sun, B. Xiao, D. Liu, and J. Wang, Deep High-Resolution Representation Learning for Human Pose Estimation, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2019).

[26]B. Xiao, H. Wu, Y. Wei, Simple Baselines for Human Pose Estimation and Tracking, European Conference on Computer Vision (ECCV), (2020).

[27]Z. Wu, Z. Liu, J. Lin, and S. Han, Lite transformer with long-short range attention, International Conference on Learning Representations (ICLR), (2020).

[28]Z. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan, Convbert: Improving bert with span-based dynamic convolution, Advances in Neural Information Processing Sytems (NeurIPS), (2020).

[29]Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, Dynamic Convolution: Attention Over Convolution Kernels, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2020), 11027-11036.

[30]A. Zeng, X. Sun, F. Huaang, M. Liu, Q. Xu, and S. Lin, Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach, European Conference on Computer Vision (ECCV), (2020).

[31]R. Dabral, A. Mundhaada, U. Kusupati, S. Afaque, A. Sharma, and A. Jain, Learning 3D Human Pose from Structure and Motion, European Conference on Computer Vision (ECCV), (2018).

[32]Y. Cai, L. Ge, J. Liu, J. Cai, T-J. Cham, J. Yuan, N. M. Thalmann, Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks, 2019 IEEE/CVF International Conference on Computer Vision (ICCV), (2019), 2272-2281.

[33]J. Lin and G. Lee, Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation, British Machine Vision Conference (BMVC), (2019).

[34]R. Yeh, Y. T. Hu, and A. Schwing, Chirality Nets for Human Pose Regression, International Conference on Neural Information Processing Systems (NeurIPS), (2019).

[35]K. Lin, L. Wang, aand Z. Liu, End-to-End Human Pose and Mesh Reconstruction with Transformers, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2021).

[36]B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, Learning Deep Features for Discriminative Localization, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2016), 2921-2929.

[37]G. Moon, J. Chang, and K. M. Lee, Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image, The IEEE Conference on International Conference on Computer Vision (ICCV), (2019).

[38]G. Pavlakos, X. Zhou, and K. Daniilidis, Ordinal depth supervision for 3D human pose estimation, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2018).

[39]G. Moon and K. M. Lee, I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose estimation and mesh estimation from a single rgb image, European Conference on Computer Vision (ECCV), (2020).

[40]S. Khan, M. Naseer, M. Hayat, S. Zamir, F. Khan, and M. Shah, Transformers in Vision: A surver, ArXiv, arXiv:2101.01169, (2021).

[41]N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillob, and S. Zagoruyko, Ene-to-End Object Detection with Transformers, ArXiv, http://arxiv.org/abs/2005.12872, (2020).

[42]S. Li and A. B. Chan, 3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network, Asian Conference on Computer Vision (ACCV), (2014).

[43]S. Li, L. Ke, K. Pratama, Y. W. Tai, C. K. Tang, K. T. Cheng, Cascaded deep monocular 3D human pose estimation with evolutionary training data, ArXiv arXiv:2006.07778 (2020).

[44] X. Sun, B. Xiao, S. Liang, Y. Wei, Integral human pose regression, ArXiv arXiv:1711.08229, (2017).

[45]A. Kanmazawa, M. J. Black, D. W. Jacobs, and J. Malik, End-to-end Recovery of Human Shape and Pose, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2017).

[46]K. Zhou, X. Han, N. Jaing, K. Jia, and J. Lu, HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation, International Conference of Computer Vision (ICCV), (@019).

[47]Y. Cheng, B. Yang, B. Waang, Y. Wending, and R. T. Tang, Occlusion Aware Networks for 3D Human Pose Estimation in Video, International Conference of Computer Vision (ICCV), (2019).

[48]K. hou, X. Han, N. Jiang, K. Jai, aand J. Lu, HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human Pose and Shape Estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). (2021).

[49]F. Huang, A. Zeng, M. Liu, Q. Lai, and Q. Xu, DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation from Multi-View Image, IEEE Winter Conference on Applications of Computer Vision (WACV), (2020), 418-427.

[50]H. Shuai, L. Wu, and Q. Liu, Adaptively Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation, ArXiv abs/2110.05092, (2021).

[51]K. Iskakov, E. Burkov, V. Lempitsky, and Y. Malkov, Learnable Triangulation of Human Pose. IEEE/CVF International Conference on Computer Vision (ICCV), (2019).

--- TRANG 15 ---
[52]H. Qiu, C. Wang, J. Wang, N. Wang, and W. Zeng, Cross View Fusion for 3D Human Pose Estimation, IEEE/CVF International Conference on Computer Vision (ICCV), (2019), 4341-4359.

[53]J. Dong, W. Jiang, Q. Huang, H. Bao, X. hou, Fast and Robust Multi-Person 3D Pose Estimation from Multiple View, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2019).

[54]Y. He, R. Yan, K. Fragkiadaki, S. Yu. Epipolr Transformers, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2020), 7776-7785.

[55]L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, . Jiang, F. Tay, J. Feng, and S. Yan, Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, IEEE International Conference on Computer Vision (ICCV) (2021), 538-547

[56]Z. Liu, N. Shun, W. Li, J. Lu, Y. Wu, C. Li, aand L. Yang, ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis, ArXiv, abs/2011.10185 (2020).

[57]H. Touvron, M. Cord, M. Doue, F. Massa, A. Sablayrolles, H. J'egou, Training data-efficient image transformers and distillation through attention, International Conference of Machine Learning (ICML), (2021).

[58]S. Gilad, N. Asaf, -M. Lihi, An Image is Worth 16x16 Words, What is a Video Worth?, ArXXiv, abs/2103.13915 (2021).

[59]K. Chaitanya, M. Joshua, D. Hang, M-S. Roderick, CpT: Convolutional Point Transformer for 3D Point Cloud Processing, ArXXiv, abs/2111.10866 (2021).

[60] P. Paaschalis and A. Antonis, PE-former: Pose Estimation Transformer, CoRR abs/2112.04981, (2021).

[61]N. Kitaev, L. Kaiser, and A. Levskaya, Reformer: The Efficient Transformer, Proceedings of International Conference on Learning Representations (ICLR) (2020).

[62]J. Sebastian, C. Aakanksha, M. Afroz, K. Lukasz, G. Wojciech, M. Henryk, and K. Jonni, Sparse is Enough in Scaling Transformers, (2021).

[63]V. Belagiannis, S. Amin, M. AAndriluka, B. Schiele, N. Navab, and S. Ilic, 3D Pictorial Structures Revisited: Multiple Human Pose Estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38 (10) (2016) 1929-1942.

[64]A. Diaz-Arias, M. Messmore, D. Shin, and S. Baek, On the role of depth predictions for 3D human pose estimation, https://arxiv.org/abs/2103.02521 (2021).

[65]X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong and W. Woo, Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting, Proceedings of the 28th International Conference on Neural Information Processing Systems, 1 (2015) 802-810.

[66] Q. Basatiaan, rnn: a Recurrent Neural Network in R, Working Papers (2016).

[67] S. Hochreiter and H. Schmidhuber, Long short-term memory, Neural computation, 9 (8) (1997) 1735-1780.

[68]W. Li, T. Hong, W. Hao, V. Picho, and L. Van Gool, MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2022).

[69]Z. Zou and W. Tang, Modulated graph convolutional network for 3D human pose estimation, IEEE International Conference on Computer Vision (ICCV), (2021), 11477-11487.

[70]K. Gong, J. Zhang, and J. Feng, PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation, CVPR, 2021.

[71]W. Li, H. Liu, R. Ding, M. Liu, P. Wang, W. Yang Exploiting temporal contexts with strided transformer for 3d human pose estimation, IEEE Transaactions on Multimedia, 2022.

[72]M. Zanﬁr, A. Zanﬁr, E. Bazavan, W. Freeman, R. Sukthankar, and C. Sminchisescu, Thundr: Transformer-based 3d human reconstruction with markers, Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.

[73]V. Sovrasov, Flops counter for convolutional networks in pytorch framework, https://github.com/sovrasov/ﬂops-counter.pytorch/, 2019-11-12.

[74]P. Michel, O. Levy, and G. Neubig, Are Sixteen Heads Really Better than One?, Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), (2019).

[75]Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio, Graph Attention Networks, International Conference on Learning Representations (ICLR), 2018

[76]P. -E. Sarlin, D. DeTone, T. Malisiewicz and A. Rabinovich, "SuperGlue: Learning Feature Matching With Graph Neural Networks," 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp. 4937-4946

[77]Liu, Junfa and Rojas, Juan and Li, Yihui and Liang, Zhijun and Guan, Yisheng and Xi, Ning and Zhu, Haifei, "A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video," 2021 IEEE International Conference on Robotics and Automation (ICRA), Xi'an, China, 2021, pp. 3374-3380

[78]Jianzhai Wu, Dewen Hu, Fengtao Xiang, Xingsheng Yuan, and Jiongming Su. 2020. 3D human pose estimation by depth map. Vis. Comput. 36, 7 (Jul 2020), 1401–1410

[79]S. Banik, A. M. GarcÍa and A. Knoll, "3D Human Pose Regression Using Graph Convolutional Network," 2021 IEEE International Conference on Image Processing (ICIP), Anchorage, AK, USA, 2021, pp. 924-928
