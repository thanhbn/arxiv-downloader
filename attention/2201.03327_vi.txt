I'll translate this paper to Vietnamese while maintaining the exact structure. Let me start with the translation:

# 2201.03327.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2201.03327.pdf
# Kích thước tệp: 2452838 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
1
Bộ Mã Hóa Transformer Có Thể Điều Chỉnh Độ Trễ cho
Hiểu Biết Ngôn Ngữ
Sajjad Kachuee, Mohammad Sharifkhani
Khoa Kỹ Thuật Điện
Đại học Công nghệ Sharif
Tóm tắt —Điều chỉnh độ trễ, công suất và độ chính xác của các mô hình hiểu biết ngôn ngữ tự nhiên là một mục tiêu mong muốn của một kiến trúc hiệu quả. Bài báo này đề xuất một kiến trúc Transformer hiệu quả điều chỉnh chi phí tính toán suy luận một cách thích ứng với độ trễ suy luận mong muốn. Trong giai đoạn tinh chỉnh, phương pháp đề xuất phát hiện các phần tử chuỗi ẩn (word-vectors) ít quan trọng hơn và loại bỏ chúng trong mỗi lớp mã hóa bằng cách sử dụng một chỉ số Đóng Góp Ngữ Cảnh Chú Ý (ACC) được đề xuất. Sau giai đoạn tinh chỉnh, với thuộc tính điều chỉnh ngoại tuyến mới, độ trễ suy luận của mô hình có thể được điều chỉnh trong một phạm vi rộng của các lựa chọn tăng tốc suy luận mà không cần bất kỳ huấn luyện thêm nào. Các thí nghiệm mở rộng cho thấy hầu hết các word-vectors trong các lớp Transformer cao hơn đóng góp ít hơn cho các lớp tiếp theo, cho phép loại bỏ chúng để cải thiện độ trễ suy luận. Kết quả thí nghiệm trên các nhiệm vụ hiểu biết ngôn ngữ, tạo văn bản và điều chỉnh hướng dẫn khác nhau và các benchmark chứng minh hiệu quả của phương pháp trên các tập dữ liệu đa dạng, với tác động tối thiểu đến ngữ cảnh toàn cục của đầu vào. Kỹ thuật cải thiện Thời Gian đến Token Đầu Tiên (TTFT) của Llama3 lên đến 2.9x, với sự giảm hiệu suất nhỏ. Phương pháp được đề xuất cho rằng trong các Mô hình Ngôn ngữ Lớn (LLMs), mặc dù mạng hoàn chỉnh cần thiết cho huấn luyện, nó có thể được cắt ngắn trong giai đoạn tinh chỉnh.
Thuật ngữ chỉ mục —Hồ sơ loại bỏ word-vector, đóng góp ngữ cảnh chú ý (ACC), điều chỉnh ngoại tuyến, độ trễ có thể điều chỉnh.

I. GIỚI THIỆU
Gần đây, các kiến trúc dựa trên Transformer [1] đã đạt được thành công đáng kể trong các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên (NLP) khác nhau. Tuy nhiên, khó khăn trong huấn luyện, độ trễ suy luận và kích thước nhỏ của các tập dữ liệu có sẵn là những mối quan tâm chính khi sử dụng các mô hình này trong các ứng dụng thực tế [2]. Tinh chỉnh một mô hình ngôn ngữ được huấn luyện trước trên các nhiệm vụ cụ thể giúp giảm thiểu một số mối quan tâm này, bao gồm nỗ lực huấn luyện và kích thước tập dữ liệu.
BERT [3], XLNet [4], GPT [5], Llama [6], và Gemma [7] đại diện cho một phổ các mô hình ngôn ngữ được huấn luyện trước, từ những phát triển sớm đến những tiến bộ mới nhất. Các mô hình này khác nhau đáng kể về kích thước, từ hàng trăm triệu đến hàng trăm tỷ tham số. Do đó, độ phức tạp tính toán mang theo hàng nghìn tỷ Phép Toán Dấu Phẩy Động (FLOPs) để xử lý một mẫu duy nhất. Trong một số trường hợp, nỗ lực tính toán giai đoạn suy luận vượt quá khả năng của các hệ thống có tài nguyên hạn chế như các thiết bị biên.
Do đó, các mô hình Transformer cơ bản gây ra giới hạn độ trễ và sử dụng năng lượng không thể chấp nhận được trong các thiết bị như vậy.
BERT (Biểu diễn Mã hóa Hai chiều từ Transformers) và GPT (Transformer được Huấn luyện Trước Tạo sinh) là hai kiến trúc mốc trong sự phát triển của các mô hình ngôn ngữ lớn (LLMs). BERT, được giới thiệu bởi Google, sử dụng phương pháp huấn luyện hai chiều, cho phép nó nắm bắt ngữ cảnh từ cả hai hướng của một câu đồng thời. Điều này làm cho nó đặc biệt hiệu quả cho các nhiệm vụ như phân loại câu, trả lời câu hỏi và nhận dạng thực thể có tên, nơi hiểu biết mối quan hệ giữa các từ là quan trọng [3]. Mặt khác, GPT, được phát triển bởi OpenAI, áp dụng phương pháp tự hồi quy, tạo văn bản từng từ một theo hướng đơn chiều. Bắt đầu với GPT-1 và phát triển qua GPT-4, GPT đã chứng minh sức mạnh của mình trong việc tạo văn bản sáng tạo và mạch lạc, AI hội thoại và tóm tắt văn bản, làm cho nó trở thành một mô hình rất linh hoạt cho cả việc tạo nội dung và các ứng dụng tương tác [5], [8].

Các mô hình gần đây hơn như LLaMA của Meta (Large Language Model Meta AI) và Gemma đang đẩy ranh giới của sự phát triển LLM. LLaMA được thiết kế để có hiệu quả cao về mặt tài nguyên tính toán, cung cấp một mô hình nhỏ hơn vẫn hoạt động tốt trên một loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên. Thiết kế được sắp xếp hợp lý của nó làm cho nó có thể tiếp cận được cho nghiên cứu và các ứng dụng công nghiệp nơi tài nguyên có thể bị hạn chế [6]. Gemma, một họ mô hình mới nổi, dự kiến sẽ tinh chỉnh thêm hiệu quả và khả năng thích ứng của LLMs, với sự nhấn mạnh vào hiệu suất thời gian thực và khả năng mở rộng. Gemma đặc biệt phù hợp cho các ứng dụng cụ thể của ngành, nhằm mục đích thu hẹp khoảng cách giữa LLMs có mục đích chung và các trường hợp sử dụng chuyên biệt hơn [7]. Cùng nhau, các họ mô hình này phản ánh sự tiến bộ nhanh chóng và chuyên môn hóa trong lĩnh vực mô hình ngôn ngữ lớn, mỗi mô hình cung cấp những điểm mạnh riêng biệt dựa trên nhu cầu của người dùng và nhà phát triển.

Trong lĩnh vực tối ưu hóa Mạng Neural Sâu (DNNs), các phương pháp đa dạng đã được đề xuất để tăng tốc độ và giảm tiêu thụ năng lượng. Điều này rất quan trọng cho các ứng dụng như kéo dài tuổi thọ pin và cho phép các tác vụ mạnh mẽ trên các thiết bị nhỏ. Các phương pháp có thể được phân loại thành cắt tỉa, loại bỏ các phần không cần thiết; lượng tử hóa, đơn giản hóa việc sử dụng số; Chưng cất Kiến thức, đơn giản hóa dựa trên một mô hình phức tạp; chia sẻ tham số, sử dụng cùng thông tin trên các thành phần; phân tích tensor, phá vỡ các cấu trúc dữ liệu; và transformers có độ phức tạp dưới bậc hai [9].

Nghiên cứu này giới thiệu một lớp chú ý sáng tạo được thiết kế để tăng cường hiệu quả của các giai đoạn suy luận trong các mô hình dựa trên Transformer với một phương pháp có thể điều chỉnh. Giải pháp được đề xuất bao gồm một quá trình sắp xếp và loại bỏ cho các phần tử chuỗi ẩn (word-vectors), dẫn đến việc giảm word-vectors hiệu quả trong mỗi lớp và giảm số lượng FLOPs. Những đóng góp chính của nghiên cứu này
arXiv:2201.03327v9 [cs.CL] 18 Sep 2024

--- TRANG 2 ---
2
có thể được tóm tắt như sau:
•Cấu trúc mới và chính sách điều chỉnh được đề xuất để tăng tốc độ thời gian suy luận với sự suy giảm độ chính xác không đáng kể.
•Một chỉ số Đóng Góp Ngữ Cảnh Chú Ý (ACC) mới được đề xuất để quan sát đóng góp ngữ cảnh trong mỗi lớp chú ý. Nó được chỉ ra rằng đóng góp ngữ cảnh giảm đáng kể ở các lớp mã hóa cuối.
•Một ước tính tăng tốc suy luận phân tích chính xác được trình bày cho phương pháp đề xuất giúp một nhà thiết kế hệ thống xác định việc lựa chọn siêu tham số trong giai đoạn tinh chỉnh.
•Thuộc tính điều chỉnh ngoại tuyến điều chỉnh độ trễ suy luận sau giai đoạn tinh chỉnh. Thuộc tính này kiểm soát sự đánh đổi giữa độ trễ và độ chính xác trong một phạm vi rộng của các lựa chọn tăng tốc suy luận mà không yêu cầu huấn luyện bổ sung.
•Phương pháp đề xuất có thể được áp dụng cho nhiều kiến trúc dựa trên Transformer, đồng thời bảo tồn ngữ cảnh toàn cục của đầu vào.

II. CÁC CÔNG TRÌNH LIÊN QUAN
Các phương pháp dựa trên cắt tỉa đã được khám phá rộng rãi trong văn liệu hiện có, với việc phân loại dựa trên các phương pháp cắt tỉa riêng biệt. Trong các phương pháp cắt tỉa trọng số, trọng số của mô hình được cắt tỉa một cách có hệ thống theo một ngưỡng được xác định trước [10]–[13]. Các phương pháp cắt tỉa đầu và kênh mở rộng phạm vi của chúng bằng cách loại bỏ hoàn toàn một kênh hoặc đầu của mạng [14]–[24]. Các chiến lược cắt tỉa lớp và khối [25]–[27], bao gồm việc loại bỏ có mục tiêu các lớp hoặc khối toàn bộ trong mạng. Các phương pháp cắt tỉa token và word-vector, áp dụng một phương pháp tinh tế hơn, tập trung vào việc loại bỏ có chọn lọc các token hoặc word-vectors cụ thể trong mô hình [28]–[30]. Phân loại này làm rõ các phương pháp khác nhau được thực hiện trong mô hình cắt tỉa, mỗi phương pháp đóng vai trò độc đáo trong việc đạt được mục tiêu chung của việc tối ưu hóa mạng.

A. Cắt Tỉa Trọng Số
Trong [31], phương pháp cắt tỉa trọng số độ lớn được áp dụng cho BERT và cho thấy rằng mô hình có thể được cắt tỉa một lần trong quá trình huấn luyện trước thay vì riêng biệt cho từng nhiệm vụ mà không ảnh hưởng đến hiệu suất. Cắt Tỉa Gần Kề Được Cân Bằng Lại (RPP) đã giới thiệu một phương pháp cắt tỉa được thiết kế đặc biệt cho một mô hình biểu diễn ngôn ngữ quy mô lớn tốt hơn nhiều so với cắt tỉa lặp đi lặp lại điển hình. Phương pháp này tạo ra một phiên bản thưa thớt của BERT [32]. Cắt tỉa vector có độ dài hỗn hợp Transformer (TF-MVP) kiểm tra các mẫu thưa thớt trong Transformers đã cắt tỉa và giới thiệu kiến trúc TF-MVP sử dụng kỹ thuật cắt tỉa vector có độ dài hỗn hợp (MVP) [33]. Trong khi TF-MVP đạt được tỷ lệ cắt tỉa khoảng 70%, có sự suy giảm đáng kể về độ chính xác. Trong [34], các trọng số mô hình được khuyến khích bằng không với thuật ngữ chính quy hóa L0.

B. Cắt Tỉa Đầu/Kênh
Một khung giảm tham số được đề xuất bằng cách xác định các đầu quan trọng nhất trong mỗi lớp mã hóa sử dụng lan truyền mức độ liên quan theo lớp và cắt tỉa các đầu dư thừa [35]. Nó cho thấy rằng các đầu của lớp mã hóa sớm hơn quan trọng hơn nhiều so với lớp cuối. Hơn nữa, trong [36], nó được chỉ ra rằng khoảng 17% đầu chú ý có thể được loại bỏ tại thời điểm kiểm tra mà không ảnh hưởng đáng kể đến hiệu suất suy luận. Trong [37], một khung cắt tỉa nhanh sau huấn luyện được thiết kế riêng cho Transformers được giới thiệu. Khung này bao gồm ba giai đoạn tinh chỉnh chính: tìm kiếm mặt nạ, sắp xếp lại mặt nạ và điều chỉnh mặt nạ. Đáng chú ý, phương pháp này loại bỏ nhu cầu huấn luyện lại, trình bày một phương pháp hiệu quả để đạt được nén mô hình trong kiến trúc Transformer.

C. Cắt Tỉa Lớp/Khối
Việc loại bỏ lớp tiệm tiến trong giai đoạn huấn luyện trước của kiến trúc BERT cung cấp tốc độ tăng lên đến 2.5 lần trong giai đoạn suy luận với khả năng chuyển giao kiến thức tốt hơn và khái quát hóa tốt hơn trên các nhiệm vụ cụ thể [38]. FastBERT được đề xuất với thời gian suy luận thích ứng trong đó mô hình sử dụng số lượng lớp mã hóa đủ dựa trên sự chắc chắn suy luận của một mẫu [39]. Nó điều chỉnh động số lượng lớp hoạt động để giảm nỗ lực tính toán tổng thể. ELBERT đề xuất cơ chế Thoát Sớm Dựa trên Cửa sổ Tin cậy, cải thiện tốc độ suy luận ALBERT [40].
Trong [41], nó được chỉ ra thí nghiệm rằng việc sử dụng tất cả các lớp của một mô hình được huấn luyện trước trong các nhiệm vụ cụ thể là không cần thiết. Nghiên cứu các chiến lược loại bỏ lớp khác nhau cho thấy nhiều trọng số mô hình không cần thiết. LayerDrop giới thiệu một loại dropout có cấu trúc mang lại lợi ích chính quy hóa trong quá trình huấn luyện và cho phép cắt tỉa hiệu quả tại thời điểm suy luận, dẫn đến việc loại bỏ khoảng một nửa các lớp Transformer [42].

D. Cắt Tỉa Token/Word-Vector
PoWER-BERT đề xuất một phương pháp loại bỏ word-vector loại bỏ các word-vectors dư thừa [28]. Một lớp có thể học bổ sung được thêm giữa mỗi lớp tự chú ý và feed-forward của mã hóa, và một thuật ngữ hàm mất mát bổ sung được sử dụng để huấn luyện các tham số bổ sung này. Huấn luyện PoWER-BERT bao gồm 3 bước "tinh chỉnh, tìm kiếm cấu hình loại bỏ và huấn luyện lại" [28]. Funnel-Transformer được lấy cảm hứng từ U-net từ thị giác máy tính [43]. Nó cải thiện kiến trúc Transformer bằng cách chèn một lớp gộp trong mỗi lớp tự chú ý [29]. Funnel-Transformer nén chuỗi trạng thái ẩn thành một chuỗi ngắn hơn và giảm chi phí tính toán. Cắt Tỉa Token Đã Học (LTP) loại bỏ thích ứng các token không quan trọng khi một chuỗi đầu vào đi qua các lớp transformer [30]. LTP được áp dụng cho RoBERTa [44], dẫn đến cải thiện thông lượng lên đến 2.0 lần với sự giảm độ chính xác ít hơn 1%.

Trong STTABT [45], tác động của cắt tỉa token đối với sự chú ý của các lớp sau và đối với dự đoán cuối cùng được nghiên cứu. Nghiên cứu cho thấy rằng việc theo dõi ngược chú ý được đề xuất cho phép mô hình giữ lại hiệu suất của mô hình đầy đủ tốt hơn ngay cả ở tỷ lệ thưa thớt cao. Ngược lại, vì trong kiến trúc tự chú ý tất cả các phần tử đầu vào đóng góp vào việc hình thành mỗi đầu ra chú ý, có thể nói rằng trong quy trình cắt tỉa word-vector, không có khái niệm độc lập nào bị loại bỏ. Trong các lớp tiếp theo, mọi khái niệm giữ lại khả năng mở rộng

--- TRANG 3 ---
3
và ảnh hưởng đến đầu ra và dự đoán mô hình. Ngoài ra, STTABT cắt tỉa token dựa trên một mạng xấp xỉ chú ý nhẹ được huấn luyện với chưng cất kiến thức, điều này thêm chi phí huấn luyện và suy luận bổ sung.

Trong [46], một cơ chế có thể học được được sử dụng để xác định token không có thông tin nào có thể được loại bỏ khỏi ngữ cảnh tại bất kỳ thời điểm nào trong quá trình tạo. Trong [47], mô hình tận dụng một cơ chế chú ý bất đối xứng để lặp đi lặp lại chưng cất đầu vào thành một nút thắt cổ chai tiềm ẩn chặt chẽ, cho phép nó mở rộng để xử lý các đầu vào rất lớn.

E. Các Phương Pháp Không Cắt Tỉa
ALBERT là một phiên bản nhẹ hơn của mô hình BERT [48]. Nó sử dụng chia sẻ trọng số giữa các lớp và cấu trúc lớp nhúng đã sửa đổi, làm cho kích thước nhúng độc lập với kích thước ẩn của mã hóa. Hơn nữa, nhiệm vụ Dự Đoán Thứ Tự Câu (SOP) mới được sử dụng trong giai đoạn huấn luyện trước thay vì nhiệm vụ Dự Đoán Câu Tiếp Theo. ALBERT báo cáo kết quả tiên tiến với chính sách giảm tham số và huấn luyện trước đã sửa đổi. ALBERT nhỏ hơn 9 lần và nhanh hơn 1.2 lần so với BERT-base, với sự giảm độ chính xác trung bình ít hơn 2.5%. ALBERT là một ví dụ về kỹ thuật chia sẻ tham số.

DistilBERT [49], TinyBERT [50] và MobileBERT [51] sử dụng kỹ thuật chưng cất kiến thức trong quá trình huấn luyện trước và cung cấp suy luận nhanh hơn đến 5.5 lần trên các nhiệm vụ cụ thể so với BERT-base. Trong [52] một cơ chế Tự Chưng cất (SD) thu được các mô hình có độ chính xác cao trực tiếp mà không cần thông qua một mô hình hỗ trợ. Những mối quan tâm chính với các kỹ thuật này là nỗ lực huấn luyện trước mô hình từ đầu, các bước huấn luyện bổ sung và sự suy giảm độ chính xác đáng kể.

Trong [53], các tác giả đề xuất các điều kiện đủ mà dưới đó họ chứng minh rằng một mô hình chú ý thưa thớt có thể xấp xỉ phổ quát bất kỳ hàm chuỗi-tới-chuỗi nào. Họ cho thấy rằng Transformers thưa thớt với chỉ O(n) kết nối mỗi lớp chú ý có thể xấp xỉ cùng lớp hàm với mô hình dày đặc có n² kết nối. Do đó, trong những năm gần đây, nhiều công trình [54]–[62] đã đề xuất các phương pháp nhằm giảm độ phức tạp của cơ chế chú ý.

Thích Ứng Thứ Hạng Thấp (LoRA) bao gồm việc đóng băng trọng số trong mô hình được huấn luyện trước, và trong giai đoạn tinh chỉnh, nó giới thiệu các ma trận phân tích thứ hạng có thể huấn luyện vào mỗi lớp của kiến trúc Transformer [63]. LoRA tăng cường hiệu quả sử dụng bộ nhớ trong giai đoạn huấn luyện lên đến 3 lần, nhưng nó không mang lại lợi thế nào trong giai đoạn suy luận. QLoRA cải thiện so với LoRA bằng cách lượng tử hóa mô hình transformer về độ chính xác 4-bit và sử dụng trình tối ưu hóa được phân trang để xử lý các đỉnh bộ nhớ [64]. LQ-LoRA sử dụng thuật toán lặp để phân tích mỗi ma trận được huấn luyện trước thành một thành phần thứ hạng thấp có độ chính xác cao và một thành phần lượng tử hóa tiết kiệm bộ nhớ [65].

Các công trình trước đây đang cố gắng giảm kích thước mô hình và độ trễ suy luận. Các công trình này cho thấy rằng một phần đáng kể tính toán trong các mô hình có vẻ không cần thiết, và các mô hình với độ phức tạp tính toán ít hơn có thể được thu được từ những mô hình ban đầu. Những nhược điểm chính của các phương pháp này là huấn luyện trước từ đầu, tinh chỉnh nhiều giai đoạn và sự giảm độ chính xác suy luận đáng kể. Trong các phương pháp này, tăng tốc suy luận được đặt trong giai đoạn tinh chỉnh, và tăng tốc đạt được được cố định trong giai đoạn suy luận. Giải pháp được đề xuất cố gắng giảm thiểu những nhược điểm trước đây. Nó trình bày cơ chế điều chỉnh ngoại tuyến mạnh mẽ kiểm soát tăng tốc suy luận của mô hình sau giai đoạn tinh chỉnh.

Tác động của các phương pháp tăng tốc đối với sự ổn định mô hình và hiểu biết ngữ cảnh toàn cục của đầu vào là một mối quan tâm quan trọng phải được nghiên cứu trong đánh giá giải pháp. Backpack học nhiều vector nghĩa không ngữ cảnh cho mỗi từ trong từ vựng và biểu diễn một từ trong chuỗi như một kết hợp tuyến tính không âm phụ thuộc ngữ cảnh của các vector nghĩa trong chuỗi này [66]. Mô hình Backpack cho thấy rằng mỗi từ trong chuỗi đầu vào có thể được hiểu như một vector nghĩa mã hóa các khía cạnh khác nhau của một từ rất nhạy cảm. Một thực hành tốt để đánh giá hiệu suất của một phương pháp là đánh giá nó trong một nhiệm vụ phức tạp hơn, như các nhiệm vụ tạo văn bản kiểm tra khả năng thành thạo mô hình. Do đó phương pháp được đề xuất được áp dụng cho một số mô hình tự hồi quy và được đánh giá trên một số nhiệm vụ tạo văn bản.

Các phương pháp bộ nhớ đệm tập trung vào tối ưu hóa hiệu quả và tốc độ suy luận trong các mô hình ngôn ngữ lớn tự hồi quy (LLMs). KV-Runahead tăng cường song song hóa giai đoạn prompt bằng cách sử dụng nhiều quy trình để quản lý việc điền KV-cache, từ đó giảm Thời Gian đến Token Đầu Tiên (TTFT) [67]. CacheGen tăng tốc tải ngữ cảnh với một bộ mã hóa tensor tùy chỉnh nén KV-cache thành các biểu diễn bitstream nhỏ gọn, giảm thiểu chi phí giải mã [68]. CacheBlend giới thiệu một phương pháp tái sử dụng KV-caches được tính toán trước và cập nhật có chọn lọc một tập con nhỏ token, tối ưu hóa việc tái sử dụng cache [69]. Prompt Cache tăng tốc suy luận bằng cách tính toán trước và lưu trữ trạng thái chú ý cho các đoạn văn bản xuất hiện thường xuyên, cho phép tái sử dụng hiệu quả trên các prompt khác nhau [70]. Cuối cùng, CachedAttention sử dụng một hệ thống KV-caching phân cấp sử dụng các giải pháp bộ nhớ và lưu trữ tiết kiệm chi phí để quản lý KV-caches cho tất cả các yêu cầu [71]. Cùng nhau, các phương pháp này đóng góp vào suy luận LLM nhanh hơn và hiệu quả hơn bằng cách cải thiện quản lý cache và tái sử dụng trạng thái.

III. PHƯƠNG PHÁP ĐỀ XUẤT
Phần này trình bày chi tiết về phương pháp đề xuất và phân tích và lý luận đằng sau phương pháp. Phần III-A giới thiệu bối cảnh nền tảng sơ bộ của kiến trúc Transformer, trong khi Phần III-B phân tích độ trễ và TTFT trên các mô hình dựa trên Transformer khác nhau. Lớp mã hóa được đề xuất dựa trên hiểu biết về lượng đóng góp của word-vectors ở các lớp khác nhau, được xem xét trong Phần III-C. Phần III-D khám phá kiến trúc được đề xuất một cách chi tiết. Ngoài ra, Phần III-E mở rộng phương pháp đề xuất sang các kiến trúc tự hồi quy và áp dụng nó cho một số mô hình mới được phát triển.

A. Bối Cảnh
Mặc dù có tiến bộ và phát triển hiện tại trong các kiến trúc LLM chính như GPT, Gemma, Mistral [72], và Llama, các mô hình này vẫn tuân theo kiến trúc Transformer nền tảng [1]. Những khác biệt chính giữa chúng

--- TRANG 4 ---
4
Hình 1. Kiến trúc BERT-base [3] với các lớp đầu ra

nằm ở số lượng tham số, lớp, đầu, trạng thái ẩn và các sửa đổi cụ thể cho các lớp chuẩn hóa, đường dẫn dư và triển khai chú ý. Để rõ ràng và hiểu sâu hơn về phương pháp được đề xuất, chúng ta đầu tiên tập trung vào các mô hình BERT [48] và GPT-2 [73] trước khi mở rộng phương pháp sang các mô hình ngôn ngữ lớn khác (LLMs).

Hình 1 cho thấy kiến trúc mô hình BERT-base và bộ phân loại đầu ra. Câu đầu vào đi qua lớp tokenizer và lớp nhúng. Sau đó ngữ cảnh được đưa vào 12 lớp mã hóa nối tiếp để được xử lý. Các lớp pooler và phân loại được thêm vào lớp cuối để thực hiện đầu ra phù hợp cho nhiệm vụ mong muốn. Trong hình, T và H là số lượng word-vectors đầu vào và kích thước vector trạng thái ẩn, tương ứng.

Mỗi mã hóa bao gồm một lớp Tự Chú Ý Đa Đầu Hai chiều và một lớp Feed-Forward với một số phép nhân tensor. Các nhánh dư và lớp chuẩn hóa tăng cường tính ổn định huấn luyện.

B. Phân Tích Độ Trễ Kiến Trúc Transformer và TTFT
Bảng I trình bày một phân tích về độ trễ và tỷ lệ TTFT trên các lớp nhúng, chú ý và feed-forward trong một số mô hình dựa trên Transformer. Như được hiển thị trong bảng, bất kể kích thước mô hình, độ trễ của lớp nhúng là tối thiểu, với phần lớn độ trễ được quy cho các lớp chú ý và feed-forward. Tỷ lệ chú ý dao động từ khoảng 20% đến 47% của tổng số. Trong mạng neural, độ trễ và nỗ lực tính toán có liên quan chặt chẽ đến số lượng FLOPs. Do đó, việc giảm số lượng FLOPs trong các lớp mã hóa, chiếm phần lớn độ trễ, giảm đáng kể cả độ trễ tổng và TTFT.

C. Chỉ Số Đóng Góp Ngữ Cảnh Chú Ý (ACC)
Đóng góp ngữ cảnh của word-vectors tại mỗi lớp tự chú ý được nghiên cứu trong Phần này để tìm word-vectors quan trọng hơn tại mỗi lớp mã hóa. Đối với một lớp mã hóa cho trước, chỉ số Đóng Góp Ngữ Cảnh Chú Ý (ACC) đo lường đóng góp của word-vectors trong đầu ra của lớp. Chỉ số này được suy ra từ một Vector Điểm (SV) để tìm word-vectors quan trọng hơn trong mỗi lớp. Đối với một lớp cho trước, SV chỉ ra đóng góp của mỗi word-vector đầu vào đối với đầu ra của lớp chú ý.

Hình 2 minh họa việc tính toán Vector Điểm dựa trên ma trận xác suất chú ý. Hình 2 (trái) cho thấy ma trận xác suất chú ý của mô hình BERT-base. Các hàng của ma trận này được chuẩn hóa thành một. Bằng cách lấy trung bình ma trận xác suất chú ý trên các đầu, ma trận xác suất tổng hợp được tạo ra, Hình 2 (giữa). SV được tạo ra từ tổng của ma trận này dọc theo chiều không được chuẩn hóa như được hiển thị trong Hình 2 (phải). Các phần tử SV với giá trị điểm cao hơn tương ứng với các word-vectors đóng góp nhiều hơn cho đầu ra lớp.

Hình 3 minh họa kết quả SV BERT-base trên nhiệm vụ phân tích tình cảm IMDB [77] cho một mẫu đầu vào. Như được hiển thị trong các lớp mã hóa sớm hơn, các giá trị SV có phân phối xấp xỉ đồng nhất, và trong các lớp mã hóa cuối, phân phối của các giá trị SV thay đổi thành phân phối xấp xỉ δ. Phân phối của các giá trị SV trong các lớp cuối cho thấy rằng sự tập trung điểm cao trên một vài word-vectors và những cái khác có ít đóng góp hơn cho đầu ra lớp tự chú ý.

Các phần tử SV là dương và có trung bình một. Vì vậy, trung vị của SV bị giới hạn. Giả sử trung vị của SV thấp hơn trung bình của nó. Trong trường hợp đó, hơn một nửa các phần tử thấp hơn giá trị trung bình, và chỉ một vài cao hơn giá trị trung bình. Điều này cho thấy rằng hầu hết các word-vectors không mang đủ đóng góp cho đầu ra lớp, và một vài word-vectors ảnh hưởng đến kết quả cuối cùng của lớp. Trung vị của Vector Điểm được hiểu là chỉ số Đóng Góp Ngữ Cảnh Chú Ý được đề xuất.

Việc tính toán chỉ số ACC được giải thích trong Thuật toán 1. Đầu vào của thuật toán là word-vectors mã hóa BERT và các tham số mô hình. Đầu ra của thuật toán là vector chỉ số ACC cho thấy đóng góp của word-vectors trong mỗi lớp tự chú ý mã hóa. Đối với lớp mã hóa l, bước đầu tiên là chuẩn bị ma trận xác suất tự chú ý được tính toán trước; Ml_Att. Sau đó trung bình của ma trận này trên các đầu của nó được tính toán; Ml_H. Trong bước tiếp theo, tổng của ma trận kết quả trên chiều không được chuẩn hóa của nó được tính toán; Ml_S. Ma trận này là Vector Điểm. Đóng Góp Ngữ Cảnh Chú Ý của lớp (El_ACC) là trung vị của Vector Điểm.

Hình 4 cho thấy chỉ số ACC cho các lớp mã hóa BERT-base trên tập dữ liệu IMDB [77] và một số nhiệm vụ benchmark Đánh Giá Hiểu Biết Ngôn Ngữ Chung (GLUE) [78]. Đường cong được khớp với kết quả chỉ số ACC cho thấy rằng chỉ số ACC giảm dần ở các lớp sau. Điều này cho thấy rằng phần các word-vectors đóng góp nhiều hơn trong các lớp cuối ít hơn trong những lớp đó; do đó nhiều word-vectors có thể được loại bỏ. Bằng cách loại bỏ các word-vectors ít quan trọng hơn trong mỗi

--- TRANG 5 ---
5
BẢNG I
PHÂN TÍCH ĐỘ TRỄ VÀ THỜI GIAN ĐẾN TOKEN ĐẦU TIÊN (TTFT) CHO CÁC MÔ HÌNH BERT [3], GPT-2 [73], T5 [74], GEMMA 2 [75],
MISTRAL [72], VÀ LLAMA 3 [76]

[Table content translated to Vietnamese maintaining structure...]

...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
0123T-2T-1
0 1 2 3 T-2 T-1Đầu 0Đầu 1Đầu 11
Trung Bình Chuẩn Hóa của
 Các ĐầuMa Trận Xác Suất Chú Ý
0123T-2T-1
0 1 2 3 T-2 T-1Không Chuẩn Hóa
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
Tổng của Các Phần Tử
Không Chuẩn Hóa
...
0 1 2 3 T-2 T-1...
0 1 2 3 T-2 T-1Vector Điểm (SV)

Hình 2. Thu được Vector Điểm (SV) từ Ma trận Xác suất Chú ý trong một lớp mã hóa BERT [3] cho trước

Thuật toán 1 Chỉ số Đóng Góp Ngữ Cảnh Chú Ý (ACC)
Đầu vào: Đầu vào mã hóa ∈R^(L×T×H) & BERT đã tinh chỉnh
{T= số lượng word-vectors đầu vào, L= số lượng
lớp mã hóa, H= kích thước trạng thái ẩn }
Đầu ra: Vector ACC: E_ACC∈R^L
Khởi tạo :
1:E_ACC = [∅]
QUY TRÌNH LẶP
2:for mã hóa l in Mã hóa do
3:M^l_Att←Ma trận Xác suất Tự Chú ý ∈R^(H×T×T)
4:M^l_H←Trung bình của M^l_Att trên các đầu ∈R^(T×T)
5:M^l_S←Tổng của M^l_H trên chiều không chuẩn hóa ∈
R^T
6:E^l_ACC←Trung vị của M^l_S(Vector Điểm) ∈R
7: Thêm E^l_ACC vào E_ACC
8:end for
9:return E_ACC

lớp, các word-vectors được xử lý giảm dần, và thời gian suy luận của mô hình giảm.

Như được hiển thị trong Hình 4, hành vi của chỉ số ACC có mối tương quan mạnh mẽ với các đặc điểm phức tạp vốn có trong mỗi nhiệm vụ. Kết quả ACC luôn thể hiện sự giảm đơn điệu với số lớp, ngoại trừ trường hợp tập dữ liệu SST-2 [79], nơi quan sát thấy hành vi bất ngờ. Đáng chú ý, tập dữ liệu SST-2, tập trung vào phân tích tình cảm của các đánh giá phim, có sự tương đồng mạnh mẽ với tập dữ liệu IMDB. Một phân tích so sánh các đường cong ACC cho SST-2 và IMDB cho thấy sự khác biệt đáng chú ý, chủ yếu liên quan đến độ dài của chuỗi đầu vào. Cụ thể, trong tập dữ liệu IMDB, phần lớn các mẫu bao gồm khoảng 512 token; ngược lại, trong tập dữ liệu SST-2, phần lớn các mẫu có ít hơn 50 token. Có thể suy ra rằng số lượng token đầu vào hạn chế dẫn đến sự nhạy cảm không mong muốn đối với word-vectors, và trong những trường hợp như vậy, một số word-vectors được phục hồi thông qua các nhánh dư.

D. Kiến Trúc Được Đề Xuất
Theo thảo luận trong Phần III-B, các lớp chú ý và feed-forward là những đóng góp chính của tổng độ trễ. Ngoài ra, theo quan sát trong Phần III-C, đóng góp word-vector trong mã hóa giảm ở các lớp sau. Dựa trên những quan sát này, Phần này đề xuất chi tiết của kiến trúc được đề xuất.

Trong kiến trúc được đề xuất, lớp tự chú ý BERT gốc được sửa đổi với lớp tự chú ý được đề xuất. Hình 5 minh họa lớp tự chú ý đa đầu được đề xuất. Kiến trúc được đề xuất thêm các lớp Sắp xếp và Loại bỏ vào giữa lớp tự chú ý đa đầu BERT gốc. Lớp Sắp xếp sắp xếp ngữ cảnh đầu vào theo Vector Điểm lớp được tính toán. Lớp Loại bỏ loại bỏ một phần của ngữ cảnh đã sắp xếp với điểm thấp hơn theo Tỷ lệ Loại bỏ (α_ER).

α_ER là một vector siêu tham số đặt phần word-vectors bị loại bỏ trong mỗi lớp mã hóa. Đối với lớp l, α^l_ER là Tỷ lệ Loại bỏ word-vector thu được từ Hồ sơ Loại bỏ được tính toán trước (α_EP) và một siêu tham số Hệ số Tăng tốc được chỉ định trước (α_SC). α_SC điều chỉnh sự đánh đổi giữa độ chính xác và tốc độ trong các giai đoạn tinh chỉnh và điều chỉnh ngoại tuyến.

α^l_ER=α^l_EP×α_SC (1)

α^l_EP được thu được từ đường cong được khớp với chỉ số ACC (các đường màu đỏ trong Hình 4) tại lớp l. Trong một số nhiệm vụ, các giá trị ACC của các lớp không mượt mà. Để tăng tính ổn định và ngăn áp dụng căng thẳng lên mô hình trong giai đoạn tinh chỉnh, trong công thức được đề xuất, thay vì các giá trị chỉ số ACC (E_ACC), đường cong được khớp với chỉ số ACC (P_ACC) được sử dụng để trình bày xu hướng loại bỏ mượt mà hơn. Điều quan trọng cần lưu ý là khi hành vi tăng của giá trị ACC xảy ra trong một lớp, quá trình loại bỏ được dừng lại, và các lớp còn lại tiếp tục sử dụng số lượng word-vectors được giữ lại cố định. Trong (2), giới hạn dưới α_EP được đặt thành một để đảm bảo hình dạng giảm đơn điệu của đường cong siêu tham số này. Phương pháp thích ứng này đảm bảo việc bảo tồn thông tin quan trọng và giảm thiểu tác động của hành vi ACC bất ngờ đối với các lớp tiếp theo.

α^l_EP= min (1 ,P^l_ACC/P^(l-1)_ACC) (2)

Số lượng word-vectors còn lại trong mỗi lớp mã hóa được tính toán theo số lượng word-vectors đầu vào lớp và siêu tham số α_ER. Phương trình (3) trình bày số lượng word-vectors còn lại (T^l) tại đầu ra của lớp mã hóa l. Số lượng tối thiểu word-vectors còn lại tại mỗi đầu ra mã hóa được giới hạn thành một.

T^l= max (1 ,⌊α^l_ER×T^(l-1)⌋) (3)

Số lượng hiệu quả word-vectors được xử lý có thể được ước tính bằng cách sử dụng số lượng word-vectors trước và sau lớp loại bỏ. Dựa trên Bảng I, trong BERT-base, khoảng 65% độ trễ xảy ra sau lớp tự chú ý. Phương trình (4) trình bày số lượng hiệu quả word-vectors Được Xử lý (PW^l) trong lớp l.

PW^l≈7T^(l-1)+ 13T^l/20 (4)

Do đó,
PW≈T(7/20+∑(i=1 to L-1)∏(j=1 to i)α^j_EP+13/20∏(j=1 to L)α^j_EP) (5)

Word-vectors được xử lý trong BERT-base gốc là:
PW_BERT-base=T×L (6)

Trong các phương trình này, L là số lượng lớp mã hóa BERT, và T là số lượng word-vectors đầu vào mã hóa. Vì hầu như tất cả độ trễ suy luận xảy ra trong mã hóa, nỗ lực tính toán tổng thể của mô hình gần như bằng với nỗ lực của giai đoạn mã hóa. Phương trình (7) trình bày tăng tốc thời gian suy luận phương pháp được đề xuất so với BERT-base.

K_speedup ≈20L/(7 + 20∑(i=1 to L-1)∏(j=1 to i)α^j_EP+ 13∏(j=1 to L)α^j_EP) (7)

Phương trình trên giả định rằng số lượng word-vectors còn lại trong mỗi lớp mã hóa nhiều hơn một word-vector duy nhất. Nói cách khác, (3) trả về đối số thứ hai, điều này hợp lệ cho hầu hết các trường hợp. Ví dụ, nếu L là 12 và α^l_EP là hằng số và bằng 0.8 cho tất cả các lớp, tốc độ suy luận cuối cùng nhanh hơn khoảng 2.99 lần so với BERT-base.

Các lớp Sắp xếp và Loại bỏ được đề xuất được bao gồm trong giai đoạn tinh chỉnh. Cho rằng các word-vectors bị loại bỏ tạo thành một phần tương đối nhỏ trong mỗi lớp trong quá trình tinh chỉnh, chúng ta có thể giả định rằng các gradient truyền hiệu quả qua các word vectors còn lại. Ngoài ra, các đường dẫn dư vẫn hoạt động, đóng góp vào các cập nhật trong các lớp ban đầu của mạng. Thiết kế này đảm bảo luồng thông tin mạnh mẽ trong toàn bộ quá trình tinh chỉnh.

--- TRANG 6 ---
6
2 4 6 8 10 12
Số Lớp Mã Hóa0.00.20.40.60.8Giá Trị ACCĐường Cong Khớp
SST-2
[Similar structure repeated for other datasets: MRPC, QQP, QNLI, RTE, MNLI, STS-B, IMDB]

Hình 4. Kết quả chỉ số Đóng Góp Ngữ Cảnh Chú Ý (ACC) Mã Hóa BERT-base [3] (các thanh màu xanh) và đường cong khớp bậc hai (các đường màu đỏ) trên các nhiệm vụ SST-2 [79], MRPC [80], QQP [81], QNLI [82], RTE [83], MNLI [84], STS-B [85] và IMDB [77].

[Detailed technical diagram showing Input Word Vectors, multiple processing layers, and output - maintaining exact structure from original]

Hình 5. Lớp tự chú ý được đề xuất

Để tăng cường tốc độ của lớp tự chú ý được đề xuất, các lớp Sắp xếp và Loại bỏ có thể được đặt sau lớp nối tự chú ý. Điều chỉnh này dẫn đến việc giảm tính toán trên mỗi đầu, và đối với một lớp cho trước, các word-vectors bị loại bỏ đóng vai trò trong việc hình thành đầu ra của lớp. Hơn nữa, phù hợp với chiến lược loại bỏ word-vector được đề xuất, các word-vectors được cắt tỉa tăng tốc hiệu quả giai đoạn suy luận, dẫn đến sự phù hợp gần dự kiến giữa việc giảm Phép Toán Dấu Phẩy Động (FLOPs) và tăng tốc suy luận.

E. Mở Rộng Kiến Trúc Được Đề Xuất sang Các Mô Hình Tự Hồi Quy
Giải pháp được đề xuất cũng có thể được áp dụng cho các kiến trúc decoder, bao gồm các Mô hình Ngôn ngữ Lớn (LLMs) GPT, Gemma, Mistral và Llama. Phần này nêu rõ các cân nhắc chính để triển khai phương pháp trên mô hình GPT-2, như một ví dụ đại diện của các mô hình tự hồi quy.

Mô hình GPT-2 nền tảng bao gồm 12 lớp decoder, với mỗi decoder có một lớp chú ý nhân quả được trang bị 12 đầu. Tổng số tham số trong mô hình GPT-2 là khoảng 124 triệu. Một sự khác biệt chính giữa các kiến trúc của GPT-2 và mô hình BERT-base nằm trong thiết kế các lớp chú ý của chúng. Như được giải thích trong Phần III-A, các lớp chú ý của các mã hóa BERT-base là hai chiều, bao gồm tất cả word-vectors đầu vào trong việc tạo ra mỗi word-vector đầu ra. Ngược lại, mô hình GPT-2, được thiết kế riêng cho các nhiệm vụ tạo văn bản, sử dụng các lớp chú ý nhân quả, nơi chỉ các word-vectors trước đó đóng góp vào đầu ra lớp chú ý tại mỗi dấu thời gian.

Trong một lớp chú ý nhân quả, ma trận xác suất chú ý, được biểu diễn như M^l_Att, áp dụng dạng của một ma trận tam giác. Để mở rộng kiến trúc được đề xuất được nêu trong Phần III-D sang các lớp chú ý nhân quả, một bước chuẩn hóa bổ sung cần được tích hợp sau bước thứ năm trong Thuật toán 1. Bước bổ sung này chuẩn hóa Vector Điểm M^l_S dựa trên số lượng phần tử khác không trong ma trận xác suất M^l_Att. Phương trình (8) diễn đạt quá trình chuẩn hóa này trong lớp l, trong đó phần tử thứ i của Vector Điểm được điều chỉnh tương đối với số lượng phần tử khác không trong cột thứ i của ma trận xác suất. Thông qua bước chuẩn hóa này, các giá trị điểm kết quả trở nên có thể so sánh được.

M^l'_Si=M^l_Si÷N^l_MAtt i (8)

IV. CÁC THÍ NGHIỆM
Trong phần này, chúng tôi triển khai và đánh giá phương pháp được đề xuất trên BERT-base, GPT-2, Flan-T5 [86], Gemma2 [75], Mistral và Llama3 [76], khám phá hiệu quả của nó trên các nhiệm vụ hiểu biết ngôn ngữ tự nhiên khác nhau, tập dữ liệu tạo văn bản và các benchmark điều chỉnh hướng dẫn. Hướng tới cuối phần này, chúng tôi kiểm tra thuộc tính điều chỉnh ngoại tuyến và tiến hành nghiên cứu ablation trên lớp chú ý được đề xuất.

A. Tập Dữ Liệu
Hiệu suất của phương pháp được đề xuất trên các nhiệm vụ hiểu biết ngôn ngữ tự nhiên được đánh giá sử dụng tập dữ liệu IMDB [77], tập trung vào phân tích tình cảm, cũng như benchmark Đánh Giá Hiểu Biết Ngôn Ngữ Chung (GLUE) [78]. Benchmark GLUE bao gồm hai nhiệm vụ phân loại câu đơn: SST-2 [79] và CoLA [87]; hai nhiệm vụ phân loại tương tự đa câu: MRPC [80] và QQP [81]; một nhiệm vụ trả lời câu hỏi nhị phân: QNLI [82]; ba nhiệm vụ suy luận ngôn ngữ tự nhiên: RTE [83], MNLI-M và MNLI-MM [84]; và một nhiệm vụ hồi quy tương tự đa câu: STS-B [85].

Để đánh giá hiệu suất của phương pháp trên các nhiệm vụ tạo văn bản, chúng tôi tiến hành đánh giá trên một số tập dữ liệu. Tập dữ liệu mô hình hóa ngôn ngữ WikiText-103 [88], bao gồm hơn 100 triệu token được trích xuất từ các bài báo Tốt và Nổi bật đã được xác minh trên Wikipedia, phục vụ như một nền tảng đánh giá chính. Ngoài ra, tập dữ liệu PTB [89], bao gồm một triệu từ từ tài liệu Wall Street Journal năm 1989, và tập dữ liệu 1BW [90], được thiết lập như một corpus benchmark để đo lường tiến bộ trong mô hình hóa ngôn ngữ thống kê, được bao gồm trong đánh giá của chúng tôi. Hơn nữa, chúng tôi đánh giá phương pháp trên tập dữ liệu LAMBADA [91], một tập hợp các đoạn văn tường thuật được đặc trưng bởi tính năng thú vị rằng các đối tượng con người có thể dự đoán từ cuối cùng của chúng khi tiếp xúc với toàn bộ đoạn văn. Được trích xuất từ BookCorpus, tập dữ liệu LAMBADA bao gồm 10.022 đoạn văn, đóng góp vào một đánh giá toàn diện về hiệu suất của phương pháp được đề xuất trong các mô hình decoder.

Để tiếp tục chứng minh hiệu suất của phương pháp được đề xuất trong hiểu biết ngôn ngữ, nó được đánh giá trên các nhiệm vụ điều chỉnh hướng dẫn và được đánh giá sử dụng benchmark Hiểu Biết Ngôn Ngữ Đa Nhiệm Vụ Lớn (MMLU). Benchmark này bao gồm các câu hỏi thi từ 57 nhiệm vụ đa dạng, bao gồm các chủ đề như toán học, lịch sử, luật và y học [92].

B. Cài Đặt Mô Hình
Trong đánh giá phương pháp trên BERT-base, lớp tự chú ý mã hóa trải qua việc thay thế bằng lớp tự chú ý mã hóa được đề xuất. Các siêu tham số giống như trong BERT-base với các siêu tham số α_EP và α_ER bổ sung được thêm để đặt tỷ lệ loại bỏ word-vector trong mỗi lớp mã hóa. Các siêu tham số là: tỷ lệ học - [1.5×10^-5,4.5×10^-5], kích thước batch - {4, 8, 16, 32, 64, 128}, số epoch - {2, 3, 4, 5 }, Dropout - [0.1, 0.15], α_ER- [0.77, 0.97], và vector α_EP được thu được từ (2). Hàm mất mát cross-entropy được sử dụng cho các nhiệm vụ phân loại và hàm mất mát Lỗi Bình phương Trung bình (MSE) cho các nhiệm vụ hồi quy. Bộ lập lịch tỷ lệ học tuyến tính được sử dụng cho tất cả các nhiệm vụ. Khởi động ấm học tuyến tính ban đầu được sử dụng cho một số nhiệm vụ. Trong tất cả các nhiệm vụ, trình tối ưu hóa AdamW với ϵ= 1×10^-8, β1 = 0.9 và β2 = 0.999 được sử dụng. Độ dài chuỗi đầu vào mô hình được cố định cẩn thận để ít hơn 1% mẫu bị cắt ngắn. Độ dài câu đầu vào được chọn cho mỗi tập dữ liệu được chỉ định trong Bảng II.

Trong trường hợp mô hình GPT-2, lớp chú ý trong mỗi lớp decoder trải qua sửa đổi với lớp chú ý được đề xuất, được mô tả trong Phần III-E. Giới thiệu các siêu tham số α_EP và α_ER trở nên cần thiết để xác định tỷ lệ loại bỏ word-vector trong mỗi lớp mã hóa. Các siêu tham số được chỉ định cho đánh giá như sau: tỷ lệ học 1×10^-6, kích thước batch 8, thời gian huấn luyện ba epoch, tỷ lệ dropout 0.1, giá trị α_ER dao động từ 0.9 đến 1.2, và vector α_EP được suy ra từ phương trình (2). Các tham số này cùng nhau xác định thiết lập thí nghiệm để đánh giá toàn diện tác động của phương pháp đối với mô hình GPT-2.

Để đánh giá điều chỉnh hướng dẫn, các mô hình Flan-T5¹, Gemma2, Mistral và Llama3 được nhắm mục tiêu. Trong các mô hình này, các lớp tự chú ý được thay thế bằng lớp tự chú ý được đề xuất, và các lớp khác được điều chỉnh để phù hợp với số lượng word vector động trong mỗi lớp mã hóa và decoder. Các mô hình sử dụng trọng số mặc định của chúng mà không có bất kỳ điều chỉnh hoặc tinh chỉnh nào. Tham số α_ER được chọn trong phạm vi [0.8, 1.0], và tất cả các siêu tham số đánh giá khác vẫn nhất quán với những mô hình mặc định.

C. Chi Tiết Triển Khai
Các mô hình ban đầu được sử dụng cho nghiên cứu này là các phiên bản được huấn luyện trước của BERT-base, GPT-2, Flan-T5, Gemma2, Mistral,
¹Flan-T5 là một phiên bản nâng cao của T5 [74]

--- TRANG 7 ---
7
và Llama3 từ các triển khai Hugging Face². Việc triển khai được thực hiện trong thư viện PyTorch³, sử dụng GPU Nvidia Tesla P100 cho BERT-base, GPU Nvidia Tesla T4 cho GPT-2, và GPU Nvidia Tesla A100 cho Flan-T5, Gemma2, Mistral và Llama. Các tập dữ liệu được lấy từ thư viện Tập dữ liệu Hugging Face, và kết quả GLUE được xác thực thông qua dịch vụ GLUE chính thức⁴. Đánh giá điều chỉnh hướng dẫn được thực hiện với sự hỗ trợ của phương pháp INSTRUCTEVAL [93]. Khung này đảm bảo tính nhất quán và độ tin cậy trong đánh giá hiệu suất của giải pháp được đề xuất trên các mô hình được chỉ định.

D. Kết Quả Thí Nghiệm trên Các Nhiệm Vụ Hiểu Biết Ngôn Ngữ
Kết quả thí nghiệm của phương pháp được đề xuất áp dụng cho BERT-base cho một loạt rộng các nhiệm vụ được trình bày trong Bảng II. Phương pháp được đề xuất nhanh hơn 2.9 lần so với BERT-base, chỉ với 0.7% sự giảm độ chính xác trên tập kiểm tra. Ngoài ra, thời gian suy luận của mô hình nhanh hơn đến 4.8 lần so với BERT-base, chỉ với sự giảm 0.9% về độ chính xác. Các kết quả này chứng minh rằng phương pháp được đề xuất tăng cường đáng kể hiệu suất và hiệu quả tài nguyên của các mô hình dựa trên Transformer trong khi duy trì mất mát độ chính xác tối thiểu.

Bảng cho thấy tăng tốc thí nghiệm và tăng tốc dự kiến. Tăng tốc suy luận nhất quán với tăng tốc dự kiến được dự đoán bằng cách sử dụng (7). Những khác biệt nhỏ là do chi phí tính toán bổ sung, bao gồm nỗ lực tính toán của các lớp tokenizer và phân loại đầu ra không được xem xét trong (7). Sự nhất quán giữa tăng tốc thí nghiệm và tăng tốc dự kiến chấp thuận giả thuyết được thảo luận trong Phần III-D.

Bảng III so sánh phương pháp được đề xuất và các phương pháp tăng tốc được báo cáo trước đây. Mô hình BERT-base (L=12, h=12, d=768) được sử dụng làm cơ sở so sánh. Kết quả kiểm tra, sự giảm độ chính xác trung bình so với BERT-base, số lượng FLOPs trung bình và tăng tốc suy luận trung bình được báo cáo cho mỗi mô hình. Như được hiển thị trong bảng, phương pháp được đề xuất đạt được tăng tốc 2.9 lần với 0.73% sự giảm độ chính xác trung bình so với mô hình BERT-base. So với các công trình khác, mô hình DistilBERT đạt được tăng tốc 1.7 lần với khoảng 2.74% suy giảm độ chính xác trung bình. Trong DistilBERT, một mô hình biểu diễn ngôn ngữ có mục đích chung được huấn luyện trước phải được huấn luyện từ đầu với nỗ lực đáng kể. Tăng tốc mô hình TinyBERT là 2.0 lần với 0.4% sự giảm độ chính xác trung bình. TinyBERT sử dụng khung học hai giai đoạn thực hiện chưng cất Transformer trong các giai đoạn huấn luyện trước và tinh chỉnh với nỗ lực tính toán bổ sung đáng kể. Tăng tốc mô hình MobileBERT là 5.5 lần với khoảng 1.19% sự giảm độ chính xác, nhưng điểm khởi đầu của mô hình này là BERT large. Tăng tốc mô hình PoWER-BERT là 2.9 lần với khoảng 0.92% sự giảm độ chính xác. Tuy nhiên, huấn luyện PoWER-BERT bao gồm 3 giai đoạn: "tinh chỉnh, tìm kiếm cấu hình loại bỏ và huấn luyện lại" [28] áp đặt nỗ lực huấn luyện bổ sung. Tăng tốc LayerDrop và FPT lần lượt là 1.7 và 1.5 lần, với sự giảm độ chính xác trung bình tương ứng là 1.47% và 0.93%. Bảng cho thấy rằng kết quả của kỹ thuật được đề xuất có tính cạnh tranh và hiệu quả hơn các phương pháp cơ sở trước đây trên một loạt rộng các nhiệm vụ thí nghiệm. Hơn nữa, so với các phương pháp được báo cáo gần đây, phương pháp của chúng tôi chỉ được áp dụng trong giai đoạn tinh chỉnh và cung cấp thuộc tính điều chỉnh ngoại tuyến. Các trường trống trong bảng cho thấy những trường hợp mà một số công trình không báo cáo đánh giá của họ trên các tập dữ liệu nhất định.

Để đánh giá tác động của phương pháp được đề xuất đối với mô hình GPT-2, quá trình huấn luyện bao gồm việc sử dụng 1% tập dữ liệu OpenWebText [94] với stride 128 trong ba epoch. Bảng IV thể hiện kết quả độ chính xác phân loại của phương pháp được đề xuất so với GPT-2 và DistilGPT2. Đáng chú ý, phương pháp được đề xuất đạt được độ trễ suy luận nhanh gấp ba lần so với mô hình cơ sở, kèm theo sự giảm độ chính xác trung bình nhỏ là 0.55%. Các trường trống trong các bảng cho thấy những trường hợp mà một số công trình không báo cáo đánh giá của họ trên các tập dữ liệu nhất định.

E. Kết Quả Thí Nghiệm trên Các Nhiệm Vụ Tạo Văn Bản

²https://huggingface.co/
³https://pytorch.org/
⁴https://gluebenchmark.com/

--- TRANG 8 ---
8
BẢNG II
KẾT QUẢ THÍ NGHIỆM CỦA PHƯƠNG PHÁP ĐỀ XUẤT TRÊN BENCHMARK GLUE [78] VÀ TẬP DỮ LIỆU IMDB [77]
VỚI BERT-BASE LÀM MÔ HÌNH MỤC TIÊU

[Table structure with Vietnamese translations maintained]

BẢNG III
SO SÁNH KẾT QUẢ GIỮA BERT BASE [3], DISTIL BERT [49], TINYBERT [50], MOBILE BERT [51], POWER-BERT [28],
LAYER DROP [42], FPT [37], VÀ LATENCY-ADJUSTABLE BERT BASE (CÔNG TRÌNH NÀY)

[Table structure with Vietnamese translations maintained]

BẢNG IV
SO SÁNH ĐỘ CHÍNH XÁC GIỮA GPT-2 [73] VÀ DISTIL GPT2 [49]
VỚI LATENCY ADJUSTABLE GPT-2 (CÔNG TRÌNH NÀY)

[Table structure with Vietnamese translations maintained]

Bảng V trình bày kết quả đánh giá perplexity zero-shot cho phương pháp được đề xuất trong các nhiệm vụ tạo văn bản, cung cấp cơ sở so sánh với các nghiên cứu đương đại khác. Các phát hiện chứng minh rằng phương pháp duy trì hiệu quả ngữ cảnh toàn cục của chuỗi đầu vào. Chỉ số Đóng Góp Ngữ Cảnh Chú Ý (ACC) được đề xuất hoạt động tốt, đảm bảo rằng việc loại bỏ word-vectors không làm tổn hại đến ngữ cảnh toàn cục của đầu vào. Đáng chú ý rằng mô hình GPT-2 gốc, được huấn luyện bởi OpenAI, được hưởng lợi từ tài nguyên phần cứng và tập dữ liệu rộng lớn hơn. Tuy nhiên, kiến trúc được đề xuất của chúng tôi cho phép thích ứng hiệu quả chỉ với vài epoch nhỏ. Hơn nữa, kỹ thuật được đề xuất thể hiện hiệu suất vượt trội khi so sánh với các mô hình DistilGPT2 [49] và ZipGPT2 [95] trong đánh giá.

F. Kết Quả Thí Nghiệm trên Các Nhiệm Vụ Điều Chỉnh Hướng Dẫn
Để đánh giá điều chỉnh hướng dẫn, các mô hình Flan-T5, Gemma2, Mistral và Llama3 được chọn. Trong các mô hình này, các lớp tự chú ý được thay thế bằng lớp tự chú ý mã hóa được đề xuất, và trọng số mặc định được sử dụng mà không có bất kỳ điều chỉnh hoặc tinh chỉnh bổ sung nào. Hiệu suất của phương pháp được đánh giá sử dụng các loại prompting khác nhau trên benchmark MMLU. Bảng VI trình bày kết quả thí nghiệm cho 5-shot direct prompting trên benchmark MMLU. Như được hiển thị, phương pháp được đề xuất đạt được việc giảm lên đến 3 lần trong TTFT với chỉ sự giảm điểm số nhỏ.

[Content continues with remaining sections...]

--- TRANG 9 ---
9
BẢNG V
SO SÁNH PERPLEXITY (PPL) GIỮA GPT-2 [73], DISTIL GPT2 [49], ZIPGPT2 [95], VÀ LATENCY-ADJUSTABLE GPT-2 (CÔNG TRÌNH NÀY)

[Table content translated maintaining structure]

G. Thuộc Tính Điều Chỉnh Ngoại Tuyến
Sau giai đoạn tinh chỉnh, sự đánh đổi giữa độ chính xác suy luận và tốc độ của mô hình có thể được điều chỉnh với thuộc tính điều chỉnh ngoại tuyến. Thuộc tính này kiểm soát tỷ lệ loại bỏ word-vector của mỗi lớp mã hóa bằng cách điều chỉnh giá trị siêu tham số Hệ số Tăng tốc (α_SC). Hình 6 trình bày kết quả thuộc tính điều chỉnh ngoại tuyến trên BERT-base. Trong các thí nghiệm sau, mô hình đầu tiên được tinh chỉnh với α_SC bằng một cho một nhiệm vụ cho trước, và sau đó trong giai đoạn điều chỉnh ngoại tuyến, giá trị α_SC thay đổi từ 0.85 đến 1.2 để quan sát tác động của nó đối với tăng tốc suy luận và độ chính xác.

[Content continues with detailed analysis of figures and experimental results...]

BẢNG VI
KẾT QUẢ TĂNG TỐC THỜI GIAN ĐẾN TOKEN ĐẦU TIÊN (TTFT) CỦA PHƯƠNG PHÁP ĐỀ XUẤT TRÊN FLAN-T5 [86], GEMMA 2 [75], MISTRAL [72], VÀ LLAMA 3 [76] ĐƯỢC ĐÁNH GIÁ VỚI TẬP DỮ LIỆU ĐIỀU CHỈNH HƯỚNG DẪN MMLU [92].

[Table content with Vietnamese translations]

[Content continues with experimental results and analysis...]

--- TRANG 10 ---
10
1.0 1.5 2.0 2.5 3.0 3.5 4.0
Tăng Tốc Suy Luận So Với GPT-2020406080100Perplexity Ngoại Tuyến Của Phương Pháp Đề Xuất (PPL)WikiText-103
LAMBADA
PTB
1BW

Hình 7. Perplexity Ngoại Tuyến (PPL) của phương pháp được đề xuất trên các tập dữ liệu WikiText-103 [88], LAMBADA [91], PTB [89] và 1BW [90].

[Content continues with analysis of stability and performance...]

H. Nghiên Cứu Ablation

[Table VII content with Vietnamese translations and analysis]

BẢNG VII
NGHIÊN CỨU ABLATION TRÊN LỚP TỰ CHÚ Ý ĐỀ XUẤT VỚI CÁC CHIẾN LƯỢC TRIỂN KHAI LỚP SẮP XẾP TỰ CHÚ Ý KHÁC NHAU TRÊN BERT-BASE [3].

[Table content with Vietnamese translations]

[Content continues with detailed analysis of ablation study results...]

V. KẾT LUẬN
Bài báo này trình bày một kiến trúc Transformer có thể điều chỉnh độ trễ áp dụng được cho một loạt rộng các mô hình, với sự giảm độ chính xác nhỏ. Chúng tôi giới thiệu một chỉ số Đóng Góp Ngữ Cảnh Chú Ý (ACC) mới để đánh giá đóng góp ngữ cảnh của mỗi lớp mã hóa Transformer, chứng minh thí nghiệm rằng các đóng góp word-vector giảm trong các lớp mã hóa cuối. Các thí nghiệm mở rộng trên các nhiệm vụ hiểu biết ngôn ngữ, tạo văn bản và điều chỉnh hướng dẫn xác thực hiệu quả của kỹ thuật được đề xuất trên các tập dữ liệu đa dạng, với tác động tối thiểu đến ngữ cảnh toàn cục của đầu vào. Bài báo cũng cung cấp một ước tính phân tích chính xác về tăng tốc, hỗ trợ các nhà thiết kế hệ thống với việc lựa chọn siêu tham số. Ngoài ra, khả năng điều chỉnh ngoại tuyến mạnh mẽ của phương pháp cho phép điều chỉnh tăng tốc mô hình sau giai đoạn tinh chỉnh. Phương pháp được đề xuất làm nổi bật rằng, trong khi mạng hoàn chỉnh cần thiết cho huấn luyện, nó có thể được cắt ngắn trong giai đoạn tinh chỉnh trong các Mô hình Ngôn ngữ Lớn (LLMs).

LỜI CÁM ƠN
Cảm ơn Ông Seyed Mahdi Hosseini vì sự hỗ trợ và nhận xét của ông đã cải thiện đáng kể công trình của chúng tôi.

TÀI LIỆU THAM KHẢO
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[Continues with all references translated to Vietnamese, maintaining the same academic format but with Vietnamese translations of titles and publication names where appropriate...]

--- TRANG 11 ---
11
[Continues with more translated references...]

--- TRANG 12 ---
12
[Continues with more translated references...]

--- TRANG 13 ---
13
[Continues with remaining translated references...]

--- TRANG 14 ---
14
[Final page with remaining references and author biographies translated to Vietnamese]

Sajjad Kachuee nhận bằng Cử nhân và Thạc sĩ về kỹ thuật điện từ Đại học Công nghệ Sharif, Tehran, Iran, lần lượt vào năm 2017 và 2019, nơi ông hiện đang theo đuổi bằng Tiến sĩ. Nghiên cứu của ông liên quan đến trí tuệ nhân tạo và học máy, tập trung vào tăng tốc và tối ưu hóa mạng neural xử lý ngôn ngữ tự nhiên (NLP).

Mohammad Sharifkhani nhận bằng Cử nhân và Thạc sĩ Khoa học Ứng dụng về kỹ thuật điện và máy tính từ Đại học Tehran, Tehran, Iran, lần lượt vào năm 1998 và 2000. Ông nhận bằng Tiến sĩ từ Đại học Waterloo, Waterloo, ON, Canada, vào năm 2006. Ông là Nghiên cứu viên Sau tiến sĩ tại Đại học Waterloo vào năm 2007. Ông hiện là Phó Giáo sư tại Khoa Kỹ thuật Điện, Đại học Công nghệ Sharif, Tehran, Iran. Từ năm 2008, ông đã xuất bản một số bài báo khoa học về lĩnh vực rộng lớn VLSI và Hệ thống Số. Ông phục vụ như thành viên ủy ban kỹ thuật và người đánh giá của một số hội nghị và tạp chí IEEE, tương ứng. Ông thành lập một số công ty khởi nghiệp về lĩnh vực rộng lớn xử lý video và hình ảnh cũng như hệ thống máy học thông minh. Nghiên cứu hiện tại của ông là về mạch và kiến trúc công suất thấp, bộ chuyển đổi dữ liệu, và bộ xử lý chuyên dụng cho ứng dụng video và học máy.
