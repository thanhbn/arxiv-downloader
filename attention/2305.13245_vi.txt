# GQA: Huấn luyện các mô hình Transformer đa truy vấn tổng quát từ các checkpoint đa đầu

Joshua Ainslie∗, James Lee-Thorp∗, Michiel de Jong∗ † †
Yury Zemlyanskiy ,Federico Lebrón ,Sumit Sanghai
Google Research

## Tóm tắt
Cơ chế chú ý đa truy vấn ( MQA ), chỉ sử dụng một đầu key-value duy nhất, tăng tốc đáng kể quá trình suy luận của bộ giải mã. Tuy nhiên, MQA có thể dẫn đến suy giảm chất lượng, và hơn nữa việc huấn luyện một mô hình riêng biệt chỉ để suy luận nhanh hơn có thể không được mong muốn. Chúng tôi (1) đề xuất một công thức để nâng cấp huấn luyện các checkpoint mô hình ngôn ngữ đa đầu hiện có thành các mô hình với MQA sử dụng 5% lượng tính toán tiền huấn luyện ban đầu, và (2) giới thiệu cơ chế chú ý truy vấn nhóm ( GQA ), một sự tổng quát hóa của cơ chế chú ý đa truy vấn sử dụng số lượng đầu key-value trung gian (nhiều hơn một, ít hơn số đầu truy vấn). Chúng tôi cho thấy rằng GQA được nâng cấp huấn luyện đạt được chất lượng gần với cơ chế chú ý đa đầu với tốc độ tương đương với MQA.

## 1 Giới thiệu
Suy luận bộ giải mã tự hồi quy là một nút thắt nghiêm trọng đối với các mô hình Transformer do chi phí băng thông bộ nhớ từ việc tải trọng số bộ giải mã và tất cả các key và value của cơ chế chú ý ở mỗi bước giải mã (Shazeer, 2019; Pope et al., 2022; de Jong et al., 2022). Băng thông bộ nhớ từ việc tải key và value có thể được giảm mạnh thông qua cơ chế chú ý đa truy vấn (Shazeer, 2019), sử dụng nhiều đầu truy vấn nhưng chỉ một đầu key và value.

Tuy nhiên, cơ chế chú ý đa truy vấn ( MQA ) có thể dẫn đến suy giảm chất lượng và bất ổn định huấn luyện, và việc huấn luyện các mô hình riêng biệt được tối ưu hóa cho chất lượng và suy luận có thể không khả thi. Hơn nữa, trong khi một số mô hình ngôn ngữ đã sử dụng cơ chế chú ý đa truy vấn, chẳng hạn như PaLM (Chowdhery et al., 2022), nhiều mô hình khác thì không, bao gồm các mô hình ngôn ngữ có sẵn công khai như T5 (Raffel et al., 2020) và LLaMA (Touvron et al., 2023).

Công trình này có hai đóng góp cho việc suy luận nhanh hơn với các mô hình ngôn ngữ lớn. Đầu tiên, chúng tôi

∗Đóng góp bằng nhau.
†Đại học Nam California. Công việc được thực hiện tại Google Research.

cho thấy rằng các checkpoint mô hình ngôn ngữ với cơ chế chú ý đa đầu ( MHA ) có thể được nâng cấp huấn luyện (Komatsuzaki et al., 2022) để sử dụng MQA với một phần nhỏ lượng tính toán huấn luyện ban đầu. Điều này trình bày một phương pháp hiệu quả về chi phí để có được các checkpoint đa truy vấn nhanh cũng như các checkpoint MHA chất lượng cao.

Thứ hai, chúng tôi đề xuất cơ chế chú ý truy vấn nhóm (GQA ), một sự nội suy giữa cơ chế chú ý đa đầu và đa truy vấn với các đầu key và value đơn lẻ cho mỗi nhóm con của các đầu truy vấn . Chúng tôi cho thấy rằng GQA được nâng cấp huấn luyện đạt được chất lượng gần với cơ chế chú ý đa đầu trong khi gần như nhanh bằng cơ chế chú ý đa truy vấn.

## 2 Phương pháp

### 2.1 Nâng cấp huấn luyện
Tạo ra một mô hình đa truy vấn từ một mô hình đa đầu diễn ra trong hai bước: đầu tiên, chuyển đổi checkpoint, và thứ hai, tiền huấn luyện bổ sung để cho phép mô hình thích nghi với cấu trúc mới của nó. Hình 1 cho thấy quy trình chuyển đổi checkpoint đa đầu thành checkpoint đa truy vấn. Các ma trận chiếu cho các đầu key và value được tính trung bình thành các ma trận chiếu đơn lẻ, điều mà chúng tôi thấy hoạt động tốt hơn so với việc chọn một đầu key và value duy nhất hoặc khởi tạo ngẫu nhiên các đầu key và value mới từ đầu.

Hình 1: Tổng quan về việc chuyển đổi từ cơ chế chú ý đa đầu sang đa truy vấn. Các ma trận chiếu key và value từ tất cả các đầu được tính trung bình thành một đầu duy nhất.

Checkpoint đã chuyển đổi sau đó được tiền huấn luyện cho một tỷ lệ nhỏ α của các bước huấn luyện ban đầu trên cùng một công thức tiền huấn luyện.

### 2.2 Cơ chế chú ý truy vấn nhóm
Cơ chế chú ý truy vấn nhóm chia các đầu truy vấn thành G nhóm , mỗi nhóm chia sẻ một đầu key và một đầu value duy nhất. GQA -G đề cập đến truy vấn nhóm với G nhóm. GQA -1, với một nhóm duy nhất và do đó đầu key và value duy nhất, tương đương với MQA , trong khi GQA -H, với số nhóm bằng số đầu, tương đương với MHA . Hình 2 cho thấy sự so sánh giữa cơ chế chú ý truy vấn nhóm và cơ chế chú ý đa đầu/đa truy vấn. Khi chuyển đổi checkpoint đa đầu thành checkpoint GQA, chúng tôi xây dựng mỗi đầu key và value của nhóm bằng cách tính trung bình tất cả các đầu ban đầu trong nhóm đó.

Hình 2: Tổng quan về phương pháp truy vấn nhóm. Cơ chế chú ý đa đầu có H đầu truy vấn, key, và value. Cơ chế chú ý đa truy vấn chia sẻ các đầu key và value đơn lẻ trên tất cả các đầu truy vấn. Cơ chế chú ý truy vấn nhóm thay vào đó chia sẻ các đầu key và value đơn lẻ cho mỗi nhóm đầu truy vấn, nội suy giữa cơ chế chú ý đa đầu và đa truy vấn.

Một số lượng nhóm trung gian dẫn đến một mô hình được nội suy có chất lượng cao hơn MQA nhưng nhanh hơn MHA , và, như chúng tôi sẽ cho thấy, đại diện cho một sự đánh đổi thuận lợi. Đi từ MHA đến MQA giảm H đầu key và value xuống một đầu key và value duy nhất, giảm kích thước của bộ đệm key-value và do đó lượng dữ liệu cần được tải theo hệ số H. Tuy nhiên, các mô hình lớn hơn thường mở rộng số lượng đầu, sao cho cơ chế chú ý đa truy vấn đại diện cho một cắt giảm mạnh mẽ hơn trong cả băng thông bộ nhớ và khả năng. GQA cho phép chúng tôi giữ cùng một mức giảm tỷ lệ trong băng thông và khả năng khi kích thước mô hình tăng.

Hơn nữa, các mô hình lớn hơn chịu đau khổ ít hơn tương đối từ chi phí băng thông bộ nhớ từ cơ chế chú ý, vì bộ đệm KV mở rộng theo chiều mô hình trong khi FLOP và tham số mô hình mở rộng theo bình phương của chiều mô hình. Cuối cùng, việc phân đoạn tiêu chuẩn cho các mô hình lớn nhân bản đầu key và value đơn lẻ theo số lượng phân vùng mô hình (Pope et al., 2022); GQA loại bỏ sự lãng phí từ việc phân vùng như vậy. Do đó, chúng tôi mong đợi GQA sẽ trình bày một sự đánh đổi đặc biệt tốt cho các mô hình lớn hơn.

Chúng tôi lưu ý rằng GQA không được áp dụng cho các lớp tự chú ý của bộ mã hóa; các biểu diễn bộ mã hóa được tính toán song song, và băng thông bộ nhớ do đó thường không phải là nút thắt chính.

## 3 Thí nghiệm

### 3.1 Thiết lập thí nghiệm
Cấu hình Tất cả các mô hình đều dựa trên kiến trúc T5.1.1 (Raffel et al., 2020), được triển khai với JAX (Bradbury et al., 2018), Flax (Heek et al., 2020), và Flaxformer1. Cho các thí nghiệm chính của chúng tôi, chúng tôi xem xét T5 Large và XXL với cơ chế chú ý đa đầu, cũng như các phiên bản được nâng cấp huấn luyện của T5 XXL với cơ chế chú ý đa truy vấn và truy vấn nhóm. Chúng tôi sử dụng trình tối ưu hóa Adafactor với cùng các siêu tham số và lịch trình tốc độ học tập như T5 (Raffel et al., 2020). Chúng tôi áp dụng MQA và GQA cho tự chú ý và chú ý chéo của bộ giải mã, nhưng không cho tự chú ý của bộ mã hóa.

Nâng cấp huấn luyện Các mô hình được nâng cấp huấn luyện được khởi tạo từ các checkpoint T5.1.1 công khai. Các đầu key và value được tính trung bình thành cấu trúc MQA hoặc GQA thích hợp, và sau đó được tiền huấn luyện thêm cho tỷ lệ α của các bước tiền huấn luyện ban đầu với thiết lập tiền huấn luyện ban đầu và tập dữ liệu từ (Raffel et al., 2020). Với α= 0.05, huấn luyện mất khoảng 600 chip-ngày TPUv3.

Dữ liệu Chúng tôi đánh giá trên các tập dữ liệu tóm tắt CNN/Daily Mail (Nallapati et al., 2016), arXiv và PubMed (Cohan et al., 2018), MediaSum (Zhu et al., 2021), và Multi-News (Fabbri et al., 2019);

1https://github.com/google/flaxformer

tập dữ liệu dịch WMT 2014 Tiếng Anh-sang-Tiếng Đức; và tập dữ liệu trả lời câu hỏi TriviaQA (Joshi et al., 2017). Chúng tôi không đánh giá trên các bộ đánh giá phân loại phổ biến như GLUE (Wang et al., 2019) vì suy luận tự hồi quy ít có thể áp dụng cho những nhiệm vụ đó.

Tinh chỉnh Cho tinh chỉnh, chúng tôi sử dụng tốc độ học tập cố định 0.001, kích thước batch 128, và tỷ lệ dropout 0.1 cho tất cả các nhiệm vụ. CNN/Daily Mail và WMT sử dụng độ dài đầu vào 512 và độ dài đầu ra 256. Các tập dữ liệu tóm tắt khác sử dụng độ dài đầu vào 2048 và độ dài đầu ra 512. Cuối cùng, TriviaQA sử dụng độ dài đầu vào 2048 và độ dài đầu ra 32. Chúng tôi huấn luyện cho đến khi hội tụ và chọn checkpoint có hiệu suất dev cao nhất. Chúng tôi sử dụng giải mã tham lam cho suy luận.

Thời gian Chúng tôi báo cáo thời gian trên mỗi mẫu trên mỗi chip TPUv4, được đo bằng xprof (Google, 2020). Cho các thí nghiệm thời gian, chúng tôi sử dụng 8 TPU với kích thước batch lớn nhất phù hợp lên đến 32 trên mỗi TPU, và song song hóa được tối ưu hóa riêng cho từng mô hình.

### 3.2 Kết quả chính
Hình 3 cho thấy hiệu suất trung bình trên tất cả các tập dữ liệu như một hàm của thời gian suy luận trung bình cho các mô hình MHA T5-Large và T5-XXL, và các mô hình MQA và GQA -8 XXL được nâng cấp huấn luyện với tỷ lệ nâng cấp huấn luyện α= 0.05. Chúng tôi thấy rằng một mô hình MQA lớn hơn được nâng cấp huấn luyện cung cấp một sự đánh đổi thuận lợi so với các mô hình MHA, với chất lượng cao hơn và suy luận nhanh hơn so với MHA -Large. Hơn nữa, GQA đạt được những cải thiện chất lượng đáng kể bổ sung, đạt hiệu suất gần với MHA -XXL với tốc độ gần với MQA . Bảng 1 chứa kết quả đầy đủ cho tất cả các tập dữ liệu.

Bảng 1: So sánh thời gian suy luận và hiệu suất trung bình trên tập dev của các mô hình T5 Large và XXL với cơ chế chú ý đa đầu, và các mô hình T5-XXL được nâng cấp huấn luyện 5% với cơ chế chú ý đa truy vấn và truy vấn nhóm trên các tập dữ liệu tóm tắt CNN/Daily Mail, arXiv, PubMed, MediaSum, và MultiNews, tập dữ liệu dịch WMT, và tập dữ liệu trả lời câu hỏi TriviaQA.

| Mô hình | Tinfer | Trung bình | CNN | arXiv | PubMed | MediaSum | MultiNews | WMT | TriviaQA |
|---------|---------|------------|-----|-------|---------|----------|-----------|------|----------|
| | s | R1 | R1 | R1 | R1 | R1 | BLEU | F1 |
| MHA-Large | 0.37 | 46.0 | 42.9 | 44.6 | 46.2 | 35.5 | 46.6 | 27.7 | 78.2 |
| MHA-XXL | 1.51 | 47.2 | 43.8 | 45.6 | 47.5 | 36.4 | 46.9 | 28.4 | 81.9 |
| MQA-XXL | 0.24 | 46.6 | 43.0 | 45.0 | 46.9 | 36.1 | 46.5 | 28.5 | 81.3 |
| GQA-8-XXL | 0.28 | 47.1 | 43.5 | 45.4 | 47.7 | 36.3 | 47.2 | 28.4 | 81.6 |

### 3.3 Phân tích ablation
Phần này trình bày các thí nghiệm để điều tra tác động của các lựa chọn mô hình hóa khác nhau. Chúng tôi đánh giá hiệu suất trên một mẫu phụ đại diện của các nhiệm vụ: CNN/Daily Mail, (tóm tắt dạng ngắn), MultiNews (tóm tắt dạng dài), và TriviaQA (trả lời câu hỏi).

Hình 3: MQA được nâng cấp huấn luyện mang lại một sự đánh đổi thuận lợi so với MHA với chất lượng cao hơn và tốc độ nhanh hơn so với MHA -Large, và GQA đạt được hiệu suất thậm chí tốt hơn với những cải thiện tốc độ tương tự và chất lượng tương đương với MHA -XXL. Hiệu suất trung bình trên tất cả các nhiệm vụ như một hàm của thời gian suy luận trung bình trên mỗi mẫu cho T5-Large và T5-XXL với cơ chế chú ý đa đầu, và T5-XXL được nâng cấp huấn luyện 5% với cơ chế chú ý MQA và GQA-8.

Chuyển đổi checkpoint Hình 4 so sánh hiệu suất của các phương pháp khác nhau để chuyển đổi checkpoint. Tính trung bình dường như hoạt động tốt nhất, tiếp theo là chọn một đầu duy nhất và sau đó khởi tạo ngẫu nhiên. Một cách trực quan, kết quả được sắp xếp theo mức độ thông tin được bảo toàn từ mô hình được tiền huấn luyện.

Hình 4: So sánh hiệu suất của các phương pháp chuyển đổi checkpoint khác nhau cho T5-Large được nâng cấp huấn luyện thành MQA với tỷ lệ α= 0.05. 'Mean' tính trung bình các đầu key và value, 'First' chọn đầu đầu tiên và 'Random' khởi tạo các đầu từ đầu.

Các bước nâng cấp huấn luyện Hình 5 cho thấy hiệu suất thay đổi như thế nào với tỷ lệ nâng cấp huấn luyện cho T5 XXL với MQA và GQA . Đầu tiên, chúng tôi lưu ý rằng GQA đã đạt được hiệu suất hợp lý sau khi chuyển đổi trong khi MQA cần nâng cấp huấn luyện để

Hình 5: Hiệu suất như một hàm của tỷ lệ nâng cấp huấn luyện cho các mô hình T5 XXL với MQA và GQA-8.

hữu ích. Cả MQA và GQA đều có lợi từ nâng cấp huấn luyện 5% với hiệu quả giảm dần từ 10%.

Số lượng nhóm Hình 6 chứng minh tác động của số lượng nhóm GQA lên tốc độ suy luận. Đối với các mô hình lớn hơn, chi phí băng thông bộ nhớ từ bộ đệm KV ít hạn chế hơn (Shazeer, 2019), trong khi việc giảm kích thước key-value mạnh hơn do số lượng đầu tăng. Kết quả là, tăng số lượng nhóm từ MQA chỉ dẫn đến sự chậm lại nhẹ ban đầu, với chi phí tăng khi chúng ta tiến gần hơn đến MHA . Chúng tôi chọn 8 nhóm như một mức trung gian thuận lợi.

Hình 6: Thời gian trên mỗi mẫu cho GQA -XXL như một hàm của số lượng nhóm GQA với độ dài đầu vào 2048 và độ dài đầu ra 512. Đi từ 1 ( MQA ) đến 8 nhóm thêm chi phí suy luận khiêm tốn, với chi phí tăng khi thêm nhiều nhóm hơn.

## 4 Công trình liên quan
Công trình này tập trung vào việc đạt được một sự đánh đổi tốt hơn giữa chất lượng bộ giải mã và thời gian suy luận thông qua việc giảm chi phí băng thông bộ nhớ (Williams et al., 2009) từ việc tải key và value. Shazeer (2019) đầu tiên đề xuất giảm chi phí này thông qua cơ chế chú ý đa truy vấn. Các công trình tiếp theo cho thấy rằng cơ chế chú ý đa truy vấn đặc biệt hữu ích cho các đầu vào dài (Pope et al., 2022; de Jong et al., 2022). Rabe (2023) độc lập phát triển GQA với việc triển khai công khai. Các công trình khác đã khám phá việc nhóm các đầu chú ý cho hiệu quả tính toán (Park et al., 2020; Luo et al., 2022; Ni et al., 2023) mà không tập trung cụ thể vào các đầu key-value, điều quyết định chi phí băng thông bộ nhớ.

Một số phương pháp khác đã được đề xuất để giảm chi phí băng thông bộ nhớ từ key và value, cũng như các tham số. Flash attention (Dao et al., 2022) cấu trúc tính toán chú ý để tránh cụ thể hóa các điểm chú ý bậc hai, giảm bộ nhớ và tăng tốc huấn luyện. Lượng tử hóa (Dettmers et al., 2022; Frantar et al., 2022) giảm kích thước của trọng số và kích hoạt, bao gồm key và value, bằng cách giảm độ chính xác. Chưng cất mô hình (Hinton et al., 2015; Gou et al., 2021) thay vào đó giảm kích thước mô hình ở một độ chính xác nhất định, sử dụng dữ liệu được tạo từ mô hình lớn hơn để tinh chỉnh mô hình nhỏ hơn. Chú ý chéo thưa thớt theo lớp (de Jong et al., 2022) loại bỏ hầu hết các lớp chú ý chéo tạo nên chi phí chính cho các đầu vào dài hơn. Lấy mẫu suy đoán (Chen et al., 2023; Leviathan et al., 2022) cải thiện nút thắt băng thông bộ nhớ bằng cách đề xuất nhiều token với một mô hình nhỏ hơn sau đó được chấm điểm song song bởi một mô hình lớn hơn.

Cuối cùng, quy trình nâng cấp huấn luyện mà chúng tôi đề xuất được lấy cảm hứng từ Komatsuzaki et al. (2022), nâng cấp huấn luyện các checkpoint T5 tiêu chuẩn thành các mô hình Mixture-of-Experts được kích hoạt thưa thớt.

## 5 Kết luận
Các mô hình ngôn ngữ tốn kém cho suy luận chủ yếu do chi phí băng thông bộ nhớ từ việc tải key và value. Cơ chế chú ý đa truy vấn giảm chi phí này với cái giá là giảm khả năng và chất lượng mô hình. Chúng tôi đề xuất chuyển đổi các mô hình chú ý đa đầu thành các mô hình đa truy vấn với một phần nhỏ lượng tính toán tiền huấn luyện ban đầu. Hơn nữa, chúng tôi giới thiệu cơ chế chú ý truy vấn nhóm, một sự nội suy của cơ chế chú ý đa truy vấn và đa đầu đạt được chất lượng gần với đa đầu ở tốc độ tương đương với cơ chế chú ý đa truy vấn.

## Hạn chế
Bài báo này tập trung vào việc cải thiện chi phí băng thông bộ nhớ từ việc tải key và value. Chi phí này quan trọng nhất khi tạo ra các chuỗi dài hơn, mà chất lượng vốn khó đánh giá. Cho tóm tắt, chúng tôi sử dụng điểm Rouge, mà chúng tôi biết là một đánh giá có lỗi không kể hết được câu chuyện; vì lý do đó, khó có thể chắc chắn rằng các sự đánh đổi của chúng tôi là đúng. Do hạn chế tính toán, chúng tôi cũng không so sánh mô hình GQA XXL của chúng tôi với một mô hình tương đương được huấn luyện từ đầu, vì vậy chúng tôi không biết hiệu suất tương đối của nâng cấp huấn luyện so với huấn luyện từ đầu. Cuối cùng, chúng tôi đánh giá tác động của nâng cấp huấn luyện và GQA chỉ trên các mô hình mã hóa-giải mã. Gần đây, các mô hình chỉ giải mã cực kỳ phổ biến, và vì các mô hình này không có tự chú ý và chú ý chéo riêng biệt, chúng tôi mong đợi GQA sẽ có lợi thế mạnh hơn so với MQA.

## Lời cảm ơn
Chúng tôi cảm ơn Santiago Ontañón, Afroz Mohiuddin, William Cohen và những người khác tại Google Research vì lời khuyên sâu sắc và thảo luận.

## Tài liệu tham khảo
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. CoRR , abs/2302.01318.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 615–621, New Orleans, Louisiana. Association for Computational Linguistics.

Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. CoRR , abs/2205.14135.

Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen. 2022. FiDO: Fusion-in-decoder optimized for stronger performance and faster inference. arXiv preprint arXiv:2212.08153 .

Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. CoRR , abs/2208.07339.

Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 1074–1084. Association for Computational Linguistics.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. GPTQ: accurate post-training quantization for generative pre-trained transformers. CoRR , abs/2210.17323.

Google. 2020. Profile your model with cloud tpu tools. https://cloud.google.com/tpu/docs/cloud-tpu-tools . Accessed: 2022-11-11.

Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. Int. J. Comput. Vis. , 129(6):1789–1819.

Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. 2020. Flax: A neural network library and ecosystem for JAX.

Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. CoRR , abs/1503.02531.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , Vancouver, Canada. Association for Computational Linguistics.

Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2022. Sparse upcycling: Training mixture-of-experts from dense checkpoints.

Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2022. Fast inference from transformers via speculative decoding. CoRR , abs/2211.17192.

Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2022. Towards lightweight transformer via group-wise transformation for vision-and-language tasks. IEEE Trans. Image Process. , 31:3386–3398.

Ramesh Nallapati, Bowen Zhou, Cícero Nogueira dos Santos, Çaglar Gülçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016 , pages 280–290. ACL.

Jinjie Ni, Rui Mao, Zonglin Yang, Han Lei, and Erik Cambria. 2023. Finding the pillars of strength for multi-head attention. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 14526–14540. Association for Computational Linguistics.

Sungrae Park, Geewook Kim, Junyeop Lee, Junbum Cha, Ji-Hoon Kim, and Hwalsuk Lee. 2020. Scale down transformer by grouping features for a lightweight character-level language model. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020 , pages 6883–6893. International Committee on Computational Linguistics.

Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 .

Markus Rabe. 2023. Memory-efficient attention. https://github.com/google/flaxformer/blob/main/flaxformer/components/attention/memory_efficient_attention.py . Accessed: 2023-05-23.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 21:140:1–140:67.

Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 .

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net.

Samuel Williams, Andrew Waterman, and David A. Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM , 52(4):65–76.

Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. Mediasum: A large-scale media interview dataset for dialogue summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 5927–5934. Association for Computational Linguistics.

## A Tính ổn định huấn luyện
Chúng tôi thấy rằng cơ chế chú ý đa truy vấn có thể dẫn đến bất ổn định huấn luyện trong quá trình tinh chỉnh, đặc biệt kết hợp với các nhiệm vụ đầu vào dài. Chúng tôi đã huấn luyện nhiều mô hình T5-Large với cơ chế chú ý đa truy vấn từ đầu. Trong mỗi trường hợp, tiền huấn luyện gặp phải những cú nhảy mất mát thường xuyên và các mô hình cuối cùng phân kỳ ngay lập tức khi tinh chỉnh trên các nhiệm vụ đầu vào dài. Các mô hình cơ chế chú ý đa truy vấn được nâng cấp huấn luyện ổn định hơn nhưng vẫn hiển thị độ biến thiên cao, vì vậy cho các mô hình đa truy vấn trên các nhiệm vụ không ổn định, chúng tôi báo cáo hiệu suất trung bình trên ba lần chạy tinh chỉnh. Tuy nhiên, các mô hình cơ chế chú ý truy vấn nhóm được nâng cấp huấn luyện dường như ổn định, vì vậy chúng tôi không điều tra thêm về nguyên nhân gốc rễ của sự bất ổn định đa truy vấn.
