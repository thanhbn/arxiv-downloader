# 2402.04347.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2402.04347.pdf
# Kích thước tệp: 5011947 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hedgehog & Porcupine: Cơ chế Attention Tuyến tính Biểu cảm với Khả năng Mô phỏng Softmax
Michael Zhang, Kush Bhatia, Hermann Kumbong và Christopher Ré
Khoa Khoa học Máy tính, Đại học Stanford
{mzhang,kushb,chrismre}@cs.stanford.edu, kumboh@stanford.edu

Tóm tắt
Các cơ chế attention tuyến tính đã cho thấy tiềm năng cải thiện hiệu quả Transformer, giảm độ phức tạp bậc hai của attention xuống tuyến tính theo độ dài chuỗi. Điều này mang lại triển vọng thú vị cho (1) huấn luyện Transformer tuyến tính từ đầu, (2) "chuyển đổi tinh chỉnh" các Transformer chuyên biệt thành phiên bản tuyến tính có thể khôi phục hiệu suất nhiệm vụ, và (3) "chuyển đổi được tiền huấn luyện" các Transformer như mô hình ngôn ngữ lớn thành phiên bản tuyến tính có thể tinh chỉnh trên các nhiệm vụ hạ nguồn. Tuy nhiên, các cơ chế attention tuyến tính thường kém hiệu suất so với attention softmax tiêu chuẩn về chất lượng. Để thu hẹp khoảng cách hiệu suất này, chúng tôi phát hiện các attention tuyến tính trước đây thiếu các tính chất quan trọng của attention softmax liên quan đến hiệu suất tốt: trọng số entropy thấp (hoặc "nhọn") và tính đơn điệu tích vô hướng. Chúng tôi tiếp tục quan sát các ánh xạ đặc trưng đơn giản đáng ngạc nhiên giữ lại những tính chất này và khớp với hiệu suất softmax, nhưng không hiệu quả khi tính toán trong attention tuyến tính. Do đó chúng tôi đề xuất Hedgehog, một attention tuyến tính có thể học giữ lại các tính chất nhọn và đơn điệu của attention softmax trong khi duy trì độ phức tạp tuyến tính. Hedgehog sử dụng các MLP huấn luyện đơn giản để tạo ra trọng số attention mô phỏng attention softmax. Các thí nghiệm cho thấy Hedgehog khôi phục hơn 99% chất lượng Transformer tiêu chuẩn trong các thiết lập huấn luyện từ đầu và chuyển đổi tinh chỉnh, vượt trội hơn các attention tuyến tính trước đây đến 6 điểm perplexity trên WikiText-103 với GPT nhân quả, và đến 8.7 điểm GLUE trên BERT hai chiều được tinh chỉnh. Hedgehog cũng cho phép chuyển đổi được tiền huấn luyện. Chuyển đổi GPT-2 được tiền huấn luyện thành biến thể attention tuyến tính đạt được 16.7 perplexity tiên tiến nhất trên WikiText-103 cho các mô hình decoder dưới bậc hai 125M. Cuối cùng chúng tôi biến Llama-2 7B được tiền huấn luyện thành Llama attention tuyến tính khả thi. Với adaptation tầm thấp, Hedgehog-Llama2 7B đạt 28.1 điểm ROUGE-1 cao hơn so với mô hình attention tiêu chuẩn cơ sở, trong khi các attention tuyến tính trước đây dẫn đến giảm 16.5 điểm.

1 Giới thiệu
Các cơ chế attention tuyến tính là phương pháp hứa hẹn để cải thiện hiệu quả Transformer. Bằng cách thay thế softmax của tích vô hướng query và key attention bằng các ánh xạ đặc trưng hàm kernel, attention tuyến tính giảm độ phức tạp thời gian và không gian của attention từ O(n²d) xuống O(ndd') trong đó n là độ dài chuỗi, d là chiều head và d' là chiều ánh xạ đặc trưng (Katharopoulos et al., 2020; Choromanski et al., 2020; Peng et al., 2021; Xiong et al., 2021; Schlag et al., 2021). Đối với các thiết lập Transformer điển hình, ví dụ với chiều head = 64 và độ dài chuỗi từ 512 đến 32K, việc mở rộng quy mô từ bậc hai xuống tuyến tính này có thể dẫn đến cải thiện đáng kể về tốc độ và bộ nhớ (Hình 6). Như các giải pháp thay thế trực tiếp cho attention softmax phổ biến (Vaswani et al., 2017), attention tuyến tính không chỉ cải thiện hiệu quả Transformer khi huấn luyện mô hình mới từ đầu mà còn có thể cải thiện hiệu quả suy luận bằng cách chuyển đổi Transformer được tiền huấn luyện thành các biến thể tuyến tính tương ứng (Kasai et al., 2021; Mao, 2022). Attention tuyến tính cho phép Transformer hiệu quả trong nhiều chế độ khác nhau:

•Huấn luyện từ đầu: huấn luyện mô hình Transformer với attention tuyến tính với mục tiêu khớp hiệu suất Transformer tiêu chuẩn, ví dụ như được kiểm tra trên các benchmark như phân loại Long Range Arena (LRA) (Tay et al., 2021) và mô hình hóa ngôn ngữ WikiText-103 (Merity et al., 2017).

•Chuyển đổi tinh chỉnh: hoán đổi attention của Transformer chuyên biệt và tinh chỉnh chúng để chuyển đổi mô hình hiện có thành phiên bản tuyến tính, với mục tiêu khôi phục hiệu suất nhiệm vụ gốc với hiệu quả được cải thiện (Kasai et al., 2021; Mao, 2022).

1arXiv:2402.04347v1 [cs.LG] 6 Feb 2024

--- TRANG 2 ---
Hình 1: Hedgehog học một ánh xạ đặc trưng attention tuyến tính có thể huấn luyện được thiết kế để mô phỏng attention tiêu chuẩn, dẫn đến attention tuyến tính biểu cảm nhưng hiệu quả cho các thiết lập huấn luyện Transformer khác nhau

•Chuyển đổi được tiền huấn luyện: làm tương tự như chuyển đổi tinh chỉnh nhưng cho Transformer được tiền huấn luyện như mô hình ngôn ngữ lớn (LLM), ví dụ để chuyển sang nhiệm vụ mới và ngữ cảnh dài hơn.

Thật không may, các cơ chế attention tuyến tính hiện có thường không thể khớp với attention softmax về chất lượng mô hình hóa. Khi huấn luyện từ đầu, attention tuyến tính đạt được 4-6 perplexity (ppl) tệ hơn so với attention softmax trên các benchmark tiêu chuẩn như WikiText-103 (Schlag et al., 2021; Irie et al., 2021; Fu et al., 2023), khoảng cách tương đương giữa Transformer 125M và 255M (Dai et al., 2019). Khi chuyển đổi mô hình được tinh chỉnh, mô hình attention tuyến tính cần các module attention bậc hai bổ sung để thu hẹp khoảng cách (Kasai et al., 2021; Mao, 2022). Người ta có thể lo ngại rằng những khoảng cách như vậy là cơ bản; ví dụ, lý thuyết gần đây sử dụng Giả thuyết Thời gian Exponential Mạnh (SETH) đã chỉ ra rằng các thuật toán thực sự dưới bậc hai chất lượng cao để xấp xỉ attention softmax có thể là không thể với độ dài chuỗi lớn n (Alman & Song, 2023; Keles et al., 2023).

Chúng tôi bắt đầu bằng cách nghiên cứu thực nghiệm tại sao khoảng cách hiệu suất này tồn tại giữa softmax tiêu chuẩn và các attention tuyến tính được đề xuất. Chúng tôi xác định hai tính chất đơn giản cho attention softmax mà các attention tuyến tính trước đây thiếu: 1) "độ nhọn" entropy thấp và 2) tính đơn điệu tích vô hướng. Chúng tôi giả thuyết rằng khoảng cách chất lượng trong attention tuyến tính tương ứng với việc thiếu hai tính chất này:

•"Độ nhọn" entropy thấp: Một cách trực quan, chúng ta muốn các attention tập trung vào token liên quan trong khi bỏ qua những token không liên quan thông qua tương tác query-key của chúng. Chúng tôi quan sát những phân phối trọng số attention entropy thấp hoặc "nhọn" này trong attention Transformer tiêu chuẩn nhưng không có trong các ánh xạ attention tuyến tính trước đây—nơi các đỉnh được kích hoạt thông qua softmax tích vô hướng có tỷ lệ bị mất qua các ánh xạ đặc trưng khác (Hình 2)—và phát hiện điều này có tương quan mạnh với hiệu suất Transformer (Hình 4).

•Tính đơn điệu tích vô hướng: Tính chất này yêu cầu trọng số attention tăng khi tích vô hướng của query và key tương ứng tăng. Một cách trực quan, việc thiếu tính đơn điệu này có thể tạo ra gradient không ổn định trong quá trình huấn luyện và tinh chỉnh, nơi việc tăng tích vô hướng query-key có thể dẫn đến giảm trọng số attention theo hướng khác (và ngược lại).

Như một bước đầu tiên để khôi phục những tính chất này, chúng tôi khám phá các ánh xạ đặc trưng đơn giản—như xấp xỉ đa thức Taylor bậc thấp cho hàm exp()—thỏa mãn hai tính chất trên (mặc dù trong các chế độ hạn chế của tích vô hướng query-key bị chặn). Trong thực tế, chúng tôi thấy rằng query và key thường bị chặn, dẫn đến attention tuyến tính khôi phục độ nhọn, tính đơn điệu và hiệu suất tiếp theo của attention softmax. Thật không may, mặc dù về mặt kỹ thuật là tuyến tính theo độ dài chuỗi, những ánh xạ đặc trưng đa thức này vẫn không hiệu quả để tính toán. Chúng cần O(nd^(p+1)) thời gian và không gian, và chúng tôi thấy cần bậc p≥2 cho hiệu suất.

Do đó chúng tôi đề xuất Hedgehog, một attention tuyến tính có thể học hiệu quả tính toán được huấn luyện để nắm bắt các tính chất softmax nhọn và đơn điệu. Không giống như các công trình trước đây đề xuất một hàm kernel cụ thể (Katharopoulos et al., 2020; Choromanski et al., 2020; Qin et al., 2022b) và các ánh xạ đặc trưng đa thức của chúng tôi, chúng tôi học các ánh xạ đặc trưng này như các MLP một lớp được huấn luyện cụ thể để khớp với trọng số attention softmax. Bằng cách ánh xạ từ R^d→R^d, chúng tôi duy trì độ phức tạp O(nd²) của các attention tuyến tính trước đây. Tuy nhiên, huấn luyện những ánh xạ này thông qua trọng số attention softmax như nhãn mềm cross-entropy, chúng tôi thấy Hedgehog có thể khớp với trọng số attention softmax với độ trung thực cao hơn nhiều (Hình 7), tạo ra trọng số entropy thấp và đơn điệu khớp với chất lượng hiệu suất attention tiêu chuẩn.

2

--- TRANG 3 ---
Chúng tôi xác thực thực nghiệm rằng khả năng biểu cảm được cải thiện của Hedgehog chuyển thành việc thu hẹp khoảng cách hiệu suất attention softmax trong ba chế độ được đề cập ở trên:

•Huấn luyện từ đầu: chúng tôi thấy Hedgehog khớp với Transformer trên các benchmark attention tiêu chuẩn như nhiệm vụ Long Range Arena (LRA) (Tay et al., 2021), và thu hẹp khoảng cách attention tuyến tính 68.6% trên mô hình hóa ngôn ngữ WikiText-103 (cải thiện lên đến 6 ppl).

•Chuyển đổi tinh chỉnh: chúng tôi thấy Hedgehog khôi phục >99% hiệu suất mô hình gốc trung bình trên các mô hình BERT-base encoder-only hai chiều 110M được tinh chỉnh trên GLUE và mô hình GPT decoder-only nhân quả 125M được tinh chỉnh trên Wikitext-103.

•Chuyển đổi được tiền huấn luyện: chúng tôi thấy Hedgehog cho phép chuyển giao hiệu quả sang nhiệm vụ mới và mở rộng quy mô hiệu quả sang ngữ cảnh dài hơn, trong khi thường vượt trội hơn các kiến trúc chuỗi dưới bậc hai hiện đại bằng cách tuyến tính hóa Transformer được tiền huấn luyện hiện có. Hedgehog-GPT-2 125M được tinh chỉnh trên Wikitext-103 đạt được 16.7 ppl tiên tiến nhất mới cho mô hình dưới bậc hai cùng kích thước.

Cuối cùng, chúng tôi chứng minh rằng Hedgehog có thể được mở rộng quy mô lên mô hình ngôn ngữ lớn hiện đại; chúng tôi chuyển đổi Llama-2 7B được tiền huấn luyện thành Llama attention tuyến tính khả thi. Với adaptation tầm thấp, Hedgehog-Llama2 7B đạt được lên đến 28.1 điểm ROUGE-1 cao hơn so với mô hình attention tiêu chuẩn cơ sở. Ngược lại, các attention tuyến tính trước đây dẫn đến mô hình gặp khó khăn trong việc tạo ra văn bản mạch lạc (với giảm 16.5 điểm ROUGE-1).

2 Kiến thức nền và Công trình Liên quan
Chúng tôi cung cấp nền tảng về tính toán attention, mô tả attention tuyến tính dựa trên đặc trưng kernel, và cuối cùng cung cấp chi tiết về các cơ chế attention tuyến tính hiện có được đề xuất trong literature.

Thiết lập attention. Gọi {q_i}^n_{i=1}, {k_i}^n_{i=1}, {v_i}^n_{i=1} biểu thị tập hợp query, key và value, với các phần tử riêng lẻ trong R^d. Gọi n biểu thị độ dài chuỗi và d biểu thị chiều head. Chúng ta tính toán đầu ra attention y_i∈R^d bằng cách đầu tiên tính toán độ tương tự giữa mỗi q_i và mọi k_j; đối với attention nhân quả chúng ta tính toán những độ tương tự này cho j≤i. Attention Transformer vanilla tính toán những độ tương tự này sử dụng tích vô hướng softmax (Vaswani et al., 2017):

y_i = Σ^i_{j=1} sim(q_i,k_j)v_j, trong đó sim(q_i,k_j) = exp(q_i^T k_j/√d) / Σ^i_{m=1} exp(q_i^T k_m/√d). (1)

Mặc dù rất biểu cảm, việc tính toán attention thông qua Eq. 1 cho tất cả {y_i}^n_{i=1} cần O(n²d) thời gian và bộ nhớ, làm cho điều này không hiệu quả cho chuỗi dài. Để cải thiện hiệu quả mà không hy sinh chất lượng, do đó chúng ta muốn các ánh xạ attention tuyến tính thay thế duy trì khả năng biểu cảm của attention tiêu chuẩn.

Attention tuyến tính và hàm kernel. Quan sát rằng exp(·) trong Eq. 1 có thể được xem như một hàm kernel, mà Tsai et al. (2019); Katharopoulos et al. (2020) chỉ ra có thể được thay thế nói chung bằng K(x,x') = φ(x)^T φ(x'). Ở đây φ:R^d→R^{d'} là một ánh xạ đặc trưng được áp dụng cho mỗi vector. Do đó chúng ta có thể tính toán attention trong thời gian và không gian tuyến tính theo độ dài chuỗi n, được thấy bằng cách viết lại Eq. 1 như:

y_i = φ(q_i) Σ^i_{j=1} φ(k_j)^T v_j / φ(q_i) Σ^i_{j=1} φ(k_j). (2)

Các ánh xạ đặc trưng trước đây. Từ phần trước, chúng ta quan sát rằng attention tuyến tính là hướng đi hứa hẹn để cải thiện hiệu quả Transformer ở cả thời gian huấn luyện và suy luận. Nhiều công trình trước đây đã đề xuất các ánh xạ đặc trưng φ nhằm mục đích vẫn hiệu quả hơn (nơi attention tuyến tính mong muốn so với attention tiêu chuẩn nếu d' < n), trong khi vẫn biểu cảm và ổn định để huấn luyện. Những cái này dao động từ φ đảm bảo trọng số attention dương, ví dụ thông qua 1 + ELU (Katharopoulos et al., 2020) hoặc ReLU (Kasai et al., 2021), đến xấp xỉ kernel softmax hoặc Gaussian thông qua đặc trưng ngẫu nhiên (Rahimi & Recht, 2007; Choromanski et al., 2020; Peng et al., 2021; Choromanski et al., 2021; Zheng et al., 2023) hoặc xấp xỉ tầm thấp (Xiong et al., 2021; Chen et al., 2021).

3

--- TRANG 4 ---
Hình 2: Độ nhọn trọng số attention. (Biểu đồ 1 - 5): Attention softmax dẫn đến entropy thấp hơn và trọng số "nhọn" chọn lọc so với các attention tuyến tính trước đây (huấn luyện từ đầu trên associative recall (Mục 3.2)). (Biểu đồ 6): Bằng cách huấn luyện để mô phỏng attention softmax, Hedgehog được đề xuất của chúng tôi khôi phục độ nhọn này như một attention tuyến tính, tương ứng với hiệu suất được cải thiện (Mục 5).

Hình 3: Tính đơn điệu trọng số attention. (Biểu đồ 1 - 5): Trái ngược với attention softmax, các attention tuyến tính trước đây không đơn điệu mượt mà trên tích vô hướng query-key được huấn luyện, dẫn đến hiệu suất kém khi chuyển đổi mô hình BERT bằng cách thay thế attention (Bảng 1). (Biểu đồ 6): Hedgehog khôi phục tính đơn điệu này, và do đó khôi phục 99% hiệu suất BERT sau chuyển đổi (Bảng 8).

3 Cải thiện Attention Tuyến tính thông qua Trọng số Nhọn và Đơn điệu
Chúng tôi bắt đầu bằng cách xác định hai tính chất quan trọng của trọng số attention mà chúng tôi giả thuyết là cần thiết cho chất lượng hiệu suất tốt. Thứ nhất, độ nhọn entropy thấp, yêu cầu ánh xạ attention có thể nắm bắt hiệu quả các token liên quan thưa thớt trong một chuỗi. Thứ hai, tính đơn điệu trên tích vô hướng query-key, yêu cầu ánh xạ attention tăng với tích vô hướng tăng, và cho phép chuyển đổi mượt mà các Transformer được tiền huấn luyện thành biến thể tuyến tính.

3.1 Tính chất cho Ánh xạ Attention Biểu cảm
Ở đây chúng tôi mô tả các tính chất nhọn và đơn điệu được giả thuyết cho attention tuyến tính mong muốn. Chúng tôi lưu ý những cái này bổ sung cho các quan sát trong quá khứ cho attention tuyến tính hiệu suất cao hơn, bao gồm trọng số attention dương (Katharopoulos et al., 2020), đặc trưng trực giao (Choromanski et al., 2020; Irie et al., 2021), hoặc tính địa phương (tăng trọng số cho value gần đó) (Qin et al., 2022a,b). Chúng tôi xác thực những tính chất này trong các attention tuyến tính trong quá khứ ở Mục 3.2, và xem trước cách attention tuyến tính Hedgehog được đề xuất của chúng tôi khôi phục những tính chất này tương ứng với hiệu suất được cải thiện (Mục 5) trong Hình 2, 3.

Độ nhọn entropy thấp. Một cách trực quan, một nguồn của hiệu quả attention là khả năng chọn lọc tăng trọng số cho token liên quan trong một chuỗi. Đây là một diễn giải phổ biến được trực quan hóa trong các kiến trúc và thiết lập Transformer khác nhau từ dịch ngôn ngữ encoder-decoder (Bahdanau et al., 2014) đến phân đoạn hình ảnh ViT (Dosovitskiy et al., 2020; Caron et al., 2021). Về mặt cơ học, softmax trên tích vô hướng query-key tạo số mũ cho độ tương tự tương đối giữa một query và mỗi key, được lượng hóa thông qua phân phối trọng số attention entropy thấp hoặc "nhọn" (Hình 2).

4

--- TRANG 5 ---
BERT-FT | 1 + ELU | ReLU | Performer | cosFormer | exp(t = 1) | exp(t = 2)
Matthew's correlation | 58.8 | 28.1 | 39.5 | 24.7 | 39.9 | 45.9 | 50.0

Bảng 1: Hiệu suất chuyển đổi tinh chỉnh của BERT được tinh chỉnh trên CoLA (BERT-FT), sử dụng các attention tuyến tính trước đây. Với tính đơn điệu kém (Hình 3), các phương pháp trước đây không thể khôi phục hiệu suất.

Các ánh xạ attention tuyến tính hoạt động bằng cách thay thế softmax bằng tích vô hướng chuẩn hóa của các ánh xạ đặc trưng thay thế (Eq. 2). Với các ánh xạ đặc trưng hiện có, chúng tôi thấy trọng số attention kết quả có thể dẫn đến entropy cao hơn nhiều hoặc phân phối đồng đều hơn. Điều này đúng ngay cả với các phương pháp được thiết kế để xấp xỉ softmax dưới giới hạn lỗi bình phương trung bình (Choromanski et al., 2020) (Performer, Hình 2) hoặc tính địa phương được áp đặt (Qin et al., 2022b) (cosFormer, Hình 2). Sự đồng đều này trong trọng số attention làm giảm khả năng mô hình hóa của attention tuyến tính dẫn đến chất lượng hiệu suất tệ hơn.

Tính đơn điệu trên tích vô hướng query-key. Tính chất này yêu cầu các ánh xạ attention đơn điệu trên tích vô hướng query-key: khi tích vô hướng tăng (giảm), trọng số attention tăng (giảm). Trong Hình 3, chúng tôi quan sát rằng trong khi attention softmax thể hiện tính đơn điệu này (biểu đồ phụ đầu tiên), các attention tuyến tính hiện có thì không. Chúng tôi tin rằng điều này có thể gây ra vấn đề huấn luyện sau khi hoán đổi attention do gradient mâu thuẫn giữa attention và các tham số mô hình gốc. Trong Hình 3, việc cố gắng tăng trọng số attention bằng cách tăng độ tương tự tích có thể thực sự dẫn đến giảm trọng số attention. Sau đó trong Mục 3.2, chúng tôi thấy điều này tương ứng với việc không thể khôi phục hiệu suất gốc khi chuyển đổi Transformer được tinh chỉnh.

3.2 Giải thích Khoảng cách Hiệu suất Attention Tuyến tính
Chúng tôi xác thực hai tính chất được giới thiệu ở trên bằng cách chỉ ra rằng (1) thiếu độ nhọn tương ứng với hiệu suất tệ hơn đáng kể khi huấn luyện từ đầu, và (2) thiếu độ nhọn và tính đơn điệu tương ứng với việc không thể khôi phục hiệu suất khi chuyển đổi mô hình được tinh chỉnh.

Huấn luyện từ đầu. Chúng tôi so sánh khả năng của các Transformer khác nhau trong việc giải quyết Associative Recall (AR) (Ba et al., 2016), một nhiệm vụ dự đoán token tiếp theo trước đây được nghiên cứu như một proxy cho khả năng mô hình hóa ngôn ngữ (Olsson et al., 2022). AR kiểm tra mức độ tốt của một mô hình có thể nhớ lại nội dung cụ thể trong một chuỗi đầu vào, được cấu trúc như một danh sách các cặp key-value kết thúc bằng một key (Bảng 12).

Hình 4: Hiệu suất associative recall tương quan mạnh với entropy attention thấp hơn; có mặt trong attention softmax nhưng không có trong các biến thể tuyến tính trước đây.

Như một đối chứng để đánh giá giả thuyết của chúng tôi, chúng tôi cũng xem xét một ánh xạ đặc trưng đơn giản được thiết kế để tạo ra "độ nhọn" nhưng không có tính đơn điệu: φ_t(x) = exp(x·t), áp dụng một hàm mũ theo từng phần tử với tỷ lệ nhiệt độ t.

Trong Hình 4, chúng tôi quan sát một tương quan mạnh giữa trọng số attention entropy thấp và độ chính xác AR. Trong khi attention softmax giải quyết nhiệm vụ AR hoàn hảo, các attention tuyến tính trước đây gặp khó khăn để đạt được ngay cả 20% độ chính xác, đồng thời thu được entropy trọng số attention lớn hơn nhiều. Như hỗ trợ thêm cho giả thuyết của chúng tôi, chúng tôi thấy rằng trong khi ánh xạ mũ φ_1 thất bại AR và tạo ra trọng số attention entropy cao tương tự, việc tăng độ nhọn với t = 2 thực sự giải quyết được nhiệm vụ.

Chuyển đổi tinh chỉnh. Tiếp theo chúng tôi so sánh các attention tuyến tính khác nhau hoạt động như thế nào trong việc khôi phục hiệu suất attention softmax gốc cho chuyển đổi tinh chỉnh. Chúng tôi áp dụng quy trình trong Kasai et al. (2021), lấy một Transformer đã được tinh chỉnh trên một nhiệm vụ cụ thể, hoán đổi các lớp attention bằng biến thể attention tuyến tính, và tinh chỉnh thêm toàn bộ mô hình trên cùng nhiệm vụ đó.

Cho thiết lập này, chúng tôi đánh giá với mô hình BERT-base-uncased (Devlin et al., 2018) được tinh chỉnh trên nhiệm vụ Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), nơi mục tiêu là phân loại xem một câu có đúng ngữ pháp hay không. Chúng tôi so sánh hiệu suất của mô hình BERT gốc (attention softmax)¹ với các mô hình được chuyển đổi attention tuyến tính. Trong Bảng 1, chúng tôi thấy rằng cũng như không có attention tuyến tính nào nắm bắt một cách mượt mà tính đơn điệu trên tích vô hướng query-key của mô hình được huấn luyện, không có attention tuyến tính nào hoàn toàn khôi phục tương quan Matthew 58.8 của BERT được tinh chỉnh gốc. Điều này bao gồm ánh xạ đặc trưng nhọn φ_2 đủ trong chế độ huấn luyện từ đầu.

¹https://huggingface.co/JeremiahZ/bert-base-uncased-cola

5

--- TRANG 6 ---
4 Hedgehog: Attention Tuyến tính Biểu cảm thông qua Mô phỏng Softmax
Chúng tôi trình bày Hedgehog, một ánh xạ đặc trưng đơn giản, hiệu quả và biểu cảm được huấn luyện để mô phỏng attention softmax. Hedgehog được dựa trên (1) việc tồn tại các xấp xỉ attention tuyến tính cho softmax khôi phục các tính chất nhọn và đơn điệu của attention tiêu chuẩn trong thực tế, và (2) chúng ta có thể tính toán hiệu quả các xấp xỉ tương tự.

Trong Mục 4.1, chúng tôi thúc đẩy Hedgehog và chỉ ra rằng (1) là có thể bằng cách xem xét lại đa thức Taylor bậc thấp. Chúng tôi thấy rằng đối với attention tuyến tính, hàm mũ Taylor hoạt động như một ánh xạ đặc trưng đơn giản đáng ngạc nhiên, khôi phục độ nhọn và tính đơn điệu trong khi khớp với hiệu suất Transformer tiêu chuẩn. Thật không may, chúng tôi cũng thấy nó gây ra các vấn đề riêng, nơi ánh xạ đặc trưng dẫn đến chiều query và key lớn và trở nên không hiệu quả để tính toán. Trong Mục 4.2, để vượt qua những thách thức này, chúng tôi đề xuất và mô tả Hedgehog, một attention tuyến tính có thể huấn luyện được huấn luyện để mô phỏng attention softmax. Trong Mục 5.1, chúng tôi chỉ ra cách này cho phép các tính chất nhọn và đơn điệu tương tự như softmax và attention mũ Taylor, trong khi giữ lại hiệu quả của các attention tuyến tính trong quá khứ.

4.1 Xấp xỉ Đa thức Đơn giản cho Attention Softmax
Từ các phát hiện của chúng tôi trong Mục 3, chúng tôi tìm kiếm một giải pháp thay thế tuyến tính hiệu quả cho softmax giữ lại các tính chất nhọn và đơn điệu của nó. Đầu tiên chúng tôi xem xét một cách tiếp cận tiềm năng đơn giản: xấp xỉ hàm mũ trong softmax bằng đa thức Taylor bậc thấp (Keles et al., 2023; Banerjee et al., 2020).

Trong khi nói chung, một xấp xỉ chất lượng cao cho softmax nên giữ lại các tính chất nhọn, đơn điệu và hiệu suất của nó, chúng tôi căn cứ cuộc điều tra của mình với hai lưu ý tiềm năng cho đa thức Taylor. Đầu tiên, nhớ lại rằng ánh xạ đặc trưng cho xấp xỉ đa thức bậc p có thể được tính toán trong thời gian và không gian O(nd^p) cho mỗi vector query và key. Do đó, trong khi điều này thực sự dưới bậc hai theo độ dài chuỗi, câu hỏi vẫn là liệu chúng ta có thể đặt p đủ thấp để làm cho việc tính toán khả thi trong khi xấp xỉ exp một cách hợp lý. Thứ hai, như một tính chất chung của đa thức, xấp xỉ Taylor chỉ theo dõi hàm gốc của nó với lỗi thấp trong các chế độ bị chặn.

Thiết lập. Để kiểm tra xấp xỉ Taylor, chúng tôi sử dụng xấp xỉ exp bậc hai, và đánh giá trên các thiết lập huấn luyện từ đầu và chuyển đổi tinh chỉnh trước đây (Mục 3.2). Chúng tôi thực hiện ánh xạ đặc trưng như exp(q^T k) ≈ φ_taylor(q)^T φ_taylor(k), trong đó φ_taylor(x) chiếu một query hoặc key d-chiều đến đặc trưng O(d²)-chiều φ_taylor(x) = [1, x_1, ..., x_d] ∪ [x_i·x_j | i, j∈[d]].

Kết quả tích cực. Chúng tôi thấy rằng xấp xỉ Taylor bậc 2 giữ lại cả tính chất nhọn và đơn điệu (Hình 5), và điều này tương ứng với hiệu suất khớp (gần) với attention softmax (Bảng 2). Chúng tôi cũng lưu ý rằng ở đây, tích vô hướng query-key BERT bị chặn trong các chế độ nơi xấp xỉ chuỗi Taylor bậc hai exp duy trì tính đơn điệu (Hình 5). Điều này gợi ý chúng ta có thể cho phép attention tuyến tính biểu cảm cho huấn luyện từ đầu và chuyển đổi tinh chỉnh.

Lưu ý. Thật không may, xấp xỉ Taylor bậc 2 không hiệu quả. Ngay cả với p = 2, chiều ánh xạ đặc trưng bây giờ là d' = 1 + d + d², dẫn đến độ phức tạp attention O(nd³). Như được tóm tắt trong Bảng 2, điều này giới thiệu một sự đánh đổi hiệu quả-hiệu lực giữa các xấp xỉ attention chức năng. Do đó, câu hỏi vẫn là liệu chúng ta có thể khôi phục khả năng biểu cảm và chất lượng mô hình hóa của softmax trong khi đạt được quy mô O(nd²) tương tự của các attention tuyến tính trong quá khứ.

[THIS IS TABLE: Bảng 2 showing comparison of different methods with columns for Method, Complexity, Spiky?, Monotonic?, Train-from-scratch (acc), and BERT-FT (MC)]

Hình 5: Xấp xỉ Taylor khôi phục độ nhọn và tính đơn điệu

6

--- TRANG 7 ---
4.2 Attention Tuyến tính Có thể Học để Mô phỏng Softmax
Insight chính của chúng tôi là thay vì dựa vào dạng hàm cố định nắm bắt các tính chất nhọn và đơn điệu của chúng ta, chúng ta có thể học các ánh xạ đặc trưng attention tuyến tính làm như vậy. Đối với mỗi khối attention, chúng tôi đề xuất các ánh xạ đặc trưng như MLP một lớp có thể huấn luyện, tương tự như công trình trước đây (Kasai et al., 2021) và hoạt động tương tự như một adapter (Houlsby et al., 2019) được chèn sau các phép chiếu query và key trong các lớp attention Transformer (Hình 1). Tuy nhiên, không giống như công trình trước đây, chúng tôi huấn luyện rõ ràng những ánh xạ đặc trưng này sao cho các lớp attention mô phỏng các tính chất của attention softmax. Chúng tôi mô tả hai thành phần cốt lõi này dưới đây, và xác thực những lựa chọn thiết kế này trong Mục 5.1.

Ánh xạ đặc trưng MLP nhọn. Nhớ lại paradigm attention tuyến tính dựa trên kernel từ Mục 2, nơi một ánh xạ đặc trưng φ:R^d→R^{d'} được áp dụng cho cả query và key để tính toán đầu ra self-attention nhân quả sử dụng phương trình 2. Tuy nhiên, không giống như công trình trước đây gắn bó với một hàm được chỉ định trước như một ánh xạ đặc trưng, chúng tôi làm cho ánh xạ đặc trưng thành một MLP có thể huấn luyện. Cụ thể, đối với thiết lập attention đơn head, chúng tôi tính toán φ_mlp(q_i)^T φ_mlp(k_j) với một MLP một lớp đơn giản như φ_mlp(x) = Φ(W^T x + b) trong đó ma trận W∈R^{d×d'} và bias b∈R^{d'} được học, và Φ là một hàm kích hoạt. Để tạo ra độ nhọn, chúng tôi đặt Φ như hàm mũ theo từng phần tử được nghiên cứu trong Mục 3.2, dẫn đến

φ_mlp(x) = [exp(w_1^T x + b), ..., exp(w_d^T x + b)]  (3)

Loss distillation trọng số attention. Để học một xấp xỉ softmax, chúng tôi huấn luyện φ_mlp để tối thiểu hóa loss cross-entropy giữa trọng số attention tuyến tính được tính toán và những cái được tính toán thông qua attention softmax. Đối với query q_i và key {k_j}_1^n, chúng tôi tính toán các loss mẫu như

L_i = -Σ_{j=1}^i [exp(q_i^T k_j) / Σ_{m=1}^i exp(q_i^T k_m)] log [φ_mlp(q_i)^T φ_mlp(k_j) / Σ_{m=1}^i φ_mlp(q_i)^T φ_mlp(k_j)]  (4)

Để huấn luyện attention Hedgehog trong Transformer attention đa lớp và đa head, chúng tôi áp dụng một MLP riêng biệt cho mỗi head và mỗi lớp, và sử dụng cùng φ_mlp cho query và key. Chúng tôi bao gồm chi tiết thực hiện thêm và pseudocode trong Phụ lục A.

5 Thí nghiệm
Trong các thí nghiệm, chúng tôi đánh giá liệu Hedgehog có khôi phục khả năng biểu cảm attention softmax trong khi giữ lại hiệu quả attention tuyến tính (Mục 5.1), và điều này cải thiện chất lượng mô hình hóa như thế nào trong các chế độ huấn luyện từ đầu (Mục 5.2), chuyển đổi tinh chỉnh (Mục 5.3), và chuyển đổi được tiền huấn luyện (Mục 5.4).

5.1 Đánh giá Hedgehog về Khả năng Biểu cảm và Hiệu quả
Trước khi đánh giá Hedgehog trên các nhiệm vụ hạ nguồn, chúng tôi nhằm mục đích xác thực các lựa chọn thiết kế của Hedgehog cho hiệu quả và khả năng biểu cảm. Chúng tôi giải quyết: (1) Ánh xạ đặc trưng nhọn và loss distillation của Hedgehog có khôi phục các tính chất nhọn và đơn điệu của attention softmax trên các nhiệm vụ associative recall và BERT CoLA trước đây không? (2) Hedgehog có đạt được hiệu quả được cải thiện so với attention softmax không?

[THIS IS TABLE: Bảng 3 showing performance comparison with columns for Method, Complexity, AR, and BERT-FT]

Hình 6: Quy mô tuyến tính Hedgehog trong thời gian wall-clock (trái) và bộ nhớ (phải). Không giống như xấp xỉ Taylor, suy luận Hedgehog có được lợi ích thực tế so với FlashAttention.

7

--- TRANG 8 ---
Hình 7: So với các attention tuyến tính trước đây, các lớp Hedgehog được huấn luyện (thứ 2 từ trái) tạo ra trọng số attention theo dõi chặt chẽ softmax (trái), với độ trung thực lớn hơn với cả hai thành phần (so với Hình 8).

Hình 8: Trọng số attention Hedgehog ablated.

[THIS IS TABLE: Bảng 4 showing comparison of different methods (HH CoLA, HH WT-103, T2R-HH CoLA, etc.) across datasets (CoLA, MRPC, MNLI, QNLI) with numerical values]

Hình 9: Hedgehog được huấn luyện trên CoLA và WT-103 khôi phục attention softmax trên dữ liệu MRPC.

[THIS IS TABLE: Bảng 5 showing sequence length comparison for Hedgehog attention across different lengths (256, 1024, 2048, 4096)]

(3) Đối với chuyển đổi, các trọng số attention được học có thực sự khớp với những cái của attention softmax "ground-truth" không? Một khi được học, điều này có chuyển giao sang ngữ cảnh dài hơn và nhiệm vụ khác nhau không?

Khôi phục các tính chất nhọn và đơn điệu softmax. Chúng tôi kiểm tra Hedgehog trong cùng thiết lập huấn luyện từ đầu associative recall (AR) và chuyển đổi tinh chỉnh BERT trên CoLA trong Mục 3.2. Đối với huấn luyện từ đầu trên AR, chúng tôi không sử dụng loss distillation, và huấn luyện mô hình end-to-end với dự đoán token tiếp theo sau khi chèn các MLP có thể học. Trong Bảng 3, chúng tôi thấy rằng Hedgehog đạt được cả độ phức tạp thuận lợi và mô hình hóa cho huấn luyện từ đầu và chuyển đổi tinh chỉnh. Điều này tương ứng với các tính chất nhọn (Hình 2) và đơn điệu (Hình 3) được lưu ý trước đây.

Khôi phục hiệu quả attention tuyến tính. Tiếp theo chúng tôi thấy quy mô O(nd²) của Hedgehog trong tính toán và bộ nhớ có thể dẫn đến lợi ích hiệu quả thực tế. Chúng tôi benchmark suy luận trong thời gian wall-clock và sử dụng bộ nhớ cho một lớp attention với 12 head và chiều head = 64 trên chuỗi lên đến n = 32K token dài (Hình 6). Hedgehog đạt được suy luận nhanh hơn gần 6x và bộ nhớ tương tự với FlashAttention (Dao et al., 2022) (tuyến tính trong bộ nhớ nhưng bậc hai trong thời gian). Trong khi đó, xấp xỉ Taylor, mặc dù O(n), có bộ nhớ lớn hơn đáng kể và tốc độ chậm hơn do d bổ sung.

Khôi phục trọng số attention softmax. Tiếp theo chúng tôi nghiên cứu sự kết hợp của ánh xạ đặc trưng và loss distillation của Hedgehog để khớp với trọng số attention softmax. Ngoài việc khôi phục các tính chất nhọn và đơn điệu, việc học khớp chính xác với trọng số có thể đặc biệt hiệu quả để chuyển đổi hoặc "distill" Transformer bậc hai được tiền huấn luyện thành biến thể tuyến tính. Để đánh giá, chúng tôi trực quan hóa trọng số attention cho các attention tuyến tính khác nhau trong thiết lập BERT-FT CoLA của chúng tôi (Hình 7). Chúng tôi thấy Hedgehog khôi phục trọng số attention tuyến tính khớp với softmax với độ trung thực cao hơn nhiều.

Để hiểu thêm về đóng góp của (1) MLP nhọn và (2) loss distillation của Hedgehog trong Mục 4.2, chúng tôi trực quan hóa trọng số attention ablated bằng (1) sử dụng loss distillation với ánh xạ đặc trưng ReLU được sử dụng trong Transformer-to-RNN (T2R-HH) (Kasai et al. (2021)), và (2) sử dụng MLP chưa huấn luyện, thay thế trọng số có thể huấn luyện bằng hàm đồng nhất (HH No Train). Chúng tôi thấy rằng huấn luyện distillation là cần thiết để khôi phục trọng số attention, và MLP nhọn cũng hữu ích cho việc khớp attention (sau đó được hỗ trợ bởi chuyển đổi Transformer được cải thiện trong Mục 5.3).

8

--- TRANG 9 ---
[THIS IS TABLE: Bảng 6 showing training results on LRA with various models and their performance across different tasks]

[THIS IS TABLE: Bảng 7 showing training results on WikiText-103 with different models and their perplexity scores]

Khái quát hóa sang dữ liệu mới và ngữ cảnh dài hơn. Cuối cùng, chúng tôi điều tra tính tổng quát của các ánh xạ đặc trưng Hedgehog được học. Chúng tôi chỉ ra attention Hedgehog được học trên dữ liệu và độ dài ngữ cảnh cụ thể vẫn có thể khớp tốt hơn với trọng số attention softmax cho dữ liệu mới và độ dài chuỗi so với các attention tuyến tính trước đây. Chúng tôi distill attention cho mô hình BERT sử dụng mẫu CoLA hoặc WikiText-103 (WT-103), và báo cáo trọng số attention so với attention softmax trên ba nhiệm vụ GLUE khác: định tính (Hình 9) và định lượng thông qua phân kỳ KL w.r.t. trọng số softmax "ground-truth" (Bảng 4). Chúng tôi bao gồm các trực quan hóa và so sánh bổ sung Phụ lục 5.

Trong Bảng 9, chúng tôi tiếp tục chỉ ra rằng việc khớp attention Hedgehog vẫn nhất quán trên các ngữ cảnh dài hơn. Sau distillation trên mẫu CoLA, chúng tôi nối các mẫu CoLA thành chuỗi dài 256 đến 4096 token (lên đến 8x độ dài ngữ cảnh mặc định 512). Sau đó chúng tôi tính toán trọng số attention sử dụng softmax và ánh xạ đặc trưng Hedgehog được học, và thấy rằng phân kỳ KL của chúng vẫn nhất quán.

5.2 Học Mô hình hóa Chuỗi từ Đầu
Chúng tôi đánh giá Transformer Hedgehog được huấn luyện từ đầu trên các benchmark phân loại chuỗi LRA phổ biến và mô hình hóa ngôn ngữ WikiText-103. Đối với huấn luyện từ đầu, chúng tôi khởi tạo MLP như ma trận đồng nhất cho ánh xạ đặc trưng Hedgehog, và huấn luyện toàn bộ mô hình end-to-end với loss cụ thể nhiệm vụ. Chúng tôi thấy Hedgehog đạt được độ chính xác trung bình tốt nhất cho cả hai nhiệm vụ trong số các attention tuyến tính (Bảng 6, 7). Đối với LRA, trong khi các mô hình không-Transformer hiện là state-of-the-art (Gu et al., 2021), công trình của chúng tôi tập trung vào xấp xỉ attention, vì vậy chúng tôi so sánh với Transformer dưới bậc hai cạnh tranh. Chúng tôi áp dụng cùng thiết lập siêu tham số với benchmark chính thức (Tay et al., 2021). Trên WikiText-103, chúng tôi áp dụng thiết lập trong Fu et al. (2023), đánh giá Transformer decoder-only 125M kiểu GPT-2 trên perplexity trên 1024 token. Hedgehog thu hẹp đáng kể khoảng cách lên đến 6 PPL.

9

--- TRANG 10 ---
5.3 Chuyển đổi Tinh chỉnh Transformer Bậc hai sang Tuyến tính
Đối với chế độ chuyển đổi Transformer tinh chỉnh, chúng tôi đánh giá khôi phục hiệu suất cho mô hình BERT-base được tinh chỉnh trên GLUE, và mô hình ViT-B/16 được huấn luyện trên ImageNet-1K. Đối với cả hai thiết lập, đầu tiên chúng tôi hoán đổi attention và huấn luyện thông qua loss distillation của chúng tôi (Mục 4.2). Sau đó chúng tôi tinh chỉnh các mô hình BERT được chuyển đổi trên nhiệm vụ gốc của chúng như trong Transformer-to-RNN (T2R) (Kasai et al., 2021).

Đối với BERT, chúng tôi so sánh Hedgehog với T2R trong Bảng 8, và thấy rằng ngược lại, chuyển đổi Hedgehog khôi phục gần 100% hiệu suất attention softmax gốc. Để kiểm tra thêm ánh xạ đặc trưng và distillation attention của Hedgehog, chúng tôi cũng so sánh với một ablation huấn luyện ánh xạ đặc trưng T2R với loss distillation của chúng tôi (T2R-HH). Chúng tôi thấy rằng huấn luyện để mô phỏng attention softmax tăng cường hiệu suất của T2R, gợi ý rằng distillation trọng số attention có thể là một bước chung để cải thiện ánh xạ đặc trưng attention tuyến tính. Tuy nhiên, hàm mũ của Hedgehog vẫn dẫn đến hiệu suất vượt trội. Chúng tôi thấy kết quả tương tự cho ViT-B/16, gợi ý Hedgehog cũng có thể áp dụng cho các phương thức khác.

5.4 Chuyển đổi Được tiền huấn luyện cho Chuyển giao Nhiệm vụ Dưới bậc hai
Cuối cùng chúng tôi đánh giá Hedgehog để chuyển đổi Transformer được tiền huấn luyện thành Transformer tuyến tính. Chúng tôi xem xét hai thiết lập: (1) Để benchmark Hedgehog và chế độ chuyển đổi được tiền huấn luyện cho mô hình hóa chuỗi dưới bậc hai, chúng tôi sử dụng cùng đánh giá WT-103 trong Mục 5.2 để chuyển đổi GPT-2 125M-parameter. (2) Như một ứng dụng sớm cho Hedgehog trên mô hình lớn hơn, chúng tôi chuyển đổi Llama-2 7B (Touvron et al., 2023) trước khi tinh chỉnh với adapter tầm thấp (LoRA) (Hu et al., 2021) trên tóm tắt SAMSum (Gliwa et al., 2019). Chúng tôi bao gồm chi tiết huấn luyện thêm trong Phụ lục B.5.

Để đo lường trực tiếp nhất chất lượng chuyển đổi được tiền huấn luyện, cho cả hai thiết lập chúng tôi so sánh với T2R. Đối với GPT-2, chúng tôi thấy Hedgehog cả vượt trội hơn T2R, và tiếp tục vượt trội hơn mô hình chuỗi dưới bậc hai hiện đại như H3 (Fu et al., 2023) và Hyena (Poli et al., 2023) (Bảng 10). Mặc dù không so sánh trực tiếp do tiền huấn luyện, chúng tôi cũng so sánh với GPT-2 zero-shot và tinh chỉnh để tham khảo. Trong khi Hedgehog cách 1 PPL so với GPT-2 tinh chỉnh hoàn toàn bậc hai, nó cải thiện đáng kể so với zero-shot trong khi tuyến tính để huấn luyện. Cuối cùng chúng tôi áp dụng Hedgehog cho chuyển đổi Llama-2, nơi Hedgehog cho phép Llama attention tuyến tính huấn luyện thông qua LoRA (xem Phụ lục C.3 cho mẫu sinh).

[THIS IS TABLE: Bảng 8 showing finetuned-conversion evaluation results for BERT models on GLUE tasks]

[THIS IS TABLE: Bảng 9 showing accuracy results for different models]

[THIS IS TABLE: Bảng 10 showing pretrained-conversion results for GPT-2 models]

[THIS IS TABLE: Bảng 11 showing Llama-2 conversion results with ROUGE scores]

6 Kết luận
Chúng tôi trình bày Hedgehog, một attention tuyến tính có thể học để mô phỏng attention softmax. Điều này cho phép huấn luyện mô hình attention tuyến tính từ đầu và chuyển đổi Transformer hiện có thành biến thể attention tuyến tính. Để thúc đẩy Hedgehog chúng tôi nghiên cứu tại sao các attention tuyến tính trước đây kém hiệu suất so với attention softmax, và xác định hai tính chất bị thiếu: (1) khả năng nắm bắt ánh xạ attention entropy thấp hoặc nhọn và (2) đơn điệu với respect đến tích vô hướng query-key cơ bản. Chúng tôi thấy huấn luyện để khớp với attention softmax dẫn đến khôi phục nhiều tính chất biểu cảm của nó, và rằng Hedgehog dẫn đến hiệu suất cạnh tranh với attention dựa trên softmax trong chế độ huấn luyện từ đầu, chuyển đổi tinh chỉnh, và chuyển đổi được tiền huấn luyện.

10

[Due to length constraints, I'll continue with the remaining pages if needed. The text continues with acknowledgements, references, and appendices.]
