# 2409.04431.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2409.04431.pdf
# File size: 6617975 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Theory, Analysis, and Best Practices for
Sigmoid Self-Attention
Jason Ramapuram∗Federico Danieli∗Eeshan Dhekane∗Floris Weers∗
Dan Busbridge∗Pierre Ablin∗Tatiana Likhomanenko∗
Jagrit Digani Zijin Gu Amitis Shidani Russ Webb
Apple
{jramapuram, f_danieli, eeshan, floris_weers, dbusbridge,
p_ablin, antares, digani, zgu26, amitis_shidani, rwebb}@apple.com
Abstract
Attention is a key part of the transformer architecture. It is a sequence-to-sequence
mapping that transforms each sequence element into a weighted sum of values.
The weights are typically obtained as the softmax of dot products between keys and
queries. Recent work has explored alternatives to softmax attention in transformers,
such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention
and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove
that transformers with sigmoid attention are universal function approximators and
benefit from improved regularity compared to softmax attention. Through detailed
empirical analysis, we identify stabilization of large initial attention norms during
the early stages of training as a crucial factor for the successful training of models
with sigmoid attention, outperforming prior attempts. We also introduce FLASH -
SIGMOID , a hardware-aware and memory-efficient implementation of sigmoid
attention yielding a 17% inference kernel speed-up over FLASH ATTENTION 2on
H100 GPUs2. Experiments across language, vision, and speech show that properly
normalized sigmoid attention matches the strong performance of softmax attention
on a wide range of domains and scales, which previous attempts at sigmoid atten-
tion were unable to fully achieve. Our work unifies prior art and establishes best
practices for sigmoid attention as a drop-in softmax replacement in transformers.
1 Introduction
The success of modern machine learning can be largely attributed to the attention mechanism
(Bahdanau et al., 2015; Vaswani et al., 2017). Attention uses a sequence-to-sequence (seq-to-seq)
map to build context-aware token representations. Classically, attention relies on the softmax function
(SoftmaxAttn ) to recover token representations as data-dependent convex combinations of values.
Despite its widespread use and effectiveness, softmax in SoftmaxAttn is not without limitations.
For instance, the softmax function can sometimes lead to a concentration of attention on just a few
features (Yang et al., 2018; Ganea et al., 2019), potentially neglecting other informative aspects of
∗Primary contributor. For a detailed breakdown of author contributions see Appendix H.
2Code is available at https://github.com/apple/ml-sigmoid-attention .
Preprint. Under review.arXiv:2409.04431v2  [cs.LG]  22 Jan 2025

--- PAGE 2 ---
the input data. Moreover, applying SoftmaxAttn requires performing a row-wise reduction along
the length of the input sequence, which in the case of efficient attention kernels (Dao et al., 2022;
Dao, 2023), slows down computations. In this work, we relax this constraint by substituting the
row-wise softmax operation with an element-wise sigmoid nonlinearity. We highlight that the central
problem with naïve sigmoid attention ( SigmoidAttn ) is that of large initial attention norms and
propose solutions to alleviate it. Our contributions are as follows:
(1) We prove SigmoidAttn is a universal function approximator on seq-to-seq tasks (Sec. 3.1).
(2) We analyze SigmoidAttn ’s regularity and provide its worst-case Jacobian bound (Sec. 3.2).
(3)We extend FLASH ATTENTION 2(Dao et al., 2022; Dao, 2023) with the sigmoid kernel, reducing
kernel inference wall-clock time by up to 17% and real world inference by up to 8% (Sec. 4).
(4) We show that SigmoidAttn matches SoftmaxAttn in various tasks and domains (Sec. 5).
2 Sigmoid Attention
LetX∈Rn×dbe the input sequence of nvectors, where each vector has dimension d. We define
three learnable weight matrices Wq∈Rd×dqk,Wk∈Rd×dqk, andWv∈Rd×dv, which are used to
compute the queries Q∈Rn×dqk, keysK∈Rn×dqk, and values V∈Rn×dvas follows:
Q=XW q,K=XW k,andV=XW v. (1)
Self-attention (Bahdanau et al., 2015; Vaswani et al., 2017) can be compactly written as
SoftmaxAttn( X) = Softmax( QKT/p
dqk)V, (2)
where the Softmax function normalizes each row of the input matrix. We replace the Softmax with
SigmoidAttn( X) =σ(QKT/p
dqk)V,
withσ:u7→sigmoid( u+b):= (1 + e−(u+b))−1.(3)
Here, σis applied element-wise to the input matrix in (3). The activation function σhas a hyper-
parameter b∈R. In App. E, we discuss an intuitive way to choose the order-optimal bias term,
resulting in b=−log(n). This choice of ballows us to make sense of SigmoidAttn for any sequence
length. Indeed, letting (y1, . . . ,yn) = SigmoidAttn( X)be the output sequence, we have
yi=nX
j=1exp(⟨Wqxi,Wkxj⟩)
exp(⟨Wqxi,Wkxj⟩) +nWvxj− − − − − →
n→+∞Z
exp(⟨Wqxi,Wkx⟩)Wvxdµ(x),(4)
where µ=1
nPn
j=1δxjis the empirical measure corresponding to X. Notably, (4) still makes sense
in the infinite length limit, where the measure µis not a sum of Diracs. Wortsman et al. (2023a) do
not use a bias, and propose a n−1normalization for various attention activations, such as sigmoid and
ReLU, but leave the reason as an open question. Our variable bias has a similar effect in the large n
limit, and we posit that recovering a finite output limit as nincreases is the why it works in practice.
A multi-head version of (3) is obtained by combining the outputs of several SigmoidAttn , as follows:
[SigmoidAttn1(X), . . . , SigmoidAttnh(X)]Wo, (5)
for a learnable output weight matrix Wo∈Rhdv×d, where hdenotes the number of heads .
3 Theoretical Properties of Sigmoid Attention
We analyze SigmoidAttn , with two objectives: (1) showing that a transformer architecture remains a
universal function approximator when SigmoidAttn replaces SoftmaxAttn , and (2) recovering a
measure of regularity of SigmoidAttn by computing its Lipschitz constant.
2

--- PAGE 3 ---
3.1 Are Transformers with Sigmoid Attention Universal Approximators?
Yun et al. (2020) demonstrate that classical transformers can approximate continuous sequence-
to-sequence functions to arbitrary precision, a property known as the Universal Approximation
Property (UAP). UAP is highly desirable as it provides proof of an architecture’s generalizability
and representation capability. As SigmoidAttn modifies the transformer architecture, it is crucial to
theoretically guarantee that this modification does not impact the representation capability and that
UAP is retained. We provide this guarantee with the following theorem.
Theorem 3.1 (UAP for SigmoidAttn ).We denote with Th,dv,r
σ the class of transformer networks
obtainable by combining an arbitrary number of SigmoidAttn layers (each of hheads of dimension
dv) followed by FFN layers of hidden dimension r. For any given continuous, permutation-equivariant
function f: Ω⊂Rn×d→Rn×dwith compact support Ω, and for any arbitrarily small error ε,
there exists a transformer network g∈ T4,1,4
σ such that
Z
Ω∥f(X)−g(X)∥p
pdX
≤ε, for 1≤p <∞. (6)
Theorem 3.1 is the exact counterpart of (Yun et al., 2020, Thm. 2), which shows UAP for classical
transformers. Our proof largely follows the same path, an outline of the original proof provided
in App. C. Here, we present an overview of the main adaptations required to prove Thm. 3.1 for
SigmoidAttn , with further details in App. C.1 and C.2.
Sigmoid Attention layers can implement contextual mappings: A key step in proving Thm. 3.1 is
showing that, even with SigmoidAttn , a sequence of transformer blocks can implement a Contextual
Mapping (Yun et al., 2020, Def. 3.1). A contextual mapping characterizes a function that maps each
input sequence element to an output uniquely dependent on the whole sequence. This property allows
a transformer to capture and store global context within each token, even if each layer only performs
pairwise comparisons. Subsequent layers can then use this global information to map individual
tokens to the correct output, ultimately approximating any arbitrary sequence-to-sequence function.
In Yun et al. (2020), the contextual mapping is assembled by modifying individual transformer blocks:
each block is tuned to react to a specific input token. By stacking a sequence of these blocks, a
transformer can be turned into an accumulator, mapping a given input token sequence to a unique
global index. This outcome is achieved via a selective shift layer (Yun et al., 2020, App. B.5):
Ψ(X;b, b′)i,1:=max kXk,1−minkXk,1ifb <Xi,1< b′
0 otherwise ,(7)
and can be approximated using classic attention. Although SigmoidAttn cannot directly approxi-
mate (7), our accumulator definition relies on an equivalent selective shift operation:
Ψσ(X;b, b′)i,1:=(P
k:Xk,1>b′Xk,1ifb <Xi,1< b′
0 otherwise ,(8)
which can be approximated by SigmoidAttn (described in App. C.1). In App. C.2.4, we show that (8)
shares similar properties with (7), allowing us to use the original proof framework in Yun et al. (2020)
and demonstrate that UAP holds in our case as well.
Our proof is largely equivalent to that in Yun et al. (2020), with two relevant differences: to
approximate (8), we require SigmoidAttn with at least four heads and shifts included in both
query and key definitions. In contrast, SoftmaxAttn requires at least two heads to approximate (7),
with shifts only in the query definition. However, this is primarily a theoretical requirement for the
proof and does not affect performance. Notably, the total number of parameters required by both
architectures for the approximation follows the same tight scaling of Yun et al. (2020).
3.2 Regularity of Sigmoid Attention
As with any layer in a neural network, the regularity of SigmoidAttn is important to study, as it gives
insights into the robustness of the corresponding network and the ease of optimizing it. The most
standard way to quantify the regularity of a layer function ϕis to compute its Lipschitz constant over a
setX, that is a constant C >0such that for all X,Y∈ X, it holds ∥ϕ(X)−ϕ(Y)∥ ≤C∥X−Y∥,
3

--- PAGE 4 ---
where ∥ · ∥ is the standard Frobenius norm. The local Lipschitz constant is the spectral norm of the
Jacobian of ϕatX. The two are related: the Lipschitz constant of ϕoverXis the greatest local
Lipschitz constant for all X∈ X. We turn to the theorem giving the regularity of SigmoidAttn :
Theorem 3.2. Define A={⟨WqxiWkxj⟩|, i, j∈ {1, . . . , n }} ⊂Rthe set of attention weights,
and the scaled activation norms σ∞=n×supu∈A|σ(u)|andσ′
∞=n×supu∈A|σ′(u)|. Then,
the Jacobian of SigmoidAttn atX= (x1, . . . ,xn)has a spectral norm of at most:
∥Wv∥2 
σ∞+ 2σ′
∞∥WT
qWk∥2 
1
nnX
i=1∥xi∥2
2!!
. (9)
The proof is found in App. D. In SigmoidAttn , if we assume that the attention weights
⟨Wqxi,Wkxj⟩are all bounded by a constant µ— this is true, e.g., if the activations are bounded
— we get σ∞≤exp(µ)andσ′
∞≤exp(µ)thanks to the choice of b=−log(n). The bound in
Thm. 3.2 depends only on the average squared-norm of the input sequence xi, while classical results
for the study of attention all rely on the largest value of ∥xi∥2
2(Kim et al., 2021; Castin et al., 2023).
This is another consequence of the simplicity of sigmoid attention and is due to the removal of the
normalizing constant in SoftmaxAttn . Our result implies that if all xiare within a ball of radius R
then the Lipschitz constant of SigmoidAttn grows at most like R2, but it is stronger since we can
apply this to unbounded distributions xi; it matters only that the second moment is bounded. This
result contrasts sharply with the bounds obtained for SoftmaxAttn : Castin et al. (2023, Thm. 3.4.)
show that there exists a sequence X= (x1, . . . ,xn)with∥xi∥2≤Rfor all isuch that the spectral
norm of the Jacobian of Attn atXis at least cR2exp(cR2)for some constant c >0. On the other
hand, our bound scales in R2: this means that the local Lipschitz constant of SigmoidAttn is much
lower than the worst local Lipschitz constant of SoftmaxAttn . Note that this result does not inform
us of the practical average case Lipschitz constant, which is likely to be much lower for both Softmax
and Sigmoid attention. Upper bounds on the Lipschitz constant of SigmoidAttn are of particular
interest to study the dynamics of attention, as done, e.g., in (Geshkovski et al., 2024, 2023)
3.3 Computational Complexity of Sigmoid and Softmax.
Table 1: Forward floating operations per token per attention head. nctxanddheadare the context length
and head dimension respectively. ∆measures the compute difference between sigmoid and softmax.
caccounts for causal ( c= (nctx+ 1)/2nctx∼1/2), or standard ( c= 1) attention. Typical values
from the 1B LLM results are nctx= 2048 ,dhead= 64 . Sigmoid and softmax share the same number
of floating operations (softmax: max-subtraction (2), exponentiation, summation, division; Sigmoid:
bias-add, sign-flip, exponentiation, addition, division). Remaining differences are due implementation
details, and are subleading ( ∼1%) compared to other attention operations like computing attention
logitsL(shown below). This analysis precludes hardware aware improvements (Section 4).
L=QKTSoftmax ( L)σ(L+b) ∆
Expression 2c nctxdhead 5c nctx 5c nctx 0
4 F LASH SIGMOID : Hardware-Aware Implementation
Memory speed has not kept pace with recent gains in computation speed (Choquette, 2023; Jouppi
et al., 2017; Hannun et al., 2023). Consequently, attention computations on modern architectures have
been IO-bound by memory accesses (Ivanov et al., 2021). FLASH ATTENTION (Dao et al., 2022) and
FLASH ATTENTION 2(Dao, 2023) address these shortcomings by optimizing GPU memory hierarchy
utilization to accelerate attention computations. Motivated by the speed boost provided by these
approaches, we develop FLASH SIGMOID , a hardware-aware implementation of SigmoidAttn . Like
previous works, F LASH SIGMOID employs three core ideas:
Tiling: Divide and Conquer Approach to Attention: Similar to FLASH ATTENTION and
FLASH ATTENTION 2,FLASH SIGMOID processes input parts in parallel to compute attention outputs
in blocks, efficiently combining partial results to generate the final attention output.
Kernel Fusion: Like FLASH ATTENTION andFLASH ATTENTION 2,FLASH SIGMOID implements
the computational steps of both forward and backward passes of SigmoidAttn as single GPU kernels,
4

--- PAGE 5 ---
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
02505007501000125015001750Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)(a) Inference mode kernels on H100.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
0100020003000400050006000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal) (b) Training mode kernels on H100.
Figure 1: Average kernel speed-up for FLASH SIGMOID over FLASH ATTENTION 2for sequence
lengths 64–78k. Inference is 17.39% faster for self-attention and 18.76% for causal attention.
Training is 6.53% faster for self-attention and 9.46% for causal attention.
minimizing memory accesses and improving memory efficiency by avoiding materialization of
intermediate activations on High-Bandwidth Memory (HBM).
Activation Recomputation: The backward pass of sigmoid attention requires the sigmoid acti-
vation matrix, which, if materialized on GPU HBM, results in slower implementation and memory
inefficiencies. FLASH SIGMOID addresses this by retaining only query, key, and value tensors for
re-computation of the sigmoid activation matrix during the backward pass. Despite increased FLOPs,
this approach proves faster in wall-clock time as well as more memory-efficient than the alterantive
approach of materializing and retaining the attention matrix.
The forward and backward pass algorithms of FLASH SIGMOID can be found in App. F.1. Here, we
highlight key differences between FLASH SIGMOID andFLASH ATTENTION /FLASH ATTENTION 2.
The point-wise nature of SigmoidAttn results in a faster and more memory-efficient implementation
by removing the need to compute the softmax normalization and materialize it to HBM. A reduction
in the number of kernel dispatches also speeds up FLASH SIGMOID . Further, FLASH SIGMOID does
not require accumulation and tracking of intermediate variables (row-sum and maximum of blocks)
in the forward and backward passes which saves computation cost and reduces register pressure. We
usesigmoid (x) = 0 .5·(1 + tanh(0.5·x))to optimize the sigmoid computation on GPU. The speed
up in FLASH SIGMOID compared to FLASH ATTENTION arises from optimizing hardware bottlenecks;
theoretically, SigmoidAttn is slower than SoftmaxAttn (Sec. 3.3).
To measure the performance improvements of FLASH SIGMOID , we compare the timings of the
kernels in its forward and backward passes against those of FLASH ATTENTION 2. The details of this
benchmarking on H100 and A100 GPUs can be found in App. F.2. Measuring GPU computation time,
we observe a 17.39% speed-up during inference and a 6.53% speed-up during training for attention
over randomly initialized data on H100 GPU (Fig. 1). In practice, these gains may be affected by
other bottlenecks, such as movement of tensors between CPU or GPU memory, computations in other
layers, and communication overhead in distributed training and inference. However, we demonstrate
thatFLASH SIGMOID speeds up training by ∼4%and inference by ∼8%in a realistic end-to-end
setup. The details of wall-clock time improvements with FLASH SIGMOID are in App. F.3. We also
note that practical machine learning workflows are dominated by inference rather than training.
5 Experiments
To empirically validate SigmoidAttn , we evaluate across several domains: supervised image classi-
fication using vision transformers (Dosovitskiy et al., 2021), self-supervised image representation
learning with SimCLR (Chen et al., 2020; Zhai et al., 2023a), Bootstrap Your Own Latent (BYOL)
(Grill et al., 2020; Busbridge et al., 2023) and Masked AutoEncoders (MAE) (He et al., 2022)
as well as automatic speech recognition (ASR) (Synnaeve et al., 2020; Gulati et al., 2020b) and
auto-regressive language modeling (LM) (Brown et al., 2020). We also validate sequence length
generalization on TED-LIUM v3 (Hernandez et al., 2018) for ASR and in small scale synthetic exper-
5

--- PAGE 6 ---
0 200 400 600
Epoch-0.00.10.30.50.7Train LossBYOL ViT-B/16
0 100 200 300
Epoch0.82.84.86.88.8SimCLR ViT-B/16
0 200 400
Epoch0.40.50.60.80.9MAE ViT-L/16
0 100 200 300
Epoch2.33.54.75.97.1Train LossSupervised ViT-B/16
100000 200000 300000
Steps23.032.241.550.860.0255M ASR Transformer
0 100000 200000 300000
Steps1.61.92.22.52.8
1B
7B H-Norm1B & 7B LLM
Sigmoid
SoftmaxFigure 2: Train losses comparing SigmoidAttn withSoftmaxAttn .
iments in App. G.5.4. Across all these domains and algorithms, we demonstrate that SigmoidAttn
matches the performance of SoftmaxAttn (Fig. 2 and 22), while offering training and inference
speed-ups as highlighted in Sec. 4. Empirically we make the following observations:
(1)SigmoidAttn is effective for vision tasks without a bias (except MAE), but relies on Layer-
Scale (Touvron et al., 2021) to match the performance of the baseline SoftmaxAttn (Fig. 10-a)
in a hyper-parameter free manner.3All results presented for SoftmaxAttn also fairly add
LayerScale unless specified.
(2)LM and ASR are sensitive to the initial norm ||σ(QKT/p
dqk)V||. Modulation is required
via (a) relative positional embeddings like ALiBi (Press et al., 2022), which reduces the initial
attention norm by shifting logit mass near zero under SigmoidAttn , (b) appropriate initialization
ofbto achieve the same effect – enabling usage of any positional embedding, (c) using hybrid-
norm (Appendices G.3.3, G.4 and G.6) at the expense of an extra normalization layer.
5.1 Ablations
We begin with ablations to dissect the benefits of each of our introduced components. To gain intuition
about SigmoidAttn , we developed a research-friendly auto-regressive (AR) LM training framework
to measure all components of attention and validate the effects of LayerScale, LayerNorm applied to
Q and K (QK norm), different positional embedding techniques, and initialization values for b.
Mitigating Large Attention Norms We train a single layer AR transformer block (E=3072,
D_FF=12288) on the realnews split of C4 (Raffel et al., 2020). We train for 216steps using a batch
size of 6 and max sequence length of 4096 using a single cycle cosine learning rate (LR) schedule
without weight decay. SigmoidAttn initially underperformed SoftmaxAttn when using absolute
sinusoidal (SinCos) (Fig. 3) or relative (Fig. 4) positional embeddings (PE), which we attribute
to high initial attention Frobenius norms, ∥σ(QKT/√
d)V∥. A corresponding evolution of the
attention distribution and sparsity can be seen in Appendix Fig. 32 and Fig. 33 on a synthetic task. To
address these larger attention norms, we propose: (a) using ALiBi (Press et al., 2022) whose relative
bias moves initial attention logit mass to the zero region under the sigmoid activation, producing
3Appendix G.2.2 demonstrates that supervised vision tasks using SigmoidAttn without LayerScale can
match baseline SoftmaxAttn performance by relying on learnable scalar bias and temperature: {b, t} ∈R.
6

--- PAGE 7 ---
Figure 3: SigmoidAttn with SinCos.
 Figure 4: SigmoidAttn with RoPE.
Figure 5: SigmoidAttn with ALiBi.
 Figure 6: SigmoidAttn with RoPE, b=−10.
equivalent train negative log-likelihoods (Fig. 5); or (b) set the attention logit bias bto a negative
offset proportional to the sequence length, b∝ −lnn(see App. G.1.2 for an ablation on b). This
enables the usage of other PE techniques like RoPE (Su et al., 2024) (Fig. 6).
Figure 7: Regularity analysis comparing SigmoidAttn vs.SoftmaxAttn (10×trials per n).
SoftmaxAttn theoretical bound is off scale and thus omitted.
Empirical Analysis of Attention Regularity To validate our theoretical analysis (Section 3.2), we
measure Jacobian norms of SigmoidAttn andSoftmaxAttn across sequence lengths (Figure 7). Us-
ing autograd, we compute exact Jacobian norms for both mechanisms, with and without HybridNorm
(Appendices G.3.3 and G.6), comparing them to theoretical bounds ( SoftmaxAttn bound omitted
as it exceeds scale). Both variants show empirical norms (solid lines) well below their theoretical
7

--- PAGE 8 ---
bounds (dashed lines). With our proposed bias initialization ( b=−ln(n)),SigmoidAttn achieves
lower norms than SoftmaxAttn in both settings, suggesting improved regularity. This aligns with its
strong task performance (Section 5). Additionally, HybridNorm (Figure 7, right) reduces norms for
both mechanisms compared to baseline (left), highlighting normalization’s role in attention stability
at longer sequences.
LayerScale To validate the need for LayerScale, we follow Wortsman et al. (2023b) to quantify the
impact on stability. All models are trained with RoPE with b∝ −lnn, using AdamW (Loshchilov &
Hutter, 2017) on the realnews split of C4 with (β1, β2) = (0 .9,0.95),ϵ= 10−8,wd= 0, batch size
24, maximum token sequence length of 512 from the T5 tokenizer (Raffel et al., 2020), cosine LR
schedule of 214steps including a linear warmup of 210steps. Models have nheads=κ,nlayers= 2×κ,
dmodel= 64×κanddfeed-forward = 256 ×κfor a scaling value κ∈ {1,2,4,8,16}leading to models
with{2.2,4.9,15.0,67.0,440.0}Mtrainable non-embedding parameters. Following Wortsman et al.
(2023b), we sweep learning rates η∈ {3×10−4,1×10−3,3×10−3,1×10−2,3×10−2,1×
10−1,3×10−1}. LR sensitivity is defined as Eη∈[a,b][min( ℓ(A(η)), ℓ0)−ℓ∗]where ℓ(A(η))is the
loss achieved by the learning algorithm Awith LR η,ℓ0is the loss at initialization, and ℓ∗is the loss
achieved by the best LR. LayerScale is initialized at 10−4. Unlike vision tasks, where LayerScale
improves performance (Fig. 10-a), in LM, we observe that SoftmaxAttn slightly benefits from
LayerScale, while the performance of SigmoidAttn remains largely unaffected.
103
102
101
Learning rate3.03.54.04.55.05.56.06.57.07.5Final loss
use_layer_scale=False
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
103
102
101
Learning rate
use_layer_scale=True
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
107108
Non-embed Params (M)101
LR sensitivity
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
107108
Non-embed Params (M)
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
Figure 8: LR sensitivity LayerScale ablation.
103
102
101
Learning rate3.03.54.04.55.05.56.06.57.07.5Final loss
qk_norm=False
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
103
102
101
Learning rate
qk_norm=True
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
107108
Non-embed Params (M)101
100LR sensitivity
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
107108
Non-embed Params (M)
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax Figure 9: LR sensitivity QK norm ablation.
50 100 150 200 250 300
Epoch
(a)65707580Test Top-1%+LayerScale,+QKNorm
+LayerScale,-QKNorm
-LayerScale,+QKNorm
n−a,-LayerScale,+QKNorm
n−a,-LayerScale,-QKNorm
50 100 150 200 250 300
Epoch
(b)MHA Softmax
MHA Sigmoid
MQA Softmax
MQA Sigmoid
50 100 150 200 250 300
Epoch
(c)Softmax, +LayerScale,+QKNorm
Sigmoid, +LayerScale,+QKNorm
TanH, +LayerScale,+QKNorm
ReLU, +LayerScale,+QKNorm
GeLU, +LayerScale,+QKNorm
ReLU2, +LayerScale, +QKNorm
ReLU2, -LayerScale, -QKNorm
Figure 10: ImageNet1k ViT-B/16 classification. (a) SigmoidAttn is robust without QK norm (+Lay-
erScale, -QKNorm). Removing LayerScale reduces accuracy by 1.0% (-LayerScale, +/-QKNorm).
n−αnormalization (Wortsman et al., 2023a) underperforms without LayerScale. (b) SigmoidAttn
multi-query attention (MQA) (Shazeer, 2019) with one head matches multi-head attention (MHA).
(c) Sigmoid with LayerScale and QK norm performs comparably to other activations, except TanH.
ReLU2(Hua et al., 2022) underperforms without LayerScale and QK norm.
Stability with QK Norm To explore the stability of SoftmaxAttn vs.SigmoidAttn we repeat the
analysis of Wortsman et al. (2023b), as described in the LayerScale analysis, to investigate the impact
8

--- PAGE 9 ---
of QK norm (Dehghani et al., 2023). For language modeling, both SigmoidAttn andSoftmaxAttn
exhibit sensitivity to learning rate changes without QK norm. However, incorporating QK norm
significantly stabilizes performance (Fig. 9). In vision tasks, SigmoidAttn demonstrates robustness
with and without QK norm (Fig. 10-a) and without the need for n−αnormalization from Wortsman
et al. (2023a).4
Multi-query attention (MQA) In Fig. 10-b we explore MQA (Shazeer, 2019) for vision using
only one head for {K,V}. We find that both SigmoidAttn andSoftmaxAttn perform equally well
with or without multiple heads even at the small scale of ViT-B/16.
Activation Function Ablations As in Wortsman et al. (2023a), various activation functions,
when combined with LayerScale and QK norm, perform equally well for vision tasks (Fig. 10-c).
However, for sequence-critical tasks like ASR, activation functions such as ReLU pose instabilities
and underperform. In the same figure, we also compare to the ReLU2proposal from Hua et al. (2022)
and find that it underperforms without LayerScale and QK norm.
5.2 Supervised Image Classification
Vision transformers (Dosovitskiy et al., 2021) extend transformers (Vaswani et al., 2017) to treat
K×Kimage grids as disparate tokens. All tokens are refined through sequential layers of self-
attention, pooled using a CLS token or global average pooling layer, and optimized using the negative
log likelihood, lnp(y|x). We train ViT-B/16 models using R224×224×3images for 300 epochs
using the recipe provided in App. G.2.4. We use the same set of training hyper-parameters for both
SoftmaxAttn andSigmoidAttn , changing only the activation function between trials. The train
negative log-likelihood is reported in Fig. 2 and the test top-1% is reported in Fig. 22. We find that
SigmoidAttn matches both the training dynamics and the evaluation performance of SoftmaxAttn .
5.3 Self-Supervised Image Representation Learning
Self-supervised representation learning (SSL) exploits vast quantities of unlabeled data to learn
semantic representations based on inductive biases such as augmentation invariance (SimCLR Chen
et al. (2020), BYOL (Grill et al., 2020)) or reconstruction from compressed representations (MAE (He
et al., 2022)). We employ vision transformer training recipes from Zhai et al. (2023a) and Busbridge
et al. (2023) (App. G.2.4) for SimCLR and BYOL. As with supervised learning, we use the same set
of training hyper-parameters for both SoftmaxAttn andSigmoidAttn , changing only the activation
function between trials. Figure 2 reports the train losses, and Fig. 22 highlights the linear probe
and finetuned test top-1%. Despite the diverse training objectives in SSL, SigmoidAttn matches
SoftmaxAttn while improving training and inference throughput (Sec. 4).
5.4 Automatic Speech Recognition (ASR)
We benchmark ASR using LibriSpeech data (Panayotov et al., 2015) on 100h and 960h settings
of paired speech and text transcriptions. Our PyTorch implementations of encoder-based vanilla
transformer (Synnaeve et al., 2020) and conformer (Gulati et al., 2020a) are trained with Connectionist
Temporal Classification (CTC) (Graves et al., 2006) w/ BF16 mixed precision, w/o QK norm and w/o
LayerScale. After extensively tuning SoftmaxAttn baselines, we switch to SigmoidAttn per (3)
without any other changes. We investigate the effects of post/pre-LayerNorm, model depth, optimizer
type, small data regime, and connection to local attention, with details in App. G.4.
Our main findings are: i) CAPE (Likhomanenko et al., 2021) PE is the most unstable for
SigmoidAttn ; ii) post-LayerNorm models with SoftmaxAttn are hard to match with stable
SigmoidAttn ; iii) w/o QK norm SigmoidAttn is unstable and significant spikes happen in both
gradient norms and training loss; iv) LayerScale is needed for generalization; v) learnable bias
b=−10gives no loss and gradient norms spikes while matching the SoftmaxAttn (which does not
benefit from the improved throughput of FLASH SIGMOID ); vi) adding a learnable bias, b=−5, toQ
instead of the attention logits also solves the initial large attention norms for CAPE and ALiBi but
not for RoPE; vii) b=−logngives rare (2-5 times) marginal gradient norms spikes with smooth
loss while matching SoftmaxAttn .
4We ablate multiplicative sequence length scaling in more detail in App. G.1.1.
9

--- PAGE 10 ---
Table 2: Word error rate (%) on LibriSpeech test sets and TED-LIUM v3 (Hernandez et al., 2018)
(“TED”, joint validation and test sets split according to duration) for transformer (255M params) with
either SoftmaxAttn orSigmoidAttn (LayerScale and QK norm are used with b=−logn) trained
on LibriSpeech 960h data (mean duration is 10-15s). Hyper-parameters are in App. G.4.
ATTN PE TEST -CLEAN TEST -OTHER TED 0-10 S TED 10-20 S TED 20-30 S TED 30S+
SOFTMAX
CAPE2.3 5.7 12.4 10.5 11.9 9.1
SIGMOID 2.4 5.5 12.4 10.3 12.3 9.7
- QK NORM UNSTABLE ,GRADIENT NORM AND LOSS SPIKES
- LAYER SCALE 2.5 6.1 13.6 11.5 13.4 8.9
SIGMOID (b=−10,LEARNABLE ) 2.3 5.5 12.1 10.5 13.0 9.3
SIGMOID (b=−5INQ,LEARNABLE ) 2.3 5.4 12.2 10.8 12.4 9.9
- QK NORM UNSTABLE ,GRADIENT NORM AND LOSS SPIKES
SOFTMAX
ROPE2.2 5.5 12.7 10.6 12.8 9.5
SIGMOID 2.3 5.4 12.3 10.1 12.3 8.6
SIGMOID (b=−10,LEARNABLE ) 2.2 5.2 12.4 10.5 12.3 21.8
+α= 1 2.7 6.6 14.1 12.0 14.5 14.9
SIGMOID (b=−5INQ,LEARNABLE ) UNSTABLE ,GRADIENT NORM AND LOSS SPIKES
SOFTMAX
ALIBI2.2 5.4 12.3 10.7 12.1 8.6
SIGMOID 2.3 5.1 12.3 10.5 12.6 9.1
SIGMOID (b=−10,LEARNABLE ) 2.2 5.2 12.4 10.4 11.7 9.1
+α= 1 2.6 6.6 13.9 11.9 14.2 8.6
SIGMOID (b=−5INQ,LEARNABLE ) 2.2 5.2 12.1 10.4 12.0 8.2
Table 2 shows the main result for pre-LayerNorm transformers with CAPE, RoPE, and ALiBi, where
SigmoidAttn uses LayerScale, QK norm, b=−logn, and no sequence normalization. The bias is
ablated with learnable bias (one per layer) in attention or Qwith or without sequence normalization.
SigmoidAttn is stabilized with bias while matching SoftmaxAttn , and b=−lognworks well. In
most cases, bias allows generalization to longer sequences without sequence normalization, except
for RoPE where it helps for longer sequences but hurts overall performance.
5.5 Autoregressive Large Language Modeling
Table 3: LLM English evaluation. All models use ALiBi. Detailed ablations in Appendix G.3.3.
MODEL SIZESEQ.
LEN.ARC
EASYARC
CHAL.HELLA -
SWAGPIQA SCIQWINO-
GRANDELAMBADA
OPENAITRIVIA QA
(1-SHOT )WEBQS
(1-SHOT )AVGSTEP
TIME (S)
SOFTMAX 1B 2 K 62.2 26.8 42.4 59.0 72.3 88.1 58.4 19.9 15.4 49.4 0.38
SIGMOID 1B 2 K 62.8 28.8 42.5 59.7 70.3 88.6 59.7 19.1 13.8 49.5 0.34
SOFTMAX 1B 4 K 62.6 27.7 42.4 58.6 71.1 88.2 58.6 18.9 14.7 49.2 0.84
SIGMOID 1B 4 K 60.5 27.3 41.3 57.8 70.5 87.0 57.6 18.9 12.6 48.2 0.67
SOFT (H-N ORM ) 1B 4 K 61.7 26.8 43.4 59.4 70.6 88.6 60.8 20.5 12.9 49.4 -
SIGM. (H-N ORM ) 1B 4 K 63.5 28.1 43.5 60.7 70.8 88.9 59.0 20.9 16.0 50.2 -
SOFT (H-N ORM ) 7B 4 K 71.2 39.9 53.2 65.5 75.6 91.8 67.2 37.7 21.8 59.0 3.85
SIGM. (H-N ORM ) 7B 4 K 72.7 40.5 53.5 66.2 76.0 92.5 66.5 39.5 21.8 59.6 3.4
We train all models using the Llama2 recipe (Touvron et al., 2023) (with ALiBi instead of RoPE) and
the RedPajama (Computer, 2023) dataset in JAX without FLASH ATTENTION using the AXLearn
framework5(App. G.3 for detailed hyper-parameters). Initial experiments at 85M parameters estab-
lished basic stability requirements (Fig. 27), with attention bias b=−log(n)(n = 4096) providing
effective results. At 1B n = 2048 scale with b=−log(n),SigmoidAttn matches the train NLL
(Fig. 2) and evaluation results of SoftmaxAttn (Tab. 3 top row) while improving throughput by
1.12×.
At the 1B n = 4096 scale, using just b=−log(n)we observe a 1.25×speedup; however, slight
instabilities prevent SigmoidAttn from matching the strong performance of SoftmaxAttn (Tab. 3
second row). We address these issues through hybrid-norm, an extra normalization layer applied on
the output of the attention operation ( x+norm(σ(QKT/p
dqk)V, more details in App. G.3 and G.6).
With hybrid-norm, SigmoidAttn matches the train NLL and slightly outperforms SoftmaxAttn on
English evaluation results (50.2% vs. 49.4% – Tab. 3 third row). In App. G.3.3 we ablate various
5https://github.com/apple/axlearn
10

--- PAGE 11 ---
design choices at 1B scale, including norm structures, position embedding techniques, and attention
bias configurations.
At 7B n = 4096, SigmoidAttn with hybrid-norm demonstrates compelling advantages compared to
SoftmaxAttn with hybrid-norm: it matches the train NLL of SoftmaxAttn (Fig. 2), while delivering
a1.13×speedup (Tab. 3 bottom row). The model shows marginal improvements on challenging tasks,
including both reasoning (ARC-Challenge: 40.5% vs 39.9%) and knowledge retrieval (TriviaQA:
39.5% vs 37.7%), with better average performance across all benchmarks (59.6% vs 59.0%). These
results establish SigmoidAttn with hybrid-norm as an efficient alternative for large-scale language
modeling.
6 Related Work
Recent studies in supervised image classification (Wightman et al., 2021) and self-supervised learning
(SSL), including approaches like SigLIP (Zhai et al., 2023b), are shifting large-scale machine learning
training from output conditional categorical distributions, traditionally parameterized by softmax
functions, to richer pointwise Bernoulli conditionals parameterized by sigmoid functions. In this
study, our focus shifts to refining the model’s internal mechanics, specifically by substituting the
softmax component of the attention mechanism with a pointwise sigmoid function.
Previous work has explored the replacing softmax with the ReLU activation in both practical (Shen
et al., 2023; Hron et al., 2020) and theoretical settings (Bai et al., 2023; Fu et al., 2023). Other
works explores using the ReLU2activation (Hua et al., 2022), exploring purely linear attention
(Katharopoulos et al., 2020; Lu et al., 2021; Koohpayegani & Pirsiavash, 2024) or cosine-similarity
based attention (Luo et al., 2018; Liu et al., 2022). Our work builds upon these explorations,
particularly Wortsman et al. (2023a), which replaces softmax with various activation functions scaled
byn−α, where ncorresponds to the sequence length and α, a hyper-parameter. However, we find
that their formulation does not match expected performance without proper binitialization and the
use of LayerScale (Fig. 10-a, App. G.1.1).
7 Conclusion
In this work, we present a comprehensive theoretical and empirical study of sigmoid attention as an
alternative to softmax attention in transformers. We prove that transformers with sigmoid attention are
universal function approximators with improved regularity, and identify LayerScale and prevention
of large initial attention norms as key factors for successful training. We introduce FLASH SIGMOID ,
a memory-efficient variant providing a 17% inference kernel speed-up. Extensive experiments across
language, vision, and speech demonstrate that properly normalized sigmoid attention matches softmax
attention performance on various tasks and scales. Our findings establish sigmoid attention as a viable
alternative, unifying prior work and establishing best practices for its application in transformers.
8 Acknowledgements
We thank Zakaria Aldeneh, Samy Bengio, Navdeep Jaitly, David Koski, Pau Rodriguez Lopez, Hadi
Pouransari, and Skyler Seto for their helpful feedback and critical discussions throughout the process
of writing this paper; Okan Akalin, Hassan Babaie, Michael Brooks, Brian Gamp, Denise Hui,
Mubarak Seyed Ibrahim, Li Li, Rajat Phull, Evan Samanas, Guillaume Seguin, and the wider Apple
infrastructure team for assistance with developing and running scalable, fault tolerant code. Names
are in alphabetical order by last name within group.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1409.0473 .
11

--- PAGE 12 ---
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
Provable in-context learning with in-context algorithm selection. In Alice Oh, Tristan Nau-
mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in
Neural Information Processing Systems 36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 ,
2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
b2e63e36c57e153b9015fece2352a9f9-Abstract-Conference.html .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .
Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane,
Xavier Suau Cuadros, and Russell Webb. How to scale your EMA. In Alice Oh, Tristan
Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances
in Neural Information Processing Systems 36: Annual Conference on Neural Information
Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 ,
2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
e7681dd6fe16052433ab68cd1555bdc9-Abstract-Conference.html .
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. In 2021 IEEE/CVF
International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October
10-17, 2021 , pp. 9630–9640. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00951. URL https:
//doi.org/10.1109/ICCV48922.2021.00951 .
Valérie Castin, Pierre Ablin, and Gabriel Peyré. Understanding the regularity of self-attention with
optimal transport. arXiv preprint arXiv:2312.14820 , 2023.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for
contrastive learning of visual representations. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of
Machine Learning Research , pp. 1597–1607. PMLR, 2020. URL http://proceedings.
mlr.press/v119/chen20j.html .
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Mon-
treal, QC, Canada, October 10-17, 2021 , pp. 9620–9629. IEEE, 2021. doi: 10.1109/ICCV48922.
2021.00950. URL https://doi.org/10.1109/ICCV48922.2021.00950 .
Jack Choquette. NVIDIA hopper H100 GPU: scaling performance. IEEE Micro , 43(3):9–17,
2023. doi: 10.1109/MM.2023.3256796. URL https://doi.org/10.1109/MM.2023.
3256796 .
Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Krashinsky. NVIDIA
A100 tensor core GPU: performance and innovation. IEEE Micro , 41(2):29–35, 2021. doi: 10.
1109/MM.2021.3061394. URL https://doi.org/10.1109/MM.2021.3061394 .
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Ben-
jamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021 . OpenReview.net, 2021. URL https://openreview.net/forum?id=
Ua6zuk0WRH .
12

--- PAGE 13 ---
Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April
2023. URL https://github.com/togethercomputer/RedPajama-Data .
Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Prac-
tical automated data augmentation with a reduced search space. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-
tual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html .
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,
abs/2307.08691, 2023. doi: 10.48550/ARXIV .2307.08691. URL https://doi.org/10.
48550/arXiv.2307.08691 .
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention:
Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mo-
hamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural In-
formation Processing Systems 35: Annual Conference on Neural Information Processing
Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 ,
2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html .
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,
Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe
Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme
Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste,
Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jas-
mijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vascon-
celos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas
Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby.
Scaling vision transformers to 22 billion parameters. In Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International
Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , vol-
ume 202 of Proceedings of Machine Learning Research , pp. 7480–7512. PMLR, 2023. URL
https://proceedings.mlr.press/v202/dehghani23a.html .
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA , pp. 248–255. IEEE
Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. URL https://doi.org/10.
1109/CVPR.2009.5206848 .
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
In9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021 . OpenReview.net, 2021. URL https://openreview.net/forum?id=
YicbFdNTTy .
Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer
learn? A study through the random features lens. In Alice Oh, Tristan Naumann,
Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neu-
ral Information Processing Systems 36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 ,
2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
274db6bf1b01d8b4f07feaeb8c46f474-Abstract-Conference.html .
Octavian Ganea, Sylvain Gelly, Gary Bécigneul, and Aliaksei Severyn. Breaking the softmax
bottleneck via learnable monotonic pointwise non-linearities. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine
13

--- PAGE 14 ---
Learning Research , pp. 2073–2082. PMLR, 2019. URL http://proceedings.mlr.
press/v97/ganea19a.html .
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical perspec-
tive on transformers. arXiv preprint arXiv:2312.10794 , 2023.
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. The emergence of clusters
in self-attention dynamics. Advances in Neural Information Processing Systems , 36, 2024.
Google. Praxis. https://github.com/google/praxis , 2024.
Alex Graves, Santiago Fernández, Faustino J. Gomez, and Jürgen Schmidhuber. Connectionist
temporal classification: labelling unsegmented sequence data with recurrent neural networks. In
William W. Cohen and Andrew W. Moore (eds.), Machine Learning, Proceedings of the Twenty-
Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006 ,
volume 148 of ACM International Conference Proceeding Series , pp. 369–376. ACM, 2006. doi:
10.1145/1143844.1143891. URL https://doi.org/10.1145/1143844.1143891 .
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond,
Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Boot-
strap your own latent - A new approach to self-supervised learning. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-
tual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
f3ada80d5c4ee70142b17b8192b2958e-Abstract.html .
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,
Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-
augmented transformer for speech recognition. In Helen Meng, Bo Xu, and Thomas Fang
Zheng (eds.), Interspeech 2020, 21st Annual Conference of the International Speech Communi-
cation Association, Virtual Event, Shanghai, China, 25-29 October 2020 , pp. 5036–5040. ISCA,
2020a. doi: 10.21437/INTERSPEECH.2020-3015. URL https://doi.org/10.21437/
Interspeech.2020-3015 .
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for
speech recognition. In Proc. Interspeech , 2020b.
Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert. MLX: Efficient and flexi-
ble machine learning on apple silicon, 2023. URL https://github.com/ml-explore .
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked
autoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pp. 15979–15988.
IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10.1109/
CVPR52688.2022.01553 .
François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Esteve.
Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation.
InSpeech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany,
September 18–22, 2018, Proceedings 20 , pp. 198–208. Springer, 2018.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: NNGP
and NTK for deep attention networks. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of
Machine Learning Research , pp. 4376–4386. PMLR, 2020. URL http://proceedings.
mlr.press/v119/hron20a.html .
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V . Le. Transformer quality in linear time. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato
(eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,
14

--- PAGE 15 ---
Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pp. 9099–9117.
PMLR, 2022. URL https://proceedings.mlr.press/v162/hua22a.html .
Niall P. Hurley and Scott T. Rickard. Comparing measures of sparsity, 2009.
Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all
you need: A case study on optimizing transformers. In Alex Smola, Alex Dimakis, and Ion Stoica
(eds.), Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9, 2021 .
mlsys.org, 2021. URL https://proceedings.mlsys.org/paper/2021/hash/
c9e1074f5b3f9fc8ea15d152add07294-Abstract.html .
Charles H. Jones. Generalized hockey stick identities and n-dimensional blockwalking. 1994. URL
https://api.semanticscholar.org/CorpusID:2088017 .
Norman P. Jouppi, Cliff Young, Nishant Patil, David A. Patterson, Gaurav Agrawal, Raminder
Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin,
Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb,
Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard
Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,
Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy,
James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin,
Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi
Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda,
Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory
Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory
Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric
Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. In
Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA 2017,
Toronto, ON, Canada, June 24-28, 2017 , pp. 1–12. ACM, 2017. doi: 10.1145/3079856.3080246.
URLhttps://doi.org/10.1145/3079856.3080246 .
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119
ofProceedings of Machine Learning Research , pp. 5156–5165. PMLR, 2020. URL http:
//proceedings.mlr.press/v119/katharopoulos20a.html .
Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention. In
International Conference on Machine Learning , pp. 5562–5571. PMLR, 2021.
Soroush Abbasi Koohpayegani and Hamed Pirsiavash. Sima: Simple softmax-free attention for vision
transformers. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2024,
Waikoloa, HI, USA, January 3-8, 2024 , pp. 2595–2605. IEEE, 2024. doi: 10.1109/WACV57701.
2024.00259. URL https://doi.org/10.1109/WACV57701.2024.00259 .
Minchul Lee, Kijong Han, and Myeong Cheol Shin. Littlebird: Efficient faster & longer transformer
for question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing , pp. 5261–5277, 2022.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. ArXiv e-prints , pp.
arXiv–1607, 2016.
Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozh-
nikov. CAPE: encoding relative positions with continuous augmented positional embeddings. In
Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual ,
pp. 16079–16092, 2021. URL https://proceedings.neurips.cc/paper/2021/
hash/865bf46435bd84fa5d89f64cf3ba7347-Abstract.html .
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng
Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer V2: scaling up capacity and reso-
lution. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New
15

--- PAGE 16 ---
Orleans, LA, USA, June 18-24, 2022 , pp. 11999–12009. IEEE, 2022. doi: 10.1109/CVPR52688.
2022.01170. URL https://doi.org/10.1109/CVPR52688.2022.01170 .
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 , 2017.
Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao
Xiang, and Li Zhang. SOFT: softmax-free transformer with linear complexity. In Marc’Aurelio
Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan
(eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural
Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pp. 21297–
21309, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
b1d10e7bafa4421218a51b1e1f1b0ba2-Abstract.html .
Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui Ren, and Qiang Yang. Cosine normalization:
Using cosine similarity instead of dot product in neural networks. In Vera Kurková, Yannis
Manolopoulos, Barbara Hammer, Lazaros S. Iliadis, and Ilias Maglogiannis (eds.), Artificial Neural
Networks and Machine Learning - ICANN 2018 - 27th International Conference on Artificial Neural
Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I , volume 11139 of Lecture Notes
in Computer Science , pp. 382–391. Springer, 2018. doi: 10.1007/978-3-030-01418-6\_38. URL
https://doi.org/10.1007/978-3-030-01418-6_38 .
OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023. doi: 10.48550/ARXIV .2303.08774.
URLhttps://doi.org/10.48550/arXiv.2303.08774 .
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR
corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics,
Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-
24, 2015 , pp. 5206–5210. IEEE, 2015. doi: 10.1109/ICASSP.2015.7178964. URL https:
//doi.org/10.1109/ICASSP.2015.7178964 .
Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and
Quoc V . Le. Specaugment: A simple data augmentation method for automatic speech recognition.
In Gernot Kubin and Zdravko Kacic (eds.), Interspeech 2019, 20th Annual Conference of the
International Speech Communication Association, Graz, Austria, 15-19 September 2019 , pp. 2613–
2617. ISCA, 2019. doi: 10.21437/INTERSPEECH.2019-2680. URL https://doi.org/
10.21437/Interspeech.2019-2680 .
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Ed-
ward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pp. 8024–
8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
bdbca288fee7f92f2bfa9f7012727740-Abstract.html .
Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear bi-
ases enables input length extrapolation. In The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URL
https://openreview.net/forum?id=R8sQPpGCv0 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-
text transformer. J. Mach. Learn. Res. , 21:140:1–140:67, 2020. URL http://jmlr.org/
papers/v21/20-074.html .
Noam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR , abs/1911.02150,
2019. URL http://arxiv.org/abs/1911.02150 .
16

--- PAGE 17 ---
Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. A study on relu and
softmax in transformer. CoRR , abs/2302.06461, 2023. doi: 10.48550/ARXIV .2302.06461. URL
https://doi.org/10.48550/arXiv.2302.06461 .
Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Ro-
former: Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063,
2024. doi: 10.1016/J.NEUCOM.2023.127063. URL https://doi.org/10.1016/j.
neucom.2023.127063 .
Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel
Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. End-to-end ASR: from
supervised to semi-supervised learning with modern architectures. In ICML 2020 Workshop on
Self-supervision in Audio and Speech , 2020. URL https://openreview.net/forum?
id=OSVxDDc360z .
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going
deeper with image transformers. In Proceedings of the IEEE/CVF international conference on
computer vision , pp. 32–42, 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/
hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html .
Ross Wightman, Hugo Touvron, and Hervé Jégou. Resnet strikes back: An improved training
procedure in timm. CoRR , abs/2110.00476, 2021. URL https://arxiv.org/abs/2110.
00476 .
Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Replacing softmax with
ReLU in vision transformers. arXiv preprint arXiv:2309.08586 , 2023a.
Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D. Co-
Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein,
Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale
transformer training instabilities. CoRR , abs/2309.14322, 2023b. doi: 10.48550/ARXIV .2309.
14322. URL https://doi.org/10.48550/arXiv.2309.14322 .
xai-org. Grok-1. https://github.com/xai-org/grok-1 , 2024. Accessed: [Insert
Access Date].
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.
InInternational Conference on Machine Learning , pp. 10524–10533. PMLR, 2020.
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax
bottleneck: A high-rank RNN language model. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings . OpenReview.net, 2018. URL https://openreview.net/forum?id=
HkwZSG-CZ .
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are
transformers universal approximators of sequence-to-sequence functions?, 2020.
Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe
Zhang, Jiatao Gu, and Joshua M. Susskind. Stabilizing transformer training by preventing attention
entropy collapse. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
17

--- PAGE 18 ---
Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML
2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine Learning
Research , pp. 40770–40803. PMLR, 2023a. URL https://proceedings.mlr.press/
v202/zhai23a.html .
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language
image pre-training. CoRR , abs/2303.15343, 2023b. doi: 10.48550/ARXIV .2303.15343. URL
https://doi.org/10.48550/arXiv.2303.15343 .
Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In Advances in Neural
Information Processing Systems 32 , Vancouver, Canada, 2019. URL https://openreview.
net/references/pdf?id=S1qBAf6rr .
18

--- PAGE 19 ---
Appendices
A Limitations 21
B Broader Impact 21
C Universal Approximation Property for Sigmoid Attention 22
C.1 Proof of Step (3): Sigmoid Transformers can Approximate Modified Sigmoid Trans-
formers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 Proof of Step (2-b): Modified Sigmoid Transformers can Implement Contextual
Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.2.1 Basic Building Blocks of Contextual Mapping . . . . . . . . . . . . . . . 24
C.2.2 Result of Applying a Sequence of Selective Shifts . . . . . . . . . . . . . . 25
C.2.3 Result of Applying One Last Global Shift Layer . . . . . . . . . . . . . . 27
C.2.4 A Sequence of Selective Shifts Followed by a Global Shift Produces a Con-
textual Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D Lipschitzness of Sigmoid Attention 32
E The Bias Term of Sigmoid Attention 33
F Details of F LASH SIGMOID 35
F.1 Details of F LASH SIGMOID Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 35
F.2 Benchmarking of F LASH SIGMOID Kernels . . . . . . . . . . . . . . . . . . . . . 38
F.3 Speed Boosts of F LASH SIGMOID in Realistic Settings . . . . . . . . . . . . . . . 39
F.4 F LASH SIGMOID with ALiBi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
F.5 Directions for Future Work on F LASH SIGMOID . . . . . . . . . . . . . . . . . . . 42
G Experiments 43
G.1 Extra Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
G.1.1 The Effect of Multiplicative Sequence Length Normalization . . . . . . . . 43
G.1.2 Attention Bias Stability Ablation . . . . . . . . . . . . . . . . . . . . . . . 44
G.2 Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
G.2.1 Test ImageNet1k Top-1% . . . . . . . . . . . . . . . . . . . . . . . . . . 44
G.2.2 LayerScale Free Sigmoid Attention . . . . . . . . . . . . . . . . . . . . . 45
G.2.3 Sigmoid Attention vs. Attention Relaxations . . . . . . . . . . . . . . . . 45
G.2.4 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
G.3 Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
G.3.1 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
G.3.2 Gradient Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
G.3.3 Norm structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
G.4 Automatic Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
G.4.1 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
19

--- PAGE 20 ---
G.4.2 Results and Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
G.5 Simple Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
G.5.1 k–Summation Problem Definition . . . . . . . . . . . . . . . . . . . . . . 52
G.5.2 Comparison to Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
G.5.3 Attention Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
G.5.4 Pair Repeat Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
G.6 Practitioner’s Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
H Contributions 56
20

--- PAGE 21 ---
A Limitations
While our work demonstrates that SigmoidAttn can serve as a viable drop-in replacement for
SoftmaxAttn in many domains and scales, there are a few key limitations to note:
(1)In large-scale (1B parameter, 4096 context length) language modeling, we observed some
gradient norm spikes and a slight performance gap between SigmoidAttn andSoftmaxAttn
(Table 3). While runs at smaller context lengths (1B parameter, n=2048) were stable
and matched SoftmaxAttn performance, we required the use of hybrid-norm to stabilize
n=4096 sequence length (and the larger 7B models). Hybrid-norm does incur a slight extra
performance penalty which we quantify in Appendix G.6.
(2)Our theoretical analysis proves that transformers with SigmoidAttn are universal function
approximators and have improved regularity compared to SoftmaxAttn . However, the
bounds we derive, while tighter than those for SoftmaxAttn , may not be maximally tight.
There could be room for further theoretical refinements.
(3)We focused our empirical evaluation on standard benchmarks in language, vision, and speech
domains. Performance on more niche or emerging applications remains to be validated.
(4)In automatic speech recognition experiments, we observed that SigmoidAttn can be sen-
sitive to the choice of positional embeddings and may require careful initialization of the
attention bias term to ensure stable training. Specifically, we found that the CAPE positional
embedding was the most unstable for SigmoidAttn . Further work is needed to develop ro-
bust initialization schemes that work well across different positional embeddings. Moreover
we found that w/o QK norm or with post-LayerNorm SigmoidAttn is unstable and can
underperforms SoftmaxAttn , thus further investigation is needed.
(5)FLASH SIGMOID demonstrates promising inference and training speed-ups by exploiting
SigmoidAttn ’s simpler kernel structure compared to SoftmaxAttn . However, realizing
these gains at scale in distributed training setups may require additional engineering to
optimize communication bottlenecks.
Despite these limitations, we believe this work establishes a strong foundation for SigmoidAttn ,
unifying prior art and demonstrating its potential as a drop-in SoftmaxAttn replacement. We hope
our theoretical grounding and empirical results motivate further research into this simple yet effective
architectural variation.
B Broader Impact
The development of efficient and theoretically grounded attention mechanisms has the potential
for significant positive impact across a range of applications. By establishing SigmoidAttn as a
viable alternative to SoftmaxAttn , our work expands the toolkit of architectural choices available to
researchers and practitioners. Positive impacts of this work may include:
(1)Improved computational efficiency: FLASH SIGMOID ’s faster kernel implementation could
lead to more efficient training and inference for attention-based models, reducing energy con-
sumption and enabling deployment on resource-constrained devices. This could democratize
access to powerful models.
(2)Theoretical understanding: Our universal approximation results and tighter bounds on the
regularity of SigmoidAttn contribute to a deeper theoretical understanding of this key
component. A stronger theoretical foundation can guide principled model design and
architectural search.
(3)Application-specific benefits: Across language, vision, and speech domains, SigmoidAttn ’s
performance could translate into improved user experiences, such as more natural language
interactions, enhanced image understanding, and robust speech recognition. These advance-
ments could have positive societal impacts, such as improved accessibility tools and more
effective educational technologies.
However, as with any foundational machine learning advance, there are also risks of negative impacts
that must be considered and mitigated:
21

--- PAGE 22 ---
(1)Fairness and bias considerations: As with any machine learning model, it is important
to carefully evaluate SigmoidAttn based models for fairness and potential biases when
applied to sensitive use cases. The unique properties of SigmoidAttn may have unexpected
interactions with data biases. Researchers and practitioners should follow best practices for
auditing and mitigating unwanted biases to ensure equitable outcomes.
(2)Environmental impact: While FLASH SIGMOID is more computationally efficient than
FLASH ATTENTION , the overall trend of scaling up attention-based models has significant
energy costs. Further efficiency improvements and the use of renewable energy sources are
important to mitigate environmental harms.
We believe that the benefits of SigmoidAttn outweigh the risks, but it is crucial for the research
community to actively consider and address these potential negative impacts. By doing so, we can
work towards a future where the efficiency and expressivity of SigmoidAttn are used for societal
benefit.
C Universal Approximation Property for Sigmoid Attention
This section is dedicated to the proof for the Universal Approximation Property for attention equipped
with sigmoid nonlinearity. The proof follows closely the one provided in Yun et al. (2020, Sec. 3),
of which we inherit much of the notation, and we encourage the interested reader to refer to the
original source for a more comprehensive understanding of its details. Here we first provide context
by outlining the main steps in the original proof, before proceeding to adapt its key components to
theSigmoidAttn case.
The proof aims at showing that a transformer network can approximate to arbitrary accuracy any
continuous, permutation-equivariant function with compact support. The proof is constructive in
nature, in that it explicitly defines the architecture (and particularly, the sequence of self-attention
and feed-forward layers) that can approximate a given target function. To do so, it proceeds in steps
(see Yun et al. (2020, Sec. 3.2)):
(1)prove that any continuous function with compact support can be approximated to arbitrary
accuracy by a piecewise constant function
(2)prove that an aptly-constructed modified transformer network, (where the softmax nonlin-
earity is substituted with a hardmax nonlinearity), can exactly represent such piecewise
constant function. This step is further divided into three sub-steps (see Yun et al. (2020,
Sec. 4)):
(a)prove that a series of feed-forward layers can quantize any input to a specific discretiza-
tion grid in the compact domain
(b)prove that a series of self-attention layers can implement a contextual mapping (see
Yun et al. (2020, Def. 3.1))
(c)prove that a series of feed-forward layers can map the output of the contextual mapping
to the desired output of the target piecewise-constant approximation
(3)prove that a (classical) transformer network can approximate such modified transformer
network to arbitrary accuracy
Fortunately, some of the steps outlined above do not rely on a specific nonlinear function being used
within the attention mechanism, and can be directly reused in our proof, virtually unchanged. Notice
however that Steps (2-b) and (3) are directly impacted by modifications to the attention layer, and
hence require adaptation in our case. This is the focus of the next sections.
C.1 Proof of Step (3): Sigmoid Transformers can Approximate Modified Sigmoid
Transformers
In Yun et al. (2020), to implement contextual mappings, the authors rely on a modified version of
transformers, for the sake of simplifying the analysis. In their modified version, the (row-wise)
softmax operation is substituted with a (row-wise) hardmax operation. This substitution is valid
because a classical transformer can still be made arbitrarily close to such modified transformer, in
22

--- PAGE 23 ---
light of the fact that
softmax (λX)λ→∞− − − − → hardmax (X). (10)
In our proof, we follow a similar strategy to define our modified sigmoid transformer (and in particular,
its self-attention mechanism). We have that
σ(λX)λ→∞− − − − → H(X), (11)
where σ(x) = (1 + e−x)−1is the (elementwise) sigmoid function, while
H(x) =

1x >0
1
2x= 0
0x <0(12)
denotes the (elementwise) Heaviside step function. This allows us to define our modified sigmoid
self-attention layer, as follows.
Definition C.1 (Modified sigmoid self-attention layer) .Given an input X∈Rd×n, the action of
a modified sigmoid self-attention layer with shifts and a single one-dimensional head is defined as
X7→X+ψ(X;q,bq,k,bk,v,o), where
ψ(X;q,bq,k,bk,v,o) =o 
vTX
H 
qTX−bT
qT 
kTX−bT
k
(13)
withq,k,v∈Rdrepresenting the query, key, and value vectors, bq,bk∈Rnthe corresponding
query and key bias vectors, while o∈Rddenotes the output vector.
Analogously to (10), (11) guarantees that sigmoid attention can approximate modified sigmoid
attention by simply increasing the magnitude of its inner parameters.
Here and in the following, the length of the input sequence is denoted as n, while drepresents the
dimensionality of the tokens. Notice that we are considering the input tensor X∈Rd×n, (as opposed
to∈Rn×d) to better align out notation with the one used in Yun et al. (2020).
C.2 Proof of Step (2-b): Modified Sigmoid Transformers can Implement Contextual
Mappings
The core of the proof consists in showing how, by opportunely combining the operations in (13),
one can build an architecture capable of implementing a contextual mapping . For completeness, we
report next the definition of such a map (see also Yun et al. (2020, Def. 3.1)).
Definition C.2 (Contextual mapping) .A map q:L→Rnfrom a finite set L⊂Rd×nis said to be a
contextual mapping if both the following conditions hold:
(i)qi(X)̸=qj(X),∀i̸=jand∀X∈L
(ii)qi(X)̸=qj(X′),∀i, jand∀X,X′∈L, withX̸=X′
where qi(X)denotes the i-th component of q(X).
Namely, a contextual mapping is such that it transforms each token in an input sequence to a value
depending uniquely on the whole sequence. By satisfying this property, we can ensure that any
element of the quantization of the input domain (achieved by Step (2-a)) can be mapped to a unique
identifying value (depending on the whole input) via a sequence of modified sigmoid self-attention
layers. It is then up to the MLP (in Step (2-c)) to correctly map this value to the corresponding output
value in the piece-wise constant approximation.
In particular, after defining a uniform discretization (characterized by the parameter δ) of the unitary
hypercube [0,1]d⊂Rd, namely
Gδ:={g:gi∈ {0, δ,2δ, . . . , 1−δ},∀i= 1. . . d}, (14)
we consider as input a tensor X(composed of columns X= [xi]n
i=1) such that
X∈L:={X:xi∈Gδ∀i= 1. . . n, andxi̸=xj∀i̸=j} ⊂Rd×n, (15)
23

--- PAGE 24 ---
that is, a 2D tensor whose columns are element of the discretization Gδ, and that all differ from each
other (at least for one element). We want to build a contextual mapping acting on L, by stacking
layers parameterized according to Def. C.1. In App. C.2.1 we define the basic building blocks of our
architecture; in App. C.2.2 we describe how to stack them, and the effect the architecture has on a
given input; finally, in App. C.2.4 we prove that this architecture indeed implements a contextual
mapping.
C.2.1 Basic Building Blocks of Contextual Mapping
The strategy we follow to assemble a contextual mapping consists in sequentially looking at each
column of the input, progressively updating and storing information regarding its content in a uniquely
identifiable manner, and finally broadcasting this information back to every element in the sequence.
The difficulty lies in the fact that each of these updates must be carried on while relying solely on
applications of the modified SigmoidAttn layer in Def. C.1. In the following, we describe how we
can tweak its parameters to achieve exactly this.
From d-dimensional quantized vectors to scalars As a first simplification, we can get rid of the
d-dimension in the Xtensor by mapping each of its columns to a corresponding identifying scalar,
uniquely defined by the specific column components. This step is also performed in Yun et al. (2020,
App. B.5), and can be achieved rather straightforwardly, by defining
v≡q≡k≡u:= [1, δ−1, δ−2, . . . , δ−d+1]T. (16)
Notice in fact that, since each column xibelongs to Gδ, it can equivalently be written in the form
xi=δ·[id0,i,id1,i, . . . , idd−1,i]T, where idj,i∈ {0,1,2, . . . , δ−1−1}represents the (indexed)
coordinate of the discretization along the j-th dimension. Scalar-multiplying Xbyuin (16), then,
turns this tuple of indices into a single one, in a bijective fashion6.
This allows us to equivalently consider a single vector uTX∈Rn, rather than the whole tensor
X∈Rd×nin the remainder of our analysis. Analogously, choosing o≡e0:= [1,0, . . . , 0]Tin (13)
constraints the effect of the layer application to impact only the first row of the tensor: the goal is then
to store in this row the result of the target contextual mapping qin Def. C.2. To slim our notation, in
the following we often refer to uTXas the vector l∈Rn, with components li.
In light of the simplification above, we can rewrite (13) more compactly, as follows:
ψ(X;q=k=v≡u,o≡e0;bq,bk) =e0lTH((l−bq)⊗(l−bk)) (17)
Notice that, since the elements of both Xanduare always non-negative, so are those of l, too.
Moreover, since we are interested in permutation-equivariant functions with respect to the columns of
X, without loss of generality we can consider the elements of l=uTXto be ordered: 0≤li< lj,
∀i < j .
Selective shift operation for sigmoid attention Since we aim to recover a contextual map by
sequentially updating the elements of l, we proceed by designing a modification of (17) which affects
only a certain selected element at a time. This is were our second simplification comes into play,
and this time it pertains the roles of the bias vectors bqandbk. Since l≥0, these vectors have the
effect of tweaking the sign of the inner arguments of the Heaviside function in (17), hence directly
impacting when its application outputs 0or1. By aptly selecting the values of bkandbq, then, we
can explicitly decide when a specific layer triggers an update, which elements are affected by the
update, and what elements to consider to compute the update itself.
More in detail, take bq=1bqandbv=1bv, for some scalars bq, bv, and with 1being the all-one
vector. Plugging this into (17), we have
˜ψ(X;bq, bk):=ψ(X;q=k=v≡u,o≡e0,bq=1bq,bk=1bk)
=e0lTH((l−1bq)⊗(l−1bk)) =e0P
i:li<bvliiflj< bkP
i:li>bvliiflj> bk;(18)
6For example, consider d= 3and the column defined as xi= [3δ,10δ,2δ]T, that is, the column identified
by the triplet of indices [3,10,2]. Multiplying by uwould then give the scalar uTxi= (3 + 10 N+ 2N2)δ,
where N=δ−1, which is uniquely identified by the single index (3 + 10 N+ 2N2).
24

--- PAGE 25 ---
notice how bqdetermines what elements of lcompose the update (as it impacts the indices considered
in the sum), while bkdefines the elements impacted by the update itself7. If we opportunely
combine four modified sigmoid self-attention heads ˜ψ(X;bq, bk), we recover, for a given index
i= 0. . . δ−d−1,
Ψ(i)(X):=X+1
2c
˜ψ 
X;bq= 0, bk= 
i−1
2
δ
−˜ψ 
X;bq= 0, bk= 
i+1
2
δ
−˜ψ 
X;bq=bk= 
i+1
2
δ
+˜ψ 
X;bq= 
i+1
2
, bk= 
i−1
2
δ

=X+1
2ce0lT
H 
l⊗ 
l− 
i−1
2
δ
−H 
l⊗ 
l− 
i+1
2
δ
−H  
l− 
i+1
2
δ
⊗ 
l− 
i+1
2
δ
+H  
l− 
i+1
2
δ
⊗ 
l− 
i−1
2
δ

=⇒Ψ(i)
1,j(X) =X1,j+cP
k:lk>iδlkiflj=iδ
0 otherwise
=⇒Ψ(i)
k>1,j(X) =Xk,j,(22)
where c≡c(δ, d, n )is a multiplicative constant which will be chosen later.
The operator assembled in (22) defines the basic layer of the architecture that we use in our proof.
Notice Ψ(i)(X)has the effect of modifying only the column xjwhich has index lj=uTxj=iδ
(if at all present in the input X). This layer covers a similar role to the selective shift operation
introduced in Yun et al. (2020, App. B.5), but it has been adapted to account for the presence of a
sigmoid nonlinearity: notice this required us to use 4-headed attention, while in Yun et al. (2020) a
2-headed version is sufficient.
C.2.2 Result of Applying a Sequence of Selective Shifts
Ultimately we want to show how, by stacking a sequence of selective shift layers (22) for increasing
i= 0. . . δ−d−1and one additional global shift, we can build an architecture capable of representing
7This can be better seen by considering independently the effects of the two parameters bk,bqon the
modified sigmoid attention matrix H((l−1bq)⊗(l−1bk)). We have in fact, with bq= 0,
lj< bklj> bk
H(l⊗(l−1bk))=
0···01···1..................
0···01···1
.(19)
This shows how, by modifying bk, one can decide which columns will receive an update: namely, all those with
index lj> bk. By combining two such operators with bk= 
i−1
2
δandbk= 
i+1
2
δ, we then recover
lj=iδ
H 
l⊗ 
l−1 
i−1
2
δ
−H 
l⊗ 
l−1 
i+1
2
δ=
0···010···0.....................
0···010···0
,(20)
which allows us to limit the update to only one specific column: the one with index lj=iδ.
The parameter bqacts analogously, but varies the output of the Heaviside function as we move down the rows,
rather than the columns. The same operator as in (20), but with bq= 
i+1
2
δgives us in fact:
lj=iδ
H  
l−1 
i+1
2
δ
⊗ 
l−1 
i−1
2
δ
−H  
l−1 
i+1
2
δ
⊗ 
l−1 
i+1
2
δ=
0···0−10···0.....................
0···0−10···0
0···010···0.....................
0···010···0
lj< iδ
lj> iδ. (21)
Finally, (22) can be recovered by combining (20) and (21): this has the effect of removing the −1’s in (21).
25

--- PAGE 26 ---
a contextual mapping. As a preliminary step, in this section we provide an explicit formula for the
result of applying such an architecture. Once again, we are proceeding analogously to Yun et al.
(2020, App. B.5.1).
After the first selective shift application Consider a quantized input sequence X∈Las defined in
(15), with its columns ordered according to their scalar indices l=uTX. The sequence of selective
shift layers Ψ(0),Ψ(1), . . . initially has no effect on the input itself, and it leaves it unchanged until we
hit the layer corresponding to the index of the first column in the input, Ψ(ˆi), where l1=uTx1=ˆiδ.
At this point, following (22), the first column of the input is modified into
x17→ Ψ(ˆi)
|,1(X) =x1+ce0X
k:lk>l1lk=x1+ce0 nX
k=1lk−l1!
(23)
while the other columns are still left untouched. In the following, we compactly refer to the quantitiesPn
k=1lk−liassi:
s= [s1, s2, . . . , s n]T:="nX
k=1lk−l1,nX
k=1lk−l2, . . . ,nX
k=1lk−ln#T
. (24)
According to (23), the index l1of column x1is then analogously mapped to
l1=uTx17→ ˜l1:=uTΨ(ˆi)
|,1(X) =uTx1+cs1=l1+cs1. (25)
Notice that, by choosing c >1, we can ensure
c >1 =⇒ ˜l1>l1+nX
k=1lk−l1>nX
k=1> li∀i, (26)
and particularly ˜l1> l2, implying that at the next (effective) application of the selective shift operation,
this term, too, will contribute to the update.
Subsequent selective shift applications Following similar considerations, the next effective update
will be applied by the layer Ψ(ˆi)withl2=uTx2=ˆiδ. At this point, the second column index is
updated as follows:
l2=uTx2 7→ ˜l2:=uTΨ(ˆi)
|,2(X) =uTx2+c X
k:lk>l2lk+˜l1!
=l2+c nX
k=1lk−l2−l1+l1+cs1!
=l2+cs2+c2s1(27)
where ˜l1is also included in light of (26), and we used the definitions (24) and (25). Continuing to
apply Ψ(i)(X), for increasing i, and unrolling the recursion, we recover
˜l3=l3+c nX
k=1lk−l1−l2−l3+˜l1+˜l2!
=l3+cs3+c2(s2+s1) +c3s1
˜l4=l4+c nX
k=1lk−l1−l2−l3−l4+˜l1+˜l2+˜l3!
=l4+cs4+c2(s3+s2+s1) +c3(s2+ 2s1) +c4s1
˜l5=l5+c nX
k=1lk−l1−l2−l3−l4−l5+˜l1+˜l2+˜l3+˜l4!
=l5+cs5+c2(s4+s3+s2+s1) +c3(s3+ 2s2+ 3s1) +c4(s2+ 3s1) +c5s1
...(28)
26

--- PAGE 27 ---
which eventually allows us to write the general formula8
˜lj:=lj+csj+j−2X
i=0ci+2j−2X
k=ik
i
sk−i+1, j = 1. . . n. (29)
C.2.3 Result of Applying One Last Global Shift Layer
After the last selective shift layer, the original input Xhas been mapped to a modified one ˜Xwhereby
each column ˜xjis characterized by the index ˜lj=uT˜xjgiven in (29). Remember our goal is to
recover a contextual mapping, but notice that these ˜ljindices are notuniquely defined by the input9;
in other words, they do not satisfy property (2) in Def. C.2. The only exception to this is the last
index ˜ln, as (loosely speaking) it has “seen” all the previous updates - and indeed in App. C.2.4 we
prove this rigorously, under some assumption on the yet-undefined coefficient c(δ, d, n ).
A straightforward way to recover a one-to-one mapping for the whole sequence, then, is to update
every index ˜ljvia a quantity directly depending on ˜ln. This is precisely what the last global shift
layer ¯Ψ(X)aims to accomplish. This last layer is also defined starting from the simplified modified
sigmoid attention (18), by picking bk= 0 andbq= 
c(δ, d, n )n+1
2
δ: if, for any input, we can
guarantee that
˜lj≤c(δ, d, n )nδ j < n and ˜ln> c(δ, d, n )nδ, (30)
then the application of the global shift layer would result in10:
¯Ψ(˜X):=˜X+cn+1˜ψ
˜X;bq=
cn+1
2
δ, bk= 0
=⇒¯Ψ1,j(˜X) =˜X1,j+cn+1˜ln
=⇒¯Ψk>1,j(˜X) =˜Xk,j.(32)
The global shift (32) is the last layer we need to define our candidate contextual mapping. Collecting
the results from this section together, our architecture is defined by sequentially composing the
selective shift layers with the global shift one,
Ψ(X):=¯Ψ◦Ψ(δ−d−1)◦ ··· ◦ Ψ(2)◦Ψ(1)(X). (33)
After being scalar-multiplied by u, this results in a sequence
q(X):=uTΨ(X) =˜l+cn+11˜ln (34)
which we aim to prove is a contextual mapping. This is shown in the next section.
8From (28), we can notice that, for a given ˜lk, the coefficients a(k)
i,jappearing in front of the various sk−ifor
each of the cjterms, are first given by a list of ones, a(k)
i,1= 1, then a list of increasing numbers a(k)
i,2=i=⇒
a(k)
−,2=cumsum (a(k)
−,1), then a list of triangular numbers a(k)
i,3=i(i+ 1)/2 =⇒a(k)
−,3=cumsum (a(k)
−,2), and
so on: a(k)
−,j=cumsum (a(k)
−,j−1). The result of iterated applications of cumsum, starting from an all-one vector,
can be compactly described via the binomial coefficient: we have in fact
ai,j= [cumsumj([1,1, . . .])]i= 
i+j−2
j−1!
.
The actual formula (29) can be recovered after a few algebraic steps, by rearranging the summation indices.
9To convince ourselves of this, it suffices to look at the formula for (25): two sequences with different
elements l̸=l′, but such that l1=l′
1ands1=s′
1(that is, withPn
i=1li=Pn
i=1l′
i) would map to the same
˜l1=˜l′
1.
10As in footnote 7, this is also better seen by considering the resulting modified sigmoid attention matrix.
Withbk= 0andbq= 
c(δ, d, n )n+1
2
δ, in fact, if condition (30) is verified, this matrix is given by
H
˜l−1 
cn+1
2
δ
⊗˜l
=
0···0.........
0···0
1···1
˜lj, j < n
˜ln. (31)
27

--- PAGE 28 ---
C.2.4 A Sequence of Selective Shifts Followed by a Global Shift Produces a Contextual
Mapping
To complete the proof, it remains to show that the recovered sequence (34) represents a contextual
mapping and, in particular, that it is (i)one-to-one in L, and that (ii)all of its elements are distinct for
different inputs. To do so, we need a few preparatory lemmas. The first few are needed to show that
each of the basic components of (34) is indeed a one-to-one map.
Lemma C.3. The map l7→sin (24) is one-to-one.
Proof. The target map can be compactly represented as a linear operator S:
l7→s:=1nX
k=1lk−l= (1⊗1−I)l=:Sl (35)
which is invertible11, denoting that l7→sis bijective.
Lemma C.4. The map l7→˜lnin (29) is one-to-one, under the condition
c(δ, d, n )>(n−1)(δ−d−1)n−1n−1
2
. (36)
Proof. Consider two vectors of column indices l,l′differing for at least one element. We have by
definition (29) that
˜ln−˜l′
n= (ln−l′
n) +c(sn−s′
n) +n−2X
i=0ci+2n−2X
k=ik
i
(sk−i+1−s′
k−i+1) (37)
By absurd, assume ˜ln−˜l′
n= 0even though ∃i:li̸=l′
i. We have then that it must hold
(l′
n−ln) =c(sn−s′
n) +n−2X
i=0ci+2n−2X
k=ik
i
(sk−i+1−s′
k−i+1)
=c 
(sn−s′
n) +n−2X
i=0ci+1n−2X
k=ik
i
(sk−i+1−s′
k−i+1)! (38)
Notice that, for c(δ, d, n )large enough, the right-hand side does not have enough granularity to
counter the left-hand side: in fact, since ln∈ {0, δ,2δ, . . . , δ−d+1−δ}, the left-hand side can attain
values
l′
n−ln∈ {0,±δ,±2δ, . . . , ±(δ−d+1−δ)} (39)
while the former, in light of the presence of the c(δ, d, n )factor, can only attain values ∈
{0,±cδ,±2cδ, . . .}. Picking c > δ−d−1, then, ensures that equality between the two sides
of (38) can only be achieved if they are both 0. In this case, we need to impose
c(s′
n−sn) =n−2X
i=0ci+1n−2X
k=ik
i
(sk−i+1−s′
k−i+1)
⇐⇒ s′
n−sn=c n−2X
i=0cin−2X
k=ik
i
(sk−i+1−s′
k−i+1)!
.(40)
Similarly, notice that12,∀i,
|si−s′
i|=nX
k=1(lk−l′
k)−(li−l′
i)=nX
k=1,k̸=i(lk−l′
k)<(n−1)(δ−d+1−δ), (41)
11Indeed its inverse can be explicitly recovered by directly applying Sherman-Morrison formula.
12This is a direct consequence of the definition of operator Sin (35): since it has 1’s everywhere but on its
diagonal, its ∞-norm is simply n−1.
28

--- PAGE 29 ---
implying that s′
n−sn∈ {0,±δ,±2δ, . . . , ±(n−1)(δ−d−1)δ}. Again, by picking c(δ, d, n )>
(n−1)(δ−d−1)we ensure that the right-hand side does not have enough granularity, and hence
c(δ, d, n )>(n−1)(δ−d−1) = ⇒ s′
n−sn= 0, (42)
implying
c n−2X
i=0cin−2X
k=ik
i
(sk−i+1−s′
k−i+1)!
= 0
⇐⇒n−2X
k=0k
0
(s′
k+1−sk+1) =c n−2X
i=1ci−1n−2X
k=ik
i
(sk−i+1−s′
k−i+1)!
⇐⇒n−2X
k=0(s′
k+1−sk+1) =c n−2X
i=1ci−1n−2X
k=ik
i
(sk−i+1−s′
k−i+1)!
.(43)
Following a similar reasoning as the one applied above shows us that picking
c(δ, d, n )>(n−1)2(δ−d−1) = ⇒n−2X
k=0(sk+1−s′
k+1) = 0 , (44)
and requires us to satisfy
c n−2X
i=1ci−1n−2X
k=ik
i
(sk−i+1−s′
k−i+1)!
= 0
⇐⇒n−2X
k=1k
1
(s′
k−sk) =c n−2X
i=2ci−2n−2X
k=ik
i
(sk−i+1−s′
k−i+1)!
⇐⇒n−2X
k=1k(s′
k−sk) =c n−2X
i=2ci−2n−2X
k=ik
i
(sk−i+1−s′
k−i+1)!
.(45)
Once again, then, by choosing
c(δ, d, n )>(n−2)(n−1)2
2(δ−d−1) = ⇒n−2X
k=1k(sk−s′
k) = 0 . (46)
This reasoning can be repeated recursively: at each step iof the recursion, by imposing a stricter
and stricter bound on c(δ, d, n )we gain more and more conditions that the quantity s′−sneeds to
satisfy:
c(δ, d, n )>(n−1)(δ−d−1)n−2X
k=ik
i
=⇒n−2X
k=ik
i
(sk−i+1−s′
k−1+1) = 0 .(47)
Notice that, every time we increase i= 0. . . n−2, these conditions involve one less term sk−i+1−
s′
k−i+1,k=i . . . n −2: if we were to collect all these conditions within a single linear system, the
system would have an upper-triangular structure, and hence be non-singular. This implies that for the
set of nindependent conditions on s−s′to hold (we have n−1in (47), plus one more in (42)),
the only possibility is that s≡s′. Because of Lemma C.3, though, this also implies l≡l′: we have
finally reached a contradiction, and proven that indeed l7→˜lnis one-to-one, under an opportune
condition on c(δ, d, n ). Such condition can be promptly recovered13by (47):
max
i=0...n−2n−2X
k=ik
i
= max
i=0...n−2n−1
i+ 1
=n−1n−1
2
. (48)
Substituting this in (47), we recover that it suffices to impose
c(δ, d, n )>(n−1)(δ−d−1)n−1n−1
2
. (49)
13This is a consequence of some useful properties of the binomial coefficient, namely the Hockey stick identity
Jones (1994), and the symmetry of k
i
with respect to i.
29

--- PAGE 30 ---
The next few lemmas are needed to bound the elements in the ˜ljsequence, which in turn are used to
prove property (ii)in Def. C.2.
Lemma C.5. ˜ljin (29) is an increasing sequence.
Proof. This can be proven directly: we have in fact, by definition (29),
˜lj>˜lj−1⇐⇒ lj+csj+j−2X
i=0ci+2j−2X
k=ik
i
sk−i+1
> lj−1+csj−1+j−3X
i=0ci+2j−3X
k=ik
i
sk−i+1
combine sums⇐⇒ (lj−lj−1)(1−c) +j−2X
i=0ci+2j−2
i
sj−1−i>0
j−2
i
≥1, ci+2≥c2⇐= (lj−lj−1)(1−c) +c2j−2X
i=0sj−1−i>0
(24)⇐⇒ (lj−lj−1)(1−c) +c2j−2X
i=0 nX
k=1lk−lj−1−i!
>0
⇐⇒ (lj−lj−1)(1−c) +c2 
(j−1)nX
k=1lk−j−1X
k=1lk!
>0
⇐⇒ (1−c)lj+ (c−1)lj−1+c2(j−2)nX
k=1lk+c2nX
k=jlk>0
⇐⇒ (c2−c+ 1)lj+ (c−1)lj−1+c2(j−2)nX
k=1lk+c2nX
k=j+1lk>0
(50)
Already with c >1, all the coefficients are positive (and at least one is non-zero), implying that the
condition above is always satisfied and that indeed ˜ljis an increasing sequence.
Lemma C.6. Under constraint (36), each term ˜lj,j >1in (29) is bounded from below by
˜lj> cjδ,
and each term ˜lj,1< j < n is bounded from above by
˜lj< cj+1δ.
Proof. We start by proving the lower bound. By definition (29), we have
˜lj=lj+csj+j−2X
i=0ci+2j−2X
k=ik
i
sk−i+1=lj+csj+cjs1+j−3X
i=0ci+2j−2X
k=ik
i
sk−i+1.(51)
Since by assumption ljis an ordered sequence without repetitions, for j >1we necessarily have
lj> l1≥0, and hence lj≥δ. All the other terms in (51) are non-negative, so we can safely claim
that
˜lj≥δ+cjδ > cjδ∀j >1, (52)
which confirms the lower bound.
30

--- PAGE 31 ---
For the upper bound, we start again from the definition of ˜lj:
˜lj=lj+csj+j−2X
i=0ci+2j−2X
k=ik
i
sk−i+1
<(δ−d−1)δ+c(n−1)(δ−d−1)δ+s1j−2X
i=0ci+2j−1
i+ 1
≤(n−1)(δ−d−1)j−1j−1
2
δjX
i=0ci= (n−1)(δ−d−1)j−1j−1
2
δ1−cj+1
1−c,(53)
where we used relationship (48) and collected all cterms within the sum. Notice that, for a given
a >1we have that
1−cj+1
1−c≤acj, (54)
provided that c≥a
a−1. In fact,
1−cj+1
1−c≤acj⇐⇒1−cj+1−acj+acj+1
1−c≤0
⇐=1
a−1+
c−a
a−1
cj≥0⇐=1
a−1≥0(55)
which is always satisfied. After substituting (54) in (53), this allows us to write
˜lj< a(n−1)(δ−d−1)j−1j−1
2
δcj. (56)
To prove that ˜lj< δcj+1, then, it remains to show that
c≥a(n−1)(δ−d−1)j−1j−1
2
∀1< j < n. (57)
Substituting condition (36) in the inequality above, we are left with proving
n−1n−1
2
≥max
j=2...n−1aj−1j−1
2
=an−2n−2
2
. (58)
The outcome depends on the parity of n. For nodd, we have
n−1n−1
2
≥an−2n−2
2
⇐⇒ 2n−1
n−1≥a, (59)
to satisfy which it suffices to pick a= 2. This requires having c≥a
a−1= 2, which is automatically
satisfied. For neven, on the other hand, the binomial coefficients simplify to
n−1n−1
2
≥an−2n−2
2
⇐⇒ 2n−1
n≥a. (60)
To satisfy this, we need to pick a= 2n−1
n, which requires c≥a
a−1= 2n−1
n−2; however, this too is
automatically satisfied by (36) provided n≥4. This completes the proof.
Lemma C.7. Under the constraint (36), condition (30) holds.
Proof. We remind that condition (30) is necessary for the correct “functioning” of the global shift
layer, and it composes of two parts. The first part requires that ˜lj< cnδ∀j < n . Thanks to
Lemma C.5, it suffices to show that ˜ln−1< cnδ, but this is already granted by the upper bound in
Lemma C.6. Analogously, for the second part, we need to show that ˜ln> cnδ: for this too we can
use the lower bound in Lemma C.6.
We finally have all the ingredients to prove the main theorem of this section:
31

--- PAGE 32 ---
Theorem C.8. The map in (34), given by
X7→q(X) =uTΨ(X)
represents a contextual mapping.
Proof. As defined in Def. C.2, a contextual mapping must satisfy two conditions. The first one is that
qi(X)̸=qj(X),∀i̸=j and ∀X∈L. (61)
This is directly proven by considering Lemma C.5: since ˜ljis a (strictly) increasing sequence, all
its elements are already distinct. The action of the last global shift layer merely translates all these
elements by a same quantity, but they remain distinct nonetheless.
The second condition for a contextual mapping is given by
qi(X)̸=qj(X′),∀i, j and∀X,X′∈L,with X̸=X′. (62)
We prove that this holds for (34) by directly considering the difference between two components i, j
for different inputs:
qi(X)−qj(X′) =˜li−˜l′
j+cn+1
˜ln−˜l′
n
= 0 ⇐⇒ ˜li−˜l′
j=cn+1
˜l′
n−˜ln
.(63)
Notice that, due to Lemma C.4, we have ˜ln−˜l′
n̸= 0and particularly, |˜ln−˜l′
n| ≥δ. On the other hand,
in light of the bounds in Lemma C.6, we have that the left-hand side |˜lj−˜li|< cnδ. Consequently,
the two sides can never cancel each other out, and the proof is complete.
D Lipschitzness of Sigmoid Attention
In the following, we report the proof for the recovering the Lipschitzness constant associated with
SigmoidAttn , as stated in Thm. 3.2.
Letting A=WT
qWk, and calling σij=σ(⟨Wqxi, Wkxj⟩)andσ′
ij=σ′(⟨Wqxi, Wkxj⟩), we find
that the Jacobian of ϕin the direction (δ1, . . . , δ n)for the sample xiis given by:
Jaci=
nX
j=1σ′
ijxjxT
jAT
δi+nX
j=1 
σ′
ijxjxT
iA+σijIp
δj, (64)
We see that this Jacobian is the sum of two terms. To control its norm, we can control each norm
individually.
The first term,Pn
j=1σ′
ijxjxT
jAT
δiis of the form UiδiwithUia matrix. Its squared-norm is
therefore:nX
i=1∥Uiδi∥2≤max
i∥Ui∥2
2∥δ∥F. (65)
Hence, its squared spectral norm is bounded by max i∥Ui∥2
2.
We now let σ′
∞be a bound on n× |σ′|; We have:
∥Ui∥2≤nX
j=1∥σ′
ijxjx⊤
jA∥2 (66)
≤σ′
∞∥A∥21
nnX
j=1∥xj∥2(67)
≤σ′
∞∥A∥2E[∥xj∥2]. (68)
We see that if the points xihave norm ≤R, then the Jacobian grows at most like R2, because it is
“quadratic” in x. However, we see that the quadratic term is likely to be mitigated by the σ′(aij)term
that goes to 0ifaijis large.
32

--- PAGE 33 ---
The second term,Pn
j=1 
σ′
ijxjxT
iA+σijIp
δj, is the sum of two terms. Here, too, we use the
triangular inequality to control their norm individually. We get:
∥nX
j=1σijδj∥2=∥δTσi∥2(69)
≤ ∥δ∥2
F∥σi∥2, (70)
where σi∈Rpis the i-th column of σij, andδ∈Rn×p. and by summing, letting σ∞an upper bound
onn× |σ(x)|:
nX
i=1∥nX
j=1σijδj∥2≤σ2
∞∥δ∥2
F. (71)
So that σ∞upper bounds the spectral norm of the last term.
For the final term,Pn
j=1σ′
ijxjxT
iAδj, define ˆδ=δAT. We get:
nX
j=1σ′
ijxjxT
iAδj=nX
j=1σ′
ij⟨xi,ˆδj⟩xj. (72)
Hence, letting Mthe matrix of entries Mij=σ′
ij⟨xi,ˆδj⟩, we see that the previous term is simply
xTMT
i, so that we get the upper bound on the norm of the term:
nX
i=1∥xTMT
i∥2≤ ∥x∥2
F∥M∥2
F (73)
and∥M∥2
F=P
ij(σ′
ij)2⟨xi,ˆδj⟩2≤1
n2σ′
∞∥x∥2
F∥A∥2
2∥δ∥2
F, giving overall:
vuutnX
i=1∥xTMT
i∥2≤σ′
∞∥A∥2E[∥xj∥2]∥δ∥F. (74)
Notice how this quantity matches the one in (68).
Finally, summing all together gives:
∥Jac∥2≤2σ′
∞∥A∥2E[∥xj∥2] +σ∞, (75)
which completes the proof.
Remark : The previous upper bound might not be tight. Indeed, intuitively, if the xiare large, then the
termσ′
ijshould be exponentially small (provided, of course, that WqxiandWkxjare not orthogonal),
which would even remove the dependency on the variance in the sigmoid attention.
E The Bias Term of Sigmoid Attention
One of the differences between SigmoidAttn andSoftmaxAttn is the normalization constant. In
SigmoidAttn , one way to emulate the effect of a normalization constant (which links all the elements
of the input together and defines a distribution over them), is to include a bias term in the definition
as proposed in (3).
For an input vector z∈Rn, the output of the sigmoid with bias bis
σb(z)i:=exp(zi)
exp(zi) + exp( −b)
Contrary to the softmax, this output cannot always sum to one because there is no normalization. We
therefore seek a value for bthatapproximately normalizes σb(z), i.e., such thatPn
i=1σb(z)i≃1.
We have
Proposition E.1. Letz∈Rn, and take m, M ∈Rsuch that for all i, it holds m≤zi≤M. Then,
the equationPn
i=1σb(z)i= 1with variable bhas a single solution b∗with
−log(n−1)−M≤b∗≤ −log(n−1)−m .
33

--- PAGE 34 ---
Proof. The function ϕ:b→Pn
i=1σb(z)iis smooth and monotonically increasing, and we have
ϕ(−log(n−1)−M)≤1andϕ(−log(n−1)−m)≥1. This shows the existence of b∗as well as
the advertised bound on b∗.
This suggests using a bof the order of −log(n); in practice we use b=−log(n).
We can also look for a bias term b, which helps to approximate the softmax function by the sigmoid
function.
We assume that softmax provides us with the true distribution p⋆, where p⋆
i=ezi
ezi+P
j̸=iezj. The
goal is to find the bias term bsuch that sigmoid function with weights over all elements denoted
byp, where pi=σb(z)i, approximates p⋆. Note that, as mentioned before, pis not necessarily a
distribution, i.e.Pn
i=1piis not always equal to one.
In technical terms, we aim to estimate the normalizing factor Z=Pn
i=1ezi. The existing approaches
for estimating Zis compute-expensive for high dimensions and requires resampling methods. Also,
the optimal value of bwould depend on the exact values of z, which is unknown beforehand.
Therefore, we propose a more intuitive way to estimate the order of bias but possibly with larger
disparity. To distribute the independent masses in SigmoidAttn , we assume that each element has
uniform weight for the model apriori, which means that none of the elements of the input vector zhas
any known importance over the others. In the simplest case when softmax is a uniform distribution,
we ideally want to have the same order of values for sigmoid as of softmax, which should be1
n.
Therefore, we can write down the following:
∀i p i=1
1 +e−(zi+b)≃1
n=p⋆
i (76)
Ideally, we would like to have 1 +e−(zi+b)≃n. Requiring that p=p∗in the case where all the zi
are0gives exp(−b) =n−1, i.e.b≃ −log(n)for large n. In the case that all the ziare bounded,
|zi| ≤M <∞for some constant M, then b≃ −(M+ log( n))≈ −max{M,log(n)}. However,
in most cases we do not know M. When the sequence length nis large enough, the constant M
loses its importance while in short sequence length, it impacts distributing the weights over elements
more. To resolve this issue, we assume that ziare sampled from a standard Gaussian distribution, i.e.
zi∼ N(0, σ2)where σ= 1. Note that this assumption comes from the fact that ziin our problem is
one of the elements of QKT/p
dqk, which is the sum of dqkrandom variables. Using Central Limit
Theorem, we can assume that ziis sampled from a Gaussian distribution. The idea is to estimate M,
such that with high probability, |zi| ≤M, i.e.P(|zi|> M)≤ϵfor a desired ϵ. Therefore, we have
P(|zi|> M) =P
|zi|>M
σσ
≤1
(M
σ)2=σ2
M2≤ϵ, (77)
where the inequality is resulted from Chebychev’s inequality. Setting σ= 1, we have M≃p
1/ϵ.
Therefore, the order-optimal value would be b≃ −max{p
1/ϵ,log(n)}, and for long sequence
length, b≃ −log(n). For example, if we want 90% accuracy in our estimation, M≈3σ= 3,
which means b≃ −max{3,log(n)}. Note that this approximation also follows the intuition that as
ngrows, we expect the SigmoidAttn without bias term overestimate the mass on each point, so we
need to normalize the mass according to nat each point as well.
On another side, one may be more interested in the gradients of p⋆andpwith respect to zito behave
similarly. We show that b≃ − log(n)is still a good choice in this scenario. Let us derive the
derivative of SigmoidAttn andSoftmaxAttn with respect to the input. We note that for any i, both
functions can be written asezi
ezi+Z−iwhere Z−iis the share of normalization factor except element i
ofz. For SoftmaxAttn ,Z−i=P
j̸=iezjand for SigmoidAttn ,Z−i=e−b. Now, we have
∂
∂ziezi
ezi+Z−i=eziZ−i
(ezi+Z−i)2. (78)
Therefore, we have the following
∂p⋆
i
∂zi=p⋆
i(1−p⋆
i) (79)
∂pi
∂zi=pi(1−pi). (80)
34

--- PAGE 35 ---
We can see that if pi≃p⋆
i, then∂pi
∂zi≃∂p⋆
i
∂zi. So, the previous choice of bias term b≃ −log(n)
approximates the order of gradients as well. In fact, this is the only valid choice even though we have
a quadratic term.
∂pi
∂zi≃∂p⋆
i
∂zi⇐⇒ p⋆
i(1−p⋆
i) =pi(1−pi) (81)
⇐⇒ (pi−p⋆
i) (pi−(1−p⋆
i)) = 0 . (82)
Which means either pi≃p⋆
iorpi≃1−p⋆
i. The first one provides us with b≃ −log(n)while the
second one cannot happen since the nominator of piis dependent on ziwhile the nominator of 1−p⋆
i
is independent of zi.
F Details of F LASH SIGMOID
This appendix provides details of the FLASH SIGMOID algorithm. We begin by discussing the
implementation details of FLASH SIGMOID , which we build as an extension of FLASH ATTENTION 2,
followed by a benchmark of the performance of the involved kernels. We show that the kernels
ofFLASH SIGMOID provide a considerable performance boost in model inference over those of
FLASH ATTENTION 2and a modest performance boost for model training. Further, we demonstrate
that the kernel speed boosts also reflect in a considerable performance gain in realistic end-to-end
experiments, with an example of training vision transformers (Dosovitskiy et al., 2021) on the
ImageNet dataset (Deng et al., 2009). Finally, we also provide kernel benchmarking details of
FLASH SIGMOID implementation by taking into account ALiBi slopes (Press et al., 2022), which is
one of the important components of SigmoidAttn as seen in the main text of the paper.
F.1 Details of F LASH SIGMOID Algorithm
Softmax vs. Sigmoid Attention: In this subsection, we discuss the implementation details of
FLASH SIGMOID algorithm, which is a hardware-aware implementation of SigmoidAttn approach.
We begin with the expressions of the forward and backward passes of softmax and sigmoid attention
mechanisms. Let Q,K, andVrepresent the query, key, and value tensors. Then, the desired
forward and backward pass expressions are reported in Tab. 4. The application of sigmoid and
SOFTMAX SIGMOID
FORWARD BACKWARD FORWARD BACKWARD
S=Q·K⊤
√
ddV=P⊤·dO S =Q·K⊤
√
ddV=P⊤·dO
P=SOFTMAX (S) dP=dO·V⊤P=σ(S) dP=dO·V⊤
O=P·V dS=P⊙(dP−ROWSUM (dO⊙O)) O=P·V dS=P⊙(1−P)⊙dP
dQ=√
d·dS·K dQ=√
d·dS·K
dK=√
d·dS⊤·Q dK=√
d·dS⊤·Q
Table 4: Description of the forward and backward passes of softmax and sigmoid attention. With ⊙,
we denote Hadamard (element-wise) multiplication.
softmax activation functions, as highlighted in orange color in Tab. 4, is the only implementation
difference in the forward passes. Similarly, the expressions for the gradients of the preactivation
(dS), as highlighted in purple color in the table above, is the only implementation difference in the
backward passes. In light of this, we implement the FLASH SIGMOID algorithm as an extension
of the FLASH ATTENTION 2(Dao, 2023) algorithm, which is a highly optimized hardware-aware
implementation of SoftmaxAttn .
Flash Attention in Brief: As pointed at in the main text, the FLASH ATTENTION (Dao et al.,
2022) and FLASH ATTENTION 2(Dao, 2023) algorithms provide hardware-aware implementations of
exact attention mechanism by optimizing for bottlenecks of modern accelerators (Choquette et al.,
2021; Choquette, 2023). These GPUs possess massive amounts (e.g., ∼80GB) of High-Bandwidth
Memory (HBM), which stores large tensors but is slow in moving the data to the accelerators. On
the other hand, they have smaller amounts (e.g., ∼20MB) of SRAM, which is often more than an
35

--- PAGE 36 ---
Algorithm 1 FLASH SIGMOID Forward Pass
1:procedure FORWARD (Q,K,V, Br, Bc):
2: """
3: inputs: Matrices Q,K,V∈Rn×dare on HBM of the GPU.
4: inputs: Integers BrandBcare the block size for queries and key-values respectively.
5:
6: outputs: Matrix O∈Rn×don HBM of the GPU.
7: # No need to output logsumexp vector L∈Rnon HBM.
8: """
9: Divide QintoTr:=⌈n
Br⌉blocks: Q1,···,QTrwithQi∈RBr×d.
10: Divide KintoTc:=⌈n
Bc⌉blocks: K1,···,KTcwithKi∈RBc×d.
11: Divide VintoTcblocks: V1,···,VTcwithVi∈RBc×d.
12: Divide OintoTrblocks: O1,···,OTrwithOi∈RBr×d.
13: fori= 1,···, Trdo
14: Load block Qifrom HBM to SRAM of the GPU.
15: On chip, initialize Oiwith zeros: Oi←0Br×d.
16: # No allocation of either row-sum ℓi∈RBror row-max mi∈RBron chip.
17: forj= 1···Tcdo
18: Load blocks Kj,Vjfrom HBM to SRAM of the GPU.
19: On chip, evaluate pre-activations: Sij←Qi·K⊤
j/√
d∈RBr×Bc.
20: On chip, evaluate sigmoid attention: Pij←σ(Sij).
21: On chip, update output block: Oi←Oi+Pij·Vj.
22: # No need to update and track ℓiandmivectors.
23: end for
24: StoreOifrom chip to HBM as the i−th block of Omatrix.
25: # No post-processing of OiorLiblocks on chip.
26: # No movement of Liblock from chip to HBM.
27: end for
28: return matrix O.
29:end procedure
order magnitude faster for carrying out actual computations using the registers/tensor cores of the
GPU. This trade-off between memory size and computation speed across hierarchies results in the
attention mechanism computation being bottlenecked by memory accesses between the HBM and the
SRAM (Ivanov et al., 2021). Consequently, flash algorithms optimize for memory accesses across
the hierarchy of GPU memory types in order to accelerate computation of attention mechanism and
its gradients. F LASH SIGMOID is no exception to this approach.
Algorithm 1 describes the forward pass and Alg. 2 describes the backward pass of the FLASH SIGMOID
algorithm. We highlight in orange color the steps in the forward pass of FLASH SIGMOID that differ
from those in FLASH ATTENTION 2by virtue of sigmoid activation. Similarly, we highlight in purple
color the differences in the backward pass. Finally, we highlight in blue color the salient points of
FLASH SIGMOID that further help minimize bottlenecking factors on modern accelerators.
Fewer Tensor Allocations, Fewer Memory Accesses, Fast-Tanh: InFLASH ATTENTION and
FLASH ATTENTION 2, the attention mechanism is computed by splitting the attention matrix into
blocks. Since softmax activation requires a row-wise reduction to compute its normalization factor
(i.e., the denominator), one needs to properly compute and track such factor across blocks. Moreover,
inFLASH ATTENTION this normalization factor is stored after being computed in the forward pass, to
have it easily accessible to further speed-up the backward pass. By contrast, substituting sigmoid
to softmax eliminates the need to allocate and move across the GPU memory hierarchy the tensors
related to the normalization factor (i.e., moving the logsumexp tensor L∈Rnon HBM in the
forward and backward passes). In addition, applying softmax in a stable manner requires tracking the
row-max variable mion chip, which instead is not needed for sigmoid activation. This further helps
reducing some on-chip operations and lowering register pressure in F LASH SIGMOID .
Moving on to the backward pass (described in Alg. 2), FLASH ATTENTION 2requires computing
rowsum (dO⊙O), which is needed to backpropagate the gradients of softmax attention outputs
36

--- PAGE 37 ---
Algorithm 2 FLASH SIGMOID Backward Pass
1:procedure BACKWARD (Q,K,V,dO, Br, Bc):
2: """
3: inputs: Matrices Q,K,V,dO∈Rn×dare on HBM of the GPU.
4: inputs: Integers BrandBcare the block size for queries and key-values respectively.
5: # No need of logsumexp vector L∈Rnto be saved for the backward pass.
6:
7: outputs: Matrices dQ,dK,dV∈Rn×don HBM of the GPU.
8: """
9: Divide QintoTr:=⌈n
Br⌉blocks: Q1,···,QTrwithQi∈RBr×d.
10: Divide KintoTc:=⌈n
Bc⌉blocks: K1,···,KTcwithKi∈RBc×d.
11: Divide VintoTcblocks: V1,···,VTcwithVi∈RBc×d.
12: Divide OintoTrblocks: O1,···,OTrwithVi∈RBr×d.
13: Divide dOintoTr:=⌈n
Br⌉blocks: dO1,···,dOTrwithdOi∈RBr×d.
14: Allocate dQon HBM and divide into Trblocks: dQ1,···,dQTrwithdQi∈RBr×d.
15: Allocate dKon HBM and divide into Tcblocks: dK1,···,dKTcwithdKi∈RBc×d.
16: Allocate dVon HBM and divide into Tcblocks: dV1,···,dVTcwithdVi∈RBc×d.
17: # No need to compute rowsum (dO⊙O)as sigmoid and its gradients are pointwise.
18: forj= 1,···, Tcdo
19: Load blocks Kj,Vjfrom HBM to SRAM of the GPU.
20: On chip, initialize dKj,dVjwith zeros: dKj←0Bc×d;dVj←0Bc×d.
21: fori= 1···Trdo
22: Load blocks Qi,dOi,dQifrom HBM to SRAM of the GPU.
23: # No need of movement of blocks rowsum (dO⊙O)iand logsumexp Li.
24: On chip, evaluate pre-activations: Sij←Qi·K⊤
j/√
d∈RBr×Bc.
25: On chip, evaluate sigmoid attention: Pij←σ(Sij).
26: On chip, update gradient of values: dVi←dVi+P⊤
ij·dOj.
27: On chip, compute gradients of attention matrix: dPij←dOi·V⊤
i∈RBr×Bc.
28: On chip, compute gradients of pre-activations: dSij←Pij⊙(1−Pij)⊙dPij.
29: Load query gradient block dQifrom HBM to SRAM, and then on to chip.
30: Update query gradient block on chip: dQi←dQi+√
d·dSij·Kj.
31: Store query gradient block dQifrom chip back to HBM.
32: On chip, update key gradient block: dKj←dKj+√
d·dS⊤
ij·Qi.
33: end for
34: StoredKj,dVjfrom chip to HBM as the j−th blocks of dK,dVmatrices respectively.
35: end for
36: return matrices dQ,dK,dV.
37:end procedure
to the preactivations. However, since sigmoid activation is applied element-wise, its gradients also
backpropagate across sigmoid element-wise, eliminating the need of the row-sum variable and the
movement of its blocks across the memory hierarchy. Another optimization of FLASH ATTENTION and
FLASH ATTENTION 2consists of partially re-computing the forward pass of attention mechanism in
the backward pass to avoid bottlenecks and speed-up the implementation. To keep the backward pass
implementation fast, they require the logsumexp variable to be available and transferred between HBM
and SRAM in the backward pass. FLASH SIGMOID , being an element-wise activation, eliminates
the need of this variable from the backward pass, and consequently, from the entire algorithm.
Finally, a major component in our implementation is the usage of GPU-based implementation of
the tanh activation. Sigmoid activation is related to Tanh activation via the following relation:
σ(x) = 0 .5·(1 + tanh (0 .5·x)). We utilize the fast GPU-implementation of Tanh activation, which
trades off some precision for better speed, in order to compute sigmoid activation in both the forward
and the backward pass. This provides a considerable speed-boost in both the forward and backward
passes of FLASH SIGMOID , while maintaining parity in performance with a naïve implementation of
sigmoid attention. Based on these points of modification, we extend FLASH ATTENTION 2to obtain
FLASH SIGMOID , a hardware-aware implementation of SigmoidAttn .
37

--- PAGE 38 ---
F.2 Benchmarking of F LASH SIGMOID Kernels
Benchmarking Setup: Having seen the details of the FLASH SIGMOID algorithm, we next consider
the benchmarking of its kernels. For this, we create a small model in PyTorch (Paszke et al., 2019)
that inputs query, key, and value tensors (all of shape [batch,tokens ,heads ,features ]) and passes
these through a number of attention layers. Mimicking the design of vision transformers (ViTB-
16/224) (Dosovitskiy et al., 2021), we set the number of heads and per-head features as 12and64,
respectively. We set a batch size of 32, and consider a 10-layer architecture. Then, for the number of
tokens sampled from a wide range of [64,78k], we compute the forward and backward passes of this
model. For these computations, we measure the kernel GPU time using PyTorch’s profiler. We carry
out our experiments on both H100 (Choquette, 2023) and A100 (Choquette et al., 2021) GPUs.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
02505007501000125015001750Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)
(a) Inference mode kernels on H100.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
0100020003000400050006000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal) (b) Training mode kernels on H100.
Figure 11: On average, for sequence lengths between [64,78k], the inference mode kernel of
FLASH SIGMOID is17.39% faster than FLASH ATTENTION 2for self-attention and 18.76% for causal
attention. The training mode kernels of FLASH SIGMOID are6.53% faster than FLASH ATTENTION 2
for self-attention and 9.46% for causal attention. Note that inference involves only the forward pass
of the model and training involves both the forward and the backward pass of the model.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
050010001500200025003000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)
(a) Inference mode kernels on A100.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
0200040006000800010000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal) (b) Training mode kernels on A100.
Figure 12: On average, for sequence lengths between [64,78k], the inference mode kernel of
FLASH SIGMOID is14.33% faster than FLASH ATTENTION 2for self-attention and 16.92% for causal
attention. The training mode kernels of FLASH SIGMOID are6.02% faster than FLASH ATTENTION 2
for self-attention and 5.27% for causal attention. Note that inference involves only the forward pass
of the model and training involves both the forward and the backward pass of the model.
Results: Figures 11 and 12 show the GPU time comparisons of kernels in inference mode and
training mode of FLASH SIGMOID andFLASH ATTENTION 2respectively. We observe that we obtain
a large average speed-boost for inference and a modest average speed-boost for training. Note that
the speed-ups in all the subsequent figures are obtained by averaging the performances for tokens
sampled in the range of [64,78k].
38

--- PAGE 39 ---
Details of Individual Kernels: Next, we also show the performance of individual flash kernels of
FLASH SIGMOID andFLASH ATTENTION 2. Note that inference mode involves only the forward pas
of the model, while training mode involves both the forward and the backward pass of the model. The
forward pass of both these approaches involves one kernel, which we term flash_fwd_kernel, and the
backward pass of both these approaches is made up of three kernels, which we term bwd_dq_dk_dv,
bwd_dot_do_o, and bwd_convert_dq. In code, the real names of these kernels are as follows.
fwd:=flash_fwd_kernel
bwd_dq_dk_dv :=flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel
bwd_dot_do_o :=flash_bwd_dot_do_o_kernel
bwd_convert_dq :=flash_bwd_convert_dq_kernel(83)
Here, we first provide a brief description of the tasks performed by each of these kernels; for a detailed
explanation, we refer the reader to FLASH ATTENTION 2(Dao, 2023) paper and code. The fwd kernel
computes the full forward pass of the model as shown in Tab. 4. The bulk of computations of the
backward pass happen in the bwd_dq_dk_dv kernel, which performs re-computation of attention
matrix and reduction of key and value gradient tensors ( dK,dV). Again, the exact steps carried out
in the backward pass can be checked from Tab. 4. The bwd_convert_dq kernel performs the reduction
of query gradient tensor ( dQ). Finally, note that the bwd_dot_do_o kernel in FLASH ATTENTION 2
performs the task of computing the rowsum (dO⊙O)tensor along with clearing of the accumulators
of query gradients ( dQ). Although FLASH SIGMOID does not require this row-sum tensor, the
clearing of accumulators of query gradients is still needed. For this reason, bwd_dot_do_o kernel
also appears in the profiling of F LASH SIGMOID .
Performance of Individual Kernels: Figures 13 and 14 show the performance comparison of
each flash kernel in FLASH SIGMOID with the corresponding kernel in FLASH ATTENTION 2when
tested on an H100 GPU and an A100 GPU respectively. We observe that on both the H100 and
A100 GPU architectures, the fwd kernel of FLASH SIGMOID is significantly faster than that of
FLASH ATTENTION 2and the bwd_dq_dk_dv kernel of FLASH SIGMOID has a modest average speed
boost over FLASH ATTENTION 2. The bwd_dot_do_o kernel in FLASH SIGMOID is significantly faster
on A100 GPUs. Note that even though the bwd_dot_do_o kernel of FLASH SIGMOID appears to be
slower on average on H100 GPUs, the kernel time of bwd_dot_do_o ( ∼5ms) is negligible compared
to that of the main bwd_dq_dk_dv kernel ( ∼5000 ms). Thus, the combined backward pass kernel in
FLASH SIGMOID time does not suffer from this slowdown. Finally, note that for bwd_convert_dq,
FLASH SIGMOID andFLASH ATTENTION 2have identical performance. This is expected, since the
task of this kernel is to reduce the gradient of the queries dQ, which is a common step in both the
approaches and is not modified in F LASH SIGMOID .
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
02505007501000125015001750Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)
fwd:
17.39% faster for
self-attention and
18.76% for causal.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
01000200030004000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)bwd_dq_dk_dv:
3.29% faster for
self-attention and
6.97% for causal.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
012345Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)bwd_dot_do_o:
2.24% slower for
self-attention and
2.17% for causal.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
0.00.51.01.52.02.53.03.54.0Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)bwd_convert_dq:
0.03% faster for
self-attention, 0.02%
slower for causal.
Figure 13: F LASH SIGMOID and F LASH ATTENTION 2 kernel comparison on H100 GPUs.
F.3 Speed Boosts of F LASH SIGMOID in Realistic Settings
In this section, we demonstrate how the performance boosts measured in App. F.2 for the individual
kernels of F LASH SIGMOID contributes to speeding-up realistic runs with end-to-end training.
Setup: As a target experiment, we consider training a vision transformer (Dosovitskiy et al., 2021)
on the ImageNet dataset (Deng et al., 2009). We create two vision transformer model variants– one
39

--- PAGE 40 ---
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
050010001500200025003000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)fwd:
14.33% faster for
self-attention and
16.92% for causal.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
010002000300040005000600070008000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)bwd_dq_dk_dv:
3.50% faster for
self-attention and
1.39% for causal.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
0246810Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)bwd_dot_do_o:
7.95% faster for
self-attention and
8.00% for causal.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
01234567Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Causal)bwd_convert_dq:
0.01% faster for
self-attention, 0.03%
slower for causal.
Figure 14: F LASH SIGMOID and F LASH ATTENTION 2 kernel comparison on A100 GPUs.
with FLASH ATTENTION 2attention and the other with FLASH SIGMOID attention. We carry out the
training of these models with a distributed data-parallel (DDP) setup using PyTorch (Paszke et al.,
2019). We perform two sets of experiments– i.the first performs DDP training on four nodes of
H100 GPUs with eight GPUs per node and EFA/RDMA interconnect for the nodes, and ii.the second
performs DDP training on four nodes of A100 GPUs with eight GPUs per node. In each set of
experiments, we use three different image sizes ( 64×64,90×90, and 100×100), along with patch
size of 1to result in different number of tokens for the underlying attention mechanism in the vision
transformer model ( 64×64 = 4096 ,90×90 = 8100 , and 100×100 = 10000 tokens). For each
of these configurations, we select batch sizes so that the GPU memory utilization would be greater
than80%. These considerations are in order to minimize, if not eliminate, other confounders that
can unfairly affect estimation speed-ups in realistic runs. For instance, a low GPU utilization would
lead to a larger number of updates, which in turn would incur unnecessary delays, variations, and
slow-downs due to across-nodes communications.
Results: The results of the runs on H100 nodes and A100 nodes are shown in Tab. 5 and 6 respectively.
There, we show how the kernel GPU times for forward and backward passes vary according to the
number of tokens considered, and include the wall-clock time of the end-to-end runs as explained
above. We observe that the kernel speed-up reflects significantly in the speed-up of inference of the
models (during testing) and modestly in the training of the models. We observe ∼8%speed-up in
wall-clock time of inference and ∼4%speed-up in wall-clock time of training.
TOKENSKERNEL GPU T IMECOMPARISON FULL RUNWALL-CLOCK TIMECOMPARISON
KERNELS FLASH ATTENTION 2 (MS) F LASH SIGMOID (MS) M ODE FLASH ATTENTION 2 (S) F LASH SIGMOID (S)
4096FWD 4.98±0.01 4 .17±0.01 (−16.31%) INFERENCE 11.17±0.18 10 .68±0.18 (−4.42%)
FWD+BWD 19.58±0.06 18 .12±0.04 (−7.45%) TRAINING 1563.39±1.30 1521 .68±2.27 (−2.67%)
8100FWD 20.46±0.05 16 .73±0.05 (−18.22%) INFERENCE 28.21±0.18 25 .93±0.17 (−8.06%)
FWD+BWD 77.63±0.13 72 .70±0.12 (−6.35%) TRAINING 4282.75±2.14 4129 .25±4.14 (−3.58%)
10000FWD 31.17±0.07 25 .49±0.05 (−18.20%) INFERENCE 38.71±0.19 35 .37±0.17 (−8.62%)
FWD+BWD 117.53±0.13 109 .87±0.12 (−6.52%) TRAINING 5990.72±2.21 5751 .43±5.77 (−3.99%)
Table 5: FLASH SIGMOID vs.FLASH ATTENTION 2on H100 nodes. The kernel GPU time for both
the approaches is reported in milliseconds and wall-clock times is reported in seconds per epoch.
TOKENSKERNEL GPU T IMECOMPARISON FULL RUNWALL-CLOCK TIMECOMPARISON
KERNELS FLASH ATTENTION 2 (MS) F LASH SIGMOID (MS) M ODE FLASH ATTENTION 2 (S) F LASH SIGMOID (S)
4096FWD 8.32±0.02 7 .84±0.03 (−5.79%) INFERENCE 19.05±0.22 18 .74±0.19 (−1.65%)
FWD+BWD 31.81±0.08 31 .11±0.08 (−2.19%) TRAINING 2795.03±2.35 2769 .44±5.10 (−0.92%)
8100FWD 33.65±0.09 27 .92±0.07 (−17.04%) INFERENCE 47.35±0.20 44 .05±0.17 (−6.96%)
FWD+BWD 128.18±0.13 119 .04±0.12 (−7.13%) TRAINING 7519.64±4.21 7254 .84±12.64 (−3.52%)
10000FWD 51.17±0.07 42 .49±0.06 (−16.96%) INFERENCE 64.61±0.32 59 .55±0.18 (−7.82%)
FWD+BWD 194.54±0.14 180 .59±0.15 (−7.17%) TRAINING 10455 .64±8.85 10052 .04±18.87 (−3.86%)
Table 6: FLASH SIGMOID vs.FLASH ATTENTION 2on A100 nodes. The kernel GPU time for both
the approaches is reported in milliseconds and wall-clock times is reported in seconds per epoch.
40

--- PAGE 41 ---
Connection of Wall-Clock Time Speed-Up and Kernel Speed-Up: From Tab. 5 and 6, it is clear
that the speed-up in kernels is larger than that in the wall-clock times of the full runs. In fact, the
speed-up in kernels is the upper bound for the speed-up that we would see in wall-clock times. To see
why, let us denote by τsmandτσthe total kernel GPU time for softmax attention and sigmoid attention
respectively. Then, the kernel speed-up is given by skernel:= 1−τσ
τsm. However, in a full run, the total
wall clock time also incorporates the time required to load data, time taken by other layers of the
underlying models, time required to communicate gradients and other data across GPUs and across
nodes, and so on. For our corresponding sigmoid and softmax runs, these extra factors are designed
to add, upon expectation, in the same extra time τ. Thus, the wall-clock time speed-up of a full run
with end-to-end training is swall-clock := 1−τσ+τ
τsm+τ. Since we have faster sigmoid kernels, we have
τσ< τ sm, which in turn shows that swall-clock = 1−τσ+τ
τsm+τ<1−τσ
τsm=skernel. This explains the speed
boost trends in kernel time versus full run wall-clock time for each setting in Tab. 5 and 6. However,
in particular, if a model performs attention mechanism over large number of tokens, the attention
mechanism, and hence the corresponding kernel time, starts to dominate the other computations in the
network. In that case, we see that the wall-clock time speed-boost is closer to the kernel speed-boost.
Mathematically, if τσ, τsm> > τ , we have: τσ+τ≈τσ,τsm+τ≈τsm. Thus, skernel≈swall-clock ,
thereby making swall-clock /skernel→1.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
02505007501000125015001750Kernel GPU Time (ms) 
FlashAttention2 + ALiBi (Full)
FlashSigmoid + ALiBi (Full)
FlashAttention2 + ALiBi (Causal)
FlashSigmoid + ALiBi (Causal)
(a) Inference mode kernels on H100.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
01000200030004000500060007000Kernel GPU Time (ms) 
FlashAttention2 + ALiBi (Full)
FlashSigmoid + ALiBi (Full)
FlashAttention2 + ALiBi (Causal)
FlashSigmoid + ALiBi (Causal) (b) Training mode kernels on H100.
Figure 15: On average, for sequence lengths between [64,78k], the inference mode kernel of
FLASH SIGMOID is17.04% faster than FLASH ATTENTION 2for self-attention and 10.87% for causal
attention. The training mode kernels of FLASH SIGMOID are8.91% faster than FLASH ATTENTION 2
for self-attention and 4.72% for causal attention. Note that inference involves only the forward pass
of the model and training involves both the forward and the backward pass of the model.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
050010001500200025003000Kernel GPU Time (ms) 
FlashAttention2 + ALiBi (Full)
FlashSigmoid + ALiBi (Full)
FlashAttention2 + ALiBi (Causal)
FlashSigmoid + ALiBi (Causal)
(a) Inference mode kernels on A100.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
020004000600080001000012000Kernel GPU Time (ms) 
FlashAttention2 + ALiBi (Full)
FlashSigmoid + ALiBi (Full)
FlashAttention2 + ALiBi (Causal)
FlashSigmoid + ALiBi (Causal) (b) Training mode kernels on A100.
Figure 16: On average, for sequence lengths between [64,78k], the inference mode kernel of
FLASH SIGMOID is12.28% faster than FLASH ATTENTION 2for self-attention and 5.30% for causal
attention. The training mode kernels of FLASH SIGMOID are14.64% faster than FLASH ATTENTION 2
for self-attention and 6.80% for causal attention. Note that inference involves only the forward pass
of the model and training involves both the forward and the backward pass of the model.
41

--- PAGE 42 ---
Significance of Wall-Clock Speed-Up of Inference: Although FLASH SIGMOID provides only
modest gains during training, the speed-up in inference is significant ( >15% for underlying kernels
and5−10% during inference of full runs). We posit that this speed-up in inference is extremely
critical as well. Contemporary large-scale models, once trained, spend a huge portion of the rest their
lifetime in inference mode (OpenAI, 2023). Thus, significant performance boosts in inference mode
have immense potential for saving resources in deployment of large models for inference.
F.4 F LASH SIGMOID with ALiBi
It is evident from the main text of the paper that improved positional embeddings, like ALiBi (Press
et al., 2022), can be crucial for certain tasks and data modalities. Thus, we also provide a FLASH -
SIGMOID implementation that incorporates ALiBi. We compare the FLASH SIGMOID with ALiBi
implementation with the F LASH ATTENTION 2 with ALiBi implementation (Dao, 2023). Figures 15
and 16 show the kernel GPU time for the forward and backward pass kernels of FLASH SIGMOID
with ALiBi implementation versus FLASH ATTENTION 2with ALiBi implementation. Again, we
observe that FLASH SIGMOID kernels for inference have significant speed-up in wall-clock time over
those in FLASH ATTENTION 2and the kernels for training also have modest wall-clock improvements.
F.5 Directions for Future Work on F LASH SIGMOID
In this section, we discussed FLASH SIGMOID , a hardware-aware implementation of the SigmoidAttn
algorithm. Then, we demonstrated via kernel benchmarking and realistic setting runs that FLASH -
SIGMOID provides significant gains in inference as well as modest gains in training of models with
attention mechanism. In this subsection we further discuss additional avenues for improving the
implementation of F LASH SIGMOID , and point out some interesting directions for future work.
Optimization of Block Shapes for Different Input and GPU Settings: As stated before, our
FLASH SIGMOID implementation builds on FLASH ATTENTION 2by adding functionality for forward
and backward pass of sigmoid attention in place of the standard softmax attention. In particular,
for all FLASH SIGMOID results discussed so far, we inherit directly from FLASH ATTENTION 2the
details of optimal block shapes, grid shapes, and other kernel launch parameters, and keep them
unchanged in our implementation. For instance, this is the case for the block sizes Br, Bcin Alg. 1
and 2, which are identical in FLASH ATTENTION 2andFLASH SIGMOID . This choice is dictated by
the need to ensure a fair comparison between the two implementations, and allows us to demonstrate
the speed-up of sigmoid attention by minimizing confounders associated with parallel computations
on different GPU architectures for different input shapes.
Although FLASH SIGMOID kernels lead to speed-ups in inference and training for both H100 and
A100 GPUs, we observe that the kernel timing speed-ups on A100 are not uniform across sequence
lengths: for a small subset of these, our kernel provides significantly lower speed-up compared to the
overall trend for other sequence lengths. Ideally, the implementation of attention mechanisms should
not assume any information on the token count in input, and it is then desirable to have uniform speed-
ups across all input lengths. Here, we show that this is achievable by simply updating the block shape
information in FLASH SIGMOID to values that are different than those in FLASH ATTENTION 2. The
implementation of FLASH ATTENTION 2is templated according to block shapes, grid shapes, and other
kernel launch parameters. Note that FLASH ATTENTION 2provides various tailored implementations,
optimized for different input shapes (e.g., different ranges of feature dimension per head), input
types (e.g., causal attention vs. self-attention, ALiBi vs. no ALiBi in attention, etc.), and GPU types
(e.g., A100 vs. H100 via checking shared memory size on GPUs). This is achieved by opportunely
selecting the kernel template parameters defining block shapes, grid shapes, and other kernel launch
parameters for parallel computation on GPUs. In our case, we create a variant of FLASH SIGMOID ,
denoted by FLASH SIGMOID†, where we update the block sizes for query and key tensors from
(Br, Bc) = (128 ,128) ofFLASH SIGMOID to(Br, Bc) = (128 ,64)ofFLASH SIGMOID†only for
our input setting (template with features per head being 64).
Experimentation and Results: For this variant, we perform kernel benchmarking as described
in App. F.2, and report the corresponding results in Fig. 17. Comparing the plots for kernel timing
with FLASH SIGMOID plots from Fig. 12, we observe that FLASH SIGMOID†not only provides a more
uniform inference and training kernel speed-up on allsequence lengths, but also improves the average
of these speed-ups across all lengths. To further bolster our observations, Tab. 7 shows the inference
mode and training mode kernel speed-ups for a subset of sequence lengths under consideration. This
42

--- PAGE 43 ---
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
050010001500200025003000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Full)
(a) Inference mode kernels on A100.
0 10000 20000 30000 40000 50000 60000 70000
Tokens 
0200040006000800010000Kernel GPU Time (ms) 
FlashAttention2 (Full)
FlashSigmoid (Full)
FlashAttention2 (Causal)
FlashSigmoid (Full)
 (b) Training mode kernels on A100.
Figure 17: On average, for sequence lengths between [64,78k], the inference mode kernel of
FLASH SIGMOID†is14.82% faster than FLASH ATTENTION 2for self-attention and 18.02% for causal
attention. The training mode kernels of FLASH SIGMOID†are6.18% faster than FLASH ATTENTION 2
for self-attention and 5.76% for causal attention. Note that inference involves only the forward pass
of the model and training involves both the forward and the backward pass of the model.
experiment indicates that it is possible to obtain higher and more uniform speed-ups in kernel timings
across a wide range of tokens by investigating optimal block shape, grid shape, and other kernel
launch parameters for each input setting and GPU type. We leave this optimization for future work.
TOKENSKERNEL GPU T IMECOMPARISON
KERNELS FLASH ATTENTION 2 (MS) F LASH SIGMOID (MS) F LASH SIGMOID†(MS)
4096FWD 8.32±0.02 7 .84±0.03 (−5.79%) 7 .26±0.02 (−13.21%)
FWD+BWD 31.81±0.08 31 .11±0.08 (−2.19%) 30 .62±0.09 (−4.03%)
8100FWD 33.65±0.09 27 .92±0.07 (−17.04%) 28 .54±0.07 (−15.50%)
FWD+BWD 128.18±0.13 119 .04±0.12 (−7.13%) 119 .85±0.13 (−6.81%)
10000FWD 51.17±0.07 42 .49±0.06 (−16.96%) 43 .53±0.09 (−15.32%)
FWD+BWD 194.54±0.14 180 .59±0.15 (−7.17%) 181 .97±0.17 (−6.87%)
16384FWD 134.19±0.12 125 .43±0.10 (−6.53%) 116 .75±0.10 (−13.40%)
FWD+BWD 494.65±0.28 482 .08±0.23 (−2.54%) 474 .52±0.28 (−4.48%)
Table 7: FLASH ATTENTION 2vs.FLASH SIGMOID vs.FLASH SIGMOID†on A100 nodes. The kernel
GPU time for all three approaches are reported in milliseconds. We observe that FLASH SIGMOID†
provides better and more uniform speed-ups across all example tokens.
G Experiments
G.1 Extra Ablations
G.1.1 The Effect of Multiplicative Sequence Length Normalization
Wortsman et al. (2023a) notes that models trained with sigmoid or ReLU attention require scaling
by the sequence length, n−ασ(QKT/p
dqk)V. We ablate this by comparing the scaled solution to
the one we propose in App. E. We also generalize the variant proposed in (Wortsman et al., 2023a)
to variadic sequence lengths such that it works with auto-regressive (AR) training, for example for
n= 3:

1 1 1
0.5−α0.5−α1
0.33−α0.33−α0.33−α

| {z }
n−α⊙"1 0 0
1 1 0
1 1 1#
|{z}
Causal Mask M⊙σ(QKT/p
dqk)V. (84)
43

--- PAGE 44 ---
Figure 18: b=−lnn.
 Figure 19: n−1normalization.
Figure 20: n−0.5
normalization.
We repeat the experiment from Fig. 5, using ALiBi positional embeddings for all trials. We apply
α={1,0.5}AR normalization proposed in (84). While there is an observable difference in terms
of the attention norm, ∥σ(QKT/p
dqk)V∥, we find that the train NLL is slightly worse for both
normalized variants (Fig. 19 and 20) in comparison to the b=−lnnvariant in Fig. 18.
G.1.2 Attention Bias Stability Ablation
To validate the stabilizing effects of attention bias we repeat the experiment from Fig. 8 and 9, keeping
all of the same hyper-parameters, while enabling QK norm and LayerScale (initialized at 10−4). We
train with a range of constant bias offsets, b∈ {− 15,−10,−6,−4,−1}and visualize the results
below in Fig. 21. We observe a systematic increase in stability (and lower SigmoidAttn NLL) for
103
102
101
Learning rate3.03.54.04.55.05.56.06.57.07.5Final loss
attn_bias=-15
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
103
102
101
Learning rate
attn_bias=-10
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
103
102
101
Learning rate
attn_bias=-6
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
103
102
101
Learning rate
attn_bias=-4
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
103
102
101
Learning rate
attn_bias=-1
which_attn_act_name
sigmoid
softmaxNon-embed
Params (M)
2.2
4.9
15.0
67.0
440.0Non-embed
Params (M)
2.2
4.9
15.0
67.0
440.0
107108
Non-embed Params (M)101
LR sensitivity
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
107108
Non-embed Params (M)
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
107108
Non-embed Params (M)
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
107108
Non-embed Params (M)
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
107108
Non-embed Params (M)
which_attn_act_name
sigmoid
softmaxwhich_attn_act_name
sigmoid
softmax
Figure 21: Attention bias ablation.
values less than −1up till −10, after which the −15plot shows an over-regularizing effect with
decreased performance.
G.2 Vision
G.2.1 Test ImageNet1k Top-1%
Fig. 22 reports the test linear probe results for the ViT-B/16 BYOL (Grill et al., 2020; Busbridge
et al., 2023), ViT-B/16 SimCLR (Chen et al., 2020; Zhai et al., 2023a) and the finetuned performance
for the ViT-L/16 MAE (He et al., 2022) and the test top-1% results for for ViT-B/16 supervised model
(Dosovitskiy et al., 2021). Across these wide range of SSL and supervised learning tasks, trained
with contrastive (SimCLR), EMA distillation (BYOL) and reconstructive objectives (MAE), we find
thatSigmoidAttn not only matches the training dynamics (Fig. 2), but also the linear probe and
finetuned performance of the baseline SoftmaxAttn .
44

--- PAGE 45 ---
0 200 400 600
Epoch0.020.040.060.080.0Test Top-1%BYOL ViT-B/16
0 100 200 300
EpochSimCLR ViT-B/16
0 10 20 30 40 50
EpochMAE ViT-L/16 Finetune
0 100 200 300
EpochSupervised ViT-B/16
Softmax
SigmoidFigure 22: ImageNet1k test top-1% for SoftmaxAttn vs.SigmoidAttn using models from Fig. 2.
G.2.2 LayerScale Free Sigmoid Attention
0.02.34.77.0NLLTrain Test
t=10, b=-10
t=-10, b=-10
no t, no b
n−aReLU2
n−aSigmoid
0 100 200 300
Epochs0306085Top-1 %
0 100 200 300
Epochs
Figure 23: A competitive SigmoidAttn
ViT-B/16 model can be learned with-
out LayerScale or QK norm using a
large initial learnable scalar tempera-
turet= 10 and bias b=−10(sim-
ilar to SigLIP (Zhai et al., 2023b)):
σ(et[QKT/p
dqk] +b)V,{b, t} ∈R.
This regularizes the model, as it must
move the temperature to a learnable
regime. The t= 10 , b=−10curve
makes no progress in train NLL or test
top-1 for ∼25 epochs (near max LR), but
ultimately outperforms baselines.
While Fig. 23 demonstrates the possibility of learning SigmoidAttn without LayerScale, it involves
task specific tuning of {t, b}. We also explored gating attention from learning (through a simple
multiply by zero) for ∼25 epochs and were able to match the t= 10, b=−10training curves from
above. However, we opted for the LayerScale method due to its simplicity.
G.2.3 Sigmoid Attention vs. Attention Relaxations
0 50 100 150 200 250 300
Epoch0.020.040.060.080.0Test Top-1%Supervised ViT-B/16
Softmax
Sigmoid
Performer
Linear
Figure 24: Supervised ViT-B/16 ImageNet1k
classification. We contrast SigmoidAttn and
SoftmaxAttn against (a) linear attention with
no activation: QKT/p
dqkand (b) fast atten-
tion via positive orthogonal random features,
used in Performer (Choromanski et al., 2021).
SigmoidAttn , like SoftmaxAttn , differs from
attention relaxations like Performer which uses
low-rank representations of the attention ma-
trix.SigmoidAttn maintains performance parity
withSoftmaxAttn , while outperforming other
efficient attention variants.
45

--- PAGE 46 ---
G.2.4 Hyper-Parameters
Table 8: SigmoidAttn SimCLR and BYOL ViT-B/16 hyperparameters.
Parameter SimCLR BYOL
Attention bias None None
LayerScale Init 10−410−4
QK Norm Yes Yes
Pos Embed SinCos Learnable
Freeze Patcher Yes No
Weight init MocoV3 (Chen et al., 2021) trunc_normal(.02)
Normalization LayerNorm LayerNorm
LR schedule Single Cycle Cosine Single Cycle Cosine
LR warmup 10 Epochs 40 Epochs
Min LR 1×10−61×10−6
Training duration 300 Epochs 600 Epochs
Optimizer AdamW AdamW
Optimizer scaling rule Linear Linear
Base Adam ( β1, β2) (0.9, 0.95) (0.9, 0.95)
Base LR 2×10−41×10−4
Base batch size 256 256
Total batch size 4096 4096
Base teacher momentum - 0.996
Weight decay 0.1 0.3
Weight decay skip bias Yes Yes
Numerical precision bf16 bf16
Stochastic depth 0.0 0.2
Augmentation stack SimCLR (Chen et al., 2020) DINO multicrop (Caron et al., 2021)
Color Jitter Scaling 0.5 (Chen et al., 2021) 1.0
Table 9: SigmoidAttn Supervised ViT-B/16 and MAE ViT-L/16 hyperparameters.
Parameter Supervised MAE
Attention bias None b=−lnn
LayerScale Init 10−410−4
QK Norm Yes Yes
Pos Embed Learnable Learnable
Architecture ViT-B/16 ViT-L/16
Mask Ratio - 0.75
Freeze Patcher No No
Weight init trunc_normal(.02) trunc_normal(.02)
Normalization LayerNorm LayerNorm
LR schedule Single Cycle Cosine Single Cycle Cosine
LR warmup 20 Epochs 40 Epochs
Min LR 1×10−60.0
Training duration 300 Epochs 400 Epochs
Optimizer AdamW AdamW
Optimizer scaling rule Linear Linear
Base Adam ( β1, β2) (0.9, 0.95) (0.9, 0.95)
Base LR 1×10−41.5×10−4
Base batch size 256 256
Total batch size 4096 4096
Weight decay 0.3 0.05
Weight decay skip bias Yes Yes
Numerical precision bf16 bf16
Stochastic depth 0.28 0.0
Augmentation stack RandAug (Cubuk et al., 2020) RRC + HFLIP
G.3 Language Model
G.3.1 Hyper-Parameters
Tab. 10 shows the hyper-parameters for the final comparison. MuP-simple (Wortsman et al., 2023b)
is used, where the peak learning rate is set to 1e-2. Weight decay is decoupled, following Loshchilov
& Hutter (2017). In addition, to confirm that applying QK-Norm does not hurt the baseline, we show
training parity with and without QK-Norm in Fig. 25.
46

--- PAGE 47 ---
Table 10: Training details for the Llama-style 1B LM training.
Parameter Value
Params 1B
Context Length 2048
Total Tokens 300B
Batch size 4M tokens
LR Schedule Cosine
LR Warmup Steps 5000
Peak LR 1e-2
Final LR 10% of peak
Optimizer AdamW
Optimizer momentum 0.9, 0.95
Weight decay 1e-4
Gradient clipping 1.0
Position encoding ALiBi
Q/K Norm Applied
Norm type RMSNorm (Zhang & Sennrich, 2019)
Norm structure Pre-norm
Num layers 24
Num heads 32
Hidden dim 2048
0 50000 100000 150000 200000 250000 300000
Steps2.002.252.502.753.003.253.503.75Train NLLWithout QK Norm
With QK Norm
Figure 25: 1B SoftmaxAttn LLM training
with and without QK Norm, converging to the
same loss.
0 50000 100000 150000 200000 250000 300000
Steps0.00.51.01.52.02.5Gradient Norm85M
1BFigure 26: 85M and 1B LLM training using
SigmoidAttn (n = 4096). Smooth training
loss curves, but gradient norm shows spikes.
0 50000 100000 150000 200000 250000 300000 350000 400000
Steps2.502.753.003.253.503.754.00Train NLLSoftmax
Sigmoid
Figure 27: 85M training using SigmoidAttn
andSoftmaxAttn (n = 4096). Training loss
matches.
0 50000 100000 150000 200000 250000 300000
Steps2.02.53.03.54.0Train NLLSoftmax
SigmoidFigure 28: 1B training using SigmoidAttn (n
= 4096). Higher sequence length with a larger
model shows a slightly different loss curve.
G.3.2 Gradient Norm
While a SigmoidAttn based LM using aforementation hyper-parameters has a smooth loss curve,
we do see more gradient norm fluctuations. See Fig. 26, where spikes larger than 0.5are not visible
in the SoftmaxAttn equivalent.
47

--- PAGE 48 ---
Table 11: Different norm structure ablations for SigmoidAttn with 1B language-modeling.
MODELSEQ.
LEN.ATTN.
BIASPOS.
ENCOD .NORMARC
EASYARC
CHALL .HELLA -
SWAGPIQA SCIQWINO-
GRANDELAMBADA
OPENAITRIVIA QA
(1-SHOT )WEBQS
(1-SHOT )AVG
SOFT. 2 K NO ALIBIPRE 62.2 26.8 42.4 59.0 72.3 88.1 58.4 19.9 15.4 49.4
SOFT. 2 K NO ROPE P RE 64.5 30.4 43.9 61.0 71.9 88.7 59.3 21.1 15.0 50.6
SIGM. 2 K YES ALIBIPRE 62.8 28.8 42.5 59.7 70.3 88.6 59.7 19.1 13.8 49.5
SIGM. 2 K YES ROPE P RE 62.2 26.9 41.4 57.9 71.1 87.8 57.3 17.3 12.8 48.3
SIGM. 2 K NO ROPE P RE 59.3 26.5 39.4 55.1 69.4 88.2 59.2 11.7 7.5 46.0
SOFT. 2 K NO ALIBIHYBRID 64.9 30.5 43.3 61.9 71.6 88.4 60.9 23.6 12.8 50.9
SIGM. 2 K YES ALIBIHYBRID 60.5 26.9 42.2 59.2 70.8 89.6 57.9 17.7 13.4 48.7
SIGM. 2 K NO ALIBIHYBRID 62.8 28.2 42.0 59.1 70.3 88.7 59.8 18.6 15.1 49.4
SOFT. 4 K NO ROPE P RE 63.3 29.3 43.3 58.1 71.3 86.9 58.8 20.4 15.6 49.7
SOFT. 4 K NO ALIBIPRE 62.6 27.7 42.4 58.6 71.1 88.2 58.6 18.9 14.7 49.2
SIGM. 4 K YES ALIBIPRE 60.5 27.3 41.3 57.8 70.5 87.0 57.6 18.9 12.6 48.2
SOFT. 4 K NO ROPE H YBRID 64.1 27.2 43.3 61.4 71.2 88.5 60.0 21.4 15.3 50.3
SOFT. 4 K NO ALIBIHYBRID 61.7 26.8 43.4 59.4 70.6 88.6 60.8 20.5 12.9 49.4
SIGM. 4 K NO ROPE H YBRID 63.3 27.1 43.4 61.3 70.4 88.2 57.5 20.5 14.8 49.6
SIGM. 4 K YES ALIBIHYBRID 63.5 28.1 43.5 60.7 70.8 88.9 59.0 20.9 16.0 50.2
SIGM. 4 K NO ALIBIHYBRID 62.4 28.9 43.5 60.8 71.3 89.6 59.2 20.2 14.3 50.0
G.3.3 Norm structure
Due to the slight performance difference observed at 4096 context length when using SigmoidAttn
versus SoftmaxAttn , and marginally lower downstream results, we evaluated various norm struc-
tures to address potential instabilities (see Tab. 11). Some of these structures replace the required
attention bias (in this case, column ’Attn. Bias’ is ’No’). All use QK-norm with RMSNorm
(Zhang & Sennrich, 2019), without LayerScale. We examined pre-norm and hybrid-norm (where
we do both pre-norm and normalization of the output of the attention layer following Xiong et al.
(2020)): norm(σ(QKT/p
dqk)V). Post-norm, which normalizes the combined residual data stream,
norm(x+σ(QKT/p
dqk)V), is omitted from our analysis as it did not train stably for SigmoidAttn .
G.4 Automatic Speech Recognition
G.4.1 Training Details
All acoustic models are fed 80 channel log-mel filterbanks with a 25ms sliding window strided by
10ms.
The transformer-based encoder model has 255M parameters: 1D convolution of kernel 7 and stride 3
followed by CAPE positional embedding if it is used and 36 transformer blocks with pre-LayerNorm,
an embedding dimension of 768, 4 heads, 3072 units in the MLP layers. The model is trained with
CTC loss and a character vocabulary, including apostrophe (‘). In additional experiments, we vary
the depth to 12 and 24 layers, and change pre-LayerNorm to post-LayerNorm.
We implemented our own conformer-based encoder model, also trained with a CTC loss and a
character vocabulary. The conformer model has 104M parameters and consists of 1D convolution of
kernel 7 and stride 3 followed by 16 conformer blocks with an embedding dimension of 512, 4 heads,
2048 units in the MLP layers. Variational noise is not used and RoPE is used as a relative positional
embedding instead of relative sinusoidal positional embedding.
For all models, SpecAugment (Park et al., 2019) is used for augmentation with 2 frequency masks
(max width 30) and 10 time masks (max width 50, ratio 0.1). All models are trained with dynamic
batching and mixed precision with BF16. Models are trained with different configurations of
optimizers and hyperparameters to have diverse coverage of use-cases. We first optimize every
configuration for SoftmaxAttn and then change only attention to the introduced configuration
ofSigmoidAttn while all other parameters are kept the same. Detailed configurations are shown
in Table 12. We train models until the greedy WER stops improving on the validation sets ( dev-clean,
dev-other ) and report final test sets ( test-clean, test-other ) greedy WER without integration of any
external language model.
For the bias term b=−logninSigmoidAttn , we do not use max sequence length as in language
model experiments. Instead, for every audio sample we use its own duration as a bias terms resulting
48

--- PAGE 49 ---
Table 12: Training details for the ASR models on LibriSpeech 100h (LS-100) and LibriSpeech 960h
(LS-960) for transformers and conformers.
Parameter Transformer LS-960 Conformer LS-960 Transformer LS-100 Transformer LS-100
Params 255M 104M 255M / 170M / 85M 255M
LayerNorm pre pre + post pre post
Dropout 0.1 0.1 0.3 0.3
Layer drop 0.1 0.0 0.3 0.3
Training steps 400k 400k 400k 500k
Batch size 3.56h 4.44h 1.1h 1.1h
LR schedule step-wise step-wise step-wise step-wise
SpecAugment start 0k 10k 0k 0k
LR Warmup Steps 64k 10k 64k 64k
Peak LR 1e-3 2e-3 0.1 0.03
LR start decay 250k 250k 200k 330k
LR decay step 50k 50k 30k 50k
Optimizer AdamW AdamW Adagrad Adagrad
Optimizer momentum 0.9, 0.999 0.9, 0.98 - -
Weight decay 1e-6 1e-6 0 0
Gradient clipping 1.0 0.5 1.0 1.0
Position encoding CAPE / ALiBi / RoPE RoPE CAPE CAPE / ALiBi / RoPE
Q/K Norm SoftmaxAttn Not Applied Not Applied Not Applied Not Applied
Q/K Norm SigmoidAttn Applied Applied Not Applied Applied
Num layers 36 16 36 / 24 / 12 36
Num heads 4 4 4 4
Table 13: Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3 (Hernandez et al.,
2018) (“TED”, joint validation and test sets with split according to audio duration) for pre-LayerNorm
transformer (255M / 170M / 85M params) with CAPE and with either SoftmaxAttn orSigmoidAttn
(w/ LayerScale, w/o QK norm, w/ b=−logn) trained on LibriSpeech 100h data (average duration
is 10-15s). Hyper-parameters can be found in Table 12.
ATTN #LAYERS DEV -CLEAN TEST -CLEAN DEV -OTHER TEST -OTHER TED 0-10 S TED 10-20 S TED 20-30 S TED 30S+
SOFTMAX 36 6.7 7.1 20.0 20.4 26.4 22.4 23.3 21.8
SIGMOID 36 7.0 7.3 20.3 20.5 26.2 23.4 23.6 21.8
b= 0 36 6.8 7.1 19.8 20.3
SOFTMAX 24 6.4 6.8 20.2 20.5 25.4 22.1 23.3 21.8
SIGMOID 24 7.1 7.3 21.0 21.3 26.6 23.3 24.0 22.0
b= 0 24 6.7 6.9 20.2 20.7
SOFTMAX 12 8.2 8.7 25.0 25.4 29.0 25.6 27.1 27.4
SIGMOID 12 8.3 8.7 24.8 25.2 29.0 25.7 26.3 25.5
b= 0 12 8.7 8.5 24.4 24.7
into non-trainable bias vector for the minibatch. For experiments with sequence normalization, we
also use not the max sequence length in the minibatch but rather the ground truth sample duration to
properly normalize encoder attention.
To evaluate behaviour for length generalization we use TED-LIUM v3 dataset Hernandez et al.
(2018) as its validation and test sets have longer audio duration than LibriSpeech: LibriSpeech has in
average 10-15s duration, while in TED-LIUM there are audio longer than 30s (the max duration of
LibriSpeech). To perform evaluation on TED-LIUM v3, we combine together validation and test sets
of TED-LIUM v3 (we don’t use them for training and hyper-parameters search and just perform final
evaluation) and split them into 4 datasets according to the duration: 0-10s, 10-20s, 20-30s, and 30s+.
For positional embeddings we use not only CAPE, but change it to AliBi or RoPE. As ALiBi
was originally introduced for the decoder only models and there is no official adoption of it yet14
for the encoder models (without causal masking), we follow the best practices found in https:
//iclr-blogposts.github.io/2024/blog/alibi-mlm/ of nonsymmetric ALiBi
with different slopes instead of symmetric version used by (Lee et al., 2022).
14See discussion in https://github.com/ofirpress/attention_with_linear_
biases/issues/5 .
49

--- PAGE 50 ---
Table 14: Word error rate (%) on LibriSpeech dev/test sets for post-LayerNorm transformer (255M)
with either SoftmaxAttn (w/o QK norm) or SigmoidAttn (by default w/ LayerScale, w/ QK norm,
w/b=−logn) trained on LibriSpeech 100h data. Hyper-parameters can be found in Table 12.
ATTN PE DEV-CLEAN TEST -CLEAN DEV -OTHER TEST -OTHER
SOFTMAX CAPE 6.4 6.5 18.4 18.2
+ QK NORM 6.1 6.3 18.2 18.1
SIGMOID 8.0 8.4 22.7 22.7
- QK NORM 7.5 7.9 22.1 27.6
- LAYER SCALE UNSTABLE ,GRADIENT NORM AND LOSS SPIKES
- QK NORM - LAYER SCALE 6.5 6.9 19.9 20.1
SIGMOID (b=−10,LEARNABLE ) 8.7 9.4 23.5 24.0
SOFTMAX ROPE 6.6 6.9 18.3 18.5
SIGMOID 6.8 7.1 20.8 20.8
SIGMOID (b=−10,LEARNABLE ) 8.7 9.4 23.5 24.0
SOFTMAX ALIBI 6.4 6.9 18.3 18.3
SIGMOID 6.9 7.2 20.8 21.1
SIGMOID (b=−10,LEARNABLE ) 6.8 7.1 20.4 20.5
Table 15: Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3 (Hernandez et al.,
2018) (“TED”, joint validation and test sets with split according to audio duration) for conformer
(104M) with RoPE and with either SoftmaxAttn orSigmoidAttn (w/ LayerScale, w/ QK norm, w/
b=−logn) trained on LibriSpeech 960h data (average duration is 10-15s). Hyper-parameters can
be found in Table 12.
ATTN DEV -CLEAN TEST -CLEAN DEV -OTHER TEST -OTHER TED 0-10 S TED 10-20 S TED 20-30 S TED 30S+
SOFTMAX 2.2 2.5 5.4 5.6 13.0 11.1 13.2 7.1
SIGMOID 2.3 2.5 5.6 5.8 13.5 10.8 13.3 10.2
SIGMOID (b=−10,LEARNABLE ) 2.4 2.7 5.8 5.8 12.9 11.1 14.1 54.9
G.4.2 Results and Ablations
Initial investigation on post-LayerNorm and pre-LayerNorm transformers on both LibriSpeech 100h
and 960h revealed that SigmoidAttn without any bias is unstable resulting in huge and frequent
gradient norm and training loss spikes throughout the training which in turn result in spikes of
validation and test WER, see Figure 29. Neither LayerScale nor QK norm were able to stabilize the
training, though we did not observe any model divergence.
Further experiments with bias term in the SigmoidAttn definition for post-LayerNorm transformers
on LibriSpeech 100h reveal that training is now stable (only few marginal spikes in gradient norm
occur, while train loss is smooth all the time). However, both LayerScale and QK norm restrict
model capacity thus not matching SoftmaxAttn . Moreover, some combination of them is needed
for the stable training, though w/o both of them we got the best performance for SigmoidAttn (still
behind SoftmaxAttn ), see Table 14. We believe, further adaptation and deeper investigation is
needed for SigmoidAttn and post-LayerNorm, though recent advances in machine learning do not
use post-LayerNorm models due to high training instability even for SoftmaxAttn .
Switching to pre-LayerNorm transformers and varying the depth of the models lead to stable training
withSigmoidAttn and bias term b=−lognwith few (2-5 times) spikes in the gradient norm and
smooth loss. In this case, SigmoidAttn matches results for SoftmaxAttn and they both generalize
to TED-LIUM data similarly, see Table 13. If the bias term is removed, SigmoidAttn can still match
SoftmaxAttn but large spikes in gradient norm and loss can occur.
Finally, we experiment with a conformer model, in Table 15. Again, we found that bias term
b=−lognstabilizes training. The learnable b=−10works though we see significant gradient
norm spikes while the train loss remains smooth. Besides, b=−logngeneralizes well to longer
sequences while learnable b=−10fails to do so with RoPE for conformer. Overall, SigmoidAttn
is able to match SoftmaxAttn having stable training with b=−logn.
In experiments with different variants of bias term for SigmoidAttn , the bias b=−lognis found
to be the most stable (only few marginal gradient norm spikes are observed with the train loss
being smooth) and it provides similar performance as SoftmaxAttn in most settings. The source of
50

--- PAGE 51 ---
100200300400500600700Train Losspost-LayerNorm pre-LayerNorm
softmax
sigmoid
102103/vextenddouble/vextenddoubledL
dW/vextenddouble/vextenddouble
0.02.55.07.510.012.515.017.520.0dev-clean WER
0 100000 200000 300000 400000 500000
Steps68101214161820dev-other WER
0 100000 200000 300000 400000 500000
StepsFigure 29: ASR Transformer model (255M) training with post-LayerNorm (left) and pre-LayerNorm
(right) on LibriSpeech 960h with SigmoidAttn (w/ bias term, b= 0, w/o QK norm, w/ LayerScale)
or with SoftmaxAttn . Huge gradient norms and training loss spikes are observed for SigmoidAttn
which can result in worse final model performance hence models for SigmoidAttn are unstable.
instability is coming from the larger attention output norms (80k for CAPE, 40k for RoPE and 20k for
AliBi while being 200 for SoftmaxAttn ). This happens due to high attention weight of every token
which can be biased towards zero with a bias term in SigmoidAttn definition. Preliminary results to
connect this to the local attention property needed at the beginning of the training for stable training
failed, as local attention did not converge well at all (it is deactivated after some initial training).
To fully benefit from the improved throughput of FLASH SIGMOID , for the bias term b=−logn
inSigmoidAttn , we experimented with configuration when the maximum audio duration in the
minibatch is used as nresulting into non-trainable bias scalar which changes between minibatches
as we use dynamic batching. Comparison between the bias vector with per sample own duration
normalization and the bias scalar as maximum duration in the minibatch is shown in Table 16: final
model performance is similar and stability is same (only 2-3 minor spikes in CAPE for gradient
norms are observed). Thus, per batch maximum audio duration can be used with b=−lognas the
final configuration.
We also experimented with hybrid-norm (see Table 16) to check if it is able to stabilize the attention
magnitudes as well as gradient norms. We did ablation with configuration similar to Table 16 with
the following changes: LayerScale after attention is replaced to LayerNorm, only RoPE is used for
positional embedding; we either keep or remove QK-norm and we either keep or remove LayerScale
in MLP part of transformer block.
First, for all variants we observe that training loss and gradient norms are smooth without any spikes
during training while we see abnormally large attention activations compared to all prior experiments.
Second, while we observe that QK-norm or its removal behave similarly, the LayerScale on top of
MLP output is necessary to get performance on par with SoftmaxAttn or with SigmoidAttn with
bias term.
51

--- PAGE 52 ---
20406080100Train Lossalibi
rope
cape
0 50000 100000 150000 200000 250000 300000 350000
Steps20406080100/vextenddouble/vextenddoubledL
dW/vextenddouble/vextenddoubleFigure 30: ASR Transformer model
(255M) training with pre-LayerNorm
on LibriSpeech 960h with SigmoidAttn
(w/ bias term, b=−logn, w/ QK norm,
w/ LayerScale) and different positional
embeddings CAPE, RoPE, ALiBi. The
biasbis able to stabilize SigmoidAttn
training: smooth training loss and only
marginal rare spikes in gradient norms
are observed.
Table 16: Word error rate (%) on LibriSpeech dev/test sets and TED-LIUM v3 (Hernandez et al., 2018)
(“TED”, joint validation and test sets split according to duration) for transformer (255M params) with
either SoftmaxAttn orSigmoidAttn (LayerScale and QK norm are used with b=−logn) trained
on LibriSpeech 960h data (mean duration is 10-15s). Hyper-parameters are in App. G.4. H-Norm
corresponds to hybrid-norm, no LS-Attn corresponds to removing the LayerScale from the attention
outputs, and no LS corresponds to removing the LayerScale from both the attention and MLP outputs.
ATTN PE DEV-CLEAN TEST -CLEAN DEV -OTHER TEST -OTHER TED 0-10 S TED 10-20 S TED 20-30 S TED 30S+
SOFTMAX
CAPE2.2 2.3 5.6 5.7 12.4 10.5 11.9 9.1
SIGMOID 2.2 2.4 5.2 5.5 12.4 10.3 12.3 9.7
SIGMOID ,b=−log(max batch n) 2.1 2.3 5.2 5.3 12.2 10.6 12.0 9.3
SOFTMAX
ROPE2.2 2.2 5.4 5.5 12.7 10.6 12.8 9.5
SIGMOID 2.0 2.3 5.2 5.4 12.3 10.1 12.3 8.6
SIGMOID ,b=−log(max batch n) 2.1 2.3 5.0 5.1 12.3 10.1 12.1 10.4
SIGMOID (H-NORM ),NOQK- NORM ,NOLS- ATTN 2.1 2.2 5.0 5.0 11.8 10.2 12.3 10.8
SIGMOID (H-NORM ),NOLS- ATTN 2.1 2.3 5.0 5.1 12.0 10.2 12.4 11.4
SIGMOID (H-NORM ),NOQK- NORM ,NOLS 2.2 2.3 5.6 5.6 13.2 10.9 13.5 11.5
SOFTMAX
ALIBI2.1 2.2 5.3 5.4 12.3 10.7 12.1 8.6
SIGMOID 2.1 2.3 5.0 5.1 12.3 10.5 12.6 9.1
SIGMOID ,b=−log(max batch n) 2.0 2.3 5.2 5.2 12.3 10.5 11.9 10.2
G.5 Simple Experiments
G.5.1 k–Summation Problem Definition
Here we look at a synthetic, simple task in order to investigate the behavior of softmax and sigmoid
attention activations. The problem chosen is to minimize the MSE loss of a Rn→Rtarget function.
In the first half of each input are samples from a N(0,1)distribution, and the second half is a k-hot
binary vector indicating which values in the first half to sum.
The results presented here are for the n= 40 problem with various values for k. Where a transformer
is used, the transformer is a single layer to aid visualization. In all cases (unless noted otherwise),
the optimizer is Adam with a constant learning rate of 0.001, and the training data is continuously
generated to preclude over-fitting.
A few examples for n= 10 (not drawn from N(0,1)) are shown below. Inputs in the second half of
the input are show in orange only as a visual aid.
1 2 3 4 5 0 0 0 0 1 →5
1 2 3 4 5 1 0 0 0 1 →6
8 1 2 0 5 0 1 1 1 0 →3
2 0 2 2 2 1 1 0 1 0 →4
52

--- PAGE 53 ---
G.5.2 Comparison to Softmax
In Figure 31, we see the performance of three architectures on the k-summation problem as k
increases. The sigmoid activated transformer has similar scaling to the softmax activation.
Figure 31: Final loss is shown after training convergence as k-summation problem complexity
increases. The ReLU MLP has two hidden layers (900, 300) for 307k parameters, while the
transformer has an embedding dimension of 120, 8 heads, and an MLP ratio of 4, giving 187k
parameters. The SigmoidAttn is applied after a learned offset initialized to -4, A+param(-4) .
G.5.3 Attention Evolution
In Figures 32 and 33, forty samples are used to monitor the single head, single layer post-activation
attention matrix as training progresses. In Figure 32, the distribution of values is visualized over
time; note the sigmoid attention is more variable but reaches comparable values at convergence. The
main difference at convergence is that the sigmoid has fewer high magnitude values than softmax
indicating a more distributed attention.
Softmax
 Sigmoid
Figure 32: The post-activation attention evolves during training on the k= 1, n= 40 summation
problem. The model has one head to simplify the visualization. Forty repeated test samples are used.
In Figure 33, metrics on the post-activation attention matrices are used and show comparable behavior
in the first half of training. In the second half of training, the SigmoidAttn can be seen to reduce in
norm and in sparsity. (see following discussion of Figure 34 for further insights).
In Figure 34, we see post-activation attention values for eight samples at training progresses. The
most notable difference between the activations is, that by the end of training, the SigmoidAttn is
less sparse in the N(0,1)self-attention in the upper-left quadrant. We can see that softmax tends to
53

--- PAGE 54 ---
Norm
 Hoyer Sparsity
Figure 33: Metrics on the post-activation attention evolve during training on the k= 1, n= 40
summation problem. The model has one head to simplify the visualization. Quartiles and mean from
40 repeated test samples are shown. On the right, the Hoyer Sparsity (Hurley & Rickard, 2009) is used
to measure the change in sparsity as training progresses: Hoyer :=√n−P
jcj√P
jc2
j
(√n−1)−1.
produce sparser values (as it is designed to) while sigmoid controls the magnitude and location of
peak attention independently, leading to a less sparse attention at the end of training.
Figure 34: For 8 samples, the post-activation attentions is visualized as training progresses on the
k= 1, n= 40 summation problem. The model has one head to simplify the visualization. The
attention is shown in pairs for each sample with softmax attention is in black and sigmoid is in blue.
A2×2block structure is evident in both cases, resulting from each halve of the input containing
different information.
G.5.4 Pair Repeat Problem
We define a synthetic task of identifying if the first two symbols in a sequence repeat. The symbols,
sibelow come from a fixed vocabulary of size K, and the repeat location (when present) is uniformly
distributed in the sequence.
f(s0, s1, s2, ..., s N) =1,if∃n >1|(s0, s1) = (sn, sn+1),
0 otherwise
A simple two layer transformer is trained on this problem. The model has an embedding dimension
of 160, MLP ratio of 4, QK norm, and layers with eight heads. The results for different model
54

--- PAGE 55 ---
architectures are shown in Figure Figure 35. The maximum input length is 22, K= 9, shorter lengths
are padding with value K, and the training set only contains lengths 14 and 15. A cosine learning
rate schedule with 5% linear warmup and a maximum learning rate of 1e-3 is used with the Adam
optimizer.
In this result, we see the sigmoid activation has higher data efficiency and similar fall-off in the out of
distribution cases. From shorter runs, we estimate that the softmax network would fit the training
with 4–5x more data. Our conjecture is that the two layer transformer more easily learns the pair
finding task with sigmoid because softmax is biased to focus on single values, though it is unclear
why multiple heads are not able to compensate for this proposed cause in the softmax case.
Figure 35: Validation accuracy for out of distribution sequence lengths is shows after 5M samples of
training; trained lengths are shown with vertical lines. Quartiles and means are shown from six trials.
The MLP has two hidden layers, ReLU activation, and a similar number of parameters. The sigmoid
transformer has a learned offset initialized to -4.
G.6 Practitioner’s Guide
In Table 17 we summarize recommended settings for practitioners who aim to use SigmoidAttn for
training in their respective domains / learning scenarios. While each setting has fully enumerated
hyper-parameters listed in the Appendix, we highlight some sane SigmoidAttn choices below.
Table 17: Simplified recipe for different domains and tasks with Sigmoid Attention. S/C/A/R refers
to using any of SinCos, CAPE, ALiBi or RoPE positional encoding methods.
DOMAIN OBJECTIVE MODEL SIZE POSEMBED QK N ORM LAYER SCALE SIGMOID BIAS NORM STRATEGY
VISION SUPERVISED 87M L EARNABLE YES YES NO PRE-NORM
BYOL 87M L EARNABLE YES YES NO PRE-NORM
SIMCLR 87M S INCOS YES YES NO PRE-NORM
MAE 304M L EARNABLE YES YES YES PRE-NORM
ASR S UPERVISED (CTC) 100M-250M S/C/A/R Y ES YES YES PRE-NORM
SUPERVISED (CTC) 100M-250M R OPE N O YES NO HYBRID -NORM
AR L ANGUAGE NEXT-TOKEN (<=2 K SEQ LEN ) 1B AL IBI YES NO YES PRE-NORM
NEXT-TOKEN (<=2 K SEQ LEN ) 1B AL IBI YES NO NO HYBRID -NORM
NEXT-TOKEN (>2 K SEQ LEN ) 1B AL IBI YES NO NO HYBRID -NORM
Stabilizing larger models beyond sequence length 2048: We propose a non-learned scalar
bias to mitigate large attention norms with SigmoidAttn (Appendix E), but observe instabilities
at sequence length n= 4096 for autoregressive language modeling (Section 5.5). Hybrid-norm
(without learnable affine parameters) resolves these instabilities (Table 11). Hybrid-norm differs
from post-norm, which normalizes the combined residual data stream and attention block output.
Hybrid-norm is used in models such as Grok-1 (xai-org, 2024) and frameworks such as Praxis
(Google, 2024) under the normalization strategy "primer_hybrid". When both SoftmaxAttn and
SigmoidAttn use hybrid-norm, we observe similar kernel speedup times as highlighted in Section 4.
55

--- PAGE 56 ---
Post-norm Hybrid-norm
norm(x+σ(QKT/p
dqk)V)x + norm (σ(QKT/p
dqk)V)
However, with LayerNorm (Lei Ba et al., 2016) only for SigmoidAttn , a token length of 10,000
is needed to achieve a performance gain of ∼5.04% for full self-attention and a token length of
1024 is needed to achieve a performance gain of 8.36% for causal self-attention on H100 GPUs.
For LayerNorm (with and without affine terms), we summarize approximate regimes for positive
throughput gains in Table 18.
ATTENTION TYPEFLASH SIGMOID WITH LAYER NORM VERSUS FLASH ATTENTION 2COMPARISON
A100 H100
AFFINE PROJECTION NOAFFINE PROJECTION AFFINE PROJECTION NOAFFINE PROJECTION
FULL 16384 (5.22%↑) 12544 (5.08%↑) 10000 (4.82%↑) 10000 (5.04%↑)
CAUSAL 12544 (4.18%↑) 5184 (4.14%↑) 2048 (7.65%↑) 1024 (8.36%↑)
Table 18: FLASH SIGMOID along with LayerNorm vs. FLASH ATTENTION 2on A100 GPUs. Based
on benchmarking on a set of randomly sampled tokens from the range [64,60000] , we report the token
T∗after which FLASH SIGMOID with normalization consistently outperforms FLASH ATTENTION 2,
along with the total CUDA time speed-up averaged over subsequent tokens ( T > T∗).
H Contributions
All authors contributed to writing this paper, designing the experiments, discussing results at each
stage of the project.
Preliminary work Preliminary viability of SigmoidAttn done by Jason Ramapuram.
Universal Function Approximation Proof of UFA (Section 3.1 and App. C) sculpted by Federico
Danieli.
Lipschitzness of Sigmoid Attention Lipschitzness analysis (Section 3.2 and App. D) molded by
Pierre Ablin.
FlashSigmoid Implementation and analysis driven by Eeshan Dhekane in collaboration with Jagrit
Digani (Section 4 and App. F).
Bias Analysis Theoretical grounding for bias (Appendix E) done by Amitis Shidani in discussion
with Pierre Ablin.
Language Modeling Results All large scale language model pretraining and evaluation (Section 5.5
and App. G.3) driven by Floris Weers.
Stability Analysis QK norm (Figure 9), LayerScale (Figure 8) and bias (Figure 21) ablations crafted
by Dan Busbridge using Attention Simulator. Attention Simulator written by Jason Ramapuram and
used to validate norm growth (Figures 3 to 6, 19 and 20).
ASR Results All ASR experiments (Section 5.4) and ablations (Appendix G.4.2) are conducted by
Tatiana Likhomanenko in discussions with Jason Ramapuram and Zijin Gu. Baseline ASR models
code is written by Zijin Gu and Tatiana Likhomanenko. Baseline models are optimized by Zijin Gu
to be close to state-of-the-art results.
Vision Results All vision experiments (Sections 5.2 and 5.3) and ablations (Figures 10, 23 and 24
and App. G.2.1) conducted and written by Jason Ramapuram.
Simple Experiments Simple experiments to compare SigmoidAttn toSoftmaxAttn , including
visualizing attention evolution and simple sequence length generalization analysis (Appendix G.5)
conducted by Russ Webb.
56
