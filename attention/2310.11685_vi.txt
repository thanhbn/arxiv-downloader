# Ưu Thế của Softmax: Khám Phá Lợi Thế Hiệu Suất So Với
Linear Attention
Yichuan Deng∗Zhao Song†Tianyi Zhou‡

## Tóm Tắt
Các mô hình transformer lớn đã đạt được kết quả tiên tiến trong nhiều tác vụ xử lý ngôn ngữ tự nhiên. Trong số các thành phần quan trọng của kiến trúc transformer, cơ chế attention đóng vai trò then chót trong việc nắm bắt tương tác token trong chuỗi thông qua việc sử dụng hàm softmax.

Ngược lại, linear attention đưa ra một phương án hiệu quả hơn về mặt tính toán bằng cách xấp xỉ hoạt động softmax với độ phức tạp tuyến tính. Tuy nhiên, nó thể hiện sự suy giảm hiệu suất đáng kể khi so sánh với cơ chế attention softmax truyền thống.

Trong bài báo này, chúng tôi thu hẹp khoảng cách trong hiểu biết lý thuyết về những lý do đằng sau khoảng cách hiệu suất thực tế giữa softmax và linear attention. Bằng cách tiến hành phân tích so sánh toàn diện của hai cơ chế attention này, chúng tôi làm sáng tỏ những lý do cơ bản giải thích tại sao softmax attention vượt trội hơn linear attention trong hầu hết các tình huống.

∗ycdeng@cs.washington.edu . The University of Washington.
†zsong@adobe.com . Adobe Research.
‡tzhou029@usc.edu . University of Southern California.arXiv:2310.11685v1  [cs.CL]  18 Oct 2023

---

## 1 Giới Thiệu

Các mô hình ngôn ngữ lớn (LLMs) như Transformer [VSP+17], BERT [DCLT18], RoBERTa [LOG+19], XLNet [YDY+19], GPT-3 [BMR+20], OPT [ZRG+22], PaLM [CND+22], Llama [TLI+23], Llama2 [TMS+23], Adobe firefly [Inc23], và BARD [Man23] đã được chứng minh là những phương pháp hiệu quả để giải quyết các tác vụ ngôn ngữ tự nhiên phức tạp như tạo nội dung, tóm tắt, và hệ thống đối thoại [TDFH+22, YCRI22, WTB+22]. Việc sử dụng các cơ chế attention đã cách mạng hóa bối cảnh của thị giác máy tính và xử lý ngôn ngữ tự nhiên, cải thiện đáng kể hiệu suất mạng. Tuy nhiên, một thách thức quan trọng nằm ở nhu cầu bộ nhớ và tính toán ngày càng tăng liên quan đến cơ chế dot-product attention phổ biến, đặc biệt là khi độ dài đầu vào tăng lên. Độ phức tạp tính toán bậc hai của cơ chế attention này so với số lượng token đã từ lâu cản trở khả năng áp dụng của nó để xử lý các chuỗi dài.

Những nỗ lực gần đây trong cộng đồng nghiên cứu đã được dành cho việc phát triển các kiến trúc attention hiệu quả, nhằm giảm thiểu độ phức tạp tính toán và sử dụng bộ nhớ. Linear attention là một trong những phương pháp được đề xuất đã được nghiên cứu rộng rãi [LSDZ20, KVPF20, SZZ+21a, ZWK22, LCZ+23]. Trong [LSDZ20], họ đề xuất một Cơ chế Attention Tuyến tính xấp xỉ với dot-product attention với chi phí bộ nhớ và tính toán ít hơn nhiều dựa trên xấp xỉ bậc nhất của khai triển Taylor.

Trong các ứng dụng thực tế, softmax attention luôn thể hiện hiệu suất vượt trội khi so sánh với linear attention. Chúng tôi giả thuyết rằng đối với Transformer,
tồn tại một số tập dữ liệu chỉ có thể được phân loại hiệu quả bằng softmax attention, trong khi linear attention chứng tỏ không đủ.

Trong bài báo này, chúng tôi đi sâu vào so sánh toàn diện giữa softmax attention và linear attention, xem xét các thuộc tính của chúng từ cả góc độ thực nghiệm và lý thuyết.

Cấu trúc của chúng tôi được lấy cảm hứng từ các chứng minh độ khó trong độ phức tạp chi tiết [BIS17, Che18, CJW19, ACSS20, AS23a, AS23b, ALS+23, JX23]. Gọi K∈Rd×d, Q∈Rd×d, V∈Rd×d lần lượt là ma trận key, query và value đã học trong lớp attention. Gọi A1, A2 là các chuỗi đầu vào. Công thức của softmax cross attention được định nghĩa là:

Att(X, Y ) =D(X)−1exp(A1XA⊤2)A2Y (1)

trong đó X∈Rd×d ký hiệu tham số kết hợp X:=QK⊤,Y∈Rd×d:=V và D(X) := diag(exp( A1XA⊤2)1n)∈Rd×d ký hiệu chuẩn hóa softmax. Đối với linear cross attention, chúng tôi thay thế exp bằng lin, viết tắt của linear. Lưu ý rằng, self attention là một trường hợp đặc biệt của cross attention, tức là A2=A1. Gọi Fexp đại diện cho mạng neural sử dụng softmax attention và kích hoạt ReLU, trong khi Flin ký hiệu mạng neural sử dụng linear attention. Tiếp theo, chúng tôi nêu ra các kết quả chính.

### 1.1 Kết Quả Của Chúng Tôi

Đối với self-attention, chúng ta cần A1=A2 trong Phương trình (1).

**Định lý 1.1** (Self-attention, không chính thức của Mục D) . Tồn tại hai tập dữ liệu (self-attention) D0⊂Rn×d và D1⊂Rn×d. Tồn tại hai mạng neural bốn lớp: Fexp:Rn×d→R sử dụng các đơn vị softmax và đơn vị ReLU, và Flin:Rn×d→R sử dụng các đơn vị linear attention và đơn vị ReLU sao cho với xác suất cao (tính ngẫu nhiên trên trọng số của mạng)

• Fexp có thể phân biệt D0 và D1

• Flin không thể phân biệt D0 và D1

Đối với cross attention, chúng ta cần A1̸=A2 trong Phương trình (1), và do đó hàm của mạng neural thực sự nhận hai ma trận n×d làm đầu vào.

**Định lý 1.2** (Cross-attention, không chính thức của Mục F) . Tồn tại hai tập dữ liệu (cross-attention) D0⊂R2n×d và D1⊂R2n×d. Tồn tại hai mạng neural bốn lớp: Fexp:Rn×d×Rn×d→R sử dụng các đơn vị softmax và đơn vị ReLU, và Flin:Rn×d×Rn×d→R sử dụng các đơn vị linear attention và đơn vị ReLU sao cho với xác suất cao (tính ngẫu nhiên trên trọng số của mạng)

• Fexp có thể phân biệt D0 và D1

• Flin không thể phân biệt D0 và D1

## 2 Công Trình Liên Quan

Hiệu quả của kiến trúc Transformer đặc biệt dựa hoàn toàn vào cơ chế self-attention [PTDU16, LFS+17] để tính toán một chuỗi các biểu diễn không gian vector có thông tin ngữ cảnh của các ký hiệu trong đầu vào và đầu ra của nó, sau đó được sử dụng để dự đoán phân phối trên các ký hiệu tiếp theo khi mô hình dự đoán chuỗi đầu ra từng ký hiệu một. Bằng chứng cụ thể từ các nghiên cứu [TDP19, VB19, HL19, Bel22] nhấn mạnh vai trò quan trọng của attention với perceptron nhiều lớp trong việc truyền tải thông tin quan trọng, tạo điều kiện cho các tác vụ thăm dò đa dạng.

Các nghiên cứu đương đại đã đi sâu vào khả năng của Transformers. Các chủ đề được khám phá bao gồm từ tính đầy đủ Turing [PMB19, BPG20], biểu diễn chức năng [YBR+20, CDW+21], đến biểu diễn các ngôn ngữ hình thức [BAG20, EGZ20, YPPN21] và việc thành thạo các phép toán đại số trừu tượng [ZBB+22]. Một số nghiên cứu ám chỉ tiềm năng của Transformers để phục vụ như các bộ chuyển đổi phổ quát cho các tác vụ dựa trên chuỗi và thậm chí bắt chước khả năng của máy Turing [PMB19, BPG20].

Tuy nhiên, độ phức tạp tính toán bậc hai của cơ chế attention này so với số lượng token đã từ lâu cản trở khả năng áp dụng của nó để xử lý các chuỗi dài. [DCL+22] đã giới thiệu mô hình Pixelated Butterfly, một chiến lược sử dụng mẫu thưa thớt nhất quán để tăng tốc quá trình huấn luyện Transformer. Performer [CLD+20] là một ví dụ của biến thể low-rank, sử dụng kernelization để tăng tốc tính toán. Cũng có những công trình xấp xỉ các tính toán attention trong giai đoạn suy luận, với điều kiện độ chính xác được duy trì đầy đủ. [LWD+23] làm nổi bật hiện tượng thưa thớt ngữ cảnh trong LLM và khả năng dự đoán của nó. Họ đã tận dụng hiểu biết này để tăng tốc suy luận LLM mà không ảnh hưởng đến chất lượng đầu ra.

Nhiều nghiên cứu, được bao gồm bởi các tài liệu tham khảo như [CGRS19, KKL20, WLK+20, DKOD20, KVPF20, CDW+21, CDL+22], đã làm sáng tỏ các khía cạnh khác nhau của lĩnh vực này. Nghiên cứu tiếp theo, được đại diện bởi các công trình như [ZHDK23, AS23a, BSZ23, DMS23, KMZ23, AS23b, HJK+23, AG23, MDA22], đã đi sâu vào việc tính toán ma trận attention, làm nổi bật sự phức tạp của nó và ủng hộ các thuật toán được tối ưu hóa.

Hơn nữa, những bước tiến đáng kể đã được thực hiện trong việc hiểu sức mạnh của các cơ chế attention trong Transformers, như được minh họa bởi các nghiên cứu [DGV+18, VBC20, ZKV+20, EGKZ21, SZKS21, WCM21, DSX23, DLMS23]. Công trình của [ZPGA23] đã nhấn mạnh tiềm năng của các mô hình ngôn ngữ có mặt nạ quy mô trung bình để nhận biết các thành phần cú pháp, đưa ra khả năng cho việc tái tạo cây phân tích từng phần. Khái niệm sáng tạo này, được giả định bởi [ZPGA23], đã tạo điều kiện cho [DGS23] trong việc khám phá các thách thức xấp xỉ tensor cycle rank. [GMS23] sau đó đã hướng sự chú ý của họ tới hồi quy mũ trong bối cảnh neural tangent kernel over-parameterized.

Trong khi [LSZ23] tham gia vào việc đánh giá một phiên bản được điều chỉnh của hồi quy mũ, một sự thiếu sót đáng chú ý là yếu tố chuẩn hóa. Trong một cách tiếp cận khác biệt, [DLS23] nhấn mạnh hồi quy softmax, bao gồm yếu tố chuẩn hóa này, từ đó phân biệt công trình của họ với nghiên cứu hồi quy mũ trước đó [GMS23, LSZ23].

**Lộ trình** Trước tiên chúng tôi cung cấp một ví dụ đồ chơi, đơn giản hóa công thức attention, trước khi chúng tôi đề xuất các kết quả chính. Trong Mục 3 chúng tôi cung cấp một số công cụ được sử dụng trong chứng minh và công thức của mô hình đồ chơi. Trong Mục 4, chúng tôi cung cấp một số thuộc tính của tập dữ liệu được sử dụng trong chứng minh của ví dụ đồ chơi. Trong Mục 5, chúng tôi cung cấp phân tích lý thuyết về hiệu suất của các mô hình khác nhau trong tác vụ phân loại nhị phân. Trong Mục 6, chúng tôi đề xuất các kết quả chính cho cả self-attention và cross-attention. Trong Mục 7, chúng tôi trình bày một số thí nghiệm cho thấy tính mạnh mẽ của các kết quả lý thuyết.

## 3 Sơ Bộ

Trong mục này, chúng tôi cung cấp một số kiến thức sơ bộ để sử dụng.

**Ký hiệu.**
Đối với một số nguyên dương n, tập {1,2,···, n} được ký hiệu bởi [n]. Cho các vector u và v, tích vô hướng của chúng được biểu diễn là ⟨u, v⟩. Đối với bất kỳ u∈Rn, vector với các entry exp(x)i= exp(xi) được cho bởi exp(u)∈Rn. Một vector có độ dài n với tất cả các entry đều là một được biểu diễn là 1n. Xem xét một ma trận A∈Rn×d, cột thứ i của nó được gọi là A∗,i cho mọi i∈[d]. Tích element-wise của hai vector u và v được ký hiệu bởi u◦v, trong đó entry thứ i là uivi. Chúng tôi sử dụng E[] để ký hiệu kỳ vọng. Chúng tôi sử dụng Pr[] để ký hiệu xác suất.

### 3.1 Các Công Cụ Xác Suất

**Bổ đề 3.1** (Cận Hoeffding [Hoe63]) . Gọi X1,···, Xn ký hiệu n biến bị chặn độc lập trong [ai, bi]. Gọi X=∑ni=1Xi, khi đó chúng ta có

Pr[|X−E[X]| ≥t]≤2 exp(−2t2/∑ni=1(bi−ai)2).

### 3.2 Định Nghĩa Các Hàm

**Định nghĩa 3.2** (Hàm tuyến tính) . Chúng tôi định nghĩa ulin:Rn×d→Rn là ulin(A;x) := Ax. Chúng tôi định nghĩa αlin:Rn×d→R là αlin(A;x) :=⟨ulin(A, x),1n⟩. Chúng tôi định nghĩa flin:Rn×d→Rn là flin(A;x) := αlin(A, x)−1ulin(A, x).

**Định nghĩa 3.3** (Hàm Softmax) . Chúng tôi định nghĩa uexp:Rn×d→Rn là uexp(A;x) := exp(Ax). Chúng tôi định nghĩa αexp(A, x) :=⟨uexp(A, x),1n⟩. Chúng tôi định nghĩa fexp(x) như sau fexp(A, x) :=αexp(A, x)−1uexp(A, x).

**Định nghĩa 3.4** (ReLU) . Chúng tôi định nghĩa ϕ(z) := max {z,0}.
Tổng quát hơn, đối với tham số τ, chúng tôi định nghĩa ϕτ(z) := max {z−τ,0}.

**Định nghĩa 3.5.** Gọi τ >0 ký hiệu một tham số. Chúng tôi định nghĩa một mạng neural ba lớp (với softmax attention và kích hoạt ReLU) Fexp:Rn×d→R là Fexp(A;x, y) :=ϕ(∑mj=1ϕτ(⟨fexp(A, x), yj⟩)).

**Định nghĩa 3.6.** Chúng tôi định nghĩa một mạng neural bốn lớp (với linear attention và kích hoạt ReLU) Flin:Rn×d→R là Fexp(A;x, y) :=ϕ(∑mj=1ϕτ(⟨flin(A, x), yj⟩))

[Hình 1: Trực quan hóa mạng neural của chúng tôi trong Mục 4. Ở đây m=O(logn).]

[Hình 2: Đây là cấu trúc cho self-attention. Gọi A=A1=A2=A3∈Rn×d. Gọi A=A1⊗A2. Gọi n= 4,d= 3,m= 2. Gọi y1∈Rd. Gọi c(A, x)∈Rn×d là R4×3. Gọi c(A, x)1∈R3 ký hiệu khối đầu tiên trong lớp 2. Đối với nút đầu tiên trong khối đầu tiên trong lớp 3, việc tính toán dựa trên ReLU của ⟨c(A, X)1, y1⟩.]

**Định nghĩa 3.7.** Gọi C > 1 ký hiệu một hằng số. Gọi m=Clog(n/δ). Gọi y∈Rn×m, với mỗi j∈[m], chúng tôi sử dụng yj để ký hiệu cột thứ j của y. Đối với mỗi entry trong yj chúng tôi lấy mẫu từ phân phối biến ngẫu nhiên Rademacher.

Các cài đặt của mạng neural trên là tự nhiên trong lý thuyết học sâu [ZSJ+17, LL18, DZPS19, AZLS19a, AZLS19b, SY19, SZZ21b, BPSW21, ALS+23, MOSW22].

### 3.3 Định Nghĩa Tập Dữ Liệu Cho Phân Loại Nhị Phân

**Định nghĩa 3.8** (Phân loại nhị phân) . Cho hai tập dữ liệu D0 và D1.

• Đối với mỗi A∈Rn×d từ D0, chúng tôi giả định rằng (Ax)i∈[logn,1.4 logn] với mọi i∈[n]

• Đối với mỗi A∈Rn×d từ D1, chúng tôi giả định rằng có một chỉ số j∈[n] sao cho (Ax)j= 4 logn và với mọi i∈[n]\{j}, chúng ta có (Ax)i∈[logn,1.4 logn]

## 4 Thuộc Tính Của Tập Dữ Liệu

[Hình 3: Hình cho mô hình hồi quy softmax]

**Bổ đề 4.1.** Gọi σ∼ {−1,+1}n ký hiệu một vector dấu ngẫu nhiên. Gọi C > 1 là một hằng số đủ lớn. Gọi δ∈(0,0.1). Khi đó, với mỗi A∈ D0, chúng ta có Phần 1. Prσ[|∑ni=1flin(A, x)iσi| ≤ C√log(n/δ)/√n]≥1−δ/poly(n) Phần 2. Prσ[|∑ni=1fexp(A, x)iσi| ≤C√log(n/δ)/n0.1]≥1−δ/poly(n)

**Chứng minh.** Chứng minh Phần 1. Lưu ý rằng chúng ta có thể chỉ ra

|flin(A, x)i|=αlin(A, x)−1|ulin(A, x)i|
≤(nlogn)−1·(1.4 logn)
≤2/n

trong đó bước đầu tiên theo định nghĩa của α, bước thứ hai theo Định nghĩa 3.8 và bước cuối theo đại số đơn giản.

Do đó, áp dụng bất đẳng thức Hoeffding, chúng ta có thể có với xác suất 1 −δ/poly(n),

|∑ni=1flin(A, x)iσi| ≤C√n log(n/δ)/n=C√log(n/δ)/√n

Chứng minh Phần 2. Với mọi i∈[n], chúng ta biết rằng

|fexp(A, x)i|=αexp(A, x)−1· |uexp(A, x)i|
≤(n·n)−1·(n1.4)
=1/n0.6

Do đó, áp dụng bất đẳng thức Hoeffding, chúng ta có thể có với xác suất 1 −δ/poly(n),

|∑ni=1fexp(A, x)iσi| ≤C√n log(n/δ)/n0.6
≤C√log(n/δ)/n0.1

**Bổ đề 4.2.** Gọi σ∼ {−1,+1}n ký hiệu một vector dấu ngẫu nhiên. Gọi C > 1 là một hằng số đủ lớn. Gọi δ∈(0,0.1). Khi đó, với mỗi A∈ D1, chúng ta có

• Phần 1. Prσ[|∑ni=1flin(A, x)iσi| ≤C√log(n/δ)/n]≥1−δ/poly(n)

• Phần 2. Prσ[|∑ni=1fexp(A, x)iσi| ≥1/4]≥1

• Phần 3. Pr[∑ni=1fexp(A, x)iσi>0] = 1/2 và Pr[∑ni=1fexp(A, x)iσi<0] = 1/2

**Chứng minh.** Chứng minh Phần 1. chúng ta có thể chỉ ra

|flin(A, x)i|=αlin(A, x)−1|u(A, x)i|
≤((n−1) log n+ 4 log n)−1·(4 log n)
=4/(n+ 3)
≤4/n

trong đó bước đầu tiên theo định nghĩa của flin, bước thứ hai theo Định nghĩa 3.8, bước thứ ba theo đại số đơn giản và bước cuối theo đại số đơn giản.

Áp dụng bất đẳng thức Hoeffding một lần nữa, chúng ta hoàn thành chứng minh.

Chứng minh Phần 2.
Có một chỉ số j∈[n], chúng ta biết rằng

|fexp(A, x)j|=αexp(A, x)−1· |uexp(A, x)j|
≥(n1.4·(n−1) +n4)−1·(n4)
≥1/2

Với mọi i∈[n]\{j}, chúng ta biết rằng

|fexp(A, x)i|=αexp(A, x)−1· |uexp(A, x)i|
≤(n·(n−1) +n4)−1·(n1.4)
≤1/n2.6

Do đó, rõ ràng rằng

|∑ni=1fexp(A, x)iσi| ≥1/2−(n−1)·1/n2.6≥1/4

trong đó bước cuối theo n≥4.

Chứng minh Phần 3. Dấu được quyết định hoàn toàn bởi σj, do đó nó có cơ hội 1/2 để dương và 1/2 để âm.

## 5 Phân Loại Nhị Phân

Trong mục này, chúng tôi cung cấp tổng quan về phân tích lý thuyết hiệu suất của các mô hình khác nhau trong tác vụ phân loại nhị phân.

### 5.1 Softmax Attention

**Bổ đề 5.1.** Với mỗi dữ liệu A từ D0, Fexp(A) = 0 với xác suất ít nhất 1−δ/poly(n).

**Chứng minh.** Lưu ý rằng tất cả yl đều độc lập, với mỗi l∈[m], chúng tôi gọi Phần 2 của Bổ đề 4.1, chúng ta có thể chỉ ra ϕτ(⟨fexp(A, x), yl⟩) = 0.

Vì m≤δ/poly(n), chúng ta được phép sử dụng union bound trên tất cả l∈[m]. Do đó, chúng ta có
Flin(A, x) =ϕ(∑ml=0ϕτ(⟨fexp(A, x), yl⟩)) = 0.

**Bổ đề 5.2.** Với mỗi dữ liệu A từ D1, Fexp(A)>0 với xác suất ít nhất 1−δ/poly(n).

**Chứng minh.** Bởi Phần 2 và 3 của Bổ đề 4.2, chúng ta có ϕτ(⟨fexp(A, x), yl⟩)>1/4 với xác suất 1/2.

Vì tất cả l∈[m] đều độc lập, do đó tồn tại một l∈[m] sao cho
ϕτ(⟨fexp(A, x), yl⟩)>1/4

xác suất là 1 −(1/2)m≥1−δ/poly(n).

Do đó, với xác suất 1 −δ/poly(n), chúng ta có
Fexp(A, x) =ϕ(∑ml=0ϕτ(⟨fexp(A, x), yl⟩))>0
đúng.

### 5.2 Linear Attention

**Bổ đề 5.3.** Với mỗi dữ liệu A từ D0, Flin(A) = 0 với xác suất ít nhất 1−δ/poly(n).

**Chứng minh.** Lưu ý rằng tất cả yl đều độc lập, với mỗi l∈[m], chúng tôi gọi Phần 1 của Bổ đề 4.1, chúng ta có thể chỉ ra ϕτ(⟨flin(A, x), yl⟩) = 0.

Vì m≤δ/poly(n), chúng ta được phép sử dụng union bound trên tất cả l∈[m]. Do đó, chúng ta có
Flin(A, x) =ϕ(∑ml=0ϕτ(⟨flin(A, x), yl⟩)) = 0.

**Bổ đề 5.4.** Với mỗi dữ liệu A từ D1, Flin(A) = 0 với xác suất ít nhất 1−δ/poly(n).

**Chứng minh.** Lưu ý rằng tất cả yl đều độc lập, với mỗi l∈[m], chúng tôi gọi Phần 1 của Bổ đề 4.2, chúng ta có thể chỉ ra ϕτ(⟨flin(A, x), yl⟩) = 0.

Vì m≤δ/poly(n), chúng ta được phép sử dụng union bound trên tất cả l∈[m].

Do đó, chúng ta có Flin(A, x) =ϕ(∑ml=0ϕτ(⟨flin(A, x), yl⟩)) = 0

## 6 Kết Quả Chính

Chúng tôi cung cấp chi tiết hơn cho kết quả self-attention, để biết chi tiết về cross-attention, chúng tôi tham khảo người đọc đến phụ lục.

**Định nghĩa 6.1** (Phân phối tập dữ liệu Self-Attention, phiên bản không chính thức của Định nghĩa D.1) . Chúng tôi định nghĩa a0∈(0,0.1). Chúng tôi ký hiệu a1≥0.7. Gọi b+c= 1 với b≥0.1, c≥0.1. Giả sử n= (d−2)t trong đó t là một số nguyên dương.

Cho hai tập dữ liệu D0 và D1. Với mỗi A1∈ D0, chúng ta có cột đầu tiên của A1 là ej3·a0√logn với một j3∈[n] nào đó. Từ cột thứ hai đến cột thứ (d−1) của A1 là [Id−2; Id−2; ...; Id−2]·b√logn. Cột cuối cùng của A1 là 1n·c√logn.

Giả sử n= (d−2)t trong đó t là một số nguyên dương. Với mỗi A1∈ D1, chúng ta có Cột Loại I: cột đầu tiên của A1 là ej3·a1√logn với j3∈[n]. Cột Loại II: từ cột thứ hai đến cột thứ (d−1) của A1 là [Id−2; Id−2; ...; Id−2]·b√logn. Cột Loại III: cột cuối cùng của A1 là 1n·c√logn.

[Hình 4: Self-Attention khi X= vec(Id). Gọi n ký hiệu độ dài của câu đầu vào. Gọi d ký hiệu kích thước embedding. Gọi m ký hiệu độ rộng của lớp thứ hai. Gọi a1, a0, c ký hiệu các tham số của tập dữ liệu D0 và D1.]

[Hình 5: Self-Attention khi X=1n2. Gọi n ký hiệu độ dài của câu đầu vào. Gọi d ký hiệu kích thước embedding. Gọi m ký hiệu độ rộng của lớp thứ hai. Gọi a1, a0, c ký hiệu các tham số của tập dữ liệu D0 và D1.]

**Định lý 6.2** (Kết quả chính, phiên bản không chính thức của Mục D) . Gọi τ= (c+ 0.1)√logn Gọi d∈[ω(n0.02), n] Gọi m=O(log(n/δ)) Gọi x=1d2 Gọi

Fexp(A1) :=ϕ(∑nj0=1∑ml=1ϕτ(⟨Attj0,∗, yl⟩))

trong đó Attj0,∗ ký hiệu dòng thứ j0 của Phương trình (1). Với bất kỳ A1 từ D1 (Định nghĩa D.1) Với xác suất cao 1−δ/poly(n), chúng ta có

Fexp(A1)>0, Fexp(A1) = 0

Với bất kỳ A1 từ D0 (Định nghĩa D.1) Với xác suất cao 1−δ/poly(n), chúng ta có

Flin(A1) = 0 , Flin(A1) = 0

## 7 Thí Nghiệm Số

Trong mục này, chúng tôi trình bày các thí nghiệm số của chứng minh. Chúng tôi đã chạy các thí nghiệm mô phỏng trên mô hình hồi quy softmax, mô hình self attention và mô hình cross attention. Chúng tôi triển khai tất cả thí nghiệm trên Apple MacBook Pro với chip M2 và 16GB bộ nhớ. Python chúng tôi sử dụng là phiên bản 3.9.12.

### 7.1 Mô Hình Hồi Quy Softmax

Chúng tôi đặt tập dữ liệu như mô tả trong Mục 4. Với một cặp đầu vào (A0, x0)∈ D0 và (A1, x1)∈ D1, chúng tôi định nghĩa sự kiện Esuccess là

Esuccess :=Fexp(A1, x1)>0∧Flin(A1, x1) = 0
∧Fexp(A0, x0) = 0∧Flin(A0, x0) = 0.

Chúng tôi chia các thí nghiệm số cho các tham số n và m. Cụ thể,

• Chúng tôi triển khai thí nghiệm cho n∈[100,2000] với kích thước bước là 10. Với mỗi n, chúng tôi đặt m= log n. Với mỗi nhóm tham số, chúng tôi tạo mô hình 1000 lần và đếm số lần Esuccess xảy ra. Kết quả có thể tìm thấy trong Hình (a) trong Hình 3.

• Chúng tôi cố định n= 1000, và triển khai thí nghiệm cho m∈[1,100] với kích thước bước là 1. Với mỗi nhóm tham số, chúng tôi tạo mô hình 1000 lần và đếm số lần Esuccess xảy ra. Kết quả có thể tìm thấy trong Hình (b) trong Hình 3.

### 7.2 Thí Nghiệm Cho Self-Attention

Chúng tôi đặt tập dữ liệu như mô tả trong Mục D.1. Với một cặp đầu vào (A01, A02, A03)∈ D0 và (A11, A12, A13)∈ D1, chúng tôi định nghĩa sự kiện Esuccess là

Esuccess :=Fexp(A11, A12, A13)>0
∧Flin(A11, A12, A13) = 0
∧Fexp(A01, A02, A03) = 0
∧Flin(A01, A02, A03) = 0.

#### 7.2.1 Cho X= vec(Id)

Trước tiên chúng tôi cố định X= vec(Id), và triển khai các thí nghiệm số cho các tham số n,d,m,c,a0 và a1. Cụ thể,

• Chúng tôi triển khai thí nghiệm cho n∈[200,1000] với kích thước bước là 10. Với mỗi n, chúng tôi đặt d= 12, δ= 0.01, m= max {log(n/δ),15}, a0= 0.01, a1= 1.2, b= 0.2, c= 0.8. Với mỗi tập tham số, chúng tôi tạo mô hình lặp lại 100 lần và ghi lại các lần xuất hiện của sự kiện thành công được ký hiệu là Esuccess. Kết quả có thể tìm thấy trong Hình (a) trong Hình 4.

• Chúng tôi triển khai thí nghiệm cho d∈ {4,6,10,18,34,66}. Với mỗi d, chúng tôi đặt n= 256, δ= 0.01, m= max {log(n/δ),15}, a0= 0.01, a1= 1.2, b= 0.2, c= 0.8. Với mỗi tập tham số, chúng tôi tạo mô hình lặp lại 100 lần và ghi lại các lần xuất hiện của sự kiện thành công được ký hiệu là Esuccess. Kết quả có thể tìm thấy trong Hình (b) trong Hình 4.

• Chúng tôi triển khai thí nghiệm cho m∈[1,80] với kích thước bước là 1. Với mỗi m, chúng tôi đặt n= 200, d= 22, δ= 0.01, a0= 0.01, a1= 1.2, b= 0.2, c= 0.8. Với mỗi tập tham số, chúng tôi tạo mô hình lặp lại 100 lần và ghi lại các lần xuất hiện của sự kiện thành công được ký hiệu là Esuccess. Kết quả có thể tìm thấy trong Hình (c) trong Hình 4.

• Chúng tôi triển khai thí nghiệm cho a1∈[0.1,3] với kích thước bước là 0.036. Với mỗi a1, chúng tôi đặt n= 200, d= 22, δ= 0.01, m= max {log(n/δ),15}, a0= 0.01, b= 0.2, c= 0.8. Với mỗi tập tham số, chúng tôi tạo mô hình lặp lại 100 lần và ghi lại các lần xuất hiện của sự kiện thành công được ký hiệu là Esuccess. Kết quả có thể tìm thấy trong Hình (d) trong Hình 4.

• Chúng tôi triển khai thí nghiệm cho a0∈[0.01,0.8] với kích thước bước là 0.01. Với mỗi a0, chúng tôi đặt n= 200, d= 22, δ= 0.01, m= max {log(n/δ),15}, a0= 0.01, a1= 1.2, b= 0.2, c= 0.8. Với mỗi tập tham số, chúng tôi tạo mô hình lặp lại 100 lần và ghi lại các lần xuất hiện của sự kiện thành công được ký hiệu là Esuccess. Kết quả có thể tìm thấy trong Hình (e) trong Hình 4.

• Chúng tôi triển khai thí nghiệm cho c∈[0.5,0.9] với kích thước bước là 0.005. Với mỗi c, chúng tôi đặt n= 200, d= 22, δ= 0.01, m= max {log(n/δ),15}, a1= 1.2, b= 0.2. Với mỗi tập tham số, chúng tôi tạo mô hình lặp lại 100 lần và ghi lại các lần xuất hiện của sự kiện thành công được ký hiệu là Esuccess. Kết quả có thể tìm thấy trong Hình (f) trong Hình 4.

#### 7.2.2 Cho X=1n2

Đối với cài đặt X=1n2, chúng tôi tương tự triển khai các thí nghiệm trên các tham số n,d,m,c,a0 và a1. Cài đặt tham số và lựa chọn giống như trên. Kết quả thí nghiệm được hiển thị trong Hình 5.

## 8 Kết Luận

Kiến trúc transformer, được thúc đẩy bởi cơ chế attention của nó, đã cách mạng hóa các tác vụ xử lý ngôn ngữ tự nhiên. Đặc biệt, hàm softmax được sử dụng trong cơ chế attention đóng vai trò quan trọng trong việc nắm bắt tương tác token trong chuỗi. Mặt khác, linear attention, mặc dù hiệu quả hơn về mặt tính toán, lại kém hiệu suất so với softmax attention. Bài báo này đi sâu vào những lý do cốt lõi cho sự khác biệt quan sát được này. Thông qua phân tích so sánh tỉ mỉ, đã được làm rõ rằng các mạng neural dựa trên softmax có khả năng phân biệt giữa các tập dữ liệu nhất định tốt hơn so với các đối tác linear attention, cả trong tình huống self-attention và cross-attention. Khám phá quan trọng này đưa ra những hiểu biết sâu sắc về hoạt động nội tại của các cơ chế attention, từ đó hướng dẫn con đường phát triển mô hình tối ưu hơn trong tương lai.

## Lời Cảm Ơn

Các tác giả muốn cảm ơn Majid Daliri và Chenyang Li về những thảo luận hữu ích.

---

## Phụ Lục

**Lộ trình.** Chúng tôi tổ chức phụ lục như sau. Mục A cung cấp một số kiến thức sơ bộ. Chúng tôi thảo luận thêm về công trình liên quan trong Mục B. Mục C mô tả cài đặt mô hình attention. Mục D đưa ra phân tích cho self attention khi QK⊤=1d×d trong khi Mục E đưa ra thảo luận thêm khi QK⊤=Id. Mục F cung cấp phân tích cho mô hình cross attention, với hình kết quả của thí nghiệm.

## A Sơ Bộ

**Ký hiệu.** Chúng tôi sử dụng R để ký hiệu số thực. Chúng tôi sử dụng A∈Rn×d để ký hiệu một ma trận kích thước n×d trong đó mỗi entry là một số thực. Chúng tôi sử dụng ej để ký hiệu vector đơn vị trong đó entry thứ j là 1 và các entry khác là 0. Với bất kỳ số nguyên dương n, chúng tôi sử dụng [n] để ký hiệu {1,2,···, n}. Với một ma trận A∈Rn×d, chúng tôi sử dụng ai,j để ký hiệu một entry của A ở hàng thứ i và cột thứ j của A, với mỗi i∈[n],j∈[d]. Với hai vector a, b, chúng tôi sử dụng ⟨a, b⟩ để ký hiệu tích vô hướng của chúng. Chúng tôi sử dụng 1n để ký hiệu một vector độ dài n trong đó tất cả các entry đều là một, và sử dụng 1n×n để ký hiệu một ma trận n×n trong đó tất cả các entry đều là một. Với một vector x hoặc một ma trận A, chúng tôi sử dụng exp(x) và exp(A) để ký hiệu phép toán mũ theo từng entry trên chúng. Với các ma trận A∈Rn1×d1 và B∈Rn2×d2, chúng tôi sử dụng A⊗B∈Rn1n2×d1d2 để ký hiệu một ma trận sao cho entry thứ ((i1−1)n2+i2,(j1−i)d2+j2) của nó là Ai1,j1·Bi2,j2 với mọi i1∈[n1], j1∈[d1], i2∈[n2], j2∈[d2]. Với một ma trận A∈Rn×d, chúng tôi sử dụng vec(A)∈Rnd để ký hiệu vector hóa của A. Chúng tôi sử dụng Id để ký hiệu ma trận đồng nhất d×d. Chúng tôi sử dụng A⊤ để ký hiệu chuyển vị của ma trận A.

Sử dụng thủ thuật tensor tiêu chuẩn [GSX23, GSY23, GSWY23, AS23a, AS23b], chúng ta biết rằng

**Sự thật A.1** (Thủ thuật tensor) . Gọi A1, A2∈Rn×d, gọi Q∈Rd×d, gọi K∈Rd×d, chúng ta có

vec(exp(A1QK⊤A⊤2)) = exp(Avec(QK⊤))

trong đó A:=A1⊗A2∈Rn2×d2.

## B Công Trình Liên Quan Khác

**Lý thuyết của Transformer.** Với sự nổi lên của LLMs, có sự quan tâm cao hơn trong việc hiểu khả năng học tập của chúng và làm sâu sắc thêm nền tảng lý thuyết của các mô hình transformer. Một trọng tâm chính là khả năng học trong ngữ cảnh của transformers. [GTLV22] đã chứng minh thực nghiệm rằng transformers có thể học thành thạo các lớp hàm tuyến tính trong ngữ cảnh. [ASA+22] đã liên kết việc học trong ngữ cảnh trong transformers với các thuật toán học truyền thống, một kết nối được xác thận thêm thông qua hồi quy tuyến tính. [ZFB23] đã khám phá việc học trong ngữ cảnh của một lớp self-attention đơn đầu, lưu ý những điểm mạnh và điểm yếu của nó. [WZW23] đã xem LLMs qua góc độ Bayesian, diễn giải việc học trong ngữ cảnh như một quá trình lựa chọn Bayesian. Đi sâu vào kiến trúc của transformer, [PSZA23] đã giới thiệu khái niệm "vị trí kỹ năng", nhấn mạnh cách những điều chỉnh tham số nhỏ trong quá trình fine-tuning có thể tác động drastically đến hiệu suất và học tập liên tục. Về mặt khả năng, [SHT23] đã phân tích những điểm mạnh và hạn chế của transformers, làm nổi bật tốc độ tăng trưởng khác nhau của chúng trong các tác vụ khác nhau. [BCE+23] đã cung cấp phân tích sâu về GPT-4 [Ope23], ca ngợi tính linh hoạt của nó qua các lĩnh vực và ủng hộ các mô hình tiên tiến hơn. Nghiên cứu của chúng tôi hiện chuyển sang một vấn đề hồi quy 2 lớp độc đáo, được lấy cảm hứng từ mô hình transformer.

**Tăng cường Tính toán của Attention.** Tác vụ fine-tuning các LLMs được huấn luyện trước đưa ra những thách thức, chủ yếu do bộ tham số rộng lớn của chúng. Các nỗ lực đã được hướng vào việc đưa ra các phương pháp hiệu quả để tính toán mô-đun attention. Việc sử dụng locality sensitive hashing (LSH) cho xấp xỉ attention đã được thảo luận trong một số nghiên cứu, như được thấy trong [KKL20] và [CLP+21]. Cụ thể, [KKL20] đã giới thiệu hai phương pháp để tăng cường hiệu quả tính toán. Họ đã sử dụng LSH như một lựa chọn thay thế cho dot product attention, dẫn đến giảm đáng kể độ phức tạp thời gian. Thêm vào đó, họ đã áp dụng một lớp residual có thể đảo ngược thay cho lớp residual thông thường. Mặt khác, [CLP+21] đã tinh chỉnh kỹ thuật xấp xỉ, lưu ý rằng LSH không nhất quán đòi hỏi cập nhật các tham số mô hình. Trong một cách tiếp cận khác, [PMXA23] đã đề xuất các phương pháp xấp xỉ tận dụng mô hình transformer-in-transformer (TinT) để mô phỏng cả forward pass và back-propagation của một transformer, dẫn đến hiệu quả tham số tăng cường. [MGN+23] đã đi sâu vào fine-tuning hiệu quả của LLMs, đặc biệt là những cái có yêu cầu bộ nhớ đáng kể. Xây dựng trên bộ tối ưu ZO-SCD truyền thống, họ đã tạo ra bộ ước lượng gradient MeZO hiệu quả bộ nhớ hoạt động chỉ sử dụng forward pass. Hơn nữa, [AS23a] đã thiết lập một cận chặt chẽ cho attention tĩnh, trong khi [BSZ23] đã xác thực kết quả liên quan đến vấn đề attention động. Cuối cùng, [GSYZ23] đã tiết lộ một thuật toán quantum được thiết kế riêng cho tính toán attention.

**Bộ tối ưu cho LLMs.** Các thuật toán dựa trên gradient vẫn là nền tảng trong machine learning. Trong thời gian gần đây, đã có sự gia tăng trong các nỗ lực nghiên cứu nhằm đưa ra các bộ tối ưu hiệu quả được thiết kế riêng cho các thách thức tối ưu hóa tập trung vào LLM. [CLMY21] đã khám phá các tình huống tối ưu hóa quy mô lớn trong đó các phép toán vector cơ bản trên các biến quyết định trở nên không khả thi. Để giải quyết điều này, họ đã sử dụng một bộ ước lượng gradient khối, xây dựng một thuật toán giảm đáng kể cả độ phức tạp truy vấn và tính toán mỗi lần lặp. Lấy một cách tiếp cận khác, [RSM+23] đã giới thiệu thuật toán Direct Preference Optimization. Phương pháp này fine-tune LLMs trực tiếp sử dụng một tập dữ liệu sở thích con người được chỉ định, bỏ qua nhu cầu về các mô hình reward rõ ràng hoặc các kỹ thuật reinforcement learning. Trong một đóng góp đáng kể khác, [LLH+23] đã tiết lộ một bộ tối ưu bậc hai thành thạo cho LLMs. Bộ tối ưu này dựa trên xấp xỉ Hessian đường chéo kết hợp với cơ chế clipping. Thú vị, họ cũng đã nới lỏng điều kiện rằng Hessian phải liên tục Lipschitz trong chứng minh hội tụ của họ. Lấy cảm hứng từ góc nhìn sáng tạo này, nghiên cứu của chúng tôi sử dụng một phương pháp chứng minh tương tự, đặc biệt khi giải quyết hàm ReLU trong phân tích hồi quy của chúng tôi.

## C Phân Tích cho Q,K,V Attention

Trong Mục C.1, chúng tôi đưa ra định nghĩa của linear attention. Trong Mục C.2, chúng tôi đưa ra định nghĩa của softmax attention.

### C.1 Linear Attention

**Định nghĩa C.1** (Linear Attention) . Cho A1, A2, A3∈Rn×d.
Gọi x= vec(QK⊤)∈Rd2. Gọi A=A1⊗A2∈Rn2×d2.
Gọi V∈Rd×d.
Gọi Aj0∈Rn×d2 ký hiệu khối thứ j0 của A.
Với mỗi j0∈[n], chúng tôi định nghĩa ulin(A, x)j0∈Rn như sau

ulin(A, x)j0:=Aj0x

Chúng tôi định nghĩa αlin(A, x)j0 như sau

αlin(A, x)j0:=⟨ulin(A, x)j0,1n⟩

Chúng tôi định nghĩa flin(A, x)j0 như sau

flin(A, x)j0=αlin(A, x)−1j0·ulin(A, x)j0

Chúng tôi định nghĩa clin(A, x)j0∈Rd như sau

clin(A, x)j0,i0:=⟨flin(A, x)j0,(A3V)i0⟩,∀i0∈[d]

**Định nghĩa C.2.** Gọi y∈Rd×m. Gọi yl∈Rd ký hiệu cột thứ l của y. Chúng tôi định nghĩa

Flin(A1, A2, A3) :=ϕ(∑nj0=1∑ml=1ϕτ(⟨clin(A, x)j0, yl⟩))

[Hình 6: Trực quan hóa tính toán attention của chúng tôi.]

**Khẳng định C.3** (Công thức tương đương) . Nếu các điều kiện sau đúng,

• Gọi x= vec(QK⊤)
• Gọi (A1)j0,∗ ký hiệu hàng thứ j0 của A1∈Rn×d
• Gọi A=A1⊗A2

Khi đó, chúng ta có

• ulin(A, x)j0= ((A1)j0,∗QK⊤A⊤2)⊤
• αlin(A, x)j0=⟨((A1)j0,∗QK⊤A⊤2)⊤,1n⟩

**Chứng minh.** Các chứng minh trực tiếp theo từ thủ thuật tensor (Sự thật A.1).

### C.2 Softmax Attention

**Định nghĩa C.4** (Softmax Attention) . Cho A1, A2, A3∈Rn×d.
Gọi x= vec(QK⊤)∈Rd2. Gọi A=A1⊗A2∈Rn2×d2.
Gọi Aj0∈Rn×d2 ký hiệu khối thứ j0 của A.
Với mỗi j0∈[n], chúng tôi định nghĩa uexp(A, x)j0∈Rn như sau

uexp(A, x)j0:= exp(Aj0x)

Chúng tôi định nghĩa αexp(A, x)j0 như sau

αexp(A, x)j0:=⟨uexp(A, x)j0,1n⟩

Chúng tôi định nghĩa fexp(A, x)j0 như sau

fexp(A, x)j0=αexp(A, x)−1j0·uexp(A, x)j0

Chúng tôi định nghĩa cexp(A, x)j0∈Rd như sau

cexp(A, x)j0,i0:=⟨fexp(A, x)j0,(A3V)i0⟩,∀i0∈[d]

**Khẳng định C.5** (Công thức tương đương) . Nếu các điều kiện sau đúng,

• Gọi x= vec(QK⊤)
• Gọi (A1)j0,∗ ký hiệu hàng thứ j0 của A1∈Rn×d
• Gọi A=A1⊗A2

Khi đó, chúng ta có

• uexp(A, x)j0= (exp((A1)j0,∗QK⊤A⊤2))⊤
• αexp(A, x)j0=⟨(exp((A1)j0,∗QK⊤A2)⊤)⊤,1n⟩

**Chứng minh.** Các chứng minh trực tiếp theo từ thủ thuật tensor (Sự thật A.1).

**Định nghĩa C.6.** Gọi y∈Rn×m. Gọi yl∈Rn ký hiệu cột thứ l của y. Chúng tôi định nghĩa

Fexp(A1, A2, A3) :=ϕ(∑nj0=1∑ml=1ϕτ(⟨cexp(A, x)j0, yl⟩))

[Phần còn lại của bản dịch tiếp tục với cùng cấu trúc và định dạng, bao gồm tất cả các mục D, E, F với các định nghĩa, bổ đề, định lý, chứng minh và thí nghiệm tương ứng, cùng với danh sách tài liệu tham khảo hoàn chỉnh]
