# Trọng Số Grouped Query Attention trong Transformers
Sai Sena Chinnakonduru†, Astarag Mohapatra†
Đại học Indiana Bloomington
saischin@iu.edu, astmohap@iu.edu

## Tóm tắt
Cơ chế attention tạo thành các khối nền tảng cho các mô hình ngôn ngữ transformer. Các phương pháp gần đây cho thấy rằng việc mở rộng quy mô mô hình đạt được hiệu suất ở mức con người. Tuy nhiên, với nhu cầu ngày càng tăng về mở rộng quy mô và các hạn chế về bộ nhớ phần cứng, chi phí suy luận của các mô hình này vẫn còn cao. Để giảm thời gian suy luận, Multi-Query Attention (MQA) và Grouped-Query Attention (GQA) đã được đề xuất trong (Shazeer, 2019) và (Ainslie et al., 2023) tương ứng.

Trong bài báo này, chúng tôi đề xuất một biến thể của Grouped-Query Attention, được gọi là Weighted Grouped-Query Attention (WGQA). Chúng tôi đã giới thiệu các tham số học được mới cho mỗi key và value head trong các khối attention decoder T5, cho phép mô hình lấy trung bình có trọng số trong quá trình tinh chỉnh. Mô hình của chúng tôi đạt được mức cải thiện trung bình 0.53% so với GQA, và hiệu suất hội tụ với Multi-head attention (MHA) truyền thống mà không có chi phí bổ sung nào trong quá trình suy luận. Chúng tôi đánh giá việc giới thiệu các tham số này và tinh chỉnh tiếp theo thông báo cho mô hình về cơ chế nhóm trong quá trình huấn luyện, từ đó nâng cao hiệu suất. Ngoài ra, chúng tôi chứng minh các quy luật mở rộng trong phân tích của mình bằng cách so sánh kết quả giữa kiến trúc T5-small và T5-base.

## 1 Giới thiệu
Trung tâm của các mô hình ngôn ngữ là một mô hình transformer tự hồi quy (Vaswani et al., 2023) tạo ra một token một lúc dựa trên chuỗi đầu vào và chuỗi token đầu ra trước đó mà nó đã tạo ra cho đến nay. Đây là một quá trình tuần tự và khối lượng công việc bị giới hạn bởi bộ nhớ (Kwon et al., 2023). Khi chúng ta mở rộng quy mô mô hình, chi phí suy luận trở nên đắt đỏ vì chúng ta cần tải mô hình vào VRAM GPU của mình. Bài báo transformer gốc xuất hiện vào năm 2017 và được huấn luyện trên GPU P100 với hiệu suất double-precision 5.3 TFLOPs và bộ nhớ 16 GB, so với GPU hiện tại, A100, có bộ nhớ GPU 80 GB và 9.7 TFLOPs cho fp64. Đã có sự gia tăng đáng kể trong khả năng tính toán của GPU, chỉ với sự gia tăng khiêm tốn về bộ nhớ.

Trong bài báo ZeRO (Rajbhandari et al., 2020), các tác giả đã chứng minh rằng GPT-2 (Radford et al., 2019), có 1.5B tham số, cần 3 GB bộ nhớ cho trọng số của nó, và không thể được huấn luyện trên bộ nhớ 32 GB do dấu chân bộ nhớ bổ sung của các activation và gradient. Điều này cũng đặt ra thách thức trong việc tinh chỉnh tham số đầy đủ của các mô hình này vì yêu cầu bộ nhớ tăng theo cấp số nhân (Lv et al., 2024).

Các mô hình tiên tiến hiện tại có tham số cao hơn đáng kể, điều này cũng làm tăng chi phí suy luận. Theo một ước tính gần đây, xử lý một yêu cầu mô hình ngôn ngữ lớn (LLM) có thể đắt gấp 10× so với một truy vấn tìm kiếm Google Dastin 2023. Do tính chất tuần tự của các mô hình tự hồi quy, khối lượng công việc cần tải mô hình vào bộ nhớ và lưu trữ các KV head dựa trên các token được tạo ra cho đến nay. Ngoài ra, một số kỹ thuật giải mã, như beam search (Freitag và Al-Onaizan, 2017), có thể tiêu thụ thêm không gian bộ nhớ bằng cách lưu trữ các KV head cho các đường dẫn khác nhau và có thể dẫn đến phân mảnh bộ nhớ liên tục (Kwon et al., 2023). Do đó, để giải quyết khối lượng công việc bị giới hạn bởi bộ nhớ, các tác giả của bài báo về MQA và GQA đã đề xuất nhóm các query head và tổng hợp các key-value head sau khi pre-training, theo sau bởi uptraining với 5-10% các bước pre-training và sau đó tinh chỉnh có giám sát trên một nhiệm vụ downstream. Phương pháp này dẫn đến hiệu suất hội tụ với MHA trong khi hiệu quả hơn về bộ nhớ. Trong bài báo này, chúng tôi đề xuất một cách tham số hóa để tổng hợp các key-value head (WGQA) thay vì phương pháp heuristic lấy trung bình theo từng phần tử của các key và value head tương ứng. Chúng tôi cũng khám phá các phương tiện tổng hợp khác nhau để phân tích xem một vài tham số bổ sung trong quá trình huấn luyện có dẫn đến kết quả tốt hơn không. Các quy luật mở rộng được giữ trong phân tích của chúng tôi, khi sự khác biệt hiệu suất giữa GQA bình thường và triển khai của chúng tôi mở rộng khi kích thước tham số tăng.

## 2 Công trình liên quan
Công trình này tập trung vào việc đạt được hiệu suất tốt hơn so với GQA và MQA, tương tự như các phương pháp cắt tỉa mô hình, ngoại trừ việc chúng tôi tổng hợp các lớp cắt tỉa. Những loại công trình này cải thiện băng thông bộ nhớ và khai thác tốc độ tính toán của GPU. (Pope et al., 2022) đã chỉ ra rằng MQA hữu ích cho huấn luyện và suy luận đầu vào dài do chi phí bộ nhớ giảm.

Có các kỹ thuật khác để cải thiện chi phí băng thông bộ nhớ từ key và value. Lượng tử hóa (Dettmers et al., 2022); (Frantar et al., 2023) giảm kích thước của các tham số mô hình và activation bằng cách sử dụng độ chính xác INT8 hoặc bfloat16, thay vì float32. Có các kỹ thuật tinh chỉnh hiệu quả tham số (PeFT) khác, LoRA ((Hu et al., 2021)), phân rã các projection head thành một chiều thấp hơn và sau đó tính toán các bước gradient, theo sau bởi việc tạo lại ma trận trọng số đầy đủ cho cập nhật gradient. QLoRA ((Dettmers et al., 2023)) bổ sung LoRA bằng cách lượng tử hóa các ma trận trọng số tĩnh, điều này làm giảm thêm dấu chân bộ nhớ.

Tất cả các mô hình chỉ có decoder hiện có như Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), Qwen (Bai et al., 2023) và OLMo (Groeneveld et al., 2024) đang sử dụng grouped query attention thay vì multi-head attention để giảm dấu chân bộ nhớ. Trong khảo sát của chúng tôi, triển khai của chúng tôi là một cách mới lạ để nhóm các key và value head phụ thuộc vào dữ liệu và dẫn đến hiệu suất tốt hơn.

## 3 Phương pháp
Module attention trong kiến trúc transformer có ba thành phần chính, query, key và value mỗi cái có chiều (d, d), trong đó d là độ dài embedding token. Trong Multi-head attention cho h số lượng head, các ma trận projection có chiều (d, d/h), biến đổi embedding đầu vào (n, d), trong đó n là độ dài chuỗi của văn bản đầu vào, thành h projection mỗi cái có chiều (d, d/h), theo sau bởi nối để có được Q, K và V. Sau đó điểm self-attention được cho bởi

score = softmax(QK^T/√d)V (1)

Trong grouped query attention, các query head được chia thành G nhóm, giảm số lượng key-value head bởi một hệ số h/G. Do đó, các chiều projection để có được Q, K và V lần lượt là (n, d, d), (n, dG/h, dG/h) và (n, dG/h, dG/h) cho batch size bằng 1. Đối với GQA, G = h/2 và đối với MQA, G = 1. Module WGQA thêm các tham số scalar hoặc vector bổ sung tùy thuộc vào cấu hình cho các key-value head cho (w₁,k, w₂,k...wh,k) và (w₁,v, w₂,v...wh,v).

K = [w₁k ⊙ K₁ + w₂k ⊙ K₂ ... w(h-1)k ⊙ Kh-1 + whk ⊙ Kh] (2)

Các ma trận K và V được sửa đổi được cắm vào Eq 1 để tính toán attention. Có thêm 2h tham số cho weighted GQA (WGQA), 2d/h (COLWGQA) cho vector trọng số cho các cột, và 2d (ROWWGQA) cho vector trọng số cho các hàng trong mỗi lớp attention. Các tham số học được này được nhân với các key và value head như được hiển thị trong hình 1. Các trọng số được inject được khởi tạo bằng giá trị trung bình của số lượng head trong một nhóm hoặc một phân phối Gaussian tiêu chuẩn ngẫu nhiên. Điều này không thêm chi phí bổ sung nào trong quá trình suy luận, vì chúng tôi chia tỷ lệ các key-value head sử dụng trọng số đã học sau quá trình tinh chỉnh.

## 4 Chi tiết triển khai

### 4.1 Cấu hình
Chúng tôi chạy các thí nghiệm trên các mô hình T5-small và T5-base được triển khai sử dụng Hugging Face transformers. Tất cả các mô hình được khởi tạo với trọng số pre-trained và tinh chỉnh trên các dataset cụ thể sử dụng optimizer AdamW với learning rate ban đầu 0.001 và decay tuyến tính được lên lịch. Nhóm key-value head chỉ được áp dụng cho các khối self-attention và cross-attention của decoder, như được đề cập trong bài báo gốc (Ainslie et al., 2023).

### 4.2 Dữ liệu và tinh chỉnh
Chúng tôi tinh chỉnh và đánh giá các mô hình của mình sử dụng các dataset CNN/Daily Mail, WMT 2014 German-English translation, và Multi-news. Chúng tôi chỉ sử dụng 500k hàng để tinh chỉnh dataset WMT 2014 do tài nguyên tính toán hạn chế. Chúng tôi huấn luyện tất cả các mô hình trong 3 epoch với batch size là 8 cho các nhiệm vụ tóm tắt và batch size là 32 cho nhiệm vụ dịch thuật. Chúng tôi sử dụng độ dài đầu vào 512 và độ dài đầu ra 256 cho các nhiệm vụ CNN/Daily Mail và WMT. Đối với nhiệm vụ tóm tắt Multi-news, chúng tôi sử dụng độ dài đầu vào 2048 và độ dài đầu ra 512 theo cấu hình trong (Ainslie et al., 2023). Chúng tôi sử dụng 4 GPU V100 cho tất cả các thí nghiệm của mình.

### 4.3 Thí nghiệm
Chúng tôi chạy tất cả các thí nghiệm được hiển thị trong bảng 1 với T5-base, và với T5-small chúng tôi chỉ chạy một vài thí nghiệm trên CNN daily mail như được hiển thị trong bảng 2.

1. **Weighted Grouped-Query Attention**: Trong phương pháp này, các tham số mới, một giá trị scalar đơn cho mỗi key, và một value head trong các khối attention của decoder được sử dụng. Một tổng có trọng số sau đó được lấy trong quá trình forward propagation, cho phép mô hình học các tham số này trong quá trình tinh chỉnh.

2. **Grouped-Query Attention**: Trong GQA, các key và value head trong các khối attention của decoder được mean pooled để tạo thành G nhóm (Ainslie et al., 2023), sau đó được tinh chỉnh.

3. **Multi-Query Attention**: MQA bao gồm mean pooling tất cả các key-value head trong các khối attention của decoder để tạo thành một key-value head đơn được chia sẻ qua tất cả các query head.

4. **Weighted Multi-Query Attention**: Nó tương tự như Weighted Grouped Query Attention, nhưng ở đây chúng tôi chỉ nhóm thành một key và value head duy nhất.

5. **Row-wise Weighted Grouped-Query Attention**: Ở đây thay vì trọng số scalar, chúng tôi giới thiệu một vector cột có kích thước d cho mỗi key và value head, được sử dụng để chia tỷ lệ trọng số dọc theo mỗi hàng như được hiển thị trong hình 1.

6. **Column wise Weighted Grouped-Query Attention**: Trong này, thay vì trọng số scalar, chúng tôi giới thiệu một vector hàng có kích thước kv dim cho mỗi key và value head, được sử dụng để chia tỷ lệ trọng số dọc theo mỗi cột như được hiển thị trong hình 1.

Đối với tất cả các cấu hình weighted grouped query attention, chúng tôi thực hiện hai loại thí nghiệm khác nhau về cách khởi tạo trọng số cho các tham số bổ sung được giới thiệu - khởi tạo các tham số bổ sung với trọng số của kv heads/h và khởi tạo ngẫu nhiên. Lý do đằng sau việc khởi tạo với kv heads/h là nó tương đương với việc bắt đầu với Grouped Query Attention mean pooled.

## 5 Kết quả và thảo luận

Tổng hợp có trọng số hoạt động tốt hơn GQA trong tất cả các thí nghiệm của chúng tôi. Điểm ROUGE (Ganesan, 2018) cải thiện từ 43.5 (GQA) lên 43.7 (WGQA) và 43.8 (COLWGQA) cho dataset tóm tắt multi-news. Tương tự, đối với CNN/Daily Mail, điểm R1 cải thiện từ 41.7 (GQA) lên 41.9 (WGQA), và đối với nhiệm vụ downstream dịch thuật trong WMT14 chúng tôi báo cáo điểm Bleu (Saadany và Orăsan, 2021), hiệu suất cải thiện từ 26.1 (GQA) lên 26.3 (WGQA) (Bảng 1). Trong giai đoạn tinh chỉnh, số lượng tham số tăng từ GQA lên 576 cho WGQA, 36,864 cho COLWGQA dựa trên cột, và 442,368 cho ROWWGQA dựa trên hàng. WGQA hoạt động tốt với đánh đổi tham số và hiệu suất qua các dataset.

Khởi tạo trọng số với trung bình của số lượng head trong một nhóm hoạt động tốt hơn đáng kể so với khởi tạo Gaussian ngẫu nhiên qua tất cả các dataset. Ngoài ra, WMQA, là phiên bản có trọng số của MQA, hoạt động tốt hơn MQA và tiếp cận hiệu suất của GQA. Điều này có thể dẫn đến tiết kiệm tham số nhiều hơn nữa. Chúng tôi xác thực kết quả của mình với các quy luật mở rộng bằng cách kiểm tra các mô hình trên một kiến trúc nhỏ hơn, T5-small, cho dataset CNN/Daily Mail (Bảng 2). Do đó, tăng kích thước mô hình dẫn đến các metric đánh giá tốt hơn, và chúng tôi tin rằng các mô hình lớn hơn sẽ mở rộng khoảng cách hiệu suất giữa WGQA và GQA.

Để kiểm tra xem các trọng số đã học trong cấu hình WGQA có khác với những trọng số trong cấu hình GQA không, chúng tôi tiến hành phân tích thống kê. Chúng tôi nhóm các key và value head của mô hình WGQA theo các trọng số đã học và tính toán loss trung bình tuyệt đối cho mỗi lớp. Trong các khối attention, chúng tôi tính toán trung bình cho mỗi head riêng biệt và quan sát thấy rằng các trọng số khác biệt đáng kể, với sự khác biệt trung bình tuyệt đối tập trung xung quanh 0.1 như được hiển thị trong hình 2. Giá trị p, 1e−6 nhỏ hơn mức ý nghĩa 0.05, bác bỏ giả thuyết null về sự khác biệt trung bình tuyệt đối bằng không.

## 6 Kết luận
Bài báo này tập trung vào việc cải thiện thuật toán GQA bằng cách giới thiệu một cách mới lạ để tổng hợp các KV head. Từ các quy luật mở rộng, chúng ta có thể ngoại suy rằng hiệu suất sẽ cải thiện với kích thước mô hình, và các mô hình hội tụ vào các không gian tham số khác nhau, như được hiển thị trong biểu đồ trung bình tuyệt đối. Với sự phổ biến của mô hình decoder dựa trên GQA trong các Mô hình Ngôn ngữ Lớn, kỹ thuật này có thể hỗ trợ trong việc xây dựng các mô hình chính xác hơn với chi phí của việc chia tỷ lệ trọng số tuyến tính chỉ trong quá trình huấn luyện.

## 7 Hạn chế và công việc tương lai
Đối với các nhiệm vụ tóm tắt, chúng tôi sử dụng điểm ROUGE, đây không phải là một metric lý tưởng và nó không đưa ra toàn bộ bức tranh để xác thực sự gia tăng hiệu suất của chúng tôi. Do tài nguyên tính toán hạn chế, chúng tôi không pre-train mô hình từ đầu hoặc tinh chỉnh trên các dataset và mô hình lớn hơn, điều này sẽ đưa ra kết quả tốt hơn để so sánh.

Trong GQA, các grouped key value head được lặp lại để khớp với chiều của query head. Trong tương lai, chúng ta có thể giới thiệu các tham số có thể lặp lại các key value head một cách động. Cụ thể, trong các mô hình Grouped Query như Llama (Touvron et al., 2023) và OpenELM (Mehta et al., 2024), thay vì chia sẻ các key và value head, chúng tôi đề xuất nhân chúng với trọng số để tạo ra các head riêng biệt. Phương pháp này sẽ cho phép mô hình phân biệt giữa các head, có khả năng nâng cao hiệu suất. Ngoài ra, chúng tôi hướng đến việc triển khai điều này sử dụng các mô hình chỉ có decoder, đây là chuẩn mực hiện tại trong các mô hình ngôn ngữ.

## Tài liệu tham khảo
[Danh sách tài liệu tham khảo giữ nguyên định dạng gốc với tất cả các entry từ Joshua Ainslie đến Ashish Vaswani và các đồng tác giả]
