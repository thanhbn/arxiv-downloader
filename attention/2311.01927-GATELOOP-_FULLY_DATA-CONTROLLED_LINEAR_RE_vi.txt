# 2311.01927.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2311.01927.pdf
# Kích thước tệp: 7432869 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
GATELOOP: TẢI HIỆN TUYẾN TÍNH ĐƯỢC KIỂM SOÁT HOÀN TOÀN BỞI DỮ LIỆU ĐỂ MÔ HÌNH HÓA CHUỖI
Tobias Katsch
Artificial Intelligence Program
Johannes Kepler University
Linz, 4040, Austria
tobias.katsch42@gmail.com
TÓM TẮT
Tái hiện Tuyến tính đã được chứng minh là một công cụ mạnh mẽ để mô hình hóa các chuỗi dài một cách hiệu quả. Trong công trình này, chúng tôi chỉ ra rằng các mô hình hiện tại không tận dụng hết tiềm năng của nó. Được thúc đẩy bởi phát hiện này, chúng tôi phát triển GateLoop, một mô hình chuỗi nền tảng tổng quát hóa các mô hình tái hiện tuyến tính như S4, S5, LRU và RetNet, bằng cách sử dụng các chuyển đổi trạng thái được kiểm soát bởi dữ liệu. Sử dụng tiến bộ lý thuyết này, GateLoop vượt trội hơn các mô hình hiện tại về mặt thực nghiệm cho mô hình hóa ngôn ngữ tự hồi quy. Phương pháp của chúng tôi đi kèm với chế độ tái hiện chi phí thấp O(l) và chế độ song song hiệu quả O(llog2l), trong đó l là độ dài chuỗi, tận dụng các triển khai quét kết hợp được tối ưu hóa cao. Hơn nữa, chúng tôi dẫn xuất chế độ attention thay thế O(l2), tiết lộ những ý nghĩa đáng chú ý cho Transformer và các kiến trúc được đề xuất gần đây. Cụ thể, chúng tôi chứng minh rằng cách tiếp cận của chúng tôi có thể được diễn giải như việc cung cấp thông tin vị trí tương đối được kiểm soát bởi dữ liệu cho Attention. Trong khi nhiều mô hình hiện tại chỉ dựa vào tổng tích lũy được kiểm soát bởi dữ liệu để tổng hợp bối cảnh, các phát hiện của chúng tôi gợi ý rằng việc kết hợp các tích tích lũy phức được kiểm soát bởi dữ liệu có thể là một bước quan trọng hướng tới các mô hình chuỗi mạnh mẽ hơn.

Hình 1: Khung làm việc GateLoop lấy các giá trị phụ thuộc đầu vào V, khóa K, truy vấn Q và chuyển đổi trạng thái A. Tại mỗi bước của tái hiện, đầu vào, trạng thái ẩn và đầu ra của vòng lặp được gated. Trong khi S4, S5, LRU hoặc RetNet quên ở tốc độ suy giảm cố định, cách tiếp cận được kiểm soát hoàn toàn bởi dữ liệu cho phép kết hợp thông tin mới phụ thuộc đầu vào, giữ lại ký ức và quên.

1 GIỚI THIỆU
Mô hình hóa các chuỗi qua các phương thức khác nhau chứa các phụ thuộc tầm xa là một thách thức trung tâm trong học máy. Về mặt lịch sử, Mạng Nơ-ron Tái hiện (RNN) đã là lựa chọn tự nhiên cho nhiệm vụ này và dẫn đến những đột phá ban đầu trong lĩnh vực này. Tuy nhiên, RNN gặp phải vấn đề gradient biến mất và bùng nổ, thường khiến chúng không ổn định khi huấn luyện trên các chuỗi dài (Hochreiter & Schmidhuber (1997)). Các biến thể có cổng như LSTM và GRU được phát triển để giải quyết vấn đề này nhưng vẫn vốn dĩ không hiệu quả để huấn luyện do bản chất tái hiện phi tuyến của chúng. Hơn nữa, bản chất tuần tự của chúng dẫn đến một thiên hướng cảm ứng đối với các đầu vào gần đây, hạn chế khả năng thực tế của chúng trong việc rút ra các phụ thuộc tầm xa. Điều này đã truyền cảm hứng cho cơ chế attention (Garg et al. (2019)), được giới thiệu lần đầu như một bổ sung cho RNN để dịch ngôn ngữ, cho phép mô hình rút ra các phụ thuộc toàn cục theo cặp giữa các điểm dữ liệu đầu vào.

Vaswani et al. (2023) đã đi xa hơn với Transformer, loại bỏ hoàn toàn tái hiện và chỉ dựa vào attention. Những ưu điểm chính của Transformer là khả năng huấn luyện song song hiệu quả trên phần cứng hiện đại và khả năng rút ra các phụ thuộc toàn cục theo cặp. Tính chất sau đi kèm với cái giá của độ phức tạp bậc hai O(l2) so với độ phức tạp tuyến tính O(l) của RNN. Điều này tạo ra một nút thắt cổ chai thực tế cho nhiều ứng dụng, ví dụ như hạn chế độ dài tài liệu mà một mô hình ngôn ngữ dựa trên transformer có thể thực hiện suy luận. Do đó, nhiều nỗ lực đã được đầu tư vào việc tìm các thay thế attention với độ phức tạp cải thiện. Trong khi các biến thể này như Reformer, Linformer và Performer cung cấp độ phức tạp giảm O(llogl) hoặc O(l), transformer gốc với chỉ những điều chỉnh nhỏ đã chiếm ưu thế do hiệu suất thực tế mạnh mẽ hơn. Hơn nữa, việc rời bỏ tái hiện đã loại bỏ thiên hướng địa phương của mô hình để chú ý nhiều hơn đến các đầu vào gần đây. Trong khi việc không có thiên hướng này có lợi cho một số nhiệm vụ, nó đã được chứng minh là bất lợi cho những nhiệm vụ khác. Điều này dẫn đến một dòng công việc dành riêng cho việc tiêm thiên hướng địa phương vào Transformer (Ma et al. (2023), Huang et al. (2023)).

Trong khi đó, các công trình của Gu et al. (2022) về việc khởi tạo các Mô hình Không gian Trạng thái (SSM) rời rạc đã dẫn đến sự hồi sinh của RNN tuyến tính để mô hình hóa các chuỗi dài. Mô hình nổi bật nhất của lớp này S4 và biến thể đường chéo đơn giản hóa S4D, đạt được kết quả đáng chú ý trên Long-range Arena (LRA) (Tay et al. (2020)), một benchmark được thiết kế để kiểm tra khả năng mô hình hóa các phụ thuộc tầm xa của mô hình. SSM có thể được huấn luyện hiệu quả bằng cách khai thác bản chất tuyến tính và bất biến thời gian của chúng. Bằng cách viết lại tái hiện tuyến tính như một tích chập dài, nó có thể được tính toán thông qua miền Fourier trong độ phức tạp thời gian O(llogl). Smith et al. (2023b) đã giới thiệu S5, tiếp tục đơn giản hóa việc áp dụng SSM và phổ biến việc sử dụng các triển khai quét kết hợp cho huấn luyện song song nhanh.

Tuy nhiên, SSM vẫn phụ thuộc nặng vào các lược đồ khởi tạo phức tạp. Được thúc đẩy bởi câu hỏi liệu việc khởi tạo tẻ nhạt như vậy có thực sự cần thiết hay không, Orvieto et al. (2023) đã phát triển Đơn vị Tái hiện Tuyến tính (LRU) ngang bằng với S4, S4D và S5 trong khi chỉ yêu cầu khởi tạo đơn giản hơn nhiều.

Những đóng góp của chúng tôi cho dòng công việc này có ba mặt:
• Chúng tôi chỉ ra rằng các mô hình hiện tại chỉ sử dụng một trường hợp đặc biệt của tái hiện tuyến tính. Được thúc đẩy bởi quan sát này, chúng tôi phát triển GateLoop, một mô hình chuỗi nền tảng tổng quát hóa các mô hình tái hiện tuyến tính hiện tại bằng cách sử dụng gating được kiểm soát bởi dữ liệu của đầu vào, trạng thái ẩn và đầu ra. GateLoop có thể được huấn luyện hiệu quả trong O(llogl) tận dụng các triển khai quét kết hợp được tối ưu hóa cao.
• Hơn nữa, chúng tôi dẫn xuất một chế độ tương đương O(l2) liên kết GateLoop với Transformer và chứng minh rằng cách tiếp cận của chúng tôi có thể được diễn giải như việc cung cấp thông tin vị trí tương đối được kiểm soát bởi dữ liệu cho attention.
• Cuối cùng, chúng tôi chứng minh hiệu quả thực nghiệm của cách tiếp cận của chúng tôi. Cụ thể, kết quả của chúng tôi cho thấy GateLoop vượt trội hơn các mô hình tiên tiến Transformer, Hyena (Poli et al. (2023)) và S5-Hyena (Smith et al. (2023a)) trên benchmark WikiText103 cho mô hình hóa ngôn ngữ tự hồi quy.

2 KIẾN THỨC CƠ BẢN
Chúng tôi xem xét nhiệm vụ xấp xỉ các ánh xạ chuỗi-đến-chuỗi. Mô hình nhận một chuỗi đầu vào đa kênh x={x1, . . . , xl} được đóng gói như một ma trận X∈Rl×dx và xuất ra Y∈Rl×dy. Một giả định phổ biến trong bối cảnh này là tính nhân quả, ngụ ý rằng để mô hình hóa yn, chỉ thông tin từ tất cả xm với m≤n mới có thể được sử dụng. Điều này cho phép các chiến lược huấn luyện hiệu quả như mô hình hóa ngôn ngữ tự hồi quy.

2

--- TRANG 2 ---
2.1 MẠNG NƠ-RON TÁI HIỆN
Một lớp Mạng Nơ-ron Tái hiện (RNN) xấp xỉ một ánh xạ chuỗi-đến-chuỗi thông qua quan hệ tái hiện sau đây bao gồm các tham số có thể học A∈Rdh×dh, B∈Rdh×dx, C∈Rdy×dh và một hàm kích hoạt σ.1
hn=σ(Ahn−1+Bxn), yn=Chn (1)
Các lựa chọn phổ biến cho σ là tanh hoặc sigmoid. Nếu chúng ta chọn σ là hàm đồng nhất, lớp RNN trở thành tuyến tính.

2.2 MÔ HÌNH KHÔNG GIAN TRẠNG THÁI
Mô hình không gian trạng thái liên tục (SSM) được đặc trưng bởi phương trình vi phân 2. Ở đây, ˜A∈Cdh×dh, ˜B∈Cdh×dx, ˜C∈Cdy×dh là các giá trị phức, hàm ℜ(.) trích xuất phần thực và ˜h(0) được định nghĩa là 0.
d˜h(t)/dt=˜A˜h(t)+˜Bx(t), y(t)=ℜ(˜C˜h(t)) (2)
Hơn nữa, ˜A có thể được chéo hóa thông qua phân tích eigenvalue ˜A=VΛV−1. Trong biểu diễn này, Λ là ma trận chéo của các eigenvalue, và V là ma trận của các eigenvector tương ứng. Bây giờ, bằng cách hấp thụ V và V−1 vào ˜C và ˜B, tương ứng, chúng ta thu được SSM được chéo hóa. Để biết thêm chi tiết về quy trình này, xin vui lòng xem Smith et al. (2023b).
˜B=V−1˜B, ˜C=˜CV, ˜h(t)=V−1˜h(t) (3a)
d˜h(t)/dt=Λ˜h(t)+˜Bx(t), y(t)=ℜ(˜C˜h(t)) (3b)

Để sử dụng SSM thực tế cho mô hình hóa chuỗi, chúng có thể được rời rạc hóa, ví dụ, thông qua zero-order hold (ZOH), phương pháp song tuyến tính, hoặc Euler. Cho một kích thước bước rời rạc cố định Δ∈R+, phương pháp ZOH tạo ra quan hệ tái hiện tuyến tính
hn=Ahn−1+Bxn, yn=ℜ(Chn) (4)
với tham số hóa:
A=exp(ΔΛ), B=Λ−1(A−I)˜B, C=˜C (5)

Rời rạc hóa mô hình không gian trạng thái (4) cho một lớp RNN tuyến tính (1) bao gồm các tham số hóa lại đặc biệt của các trọng số của nó. Trong khi kết quả này đơn giản là nghiệm của việc áp dụng phương pháp ZOH, điều đáng chú ý là tính diễn giải của nó. Cụ thể, hãy xem xét ảnh hưởng của kích thước bước rời rạc hóa:
lim Δ→0(A,B)=(I,0) (6)

Trong giới hạn Δ→0, không có thông tin mới nào đi vào mô hình không gian trạng thái và trạng thái ẩn vẫn không đổi. Một Δ nhỏ dẫn đến một ánh xạ chuỗi-đến-chuỗi với tốc độ thay đổi nhỏ, trong khi một Δ lớn dẫn đến tốc độ thay đổi lớn. Rõ ràng là kích thước bước có tác động quan trọng đến các tính chất giữ lại/quên của mô hình. Đối với S5, Smith et al. (2023b) định nghĩa Δ như một vector tham số có thể học, trong đó các giá trị mặc định cho khởi tạo được phân bố theo logarit từ 0.001 đến 0.1. Điều này được thực hiện để tạo điều kiện cho việc học các phụ thuộc qua các thang thời gian khác nhau.

Gu et al. (2022) quan sát rằng việc huấn luyện SSM với khởi tạo tham số ngây thơ cho chuyển đổi trạng thái ˜A không hiệu quả trong thực tế. Dựa trên các kết quả nén bộ nhớ lý thuyết, họ phát triển khung HiPPO, mà họ sử dụng để tìm các khởi tạo phù hợp. Các mô hình của lớp này bao gồm S4, DSS, S4D và S5. Các khởi tạo khác, không dựa vào lý thuyết HiPPO, cũng như không dựa vào sự tương ứng với biểu diễn SSM liên tục đã được đề xuất như cho LRU (Orvieto et al. (2023)) và RetNet (Sun et al. (2023)).

S4D: Khởi tạo tất định S4D-Lin định nghĩa chuyển đổi trạng thái chéo ˜a tại chiều kênh k là ˜ak=−1/2+iπk. Thay vào đó, khởi tạo S4D-Inv là ˜ak=−1/2+il/π(l/k+1+1). Ở đây, ˜a được tham số hóa trong không gian liên tục. Thông qua rời rạc hóa ZOH của nó, a được thu được.

LRU: Khởi tạo hàm mũ ổn định được định nghĩa là a=exp(−exp(α)+iexp(θ)), trong đó α và θ là các tham số có thể học.

RetNet: Sun et al. (2023) áp dụng một công thức chuyển đổi trạng thái cố định liên quan chặt chẽ đến nhúng vị trí xPos cho transformer (Sun et al. (2022)). Đối với mô hình này, chúng ta có a=γexp(iθ) với khởi tạo magnitude γ=1−2−5−c, trong đó c là một số hằng số dương.

3 TÁI HIỆN TUYẾN TÍNH ĐƯỢC KIỂM SOÁT BỞI DỮ LIỆU
Việc kết hợp kiểm soát dữ liệu vào các mô hình học sâu đã được chứng minh là rất thành công trong việc phát triển các mô hình chuỗi hiệu suất cao. Transformer, về cốt lõi, được xây dựng trên toán tử tuyến tính được kiểm soát bởi dữ liệu được triển khai bởi attention (Massaroli et al. (2021)). Hơn nữa, Fu et al. (2023) chỉ ra rằng, SSM thiếu kiểm soát dữ liệu cần thiết để mô hình hóa ngôn ngữ một cách đầy đủ. Dựa trên quan sát này, họ phát triển H3 sử dụng SSM kết hợp với gating theo từng phần tử được kiểm soát bởi dữ liệu. Với bổ sung này, họ giảm khoảng cách biểu đạt giữa Transformer và các mô hình dựa trên SSM cho các nhiệm vụ mô hình hóa ngôn ngữ. Được truyền cảm hứng bởi những phát hiện này, chúng tôi đưa paradigm kiểm soát dữ liệu đi xa hơn.

Hình 2: Bỏ qua B, C và áp dụng ℜ(.) để rõ ràng, trước tiên chúng tôi định nghĩa các gate đầu vào và đầu ra kn, qn∈C1×dh (vector hàng), theo Sun et al. (2023). Tiếp theo, như đóng góp cốt lõi của chúng tôi, chúng tôi thay thế chuyển đổi trạng thái tĩnh bằng các chuyển đổi trạng thái nhận biết nội dung (chéo) an∈Cdh×dh. Điều này cho phép kiểm soát thay đổi theo thời gian đối với hành vi quên và giữ lại. Trong khi qn và kn hoạt động như các gate đầu vào và đầu ra tương ứng, an có thể được diễn giải như một gate quên và giữ lại. Đặt mọi thứ lại với nhau, chúng ta thu được GateLoop, được đặc trưng bởi quan hệ tái hiện tuyến tính 7. Chúng tôi giả thuyết rằng, cho phép kiểm soát thay đổi theo thời gian đối với hành vi quên/giữ lại có thể cho phép các mô hình chuỗi giữ các ký ức quan trọng lâu hơn và loại bỏ các ký ức không quan trọng nhanh hơn so với chỉ dựa vào các gate tĩnh. Trong phần 5 chúng tôi trình bày kết quả thực nghiệm xác nhận giả thuyết này.

hn=hn−1⊙an+k⊤nvn (7)
yn=qnhn (8)

Để tổng quát, chúng tôi định nghĩa một tích ngoài đi vào gate loop dẫn đến một trạng thái ẩn hn có hình dạng Cdh×dh. Chọn một biến thể max-headed (thực tế), tức là dh=1, chúng ta thu được trường hợp SISO trùng khớp với các định nghĩa trước đó và gating theo từng phần tử khi song song hóa qua nhiều kênh. Mở rộng quan hệ tái hiện tạo ra phương trình 9, bao gồm một tổng tích lũy trên các bước thời gian trước đó được giảm giá bởi một tích tích lũy của các chuyển đổi trạng thái.

yn=qn∑(m=1 to n)k⊤mvmΠ(j=m+1 to n)aj (9)

4

--- TRANG 3 ---
3.1 QUAN HỆ VỚI CÁC MÔ HÌNH KHÁC

S4, S4D, LRU: Các mô hình này được thu được như một trường hợp đặc biệt của GateLoop khi không sử dụng gating nhận biết nội dung, cũng như các chuyển đổi trạng thái được kiểm soát bởi dữ liệu và chỉ sử dụng chế độ SISO. Quan hệ tái hiện tuyến tính định nghĩa của chúng có thể được mở rộng thành một biểu thức tương đương với việc tích chập v với một bộ lọc có cấu trúc. Ngược lại, GateLoop không thể được tính toán thông qua tích chập và thay vào đó chúng tôi sử dụng quét kết hợp để tính toán hiệu quả. Điều này được nêu trong phần 3.2.
yn=∑(m=1 to n)vman−m=(V∗(1dh,a,...,al−1))n (10)

Hyena: Poli et al. (2023) thu được Hyena như tổng quát hóa của H3 dựa trên SSM bằng cách xem xét các tích chập ngầm dài được định nghĩa tùy ý có dạng yn=v∗(K1,...,Kl). Do đó, cả GateLoop và Hyena đều là các tổng quát hóa loại trừ lẫn nhau của lớp RNN tuyến tính.

RetNet: Phương pháp của chúng tôi suy biến thành RetNet khi giữ các gate đầu vào và đầu ra được kiểm soát bởi dữ liệu nhưng cố định gate chuyển đổi trạng thái.
yn=qn∑(m=1 to n)k⊤mvman−m (11)

3.2 TÍNH TOÁN QUÉT KẾT HỢP HIỆU QUẢ

Smith et al. (2023b) đã phổ biến việc sử dụng các triển khai quét kết hợp để tính toán hiệu quả song song của tái hiện tuyến tính. Trong phần này, chúng tôi tổng quát hóa cách tiếp cận của họ để dẫn xuất một phương pháp hiệu quả để tính toán quan hệ tái hiện 7 cho n=1...l được song song hóa trong độ phức tạp thời gian O(llog2l). Cho một toán tử kết hợp tùy ý •, và một chuỗi các phần tử {xn}ln=1, một quét kết hợp tính toán tổng tiền tố tất cả của chúng Σ.
Σ({xn}ln=1)=((x1),(x1•x2),(x1•x2•x3),...,(x1•x2•...•xl)) (12)

Quan hệ tái hiện trong 7 thỏa mãn dạng này khi sắp xếp các phần tử an và k⊤nvn như các phần tử lá tuple {xn}ln=1={(an,k⊤nvn)}ln=1 và định nghĩa • như sau.
p•q=(p1,p2)•(q1,q2)=(p1q1,q1p2+q2) (13)

Để biết thông tin chi tiết hơn về các thuật toán tổng tiền tố, chúng tôi tham khảo Blelloch (1990). Quét kết hợp tính toán tổng tiền tố hiệu quả song song thông qua áp dụng toán tử nhị phân trên một đồ thị cây tính toán. Một hình ảnh hóa của quá trình này và chứng minh tính kết hợp của toán tử nhị phân liên quan có thể được tìm thấy trong phụ lục B. Lưu ý rằng, quét song song có thể tạo ra nút thắt cổ chai bộ nhớ làm việc trong thực tế cho l×nrheads×dh×dh lớn. Trong phần sau, chúng tôi cung cấp một triển khai python JAX đơn giản của toán tử GateLoop.

5

--- TRANG 4 ---
3.3 BIỂU DIỄN ATTENTION THAY THẾ

Trong phần này, chúng tôi dẫn xuất một chế độ attention thay thế tương đương về mặt toán học để tính toán tái hiện trong O(l2). Để làm điều này, trước tiên chúng tôi viết lại tích tích lũy của các chuyển đổi trạng thái để tách các biến n và m.

yn=qn∑(m=1 to n)k⊤mvm[Π(j=1 to n)aj][Π(j=1 to m)a−1j] (14)

=∑(m=1 to n)[qnΠ(j=1 to n)aj][kmΠ(j=1 to m)a−1j]⊤vm (15)

Sử dụng sắp xếp này, chúng ta có thể tính toán thuận tiện tích-tích-lũy-tiền-tố πn của các chuyển đổi trạng thái.
πn=Π(j=1 to n)aj (16)
yn=∑(m=1 to n)(qnπn)[kmπ−1m]⊤vm (17)

Từ đây, công thức attention thay thế song song O(l2) có thể được thu được bằng cách đóng gói tích-tích-lũy-tiền-tố trong một ma trận Π(A)∈Cl×d và bằng cách áp dụng một mặt nạ nhân quả M∈Rl×l cho ma trận attention thay thế kết quả.
Q=Q⊙Π(A) (18)
K=K⊙Π(A)−1 (19)
Mnm={1 nếu n≥m, 0 nếu n<m} (20)
Y=(QK⊤⊙M)V (21)

Hình 3: Xem xét công thức thay thế này, cách tiếp cận của chúng tôi có thể được diễn giải như việc cung cấp thông tin vị trí tương đối được kiểm soát bởi dữ liệu cho Attention. Lưu ý rằng, công thức này khó thực hiện trong thực tế do nguy cơ underflow trong quá trình tính toán tích tích lũy.

3.4 TỔNG QUÁT HÓA SOFTMAX-ATTENTION

Biểu diễn O(l2) cũng mở ra cơ hội tổng quát hóa cho các dạng attention (phi tuyến) khác. Đối với softmax attention, điều này có thể đạt được bằng cách đơn giản che mặt nạ ma trận tam giác trên của điểm attention được truyền thông tin vị trí tương đối bằng −∞ và sau đó áp dụng softmax. Softmax đặt các mục −∞ thành 0 dẫn đến việc tái cân bằng mong muốn của các điểm attention.
M−∞(X)={Xij nếu i≥j, −∞ nếu i<j} (22)
Y=Softmax(M−∞(QK⊤))V (23)

6

--- TRANG 5 ---
4 TRIỂN KHAI THỰC TẾ

Để sử dụng khung GateLoop một cách thực tế, chúng tôi định nghĩa một mô hình đơn giản nhưng mạnh mẽ. Tính toán quét song song được nêu trong phần 3.2 đã được sử dụng cho tất cả các thí nghiệm. Để thu được các giá trị vn, khóa kn, và truy vấn qn, chúng tôi áp dụng các phép chiếu tuyến tính cho đầu vào xn, theo Vaswani et al. (2023). Như được đề xuất bởi Orvieto et al. (2023) và Sun et al. (2023), chúng tôi kiểm soát magnitude và phase của các chuyển đổi trạng thái riêng biệt.

qn=Linearq(xn), kn=Lineark(xn), vn=Linearv(xn) (24)
an=f(Linearγ(xn))exp(ig(Linearθ(xn))) (25)

Được truyền cảm hứng bởi việc rời rạc hóa mô hình không gian trạng thái, Orvieto et al. (2023) sử dụng tham số hóa không được kiểm soát bởi dữ liệu cho magnitude |a|=exp(−exp(α)), và cho phase arg(a)=exp(β) trong đó α và β là các tham số mô hình. Điều này hạn chế magnitude |a| trong khoảng (0,1) ngăn chặn sự bùng nổ của an−m cho n→∞.

Hình 4: Kích hoạt biên độ hàm mũ ổn định được triển khai bởi LRU thiên về các biên độ gần 1. Thiên hướng này rõ ràng khi vẽ hàm kích hoạt biên độ hàm mũ ổn định (trung tâm). Ngược lại, hàm sigmoid không có thiên hướng này. Cho các thí nghiệm của chúng tôi, chúng tôi chọn sigmoid như kích hoạt magnitude. Bởi vì phần ảo của một chuyển đổi trạng thái riêng lẻ không nhất thiết phải được hạn chế trong một khoảng cụ thể, chúng tôi bỏ qua kích hoạt phase. Để biết chi tiết mô hình, chúng tôi tham khảo phụ lục C.

5 KẾT QUẢ THỰC NGHIỆM

Trong phần này, chúng tôi báo cáo kết quả thực nghiệm xác thực giả thuyết của chúng tôi rằng các chuyển đổi trạng thái được kiểm soát bởi dữ liệu mang lại lợi ích thực nghiệm trong mô hình hóa chuỗi. Trước tiên chúng tôi thiết kế một nhiệm vụ mô hình hóa ngôn ngữ tổng hợp cung cấp những hiểu biết có thể diễn giải cho phương pháp của chúng tôi. Hơn nữa, chúng tôi đánh giá hiệu suất của phương pháp chúng tôi cho mô hình hóa ngôn ngữ tự nhiên tự hồi quy. Cho điều này chúng tôi tiến hành thí nghiệm trên benchmark WikiText-103 được công nhận rộng rãi.

5.1 CHÂN TRỜI BỘ NHỚ

Các tập dữ liệu tổng hợp đã đóng một vai trò quan trọng để hướng dẫn phát triển mô hình, làm nổi bật các ưu điểm và nhược điểm cụ thể của mô hình và để cải thiện khả năng diễn giải mô hình. (Olsson et al. (2022), Fu et al. (2023)). Chúng tôi định nghĩa nhiệm vụ tổng hợp riêng của chúng tôi, được thiết kế đặc biệt để xác thực lợi thế thực nghiệm của các chuyển đổi trạng thái được kiểm soát bởi dữ liệu so với không được kiểm soát bởi dữ liệu. Tập dữ liệu Memory Horizon cho mô hình hóa ngôn ngữ tổng hợp tự hồi quy được chỉ định thông qua một phạm vi số đầu vào, một token reset, độ dài chuỗi và số lượng reset ngẫu nhiên trên mỗi mẫu. Để giải quyết nhiệm vụ này thành công, tại mỗi bước thời gian, thông tin đầu vào quá khứ trở về token reset tiền cấp cuối cùng cần được ghi nhớ. Chúng tôi tham khảo phụ lục A để biết chi tiết về hàm nén mục tiêu cơ bản và các tham số xây dựng tập dữ liệu. Nhiệm vụ được thiết kế để ủng hộ các mô hình có thể quên ký ức trước token reset gặp phải. Mặc dù đây là một ngôn ngữ tổng hợp, chúng tôi giả thuyết và sau đó chứng minh trong phần 5.2, rằng khả năng cơ bản để quên ký ức dựa trên đầu vào là quan trọng để mô hình hóa hiệu quả các chuỗi từ các phương thức thực tế hơn.

7

--- TRANG 6 ---
Hình 5: Chúng tôi hình ảnh hóa các magnitude chuyển đổi trạng thái được áp dụng của mô hình tái hiện tuyến tính được kiểm soát hoàn toàn bởi dữ liệu đã được huấn luyện, sử dụng một chuỗi ví dụ từ tập dữ liệu Memory Horizon. Chi tiết tập dữ liệu và siêu tham số có thể được tìm thấy trong phụ lục A và C.1 tương ứng. Đối với tất cả các lớp và kênh mô hình (theo chiều dọc), các kích hoạt magnitude được vẽ dọc theo độ dài chuỗi (theo chiều ngang). Hơn nữa, các trung bình kích hoạt magnitude qua các kênh và lớp được hiển thị. Như đã giả thuyết, thông qua tái hiện tuyến tính được kiểm soát bởi dữ liệu, mô hình này có thể học quên ký ức phụ thuộc đầu vào bằng cách áp dụng một chuyển đổi trạng thái (gần như) bằng không tại các vị trí reset lý tưởng, hiệu quả làm trống trạng thái ẩn của nó cho thông tin liên quan mới.

Loại chuyển đổi trạng thái | Độ chính xác Test
Được Kiểm soát bởi Dữ liệu | 0.43
Cố định | 0.25

Hình 6: Chúng tôi so sánh độ chính xác test của thực thể mô hình GateLoop với mô hình tái hiện tuyến tính thứ hai đã được huấn luyện, chỉ khác ở việc sử dụng chuyển đổi trạng thái cố định. Kết quả cho thấy việc làm cho cơ chế quên/giữ lại phụ thuộc đầu vào cải thiện độ chính xác test đáng kể.

Hình 7: Chúng tôi vẽ độ chính xác test theo nhịp bộ nhớ yêu cầu. Không có gì đáng ngạc nhiên, việc dự đoán token đúng trở nên khó khăn hơn khi dung lượng bộ nhớ cần thiết tăng lên. Đối với tất cả các nhịp bộ nhớ yêu cầu, biến thể được kiểm soát hoàn toàn bởi dữ liệu hoạt động tốt hơn biến thể 'cố định'. Trong khi hiệu suất của biến thể mô hình sau giảm nhanh sau khi nhịp bộ nhớ yêu cầu vượt quá 50, biến thể mô hình trước duy trì hiệu suất tương đương trong thời gian dài gấp đôi. Kết luận, nhiệm vụ mô hình hóa ngôn ngữ tổng hợp đơn giản này xác nhận rằng kiểm soát phụ thuộc dữ liệu đối với các tính chất quên/giữ lại có thể cải thiện khả năng mô hình hóa chuỗi trong thực tế.

5.2 WIKITEXT103

Tập dữ liệu WikiText103 cho mô hình hóa ngôn ngữ tự nhiên tự hồi quy bao gồm hơn 100 triệu token được trích xuất từ các bài viết Wikipedia đã được xác minh. Chúng tôi kiểm tra mô hình tái hiện tuyến tính được kiểm soát hoàn toàn bởi dữ liệu của chúng tôi so với cuộc cạnh tranh tiên tiến. Chi tiết mô hình được báo cáo trong phần C.

8

--- TRANG 7 ---
Bảng 1: So sánh perplexity test WikiText103 (thấp hơn là tốt hơn) của các mô hình khác nhau. Tất cả các mô hình sử dụng cùng một tokenizer. Kết quả cho các mô hình khác được lấy từ Poli et al. (2023) và Smith et al. (2023a).

Mô hình | Tham số | Test Perplexity
Transformer | 125M | 18.6
Hybrid H3 | 125M | 18.5
Performer | 125M | 26.8
Reformer | 125M | 26.0
Linear Attention | 125M | 25.6
Transformer-XL | 258M | 18.4
Hyena | 125M | 18.5
S5-Hyena | 125M | 18.3
GateLoop | 125M | 13.4

GateLoop có một bước nhảy hiệu suất đáng kể vượt trước các mô hình hiện tại trong khi cung cấp các ưu điểm như tránh các lớp softmax-attention (không giống Transformer và Hybrid H3), loại bỏ nhu cầu khởi tạo tẻ nhạt (không giống State Space Models), và không yêu cầu các tích chập ngầm dài (không giống Hyena).

Hình 8: Chúng tôi vẽ các chuyển đổi trạng thái của mô hình đã được huấn luyện cho một batch đầu vào test ngẫu nhiên tại các lớp 0 và 8. Chúng tôi quan sát các mẫu có cấu trúc trong chuyển đổi trạng thái được kiểm soát bởi dữ liệu. Trong khi chúng tôi để lại khả năng diễn giải cho công việc tương lai, chúng tôi chỉ ra rằng những mẫu này cho thấy mô hình đã được huấn luyện cố ý sử dụng gating được kiểm soát bởi dữ liệu của chuyển đổi trạng thái (và do đó quên và giữ lại ký ức) bằng cách áp dụng nhiều loại magnitude và phase.

6 CÔNG VIỆC TƯƠNG LAI

Trong khi trọng tâm chính của chúng tôi trong bài báo này là thiết lập nền tảng để xây dựng RNN tuyến tính được kiểm soát hoàn toàn bởi dữ liệu, chúng tôi nhận ra vô số cơ hội cho nghiên cứu tương lai. Một hướng bao gồm khám phá các hiệu ứng của các chiến lược khởi tạo khác nhau, kích hoạt biên độ và phase. Hơn nữa, chúng tôi đề xuất rằng công việc tương lai nên tập trung vào khả năng diễn giải của các chuyển đổi trạng thái đã học để có được hiểu biết sâu sắc hơn về hoạt động bên trong của mô hình.

7 KẾT LUẬN

Chúng tôi giới thiệu GateLoop, một RNN tuyến tính được kiểm soát hoàn toàn bởi dữ liệu tổng quát hóa các mô hình tái hiện tuyến tính hiện tại bằng cách tận dụng gating được kiểm soát bởi dữ liệu của đầu vào và đầu ra và các chuyển đổi trạng thái. Trong khi phương pháp của chúng tôi đi kèm với độ phức tạp runtime tuyến tính O(l), chúng tôi dẫn xuất một chiến lược huấn luyện song song hiệu quả O(llogl) sử dụng quét song song. Hơn nữa, GateLoop có thể được tái công thức trong một chế độ attention thay thế tương đương O(l2) tiết lộ rằng, cơ chế của nó có thể được diễn giải như việc cung cấp thông tin vị trí tương đối cho Attention. Cuối cùng chúng tôi xác thực thực nghiệm rằng, tái hiện tuyến tính được kiểm soát hoàn toàn bởi dữ liệu có hiệu suất cao cho mô hình hóa ngôn ngữ tự hồi quy.

9

--- TRANG 8 ---
TÀI LIỆU THAM KHẢO

Guy Blelloch. Prefix sums and their applications. Tech. rept. CMU-CS-90-190, School of Computer Science, Carnegie Mellon, 1990.

Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models, 2023.

Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. Jointly learning to align and translate with transformer models, 2019.

Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces, 2022.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8): 1735–1780, nov 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.

Feiqing Huang, Kexin Lu, Yuxi CAI, Zhen Qin, Yanwen Fang, Guangjian Tian, and Guodong Li. Encoding recurrence into transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=7YfHla7IxBJ.

Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention, 2023.

Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting neural odes, 2021.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022.

Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences, 2023.

Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models, 2023.

Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified State Space Layers for Sequence Modeling [source code]. https://github.com/lindermanlab/S5, 2023a.

Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2023b.

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer, 2022.

Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers, 2020.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.

10

--- TRANG 9 ---
A CHI TIẾT TẬP DỮ LIỆU MEMORY HORIZON

Trong phần này, chúng tôi mô tả chi tiết của Tập dữ liệu Memory Horizon cho mô hình hóa ngôn ngữ tổng hợp. Mục tiêu của tập dữ liệu này là làm nổi bật lợi thế của các chuyển đổi trạng thái được kiểm soát bởi dữ liệu so với không được kiểm soát bởi dữ liệu cho các mô hình tái hiện tuyến tính.

Bảng 2: Bảng này liệt kê các tham số chúng tôi sử dụng để xây dựng Tập dữ liệu Memory Horizon. Từ vựng đầu vào bao gồm một token reset và các token số cho tất cả các số trong phạm vi số đầu vào. Từ vựng đầu ra bao gồm các token số từ 0 đến số đầu ra tối đa.

Tham số | Giá trị
Phạm vi số đầu vào | [0,4]
Độ dài chuỗi | 1024
Reset trên mỗi mẫu | 3
Đầu ra tối đa | 50
Số lượng mẫu | 2000

Hơn nữa, chúng tôi áp dụng một hàm nén bộ nhớ tính toán token mục tiêu dựa trên một danh sách các token số đầu vào. Danh sách này mở rộng từ token reset gần nhất đến cuối chuỗi đầu vào, hoặc nếu không có token reset nào, từ đầu chuỗi. Hàm tính toán một tổng luân phiên của các tích bằng cách nhân các cặp số từ các đầu đối diện của danh sách. Thao tác luân phiên giữa cộng và trừ cho mỗi cặp. Trong trường hợp danh sách có số phần tử lẻ, phần tử giữa được cộng hoặc trừ, tùy thuộc vào thao tác hiện tại. Cuối cùng, kết quả được lấy modulo một số được chỉ định để nén giá trị bộ nhớ.

11

--- TRANG 10 ---
B QUÉT SONG SONG

Hình 9: Chúng tôi hình ảnh hóa quét song song bao gồm toán tử GateLoop cho 4 phần tử đầu tiên. Để hoàn chỉnh, chúng tôi chỉ ra tính kết hợp của toán tử nhị phân được sử dụng.

Chứng minh.
(a•b)•c = (a1b1, a2+b2)•(c1, c2)
= (a1b1c1, c1(a2+b2)+c2)
= (a1b1c1, c1a2+c1b2+c2)

a•(b•c) = a•(b1c1, b2+c2)
= (a1b1c1, c1a2+c1b2+c2)
= (a1b1c1, c1a2+c1b2+c2)

C CHI TIẾT MÔ HÌNH

Mỗi lớp mô hình bao gồm:
• Một khối Time-Mixing tổng hợp thông tin qua chiều thời gian. Trong trường hợp này, đây là toán tử GateLoop với các đầu vào nhận biết nội dung được định nghĩa. Chúng tôi sử dụng trọng số có giá trị thực cho phép chiếu tuyến tính liên quan và chỉ trả về phần thực của đầu ra GateLoop.
• Một khối Channel-Mixing được thiết kế để xấp xỉ các hàm dọc theo chiều kênh. Trong thí nghiệm này, một FNN đơn giản được áp dụng theo điểm cho các vector chuỗi.
• Skip-Connections và Layer Normalization, được khuyến nghị để cho phép thông tin bỏ qua channel/time mixing và ổn định huấn luyện.

Các mô hình bao gồm:
• Một nhúng token đầu vào đã học.
• Một ngăn xếp các lớp L model, với số lượng cụ thể tùy thuộc vào loại mô hình.
• Một đầu ngôn ngữ, là một phép chiếu tuyến tính ánh xạ đầu ra của lớp cuối cùng thành một phân phối xác suất (thực tế là logits) trên từ vựng. Mô hình được huấn luyện để mô hình hóa phân phối xác suất trên các token đầu ra có thể cho trước bối cảnh đầu vào hiện tại.

12

--- TRANG 11 ---
Hình 10: Hình ảnh hóa kiến trúc mô hình đầy đủ.

C.1 SIÊU THAM SỐ MEMORY HORIZON

Bảng 3: Siêu tham số mô hình được sử dụng cho thí nghiệm MemoryHorizon.

Siêu tham số | Giá trị
Số epoch | 300
Kích thước batch | 32
Tốc độ học | 0.0025
Optimizer | AdamW
Momentum optimizer (β1, β2) | 0.9, 0.98
Weight decay | 0.05
Lịch trình tốc độ học | cosine decay (linear warm-up)
Số bước warm-up | 10000
nlayer | 4
dchannel mixing | 128
dmodel | 64
dqk | 64
dv | 64
nrheads | 64
dh | 1
kích hoạt magnitude | sigmoid
kích hoạt phase | identity

13

--- TRANG 12 ---
C.2 SIÊU THAM SỐ WIKITEXT103

Bảng 4: Siêu tham số được sử dụng cho thí nghiệm WikiText103. Chúng tôi áp dụng tốc độ học nhỏ hơn cho các phép chiếu kiểm soát chuyển đổi trạng thái. Hơn nữa, không có weight decay được áp dụng cho các tham số này.

Siêu tham số | Giá trị
Số epoch | 100
Kích thước batch | 16
Tốc độ học cơ bản | 0.000125
Tốc độ học chuyển đổi trạng thái | 0.0001
Optimizer | AdamW
Momentum optimizer (β1, β2) | 0.9, 0.98
Weight decay | 0.25
Lịch trình tốc độ học | cosine decay (linear warm-up)
Số bước warm-up | 5000
nlayer | 12
dchannel mixing | 1872
dmodel | 624
dqk | 624
dv | 624
nrheads | 624
dh | 1
kích hoạt magnitude | sigmoid
kích hoạt phase | identity

14
