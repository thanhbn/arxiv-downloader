# 2312.07305.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2312.07305.pdf
# File size: 519403 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SCCA: Shifted Cross Chunk Attention for long contextual semantic
expansion
Yuxiang Guo
Beihang University
irisg@buaa.edu.cn
Abstract
Sparse attention as a efficient method can sig-
nificantly decrease the computation cost, but
current sparse attention tend to rely on window
self attention which block the global informa-
tion flow. For this problem, we present Shifted
Cross Chunk Attention (SCCA), using different
KV shifting strategy to extend respective field
in each attention layer. Except, we combine
Dilated Attention(DA) and Dilated Neighbor-
hood Attention(DNA) to present Shifted Di-
lated Attention(SDA). Both SCCA and SDA
can accumulate attention results in multi head
attention to obtain approximate respective field
in full attention. In this paper, we conduct lan-
guage modeling experiments using different
pattern of SCCA and combination of SCCA
and SDA. The proposed shifted cross chunk at-
tention (SCCA) can effectively extend large
language models (LLMs) to longer context
combined with Positional interpolation(PI) and
LoRA than current sparse attention.. Notably,
SCCA adopts LLaMA2 7B from 4k context to
8k in single V100. This attention pattern can
provide a Plug-and-play fine-tuning method to
extend models’ context while retaining their
original architectures, and is compatible with
most existing techniques, like FlashAttention-
2.
1 Introduction
The Transformer architecture is rapidly becoming
one of the most widely applied deep learning ar-
chitectures, and the emergence of Large Language
Models (LLMs) using Transformer has brought
improvements to many tasks. However, a signif-
icant challenge lies in the quadratic computation
complexity introduced by the vanilla transformer,
which hinders the increase in input length.
Some researchers opt for using sparse attention
patterns to reduce computing complexity and save
memory. While sparse transformers like local atten-
tion (Qiu et al., 2020) and sliding window context(Beltagy et al., 2020) based on window size are pro-
posed, these attention pattern face a limitation in in-
formation flow within the window or chunk. Other
approaches, such as dilated window attention (Belt-
agy et al., 2020) and sparse Transformer (Child
et al., 2019), require changes to the model structure
and lack a corresponding CUDA-friendly imple-
mentation. Swin Transformer (Liu et al., 2021)
and Dilated Neighborhood Attention (DNA) pro-
vide a cross-layer attention pattern in chunk-based
attention, introducing information flow between
different chunks or windows. However, global in-
formation flow remains lacking in these methods.
While current LLMs have revolutionized lan-
guage modeling and showcased impressive task per-
formance (Dasigi et al., 2021; Cohan et al., 2018;
Koˇcisk´y et al., 2018; Shi et al., 2023; Huang et al.,
2021; Shaham et al., 2022; Bai et al., 2023), they
are constrained by pre-defined context window size.
The performance significantly declines when input
tokens exceed these limited context length. Direct
context extrapolation in LLMs using positional em-
bedding, such as RoPE, can lead to catastrophic
consequences. To address this out-of-distribution
problem, various Position Interpolation algorithms
(Chen et al., 2023a; Peng and Quesnelle, 2023;
Peng et al., 2023) have been introduced. While
these methods effectively extrapolate the length of
LLMs using RoPE, full fine-tuning is still required.
Longlora (Chen et al., 2023b) introduces a new
insight that sparse attention can be used in the fine-
tuning process to extrapolate the context length of
LLMs, resulting in non-trivial computation savings
with performance similar to full fine-tuning. How-
ever, the lack of information flow persists in the
length-extending process, emphasizing the impor-
tance of an information-efficient attention pattern.
In this paper, we propose Shifted Cross Chunk
Attention (SCCA), which utilizes different key-
value (KV) shift strategies to enable queries to
directly attend outside the same window. WearXiv:2312.07305v1  [cs.CL]  12 Dec 2023

--- PAGE 2 ---
provide two shifting strategies, SCCA fixed and
SCCA flow, to introduce different information
flows, with SCCA flow achieving approximate full
attention within its respective field during the ac-
cumulation of different head attention results with
linear complexity. Additionally, we combine Di-
lated Attention (DA) and Dilated Neighborhood
Attention (DNA) to present Shifted Dilated Atten-
tion (SDA). Both SCCA and SDA can accumulate
attention results in multi-head attention to obtain
an approximate respective field in full attention. To
evaluate the effectiveness of the attention pattern in
extending LLMs’ context length, we conduct lan-
guage modeling experiments using different SCCA
patterns and a combination of SCCA and SDA on
the PG19 validation split and Proof test split. The
proposed SCCA can extend LLMs to a longer con-
text in a more efficient way, combined with Posi-
tional Interpolation (PI) and LoRA, compared to
S2attention used in Longlora (Chen et al., 2023b).
Both SCCA and SDA are plug-and-play fine-tuning
methods which can extend model contexts while
retaining their original architectures.
2 Related work
2.1 Sparse attention
Sparse attention tend to conduct self attention oper-
ation in a sub token sets of a sequence to decrease
computing time and memory. Blockwise attention,
also named local attention (Qiu et al., 2020) break
a sequence with N tokens into n non-overlapping
windows with N/n tokens. The local attention al-
lows one query to attend to tokens within the same
window. Based on this window context, different
sparse pattern are proposed. Sliding window atten-
tion (Beltagy et al., 2020) adapt sliding window
to conduct local attention. Dilated sliding window
further increases the receptive field in a “dilated”
sliding window way (Beltagy et al., 2020). This
is analogous to dilated CNNs (Oord et al., 2016)
where the window has gaps of size dilation d. The
fixed pattern of sparse Transformer(Child et al.,
2019) is composed of a local attention and a strided
attention. Stried attention allow query Q attend
to tokens that are not in the same window. Swin
transformer(Liu et al., 2021) provide a shifted win-
dow attention to allow self-attention computation
both to non-overlapping local windows and cross-
window connection. similar to dilated window at-
tention, LongNet (Ding et al., 2023) and Dilated
Neighborhood Attention(Hassani and Shi, 2023)extend different window size and adapt different
gaps of size dilation d.
2.2 Length extrapolation in LLMs
Length extrapolation aims to ensure that the model
continues to perform well, even when the number
of input tokens during inference exceeds the size of
the context window on which the model is trained
(Press et al., 2021). While certain techniques such
as ALiBi (Press et al., 2022) and LeX (Sun et al.,
2023) enable length extrapolation of Transformers,
i.e. train on short context windows and inference
on longer ones, many existing pre-trained LLMs,
including LLaMA (Touvron et al., 2023), use posi-
tional encodings that have weak extrapolation prop-
erties (e.g., RoPE (Su et al., 2021)). One one ques-
tion exists in these LLMs is directly extrapolate
context length in inference processing can bring a
catastrophic performance and training LLMs with
long context from scratch is prohibitively expen-
sive for most researchers. Position Interpolation
(Chen et al., 2023a) introduces a modification upon
RoPE and extends the context length of LLaMA
to 32768. Subsequently, a range of Positional In-
terpolation (PI) strategies like NTK ((Peng and
Quesnelle, 2023)) and YaRN (Peng et al., 2023)
have been introduced. While these methods make
the length extrapolation of LLMs using RoPE ef-
fective, full fine-tuning is still required. Longlora
(Chen et al., 2023b) propose a new insight that
sparse attention can be used in fine-tuing process
to extrapolate the contex, leading to non-trivial
computation saving with similar performance to
fine-tuning. Different from training in a full-length,
some researchers choice to design suitable training
strategy to extend context length in original con-
text window. PoSE (Zhu et al., 2023) manages to
decouple train / target length, requiring only the
original context size for fine-tuning.
2.3 LongBench
The field of NLP has long sought to endow ma-
chines with the ability to understand and reason
over a long context (Dasigi et al., 2021). Tasks
such as summarization (Cohan et al., 2018) and
question answering (Ko ˇcisk´y et al., 2018) based
on books, report (Huang et al., 2021), and docu-
ments (Pang et al., 2022), and code generation at
the repository level demand the ability to model
long context sequences that span thousands or even
tens of thousands of tokens in length ( ?). Long-
Bench is the first bilingual, multi-task benchmark

--- PAGE 3 ---
Pseudocode of SCCA flow in PyTorch-like style.
# B: batch size; H: head number; N: sequence length; D: dimension of each attention head
# index : number of heads conduct same shift pattern
# w: group size; # H: number of attention heads;
# K and V in shape (B, H, N, D)
# Key line 2: each index heads shift KV i*w on N length sequence
for i in range(num group):
kv[:, i*index:(i+1)*index] = qkv[:, i*index:(i+1)*index].roll(w*i, dims=2)
kv=kv.reshape(B,H,N
w,w,D)
After shifting KV we need split shifted KV intoN
wchunks
tailored for long context understanding. Long-
Bench (Bai et al., 2023) is composed of 6 ma-
jor task categories and 21 different tasks, cover-
ing key long-text application scenarios including
multi-document QA, single-document QA, summa-
rization, few-shot learning, code completion, and
synthesis tasks. LongBench contains 4,750 test in-
stances, with an average length of 6,711 words for
English instances (including code).
3 Shifted cross chunk attention
Standard self attention using softmax to compute
attention weights of Query Q={Q1, Q2, ..., Q h}
attending Key K={K1, K2, ..., K h}, then dot
Value V={V1, V2, ..., V h}following equation (1),
his the head number, QiKiandQirepresents the
ith head vector in multi head attention. Nrepre-
sents the token number in a sequence. ki,viandqi
represents ith token vector in one head.
Attention (Q, K, V ) =softmax (QKT
√
d)V (1)
We first split QKV vector into mchunks, each
chunk contains wtokens, where m=N
w. Dif-
ferent from S2attention , which redistricts win-
dow by shift Q,KandV, we just shift K and V
and keep the window partition still to make query
Qcito attend Kcjwhere 1<=j <=m. Fig-
ure 1 shows two different patterns in Shifted cross
chunk attention (SCCA for abbreviation) in multi
head attention scenario. Figure 1(a) represents the
SCCA fixed pattern, in which half heads can only
attend within window and the other heads can at-
tend to other window by using SCCA. Figure 1(b)
shows the SCCA flow pattern, each window can
attend to other windows by shifting KV in different
distance in different heads. Where g=w
23.1 Fixed shifted cross chunk attention
KiandViin ith head with shifting will be rear-
ranged into SKiandSVilike equation (2) and
equation (3)
SKi={kN−g−1, kN−g, ..., k N−1, k0, K1, ..., k N−g}(2)
SVi={vN−g−1, vN−g, ..., v N−1, v0, v1, ..., v N−g}(3)
After shifting KV vector we need split then into
different chunks based on window, figure 1 is an ex-
ample which contains four chunks, and each chunk
is composed of four tokens. Then the KV matrix
can be described into Equation (4) and Equation
(5).
K={SK 1, SK 2, ..., SK h/2, Kh/2+1, Kh/2+2, , ..., K h}
(4)
V={SV1, SV 2, ..., SV h/2, Vh/2+1, Vh/2+2, ..., V h}(5)
Qci,KciandVcirespectively represents ith chunk
in multi head Q K V vector, and each chunk con-
tains shifted and non-shifted KV tokens. After
splitting long sequence into chunks, SCCA con-
duct attention operation within each chunk follow-
ing Equation (6).
Attention (Q, K, V ) =
softmax (Qc1KT
c1√
d)Vc1
softmax (Qc2KT
c2√
d)Vc2
...
softmax (Qc3KT
c3√
d)Vc3
(6)
3.2 Flow shifted cross chunk attention
Different like lase section we just shift fix half
group size, this section we propose a new shift
pattern which different head shift different chunk
size to explore the receptive field in one layer.
Figure 1(b) shows the certain process in
SCCA flow during shifting all heads in a different
shift distance. In this situation, shift pattern follows

--- PAGE 4 ---
N heads query
SCA
SCA
SCA
SCAN heads key
N heads valueRolling
ShiftChunkN heads keyHidden 
states
Cross chunk self attentionchunkq_proj
k_proj v_proj
o_proj
Shift and chunk K/VChunk Q
New
Hidden 
statesN heads key
N heads value N heads valueN heads query(a) Shift half chunk
N heads query
SCA
SCA
SCA
SCAN heads key
N heads valueRolling
ShiftChunkN heads keyHidden 
states
Cross chunk self attentionchunkq_proj
k_proj v_proj
o_proj
Shift and chunk k/vChunk q
New
Hidden 
states (b) Shift all group
Figure 1: Two patterns in SCCA, the left figure shows the half head will be right shift half group tokens, the right
figure shows the SCCA flowpattern which each head shift different group number tokens to make query can attend
to all tokens in attention operation
the group number. The target of this pattern is to
simulate the receptive field of full attention through
multi head mechanism.Algorithm ??shows the im-
plementation pseudocode in SCCA flow. Shifting
KV vector and keep query still as Equation (7) (8)
and Equation (9) can explore respective field in one
attention layer by accumulating computing results
of multiple heads. Where wrepresents the group
size in each chunk, and m=N
w, which means one
sequence can be split into mchunks. t=h
/mrep-
resents the head number which have the same shift
distance.
Qij = {qjw+1, qiw+1, qiw+1, ..., q (j+1)w}
means jth chunk query vector in head i,
Kij = {kjw+1, kiw+1, kiw+1, ..., k (j+1)w}
means jth chunk key vector in head i,
Vij={vjw+1, viw+1, viw+1, ..., v (j+1)w}means
jth chunk value vector in head i.
Q=
Q11, Q12, Q13, ..., Q 1m
Q21, Q22, Q23, ..., Q 2m
...
Qh1, Qh2, Qh3, ..., Q hm
(7)
K=
K11, K12, ..., K 1m−1, K1m
...
Kt1, Ktc2, ..., K tcm−1, Ktm
Kt+12, Kt+13, ..., K t+1m, Kt+11
...
K2t2, K2t3, ..., K 2tm, K2t1
...
...
Kh−t+1m, Kh−t+11, ..., K h−t+1m−2, Kh−t+1m−1
...
Khm, Kh1, ..., K hm−2, Khm−1
(8)
V=
V11, V12, ..., V 1m−1, V1m
...
Vt1, Vtc2, ..., V tcm−1, Vtm
Vt+12, Vt+13, ..., V t+1m, Vt+11
...
V2t2, V2t3, ..., V 2tm, V2t1
...
...
Vh−t+1m, Vh−t+11, ..., V h−t+1m−2, Vh−t+1m−1
...
Vhm, Vh1, ..., V hm−2, Vhm−1
(9)
(a) Dilated distance=2
 (b) Dilated distance=4
Figure 2: Illustration of two different pattern in DAT,
DAT conduct sliding window attention in each head.
Top figure shows the pattern dilated distance=2, and
adjacent head tokens start from 1 and 2 respectively,
then repeat this processh
2times. Bottom figure shows
the pattern dilated distance=4, and adjacent head tokens
start from 1, 2, 3, 4 respectively, then repeat this process
h
4times in multi head attention process
After shifting we split into sequencs and con-
duct window attention like Equation (6) to reduce
computing memory and time cost.
4 LongMixed
In this section we propose a new combination that
different sparse attention can be combined to im-
prove model performance in fine-tuning process to
extrapolate context length in LLMs.
Inspired by DAT (Hassani and Shi, 2023) and
LongNet (Ding et al., 2023), we propose Shifted
Dilated Attention (SDA), a sparse global attention.
Figure 2 shows the two patterns of SDA. Similar to
DAT, we select computing tokens in global space,
different from DAT conducting shifted computing
in different attention layer, we shifting start posi-
tion in each head, and this operation is similar to

--- PAGE 5 ---
Table 1: Perplexity of models extended to 8k context
size via PI and different sparse attention pattern on PG19
validation set
attention 8192
LLaMA 2 1000
S2(Chen et al., 2023b) 9.41
SCCA fixed 9.17
SCCA flow 9.47
LongMixed 8.73
Dilated Attention (DA) in LongNet. The difference
from LongNet is that DA select dilated tokens in a
segment which contains a subset of global tokens,
and we directly conduct DA in the whole global
space and do not split any segments or chunks. Fig-
ure 2(a) shows the SDA attention pattern where
dilated distance equal to 2 and 2(b) is the pattern
where dilated distance equal to 4. This method con-
duct a sliding dilated token selection in a sequence
in different heads, and start index is begin from
1,2,3, ..., θ , where θis dilated distance.
Different attention pattern can be combined
during fine-tuning process to extrapolate context
length. In this section, we combine SCCA fixed
and SDA into LongMixed.
5 Experiments
5.1 Settings
Datasets , we use a subset of RedPajama (Com-
puter, 2023) dataset for fine-tuning next token pre-
diction task, we select training samples which to-
ken length larger than 8192 by using LLaMA to-
kenizer. The number of total training samples is
21768. We evaluate the perplexity on PG19 vali-
dation split and Proof-pile dataset (Zhangir Azer-
bayev and Piotrowski, 2022) test split.
Model We select LLaMA2-7B base model as
our evaluation model and compare to the most sim-
ilar attention pattern S2attention. Both two atten-
tion pattern conduct the same training settings.
Attention pattern setting ForSCCA fixed and
SCCA flow, we set chunk number m= 4 and
SCCA fixed right shift halfN
mtokens in half heads,
SCCA flow shiftiwtokens in different head. For
LogMixted, 8 heads are selected to conduct SDA 2
and 16 heads are selected to conduct SDA 4, the
other heads conduct SCCA fixted .
Training and evualtion setting We use Deep-
Speed (Rasley et al., 2020) in Stage 3 during fine-
tuning and LoRA(Hu et al., 2022) setting is thesame as Longlora (Chen et al., 2023b). We use
Adamw Optimizer and the learning rate is set to
2e-5, we use constant and linear learning rate with
warmup, warmup step is 20. We set per-device
batch size as 1 in 32G 8*V100, which means the
global batch size is 8. We fine-tune 1 epoch in
21768 training samples in RedPajama. We evalua-
tion perplexity scores at various evaluation context
window sizes, ranging from 1024 to 8192. For eval-
uation efficiency, we set the stride of the sliding
window to 256 and use 4-bit quantization tech-
nique.
5.2 language modeling results
In Table 2, we report the perplexity for our models
and baseline S2attention on Proof-pile and PG19
datasets. Under certain training context lengths,
SCCA fixed and LongMixed achieve better per-
plexity with 1024,2048,4096 even in 8192 context
thanS2attention. This indicates the effectiveness
of our efficient attention pattern. In Table 2, for
the same training and evaluation context length
cases, the perplexity decreases as the context size
increases. we find some perplexity degradation on
small context sizes for the extended models. This
is a known limitation of Position Interpolation.
References
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
and Juanzi Li. 2023. Longbench: A bilingual, mul-
titask benchmark for long context understanding.
arXiv preprint arXiv:2308.14508 .
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023a. Extending context window
of large language models via positional interpolation.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-
glora: Efficient fine-tuning of long-context large lan-
guage models.
Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. 2019. Generating long sequences with
sparse transformers.
Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, and Nazli
Goharian. 2018. A discourse-aware attention model
for abstractive summarization of long documents. In
Proceedings of the 2018 Conference of the North

--- PAGE 6 ---
Table 2: Perplexity of models extended to 8k context size via PI and different sparse attention pattern on PG19
validation set and Proof test set. The training dataset come from a subset of RedPajama .We show that our proposed
attention pattern have a better performance in 8k context than S2attention
PG19
attention 1024 2048 4096 8192
S2(Chen et al., 2023b) 11.71 10.73 9.98 9.41
SCCA fixed 11.26 10.33 9.63 9.17
SCCA flow 11.59 10.64 9.94 9.47
LongMixed 10.49 9.65 9.10 8.73Proof
1024 2048 4096 8192
3.99 3.83 3.15 2.96
3.95 3.43 3.09 2.88
3.99 3.47 3.13 2.91
3.96 3.46 3.12 2.90
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers) , pages 615–621, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.
Together Computer. 2023. Redpajama: An open
source recipe to reproduce llama training dataset.
https://github.com/togethercomputer/
RedPajama-Data .
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
Noah Smith, and Matt Gardner. 2021. A dataset of
information-seeking questions and answers anchored
in research papers. pages 4599–4610.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,
Shaohan Huang, Wenhui Wang, Nanning Zheng, and
Furu Wei. 2023. Longnet: Scaling transformers to
1,000,000,000 tokens.
Ali Hassani and Humphrey Shi. 2023. Dilated neigh-
borhood attention transformer.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. LoRA: Low-rank adaptation of
large language models. In International Conference
on Learning Representations .
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, and Lu Wang. 2021. Efficient attentions for long
document summarization. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1419–1436, Online.
Association for Computational Linguistics.
Tom´aˇs Koˇcisk´y, Jonathan Schwarz, Phil Blunsom, Chris
Dyer, Karl Moritz Hermann, G ´abor Melis, and Ed-
ward Grefenstette. 2018. The NarrativeQA reading
comprehension challenge. Transactions of the Asso-
ciation for Computational Linguistics , 6:317–328.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Baining Guo. 2021.
Swin transformer: Hierarchical vision transformer
using shifted windows. 2021 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , pages
9992–10002.Aaron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. 2016. Wavenet: A generative model
for raw audio. arXiv preprint arXiv:1609.03499 .
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,
Nikita Nangia, Jason Phang, Angelica Chen, Vishakh
Padmakumar, Johnny Ma, Jana Thompson, He He,
and Samuel Bowman. 2022. QuALITY: Question
answering with long input texts, yes! In Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 5336–5358,
Seattle, United States. Association for Computational
Linguistics.
Bowen Peng and Jeffrey Quesnelle. 2023. Ntk-
aware scaled rope allows llama models to
have extended (8k+) context size without any
fine-tuning and minimal perplexity degrada-
tion. https://www.reddit.com/r/LocalLLaMA/
comments/14lz7j5/ntkaware scaled rope
allows llama models tohave .
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models.
Ofir Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Confer-
ence on Learning Representations .
Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,
Sinong Wang, and Jie Tang. 2020. Blockwise self-
attention for long document understanding. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020 , pages 2555–2565, Online. Association
for Computational Linguistics.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He. 2020. Deepspeed: System opti-
mizations enable training deep learning models with
over 100 billion parameters. In Proceedings of the
26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining , KDD ’20,
page 3505–3506, New York, NY , USA. Association
for Computing Machinery.
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori
Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,

--- PAGE 7 ---
Mor Geva, Jonathan Berant, and Omer Levy. 2022.
SCROLLS: Standardized CompaRison over long lan-
guage sequences. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 12007–12021, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Shuming Shi, Enbo Zhao, Wei Bi, Deng Cai, Leyang
Cui, Xinting Huang, Haiyun Jiang, Duyu Tang,
Kaiqiang Song, Longyue Wang, Chenyan Huang,
Guoping Huang, Yan Wang, and Piji Li. 2023. Ef-
fidit: An assistant for improving writing efficiency.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
3: System Demonstrations) , pages 508–515, Toronto,
Canada. Association for Computational Linguistics.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2023. A length-extrapolatable
transformer. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers) , pages 14590–14604,
Toronto, Canada. Association for Computational Lin-
guistics.
Edward Ayers Zhangir Azerbayev and Bartosz Pi-
otrowski. 2022. Proof-pile. https://github.com/
zhangir-azerbayev/proof-pile .
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-
hao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-
cient context window extension of llms via positional
skip-wise training.
A Example Appendix
To be continued.
