# 2210.05144.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2210.05144.pdf
# Kích thước file: 646464 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Hỗn hợp các Đầu Chú ý: Lựa chọn Đầu Chú ý cho Mỗi Token
Xiaofeng Zhang1;2, Yikang Shen3;4, Zeyu Huang1;2, Jie Zhou4, Wenge Rong1, Zhang Xiong1
1Phòng thí nghiệm trọng điểm quốc gia về Môi trường phát triển phần mềm,
Khoa Khoa học máy tính và Kỹ thuật, Đại học Beihang, Trung Quốc
2Trường Kỹ sư Trung-Pháp, Đại học Beihang, Trung Quốc
3Mila, Đại học Montreal, Canada
4Wechat AI, Tencent, Trung Quốc
Tóm tắt
Mạng Hỗn hợp Chuyên gia (MoE) đã được đề xuất như một cách hiệu quả để mở rộng quy mô khả năng mô hình và triển khai tính toán có điều kiện. Tuy nhiên, nghiên cứu về các thành phần MoE chủ yếu tập trung vào lớp feedforward trong kiến trúc Transformer. Bài báo này đề xuất Hỗn hợp các Đầu Chú ý (MoA), một kiến trúc mới kết hợp chú ý đa đầu với cơ chế MoE. MoA bao gồm một tập hợp các đầu chú ý mà mỗi đầu có bộ tham số riêng của nó. Với một đầu vào, một bộ định tuyến động lựa chọn một tập con k đầu chú ý cho mỗi token. Lược đồ tính toán có điều kiện này cho phép MoA đạt được hiệu suất mạnh hơn so với lớp chú ý đa đầu tiêu chuẩn. Hơn nữa, MoA được cổng thưa thớt có thể dễ dàng mở rộng số lượng đầu chú ý và số lượng tham số trong khi vẫn duy trì hiệu quả tính toán. Ngoài việc cải thiện hiệu suất, MoA cũng tự động phân biệt tính hữu ích của các đầu, cung cấp một góc nhìn mới để thảo luận về khả năng diễn giải của mô hình. Chúng tôi đã tiến hành thí nghiệm trên một số nhiệm vụ quan trọng, bao gồm Dịch máy và Mô hình hóa ngôn ngữ có mặt nạ. Các thí nghiệm đã cho thấy kết quả hứa hẹn trên một số nhiệm vụ so với các baseline mạnh liên quan đến các mô hình lớn và rất sâu1.

1 Giới thiệu
Trong những năm gần đây, các mô hình lớn đã trở thành một xu hướng phổ biến trong nghiên cứu Xử lý ngôn ngữ tự nhiên, đặc biệt là Transformer quy mô lớn (Vaswani et al., 2017). Khả năng của mô hình đã tăng từ hàng triệu tham số (Devlin et al., 2019; Liu et al., 2019), đến hàng tỷ tham số (Shoeybi et al., 2019; Raffel et al., 2020; Wang et al., 2022), thậm chí đến hàng nghìn tỷ tham số (Du et al., 2021; Fedus et al., 2021). Tuy nhiên, những mô hình quy mô lớn này đòi hỏi tính toán nhiều hơn đáng kể so với các mô hình quy mô nhỏ. Một xu hướng phổ biến là sử dụng tính toán có điều kiện với mô hình được kích hoạt thưa thớt để tìm kiếm hiệu quả tính toán lớn hơn. Do đó, chỉ một phần tham số của mô hình được sử dụng cho một đầu vào cụ thể trong quá trình tính toán tiến, điều này làm giảm tải tính toán.

Trong số những nỗ lực này, Hỗn hợp Chuyên gia (MoE) (Jacobs et al., 1991; Jordan và Jacobs, 1994) là một kỹ thuật thiết yếu. Kể từ lần đầu tiên áp dụng hỗn hợp chuyên gia vào kiến trúc Transformer (Shazeer et al., 2018), các nhà nghiên cứu chủ yếu tập trung vào việc kết hợp lớp Mạng Feed-Forward và Hỗn hợp Chuyên gia. Các công trình gần đây đã thảo luận về cách có được chiến lược định tuyến tốt hơn (Shazeer et al., 2017; Dua et al., 2021; Lewis et al., 2021; Nie et al., 2021) hoặc cách mở rộng quy mô Hỗn hợp Chuyên gia trên các nút khác nhau của GPU (Lepikhin et al., 2021; Fedus et al., 2021). Tuy nhiên, ít nỗ lực đã khám phá khả năng kết hợp MoE với cơ chế Chú ý Đa Đầu.

Bên cạnh đó, nghiên cứu trước đây đã điều tra tính hữu ích của các đầu chú ý khác nhau. Peng et al. (2020) phát hiện rằng sự kết hợp (tái phân bổ) của một tập con các đầu chú ý giúp ích cho nhiệm vụ Dịch thuật vì họ cắt tỉa các đầu chú ý vô dụng. Trong lĩnh vực phân tích cú pháp phụ thuộc, các nhà nghiên cứu đã tiết lộ rằng một số đầu chú ý trong các mô hình ngôn ngữ giống BERT (Devlin et al., 2019; Liu et al., 2019) mô hình hóa các loại phụ thuộc riêng lẻ (Htut et al., 2019) và các chức năng cú pháp (Shen et al., 2022). Voita et al. (2019) cho rằng các đầu chú ý có các chức năng khác nhau có thể được phân loại thành ba loại. Không cần phải đi qua tất cả các đầu chú ý đa dạng cho một token đầu vào nếu chúng ta có thể chọn một số đầu chú ý liên quan có chức năng phù hợp. Do đó, chúng tôi hình thành một cơ chế chú ý lựa chọn các đầu chú ý khác nhau cho mỗi token.

Dựa trên thảo luận trên, chúng tôi đề xuất Hỗn hợp các Đầu Chú ý (MoA) (Phần 4), một cơ chế chú ý lựa chọn các đầu chú ý khác nhau cho các đầu vào khác nhau. Một minh họa đơn giản của ý tưởng này được thể hiện trong Hình 1. MoA bao gồm một tập hợp các đầu chú ý với các tham số khác nhau. Với một đầu vào, một mạng định tuyến động lựa chọn một tập con k đầu chú ý cho mỗi token. Đầu ra là tổng có trọng số của các đầu chú ý được chọn với độ tin cậy được tính bởi mạng định tuyến.

Chúng tôi đã tiến hành thí nghiệm trên hai nhiệm vụ: Dịch máy và Mô hình hóa ngôn ngữ có mặt nạ (Phần 5). Các thí nghiệm cho thấy kết quả hứa hẹn so với một số baseline mạnh. Trong tất cả các nhiệm vụ, hỗn hợp các đầu chú ý đề xuất của chúng tôi vượt trội hơn kiến trúc Transformer gốc (Vaswani et al., 2017). Mô hình của chúng tôi vượt qua nhiều mô hình lớn hoặc đạt được kết quả tương đương chỉ với một nửa chi phí tính toán. Những đóng góp của chúng tôi có thể được tóm tắt thành ba khía cạnh: 1) Chúng tôi đề xuất một cơ chế chú ý mới gọi là Hỗn hợp các Đầu Chú ý, kết hợp ý tưởng của Hỗn hợp Chuyên gia với cơ chế chú ý. 2) MoA có thể cải thiện hiệu suất của mô hình mà không thêm đáng kể tham số và chi phí tính toán. 3) MoA dễ dàng mở rộng quy mô trong khi duy trì với độ phức tạp tính toán được kiềm chế, dẫn đến cải thiện hiệu suất hơn nữa.

2 Công trình liên quan
Hỗn hợp Chuyên gia Hỗn hợp Chuyên gia (MoE) được giới thiệu lần đầu vào những năm 1990 (Jacobs et al., 1991; Jordan và Jacobs, 1994). Shazeer et al. (2017) đã áp dụng phương pháp này vào các kiến trúc học sâu hiện đại (LSTM; Hochreiter và Schmidhuber 1997) và chứng minh hiệu quả của nó trong Mô hình hóa ngôn ngữ và Dịch máy. MoE được sử dụng để thay thế các lớp FFN trong kiến trúc Transformer (Vaswani et al., 2017) bởi thư viện Mesh Tensorflow (Shazeer et al., 2018). Gshard (Lepikhin et al., 2021) là một mô-đun nhẹ giúp mở rộng quy mô Transformer dịch máy neural đa ngôn ngữ với Hỗn hợp Chuyên gia có Cổng Thưa thớt vượt quá 600 tỷ tham số. Trong Switch Transformer (Fedus et al., 2021), các tác giả đã mở rộng quy mô kiến trúc Transformer tích hợp MoE hướng tới các mô hình nghìn tỷ tham số. GLaM (Du et al., 2021) sử dụng kiến trúc chỉ decoder để thực hiện pre-training mô hình ngôn ngữ. Rajbhandari et al. (2022) đề xuất Pyramid-Residual-MoE cho kích thước mô hình nhỏ hơn và suy luận nhanh.

Các chiến lược định tuyến khác nhau (Shazeer et al., 2017; Dua et al., 2021; Lewis et al., 2021; Nie et al., 2021) đã được điều tra để ổn định việc huấn luyện MoE và cân bằng tải chuyên gia. Chi et al. (2022) chỉ ra vấn đề sụp đổ biểu diễn trong các mô hình Hỗn hợp Chuyên gia thưa thớt và giải quyết bằng chiến lược định tuyến hai giai đoạn.

Kiến trúc Dịch máy Với kiến trúc Transformer gốc (Vaswani et al., 2017), Ott et al. (2018) phát hiện rằng huấn luyện với độ chính xác giảm và batch lớn có thể cải thiện hiệu suất dịch thuật. Một số mô hình có hiệu suất tốt hơn trong dịch thuật bằng cách sử dụng quy mô Transformer lớn hơn. Liu et al. (2020a) làm sâu encoder và decoder của Transformer bằng cách khởi tạo mô hình một cách thích hợp. DeepNet (Wang et al., 2022) mở rộng quy mô Transformer lên đến 1.000 lớp bằng cách giới thiệu một hàm chuẩn hóa mới. Tuy nhiên, những phương pháp này đòi hỏi một lượng lớn chi phí tính toán. Một số mô hình thay đổi mô-đun self-attention. Peng et al. (2020) đề xuất mô hình MAE. Việc tái phân bổ các đầu chú ý đạt được hiệu suất tốt hơn trong Dịch thuật, vì mô hình cắt tỉa các đầu chú ý đa đầu vô dụng. Tuy nhiên, phương pháp của họ khó mở rộng quy mô và đạt được cải thiện kết quả hơn nữa vì nó cần sử dụng tất cả các đầu chú ý trong mô hình thay vì kích hoạt chúng một cách thưa thớt. Nó cũng đòi hỏi các bước huấn luyện descent khối tọa độ phức tạp. Wu et al. (2019) đề xuất DynamicConv và LightConv bằng cách thay thế cơ chế self-attention bằng convolution nhẹ.

Chuyên môn hóa các Đầu Chú ý Kể từ khi công bố kiến trúc Transformer (Vaswani et al., 2017), nhiều nhà nghiên cứu đã quan tâm đến việc phân tích cách thức hoạt động của cơ chế chú ý. Voita et al. (2019) đã phân tích có hệ thống các đầu chú ý trong encoder và phân loại chúng thành ba tập con chức năng: vị trí, cú pháp và từ hiếm. Khi xử lý phân tích cú pháp phụ thuộc, các nhà nghiên cứu cũng quan sát hiện tượng tương tự rằng các đầu khác nhau có thể nắm bắt các chức năng cú pháp khác nhau (Htut et al., 2019; Shen et al., 2022).

3 Kiến thức cơ bản
3.1 Hỗn hợp Chuyên gia
MoE (Shazeer et al., 2017) chứa một tập hợp các mạng chuyên gia E1;E2;:::;EN và một mạng định tuyến G. Đầu ra của MoE là tổng có trọng số của đầu ra của mỗi chuyên gia. Mạng định tuyến tính toán xác suất cho mỗi chuyên gia. Chính thức, đầu ra của MoE có thể được viết như:
y=NX
i=1G(x)iEi(x) (1)

Mạng định tuyến G là mạng Định tuyến Top-k Có nhiễu. Trước hàm softmax, họ thêm nhiễu Gaussian vào các logit có cổng, xem Phương trình 3. Sau đó, họ chỉ giữ k giá trị cao nhất, đặt các giá trị cổng còn lại bằng 0, xem Phương trình 2.
G(x) = Softmax(TopK( H(x);k)) (2)
H(x)i=(xWg)i+(0;1) (3)
Softplus((xWnoise)i)

3.2 Chú ý đa đầu
Vaswani et al. (2017) đề xuất kiến trúc encoder-decoder Transformer, chứa mô-đun chú ý đa đầu. Các đầu khác nhau từ mô-đun chú ý đa đầu chú ý đến thông tin từ các không gian con biểu diễn khác nhau, học đầu vào từ các góc độ khác nhau.

Thực hiện chú ý đa đầu với k đầu, Q;K;V được chiếu tuyến tính k lần với các phép chiếu tuyến tính khác nhau, đã học được đến các không gian con. Trên mỗi Q và K được chiếu, điểm chú ý được tính toán qua Phương trình 4. Các giá trị có nguồn gốc từ các đầu khác nhau được chiếu trở lại kích thước chiều mô hình và cộng lại, với Phương trình 5.

Watt
i=SoftmaxQWq
i(KWk
i)T
pdk
(4)
y=kX
i=1
Watt
iVWv
i
Wo
i (5)

trong đó Wq
i;Wk
i;Wv
i2RdmdhandWo
i2Rdhdm,dk là chiều của khóa K.

4 Hỗn hợp các Đầu Chú ý
Trong công trình này, chúng tôi đề xuất một biến thể của chú ý đa đầu cho Transformer gọi là Hỗn hợp các Đầu Chú ý (MoA), được minh họa trong Hình 2. MoA bao gồm hai thành phần chính, mạng định tuyến G và một nhóm N chuyên gia chú ý {E1;:::;EN}.

Tương tự như self-attention đa đầu tiêu chuẩn, đầu vào của MoA bao gồm ba chuỗi, chuỗi truy vấn Q, chuỗi khóa K và chuỗi giá trị V. Chúng tôi ký hiệu qt là vector truy vấn tại bước thời gian t. Đối với mỗi qt, mạng định tuyến G chọn một tập con k chuyên gia G(qt){Ei} dựa trên qt và gán trọng số wi cho mỗi chuyên gia được chọn. Sau đó, những chuyên gia được chọn này lấy qt, K và V làm đầu vào và tính toán đầu ra Ei(qt;K;V). Đầu ra của MoA là tổng có trọng số của đầu ra các chuyên gia được chọn. Chính thức, đầu ra MoA tại bước thời gian t có thể được viết như:
yt=X
i2G(qt)wi;tEi(qt;K;V ) (6)

4.1 Mạng Định tuyến
Tương tự như các phương pháp hỗn hợp chuyên gia trước đây, mạng định tuyến gán các chuyên gia chú ý cho truy vấn đầu vào. Để chọn k chuyên gia cho truy vấn qt, chúng tôi tính toán xác suất định tuyến pi cho mỗi chuyên gia Ei. Xác suất định tuyến được mô hình hóa với một lớp tuyến tính Wg và hàm softmax:
pi;t= Softmax i(qtWg) (7)

Dựa trên xác suất định tuyến p, chúng tôi chọn k chuyên gia chú ý top-k trong số tất cả N chuyên gia chú ý với xác suất lớn nhất. Chính thức, mạng định tuyến được định nghĩa là:
G(Q) = TopK(pi;t;k) (8)

trong đó Wg2RdmN, đại diện cho ma trận định tuyến. Sau đó, chúng tôi chuẩn hóa lại xác suất định tuyến của các chuyên gia được chọn để có được trọng số chuyên gia chuẩn hóa:
wi;t=pi
DetachP
j2G(qt)pj (9)

trong đó Detach() là một hàm dừng lan truyền gradient ngược. Nói cách khác, mẫu số nhận gradient bằng không trong quá trình huấn luyện. Chúng tôi thực nghiệm thấy rằng thủ thuật này giúp mạng định tuyến học xác suất định tuyến tốt hơn.

4.2 Chuyên gia Chú ý
Một chuyên gia chú ý chứa bốn ma trận chiếu khác nhau, Wq, Wk, Wv và Wo. Tính toán chú ý tương tự như chú ý đa đầu. Chúng tôi đầu tiên tính toán trọng số chú ý cho các khóa.
Watt
i;t=SoftmaxqtWq
i(KWk)T
pdh
(10)

trong đó Wq
i2Rdmdh là ma trận chiếu truy vấn, Wk2Rdmdh là ma trận chiếu khóa, dm là kích thước trạng thái ẩn, dh được gọi là chiều đầu. Sau đó chúng tôi tính toán tổng có trọng số của các giá trị:
oi;t=Watt
i;tVWv(11)

trong đó Wv2Rdmdh là ma trận chiếu giá trị. Cuối cùng, đầu ra chú ý được thu được bằng cách chiếu oi;t trở lại không gian trạng thái ẩn:
Ei(qt;K;V ) =oi;tWo
i (12)

trong đó Wo
i2Rdhdm là ma trận chiếu đầu ra.

Trong chú ý đa đầu, các ma trận chiếu Wq, Wk, Wv và Wo đều khác nhau giữa các đầu chú ý. MoA chia sẻ Wk và Wv giữa các chuyên gia chú ý để giảm độ phức tạp tính toán. Các chuyên gia chú ý chỉ được phân biệt bởi Wq
i và Wo
i. Do đó, phép chiếu ma trận đắt đỏ của chuỗi khóa KWk và chuỗi giá trị VWv có thể được tính trước và chia sẻ cho tất cả các chuyên gia chú ý. Mỗi chuyên gia chỉ cần tính toán phép chiếu vector của truy vấn qtWq
i và đầu ra oi;tWo
i. Thiết kế này có thể giảm đáng kể độ phức tạp tính toán và không gian trong khi số lượng chuyên gia lớn.

4.3 Hàm mất mát Huấn luyện
Công trình trước đây (Shazeer et al., 2017) đã quan sát thấy rằng mạng định tuyến có xu hướng hội tụ đến trạng thái luôn tạo ra trọng số lớn cho cùng một vài chuyên gia, điều này cho thấy sự thiếu hữu ích của tất cả các chuyên gia. Theo Shazeer et al. (2017) và Fedus et al. (2021), chúng tôi thêm một hàm mất mát phụ trợ để cân bằng tải của các chuyên gia khác nhau.

Với N chuyên gia và một chuỗi với T truy vấn Q={q1;q2;:::;qT}, hàm mất mát phụ trợ La có thể được tính toán như:
La(Q) =NNX
i=1fiPi (13)

trong đó fi là số token được gán cho chuyên gia thứ i,
fi=TX
t=1i2G(qt) (14)

trong đó  đại diện cho Ký hiệu Kronecker. Pi là tổng xác suất router được phân bổ cho chuyên gia thứ i,
Pi=TX
t=1pi;t (15)

Sau đó chúng được chuẩn hóa với norm 1 theo cột chuyên gia. Về mặt toán học, fi không khả vi trong khi Pi thì có. Do đó, fi lớn hơn sẽ dẫn đến đạo hàm lớn hơn. Điều này phạt Pi làm cho Pi lớn hơn trở nên nhỏ hơn. Hơn nữa, Pi được tính bởi softmax. Do đó Pi nhỏ hơn sẽ trở nên lớn hơn.

Zoph et al. (2022) giới thiệu hàm mất mát router z-loss (Phương trình 16) để phạt các logit lớn vào mạng gating, điều này có thể ổn định việc huấn luyện và cải thiện hiệu suất.
Lz(x) =1
TTX
j=1 
logNX
t=1exi;t!2
(16)

trong đó xi;t là logit pre-softmax được tính bởi router cho chuyên gia thứ i và truy vấn đầu vào qt. Mỗi mô-đun hỗn hợp các đầu chú ý có một hàm mất mát phụ trợ và một hàm mất mát router z-loss. Chúng tôi cộng chúng lại với nhau và thêm với hệ số nhân α và β tương ứng vào tổng hàm mất mát mô hình trong quá trình huấn luyện. Trong suốt công trình này, chúng tôi sử dụng α = 0:01 và β = 0:001 để đảm bảo hiệu quả của hai hàm mất mát được thêm và không làm rối loạn hàm mất mát cross-entropy chính của mô hình.
L=Lmodel +X
8MoA module(La+Lz)(17)

Để xác thực tính hữu ích của những hàm mất mát phụ trợ này, chúng tôi đã tiến hành các thử nghiệm ablation và kết quả được thể hiện trong Phụ lục D.

4.4 Độ phức tạp Tính toán và Số lượng Tham số
Một mặt, với một chuỗi có T token, lượng tính toán được yêu cầu bởi một lớp MoA chọn k chuyên gia top-k là
CMoA =kT2dh+ 2(k+ 1)Tdhdm (18)

trong đó kdh là tổng chiều đầu của các chuyên gia được chọn. Nó đại diện cho lượng thông tin tối đa có thể được thu thập bởi một lớp MoA cho một token. Mặt khác, lượng tính toán được yêu cầu bởi Chú ý Đa Đầu (MHA) tiêu chuẩn là
CMHA =T2dm+ 4Td2
m (19)

trong đó dm là tổng chiều đầu. Nếu kdh≈dm, độ phức tạp tính toán của MoA nhỏ hơn so với MHA. Nói cách khác, MoA có thể thu thập nhiều thông tin hơn cho mỗi token trong khi duy trì mức độ phức tạp tính toán tương tự như MHA.

Về số lượng tham số, với Hỗn hợp các Đầu Chú ý có E chuyên gia chú ý, số lượng tham số trong MoA và MHA là:
MMoA = (2E+ 2)dhdm; M MHA = 4d2
m
(20)

Khi k=E và Edh≈dm, số lượng tham số trong MoA nhỏ hơn MHA. Nói cách khác, MoA có thể thu thập nhiều thông tin hơn cho mỗi token trong khi duy trì số lượng tham số tương tự như MHA. Chi tiết tính toán có trong Phụ lục B.

Thảo luận trên cho thấy rằng, từ góc độ thu thập thông tin, MoA hiệu quả hơn về tính toán và tham số so với MHA tiêu chuẩn. Kết quả thực nghiệm của chúng tôi trong Phần 5 cũng hỗ trợ thực nghiệm cho giả thuyết này. Ngoài ra, độ phức tạp thời gian của MoA được quyết định bởi số lượng đầu chú ý k và chiều đầu chú ý dh, không phải tổng số tham số của mô hình. Người ta có thể tùy ý tăng lượng tham số trong MoA mà không tăng độ phức tạp tính toán của nó.

5 Thí nghiệm
5.1 Dịch máy
Bộ dữ liệu Chúng tôi huấn luyện mô hình Hỗn hợp Chú ý của chúng tôi trên bộ dữ liệu WMT 2014 Tiếng Anh-Đức và Tiếng Anh-Pháp (Bojar et al., 2014). Theo cài đặt thực nghiệm được sử dụng trong Liu et al. (2020b), tất cả các câu được mã hóa sử dụng mã hóa cặp byte (Sennrich et al., 2016). Đối với cả hai nhiệm vụ, chúng tôi sử dụng từ điển kết hợp và chia sẻ tất cả word embedding của encoder và decoder. Đối với Tiếng Anh-Đức, kích thước từ vựng chia sẻ của họ được đặt là 32k. Đối với Tiếng Anh-Pháp, kích thước từ vựng chia sẻ của họ được đặt là 40k.

Chi tiết Huấn luyện và Đánh giá Chúng tôi sử dụng Adam Optimizer (Kingma và Ba, 2015) với tốc độ học 7e-4 và bộ lập lịch tốc độ học căn bậc hai nghịch đảo. Trong quá trình huấn luyện, chúng tôi sử dụng làm mượt nhãn (Szegedy et al., 2016) với giá trị 0.1. Chi tiết huấn luyện khác có thể tìm thấy trong Phụ lục C.

Để đánh giá, chúng tôi tính trung bình các checkpoint của 10 epoch cuối. Chúng tôi liệt kê điểm BLEU (Papineni et al., 2002) được tính với MULTI-BLEU.PERL, và áp dụng xử lý hậu kỳ compound split được giới thiệu trong Vaswani et al. (2017). Chúng tôi sử dụng MACs (Các phép toán Nhân-Tích lũy) để đánh giá độ phức tạp tính toán của các mô hình khác nhau trên đầu vào cố định. Chi tiết tính toán MACs có trong Phụ lục E.

Baseline Chúng tôi so sánh với một số baseline mạnh: Transformer base và big (Vaswani et al., 2017), Transformer big (Ott et al., 2018) với huấn luyện độ chính xác giảm và batch lớn, DynamicConv (Wu et al., 2019) bằng cách thay thế cơ chế self-attention bằng convolution nhẹ, MAE-7 với tái phân bổ các đầu chú ý được đề xuất bởi Peng et al. (2020), Admin (Liu et al., 2020a) làm sâu kiến trúc Transformer.

Đối với mô hình của chúng tôi, ba tham số được sử dụng để phân biệt các biến thể của nó, một là số lượng đầu chú ý được kích hoạt (K) mỗi token, một là tổng số chuyên gia (E), một cái khác là chiều chuyên gia chú ý (D). Ví dụ, mô hình MoA base của chúng tôi được ký hiệu là 8K8E128D, vì nó có 8 chuyên gia chú ý, 128 chiều mỗi chuyên gia, và tất cả 8 chuyên gia được kích hoạt cho mỗi token. Mô hình MoA big của chúng tôi là 16K32E256D vì nó có 32 chuyên gia chú ý và kích hoạt thưa thớt 16 chuyên gia top cho mỗi token.

Kết quả Kết quả trên tập test của bộ dữ liệu WMT14 EnDe và WMT14 EnFr được thể hiện trong Bảng 1. Bảng được chia thành 2 phần, phần trên dành cho các mô hình base và phần dưới cho các mô hình lớn. Trên tất cả các bộ dữ liệu, MoA base vượt trội hơn Transformer base và Admin 6L-6L ít nhất 0.6 BLEU. Trên bộ dữ liệu WMT14 EnFr, MoA base cũng vượt trội hơn Transformer big. Trên bộ dữ liệu WMT14 EnDe, MoA base đạt kết quả tương đương với mô hình Mixture of Attention Experts (MAE-7), đây là hiệu suất state-of-the-art cho các mô hình cấp base. MACs của MAE-7 và mô hình của chúng tôi có thể so sánh được trong cài đặt 8 đầu chú ý. Trong khi cả hai mô hình đều tận dụng ý tưởng về trọng số các đầu chú ý, MoA dễ triển khai hơn và không đòi hỏi các bước huấn luyện descent tọa độ khối phức tạp. So với self-attention đa đầu tiêu chuẩn, cơ chế định tuyến chú ý nhiều hơn đến các đầu chú ý có thông tin hơn cho mỗi token, do đó cho phép mô hình MoA base đạt được hiệu quả tính toán và tham số tốt hơn.

Trong cài đặt quy mô lớn, MoA big liên tục vượt trội hơn các mô hình transformer big tiêu chuẩn, mặc dù yêu cầu tính toán ít hơn đáng kể. So với các mô hình có nhiều tham số hơn, MoA vẫn rất cạnh tranh. Chỉ có Admin 60L-12L vượt trội hơn MoA big trên cả hai bộ dữ liệu. Tuy nhiên, mô hình có nhiều tham số hơn và yêu cầu khoảng hai lần MACs. MACs của MoA big là 1220M, đây là lượng thấp nhất trong số các mô hình quy mô lớn. Kết quả này cho thấy rằng phương pháp đề xuất của chúng tôi có thể dễ dàng mở rộng quy mô đến một lượng lớn tham số và đạt được kết quả tốt mà không tăng đáng kể gánh nặng cho hệ thống tính toán.

5.2 Mô hình hóa ngôn ngữ có mặt nạ
Mô hình hóa ngôn ngữ có mặt nạ là mục tiêu huấn luyện tiêu chuẩn cho nhiều Mô hình ngôn ngữ được Huấn luyện trước (PLM), bao gồm BERT (Devlin et al., 2019) và RoBERTa (Liu et al., 2019). Nhiệm vụ thay thế một mẫu ngẫu nhiên các token trong chuỗi đầu vào bằng token đặc biệt [MASK]. Mục tiêu huấn luyện là hàm mất mát cross-entropy về việc dự đoán các token bị che. Để mô phỏng tốt hơn quy trình huấn luyện PLM, chúng tôi áp dụng cài đặt được giới thiệu trong RoBERTa (Liu et al., 2019) để tiến hành thí nghiệm mô hình hóa ngôn ngữ có mặt nạ.

Bộ dữ liệu Chúng tôi tiến hành mô hình hóa ngôn ngữ có mặt nạ trên bộ dữ liệu wikitext-103 (Merity et al., 2016). Corpus bao gồm hơn 100 triệu token được thu thập từ các bài viết Good và Featured đã được xác minh trên English Wikipedia. Theo cài đặt trong Merity et al. (2016), tập training/validation/test có 103M/218K/246K từ. Corpus được token hóa với từ vựng 50K subword được sử dụng trong RoBERTa và ban đầu được giới thiệu trong GPT (Radford et al., 2019).

Cài đặt Sau đó chúng tôi huấn luyện mô hình với chiến lược che động và định dạng đầu vào câu đầy đủ. Để tránh overfitting trên corpus huấn luyện, chúng tôi áp dụng mô hình RoBERTa kích thước trung bình làm mô hình base, với word embedding 512-dim, mạng feed-forward 2048-dim, 8 đầu và 8 lớp. Chi tiết huấn luyện có thể tìm thấy trong Phụ lục C. Perplexity được sử dụng làm metric đánh giá.

Kết quả Bảng 2 cho thấy perplexity trên dữ liệu test WikiText-103. Trong khi sử dụng lượng tham số tương tự, MoA vượt trội hơn mô hình transformer tiêu chuẩn 0.13 perplexity. Hơn nữa, hiệu suất đồng thời cải thiện với sự tăng của số lượng chuyên gia E và kích thước đầu D, trong khi số lượng đầu được chọn K và độ phức tạp tính toán vẫn giữ nguyên. Quan sát cho thấy rằng mô hình của chúng tôi có khả năng tăng hiệu suất của mô hình trong khi duy trì độ phức tạp tính toán.

5.3 Phân tích Mô hình
Ảnh hưởng tham số MoA Chúng tôi nghiên cứu ảnh hưởng của ba tham số, K, E và D, trên Bộ dữ liệu WMT14 En-De. Kết quả được thể hiện trong Bảng 3. Đối với chiều chuyên gia D, chúng tôi kiểm soát K= 8 và E= 32, thay đổi chiều chuyên gia D với 64, 128 và 256. Khi kích thước chiều chuyên gia D tăng (hàng C, D, E trong Bảng 3), PPL trên tập validation và điểm BLEU trên tập test đều cải thiện. Cải thiện này là do sự tăng tham số. Với kích thước chiều chuyên gia lớn hơn, mỗi chuyên gia có nhiều tham số hơn, và chi phí tính toán tăng. Chúng tôi tin rằng sự tăng chi phí tính toán là có thể chấp nhận được. Như trong Bảng 1, mô hình Transformer big có MACs là 2090M, đạt BLEU 28.4. Tuy nhiên, bằng cách tăng kích thước ẩn của chuyên gia, chúng tôi có thể đạt BLEU 28.8 với MACs ở 841M (<<2090M).

Đối với số lượng chuyên gia chú ý E, chúng tôi kiểm soát K= 8 và D= 256, chọn ba giá trị khác nhau của E, 8, 16 và 32. Khi thêm số lượng chuyên gia, PPL trên tập valid giảm xuống, chỉ ra khả năng mở rộng quy mô liên tục của mô hình chúng tôi. Điểm BLEU trên tập test không thay đổi với PPL, và điều này có thể là do mục tiêu huấn luyện không được liên kết trực tiếp với tính toán điểm BLEU. Tuy nhiên, chúng tôi vẫn quan sát thấy rằng 32 chuyên gia có thể đạt BLEU tốt hơn so với hai cài đặt khác. Vì số lượng đầu chú ý được chọn K vẫn không thay đổi, MACs cho ba cài đặt này là giống nhau. Do đó, MoA cho phép chúng tôi cải thiện khả năng mô hình bằng cách thêm nhiều tham số hơn mà không thay đổi độ phức tạp tính toán.

Đối với số lượng đầu chú ý được chọn K, chúng tôi thử nghiệm ba số lượng đầu chú ý được chọn K, 4, 8 và 16, cố định E= 32 và D= 256. Với sự tăng số lượng đầu chú ý được chọn, chúng tôi quan sát thấy rằng PPL trên tập valid giảm và điểm BLEU trên tập test tăng lên. Vì số lượng chuyên gia chú ý vẫn giữ nguyên, tổng tham số của mô hình vẫn ở 200M. Kết quả này cho thấy sự đánh đổi giữa hiệu quả tính toán và hiệu suất. Mô hình cần nhiều tính toán hơn để có hiệu suất tốt hơn khi MACs thay đổi từ 654M đến 1220M.

Tải chuyên gia MoA Cân bằng tải là một vấn đề lâu dài của các mô hình MoE (Fedus et al., 2021). Hình 3 cho thấy tải chuyên gia của lớp thứ tư của mô hình MoA big. Nó vẽ phần trăm của mỗi chuyên gia được sử dụng trong tập development của WMT14 EN-DE. Đối với mỗi token đầu vào và MoA big, 16 đầu chú ý được chọn trong số 32 chuyên gia chú ý. Hình này cho thấy tải tương đối dựa trên, trong đó chuyên gia được sử dụng nhiều nhất được chọn bởi 5% token và chuyên gia được sử dụng ít nhất được chọn bởi 1%, và tải của hầu hết chuyên gia nằm giữa 2-4%. Quan sát này cho thấy rằng các token đầu vào được phân bổ đều giữa các chuyên gia chú ý. Các chuyên gia được gán cho các vai trò khác nhau với các nhóm token đầu vào khác nhau đáng kể. Tải cho mỗi chuyên gia của các lớp khác nhau được thể hiện trong Phụ lục A.

Khả năng diễn giải MoA: Chuyên môn hóa các Chuyên gia Chúng tôi nghiên cứu trong phần này liệu các chuyên gia khác nhau có sở hữu "chuyên môn" khác nhau không. Chúng tôi cố gắng tìm các token có khả năng cao nhất để đồng xuất hiện với mỗi chuyên gia. Chúng tôi tính toán thông tin tương hỗ pointwise (PMI; Church và Hanks 1990) giữa token và chuyên gia:
PMI(token i;chuyên gia j) =p(token i;chuyên gia j)
p(token i)p(chuyên gia j).

Đối với mỗi chuyên gia, PMI càng lớn, token càng liên quan đến chuyên gia này. Bảng 4 liệt kê các token chỉ thị nhất của mỗi chuyên gia cho lớp encoder đầu tiên của 16K32E512D. Nhiều chuyên gia được liên kết với danh từ trong cùng chủ đề, ví dụ: Vị trí, Tên, Công nghệ, v.v. Chúng tôi cũng phát hiện rằng một số chuyên gia khác được liên kết với tính từ và trạng từ. Ví dụ, Chuyên gia 29 liên quan đến trạng từ, và Chuyên gia 24 được kết nối với phản ứng của con người, nơi một số token là tính từ. Chúng tôi cũng nghiên cứu mối quan hệ giữa chuyên gia và token đầu vào cho các lớp khác của encoder, nhưng khó tìm thấy các mô hình rõ ràng trong các lớp khác.

6 Kết luận
Công trình này giới thiệu Hỗn hợp các Đầu Chú ý (MoA). MoA chứa một tập hợp các chuyên gia chú ý và một mạng định tuyến. Với một đầu vào, MoA gán một xác suất cho mỗi chuyên gia bởi mạng định tuyến và chọn các chuyên gia top-K. Đầu ra của MoA là tổng có trọng số của các chuyên gia chú ý được chọn. Cơ chế trọng số cho phép các token khác nhau tập trung vào các chuyên gia khác nhau, do đó cải thiện hiệu quả tham số và tính toán của mô hình. Kết quả thực nghiệm cho thấy rằng một mô hình MoA quy mô base có thể đạt được hiệu suất tương đương hoặc tốt hơn so với mô hình transformer big. Hơn nữa, MoA có thể cải thiện hiệu suất của nó bằng cách thêm nhiều chuyên gia chú ý hơn trong khi duy trì độ phức tạp tính toán tương đối nhỏ. Bằng cách này, MoA có thể đạt được hiệu suất tương đương với các mô hình sâu hơn và đắt đỏ hơn về mặt tính toán. Phân tích khả năng diễn giải cho thấy rằng các chuyên gia chú ý khác nhau có xu hướng chuyên môn hóa trong một loại token đầu vào cụ thể.

Hạn chế
Trong công trình này, chúng tôi mở rộng quy mô MoA lên tối đa 64 chuyên gia. Tuy nhiên, liên quan đến các công trình kết hợp hỗn hợp chuyên gia với lớp FFN, họ có thể mở rộng số lượng chuyên gia lên hàng nghìn. Trong tương lai, chúng tôi sẽ khám phá giới hạn của khả năng mở rộng quy mô của MoA.

Triển khai MoA của chúng tôi không tối ưu. Mã của chúng tôi không thể khám phá đầy đủ khả năng tính toán song song của GPU. Triển khai hiện tại của chúng tôi dành thêm thời gian cho các thao tác sao chép bộ nhớ. Mặc dù độ phức tạp tính toán (MACs) của MoA tương đối thấp so với các baseline khác, thời gian chạy của triển khai chúng tôi không tối ưu. Trong tương lai, nếu chúng tôi có thể tối ưu hóa triển khai ở cấp độ cuda kernel để loại bỏ các thao tác sao chép bộ nhớ, chúng tôi kỳ vọng ít nhất một nửa thời gian wall-clock. Điều này sẽ làm cho một khối MoA nhanh như một khối chú ý tiêu chuẩn.

Tương tự như kiến trúc Transformer, MoA cần tìm kiếm siêu tham số cẩn thận để đạt kết quả thỏa mãn.

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Phòng thí nghiệm trọng điểm quốc gia về Môi trường phát triển phần mềm của Trung Quốc theo Tài trợ SKLSDE-2021ZX-16.

Tài liệu tham khảo
Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, và Ales Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. Trong Proceedings of the Ninth Workshop on Statistical Machine Translation, trang 12–58. The Association for Computer Linguistics.

Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, và Furu Wei. 2022. On the representation collapse of sparse mixture of experts. CoRR, abs/2204.09179.

Kenneth Ward Church và Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguistics, 16(1):22–29.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 4171–4186. Association for Computational Linguistics.

Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, và Claire Cui. 2021. Glam: Efficient scaling of language models with mixture-of-experts. CoRR, abs/2112.06905.

Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, và Angela Fan. 2021. Tricks for training sparse translation models. CoRR, abs/2110.08246.

William Fedus, Barret Zoph, và Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961.

Sepp Hochreiter và Jürgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735–1780.

Phu Mon Htut, Jason Phang, Shikha Bordia, và Samuel R. Bowman. 2019. Do attention heads in BERT track syntactic dependencies? CoRR, abs/1911.12246.

Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, và Geoffrey E. Hinton. 1991. Adaptive mixtures of local experts. Neural Comput., 3(1):79–87.

Michael I. Jordan và Robert A. Jacobs. 1994. Hierarchical mixtures of experts and the EM algorithm. Neural Comput., 6(2):181–214.

Diederik P. Kingma và Jimmy Ba. 2015. Adam: A method for stochastic optimization. Trong 3rd International Conference on Learning Representations.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. 2021. Gshard: Scaling giant models with conditional computation and automatic sharding. Trong 9th International Conference on Learning Representations. OpenReview.net.

Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, và Luke Zettlemoyer. 2021. BASE layers: Simplifying training of large, sparse models. Trong Proceedings of the 38th International Conference on Machine Learning, tập 139 của Proceedings of Machine Learning Research, trang 6265–6274. PMLR.

Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, và Jiawei Han. 2020a. Understanding the difficulty of training transformers. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, trang 5747–5763. Association for Computational Linguistics.

Xiaodong Liu, Kevin Duh, Liyuan Liu, và Jianfeng Gao. 2020b. Very deep transformers for neural machine translation. CoRR, abs/2008.07772.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Pointer sentinel mixture models.

Xiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao Yang, Zhi Yang, và Bin Cui. 2021. Dense-to-sparse gate for mixture-of-experts. CoRR, abs/2112.14397.

Myle Ott, Sergey Edunov, David Grangier, và Michael Auli. 2018. Scaling neural machine translation. Trong Proceedings of the Third Conference on Machine Translation: Research Papers, trang 1–9, Brussels, Belgium. Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, trang 311–318. ACL.

Hao Peng, Roy Schwartz, Dianqi Li, và Noah A. Smith. 2020. A mixture of h - 1 heads is better than h heads. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 6566–6577. Association for Computational Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.

Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, và Yuxiong He. 2022. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale. CoRR, abs/2201.05596.

Rico Sennrich, Barry Haddow, và Alexandra Birch. 2016. Neural machine translation of rare words with subword units. Trong Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 1715–1725, Berlin, Germany. Association for Computational Linguistics.

Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, và Blake A. Hechtman. 2018. Mesh-tensorflow: Deep learning for supercomputers. Trong Advances in Neural Information Processing Systems 31, trang 10435–10444.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, và Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. Trong 5th International Conference on Learning Representations. OpenReview.net.

Yikang Shen, Shawn Tan, Alessandro Sordoni, Peng Li, Jie Zhou, và Aaron C. Courville. 2022. Unsupervised dependency graph network. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 4767–4784. Association for Computational Linguistics.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, và Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. CoRR, abs/1909.08053.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, và Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. Trong 2016 IEEE Conference on Computer Vision and Pattern Recognition, trang 2818–2826. IEEE Computer Society.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in Neural Information Processing Systems 30, trang 5998–6008.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, và Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. Trong Proceedings of the 57th Conference of the Association for Computational Linguistics, trang 5797–5808. Association for Computational Linguistics.

Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, và Furu Wei. 2022. Deepnet: Scaling transformers to 1, 000 layers. CoRR, abs/2203.00555.

Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, và Michael Auli. 2019. Pay less attention with lightweight and dynamic convolutions. Trong 7th International Conference on Learning Representations. OpenReview.net.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, và William Fedus. 2022. Designing effective sparse expert models. CoRR, abs/2202.08906.

--- TRANG 12 ---
A Phần trăm tải chuyên gia

Chúng tôi so sánh phân phối thuộc tính của token trên các chuyên gia khác nhau cho các lớp encoder khác nhau của 16K32E512D. Kết quả được thể hiện trong Hình 4. Phần trăm tải cho mỗi chuyên gia trong các lớp khác nhau tương đối cân bằng.

B Chứng minh Độ phức tạp Tính toán
Với Hỗn hợp các Đầu Chú ý có E chuyên gia chú ý, một lớp MoA có (2E+ 2)dheaddmodel tham số. Một lớp chú ý đa đầu có 4d2model tham số. Để so sánh hai độ phức tạp này, chúng tôi tiến hành phân số của chúng.

2(E+ 1)dheaddmodel
4d2model
=(E+ 1)
2KEdhead
dmodel

Chúng tôi ký hiệu q=Edhead
dmodel, sau đó chúng tôi có

1
2+1
2E
q

Khi Edhead≈dmodel, chúng tôi có q≈1, và
1
2+1
2E

đây là đường cong giống hyperbolic, với giá trị bằng 1 khi E= 1. Do đó, nếu E > 1, chúng tôi có tỷ lệ giữa tham số của MoA và tham số của chú ý đa đầu nhỏ hơn 1. Do đó, lớp MoA chứa ít tham số hơn lớp chú ý đa đầu.

C Chi tiết Huấn luyện
Tất cả các mô hình của chúng tôi được huấn luyện trên 32 GPU V100. Chúng tôi sử dụng Adam Optimizer (Kingma và Ba, 2015) với β1= 0:9, β2= 0:98 và ε= 1e−9. Chúng tôi sử dụng bộ lập lịch tốc độ học căn bậc hai nghịch đảo cho các nhiệm vụ dịch thuật và bộ lập lịch tuyến tính cho nhiệm vụ mô hình ngôn ngữ có mặt nạ. Trong quá trình huấn luyện, chúng tôi sử dụng làm mượt nhãn (Szegedy et al., 2016) với giá trị 0.1. Siêu tham số huấn luyện khác có thể tìm thấy trong Bảng 6.

D Tính hữu ích của các hàm mất mát phụ trợ khác nhau
Chúng tôi áp dụng hai hàm mất mát phụ trợ khác nhau để cân bằng tải của các chuyên gia, một là La được đề xuất bởi Fedus et al. (2021), cái khác là Lz được đề xuất bởi Zoph et al. (2022). Để xác thực tính hữu ích của hai hàm mất mát phụ trợ này, chúng tôi đã tiến hành một số thử nghiệm ablation. Kết quả được thể hiện trong Bảng 7. Với các kết hợp khác nhau của hàm mất mát phụ trợ và các hệ số khác nhau, chúng tôi phát hiện rằng 0.01 La+ 0.001Lz đạt được điểm BLEU tốt nhất trên tập test WMT14 EnDe.

E Tính toán MACs
PTFLOPS khởi chạy một mô hình đã cho trên một tensor ngẫu nhiên (với các hình dạng đầu vào được xác định trước) và ước tính lượng tính toán (các thao tác nhân-cộng) trong quá trình suy luận. Chúng tôi cần xác định các hình dạng của đầu vào khi sử dụng PTFLOPS để tính toán MACs. Đối với các mô hình dịch thuật, chúng tôi đặt độ dài chuỗi encoder và độ dài chuỗi decoder là 10. Chúng tôi đặt kích thước batch là 1. Đối với các mô hình mô hình hóa ngôn ngữ, chúng tôi đặt độ dài chuỗi là 128 và kích thước batch là 1. Với các hình dạng đầu vào được xác định trước, PTFLOPS sẽ tiến hành quá trình forward của mô hình đã cho và phản hồi MACs.
