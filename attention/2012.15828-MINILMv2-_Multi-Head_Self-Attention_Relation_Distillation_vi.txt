# 2012.15828.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2012.15828.pdf
# Kích thước tệp: 537362 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
MINILMv2: Multi-Head Self-Attention Relation Distillation
for Compressing Pretrained Transformers
Wenhui Wang Hangbo Bao Shaohan Huang Li Dong Furu Wei
Microsoft Research
{wenwan,t-habao,shaohanh,lidong1,fuwei}@microsoft.com
Tóm tắt
Chúng tôi tổng quát hóa việc chưng cất self-attention sâu
trong MINILM (Wang et al., 2020) bằng cách chỉ sử
dụng chưng cất quan hệ self-attention cho việc nén
task-agnostic của các Transformer được pretrain.
Cụ thể, chúng tôi định nghĩa quan hệ multi-head self-
attention như là tích vô hướng có tỷ lệ giữa các cặp
vector query, key và value trong mỗi module self-
attention. Sau đó chúng tôi sử dụng kiến thức quan hệ
trên để huấn luyện mô hình học sinh. Bên cạnh tính
đơn giản và nguyên tắc thống nhất, thuận lợi hơn, không
có giới hạn về số lượng attention head của học sinh,
trong khi hầu hết công việc trước đây phải đảm bảo
cùng số lượng head giữa giáo viên và học sinh. Hơn
nữa, các quan hệ self-attention chi tiết có xu hướng
khai thác đầy đủ kiến thức tương tác được học bởi
Transformer. Ngoài ra, chúng tôi kiểm tra kỹ lưỡng
chiến lược lựa chọn layer cho các mô hình giáo viên,
thay vì chỉ dựa vào layer cuối như trong MINILM.
Chúng tôi tiến hành các thí nghiệm mở rộng về nén
cả các mô hình pretrain đơn ngữ và đa ngữ. Kết quả
thí nghiệm chứng minh rằng các mô hình¹ được chưng
cất từ giáo viên kích thước base và large (BERT,
RoBERTa và XLM-R) vượt trội so với state-of-the-art.

1 Giới thiệu
Các Transformer được pretrain (Radford et al., 2018; Devlin et al., 2018; Dong et al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019; Bao et al., 2020; Radford et al., 2019; Raffel et al., 2019; Lewis et al., 2019a) đã rất thành công cho một loạt rộng các tác vụ xử lý ngôn ngữ tự nhiên. Tuy nhiên, những mô hình này thường bao gồm hàng trăm triệu tham số và ngày càng lớn hơn. Điều này mang lại thách thức cho việc fine-tuning và phục vụ trực tuyến trong các ứng dụng thực tế do những hạn chế về tài nguyên tính toán và độ trễ.

Chưng cất kiến thức (KD; Hinton et al. 2015, Romero et al. 2015) đã được sử dụng rộng rãi để nén các Transformer được pretrain, chuyển giao kiến thức của mô hình lớn (giáo viên) sang mô hình nhỏ (học sinh) bằng cách giảm thiểu sự khác biệt giữa các đặc trưng của giáo viên và học sinh. Xác suất mục tiêu mềm (soft labels) và các biểu diễn trung gian thường được sử dụng để thực hiện huấn luyện KD. Trong công trình này, chúng tôi tập trung vào nén task-agnostic của các Transformer được pretrain (Sanh et al., 2019; Tsai et al., 2019; Jiao et al., 2019; Sun et al., 2019b; Wang et al., 2020). Các mô hình học sinh được chưng cất từ các Transformer lớn được pretrain sử dụng các corpus văn bản quy mô lớn. Mô hình task-agnostic được chưng cất có thể được fine-tune trực tiếp trên các tác vụ downstream, và có thể được sử dụng để khởi tạo chưng cất task-specific.

DistilBERT (Sanh et al., 2019) sử dụng xác suất mục tiêu mềm cho dự đoán masked language modeling và đầu ra embedding để huấn luyện học sinh. Mô hình học sinh được khởi tạo từ giáo viên bằng cách lấy một layer trong hai. TinyBERT (Jiao et al., 2019) sử dụng hidden states và phân phối self-attention (tức là attention maps và weights), và áp dụng hàm đồng nhất để ánh xạ các layer học sinh và giáo viên cho chưng cất theo layer. MobileBERT (Sun et al., 2019b) giới thiệu các mô hình giáo viên và học sinh được thiết kế đặc biệt sử dụng cấu trúc inverted-bottleneck và bottleneck để giữ số layer và hidden size giống nhau, chuyển giao các hidden states và phân phối self-attention theo layer. MINILM (Wang et al., 2020) đề xuất chưng cất self-attention sâu, sử dụng phân phối self-attention và quan hệ value để giúp học sinh bắt chước sâu các module self-attention của giáo viên. MINILM cho thấy rằng chuyển giao kiến thức của layer cuối của giáo viên đạt hiệu suất tốt hơn chưng cất theo layer.

--- TRANG 2 ---
lation. Tóm lại, hầu hết công việc trước đây dựa vào phân phối self-attention để thực hiện huấn luyện KD, dẫn đến hạn chế là số lượng attention head của mô hình học sinh phải giống với giáo viên của nó.

Trong công trình này, chúng tôi tổng quát hóa và đơn giản hóa chưng cất self-attention sâu của MINILM (Wang et al., 2020) bằng cách sử dụng chưng cất quan hệ self-attention. Chúng tôi giới thiệu quan hệ multi-head self-attention được tính toán bằng tích vô hướng có tỷ lệ của các cặp queries, keys và values, điều này hướng dẫn việc huấn luyện học sinh. Lấy vector query làm ví dụ, để có được queries của nhiều relation head, trước tiên chúng tôi nối các query vector của các attention head khác nhau, sau đó chia vector đã nối theo số lượng relation head mong muốn. Sau đó, đối với các mô hình giáo viên và học sinh với số lượng attention head khác nhau, chúng tôi có thể căn chỉnh queries của chúng với cùng số lượng relation head để chưng cất. Hơn nữa, sử dụng số lượng relation head lớn hơn mang lại kiến thức self-attention chi tiết hơn, giúp học sinh đạt được sự bắt chước sâu hơn module self-attention của giáo viên.

Ngoài ra, đối với giáo viên kích thước lớn (24 layer, 1024 hidden size), các thí nghiệm mở rộng cho thấy rằng chuyển giao một layer giữa phía trên có xu hướng hoạt động tốt hơn việc sử dụng layer cuối như trong MINILM.

Kết quả thí nghiệm cho thấy rằng các mô hình đơn ngữ được chưng cất từ BERT và RoBERTa, và các mô hình đa ngữ được chưng cất từ XLM-R vượt trội so với các mô hình state-of-the-art ở các kích thước tham số khác nhau. Mô hình 6×768 (6 layer, 768 hidden size) được chưng cất từ BERT LARGE nhanh hơn 2.0 lần, đồng thời, hoạt động tốt hơn BERT BASE. Mô hình kích thước base được chưng cất từ RoBERTa LARGE vượt trội so với RoBERTa BASE sử dụng ít ví dụ huấn luyện hơn nhiều.

Tóm lại, những đóng góp của chúng tôi bao gồm:
• Chúng tôi tổng quát hóa và đơn giản hóa chưng cất self-attention sâu trong MINILM bằng cách giới thiệu chưng cất quan hệ multi-head self-attention, mang lại kiến thức self-attention chi tiết hơn và cho phép linh hoạt hơn về số lượng attention head của học sinh.

• Chúng tôi tiến hành các thí nghiệm chưng cất mở rộng trên các giáo viên kích thước lớn khác nhau và phát hiện rằng sử dụng kiến thức của layer giữa phía trên của giáo viên đạt hiệu suất tốt hơn.

• Kết quả thí nghiệm chứng minh tính hiệu quả của phương pháp chúng tôi cho các giáo viên đơn ngữ và đa ngữ khác nhau ở kích thước base và large.

2 Công trình liên quan

2.1 Mạng backbone: Transformer
Multi-layer Transformer (Vaswani et al., 2017) đã được áp dụng rộng rãi trong các mô hình pretrain. Mỗi layer Transformer bao gồm một sub-layer self-attention và một sub-layer feed-forward fully connected theo vị trí.

Self-Attention Transformer dựa vào multi-head self-attention để nắm bắt các phụ thuộc giữa các từ. Cho đầu ra của layer Transformer trước đó H^(l-1) ∈ R^(|x|×d_h), đầu ra của một attention head O^(l,a); a ∈ [1, A_h] được tính toán qua:

Q^(l,a) = H^(l-1)W_Q^(l,a)  (1)
K^(l,a) = H^(l-1)W_K^(l,a)  (2)
V^(l,a) = H^(l-1)W_V^(l,a)  (3)
O^(l,a) = softmax(Q^(l,a)K^(l,a)T/√d_k)V^(l,a)  (4)

Đầu ra layer trước được chiếu tuyến tính thành queries, keys và values sử dụng ma trận tham số W_Q^(l,a), W_K^(l,a), W_V^(l,a) ∈ R^(d_h×d_k), tương ứng. Phân phối self-attention được tính toán qua tích vô hướng có tỷ lệ của queries và keys. Các trọng số này được gán cho các vector value tương ứng để có được đầu ra attention. |x| đại diện cho độ dài của chuỗi đầu vào. A_h và d_h chỉ số lượng attention head và hidden size. d_k là kích thước attention head. d_k × A_h thường bằng d_h.

2.2 Mô hình ngôn ngữ được pretrain
Pre-training đã dẫn đến những cải thiện mạnh mẽ trên nhiều tác vụ xử lý ngôn ngữ tự nhiên. Các mô hình ngôn ngữ được pretrain được học trên lượng lớn dữ liệu văn bản, sau đó được fine-tune để thích ứng với các tác vụ cụ thể. BERT (Devlin et al., 2018) đề xuất pretrain một Transformer hai chiều sâu sử dụng mục tiêu masked language modeling (MLM). UNILM (Dong et al., 2019) được pretrain chung trên ba loại mục tiêu language modeling để thích ứng với cả tác vụ hiểu và sinh. RoBERTa (Liu et al., 2019) đạt hiệu suất mạnh bằng cách huấn luyện lâu hơn sử dụng batch size lớn và nhiều dữ liệu văn bản hơn. MASS (Song et al., 2019), T5 (Raffel et al., 2019) và BART (Lewis et al.,

--- TRANG 3 ---
[Trang này chứa Figure 1 - Biểu đồ tổng quan về chưng cất quan hệ multi-head self-attention]

Figure 1: Tổng quan về chưng cất quan hệ multi-head self-attention. Chúng tôi giới thiệu quan hệ multi-head self-attention được tính toán bằng tích vô hướng có tỷ lệ của các cặp queries, keys và values để hướng dẫn việc huấn luyện học sinh. Để có được vector (queries, keys và values) của nhiều relation head, trước tiên chúng tôi nối các vector self-attention của các attention head khác nhau sau đó chia chúng theo số lượng relation head mong muốn. Chúng tôi chọn chuyển giao quan hệ self-attention Q-Q, K-K và V-V để đạt sự cân bằng giữa hiệu suất và tốc độ huấn luyện. Đối với giáo viên kích thước lớn, chúng tôi chuyển giao kiến thức self-attention của một layer giữa phía trên của giáo viên. Đối với giáo viên kích thước base, sử dụng layer cuối đạt hiệu suất tốt hơn.

2019a) sử dụng cấu trúc encoder-decoder tiêu chuẩn và pretrain decoder theo cách auto-regressive. Bên cạnh các mô hình pretrain đơn ngữ, các mô hình pretrain đa ngữ (Devlin et al., 2018; Lample and Conneau, 2019; Chi et al., 2019; Conneau et al., 2019; Chi et al., 2020) cũng thúc đẩy state-of-the-art về hiểu và sinh đa ngôn ngữ.

2.3 Chưng cất kiến thức
Chưng cất kiến thức đã được chứng minh là một cách đầy hứa hẹn để nén các mô hình lớn trong khi duy trì độ chính xác. Kiến thức của một hoặc một tập hợp các mô hình lớn được sử dụng để hướng dẫn việc huấn luyện các mô hình nhỏ. Hinton et al. (2015) đề xuất sử dụng xác suất mục tiêu mềm để huấn luyện các mô hình học sinh. Kiến thức chi tiết hơn như hidden states (Romero et al., 2015) và phân phối attention (Zagoruyko and Komodakis, 2017; Hu et al., 2018) được giới thiệu để cải thiện mô hình học sinh.

Trong công trình này, chúng tôi tập trung vào chưng cất kiến thức task-agnostic của các Transformer được pretrain. Mô hình task-agnostic được chưng cất có thể được fine-tune để thích ứng với các tác vụ downstream. Nó cũng có thể được sử dụng để khởi tạo chưng cất task-specific (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), sử dụng mô hình giáo viên được fine-tune để hướng dẫn việc huấn luyện học sinh trên các tác vụ cụ thể. Kiến thức được sử dụng cho chưng cất và hàm ánh xạ layer là hai điểm quan trọng cho chưng cất task-agnostic của các Transformer được pretrain. Hầu hết công việc trước đây sử dụng xác suất mục tiêu mềm, hidden states, phân phối self-attention và value-relation để huấn luyện mô hình học sinh. Đối với hàm ánh xạ layer, TinyBERT (Jiao et al., 2019) sử dụng chiến lược đồng nhất để ánh xạ các layer giáo viên và học sinh. MobileBERT (Sun et al., 2019b) giả định học sinh có cùng số layer với giáo viên để thực hiện chưng cất theo layer. MINILM (Wang et al., 2020) chuyển giao kiến thức self-attention của layer cuối của giáo viên sang layer Transformer cuối của học sinh. Khác với công việc trước đây, phương pháp của chúng tôi sử dụng quan hệ multi-head self-attention để loại bỏ hạn chế về số lượng attention head của học sinh. Hơn nữa, chúng tôi cho thấy rằng chuyển giao kiến thức self-attention của một layer giữa phía trên của mô hình giáo viên kích thước lớn hiệu quả hơn.

3 Chưng cất quan hệ Multi-Head Self-Attention

Theo MINILM (Wang et al., 2020), ý tưởng chính của phương pháp chúng tôi là bắt chước sâu module self-attention của giáo viên, vẽ ra các phụ thuộc giữa các từ và là thành phần quan trọng của Transformer. MINILM sử dụng phân phối self-attention của giáo viên để huấn luyện mô hình học sinh. Điều này mang lại

--- TRANG 4 ---
[Bảng 1 chứa kết quả của các mô hình học sinh được chưng cất từ giáo viên kích thước base và large]

Table 1: Kết quả của các học sinh được chưng cất từ giáo viên kích thước base và large trên tập phát triển của GLUE và SQuAD 2.0. Chúng tôi báo cáo F1 cho SQuAD 2.0, hệ số tương quan Matthews cho CoLA, và độ chính xác cho các dataset khác. Kết quả GLUE của DistilBERT được lấy từ Sanh et al. (2019). Các kết quả còn lại của DistilBERT, TinyBERT², BERT SMALL, Truncated BERT BASE và MINILM được lấy từ Wang et al. (2020). BERT SMALL (Turc et al., 2019) được huấn luyện sử dụng mục tiêu MLM, không sử dụng KD. Chúng tôi cũng báo cáo kết quả của truncated BERT BASE và truncated RoBERTa BASE, loại bỏ 6 layer trên cùng của mô hình base. Việc bỏ layer trên cùng đã được chứng minh là một baseline mạnh (Sajjad et al., 2020). Kết quả fine-tuning là trung bình của 4 lần chạy.

hạn chế về số lượng attention head của học sinh, được yêu cầu phải giống với giáo viên. Để giới thiệu kiến thức self-attention chi tiết hơn và tránh sử dụng phân phối self-attention của giáo viên, chúng tôi tổng quát hóa chưng cất self-attention sâu trong MINILM và giới thiệu quan hệ multi-head self-attention của các cặp queries, keys và values để huấn luyện học sinh. Bên cạnh đó, chúng tôi tiến hành các thí nghiệm mở rộng và phát hiện rằng lựa chọn layer của mô hình giáo viên rất quan trọng cho việc chưng cất các mô hình kích thước lớn. Figure 1 đưa ra tổng quan về phương pháp của chúng tôi.

3.1 Quan hệ Multi-Head Self-Attention

Quan hệ multi-head self-attention được có được bằng tích vô hướng có tỷ lệ của các cặp³ queries, keys và values của nhiều relation head. Lấy query vector làm ví dụ, để có được queries của nhiều relation head, trước tiên chúng tôi nối queries của các attention head khác nhau sau đó chia vector đã nối dựa trên số lượng relation head mong muốn. Thao tác tương tự cũng được thực hiện trên keys và values. Đối với các mô hình giáo viên và học sinh sử dụng số lượng attention head khác nhau, chúng tôi chuyển đổi queries, keys và values của chúng từ số lượng attention head khác nhau thành

²Ngoài chưng cất task-agnostic, TinyBERT sử dụng chưng cất task-specific và data augmentation để cải thiện thêm mô hình. Chúng tôi báo cáo kết quả fine-tuning của mô hình task-agnostic công khai của họ.

³Có chín loại quan hệ self-attention, như quan hệ query-query, key-key, key-value và query-value.

vector của cùng số lượng relation head để thực hiện huấn luyện KD. Phương pháp của chúng tôi loại bỏ hạn chế về số lượng attention head của các mô hình học sinh. Hơn nữa, sử dụng nhiều relation head hơn trong việc tính toán quan hệ self-attention mang lại kiến thức self-attention chi tiết hơn và cải thiện hiệu suất của mô hình học sinh.

Chúng tôi sử dụng A₁, A₂, A₃ để biểu thị queries, keys và values của nhiều relation head. KL-divergence giữa quan hệ multi-head self-attention của giáo viên và học sinh được sử dụng làm mục tiêu huấn luyện:

L = Σᵢ₌₁³ Σⱼ₌₁³ λᵢⱼLᵢⱼ    (5)

Lᵢⱼ = 1/(Aᵣ|x|) Σₐ₌₁^Aᵣ Σₜ₌₁^|x| D_KL(R^T_{ij,l,a,t}||R^S_{ij,m,a,t})    (6)

R^T_{ij,l,a} = softmax(A^T_{i,l,a}A^T_{j,l,a}ᵀ/√dᵣ)    (7)

R^S_{ij,m,a} = softmax(A^S_{i,m,a}A^S_{j,m,a}ᵀ/√d'ᵣ)    (8)

trong đó A^T_{i,l,a} ∈ R^{|x|×dᵣ} và A^S_{i,m,a} ∈ R^{|x|×d'ᵣ} (i ∈ [1,3]) là queries, keys và values của một relation head của layer l thứ của giáo viên và layer m thứ của học sinh. dᵣ và d'ᵣ là kích thước relation head của mô hình giáo viên và học sinh. R^T_{ij,l} ∈ R^{Aᵣ×|x|×|x|} là quan hệ self-attention của A^T_{i,l} và A^T_{j,l} của mô hình giáo viên.

--- TRANG 5 ---
R^T_{ij,l,a} ∈ R^{|x|×|x|} là quan hệ self-attention của một relation head của giáo viên. R^S_{ij,m} ∈ R^{Ar×|x|×|x|} là quan hệ self-attention của mô hình học sinh. Ví dụ, R^T_{11,l} đại diện cho quan hệ Q-Q attention của giáo viên trong Figure 1. Ar là số lượng relation head. Nếu số lượng relation head và attention head giống nhau, quan hệ Q-K tương đương với attention weight trong module self-attention.

λ_{ij} ∈ {0,1} là trọng số được gán cho mỗi loss quan hệ self-attention. Chúng tôi chuyển giao quan hệ self-attention query-query, key-key và value-value để cân bằng hiệu suất và chi phí huấn luyện.

3.2 Lựa chọn layer của mô hình giáo viên

Bên cạnh kiến thức được sử dụng cho chưng cất, hàm ánh xạ giữa các layer giáo viên và học sinh là một yếu tố quan trọng khác. Như trong MINILM, chúng tôi chỉ chuyển giao kiến thức self-attention của một trong các layer của giáo viên sang layer cuối của học sinh. Chỉ chưng cất một layer của mô hình giáo viên là nhanh và hiệu quả. Khác với công việc trước đây thường tiến hành thí nghiệm trên giáo viên kích thước base, chúng tôi thí nghiệm với các giáo viên kích thước lớn khác nhau và phát hiện rằng chuyển giao kiến thức self-attention của một layer giữa phía trên hoạt động tốt hơn việc sử dụng các layer khác. Đối với BERT LARGE và BERT LARGE-WWM, chuyển giao layer thứ 21 (bắt đầu từ một) đạt hiệu suất tốt nhất. Đối với RoBERTa LARGE và XLM-R LARGE, sử dụng kiến thức self-attention của layer thứ 19 đạt hiệu suất tốt hơn. Đối với giáo viên kích thước base, chúng tôi cũng phát hiện rằng sử dụng layer cuối của giáo viên hoạt động tốt hơn.

4 Thí nghiệm

Chúng tôi tiến hành các thí nghiệm chưng cất trên các mô hình giáo viên khác nhau bao gồm BERT BASE, BERT LARGE, BERT LARGE-WWM, RoBERTa BASE, RoBERTa LARGE, XLM-R BASE và XLM-R LARGE.

4.1 Thiết lập

Chúng tôi sử dụng phiên bản uncased cho ba mô hình giáo viên BERT. Đối với dữ liệu pre-training, chúng tôi sử dụng English Wikipedia và BookCorpus (Zhu et al., 2015). Chúng tôi huấn luyện các mô hình học sinh sử dụng 256 làm batch size và 6e-4 làm learning rate đỉnh trong 400,000 step. Chúng tôi sử dụng linear warmup trong 4,000 step đầu và linear decay. Chúng tôi sử dụng Adam (Kingma and Ba, 2015) với β₁ = 0.9, β₂ = 0.999. Độ dài chuỗi tối đa được đặt là 512. Tỷ lệ dropout và weight decay là 0.1 và 0.01. Số lượng attention head là 12 cho tất cả mô hình học sinh. Số lượng relation head là 48 và 64 cho mô hình giáo viên kích thước base và large, tương ứng. Các mô hình học sinh được khởi tạo ngẫu nhiên.

Đối với các mô hình được chưng cất từ RoBERTa, chúng tôi sử dụng các dataset pre-training tương tự như trong Liu et al. (2019). Đối với mô hình học sinh 12×768, chúng tôi sử dụng Adam với β₁ = 0.9, β₂ = 0.98. Các siêu tham số còn lại giống như các mô hình được chưng cất từ BERT.

Đối với các mô hình học sinh đa ngữ được chưng cất từ XLM-R, chúng tôi thực hiện huấn luyện sử dụng cùng dataset như trong Conneau et al. (2019) trong 1,000,000 step. Chúng tôi tiến hành các thí nghiệm chưng cất sử dụng 8 GPU V100 với huấn luyện mixed precision.

4.2 Tác vụ downstream

Theo các công trình pre-training trước đây (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) và chưng cất task-agnostic (Sun et al., 2019b; Jiao et al., 2019), chúng tôi đánh giá các mô hình học sinh tiếng Anh trên benchmark GLUE và extractive question answering. Các mô hình đa ngữ được đánh giá trên cross-lingual natural language inference và cross-lingual question answering.

GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) bao gồm hai tác vụ phân loại câu đơn (SST-2 (Socher et al., 2013) và CoLA (Warstadt et al., 2018)), ba tác vụ tương tự và paraphrase (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) và QQP), và bốn tác vụ suy luận (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli

--- TRANG 6 ---
[Bảng 2 và 3 chứa kết quả hiệu suất trên các dataset]

Table 2: Kết quả của các học sinh 6×768 được chưng cất từ BERT trên tập test GLUE và tập dev SQuAD 2.0. Các kết quả báo cáo được fine-tune trực tiếp trên các tác vụ downstream. Chúng tôi báo cáo F1 cho SQuAD 2.0, QQP và MRPC, tương quan Spearman cho STS-B, hệ số tương quan Matthews cho CoLA và độ chính xác cho phần còn lại.

Table 3: Kết quả của các mô hình 12×768 trên tập dev của benchmark GLUE và SQuAD 2.0. Kết quả fine-tuning là trung bình của 4 lần chạy cho mỗi tác vụ. Chúng tôi báo cáo F1 cho SQuAD 2.0, tương quan Pearson cho STS-B, hệ số tương quan Matthews cho CoLA và độ chính xác cho phần còn lại.

et al., 2009) và WNLI (Levesque et al., 2012)).

Extractive Question Answering Tác vụ này nhằm dự đoán một sub-span liên tục của đoạn văn để trả lời câu hỏi. Chúng tôi đánh giá trên SQuAD 2.0 (Rajpurkar et al., 2018), đã được phục vụ như một benchmark question answering chính.

Cross-lingual Natural Language Inference (XNLI) XNLI (Conneau et al., 2018) là một benchmark phân loại đa ngôn ngữ. Nó nhằm xác định mối quan hệ ngữ nghĩa giữa hai câu và cung cấp các instance trong 15 ngôn ngữ.

Cross-lingual Question Answering Chúng tôi sử dụng MLQA (Lewis et al., 2019b) để đánh giá các mô hình đa ngữ. MLQA mở rộng dataset SQuAD tiếng Anh (Rajpurkar et al., 2016) sang bảy ngôn ngữ.

4.3 Kết quả chính

Table 1 trình bày kết quả dev của các mô hình 6×384 và 6×768 được chưng cất từ BERT BASE, BERT LARGE và RoBERTa LARGE trên GLUE và SQuAD 2.0. (1) Các phương pháp trước đây (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Wang et al., 2020) thường chưng cất BERT BASE thành mô hình 6-layer với 768 hidden size. Chúng tôi đầu tiên báo cáo kết quả của cùng thiết lập. Mô hình 6×768 của chúng tôi vượt trội so với DistilBERT, TinyBERT, MINILM và hai baseline BERT trên hầu hết các tác vụ. Hơn nữa, phương pháp của chúng tôi cho phép linh hoạt hơn về số lượng attention head của các mô hình học sinh. (2) Cả mô hình 6×384 và 6×768 được chưng cất từ BERT LARGE đều vượt trội so với các mô hình được chưng cất từ BERT BASE. Mô hình 6×768 được chưng cất từ BERT LARGE nhanh hơn 2.0 lần so với BERT BASE, đồng thời đạt hiệu suất tốt hơn. (3) Các mô hình học sinh được chưng cất từ RoBERTa LARGE đạt được những cải thiện thêm. Giáo viên tốt hơn dẫn đến học sinh tốt hơn. Chưng cất quan hệ multi-head self-attention hiệu quả cho các Transformer được pretrain kích thước lớn khác nhau.

Chúng tôi báo cáo kết quả của các học sinh 6×768 được chưng cất từ BERT BASE và BERT LARGE trên tập test GLUE và tập dev SQuAD 2.0 trong Table 2. Mô hình 6×768 được chưng cất từ BERT BASE giữ lại hơn 99% độ chính xác của giáo viên trong khi sử dụng 50% tham số Transformer. Mô hình 6×768 được chưng cất từ BERT LARGE so sánh thuận lợi với BERT BASE.

Chúng tôi nén RoBERTa LARGE và BERT LARGE thành mô hình học sinh kích thước base. Kết quả dev của benchmark GLUE và SQuAD 2.0 được hiển thị trong Table 3. Các mô hình kích thước base được chưng cất từ giáo viên kích thước lớn vượt trội so với BERT BASE và RoBERTa BASE. Phương pháp của chúng tôi có thể được áp dụng để huấn luyện học sinh ở các kích thước tham số khác nhau. Hơn nữa, học sinh được chưng cất từ RoBERTa LARGE sử dụng batch size huấn luyện nhỏ hơn nhiều (gần 3×2 nhỏ hơn) và ít step huấn luyện hơn so với RoBERTa BASE. Phương pháp của chúng tôi yêu cầu ít ví dụ huấn luyện hơn nhiều.

Hầu hết công việc trước đây tiến hành thí nghiệm sử dụng giáo viên kích thước base. Để so sánh với các phương pháp trước đây trên giáo viên kích thước lớn, chúng tôi tái implement MINILM và nén BERT LARGE-WWM thành mô hình học sinh 12×384. Kết quả dev của SQuAD 2.0, MNLI-m và SST-2 được trình bày trong Table 4. Phương pháp của chúng tôi cũng vượt trội so với MINILM cho giáo viên kích thước lớn. Hơn nữa, chúng tôi báo cáo kết quả của việc chưng cất một layer giữa phía trên thay vì layer cuối cho MINILM. Lựa chọn layer cũng hiệu quả cho MINILM khi chưng cất giáo viên kích thước lớn.

Table 4: So sánh các phương pháp khác nhau sử dụng BERT LARGE-WWM làm giáo viên. Chúng tôi báo cáo kết quả dev của mô hình học sinh 12×384 với 128 embedding size.

Table 5 và Table 6 hiển thị kết quả của các mô hình học sinh được chưng cất từ XLM-R trên XNLI và MLQA. Đối với XNLI, mô hình đơn tốt nhất được chọn trên tập dev chung của tất cả các ngôn ngữ như trong Conneau et al. (2019). Theo Lewis et al. (2019b), chúng tôi áp dụng SQuAD 1.1 làm dữ liệu huấn luyện và đánh giá trên tập phát triển MLQA tiếng Anh để

--- TRANG 7 ---
[Bảng 5 và 6 chứa kết quả đa ngữ]

Table 5: Kết quả phân loại đa ngôn ngữ của các mô hình đa ngữ trên XNLI. Chúng tôi báo cáo độ chính xác trên mỗi trong 15 ngôn ngữ XNLI và độ chính xác trung bình. #L và #H chỉ số lượng layer và hidden size.

Table 6: Kết quả question answering đa ngôn ngữ của các mô hình đa ngữ trên MLQA. Chúng tôi báo cáo điểm F1 và EM (exact match) trên mỗi trong 7 ngôn ngữ MLQA. #L và #H chỉ số lượng layer và hidden size. ᵞ chỉ kết quả fine-tuning của chúng tôi cho XLM-R BASE.

early stopping. Mô hình 6×384 của chúng tôi vượt trội so với mBERT (Devlin et al., 2018) với tốc độ nhanh hơn 5.3 lần. Phương pháp của chúng tôi cũng hoạt động tốt hơn MINILM, điều này tiếp tục xác nhận tính hiệu quả của chưng cất quan hệ multi-head self-attention. Chuyển giao quan hệ multi-head self-attention có thể mang lại kiến thức self-attention chi tiết hơn.

4.4 Nghiên cứu loại bỏ

Hiệu quả của việc sử dụng các quan hệ self-attention khác nhau

Chúng tôi thực hiện nghiên cứu loại bỏ để phân tích đóng góp của các quan hệ self-attention khác nhau. Kết quả dev của ba tác vụ được minh họa trong Table 7. Quan hệ self-attention Q-Q, K-K và V-V đóng góp tích cực vào kết quả cuối cùng. Bên cạnh đó, chúng tôi cũng so sánh Q-Q + K-K + V-V với Q-K + V-V cho rằng queries và keys được sử dụng để tính toán phân phối self-attention trong module self-attention. Kết quả thí nghiệm cho thấy rằng sử dụng Q-Q + K-K + V-V đạt hiệu suất tốt hơn.

Table 7: Nghiên cứu loại bỏ các quan hệ self-attention khác nhau. Chúng tôi báo cáo kết quả của mô hình học sinh 6×384 được chưng cất từ BERT BASE. Số lượng relation head là 12.

Hiệu quả của việc chưng cất các layer giáo viên khác nhau

Figure 2 trình bày kết quả của mô hình 6×384 được chưng cất từ các layer khác nhau của BERT BASE, BERT LARGE và XLM-R LARGE. Đối với BERT BASE, sử dụng layer cuối đạt hiệu suất tốt hơn các layer khác. Đối với BERT LARGE và XLM-R LARGE, chúng tôi phát hiện rằng sử dụng một trong các layer giữa phía trên đạt hiệu suất tốt nhất. Xu hướng tương tự cũng được quan sát cho BERT LARGE-WWM và RoBERTa LARGE.

Hiệu quả của số lượng relation head khác nhau

Table 8 hiển thị kết quả của mô hình 6×384 được chưng cất từ BERT BASE và RoBERTa BASE sử dụng số lượng relation head khác nhau. Sử dụng số lượng relation head lớn hơn đạt hiệu suất tốt hơn. Nhiều

Table 8: Kết quả của mô hình 6×384 sử dụng số lượng relation head khác nhau.

--- TRANG 8 ---
[Bảng 9 và Figure 2]

Table 9: So sánh giữa MobileBERT và mô hình cùng kích thước (~12 layer, 384 hidden size và 128 embedding size) được chưng cất từ BERT LARGE (Whole Word Masking) trên tập test GLUE và tập dev SQuAD 2.0. Theo MobileBERT (Sun et al., 2019b), các kết quả báo cáo được fine-tune trực tiếp trên các tác vụ downstream. Chúng tôi tính toán speedup của MobileBERT theo độ trễ báo cáo của họ.

Figure 2: Các mô hình 6×384 được huấn luyện sử dụng các layer BERT BASE (a), BERT LARGE (b) và XLM-R LARGE (c) khác nhau.

kiến thức self-attention chi tiết có thể được nắm bắt bằng cách sử dụng nhiều relation head hơn, điều này giúp học sinh bắt chước sâu module self-attention của giáo viên. Bên cạnh đó, chúng tôi phát hiện rằng số lượng relation head không cần phải là bội số dương của cả số lượng attention head của học sinh và giáo viên. Relation head có thể là một phần của một attention head đơn hoặc chứa các phần từ nhiều attention head.

5 Thảo luận

5.1 So sánh với MobileBERT

MobileBERT (Sun et al., 2019b) nén một mô hình giáo viên được thiết kế đặc biệt (với kích thước BERT LARGE) với các module inverted bottleneck thành một học sinh 24-layer sử dụng các module bottleneck. Vì mục tiêu của chúng tôi là nén các mô hình lớn khác nhau (ví dụ BERT và RoBERTa) thành các mô hình nhỏ sử dụng kiến trúc Transformer tiêu chuẩn, chúng tôi lưu ý rằng mô hình học sinh của chúng tôi không thể so sánh trực tiếp với MobileBERT. Chúng tôi cung cấp kết quả của một mô hình học sinh với cùng kích thước tham số để tham khảo. Một mô hình kích thước lớn công khai (BERT LARGE-WWM) được sử dụng làm giáo viên, đạt hiệu suất tương tự như giáo viên của MobileBERT. Chúng tôi chưng cất BERT LARGE-WWM thành một mô hình học sinh (~25M tham số) sử dụng cùng dữ liệu huấn luyện (tức là English Wikipedia và BookCorpus). Kết quả test của GLUE và kết quả dev của SQuAD 2.0 được minh họa trong Table 9. Mô hình của chúng tôi vượt trội so với MobileBERT trên hầu hết các tác vụ với tốc độ suy luận nhanh hơn. Hơn nữa, phương pháp của chúng tôi có thể được áp dụng cho các giáo viên khác nhau và có ít hạn chế hơn nhiều đối với học sinh.

Chúng tôi cũng quan sát rằng mô hình của chúng tôi hoạt động tương đối kém hơn trên CoLA so với MobileBERT. Tác vụ của CoLA là đánh giá tính chấp nhận được về mặt ngữ pháp của một câu. Nó yêu cầu kiến thức ngôn ngữ học chi tiết hơn có thể được học từ các mục tiêu language modeling. Fine-tuning mô hình sử dụng mục tiêu MLM như trong MobileBERT mang lại cải thiện cho CoLA. Tuy nhiên, các thí nghiệm sơ bộ của chúng tôi cho thấy rằng chiến lược này sẽ dẫn đến giảm nhẹ cho các tác vụ GLUE khác.

--- TRANG 9 ---
5.2 Kết quả của nhiều quan hệ Self-Attention hơn

Trong Table 9 và 10, chúng tôi báo cáo kết quả của các học sinh được huấn luyện sử dụng nhiều quan hệ self-attention hơn (quan hệ Q-K, K-Q, Q-V, V-Q, K-V và V-K). Chúng tôi quan sát những cải thiện trên hầu hết các tác vụ, đặc biệt cho các mô hình học sinh được chưng cất từ BERT. Kiến thức self-attention chi tiết trong nhiều quan hệ attention hơn cải thiện các học sinh của chúng tôi. Tuy nhiên, việc giới thiệu nhiều quan hệ self-attention hơn cũng mang lại chi phí tính toán cao hơn. Để đạt được sự cân bằng giữa hiệu suất và chi phí tính toán, chúng tôi chọn chuyển giao quan hệ self-attention Q-Q, K-K và V-V thay vì tất cả quan hệ self-attention trong công việc này.

Table 10: Kết quả của việc giới thiệu nhiều quan hệ self-attention hơn (quan hệ Q-K, K-Q, Q-V, V-Q, K-V và V-K).

6 Kết luận

Chúng tôi tổng quát hóa chưng cất self-attention sâu trong MINILM bằng cách sử dụng quan hệ multi-head self-attention để huấn luyện học sinh. Phương pháp của chúng tôi giới thiệu kiến thức self-attention chi tiết hơn và loại bỏ hạn chế về số lượng attention head của học sinh. Hơn nữa, chúng tôi cho thấy rằng chuyển giao kiến thức self-attention của một layer giữa phía trên đạt hiệu suất tốt hơn cho giáo viên kích thước lớn. Các mô hình đơn ngữ và đa ngữ được chưng cất từ BERT, RoBERTa và XLM-R có được hiệu suất cạnh tranh và vượt trội so với các phương pháp state-of-the-art. Đối với công việc tương lai, chúng tôi đang khám phá một thuật toán lựa chọn layer tự động. Chúng tôi cũng muốn áp dụng phương pháp của mình cho các Transformer được pretrain lớn hơn.

Tài liệu tham khảo

Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, and Edward Guo. 2019. Knowledge distillation from internal representations. CoRR, abs/1910.03723.

Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020. Unilmv2: Pseudo-masked language models for unified language model pre-training. arXiv preprint arXiv:2002.12804.

Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.

Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC'09.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055.

Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xianling Mao, and Heyan Huang. 2019. Cross-lingual natural language generation via pre-training. CoRR, abs/1909.10481.

Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, XianLing Mao, Heyan Huang, and Ming Zhou. 2020. Infoxlm: An information-theoretic framework for cross-lingual language model pre-training. CoRR, abs/2007.07834.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. CoRR, abs/1911.02116.

Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).

Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, pages 177–190, Berlin, Heidelberg. Springer-Verlag.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.

William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In 33rd Conference on Neural Information Processing Systems (NeurIPS 2019).

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague. Association for Computational Linguistics.

--- TRANG 10 ---
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. CoRR, abs/1503.02531.

Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. 2020. Dynabert: Dynamic BERT with adaptive width and depth. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan Yang, and Ming Zhou. 2018. Attention-guided answer distillation for machine reading comprehension. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2077–2086.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling BERT for natural language understanding. CoRR, abs/1909.10351.

Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2019. Spanbert: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, San Diego, CA.

Guillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. Advances in Neural Information Processing Systems (NeurIPS).

Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.

Patrick S. H. Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019b. MLQA: evaluating cross-lingual extractive question answering. CoRR, abs/1910.07475.

Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min Yang, and Yaohong Jin. 2020. BERT-EMD: many-to-many layer mapping for BERT compression with earth mover's distance. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 3009–3018. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.

Subhabrata Mukherjee and Ahmed Hassan Awadallah. 2020. Xtremedistil: Multi-stage distillation for massive multilingual models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 2221–2234. Association for Computational Linguistics.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, pages 784–789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2015. Fitnets: Hints for thin deep nets. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2020. Poor man's BERT: smaller and faster transformer models. CoRR, abs/2004.03844.

Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.

--- TRANG 11 ---
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. Mass: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450.

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019a. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4322–4331.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2019b. Mobilebert: Task-agnostic compression of bert by progressive knowledge transfer.

Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. 2019. Small and practical BERT models for sequence labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 3630–3634.

Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: The impact of student initialization on knowledge distillation. CoRR, abs/1908.08962.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.

Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume

Corpus #Train #Dev #Test Metrics
Tác vụ câu đơn
CoLA 8.5k 1k 1k Matthews Corr
SST-2 67k 872 1.8k Accuracy
Tác vụ tương tự và Paraphrase
QQP 364k 40k 391k Accuracy/F1
MRPC 3.7k 408 1.7k Accuracy/F1
STS-B 7k 1.5k 1.4k Pearson/Spearman Corr
Tác vụ suy luận
MNLI 393k 20k 20k Accuracy
RTE 2.5k 276 3k Accuracy
QNLI 105k 5.5k 5.5k Accuracy
WNLI 634 71 146 Accuracy

Table 11: Tóm tắt benchmark GLUE.

#Train #Dev #Test Metrics
130,319 11,873 8,862 Exact Match/F1

Table 12: Thống kê dataset và metrics của SQuAD 2.0.

1 (Long Papers), pages 1112–1122, New Orleans, Louisiana. Association for Computational Linguistics.

Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. CoRR, abs/2002.02925.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. In 33rd Conference on Neural Information Processing Systems (NeurIPS 2019).

Sergey Zagoruyko and Nikos Komodakis. 2017. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.

A Benchmark GLUE

Tóm tắt các dataset được sử dụng cho benchmark General Language Understanding Evaluation (GLUE)⁴ (Wang et al., 2019) được trình bày trong Table 11.

⁴https://gluebenchmark.com/

--- TRANG 12 ---
B SQuAD 2.0

Chúng tôi trình bày thống kê dataset và metrics của SQuAD 2.0⁵ (Rajpurkar et al., 2018) trong Table 12.

C Siêu tham số cho Fine-tuning

Extractive Question Answering Đối với SQuAD 2.0, độ dài chuỗi tối đa là 384. Batch size được đặt là 32. Chúng tôi chọn learning rate từ {3e-5, 6e-5, 8e-5, 9e-5} và fine-tune mô hình trong 3 epoch. Tỷ lệ warmup và weight decay là 0.1 và 0.01.

GLUE Độ dài chuỗi tối đa là 128 cho benchmark GLUE. Chúng tôi đặt batch size là 32, chọn learning rate từ {1e-5, 1.5e-5, 2e-5, 3e-5, 5e-5} và epoch từ {3, 5, 10} cho các mô hình học sinh khác nhau. Chúng tôi fine-tune tác vụ CoLA với step huấn luyện dài hơn (25 epoch). Tỷ lệ warmup và weight decay là 0.1 và 0.01.

Cross-lingual Natural Language Inference (XNLI) Độ dài chuỗi tối đa là 256 cho XNLI. Chúng tôi fine-tune 10 epoch sử dụng 64 làm batch size. Learning rate được chọn từ {3e-5, 4e-5, 5e-5, 6e-5}.

Cross-lingual Question Answering Đối với MLQA, độ dài chuỗi tối đa là 512. Chúng tôi fine-tune 4 epoch sử dụng 32 làm batch size. Learning rate được chọn từ {3e-5, 4e-5, 5e-5, 6e-5}.

⁵http://stanford-qa.com
