# 2211.04076.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2211.04076.pdf
# Kích thước file: 92932 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
arXiv:2211.04076v1  [cs.LG]  8 Nov 2022Xấp xỉ Tự chú ý Tuyến tính thông qua
Nhân Feedforward Có thể Huấn luyện
Uladzislau Yorsh và Alexander Kovalenko[0000−0002−7194−1874]
Khoa Công nghệ Thông tin, Đại học Kỹ thuật Séc tại Prague
Prague, Cộng hòa Séc
{yorshula, kovalale }@fit.cvut.cz
Hạn chế nghiêm trọng của Transformers [18] do độ phức tạp bậc hai
của cơ chế tự chú ý đã thúc đẩy một lĩnh vực nghiên cứu mới về Trans-
formers hiệu quả [17], nhằm xấp xỉ kiến trúc gốc với các mô hình
nhanh hơn về mặt tiệm cận.

Mặc dù Transformers rất phổ biến, không thiên vị và có thể xử lý
các phụ thuộc dài tùy ý, độ phức tạp không gian và thời gian bậc hai
hạn chế ứng dụng Transformer trên các chuỗi dài. Trong bối cảnh này, nhiều
phát hiện về việc xấp xỉ attention với các mô-đun nhanh hơn về mặt tiệm cận
đã được thực hiện để xử lý các chuỗi dài hơn. Tuy nhiên, do thiếu một
benchmark thống nhất và có hệ thống, việc đánh giá tổng thể vẫn không chắc chắn cho đến
khi Tay et al. [16] xuất bản benchmark cho các mô hình Transformer hiệu quả được gọi là
"Long Range Arena", bao gồm các tác vụ với nhiều loại dữ liệu khác nhau.

Trong việc theo đuổi tính toán nhanh hơn, Transformers Hiệu quả thể hiện
một sự đa dạng ấn tượng về các phương pháp—các mô hình đạt được độ phức tạp attention
dưới bậc hai có thể sử dụng khái niệm thưa thớt [2,3,15] hoặc xấp xỉ hạng thấp
của đầu vào [19,20] để giảm số lượng khóa được chú ý; các cách khác để giảm
độ phức tạp bao gồm băm nhạy cảm với vị trí [12], pooling khóa [21], bộ nhớ
bổ sung để lưu trữ thông tin ở dạng nén [7,14] hoặc kết hợp với
các kiến trúc khác, như CNNs [1,8].

Thường dựa trên nền tảng toán học vững chắc, các phương pháp kernelized
cho phép xấp xỉ attention với độ phức tạp tuyến tính trong khi vẫn duy trì độ chính xác cao.
Công trình của Katharopoulos et al. [11] mô tả một xấp xỉ bao gồm
việc tính toán attention bằng tích vô hướng của queries và keys được chiếu. Tiếp theo,
công trình của Choromanski et al.k [4] chứng minh rằng một xấp xỉ như vậy
có thể chính xác tùy ý và vững chắc về mặt toán học, trong khi công trình
của Chowdhury et al. [5] báo cáo rằng phép chiếu có thể được học. Do đó,
trong bài báo này chúng tôi muốn mở rộng ý tưởng về các phương pháp kernel có thể huấn luyện
để xấp xỉ cơ chế tự chú ý của kiến trúc Transformer.

Đóng góp của chúng tôi: cho rằng mạng nơ-ron feedforward với ít nhất một
lớp ẩn [6], độ phi tuyến tùy ý, và số lượng nơ-ron tùy ý có thể
xấp xỉ bất kỳ hàm có tính chất tốt nào với độ chính xác bất kỳ, điều đó cho
mạng nơ-ron feedforward tiềm năng trở thành bộ xấp xỉ toàn cầu [9]. Do đó, chúng tôi
đề xuất rằng hàm kernel có thể huấn luyện φ(·) có thể xấp xỉ attention softmax
truyền thống một cách hiệu quả. Vì vậy, chúng tôi nghiên cứu khả năng sử dụng
mạng nơ-ron feedforward để biểu diễn φ(·). Chúng tôi thực nghiệm với kiến trúc của φ(·) và
kiểm tra hiệu suất của nó trên ba tác vụ Long Range Arena—phân loại văn bản,

--- TRANG 2 ---
2 Yorsh et al.
khớp tài liệu và ListOps, tuân theo hướng dẫn về giới hạn [16] số lượng
tham số có thể huấn luyện trong mô hình để cung cấp các chỉ số so sánh được.

Mô hình Kernelized. Các mô hình kernelized dựa trên phân tích nhân tử
sau đây của một phép toán attention:

Att(qi,K,V) =L/summationdisplay
j=1κ(qi,kj)/summationtextL
j′=1κ(qi,kj′)vj≈φ(qi)T/summationtextL
j=1φ(kj)vT
j
φ(qi)T/summationtextL
j=1φ(kj)

trong đó qi là một token query, K và V là các ma trận key và value, κ(q,k) là một hàm
kernel để mô hình hóa (exp( qTk) cho một Transformer cơ bản) và φ(·) là một hàm chiếu
mà chúng tôi xấp xỉ. φ(·) cần phải dương để duy trì ổn định số học, và có thể
thay đổi từ các hàm đơn giản như ELU + 1 đến các xấp xỉ kernel softmax ngẫu nhiên.
Trong công trình của chúng tôi, thay vì các chiến lược xấp xỉ với prior mạnh,
chúng tôi sử dụng một hàm tổng quát như feedforward NN.

Kernel Feedforward. Chúng tôi bắt đầu với một FFN một lớp, được định nghĩa là:
φ(X) =Softplus (XW)
trong đó W∈Rn×n là ma trận trọng số lớp. Đáng ngạc nhiên, mô hình này đã
thể hiện sự cải thiện hiệu suất đáng kể so với Performer và gần với
người dẫn đầu của bài báo LRA gốc. Theo [10], chúng tôi có thể tăng cường hiệu suất
bằng cách ép buộc tính trực giao thông qua khởi tạo trực giao và regularization.

Chúng tôi cũng thử xếp chồng nhiều lớp hơn, nhưng không quan sát thấy cải thiện hiệu suất
– có hoặc không có các lớp normalization ở giữa. Chúng tôi đã thử các độ phi tuyến
GELU và logistic sigmoid.

Kernel GLU. Gated Linear Units được định nghĩa là:
GLU(X) =XWf⊙σ(XWg)
trong đó σ(·) là logistic sigmoid và Wf,Wg là các ma trận trọng số. Lớp này cung cấp
độ phi tuyến theo từng phần tử và có thể biểu diễn các hàm phức tạp hơn,
nhưng yêu cầu tham số hóa gấp đôi so với một lớp tuyến tính.

Đối với mục đích của mô hình chúng tôi, chúng tôi cần sửa đổi GLU cuối cùng để ép buộc
đầu ra dương:
GLUoutput(X) =Softplus (XWf)⊙σ(XWg)

Chúng tôi cũng ép buộc tính trực giao của Wf trong các đơn vị này theo cách tương tự như
trong phần trước. Chúng tôi gọi các đơn vị được regularized này là O(rthogon al)GLU.

Để giảm thiểu sự tăng trưởng tham số hóa, chúng tôi áp dụng biến đổi theo head, và
gợi ý rằng gating không yêu cầu lượng thông tin như biến đổi đầu vào.
Vì vậy, chúng tôi có thể xấp xỉ Wg với, chẳng hạn, hai ma trận hạng thấp
có kích thước n×r tương ứng r×n trong đó r <n/2 là hạng xấp xỉ Wg.
Chúng tôi gọi đơn vị này là A(pproximated)OGLU.

--- TRANG 3 ---
Linear Self-Attention Approximation 3

Gating. So với FFN một lớp trực giao, mô hình OGLU một lớp
hội tụ nhanh hơn và cho thấy phương sai điểm số ít hơn đáng kể giữa
các lần chạy. Các đơn vị này cũng có thể được xếp chồng tuần tự với lợi ích,
đến một mức độ nhất định. Mặt khác, tham số hóa gấp đôi sẽ không cho phép xếp chồng
nhiều hơn hai đơn vị mà không vượt quá 10% tham số bổ sung.

Bằng cách xấp xỉ ma trận trọng số gating, chúng tôi có thể xếp chồng nhiều đơn vị hơn
– nhưng theo Bảng 1, điều này không mang lại lợi thế với chi phí tính toán
cao hơn. Chúng tôi sử dụng các ma trận hạng r=n/4 để xấp xỉ gate,
giảm tham số hóa lớp 25%.

Thí nghiệm. Theo khuyến nghị từ [16], chúng tôi nhân bản lịch trình
học và tất cả các siêu tham số liên quan đến mô hình của chúng tôi, trong khi
giữ tham số hóa bổ sung dưới 10%. Do hạn chế về sức mạnh tính toán,
chúng tôi chỉ giới hạn mình trong ba tác vụ LRA—phân loại văn bản BPE,
khớp văn bản BPE và ListOps, với độ dài đầu vào 4K/4K/2K
tương ứng. Để cung cấp kết quả có thể so sánh và tái tạo được, chúng tôi sử dụng
tích lũy gradient để mô phỏng kích thước batch lớn hơn. Mỗi mô hình được
huấn luyện năm lần để quan sát hành vi mô hình và tránh cái gọi là black swans—
các seed ngẫu nhiên cho kết quả khác biệt hoàn toàn [13]. Kết quả trung bình và tốt nhất
được báo cáo trong Bảng 1.

Mô hình Độ phức tạp Phân loại Khớp ListOps
Transformer O(L2) 64.27 57.46 36.37
Linear kernel†O(CL) 65.77 73.51 18.54
1×GLU O(CL) 65.82 72.17 18.67
2×GLU O(CL) 65.99 73.36 18.42
3×GLU O(CL) 65.87 72.60 18.68
Orth. linear kernel O(CL) 65.86 72.63 18.19
1×OGLU O(CL) 65.95 72.50 18.45
2×OGLU O(CL) 66.02 72.96 18.32
3×AOGLU O(CL) 66.06 72.57 18.45

Bảng 1: Kết quả của các mô hình chúng tôi trên các tác vụ LRA được chọn, kết quả trung bình
cho năm lần chạy. Chúng tôi ký hiệu bằng † các mô hình cho thấy phương sai đáng kể trong kết quả.

Lời cảm ơn
Nghiên cứu này được hỗ trợ bởi Bộ Giáo dục, Thanh niên và Thể thao Séc
từ Chương trình Hoạt động Séc về Nghiên cứu, Phát triển và Giáo dục,
theo thỏa thuận tài trợ số CZ.02.1.01/0.0/0.0/15003/0000421.

--- TRANG 4 ---
4 Yorsh et al.
Tài liệu tham khảo
1. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Mạng tích chập được tăng cường attention (2020)
2. Beltagy, I., Peters, M.E., Cohan, A.: Longformer: Transformer tài liệu dài (2020)
3. Child, R., Gray, S., Radford, A., Sutskever, I.: Tạo ra các chuỗi dài với sparse transformers (2019)
4. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L., Weller, A.: Suy nghĩ lại về attention với performers (2021)
5. Chowdhury, S.P., Solomou, A., Dubey, A., Sachan, M.: Về việc học kernel transformer (2021)
6. Cybenko, G.: Xấp xỉ bằng siêu vị trí của hàm sigmoid. Toán học điều khiển, tín hiệu và hệ thống 2(4), 303–314 (1989)
7. Dai, Z., Yang, Z., Yang, Y., Carbonell, J.G., Le, Q.V., Salakhutdinov, R.: Transformer-xl: Các mô hình ngôn ngữ chú ý vượt ra ngoài ngữ cảnh độ dài cố định. CoRR abs/1901.02860 (2019), http://arxiv.org/abs/1901.02860
8. Gulati, A., Qin, J., Chiu, C.C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., Pang, R.: Conformer: Transformer được tăng cường tích chập cho nhận dạng giọng nói (2020)
9. Hornik, K.: Khả năng xấp xỉ của mạng feedforward đa lớp. Mạng nơ-ron 4(2), 251–257 (1991)
10. Jia, K., Li, S., Wen, Y., Liu, T., Tao, D.: Mạng nơ-ron sâu trực giao (2019)
11. Katharopoulos, A., Vyas, A., Pappas, N., Fleuret, F.: Transformers là RNNs: Transformers tự hồi quy nhanh với attention tuyến tính (2020)
12. Kitaev, N., Lukasz Kaiser, Levskaya, A.: Reformer: Transformer hiệu quả (2020)
13. Picard, D.: Torch.manual seed(3407) là tất cả những gì bạn cần: Về ảnh hưởng của seed ngẫu nhiên trong kiến trúc deep learning cho thị giác máy tính (2021)
14. Rae, J.W., Potapenko, A., Jayakumar, S.M., Lillicrap, T.P.: Transformers nén cho mô hình hóa chuỗi tầm xa (2019)
15. Roy, A., Saffar, M., Vaswani, A., Grangier, D.: Attention thưa thớt dựa trên nội dung hiệu quả với routing transformers (2020)
16. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., Metzler, D.: Long range arena: Một benchmark cho transformers hiệu quả (2020)
17. Tay, Y., Dehghani, M., Bahri, D., Metzler, D.: Transformers hiệu quả: Một khảo sát (2020)
18. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (2017)
19. Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Tự chú ý với độ phức tạp tuyến tính. CoRR abs/2006.04768 (2020), https://arxiv.org/abs/2006.04768
20. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V.: Nyströmformer: Một thuật toán dựa trên Nyström để xấp xỉ tự chú ý. CoRR abs/2102.03902 (2021), https://arxiv.org/abs/2102.03902
21. Zhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., Chen, W.: Poolingformer: Mô hình hóa tài liệu dài với pooling attention. CoRR abs/2105.04371 (2021), https://arxiv.org/abs/2105.04371