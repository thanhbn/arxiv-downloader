# 2009.14794.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2009.14794.pdf
# File size: 5880177 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2021
RETHINKING ATTENTION WITH PERFORMERS
Krzysztof Choromanski1, Valerii Likhosherstov2, David Dohan1, Xingyou Song1
Andreea Gane1, Tamas Sarlos1, Peter Hawkins1, Jared Davis3, Afroz Mohiuddin1
Lukasz Kaiser1, David Belanger1, Lucy Colwell1;2, Adrian Weller2;4
1Google2University of Cambridge3DeepMind4Alan Turing Institute
ABSTRACT
We introduce Performers , Transformer architectures which can estimate regular
(softmax) full-rank-attention Transformers with provable accuracy, but using only
linear (as opposed to quadratic) space and time complexity, without relying on
any priors such as sparsity or low-rankness. To approximate softmax attention-
kernels, Performers use a novel Fast Attention Via positive Orthogonal Random
features approach (FA VOR+), which may be of independent interest for scalable
kernel methods. FA VOR+ can also be used to efﬁciently model kernelizable
attention mechanisms beyond softmax. This representational power is crucial to
accurately compare softmax with other kernels for the ﬁrst time on large-scale tasks,
beyond the reach of regular Transformers, and investigate optimal attention-kernels.
Performers are linear architectures fully compatible with regular Transformers
and with strong theoretical guarantees: unbiased or nearly-unbiased estimation
of the attention matrix, uniform convergence and low estimation variance. We
tested Performers on a rich set of tasks stretching from pixel-prediction through
text models to protein sequence modeling. We demonstrate competitive results
with other examined efﬁcient sparse and dense attention methods, showcasing
effectiveness of the novel attention-learning paradigm leveraged by Performers.
1 I NTRODUCTION AND RELATED WORK
Transformers (Vaswani et al., 2017; Dehghani et al., 2019) are powerful neural network architectures
that have become SOTA in several areas of machine learning including natural language processing
(NLP) (e.g. speech recognition (Luo et al., 2020)), neural machine translation (NMT) (Chen et al.,
2018), document generation/summarization, time series prediction, generative modeling (e.g. image
generation (Parmar et al., 2018)), music generation (Huang et al., 2019), and bioinformatics (Rives
et al., 2019; Madani et al., 2020; Ingraham et al., 2019; Elnaggar et al., 2019; Du et al., 2020).
Transformers rely on a trainable attention mechanism that identiﬁes complex dependencies between
the elements of each input sequence. Unfortunately, the regular Transformer scales quadratically
with the number of tokens Lin the input sequence, which is prohibitively expensive for large L
and precludes its usage in settings with limited computational resources even for moderate values
ofL. Several solutions have been proposed to address this issue (Beltagy et al., 2020; Gulati et al.,
2020; Chan et al., 2020; Child et al., 2019; Bello et al., 2019). Most approaches restrict the attention
mechanism to attend to local neighborhoods (Parmar et al., 2018) or incorporate structural priors
on attention such as sparsity (Child et al., 2019), pooling-based compression (Rae et al., 2020)
clustering/binning/convolution techniques (e.g. (Roy et al., 2020) which applies k-means clustering
to learn dynamic sparse attention regions, or (Kitaev et al., 2020), where locality sensitive hashing
is used to group together tokens of similar embeddings), sliding windows (Beltagy et al., 2020),
or truncated targeting (Chelba et al., 2020). There is also a long line of research on using dense
attention matrices, but deﬁned by low-rank kernels substituting softmax (Katharopoulos et al., 2020;
Shen et al., 2018). Those methods critically rely on kernels admitting explicit representations as
dot-products of ﬁnite positive-feature vectors.
The approaches above do not aim to approximate regular attention, but rather propose simpler and
more tractable attention mechanisms, often by incorporating additional constraints (e.g. identical
query and key sets as in (Kitaev et al., 2020)), or by trading regular with sparse attention using more
Equal contribution. Correspondence to {kchoro,lcolwell}@google.com .
Code for Transformer models on protein data can be found in github.com/google-research/
google-research/tree/master/protein_lm and Performer code can be found in github.com/
google-research/google-research/tree/master/performer . Google AI Blog: https://
ai.googleblog.com/2020/10/rethinking-attention-with-performers.html
1arXiv:2009.14794v4  [cs.LG]  19 Nov 2022

--- PAGE 2 ---
Published as a conference paper at ICLR 2021
layers (Child et al., 2019). Unfortunately, there is a lack of rigorous guarantees for the representation
power produced by such methods, and sometimes the validity of sparsity patterns can only be veriﬁed
empirically through trial and error by constructing special GPU operations (e.g. either writing C++
CUDA kernels (Child et al., 2019) or using TVMs (Beltagy et al., 2020)). Other techniques which
aim to reduce Transformers’ space complexity include reversible residual layers allowing one-time
activation storage in training (Kitaev et al., 2020) and shared attention weights (Xiao et al., 2019).
These constraints may impede application to long-sequence problems, where approximations of
the attention mechanism are not sufﬁcient. Approximations based on truncated back-propagation
(Dai et al., 2019) are also unable to capture long-distance correlations since the gradients are only
propagated inside a localized window. Other methods propose biased estimation of regular attention
but only in the non-causal setting and with large mean squared error (Wang et al., 2020).
In response, we introduce the ﬁrst Transformer architectures, Performers , capable of provably
accurate and practical estimation of regular (softmax) full-rank attention, but of only linear space
and time complexity and not relying on any priors such as sparsity or low-rankness. Performers
use the Fast Attention Via positive Orthogonal Random features (FA VOR+) mechanism, leveraging
new methods for approximating softmax and Gaussian kernels, which we propose. We believe
these methods are of independent interest, contributing to the theory of scalable kernel methods.
Consequently, Performers are the ﬁrst linear architectures fully compatible (via small amounts
of ﬁne-tuning) with regular Transformers, providing strong theoretical guarantees: unbiased or
nearly-unbiased estimation of the attention matrix, uniform convergence and lower variance of the
approximation.
FA VOR+ can be also applied to efﬁciently model other kernelizable attention mechanisms beyond
softmax. This representational power is crucial to accurately compare softmax with other kernels
for the ﬁrst time on large-scale tasks, that are beyond the reach of regular Transformers, and ﬁnd for
them optimal attention-kernels. FA VOR+ can also be applied beyond the Transformer scope as a
more scalable replacement for regular attention, which itself has a wide variety of uses in computer
vision (Fu et al., 2019), reinforcement learning (Zambaldi et al., 2019), training with softmax cross
entropy loss, and even combinatorial optimization (Vinyals et al., 2015).
We test Performers on a rich set of tasks ranging from pixel-prediction through text models to protein
sequence modeling. We demonstrate competitive results with other examined efﬁcient sparse and
dense attention methods, showcasing the effectiveness of the novel attention-learning paradigm
leveraged by Performers. We emphasize that in principle, FA VOR+ can also be combined with other
techniques, such as reversible layers (Kitaev et al., 2020) or cluster-based attention (Roy et al., 2020).
2 FAVOR+ M ECHANISM & P OSITIVE ORTHOGONAL RANDOM FEATURES
Below we describe in detail the FA VOR+ mechanism - the backbone of the Performer0sarchitecture.
We introduce a new method for estimating softmax (and Gaussian) kernels with positive orthogonal
random features which FA VOR+ leverages for the robust and unbiased estimation of regular (softmax)
attention and show how FA VOR+ can be applied for other attention-kernels.
2.1 P RELIMINARIES -REGULAR ATTENTION MECHANISM
LetLbe the size of an input sequence of tokens. Then regular dot-product attention (Vaswani et al.,
2017) is a mapping which accepts matrices Q;K;V2RLdas input where dis the hidden dimension
(dimension of the latent representation). Matrices Q;K;Vare intermediate representations of the
input and their rows can be interpreted as queries ,keys andvalues of the continuous dictionary data
structure respectively. Bidirectional (or non-directional (Devlin et al., 2018)) dot-product attention
has the following form, where A2RLLis the so-called attention matrix :
Att$(Q;K;V) =D 1AV;A= exp( QK>=p
d);D= diag( A1L): (1)
Here exp()is applied elementwise, 1Lis the all-ones vector of length L, and diag()is a diagonal
matrix with the input vector as the diagonal. Time and space complexity of computing (1) are O(L2d)
andO(L2+Ld)respectively, because Ahas to be stored explicitly. Hence, in principle, dot-product
attention of type (1) is incompatible with end-to-end processing of long sequences. Bidirectional
attention is applied in encoder self-attention and encoder-decoder attention in Seq2Seq architectures.
Another important type of attention is unidirectional dot-product attention which has the form:
Att!(Q;K;V) =eD 1eAV;eA= tril( A);eD= diag(eA1L); (2)
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2021
where tril()returns the lower-triangular part of the argument matrix including the diagonal. As
discussed in (Vaswani et al., 2017), unidirectional attention is used for autoregressive generative
modelling, e.g. as self-attention in generative Transformers as well as the decoder part of Seq2Seq
Transformers.
We will show that attention matrix Acan be approximated up to any precision in time O(Ld2log(d)).
For comparison, popular methods leveraging sparsity via Locality-Sensitive Hashing (LSH) tech-
niques (Kitaev et al., 2020) have O(Ld2logL)time complexity. In the main body of the paper we
will describe FA VOR+ for bidirectional attention. Completely analogous results can be obtained for
the unidirectional variant via the mechanism of preﬁx-sums (all details in the Appendix B.1).
2.2 G ENERALIZED KERNELIZABLE ATTENTION
FA VOR+ works for attention blocks using matrices A2RLLof the form A(i;j) = K( q>
i;k>
j),
withqi=kjstanding for the ith=jthquery/key row-vector in Q=Kand kernel K :RdRd!R+
deﬁned for the (usually randomized) mapping: :Rd!Rr
+(for somer>0) as:
K(x;y) =E[(x)>(y)]: (3)
We call(u)arandom feature map foru2Rd. ForQ0;K02RLrwith rows given as (q>
i)>
and(k>
i)>respectively, Equation 3 leads directly to the efﬁcient attention mechanism of the form:
\Att$(Q;K;V) =bD 1(Q0((K0)>V));bD= diag( Q0((K0)>1L)): (4)
Here\Att$stands for the approximate attention and brackets indicate the order of computations. It is
easy to see that such a mechanism is characterized by space complexity O(Lr+Ld+rd)and time
complexityO(Lrd)as opposed to O(L2+Ld)andO(L2d)of the regular attention (see also Fig. 1).
Figure 1: Approximation of the regular attention mechanism AV (beforeD 1-renormalization) via (random)
feature maps. Dashed-blocks indicate order of computation with corresponding time complexities attached.
The above scheme constitutes the FA-part of the FA VOR+ mechanism. The remaining OR+ part
answers the following questions: (1)How expressive is the attention model deﬁned in Equation 3,
and in particular, can we use it in principle to approximate regular softmax attention ? (2)How do
we implement it robustly in practice, and in particular, can we choose rLforLdto obtain
desired space and time complexity gains? We answer these questions in the next sections.
2.3 H OW TO AND HOW NOT TO APPROXIMATE SOFTMAX -KERNELS FOR ATTENTION
It turns out that by taking of the following form for functions f1;:::;fl:R!R, function
g:Rd!Rand deterministic vectors !ior!1;:::;!miidD for some distribution D2P (Rd):
(x) =h(x)pm(f1(!>
1x);:::;f 1(!>
mx);:::;fl(!>
1x);:::;fl(!>
mx)); (5)
we can model most kernels used in practice. Furthermore, in most cases Dis isotropic (i.e. with
pdf function constant on a sphere), usually Gaussian. For example, by taking h(x) = 1 ,l= 1and
D=N(0;Id)we obtain estimators of the so-called PNG-kernels (Choromanski et al., 2017) (e.g.
f1= sgn corresponds to the angular kernel). Conﬁgurations: h(x) = 1 ,l= 2,f1= sin ,f2= cos
correspond to shift-invariant kernels, in particular D=N(0;Id)leads to the Gaussian kernel Kgauss
(Rahimi & Recht, 2007). The softmax-kernel which deﬁnes regular attention matrix Ais given as:
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2021
SM(x;y)def= exp( x>y): (6)
In the above, without loss of generality, we omitp
d-renormalization since we can equivalently
renormalize input keys and queries. Since: SM(x;y) = exp(kxk2
2)Kgauss(x;y) exp(kyk2
2), based
on what we have said, we obtain random feature map unbiased approximation of SM(x;y)using
trigonometric functions with: h(x) = exp(kxk2
2),l= 2,f1= sin ,f2= cos . We call itdSMtrig
m(x;y).
There is however a caveat there. The attention module from (1) constructs for each token, a convex
combination of value-vectors with coefﬁcients given as corresponding renormalized kernel scores.
That is why kernels producing non-negative scores are used. Applying random feature maps with
potentially negative dimension-values ( sin=cos) leads to unstable behaviours, especially when kernel
scores close to 0(which is the case for many entries of Acorresponding to low relevance tokens) are
approximated by estimators with large variance in such regions. This results in abnormal behaviours,
e.g. negative-diagonal-values renormalizers D 1, and consequently either completely prevents
training or leads to sub-optimal models. We demonstrate empirically that this is what happens for
dSMtrig
mand provide detailed theoretical explanations showing that the variance of dSMtrig
mis large
as approximated values tend to 0(see: Section 3). This is one of the main reasons why the robust
random feature map mechanism for approximating regular softmax attention was never proposed.
We propose a robust mechanism in this paper. Furthermore, the variance of our new unbiased positive
random feature map estimator tends to 0as approximated values tend to 0(see: Section 3).
Lemma 1 (Positive Random Features (PRFs) for Softmax) .Forx;y2Rd,z=x+ywe have:
SM(x;y) =E!N(0;Id)h
exp
!>x kxk2
2
exp
!>y kyk2
2i
= E!N(0;Id)cosh(!>z);(7)
where  = exp( kxk2+kyk2
2)andcosh is hyperbolic cosine. Consequently, softmax-kernel admits a
positive random feature map unbiased approximation with h(x) = exp( kxk2
2),l= 1,f1= exp
andD=N(0;Id)or:h(x) =1p
2exp( kxk2
2),l= 2,f1(u) = exp(u),f2(u) = exp( u)and the
sameD(the latter for further variance reduction). We call related estimators: dSM+
manddSMhyp+
m .
Figure 2: Left: Symmetrized (around origin) utility function r(deﬁned as the ratio of the mean squared errors
(MSEs) of estimators built on: trigonometric and positive random features) as a function of the angle (in
radians) between input feature vectors and their lengths l. Larger values indicate regions of (;l)-space with
better performance of positive random features. We see that for critical regions with large enough (small
enough softmax-kernel values) our method is arbitrarily more accurate than trigonometric random features. Plot
presented for domain [ ;][ 2;2].Right: The slice of function rfor ﬁxedl= 1 and varying angle .
Right Upper Corner: Comparison of the MSEs of both the estimators in a low softmax-kernel value region.
In Fig. 2 we visualize the advantages of positive versus standard trigonometric random features. In
critical regions, where kernel values are small and need careful approximation, our method outper-
forms its counterpart. In Section 4 we further conﬁrm our method’s advantages empirically, using
positive features to efﬁciently train softmax-based linear Transformers. If we replace in (7) !withp
d!
k!k, we obtain the so-called regularized softmax-kernel SMREG which we can approximate in
a similar manner, simply changing D=N(0;Id)toD= Unif(p
dSd 1), a distribution correspond-
ing to Haar measure on the sphere of radiusp
dinRd, obtaining estimator \SMREG+
m. As we show
in Section 3, such random features can also be used to accurately approximate regular softmax-kernel.
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2021
2.4 O RTHOGONAL RANDOM FEATURES (ORF S)
The above constitutes the R+ part of the FA VOR+ method. It remains to explain the O-part. To
further reduce the variance of the estimator (so that we can use an even smaller number of random
featuresr), we entangle different random samples !1;:::;!mto be exactly orthogonal. This can be
done while maintaining unbiasedness whenever isotropic distributions Dare used (i.e. in particular
in all kernels we considered so far) by the standard Gram-Schmidt orthogonalization procedure (see
(Choromanski et al., 2017) for details). ORFs is a well-known method, yet it turns out that it works
particularly well with our introduced PRFs for softmax. This leads to the ﬁrst theoretical results
showing that ORFs can be applied to reduce the variance of softmax/Gaussian kernel estimators for
anydimensionality drather than just asymptotically for large enough d(as is the case for previous
methods, see: next section) and leads to the ﬁrst exponentially small bounds on large deviations
probabilities that are strictly smaller than for non-orthogonal methods. Positivity of random features
plays a key role in these bounds. The ORF mechanism requires md, but this will be the case in all
our experiments. The pseudocode of the entire FA VOR+ algorithm is given in Appendix B.
Our theoretical results are tightly aligned with experiments. We show in Section 4 that PRFs+ORFs
drastically improve accuracy of the approximation of the attention matrix and enable us to reduce r
which results in an accurate as well as space and time efﬁcient mechanism which we call FA VOR+.
3 T HEORETICAL RESULTS
We present here the theory of positive orthogonal random features for softmax-kernel estimation. All
these results can be applied also to the Gaussian kernel, since as explained in the previous section,
one can be obtained from the other by renormalization (see: Section 2.3). All proofs and additional
more general theoretical results with a discussion are given in the Appendix.
Lemma 2 (positive (hyperbolic) versus trigonometric random features) .The following is true:
MSE(dSMtrig
m(x;y)) =1
2mexp(kx+yk2)SM 2(x;y)(1 exp( kx yk2))2;
MSE(dSM+
m(x;y)) =1
mexp(kx+yk2)SM2(x;y)(1 exp( kx+yk2));
MSE(dSMhyp+
m(x;y)) =1
2(1 exp( kx+yk2))MSE(dSM+
m(x;y));(8)
for independent random samples !i, and where MSE stands for the mean squared error.
Thus, for SM(x;y)!0we have: MSE(dSMtrig
m(x;y))!1 andMSE(dSM+
m(x;y))!0. Further-
more, the hyperbolic estimator provides additional accuracy improvements that are strictly better
than those from dSM+
2m(x;y)with twice as many random features. The next result shows that the
regularized softmax-kernel is in practice an accurate proxy of the softmax-kernel in attention.
Theorem 1 (regularized versus softmax-kernel) .Assume that the L1-norm of the attention matrix
for the softmax-kernel satisﬁes: kAk1Cfor some constant C1. Denote by Aregthe
corresponding attention matrix for the regularized softmax-kernel. The following holds:
inf
i;jAreg(i;j)
A(i;j)1 2
d1
3+o1
d1
3
;andsup
i;jAreg(i;j)
A(i;j)1: (9)
Furthermore, the latter holds for d2even if theL1-norm condition is not satisﬁed, i.e. the
regularized softmax-kernel is a universal lower bound for the softmax-kernel.
Consequently, positive random features for SMREG can be used to approximate the softmax-kernel.
Our next result shows that orthogonality provably reduces mean squared error of the estimation with
positive random features for any dimensionality d>0and we explicitly provide the gap.
Theorem 2. IfdSMort+
m(x;y)stands for the modiﬁcation of dSM+
m(x;y)with orthogonal random
features (and thus for md), then the following holds for any d>0:
MSE(dSMort+
m(x;y))MSE(dSM+
m(x;y)) 2(m 1)
m(d+ 2)
SM(x;y) exp
 kxk2+kyk2
22
:
(10)
Furthermore, completely analogous result holds for the regularized softmax-kernel SMREG .
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2021
For the regularized softmax-kernel, orthogonal features provide additional concentration results - the
ﬁrst exponentially small bounds for probabilities of estimators’ tails that are strictly better than for
non-orthogonal variants for every d>0. Our next result enables us to explicitly estimate the gap.
Theorem 3. Letx;y2Rd. The following holds for any a>SMREG( x;y),>0andmd:
P[\SMREG+
m(x;y)>a]exp( ma)MZ()m;P[\SMREGort+
m(x;y)>a]
exp( ma)
MZ()m exp
 m
2(kxk2+kyk2)4m(m 1)
4(d+ 2)kx+yk4
where \SMREGort+
m(x;y)stands for the modiﬁcation of \SMREG+
m(x;y)with ORFs, X=
 exp(p
d!>
k!k2(x+y)),!N (0;Id),is as in Lemma 1 and MZis the moment generating
function ofZ.
We see that ORFs provide exponentially small and sharper bounds for critical regions where the
softmax-kernel is small. Below we show that even for the SMtrigmechanism with ORFs, it sufﬁces to
takem= (dlog(d))random projections to accurately approximate the attention matrix (thus if not
attention renormalization, PRFs would not be needed). In general, mdepends on the dimensionality
dof the embeddings, radius Rof the ball where all queries/keys live and precision parameter (see:
Appendix F.6 for additional discussion), but does not depend on input sequence length L.
Theorem 4 (uniform convergence for attention approximation) .Assume that L2-norms of
queries/keys are upper-bounded by R > 0. Deﬁnel=Rd 1
4and takeh= exp(l2
2). Then
for any >0,=
(h)2and the number of random projections m= (d
2log(4d3
4R
))the fol-
lowing holds for the attention approximation mechanism leveraging estimators dSMtrigwith ORFs:
kbA Ak1with any constant probability, where bAapproximates the attention matrix A.
4 E XPERIMENTS
We implemented our setup on top of pre-existing Transformer training code in Jax (Frostig et al.,
2018) optimized with just-in-time ( jax.jit ) compilation, and complement our theory with em-
pirical evidence to demonstrate the practicality of FA VOR+ in multiple settings. Unless explicitly
stated, a Performer replaces only the attention component with our method, while all other com-
ponents are exactly the same as for the regular Transformer. For shorthand notation, we denote
unidirectional/causal modelling as (U)and bidirectional/masked language modelling as (B).
In terms of baselines, we use other Transformer models for comparison, although some of them
are restricted to only one case - e.g. Reformer (Kitaev et al., 2020) is only (U), and Linformer
(Wang et al., 2020) is only (B). Furthermore, we use PG-19 (Rae et al., 2020) as an alternative (B)
pretraining benchmark, as it is made for long-length sequence training compared to the (now publicly
unavailable) BookCorpus (Zhu et al., 2015) + Wikipedia dataset used in BERT (Devlin et al., 2018)
and Linformer. All model and tokenization hyperparameters are shown in Appendix A.
Figure 3: Comparison of Transformer and Performer in terms of forward and backward pass speed and
maximumLallowed. "X" (OPT) denotes the maximum possible speedup achievable, when attention simply
returns the V-matrix. Plots shown up to when a model produces an out of memory error on a V100 GPU with
16GB. V ocabulary size used was 256. Best in color.
4.1 C OMPUTATIONAL COSTS
We compared speed-wise the backward pass of the Transformer and the Performer in (B) setting,
as it is one of the main computational bottlenecks during training, when using the regular default
size(nheads;nlayers;dff;d) = (8;6;2048;512), wheredffdenotes the width of the MLP layers.
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2021
We observed (Fig. 3) that in terms of L, the Performer reaches nearly linear time and sub-quadratic
memory consumption (since the explicit O(L2)attention matrix is not stored). In fact, the Performer
achieves nearly optimal speedup and memory efﬁciency possible, depicted by the "X"-line when
attention is replaced with the "identity function" simply returning the V-matrix. The combination of
both memory and backward pass efﬁciencies for large Lallows respectively, large batch training and
lower wall clock time per gradient step. Extensive additional results are demonstrated in Appendix E
by varying layers, raw attention, and architecture sizes.
4.2 S OFTMAX ATTENTION APPROXIMATION ERROR
We further examined the approximation error via FA VOR+ in Fig. 4. We demonstrate that 1.
Orthogonal features produce lower error than unstructured (IID) features, 2.Positive features produce
lower error than trigonometric sin/cosfeatures. These two empirically validate the PORF mechanism.
Figure 4: MSE of the approximation output when comparing Orthogonal vs IID features and trigonometric
sin/cosvs positive features. We took L= 4096;d= 16 , and varied the number of random samples m. Standard
deviations shown across 15 samples of appropriately normalized random matrix input data.
To further improve overall approximation of attention blocks across multiple iterations which further
improves training, random samples should be periodically redrawn (Fig. 5, right). This is a cheap
procedure, but can be further optimized (Appendix B.2).
4.3 S OFTMAX APPROXIMATION ON TRANSFORMERS
Even if the approximation of the attention mechanism is tight, small errors can easily propagate
throughout multiple Transformer layers (e.g. MLPs, multiple heads), as we show in Fig. 14
(Appendix). In other words, the model’s Lipschitz constant can easily scale up small attention
approximation error, which means that very tight approximations may sometimes be needed. Thus,
when applying FA VOR(+)’s softmax approximations on a Transformer model (i.e. "Performer-X-
SOFTMAX"), we demonstrate that:
1.Backwards compatibility with pretrained models is available as a beneﬁt from softmax approxima-
tion, via small ﬁnetuning (required due to error propagation) even for trigonometric features (Fig. 5,
left) on the LM1B dataset (Chelba et al., 2014). However, when on larger dataset PG-19, 2.Positive
(POS) softmax features (with redrawing) become crucial for achieving performance matching regular
Transformers (Fig. 5, right).
Figure 5: We transferred the original pretrained Transformer’s weights into the Performer, which produces
an initial non-zero 0.07 accuracy (dotted orange line), but quickly recovers accuracy in a small fraction of the
original number of gradient steps. However on PG-19, Trigonometric (TRIG) softmax approximation becomes
highly unstable (full curve in Appendix D.2), while positive features (POS) (without redrawing) and Linformer
(which also approximates softmax) even with redrawn projections , plateau at the same perplexity. Positive
softmax with feature redrawing is necessary to match the Transformer, with SMREG (regularization from Sec.
3) allowing faster convergence. Additional ablation studies over many attention kernels, showing also that
trigonometric random features lead even to NaN values in training are given in Appendix D.3.
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2021
4.4 M ULTIPLE LAYER TRAINING FOR PROTEINS
We further benchmark the Performer on both (U) and (B) cases by training a 36-layer model using
protein sequences from the Jan. 2019 release of TrEMBL (Consortium, 2019), similar to (Madani
et al., 2020). In Fig. 6, the Reformer and Linformer signiﬁcantly drop in accuracy on the protein
dataset. Furthermore, the usefulness of generalized attention is evidenced by Performer-RELU (taking
f= ReLU in Equation 5) achieving the highest accuracy in both (U) and (B) cases. Our proposed
softmax approximation is also shown to be tight, achieving the same accuracy as the exact-softmax
Transformer and conﬁrming our theoretical claims from Section 3.
Figure 6: Train = Dashed, Validation = Solid. For TrEMBL, we used the exact same model parameters
(nheads;nlayers;dff;d) = (8;36;1024;512) from (Madani et al., 2020) for all runs. For fairness, all TrEMBL
experiments used 16x16 TPU-v2’s. Batch sizes were maximized for each separate run given the compute
constraints. Hyperparameters can be found in Appendix A. Extended results including dataset statistics, out of
distribution evaluations, and visualizations, can be found in Appendix C.
4.5 L ARGE LENGTH TRAINING - COMMON DATASETS
On the standard (U) ImageNet64 benchmark from (Parmar et al., 2018) with L= 12288 which is
unfeasible for regular Transformers, we set all models to use the same (nheads;dff;d)but varying
nlayers . Performer/6-layers matches the Reformer/12-layers, while the Performer/12-layers matches
the Reformer/24-layers (Fig. 7: left). Depending on hardware (TPU or GPU), we also found that the
Performer can be 2x faster than the Reformer via Jax optimizations for the (U) setting.
For a proof of principle study, we also create an initial protein benchmark for predicting interactions
among groups of proteins by concatenating protein sequences to length L= 8192 from TrEMBL,
long enough to model protein interaction networks without the large sequence alignments required by
existing methods (Cong et al., 2019). In this setting, a regular Transformer overloads memory even at
a batch size of 1per chip, by a wide margin. Thus as a baseline, we were forced to use a signiﬁcantly
smaller variant, reducing to (nheads;nlayers;dff;d) = (8;f1;2;3g;256;256) . Meanwhile, the Per-
former trains efﬁciently at a batch size of 8 per chip using the standard (8;6;2048;512) architecture.
We see in Fig. 7 (right subﬁgure) that the smaller Transformer ( nlayer = 3) is quickly bounded at
19%, while the Performer is able to train continuously to 24%.
Figure 7: Train = Dashed, Validation = Solid. For ImageNet64, all models used the standard (nheads;dff;d) =
(8;2048;512) . We further show that our positive softmax approximation achieves the same performance as
ReLU in Appendix D.2. For concatenated TrEMBL, we varied nlayers2f1;2;3gfor the smaller Transformer.
Hyperparameters can be found in Appendix A.
5 C ONCLUSION
We presented Performer , a new type of Transformer, relying on our Fast Attention Via positive Or-
thogonal Random features (FA VOR+) mechanism to signiﬁcantly improve space and time complexity
of regular Transformers. Our mechanism provides to our knowledge the ﬁrst effective unbiased esti-
mation of the original softmax-based Transformer with linear space and time complexity and opens
new avenues in the research on Transformers and the role of non-sparsifying attention mechanisms.
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2021
6 B ROADER IMPACT
We believe that the presented algorithm can be impactful in various ways:
Biology and Medicine: Our method has the potential to directly impact research on biological
sequence analysis by enabling the Transformer to be applied to much longer sequences without
constraints on the structure of the attention matrix. The initial application that we consider is the
prediction of interactions between proteins on the proteome scale. Recently published approaches
require large evolutionary sequence alignments, a bottleneck for applications to mammalian genomes
(Cong et al., 2019). The potentially broad translational impact of applying these approaches to biolog-
ical sequences was one of the main motivations of this work. We believe that modern bioinformatics
can immensely beneﬁt from new machine learning techniques with Transformers being among the
most promising. Scaling up these methods to train faster more accurate language models opens
the door to the ability to design sets of molecules with pre-speciﬁed interaction properties. These
approaches could be used to augment existing physics-based design strategies that are of critical
importance for example in the development of new nanoparticle vaccines (Marcandalli et al., 2019).
Environment: As we have shown, Performers with FA VOR+ are characterized by much lower
compute costs and substantially lower space complexity which can be directly translated to CO2
emission reduction (Strubell et al., 2019) and lower energy consumption (You et al., 2020), as regular
Transformers require very large computational resources.
Research on Transformers: We believe that our results can shape research on efﬁcient Transformers
architectures, guiding the ﬁeld towards methods with strong mathematical foundations. Our research
may also hopefully extend Transformers also beyond their standard scope (e.g. by considering the
Generalized Attention mechanism and connections with kernels). Exploring scalable Transformer
architectures that can handle Lof the order of magnitude few thousands and more, preserving
accuracy of the baseline at the same time, is a gateway to new breakthroughs in bio-informatics,
e.g. language modeling for proteins, as we explained in the paper. Our presented method can be
potentially a ﬁrst step.
Backward Compatibility: Our Performer can be used on the top of a regular pre-trained Transformer
as opposed to other Transformer variants. Even if up-training is not required, FA VOR+ can still be
used for fast inference with no loss of accuracy. We think about this backward compatibility as a
very important additional feature of the presented techniques that might be particularly attractive for
practitioners.
Attention Beyond Transformers: Finally, FA VOR+ can be applied to approximate exact attention
also outside the scope of Transformers. This opens a large volume of new potential applications
including: hierarchical attention networks (HANS) (Yang et al., 2016), graph attention networks
(Velickovic et al., 2018), image processing (Fu et al., 2019), and reinforcement learning/robotics
(Tang et al., 2020).
7 A CKNOWLEDGEMENTS
We thank Nikita Kitaev and Wojciech Gajewski for multiple discussions on the Reformer, and
also thank Aurko Roy and Ashish Vaswani for multiple discussions on the Routing Transformer.
We further thank Joshua Meier, John Platt, and Tom Weingarten for many fruitful discussions on
biological data and useful comments on this draft. We lastly thank Yi Tay and Mostafa Dehghani for
discussions on comparing baselines.
Valerii Likhosherstov acknowledges support from the Cambridge Trust and DeepMind. Lucy Colwell
acknowledges support from the Simons Foundation. Adrian Weller acknowledges support from a
Turing AI Fellowship under grant EP/V025379/1, The Alan Turing Institute under EPSRC grant
EP/N510129/1 and U/B/000074, and the Leverhulme Trust via CFI.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2021
REFERENCES
Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V . Le. Attention augmented
convolutional networks. CoRR , abs/1904.09925, 2019. URL http://arxiv.org/abs/
1904.09925 .
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
CoRR , abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150 .
William Chan, Chitwan Saharia, Geoffrey E. Hinton, Mohammad Norouzi, and Navdeep Jaitly.
Imputer: Sequence modelling via imputation and dynamic programming. CoRR , abs/2002.08926,
2020. URL https://arxiv.org/abs/2002.08926 .
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
InINTERSPEECH 2014, 15th Annual Conference of the International Speech Communication
Association, Singapore, September 14-18, 2014 , pp. 2635–2639, 2014.
Ciprian Chelba, Mia Xu Chen, Ankur Bapna, and Noam Shazeer. Faster transformer decoding:
N-gram masked self-attention. CoRR , abs/2001.04589, 2020. URL https://arxiv.org/
abs/2001.04589 .
Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George F. Foster,
Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Combining
recent advances in neural machine translation. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20,
2018, Volume 1: Long Papers , pp. 76–86. Association for Computational Linguistics, 2018. doi:
10.18653/v1/P18-1008. URL https://www.aclweb.org/anthology/P18-1008/ .
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR , abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509 .
Krzysztof Choromanski, Carlton Downey, and Byron Boots. Initialization matters: Orthogonal predic-
tive state recurrent neural networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings .
OpenReview.net, 2018a. URL https://openreview.net/forum?id=HJJ23bW0b .
Krzysztof Choromanski, Mark Rowland, Tamás Sarlós, Vikas Sindhwani, Richard E. Turner, and
Adrian Weller. The geometry of random features. In International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary
Islands, Spain , volume 84 of Proceedings of Machine Learning Research , pp. 1–9. PMLR, 2018b.
URLhttp://proceedings.mlr.press/v84/choromanski18a.html .
Krzysztof Choromanski, Aldo Pacchiano, Jeffrey Pennington, and Yunhao Tang. KAMA-NNs:
Low-dimensional rotation based neural networks. In The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan ,
volume 89 of Proceedings of Machine Learning Research , pp. 236–245. PMLR, 2019a. URL
http://proceedings.mlr.press/v89/choromanski19a.html .
Krzysztof Choromanski, Mark Rowland, Wenyu Chen, and Adrian Weller. Unifying orthog-
onal Monte Carlo methods. In Proceedings of the 36th International Conference on Ma-
chine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of
Proceedings of Machine Learning Research , pp. 1203–1212. PMLR, 2019b. URL http:
//proceedings.mlr.press/v97/choromanski19a.html .
Krzysztof Marcin Choromanski, Mark Rowland, and Adrian Weller. The unreasonable effectiveness
of structured random orthogonal embeddings. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA , pp. 219–228, 2017.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2021
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In 4th International Conference on Learning Represen-
tations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings , 2016.
URLhttp://arxiv.org/abs/1511.07289 .
Qian Cong, Ivan Anishchenko, Sergey Ovchinnikov, and David Baker. Protein interaction networks
revealed by proteome coevolution. Science , 365(6449):185–189, 2019.
UniProt Consortium. Uniprot: a worldwide hub of protein knowledge. Nucleic acids research , 47
(D1):D506–D515, 2019.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to
Algorithms, 3rd Edition . MIT Press, 2009. ISBN 978-0-262-03384-8. URL http://mitpress.
mit.edu/books/introduction-algorithms .
Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V . Le, and Ruslan
Salakhutdinov. Transformer-XL: Language modeling with longer-term dependency, 2019. URL
https://openreview.net/forum?id=HJePno0cYm .
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
transformers. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. URL https://openreview.
net/forum?id=HyzdRiR9Y7 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR , abs/1810.04805, 2018. URL
http://arxiv.org/abs/1810.04805 .
Yilun Du, Joshua Meier, Jerry Ma, Rob Fergus, and Alexander Rives. Energy-based models for
atomic-resolution protein conformations. arXiv preprint arXiv:2004.13167 , 2020.
Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, and Burkhard Rost. End-to-end multitask
learning, from protein language to protein features without alignments. bioRxiv , pp. 864405, 2019.
Roy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via high-
level tracing. In Conference on Machine Learning and Systems 2018 , 2018. URL http://www.
sysml.cc/doc/2018/146.pdf .
Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention
network for scene segmentation. In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pp. 3146–3154, 2019.
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented
transformer for speech recognition, 2020.
Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam
Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music
transformer: Generating music with long-term structure. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net,
2019. URL https://openreview.net/forum?id=rJe4ShAcF7 .
John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-
based protein design. In Advances in Neural Information Processing Systems , pp. 15794–15805,
2019.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. CoRR , abs/2006.16236, 2020. URL
https://arxiv.org/abs/2006.16236 .
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id=
rkgNKkHtvB .
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2021
Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of
bert. arXiv preprint arXiv:1908.08593 , 2019.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. CoRR , abs/1808.06226, 2018. URL http:
//arxiv.org/abs/1808.06226 .
Richard E. Ladner and Michael J. Fischer. Parallel preﬁx computation. J. ACM , 27(4):831–838,
October 1980. ISSN 0004-5411. doi: 10.1145/322217.322232. URL https://doi.org/10.
1145/322217.322232 .
Han Lin, Haoxian Chen, Tianyi Zhang, Clément Laroche, and Krzysztof Choromanski. Demystifying
orthogonal Monte Carlo and beyond. CoRR , abs/2005.13590, 2020.
Haoneng Luo, Shiliang Zhang, Ming Lei, and Lei Xie. Simpliﬁed self-attention for transformer-based
end-to-end speech recognition. CoRR , abs/2005.10463, 2020. URL https://arxiv.org/
abs/2005.10463 .
Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,
Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. CoRR ,
abs/2004.03497, 2020. URL https://arxiv.org/abs/2004.03497 .
Jessica Marcandalli, Brooke Fiala, Sebastian Ols, Michela Perotti, Willem de van der Schueren, Joost
Snijder, Edgar Hodge, Mark Benhaim, Rashmi Ravichandran, Lauren Carter, et al. Induction of
potent neutralizing antibody responses by a designed protein nanoparticle vaccine for respiratory
syncytial virus. Cell, 176(6):1420–1431, 2019.
Nikita Nangia and Samuel R. Bowman. Listops: A diagnostic dataset for latent tree learning.
InProceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 2-4, 2018,
Student Research Workshop , pp. 92–99, 2018. doi: 10.18653/v1/n18-4013. URL https:
//doi.org/10.18653/v1/n18-4013 .
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,
and Dustin Tran. Image transformer. In Proceedings of the 35th International Conference
on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 ,
volume 80 of Proceedings of Machine Learning Research , pp. 4052–4061. PMLR, 2018. URL
http://proceedings.mlr.press/v80/parmar18a.html .
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Com-
pressive transformers for long-range sequence modelling. In International Conference on Learning
Representations , 2020. URL https://openreview.net/forum?id=SylKikSYDH .
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference
on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December
3-6, 2007 , pp. 1177–1184. Curran Associates, Inc., 2007. URL http://papers.nips.cc/
paper/3182-random-features-for-large-scale-kernel-machines .
Alexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C. Zitnick, Jerry Ma, and
Rob Fergus. Biological structure and function emerge from scaling unsupervised learning to 250
million protein sequences. bioArxiv , 04 2019. doi: 10.1101/622803.
Mark Rowland, Jiri Hron, Yunhao Tang, Krzysztof Choromanski, Tamás Sarlós, and Adrian Weller.
Orthogonal estimation of Wasserstein distances. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan , volume 89
ofProceedings of Machine Learning Research , pp. 186–195. PMLR, 2019. URL http://
proceedings.mlr.press/v89/rowland19a.html .
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse
attention with routing transformers. CoRR , abs/2003.05997, 2020. URL https://arxiv.
org/abs/2003.05997 .
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2021
Zhuoran Shen, Mingyuan Zhang, Shuai Yi, Junjie Yan, and Haiyu Zhao. Factorized attention:
Self-attention with linear complexities. CoRR , abs/1812.01243, 2018. URL http://arxiv.
org/abs/1812.01243 .
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for
deep learning in NLP. CoRR , abs/1906.02243, 2019. URL http://arxiv.org/abs/1906.
02243 .
Yujin Tang, Duong Nguyen, and David Ha. Neuroevolution of self-interpretable agents. CoRR ,
abs/2003.08165, 2020. URL https://arxiv.org/abs/2003.08165 .
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient
transformers. 2021.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Transformer dissection: An uniﬁed understanding for transformer’s attention via the
lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pp. 4335–4344, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems 30 , pp. 5998–6008. Curran Associates, Inc., 2017. URL http://papers.
nips.cc/paper/7181-attention-is-all-you-need.pdf .
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings .
OpenReview.net, 2018. URL https://openreview.net/forum?id=rJXMpikCZ .
Jesse Vig. A multiscale visualization of attention in the transformer model. arXiv preprint
arXiv:1906.05714 , 2019.
Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language
model. CoRR , abs/1906.04284, 2019. URL http://arxiv.org/abs/1906.04284 .
Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema
Rajani. Bertology meets biology: Interpreting attention in protein language models. CoRR ,
abs/2006.15222, 2020. URL https://arxiv.org/abs/2006.15222 .
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems 28: Annual Conference on Neural Information Processing Systems
2015, December 7-12, 2015, Montreal, Quebec, Canada , pp. 2692–2700, 2015.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. CoRR , abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.
04768 .
Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and Tongran Liu. Sharing attention weights for
fast transformer. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial
Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019 , pp. 5292–5298. ijcai.org, 2019. doi:
10.24963/ijcai.2019/735. URL https://doi.org/10.24963/ijcai.2019/735 .
Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and Eduard H. Hovy.
Hierarchical attention networks for document classiﬁcation. In NAACL HLT 2016, The 2016
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, San Diego California, USA, June 12-17, 2016 , pp. 1480–1489.
The Association for Computational Linguistics, 2016. doi: 10.18653/v1/n16-1174. URL https:
//doi.org/10.18653/v1/n16-1174 .
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2021
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G. Baraniuk,
Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Toward more efﬁcient training of
deep networks. In International Conference on Learning Representations , 2020. URL https:
//openreview.net/forum?id=BJxsrgStvr .
Felix X. Yu, Ananda Theertha Suresh, Krzysztof Marcin Choromanski, Daniel N. Holtmann-Rice,
and Sanjiv Kumar. Orthogonal random features. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain , pp. 1975–1983, 2016.
Vinícius Flores Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin,
Karl Tuyls, David P. Reichert, Timothy P. Lillicrap, Edward Lockhart, Murray Shanahan, Victoria
Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter W. Battaglia. Deep
reinforcement learning with relational inductive biases. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 , 2019.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV
2015, Santiago, Chile, December 7-13, 2015 , pp. 19–27, 2015. doi: 10.1109/ICCV .2015.11. URL
https://doi.org/10.1109/ICCV.2015.11 .
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2021
APPENDIX: R ETHINKING ATTENTION WITH PERFORMERS
A H YPERPARAMETERS FOR EXPERIMENTS
This optimal setting (including comparisons to approximate softmax) we use for the Performer
is speciﬁed in the Generalized Attention (Subsec. A.4), and unless speciﬁcally mentioned (e.g.
using name "Performer-SOFTMAX"), "Performer" refers to using this generalized attention
setting.
A.1 M ETRICS
We report the following evaluation metrics:
1.Accuracy : For unidirectional models, we measure the accuracy on next-token prediction,
averaged across all sequence positions in the dataset. For bidirectional models, we mask
each token with 15% probability (same as (Devlin et al., 2018)) and measure accuracy across
the masked positions.
2.Perplexity : For unidirectional models, we measure perplexity across all sequence positions
in the dataset. For bidirectional models, similar to the accuracy case, we measure perplexity
across the masked positions.
3.Bits Per Dimension/Character (BPD/BPC) : This calculated by loss divided by ln(2) .
We used the full evaluation dataset for TrEMBL in the plots in the main section, while for other
datasets such as ImageNet64 and PG-19 which have very large evaluation dataset sizes, we used
random batches (>2048 samples) for plotting curves.
A.1.1 PG-19 P REPROCESSING
The PG-19 dataset (Rae et al., 2020) is presented as a challenging long range text modeling task.
It consists of out-of-copyright Project Gutenberg books published before 1919. It does not have a
ﬁxed vocabulary size, instead opting for any tokenization which can model an arbitrary string of
text. We use a unigram SentencePiece vocabulary (Kudo & Richardson, 2018) with 32768 tokens,
which maintains whitespace and is completely invertible to the original book text. Perplexities
are calculated as the average log-likelihood per token, multiplied by the ratio of the sentencepiece
tokenization to number of tokens in the original dataset. The original dataset token count per split is:
train=1973136207, validation=3007061, test=6966499. Our sentencepiece tokenization yields the
following token counts per split: train=3084760726, valid=4656945, and test=10699704. This gives
log likelihood multipliers of train=1.5634, valid=1.5487, test=1.5359 per split before computing
perplexity, which is equal to exp( log likelihood multiplier loss).
Preprocessing for TrEMBL is extensively explained in Appendix C.
A.2 T RAINING HYPERPARAMETERS
Unless speciﬁcally stated, all Performer + Transformer runs by default used 0:5grad clip, 0:1weight
decay, 0:1dropout, 10 3ﬁxed learning rate with Adam hyperparameters (1= 0:9;2= 0:98;=
10 9), with batch size maximized (until TPU memory overload) for a speciﬁc model.
All 36-layer protein experiments used the same amount of compute (i.e. 16x16 TPU-v2, 8GB per
chip). For concatenated experiments, 16x16 TPU-v2’s were also used for the Performer, while 8x8’s
were used for the 1-3 layer (d= 256) Transformer models (using 16x16 did not make a difference in
accuracy).
Note that Performers are using the same training hyperparameters as Transformers, yet
achieving competitive results - this shows that FA VOR can act as a simple drop-in without needing
much tuning.
A.3 A PPROXIMATE SOFTMAX ATTENTION DEFAULT VALUES
The optimal values, set to default parameters1, are: renormalize_attention = True, numerical stabilizer
=10 6, number of features = 256, ortho_features = True, ortho_scaling = 0.0.
1https://github.com/google-research/google-research/blob/master/
performer/fast_attention
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2021
A.4 G ENERALIZED ATTENTION DEFAULT VALUES
The optimal values, set to default parameters2, are: renormalize_attention = True, numerical stabilizer
= 0.0, number of features = 256, kernel = ReLU, kernel_epsilon = 10 3.
A.5 R EFORMER DEFAULT VALUES
For the Reformer, we used the same hyperparameters as mentioned for protein experiments, without
gradient clipping, while using the defaults3(which instead use learning rate decay) for ImageNet-64.
In both cases, the Reformer used the same default LSH attention parameters.
A.6 L INFORMER DEFAULT VALUES
Using our standard pipeline as mentioned above, we replaced the attention function with the Linformer
variant via Jax, with = 10 6;k= 600 (same notation used in the paper (Wang et al., 2020)), where
is the exponent in a renormalization procedure using e as a multiplier in order to approximate
softmax, while kis the dimension of the projections of the QandKmatrices. As a sanity check,
we found that our Linformer implementation in Jax correctly approximated exact softmax’s output
within 0:02error for all entries.
Note that for rigorous comparisons, our Linformer hyperparameters are even stronger than the defaults
found in (Wang et al., 2020), as:
•We usek= 600 , which is more than twice than the default k= 256 from the paper, and
also twice than our default m= 256 number of features.
• We also use redrawing, which avoids "unlucky" projections on QandK.
2https://github.com/google-research/google-research/blob/master/
performer/fast_attention
3https://github.com/google/trax/blob/master/trax/supervised/configs/
reformer_imagenet64.gin
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2021
B M AINALGORITHM : FAVOR+
We outline the main algorithm for FA VOR+ formally:
Algorithm 1: FA VOR+ (bidirectional or unidirectional).
Input : Q;K;V2RLd,isBidirectional - binary ﬂag.
Result:dAtt$(Q;K;V)2RLLifisBidirectional ,dAtt!(Q;K;V)2RLLotherwise.
Compute Q0andK0as described in Section 2.2 and Section 2.3 and take C:= [V 1L];
ifisBidirectional then
Buf1:= (K0)>C2RM(d+1);Buf2:=Q0Buf12RL(d+1);
else
Compute Gand its preﬁx-sum tensor GPSaccording to (11);
Buf2:=GPS
1;:;:Q0
1:::GPS
L;:;:Q0
L>2RL(d+1);
end
[Buf3buf4] := Buf 2;Buf32RLd;buf42RL;
return diag(buf 4) 1Buf3;
B.1 U NIDIRECTIONAL CASE AND PREFIX SUMS
We explain how our analysis from Section 2.2 can be extended to the unidirectional mechanism
in this section. Notice that this time attention matrix Ais masked, i.e. all its entries not in the
lower-triangular part (which contains the diagonal) are zeroed (see also Fig. 8).
Figure 8: Visual representation of the preﬁx-sum algorithm for unidirectional attention. For clarity, we omit
attention normalization in this visualization. The algorithm keeps the preﬁx-sum which is a matrix obtained
by summing the outer products of random features corresponding to keys with value-vectors. At each given
iteration of the preﬁx-sum algorithm, a random feature vector corresponding to a query is multiplied by the most
recent preﬁx-sum (obtained by summing all outer-products corresponding to preceding tokens) to obtain a new
row of the matrix AV which is output by the attention mechanism.
For the unidirectional case, our analysis is similar as for the bidirectional case, but this time our goal is
to compute tril(Q0(K0)>)Cwithout constructing and storing the LL-sized matrix tril(Q0(K0)>)
explicitly, where C= [V1L]2RL(d+1). In order to do so, observe that 81iL:
[tril(Q0(K0)>)C]i=GPS
i;:;:Q0
i;GPS
i;:;:=iX
j=1Gj;:;:;Gj;:;:=K0
jC>
j2RM(d+1)(11)
where G;GPS2RLM(d+1)are 3d-tensors. Each slice GPS
:;l;pis therefore a result of a preﬁx-sum
(or cumulative-sum) operation applied to G:;l;p:GPS
i;l;p=Pi
j=1Gi;l;p. An efﬁcient algorithm to
compute the preﬁx-sum of Lelements takes O(L)total steps and O(logL)time when computed in
parallel (Ladner & Fischer, 1980; Cormen et al., 2009). See Algorithm 1 for the whole approach.
B.2 O RTHOGONAL RANDOM FEATURES - EXTENSIONS
As mentioned in the main text, for isotropic 
(true for most practical applications, including regular
attention), instead of sampling !iindependently, we can use orthogonal random features (ORF) (Yu
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2021
et al., 2016; Choromanski et al., 2017; 2018b): these maintain the marginal distributions of samples
!iwhile enforcing that different samples are orthogonal. If we need m>d , ORFs still can be used
locally within each ddblock of W(Yu et al., 2016).
ORFs were introduced to reduce the variance of Monte Carlo estimators (Yu et al., 2016; Choromanski
et al., 2017; 2018b; 2019a; Rowland et al., 2019; Choromanski et al., 2018a; 2019b) and we showed
in the theoretical and experimental sections from the main body that they do indeed lead to more
accurate approximations and substantially better downstream results. There exist several variants of
the ORF-mechanism and in the main body we discussed only the base one (that we refer to here as
regular ). Below we brieﬂy review the most efﬁcient ORF mechanisms (based on their strengths and
costs) to present the most complete picture.
(1) Regular ORFs [R-ORFs]: Applies Gaussian orthogonal matrices (Yu et al., 2016). Encodes
matrix Wof!-samples (with different rows corresponding to different samples) in O(md)space.
Provides algorithm for computing Wx inO(md)time for any x2Rd. Gives unbiased estimation.
Requires one-time O(md2)preprocessing (Gram-Schmidt orthogonalization).
(2) Hadamard/Givens ORFs [H/G-ORFs]: Applies random Hadamard (Choromanski et al., 2017)
or Givens matrices (Choromanski et al., 2019b). Encodes matrix WinO(m)orO(mlog(d))space.
Provides algorithm for computing Wx inO(mlog(d))time for any x2Rd. Gives small bias
(tending to 0withd!1 ).
B.3 T IME AND SPACE COMPLEXITY - DETAILED ANALYSIS
We see that a variant of bidirectional FA VOR+ using iid samples or R-ORFs has O(md+Ld+mL)
space complexity as opposed to (L2+Ld)space complexity of the baseline. Unidirectional
FA VOR+ using fast preﬁx-sum pre-computation in parallel (Ladner & Fischer, 1980; Cormen et al.,
2009) hasO(mLd)space complexity to store GPSwhich can be reduced to O(md+Ld+mL)by
running a simple (though non-parallel in L) aggregation of GPS
i;:;:without storing the whole tensor
GPSin memory. From Subsec. B.2, we know that if instead we use G-ORFs, then space complexity
is reduced to O(mlog(d) +Ld+mL)and if the H-ORFs mechanism is used, then space is further
reduced toO(m+Ld+mL) =O(Ld+mL). Thus form;dLall our variants provide substantial
space complexity improvements since they do not need to store the attention matrix explicitly.
The time complexity of Algorithm 1 is O(Lmd)(note that constructing Q0andK0can be done in
timeO(Lmd)). Note that the time complexity of our method is much lower than O(L2d)of the
baseline for Lm.
As explained in Subsec. B.2, the R-ORF mechanism incurs an extra one-time O(md2)cost (negligible
compared to the O(Lmd)term forLd). H-ORFs or G-ORFs do not have this cost, and when
FA VOR+ uses them, computing Q0andK0can be conducted in time O(Llog(m)d)as opposed
toO(Lmd)(see: Subsec. B.2). Thus even though H/G-ORFs do not change the asymptotic time
complexity, they improve the constant factor from the leading term. This might play an important
role in training very large models.
The number of random features mallows a trade-off between computational complexity and the level
of approximation: bigger mresults in higher computation costs, but also in a lower variance of the
estimate of A. In the theoretical section from the main body we showed that in practice we can take
M= (dlog(d)).
Observe that the FA VOR+ algorithm is highly-parallelizable, and beneﬁts from fast matrix multiplica-
tion and broadcasted operations on GPUs or TPUs.
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2021
C E XPERIMENTAL DETAILS FOR PROTEIN MODELING TASKS
C.1 T REMBL D ATASET
Dataset Set Name CountLength Statistics
Min Max Mean STD Median
TrEMBLTrain 104,863,744 2 74,488 353.09 311.16 289.00
Valid 102,400 7 11,274 353.62 307.42 289.00
Test 1,033,216 8 32,278 353.96 312.23 289.00
OOD 29,696 24 4,208 330.96 269.86 200.00
TrEMBL
(concat)Train 4,532,2248,192 8,192 8,192 0 8,192
Valid 4,096
Table 1: Statistics for the TrEMBL single sequence and the long sequence task.
We used the TrEMBL dataset4, which contains 139,394,261 sequences of which 106,030,080 are
unique. While the training dataset appears smaller than the one used in Madani et al. (Madani et al.,
2020), we argue that it includes most of the relevant sequences. Speciﬁcally, the TrEMBL dataset
consists of the subset of UniProtKB sequences that have been computationally analyzed but not
manually curated, and accounts for 99:5%of the total number of sequences in the UniProtKB
dataset5.
Following the methodology described in Madani et al. (Madani et al., 2020), we used both an
OOD-Test set, where a selected subset of Pfam families are held-out for valuation, and an IID
split, where the remaining protein sequences are split randomly into train, valid, and test tests. We
held-out the following protein families (PF18369, PF04680, PF17988, PF12325, PF03272, PF03938,
PF17724, PF10696, PF11968, PF04153, PF06173, PF12378, PF04420, PF10841, PF06917, PF03492,
PF06905, PF15340, PF17055, PF05318), which resulted in 29,696 OOD sequences. We note that,
due to deduplication and potential TrEMBL version mismatch, our OOD-Test set does not match
exactly the one in Madani et al. (Madani et al., 2020). We also note that this OOD-Test selection
methodology does not guarantee that the evaluation sequences are within a minimum distance from
the sequences used during training. In future work, we will include rigorous distance based splits.
The statistics for the resulting dataset splits are reported in Table 1. In the standard sequence modeling
task, given the length statistics that are reported in the table, we clip single sequences to maximum
lengthL= 1024 , which results in few sequences being truncated signiﬁcantly.
In the long sequence task, the training and validation sets are obtained by concatenating the sequences,
separated by an end-of-sequence token, and grouping the resulting chain into non-overlapping
sequences of length L= 8192 .
C.2 E MPIRICAL BASELINE
Figure 9: Visualization of the estimated empirical distribution for the 20 standard amino acids, colored by their
class. Note the consistency with the statistics on the TrEMBL web page.
A random baseline, with uniform probability across all the vocabulary tokens at every position, has
accuracy 5%(when including only the 20 standard amino acids) and 4%(when also including the
5 anomalous amino acids (Consortium, 2019)). However, the empirical frequencies of the various
4https://www.uniprot.org/statistics/TrEMBL
5https://www.uniprot.org/uniprot/
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2021
amino acids in our dataset may be far from uniform, so we also consider an empirical baseline where
the amino acid probabilities are proportional to their empirical frequencies in the training set.
Figure 9 shows the estimated empirical distribution. We use both the standard and anomalous
amino acids, and we crop sequences to length 1024 to match the data processing performed for the
Transformer models. The ﬁgure shows only the 20 standard amino acids, colored by their class, for
comparison with the visualization on the TrEMBL web page6.
C.3 T ABULAR RESULTS
Table 2 contains the results on the single protein sequence modeling task ( L= 1024 ). We report
accuracy and perplexity as deﬁned in Appendix A:
Model Type Set Name Model Accuracy Perplexity
UNITestEmpirical Baseline 9.92 17.80
Transformer 30.80 9.37
Performer (generalized) 31.58 9.17
OODEmpirical Baseline 9.07 17.93
Transformer 19.70 13.20
Performer (generalized) 18.44 13.63
BIDTestTransformer 33.32 9.22
Performer (generalized) 36.09 8.36
Performer (softmax) 33.00 9.24
OODTransformer 25.07 12.09
Performer (generalized) 24.10 12.26
Performer (softmax) 23.48 12.41
Table 2: Results on single protein sequence modeling ( L= 1024 ). We note that the empirical
baseline results are applicable to both the unidirectional (UNI) and bidirectional (BID) models.
C.4 A TTENTION MATRIX ILLUSTRATION
In this section we illustrate the attention matrices produced by a Performer model. We focus on the
bidirectional case and choose one Performer model trained on the standard single-sequence TrEMBL
task for over 500K steps. The same analysis can be applied to unidirectional Performers as well.
We note that while the Transformer model instantiates the attention matrix in order to compute the
attention output that incorporates the (queries Q, keysK, valuesV) triplet (see Eq. 1 in the main
paper), the FA VOR mechanism returns the attention output directly (see Algorithm 1). To account for
this discrepancy, we extract the attention matrices by applying each attention mechanism twice: once
on each original (Q;K;V )triple to obtain the attention output, and once on a modiﬁed (Q;K;V)
triple, where Vcontains one-hot indicators for each position index, to obtain the attention matrix.
The choice of Vensures that the dimension of the attention output is equal to the sequence length,
and that a non-zero output on a dimension ican only arise from a non-zero attention weight to the ith
sequence position. Indeed, in the Transformer case, when comparing the output of this procedure
with the instantiated attention matrix, the outputs match.
Attention matrix example. We start by visualizing the attention matrix for an individual protein
sequence. We use the BPT1_BOVIN protein sequence7, one of the most extensively studied globular
proteins, which contains 100 amino acids. In Figure 10, we show the attention matrices for the ﬁrst
4 layers. Note that many heads show a diagonal pattern, where each node attends to its neighbors,
and some heads show a vertical pattern, where each head attends to the same ﬁxed positions. These
patterns are consistent with the patterns found in Transformer models trained on natural language
6https://www.uniprot.org/statistics/TrEMBL
7https://www.uniprot.org/uniprot/P00974
20

--- PAGE 21 ---
Published as a conference paper at ICLR 2021
(Kovaleva et al., 2019). In Figure 12 we highlight these attention patterns by focusing on the ﬁrst 25
tokens, and in Figure 11, we illustrate in more detail two attention heads.
Amino acid similarity. Furthermore, we analyze the amino-acid similarity matrix estimated from
the attention matrices produced by the Performer model, as described in Vig et al. (Vig et al., 2020).
We aggregate the attention matrix across 800 sequences. The resulting similarity matrix is illustrated
in Figure 13. Note that the Performer recognizes highly similar amino acid pairs such as (D, E) and
(F, Y).
Figure 10: We show the attention matrices for the ﬁrst 4 layers and all 8 heads (each row is a layer, each column
is head index, each cell contains the attention matrix across the entire BPT1_BOVIN protein sequence). Note
that many heads show a diagonal pattern, where each node attends to its neighbors, and some heads show a
vertical pattern, where each head attends to the same ﬁxed positions.
Figure 11: We illustrate in more detail two attention heads. The sub-ﬁgures correspond respectively to: (1)
Head 1-2 (second layer, third head), (2)Head 4-1 (ﬁfth layer, second head). Note the block attention in Head 1-2
and the vertical attention (to the start token (‘M’) and the 85th token (‘C’)) in Head 4-1.
21

--- PAGE 22 ---
Published as a conference paper at ICLR 2021
Figure 12: We highlight the attention patterns by restricting our attention to the ﬁrst 25 tokens (note that we do
not renormalize the attention to these tokens). The illustration is based on Vig et al. (Vig, 2019; Vig & Belinkov,
2019). Note that, similar to prior work on protein Transformers (Madani et al., 2020), the attention matrices
include both local and global patterns.
ACDEFGHIKLMNPQRSTVWYA
C
D
E
F
G
H
I
K
L
M
N
P
Q
R
S
T
V
W
Y
0.00.20.40.60.81.0
ACDEFGHIKLMNPQRSTVWYA
C
D
E
F
G
H
I
K
L
M
N
P
Q
R
S
T
V
W
Y
0.00.20.40.60.8
Figure 13: Amino acid similarity matrix estimated from attention matrices aggregated across a small subset
of sequences, as described in Vig et al. (Vig et al., 2020). The sub-ﬁgures correspond respectively to: (1)the
normalized BLOSUM matrix, (2)the amino acid similarity estimated via a trained Performer model. Note that
the Performer recognizes highly similar amino acid pairs such as (D, E) and (F, Y).
22

--- PAGE 23 ---
Published as a conference paper at ICLR 2021
D E XTENDED APPROXIMATION AND COMPARISON RESULTS
D.1 B ACKWARDS COMPATIBILITY - ERROR PROPAGATION
Although mentioned previously (Sec. 4.2) that the Performer with additional ﬁnetuning is backwards
compatible with the Transformer, we demonstrate below in Fig. 14 that error propagation due to non-
attention components of the Transformer is one of the primary reasons that pretrained Transformer
weights cannot be immediately used for inference on the corresponding Performer.
Figure 14: Output approximation errors between a vanilla Transformer and a Performer (with
orthogonal features) for varying numbers of layers.
D.2 A PPROXIMATE SOFTMAX - EXTENDED PROPERTIES
We show the following properties of our softmax approximation, in Fig. 15:
Redrawing: While the beneﬁts of redrawing features was shown in Subsec. 4.3 of the main body of
the paper, we also demonstrate its beneﬁts when there are multiple layers with large scale (16x16
TPU-v2) training.
Unidirectional: While we have shown on TrEMBL that Performer with generalized ReLU attention
outperforms softmax, we also show that approximate softmax attention can still be a solid choice, for
example on ImageNet64 (U). After 100K steps of training, the Performer-ReLU, Performer-Softmax,
and Performer-Softmax (SMREG) variants achieve respectively, 3.67, 3.69, 3.67 BPD.
Instability of Trigonometric Features: We see the full view of the unstable training curve when
using Trigonometric softmax.
Figure 15: Best viewed zoomed in. Left: The importance of redrawing features. If redrawing is not
used, an "unlucky" set of random features may cause training degradation, shown by the early-stopped
curve with Seed 1, while a ‘lucky’ set of random features may cause no issue, shown by the curve
with Seed 2. Redrawing allows the training to correct itself, as seen at the black vertical line. Middle:
Using the same 8x8 TPU-v2 compute and same 6-layer standard model, approximate softmax with
positive features achieves the same result as generalized ReLU attention. Right: Zoomed out view of
right subﬁgure of Fig. 5, showing that Trigonometric softmax causes very unstable training behaviors.
D.3 G ENERALIZED ATTENTION
We investigated Generalized Attention mechanisms (mentioned in Sec. 2.2) on TrEMBL when
L= 512 for various kernel functions. This is similar to (Tsai et al., 2019) which also experiments
with various attention kernels for natural language. Using hyperparameter sweeps across multiple
23

--- PAGE 24 ---
Published as a conference paper at ICLR 2021
variables in FA VOR, we compared several kernels and also renormalization on/off (Fig. 16 and
Fig. 17), where Renormalize corresponds to applying D 1operator in attention, as for the standard
mechanism, though we noticed that disabling it does not necessarily hurt accuracy) to produce the
best training conﬁguration for the Performer. We note that the effective batch size slightly affects
the rankings (as shown by the difference between 2x2 and 4x4 TPU runs) - we by default use the
generalized ReLU kernel with other default hyperparameters shown in Appendix A, as we observed
that they are empirically optimal for large batch size runs (i.e. 8x8 or 16x16 TPU’s).
Figure 16: To emphasize the highest accuracy runs but also show the NaN issues with certain kernels
which caused runs to stop early, we set both x and y axes to be log-scale. We tested kernels deﬁned
by different functions f(see: Sec. 2.2): sigmoid, exponential, ReLU, absolute, gelu, cosine (original
softmax approximation), tanh, and identity. All training runs were performed on 2x2 TPU-v2’s, 128
batch size per device.
Figure 17: We also performed a similar setup as Fig. 16 for 4x4 TPU-v2’s.
D.4 C OMPARISON WITH LINEAR TRANSFORMER
We use the attention implementation of the Linear Transformer from (Katharopoulos et al., 2020),
which mainly involves setting our feature map (x) =elu(x) + 1 , where elu(x)is the shifted-eLU
function from (Clevert et al., 2016).
Figure 18: Left: In the unidirectional 36-ProGen setting, we ran 3 seeds of the Linear Transformer,
and found that all 3 seeds produced exploding gradients very early on, stopping the training run.
Right: The Linear Transformer in the bidirectional setting also produced an exploding gradient in
the middle of training, near 125K steps. Exploding gradients can be evidenced by the sharp drop in
train accuracy right before a NaN error.
For the sake of fairness and to prevent confounding results, while (Katharopoulos et al., 2020) also
uses the GeLU nonlinearity for the MLPs in the Linear Transformer, we instead use the original
ReLU nonlinearity. We also used the exact same training hyperparameters as Performer-ReLU on
24

--- PAGE 25 ---
Published as a conference paper at ICLR 2021
our exact ProGen setting from Fig. 6. Ultimately, we empirically found that the Linear Transformer
possessed numerical instability during training via unstable training curves, ultimately stopping
training by producing exploding gradients (NaNs) (Fig. 18).
D.5 L ONG RANGE ARENA
Performers are compared against many additional (scalable and not scalable) methods not included
in our paper: Local Attention ,Sparse Attention ,Longformer ,Sinkhorn Transformer ,Synthesizer ,
Big Bird and the aforementioned Linear Transformer on challenging long range context tasks in the
Long Range Arena (Tay et al., 2021), with Fig. 19 displaying the original paper’s results. Performers
obtain the largest LRA (Long Range Arena) score among all tested scalable Transformers methods
(which we deﬁne by having speed of > 100 examples/sec).
Tasks used for comparison include: (1)a longer variation of the standard ListOps task proposed in
(Nangia & Bowman, 2018), (2)byte-level text classiﬁcation using real-world data, (3)byte-level
document retrieval, (4)image classiﬁcation on sequences of pixels, and (5)Pathﬁnder task (long-
range spatial dependency problem). In the Long Range Arena paper, the authors found that all models
do not learn anything on Path-X task (denoted by FAIL), contrary to the Pathﬁnder task, which shows
that increasing the sequence length can cause seriously difﬁculties for model training.
Figure 19: Upper Table: Results on Long-Range Arena benchmark. Best model is in boldface and
second best is underlined. Lower Table: Benchmark results of all X-former models with a consistent
batch size of 32 across all models. The authors report relative speed increase/decrease in comparison
with the vanilla Transformer in brackets besides the steps per second. Memory usage refers to per
device memory usage across each TPU device. Benchmarks are run on 4x4 TPU-v3 chips. Right
Fig: Performance (y-axis), speed (x-axis), and memory footprint (size of the circles) of different
models.
25

--- PAGE 26 ---
Published as a conference paper at ICLR 2021
E C OMPUTATION COSTS - EXTENDED RESULTS
In this subsection, we empirically measure computational costs in terms wall clock time on forward
and backward passes for three scenarios in Fig. 20:
1.Performer, with varying number of layers. We show that our method can scale up to (but not
necessarily limited to) even 20 layers.
2.Attention time complexities when comparing standard attention (from Transformer) and
FA VOR (from Performer). Note that the maximum memory size here is not reﬂective of
the maximum memory size in an actual model (shown below), as this benchmark requires
computing explicit tensors (causing memory increases) in Jax, while a model does not.
3.Time complexities when comparing the Transformer and Performer models. "X" (OPT)
denotes the maximum possible speedup achievable, when attention simply returns the V-
vector, showing that the Performer is nearly optimal. We see that the maximum possible
power of 2 length allowed on a V100 GPU (16GB) is 215= 32768 using regular dimensions.
Since some of the computational bottleneck in the Transformer may originate from the ex-
tra feed-forward layers (Kitaev et al., 2020), we also benchmark the “Small" version, i.e.
(nheads;nlayers;dff;d) = (1;6;64;64)as well, when the attention component is the dominant
source of computation and memory. We remind the reader that the “Regular" version consists of
(nheads;nlayers;dff;d) = (8;6;2048;512) .
Figure 20: Captions (1) and (2) for each 2x2 subﬁgure mentioned above.
26

--- PAGE 27 ---
Published as a conference paper at ICLR 2021
Figure 21: Caption (3) for this 2x2 subﬁgure mentioned above.
27

--- PAGE 28 ---
Published as a conference paper at ICLR 2021
F T HEORETICAL RESULTS
We provide here the proofs of all theoretical results presented in the paper.
F.1 P ROOF OF LEMMA 1
Proof. We ﬁrst deduce that for any a;b2Rd
SM(x;y) = exp( x>y) = exp( kxk2=2)exp(kx+yk2=2)exp( kyk2=2):
Next, let w2Rd. We use the fact that
(2) d=2Z
exp( kw ck2
2=2)dw= 1
for any c2Rdand derive:
exp(kx+yk2=2) = (2) d=2exp(kx+yk2=2)Z
exp( kw (x+y)k2=2)dw
= (2) d=2Z
exp( kwk2=2 +w>(x+y) kx+yk2=2 +kx+yk2=2)dw
= (2) d=2Z
exp( kwk2=2 +w>(x+y))dw
= (2) d=2Z
exp( kwk2=2)exp(w>x)exp(w>y)dw
=E!N(0d;Id)[exp(!>x)exp(!>y)]:
That completes the proof of the ﬁrst part of the lemma. An identity involving hyperbolic cosine
function is implied by the fact that for every u2Rdand!N(0;Id)the following is true:
E[exp(!>u)] =1X
i=0E[(!>u)2i]
(2i)!=1
21X
i=0E[(!>u)2i] +E[( !>u)2i]
(2i)!: (12)
The cancellation of the odd moments E[(!>u)2i+1]follows directly from the fact that !is taken
from the isotropic distribution (i.e. distribution with pdf function constant on each sphere). That
completes the proof.
F.2 P ROOF OF LEMMA 2
Proof. Denote: z=x+yand =x y. Note that by using standard trigonometric identities (and
the fact that the variance of the sum of independent random variables is the sum of variances of those
random variables), we can get the following for !N(0;Id):
MSE(dSMtrig
m(x;y)) =1
mexp(kxk2+kyk2)Var(cos(!>)): (13)
Using the fact that (see: Lemma 1 in (Yu et al., 2016); note that in that lemma they use notation: z
for what we denote as: kk):
Var(cos(!>)) =1
2(1 exp( kk2))2; (14)
we obtain:
MSE(dSMtrig
m(x;y)) =1
2mexp(kxk2+kyk2)(1 exp( kk2))2=
1
2mexp(kzk2)SM 2(x;y)(1 exp( kk2))2;(15)
which completes the ﬁrst part of the proof. To obtain the formula for: MSE(dSM+
m(x;y))notice ﬁrst
that:
E!N(0;Id)[exp(!>z)] = exp(kzk2
2): (16)
28

--- PAGE 29 ---
Published as a conference paper at ICLR 2021
The above immediately follows from the fact that positive random feature maps provide unbiased
estimation of the softmax-kernel, thus the following is true:
SM(x;y) = exp( kxk2+kyk2
2)E!N(0;Id)[exp(!>z)]: (17)
Therefore we obtain:
MSE(dSM+
m(x;y)) =1
mexp( (kxk2+kyk2))Var(exp(!>z)) =
1
mexp( (kxk2+kyk2)) 
E[exp(2!>z)] (E[exp(!>z)])2
=
1
mexp( (kxk2+kyk2))(exp(2kzk2) exp(kzk2));(18)
where the last equality follows from Equation 16. Therefore we have:
MSE(dSM+
m(x;y)) =1
mexp( (kxk2+kyk2)) exp(kzk2)(exp(kzk2) 1) =
1
mexp(kzk2)SM2(x;y)(1 exp( kzk2)):(19)
Finally,
MSE(dSMhyp+
m(x;y)) =1
4mexp( kxk2+kyk2
2)2(Var(exp(!>z)) + Var(exp( !>z))+
2Cov(exp(!>z));exp( !>z)))) =1
4mexp( kxk2+kyk2
2)2(2Var(exp(!>z))+
2Cov(exp(!>z));exp( !>z))))) =1
2mexp( (kxk2+kyk2))
(Var(exp(!>z)) + 1 (E[exp(!>z)])2) =1
2mexp( (kxk2+kyk2))
(exp(2kzk2) exp(kzk2) + 1 exp(kzk2)) =1
2mexp( (kxk2+kyk2))(exp(kzk2) 1)2
=1
2(1 exp( kzk2))MSE(dSM+
m(x;y)):
(20)
In the chain of equalities above we used the fact that random variables exp(!>z)andexp( !>z)
have the same distribution. This is true since !and !have the same distribution ( !is Gaussian).
That completes the proof.
F.3 P ROOF OF THEOREM 1
Proof. Letx;y2Rdbe respectively a query/key. Note that from the deﬁnition of SMREG( x;y)
we have for z=x+y:
SMREG( x;y) = exp( kxk2+kyk2
2)1X
k=01
(2k)!kzk2kdkE!N(0;Id)[(!
k!k2e1)2k]; (21)
where e1def= (1;0;:::;0)>2Rd. To obtain the above we used the fact that N(0;Id)is isotropic (that
in particular implies zeroing of the even terms in the Taylor expansion).
Let us denote: A(k;d)def=E!N(0;Id)[(!
k!k2e1)2k]. It turns out that:
A(2k;d) =(2k 1)!!
(d+ 2k 2)(d+ 2k 4):::d: (22)
The proof of that fact can be found in the supplement of (Choromanski et al., 2018b), yet we provide
it below for completeness and the convenience of the Reader:
29

--- PAGE 30 ---
Published as a conference paper at ICLR 2021
Lemma 3. ExpressionA(2k;d)satisﬁes the following for k2N:
A(2k;d) =(2k 1)!!
(d+ 2k 2)(d+ 2k 4):::d: (23)
Proof. Note ﬁrst that for d2the density function pd()of the angle between a vector r2Rd
chosen uniformly at random from the unit sphere and e1is given by the following formula:
pd() =sind 2()R
0sind 2()d: (24)
Let us denote: F(k;d)def=R
0cosk() sind()d. Using partial integration, we get:
Z
0cosk() sind()d=Z
0cosk 1() sind()(sin())0d=
cosk 1() sind+1()j
0 Z
0sin()((k 1) cosk 2()( sin()) sind()+
dcosk() sind 1())d:(25)
Thus we conclude that: F(k;d) =k 1
d+1F(k 2;d+ 2) . Therefore we have:
F(2k;d) =(2k 1)!!
(d+ 1)(d+ 3):::(d+ 2k 1)Z
0sind+2k()d: (26)
We again conduct partial integration and get:
Z
0sind()d= 1
dsind 1() cos()j
0+
d 1
dZ
0sind 2()d=d 1
dZ
0sind 2()d:(27)
Therefore we conclude that:
A(2k;d) =1
d 3
d 2d 5
d 4:::(2k 1)!!
(d 1)(d+ 1):::(d+ 2k 3)d+ 2k 3
d+ 2k 2d+ 2k 5
d+ 2k 4::::=
(2k 1)!!
(d+ 2k 2)(d+ 2k 4):::d;(28)
which completes the proof.
Applying the above lemma, we get:
SMREG( x;y) = exp( kxk2+kyk2
2)1X
k=01
(2k)!kzk2kdk (2k 1)!!
(d+ 2k 2)(d+ 2k 4):::d
= exp( kxk2+kyk2
2)1X
k=0wk
k!f(k;d);
(29)
wherew=kzk2
2andf(k;d) =dk
(d+2k 2)(d+2k 4):::d.
Thus we obtain:
SMREG( x;y)
SM(x;y)=e w1X
k=0wk
k!f(k;d): (30)
Note ﬁrst that for k1we have:f(k;d)1, thus:
SMREG( x;y)SM(x;y): (31)
30

--- PAGE 31 ---
Published as a conference paper at ICLR 2021
We also have for l=d1
3:
SMREG( x;y)
SM(x;y)=e wlX
k=0wk
k!f(k;d) +e w1X
k=l+1wk
k!f(k;d)
f(l;d)e wlX
k=0wk
k!+e w1X
k=l+1wk
k!f(k;d)f(l;d)(1 e w1X
k=l+1wk
k!) =
f(l;d)(1 P[Po(w)>l]);(32)
where Po(w)stands for the random variable of Poisson distribution with parameter w. Therefore we
get fort= ln(l
w):
SMREG( x;y)
SM(x;y)(1 2l 2
d)l(1 P[Po(w)>l])
exp(lln(1 2l 2
d))(1 P[tPo(w)tl]) =
exp 
l1X
i=1( 1)i(2l 2
d)i
i!
(1 P[exp(tPo(w) tl)1])
exp( 2
d1
3+o(1
d1
3))(1 exp( tl)E[exp(tPo(w))]) =
exp( 2
d1
3+o(1
d1
3))(1 exp( w l(t 1)));(33)
where the last equality is implied by the formula for the Laplace Transform for the Poisson random
variable:
E[exp(tPo(w))] = exp(w(exp(t) 1)): (34)
Notice that: w=kzk2
2=ln(SM( x;x))+ln(SM( y;y))+2 ln(SM( x;y))
22 ln(C). We conclude that:
SMREG( x;y)
SM(x;y)(1 2
d1
3+o(1
d1
3))(1 C 2(d1
3
2eln(C)) d1
3) = 1 2
d1
3+o(1
d1
3):(35)
That completes the proof.
F.4 P ROOFS OF THEOREM 2,T HEOREM 3 & B EAUTIFUL FUNCTIONS
We will provide here much more general theoretical results which will imply Theorem 3 and Theorem
2. We need the following deﬁnition:
Deﬁnition 1. We say that function F:Rn!Ris beautiful if Fcan be expressed as:
F
;g(z) =E!
[g(!>z)]; (36)
for a probabilistic isotropic distribution 
, and where g:R!Ris an entire function with non-
negative power-series coefﬁcients (i.e. g(x) =P1
i=0aixifor everyx2Rand withai0for
i= 0;1;:::). In the formula above we assume that the expectation on the RHS exists.
Interestingly, beautiful functions can be used to deﬁne softmax and consequently, Gaussian kernels
(both standard and regularized), leading to our PRF mechanism presented in the main body of the
paper, as we explain below.
Remark 1. If one takes 
 =N(0;Id)(note thatN(0;Id)is isotropic) and g:x!exp(x)(suchg
is clearly entire with nonnegative power-series coefﬁcient) then the following is true for z=x+y:
SM(x;y) = exp( kxk2+kyk2
2)F
;g(z): (37)
Similarly: SMREG( x;y) = exp( kxk2+kyk2
2)F
reg;g(z), where 
regstands for the distribution
corresponding to Haar measure on the sphere of radiusp
d(which is clearly isotropic). Therefore
general concentration results for Monte Carlo estimators of beautiful functions immediately imply
corresponding results for the (standard and regularized) softmax (and thus also Gaussian) kernel.
31

--- PAGE 32 ---
Published as a conference paper at ICLR 2021
We will consider two estimators of the beautiful functions from Deﬁnition 1 that directly lead
(through Remark 1) to: PRF-based approximation of the softmax-kernel and its enhanced version
with orthogonal features. Standard Monte Carlo estimator samples independently !iid
1;:::;!iid
miid
,
wheremstands for the number of samples and then computes:
bFiid
m(z)def=1
mmX
i=1g((!iid
i)>z): (38)
Orthogonal Monte Carlo estimator samples !ort
1;:::;!ort
m(md) in such a way that marginally we
have:!ort
i
, but(!ort
i)>!ort
j= 0fori6=j(such an orthogonal ensemble can be always created
if
is isotropic, as we already mentioned in the main body of the paper). We deﬁne:
bFort
m(z)def=1
mmX
i=1g((!ort
i)>z): (39)
F.4.1 O RTHOGONALITY UNIVERSALLY IMPROVES CONCENTRATION
Denote byMZ() =E[eZ]a moment generating function of the random variable Z. Note ﬁrst
that estimators of beautiful functions based on standard Monte Carlo procedure using independent
vectors!iid
iguarantee strong concentration bounds since independent !is provide a way to obtain
exponentially small upper bounds on failure probabilities through moment generating functions. We
summarize this classic observation which is a standard application of Markov’s Inequality below.
Lemma 4. Consider an estimator bFiid
m(z)of the beautiful function Fevaluated at z. Then the
following holds for any a>F (z),>0:
P[bFiid
m(z)>a]exp(ma)MX()m; (40)
whereX=g(w>z),wD.
The above result provides us with exponentially small (in Legendre Transform) upper bounds on tail
probabilities for the standard estimator. Below we provide our two main theoretical results.
Theorem 5 (orthogonality provides smaller tails) .IfF
;gis a beautiful function then the following
holds formd,Xas in Lemma 4 and any a>F (z),>0:
P[bFort
m(z))>a]exp( ma)
MX()m 4m(m 1)
4d2(d+ 2)aM 2
0a2
1kzk4(Ek!k2)2
: (41)
This result shows that features obtained from the ensembles of pairwise orthogonal random vectors
provide exponentially small bounds on tail probabilities and that these bounds are strictly better than
for estimators using unstructured features. Furthermore, the result is universal , i.e. holds for any
dimensionality d, not just asymptotically for dlarge enough.
We also obtain similar result regarding mean squared errors (MSEs) of the considered estimators:
Theorem 6. IfF
;gis a beautiful function then the following holds for md:
MSE(bFort
m(z))MSE(bFiid
m(z)) (1 1
m)2
d+ 2(F
;g(z) a0)2: (42)
As before, an orthogonal estimator leads to better concentration results and as before, this is the case
for anyd>0, not only asymptotically for large enough d.
Note that from what we have said above, Theorem 2 and Theorem 3 follow immediately from
Theorem 6 and Theorem 5 respectively.
Thus in the remainder of this section we will prove Theorem 6 and Theorem 5.
F.4.2 P ROOF OF THEOREM 5
Proof. Note that by the analogous application of Markov’s Inequality as in Lemma 4, we get:
P[bFort
m(z))>a]E[e(Xort
1+:::+Xort
m)]
ema; (43)
32

--- PAGE 33 ---
Published as a conference paper at ICLR 2021
where we have: Xort
i=g((!ort
i)>z). We see that it sufﬁces to show that for any >0the following
holds:E[e(Xort
1+:::+Xort
m)]<E[e(Xiid
1+:::+Xiid
m)]. We have:
E[e(Xort
1+:::+Xort
m)] =E[1X
j=0(Pm
i=1Xort
i)j
j!] =E[1X
j=0j
j!(mX
i=1Xort
i)j] =
1X
j=0j
j!E[(mX
i=1Xort
i)j] =1X
j=0j
j!E[X
(j1;:::;j m)2Sjj
j1;:::;jm
(Xort
1)j1:::(Xort
m)jm];(44)
whereSj=f(j1;:::;jm)2N:::N:j1;:::;jm0;j1+:::+jm=jg.
Thus we have:
E[e(Xort
1+:::+Xort
m)] =1X
j=0j
j!X
(j1;:::;j m)2Sjj
j1;:::;jm
E[(Xort
1)j1:::(Xort
m)jm]: (45)
Similarly, we get:
E[e(Xiid
1+:::+Xiid
m)] =1X
j=0j
j!X
(j1;:::;j m)2Sjj
j1;:::;jm
E[(Xiid
1)j1:::(Xiid
m)jm]: (46)
Therefore we get:
 =E[e(Xiid
1+:::+Xiid
m)] E[e(Xort
1+:::+Xort
m)]
=1X
j=0j
j!X
(j1;:::;j m)2Sjj
j1;:::;jm 
E[(Xiid
1)j1:::(Xiid
m)jm] E[(Xort
1)j1:::(Xort
m)jm]
(47)
Note ﬁrst that using the fact that fis entire, we can rewrite each Xort
ias:
Xort
i=1X
s=0as((!ort
i)>z)s; (48)
wheref(x) =P1
s=0asxsanda0;a1;:::0. Similarly,
Xiid
i=1X
s=0as((!iid
i)>z)s: (49)
By plugging in the above formulae for Xort
iandXiid
iint the formula for and expanding power-
expressions, we obtain:
 =1X
j=0j
j!X
(j1;:::;j m)2Sjj
j1;:::;jmX
(d1;:::;d m)2D(j1;:::;j m)bcj1;:::;j m(d1;:::;dm)b(d1;:::;dm);
(50)
for some ordered subsets of indices (with potentially repeating entries) D(j1;:::;jm)and some
nonnegativebcj1;:::;j m(d1;:::;dm)(exact formula for those can be given but we do not need it to
complete the proof and since it is technical, it would unnecessarily complicate the proof so we skip
it) andb(d1;:::;dm)deﬁned as:
b(d1;:::;dm) =E[((!iid
1)>z)d1:::((!iid
m)>z)dm] E[((!ort
1)>z)d1:::((!ort
m)>z)dm]:(51)
Our next goal is to re-write the formula for b(d1;:::;dm). Denote:
Y= ((!ort
1)>z)d1:::((!ort
m)>z)dm: (52)
33

--- PAGE 34 ---
Published as a conference paper at ICLR 2021
Observe that Yhas the same distribution as Y0deﬁned as:
Y0= (e>
1g
kgk2kzk2)d1:::(e>
mg
kgk2kzk2)dm(k!ort
1k2)d1:::(k!ort
mk2)dm; (53)
where gis a Gaussian vector taken from the N(0;Id)distribution, independently from:
k!ort
1k2;:::;k!ort
mk2.
This comes from the fact that for a ﬁxed zone can think about the set:!ort
1
k!ort
1k2;:::;!ort
m
k!ortmk2as a
random rotation of the system of mcanonical basis vectors: e1;:::;em. Thus instead of applying
a random rotation to: e1;:::;em, one can equivalently randomly rotate vector z. Randomly rotated
vector zhas the same distribution as:g
kgk2kzk2.
Now note that lengths of vectors !ort
1;:::;!ort
mare chosen independently.
Therefore we obtain:
E[((!ort
1)>z)d1:::((!ort
m)>z)dm] =
E[(k!ort
1k2)d1]:::E[(k!ort
mk2)dm]E[(e>
1v)d1:::(e>
mv)dm]kzkd1+:::+dm
2;(54)
where vg
kgk2.
Denote g= (g1;:::;gd)>. Thus we obtain:
E[((!ort
1)>z)d1:::((!ort
m)>z)dm] =
E[(k!ort
1k2)d1]:::E[(k!ort
mk2)dm]kzkd1+:::+dm
2 E[gd1:::
1gdmmp
g2
1+:::+g2
dd1+:::+dm](55)
Now let us focus on the second expression from the formula on b(d1;:::;dm). We have:
E[((!iid
1)>z)d1:::((!iid
m)>z)dm] =mY
i=1E[((!iid
i)>z)di] =
E[(k!iid
1k2)d1]:::E[(k!iid
mk2)dm]kzkd1+:::+dm
2mY
i=1E[gdi
ip
g2
1+:::+g2
ddi];(56)
where the ﬁrst equality comes from the fact that different !iid
is are independent and the second one is
implied by the analogous analysis to the one conducted above.
We will need the following lemma:
Lemma 5. For everys2N+such thatsnand everyk1;:::;ks2N+the following holds:
E[gk1
1:::gkssp
g2
1+:::+g2
dk1+:::+ks] =Qs
i=1E[gki
i]
E[p
g2
1+:::+g2
dk1+:::+ks]: (57)
Proof. Taker=g
kgk2k~gk2, where ~gis an independent copy of g. Note that rg. We have:
E[rk1
1]:::E[rks
s] =E[rk1
1:::rks
s] =E[gk1
1:::gkssp
g2
1+:::+g2
dk1+:::+ks]E[k~gkk1+:::+ks
2 ];(58)
where the ﬁrst equality comes from the independence of different elements of z= (z1;:::;zn)>and
the second equality is implied by the fact that ~gis independent from g.
Therefore we have:
E[gk1
1:::gkssp
g2
1+:::+g2
dk1+:::+ks] =E[rk1
1]:::E[rkss]
E[k~gkk1+:::+ks
2 ]: (59)
That completes the proof since zgand~gg.
34

--- PAGE 35 ---
Published as a conference paper at ICLR 2021
Note that by Lemma 5, we can rewrite the right expression from the formula on b(d1;:::;dm)as:
E[(k!ort
1k2)d1]:::E[(k!ort
mk2)dm]kzkd1+:::+dm
2Qm
i=1E[gdi
i]
E[p
g2
1+:::+g2
dd1+:::+dm]: (60)
The left expression from the formula on b(d1;:::;dm)can be rewritten as:
L(d1;:::;dm) =E[(k!iid
1k2)d1]:::E[(k!iid
mk2)dm]kzkd1+:::+dm
2Qm
i=1E[gdi
i]
E[p
g2
1+:::+g2
dd1]:::E[p
g2
1+:::+g2
ddm]:(61)
Since marginal distributions of !ort
iand!iid
iare the same, we can rewrite b(d1;:::;dn)as:
b(d1;:::;dm) =L(d1;:::;dm)(1 (d1;:::;dm)); (62)
where(d1;:::;dm)is deﬁned as:
(d1;:::;dm) =E[p
g2
1+:::+g2
dd1]:::E[p
g2
1+:::+g2
ddm]
E[p
g2
1+:::+g2
dd1+:::+dm](63)
We need now few observations regarding b(d1;:::;dm). Note ﬁrsr that since odd moments of
the Gaussian scalar distribution N(0;1)are zero,b(d1;:::;dm)is zero if at least of of diis odd.
Furthermore, \(d1;:::;dm)is trivially zero if all but at most one diare zero.
With our new notation, can be rewritten as:
 =1X
j=0j
j!X
(j1;:::;j m)2Sjj
j1;:::;jmX
(d1;:::;d m)2D(j1;:::;j m)bcj1;:::;j m(d1;:::;dm)
L(d1;:::;dm)(1 (d1;:::;dm));
Note also that we have:
e(Xiid
1+:::+Xiid
m)=1X
j=0j
j!X
(j1;:::;j m)2Sjj
j1;:::;jmX
(d1;:::;d m)2D(j1;:::;j m)bcj1;:::;j m(d1;:::;dm)
L(d1;:::;dm):
Therefore (see: our observations on b(d1;:::;dm)) to complete the proof it sufﬁces to show that:
(d1;:::;dm)d
d+2if at least two: di,djfori6=jare nonzero and all diare even.
Lemma 6. The following holds if for some i6=jwe have:di;dj>0and alldiare even:
(d1;:::;dm)d
d+ 2: (64)
Proof. Note that(d1;:::;dm)can be rewritten as:
(d1;:::;dm) =Qm
i=1d(di)
d(Pm
i=1di); (65)
whered(j)stands for the jthmoment of the -distribution with ddegrees of freedom. Note that
d(j) = 2j
2 (d+j
2)
 (d
2), where  is the so-called Gamma-function .
Using the fact that:  (n) = (n 1)!and (n+1
2) =(2n 1)!!
2npforn2N+, it is easy to see that
for a ﬁxedd, the RHS of the Equality 65 is maximized when di=dj= 2anddk= 0for somei6=j
andk =2fi;jg. Furthermore, straightforward calculations show that in that case the value of the RHS
from Equality 65 isd
d+2. That completes the proof of the Lemma.
35

--- PAGE 36 ---
Published as a conference paper at ICLR 2021
ByD0(j1;:::;jm)denote a subset of D(j1;:::;jm)formed by only keeping d1;:::;dmsuch that
for somei6=j,di;dj>0and alldiare even. As we have shown above, b(d1;:::;dm) = 0 when
(d1;:::;dm)=2D0(j1;:::;jm). Otherwise,
b(d1;:::;dm)2
d+ 2(d1;:::;dm)0:
Hence, since all terms in the sum
 =1X
j=0j
j!X
(j1;:::;j m)2Sjj
j1;:::;jmX
(d1;:::;d m)2D(j1;:::;j m)bcj1;:::;j m(d1;:::;dm) (66)
b(d1;:::;dm): (67)
are nonnegative, we’ll get a lower bound on by only taking a subset of these terms. For this subset,
we takej= 4, a subset ofS4with only two nonzero jk1=jk2= 2 for somek16=k2(there are m
2
combinations of such j1;:::;jm). Then, we take only those d1;:::;dmfromD(j1;:::;jm)
which correspond to s= 1 in (49) fork1;k2ands= 0 for all other k’s. Hence,dk1=dk2= 2
and all other dk’s are zero and the corresponding weight from the second sum in (67) would be
a2
1am 2
0. Ford1;:::;dmin such set, we’ll have (d1;:::;dm)d
d+2by Lemma 6 and, hence,
b(d1;:::;dm)2
d+2(d1;:::;dm). As the result, we get the following lower bound on :
24
4!(d+ 2)m
24
2;2;0;:::; 0
a2
1am 2
0(2;2;0;:::; 0)
=4m(m 1)
4(d+ 2)a2
1am 2
0(2;2;0;:::; 0)
=4m(m 1)
4(d+ 2)a2
1am 2
0kzk4 
Ek!k22(E(g2
1))2
(Ekgk2)2:
Since gN(0;1)d,Eg2
1= 1andEkgk2=dEg2
1=d. This results in
4m(m 1)
4d2(d+ 2)a2
1am 2
0kzk4 
Ek!k22(68)
which concludes the proof.
F.4.3 P ROOF OF THEOREM 6
Proof. We will use the notation from the proof of Theorem 5. Since both estimators: bFort
m(z)and
bFiid
m(z)are unbiased, we have: MSE(bFort
m(z)) = Var(bFort
m(z))andMSE(bFiid
m(z)) = Var(bFiid
m(z)).
We have:
Var(bFiid
m(z)) =E[(bFiid
m(z) E[bFiid
m(z)])2] =E[(bFiid
m(z))2] F2(z): (69)
Similarly,
Var(bFort
m(z)) =E[(bFort
m(z))2] F2(z): (70)
We have:
E[(bFiid
m(z))2] =1
m2mX
i=1E[(Xiid
i)2] +1
m2X
i6=jE[Xiid
iXiid
j]: (71)
Similarly, we get:
E[(bFort
m(z))2] =1
m2mX
i=1E[(Xort
i)2] +1
m2X
i6=jE[Xort
iXort
j]: (72)
36

--- PAGE 37 ---
Published as a conference paper at ICLR 2021
Therefore, since marginal distributions of Xiid
iandXort
iare the same, we have:
MSE(bFiid
m(z)) MSE(bFort
m(z)) =m
2
21
m2(E[Xiid
1Xiid
2] E[Xort
1Xort
2])
= (1 1
m)(E[Xiid
1Xiid
2] E[Xort
1Xort
2])(73)
Plugging in the formula for Xort
iandXiid
ifrom Equation 48 and Equation 49, and using our analysis
from the proof of Theorem 3 we obtain:
MSE(bFiid
m(z)) MSE(bFort
m(z)) = (1 1
m)1X
t;u=0ataukzkt+u
2E[k!kt
2]E[k!ku
2]
E[rt]E[ru]
E[p
g2
1+:::+g2
dt]E[p
g2
1+:::+g2
du](1 (t;u)):(74)
for!
andrN(0;1).
Based on the deﬁnition of (63), ift= 0oru= 0,(t;u) = 1 and the whole corresponding term in
the sum (74) is zero. Also, if tis odd,E(rt) = 0 and, again, the corresponding term in the sum (74)
is zero. Same holds for ufrom (74). Based on the analysis from Theorem 5’s proof and F
;g(z)’s
deﬁnition we have:
F
;g(z) =1X
t=0atkzkt
2E[k!kt
2]E[rt]
E[p
g2
1+:::+g2
dt]=1X
t=0a2tkzk2t
2E[k!k2t
2]E[r2t]
E[p
g2
1+:::+g2
d2t]
where in the second transition we use the fact that E[rt] = 0 for oddt.
Hence, we can rewrite (74) by excluding terms which are deﬁnitely zero and using Lemma 6:
MSE(bFiid
m(z)) MSE(bFort
m(z))(1 1
m)2
d+ 21X
t;u=1a2ta2ukzk2t+2u
2E[k!k2t
2]E[k!k2u
2]
E[r2t]E[r2u]
E[p
g2
1+:::+g2
d2t]E[p
g2
1+:::+g2
d2u]
= (1 1
m)2
d+ 2 1X
t=1a2tkzk2t
2E[k!k2t
2]E[r2t]
E[p
g2
1+:::+g2
d2t]!2
= (1 1
m)2
d+ 2(F
;g(z) a0)2:
(75)
That completes the proof.
F.5 P ROOF OF THEOREM 4
We showed in the main body of the paper that in contrast to other methods approximating the
attention matrix A, our algorithm provides strong concentration guarantees. This is the case also
for trigonometric random features, yet, as discussed in the main body of the paper, due to attention
renormalization and higher variance of the estimation of small entries of the attention matrix,
trigonometric mechanism is sub-optimal. We show here that mopt, the optimal number of random
projections for the trigonometric orthogonal mechanism for accurate estimation of the attention
matrix does not depend on Lbut only on d. In fact, we prove that if we take mopt= (dlog(d)),
then withO(Ld2log(d))-time, we can approximate Aup to any precision, regardless of the number
of tokensL. In order to provide those guarantees, we leverage recent research on the theory of
negative dependence for ORFs (Lin et al., 2020).
We prove the more general version of Theorem 4 from the main body of the paper:
Theorem 7 (Uniform convergence for the trigonometric mechanism) .Deﬁne entries of the attention
matrix Aas follows: Ai;j=g(q>
i)K(1
d1
4q>
i;1
d1
4k>
j)h(k>
j)for someg;h:Rd!Rand where K
37

--- PAGE 38 ---
Published as a conference paper at ICLR 2021
is a radial basis function (RBF) kernel (Choromanski et al., 2018b) with corresponding spectral
distribution 
(e.g. Gaussian kernel for which 
 =N(0;Id)). Assume that the rows of matrices Q
andKare taken from a ball B(R)of radiusR, centered at 0(i.e. norms of queries and keys are upper-
bounded by R). Deﬁnel=Rd 1
4and takeg= max x2B(l)jg(x)jandh= max x2B(l)jh(x)j.
Then for any  >0,=
ghand the number of random projections m= 
(d
2log(4R
d1
4))for
=E!
[!>!]the following holds: kbA Ak1with any constant probability, where bA
approximates generalized attention matrix via orthogonal trigonometric random features.
The result holds in particular for regular softmax-attention for which Kis a Gaussian kernel and
g(x) =h(x) = exp(kxk2
2). In that case mopt= 
(d
2log(4d3
4R
))since=d.
Proof. LetDQbe a diagonal matrix with entries of the form: g(q>
i)and let DKbe a diagonal matrix
with entries of the form: h(k>
i). Denote B= [K(1
d1
4q>
i;1
d1
4k>
j)]i;j2RLL. Denote bybAand
approximation of the attention matrix obtained from trigonometric orthogonal random features and
bybBan approximation of matrix Bthat those random features provide. We rely on Theorem 3 from
(Lin et al., 2020). Note that we can apply it in our case, since for RBF kernels the corresponding
functionsfisatisfyf1(x) = sin(x),f2(x) = cos(x)(thus in particular are bounded). Also, it is not
hard to observe (see for instance analysis in Claim 1 from (Rahimi & Recht, 2007)) that we can take:
Lf= 1(forLfas in Theorem 3 from (Lin et al., 2020)). Using Theorem 3 from (Lin et al., 2020),
we conclude that:
kbB Bk1 (76)
with any constant probability as long as m= 
(d
2) log(diam(M)
), where=E[!>!]andM
is the diameter of the smallest ball Mcontaining all vectors of the form z=Qi
d1
4 Kj
d1
4. Since
kQik2;kKjk2R, we conclude thatkzk22R
d1
4and thus one can take diam(M) =4R
d1
4. We have:
kbA Ak1=kDQ(bB B)DKk1kDQk1kbB Bk1kDKk1gh(77)
Taking=
ghcompletes the proof.
F.6 D ISCUSSION OF THEOREM 4
As a consequence of Theorem 4, the number mof random projections required to approximate the
attention matrix within error is a function of data dimensionality d, the parameter and the radius
Rof the ball within which the queries and keys live:
m= 	(;d;R ):
The dependence on dandis fairly easy to understand: with a larger dimensionality dwe need more
random projeections (on the order of magnitude dlog(d)) to get an approximation within error. The
dependence on Rmeans that the length of queries and keys cannot grow at a ﬁxed mif we want to
retain the quality of the approximation. In particular, this means that FA VOR cannot approximate
hard attention on sequences of unlimited length with a ﬁxed m. When the sequence length increases,
even the standard attention requires longer and longer vectors to make the softmax concentrated
enough to pick single elements. Nevertheless, as seen in our experiments, this limitation does not
manifest itself in practice at the lengths we experimented with.
38
