# 2305.15099.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2305.15099.pdf
# Kích thước tệp: 429609 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Fourier Transformer: Mô hình hóa phạm vi dài nhanh
bằng cách loại bỏ tính dư thừa chuỗi với toán tử FFT
Ziwei He♢, Meng Yang†, Minwei Feng†, Jingcheng Yin†,
Xinbing Wang♢,Jingwen Leng♢và Zhouhan Lin♢∗
♢Đại học Giao thông Thượng Hải†Netease BizEase
{ziwei.he, xwang8, leng-jw}@sjtu.edu.cn∗lin.zhouhan@gmail.com
Tóm tắt
Mô hình transformer được biết đến là đòi hỏi
tính toán cao và cực kỳ tốn kém cho các chuỗi
dài, vì mô-đun tự chú ý sử dụng độ phức tạp
thời gian và không gian bậc hai theo chiều dài
chuỗi. Nhiều nhà nghiên cứu đã tập trung vào
việc thiết kế các dạng tự chú ý mới hoặc giới
thiệu các tham số mới để vượt qua hạn chế này,
tuy nhiên phần lớn trong số đó ngăn cản mô
hình kế thừa trọng số từ các mô hình được huấn
luyện trước lớn. Trong nghiên cứu này, tính
không hiệu quả của transformer đã được xử lý
từ một góc độ khác. Chúng tôi đề xuất Fourier
Transformer, một phương pháp đơn giản nhưng
hiệu quả bằng cách loại bỏ dần các tính dư thừa
trong chuỗi ẩn sử dụng toán tử Biến đổi Fourier
Nhanh (FFT) có sẵn để thực hiện Biến đổi Cosine
Rời rạc (DCT). Fourier Transformer có thể giảm
đáng kể chi phí tính toán trong khi vẫn giữ được
khả năng kế thừa từ các mô hình được huấn luyện
trước lớn khác nhau. Các thí nghiệm cho thấy mô
hình của chúng tôi đạt được hiệu suất tiên tiến
nhất trong số tất cả các mô hình dựa trên transformer
trên benchmark mô hình hóa phạm vi dài LRA với
cải thiện đáng kể về cả tốc độ và không gian. Đối
với các tác vụ seq-to-seq sinh tạo bao gồm
CNN/DailyMail và ELI5, bằng cách kế thừa trọng
số BART, mô hình của chúng tôi vượt trội hơn
BART tiêu chuẩn và các mô hình hiệu quả khác.1
1 Giới thiệu
Transformers (Vaswani et al., 2017), đặc biệt
khi được trang bị huấn luyện trước quy mô lớn (De-
vlin et al., 2018; Lewis et al., 2019; Raffel et al.,
2020) đã trở thành kiến trúc cốt lõi trong hầu hết
các tác vụ xử lý ngôn ngữ tự nhiên (NLP), bao
gồm cả các tác vụ chỉ mã hóa như phân loại câu,
gán nhãn chuỗi (Liu et al., 2019), và các tác vụ
mã hóa-giải mã như tóm tắt văn bản
∗Zhouhan Lin là tác giả liên hệ.
1Mã nguồn của chúng tôi được công bố tại https://github.com/
LUMIA-Group/FourierTransformervà trả lời câu hỏi (Lewis et al., 2019). Tuy
nhiên, do độ phức tạp bậc hai của mô-đun tự chú
ý (Lin et al., 2017), việc áp dụng các mô hình này
trên các chuỗi dài có thể cực kỳ tốn kém. Kết quả
là, nhiều nỗ lực đã được đầu tư vào việc phát triển
các biến thể Transformer hiệu quả khác nhau (Tay
et al., 2020b), cũng như thiết lập các bộ thử nghiệm
tiêu chuẩn hóa cho các chuỗi dài như Long Range
Arena (LRA) (Tay et al., 2020a).
Hầu hết các Transformer hiệu quả đều thiết kế
các biến thể chú ý đặc biệt để giảm độ phức tạp
(Tay et al., 2020b). Một số trong số đó đạt được
điều này bằng cách chiếu các thành phần trong tự
chú ý vào các xấp xỉ thứ hạng thấp của nó (Wang
et al., 2020; Zhu et al., 2021; Winata et al., 2020,
v.v.), hoặc dựa vào kernelization để tính toán ngầm
định ma trận chú ý (Katharopoulos et al., 2020;
Choromanski et al., 2020b; Peng et al., 2021;
Choromanski et al., 2020a, v.v.).
Do việc giới thiệu các ma trận chiếu hoặc tham
số bổ sung, các mô hình này không thể kế thừa
các tham số mô hình được huấn luyện trước. Tuy
nhiên, vì các mô hình ngôn ngữ lớn được huấn
luyện trước (LLM) đã ảnh hưởng cơ bản đến cộng
đồng NLP, việc tách rời kiến trúc mô hình khỏi
LLM đòi hỏi huấn luyện trước từ đầu trên mô hình
được thiết kế, điều này cực kỳ đòi hỏi tài nguyên
đối với hầu hết các nhà thực hành.
Các phương pháp khác nhắm vào việc tính toán
một phần của ma trận chú ý, bằng cách tuân theo
một số mẫu được định nghĩa trước (Child et al.,
2019; Qiu et al., 2020; Ho et al., 2019, v.v.). Một
số trong số đó cho phép mẫu có thể học được
(Sukhbaatar et al., 2019; Roy et al., 2021, v.v.).
Hầu hết các mẫu đều yêu cầu các kernel CUDA
tùy chỉnh hoặc các toán tử đặc biệt để đạt được
tăng tốc được tuyên bố (Wu et al., 2019; Child
et al., 2019; Beltagy et al., 2020), điều này tạo ra
thử thách bổ sung trong việc triển khai các mô
hình này trên các thiết bị edge hoặc phần cứng
đặc biệt như TPU.
Hơn nữa, một số phương pháp liên quan đến
các bước tính toán bổ sung đáng kể, trong

--- TRANG 2 ---
thực tế có thể làm phản tác dụng với độ phức tạp
thời gian và bộ nhớ mà chúng giảm, đặc biệt đối
với các chuỗi ngắn và trung bình (Kitaev et al.,
2020; Roy et al., 2021).
Một yếu tố cốt lõi đằng sau các phương pháp
khác nhau là sự tồn tại của tính dư thừa trong
ma trận chú ý và trạng thái ẩn. Ví dụ, Wang et al.
(2020) cung cấp phân tích phổ trên ma trận tự chú
ý, chỉ ra rằng ma trận chú ý học được có thứ hạng
thấp, điều này cho phép họ học một xấp xỉ thứ
hạng thấp của ma trận chú ý. Lấy cảm hứng từ
hướng nghiên cứu này, trong nghiên cứu này, chúng
tôi phân tích phổ công suất của các trạng thái ẩn
trong chiều thời gian qua các lớp khác nhau trong
Hình 1, và cho thấy phổ công suất ngày càng tập
trung vào các bin tần số thấp hơn khi lớp càng sâu.
Trong nghiên cứu này, chúng tôi đề xuất Fourier
Transformer, mà thậm chí không yêu cầu học ma
trận chiếu để xấp xỉ tự chú ý.
Fourier Transformer tận dụng quan sát của chúng
tôi về phổ công suất của các trạng thái ẩn, nó loại
bỏ dần các tính dư thừa chuỗi qua các lớp khác
nhau bằng cách downsampling các trạng thái ẩn
với Biến đổi Cosine Rời rạc (DCT), một biến thể
của biến đổi Fourier tạo ra các giá trị thực.
DCT trong Fourier Transformer được đề xuất
của chúng tôi có thể được triển khai với toán tử
Biến đổi Fourier Nhanh (FFT). Nhờ ứng dụng
sâu rộng trong nén ảnh và xử lý tín hiệu, nó là
một trong những toán tử được sử dụng rộng rãi
nhất và được tối ưu hóa cao trong nhiều framework
khác nhau và thậm chí trên các thiết bị edge, cung
cấp độ phức tạp O(nlogn) và lên đến O(logn)
trong các triển khai song song với overhead không
đáng kể. Kết quả là, Fourier Transformer dễ dàng
triển khai trên nhiều loại thiết bị, không cần thiết
phải thiết kế các kernel CUDA đặc biệt. Ngoài ra,
kết quả thí nghiệm trên các tác vụ LRA cho thấy
nó hoạt động nhanh hơn đáng kể so với nhiều
Transformer hiệu quả khác, trong khi đạt được
hiệu suất tiên tiến nhất trong số các mô hình hiệu
quả dựa trên Transformer.
Mặt khác, vì DCT là một phép biến đổi tuyến
tính, có thể đảo ngược, và tự chú ý không bị can
thiệp trong mô hình của chúng tôi, Fourier Trans-
former được đề xuất có thể kế thừa trọng số được
huấn luyện trước từ các mô hình ngôn ngữ lớn mà
không làm tổn hại hiệu suất. Kết quả thí nghiệm
trên CNN-DailyMail (Hermann et al., 2015) và
ELI5 (Fan et al., 2019c) cho thấy mô hình của
chúng tôi có thể vượt trội hơn BART (Lewis et al.,
2019) và các Transformer hiệu quả khác bằng cách
kế thừa và tinh chỉnh trên BART. Hơn nữa, với
lượng rất nhỏ huấn luyện trước thêm trước khi tinh
chỉnh, hiệu suất của nó có thể được cải thiện thêm.
2 Nghiên cứu liên quan
Downsampling các trạng thái ẩn Không có
nhiều nghiên cứu downsample chiều dài chuỗi cho
ngôn ngữ tự nhiên. Nghiên cứu gần nhất là Funnel
Transformer (Dai et al., 2020), mà giảm dần chiều
dài chuỗi query thông qua strided mean pooling,
trong khi giữ nguyên chiều dài chuỗi key và value.
Fourier Transformer nén cả ba chuỗi cùng nhau
và mang lại tăng tốc tính toán nhiều hơn so với
Funnel Transformer. Lưu ý rằng Funnel Transformer
cần tái đầu tư các tính toán đã tiết kiệm để xây
dựng một mô hình lớn hơn để đạt hiệu suất tốt
hơn, điều này vô hiệu hóa khả năng kế thừa trọng
số được huấn luyện trước.
Đối với các nghiên cứu khác, Charformer (Tay
et al., 2021b) thiết kế một mô-đun tokenization
khả vi cũng dựa vào strided mean pooling để
downsample chuỗi byte của nó. Nyströmformer
(Xiong et al., 2021) xấp xỉ ma trận chú ý thông
qua phương pháp Nyström, mà hiệu quả downsample
các chuỗi query và key. Do lớp convolution theo
chiều sâu bổ sung, nó lại không thể tận dụng các
mô hình được huấn luyện trước.
Trong quan điểm rộng hơn, downsampling đã
được ưa chuộng hơn trong thị giác máy tính. (Chen
et al., 2020) downsample quyết liệt đầu vào thô
thành một vector 1D. Perceiver (Jaegle et al., 2021)
áp dụng cơ chế chú ý bất đối xứng để chưng cất
đầu vào thành một bottleneck tiềm ẩn chặt chẽ.
Gần như tất cả các mô hình thị giác này được thiết
kế cho các tác vụ thị giác chỉ mã hóa hơn là các
tác vụ NLP kiểu mã hóa-giải mã.
Biến đổi Fourier cho Transformer Có nhiều
nghiên cứu gần đây kết hợp biến đổi Fourier vào
Transformer. FNet (Lee-Thorp et al., 2021) áp
dụng phương pháp cực đoan hơn bằng cách thay
thế toàn bộ tự chú ý bằng 2D FFT, loại bỏ toàn
bộ phần ảo để tránh số phức. Performer (Choromanski
et al., 2020a) giới thiệu các đặc trưng Fourier ngẫu
nhiên trực giao để xấp xỉ chú ý softmax. FSAT
(Zhuang et al., 2022) sử dụng 1D FFT dọc theo
chiều chuỗi để học cấu trúc thưa thớt của ma trận
chú ý. DCTFormer (Scribano et al., 2022) dịch
các chuỗi vào miền tần số và thực hiện tự chú ý
ở đó trước khi chiếu chúng trở lại, do tính phi
tuyến trong mạng, tự chú ý được huấn luyện trong
miền tần số khác biệt đáng kể so với trong miền
thời gian. Do đó, tất cả các mô hình

--- TRANG 3 ---
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 0 (word embedding)
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 1
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 2
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 3
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 4
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 5
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 6
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 7
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 8
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 9
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 10
0 50 100 150 200 2500.00.20.40.60.81.01.2lớp 11Hình 1: Phổ công suất của các trạng thái ẩn đầu vào từ các lớp khác nhau trong mô hình RoBERTa được huấn luyện
trước (Liu et al., 2019). Trục ngang đại diện cho các bin tần số, bắt đầu từ các thành phần tần số thấp ở bên trái.
Trục dọc là các biên độ tương ứng. Biên độ được tính trung bình trên tất cả các chiều ẩn và trên toàn bộ tập
validation của Wiki-103 (Merity et al., 2016). Vì các đầu vào là số thực, các thành phần tần số dương và âm
là liên hợp từng cặp. Do đó chúng tôi chỉ vẽ biên độ của nửa dương của các tần số.
đã thảo luận ở trên đều mất khả năng kế thừa trọng
số được huấn luyện trước.
3 Kiến thức cơ bản
3.1 Biến đổi Cosine Rời rạc
Biến đổi Cosine Rời rạc (DCT) biểu diễn một
chuỗi các số thực theo tổng các hàm cosine với
các tần số khác nhau. Vì DCT chỉ tạo ra các giá
trị thực, nó là một thay thế cho biến đổi Fourier
trong lĩnh vực số thực. Nó đã là phép biến đổi
cốt lõi đằng sau định dạng nén ảnh có mất mát
JPEG2.
Chính thức, đối với một chuỗi N số thực
{xn}={x0, x1, ...x N−1}, DCT biến đổi nó vào
miền tần số thông qua3:
yk=αkN−1X
n=0xncosπk(2n+ 1)
2N
(1)
trong đó k∈ {0, ..., N −1}và αk là một hệ số
liên quan đến k:
αk=

q
1
Nif k = 0,q
2
Notherwise(2)
Chuỗi gốc {xn} có thể được khôi phục
với DCT nghịch đảo (IDCT):
xn=N−1X
k=0αkykcosπk(2n+ 1)
2N
(3)
mà chúng tôi sẽ ký hiệu là {xn}=IDCT ({yk}).
2https://jpeg.org/jpeg/
3Có một số biến thể hơi khác nhau của DCT. Ở đây
chúng tôi sử dụng biến thể type-II phổ biến nhất trong bài báo này.Thực tế, DCT có thể được tính toán bằng cách
sử dụng toán tử FFT. Đầu tiên, gọi {un} là {xn}
được xáo trộn bằng cách xen kẽ các giá trị của
nó ở các vị trí chẵn và lẻ. Chính thức, khi N là
một số nguyên lẻ, {un} được cho bởi
{un}={x0, x2, ..., x N−1, xN−2, xN−4, ..., x 1}
(4)
Khi N là chẵn, một xáo trộn tương tự được áp
dụng. Sau đó chúng ta biến đổi {un} vào miền
tần số của nó thông qua FFT:
{vk}=FFT ({un}) (5)
trong đó k∈ {0, ..., N −1}và {vk} là một chuỗi
có độ dài N. DCT của chuỗi gốc {xn} do đó có
thể được tính từ {vk}:
yk=cosπk
2N
Re(vk)−sinπk
2N
Im(vk)
(6)
trong đó Re(·)và Im(·)đại diện cho phần thực
và phần ảo, tương ứng.
3.2 Phổ Công suất của Trạng thái Ẩn
Transformer
Phổ công suất của một chuỗi rời rạc mô tả phân
phối công suất tín hiệu theo các thành phần tần
số, đó là biên độ của các thành phần tần số được
tạo ra bởi biến đổi Fourier. Đối với một lớp nhất
định trong Transformer, các trạng thái ẩn của nó
có thể được coi như một chuỗi các vector ẩn, dọc
theo chiều thời gian. Để phân tích phổ công suất
của lớp, chúng tôi thực hiện biến đổi Fourier 1D
độc lập dọc theo chiều thời gian

--- TRANG 4 ---
IDCT
DCT
hnhn
Khối 1Khối 2Khối 3
Upsample
Upsample
Residual+
+Cho Cài đặt Chỉ EncoderSoftmaxXác suất
ClassTới classification head
Cho Cài đặt Encoder-Decoder
DecoderBộ lọc Phổ
Bộ lọc Phổ
Cắt ngắn~
Đầu vàoHình 2: Kiến trúc Mô hình Tổng thể
chiều cho các vector ẩn, tính toán các biên độ
tương ứng, và tính trung bình trên tất cả các chiều
trong lớp đó. Ngoài ra, chúng tôi tính phổ trung
bình trên nhiều chuỗi văn bản để loại bỏ nhiễu
theo từng ví dụ.
Hình 1 cho thấy phổ công suất cho các lớp khác
nhau trong mô hình RoBERTa-base được huấn
luyện trước (Liu et al., 2019). Hình con trên-trái
cho thấy phổ công suất của word embedding tương
đối phẳng, phân phối năng lượng của nó gần như
đồng đều trên tất cả các thành phần tần số với
một số đỉnh ở tần số thấp. Khi lớp càng sâu, năng
lượng bắt đầu tập trung về phía tần số thấp và
các đỉnh bắt đầu làm mịn, để lại một đuôi dài ở
phía tần số cao. Xu hướng này chỉ ra rằng các
trạng thái ẩn trong các lớp sâu hơn có tương quan
cục bộ nhiều hơn, điều này tạo không gian cho
biến đổi Fourier để nén các tính dư thừa.
4 Fourier Transformer
4.1 Kiến trúc Mô hình
Kiến trúc tổng thể của Fourier Transformer được
mô tả trong Hình 2. Nói chung, chúng tôi chèn
các bộ lọc phổ giữa các lớp trong Transformer,
bên trong đó chúng tôi sử dụng DCT và IDCT
để downsample chiều dài chuỗi. Nhiều bộ lọc
phổ có thể làm việc cùng nhau để chia các lớp
Transformer thành các khối khác nhau, do đó giảm
dần chiều dài chuỗi. Chúng tôi để nguyên tự chú
ý để giữ lại khả năng kế thừa trọng số được huấn
luyện trước.
Về bộ lọc phổ, nó bao gồm ba bước, tức là,
transform, truncate, và reverse. Chính thức, đối
với một chuỗi ẩn đến {hn},0< n < N −1 chứa
N vector ẩn hn∈RD trong đó D là kích thước
ẩn của mô hình, bộ lọc phổ đầu tiên biến đổi nó
vào miền tần số thông qua 1D-DCT:
{yk}=DCT ({hn}), 0< k < N −1(7)
Lưu ý rằng DCT được áp dụng độc lập trên tất
cả chiều trong {hn}, do đó chỉ biến đổi dọc theo
chiều thời gian.
Tiếp theo, {yk} được cắt ngắn bằng cách cắt
bỏ các chiều cuối trên phía tần số cao. Đối với
các chuỗi có độ dài khác nhau, chúng tôi cố định
một tỉ lệ r∈(0,1), đây là một siêu tham số, để
xác định số lượng thành phần tần số cần giữ lại.
Do đó độ dài của {yk} được cắt ngắn từ N thành
⌈rN⌉.4
Cuối cùng, chuỗi ngắn hơn kết quả {yk},0<
k <⌈rN⌉ −1 có thể được biến đổi trở lại miền
thời gian thông qua IDCT, tạo ra một chuỗi ngắn
hơn của {˜hn}:
{˜hn}=IDCT ({yk}), 0< n < ⌈rN⌉ −1
(8)
Một lần nữa, IDCT cũng được thực hiện chỉ
trong chiều thời gian. Các trạng thái ẩn ngắn hơn
kết quả được truyền tới các lớp trên.
Tùy thuộc vào loại tác vụ, các phần tiếp theo
khác nhau. Chúng tôi sẽ trình bày chi tiết trong
các cài đặt chỉ encoder và encoder-decoder.
Cài đặt Chỉ Encoder Đối với các tác vụ chỉ
encoder như phân loại văn bản, đầu ra cuối cùng
của
4chúng tôi đã thử nghiệm với nhiều cách cắt ngắn khác nhau ở đây, như
cắt bỏ các thành phần tần số thấp, cắt bỏ những cái có biên độ trung bình
thấp hơn, loại bỏ các thành phần DC và chuẩn hóa lại chuỗi, trừ một giá
trị chung trên tất cả các thành phần và chuẩn hóa lại, hoặc giữ lại các thành
phần tương ứng với các đỉnh. Thú vị là, cách cổ điển đơn giản chỉ cắt bỏ
những cái tần số cao hóa ra hoạt động tốt nhất.

--- TRANG 5 ---
encoder được mong đợi là một vector có kích
thước cố định, sau đó được đưa vào hồi quy logistic
để dự đoán xác suất lớp. Trong nghiên cứu này,
trong khi mô hình được huấn luyện từ đầu, chúng
tôi đơn giản sử dụng mean pooling trên toàn bộ
chuỗi đầu ra để tạo ra vector này; ngược lại khi
mô hình kế thừa một token [CLS] từ các mô hình
được huấn luyện trước, chúng tôi sử dụng embedding
tại token đó thay thế.
Cài đặt Encoder-Decoder Đối với các tác vụ
sinh ngôn ngữ liên quan đến cả encoder và decoder,
có một chú ý encoder-decoder tham dự vào các
trạng thái encoder tại mỗi bước decoder. Tuy nhiên,
chú ý encoder-decoder yêu cầu độ phân giải vị
trí tinh tế để hoạt động tốt. Kết quả là chúng tôi
làm theo Dai et al. (2020) để upsample các chuỗi
ngắn hơn trở lại độ dài gốc của chúng, và cộng
các chuỗi ẩn được upsample tại tất cả các khối
lại với nhau trước khi đưa chúng vào decoder.
Cụ thể hơn, chúng tôi sử dụng nội suy láng giềng
gần nhất không có tham số cho upsampling, và
chúng tôi chuẩn hóa lại chuỗi sau khi cộng các
chuỗi được upsample.
4.2 Huấn luyện Trước Thêm
Vì DCT có thể đảo ngược thông qua IDCT, mô
hình được đề xuất xấp xỉ liền mạch Transformer
thông thường khi r tăng lên. Hình 3 cho thấy rằng
trong khi tinh chỉnh trực tiếp trên trọng số BART
(Lewis et al., 2019), mô hình hoạt động tương đối
tốt khi lên đến 70% thành phần tần số bị cắt ngắn.
Tuy nhiên, vì upsampling và cộng các chuỗi được
upsample vẫn khác với Transformer gốc, chúng
tôi vẫn có thể nén giọt cuối cùng bằng cách áp
dụng một lượng nhỏ huấn luyện trước thêm trước
khi tinh chỉnh, và cải thiện thêm hiệu suất mô hình.
Loại huấn luyện trước thêm này thuận lợi hơn
nhiều so với huấn luyện trước tùy chỉnh từ đầu,
có thể tốn lượng lớn tài nguyên tính toán.
Như một ví dụ cụ thể, huấn luyện trước thêm
mô hình của chúng tôi trên BART-Large tiêu thụ
khoảng 10GB dữ liệu và mất khoảng 4 ngày trên
2 GPU NVidia A100, trong khi huấn luyện trước
BART từ đầu cần tiêu thụ 160GB dữ liệu, mất
khoảng 1000 ngày với cùng thiết bị. So với huấn
luyện trước tùy chỉnh từ đầu, tận dụng trọng số
BART và huấn luyện trước thêm mất ít hơn 2 bậc
độ lớn tài nguyên tính toán, trong khi vẫn có thể
đưa mô hình đến hiệu suất tương tự hoặc thậm
chí tốt hơn.
4.3 Phân tích Độ phức tạp
Đối với một lớp Transformer tiêu chuẩn với chiều
mô hình D, bao gồm tự chú ý và 2 lớp feed-forward,
độ phức tạp thời gian và bộ nhớ của việc xử lý
một chuỗi đầu vào có độ dài N là O(N2D+ND2)
và O(N2+ND), tương ứng. Với toán tử FFT mô
hình của chúng tôi có thể nén chiều dài chuỗi từ
N thành ⌈rN⌉ trong độ phức tạp thời gian O(NlogN).
Do đó Fourier Transformer có độ phức tạp thời
gian và bộ nhớ O(r2N2D+rND2+NlogN) và
O(r2N2+rND ) mỗi lần chiều dài chuỗi được
giảm. Thực tế, với triển khai song song của FFT,
số hạng độ phức tạp thời gian O(NlogN) bổ sung
là không đáng kể so với hai số hạng khác. Tăng
tốc có thể trở nên ấn tượng hơn khi chiều dài chuỗi
tương đối dài. Chúng tôi tham khảo độc giả đến
Phần 5.1 để biết thêm chi tiết.
5 Thí nghiệm
Trong phần này, chúng tôi thử nghiệm mô hình
của chúng tôi trong cả hai cài đặt chỉ encoder và
encoder-decoder trong các bộ dữ liệu khác nhau
liên quan đến các chuỗi dài.
5.1 Tác vụ Chỉ Encoder
Để kiểm tra khả năng của mô hình trên các tác
vụ chỉ encoder, chúng tôi chọn 5 tác vụ trong
benchmark Long Range Arena (LRA) được sử
dụng rộng rãi (Tay et al., 2020a). LRA được thiết
kế để đánh giá các transformer hiệu quả trong
tình huống ngữ cảnh dài, với độ dài chuỗi đầu
vào từ 1K đến 8K. Các bộ dữ liệu trong LRA
đến từ các nguồn phong phú, bao gồm ngôn ngữ
tự nhiên, pixel ảnh, biểu thức toán học, v.v. Cụ
thể hơn, chúng là:
ListOps Một bộ dữ liệu các biểu thức toán học
yêu cầu mô hình tính giá trị đầu ra của một biểu
thức toán học với độ dài chuỗi lên đến 2K.
Text Một tác vụ phân loại văn bản ở mức byte,
với độ dài chuỗi cố định 4K yêu cầu mô hình
xử lý tính hợp thành.
Retrieval Một tác vụ truy xuất tài liệu ở mức
byte với độ dài tối đa 8K kiểm tra khả năng nén
các chuỗi dài của mô hình.
Image Một tác vụ phân loại ảnh yêu cầu mô
hình học quan hệ không gian 2D giữa các pixel
đầu vào bằng cách đọc tuần tự các pixel. Độ dài
chuỗi được cố định ở 1K.

--- TRANG 6 ---
Mô hình ListOps Text Retrieval Image Pathfinder Trung bình
Transformer (Vaswani et al., 2017) 36.37 64.27 57.46 42.44 71.40 54.39
Longformer (Beltagy et al., 2020) 35.63 62.85 56.89 42.22 69.71 53.46
Linformer (Wang et al., 2020) 35.70 53.94 52.27 38.56 76.34 51.36
Reformer (Kitaev et al., 2020) 37.27 56.10 53.40 38.07 68.50 50.67
Synthesizer (Tay et al., 2021a) 36.99 61.68 54.67 41.61 69.45 52.88
BigBird (Zaheer et al., 2020) 36.05 64.02 59.29 40.83 74.87 55.01
Performer (Choromanski et al., 2020a) 18.01 65.40 53.82 42.77 77.50 51.41
FNet (Lee-Thorp et al., 2021) 35.55 65.11 59.61 38.67 77.80 55.30
Nyström (Xiong et al., 2021) 37.15 65.52 79.56 41.58 70.94 58.95
Luna-256 (Ma et al., 2021) 37.25 64.57 79.29 47.38 77.32 61.24
FSAT (Zhuang et al., 2022) 46.85 65.95 81.11 49.97 77.32 64.24
Fourier Transformer (của chúng tôi) 40.73 75.02 85.35 53.17 83.43 67.54
Bảng 1: Kết quả trên benchmark LRA. Chúng tôi báo cáo độ chính xác phân loại cho mỗi tác vụ và độ chính xác
trung bình trên tất cả các tác vụ. Kết quả từ Longformer đến Performer được lấy từ Tay et al. (2020a), phần còn
lại được lấy từ các bài báo tương ứng của chúng. Đối với mô hình FSAT trên tác vụ Text, chúng tôi chỉ xem xét
kết quả không có convolution.
Bước mỗi giây ↑ Sử dụng Bộ nhớ Đỉnh ↓
Mô hình 1K 2K 3K 4K 1K 2K 3K 4K
Transformer 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x 1.0x
Reformer 0.5x 0.4x 0.7x 0.8x 0.56x 0.37x 0.28x 0.24x
BigBird 0.9x 0.8x 1.2x 1.1x 0.91x 0.56x 0.4x 0.3x
Synthesizer 1.1x 1.2x 2.9x 1.4x 0.76x 0.75x 0.74x 0.74x
FSAT 1.1x 1.5x 2x 2.5x 0.53x 0.27x 0.21x 0.16x
Linformer 1.2x 1.9x 3.7x 5.5x 0.44x 0.21x 0.18x 0.1x
Performer 1.2x 1.9x 3.8x 5.7x 0.44x 0.22x 0.15x 0.11x
Fourier Transformer (của chúng tôi) 6.9x12.2x16.8x17.7x0.23x0.19x 0.18x 0.18x
Bảng 2: Tốc độ và tiêu thụ bộ nhớ trên benchmark LRA đối với tác vụ Text với độ dài đầu vào 1K, 2K, 3K
và 4K. Kết quả từ Reformer đến Performer được lấy từ Zhuang et al. (2022). Tốc độ và tiêu thụ bộ nhớ được
liệt kê là tỷ lệ so với Transformer thông thường.
Pathfinder Một tác vụ phân loại ảnh tổng hợp
với độ dài đầu vào cố định 1K yêu cầu mô hình
nắm bắt các phụ thuộc không gian phạm vi dài.
5.1.1 Chi tiết Triển khai
Chúng tôi chạy thí nghiệm trên benchmark LRA
theo sát các cấu hình trong (Tay et al., 2020a),
bao gồm tiền xử lý dữ liệu, chia tách dữ liệu,
kiến trúc mô hình, siêu tham số (số lớp, chiều
ẩn, v.v.). Chúng tôi đánh giá theo độ chính xác
phân loại. Triển khai của chúng tôi dựa trên (Xiong
et al., 2021). Để đơn giản, chúng tôi báo cáo kết
quả của mô hình trên năm tác vụ với cùng ngân
sách nén. Chúng tôi giảm mạnh 80% độ dài chuỗi
đầu vào tại lớp đầu tiên.
5.1.2 Hiệu suất & Hiệu quả
Kết quả trên năm tác vụ nói trên được tóm tắt
trong Bảng 1. Chúng tôi so sánh Fourier Transformer
với một loạt các mô hình dựa trên Transformer
đã được công bố trước đây, và nó đạt được kết
quả tiên tiến nhất mới trên bốn trong số năm tác
vụ. Mô hình được đề xuất của chúng tôi cải thiện
so với mô hình SOTA trước đó (Zhuang et al.,
2022) trên Text, Retrieval, Image và Pathfinder
lần lượt 9.07%, 4.24%, 3.20%, 6.11% giá trị tuyệt
đối, đây là một biên độ lớn. Đáng chú ý, mô hình
của chúng tôi không đánh bại FSAT (Zhuang et al.,
2022) trên tác vụ ListOps và xếp thứ 2 trong danh
sách. Chúng tôi suy đoán rằng đó là do các giá
trị biểu thức toán học nhạy cảm hơn

--- TRANG 7 ---
với các token riêng lẻ trong chuỗi, do đó nhạy
cảm hơn với downsampling.
Tiếp theo, lấy tác vụ phân loại văn bản ở mức
byte (bộ dữ liệu Text) làm bệ thử, chúng tôi đánh
giá định lượng hiệu quả thời gian và bộ nhớ của
mô hình và các mô hình cạnh tranh khác trên các
độ dài đầu vào khác nhau. Kết quả được tóm tắt
trong Bảng 2. Lưu ý rằng, do hạn chế bộ nhớ
GPU cho Transformer thông thường, kết quả trên
độ dài 1K, 2K và 3K được chạy với batch size
32, và 4K với batch size 16. Chúng tôi tính toán
các tỷ lệ tương ứng của mô hình so với Transformer
thông thường trên các cài đặt batch giống hệt
nhau, và tính thời gian trên GPU NVidia A100-80G.
So với các transformer hiệu quả khác, Fourier
Transformer giảm đáng kể thời gian tiêu thụ trên
cả chuỗi ngắn và dài, để lại các mô hình khác
phía sau một biên độ lớn, trong khi giữ tiết kiệm
bộ nhớ ổn định khi độ dài chuỗi tăng.
5.2 Tác vụ Encoder-Decoder
Mô hình cho các tác vụ encoder-decoder được
trang bị decoder để thực hiện sinh văn bản. Cho
cài đặt này, chúng tôi chọn hai bộ dữ liệu văn
bản dài trong các tác vụ tóm tắt và trả lời câu
hỏi, tức là, CNN/DailyMail (Hermann et al., 2015)
và ELI5 (Fan et al., 2019c), với độ dài chuỗi trung
bình lần lượt là 0.8K và 5K.
CNN/DailyMail Một bộ dữ liệu tóm tắt chứa
hơn 280K bài báo tin tức (trung bình 766 token)
từ các câu chuyện tin tức trên website CNN và
Daily Mail được ghép cặp với tóm tắt được tạo
bởi con người (trung bình 53 token). Chúng tôi
làm theo chuyển đổi và đánh giá hiệu suất theo
điểm Rouge (Rouge-1, Rouge-2, Rouge-L) (Lin,
2004).
ELI5 Một bộ dữ liệu trả lời câu hỏi chứa hơn
270K cặp câu hỏi-trả lời phức tạp, đa dạng và
có độ dài đoạn văn được thu thập từ các subreddit,
số token trung bình cho đầu vào và đích lần lượt
là 5140 và 693. Theo chuyển đổi, chúng tôi đánh
giá nó trong cả điểm Rouge-L và F1.
5.2.1 Chi tiết Triển khai
Vì trên cả hai bộ dữ liệu, các mô hình được huấn
luyện trước để lại khoảng cách lớn so với những
mô hình không được huấn luyện trước, việc báo
cáo kết quả mà không huấn luyện trước ít ý nghĩa
hơn. Do đó, chúng tôi báo cáo kết quả của mô
hình kế thừa trọng số BART-large (Lewis et al.,
2019). Chúng tôi thường thử nghiệm hai cài đặt,
đó là: 1) tinh chỉnh trực tiếp mô hình trên bộ dữ
liệu, và 2) thực hiện huấn luyện trước thêm trước
khi tinh chỉnh. Để thuận tiện, chúng tôi gọi chúng
là Fourier-BART và Fourier-BART-FP tương ứng
trong phần còn lại của bài báo.
Fourier-BART có cùng kiến trúc với BART-large.
Nó đơn giản áp dụng thiết kế 2 khối, khối đầu
tiên chứa 2 lớp transformer liên tiếp đầu tiên, 10
lớp còn lại thuộc về khối thứ hai. Đối với CNN/DailyMail,
50% thành phần tần số bị cắt ngắn, trong khi đối
với ELI5 70% bị cắt ngắn vì nó có độ dài chuỗi
dài hơn nhiều.
Fourier-BART-FP có cùng cài đặt với Fourier-BART,
ngoại trừ trước khi tinh chỉnh trên các tác vụ downstream
nó được huấn luyện trước thêm 1 epoch trên 10GB
văn bản với các mục tiêu huấn luyện trước BART
gốc. Văn bản được cắt ngẫu nhiên từ corpus Pile
(Gao et al., 2020).
5.2.2 Hiệu suất & Hiệu quả
CNN/DailyMail Trên tác vụ tóm tắt, trong phạm
vi các mô hình hiệu quả, chúng tôi so sánh mô
hình với BigBird (Zaheer et al., 2020), ST-MoE
(Zoph et al., 2022) và Switch Transformer (Fedus
et al., 2021), đây là các baseline mạnh từ tài liệu
gần đây. Cả ST-MoE và Switch Transformer đều
nhắm vào việc kích hoạt chỉ một phần tham số
để cải thiện hiệu quả. Bigbird xấp xỉ ma trận chú
ý đầy đủ với một ma trận thưa để cải thiện FLOPs.
Ngoài ra, chúng tôi đặt hiệu suất BART tiêu chuẩn
(Lewis et al., 2019) làm baseline.
Kết quả được liệt kê trong Bảng 3. Fourier-BART
được đề xuất của chúng tôi kế thừa liên tiếp lợi
thế của BART, đạt hiệu suất ở mức mô hình được
huấn luyện trước. Với lượng nhỏ huấn luyện trước
thêm, nó đạt hiệu suất tốt nhất trong số tất cả đối
thủ cạnh tranh. Lưu ý rằng Fourier-BART được
xây dựng dựa trên BART và chia sẻ cùng kích
thước mô hình với BART-400M với ít tính toán
hơn nhiều, tuy nhiên nó có thể vượt trội hơn BART-400M
tiêu chuẩn với một biên độ hợp lý.
Về hiệu quả, gần như không thể tái tạo tất cả
các mô hình được liệt kê trong Bảng 3 và điều
tra hiệu quả của chúng, vì vậy chúng tôi chọn chỉ
đánh giá BART-400M tiêu chuẩn và Fourier-BART-400M
được đề xuất theo FLOPs. Như đã trình bày trong
Phần 5.2.1, chúng tôi loại bỏ 50% từ chuỗi ẩn
ở lớp transformer thứ ba, mặc dù hai mô hình có
cùng kích thước chính xác, FLOPs được đầu tư
trong BART-400M tiêu chuẩn là 1.6 lần của Fourier-BART-400M.
Do upsampling và giải mã tự hồi quy, việc giảm
tổng thể

--- TRANG 8 ---
Mô hình R-1 R-2 R-L
BART-400M 44.16 21.28 40.90
ST-MOE-L-770M - 20.7 -
Switch Trans.-223M - 19.6 -
BigBird-Large 43.84 21.11 40.74
Fourier-BART-400M 44.65 21.48 41.30
Fourier-BART-FP-400M 44.76 21.55 41.34
Bảng 3: Điểm Rouge trên CNN/DailyMail. Kết quả
đều được lấy từ các bài báo tương ứng của chúng. R-1
và R-L của ST-MOE và Switch Transformer không được
báo cáo trong bài báo của chúng. Số sau tên mô hình
biểu thị kích thước mô hình. Kích thước mô hình cho
BigBird không được đề cập trong bài báo của chúng
thật không may.
Mô hình RL F1
LayerDrop-240M 23.4 -
E-MCA-240M 24.0 -
c-REALM*-596M 23.2 22.9
EMAT*-446M 20.91 19.03
KID*-406M 26.3 -
BART-large-400M 26.8 26.6
Fourier-BART-400M 26.2 25.98
Fourier-BART-FP-400M 26.9 26.73
Bảng 4: Hiệu suất mô hình trên ELI5. Kết quả từ E-MCA
đến KID được lấy từ các bài báo tương ứng của chúng.
* biểu thị kết quả sử dụng benchmark Kilt (Petroni et al.,
2020), có tập dev và test nhỏ hơn.
trong tính toán không đáng kể như những cái trên
LRA.
ELI5 Trên tác vụ trả lời câu hỏi, chúng tôi so
sánh mô hình với LayerDrop (Fan et al., 2019b),
E-MCA (Fan et al., 2019a), c-REALMS (Krishna
et al., 2021), EMAT (Wu et al., 2022) và KID
(Liu et al., 2022). Để cung cấp so sánh công bằng,
kết quả của BART-large là kết quả được tái tạo
của chúng tôi trên phiên bản mới nhất của fairseq
(Ott et al., 2019), cao hơn nhiều so với kết quả
được báo cáo trong bài báo BART gốc. Lưu ý
rằng ở đây chúng tôi thậm chí còn so sánh với
các mô hình nhạy cảm hiệu suất, vì trong danh
sách chỉ EMAT và LayerDrop tập trung vào việc
giảm độ phức tạp. Như được thể hiện trong Bảng
4, Fourier-BART-FP của chúng tôi đã vượt qua
tất cả các mô hình cạnh tranh trên cả điểm Rouge-L
và F1.
Về hiệu quả, khi loại bỏ 70% thành phần tần
số (được trình bày trong Phần 5.2.1), FLOPs
được đầu tư trong BART tiêu chuẩn là 1.9 lần
của Fourier-BART.
10% 30% 50% 70% 90% 100%
Tỷ lệ Giữ lại23.023.524.024.525.025.526.026.5GiátrịRL
F1Hình 3: R1, R2, RL và F1 trên ELI5. trục x đại diện cho
tỷ lệ giữ lại r.
5.3 Phân tích về Tỷ lệ Giữ lại r
Một câu hỏi quan trọng nảy sinh là mô hình nhạy
cảm như thế nào đối với tỷ lệ giữ lại các thành
phần tần số. Để điều tra điều này, chúng tôi thử
nghiệm mô hình trong bộ dữ liệu ELI5 bằng cách
quét r từ 0.1 đến 1. Chúng tôi không thực hiện
huấn luyện trước thêm trên mỗi cài đặt do hạn
chế tính toán. Kết quả được thể hiện trong Hình
3. Hiệu suất vẫn khá tốt cho đến khi ít hơn 30%
thành phần tần số được giữ lại. Khi chúng tôi cố
gắng cắt ngắn nhiều thành phần hơn vượt qua tỷ
lệ đó, hiệu suất bắt đầu giảm đáng kể. Đây là một
kết quả khá thỏa mãn cho thấy mô hình hoạt động
ổn định tin cậy trong một phạm vi rộng của các
r hợp lý.
6 Kết luận
Trong nghiên cứu này, chúng tôi giới thiệu biến
đổi cosine rời rạc để downsample dần các trạng
thái ẩn trong mô hình Transformer bằng cách tận
dụng các tương quan cục bộ giữa các trạng thái
ẩn trong các lớp trên. Phương pháp của chúng
tôi có thể giảm đáng kể tính toán cần thiết bởi
Transformer thông thường, trong khi có thể đạt
được hiệu suất thậm chí tốt hơn trong các tác vụ
khác nhau. Hơn nữa, nó có thể kế thừa trọng số
mô hình được huấn luyện trước, đây là một lợi
thế đáng chú ý so với hầu hết các Transformer
hiệu quả.
7 Hạn chế
Mặc dù phương pháp của chúng tôi thể hiện tăng
tốc lớn trong các cài đặt chỉ encoder, nó không
mang lại tăng tốc ấn tượng như vậy trong cài đặt
encoder-decoder. Điều này là do các bước giải
mã tự hồi quy trong decoder, phải được thực hiện
tuần tự. Tăng tốc điều đó với DCT yêu cầu cập
nhật tăng dần đầu ra DCT từng bước dựa trên
đầu ra của các bước thời gian trước đó, điều này
về mặt lý thuyết có thể nhưng không dễ tối ưu
hiệu quả của nó. Chúng tôi dự định tăng tốc thêm
nó theo hướng này trong nghiên cứu tương lai.
Lời cảm ơn
Nghiên cứu này được tài trợ bởi quỹ Quỹ Khoa
học Tự nhiên Quốc gia Trung Quốc (NSFC) (Số
62106143), và Chương trình Pujiang Thượng Hải
(Số 21PJ1405700).
Tài liệu tham khảo
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu,
Heewoo Jun, David Luan, and Ilya Sutskever. 2020.
Generative pretraining from pixels. In International
conference on machine learning , pages 1691–1703.
PMLR.
Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509 .
Krzysztof Choromanski, Valerii Likhosherstov, David
Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,
Peter Hawkins, Jared Davis, David Belanger, Lucy
Colwell, et al. 2020a. Masked language modeling
for proteins via linearly scalable long-context trans-
formers. arXiv preprint arXiv:2006.03555 .
Krzysztof Choromanski, Valerii Likhosherstov, David
Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
Lukasz Kaiser, et al. 2020b. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794 .
Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.
2020. Funnel-transformer: Filtering out sequential
redundancy for efficient language processing. Ad-
vances in neural information processing systems ,
33:4271–4282.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Angela Fan, Claire Gardent, Chloé Braud, and Antoine
Bordes. 2019a. Using local knowledge graph con-
struction to scale seq2seq models to multi-document
inputs. arXiv preprint arXiv:1910.08435 .
Angela Fan, Edouard Grave, and Armand Joulin. 2019b.
Reducing transformer depth on demand with struc-
tured dropout. arXiv preprint arXiv:1909.11556 .

--- TRANG 9 ---
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019c. ELI5:
long form question answering. In Proceedings of
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers , pages
3558–3567. Association for Computational Linguis-
tics.
William Fedus, Barret Zoph, and Noam Shazeer. 2021.
Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. Advances in neural information
processing systems , 28.
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn,
and Tim Salimans. 2019. Axial attention in
multidimensional transformers. arXiv preprint
arXiv:1912.12180 .
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol
Vinyals, Andrew Zisserman, and Joao Carreira. 2021.
Perceiver: General perception with iterative atten-
tion. In International conference on machine learn-
ing, pages 4651–4664. PMLR.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
rnns: Fast autoregressive transformers with linear
attention. In International Conference on Machine
Learning , pages 5156–5165. PMLR.
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. arXiv
preprint arXiv:2001.04451 .
Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.
Hurdles to progress in long-form question answering.
arXiv preprint arXiv:2103.06332 .
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and
Santiago Ontanon. 2021. Fnet: Mixing tokens with
fourier transforms. arXiv preprint arXiv:2105.03824 .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461 .
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.

--- TRANG 10 ---
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130 .
Ruibo Liu, Guoqing Zheng, Shashank Gupta, Rad-
hika Gaonkar, Chongyang Gao, Soroush V osoughi,
Milad Shokouhi, and Ahmed Hassan Awadallah.
2022. Knowledge infused decoding. arXiv preprint
arXiv:2204.03084 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,
Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021.
Luna: Linear unified nested attention. Advances
in Neural Information Processing Systems , 34:2441–
2453.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.arXiv preprint arXiv:1609.07843 .
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of NAACL-HLT
2019: Demonstrations .
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz, Noah A Smith, and Lingpeng Kong.
2021. Random feature attention. arXiv preprint
arXiv:2103.02143 .
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Mail-
lard, et al. 2020. Kilt: a benchmark for knowl-
edge intensive language tasks. arXiv preprint
arXiv:2009.02252 .
Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,
Sinong Wang, and Jie Tang. 2020. Blockwise self-
attention for long document understanding. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020 , pages 2555–2565.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res. , 21(140):1–67.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier. 2021. Efficient content-based sparse
attention with routing transformers. Transactions of
the Association for Computational Linguistics , 9:53–
68.
Carmelo Scribano, Giorgia Franchini, Marco Prato,
and Marko Bertogna. 2022. Dct-former: Efficient
self-attention withdiscrete cosine transform. arXiv
preprint arXiv:2203.01178 .Sainbayar Sukhbaatar, Édouard Grave, Piotr Bo-
janowski, and Armand Joulin. 2019. Adaptive at-
tention span in transformers. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics , pages 331–335.
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,
Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Re-
thinking self-attention for transformer models. In
International conference on machine learning , pages
10183–10192. PMLR.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. 2020a. Long
range arena: A benchmark for efficient transformers.
arXiv preprint arXiv:2011.04006 .
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald
Metzler. 2020b. Efficient transformers: A survey.
ACM Computing Surveys (CSUR) .
Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta,
Hyung Won Chung, Dara Bahri, Zhen Qin, Si-
mon Baumgartner, Cong Yu, and Donald Metzler.
2021b. Charformer: Fast character transformers via
gradient-based subword tokenization. arXiv preprint
arXiv:2106.12672 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 .
Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang
Lin, Zihan Liu, and Pascale Fung. 2020. Lightweight
and efficient end-to-end speech recognition us-
ing low-rank transformer. In ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 6144–6148.
IEEE.
Felix Wu, Angela Fan, Alexei Baevski, Yann N
Dauphin, and Michael Auli. 2019. Pay less attention
with lightweight and dynamic convolutions. arXiv
preprint arXiv:1901.10430 .
Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Min-
ervini, Pontus Stenetorp, and Sebastian Riedel.
2022. An efficient memory-augmented transformer
for knowledge-intensive nlp tasks. arXiv preprint
arXiv:2210.16773 .
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty,
Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.
2021. Nyströmformer: A nyström-based algorithm
for approximating self-attention. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 35, pages 14138–14148.

--- TRANG 11 ---
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. Advances in Neural Information
Processing Systems , 33:17283–17297.
Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad
Shoeybi, Tom Goldstein, Anima Anandkumar, and
Bryan Catanzaro. 2021. Long-short transformer: Ef-
ficient transformers for language and vision. Ad-
vances in Neural Information Processing Systems ,
34:17723–17736.
Yimeng Zhuang, Jing Zhang, and Mei Tu. 2022. Long-
range sequence modeling with predictable sparse at-
tention. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 234–243.
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-
ping Huang, Jeff Dean, Noam Shazeer, and William
Fedus. 2022. Designing effective sparse expert mod-
els.arXiv preprint arXiv:2202.08906 .
