# 2009.09672.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2009.09672.pdf
# File size: 416337 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Alleviating the Inequality of Attention Heads
for Neural Machine Translation
Zewei Sun1,, Shujian Huang2,3, Xin-Yu Dai2, Jiajun Chen2
1ByteDance AI Lab
2State Key Laboratory for Novel Software Technology, Nanjing University
3Peng Cheng Laboratory, China
sunzewei.v@bytedance.com ,{huangsj,daixinyu,chenjj}@nju.edu.cn
Abstract
Recent studies show that the attention heads
in Transformer are not equal (V oita et al.,
2019; Michel et al., 2019). We relate this phe-
nomenon to the imbalance training of multi-
head attention and the model dependence on
speciÔ¨Åc heads. To tackle this problem, we pro-
pose a simple masking method: HeadMask ,
in two speciÔ¨Åc ways. Experiments show that
translation improvements are achieved on mul-
tiple language pairs. Subsequent empirical
analyses also support our assumption and con-
Ô¨Årm the effectiveness of the method.
1 Introduction
Recently, more and more novel network structures
of neural machine translation(NMT) have been
proposed (Bahdanau et al., 2015; Barone et al.,
2017; Gehring et al., 2017; Vaswani et al., 2017),
among which Transformer (Vaswani et al., 2017)
achieves the best results. One important difference
between Transformer and other translation models
is its multi-head attention mechanism.
Some interesting phenomena of the attention
heads are discovered recently. V oita et al. (2019)
Ô¨Ånd that only a small subset of heads appear to
be important for the translation task and vast ma-
jority of heads can be removed without seriously
affecting performance. Michel et al. (2019) also
Ô¨Ånd that several heads can be removed from trained
transformer models without statistically signiÔ¨Åcant
degradation in test performance. It turns out that
not all heads are equally important.
We speculate that this can be attributed to the im-
balanced training of multi-head attention, as some
heads are not trained adequately and contribute lit-
tle to the model. However, this can be turned into
the bottleneck for the whole model. For an analogy,
if a soccer player gets used to using the right foot
and spares more training opportunities for it, it will
* Work was done while at NJUbe stronger and stronger. As a result, the right foot
is further relied on, while the left foot receives less
training and gradually turns into the limitation.
In this paper, we Ô¨Årstly empirically conÔ¨Årm the
inequality in multi-head attention. Then a new
training method with two variants is proposed to
avoid the bottleneck and improve the translation
performance. Further analyses are also made to
verify the assumption.
2 Head Inequality
Following Michel et al. (2019), we deÔ¨Åne the im-
portance of an attention head has
Ih=ExX@L(x)
@h(1)
whereL(x)is the loss on sample x and is the
head mask variable with values in {0, 1}. Intu-
itively, ifhead his important, switching hwill
have a signiÔ¨Åcant effect on the loss. Applying the
chain rule yields the Ô¨Ånal expression for Ih:
Ih=ExXAtth(x)T@L(x)
@Atth(x)(2)
This is equivalent to the Taylor expansion
method from Molchanov et al. (2017). In Trans-
former base (Vaswani et al., 2017), there are 3
types of attention (encoder self attention, decoder
self attention, encoder-decoder attention) with 6
layers per type and 8 heads per layer. Therefore,
it amounts to 144 heads. We divide them into 8
groups with 18 heads (12.5%) each group accord-
ing to their importance Ih, among which, 1-18 are
the most important and so on.
We then mask different groups of the heads. As
is shown in Figure 1, masking a group of unimpor-
tant heads has little effect on the translation quality
while masking important heads leads to a signiÔ¨Å-
cant drop of performance. Surprisingly, almost half
of the heads are not important, as it makes almost
no difference whether they are masked or not.arXiv:2009.09672v2  [cs.CL]  31 Aug 2022

--- PAGE 2 ---
None
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Mask different groups of heads (sorted by importance)20253035404550BLEUBLEU with different head masks
BaselineFigure 1: Mask the heads in the same group. Important
ones matter much more than unimportant ones.
We also gradually masking more heads group
by group in the ascending order and descending
order, respectively. As is shown in Figure 2, the
line starting with unimportant heads drops much
slower than the one starting with important ones. It
fully illustrates the inequality of different heads.
012.5% 25% 37.5% 50% 62.5% 75% 87.5% 100%
Proportion of head mask coverage01020304050BLEU
BLEU with different proportion of head mask coverage
Unimportant heads first
Important heads first
Figure 2: Mask all heads in the ascending order and
descending order. The drop curves differ greatly.
Figure 1 and Figure 2 further demonstrates the
inequality of the importance of attention heads. A
simple assumption for explanation is that some
heads coincidentally get more updating opportu-
nities in the early stage, which makes the model
learning to depend on them gradually. As a re-
sult, the model increasingly draws a strong con-
nection with these speciÔ¨Åc heads while this local
dependence prevents the rest attention heads from
adequate training and restricts the overall capacity.
3 HeadMask
Since the problem refers to the unfair training of
attention heads, it is natural for us to explicitly
balance the training chances. We propose a simple
method: HeadMask , which masks certain heads
during training in two speciÔ¨Åc ways.3.1 Mask Randomly
The Ô¨Årst one is randomly picking heads and mask-
ing them in each batch. It ensures every head gets
relatively equal opportunities of training and avoid
partial dependence, as is shown in Algorithm 1.
For the soccer analogy, it is like training the feet
randomly, making both receive the same amount
of practice.
Algorithm 1 HeadMask: Mask Randomly
Input:q;k;v for attention, number of masks n
Output: masked context
1:forbatch in datasets do
2: heads = random.sample(all_heads, n)
3: forhead in heads do
4:head = 0
5: end for
6: context = attn( )
7:end for
3.2 Mask Important Ones
The second one is masking the most important
heads. By forcing the model neglects important
heads, we hope more training chances are assigned
to weaker heads. For the soccer analogy, it means
training the left foot more if the right foot domi-
nates. And once reversed, train contrarily. Its main
idea is about suppressing addicted training. Specif-
ically, the network Ô¨Årstly proceeds feed-forward
calculation and back propagation without updating
parameters to yield the importance of heads. And
after picking the most important heads by sorting,
mask them. During training, we only use the rest
part of networks to reach the Ô¨Ånal loss and update
parameters, as is shown in algorithm 2.
Algorithm 2 HeadMask: Mask Important Ones
Input:q;k;v for attention, number of masks n
Output: masked context
1:forbatch in datasets do
2: calculate Lby feed-forward
3: back propagation without updating params
4: calculate importance of all heads I
5: heads = argmaxn(I)
6: forhead in heads do
7:head = 0
8: end for
9: context = attn( )
10: calculate Lby feed-forward
11: back propagation and update params
12:end for

--- PAGE 3 ---
4 Experiments
4.1 Datasets and Systems
We conduct experiments on four datasets, including
three low-resource ones (less than 1 million). We
use BPE (Sennrich et al., 2016) for Zh-En (Zheng
et al., 2018) and Ro-En, adopt the preprocessed
versions from Luong and Manning (2015) as well
as the settings of Huang et al. (2017) for Vi-En,
and follow the joint-BPE settings of Sennrich et al.
(2017) for Tr-EN. More information is in Table 1.
Datasets Scale Dev Test
NIST Zh-En 1.34M MT03 MT04/05/06
WMT16 Ro-En 608K newstest2015 newstest2016
IWSLT15 Vi-En 133K tst2012 tst2013
WMT17 Tr-En 207K newstest2016 newstest2017
Table 1: The information of our datasets
We follow Transformer base setting (Vaswani
et al., 2017; Sun et al., 2022). Parameters are op-
timized by Adam (Kingma and Ba, 2015), with
1= 0:9,2= 0:98, and= 10 9. The
learning rate is scheduled according to Vaswani
et al. (2017), with warmup _steps = 4000 . Label
smoothing (Szegedy et al., 2016) of value=0.1 and
dropout (Srivastava et al., 2014) of value=0.1 are
also adopted.
Comparison We compare the baseline with
masking randomly (Random-N) and masking im-
portant ones (Impt-N), where N is the mask number.
In this paper, we mainly employ N= 18(12:5%).
4.2 Results
As is shown in Table 2,3,4, except for Vi-En ex-
periments, Impt-18 yields enhancement over all
language directions and reach the best result on the
experiment of Ro !En. And Random-18 obtains
steady improvements over all pairs and is obviously
better than Impt-18. It seems the aggressive mask-
ing strategy at important heads can be too harshand reversely restrict the model. And the random
method is more expert in building a rational train-
ing pattern. In conclusion, reducing the unbalanced
training among attention heads can effectively im-
prove the translation quality.
Test sets MT04 MT05 MT06
Baseline 46.62 43.46 43.09
Impt-18 46.94 (+0.28) 44.19 (+0.73) 43.16 (+0.07)
Random-18 47.04 (+0.42) 44.33 (+0.87) 43.88 (+0.79)
Table 2: Results on Experiments of Zh !En
Directions Ro !En Vi !En Tr !En
Baseline 32.17 26.49 17.29
Impt-18 32.95 (+0.78) 26.36 (-0.13) 17.48 (+0.19)
Random-18 32.85 (+0.68) 26.85 (+0.36) 17.56 (+0.27)
Table 3: Results on Experiments of Ro/Vi/Tr !En
Directions En !Ro En !Vi En !Tr
Baseline 31.98 28.07 15.74
Impt-18 32.47 (+0.49) 28.06 (-0.01) 16.10 (+0.36)
Random-18 32.64 (+0.66) 28.46 (+0.39) 16.16 (+0.42)
Table 4: Results on Experiments of En !Ro/Vi/Tr
4.3 Statistical Analysis
4.3.1 Flatter Distribution
To evaluate the adjusted training of heads, we check
the distribution of head importance. As is shown
in Figure 3, our methods make the importance dis-
tribution Ô¨Çatter. And the overall variance and mean
are also calculated, as is shown in Table 5,6. Com-
pared with Baseline, Impt-18 and Random-18 sig-
niÔ¨Åcantly reduce the variance of attention heads,
achieving the goal of more equal training. And the
mean also decreases, which proves the decline of
dependence on every individual head. More speciÔ¨Å-
cally, Impt-18 can better resolve the imbalance, for
it well prevent the emergence of ‚Äúsuper‚Äù heads.
1                 18                 36                 54                 72                 90                 108                 126                144
Heads sorted by importance01020304050607080ImportanceDistribution of head importance
Baseline
(a) Baseline
1                 18                 36                 54                 72                 90                 108                 126                144
Heads sorted by importance01020304050607080ImportanceDistribution of head importance
Random-18 (b) Random-18
1                 18                 36                 54                 72                 90                 108                 126                144
Heads sorted by importance01020304050607080ImportanceDistribution of head importance
Impt-18 (c) Impt-18
Figure 3: Distribution of importance of attention heads. Our methods make the whole distribution much Ô¨Çatter.

--- PAGE 4 ---
None
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Mask different groups of heads (sorted by importance)20253035404550BLEUBLEU with different head masks
Baseline(a) Baseline
None
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Mask different groups of heads (sorted by importance)20253035404550BLEUBLEU with different head masks
Random-18 (b) Random-18
None
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Mask different groups of heads (sorted by importance)20253035404550BLEUBLEU with different head masks
Impt-18 (c) Impt-18
Figure 4: Our methods signiÔ¨Åcantly maintain the performance even if the important heads are masked.
0
12.5%25%37.5%50%62.5%75%87.5% 100%
Proportion of head mask coverage01020304050BLEU
BLEU with different proportion of head mask coverage
Baseline      (unimportant heads first)
Baseline      (important heads first)
Random-18 (important heads first)
Impt-18       (important heads first)
(a) Mask 18 heads in training
0
12.5%25%37.5%50%62.5%75%87.5% 100%
Proportion of head mask coverage01020304050BLEU
BLEU with different proportion of head mask coverage
Baseline      (unimportant heads first)
Baseline      (important heads first)
Random-36 (important heads first)
Impt-36       (important heads first) (b) Mask 36 heads in training
0
12.5%25%37.5%50%62.5%75%87.5% 100%
Proportion of head mask coverage01020304050BLEU
BLEU with different proportion of head mask coverage
Baseline      (unimportant heads first)
Baseline      (important heads first)
Random-54 (important heads first)
Impt-54       (important heads first) (c) Mask 54 heads in training
Figure 5: As the number of masked heads grows, the drop curves starting with important heads are moving up.
Directions Zh2En Ro2En Vi2En Tr2En
Baseline 77.28 552.93 100.73 1767.70
Random-18 33.21 255.98 48.28 900.70
Impt-18 9.13 72.73 14.13 188.87
Table 5: Our methods greatly reduce the Variance of the head
importance, illustrating the improved equality of heads.
Directions Zh2En Ro2En Vi2En Tr2En
Baseline 27.15 47.18 17.96 83.79
Random-18 19.62 39.96 14.86 74.05
Impt-18 18.95 37.30 18.96 85.12
Table 6: Our methods reduce the Mean of the head impor-
tance, illustrating the lessened dependence on each head.
4.3.2 Weaker Dependence
We repeat the experiments of masking different
groups of heads. As is shown in Figure 4, the
translation quality is still maintained even if impor-
tant heads are masked, proving the dependence on
them has decreased. And Impt-18 performs more
steadily since it is accustomed to such situations.
4.3.3 More Robust Models
We also repeat the experiments of masking all
heads, as is shown in Figure 5. The two middle
lines originally lie in the same place as the bottom
one. As the number of masked heads in training (N)
grows, they gradually move up and approach the
top line where unimportant heads are masked Ô¨Årst.It shows our methods make the model rely less on
the important heads and become more robust.
5 Related Works
Recently, many analytical works about multi-head
attention come out (Raganato and Tiedemann,
2018; Tang et al., 2018; V oita et al., 2019; Michel
et al., 2019; Sun et al., 2020; Behnke and HeaÔ¨Åeld,
2020). And for the inequality of the networks,
some studies focus on the model level (Frankle and
Carbin, 2019; Sun et al., 2021), layer level (Zhang
et al., 2019), and neuron level (Bau et al., 2019).
For the mask algorithm, there are also works on the
layer level (Fan et al., 2020), word level (Provilkov
et al., 2019), and neuron level (Srivastava et al.,
2014). Different from them, we mainly study the
attention level and conduct a statistical analysis.
6 Conclusion
In this paper, we empirically validate the inequal-
ity of attention heads in Transformer and come up
with an assumption of imbalanced training. Corre-
spondingly, we propose a speciÔ¨Åc method in two
ways to resolve the issue. Experiments show the
improvements on multiple language pairs. And de-
tailed analysis shows the alleviation of the problem
and the effectiveness of our techniques.

--- PAGE 5 ---
7 Acknowledgements
We would like to thank the anonymous review-
ers for their insightful comments. Shujian Huang
is the corresponding author. This work is sup-
ported by National Science Foundation of China
(No. 6217020152).
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR .
Antonio Valerio Miceli Barone, Jindrich Helcl, Rico
Sennrich, Barry Haddow, and Alexandra Birch.
2017. Deep architectures for neural machine trans-
lation. In WMT .
Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir
Durrani, Fahim Dalvi, and James Glass. 2019. Iden-
tifying and controlling important neurons in neural
machine translation. In ICLR .
Maximiliana Behnke and Kenneth HeaÔ¨Åeld. 2020. Los-
ing heads in the lottery: Pruning transformer atten-
tion in neural machine translation. In EMNLP .
Angela Fan, Edouard Grave, and Armand Joulin. 2020.
Reducing transformer depth on demand with struc-
tured dropout. In ICLR .
Jonathan Frankle and Michael Carbin. 2019. The lot-
tery ticket hypothesis: Finding sparse, trainable neu-
ral networks. In ICLR .
Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann Dauphin. 2017. Convolutional se-
quence to sequence learning. In ICML .
Po-Sen Huang, Chong Wang, Dengyong Zhou, and
Li Deng. 2017. Neural phrase-based machine trans-
lation. arXiv , abs/1706.05565.
Diederick P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR .
Minh-Thang Luong and Christopher D Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In IWSLT .
Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? In
NeurIPS .
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo
Aila, and Jan Kautz. 2017. Pruning convolutional
neural networks for resource efÔ¨Åcient inference. In
ICLR .
Ivan Provilkov, Dmitrii Emelianenko, and Elena V oita.
2019. Bpe-dropout: Simple and effective subword
regularization. arXiv , abs/1910.13267.Alessandro Raganato and J√∂rg Tiedemann. 2018.
An analysis of encoder representations in
transformer-based machine translation. In Black-
boxNLP@EMNLP .
Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth HeaÔ¨Åeld, An-
tonio Valerio Miceli Barone, and Philip Williams.
2017. The university of edinburgh‚Äôs neural mt sys-
tems for wmt17. In WMT .
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In ACL.
Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overÔ¨Åtting. JMLR , 15(1):1929‚Äì
1958.
Zewei Sun, Shujian Huang, Hao-Ran Wei, Xin-yu Dai,
and Jiajun Chen. 2020. Generating diverse transla-
tion by manipulating multi-head attention. In AAAI .
Zewei Sun, Mingxuan Wang, and Lei Li. 2021. Multi-
lingual translation via grafting pre-trained language
models. In EMNLP .
Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi
Zhao, Shujian Huang, Jiajun Chen, and Lei Li. 2022.
Rethinking document-level neural machine transla-
tion. In ACL.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. 2016. Re-
thinking the inception architecture for computer vi-
sion. In CVPR .
Gongbo Tang, Rico Sennrich, and Joakim Nivre. 2018.
An analysis of attention mechanisms: The case of
word sense disambiguation in neural machine trans-
lation. In WMT .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS .
Elena V oita, David Talbot, F. Moiseev, Rico Sennrich,
and Ivan Titov. 2019. Analyzing multi-head self-
attention: Specialized heads do the heavy lifting, the
rest can be pruned. In ACL.
Biao Zhang, Ivan Titov, and Rico Sennrich. 2019. Im-
proving deep transformer with depth-scaled initial-
ization and merged attention. In EMNLP-IJCNLP .
Zaixiang Zheng, Shujian Huang, Zewei Sun, Rongx-
iang Weng, Xinyu Dai, and Jiajun Chen. 2018.
Learning to discriminate noises for incorporating
external information in neural machine translation.
arXiv , abs/1810.10317.
