# 2307.14995.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2307.14995.pdf
# File size: 895654 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Technical Report
TRANS NORMER LLM: A F ASTER AND BETTER LARGE
LANGUAGE MODEL WITH IMPROVED TRANS NORMER
Zhen Qin‚ôØ, Dong Li‚ôØ, Weigao Sun‚ôØ, Weixuan Sun‚ôØ, Xuyang Shen‚ôØ,
Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, Yiran Zhong‚àó
OpenNLPLab, Shanghai AI Laboratory
https://github.com/OpenNLPLab/TransnormerLLM
ABSTRACT
We present TransNormerLLM, the first linear attention-based Large Language
Model (LLM) that outperforms conventional softmax attention-based models in
terms of both accuracy and efficiency. TransNormerLLM evolves from the pre-
vious linear attention architecture TransNormer (Qin et al., 2022a) by making
advanced modifications that include positional embedding, linear attention ac-
celeration, gating mechanism, tensor normalization, and inference acceleration
and stabilization. Specifically, we use LRPE (Qin et al., 2023b) together with an
exponential decay to avoid attention dilution issues while allowing the model to
retain global interactions between tokens. Additionally, we propose Lightning
Attention, a cutting-edge technique that accelerates linear attention by more than
twice in runtime and reduces memory usage by a remarkable four times. To further
enhance the performance of TransNormer, we leverage a gating mechanism to
smooth training and a new tensor normalization scheme to accelerate the model,
resulting in an impressive acceleration of over 20%. Furthermore, we develop a
robust inference algorithm that ensures numerical stability and consistent inference
speed, regardless of the sequence length, showcasing superior efficiency during
both training and inference stages. We also implement an efficient model paral-
lel schema for TransNormerLLM, enabling seamless deployment on large-scale
clusters and facilitating expansion to even more extensive models, i.e.,LLMs with
175B parameters. We validate our model design through a series of ablations
and train models with sizes of 385M, 1B, and 7B on our self-collected corpus.
Benchmark results demonstrate that our models not only match the performance of
state-of-the-art LLMs with Transformer but are also significantly faster.
1 I NTRODUCTION
The field of Natural Language Processing (NLP) has been revolutionized by the advent of large-scale
language models (LLMs) (Touvron et al., 2023a; Biderman et al., 2023; Brown et al., 2020). These
models have demonstrated exceptional performance across a multitude of tasks, elevating abilities to
comprehend, generate, and interact with human languages in computational frameworks. Previous
language modeling development has predominantly centered around Transformer architectures, with
seminal models such as vanilla Transformer (Vaswani et al., 2017), GPT series (Radford et al., 2018;
2019; Brown et al., 2020), BERT (Devlin et al., 2019), and BART (Lewis et al., 2019) standing as
standard backbones in related fields. The success of Transformer architectures is premised on the
softmax attention mechanism, which discerns dependencies among input tokens in a data-driven
scheme and has global position awareness, offering the model an effective way to handle the long-
range dynamism of natural language.
Nevertheless, conventional Transformers are not without their constraints. Primarily, their quadratic
time complexity with respect to the sequence length limits their scalability and hampers efficiency
in terms of computational resources and time during the training and inference stages. Numerous
efficient sequence modeling methods have been proposed in an attempt to reduce the quadratic time
complexity to linear (Katharopoulos et al., 2020; Choromanski et al., 2021; Qin et al., 2022b; Zheng
‚àóYiran Zhong is the corresponding author. Email: zhongyiran@gmail.com .‚ôØequal contribution.
1arXiv:2307.14995v2  [cs.CL]  19 Jan 2024

--- PAGE 2 ---
Technical Report
et al., 2023; 2022). However, there are two reasons that prohibit them to be applied to LLMs: 1)
their performance in language modeling is often unsatisfactory; 2) they do not demonstrate speed
advantages in real-world scenarios.
In this paper, we introduce TransNormerLLM, the first linear attention-based LLM that surpasses con-
ventional softmax attention in both accuracy and efficiency. The development of TransNormerLLM
builds upon the foundations of the previous linear attention architecture, TransNormer (Qin et al.,
2022a), while incorporating a series of advanced modifications to achieve superior performance. The
key enhancements in TransNormerLLM include positional embedding, linear attention acceleration,
gating mechanism, tensor normalization, and inference acceleration.
One notable improvement is the replacement of the TransNormer‚Äôs DiagAttention with Linear
Attention to enhance global interactions. To address the issue of dilution, we introduced LRPE (Qin
et al., 2023b) with exponential decay (Press et al., 2022; Qin et al., 2023a; Peng et al., 2023a).
Lightning Attention, a novel technique that significantly accelerates linear attention during training is
introduced, resulting in a more than two-fold improvement, while also reducing memory usage by
four times with IO awareness. Furthermore, we simplified GLU and Normalization, with the latter
leading to a 20% speedup. A robust inference algorithm ensures the stability of numerical values and
constant inference speed, regardless of the sequence length, thereby enhancing the efficiency of our
model during both training and inference stages.
We validate the efficacy of TransNormerLLM on our self-collected pre-train corpus, which is more
than6TB in size and contains over 2trillion tokens. We expand the original TransNormer model,
ranging from 385M to 175B parameters, and benchmark models with sizes of 385M, 1B, and 7B.
The benchmark results demonstrate that our models achieve competitive performance with existing
state-of-the-art transformer-based LLMs with similar sizes while also having faster inference speeds.
We will open-source our pre-trained models, enabling researchers and practitioners to build upon our
work and explore efficient transformer structures in LLMs.
2 R ELATED WORK
2.1 T RANSFORMER -BASED LLM S
In recent years, the field of Large Language Models (LLMs) has experienced significant advance-
ments. Adhering to the scaling laws (Kaplan et al., 2020), various LLMs with over 100 billion
parameters have been introduced, such as GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2022),
PaLM (Chowdhery et al., 2022), GLM (Du et al., 2022) and etc.. More specialized models like
Galactica (Taylor et al., 2022) have also emerged for specific domains like science. A notable
development is Chinchilla (Hoffmann et al., 2022), an LLM model with 70 billion parameters that
redefines these scaling laws, focusing on the number of tokens rather than model weights. Further-
more, LLaMA (Touvron et al., 2023a) has also sparked interest due to its promising performance
and open-source availability. The discourse around LLMs also encompasses the dynamics between
open-source and closed-source models. Open-source models such as BLOOM (Workshop et al.,
2023), OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023a), Pythia (Biderman et al., 2023) and
Falcon (Penedo et al., 2023) are rising to compete against their closed-source counterparts, including
GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022). To speed up training, Sparse
Attention (Child et al., 2019; Beltagy et al., 2020) was introduced, but among large models, only
GPT-3 adopted it (Brown et al., 2020; Scao et al., 2022).
2.2 N ON-TRANSFORMER -BASED LLM SCANDIDATES
Despite the proliferation of Transformer-based large models in the research community, a portion
of recent work has prioritized addressing its square time complexity. This focus has led to the
exploration and development of a series of model architectures that diverge from the traditional
Transformer structure. Among them, four significant contenders‚Äîlinear transformers, state space
model, long convolution, and linear recurrence‚Äîhave shown promising results as substitutes for
self-attention (SA) modules when modeling long sequences. These alternatives are favored for their
superior asymptotic time complexity and competitive performances.
Linear Transformer Linear Transformer decomposes Softmax Attention into the form of the inner
product of hidden representations, which allows it to use the "Right Product Trick," where the product
2

--- PAGE 3 ---
Technical Report
of keys and values is computed to avoid the quadratic n√ónmatrix. Different methods utilize various
hidden representations. For example, Katharopoulos et al. (2020) use 1+elu as an activation function,
Qin et al. (2022b) use the cosine function to approximate the properties of softmax, and Ke et al.
(2021); Zheng et al. (2022; 2023) approximate softmax through theoretical approaches. Although its
theoretical complexity is O(nd2), the actual computational efficiency of Linear Attention becomes
quite low when used in causal attention due to the need for cumsum operations (Hua et al., 2022).
On the other hand, most Linear Transformers still exhibit a certain performance gap compared to
traditional Transformers (Katharopoulos et al., 2020; Liu et al., 2022).
State Space Model State Space Model is based on the State Space Equation for sequence mod-
eling (Gu et al., 2022b), using special initialization (Gu et al., 2020; 2022a), diagonalization as-
sumptions (Gupta et al., 2022), and some techniques (Dao et al., 2022b) to achieve performance
comparable to Transformers. On the other hand, due to the characteristics of the State Space Equation,
it enables inference to be conducted within constant complexity (Gu et al., 2022b).
Long Convolution Long convolution models (Qin et al., 2023a; Fu et al., 2023) utilize a kernel size
equal to the input sequence length, facilitating a wider context compared to traditional convolutions.
Training these models involves the efficient O(nlogn)Fast Fourier Transforms (FFT) algorithm.
However, long convolutions pose certain challenges, such as the need for causal convolution inference,
which necessitates caching all historical computations similar to SA‚Äôs key-value (KV) cache. The
memory requirements for handling long sequences, coupled with the higher inference complexity
compared to RNNs, make them less ideal for processing long sequences.
Linear RNN Linear RNNs (Orvieto et al., 2023; Peng et al., 2023b), in contrast, stand out as more
suitable replacements for SA in long-sequence modeling. A notable example is the RWKV (Peng
et al., 2023b) model, a linear RNN-based LLM that has shown competitive performance against
similarly scaled GPT models.
3 T RANS NORMER LLM
3.1 A RCHITECTURE IMPROVEMENT
In this section, we thoroughly investigate each module of the network and propose several improve-
ments to achieve an optimal balance between efficiency and performance. Below, we outline the key
designs of each block along with the inspiration behind each change. For the details of configurations
for TransNormerLLM variants from 385M to 175B parameters, see Appendix A.
3.1.1 I MPROVEMENT 1: P OSITION ENCODING
In TransNormer, DiagAttention is used at the lower layers to avoid dilution issues. However, this
leads to a lack of global interaction between tokens. In TransNormerLLM, we leverage LRPE (Qin
et al., 2023b) with exponential decay (Press et al., 2022; Qin et al., 2023a; Peng et al., 2023b) to
address this issue, retaining full attention at the lower layers. The expression of our position encoding
is as follows:
ast=q‚ä§
sktŒªs‚àítexpiŒ∏(s‚àít). (1)
which we call LRPE-d - Linearized Relative Positional Encoding with exponential decay. Similar to
the original LRPE, we set Œ∏to be learnable. We empirically find that rather than applying LRPE-d to
every layer, applying it to the first layer and keeping other layers with exponential decay can speed
up training by approximately 15-20% but only with a subtle effect on the performance.
Note that this position encoding is fully compatible with Linear Attention, as it can be decomposed
with respect to sandtseparately. The value of Œªfor the h-th head in the l-th layer (assuming there
are a total of Hheads and Llayers) is given by:
Œª= exp 
‚àí8h
H√ó 
1‚àíl
L
. (2)
Here,8h
Hcorresponds to the decay rate of the h-th head, while 
1‚àíl
L
corresponds to the decay rate
of the l-th layer. The term 
1‚àíl
L
ensures that the Theoretical Receptive Fields (TRF) (Qin et al.,
2023c) at the lower layers is smaller compared to the higher layers, which aligns with TransNormer‚Äôs
motivation. It should be noted that the decay rate in the last layer is set to 1, allowing each token
to attend to global information. We choose Œªto be non-learnable since we empirically found that
gradients become unstable when Œªis learnable, leading to NaN values.
3

--- PAGE 4 ---
Technical Report
3.1.2 I MPROVEMENT 2: G ATING MECHANISM
Gate can enhance the performance of the model and smooth the training process. In TransNormer-
LLM, we adopted the approach from Flash (Hua et al., 2022) and used the structure of Gated Linear
Attention (GLA) in token mixing:
TokenMixer : O= Norm( QK‚ä§V)‚äôU, (3)
where: Q=œï(XW q),K=œï(XW k),V=XW v,U=XW u. (4)
We choose œïto be swish (Ramachandran et al., 2017) activation function as we empirically find that
it outperforms other activation functions, as shown in Table 6.
To further accelerate the model, we propose Simple GLU (SGLU), which removes the activation
function from the original GLU structure as the gate itself can introduce non-linearity. Therefore, our
channel mixing becomes:
ChannelMixer : O= [V‚äôU]Wo,V=XW v,U=XW u, (5)
We empirically find that not using an activation function in GLU will not lead to any performance
loss, as demonstrated in Table 7.
3.1.3 I MPROVEMENT 3: T ENSOR NORMALIZATION
We employ the NormAttention introduced in TransNormer (Qin et al., 2022a) as follows:
O= Norm(( QK‚ä§)V) (6)
This attention mechanism eliminates the softmax and scaling operation. Moreover, it can be trans-
formed into linear attention through right multiplication:
O= Norm( Q(K‚ä§V)) (7)
This linear form allows for recurrent prediction with a complexity of O(nd2), making it efficient
during inference. Specifically, we only update K‚ä§Vin a recurrent manner without computing the full
attention matrix. In TransNormerLLM, we replace the RMSNorm with a new simple normalization
function called SimpleRMSNorm, abbreviated as SRMSNorm:
SRMSNorm( x) =x
‚à•x‚à•2/‚àö
d. (8)
We empirically find that using SRMSNorm does not lead to any performance loss, as demonstrated in
the ablation study in Table. 8.
3.1.4 T HE OVERALL STRUCTURE
ùëãSGLUAddSGLU
ùëàùëÑùêæùëâ
SRMSNormGLA**ùëàùëâ
GLASRMSNormAddSRMSNorm
Figure 1: Architecture overview of the proposed
model. Each transformer block is composed of
a Gated Linear Attention(GLA) for token mixing
and a Simple Gated Linear Unit (SGLU) for chan-
nel mixing. We apply pre-norm for both modules.The overall structure is illustrated in Figure 1.
In this structure, the input Xis updated through
two consecutive steps: First, it undergoes Gated
Linear Attention (GLA) with the application of
SimpleRMSNorm (SRMSNorm) normalization.
Then, it goes through the Simple Gated Linear
Unit (SGLU) with SRMSNorm normalization
again. This overall architecture helps improve
the model‚Äôs performance based on the PreNorm
approach. The pseudo-code of the overall pro-
cess is as follows:
X=X+ GLA(SRMSNorm( X)),
X=X+ SGLU(SRMSNorm( X)).(9)
3.2 T RAINING OPTIMIZATION
3.2.1 L IGHTNING ATTENTION
The structure of linear attention allows for ef-
ficient attention calculation with a complexity
4

--- PAGE 5 ---
Technical Report
ofO(nd2)through right-multiplication. However, for causal prediction, right-multiplication is not
efficient as it necessitates cumsum computation (Hua et al., 2022), which hinders parallelism training.
As a result, during training, we continue to use the conventional left-multiplication version. To
accelerate attention calculations, we introduce the Lightning Attention algorithm inspired by (Dao,
2023; Dao et al., 2022a), which makes our linear attention IO-friendly. It computes the following:
O= (QK‚ä§‚äôM)V. (10)
Here,Mis the attention mask which enables lower triangular causal masking and positional encoding.
In the Lightning Attention, we split the inputs Q,K,Vinto blocks, load them from slow HBM to
fast SRAM, then compute the attention output with respect to those blocks. Then we accumulate
the final results. The computation speed is accelerated by avoiding the operations on slow HBM.
The implementation details of Lightning Attention are shown in Appendix B, where Algorithm 3 for
forward pass and Algorithm 4 for backward pass.
3.2.2 M ODEL PARALLELISM ON TRANS NORMER LLM
To effectively execute large-scale pre-training for TransNormerLLM, we have put efforts on sys-
tem optimization encompassing various dimensions. Specifically, we employ fully sharded data
parallelism (FSDP) (Zhao et al., 2023), a technique that shards all model parameters, gradients, and
optimizer state tensors across the entire cluster. This strategic partition significantly reduces the
memory footprint on each individual GPU, thereby enhancing memory utilization. In our pursuit of
greater efficiency, we leverage activation checkpointing (Shoeybi et al., 2019), which minimizes the
cached activations in memory during the forward pass. Instead of retaining these activations, they
are recomputed when calculating gradients in the backward pass. This approach saves huge GPU
memory thus enable to apply bigger batch size. Furthermore, we harness automatic mixed precision
(AMP) (Micikevicius et al., 2017) to simultaneously save GPU memory and expedite computational
speed. It‚Äôs noteworthy that in our experimental setup, we employ BFloat16 (Kalamkar et al., 2019)
due to its observed advantage in enhancing the training stability of TransNormerLLM models.
In addition to the previously mentioned optimization endeavors, we delve deeper into the realm of
system engineering by implementing model parallelism specifically tailored to linear transformers,
drawing inspiration from Megatron-LM model parallelism (Shoeybi et al., 2019). In a standard
transformer model, each transformer layer comprises a self-attention block followed by a two-layer
multi-layer perceptron (MLP) block. Megatron-LM model parallelism independently addresses these
two constituent blocks. Similarly, within the architecture of TransNormerLLM, characterized by its
two primary components, SGLU and GLA, we apply model parallelism to each of these components
separately. The intricate details of our model parallelism strategies are elaborated below.
Model Parallelism on SGLU Recall the SGLU structure in (5):
O= [(XW v)‚äô(XW u)]Wo, (11)
The model parallelism adaptation of SGLU is as follows:
[O‚Ä≤
1,O‚Ä≤
2] =X[W1
v,W2
v]‚äôX[W1
u,W2
u],= [XW1
v,XW2
v]‚äô[XW1
u,XW2
u], (12)
which splits the weight matrices WvandWualong their columns and obtains an output matrix
splitting along its columns too. Then the split output [O1,O2]is multiplied by another matrix which
is split along its rows as:
O= [O‚Ä≤
1,O‚Ä≤
2][W1
o,W2
o]‚ä§=O‚Ä≤
1W1
o+O‚Ä≤
2W2
o (13)
Similar with model parallelism in Megatron-LM, this whole procedure splits three general matrix
multiplies (GEMMs) inside the SGLU block across multiple GPUs and only introduces a single
all-reduce collective communication operation in both the forward and backward passes, respectively.
Model Parallelism on GLA Recall the GLA block in (3) and (4), its model parallelism version is:
[O1,O2] = SRMSNorm( QK‚ä§V)‚äôU, (14)
where:
Q= [œï(XW1
q), œï(XW2
q)],K= [œï(XW1
q), œï(XW2
q)],V=X[W1
v,W2
v],U=X[W1
u,W2
u],(15)
Note that in our implementation, we use the combined QKVU projection to improve computation
efficiency for linear attention. The obtained split output matrix [O1,O2]again is multiplied by a
weight matrix split along its columns which is similar to (13).
5

--- PAGE 6 ---
Technical Report
Algorithm 1 Origin Inference Algorithm
Input: qt,kt,vt, t= 1, . . . , n ;
Output: ot, t= 1, . . . , n ;
Initialize: [kv]0=0;
fort= 1, . . . , n do
[kv]t= [kv]t‚àí1+ktŒª‚àítv‚ä§
t,
ot=qtŒªt[kv]t.
end forAlgorithm 2 Robust Inference Algorithm
Input: qt,kt,vt, t= 1, . . . , n ;
Output: ot, t= 1, . . . , n ;
Initialize: [kv]0=0;
fort= 1, . . . , n do
[kv]t=Œª[kv]t‚àí1+ktv‚ä§
t,
ot=qt[kv]t.
end for
3.3 R OBUST INFERENCE
In this section, we discuss the inference problem in TransNormerLLM. It is important to note that the
formula 1 can be decomposed into the following form:
ast= (qsŒªsexpiŒ∏s)‚ä§(ktŒª‚àítexpiŒ∏t). (16)
This allows TransNormerLLM to perform inference in the form of an RNN. Details of the procedure
are shown in Algorithm 1. However, it is worth noting that Œª <1, which results in:
‚à•qsŒªsexpiŒ∏s‚à•2=‚à•qs‚à•2Œªs‚Üí0,‚à•ktŒª‚àítexpiŒ∏t‚à•2=‚à•kt‚à•2Œª‚àít‚Üí ‚àû , (17)
leading to numerical precision issues.
To avoid these issues, we propose a Robust Inference Algorithm in 2. Since ‚à•qsexpiŒ∏s‚à•=‚à•qs‚à•,
‚à•ktexpiŒ∏t‚à•=‚à•kt‚à•, for simplicity, we will omit LRPE (Qin et al., 2023b) in the subsequent
discussions, considering only ast=q‚ä§
sktŒªs‚àít.We provide a mathematical proof of [kv]t=
Œª‚àít[kv]tin Appendix C
4 E XPERIMENTS
We use PyTorch (Paszke et al., 2019) and Triton (Tillet et al., 2019) to implement TransNormerLLM
in Metaseq framework (Zhang et al., 2022). Our model is trained using Adam optimizer (Kingma &
Ba, 2017), and we employ FSDP to efficiently scale our model to NVIDIA A100 80G clusters. We
additionally leverage the model parallel as appropriate to optimize performance. In ablation studies,
all models are trained on a sampled corpus from our corpus with 300B tokens. In order to reduce
the fluctuation of Losses and PPLs in the tables below, we compute the average Losses and PPLs of
the last 1k iterations as the final metrics. For our benchmark models, we train our 385M, 1B, and
7B models on our corpus for 1 trillion, 1.2 trillion, and 1.4 trillion tokens respectively. We use an
input sequence length of 8192 tokens in our pretraining process. For a comprehensive understanding
of our corpus, encompassing intricate details such as data preprocessing methods and tokenization
procedures, we direct interested readers to Appendix D.
4.1 A RCHITECTURE ABLATIONS
Transformer vsTransNormerLLM We carried out a meticulous series of comparative tests
between our TransNormerLLM and Transformer, spanning over an array of disparate sizes. The com-
parative performance of these models is clearly illustrated in Table 1. Under identical configurations,
it becomes evident that our TransNormerLLM exhibits a superior performance profile compared to
Transformer. We observed that TransNormerLLM outperformed Transformer by a remarkable 5%
at the size of 385M. More importantly, as the size reached 1B, this superiority became even more
pronounced, with an advantage of 9% for TransNormerLLM over Transformer.
Table 1: Transformer vsTransNormerLLM. TransNormerLLM performs better than Transformer in size of
385M and 1B under identical configurations by 5% and 9%, respectively.
Model Size 385M 1B
Method Updates Loss PPL Updates Loss PPL
Transformer 100K 2.362 5.160 100K 2.061 4.765
TransNormerLLM 100K 2.248 4.770 100K 1.896 3.729
Table 2: TransNormer vsTransNormerLLM.
Method Params Updates Loss PPL
TransNormerLLM 385M 100K 2.248 4.770
TransNormer-T1 379M 100K 2.290 4.910
TransNormer-T2 379M 100K 2.274 4.858TransNormer vsTransNormerLLM We
compare the original TransNormer and the im-
proved TransNormerLLM and the results are
shown in Table 2. TransNormerLLM exhibited
an enhancement of 2% and 1% respectively.
6

--- PAGE 7 ---
Technical Report
Table 3: Positional encoding. LRPE-d leads to
the most optimal outcome.
PE Methods Params Updates Loss PPL
Mix 385M 100K 2.248 4.770
APE 386M 100K 2.387 5.253
Exp-Decay 385M 100K 2.267 4.834
LRPE 385M 100K 2.287 4.899
LRPE-d 385M 100K 2.236 4.728Positional Encoding In the positional encod-
ing experiment, we conducted a series of tests,
comparing Mix (LRPE-d for the first layer, Exp-
Decay for the rest), APE (Absolute Positional
Encoding), LRPE, Exp-Decay (Exponential De-
cay), and LRPE-d. As evident from Table 3,
Ours and LRPE-d achieve better performance
than other options. We select the Mix positional
encoding as it boosts the training speed up to
20% while only slightly worse than LRPE-d.Table 4: Ablations on decay temperature. The
results of decay temperature proved to be superior.
Temperature Params Updates Loss PPL
w/ temperature 385M 100K 2.248 4.770
w/o temperature 385M 100K 2.258 4.804We also perform ablations on the decay tem-
perature 
1‚àíl
L
in Eq. 2. The perplexity of
the TransNormerLLM is reduced by adding the
decay temperature, as shown in Table 4.
Table 5: Ablations on gating mechanism. The
performance with the gate proved to be superior.
Gate Params Updates Loss PPL
w/ gate 385M 100K 2.248 4.770
w/o gate 379M 100K 2.263 4.820Gating Mechanism We conduct ablation stud-
ies to examine the effect of including the gating
mechanism. As observed in Table 5, gate en-
abled the reduction of the loss value from 2.263
to 2.248.
Table 6: Ablations on GLA activation functions.
The results obtained from different activation func-
tions were virtually identical.
GLA Act Params Updates Loss PPL
Swish 385M 100K 2.248 4.770
No Act 385M 100K 2.283 4.882
1+elu 385M 100K 2.252 4.767GLA Activation Functions We conducted ex-
periments on the GLA (Gated Linear Attention)
structure with respect to the activation function.
As shown in Table 6, using Swish and 1+elu
leads to similar performance. However, in our
experiments, using 1+elu in our 7B model may
encounter a NaN problem, so we use Swish in
our model.
Table 7: Ablations on GLU activation functions.
The exclusion of the activation function had no
negative impact on the results.
GLU Act Params Updates Loss PPL
No Act 385M 100K 2.248 4.770
Swish 385M 100K 2.254 4.788GLU Activation Functions We conduct an
experiment by removing the activation function
within the Gated Linear Units (GLU) structure.
As shown in Table 7, the results reveal that this
alteration had a negligible impact on the final
outcome. As a result, we decide to adopt the
Simple Gated Linear Units (SGLU) structure in
our final model configuration.
Table 8: Normalization Functions. The deviation
in results among the bellowing normalization func-
tions is minimal.
Norm Type Params Updates Loss PPL
SRMSNorm 385M 100K 2.248 4.770
RMSNorm 385M 100K 2.247 4.766
LayerNorm 385M 100K 2.247 4.765Normalization functions In our study, we
conducted a series of ablation tests employing
various normalization methods including SRM-
SNorm, RMSNorm and LayerNorm. The results
indicate that there is almost no difference among
these methods when applied to TransNormer-
LLM. Nevertheless, during the course of our
testing, we revisited and re-engineered the SRM-
SNorm using Triton. As it is shown in Figure 2, empirical evidence supports that our modification
offers a significant boost in computational speed when operating with larger dimensions, compared
to the PyTorch implementation methods.
Lightning Attention We conducted a speed and memory comparison between our Lightning
Attention and the baseline, which is the PyTorch implementation of the NormAttention (Qin et al.,
2022a). Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass. Baseline
runtime grows quadratically with sequence length, while Lightning Attention operates significantly
faster, at least 2√ófaster than the PyTorch implementation. Figure 3 (right) reports the memory
footprint of Lightning Attention compared to the baseline. The memory footprint of Lightning
Attention grows linearly with sequence length, which is up to 4√ómore efficient than the baseline
when the sequence length is 8192. Our proposed Lightning Attention achieves superior efficiency.
7

--- PAGE 8 ---
Technical Report
0.00.30.60.91.21.51.82.12.4
12825651210242048409681921638432768Runtime (s)Sequence lengthForward PassTriton SRMSNormPyTorch SRMSNorm
0.00.61.21.82.43.03.64.24.85.4
12825651210242048409681921638432768Runtime (s)Sequence lengthBackward PassTriton SRMSNormPyTorch SRMSNorm
0.00.20.40.60.81.01.21.4
512102420484096819216384Runtime (s)Feature dimensionForward PassTriton SRMSNormPyTorch SRMSNorm0.00.51.01.52.02.53.03.5
512102420484096819216384Runtime (s)Feature dimensionBackward PassTriton SRMSNormPyTorch SRMSNorm
Figure 2: Performance Evaluation of SRMSNorm Implementation. The upper figures exhibit the
runtime comparison of the forward pass (left) and backward pass (right) for different sequence lengths,
with a fixed feature dimension of 3072. The lower two figures illustrate the runtime comparison for
various feature dimensions, with a fixed sequence length of 4096.
010203040
5121024204840968192Memory footprint (GB)
Sequence LengthMemory Footprint4x050100150200250
5121024204840968192Runtime (ms)
Sequence LengthRuntime (FwdPass + BwdPass)2xLightningAttnOrignalLinear AttnLightningAttnOrignalLinear Attn
Figure 3: Memory and speed comparison between linear attention and lightning attention. Left:
runtime of forward + backward pass milliseconds for different sequence lengths, with a fixed feature
dimension of 2048. Right: memory footprints of forward + backward pass for different sequence
lengths, with a fixed feature dimension of 2048.
0.03.06.09.012.015.0
25651210242048409681921638432000Runtime (ms)Sequence lengthInference TimeTransNormerLLMTransformer0.03.06.09.012.015.0
25651210242048409681921638432000Memory (GB)Sequence lengthInference Memory FootprintTransNormerLLMTransformer
Figure 4: Inference Time and Memory Footprint. Left: inference runtime measured in milliseconds
across different sequence lengths. Right: memory consumption during inference for varying sequence
lengths. It is noteworthy that as the sequence length increases, TransNormerLLM demonstrates a
consistent inference time and memory footprint.
4.2 B ENCHMARKS
In order to validate the effectiveness of TransNormerLLM, we tested our 385M, 1B, and 7B models
on Commonsense Reasoning Task, MMLU(Hendrycks et al., 2021), CMMLU(Li et al., 2023), and
C-Eval(Huang et al., 2023). For comparison, we selected several open-source models as competitors,
including Transformer-based models such as OPT (Zhang et al., 2022), Pythia (Biderman et al., 2023),
BLOOM (Workshop et al., 2023), GPT-Neo (Black et al., 2022), GPT-J (Wang & Komatsuzaki, 2021),
MPT (Team et al., 2023), Falcon (Almazrouei et al., 2023), LLaMA1/2 (Touvron et al., 2023a;b),
OpenLLAMA v1/v2 (Geng & Liu, 2023), Baichuan 1/2 (Baichuan, 2023), ChatGLM 1/2 (Zeng et al.,
2022; Du et al., 2022), and non-Transformer model RWKV (Peng et al., 2023a). It can be observed
that, compared to these models, TransNormerLLM remains highly competitive.
8

--- PAGE 9 ---
Technical Report
Table 9: Performance Comparison on Commonsense Reasoning and Aggregated Benchmarks.
For a fair comparison, we report competing methods‚Äô results reproduced by us using their released
models. Official results are denoted in italics . PS: parameter size (billion). T: tokens (trillion). HS:
HellaSwag. WG: WinoGrande.
Model PS T BoolQ PIQA HS WG ARC-e ARC-c OBQA MMLU CMMLU C-Eval
OPT 0.35 0.30 57.74 64.58 36.69 52.49 44.02 23.89 28.20 26.02 25.34 25.71
Pythia 0.40 0.30 60.40 67.08 40.52 53.59 51.81 24.15 29.40 25.99 25.16 24.81
BLOOM 0.56 0.35 55.14 64.09 36.97 52.80 47.35 23.98 28.20 24.80 25.35 27.14
RWKV 0.43 - - 67.52 40.90 51.14 52.86 25.17 32.40 24.85 - -
Ours 0.39 1.0 62.14 66.70 46.27 54.46 55.43 27.99 32.40 25.90 25.05 25.24
GPT-Neo 1.3 0.3 61.99 71.11 48.93 54.93 56.19 25.85 33.60 24.82 26.03 23.94
OPT 1.3 0.3 57.77 71.71 53.70 59.35 57.24 29.69 33.20 24.96 24.97 25.32
Pythia 1.4 0.3 60.73 70.67 47.18 53.51 56.99 26.88 31.40 26.55 25.13 24.25
BLOOM 1.1 0.35 59.08 67.14 42.98 54.93 51.47 25.68 29.40 27.30 25.09 26.50
RWKV 1.5 - - 72.36 52.48 54.62 60.48 29.44 34.00 25.77 - -
Falcon 1.0 0.35 61.38 75.14 61.50 60.30 63.38 32.17 35.60 25.28 24.88 25.66
Ours 1.0 1.2 63.27 72.09 56.49 60.38 63.68 35.24 36.60 27.10 25.88 26.01
GPT-J 6.9 0.3 65.44 75.41 66.25 64.09 66.92 36.60 38.20 25.40 26.47 23.39
OPT 6.7 0.3 66.18 76.22 67.21 65.19 65.66 34.64 37.20 24.57 25.36 25.32
Pythia 6.9 0.3 63.46 75.14 63.92 60.77 67.34 35.41 37.00 24.64 25.56 26.40
BLOOM 7.1 0.35 62.91 72.69 62.33 64.01 65.11 33.45 35.80 26.25 24.97 24.25
RWKV 7.4 - - 76.06 65.51 61.01 67.80 37.46 40.20 24.96 - -
MPT 6.9 1.0 73.88 79.43 76.25 68.27 74.79 41.72 42.20 30.80 25.99 24.06
Falcon 7.2 1.5 73.73 79.38 76.3 67.17 74.62 43.60 43.80 27.79 25.73 22.92
Baichuan1 7.0 1.2 70.09 76.01 70.06 64.09 71.72 40.53 38.20 42.30 44.43 42.80
Baichuan2 7.0 2.6 72.72 76.50 72.17 68.35 75.17 42.32 39.60 54.16 57.07 54.00
ChatGLM1 6.7 1.0 74.74 68.88 45.57 52.25 48.78 31.66 36.80 40.63 37.48 40.23
ChatGLM2 7.1 1.4 77.65 69.37 50.51 57.62 59.13 34.30 37.00 45.46 48.80 52.55
OpenLLaMAv1 6.7 1.0 70.43 75.68 69.23 66.69 71.17 38.57 39.00 30.49 25.40 26.09
OpenLLaMAv2 6.7 1.0 72.20 78.84 74.51 65.67 72.39 41.30 41.00 41.29 29.58 30.01
LLaMA1 6.7 1.0 76.50 79.80 76.10 70.10 72.80 47.60 57.20 35.10 25.62 25.72
LLaMA2 6.7 2.0 77.68 78.07 76.02 68.98 76.30 46.33 44.20 45.30 32.96 33.20
Ours 6.8 1.4 75.87 80.09 75.21 66.06 75.42 44.40 63.40 43.10 47.99 43.18
Commonsense Reasoning We report BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2019), SIQA
(Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC
easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and their average.
We report 0-shot results for all benchmarks using LM-Eval-Harness (Gao et al., 2021). All of our
models achieve competitive performance compared to existing state-of-the-art LLMs, showcasing a
remarkable ability to comprehend and apply commonsense reasoning.
Aggregated Benchmarks We report the overall results for MMLU (Hendrycks et al., 2021),
CMMLU (Li et al., 2023), C-Eval (Huang et al., 2023). Official scripts were used for evaluating
MMLU, CMMLU, and C-Eval, with all evaluation results being conducted with a 5-shot setup. In
comparison to top-tier open-source models available in the industry, our models have demonstrated
matched performance in both English and Chinese benchmarks.
4.3 S CALING TO 175B
Furthermore, we have carried out a series of experiments to assess the efficacy of model parallelism
as applied to the TransNormerLLM architecture. The comprehensive outcomes of these experiments
have been thoughtfully presented in Appendix E.1. Moreover, our research extends to the meticulous
evaluation of various cutting-edge system optimization techniques. This evaluation encompasses
their impact on both training speed and context length across models ranging from 7B to 175B in
scale. We have thoughtfully documented the detailed results of these experiments in Appendix E.2.
5 C ONCLUSION
We introduced TransNormerLLM in this paper, an improved TransNormer that is tailored for LLMs.
Our TransNormerLLM consistently outperformed Transformers in both accuracy and efficiency.
Extensive ablations demonstrate the effectiveness of our modifications and innovations in position
encoding, gating mechanism, activation functions, normalization functions, and lightning attentions.
These modifications collectively contribute to TransNormerLLM‚Äôs outstanding performance, posi-
tioning it as a promising choice for state-of-the-art language models. The benchmark results for
models with sizes of 385 million, 1 billion, and 7 billion parameters unequivocally demonstrate that
TransNormerLLM not only matches the performance of current leading Transformer-based Large
Language Models (LLMs) but also enjoys faster inference speeds. We will release our pre-trained
TransNormerLLM models to foster community advancements in efficient LLM.
9

--- PAGE 10 ---
Technical Report
REFERENCES
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojo-
caru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al.
Falcon-40b: an open large language model with state-of-the-art performance. Technical report,
Technical report, Technology Innovation Institute, 2023.
Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305 , 2023.
URLhttps://arxiv.org/abs/2309.10305 .
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer,
2020.
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan,
Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,
Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models
across training and scaling, 2023.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about
physical commonsense in natural language, 2019.
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745 , 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877‚Äì1901, 2020.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers, 2019.
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with
performers. In International Conference on Learning Representations , 2021. URL https:
//openreview.net/forum?id=Ua6zuk0WRH .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon
Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,
Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge,
2018.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
preprint arXiv:2307.08691 , 2023.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. FlashAttention: Fast and
memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing
Systems , 2022a.
10

--- PAGE 11 ---
Technical Report
Tri Dao, Daniel Y . Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R√©. Hun-
gry hungry hippos: Towards language modeling with state space models. CoRR , abs/2212.14052,
2022b. doi: 10.48550/arXiv.2212.14052. URL https://doi.org/10.48550/arXiv.
2212.14052 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pp. 4171‚Äì4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//aclanthology.org/N19-1423 .
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:
General language model pretraining with autoregressive blank infilling, 2022.
Daniel Y . Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri
Rudra, and Christopher R√©. Simple hardware-efficient long convolutions for sequence modeling.
CoRR , abs/2302.06646, 2023. doi: 10.48550/arXiv.2302.06646. URL https://doi.org/10.
48550/arXiv.2302.06646 .
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot
language model evaluation. Version v0. 0.1. Sept , 2021.
Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama. URL: https://github.
com/openlm-research/open_llama , 2023.
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with
optimal polynomial projections, 2020.
Albert Gu, Karan Goel, Ankit Gupta, and Christopher R√©. On the param-
eterization and initialization of diagonal state space models. In NeurIPS ,
2022a. URL http://papers.nips.cc/paper_files/paper/2022/hash/
e9a32fade47b906de908431991440f7c-Abstract-Conference.html .
Albert Gu, Karan Goel, and Christopher R√©. Efficiently modeling long sequences with structured
state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net, 2022b. URL https://openreview.net/
forum?id=uYLFoz1vlAC .
Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured
state spaces. In NeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/
2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.
html .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding, 2021.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,
Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.
Training compute-optimal large language models, 2022.
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv
preprint arXiv:2202.10447 , 2022.
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,
Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A
multi-level multi-discipline chinese evaluation suite for foundation models, 2023.
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,
Sasikanth Avancha, Dharma Teja V ooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,
et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322 , 2019.
11

--- PAGE 12 ---
Technical Report
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,
2020.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International Conference on Machine
Learning , pp. 5156‚Äì5165. PMLR, 2020.
Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In
International Conference on Learning Representations , 2021. URL https://openreview.
net/forum?id=09-528y2Fgf .
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension, 2019.
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy
Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.
Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neural
architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955 , 2022.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740 , 2017.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering, 2018.
Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu,
and Soham De. Resurrecting recurrent neural networks for long sequences, 2023.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems 32 , pp.
8024‚Äì8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb
dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin
Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , Xuzheng He, Haowen Hou, Przemyslaw
Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri,
Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak,
Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv:
Reinventing rnns for the transformer era, 2023a.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin
Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , Xuzheng He, Haowen Hou, Przemyslaw
Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri,
Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak,
Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv:
Reinventing rnns for the transformer era, 2023b.
Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation. In International Conference on Learning Representations , 2022. URL
https://openreview.net/forum?id=R8sQPpGCv0 .
12

--- PAGE 13 ---
Technical Report
Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong.
The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pp. 7025‚Äì7041, Abu Dhabi, United Arab Emirates, December
2022a. Association for Computational Linguistics. URL https://aclanthology.org/
2022.emnlp-main.473 .
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng
Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Confer-
ence on Learning Representations , 2022b. URL https://openreview.net/forum?id=
Bl8CQrx2Up4 .
Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng
Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh
International Conference on Learning Representations , 2023a. URL https://openreview.
net/forum?id=IxmWsm4xrua .
Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng
Kong, and Yiran Zhong. Linearized relative positional encoding. Transactions on Machine
Learning Research , 2023b.
Zhen Qin, Yiran Zhong, and Hui Deng. Exploring transformer extrapolation, 2023c.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/
openai-assets/research-covers/language-unsupervised/language_
understanding_paper.pdf , 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,
Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,
Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron
Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen
Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,
Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,
Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,
Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d‚ÄôAutume,
Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,
Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,
and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher,
2022.
Prajit Ramachandran, Barret Zoph, and Quoc V . Le. Searching for activation functions, 2017.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale, 2019.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense
reasoning about social interactions, 2019.
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella
Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh,
Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy.
What language model to train if you have one million gpu hours?, 2022.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 , 2019.
13

--- PAGE 14 ---
Technical Report
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,
Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science,
2022.
MosaicML NLP Team et al. Introducing mpt-7b: A new standard for open-source, commercially
usable llms, 2023. URL www. mosaicml. com/blog/mpt-7b. Accessed , pp. 05‚Äì05, 2023.
Philippe Tillet, Hsiang-Tsung Kung, and David D. Cox. Triton: an intermediate language and compiler
for tiled neural network computations. Proceedings of the 3rd ACM SIGPLAN International
Workshop on Machine Learning and Programming Languages , 2019.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e
Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language
models. arXiv preprint arXiv:2302.13971 , 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023b.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems , 30, 2017.
Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model,
2021.
BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana
Ili¬¥c, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon, Matthias
Gall√©, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka
Ammanamanchi, Thomas Wang, Beno√Æt Sagot, Niklas Muennighoff, Albert Villanova del Moral,
Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu
Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren√ßon, Yacine
Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa,
Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,
Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani,
Dragomir Radev, Eduardo Gonz√°lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan,
Francesco De Toni, G√©rard Dupont, Germ√°n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza
Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la
Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J√∂rg Frohberg, Joseph Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra, Leon Weber,
Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu√±oz, Maraim
Masoud, Mar√≠a Grandury, Mario ≈†a≈°ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike
Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora
Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter
Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani,
Roberto Luis L√≥pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik
Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor,
Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush,
Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu,
Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta¬∏ sar,
14

--- PAGE 15 ---
Technical Report
Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee, Abheesht Sharma, Andrea Santilli, Antoine
Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit
Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful
Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng
Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala
Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked
Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang,
Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max
Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas
Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran√ßois
Lavall√©e, R√©mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St√©phane Requena,
Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure
Ligozat, Arjun Subramonian, Aur√©lie N√©v√©ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bogdanov, Genta Indra Winata, Hailey
Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo
Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton
Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian
Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz
Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan
Belinkov, Zachary Bamberger, Zden Àáek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour,
Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol,
Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade,
Bharat Saxena, Carlos Mu√±oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis
David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima
Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman,
Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra,
Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael
McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu
Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan,
Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito,
Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl√©mentine Fourrier, Daniel Le√≥n Peri√±√°n, Daniel Molano,
Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak,
Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde,
Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,
Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P√†mies, Maria A Castillo, Marianna
Nezhurina, Mario S√§nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De
Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan
Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya
Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel
Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott,
Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th√©o Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan
Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine
really finish your sentence?, 2019.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 , 2022.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt
Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
Opt: Open pre-trained transformer language models, 2022.
15

--- PAGE 16 ---
Technical Report
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid
Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data
parallel. arXiv preprint arXiv:2304.11277 , 2023.
Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mecha-
nism. In International Conference on Machine Learning , pp. 27011‚Äì27041. PMLR, 2022.
Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong. Efficient attention via control variates.
InInternational Conference on Learning Representations , 2023. URL https://openreview.
net/forum?id=G-uNfHKrj46 .
16

--- PAGE 17 ---
Technical Report
Appendix
A M ODEL
We present distinct model variants of the TransNormerLLM architecture, delineating their respective
configurations with regard to parameters, layers, attention heads, and hidden dimensions. The detailed
specifications are meticulously tabulated in Table 10.
Table 10: TransNormerLLM Model Variants.
Model Size Non-Embedding Params Layers Hidden Dim Heads Equivalent Models
385M 384,974,848 24 1024 8 Pythia-410M
1B 992,165,888 16 2048 16 Pythia-1B
3B 2,876,006,400 32 2560 20 Pythia-2.8B
7B 6,780,547,072 30 4096 32 LLAMA-6.7B
13B 12,620,195,840 36 5120 40 LLAMA-13B
65B 63,528,009,728 72 8192 64 LLAMA-65B
175B 173,356,498,944 88 12288 96 GPT-3
B L IGHTNING ATTENTION
We present the algorithm details of Lightning Attention includes forward pass and backward pass in
Algorithm 3 and 4, respectively.
Algorithm 3 Lightning Attention Forward Pass
Input: Q,K,V‚ààRn√ód, attention mask M‚ààRn√ón, block sizes Bc, Br;
Initialize: O=0‚ààRn√ód;
Divide QintoTr=n
Brblocks Q1,Q2, ...QTrof size Br√ódeach.
Divide K,VintoTc=n
Bcblocks K1,K2, ...KTc,V1,V2, ...VTcof size Bc√ódeach.
Divide OintoTr=n
Brblocks O1,O2, ...OTrof size Br√ódeach.
Divide MintoTr√óTcblocks M11,M12, ...MTr,Tcof size Br√óBceach.
for1‚â§i‚â§Trdo
LoadQi‚ààRBr√ódfrom HBM to on-chip SRAM.
Initialize Oi=0‚ààRBr√ódon SRAM.
for1‚â§j‚â§Tcdo
LoadKj,Vj‚ààRBc√ódfrom HBM to on-chip SRAM.
LoadMij‚ààRBc√óBcfrom HBM to on-chip SRAM.
On chip, compute Aij= [QiK‚ä§
j]‚äôMij‚ààRBr√óBc.
On chip, compute Oi=Oi+AijVj‚ààRBr√ód.
end for
Write Oito HBM as the i-th block of O.
end for
return O
C P ROVING ROBUST INFERENCE ALGORITHM
We will use induction to prove: [kv]t=Œª‚àít[kv]t.
Base Case ( n= 1):
[kv]1= ([kv]0+k1Œª‚àí1v‚ä§
1)
=Œª‚àí1(k1v‚ä§
1)
=Œª‚àí1[kv]1.(18)
17

--- PAGE 18 ---
Technical Report
Algorithm 4 Lightning Attention Backward Pass
Input: Q,K,V,dO‚ààRn√ód, attention mask M‚ààRn√ón, on-chip SRAM of size M, block sizes Bc, Br;
Initialize: dQ=dK=dV=0‚ààRn√ód;
Divide QintoTr=n
Brblocks Q1,Q2, ...QTrof size Br√ódeach.
Divide K,VintoTc=n
Bcblocks K1,K2, ...KTc,V1,V2, ...VTcof size Bc√ódeach.
Divide O,dOintoTr=n
Brblocks O1,O2, ...OTr,dO1,dO2, ...dOTrof size Br√ódeach
Divide MintoTr√óTcblocks M11,M12, ...MTr,Tcof size Br√óBceach.
for1‚â§j‚â§Tcdo
LoadKj,Vj‚ààRBc√ódfrom HBM to on-chip SRAM.
Initialize dKj=dVj=0‚ààRBc√ódon SRAM.
for1‚â§i‚â§Trdo
LoadQi,Oi,dOi‚ààRBr√ódfrom HBM to on-chip SRAM.
LoadMij‚ààRBc√óBcfrom HBM to on-chip SRAM.
Initialize dKj=dVj=0‚ààRBc√ódon SRAM.
On chip, compute Aij= [QiK‚ä§
j]‚äôMij‚ààRBr√óBc.
On chip, compute dVj=dVj+A‚ä§
ijdOi‚ààRBc√ód.
On chip, compute dAij= [dOiV‚ä§
j]‚äôMij‚ààRBr√óBc.
On chip, compute dKj=dkj+dA‚ä§
ijVj‚ààRBc√ód.
LoaddQifrom HBM to SRAM, then on chip, compute dQi=dKi+dAijKj‚ààRBr√ód,
write back to HBM.
end for
Write dKj,dVjto HBM as the j-th block of dK,dV.
end for
retundQ,dK,dV
Assume the statement holds for n=m‚àí1, i.e., [kv]m‚àí1=Œª‚àí(m‚àí1)[kv]m‚àí1. Then, when n=m:
[kv]m= [kv]m‚àí1+kmŒª‚àímv‚ä§
m
=Œª‚àí(m‚àí1)[kv]m‚àí1+kmŒª‚àímv‚ä§
m
=Œª‚àím(Œª[kv]m‚àí1+kmv‚ä§
m)
=Œª‚àím[kv]m,(19)
the statement holds. Therefore, by induction, the statement holds for all n‚â•1.
Thus, both the Origin Inference Algorithm and the Robust Inference Algorithm yield the same results.
D C ORPUS
We gather an extensive corpus of publicly accessible text from the internet, totaling over 700TB in
size. The collected data are processed by our data preprocessing procedure as shown in Figure 5,
leaving a 6TB cleaned corpus with roughly 2 trillion tokens. We categorize our data sources to
provide better transparency and understanding. The specifics of these categories are outlined in
Table 11.
D.1 D ATA PREPROCESSING
Our data preprocessing procedure consists of three steps: 1). rule-based filtering, 2). deduplication,
and 3). a self-cleaning scheme. Before being added to the training corpus, the cleaned corpus needs
to be evaluated by humans.
Rule-based filtering The rules we used to filter our collected data are listed as follows:
‚Ä¢Removal of HTML Tags and URLs: The initial step in our process is the elimination of
HTML tags and web URLs from the text. This is achieved through regular expression
techniques that identify these patterns and remove them, ensuring the language model
focuses on meaningful textual content.
‚Ä¢Elimination of Useless or Abnormal Strings: Subsequently, the cleaned dataset undergoes a
second layer of refinement where strings that do not provide value, such as aberrant strings
18

--- PAGE 19 ---
Technical Report
AcademicwritingsBooksCodeWeb‚Ä¶
Self-CleanSchemeModel-based Filtering
Evaluation ModelHumanEvaluation
DeduplicationRule-basedFiltering
Training datax N
Figure 5: Data Preprocess Procedure. The collected data undergoes a process of rule-based filtering
and deduplication, followed by our self-clean data processing strategy: model-based filtering, human
evaluation, and evaluation model. After several iterations of the above cycle, we obtain high-quality
training data at around 2T tokens.
or garbled text, are identified and excised. This process relies on predefined rules that
categorize certain string patterns as non-contributing elements.
‚Ä¢Deduplication of Punctuation Marks: We address the problem of redundant punctuation
marks in the data. Multiple consecutive punctuation marks can distort the natural flow and
structure of sentences when training the model. We employ a rule-based system that trims
these duplications down to a single instance of each punctuation mark.
‚Ä¢Handling Special Characters: Unusual or special characters that are not commonly part of
the language‚Äôs text corpus are identified and either removed or replaced with a standardized
representation.
‚Ä¢Number Standardization: Numerical figures may be presented in various formats across dif-
ferent texts. These numbers are standardized into a common format to maintain consistency.
‚Ä¢Preservation of Markdown/LaTeX Formats: While removing non-textual elements, excep-
tions are made for texts in Markdown and LaTeX formats. Given their structured nature and
ubiquitous use in academia and documentation, preserving these formats can enhance the
model‚Äôs ability to understand and generate similarly formatted text.
Deduplication To ensure the uniqueness of our data and avert the risk of overfitting, we employ an
efficient de-duplication strategy at the document or line level using MinHash and Locality-Sensitive
Hashing (LSH) algorithms. This combination of MinHash and LSH ensures a balance between
computational efficiency and accuracy in the deduplication process, providing a robust mechanism
for data deduplication and text watermark removal.
Self-cleaning scheme Our data self-cleaning process involves an iterative loop of the following
three steps to continuously refine and enhance the quality of our dataset. An issue of using model-
based data filters is that the filtered data will have a similar distribution as the evaluation model,
which may have a significant impact on the diversity of the training data. Assuming that the majority
of the pre-processed data is of high quality, we can train an evaluation model on the entire set of
pre-processed data, and the model will automatically smooth the data manifold distribution and outlet
low-quality data while retaining the majority of the diversities.
The self-cleaning scheme unfolds as follows:
‚Ä¢Evaluation Model: We train a 385M model on the pre-processed corpus to act as a data
quality filter.
19

--- PAGE 20 ---
Technical Report
Table 11: Statistics of our corpus. For each category, we list the number of epochs performed on the
subset when training on the 2 trillion tokens, as well as the number of tokens and disk sizes. We also
list the table on the right according to the language distribution.
Dataset Epochs Tokens Disk size
Academic Writings 1.53 200 B 672 GB
Books 2.49 198 B 723 GB
Code 0.44 689 B 1.4 TB
Encyclopedia 1.51 5 B 18 GB
Filtered Webpages 1.00 882 B 3.1 TB
Others 0.63 52 B 154 GB
Total - 2026 B 6 TBLanguage Tokens Disk size
English 743 B 2.9 TB
Chiese 555 B 1.7 TB
Code 689 B 1.4 TB
Others 39 B 89 GB
Total 2026 B 6 TB
‚Ä¢Model-Based Data Filtering: We use the evaluation model to assess each piece of data with
perplexity. Only data achieving a score above a certain threshold is preserved for the next
step. Low-quality data are weeded out at this stage.
‚Ä¢Human Evaluation: We sample a small portion of the filtered data and manually evaluate
the quality.
These steps are repeated in cycles, with each iteration improving the overall quality of the data and
ensuring the resulting model is trained on relevant, high-quality text. This self-cleaning process
provides a robust mechanism for maintaining data integrity, thereby enhancing the performance of
the resulting language model.
D.2 T OKENIZATION
We tokenize the data with the Byte-Pair Encoding (BPE) algorithm. Notably, to enhance compatibility
with Chinese language content, a significant number of common and uncommon Chinese characters
have been incorporated into our vocabulary. In cases where vocabulary items are not present in the
dictionary, the words are broken down into their constituent UTF-8 characters. This strategy ensures
comprehensive coverage and flexibility for diverse linguistic input during model training.
E A DDITIONAL EXPERIMENTAL RESULTS
E.1 M ODEL PARALLELISM ON TRANS NORMER LLM
We conduct a series of experiments with a 7B TransNormerLLM model to investigate the performance
of model parallelism on TransNormerLLM in terms of speed and memory. These tests are carried
out on a single Nvidia DGX node that houses eight A100 80G GPUs linked by NVLink. In this
experiment, FSDP is enabled and Flash Attention (Dao et al., 2022a) is used on the Transformer.
Table 12 shows the results for training speed and memory consumption.
It can be seen that model parallelism has a significant effect on memory conservation, as increasing
the number of partitions for the model results in lower memory consumption per GPU. Due to
NVLink constraints, we kept the dimension of model parallelism within 8 in all of our experiments.
The TransNormerLLM-7B model requires only 24.1GB of memory on a single GPU when the model
parallel size is set to 8, representing a significant memory reduction of 62.3% when compared to the
model parallel size of 1. In comparison, the Transformer-7B model consumes 28.7GB of memory
under the same configuration. While model parallelism conserves memory, it is worth noting that
training speed is only marginally reduced. TransNormerLLM consistently outperforms Transformer
by a wide margin.
E.2 S TRESS TESTS ON MODEL SIZE AND CONTEXT LENGTH
A series of stress tests are performed to assess the efficacy of the designed system optimization strategy.
The model is scaled up to 175B, which is the largest released version of the TransNormerLLM
model. However, this augmentation poses significant training challenges. We use a wide range of
distributed training techniques to effectively train such a large model, with the goal of reducing GPU
memory consumption while increasing computational and communication efficiencies. To ensure the
20

--- PAGE 21 ---
Technical Report
Table 12: Model Parallelism Performance. We compare the model parallelism performance of
Transformer-7B with Flash Attention and TransNormerLLM-7B with Lightning Attention on a single
A100 node with NVLink. All experiments use a batch size of 2 and a context length of 2048.
Model Model Parallel Size Tokens/s Allocated Memory/GPU Memory Saved
Transformer-7B1 26896.1 66.3 GB -
2 24973.7 44.6 GB 32.7%
4 22375.8 40.2 GB 39.4%
8 19973.6 28.7 GB 56.7%
TransNormerLLM-7B1 32048.6 64.0 GB -
2 29750.4 41.0 GB 35.9%
4 27885.2 36.3 GB 43.3%
8 24280.0 24.1 GB 62.3%
feasibility of training these massive TransNormerLLM models, Lightning Attention, FSDP, Model
Parallelism, AMP, and Activation Checkpointing are used. For the Transformer models, we use Flash
Attention (Dao et al., 2022a) in all experiments.
Model Size We perform training experiments on variously sized Transformer and TransNormer-
LLM models using a large-scale A100 80G GPU cluster, as shown in Table 13. To achieve the
maximum speed for various model sizes, we keep the context length constant at 2048 and increased
the batch size until we reached the GPU memory limit. TransNormerLLMs consistently outper-
form their Transformer counterparts in terms of computation speed. This observation validates the
TransNormerLLM model‚Äôs advantageous linear computational complexity, reinforcing its efficacy.
Table 13: Efficiency of training models with different sizes. For comparative purposes, we keep the
context length fixed at 2048 and increased the batch size for both transformer and TransNormerLLM
to achieve their maximum speeds without encountering out-of-memory issues.
Model Model Size Tokens/sec/GPU Allocated Memory/GPU
Transformer7B 3362.7 72.5 GB
13B 1735.6 70.6 GB
65B 318.2 73.2 GB
175B 106.2 69.5 GB
TransNormerLLM7B 4081.0 71.9 GB
13B 2104.3 73.8 GB
65B 406.9 69.4 GB
175B 136.6 70.3 GB
Context Length One of the strengths of TransNormerLLM lies in its utilization of linear attention
computation, which exhibits computational and storage complexities linearly correlated with the
sequence length. To validate this outstanding characteristic of TransNormerLLM, we conduct training
experiments on Transformer and TransNormerLLM models with varying parameter sizes. While
maintaining a batch size of 1, we aim to maximize the context length. All experiments run on a small
cluster with 64 A100 GPUs. The results, as presented in Table 14, demonstrate the remarkable long
context length training capability of TransNormerLLM. Under comparable computational resources,
the TransNormerLLM model exhibits the ability to train with longer context lengths compared to
conventional Transformer models and achieve higher computational speeds in the process.
21

--- PAGE 22 ---
Technical Report
Table 14: Maximum context length for training Transformer and TransNormerLLM. We com-
pare the maximum context lengths with different model sizes between Transformer and TransNormer-
LLM on 64 A100 80G GPUs. All experiments use a batch size of 1.
Model Model Size Context Length Relative Speed Allocated Memory/GPU
Transformer7B 37K 1 71.1 GB
13B 24K 1 68.0 GB
65B 19K 1 73.3 GB
175B 10K 1 66.9 GB
TransNormerLLM7B 48K 1.21 65.8 GB
13B 35K 1.23 61.0 GB
65B 23K 1.29 68.2 GB
175B 12K 1.35 63.5 GB
22
