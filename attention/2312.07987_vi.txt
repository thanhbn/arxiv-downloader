# 2312.07987.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2312.07987.pdf
# Kích thước file: 1070079 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
SwitchHead: Tăng tốc Transformers với
Mixture-of-Experts Attention
Róbert Csordás1†Piotr Pi˛ ekos2Kazuki Irie3†Jürgen Schmidhuber2,4
1Stanford University, Stanford, CA, USA
2AI Initiative, KAUST, Thuwal, Saudi Arabia
3Center for Brain Science, Harvard University, Cambridge, MA, USA
4The Swiss AI Lab IDSIA, USI & SUPSI, Lugano, Switzerland
rcsordas@stanford.edu ,piotr.piekos@kaust.edu.sa ,
kirie@fas.harvard.edu ,juergen@idsia.ch
Tóm tắt
Mặc dù có nhiều công trình gần đây về Mixture of Experts (MoEs) cho các mô hình ngôn ngữ Transformer tiết kiệm tài nguyên, các phương pháp hiện tại chủ yếu tập trung vào MoEs cho các lớp feedforward. Các nỗ lực trước đây trong việc mở rộng MoE cho lớp self-attention không thể đạt được hiệu suất của baseline có cùng số tham số. SwitchHead mới của chúng tôi là một phương pháp MoE hiệu quả cho lớp attention thành công trong việc giảm cả yêu cầu tính toán và bộ nhớ, đạt được tăng tốc wall-clock, trong khi vẫn đạt hiệu suất mô hình hóa ngôn ngữ của Transformer baseline. Cơ chế MoE mới của chúng tôi cho phép SwitchHead tính toán ít hơn tới 8 lần số ma trận attention so với Transformer tiêu chuẩn. SwitchHead cũng có thể được kết hợp với các lớp feedforward MoE, tạo ra các Transformers "SwitchAll" hoàn toàn MoE. Đối với mô hình 262M tham số của chúng tôi được huấn luyện trên C4, SwitchHead đạt perplexity tương đương với các mô hình tiêu chuẩn chỉ với 44% tính toán và 27% sử dụng bộ nhớ. Các thí nghiệm zero-shot trên các nhiệm vụ downstream xác nhận hiệu suất của SwitchHead, ví dụ, đạt được hơn 3.5% cải thiện tuyệt đối trên BliMP so với baseline với tài nguyên tính toán tương đương.1
1 Giới thiệu
Hình 1: Biểu diễn sơ đồ của SwitchHead. Nó bao gồm một vài heads độc lập, mỗi head có nhiều experts cho các phép chiếu value và output. Mỗi head có một ma trận attention duy nhất.
Các mô hình ngôn ngữ lớn (LLMs) đã cho thấy khả năng đáng chú ý [ 1,2,3,4] và tính linh hoạt tuyệt vời [5]. Tuy nhiên, việc huấn luyện các Transformers lớn [ 6,7] đòi hỏi một lượng sức mạnh tính toán và bộ nhớ đáng kể, điều này không thể tiếp cận được đối với hầu hết các nhà nghiên cứu, các tổ chức học thuật và thậm chí các công ty.
†Công việc thực hiện tại IDSIA.
1Mã nguồn của chúng tôi được công khai: https://github.com/robertcsordas/switchhead
Hội nghị thứ 38 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2024).arXiv:2312.07987v3  [cs.LG]  30 Sep 2024

--- TRANG 2 ---
Ngay cả việc chạy chúng ở chế độ suy luận—thường ít tốn tài nguyên hơn nhiều—cũng đòi hỏi nỗ lực kỹ thuật đáng kể [8]. Tăng tốc Transformers vẫn là một câu hỏi nghiên cứu quan trọng.
Trong bối cảnh này, các lớp Mixture of Experts (MoE) [ 9,10,11] đã trở nên phổ biến để mở rộng quy mô Transformers một cách hiệu quả lên một số lượng lớn tham số [ 12,13,14,15,16,17]. Tuy nhiên, hầu hết các công trình này chủ yếu tập trung vào việc áp dụng MoE cho các khối feedforward 2 lớp [6], tức là các thành phần multi-layer perceptron (MLP) của Transformer, trong khi giữ nguyên các lớp self-attention.
Cho rằng attention cũng chiếm một lượng đáng kể tính toán và sử dụng bộ nhớ trong Transformers (đặc biệt đối với kích thước ngữ cảnh dài), việc sử dụng MoE cho attention có tiềm năng cải thiện thêm hiệu quả tài nguyên trong Transformers. Trong khi attention dựa trên MoE vẫn chưa được khám phá nhiều nói chung, có các công trình hiện tại về các phương pháp MoE cho attention [ 18,19]. Tuy nhiên, trong thực tế, các phương pháp được đề xuất trước đây thường đòi hỏi nhiều thủ thuật kỹ thuật để huấn luyện thành công, và quan trọng nhất, chỉ đạt được một sự giảm khiêm tốn trong yêu cầu tính toán và bộ nhớ cuối cùng (như chúng tôi cũng xác nhận trong các thí nghiệm của mình).
Ở đây, chúng tôi trình bày một phương pháp attention dựa trên MoE mới, SwitchHead, có cơ chế cho phép giảm số lượng ma trận attention cần được tính toán và lưu trữ. Theo σ-MoE [17], phương pháp của chúng tôi sử dụng hàm kích hoạt lựa chọn không cạnh tranh (sigmoid), và không đòi hỏi regularization hoặc các thủ thuật bổ sung để huấn luyện ổn định. Quan trọng là, chúng tôi chỉ ra rằng có thể tính toán các phép chiếu MoE bên ngoài lõi attention, điều này cho phép giảm đáng kể số lượng bản đồ attention được tính toán, dẫn đến tiết kiệm tài nguyên đáng kể. Nghiên cứu kỹ lưỡng của chúng tôi cho thấy rằng chỉ cần chọn các phép chiếu value và output từ một nhóm experts và chia sẻ keys và queries giữa chúng là đủ.
Chúng tôi đánh giá phương pháp của mình trên C4 [ 20], Enwik8 [ 21], peS2o [ 22] và Wikitext 103 [ 23], với hai kích thước mô hình (47M và 262M). Ngoài ra, chúng tôi đo hiệu suất zero-shot của các mô hình chính trên các bộ dữ liệu Lambada [ 24], BLiMP [ 25], và Children's Books Test [ 26]. Các thí nghiệm của chúng tôi chứng minh rằng SwitchHead có thể đạt được hiệu suất so sánh được với các baseline có cùng số tham số chỉ với một phần nhỏ ngân sách tính toán và bộ nhớ. Ngoài ra, chúng tôi giới thiệu "SwitchAll", một mô hình Transformer hoàn toàn dựa trên MoE, kết hợp lớp MLP dựa trên σ-MoE với SwitchHead attention của chúng tôi, thường vượt trội hơn các baselines dense với cùng ngân sách tham số.
Cuối cùng, chúng tôi phân tích các bản đồ attention của SwitchHead. Chúng tôi phát hiện rằng các bản đồ attention được lấy trên tất cả các heads về mặt định tính tương tự như các baselines dense, cho thấy một sự giảm đáng kể trong redundancy mà không mất đi tính biểu cảm. Ngoài ra, việc lựa chọn experts thường có thể giải thích được.
2 Phương pháp
2.1 Kiến thức nền tảng
Lớp multi-head self-attention (MHA) tiêu chuẩn [ 6] bao gồm bốn bước chính: (1) tính toán các phép chiếu key, query, và value, (2) tính toán ma trận attention, (3) sử dụng ma trận attention để chiếu các values, và (4) ánh xạ các values đã chiếu thành output. Gọi h,T,nheads,dmodel,dhead là các số nguyên dương. Gọi x∈RT×dmodel là đầu vào của lớp MHA với nheads heads, T là độ dài chuỗi, và dmodel biểu thị kích thước của các biểu diễn ẩn của mô hình. Wh
{K,V,Q }∈Rdmodel×dhead là các ma trận chiếu cho head h∈ {1, ..., n heads}. Sau đó Kh=xWh
K,Qh=xWh
Q, và Vh=xWh
V(do đó Kh,Qh,Vh∈RT×dhead) là các keys, queries, và values, tương ứng. Ma trận attention cho head h,Ah∈RT×T, và output y∈RT×dmodel được tính như sau:
Ah= softmax1√dheadQhKh⊺
(1)
y= (A1V1|A2V2|...|AnheadsVnheads)WO (2)
trong đó |biểu thị concatenation trong chiều cuối cùng, softmax( ·)cũng trên chiều cuối cùng, và WO∈Rnheadsdhead×dmodel. Tuy nhiên, một công thức thay thế phản ánh vai trò của WO tốt hơn. Hãy chia WO theo chiều đầu tiên thành các ma trận con cho mỗi head, Wh
O∈Rdhead×dmodel, sao cho WO=
W1
O⊺|W2
O⊺|...|Wnheads
O⊺⊺. Trong trường hợp này, output (Eq. 2) có thể được viết tương đương như:
y=X
hAhVhWh
O (3)
2

--- TRANG 3 ---
Từ đây, có thể thấy rằng tất cả các tính toán đều cục bộ với mỗi head. Tính toán ma trận attention Ah và readout AhVh đòi hỏi tính toán theo thứ tự O(nheadsdheadT2)MACs (phép nhân-tích lũy2). Trong quá trình huấn luyện, nó đòi hỏi lưu trữ O(nheadsT2) cho các ma trận attention và O(nheadsTdhead) để lưu trữ các kết quả phụ của các phép chiếu. Với một chuỗi đủ dài, việc tính toán ma trận attention và chiếu các values sẽ chiếm ưu thế trong yêu cầu tính toán do sự phụ thuộc bậc hai vào độ dài chuỗi T.
2.2 Từ Dense đến SwitchHead Attention Layer
Mục tiêu của chúng tôi là đạt được sự giảm tài nguyên trong khi duy trì các tính chất cơ bản của attention và giữ lại một ma trận attention hoàn toàn biểu cảm. Để làm điều đó, chúng tôi bắt đầu từ quan sát sau: các LLMs hiện đại sử dụng hàng chục heads [ 2,27]. Liệu tất cả chúng có thật sự cần thiết không? Như chúng tôi chỉ ra sau đó trong Sec. 3, thực sự, việc giảm một cách ngây thơ số lượng heads (trong khi giữ cùng số tham số bằng cách tăng chiều head) dẫn đến mất hiệu suất. Việc giải thích lý do cho nhu cầu có nhiều heads nằm ngoài phạm vi của bài báo này. Tuy nhiên, đây là một số giả thuyết: (1) chúng cung cấp nhiều đầu vào cho các phép toán mà mạng thực hiện trong mỗi bước, (2) chúng được chuyên môn hóa và chỉ cung cấp đầu vào cho các phép toán cụ thể (trong trường hợp này, mỗi phép toán sẽ sử dụng một tập con khác nhau của các heads), (3) chúng có thể cung cấp các outputs đa dạng do các khởi tạo khác nhau, một số thành công hơn những cái khác, do đó cho phép học tập tốt hơn. Trong số này, (2) và (3) có thể mang lại cơ hội tiết kiệm tài nguyên: nếu không phải tất cả heads đều cần thiết cùng một lúc, có thể chuyển đổi giữa chúng tùy thuộc vào ngữ cảnh.
Một phương pháp ngây thơ để đạt được điều này là sử dụng tín hiệu gating bằng một phép chiếu tuyến tính WS∈Rdmodel×nheads, và sử dụng các heads có điểm số cao nhất, bằng cách thay thế Eq. 3 bằng Eq. 6:
s=σ(xWS) (4)
E= arg topk( s, k),E ⊂ { 1, ..., n heads} (5)
y[t, c] =X
h∈Es[t, h](AhVhWh
O)[t, c] (6)
trong đó y[t, c]∈R biểu thị việc lập chỉ mục phần tử cụ thể của ma trận output y∈RT×dmodel, cho timestep t và channel c, và k là số lượng experts hoạt động. Theo phương pháp σ-MoE [ 17], chúng tôi sử dụng hàm lựa chọn không cạnh tranh (sigmoid σ trong Eq. 4). Bây giờ, hãy định nghĩa phía nguồn của attention là các keys và values và phía đích là các queries và output. Theo trực giác, phương pháp trên tương ứng với việc chọn một tập con các attention heads chỉ dựa trên phía đích3.
Các thí nghiệm sơ bộ của chúng tôi xác nhận rằng phương pháp này thực sự khả thi cho mô hình hóa ngôn ngữ trên WikiText-103. Tuy nhiên, khó đạt được tăng tốc và tiết kiệm bộ nhớ với phương pháp này. Để hiểu tại sao, hãy chú ý rằng các phần tử của ma trận attention Ah phụ thuộc vào các cặp tokens, một cho phía nguồn và một cho phía đích, nhưng việc lựa chọn chỉ được thực hiện dựa trên phía đích. Do đó, trong trường hợp xấu nhất, đối với mỗi đích, một nguồn khác có thể được chọn, trong trường hợp đó tất cả các phép chiếu nguồn có thể phải được tính toán cho các keys và values, điều mà chúng tôi muốn tránh.
Thay vào đó, chúng tôi đề xuất cải thiện phương pháp trên bằng cách giới thiệu các tính toán có điều kiện cho các phép chiếu nguồn và đích độc lập với nhau. Tức là, chúng tôi tham số hóa mỗi phép chiếu key, query, value, output bằng một MoE độc lập. Điều này tránh các tính toán có điều kiện liên quan đến chính ma trận attention. Giải pháp của chúng tôi thực hiện điều này bằng cách sử dụng Mixtures of Experts (MoEs). Khái niệm "heads" không còn được định nghĩa rõ ràng theo nghĩa thông thường: chúng tôi định nghĩa lại một head như một thể hiện của một ma trận attention được tính toán. Chúng tôi gọi tổng số của chúng là nheads. Đối với mỗi head h, chúng tôi định nghĩa một danh sách riêng biệt gồm E experts. Tổng số experts sau đó là nheads·E. Sau đó, các ma trận chiếu trở thành Wh,e
K,Wh,e
Q,Wh,e
V và Wh,e
O∈Rdhead×dmodel, trong đó h biểu thị chỉ số head và e là expert cụ thể. Sau đó chúng tôi tính toán việc lựa chọn expert phía nguồn như sau:
sh
S=σ(xWh
S) (7)
Eh
S= arg topk( sh
S, k),Eh
S⊂ {1, ..., E} (8)
2Số lượng MACs là một metric được sử dụng trong công trình trước đây [ 18], độc lập với cả phần cứng cụ thể và triển khai, không giống như thời gian wall-clock. Đối với các đo lường thời gian wall-clock, xem Sec. 3.7.
3Để làm rõ, chúng tôi phân bổ một hàm routing cho mỗi phép chiếu key/value/query; các hàm routing này thuộc về phía nguồn hoặc đích tương ứng. Nếu chúng ta so sánh Eq. 10 và Eq. 6, có thể nhận thấy rằng hàm routing trong Eq. 6 hiệu quả tương ứng với những gì chúng tôi định nghĩa là routing phía đích trong Eq. 10.
3

--- TRANG 4 ---
trong đó Wh
S∈Rdmodel×E. Chúng tôi tính toán các experts phía đích tương tự: sh
D=σ(xWh
D), Eh
D= arg topk( sh
D, k),Eh
S⊂ {1, ..., E},Wh
D∈Rdmodel×E. Sau đó, phép chiếu value Vh được tính như một tổng có trọng số của các experts được chọn:
Vh=X
e∈Eh
Ssh
S[e]xWh,e
V (9)
Các phép chiếu key và query được tính tương tự: Kh=P
e∈Eh
Ssh
S[e]xWh,e
K, và Qh=
P
e∈Eh
Dsh
D[e]xWh,e
Q. Phép chiếu output cũng trở thành một MoE:
y=nheads−1X
h=0X
e∈Eh
Dsh
D[e]AhVhWh,e
O (10)
Như chúng tôi sẽ chỉ ra, không cần thiết phải làm tất cả các phép chiếu thành MoEs. Trong Mục 3.1 chúng tôi chỉ ra rằng việc giữ một bản sao duy nhất, đặc trữ cho head của các phép chiếu query và key và tái sử dụng chúng cho tất cả experts là có lợi. Chúng tôi gọi phương pháp này là SwitchHead.
Về bản chất, SwitchHead giảm số lượng ma trận attention phải được tính toán ( nheads) một cách đáng kể, bằng cách sử dụng nhiều experts cho mỗi head. Lưu ý rằng phương pháp của chúng tôi không phụ thuộc vào việc triển khai cụ thể của attention, cho phép thí nghiệm và nghiên cứu dễ dàng. Một biểu diễn sơ đồ được hiển thị trong Hình 1.
Bảng 1: Hiệu suất của SwitchHead so với các biến thể MoA khác nhau. MoA có thể vượt trội hơn baseline, nhưng chỉ với giá phải sử dụng đáng kể nhiều tính toán và bộ nhớ hơn. Ngoài ra, SwitchHead vượt trội hơn Transformer dense baseline. Các kết quả này trên Wikitext 103. Bảng được sắp xếp theo perplexity của mô hình.
#total params Model nheads Perplexity ↓ MACs Mem (floats)
47M SwitchHead 2 12.27 170.4M 0.8M
Transformer 10 12.31 453.4M 3.5M
MoA 4 12.60 223.5M 1.3M
MoA 6 12.64 306.8M 1.9M
MoA 8 12.77 390.2M 2.6M
MoA 2 12.84 140.1M 0.7M
262M MoA 8 9.50 2.9G 9.9M
SwitchHead 2 9.55 2.0G 2.9M
Transformer 16 9.66 5.4G 21.0M
MoA 12 9.68 4.1G 14.7M
MoA 4 9.69 1.7G 5.1M
MoA 2 9.87 1.1G 2.7M
3 Thí nghiệm
Chúng tôi tiến hành các thí nghiệm trong thiết lập parameter-matched [ 17] phản ánh tốt hơn nhiệm vụ mô hình hóa ngôn ngữ (so với thiết lập FLOPS-matched thường được sử dụng để đánh giá MoEs). Các thí nghiệm chính của chúng tôi sử dụng Transformer XL, vì chúng tôi thấy chúng vượt trội một cách nhất quán và đáng kể so với các baselines dựa trên RoPE [ 28] với một lượng tính toán cố định. Chúng tôi cung cấp chi tiết của phân tích này trong Phụ lục A.4. Các kết luận về hiệu quả của SwitchHead nhất quán trong cả hai trường hợp.
Như một đặc điểm quan trọng, trong thiết lập parameter-matched này, chúng tôi luôn cấu hình Switchhead sao cho nó khớp với perplexity của Transformer dense baseline, và chúng tôi tối đa hóa sự giảm tài nguyên của nó. Để làm điều này, chúng tôi tuân theo một quy trình có hệ thống. Đầu tiên, chúng tôi đặt nheads∗E giống như nheads của dense baseline. Chúng tôi bắt đầu với việc đặt nheads= 2 và k= 2, điều này cung cấp sự giảm tài nguyên nhiều nhất. Nếu mô hình kết quả hoạt động kém hơn, chúng tôi tăng k. Nếu k= 4 cũng hoạt động kém, chúng tôi đặt nheads= 4 và k= 2. Chúng tôi luôn đặt dhead sao cho tổng số tham số của mô hình kết quả khớp với số tham số của baseline. Quy trình đơn giản hợp lý này đảm bảo một lượng tiết kiệm tài nguyên tốt, trong khi tránh việc thực hiện tìm kiếm siêu tham số đắt đỏ.
4

--- TRANG 5 ---
Lưu ý rằng tất cả các lợi ích perplexity được thấy trong các bảng kết quả chính là sản phẩm phụ của việc khớp không hoàn hảo, và mục tiêu của chúng tôi là đạt được sự giảm trong yêu cầu tài nguyên, trừ khi được ghi chú khác (Xem Sec. 3.5). Các siêu tham số chi tiết của tất cả các mô hình của chúng tôi có thể được tìm thấy trong Sec. A.5 trong Phụ lục. Chúng tôi sử dụng và áp dụng Triton kernel của σ-MoE [17] cho mục đích của chúng tôi.
Đối với tất cả các bộ dữ liệu ngoại trừ Enwik8 cấp ký tự [ 21], chúng tôi sử dụng các đơn vị sub-word [ 29,30] thu được với tokenizer SentencePiece [ 31] với kích thước từ vựng 8k tokens. Đối với hầu hết các thí nghiệm của chúng tôi, chúng tôi sử dụng Transformer XL [ 32] với kích thước ngữ cảnh gấp đôi kích thước của chunk hoạt động/hiện tại, vì chúng tôi thấy nó hiệu quả về tài nguyên hơn đáng kể so với thiết lập tiêu chuẩn. Tuy nhiên, để chỉ ra rằng phương pháp của chúng tôi cũng cạnh tranh trong Transformer tiêu chuẩn với mã hóa vị trí RoPE, chúng tôi cũng chứng minh các phát hiện chính của mình trong thiết lập này (Phụ lục A.4).
Tất cả các mô hình được huấn luyện trong 100k batches. Một số bộ dữ liệu mà chúng tôi xem xét (C4 [ 20], và peS2o [ 22]) lớn hơn nhiều. Trong trường hợp này, chúng tôi huấn luyện trên 105∗T∗Nbatch tokens đầu tiên của bộ dữ liệu.
3.1 Những phép chiếu nào cần MoE?
Như đã thảo luận trong Sec. 2.2, mỗi phép chiếu tuyến tính (keys, values, queries, và output) có thể được thay thế độc lập bằng một MoE. Ở đây chúng tôi đầu tiên kiểm tra phép chiếu nào được hưởng lợi từ việc thay thế như vậy. Vì chúng tôi nhắm đến thiết lập parameter-matched, việc sử dụng MoE nơi không cần thiết có thể có tác động tiêu cực. Vì các experts sử dụng một phần đáng kể ngân sách tham số, chúng có thể làm giảm số lượng tham số có sẵn cho các phần hữu ích hơn của mô hình. Do đó, chúng tôi đã thực hiện tìm kiếm trên tất cả các kết hợp có thể của MoE so với các phép chiếu cố định với hai heads hoạt động và so sánh chúng với baseline parameter-matched. Chúng tôi thấy rằng phép chiếu output là cần thiết để khớp với hiệu suất của baseline (để biết kết quả chi tiết tham khảo Tab. 6 trong phụ lục). Việc có MoE trong các phép chiếu key và query hóa ra không cần thiết. Các mô hình không có MoE output và value hoạt động kém hơn dense baseline với nheads= 2 heads.
Tóm lại, mô hình hoạt động tốt nhất là mô hình sử dụng MoE cho các phép chiếu value và output. Chúng tôi sử dụng biến thể mô hình này trong phần còn lại của các thí nghiệm trong bài báo này.
3.2 So sánh với MoA
Phương pháp liên quan nhất đến của chúng tôi là cái gọi là Mixture of Attention Heads, hay MoA [ 18]. Không giống như SwitchHead, MoA sử dụng một phép chiếu key và value duy nhất và chọn nheads phép chiếu query và output hoạt động từ một nhóm E experts.
MoA tính toán bản đồ attention cho mỗi expert được chọn và tính trung bình có trọng số của chúng sau khi tính toán attention diễn ra. Ngược lại, SwitchHead tính trung bình có trọng số của K experts được chọn trước và sau tính toán attention. Vì điều này, trong thực tế, cùng một perplexity được đạt với số lượng ma trận attention được tính toán cần thiết ( nheads) thấp hơn nhiều đối với SwitchHead so với MoA, cho phép tiết kiệm tài nguyên đáng kể.
Ngoài ra, không giống như MoA, SwitchHead sử dụng hàm kích hoạt không cạnh tranh (sigmoid) [ 17]. Chúng tôi xác nhận rằng với điều này, phương pháp của chúng tôi hoạt động tốt mà không cần bất kỳ regularization nào, trong khi MoA đòi hỏi ba regularizers khác nhau.
Chúng tôi so sánh phương pháp của mình với MoA trong Bảng 1. Có thể thấy rằng trong khi MoA có thể vượt trội nhẹ so với phương pháp của chúng tôi về mặt perplexity, nó chỉ có thể làm được điều đó với giá phải sử dụng đáng kể nhiều tài nguyên hơn. Với ngân sách tính toán và bộ nhớ tương tự, phương pháp của chúng tôi nhất quán vượt trội hơn MoA.
3.3 Hiệu suất trên các bộ dữ liệu khác nhau
Chúng tôi kiểm tra các phương pháp của mình trên một tập hợp đa dạng các bộ dữ liệu mô hình hóa ngôn ngữ, bao gồm C4 [ 20], Enwik8 [ 21], peS2o [ 22], ở hai quy mô khác nhau: 47M và 262M tham số. Chúng tôi chọn thiết lập thí nghiệm này có tính đến ngân sách tính toán của chúng tôi và sự tin tưởng vào kết quả của chúng tôi nhất quán qua các cấu hình khác nhau.
Kết quả được hiển thị trong Bảng 2. Chúng tôi so sánh các mô hình của mình với hai baselines: một có cùng số heads như tổng số experts ( nheads·E) của các mô hình SwitchHead, và cái kia có cùng số heads như số ma trận attention hoạt động ( nheads) như các mô hình của chúng tôi. Các 5

--- TRANG 6 ---
Bảng 2: Hiệu suất của SwitchHead so với baselines trên các bộ dữ liệu và kích thước mô hình khác nhau. Có thể thấy rằng hiệu suất dự đoán của mô hình SwitchHead của chúng tôi so sánh được với các baselines, và luôn tốt hơn baseline có số heads bằng nhau. Perplexity được hiển thị cho các bộ dữ liệu Wikitext 103, C4 và peS2o, và bits/character (bpc) cho Enwik8. Các mô hình được sắp xếp theo perplexity.
Dataset #total params Model nheads ppl/bpc ↓MACs Mem (floats)
C4 47M SwitchHead 2 22.53 203M 0.8M
Transformer 10 22.71 453M 3.5M
Transformer 2 23.71 453M 1.4M
262M SwitchHead 4 16.23 2.4G 5.6M
Transformer 16 16.28 5.4G 21M
Transformer 4 17.09 5.4G 8.4M
Wikitext 103 47M SwitchHead 2 12.31 170M 0.8M
Transformer 10 12.32 453M 3.5M
Transformer 2 12.73 453M 1.4M
262M SwitchHead 2 9.77 2.0G 2.9M
Transformer 16 9.80 5.4G 21M
Transformer 2 10.09 5.4G 6.3M
peS2o 47M Transformer 10 12.83 453M 3.5M
SwitchHead 2 12.84 203M 0.8M
Transformer 2 13.37 453M 1.4M
262M Transformer 16 9.78 5.4G 21M
SwitchHead 4 9.86 2.4G 5.6M
Transformer 4 10.11 5.4G 8.4M
Enwik8 41M Transformer 8 1.10 1.6G 10M
SwitchHead 2 1.10 709M 2.8M
Transformer 2 1.13 1.6G 4.2M
mô hình của chúng tôi khớp chặt chẽ với hiệu suất của baseline đầy đủ, nhiều head với một phần nhỏ yêu cầu bộ nhớ và tính toán (xem Sec. 3.7 để biết thêm chi tiết).
Ngoài ra, chúng tôi xác minh hiệu suất của các mô hình được huấn luyện trên bộ dữ liệu C4 trong các nhiệm vụ downstream theo cách zero-shot. Chúng tôi xem xét Lambada [ 24], BLiMP [ 25] và Children's Book Test (CBT) [26]. Kết quả được hiển thị trong Bảng 4: các mô hình SwitchHead của chúng tôi nhất quán vượt trội hoặc khớp với hiệu suất của các mô hình Transformer dense baseline.
3.4 SwitchAll
Mục tiêu đạt được các Transformers tiết kiệm tài nguyên hơn bao gồm việc giảm yêu cầu tài nguyên của cả các lớp MLP và attention. σ-MoE [ 17] gần đây được đề xuất như một phương pháp MoE tiết kiệm tham số để tăng tốc các lớp MLP. Tuy nhiên, vẫn chưa rõ liệu nó có thể được kết hợp hiệu quả với SwitchHead của chúng tôi, hoặc có thể có một số tác dụng tương tác tiêu cực nào đó nếu được kết hợp trong "SwitchAll", nơi mọi lớp đều dựa trên MoE.
Để xác minh điều này, chúng tôi lấy kiến trúc baseline của Csordás et al. [17] mà không có bất kỳ thay đổi siêu tham số nào và thay thế lớp attention bằng SwitchHead. Các siêu tham số cho attention được lấy trực tiếp từ các thí nghiệm được hiển thị trong Tab. 2. Kết quả được hiển thị trong Tab. 3. Mô hình hoàn toàn MoE kết hợp thường vượt trội hơn các baselines dense cho mỗi bộ dữ liệu và kích thước mô hình được xem xét, ngoại trừ trường hợp mô hình 262M tham số trên bộ dữ liệu C4.
3.5 Thiết lập MAC-Matched
Tất cả các thí nghiệm của chúng tôi cho đến nay được hiệu chỉnh sao cho hiệu suất dự đoán (perplexity) khớp với hiệu suất của Transformer baseline, và chúng tôi nhắm đến tiết kiệm tài nguyên tối đa. Tuy nhiên, đây cũng là một câu hỏi hợp lệ để hỏi hiệu suất của SwitchHead trong thiết lập MAC-matched là gì, nơi yêu cầu tính toán của mô hình của chúng tôi được khớp với những yêu cầu của baseline. Chúng tôi đạt được điều này bằng cách tăng dhead và nheads cho đến khi chúng tôi có cùng yêu cầu MAC như baseline. Điều này dẫn đến một mô hình với nhiều tham số hơn. Đối với Transformer XL nhỏ, chúng tôi tăng dhead từ 76 thành 6

--- TRANG 7 ---
112 và nheads từ 2 thành 3. Đối với XL lớn, chúng tôi tăng nheads từ 4 thành 6 và dhead từ 112 thành 168. Đối với mô hình RoPE nhỏ, chúng tôi thay đổi nheads từ 2 thành 3 và dmodel từ 64 thành 84, đối với lớn nheads từ 4 thành 6 và dmodel từ 112 thành 168. Chúng tôi hiển thị kết quả trong Tab. 4: các mô hình MAC-matched vượt trội hơn những mô hình khác với một khoảng cách lớn cả về perplexity và hiệu suất nhiệm vụ zero-shot.
3.6 Lựa chọn được chia sẻ
Để tiết kiệm thời gian hơn nữa, chúng tôi có thể chia sẻ việc lựa chọn expert giữa phía nguồn và đích. Tăng tốc được đạt được bằng cách giảm số lượng các bước sắp xếp và top-k so với SwitchHead đầy đủ. Tuy nhiên, điều này dẫn đến mất hiệu suất nhỏ, có thể được chấp nhận trong một số trường hợp mà tăng tốc quan trọng hơn. Xem Tab. 4 để biết thêm chi tiết.
3.7 Ước tính Thời gian Wall-Clock và Sử dụng Bộ nhớ
Trong tất cả các bảng của chúng tôi, chúng tôi báo cáo số lượng phép toán nhân-tích lũy (MAC) theo Zhang et al. [18]. Lý do cho điều này là thời gian wall-clock thực tế rất phụ thuộc vào triển khai và phần cứng. Tuy nhiên, chúng tôi đã đo runtime và tổng sử dụng bộ nhớ của toàn bộ pipeline huấn luyện của chúng tôi (bao gồm cả lớp feedforward) để chứng minh rằng triển khai hiện tại (chưa tối ưu) của chúng tôi đã có khả năng cung cấp tăng tốc thời gian wall-clock. Chúng tôi hiển thị kết quả trong Tab. 5. Các phép đo được thực hiện trên phần cứng giống hệt nhau với cùng triển khai (bao gồm cho lõi attention), sự khác biệt duy nhất là các phép chiếu dựa trên MoE cho attention. Có thể thấy rằng đối với cả hai quy mô, SwitchHead huấn luyện nhanh hơn khoảng 1.5 lần, trong khi sử dụng 61%-67% bộ nhớ so với baseline.
Chúng tôi cũng báo cáo hiệu suất của MoA để tham khảo trong Bảng 5. Để đo sử dụng tài nguyên của MoA, chúng tôi chọn mô hình MoA nhanh nhất có thể khớp với hiệu suất của dense baseline, hoặc đơn giản là mô hình MoA tốt nhất khi không có mô hình MoA nào có thể khớp với hiệu suất baseline. Điều này dẫn đến việc chọn MoA với H= 4 cho mô hình 47M và MoA với H= 8 cho mô hình 262M tham số. SwitchHead vượt trội hơn MoA ở cả hai quy mô, cả về thời gian wall clock và yêu cầu bộ nhớ. Lưu ý rằng các phép đo này cũng bao gồm các lớp MLP, optimizer, và đồng bộ hóa gradient trong trường hợp huấn luyện đa GPU.
Bảng 3: Hiệu suất của SwitchAll (SwitchHead + σ-MoE [ 17]) trên các bộ dữ liệu và kích thước mô hình khác nhau. Mô hình SwitchAll của chúng tôi gần hoặc tốt hơn so với các baselines. Các mô hình được sắp xếp theo perplexity. Lưu ý: Chúng tôi hiển thị số lượng tham số của mô hình dense. Số lượng tham số cho mô hình SwitchAll lớn là 259M vì việc khớp tham số không hoàn hảo.
Dataset #total params Model nheads ppl↓MACs Mem (floats)
Wikitext 103 47M SwitchAll 2 12.17 170M 0.8M
Transformer 10 12.32 453M 3.5M
262M Transformer 16 9.80 5.4G 21M
SwitchAll 4 9.81 2.4G 5.6M
C4 47M SwitchAll 2 22.09 202M 0.8M
Transformer 10 22.63 453M 3.5M
262M SwitchAll 4 16.45 2.4G 5.6M
Transformer 16 16.58 5.4G 21M
peS2o 47M SwitchAll 2 12.56 202M 0.8M
Transformer 10 12.83 453M 3.5M
262M Transformer 16 9.78 5.4G 21M
SwitchAll 4 9.86 2.4G 5.6M
4 Phân tích
Để xem mạng sử dụng các attention heads như thế nào, chúng tôi đã huấn luyện một Transformer nhỏ, 6 lớp, 8 heads trên ListOps [ 33,34]. Lý do cho sự lựa chọn này là các nhiệm vụ thuật toán nhỏ có xu hướng dễ giải thích hơn so với các nhiệm vụ mô hình hóa ngôn ngữ. Chúng tôi cũng huấn luyện một SwitchHead parameter-matched, 2 heads 7

--- TRANG 8 ---
Bảng 4: Hiệu suất của SwitchHead được huấn luyện trên bộ dữ liệu C4, so với Transformer dense baseline có số tham số khớp.
Model #total params ppl ↓Lambada ↑BLiMP ↑CBT ↑
SwitchHead 47M 22.53 20 .4% 75 .7% -
Transformer 47M 22.71 20 .4% 73 .6% -
SwitchHead MAC-matched 63M 21.18 23 .5% 77 .1% -
SwitchHead Shared selection 47M 22.81 20 .0% 74 .6% -
SwitchHead 262M 16.23 29 .4% 79 .6% 83 .3%
Transformer 262M 16.28 28 .2% 76 .1% 83 .6%
SwitchHead MAC-matched 376M 15.43 30 .2% 79 .4% 84 .2%
SwitchHead Shared selection 262M 16.49 28 .6% 79 .4% 82 .7%
Bảng 5: Sử dụng tài nguyên thực tế của phương pháp chúng tôi. Các số được hiển thị dưới đây là cho thời gian huấn luyện của toàn bộ pipeline, bao gồm các lớp feedforward. Có thể thấy rằng SwitchHead trong triển khai hiện tại giảm cả runtime và sử dụng bộ nhớ với hệ số 1.4-1.5.
Size Model ms/iteration Rel. iter. time RAM/GPU Rel. Mem. #GPUs GPU type
47MTransformer 473ms/iter 1.0 20.5G 1.0
1 RTX 3090 SwitchHead 342ms/iter 0.72 13.5G 0.65
MoA 412ms/iter 0.87 15.3G 0.75
262MTransformer 670ms/iter 1.0 20.5G 1.0
8 V100 SwitchHead 442ms/iter 0.65 12.5G 0.61
MoA 851ms/iter 1.27 16.4G 0.80
mô hình. Cả hai mô hình đạt được khoảng 95% độ chính xác trên tập validation IID được giữ lại, trái ngược với mô hình dense 2 head, bão hòa khoảng 80%. Lưu ý rằng ListOps là một nhiệm vụ phân loại và không sử dụng autoregressive masking.
Chúng tôi trực quan hóa maximum của attention heads cho mỗi lớp, cả cho Transformer tiêu chuẩn (Fig. 2a) và SwitchHead (Fig. 2b). Các bản đồ attention định tính tương tự. Do khởi tạo khác nhau và động học học tập, do đó sự chồng chéo giữa hai mô hình sẽ không hoàn hảo. Các trực quan hóa bản đồ attention hoàn chỉnh có thể được tìm thấy trong Fig. 4 và 3 trong phụ lục.
Ngoài ra, chúng tôi phân tích các attention heads riêng lẻ cho SwitchHead. Chúng tôi thấy rằng thường có thể giải thích các trọng số lựa chọn: trên các nhiệm vụ synthetic, các output experts chuyên môn hóa theo các phép toán khác nhau, trong khi các input ones phân biệt số và dấu ngoặc đóng. Bản đồ attention chính nó dường như phân phối thông tin về các chunks liền kề của các số (xem Fig. 5 trong phụ lục).
Các bản đồ attention của các mô hình ngôn ngữ khó giải thích hơn. Tuy nhiên, chúng tôi trực quan hóa các bản đồ attention của Transformer XL 47M tham số và mô hình SwitchHead từ Tab. 2. Chúng tôi thấy chúng định tính tương tự. Chúng tôi cũng xác định các induction heads [ 35] trong cả hai mô hình, một số ví dụ được hiển thị cho SwitchHead trong Fig. 6a và cho Transformer trong Fig. 6b trong phụ lục. Các mô hình attention line dọc điển hình khác được hiển thị trong Fig. 6c và 6d.
5 Công trình liên quan
Phương pháp liên quan gần nhất với chúng tôi là MoA [ 18], giới thiệu attention kiểu MoE. Nó định nghĩa mỗi attention head như một expert nhưng chia sẻ các phép chiếu key và value giữa chúng. Không giống như trong Switchhead của chúng tôi, mỗi expert được chọn đòi hỏi một ma trận attention riêng biệt, điều này tăng đáng kể sử dụng bộ nhớ của nó. Do việc sử dụng hàm kích hoạt dựa trên softmax cạnh tranh trong mạng lựa chọn, nó đòi hỏi regularization phức tạp để ngăn chặn sự sụp đổ expert [ 17]. Trong công thức gốc, số lượng heads hoạt động cao. Các thí nghiệm của chúng tôi cũng xác nhận rằng MoA cần nhiều attention heads để khớp với hiệu suất của dense baseline (xem Sec. 3.2), và nó chỉ có thể làm được điều đó với ngân sách tài nguyên cao hơn đáng kể so với phương pháp của chúng tôi.
Nguyen et al. [36] phân tích các ma trận attention, và họ kết luận rằng chúng thường có rank thấp. Được thúc đẩy bởi điều này, các tác giả xây dựng một vài (ví dụ, 2) "ma trận attention toàn cục", và họ tính toán mỗi ma trận cục bộ cho các heads cụ thể bằng trung bình có trọng số của những ma trận đó. Tuy nhiên, họ lấy trung bình các logits, 8

--- TRANG 9 ---
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8(a) Transformer, Lớp 3
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8 (b) SwitchHead Lớp 3
Hình 2: Bản đồ attention của (a) Transformer tiêu chuẩn và (b) SwitchHead. Maximum của tất cả heads trong lớp đã cho được hiển thị.
không phải ma trận cuối cùng, vì vậy mỗi ma trận cụ thể cho head vẫn phải được tính toán. Điều này có nghĩa là trong trường hợp tốt nhất, họ chỉ có thể tiết kiệm một nửa tính toán liên quan đến ma trận attention vì readout (Eq. 3) vẫn cần thiết. Vì lý do tương tự, việc tiết kiệm bộ nhớ cũng thấp.
Peng et al. [19] đề xuất tái trọng số hóa đóng góp của mỗi head bằng một hàm gating. Tuy nhiên, họ chỉ giảm tổng số attention heads một, có lẽ để bù đắp cho các tham số được sử dụng bởi logic lựa chọn. Mục tiêu của họ không phải là giảm sử dụng tài nguyên mà là có hiệu suất dự đoán tốt hơn, điều mà họ đạt được. Họ sử dụng cơ chế lựa chọn cạnh tranh dựa trên softmax. Để tránh sụp đổ, hàm gating chỉ được huấn luyện trong một số bước.
Rộng hơn, đã có một số công trình về MoE để tăng tốc các mô hình ngôn ngữ. Shazeer et al. [11] giới thiệu sparsely-gated mixture of experts. Fedus et al. [37] giới thiệu Mixture of Experts trong Transformers. Lepikhin et al. [13] huấn luyện một LLM dựa trên MoE, và Clark et al. [15] phân tích các quy luật scaling của các mô hình MoE. Lewis et al. [12] giới thiệu một phương pháp thay thế để ngăn chặn sụp đổ. Tuy nhiên, không có phương pháp nào trong số này tập trung vào thiết lập parameter-matched quan trọng. Csordás et al. [17] giới thiệu phương pháp MoE dựa trên kích hoạt không cạnh tranh, σ-MoE, được chỉ ra là thành công trong thiết lập như vậy, nhưng các tác giả chỉ tập trung vào tăng tốc các MLPs và không phải attention.
Multi-Query attention [ 38] sử dụng một phép chiếu key và value duy nhất được chia sẻ giữa các heads trong khi sử dụng nhiều queries. Các phát hiện của chúng tôi cho thấy rằng cấu hình như vậy là không tối ưu: việc sử dụng nhiều phép chiếu output và value là lựa chọn quan trọng nhất trong thiết kế mô hình của chúng tôi.
Dao et al. [39] cung cấp triển khai CUDA nhận biết phần cứng của toàn bộ lớp attention, tránh lưu trữ ma trận attention. Bằng cách tiết kiệm băng thông bộ nhớ theo cách này, họ đạt được tăng tốc thời gian wall clock đáng kể, mặc dù ma trận attention nên được tính toán lại trong lượt truyền ngược. Điều này trực giao với phương pháp của chúng tôi và chúng có thể được kết hợp để tăng tốc thêm.
6 Hạn chế
Các mô hình của chúng tôi có kích thước khiêm tốn so với các LLMs hiện đại tiên tiến. Tuy nhiên, việc huấn luyện các mô hình như vậy được ước tính có chi phí hàng triệu đô la, điều mà chúng tôi không thể chi trả. Thay vào đó, chúng tôi nhắm đến việc chỉ ra tính linh hoạt của mô hình của mình bằng cách chọn một tập hợp đa dạng các bộ dữ liệu, bao gồm Enwik 8, Wikitext 103, C4 và peS2o, và các mã hóa vị trí khác nhau, như mã hóa vị trí tương đối kiểu Transformer-XL và RoPE. Chúng tôi cũng chứng minh tính cạnh tranh của các mô hình của mình trong các nhiệm vụ downstream zero-shot. Chúng tôi tin rằng bằng chứng mà chúng tôi cung cấp là đủ để một nhóm nghiên cứu có lượng tài nguyên lớn hơn có thể xác minh các phát hiện của chúng tôi trong một mô hình hiện đại.
9

--- TRANG 10 ---
Triton kernel mà chúng tôi sử dụng hiện tại khoảng 60% tốc độ của một phép nhân ma trận dense duy nhất có kích thước của một expert duy nhất với cuBLAS. Ngay cả điều này, chúng tôi đã cho thấy tăng tốc thời gian wall-clock. Chúng tôi ước tính rằng 80-90% nên có thể đạt được với một kernel tối ưu hơn. Huấn luyện song song mô hình đòi hỏi việc triển khai một hệ thống cân bằng tải có thể di chuyển experts một cách động giữa các GPUs.
7 Kết luận
Trên một loạt rộng các bộ dữ liệu mô hình hóa ngôn ngữ với các kích thước mô hình khác nhau, phương pháp attention dựa trên Mixture-of-Experts (MoE) mới của chúng tôi gọi là SwitchHead đạt được hiệu suất của các đối tác dense có cùng tham số, chỉ với một phần nhỏ chi phí tính toán và sử dụng bộ nhớ. SwitchHead giảm đáng kể số lượng ma trận attention phải được tính toán, bằng cách sử dụng MoE cho các phép chiếu value và output. Phương pháp của chúng tôi ổn định và không cần regularization bổ sung để ngăn chặn các giải pháp suy thoái (một vấn đề thực tế nổi tiếng trong nhiều mô hình MoE hiện tại). Phương pháp của chúng tôi cũng có thể được kết hợp thành công với các lớp MLP MoE, để thu được "SwitchAll" nơi mọi lớp của Transformer đều dựa trên MoE, đạt được sự giảm khổng lồ trong yêu cầu tài nguyên.
Lời cảm ơn
Nghiên cứu này được tài trợ một phần bởi tài trợ tiên tiến ERC số: 742870, dự án AlgoRNN, và bởi tài trợ Quỹ Khoa học Quốc gia Thụy Sĩ số: 200021_192356, dự án NEUSYM. Chúng tôi biết ơn những đóng góp phần cứng từ NVIDIA và IBM. Các tài nguyên được sử dụng cho công việc này được cung cấp một phần bởi Trung tâm Siêu máy tính Quốc gia Thụy Sĩ (CSCS) các dự án d123 và s1205.
Tài liệu tham khảo
[1]Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
[2]Tom B Brown et al. Language models are few-shot learners. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , Virtual only, December 2020.
[3] OpenAI. Chatgpt. https://openai.com/blog/chatgpt , 2022.
[4] OpenAI. GPT-4 technical report. Preprint arXiv:2303.08774 , 2023.
[5]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. Preprint arXiv:2303.12712 , 2023.
[6]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural Information Processing Systems (NIPS) , pages 5998–6008, Long Beach, CA, USA, December 2017.
[7]Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Neural Computation , 4(1):131–139, 1992.
[8] Georgi Gerganov. llama.cpp. https://github.com/ggerganov/llama.cpp , 2023.
[9]John B. Hampshire II and Alexander H. Waibel. The meta-pi network: connectionist rapid adaptation for high-performance multi-speaker phoneme recognition. In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP) , pages 165–168, Albuquerque, New Mexico, USA, April 1990.
[10] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton.. Adaptive mixtures of local experts. Neural Compututaion , 3(1):79–87, 1991.
[11] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In Int. Conf. on Learning Representations (ICLR) , Toulon, France, April 2017.
[12] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers: Simplifying training of large, sparse models. In Proc. Int. Conf. on Machine Learning (ICML) , volume 139, pages 6265–6274, Virtual only, July 2021.
10

--- TRANG 11 ---
[13] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In Int. Conf. on Learning Representations (ICLR) , Virtual only, May 2021.
[14] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research (JMLR) , 23(1):5232–5270, 2022.
[15] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack W. Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for routed language models. Preprint arXiv:2202.01169 , 2022.
[16] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture of experts. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , New Orleans, Louisiana, USA, December 2022.
[17] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. Approximating two-layer feedforward networks for efficient transformers. In Findings of the Association for Computational Linguistics: EMNLP 2023 , November 2023.
[18] Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, and Zhang Xiong. Mixture of attention heads: Selecting attention heads per token. In Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP) , pages 4150–4162, Abu Dhabi, United Arab Emirates, December 2022.
[19] Hao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith. A mixture of h - 1 heads is better than h heads. In Proc. Association for Computational Linguistics (ACL) , pages 6566–6577, Virtual only, July 2020.
[20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR) , 21:140:1–140:67, 2020.
[21] Marcus Hutter. The human knowledge compression prize. http://prize.hutter1.net , 2006.
[22] Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. https://github.com/allenai/pes2o .
[23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In Int. Conf. on Learning Representations (ICLR) , Toulon, France, April 2017.
[24] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proc. Association for Computational Linguistics (ACL) , Berlin, Germany, August 2016.
[25] Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. Blimp: The benchmark of linguistic minimal pairs for english. Transactions of the Association for Computational Linguistics (TACL) , 8:377–392, 2020.
[26] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks principle: Reading children's books with explicit memory representations. In Int. Conf. on Learning Representations (ICLR) , San Juan, Puerto Rico, May 2016.
[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. Preprint arXiv:2302.13971 , 2023.
[28] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced transformer with rotary position embedding. Preprint arXiv:2104.09864 , 2021.
11

--- TRANG 12 ---
[29] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proc. Association for Computational Linguistics (ACL) , pages 1715–1725, Berlin, Germany, August 2016.
[30] Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP) , pages 5149–5152, Kyoto, Japan, March 2012.
[31] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP) , pages 66–71, Brussels, Belgium, October 2018.
[32] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proc. Association for Computational Linguistics (ACL) , pages 2978–2988, Florence, Italy, 2019.
[33] Nikita Nangia and Samuel R. Bowman. ListOps: A diagnostic dataset for latent tree learning. InProc. North American Chapter of the Association for Computational Linguistics on Human Language Technologies (NAACL-HLT) , pages 92–99, New Orleans, USA, June 2018.
[34] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The neural data router: Adaptive control flow in transformers improves systematic generalization. In Int. Conf. on Learning Representations (ICLR) , Virtual only, April 2022.
[35] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread , 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
[36] Tan Nguyen, Tam Nguyen, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Duy Khuong Nguyen, Nhat Ho, and Stanley J. Osher. Improving transformer with an admixture of attention heads. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , New Orleans, LA, USA, November 2022.
[37] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Preprint arXiv:2101.03961 , 2021.
[38] Noam Shazeer. Fast transformer decoding: One write-head is all you need. Preprint arXiv:1911.02150 , 2019.
[39] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , New Orleans, Louisiana, USA, December 2022.
[40] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Int. Conf. on Learning Representations (ICLR) , San Diego, CA, USA, May 2015.
12

--- TRANG 13 ---
A Phụ lục
A.1 Một bình luận về Flash Attention
Sự giảm tài nguyên từ Flash Attention có thể, trong nhiều trường hợp, lớn hơn những sự giảm từ phương pháp của chúng tôi một mình. Tuy nhiên, Flash Attention phụ thuộc vào việc đánh đổi băng thông bộ nhớ/tính toán cụ thể cho GPU, có thể không có sẵn trên tất cả phần cứng, đặc biệt trên các thiết bị edge. SwitchHead và FlashAttention cũng có thể được kết hợp để tăng tốc thêm. Chúng tôi đã chứng minh tính khả thi của thiết lập này trong các thí nghiệm RoPE của chúng tôi. Ngoài ra, một số kiến trúc nhất định, như shared-layer transformers, có thể đòi hỏi sự tăng mạnh về số lượng heads, mà chỉ FlashAttention một mình có thể không thể làm được.
A.2 Sử dụng tài nguyên của các phương pháp khác nhau
Trong phần này, chúng tôi thảo luận về sử dụng tính toán và bộ nhớ của các biến thể attention khác nhau. Chúng tôi sẽ định nghĩa tính toán theo số lượng phép toán nhân-tích lũy (MACs, cũng được sử dụng bởi Zhang et al. [18]), được định nghĩa tốt hơn so với FLOPs (ví dụ, một bước của phép nhân ma trận tính là 1 FLOP hay 2? Chúng ta có bao gồm softmax không?). Tất cả các tính toán sẽ được trình bày cho một lớp attention duy nhất cho một chuỗi duy nhất, và chúng được trình bày theo cách này trong tất cả các bảng của chúng tôi. Cả yêu cầu bộ nhớ và tính toán đều mở rộng tuyến tính với cả kích thước batch và số lượng lớp.
Xét một chuỗi đầu vào có độ dài T, với kích thước biểu diễn dmodel. Gọi dhead là chiều rộng của các phép chiếu key, query và value được sử dụng cho lớp attention. Đối với attention kiểu Transformer XL, gọi kích thước của ngữ cảnh là CT, trong đó C−1 là số chunks quá khứ được bao gồm trong ngữ cảnh của bước attention hiện tại. Chúng ta có thể chia tính toán thành hai phần chính: tính toán các phép chiếu, không liên quan đến bản đồ attention, và tính toán bản đồ attention và chiếu chuỗi values bằng cách sử dụng nó.
Đầu tiên, xét trường hợp của Transformer XL tiêu chuẩn [ 32]. Ở đây, từ đầu vào x∈RT×dmodel, chúng ta tính toán Kh,Qh,Vh∈RT×dhead bằng cách sử dụng các ma trận chiếu có hình dạng Rdmodel×dhead. Output sau attention được chiếu theo cách tương tự (Eq. 3). Do đó, các phép chiếu tốn tổng cộng 4TdmodeldheadMACs mỗi head. Đối với backpropagation, chúng ta phải lưu trữ tất cả các kết quả trung gian. Điều này tốn Tdhead số cho Kh,Qh và Vh. Ngoài ra, các values đã chiếu nên được lưu trữ. Chúng có hình dạng giống hệt nhau, do đó, tổng bộ nhớ được sử dụng bởi các phép chiếu là 4Tdhead số mỗi head. Bây giờ xét sử dụng tài nguyên liên quan đến ma trận attention. Nó bao gồm việc tính toán tích của QhKh⊺, tốn dheadCT2MACs (phép nhân với C cần thiết vì hình dạng của Kh và Vh đối với Transformer XL là CT×dhead). Phép chiếu của các values với ma trận attention AhVh tương tự. Đối với sử dụng bộ nhớ, attention cần CT2 số, nhưng nó cần được lưu trữ cả trước và sau hàm kích hoạt. Ngoài ra, việc tính toán phép chiếu của các mã hóa vị trí là cần thiết. Điều này phụ thuộc vào triển khai, nhưng trong trường hợp của chúng tôi, nó bao gồm một phép nhân ma trận, và tổng lượng tính toán là 2dheaddmodelTC, và nó cần 2dheadTC số lưu trữ. Do đó yêu cầu tài nguyên là:
NXL
MAC=nheads
4Tdheaddmodel+ 2CT2dhead+ 2CTd headdmodel
(11)
NXL
mem=nheads
4Tdhead+ 2CT2+ 2CTd head
(12)
Sử dụng tài nguyên của SwitchHead khác. Đầu tiên, số lượng heads nheads được giảm đáng kể, nhưng dhead thường lớn hơn. Ngoài ra, có k experts hoạt động cùng một lúc. Ở đây, chúng tôi chỉ xét trường hợp mà value và outputs là experts, nhưng Qh và Kh không phải (phiên bản này hoạt động tốt nhất; xem Sec. 3.1). Sau đó, chúng ta có hai phép chiếu giống hệt với Transformer XL, và hai phép chiếu dựa trên MoE. Những cái này sử dụng Tkd modeldheadMACs để tính toán phép chiếu và thêm Tkd head khác để tính trung bình có trọng số của chúng. Với triển khai kernel thông minh, sử dụng bộ nhớ không bị ảnh hưởng bởi k, do đó công thức vẫn giống như Eq. 12 (lưu ý, tuy nhiên, nheads và dhead rất khác trong thực tế). Yêu cầu tính toán có thể được tính như:
NSwitchHead
MAC =nheads
2Tdheaddmodel+2Tkd head(dmodel+1)+2 CT2dhead+2CTd headdmodel
(13)
Ngoài ra, logic lựa chọn expert cần tài nguyên bổ sung tối thiểu, có thể được bỏ qua. Lưu ý rằng sự so sánh giữa MACs của tiêu chuẩn (Eq. 11) và SwitchHead (Eq. 13) phụ thuộc vào các giá trị chính xác của các siêu tham số. Tuy nhiên, như chúng ta sẽ thấy trong Sec. 3, trong các 13

--- TRANG 14 ---
Bảng 6: Hiệu suất của SwitchHead với E= 5 experts và nheads= 2 heads. Các phép chiếu khác nhau là experts hoặc cố định cho head đã cho. Các cột V, K, Q, và O cho thấy liệu phép chiếu đã cho có phải là expert hay không. Baseline parameter-matched với nheads= 10 và nheads= 2 được hiển thị. Các mô hình được sắp xếp theo perplexity. Các mô hình 47M tham số trên Wikitext 103.
Model nheads V K Q O Perplexity ↓
SwitchHead 2 Y N N Y 12.27
SwitchHead 2 N N N Y 12.30
Transformer 10 - - - - 12.31
SwitchHead 2 N Y N Y 12.36
SwitchHead 2 Y Y N Y 12.37
SwitchHead 2 Y N Y Y 12.42
SwitchHead 2 Y N N N 12.45
SwitchHead 2 N N Y Y 12.45
SwitchHead 2 Y N Y N 12.51
SwitchHead 2 Y Y Y Y 12.57
SwitchHead 2 N Y Y Y 12.59
SwitchHead 2 Y Y Y N 12.61
SwitchHead 2 Y Y N N 12.69
Transformer 2 - - - - 12.74
SwitchHead 2 N N Y N 12.75
SwitchHead 2 N Y N N 12.79
SwitchHead 2 N Y Y N 12.90
cấu hình điển hình của chúng tôi, SwitchHead cung cấp hiệu suất dự đoán tốt với nheads thấp hơn đáng kể so với Transformer tiêu chuẩn, dẫn đến sử dụng tài nguyên giảm cuối cùng.
Yêu cầu tài nguyên của MoA [ 19] rất tương tự với Transformer XL, ngoại trừ việc nó sử dụng một phép chiếu key và value được chia sẻ duy nhất cho mỗi head.
NMoA
MAC= (2nheads+ 2)Tdheaddmodel+ 2nheadsCT2dhead+ 2CTd headdmodel (14)
NMoA
mem= (2nheads+ 2)Tdhead+ 2nheadsCT2+ 2CTd head (15)
A.3 Tầm quan trọng của các phép chiếu khác nhau
Để phân tích phép chiếu nào quan trọng nhất để trở thành mixture-of-experts, chúng tôi đã thử triệt để tất cả các kết hợp. Chúng tôi phân tích các mô hình 47M tham số của mình trên bộ dữ liệu WikiText 103. Chúng tôi hiển thị kết quả trong Tab. 6. Chúng tôi cũng bao gồm baseline parameter-matched với hai heads, phục vụ như cận dưới cho hiệu suất. Chúng tôi thấy rằng các phép chiếu value và output quan trọng nhất, và việc có các phép chiếu key và query làm tổn hại hiệu suất. Điều này có thể vì chúng tôi thực hiện tất cả các thí nghiệm của mình trong thiết lập parameter-matched. Việc phân bổ tham số cho các phép chiếu này sử dụng ngân sách có thể được chi tiêu cho các phần khác của mạng. Trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi thấy rằng, cho phép ngân sách tham số tăng, nhiều experts luôn giúp ích.
A.4 Mã hóa vị trí RoPE
Tất cả các thí nghiệm của chúng tôi trong bài báo chính đã sử dụng mô hình Transformer XL. Do đó, vẫn chưa rõ liệu SwitchHead có cụ thể cho mô hình này hay cũng có thể được sử dụng với các phương pháp attention khác. Như một thay thế, chúng tôi xem xét mã hóa vị trí RoPE [ 28] mà không có bộ nhớ cache XL (do đó, các ma trận attention là vuông). Đây là thiết lập tiêu chuẩn được sử dụng bởi các mô hình ngôn ngữ hiện đại, như tất cả các phiên bản của Llama [ 27]. Chúng tôi đã kiểm tra các mô hình này trong Wikitext 103 và C4. Kết quả được hiển thị trong Tab. 7, và hiệu suất zero-shot trên các nhiệm vụ downstream trong Tab. 8. Điều này cho thấy rằng SwitchHead cũng hoạt động tốt trong thiết lập tiêu chuẩn và không gắn liền với Transformer XL.
A.5 Siêu tham số
Chúng tôi huấn luyện tất cả các mô hình của mình với optimizer Adam [ 40], với kích thước batch 64, tỷ lệ học 0.00025, và gradient clipping với chuẩn tối đa κ. Các mô hình lớn ( >200K tham số) sử dụng tỷ lệ học 14

--- TRANG 15 ---
Bảng 7: Perplexity của SwitchHead so với dense baseline, sử dụng mã hóa vị trí RoPE và không có bộ nhớ cache XL. Sử dụng bộ nhớ được chỉ định theo số lượng floats. Các mô hình được sắp xếp theo perplexity.
Dataset #total params Model nheads ppl↓ MACs Memory
Wikitext 103 45M SwitchHead 2 12.75 285.6M 1.3M
Transformer 10 12.78 560.9M 6.1M
Transformer 2 12.96 560.9M 1.9M
244M SwitchHead 4 10.00 4.2G 18.4M
Transformer 16 10.17 6.4G 37.7M
Transformer 2 10.26 6.4G 8.4M
C4 45M SwitchHead 2 23.69 285.6M 1.3M
Transformer 10 23.79 560.9M 6.1M
244M SwitchHead 4 16.41 4.2G 18.4M
Transformer 16 16.35 6.4G 37.7M
Bảng 8: Hiệu suất nhiệm vụ zero-shot của SwitchHead sử dụng mã hóa vị trí RoPE và không có bộ nhớ cache XL, được huấn luyện trên bộ dữ liệu C4, so với Transformer dense baseline có số tham số khớp.
Model #total params ppl ↓Lambada ↑BLiMP ↑CBT ↑
SwitchHead 45M 23.69 20 .9% 77 .3% -
Transformer 45M 23.76 20 .3% 73 .8% -
SwitchHead MAC-matched 54M 22.18 22 .6% 77 .4% -
SwitchHead Shared selection 45M 23.63 20 .3% 76 .0% -
SwitchHead 243M 16.41 30 .5% 79 .9% 83 .8%
Transformer 243M 16.35 29 .8% 76 .1% 83 .9%
SwitchHead MAC-matched 314M 15.63 30 .5% 80 .5% 84 .6%
SwitchHead Shared selection 243M 16.59 28 .1% 79 .1% 83 .7%
warm-up 4k bước. Tất cả các mô hình, ngoại trừ mô hình SwitchAll, sử dụng dropout trên các lớp MLP, 0.1 cho các mô hình nhỏ và 0.2 cho các mô hình lớn. Các siêu tham số chi tiết được hiển thị trong Tab. 9. Các siêu tham số liên quan đến σ-MoE cho các mô hình SwitchAll giống hệt với của Csordás et al. [17]. Đối với các mô hình Transformer XL, chúng tôi luôn sử dụng một chunk bổ sung duy nhất của ngữ cảnh, cả trong thời gian huấn luyện và validation. dhead và dff được suy ra theo cách có hệ thống, xem Sec. 3 để biết thêm chi tiết.
A.6 Một lưu ý về số lượng tham số của SwitchAll
Có thể thấy trong Tab. 3 rằng số lượng tham số của các mô hình SwitchAll thường ít hơn so với các đối tác dense của chúng. Lý do là chúng tôi thường bù đắp cho sự khác biệt cuối cùng trong số lượng tham số bằng cách tăng dff (xem Sec. 3 để biết chi tiết của việc khớp tham số). Tuy nhiên, điều đó chỉ có thể được thực hiện theo cách rất thô với σ-MoE: kích thước của tất cả experts phải được tăng cùng một lúc, và CUDA kernel chỉ hỗ trợ kích thước bội số của 4. Do đó, việc tăng kích thước của các experts sẽ thêm quá nhiều tham số và mô hình sẽ vượt quá baseline. Vì lý do này, chúng tôi đơn giản giữ các siêu tham số cho Csordás et al. [17] và kết hợp chúng với cấu hình SwitchHead của chúng tôi từ Tab. 2.
A.7 Trực quan hóa tất cả Attention Heads
Như đã thảo luận trong Sec. 4, chúng tôi phân tích các bản đồ attention của SwitchHead và so sánh chúng với các mô hình dense. Chúng tôi hiển thị tất cả các bản đồ attention của các mô hình được huấn luyện trên ListOps trong Fig. 3 và Fig. 3. Chúng tôi hiển thị các heads riêng lẻ của SwitchHead, bao gồm các điểm lựa chọn expert trong Fig. 5. Một số bản đồ attention được chọn của các mô hình 47M tham số của chúng tôi trên Wikitext 103 được hiển thị trong Fig. 6.
A.8 Yêu cầu Tính toán
Chúng tôi báo cáo tính toán được sử dụng cho các thí nghiệm của mình, bao gồm loại GPU, số lượng (số GPUs được sử dụng cho mỗi thí nghiệm, và không phải tổng số trong máy), và runtime trong định dạng "hh:mm" 15

--- TRANG 16 ---
Bảng 9: Siêu tham số được sử dụng cho các mô hình của chúng tôi.
Model Dataset nheads #params dhead dffEk Tnlayers κ
SwitchHead
C42 47M 76 2080 5 3 256 16 0.1
Transformer 10 47M 41 2053 - - 256 16 0.1
Transformer 2 47M 205 2053 - - 256 16 0.1
SwitchHead
C44 262M 112 4188 4 2 512 18 0.25
Transformer 16 262M 64 4110 - - 512 18 0.25
Transformer 4 262M 256 4110 - - 512 18 0.25
SwitchHead
Wikitext 1032 47M 76 2080 5 2 256 16 0.1
Transformer 10 47M 41 2053 - - 256 16 0.1
Transformer 2 47M 205 2053 - - 256 16 0.1
SwitchHead
Wikitext 1032 262M 132 4147 8 4 512 18 0.25
Transformer 16 262M 64 4110 - - 512 18 0.25
Transformer 2 262M 512 4110 - - 512 18 0.25
SwitchHead
peS2o2 47M 76 2080 5 3 256 16 0.1
Transformer 10 47M 41 2053 - - 256 16 0.1
Transformer 2 47M 205 2053 - - 256 16 0.1
SwitchHead
peS2o4 262M 112 4188 4 2 512 18 0.25
Transformer 16 262M 64 4110 - - 512 18 0.25
Transformer 4 262M 256 4110 - - 512 18 0.25
SwitchHead
Enwik82 41M 112 2088 4 2 512 12 0.25
Transformer 8 41M 64 2053 - - 512 12 0.25
Transformer 2 41M 256 2053 - - 512 12 0.25
SwitchHead (RoPE)Wikitext 1032 45M 64 2092 5 3 512 16 0.1
Transformer (RoPE) 10 45M 41 2053 - - 512 16 0.1
SwitchHead (RoPE)Wikitext 1034 243M 100 4136 4 2 1024 18 0.25
Transformer (RoPE) 16 244M 64 4110 - - 1024 18 0.25
SwitchAll Wikitext 103 2 47M 76 1648 5 2 256 16 0.25
SwitchAll Wikitext 103 4 259M 112 4096 4 2 512 18 0.25
SwitchAll C4 2 47M 76 1648 5 3 256 16 0.25
SwitchAll C4 4 259M 112 4096 4 2 512 18 0.25
SwitchAll peS2o 2 47M 76 1648 5 3 256 16 0.25
SwitchAll peS2o 4 259M 112 4096 4 2 512 18 0.25
trong Tab. 10. Chúng tôi báo cáo tổng số CPUs ( NCPU) và RAM vì chúng được chia sẻ giữa các lần chạy đồng thời. Lưu ý rằng hầu hết các thí nghiệm được thực hiện trước khi triển khai kernel dựa trên Triton nhanh hơn nhiều. Vì điều này, runtimes xuất hiện dài hơn cho SwitcHead so với baseline. Để biết các benchmark thời gian với kernel mới của chúng tôi, xem Tab. 5.
Lưu ý rằng chúng tôi chỉ báo cáo các tài nguyên được sử dụng cho bài báo ở đây. Chúng tôi ước tính rằng tổng chi phí của các thí nghiệm thất bại và các lần chạy sơ bộ cao hơn khoảng 10 lần so với điều này.
16

--- TRANG 17 ---
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.10.20.30.40.50.60.70.8(a) Lớp 1
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8 (b) Lớp 2
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8 (c) Lớp 3
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.0250.0500.0750.1000.1250.1500.175
(d) Lớp 4
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.050.100.150.200.25 (e) Lớp 5
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.050.100.150.200.250.300.35 (f) Lớp 6
Hình 3: Maximum của tất cả các bản đồ attention cho mô hình SwitchHead trên ListOps.
17

--- TRANG 18 ---
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8(a) Lớp 1
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8 (b) Lớp 2
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8 (c) Lớp 3
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.20.40.60.8
(d) Lớp 4
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.10.20.30.40.50.60.70.8 (e) Lớp 5
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcB
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edest
0.050.100.150.200.250.300.350.40 (f) Lớp 6
Hình 4: Maximum của tất cả các bản đồ attention cho mô hình Transformer tiêu chuẩn trên ListOps.
18

--- TRANG 19 ---
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.10.20.30.40.50.60.70.8
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv(a) Lớp 1, head 1
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.050.100.150.200.250.300.35
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (b) Lớp 1, head 2
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.10.20.30.40.50.60.70.8
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (c) Lớp 2, head 1
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.20.40.60.8
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv
(d) Lớp 2, head 2
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.20.40.60.8
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (e) Lớp 3, head 1
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.00.20.40.60.8
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (f) Lớp 3, head 2
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.0250.0500.0750.1000.1250.1500.175
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv
(g) Lớp 4, head 1
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.020.040.060.080.100.120.140.16
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (h) Lớp 4, head 2
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.050.100.150.200.25
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (i) Lớp 5, head 1
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.020.040.060.080.100.120.140.16
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv
(j) Lớp 5, head 2
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.050.100.150.200.250.300.35
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (k) Lớp 6, head 1
B
[MED
[MED
[MIN
0
8
9
5
]
8
8
0
9
]
6
9
6
[MAX
7
6
[MAX
4
[MAX
5
[MAX
3
8
0
8
]
[MIN
2
6
]
5
]
1
5
4
]
5
]
]
Edesto
0.0250.0500.0750.1000.1250.1500.1750.200
B
[MED[MED[MIN0895]8809]696
[MAX76
[MAX4
[MAX5
[MAX3808]
[MIN26]5]154]5]]E
srcv (l) Lớp 6, head 2
Hình 5: Chi tiết cho các heads riêng lẻ của mô hình SwitchHead trên ListOps. Ở phía bên trái của mỗi biểu đồ attention, việc lựa chọn expert phép chiếu output được hiển thị. Tương tự, ở phía dưới, việc lựa chọn phép chiếu value có thể nhìn thấy. Trong các bản đồ lựa chọn, màu xanh đậm luôn tương ứng với 1, trong khi màu trắng là 0. Thang đo thích ứng được hiển thị ở bên phải của bản đồ attention chỉ dành cho bản đồ đó.
19

--- TRANG 20 ---
lob
sterred
"
on
cook
ing.
M
atingoccurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atchinginto
pl
anktoniclar
vae.
H
omarusg
ammarusis
a
highly
est
e
emedfood
,
and
is
widely
caught
using
lob
sterp
ots,
mostly
around
the
British
Is
les.
=
=
Desc
ription=
=
H
omarusg
ammarusis
a
large
cr
ustacean,
with
a
body
length
up
to
60
cent
imetres(
24in
)
and
we
ighingup
to
5
 6kil
og
rams
srcster
red
"
on
cook
ing
.
M
ating
occurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atch
ing
into
pl
ank
ton
ic
lar
v
ae
.
H
om
ar
us
g
am
mar
us
is
a
highly
est
e
emed
food
,
and
is
widely
caught
using
lob
ster
p
ots
,
mostly
around
the
British
Is
les
.
=
=
Desc
ription
=
=
H
om
ar
us
g
am
mar
us
is
a
large
cr
ust
ace
an
,
with
a
body
length
up
to
6
0
cent
imet
res
(
2
4
in
)
and
we
igh
ing
up
to
5
6
kil
og
rams
(
dest
0.00.20.40.60.8(a) SwitchHead Lớp 12. Induction head.
lob
sterred
"
on
cook
ing.
M
atingoccurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atchinginto
pl
anktoniclar
vae.
H
omarusg
ammarusis
a
highly
est
e
emedfood
,
and
is
widely
caught
using
lob
sterp
ots,
mostly
around
the
British
Is
les.
=
=
Desc
ription=
=
H
omarusg
ammarusis
a
large
cr
ustacean,
with
a
body
length
up
to
60
cent
imetres(
24in
)
and
we
ighingup
to
5
 6kil
og
rams
srcster
red
"
on
cook
ing
.
M
ating
occurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atch
ing
into
pl
ank
ton
ic
lar
v
ae
.
H
om
ar
us
g
am
mar
us
is
a
highly
est
e
emed
food
,
and
is
widely
caught
using
lob
ster
p
ots
,
mostly
around
the
British
Is
les
.
=
=
Desc
ription
=
=
H
om
ar
us
g
am
mar
us
is
a
large
cr
ust
ace
an
,
with
a
body
length
up
to
6
0
cent
imet
res
(
2
4
in
)
and
we
igh
ing
up
to
5
6
kil
og
rams
(
dest
0.00.20.40.60.8 (b) Transformer XL Lớp 10. Induction head.
lob
sterred
"
on
cook
ing.
M
atingoccurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atchinginto
pl
anktoniclar
vae.
H
omarusg
ammarusis
a
highly
est
e
emedfood
,
and
is
widely
caught
using
lob
sterp
ots,
mostly
around
the
British
Is
les.
=
=
Desc
ription=
=
H
omarusg
ammarusis
a
large
cr
ustacean,
with
a
body
length
up
to
60
cent
imetres(
24in
)
and
we
ighingup
to
5
 6kil
og
rams
srcster
red
"
on
cook
ing
.
M
ating
occurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atch
ing
into
pl
ank
ton
ic
lar
v
ae
.
H
om
ar
us
g
am
mar
us
is
a
highly
est
e
emed
food
,
and
is
widely
caught
using
lob
ster
p
ots
,
mostly
around
the
British
Is
les
.
=
=
Desc
ription
=
=
H
om
ar
us
g
am
mar
us
is
a
large
cr
ust
ace
an
,
with
a
body
length
up
to
6
0
cent
imet
res
(
2
4
in
)
and
we
igh
ing
up
to
5
6
kil
og
rams
(
dest
0.00.20.40.60.8
(c) SwitchHead Lớp 9. Mô hình sọc.
lob
sterred
"
on
cook
ing.
M
atingoccurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atchinginto
pl
anktoniclar
vae.
H
omarusg
ammarusis
a
highly
est
e
emedfood
,
and
is
widely
caught
using
lob
sterp
ots,
mostly
around
the
British
Is
les.
=
=
Desc
ription=
=
H
omarusg
ammarusis
a
large
cr
ustacean,
with
a
body
length
up
to
60
cent
imetres(
24in
)
and
we
ighingup
to
5
 6kil
og
rams
srcster
red
"
on
cook
ing
.
M
ating
occurs
in
the
summer
,
producing
eggs
which
are
carried
by
the
females
for
up
to
a
year
before
h
atch
ing
into
pl
ank
ton
ic
lar
v
ae
.
H
om
ar
us
g
am
mar
us
is
a
highly
est
e
emed
food
,
and
is
widely
caught
using
lob
ster
p
ots
,
mostly
around
the
British
Is
les
.
=
=
Desc
ription
=
=
H
om
ar
us
g
am
mar
us
is
a
large
cr
ust
ace
an
,
with
a
body
length
up
to
6
0
cent
imet
res
(
2
4
in
)
and
we
igh
ing
up
to
5
6
kil
og
rams
(
dest
0.00.20.40.60.8 (d) Transformer XL Lớp 8. Mô hình sọc.
Hình 6: Induction head sao chép tên hiếm "Homarus" trong (a) SwitchHead và (b) Transformer XL baseline. Ma trận attention là vuông vì nó là chunk đầu tiên của chuỗi, không có ngữ cảnh bổ sung nào. Mô hình line dọc điển hình trong (c) SwitchHead và (b) Transformer XL baseline.
20

--- TRANG 21 ---
Bảng 10: Thông tin phần cứng huấn luyện cho các thí nghiệm được báo cáo trong bài báo
Model #params Dataset G GPU Type NGPU NCPU RAM Duration
SwitchAll 259M C4 4 V100-32GB-LS 8 40 503G 24:06
SwitchAll 259M peS2o 4 V100-32GB-LS 8 40 503G 30:00
SwitchAll 259M Wikitext 103 4 RTX 4090 4 24 251G 22:58
SwitchAll 47M C4 2 RTX 3090 1 24 220G 22:14
SwitchAll 47M peS2o 2 RTX 3090 1 24 220G 22:49
SwitchAll 47M Wikitext 103 2 RTX 3090 1 24 251G 6:03
SwitchHead 243M Wikitext 103 4 V100-32GB 4 40 503G 147:09
SwitchHead 262M C4 4 V100-32GB-LS 8 40 503G 26:38
SwitchHead 262M peS2o 4 V100-32GB-LS 8 40 503G 27:43
SwitchHead 262M Wikitext 103 2 V100-32GB 4 40 503G 31:42
SwitchHead 41M Enwik8 2 V100-32GB 1 40 503G 13:45
SwitchHead 45M Wikitext 103 2 RTX 3090 1 24 251G 17:28
SwitchHead 47M C4 2 V100-32GB 1 40 503G 15:36
SwitchHead 47M peS2o 2 V100-32GB 1 40 503G 16:17
SwitchHead 47M Wikitext 103 2 RTX 3090 1 24 251G 13:09
Transformer 262M C4 4 V100-32GB 8 40 503G 11:55
Transformer 262M C4 16 V100-32GB-LS 8 40 503G 20:21
Transformer 262M peS2o 4 V100-32GB 8 40 503G 17:08
Transformer 262M peS2o 16 V100-32GB-LS 8 40 503G 25:56
Transformer 262M Wikitext 103 2 P100-16GB 8 12 62G 0:00
Transformer 262M Wikitext 103 16 A100-80GB 2 64 503G 31:51
Transformer 41M Enwik8 2 RTX 3090 1 24 220G 15:38
Transformer 41M Enwik8 8 V100-32GB-LS 2 40 503G 16:04
Transformer 47M C4 2 V100-32GB 1 40 503G 10:29
Transformer 47M C4 10 V100-32GB 1 40 503G 16:57
Transformer 47M peS2o 2 V100-32GB 1 40 503G 11:07
Transformer 47M peS2o 10 V100-32GB 1 40 503G 17:55
Transformer 47M Wikitext 103 2 V100-32GB 1 40 503G 10:06
Transformer 47M Wikitext 103 10 V100-32GB 1 40 503G 18:51
Transformer (RoPE) 244M Wikitext 103 16 RTX 3090 4 24 251G 30:30
Transformer (RoPE) 45M Wikitext 103 10 V100-32GB 1 40 503G 15:30
21
